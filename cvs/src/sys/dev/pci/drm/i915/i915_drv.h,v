head	1.78;
access;
symbols
	OPENBSD_6_1:1.74.0.6
	OPENBSD_6_1_BASE:1.74
	OPENBSD_6_0:1.74.0.4
	OPENBSD_6_0_BASE:1.74
	OPENBSD_5_9:1.73.0.2
	OPENBSD_5_9_BASE:1.73
	OPENBSD_5_8:1.66.0.4
	OPENBSD_5_8_BASE:1.66
	OPENBSD_5_7:1.56.0.4
	OPENBSD_5_7_BASE:1.56
	OPENBSD_5_6:1.52.0.4
	OPENBSD_5_6_BASE:1.52
	OPENBSD_5_5:1.50.0.4
	OPENBSD_5_5_BASE:1.50
	OPENBSD_5_4:1.24.0.2
	OPENBSD_5_4_BASE:1.24;
locks; strict;
comment	@ * @;


1.78
date	2017.09.30.07.36.56;	author robert;	state Exp;
branches;
next	1.77;
commitid	XkWOvf4INOmfke72;

1.77
date	2017.07.19.22.05.58;	author kettenis;	state Exp;
branches;
next	1.76;
commitid	e9Q72NCdXpHdLN3j;

1.76
date	2017.07.05.20.30.13;	author kettenis;	state Exp;
branches;
next	1.75;
commitid	wA527vsQhGJhZItC;

1.75
date	2017.07.01.16.14.10;	author kettenis;	state Exp;
branches;
next	1.74;
commitid	KnwRPOZok9A30HI4;

1.74
date	2016.04.05.21.22.02;	author kettenis;	state Exp;
branches;
next	1.73;
commitid	HESbnwM3z1fH4IBz;

1.73
date	2015.12.09.05.17.44;	author jsg;	state Exp;
branches;
next	1.72;
commitid	s0XYUHVZL3Xim3bC;

1.72
date	2015.11.01.14.07.43;	author jsg;	state Exp;
branches;
next	1.71;
commitid	2BBJb4X7CkET778r;

1.71
date	2015.10.29.07.47.03;	author kettenis;	state Exp;
branches;
next	1.70;
commitid	QDyL8dFxcUx9R2yh;

1.70
date	2015.10.17.21.41.12;	author kettenis;	state Exp;
branches;
next	1.69;
commitid	bHt1y1EnMlJS0gRQ;

1.69
date	2015.09.28.17.29.56;	author kettenis;	state Exp;
branches;
next	1.68;
commitid	UgiNWTglvSDYWYPZ;

1.68
date	2015.09.26.11.17.15;	author kettenis;	state Exp;
branches;
next	1.67;
commitid	DUG1LQonw3dWeI9h;

1.67
date	2015.09.23.23.12.12;	author kettenis;	state Exp;
branches;
next	1.66;
commitid	lQlppvmETCN49oZe;

1.66
date	2015.06.26.15.22.23;	author kettenis;	state Exp;
branches;
next	1.65;
commitid	zE715ZsjyYsYQNQ0;

1.65
date	2015.06.24.17.59.42;	author kettenis;	state Exp;
branches;
next	1.64;
commitid	3wz7SV1D1yJbfWE9;

1.64
date	2015.06.24.08.32.39;	author kettenis;	state Exp;
branches;
next	1.63;
commitid	hfEqCdm8ecmxIgUE;

1.63
date	2015.06.04.06.11.21;	author jsg;	state Exp;
branches;
next	1.62;
commitid	yCghTcY7mTgiU7J1;

1.62
date	2015.04.18.11.41.28;	author jsg;	state Exp;
branches;
next	1.61;
commitid	c3CbYQJYD10xhd6O;

1.61
date	2015.04.17.00.54.42;	author jsg;	state Exp;
branches;
next	1.60;
commitid	LqdQe79hlknpVtvI;

1.60
date	2015.04.11.05.10.13;	author jsg;	state Exp;
branches;
next	1.59;
commitid	pjaRMzmEKTQk8EZt;

1.59
date	2015.04.11.02.24.43;	author jsg;	state Exp;
branches;
next	1.58;
commitid	CNp7DDD9wruE5rh2;

1.58
date	2015.04.05.11.53.53;	author kettenis;	state Exp;
branches;
next	1.57;
commitid	3YXcRggXXMDC9Cpg;

1.57
date	2015.04.03.13.10.59;	author jsg;	state Exp;
branches;
next	1.56;
commitid	PZIJ62HYZVx8fbpI;

1.56
date	2015.02.12.04.56.03;	author kettenis;	state Exp;
branches;
next	1.55;
commitid	adfbJ0ccUTdhFGhI;

1.55
date	2015.01.27.03.17.36;	author dlg;	state Exp;
branches;
next	1.54;
commitid	MyKPm9Q3dQu92BiX;

1.54
date	2014.12.20.16.34.27;	author krw;	state Exp;
branches;
next	1.53;
commitid	HzEf4vz6A0HRFIhU;

1.53
date	2014.11.06.05.48.42;	author jsg;	state Exp;
branches;
next	1.52;
commitid	xwqnjiedsVXzBXqo;

1.52
date	2014.05.12.19.29.16;	author kettenis;	state Exp;
branches;
next	1.51;

1.51
date	2014.03.25.17.44.39;	author mpi;	state Exp;
branches;
next	1.50;

1.50
date	2014.02.19.01.20.12;	author jsg;	state Exp;
branches;
next	1.49;

1.49
date	2014.02.19.01.14.41;	author jsg;	state Exp;
branches;
next	1.48;

1.48
date	2014.01.24.04.05.06;	author jsg;	state Exp;
branches;
next	1.47;

1.47
date	2014.01.24.03.21.17;	author jsg;	state Exp;
branches;
next	1.46;

1.46
date	2014.01.23.10.42.57;	author jsg;	state Exp;
branches;
next	1.45;

1.45
date	2013.12.11.20.31.43;	author kettenis;	state Exp;
branches;
next	1.44;

1.44
date	2013.12.05.13.29.56;	author kettenis;	state Exp;
branches;
next	1.43;

1.43
date	2013.12.03.16.19.43;	author kettenis;	state Exp;
branches;
next	1.42;

1.42
date	2013.12.01.20.33.57;	author kettenis;	state Exp;
branches;
next	1.41;

1.41
date	2013.12.01.11.47.13;	author kettenis;	state Exp;
branches;
next	1.40;

1.40
date	2013.11.30.20.13.36;	author kettenis;	state Exp;
branches;
next	1.39;

1.39
date	2013.11.30.20.03.32;	author kettenis;	state Exp;
branches;
next	1.38;

1.38
date	2013.11.27.20.13.30;	author kettenis;	state Exp;
branches;
next	1.37;

1.37
date	2013.11.20.02.03.52;	author jsg;	state Exp;
branches;
next	1.36;

1.36
date	2013.11.19.19.14.09;	author kettenis;	state Exp;
branches;
next	1.35;

1.35
date	2013.11.19.15.08.04;	author jsg;	state Exp;
branches;
next	1.34;

1.34
date	2013.11.17.20.04.47;	author kettenis;	state Exp;
branches;
next	1.33;

1.33
date	2013.11.17.18.47.13;	author kettenis;	state Exp;
branches;
next	1.32;

1.32
date	2013.11.16.16.15.36;	author kettenis;	state Exp;
branches;
next	1.31;

1.31
date	2013.10.29.06.30.57;	author jsg;	state Exp;
branches;
next	1.30;

1.30
date	2013.10.05.07.30.05;	author jsg;	state Exp;
branches;
next	1.29;

1.29
date	2013.09.30.06.47.48;	author jsg;	state Exp;
branches;
next	1.28;

1.28
date	2013.09.18.08.50.28;	author jsg;	state Exp;
branches;
next	1.27;

1.27
date	2013.08.13.10.23.49;	author jsg;	state Exp;
branches;
next	1.26;

1.26
date	2013.08.07.19.49.06;	author kettenis;	state Exp;
branches;
next	1.25;

1.25
date	2013.08.07.00.04.28;	author jsg;	state Exp;
branches;
next	1.24;

1.24
date	2013.07.05.07.20.27;	author jsg;	state Exp;
branches;
next	1.23;

1.23
date	2013.07.04.09.52.29;	author jsg;	state Exp;
branches;
next	1.22;

1.22
date	2013.05.21.22.12.58;	author kettenis;	state Exp;
branches;
next	1.21;

1.21
date	2013.05.18.21.43.42;	author kettenis;	state Exp;
branches;
next	1.20;

1.20
date	2013.05.11.15.56.28;	author kettenis;	state Exp;
branches;
next	1.19;

1.19
date	2013.05.09.15.00.41;	author kettenis;	state Exp;
branches;
next	1.18;

1.18
date	2013.05.08.23.01.36;	author kettenis;	state Exp;
branches;
next	1.17;

1.17
date	2013.05.05.13.55.36;	author kettenis;	state Exp;
branches;
next	1.16;

1.16
date	2013.05.05.13.02.46;	author kettenis;	state Exp;
branches;
next	1.15;

1.15
date	2013.04.21.14.41.26;	author kettenis;	state Exp;
branches;
next	1.14;

1.14
date	2013.04.17.20.04.04;	author kettenis;	state Exp;
branches;
next	1.13;

1.13
date	2013.04.14.19.04.37;	author kettenis;	state Exp;
branches;
next	1.12;

1.12
date	2013.04.03.07.36.57;	author jsg;	state Exp;
branches;
next	1.11;

1.11
date	2013.03.31.11.43.23;	author kettenis;	state Exp;
branches;
next	1.10;

1.10
date	2013.03.30.04.57.53;	author jsg;	state Exp;
branches;
next	1.9;

1.9
date	2013.03.28.19.36.14;	author kettenis;	state Exp;
branches;
next	1.8;

1.8
date	2013.03.28.11.51.05;	author jsg;	state Exp;
branches;
next	1.7;

1.7
date	2013.03.28.05.13.07;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2013.03.25.19.50.56;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2013.03.22.22.51.00;	author kettenis;	state Exp;
branches;
next	1.4;

1.4
date	2013.03.22.06.19.56;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2013.03.21.08.27.32;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2013.03.20.12.37.41;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2013.03.18.12.36.52;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.78
log
@Add preliminary kabylake support to inteldrm(4) by backporting the relevant
commits from linux-4.8.x.
The changes are quiet minimal due to the fact that kabylake and skylake share
most of the code because they are both gen9 graphics.
This was tested by many and was also in snapshots for a while.

ok kettenis@@
@
text
@/* $OpenBSD: i915_drv.h,v 1.77 2017/07/19 22:05:58 kettenis Exp $ */
/* i915_drv.h -- Private header for the I915 driver -*- linux-c -*-
 */
/*
 *
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 */

#ifndef _I915_DRV_H_
#define _I915_DRV_H_

#include <dev/pci/drm/i915_drm.h>
#include <dev/pci/drm/drm_fourcc.h>

#include "i915_reg.h"
#include "intel_bios.h"
#include "intel_ringbuffer.h"
#include "intel_lrc.h"
#include "i915_gem_gtt.h"
#include "i915_gem_render_state.h"
#include "intel_guc.h"

struct sg_table;

#define CONFIG_DRM_I915_FBDEV 1
#define CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT 1

#include "acpi.h"
#if NACPI > 0
#define CONFIG_ACPI
#endif

#include "drm.h"
#include "vga.h"

#include <dev/ic/mc6845reg.h>
#include <dev/ic/pcdisplayvar.h>
#include <dev/ic/vgareg.h>
#include <dev/ic/vgavar.h>

#include <sys/task.h>
#include <dev/pci/vga_pcivar.h>
#include <dev/wscons/wsconsio.h>
#include <dev/wscons/wsdisplayvar.h>
#include <dev/rasops/rasops.h>

extern int intel_enable_gtt(void);
extern void intel_gtt_chipset_flush(void);
extern int intel_gmch_probe(struct pci_dev *, struct pci_dev *, void *);
extern void intel_gtt_get(u64 *, size_t *, phys_addr_t *, u64 *);
extern void intel_gtt_insert_sg_entries(struct sg_table *, unsigned int,
					unsigned int);
extern void intel_gtt_clear_range(unsigned int, unsigned int);
extern void intel_gmch_remove(void);

#ifdef __i386__

static inline u_int64_t
bus_space_read_8(bus_space_tag_t t, bus_space_handle_t h, bus_size_t o)
{
	u_int64_t lo, hi;

	lo = bus_space_read_4(t, h, o);
	hi = bus_space_read_4(t, h, o + 4);
	return (lo | (hi << 32));
}

static inline void
bus_space_write_8(bus_space_tag_t t, bus_space_handle_t h, bus_size_t o,
    u_int64_t v)
{
	bus_space_write_4(t, h, o, v);
	bus_space_write_4(t, h, o + 4, v >> 32);
}

#endif

/*
 * The Bridge device's PCI config space has information about the
 * fb aperture size and the amount of pre-reserved memory.
 * This is all handled in the intel-gtt.ko module. i915.ko only
 * cares about the vga bit for the vga rbiter.
 */
#define INTEL_GMCH_CTRL		0x52
#define INTEL_GMCH_VGA_DISABLE  (1 << 1)
#define SNB_GMCH_CTRL		0x50
#define    SNB_GMCH_GGMS_SHIFT	8 /* GTT Graphics Memory Size */
#define    SNB_GMCH_GGMS_MASK	0x3
#define    SNB_GMCH_GMS_SHIFT   3 /* Graphics Mode Select */
#define    SNB_GMCH_GMS_MASK    0x1f
#define    BDW_GMCH_GGMS_SHIFT	6
#define    BDW_GMCH_GGMS_MASK	0x3
#define    BDW_GMCH_GMS_SHIFT   8
#define    BDW_GMCH_GMS_MASK    0xff

#define I830_GMCH_CTRL			0x52

#define I830_GMCH_GMS_MASK		0x70
#define I830_GMCH_GMS_LOCAL		0x10
#define I830_GMCH_GMS_STOLEN_512	0x20
#define I830_GMCH_GMS_STOLEN_1024	0x30
#define I830_GMCH_GMS_STOLEN_8192	0x40

#define I855_GMCH_GMS_MASK		0xF0
#define I855_GMCH_GMS_STOLEN_0M		0x0
#define I855_GMCH_GMS_STOLEN_1M		(0x1 << 4)
#define I855_GMCH_GMS_STOLEN_4M		(0x2 << 4)
#define I855_GMCH_GMS_STOLEN_8M		(0x3 << 4)
#define I855_GMCH_GMS_STOLEN_16M	(0x4 << 4)
#define I855_GMCH_GMS_STOLEN_32M	(0x5 << 4)
#define I915_GMCH_GMS_STOLEN_48M	(0x6 << 4)
#define I915_GMCH_GMS_STOLEN_64M	(0x7 << 4)
#define G33_GMCH_GMS_STOLEN_128M	(0x8 << 4)
#define G33_GMCH_GMS_STOLEN_256M	(0x9 << 4)
#define INTEL_GMCH_GMS_STOLEN_96M	(0xa << 4)
#define INTEL_GMCH_GMS_STOLEN_160M	(0xb << 4)
#define INTEL_GMCH_GMS_STOLEN_224M	(0xc << 4)
#define INTEL_GMCH_GMS_STOLEN_352M	(0xd << 4)

#define I830_DRB3		0x63
#define I85X_DRB3		0x43
#define I865_TOUD		0xc4

#define I830_ESMRAMC		0x91
#define I845_ESMRAMC		0x9e
#define I85X_ESMRAMC		0x61
#define    TSEG_ENABLE		(1 << 0)
#define    I830_TSEG_SIZE_512K	(0 << 1)
#define    I830_TSEG_SIZE_1M	(1 << 1)
#define    I845_TSEG_SIZE_MASK	(3 << 1)
#define    I845_TSEG_SIZE_512K	(2 << 1)
#define    I845_TSEG_SIZE_1M	(3 << 1)

struct intel_gtt {
	/* Size of memory reserved for graphics by the BIOS */
	unsigned int stolen_size;
	/* Total number of gtt entries. */
	unsigned int gtt_total_entries;
	/* Part of the gtt that is mappable by the cpu, for those chips where
	 * this is not the full gtt. */
	unsigned int gtt_mappable_entries;
	/* Share the scratch page dma with ppgtts. */
	bus_addr_t scratch_page_dma;
	struct drm_dmamem *scratch_page;
	/* for ppgtt PDE access */
	bus_space_handle_t gtt;
	/* needed for ioremap in drm/i915 */
	bus_addr_t gma_bus_addr;
};

/* General customization:
 */

#define DRIVER_NAME		"i915"
#define DRIVER_DESC		"Intel Graphics"
#define DRIVER_DATE		"20151010"

#define MISSING_CASE(x) WARN(1, "Missing switch case (%lu) in %s\n", \
			     (long) (x), __func__);

#define I915_STATE_WARN(condition, format...) WARN(condition, format)
#define I915_STATE_WARN_ON(condition) WARN_ON(condition)

static inline const char *yesno(bool v)
{
	return v ? "yes" : "no";
}

enum pipe {
	INVALID_PIPE = -1,
	PIPE_A = 0,
	PIPE_B,
	PIPE_C,
	_PIPE_EDP,
	I915_MAX_PIPES = _PIPE_EDP
};
#define pipe_name(p) ((p) + 'A')

enum transcoder {
	TRANSCODER_A = 0,
	TRANSCODER_B,
	TRANSCODER_C,
	TRANSCODER_EDP,
	I915_MAX_TRANSCODERS
};
#define transcoder_name(t) ((t) + 'A')

/*
 * I915_MAX_PLANES in the enum below is the maximum (across all platforms)
 * number of planes per CRTC.  Not all platforms really have this many planes,
 * which means some arrays of size I915_MAX_PLANES may have unused entries
 * between the topmost sprite plane and the cursor plane.
 */
enum plane {
	PLANE_A = 0,
	PLANE_B,
	PLANE_C,
	PLANE_CURSOR,
	I915_MAX_PLANES,
};
#define plane_name(p) ((p) + 'A')

#define sprite_name(p, s) ((p) * INTEL_INFO(dev)->num_sprites[(p)] + (s) + 'A')

enum port {
	PORT_A = 0,
	PORT_B,
	PORT_C,
	PORT_D,
	PORT_E,
	I915_MAX_PORTS
};
#define port_name(p) ((p) + 'A')

#define I915_NUM_PHYS_VLV 2

enum dpio_channel {
	DPIO_CH0,
	DPIO_CH1
};

enum dpio_phy {
	DPIO_PHY0,
	DPIO_PHY1
};

enum intel_display_power_domain {
	POWER_DOMAIN_PIPE_A,
	POWER_DOMAIN_PIPE_B,
	POWER_DOMAIN_PIPE_C,
	POWER_DOMAIN_PIPE_A_PANEL_FITTER,
	POWER_DOMAIN_PIPE_B_PANEL_FITTER,
	POWER_DOMAIN_PIPE_C_PANEL_FITTER,
	POWER_DOMAIN_TRANSCODER_A,
	POWER_DOMAIN_TRANSCODER_B,
	POWER_DOMAIN_TRANSCODER_C,
	POWER_DOMAIN_TRANSCODER_EDP,
	POWER_DOMAIN_PORT_DDI_A_2_LANES,
	POWER_DOMAIN_PORT_DDI_A_4_LANES,
	POWER_DOMAIN_PORT_DDI_B_2_LANES,
	POWER_DOMAIN_PORT_DDI_B_4_LANES,
	POWER_DOMAIN_PORT_DDI_C_2_LANES,
	POWER_DOMAIN_PORT_DDI_C_4_LANES,
	POWER_DOMAIN_PORT_DDI_D_2_LANES,
	POWER_DOMAIN_PORT_DDI_D_4_LANES,
	POWER_DOMAIN_PORT_DDI_E_2_LANES,
	POWER_DOMAIN_PORT_DSI,
	POWER_DOMAIN_PORT_CRT,
	POWER_DOMAIN_PORT_OTHER,
	POWER_DOMAIN_VGA,
	POWER_DOMAIN_AUDIO,
	POWER_DOMAIN_PLLS,
	POWER_DOMAIN_AUX_A,
	POWER_DOMAIN_AUX_B,
	POWER_DOMAIN_AUX_C,
	POWER_DOMAIN_AUX_D,
	POWER_DOMAIN_GMBUS,
	POWER_DOMAIN_INIT,

	POWER_DOMAIN_NUM,
};

#define POWER_DOMAIN_PIPE(pipe) ((pipe) + POWER_DOMAIN_PIPE_A)
#define POWER_DOMAIN_PIPE_PANEL_FITTER(pipe) \
		((pipe) + POWER_DOMAIN_PIPE_A_PANEL_FITTER)
#define POWER_DOMAIN_TRANSCODER(tran) \
	((tran) == TRANSCODER_EDP ? POWER_DOMAIN_TRANSCODER_EDP : \
	 (tran) + POWER_DOMAIN_TRANSCODER_A)

enum hpd_pin {
	HPD_NONE = 0,
	HPD_TV = HPD_NONE,     /* TV is known to be unreliable */
	HPD_CRT,
	HPD_SDVO_B,
	HPD_SDVO_C,
	HPD_PORT_A,
	HPD_PORT_B,
	HPD_PORT_C,
	HPD_PORT_D,
	HPD_PORT_E,
	HPD_NUM_PINS
};

#define for_each_hpd_pin(__pin) \
	for ((__pin) = (HPD_NONE + 1); (__pin) < HPD_NUM_PINS; (__pin)++)

struct i915_hotplug {
	struct work_struct hotplug_work;

	struct {
		unsigned long last_jiffies;
		int count;
		enum {
			HPD_ENABLED = 0,
			HPD_DISABLED = 1,
			HPD_MARK_DISABLED = 2
		} state;
	} stats[HPD_NUM_PINS];
	u32 event_bits;
	struct delayed_work reenable_work;

	struct intel_digital_port *irq_port[I915_MAX_PORTS];
	u32 long_port_mask;
	u32 short_port_mask;
	struct work_struct dig_port_work;

	/*
	 * if we get a HPD irq from DP and a HPD irq from non-DP
	 * the non-DP HPD could block the workqueue on a mode config
	 * mutex getting, that userspace may have taken. However
	 * userspace is waiting on the DP workqueue to run which is
	 * blocked behind the non-DP one.
	 */
	struct workqueue_struct *dp_wq;
};

#define I915_GEM_GPU_DOMAINS \
	(I915_GEM_DOMAIN_RENDER | \
	 I915_GEM_DOMAIN_SAMPLER | \
	 I915_GEM_DOMAIN_COMMAND | \
	 I915_GEM_DOMAIN_INSTRUCTION | \
	 I915_GEM_DOMAIN_VERTEX)

#define for_each_pipe(__dev_priv, __p) \
	for ((__p) = 0; (__p) < INTEL_INFO(__dev_priv)->num_pipes; (__p)++)
#define for_each_plane(__dev_priv, __pipe, __p)				\
	for ((__p) = 0;							\
	     (__p) < INTEL_INFO(__dev_priv)->num_sprites[(__pipe)] + 1;	\
	     (__p)++)
#define for_each_sprite(__dev_priv, __p, __s)				\
	for ((__s) = 0;							\
	     (__s) < INTEL_INFO(__dev_priv)->num_sprites[(__p)];	\
	     (__s)++)

#define for_each_crtc(dev, crtc) \
	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head)

#define for_each_intel_plane(dev, intel_plane) \
	list_for_each_entry(intel_plane,			\
			    &dev->mode_config.plane_list,	\
			    base.head)

#define for_each_intel_plane_on_crtc(dev, intel_crtc, intel_plane)	\
	list_for_each_entry(intel_plane,				\
			    &(dev)->mode_config.plane_list,		\
			    base.head)					\
		if ((intel_plane)->pipe == (intel_crtc)->pipe)

#define for_each_intel_crtc(dev, intel_crtc) \
	list_for_each_entry(intel_crtc, &dev->mode_config.crtc_list, base.head)

#define for_each_intel_encoder(dev, intel_encoder)		\
	list_for_each_entry(intel_encoder,			\
			    &(dev)->mode_config.encoder_list,	\
			    base.head)

#define for_each_intel_connector(dev, intel_connector)		\
	list_for_each_entry(intel_connector,			\
			    &dev->mode_config.connector_list,	\
			    base.head)

#define for_each_encoder_on_crtc(dev, __crtc, intel_encoder) \
	list_for_each_entry((intel_encoder), &(dev)->mode_config.encoder_list, base.head) \
		if ((intel_encoder)->base.crtc == (__crtc))

#define for_each_connector_on_encoder(dev, __encoder, intel_connector) \
	list_for_each_entry((intel_connector), &(dev)->mode_config.connector_list, base.head) \
		if ((intel_connector)->base.encoder == (__encoder))

#define for_each_power_domain(domain, mask)				\
	for ((domain) = 0; (domain) < POWER_DOMAIN_NUM; (domain)++)	\
		if ((1 << (domain)) & (mask))

struct inteldrm_softc;
#define drm_i915_private inteldrm_softc
struct i915_mm_struct;
struct i915_mmu_object;

struct drm_i915_file_private {
	struct drm_i915_private *dev_priv;
	struct drm_file *file;

	struct {
		spinlock_t lock;
		struct list_head request_list;
/* 20ms is a fairly arbitrary limit (greater than the average frame time)
 * chosen to prevent the CPU getting more than a frame ahead of the GPU
 * (when using lax throttling for the frontbuffer). We also use it to
 * offer free GPU waitboosts for severely congested workloads.
 */
#define DRM_I915_THROTTLE_JIFFIES msecs_to_jiffies(20)
	} mm;
	struct idr context_idr;

	struct intel_rps_client {
		struct list_head link;
		unsigned boosts;
	} rps;

	struct intel_engine_cs *bsd_ring;
};

enum intel_dpll_id {
	DPLL_ID_PRIVATE = -1, /* non-shared dpll in use */
	/* real shared dpll ids must be >= 0 */
	DPLL_ID_PCH_PLL_A = 0,
	DPLL_ID_PCH_PLL_B = 1,
	/* hsw/bdw */
	DPLL_ID_WRPLL1 = 0,
	DPLL_ID_WRPLL2 = 1,
	DPLL_ID_SPLL = 2,

	/* skl */
	DPLL_ID_SKL_DPLL1 = 0,
	DPLL_ID_SKL_DPLL2 = 1,
	DPLL_ID_SKL_DPLL3 = 2,
};
#define I915_NUM_PLLS 3

struct intel_dpll_hw_state {
	/* i9xx, pch plls */
	uint32_t dpll;
	uint32_t dpll_md;
	uint32_t fp0;
	uint32_t fp1;

	/* hsw, bdw */
	uint32_t wrpll;
	uint32_t spll;

	/* skl */
	/*
	 * DPLL_CTRL1 has 6 bits for each each this DPLL. We store those in
	 * lower part of ctrl1 and they get shifted into position when writing
	 * the register.  This allows us to easily compare the state to share
	 * the DPLL.
	 */
	uint32_t ctrl1;
	/* HDMI only, 0 when used for DP */
	uint32_t cfgcr1, cfgcr2;

	/* bxt */
	uint32_t ebb0, ebb4, pll0, pll1, pll2, pll3, pll6, pll8, pll9, pll10,
		 pcsdw12;
};

struct intel_shared_dpll_config {
	unsigned crtc_mask; /* mask of CRTCs sharing this PLL */
	struct intel_dpll_hw_state hw_state;
};

struct intel_shared_dpll {
	struct intel_shared_dpll_config config;

	int active; /* count of number of active CRTCs (i.e. DPMS on) */
	bool on; /* is the PLL actually active? Disabled during modeset */
	const char *name;
	/* should match the index in the dev_priv->shared_dplls array */
	enum intel_dpll_id id;
	/* The mode_set hook is optional and should be used together with the
	 * intel_prepare_shared_dpll function. */
	void (*mode_set)(struct drm_i915_private *dev_priv,
			 struct intel_shared_dpll *pll);
	void (*enable)(struct drm_i915_private *dev_priv,
		       struct intel_shared_dpll *pll);
	void (*disable)(struct drm_i915_private *dev_priv,
			struct intel_shared_dpll *pll);
	bool (*get_hw_state)(struct drm_i915_private *dev_priv,
			     struct intel_shared_dpll *pll,
			     struct intel_dpll_hw_state *hw_state);
};

#define SKL_DPLL0 0
#define SKL_DPLL1 1
#define SKL_DPLL2 2
#define SKL_DPLL3 3

/* Used by dp and fdi links */
struct intel_link_m_n {
	uint32_t	tu;
	uint32_t	gmch_m;
	uint32_t	gmch_n;
	uint32_t	link_m;
	uint32_t	link_n;
};

void intel_link_compute_m_n(int bpp, int nlanes,
			    int pixel_clock, int link_clock,
			    struct intel_link_m_n *m_n);

/* Interface history:
 *
 * 1.1: Original.
 * 1.2: Add Power Management
 * 1.3: Add vblank support
 * 1.4: Fix cmdbuffer path, add heap destroy
 * 1.5: Add vblank pipe configuration
 * 1.6: - New ioctl for scheduling buffer swaps on vertical blank
 *      - Support vertical blank on secondary display pipe
 */
#define DRIVER_MAJOR		1
#define DRIVER_MINOR		6
#define DRIVER_PATCHLEVEL	0

#define WATCH_LISTS	0

struct opregion_header;
struct opregion_acpi;
struct opregion_swsci;
struct opregion_asle;

struct intel_opregion {
	struct opregion_header *header;
	struct opregion_acpi *acpi;
	struct opregion_swsci *swsci;
	u32 swsci_gbda_sub_functions;
	u32 swsci_sbcb_sub_functions;
	struct opregion_asle *asle;
	void *vbt;
	u32 *lid_state;
	struct work_struct asle_work;
};
#define OPREGION_SIZE            (8*1024)

struct intel_overlay;
struct intel_overlay_error_state;

#define I915_FENCE_REG_NONE -1
#define I915_MAX_NUM_FENCES 32
/* 32 fences + sign bit for FENCE_REG_NONE */
#define I915_MAX_NUM_FENCE_BITS 6

struct drm_i915_fence_reg {
	struct list_head lru_list;
	struct drm_i915_gem_object *obj;
	int pin_count;
};

struct sdvo_device_mapping {
	u8 initialized;
	u8 dvo_port;
	u8 slave_addr;
	u8 dvo_wiring;
	u8 i2c_pin;
	u8 ddc_pin;
};

struct intel_display_error_state;

struct drm_i915_error_state {
	struct kref ref;
	struct timeval time;

	char error_msg[128];
	int iommu;
	u32 reset_count;
	u32 suspend_count;

	/* Generic register state */
	u32 eir;
	u32 pgtbl_er;
	u32 ier;
	u32 gtier[4];
	u32 ccid;
	u32 derrmr;
	u32 forcewake;
	u32 error; /* gen6+ */
	u32 err_int; /* gen7 */
	u32 fault_data0; /* gen8, gen9 */
	u32 fault_data1; /* gen8, gen9 */
	u32 done_reg;
	u32 gac_eco;
	u32 gam_ecochk;
	u32 gab_ctl;
	u32 gfx_mode;
	u32 extra_instdone[I915_NUM_INSTDONE_REG];
	u64 fence[I915_MAX_NUM_FENCES];
	struct intel_overlay_error_state *overlay;
	struct intel_display_error_state *display;
	struct drm_i915_error_object *semaphore_obj;

	struct drm_i915_error_ring {
		bool valid;
		/* Software tracked state */
		bool waiting;
		int hangcheck_score;
		enum intel_ring_hangcheck_action hangcheck_action;
		int num_requests;

		/* our own tracking of ring head and tail */
		u32 cpu_ring_head;
		u32 cpu_ring_tail;

		u32 semaphore_seqno[I915_NUM_RINGS - 1];

		/* Register state */
		u32 start;
		u32 tail;
		u32 head;
		u32 ctl;
		u32 hws;
		u32 ipeir;
		u32 ipehr;
		u32 instdone;
		u32 bbstate;
		u32 instpm;
		u32 instps;
		u32 seqno;
		u64 bbaddr;
		u64 acthd;
		u32 fault_reg;
		u64 faddr;
		u32 rc_psmi; /* sleep state */
		u32 semaphore_mboxes[I915_NUM_RINGS - 1];

		struct drm_i915_error_object {
			int page_count;
			u64 gtt_offset;
			u32 *pages[0];
		} *ringbuffer, *batchbuffer, *wa_batchbuffer, *ctx, *hws_page;

		struct drm_i915_error_request {
			long jiffies;
			u32 seqno;
			u32 tail;
		} *requests;

		struct {
			u32 gfx_mode;
			union {
				u64 pdp[4];
				u32 pp_dir_base;
			};
		} vm_info;

		pid_t pid;
		char comm[TASK_COMM_LEN];
	} ring[I915_NUM_RINGS];

	struct drm_i915_error_buffer {
		u32 size;
		u32 name;
		u32 rseqno[I915_NUM_RINGS], wseqno;
		u64 gtt_offset;
		u32 read_domains;
		u32 write_domain;
		s32 fence_reg:I915_MAX_NUM_FENCE_BITS;
		s32 pinned:2;
		u32 tiling:2;
		u32 dirty:1;
		u32 purgeable:1;
		u32 userptr:1;
		s32 ring:4;
		u32 cache_level:3;
	} **active_bo, **pinned_bo;

	u32 *active_bo_count, *pinned_bo_count;
	u32 vm_count;
};

struct intel_connector;
struct intel_encoder;
struct intel_crtc_state;
struct intel_initial_plane_config;
struct intel_crtc;
struct intel_limit;
struct dpll;

struct drm_i915_display_funcs {
	int (*get_display_clock_speed)(struct drm_device *dev);
	int (*get_fifo_size)(struct drm_device *dev, int plane);
	/**
	 * find_dpll() - Find the best values for the PLL
	 * @@limit: limits for the PLL
	 * @@crtc: current CRTC
	 * @@target: target frequency in kHz
	 * @@refclk: reference clock frequency in kHz
	 * @@match_clock: if provided, @@best_clock P divider must
	 *               match the P divider from @@match_clock
	 *               used for LVDS downclocking
	 * @@best_clock: best PLL values found
	 *
	 * Returns true on success, false on failure.
	 */
	bool (*find_dpll)(const struct intel_limit *limit,
			  struct intel_crtc_state *crtc_state,
			  int target, int refclk,
			  struct dpll *match_clock,
			  struct dpll *best_clock);
	void (*update_wm)(struct drm_crtc *crtc);
	void (*update_sprite_wm)(struct drm_plane *plane,
				 struct drm_crtc *crtc,
				 uint32_t sprite_width, uint32_t sprite_height,
				 int pixel_size, bool enable, bool scaled);
	int (*modeset_calc_cdclk)(struct drm_atomic_state *state);
	void (*modeset_commit_cdclk)(struct drm_atomic_state *state);
	/* Returns the active state of the crtc, and if the crtc is active,
	 * fills out the pipe-config with the hw state. */
	bool (*get_pipe_config)(struct intel_crtc *,
				struct intel_crtc_state *);
	void (*get_initial_plane_config)(struct intel_crtc *,
					 struct intel_initial_plane_config *);
	int (*crtc_compute_clock)(struct intel_crtc *crtc,
				  struct intel_crtc_state *crtc_state);
	void (*crtc_enable)(struct drm_crtc *crtc);
	void (*crtc_disable)(struct drm_crtc *crtc);
	void (*audio_codec_enable)(struct drm_connector *connector,
				   struct intel_encoder *encoder,
				   const struct drm_display_mode *adjusted_mode);
	void (*audio_codec_disable)(struct intel_encoder *encoder);
	void (*fdi_link_train)(struct drm_crtc *crtc);
	void (*init_clock_gating)(struct drm_device *dev);
	int (*queue_flip)(struct drm_device *dev, struct drm_crtc *crtc,
			  struct drm_framebuffer *fb,
			  struct drm_i915_gem_object *obj,
			  struct drm_i915_gem_request *req,
			  uint32_t flags);
	void (*update_primary_plane)(struct drm_crtc *crtc,
				     struct drm_framebuffer *fb,
				     int x, int y);
	void (*hpd_irq_setup)(struct drm_device *dev);
	/* clock updates for mode set */
	/* cursor updates */
	/* render clock increase/decrease */
	/* display clock increase/decrease */
	/* pll clock increase/decrease */
};

enum forcewake_domain_id {
	FW_DOMAIN_ID_RENDER = 0,
	FW_DOMAIN_ID_BLITTER,
	FW_DOMAIN_ID_MEDIA,

	FW_DOMAIN_ID_COUNT
};

enum forcewake_domains {
	FORCEWAKE_RENDER = (1 << FW_DOMAIN_ID_RENDER),
	FORCEWAKE_BLITTER = (1 << FW_DOMAIN_ID_BLITTER),
	FORCEWAKE_MEDIA	= (1 << FW_DOMAIN_ID_MEDIA),
	FORCEWAKE_ALL = (FORCEWAKE_RENDER |
			 FORCEWAKE_BLITTER |
			 FORCEWAKE_MEDIA)
};

struct intel_uncore_funcs {
	void (*force_wake_get)(struct drm_i915_private *dev_priv,
							enum forcewake_domains domains);
	void (*force_wake_put)(struct drm_i915_private *dev_priv,
							enum forcewake_domains domains);

	uint8_t  (*mmio_readb)(struct drm_i915_private *dev_priv, off_t offset, bool trace);
	uint16_t (*mmio_readw)(struct drm_i915_private *dev_priv, off_t offset, bool trace);
	uint32_t (*mmio_readl)(struct drm_i915_private *dev_priv, off_t offset, bool trace);
	uint64_t (*mmio_readq)(struct drm_i915_private *dev_priv, off_t offset, bool trace);

	void (*mmio_writeb)(struct drm_i915_private *dev_priv, off_t offset,
				uint8_t val, bool trace);
	void (*mmio_writew)(struct drm_i915_private *dev_priv, off_t offset,
				uint16_t val, bool trace);
	void (*mmio_writel)(struct drm_i915_private *dev_priv, off_t offset,
				uint32_t val, bool trace);
	void (*mmio_writeq)(struct drm_i915_private *dev_priv, off_t offset,
				uint64_t val, bool trace);
};

struct intel_uncore {
	spinlock_t lock; /** lock is also taken in irq contexts. */

	struct intel_uncore_funcs funcs;

	unsigned fifo_count;
	enum forcewake_domains fw_domains;

	struct intel_uncore_forcewake_domain {
		struct drm_i915_private *i915;
		enum forcewake_domain_id id;
		unsigned wake_count;
		struct timeout timer;
		u32 reg_set;
		u32 val_set;
		u32 val_clear;
		u32 reg_ack;
		u32 reg_post;
		u32 val_reset;
	} fw_domain[FW_DOMAIN_ID_COUNT];
};

/* Iterate over initialised fw domains */
#define for_each_fw_domain_mask(domain__, mask__, dev_priv__, i__) \
	for ((i__) = 0, (domain__) = &(dev_priv__)->uncore.fw_domain[0]; \
	     (i__) < FW_DOMAIN_ID_COUNT; \
	     (i__)++, (domain__) = &(dev_priv__)->uncore.fw_domain[i__]) \
		if (((mask__) & (dev_priv__)->uncore.fw_domains) & (1 << (i__)))

#define for_each_fw_domain(domain__, dev_priv__, i__) \
	for_each_fw_domain_mask(domain__, FORCEWAKE_ALL, dev_priv__, i__)

enum csr_state {
	FW_UNINITIALIZED = 0,
	FW_LOADED,
	FW_FAILED
};

struct intel_csr {
	const char *fw_path;
	uint32_t *dmc_payload;
	uint32_t dmc_fw_size;
	uint32_t mmio_count;
	uint32_t mmioaddr[8];
	uint32_t mmiodata[8];
	enum csr_state state;
};

#define DEV_INFO_FOR_EACH_FLAG(func, sep) \
	func(is_mobile) sep \
	func(is_i85x) sep \
	func(is_i915g) sep \
	func(is_i945gm) sep \
	func(is_g33) sep \
	func(need_gfx_hws) sep \
	func(is_g4x) sep \
	func(is_pineview) sep \
	func(is_broadwater) sep \
	func(is_crestline) sep \
	func(is_ivybridge) sep \
	func(is_valleyview) sep \
	func(is_haswell) sep \
	func(is_skylake) sep \
	func(is_broxton) sep \
	func(is_kabylake) sep \
	func(is_preliminary) sep \
	func(has_fbc) sep \
	func(has_pipe_cxsr) sep \
	func(has_hotplug) sep \
	func(cursor_needs_physical) sep \
	func(has_overlay) sep \
	func(overlay_needs_physical) sep \
	func(supports_tv) sep \
	func(has_llc) sep \
	func(has_ddi) sep \
	func(has_fpga_dbg)

#define DEFINE_FLAG(name) u8 name:1
#define SEP_SEMICOLON ;

struct intel_device_info {
	u32 display_mmio_offset;
	u16 device_id;
	u8 num_pipes:3;
	u8 num_sprites[I915_MAX_PIPES];
	u8 gen;
	u8 ring_mask; /* Rings supported by the HW */
	DEV_INFO_FOR_EACH_FLAG(DEFINE_FLAG, SEP_SEMICOLON);
	/* Register offsets for the various display pipes and transcoders */
	int pipe_offsets[I915_MAX_TRANSCODERS];
	int trans_offsets[I915_MAX_TRANSCODERS];
	int palette_offsets[I915_MAX_PIPES];
	int cursor_offsets[I915_MAX_PIPES];

	/* Slice/subslice/EU info */
	u8 slice_total;
	u8 subslice_total;
	u8 subslice_per_slice;
	u8 eu_total;
	u8 eu_per_subslice;
	/* For each slice, which subslice(s) has(have) 7 EUs (bitfield)? */
	u8 subslice_7eu[3];
	u8 has_slice_pg:1;
	u8 has_subslice_pg:1;
	u8 has_eu_pg:1;
};

#undef DEFINE_FLAG
#undef SEP_SEMICOLON

enum i915_cache_level {
	I915_CACHE_NONE = 0,
	I915_CACHE_LLC, /* also used for snoopable memory on non-LLC */
	I915_CACHE_L3_LLC, /* gen7+, L3 sits between the domain specifc
			      caches, eg sampler/render caches, and the
			      large Last-Level-Cache. LLC is coherent with
			      the CPU, but L3 is only visible to the GPU. */
	I915_CACHE_WT, /* hsw:gt3e WriteThrough for scanouts */
};

struct i915_ctx_hang_stats {
	/* This context had batch pending when hang was declared */
	unsigned batch_pending;

	/* This context had batch active when hang was declared */
	unsigned batch_active;

	/* Time when this context was last blamed for a GPU reset */
	unsigned long guilty_ts;

	/* If the contexts causes a second GPU hang within this time,
	 * it is permanently banned from submitting any more work.
	 */
	unsigned long ban_period_seconds;

	/* This context is banned to submit more work */
	bool banned;
};

/* This must match up with the value previously used for execbuf2.rsvd1. */
#define DEFAULT_CONTEXT_HANDLE 0

#define CONTEXT_NO_ZEROMAP (1<<0)
/**
 * struct intel_context - as the name implies, represents a context.
 * @@ref: reference count.
 * @@user_handle: userspace tracking identity for this context.
 * @@remap_slice: l3 row remapping information.
 * @@flags: context specific flags:
 *         CONTEXT_NO_ZEROMAP: do not allow mapping things to page 0.
 * @@file_priv: filp associated with this context (NULL for global default
 *	       context).
 * @@hang_stats: information about the role of this context in possible GPU
 *		hangs.
 * @@ppgtt: virtual memory space used by this context.
 * @@legacy_hw_ctx: render context backing object and whether it is correctly
 *                initialized (legacy ring submission mechanism only).
 * @@link: link in the global list of contexts.
 *
 * Contexts are memory images used by the hardware to store copies of their
 * internal state.
 */
struct intel_context {
	struct kref ref;
	int user_handle;
	uint8_t remap_slice;
	struct drm_i915_private *i915;
	int flags;
	struct drm_i915_file_private *file_priv;
	struct i915_ctx_hang_stats hang_stats;
	struct i915_hw_ppgtt *ppgtt;

	/* Legacy ring buffer submission */
	struct {
		struct drm_i915_gem_object *rcs_state;
		bool initialized;
	} legacy_hw_ctx;

	/* Execlists */
	struct {
		struct drm_i915_gem_object *state;
		struct intel_ringbuffer *ringbuf;
		int pin_count;
	} engine[I915_NUM_RINGS];

	struct list_head link;
};

enum fb_op_origin {
	ORIGIN_GTT,
	ORIGIN_CPU,
	ORIGIN_CS,
	ORIGIN_FLIP,
	ORIGIN_DIRTYFB,
};

struct i915_fbc {
	/* This is always the inner lock when overlapping with struct_mutex and
	 * it's the outer lock when overlapping with stolen_lock. */
	struct rwlock lock;
	unsigned long uncompressed_size;
	unsigned threshold;
	unsigned int fb_id;
	unsigned int possible_framebuffer_bits;
	unsigned int busy_bits;
	struct intel_crtc *crtc;
	int y;

	struct drm_mm_node compressed_fb;
	struct drm_mm_node *compressed_llb;

	bool false_color;

	/* Tracks whether the HW is actually enabled, not whether the feature is
	 * possible. */
	bool enabled;

	struct intel_fbc_work {
		struct delayed_work work;
		struct intel_crtc *crtc;
		struct drm_framebuffer *fb;
	} *fbc_work;

	enum no_fbc_reason {
		FBC_OK, /* FBC is enabled */
		FBC_UNSUPPORTED, /* FBC is not supported by this chipset */
		FBC_NO_OUTPUT, /* no outputs enabled to compress */
		FBC_STOLEN_TOO_SMALL, /* not enough space for buffers */
		FBC_UNSUPPORTED_MODE, /* interlace or doublescanned mode */
		FBC_MODE_TOO_LARGE, /* mode too large for compression */
		FBC_BAD_PLANE, /* fbc not supported on plane */
		FBC_NOT_TILED, /* buffer not tiled */
		FBC_MULTIPLE_PIPES, /* more than one pipe active */
		FBC_MODULE_PARAM,
		FBC_CHIP_DEFAULT, /* disabled by default on this chip */
		FBC_ROTATION, /* rotation is not supported */
		FBC_IN_DBG_MASTER, /* kernel debugger is active */
		FBC_BAD_STRIDE, /* stride is not supported */
		FBC_PIXEL_RATE, /* pixel rate is too big */
		FBC_PIXEL_FORMAT /* pixel format is invalid */
	} no_fbc_reason;

	bool (*fbc_enabled)(struct drm_i915_private *dev_priv);
	void (*enable_fbc)(struct intel_crtc *crtc);
	void (*disable_fbc)(struct drm_i915_private *dev_priv);
};

/**
 * HIGH_RR is the highest eDP panel refresh rate read from EDID
 * LOW_RR is the lowest eDP panel refresh rate found from EDID
 * parsing for same resolution.
 */
enum drrs_refresh_rate_type {
	DRRS_HIGH_RR,
	DRRS_LOW_RR,
	DRRS_MAX_RR, /* RR count */
};

enum drrs_support_type {
	DRRS_NOT_SUPPORTED = 0,
	STATIC_DRRS_SUPPORT = 1,
	SEAMLESS_DRRS_SUPPORT = 2
};

struct intel_dp;
struct i915_drrs {
	struct rwlock mutex;
	struct delayed_work work;
	struct intel_dp *dp;
	unsigned busy_frontbuffer_bits;
	enum drrs_refresh_rate_type refresh_rate_type;
	enum drrs_support_type type;
};

struct i915_psr {
	struct rwlock lock;
	bool sink_support;
	bool source_ok;
	struct intel_dp *enabled;
	bool active;
	struct delayed_work work;
	unsigned busy_frontbuffer_bits;
	bool psr2_support;
	bool aux_frame_sync;
};

enum intel_pch {
	PCH_NONE = 0,	/* No PCH present */
	PCH_IBX,	/* Ibexpeak PCH */
	PCH_CPT,	/* Cougarpoint PCH */
	PCH_LPT,	/* Lynxpoint PCH */
	PCH_SPT,        /* Sunrisepoint PCH */
	PCH_KBP,	/* Kabypoint PCH */
	PCH_NOP,
};

enum intel_sbi_destination {
	SBI_ICLK,
	SBI_MPHY,
};

#define QUIRK_PIPEA_FORCE (1<<0)
#define QUIRK_LVDS_SSC_DISABLE (1<<1)
#define QUIRK_INVERT_BRIGHTNESS (1<<2)
#define QUIRK_BACKLIGHT_PRESENT (1<<3)
#define QUIRK_PIPEB_FORCE (1<<4)
#define QUIRK_PIN_SWIZZLED_PAGES (1<<5)

struct intel_fbdev;
struct intel_fbc_work;

struct intel_gmbus {
	struct i2c_adapter adapter;
	u32 force_bit;
	u32 reg0;
	u32 gpio_reg;
#ifdef __linux__
	struct i2c_algo_bit_data bit_algo;
#endif
	struct drm_i915_private *dev_priv;
};

struct i915_suspend_saved_registers {
	u32 saveDSPARB;
	u32 saveLVDS;
	u32 savePP_ON_DELAYS;
	u32 savePP_OFF_DELAYS;
	u32 savePP_ON;
	u32 savePP_OFF;
	u32 savePP_CONTROL;
	u32 savePP_DIVISOR;
	u32 saveFBC_CONTROL;
	u32 saveCACHE_MODE_0;
	u32 saveMI_ARB_STATE;
	u32 saveSWF0[16];
	u32 saveSWF1[16];
	u32 saveSWF3[3];
	uint64_t saveFENCE[I915_MAX_NUM_FENCES];
	u32 savePCH_PORT_HOTPLUG;
	u16 saveGCDGMBUS;
};

struct vlv_s0ix_state {
	/* GAM */
	u32 wr_watermark;
	u32 gfx_prio_ctrl;
	u32 arb_mode;
	u32 gfx_pend_tlb0;
	u32 gfx_pend_tlb1;
	u32 lra_limits[GEN7_LRA_LIMITS_REG_NUM];
	u32 media_max_req_count;
	u32 gfx_max_req_count;
	u32 render_hwsp;
	u32 ecochk;
	u32 bsd_hwsp;
	u32 blt_hwsp;
	u32 tlb_rd_addr;

	/* MBC */
	u32 g3dctl;
	u32 gsckgctl;
	u32 mbctl;

	/* GCP */
	u32 ucgctl1;
	u32 ucgctl3;
	u32 rcgctl1;
	u32 rcgctl2;
	u32 rstctl;
	u32 misccpctl;

	/* GPM */
	u32 gfxpause;
	u32 rpdeuhwtc;
	u32 rpdeuc;
	u32 ecobus;
	u32 pwrdwnupctl;
	u32 rp_down_timeout;
	u32 rp_deucsw;
	u32 rcubmabdtmr;
	u32 rcedata;
	u32 spare2gh;

	/* Display 1 CZ domain */
	u32 gt_imr;
	u32 gt_ier;
	u32 pm_imr;
	u32 pm_ier;
	u32 gt_scratch[GEN7_GT_SCRATCH_REG_NUM];

	/* GT SA CZ domain */
	u32 tilectl;
	u32 gt_fifoctl;
	u32 gtlc_wake_ctrl;
	u32 gtlc_survive;
	u32 pmwgicz;

	/* Display 2 CZ domain */
	u32 gu_ctl0;
	u32 gu_ctl1;
	u32 pcbr;
	u32 clock_gate_dis2;
};

struct intel_rps_ei {
	u32 cz_clock;
	u32 render_c0;
	u32 media_c0;
};

struct intel_gen6_power_mgmt {
	/*
	 * work, interrupts_enabled and pm_iir are protected by
	 * dev_priv->irq_lock
	 */
	struct work_struct work;
	bool interrupts_enabled;
	u32 pm_iir;

	/* Frequencies are stored in potentially platform dependent multiples.
	 * In other words, *_freq needs to be multiplied by X to be interesting.
	 * Soft limits are those which are used for the dynamic reclocking done
	 * by the driver (raise frequencies under heavy loads, and lower for
	 * lighter loads). Hard limits are those imposed by the hardware.
	 *
	 * A distinction is made for overclocking, which is never enabled by
	 * default, and is considered to be above the hard limit if it's
	 * possible at all.
	 */
	u8 cur_freq;		/* Current frequency (cached, may not == HW) */
	u8 min_freq_softlimit;	/* Minimum frequency permitted by the driver */
	u8 max_freq_softlimit;	/* Max frequency permitted by the driver */
	u8 max_freq;		/* Maximum frequency, RP0 if not overclocking */
	u8 min_freq;		/* AKA RPn. Minimum frequency */
	u8 idle_freq;		/* Frequency to request when we are idle */
	u8 efficient_freq;	/* AKA RPe. Pre-determined balanced frequency */
	u8 rp1_freq;		/* "less than" RP0 power/freqency */
	u8 rp0_freq;		/* Non-overclocked max frequency. */

	u8 up_threshold; /* Current %busy required to uplock */
	u8 down_threshold; /* Current %busy required to downclock */

	int last_adj;
	enum { LOW_POWER, BETWEEN, HIGH_POWER } power;

	spinlock_t client_lock;
	struct list_head clients;
	bool client_boost;

	bool enabled;
	struct delayed_work delayed_resume_work;
	unsigned boosts;

	struct intel_rps_client semaphores, mmioflips;

	/* manual wa residency calculations */
	struct intel_rps_ei ei;

	/*
	 * Protects RPS/RC6 register access and PCU communication.
	 * Must be taken after struct_mutex if nested. Note that
	 * this lock may be held for long periods of time when
	 * talking to hw - so only take it when talking to hw!
	 */
	struct rwlock hw_lock;
};

/* defined intel_pm.c */
extern spinlock_t mchdev_lock;

struct intel_ilk_power_mgmt {
	u8 cur_delay;
	u8 min_delay;
	u8 max_delay;
	u8 fmax;
	u8 fstart;

	u64 last_count1;
	unsigned long last_time1;
	unsigned long chipset_power;
	u64 last_count2;
	u64 last_time2;
	unsigned long gfx_power;
	u8 corr;

	int c_m;
	int r_t;
};

struct drm_i915_private;
struct i915_power_well;

struct i915_power_well_ops {
	/*
	 * Synchronize the well's hw state to match the current sw state, for
	 * example enable/disable it based on the current refcount. Called
	 * during driver init and resume time, possibly after first calling
	 * the enable/disable handlers.
	 */
	void (*sync_hw)(struct drm_i915_private *dev_priv,
			struct i915_power_well *power_well);
	/*
	 * Enable the well and resources that depend on it (for example
	 * interrupts located on the well). Called after the 0->1 refcount
	 * transition.
	 */
	void (*enable)(struct drm_i915_private *dev_priv,
		       struct i915_power_well *power_well);
	/*
	 * Disable the well and resources that depend on it. Called after
	 * the 1->0 refcount transition.
	 */
	void (*disable)(struct drm_i915_private *dev_priv,
			struct i915_power_well *power_well);
	/* Returns the hw enabled state. */
	bool (*is_enabled)(struct drm_i915_private *dev_priv,
			   struct i915_power_well *power_well);
};

/* Power well structure for haswell */
struct i915_power_well {
	const char *name;
	bool always_on;
	/* power well enable/disable usage count */
	int count;
	/* cached hw enabled state */
	bool hw_enabled;
	unsigned long domains;
	unsigned long data;
	const struct i915_power_well_ops *ops;
};

struct i915_power_domains {
	/*
	 * Power wells needed for initialization at driver init and suspend
	 * time are on. They are kept on until after the first modeset.
	 */
	bool init_power_on;
	bool initializing;
	int power_well_count;

	struct rwlock lock;
	int domain_use_count[POWER_DOMAIN_NUM];
	struct i915_power_well *power_wells;
};

#define MAX_L3_SLICES 2
struct intel_l3_parity {
	u32 *remap_info[MAX_L3_SLICES];
	struct work_struct error_work;
	int which_slice;
};

struct i915_gem_mm {
	/** Memory allocator for GTT stolen memory */
	struct drm_mm stolen;
	/** Protects the usage of the GTT stolen memory allocator. This is
	 * always the inner lock when overlapping with struct_mutex. */
	struct rwlock stolen_lock;

	/** List of all objects in gtt_space. Used to restore gtt
	 * mappings on resume */
	struct list_head bound_list;
	/**
	 * List of objects which are not bound to the GTT (thus
	 * are idle and not used by the GPU) but still have
	 * (presumably uncached) pages still attached.
	 */
	struct list_head unbound_list;

	/** Usable portion of the GTT for GEM */
	unsigned long stolen_base; /* limited to low memory (32-bit) */

	/** PPGTT used for aliasing the PPGTT with the GTT */
	struct i915_hw_ppgtt *aliasing_ppgtt;

#ifdef __linux__
	struct notifier_block oom_notifier;
	struct shrinker shrinker;
	bool shrinker_no_lock_stealing;
#endif

	/** LRU list of objects with fence regs on them. */
	struct list_head fence_list;

	/**
	 * We leave the user IRQ off as much as possible,
	 * but this means that requests will finish and never
	 * be retired once the system goes idle. Set a timer to
	 * fire periodically while the ring is running. When it
	 * fires, go retire requests.
	 */
	struct delayed_work retire_work;

	/**
	 * When we detect an idle GPU, we want to turn on
	 * powersaving features. So once we see that there
	 * are no more requests outstanding and no more
	 * arrive within a small period of time, we fire
	 * off the idle_work.
	 */
	struct delayed_work idle_work;

	/**
	 * Are we in a non-interruptible section of code like
	 * modesetting?
	 */
	bool interruptible;

	/**
	 * Is the GPU currently considered idle, or busy executing userspace
	 * requests?  Whilst idle, we attempt to power down the hardware and
	 * display clocks. In order to reduce the effect on performance, there
	 * is a slight delay before we do so.
	 */
	bool busy;

	/* the indicator for dispatch video commands on two BSD rings */
	int bsd_ring_dispatch_index;

	/** Bit 6 swizzling required for X tiling */
	uint32_t bit_6_swizzle_x;
	/** Bit 6 swizzling required for Y tiling */
	uint32_t bit_6_swizzle_y;

	/* accounting, useful for userland debugging */
	spinlock_t object_stat_lock;
	size_t object_memory;
	u32 object_count;
};

struct drm_i915_error_state_buf {
	struct drm_i915_private *i915;
	unsigned bytes;
	unsigned size;
	int err;
	u8 *buf;
	loff_t start;
	loff_t pos;
};

struct i915_error_state_file_priv {
	struct drm_device *dev;
	struct drm_i915_error_state *error;
};

struct i915_gpu_error {
	/* For hangcheck timer */
#define DRM_I915_HANGCHECK_PERIOD 1500 /* in ms */
#define DRM_I915_HANGCHECK_JIFFIES msecs_to_jiffies(DRM_I915_HANGCHECK_PERIOD)
	/* Hang gpu twice in this window and your context gets banned */
#define DRM_I915_CTX_BAN_PERIOD DIV_ROUND_UP(8*DRM_I915_HANGCHECK_PERIOD, 1000)

	struct workqueue_struct *hangcheck_wq;
	struct delayed_work hangcheck_work;

	/* For reset and error_state handling. */
	spinlock_t lock;
	/* Protected by the above dev->gpu_error.lock. */
	struct drm_i915_error_state *first_error;

	unsigned long missed_irq_rings;

	/**
	 * State variable controlling the reset flow and count
	 *
	 * This is a counter which gets incremented when reset is triggered,
	 * and again when reset has been handled. So odd values (lowest bit set)
	 * means that reset is in progress and even values that
	 * (reset_counter >> 1):th reset was successfully completed.
	 *
	 * If reset is not completed succesfully, the I915_WEDGE bit is
	 * set meaning that hardware is terminally sour and there is no
	 * recovery. All waiters on the reset_queue will be woken when
	 * that happens.
	 *
	 * This counter is used by the wait_seqno code to notice that reset
	 * event happened and it needs to restart the entire ioctl (since most
	 * likely the seqno it waited for won't ever signal anytime soon).
	 *
	 * This is important for lock-free wait paths, where no contended lock
	 * naturally enforces the correct ordering between the bail-out of the
	 * waiter and the gpu reset work code.
	 */
	atomic_t reset_counter;

#define I915_RESET_IN_PROGRESS_FLAG	1
#define I915_WEDGED			(1 << 31)

	/**
	 * Waitqueue to signal when the reset has completed. Used by clients
	 * that wait for dev_priv->mm.wedged to settle.
	 */
	wait_queue_head_t reset_queue;

	/* Userspace knobs for gpu hang simulation;
	 * combines both a ring mask, and extra flags
	 */
	u32 stop_rings;
#define I915_STOP_RING_ALLOW_BAN       (1 << 31)
#define I915_STOP_RING_ALLOW_WARN      (1 << 30)

	/* For missed irq/seqno simulation. */
	unsigned int test_irq_rings;

	/* Used to prevent gem_check_wedged returning -EAGAIN during gpu reset   */
	bool reload_in_reset;
};

enum modeset_restore {
	MODESET_ON_LID_OPEN,
	MODESET_DONE,
	MODESET_SUSPENDED,
};

#define DP_AUX_A 0x40
#define DP_AUX_B 0x10
#define DP_AUX_C 0x20
#define DP_AUX_D 0x30

#define DDC_PIN_B  0x05
#define DDC_PIN_C  0x04
#define DDC_PIN_D  0x06

struct ddi_vbt_port_info {
	/*
	 * This is an index in the HDMI/DVI DDI buffer translation table.
	 * The special value HDMI_LEVEL_SHIFT_UNKNOWN means the VBT didn't
	 * populate this field.
	 */
#define HDMI_LEVEL_SHIFT_UNKNOWN	0xff
	uint8_t hdmi_level_shift;

	uint8_t supports_dvi:1;
	uint8_t supports_hdmi:1;
	uint8_t supports_dp:1;

	uint8_t alternate_aux_channel;
	uint8_t alternate_ddc_pin;

	uint8_t dp_boost_level;
	uint8_t hdmi_boost_level;
};

enum psr_lines_to_wait {
	PSR_0_LINES_TO_WAIT = 0,
	PSR_1_LINE_TO_WAIT,
	PSR_4_LINES_TO_WAIT,
	PSR_8_LINES_TO_WAIT
};

struct intel_vbt_data {
	struct drm_display_mode *lfp_lvds_vbt_mode; /* if any */
	struct drm_display_mode *sdvo_lvds_vbt_mode; /* if any */

	/* Feature bits */
	unsigned int int_tv_support:1;
	unsigned int lvds_dither:1;
	unsigned int lvds_vbt:1;
	unsigned int int_crt_support:1;
	unsigned int lvds_use_ssc:1;
	unsigned int display_clock_mode:1;
	unsigned int fdi_rx_polarity_inverted:1;
	unsigned int has_mipi:1;
	int lvds_ssc_freq;
	unsigned int bios_lvds_val; /* initial [PCH_]LVDS reg val in VBIOS */

	enum drrs_support_type drrs_type;

	/* eDP */
	int edp_rate;
	int edp_lanes;
	int edp_preemphasis;
	int edp_vswing;
	bool edp_initialized;
	bool edp_support;
	int edp_bpp;
	struct edp_power_seq edp_pps;

	struct {
		bool full_link;
		bool require_aux_wakeup;
		int idle_frames;
		enum psr_lines_to_wait lines_to_wait;
		int tp1_wakeup_time;
		int tp2_tp3_wakeup_time;
	} psr;

	struct {
		u16 pwm_freq_hz;
		bool present;
		bool active_low_pwm;
		u8 min_brightness;	/* min_brightness/255 of max */
	} backlight;

#ifndef __linux__
	/* MIPI DSI */
	struct {
		u16 port;
		u16 panel_id;
		struct mipi_config *config;
		struct mipi_pps_data *pps;
		u8 seq_version;
		u32 size;
		u8 *data;
		u8 *sequence[MIPI_SEQ_MAX];
	} dsi;
#endif

	int crt_ddc_pin;

	int child_dev_num;
	union child_device_config *child_dev;

	struct ddi_vbt_port_info ddi_port_info[I915_MAX_PORTS];
};

enum intel_ddb_partitioning {
	INTEL_DDB_PART_1_2,
	INTEL_DDB_PART_5_6, /* IVB+ */
};

struct intel_wm_level {
	bool enable;
	uint32_t pri_val;
	uint32_t spr_val;
	uint32_t cur_val;
	uint32_t fbc_val;
};

struct ilk_wm_values {
	uint32_t wm_pipe[3];
	uint32_t wm_lp[3];
	uint32_t wm_lp_spr[3];
	uint32_t wm_linetime[3];
	bool enable_fbc_wm;
	enum intel_ddb_partitioning partitioning;
};

struct vlv_pipe_wm {
	uint16_t primary;
	uint16_t sprite[2];
	uint8_t cursor;
};

struct vlv_sr_wm {
	uint16_t plane;
	uint8_t cursor;
};

struct vlv_wm_values {
	struct vlv_pipe_wm pipe[3];
	struct vlv_sr_wm sr;
	struct {
		uint8_t cursor;
		uint8_t sprite[2];
		uint8_t primary;
	} ddl[3];
	uint8_t level;
	bool cxsr;
};

struct skl_ddb_entry {
	uint16_t start, end;	/* in number of blocks, 'end' is exclusive */
};

static inline uint16_t skl_ddb_entry_size(const struct skl_ddb_entry *entry)
{
	return entry->end - entry->start;
}

static inline bool skl_ddb_entry_equal(const struct skl_ddb_entry *e1,
				       const struct skl_ddb_entry *e2)
{
	if (e1->start == e2->start && e1->end == e2->end)
		return true;

	return false;
}

struct skl_ddb_allocation {
	struct skl_ddb_entry pipe[I915_MAX_PIPES];
	struct skl_ddb_entry plane[I915_MAX_PIPES][I915_MAX_PLANES]; /* packed/uv */
	struct skl_ddb_entry y_plane[I915_MAX_PIPES][I915_MAX_PLANES];
};

struct skl_wm_values {
	bool dirty[I915_MAX_PIPES];
	struct skl_ddb_allocation ddb;
	uint32_t wm_linetime[I915_MAX_PIPES];
	uint32_t plane[I915_MAX_PIPES][I915_MAX_PLANES][8];
	uint32_t plane_trans[I915_MAX_PIPES][I915_MAX_PLANES];
};

struct skl_wm_level {
	bool plane_en[I915_MAX_PLANES];
	uint16_t plane_res_b[I915_MAX_PLANES];
	uint8_t plane_res_l[I915_MAX_PLANES];
};

/*
 * This struct helps tracking the state needed for runtime PM, which puts the
 * device in PCI D3 state. Notice that when this happens, nothing on the
 * graphics device works, even register access, so we don't get interrupts nor
 * anything else.
 *
 * Every piece of our code that needs to actually touch the hardware needs to
 * either call intel_runtime_pm_get or call intel_display_power_get with the
 * appropriate power domain.
 *
 * Our driver uses the autosuspend delay feature, which means we'll only really
 * suspend if we stay with zero refcount for a certain amount of time. The
 * default value is currently very conservative (see intel_runtime_pm_enable), but
 * it can be changed with the standard runtime PM files from sysfs.
 *
 * The irqs_disabled variable becomes true exactly after we disable the IRQs and
 * goes back to false exactly before we reenable the IRQs. We use this variable
 * to check if someone is trying to enable/disable IRQs while they're supposed
 * to be disabled. This shouldn't happen and we'll print some error messages in
 * case it happens.
 *
 * For more, read the Documentation/power/runtime_pm.txt.
 */
struct i915_runtime_pm {
	bool suspended;
	bool irqs_enabled;
};

enum intel_pipe_crc_source {
	INTEL_PIPE_CRC_SOURCE_NONE,
	INTEL_PIPE_CRC_SOURCE_PLANE1,
	INTEL_PIPE_CRC_SOURCE_PLANE2,
	INTEL_PIPE_CRC_SOURCE_PF,
	INTEL_PIPE_CRC_SOURCE_PIPE,
	/* TV/DP on pre-gen5/vlv can't use the pipe source. */
	INTEL_PIPE_CRC_SOURCE_TV,
	INTEL_PIPE_CRC_SOURCE_DP_B,
	INTEL_PIPE_CRC_SOURCE_DP_C,
	INTEL_PIPE_CRC_SOURCE_DP_D,
	INTEL_PIPE_CRC_SOURCE_AUTO,
	INTEL_PIPE_CRC_SOURCE_MAX,
};

struct intel_pipe_crc_entry {
	uint32_t frame;
	uint32_t crc[5];
};

#define INTEL_PIPE_CRC_ENTRIES_NR	128
struct intel_pipe_crc {
	spinlock_t lock;
	bool opened;		/* exclusive access to the result file */
	struct intel_pipe_crc_entry *entries;
	enum intel_pipe_crc_source source;
	int head, tail;
	wait_queue_head_t wq;
};

struct i915_frontbuffer_tracking {
	struct rwlock lock;

	/*
	 * Tracking bits for delayed frontbuffer flushing du to gpu activity or
	 * scheduled flips.
	 */
	unsigned busy_bits;
	unsigned flip_bits;
};

struct i915_wa_reg {
	u32 addr;
	u32 value;
	/* bitmask representing WA bits */
	u32 mask;
};

#define I915_MAX_WA_REGS 16

struct i915_workarounds {
	struct i915_wa_reg reg[I915_MAX_WA_REGS];
	u32 count;
};

struct i915_virtual_gpu {
	bool active;
};

struct i915_execbuffer_params {
	struct drm_device               *dev;
	struct drm_file                 *file;
	uint32_t                        dispatch_flags;
	uint32_t                        args_batch_start_offset;
	uint64_t                        batch_obj_vm_offset;
	struct intel_engine_cs          *ring;
	struct drm_i915_gem_object      *batch_obj;
	struct intel_context            *ctx;
	struct drm_i915_gem_request     *request;
};

struct inteldrm_softc {
	struct device sc_dev;
	bus_dma_tag_t dmat;
	bus_space_tag_t bst;
	struct agp_map *agph;
	bus_space_handle_t opregion_ioh;

	struct drm_device *dev;
	struct pool objects;
	struct pool vmas;
	struct pool requests;

	const struct intel_device_info info;

	int relative_constants_mode;

	pci_chipset_tag_t pc;
	pcitag_t tag;
	struct extent *memex;
	pci_intr_handle_t ih;
	void *irqh;

	struct vga_pci_bar bar;
	struct vga_pci_bar *regs;

	int nscreens;
	void (*switchcb)(void *, int, int);
	void *switchcbarg;
	void *switchcookie;
	struct task switchtask;
	struct rasops_info ro;

	struct task burner_task;
	int burner_fblank;

	struct backlight_device *backlight;

	struct intel_uncore uncore;

	struct intel_guc guc;

	struct intel_csr csr;

	/* Display CSR-related protection */
	struct rwlock csr_lock;

	struct intel_gmbus gmbus[GMBUS_NUM_PINS];

	/** gmbus_mutex protects against concurrent usage of the single hw gmbus
	 * controller on different i2c buses. */
	struct rwlock gmbus_mutex;

	/**
	 * Base address of the gmbus and gpio block.
	 */
	uint32_t gpio_mmio_base;

	/* MMIO base address for MIPI regs */
	uint32_t mipi_mmio_base;

	wait_queue_head_t gmbus_wait_queue;

	struct pci_dev *bridge_dev;
	struct intel_engine_cs ring[I915_NUM_RINGS];
	struct drm_i915_gem_object *semaphore_obj;
	uint32_t last_seqno, next_seqno;

	struct drm_dma_handle *status_page_dmah;
	struct resource mch_res;
	union flush {
		struct {
			bus_space_tag_t		bst;
			bus_space_handle_t	bsh;
		} i9xx;
		struct {
			bus_dma_segment_t	seg;
			caddr_t			kva;
		} i8xx;
	}			 ifp;
	struct vm_page *pgs;

	/* protects the irq masks */
	spinlock_t irq_lock;

	/* protects the mmio flip data */
	spinlock_t mmio_flip_lock;

	bool display_irqs_enabled;

#ifdef noyet
	/* To control wakeup latency, e.g. for irq-driven dp aux transfers. */
	struct pm_qos_request pm_qos;
#endif

	/* Sideband mailbox protection */
	struct rwlock sb_lock;

	/** Cached value of IMR to avoid reads in updating the bitfield */
	union {
		u32 irq_mask;
		u32 de_irq_mask[I915_MAX_PIPES];
	};
	u32 gt_irq_mask;
	u32 pm_irq_mask;
	u32 pm_rps_events;
	u32 pipestat_irq_mask[I915_MAX_PIPES];

	struct i915_hotplug hotplug;
	struct i915_fbc fbc;
	struct i915_drrs drrs;
	struct intel_opregion opregion;
	struct intel_vbt_data vbt;

	bool preserve_bios_swizzle;

	/* overlay */
	struct intel_overlay *overlay;

	/* backlight registers and fields in struct intel_panel */
	struct rwlock backlight_lock;

	/* LVDS info */
	bool no_aux_handshake;

	/* protects panel power sequencer state */
	struct rwlock pps_mutex;

	struct drm_i915_fence_reg fence_regs[I915_MAX_NUM_FENCES]; /* assume 965 */
	int num_fence_regs; /* 8 on pre-965, 16 otherwise */

	unsigned int fsb_freq, mem_freq, is_ddr3;
	unsigned int skl_boot_cdclk;
	unsigned int cdclk_freq, max_cdclk_freq;
	unsigned int max_dotclk_freq;
	unsigned int hpll_freq;
	unsigned int czclk_freq;

	/**
	 * wq - Driver workqueue for GEM.
	 *
	 * NOTE: Work items scheduled here are not allowed to grab any modeset
	 * locks, for otherwise the flushing done in the pageflip code will
	 * result in deadlocks.
	 */
	struct workqueue_struct *wq;

	/* Display functions */
	struct drm_i915_display_funcs display;

	/* PCH chipset type */
	enum intel_pch pch_type;
	unsigned short pch_id;

	unsigned long quirks;

	enum modeset_restore modeset_restore;
	struct rwlock modeset_restore_lock;

	struct list_head vm_list; /* Global list of all address spaces */
	struct i915_gtt gtt; /* VM representing the global address space */

	struct i915_gem_mm mm;
	DECLARE_HASHTABLE(mm_structs, 7);
	struct rwlock mm_lock;

	/* Kernel Modesetting */

	struct sdvo_device_mapping sdvo_mappings[2];

	struct drm_crtc *plane_to_crtc_mapping[I915_MAX_PIPES];
	struct drm_crtc *pipe_to_crtc_mapping[I915_MAX_PIPES];
	wait_queue_head_t pending_flip_queue;

#ifdef CONFIG_DEBUG_FS
	struct intel_pipe_crc pipe_crc[I915_MAX_PIPES];
#endif

	int num_shared_dpll;
	struct intel_shared_dpll shared_dplls[I915_NUM_PLLS];
	int dpio_phy_iosf_port[I915_NUM_PHYS_VLV];

	struct i915_workarounds workarounds;

	/* Reclocking support */
	bool render_reclock_avail;

	struct i915_frontbuffer_tracking fb_tracking;

	u16 orig_clock;

	bool mchbar_need_disable;

	struct intel_l3_parity l3_parity;

	/* Cannot be determined by PCIID. You must always read a register. */
	size_t ellc_size;

	/* gen6+ rps state */
	struct intel_gen6_power_mgmt rps;

	/* ilk-only ips/rps state. Everything in here is protected by the global
	 * mchdev_lock in intel_pm.c */
	struct intel_ilk_power_mgmt ips;

	struct i915_power_domains power_domains;

	struct i915_psr psr;

	struct i915_gpu_error gpu_error;

	struct drm_i915_gem_object *vlv_pctx;

#ifdef CONFIG_DRM_FBDEV_EMULATION
	/* list of fbdev register on this device */
	struct intel_fbdev *fbdev;
	struct work_struct fbdev_suspend_work;
#endif

	struct drm_property *broadcast_rgb_property;
	struct drm_property *force_audio_property;

	/* hda/i915 audio component */
	struct i915_audio_component *audio_component;
	bool audio_component_registered;
	/**
	 * av_mutex - mutex for audio/video sync
	 *
	 */
	struct rwlock av_mutex;

	uint32_t hw_context_size;
	struct list_head context_list;

	u32 fdi_rx_config;

	u32 chv_phy_control;

	u32 suspend_count;
	struct i915_suspend_saved_registers regfile;
	struct vlv_s0ix_state vlv_s0ix_state;

	struct {
		/*
		 * Raw watermark latency values:
		 * in 0.1us units for WM0,
		 * in 0.5us units for WM1+.
		 */
		/* primary */
		uint16_t pri_latency[5];
		/* sprite */
		uint16_t spr_latency[5];
		/* cursor */
		uint16_t cur_latency[5];
		/*
		 * Raw watermark memory latency values
		 * for SKL for all 8 levels
		 * in 1us units.
		 */
		uint16_t skl_latency[8];

		/*
		 * The skl_wm_values structure is a bit too big for stack
		 * allocation, so we keep the staging struct where we store
		 * intermediate results here instead.
		 */
		struct skl_wm_values skl_results;

		/* current hardware state */
		union {
			struct ilk_wm_values hw;
			struct skl_wm_values skl_hw;
			struct vlv_wm_values vlv;
		};

		uint8_t max_level;
	} wm;

	struct i915_runtime_pm pm;

	/* Abstract the submission mechanism (legacy ringbuffer or execlists) away */
	struct {
		int (*execbuf_submit)(struct i915_execbuffer_params *params,
				      struct drm_i915_gem_execbuffer2 *args,
				      struct list_head *vmas);
		int (*init_rings)(struct drm_device *dev);
		void (*cleanup_ring)(struct intel_engine_cs *ring);
		void (*stop_ring)(struct intel_engine_cs *ring);
	} gt;

	bool edp_low_vswing;

	/* perform PHY state sanity checks? */
	bool chv_phy_assert[2];

	/*
	 * NOTE: This is the dri1/ums dungeon, don't add stuff here. Your patch
	 * will be rejected. Instead look for a better place.
	 */
};

static inline struct drm_i915_private *to_i915(const struct drm_device *dev)
{
	return dev->dev_private;
}

#ifdef __linux__
static inline struct drm_i915_private *dev_to_i915(struct device *dev)
{
	return to_i915(dev_get_drvdata(dev));
}
#endif

static inline struct drm_i915_private *guc_to_i915(struct intel_guc *guc)
{
	return container_of(guc, struct drm_i915_private, guc);
}

/* Iterate over initialised rings */
#define for_each_ring(ring__, dev_priv__, i__) \
	for ((i__) = 0; (i__) < I915_NUM_RINGS; (i__)++) \
		if (((ring__) = &(dev_priv__)->ring[(i__)]), intel_ring_initialized((ring__)))

enum hdmi_force_audio {
	HDMI_AUDIO_OFF_DVI = -2,	/* no aux data for HDMI-DVI converter */
	HDMI_AUDIO_OFF,			/* force turn off HDMI audio */
	HDMI_AUDIO_AUTO,		/* trust EDID */
	HDMI_AUDIO_ON,			/* force turn on HDMI audio */
};

#define I915_GTT_OFFSET_NONE ((u32)-1)

struct drm_i915_gem_object_ops {
	/* Interface between the GEM object and its backing storage.
	 * get_pages() is called once prior to the use of the associated set
	 * of pages before to binding them into the GTT, and put_pages() is
	 * called after we no longer need them. As we expect there to be
	 * associated cost with migrating pages between the backing storage
	 * and making them available for the GPU (e.g. clflush), we may hold
	 * onto the pages after they are no longer referenced by the GPU
	 * in case they may be used again shortly (for example migrating the
	 * pages to a different memory domain within the GTT). put_pages()
	 * will therefore most likely be called when the object itself is
	 * being released or under memory pressure (where we attempt to
	 * reap pages for the shrinker).
	 */
	int (*get_pages)(struct drm_i915_gem_object *);
	void (*put_pages)(struct drm_i915_gem_object *);
	int (*dmabuf_export)(struct drm_i915_gem_object *);
	void (*release)(struct drm_i915_gem_object *);
};

/*
 * Frontbuffer tracking bits. Set in obj->frontbuffer_bits while a gem bo is
 * considered to be the frontbuffer for the given plane interface-wise. This
 * doesn't mean that the hw necessarily already scans it out, but that any
 * rendering (by the cpu or gpu) will land in the frontbuffer eventually.
 *
 * We have one bit per pipe and per scanout plane type.
 */
#define INTEL_MAX_SPRITE_BITS_PER_PIPE 5
#define INTEL_FRONTBUFFER_BITS_PER_PIPE 8
#define INTEL_FRONTBUFFER_BITS \
	(INTEL_FRONTBUFFER_BITS_PER_PIPE * I915_MAX_PIPES)
#define INTEL_FRONTBUFFER_PRIMARY(pipe) \
	(1 << (INTEL_FRONTBUFFER_BITS_PER_PIPE * (pipe)))
#define INTEL_FRONTBUFFER_CURSOR(pipe) \
	(1 << (1 + (INTEL_FRONTBUFFER_BITS_PER_PIPE * (pipe))))
#define INTEL_FRONTBUFFER_SPRITE(pipe, plane) \
	(1 << (2 + plane + (INTEL_FRONTBUFFER_BITS_PER_PIPE * (pipe))))
#define INTEL_FRONTBUFFER_OVERLAY(pipe) \
	(1 << (2 + INTEL_MAX_SPRITE_BITS_PER_PIPE + (INTEL_FRONTBUFFER_BITS_PER_PIPE * (pipe))))
#define INTEL_FRONTBUFFER_ALL_MASK(pipe) \
	(0xff << (INTEL_FRONTBUFFER_BITS_PER_PIPE * (pipe)))

struct drm_i915_gem_object {
	struct drm_gem_object base;

	const struct drm_i915_gem_object_ops *ops;

	/** List of VMAs backed by this object */
	struct list_head vma_list;

	/** Stolen memory for this object, instead of being backed by shmem. */
	struct drm_mm_node *stolen;
	struct list_head global_list;

	struct list_head ring_list[I915_NUM_RINGS];
	/** Used in execbuf to temporarily hold a ref */
	struct list_head obj_exec_link;

	struct list_head batch_pool_link;

	/**
	 * This is set if the object is on the active lists (has pending
	 * rendering and so a non-zero seqno), and is not set if it i s on
	 * inactive (ready to be unbound) list.
	 */
	unsigned int active:I915_NUM_RINGS;

	/**
	 * This is set if the object has been written to since last bound
	 * to the GTT
	 */
	unsigned int dirty:1;

	/**
	 * Fence register bits (if any) for this object.  Will be set
	 * as needed when mapped into the GTT.
	 * Protected by dev->struct_mutex.
	 */
	signed int fence_reg:I915_MAX_NUM_FENCE_BITS;

	/**
	 * Advice: are the backing pages purgeable?
	 */
	unsigned int madv:2;

	/**
	 * Current tiling mode for the object.
	 */
	unsigned int tiling_mode:2;
	/**
	 * Whether the tiling parameters for the currently associated fence
	 * register have changed. Note that for the purposes of tracking
	 * tiling changes we also treat the unfenced register, the register
	 * slot that the object occupies whilst it executes a fenced
	 * command (such as BLT on gen2/3), as a "fence".
	 */
	unsigned int fence_dirty:1;

	/**
	 * Is the object at the current location in the gtt mappable and
	 * fenceable? Used to avoid costly recalculations.
	 */
	unsigned int map_and_fenceable:1;

	/**
	 * Whether the current gtt mapping needs to be mappable (and isn't just
	 * mappable by accident). Track pin and fault separate for a more
	 * accurate mappable working set.
	 */
	unsigned int fault_mappable:1;

	/*
	 * Is the object to be mapped as read-only to the GPU
	 * Only honoured if hardware has relevant pte bit
	 */
	unsigned long gt_ro:1;
	unsigned int cache_level:3;
	unsigned int cache_dirty:1;

	unsigned int frontbuffer_bits:INTEL_FRONTBUFFER_BITS;

	unsigned int pin_display;

	struct sg_table *pages;
	int pages_pin_count;
	struct get_page {
		struct scatterlist *sg;
		int last;
	} get_page;

	/* prime dma-buf support */
	void *dma_buf_vmapping;
	int vmapping_count;

	/** Breadcrumb of last rendering to the buffer.
	 * There can only be one writer, but we allow for multiple readers.
	 * If there is a writer that necessarily implies that all other
	 * read requests are complete - but we may only be lazily clearing
	 * the read requests. A read request is naturally the most recent
	 * request on a ring, so we may have two different write and read
	 * requests on one ring where the write request is older than the
	 * read request. This allows for the CPU to read from an active
	 * buffer by only waiting for the write to complete.
	 * */
	struct drm_i915_gem_request *last_read_req[I915_NUM_RINGS];
	struct drm_i915_gem_request *last_write_req;
	/** Breadcrumb of last fenced GPU access to the buffer. */
	struct drm_i915_gem_request *last_fenced_req;

	/** Current tiling stride for the object, if it's tiled. */
	uint32_t stride;

	/** References from framebuffers, locks out tiling changes. */
	unsigned long framebuffer_references;

	/** Record of address bit 17 of each page at last unbind. */
	unsigned long *bit_17;

	struct i915_gem_userptr {
		uintptr_t ptr;
		unsigned read_only :1;
		unsigned workers :4;
#define I915_GEM_USERPTR_MAX_WORKERS 15

		struct i915_mm_struct *mm;
		struct i915_mmu_object *mmu_object;
		struct work_struct *work;
	} userptr;

	/** for phys allocated objects */
	drm_dma_handle_t *phys_handle;
};
#define to_intel_bo(x) container_of(x, struct drm_i915_gem_object, base)

void i915_gem_track_fb(struct drm_i915_gem_object *old,
		       struct drm_i915_gem_object *new,
		       unsigned frontbuffer_bits);

/**
 * Request queue structure.
 *
 * The request queue allows us to note sequence numbers that have been emitted
 * and may be associated with active buffers to be retired.
 *
 * By keeping this list, we can avoid having to do questionable sequence
 * number comparisons on buffer last_read|write_seqno. It also allows an
 * emission time to be associated with the request for tracking how far ahead
 * of the GPU the submission is.
 *
 * The requests are reference counted, so upon creation they should have an
 * initial reference taken using kref_init
 */
struct drm_i915_gem_request {
	struct kref ref;

	/** On Which ring this request was generated */
	struct drm_i915_private *i915;
	struct intel_engine_cs *ring;

	 /** GEM sequence number associated with the previous request,
	  * when the HWS breadcrumb is equal to this the GPU is processing
	  * this request.
	  */
	u32 previous_seqno;

	 /** GEM sequence number associated with this request,
	  * when the HWS breadcrumb is equal or greater than this the GPU
	  * has finished processing this request.
	  */
	u32 seqno;

	/** Position in the ringbuffer of the start of the request */
	u32 head;

	/**
	 * Position in the ringbuffer of the start of the postfix.
	 * This is required to calculate the maximum available ringbuffer
	 * space without overwriting the postfix.
	 */
	 u32 postfix;

	/** Position in the ringbuffer of the end of the whole request */
	u32 tail;

	/**
	 * Context and ring buffer related to this request
	 * Contexts are refcounted, so when this request is associated with a
	 * context, we must increment the context's refcount, to guarantee that
	 * it persists while any request is linked to it. Requests themselves
	 * are also refcounted, so the request will only be freed when the last
	 * reference to it is dismissed, and the code in
	 * i915_gem_request_free() will then decrement the refcount on the
	 * context.
	 */
	struct intel_context *ctx;
	struct intel_ringbuffer *ringbuf;

	/** Batch buffer related to this request if any (used for
	    error state dump only) */
	struct drm_i915_gem_object *batch_obj;

	/** Time at which this request was emitted, in jiffies. */
	unsigned long emitted_jiffies;

	/** global list entry for this request */
	struct list_head list;

	struct drm_i915_file_private *file_priv;
	/** file_priv list entry for this request */
	struct list_head client_list;

	/** process identifier submitting this request */
	struct pid *pid;

	/**
	 * The ELSP only accepts two elements at a time, so we queue
	 * context/tail pairs on a given queue (ring->execlist_queue) until the
	 * hardware is available. The queue serves a double purpose: we also use
	 * it to keep track of the up to 2 contexts currently in the hardware
	 * (usually one in execution and the other queued up by the GPU): We
	 * only remove elements from the head of the queue when the hardware
	 * informs us that an element has been completed.
	 *
	 * All accesses to the queue are mediated by a spinlock
	 * (ring->execlist_lock).
	 */

	/** Execlist link in the submission queue.*/
	struct list_head execlist_link;

	/** Execlists no. of times this request has been sent to the ELSP */
	int elsp_submitted;

};

int i915_gem_request_alloc(struct intel_engine_cs *ring,
			   struct intel_context *ctx,
			   struct drm_i915_gem_request **req_out);
void i915_gem_request_cancel(struct drm_i915_gem_request *req);
void i915_gem_request_free(struct kref *req_ref);
int i915_gem_request_add_to_client(struct drm_i915_gem_request *req,
				   struct drm_file *file);

static inline uint32_t
i915_gem_request_get_seqno(struct drm_i915_gem_request *req)
{
	return req ? req->seqno : 0;
}

static inline struct intel_engine_cs *
i915_gem_request_get_ring(struct drm_i915_gem_request *req)
{
	return req ? req->ring : NULL;
}

static inline struct drm_i915_gem_request *
i915_gem_request_reference(struct drm_i915_gem_request *req)
{
	if (req)
		kref_get(&req->ref);
	return req;
}

static inline void
i915_gem_request_unreference(struct drm_i915_gem_request *req)
{
	WARN_ON(!mutex_is_locked(&req->ring->dev->struct_mutex));
	kref_put(&req->ref, i915_gem_request_free);
}

static inline void
i915_gem_request_unreference__unlocked(struct drm_i915_gem_request *req)
{
	struct drm_device *dev;

	if (!req)
		return;

	dev = req->ring->dev;
	if (kref_put_mutex(&req->ref, i915_gem_request_free, &dev->struct_mutex))
		mutex_unlock(&dev->struct_mutex);
}

static inline void i915_gem_request_assign(struct drm_i915_gem_request **pdst,
					   struct drm_i915_gem_request *src)
{
	if (src)
		i915_gem_request_reference(src);

	if (*pdst)
		i915_gem_request_unreference(*pdst);

	*pdst = src;
}

/*
 * XXX: i915_gem_request_completed should be here but currently needs the
 * definition of i915_seqno_passed() which is below. It will be moved in
 * a later patch when the call to i915_seqno_passed() is obsoleted...
 */

/*
 * A command that requires special handling by the command parser.
 */
struct drm_i915_cmd_descriptor {
	/*
	 * Flags describing how the command parser processes the command.
	 *
	 * CMD_DESC_FIXED: The command has a fixed length if this is set,
	 *                 a length mask if not set
	 * CMD_DESC_SKIP: The command is allowed but does not follow the
	 *                standard length encoding for the opcode range in
	 *                which it falls
	 * CMD_DESC_REJECT: The command is never allowed
	 * CMD_DESC_REGISTER: The command should be checked against the
	 *                    register whitelist for the appropriate ring
	 * CMD_DESC_MASTER: The command is allowed if the submitting process
	 *                  is the DRM master
	 */
	u32 flags;
#define CMD_DESC_FIXED    (1<<0)
#define CMD_DESC_SKIP     (1<<1)
#define CMD_DESC_REJECT   (1<<2)
#define CMD_DESC_REGISTER (1<<3)
#define CMD_DESC_BITMASK  (1<<4)
#define CMD_DESC_MASTER   (1<<5)

	/*
	 * The command's unique identification bits and the bitmask to get them.
	 * This isn't strictly the opcode field as defined in the spec and may
	 * also include type, subtype, and/or subop fields.
	 */
	struct {
		u32 value;
		u32 mask;
	} cmd;

	/*
	 * The command's length. The command is either fixed length (i.e. does
	 * not include a length field) or has a length field mask. The flag
	 * CMD_DESC_FIXED indicates a fixed length. Otherwise, the command has
	 * a length mask. All command entries in a command table must include
	 * length information.
	 */
	union {
		u32 fixed;
		u32 mask;
	} length;

	/*
	 * Describes where to find a register address in the command to check
	 * against the ring's register whitelist. Only valid if flags has the
	 * CMD_DESC_REGISTER bit set.
	 *
	 * A non-zero step value implies that the command may access multiple
	 * registers in sequence (e.g. LRI), in that case step gives the
	 * distance in dwords between individual offset fields.
	 */
	struct {
		u32 offset;
		u32 mask;
		u32 step;
	} reg;

#define MAX_CMD_DESC_BITMASKS 3
	/*
	 * Describes command checks where a particular dword is masked and
	 * compared against an expected value. If the command does not match
	 * the expected value, the parser rejects it. Only valid if flags has
	 * the CMD_DESC_BITMASK bit set. Only entries where mask is non-zero
	 * are valid.
	 *
	 * If the check specifies a non-zero condition_mask then the parser
	 * only performs the check when the bits specified by condition_mask
	 * are non-zero.
	 */
	struct {
		u32 offset;
		u32 mask;
		u32 expected;
		u32 condition_offset;
		u32 condition_mask;
	} bits[MAX_CMD_DESC_BITMASKS];
};

/*
 * A table of commands requiring special handling by the command parser.
 *
 * Each ring has an array of tables. Each table consists of an array of command
 * descriptors, which must be sorted with command opcodes in ascending order.
 */
struct drm_i915_cmd_table {
	const struct drm_i915_cmd_descriptor *table;
	int count;
};

/* Note that the (struct drm_i915_private *) cast is just to shut up gcc. */
#define __I915__(p) ({ \
	struct drm_i915_private *__p; \
	if (__builtin_types_compatible_p(typeof(*p), struct drm_i915_private)) \
		__p = (struct drm_i915_private *)p; \
	else if (__builtin_types_compatible_p(typeof(*p), struct drm_device)) \
		__p = to_i915((struct drm_device *)p); \
	else \
		BUILD_BUG(); \
	__p; \
})
#define INTEL_INFO(p) 	(&__I915__(p)->info)
#define INTEL_DEVID(p)	(INTEL_INFO(p)->device_id)
#define INTEL_REVID(p)	(__I915__(p)->dev->pdev->revision)

#define REVID_FOREVER	(0xff)

/*
 * Return true if revision is in range [since,until] inclusive.
 *
 * Use 0 for open-ended since, and REVID_FOREVER for open-ended until.
 */
#define IS_REVID(p, since, until) \
	(INTEL_REVID(p) >= (since) && INTEL_REVID(p) <= (until))

#define IS_I830(dev)		(INTEL_DEVID(dev) == 0x3577)
#define IS_845G(dev)		(INTEL_DEVID(dev) == 0x2562)
#define IS_I85X(dev)		(INTEL_INFO(dev)->is_i85x)
#define IS_I865G(dev)		(INTEL_DEVID(dev) == 0x2572)
#define IS_I915G(dev)		(INTEL_INFO(dev)->is_i915g)
#define IS_I915GM(dev)		(INTEL_DEVID(dev) == 0x2592)
#define IS_I945G(dev)		(INTEL_DEVID(dev) == 0x2772)
#define IS_I945GM(dev)		(INTEL_INFO(dev)->is_i945gm)
#define IS_BROADWATER(dev)	(INTEL_INFO(dev)->is_broadwater)
#define IS_CRESTLINE(dev)	(INTEL_INFO(dev)->is_crestline)
#define IS_GM45(dev)		(INTEL_DEVID(dev) == 0x2A42)
#define IS_G4X(dev)		(INTEL_INFO(dev)->is_g4x)
#define IS_PINEVIEW_G(dev)	(INTEL_DEVID(dev) == 0xa001)
#define IS_PINEVIEW_M(dev)	(INTEL_DEVID(dev) == 0xa011)
#define IS_PINEVIEW(dev)	(INTEL_INFO(dev)->is_pineview)
#define IS_G33(dev)		(INTEL_INFO(dev)->is_g33)
#define IS_IRONLAKE_M(dev)	(INTEL_DEVID(dev) == 0x0046)
#define IS_IVYBRIDGE(dev)	(INTEL_INFO(dev)->is_ivybridge)
#define IS_IVB_GT1(dev)		(INTEL_DEVID(dev) == 0x0156 || \
				 INTEL_DEVID(dev) == 0x0152 || \
				 INTEL_DEVID(dev) == 0x015a)
#define IS_VALLEYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview)
#define IS_CHERRYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview && IS_GEN8(dev))
#define IS_HASWELL(dev)	(INTEL_INFO(dev)->is_haswell)
#define IS_BROADWELL(dev)	(!INTEL_INFO(dev)->is_valleyview && IS_GEN8(dev))
#define IS_SKYLAKE(dev)	(INTEL_INFO(dev)->is_skylake)
#define IS_BROXTON(dev)	(INTEL_INFO(dev)->is_broxton)
#define IS_KABYLAKE(dev)	(INTEL_INFO(dev)->is_kabylake)
#define IS_MOBILE(dev)		(INTEL_INFO(dev)->is_mobile)
#define IS_HSW_EARLY_SDV(dev)	(IS_HASWELL(dev) && \
				 (INTEL_DEVID(dev) & 0xFF00) == 0x0C00)
#define IS_BDW_ULT(dev)		(IS_BROADWELL(dev) && \
				 ((INTEL_DEVID(dev) & 0xf) == 0x6 ||	\
				 (INTEL_DEVID(dev) & 0xf) == 0xb ||	\
				 (INTEL_DEVID(dev) & 0xf) == 0xe))
/* ULX machines are also considered ULT. */
#define IS_BDW_ULX(dev)		(IS_BROADWELL(dev) && \
				 (INTEL_DEVID(dev) & 0xf) == 0xe)
#define IS_BDW_GT3(dev)		(IS_BROADWELL(dev) && \
				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
#define IS_HSW_ULT(dev)		(IS_HASWELL(dev) && \
				 (INTEL_DEVID(dev) & 0xFF00) == 0x0A00)
#define IS_HSW_GT3(dev)		(IS_HASWELL(dev) && \
				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
/* ULX machines are also considered ULT. */
#define IS_HSW_ULX(dev)		(INTEL_DEVID(dev) == 0x0A0E || \
				 INTEL_DEVID(dev) == 0x0A1E)
#define IS_SKL_ULT(dev)		(INTEL_DEVID(dev) == 0x1906 || \
				 INTEL_DEVID(dev) == 0x1913 || \
				 INTEL_DEVID(dev) == 0x1916 || \
				 INTEL_DEVID(dev) == 0x1921 || \
				 INTEL_DEVID(dev) == 0x1926)
#define IS_SKL_ULX(dev)		(INTEL_DEVID(dev) == 0x190E || \
				 INTEL_DEVID(dev) == 0x1915 || \
				 INTEL_DEVID(dev) == 0x191E)
#define IS_KBL_ULT(dev)		(INTEL_DEVID(dev) == 0x5906 || \
				 INTEL_DEVID(dev) == 0x5913 || \
				 INTEL_DEVID(dev) == 0x5916 || \
				 INTEL_DEVID(dev) == 0x5921 || \
				 INTEL_DEVID(dev) == 0x5926)
#define IS_KBL_ULX(dev)		(INTEL_DEVID(dev) == 0x590E || \
				 INTEL_DEVID(dev) == 0x5915 || \
				 INTEL_DEVID(dev) == 0x591E)
#define IS_SKL_GT3(dev)		(IS_SKYLAKE(dev) && \
				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
#define IS_SKL_GT4(dev)		(IS_SKYLAKE(dev) && \
				 (INTEL_DEVID(dev) & 0x00F0) == 0x0030)

#define IS_PRELIMINARY_HW(intel_info) ((intel_info)->is_preliminary)

#define SKL_REVID_A0		(0x0)
#define SKL_REVID_B0		(0x1)
#define SKL_REVID_C0		(0x2)
#define SKL_REVID_D0		(0x3)
#define SKL_REVID_E0		(0x4)
#define SKL_REVID_F0		(0x5)

#define IS_SKL_REVID(p, since, until) (IS_SKYLAKE(p) && IS_REVID(p, since, until))

#define BXT_REVID_A0		(0x0)
#define BXT_REVID_A1		(0x1)
#define BXT_REVID_B0		(0x3)
#define BXT_REVID_C0		(0x9)

#define IS_BXT_REVID(p, since, until) (IS_BROXTON(p) && IS_REVID(p, since, until))

#define KBL_REVID_A0		(0x0)
#define KBL_REVID_B0		(0x1)
#define KBL_REVID_C0		(0x2)
#define KBL_REVID_D0		(0x3)
#define KBL_REVID_E0		(0x4)

#define IS_KBL_REVID(p, since, until) (IS_KABYLAKE(p) && IS_REVID(p, since, until))

/*
 * The genX designation typically refers to the render engine, so render
 * capability related checks should use IS_GEN, while display and other checks
 * have their own (e.g. HAS_PCH_SPLIT for ILK+ display, IS_foo for particular
 * chips, etc.).
 */
#define IS_GEN2(dev)	(INTEL_INFO(dev)->gen == 2)
#define IS_GEN3(dev)	(INTEL_INFO(dev)->gen == 3)
#define IS_GEN4(dev)	(INTEL_INFO(dev)->gen == 4)
#define IS_GEN5(dev)	(INTEL_INFO(dev)->gen == 5)
#define IS_GEN6(dev)	(INTEL_INFO(dev)->gen == 6)
#define IS_GEN7(dev)	(INTEL_INFO(dev)->gen == 7)
#define IS_GEN8(dev)	(INTEL_INFO(dev)->gen == 8)
#define IS_GEN9(dev)	(INTEL_INFO(dev)->gen == 9)

#define RENDER_RING		(1<<RCS)
#define BSD_RING		(1<<VCS)
#define BLT_RING		(1<<BCS)
#define VEBOX_RING		(1<<VECS)
#define BSD2_RING		(1<<VCS2)
#define HAS_BSD(dev)		(INTEL_INFO(dev)->ring_mask & BSD_RING)
#define HAS_BSD2(dev)		(INTEL_INFO(dev)->ring_mask & BSD2_RING)
#define HAS_BLT(dev)		(INTEL_INFO(dev)->ring_mask & BLT_RING)
#define HAS_VEBOX(dev)		(INTEL_INFO(dev)->ring_mask & VEBOX_RING)
#define HAS_LLC(dev)		(INTEL_INFO(dev)->has_llc)
#define HAS_WT(dev)		((IS_HASWELL(dev) || IS_BROADWELL(dev)) && \
				 __I915__(dev)->ellc_size)
#define I915_NEED_GFX_HWS(dev)	(INTEL_INFO(dev)->need_gfx_hws)

#define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 6)
#define HAS_LOGICAL_RING_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 8)
#define USES_PPGTT(dev)		(i915.enable_ppgtt)
#define USES_FULL_PPGTT(dev)	(i915.enable_ppgtt >= 2)
#define USES_FULL_48BIT_PPGTT(dev)	(i915.enable_ppgtt == 3)

#define HAS_OVERLAY(dev)		(INTEL_INFO(dev)->has_overlay)
#define OVERLAY_NEEDS_PHYSICAL(dev)	(INTEL_INFO(dev)->overlay_needs_physical)

/* Early gen2 have a totally busted CS tlb and require pinned batches. */
#define HAS_BROKEN_CS_TLB(dev)		(IS_I830(dev) || IS_845G(dev))
/*
 * dp aux and gmbus irq on gen4 seems to be able to generate legacy interrupts
 * even when in MSI mode. This results in spurious interrupt warnings if the
 * legacy irq no. is shared with another device. The kernel then disables that
 * interrupt source and so prevents the other device from working properly.
 */
#define HAS_AUX_IRQ(dev) (INTEL_INFO(dev)->gen >= 5)
#define HAS_GMBUS_IRQ(dev) (INTEL_INFO(dev)->gen >= 5)

/* With the 945 and later, Y tiling got adjusted so that it was 32 128-byte
 * rows, which changed the alignment requirements and fence programming.
 */
#define HAS_128_BYTE_Y_TILING(dev) (!IS_GEN2(dev) && !(IS_I915G(dev) || \
						      IS_I915GM(dev)))
#define SUPPORTS_TV(dev)		(INTEL_INFO(dev)->supports_tv)
#define I915_HAS_HOTPLUG(dev)		 (INTEL_INFO(dev)->has_hotplug)

#define HAS_FW_BLC(dev) (INTEL_INFO(dev)->gen > 2)
#define HAS_PIPE_CXSR(dev) (INTEL_INFO(dev)->has_pipe_cxsr)
#define HAS_FBC(dev) (INTEL_INFO(dev)->has_fbc)

#define HAS_IPS(dev)		(IS_HSW_ULT(dev) || IS_BROADWELL(dev))

#define HAS_DP_MST(dev)		(IS_HASWELL(dev) || IS_BROADWELL(dev) || \
				 INTEL_INFO(dev)->gen >= 9)

#define HAS_DDI(dev)		(INTEL_INFO(dev)->has_ddi)
#define HAS_FPGA_DBG_UNCLAIMED(dev)	(INTEL_INFO(dev)->has_fpga_dbg)
#define HAS_PSR(dev)		(IS_HASWELL(dev) || IS_BROADWELL(dev) || \
				 IS_VALLEYVIEW(dev) || IS_CHERRYVIEW(dev) || \
				 IS_SKYLAKE(dev) || IS_KABYLAKE(dev))
#define HAS_RUNTIME_PM(dev)	(IS_GEN6(dev) || IS_HASWELL(dev) || \
				 IS_BROADWELL(dev) || IS_VALLEYVIEW(dev) || \
				 IS_SKYLAKE(dev) || IS_KABYLAKE(dev))
#define HAS_RC6(dev)		(INTEL_INFO(dev)->gen >= 6)
#define HAS_RC6p(dev)		(INTEL_INFO(dev)->gen == 6 || IS_IVYBRIDGE(dev))

#define HAS_CSR(dev)	(IS_GEN9(dev))

#define HAS_GUC_UCODE(dev)	(IS_GEN9(dev) && !IS_KABYLAKE(dev))
#define HAS_GUC_SCHED(dev)	(IS_GEN9(dev) && !IS_KABYLAKE(dev))

#define HAS_RESOURCE_STREAMER(dev) (IS_HASWELL(dev) || \
				    INTEL_INFO(dev)->gen >= 8)

#define HAS_CORE_RING_FREQ(dev)	(INTEL_INFO(dev)->gen >= 6 && \
				 !IS_VALLEYVIEW(dev) && !IS_BROXTON(dev))

#define INTEL_PCH_DEVICE_ID_MASK		0xff00
#define INTEL_PCH_IBX_DEVICE_ID_TYPE		0x3b00
#define INTEL_PCH_CPT_DEVICE_ID_TYPE		0x1c00
#define INTEL_PCH_PPT_DEVICE_ID_TYPE		0x1e00
#define INTEL_PCH_LPT_DEVICE_ID_TYPE		0x8c00
#define INTEL_PCH_LPT_LP_DEVICE_ID_TYPE		0x9c00
#define INTEL_PCH_SPT_DEVICE_ID_TYPE		0xA100
#define INTEL_PCH_SPT_LP_DEVICE_ID_TYPE		0x9D00
#define INTEL_PCH_KBP_DEVICE_ID_TYPE		0xA200
#define INTEL_PCH_P2X_DEVICE_ID_TYPE		0x7100
#define INTEL_PCH_QEMU_DEVICE_ID_TYPE		0x2900 /* qemu q35 has 2918 */

#define INTEL_PCH_TYPE(dev) (__I915__(dev)->pch_type)
#define HAS_PCH_KBP(dev_priv) (INTEL_PCH_TYPE(dev_priv) == PCH_KBP)
#define HAS_PCH_SPT(dev) (INTEL_PCH_TYPE(dev) == PCH_SPT)
#define HAS_PCH_LPT(dev) (INTEL_PCH_TYPE(dev) == PCH_LPT)
#define HAS_PCH_LPT_LP(dev) (__I915__(dev)->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE)
#define HAS_PCH_CPT(dev) (INTEL_PCH_TYPE(dev) == PCH_CPT)
#define HAS_PCH_IBX(dev) (INTEL_PCH_TYPE(dev) == PCH_IBX)
#define HAS_PCH_NOP(dev) (INTEL_PCH_TYPE(dev) == PCH_NOP)
#define HAS_PCH_SPLIT(dev) (INTEL_PCH_TYPE(dev) != PCH_NONE)

#define HAS_GMCH_DISPLAY(dev) (INTEL_INFO(dev)->gen < 5 || IS_VALLEYVIEW(dev))

/* DPF == dynamic parity feature */
#define HAS_L3_DPF(dev) (IS_IVYBRIDGE(dev) || IS_HASWELL(dev))
#define NUM_L3_SLICES(dev) (IS_HSW_GT3(dev) ? 2 : HAS_L3_DPF(dev))

#define GT_FREQUENCY_MULTIPLIER 50
#define GEN9_FREQ_SCALER 3

#include "i915_trace.h"

extern const struct drm_ioctl_desc i915_ioctls[];
extern int i915_max_ioctl;

#ifdef __linux__
extern int i915_suspend_switcheroo(struct drm_device *dev, pm_message_t state);
extern int i915_resume_switcheroo(struct drm_device *dev);
#endif

/* i915_params.c */
struct i915_params {
	int modeset;
	int panel_ignore_lid;
	int semaphores;
	int lvds_channel_mode;
	int panel_use_ssc;
	int vbt_sdvo_panel_type;
	int enable_rc6;
	int enable_fbc;
	int enable_ppgtt;
	int enable_execlists;
	int enable_psr;
	unsigned int preliminary_hw_support;
	int disable_power_well;
	int enable_ips;
	int invert_brightness;
	int enable_cmd_parser;
	/* leave bools at the end to not create holes */
	bool enable_hangcheck;
	bool fastboot;
	bool prefault_disable;
	bool load_detect_test;
	bool reset;
	bool disable_display;
	bool disable_vtd_wa;
	bool enable_guc_submission;
	int guc_log_level;
	int use_mmio_flip;
	int mmio_debug;
	bool verbose_state_checks;
	bool nuclear_pageflip;
	int edp_vswing;
};
extern struct i915_params i915 __read_mostly;

				/* i915_dma.c */
extern int i915_driver_load(struct drm_device *, unsigned long flags);
extern int i915_driver_unload(struct drm_device *);
extern int i915_driver_open(struct drm_device *dev, struct drm_file *file);
extern void i915_driver_lastclose(struct drm_device * dev);
extern void i915_driver_preclose(struct drm_device *dev,
				 struct drm_file *file);
extern void i915_driver_postclose(struct drm_device *dev,
				  struct drm_file *file);
#ifdef CONFIG_COMPAT
extern long i915_compat_ioctl(struct file *filp, unsigned int cmd,
			      unsigned long arg);
#endif
extern int intel_gpu_reset(struct drm_device *dev);
extern bool intel_has_gpu_reset(struct drm_device *dev);
extern int i915_reset(struct drm_device *dev);
extern unsigned long i915_chipset_val(struct drm_i915_private *dev_priv);
extern unsigned long i915_mch_val(struct drm_i915_private *dev_priv);
extern unsigned long i915_gfx_val(struct drm_i915_private *dev_priv);
extern void i915_update_gfx_val(struct drm_i915_private *dev_priv);
int vlv_force_gfx_clock(struct drm_i915_private *dev_priv, bool on);
void i915_firmware_load_error_print(const char *fw_path, int err);

/* intel_hotplug.c */
void intel_hpd_irq_handler(struct drm_device *dev, u32 pin_mask, u32 long_mask);
void intel_hpd_init(struct drm_i915_private *dev_priv);
void intel_hpd_init_work(struct drm_i915_private *dev_priv);
void intel_hpd_cancel_work(struct drm_i915_private *dev_priv);
bool intel_hpd_pin_to_port(enum hpd_pin pin, enum port *port);

/* i915_irq.c */
void i915_queue_hangcheck(struct drm_device *dev);
__printf(3, 4)
void i915_handle_error(struct drm_device *dev, bool wedged,
		       const char *fmt, ...);

extern void intel_irq_init(struct drm_i915_private *dev_priv);
int intel_irq_install(struct drm_i915_private *dev_priv);
void intel_irq_uninstall(struct drm_i915_private *dev_priv);

extern void intel_uncore_sanitize(struct drm_device *dev);
extern void intel_uncore_early_sanitize(struct drm_device *dev,
					bool restore_forcewake);
extern void intel_uncore_init(struct drm_device *dev);
extern void intel_uncore_check_errors(struct drm_device *dev);
extern void intel_uncore_fini(struct drm_device *dev);
extern void intel_uncore_forcewake_reset(struct drm_device *dev, bool restore);
const char *intel_uncore_forcewake_domain_to_str(const enum forcewake_domain_id id);
void intel_uncore_forcewake_get(struct drm_i915_private *dev_priv,
				enum forcewake_domains domains);
void intel_uncore_forcewake_put(struct drm_i915_private *dev_priv,
				enum forcewake_domains domains);
/* Like above but the caller must manage the uncore.lock itself.
 * Must be used with I915_READ_FW and friends.
 */
void intel_uncore_forcewake_get__locked(struct drm_i915_private *dev_priv,
					enum forcewake_domains domains);
void intel_uncore_forcewake_put__locked(struct drm_i915_private *dev_priv,
					enum forcewake_domains domains);
void assert_forcewakes_inactive(struct drm_i915_private *dev_priv);
static inline bool intel_vgpu_active(struct drm_device *dev)
{
#ifdef __linux__
	return to_i915(dev)->vgpu.active;
#else
	return false;
#endif
}

void
i915_enable_pipestat(struct drm_i915_private *dev_priv, enum pipe pipe,
		     u32 status_mask);

void
i915_disable_pipestat(struct drm_i915_private *dev_priv, enum pipe pipe,
		      u32 status_mask);

void valleyview_enable_display_irqs(struct drm_i915_private *dev_priv);
void valleyview_disable_display_irqs(struct drm_i915_private *dev_priv);
void i915_hotplug_interrupt_update(struct drm_i915_private *dev_priv,
				   uint32_t mask,
				   uint32_t bits);
void
ironlake_enable_display_irq(struct drm_i915_private *dev_priv, u32 mask);
void
ironlake_disable_display_irq(struct drm_i915_private *dev_priv, u32 mask);
void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
				  uint32_t interrupt_mask,
				  uint32_t enabled_irq_mask);
#define ibx_enable_display_interrupt(dev_priv, bits) \
	ibx_display_interrupt_update((dev_priv), (bits), (bits))
#define ibx_disable_display_interrupt(dev_priv, bits) \
	ibx_display_interrupt_update((dev_priv), (bits), 0)

/* i915_gem.c */
int i915_gem_create_ioctl(struct drm_device *dev, void *data,
			  struct drm_file *file_priv);
int i915_gem_pread_ioctl(struct drm_device *dev, void *data,
			 struct drm_file *file_priv);
int i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
			  struct drm_file *file_priv);
int i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
int i915_gem_mmap_gtt_ioctl(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
int i915_gem_set_domain_ioctl(struct drm_device *dev, void *data,
			      struct drm_file *file_priv);
int i915_gem_sw_finish_ioctl(struct drm_device *dev, void *data,
			     struct drm_file *file_priv);
void i915_gem_execbuffer_move_to_active(struct list_head *vmas,
					struct drm_i915_gem_request *req);
void i915_gem_execbuffer_retire_commands(struct i915_execbuffer_params *params);
int i915_gem_ringbuffer_submission(struct i915_execbuffer_params *params,
				   struct drm_i915_gem_execbuffer2 *args,
				   struct list_head *vmas);
int i915_gem_execbuffer(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
int i915_gem_execbuffer2(struct drm_device *dev, void *data,
			 struct drm_file *file_priv);
int i915_gem_busy_ioctl(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
int i915_gem_get_caching_ioctl(struct drm_device *dev, void *data,
			       struct drm_file *file);
int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
			       struct drm_file *file);
int i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
			    struct drm_file *file_priv);
int i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
			   struct drm_file *file_priv);
int i915_gem_set_tiling(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
int i915_gem_get_tiling(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
int i915_gem_init_userptr(struct drm_device *dev);
int i915_gem_userptr_ioctl(struct drm_device *dev, void *data,
			   struct drm_file *file);
int i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
				struct drm_file *file_priv);
int i915_gem_wait_ioctl(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
void i915_gem_load(struct drm_device *dev);
void *i915_gem_object_alloc(struct drm_device *dev);
void i915_gem_object_free(struct drm_i915_gem_object *obj);
void i915_gem_object_init(struct drm_i915_gem_object *obj,
			 const struct drm_i915_gem_object_ops *ops);
struct drm_i915_gem_object *i915_gem_alloc_object(struct drm_device *dev,
						  size_t size);
struct drm_i915_gem_object *i915_gem_object_create_from_data(
		struct drm_device *dev, const void *data, size_t size);
void i915_gem_free_object(struct drm_gem_object *obj);
void i915_gem_vma_destroy(struct i915_vma *vma);

/* Flags used by pin/bind&friends. */
#define PIN_MAPPABLE	(1<<0)
#define PIN_NONBLOCK	(1<<1)
#define PIN_GLOBAL	(1<<2)
#define PIN_OFFSET_BIAS	(1<<3)
#define PIN_USER	(1<<4)
#define PIN_UPDATE	(1<<5)
#define PIN_ZONE_4G	(1<<6)
#define PIN_HIGH	(1<<7)
#define PIN_OFFSET_MASK (~4095)
int __must_check
i915_gem_object_pin(struct drm_i915_gem_object *obj,
		    struct i915_address_space *vm,
		    uint32_t alignment,
		    uint64_t flags);
int __must_check
i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
			 const struct i915_ggtt_view *view,
			 uint32_t alignment,
			 uint64_t flags);

int i915_vma_bind(struct i915_vma *vma, enum i915_cache_level cache_level,
		  u32 flags);
void __i915_vma_set_map_and_fenceable(struct i915_vma *vma);
int __must_check i915_vma_unbind(struct i915_vma *vma);
/*
 * BEWARE: Do not use the function below unless you can _absolutely_
 * _guarantee_ VMA in question is _not in use_ anywhere.
 */
int __must_check __i915_vma_unbind_no_wait(struct i915_vma *vma);
int i915_gem_object_put_pages(struct drm_i915_gem_object *obj);
void i915_gem_release_all_mmaps(struct drm_i915_private *dev_priv);
void i915_gem_release_mmap(struct drm_i915_gem_object *obj);

int i915_gem_obj_prepare_shmem_read(struct drm_i915_gem_object *obj,
				    int *needs_clflush);

int __must_check i915_gem_object_get_pages(struct drm_i915_gem_object *obj);

static inline int __sg_page_count(struct scatterlist *sg)
{
	return sg->length >> PAGE_SHIFT;
}

#ifdef __linux__
static inline struct page *
i915_gem_object_get_page(struct drm_i915_gem_object *obj, int n)
{
	if (WARN_ON(n >= obj->base.size >> PAGE_SHIFT))
		return NULL;

	if (n < obj->get_page.last) {
		obj->get_page.sg = obj->pages->sgl;
		obj->get_page.last = 0;
	}

	while (obj->get_page.last + __sg_page_count(obj->get_page.sg) <= n) {
		obj->get_page.last += __sg_page_count(obj->get_page.sg++);
		if (unlikely(sg_is_chain(obj->get_page.sg)))
			obj->get_page.sg = sg_chain_ptr(obj->get_page.sg);
	}

	return nth_page(sg_page(obj->get_page.sg), n - obj->get_page.last);
}
#else
static inline struct vm_page *
i915_gem_object_get_page(struct drm_i915_gem_object *obj, int n)
{
	if (WARN_ON(n >= obj->base.size >> PAGE_SHIFT))
		return NULL;

	if (n < obj->get_page.last) {
		obj->get_page.sg = obj->pages->sgl;
		obj->get_page.last = 0;
	}

	while (obj->get_page.last + __sg_page_count(obj->get_page.sg) <= n)
		obj->get_page.last += __sg_page_count(obj->get_page.sg++);

	return PHYS_TO_VM_PAGE(obj->get_page.sg->dma_address +
			       (n - obj->get_page.last) * PAGE_SIZE);
}
#endif

static inline void i915_gem_object_pin_pages(struct drm_i915_gem_object *obj)
{
	BUG_ON(obj->pages == NULL);
	obj->pages_pin_count++;
}
static inline void i915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)
{
	BUG_ON(obj->pages_pin_count == 0);
	obj->pages_pin_count--;
}

int __must_check i915_mutex_lock_interruptible(struct drm_device *dev);
int i915_gem_object_sync(struct drm_i915_gem_object *obj,
			 struct intel_engine_cs *to,
			 struct drm_i915_gem_request **to_req);
void i915_vma_move_to_active(struct i915_vma *vma,
			     struct drm_i915_gem_request *req);
int i915_gem_dumb_create(struct drm_file *file_priv,
			 struct drm_device *dev,
			 struct drm_mode_create_dumb *args);
int i915_gem_mmap_gtt(struct drm_file *file_priv, struct drm_device *dev,
		      uint32_t handle, uint64_t *offset);
/**
 * Returns true if seq1 is later than seq2.
 */
static inline bool
i915_seqno_passed(uint32_t seq1, uint32_t seq2)
{
	return (int32_t)(seq1 - seq2) >= 0;
}

static inline bool i915_gem_request_started(struct drm_i915_gem_request *req,
					   bool lazy_coherency)
{
	u32 seqno = req->ring->get_seqno(req->ring, lazy_coherency);
	return i915_seqno_passed(seqno, req->previous_seqno);
}

static inline bool i915_gem_request_completed(struct drm_i915_gem_request *req,
					      bool lazy_coherency)
{
	u32 seqno = req->ring->get_seqno(req->ring, lazy_coherency);
	return i915_seqno_passed(seqno, req->seqno);
}

int __must_check i915_gem_get_seqno(struct drm_device *dev, u32 *seqno);
int __must_check i915_gem_set_seqno(struct drm_device *dev, u32 seqno);

struct drm_i915_gem_request *
i915_gem_find_active_request(struct intel_engine_cs *ring);

bool i915_gem_retire_requests(struct drm_device *dev);
void i915_gem_retire_requests_ring(struct intel_engine_cs *ring);
int __must_check i915_gem_check_wedge(struct i915_gpu_error *error,
				      bool interruptible);

static inline bool i915_reset_in_progress(struct i915_gpu_error *error)
{
	return unlikely(atomic_read(&error->reset_counter)
			& (I915_RESET_IN_PROGRESS_FLAG | I915_WEDGED));
}

static inline bool i915_terminally_wedged(struct i915_gpu_error *error)
{
	return atomic_read(&error->reset_counter) & I915_WEDGED;
}

static inline u32 i915_reset_count(struct i915_gpu_error *error)
{
	return ((atomic_read(&error->reset_counter) & ~I915_WEDGED) + 1) / 2;
}

static inline bool i915_stop_ring_allow_ban(struct drm_i915_private *dev_priv)
{
	return dev_priv->gpu_error.stop_rings == 0 ||
		dev_priv->gpu_error.stop_rings & I915_STOP_RING_ALLOW_BAN;
}

static inline bool i915_stop_ring_allow_warn(struct drm_i915_private *dev_priv)
{
	return dev_priv->gpu_error.stop_rings == 0 ||
		dev_priv->gpu_error.stop_rings & I915_STOP_RING_ALLOW_WARN;
}

void i915_gem_reset(struct drm_device *dev);
bool i915_gem_clflush_object(struct drm_i915_gem_object *obj, bool force);
int __must_check i915_gem_init(struct drm_device *dev);
int i915_gem_init_rings(struct drm_device *dev);
int __must_check i915_gem_init_hw(struct drm_device *dev);
int i915_gem_l3_remap(struct drm_i915_gem_request *req, int slice);
void i915_gem_init_swizzling(struct drm_device *dev);
void i915_gem_cleanup_ringbuffer(struct drm_device *dev);
int __must_check i915_gpu_idle(struct drm_device *dev);
int __must_check i915_gem_suspend(struct drm_device *dev);
void __i915_add_request(struct drm_i915_gem_request *req,
			struct drm_i915_gem_object *batch_obj,
			bool flush_caches);
#define i915_add_request(req) \
	__i915_add_request(req, NULL, true)
#define i915_add_request_no_flush(req) \
	__i915_add_request(req, NULL, false)
int __i915_wait_request(struct drm_i915_gem_request *req,
			unsigned reset_counter,
			bool interruptible,
			s64 *timeout,
			struct intel_rps_client *rps);
int __must_check i915_wait_request(struct drm_i915_gem_request *req);
#ifdef __linux__
int i915_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
#else
int i915_gem_fault(struct drm_gem_object *gem_obj, struct uvm_faultinfo *ufi,
		   off_t offset, vaddr_t vaddr, vm_page_t *pps, int npages,
		   int centeridx, vm_prot_t access_type, int flags);
#endif
int __must_check
i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
			       bool readonly);
int __must_check
i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj,
				  bool write);
int __must_check
i915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *obj, bool write);
int __must_check
i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
				     u32 alignment,
				     struct intel_engine_cs *pipelined,
				     struct drm_i915_gem_request **pipelined_request,
				     const struct i915_ggtt_view *view);
void i915_gem_object_unpin_from_display_plane(struct drm_i915_gem_object *obj,
					      const struct i915_ggtt_view *view);
int i915_gem_object_attach_phys(struct drm_i915_gem_object *obj,
				int align);
int i915_gem_open(struct drm_device *dev, struct drm_file *file);
void i915_gem_release(struct drm_device *dev, struct drm_file *file);

uint32_t
i915_gem_get_gtt_size(struct drm_device *dev, uint32_t size, int tiling_mode);
uint32_t
i915_gem_get_gtt_alignment(struct drm_device *dev, uint32_t size,
			    int tiling_mode, bool fenced);

int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
				    enum i915_cache_level cache_level);

#ifdef notyet
struct drm_gem_object *i915_gem_prime_import(struct drm_device *dev,
				struct dma_buf *dma_buf);

struct dma_buf *i915_gem_prime_export(struct drm_device *dev,
				struct drm_gem_object *gem_obj, int flags);
#endif

u64 i915_gem_obj_ggtt_offset_view(struct drm_i915_gem_object *o,
				  const struct i915_ggtt_view *view);
u64 i915_gem_obj_offset(struct drm_i915_gem_object *o,
			struct i915_address_space *vm);
static inline u64
i915_gem_obj_ggtt_offset(struct drm_i915_gem_object *o)
{
	return i915_gem_obj_ggtt_offset_view(o, &i915_ggtt_view_normal);
}

bool i915_gem_obj_bound_any(struct drm_i915_gem_object *o);
bool i915_gem_obj_ggtt_bound_view(struct drm_i915_gem_object *o,
				  const struct i915_ggtt_view *view);
bool i915_gem_obj_bound(struct drm_i915_gem_object *o,
			struct i915_address_space *vm);

unsigned long i915_gem_obj_size(struct drm_i915_gem_object *o,
				struct i915_address_space *vm);
struct i915_vma *
i915_gem_obj_to_vma(struct drm_i915_gem_object *obj,
		    struct i915_address_space *vm);
struct i915_vma *
i915_gem_obj_to_ggtt_view(struct drm_i915_gem_object *obj,
			  const struct i915_ggtt_view *view);

struct i915_vma *
i915_gem_obj_lookup_or_create_vma(struct drm_i915_gem_object *obj,
				  struct i915_address_space *vm);
struct i915_vma *
i915_gem_obj_lookup_or_create_ggtt_vma(struct drm_i915_gem_object *obj,
				       const struct i915_ggtt_view *view);

static inline struct i915_vma *
i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj)
{
	return i915_gem_obj_to_ggtt_view(obj, &i915_ggtt_view_normal);
}
bool i915_gem_obj_is_pinned(struct drm_i915_gem_object *obj);

/* Some GGTT VM helpers */
#define i915_obj_to_ggtt(obj) \
	(&((struct drm_i915_private *)(obj)->base.dev->dev_private)->gtt.base)
static inline bool i915_is_ggtt(struct i915_address_space *vm)
{
	struct i915_address_space *ggtt =
		&((struct drm_i915_private *)(vm)->dev->dev_private)->gtt.base;
	return vm == ggtt;
}

static inline struct i915_hw_ppgtt *
i915_vm_to_ppgtt(struct i915_address_space *vm)
{
	WARN_ON(i915_is_ggtt(vm));

	return container_of(vm, struct i915_hw_ppgtt, base);
}


static inline bool i915_gem_obj_ggtt_bound(struct drm_i915_gem_object *obj)
{
	return i915_gem_obj_ggtt_bound_view(obj, &i915_ggtt_view_normal);
}

static inline unsigned long
i915_gem_obj_ggtt_size(struct drm_i915_gem_object *obj)
{
	return i915_gem_obj_size(obj, i915_obj_to_ggtt(obj));
}

static inline int __must_check
i915_gem_obj_ggtt_pin(struct drm_i915_gem_object *obj,
		      uint32_t alignment,
		      unsigned flags)
{
	return i915_gem_object_pin(obj, i915_obj_to_ggtt(obj),
				   alignment, flags | PIN_GLOBAL);
}

static inline int
i915_gem_object_ggtt_unbind(struct drm_i915_gem_object *obj)
{
	return i915_vma_unbind(i915_gem_obj_to_ggtt(obj));
}

void i915_gem_object_ggtt_unpin_view(struct drm_i915_gem_object *obj,
				     const struct i915_ggtt_view *view);
static inline void
i915_gem_object_ggtt_unpin(struct drm_i915_gem_object *obj)
{
	i915_gem_object_ggtt_unpin_view(obj, &i915_ggtt_view_normal);
}

/* i915_gem_fence.c */
int __must_check i915_gem_object_get_fence(struct drm_i915_gem_object *obj);
int __must_check i915_gem_object_put_fence(struct drm_i915_gem_object *obj);

bool i915_gem_object_pin_fence(struct drm_i915_gem_object *obj);
void i915_gem_object_unpin_fence(struct drm_i915_gem_object *obj);

void i915_gem_restore_fences(struct drm_device *dev);

void i915_gem_detect_bit_6_swizzle(struct drm_device *dev);
void i915_gem_object_do_bit_17_swizzle(struct drm_i915_gem_object *obj);
void i915_gem_object_save_bit_17_swizzle(struct drm_i915_gem_object *obj);

/* i915_gem_context.c */
int __must_check i915_gem_context_init(struct drm_device *dev);
void i915_gem_context_fini(struct drm_device *dev);
void i915_gem_context_reset(struct drm_device *dev);
int i915_gem_context_open(struct drm_device *dev, struct drm_file *file);
int i915_gem_context_enable(struct drm_i915_gem_request *req);
void i915_gem_context_close(struct drm_device *dev, struct drm_file *file);
int i915_switch_context(struct drm_i915_gem_request *req);
struct intel_context *
i915_gem_context_get(struct drm_i915_file_private *file_priv, u32 id);
void i915_gem_context_free(struct kref *ctx_ref);
struct drm_i915_gem_object *
i915_gem_alloc_context_obj(struct drm_device *dev, size_t size);
static inline void i915_gem_context_reference(struct intel_context *ctx)
{
	kref_get(&ctx->ref);
}

static inline void i915_gem_context_unreference(struct intel_context *ctx)
{
	kref_put(&ctx->ref, i915_gem_context_free);
}

static inline bool i915_gem_context_is_default(const struct intel_context *c)
{
	return c->user_handle == DEFAULT_CONTEXT_HANDLE;
}

int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,
				  struct drm_file *file);
int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,
				   struct drm_file *file);
int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
				    struct drm_file *file_priv);
int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
				    struct drm_file *file_priv);

/* i915_gem_evict.c */
int __must_check i915_gem_evict_something(struct drm_device *dev,
					  struct i915_address_space *vm,
					  int min_size,
					  unsigned alignment,
					  unsigned cache_level,
					  unsigned long start,
					  unsigned long end,
					  unsigned flags);
int i915_gem_evict_vm(struct i915_address_space *vm, bool do_idle);

/* belongs in i915_gem_gtt.h */
static inline void i915_gem_chipset_flush(struct drm_device *dev)
{
	if (INTEL_INFO(dev)->gen < 6)
		intel_gtt_chipset_flush();
}

/* i915_gem_stolen.c */
int i915_gem_stolen_insert_node(struct drm_i915_private *dev_priv,
				struct drm_mm_node *node, u64 size,
				unsigned alignment);
int i915_gem_stolen_insert_node_in_range(struct drm_i915_private *dev_priv,
					 struct drm_mm_node *node, u64 size,
					 unsigned alignment, u64 start,
					 u64 end);
void i915_gem_stolen_remove_node(struct drm_i915_private *dev_priv,
				 struct drm_mm_node *node);
int i915_gem_init_stolen(struct drm_device *dev);
void i915_gem_cleanup_stolen(struct drm_device *dev);
struct drm_i915_gem_object *
i915_gem_object_create_stolen(struct drm_device *dev, u32 size);
struct drm_i915_gem_object *
i915_gem_object_create_stolen_for_preallocated(struct drm_device *dev,
					       u32 stolen_offset,
					       u32 gtt_offset,
					       u32 size);

/* i915_gem_shrinker.c */
unsigned long i915_gem_shrink(struct drm_i915_private *dev_priv,
			      unsigned long target,
			      unsigned flags);
#define I915_SHRINK_PURGEABLE 0x1
#define I915_SHRINK_UNBOUND 0x2
#define I915_SHRINK_BOUND 0x4
#define I915_SHRINK_ACTIVE 0x8
unsigned long i915_gem_shrink_all(struct drm_i915_private *dev_priv);
void i915_gem_shrinker_init(struct drm_i915_private *dev_priv);


/* i915_gem_tiling.c */
static inline bool i915_gem_object_needs_bit17_swizzle(struct drm_i915_gem_object *obj)
{
	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;

	return dev_priv->mm.bit_6_swizzle_x == I915_BIT_6_SWIZZLE_9_10_17 &&
		obj->tiling_mode != I915_TILING_NONE;
}

/* i915_gem_debug.c */
#if WATCH_LISTS
int i915_verify_lists(struct drm_device *dev);
#else
#define i915_verify_lists(dev) 0
#endif

/* i915_debugfs.c */
int i915_debugfs_init(struct drm_minor *minor);
void i915_debugfs_cleanup(struct drm_minor *minor);
#ifdef CONFIG_DEBUG_FS
int i915_debugfs_connector_add(struct drm_connector *connector);
void intel_display_crc_init(struct drm_device *dev);
#else
static inline int i915_debugfs_connector_add(struct drm_connector *connector)
{ return 0; }
static inline void intel_display_crc_init(struct drm_device *dev) {}
#endif

/* i915_gpu_error.c */
__printf(2, 3)
void i915_error_printf(struct drm_i915_error_state_buf *e, const char *f, ...);
int i915_error_state_to_str(struct drm_i915_error_state_buf *estr,
			    const struct i915_error_state_file_priv *error);
int i915_error_state_buf_init(struct drm_i915_error_state_buf *eb,
			      struct drm_i915_private *i915,
			      size_t count, loff_t pos);
static inline void i915_error_state_buf_release(
	struct drm_i915_error_state_buf *eb)
{
	kfree(eb->buf);
}
void i915_capture_error_state(struct drm_device *dev, bool wedge,
			      const char *error_msg);
void i915_error_state_get(struct drm_device *dev,
			  struct i915_error_state_file_priv *error_priv);
void i915_error_state_put(struct i915_error_state_file_priv *error_priv);
void i915_destroy_error_state(struct drm_device *dev);

void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone);
const char *i915_cache_level_str(struct drm_i915_private *i915, int type);

/* i915_cmd_parser.c */
int i915_cmd_parser_get_version(void);
int i915_cmd_parser_init_ring(struct intel_engine_cs *ring);
void i915_cmd_parser_fini_ring(struct intel_engine_cs *ring);
bool i915_needs_cmd_parser(struct intel_engine_cs *ring);
int i915_parse_cmds(struct intel_engine_cs *ring,
		    struct drm_i915_gem_object *batch_obj,
		    struct drm_i915_gem_object *shadow_batch_obj,
		    u32 batch_start_offset,
		    u32 batch_len,
		    bool is_master);

/* i915_suspend.c */
extern int i915_save_state(struct drm_device *dev);
extern int i915_restore_state(struct drm_device *dev);

/* i915_sysfs.c */
void i915_setup_sysfs(struct drm_device *dev_priv);
void i915_teardown_sysfs(struct drm_device *dev_priv);

/* intel_i2c.c */
extern int intel_setup_gmbus(struct drm_device *dev);
extern void intel_teardown_gmbus(struct drm_device *dev);
extern bool intel_gmbus_is_valid_pin(struct drm_i915_private *dev_priv,
				     unsigned int pin);

extern struct i2c_adapter *
intel_gmbus_get_adapter(struct drm_i915_private *dev_priv, unsigned int pin);
extern void intel_gmbus_set_speed(struct i2c_adapter *adapter, int speed);
extern void intel_gmbus_force_bit(struct i2c_adapter *adapter, bool force_bit);
static inline bool intel_gmbus_is_forced_bit(struct i2c_adapter *adapter)
{
	return container_of(adapter, struct intel_gmbus, adapter)->force_bit;
}
extern void intel_i2c_reset(struct drm_device *dev);

/* intel_bios.c */
bool intel_bios_is_port_present(struct drm_i915_private *dev_priv, enum port port);

/* intel_opregion.c */
#ifdef CONFIG_ACPI
extern int intel_opregion_setup(struct drm_device *dev);
extern void intel_opregion_init(struct drm_device *dev);
extern void intel_opregion_fini(struct drm_device *dev);
extern void intel_opregion_asle_intr(struct drm_device *dev);
extern int intel_opregion_notify_encoder(struct intel_encoder *intel_encoder,
					 bool enable);
extern int intel_opregion_notify_adapter(struct drm_device *dev,
					 pci_power_t state);
#else
static inline int intel_opregion_setup(struct drm_device *dev) { return 0; }
static inline void intel_opregion_init(struct drm_device *dev) { return; }
static inline void intel_opregion_fini(struct drm_device *dev) { return; }
static inline void intel_opregion_asle_intr(struct drm_device *dev) { return; }
static inline int
intel_opregion_notify_encoder(struct intel_encoder *intel_encoder, bool enable)
{
	return 0;
}
static inline int
intel_opregion_notify_adapter(struct drm_device *dev, pci_power_t state)
{
	return 0;
}
#endif

/* intel_acpi.c */
#ifdef CONFIG_ACPI
extern void intel_register_dsm_handler(void);
extern void intel_unregister_dsm_handler(void);
#else
static inline void intel_register_dsm_handler(void) { return; }
static inline void intel_unregister_dsm_handler(void) { return; }
#endif /* CONFIG_ACPI */

/* modesetting */
extern void intel_modeset_init_hw(struct drm_device *dev);
extern void intel_modeset_init(struct drm_device *dev);
extern void intel_modeset_gem_init(struct drm_device *dev);
extern void intel_modeset_cleanup(struct drm_device *dev);
extern void intel_connector_unregister(struct intel_connector *);
extern int intel_modeset_vga_set_state(struct drm_device *dev, bool state);
extern void intel_display_resume(struct drm_device *dev);
extern void i915_redisable_vga(struct drm_device *dev);
extern void i915_redisable_vga_power_on(struct drm_device *dev);
extern bool ironlake_set_drps(struct drm_device *dev, u8 val);
extern void intel_init_pch_refclk(struct drm_device *dev);
extern void intel_set_rps(struct drm_device *dev, u8 val);
extern void intel_set_memory_cxsr(struct drm_i915_private *dev_priv,
				  bool enable);
extern void intel_detect_pch(struct drm_device *dev);
extern int intel_trans_dp_port_sel(struct drm_crtc *crtc);
extern int intel_enable_rc6(const struct drm_device *dev);

extern bool i915_semaphore_is_enabled(struct drm_device *dev);
int i915_reg_read_ioctl(struct drm_device *dev, void *data,
			struct drm_file *file);
int i915_get_reset_stats_ioctl(struct drm_device *dev, void *data,
			       struct drm_file *file);

/* overlay */
extern struct intel_overlay_error_state *intel_overlay_capture_error_state(struct drm_device *dev);
extern void intel_overlay_print_error_state(struct drm_i915_error_state_buf *e,
					    struct intel_overlay_error_state *error);

extern struct intel_display_error_state *intel_display_capture_error_state(struct drm_device *dev);
extern void intel_display_print_error_state(struct drm_i915_error_state_buf *e,
					    struct drm_device *dev,
					    struct intel_display_error_state *error);

int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u32 mbox, u32 *val);
int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u32 mbox, u32 val);

/* intel_sideband.c */
u32 vlv_punit_read(struct drm_i915_private *dev_priv, u32 addr);
void vlv_punit_write(struct drm_i915_private *dev_priv, u32 addr, u32 val);
u32 vlv_nc_read(struct drm_i915_private *dev_priv, u8 addr);
u32 vlv_gpio_nc_read(struct drm_i915_private *dev_priv, u32 reg);
void vlv_gpio_nc_write(struct drm_i915_private *dev_priv, u32 reg, u32 val);
u32 vlv_cck_read(struct drm_i915_private *dev_priv, u32 reg);
void vlv_cck_write(struct drm_i915_private *dev_priv, u32 reg, u32 val);
u32 vlv_ccu_read(struct drm_i915_private *dev_priv, u32 reg);
void vlv_ccu_write(struct drm_i915_private *dev_priv, u32 reg, u32 val);
u32 vlv_bunit_read(struct drm_i915_private *dev_priv, u32 reg);
void vlv_bunit_write(struct drm_i915_private *dev_priv, u32 reg, u32 val);
u32 vlv_gps_core_read(struct drm_i915_private *dev_priv, u32 reg);
void vlv_gps_core_write(struct drm_i915_private *dev_priv, u32 reg, u32 val);
u32 vlv_dpio_read(struct drm_i915_private *dev_priv, enum pipe pipe, int reg);
void vlv_dpio_write(struct drm_i915_private *dev_priv, enum pipe pipe, int reg, u32 val);
u32 intel_sbi_read(struct drm_i915_private *dev_priv, u16 reg,
		   enum intel_sbi_destination destination);
void intel_sbi_write(struct drm_i915_private *dev_priv, u16 reg, u32 value,
		     enum intel_sbi_destination destination);
u32 vlv_flisdsi_read(struct drm_i915_private *dev_priv, u32 reg);
void vlv_flisdsi_write(struct drm_i915_private *dev_priv, u32 reg, u32 val);

int intel_gpu_freq(struct drm_i915_private *dev_priv, int val);
int intel_freq_opcode(struct drm_i915_private *dev_priv, int val);

#define I915_READ8(reg)		dev_priv->uncore.funcs.mmio_readb(dev_priv, (reg), true)
#define I915_WRITE8(reg, val)	dev_priv->uncore.funcs.mmio_writeb(dev_priv, (reg), (val), true)

#define I915_READ16(reg)	dev_priv->uncore.funcs.mmio_readw(dev_priv, (reg), true)
#define I915_WRITE16(reg, val)	dev_priv->uncore.funcs.mmio_writew(dev_priv, (reg), (val), true)
#define I915_READ16_NOTRACE(reg)	dev_priv->uncore.funcs.mmio_readw(dev_priv, (reg), false)
#define I915_WRITE16_NOTRACE(reg, val)	dev_priv->uncore.funcs.mmio_writew(dev_priv, (reg), (val), false)

#define I915_READ(reg)		dev_priv->uncore.funcs.mmio_readl(dev_priv, (reg), true)
#define I915_WRITE(reg, val)	dev_priv->uncore.funcs.mmio_writel(dev_priv, (reg), (val), true)
#define I915_READ_NOTRACE(reg)		dev_priv->uncore.funcs.mmio_readl(dev_priv, (reg), false)
#define I915_WRITE_NOTRACE(reg, val)	dev_priv->uncore.funcs.mmio_writel(dev_priv, (reg), (val), false)

/* Be very careful with read/write 64-bit values. On 32-bit machines, they
 * will be implemented using 2 32-bit writes in an arbitrary order with
 * an arbitrary delay between them. This can cause the hardware to
 * act upon the intermediate value, possibly leading to corruption and
 * machine death. You have been warned.
 */
#define I915_WRITE64(reg, val)	dev_priv->uncore.funcs.mmio_writeq(dev_priv, (reg), (val), true)
#define I915_READ64(reg)	dev_priv->uncore.funcs.mmio_readq(dev_priv, (reg), true)

#define I915_READ64_2x32(lower_reg, upper_reg) ({			\
	u32 upper, lower, old_upper, loop = 0;				\
	upper = I915_READ(upper_reg);					\
	do {								\
		old_upper = upper;					\
		lower = I915_READ(lower_reg);				\
		upper = I915_READ(upper_reg);				\
	} while (upper != old_upper && loop++ < 2);			\
	(u64)upper << 32 | lower; })

#define POSTING_READ(reg)	(void)I915_READ_NOTRACE(reg)
#define POSTING_READ16(reg)	(void)I915_READ16_NOTRACE(reg)

/* These are untraced mmio-accessors that are only valid to be used inside
 * criticial sections inside IRQ handlers where forcewake is explicitly
 * controlled.
 * Think twice, and think again, before using these.
 * Note: Should only be used between intel_uncore_forcewake_irqlock() and
 * intel_uncore_forcewake_irqunlock().
 */
#ifdef __linux__
#define I915_READ_FW(reg__) readl(dev_priv->regs + (reg__))
#define I915_WRITE_FW(reg__, val__) writel(val__, dev_priv->regs + (reg__))
#else
#define I915_READ_FW(reg__) bus_space_read_4(dev_priv->regs->bst, dev_priv->regs->bsh, (reg__))
#define I915_WRITE_FW(reg__, val__) bus_space_write_4(dev_priv->regs->bst, dev_priv->regs->bsh, (reg__), (val__))
#endif
#define POSTING_READ_FW(reg__) (void)I915_READ_FW(reg__)

/* "Broadcast RGB" property */
#define INTEL_BROADCAST_RGB_AUTO 0
#define INTEL_BROADCAST_RGB_FULL 1
#define INTEL_BROADCAST_RGB_LIMITED 2

static inline uint32_t i915_vgacntrl_reg(struct drm_device *dev)
{
	if (IS_VALLEYVIEW(dev))
		return VLV_VGACNTRL;
	else if (INTEL_INFO(dev)->gen >= 5)
		return CPU_VGACNTRL;
	else
		return VGACNTRL;
}

static inline void __user *to_user_ptr(u64 address)
{
	return (void __user *)(uintptr_t)address;
}

static inline unsigned long msecs_to_jiffies_timeout(const unsigned int m)
{
	unsigned long j = msecs_to_jiffies(m);

	return min_t(unsigned long, MAX_JIFFY_OFFSET, j + 1);
}

static inline unsigned long nsecs_to_jiffies_timeout(const u64 n)
{
        return min_t(u64, MAX_JIFFY_OFFSET, nsecs_to_jiffies64(n) + 1);
}

static inline unsigned long
timespec_to_jiffies_timeout(const struct timespec *value)
{
	unsigned long j = timespec_to_jiffies(value);

	return min_t(unsigned long, MAX_JIFFY_OFFSET, j + 1);
}

/*
 * If you need to wait X milliseconds between events A and B, but event B
 * doesn't happen exactly after event A, you record the timestamp (jiffies) of
 * when event A happened, then just before event B you call this function and
 * pass the timestamp as the first argument, and X as the second argument.
 */
#ifdef __linux__
static inline void
wait_remaining_ms_from_jiffies(unsigned long timestamp_jiffies, int to_wait_ms)
{
	unsigned long target_jiffies, tmp_jiffies, remaining_jiffies;

	/*
	 * Don't re-read the value of "jiffies" every time since it may change
	 * behind our back and break the math.
	 */
	tmp_jiffies = jiffies;
	target_jiffies = timestamp_jiffies +
			 msecs_to_jiffies_timeout(to_wait_ms);

	if (time_after(target_jiffies, tmp_jiffies)) {
		remaining_jiffies = target_jiffies - tmp_jiffies;
		while (remaining_jiffies)
			remaining_jiffies =
			    schedule_timeout_uninterruptible(remaining_jiffies);
	}
}
#else
static inline void
wait_remaining_ms_from_jiffies(unsigned long timestamp_jiffies, int to_wait_ms)
{
	unsigned long target_jiffies, tmp_jiffies, remaining_jiffies;

	if (cold) {
		delay(to_wait_ms * 1000);
		return;
	}

	/*
	 * Don't re-read the value of "jiffies" every time since it may change
	 * behind our back and break the math.
	 */
	tmp_jiffies = jiffies;
	target_jiffies = timestamp_jiffies +
			 msecs_to_jiffies_timeout(to_wait_ms);

	while (time_after(target_jiffies, tmp_jiffies)) {
		remaining_jiffies = target_jiffies - tmp_jiffies;
		tsleep(&tmp_jiffies, PWAIT, "wrmfj", remaining_jiffies);
		tmp_jiffies = jiffies;
	}
}
#endif

static inline void i915_trace_irq_get(struct intel_engine_cs *ring,
				      struct drm_i915_gem_request *req)
{
	if (ring->trace_irq_req == NULL && ring->irq_get(ring))
		i915_gem_request_assign(&ring->trace_irq_req, req);
}

#endif
@


1.77
log
@Implement drm_pci_alloc() and drm_pci_free() and use them to reduce the
diffs with Linux.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.76 2017/07/05 20:30:13 kettenis Exp $ */
d852 2
d1081 1
d2571 10
d2607 2
a2608 1
#define IS_BROXTON(dev)	(!INTEL_INFO(dev)->is_skylake && IS_GEN9(dev))
d2636 8
d2658 2
d2661 1
d2665 10
d2745 1
a2745 1
				 IS_SKYLAKE(dev))
d2748 1
a2748 1
				 IS_SKYLAKE(dev))
d2754 2
a2755 2
#define HAS_GUC_UCODE(dev)	(IS_GEN9(dev))
#define HAS_GUC_SCHED(dev)	(IS_GEN9(dev))
d2771 1
d2776 1
@


1.76
log
@Fix native/raw backlight support in inteldrm(4).
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.75 2017/07/01 16:14:10 kettenis Exp $ */
d1852 1
a1852 1
	struct drm_dmamem *status_page_dmah;
@


1.75
log
@Update inteldrm(4) to code based on Linux 4.4.70.  This brings us support for
Skylake and Cherryview and better support for Broadwell and Valleyview.  Also
adds MST support.  Some tweaks to the TTM code and radeondrm(4) to keep it
working with the updated generic DRM code needed for inteldrm(4).

Tested by many.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.74 2016/04/05 21:22:02 kettenis Exp $ */
d1820 1
a1820 8
	struct backlight_device {
		struct intel_connector *connector;
		struct {
			uint32_t brightness;
			uint32_t max_brightness;
			uint32_t power;
		} props;
	} backlight;
@


1.74
log
@Use the "idr" API here as well, reducing the diffs to Linux.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.73 2015/12/09 05:17:44 jsg Exp $ */
d34 3
a37 1
#include <dev/pci/drm/i915_drm.h>
d40 4
a45 1
#define i2c_adapter i2c_controller
d69 1
d72 4
a75 1
extern void intel_gtt_get(size_t *, size_t *, phys_addr_t *, unsigned long *);
d120 6
d142 14
a175 2
#define DRIVER_AUTHOR		"Tungsten Graphics, Inc."

d178 12
a189 1
#define DRIVER_DATE		"20080730"
d196 2
a197 1
	I915_MAX_PIPES
d205 2
a206 1
	TRANSCODER_EDP = 0xF,
d210 6
d220 2
d225 1
a225 1
#define sprite_name(p, s) ((p) * dev_priv->num_plane + (s) + 'A')
d237 1
a237 1
#define I915_NUM_PHYS_VLV 1
d260 12
d274 6
a284 2
#define POWER_DOMAIN_MASK (BIT(POWER_DOMAIN_NUM) - 1)

a291 8
#define HSW_ALWAYS_ON_POWER_DOMAINS (		\
	BIT(POWER_DOMAIN_PIPE_A) |		\
	BIT(POWER_DOMAIN_TRANSCODER_EDP))
#define BDW_ALWAYS_ON_POWER_DOMAINS (		\
	BIT(POWER_DOMAIN_PIPE_A) |		\
	BIT(POWER_DOMAIN_TRANSCODER_EDP) |	\
	BIT(POWER_DOMAIN_PIPE_A_PANEL_FITTER))

a293 1
	HPD_PORT_A = HPD_NONE, /* PORT_A is internal */
d298 1
d302 1
d306 33
d346 37
a382 1
#define for_each_pipe(p) for ((p) = 0; (p) < INTEL_INFO(dev)->num_pipes; (p)++)
d388 8
d398 26
d428 11
a438 2
	DPLL_ID_PCH_PLL_A,
	DPLL_ID_PCH_PLL_B,
d440 1
a440 1
#define I915_NUM_PLLS 2
d443 1
d448 24
d475 2
a476 1
	int refcount; /* count of number of CRTCs sharing this PLL */
d482 2
a483 1
	struct intel_dpll_hw_state hw_state;
d495 5
a512 6
struct intel_ddi_plls {
	int spll_refcount;
	int wrpll1_refcount;
	int wrpll2_refcount;
};

a527 15
#define WATCH_GTT	0

#define I915_GEM_PHYS_CURSOR_0 1
#define I915_GEM_PHYS_CURSOR_1 2
#define I915_GEM_PHYS_OVERLAY_REGS 3
#define I915_MAX_PHYS_OBJECT (I915_GEM_PHYS_OVERLAY_REGS)

struct drm_i915_gem_phys_object {
	int id;
#ifdef __linux__
	struct page **page_list;
#endif
	drm_dma_handle_t *handle;
	struct drm_i915_gem_object *cur_obj;
};
d535 3
a537 3
	struct opregion_header __iomem *header;
	struct opregion_acpi __iomem *acpi;
	struct opregion_swsci __iomem *swsci;
d540 3
a542 3
	struct opregion_asle __iomem *asle;
	void __iomem *vbt;
	u32 __iomem *lid_state;
a549 6
#ifdef __linux__
struct drm_i915_master_private {
	drm_local_map_t *sarea;
	struct _drm_i915_sarea *sarea_priv;
};
#endif
d574 8
d585 1
a588 15
	bool waiting[I915_NUM_RINGS];
	u32 pipestat[I915_MAX_PIPES];
	u32 tail[I915_NUM_RINGS];
	u32 head[I915_NUM_RINGS];
	u32 ctl[I915_NUM_RINGS];
	u32 ipeir[I915_NUM_RINGS];
	u32 ipehr[I915_NUM_RINGS];
	u32 instdone[I915_NUM_RINGS];
	u32 acthd[I915_NUM_RINGS];
	u32 semaphore_mboxes[I915_NUM_RINGS][I915_NUM_RINGS - 1];
	u32 semaphore_seqno[I915_NUM_RINGS][I915_NUM_RINGS - 1];
	u32 rc_psmi[I915_NUM_RINGS]; /* sleep state */
	/* our own tracking of ring head and tail */
	u32 cpu_ring_head[I915_NUM_RINGS];
	u32 cpu_ring_tail[I915_NUM_RINGS];
d591 7
a597 3
	u32 bbstate[I915_NUM_RINGS];
	u32 instpm[I915_NUM_RINGS];
	u32 instps[I915_NUM_RINGS];
a598 5
	u32 seqno[I915_NUM_RINGS];
	u64 bbaddr[I915_NUM_RINGS];
	u32 fault_reg[I915_NUM_RINGS];
	u32 done_reg;
	u32 faddr[I915_NUM_RINGS];
d600 4
a603 1
	struct timeval time;
d606 32
d640 1
a640 1
			u32 gtt_offset;
d642 2
a643 1
		} *ringbuffer, *batchbuffer, *ctx;
d649 11
a659 1
		int num_requests;
d661 1
d665 2
a666 2
		u32 rseqno, wseqno;
		u32 gtt_offset;
d674 1
d678 1
d680 1
a680 4
	struct intel_overlay_error_state *overlay;
	struct intel_display_error_state *display;
	int hangcheck_score[I915_NUM_RINGS];
	enum intel_ring_hangcheck_action hangcheck_action[I915_NUM_RINGS];
d684 3
a686 1
struct intel_crtc_config;
a691 3
	bool (*fbc_enabled)(struct drm_device *dev);
	void (*enable_fbc)(struct drm_crtc *crtc);
	void (*disable_fbc)(struct drm_device *dev);
d708 1
a708 1
			  struct drm_crtc *crtc,
d715 4
a718 3
				 uint32_t sprite_width, int pixel_size,
				 bool enable, bool scaled);
	void (*modeset_global_resources)(struct drm_device *dev);
d722 5
a726 4
				struct intel_crtc_config *);
	int (*crtc_mode_set)(struct drm_crtc *crtc,
			     int x, int y,
			     struct drm_framebuffer *old_fb);
d729 4
a732 4
	void (*off)(struct drm_crtc *crtc);
	void (*write_eld)(struct drm_connector *connector,
			  struct drm_crtc *crtc,
			  struct drm_display_mode *mode);
d738 1
d740 3
a742 2
	int (*update_plane)(struct drm_crtc *crtc, struct drm_framebuffer *fb,
			    int x, int y);
d749 1
d751 15
a765 6
	int (*setup_backlight)(struct intel_connector *connector);
	uint32_t (*get_backlight)(struct intel_connector *connector);
	void (*set_backlight)(struct intel_connector *connector,
			      uint32_t level);
	void (*disable_backlight)(struct intel_connector *connector);
	void (*enable_backlight)(struct intel_connector *connector);
d770 1
a770 1
							int fw_engine);
d772 1
a772 1
							int fw_engine);
d795 1
a795 1
	unsigned forcewake_count;
d797 38
a834 4
	unsigned fw_rendercount;
	unsigned fw_mediacount;

	struct delayed_work force_wake_work;
d851 1
d869 1
d871 1
d875 17
a906 133
typedef uint32_t gen6_gtt_pte_t;

struct i915_address_space {
	struct drm_mm mm;
	struct drm_device *dev;
	struct list_head global_link;
	unsigned long start;		/* Start offset always 0 for dri2 */
	size_t total;		/* size addr space maps (ex. 2GB for ggtt) */

	struct {
		dma_addr_t addr;
		struct drm_dmamem *page;
	} scratch;

	/**
	 * List of objects currently involved in rendering.
	 *
	 * Includes buffers having the contents of their GPU caches
	 * flushed, not necessarily primitives.  last_rendering_seqno
	 * represents when the rendering involved will be completed.
	 *
	 * A reference is held on the buffer while on this list.
	 */
	struct list_head active_list;

	/**
	 * LRU list of objects which are not in the ringbuffer and
	 * are ready to unbind, but are still in the GTT.
	 *
	 * last_rendering_seqno is 0 while an object is in this list.
	 *
	 * A reference is not held on the buffer while on this list,
	 * as merely being GTT-bound shouldn't prevent its being
	 * freed, and we'll pull it off the list in the free path.
	 */
	struct list_head inactive_list;

	/* FIXME: Need a more generic return type */
	gen6_gtt_pte_t (*pte_encode)(dma_addr_t addr,
				     enum i915_cache_level level,
				     bool valid); /* Create a valid PTE */
	void (*clear_range)(struct i915_address_space *vm,
			    unsigned int first_entry,
			    unsigned int num_entries,
			    bool use_scratch);
	void (*insert_entries)(struct i915_address_space *vm,
			       struct vm_page **pages,
			       unsigned int num_entries,
			       unsigned int first_entry,
			       enum i915_cache_level cache_level);
	void (*cleanup)(struct i915_address_space *vm);
};

/* The Graphics Translation Table is the way in which GEN hardware translates a
 * Graphics Virtual Address into a Physical Address. In addition to the normal
 * collateral associated with any va->pa translations GEN hardware also has a
 * portion of the GTT which can be mapped by the CPU and remain both coherent
 * and correct (in cases like swizzling). That region is referred to as GMADR in
 * the spec.
 */
struct i915_gtt {
	struct i915_address_space base;
	size_t stolen_size;		/* Total size of stolen memory */

	unsigned long mappable_end;	/* End offset that we can CPU map */
	struct io_mapping *mappable;	/* Mapping to our CPU mappable region */
	phys_addr_t mappable_base;	/* PA of our GMADR */

	/** "Graphics Stolen Memory" holds the global PTEs */
	void __iomem *gsm;

	bool do_idle_maps;

	int mtrr;

	/* global gtt ops */
	int (*gtt_probe)(struct drm_device *dev, size_t *gtt_total,
			  size_t *stolen, phys_addr_t *mappable_base,
			  unsigned long *mappable_end);
};
#define gtt_total_entries(gtt) ((gtt).base.total >> PAGE_SHIFT)

struct i915_hw_ppgtt {
	struct i915_address_space base;
	unsigned num_pd_entries;
	union {
		struct vm_page **pt_pages;
		struct vm_page *gen8_pt_pages;
	};
	struct vm_page *pd_pages;
	int num_pd_pages;
	int num_pt_pages;
	union {
		uint32_t pd_offset;
		dma_addr_t pd_dma_addr[4];
	};
	union {
		dma_addr_t *pt_dma_addr;
		dma_addr_t *gen8_pt_dma_addr[4];
	};
	int (*enable)(struct drm_device *dev);
};

/**
 * A VMA represents a GEM BO that is bound into an address space. Therefore, a
 * VMA's presence cannot be guaranteed before binding, or after unbinding the
 * object into/from the address space.
 *
 * To make things as simple as possible (ie. no refcounting), a VMA's lifetime
 * will always be <= an objects lifetime. So object refcounting should cover us.
 */
struct i915_vma {
	struct drm_mm_node node;
	struct drm_i915_gem_object *obj;
	struct i915_address_space *vm;

	/** This object's place on the active/inactive lists */
	struct list_head mm_list;

	struct list_head vma_link; /* Link in the object's VMA list */

	/** This vma's place in the batchbuffer or on the eviction list */
	struct list_head exec_list;

	/**
	 * Used for performing relocations during execbuffer insertion.
	 */
	struct hlist_node exec_node;
	unsigned long exec_handle;
	struct drm_i915_gem_exec_object2 *exec_entry;

};

d917 5
d927 23
a949 2
#define DEFAULT_CONTEXT_ID 0
struct i915_hw_context {
d951 1
a951 2
	int id;
	bool is_initialized;
d953 2
a955 2
	struct intel_ring_buffer *ring;
	struct drm_i915_gem_object *obj;
d957 14
d975 8
d984 5
a988 1
	unsigned long size;
d990 3
a992 1
	enum plane plane;
d995 1
a995 1
	struct drm_mm_node *compressed_fb;
d998 6
d1006 1
a1006 1
		struct drm_crtc *crtc;
d1022 5
d1028 31
d1062 1
d1065 6
d1078 1
d1090 3
a1092 1
#define QUIRK_NO_PCH_PWM_ENABLE (1<<3)
d1098 1
a1098 3
	struct i2c_controller controller;
	u32 port;
	u32 speed;
d1102 3
a1108 3
	u8 saveLBB;
	u32 saveDSPACNTR;
	u32 saveDSPBCNTR;
a1109 66
	u32 savePIPEACONF;
	u32 savePIPEBCONF;
	u32 savePIPEASRC;
	u32 savePIPEBSRC;
	u32 saveFPA0;
	u32 saveFPA1;
	u32 saveDPLL_A;
	u32 saveDPLL_A_MD;
	u32 saveHTOTAL_A;
	u32 saveHBLANK_A;
	u32 saveHSYNC_A;
	u32 saveVTOTAL_A;
	u32 saveVBLANK_A;
	u32 saveVSYNC_A;
	u32 saveBCLRPAT_A;
	u32 saveTRANSACONF;
	u32 saveTRANS_HTOTAL_A;
	u32 saveTRANS_HBLANK_A;
	u32 saveTRANS_HSYNC_A;
	u32 saveTRANS_VTOTAL_A;
	u32 saveTRANS_VBLANK_A;
	u32 saveTRANS_VSYNC_A;
	u32 savePIPEASTAT;
	u32 saveDSPASTRIDE;
	u32 saveDSPASIZE;
	u32 saveDSPAPOS;
	u32 saveDSPAADDR;
	u32 saveDSPASURF;
	u32 saveDSPATILEOFF;
	u32 savePFIT_PGM_RATIOS;
	u32 saveBLC_HIST_CTL;
	u32 saveBLC_PWM_CTL;
	u32 saveBLC_PWM_CTL2;
	u32 saveBLC_HIST_CTL_B;
	u32 saveBLC_CPU_PWM_CTL;
	u32 saveBLC_CPU_PWM_CTL2;
	u32 saveFPB0;
	u32 saveFPB1;
	u32 saveDPLL_B;
	u32 saveDPLL_B_MD;
	u32 saveHTOTAL_B;
	u32 saveHBLANK_B;
	u32 saveHSYNC_B;
	u32 saveVTOTAL_B;
	u32 saveVBLANK_B;
	u32 saveVSYNC_B;
	u32 saveBCLRPAT_B;
	u32 saveTRANSBCONF;
	u32 saveTRANS_HTOTAL_B;
	u32 saveTRANS_HBLANK_B;
	u32 saveTRANS_HSYNC_B;
	u32 saveTRANS_VTOTAL_B;
	u32 saveTRANS_VBLANK_B;
	u32 saveTRANS_VSYNC_B;
	u32 savePIPEBSTAT;
	u32 saveDSPBSTRIDE;
	u32 saveDSPBSIZE;
	u32 saveDSPBPOS;
	u32 saveDSPBADDR;
	u32 saveDSPBSURF;
	u32 saveDSPBTILEOFF;
	u32 saveVGA0;
	u32 saveVGA1;
	u32 saveVGA_PD;
	u32 saveVGACNTRL;
	u32 saveADPA;
a1112 3
	u32 saveDVOA;
	u32 saveDVOB;
	u32 saveDVOC;
a1116 6
	u32 savePFIT_CONTROL;
	u32 save_palette_a[256];
	u32 save_palette_b[256];
	u32 saveDPFC_CB_BASE;
	u32 saveFBC_CFB_BASE;
	u32 saveFBC_LL_BASE;
a1117 10
	u32 saveFBC_CONTROL2;
	u32 saveIER;
	u32 saveIIR;
	u32 saveIMR;
	u32 saveDEIER;
	u32 saveDEIMR;
	u32 saveGTIER;
	u32 saveGTIMR;
	u32 saveFDI_RXA_IMR;
	u32 saveFDI_RXB_IMR;
d1122 1
a1122 8
	u32 saveSWF2[3];
	u8 saveMSR;
	u8 saveSR[8];
	u8 saveGR[25];
	u8 saveAR_INDEX;
	u8 saveAR[21];
	u8 saveDACMASK;
	u8 saveCR[37];
a1123 39
	u32 saveCURACNTR;
	u32 saveCURAPOS;
	u32 saveCURABASE;
	u32 saveCURBCNTR;
	u32 saveCURBPOS;
	u32 saveCURBBASE;
	u32 saveCURSIZE;
	u32 saveDP_B;
	u32 saveDP_C;
	u32 saveDP_D;
	u32 savePIPEA_GMCH_DATA_M;
	u32 savePIPEB_GMCH_DATA_M;
	u32 savePIPEA_GMCH_DATA_N;
	u32 savePIPEB_GMCH_DATA_N;
	u32 savePIPEA_DP_LINK_M;
	u32 savePIPEB_DP_LINK_M;
	u32 savePIPEA_DP_LINK_N;
	u32 savePIPEB_DP_LINK_N;
	u32 saveFDI_RXA_CTL;
	u32 saveFDI_TXA_CTL;
	u32 saveFDI_RXB_CTL;
	u32 saveFDI_TXB_CTL;
	u32 savePFA_CTL_1;
	u32 savePFB_CTL_1;
	u32 savePFA_WIN_SZ;
	u32 savePFB_WIN_SZ;
	u32 savePFA_WIN_POS;
	u32 savePFB_WIN_POS;
	u32 savePCH_DREF_CONTROL;
	u32 saveDISP_ARB_CTL;
	u32 savePIPEA_DATA_M1;
	u32 savePIPEA_DATA_N1;
	u32 savePIPEA_LINK_M1;
	u32 savePIPEA_LINK_N1;
	u32 savePIPEB_DATA_M1;
	u32 savePIPEB_DATA_N1;
	u32 savePIPEB_LINK_M1;
	u32 savePIPEB_LINK_N1;
	u32 saveMCHBAR_RENDER_STANDBY;
d1125 69
d1197 4
a1200 1
	/* work and pm_iir are protected by dev_priv->irq_lock */
d1202 1
d1205 22
a1226 9
	/* The below variables an all the rps hw state are protected by
	 * dev->struct mutext. */
	u8 cur_delay;
	u8 min_delay;
	u8 max_delay;
	u8 rpe_delay;
	u8 rp1_delay;
	u8 rp0_delay;
	u8 hw_max;
d1231 4
d1237 6
d1246 3
a1248 1
	 * Must be taken after struct_mutex if nested.
d1267 1
a1267 1
	struct timespec last_time2;
d1273 4
d1278 25
a1302 2
	struct drm_i915_gem_object *pwrctx;
	struct drm_i915_gem_object *renderctx;
d1311 2
d1314 2
a1315 5
	void *data;
	void (*set)(struct drm_device *dev, struct i915_power_well *power_well,
		    bool enable);
	bool (*is_enabled)(struct drm_device *dev,
			   struct i915_power_well *power_well);
d1324 1
a1331 25
struct i915_dri1_state {
	unsigned allow_batchbuffer : 1;
	u32 __iomem *gfx_hws_cpu_addr;

	unsigned int cpp;
	int back_offset;
	int front_offset;
	int current_page;
	int page_flipping;

	uint32_t counter;
};

struct i915_ums_state {
	/**
	 * Flag if the X Server, and thus DRM, is not currently in
	 * control of the device.
	 *
	 * This is set between LeaveVT and EnterVT.  It needs to be
	 * replaced with a semaphore.  It also needs to be
	 * transitioned away from for kernel modesetting.
	 */
	int mm_suspended;
};

d1342 4
d1362 3
a1364 1
//	struct shrinker inactive_shrinker;
d1366 1
d1395 11
a1410 3
	/* storage for physical objects */
	struct drm_i915_gem_phys_object *phys_objs[I915_MAX_PHYS_OBJECT];

d1418 1
d1439 2
a1440 1
	struct timeout hangcheck_timer;
a1445 2
	struct work_struct work;

d1481 6
a1486 2
	/* For gpu hang simulation. */
	unsigned int stop_rings;
d1490 3
d1501 9
d1511 6
d1522 13
d1549 1
d1553 2
d1566 9
d1576 1
d1578 1
d1581 1
d1584 1
d1586 6
d1593 1
d1625 61
d1687 4
a1690 11
 * This struct tracks the state needed for the Package C8+ feature.
 *
 * Package states C8 and deeper are really deep PC states that can only be
 * reached when all the devices on the system allow it, so even if the graphics
 * device allows PC8+, it doesn't mean the system will actually get to these
 * states.
 *
 * Our driver only allows PC8+ when all the outputs are disabled, the power well
 * is disabled and the GPU is idle. When these conditions are met, we manually
 * do the other conditions: disable the interrupts, clocks and switch LCPLL
 * refclk to Fclk.
d1692 3
a1694 4
 * When we really reach PC8 or deeper states (not just when we allow it) we lose
 * the state of some registers, so when we come back from PC8+ we need to
 * restore this state. We don't get into PC8+ if we're not in RC6, so we don't
 * need to take care of the registers kept by RC6.
d1696 4
a1699 21
 * The interrupt disabling is part of the requirements. We can only leave the
 * PCH HPD interrupts enabled. If we're in PC8+ and we get another interrupt we
 * can lock the machine.
 *
 * Ideally every piece of our code that needs PC8+ disabled would call
 * hsw_disable_package_c8, which would increment disable_count and prevent the
 * system from reaching PC8+. But we don't have a symmetric way to do this for
 * everything, so we have the requirements_met and gpu_idle variables. When we
 * switch requirements_met or gpu_idle to true we decrease disable_count, and
 * increase it in the opposite case. The requirements_met variable is true when
 * all the CRTCs, encoders and the power well are disabled. The gpu_idle
 * variable is true when the GPU is idle.
 *
 * In addition to everything, we only actually enable PC8+ if disable_count
 * stays at zero for at least some seconds. This is implemented with the
 * enable_work variable. We do this so we don't enable/disable PC8 dozens of
 * consecutive times when all screens are disabled and some background app
 * queries the state of our connectors, or we have some application constantly
 * waking up to use the GPU. Only after the enable_work function actually
 * enables PC8+ the "enable" variable will become true, which means that it can
 * be false even if disable_count is 0.
d1705 1
a1705 3
 * case it happens, but if it actually happens we'll also update the variables
 * inside struct regsave so when we restore the IRQs they will contain the
 * latest expected values.
d1707 1
a1707 1
 * For more, read "Display Sequences for Package C8" on our documentation.
a1708 19
struct i915_package_c8 {
	bool requirements_met;
	bool gpu_idle;
	bool irqs_disabled;
	/* Only true after the delayed work task actually enables it. */
	bool enabled;
	int disable_count;
	struct rwlock lock;
	struct delayed_work enable_work;

	struct {
		uint32_t deimr;
		uint32_t sdeimr;
		uint32_t gtimr;
		uint32_t gtier;
		uint32_t gen6_pmimr;
	} regsave;
};

d1711 1
d1744 42
a1785 1
typedef struct inteldrm_softc {
a1786 1
	struct drm_device *dev;
d1792 6
a1797 1
	const struct intel_device_info *info;
d1818 1
a1818 1
	int burner_dpms_mode;
d1825 1
d1831 6
a1836 1
	struct intel_gmbus gmbus[GMBUS_NUM_PORTS];
d1838 1
d1842 1
a1842 1
	struct mutex gmbus_mutex;
d1849 3
d1855 2
a1856 1
	struct intel_ring_buffer ring[I915_NUM_RINGS];
d1859 1
a1859 1
	drm_dma_handle_t *status_page_dmah;
a1872 2
	atomic_t irq_received;

d1876 5
d1886 2
a1887 2
	/* DPIO indirect register protection */
	struct rwlock dpio_lock;
d1896 2
d1899 1
a1899 16
	struct work_struct hotplug_work;
	bool enable_hotplug_processing;
	struct {
		unsigned long hpd_last_jiffies;
		int hpd_cnt;
		enum {
			HPD_ENABLED = 0,
			HPD_DISABLED = 1,
			HPD_MARK_DISABLED = 2
		} hpd_mark;
	} hpd_stats[HPD_NUM_PINS];
	u32 hpd_event_bits;
	struct timeout hotplug_reenable_timer;

	int num_plane;

d1901 1
d1905 2
d1911 1
a1911 1
	spinlock_t backlight_lock;
d1916 3
a1919 1
	int fence_reg_start; /* 4 if userland hasn't ioctl'd us yet */
d1923 5
d1951 1
a1951 1
	struct i915_gtt gtt; /* VMA representing the global address space */
d1954 2
d1961 2
a1962 2
	struct drm_crtc *plane_to_crtc_mapping[3];
	struct drm_crtc *pipe_to_crtc_mapping[3];
a1970 1
	struct intel_ddi_plls ddi_plls;
d1973 2
d1977 3
a1979 3
	bool lvds_downclock_avail;
	/* indicates the reduced downclock for LVDS*/
	int lvds_downclock;
d2004 1
a2004 1
#ifdef CONFIG_DRM_I915_FBDEV
d2007 1
a2009 6
	/*
	 * The console may be contended at resume, but we don't
	 * want it to block on it.
	 */
	struct work_struct console_resume_work;

d2013 9
d2027 3
d2031 1
d2045 13
d2060 7
a2066 1
		struct ilk_wm_values hw;
d2069 13
a2081 1
	struct i915_package_c8 pc8;
d2083 2
a2084 1
	struct i915_runtime_pm pm;
d2086 5
a2090 6
	/* Old dri1 support infrastructure, beware the dragons ya fools entering
	 * here! */
	struct i915_dri1_state dri1;
	/* Old ums support infrastructure, same warning applies. */
	struct i915_ums_state ums;
} drm_i915_private_t;
d2097 12
d2139 2
d2143 23
d2178 1
a2178 1
	struct list_head ring_list;
d2182 2
d2189 1
a2189 1
	unsigned int active:1;
a2221 12
	/** How many users have pinned this object in GTT space. The following
	 * users can each hold at most one reference: pwrite/pread, pin_ioctl
	 * (via user_pin_count), execbuffer (objects are not allowed multiple
	 * times for the same batchbuffer), and the framebuffer code. When
	 * switching/pageflipping, the framebuffer code has at most two buffers
	 * pinned per crtc.
	 *
	 * In the worst case this is 1 + 1 + 1 + 2*2 = 7. That would fit into 3
	 * bits with absolutely no headroom. So use 4 bits. */
	unsigned int pin_count:4;
#define DRM_I915_GEM_OBJECT_MAX_PIN_COUNT 0xf

a2233 2
	unsigned int pin_mappable:1;
	unsigned int pin_display:1;
d2236 2
a2237 1
	 * Is the GPU currently using a fence to access this buffer,
d2239 3
a2241 2
	unsigned int pending_fenced_gpu_access:1;
	unsigned int fenced_gpu_access:1;
d2243 1
a2243 1
	unsigned int cache_level:3;
d2245 1
a2245 3
	unsigned int has_aliasing_ppgtt_mapping:1;
	unsigned int has_global_gtt_mapping:1;
	unsigned int has_dma_mapping:1;
a2246 1
#ifdef __linux__
a2247 3
#else
	struct vm_page **pages;
#endif
d2249 4
d2258 12
a2269 5
	struct intel_ring_buffer *ring;

	/** Breadcrumb of last rendering to the buffer. */
	uint32_t last_read_seqno;
	uint32_t last_write_seqno;
d2271 1
a2271 1
	uint32_t last_fenced_seqno;
d2282 10
a2291 3
	/** User space pin count and filp owning the pin */
	unsigned long user_pin_count;
	struct drm_file *pin_filp;
d2293 2
a2294 2
	/** for phy allocated objects */
	struct drm_i915_gem_phys_object *phys_obj;
d2296 1
a2296 1
#define to_gem_object(obj) (&((struct drm_i915_gem_object *)(obj))->base)
d2298 3
a2300 1
#define to_intel_bo(x) container_of(x, struct drm_i915_gem_object, base)
d2308 7
a2314 3
 * By keeping this list, we can avoid having to do questionable
 * sequence-number comparisons on buffer last_rendering_seqnos, and associate
 * an emission time with seqnos for tracking how far ahead of the GPU we are.
d2317 2
d2320 2
a2321 1
	struct intel_ring_buffer *ring;
d2323 11
a2333 2
	/** GEM sequence number associated with this request. */
	uint32_t seqno;
d2338 8
a2345 1
	/** Position in the ringbuffer of the end of the request */
d2348 12
a2359 2
	/** Context related to this request */
	struct i915_hw_context *ctx;
d2361 2
a2362 1
	/** Batch buffer related to this request if any */
d2374 23
d2399 128
a2526 2
struct drm_i915_file_private {
	struct drm_i915_private *dev_priv;
d2528 12
d2541 7
a2547 5
		spinlock_t lock;
		struct list_head request_list;
		struct delayed_work idle_work;
	} mm;
	struct idr context_idr;
d2549 9
a2557 2
	struct i915_ctx_hang_stats hang_stats;
	atomic_t rps_wait_boost;
d2560 14
a2573 1
#define INTEL_INFO(dev)	(to_i915(dev)->info)
d2575 2
a2576 2
#define IS_I830(dev)		((dev)->pdev->device == 0x3577)
#define IS_845G(dev)		((dev)->pdev->device == 0x2562)
d2578 1
a2578 1
#define IS_I865G(dev)		((dev)->pdev->device == 0x2572)
d2580 2
a2581 2
#define IS_I915GM(dev)		((dev)->pdev->device == 0x2592)
#define IS_I945G(dev)		((dev)->pdev->device == 0x2772)
d2585 1
a2585 1
#define IS_GM45(dev)		((dev)->pdev->device == 0x2A42)
d2587 2
a2588 2
#define IS_PINEVIEW_G(dev)	((dev)->pdev->device == 0xa001)
#define IS_PINEVIEW_M(dev)	((dev)->pdev->device == 0xa011)
d2591 1
a2591 1
#define IS_IRONLAKE_M(dev)	((dev)->pdev->device == 0x0046)
d2593 3
a2595 6
#define IS_IVB_GT1(dev)		((dev)->pdev->device == 0x0156 || \
				 (dev)->pdev->device == 0x0152 || \
				 (dev)->pdev->device == 0x015a)
#define IS_SNB_GT1(dev)		((dev)->pdev->device == 0x0102 || \
				 (dev)->pdev->device == 0x0106 || \
				 (dev)->pdev->device == 0x010A)
d2597 1
d2599 3
a2601 1
#define IS_BROADWELL(dev)	(INTEL_INFO(dev)->gen == 8)
d2604 1
a2604 1
				 ((dev)->pdev->device & 0xFF00) == 0x0C00)
d2606 8
a2613 3
				 (((dev)->pdev->device & 0xf) == 0x6 || \
				 ((dev)->pdev->device & 0xf) == 0xb || \
				 ((dev)->pdev->device & 0xf) == 0xe))
d2615 1
a2615 2
				 ((dev)->pdev->device & 0xFF00) == 0x0A00)
#define IS_ULT(dev)		(IS_HSW_ULT(dev) || IS_BDW_ULT(dev))
d2617 1
a2617 1
				 ((dev)->pdev->device & 0x00F0) == 0x0020)
d2619 15
a2633 2
#define IS_HSW_ULX(dev)		((dev)->pdev->device == 0x0A0E || \
				 (dev)->pdev->device == 0x0A1E)
d2636 11
d2660 1
d2666 8
a2673 5
#define HAS_BSD(dev)            (INTEL_INFO(dev)->ring_mask & BSD_RING)
#define HAS_BLT(dev)            (INTEL_INFO(dev)->ring_mask & BLT_RING)
#define HAS_VEBOX(dev)            (INTEL_INFO(dev)->ring_mask & VEBOX_RING)
#define HAS_LLC(dev)            (INTEL_INFO(dev)->has_llc)
#define HAS_WT(dev)            (IS_HASWELL(dev) && to_i915(dev)->ellc_size)
d2677 4
a2680 1
#define HAS_ALIASING_PPGTT(dev)	(INTEL_INFO(dev)->gen >=6 && !IS_VALLEYVIEW(dev))
a2700 3
#define SUPPORTS_DIGITAL_OUTPUTS(dev)	(!IS_GEN2(dev) && !IS_PINEVIEW(dev))
#define SUPPORTS_INTEGRATED_HDMI(dev)	(IS_G4X(dev) || IS_GEN5(dev))
#define SUPPORTS_INTEGRATED_DP(dev)	(IS_G4X(dev) || IS_GEN5(dev))
d2708 4
a2711 1
#define HAS_IPS(dev)		(IS_ULT(dev) || IS_BROADWELL(dev))
d2715 19
a2733 3
#define HAS_PSR(dev)		(IS_HASWELL(dev) || IS_BROADWELL(dev))
#define HAS_PC8(dev)		(IS_HASWELL(dev)) /* XXX HSW:ULX */
#define HAS_RUNTIME_PM(dev)	(IS_HASWELL(dev))
d2741 4
d2746 2
a2747 1
#define INTEL_PCH_TYPE(dev) (to_i915(dev)->pch_type)
d2749 1
d2755 2
d2762 1
a2767 20
extern unsigned int i915_fbpercrtc __always_unused;
extern int i915_panel_ignore_lid __read_mostly;
extern unsigned int i915_powersave __read_mostly;
extern int i915_semaphores __read_mostly;
extern unsigned int i915_lvds_downclock __read_mostly;
extern int i915_lvds_channel_mode __read_mostly;
extern int i915_panel_use_ssc __read_mostly;
extern int i915_vbt_sdvo_panel_type __read_mostly;
extern int i915_enable_rc6 __read_mostly;
extern int i915_enable_fbc __read_mostly;
extern bool i915_enable_hangcheck __read_mostly;
extern int i915_enable_ppgtt __read_mostly;
extern int i915_enable_psr __read_mostly;
extern unsigned int i915_preliminary_hw_support __read_mostly;
extern int i915_disable_power_well __read_mostly;
extern int i915_enable_ips __read_mostly;
extern bool i915_fastboot __read_mostly;
extern int i915_enable_pc8 __read_mostly;
extern int i915_pc8_timeout __read_mostly;
extern bool i915_prefault_disable __read_mostly;
d2770 2
a2771 4
extern int i915_suspend(struct drm_device *dev, pm_message_t state);
extern int i915_resume(struct drm_device *dev);
extern int i915_master_create(struct drm_device *dev, struct drm_master *master);
extern void i915_master_destroy(struct drm_device *dev, struct drm_master *master);
d2774 36
a2810 2
void i915_update_dri1_breadcrumb(struct drm_device *dev);
extern void i915_kernel_lost_context(struct drm_device * dev);
d2813 1
a2813 1
extern int i915_driver_open(struct drm_device *dev, struct drm_file *file_priv);
d2816 1
a2816 1
				 struct drm_file *file_priv);
d2818 1
a2818 2
				  struct drm_file *file_priv);
extern int i915_driver_device_is_agp(struct drm_device * dev);
a2822 3
extern int i915_emit_box(struct drm_device *dev,
			 struct drm_clip_rect *box,
			 int DR1, int DR4);
d2824 1
d2830 2
d2833 6
a2838 1
extern void intel_console_resume(struct work_struct *work);
d2842 7
a2848 4
void i915_handle_error(struct drm_device *dev, bool wedged);

extern void intel_irq_init(struct drm_device *dev);
extern void intel_hpd_init(struct drm_device *dev);
d2851 2
a2852 1
extern void intel_uncore_early_sanitize(struct drm_device *dev);
d2856 22
d2880 2
a2881 1
i915_enable_pipestat(drm_i915_private_t *dev_priv, enum pipe pipe, u32 mask);
d2884 19
a2902 1
i915_disable_pipestat(drm_i915_private_t *dev_priv, enum pipe pipe, u32 mask);
a2904 2
int i915_gem_init_ioctl(struct drm_device *dev, void *data,
			struct drm_file *file_priv);
d2919 6
a2928 4
int i915_gem_pin_ioctl(struct drm_device *dev, void *data,
		       struct drm_file *file_priv);
int i915_gem_unpin_ioctl(struct drm_device *dev, void *data,
			 struct drm_file *file_priv);
a2938 4
int i915_gem_entervt_ioctl(struct drm_device *dev, void *data,
			   struct drm_file *file_priv);
int i915_gem_leavevt_ioctl(struct drm_device *dev, void *data,
			   struct drm_file *file_priv);
d2943 3
d2957 2
d2962 24
a2985 6
int __must_check i915_gem_object_pin(struct drm_i915_gem_object *obj,
				     struct i915_address_space *vm,
				     uint32_t alignment,
				     bool map_and_fenceable,
				     bool nonblocking);
void i915_gem_object_unpin(struct drm_i915_gem_object *obj);
d2987 5
a2991 1
int __must_check i915_gem_object_ggtt_unbind(struct drm_i915_gem_object *obj);
d2995 3
a2997 1
void i915_gem_lastclose(struct drm_device *dev);
d3000 6
d3007 2
a3008 1
static inline struct page *i915_gem_object_get_page(struct drm_i915_gem_object *obj, int n)
d3010 2
a3011 1
	struct sg_page_iter sg_iter;
d3013 10
a3022 2
	for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, n)
		return sg_page_iter_page(&sg_iter);
d3024 1
a3024 1
	return NULL;
d3027 2
a3028 1
static inline struct vm_page *i915_gem_object_get_page(struct drm_i915_gem_object *obj, int n)
d3030 13
a3042 1
	return (obj->pages[n]);
d3045 1
d3059 2
a3060 1
			 struct intel_ring_buffer *to);
d3062 1
a3062 1
			     struct intel_ring_buffer *ring);
d3077 2
a3078 7
int __must_check i915_gem_get_seqno(struct drm_device *dev, u32 *seqno);
int __must_check i915_gem_set_seqno(struct drm_device *dev, u32 seqno);
int __must_check i915_gem_object_get_fence(struct drm_i915_gem_object *obj);
int __must_check i915_gem_object_put_fence(struct drm_i915_gem_object *obj);

static inline bool
i915_gem_object_pin_fence(struct drm_i915_gem_object *obj)
d3080 2
a3081 6
	if (obj->fence_reg != I915_FENCE_REG_NONE) {
		struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
		dev_priv->fence_regs[obj->fence_reg].pin_count++;
		return true;
	} else
		return false;
d3084 2
a3085 2
static inline void
i915_gem_object_unpin_fence(struct drm_i915_gem_object *obj)
d3087 2
a3088 5
	if (obj->fence_reg != I915_FENCE_REG_NONE) {
		struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
		WARN_ON(dev_priv->fence_regs[obj->fence_reg].pin_count <= 0);
		dev_priv->fence_regs[obj->fence_reg].pin_count--;
	}
d3091 6
d3098 1
a3098 1
void i915_gem_retire_requests_ring(struct intel_ring_buffer *ring);
d3101 1
d3118 12
a3131 1
int __must_check i915_gem_object_finish_gpu(struct drm_i915_gem_object *obj);
d3133 1
d3135 1
a3135 1
int i915_gem_l3_remap(struct intel_ring_buffer *ring, int slice);
d3140 16
a3155 8
int __i915_add_request(struct intel_ring_buffer *ring,
		       struct drm_file *file,
		       struct drm_i915_gem_object *batch_obj,
		       u32 *seqno);
#define i915_add_request(ring, seqno) \
	__i915_add_request(ring, NULL, NULL, seqno)
int __must_check i915_wait_seqno(struct intel_ring_buffer *ring,
				 uint32_t seqno);
d3159 4
d3171 6
a3176 5
				     struct intel_ring_buffer *pipelined);
void i915_gem_object_unpin_from_display_plane(struct drm_i915_gem_object *obj);
int i915_gem_attach_phys_object(struct drm_device *dev,
				struct drm_i915_gem_object *obj,
				int id,
a3177 3
void i915_gem_detach_phys_object(struct drm_device *dev,
				 struct drm_i915_gem_object *obj);
void i915_gem_free_all_phys_object(struct drm_device *dev);
d3198 9
a3206 1
void i915_gem_restore_fences(struct drm_device *dev);
a3207 2
unsigned long i915_gem_obj_offset(struct drm_i915_gem_object *o,
				  struct i915_address_space *vm);
d3209 2
d3213 1
d3216 7
a3222 2
struct i915_vma *i915_gem_obj_to_vma(struct drm_i915_gem_object *obj,
				     struct i915_address_space *vm);
d3226 3
d3230 6
a3235 1
struct i915_vma *i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj);
d3238 1
a3238 1
#define obj_to_ggtt(obj) \
d3247 2
a3248 1
static inline bool i915_gem_obj_ggtt_bound(struct drm_i915_gem_object *obj)
d3250 3
a3252 1
	return i915_gem_obj_bound(obj, obj_to_ggtt(obj));
d3255 2
a3256 2
static inline unsigned long
i915_gem_obj_ggtt_offset(struct drm_i915_gem_object *obj)
d3258 1
a3258 1
	return i915_gem_obj_offset(obj, obj_to_ggtt(obj));
d3264 1
a3264 1
	return i915_gem_obj_size(obj, obj_to_ggtt(obj));
d3270 8
a3277 2
		      bool map_and_fenceable,
		      bool nonblocking)
d3279 1
a3279 2
	return i915_gem_object_pin(obj, obj_to_ggtt(obj), alignment,
				   map_and_fenceable, nonblocking);
d3282 21
d3306 3
d3310 3
a3312 2
int i915_switch_context(struct intel_ring_buffer *ring,
			struct drm_file *file, int to_id);
d3314 3
a3316 1
static inline void i915_gem_context_reference(struct i915_hw_context *ctx)
d3321 1
a3321 1
static inline void i915_gem_context_unreference(struct i915_hw_context *ctx)
d3326 5
a3330 4
struct i915_ctx_hang_stats * __must_check
i915_gem_context_get_hang_stats(struct drm_device *dev,
				struct drm_file *file,
				u32 id);
d3335 4
a3338 27

/* i915_gem_gtt.c */
void i915_gem_cleanup_aliasing_ppgtt(struct drm_device *dev);
void i915_ppgtt_bind_object(struct i915_hw_ppgtt *ppgtt,
			    struct drm_i915_gem_object *obj,
			    enum i915_cache_level cache_level);
void i915_ppgtt_unbind_object(struct i915_hw_ppgtt *ppgtt,
			      struct drm_i915_gem_object *obj);

void i915_check_and_clear_faults(struct drm_device *dev);
void i915_gem_suspend_gtt_mappings(struct drm_device *dev);
void i915_gem_restore_gtt_mappings(struct drm_device *dev);
int __must_check i915_gem_gtt_prepare_object(struct drm_i915_gem_object *obj);
void i915_gem_gtt_bind_object(struct drm_i915_gem_object *obj,
				enum i915_cache_level cache_level);
void i915_gem_gtt_unbind_object(struct drm_i915_gem_object *obj);
void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj);
void i915_gem_init_global_gtt(struct drm_device *dev);
void i915_gem_setup_global_gtt(struct drm_device *dev, unsigned long start,
			       unsigned long mappable_end, unsigned long end);
int i915_gem_gtt_init(struct drm_device *dev);
static inline void i915_gem_chipset_flush(struct drm_device *dev)
{
	if (INTEL_INFO(dev)->gen < 6)
		intel_gtt_chipset_flush();
}

d3346 3
a3348 2
					  bool mappable,
					  bool nonblock);
d3350 7
a3356 1
int i915_gem_evict_everything(struct drm_device *dev);
d3359 9
a3368 2
int i915_gem_stolen_setup_compression(struct drm_device *dev, int size);
void i915_gem_stolen_cleanup_compression(struct drm_device *dev);
d3377 12
a3388 1
void i915_gem_object_release_stolen(struct drm_i915_gem_object *obj);
d3393 1
a3393 1
	drm_i915_private_t *dev_priv = obj->base.dev->dev_private;
a3398 4
void i915_gem_detect_bit_6_swizzle(struct drm_device *dev);
void i915_gem_object_do_bit_17_swizzle(struct drm_i915_gem_object *obj);
void i915_gem_object_save_bit_17_swizzle(struct drm_i915_gem_object *obj);

a3405 1
#ifdef __linux__
d3410 1
d3413 2
a3416 1
#endif
d3419 1
a3419 1
//__printf(2, 3)
d3424 1
d3431 2
a3432 1
void i915_capture_error_state(struct drm_device *dev);
d3439 13
a3451 1
const char *i915_cache_level_str(int type);
a3456 4
/* i915_ums.c */
void i915_save_display_reg(struct drm_device *dev);
void i915_restore_display_reg(struct drm_device *dev);

d3464 2
a3465 4
static inline bool intel_gmbus_is_port_valid(unsigned port)
{
	return (port >= GMBUS_PORT_SSC && port <= GMBUS_PORT_DPD);
}
d3467 2
a3468 2
extern struct i2c_adapter *intel_gmbus_get_adapter(
		struct drm_i915_private *dev_priv, unsigned port);
d3473 1
a3473 1
	return container_of(adapter, struct intel_gmbus, controller)->force_bit;
d3477 3
a3480 1
struct intel_encoder;
a3517 1
extern void intel_modeset_suspend_hw(struct drm_device *dev);
d3523 1
a3523 2
extern void intel_modeset_setup_hw_state(struct drm_device *dev,
					 bool force_restore);
d3525 1
a3525 2
extern bool intel_fbc_enabled(struct drm_device *dev);
extern void intel_disable_fbc(struct drm_device *dev);
d3528 3
a3530 4
extern void gen6_set_rps(struct drm_device *dev, u8 val);
extern void valleyview_set_rps(struct drm_device *dev, u8 val);
extern int valleyview_rps_max_freq(struct drm_i915_private *dev_priv);
extern int valleyview_rps_min_freq(struct drm_i915_private *dev_priv);
d3551 2
a3552 9
/* On SNB platform, before reading ring registers forcewake bit
 * must be set to prevent GT core from power down and stale values being
 * returned.
 */
void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine);
void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine);

int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u8 mbox, u32 *val);
int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u8 mbox, u32 val);
d3555 2
a3556 2
u32 vlv_punit_read(struct drm_i915_private *dev_priv, u8 addr);
void vlv_punit_write(struct drm_i915_private *dev_priv, u8 addr, u32 val);
d3577 2
a3578 21
int vlv_gpu_freq(struct drm_i915_private *dev_priv, int val);
int vlv_freq_opcode(struct drm_i915_private *dev_priv, int val);

void vlv_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine);
void vlv_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine);

#define FORCEWAKE_VLV_RENDER_RANGE_OFFSET(reg) \
	(((reg) >= 0x2000 && (reg) < 0x4000) ||\
	((reg) >= 0x5000 && (reg) < 0x8000) ||\
	((reg) >= 0xB000 && (reg) < 0x12000) ||\
	((reg) >= 0x2E000 && (reg) < 0x30000))

#define FORCEWAKE_VLV_MEDIA_RANGE_OFFSET(reg)\
	(((reg) >= 0x12000 && (reg) < 0x14000) ||\
	((reg) >= 0x22000 && (reg) < 0x24000) ||\
	((reg) >= 0x30000 && (reg) < 0x40000))

#define FORCEWAKE_RENDER	(1 << 0)
#define FORCEWAKE_MEDIA		(1 << 1)
#define FORCEWAKE_ALL		(FORCEWAKE_RENDER | FORCEWAKE_MEDIA)

d3593 6
d3602 10
d3615 16
d3638 3
a3640 1
	if (HAS_PCH_SPLIT(dev))
a3641 2
	else if (IS_VALLEYVIEW(dev))
		return VLV_VGACNTRL;
d3658 5
d3669 61
@


1.73
log
@Backport some commits from mainline linux to enable High Bit Rate 2
(HBR2) for Broadwell and non-ULX Haswell DisplayPort.  This enables
support for 3840x2160 60Hz SST.

Initial patch from and tested by Scot Doyle.

drm/i915: Enable 5.4Ghz (HBR2) link rate for Displayport 1.2-capable devices
from Todd Previte
06ea66b6bb445043dc25a9626254d5c130093199

drm/i915: don't try DP_LINK_BW_5_4 on HSW ULX
from Paulo Zanoni
9bbfd20abe5025adbb0ac75160bd2e41158a9e83

drm/i915/dp: add missing \n in the TPS3 debug message
from Jani Nikula
f8d8a672f9370278ae2c9752ad3021662dbc42fd

drm/i915/dp: only use training pattern 3 on platforms that support it
from Jani Nikula
7809a61176b385ebb3299ea43c58b1bb31ffb8c0
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.72 2015/11/01 14:07:43 jsg Exp $ */
a788 6
struct i915_ctx_handle {
	SPLAY_ENTRY(i915_ctx_handle)	 entry;
	struct i915_hw_context		*ctx;
	uint32_t			 handle;
};

d1903 1
a1903 2
	SPLAY_HEAD(i915_ctx_tree, i915_ctx_handle) ctx_tree;
	uint32_t ctx_id;
@


1.72
log
@Bring in some commits from Rodrigo Vivi in linux to correct
the IS_BDW_ULT macro.

drm/i915: BDW Fix Halo PCI IDs marked as ULT.
6b96d705f3cf435b0b8835b12c9742513c77fed6 in mainline linux

This will prevent the desktop "Iris Pro Graphics 6200" from being
misidentified as ULT.

drm/i915/bdw: PCI IDs ending in 0xb are ULT.
0dc6f20b9803f09726bbb682649d35cda8ef5b5d in mainline linux

This will make the mobile "Iris Graphics 6100" be correctly
identified as being a ULT part.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.71 2015/10/29 07:47:03 kettenis Exp $ */
d1957 3
@


1.71
log
@Make inteldrm(4) attach to pci(4) instead of vga(4) just like radeondrm(4).
This is needed for machines where Intel graphics isn't the primary graphics
device and on systems with UEFI firmware that put the device in non-VGA mode.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.70 2015/10/17 21:41:12 kettenis Exp $ */
d1949 2
a1950 2
				 (((dev)->pdev->device & 0xf) == 0x2  || \
				 ((dev)->pdev->device & 0xf) == 0x6 || \
@


1.70
log
@Fix the code that sets up the MCH BAR on systems where the (buggy) BIOS
doesn't do this for us.  The code was poking registers on the wrong PCI
device.  We were just lucky that it worked on most systems.

This should fix machines such as the Asus EeePC 701 and get rid of the

error: [drm:pid0:i915_gem_detect_bit_6_swizzle] *ERROR* Couldn't read from
MC HBAR.  Disabling tiling.

messages on that machine.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.69 2015/09/28 17:29:56 kettenis Exp $ */
d1460 1
@


1.69
log
@The Linux code that handles the DPMS mode for inteldrm(4) can sleep now.
Adopt the approach taken by radeondrm(4) and hand the "burner" work off
to a task.

Avoids the panic reported by Gerald Hanuer, who also tested this fix.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.68 2015/09/26 11:17:15 kettenis Exp $ */
a1500 1
#ifdef __linux__
a1501 1
#endif
@


1.68
log
@Make the PPGTT code work.  Seems to fix the caching issues on Broadwell.
Comments on some of the later Broadwell-related commits in the Linux tree
seem to say that the PPAT flags in for the (global) GTT are simply broken in
the hardware.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.67 2015/09/23 23:12:12 kettenis Exp $ */
d1468 3
@


1.67
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.66 2015/06/26 15:22:23 kettenis Exp $ */
d713 2
a714 2
		struct page **pt_pages;
		struct page *gen8_pt_pages;
d716 1
a716 1
	struct page *pd_pages;
@


1.66
log
@Add Linux completion API and use it.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.65 2015/06/24 17:59:42 kettenis Exp $ */
d39 6
d64 63
d154 1
d177 2
d189 66
a254 1
#define I915_GEM_GPU_DOMAINS	(~(I915_GEM_DOMAIN_CPU | I915_GEM_DOMAIN_GTT))
d262 19
a280 1
struct intel_pch_pll {
d284 27
a310 5
	int pll_reg;
	int fp0_reg;
	int fp1_reg;
};
#define I915_NUM_PLLS 2
a331 1
#define WATCH_COHERENCY	0
a352 2
struct inteldrm_softc;
#define drm_i915_private inteldrm_softc
d358 2
d363 1
d370 1
a370 1
#ifndef __OpenBSD__
d377 3
a379 3
#define I915_MAX_NUM_FENCES 16
/* 16 fences + sign bit for FENCE_REG_NONE */
#define I915_MAX_NUM_FENCE_BITS 5
d399 1
a399 1
	int ref;
d423 1
d428 1
a428 1
	u64 bbaddr;
d435 1
d440 1
a440 1
		} *ringbuffer, *batchbuffer;
d461 3
a463 3
		u32 cache_level:2;
	} *active_bo, *pinned_bo;
	u32 active_bo_count, pinned_bo_count;
d466 2
d470 6
d478 1
a478 1
	void (*enable_fbc)(struct drm_crtc *crtc, unsigned long interval);
d482 23
a504 5
	void (*update_wm)(struct drm_device *dev);
	void (*update_sprite_wm)(struct drm_device *dev, int pipe,
				 uint32_t sprite_width, int pixel_size);
	void (*update_linetime_wm)(struct drm_device *dev, int pipe,
				 struct drm_display_mode *mode);
d506 4
a510 2
			     struct drm_display_mode *mode,
			     struct drm_display_mode *adjusted_mode,
d517 2
a518 1
			  struct drm_crtc *crtc);
d523 2
a524 1
			  struct drm_i915_gem_object *obj);
d527 1
d533 28
d563 42
a604 30
struct drm_i915_gt_funcs {
	void (*force_wake_get)(struct drm_i915_private *dev_priv);
	void (*force_wake_put)(struct drm_i915_private *dev_priv);
};

#define DEV_INFO_FLAGS \
	DEV_INFO_FLAG(is_mobile) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_i85x) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_i915g) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_i945gm) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_g33) DEV_INFO_SEP \
	DEV_INFO_FLAG(need_gfx_hws) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_g4x) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_pineview) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_broadwater) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_crestline) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_ivybridge) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_valleyview) DEV_INFO_SEP \
	DEV_INFO_FLAG(is_haswell) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_force_wake) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_fbc) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_pipe_cxsr) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_hotplug) DEV_INFO_SEP \
	DEV_INFO_FLAG(cursor_needs_physical) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_overlay) DEV_INFO_SEP \
	DEV_INFO_FLAG(overlay_needs_physical) DEV_INFO_SEP \
	DEV_INFO_FLAG(supports_tv) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_bsd_ring) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_blt_ring) DEV_INFO_SEP \
	DEV_INFO_FLAG(has_llc)
d607 1
d610 96
a705 24
	u8 is_mobile:1;
	u8 is_i85x:1;
	u8 is_i915g:1;
	u8 is_i945gm:1;
	u8 is_g33:1;
	u8 need_gfx_hws:1;
	u8 is_g4x:1;
	u8 is_pineview:1;
	u8 is_broadwater:1;
	u8 is_crestline:1;
	u8 is_ivybridge:1;
	u8 is_valleyview:1;
	u8 has_force_wake:1;
	u8 is_haswell:1;
	u8 has_fbc:1;
	u8 has_pipe_cxsr:1;
	u8 has_hotplug:1;
	u8 cursor_needs_physical:1;
	u8 has_overlay:1;
	u8 overlay_needs_physical:1;
	u8 supports_tv:1;
	u8 has_bsd_ring:1;
	u8 has_blt_ring:1;
	u8 has_llc:1;
d707 1
a708 2
#define I915_PPGTT_PD_ENTRIES 512
#define I915_PPGTT_PT_ENTRIES 1024
d710 1
a710 1
	struct drm_device *dev;
d712 16
a727 4
	struct vm_page **pt_pages;
	uint32_t pd_offset;
	dma_addr_t *pt_dma_addr;
	dma_addr_t scratch_page_dma_addr;
d730 43
d777 1
d780 1
d784 3
d795 33
a827 9
enum no_fbc_reason {
	FBC_NO_OUTPUT, /* no outputs enabled to compress */
	FBC_STOLEN_TOO_SMALL, /* not enough space to hold compressed buffers */
	FBC_UNSUPPORTED_MODE, /* interlace or doublescanned mode */
	FBC_MODE_TOO_LARGE, /* mode too large for compression */
	FBC_BAD_PLANE, /* fbc not supported on plane */
	FBC_NOT_TILED, /* buffer not tiled */
	FBC_MULTIPLE_PIPES, /* more than one pipe active */
	FBC_MODULE_PARAM,
d835 1
d899 1
d1015 1
a1017 3
	/* lock - irqsave spinlock that protectects the work_struct and
	 * pm_iir. */
	spinlock_t lock;
d1024 7
d1032 1
d1042 3
d1067 27
d1107 13
d1121 1
a1121 1
	u32 *remap_info;
d1123 317
d1442 1
a1442 1
struct inteldrm_softc {
d1456 1
d1469 7
a1475 2
	int sc_offset;
	int (*sc_copyrows)(void *, int, int, int);
d1477 1
a1477 8
	struct drm_i915_gt_funcs gt;
	/** gt_fifo_count and the subsequent register write are synchronized
	 * with dev->struct_mutex. */
	unsigned gt_fifo_count;
	/** forcewake_count is protected by gt_lock */
	unsigned forcewake_count;
	/** gt_lock is also taken in irq contexts. */
	struct mutex gt_lock;
d1481 1
d1491 2
d1495 1
a1495 1
	uint32_t next_seqno;
d1498 1
a1498 1
#ifdef notyet
d1518 5
d1524 1
a1524 1
	spinlock_t dpio_lock;
d1527 4
a1530 2
	u32 pipestat[2];
	u32 irq_mask;
d1532 1
a1532 1
	u32 pch_irq_mask;
a1533 1
	u32 hotplug_supported_mask;
d1535 12
d1548 1
a1548 17
	int num_pch_pll;

	/* For hangcheck timer */
#define DRM_I915_HANGCHECK_PERIOD 1500 /* in ms */
#define DRM_I915_HANGCHECK_JIFFIES msecs_to_jiffies(DRM_I915_HANGCHECK_PERIOD)
	struct timeout hangcheck_timer;
	int hangcheck_count;
	uint32_t last_acthd[I915_NUM_RINGS];
	uint32_t prev_instdone[I915_NUM_INSTDONE_REG];

	unsigned int stop_rings;

	unsigned long cfb_size;
	unsigned int cfb_fb;
	enum plane cfb_plane;
	int cfb_y;
	struct intel_fbc_work *fbc_work;
d1550 1
d1552 1
d1556 3
a1558 1
	bool sprite_scaling_enabled;
a1560 27
	int backlight_level;  /* restore backlight to this value */
	bool backlight_enabled;
	struct drm_display_mode *lfp_lvds_vbt_mode; /* if any */
	struct drm_display_mode *sdvo_lvds_vbt_mode; /* if any */

	/* Feature bits from the VBIOS */
	unsigned int int_tv_support:1;
	unsigned int lvds_dither:1;
	unsigned int lvds_vbt:1;
	unsigned int int_crt_support:1;
	unsigned int lvds_use_ssc:1;
	unsigned int display_clock_mode:1;
	unsigned int fdi_rx_polarity_inverted:1;
	int lvds_ssc_freq;
	unsigned int bios_lvds_val; /* initial [PCH_]LVDS reg val in VBIOS */
	unsigned int lvds_val; /* used for checking LVDS channel mode */
	struct {
		int rate;
		int lanes;
		int preemphasis;
		int vswing;

		bool initialized;
		bool support;
		int bpp;
		struct edp_power_seq pps;
	} edp;
a1562 1
	int crt_ddc_pin;
d1569 7
a1575 5
	spinlock_t error_lock;
	/* Protected by dev->error_lock. */
	struct drm_i915_error_state *first_error;
	struct work_struct error_work;
	struct completion error_completion;
a1577 3
	/* number of ioctls + faults in flight */
	int entries;

d1587 2
a1588 78
	/* Register state */
	bool modeset_on_lid;

	struct {
		/** Bridge to intel-gtt-ko */
		struct intel_gtt *gtt;
		/** Memory allocator for GTT stolen memory */
		struct drm_mm stolen;
		/** Memory allocator for GTT */
		struct drm_mm gtt_space;
		/** List of all objects in gtt_space. Used to restore gtt
		 * mappings on resume */
		struct list_head bound_list;
		/**
		 * List of objects which are not bound to the GTT (thus
		 * are idle and not used by the GPU) but still have
		 * (presumably uncached) pages still attached.
		 */
		struct list_head unbound_list;

		/** Usable portion of the GTT for GEM */
		unsigned long gtt_start;
		unsigned long gtt_mappable_end;
		unsigned long gtt_end;
		unsigned long stolen_base; /* limited to low memory (32-bit) */

		struct io_mapping *gtt_mapping;
		phys_addr_t gtt_base_addr;
		int gtt_mtrr;

		/** PPGTT used for aliasing the PPGTT with the GTT */
		struct i915_hw_ppgtt *aliasing_ppgtt;

#ifdef notyet
		struct shrinker inactive_shrinker;
#endif
		bool shrinker_no_lock_stealing;

		/**
		 * List of objects currently involved in rendering.
		 *
		 * Includes buffers having the contents of their GPU caches
		 * flushed, not necessarily primitives.  last_rendering_seqno
		 * represents when the rendering involved will be completed.
		 *
		 * A reference is held on the buffer while on this list.
		 */
		struct list_head active_list;

		/**
		 * LRU list of objects which are not in the ringbuffer and
		 * are ready to unbind, but are still in the GTT.
		 *
		 * last_rendering_seqno is 0 while an object is in this list.
		 *
		 * A reference is not held on the buffer while on this list,
		 * as merely being GTT-bound shouldn't prevent its being
		 * freed, and we'll pull it off the list in the free path.
		 */
		struct list_head inactive_list;

		/** LRU list of objects with fence regs on them. */
		struct list_head fence_list;

		/**
		 * We leave the user IRQ off as much as possible,
		 * but this means that requests will finish and never
		 * be retired once the system goes idle. Set a timer to
		 * fire periodically while the ring is running. When it
		 * fires, go retire requests.
		 */
		struct delayed_work retire_work;

		/**
		 * Are we in a non-interruptible section of code like
		 * modesetting?
		 */
		bool interruptible;
d1590 2
a1591 9
		/**
		 * Flag if the X Server, and thus DRM, is not currently in
		 * control of the device.
		 *
		 * This is set between LeaveVT and EnterVT.  It needs to be
		 * replaced with a semaphore.  It also needs to be
		 * transitioned away from for kernel modesetting.
		 */
		int suspended;
d1593 1
a1593 23
		/**
		 * Flag if the hardware appears to be wedged.
		 *
		 * This is set when attempts to idle the device timeout.
		 * It prevents command submission from occurring and makes
		 * every pending request fail
		 */
		atomic_t wedged;

		/** Bit 6 swizzling required for X tiling */
		uint32_t bit_6_swizzle_x;
		/** Bit 6 swizzling required for Y tiling */
		uint32_t bit_6_swizzle_y;

		/* storage for physical objects */
		struct drm_i915_gem_phys_object *phys_objs[I915_MAX_PHYS_OBJECT];

		/* accounting, useful for userland debugging */
		size_t gtt_total;
		size_t mappable_gtt_total;
		size_t object_memory;
		u32 object_count;
	} mm;
a1597 4
	/* indicate whether the LVDS_BORDER should be enabled or not */
	unsigned int lvds_border_bits;
	/* Panel fitter placement and size for Ironlake+ */
	u32 pch_pf_pos, pch_pf_size;
d1603 6
a1608 1
	struct intel_pch_pll pch_plls[I915_NUM_PLLS];
d1610 1
a1617 2
	int child_dev_num;
	struct child_device_config *child_dev;
d1623 3
d1633 3
a1635 1
	enum no_fbc_reason no_fbc_reason;
d1637 1
a1637 2
	struct drm_mm_node *compressed_fb;
	struct drm_mm_node *compressed_llb;
d1639 1
a1639 1
	unsigned long last_gpu_reset;
d1641 1
d1644 1
a1649 1
#ifdef notyet
a1650 3
#endif

	struct backlight_device *backlight;
a1654 1
	bool hw_contexts_disabled;
d1656 1
d1662 21
d1686 8
a1693 2
};
typedef struct drm_i915_private drm_i915_private_t;
d1707 1
a1707 5
enum i915_cache_level {
	I915_CACHE_NONE = 0,
	I915_CACHE_LLC,
	I915_CACHE_LLC_MLC, /* gen6+, in docs at least! */
};
d1732 6
a1737 3
	/** Current space allocated to this object in the GTT, if any. */
	struct drm_mm_node *gtt_space;
	struct list_head gtt_list;
a1738 1
	/** This object's place on the active/inactive lists */
d1740 2
a1741 3
	struct list_head mm_list;
	/** This object's place in the batchbuffer or on the eviction list */
	struct list_head exec_list;
d1806 1
d1814 1
a1814 1
	unsigned int cache_level:2;
d1820 3
d1824 1
a1830 14
	/**
	 * Used for performing relocations during execbuffer insertion.
	 */
	LIST_ENTRY(drm_i915_gem_object) exec_node;
	unsigned long exec_handle;
	struct drm_i915_gem_exec_object2 *exec_entry;

	/**
	 * Current offset of the object in GTT space.
	 *
	 * This is the same as gtt_space->start
	 */
	uint32_t gtt_offset;

d1842 3
d1849 1
a1849 1
	uint32_t user_pin_count;
a1853 7

	/**
	 * Number of crtcs where this object is currently the fb, but
	 * will be page flipped away on the next vblank.  When it
	 * reaches 0, dev_priv->pending_flip_queue will be woken up.
	 */
	atomic_t pending_flip;
d1876 4
a1879 1
	/** Postion in the ringbuffer of the end of the request */
d1882 6
d1900 2
d1903 1
a1903 1
		struct mutex lock;
d1905 1
d1909 3
d1914 1
a1914 1
#define INTEL_INFO(dev)	(((struct drm_i915_private *) (dev)->dev_private)->info)
d1916 2
a1917 2
#define IS_I830(dev)		((dev)->pci_device == 0x3577)
#define IS_845G(dev)		((dev)->pci_device == 0x2562)
d1919 1
a1919 1
#define IS_I865G(dev)		((dev)->pci_device == 0x2572)
d1921 2
a1922 2
#define IS_I915GM(dev)		((dev)->pci_device == 0x2592)
#define IS_I945G(dev)		((dev)->pci_device == 0x2772)
d1926 1
a1926 1
#define IS_GM45(dev)		((dev)->pci_device == 0x2A42)
d1928 2
a1929 2
#define IS_PINEVIEW_G(dev)	((dev)->pci_device == 0xa001)
#define IS_PINEVIEW_M(dev)	((dev)->pci_device == 0xa011)
d1932 1
a1932 2
#define IS_IRONLAKE_D(dev)	((dev)->pci_device == 0x0042)
#define IS_IRONLAKE_M(dev)	((dev)->pci_device == 0x0046)
d1934 6
a1939 6
#define IS_IVB_GT1(dev)		((dev)->pci_device == 0x0156 || \
				 (dev)->pci_device == 0x0152 ||	\
				 (dev)->pci_device == 0x015a)
#define IS_SNB_GT1(dev)		((dev)->pci_device == 0x0102 || \
				 (dev)->pci_device == 0x0106 ||	\
				 (dev)->pci_device == 0x010A)
d1942 1
d1944 12
a1955 2
#define IS_ULT(dev)		(IS_HASWELL(dev) && \
				 ((dev)->pci_device & 0xFF00) == 0x0A00)
d1969 1
d1971 7
a1977 2
#define HAS_BSD(dev)            (INTEL_INFO(dev)->has_bsd_ring)
#define HAS_BLT(dev)            (INTEL_INFO(dev)->has_blt_ring)
d1979 1
d1990 8
a2006 1
#define SUPPORTS_EDP(dev)		(IS_IRONLAKE_M(dev))
a2008 2
/* dsparb controlled by hw only */
#define DSPARB_HWCONTROL(dev) (IS_G4X(dev) || IS_IRONLAKE(dev))
d2012 1
a2012 1
#define I915_HAS_FBC(dev) (INTEL_INFO(dev)->has_fbc)
d2014 1
a2014 1
#define HAS_PIPE_CONTROL(dev) (INTEL_INFO(dev)->gen >= 5)
d2016 5
a2020 1
#define HAS_DDI(dev)		(IS_HASWELL(dev))
d2029 1
a2029 1
#define INTEL_PCH_TYPE(dev) (((struct drm_i915_private *)(dev)->dev_private)->pch_type)
d2033 1
d2036 3
a2038 3
#define HAS_FORCE_WAKE(dev) (INTEL_INFO(dev)->has_force_wake)

#define HAS_L3_GPU_CACHE(dev) (IS_IVYBRIDGE(dev) || IS_HASWELL(dev))
d2044 1
a2044 22
/**
 * RC6 is a special power stage which allows the GPU to enter an very
 * low-voltage mode when idle, using down to 0V while at this stage.  This
 * stage is entered automatically when the GPU is idle when RC6 support is
 * enabled, and as soon as new workload arises GPU wakes up automatically as well.
 *
 * There are different RC6 modes available in Intel GPU, which differentiate
 * among each other with the latency required to enter and leave RC6 and
 * voltage consumed by the GPU in different states.
 *
 * The combination of the following flags define which states GPU is allowed
 * to enter, while RC6 is the normal RC6 state, RC6p is the deep RC6, and
 * RC6pp is deepest RC6. Their support by hardware varies according to the
 * GPU, BIOS, chipset and platform. RC6 is usually the safest one and the one
 * which brings the most power savings; deeper states save more power, but
 * require higher latency to switch to and wake up.
 */
#define INTEL_RC6_ENABLE			(1<<0)
#define INTEL_RC6p_ENABLE			(1<<1)
#define INTEL_RC6pp_ENABLE			(1<<2)

extern struct drm_ioctl_desc i915_ioctls[];
d2058 1
d2060 6
a2084 2
extern void i915_driver_close(struct drm_device *dev,
				  struct drm_file *file_priv);
a2099 1
#ifdef __linux__
a2100 5
#endif
int intel_setup_mchbar(struct inteldrm_softc *,
    struct pci_attach_args *);
int i915_getparam(struct drm_device *, void *, struct drm_file *);
int i915_setparam(struct drm_device *, void *, struct drm_file *);
d2103 1
a2103 1
void i915_hangcheck_elapsed(void *);
d2107 1
a2107 3
extern void intel_pm_init(struct drm_device *dev);
extern void intel_gt_init(struct drm_device *dev);
extern void intel_gt_sanitize(struct drm_device *dev);
d2109 5
a2113 3
#ifdef notyet
void i915_error_state_free(struct kref *error_ref);
#endif
d2116 1
a2116 1
i915_enable_pipestat(drm_i915_private_t *dev_priv, int pipe, u32 mask);
d2119 1
a2119 10
i915_disable_pipestat(drm_i915_private_t *dev_priv, int pipe, u32 mask);

void intel_enable_asle(struct drm_device *dev);

#ifdef CONFIG_DEBUG_FS
extern void i915_destroy_error_state(struct drm_device *dev);
#else
#define i915_destroy_error_state(x)
#endif

d2169 2
a2170 1
int i915_gem_init_object(struct drm_gem_object *obj);
d2176 2
d2179 1
d2184 4
a2187 1
int __must_check i915_gem_object_unbind(struct drm_i915_gem_object *obj);
a2189 1
int i915_gem_ring_throttle(struct drm_device *, struct drm_file *);
a2190 1
#ifdef __linux
d2192 1
d2195 6
a2200 11
	struct scatterlist *sg = obj->pages->sgl;
	int nents = obj->pages->nents;
	while (nents > SG_MAX_SINGLE_ALLOC) {
		if (n < SG_MAX_SINGLE_ALLOC - 1)
			break;

		sg = sg_chain_ptr(sg + SG_MAX_SINGLE_ALLOC - 1);
		n -= SG_MAX_SINGLE_ALLOC - 1;
		nents -= SG_MAX_SINGLE_ALLOC - 1;
	}
	return sg_page(sg+n);
d2203 4
a2206 1
int i915_gem_object_get_pages(struct drm_i915_gem_object *obj);
a2207 1

d2222 2
a2223 3
void i915_gem_object_move_to_active(struct drm_i915_gem_object *obj,
				    struct intel_ring_buffer *ring);

a2228 2
int i915_gem_dumb_destroy(struct drm_file *file_priv, struct drm_device *dev,
			  uint32_t handle);
d2238 2
a2239 2
extern int i915_gem_get_seqno(struct drm_device *dev, u32 *seqno);

d2259 1
d2264 1
a2264 1
void i915_gem_retire_requests(struct drm_device *dev);
d2266 1
a2266 1
int __must_check i915_gem_check_wedge(struct drm_i915_private *dev_priv,
d2268 15
d2285 1
a2285 4
void i915_gem_clflush_object(struct drm_i915_gem_object *obj);
int __must_check i915_gem_object_set_domain(struct drm_i915_gem_object *obj,
					    uint32_t read_domains,
					    uint32_t write_domain);
d2289 1
a2289 1
void i915_gem_l3_remap(struct drm_device *dev);
a2290 1
void i915_gem_init_ppgtt(struct drm_device *dev);
d2293 7
a2299 4
int __must_check i915_gem_idle(struct drm_device *dev);
int i915_add_request(struct intel_ring_buffer *ring,
		     struct drm_file *file,
		     u32 *seqno);
d2302 3
a2304 2
int i915_gem_fault(struct drm_gem_object *, struct uvm_faultinfo *, off_t,
    vaddr_t, vm_page_t *, int, int, vm_prot_t, int );
d2314 1
d2322 1
d2326 4
a2329 3
i915_gem_get_unfenced_gtt_alignment(struct drm_device *dev,
				    uint32_t size,
				    int tiling_mode);
d2342 53
a2394 6
/* i915_drv.c */
void intel_gtt_chipset_flush(struct drm_device *);
int intel_gpu_reset(struct drm_device *);
int i915_reset(struct drm_device *);
void inteldrm_timeout(void *);
bool i915_semaphore_is_enabled(struct drm_device *);
d2397 1
a2397 1
void i915_gem_context_init(struct drm_device *dev);
d2402 15
a2422 1
int __must_check i915_gem_init_aliasing_ppgtt(struct drm_device *dev);
d2430 2
d2438 3
a2440 4
void i915_gem_init_global_gtt(struct drm_device *dev,
			      unsigned long start,
			      unsigned long mappable_end,
			      unsigned long end);
a2441 1
void i915_gem_gtt_fini(struct drm_device *dev);
d2445 1
a2445 1
		intel_gtt_chipset_flush(dev);
d2450 3
a2452 1
int __must_check i915_gem_evict_something(struct drm_device *dev, int min_size,
d2457 1
d2462 2
d2465 8
d2475 8
a2487 2
void i915_gem_dump_object(struct drm_i915_gem_object *obj, int len,
			  const char *where, uint32_t mark);
a2492 4
void i915_gem_object_check_coherency(struct drm_i915_gem_object *obj,
				     int handle);
void i915_gem_dump_object(struct drm_i915_gem_object *obj, int len,
			  const char *where, uint32_t mark);
d2494 1
a2495 1
#ifdef __linux__
d2498 5
d2505 21
d2530 3
a2532 3
/* i915_suspend.c */
extern int i915_save_state(struct drm_device *dev);
extern int i915_restore_state(struct drm_device *dev);
d2546 1
a2546 1
extern struct i2c_controller *intel_gmbus_get_adapter(
d2548 3
a2550 3
extern void intel_gmbus_set_speed(struct i2c_controller *adapter, int speed);
extern void intel_gmbus_force_bit(struct i2c_controller *adapter, bool force_bit);
static inline bool intel_gmbus_is_forced_bit(struct i2c_controller *i2c)
d2552 1
a2552 1
	return container_of(i2c, struct intel_gmbus, controller)->force_bit;
d2557 2
a2559 1
#ifdef CONFIG_ACPI
d2563 4
a2566 2
extern void intel_opregion_gse_intr(struct drm_device *dev);
extern void intel_opregion_enable_asle(struct drm_device *dev);
d2568 1
d2572 10
a2581 2
static inline void intel_opregion_gse_intr(struct drm_device *dev) { return; }
static inline void intel_opregion_enable_asle(struct drm_device *dev) { return; }
d2595 1
d2599 1
d2603 1
d2609 3
a2614 1
int i915_load_modeset_init(struct drm_device *dev);
d2619 2
a2622 1
#ifdef CONFIG_DEBUG_FS
d2624 2
a2625 1
extern void intel_overlay_print_error_state(struct seq_file *m, struct intel_overlay_error_state *error);
d2628 1
a2628 1
extern void intel_display_print_error_state(struct seq_file *m,
a2630 1
#endif
d2636 2
a2637 3
void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv);
void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv);
int __gen6_gt_wait_for_fifo(struct drm_i915_private *dev_priv);
d2642 60
a2701 6
static __inline void
write8(struct inteldrm_softc *dev_priv, bus_size_t reg, uint8_t val)
{
	bus_space_write_1(dev_priv->regs->bst,dev_priv->regs->bsh, (reg),
	    (val));
}
d2703 2
a2704 6
static __inline void
write16(struct inteldrm_softc *dev_priv, bus_size_t reg, uint16_t val)
{
	bus_space_write_2(dev_priv->regs->bst,dev_priv->regs->bsh, (reg),
	    (val));
}
d2706 4
a2709 6
static __inline void
write32(struct inteldrm_softc *dev_priv, bus_size_t reg, uint32_t val)
{
	bus_space_write_4(dev_priv->regs->bst,dev_priv->regs->bsh, (reg),
	    (val));
}
d2711 1
a2711 3
/* XXX need bus_space_write_8, this evaluated arguments twice */
static __inline void
write64(struct inteldrm_softc *dev_priv, bus_size_t reg, uint64_t val)
d2713 6
a2718 4
	bus_space_write_4(dev_priv->regs->bst, dev_priv->regs->bsh,
	    reg, (u_int32_t)val);
	bus_space_write_4(dev_priv->regs->bst, dev_priv->regs->bsh,
	    reg + 4, upper_32_bits(val));
d2721 1
a2721 2
static __inline uint8_t
read8(struct inteldrm_softc *dev_priv, bus_size_t reg)
d2723 1
a2723 2
	return (bus_space_read_1(dev_priv->regs->bst, dev_priv->regs->bsh,
	    (reg)));
d2726 1
a2726 2
static __inline uint16_t
read16(struct inteldrm_softc *dev_priv, bus_size_t reg)
d2728 1
a2728 3
	return (bus_space_read_2(dev_priv->regs->bst, dev_priv->regs->bsh,
	    (reg)));
}
d2730 1
a2730 5
static __inline uint32_t
read32(struct inteldrm_softc *dev_priv, bus_size_t reg)
{
	return (bus_space_read_4(dev_priv->regs->bst, dev_priv->regs->bsh,
	    (reg)));
d2733 2
a2734 2
static __inline uint64_t
read64(struct inteldrm_softc *dev_priv, bus_size_t reg)
d2736 1
a2736 6
	u_int32_t low, high;

	low = bus_space_read_4(dev_priv->regs->bst,
	    dev_priv->regs->bsh, reg);
	high = bus_space_read_4(dev_priv->regs->bst,
	    dev_priv->regs->bsh, reg + 4);
d2738 1
a2738 1
	return ((u_int64_t)low | ((u_int64_t)high << 32));
a2739 43


#define __i915_read(x, y) \
	u##x i915_read##x(struct drm_i915_private *dev_priv, u32 reg);

__i915_read(8, b)
__i915_read(16, w)
__i915_read(32, l)
__i915_read(64, q)
#undef __i915_read

#define __i915_write(x, y) \
	void i915_write##x(struct drm_i915_private *dev_priv, u32 reg, u##x val);

__i915_write(8, b)
__i915_write(16, w)
__i915_write(32, l)
__i915_write(64, q)
#undef __i915_write

#define I915_READ8(reg)		i915_read8(dev_priv, (reg))
#define I915_WRITE8(reg, val)	i915_write8(dev_priv, (reg), (val))

#define I915_READ16(reg)	i915_read16(dev_priv, (reg))
#define I915_WRITE16(reg, val)	i915_write16(dev_priv, (reg), (val))
#define I915_READ16_NOTRACE(reg)	bus_space_read_2(dev_priv->regs->bst, \
					    dev_priv->regs->bsh, (reg))
#define I915_WRITE16_NOTRACE(reg,val)	bus_space_write_2(dev_priv->regs->bst, \
					    dev_priv->regs->bsh, (reg), (val))

#define I915_READ(reg)		i915_read32(dev_priv, (reg))
#define I915_WRITE(reg, val)	i915_write32(dev_priv, (reg), (val))
#define I915_READ_NOTRACE(reg)		bus_space_read_4(dev_priv->regs->bst, \
					    dev_priv->regs->bsh, (reg))
#define I915_WRITE_NOTRACE(reg,val)	bus_space_write_4(dev_priv->regs->bst, \
					    dev_priv->regs->bsh, (reg), (val))

#define I915_WRITE64(reg, val)	i915_write64(dev_priv, (reg), (val))
#define I915_READ64(reg)	i915_read64(dev_priv, (reg))

#define POSTING_READ(reg)	(void)I915_READ_NOTRACE(reg)
#define POSTING_READ16(reg)	(void)I915_READ16_NOTRACE(reg)

@


1.65
log
@Linux jiffies and OpenBSD ticks are the same thing.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.64 2015/06/24 08:32:39 kettenis Exp $ */
d819 1
a819 2
	int error_completion;
	struct mutex error_completion_lock;
@


1.64
log
@Introduce Linux work queue APIs and use them.  As a side-effect, this will
move some of the work from the system task queue to the driver-specific
task queue.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.63 2015/06/04 06:11:21 jsg Exp $ */
d1220 1
a1220 1
	unsigned long emitted_ticks;
@


1.63
log
@Switch intel_gmbus_is_port_valid and intel_gmbus_is_forced_bit from
extern inline back to static inline so the kernel will build on
compilers that default to c99 inline semantics.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.62 2015/04/18 11:41:28 jsg Exp $ */
d607 1
a607 1
	struct task task;
d619 1
a619 2
	struct task delayed_resume_task;
	struct timeout delayed_resume_to;
d665 1
a665 1
	struct task error_task;
d752 1
a752 1
	struct task hotplug_task;
d818 1
a818 1
	struct task error_task;
d821 1
d906 1
a906 3
		struct timeout retire_timer;
		struct taskq *retire_taskq;
		struct task retire_task;
@


1.62
log
@define and use trace macros
discussed with kettenis
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.61 2015/04/17 00:54:42 jsg Exp $ */
d1751 1
a1751 1
extern inline bool intel_gmbus_is_port_valid(unsigned port)
d1760 1
a1760 1
extern inline bool intel_gmbus_is_forced_bit(struct i2c_controller *i2c)
@


1.61
log
@Make drm ioctls table driven.  Further reduces the diff to linux.
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.60 2015/04/11 05:10:13 jsg Exp $ */
d1341 2
@


1.60
log
@change back to spinlock_t/DEFINE_SPINLOCK
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.59 2015/04/11 02:24:43 jsg Exp $ */
a1362 1
#ifdef notyet
a1363 1
#endif
@


1.59
log
@Rename i915_gem_chipset_flush() to intel_gtt_chipset_flush()
so we can use the inline definition of i915_gem_chipset_flush()
that avoids the flush entirely on gen >= 6.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.58 2015/04/05 11:53:53 kettenis Exp $ */
d611 1
a611 1
	struct mutex lock;
@


1.58
log
@Another round of reducing diffs with Linux.  This one moves the various
copy_to_user and copy_from_user functions into drm_linux.h and uses them
instead of copyin/copyout and DRM_COPY_*.  Also move the timespec functions,
and put i915_gem_object_is_purgable() where it belongs.

Uncovered a bug where the arguments to copyout() were in the wrong order.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.57 2015/04/03 13:10:59 jsg Exp $ */
d1655 1
a1655 1
void i915_gem_chipset_flush(struct drm_device *);
a1692 1
#ifdef notyet
d1696 1
a1696 1
		intel_gtt_chipset_flush();
a1697 1
#endif
@


1.57
log
@resync i915_drv.h to make it diffable to linux
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.56 2015/02/12 04:56:03 kettenis Exp $ */
a1591 6
}

static inline int
i915_gem_object_is_purgeable(struct drm_i915_gem_object *obj)
{
	return obj->madv == I915_MADV_DONTNEED;
@


1.56
log
@Rename the struct device member of inteldrm_softc to sc_dev and rename the
pointer to the drm subdevice to dev such that we can match the linux code
better.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.55 2015/01/27 03:17:36 dlg Exp $ */
d40 4
d117 2
d155 4
d166 4
a169 1
	struct drm_dmamem *handle;
d173 4
d178 110
d326 2
a327 2
	void (*force_wake_get)(struct inteldrm_softc *dev_priv);
	void (*force_wake_put)(struct inteldrm_softc *dev_priv);
d330 26
d385 12
a423 36
struct opregion_header;
struct opregion_acpi;
struct opregion_swsci;
struct opregion_asle;

struct intel_opregion {
	struct opregion_header *header;
	struct opregion_acpi *acpi;
	struct opregion_swsci *swsci;
	struct opregion_asle *asle;
	void *vbt;
	u32 *lid_state;
};
#define OPREGION_SIZE            (8*1024)


#define I915_FENCE_REG_NONE -1
#define I915_MAX_NUM_FENCES 16
/* 16 fences + sign bit for FENCE_REG_NONE */
#define I915_MAX_NUM_FENCE_BITS 5

struct drm_i915_fence_reg {
	struct list_head lru_list;
	struct drm_i915_gem_object *obj;
	int pin_count;
};

struct sdvo_device_mapping {
	u8 initialized;
	u8 dvo_port;
	u8 slave_addr;
	u8 dvo_wiring;
	u8 i2c_pin;
	u8 ddc_pin;
};

d436 4
a439 4
#define QUIRK_PIPEA_FORCE	(1<<0)
#define QUIRK_LVDS_SSC_DISABLE	(1<<1)
#define QUIRK_INVERT_BRIGHTNESS	(1<<2)
#define QUIRK_NO_PCH_PWM_ENABLE	(1<<3)
d442 1
d451 1
a451 1
	struct inteldrm_softc *dev_priv;
d651 13
a668 8
/*
 * lock ordering:
 * exec lock,
 * request lock
 * list lock.
 *
 * XXX fence lock ,object lock
 */
d670 8
a677 24
	struct device		 sc_dev;
	struct drm_device	*dev;
	bus_dma_tag_t		 dmat;
	bus_space_tag_t		 bst;
	struct agp_map		*agph;
	bus_space_handle_t	 opregion_ioh;

	u_long			 flags;

	struct intel_gmbus	 gmbus[GMBUS_NUM_PORTS];

	pci_chipset_tag_t	 pc;
	pcitag_t		 tag;
	pci_intr_handle_t	 ih;
	void			*irqh;

	struct vga_pci_bar	*regs;

	int			 nscreens;
	void			(*switchcb)(void *, int, int);
	void			*switchcbarg;
	void			*switchcookie;
	struct task		 switchtask;
	struct rasops_info	 ro;
d679 15
a693 2
	int	sc_offset;
	int	(*sc_copyrows)(void *, int, int, int);
d695 2
a696 1
	uint32_t		 gpio_mmio_base;
d707 12
a718 1
	drm_i915_sarea_t *sarea_priv;
d722 4
a725 2
	struct drm_dmamem *status_page_dmah;

d736 1
a736 1
	struct vm_page		*pgs;
d738 4
a741 5
	/* Protects user_irq_refcount and irq_mask reg */
	struct mutex		 irq_lock;
	/* Refcount for user irq, only enabled when needed */
	int			 user_irq_refcount;
	u_int32_t		 irq_mask;
d744 7
a750 6
	struct mutex		 dpio_lock;
	/* Cached value of IMR to avoid reads in updating the bitfield */
	u_int32_t		 pipestat[2];
	/* these two  ironlake only, we should union this with pipestat XXX */
	u_int32_t		 gt_irq_mask;
	u_int32_t		 pch_irq_mask;
d752 2
a753 2
	u_int32_t		 hotplug_supported_mask;
	struct task	 	 hotplug_task;
d755 1
a755 1
	int			 num_pch_pll;
d757 15
a771 1
	struct intel_opregion	 opregion;
d773 1
a773 1
	int crt_ddc_pin;
d776 1
a776 1
        struct intel_overlay *overlay;
d809 4
a812 2
	struct intel_pch_pll	 pch_plls[I915_NUM_PLLS];
	struct intel_ddi_plls	 ddi_plls;
d814 1
a814 7
	/* Reclocking support */
	bool			 render_reclock_avail;
	bool			 lvds_downclock_avail;
	int			 lvds_downclock;
	bool			 busy;
	int			 child_dev_num;
	struct child_device_config *child_dev;
d816 6
a821 3
	struct drm_i915_fence_reg fence_regs[16]; /* 965 */
	int			 fence_reg_start; /* 4 by default */
	int			 num_fence_regs; /* 8 pre-965, 16 post */
d824 1
a824 1
	int			 entries;
d826 2
a827 1
	struct task error_task;
d829 1
a832 4
	struct drm_i915_display_funcs display;

	struct timeout idle_timeout;

d841 2
d859 5
d865 7
a871 1
		bus_addr_t gtt_base_addr;
d874 1
a874 2
		 * List of objects currently involved in rendering from the
		 * ringbuffer.
d877 1
a877 1
		 * flushed, not necessarily primitives. last_rendering_seqno
d888 1
a888 1
		 * last_rendering_seqno is 0 while an object is in this list
d896 1
a896 1
		/* Fence LRU */
d904 1
a904 1
		 * fires, go retire requests in a workq.
d907 2
d930 1
a930 1
		 * It prevents command submission from occuring and makes
d933 1
a933 1
		int wedged;
a947 4

		/* for gem retire handler */
		struct taskq *retire_taskq;
		struct task retire_task;
d950 1
a950 10
	/* for hangcheck */
#define DRM_I915_HANGCHECK_PERIOD 1500 /* in ms */
	struct timeout hangcheck_timer;
	int hangcheck_count;
	uint32_t last_acthd[I915_NUM_RINGS];
	uint32_t prev_instdone[I915_NUM_INSTDONE_REG];

	const struct intel_device_info *info;

	int relative_constants_mode;
d960 1
a960 1
	int pending_flip_queue;
d962 2
a963 1
	bool flip_pending_is_done;
d965 8
a972 8
	struct drm_connector *int_lvds_connector;
	struct drm_connector *int_edp_connector;

	u8 cur_delay;
	u8 min_delay;
	u8 max_delay;
	u8 fmax;
	u8 fstart;
d974 1
a974 9
	u64 last_count1;
	unsigned long last_time1;
	unsigned long chipset_power;
	u64 last_count2;
//	struct timespec last_time2;
	unsigned long gfx_power;
	int c_m;
	int r_t;
	u8 corr;
d987 2
a988 7
	unsigned int stop_rings;

	unsigned long cfb_size;
	unsigned int cfb_fb;
	int cfb_plane;
	int cfb_y;
	struct intel_fbc_work *fbc_work;
d990 1
a990 1
	unsigned int fsb_freq, mem_freq, is_ddr3;
d992 2
a993 2
	int error_completion;
	struct mutex error_completion_lock;
d995 7
a1001 1
	time_t last_gpu_reset;
d1003 1
a1003 1
	struct intel_fbdev *fbdev;
d1014 4
a1018 1
#define drm_i915_private inteldrm_softc
d1034 1
a1034 1
	I915_CACHE_NONE,
d1036 1
a1036 1
	I915_CACHE_LLC_MLC, /* gen6+ */
a1056 36
struct inteldrm_file {
	struct drm_file	file_priv;
	struct {
	} mm;
};

/* chip type flags */
#define CHIP_I830	0x00001
#define CHIP_I845G	0x00002
#define CHIP_I85X	0x00004
#define CHIP_I865G	0x00008
#define CHIP_I9XX	0x00010
#define CHIP_I915G	0x00020
#define CHIP_I915GM	0x00040
#define CHIP_I945G	0x00080
#define CHIP_I945GM	0x00100
#define CHIP_I965	0x00200
#define CHIP_I965GM	0x00400
#define CHIP_G33	0x00800
#define CHIP_GM45	0x01000
#define CHIP_G4X	0x02000
#define CHIP_M		0x04000
#define CHIP_HWS	0x08000
#define CHIP_GEN2	0x10000
#define CHIP_GEN3	0x20000
#define CHIP_GEN4	0x40000
#define CHIP_GEN6	0x80000
#define	CHIP_PINEVIEW	0x100000
#define	CHIP_IRONLAKE	0x200000
#define CHIP_IRONLAKE_D	0x400000
#define CHIP_IRONLAKE_M	0x800000
#define CHIP_SANDYBRIDGE	0x1000000
#define CHIP_IVYBRIDGE	0x2000000
#define CHIP_GEN7	0x4000000

/** driver private structure attached to each drm_gem_object */
d1066 1
a1066 1
	/** This object's place on the active/flushing/inactive lists */
d1151 4
d1195 1
a1195 1
	int pending_flip;
a1200 9
struct drm_i915_file_private {
	struct {
		struct mutex lock;
		struct list_head request_list;
	} mm;
	SPLAY_HEAD(i915_ctx_tree, i915_ctx_handle) ctx_tree;
	uint32_t ctx_id;
};

a1211 1
	struct list_head			list;
d1213 2
a1214 1
	struct intel_ring_buffer	*ring;
d1216 2
a1217 1
	uint32_t			seqno;
d1219 9
a1227 4
	uint32_t			tail;
	/** Time at which this request was emitted, in ticks. */
	unsigned long			emitted_ticks;
	struct drm_i915_file_private	*file_priv;
d1229 1
a1229 1
	struct list_head		client_list;
d1232 8
a1239 12
u_int32_t	inteldrm_read_hws(struct inteldrm_softc *, int);
int		intel_ring_begin(struct intel_ring_buffer *, int);
void		intel_ring_emit(struct intel_ring_buffer *, u_int32_t);
void		intel_ring_advance(struct intel_ring_buffer *);
void		inteldrm_update_ring(struct intel_ring_buffer *);
int		inteldrm_pipe_enabled(struct inteldrm_softc *, int);
int		i915_init_phys_hws(struct inteldrm_softc *, bus_dma_tag_t);

unsigned long	i915_chipset_val(struct inteldrm_softc *dev_priv);
unsigned long	i915_mch_val(struct inteldrm_softc *dev_priv);
unsigned long	i915_gfx_val(struct inteldrm_softc *dev_priv);
void		i915_update_gfx_val(struct inteldrm_softc *);
d1241 1
a1241 2
int		intel_init_render_ring_buffer(struct drm_device *);
void		intel_cleanup_ring_buffer(struct intel_ring_buffer *);
d1243 30
a1272 1
/* i915_irq.c */
d1274 17
a1290 10
extern int i915_driver_irq_install(struct drm_device * dev);
extern void i915_driver_irq_uninstall(struct drm_device * dev);
extern void i915_user_irq_get(struct inteldrm_softc *);
extern void i915_user_irq_put(struct inteldrm_softc *);
void	i915_enable_pipestat(struct inteldrm_softc *, int, u_int32_t);
void	i915_disable_pipestat(struct inteldrm_softc *, int, u_int32_t);
void	intel_irq_init(struct drm_device *dev);
void	i915_hangcheck_elapsed(void *);
void	i915_handle_error(struct drm_device *dev, bool wedged);
void	intel_enable_asle(struct drm_device *);
d1292 2
a1293 3
extern void intel_pm_init(struct drm_device *dev);
extern void intel_gt_init(struct drm_device *dev);
extern void intel_gt_sanitize(struct drm_device *dev);
d1295 2
a1296 75
/* gem */
/* Ioctls */
int	i915_gem_init_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_create_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_pread_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_pwrite_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_set_domain_ioctl(struct drm_device *, void *,
	    struct drm_file *);
int	i915_gem_execbuffer2(struct drm_device *, void *, struct drm_file *);
int	i915_gem_pin_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_unpin_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_busy_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_entervt_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_leavevt_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_get_aperture_ioctl(struct drm_device *, void *,
	    struct drm_file *);
int	i915_gem_set_tiling(struct drm_device *, void *, struct drm_file *);
int	i915_gem_get_tiling(struct drm_device *, void *, struct drm_file *);
int	i915_gem_mmap_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_mmap_gtt_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_madvise_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_sw_finish_ioctl(struct drm_device *, void *, struct drm_file *);
int	i915_gem_get_caching_ioctl(struct drm_device *, void *,
	    struct drm_file *);
int	i915_gem_set_caching_ioctl(struct drm_device *, void *,
	    struct drm_file *);
int	i915_gem_wait_ioctl(struct drm_device *, void *, struct drm_file *);

/* GEM memory manager functions */
int	i915_gem_init_object(struct drm_gem_object *);
void	i915_gem_free_object(struct drm_gem_object *);
int	i915_gem_object_pin(struct drm_i915_gem_object *, uint32_t, bool, bool);
void	i915_gem_object_unpin(struct drm_i915_gem_object *);
void	i915_gem_retire_requests(struct drm_device *);
void	i915_gem_retire_requests_ring(struct intel_ring_buffer *);
int	i915_gem_check_wedge(struct inteldrm_softc *,
			     bool interruptible);

void	i915_gem_retire_work_handler(void *);
int	i915_gem_idle(struct drm_device *);
void	i915_gem_object_move_to_active(struct drm_i915_gem_object *,
	    struct intel_ring_buffer *);
int	i915_add_request(struct intel_ring_buffer *, struct drm_file *, u32 *);
void	i915_gem_init_swizzling(struct drm_device *);
void	i915_gem_cleanup_ringbuffer(struct drm_device *);
int	i915_gem_ring_throttle(struct drm_device *, struct drm_file *);
void	i915_dispatch_gem_execbuffer(struct intel_ring_buffer *,
	    struct drm_i915_gem_execbuffer2 *, uint64_t);

extern int i915_gem_get_seqno(struct drm_device *, u32 *);

int	i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *,
	    bool);
int	i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *,
	    u32, struct intel_ring_buffer *);
int	i915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *,
	    bool);

int	i915_gem_init(struct drm_device *);
int	i915_gem_mmap_gtt(struct drm_file *, struct drm_device *,
	    uint32_t, uint64_t *);
int	i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
	    enum i915_cache_level cache_level);

int	i915_gem_object_finish_gpu(struct drm_i915_gem_object *);
int	i915_gem_init_hw(struct drm_device *);

/* Debug functions, mostly called from ddb */
void	i915_gem_seqno_info(int);
void	i915_interrupt_info(int);
void	i915_gem_fence_regs_info(int);
void	i915_hws_info(int);
void	i915_batchbuffer_info(int);
void	i915_ringbuffer_data(int);
void	i915_ringbuffer_info(int);
d1298 2
a1299 3
int i915_gem_object_unbind(struct drm_i915_gem_object *);
int i915_wait_seqno(struct intel_ring_buffer *, uint32_t);
#define I915_GEM_GPU_DOMAINS	(~(I915_GEM_DOMAIN_CPU | I915_GEM_DOMAIN_GTT))
d1301 17
a1317 4
void i915_gem_detach_phys_object(struct drm_device *,
    struct drm_i915_gem_object *);
int i915_gem_attach_phys_object(struct drm_device *dev,
    struct drm_i915_gem_object *, int, int);
d1319 1
a1319 18
int i915_gem_dumb_create(struct drm_file *, struct drm_device *,
    struct drm_mode_create_dumb *);
int i915_gem_mmap_gtt(struct drm_file *, struct drm_device *,
    uint32_t, uint64_t *);
int i915_gem_dumb_destroy(struct drm_file *, struct drm_device *,
    uint32_t);

/* i915_dma.c */
void	i915_driver_lastclose(struct drm_device *);
int	intel_setup_mchbar(struct inteldrm_softc *,
	    struct pci_attach_args *);
void	intel_teardown_mchbar(struct inteldrm_softc *,
	    struct pci_attach_args *, int);
int	i915_getparam(struct drm_device *, void *, struct drm_file *);
int	i915_setparam(struct drm_device *, void *, struct drm_file *);
void	i915_kernel_lost_context(struct drm_device *);
int	i915_driver_open(struct drm_device *, struct drm_file *);
void	i915_driver_close(struct drm_device *, struct drm_file *);
d1321 345
a1665 6
/* i915_drv.c */
void	i915_gem_chipset_flush(struct drm_device *);
int	intel_gpu_reset(struct drm_device *);
int	i915_reset(struct drm_device *);
void	inteldrm_timeout(void *);
bool	i915_semaphore_is_enabled(struct drm_device *);
d1678 30
d1716 4
d1721 3
a1723 3
void	i915_gem_detect_bit_6_swizzle(struct drm_device *);
void	i915_gem_object_do_bit_17_swizzle(struct drm_i915_gem_object *);
void	i915_gem_object_save_bit_17_swizzle(struct drm_i915_gem_object *);
d1726 5
d1732 15
d1749 6
a1754 2
extern int i915_save_state(struct drm_device *);
extern int i915_restore_state(struct drm_device *);
d1757 3
a1759 2
extern int intel_setup_gmbus(struct inteldrm_softc *);
static inline bool intel_gmbus_is_port_valid(unsigned port)
d1764 5
a1768 3
struct i2c_controller *intel_gmbus_get_adapter(drm_i915_private_t *, unsigned);
extern void intel_gmbus_force_bit(struct i2c_controller *, bool);
static inline bool intel_gmbus_is_forced_bit(struct i2c_controller *i2c)
d1772 1
a1772 39

/* i915_gem.c */
int i915_gem_fault(struct drm_gem_object *, struct uvm_faultinfo *, off_t,
    vaddr_t, vm_page_t *, int, int, vm_prot_t, int );
void i915_gem_object_init(struct drm_i915_gem_object *obj,
			 const struct drm_i915_gem_object_ops *ops);
struct drm_i915_gem_object *i915_gem_alloc_object(struct drm_device *dev,
						  size_t size);
int i915_gpu_idle(struct drm_device *);
int i915_gem_object_get_fence(struct drm_i915_gem_object *);
int i915_gem_object_put_fence(struct drm_i915_gem_object *);
void i915_gem_reset(struct drm_device *);
void i915_gem_clflush_object(struct drm_i915_gem_object *);
void i915_gem_release(struct drm_device *, struct drm_file *);
void i915_gem_release_mmap(struct drm_i915_gem_object *);
void i915_gem_load(struct drm_device *dev);

uint32_t
i915_gem_get_unfenced_gtt_alignment(struct drm_device *dev,
				    uint32_t size,
				    int tiling_mode);

int i915_gem_object_sync(struct drm_i915_gem_object *,
    struct intel_ring_buffer *);

int i915_gem_object_get_pages(struct drm_i915_gem_object *obj);

static inline void i915_gem_object_pin_pages(struct drm_i915_gem_object *obj)
{
	BUG_ON(obj->pages == NULL);
	obj->pages_pin_count++;
}
static inline void i915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)
{
	BUG_ON(obj->pages_pin_count == 0);
	obj->pages_pin_count--;
}

int i915_mutex_lock_interruptible(struct drm_device *dev);
d1775 2
a1776 2
int intel_opregion_setup(struct drm_device *dev);
#if NACPI > 0
d1790 8
a1797 14
/* i915_gem_gtt.c */
void i915_gem_cleanup_aliasing_ppgtt(struct drm_device *dev);
void i915_gem_restore_gtt_mappings(struct drm_device *dev);
int __must_check i915_gem_gtt_prepare_object(struct drm_i915_gem_object *obj);
void i915_gem_gtt_bind_object(struct drm_i915_gem_object *obj,
				enum i915_cache_level cache_level);
void i915_gem_gtt_unbind_object(struct drm_i915_gem_object *obj);
void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj);
void i915_gem_init_global_gtt(struct drm_device *dev,
			      unsigned long start,
			      unsigned long mappable_end,
			      unsigned long end);
int i915_gem_gtt_init(struct drm_device *dev);
void i915_gem_gtt_fini(struct drm_device *dev);
d1812 1
a1812 1
extern void intel_detect_pch(struct inteldrm_softc *dev_priv);
d1814 1
a1815 1
extern int intel_enable_rc6(const struct drm_device *dev);
d1817 13
a1829 11
extern struct intel_overlay_error_state *intel_overlay_capture_error_state(
    struct drm_device *dev);
#ifdef notyet
extern void intel_overlay_print_error_state(struct sbuf *m,
    struct intel_overlay_error_state *error);
#endif
extern struct intel_display_error_state *intel_display_capture_error_state(
    struct drm_device *dev);
#ifdef notyet
extern void intel_display_print_error_state(struct sbuf *m,
    struct drm_device *dev, struct intel_display_error_state *error);
d1836 3
a1838 3
void gen6_gt_force_wake_get(struct inteldrm_softc *dev_priv);
void gen6_gt_force_wake_put(struct inteldrm_softc *dev_priv);
int __gen6_gt_wait_for_fifo(struct inteldrm_softc *dev_priv);
d1840 2
a1841 2
int sandybridge_pcode_read(struct inteldrm_softc *dev_priv, u8 mbox, u32 *val);
int sandybridge_pcode_write(struct inteldrm_softc *dev_priv, u8 mbox, u32 val);
a1949 226
#define INTELDRM_VERBOSE 0
#if INTELDRM_VERBOSE > 0
#define	INTELDRM_VPRINTF(fmt, args...) DRM_INFO(fmt, ##args)
#else
#define	INTELDRM_VPRINTF(fmt, args...)
#endif

#define LP_RING(d) (&((struct inteldrm_softc *)(d))->ring[RCS])

#define BEGIN_LP_RING(n) \
	intel_ring_begin(LP_RING(dev_priv), (n))

#define OUT_RING(x) \
	intel_ring_emit(LP_RING(dev_priv), (x))

#define ADVANCE_LP_RING() \
	intel_ring_advance(LP_RING(dev_priv))

/* MCH IFP BARs */
#define	I915_IFPADDR	0x60
#define I965_IFPADDR	0x70

/**
 * Reads a dword out of the status page, which is written to from the command
 * queue by automatic updates, MI_REPORT_HEAD, MI_STORE_DATA_INDEX, or
 * MI_STORE_DATA_IMM.
 *
 * The following dwords have a reserved meaning:
 * 0x00: ISR copy, updated when an ISR bit not set in the HWSTAM changes.
 * 0x04: ring 0 head pointer
 * 0x05: ring 1 head pointer (915-class)
 * 0x06: ring 2 head pointer (915-class)
 * 0x10-0x1b: Context status DWords (GM45)
 * 0x1f: Last written status offset. (GM45)
 *
 * The area from dword 0x20 to 0x3ff is available for driver usage.
 */
#define READ_HWSP(dev_priv, reg)  inteldrm_read_hws(dev_priv, reg)
#define I915_GEM_HWS_INDEX		0x20

#define INTEL_INFO(dev) (((struct inteldrm_softc *) (dev)->dev_private)->info)

/* Chipset type macros */

#define IS_I830(dev)		((dev)->pci_device == 0x3577)
#define IS_845G(dev)		((dev)->pci_device == 0x2562)
#define IS_I85X(dev)		(INTEL_INFO(dev)->is_i85x)
#define IS_I865G(dev)		((dev)->pci_device == 0x2572)
#define IS_I915G(dev)		(INTEL_INFO(dev)->is_i915g)
#define IS_I915GM(dev)		((dev)->pci_device == 0x2592)
#define IS_I945G(dev)		((dev)->pci_device == 0x2772)
#define IS_I945GM(dev)		(INTEL_INFO(dev)->is_i945gm)
#define IS_BROADWATER(dev)	(INTEL_INFO(dev)->is_broadwater)
#define IS_CRESTLINE(dev)	(INTEL_INFO(dev)->is_crestline)
#define IS_GM45(dev)		((dev)->pci_device == 0x2A42)
#define IS_G4X(dev)		(INTEL_INFO(dev)->is_g4x)
#define IS_PINEVIEW_G(dev)	((dev)->pci_device == 0xa001)
#define IS_PINEVIEW_M(dev)	((dev)->pci_device == 0xa011)
#define IS_PINEVIEW(dev)	(INTEL_INFO(dev)->is_pineview)
#define IS_G33(dev)		(INTEL_INFO(dev)->is_g33)
#define IS_IRONLAKE_D(dev)	((dev)->pci_device == 0x0042)
#define IS_IRONLAKE_M(dev)	((dev)->pci_device == 0x0046)
#define IS_IVYBRIDGE(dev)	(INTEL_INFO(dev)->is_ivybridge)
#define IS_IVB_GT1(dev)		((dev)->pci_device == 0x0156 || \
				 (dev)->pci_device == 0x0152 || \
				 (dev)->pci_device == 0x015a)
#define IS_SNB_GT1(dev)		((dev)->pci_device == 0x0102 || \
				 (dev)->pci_device == 0x0106 || \
				 (dev)->pci_device == 0x010A)
#define IS_MOBILE(dev)		(INTEL_INFO(dev)->is_mobile)

#define IS_I9XX(dev)		(INTEL_INFO(dev)->gen >= 3)
#define IS_IRONLAKE(dev)	(INTEL_INFO(dev)->gen == 5)

#define IS_SANDYBRIDGE(dev)	(INTEL_INFO(dev)->gen == 6)
#define IS_SANDYBRIDGE_D(dev)	(IS_SANDYBRIDGE(dev) && \
 (INTEL_INFO(dev)->is_mobile == 0))
#define IS_SANDYBRIDGE_M(dev)	(IS_SANDYBRIDGE(dev) && \
 (INTEL_INFO(dev)->is_mobile == 1))
#define IS_VALLEYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview)
#define IS_HASWELL(dev)	(INTEL_INFO(dev)->is_haswell)
#define IS_ULT(dev)	(IS_HASWELL(dev) && \
			 ((dev)->pci_device & 0xFF00) == 0x0A00)

/*
 * The genX designation typically refers to the render engine, so render
 * capability related checks should use IS_GEN, while display and other checks
 * have their own (e.g. HAS_PCH_SPLIT for ILK+ display, IS_foo for particular
 * chips, etc.).
 */
#define IS_GEN2(dev)		(INTEL_INFO(dev)->gen == 2)
#define IS_GEN3(dev)		(INTEL_INFO(dev)->gen == 3)
#define IS_GEN4(dev)		(INTEL_INFO(dev)->gen == 4)
#define IS_GEN5(dev)		(INTEL_INFO(dev)->gen == 5)
#define IS_GEN6(dev)		(INTEL_INFO(dev)->gen == 6)
#define IS_GEN7(dev)		(INTEL_INFO(dev)->gen == 7)

#define HAS_BSD(dev)		(INTEL_INFO(dev)->has_bsd_ring)
#define HAS_BLT(dev)		(INTEL_INFO(dev)->has_blt_ring)
#define HAS_LLC(dev)		(INTEL_INFO(dev)->has_llc)
#define I915_NEED_GFX_HWS(dev)	(INTEL_INFO(dev)->need_gfx_hws)

#define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 6)
#define HAS_ALIASING_PPGTT(dev)	(INTEL_INFO(dev)->gen >=6 && !IS_VALLEYVIEW(dev))

#define HAS_OVERLAY(dev)	(INTEL_INFO(dev)->has_overlay)
#define OVERLAY_NEEDS_PHYSICAL(dev) \
    (INTEL_INFO(dev)->overlay_needs_physical)

/* Early gen2 have a totally busted CS tlb and require pinned batches. */
#define HAS_BROKEN_CS_TLB(dev)	(IS_I830(dev) || IS_845G(dev))

/*
 * With the 945 and later, Y tiling got adjusted so that it was 32 128-byte
 * rows, which changes the alignment requirements and fence programming.
 */
#define HAS_128_BYTE_Y_TILING(dev) (IS_I9XX(dev) &&	\
	!(IS_I915G(dev) || IS_I915GM(dev)))

#define HAS_RESET(dev)	(INTEL_INFO(dev)->gen >= 4 && \
    (!IS_GEN6(dev)) && (!IS_GEN7(dev)))

#define SUPPORTS_DIGITAL_OUTPUTS(dev)	(!IS_GEN2(dev) && !IS_PINEVIEW(dev))
#define SUPPORTS_INTEGRATED_HDMI(dev)	(IS_G4X(dev) || IS_GEN5(dev))
#define SUPPORTS_INTEGRATED_DP(dev)	(IS_G4X(dev) || IS_GEN5(dev))
#define SUPPORTS_EDP(dev)		(IS_IRONLAKE_M(dev))
#define SUPPORTS_TV(dev)		(INTEL_INFO(dev)->supports_tv)
#define I915_HAS_HOTPLUG(dev)		(INTEL_INFO(dev)->has_hotplug)

#define HAS_FW_BLC(dev)		(INTEL_INFO(dev)->gen > 2)
#define HAS_PIPE_CXSR(dev)	(INTEL_INFO(dev)->has_pipe_cxsr)
#define I915_HAS_FBC(dev)	(INTEL_INFO(dev)->has_fbc)

#define HAS_PIPE_CONTROL(dev)	(INTEL_INFO(dev)->gen >= 5)

#define HAS_DDI(dev)		(IS_HASWELL(dev))

#define INTEL_PCH_DEVICE_ID_MASK		0xff00
#define INTEL_PCH_IBX_DEVICE_ID_TYPE		0x3b00
#define INTEL_PCH_CPT_DEVICE_ID_TYPE		0x1c00
#define INTEL_PCH_PPT_DEVICE_ID_TYPE		0x1e00
#define INTEL_PCH_LPT_DEVICE_ID_TYPE		0x8c00
#define INTEL_PCH_LPT_LP_DEVICE_ID_TYPE		0x9c00

#define INTEL_PCH_TYPE(dev)	(((struct inteldrm_softc *) (dev)->dev_private)->pch_type)
#define HAS_PCH_LPT(dev)	(INTEL_PCH_TYPE(dev) == PCH_LPT)
#define HAS_PCH_CPT(dev)	(INTEL_PCH_TYPE(dev) == PCH_CPT)
#define HAS_PCH_IBX(dev)	(INTEL_PCH_TYPE(dev) == PCH_IBX)
#define HAS_PCH_SPLIT(dev)	(INTEL_PCH_TYPE(dev) != PCH_NONE)

#define HAS_FORCE_WAKE(dev) (INTEL_INFO(dev)->has_force_wake)

#define HAS_L3_GPU_CACHE(dev) (IS_IVYBRIDGE(dev) || IS_HASWELL(dev))

#define PRIMARY_RINGBUFFER_SIZE         (128*1024)

#define  __EXEC_OBJECT_HAS_FENCE (1<<30)

/**
 * RC6 is a special power stage which allows the GPU to enter an very
 * low-voltage mode when idle, using down to 0V while at this stage.  This
 * stage is entered automatically when the GPU is idle when RC6 support is
 * enabled, and as soon as new workload arises GPU wakes up automatically as well.
 * 
 * There are different RC6 modes available in Intel GPU, which differentiate
 * among each other with the latency required to enter and leave RC6 and
 * voltage consumed by the GPU in different states.
 * 
 * The combination of the following flags define which states GPU is allowed
 * to enter, while RC6 is the normal RC6 state, RC6p is the deep RC6, and
 * RC6pp is deepest RC6. Their support by hardware varies according to the
 * GPU, BIOS, chipset and platform. RC6 is usually the safest one and the one
 * which brings the most power savings; deeper states save more power, but
 * require higher latency to switch to and wake up.
 */
#define INTEL_RC6_ENABLE			(1<<0)
#define INTEL_RC6p_ENABLE			(1<<1)
#define INTEL_RC6pp_ENABLE			(1<<2)

extern unsigned int i915_lvds_downclock;
extern int i915_lvds_channel_mode;
extern int i915_panel_use_ssc;
extern int i915_panel_ignore_lid;
extern unsigned int i915_powersave;
extern int i915_semaphores;
extern int i915_vbt_sdvo_panel_type;
extern int i915_enable_rc6;
extern int i915_enable_fbc;
extern bool i915_enable_hangcheck;

/* Inlines */

/**
 * Returns true if seq1 is later than seq2.
 */
static __inline int
i915_seqno_passed(uint32_t seq1, uint32_t seq2)
{
	return ((int32_t)(seq1 - seq2) >= 0);
}

static inline bool
i915_gem_object_pin_fence(struct drm_i915_gem_object *obj)
{
	if (obj->fence_reg != I915_FENCE_REG_NONE) {
		struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
		dev_priv->fence_regs[obj->fence_reg].pin_count++;
		return true;
	} else
		return false;
}

static inline void
i915_gem_object_unpin_fence(struct drm_i915_gem_object *obj)
{
	if (obj->fence_reg != I915_FENCE_REG_NONE) {
		struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
		dev_priv->fence_regs[obj->fence_reg].pin_count--;
	}
}

static inline int
i915_gem_object_is_purgeable(struct drm_i915_gem_object *obj)
{
	return obj->madv == I915_MADV_DONTNEED;
}
@


1.55
log
@remove the second void * argument on tasks.

when workqs were introduced, we provided a second argument so you
could pass a thing and some context to work on it in. there were
very few things that took advantage of the second argument, so when
i introduced pools i suggested removing it. since tasks were meant
to replace workqs, it was requested that we keep the second argument
to make porting from workqs to tasks easier.

now that workqs are gone, i had a look at the use of the second
argument again and found only one good use of it (vdsp(4) on sparc64
if you're interested) and a tiny handful of questionable uses. the
vast majority of tasks only used a single argument. i have since
modified all tasks that used two args to only use one, so now we
can remove the second argument.

so this is a mechanical change. all tasks only passed NULL as their
second argument, so we can just remove it.

ok krw@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.54 2014/12/20 16:34:27 krw Exp $ */
d535 2
a536 2
	struct device		 dev;
	struct device		*drmdev;
@


1.54
log
@Replace switch workq with taskq.

Diff from blambert@@, double ok@@ kettenis.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.53 2014/11/06 05:48:42 jsg Exp $ */
d1183 1
a1183 1
void	i915_gem_retire_work_handler(void *, void*);
@


1.53
log
@Add the required includes for the conditional parts of
struct vga_pci_softc in vga_pcivar.h

Original diff from guenther@@ changed to incorporate feedback
from kettenis@@ and myself.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.52 2014/05/12 19:29:16 kettenis Exp $ */
d556 2
a557 1
	struct workq_task	 switchwqt;
@


1.52
log
@Move GTT management for Sandy Bridge and up into inteldrm(4).  This makes
it possible to use the non-mappable part of the GTT, prepares the way for
using the PPGTT and reduces the diffs with Linux.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.51 2014/03/25 17:44:39 mpi Exp $ */
d39 9
a52 2

#include "acpi.h"
@


1.51
log
@Remove and shuffle some includes to reduce their number since drmP.h
is included by a lot of files.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.50 2014/02/19 01:20:12 jsg Exp $ */
d47 17
d693 2
d1360 2
@


1.50
log
@drm/i915: fix missed hunk after GT access breakage

From Ben Widawsky
991d4b19f95e3baa4297d57413ca64e7caa1d959 in ubuntu 3.8
e1b4d3036c07ff137955fb1c0197ab62534f46ec in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.49 2014/02/19 01:14:41 jsg Exp $ */
d40 1
@


1.49
log
@drm/i915: fix up gt init sequence fallout

From Daniel Vetter
fc832386574c43961b8b0f177d0062132be1d13b in ubuntu 3.8
181d1b9e31c668259d3798c521672afb8edd355c in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.48 2014/01/24 04:05:06 jsg Exp $ */
d1113 1
@


1.48
log
@drm/i915: Move num_pipes to intel info

From Ben Widawsky
7ed1faada973243b6e11fa209ada91c9cc1dab53 in ubuntu 3.8
7eb552aeae058a88eece91b902dd51fde45b1f41 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.47 2014/01/24 03:21:17 jsg Exp $ */
d1114 1
a1114 1
extern void intel_gt_reset(struct drm_device *dev);
@


1.47
log
@drm/i915: quirk no PCH_PWM_ENABLE for Dell XPS13 backlight

From Kamal Mostafa
9f5b330036e6f771438b22770f6c49e2c2eaf0e1 in ubuntu 3.8
e85843bec6c2ea7c10ec61238396891cc2b753a9 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.46 2014/01/23 10:42:57 jsg Exp $ */
d88 1
a88 1
#define for_each_pipe(p) for ((p) = 0; (p) < dev_priv->num_pipe; (p)++)
d179 1
a582 1
	int			 num_pipe;
@


1.46
log
@drm/i915: add HAS_DDI check

From Paulo Zanoni
d99994528e9946a78a505ca752fa3b73aa8c5a76 in ubuntu 3.8
affa935440733a79c5a9eb0e5357e2564ca4b355 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.45 2013/12/11 20:31:43 kettenis Exp $ */
d284 1
@


1.45
log
@Make obj->pages a simple array instead of an array of bus_dma_segment_t's.
Simplifies things a bit and reduces the diffs with Linux a bit too.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.44 2013/12/05 13:29:56 kettenis Exp $ */
d1622 2
@


1.44
log
@Rename 'struct drm_obj' to 'struct drm_gem_object' to reduce the diffs with
Linux.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.43 2013/12/03 16:19:43 kettenis Exp $ */
d1000 1
a1000 1
	bus_dma_segment_t *pages;
@


1.43
log
@Remove some unused cruft.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.42 2013/12/01 20:33:57 kettenis Exp $ */
d910 1
a910 1
	struct drm_obj				 base;
d919 2
a920 2
	struct list_head			 ring_list;
	struct list_head			 mm_list;
d922 1
a922 1
	struct list_head			 exec_list;
d1039 1
a1039 1
	 * Number of crtcs where this object is currently the fb, but   
d1043 1
a1043 1
	int					 pending_flip;
d1047 1
a1047 1
#define to_intel_bo(x) container_of(x,struct drm_i915_gem_object, base)
d1144 2
a1145 2
int	i915_gem_init_object(struct drm_obj *);
void	i915_gem_free_object(struct drm_obj *);
d1272 1
a1272 1
int i915_gem_fault(struct drm_obj *, struct uvm_faultinfo *, off_t,
@


1.42
log
@Remove some prototypes for functions that no longer exist.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.41 2013/12/01 11:47:13 kettenis Exp $ */
a907 5
/* flags we use in drm_obj's do_flags */
#define I915_IN_EXEC		0x0020	/* being processed in execbuffer */
#define I915_USER_PINNED	0x0040	/* BO has been pinned from userland */
#define I915_EXEC_NEEDS_FENCE	0x0800	/* being processed but will need fence*/

a1219 1
void	inteldrm_purge_obj(struct drm_obj *);
@


1.41
log
@Bring back the DRM_IOCTL_I915_GEM_WAIT diff now that I've figured out what
made it fail (WARN_ON() was evaluating its argument multiple times).
This time, also advertise support through I915_PARAM_HAS_WAIT_TIMEOUT.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.40 2013/11/30 20:13:36 kettenis Exp $ */
a1176 1
int	i915_gem_object_flush_gpu_write_domain(struct drm_i915_gem_object *);
a1284 1
void i915_gem_object_move_to_flushing(struct drm_i915_gem_object *);
@


1.40
log
@Oops!  Only intended to commit the i915_dma.c changes in the previous commit.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.38 2013/11/27 20:13:30 kettenis Exp $ */
d1146 1
@


1.39
log
@Reorder some case statements to reduce the diffs with Linux.
@
text
@a1145 1
int	i915_gem_wait_ioctl(struct drm_device *, void *, struct drm_file *);
@


1.38
log
@Add 'struct drm_i915_gem_objects_ops' and use it; reduces diffs with Linux.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.37 2013/11/20 02:03:52 jsg Exp $ */
d1146 1
@


1.37
log
@switch to the drm_mm based gtt eviction code from linux 3.8.13
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.36 2013/11/19 19:14:09 kettenis Exp $ */
d855 18
d917 2
a1148 1
void	i915_gem_object_init(struct drm_i915_gem_object *);
d1280 4
a1283 2
struct drm_i915_gem_object *
    i915_gem_alloc_object(struct drm_device *, size_t);
@


1.36
log
@Move the GTT management into the inteldrm driver.  It is really obvious now
that this is necessary as on some hardware we need guard pages between
regions that have different cache attributes.  Even if this appears to cause
regressions on some hardware, this change is a necessary (but not sufficient)
step to fix the cache coherency problems on the affected hardware.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.35 2013/11/19 15:08:04 jsg Exp $ */
d1225 6
a1230 3
int i915_gem_evict_everything(struct drm_device *);
int i915_gem_evict_something(struct inteldrm_softc *, size_t);
int i915_gem_evict_inactive(struct inteldrm_softc *);
@


1.35
log
@backout the DRM_IOCTL_I915_GEM_WAIT commit
it seems to leave X unuseable on resume on at least snb/ivb
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.34 2013/11/17 20:04:47 kettenis Exp $ */
d251 2
a509 1
	bus_dma_tag_t		 agpdmat; /* tag from intagp for GEM */
d655 4
d899 2
a907 19
	/* GTT binding. */
	bus_dmamap_t				 dmamap;
	/* Current offset of the object in GTT space. */
	bus_addr_t				 gtt_offset;
	struct intel_ring_buffer		*ring;
	u_int32_t				*bit_17;
	/* extra flags to bus_dma */
	int					 dma_flags;
	/* Fence register for this object. needed for tiling. */
	int					 fence_reg;

	/** Breadcrumb of last rendering to the buffer. */
	u_int32_t				 last_read_seqno;
	u_int32_t				 last_write_seqno;
	/** Breadcrumb of last fenced GPU access to the buffer. */
	u_int32_t				 last_fenced_seqno;
	/** Current tiling mode for the object. */
	u_int32_t				 tiling_mode;
	u_int32_t				 stride;
d923 7
d935 4
d983 1
d995 21
d1312 9
a1320 2
void i915_gem_gtt_rebind_object(struct drm_i915_gem_object *obj,
				enum i915_cache_level);
@


1.34
log
@Remove some more dead code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.33 2013/11/17 18:47:13 kettenis Exp $ */
a1104 1
int	i915_gem_wait_ioctl(struct drm_device *, void *, struct drm_file *);
@


1.33
log
@Implement DRM_IOCTL_I915_GEM_WAIT.  Based on an earlier diff from jsg@@

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.32 2013/11/16 16:15:36 kettenis Exp $ */
a1155 5
#ifdef WATCH_INACTIVE
void inteldrm_verify_inactive(struct inteldrm_softc *, char *, int);
#else
#define inteldrm_verify_inactive(dev,file,line)
#endif
@


1.32
log
@Remove some dead code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.31 2013/10/29 06:30:57 jsg Exp $ */
d1105 1
@


1.31
log
@Move most of the uses of workqs in drm to the new task/taskq api.
Prevents unintended multiple additions to workqs that was causing
hangs on radeon, and lets us remove tasks more closely matching
the behaviour of the original linux code.

ok kettenis@@
cause of the ttm/radeon hangs debugged by claudio@@ and kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.30 2013/10/05 07:30:05 jsg Exp $ */
a1124 4
int	i915_gem_get_relocs_from_user(struct drm_i915_gem_exec_object2 *,
	    u_int32_t, struct drm_i915_gem_relocation_entry **);
int	i915_gem_put_relocs_to_user(struct drm_i915_gem_exec_object2 *,
	    u_int32_t, struct drm_i915_gem_relocation_entry *);
a1126 3
int	i915_gem_object_pin_and_relocate(struct drm_obj *,
	    struct drm_file *, struct drm_i915_gem_exec_object2 *,
	    struct drm_i915_gem_relocation_entry *);
@


1.30
log
@add and use gtt mapping flags, further reduces the diff to linux
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.29 2013/09/30 06:47:48 jsg Exp $ */
d39 1
a39 1
#include <sys/workq.h>
d448 1
a448 1
	struct workq_task task;
d460 1
a460 1
	struct workq_task delayed_resume_task;
d494 1
a494 1
	struct workq_task error_task;
a560 1
	struct workq		*workq;
d578 1
a578 1
	struct workq_task	 hotplug_task;
d639 1
a639 1
	struct workq_task error_task;
d743 4
@


1.29
log
@move the read/write functions and macros closer to linux
noteably this uses the gt lock and force wake in some cases
and includes a gen5/ironlake errata.
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.28 2013/09/18 08:50:28 jsg Exp $ */
d978 3
@


1.28
log
@sync the execbuffer relocation code with linux 3.8.13
with the fastpath and cpu relocs disabled for now.
eb_* functions based on code in FreeBSD.

ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.27 2013/08/13 10:23:49 jsg Exp $ */
d1342 21
d1365 1
a1365 1
write64(struct inteldrm_softc *dev_priv, bus_size_t off, u_int64_t reg)
d1368 1
a1368 1
	    off, (u_int32_t)reg);
d1370 8
a1377 1
	    off + 4, upper_32_bits(reg));
d1380 16
a1395 2
static __inline u_int64_t
read64(struct inteldrm_softc *dev_priv, bus_size_t off)
d1400 1
a1400 1
	    dev_priv->regs->bsh, off);
d1402 1
a1402 1
	    dev_priv->regs->bsh, off + 4);
a1406 1
#define I915_READ64(off)	read64(dev_priv, off)
d1408 34
a1441 1
#define I915_WRITE64(off, reg)	write64(dev_priv, off, reg)
d1443 2
a1444 14
#define I915_READ(reg)		bus_space_read_4(dev_priv->regs->bst,	\
				    dev_priv->regs->bsh, (reg))
#define I915_READ_NOTRACE(reg)	I915_READ(reg)
#define I915_WRITE(reg,val)	bus_space_write_4(dev_priv->regs->bst,	\
				    dev_priv->regs->bsh, (reg), (val))
#define I915_WRITE_NOTRACE(reg,val)	I915_WRITE(reg, val)
#define I915_READ16(reg)	bus_space_read_2(dev_priv->regs->bst,	\
				    dev_priv->regs->bsh, (reg))
#define I915_WRITE16(reg,val)	bus_space_write_2(dev_priv->regs->bst,	\
				    dev_priv->regs->bsh, (reg), (val))
#define I915_READ8(reg)		bus_space_read_1(dev_priv->regs->bst,	\
				    dev_priv->regs->bsh, (reg))
#define I915_WRITE8(reg,val)	bus_space_write_1(dev_priv->regs->bst,	\
				    dev_priv->regs->bsh, (reg), (val))
d1446 2
a1447 2
#define POSTING_READ(reg)	(void)I915_READ(reg)
#define POSTING_READ16(reg)	(void)I915_READ16(reg)
@


1.27
log
@add static back to functions that originally had it
reduces the diff to linux and makes ddb hangman a little easier
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.26 2013/08/07 19:49:06 kettenis Exp $ */
d985 1
@


1.26
log
@Another major overhaul of the inteldrm(4) GEM code, bringing us considerably
closer to the Linux 3.8.13 codebase.  Almost certainly squashes a few more
bugs.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.25 2013/08/07 00:04:28 jsg Exp $ */
a1040 1
int		ring_wait_for_space(struct intel_ring_buffer *, int n);
a1052 4
int		intel_init_ring_buffer(struct drm_device *,
		    struct intel_ring_buffer *);
int		init_ring_common(struct intel_ring_buffer *);

a1059 3
extern int i915_enable_vblank(struct drm_device *dev, int crtc);
extern void i915_disable_vblank(struct drm_device *dev, int crtc);
extern u32 i915_get_vblank_counter(struct drm_device *dev, int crtc);
d1066 1
d1101 1
a1113 1
void	i915_gem_object_move_to_inactive(struct drm_i915_gem_object *);
a1114 2
int	init_pipe_control(struct intel_ring_buffer *);
void	cleanup_status_page(struct intel_ring_buffer *);
a1126 3
int	i915_gem_execbuffer_reserve_object(struct drm_i915_gem_object *,
	    struct intel_ring_buffer *);
void	i915_gem_execbuffer_unreserve_object(struct drm_i915_gem_object *);
a1136 1
int	i915_gem_object_wait_rendering(struct drm_i915_gem_object *, bool);
a1143 9
void	sandybridge_write_fence_reg(struct drm_device *, int,
	    struct drm_i915_gem_object *);
void	i965_write_fence_reg(struct drm_device *, int,
	    struct drm_i915_gem_object *);
void	i915_write_fence_reg(struct drm_device *, int,
	    struct drm_i915_gem_object *);
void	i830_write_fence_reg(struct drm_device *, int,
	    struct drm_i915_gem_object *);

a1216 3
int	i915_gem_swizzle_page(struct vm_page *page);
bool	i915_tiling_ok(struct drm_device *, int, int, int);
bool	i915_gem_object_fence_ok(struct drm_i915_gem_object *, int);
a1221 2
extern void i915_save_display(struct drm_device *);
extern void i915_restore_display(struct drm_device *);
a1239 3
int i915_gem_create(struct drm_file *, struct drm_device *, uint64_t,
    uint32_t *);
void init_ring_lists(struct intel_ring_buffer *);
a1245 3
void i915_gem_write_fence(struct drm_device *, int,
    struct drm_i915_gem_object *);
void i915_gem_reset_fences(struct drm_device *);
a1310 1
extern void ironlake_init_pch_refclk(struct drm_device *dev);
a1315 5

extern void __gen6_gt_force_wake_get(struct inteldrm_softc *dev_priv);
extern void __gen6_gt_force_wake_mt_get(struct inteldrm_softc *dev_priv);
extern void __gen6_gt_force_wake_put(struct inteldrm_softc *dev_priv);
extern void __gen6_gt_force_wake_mt_put(struct inteldrm_softc *dev_priv);
@


1.25
log
@add support for hardware contexts on recent intel hardware
based on the code in linux 3.8.13
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.24 2013/07/05 07:20:27 jsg Exp $ */
a562 1
	size_t			 max_gem_obj_size; /* XXX */
d741 3
a899 1
	bus_dma_segment_t			*dma_segs;
a907 4
	/** refcount for times pinned this object in GTT space */
	int					 pin_count;
	/* number of times pinned by pin ioctl. */
	u_int					 user_pin_count;
d945 12
d979 3
d988 4
d1205 2
a1206 2
int	i915_getparam(struct inteldrm_softc *dev_priv, void *data);
int	i915_setparam(struct inteldrm_softc *dev_priv, void *data);
a1211 1
void	inteldrm_set_max_obj_size(struct inteldrm_softc *);
d1294 13
@


1.24
log
@make use of the drm_i915_private macro to reduce the diff to linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.23 2013/07/04 09:52:29 jsg Exp $ */
d206 16
d817 3
d995 2
d1203 11
@


1.23
log
@set CPT FDI RX polarity bits based on VBT
from linux 3.8.13
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.22 2013/05/21 22:12:58 kettenis Exp $ */
d1568 1
a1568 1
		drm_i915_private_t *dev_priv = obj->base.dev->dev_private;
d1579 1
a1579 1
		drm_i915_private_t *dev_priv = obj->base.dev->dev_private;
@


1.22
log
@Delete unused function.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.21 2013/05/18 21:43:42 kettenis Exp $ */
d590 1
@


1.21
log
@Add parameters describing the usable part of the GTT and enable the checks
that use them.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.20 2013/05/11 15:56:28 kettenis Exp $ */
a1173 1
void	inteldrm_wipe_mappings(struct drm_obj *);
@


1.20
log
@Remove some #ifdef 0'd code that we're never going to use again.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.19 2013/05/09 15:00:41 kettenis Exp $ */
d649 5
d722 3
@


1.19
log
@Add a #define such that we can say "struct drm_i915_private"  to reduce the
diffs with Linux.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.18 2013/05/08 23:01:36 kettenis Exp $ */
a1574 14

#if 0
static __inline int
i915_obj_purgeable(struct drm_i915_gem_object *obj_priv)
{
	return (obj_priv->base.do_flags & I915_DONTNEED);
}

static __inline int
i915_obj_purged(struct drm_i915_gem_object *obj_priv)
{
	return (obj_priv->base.do_flags & I915_PURGED);
}
#endif
@


1.18
log
@The "locking" in i915_gem_object_move_to_inactive() makes no sense.  Get rid
of it.  It's implemented using "simple" locks anyway, which are no-ops.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.17 2013/05/05 13:55:36 kettenis Exp $ */
d796 2
a797 1
typedef struct inteldrm_softc drm_i915_private_t;
@


1.17
log
@Add nonblocking argument to i915_gem_object_pin() and
i915_gem_object_bind_to_gtt().
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.16 2013/05/05 13:02:46 kettenis Exp $ */
a1073 1
void	i915_gem_object_move_to_inactive_locked(struct drm_i915_gem_object *);
@


1.16
log
@With KMS, the inteldrm_quiesce dance isn't needed anymore.  Zap the code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.15 2013/04/21 14:41:26 kettenis Exp $ */
d1062 1
a1062 1
int	i915_gem_object_pin(struct drm_i915_gem_object *, uint32_t, bool);
@


1.15
log
@Move GEM initialization code into its own function like Linux has.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.14 2013/04/17 20:04:04 kettenis Exp $ */
a620 4
#define	INTELDRM_QUIET		0x01 /* suspend close, get off the hardware */
#define	INTELDRM_WEDGED		0x02 /* chipset hung pending reset */
#define	INTELDRM_SUSPENDED	0x04 /* in vt switch, no commands */
	int			 sc_flags; /* quiet, suspended, hung */
@


1.14
log
@Another round of reducing diffs with Linux code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.13 2013/04/14 19:04:37 kettenis Exp $ */
d646 6
d1185 1
a1185 2
void	i915_gem_detect_bit_6_swizzle(struct inteldrm_softc *, 
	    struct pci_attach_args *);
d1234 1
@


1.13
log
@Take a different approach towards framebuffer accelartion.  Instead of using
the blitter, scroll by double-mapping the framebuffer and reprogramming the
registers that determine the first visible pixel, much in the same way as the
vga text console uses the 6845.  This makes scrolling very fast, and since we
no longer need to issue commands to any of the rings, we can enable this when
X is running and safely scroll when printing panic messages or if we've
entered ddb.

Testes by many.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.12 2013/04/03 07:36:57 jsg Exp $ */
d530 1
a530 1
	struct intel_ring_buffer rings[I915_NUM_RINGS];
d799 1
a799 1
		if (((ring__) = &(dev_priv__)->rings[(i__)]), intel_ring_initialized((ring__)))
d1062 1
a1062 2
void	i915_gem_retire_request(struct inteldrm_softc *,
	    struct drm_i915_gem_request *);
d1068 1
a1068 1
int	i915_gem_idle(struct inteldrm_softc *);
a1134 1
void i915_gem_retire_requests(struct inteldrm_softc *);
d1174 1
a1174 1
int i915_gem_evict_everything(struct inteldrm_softc *);
d1362 1
a1362 1
#define LP_RING(d) (&((struct inteldrm_softc *)(d))->rings[RCS])
@


1.12
log
@move i915_gem_find_inactive_object() into i915_gem_evict.c
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.11 2013/03/31 11:43:23 kettenis Exp $ */
d515 2
a516 5
	int	noaccel;
	int	(*noaccel_copycols)(void *, int, int, int, int);
	int	(*noaccel_erasecols)(void *, int, int, int, long);
	int	(*noaccel_copyrows)(void *, int, int, int);
	int	(*noaccel_eraserows)(void *, int, int, long);
@


1.11
log
@Add bit banging support.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.10 2013/03/30 04:57:53 jsg Exp $ */
a1095 3
struct drm_obj	*i915_gem_find_inactive_object(struct inteldrm_softc *,
		     size_t);

a1139 2
struct drm_obj  *i915_gem_find_inactive_object(struct inteldrm_softc *,
	size_t);
@


1.10
log
@go back to the old method of execbuffer pinning
should fix problems noticed by Ralf Horstmann and bluhm@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.9 2013/03/28 19:36:14 kettenis Exp $ */
a250 5
struct gmbus_port {
	struct inteldrm_softc *dev_priv;
	int port;
};

d270 7
a276 2
	struct i2c_controller	 controller;
	struct gmbus_port	 gp;
a1207 1
struct i2c_controller *intel_gmbus_get_adapter(drm_i915_private_t *, unsigned);
d1211 7
@


1.9
log
@Add i915_enable_hangcheck parameter.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.8 2013/03/28 11:51:05 jsg Exp $ */
d1091 1
a1091 1
	    struct drm_i915_gem_relocation_entry *, struct intel_ring_buffer *);
@


1.8
log
@add i915_gem_execbuffer_reserve_object and friends and move
the execbuffer pinning closer to linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.7 2013/03/28 05:13:07 jsg Exp $ */
d1545 1
@


1.7
log
@add the ioctls to get/set the caching level of a buffer object
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.6 2013/03/25 19:50:56 kettenis Exp $ */
d866 2
d942 6
d1091 4
a1094 1
	    struct drm_i915_gem_relocation_entry *);
@


1.6
log
@Use the new rasops multiple screen support to provide proper virtual
terminals.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.5 2013/03/22 22:51:00 kettenis Exp $ */
d1047 4
@


1.5
log
@Move i915_gem_gtt_map_ioctl() from i915_drv.c to i915_gem.c and rename it
to i915_gem_mmap_ioctl().
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.4 2013/03/22 06:19:56 jsg Exp $ */
d515 5
a519 2
	int			 noaccel;
	struct wsdisplay_emulops noaccel_ops;
@


1.4
log
@implement DRM_IOCTL_I915_GEM_SW_FINISH
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.3 2013/03/21 08:27:32 jsg Exp $ */
d1040 1
a1040 1
int	i915_gem_gtt_map_ioctl(struct drm_device *, void *, struct drm_file *);
@


1.3
log
@Enable the opregion code but keep the parts that try to talk to acpi
disabled for now.  Makes the brightness keys on my x230 work.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.2 2013/03/20 12:37:41 jsg Exp $ */
d1043 1
@


1.2
log
@Backout some changes introduced in linux 3.8.3 which are known
to cause problems and have been reverted in linux 3.8.4-rc1:

"drm/i915: reorder setup sequence to have irqs for output setup"
"drm/i915: enable irqs earlier when resuming"

ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.1 2013/03/18 12:36:52 jsg Exp $ */
d44 2
d1017 1
d1227 2
a1228 1
extern int intel_opregion_init(struct drm_device *dev);
d1230 10
a1239 2
extern void opregion_asle_intr(struct drm_device *dev);
extern void opregion_enable_asle(struct drm_device *dev);
@


1.1
log
@Significantly increase the wordlist for ddb hangman,
and update our device independent DRM code and the Intel DRM code
to be mostly in sync with Linux 3.8.3.  Among other things this
brings support for kernel modesetting and enables use of
the rings on gen6+ Intel hardware.

Based on some earlier work from matthieu@@ with some hints from FreeBSD
and with lots of help from kettenis@@ (including a beautiful accelerated
wscons framebuffer console!)

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.h,v 1.73 2012/09/25 10:19:46 jsg Exp $ */
a562 1
	bool			 enable_hotplug_processing;
@

