head	1.14;
access;
symbols
	OPENBSD_6_1:1.14.0.4
	OPENBSD_6_1_BASE:1.14
	OPENBSD_6_0:1.14.0.6
	OPENBSD_6_0_BASE:1.14
	OPENBSD_5_9:1.14.0.2
	OPENBSD_5_9_BASE:1.14
	OPENBSD_5_8:1.9.0.8
	OPENBSD_5_8_BASE:1.9
	OPENBSD_5_7:1.9.0.6
	OPENBSD_5_7_BASE:1.9
	OPENBSD_5_6:1.9.0.4
	OPENBSD_5_6_BASE:1.9
	OPENBSD_5_5:1.8.0.4
	OPENBSD_5_5_BASE:1.8
	OPENBSD_5_4:1.1.0.2
	OPENBSD_5_4_BASE:1.1;
locks; strict;
comment	@ * @;


1.14
date	2015.10.18.18.00.45;	author kettenis;	state Exp;
branches;
next	1.13;
commitid	xaeWBOzI0eLbZgNM;

1.13
date	2015.09.26.13.15.25;	author kettenis;	state Exp;
branches;
next	1.12;
commitid	YG756ccQGEw214S0;

1.12
date	2015.09.26.11.17.15;	author kettenis;	state Exp;
branches;
next	1.11;
commitid	DUG1LQonw3dWeI9h;

1.11
date	2015.09.25.16.15.19;	author jsg;	state Exp;
branches;
next	1.10;
commitid	qBczsgFZrSbAS824;

1.10
date	2015.09.23.23.12.12;	author kettenis;	state Exp;
branches;
next	1.9;
commitid	lQlppvmETCN49oZe;

1.9
date	2014.05.12.19.29.16;	author kettenis;	state Exp;
branches;
next	1.8;

1.8
date	2013.12.11.20.31.43;	author kettenis;	state Exp;
branches;
next	1.7;

1.7
date	2013.12.07.10.53.29;	author kettenis;	state Exp;
branches;
next	1.6;

1.6
date	2013.12.01.20.19.15;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2013.11.19.19.14.09;	author kettenis;	state Exp;
branches;
next	1.4;

1.4
date	2013.10.05.07.30.06;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2013.08.13.10.23.49;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2013.08.09.07.55.42;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2013.03.18.12.36.52;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.14
log
@Backport another Broadwell fix from Linux 3.15

Ben Widawsky
drm/i915: Provide PDP updates via MMIO
e178f7057b81c87a7ceaae0ca204487b6f7eedcf

Doesn't make resume work, but at least it prevents the machine from hanging
and/or resetting.
@
text
@/*	$OpenBSD: i915_gem_gtt.c,v 1.13 2015/09/26 13:15:25 kettenis Exp $	*/
/*
 * Copyright Â© 2010 Daniel Vetter
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 *
 */

#include <dev/pci/drm/drmP.h>
#include <dev/pci/drm/i915_drm.h>
#include "i915_drv.h"
#include "i915_trace.h"
#include "intel_drv.h"

/* XXX */
#define _PAGE_PRESENT	PG_V
#define _PAGE_RW	PG_RW
#define _PAGE_PAT	PG_PAT
#define _PAGE_PWT	PG_WT
#define _PAGE_PCD	PG_N

static void gen8_setup_private_ppat(struct drm_i915_private *dev_priv);

#define GEN6_PPGTT_PD_ENTRIES 512
#define I915_PPGTT_PT_ENTRIES (PAGE_SIZE / sizeof(gen6_gtt_pte_t))
typedef uint64_t gen8_gtt_pte_t;
typedef gen8_gtt_pte_t gen8_ppgtt_pde_t;

/* PPGTT stuff */
#define GEN6_GTT_ADDR_ENCODE(addr)	((addr) | (((addr) >> 28) & 0xff0))
#define HSW_GTT_ADDR_ENCODE(addr)	((addr) | (((addr) >> 28) & 0x7f0))

#define GEN6_PDE_VALID			(1 << 0)
/* gen6+ has bit 11-4 for physical addr bit 39-32 */
#define GEN6_PDE_ADDR_ENCODE(addr)	GEN6_GTT_ADDR_ENCODE(addr)

#define GEN6_PTE_VALID			(1 << 0)
#define GEN6_PTE_UNCACHED		(1 << 1)
#define HSW_PTE_UNCACHED		(0)
#define GEN6_PTE_CACHE_LLC		(2 << 1)
#define GEN7_PTE_CACHE_L3_LLC		(3 << 1)
#define GEN6_PTE_ADDR_ENCODE(addr)	GEN6_GTT_ADDR_ENCODE(addr)
#define HSW_PTE_ADDR_ENCODE(addr)	HSW_GTT_ADDR_ENCODE(addr)

/* Cacheability Control is a 4-bit value. The low three bits are stored in *
 * bits 3:1 of the PTE, while the fourth bit is stored in bit 11 of the PTE.
 */
#define HSW_CACHEABILITY_CONTROL(bits)	((((bits) & 0x7) << 1) | \
					 (((bits) & 0x8) << (11 - 3)))
#define HSW_WB_LLC_AGE3			HSW_CACHEABILITY_CONTROL(0x2)
#define HSW_WB_LLC_AGE0			HSW_CACHEABILITY_CONTROL(0x3)
#define HSW_WB_ELLC_LLC_AGE0		HSW_CACHEABILITY_CONTROL(0xb)
#define HSW_WB_ELLC_LLC_AGE3		HSW_CACHEABILITY_CONTROL(0x8)
#define HSW_WT_ELLC_LLC_AGE0		HSW_CACHEABILITY_CONTROL(0x6)
#define HSW_WT_ELLC_LLC_AGE3		HSW_CACHEABILITY_CONTROL(0x7)

#define GEN8_PTES_PER_PAGE		(PAGE_SIZE / sizeof(gen8_gtt_pte_t))
#define GEN8_PDES_PER_PAGE		(PAGE_SIZE / sizeof(gen8_ppgtt_pde_t))
#define GEN8_LEGACY_PDPS		4

#define PPAT_UNCACHED_INDEX		(_PAGE_PWT | _PAGE_PCD)
#define PPAT_CACHED_PDE_INDEX		0 /* WB LLC */
#define PPAT_CACHED_INDEX		_PAGE_PAT /* WB LLCeLLC */
#define PPAT_DISPLAY_ELLC_INDEX		_PAGE_PCD /* WT eLLC */

static inline gen8_gtt_pte_t gen8_pte_encode(dma_addr_t addr,
					     enum i915_cache_level level,
					     bool valid)
{
	gen8_gtt_pte_t pte = valid ? _PAGE_PRESENT | _PAGE_RW : 0;
	pte |= addr;
	if (level != I915_CACHE_NONE)
		pte |= PPAT_CACHED_INDEX;
	else
		pte |= PPAT_UNCACHED_INDEX;
	return pte;
}

static inline gen8_ppgtt_pde_t gen8_pde_encode(struct drm_device *dev,
					     dma_addr_t addr,
					     enum i915_cache_level level)
{
	gen8_ppgtt_pde_t pde = _PAGE_PRESENT | _PAGE_RW;
	pde |= addr;
	if (level != I915_CACHE_NONE)
		pde |= PPAT_CACHED_PDE_INDEX;
	else
		pde |= PPAT_UNCACHED_INDEX;
	return pde;
}

static gen6_gtt_pte_t snb_pte_encode(dma_addr_t addr,
				     enum i915_cache_level level,
				     bool valid)
{
	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
	pte |= GEN6_PTE_ADDR_ENCODE(addr);

	switch (level) {
	case I915_CACHE_L3_LLC:
	case I915_CACHE_LLC:
		pte |= GEN6_PTE_CACHE_LLC;
		break;
	case I915_CACHE_NONE:
		pte |= GEN6_PTE_UNCACHED;
		break;
	default:
		WARN_ON(1);
	}

	return pte;
}

static gen6_gtt_pte_t ivb_pte_encode(dma_addr_t addr,
				     enum i915_cache_level level,
				     bool valid)
{
	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
	pte |= GEN6_PTE_ADDR_ENCODE(addr);

	switch (level) {
	case I915_CACHE_L3_LLC:
		pte |= GEN7_PTE_CACHE_L3_LLC;
		break;
	case I915_CACHE_LLC:
		pte |= GEN6_PTE_CACHE_LLC;
		break;
	case I915_CACHE_NONE:
		pte |= GEN6_PTE_UNCACHED;
		break;
	default:
		WARN_ON(1);
	}

	return pte;
}

#define BYT_PTE_WRITEABLE		(1 << 1)
#define BYT_PTE_SNOOPED_BY_CPU_CACHES	(1 << 2)

static gen6_gtt_pte_t byt_pte_encode(dma_addr_t addr,
				     enum i915_cache_level level,
				     bool valid)
{
	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
	pte |= GEN6_PTE_ADDR_ENCODE(addr);

	/* Mark the page as writeable.  Other platforms don't have a
	 * setting for read-only/writable, so this matches that behavior.
	 */
	pte |= BYT_PTE_WRITEABLE;

	if (level != I915_CACHE_NONE)
		pte |= BYT_PTE_SNOOPED_BY_CPU_CACHES;

	return pte;
}

static gen6_gtt_pte_t hsw_pte_encode(dma_addr_t addr,
				     enum i915_cache_level level,
				     bool valid)
{
	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
	pte |= HSW_PTE_ADDR_ENCODE(addr);

	if (level != I915_CACHE_NONE)
		pte |= HSW_WB_LLC_AGE3;

	return pte;
}

static gen6_gtt_pte_t iris_pte_encode(dma_addr_t addr,
				      enum i915_cache_level level,
				      bool valid)
{
	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
	pte |= HSW_PTE_ADDR_ENCODE(addr);

	switch (level) {
	case I915_CACHE_NONE:
		break;
	case I915_CACHE_WT:
		pte |= HSW_WT_ELLC_LLC_AGE3;
		break;
	default:
		pte |= HSW_WB_ELLC_LLC_AGE3;
		break;
	}

	return pte;
}

/* Broadwell Page Directory Pointer Descriptors */
static int gen8_write_pdp(struct intel_ring_buffer *ring, unsigned entry,
			   uint64_t val, bool synchronous)
{
	struct drm_i915_private *dev_priv = ring->dev->dev_private;
	int ret;

	BUG_ON(entry >= 4);

	if (synchronous) {
		I915_WRITE(GEN8_RING_PDP_UDW(ring, entry), val >> 32);
		I915_WRITE(GEN8_RING_PDP_LDW(ring, entry), (u32)val);
		return 0;
	}

	ret = intel_ring_begin(ring, 6);
	if (ret)
		return ret;

	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
	intel_ring_emit(ring, GEN8_RING_PDP_UDW(ring, entry));
	intel_ring_emit(ring, (u32)(val >> 32));
	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
	intel_ring_emit(ring, GEN8_RING_PDP_LDW(ring, entry));
	intel_ring_emit(ring, (u32)(val));
	intel_ring_advance(ring);

	return 0;
}

static int gen8_ppgtt_enable(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct intel_ring_buffer *ring;
	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
	int i, j, ret;

	/* bit of a hack to find the actual last used pd */
	int used_pd = ppgtt->num_pd_entries / GEN8_PDES_PER_PAGE;

	for_each_ring(ring, dev_priv, j) {
		I915_WRITE(RING_MODE_GEN7(ring),
			   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
	}

	for (i = used_pd - 1; i >= 0; i--) {
		dma_addr_t addr = ppgtt->pd_dma_addr[i];
		for_each_ring(ring, dev_priv, j) {
			ret = gen8_write_pdp(ring, i, addr,
					     i915_reset_in_progress(&dev_priv->gpu_error));
			if (ret)
				goto err_out;
		}
	}
	return 0;

err_out:
	for_each_ring(ring, dev_priv, j)
		I915_WRITE(RING_MODE_GEN7(ring),
			   _MASKED_BIT_DISABLE(GFX_PPGTT_ENABLE));
	return ret;
}

static void gen8_ppgtt_clear_range(struct i915_address_space *vm,
				   unsigned first_entry,
				   unsigned num_entries,
				   bool use_scratch)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	gen8_gtt_pte_t *pt_vaddr, scratch_pte;
	unsigned act_pt = first_entry / GEN8_PTES_PER_PAGE;
	unsigned first_pte = first_entry % GEN8_PTES_PER_PAGE;
	unsigned last_pte, i;

	scratch_pte = gen8_pte_encode(ppgtt->base.scratch.addr,
				      I915_CACHE_LLC, use_scratch);

	while (num_entries) {
		struct vm_page *page_table = &ppgtt->gen8_pt_pages[act_pt];

		last_pte = first_pte + num_entries;
		if (last_pte > GEN8_PTES_PER_PAGE)
			last_pte = GEN8_PTES_PER_PAGE;

		pt_vaddr = kmap_atomic(page_table);

		for (i = first_pte; i < last_pte; i++)
			pt_vaddr[i] = scratch_pte;

		kunmap_atomic(pt_vaddr);

		num_entries -= last_pte - first_pte;
		first_pte = 0;
		act_pt++;
	}
}

#ifdef __linux__
static void gen8_ppgtt_insert_entries(struct i915_address_space *vm,
				      struct sg_table *pages,
				      unsigned first_entry,
				      enum i915_cache_level cache_level)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	gen8_gtt_pte_t *pt_vaddr;
	unsigned act_pt = first_entry / GEN8_PTES_PER_PAGE;
	unsigned act_pte = first_entry % GEN8_PTES_PER_PAGE;
	struct sg_page_iter sg_iter;

	pt_vaddr = NULL;
	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
		if (pt_vaddr == NULL)
			pt_vaddr = kmap_atomic(&ppgtt->gen8_pt_pages[act_pt]);

		pt_vaddr[act_pte] =
			gen8_pte_encode(sg_page_iter_dma_address(&sg_iter),
					cache_level, true);
		if (++act_pte == GEN8_PTES_PER_PAGE) {
			kunmap_atomic(pt_vaddr);
			pt_vaddr = NULL;
			act_pt++;
			act_pte = 0;
		}
	}
	if (pt_vaddr)
		kunmap_atomic(pt_vaddr);
}
#else
static void gen8_ppgtt_insert_entries(struct i915_address_space *vm,
				      struct vm_page **pages,
				      unsigned int num_entries,
				      unsigned first_entry,
				      enum i915_cache_level cache_level)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	gen8_gtt_pte_t *pt_vaddr;
	unsigned act_pt = first_entry / GEN8_PTES_PER_PAGE;
	unsigned act_pte = first_entry % GEN8_PTES_PER_PAGE;
	int i;

	pt_vaddr = NULL;
	for (i = 0; i < num_entries; i++) {
		if (pt_vaddr == NULL)
			pt_vaddr = kmap_atomic(&ppgtt->gen8_pt_pages[act_pt]);

		pt_vaddr[act_pte] =
			gen8_pte_encode(VM_PAGE_TO_PHYS(pages[i]),
					cache_level, true);
		if (++act_pte == GEN8_PTES_PER_PAGE) {
			kunmap_atomic(pt_vaddr);
			pt_vaddr = NULL;
			act_pt++;
			act_pte = 0;
		}
	}
	if (pt_vaddr)
		kunmap_atomic(pt_vaddr);
}
#endif

static void gen8_ppgtt_cleanup(struct i915_address_space *vm)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	int i, j;

	drm_mm_takedown(&vm->mm);

	for (i = 0; i < ppgtt->num_pd_pages ; i++) {
		if (ppgtt->pd_dma_addr[i]) {
			pci_unmap_page(ppgtt->base.dev->pdev,
				       ppgtt->pd_dma_addr[i],
				       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);

			for (j = 0; j < GEN8_PDES_PER_PAGE; j++) {
				dma_addr_t addr = ppgtt->gen8_pt_dma_addr[i][j];
				if (addr)
					pci_unmap_page(ppgtt->base.dev->pdev,
						       addr,
						       PAGE_SIZE,
						       PCI_DMA_BIDIRECTIONAL);

			}
		}
		kfree(ppgtt->gen8_pt_dma_addr[i]);
	}

	__free_pages(ppgtt->gen8_pt_pages, get_order(ppgtt->num_pt_pages << PAGE_SHIFT));
	__free_pages(ppgtt->pd_pages, get_order(ppgtt->num_pd_pages << PAGE_SHIFT));
}

/**
 * GEN8 legacy ppgtt programming is accomplished through 4 PDP registers with a
 * net effect resembling a 2-level page table in normal x86 terms. Each PDP
 * represents 1GB of memory
 * 4 * 512 * 512 * 4096 = 4GB legacy 32b address space.
 *
 * TODO: Do something with the size parameter
 **/
static int gen8_ppgtt_init(struct i915_hw_ppgtt *ppgtt, uint64_t size)
{
	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
	struct vm_page *pt_pages;
	int i, j, ret = -ENOMEM;
	const int max_pdp = DIV_ROUND_UP(size, 1 << 30);
	const int num_pt_pages = GEN8_PDES_PER_PAGE * max_pdp;

	if (size % (1<<30))
		DRM_INFO("Pages will be wasted unless GTT size (%llu) is divisible by 1GB\n", size);

	/* FIXME: split allocation into smaller pieces. For now we only ever do
	 * this once, but with full PPGTT, the multiple contiguous allocations
	 * will be bad.
	 */
	ppgtt->pd_pages = alloc_pages(GFP_KERNEL, get_order(max_pdp << PAGE_SHIFT));
	if (!ppgtt->pd_pages)
		return -ENOMEM;

	pt_pages = alloc_pages(GFP_KERNEL, get_order(num_pt_pages << PAGE_SHIFT));
	if (!pt_pages) {
		__free_pages(ppgtt->pd_pages, get_order(max_pdp << PAGE_SHIFT));
		return -ENOMEM;
	}

	ppgtt->gen8_pt_pages = pt_pages;
	ppgtt->num_pd_pages = 1 << get_order(max_pdp << PAGE_SHIFT);
	ppgtt->num_pt_pages = 1 << get_order(num_pt_pages << PAGE_SHIFT);
	ppgtt->num_pd_entries = max_pdp * GEN8_PDES_PER_PAGE;
	ppgtt->enable = gen8_ppgtt_enable;
	ppgtt->base.clear_range = gen8_ppgtt_clear_range;
	ppgtt->base.insert_entries = gen8_ppgtt_insert_entries;
	ppgtt->base.cleanup = gen8_ppgtt_cleanup;
	ppgtt->base.scratch = dev_priv->gtt.base.scratch;
	ppgtt->base.start = 0;
	ppgtt->base.total = ppgtt->num_pt_pages * GEN8_PTES_PER_PAGE * PAGE_SIZE;

	BUG_ON(ppgtt->num_pd_pages > GEN8_LEGACY_PDPS);

	/*
	 * - Create a mapping for the page directories.
	 * - For each page directory:
	 *      allocate space for page table mappings.
	 *      map each page table
	 */
	for (i = 0; i < max_pdp; i++) {
		dma_addr_t temp;
		temp = pci_map_page(ppgtt->base.dev->pdev,
				    &ppgtt->pd_pages[i], 0,
				    PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
		if (pci_dma_mapping_error(ppgtt->base.dev->pdev, temp))
			goto err_out;

		ppgtt->pd_dma_addr[i] = temp;

		ppgtt->gen8_pt_dma_addr[i] = kmalloc(sizeof(dma_addr_t) * GEN8_PDES_PER_PAGE, GFP_KERNEL);
		if (!ppgtt->gen8_pt_dma_addr[i])
			goto err_out;

		for (j = 0; j < GEN8_PDES_PER_PAGE; j++) {
			struct vm_page *p = &pt_pages[i * GEN8_PDES_PER_PAGE + j];
			temp = pci_map_page(ppgtt->base.dev->pdev,
					    p, 0, PAGE_SIZE,
					    PCI_DMA_BIDIRECTIONAL);

			if (pci_dma_mapping_error(ppgtt->base.dev->pdev, temp))
				goto err_out;

			ppgtt->gen8_pt_dma_addr[i][j] = temp;
		}
	}

	/* For now, the PPGTT helper functions all require that the PDEs are
	 * plugged in correctly. So we do that now/here. For aliasing PPGTT, we
	 * will never need to touch the PDEs again */
	for (i = 0; i < max_pdp; i++) {
		gen8_ppgtt_pde_t *pd_vaddr;
		pd_vaddr = kmap_atomic(&ppgtt->pd_pages[i]);
		for (j = 0; j < GEN8_PDES_PER_PAGE; j++) {
			dma_addr_t addr = ppgtt->gen8_pt_dma_addr[i][j];
			pd_vaddr[j] = gen8_pde_encode(ppgtt->base.dev, addr,
						      I915_CACHE_LLC);
		}
		kunmap_atomic(pd_vaddr);
	}

	ppgtt->base.clear_range(&ppgtt->base, 0,
				ppgtt->num_pd_entries * GEN8_PTES_PER_PAGE,
				true);

	DRM_DEBUG_DRIVER("Allocated %d pages for page directories (%d wasted)\n",
			 ppgtt->num_pd_pages, ppgtt->num_pd_pages - max_pdp);
	DRM_DEBUG_DRIVER("Allocated %d pages for page tables (%lld wasted)\n",
			 ppgtt->num_pt_pages,
			 (ppgtt->num_pt_pages - num_pt_pages) +
			 size % (1<<30));
	return 0;

err_out:
	ppgtt->base.cleanup(&ppgtt->base);
	return ret;
}

static void gen6_write_pdes(struct i915_hw_ppgtt *ppgtt)
{
	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
	gen6_gtt_pte_t __iomem *pd_addr;
	uint32_t pd_entry;
	int i;

	WARN_ON(ppgtt->pd_offset & 0x3f);
	pd_addr = (gen6_gtt_pte_t __iomem*)dev_priv->gtt.gsm +
		ppgtt->pd_offset / sizeof(gen6_gtt_pte_t);
	for (i = 0; i < ppgtt->num_pd_entries; i++) {
		dma_addr_t pt_addr;

		pt_addr = ppgtt->pt_dma_addr[i];
		pd_entry = GEN6_PDE_ADDR_ENCODE(pt_addr);
		pd_entry |= GEN6_PDE_VALID;

		writel(pd_entry, pd_addr + i);
	}
	readl(pd_addr);
}

static int gen6_ppgtt_enable(struct drm_device *dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	uint32_t pd_offset;
	struct intel_ring_buffer *ring;
	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
	int i;

	BUG_ON(ppgtt->pd_offset & 0x3f);

	gen6_write_pdes(ppgtt);

	pd_offset = ppgtt->pd_offset;
	pd_offset /= 64; /* in cachelines, */
	pd_offset <<= 16;

	if (INTEL_INFO(dev)->gen == 6) {
		uint32_t ecochk, gab_ctl, ecobits;

		ecobits = I915_READ(GAC_ECO_BITS);
		I915_WRITE(GAC_ECO_BITS, ecobits | ECOBITS_SNB_BIT |
					 ECOBITS_PPGTT_CACHE64B);

		gab_ctl = I915_READ(GAB_CTL);
		I915_WRITE(GAB_CTL, gab_ctl | GAB_CTL_CONT_AFTER_PAGEFAULT);

		ecochk = I915_READ(GAM_ECOCHK);
		I915_WRITE(GAM_ECOCHK, ecochk | ECOCHK_SNB_BIT |
				       ECOCHK_PPGTT_CACHE64B);
		I915_WRITE(GFX_MODE, _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
	} else if (INTEL_INFO(dev)->gen >= 7) {
		uint32_t ecochk, ecobits;

		ecobits = I915_READ(GAC_ECO_BITS);
		I915_WRITE(GAC_ECO_BITS, ecobits | ECOBITS_PPGTT_CACHE64B);

		ecochk = I915_READ(GAM_ECOCHK);
		if (IS_HASWELL(dev)) {
			ecochk |= ECOCHK_PPGTT_WB_HSW;
		} else {
			ecochk |= ECOCHK_PPGTT_LLC_IVB;
			ecochk &= ~ECOCHK_PPGTT_GFDT_IVB;
		}
		I915_WRITE(GAM_ECOCHK, ecochk);
		/* GFX_MODE is per-ring on gen7+ */
	}

	for_each_ring(ring, dev_priv, i) {
		if (INTEL_INFO(dev)->gen >= 7)
			I915_WRITE(RING_MODE_GEN7(ring),
				   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));

		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
		I915_WRITE(RING_PP_DIR_BASE(ring), pd_offset);
	}
	return 0;
}

/* PPGTT support for Sandybdrige/Gen6 and later */
static void gen6_ppgtt_clear_range(struct i915_address_space *vm,
				   unsigned first_entry,
				   unsigned num_entries,
				   bool use_scratch)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	gen6_gtt_pte_t *pt_vaddr, scratch_pte;
	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
	unsigned first_pte = first_entry % I915_PPGTT_PT_ENTRIES;
	unsigned last_pte, i;

	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true);

	while (num_entries) {
		last_pte = first_pte + num_entries;
		if (last_pte > I915_PPGTT_PT_ENTRIES)
			last_pte = I915_PPGTT_PT_ENTRIES;

		pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);

		for (i = first_pte; i < last_pte; i++)
			pt_vaddr[i] = scratch_pte;

		kunmap_atomic(pt_vaddr);

		num_entries -= last_pte - first_pte;
		first_pte = 0;
		act_pt++;
	}
}

#ifdef __linux__
static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
				      struct sg_table *pages,
				      unsigned first_entry,
				      enum i915_cache_level cache_level)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	gen6_gtt_pte_t *pt_vaddr;
	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
	unsigned act_pte = first_entry % I915_PPGTT_PT_ENTRIES;
	struct sg_page_iter sg_iter;

	pt_vaddr = NULL;
	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
		if (pt_vaddr == NULL)
			pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);

		pt_vaddr[act_pte] =
			vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
				       cache_level, true);
		if (++act_pte == I915_PPGTT_PT_ENTRIES) {
			kunmap_atomic(pt_vaddr);
			pt_vaddr = NULL;
			act_pt++;
			act_pte = 0;
		}
	}
	if (pt_vaddr)
		kunmap_atomic(pt_vaddr);
}
#else
static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
				      struct vm_page **pages,
				      unsigned int num_entries,
				      unsigned first_entry,
				      enum i915_cache_level cache_level)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	gen6_gtt_pte_t *pt_vaddr;
	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
	unsigned act_pte = first_entry % I915_PPGTT_PT_ENTRIES;
	int i;

	pt_vaddr = NULL;
	for (i = 0; i < num_entries; i++) {
		if (pt_vaddr == NULL)
			pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);

		pt_vaddr[act_pte] =
			vm->pte_encode(VM_PAGE_TO_PHYS(pages[i]),
				       cache_level, true);
		if (++act_pte == I915_PPGTT_PT_ENTRIES) {
			kunmap_atomic(pt_vaddr);
			pt_vaddr = NULL;
			act_pt++;
			act_pte = 0;
		}
	}
	if (pt_vaddr)
		kunmap_atomic(pt_vaddr);
}
#endif

static void gen6_ppgtt_cleanup(struct i915_address_space *vm)
{
	struct i915_hw_ppgtt *ppgtt =
		container_of(vm, struct i915_hw_ppgtt, base);
	int i;

	drm_mm_takedown(&ppgtt->base.mm);

	if (ppgtt->pt_dma_addr) {
		for (i = 0; i < ppgtt->num_pd_entries; i++)
			pci_unmap_page(ppgtt->base.dev->pdev,
				       ppgtt->pt_dma_addr[i],
				       4096, PCI_DMA_BIDIRECTIONAL);
	}

	kfree(ppgtt->pt_dma_addr);
	for (i = 0; i < ppgtt->num_pd_entries; i++)
		__free_page(ppgtt->pt_pages[i]);
	kfree(ppgtt->pt_pages);
	kfree(ppgtt);
}

static int gen6_ppgtt_init(struct i915_hw_ppgtt *ppgtt)
{
	struct drm_device *dev = ppgtt->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	unsigned first_pd_entry_in_global_pt;
	int i;
	int ret = -ENOMEM;

	/* ppgtt PDEs reside in the global gtt pagetable, which has 512*1024
	 * entries. For aliasing ppgtt support we just steal them at the end for
	 * now. */
	first_pd_entry_in_global_pt = gtt_total_entries(dev_priv->gtt);

	ppgtt->base.pte_encode = dev_priv->gtt.base.pte_encode;
	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
	ppgtt->enable = gen6_ppgtt_enable;
	ppgtt->base.clear_range = gen6_ppgtt_clear_range;
	ppgtt->base.insert_entries = gen6_ppgtt_insert_entries;
	ppgtt->base.cleanup = gen6_ppgtt_cleanup;
	ppgtt->base.scratch = dev_priv->gtt.base.scratch;
	ppgtt->base.start = 0;
	ppgtt->base.total = GEN6_PPGTT_PD_ENTRIES * I915_PPGTT_PT_ENTRIES * PAGE_SIZE;
	ppgtt->pt_pages = kcalloc(ppgtt->num_pd_entries, sizeof(struct page *),
				  GFP_KERNEL);
	if (!ppgtt->pt_pages)
		return -ENOMEM;

	for (i = 0; i < ppgtt->num_pd_entries; i++) {
		ppgtt->pt_pages[i] = alloc_page(GFP_KERNEL);
		if (!ppgtt->pt_pages[i])
			goto err_pt_alloc;
	}

	ppgtt->pt_dma_addr = kcalloc(ppgtt->num_pd_entries, sizeof(dma_addr_t),
				     GFP_KERNEL);
	if (!ppgtt->pt_dma_addr)
		goto err_pt_alloc;

	for (i = 0; i < ppgtt->num_pd_entries; i++) {
		dma_addr_t pt_addr;

		pt_addr = pci_map_page(dev->pdev, ppgtt->pt_pages[i], 0, 4096,
				       PCI_DMA_BIDIRECTIONAL);

		if (pci_dma_mapping_error(dev->pdev, pt_addr)) {
			ret = -EIO;
			goto err_pd_pin;

		}
		ppgtt->pt_dma_addr[i] = pt_addr;
	}

	ppgtt->base.clear_range(&ppgtt->base, 0,
				ppgtt->num_pd_entries * I915_PPGTT_PT_ENTRIES, true);

	ppgtt->pd_offset = first_pd_entry_in_global_pt * sizeof(gen6_gtt_pte_t);

	return 0;

err_pd_pin:
	if (ppgtt->pt_dma_addr) {
		for (i--; i >= 0; i--)
			pci_unmap_page(dev->pdev, ppgtt->pt_dma_addr[i],
				       4096, PCI_DMA_BIDIRECTIONAL);
	}
err_pt_alloc:
	kfree(ppgtt->pt_dma_addr);
	for (i = 0; i < ppgtt->num_pd_entries; i++) {
		if (ppgtt->pt_pages[i])
			__free_page(ppgtt->pt_pages[i]);
	}
	kfree(ppgtt->pt_pages);

	return ret;
}

static int i915_gem_init_aliasing_ppgtt(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct i915_hw_ppgtt *ppgtt;
	int ret;

	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
	if (!ppgtt)
		return -ENOMEM;

	ppgtt->base.dev = dev;

	if (INTEL_INFO(dev)->gen < 8)
		ret = gen6_ppgtt_init(ppgtt);
	else if (IS_GEN8(dev))
		ret = gen8_ppgtt_init(ppgtt, dev_priv->gtt.base.total);
	else
		BUG();

	if (ret)
		kfree(ppgtt);
	else {
		dev_priv->mm.aliasing_ppgtt = ppgtt;
		drm_mm_init(&ppgtt->base.mm, ppgtt->base.start,
			    ppgtt->base.total);
	}

	return ret;
}

void i915_gem_cleanup_aliasing_ppgtt(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;

	if (!ppgtt)
		return;

	ppgtt->base.cleanup(&ppgtt->base);
	dev_priv->mm.aliasing_ppgtt = NULL;
}

void i915_ppgtt_bind_object(struct i915_hw_ppgtt *ppgtt,
			    struct drm_i915_gem_object *obj,
			    enum i915_cache_level cache_level)
{
	ppgtt->base.insert_entries(&ppgtt->base, obj->pages,
				   obj->base.size >> PAGE_SHIFT,
				   i915_gem_obj_ggtt_offset(obj) >> PAGE_SHIFT,
				   cache_level);
}

void i915_ppgtt_unbind_object(struct i915_hw_ppgtt *ppgtt,
			      struct drm_i915_gem_object *obj)
{
	ppgtt->base.clear_range(&ppgtt->base,
				i915_gem_obj_ggtt_offset(obj) >> PAGE_SHIFT,
				obj->base.size >> PAGE_SHIFT,
				true);
}

extern int intel_iommu_gfx_mapped;
/* Certain Gen5 chipsets require require idling the GPU before
 * unmapping anything from the GTT when VT-d is enabled.
 */
static inline bool needs_idle_maps(struct drm_device *dev)
{
#ifdef CONFIG_INTEL_IOMMU
	/* Query intel_iommu to see if we need the workaround. Presumably that
	 * was loaded first.
	 */
	if (IS_GEN5(dev) && IS_MOBILE(dev) && intel_iommu_gfx_mapped)
		return true;
#endif
	return false;
}

static bool do_idling(struct drm_i915_private *dev_priv)
{
	bool ret = dev_priv->mm.interruptible;

	if (unlikely(dev_priv->gtt.do_idle_maps)) {
		dev_priv->mm.interruptible = false;
		if (i915_gpu_idle(dev_priv->dev)) {
			DRM_ERROR("Couldn't idle GPU\n");
			/* Wait a bit, in hopes it avoids the hang */
			udelay(10);
		}
	}

	return ret;
}

static void undo_idling(struct drm_i915_private *dev_priv, bool interruptible)
{
	if (unlikely(dev_priv->gtt.do_idle_maps))
		dev_priv->mm.interruptible = interruptible;
}

void i915_check_and_clear_faults(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct intel_ring_buffer *ring;
	int i;

	if (INTEL_INFO(dev)->gen < 6)
		return;

	for_each_ring(ring, dev_priv, i) {
		u32 fault_reg;
		fault_reg = I915_READ(RING_FAULT_REG(ring));
		if (fault_reg & RING_FAULT_VALID) {
			DRM_DEBUG_DRIVER("Unexpected fault\n"
					 "\tAddr: 0x%08x\\n"
					 "\tAddress space: %s\n"
					 "\tSource ID: %d\n"
					 "\tType: %d\n",
					 fault_reg & ~PAGE_MASK,
					 fault_reg & RING_FAULT_GTTSEL_MASK ? "GGTT" : "PPGTT",
					 RING_FAULT_SRCID(fault_reg),
					 RING_FAULT_FAULT_TYPE(fault_reg));
			I915_WRITE(RING_FAULT_REG(ring),
				   fault_reg & ~RING_FAULT_VALID);
		}
	}
	POSTING_READ(RING_FAULT_REG(&dev_priv->ring[RCS]));
}

static void i915_ggtt_flush(struct drm_i915_private *dev_priv)
{
	if (INTEL_INFO(dev_priv->dev)->gen < 6) {
		intel_gtt_chipset_flush();
	} else {
		I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
		POSTING_READ(GFX_FLSH_CNTL_GEN6);
	}
}

void i915_gem_suspend_gtt_mappings(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;

	/* Don't bother messing with faults pre GEN6 as we have little
	 * documentation supporting that it's a good idea.
	 */
	if (INTEL_INFO(dev)->gen < 6)
		return;

	i915_check_and_clear_faults(dev);

	dev_priv->gtt.base.clear_range(&dev_priv->gtt.base,
				       dev_priv->gtt.base.start / PAGE_SIZE,
				       dev_priv->gtt.base.total / PAGE_SIZE,
				       true);

	i915_ggtt_flush(dev_priv);
}

void i915_gem_restore_gtt_mappings(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct drm_i915_gem_object *obj;

	i915_check_and_clear_faults(dev);

	/* First fill our portion of the GTT with scratch pages */
	dev_priv->gtt.base.clear_range(&dev_priv->gtt.base,
				       dev_priv->gtt.base.start / PAGE_SIZE,
				       dev_priv->gtt.base.total / PAGE_SIZE,
				       true);

	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
		i915_gem_clflush_object(obj, obj->pin_display);
		i915_gem_gtt_bind_object(obj, obj->cache_level);
	}

	if (INTEL_INFO(dev)->gen >= 8)
		gen8_setup_private_ppat(dev_priv);

	i915_ggtt_flush(dev_priv);
}

int i915_gem_gtt_prepare_object(struct drm_i915_gem_object *obj)
{
	if (obj->has_dma_mapping)
		return 0;

#ifdef __linux__
	if (!dma_map_sg(&obj->base.dev->pdev->dev,
			obj->pages->sgl, obj->pages->nents,
			PCI_DMA_BIDIRECTIONAL))
		return -ENOSPC;
#endif

	return 0;
}

static inline void gen8_set_pte(void __iomem *addr, gen8_gtt_pte_t pte)
{
#ifdef writeq
	writeq(pte, addr);
#else
	iowrite32((u32)pte, addr);
	iowrite32(pte >> 32, addr + 4);
#endif
}

#ifdef __linux__
static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
				     struct sg_table *st,
				     unsigned int first_entry,
				     enum i915_cache_level level)
{
	struct drm_i915_private *dev_priv = vm->dev->dev_private;
	gen8_gtt_pte_t __iomem *gtt_entries =
		(gen8_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
	int i = 0;
	struct sg_page_iter sg_iter;
	dma_addr_t addr;

	for_each_sg_page(st->sgl, &sg_iter, st->nents, 0) {
		addr = sg_dma_address(sg_iter.sg) +
			(sg_iter.sg_pgoffset << PAGE_SHIFT);
		gen8_set_pte(&gtt_entries[i],
			     gen8_pte_encode(addr, level, true));
		i++;
	}

	/*
	 * XXX: This serves as a posting read to make sure that the PTE has
	 * actually been updated. There is some concern that even though
	 * registers and PTEs are within the same BAR that they are potentially
	 * of NUMA access patterns. Therefore, even with the way we assume
	 * hardware should work, we must keep this posting read for paranoia.
	 */
	if (i != 0)
		WARN_ON(readq(&gtt_entries[i-1])
			!= gen8_pte_encode(addr, level, true));

	/* This next bit makes the above posting read even more important. We
	 * want to flush the TLBs only after we're certain all the PTE updates
	 * have finished.
	 */
	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
	POSTING_READ(GFX_FLSH_CNTL_GEN6);
}
#else
static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
				     struct vm_page **pages,
				     unsigned int num_entries,
				     unsigned int first_entry,
				     enum i915_cache_level level)
{
	struct drm_i915_private *dev_priv = vm->dev->dev_private;
	gen8_gtt_pte_t __iomem *gtt_entries =
		(gen8_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
	int i = 0;
	dma_addr_t addr;

	while (i < num_entries) {
		addr = VM_PAGE_TO_PHYS(pages[i]);
		gen8_set_pte(&gtt_entries[i],
			     gen8_pte_encode(addr, level, true));
		i++;
	}

	/*
	 * XXX: This serves as a posting read to make sure that the PTE has
	 * actually been updated. There is some concern that even though
	 * registers and PTEs are within the same BAR that they are potentially
	 * of NUMA access patterns. Therefore, even with the way we assume
	 * hardware should work, we must keep this posting read for paranoia.
	 */
	if (i != 0)
		WARN_ON(readq(&gtt_entries[i-1])
			!= gen8_pte_encode(addr, level, true));

	/* This next bit makes the above posting read even more important. We
	 * want to flush the TLBs only after we're certain all the PTE updates
	 * have finished.
	 */
	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
	POSTING_READ(GFX_FLSH_CNTL_GEN6);
}
#endif

/*
 * Binds an object into the global gtt with the specified cache level. The object
 * will be accessible to the GPU via commands whose operands reference offsets
 * within the global GTT as well as accessible by the GPU through the GMADR
 * mapped BAR (dev_priv->mm.gtt->gtt).
 */
#ifdef __linux__
static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
				     struct sg_table *st,
				     unsigned int first_entry,
				     enum i915_cache_level level)
{
	struct drm_i915_private *dev_priv = vm->dev->dev_private;
	gen6_gtt_pte_t __iomem *gtt_entries =
		(gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
	int i = 0;
	struct sg_page_iter sg_iter;
	dma_addr_t addr;

	for_each_sg_page(st->sgl, &sg_iter, st->nents, 0) {
		addr = sg_page_iter_dma_address(&sg_iter);
		iowrite32(vm->pte_encode(addr, level, true), &gtt_entries[i]);
		i++;
	}

	/* XXX: This serves as a posting read to make sure that the PTE has
	 * actually been updated. There is some concern that even though
	 * registers and PTEs are within the same BAR that they are potentially
	 * of NUMA access patterns. Therefore, even with the way we assume
	 * hardware should work, we must keep this posting read for paranoia.
	 */
	if (i != 0)
		WARN_ON(readl(&gtt_entries[i-1]) !=
			vm->pte_encode(addr, level, true));

	/* This next bit makes the above posting read even more important. We
	 * want to flush the TLBs only after we're certain all the PTE updates
	 * have finished.
	 */
	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
	POSTING_READ(GFX_FLSH_CNTL_GEN6);
}
#else
static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
				     struct vm_page **pages,
				     unsigned int num_entries,
				     unsigned int first_entry,
				     enum i915_cache_level level)
{
	struct drm_i915_private *dev_priv = vm->dev->dev_private;
	gen6_gtt_pte_t __iomem *gtt_entries =
		(gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
	int i = 0;
	dma_addr_t addr;

	while (i < num_entries) { 
		addr = VM_PAGE_TO_PHYS(pages[i]);
		iowrite32(vm->pte_encode(addr, level, true), &gtt_entries[i]);
		i++;
	}

	/* XXX: This serves as a posting read to make sure that the PTE has
	 * actually been updated. There is some concern that even though
	 * registers and PTEs are within the same BAR that they are potentially
	 * of NUMA access patterns. Therefore, even with the way we assume
	 * hardware should work, we must keep this posting read for paranoia.
	 */
	if (i != 0)
		WARN_ON(readl(&gtt_entries[i-1]) !=
			vm->pte_encode(addr, level, true));

	/* This next bit makes the above posting read even more important. We
	 * want to flush the TLBs only after we're certain all the PTE updates
	 * have finished.
	 */
	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
	POSTING_READ(GFX_FLSH_CNTL_GEN6);
}
#endif

static void gen8_ggtt_clear_range(struct i915_address_space *vm,
				  unsigned int first_entry,
				  unsigned int num_entries,
				  bool use_scratch)
{
	struct drm_i915_private *dev_priv = vm->dev->dev_private;
	gen8_gtt_pte_t scratch_pte, __iomem *gtt_base =
		(gen8_gtt_pte_t __iomem *) dev_priv->gtt.gsm + first_entry;
	const int max_entries = gtt_total_entries(dev_priv->gtt) - first_entry;
	int i;

	if (WARN(num_entries > max_entries,
		 "First entry = %d; Num entries = %d (max=%d)\n",
		 first_entry, num_entries, max_entries))
		num_entries = max_entries;

	scratch_pte = gen8_pte_encode(vm->scratch.addr,
				      I915_CACHE_LLC,
				      use_scratch);
	for (i = 0; i < num_entries; i++)
		gen8_set_pte(&gtt_base[i], scratch_pte);
	readl(gtt_base);
}

static void gen6_ggtt_clear_range(struct i915_address_space *vm,
				  unsigned int first_entry,
				  unsigned int num_entries,
				  bool use_scratch)
{
	struct drm_i915_private *dev_priv = vm->dev->dev_private;
	gen6_gtt_pte_t scratch_pte, __iomem *gtt_base =
		(gen6_gtt_pte_t __iomem *) dev_priv->gtt.gsm + first_entry;
	const int max_entries = gtt_total_entries(dev_priv->gtt) - first_entry;
	int i;

	if (WARN(num_entries > max_entries,
		 "First entry = %d; Num entries = %d (max=%d)\n",
		 first_entry, num_entries, max_entries))
		num_entries = max_entries;

	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, use_scratch);

	for (i = 0; i < num_entries; i++)
		iowrite32(scratch_pte, &gtt_base[i]);
	readl(gtt_base);
}

#ifdef __linux__
static void i915_ggtt_insert_entries(struct i915_address_space *vm,
				     struct sg_table *st,
				     unsigned int pg_start,
				     enum i915_cache_level cache_level)
{
	unsigned int flags = (cache_level == I915_CACHE_NONE) ?
		AGP_USER_MEMORY : AGP_USER_CACHED_MEMORY;

	intel_gtt_insert_sg_entries(st, pg_start, flags);

}
#else
static void i915_ggtt_insert_entries(struct i915_address_space *vm,
				     struct vm_page **pages,
				     unsigned int num_entries,
				     unsigned int pg_start,
				     enum i915_cache_level cache_level)
{
	unsigned int flags = (cache_level == I915_CACHE_NONE) ?
		0 : BUS_DMA_COHERENT;
	struct agp_softc *sc = vm->dev->agp->agpdev;
	bus_addr_t addr = sc->sc_apaddr + (pg_start << PAGE_SHIFT);
	int i;

	for (i = 0; i < num_entries; i++) {
		sc->sc_methods->bind_page(sc->sc_chipc, addr,
		    VM_PAGE_TO_PHYS(pages[i]), flags);
		addr += PAGE_SIZE;
	}
}
#endif

#ifdef __linux__
static void i915_ggtt_clear_range(struct i915_address_space *vm,
				  unsigned int first_entry,
				  unsigned int num_entries,
				  bool unused)
{
	intel_gtt_clear_range(first_entry, num_entries);
}
#else
static void i915_ggtt_clear_range(struct i915_address_space *vm,
				  unsigned int first_entry,
				  unsigned int num_entries,
				  bool unused)
{
	struct agp_softc *sc = vm->dev->agp->agpdev;
	bus_addr_t addr = sc->sc_apaddr + (first_entry << PAGE_SHIFT);
	int i;

	for (i = 0; i < num_entries; i++) {
		sc->sc_methods->unbind_page(sc->sc_chipc, addr);
		addr += PAGE_SIZE;
	}
}
#endif

void i915_gem_gtt_bind_object(struct drm_i915_gem_object *obj,
			      enum i915_cache_level cache_level)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	const unsigned long entry = i915_gem_obj_ggtt_offset(obj) >> PAGE_SHIFT;

	dev_priv->gtt.base.insert_entries(&dev_priv->gtt.base, obj->pages,
					  obj->base.size >> PAGE_SHIFT,
					  entry,
					  cache_level);

	obj->has_global_gtt_mapping = 1;
}

void i915_gem_gtt_unbind_object(struct drm_i915_gem_object *obj)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	const unsigned long entry = i915_gem_obj_ggtt_offset(obj) >> PAGE_SHIFT;

	dev_priv->gtt.base.clear_range(&dev_priv->gtt.base,
				       entry,
				       obj->base.size >> PAGE_SHIFT,
				       true);

	obj->has_global_gtt_mapping = 0;
}

void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	bool interruptible;

	interruptible = do_idling(dev_priv);

#ifdef __linux__
	if (!obj->has_dma_mapping)
		dma_unmap_sg(&dev->pdev->dev,
			     obj->pages->sgl, obj->pages->nents,
			     PCI_DMA_BIDIRECTIONAL);
#endif

	undo_idling(dev_priv, interruptible);
}

static void i915_gtt_color_adjust(struct drm_mm_node *node,
				  unsigned long color,
				  unsigned long *start,
				  unsigned long *end)
{
	if (node->color != color)
		*start += 4096;

	if (!list_empty(&node->node_list)) {
		node = list_entry(node->node_list.next,
				  struct drm_mm_node,
				  node_list);
		if (node->allocated && node->color != color)
			*end -= 4096;
	}
}

void i915_gem_setup_global_gtt(struct drm_device *dev,
			       unsigned long start,
			       unsigned long mappable_end,
			       unsigned long end)
{
	/* Let GEM Manage all of the aperture.
	 *
	 * However, leave one page at the end still bound to the scratch page.
	 * There are a number of places where the hardware apparently prefetches
	 * past the end of the object, and we've seen multiple hangs with the
	 * GPU head pointer stuck in a batchbuffer bound at the last page of the
	 * aperture.  One page should be enough to keep any prefetching inside
	 * of the aperture.
	 */
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct i915_address_space *ggtt_vm = &dev_priv->gtt.base;
	struct drm_mm_node *entry;
	struct drm_i915_gem_object *obj;
	unsigned long hole_start, hole_end;

	BUG_ON(mappable_end > end);

	/* Subtract the guard page ... */
	drm_mm_init(&ggtt_vm->mm, start, end - start - PAGE_SIZE);
	if (!HAS_LLC(dev))
		dev_priv->gtt.base.mm.color_adjust = i915_gtt_color_adjust;

	/* Mark any preallocated objects as occupied */
	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
		struct i915_vma *vma = i915_gem_obj_to_vma(obj, ggtt_vm);
		int ret;
		DRM_DEBUG_KMS("reserving preallocated space: %lx + %zx\n",
			      i915_gem_obj_ggtt_offset(obj), obj->base.size);

		WARN_ON(i915_gem_obj_ggtt_bound(obj));
		ret = drm_mm_reserve_node(&ggtt_vm->mm, &vma->node);
		if (ret)
			DRM_DEBUG_KMS("Reservation failed\n");
		obj->has_global_gtt_mapping = 1;
	}

	dev_priv->gtt.base.start = start;
	dev_priv->gtt.base.total = end - start;

	/* Clear any non-preallocated blocks */
	drm_mm_for_each_hole(entry, &ggtt_vm->mm, hole_start, hole_end) {
		const unsigned long count = (hole_end - hole_start) / PAGE_SIZE;
		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
			      hole_start, hole_end);
		ggtt_vm->clear_range(ggtt_vm, hole_start / PAGE_SIZE, count, true);
	}

	/* And finally clear the reserved guard page */
	ggtt_vm->clear_range(ggtt_vm, end / PAGE_SIZE - 1, 1, true);
}

static bool
intel_enable_ppgtt(struct drm_device *dev)
{
	if (i915_enable_ppgtt >= 0)
		return i915_enable_ppgtt;

#ifdef CONFIG_INTEL_IOMMU
	/* Disable ppgtt on SNB if VT-d is on. */
	if (INTEL_INFO(dev)->gen == 6 && intel_iommu_gfx_mapped)
		return false;
#endif

	return true;
}

void i915_gem_init_global_gtt(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	unsigned long gtt_size, mappable_size;

	gtt_size = dev_priv->gtt.base.total;
	mappable_size = dev_priv->gtt.mappable_end;

	if (intel_enable_ppgtt(dev) && HAS_ALIASING_PPGTT(dev)) {
		int ret;

		if (INTEL_INFO(dev)->gen <= 7) {
			/* PPGTT pdes are stolen from global gtt ptes, so shrink the
			 * aperture accordingly when using aliasing ppgtt. */
			gtt_size -= GEN6_PPGTT_PD_ENTRIES * PAGE_SIZE;
		}

		i915_gem_setup_global_gtt(dev, 0, mappable_size, gtt_size);

		ret = i915_gem_init_aliasing_ppgtt(dev);
		if (!ret)
			return;

		DRM_ERROR("Aliased PPGTT setup failed %d\n", ret);
		drm_mm_takedown(&dev_priv->gtt.base.mm);
		if (INTEL_INFO(dev)->gen < 8)
			gtt_size += GEN6_PPGTT_PD_ENTRIES*PAGE_SIZE;
	}
	i915_gem_setup_global_gtt(dev, 0, mappable_size, gtt_size);
}

#ifdef __linux__

static int setup_scratch_page(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct page *page;
	dma_addr_t dma_addr;

	page = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);
	if (page == NULL)
		return -ENOMEM;
	get_page(page);
	set_pages_uc(page, 1);

#ifdef CONFIG_INTEL_IOMMU
	dma_addr = pci_map_page(dev->pdev, page, 0, PAGE_SIZE,
				PCI_DMA_BIDIRECTIONAL);
	if (pci_dma_mapping_error(dev->pdev, dma_addr))
		return -EINVAL;
#else
	dma_addr = page_to_phys(page);
#endif
	dev_priv->gtt.base.scratch.page = page;
	dev_priv->gtt.base.scratch.addr = dma_addr;

	return 0;
}

static void teardown_scratch_page(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct page *page = dev_priv->gtt.base.scratch.page;

	set_pages_wb(page, 1);
	pci_unmap_page(dev->pdev, dev_priv->gtt.base.scratch.addr,
		       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
	put_page(page);
	__free_page(page);
}

#else

static int setup_scratch_page(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct drm_dmamem *page;

	page = drm_dmamem_alloc(dev_priv->dmat, PAGE_SIZE, 0, 1, PAGE_SIZE,
	    BUS_DMA_NOCACHE, 0);
	if (page == NULL)
		return -ENOMEM;

	dev_priv->gtt.base.scratch.page = page;
	dev_priv->gtt.base.scratch.addr = page->segs[0].ds_addr;

	return 0;
}

static void teardown_scratch_page(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;

	drm_dmamem_free(dev_priv->dmat, dev_priv->gtt.base.scratch.page);
}

#endif

static inline unsigned int gen6_get_total_gtt_size(u16 snb_gmch_ctl)
{
	snb_gmch_ctl >>= SNB_GMCH_GGMS_SHIFT;
	snb_gmch_ctl &= SNB_GMCH_GGMS_MASK;
	return snb_gmch_ctl << 20;
}

static inline unsigned int gen8_get_total_gtt_size(u16 bdw_gmch_ctl)
{
	bdw_gmch_ctl >>= BDW_GMCH_GGMS_SHIFT;
	bdw_gmch_ctl &= BDW_GMCH_GGMS_MASK;
	if (bdw_gmch_ctl)
		bdw_gmch_ctl = 1 << bdw_gmch_ctl;
	if (bdw_gmch_ctl > 4) {
		WARN_ON(!i915_preliminary_hw_support);
		return 4<<20;
	}

	return bdw_gmch_ctl << 20;
}

static inline size_t gen6_get_stolen_size(u16 snb_gmch_ctl)
{
	snb_gmch_ctl >>= SNB_GMCH_GMS_SHIFT;
	snb_gmch_ctl &= SNB_GMCH_GMS_MASK;
	return snb_gmch_ctl << 25; /* 32 MB units */
}

static inline size_t gen8_get_stolen_size(u16 bdw_gmch_ctl)
{
	bdw_gmch_ctl >>= BDW_GMCH_GMS_SHIFT;
	bdw_gmch_ctl &= BDW_GMCH_GMS_MASK;
	return bdw_gmch_ctl << 25; /* 32 MB units */
}

#ifdef __linux__

static int ggtt_probe_common(struct drm_device *dev,
			     size_t gtt_size)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	phys_addr_t gtt_phys_addr;
	int ret;

	/* For Modern GENs the PTEs and register space are split in the BAR */
	gtt_phys_addr = pci_resource_start(dev->pdev, 0) +
		(pci_resource_len(dev->pdev, 0) / 2);

	dev_priv->gtt.gsm = ioremap_wc(gtt_phys_addr, gtt_size);
	if (!dev_priv->gtt.gsm) {
		DRM_ERROR("Failed to map the gtt page table\n");
		return -ENOMEM;
	}

	ret = setup_scratch_page(dev);
	if (ret) {
		DRM_ERROR("Scratch setup failed\n");
		/* iounmap will also get called at remove, but meh */
		iounmap(dev_priv->gtt.gsm);
	}

	return ret;
}

#else

static int ggtt_probe_common(struct drm_device *dev,
			     size_t gtt_size)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	bus_space_handle_t gsm;
	bus_addr_t addr;
	bus_size_t size;
	pcireg_t type;
	int ret;

	type = pci_mapreg_type(dev_priv->pc, dev_priv->tag, 0x10);
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x10, type,
	    &addr, &size, NULL);
	if (ret)
		return ret;

	/* For Modern GENs the PTEs and register space are split in the BAR */
	ret = -bus_space_map(dev_priv->bst, addr + size / 2, gtt_size,
	    BUS_SPACE_MAP_PREFETCHABLE | BUS_SPACE_MAP_LINEAR, &gsm);
	if (ret) {
		DRM_ERROR("Failed to map the gtt page table\n");
		return ret;
	}
	dev_priv->gtt.gsm = bus_space_vaddr(dev_priv->bst, gsm);

	ret = setup_scratch_page(dev);
	if (ret)
		DRM_ERROR("Scratch setup failed\n");

	return ret;
}

#endif

/* The GGTT and PPGTT need a private PPAT setup in order to handle cacheability
 * bits. When using advanced contexts each context stores its own PAT, but
 * writing this data shouldn't be harmful even in those cases. */
static void gen8_setup_private_ppat(struct drm_i915_private *dev_priv)
{
#define GEN8_PPAT_UC		(0<<0)
#define GEN8_PPAT_WC		(1<<0)
#define GEN8_PPAT_WT		(2<<0)
#define GEN8_PPAT_WB		(3<<0)
#define GEN8_PPAT_ELLC_OVERRIDE	(0<<2)
/* FIXME(BDW): Bspec is completely confused about cache control bits. */
#define GEN8_PPAT_LLC		(1<<2)
#define GEN8_PPAT_LLCELLC	(2<<2)
#define GEN8_PPAT_LLCeLLC	(3<<2)
#define GEN8_PPAT_AGE(x)	(x<<4)
#define GEN8_PPAT(i, x) ((uint64_t) (x) << ((i) * 8))
	uint64_t pat;

	pat = GEN8_PPAT(0, GEN8_PPAT_WB | GEN8_PPAT_LLC)     | /* for normal objects, no eLLC */
	      GEN8_PPAT(1, GEN8_PPAT_WC | GEN8_PPAT_LLCELLC) | /* for something pointing to ptes? */
	      GEN8_PPAT(2, GEN8_PPAT_WT | GEN8_PPAT_LLCELLC) | /* for scanout with eLLC */
	      GEN8_PPAT(3, GEN8_PPAT_UC)                     | /* Uncached objects, mostly for scanout */
	      GEN8_PPAT(4, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(0)) |
	      GEN8_PPAT(5, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(1)) |
	      GEN8_PPAT(6, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(2)) |
	      GEN8_PPAT(7, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(3));

	/* XXX: spec defines this as 2 distinct registers. It's unclear if a 64b
	 * write would work. */
	I915_WRITE(GEN8_PRIVATE_PAT, pat);
	I915_WRITE(GEN8_PRIVATE_PAT + 4, pat >> 32);
}

static int gen8_gmch_probe(struct drm_device *dev,
			   size_t *gtt_total,
			   size_t *stolen,
			   phys_addr_t *mappable_base,
			   unsigned long *mappable_end)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	unsigned int gtt_size;
	u16 snb_gmch_ctl;
	int ret;

#ifdef __linux__
	/* TODO: We're not aware of mappable constraints on gen8 yet */
	*mappable_base = pci_resource_start(dev->pdev, 2);
	*mappable_end = pci_resource_len(dev->pdev, 2);

	if (!pci_set_dma_mask(dev->pdev, DMA_BIT_MASK(39)))
		pci_set_consistent_dma_mask(dev->pdev, DMA_BIT_MASK(39));
#else
	pcireg_t type = pci_mapreg_type(dev_priv->pc, dev_priv->tag, 0x18);
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x18, type,
	    mappable_base, mappable_end, NULL);
	if (ret)
		return ret;
#endif

	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);

	*stolen = gen8_get_stolen_size(snb_gmch_ctl);

	gtt_size = gen8_get_total_gtt_size(snb_gmch_ctl);
	*gtt_total = (gtt_size / sizeof(gen8_gtt_pte_t)) << PAGE_SHIFT;

	gen8_setup_private_ppat(dev_priv);

	ret = ggtt_probe_common(dev, gtt_size);

	dev_priv->gtt.base.clear_range = gen8_ggtt_clear_range;
	dev_priv->gtt.base.insert_entries = gen8_ggtt_insert_entries;

	return ret;
}

static int gen6_gmch_probe(struct drm_device *dev,
			   size_t *gtt_total,
			   size_t *stolen,
			   phys_addr_t *mappable_base,
			   unsigned long *mappable_end)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	unsigned int gtt_size;
	u16 snb_gmch_ctl;
	int ret;

#ifdef __linux__
	*mappable_base = pci_resource_start(dev->pdev, 2);
	*mappable_end = pci_resource_len(dev->pdev, 2);
#else
	pcireg_t type = pci_mapreg_type(dev_priv->pc, dev_priv->tag, 0x18);
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x18, type,
	    mappable_base, mappable_end, NULL);
	if (ret)
		return ret;
#endif

	/* 64/512MB is the current min/max we actually know of, but this is just
	 * a coarse sanity check.
	 */
	if ((*mappable_end < (64<<20) || (*mappable_end > (512<<20)))) {
		DRM_ERROR("Unknown GMADR size (%lx)\n",
			  dev_priv->gtt.mappable_end);
		return -ENXIO;
	}

#ifdef __linux__
	if (!pci_set_dma_mask(dev->pdev, DMA_BIT_MASK(40)))
		pci_set_consistent_dma_mask(dev->pdev, DMA_BIT_MASK(40));
#endif
	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);

	*stolen = gen6_get_stolen_size(snb_gmch_ctl);

	gtt_size = gen6_get_total_gtt_size(snb_gmch_ctl);
	*gtt_total = (gtt_size / sizeof(gen6_gtt_pte_t)) << PAGE_SHIFT;

	ret = ggtt_probe_common(dev, gtt_size);

	dev_priv->gtt.base.clear_range = gen6_ggtt_clear_range;
	dev_priv->gtt.base.insert_entries = gen6_ggtt_insert_entries;

	return ret;
}

static void gen6_gmch_remove(struct i915_address_space *vm)
{

#ifdef __linux__
	struct i915_gtt *gtt = container_of(vm, struct i915_gtt, base);
#endif

	drm_mm_takedown(&vm->mm);
#ifdef __linux__
	iounmap(gtt->gsm);
#endif
	teardown_scratch_page(vm->dev);
}

static int i915_gmch_probe(struct drm_device *dev,
			   size_t *gtt_total,
			   size_t *stolen,
			   phys_addr_t *mappable_base,
			   unsigned long *mappable_end)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret;

	ret = intel_gmch_probe(dev_priv->bridge_dev, dev_priv->dev->pdev, NULL);
	if (!ret) {
		DRM_ERROR("failed to set up gmch\n");
		return -EIO;
	}

	intel_gtt_get(gtt_total, stolen, mappable_base, mappable_end);

	dev_priv->gtt.do_idle_maps = needs_idle_maps(dev_priv->dev);
	dev_priv->gtt.base.clear_range = i915_ggtt_clear_range;
	dev_priv->gtt.base.insert_entries = i915_ggtt_insert_entries;

	if (unlikely(dev_priv->gtt.do_idle_maps))
		DRM_INFO("applying Ironlake quirks for intel_iommu\n");

	return 0;
}

static void i915_gmch_remove(struct i915_address_space *vm)
{
	intel_gmch_remove();
}

int i915_gem_gtt_init(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct i915_gtt *gtt = &dev_priv->gtt;
	int ret;

	if (INTEL_INFO(dev)->gen <= 5) {
		gtt->gtt_probe = i915_gmch_probe;
		gtt->base.cleanup = i915_gmch_remove;
	} else if (INTEL_INFO(dev)->gen < 8) {
		gtt->gtt_probe = gen6_gmch_probe;
		gtt->base.cleanup = gen6_gmch_remove;
		if (IS_HASWELL(dev) && dev_priv->ellc_size)
			gtt->base.pte_encode = iris_pte_encode;
		else if (IS_HASWELL(dev))
			gtt->base.pte_encode = hsw_pte_encode;
		else if (IS_VALLEYVIEW(dev))
			gtt->base.pte_encode = byt_pte_encode;
		else if (INTEL_INFO(dev)->gen >= 7)
			gtt->base.pte_encode = ivb_pte_encode;
		else
			gtt->base.pte_encode = snb_pte_encode;
	} else {
		dev_priv->gtt.gtt_probe = gen8_gmch_probe;
		dev_priv->gtt.base.cleanup = gen6_gmch_remove;
	}

	ret = gtt->gtt_probe(dev, &gtt->base.total, &gtt->stolen_size,
			     &gtt->mappable_base, &gtt->mappable_end);
	if (ret)
		return ret;

	gtt->base.dev = dev;

	/* GMADR is the PCI mmio aperture into the global GTT. */
	DRM_INFO("Memory usable by graphics device = %zdM\n",
		 gtt->base.total >> 20);
	DRM_DEBUG_DRIVER("GMADR size = %ldM\n", gtt->mappable_end >> 20);
	DRM_DEBUG_DRIVER("GTT stolen size = %zdM\n", gtt->stolen_size >> 20);

	return 0;
}
@


1.13
log
@Apparently Valleyview / Bay Trail has 32-bit BARs, so fetch the type instead
of hardcoding them to be 64-bit.  Figured out by sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.12 2015/09/26 11:17:15 kettenis Exp $	*/
d212 1
a212 1
			   uint64_t val)
d214 1
d219 6
d258 2
a259 1
			ret = gen8_write_pdp(ring, i, addr);
@


1.12
log
@Make the PPGTT code work.  Seems to fix the caching issues on Broadwell.
Comments on some of the later Broadwell-related commits in the Linux tree
seem to say that the PPAT flags in for the (global) GTT are simply broken in
the hardware.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.11 2015/09/25 16:15:19 jsg Exp $	*/
d1560 1
d1563 3
a1565 2
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x10,
	    PCI_MAPREG_MEM_TYPE_64BIT, &addr, &size, NULL);
d1639 3
a1641 2
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x18,
	    PCI_MAPREG_MEM_TYPE_64BIT, mappable_base, mappable_end, NULL);
d1678 3
a1680 2
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x18,
	    PCI_MAPREG_MEM_TYPE_64BIT, mappable_base, mappable_end, NULL);
@


1.11
log
@3.14 backports of some Broadwell fixes from
http://lists.freedesktop.org/archives/intel-gfx/2014-March/042121.html

Ben Widawsky
drm/i915/bdw: Restore PPAT on thaw
a2319c08bfd849ea32b4f890ce92df86074c5731

Ville Syrjala
drm/i915: We implement WaDisableAsyncFlipPerfMode:bdw
8285222c487b61c48b9b955b82598544c3c06050

Ben Widawsky
drm/i915/bdw: Use scratch page table for GEN8 PPGTT
8407bb9129da95fc4099b84cdbbc23e6d4f66aee

Jani Nikula
drm/i915: don't flood the logs about bdw semaphores
c923facd535b97972b5bb7d3df4fcafd61a63a5e

Ville Syrjala
drm/i915: Implement WaDisableSDEUnitClockGating:bdw
4f1ca9e94057de098d65bc7477e8f89dd51609aa

Ville Syrjala
drm/i915: Don't clobber CHICKEN_PIPESL_1 on BDW
c7c656226842679bcd9f39dc24441b4ff398a850

Kenneth Graunke
drm/i915: Add a partial instruction shootdown workaround on Broadwell.
c8966e1058e1e8ae2eec4211157847032829697a

Damien Lespiau
drm/i915/bdw: The TLB invalidation mechanism has been removed from INSTPM
dc616b89dbc4bb6a99884d214bd1ed1e0eef59a0

Kenneth Graunke
drm/i915: Add thread stall DOP clock gating workaround on Broadwell.
1411e6a57a1836ba8a3d4f17c8733b2fbaf0f005

Ville Syrjala
drm/i915: Disable semaphore wait event idle message on BDW
295e8bb73a4785b65db6655fbf6ad57c4177b551

Mika Kuoppala
drm/i915: Do forcewake reset on gen8
0a089e3355d77f758e46db54a0a81d4b58a28cc3

Mika Kuoppala
drm/i915: Fix forcewake counts for gen8
e9dbd2b20201b49b04476d2e5763faa822967913

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.10 2015/09/23 23:12:12 kettenis Exp $	*/
a209 1
#ifdef notyet
d281 1
a281 1
		struct page *page_table = &ppgtt->gen8_pt_pages[act_pt];
d300 1
d331 33
d407 1
a407 1
	struct page *pt_pages;
d464 1
a464 1
			struct page *p = &pt_pages[i * GEN8_PDES_PER_PAGE + j];
d620 1
d651 33
a781 1
#endif
a784 1
#ifdef notyet
a810 3
#else
	return 0;
#endif
@


1.10
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d39 2
d373 1
d404 1
d896 3
@


1.9
log
@Move GTT management for Sandy Bridge and up into inteldrm(4).  This makes
it possible to use the non-mappable part of the GTT, prepares the way for
using the PPGTT and reduces the diffs with Linux.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.8 2013/12/11 20:31:43 kettenis Exp $	*/
d27 1
a27 1
#include <dev/pci/drm/drm.h>
d32 11
a42 1
typedef uint32_t gtt_pte_t;
d46 1
d56 1
a56 1
#define GEN6_PTE_CACHE_LLC_MLC		(3 << 1)
d58 1
d60 72
a131 3
static inline gtt_pte_t pte_encode(struct drm_device *dev,
				   bus_addr_t addr,
				   enum i915_cache_level level)
d133 1
a133 1
	gtt_pte_t pte = GEN6_PTE_VALID;
d137 2
a138 6
	case I915_CACHE_LLC_MLC:
		/* Haswell doesn't set L3 this way */
		if (IS_HASWELL(dev))
			pte |= GEN6_PTE_CACHE_LLC;
		else
			pte |= GEN6_PTE_CACHE_LLC_MLC;
d144 1
a144 4
		if (IS_HASWELL(dev))
			pte |= HSW_PTE_UNCACHED;
		else
			pte |= GEN6_PTE_UNCACHED;
d147 1
a147 1
		BUG();
d150 54
d209 340
d551 1
a551 1
static void i915_ppgtt_clear_range(struct i915_hw_ppgtt *ppgtt,
d553 2
a554 1
				   unsigned num_entries)
d556 4
a559 3
	gtt_pte_t *pt_vaddr;
	gtt_pte_t scratch_pte;
	unsigned act_pd = first_entry / I915_PPGTT_PT_ENTRIES;
d563 1
a563 2
	scratch_pte = pte_encode(ppgtt->dev, ppgtt->scratch_page_dma_addr,
				 I915_CACHE_LLC);
d570 1
a570 1
		pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pd]);
d579 1
a579 1
		act_pd++;
d583 32
a614 1
int i915_gem_init_aliasing_ppgtt(struct drm_device *dev)
d616 23
a639 1
	struct i915_hw_ppgtt *ppgtt;
d647 1
a647 5
	first_pd_entry_in_global_pt = dev_priv->mm.gtt->gtt_total_entries - I915_PPGTT_PD_ENTRIES;

	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
	if (!ppgtt)
		return ret;
d649 10
a658 3
	ppgtt->dev = dev;
	ppgtt->num_pd_entries = I915_PPGTT_PD_ENTRIES;
	ppgtt->pt_pages = kzalloc(sizeof(struct page *)*ppgtt->num_pd_entries,
d661 1
a661 1
		goto err_ppgtt;
d669 7
a675 6
	if (dev_priv->mm.gtt->needs_dmar) {
		ppgtt->pt_dma_addr = kzalloc(sizeof(dma_addr_t)
						*ppgtt->num_pd_entries,
					     GFP_KERNEL);
		if (!ppgtt->pt_dma_addr)
			goto err_pt_alloc;
d677 2
a678 2
		for (i = 0; i < ppgtt->num_pd_entries; i++) {
			dma_addr_t pt_addr;
d680 3
a682 8
			pt_addr = pci_map_page(dev->pdev, ppgtt->pt_pages[i],
					       0, 4096,
					       PCI_DMA_BIDIRECTIONAL);

			if (pci_dma_mapping_error(dev->pdev,
						  pt_addr)) {
				ret = -EIO;
				goto err_pd_pin;
a683 2
			}
			ppgtt->pt_dma_addr[i] = pt_addr;
d685 1
d688 2
a689 1
	ppgtt->scratch_page_dma_addr = dev_priv->mm.gtt->scratch_page_dma;
d691 1
a691 6
	i915_ppgtt_clear_range(ppgtt, 0,
			       ppgtt->num_pd_entries*I915_PPGTT_PT_ENTRIES);

	ppgtt->pd_offset = (first_pd_entry_in_global_pt)*sizeof(gtt_pte_t);

	dev_priv->mm.aliasing_ppgtt = ppgtt;
a707 2
err_ppgtt:
	kfree(ppgtt);
d711 1
a711 1
#endif /* notyet */
d713 1
a713 1
void i915_gem_cleanup_aliasing_ppgtt(struct drm_device *dev)
a714 1
	printf("%s stub\n", __func__);
d717 2
a718 2
	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
	int i;
d720 1
d722 10
a731 1
		return;
d733 6
a738 4
	if (ppgtt->pt_dma_addr) {
		for (i = 0; i < ppgtt->num_pd_entries; i++)
			pci_unmap_page(dev->pdev, ppgtt->pt_dma_addr[i],
				       4096, PCI_DMA_BIDIRECTIONAL);
d741 3
a743 5
	kfree(ppgtt->pt_dma_addr);
	for (i = 0; i < ppgtt->num_pd_entries; i++)
		__free_page(ppgtt->pt_pages[i]);
	kfree(ppgtt->pt_pages);
	kfree(ppgtt);
d747 1
a747 5
#ifdef notyet
static void i915_ppgtt_insert_sg_entries(struct i915_hw_ppgtt *ppgtt,
					 const struct sg_table *pages,
					 unsigned first_entry,
					 enum i915_cache_level cache_level)
d749 2
a750 31
	gtt_pte_t *pt_vaddr;
	unsigned act_pd = first_entry / I915_PPGTT_PT_ENTRIES;
	unsigned first_pte = first_entry % I915_PPGTT_PT_ENTRIES;
	unsigned i, j, m, segment_len;
	dma_addr_t page_addr;
	struct scatterlist *sg;

	/* init sg walking */
	sg = pages->sgl;
	i = 0;
	segment_len = sg_dma_len(sg) >> PAGE_SHIFT;
	m = 0;

	while (i < pages->nents) {
		pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pd]);

		for (j = first_pte; j < I915_PPGTT_PT_ENTRIES; j++) {
			page_addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
			pt_vaddr[j] = pte_encode(ppgtt->dev, page_addr,
						 cache_level);

			/* grab the next page */
			if (++m == segment_len) {
				if (++i == pages->nents)
					break;

				sg = sg_next(sg);
				segment_len = sg_dma_len(sg) >> PAGE_SHIFT;
				m = 0;
			}
		}
d752 2
a753 1
		kunmap_atomic(pt_vaddr);
d755 2
a756 3
		first_pte = 0;
		act_pd++;
	}
d763 4
a766 4
	i915_ppgtt_insert_sg_entries(ppgtt,
				     obj->pages,
				     obj->gtt_space->start >> PAGE_SHIFT,
				     cache_level);
d772 4
a775 3
	i915_ppgtt_clear_range(ppgtt,
			       obj->gtt_space->start >> PAGE_SHIFT,
			       obj->base.size >> PAGE_SHIFT);
d778 5
a782 1
void i915_gem_init_ppgtt(struct drm_device *dev)
d784 8
a791 58
	drm_i915_private_t *dev_priv = dev->dev_private;
	uint32_t pd_offset;
	struct intel_ring_buffer *ring;
	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
	uint32_t __iomem *pd_addr;
	uint32_t pd_entry;
	int i;

	if (!dev_priv->mm.aliasing_ppgtt)
		return;


	pd_addr = dev_priv->mm.gtt->gtt + ppgtt->pd_offset/sizeof(uint32_t);
	for (i = 0; i < ppgtt->num_pd_entries; i++) {
		dma_addr_t pt_addr;

		if (dev_priv->mm.gtt->needs_dmar)
			pt_addr = ppgtt->pt_dma_addr[i];
		else
			pt_addr = page_to_phys(ppgtt->pt_pages[i]);

		pd_entry = GEN6_PDE_ADDR_ENCODE(pt_addr);
		pd_entry |= GEN6_PDE_VALID;

		writel(pd_entry, pd_addr + i);
	}
	readl(pd_addr);

	pd_offset = ppgtt->pd_offset;
	pd_offset /= 64; /* in cachelines, */
	pd_offset <<= 16;

	if (INTEL_INFO(dev)->gen == 6) {
		uint32_t ecochk, gab_ctl, ecobits;

		ecobits = I915_READ(GAC_ECO_BITS);
		I915_WRITE(GAC_ECO_BITS, ecobits | ECOBITS_PPGTT_CACHE64B);

		gab_ctl = I915_READ(GAB_CTL);
		I915_WRITE(GAB_CTL, gab_ctl | GAB_CTL_CONT_AFTER_PAGEFAULT);

		ecochk = I915_READ(GAM_ECOCHK);
		I915_WRITE(GAM_ECOCHK, ecochk | ECOCHK_SNB_BIT |
				       ECOCHK_PPGTT_CACHE64B);
		I915_WRITE(GFX_MODE, _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
	} else if (INTEL_INFO(dev)->gen >= 7) {
		I915_WRITE(GAM_ECOCHK, ECOCHK_PPGTT_CACHE64B);
		/* GFX_MODE is per-ring on gen7+ */
	}

	for_each_ring(ring, dev_priv, i) {
		if (INTEL_INFO(dev)->gen >= 7)
			I915_WRITE(RING_MODE_GEN7(ring),
				   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));

		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
		I915_WRITE(RING_PP_DIR_BASE(ring), pd_offset);
	}
a792 1
#endif
d798 1
a798 2
#if 0
	if (unlikely(dev_priv->mm.gtt->do_idle_maps)) {
a805 1
#endif
d812 1
a812 2
#if 0
	if (unlikely(dev_priv->mm.gtt->do_idle_maps))
a813 1
#endif
d816 1
a816 6

#ifdef __linux__

static void i915_ggtt_clear_range(struct drm_device *dev,
				 unsigned first_entry,
				 unsigned num_entries)
d819 1
a819 3
	gtt_pte_t scratch_pte;
	gtt_pte_t __iomem *gtt_base = dev_priv->mm.gtt->gtt + first_entry;
	const int max_entries = dev_priv->mm.gtt->gtt_total_entries - first_entry;
d822 1
a822 2
	if (INTEL_INFO(dev)->gen < 6) {
		intel_gtt_clear_range(first_entry, num_entries);
d824 17
d842 2
d845 8
a852 9
	if (WARN(num_entries > max_entries,
		 "First entry = %d; Num entries = %d (max=%d)\n",
		 first_entry, num_entries, max_entries))
		num_entries = max_entries;

	scratch_pte = pte_encode(dev, dev_priv->mm.gtt->scratch_page_dma, I915_CACHE_LLC);
	for (i = 0; i < num_entries; i++)
		iowrite32(scratch_pte, &gtt_base[i]);
	readl(gtt_base);
d855 1
a855 5
#else

static void i915_ggtt_clear_range(struct drm_device *dev,
				 unsigned first_entry,
				 unsigned num_entries)
a857 3
	gtt_pte_t scratch_pte;
	const int max_entries = dev_priv->mm.gtt->gtt_total_entries - first_entry;
	int i;
d859 4
a862 9
	if (INTEL_INFO(dev)->gen < 6) {
		struct agp_softc *sc = dev->agp->agpdev;
		bus_addr_t addr = sc->sc_apaddr + (first_entry << PAGE_SHIFT);
		int i;

		for (i = 0; i < num_entries; i++) {
			sc->sc_methods->unbind_page(sc->sc_chipc, addr);
			addr += PAGE_SIZE;
		}
a863 1
	}
d865 6
a870 4
	if (WARN(num_entries > max_entries,
		 "First entry = %d; Num entries = %d (max=%d)\n",
		 first_entry, num_entries, max_entries))
		num_entries = max_entries;
d872 1
a872 6
	scratch_pte = pte_encode(dev, dev_priv->mm.gtt->scratch_page_dma, I915_CACHE_LLC);
	for (i = 0; i < num_entries; i++)
		bus_space_write_4(dev_priv->bst, dev_priv->mm.gtt->gtt,
		    (i + first_entry) * sizeof(gtt_pte_t), scratch_pte);
	bus_space_read_4(dev_priv->bst, dev_priv->mm.gtt->gtt,
	    first_entry * sizeof(gtt_pte_t));
a874 2
#endif

d880 2
d883 4
a886 2
	i915_ggtt_clear_range(dev, dev_priv->mm.gtt_start / PAGE_SIZE,
			      (dev_priv->mm.gtt_end - dev_priv->mm.gtt_start) / PAGE_SIZE);
d888 2
a889 2
	list_for_each_entry(obj, &dev_priv->mm.bound_list, gtt_list) {
		i915_gem_clflush_object(obj);
d893 1
a893 1
	i915_gem_chipset_flush(dev);
d901 1
a901 1
#if 0
d905 93
a997 1
		return -ENOSPC;
a999 5
	return 0;
}

#ifdef __linux__

d1006 11
a1016 12
static void gen6_ggtt_bind_object(struct drm_i915_gem_object *obj,
				  enum i915_cache_level level)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct sg_table *st = obj->pages;
	struct scatterlist *sg = st->sgl;
	const int first_entry = obj->gtt_space->start >> PAGE_SHIFT;
	const int max_entries = dev_priv->mm.gtt->gtt_total_entries - first_entry;
	gtt_pte_t __iomem *gtt_entries = dev_priv->mm.gtt->gtt + first_entry;
	int unused, i = 0;
	unsigned int len, m = 0;
d1019 4
a1022 7
	for_each_sg(st->sgl, sg, st->nents, unused) {
		len = sg_dma_len(sg) >> PAGE_SHIFT;
		for (m = 0; m < len; m++) {
			addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
			iowrite32(pte_encode(dev, addr, level), &gtt_entries[i]);
			i++;
		}
a1024 3
	BUG_ON(i > max_entries);
	BUG_ON(i != obj->base.size / PAGE_SIZE);

d1032 2
a1033 1
		WARN_ON(readl(&gtt_entries[i-1]) != pte_encode(dev, addr, level));
a1041 1

d1043 11
d1055 4
a1058 23
/*
 * Binds an object into the global gtt with the specified cache level. The object
 * will be accessible to the GPU via commands whose operands reference offsets
 * within the global GTT as well as accessible by the GPU through the GMADR
 * mapped BAR (dev_priv->mm.gtt->gtt).
 */
static void gen6_ggtt_bind_object(struct drm_i915_gem_object *obj,
				  enum i915_cache_level level)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	const int first_entry = obj->gtt_space->start >> PAGE_SHIFT;
	const int max_entries = dev_priv->mm.gtt->gtt_total_entries - first_entry;
	int page_count = obj->base.size >> PAGE_SHIFT;
	bus_addr_t addr;
	int i;

	for (i = 0; i < page_count; i++) {
		struct vm_page *page = obj->pages[i];
		addr = VM_PAGE_TO_PHYS(page);
		bus_space_write_4(dev_priv->bst, dev_priv->mm.gtt->gtt,
		    (i + first_entry) * sizeof(gtt_pte_t),
		    pte_encode(dev, addr, level));
a1060 3
	BUG_ON(i > max_entries);
	BUG_ON(i != obj->base.size / PAGE_SIZE);

d1068 2
a1069 3
		WARN_ON(bus_space_read_4(dev_priv->bst, dev_priv->mm.gtt->gtt,
		     (i + first_entry - 1) * sizeof(gtt_pte_t)) !=
		     pte_encode(dev, addr, level));
d1078 41
d1120 6
a1125 1
#endif
d1128 23
d1152 4
a1155 12
void i915_gem_gtt_bind_object(struct drm_i915_gem_object *obj,
			      enum i915_cache_level cache_level)
{
	struct drm_device *dev = obj->base.dev;
	if (INTEL_INFO(dev)->gen < 6) {
		unsigned int flags = (cache_level == I915_CACHE_NONE) ?
			AGP_USER_MEMORY : AGP_USER_CACHED_MEMORY;
		intel_gtt_insert_sg_entries(obj->pages,
					    obj->gtt_space->start >> PAGE_SHIFT,
					    flags);
	} else {
		gen6_ggtt_bind_object(obj, cache_level);
d1157 2
d1160 7
a1166 1
	obj->has_global_gtt_mapping = 1;
d1168 9
d1178 6
a1183 1
#else
d1189 7
a1195 16
	if (INTEL_INFO(dev)->gen < 6) {
		unsigned int flags = (cache_level == I915_CACHE_NONE) ?
			0 : BUS_DMA_COHERENT;
		struct agp_softc *sc = dev->agp->agpdev;
		bus_addr_t addr = sc->sc_apaddr + obj->gtt_space->start;
		int page_count = obj->base.size >> PAGE_SHIFT;
		int i;

		for (i = 0; i < page_count; i++) {
			sc->sc_methods->bind_page(sc->sc_chipc, addr,
			    VM_PAGE_TO_PHYS(obj->pages[i]), flags);
			addr += PAGE_SIZE;
		}
	} else {
		gen6_ggtt_bind_object(obj, cache_level);
	}
a1199 2
#endif

d1202 8
a1209 3
	i915_ggtt_clear_range(obj->base.dev,
			      obj->gtt_space->start >> PAGE_SHIFT,
			      obj->base.size >> PAGE_SHIFT);
d1222 1
a1222 1
#ifdef notyet
d1249 72
a1320 4
void i915_gem_init_global_gtt(struct drm_device *dev,
			      unsigned long start,
			      unsigned long mappable_end,
			      unsigned long end)
d1322 8
a1329 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1331 5
a1335 4
	/* Substract the guard page ... */
	drm_mm_init(&dev_priv->mm.gtt_space, start, end - start - PAGE_SIZE);
	if (!HAS_LLC(dev))
		dev_priv->mm.gtt_space.color_adjust = i915_gtt_color_adjust;
d1337 1
a1337 5
	dev_priv->mm.gtt_start = start;
	dev_priv->mm.gtt_mappable_end = mappable_end;
	dev_priv->mm.gtt_end = end;
	dev_priv->mm.gtt_total = end - start;
	dev_priv->mm.mappable_gtt_total = min(end, mappable_end) - start;
d1339 10
a1348 2
	/* ... but ensure that we clear the entire range. */
	i915_ggtt_clear_range(dev, start / PAGE_SIZE, (end-start) / PAGE_SIZE);
d1373 2
a1374 2
	dev_priv->mm.gtt->scratch_page = page;
	dev_priv->mm.gtt->scratch_page_dma = dma_addr;
d1382 4
a1385 2
	set_pages_wb(dev_priv->mm.gtt->scratch_page, 1);
	pci_unmap_page(dev->pdev, dev_priv->mm.gtt->scratch_page_dma,
d1387 2
a1388 2
	put_page(dev_priv->mm.gtt->scratch_page);
	__free_page(dev_priv->mm.gtt->scratch_page);
d1403 2
a1404 2
	dev_priv->mm.gtt->scratch_page = page;
	dev_priv->mm.gtt->scratch_page_dma = page->segs[0].ds_addr;
d1413 1
a1413 1
	drm_dmamem_free(dev_priv->dmat, dev_priv->mm.gtt->scratch_page);
d1425 15
a1439 1
static inline unsigned int gen6_get_stolen_size(u16 snb_gmch_ctl)
d1446 1
a1446 1
static inline unsigned int gen7_get_stolen_size(u16 snb_gmch_ctl)
d1448 3
a1450 5
	static const int stolen_decoder[] = {
		0, 0, 0, 0, 0, 32, 48, 64, 128, 256, 96, 160, 224, 352};
	snb_gmch_ctl >>= IVB_GMCH_GMS_SHIFT;
	snb_gmch_ctl &= IVB_GMCH_GMS_MASK;
	return stolen_decoder[snb_gmch_ctl] << 20;
d1455 2
a1456 1
int i915_gem_gtt_init(struct drm_device *dev)
d1459 1
a1459 2
	phys_addr_t gtt_bus_addr;
	u16 snb_gmch_ctl;
d1462 3
a1464 9
	/* On modern platforms we need not worry ourself with the legacy
	 * hostbridge query stuff. Skip it entirely
	 */
	if (INTEL_INFO(dev)->gen < 6) {
		ret = intel_gmch_probe(dev_priv->bridge_dev, dev->pdev, NULL);
		if (!ret) {
			DRM_ERROR("failed to set up gmch\n");
			return -EIO;
		}
d1466 4
a1469 7
		dev_priv->mm.gtt = intel_gtt_get();
		if (!dev_priv->mm.gtt) {
			DRM_ERROR("Failed to initialize GTT\n");
			intel_gmch_remove();
			return -ENODEV;
		}
		return 0;
d1472 6
a1477 3
	dev_priv->mm.gtt = kzalloc(sizeof(*dev_priv->mm.gtt), GFP_KERNEL);
	if (!dev_priv->mm.gtt)
		return -ENOMEM;
d1479 2
a1480 2
	if (!pci_set_dma_mask(dev->pdev, DMA_BIT_MASK(40)))
		pci_set_consistent_dma_mask(dev->pdev, DMA_BIT_MASK(40));
d1482 1
a1482 3
#ifdef CONFIG_INTEL_IOMMU
	dev_priv->mm.gtt->needs_dmar = 1;
#endif
d1484 8
a1491 3
	/* For GEN6+ the PTEs for the ggtt live at 2MB + BAR0 */
	gtt_bus_addr = pci_resource_start(dev->pdev, 0) + (2<<20);
	dev_priv->mm.gtt->gma_bus_addr = pci_resource_start(dev->pdev, 2);
d1493 4
a1496 8
	/* i9xx_setup */
	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);
	dev_priv->mm.gtt->gtt_total_entries =
		gen6_get_total_gtt_size(snb_gmch_ctl) / sizeof(gtt_pte_t);
	if (INTEL_INFO(dev)->gen < 7)
		dev_priv->mm.gtt->stolen_size = gen6_get_stolen_size(snb_gmch_ctl);
	else
		dev_priv->mm.gtt->stolen_size = gen7_get_stolen_size(snb_gmch_ctl);
d1498 6
a1503 10
	dev_priv->mm.gtt->gtt_mappable_entries = pci_resource_len(dev->pdev, 2) >> PAGE_SHIFT;
	/* 64/512MB is the current min/max we actually know of, but this is just a
	 * coarse sanity check.
	 */
	if ((dev_priv->mm.gtt->gtt_mappable_entries >> 8) < 64 ||
	    dev_priv->mm.gtt->gtt_mappable_entries > dev_priv->mm.gtt->gtt_total_entries) {
		DRM_ERROR("Unknown GMADR entries (%d)\n",
			  dev_priv->mm.gtt->gtt_mappable_entries);
		ret = -ENXIO;
		goto err_out;
d1505 1
d1508 1
a1508 1
	if (ret) {
a1509 18
		goto err_out;
	}

	dev_priv->mm.gtt->gtt = ioremap_wc(gtt_bus_addr,
					   dev_priv->mm.gtt->gtt_total_entries * sizeof(gtt_pte_t));
	if (!dev_priv->mm.gtt->gtt) {
		DRM_ERROR("Failed to map the gtt page table\n");
		teardown_scratch_page(dev);
		ret = -ENOMEM;
		goto err_out;
	}

	/* GMADR is the PCI aperture used by SW to access tiled GFX surfaces in a linear fashion. */
	DRM_INFO("Memory usable by graphics device = %dM\n", dev_priv->mm.gtt->gtt_total_entries >> 8);
	DRM_DEBUG_DRIVER("GMADR size = %dM\n", dev_priv->mm.gtt->gtt_mappable_entries >> 8);
	DRM_DEBUG_DRIVER("GTT stolen size = %dM\n", dev_priv->mm.gtt->stolen_size >> 20);

	return 0;
a1510 4
err_out:
	kfree(dev_priv->mm.gtt);
	if (INTEL_INFO(dev)->gen < 6)
		intel_gmch_remove();
d1514 40
a1553 1
void i915_gem_gtt_fini(struct drm_device *dev)
d1556 8
a1563 6
	iounmap(dev_priv->mm.gtt->gtt);
	teardown_scratch_page(dev);
	if (INTEL_INFO(dev)->gen < 6)
		intel_gmch_remove();
	kfree(dev_priv->mm.gtt);
}
d1565 2
d1568 16
d1585 2
a1586 1
static void intel_gmch_remove(void) {};
d1588 8
a1595 1
int i915_gem_gtt_init(struct drm_device *dev)
d1598 1
a1598 2
	bus_addr_t gtt_bus_addr;
	bus_size_t size;
d1602 8
a1609 10
	/* On modern platforms we need not worry ourself with the legacy
	 * hostbridge query stuff. Skip it entirely
	 */
	if (INTEL_INFO(dev)->gen < 6) {
#if 0
		ret = intel_gmch_probe(dev_priv->bridge_dev, dev->pdev, NULL);
		if (!ret) {
			DRM_ERROR("failed to set up gmch\n");
			return -EIO;
		}
d1612 7
a1618 12
		dev_priv->mm.gtt = kzalloc(sizeof(*dev_priv->mm.gtt), GFP_KERNEL);
		if (!dev_priv->mm.gtt) {
			DRM_ERROR("Failed to initialize GTT\n");
			intel_gmch_remove();
			return -ENODEV;
		}
		dev_priv->mm.gtt->gtt_mappable_entries =
		    dev->agp->info.ai_aperture_size >> PAGE_SHIFT;
		dev_priv->mm.gtt->gtt_total_entries =
		    dev_priv->mm.gtt->gtt_mappable_entries;
		dev_priv->mm.gtt->gma_bus_addr = dev->agp->base;
		return 0;
d1621 1
a1621 5
	dev_priv->mm.gtt = kzalloc(sizeof(*dev_priv->mm.gtt), GFP_KERNEL);
	if (!dev_priv->mm.gtt)
		return -ENOMEM;

#if 0
d1625 17
d1643 2
a1644 2
#ifdef CONFIG_INTEL_IOMMU
	dev_priv->mm.gtt->needs_dmar = 1;
d1647 6
a1652 11
	/* For GEN6+ the PTEs for the ggtt live at 2MB + BAR0 */
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x10,
	    PCI_MAPREG_MEM_TYPE_64BIT, &gtt_bus_addr, NULL, NULL);
	if (ret)
		goto err_out;
	gtt_bus_addr += (2<<20);
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x18,
	    PCI_MAPREG_MEM_TYPE_64BIT, &dev_priv->mm.gtt->gma_bus_addr,
	    NULL, NULL);
	if (ret)
		goto err_out;
d1654 8
a1661 8
	/* i9xx_setup */
	snb_gmch_ctl = pci_conf_read(dev_priv->pc, dev_priv->tag, SNB_GMCH_CTRL);
	dev_priv->mm.gtt->gtt_total_entries =
		gen6_get_total_gtt_size(snb_gmch_ctl) / sizeof(gtt_pte_t);
	if (INTEL_INFO(dev)->gen < 7)
		dev_priv->mm.gtt->stolen_size = gen6_get_stolen_size(snb_gmch_ctl);
	else
		dev_priv->mm.gtt->stolen_size = gen7_get_stolen_size(snb_gmch_ctl);
d1663 4
a1666 14
	ret = -pci_mapreg_info(dev_priv->pc, dev_priv->tag, 0x18,
	    PCI_MAPREG_MEM_TYPE_64BIT, NULL, &size, NULL);
	if (ret)
		goto err_out;
	dev_priv->mm.gtt->gtt_mappable_entries = size >> PAGE_SHIFT;
	/* 64/512MB is the current min/max we actually know of, but this is just a
	 * coarse sanity check.
	 */
	if ((dev_priv->mm.gtt->gtt_mappable_entries >> 8) < 64 ||
	    dev_priv->mm.gtt->gtt_mappable_entries > dev_priv->mm.gtt->gtt_total_entries) {
		DRM_ERROR("Unknown GMADR entries (%d)\n",
			  dev_priv->mm.gtt->gtt_mappable_entries);
		ret = -ENXIO;
		goto err_out;
d1669 1
a1669 5
	ret = setup_scratch_page(dev);
	if (ret) {
		DRM_ERROR("Scratch setup failed\n");
		goto err_out;
	}
d1671 3
a1673 19
#if 0
	if (bus_space_map(dev_priv->bst, gtt_bus_addr,
	    dev_priv->mm.gtt->gtt_total_entries * sizeof(gtt_pte_t),
	    BUS_SPACE_MAP_PREFETCHABLE, &dev_priv->mm.gtt->gtt)) {
		DRM_ERROR("Failed to map the gtt page table\n");
		teardown_scratch_page(dev);
		ret = -ENOMEM;
		goto err_out;
	}
#else
	if (bus_space_subregion(dev_priv->bst, dev_priv->regs->bsh, (2<<20),
	    dev_priv->mm.gtt->gtt_total_entries * sizeof(gtt_pte_t),
	    &dev_priv->mm.gtt->gtt)) {
		DRM_ERROR("Failed to map the gtt page table %d\n", ret);
		teardown_scratch_page(dev);
		ret = -ENOMEM;
		goto err_out;
	}
#endif
d1675 2
a1676 4
	/* GMADR is the PCI aperture used by SW to access tiled GFX surfaces in a linear fashion. */
	DRM_INFO("Memory usable by graphics device = %dM\n", dev_priv->mm.gtt->gtt_total_entries >> 8);
	DRM_DEBUG_DRIVER("GMADR size = %dM\n", dev_priv->mm.gtt->gtt_mappable_entries >> 8);
	DRM_DEBUG_DRIVER("GTT stolen size = %dM\n", dev_priv->mm.gtt->stolen_size >> 20);
d1679 1
d1681 3
a1683 5
err_out:
	kfree(dev_priv->mm.gtt);
	if (INTEL_INFO(dev)->gen < 6)
		intel_gmch_remove();
	return ret;
d1686 1
a1686 1
void i915_gem_gtt_fini(struct drm_device *dev)
d1689 38
a1726 5
//	iounmap(dev_priv->mm.gtt->gtt);
	teardown_scratch_page(dev);
	if (INTEL_INFO(dev)->gen < 6)
		intel_gmch_remove();
	kfree(dev_priv->mm.gtt);
a1727 2

#endif
@


1.8
log
@Make obj->pages a simple array instead of an array of bus_dma_segment_t's.
Simplifies things a bit and reduces the diffs with Linux a bit too.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.7 2013/12/07 10:53:29 kettenis Exp $	*/
a33 1
#ifdef notyet
d49 1
a49 1
				   dma_addr_t addr,
d80 2
d377 2
a378 1
#if 0
d404 1
d406 1
d411 3
a413 2
	struct agp_softc *sc = dev->agp->agpdev;
	bus_addr_t addr = sc->sc_apaddr + (first_entry << PAGE_SHIFT);
d416 10
a425 3
	for (i = 0; i < num_entries; i++) {
		sc->sc_methods->unbind_page(sc->sc_chipc, addr);
		addr += PAGE_SIZE;
d427 12
d440 1
d475 2
a476 1
#ifdef notyet
d525 50
d577 2
a578 1
#if 0
d595 1
d597 1
d602 15
a616 25
	unsigned int flags = (cache_level == I915_CACHE_NONE) ?
		0 : BUS_DMA_COHERENT;
	struct agp_softc *sc = dev->agp->agpdev;
	bus_addr_t addr = sc->sc_apaddr + obj->gtt_space->start;
	int page_count = obj->base.size >> PAGE_SHIFT;
	int i;

	switch (cache_level) {
	case I915_CACHE_NONE:
		flags |= BUS_DMA_GTT_NOCACHE;
		break;
	case I915_CACHE_LLC:
		flags |= BUS_DMA_GTT_CACHE_LLC;
		break;
	case I915_CACHE_LLC_MLC:
		flags |= BUS_DMA_GTT_CACHE_LLC_MLC;
		break;
	default:
		BUG();
	}

	for (i = 0; i < page_count; i++) {
		sc->sc_methods->bind_page(sc->sc_chipc, addr,
		    VM_PAGE_TO_PHYS(obj->pages[i]), flags);
		addr += PAGE_SIZE;
d621 1
d690 2
a691 1
#ifdef notyet
d728 27
d778 2
d880 140
a1019 1
#endif /* notyet */
@


1.7
log
@No need to blow away the cache after updating GTT entries.  The driver already
flushes individual cache lines when needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.6 2013/12/01 20:19:15 kettenis Exp $	*/
a525 1
	bus_dma_segment_t *segp;
d528 1
a528 1
	int i, n;
a543 2
	segp = &obj->pages[0];
	n = 0;
d546 1
a546 6
					  segp->ds_addr + n, flags);
		n += PAGE_SIZE;
		if (n >= segp->ds_len) {
			n = 0;
			segp++;
		}
@


1.6
log
@The flush_tlb() method is a no-op so don't bother.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.5 2013/11/19 19:14:09 kettenis Exp $	*/
a414 1
	agp_flush_cache();
a556 1
	agp_flush_cache();
@


1.5
log
@Move the GTT management into the inteldrm driver.  It is really obvious now
that this is necessary as on some hardware we need guard pages between
regions that have different cache attributes.  Even if this appears to cause
regressions on some hardware, this change is a necessary (but not sufficient)
step to fix the cache coherency problems on the affected hardware.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.4 2013/10/05 07:30:06 jsg Exp $	*/
a415 1
	sc->sc_methods->flush_tlb(sc->sc_chipc);
a558 1
	sc->sc_methods->flush_tlb(sc->sc_chipc);
@


1.4
log
@add and use gtt mapping flags, further reduces the diff to linux
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.3 2013/08/13 10:23:49 jsg Exp $	*/
a31 1
#ifdef notyet
d34 1
d347 1
d353 1
d362 1
d369 1
d372 1
d376 1
d402 17
a418 1
#endif /* notyet */
d422 1
a422 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d425 4
d431 1
a431 1
		i915_gem_gtt_rebind_object(obj, obj->cache_level);
a436 1
#ifdef notyet
d442 1
d447 1
d452 1
d501 1
d503 1
d520 45
d583 1
d588 1
d632 1
a792 27

void
i915_gem_gtt_rebind_object(struct drm_i915_gem_object *obj,
			   enum i915_cache_level cache_level)
{
	struct drm_device *dev = obj->base.dev;
	drm_i915_private_t *dev_priv = dev->dev_private;
	int flags = obj->dma_flags;

	switch (cache_level) {
	case I915_CACHE_NONE:
		flags |= BUS_DMA_GTT_NOCACHE;
		break;
	case I915_CACHE_LLC:
		flags |= BUS_DMA_GTT_CACHE_LLC;
		break;
	case I915_CACHE_LLC_MLC:
		flags |= BUS_DMA_GTT_CACHE_LLC_MLC;
		break;
	default:
		BUG();
	}

	agp_bus_dma_rebind(dev_priv->agpdmat, obj->dmamap, flags);

	obj->has_global_gtt_mapping = 1;
}
@


1.3
log
@add static back to functions that originally had it
reduces the diff to linux and makes ddb hangman a little easier
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.2 2013/08/09 07:55:42 jsg Exp $	*/
d739 2
@


1.2
log
@add commented out versions of unused functions present in the original
files to reduce the diff to linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_gtt.c,v 1.1 2013/03/18 12:36:52 jsg Exp $	*/
d398 1
a398 2
void
i915_gem_restore_gtt_mappings(struct drm_device *dev)
@


1.1
log
@Significantly increase the wordlist for ddb hangman,
and update our device independent DRM code and the Intel DRM code
to be mostly in sync with Linux 3.8.3.  Among other things this
brings support for kernel modesetting and enables use of
the rings on gen6+ Intel hardware.

Based on some earlier work from matthieu@@ with some hints from FreeBSD
and with lots of help from kettenis@@ (including a beautiful accelerated
wscons framebuffer console!)

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d29 1
d32 167
a198 2
void
i915_gem_cleanup_aliasing_ppgtt(struct drm_device *dev)
d201 194
d396 1
d411 305
@

