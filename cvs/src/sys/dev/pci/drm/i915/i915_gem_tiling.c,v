head	1.20;
access;
symbols
	OPENBSD_6_1:1.19.0.8
	OPENBSD_6_1_BASE:1.19
	OPENBSD_6_0:1.19.0.6
	OPENBSD_6_0_BASE:1.19
	OPENBSD_5_9:1.19.0.2
	OPENBSD_5_9_BASE:1.19
	OPENBSD_5_8:1.18.0.4
	OPENBSD_5_8_BASE:1.18
	OPENBSD_5_7:1.16.0.4
	OPENBSD_5_7_BASE:1.16
	OPENBSD_5_6:1.14.0.6
	OPENBSD_5_6_BASE:1.14
	OPENBSD_5_5:1.14.0.4
	OPENBSD_5_5_BASE:1.14
	OPENBSD_5_4:1.5.0.2
	OPENBSD_5_4_BASE:1.5;
locks; strict;
comment	@ * @;


1.20
date	2017.07.01.16.14.10;	author kettenis;	state Exp;
branches;
next	1.19;
commitid	KnwRPOZok9A30HI4;

1.19
date	2015.09.23.23.12.12;	author kettenis;	state Exp;
branches;
next	1.18;
commitid	lQlppvmETCN49oZe;

1.18
date	2015.04.18.14.47.34;	author jsg;	state Exp;
branches;
next	1.17;
commitid	c1fUeeFWMNg4COgR;

1.17
date	2015.04.08.02.28.13;	author jsg;	state Exp;
branches;
next	1.16;
commitid	pBZw25gbMMahiUV2;

1.16
date	2015.02.10.01.39.32;	author jsg;	state Exp;
branches;
next	1.15;
commitid	a8Vt7gSt34kmziIS;

1.15
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.14;
commitid	yv0ECmCdICvq576h;

1.14
date	2014.01.30.15.10.48;	author kettenis;	state Exp;
branches;
next	1.13;

1.13
date	2014.01.21.08.57.22;	author kettenis;	state Exp;
branches;
next	1.12;

1.12
date	2013.12.15.11.17.36;	author kettenis;	state Exp;
branches;
next	1.11;

1.11
date	2013.12.11.20.31.43;	author kettenis;	state Exp;
branches;
next	1.10;

1.10
date	2013.11.20.21.55.23;	author kettenis;	state Exp;
branches;
next	1.9;

1.9
date	2013.11.19.19.14.09;	author kettenis;	state Exp;
branches;
next	1.8;

1.8
date	2013.10.29.06.30.57;	author jsg;	state Exp;
branches;
next	1.7;

1.7
date	2013.08.13.10.23.49;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2013.08.07.19.49.07;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2013.05.18.21.43.42;	author kettenis;	state Exp;
branches;
next	1.4;

1.4
date	2013.05.18.16.45.34;	author kettenis;	state Exp;
branches;
next	1.3;

1.3
date	2013.04.21.14.41.26;	author kettenis;	state Exp;
branches;
next	1.2;

1.2
date	2013.03.29.05.15.42;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2013.03.18.12.36.52;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.20
log
@Update inteldrm(4) to code based on Linux 4.4.70.  This brings us support for
Skylake and Cherryview and better support for Broadwell and Valleyview.  Also
adds MST support.  Some tweaks to the TTM code and radeondrm(4) to keep it
working with the updated generic DRM code needed for inteldrm(4).

Tested by many.
@
text
@/*
 * Copyright Â© 2008 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 *
 * Authors:
 *    Eric Anholt <eric@@anholt.net>
 *
 */

#ifdef __linux__
#include <linux/string.h>
#include <linux/bitops.h>
#endif
#include <dev/pci/drm/drmP.h>
#include <dev/pci/drm/i915_drm.h>
#include "i915_drv.h"

/**
 * DOC: buffer object tiling
 *
 * i915_gem_set_tiling() and i915_gem_get_tiling() is the userspace interface to
 * declare fence register requirements.
 *
 * In principle GEM doesn't care at all about the internal data layout of an
 * object, and hence it also doesn't care about tiling or swizzling. There's two
 * exceptions:
 *
 * - For X and Y tiling the hardware provides detilers for CPU access, so called
 *   fences. Since there's only a limited amount of them the kernel must manage
 *   these, and therefore userspace must tell the kernel the object tiling if it
 *   wants to use fences for detiling.
 * - On gen3 and gen4 platforms have a swizzling pattern for tiled objects which
 *   depends upon the physical page frame number. When swapping such objects the
 *   page frame number might change and the kernel must be able to fix this up
 *   and hence now the tiling. Note that on a subset of platforms with
 *   asymmetric memory channel population the swizzling pattern changes in an
 *   unknown way, and for those the kernel simply forbids swapping completely.
 *
 * Since neither of this applies for new tiling layouts on modern platforms like
 * W, Ys and Yf tiling GEM only allows object tiling to be set to X or Y tiled.
 * Anything else can be handled in userspace entirely without the kernel's
 * invovlement.
 */

/* Check pitch constriants for all chips & tiling formats */
static bool
i915_tiling_ok(struct drm_device *dev, int stride, int size, int tiling_mode)
{
	int tile_width;

	/* Linear is always fine */
	if (tiling_mode == I915_TILING_NONE)
		return true;

	if (IS_GEN2(dev) ||
	    (tiling_mode == I915_TILING_Y && HAS_128_BYTE_Y_TILING(dev)))
		tile_width = 128;
	else
		tile_width = 512;

	/* check maximum stride & object size */
	/* i965+ stores the end address of the gtt mapping in the fence
	 * reg, so dont bother to check the size */
	if (INTEL_INFO(dev)->gen >= 7) {
		if (stride / 128 > GEN7_FENCE_MAX_PITCH_VAL)
			return false;
	} else if (INTEL_INFO(dev)->gen >= 4) {
		if (stride / 128 > I965_FENCE_MAX_PITCH_VAL)
			return false;
	} else {
		if (stride > 8192)
			return false;

		if (IS_GEN3(dev)) {
			if (size > I830_FENCE_MAX_SIZE_VAL << 20)
				return false;
		} else {
			if (size > I830_FENCE_MAX_SIZE_VAL << 19)
				return false;
		}
	}

	if (stride < tile_width)
		return false;

	/* 965+ just needs multiples of tile width */
	if (INTEL_INFO(dev)->gen >= 4) {
		if (stride & (tile_width - 1))
			return false;
		return true;
	}

	/* Pre-965 needs power of two tile widths */
	if (stride & (stride - 1))
		return false;

	return true;
}

/* Is the current GTT allocation valid for the change in tiling? */
static bool
i915_gem_object_fence_ok(struct drm_i915_gem_object *obj, int tiling_mode)
{
	u32 size;

	if (tiling_mode == I915_TILING_NONE)
		return true;

	if (INTEL_INFO(obj->base.dev)->gen >= 4)
		return true;

	if (INTEL_INFO(obj->base.dev)->gen == 3) {
		if (i915_gem_obj_ggtt_offset(obj) & ~I915_FENCE_START_MASK)
			return false;
	} else {
		if (i915_gem_obj_ggtt_offset(obj) & ~I830_FENCE_START_MASK)
			return false;
	}

	size = i915_gem_get_gtt_size(obj->base.dev, obj->base.size, tiling_mode);
	if (i915_gem_obj_ggtt_size(obj) != size)
		return false;

	if (i915_gem_obj_ggtt_offset(obj) & (size - 1))
		return false;

	return true;
}

/**
 * i915_gem_set_tiling - IOCTL handler to set tiling mode
 * @@dev: DRM device
 * @@data: data pointer for the ioctl
 * @@file: DRM file for the ioctl call
 *
 * Sets the tiling mode of an object, returning the required swizzling of
 * bit 6 of addresses in the object.
 *
 * Called by the user via ioctl.
 *
 * Returns:
 * Zero on success, negative errno on failure.
 */
int
i915_gem_set_tiling(struct drm_device *dev, void *data,
		   struct drm_file *file)
{
	struct drm_i915_gem_set_tiling *args = data;
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct drm_i915_gem_object *obj;
	int ret = 0;

	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
	if (&obj->base == NULL)
		return -ENOENT;

	if (!i915_tiling_ok(dev,
			    args->stride, obj->base.size, args->tiling_mode)) {
		drm_gem_object_unreference_unlocked(&obj->base);
		return -EINVAL;
	}

	mutex_lock(&dev->struct_mutex);
	if (obj->pin_display || obj->framebuffer_references) {
		ret = -EBUSY;
		goto err;
	}

	if (args->tiling_mode == I915_TILING_NONE) {
		args->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
		args->stride = 0;
	} else {
		if (args->tiling_mode == I915_TILING_X)
			args->swizzle_mode = dev_priv->mm.bit_6_swizzle_x;
		else
			args->swizzle_mode = dev_priv->mm.bit_6_swizzle_y;

		/* Hide bit 17 swizzling from the user.  This prevents old Mesa
		 * from aborting the application on sw fallbacks to bit 17,
		 * and we use the pread/pwrite bit17 paths to swizzle for it.
		 * If there was a user that was relying on the swizzle
		 * information for drm_intel_bo_map()ed reads/writes this would
		 * break it, but we don't have any of those.
		 */
		if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_17)
			args->swizzle_mode = I915_BIT_6_SWIZZLE_9;
		if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_10_17)
			args->swizzle_mode = I915_BIT_6_SWIZZLE_9_10;

		/* If we can't handle the swizzling, make it untiled. */
		if (args->swizzle_mode == I915_BIT_6_SWIZZLE_UNKNOWN) {
			args->tiling_mode = I915_TILING_NONE;
			args->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
			args->stride = 0;
		}
	}

	if (args->tiling_mode != obj->tiling_mode ||
	    args->stride != obj->stride) {
		/* We need to rebind the object if its current allocation
		 * no longer meets the alignment restrictions for its new
		 * tiling mode. Otherwise we can just leave it alone, but
		 * need to ensure that any fence register is updated before
		 * the next fenced (either through the GTT or by the BLT unit
		 * on older GPUs) access.
		 *
		 * After updating the tiling parameters, we then flag whether
		 * we need to update an associated fence register. Note this
		 * has to also include the unfenced register the GPU uses
		 * whilst executing a fenced command for an untiled object.
		 */
		if (obj->map_and_fenceable &&
		    !i915_gem_object_fence_ok(obj, args->tiling_mode))
			ret = i915_gem_object_ggtt_unbind(obj);

		if (ret == 0) {
			if (obj->pages &&
			    obj->madv == I915_MADV_WILLNEED &&
			    dev_priv->quirks & QUIRK_PIN_SWIZZLED_PAGES) {
				if (args->tiling_mode == I915_TILING_NONE)
					i915_gem_object_unpin_pages(obj);
				if (obj->tiling_mode == I915_TILING_NONE)
					i915_gem_object_pin_pages(obj);
			}

			obj->fence_dirty =
				obj->last_fenced_req ||
				obj->fence_reg != I915_FENCE_REG_NONE;

			obj->tiling_mode = args->tiling_mode;
			obj->stride = args->stride;

			/* Force the fence to be reacquired for GTT access */
			i915_gem_release_mmap(obj);
		}
	}
	/* we have to maintain this existing ABI... */
	args->stride = obj->stride;
	args->tiling_mode = obj->tiling_mode;

	/* Try to preallocate memory required to save swizzling on put-pages */
	if (i915_gem_object_needs_bit17_swizzle(obj)) {
		if (obj->bit_17 == NULL) {
			obj->bit_17 = kcalloc(BITS_TO_LONGS(obj->base.size >> PAGE_SHIFT),
					      sizeof(long), GFP_KERNEL);
		}
	} else {
		kfree(obj->bit_17);
		obj->bit_17 = NULL;
	}

err:
	drm_gem_object_unreference(&obj->base);
	mutex_unlock(&dev->struct_mutex);

	return ret;
}

/**
 * i915_gem_get_tiling - IOCTL handler to get tiling mode
 * @@dev: DRM device
 * @@data: data pointer for the ioctl
 * @@file: DRM file for the ioctl call
 *
 * Returns the current tiling mode and required bit 6 swizzling for the object.
 *
 * Called by the user via ioctl.
 *
 * Returns:
 * Zero on success, negative errno on failure.
 */
int
i915_gem_get_tiling(struct drm_device *dev, void *data,
		   struct drm_file *file)
{
	struct drm_i915_gem_get_tiling *args = data;
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct drm_i915_gem_object *obj;

	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
	if (&obj->base == NULL)
		return -ENOENT;

	mutex_lock(&dev->struct_mutex);

	args->tiling_mode = obj->tiling_mode;
	switch (obj->tiling_mode) {
	case I915_TILING_X:
		args->swizzle_mode = dev_priv->mm.bit_6_swizzle_x;
		break;
	case I915_TILING_Y:
		args->swizzle_mode = dev_priv->mm.bit_6_swizzle_y;
		break;
	case I915_TILING_NONE:
		args->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
		break;
	default:
		DRM_ERROR("unknown tiling mode\n");
	}

	/* Hide bit 17 from the user -- see comment in i915_gem_set_tiling */
	if (dev_priv->quirks & QUIRK_PIN_SWIZZLED_PAGES)
		args->phys_swizzle_mode = I915_BIT_6_SWIZZLE_UNKNOWN;
	else
		args->phys_swizzle_mode = args->swizzle_mode;
	if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_17)
		args->swizzle_mode = I915_BIT_6_SWIZZLE_9;
	if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_10_17)
		args->swizzle_mode = I915_BIT_6_SWIZZLE_9_10;

	drm_gem_object_unreference(&obj->base);
	mutex_unlock(&dev->struct_mutex);

	return 0;
}
@


1.19
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@a0 16
/*	$OpenBSD: i915_gem_tiling.c,v 1.18 2015/04/18 14:47:34 jsg Exp $	*/
/*
 * Copyright (c) 2008-2009 Owain G. Ainsworth <oga@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
d28 4
a32 1
#include <dev/pci/drm/drm.h>
d36 2
a37 6
#include <machine/pmap.h>

#include <sys/queue.h>
#include <sys/task.h>

/** @@file i915_gem_tiling.c
d39 2
a40 1
 * Support for managing tiling state of buffer objects.
d42 19
a60 43
 * The idea behind tiling is to increase cache hit rates by rearranging
 * pixel data so that a group of pixel accesses are in the same cacheline.
 * Performance improvement from doing this on the back/depth buffer are on
 * the order of 30%.
 *
 * Intel architectures make this somewhat more complicated, though, by
 * adjustments made to addressing of data when the memory is in interleaved
 * mode (matched pairs of DIMMS) to improve memory bandwidth.
 * For interleaved memory, the CPU sends every sequential 64 bytes
 * to an alternate memory channel so it can get the bandwidth from both.
 *
 * The GPU also rearranges its accesses for increased bandwidth to interleaved
 * memory, and it matches what the CPU does for non-tiled.  However, when tiled
 * it does it a little differently, since one walks addresses not just in the
 * X direction but also Y.  So, along with alternating channels when bit
 * 6 of the address flips, it also alternates when other bits flip --  Bits 9
 * (every 512 bytes, an X tile scanline) and 10 (every two X tile scanlines)
 * are common to both the 915 and 965-class hardware.
 *
 * The CPU also sometimes XORs in higher bits as well, to improve
 * bandwidth doing strided access like we do so frequently in graphics.  This
 * is called "Channel XOR Randomization" in the MCH documentation.  The result
 * is that the CPU is XORing in either bit 11 or bit 17 to bit 6 of its address
 * decode.
 *
 * All of this bit 6 XORing has an effect on our memory management,
 * as we need to make sure that the 3d driver can correctly address object
 * contents.
 *
 * If we don't have interleaved memory, all tiling is safe and no swizzling is
 * required.
 *
 * When bit 17 is XORed in, we simply refuse to tile at all.  Bit
 * 17 is not just a page offset, so as we page an objet out and back in,
 * individual pages in it will have different bit 17 addresses, resulting in
 * each 64 bytes being swapped with its neighbor!
 *
 * Otherwise, if interleaved, we have to tell the 3d driver what the address
 * swizzling it needs to do is, since it's writing with the CPU to the pages
 * (bit 6 and potentially bit 11 XORed in), and the GPU is reading from the
 * pages (bit 6, 9, and 10 XORed in), resulting in a cumulative bit swizzling
 * required by the CPU of XORing in bit 6, 9, 10, and potentially 11, in order
 * to match what the GPU expects.
a62 120
/**
 * Detects bit 6 swizzling of address lookup between IGD access and CPU
 * access through main memory.
 */
void
i915_gem_detect_bit_6_swizzle(struct drm_device *dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	uint32_t swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN;
	uint32_t swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN;

	if (IS_VALLEYVIEW(dev)) {
		swizzle_x = I915_BIT_6_SWIZZLE_NONE;
		swizzle_y = I915_BIT_6_SWIZZLE_NONE;
	} else if (INTEL_INFO(dev)->gen >= 6) {
		uint32_t dimm_c0, dimm_c1;
		dimm_c0 = I915_READ(MAD_DIMM_C0);
		dimm_c1 = I915_READ(MAD_DIMM_C1);
		dimm_c0 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
		dimm_c1 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
		/* Enable swizzling when the channels are populated with
		 * identically sized dimms. We don't need to check the 3rd
		 * channel because no cpu with gpu attached ships in that
		 * configuration. Also, swizzling only makes sense for 2
		 * channels anyway. */
		if (dimm_c0 == dimm_c1) {
			swizzle_x = I915_BIT_6_SWIZZLE_9_10;
			swizzle_y = I915_BIT_6_SWIZZLE_9;
		} else {
			swizzle_x = I915_BIT_6_SWIZZLE_NONE;
			swizzle_y = I915_BIT_6_SWIZZLE_NONE;
		}
	} else if (IS_GEN5(dev)) {
		/* On Ironlake whatever DRAM config, GPU always do
		 * same swizzling setup.
		 */
		swizzle_x = I915_BIT_6_SWIZZLE_9_10;
		swizzle_y = I915_BIT_6_SWIZZLE_9;
	} else if (IS_GEN2(dev)) {
		/* As far as we know, the 865 doesn't have these bit 6
		 * swizzling issues.
		 */
		swizzle_x = I915_BIT_6_SWIZZLE_NONE;
		swizzle_y = I915_BIT_6_SWIZZLE_NONE;
	} else if (IS_MOBILE(dev) || (IS_GEN3(dev) && !IS_G33(dev))) {
		uint32_t dcc;

		/* On 9xx chipsets, channel interleave by the CPU is
		 * determined by DCC.  For single-channel, neither the CPU
		 * nor the GPU do swizzling.  For dual channel interleaved,
		 * the GPU's interleave is bit 9 and 10 for X tiled, and bit
		 * 9 for Y tiled.  The CPU's interleave is independent, and
		 * can be based on either bit 11 (haven't seen this yet) or
		 * bit 17 (common).
		 */
		dcc = I915_READ(DCC);
		switch (dcc & DCC_ADDRESSING_MODE_MASK) {
		case DCC_ADDRESSING_MODE_SINGLE_CHANNEL:
		case DCC_ADDRESSING_MODE_DUAL_CHANNEL_ASYMMETRIC:
			swizzle_x = I915_BIT_6_SWIZZLE_NONE;
			swizzle_y = I915_BIT_6_SWIZZLE_NONE;
			break;
		case DCC_ADDRESSING_MODE_DUAL_CHANNEL_INTERLEAVED:
			if (dcc & DCC_CHANNEL_XOR_DISABLE) {
				/* This is the base swizzling by the GPU for
				 * tiled buffers.
				 */
				swizzle_x = I915_BIT_6_SWIZZLE_9_10;
				swizzle_y = I915_BIT_6_SWIZZLE_9;
			} else if ((dcc & DCC_CHANNEL_XOR_BIT_17) == 0) {
				/* Bit 11 swizzling by the CPU in addition. */
				swizzle_x = I915_BIT_6_SWIZZLE_9_10_11;
				swizzle_y = I915_BIT_6_SWIZZLE_9_11;
			} else {
				/* Bit 17 swizzling by the CPU in addition. */
				swizzle_x = I915_BIT_6_SWIZZLE_9_10_17;
				swizzle_y = I915_BIT_6_SWIZZLE_9_17;
			}
			break;
		}
		if (dcc == 0xffffffff) {
			DRM_ERROR("Couldn't read from MCHBAR.  "
				  "Disabling tiling.\n");
			swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN;
			swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN;
		}
	} else {
		/* The 965, G33, and newer, have a very flexible memory
		 * configuration.  It will enable dual-channel mode
		 * (interleaving) on as much memory as it can, and the GPU
		 * will additionally sometimes enable different bit 6
		 * swizzling for tiled objects from the CPU.
		 *
		 * Here's what I found on the G965:
		 *    slot fill         memory size  swizzling
		 * 0A   0B   1A   1B    1-ch   2-ch
		 * 512  0    0    0     512    0     O
		 * 512  0    512  0     16     1008  X
		 * 512  0    0    512   16     1008  X
		 * 0    512  0    512   16     1008  X
		 * 1024 1024 1024 0     2048   1024  O
		 *
		 * We could probably detect this based on either the DRB
		 * matching, which was the case for the swizzling required in
		 * the table above, or from the 1-ch value being less than
		 * the minimum size of a rank.
		 */
		if (I915_READ16(C0DRB3) != I915_READ16(C1DRB3)) {
			swizzle_x = I915_BIT_6_SWIZZLE_NONE;
			swizzle_y = I915_BIT_6_SWIZZLE_NONE;
		} else {
			swizzle_x = I915_BIT_6_SWIZZLE_9_10;
			swizzle_y = I915_BIT_6_SWIZZLE_9;
		}
	}

	dev_priv->mm.bit_6_swizzle_x = swizzle_x;
	dev_priv->mm.bit_6_swizzle_y = swizzle_y;
}

d149 5
d156 5
d167 1
a167 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d181 4
a184 3
	if (obj->pin_count || obj->framebuffer_references) {
		drm_gem_object_unreference_unlocked(&obj->base);
		return -EBUSY;
a215 1
	mutex_lock(&dev->struct_mutex);
d230 3
d234 9
a242 15
		obj->map_and_fenceable =
			!i915_gem_obj_ggtt_bound(obj) ||
			(i915_gem_obj_ggtt_offset(obj) +
			 obj->base.size <= dev_priv->gtt.mappable_end &&
			 i915_gem_object_fence_ok(obj, args->tiling_mode));

		/* Rebind if we need a change of alignment */
		if (!obj->map_and_fenceable) {
			u32 unfenced_align =
				i915_gem_get_gtt_alignment(dev, obj->base.size,
							    args->tiling_mode,
							    false);
			if (i915_gem_obj_ggtt_offset(obj) & (unfenced_align - 1))
				ret = i915_gem_object_ggtt_unbind(obj);
		}
a243 1
		if (ret == 0) {
d245 1
a245 1
				obj->fenced_gpu_access ||
d270 1
d278 5
d284 5
d295 1
a295 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d320 4
a332 68
}

/**
 * Swap every 64 bytes of this page around, to account for it having a new
 * bit 17 of its physical address and therefore being interpreted differently
 * by the GPU.
 */
static void
i915_gem_swizzle_page(struct vm_page *page)
{
	char temp[64];
	char *vaddr;
	int i;

	vaddr = kmap(page);

	for (i = 0; i < PAGE_SIZE; i += 128) {
		memcpy(temp, &vaddr[i], 64);
		memcpy(&vaddr[i], &vaddr[i + 64], 64);
		memcpy(&vaddr[i + 64], temp, 64);
	}

	kunmap(vaddr);
}

void
i915_gem_object_do_bit_17_swizzle(struct drm_i915_gem_object *obj)
{
	int page_count = obj->base.size >> PAGE_SHIFT;
	int i;

	if (obj->bit_17 == NULL)
		return;

	for (i = 0; i < page_count; i++) {
		struct vm_page *page = obj->pages[i];
		char new_bit_17 = page_to_phys(page) >> 17;
		if ((new_bit_17 & 0x1) !=
		    (test_bit(i, obj->bit_17) != 0)) {
			i915_gem_swizzle_page(page);
			set_page_dirty(page);
		}
	}
}

void
i915_gem_object_save_bit_17_swizzle(struct drm_i915_gem_object *obj)
{
	int page_count = obj->base.size >> PAGE_SHIFT;
	int i;

	if (obj->bit_17 == NULL) {
		obj->bit_17 = kcalloc(BITS_TO_LONGS(page_count),
				      sizeof(long), GFP_KERNEL);
		if (obj->bit_17 == NULL) {
			DRM_ERROR("Failed to allocate memory for bit 17 "
				  "record\n");
			return;
		}
	}

	for (i = 0; i < page_count; i++) {
		struct vm_page *page = obj->pages[i];
		if (page_to_phys(page) & (1 << 17))
			set_bit(i, obj->bit_17);
		else
			clear_bit(i, obj->bit_17);
	}
@


1.18
log
@another round of reducing the diff to linux
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.17 2015/04/08 02:28:13 jsg Exp $	*/
d240 6
a245 3
	if (INTEL_INFO(dev)->gen >= 4) {
		/* i965 stores the end address of the gtt mapping in the fence
		 * reg, so dont bother to check the size */
d261 3
a271 3
	if (stride < tile_width)
		return false;

d291 1
a291 1
		if (obj->gtt_offset & ~I915_FENCE_START_MASK)
d294 1
a294 1
		if (obj->gtt_offset & ~I830_FENCE_START_MASK)
d298 2
a299 13
	/*
	 * Previous chips need to be aligned to the size of the smallest
	 * fence register that can contain the object.
	 */
	if (INTEL_INFO(obj->base.dev)->gen == 3)
		size = 1024*1024;
	else
		size = 512*1024;

	while (size < obj->base.size)
		size <<= 1;

	if (obj->gtt_space->size != size)
d302 1
a302 1
	if (obj->gtt_offset & (size - 1))
d331 1
a331 1
	if (obj->pin_count) {
d382 3
a384 2
			obj->gtt_space == NULL ||
			(obj->gtt_offset + obj->base.size <= dev_priv->mm.gtt_mappable_end &&
d389 6
a394 6
			u32 unfenced_alignment =
				i915_gem_get_unfenced_gtt_alignment(dev,
								    obj->base.size,
								    args->tiling_mode);
			if (obj->gtt_offset & (unfenced_alignment - 1))
				ret = i915_gem_object_unbind(obj);
d412 12
d524 2
a525 6
		/* round up number of pages to a multiple of 32 so we know what
		 * size to make the bitmask. XXX this is wasteful with malloc
		 * and a better way should be done
		 */
		size_t nb17 = ((page_count + 31) & ~31)/32;
		obj->bit_17 = kmalloc(nb17 * sizeof(u_int32_t), GFP_KERNEL);
@


1.17
log
@ttm has it's own version of kmap/kunmap that uses
kernel_map/uvm_km_valloc and i915 has a version that uses
phys_map/uvm_km_valloc_wait as calling code assumes kmap would
sleep if no memory is available.

Move these and ttm's vmap/vunmap into the linux compat files
and make them all use phys_map/uvm_km_valloc_wait.

looks good kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.16 2015/02/10 01:39:32 jsg Exp $	*/
d503 1
a503 1
		char new_bit_17 = VM_PAGE_TO_PHYS(page) >> 17;
d507 1
a507 1
			atomic_clearbits_int(&page->pg_flags, PG_CLEAN);
d534 1
a534 1
		if (VM_PAGE_TO_PHYS(page) & (1 << 17))
@


1.16
log
@Remove DRM_LOCK macros, rename dev_lock to struct_mutex and directly
call linux style lock functions where these macros were used.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.15 2014/11/16 12:31:00 deraadt Exp $	*/
d475 1
a475 1
i915_gem_swizzle_page(struct vm_page *pg)
a479 1
	vaddr_t	 va;
d481 1
a481 8
#if defined (__HAVE_PMAP_DIRECT)
	va = pmap_map_direct(pg);
#else
	va = uvm_km_valloc_wait(phys_map, PAGE_SIZE);
	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());
#endif
	vaddr = (char *)va;
d489 1
a489 7
#if defined (__HAVE_PMAP_DIRECT)
	pmap_unmap_direct(va);
#else
	pmap_kremove(va, PAGE_SIZE);
	pmap_update(pmap_kernel());
	uvm_km_free_wakeup(phys_map, va, PAGE_SIZE);
#endif
@


1.15
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.14 2014/01/30 15:10:48 kettenis Exp $	*/
d373 1
a373 1
	DRM_LOCK();
d420 1
a420 1
	DRM_UNLOCK();
d440 1
a440 1
	DRM_LOCK();
d464 1
a464 1
	DRM_UNLOCK();
@


1.14
log
@Get rid of the simple locks; they're lies.  Also get rid of the holding
logic for uvm objects; it's not necessary anymore as far as I can tell, at
least as long as we run all this code under the kernel lock.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.13 2014/01/21 08:57:22 kettenis Exp $	*/
d486 1
a486 1
	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), VM_PROT_READ|VM_PROT_WRITE);
@


1.13
log
@Use Linux compat functions to do kernel memory allocations in the bits of code
that are shared with Linux.  Use OpenBSD functions in the few sports where we
have our own implementation of bits.

discussed with jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.12 2013/12/15 11:17:36 kettenis Exp $	*/
a331 1
	drm_hold_object(&obj->base);
d335 2
a336 2
		ret = -EINVAL;
		goto out;
d340 2
a341 2
		ret = -EBUSY;
		goto out;
d419 1
a420 2
out:
	drm_unhold_and_unref(&obj->base);
a438 1
	drm_hold_object(&obj->base);
d463 1
a464 1
	drm_unhold_and_unref(&obj->base);
@


1.12
log
@Abuse phys_map to reliably wait for kva.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.11 2013/12/11 20:31:43 kettenis Exp $	*/
d541 1
a541 1
		obj->bit_17 = drm_alloc(nb17 * sizeof(u_int32_t));
@


1.11
log
@Make obj->pages a simple array instead of an array of bus_dma_segment_t's.
Simplifies things a bit and reduces the diffs with Linux a bit too.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.10 2013/11/20 21:55:23 kettenis Exp $	*/
d488 1
a488 1
	va = uvm_km_valloc_wait(kernel_map, PAGE_SIZE);
d505 1
a505 1
	uvm_km_free(kernel_map, va, PAGE_SIZE);
@


1.10
log
@Silly Linux-style negative errno returns.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.9 2013/11/19 19:14:09 kettenis Exp $	*/
d477 1
a477 1
static int
d488 2
a489 4
	va = uvm_km_valloc(kernel_map, PAGE_SIZE);
	if (va == 0)
		return (ENOMEM);
	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW);
a506 1
	return (0);
a511 2
	struct vm_page *pg;
	bus_dma_segment_t *segp;
d513 1
a513 1
	int i, n, ret;
a517 2
	segp = &obj->pages[0];
	n = 0;
d519 2
a520 1
		char new_bit_17 = (segp->ds_addr + n) >> 17;
d523 2
a524 15
			/* XXX move this to somewhere where we already have pg */
			pg = PHYS_TO_VM_PAGE(segp->ds_addr + n);
			KASSERT(pg != NULL);
			ret = i915_gem_swizzle_page(pg);
			if (ret) {
				printf("%s: page swizzle failed\n", __func__);
				return;
			}
			atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
		}

		n += PAGE_SIZE;
		if (n >= segp->ds_len) {
			n = 0;
			segp++;
a531 1
	bus_dma_segment_t *segp;
d533 1
a533 1
	int i, n;
a548 2
	segp = &obj->pages[0];
	n = 0;
d550 2
a551 1
		if ((segp->ds_addr + n) & (1 << 17))
a554 6

		n += PAGE_SIZE;
		if (n >= segp->ds_len) {
			n = 0;
			segp++;
		}
@


1.9
log
@Move the GTT management into the inteldrm driver.  It is really obvious now
that this is necessary as on some hardware we need guard pages between
regions that have different cache attributes.  Even if this appears to cause
regressions on some hardware, this change is a necessary (but not sufficient)
step to fix the cache coherency problems on the affected hardware.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.8 2013/10/29 06:30:57 jsg Exp $	*/
d331 1
a331 1
		return ENOENT;
d336 1
a336 1
		ret = EINVAL;
d341 1
a341 1
		ret = EBUSY;
d440 1
a440 1
		return ENOENT;
@


1.8
log
@Move most of the uses of workqs in drm to the new task/taskq api.
Prevents unintended multiple additions to workqs that was causing
hangs on radeon, and lets us remove tasks more closely matching
the behaviour of the original linux code.

ok kettenis@@
cause of the ttm/radeon hangs debugged by claudio@@ and kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.7 2013/08/13 10:23:49 jsg Exp $	*/
d307 1
a307 1
	if (obj->dmamap->dm_segs[0].ds_len != size)
d391 1
a391 1
			obj->dmamap == NULL ||
@


1.7
log
@add static back to functions that originally had it
reduces the diff to linux and makes ddb hangman a little easier
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.6 2013/08/07 19:49:07 kettenis Exp $	*/
d52 1
a52 1
#include <sys/workq.h>
@


1.6
log
@Another major overhaul of the inteldrm(4) GEM code, bringing us considerably
closer to the Linux 3.8.13 codebase.  Almost certainly squashes a few more
bugs.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.5 2013/05/18 21:43:42 kettenis Exp $	*/
d224 1
a224 1
bool
d275 2
a276 1
bool
d477 1
a477 1
int
@


1.5
log
@Add parameters describing the usable part of the GTT and enable the checks
that use them.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.4 2013/05/18 16:45:34 kettenis Exp $	*/
d522 1
a522 1
	segp = &obj->dma_segs[0];
d568 1
a568 1
	segp = &obj->dma_segs[0];
@


1.4
log
@Sync with Linux.  Mostly comment and whitespace changes, but adds some missing
locking, and enables some #ifdef'ed out code.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.3 2013/04/21 14:41:26 kettenis Exp $	*/
a390 1
#ifdef notyet
a391 3
#else
			(
#endif
@


1.3
log
@Move GEM initialization code into its own function like Linux has.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.2 2013/03/29 05:15:42 jsg Exp $	*/
d136 1
a136 2
		/*
		 * On Ironlake whatever DRAM config, GPU always do
d150 7
a156 6
		/* On 915-945 and GM965, channel interleave by the CPU is
		 * determined by DCC.  The CPU will alternate based on bit 6
		 * in interleaved mode, and the GPU will then also alternate
		 * on bit 6, 9, and 10 for X, but the CPU may also optionally
		 * alternate based on bit 17 (XOR not disabled and XOR
		 * bit == 17).
d191 1
a191 1
		 * configuration. It will enable dual-channel mode
d196 8
a203 9
		 * Here's what I found on G965:
		 *
		 *    slot fill			memory size	swizzling
		 * 0A   0B	1A	1B	1-ch	2-ch
		 * 512	0	0	0	512	0	O
		 * 512	0	512	0	16	1008	X
		 * 512	0	0	512	16	1008	X
		 * 0	512	0	512	16	1008	X
		 * 1024	1024	1024	0	2048	1024	O
d306 1
a306 2
#if 0
	if (obj->gtt_space->size != size)
a307 1
#endif
d323 1
a323 1
	struct drm_i915_gem_set_tiling	*args = data;
d352 13
d373 1
d377 11
a387 11
		* no longer meets the alignment restrictions for its new
		* tiling mode. Otherwise we can just leave it alone, but
		* need to ensure that any fence register is updated before
		* the next fenced (either through the GTT or by the BLT unit
		* on older GPUs) access.
		*
		* After updating the tiling parameters, we then flag whether
		* we need to update an associated fence register. Note this
		* has to also include the unfenced register the GPU uses
		* whilst executing a fenced command for an untiled object.
		*/
d389 2
a390 2
		obj->map_and_fenceable = 
		    obj->dmamap == NULL ||
d392 1
a392 2
		    (obj->gtt_offset + obj->base.size <=
		    dev_priv->mm.gtt_mappable_end &&
d394 1
a394 1
		    (
d396 1
a396 1
		    i915_gem_object_fence_ok(obj, args->tiling_mode));
d401 3
a403 3
			    i915_gem_get_unfenced_gtt_alignment(dev,
								obj->base.size,
								args->tiling_mode);
d415 1
a415 1
			
d423 1
d427 1
a427 1
	return (ret);
d437 1
a437 1
	struct drm_i915_gem_get_tiling	*args = data;
d443 1
a443 1
		return (ENOENT);
d446 2
d463 7
d475 5
d483 3
a486 2
	int	 i;
	u_int8_t temp[64], *vaddr;
d497 1
a497 1
	vaddr = (u_int8_t *)va;
d518 4
a521 6
	struct drm_device	*dev = obj->base.dev;
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct vm_page		*pg;
	bus_dma_segment_t	*segp;
	int			 page_count = obj->base.size >> PAGE_SHIFT;
	int                      i, n, ret;
d523 1
a523 2
	if (dev_priv->mm.bit_6_swizzle_x != I915_BIT_6_SWIZZLE_9_10_17 ||
	    obj->bit_17 == NULL)
d529 3
a531 5
		/* compare bit 17 with previous one (in case we swapped).
		 * if they don't match we'll have to swizzle the page
		 */
		if ((((segp->ds_addr + n) >> 17) & 0x1) !=
		    test_bit(i, obj->bit_17)) {
d536 2
a537 1
			if (ret)
d539 1
a548 1

d554 3
a556 8
	struct drm_device	*dev = obj->base.dev;
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	bus_dma_segment_t	*segp;
	int			 page_count = obj->base.size >> PAGE_SHIFT;
	int			 i, n;

	if (dev_priv->mm.bit_6_swizzle_x != I915_BIT_6_SWIZZLE_9_10_17)
		return;
d565 3
a567 1
		if (obj-> bit_17 == NULL) {
a569 1

@


1.2
log
@reduce the diff to linux in i915_gem_get/set_tiling
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_tiling.c,v 1.1 2013/03/18 12:36:52 jsg Exp $	*/
d108 1
a108 2
i915_gem_detect_bit_6_swizzle(struct inteldrm_softc *dev_priv,
    struct pci_attach_args *bpa)
d110 3
a112 4
	struct drm_device	*dev = (struct drm_device *)dev_priv->drmdev;
	uint32_t		 swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN;
	uint32_t		 swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN;
	int			 need_disable;
a150 3
		/* try to enable MCHBAR, a lot of biosen disable it */
		need_disable = intel_setup_mchbar(dev_priv, bpa);

a188 2

		intel_teardown_mchbar(dev_priv, bpa, need_disable);
@


1.1
log
@Significantly increase the wordlist for ddb hangman,
and update our device independent DRM code and the Intel DRM code
to be mostly in sync with Linux 3.8.3.  Among other things this
brings support for kernel modesetting and enables use of
the rings on gen6+ Intel hardware.

Based on some earlier work from matthieu@@ with some hints from FreeBSD
and with lots of help from kettenis@@ (including a beautiful accelerated
wscons framebuffer console!)

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d331 1
a331 1
		   struct drm_file *file_priv)
d334 8
a341 10
	struct inteldrm_softc		*dev_priv = dev->dev_private;
	struct drm_obj			*obj;
	struct drm_i915_gem_object	*obj_priv;
	int				 ret = 0;

	obj = drm_gem_object_lookup(dev, file_priv, args->handle);
	if (obj == NULL)
		return (EBADF);
	obj_priv = to_intel_bo(obj);
	drm_hold_object(obj);
d343 3
a345 2
	if (obj_priv->pin_count != 0) {
		ret = EBUSY;
d348 3
a350 3
	if (i915_tiling_ok(dev, args->stride, obj->size,
	    args->tiling_mode) == 0) {
		ret = EINVAL;
d370 14
a383 9
	if (args->tiling_mode != obj_priv->tiling_mode ||
	    args->stride != obj_priv->stride) {
		/*
		 * We need to rebind the object if its current allocation no
		 * longer meets the alignment restrictions for its new tiling
		 * mode. Otherwise we can leave it alone, but must clear any
		 * fence register.
		 */
		/* fence may no longer be correct, wipe it */
d385 2
a386 2
		obj_priv->map_and_fenceable = 
		    obj_priv->dmamap == NULL ||
d388 1
a388 1
		    (obj_priv->gtt_offset + obj->size <=
d393 1
a393 1
		    i915_gem_object_fence_ok(obj_priv, args->tiling_mode));
d396 1
a396 1
		if (!obj_priv->map_and_fenceable) {
d399 1
a399 1
								obj_priv->base.size,
d401 2
a402 2
			if (obj_priv->gtt_offset & (unfenced_alignment - 1))
				ret = i915_gem_object_unbind(obj_priv);
d406 3
a408 3
			obj_priv->fence_dirty =
				obj_priv->fenced_gpu_access ||
				obj_priv->fence_reg != I915_FENCE_REG_NONE;
d410 2
a411 2
			obj_priv->tiling_mode = args->tiling_mode;
			obj_priv->stride = args->stride;
d414 1
a414 1
			i915_gem_release_mmap(obj_priv);
a415 1

d418 2
a419 2
	args->stride = obj_priv->stride;
	args->tiling_mode = obj_priv->tiling_mode;
d421 1
a421 1
	drm_unhold_and_unref(obj);
d431 1
a431 1
		   struct drm_file *file_priv)
d434 7
a440 9
	struct inteldrm_softc		*dev_priv = dev->dev_private;
	struct drm_obj			*obj;
	struct drm_i915_gem_object	*obj_priv;

	obj = drm_gem_object_lookup(dev, file_priv, args->handle);
	if (obj == NULL)
		return (EBADF);
	drm_hold_object(obj);
	obj_priv = to_intel_bo(obj);
d442 2
a443 2
	args->tiling_mode = obj_priv->tiling_mode;
	switch (obj_priv->tiling_mode) {
d457 1
a457 1
	drm_unhold_and_unref(obj);
@

