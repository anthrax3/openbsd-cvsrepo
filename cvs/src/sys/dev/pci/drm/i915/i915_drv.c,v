head	1.107;
access;
symbols
	OPENBSD_6_1:1.101.0.4
	OPENBSD_6_1_BASE:1.101
	OPENBSD_6_0:1.100.0.4
	OPENBSD_6_0_BASE:1.100
	OPENBSD_5_9:1.99.0.2
	OPENBSD_5_9_BASE:1.99
	OPENBSD_5_8:1.85.0.4
	OPENBSD_5_8_BASE:1.85
	OPENBSD_5_7:1.75.0.4
	OPENBSD_5_7_BASE:1.75
	OPENBSD_5_6:1.67.0.4
	OPENBSD_5_6_BASE:1.67
	OPENBSD_5_5:1.63.0.4
	OPENBSD_5_5_BASE:1.63
	OPENBSD_5_4:1.35.0.2
	OPENBSD_5_4_BASE:1.35;
locks; strict;
comment	@ * @;


1.107
date	2017.08.19.20.12.22;	author kettenis;	state Exp;
branches;
next	1.106;
commitid	xVj9887AjWCSNShJ;

1.106
date	2017.07.19.14.34.10;	author kettenis;	state Exp;
branches;
next	1.105;
commitid	es0BDttRZ5dMyViE;

1.105
date	2017.07.12.20.12.19;	author kettenis;	state Exp;
branches;
next	1.104;
commitid	63xYTpLuz3Z38Pwu;

1.104
date	2017.07.05.20.30.13;	author kettenis;	state Exp;
branches;
next	1.103;
commitid	wA527vsQhGJhZItC;

1.103
date	2017.07.01.16.14.10;	author kettenis;	state Exp;
branches;
next	1.102;
commitid	KnwRPOZok9A30HI4;

1.102
date	2017.06.19.11.00.18;	author fcambus;	state Exp;
branches;
next	1.101;
commitid	jFTpf0Jm0Zvb9lOz;

1.101
date	2017.01.08.12.11.54;	author fcambus;	state Exp;
branches;
next	1.100;
commitid	gSiPorOnVEG1sSRV;

1.100
date	2016.04.08.08.27.53;	author kettenis;	state Exp;
branches;
next	1.99;
commitid	mS4ttEBzpAfn3sVx;

1.99
date	2015.12.31.12.36.04;	author kettenis;	state Exp;
branches;
next	1.98;
commitid	5535SD1wd1U93IdW;

1.98
date	2015.11.22.15.35.49;	author kettenis;	state Exp;
branches;
next	1.97;
commitid	sDPS7MogSiSWMILu;

1.97
date	2015.11.01.03.42.56;	author jsg;	state Exp;
branches;
next	1.96;
commitid	RVOUUx1XE6IGWpEo;

1.96
date	2015.10.30.11.21.01;	author kettenis;	state Exp;
branches;
next	1.95;
commitid	VcCJwOOmRBie2Wwe;

1.95
date	2015.10.29.07.47.03;	author kettenis;	state Exp;
branches;
next	1.94;
commitid	QDyL8dFxcUx9R2yh;

1.94
date	2015.10.17.21.41.12;	author kettenis;	state Exp;
branches;
next	1.93;
commitid	bHt1y1EnMlJS0gRQ;

1.93
date	2015.09.28.17.29.56;	author kettenis;	state Exp;
branches;
next	1.92;
commitid	UgiNWTglvSDYWYPZ;

1.92
date	2015.09.28.06.47.23;	author kettenis;	state Exp;
branches;
next	1.91;
commitid	ZUJXu9sGoi3Fv5PQ;

1.91
date	2015.09.26.22.00.00;	author kettenis;	state Exp;
branches;
next	1.90;
commitid	Mvg1eWGKMP5bOyT1;

1.90
date	2015.09.26.19.52.16;	author kettenis;	state Exp;
branches;
next	1.89;
commitid	PpEJMvqmELqAFnQv;

1.89
date	2015.09.25.16.15.19;	author jsg;	state Exp;
branches;
next	1.88;
commitid	qBczsgFZrSbAS824;

1.88
date	2015.09.25.16.05.59;	author kettenis;	state Exp;
branches;
next	1.87;
commitid	VUNN2KLcPDnIRg2Q;

1.87
date	2015.09.24.21.31.22;	author kettenis;	state Exp;
branches;
next	1.86;
commitid	uaGyNb8gI7FWFg7v;

1.86
date	2015.09.23.23.12.11;	author kettenis;	state Exp;
branches;
next	1.85;
commitid	lQlppvmETCN49oZe;

1.85
date	2015.06.26.15.22.23;	author kettenis;	state Exp;
branches;
next	1.84;
commitid	zE715ZsjyYsYQNQ0;

1.84
date	2015.06.24.08.32.39;	author kettenis;	state Exp;
branches;
next	1.83;
commitid	hfEqCdm8ecmxIgUE;

1.83
date	2015.04.18.14.47.34;	author jsg;	state Exp;
branches;
next	1.82;
commitid	c1fUeeFWMNg4COgR;

1.82
date	2015.04.18.11.41.28;	author jsg;	state Exp;
branches;
next	1.81;
commitid	c3CbYQJYD10xhd6O;

1.81
date	2015.04.18.11.21.12;	author jsg;	state Exp;
branches;
next	1.80;
commitid	hAuw2OlcOnGtwHzA;

1.80
date	2015.04.18.11.05.32;	author jsg;	state Exp;
branches;
next	1.79;
commitid	N5rmYm7ybHimJmMa;

1.79
date	2015.04.17.00.54.42;	author jsg;	state Exp;
branches;
next	1.78;
commitid	LqdQe79hlknpVtvI;

1.78
date	2015.04.12.11.26.54;	author jsg;	state Exp;
branches;
next	1.77;
commitid	syZU9J25izIJ2cm1;

1.77
date	2015.04.11.02.24.43;	author jsg;	state Exp;
branches;
next	1.76;
commitid	CNp7DDD9wruE5rh2;

1.76
date	2015.04.03.13.10.59;	author jsg;	state Exp;
branches;
next	1.75;
commitid	PZIJ62HYZVx8fbpI;

1.75
date	2015.02.12.04.56.03;	author kettenis;	state Exp;
branches;
next	1.74;
commitid	adfbJ0ccUTdhFGhI;

1.74
date	2015.02.11.07.01.36;	author jsg;	state Exp;
branches;
next	1.73;
commitid	dLgISW35NAmGN8Xl;

1.73
date	2015.02.10.10.50.49;	author jsg;	state Exp;
branches;
next	1.72;
commitid	aHLMSW1RfE1rmMw9;

1.72
date	2015.02.10.01.39.32;	author jsg;	state Exp;
branches;
next	1.71;
commitid	a8Vt7gSt34kmziIS;

1.71
date	2015.02.09.03.15.41;	author dlg;	state Exp;
branches;
next	1.70;
commitid	jVd0KngVszV2FEfg;

1.70
date	2015.01.27.03.17.36;	author dlg;	state Exp;
branches;
next	1.69;
commitid	MyKPm9Q3dQu92BiX;

1.69
date	2014.12.20.16.34.27;	author krw;	state Exp;
branches;
next	1.68;
commitid	HzEf4vz6A0HRFIhU;

1.68
date	2014.10.25.14.20.09;	author kettenis;	state Exp;
branches;
next	1.67;
commitid	SiWaMEyZSHR4LF76;

1.67
date	2014.07.12.14.18.06;	author jsg;	state Exp;
branches;
next	1.66;
commitid	svfXxzit15NakLv6;

1.66
date	2014.05.12.19.29.16;	author kettenis;	state Exp;
branches;
next	1.65;

1.65
date	2014.03.16.03.34.32;	author jsg;	state Exp;
branches;
next	1.64;

1.64
date	2014.03.13.12.45.04;	author kettenis;	state Exp;
branches;
next	1.63;

1.63
date	2014.02.23.09.36.52;	author kettenis;	state Exp;
branches;
next	1.62;

1.62
date	2014.02.19.01.20.12;	author jsg;	state Exp;
branches;
next	1.61;

1.61
date	2014.02.19.01.17.59;	author jsg;	state Exp;
branches;
next	1.60;

1.60
date	2014.02.19.01.14.41;	author jsg;	state Exp;
branches;
next	1.59;

1.59
date	2014.02.19.01.12.25;	author jsg;	state Exp;
branches;
next	1.58;

1.58
date	2014.01.24.04.05.06;	author jsg;	state Exp;
branches;
next	1.57;

1.57
date	2014.01.24.00.46.45;	author jsg;	state Exp;
branches;
next	1.56;

1.56
date	2014.01.22.04.50.57;	author deraadt;	state Exp;
branches;
next	1.55;

1.55
date	2013.12.06.11.17.20;	author kettenis;	state Exp;
branches;
next	1.54;

1.54
date	2013.12.01.13.53.52;	author kettenis;	state Exp;
branches;
next	1.53;

1.53
date	2013.12.01.11.47.13;	author kettenis;	state Exp;
branches;
next	1.52;

1.52
date	2013.11.30.20.13.36;	author kettenis;	state Exp;
branches;
next	1.51;

1.51
date	2013.11.30.20.03.32;	author kettenis;	state Exp;
branches;
next	1.50;

1.50
date	2013.11.19.19.14.09;	author kettenis;	state Exp;
branches;
next	1.49;

1.49
date	2013.11.19.15.08.03;	author jsg;	state Exp;
branches;
next	1.48;

1.48
date	2013.11.17.20.04.47;	author kettenis;	state Exp;
branches;
next	1.47;

1.47
date	2013.11.17.18.47.13;	author kettenis;	state Exp;
branches;
next	1.46;

1.46
date	2013.11.17.13.41.25;	author kettenis;	state Exp;
branches;
next	1.45;

1.45
date	2013.11.16.16.15.36;	author kettenis;	state Exp;
branches;
next	1.44;

1.44
date	2013.10.30.02.11.32;	author dlg;	state Exp;
branches;
next	1.43;

1.43
date	2013.10.29.06.30.57;	author jsg;	state Exp;
branches;
next	1.42;

1.42
date	2013.10.21.10.36.24;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2013.10.20.20.07.29;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2013.10.20.10.43.47;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2013.09.30.06.47.48;	author jsg;	state Exp;
branches;
next	1.38;

1.38
date	2013.09.08.11.59.44;	author jsg;	state Exp;
branches;
next	1.37;

1.37
date	2013.08.07.19.49.05;	author kettenis;	state Exp;
branches;
next	1.36;

1.36
date	2013.08.07.00.04.28;	author jsg;	state Exp;
branches;
next	1.35;

1.35
date	2013.07.05.07.20.27;	author jsg;	state Exp;
branches;
next	1.34;

1.34
date	2013.07.01.20.16.58;	author kettenis;	state Exp;
branches;
next	1.33;

1.33
date	2013.06.06.16.14.26;	author jsg;	state Exp;
branches;
next	1.32;

1.32
date	2013.06.01.02.03.30;	author kettenis;	state Exp;
branches;
next	1.31;

1.31
date	2013.05.21.22.12.58;	author kettenis;	state Exp;
branches;
next	1.30;

1.30
date	2013.05.17.12.03.42;	author kettenis;	state Exp;
branches;
next	1.29;

1.29
date	2013.05.16.21.14.11;	author kettenis;	state Exp;
branches;
next	1.28;

1.28
date	2013.05.15.10.24.36;	author jsg;	state Exp;
branches;
next	1.27;

1.27
date	2013.05.05.13.55.36;	author kettenis;	state Exp;
branches;
next	1.26;

1.26
date	2013.05.05.13.02.45;	author kettenis;	state Exp;
branches;
next	1.25;

1.25
date	2013.05.05.12.30.41;	author kettenis;	state Exp;
branches;
next	1.24;

1.24
date	2013.05.04.13.18.29;	author kettenis;	state Exp;
branches;
next	1.23;

1.23
date	2013.04.30.19.56.46;	author kettenis;	state Exp;
branches;
next	1.22;

1.22
date	2013.04.21.14.41.26;	author kettenis;	state Exp;
branches;
next	1.21;

1.21
date	2013.04.18.20.19.32;	author kettenis;	state Exp;
branches;
next	1.20;

1.20
date	2013.04.17.21.34.58;	author kettenis;	state Exp;
branches;
next	1.19;

1.19
date	2013.04.17.20.04.04;	author kettenis;	state Exp;
branches;
next	1.18;

1.18
date	2013.04.14.19.04.37;	author kettenis;	state Exp;
branches;
next	1.17;

1.17
date	2013.04.05.02.54.51;	author jsg;	state Exp;
branches;
next	1.16;

1.16
date	2013.04.03.19.57.17;	author kettenis;	state Exp;
branches;
next	1.15;

1.15
date	2013.04.03.07.36.57;	author jsg;	state Exp;
branches;
next	1.14;

1.14
date	2013.03.30.13.14.33;	author kettenis;	state Exp;
branches;
next	1.13;

1.13
date	2013.03.30.04.57.53;	author jsg;	state Exp;
branches;
next	1.12;

1.12
date	2013.03.28.23.19.26;	author jsg;	state Exp;
branches;
next	1.11;

1.11
date	2013.03.28.19.36.14;	author kettenis;	state Exp;
branches;
next	1.10;

1.10
date	2013.03.28.11.51.05;	author jsg;	state Exp;
branches;
next	1.9;

1.9
date	2013.03.28.05.13.07;	author jsg;	state Exp;
branches;
next	1.8;

1.8
date	2013.03.26.21.01.02;	author kettenis;	state Exp;
branches;
next	1.7;

1.7
date	2013.03.25.19.50.56;	author kettenis;	state Exp;
branches;
next	1.6;

1.6
date	2013.03.22.22.51.00;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2013.03.22.06.19.56;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2013.03.20.12.37.41;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2013.03.19.21.07.31;	author kettenis;	state Exp;
branches;
next	1.2;

1.2
date	2013.03.19.19.13.01;	author kettenis;	state Exp;
branches;
next	1.1;

1.1
date	2013.03.18.12.36.51;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.107
log
@Rotate framebuffer (counter-clockwise) when width < height.

ok mpi@@
@
text
@/* i915_drv.c -- i830,i845,i855,i865,i915 driver -*- linux-c -*-
 */
/*
 *
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 */

#ifdef __linux__
#include <linux/device.h>
#include <linux/acpi.h>
#endif
#include <dev/pci/drm/drmP.h>
#include <dev/pci/drm/i915_drm.h>
#include <dev/pci/drm/i915_pciids.h>
#include "i915_drv.h"
#include "i915_trace.h"
#include "intel_drv.h"

#ifdef __linux__
#include <linux/console.h>
#include <linux/module.h>
#include <linux/pm_runtime.h>
#endif
#include <dev/pci/drm/drm_crtc_helper.h>

static struct drm_driver driver;

#define GEN_DEFAULT_PIPEOFFSETS \
	.pipe_offsets = { PIPE_A_OFFSET, PIPE_B_OFFSET, \
			  PIPE_C_OFFSET, PIPE_EDP_OFFSET }, \
	.trans_offsets = { TRANSCODER_A_OFFSET, TRANSCODER_B_OFFSET, \
			   TRANSCODER_C_OFFSET, TRANSCODER_EDP_OFFSET }, \
	.palette_offsets = { PALETTE_A_OFFSET, PALETTE_B_OFFSET }

#define GEN_CHV_PIPEOFFSETS \
	.pipe_offsets = { PIPE_A_OFFSET, PIPE_B_OFFSET, \
			  CHV_PIPE_C_OFFSET }, \
	.trans_offsets = { TRANSCODER_A_OFFSET, TRANSCODER_B_OFFSET, \
			   CHV_TRANSCODER_C_OFFSET, }, \
	.palette_offsets = { PALETTE_A_OFFSET, PALETTE_B_OFFSET, \
			     CHV_PALETTE_C_OFFSET }

#define CURSOR_OFFSETS \
	.cursor_offsets = { CURSOR_A_OFFSET, CURSOR_B_OFFSET, CHV_CURSOR_C_OFFSET }

#define IVB_CURSOR_OFFSETS \
	.cursor_offsets = { CURSOR_A_OFFSET, IVB_CURSOR_B_OFFSET, IVB_CURSOR_C_OFFSET }

static const struct intel_device_info intel_i830_info = {
	.gen = 2, .is_mobile = 1, .cursor_needs_physical = 1, .num_pipes = 2,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_845g_info = {
	.gen = 2, .num_pipes = 1,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_i85x_info = {
	.gen = 2, .is_i85x = 1, .is_mobile = 1, .num_pipes = 2,
	.cursor_needs_physical = 1,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.has_fbc = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_i865g_info = {
	.gen = 2, .num_pipes = 1,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_i915g_info = {
	.gen = 3, .is_i915g = 1, .cursor_needs_physical = 1, .num_pipes = 2,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};
static const struct intel_device_info intel_i915gm_info = {
	.gen = 3, .is_mobile = 1, .num_pipes = 2,
	.cursor_needs_physical = 1,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.supports_tv = 1,
	.has_fbc = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};
static const struct intel_device_info intel_i945g_info = {
	.gen = 3, .has_hotplug = 1, .cursor_needs_physical = 1, .num_pipes = 2,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};
static const struct intel_device_info intel_i945gm_info = {
	.gen = 3, .is_i945gm = 1, .is_mobile = 1, .num_pipes = 2,
	.has_hotplug = 1, .cursor_needs_physical = 1,
	.has_overlay = 1, .overlay_needs_physical = 1,
	.supports_tv = 1,
	.has_fbc = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_i965g_info = {
	.gen = 4, .is_broadwater = 1, .num_pipes = 2,
	.has_hotplug = 1,
	.has_overlay = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_i965gm_info = {
	.gen = 4, .is_crestline = 1, .num_pipes = 2,
	.is_mobile = 1, .has_fbc = 1, .has_hotplug = 1,
	.has_overlay = 1,
	.supports_tv = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_g33_info = {
	.gen = 3, .is_g33 = 1, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_overlay = 1,
	.ring_mask = RENDER_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_g45_info = {
	.gen = 4, .is_g4x = 1, .need_gfx_hws = 1, .num_pipes = 2,
	.has_pipe_cxsr = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_gm45_info = {
	.gen = 4, .is_g4x = 1, .num_pipes = 2,
	.is_mobile = 1, .need_gfx_hws = 1, .has_fbc = 1,
	.has_pipe_cxsr = 1, .has_hotplug = 1,
	.supports_tv = 1,
	.ring_mask = RENDER_RING | BSD_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_pineview_info = {
	.gen = 3, .is_g33 = 1, .is_pineview = 1, .is_mobile = 1, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_overlay = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_ironlake_d_info = {
	.gen = 5, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_ironlake_m_info = {
	.gen = 5, .is_mobile = 1, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_fbc = 1,
	.ring_mask = RENDER_RING | BSD_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_sandybridge_d_info = {
	.gen = 6, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_fbc = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING,
	.has_llc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_sandybridge_m_info = {
	.gen = 6, .is_mobile = 1, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_fbc = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING,
	.has_llc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

#define GEN7_FEATURES  \
	.gen = 7, .num_pipes = 3, \
	.need_gfx_hws = 1, .has_hotplug = 1, \
	.has_fbc = 1, \
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING, \
	.has_llc = 1

static const struct intel_device_info intel_ivybridge_d_info = {
	GEN7_FEATURES,
	.is_ivybridge = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_ivybridge_m_info = {
	GEN7_FEATURES,
	.is_ivybridge = 1,
	.is_mobile = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_ivybridge_q_info = {
	GEN7_FEATURES,
	.is_ivybridge = 1,
	.num_pipes = 0, /* legal, last one wins */
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_valleyview_m_info = {
	GEN7_FEATURES,
	.is_mobile = 1,
	.num_pipes = 2,
	.is_valleyview = 1,
	.display_mmio_offset = VLV_DISPLAY_BASE,
	.has_fbc = 0, /* legal, last one wins */
	.has_llc = 0, /* legal, last one wins */
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_valleyview_d_info = {
	GEN7_FEATURES,
	.num_pipes = 2,
	.is_valleyview = 1,
	.display_mmio_offset = VLV_DISPLAY_BASE,
	.has_fbc = 0, /* legal, last one wins */
	.has_llc = 0, /* legal, last one wins */
	GEN_DEFAULT_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_haswell_d_info = {
	GEN7_FEATURES,
	.is_haswell = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_haswell_m_info = {
	GEN7_FEATURES,
	.is_haswell = 1,
	.is_mobile = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_broadwell_d_info = {
	.gen = 8, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	.has_llc = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_broadwell_m_info = {
	.gen = 8, .is_mobile = 1, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	.has_llc = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_broadwell_gt3d_info = {
	.gen = 8, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING | BSD2_RING,
	.has_llc = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_broadwell_gt3m_info = {
	.gen = 8, .is_mobile = 1, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING | BSD2_RING,
	.has_llc = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_cherryview_info = {
	.gen = 8, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	.is_valleyview = 1,
	.display_mmio_offset = VLV_DISPLAY_BASE,
	GEN_CHV_PIPEOFFSETS,
	CURSOR_OFFSETS,
};

static const struct intel_device_info intel_skylake_info = {
	.is_skylake = 1,
	.gen = 9, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	.has_llc = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_skylake_gt3_info = {
	.is_skylake = 1,
	.gen = 9, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING | BSD2_RING,
	.has_llc = 1,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

static const struct intel_device_info intel_broxton_info = {
	.is_preliminary = 1,
	.gen = 9,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
	.num_pipes = 3,
	.has_ddi = 1,
	.has_fpga_dbg = 1,
	.has_fbc = 1,
	GEN_DEFAULT_PIPEOFFSETS,
	IVB_CURSOR_OFFSETS,
};

/*
 * Make sure any device matches here are from most specific to most
 * general.  For example, since the Quanta match is based on the subsystem
 * and subvendor IDs, we need it to come before the more general IVB
 * PCI ID matches, otherwise we'll use the wrong info struct above.
 */
#define INTEL_PCI_IDS \
	INTEL_I830_IDS(&intel_i830_info),	\
	INTEL_I845G_IDS(&intel_845g_info),	\
	INTEL_I85X_IDS(&intel_i85x_info),	\
	INTEL_I865G_IDS(&intel_i865g_info),	\
	INTEL_I915G_IDS(&intel_i915g_info),	\
	INTEL_I915GM_IDS(&intel_i915gm_info),	\
	INTEL_I945G_IDS(&intel_i945g_info),	\
	INTEL_I945GM_IDS(&intel_i945gm_info),	\
	INTEL_I965G_IDS(&intel_i965g_info),	\
	INTEL_G33_IDS(&intel_g33_info),		\
	INTEL_I965GM_IDS(&intel_i965gm_info),	\
	INTEL_GM45_IDS(&intel_gm45_info), 	\
	INTEL_G45_IDS(&intel_g45_info), 	\
	INTEL_PINEVIEW_IDS(&intel_pineview_info),	\
	INTEL_IRONLAKE_D_IDS(&intel_ironlake_d_info),	\
	INTEL_IRONLAKE_M_IDS(&intel_ironlake_m_info),	\
	INTEL_SNB_D_IDS(&intel_sandybridge_d_info),	\
	INTEL_SNB_M_IDS(&intel_sandybridge_m_info),	\
	INTEL_IVB_Q_IDS(&intel_ivybridge_q_info), /* must be first IVB */ \
	INTEL_IVB_M_IDS(&intel_ivybridge_m_info),	\
	INTEL_IVB_D_IDS(&intel_ivybridge_d_info),	\
	INTEL_HSW_D_IDS(&intel_haswell_d_info), \
	INTEL_HSW_M_IDS(&intel_haswell_m_info), \
	INTEL_VLV_M_IDS(&intel_valleyview_m_info),	\
	INTEL_VLV_D_IDS(&intel_valleyview_d_info),	\
	INTEL_BDW_GT12M_IDS(&intel_broadwell_m_info),	\
	INTEL_BDW_GT12D_IDS(&intel_broadwell_d_info),	\
	INTEL_BDW_GT3M_IDS(&intel_broadwell_gt3m_info),	\
	INTEL_BDW_GT3D_IDS(&intel_broadwell_gt3d_info), \
	INTEL_CHV_IDS(&intel_cherryview_info),	\
	INTEL_SKL_GT1_IDS(&intel_skylake_info),	\
	INTEL_SKL_GT2_IDS(&intel_skylake_info),	\
	INTEL_SKL_GT3_IDS(&intel_skylake_gt3_info),	\
	INTEL_BXT_IDS(&intel_broxton_info)

static const struct drm_pcidev pciidlist[] = {		/* aka */
	INTEL_PCI_IDS,
	{0, 0, 0}
};

MODULE_DEVICE_TABLE(pci, pciidlist);

#ifdef notyet
static enum intel_pch intel_virt_detect_pch(struct drm_device *dev)
{
	enum intel_pch ret = PCH_NOP;

	/*
	 * In a virtualized passthrough environment we can be in a
	 * setup where the ISA bridge is not able to be passed through.
	 * In this case, a south bridge can be emulated and we have to
	 * make an educated guess as to which PCH is really there.
	 */

	if (IS_GEN5(dev)) {
		ret = PCH_IBX;
		DRM_DEBUG_KMS("Assuming Ibex Peak PCH\n");
	} else if (IS_GEN6(dev) || IS_IVYBRIDGE(dev)) {
		ret = PCH_CPT;
		DRM_DEBUG_KMS("Assuming CouarPoint PCH\n");
	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
		ret = PCH_LPT;
		DRM_DEBUG_KMS("Assuming LynxPoint PCH\n");
	} else if (IS_SKYLAKE(dev)) {
		ret = PCH_SPT;
		DRM_DEBUG_KMS("Assuming SunrisePoint PCH\n");
	}

	return ret;
}
#endif

static int
intel_pch_match(struct pci_attach_args *pa)
{
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_INTEL &&
	    PCI_CLASS(pa->pa_class) == PCI_CLASS_BRIDGE &&
	    PCI_SUBCLASS(pa->pa_class) == PCI_SUBCLASS_BRIDGE_ISA)
		return (1);
	return (0);
}

void intel_detect_pch(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct pci_attach_args pa;

	/* In all current cases, num_pipes is equivalent to the PCH_NOP setting
	 * (which really amounts to a PCH but no South Display).
	 */
	if (INTEL_INFO(dev)->num_pipes == 0) {
		dev_priv->pch_type = PCH_NOP;
		return;
	}

	/*
	 * The reason to probe ISA bridge instead of Dev31:Fun0 is to
	 * make graphics device passthrough work easy for VMM, that only
	 * need to expose ISA bridge to let driver know the real hardware
	 * underneath. This is a requirement from virtualization team.
	 *
	 * In some virtualized environments (e.g. XEN), there is irrelevant
	 * ISA bridge in the system. To work reliably, we should scan trhough
	 * all the ISA bridge devices and check for the first match, instead
	 * of only checking the first one.
	 */
	if (pci_find_device(&pa, intel_pch_match)) {
		if (PCI_VENDOR(pa.pa_id) == PCI_VENDOR_ID_INTEL) {
			unsigned short id = PCI_PRODUCT(pa.pa_id) & INTEL_PCH_DEVICE_ID_MASK;
			dev_priv->pch_id = id;

			if (id == INTEL_PCH_IBX_DEVICE_ID_TYPE) {
				dev_priv->pch_type = PCH_IBX;
				DRM_DEBUG_KMS("Found Ibex Peak PCH\n");
				WARN_ON(!IS_GEN5(dev));
			} else if (id == INTEL_PCH_CPT_DEVICE_ID_TYPE) {
				dev_priv->pch_type = PCH_CPT;
				DRM_DEBUG_KMS("Found CougarPoint PCH\n");
				WARN_ON(!(IS_GEN6(dev) || IS_IVYBRIDGE(dev)));
			} else if (id == INTEL_PCH_PPT_DEVICE_ID_TYPE) {
				/* PantherPoint is CPT compatible */
				dev_priv->pch_type = PCH_CPT;
				DRM_DEBUG_KMS("Found PantherPoint PCH\n");
				WARN_ON(!(IS_GEN6(dev) || IS_IVYBRIDGE(dev)));
			} else if (id == INTEL_PCH_LPT_DEVICE_ID_TYPE) {
				dev_priv->pch_type = PCH_LPT;
				DRM_DEBUG_KMS("Found LynxPoint PCH\n");
				WARN_ON(!IS_HASWELL(dev) && !IS_BROADWELL(dev));
				WARN_ON(IS_HSW_ULT(dev) || IS_BDW_ULT(dev));
			} else if (id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE) {
				dev_priv->pch_type = PCH_LPT;
				DRM_DEBUG_KMS("Found LynxPoint LP PCH\n");
				WARN_ON(!IS_HASWELL(dev) && !IS_BROADWELL(dev));
				WARN_ON(!IS_HSW_ULT(dev) && !IS_BDW_ULT(dev));
			} else if (id == INTEL_PCH_SPT_DEVICE_ID_TYPE) {
				dev_priv->pch_type = PCH_SPT;
				DRM_DEBUG_KMS("Found SunrisePoint PCH\n");
				WARN_ON(!IS_SKYLAKE(dev));
			} else if (id == INTEL_PCH_SPT_LP_DEVICE_ID_TYPE) {
				dev_priv->pch_type = PCH_SPT;
				DRM_DEBUG_KMS("Found SunrisePoint LP PCH\n");
				WARN_ON(!IS_SKYLAKE(dev));
#ifdef notyet
			} else if ((id == INTEL_PCH_P2X_DEVICE_ID_TYPE) ||
				   ((id == INTEL_PCH_QEMU_DEVICE_ID_TYPE) &&
				    pch->subsystem_vendor == 0x1af4 &&
				    pch->subsystem_device == 0x1100)) {
				dev_priv->pch_type = intel_virt_detect_pch(dev);
#endif
			}
		}
	} else
		DRM_DEBUG_KMS("No PCH found.\n");
}

bool i915_semaphore_is_enabled(struct drm_device *dev)
{
	if (INTEL_INFO(dev)->gen < 6)
		return false;

	if (i915.semaphores >= 0)
		return i915.semaphores;

	/* TODO: make semaphores and Execlists play nicely together */
	if (i915.enable_execlists)
		return false;

	/* Until we get further testing... */
	if (IS_GEN8(dev))
		return false;

#ifdef CONFIG_INTEL_IOMMU
	/* Enable semaphores on SNB when IO remapping is off */
	if (INTEL_INFO(dev)->gen == 6 && intel_iommu_gfx_mapped)
		return false;
#endif

	return true;
}

void i915_firmware_load_error_print(const char *fw_path, int err)
{
	DRM_ERROR("failed to load firmware %s (%d)\n", fw_path, err);

	/*
	 * If the reason is not known assume -ENOENT since that's the most
	 * usual failure mode.
	 */
	if (!err)
		err = -ENOENT;

	if (!(IS_BUILTIN(CONFIG_DRM_I915) && err == -ENOENT))
		return;

	DRM_ERROR(
	  "The driver is built-in, so to load the firmware you need to\n"
	  "include it either in the kernel (see CONFIG_EXTRA_FIRMWARE) or\n"
	  "in your initrd/initramfs image.\n");
}

static void intel_suspend_encoders(struct drm_i915_private *dev_priv)
{
	struct drm_device *dev = dev_priv->dev;
	struct drm_encoder *encoder;

	drm_modeset_lock_all(dev);
	list_for_each_entry(encoder, &dev->mode_config.encoder_list, head) {
		struct intel_encoder *intel_encoder = to_intel_encoder(encoder);

		if (intel_encoder->suspend)
			intel_encoder->suspend(intel_encoder);
	}
	drm_modeset_unlock_all(dev);
}

static int intel_suspend_complete(struct drm_i915_private *dev_priv);
static int vlv_resume_prepare(struct drm_i915_private *dev_priv,
			      bool rpm_resume);
static int skl_resume_prepare(struct drm_i915_private *dev_priv);
static int bxt_resume_prepare(struct drm_i915_private *dev_priv);


static int i915_drm_suspend(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	pci_power_t opregion_target_state;
	int error;

	/* ignore lid events during suspend */
	mutex_lock(&dev_priv->modeset_restore_lock);
	dev_priv->modeset_restore = MODESET_SUSPENDED;
	mutex_unlock(&dev_priv->modeset_restore_lock);

	/* We do a lot of poking in a lot of registers, make sure they work
	 * properly. */
	intel_display_set_init_power(dev_priv, true);

	drm_kms_helper_poll_disable(dev);

	pci_save_state(dev->pdev);

	error = i915_gem_suspend(dev);
	if (error) {
		dev_err(&dev->pdev->dev,
			"GEM idle failed, resume might fail\n");
		return error;
	}

#ifdef notyet
	intel_guc_suspend(dev);
#endif

	intel_suspend_gt_powersave(dev);

	/*
	 * Disable CRTCs directly since we want to preserve sw state
	 * for _thaw. Also, power gate the CRTC power wells.
	 */
	drm_modeset_lock_all(dev);
	intel_display_suspend(dev);
	drm_modeset_unlock_all(dev);

	intel_dp_mst_suspend(dev);

	intel_runtime_pm_disable_interrupts(dev_priv);
	intel_hpd_cancel_work(dev_priv);

	intel_suspend_encoders(dev_priv);

	intel_suspend_hw(dev);

	i915_gem_suspend_gtt_mappings(dev);

	i915_save_state(dev);

	opregion_target_state = PCI_D3cold;
#if IS_ENABLED(CONFIG_ACPI_SLEEP)
	if (acpi_target_system_state() < ACPI_STATE_S3)
		opregion_target_state = PCI_D1;
#endif
	intel_opregion_notify_adapter(dev, opregion_target_state);

	intel_uncore_forcewake_reset(dev, false);
	intel_opregion_fini(dev);

#ifdef __linux__
	intel_fbdev_set_suspend(dev, FBINFO_STATE_SUSPENDED, true);
#endif

	dev_priv->suspend_count++;

	intel_display_set_init_power(dev_priv, false);

	return 0;
}

static int i915_drm_suspend_late(struct drm_device *drm_dev, bool hibernation)
{
	struct drm_i915_private *dev_priv = drm_dev->dev_private;
	int ret;

	ret = intel_suspend_complete(dev_priv);

	if (ret) {
		DRM_ERROR("Suspend complete failed: %d\n", ret);

		return ret;
	}

	pci_disable_device(drm_dev->pdev);
#ifdef notyet
	/*
	 * During hibernation on some platforms the BIOS may try to access
	 * the device even though it's already in D3 and hang the machine. So
	 * leave the device in D0 on those platforms and hope the BIOS will
	 * power down the device properly. The issue was seen on multiple old
	 * GENs with different BIOS vendors, so having an explicit blacklist
	 * is inpractical; apply the workaround on everything pre GEN6. The
	 * platforms where the issue was seen:
	 * Lenovo Thinkpad X301, X61s, X60, T60, X41
	 * Fujitsu FSC S7110
	 * Acer Aspire 1830T
	 */
	if (!(hibernation && INTEL_INFO(dev_priv)->gen < 6))
		pci_set_power_state(drm_dev->pdev, PCI_D3hot);
#endif

	return 0;
}

#ifdef __linux__
int i915_suspend_switcheroo(struct drm_device *dev, pm_message_t state)
{
	int error;

	if (!dev || !dev->dev_private) {
		DRM_ERROR("dev: %p\n", dev);
		DRM_ERROR("DRM not initialized, aborting suspend.\n");
		return -ENODEV;
	}

	if (WARN_ON_ONCE(state.event != PM_EVENT_SUSPEND &&
			 state.event != PM_EVENT_FREEZE))
		return -EINVAL;

	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	error = i915_drm_suspend(dev);
	if (error)
		return error;

	return i915_drm_suspend_late(dev, false);
}
#endif

static int i915_drm_resume(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;

	mutex_lock(&dev->struct_mutex);
	i915_gem_restore_gtt_mappings(dev);
	mutex_unlock(&dev->struct_mutex);

	i915_restore_state(dev);
	intel_opregion_setup(dev);

	intel_init_pch_refclk(dev);
	drm_mode_config_reset(dev);

	/*
	 * Interrupts have to be enabled before any batches are run. If not the
	 * GPU will hang. i915_gem_init_hw() will initiate batches to
	 * update/restore the context.
	 *
	 * Modeset enabling in intel_modeset_init_hw() also needs working
	 * interrupts.
	 */
	intel_runtime_pm_enable_interrupts(dev_priv);

	mutex_lock(&dev->struct_mutex);
	if (i915_gem_init_hw(dev)) {
		DRM_ERROR("failed to re-initialize GPU, declaring wedged!\n");
			atomic_or(I915_WEDGED, &dev_priv->gpu_error.reset_counter);
	}
	mutex_unlock(&dev->struct_mutex);

#ifdef notyet
	intel_guc_resume(dev);
#endif

	intel_modeset_init_hw(dev);

	spin_lock_irq(&dev_priv->irq_lock);
	if (dev_priv->display.hpd_irq_setup)
		dev_priv->display.hpd_irq_setup(dev);
	spin_unlock_irq(&dev_priv->irq_lock);

	drm_modeset_lock_all(dev);
	intel_display_resume(dev);
	drm_modeset_unlock_all(dev);

	intel_dp_mst_resume(dev);

	/*
	 * ... but also need to make sure that hotplug processing
	 * doesn't cause havoc. Like in the driver load code we don't
	 * bother with the tiny race here where we might loose hotplug
	 * notifications.
	 * */
	intel_hpd_init(dev_priv);
	/* Config may have changed between suspend and resume */
	drm_helper_hpd_irq_event(dev);

	intel_opregion_init(dev);

#ifdef __linux__
	intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING, false);
#endif

	mutex_lock(&dev_priv->modeset_restore_lock);
	dev_priv->modeset_restore = MODESET_DONE;
	mutex_unlock(&dev_priv->modeset_restore_lock);

	intel_opregion_notify_adapter(dev, PCI_D0);

	drm_kms_helper_poll_enable(dev);

	return 0;
}

static int i915_drm_resume_early(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret = 0;

	/*
	 * We have a resume ordering issue with the snd-hda driver also
	 * requiring our device to be power up. Due to the lack of a
	 * parent/child relationship we currently solve this with an early
	 * resume hook.
	 *
	 * FIXME: This should be solved with a special hdmi sink device or
	 * similar so that power domains can be employed.
	 */
	if (pci_enable_device(dev->pdev))
		return -EIO;

	pci_set_master(dev->pdev);

	if (IS_VALLEYVIEW(dev_priv))
		ret = vlv_resume_prepare(dev_priv, false);
	if (ret)
		DRM_ERROR("Resume prepare failed: %d, continuing anyway\n",
			  ret);

	intel_uncore_early_sanitize(dev, true);

	if (IS_BROXTON(dev))
		ret = bxt_resume_prepare(dev_priv);
	else if (IS_SKYLAKE(dev_priv))
		ret = skl_resume_prepare(dev_priv);
	else if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
		hsw_disable_pc8(dev_priv);

	intel_uncore_sanitize(dev);
	intel_power_domains_init_hw(dev_priv);

	return ret;
}

#ifdef __linux__
int i915_resume_switcheroo(struct drm_device *dev)
{
	int ret;

	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	ret = i915_drm_resume_early(dev);
	if (ret)
		return ret;

	return i915_drm_resume(dev);
}
#endif

/**
 * i915_reset - reset chip after a hang
 * @@dev: drm device to reset
 *
 * Reset the chip.  Useful if a hang is detected. Returns zero on successful
 * reset or otherwise an error code.
 *
 * Procedure is fairly simple:
 *   - reset the chip using the reset reg
 *   - re-init context state
 *   - re-init hardware status page
 *   - re-init ring buffer
 *   - re-init interrupt state
 *   - re-init display
 */
int i915_reset(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	bool simulated;
	int ret;

	intel_reset_gt_powersave(dev);

	mutex_lock(&dev->struct_mutex);

	i915_gem_reset(dev);

	simulated = dev_priv->gpu_error.stop_rings != 0;

	ret = intel_gpu_reset(dev);

	/* Also reset the gpu hangman. */
	if (simulated) {
		DRM_INFO("Simulated gpu hang, resetting stop_rings\n");
		dev_priv->gpu_error.stop_rings = 0;
		if (ret == -ENODEV) {
			DRM_INFO("Reset not implemented, but ignoring "
				 "error for simulated gpu hangs\n");
			ret = 0;
		}
	}

	if (i915_stop_ring_allow_warn(dev_priv))
		pr_notice("drm/i915: Resetting chip after gpu hang\n");

	if (ret) {
		DRM_ERROR("Failed to reset chip: %i\n", ret);
		mutex_unlock(&dev->struct_mutex);
		return ret;
	}

	intel_overlay_reset(dev_priv);

	/* Ok, now get things going again... */

	/*
	 * Everything depends on having the GTT running, so we need to start
	 * there.  Fortunately we don't need to do this unless we reset the
	 * chip at a PCI level.
	 *
	 * Next we need to restore the context, but we don't use those
	 * yet either...
	 *
	 * Ring buffer needs to be re-initialized in the KMS case, or if X
	 * was running at the time of the reset (i.e. we weren't VT
	 * switched away).
	 */

	/* Used to prevent gem_check_wedged returning -EAGAIN during gpu reset */
	dev_priv->gpu_error.reload_in_reset = true;

	ret = i915_gem_init_hw(dev);

	dev_priv->gpu_error.reload_in_reset = false;

	mutex_unlock(&dev->struct_mutex);
	if (ret) {
		DRM_ERROR("Failed hw init on reset %d\n", ret);
		return ret;
	}

	/*
	 * rps/rc6 re-init is necessary to restore state lost after the
	 * reset and the re-install of gt irqs. Skip for ironlake per
	 * previous concerns that it doesn't respond well to some forms
	 * of re-init after reset.
	 */
	if (INTEL_INFO(dev)->gen > 5)
		intel_enable_gt_powersave(dev);

	return 0;
}

#ifdef __linux__
static int i915_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
	struct intel_device_info *intel_info =
		(struct intel_device_info *) ent->driver_data;

	if (IS_PRELIMINARY_HW(intel_info) && !i915.preliminary_hw_support) {
		DRM_INFO("This hardware requires preliminary hardware support.\n"
			 "See CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT, and/or modparam preliminary_hw_support\n");
		return -ENODEV;
	}

	/* Only bind to function 0 of the device. Early generations
	 * used function 1 as a placeholder for multi-head. This causes
	 * us confusion instead, especially on the systems where both
	 * functions have the same PCI-ID!
	 */
	if (PCI_FUNC(pdev->devfn))
		return -ENODEV;

	return drm_get_pci_dev(pdev, ent, &driver);
}

static void
i915_pci_remove(struct pci_dev *pdev)
{
	struct drm_device *dev = pci_get_drvdata(pdev);

	drm_put_dev(dev);
}

static int i915_pm_suspend(struct device *dev)
{
	struct pci_dev *pdev = to_pci_dev(dev);
	struct drm_device *drm_dev = pci_get_drvdata(pdev);

	if (!drm_dev || !drm_dev->dev_private) {
		dev_err(dev, "DRM not initialized, aborting suspend.\n");
		return -ENODEV;
	}

	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	return i915_drm_suspend(drm_dev);
}

static int i915_pm_suspend_late(struct device *dev)
{
	struct drm_device *drm_dev = dev_to_i915(dev)->dev;

	/*
	 * We have a suspend ordering issue with the snd-hda driver also
	 * requiring our device to be power up. Due to the lack of a
	 * parent/child relationship we currently solve this with an late
	 * suspend hook.
	 *
	 * FIXME: This should be solved with a special hdmi sink device or
	 * similar so that power domains can be employed.
	 */
	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	return i915_drm_suspend_late(drm_dev, false);
}

static int i915_pm_poweroff_late(struct device *dev)
{
	struct drm_device *drm_dev = dev_to_i915(dev)->dev;

	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	return i915_drm_suspend_late(drm_dev, true);
}

static int i915_pm_resume_early(struct device *dev)
{
	struct drm_device *drm_dev = dev_to_i915(dev)->dev;

	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	return i915_drm_resume_early(drm_dev);
}

static int i915_pm_resume(struct device *dev)
{
	struct drm_device *drm_dev = dev_to_i915(dev)->dev;

	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;

	return i915_drm_resume(drm_dev);
}
#endif

static int skl_suspend_complete(struct drm_i915_private *dev_priv)
{
	/* Enabling DC6 is not a hard requirement to enter runtime D3 */

	skl_uninit_cdclk(dev_priv);

	return 0;
}

static int hsw_suspend_complete(struct drm_i915_private *dev_priv)
{
	hsw_enable_pc8(dev_priv);

	return 0;
}

static int bxt_suspend_complete(struct drm_i915_private *dev_priv)
{
	struct drm_device *dev = dev_priv->dev;

	/* TODO: when DC5 support is added disable DC5 here. */

	broxton_ddi_phy_uninit(dev);
	broxton_uninit_cdclk(dev);
	bxt_enable_dc9(dev_priv);

	return 0;
}

static int bxt_resume_prepare(struct drm_i915_private *dev_priv)
{
	struct drm_device *dev = dev_priv->dev;

	/* TODO: when CSR FW support is added make sure the FW is loaded */

	bxt_disable_dc9(dev_priv);

	/*
	 * TODO: when DC5 support is added enable DC5 here if the CSR FW
	 * is available.
	 */
	broxton_init_cdclk(dev);
	broxton_ddi_phy_init(dev);
	intel_prepare_ddi(dev);

	return 0;
}

static int skl_resume_prepare(struct drm_i915_private *dev_priv)
{
	struct drm_device *dev = dev_priv->dev;

	skl_init_cdclk(dev_priv);
	intel_csr_load_program(dev);

	return 0;
}

/*
 * Save all Gunit registers that may be lost after a D3 and a subsequent
 * S0i[R123] transition. The list of registers needing a save/restore is
 * defined in the VLV2_S0IXRegs document. This documents marks all Gunit
 * registers in the following way:
 * - Driver: saved/restored by the driver
 * - Punit : saved/restored by the Punit firmware
 * - No, w/o marking: no need to save/restore, since the register is R/O or
 *                    used internally by the HW in a way that doesn't depend
 *                    keeping the content across a suspend/resume.
 * - Debug : used for debugging
 *
 * We save/restore all registers marked with 'Driver', with the following
 * exceptions:
 * - Registers out of use, including also registers marked with 'Debug'.
 *   These have no effect on the driver's operation, so we don't save/restore
 *   them to reduce the overhead.
 * - Registers that are fully setup by an initialization function called from
 *   the resume path. For example many clock gating and RPS/RC6 registers.
 * - Registers that provide the right functionality with their reset defaults.
 *
 * TODO: Except for registers that based on the above 3 criteria can be safely
 * ignored, we save/restore all others, practically treating the HW context as
 * a black-box for the driver. Further investigation is needed to reduce the
 * saved/restored registers even further, by following the same 3 criteria.
 */
static void vlv_save_gunit_s0ix_state(struct drm_i915_private *dev_priv)
{
	struct vlv_s0ix_state *s = &dev_priv->vlv_s0ix_state;
	int i;

	/* GAM 0x4000-0x4770 */
	s->wr_watermark		= I915_READ(GEN7_WR_WATERMARK);
	s->gfx_prio_ctrl	= I915_READ(GEN7_GFX_PRIO_CTRL);
	s->arb_mode		= I915_READ(ARB_MODE);
	s->gfx_pend_tlb0	= I915_READ(GEN7_GFX_PEND_TLB0);
	s->gfx_pend_tlb1	= I915_READ(GEN7_GFX_PEND_TLB1);

	for (i = 0; i < ARRAY_SIZE(s->lra_limits); i++)
		s->lra_limits[i] = I915_READ(GEN7_LRA_LIMITS(i));

	s->media_max_req_count	= I915_READ(GEN7_MEDIA_MAX_REQ_COUNT);
	s->gfx_max_req_count	= I915_READ(GEN7_GFX_MAX_REQ_COUNT);

	s->render_hwsp		= I915_READ(RENDER_HWS_PGA_GEN7);
	s->ecochk		= I915_READ(GAM_ECOCHK);
	s->bsd_hwsp		= I915_READ(BSD_HWS_PGA_GEN7);
	s->blt_hwsp		= I915_READ(BLT_HWS_PGA_GEN7);

	s->tlb_rd_addr		= I915_READ(GEN7_TLB_RD_ADDR);

	/* MBC 0x9024-0x91D0, 0x8500 */
	s->g3dctl		= I915_READ(VLV_G3DCTL);
	s->gsckgctl		= I915_READ(VLV_GSCKGCTL);
	s->mbctl		= I915_READ(GEN6_MBCTL);

	/* GCP 0x9400-0x9424, 0x8100-0x810C */
	s->ucgctl1		= I915_READ(GEN6_UCGCTL1);
	s->ucgctl3		= I915_READ(GEN6_UCGCTL3);
	s->rcgctl1		= I915_READ(GEN6_RCGCTL1);
	s->rcgctl2		= I915_READ(GEN6_RCGCTL2);
	s->rstctl		= I915_READ(GEN6_RSTCTL);
	s->misccpctl		= I915_READ(GEN7_MISCCPCTL);

	/* GPM 0xA000-0xAA84, 0x8000-0x80FC */
	s->gfxpause		= I915_READ(GEN6_GFXPAUSE);
	s->rpdeuhwtc		= I915_READ(GEN6_RPDEUHWTC);
	s->rpdeuc		= I915_READ(GEN6_RPDEUC);
	s->ecobus		= I915_READ(ECOBUS);
	s->pwrdwnupctl		= I915_READ(VLV_PWRDWNUPCTL);
	s->rp_down_timeout	= I915_READ(GEN6_RP_DOWN_TIMEOUT);
	s->rp_deucsw		= I915_READ(GEN6_RPDEUCSW);
	s->rcubmabdtmr		= I915_READ(GEN6_RCUBMABDTMR);
	s->rcedata		= I915_READ(VLV_RCEDATA);
	s->spare2gh		= I915_READ(VLV_SPAREG2H);

	/* Display CZ domain, 0x4400C-0x4402C, 0x4F000-0x4F11F */
	s->gt_imr		= I915_READ(GTIMR);
	s->gt_ier		= I915_READ(GTIER);
	s->pm_imr		= I915_READ(GEN6_PMIMR);
	s->pm_ier		= I915_READ(GEN6_PMIER);

	for (i = 0; i < ARRAY_SIZE(s->gt_scratch); i++)
		s->gt_scratch[i] = I915_READ(GEN7_GT_SCRATCH(i));

	/* GT SA CZ domain, 0x100000-0x138124 */
	s->tilectl		= I915_READ(TILECTL);
	s->gt_fifoctl		= I915_READ(GTFIFOCTL);
	s->gtlc_wake_ctrl	= I915_READ(VLV_GTLC_WAKE_CTRL);
	s->gtlc_survive		= I915_READ(VLV_GTLC_SURVIVABILITY_REG);
	s->pmwgicz		= I915_READ(VLV_PMWGICZ);

	/* Gunit-Display CZ domain, 0x182028-0x1821CF */
	s->gu_ctl0		= I915_READ(VLV_GU_CTL0);
	s->gu_ctl1		= I915_READ(VLV_GU_CTL1);
	s->pcbr			= I915_READ(VLV_PCBR);
	s->clock_gate_dis2	= I915_READ(VLV_GUNIT_CLOCK_GATE2);

	/*
	 * Not saving any of:
	 * DFT,		0x9800-0x9EC0
	 * SARB,	0xB000-0xB1FC
	 * GAC,		0x5208-0x524C, 0x14000-0x14C000
	 * PCI CFG
	 */
}

static void vlv_restore_gunit_s0ix_state(struct drm_i915_private *dev_priv)
{
	struct vlv_s0ix_state *s = &dev_priv->vlv_s0ix_state;
	u32 val;
	int i;

	/* GAM 0x4000-0x4770 */
	I915_WRITE(GEN7_WR_WATERMARK,	s->wr_watermark);
	I915_WRITE(GEN7_GFX_PRIO_CTRL,	s->gfx_prio_ctrl);
	I915_WRITE(ARB_MODE,		s->arb_mode | (0xffff << 16));
	I915_WRITE(GEN7_GFX_PEND_TLB0,	s->gfx_pend_tlb0);
	I915_WRITE(GEN7_GFX_PEND_TLB1,	s->gfx_pend_tlb1);

	for (i = 0; i < ARRAY_SIZE(s->lra_limits); i++)
		I915_WRITE(GEN7_LRA_LIMITS(i), s->lra_limits[i]);

	I915_WRITE(GEN7_MEDIA_MAX_REQ_COUNT, s->media_max_req_count);
	I915_WRITE(GEN7_GFX_MAX_REQ_COUNT, s->gfx_max_req_count);

	I915_WRITE(RENDER_HWS_PGA_GEN7,	s->render_hwsp);
	I915_WRITE(GAM_ECOCHK,		s->ecochk);
	I915_WRITE(BSD_HWS_PGA_GEN7,	s->bsd_hwsp);
	I915_WRITE(BLT_HWS_PGA_GEN7,	s->blt_hwsp);

	I915_WRITE(GEN7_TLB_RD_ADDR,	s->tlb_rd_addr);

	/* MBC 0x9024-0x91D0, 0x8500 */
	I915_WRITE(VLV_G3DCTL,		s->g3dctl);
	I915_WRITE(VLV_GSCKGCTL,	s->gsckgctl);
	I915_WRITE(GEN6_MBCTL,		s->mbctl);

	/* GCP 0x9400-0x9424, 0x8100-0x810C */
	I915_WRITE(GEN6_UCGCTL1,	s->ucgctl1);
	I915_WRITE(GEN6_UCGCTL3,	s->ucgctl3);
	I915_WRITE(GEN6_RCGCTL1,	s->rcgctl1);
	I915_WRITE(GEN6_RCGCTL2,	s->rcgctl2);
	I915_WRITE(GEN6_RSTCTL,		s->rstctl);
	I915_WRITE(GEN7_MISCCPCTL,	s->misccpctl);

	/* GPM 0xA000-0xAA84, 0x8000-0x80FC */
	I915_WRITE(GEN6_GFXPAUSE,	s->gfxpause);
	I915_WRITE(GEN6_RPDEUHWTC,	s->rpdeuhwtc);
	I915_WRITE(GEN6_RPDEUC,		s->rpdeuc);
	I915_WRITE(ECOBUS,		s->ecobus);
	I915_WRITE(VLV_PWRDWNUPCTL,	s->pwrdwnupctl);
	I915_WRITE(GEN6_RP_DOWN_TIMEOUT,s->rp_down_timeout);
	I915_WRITE(GEN6_RPDEUCSW,	s->rp_deucsw);
	I915_WRITE(GEN6_RCUBMABDTMR,	s->rcubmabdtmr);
	I915_WRITE(VLV_RCEDATA,		s->rcedata);
	I915_WRITE(VLV_SPAREG2H,	s->spare2gh);

	/* Display CZ domain, 0x4400C-0x4402C, 0x4F000-0x4F11F */
	I915_WRITE(GTIMR,		s->gt_imr);
	I915_WRITE(GTIER,		s->gt_ier);
	I915_WRITE(GEN6_PMIMR,		s->pm_imr);
	I915_WRITE(GEN6_PMIER,		s->pm_ier);

	for (i = 0; i < ARRAY_SIZE(s->gt_scratch); i++)
		I915_WRITE(GEN7_GT_SCRATCH(i), s->gt_scratch[i]);

	/* GT SA CZ domain, 0x100000-0x138124 */
	I915_WRITE(TILECTL,			s->tilectl);
	I915_WRITE(GTFIFOCTL,			s->gt_fifoctl);
	/*
	 * Preserve the GT allow wake and GFX force clock bit, they are not
	 * be restored, as they are used to control the s0ix suspend/resume
	 * sequence by the caller.
	 */
	val = I915_READ(VLV_GTLC_WAKE_CTRL);
	val &= VLV_GTLC_ALLOWWAKEREQ;
	val |= s->gtlc_wake_ctrl & ~VLV_GTLC_ALLOWWAKEREQ;
	I915_WRITE(VLV_GTLC_WAKE_CTRL, val);

	val = I915_READ(VLV_GTLC_SURVIVABILITY_REG);
	val &= VLV_GFX_CLK_FORCE_ON_BIT;
	val |= s->gtlc_survive & ~VLV_GFX_CLK_FORCE_ON_BIT;
	I915_WRITE(VLV_GTLC_SURVIVABILITY_REG, val);

	I915_WRITE(VLV_PMWGICZ,			s->pmwgicz);

	/* Gunit-Display CZ domain, 0x182028-0x1821CF */
	I915_WRITE(VLV_GU_CTL0,			s->gu_ctl0);
	I915_WRITE(VLV_GU_CTL1,			s->gu_ctl1);
	I915_WRITE(VLV_PCBR,			s->pcbr);
	I915_WRITE(VLV_GUNIT_CLOCK_GATE2,	s->clock_gate_dis2);
}

int vlv_force_gfx_clock(struct drm_i915_private *dev_priv, bool force_on)
{
	u32 val;
	int err;

#define COND (I915_READ(VLV_GTLC_SURVIVABILITY_REG) & VLV_GFX_CLK_STATUS_BIT)

	val = I915_READ(VLV_GTLC_SURVIVABILITY_REG);
	val &= ~VLV_GFX_CLK_FORCE_ON_BIT;
	if (force_on)
		val |= VLV_GFX_CLK_FORCE_ON_BIT;
	I915_WRITE(VLV_GTLC_SURVIVABILITY_REG, val);

	if (!force_on)
		return 0;

	err = wait_for(COND, 20);
	if (err)
		DRM_ERROR("timeout waiting for GFX clock force-on (%08x)\n",
			  I915_READ(VLV_GTLC_SURVIVABILITY_REG));

	return err;
#undef COND
}

static int vlv_allow_gt_wake(struct drm_i915_private *dev_priv, bool allow)
{
	u32 val;
	int err = 0;

	val = I915_READ(VLV_GTLC_WAKE_CTRL);
	val &= ~VLV_GTLC_ALLOWWAKEREQ;
	if (allow)
		val |= VLV_GTLC_ALLOWWAKEREQ;
	I915_WRITE(VLV_GTLC_WAKE_CTRL, val);
	POSTING_READ(VLV_GTLC_WAKE_CTRL);

#define COND (!!(I915_READ(VLV_GTLC_PW_STATUS) & VLV_GTLC_ALLOWWAKEACK) == \
	      allow)
	err = wait_for(COND, 1);
	if (err)
		DRM_ERROR("timeout disabling GT waking\n");
	return err;
#undef COND
}

static int vlv_wait_for_gt_wells(struct drm_i915_private *dev_priv,
				 bool wait_for_on)
{
	u32 mask;
	u32 val;
	int err;

	mask = VLV_GTLC_PW_MEDIA_STATUS_MASK | VLV_GTLC_PW_RENDER_STATUS_MASK;
	val = wait_for_on ? mask : 0;
#define COND ((I915_READ(VLV_GTLC_PW_STATUS) & mask) == val)
	if (COND)
		return 0;

	DRM_DEBUG_KMS("waiting for GT wells to go %s (%08x)\n",
			wait_for_on ? "on" : "off",
			I915_READ(VLV_GTLC_PW_STATUS));

	/*
	 * RC6 transitioning can be delayed up to 2 msec (see
	 * valleyview_enable_rps), use 3 msec for safety.
	 */
	err = wait_for(COND, 3);
	if (err)
		DRM_ERROR("timeout waiting for GT wells to go %s\n",
			  wait_for_on ? "on" : "off");

	return err;
#undef COND
}

static void vlv_check_no_gt_access(struct drm_i915_private *dev_priv)
{
	if (!(I915_READ(VLV_GTLC_PW_STATUS) & VLV_GTLC_ALLOWWAKEERR))
		return;

	DRM_ERROR("GT register access while GT waking disabled\n");
	I915_WRITE(VLV_GTLC_PW_STATUS, VLV_GTLC_ALLOWWAKEERR);
}

static int vlv_suspend_complete(struct drm_i915_private *dev_priv)
{
	u32 mask;
	int err;

	/*
	 * Bspec defines the following GT well on flags as debug only, so
	 * don't treat them as hard failures.
	 */
	(void)vlv_wait_for_gt_wells(dev_priv, false);

	mask = VLV_GTLC_RENDER_CTX_EXISTS | VLV_GTLC_MEDIA_CTX_EXISTS;
	WARN_ON((I915_READ(VLV_GTLC_WAKE_CTRL) & mask) != mask);

	vlv_check_no_gt_access(dev_priv);

	err = vlv_force_gfx_clock(dev_priv, true);
	if (err)
		goto err1;

	err = vlv_allow_gt_wake(dev_priv, false);
	if (err)
		goto err2;

	if (!IS_CHERRYVIEW(dev_priv->dev))
		vlv_save_gunit_s0ix_state(dev_priv);

	err = vlv_force_gfx_clock(dev_priv, false);
	if (err)
		goto err2;

	return 0;

err2:
	/* For safety always re-enable waking and disable gfx clock forcing */
	vlv_allow_gt_wake(dev_priv, true);
err1:
	vlv_force_gfx_clock(dev_priv, false);

	return err;
}

static int vlv_resume_prepare(struct drm_i915_private *dev_priv,
				bool rpm_resume)
{
	struct drm_device *dev = dev_priv->dev;
	int err;
	int ret;

	/*
	 * If any of the steps fail just try to continue, that's the best we
	 * can do at this point. Return the first error code (which will also
	 * leave RPM permanently disabled).
	 */
	ret = vlv_force_gfx_clock(dev_priv, true);

	if (!IS_CHERRYVIEW(dev_priv->dev))
		vlv_restore_gunit_s0ix_state(dev_priv);

	err = vlv_allow_gt_wake(dev_priv, true);
	if (!ret)
		ret = err;

	err = vlv_force_gfx_clock(dev_priv, false);
	if (!ret)
		ret = err;

	vlv_check_no_gt_access(dev_priv);

	if (rpm_resume) {
		intel_init_clock_gating(dev);
		i915_gem_restore_fences(dev);
	}

	return ret;
}

#ifdef __linux__
static int intel_runtime_suspend(struct device *device)
{
	struct pci_dev *pdev = to_pci_dev(device);
	struct drm_device *dev = pci_get_drvdata(pdev);
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret;

	if (WARN_ON_ONCE(!(dev_priv->rps.enabled && intel_enable_rc6(dev))))
		return -ENODEV;

	if (WARN_ON_ONCE(!HAS_RUNTIME_PM(dev)))
		return -ENODEV;

	DRM_DEBUG_KMS("Suspending device\n");

	/*
	 * We could deadlock here in case another thread holding struct_mutex
	 * calls RPM suspend concurrently, since the RPM suspend will wait
	 * first for this RPM suspend to finish. In this case the concurrent
	 * RPM resume will be followed by its RPM suspend counterpart. Still
	 * for consistency return -EAGAIN, which will reschedule this suspend.
	 */
	if (!mutex_trylock(&dev->struct_mutex)) {
		DRM_DEBUG_KMS("device lock contention, deffering suspend\n");
		/*
		 * Bump the expiration timestamp, otherwise the suspend won't
		 * be rescheduled.
		 */
		pm_runtime_mark_last_busy(device);

		return -EAGAIN;
	}
	/*
	 * We are safe here against re-faults, since the fault handler takes
	 * an RPM reference.
	 */
	i915_gem_release_all_mmaps(dev_priv);
	mutex_unlock(&dev->struct_mutex);

	intel_guc_suspend(dev);

	intel_suspend_gt_powersave(dev);
	intel_runtime_pm_disable_interrupts(dev_priv);

	ret = intel_suspend_complete(dev_priv);
	if (ret) {
		DRM_ERROR("Runtime suspend failed, disabling it (%d)\n", ret);
		intel_runtime_pm_enable_interrupts(dev_priv);

		return ret;
	}

	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
	intel_uncore_forcewake_reset(dev, false);
	dev_priv->pm.suspended = true;

	/*
	 * FIXME: We really should find a document that references the arguments
	 * used below!
	 */
	if (IS_BROADWELL(dev)) {
		/*
		 * On Broadwell, if we use PCI_D1 the PCH DDI ports will stop
		 * being detected, and the call we do at intel_runtime_resume()
		 * won't be able to restore them. Since PCI_D3hot matches the
		 * actual specification and appears to be working, use it.
		 */
		intel_opregion_notify_adapter(dev, PCI_D3hot);
	} else {
		/*
		 * current versions of firmware which depend on this opregion
		 * notification have repurposed the D1 definition to mean
		 * "runtime suspended" vs. what you would normally expect (D3)
		 * to distinguish it from notifications that might be sent via
		 * the suspend path.
		 */
		intel_opregion_notify_adapter(dev, PCI_D1);
	}

	assert_forcewakes_inactive(dev_priv);

	DRM_DEBUG_KMS("Device suspended\n");
	return 0;
}

static int intel_runtime_resume(struct device *device)
{
	struct pci_dev *pdev = to_pci_dev(device);
	struct drm_device *dev = pci_get_drvdata(pdev);
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret = 0;

	if (WARN_ON_ONCE(!HAS_RUNTIME_PM(dev)))
		return -ENODEV;

	DRM_DEBUG_KMS("Resuming device\n");

	intel_opregion_notify_adapter(dev, PCI_D0);
	dev_priv->pm.suspended = false;

	intel_guc_resume(dev);

	if (IS_GEN6(dev_priv))
		intel_init_pch_refclk(dev);

	if (IS_BROXTON(dev))
		ret = bxt_resume_prepare(dev_priv);
	else if (IS_SKYLAKE(dev))
		ret = skl_resume_prepare(dev_priv);
	else if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
		hsw_disable_pc8(dev_priv);
	else if (IS_VALLEYVIEW(dev_priv))
		ret = vlv_resume_prepare(dev_priv, true);

	/*
	 * No point of rolling back things in case of an error, as the best
	 * we can do is to hope that things will still work (and disable RPM).
	 */
	i915_gem_init_swizzling(dev);
	gen6_update_ring_freq(dev);

	intel_runtime_pm_enable_interrupts(dev_priv);

	/*
	 * On VLV/CHV display interrupts are part of the display
	 * power well, so hpd is reinitialized from there. For
	 * everyone else do it here.
	 */
	if (!IS_VALLEYVIEW(dev_priv))
		intel_hpd_init(dev_priv);

	intel_enable_gt_powersave(dev);

	if (ret)
		DRM_ERROR("Runtime resume failed, disabling it (%d)\n", ret);
	else
		DRM_DEBUG_KMS("Device resumed\n");

	return ret;
}
#endif

/*
 * This function implements common functionality of runtime and system
 * suspend sequence.
 */
static int intel_suspend_complete(struct drm_i915_private *dev_priv)
{
	int ret;

	if (IS_BROXTON(dev_priv))
		ret = bxt_suspend_complete(dev_priv);
	else if (IS_SKYLAKE(dev_priv))
		ret = skl_suspend_complete(dev_priv);
	else if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
		ret = hsw_suspend_complete(dev_priv);
	else if (IS_VALLEYVIEW(dev_priv))
		ret = vlv_suspend_complete(dev_priv);
	else
		ret = 0;

	return ret;
}

#ifdef __linux__
static const struct dev_pm_ops i915_pm_ops = {
	/*
	 * S0ix (via system suspend) and S3 event handlers [PMSG_SUSPEND,
	 * PMSG_RESUME]
	 */
	.suspend = i915_pm_suspend,
	.suspend_late = i915_pm_suspend_late,
	.resume_early = i915_pm_resume_early,
	.resume = i915_pm_resume,

	/*
	 * S4 event handlers
	 * @@freeze, @@freeze_late    : called (1) before creating the
	 *                            hibernation image [PMSG_FREEZE] and
	 *                            (2) after rebooting, before restoring
	 *                            the image [PMSG_QUIESCE]
	 * @@thaw, @@thaw_early       : called (1) after creating the hibernation
	 *                            image, before writing it [PMSG_THAW]
	 *                            and (2) after failing to create or
	 *                            restore the image [PMSG_RECOVER]
	 * @@poweroff, @@poweroff_late: called after writing the hibernation
	 *                            image, before rebooting [PMSG_HIBERNATE]
	 * @@restore, @@restore_early : called after rebooting and restoring the
	 *                            hibernation image [PMSG_RESTORE]
	 */
	.freeze = i915_pm_suspend,
	.freeze_late = i915_pm_suspend_late,
	.thaw_early = i915_pm_resume_early,
	.thaw = i915_pm_resume,
	.poweroff = i915_pm_suspend,
	.poweroff_late = i915_pm_poweroff_late,
	.restore_early = i915_pm_resume_early,
	.restore = i915_pm_resume,

	/* S0ix (via runtime suspend) event handlers */
	.runtime_suspend = intel_runtime_suspend,
	.runtime_resume = intel_runtime_resume,
};

static const struct vm_operations_struct i915_gem_vm_ops = {
	.fault = i915_gem_fault,
	.open = drm_gem_vm_open,
	.close = drm_gem_vm_close,
};

static const struct file_operations i915_driver_fops = {
	.owner = THIS_MODULE,
	.open = drm_open,
	.release = drm_release,
	.unlocked_ioctl = drm_ioctl,
	.mmap = drm_gem_mmap,
	.poll = drm_poll,
	.read = drm_read,
#ifdef CONFIG_COMPAT
	.compat_ioctl = i915_compat_ioctl,
#endif
	.llseek = noop_llseek,
};
#endif

static struct drm_driver driver = {
	/* Don't use MTRRs here; the Xserver or userspace app should
	 * deal with them for Intel hardware.
	 */
	.driver_features =
	    DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED | DRIVER_GEM | DRIVER_PRIME |
	    DRIVER_RENDER | DRIVER_MODESET,
#ifdef __linux__
	.load = i915_driver_load,
	.unload = i915_driver_unload,
#endif
	.open = i915_driver_open,
	.lastclose = i915_driver_lastclose,
	.preclose = i915_driver_preclose,
	.postclose = i915_driver_postclose,
#ifdef __linux__
	.set_busid = drm_pci_set_busid,
#endif

#if defined(CONFIG_DEBUG_FS)
	.debugfs_init = i915_debugfs_init,
	.debugfs_cleanup = i915_debugfs_cleanup,
#endif
	.gem_free_object = i915_gem_free_object,
#ifdef __linux__
	.gem_vm_ops = &i915_gem_vm_ops,
#else
	.gem_fault = i915_gem_fault,
#endif

#ifdef notyet
	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
	.gem_prime_export = i915_gem_prime_export,
	.gem_prime_import = i915_gem_prime_import,
#endif

	.dumb_create = i915_gem_dumb_create,
	.dumb_map_offset = i915_gem_mmap_gtt,
	.dumb_destroy = drm_gem_dumb_destroy,
	.ioctls = i915_ioctls,
#ifdef __linux__
	.fops = &i915_driver_fops,
#endif
	.name = DRIVER_NAME,
	.desc = DRIVER_DESC,
	.date = DRIVER_DATE,
	.major = DRIVER_MAJOR,
	.minor = DRIVER_MINOR,
	.patchlevel = DRIVER_PATCHLEVEL,
};

#ifdef __linux__

static struct pci_driver i915_pci_driver = {
	.name = DRIVER_NAME,
	.id_table = pciidlist,
	.probe = i915_pci_probe,
	.remove = i915_pci_remove,
	.driver.pm = &i915_pm_ops,
};

static int __init i915_init(void)
{
	driver.num_ioctls = i915_max_ioctl;

	/*
	 * Enable KMS by default, unless explicitly overriden by
	 * either the i915.modeset prarameter or by the
	 * vga_text_mode_force boot option.
	 */

	if (i915.modeset == 0)
		driver.driver_features &= ~DRIVER_MODESET;

#ifdef CONFIG_VGA_CONSOLE
	if (vgacon_text_force() && i915.modeset == -1)
		driver.driver_features &= ~DRIVER_MODESET;
#endif

	if (!(driver.driver_features & DRIVER_MODESET)) {
		/* Silently fail loading to not upset userspace. */
		DRM_DEBUG_DRIVER("KMS and UMS disabled.\n");
		return 0;
	}

	if (i915.nuclear_pageflip)
		driver.driver_features |= DRIVER_ATOMIC;

	return drm_pci_init(&driver, &i915_pci_driver);
}

static void __exit i915_exit(void)
{
	if (!(driver.driver_features & DRIVER_MODESET))
		return; /* Never loaded a driver. */

	drm_pci_exit(&driver, &i915_pci_driver);
}

module_init(i915_init);
module_exit(i915_exit);

#endif

MODULE_AUTHOR("Tungsten Graphics, Inc.");
MODULE_AUTHOR("Intel Corporation");

MODULE_DESCRIPTION(DRIVER_DESC);
MODULE_LICENSE("GPL and additional rights");

#ifdef __OpenBSD__

#ifdef __amd64__
#include "efifb.h"
#endif

#if NEFIFB > 0
#include <machine/efifbvar.h>
#endif

#include "intagp.h"

#if NINTAGP > 0
int	intagpsubmatch(struct device *, void *, void *);
int	intagp_print(void *, const char *);

int
intagpsubmatch(struct device *parent, void *match, void *aux)
{
	extern struct cfdriver intagp_cd;
	struct cfdata *cf = match;

	/* only allow intagp to attach */
	if (cf->cf_driver == &intagp_cd)
		return ((*cf->cf_attach->ca_match)(parent, match, aux));
	return (0);
}

int
intagp_print(void *vaa, const char *pnp)
{
	if (pnp)
		printf("intagp at %s", pnp);
	return (UNCONF);
}
#endif

int	inteldrm_wsioctl(void *, u_long, caddr_t, int, struct proc *);
paddr_t	inteldrm_wsmmap(void *, off_t, int);
int	inteldrm_alloc_screen(void *, const struct wsscreen_descr *,
	    void **, int *, int *, long *);
void	inteldrm_free_screen(void *, void *);
int	inteldrm_show_screen(void *, void *, int,
	    void (*)(void *, int, int), void *);
void	inteldrm_doswitch(void *);
void	inteldrm_enter_ddb(void *, void *);
int	inteldrm_load_font(void *, void *, struct wsdisplay_font *);
int	inteldrm_list_font(void *, struct wsdisplay_font *);
int	inteldrm_getchar(void *, int, int, struct wsdisplay_charcell *);
void	inteldrm_burner(void *, u_int, u_int);
void	inteldrm_burner_cb(void *);

struct wsscreen_descr inteldrm_stdscreen = {
	"std",
	0, 0,
	0,
	0, 0,
	WSSCREEN_UNDERLINE | WSSCREEN_HILIT |
	WSSCREEN_REVERSE | WSSCREEN_WSCOLORS
};

const struct wsscreen_descr *inteldrm_scrlist[] = {
	&inteldrm_stdscreen,
};

struct wsscreen_list inteldrm_screenlist = {
	nitems(inteldrm_scrlist), inteldrm_scrlist
};

struct wsdisplay_accessops inteldrm_accessops = {
	.ioctl = inteldrm_wsioctl,
	.mmap = inteldrm_wsmmap,
	.alloc_screen = inteldrm_alloc_screen,
	.free_screen = inteldrm_free_screen,
	.show_screen = inteldrm_show_screen,
	.enter_ddb = inteldrm_enter_ddb,
	.getchar = inteldrm_getchar,
	.load_font = inteldrm_load_font,
	.list_font = inteldrm_list_font,
	.burn_screen = inteldrm_burner
};

extern int (*ws_get_param)(struct wsdisplay_param *);
extern int (*ws_set_param)(struct wsdisplay_param *);

int
inteldrm_wsioctl(void *v, u_long cmd, caddr_t data, int flag, struct proc *p)
{
	struct inteldrm_softc *dev_priv = v;
	struct backlight_device *bd = dev_priv->backlight;
	struct rasops_info *ri = &dev_priv->ro;
	struct wsdisplay_fbinfo *wdf;
	struct wsdisplay_param *dp = (struct wsdisplay_param *)data;

	switch (cmd) {
	case WSDISPLAYIO_GTYPE:
		*(int *)data = WSDISPLAY_TYPE_INTELDRM;
		return 0;
	case WSDISPLAYIO_GINFO:
		wdf = (struct wsdisplay_fbinfo *)data;
		wdf->width = ri->ri_width;
		wdf->height = ri->ri_height;
		wdf->depth = ri->ri_depth;
		wdf->cmsize = 0;
		return 0;
	case WSDISPLAYIO_GETPARAM:
		if (ws_get_param && ws_get_param(dp) == 0)
			return 0;

		if (bd == NULL)
			return -1;

		switch (dp->param) {
		case WSDISPLAYIO_PARAM_BRIGHTNESS:
			dp->min = 0;
			dp->max = bd->props.max_brightness;
			dp->curval = bd->ops->get_brightness(bd);
			return (dp->max > dp->min) ? 0 : -1;
		}
		break;
	case WSDISPLAYIO_SETPARAM:
		if (ws_set_param && ws_set_param(dp) == 0)
			return 0;

		if (bd == NULL || dp->curval > bd->props.max_brightness)
			return -1;

		switch (dp->param) {
		case WSDISPLAYIO_PARAM_BRIGHTNESS:
			bd->props.brightness = dp->curval;
			backlight_update_status(bd);
			return 0;
		}
		break;
	}

	return (-1);
}

paddr_t
inteldrm_wsmmap(void *v, off_t off, int prot)
{
	return (-1);
}

int
inteldrm_alloc_screen(void *v, const struct wsscreen_descr *type,
    void **cookiep, int *curxp, int *curyp, long *attrp)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;

	return rasops_alloc_screen(ri, cookiep, curxp, curyp, attrp);
}

void
inteldrm_free_screen(void *v, void *cookie)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;

	return rasops_free_screen(ri, cookie);
}

int
inteldrm_show_screen(void *v, void *cookie, int waitok,
    void (*cb)(void *, int, int), void *cbarg)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;

	if (cookie == ri->ri_active)
		return (0);

	dev_priv->switchcb = cb;
	dev_priv->switchcbarg = cbarg;
	dev_priv->switchcookie = cookie;
	if (cb) {
		task_add(systq, &dev_priv->switchtask);
		return (EAGAIN);
	}

	inteldrm_doswitch(v);

	return (0);
}

void
inteldrm_doswitch(void *v)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;
	struct drm_device *dev = dev_priv->dev;

	rasops_show_screen(ri, dev_priv->switchcookie, 0, NULL, NULL);
	intel_fbdev_restore_mode(dev);

	if (dev_priv->switchcb)
		(*dev_priv->switchcb)(dev_priv->switchcbarg, 0, 0);
}

void
inteldrm_enter_ddb(void *v, void *cookie)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;
	struct drm_fb_helper *helper = &dev_priv->fbdev->helper;

	if (cookie == ri->ri_active)
		return;

	rasops_show_screen(ri, cookie, 0, NULL, NULL);
	drm_fb_helper_debug_enter(helper->fbdev);
}

int
inteldrm_getchar(void *v, int row, int col, struct wsdisplay_charcell *cell)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;

	return rasops_getchar(ri, row, col, cell);
}

int
inteldrm_load_font(void *v, void *cookie, struct wsdisplay_font *font)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;

	return rasops_load_font(ri, cookie, font);
}

int
inteldrm_list_font(void *v, struct wsdisplay_font *font)
{
	struct inteldrm_softc *dev_priv = v;
	struct rasops_info *ri = &dev_priv->ro;

	return rasops_list_font(ri, font);
}

void
inteldrm_burner(void *v, u_int on, u_int flags)
{
	struct inteldrm_softc *dev_priv = v;

	task_del(systq, &dev_priv->burner_task);

	if (on)
		dev_priv->burner_fblank = FB_BLANK_UNBLANK;
	else {
		if (flags & WSDISPLAY_BURN_VBLANK)
			dev_priv->burner_fblank = FB_BLANK_VSYNC_SUSPEND;
		else
			dev_priv->burner_fblank = FB_BLANK_NORMAL;
	}

	/*
	 * Setting the DPMS mode may sleep while waiting for the display
	 * to come back on so hand things off to a taskq.
	 */
	task_add(systq, &dev_priv->burner_task);
}

void
inteldrm_burner_cb(void *arg1)
{
	struct inteldrm_softc *dev_priv = arg1;
	struct drm_fb_helper *helper = &dev_priv->fbdev->helper;

	drm_fb_helper_blank(dev_priv->burner_fblank, helper->fbdev);
}

int
inteldrm_backlight_update_status(struct backlight_device *bd)
{
	struct wsdisplay_param dp;

	dp.param = WSDISPLAYIO_PARAM_BRIGHTNESS;
	dp.curval = bd->props.brightness;
	ws_set_param(&dp);
	return 0;
}

int
inteldrm_backlight_get_brightness(struct backlight_device *bd)
{
	struct wsdisplay_param dp;

	dp.param = WSDISPLAYIO_PARAM_BRIGHTNESS;
	ws_get_param(&dp);
	return dp.curval;
}

const struct backlight_ops inteldrm_backlight_ops = {
	.update_status = inteldrm_backlight_update_status,
	.get_brightness = inteldrm_backlight_get_brightness
};

int	inteldrm_match(struct device *, void *, void *);
void	inteldrm_attach(struct device *, struct device *, void *);
int	inteldrm_detach(struct device *, int);
int	inteldrm_activate(struct device *, int);

struct cfattach inteldrm_ca = {
	sizeof(struct inteldrm_softc), inteldrm_match, inteldrm_attach,
	inteldrm_detach, inteldrm_activate
};

struct cfdriver inteldrm_cd = {
	0, "inteldrm", DV_DULL
};

void	inteldrm_init_backlight(struct inteldrm_softc *);
int	inteldrm_intr(void *);

int
inteldrm_match(struct device *parent, void *match, void *aux)
{
	struct pci_attach_args *pa = aux;

	if (drm_pciprobe(aux, pciidlist) && pa->pa_function == 0)
		return 20;
	return 0;
}

void
inteldrm_attach(struct device *parent, struct device *self, void *aux)
{
	struct inteldrm_softc *dev_priv = (struct inteldrm_softc *)self;
	struct drm_device *dev;
	struct pci_attach_args *pa = aux;
	const struct drm_pcidev *id;
	struct intel_device_info *info, *device_info;
	struct rasops_info *ri = &dev_priv->ro;
	struct wsemuldisplaydev_attach_args aa;
	extern int vga_console_attached;
	int mmio_bar, mmio_size, mmio_type;
	int console = 0;

	dev_priv->pc = pa->pa_pc;
	dev_priv->tag = pa->pa_tag;
	dev_priv->dmat = pa->pa_dmat;
	dev_priv->bst = pa->pa_memt;
	dev_priv->memex = pa->pa_memex;
	dev_priv->regs = &dev_priv->bar;

	if (PCI_CLASS(pa->pa_class) == PCI_CLASS_DISPLAY &&
	    PCI_SUBCLASS(pa->pa_class) == PCI_SUBCLASS_DISPLAY_VGA &&
	    (pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_COMMAND_STATUS_REG)
	    & (PCI_COMMAND_IO_ENABLE | PCI_COMMAND_MEM_ENABLE))
	    == (PCI_COMMAND_IO_ENABLE | PCI_COMMAND_MEM_ENABLE)) {
		vga_console_attached = 1;
		console = 1;
	}

#if NEFIFB > 0
	if (efifb_is_console(pa))
		console = 1;
#endif

	printf("\n");

	driver.num_ioctls = i915_max_ioctl;

	if (i915.nuclear_pageflip)
		driver.driver_features |= DRIVER_ATOMIC;

	dev_priv->dev = dev = (struct drm_device *)
	    drm_attach_pci(&driver, pa, 0, console, self);

	id = drm_find_description(PCI_VENDOR(pa->pa_id),
	    PCI_PRODUCT(pa->pa_id), pciidlist);
	info = (struct intel_device_info *)id->driver_data;

	/* Setup the write-once "constant" device info */
	device_info = (struct intel_device_info *)&dev_priv->info;
	memcpy(device_info, info, sizeof(dev_priv->info));
	device_info->device_id = dev->pdev->device;

	mmio_bar = IS_GEN2(dev) ? 0x14 : 0x10;
	/* Before gen4, the registers and the GTT are behind different BARs.
	 * However, from gen4 onwards, the registers and the GTT are shared
	 * in the same BAR, so we want to restrict this ioremap from
	 * clobbering the GTT which we want ioremap_wc instead. Fortunately,
	 * the register BAR remains the same size for all the earlier
	 * generations up to Ironlake.
	 */
	if (info->gen < 5)
		mmio_size = 512*1024;
	else
		mmio_size = 2*1024*1024;

	mmio_type = pci_mapreg_type(pa->pa_pc, pa->pa_tag, mmio_bar);
	if (pci_mapreg_map(pa, mmio_bar, mmio_type, 0, &dev_priv->regs->bst,
	    &dev_priv->regs->bsh, &dev_priv->regs->base,
	    &dev_priv->regs->size, mmio_size)) {
		printf("%s: can't map registers\n",
		    dev_priv->sc_dev.dv_xname);
		return;
	}

#if NINTAGP > 0
	if (info->gen <= 5) {
		config_found_sm(self, aux, intagp_print, intagpsubmatch);
		dev->agp = drm_agp_init();
		if (dev->agp) {
			if (drm_mtrr_add(dev->agp->info.ai_aperture_base,
			    dev->agp->info.ai_aperture_size, DRM_MTRR_WC) == 0)
				dev->agp->mtrr = 1;
		}
	}
#endif

	if (IS_I945G(dev) || IS_I945GM(dev))
		pa->pa_flags &= ~PCI_FLAGS_MSI_ENABLED;

	if (pci_intr_map_msi(pa, &dev_priv->ih) != 0 &&
	    pci_intr_map(pa, &dev_priv->ih) != 0) {
		printf("%s: couldn't map interrupt\n",
		    dev_priv->sc_dev.dv_xname);
		return;
	}

	printf("%s: %s\n", dev_priv->sc_dev.dv_xname,
	    pci_intr_string(dev_priv->pc, dev_priv->ih));

	dev_priv->irqh = pci_intr_establish(dev_priv->pc, dev_priv->ih,
	    IPL_TTY, inteldrm_intr, dev_priv, dev_priv->sc_dev.dv_xname);
	if (dev_priv->irqh == NULL) {
		printf("%s: couldn't establish interrupt\n",
		    dev_priv->sc_dev.dv_xname);
		return;
	}
	dev->pdev->irq = -1;

	if (i915_driver_load(dev, id->driver_data))
		return;

#if NEFIFB > 0
	if (efifb_is_console(pa))
		efifb_cndetach();
#endif

	printf("%s: %dx%d, %dbpp\n", dev_priv->sc_dev.dv_xname,
	    ri->ri_width, ri->ri_height, ri->ri_depth);

	intel_fbdev_restore_mode(dev);

	inteldrm_init_backlight(dev_priv);

	ri->ri_flg = RI_CENTER | RI_WRONLY | RI_VCONS | RI_CLEAR;
	if (ri->ri_width < ri->ri_height)
		ri->ri_flg |= RI_ROTATE_CCW;
	ri->ri_hw = dev_priv;
	rasops_init(ri, 160, 160);

	task_set(&dev_priv->switchtask, inteldrm_doswitch, dev_priv);
	task_set(&dev_priv->burner_task, inteldrm_burner_cb, dev_priv);

	inteldrm_stdscreen.capabilities = ri->ri_caps;
	inteldrm_stdscreen.nrows = ri->ri_rows;
	inteldrm_stdscreen.ncols = ri->ri_cols;
	inteldrm_stdscreen.textops = &ri->ri_ops;
	inteldrm_stdscreen.fontwidth = ri->ri_font->fontwidth;
	inteldrm_stdscreen.fontheight = ri->ri_font->fontheight;

	aa.console = console;
	aa.scrdata = &inteldrm_screenlist;
	aa.accessops = &inteldrm_accessops;
	aa.accesscookie = dev_priv;
	aa.defaultscreens = 0;

	if (console) {
		long defattr;

		/*
		 * Clear the entire screen if we're doing rotation to
		 * make sure no unrotated content survives.
		 */
		if (ri->ri_flg & RI_ROTATE_CCW)
			memset(ri->ri_bits, 0, ri->ri_height * ri->ri_stride);

		ri->ri_ops.alloc_attr(ri->ri_active, 0, 0, 0, &defattr);
		wsdisplay_cnattach(&inteldrm_stdscreen, ri->ri_active,
		    0, 0, defattr);
	}

	config_found_sm(self, &aa, wsemuldisplaydevprint,
	    wsemuldisplaydevsubmatch);
	return;
}

int
inteldrm_detach(struct device *self, int flags)
{
	return 0;
}

int
inteldrm_activate(struct device *self, int act)
{
	struct inteldrm_softc *dev_priv = (struct inteldrm_softc *)self;
	struct drm_device *dev = dev_priv->dev;
	int rv = 0;

	if (dev == NULL)
		return (0);

	switch (act) {
	case DVACT_QUIESCE:
		rv = config_suspend((struct device *)dev, act);
		i915_drm_suspend(dev);
		i915_drm_suspend_late(dev, false);
		break;
	case DVACT_SUSPEND:
		if (dev->agp)
			config_suspend(dev->agp->agpdev->sc_chipc, act);
		break;
	case DVACT_RESUME:
		if (dev->agp)
			config_suspend(dev->agp->agpdev->sc_chipc, act);
		break;
	case DVACT_WAKEUP:
		i915_drm_resume_early(dev);
		i915_drm_resume(dev);
		intel_fbdev_restore_mode(dev);
		rv = config_suspend((struct device *)dev, act);
		break;
	}

	return (rv);
}

void
inteldrm_native_backlight(struct inteldrm_softc *dev_priv)
{
	struct drm_device *dev = dev_priv->dev;
	struct intel_connector *intel_connector;

	list_for_each_entry(intel_connector,
	    &dev->mode_config.connector_list, base.head) {
		struct drm_connector *connector = &intel_connector->base;
		struct intel_panel *panel = &intel_connector->panel;
		struct backlight_device *bd = panel->backlight.device;

		if (!panel->backlight.present)
			continue;

		connector->backlight_device = bd;
		connector->backlight_property = drm_property_create_range(dev,
		    0, "Backlight", 0, bd->props.max_brightness);
		drm_object_attach_property(&connector->base,
		    connector->backlight_property, bd->props.brightness);

		/*
		 * Use backlight from the first connector that has one
		 * for wscons(4).
		 */
		if (dev_priv->backlight == NULL)
			dev_priv->backlight = bd;
	}
}

void
inteldrm_firmware_backlight(struct inteldrm_softc *dev_priv,
    struct wsdisplay_param *dp)
{
	struct drm_device *dev = dev_priv->dev;
	struct intel_connector *intel_connector;
	struct backlight_properties props;
	struct backlight_device *bd;

	memset(&props, 0, sizeof(props));
	props.type = BACKLIGHT_FIRMWARE;
	props.brightness = dp->curval;
	bd = backlight_device_register(dev->device.dv_xname, NULL, NULL,
	    &inteldrm_backlight_ops, &props);

	list_for_each_entry(intel_connector,
	    &dev->mode_config.connector_list, base.head) {
		struct drm_connector *connector = &intel_connector->base;

		if (connector->connector_type != DRM_MODE_CONNECTOR_LVDS &&
		    connector->connector_type != DRM_MODE_CONNECTOR_eDP &&
		    connector->connector_type != DRM_MODE_CONNECTOR_DSI)
			continue;

		connector->backlight_device = bd;
		connector->backlight_property = drm_property_create_range(dev,
		    0, "Backlight", dp->min, dp->max);
		drm_object_attach_property(&connector->base,
		    connector->backlight_property, dp->curval);
	}
}

void
inteldrm_init_backlight(struct inteldrm_softc *dev_priv)
{
	struct drm_device *dev = dev_priv->dev;
	struct wsdisplay_param dp;

	drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);

	dp.param = WSDISPLAYIO_PARAM_BRIGHTNESS;
	if (ws_get_param && ws_get_param(&dp) == 0)
		inteldrm_firmware_backlight(dev_priv, &dp);
	else
		inteldrm_native_backlight(dev_priv);

	drm_modeset_unlock(&dev->mode_config.connection_mutex);
}

int
inteldrm_intr(void *arg)
{
	struct inteldrm_softc *dev_priv = arg;
	struct drm_device *dev = dev_priv->dev;

	return dev->driver->irq_handler(0, dev);
}

#endif
@


1.106
log
@Add a enter_ddb() "accessop" to wsdisplay(4) to allow KMS drivers to bypass
the modeset lock when entering ddb.  This avoids triggering various asserts
when the kernel panics while running X.

ok deraadt@@
@
text
@d2290 2
d2313 7
@


1.105
log
@Add a "Backlight" property to connectors with an associated backlight
controller for the inteldrm(4) driver.  If wscons(4) provides backlight
control, prefer ir over raw hardware control and attach it to LVDS, eDP
and DSI connectors which are the connector types typically connected to
laptop screens.
@
text
@d1882 1
d1912 1
d2036 14
@


1.104
log
@Fix native/raw backlight support in inteldrm(4).
@
text
@d2095 26
d2135 1
a2155 1
	struct intel_connector *intel_connector;
d2271 1
a2271 12
	/* Grab backlight from the first connector that has one. */
	drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
	list_for_each_entry(intel_connector, &dev->mode_config.connector_list,
	    base.head) {
		struct intel_panel *panel = &intel_connector->panel;

		if (panel->backlight.present) {
			dev_priv->backlight = panel->backlight.device;
			break;
		}
	}
	drm_modeset_unlock(&dev->mode_config.connection_mutex);
d2345 79
@


1.103
log
@Update inteldrm(4) to code based on Linux 4.4.70.  This brings us support for
Skylake and Cherryview and better support for Broadwell and Valleyview.  Also
adds MST support.  Some tweaks to the TTM code and radeondrm(4) to keep it
working with the updated generic DRM code needed for inteldrm(4).

Tested by many.
@
text
@d1924 1
a1924 1
	struct drm_device *dev = dev_priv->dev;
d1944 1
a1944 1
		if (dev_priv->backlight.connector == NULL)
d1950 2
a1951 2
			dp->max = dev_priv->backlight.props.max_brightness;
			dp->curval = dev_priv->backlight.props.brightness;
d1959 1
a1959 2
		if (dev_priv->backlight.connector == NULL ||
		    dp->curval > dev_priv->backlight.props.max_brightness)
d1964 2
a1965 4
			mutex_lock(&dev->mode_config.mutex);
			intel_panel_set_backlight_acpi(dev_priv->backlight.connector,
			    dp->curval, dev_priv->backlight.props.max_brightness);
			mutex_unlock(&dev->mode_config.mutex);
d2129 1
d2244 13
@


1.102
log
@Add a handler for the WSDISPLAYIO_GINFO ioctl in inteldrm, allowing
to retrieve basic information about a framebuffer display.

OK visa@@, tedu@@
@
text
@d1 1
a1 15
/* $OpenBSD: i915_drv.c,v 1.101 2017/01/08 12:11:54 fcambus Exp $ */
/*
 * Copyright (c) 2008-2009 Owain G. Ainsworth <oga@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
d30 4
a34 1
#include <dev/pci/drm/drm.h>
d36 1
a36 1
#include <dev/pci/drm/i915_pciids.h> /* XXX */
a40 13
#include <machine/pmap.h>

#define IS_I9XX(dev)	(INTEL_INFO(dev)->gen >= 3)
/* MCH IFP BARs */
#define I915_IFPADDR	0x60
#define I965_IFPADDR	0x70

struct inteldrm_file {
	struct drm_file	file_priv;
	struct {
	} mm;
};

d42 22
a63 126
static int i915_modeset __read_mostly = -1;
module_param_named(modeset, i915_modeset, int, 0400);
MODULE_PARM_DESC(modeset,
		"Use kernel modesetting [KMS] (0=DRM_I915_KMS from .config, "
		"1=on, -1=force vga console preference [default])");
#endif

unsigned int i915_fbpercrtc __always_unused = 0;
module_param_named(fbpercrtc, i915_fbpercrtc, int, 0400);

int i915_panel_ignore_lid __read_mostly = 1;
module_param_named(panel_ignore_lid, i915_panel_ignore_lid, int, 0600);
MODULE_PARM_DESC(panel_ignore_lid,
		"Override lid status (0=autodetect, 1=autodetect disabled [default], "
		"-1=force lid closed, -2=force lid open)");

unsigned int i915_powersave __read_mostly = 1;
module_param_named(powersave, i915_powersave, int, 0600);
MODULE_PARM_DESC(powersave,
		"Enable powersavings, fbc, downclocking, etc. (default: true)");

int i915_semaphores __read_mostly = -1;
module_param_named(semaphores, i915_semaphores, int, 0400);
MODULE_PARM_DESC(semaphores,
		"Use semaphores for inter-ring sync (default: -1 (use per-chip defaults))");

int i915_enable_rc6 __read_mostly = -1;
module_param_named(i915_enable_rc6, i915_enable_rc6, int, 0400);
MODULE_PARM_DESC(i915_enable_rc6,
		"Enable power-saving render C-state 6. "
		"Different stages can be selected via bitmask values "
		"(0 = disable; 1 = enable rc6; 2 = enable deep rc6; 4 = enable deepest rc6). "
		"For example, 3 would enable rc6 and deep rc6, and 7 would enable everything. "
		"default: -1 (use per-chip default)");

int i915_enable_fbc __read_mostly = -1;
module_param_named(i915_enable_fbc, i915_enable_fbc, int, 0600);
MODULE_PARM_DESC(i915_enable_fbc,
		"Enable frame buffer compression for power savings "
		"(default: -1 (use per-chip default))");

unsigned int i915_lvds_downclock __read_mostly = 0;
module_param_named(lvds_downclock, i915_lvds_downclock, int, 0400);
MODULE_PARM_DESC(lvds_downclock,
		"Use panel (LVDS/eDP) downclocking for power savings "
		"(default: false)");

int i915_lvds_channel_mode __read_mostly;
module_param_named(lvds_channel_mode, i915_lvds_channel_mode, int, 0600);
MODULE_PARM_DESC(lvds_channel_mode,
		 "Specify LVDS channel mode "
		 "(0=probe BIOS [default], 1=single-channel, 2=dual-channel)");

int i915_panel_use_ssc __read_mostly = -1;
module_param_named(lvds_use_ssc, i915_panel_use_ssc, int, 0600);
MODULE_PARM_DESC(lvds_use_ssc,
		"Use Spread Spectrum Clock with panels [LVDS/eDP] "
		"(default: auto from VBT)");

int i915_vbt_sdvo_panel_type __read_mostly = -1;
module_param_named(vbt_sdvo_panel_type, i915_vbt_sdvo_panel_type, int, 0600);
MODULE_PARM_DESC(vbt_sdvo_panel_type,
		"Override/Ignore selection of SDVO panel mode in the VBT "
		"(-2=ignore, -1=auto [default], index in VBT BIOS table)");

static bool i915_try_reset __read_mostly = true;
module_param_named(reset, i915_try_reset, bool, 0600);
MODULE_PARM_DESC(reset, "Attempt GPU resets (default: true)");

bool i915_enable_hangcheck __read_mostly = true;
module_param_named(enable_hangcheck, i915_enable_hangcheck, bool, 0644);
MODULE_PARM_DESC(enable_hangcheck,
		"Periodically check GPU activity for detecting hangs. "
		"WARNING: Disabling this can cause system wide hangs. "
		"(default: true)");

int i915_enable_ppgtt __read_mostly = -1;
module_param_named(i915_enable_ppgtt, i915_enable_ppgtt, int, 0400);
MODULE_PARM_DESC(i915_enable_ppgtt,
		"Enable PPGTT (default: true)");

int i915_enable_psr __read_mostly = 0;
module_param_named(enable_psr, i915_enable_psr, int, 0600);
MODULE_PARM_DESC(enable_psr, "Enable PSR (default: false)");

unsigned int i915_preliminary_hw_support __read_mostly = IS_ENABLED(CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT);
module_param_named(preliminary_hw_support, i915_preliminary_hw_support, int, 0600);
MODULE_PARM_DESC(preliminary_hw_support,
		"Enable preliminary hardware support.");

int i915_disable_power_well __read_mostly = 1;
module_param_named(disable_power_well, i915_disable_power_well, int, 0600);
MODULE_PARM_DESC(disable_power_well,
		 "Disable the power well when possible (default: true)");

int i915_enable_ips __read_mostly = 0;
module_param_named(enable_ips, i915_enable_ips, int, 0600);
MODULE_PARM_DESC(enable_ips, "Enable IPS (default: true)");

bool i915_fastboot __read_mostly = 0;
module_param_named(fastboot, i915_fastboot, bool, 0600);
MODULE_PARM_DESC(fastboot, "Try to skip unnecessary mode sets at boot time "
		 "(default: false)");

int i915_enable_pc8 __read_mostly = 1;
module_param_named(enable_pc8, i915_enable_pc8, int, 0600);
MODULE_PARM_DESC(enable_pc8, "Enable support for low power package C states (PC8+) (default: true)");

int i915_pc8_timeout __read_mostly = 5000;
module_param_named(pc8_timeout, i915_pc8_timeout, int, 0600);
MODULE_PARM_DESC(pc8_timeout, "Number of msecs of idleness required to enter PC8+ (default: 5000)");

bool i915_prefault_disable __read_mostly;
module_param_named(prefault_disable, i915_prefault_disable, bool, 0600);
MODULE_PARM_DESC(prefault_disable,
		"Disable page prefaulting for pread/pwrite/reloc (default:false). For developers only.");

const struct intel_device_info *
	i915_get_device_id(int);
int	inteldrm_probe(struct device *, void *, void *);
void	inteldrm_attach(struct device *, struct device *, void *);
int	inteldrm_detach(struct device *, int);
int	inteldrm_activate(struct device *, int);
int	inteldrm_intr(void *);
int	inteldrm_ioctl(struct drm_device *, u_long, caddr_t, struct drm_file *);
int	inteldrm_doioctl(struct drm_device *, u_long, caddr_t, struct drm_file *);
d65 2
a66 1
int	inteldrm_gmch_match(struct pci_attach_args *);
d68 2
a69 19
void	i915_alloc_ifp(struct inteldrm_softc *, struct pci_attach_args *);
void	i965_alloc_ifp(struct inteldrm_softc *, struct pci_attach_args *);
void	intel_gtt_chipset_setup(struct drm_device *);

/* i915_dma.c */
int	i915_get_bridge_dev(struct drm_device *);
void	intel_setup_mchbar(struct drm_device *);
void	intel_teardown_mchbar(struct drm_device *);
int	i915_load_modeset_init(struct drm_device *);

#undef INTEL_VGA_DEVICE
#define INTEL_VGA_DEVICE(id, info) {		\
	.class = PCI_CLASS_DISPLAY << 16,	\
	.class_mask = 0xff0000,			\
	.vendor = 0x8086,			\
	.device = id,				\
	.subvendor = PCI_ANY_ID,		\
	.subdevice = PCI_ANY_ID,		\
	.driver_data = (unsigned long) info }
d75 2
d83 2
d93 2
d101 2
d109 2
d119 2
d126 2
d136 2
d145 2
d155 2
d164 2
d172 2
d182 2
d190 2
d198 2
d207 2
d217 2
d227 2
d241 2
d249 2
d257 2
d269 2
d280 2
d290 2
d301 2
d311 4
d323 77
d427 1
d434 9
a442 2
	INTEL_BDW_M_IDS(&intel_broadwell_m_info),	\
	INTEL_BDW_D_IDS(&intel_broadwell_d_info)
d449 1
a449 11
static struct drm_driver_info inteldrm_driver = {
	.buf_priv_size		= 1,	/* No dev_priv */
	.file_priv_size		= sizeof(struct inteldrm_file),
	.open = i915_driver_open,
	.lastclose = i915_driver_lastclose,
	.preclose = i915_driver_preclose,
	.postclose = i915_driver_postclose,

	.gem_free_object	= i915_gem_free_object,
	.gem_fault		= i915_gem_fault,
	.gem_size		= sizeof(struct drm_i915_gem_object),
d451 4
a454 4
	.dumb_create = i915_gem_dumb_create,
	.dumb_map_offset = i915_gem_mmap_gtt,
	.dumb_destroy = drm_gem_dumb_destroy,
	.ioctls	= i915_ioctls,
d456 6
a461 16
	.name = DRIVER_NAME,
	.desc = DRIVER_DESC,
	.date = DRIVER_DATE,
	.major = DRIVER_MAJOR,
	.minor = DRIVER_MINOR,
	.patchlevel = DRIVER_PATCHLEVEL,

	.driver_features =
	    DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED | DRIVER_GEM |
	    DRIVER_MODESET,
};

const struct intel_device_info *
i915_get_device_id(int device)
{
	const struct drm_pcidev *did;
d463 12
a474 4
	for (did = &pciidlist[0]; did->device != 0; did++) {
		if (did->device != device)
			continue;
		return ((const struct intel_device_info *)did->driver_data);
a475 7
	return (NULL);
}

int
inteldrm_probe(struct device *parent, void *match, void *aux)
{
	struct pci_attach_args *pa = aux;
d477 1
a477 3
	if (drm_pciprobe(aux, pciidlist) && pa->pa_function == 0)
		return 20;
	return 0;
d479 1
d543 15
d569 7
a579 3
	if (i915_semaphores >= 0)
		return i915_semaphores;

d589 43
a631 1
static int i915_drm_freeze(struct drm_device *dev)
d634 2
a635 3
	struct drm_crtc *crtc;

	intel_runtime_pm_get(dev_priv);
d644 1
a644 2
	hsw_disable_package_c8(dev_priv);
	intel_display_set_init_power(dev, true);
a647 1
#ifdef __linux__
d649 10
d661 11
a671 10
	/* If KMS is active, we do the leavevt stuff here */
	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
		int error;

		error = i915_gem_suspend(dev);
		if (error) {
			dev_err(&dev->pdev->dev,
				"GEM idle failed, resume might fail\n");
			return error;
		}
d673 2
a674 1
		cancel_delayed_work_sync(&dev_priv->rps.delayed_resume_work);
d676 1
a676 10
		drm_irq_uninstall(dev);
		dev_priv->enable_hotplug_processing = false;
		/*
		 * Disable CRTCs directly since we want to preserve sw state
		 * for _thaw.
		 */
		mutex_lock(&dev->mode_config.mutex);
		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head)
			dev_priv->display.crtc_disable(crtc);
		mutex_unlock(&dev->mode_config.mutex);
d678 1
a678 2
		intel_modeset_suspend_hw(dev);
	}
d684 8
d695 39
a733 3
	console_lock();
	intel_fbdev_set_suspend(dev, FBINFO_STATE_SUSPENDED);
	console_unlock();
d740 1
a740 1
int i915_suspend(struct drm_device *dev, pm_message_t state)
d750 3
a752 3
	if (state.event == PM_EVENT_PRETHAW)
		return 0;

d757 1
a757 1
	error = i915_drm_freeze(dev);
d761 1
a761 25
	if (state.event == PM_EVENT_SUSPEND) {
		/* Shut down the device */
		pci_disable_device(dev->pdev);
		pci_set_power_state(dev->pdev, PCI_D3hot);
	}

	return 0;
}

void intel_console_resume(struct work_struct *work)
{
	struct drm_i915_private *dev_priv =
		container_of(work, struct drm_i915_private,
			     console_resume_work);
	struct drm_device *dev = dev_priv->dev;

	console_lock();
	intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING);
	console_unlock();
}

#else

void intel_console_resume(struct work_struct *work)
{
a762 1

d765 1
a765 1
static void intel_resume_hotplug(struct drm_device *dev)
d767 1
a767 2
	struct drm_mode_config *mode_config = &dev->mode_config;
	struct intel_encoder *encoder;
d769 3
a771 2
	mutex_lock(&mode_config->mutex);
	DRM_DEBUG_KMS("running encoder hotplug functions\n");
d773 2
a774 3
	list_for_each_entry(encoder, &mode_config->encoder_list, base.head)
		if (encoder->hot_plug)
			encoder->hot_plug(encoder);
d776 2
a777 1
	mutex_unlock(&mode_config->mutex);
d779 9
a787 3
	/* Just fire off a uevent and let userspace tell us what to do */
	drm_helper_hpd_irq_event(dev);
}
d789 4
a792 19
static int i915_drm_thaw_early(struct drm_device *dev)
{
	intel_uncore_early_sanitize(dev);
	intel_uncore_sanitize(dev);
	intel_power_domains_init_hw(dev);

	return 0;
}

static int __i915_drm_thaw(struct drm_device *dev, bool restore_gtt_mappings)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	int error = 0;

	if (drm_core_check_feature(dev, DRIVER_MODESET) &&
	    restore_gtt_mappings) {
		mutex_lock(&dev->struct_mutex);
		i915_gem_restore_gtt_mappings(dev);
		mutex_unlock(&dev->struct_mutex);
d794 1
d796 3
a798 6
	i915_restore_state(dev);
	intel_opregion_setup(dev);

	/* KMS EnterVT equivalent */
	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
		intel_init_pch_refclk(dev);
d800 1
a800 1
		mutex_lock(&dev->struct_mutex);
d802 4
a805 2
		error = i915_gem_init_hw(dev);
		mutex_unlock(&dev->struct_mutex);
d807 3
a809 2
		/* We need working interrupts for modeset enabling ... */
		drm_irq_install(dev);
d811 1
a811 1
		intel_modeset_init_hw(dev);
d813 9
a821 16
		drm_modeset_lock_all(dev);
		drm_mode_config_reset(dev);
		intel_modeset_setup_hw_state(dev, true);
		drm_modeset_unlock_all(dev);

		/*
		 * ... but also need to make sure that hotplug processing
		 * doesn't cause havoc. Like in the driver load code we don't
		 * bother with the tiny race here where we might loose hotplug
		 * notifications.
		 * */
		intel_hpd_init(dev);
		dev_priv->enable_hotplug_processing = true;
		/* Config may have changed between suspend and resume */
		intel_resume_hotplug(dev);
	}
d826 1
a826 11
	/*
	 * The console lock can be pretty contented on resume due
	 * to all the printk activity.  Try to keep it out of the hot
	 * path of resume if possible.
	 */
	if (console_trylock()) {
		intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING);
		console_unlock();
	} else {
		schedule_work(&dev_priv->console_resume_work);
	}
a828 4
	/* Undo what we did at i915_drm_freeze so the refcount goes back to the
	 * expected level. */
	hsw_enable_package_c8(dev_priv);

d833 1
a833 3
	intel_runtime_pm_put(dev_priv);
	return error;
}
d835 1
a835 4
static int i915_drm_thaw(struct drm_device *dev)
{
	if (drm_core_check_feature(dev, DRIVER_MODESET))
		i915_check_and_clear_faults(dev);
d837 1
a837 1
	return __i915_drm_thaw(dev, true);
d840 1
a840 2
#ifdef __linux__
static int i915_resume_early(struct drm_device *dev)
d842 2
a843 2
	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
		return 0;
d859 19
a877 1
	return i915_drm_thaw_early(dev);
d880 2
a881 1
int i915_resume(struct drm_device *dev)
a882 1
	struct drm_i915_private *dev_priv = dev->dev_private;
d885 4
a888 6
	/*
	 * Platforms with opregion should have sane BIOS, older ones (gen3 and
	 * earlier) need to restore the GTT mappings since the BIOS might clear
	 * all our scratch PTEs.
	 */
	ret = __i915_drm_thaw(dev, !dev_priv->opregion.header);
d892 1
a892 10
	drm_kms_helper_poll_enable(dev);
	return 0;
}

static int i915_resume_legacy(struct drm_device *dev)
{
	i915_resume_early(dev);
	i915_resume(dev);

	return 0;
d913 1
a913 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d917 1
a917 2
	if (!i915_try_reset)
		return 0;
d938 3
d947 2
a962 3
	if (drm_core_check_feature(dev, DRIVER_MODESET) ||
			!dev_priv->ums.mm_suspended) {
		dev_priv->ums.mm_suspended = 0;
d964 6
a969 6
		ret = i915_gem_init_hw(dev);
		mutex_unlock(&dev->struct_mutex);
		if (ret) {
			DRM_ERROR("Failed hw init on reset %d\n", ret);
			return ret;
		}
d971 4
a974 5
		drm_irq_uninstall(dev);
		drm_irq_install(dev);
		intel_hpd_init(dev);
	} else {
		mutex_unlock(&dev->struct_mutex);
d977 9
d989 853
a1841 3
#ifdef __amd64__
#include "efifb.h"
#endif
d1874 13
a1886 13
int inteldrm_wsioctl(void *, u_long, caddr_t, int, struct proc *);
paddr_t inteldrm_wsmmap(void *, off_t, int);
int inteldrm_alloc_screen(void *, const struct wsscreen_descr *,
    void **, int *, int *, long *);
void inteldrm_free_screen(void *, void *);
int inteldrm_show_screen(void *, void *, int,
    void (*)(void *, int, int), void *);
void inteldrm_doswitch(void *);
int inteldrm_load_font(void *, void *, struct wsdisplay_font *);
int inteldrm_list_font(void *, struct wsdisplay_font *);
int inteldrm_getchar(void *, int, int, struct wsdisplay_charcell *);
void inteldrm_burner(void *, u_int, u_int);
void inteldrm_burner_cb(void *);
d1966 1
a1966 1
			intel_panel_set_backlight(dev_priv->backlight.connector,
d2074 1
a2074 1
		dev_priv->burner_dpms_mode = DRM_MODE_DPMS_ON;
d2077 1
a2077 1
			dev_priv->burner_dpms_mode = DRM_MODE_DPMS_OFF;
d2079 1
a2079 1
			dev_priv->burner_dpms_mode = DRM_MODE_DPMS_STANDBY;
d2095 1
a2095 1
	drm_fb_helper_dpms(helper, dev_priv->burner_dpms_mode);
d2098 16
d2115 1
a2115 1
inteldrm_intr(void *arg)
d2117 1
a2117 2
	struct inteldrm_softc *dev_priv = arg;
	struct drm_device *dev = dev_priv->dev;
d2119 3
a2121 1
	return dev->driver->irq_handler(0, dev);
d2128 1
d2130 2
d2135 1
a2135 5
	struct drm_device *dev;
	const struct intel_device_info *info;
	int ret = 0, mmio_bar, mmio_size;
	pcireg_t mmio_type;
	uint32_t aperture_size;
a2136 6
	int i;

	info = i915_get_device_id(PCI_PRODUCT(pa->pa_id));
	KASSERT(info->gen != 0);

	dev_priv->info = info;
d2161 1
a2161 1
	inteldrm_driver.num_ioctls = i915_max_ioctl;
d2163 2
a2164 2
	dev = dev_priv->dev = (struct drm_device *)
	    drm_attach_pci(&inteldrm_driver, pa, 0, console, self);
d2166 2
a2167 2
	intel_gtt_chipset_setup(dev);
	mtx_init(&mchdev_lock, IPL_TTY);
d2169 3
a2171 7
	mtx_init(&dev_priv->irq_lock, IPL_TTY);
	mtx_init(&dev_priv->gpu_error.lock, IPL_TTY);
	mtx_init(&dev_priv->backlight_lock, IPL_TTY);
	mtx_init(&dev_priv->uncore.lock, IPL_TTY);
	mtx_init(&dev_priv->mm.object_stat_lock, IPL_TTY);
	rw_init(&dev_priv->dpio_lock, "dpio");
	rw_init(&dev_priv->modeset_restore_lock, "rest");
d2173 4
a2176 1
	intel_pm_setup(dev);
d2178 1
a2178 20
#ifdef __linux__
	intel_display_crc_init(dev);

	i915_dump_device_info(dev_priv);

	/* Not all pre-production machines fall into this category, only the
	 * very first ones. Almost everything should work, except for maybe
	 * suspend/resume. And we don't implement workarounds that affect only
	 * pre-production machines. */
	if (IS_HSW_EARLY_SDV(dev))
		DRM_INFO("This is an early pre-production Haswell machine. "
			 "It may not be fully functional.\n");
#endif

	if (i915_get_bridge_dev(dev)) {
		ret = -EIO;
		goto free_priv;
	}

	mmio_bar = IS_GEN2(dev) ? 1 : 0;
a2190 1
	mmio_bar = 0x10 + mmio_bar * 0x04;
d2195 1
a2195 1
		printf("%s: can't map mmio space\n",
d2197 1
a2197 1
		goto free_priv;
d2201 1
a2201 1
	if (dev_priv->info->gen <= 5) {
a2211 112
	intel_uncore_early_sanitize(dev);

	/* This must be called before any calls to HAS_PCH_* */
	intel_detect_pch(dev);

	intel_uncore_init(dev);

	ret = i915_gem_gtt_init(dev);
	if (ret)
		goto out_regs;

#ifdef __linux__
	if (drm_core_check_feature(dev, DRIVER_MODESET))
		i915_kick_out_firmware_fb(dev_priv);

	pci_set_master(dev->pdev);

	/* overlay on gen2 is broken and can't address above 1G */
	if (IS_GEN2(dev))
		dma_set_coherent_mask(&dev->pdev->dev, DMA_BIT_MASK(30));

	/* 965GM sometimes incorrectly writes to hardware status page (HWS)
	 * using 32bit addressing, overwriting memory if HWS is located
	 * above 4GB.
	 *
	 * The documentation also mentions an issue with undefined
	 * behaviour if any general state is accessed within a page above 4GB,
	 * which also needs to be handled carefully.
	 */
	if (IS_BROADWATER(dev) || IS_CRESTLINE(dev))
		dma_set_coherent_mask(&dev->pdev->dev, DMA_BIT_MASK(32));
#endif

	aperture_size = dev_priv->gtt.mappable_end;

#ifdef __linux__
	dev_priv->gtt.mappable =
		io_mapping_create_wc(dev_priv->gtt.mappable_base,
				     aperture_size);
	if (dev_priv->gtt.mappable == NULL) {
		ret = -EIO;
		goto out_gtt;
	}

	dev_priv->gtt.mtrr = arch_phys_wc_add(dev_priv->gtt.mappable_base,
					      aperture_size);
#else
	/* XXX would be a lot nicer to get agp info before now */
	uvm_page_physload(atop(dev_priv->gtt.mappable_base),
	    atop(dev_priv->gtt.mappable_base + aperture_size),
	    atop(dev_priv->gtt.mappable_base),
	    atop(dev_priv->gtt.mappable_base + aperture_size),
	    PHYSLOAD_DEVICE);
	/* array of vm pages that physload introduced. */
	dev_priv->pgs = PHYS_TO_VM_PAGE(dev_priv->gtt.mappable_base);
	KASSERT(dev_priv->pgs != NULL);
	/*
	 * XXX mark all pages write combining so user mmaps get the right
	 * bits. We really need a proper MI api for doing this, but for now
	 * this allows us to use PAT where available.
	 */
	for (i = 0; i < atop(aperture_size); i++)
		atomic_setbits_int(&(dev_priv->pgs[i].pg_flags), PG_PMAP_WC);
	if (agp_init_map(dev_priv->bst, dev_priv->gtt.mappable_base,
	    aperture_size, BUS_SPACE_MAP_LINEAR | BUS_SPACE_MAP_PREFETCHABLE,
	    &dev_priv->agph))
		panic("can't map aperture");
#endif

	/* The i915 workqueue is primarily used for batched retirement of
	 * requests (and thus managing bo) once the task has been completed
	 * by the GPU. i915_gem_retire_requests() is called directly when we
	 * need high-priority retirement, such as waiting for an explicit
	 * bo.
	 *
	 * It is also used for periodic low-priority events, such as
	 * idle-timers and recording error state.
	 *
	 * All tasks on the workqueue are expected to acquire the dev mutex
	 * so there is no point in running more than one instance of the
	 * workqueue at any time.  Use an ordered one.
	 */
	dev_priv->wq = alloc_ordered_workqueue("i915", 0);
	if (dev_priv->wq == NULL) {
		DRM_ERROR("Failed to create our workqueue.\n");
		ret = -ENOMEM;
		goto out_mtrrfree;
	}

	intel_irq_init(dev);
	intel_uncore_sanitize(dev);

	/* Try to make sure MCHBAR is enabled before poking at it */
	intel_setup_mchbar(dev);
	intel_setup_gmbus(dev);
	intel_opregion_setup(dev);

	intel_setup_bios(dev);

	i915_gem_load(dev);

	/* On the 945G/GM, the chipset reports the MSI capability on the
	 * integrated graphics even though the support isn't actually there
	 * according to the published specs.  It doesn't appear to function
	 * correctly in testing on 945G.
	 * This may be a side effect of MSI having been made available for PEG
	 * and the registers being closely associated.
	 *
	 * According to chipset errata, on the 965GM, MSI interrupts may
	 * be lost or delayed, but we use them anyways to avoid
	 * stuck interrupts on some machines.
	 */
d2219 1
a2219 1
		goto out_gem_unload;
d2230 1
a2230 24
		goto out_gem_unload;
	}

	dev_priv->num_plane = 1;
	if (IS_VALLEYVIEW(dev))
		dev_priv->num_plane = 2;

	if (INTEL_INFO(dev)->num_pipes) {
		ret = drm_vblank_init(dev, INTEL_INFO(dev)->num_pipes);
		if (ret)
			goto out_gem_unload;
	}

	intel_power_domains_init(dev);

	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
		ret = i915_load_modeset_init(dev);
		if (ret < 0) {
			DRM_ERROR("failed to init modeset\n");
			goto out_power_well;
		}
	} else {
		/* Start out suspended in ums mode. */
		dev_priv->ums.mm_suspended = 1;
d2232 1
d2234 1
a2234 19
#ifdef __linux__
	i915_setup_sysfs(dev);
#endif

	if (INTEL_INFO(dev)->num_pipes) {
		/* Must be done after probing outputs */
		intel_opregion_init(dev);
#ifdef __linux__
		acpi_video_register();
#endif
	}

	if (IS_GEN5(dev))
		intel_gpu_ips_init(dev_priv);

	intel_init_runtime_pm(dev_priv);

	/* Check if we managed to set up a framebuffer. */
	if (ri->ri_bits == NULL)
a2277 39

out_power_well:
	intel_power_domains_remove(dev);
	drm_vblank_cleanup(dev);
out_gem_unload:
#ifdef notyet
	if (dev_priv->mm.inactive_shrinker.scan_objects)
		unregister_shrinker(&dev_priv->mm.inactive_shrinker);
#endif

	if (dev_priv->irqh)
		pci_intr_disestablish(dev_priv->pc, dev_priv->irqh);

	list_del(&dev_priv->gtt.base.global_link);
	drm_mm_takedown(&dev_priv->gtt.base.mm);
	dev_priv->gtt.base.cleanup(&dev_priv->gtt.base);

	intel_teardown_gmbus(dev);
	intel_teardown_mchbar(dev);
#ifdef notyet
	pm_qos_remove_request(&dev_priv->pm_qos);
#endif
	destroy_workqueue(dev_priv->wq);
out_mtrrfree:
#ifdef __linux__
	arch_phys_wc_del(dev_priv->gtt.mtrr);
	io_mapping_free(dev_priv->gtt.mappable);
#endif
out_regs:
	intel_uncore_fini(dev);
#ifdef __linux__
	pci_iounmap(dev->pdev, dev_priv->regs);
#else
	bus_space_unmap(dev_priv->regs->bst, dev_priv->regs->bsh,
	    dev_priv->regs->size);
#endif
free_priv:
	dev->dev_private = NULL;
	return;
d2283 1
a2283 26
	struct inteldrm_softc	*dev_priv = (struct inteldrm_softc *)self;
	struct drm_device	*dev = dev_priv->dev;

	/* this will quiesce any dma that's going on and kill the timeouts. */
	if (dev_priv->dev != NULL) {
		config_detach((struct device *)dev_priv->dev, flags);
		dev_priv->dev = NULL;
	}

	if (IS_I9XX(dev) && dev_priv->ifp.i9xx.bsh != 0) {
		bus_space_unmap(dev_priv->ifp.i9xx.bst, dev_priv->ifp.i9xx.bsh,
		    PAGE_SIZE);
	} else if ((IS_I830(dev) || IS_845G(dev) || IS_I85X(dev) ||
	    IS_I865G(dev)) && dev_priv->ifp.i8xx.kva != NULL) {
		bus_dmamem_unmap(dev_priv->dmat, dev_priv->ifp.i8xx.kva,
		     PAGE_SIZE);
		bus_dmamem_free(dev_priv->dmat, &dev_priv->ifp.i8xx.seg, 1);
	}

	pci_intr_disestablish(dev_priv->pc, dev_priv->irqh);

	if (dev_priv->regs != NULL)
		bus_space_unmap(dev_priv->regs->bst, dev_priv->regs->bsh,
		    dev_priv->regs->size);

	return (0);
d2299 2
a2300 1
		i915_drm_freeze(dev);
d2311 2
a2312 2
		i915_drm_thaw_early(dev);
		i915_drm_thaw(dev);
a2320 13
struct cfattach inteldrm_ca = {
	sizeof(struct inteldrm_softc), inteldrm_probe, inteldrm_attach,
	inteldrm_detach, inteldrm_activate
};

struct cfdriver inteldrm_cd = {
	0, "inteldrm", DV_DULL
};

/*
 * We're intel IGD, bus 0 function 0 dev 0 should be the GMCH, so it should
 * be Intel
 */
d2322 1
a2322 1
inteldrm_gmch_match(struct pci_attach_args *pa)
d2324 2
a2325 7
	if (pa->pa_bus == 0 && pa->pa_device == 0 && pa->pa_function == 0 &&
	    PCI_VENDOR(pa->pa_id) == PCI_VENDOR_INTEL &&
	    PCI_CLASS(pa->pa_class) == PCI_CLASS_BRIDGE &&
	    PCI_SUBCLASS(pa->pa_class) == PCI_SUBCLASS_BRIDGE_HOST)
		return (1);
	return (0);
}
d2327 1
a2327 31
void
i915_alloc_ifp(struct inteldrm_softc *dev_priv, struct pci_attach_args *bpa)
{
	bus_addr_t	addr;
	u_int32_t	reg;

	dev_priv->ifp.i9xx.bst = bpa->pa_memt;

	reg = pci_conf_read(bpa->pa_pc, bpa->pa_tag, I915_IFPADDR);
	if (reg & 0x1) {
		addr = (bus_addr_t)reg;
		addr &= ~0x1;
		/* XXX extents ... need data on whether bioses alloc or not. */
		if (bus_space_map(bpa->pa_memt, addr, PAGE_SIZE, 0,
		    &dev_priv->ifp.i9xx.bsh) != 0)
			goto nope;
		return;
	} else if (bpa->pa_memex == NULL ||
	    extent_alloc_subregion(bpa->pa_memex, 0x100000, 0xffffffff,
	    PAGE_SIZE, PAGE_SIZE, 0, 0, 0, &addr) ||
	    bus_space_map(bpa->pa_memt, addr, PAGE_SIZE, 0,
	    &dev_priv->ifp.i9xx.bsh))
		goto nope;

	pci_conf_write(bpa->pa_pc, bpa->pa_tag, I915_IFPADDR, addr | 0x1);

	return;

nope:
	dev_priv->ifp.i9xx.bsh = 0;
	printf("%s: no ifp\n", dev_priv->sc_dev.dv_xname);
d2330 1
a2330 131
void
i965_alloc_ifp(struct inteldrm_softc *dev_priv, struct pci_attach_args *bpa)
{
	bus_addr_t	addr;
	u_int32_t	lo, hi;

	dev_priv->ifp.i9xx.bst = bpa->pa_memt;

	hi = pci_conf_read(bpa->pa_pc, bpa->pa_tag, I965_IFPADDR + 4);
	lo = pci_conf_read(bpa->pa_pc, bpa->pa_tag, I965_IFPADDR);
	if (lo & 0x1) {
		addr = (((u_int64_t)hi << 32) | lo);
		addr &= ~0x1;
		/* XXX extents ... need data on whether bioses alloc or not. */
		if (bus_space_map(bpa->pa_memt, addr, PAGE_SIZE, 0,
		    &dev_priv->ifp.i9xx.bsh) != 0)
			goto nope;
		return;
	} else if (bpa->pa_memex == NULL ||
	    extent_alloc_subregion(bpa->pa_memex, 0x100000, 0xffffffff,
	    PAGE_SIZE, PAGE_SIZE, 0, 0, 0, &addr) ||
	    bus_space_map(bpa->pa_memt, addr, PAGE_SIZE, 0,
	    &dev_priv->ifp.i9xx.bsh))
		goto nope;

	pci_conf_write(bpa->pa_pc, bpa->pa_tag, I965_IFPADDR + 4,
	    upper_32_bits(addr));
	pci_conf_write(bpa->pa_pc, bpa->pa_tag, I965_IFPADDR,
	    (addr & 0xffffffff) | 0x1);

	return;

nope:
	dev_priv->ifp.i9xx.bsh = 0;
	printf("%s: no ifp\n", dev_priv->sc_dev.dv_xname);
}

void
intel_gtt_chipset_setup(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct pci_attach_args bpa;

	if (INTEL_INFO(dev)->gen >= 6)
		return;

	if (pci_find_device(&bpa, inteldrm_gmch_match) == 0) {
		printf("%s: can't find GMCH\n",
		    dev_priv->sc_dev.dv_xname);
		return;
	}

	/* Set up the IFP for chipset flushing */
	if (IS_I915G(dev) || IS_I915GM(dev) || IS_I945G(dev) ||
	    IS_I945GM(dev)) {
		i915_alloc_ifp(dev_priv, &bpa);
	} else if (INTEL_INFO(dev)->gen >= 4 || IS_G33(dev)) {
		i965_alloc_ifp(dev_priv, &bpa);
	} else {
		int nsegs;
		/*
		 * I8XX has no flush page mechanism, we fake it by writing until
		 * the cache is empty. allocate a page to scribble on
		 */
		dev_priv->ifp.i8xx.kva = NULL;
		if (bus_dmamem_alloc(dev_priv->dmat, PAGE_SIZE, 0, 0,
		    &dev_priv->ifp.i8xx.seg, 1, &nsegs, BUS_DMA_WAITOK) == 0) {
			if (bus_dmamem_map(dev_priv->dmat, &dev_priv->ifp.i8xx.seg,
			    1, PAGE_SIZE, &dev_priv->ifp.i8xx.kva, 0) != 0) {
				bus_dmamem_free(dev_priv->dmat,
				    &dev_priv->ifp.i8xx.seg, nsegs);
				dev_priv->ifp.i8xx.kva = NULL;
			}
		}
	}
}

void
intel_gtt_chipset_flush(void)
{
	struct inteldrm_softc *dev_priv = (void *)inteldrm_cd.cd_devs[0];

	/*
	 * Write to this flush page flushes the chipset write cache.
	 * The write will return when it is done.
	 */
	if (IS_I9XX(dev_priv->dev)) {
	    if (dev_priv->ifp.i9xx.bsh != 0)
		bus_space_write_4(dev_priv->ifp.i9xx.bst,
		    dev_priv->ifp.i9xx.bsh, 0, 1);
	} else {
		int i;

		wbinvd();

#define I830_HIC        0x70

		I915_WRITE(I830_HIC, (I915_READ(I830_HIC) | (1<<31)));
		for (i = 1000; i; i--) {
			if (!(I915_READ(I830_HIC) & (1<<31)))
				break;
			delay(100);
		}

	}
}

int
intel_gmch_probe(struct pci_dev *bridge_pdev, struct pci_dev *gpu_pdev,
		 void *bridge)
{
	return 1;
}

void
intel_gtt_get(size_t *gtt_total, size_t *stolen_size,
    phys_addr_t *mappable_base, unsigned long *mappable_end)
{
	struct inteldrm_softc *dev_priv = (void *)inteldrm_cd.cd_devs[0];
	struct agp_info *ai = &dev_priv->dev->agp->info;
	
	*gtt_total = ai->ai_aperture_size;
	*stolen_size = 0;
	*mappable_base = ai->ai_aperture_base;
	*mappable_end = ai->ai_aperture_size;
}

void
intel_gmch_remove(void)
{
}
@


1.101
log
@Display color depth alongside resolution when attaching inteldrm and
radeondrm, using the same scheme as efifb(4).

OK mpi@@, visa@@, kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.100 2016/04/08 08:27:53 kettenis Exp $ */
d1040 2
d1047 7
@


1.100
log
@Get rid of some infrastrcuture that is now obsolete and synchronize some of
the data structures in drmP.h with Linux 3.14.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.99 2015/12/31 12:36:04 kettenis Exp $ */
d1511 2
a1512 2
	printf("%s: %dx%d\n", dev_priv->sc_dev.dv_xname,
	    ri->ri_width, ri->ri_height);
@


1.99
log
@Unconditionally set the "switchcookie".  Fixes synchronous VT switching.
Matches what radeondrm(4) already did.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.98 2015/11/22 15:35:49 kettenis Exp $ */
d478 3
a480 1
	.flags = DRIVER_HAVE_IRQ | DRIVER_GEM | DRIVER_MODESET,
@


1.98
log
@Remove drm_gem_object_alloc() and associated infrastructure.  It's unused and
has been removed upstream as well.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.97 2015/11/01 03:42:56 jsg Exp $ */
d1118 1
a1119 1
		dev_priv->switchcookie = cookie;
@


1.97
log
@drm/i915: Fix and clean BDW PCH identification

From Rodrigo Vivi
a35cc9d0c0118fb18f7c5dd7a44adb454868a679 in mainline linux

This removes some warnings that incorrectly trigger on desktop broadwell.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.96 2015/10/30 11:21:01 kettenis Exp $ */
a461 1
//	.gem_init_object	= i915_gem_init_object,
@


1.96
log
@Fix interaction between inteldrm(4) and efifb(4).  If we were booted by UEFI
firmware and the efifb(4) framebuffer address matches one of the BARs
associated with the inteldrm(4) device, make inteldrm(4) the console and
prevent efifb(4) from attaching.  Make sure that we do a full clear of the
framebuffer when inteldrm(4) attaches to prevent the contents of the old
framebuffer from showing up.

Based on an earlier diff from yasuoka@@
ok yasuoka@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.95 2015/10/29 07:47:03 kettenis Exp $ */
d560 2
a561 8
				WARN_ON(!IS_HASWELL(dev));
				WARN_ON(IS_ULT(dev));
			} else if (IS_BROADWELL(dev)) {
				dev_priv->pch_type = PCH_LPT;
				dev_priv->pch_id =
					INTEL_PCH_LPT_LP_DEVICE_ID_TYPE;
				DRM_DEBUG_KMS("This is Broadwell, assuming "
					      "LynxPoint LP PCH\n");
d565 2
a566 2
				WARN_ON(!IS_HASWELL(dev));
				WARN_ON(!IS_ULT(dev));
@


1.95
log
@Make inteldrm(4) attach to pci(4) instead of vga(4) just like radeondrm(4).
This is needed for machines where Intel graphics isn't the primary graphics
device and on systems with UEFI firmware that put the device in non-VGA mode.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.94 2015/10/17 21:41:12 kettenis Exp $ */
d959 8
a1233 9
	if (PCI_CLASS(pa->pa_class) == PCI_CLASS_DISPLAY &&
	    PCI_SUBCLASS(pa->pa_class) == PCI_SUBCLASS_DISPLAY_VGA &&
	    (pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_COMMAND_STATUS_REG)
	    & (PCI_COMMAND_IO_ENABLE | PCI_COMMAND_MEM_ENABLE))
	    == (PCI_COMMAND_IO_ENABLE | PCI_COMMAND_MEM_ENABLE)) {
		console = 1;
		vga_console_attached = 1;
	}

d1246 14
d1511 5
d1521 1
a1521 1
	ri->ri_flg = RI_CENTER | RI_WRONLY | RI_VCONS;
@


1.94
log
@Fix the code that sets up the MCH BAR on systems where the (buggy) BIOS
doesn't do this for us.  The code was poking registers on the wrong PCI
device.  We were just lucky that it worked on most systems.

This should fix machines such as the Asus EeePC 701 and get rid of the

error: [drm:pid0:i915_gem_detect_bit_6_swizzle] *ERROR* Couldn't read from
MC HBAR.  Disabling tiling.

messages on that machine.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.93 2015/09/28 17:29:56 kettenis Exp $ */
d479 1
a479 3
	.flags			= DRIVER_AGP | DRIVER_AGP_REQUIRE |
				    DRIVER_HAVE_IRQ | DRIVER_GEM |
				    DRIVER_MODESET,
d498 5
a502 1
	return (drm_pciprobe((struct pci_attach_args *)aux, pciidlist));
d959 26
a1213 1
	struct vga_pci_softc *vga_sc = (struct vga_pci_softc *)parent;
d1217 1
a1217 2
	extern int wsdisplay_console_initted;
	struct vga_pci_bar *bar;
d1221 1
d1223 1
d1226 9
d1245 1
a1248 3
	if (dev_priv->info->gen >= 6)
		inteldrm_driver.flags &= ~(DRIVER_AGP | DRIVER_AGP_REQUIRE);

a1250 1
	/* All intel chipsets need to be treated as agp, so just pass one */
d1252 1
a1252 1
	    drm_attach_pci(&inteldrm_driver, pa, 1, 1, self);
d1299 6
a1304 4
	/* we need to use this api for now due to sharing with intagp */
	bar = vga_pci_bar_info(vga_sc, mmio_bar);
	if (bar == NULL) {
		printf("%s: can't get BAR info\n",
d1309 9
a1317 5
	dev_priv->regs = vga_pci_bar_map(vga_sc, bar->addr, mmio_size, 0);
	if (dev_priv->regs == NULL) {
		printf("%s: can't map mmio space\n",
		    dev_priv->sc_dev.dv_xname);
		goto free_priv;
d1319 1
d1517 1
a1517 1
	aa.console = 0;
d1523 1
a1523 1
	if (wsdisplay_console_initted) {
a1528 1
		aa.console = 1;
d1531 2
a1532 2
	vga_sc->sc_type = -1;
	config_found(parent, &aa, wsemuldisplaydevprint);
d1567 2
a1568 1
	vga_pci_bar_unmap(dev_priv->regs);
d1600 2
a1601 1
		vga_pci_bar_unmap(dev_priv->regs);
d1618 1
a1618 1
		rv = config_activate_children(self, act);
d1622 2
d1626 2
d1633 1
a1633 1
		rv = config_activate_children(self, act);
@


1.93
log
@The Linux code that handles the DPMS mode for inteldrm(4) can sleep now.
Adopt the approach taken by radeondrm(4) and hand the "burner" work off
to a task.

Avoids the panic reported by Gerald Hanuer, who also tested this fix.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.92 2015/09/28 06:47:23 kettenis Exp $ */
d200 1
d1207 1
d1245 1
a1250 1
#endif
@


1.92
log
@Remove the "Quanta Transcode" device from the list of supported hardware.
It's only supposed to match certain subvendor/subdevice IDs, but our code
doesn't check those.  The result is that it (incorrectly) overrides the
generic match for the HD Graphics P4000 as found on some Xeon E3 CPUs.
This device is supposedly a castrated version of that device with the
display output parts fused off.  According to the original Linux commit
it is "some HW being used for a demo", and there have been proposals to
remove it from the Linux tree as well.  It is unlikely that OpenBSD will
ever run on this particular hardware.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.91 2015/09/26 22:00:00 kettenis Exp $ */
d969 1
d1144 2
a1145 2
	struct drm_fb_helper *helper = &dev_priv->fbdev->helper;
	int dpms_mode;
d1148 1
a1148 1
		dpms_mode = DRM_MODE_DPMS_ON;
d1151 1
a1151 1
			dpms_mode = DRM_MODE_DPMS_OFF;
d1153 1
a1153 1
			dpms_mode = DRM_MODE_DPMS_STANDBY;
d1156 14
a1169 1
	drm_fb_helper_dpms(helper, dpms_mode);
d1465 1
@


1.91
log
@Try a little bit harder to clean up if attaching inteldrm(4) fails.
The crucial bit is that we now clear dev->dev_priv, which prevents the X
server from opening /dev/drmN and crashing the kernel because the driver
isn't fully initialized.

While there, try a little bit harder to print error messages the proper way.
Things will still look ugly though if the failure is somewhere in the Linux
code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.90 2015/09/26 19:52:16 kettenis Exp $ */
a213 10
#undef INTEL_QUANTA_VGA_DEVICE
#define INTEL_QUANTA_VGA_DEVICE(info) {		\
	.class = PCI_CLASS_DISPLAY << 16,	\
	.class_mask = 0xff0000,			\
	.vendor = 0x8086,			\
	.device = 0x16a,			\
	.subvendor = 0x152d,			\
	.subdevice = 0x8990,			\
	.driver_data = (unsigned long) info }

a438 1
	INTEL_IVB_Q_IDS(&intel_ivybridge_q_info), /* must be first IVB */ \
@


1.90
log
@Update drm_irq.c to the version from Linux 3.14.52.
Disable the DRM_IOCTL_IRQ_BUSID and DRM_IOCTL_CONTROL ioctls.
These are legacy ioctls for DRI1 support, which we no longer support on
OpenBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.89 2015/09/25 16:15:19 jsg Exp $ */
d201 1
d1181 8
a1188 8
	struct inteldrm_softc	*dev_priv = (struct inteldrm_softc *)self;
	struct vga_pci_softc	*vga_sc = (struct vga_pci_softc *)parent;
	struct pci_attach_args	*pa = aux;
	struct vga_pci_bar	*bar;
	struct drm_device	*dev;
	const struct drm_pcidev	*id_entry;
	int			 i;
	uint16_t		 pci_device;
d1192 1
d1194 1
a1194 4
	id_entry = drm_find_description(PCI_VENDOR(pa->pa_id),
	    PCI_PRODUCT(pa->pa_id), pciidlist);
	pci_device = PCI_PRODUCT(pa->pa_id);
	info = i915_get_device_id(pci_device);
a1214 2
	printf("%s", dev_priv->sc_dev.dv_xname);

a1231 1
#endif
a1240 1
#ifdef __linux__
d1263 3
a1265 2
		printf(": can't get BAR info\n");
		return;
d1270 3
a1272 2
		printf(": can't map mmio space\n");
		return;
a1356 1
#ifdef __linux__
a1362 8
#else
	dev_priv->wq = (struct workqueue_struct *)
	    taskq_create("intelrel", 1, IPL_TTY, 0);
	if (dev_priv->wq == NULL) {
		printf(": couldn't create taskq\n");
		goto out_mtrrfree;
	}
#endif
d1392 3
a1394 2
		printf(": couldn't map interrupt\n");
		return;
d1397 2
a1398 1
	printf(": %s", pci_intr_string(dev_priv->pc, dev_priv->ih));
d1403 3
a1405 2
		printf(": couldn't  establish interrupt\n");
		return;
d1448 1
a1448 6
#if 1
{
	extern int wsdisplay_console_initted;
	struct wsemuldisplaydev_attach_args aa;
	struct rasops_info *ri = &dev_priv->ro;

d1452 3
a1484 2
	printf(", %dx%d\n", ri->ri_width, ri->ri_height);

d1487 1
a1487 2
}
#endif
d1490 2
d1493 18
d1512 4
d1517 8
d1642 1
a1642 1
	printf(": no ifp ");
d1679 1
a1679 1
	printf(": no ifp ");
d1692 2
a1693 1
		printf(": can't find GMCH\n");
@


1.89
log
@3.14 backports of some Broadwell fixes from
http://lists.freedesktop.org/archives/intel-gfx/2014-March/042121.html

Ben Widawsky
drm/i915/bdw: Restore PPAT on thaw
a2319c08bfd849ea32b4f890ce92df86074c5731

Ville Syrjala
drm/i915: We implement WaDisableAsyncFlipPerfMode:bdw
8285222c487b61c48b9b955b82598544c3c06050

Ben Widawsky
drm/i915/bdw: Use scratch page table for GEN8 PPGTT
8407bb9129da95fc4099b84cdbbc23e6d4f66aee

Jani Nikula
drm/i915: don't flood the logs about bdw semaphores
c923facd535b97972b5bb7d3df4fcafd61a63a5e

Ville Syrjala
drm/i915: Implement WaDisableSDEUnitClockGating:bdw
4f1ca9e94057de098d65bc7477e8f89dd51609aa

Ville Syrjala
drm/i915: Don't clobber CHICKEN_PIPESL_1 on BDW
c7c656226842679bcd9f39dc24441b4ff398a850

Kenneth Graunke
drm/i915: Add a partial instruction shootdown workaround on Broadwell.
c8966e1058e1e8ae2eec4211157847032829697a

Damien Lespiau
drm/i915/bdw: The TLB invalidation mechanism has been removed from INSTPM
dc616b89dbc4bb6a99884d214bd1ed1e0eef59a0

Kenneth Graunke
drm/i915: Add thread stall DOP clock gating workaround on Broadwell.
1411e6a57a1836ba8a3d4f17c8733b2fbaf0f005

Ville Syrjala
drm/i915: Disable semaphore wait event idle message on BDW
295e8bb73a4785b65db6655fbf6ad57c4177b551

Mika Kuoppala
drm/i915: Do forcewake reset on gen8
0a089e3355d77f758e46db54a0a81d4b58a28cc3

Mika Kuoppala
drm/i915: Fix forcewake counts for gen8
e9dbd2b20201b49b04476d2e5763faa822967913

ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.88 2015/09/25 16:05:59 kettenis Exp $ */
d489 1
a489 1
				    DRIVER_MTRR | DRIVER_IRQ | DRIVER_GEM |
@


1.88
log
@Disable IPS for now.  Identified by jsg@@ as the cause for the unsynched
display after waking it from standby or after VT switches.  Can't seem to get
him to commit this workaround while he his hunting for a proper fix.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.87 2015/09/24 21:31:22 kettenis Exp $ */
d592 1
a592 2
	if (IS_GEN8(dev)) {
		WARN_ON(!i915_preliminary_hw_support);
a593 1
	}
@


1.87
log
@Enable MSIs on hardware that supports it.  On the Thinkpad X1, the APIC pin
is shared with the disk, which results in noticable latency during heavy
disk activity.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.86 2015/09/23 23:12:11 kettenis Exp $ */
d161 1
a161 1
int i915_enable_ips __read_mostly = 1;
@


1.86
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.85 2015/06/26 15:22:23 kettenis Exp $ */
d1218 2
d1404 2
a1405 1
	if (pci_intr_map(pa, &dev_priv->ih) != 0) {
d1410 2
a1411 4
	/*
	 * set up interrupt handler, note that we don't switch the interrupt
	 * on until the X server talks to us, kms will change this.
	 */
d1498 1
a1498 1
	printf("%s: %dx%d\n", dev_priv->sc_dev.dv_xname, ri->ri_width, ri->ri_height);
@


1.85
log
@Add Linux completion API and use it.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.84 2015/06/24 08:32:39 kettenis Exp $ */
d17 2
a18 2
/*-
 * Copyright © 2008 Intel Corporation
a19 1
 * copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.
d23 6
a28 5
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
d30 3
a32 3
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
d34 7
a40 11
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *    Gareth Hughes <gareth@@valinux.com>
 *    Eric Anholt <eric@@anholt.net>
d47 1
a53 9
#include <sys/queue.h>
#include <sys/task.h>
#if 0
#	define INTELDRM_WATCH_COHERENCY
#	define WATCH_INACTIVE
#endif

extern struct mutex mchdev_lock;

d88 1
a88 1
module_param_named(semaphores, i915_semaphores, int, 0600);
d143 1
a143 1
module_param_named(i915_enable_ppgtt, i915_enable_ppgtt, int, 0600);
d147 5
a151 1
unsigned int i915_preliminary_hw_support __read_mostly = 0;
d154 28
a181 3
		"Enable preliminary hardware support. "
		"Enable Haswell and ValleyView Support. "
		"(default: false)");
d189 1
d197 5
d203 1
d213 10
d226 1
d232 1
d239 2
d246 1
d252 1
d259 2
d265 1
d272 2
d280 1
d288 1
d295 1
d301 1
a301 1
	.has_bsd_ring = 1,
d309 1
a309 1
	.has_bsd_ring = 1,
d321 1
a321 1
	.has_bsd_ring = 1,
d328 1
a328 1
	.has_bsd_ring = 1,
d334 2
a335 2
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
a336 1
	.has_force_wake = 1,
d343 1
a343 2
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
a344 1
	.has_force_wake = 1,
d347 7
d355 2
a356 6
	.is_ivybridge = 1, .gen = 7, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
	.has_llc = 1,
	.has_force_wake = 1,
d360 9
a368 7
	.is_ivybridge = 1, .gen = 7, .is_mobile = 1, .num_pipes = 3,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_fbc = 0,	/* FBC is not enabled on Ivybridge mobile yet */
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
	.has_llc = 1,
	.has_force_wake = 1,
d372 3
a374 5
	.gen = 7, .is_mobile = 1, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_fbc = 0,
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
d376 3
d382 2
a383 5
	.gen = 7, .num_pipes = 2,
	.need_gfx_hws = 1, .has_hotplug = 1,
	.has_fbc = 0,
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
d385 3
d391 18
a408 1
	.is_haswell = 1, .gen = 7, .num_pipes = 3,
d410 1
a410 2
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
d412 1
a412 1
	.has_force_wake = 1,
d415 2
a416 2
static const struct intel_device_info intel_haswell_m_info = {
	.is_haswell = 1, .gen = 7, .is_mobile = 1, .num_pipes = 3,
d418 1
a418 2
	.has_bsd_ring = 1,
	.has_blt_ring = 1,
d420 1
a420 1
	.has_force_wake = 1,
d423 37
a459 105
static const struct drm_pcidev inteldrm_pciidlist[] = {		/* aka */
	INTEL_VGA_DEVICE(0x3577, &intel_i830_info),		/* I830_M */
	INTEL_VGA_DEVICE(0x2562, &intel_845g_info),		/* 845_G */
	INTEL_VGA_DEVICE(0x3582, &intel_i85x_info),		/* I855_GM */
	INTEL_VGA_DEVICE(0x358e, &intel_i85x_info),
	INTEL_VGA_DEVICE(0x2572, &intel_i865g_info),		/* I865_G */
	INTEL_VGA_DEVICE(0x2582, &intel_i915g_info),		/* I915_G */
	INTEL_VGA_DEVICE(0x258a, &intel_i915g_info),		/* E7221_G */
	INTEL_VGA_DEVICE(0x2592, &intel_i915gm_info),		/* I915_GM */
	INTEL_VGA_DEVICE(0x2772, &intel_i945g_info),		/* I945_G */
	INTEL_VGA_DEVICE(0x27a2, &intel_i945gm_info),		/* I945_GM */
	INTEL_VGA_DEVICE(0x27ae, &intel_i945gm_info),		/* I945_GME */
	INTEL_VGA_DEVICE(0x2972, &intel_i965g_info),		/* I946_GZ */
	INTEL_VGA_DEVICE(0x2982, &intel_i965g_info),		/* G35_G */
	INTEL_VGA_DEVICE(0x2992, &intel_i965g_info),		/* I965_Q */
	INTEL_VGA_DEVICE(0x29a2, &intel_i965g_info),		/* I965_G */
	INTEL_VGA_DEVICE(0x29b2, &intel_g33_info),		/* Q35_G */
	INTEL_VGA_DEVICE(0x29c2, &intel_g33_info),		/* G33_G */
	INTEL_VGA_DEVICE(0x29d2, &intel_g33_info),		/* Q33_G */
	INTEL_VGA_DEVICE(0x2a02, &intel_i965gm_info),		/* I965_GM */
	INTEL_VGA_DEVICE(0x2a12, &intel_i965gm_info),		/* I965_GME */
	INTEL_VGA_DEVICE(0x2a42, &intel_gm45_info),		/* GM45_G */
	INTEL_VGA_DEVICE(0x2e02, &intel_g45_info),		/* IGD_E_G */
	INTEL_VGA_DEVICE(0x2e12, &intel_g45_info),		/* Q45_G */
	INTEL_VGA_DEVICE(0x2e22, &intel_g45_info),		/* G45_G */
	INTEL_VGA_DEVICE(0x2e32, &intel_g45_info),		/* G41_G */
	INTEL_VGA_DEVICE(0x2e42, &intel_g45_info),		/* B43_G */
	INTEL_VGA_DEVICE(0x2e92, &intel_g45_info),		/* B43_G.1 */
	INTEL_VGA_DEVICE(0xa001, &intel_pineview_info),
	INTEL_VGA_DEVICE(0xa011, &intel_pineview_info),
	INTEL_VGA_DEVICE(0x0042, &intel_ironlake_d_info),
	INTEL_VGA_DEVICE(0x0046, &intel_ironlake_m_info),
	INTEL_VGA_DEVICE(0x0102, &intel_sandybridge_d_info),
	INTEL_VGA_DEVICE(0x0112, &intel_sandybridge_d_info),
	INTEL_VGA_DEVICE(0x0122, &intel_sandybridge_d_info),
	INTEL_VGA_DEVICE(0x0106, &intel_sandybridge_m_info),
	INTEL_VGA_DEVICE(0x0116, &intel_sandybridge_m_info),
	INTEL_VGA_DEVICE(0x0126, &intel_sandybridge_m_info),
	INTEL_VGA_DEVICE(0x010A, &intel_sandybridge_d_info),
	INTEL_VGA_DEVICE(0x0156, &intel_ivybridge_m_info), /* GT1 mobile */
	INTEL_VGA_DEVICE(0x0166, &intel_ivybridge_m_info), /* GT2 mobile */
	INTEL_VGA_DEVICE(0x0152, &intel_ivybridge_d_info), /* GT1 desktop */
	INTEL_VGA_DEVICE(0x0162, &intel_ivybridge_d_info), /* GT2 desktop */
	INTEL_VGA_DEVICE(0x015a, &intel_ivybridge_d_info), /* GT1 server */
	INTEL_VGA_DEVICE(0x016a, &intel_ivybridge_d_info), /* GT2 server */
	INTEL_VGA_DEVICE(0x0402, &intel_haswell_d_info), /* GT1 desktop */
	INTEL_VGA_DEVICE(0x0412, &intel_haswell_d_info), /* GT2 desktop */
	INTEL_VGA_DEVICE(0x0422, &intel_haswell_d_info), /* GT3 desktop */
	INTEL_VGA_DEVICE(0x040a, &intel_haswell_d_info), /* GT1 server */
	INTEL_VGA_DEVICE(0x041a, &intel_haswell_d_info), /* GT2 server */
	INTEL_VGA_DEVICE(0x042a, &intel_haswell_d_info), /* GT3 server */
	INTEL_VGA_DEVICE(0x0406, &intel_haswell_m_info), /* GT1 mobile */
	INTEL_VGA_DEVICE(0x0416, &intel_haswell_m_info), /* GT2 mobile */
	INTEL_VGA_DEVICE(0x0426, &intel_haswell_m_info), /* GT2 mobile */
	INTEL_VGA_DEVICE(0x040B, &intel_haswell_d_info), /* GT1 reserved */
	INTEL_VGA_DEVICE(0x041B, &intel_haswell_d_info), /* GT2 reserved */
	INTEL_VGA_DEVICE(0x042B, &intel_haswell_d_info), /* GT3 reserved */
	INTEL_VGA_DEVICE(0x040E, &intel_haswell_d_info), /* GT1 reserved */
	INTEL_VGA_DEVICE(0x041E, &intel_haswell_d_info), /* GT2 reserved */
	INTEL_VGA_DEVICE(0x042E, &intel_haswell_d_info), /* GT3 reserved */
	INTEL_VGA_DEVICE(0x0C02, &intel_haswell_d_info), /* SDV GT1 desktop */
	INTEL_VGA_DEVICE(0x0C12, &intel_haswell_d_info), /* SDV GT2 desktop */
	INTEL_VGA_DEVICE(0x0C22, &intel_haswell_d_info), /* SDV GT3 desktop */
	INTEL_VGA_DEVICE(0x0C0A, &intel_haswell_d_info), /* SDV GT1 server */
	INTEL_VGA_DEVICE(0x0C1A, &intel_haswell_d_info), /* SDV GT2 server */
	INTEL_VGA_DEVICE(0x0C2A, &intel_haswell_d_info), /* SDV GT3 server */
	INTEL_VGA_DEVICE(0x0C06, &intel_haswell_m_info), /* SDV GT1 mobile */
	INTEL_VGA_DEVICE(0x0C16, &intel_haswell_m_info), /* SDV GT2 mobile */
	INTEL_VGA_DEVICE(0x0C26, &intel_haswell_m_info), /* SDV GT3 mobile */
	INTEL_VGA_DEVICE(0x0C0B, &intel_haswell_d_info), /* SDV GT1 reserved */
	INTEL_VGA_DEVICE(0x0C1B, &intel_haswell_d_info), /* SDV GT2 reserved */
	INTEL_VGA_DEVICE(0x0C2B, &intel_haswell_d_info), /* SDV GT3 reserved */
	INTEL_VGA_DEVICE(0x0C0E, &intel_haswell_d_info), /* SDV GT1 reserved */
	INTEL_VGA_DEVICE(0x0C1E, &intel_haswell_d_info), /* SDV GT2 reserved */
	INTEL_VGA_DEVICE(0x0C2E, &intel_haswell_d_info), /* SDV GT3 reserved */
	INTEL_VGA_DEVICE(0x0A02, &intel_haswell_d_info), /* ULT GT1 desktop */
	INTEL_VGA_DEVICE(0x0A12, &intel_haswell_d_info), /* ULT GT2 desktop */
	INTEL_VGA_DEVICE(0x0A22, &intel_haswell_d_info), /* ULT GT3 desktop */
	INTEL_VGA_DEVICE(0x0A0A, &intel_haswell_d_info), /* ULT GT1 server */
	INTEL_VGA_DEVICE(0x0A1A, &intel_haswell_d_info), /* ULT GT2 server */
	INTEL_VGA_DEVICE(0x0A2A, &intel_haswell_d_info), /* ULT GT3 server */
	INTEL_VGA_DEVICE(0x0A06, &intel_haswell_m_info), /* ULT GT1 mobile */
	INTEL_VGA_DEVICE(0x0A16, &intel_haswell_m_info), /* ULT GT2 mobile */
	INTEL_VGA_DEVICE(0x0A26, &intel_haswell_m_info), /* ULT GT3 mobile */
	INTEL_VGA_DEVICE(0x0A0B, &intel_haswell_d_info), /* ULT GT1 reserved */
	INTEL_VGA_DEVICE(0x0A1B, &intel_haswell_d_info), /* ULT GT2 reserved */
	INTEL_VGA_DEVICE(0x0A2B, &intel_haswell_d_info), /* ULT GT3 reserved */
	INTEL_VGA_DEVICE(0x0A0E, &intel_haswell_m_info), /* ULT GT1 reserved */
	INTEL_VGA_DEVICE(0x0A1E, &intel_haswell_m_info), /* ULT GT2 reserved */
	INTEL_VGA_DEVICE(0x0A2E, &intel_haswell_m_info), /* ULT GT3 reserved */
	INTEL_VGA_DEVICE(0x0D02, &intel_haswell_d_info), /* CRW GT1 desktop */
	INTEL_VGA_DEVICE(0x0D12, &intel_haswell_d_info), /* CRW GT2 desktop */
	INTEL_VGA_DEVICE(0x0D22, &intel_haswell_d_info), /* CRW GT3 desktop */
	INTEL_VGA_DEVICE(0x0D0A, &intel_haswell_d_info), /* CRW GT1 server */
	INTEL_VGA_DEVICE(0x0D1A, &intel_haswell_d_info), /* CRW GT2 server */
	INTEL_VGA_DEVICE(0x0D2A, &intel_haswell_d_info), /* CRW GT3 server */
	INTEL_VGA_DEVICE(0x0D06, &intel_haswell_m_info), /* CRW GT1 mobile */
	INTEL_VGA_DEVICE(0x0D16, &intel_haswell_m_info), /* CRW GT2 mobile */
	INTEL_VGA_DEVICE(0x0D26, &intel_haswell_m_info), /* CRW GT3 mobile */
	INTEL_VGA_DEVICE(0x0D0B, &intel_haswell_d_info), /* CRW GT1 reserved */
	INTEL_VGA_DEVICE(0x0D1B, &intel_haswell_d_info), /* CRW GT2 reserved */
	INTEL_VGA_DEVICE(0x0D2B, &intel_haswell_d_info), /* CRW GT3 reserved */
	INTEL_VGA_DEVICE(0x0D0E, &intel_haswell_d_info), /* CRW GT1 reserved */
	INTEL_VGA_DEVICE(0x0D1E, &intel_haswell_d_info), /* CRW GT2 reserved */
	INTEL_VGA_DEVICE(0x0D2E, &intel_haswell_d_info), /* CRW GT3 reserved */
d466 4
a469 3
	.open 			= i915_driver_open,
	.close			= i915_driver_close,
	.lastclose		= i915_driver_lastclose,
d471 1
a471 1
	.gem_init_object	= i915_gem_init_object,
d476 11
a486 11
	.dumb_create		= i915_gem_dumb_create,
	.dumb_map_offset	= i915_gem_mmap_gtt,
	.dumb_destroy		= i915_gem_dumb_destroy,

	.ioctls			= i915_ioctls,
	.name			= DRIVER_NAME,
	.desc			= DRIVER_DESC,
	.date			= DRIVER_DATE,
	.major			= DRIVER_MAJOR,
	.minor			= DRIVER_MINOR,
	.patchlevel		= DRIVER_PATCHLEVEL,
d498 1
a498 1
	for (did = &inteldrm_pciidlist[0]; did->device != 0; did++) {
d509 75
a583 2
	return (drm_pciprobe((struct pci_attach_args *)aux,
	    inteldrm_pciidlist));
d589 7
a595 1
		return 0;
d606 1
a606 1
	return 1;
d612 13
d628 1
a628 1
#if 0
d634 3
a636 1
		int error = i915_gem_idle(dev);
d645 10
a654 1
		intel_modeset_disable(dev);
d656 1
a656 1
		drm_irq_uninstall(dev);
d659 2
d665 83
a747 2
	/* Modeset on resume, not lid events */
	dev_priv->modeset_on_lid = 0;
d752 1
a752 1
static int __i915_drm_thaw(struct drm_device *dev)
d757 7
a771 1
		dev_priv->mm.suspended = 0;
d776 3
d780 2
d783 13
a795 2
		intel_modeset_setup_hw_state(dev, false);
		drm_irq_install(dev);
d800 21
a820 1
	dev_priv->modeset_on_lid = 0;
d822 1
d828 33
a860 1
	int error = 0;
d862 8
a869 1
	intel_gt_sanitize(dev);
d871 3
a873 5
	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
		mutex_lock(&dev->struct_mutex);
		i915_gem_restore_gtt_mappings(dev);
		mutex_unlock(&dev->struct_mutex);
	}
d875 4
a878 1
	__i915_drm_thaw(dev);
d880 1
a880 1
	return error;
d882 1
d884 14
a897 3
/*
 * We're intel IGD, bus 0 function 0 dev 0 should be the GMCH, so it should
 * be Intel
d899 1
a899 2
int
inteldrm_gmch_match(struct pci_attach_args *pa)
d901 65
a965 6
	if (pa->pa_bus == 0 && pa->pa_device == 0 && pa->pa_function == 0 &&
	    PCI_VENDOR(pa->pa_id) == PCI_VENDOR_INTEL &&
	    PCI_CLASS(pa->pa_class) == PCI_CLASS_BRIDGE &&
	    PCI_SUBCLASS(pa->pa_class) == PCI_SUBCLASS_BRIDGE_HOST)
		return (1);
	return (0);
a1019 1
	extern u32 _intel_panel_get_max_backlight(struct drm_device *);
d1029 3
d1035 2
a1036 2
			dp->max = _intel_panel_get_max_backlight(dev);
			dp->curval = dev_priv->backlight_level;
d1044 4
d1050 4
a1053 1
			intel_panel_set_backlight(dev, dp->curval);
d1118 1
a1118 1
	intel_fb_restore_mode(dev);
a1169 6
/*
 * Accelerated routines.
 */

int inteldrm_copyrows(void *, int, int, int);

d1171 1
a1171 1
inteldrm_copyrows(void *cookie, int src, int dst, int num)
d1173 2
a1174 2
	struct rasops_info *ri = cookie;
	struct inteldrm_softc *sc = ri->ri_hw;
d1176 2
a1177 7
	if ((dst == 0 && (src + num) == ri->ri_rows) ||
	    (src == 0 && (dst + num) == ri->ri_rows)) {
		struct inteldrm_softc *dev_priv = sc;
		struct drm_fb_helper *helper = &dev_priv->fbdev->helper;
		size_t size = dev_priv->fbdev->ifb.obj->base.size / 2;
		int stride = ri->ri_font->fontheight * ri->ri_stride;
		int i;
d1179 2
a1180 51
		if (dst == 0) {
			int delta = src * stride;
			bzero(ri->ri_bits, delta);

			sc->sc_offset += delta;
			ri->ri_bits += delta;
			ri->ri_origbits += delta;
			if (sc->sc_offset > size) {
				sc->sc_offset -= size;
				ri->ri_bits -= size;
				ri->ri_origbits -= size;
			}
		} else {
			int delta = dst * stride;
			bzero(ri->ri_bits + num * stride, delta);

			sc->sc_offset -= delta;
			ri->ri_bits -= delta;
			ri->ri_origbits -= delta;
			if (sc->sc_offset < 0) {
				sc->sc_offset += size;
				ri->ri_bits += size;
				ri->ri_origbits += size;
			}
		}

		for (i = 0; i < helper->crtc_count; i++) {
			struct drm_mode_set *mode_set =
			    &helper->crtc_info[i].mode_set;
			struct drm_crtc *crtc = mode_set->crtc;
			struct drm_framebuffer *fb = helper->fb;

			if (!crtc->enabled)
				continue;

			mode_set->x = (sc->sc_offset % ri->ri_stride) /
			    (ri->ri_depth / 8);
			mode_set->y = sc->sc_offset / ri->ri_stride;
			if (fb == crtc->fb)
				dev_priv->display.update_plane(crtc, fb,
				    mode_set->x, mode_set->y);
		}

		return 0;
	}

	return sc->sc_copyrows(cookie, src, dst, num);
}

void
inteldrm_attach(struct device *parent, struct device *self, void *aux)
d1184 1
a1184 1
	struct pci_attach_args	*pa = aux, bpa;
d1190 3
a1192 1
	uint32_t		 aperture_size;
d1195 1
a1195 1
	    PCI_PRODUCT(pa->pa_id), inteldrm_pciidlist);
d1197 4
a1200 2
	dev_priv->info = i915_get_device_id(pci_device);
	KASSERT(dev_priv->info->gen != 0);
d1218 3
d1222 29
a1250 5
	mtx_init(&dev_priv->rps.lock, IPL_TTY);
	mtx_init(&dev_priv->dpio_lock, IPL_TTY);
	mtx_init(&dev_priv->gt_lock, IPL_TTY);
	mtx_init(&mchdev_lock, IPL_TTY);
	rw_init(&dev_priv->rps.hw_lock, "rpshw");
d1252 12
a1263 1
	task_set(&dev_priv->switchtask, inteldrm_doswitch, dev_priv);
d1266 1
a1266 1
	bar = vga_pci_bar_info(vga_sc, (IS_I9XX(dev) ? 0 : 1));
d1272 1
a1272 1
	dev_priv->regs = vga_pci_bar_map(vga_sc, bar->addr, 0, 0);
d1278 3
d1283 51
d1335 63
a1397 2
	 * i945G/GM report MSI capability despite not actually supporting it.
	 * so explicitly disable it.
a1406 4
	i915_gem_gtt_init(dev);

	intel_irq_init(dev);

d1411 2
a1412 3
	dev_priv->irqh = pci_intr_establish(dev_priv->pc, dev_priv->ih, IPL_TTY,
	    inteldrm_driver.irq_handler,
	    dev_priv, dev_priv->sc_dev.dv_xname);
d1418 8
a1425 5
	dev_priv->wq = (struct workqueue_struct *)
	    taskq_create("intelrel", 1, IPL_TTY, 0);
	if (dev_priv->wq == NULL) {
		printf("couldn't create taskq\n");
		return;
d1428 1
a1428 4
	/* GEM init */
	timeout_set(&dev_priv->hangcheck_timer, i915_hangcheck_elapsed, dev_priv);
	dev_priv->next_seqno = 1;
	dev_priv->mm.suspended = 1;
d1430 6
a1435 11
	if (pci_find_device(&bpa, inteldrm_gmch_match) == 0) {
		printf(": can't find GMCH\n");
		return;
	}

	/* Set up the IFP for chipset flushing */
	if (IS_I915G(dev) || IS_I915GM(dev) || IS_I945G(dev) ||
	    IS_I945GM(dev)) {
		i915_alloc_ifp(dev_priv, &bpa);
	} else if (INTEL_INFO(dev)->gen >= 4 || IS_G33(dev)) {
		i965_alloc_ifp(dev_priv, &bpa);
d1437 2
a1438 15
		int nsegs;
		/*
		 * I8XX has no flush page mechanism, we fake it by writing until
		 * the cache is empty. allocate a page to scribble on
		 */
		dev_priv->ifp.i8xx.kva = NULL;
		if (bus_dmamem_alloc(pa->pa_dmat, PAGE_SIZE, 0, 0,
		    &dev_priv->ifp.i8xx.seg, 1, &nsegs, BUS_DMA_WAITOK) == 0) {
			if (bus_dmamem_map(pa->pa_dmat, &dev_priv->ifp.i8xx.seg,
			    1, PAGE_SIZE, &dev_priv->ifp.i8xx.kva, 0) != 0) {
				bus_dmamem_free(pa->pa_dmat,
				    &dev_priv->ifp.i8xx.seg, nsegs);
				dev_priv->ifp.i8xx.kva = NULL;
			}
		}
d1441 3
a1443 2
        /* Try to make sure MCHBAR is enabled before poking at it */
        intel_setup_mchbar(dev_priv, &bpa);
d1445 6
a1450 5
	i915_gem_load(dev);

	if (drm_vblank_init(dev, INTEL_INFO(dev)->num_pipes)) {
		printf(": vblank init failed\n");
		return;
d1453 2
a1454 2
	aperture_size = dev_priv->mm.gtt->gtt_mappable_entries << PAGE_SHIFT;
	dev_priv->mm.gtt_base_addr = dev_priv->mm.gtt->gma_bus_addr;
d1456 1
a1456 33
	intel_pm_init(dev);
	intel_gt_sanitize(dev);
	intel_gt_init(dev);

	intel_opregion_setup(dev);
	intel_setup_bios(dev);
	intel_setup_gmbus(dev);

	/* XXX would be a lot nicer to get agp info before now */
	uvm_page_physload(atop(dev_priv->mm.gtt_base_addr),
	    atop(dev_priv->mm.gtt_base_addr + aperture_size),
	    atop(dev_priv->mm.gtt_base_addr),
	    atop(dev_priv->mm.gtt_base_addr + aperture_size),
	    PHYSLOAD_DEVICE);
	/* array of vm pages that physload introduced. */
	dev_priv->pgs = PHYS_TO_VM_PAGE(dev_priv->mm.gtt_base_addr);
	KASSERT(dev_priv->pgs != NULL);
	/*
	 * XXX mark all pages write combining so user mmaps get the right
	 * bits. We really need a proper MI api for doing this, but for now
	 * this allows us to use PAT where available.
	 */
	for (i = 0; i < atop(aperture_size); i++)
		atomic_setbits_int(&(dev_priv->pgs[i].pg_flags), PG_PMAP_WC);
	if (agp_init_map(dev_priv->bst, dev_priv->mm.gtt_base_addr,
	    aperture_size, BUS_SPACE_MAP_LINEAR | BUS_SPACE_MAP_PREFETCHABLE,
	    &dev_priv->agph))
		panic("can't map aperture");

	/* XXX */
	if (drm_core_check_feature(dev, DRIVER_MODESET))
		i915_load_modeset_init(dev);
	intel_opregion_init(dev);
d1467 1
a1467 1
	intel_fb_restore_mode(dev);
d1469 2
a1470 1
	ri->ri_flg = RI_CENTER | RI_VCONS;
d1473 1
a1473 14
	ri->ri_hw = dev_priv;
	dev_priv->sc_copyrows = ri->ri_copyrows;
	ri->ri_copyrows = inteldrm_copyrows;

	/*
	 * On older hardware the fast scrolling code causes page table
	 * errors.  As a workaround, we set the "avoid framebuffer
	 * reads" flag, which has the side-effect of disabling the
	 * fast scrolling code, but still gives us a half-decent
	 * scrolling speed.
	 */
	if (INTEL_INFO(dev)->gen < 3 || IS_I915G(dev) || IS_I915GM(dev))
		ri->ri_flg |= RI_WRONLY;
	ri->ri_flg |= RI_WRONLY;
d1503 6
d1561 1
d1563 1
a1563 1
		intel_fb_restore_mode(dev);
d1580 15
d1666 40
a1705 1
intel_gtt_chipset_flush(struct drm_device *dev)
d1707 1
a1707 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1713 1
a1713 1
	if (IS_I9XX(dev)) {
d1734 3
a1736 1
static int i8xx_do_reset(struct drm_device *dev)
d1738 1
a1738 26
	struct drm_i915_private *dev_priv = dev->dev_private;

	if (IS_I85X(dev))
		return -ENODEV;

	I915_WRITE(D_STATE, I915_READ(D_STATE) | DSTATE_GFX_RESET_I830);
	POSTING_READ(D_STATE);

	if (IS_I830(dev) || IS_845G(dev)) {
		I915_WRITE(DEBUG_RESET_I830,
			   DEBUG_RESET_DISPLAY |
			   DEBUG_RESET_RENDER |
			   DEBUG_RESET_FULL);
		POSTING_READ(DEBUG_RESET_I830);
		drm_msleep(1, "8res1");

		I915_WRITE(DEBUG_RESET_I830, 0);
		POSTING_READ(DEBUG_RESET_I830);
	}

	drm_msleep(1, "8res2");

	I915_WRITE(D_STATE, I915_READ(D_STATE) & ~DSTATE_GFX_RESET_I830);
	POSTING_READ(D_STATE);

	return 0;
d1741 3
a1743 1
static int i965_reset_complete(struct drm_device *dev)
d1745 7
a1751 219
	struct drm_i915_private *dev_priv = dev->dev_private;
	u8 gdrst;
	gdrst = (pci_conf_read(dev_priv->pc, dev_priv->tag, I965_GDRST) >> 24);
	return (gdrst & GRDOM_RESET_ENABLE) == 0;
}

static int i965_do_reset(struct drm_device *dev)
{
	int ret;
	struct drm_i915_private *dev_priv = dev->dev_private;
	pcireg_t reg;

	/*
	 * Set the domains we want to reset (GRDOM/bits 2 and 3) as
	 * well as the reset bit (GR/bit 0).  Setting the GR bit
	 * triggers the reset; when done, the hardware will clear it.
	 */
	reg = pci_conf_read(dev_priv->pc, dev_priv->tag, I965_GDRST);
	reg |= ((GRDOM_RENDER | GRDOM_RESET_ENABLE) << 24);
	pci_conf_write(dev_priv->pc, dev_priv->tag, I965_GDRST, reg);

	ret =  wait_for(i965_reset_complete(dev), 500);
	if (ret)
		return ret;

	/* We can't reset render&media without also resetting display ... */
	reg = pci_conf_read(dev_priv->pc, dev_priv->tag, I965_GDRST);
	reg |= ((GRDOM_MEDIA | GRDOM_RESET_ENABLE) << 24);
	pci_conf_write(dev_priv->pc, dev_priv->tag, I965_GDRST, reg);

	return wait_for(i965_reset_complete(dev), 500);
}

static int ironlake_do_reset(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	u32 gdrst;
	int ret;

	gdrst = I915_READ(MCHBAR_MIRROR_BASE + ILK_GDSR);
	I915_WRITE(MCHBAR_MIRROR_BASE + ILK_GDSR,
		   gdrst | GRDOM_RENDER | GRDOM_RESET_ENABLE);
	ret = wait_for(I915_READ(MCHBAR_MIRROR_BASE + ILK_GDSR) & 0x1, 500);
	if (ret)
		return ret;

	/* We can't reset render&media without also resetting display ... */
	gdrst = I915_READ(MCHBAR_MIRROR_BASE + ILK_GDSR);
	I915_WRITE(MCHBAR_MIRROR_BASE + ILK_GDSR,
		   gdrst | GRDOM_MEDIA | GRDOM_RESET_ENABLE);
	return wait_for(I915_READ(MCHBAR_MIRROR_BASE + ILK_GDSR) & 0x1, 500);
}

static int gen6_do_reset(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret = 0;
	unsigned long irqflags;

	/* Hold gt_lock across reset to prevent any register access
	 * with forcewake not set correctly
	 */
	spin_lock_irqsave(&dev_priv->gt_lock, irqflags);

	/* Reset the chip */

	/* GEN6_GDRST is not in the gt power well, no need to check
	 * for fifo space for the write or forcewake the chip for
	 * the read
	 */
	I915_WRITE_NOTRACE(GEN6_GDRST, GEN6_GRDOM_FULL);

	/* Spin waiting for the device to ack the reset request */
	ret = wait_for((I915_READ_NOTRACE(GEN6_GDRST) & GEN6_GRDOM_FULL) == 0, 500);

	/* If reset with a user forcewake, try to restore, otherwise turn it off */
	if (dev_priv->forcewake_count)
		dev_priv->gt.force_wake_get(dev_priv);
	else
		dev_priv->gt.force_wake_put(dev_priv);

	/* Restore fifo count */
	dev_priv->gt_fifo_count = I915_READ_NOTRACE(GT_FIFO_FREE_ENTRIES);

	spin_unlock_irqrestore(&dev_priv->gt_lock, irqflags);
	return ret;
}

int intel_gpu_reset(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret = -ENODEV;

	switch (INTEL_INFO(dev)->gen) {
	case 7:
	case 6:
		ret = gen6_do_reset(dev);
		break;
	case 5:
		ret = ironlake_do_reset(dev);
		break;
	case 4:
		ret = i965_do_reset(dev);
		break;
	case 2:
		ret = i8xx_do_reset(dev);
		break;
	}

	/* Also reset the gpu hangman. */
	if (dev_priv->stop_rings) {
		DRM_DEBUG("Simulated gpu hang, resetting stop_rings\n");
		dev_priv->stop_rings = 0;
		if (ret == -ENODEV) {
			DRM_ERROR("Reset not implemented, but ignoring "
				  "error for simulated gpu hangs\n");
			ret = 0;
		}
	}

	return ret;
}

/**
 * i915_reset - reset chip after a hang
 * @@dev: drm device to reset
 *
 * Reset the chip.  Useful if a hang is detected. Returns zero on successful
 * reset or otherwise an error code.
 *
 * Procedure is fairly simple:
 *   - reset the chip using the reset reg
 *   - re-init context state
 *   - re-init hardware status page
 *   - re-init ring buffer
 *   - re-init interrupt state
 *   - re-init display
 */
int i915_reset(struct drm_device *dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	int ret;

	if (!i915_try_reset)
		return 0;

	mutex_lock(&dev->struct_mutex);

	i915_gem_reset(dev);

	ret = -ENODEV;
	if (get_seconds() - dev_priv->last_gpu_reset < 5)
		DRM_ERROR("GPU hanging too fast, declaring wedged!\n");
	else
		ret = intel_gpu_reset(dev);

	dev_priv->last_gpu_reset = get_seconds();
	if (ret) {
		DRM_ERROR("Failed to reset chip.\n");
		mutex_unlock(&dev->struct_mutex);
		return ret;
	}

	/* Ok, now get things going again... */

	/*
	 * Everything depends on having the GTT running, so we need to start
	 * there.  Fortunately we don't need to do this unless we reset the
	 * chip at a PCI level.
	 *
	 * Next we need to restore the context, but we don't use those
	 * yet either...
	 *
	 * Ring buffer needs to be re-initialized in the KMS case, or if X
	 * was running at the time of the reset (i.e. we weren't VT
	 * switched away).
	 */
	if (drm_core_check_feature(dev, DRIVER_MODESET) ||
			!dev_priv->mm.suspended) {
		struct intel_ring_buffer *ring;
		int i;

		dev_priv->mm.suspended = 0;

		i915_gem_init_swizzling(dev);

		for_each_ring(ring, dev_priv, i)
			ring->init(ring);

		i915_gem_context_init(dev);
#ifdef notyet
		i915_gem_init_ppgtt(dev);
#endif

		/*
		 * It would make sense to re-init all the other hw state, at
		 * least the rps/rc6/emon init done within modeset_init_hw. For
		 * some unknown reason, this blows up my ilk, so don't.
		 */

		mutex_unlock(&dev->struct_mutex);

		drm_irq_uninstall(dev);
		drm_irq_install(dev);
	} else {
		mutex_unlock(&dev->struct_mutex);
	}

	return 0;
}

static int
intel_pch_match(struct pci_attach_args *pa)
{
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_INTEL &&
	    PCI_CLASS(pa->pa_class) == PCI_CLASS_BRIDGE &&
	    PCI_SUBCLASS(pa->pa_class) == PCI_SUBCLASS_BRIDGE_ISA)
		return (1);
	return (0);
d1755 1
a1755 146
intel_detect_pch(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct pci_attach_args	pa;
	unsigned short id;
	if (pci_find_device(&pa, intel_pch_match) == 0) {
		DRM_DEBUG_KMS("No Intel PCI-ISA bridge found\n");
	}
	id = PCI_PRODUCT(pa.pa_id) & INTEL_PCH_DEVICE_ID_MASK;
	dev_priv->pch_id = id;

	switch (id) {
	case INTEL_PCH_IBX_DEVICE_ID_TYPE:
		dev_priv->pch_type = PCH_IBX;
		dev_priv->num_pch_pll = 2;
		DRM_DEBUG_KMS("Found Ibex Peak PCH\n");
		break;
	case INTEL_PCH_CPT_DEVICE_ID_TYPE:
		dev_priv->pch_type = PCH_CPT;
		dev_priv->num_pch_pll = 2;
		DRM_DEBUG_KMS("Found CougarPoint PCH\n");
		break;
	case INTEL_PCH_PPT_DEVICE_ID_TYPE:
		/* PantherPoint is CPT compatible */
		dev_priv->pch_type = PCH_CPT;
		dev_priv->num_pch_pll = 2;
		DRM_DEBUG_KMS("Found PatherPoint PCH\n");
		break;
	case INTEL_PCH_LPT_DEVICE_ID_TYPE:
		dev_priv->pch_type = PCH_LPT;
		dev_priv->num_pch_pll = 0;
		DRM_DEBUG_KMS("Found LynxPoint PCH\n");
		break;
	case INTEL_PCH_LPT_LP_DEVICE_ID_TYPE:
		dev_priv->pch_type = PCH_LPT;
		dev_priv->num_pch_pll = 0;
		DRM_DEBUG_KMS("Found LynxPoint LP PCH\n");
		break;
	default:
		DRM_DEBUG_KMS("No PCH detected\n");
	}
}

/* We give fast paths for the really cool registers */
#define NEEDS_FORCE_WAKE(dev, reg) \
	((HAS_FORCE_WAKE(dev)) && \
	 ((reg) < 0x40000) &&            \
	 ((reg) != FORCEWAKE))

static bool IS_DISPLAYREG(u32 reg)
{
	/*
	 * This should make it easier to transition modules over to the
	 * new register block scheme, since we can do it incrementally.
	 */
	if (reg >= VLV_DISPLAY_BASE)
		return false;

	if (reg >= RENDER_RING_BASE &&
	    reg < RENDER_RING_BASE + 0xff)
		return false;
	if (reg >= GEN6_BSD_RING_BASE &&
	    reg < GEN6_BSD_RING_BASE + 0xff)
		return false;
	if (reg >= BLT_RING_BASE &&
	    reg < BLT_RING_BASE + 0xff)
		return false;

	if (reg == PGTBL_ER)
		return false;

	if (reg >= IPEIR_I965 &&
	    reg < HWSTAM)
		return false;

	if (reg == MI_MODE)
		return false;

	if (reg == GFX_MODE_GEN7)
		return false;

	if (reg == RENDER_HWS_PGA_GEN7 ||
	    reg == BSD_HWS_PGA_GEN7 ||
	    reg == BLT_HWS_PGA_GEN7)
		return false;

	if (reg == GEN6_BSD_SLEEP_PSMI_CONTROL ||
	    reg == GEN6_BSD_RNCID)
		return false;

	if (reg == GEN6_BLITTER_ECOSKPD)
		return false;

	if (reg >= 0x4000c &&
	    reg <= 0x4002c)
		return false;

	if (reg >= 0x4f000 &&
	    reg <= 0x4f08f)
		return false;

	if (reg >= 0x4f100 &&
	    reg <= 0x4f11f)
		return false;

	if (reg >= VLV_MASTER_IER &&
	    reg <= GEN6_PMIER)
		return false;

	if (reg >= FENCE_REG_SANDYBRIDGE_0 &&
	    reg < (FENCE_REG_SANDYBRIDGE_0 + (16*8)))
		return false;

	if (reg >= VLV_IIR_RW &&
	    reg <= VLV_ISR)
		return false;

	if (reg == FORCEWAKE_VLV ||
	    reg == FORCEWAKE_ACK_VLV)
		return false;

	if (reg == GEN6_GDRST)
		return false;

	switch (reg) {
	case _3D_CHICKEN3:
	case IVB_CHICKEN3:
	case GEN7_COMMON_SLICE_CHICKEN1:
	case GEN7_L3CNTLREG1:
	case GEN7_L3_CHICKEN_MODE_REGISTER:
	case GEN7_ROW_CHICKEN2:
	case GEN7_L3SQCREG4:
	case GEN7_SQ_CHICKEN_MBCUNIT_CONFIG:
	case GEN7_HALF_SLICE_CHICKEN1:
	case GEN6_MBCTL:
	case GEN6_UCGCTL2:
		return false;
	default:
		break;
	}

	return true;
}

static void
ilk_dummy_write(struct drm_i915_private *dev_priv)
a1756 114
	/* WaIssueDummyWriteToWakeupFromRC6: Issue a dummy write to wake up the
	 * chip from rc6 before touching it for real. MI_MODE is masked, hence
	 * harmless to write 0 into. */
	I915_WRITE_NOTRACE(MI_MODE, 0);
}

#define __i915_read(x, y) \
u##x i915_read##x(struct drm_i915_private *dev_priv, u32 reg) { \
	unsigned long irqflags; \
	u##x val = 0; \
	spin_lock_irqsave(&dev_priv->gt_lock, irqflags); \
	if (IS_GEN5(dev_priv->dev)) \
		ilk_dummy_write(dev_priv); \
	if (NEEDS_FORCE_WAKE((dev_priv->dev), (reg))) { \
		if (dev_priv->forcewake_count == 0) \
			dev_priv->gt.force_wake_get(dev_priv); \
		val = read##x(dev_priv, reg); \
		if (dev_priv->forcewake_count == 0) \
			dev_priv->gt.force_wake_put(dev_priv); \
	} else if (IS_VALLEYVIEW(dev_priv->dev) && IS_DISPLAYREG(reg)) { \
		val = read##x(dev_priv, reg + 0x180000);		\
	} else { \
		val = read##x(dev_priv, reg); \
	} \
	spin_unlock_irqrestore(&dev_priv->gt_lock, irqflags); \
	trace_i915_reg_rw(false, reg, val, sizeof(val)); \
	return val; \
}

__i915_read(8, b)
__i915_read(16, w)
__i915_read(32, l)
__i915_read(64, q)
#undef __i915_read

#define __i915_write(x, y) \
void i915_write##x(struct drm_i915_private *dev_priv, u32 reg, u##x val) { \
	unsigned long irqflags; \
	u32 __fifo_ret = 0; \
	trace_i915_reg_rw(true, reg, val, sizeof(val)); \
	spin_lock_irqsave(&dev_priv->gt_lock, irqflags); \
	if (NEEDS_FORCE_WAKE((dev_priv->dev), (reg))) { \
		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
	} \
	if (IS_GEN5(dev_priv->dev)) \
		ilk_dummy_write(dev_priv); \
	if (IS_HASWELL(dev_priv->dev) && (I915_READ_NOTRACE(GEN7_ERR_INT) & ERR_INT_MMIO_UNCLAIMED)) { \
		DRM_ERROR("Unknown unclaimed register before writing to %x\n", reg); \
		I915_WRITE_NOTRACE(GEN7_ERR_INT, ERR_INT_MMIO_UNCLAIMED); \
	} \
	if (IS_VALLEYVIEW(dev_priv->dev) && IS_DISPLAYREG(reg)) { \
		write##x(dev_priv, reg + 0x180000, val);		\
	} else {							\
		write##x(dev_priv, reg, val);			\
	}								\
	if (unlikely(__fifo_ret)) { \
		gen6_gt_check_fifodbg(dev_priv); \
	} \
	if (IS_HASWELL(dev_priv->dev) && (I915_READ_NOTRACE(GEN7_ERR_INT) & ERR_INT_MMIO_UNCLAIMED)) { \
		DRM_ERROR("Unclaimed write to %x\n", reg); \
		write32(dev_priv, GEN7_ERR_INT, ERR_INT_MMIO_UNCLAIMED);	\
	} \
	spin_unlock_irqrestore(&dev_priv->gt_lock, irqflags); \
}
__i915_write(8, b)
__i915_write(16, w)
__i915_write(32, l)
__i915_write(64, q)
#undef __i915_write

static const struct register_whitelist {
	uint64_t offset;
	uint32_t size;
	uint32_t gen_bitmask; /* support gens, 0x10 for 4, 0x30 for 4 and 5, etc. */
} whitelist[] = {
	{ RING_TIMESTAMP(RENDER_RING_BASE), 8, 0xF0 },
};

int i915_reg_read_ioctl(struct drm_device *dev,
			void *data, struct drm_file *file)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct drm_i915_reg_read *reg = data;
	struct register_whitelist const *entry = whitelist;
	int i;

	for (i = 0; i < ARRAY_SIZE(whitelist); i++, entry++) {
		if (entry->offset == reg->offset &&
		    (1 << INTEL_INFO(dev)->gen & entry->gen_bitmask))
			break;
	}

	if (i == ARRAY_SIZE(whitelist))
		return -EINVAL;

	switch (entry->size) {
	case 8:
		reg->val = I915_READ64(reg->offset);
		break;
	case 4:
		reg->val = I915_READ(reg->offset);
		break;
	case 2:
		reg->val = I915_READ16(reg->offset);
		break;
	case 1:
		reg->val = I915_READ8(reg->offset);
		break;
	default:
		WARN_ON(1);
		return -EINVAL;
	}

	return 0;
@


1.84
log
@Introduce Linux work queue APIs and use them.  As a side-effect, this will
move some of the work from the system task queue to the driver-specific
task queue.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.83 2015/04/18 14:47:34 jsg Exp $ */
a931 1
	mtx_init(&dev_priv->error_completion_lock, IPL_NONE);
a989 1
	dev_priv->error_completion = 0;
@


1.83
log
@another round of reducing the diff to linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.82 2015/04/18 11:41:28 jsg Exp $ */
a175 1
void	inteldrm_timeout(void *);
d550 1
a550 2
		timeout_del(&dev_priv->rps.delayed_resume_to);
		task_del(systq, &dev_priv->rps.delayed_resume_task);
d980 3
a982 2
	dev_priv->mm.retire_taskq = taskq_create("intelrel", 1, IPL_TTY, 0);
	if (dev_priv->mm.retire_taskq == NULL) {
a1293 8
}

void
inteldrm_timeout(void *arg)
{
	struct inteldrm_softc *dev_priv = arg;

	task_add(dev_priv->mm.retire_taskq, &dev_priv->mm.retire_task);
@


1.82
log
@define and use trace macros
discussed with kettenis
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.81 2015/04/18 11:21:12 jsg Exp $ */
a180 4
int	i915_drm_freeze(struct drm_device *);
int	__i915_drm_thaw(struct drm_device *);
int	i915_drm_thaw(struct drm_device *);

d358 1
a358 1
const static struct drm_pcidev inteldrm_pciidlist[] = {		/* aka */
d515 1
a515 2
bool
i915_semaphore_is_enabled(struct drm_device *dev)
d532 1
a532 2
int
i915_drm_freeze(struct drm_device *dev)
d569 1
a569 2
int
__i915_drm_thaw(struct drm_device *dev)
d600 1
a600 2
int
i915_drm_thaw(struct drm_device *dev)
d1488 1
a1488 1
	if (time_second - dev_priv->last_gpu_reset < 5)
d1493 1
a1493 1
	dev_priv->last_gpu_reset = time_second;
d1775 46
@


1.81
log
@Remove some unused debug functions.  If we're interested in
these in future we should add i915_debugfs.c
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.80 2015/04/18 11:05:32 jsg Exp $ */
d52 1
a53 1
#include "i915_trace.h"
@


1.80
log
@add and use module param macros
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.79 2015/04/17 00:54:42 jsg Exp $ */
a1554 365

/*
 * Debug code from here.
 */

#if (INTELDRM_DEBUG > 1)

int i915_gem_object_list_info(int, uint);

static const char *get_pin_flag(struct drm_i915_gem_object *obj)
{
	if (obj->user_pin_count > 0)
		return "P";
	if (obj->pin_count > 0)
		return "p";
	else
		return " ";
}

static const char *get_tiling_flag(struct drm_i915_gem_object *obj)
{
    switch (obj->tiling_mode) {
    default:
    case I915_TILING_NONE: return " ";
    case I915_TILING_X: return "X";
    case I915_TILING_Y: return "Y";
    }
}

static const char *
cache_level_str(int type)
{
	switch (type) {
	case I915_CACHE_NONE: return " uncached";
	case I915_CACHE_LLC: return " snooped (LLC)";
	case I915_CACHE_LLC_MLC: return " snooped (LLC+MLC)";
	default: return "";
	}
}

static void
describe_obj(struct drm_i915_gem_object *obj)
{
	printf("%p: %s%s %8zdKiB %04x %04x %d %d %d%s%s%s",
		   &obj->base,
		   get_pin_flag(obj),
		   get_tiling_flag(obj),
		   obj->base.size / 1024,
		   obj->base.read_domains,
		   obj->base.write_domain,
		   obj->last_read_seqno,
		   obj->last_write_seqno,
		   obj->last_fenced_seqno,
		   cache_level_str(obj->cache_level),
		   obj->dirty ? " dirty" : "",
		   obj->madv == I915_MADV_DONTNEED ? " purgeable" : "");
	if (obj->base.name)
		printf(" (name: %d)", obj->base.name);
	if (obj->pin_count)
		printf(" (pinned x %d)", obj->pin_count);
	if (obj->fence_reg != I915_FENCE_REG_NONE)
		printf(" (fence: %d)", obj->fence_reg);
#if 0
	if (obj->gtt_space != NULL)
		printf(" (gtt offset: %08x, size: %08x)",
			   obj->gtt_offset, (unsigned int)obj->gtt_space->size);
	if (obj->pin_mappable || obj->fault_mappable) {
		char s[3], *t = s;
		if (obj->pin_mappable)
			*t++ = 'p';
		if (obj->fault_mappable)
			*t++ = 'f';
		*t = '\0';
		printf(" (%s mappable)", s);
	}
#endif
	if (obj->ring != NULL)
		printf(" (%s)", obj->ring->name);
}

#define ACTIVE_LIST 0
#define INACTIVE_LIST 1

int
i915_gem_object_list_info(int kdev, uint list)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct list_head *head;
	drm_i915_private_t *dev_priv = dev->dev_private;
	struct drm_i915_gem_object *obj;
	size_t total_obj_size, total_gtt_size;
	int count;
#if 0
	int ret;

	ret = mutex_lock_interruptible(&dev->struct_mutex);
	if (ret)
		return ret;
#endif

	switch (list) {
	case ACTIVE_LIST:
		printf("Active:\n");
		head = &dev_priv->mm.active_list;
		break;
	case INACTIVE_LIST:
		printf("Inactive:\n");
		head = &dev_priv->mm.inactive_list;
		break;
	default:
//		mutex_unlock(&dev->struct_mutex);
		return -EINVAL;
	}

	total_obj_size = total_gtt_size = count = 0;
	list_for_each_entry(obj, head, mm_list) {
		printf("   ");
		describe_obj(obj);
		printf("\n");
		total_obj_size += obj->base.size;
		count++;
	}
//	mutex_unlock(&dev->struct_mutex);

	printf("Total %d objects, %zu bytes, %zu GTT size\n",
		   count, total_obj_size);
	return 0;
}


static void i915_ring_seqno_info(struct intel_ring_buffer *ring)
{
	if (ring->get_seqno) {
		printf("Current sequence (%s): %d\n",
		       ring->name, ring->get_seqno(ring, false));
	}
}

void
i915_gem_seqno_info(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct intel_ring_buffer *ring;
	int			 i;

	for_each_ring(ring, dev_priv, i)
		i915_ring_seqno_info(ring);
}

void
i915_interrupt_info(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct intel_ring_buffer *ring;
	int			 i, pipe;

	if (IS_VALLEYVIEW(dev)) {
		printf("Display IER:\t%08x\n",
			   I915_READ(VLV_IER));
		printf("Display IIR:\t%08x\n",
			   I915_READ(VLV_IIR));
		printf("Display IIR_RW:\t%08x\n",
			   I915_READ(VLV_IIR_RW));
		printf("Display IMR:\t%08x\n",
			   I915_READ(VLV_IMR));
		for_each_pipe(pipe)
			printf("Pipe %c stat:\t%08x\n",
				   pipe_name(pipe),
				   I915_READ(PIPESTAT(pipe)));

		printf("Master IER:\t%08x\n",
			   I915_READ(VLV_MASTER_IER));

		printf("Render IER:\t%08x\n",
			   I915_READ(GTIER));
		printf("Render IIR:\t%08x\n",
			   I915_READ(GTIIR));
		printf("Render IMR:\t%08x\n",
			   I915_READ(GTIMR));

		printf("PM IER:\t\t%08x\n",
			   I915_READ(GEN6_PMIER));
		printf("PM IIR:\t\t%08x\n",
			   I915_READ(GEN6_PMIIR));
		printf("PM IMR:\t\t%08x\n",
			   I915_READ(GEN6_PMIMR));

		printf("Port hotplug:\t%08x\n",
			   I915_READ(PORT_HOTPLUG_EN));
		printf("DPFLIPSTAT:\t%08x\n",
			   I915_READ(VLV_DPFLIPSTAT));
		printf("DPINVGTT:\t%08x\n",
			   I915_READ(DPINVGTT));

	} else if (!HAS_PCH_SPLIT(dev)) {
		printf("Interrupt enable:    %08x\n",
			   I915_READ(IER));
		printf("Interrupt identity:  %08x\n",
			   I915_READ(IIR));
		printf("Interrupt mask:      %08x\n",
			   I915_READ(IMR));
		for_each_pipe(pipe)
			printf("Pipe %c stat:         %08x\n",
				   pipe_name(pipe),
				   I915_READ(PIPESTAT(pipe)));
	} else {
		printf("North Display Interrupt enable:		%08x\n",
			   I915_READ(DEIER));
		printf("North Display Interrupt identity:	%08x\n",
			   I915_READ(DEIIR));
		printf("North Display Interrupt mask:		%08x\n",
			   I915_READ(DEIMR));
		printf("South Display Interrupt enable:		%08x\n",
			   I915_READ(SDEIER));
		printf("South Display Interrupt identity:	%08x\n",
			   I915_READ(SDEIIR));
		printf("South Display Interrupt mask:		%08x\n",
			   I915_READ(SDEIMR));
		printf("Graphics Interrupt enable:		%08x\n",
			   I915_READ(GTIER));
		printf("Graphics Interrupt identity:		%08x\n",
			   I915_READ(GTIIR));
		printf("Graphics Interrupt mask:		%08x\n",
			   I915_READ(GTIMR));
	}
#if 0
	printf("Interrupts received: %d\n",
		   atomic_read(&dev_priv->irq_received));
#endif
	for_each_ring(ring, dev_priv, i) {
		if (IS_GEN6(dev) || IS_GEN7(dev)) {
			printf(
				   "Graphics Interrupt mask (%s):	%08x\n",
				   ring->name, I915_READ_IMR(ring));
		}
		i915_ring_seqno_info(ring);
	}
}

void
i915_gem_fence_regs_info(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	int i;

	printf("Reserved fences = %d\n", dev_priv->fence_reg_start);
	printf("Total fences = %d\n", dev_priv->num_fence_regs);
	for (i = 0; i < dev_priv->num_fence_regs; i++) {
		struct drm_i915_gem_object *obj = dev_priv->fence_regs[i].obj;

		printf("Fence %d, pin count = %d, object = ",
			   i, dev_priv->fence_regs[i].pin_count);
		if (obj == NULL)
			printf("unused");
		else
			describe_obj(obj);
		printf("\n");
	}
}

void
i915_hws_info(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct intel_ring_buffer *ring = &dev_priv->rings[RCS];
	int i;
	volatile u32 *hws;

	hws = (volatile u32 *)ring->status_page.page_addr;
	if (hws == NULL)
		return;

	for (i = 0; i < 4096 / sizeof(u32) / 4; i += 4) {
		printf("0x%08x: 0x%08x 0x%08x 0x%08x 0x%08x\n",
			   i * 4,
			   hws[i], hws[i + 1], hws[i + 2], hws[i + 3]);
	}
}

static void
i915_dump_pages(bus_space_tag_t bst, bus_space_handle_t bsh,
    bus_size_t size)
{
	bus_addr_t	offset = 0;
	int		i = 0;

	/*
	 * this is a bit odd so i don't have to play with the intel
	 * tools too much.
	 */
	for (offset = 0; offset < size; offset += 4, i += 4) {
		if (i == PAGE_SIZE)
			i = 0;
		printf("%08x :  %08x\n", i, bus_space_read_4(bst, bsh,
		    offset));
	}
}

void
i915_batchbuffer_info(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct drm_obj		*obj;
	struct drm_i915_gem_object *obj_priv;
	bus_space_handle_t	 bsh;
	int			 ret;

	list_for_each_entry(obj_priv, &dev_priv->mm.active_list, mm_list) {
		obj = &obj_priv->base;
		if (obj->read_domains & I915_GEM_DOMAIN_COMMAND) {
			if ((ret = agp_map_subregion(dev_priv->agph,
			    obj_priv->gtt_offset, obj->size, &bsh)) != 0) {
				DRM_ERROR("Failed to map pages: %d\n", ret);
				return;
			}
			printf("--- gtt_offset = 0x%08x\n",
			    obj_priv->gtt_offset);
			i915_dump_pages(dev_priv->bst, bsh, obj->size);
			agp_unmap_subregion(dev_priv->agph,
			    dev_priv->rings[RCS].bsh, obj->size);
		}
	}
}

void
i915_ringbuffer_data(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	bus_size_t		 off;

	if (!dev_priv->rings[RCS].obj) {
		printf("No ringbuffer setup\n");
		return;
	}

	for (off = 0; off < dev_priv->rings[RCS].size; off += 4)
		printf("%08x :  %08x\n", off, bus_space_read_4(dev_priv->bst,
		    dev_priv->rings[RCS].bsh, off));
}

void
i915_ringbuffer_info(int kdev)
{
	struct drm_device	*dev = drm_get_device_from_kdev(kdev);
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	u_int32_t		 head, tail;

	head = I915_READ(PRB0_HEAD) & HEAD_ADDR;
	tail = I915_READ(PRB0_TAIL) & TAIL_ADDR;

	printf("RingHead :  %08x\n", head);
	printf("RingTail :  %08x\n", tail);
	printf("RingMask :  %08x\n", dev_priv->rings[RCS].size - 1);
	printf("RingSize :  %08lx\n", dev_priv->rings[RCS].size);
	printf("Acthd :  %08x\n", I915_READ(INTEL_INFO(dev)->gen >= 4 ?
	    ACTHD_I965 : ACTHD));
}

#endif
@


1.79
log
@Make drm ioctls table driven.  Further reduces the diff to linux.
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.78 2015/04/12 11:26:54 jsg Exp $ */
d77 7
a83 17
/*
 * Override lid status (0=autodetect, 1=autodetect disabled [default],
 * -1=force lid closed, -2=force lid open)
 */
int i915_panel_ignore_lid = 1;

/* Enable powersavings, fbc, downclocking, etc. (default: true) */
unsigned int i915_powersave = 1;

/* Use semaphores for inter-ring sync (default: -1 (use per-chip defaults)) */
int i915_semaphores = -1;

/*
 * Enable frame buffer compression for power savings
 * (default: -1 (use per-chip default))
 */
int i915_enable_fbc = -1;
d85 2
a86 8
/*
 * Enable power-saving render C-state 6.
 * Different stages can be selected via bitmask values
 * (0 = disable; 1 = enable rc6; 2 = enable deep rc6; 4 = enable deepest rc6).
 * For example, 3 would enable rc6 and deep rc6, and 7 would enable everything.
 * default: -1 (use per-chip default)
 */
int i915_enable_rc6 = -1;
d88 77
a164 27
/* Use panel (LVDS/eDP) downclocking for power savings (default: false) */
unsigned int i915_lvds_downclock = 0;

/*
 * Specify LVDS channel mode
 * (0=probe BIOS [default], 1=single-channel, 2=dual-channel)
 */
int i915_lvds_channel_mode = 0;

/*
 * Use Spread Spectrum Clock with panels [LVDS/eDP]
 * (default: auto from VBT)
 */
int i915_panel_use_ssc = -1;

/*
 * Override/Ignore selection of SDVO panel mode in the VBT
 * (-2=ignore, -1=auto [default], index in VBT BIOS table)
 */
int i915_vbt_sdvo_panel_type = -1;

/*
 * Periodically check GPU activity for detecting hangs.
 * WARNING: Disabling this can cause system wide hangs.
 * (default: true)
 */
bool i915_enable_hangcheck = true;
a1487 1
#ifdef notyet
a1489 1
#endif
@


1.78
log
@change back to wait_for/wait_for_atomic_us
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.77 2015/04/11 02:24:43 jsg Exp $ */
a438 1
	.ioctl			= inteldrm_ioctl,
d452 1
d897 2
a1169 101

int
inteldrm_ioctl(struct drm_device *dev, u_long cmd, caddr_t data,
    struct drm_file *file_priv)
{
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	int			 error = 0;

	dev_priv->entries++;

	error = -inteldrm_doioctl(dev, cmd, data, file_priv);

	dev_priv->entries--;
	return (error);
}

int
inteldrm_doioctl(struct drm_device *dev, u_long cmd, caddr_t data,
    struct drm_file *file_priv)
{
	if (file_priv->authenticated == 1) {
		switch (cmd) {
		case DRM_IOCTL_I915_GETPARAM:
			return (i915_getparam(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_EXECBUFFER2:
			return (i915_gem_execbuffer2(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_BUSY:
			return (i915_gem_busy_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_THROTTLE:
			return (i915_gem_ring_throttle(dev, file_priv));
		case DRM_IOCTL_I915_GEM_MMAP:
			return (i915_gem_mmap_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_MMAP_GTT:
			return (i915_gem_mmap_gtt_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_CREATE:
			return (i915_gem_create_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_PREAD:
			return (i915_gem_pread_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_PWRITE:
			return (i915_gem_pwrite_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_SET_DOMAIN:
			return (i915_gem_set_domain_ioctl(dev, data,
			    file_priv));
		case DRM_IOCTL_I915_GEM_SET_TILING:
			return (i915_gem_set_tiling(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_GET_TILING:
			return (i915_gem_get_tiling(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_GET_APERTURE:
			return (i915_gem_get_aperture_ioctl(dev, data,
			    file_priv));
		case DRM_IOCTL_I915_GET_PIPE_FROM_CRTC_ID:
			return (intel_get_pipe_from_crtc_id(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_MADVISE:
			return (i915_gem_madvise_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_SW_FINISH:
			return (i915_gem_sw_finish_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_SET_CACHING:
			return (i915_gem_set_caching_ioctl(dev, data,
			    file_priv));
		case DRM_IOCTL_I915_GEM_GET_CACHING:
			return (i915_gem_get_caching_ioctl(dev, data,
			    file_priv));
		case DRM_IOCTL_I915_GEM_WAIT:
			return (i915_gem_wait_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_CONTEXT_CREATE:
			return (i915_gem_context_create_ioctl(dev, data,
			    file_priv));
		case DRM_IOCTL_I915_GEM_CONTEXT_DESTROY:
			return (i915_gem_context_destroy_ioctl(dev, data,
			    file_priv));
		default:
			break;
		}
	}

	if (file_priv->master == 1) {
		switch (cmd) {
		case DRM_IOCTL_I915_SETPARAM:
			return (i915_setparam(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_INIT:
			return (i915_gem_init_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_ENTERVT:
			return (i915_gem_entervt_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_LEAVEVT:
			return (i915_gem_leavevt_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_PIN:
			return (i915_gem_pin_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_GEM_UNPIN:
			return (i915_gem_unpin_ioctl(dev, data, file_priv));
		case DRM_IOCTL_I915_OVERLAY_PUT_IMAGE:
			return (intel_overlay_put_image(dev, data, file_priv));
		case DRM_IOCTL_I915_OVERLAY_ATTRS:
			return (intel_overlay_attrs(dev, data, file_priv));
		case DRM_IOCTL_I915_GET_SPRITE_COLORKEY:
			return (intel_sprite_get_colorkey(dev, data, file_priv));
		case DRM_IOCTL_I915_SET_SPRITE_COLORKEY:
			return (intel_sprite_set_colorkey(dev, data, file_priv));
		}
	}
	return -EINVAL;
}
@


1.77
log
@Rename i915_gem_chipset_flush() to intel_gtt_chipset_flush()
so we can use the inline definition of i915_gem_chipset_flush()
that avoids the flush entirely on gen >= 6.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.76 2015/04/03 13:10:59 jsg Exp $ */
d1418 1
a1420 1
	int retries;
d1431 3
a1433 9
	for (retries = 500; retries > 0 ; retries--) {
		if (i965_reset_complete(dev))
			break;
		DELAY(1000);
	}
	if (retries == 0) {
		DRM_ERROR("965 reset timed out\n");
		return -ETIMEDOUT;
	}
d1440 1
a1440 11
	for (retries = 500; retries > 0 ; retries--) {
		if (i965_reset_complete(dev))
			break;
		DELAY(1000);
	}
	if (retries == 0) {
		DRM_ERROR("965 reset 2 timed out\n");
		return -ETIMEDOUT;
	}

	return (0);
d1447 1
a1447 1
	int retries;
d1452 3
a1454 9
	for (retries = 500; retries > 0 ; retries--) {
		if (I915_READ(MCHBAR_MIRROR_BASE + ILK_GDSR) & 0x1)
			break;
		DELAY(1000);
	}
	if (retries == 0) {
		DRM_ERROR("ironlake reset timed out\n");
		return -ETIMEDOUT;		
	}
d1460 1
a1460 11
	for (retries = 500; retries > 0 ; retries--) {
		if (I915_READ(MCHBAR_MIRROR_BASE + ILK_GDSR) & 0x1)
			break;
		DELAY(1000);
	}
	if (retries == 0) {
		DRM_ERROR("ironlake reset timed out\n");
		return -ETIMEDOUT;		
	}
	
	return (0);
a1467 1
	int retries;
d1483 1
a1483 9
	for (retries = 500; retries > 0 ; retries--) {
		if ((I915_READ_NOTRACE(GEN6_GDRST) & GEN6_GRDOM_FULL) == 0)
			break;
		DELAY(1000);
	}
	if (retries == 0) {
		DRM_ERROR("gen6 reset timed out\n");
		ret = -ETIMEDOUT;
	}
@


1.76
log
@resync i915_drv.h to make it diffable to linux
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.75 2015/02/12 04:56:03 kettenis Exp $ */
d1341 1
a1341 1
i915_gem_chipset_flush(struct drm_device *dev)
@


1.75
log
@Rename the struct device member of inteldrm_softc to sc_dev and rename the
pointer to the drm subdevice to dev such that we can match the linux code
better.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.74 2015/02/11 07:01:36 jsg Exp $ */
d66 11
d924 1
a924 1
	intel_detect_pch(dev_priv);
d1014 1
a1014 1
	intel_setup_gmbus(dev_priv);
d2040 1
a2040 1
intel_detect_pch(struct inteldrm_softc *dev_priv)
d2042 1
@


1.74
log
@Switch most printf style functions calls back to linux function names
and move DRM_INFO/pr_info/dev_info messages under DRMDEBUG.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.73 2015/02/10 10:50:49 jsg Exp $ */
d645 1
a645 1
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev;
d733 1
a733 1
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev;
d887 2
a888 3
	dev_priv->drmdev = drm_attach_pci(&inteldrm_driver, pa, 1, 1, self);

	dev = (struct drm_device *)dev_priv->drmdev;
d937 1
a937 1
	    dev_priv, dev_priv->dev.dv_xname);
d1082 1
a1082 1
	printf("%s: %dx%d\n", dev_priv->dev.dv_xname, ri->ri_width, ri->ri_height);
d1094 1
a1094 1
	struct drm_device	*dev = (struct drm_device *)dev_priv->drmdev;
d1097 3
a1099 3
	if (dev_priv->drmdev != NULL) {
		config_detach(dev_priv->drmdev, flags);
		dev_priv->drmdev = NULL;
d1124 1
a1124 1
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev;
a2182 1
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev; \
d2186 1
a2186 1
	if (IS_GEN5(dev)) \
d2188 1
a2188 1
	if (NEEDS_FORCE_WAKE((dev), (reg))) { \
d2194 1
a2194 1
	} else if (IS_VALLEYVIEW(dev) && IS_DISPLAYREG(reg)) { \
a2213 1
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev; \
d2216 1
a2216 1
	if (NEEDS_FORCE_WAKE((dev), (reg))) { \
d2219 1
a2219 1
	if (IS_GEN5(dev)) \
d2221 1
a2221 1
	if (IS_HASWELL(dev) && (I915_READ_NOTRACE(GEN7_ERR_INT) & ERR_INT_MMIO_UNCLAIMED)) { \
d2225 1
a2225 1
	if (IS_VALLEYVIEW(dev) && IS_DISPLAYREG(reg)) { \
d2233 1
a2233 1
	if (IS_HASWELL(dev) && (I915_READ_NOTRACE(GEN7_ERR_INT) & ERR_INT_MMIO_UNCLAIMED)) { \
@


1.73
log
@switch most mtx_* calls back to linux spinlocks
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.72 2015/02/10 01:39:32 jsg Exp $ */
d507 2
a508 1
			printf("GEM idle failed, resume might fail\n");
@


1.72
log
@Remove DRM_LOCK macros, rename dev_lock to struct_mutex and directly
call linux style lock functions where these macros were used.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.71 2015/02/09 03:15:41 dlg Exp $ */
d1488 1
d1494 1
a1494 1
	mtx_enter(&dev_priv->gt_lock);
d1524 1
a1524 1
	mtx_leave(&dev_priv->gt_lock);
d2184 1
d2186 1
a2186 1
	mtx_enter(&dev_priv->gt_lock); \
d2200 1
a2200 1
	mtx_leave(&dev_priv->gt_lock); \
d2213 1
d2217 1
a2217 1
	mtx_enter(&dev_priv->gt_lock); \
d2239 1
a2239 1
	mtx_leave(&dev_priv->gt_lock); \
@


1.71
log
@we want to defer work traditionally (in openbsd) handled in an
interrupt context to a taskq running in a thread. however, there
is a concern that if we do that then we allow accidental use of
sleeping APIs in this work, which will make it harder to move the
work back to interrupts in the future.

guenther and kettenis came up with the idea of marking a proc with
CANTSLEEP which the sleep paths can check and panic on.

this builds on that so you create taskqs that run with CANTSLEEP
set except when they need to sleep for more tasks to run.

the taskq_create api is changed to take a flags argument so users
can specify CANTSLEEP. MPSAFE is also passed via this flags field
now.  this means archs that defined IPL_MPSAFE to 0 can now create
mpsafe taskqs too.

lots of discussion at s2k15
ok guenther@@ miod@@ mpi@@ tedu@@ pelikan@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.70 2015/01/27 03:17:36 dlg Exp $ */
d542 1
a542 1
		DRM_LOCK();
d546 1
a546 1
		DRM_UNLOCK();
d569 1
a569 1
		DRM_LOCK();
d571 1
a571 1
		DRM_UNLOCK();
d1587 1
a1587 1
	DRM_LOCK();
d1600 1
a1600 1
		DRM_UNLOCK();
d1641 1
a1641 1
		DRM_UNLOCK();
d1646 1
a1646 1
		DRM_UNLOCK();
@


1.70
log
@remove the second void * argument on tasks.

when workqs were introduced, we provided a second argument so you
could pass a thing and some context to work on it in. there were
very few things that took advantage of the second argument, so when
i introduced pools i suggested removing it. since tasks were meant
to replace workqs, it was requested that we keep the second argument
to make porting from workqs to tasks easier.

now that workqs are gone, i had a look at the use of the second
argument again and found only one good use of it (vdsp(4) on sparc64
if you're interested) and a tiny handful of questionable uses. the
vast majority of tasks only used a single argument. i have since
modified all tasks that used two args to only use one, so now we
can remove the second argument.

so this is a mechanical change. all tasks only passed NULL as their
second argument, so we can just remove it.

ok krw@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.69 2014/12/20 16:34:27 krw Exp $ */
d943 1
a943 1
	dev_priv->mm.retire_taskq = taskq_create("intelrel", 1, IPL_TTY);
@


1.69
log
@Replace switch workq with taskq.

Diff from blambert@@, double ok@@ kettenis.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.68 2014/10/25 14:20:09 kettenis Exp $ */
d602 1
a602 1
void inteldrm_doswitch(void *, void *);
d722 1
a722 1
	inteldrm_doswitch(v, cookie);
d728 1
a728 1
inteldrm_doswitch(void *v, void *dummy)
d898 1
a898 1
	task_set(&dev_priv->switchtask, inteldrm_doswitch, dev_priv, NULL);
@


1.68
log
@Don't attempt to suspend/resume a partially attached drm(4) driver.
Fixes a crash upon resume with an ATI FireMV 2400 card.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.67 2014/07/12 14:18:06 jsg Exp $ */
d717 2
a718 2
		workq_queue_task(NULL, &dev_priv->switchwqt, 0,
		    inteldrm_doswitch, v, cookie);
d728 1
a728 1
inteldrm_doswitch(void *v, void *cookie)
d734 1
a734 1
	rasops_show_screen(ri, cookie, 0, NULL, NULL);
d897 2
@


1.67
log
@remove ifdef'd out valleyview/baytrail device lists

on linux versions before 3.11 i915_pci_probe() doesn't match valleyview
unless i915_preliminary_hw_support is specified as a module parameter.

our drm code is currently based on linux 3.8, so it seems unlikely
that the valleyview support we have works.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.66 2014/05/12 19:29:16 kettenis Exp $ */
d1124 3
@


1.66
log
@Move GTT management for Sandy Bridge and up into inteldrm(4).  This makes
it possible to use the non-mappable part of the GTT, prepares the way for
using the PPGTT and reduces the diffs with Linux.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.65 2014/03/16 03:34:32 jsg Exp $ */
a421 8
#ifdef notyet
	INTEL_VGA_DEVICE(0x0f30, &intel_valleyview_m_info),
	INTEL_VGA_DEVICE(0x0f31, &intel_valleyview_m_info),
	INTEL_VGA_DEVICE(0x0f32, &intel_valleyview_m_info),
	INTEL_VGA_DEVICE(0x0f33, &intel_valleyview_m_info),
	INTEL_VGA_DEVICE(0x0157, &intel_valleyview_m_info),
	INTEL_VGA_DEVICE(0x0155, &intel_valleyview_d_info),
#endif
@


1.65
log
@drm/i915: Do not clobber config status after a forced restore of hw state

From Chris Wilson
790dddadff219165a4666def708c16e8db2c5a7d in ubuntu 3.8
edd5b13313551d6b04acfb90d3db58ed3cf3c814 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.64 2014/03/13 12:45:04 kettenis Exp $ */
d875 1
d890 3
d933 2
d1000 3
d1012 4
a1015 3
	uvm_page_physload(atop(dev->agp->base), atop(dev->agp->base +
	    dev->agp->info.ai_aperture_size), atop(dev->agp->base),
	    atop(dev->agp->base + dev->agp->info.ai_aperture_size),
d1018 1
a1018 1
	dev_priv->pgs = PHYS_TO_VM_PAGE(dev->agp->base);
d1025 1
a1025 1
	for (i = 0; i < atop(dev->agp->info.ai_aperture_size); i++)
d1027 3
a1029 3
	if (agp_init_map(dev_priv->bst, dev->agp->base,
	    dev->agp->info.ai_aperture_size, BUS_SPACE_MAP_LINEAR |
	    BUS_SPACE_MAP_PREFETCHABLE, &dev_priv->agph))
@


1.64
log
@Block userland from entering drm code (and make sure it is no longer in there)
before we truly start quiescing the device and don't unblock until we've
finished waking the device up.  Fixes issues with suspend/resume on inteldrm(4)
and perhaps radeondrm(4) as well.

i"it is the right place to stall" deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.63 2014/02/23 09:36:52 kettenis Exp $ */
d557 1
@


1.63
log
@Give drm(4) a console locator just like wsdisplay(4) such that we can make
sure /dev/drm0 always matches the primary display.

ok mpi@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.62 2014/02/19 01:20:12 jsg Exp $ */
d1116 1
a1116 1
inteldrm_activate(struct device *arg, int act)
d1118 3
a1120 2
	struct inteldrm_softc	*dev_priv = (struct inteldrm_softc *)arg;
	struct drm_device	*dev = (struct drm_device *)dev_priv->drmdev;
d1124 1
d1129 2
d1134 1
d1138 1
a1138 1
	return (0);
@


1.62
log
@drm/i915: fix missed hunk after GT access breakage

From Ben Widawsky
991d4b19f95e3baa4297d57413ca64e7caa1d959 in ubuntu 3.8
e1b4d3036c07ff137955fb1c0197ab62534f46ec in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.61 2014/02/19 01:17:59 jsg Exp $ */
d889 1
a889 1
	dev_priv->drmdev = drm_attach_pci(&inteldrm_driver, pa, 1, self);
@


1.61
log
@drm/i915: initialize gt_lock early with other spin locks

From Jani Nikula
b5ff26dd315431af29d270c338dfabd48ce29073 in ubuntu 3.8
14c5cec5d0cd73e7e9d4fbea2bbfeea8f3ade871 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.60 2014/02/19 01:14:41 jsg Exp $ */
d993 1
@


1.60
log
@drm/i915: fix up gt init sequence fallout

From Daniel Vetter
fc832386574c43961b8b0f177d0062132be1d13b in ubuntu 3.8
181d1b9e31c668259d3798c521672afb8edd355c in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.59 2014/02/19 01:12:25 jsg Exp $ */
d896 1
@


1.59
log
@drm/i915: fix long-standing SNB regression in power consumption after resume v2

From Konstantin Khlebnikov
e508abbbfbefacf721cc1faf39fe5105d179f489 in ubuntu 3.8
7dcd2677ea912573d9ed4bcd629b0023b2d11505 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.58 2014/01/24 04:05:06 jsg Exp $ */
d573 1
a573 1
	intel_gt_reset(dev);
d992 1
a993 1
	intel_gt_reset(dev);
@


1.58
log
@drm/i915: Move num_pipes to intel info

From Ben Widawsky
7ed1faada973243b6e11fa209ada91c9cc1dab53 in ubuntu 3.8
7eb552aeae058a88eece91b902dd51fde45b1f41 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.57 2014/01/24 00:46:45 jsg Exp $ */
d893 7
a986 8
	mtx_init(&dev_priv->irq_lock, IPL_TTY);
	mtx_init(&dev_priv->rps.lock, IPL_TTY);
	mtx_init(&dev_priv->dpio_lock, IPL_TTY);
	mtx_init(&mchdev_lock, IPL_TTY);
	mtx_init(&dev_priv->error_completion_lock, IPL_NONE);

	rw_init(&dev_priv->rps.hw_lock, "rpshw");

d993 1
@


1.57
log
@drm/i915: add more VLV IDs

b102182257eb31597fc0bba5c1730a29d3d290c8 in ubuntu 3.8
d7fee5f6faea17b6e702eba90037ab8f716faf8e in mainline linux

valleyview/baytrail support remains disabled for now
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.56 2014/01/22 04:50:57 deraadt Exp $ */
d150 1
a150 1
	.gen = 2, .is_mobile = 1, .cursor_needs_physical = 1,
d155 1
a155 1
	.gen = 2,
d160 1
a160 1
	.gen = 2, .is_i85x = 1, .is_mobile = 1,
d166 1
a166 1
	.gen = 2,
d171 1
a171 1
	.gen = 3, .is_i915g = 1, .cursor_needs_physical = 1,
d175 1
a175 1
	.gen = 3, .is_mobile = 1,
d181 1
a181 1
	.gen = 3, .has_hotplug = 1, .cursor_needs_physical = 1,
d185 1
a185 1
	.gen = 3, .is_i945gm = 1, .is_mobile = 1,
d192 1
a192 1
	.gen = 4, .is_broadwater = 1,
d198 1
a198 1
	.gen = 4, .is_crestline = 1,
d205 1
a205 1
	.gen = 3, .is_g33 = 1,
d211 1
a211 1
	.gen = 4, .is_g4x = 1, .need_gfx_hws = 1,
d217 1
a217 1
	.gen = 4, .is_g4x = 1,
d225 1
a225 1
	.gen = 3, .is_g33 = 1, .is_pineview = 1, .is_mobile = 1,
d231 1
a231 1
	.gen = 5,
d237 1
a237 1
	.gen = 5, .is_mobile = 1,
d244 1
a244 1
	.gen = 6,
d253 1
a253 1
	.gen = 6, .is_mobile = 1,
d263 1
a263 1
	.is_ivybridge = 1, .gen = 7,
d272 1
a272 1
	.is_ivybridge = 1, .gen = 7, .is_mobile = 1,
d282 1
a282 1
	.gen = 7, .is_mobile = 1,
d291 1
a291 1
	.gen = 7,
d300 1
a300 1
	.is_haswell = 1, .gen = 7,
d309 1
a309 1
	.is_haswell = 1, .gen = 7, .is_mobile = 1,
d988 1
a988 8
	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev))
		dev_priv->num_pipe = 3;
	else if (IS_MOBILE(dev) || !IS_GEN2(dev))
		dev_priv->num_pipe = 2;
	else
		dev_priv->num_pipe = 1;

	if (drm_vblank_init(dev, dev_priv->num_pipe)) {
@


1.56
log
@bring inteldrm back up after resume using DVACT_WAKEUP (non-cold, able
to tsleep, etc) rather than DVACT_RESUME (cold, interrupts disabled,
super restrictive)
tested by jcs and myself, investigated with kettenis
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.55 2013/12/06 11:17:20 kettenis Exp $ */
d424 3
@


1.55
log
@Another round of reducing diffs with Linux.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.54 2013/12/01 13:53:52 kettenis Exp $ */
d1129 1
a1129 1
	case DVACT_RESUME:
@


1.54
log
@drm/i915: Adding more reserved PCI IDs for Haswell.

From Rodrigo Vivi
349fe2edae985d44dc8ca27c6441b20408a9e184 in ubuntu 3.8
1c98b4871cca4b7ce07e19f92f934d47cf7210b0 in mainline linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.53 2013/12/01 11:47:13 kettenis Exp $ */
a132 7
/* For reset and suspend */
int	i8xx_do_reset(struct drm_device *);
int	i965_do_reset(struct drm_device *);
int	i965_reset_complete(struct drm_device *);
int	ironlake_do_reset(struct drm_device *);
int	gen6_do_reset(struct drm_device *);

d1356 1
a1356 2
int
i8xx_do_reset(struct drm_device *dev)
d1358 1
a1358 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1386 1
a1386 2
int
i965_reset_complete(struct drm_device *dev)
d1388 1
a1388 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1394 1
a1394 2
int
i965_do_reset(struct drm_device *dev)
d1396 1
a1396 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1437 1
a1437 2
int
ironlake_do_reset(struct drm_device *dev)
d1473 1
a1473 2
int
gen6_do_reset(struct drm_device *dev)
d1516 1
a1516 2
int
intel_gpu_reset(struct drm_device *dev)
d1566 1
a1566 2
int
i915_reset(struct drm_device *dev)
@


1.53
log
@Bring back the DRM_IOCTL_I915_GEM_WAIT diff now that I've figured out what
made it fail (WARN_ON() was evaluating its argument multiple times).
This time, also advertise support through I915_PARAM_HAS_WAIT_TIMEOUT.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.52 2013/11/30 20:13:36 kettenis Exp $ */
d371 1
a371 1
	INTEL_VGA_DEVICE(0x0422, &intel_haswell_d_info), /* GT2 desktop */
d374 1
a374 1
	INTEL_VGA_DEVICE(0x042a, &intel_haswell_d_info), /* GT2 server */
d378 6
d386 1
a386 1
	INTEL_VGA_DEVICE(0x0C22, &intel_haswell_d_info), /* SDV GT2 desktop */
d389 1
a389 1
	INTEL_VGA_DEVICE(0x0C2A, &intel_haswell_d_info), /* SDV GT2 server */
d392 7
a398 1
	INTEL_VGA_DEVICE(0x0C26, &intel_haswell_m_info), /* SDV GT2 mobile */
d401 1
a401 1
	INTEL_VGA_DEVICE(0x0A22, &intel_haswell_d_info), /* ULT GT2 desktop */
d404 1
a404 1
	INTEL_VGA_DEVICE(0x0A2A, &intel_haswell_d_info), /* ULT GT2 server */
d407 7
a413 1
	INTEL_VGA_DEVICE(0x0A26, &intel_haswell_m_info), /* ULT GT2 mobile */
d416 1
a416 1
	INTEL_VGA_DEVICE(0x0D22, &intel_haswell_d_info), /* CRW GT2 desktop */
d419 1
a419 1
	INTEL_VGA_DEVICE(0x0D2A, &intel_haswell_d_info), /* CRW GT2 server */
d422 7
a428 1
	INTEL_VGA_DEVICE(0x0D26, &intel_haswell_m_info), /* CRW GT2 mobile */
@


1.52
log
@Oops!  Only intended to commit the i915_dma.c changes in the previous commit.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.50 2013/11/19 19:14:09 kettenis Exp $ */
d1191 2
@


1.51
log
@Reorder some case statements to reduce the diffs with Linux.
@
text
@a1190 2
		case DRM_IOCTL_I915_GEM_WAIT:
			return (i915_gem_wait_ioctl(dev, data, file_priv));
@


1.50
log
@Move the GTT management into the inteldrm driver.  It is really obvious now
that this is necessary as on some hardware we need guard pages between
regions that have different cache attributes.  Even if this appears to cause
regressions on some hardware, this change is a necessary (but not sufficient)
step to fix the cache coherency problems on the affected hardware.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.49 2013/11/19 15:08:03 jsg Exp $ */
d1191 2
@


1.49
log
@backout the DRM_IOCTL_I915_GEM_WAIT commit
it seems to leave X unuseable on resume on at least snb/ivb
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.48 2013/11/17 20:04:47 kettenis Exp $ */
d1038 1
@


1.48
log
@Remove some more dead code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.47 2013/11/17 18:47:13 kettenis Exp $ */
a1189 2
		case DRM_IOCTL_I915_GEM_WAIT:
			return (i915_gem_wait_ioctl(dev, data, file_priv));
@


1.47
log
@Implement DRM_IOCTL_I915_GEM_WAIT.  Based on an earlier diff from jsg@@

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.46 2013/11/17 13:41:25 kettenis Exp $ */
a1632 18
#ifdef WATCH_INACTIVE
void
inteldrm_verify_inactive(struct inteldrm_softc *dev_priv, char *file,
    int line)
{
	struct drm_obj		*obj;
	struct drm_i915_gem_object *obj_priv;

	TAILQ_FOREACH(obj_priv, &dev_priv->mm.inactive_list, list) {
		obj = &obj_priv->base;
		if (obj_priv->pin_count || obj_priv->active ||
		    obj->write_domain & I915_GEM_GPU_DOMAINS)
			DRM_ERROR("inactive %p (p $d a $d w $x) %s:%d\n",
			    obj, obj_priv->pin_count, obj_priv->active,
			    obj->write_domain, file, line);
	}
}
#endif /* WATCH_INACTIVE */
@


1.46
log
@Distinguish between inteldrm and radeondrm.

ok jsg@@, miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.45 2013/11/16 16:15:36 kettenis Exp $ */
d1190 2
@


1.45
log
@Remove some dead code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.44 2013/10/30 02:11:32 dlg Exp $ */
d637 1
a637 1
		*(int *)data = WSDISPLAY_TYPE_KMS;
@


1.44
log
@deprecate taskq_systq() and replace it with extern struct taskq
*const systq defined in task.h

this reduces the cost of using the system taskq and looks less ugly.

requested by and ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.43 2013/10/29 06:30:57 jsg Exp $ */
a1325 226
}

/**
 * Pin an object to the GTT and evaluate the relocations landing in it.
 */
int
i915_gem_object_pin_and_relocate(struct drm_obj *obj,
    struct drm_file *file_priv, struct drm_i915_gem_exec_object2 *entry,
    struct drm_i915_gem_relocation_entry *relocs)
{
	struct drm_device	*dev = obj->dev;
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct drm_obj		*target_obj;
	struct drm_i915_gem_object *obj_priv = to_intel_bo(obj);
	bus_space_handle_t	 bsh;
	int			 i, ret, needs_fence;

	DRM_ASSERT_HELD(obj);
	needs_fence = ((entry->flags & EXEC_OBJECT_NEEDS_FENCE) &&
	    obj_priv->tiling_mode != I915_TILING_NONE);
	if (needs_fence)
		atomic_setbits_int(&obj->do_flags, I915_EXEC_NEEDS_FENCE);

	/* Choose the GTT offset for our buffer and put it there. */
	ret = i915_gem_object_pin(obj_priv, (u_int32_t)entry->alignment,
	    needs_fence, false);
	if (ret)
		return ret;

	if (needs_fence) {
		ret = i915_gem_object_get_fence(obj_priv);
		if (ret)
			return ret;

		if (i915_gem_object_pin_fence(obj_priv))
			obj->do_flags |= __EXEC_OBJECT_HAS_FENCE;

		obj_priv->pending_fenced_gpu_access = true;
	}

	entry->offset = obj_priv->gtt_offset;

	/* Apply the relocations, using the GTT aperture to avoid cache
	 * flushing requirements.
	 */
	for (i = 0; i < entry->relocation_count; i++) {
		struct drm_i915_gem_relocation_entry *reloc = &relocs[i];
		struct drm_i915_gem_object *target_obj_priv;
		uint32_t reloc_val, reloc_offset;

		target_obj = drm_gem_object_lookup(obj->dev, file_priv,
		    reloc->target_handle);
		/* object must have come before us in the list */
		if (target_obj == NULL) {
			i915_gem_object_unpin(obj_priv);
			return -ENOENT;
		}
		if ((target_obj->do_flags & I915_IN_EXEC) == 0) {
			printf("%s: object not already in execbuffer\n",
			__func__);
			ret = -EBADF;
			goto err;
		}

		target_obj_priv = to_intel_bo(target_obj);

		/* The target buffer should have appeared before us in the
		 * exec_object list, so it should have a GTT space bound by now.
		 */
		if (target_obj_priv->dmamap == 0) {
			DRM_ERROR("No GTT space found for object %d\n",
				  reloc->target_handle);
			ret = -EINVAL;
			goto err;
		}

		/* must be in one write domain and one only */
		if (reloc->write_domain & (reloc->write_domain - 1)) {
			ret = -EINVAL;
			goto err;
		}
		if (reloc->read_domains & I915_GEM_DOMAIN_CPU ||
		    reloc->write_domain & I915_GEM_DOMAIN_CPU) {
			DRM_ERROR("relocation with read/write CPU domains: "
			    "obj %p target %d offset %d "
			    "read %08x write %08x", obj,
			    reloc->target_handle, (int)reloc->offset,
			    reloc->read_domains, reloc->write_domain);
			ret = -EINVAL;
			goto err;
		}

		if (reloc->write_domain && target_obj->pending_write_domain &&
		    reloc->write_domain != target_obj->pending_write_domain) {
			DRM_ERROR("Write domain conflict: "
				  "obj %p target %d offset %d "
				  "new %08x old %08x\n",
				  obj, reloc->target_handle,
				  (int) reloc->offset,
				  reloc->write_domain,
				  target_obj->pending_write_domain);
			ret = -EINVAL;
			goto err;
		}

		target_obj->pending_read_domains |= reloc->read_domains;
		target_obj->pending_write_domain |= reloc->write_domain;


		if (reloc->offset > obj->size - 4) {
			DRM_ERROR("Relocation beyond object bounds: "
				  "obj %p target %d offset %d size %d.\n",
				  obj, reloc->target_handle,
				  (int) reloc->offset, (int) obj->size);
			ret = -EINVAL;
			goto err;
		}
		if (reloc->offset & 3) {
			DRM_ERROR("Relocation not 4-byte aligned: "
				  "obj %p target %d offset %d.\n",
				  obj, reloc->target_handle,
				  (int) reloc->offset);
			ret = -EINVAL;
			goto err;
		}

		/* Map the page containing the relocation we're going to
		 * perform.
		 */
		reloc_offset = obj_priv->gtt_offset + reloc->offset;
		reloc_val = target_obj_priv->gtt_offset + reloc->delta;

		if (target_obj_priv->gtt_offset == reloc->presumed_offset) {
			drm_gem_object_unreference(target_obj);
			 continue;
		}

		ret = i915_gem_object_set_to_gtt_domain(obj_priv, true);
		if (ret)
			goto err;

		if ((ret = agp_map_subregion(dev_priv->agph,
		    trunc_page(reloc_offset), PAGE_SIZE, &bsh)) != 0) {
			DRM_ERROR("map failed...\n");
			goto err;
		}

		bus_space_write_4(dev_priv->bst, bsh, reloc_offset & PAGE_MASK,
		     reloc_val);

		reloc->presumed_offset = target_obj_priv->gtt_offset;

		agp_unmap_subregion(dev_priv->agph, bsh, PAGE_SIZE);
		drm_gem_object_unreference(target_obj);
	}

	return 0;

err:
	/* we always jump to here mid-loop */
	drm_gem_object_unreference(target_obj);
	i915_gem_object_unpin(obj_priv);
	return ret;
}

int
i915_gem_get_relocs_from_user(struct drm_i915_gem_exec_object2 *exec_list,
    u_int32_t buffer_count, struct drm_i915_gem_relocation_entry **relocs)
{
	u_int32_t	reloc_count = 0, reloc_index = 0, i;
	int		ret, relocs_max;

	relocs_max = INT_MAX / sizeof(struct drm_i915_gem_relocation_entry);

	*relocs = NULL;
	for (i = 0; i < buffer_count; i++) {
		/* First check for malicious input causing overflow in
		 * the worst case where we need to allocate the entire
		 * relocation tree as a single array.
		 */
		if (exec_list[i].relocation_count > relocs_max - reloc_count)
			return (EINVAL);
		reloc_count += exec_list[i].relocation_count;
	}

	if (reloc_count == 0)
		return (0);

	if (SIZE_MAX / reloc_count < sizeof(**relocs))
		return (EINVAL);
	*relocs = drm_alloc(reloc_count * sizeof(**relocs));
	for (i = 0; i < buffer_count; i++) {
		if ((ret = copyin((void *)(uintptr_t)exec_list[i].relocs_ptr,
		    &(*relocs)[reloc_index], exec_list[i].relocation_count *
		    sizeof(**relocs))) != 0) {
			drm_free(*relocs);
			*relocs = NULL;
			return (ret);
		}
		reloc_index += exec_list[i].relocation_count;
	}

	return (0);
}

int
i915_gem_put_relocs_to_user(struct drm_i915_gem_exec_object2 *exec_list,
    u_int32_t buffer_count, struct drm_i915_gem_relocation_entry *relocs)
{
	u_int32_t	reloc_count = 0, i;
	int		ret = 0;

	if (relocs == NULL)
		return (0);

	for (i = 0; i < buffer_count; i++) {
		if ((ret = copyout(&relocs[reloc_count],
		    (void *)(uintptr_t)exec_list[i].relocs_ptr,
		    exec_list[i].relocation_count * sizeof(*relocs))) != 0)
			break;
		reloc_count += exec_list[i].relocation_count;
	}

	drm_free(relocs);

	return (ret);
@


1.43
log
@Move most of the uses of workqs in drm to the new task/taskq api.
Prevents unintended multiple additions to workqs that was causing
hangs on radeon, and lets us remove tasks more closely matching
the behaviour of the original linux code.

ok kettenis@@
cause of the ttm/radeon hangs debugged by claudio@@ and kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.42 2013/10/21 10:36:24 miod Exp $ */
d500 1
a500 1
		task_del(taskq_systq(), &dev_priv->rps.delayed_resume_task);
@


1.42
log
@Add load_font and list_font accessops to all rasops-based wsdisplay drivers.
Trivial except for tga(4) and gpx(4/vax) which need a bit more care setting
up a new font.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.41 2013/10/20 20:07:29 miod Exp $ */
d58 1
a58 1
#include <sys/workq.h>
d500 1
d914 3
a916 3
	dev_priv->workq = workq_create("intelrel", 1, IPL_TTY);
	if (dev_priv->workq == NULL) {
		printf("couldn't create workq\n");
a1557 1
	int err;
d1559 1
a1559 4
	err = workq_add_task(dev_priv->workq, 0, i915_gem_retire_work_handler,
	    dev_priv, NULL);
	if (err)
		DRM_ERROR("failed to run retire handler\n");
@


1.41
log
@Use C99 named initializers for struct wsdisplay_accessops fields.
No functional change.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.40 2013/10/20 10:43:47 miod Exp $ */
d589 2
d618 2
d734 18
@


1.40
log
@WSDISPLAYIO_GTYPE ioctl support for KMS drivers. ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.39 2013/09/30 06:47:48 jsg Exp $ */
d610 7
a616 9
	inteldrm_wsioctl,
	inteldrm_wsmmap,
	inteldrm_alloc_screen,
	inteldrm_free_screen,
	inteldrm_show_screen,
	NULL,
	NULL,
	inteldrm_getchar,
	inteldrm_burner
@


1.39
log
@move the read/write functions and macros closer to linux
noteably this uses the gt lock and force wake in some cases
and includes a gen5/ironlake errata.
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.38 2013/09/08 11:59:44 jsg Exp $ */
d633 3
@


1.38
log
@switch to using linux style pci match tables
fixes some omissions and flag errors for radeon
and removes the duplicate table for i915
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.37 2013/08/07 19:49:05 kettenis Exp $ */
d53 1
d2270 173
@


1.37
log
@Another major overhaul of the inteldrm(4) GEM code, bringing us considerably
closer to the Linux 3.8.13 codebase.  Almost certainly squashes a few more
bugs.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.36 2013/08/07 00:04:28 jsg Exp $ */
d146 9
d323 86
a408 105
const static struct drm_pcidev inteldrm_pciidlist[] = {
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82830M_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82845G_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82854_IGD,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82855GM_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82865G_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82915G_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_E7221_IGD,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82915GM_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82945G_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82945GM_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82945GME_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82946GZ_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G35_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q965_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G965_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82GM965_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82GME965_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G33_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q35_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q33_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82GM45_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_4SERIES_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q45_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G45_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G41_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82B43_IGD_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82B43_IGD_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_PINEVIEW_IGC_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_PINEVIEW_M_IGC_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CLARKDALE_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_ARRANDALE_IGD,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_GT1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_M_GT1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_S_GT,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_GT2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_M_GT2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_GT2_PLUS,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_M_GT2_PLUS,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_D_GT1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_M_GT1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_S_GT1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_D_GT2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_M_GT2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_S_GT2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_GT2_2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT1_1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT2_1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT3_1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT1_2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT2_2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT3_2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_SDV_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_SDV_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_SDV_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_SDV_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_SDV_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_SDV_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_SDV_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_SDV_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_SDV_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT1_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT2_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT3_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT1_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT2_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT3_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_ULT_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_ULT_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_ULT_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_ULT_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_ULT_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_ULT_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_ULT_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_ULT_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_ULT_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT1_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT2_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT3_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT1_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT2_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT3_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_CRW_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_CRW_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_CRW_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_CRW_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_CRW_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_CRW_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_CRW_GT1,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_CRW_GT2,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_CRW_GT3,		0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT1_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT2_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT3_1,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT1_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT2_2,	0 },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT3_2,	0 },
a411 216
static const struct intel_gfx_device_id {
	int vendor;
        int device;
        const struct intel_device_info *info;
} inteldrm_pciidlist_info[] = {
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82830M_IGD,
	    &intel_i830_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82845G_IGD,
	    &intel_845g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82854_IGD,
	    &intel_i85x_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82855GM_IGD,
	    &intel_i85x_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82865G_IGD,
	    &intel_i865g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82915G_IGD_1,
	    &intel_i915g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_E7221_IGD,
	    &intel_i915g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82915GM_IGD_1,
	    &intel_i915gm_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82945G_IGD_1,
	    &intel_i945g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82945GM_IGD_1,
	    &intel_i945gm_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82945GME_IGD_1,
	    &intel_i945gm_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82946GZ_IGD_1,
	    &intel_i965g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G35_IGD_1,
	    &intel_i965g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q965_IGD_1,
	    &intel_i965g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G965_IGD_1,
	    &intel_i965g_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82GM965_IGD_1,
	    &intel_i965gm_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82GME965_IGD_1,
	    &intel_i965gm_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G33_IGD_1,
	    &intel_g33_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q35_IGD_1,
	    &intel_g33_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q33_IGD_1,
	    &intel_g33_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82GM45_IGD_1,
	    &intel_gm45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_4SERIES_IGD,
	    &intel_g45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82Q45_IGD_1,
	    &intel_g45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G45_IGD_1,
	    &intel_g45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82G41_IGD_1,
	    &intel_g45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82B43_IGD_1,
	    &intel_g45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82B43_IGD_2,
	    &intel_g45_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_PINEVIEW_IGC_1,
	    &intel_pineview_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_PINEVIEW_M_IGC_1,
	    &intel_pineview_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CLARKDALE_IGD,
	    &intel_ironlake_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_ARRANDALE_IGD,
	    &intel_ironlake_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_GT1,
	    &intel_sandybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_M_GT1,
	    &intel_sandybridge_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_S_GT,
	    &intel_sandybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_GT2,
	    &intel_sandybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_M_GT2,
	    &intel_sandybridge_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_GT2_PLUS,
	    &intel_sandybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE2G_M_GT2_PLUS,
	    &intel_sandybridge_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_D_GT1,
	    &intel_ivybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_M_GT1,
	    &intel_ivybridge_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_S_GT1,
	    &intel_ivybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_D_GT2,
	    &intel_ivybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_M_GT2,
	    &intel_ivybridge_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE3G_S_GT2,
	    &intel_ivybridge_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_GT1, /* GT1 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_GT2, /* GT2 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_GT3, /* GT3 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_GT1, /* GT1 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_GT2, /* GT2 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_GT3, /* GT3 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_GT1, /* GT1 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_GT2, /* GT2 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_GT2_2, /* GT2 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT1_1, /* GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT2_1, /* GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT3_1, /* GT3 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT1_2, /* GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT2_2, /* GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_GT3_2, /* GT3 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_SDV_GT1, /* SDV GT1 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_SDV_GT2, /* SDV GT2 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_SDV_GT3, /* SDV GT3 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_SDV_GT1, /* SDV GT1 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_SDV_GT2, /* SDV GT2 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_SDV_GT3, /* SDV GT3 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_SDV_GT1, /* SDV GT1 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_SDV_GT2, /* SDV GT2 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_SDV_GT3, /* SDV GT3 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT1_1, /* SDV GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT2_1, /* SDV GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT3_1, /* SDV GT3 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT1_2, /* SDV GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT2_2, /* SDV GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_SDV_GT3_2, /* SDV GT3 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_ULT_GT1, /* ULT GT1 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_ULT_GT2, /* ULT GT2 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_ULT_GT3, /* ULT GT3 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_ULT_GT1, /* ULT GT1 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_ULT_GT2, /* ULT GT2 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_ULT_GT3, /* ULT GT3 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_ULT_GT1, /* ULT GT1 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_ULT_GT2, /* ULT GT2 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_ULT_GT3, /* ULT GT3 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT1_1, /* ULT GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT2_1, /* ULT GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT3_1, /* ULT GT3 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT1_2, /* ULT GT1 reserved */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT2_2, /* ULT GT2 reserved */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_ULT_GT3_2, /* ULT GT3 reserved */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_CRW_GT1, /* CRW GT1 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_CRW_GT2, /* CRW GT2 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_D_CRW_GT3, /* CRW GT3 desktop */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_CRW_GT1, /* CRW GT1 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_CRW_GT2, /* CRW GT2 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_S_CRW_GT3, /* CRW GT3 server */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_CRW_GT1, /* CRW GT1 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_CRW_GT2, /* CRW GT2 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_M_CRW_GT3, /* CRW GT3 mobile */
	    &intel_haswell_m_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT1_1, /* CRW GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT2_1, /* CRW GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT3_1, /* CRW GT3 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT1_2, /* CRW GT1 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT2_2, /* CRW GT2 reserved */
	    &intel_haswell_d_info },
	{PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_CORE4G_R_CRW_GT3_2, /* CRW GT3 reserved */
	    &intel_haswell_d_info },
	{0, 0, NULL}
};

d444 1
a444 1
	const struct intel_gfx_device_id *did;
d446 1
a446 1
	for (did = &inteldrm_pciidlist_info[0]; did->device != 0; did++) {
d449 1
a449 1
		return (did->info);
@


1.36
log
@add support for hardware contexts on recent intel hardware
based on the code in linux 3.8.13
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.35 2013/07/05 07:20:27 jsg Exp $ */
d1339 1
a1339 1
	error = inteldrm_doioctl(dev, cmd, data, file_priv);
a1348 2
	struct inteldrm_softc	*dev_priv = dev->dev_private;

d1352 1
a1352 1
			return (i915_getparam(dev_priv, data));
d1405 1
a1405 1
			return (i915_setparam(dev_priv, data));
d1426 1
a1426 1
	return (EINVAL);
a1528 14
void
inteldrm_set_max_obj_size(struct inteldrm_softc *dev_priv)
{
	struct drm_device	*dev = (struct drm_device *)dev_priv->drmdev;

	/*
	 * Allow max obj size up to the size where ony 2 would fit the
	 * aperture, but some slop exists due to alignment etc
	 */
	dev_priv->max_gem_obj_size = (dev->gtt_total -
	    atomic_read(&dev->pin_memory)) * 3 / 4 / 2;

}

d1582 1
a1582 1
			return (ENOENT);
d1587 1
a1587 1
			ret = EBADF;
d1599 1
a1599 1
			ret = EINVAL;
d1605 1
a1605 1
			ret = EINVAL;
d1615 1
a1615 1
			ret = EINVAL;
d1628 1
a1628 1
			ret = EINVAL;
d1641 1
a1641 1
			ret = EINVAL;
d1649 1
a1649 1
			ret = EINVAL;
d1665 1
a1665 1
		if (ret != 0)
d1689 1
a1689 1
	return (ret);
d1758 2
a1759 1
	struct inteldrm_softc	*dev_priv = arg;
d1761 3
a1763 2
	if (workq_add_task(dev_priv->workq, 0, i915_gem_retire_work_handler,
	    dev_priv, NULL) == ENOMEM)
@


1.35
log
@make use of the drm_i915_private macro to reduce the diff to linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.34 2013/07/01 20:16:58 kettenis Exp $ */
d1393 6
d2051 1
a2052 1
		i915_gem_context_init(dev);
@


1.34
log
@The fast scrolling code causes random page table errors on older hardware.
Disable that code and use the write-only rasops code instead on the affected
chips.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.33 2013/06/06 16:14:26 jsg Exp $ */
d708 1
a708 1
	struct inteldrm_softc *dev_priv = dev->dev_private;
d744 1
a744 1
	struct inteldrm_softc *dev_priv = dev->dev_private;
d1862 1
a1862 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1899 1
a1899 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1943 1
a1943 1
	drm_i915_private_t *dev_priv = dev->dev_private;
@


1.33
log
@Add the remaining support code for 4th gen Intel Core/Haswell graphics
and match the same pci devices Linux does.  Untested for lack of
hardware but should work.  Note that 3D/OpenGL won't work until
we update to a newer version of Mesa, which can't happen until
the Radeon KMS work is ready.

ok deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.32 2013/06/01 02:03:30 kettenis Exp $ */
d1229 10
@


1.32
log
@Make mutexes that get used in interrupts IPL_TTY instead of IPL_NONE.

ok jsg@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.31 2013/05/21 22:12:58 kettenis Exp $ */
d359 60
d515 120
@


1.31
log
@Delete unused function.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.30 2013/05/17 12:03:42 kettenis Exp $ */
d982 3
a984 3
	mtx_init(&dev_priv->rps.lock, IPL_NONE);
	mtx_init(&dev_priv->dpio_lock, IPL_NONE);
	mtx_init(&mchdev_lock, IPL_NONE);
@


1.30
log
@wsmoused support
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.29 2013/05/16 21:14:11 kettenis Exp $ */
a1346 19
}

void
inteldrm_wipe_mappings(struct drm_obj *obj)
{
	struct drm_i915_gem_object *obj_priv = to_intel_bo(obj);
	struct drm_device	*dev = obj->dev;
	struct inteldrm_softc	*dev_priv = dev->dev_private;
	struct vm_page		*pg;

	DRM_ASSERT_HELD(obj);
	/* make sure any writes hit the bus before we do whatever change
	 * that prompted us to kill the mappings.
	 */
	DRM_MEMORYBARRIER();
	/* nuke all our mappings. XXX optimise. */
	for (pg = &dev_priv->pgs[atop(obj_priv->gtt_offset)]; pg !=
	    &dev_priv->pgs[atop(obj_priv->gtt_offset + obj->size)]; pg++)
		pmap_page_protect(pg, VM_PROT_NONE);
@


1.29
log
@burner support
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.28 2013/05/15 10:24:36 jsg Exp $ */
d634 1
d662 1
a662 1
	NULL,
d765 9
@


1.28
log
@sync the list of pre haswell intel video devices with linux
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.27 2013/05/05 13:55:36 kettenis Exp $ */
d634 1
d658 5
a662 1
	inteldrm_show_screen
d764 19
@


1.27
log
@Add nonblocking argument to i915_gem_object_pin() and
i915_gem_object_bind_to_gtt().
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.26 2013/05/05 13:02:45 kettenis Exp $ */
d317 1
d336 1
a336 1
	{PCI_VENDOR_INTEL, 0x2E02,				0 },
d340 2
d348 1
d371 2
d409 1
a409 1
	{PCI_VENDOR_INTEL, 0x2E02,
d417 4
d433 2
@


1.26
log
@With KMS, the inteldrm_quiesce dance isn't needed anymore.  Zap the code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.25 2013/05/05 12:30:41 kettenis Exp $ */
d1345 1
a1345 1
	    needs_fence);
@


1.25
log
@Remove some #ifdef'ed out code that's never going to get used again.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.24 2013/05/04 13:18:29 kettenis Exp $ */
a130 1
void	inteldrm_quiesce(struct inteldrm_softc *);
a1071 1
//		inteldrm_quiesce(dev_priv);
a1074 1
//		i915_save_state(dev);
a1076 4
//		i915_restore_state(dev);
//		/* entrypoints can stop sleeping now */
//		atomic_clearbits_int(&dev_priv->sc_flags, INTELDRM_QUIET);
//		wakeup(&dev_priv->flags);
a1100 4
	while ((dev_priv->sc_flags & INTELDRM_QUIET) && error == 0)
		error = tsleep(&dev_priv->flags, PCATCH, "intelioc", 0);
	if (error)
		return (error);
a1105 2
	if (dev_priv->sc_flags & INTELDRM_QUIET)
		wakeup(&dev_priv->entries);
a1545 27
}

void
inteldrm_quiesce(struct inteldrm_softc *dev_priv)
{
	/*
	 * Right now we depend on X vt switching, so we should be
	 * already suspended, but fallbacks may fault, etc.
	 * Since we can't readback the gtt to reset what we have, make
	 * sure that everything is unbound.
	 */
	KASSERT(dev_priv->mm.suspended);
	KASSERT(dev_priv->ring[RCS].obj == NULL);
	atomic_setbits_int(&dev_priv->sc_flags, INTELDRM_QUIET);
	while (dev_priv->entries)
		tsleep(&dev_priv->entries, 0, "intelquiet", 0);
	/*
	 * nothing should be dirty WRT the chip, only stuff that's bound
	 * for gtt mapping. Nothing should be pinned over vt switch, if it
	 * is then rendering corruption will occur due to api misuse, shame.
	 */
	KASSERT(list_empty(&dev_priv->mm.active_list));
	/* Disabled because root could panic the kernel if this was enabled */
	/* KASSERT(dev->pin_count == 0); */

	/* can't fail since uninterruptible */
	(void)i915_gem_evict_inactive(dev_priv);
@


1.24
log
@In i915_alloc_ifp() and i965_alloc_ifp() use extent_alloc_subregion() to make
sure we allocate outside the Legacy Address Range.  Gets rid of the "no ifp"
warning on the x41.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.23 2013/04/30 19:56:46 kettenis Exp $ */
a1045 10

#if 0
	if (!I915_NEED_GFX_HWS(dev) && dev_priv->hws_dmamem) {
		drm_dmamem_free(dev_priv->dmat, dev_priv->hws_dmamem);
		dev_priv->hws_dmamem = NULL;
		/* Need to rewrite hardware status page */
		I915_WRITE(HWS_PGA, 0x1ffff000);
//		dev_priv->hw_status_page = NULL;
	}
#endif
@


1.23
log
@Clear the right pixels when scrolling backwards.

Fixes issue reported by tedu@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.22 2013/04/21 14:41:26 kettenis Exp $ */
d1229 5
a1233 3
	} else if (bpa->pa_memex == NULL || extent_alloc(bpa->pa_memex,
	    PAGE_SIZE, PAGE_SIZE, 0, 0, 0, &addr) || bus_space_map(bpa->pa_memt,
	    addr, PAGE_SIZE, 0, &dev_priv->ifp.i9xx.bsh))
d1263 5
a1267 3
	} else if (bpa->pa_memex == NULL || extent_alloc(bpa->pa_memex,
	    PAGE_SIZE, PAGE_SIZE, 0, 0, 0, &addr) || bus_space_map(bpa->pa_memt,
	    addr, PAGE_SIZE, 0, &dev_priv->ifp.i9xx.bsh))
@


1.22
log
@Move GEM initialization code into its own function like Linux has.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.21 2013/04/18 20:19:32 kettenis Exp $ */
d767 1
d771 1
a771 1
			int delta = src * ri->ri_font->fontheight * ri->ri_stride;
d783 2
a784 1
			int delta = dst * ri->ri_font->fontheight * ri->ri_stride;
a793 2

			bzero(ri->ri_bits, delta);
@


1.21
log
@"160 chars ought to be enough for everybody"
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.20 2013/04/17 21:34:58 kettenis Exp $ */
a896 7
	INIT_LIST_HEAD(&dev_priv->mm.active_list);
	INIT_LIST_HEAD(&dev_priv->mm.inactive_list);
	INIT_LIST_HEAD(&dev_priv->mm.bound_list);
	INIT_LIST_HEAD(&dev_priv->mm.fence_list);
	for (i = 0; i < I915_NUM_RINGS; i++)
		init_ring_lists(&dev_priv->ring[i]);
	timeout_set(&dev_priv->mm.retire_timer, inteldrm_timeout, dev_priv);
a901 28
	/* On GEN3 we really need to make sure the ARB C3 LP bit is set */
	if (IS_GEN3(dev)) {
		u_int32_t tmp = I915_READ(MI_ARB_STATE);
		if (!(tmp & MI_ARB_C3_LP_WRITE_ENABLE)) {
			/*
			 * arb state is a masked write, so set bit + bit
			 * in mask
			 */
			I915_WRITE(MI_ARB_STATE,
			           _MASKED_BIT_ENABLE(MI_ARB_C3_LP_WRITE_ENABLE));
		}
	}

	dev_priv->relative_constants_mode = I915_EXEC_CONSTANTS_REL_GENERAL;

	/* Old X drivers will take 0-2 for front, back, depth buffers */
	if (!drm_core_check_feature(dev, DRIVER_MODESET))
		dev_priv->fence_reg_start = 3;

	if (INTEL_INFO(dev)->gen >= 4 || IS_I945G(dev) ||
	    IS_I945GM(dev) || IS_G33(dev))
		dev_priv->num_fence_regs = 16;
	else
		dev_priv->num_fence_regs = 8;

	/* Initialise fences to zero, else on some macs we'll get corruption */
	i915_gem_reset_fences(dev);

d931 2
a932 3
	i915_gem_detect_bit_6_swizzle(dev_priv, &bpa);

	dev_priv->mm.interruptible = true;
d934 1
a934 2
	printf("%s: %s\n", dev_priv->dev.dv_xname,
	    pci_intr_string(pa->pa_pc, dev_priv->ih));
@


1.20
log
@Also accelerate scrolling backwards.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.19 2013/04/17 20:04:04 kettenis Exp $ */
d1036 1
a1036 1
	rasops_init(ri, 96, 132);
@


1.19
log
@Another round of reducing diffs with Linux code.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.18 2013/04/14 19:04:37 kettenis Exp $ */
d762 2
a763 1
	if (dst == 0 && (src + num) == ri->ri_rows) {
a766 1
		int delta = src * ri->ri_font->fontheight * ri->ri_stride;
d769 23
a791 1
		bzero(ri->ri_bits, delta);
d793 1
a793 7
		sc->sc_offset += delta;
		ri->ri_bits += delta;
		ri->ri_origbits += delta;
		if (sc->sc_offset > size) {
			sc->sc_offset -= size;
			ri->ri_bits -= size;
			ri->ri_origbits -= size;
@


1.18
log
@Take a different approach towards framebuffer accelartion.  Instead of using
the blitter, scroll by double-mapping the framebuffer and reprogramming the
registers that determine the first visible pixel, much in the same way as the
vga text console uses the 6845.  This makes scrolling very fast, and since we
no longer need to issue commands to any of the rings, we can enable this when
X is running and safely scroll when printing panic messages or if we've
entered ddb.

Testes by many.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.17 2013/04/05 02:54:51 jsg Exp $ */
d527 1
a527 1
		int error = i915_gem_idle(dev_priv);
d530 1
a530 1
			return (error);
d886 1
a886 1
		init_ring_lists(&dev_priv->rings[i]);
d1598 1
a1598 1
	KASSERT(dev_priv->rings[RCS].obj == NULL);
@


1.17
log
@move the bounds check for execbuffer relocation count closer to linux
ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.16 2013/04/03 19:57:17 kettenis Exp $ */
a753 2
int inteldrm_copycols(void *, int, int, int, int);
int inteldrm_erasecols(void *, int, int, int, long);
a754 48
int inteldrm_eraserows(void *cookie, int, int, long);
void inteldrm_copyrect(struct inteldrm_softc *, int, int, int, int, int, int);
void inteldrm_fillrect(struct inteldrm_softc *, int, int, int, int, int);

int
inteldrm_copycols(void *cookie, int row, int src, int dst, int num)
{
	struct rasops_info *ri = cookie;
	struct inteldrm_softc *sc = ri->ri_hw;
	struct drm_device *dev = (struct drm_device *)sc->drmdev;

	if (dev->open_count > 0 || sc->noaccel)
		return sc->noaccel_copycols(cookie, row, src, dst, num);

	num *= ri->ri_font->fontwidth;
	src *= ri->ri_font->fontwidth;
	dst *= ri->ri_font->fontwidth;
	row *= ri->ri_font->fontheight;

	inteldrm_copyrect(sc, ri->ri_xorigin + src, ri->ri_yorigin + row,
	    ri->ri_xorigin + dst, ri->ri_yorigin + row,
	    num, ri->ri_font->fontheight);

	return 0;
}

int
inteldrm_erasecols(void *cookie, int row, int col, int num, long attr)
{
	struct rasops_info *ri = cookie;
	struct inteldrm_softc *sc = ri->ri_hw;
	struct drm_device *dev = (struct drm_device *)sc->drmdev;
	int bg, fg;

	if (dev->open_count > 0 || sc->noaccel)
		return sc->noaccel_erasecols(cookie, row, col, num, attr);

	ri->ri_ops.unpack_attr(cookie, attr, &fg, &bg, NULL);

	row *= ri->ri_font->fontheight;
	col *= ri->ri_font->fontwidth;
	num *= ri->ri_font->fontwidth;

	inteldrm_fillrect(sc, ri->ri_xorigin + col, ri->ri_yorigin + row,
	    num, ri->ri_font->fontheight, ri->ri_devcmap[bg]);

	return 0;
}
a760 1
	struct drm_device *dev = (struct drm_device *)sc->drmdev;
d762 17
a778 2
	if (dev->open_count > 0 || sc->noaccel)
		return sc->noaccel_copyrows(cookie, src, dst, num);
d780 16
a795 3
	num *= ri->ri_font->fontheight;
	src *= ri->ri_font->fontheight;
	dst *= ri->ri_font->fontheight;
d797 1
a797 78
	inteldrm_copyrect(sc, ri->ri_xorigin, ri->ri_yorigin + src,
	    ri->ri_xorigin, ri->ri_yorigin + dst, ri->ri_emuwidth, num);

	return 0;
}

int
inteldrm_eraserows(void *cookie, int row, int num, long attr)
{
	struct rasops_info *ri = cookie;
	struct inteldrm_softc *sc = ri->ri_hw;
	struct drm_device *dev = (struct drm_device *)sc->drmdev;
	int bg, fg;
	int x, y, w;

	if (dev->open_count > 0 || sc->noaccel)
		return sc->noaccel_eraserows(cookie, row, num, attr);

	ri->ri_ops.unpack_attr(cookie, attr, &fg, &bg, NULL);

	if ((num == ri->ri_rows) && ISSET(ri->ri_flg, RI_FULLCLEAR)) {
		num = ri->ri_height;
		x = y = 0;
		w = ri->ri_width;
	} else {
		num *= ri->ri_font->fontheight;
		x = ri->ri_xorigin;
		y = ri->ri_yorigin + row * ri->ri_font->fontheight;
		w = ri->ri_emuwidth;
	}
	inteldrm_fillrect(sc, x, y, w, num, ri->ri_devcmap[bg]);

	return 0;
}

void
inteldrm_copyrect(struct inteldrm_softc *dev_priv, int sx, int sy,
    int dx, int dy, int w, int h)
{
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev;
	bus_addr_t base = dev_priv->fbdev->ifb.obj->gtt_offset;
	uint32_t pitch = dev_priv->fbdev->ifb.base.pitches[0];
	struct intel_ring_buffer *ring;
	uint32_t seqno;
	int ret, i;

	if (HAS_BLT(dev))
		ring = &dev_priv->rings[BCS];
	else
		ring = &dev_priv->rings[RCS];

	ret = intel_ring_begin(ring, 8);
	if (ret)
		return;

	intel_ring_emit(ring, XY_SRC_COPY_BLT_CMD |
	    XY_SRC_COPY_BLT_WRITE_ALPHA | XY_SRC_COPY_BLT_WRITE_RGB);
	intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_GXCOPY | pitch);
	intel_ring_emit(ring, (dx << 0) | (dy << 16));
	intel_ring_emit(ring, ((dx + w) << 0) | ((dy + h) << 16));
	intel_ring_emit(ring, base);
	intel_ring_emit(ring, (sx << 0) | (sy << 16));
	intel_ring_emit(ring, pitch);
	intel_ring_emit(ring, base);
	intel_ring_advance(ring);

	ret = ring->flush(ring, 0, I915_GEM_GPU_DOMAINS);
	if (ret)
		return;

	ret = i915_add_request(ring, NULL, &seqno);
	if (ret)
		return;

	for (i = 1000000; i != 0; i--) {
		if (i915_seqno_passed(ring->get_seqno(ring, true), seqno))
			break;
		DELAY(1);
d800 1
a800 52
	i915_gem_retire_requests_ring(ring);
}

void
inteldrm_fillrect(struct inteldrm_softc *dev_priv, int x, int y,
    int w, int h, int color)
{
	struct drm_device *dev = (struct drm_device *)dev_priv->drmdev;
	bus_addr_t base = dev_priv->fbdev->ifb.obj->gtt_offset;
	uint32_t pitch = dev_priv->fbdev->ifb.base.pitches[0];
	struct intel_ring_buffer *ring;
	uint32_t seqno;
	int ret, i;

	if (HAS_BLT(dev))
		ring = &dev_priv->rings[BCS];
	else
		ring = &dev_priv->rings[RCS];

	ret = intel_ring_begin(ring, 6);
	if (ret)
		return;

#define XY_COLOR_BLT_CMD		((2<<29)|(0x50<<22)|4)
#define XY_COLOR_BLT_WRITE_ALPHA	(1<<21)
#define XY_COLOR_BLT_WRITE_RGB		(1<<20)
#define   BLT_ROP_PATCOPY		(0xf0<<16)

	intel_ring_emit(ring, XY_COLOR_BLT_CMD |
	    XY_COLOR_BLT_WRITE_ALPHA | XY_COLOR_BLT_WRITE_RGB);
	intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_PATCOPY | pitch);
	intel_ring_emit(ring, (x << 0) | (y << 16));
	intel_ring_emit(ring, ((x + w) << 0) | ((y + h) << 16));
	intel_ring_emit(ring, base);
	intel_ring_emit(ring, color);
	intel_ring_advance(ring);

	ret = ring->flush(ring, 0, I915_GEM_GPU_DOMAINS);
	if (ret)
		return;

	ret = i915_add_request(ring, NULL, &seqno);
	if (ret)
		return;

	for (i = 1000000; i != 0; i--) {
		if (i915_seqno_passed(ring->get_seqno(ring, true), seqno))
			break;
		DELAY(1);
	}

	i915_gem_retire_requests_ring(ring);
d1023 1
a1023 4
	dev_priv->noaccel_copyrows = ri->ri_copyrows;
	dev_priv->noaccel_copycols = ri->ri_copycols;
	dev_priv->noaccel_eraserows = ri->ri_eraserows;
	dev_priv->noaccel_erasecols = ri->ri_erasecols;
a1024 3
	ri->ri_copycols = inteldrm_copycols;
	ri->ri_eraserows = inteldrm_eraserows;
	ri->ri_erasecols = inteldrm_erasecols;
d1048 2
a1104 1
		dev_priv->noaccel = 1;
a1116 1
		dev_priv->noaccel = 0;
@


1.16
log
@Return ENOENT instead of EBADF if looking up a gem object fails.
Return EINVAL instead of EBADF if the same object is specified twice in
an execbuffer2 call.
This is what Linux does.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.15 2013/04/03 07:36:57 jsg Exp $ */
d1687 3
a1689 1
	int		ret;
d1693 5
a1697 1
		if (reloc_count + exec_list[i].relocation_count < reloc_count)
@


1.15
log
@move i915_gem_find_inactive_object() into i915_gem_evict.c
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.14 2013/03/30 13:14:33 kettenis Exp $ */
d1572 1
a1572 1
			return (EBADF);
@


1.14
log
@If the ws_{get|set}_param hooks are set, use those for brightness control
instead of poking the hardware directly.  Prevents some funny interaction
between using the brightness keys and wsconsctl(8) on the Dell XPS M1330.
Changing the brightness behind ACPI's back is apparently not such a good idea.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.13 2013/03/30 04:57:53 jsg Exp $ */
a1497 41
}

struct drm_obj *
i915_gem_find_inactive_object(struct inteldrm_softc *dev_priv,
    size_t min_size)
{
	struct drm_obj		*obj, *best = NULL, *first = NULL;
	struct drm_i915_gem_object *obj_priv;

	/*
	 * We don't need references to the object as long as we hold the list
	 * lock, they won't disappear until we release the lock.
	 */
	list_for_each_entry(obj_priv, &dev_priv->mm.inactive_list, mm_list) {
		obj = &obj_priv->base;
		if (obj->size >= min_size) {
			if ((!obj_priv->dirty ||
			    i915_gem_object_is_purgeable(obj_priv)) &&
			    (best == NULL || obj->size < best->size)) {
				best = obj;
				if (best->size == min_size)
					break;
			}
		}
		if (first == NULL)
			first = obj;
	}
	if (best == NULL)
		best = first;
	if (best) {
		drm_ref(&best->uobj);
		/*
		 * if we couldn't grab it, we may as well fail and go
		 * onto the next step for the sake of simplicity.
		 */
		if (drm_try_hold_object(best) == 0) {
			drm_unref(&best->uobj);
			best = NULL;
		}
	}
	return (best);
@


1.13
log
@go back to the old method of execbuffer pinning
should fix problems noticed by Ralf Horstmann and bluhm@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.12 2013/03/28 23:19:26 jsg Exp $ */
d649 3
d662 3
d674 3
@


1.12
log
@add support for relaxed deltas
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.11 2013/03/28 19:36:14 kettenis Exp $ */
d1557 1
a1557 1
    struct drm_i915_gem_relocation_entry *relocs, struct intel_ring_buffer *ring)
d1564 1
a1564 1
	int			 i, ret;
d1567 18
d1586 4
a1589 3
	ret = i915_gem_execbuffer_reserve_object(obj_priv, ring);
	if (ret)
		return (ret);
@


1.11
log
@Add i915_enable_hangcheck parameter.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.10 2013/03/28 11:51:05 jsg Exp $ */
a1651 6
			ret = EINVAL;
			goto err;
		}

		if (reloc->delta > target_obj->size) {
			DRM_ERROR("reloc larger than target\n");
@


1.10
log
@add i915_gem_execbuffer_reserve_object and friends and move
the execbuffer pinning closer to linux
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.9 2013/03/28 05:13:07 jsg Exp $ */
d112 7
@


1.9
log
@add the ioctls to get/set the caching level of a buffer object
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.8 2013/03/26 21:01:02 kettenis Exp $ */
d1550 1
a1550 1
    struct drm_i915_gem_relocation_entry *relocs)
d1557 1
a1557 1
	int			 i, ret, needs_fence;
d1560 2
a1561 8
	needs_fence = ((entry->flags & EXEC_OBJECT_NEEDS_FENCE) &&
	    obj_priv->tiling_mode != I915_TILING_NONE);
	if (needs_fence)
		atomic_setbits_int(&obj->do_flags, I915_EXEC_NEEDS_FENCE);

	/* Choose the GTT offset for our buffer and put it there. */
	ret = i915_gem_object_pin(obj_priv, (u_int32_t)entry->alignment,
	    needs_fence);
d1563 1
a1563 14
		return ret;

	if (needs_fence) {
		ret = i915_gem_object_get_fence(obj_priv);
		if (ret)
			return ret;

		if (i915_gem_object_pin_fence(obj_priv))
			obj->do_flags |= __EXEC_OBJECT_HAS_FENCE;

		obj_priv->pending_fenced_gpu_access = true;
	}

	entry->offset = obj_priv->gtt_offset;
@


1.8
log
@Short-circuit screen switching if we're switching to the screen that's
currently active.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.7 2013/03/25 19:50:56 kettenis Exp $ */
d1336 6
@


1.7
log
@Use the new rasops multiple screen support to provide proper virtual
terminals.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.6 2013/03/22 22:51:00 kettenis Exp $ */
d702 4
@


1.6
log
@Move i915_gem_gtt_map_ioctl() from i915_drv.c to i915_gem.c and rename it
to i915_gem_mmap_ioctl().
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.5 2013/03/22 06:19:56 jsg Exp $ */
d685 1
a685 9
	if (dev_priv->nscreens > 8)
		return (ENOMEM);

	*cookiep = ri;
	*curxp = 0;
	*curyp =0;
	ri->ri_ops.alloc_attr(ri, 0, 0, 0, attrp);
	dev_priv->nscreens++;
	return (0);
d692 1
d694 1
a694 1
	dev_priv->nscreens--;
d720 1
d723 1
d749 1
a749 1
		return sc->noaccel_ops.copycols(cookie, row, src, dst, num);
d772 1
a772 1
		return sc->noaccel_ops.erasecols(cookie, row, col, num, attr);
d794 1
a794 1
		return sc->noaccel_ops.copyrows(cookie, src, dst, num);
d816 1
a816 1
		return sc->noaccel_ops.eraserows(cookie, row, num, attr);
d1150 1
a1150 1
	ri->ri_flg = RI_CENTER;
a1152 2
	dev_priv->noaccel_ops = ri->ri_ops;

d1154 8
a1161 4
	ri->ri_ops.copyrows = inteldrm_copyrows;
	ri->ri_ops.copycols = inteldrm_copycols;
	ri->ri_ops.eraserows = inteldrm_eraserows;
	ri->ri_ops.erasecols = inteldrm_erasecols;
d1179 3
a1181 2
		ri->ri_ops.alloc_attr(ri, 0, 0, 0, &defattr);
		wsdisplay_cnattach(&inteldrm_stdscreen, ri, 0, 0, defattr);
@


1.5
log
@implement DRM_IOCTL_I915_GEM_SW_FINISH
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.4 2013/03/20 12:37:41 jsg Exp $ */
d1309 1
a1309 1
			return (i915_gem_gtt_map_ioctl(dev, data, file_priv));
a1473 51
}

int
i915_gem_gtt_map_ioctl(struct drm_device *dev, void *data,
    struct drm_file *file_priv)
{
	struct drm_i915_gem_mmap	*args = data;
	struct drm_obj			*obj;
	struct drm_i915_gem_object	*obj_priv;
	vaddr_t				 addr;
	voff_t				 offset;
	vsize_t				 end, nsize;
	int				 ret;

	obj = drm_gem_object_lookup(dev, file_priv, args->handle);
	if (obj == NULL)
		return (EBADF);

	/* Since we are doing purely uvm-related operations here we do
	 * not need to hold the object, a reference alone is sufficient
	 */
	obj_priv = to_intel_bo(obj);

	/* Check size. Also ensure that the object is not purgeable */
	if (args->size == 0 || args->offset > obj->size || args->size >
	    obj->size || (args->offset + args->size) > obj->size ||
	    i915_gem_object_is_purgeable(obj_priv)) {
		ret = EINVAL;
		goto done;
	}

	end = round_page(args->offset + args->size);
	offset = trunc_page(args->offset);
	nsize = end - offset;

	/*
	 * We give our reference from object_lookup to the mmap, so only
	 * must free it in the case that the map fails.
	 */
	addr = 0;
	ret = uvm_map(&curproc->p_vmspace->vm_map, &addr, nsize, &obj->uobj,
	    offset, 0, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW,
	    UVM_INH_SHARE, UVM_ADV_RANDOM, 0));

done:
	if (ret == 0)
		args->addr_ptr = (uint64_t) addr + (args->offset & PAGE_MASK);
	else
		drm_unref(&obj->uobj);

	return (ret);
@


1.4
log
@Backout some changes introduced in linux 3.8.3 which are known
to cause problems and have been reverted in linux 3.8.4-rc1:

"drm/i915: reorder setup sequence to have irqs for output setup"
"drm/i915: enable irqs earlier when resuming"

ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.3 2013/03/19 21:07:31 kettenis Exp $ */
d1332 2
@


1.3
log
@Don't advertise brightness control if it isn't supported.

problem noted and fix tested by todd@@
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.2 2013/03/19 19:13:01 kettenis Exp $ */
a530 1
		dev_priv->enable_hotplug_processing = false;
a561 3
		/* We need working interrupts for modeset enabling ... */
		drm_irq_install(dev);

d564 1
a564 8

		/*
		 * ... but also need to make sure that hotplug processing
		 * doesn't cause havoc. Like in the driver load code we don't
		 * bother with the tiny race here where we might loose hotplug
		 * notifications.
		 * */
		dev_priv->enable_hotplug_processing = true;
@


1.2
log
@Clean up a dmesg wart.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.1 2013/03/18 12:36:51 jsg Exp $ */
d668 1
a668 1
			return 0;
@


1.1
log
@Significantly increase the wordlist for ddb hangman,
and update our device independent DRM code and the Intel DRM code
to be mostly in sync with Linux 3.8.3.  Among other things this
brings support for kernel modesetting and enables use of
the rings on gen6+ Intel hardware.

Based on some earlier work from matthieu@@ with some hints from FreeBSD
and with lots of help from kettenis@@ (including a beautiful accelerated
wscons framebuffer console!)

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/* $OpenBSD: i915_drv.c,v 1.123 2012/09/25 10:19:46 jsg Exp $ */
d973 2
d1101 2
a1102 1
	printf(": %s\n", pci_intr_string(pa->pa_pc, dev_priv->ih));
@

