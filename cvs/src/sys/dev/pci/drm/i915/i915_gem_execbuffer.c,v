head	1.42;
access;
symbols
	OPENBSD_6_2_BASE:1.42
	OPENBSD_6_1:1.41.0.6
	OPENBSD_6_1_BASE:1.41
	OPENBSD_6_0:1.41.0.4
	OPENBSD_6_0_BASE:1.41
	OPENBSD_5_9:1.39.0.2
	OPENBSD_5_9_BASE:1.39
	OPENBSD_5_8:1.38.0.4
	OPENBSD_5_8_BASE:1.38
	OPENBSD_5_7:1.35.0.4
	OPENBSD_5_7_BASE:1.35
	OPENBSD_5_6:1.29.0.4
	OPENBSD_5_6_BASE:1.29
	OPENBSD_5_5:1.26.0.4
	OPENBSD_5_5_BASE:1.26
	OPENBSD_5_4:1.7.0.2
	OPENBSD_5_4_BASE:1.7;
locks; strict;
comment	@ * @;


1.42
date	2017.07.01.16.14.10;	author kettenis;	state Exp;
branches;
next	1.41;
commitid	KnwRPOZok9A30HI4;

1.41
date	2016.04.08.08.27.53;	author kettenis;	state Exp;
branches;
next	1.40;
commitid	mS4ttEBzpAfn3sVx;

1.40
date	2016.04.05.20.46.45;	author kettenis;	state Exp;
branches;
next	1.39;
commitid	BLB92V3xeQqiMrY4;

1.39
date	2015.09.23.23.12.12;	author kettenis;	state Exp;
branches;
next	1.38;
commitid	lQlppvmETCN49oZe;

1.38
date	2015.04.12.17.10.07;	author kettenis;	state Exp;
branches;
next	1.37;
commitid	7RIU3AxWXbuzxDet;

1.37
date	2015.04.11.04.36.10;	author jsg;	state Exp;
branches;
next	1.36;
commitid	TECQYb9n8eZTOhib;

1.36
date	2015.04.05.11.53.53;	author kettenis;	state Exp;
branches;
next	1.35;
commitid	3YXcRggXXMDC9Cpg;

1.35
date	2015.02.12.08.48.32;	author jsg;	state Exp;
branches;
next	1.34;
commitid	b7XA83Agvw5SEHRi;

1.34
date	2015.02.12.06.52.11;	author jsg;	state Exp;
branches;
next	1.33;
commitid	q8lsF9SxaT1LyCh4;

1.33
date	2015.02.12.02.12.02;	author kettenis;	state Exp;
branches;
next	1.32;
commitid	cYXtgYH6nnLqDRGU;

1.32
date	2015.02.10.01.39.32;	author jsg;	state Exp;
branches;
next	1.31;
commitid	a8Vt7gSt34kmziIS;

1.31
date	2014.12.09.07.05.06;	author doug;	state Exp;
branches;
next	1.30;
commitid	zM5ckwX4kwwmipG0;

1.30
date	2014.09.20.16.15.16;	author kettenis;	state Exp;
branches;
next	1.29;
commitid	6wfZPYqhJIzN4SFE;

1.29
date	2014.07.12.18.48.52;	author tedu;	state Exp;
branches;
next	1.28;
commitid	OBNa5kfxQ2UXoiIw;

1.28
date	2014.04.01.20.16.50;	author kettenis;	state Exp;
branches;
next	1.27;

1.27
date	2014.03.17.22.15.24;	author kettenis;	state Exp;
branches;
next	1.26;

1.26
date	2014.01.21.08.57.22;	author kettenis;	state Exp;
branches;
next	1.25;

1.25
date	2013.12.11.20.31.43;	author kettenis;	state Exp;
branches;
next	1.24;

1.24
date	2013.12.07.10.48.35;	author kettenis;	state Exp;
branches;
next	1.23;

1.23
date	2013.12.07.10.46.26;	author kettenis;	state Exp;
branches;
next	1.22;

1.22
date	2013.12.05.13.29.56;	author kettenis;	state Exp;
branches;
next	1.21;

1.21
date	2013.11.30.20.13.36;	author kettenis;	state Exp;
branches;
next	1.20;

1.20
date	2013.11.30.20.03.32;	author kettenis;	state Exp;
branches;
next	1.19;

1.19
date	2013.11.27.22.20.19;	author kettenis;	state Exp;
branches;
next	1.18;

1.18
date	2013.11.19.19.14.09;	author kettenis;	state Exp;
branches;
next	1.17;

1.17
date	2013.11.16.18.24.59;	author kettenis;	state Exp;
branches;
next	1.16;

1.16
date	2013.10.29.06.30.57;	author jsg;	state Exp;
branches;
next	1.15;

1.15
date	2013.10.05.07.30.06;	author jsg;	state Exp;
branches;
next	1.14;

1.14
date	2013.09.18.08.50.28;	author jsg;	state Exp;
branches;
next	1.13;

1.13
date	2013.08.13.10.23.49;	author jsg;	state Exp;
branches;
next	1.12;

1.12
date	2013.08.09.08.14.55;	author jsg;	state Exp;
branches;
next	1.11;

1.11
date	2013.08.09.07.55.42;	author jsg;	state Exp;
branches;
next	1.10;

1.10
date	2013.08.08.21.35.56;	author kettenis;	state Exp;
branches;
next	1.9;

1.9
date	2013.08.07.19.49.07;	author kettenis;	state Exp;
branches;
next	1.8;

1.8
date	2013.08.07.00.04.28;	author jsg;	state Exp;
branches;
next	1.7;

1.7
date	2013.05.05.13.55.36;	author kettenis;	state Exp;
branches;
next	1.6;

1.6
date	2013.04.17.20.04.04;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2013.04.03.19.57.17;	author kettenis;	state Exp;
branches;
next	1.4;

1.4
date	2013.03.30.04.57.53;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2013.03.28.19.38.53;	author kettenis;	state Exp;
branches;
next	1.2;

1.2
date	2013.03.28.11.51.05;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2013.03.18.12.36.52;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.42
log
@Update inteldrm(4) to code based on Linux 4.4.70.  This brings us support for
Skylake and Cherryview and better support for Broadwell and Valleyview.  Also
adds MST support.  Some tweaks to the TTM code and radeondrm(4) to keep it
working with the updated generic DRM code needed for inteldrm(4).

Tested by many.
@
text
@/*
 * Copyright Â© 2008,2010 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 *
 * Authors:
 *    Eric Anholt <eric@@anholt.net>
 *    Chris Wilson <chris@@chris-wilson.co.uk>
 *
 */

#include <dev/pci/drm/drmP.h>
#include <dev/pci/drm/i915_drm.h>
#include "i915_drv.h"
#include "i915_trace.h"
#include "intel_drv.h"
#ifdef __linux__
#include <linux/dma_remapping.h>
#include <linux/uaccess.h>
#endif

#define  __EXEC_OBJECT_HAS_PIN (1<<31)
#define  __EXEC_OBJECT_HAS_FENCE (1<<30)
#define  __EXEC_OBJECT_NEEDS_MAP (1<<29)
#define  __EXEC_OBJECT_NEEDS_BIAS (1<<28)

#define BATCH_OFFSET_BIAS (256*1024)

struct eb_vmas {
	struct list_head vmas;
	int and;
	union {
		struct i915_vma *lut[0];
		struct hlist_head buckets[0];
	};
};

static struct eb_vmas *
eb_create(struct drm_i915_gem_execbuffer2 *args)
{
	struct eb_vmas *eb = NULL;

	if (args->flags & I915_EXEC_HANDLE_LUT) {
		unsigned size = args->buffer_count;
		size *= sizeof(struct i915_vma *);
		size += sizeof(struct eb_vmas);
		eb = kmalloc(size, GFP_TEMPORARY | __GFP_NOWARN | __GFP_NORETRY);
	}

	if (eb == NULL) {
		unsigned size = args->buffer_count;
		unsigned count = PAGE_SIZE / sizeof(struct hlist_head) / 2;
		BUILD_BUG_ON_NOT_POWER_OF_2(PAGE_SIZE / sizeof(struct hlist_head));
		while (count > 2*size)
			count >>= 1;
		eb = kzalloc(count*sizeof(struct hlist_head) +
			     sizeof(struct eb_vmas),
			     GFP_TEMPORARY);
		if (eb == NULL)
			return eb;

		eb->and = count - 1;
	} else
		eb->and = -args->buffer_count;

	INIT_LIST_HEAD(&eb->vmas);
	return eb;
}

static void
eb_reset(struct eb_vmas *eb)
{
	if (eb->and >= 0)
		memset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));
}

static int
eb_lookup_vmas(struct eb_vmas *eb,
	       struct drm_i915_gem_exec_object2 *exec,
	       const struct drm_i915_gem_execbuffer2 *args,
	       struct i915_address_space *vm,
	       struct drm_file *file)
{
	struct drm_i915_gem_object *obj;
	struct list_head objects;
	int i, ret;

	INIT_LIST_HEAD(&objects);
	spin_lock(&file->table_lock);
	/* Grab a reference to the object and release the lock so we can lookup
	 * or create the VMA without using GFP_ATOMIC */
	for (i = 0; i < args->buffer_count; i++) {
		obj = to_intel_bo(idr_find(&file->object_idr, exec[i].handle));
		if (obj == NULL) {
			spin_unlock(&file->table_lock);
			DRM_DEBUG("Invalid object handle %d at index %d\n",
				   exec[i].handle, i);
			ret = -ENOENT;
			goto err;
		}

		if (!list_empty(&obj->obj_exec_link)) {
			spin_unlock(&file->table_lock);
			DRM_DEBUG("Object %p [handle %d, index %d] appears more than once in object list\n",
				   obj, exec[i].handle, i);
			ret = -EINVAL;
			goto err;
		}

		drm_gem_object_reference(&obj->base);
		list_add_tail(&obj->obj_exec_link, &objects);
	}
	spin_unlock(&file->table_lock);

	i = 0;
	while (!list_empty(&objects)) {
		struct i915_vma *vma;

		obj = list_first_entry(&objects,
				       struct drm_i915_gem_object,
				       obj_exec_link);

		/*
		 * NOTE: We can leak any vmas created here when something fails
		 * later on. But that's no issue since vma_unbind can deal with
		 * vmas which are not actually bound. And since only
		 * lookup_or_create exists as an interface to get at the vma
		 * from the (obj, vm) we don't run the risk of creating
		 * duplicated vmas for the same vm.
		 */
		vma = i915_gem_obj_lookup_or_create_vma(obj, vm);
		if (IS_ERR(vma)) {
			DRM_DEBUG("Failed to lookup VMA\n");
			ret = PTR_ERR(vma);
			goto err;
		}

		/* Transfer ownership from the objects list to the vmas list. */
		list_add_tail(&vma->exec_list, &eb->vmas);
		list_del_init(&obj->obj_exec_link);

		vma->exec_entry = &exec[i];
		if (eb->and < 0) {
			eb->lut[i] = vma;
		} else {
			uint32_t handle = args->flags & I915_EXEC_HANDLE_LUT ? i : exec[i].handle;
			vma->exec_handle = handle;
			hlist_add_head(&vma->exec_node,
				       &eb->buckets[handle & eb->and]);
		}
		++i;
	}

	return 0;


err:
	while (!list_empty(&objects)) {
		obj = list_first_entry(&objects,
				       struct drm_i915_gem_object,
				       obj_exec_link);
		list_del_init(&obj->obj_exec_link);
		drm_gem_object_unreference(&obj->base);
	}
	/*
	 * Objects already transfered to the vmas list will be unreferenced by
	 * eb_destroy.
	 */

	return ret;
}

static struct i915_vma *eb_get_vma(struct eb_vmas *eb, unsigned long handle)
{
	if (eb->and < 0) {
		if (handle >= -eb->and)
			return NULL;
		return eb->lut[handle];
	} else {
		struct hlist_head *head;
		struct hlist_node *node;

		head = &eb->buckets[handle & eb->and];
		hlist_for_each(node, head) {
			struct i915_vma *vma;

			vma = hlist_entry(node, struct i915_vma, exec_node);
			if (vma->exec_handle == handle)
				return vma;
		}
		return NULL;
	}
}

static void
i915_gem_execbuffer_unreserve_vma(struct i915_vma *vma)
{
	struct drm_i915_gem_exec_object2 *entry;
	struct drm_i915_gem_object *obj = vma->obj;

	if (!drm_mm_node_allocated(&vma->node))
		return;

	entry = vma->exec_entry;

	if (entry->flags & __EXEC_OBJECT_HAS_FENCE)
		i915_gem_object_unpin_fence(obj);

	if (entry->flags & __EXEC_OBJECT_HAS_PIN)
		vma->pin_count--;

	entry->flags &= ~(__EXEC_OBJECT_HAS_FENCE | __EXEC_OBJECT_HAS_PIN);
}

static void eb_destroy(struct eb_vmas *eb)
{
	while (!list_empty(&eb->vmas)) {
		struct i915_vma *vma;

		vma = list_first_entry(&eb->vmas,
				       struct i915_vma,
				       exec_list);
		list_del_init(&vma->exec_list);
		i915_gem_execbuffer_unreserve_vma(vma);
		drm_gem_object_unreference(&vma->obj->base);
	}
	kfree(eb);
}

static inline int use_cpu_reloc(struct drm_i915_gem_object *obj)
{
	return (HAS_LLC(obj->base.dev) ||
		obj->base.write_domain == I915_GEM_DOMAIN_CPU ||
		obj->cache_level != I915_CACHE_NONE);
}

static int
relocate_entry_cpu(struct drm_i915_gem_object *obj,
		   struct drm_i915_gem_relocation_entry *reloc,
		   uint64_t target_offset)
{
	struct drm_device *dev = obj->base.dev;
	uint32_t page_offset = offset_in_page(reloc->offset);
	uint64_t delta = reloc->delta + target_offset;
	char *vaddr;
	int ret;

	ret = i915_gem_object_set_to_cpu_domain(obj, true);
	if (ret)
		return ret;

	vaddr = kmap_atomic(i915_gem_object_get_page(obj,
				reloc->offset >> PAGE_SHIFT));
	*(uint32_t *)(vaddr + page_offset) = lower_32_bits(delta);

	if (INTEL_INFO(dev)->gen >= 8) {
		page_offset = offset_in_page(page_offset + sizeof(uint32_t));

		if (page_offset == 0) {
			kunmap_atomic(vaddr);
			vaddr = kmap_atomic(i915_gem_object_get_page(obj,
			    (reloc->offset + sizeof(uint32_t)) >> PAGE_SHIFT));
		}

		*(uint32_t *)(vaddr + page_offset) = upper_32_bits(delta);
	}

	kunmap_atomic(vaddr);

	return 0;
}

static int
relocate_entry_gtt(struct drm_i915_gem_object *obj,
		   struct drm_i915_gem_relocation_entry *reloc,
		   uint64_t target_offset)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_i915_private *dev_priv = dev->dev_private;
	uint64_t delta = reloc->delta + target_offset;
	uint64_t offset;
	void __iomem *reloc_page;
	bus_space_handle_t bsh;
	int ret;

	ret = i915_gem_object_set_to_gtt_domain(obj, true);
	if (ret)
		return ret;

	ret = i915_gem_object_put_fence(obj);
	if (ret)
		return ret;

	/* Map the page containing the relocation we're going to perform.  */
	offset = i915_gem_obj_ggtt_offset(obj);
	offset += reloc->offset;
#ifdef __linux__
	reloc_page = io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
					      offset & PAGE_MASK);
#else
	agp_map_atomic(dev_priv->agph, trunc_page(offset), &bsh);
	reloc_page = bus_space_vaddr(dev_priv->bst, bsh);
#endif
	iowrite32(lower_32_bits(delta), reloc_page + offset_in_page(offset));

	if (INTEL_INFO(dev)->gen >= 8) {
		offset += sizeof(uint32_t);

		if (offset_in_page(offset) == 0) {
#ifdef __linux__
			io_mapping_unmap_atomic(reloc_page);
			reloc_page =
				io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
							 offset);
#else
			agp_unmap_atomic(dev_priv->agph, bsh);
			agp_map_atomic(dev_priv->agph, offset, &bsh);
			reloc_page = bus_space_vaddr(dev_priv->bst, bsh);
#endif
		}

		iowrite32(upper_32_bits(delta),
			  reloc_page + offset_in_page(offset));
	}

#ifdef __linux__
	io_mapping_unmap_atomic(reloc_page);
#else
	agp_unmap_atomic(dev_priv->agph, bsh);
#endif

	return 0;
}

static void
clflush_write32(void *addr, uint32_t value)
{
	/* This is not a fast path, so KISS. */
	drm_clflush_virt_range(addr, sizeof(uint32_t));
	*(uint32_t *)addr = value;
	drm_clflush_virt_range(addr, sizeof(uint32_t));
}

static int
relocate_entry_clflush(struct drm_i915_gem_object *obj,
		       struct drm_i915_gem_relocation_entry *reloc,
		       uint64_t target_offset)
{
	struct drm_device *dev = obj->base.dev;
	uint32_t page_offset = offset_in_page(reloc->offset);
	uint64_t delta = (int)reloc->delta + target_offset;
	char *vaddr;
	int ret;

	ret = i915_gem_object_set_to_gtt_domain(obj, true);
	if (ret)
		return ret;

	vaddr = kmap_atomic(i915_gem_object_get_page(obj,
				reloc->offset >> PAGE_SHIFT));
	clflush_write32(vaddr + page_offset, lower_32_bits(delta));

	if (INTEL_INFO(dev)->gen >= 8) {
		page_offset = offset_in_page(page_offset + sizeof(uint32_t));

		if (page_offset == 0) {
			kunmap_atomic(vaddr);
			vaddr = kmap_atomic(i915_gem_object_get_page(obj,
			    (reloc->offset + sizeof(uint32_t)) >> PAGE_SHIFT));
		}

		clflush_write32(vaddr + page_offset, upper_32_bits(delta));
	}

	kunmap_atomic(vaddr);

	return 0;
}

static int
i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,
				   struct eb_vmas *eb,
				   struct drm_i915_gem_relocation_entry *reloc)
{
	struct drm_device *dev = obj->base.dev;
	struct drm_gem_object *target_obj;
	struct drm_i915_gem_object *target_i915_obj;
	struct i915_vma *target_vma;
	uint64_t target_offset;
	int ret;

	/* we've already hold a reference to all valid objects */
	target_vma = eb_get_vma(eb, reloc->target_handle);
	if (unlikely(target_vma == NULL))
		return -ENOENT;
	target_i915_obj = target_vma->obj;
	target_obj = &target_vma->obj->base;

	target_offset = target_vma->node.start;

	/* Sandybridge PPGTT errata: We need a global gtt mapping for MI and
	 * pipe_control writes because the gpu doesn't properly redirect them
	 * through the ppgtt for non_secure batchbuffers. */
	if (unlikely(IS_GEN6(dev) &&
	    reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION)) {
		ret = i915_vma_bind(target_vma, target_i915_obj->cache_level,
				    PIN_GLOBAL);
		if (WARN_ONCE(ret, "Unexpected failure to bind target VMA!"))
			return ret;
	}

	/* Validate that the target is in a valid r/w GPU domain */
	if (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {
		DRM_DEBUG("reloc with multiple write domains: "
			  "obj %p target %d offset %d "
			  "read %08x write %08x",
			  obj, reloc->target_handle,
			  (int) reloc->offset,
			  reloc->read_domains,
			  reloc->write_domain);
		return -EINVAL;
	}
	if (unlikely((reloc->write_domain | reloc->read_domains)
		     & ~I915_GEM_GPU_DOMAINS)) {
		DRM_DEBUG("reloc with read/write non-GPU domains: "
			  "obj %p target %d offset %d "
			  "read %08x write %08x",
			  obj, reloc->target_handle,
			  (int) reloc->offset,
			  reloc->read_domains,
			  reloc->write_domain);
		return -EINVAL;
	}

	target_obj->pending_read_domains |= reloc->read_domains;
	target_obj->pending_write_domain |= reloc->write_domain;

	/* If the relocation already has the right value in it, no
	 * more work needs to be done.
	 */
	if (target_offset == reloc->presumed_offset)
		return 0;

	/* Check that the relocation address is valid... */
	if (unlikely(reloc->offset >
		obj->base.size - (INTEL_INFO(dev)->gen >= 8 ? 8 : 4))) {
		DRM_DEBUG("Relocation beyond object bounds: "
			  "obj %p target %d offset %d size %d.\n",
			  obj, reloc->target_handle,
			  (int) reloc->offset,
			  (int) obj->base.size);
		return -EINVAL;
	}
	if (unlikely(reloc->offset & 3)) {
		DRM_DEBUG("Relocation not 4-byte aligned: "
			  "obj %p target %d offset %d.\n",
			  obj, reloc->target_handle,
			  (int) reloc->offset);
		return -EINVAL;
	}

	/* We can't wait for rendering with pagefaults disabled */
	if (obj->active && pagefault_disabled())
		return -EFAULT;

	if (use_cpu_reloc(obj))
		ret = relocate_entry_cpu(obj, reloc, target_offset);
	else if (obj->map_and_fenceable)
		ret = relocate_entry_gtt(obj, reloc, target_offset);
	else if (cpu_has_clflush)
		ret = relocate_entry_clflush(obj, reloc, target_offset);
	else {
		WARN_ONCE(1, "Impossible case in relocation handling\n");
		ret = -ENODEV;
	}

	if (ret)
		return ret;

	/* and update the user's relocation entry */
	reloc->presumed_offset = target_offset;

	return 0;
}

static int
i915_gem_execbuffer_relocate_vma(struct i915_vma *vma,
				 struct eb_vmas *eb)
{
#define N_RELOC(x) ((x) / sizeof(struct drm_i915_gem_relocation_entry))
	struct drm_i915_gem_relocation_entry stack_reloc[N_RELOC(512)];
	struct drm_i915_gem_relocation_entry __user *user_relocs;
	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
	int remain, ret;

	user_relocs = to_user_ptr(entry->relocs_ptr);

	remain = entry->relocation_count;
	while (remain) {
		struct drm_i915_gem_relocation_entry *r = stack_reloc;
		int count = remain;
		if (count > ARRAY_SIZE(stack_reloc))
			count = ARRAY_SIZE(stack_reloc);
		remain -= count;

		if (__copy_from_user_inatomic(r, user_relocs, count*sizeof(r[0])))
			return -EFAULT;

		do {
			u64 offset = r->presumed_offset;

			ret = i915_gem_execbuffer_relocate_entry(vma->obj, eb, r);
			if (ret)
				return ret;

			if (r->presumed_offset != offset &&
			    __copy_to_user_inatomic(&user_relocs->presumed_offset,
						    &r->presumed_offset,
						    sizeof(r->presumed_offset))) {
				return -EFAULT;
			}

			user_relocs++;
			r++;
		} while (--count);
	}

	return 0;
#undef N_RELOC
}

static int
i915_gem_execbuffer_relocate_vma_slow(struct i915_vma *vma,
				      struct eb_vmas *eb,
				      struct drm_i915_gem_relocation_entry *relocs)
{
	const struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
	int i, ret;

	for (i = 0; i < entry->relocation_count; i++) {
		ret = i915_gem_execbuffer_relocate_entry(vma->obj, eb, &relocs[i]);
		if (ret)
			return ret;
	}

	return 0;
}

static int
i915_gem_execbuffer_relocate(struct eb_vmas *eb)
{
	struct i915_vma *vma;
	int ret = 0;

	/* This is the fast path and we cannot handle a pagefault whilst
	 * holding the struct mutex lest the user pass in the relocations
	 * contained within a mmaped bo. For in such a case we, the page
	 * fault handler would call i915_gem_fault() and we would try to
	 * acquire the struct mutex again. Obviously this is bad and so
	 * lockdep complains vehemently.
	 */
	pagefault_disable();
	list_for_each_entry(vma, &eb->vmas, exec_list) {
		ret = i915_gem_execbuffer_relocate_vma(vma, eb);
		if (ret)
			break;
	}
	pagefault_enable();

	return ret;
}

static bool only_mappable_for_reloc(unsigned int flags)
{
	return (flags & (EXEC_OBJECT_NEEDS_FENCE | __EXEC_OBJECT_NEEDS_MAP)) ==
		__EXEC_OBJECT_NEEDS_MAP;
}

static int
i915_gem_execbuffer_reserve_vma(struct i915_vma *vma,
				struct intel_engine_cs *ring,
				bool *need_reloc)
{
	struct drm_i915_gem_object *obj = vma->obj;
	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
	uint64_t flags;
	int ret;

	flags = PIN_USER;
	if (entry->flags & EXEC_OBJECT_NEEDS_GTT)
		flags |= PIN_GLOBAL;

	if (!drm_mm_node_allocated(&vma->node)) {
		/* Wa32bitGeneralStateOffset & Wa32bitInstructionBaseOffset,
		 * limit address to the first 4GBs for unflagged objects.
		 */
		if ((entry->flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS) == 0)
			flags |= PIN_ZONE_4G;
		if (entry->flags & __EXEC_OBJECT_NEEDS_MAP)
			flags |= PIN_GLOBAL | PIN_MAPPABLE;
		if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS)
			flags |= BATCH_OFFSET_BIAS | PIN_OFFSET_BIAS;
		if ((flags & PIN_MAPPABLE) == 0)
			flags |= PIN_HIGH;
	}

	ret = i915_gem_object_pin(obj, vma->vm, entry->alignment, flags);
	if ((ret == -ENOSPC  || ret == -E2BIG) &&
	    only_mappable_for_reloc(entry->flags))
		ret = i915_gem_object_pin(obj, vma->vm,
					  entry->alignment,
					  flags & ~PIN_MAPPABLE);
	if (ret)
		return ret;

	entry->flags |= __EXEC_OBJECT_HAS_PIN;

	if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
		ret = i915_gem_object_get_fence(obj);
		if (ret)
			return ret;

		if (i915_gem_object_pin_fence(obj))
			entry->flags |= __EXEC_OBJECT_HAS_FENCE;
	}

	if (entry->offset != vma->node.start) {
		entry->offset = vma->node.start;
		*need_reloc = true;
	}

	if (entry->flags & EXEC_OBJECT_WRITE) {
		obj->base.pending_read_domains = I915_GEM_DOMAIN_RENDER;
		obj->base.pending_write_domain = I915_GEM_DOMAIN_RENDER;
	}

	return 0;
}

static bool
need_reloc_mappable(struct i915_vma *vma)
{
	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;

	if (entry->relocation_count == 0)
		return false;

	if (!i915_is_ggtt(vma->vm))
		return false;

	/* See also use_cpu_reloc() */
	if (HAS_LLC(vma->obj->base.dev))
		return false;

	if (vma->obj->base.write_domain == I915_GEM_DOMAIN_CPU)
		return false;

	return true;
}

static bool
eb_vma_misplaced(struct i915_vma *vma)
{
	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
	struct drm_i915_gem_object *obj = vma->obj;

	WARN_ON(entry->flags & __EXEC_OBJECT_NEEDS_MAP &&
	       !i915_is_ggtt(vma->vm));

	if (entry->alignment &&
	    vma->node.start & (entry->alignment - 1))
		return true;

	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS &&
	    vma->node.start < BATCH_OFFSET_BIAS)
		return true;

	/* avoid costly ping-pong once a batch bo ended up non-mappable */
	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP && !obj->map_and_fenceable)
		return !only_mappable_for_reloc(entry->flags);

	if ((entry->flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS) == 0 &&
	    (vma->node.start + vma->node.size - 1) >> 32)
		return true;

	return false;
}

static int
i915_gem_execbuffer_reserve(struct intel_engine_cs *ring,
			    struct list_head *vmas,
			    struct intel_context *ctx,
			    bool *need_relocs)
{
	struct drm_i915_gem_object *obj;
	struct i915_vma *vma;
	struct i915_address_space *vm;
	struct list_head ordered_vmas;
	bool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;
	int retry;

	i915_gem_retire_requests_ring(ring);

	vm = list_first_entry(vmas, struct i915_vma, exec_list)->vm;

	INIT_LIST_HEAD(&ordered_vmas);
	while (!list_empty(vmas)) {
		struct drm_i915_gem_exec_object2 *entry;
		bool need_fence, need_mappable;

		vma = list_first_entry(vmas, struct i915_vma, exec_list);
		obj = vma->obj;
		entry = vma->exec_entry;

		if (ctx->flags & CONTEXT_NO_ZEROMAP)
			entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;

		if (!has_fenced_gpu_access)
			entry->flags &= ~EXEC_OBJECT_NEEDS_FENCE;
		need_fence =
			entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
			obj->tiling_mode != I915_TILING_NONE;
		need_mappable = need_fence || need_reloc_mappable(vma);

		if (need_mappable) {
			entry->flags |= __EXEC_OBJECT_NEEDS_MAP;
			list_move(&vma->exec_list, &ordered_vmas);
		} else
			list_move_tail(&vma->exec_list, &ordered_vmas);

		obj->base.pending_read_domains = I915_GEM_GPU_DOMAINS & ~I915_GEM_DOMAIN_COMMAND;
		obj->base.pending_write_domain = 0;
	}
	list_splice(&ordered_vmas, vmas);

	/* Attempt to pin all of the buffers into the GTT.
	 * This is done in 3 phases:
	 *
	 * 1a. Unbind all objects that do not match the GTT constraints for
	 *     the execbuffer (fenceable, mappable, alignment etc).
	 * 1b. Increment pin count for already bound objects.
	 * 2.  Bind new objects.
	 * 3.  Decrement pin count.
	 *
	 * This avoid unnecessary unbinding of later objects in order to make
	 * room for the earlier objects *unless* we need to defragment.
	 */
	retry = 0;
	do {
		int ret = 0;

		/* Unbind any ill-fitting objects or pin. */
		list_for_each_entry(vma, vmas, exec_list) {
			if (!drm_mm_node_allocated(&vma->node))
				continue;

			if (eb_vma_misplaced(vma))
				ret = i915_vma_unbind(vma);
			else
				ret = i915_gem_execbuffer_reserve_vma(vma, ring, need_relocs);
			if (ret)
				goto err;
		}

		/* Bind fresh objects */
		list_for_each_entry(vma, vmas, exec_list) {
			if (drm_mm_node_allocated(&vma->node))
				continue;

			ret = i915_gem_execbuffer_reserve_vma(vma, ring, need_relocs);
			if (ret)
				goto err;
		}

err:
		if (ret != -ENOSPC || retry++)
			return ret;

		/* Decrement pin count for bound objects */
		list_for_each_entry(vma, vmas, exec_list)
			i915_gem_execbuffer_unreserve_vma(vma);

		ret = i915_gem_evict_vm(vm, true);
		if (ret)
			return ret;
	} while (1);
}

static int
i915_gem_execbuffer_relocate_slow(struct drm_device *dev,
				  struct drm_i915_gem_execbuffer2 *args,
				  struct drm_file *file,
				  struct intel_engine_cs *ring,
				  struct eb_vmas *eb,
				  struct drm_i915_gem_exec_object2 *exec,
				  struct intel_context *ctx)
{
	struct drm_i915_gem_relocation_entry *reloc;
	struct i915_address_space *vm;
	struct i915_vma *vma;
	bool need_relocs;
	int *reloc_offset;
	int i, total, ret;
	unsigned count = args->buffer_count;

	vm = list_first_entry(&eb->vmas, struct i915_vma, exec_list)->vm;

	/* We may process another execbuffer during the unlock... */
	while (!list_empty(&eb->vmas)) {
		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_list);
		list_del_init(&vma->exec_list);
		i915_gem_execbuffer_unreserve_vma(vma);
		drm_gem_object_unreference(&vma->obj->base);
	}

	mutex_unlock(&dev->struct_mutex);

	total = 0;
	for (i = 0; i < count; i++)
		total += exec[i].relocation_count;

	reloc_offset = drm_malloc_ab(count, sizeof(*reloc_offset));
	reloc = drm_malloc_ab(total, sizeof(*reloc));
	if (reloc == NULL || reloc_offset == NULL) {
		drm_free_large(reloc);
		drm_free_large(reloc_offset);
		mutex_lock(&dev->struct_mutex);
		return -ENOMEM;
	}

	total = 0;
	for (i = 0; i < count; i++) {
		struct drm_i915_gem_relocation_entry __user *user_relocs;
		u64 invalid_offset = (u64)-1;
		int j;

		user_relocs = to_user_ptr(exec[i].relocs_ptr);

		if (copy_from_user(reloc+total, user_relocs,
				   exec[i].relocation_count * sizeof(*reloc))) {
			ret = -EFAULT;
			mutex_lock(&dev->struct_mutex);
			goto err;
		}

		/* As we do not update the known relocation offsets after
		 * relocating (due to the complexities in lock handling),
		 * we need to mark them as invalid now so that we force the
		 * relocation processing next time. Just in case the target
		 * object is evicted and then rebound into its old
		 * presumed_offset before the next execbuffer - if that
		 * happened we would make the mistake of assuming that the
		 * relocations were valid.
		 */
		for (j = 0; j < exec[i].relocation_count; j++) {
			if (__copy_to_user(&user_relocs[j].presumed_offset,
					   &invalid_offset,
					   sizeof(invalid_offset))) {
				ret = -EFAULT;
				mutex_lock(&dev->struct_mutex);
				goto err;
			}
		}

		reloc_offset[i] = total;
		total += exec[i].relocation_count;
	}

	ret = i915_mutex_lock_interruptible(dev);
	if (ret) {
		mutex_lock(&dev->struct_mutex);
		goto err;
	}

	/* reacquire the objects */
	eb_reset(eb);
	ret = eb_lookup_vmas(eb, exec, args, vm, file);
	if (ret)
		goto err;

	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, ctx, &need_relocs);
	if (ret)
		goto err;

	list_for_each_entry(vma, &eb->vmas, exec_list) {
		int offset = vma->exec_entry - exec;
		ret = i915_gem_execbuffer_relocate_vma_slow(vma, eb,
							    reloc + reloc_offset[offset]);
		if (ret)
			goto err;
	}

	/* Leave the user relocations as are, this is the painfully slow path,
	 * and we want to avoid the complication of dropping the lock whilst
	 * having buffers reserved in the aperture and so causing spurious
	 * ENOSPC for random operations.
	 */

err:
	drm_free_large(reloc);
	drm_free_large(reloc_offset);
	return ret;
}

static int
i915_gem_execbuffer_move_to_gpu(struct drm_i915_gem_request *req,
				struct list_head *vmas)
{
	const unsigned other_rings = ~intel_ring_flag(req->ring);
	struct i915_vma *vma;
	uint32_t flush_domains = 0;
	bool flush_chipset = false;
	int ret;

	list_for_each_entry(vma, vmas, exec_list) {
		struct drm_i915_gem_object *obj = vma->obj;

		if (obj->active & other_rings) {
			ret = i915_gem_object_sync(obj, req->ring, &req);
			if (ret)
				return ret;
		}

		if (obj->base.write_domain & I915_GEM_DOMAIN_CPU)
			flush_chipset |= i915_gem_clflush_object(obj, false);

		flush_domains |= obj->base.write_domain;
	}

	if (flush_chipset)
		i915_gem_chipset_flush(req->ring->dev);

	if (flush_domains & I915_GEM_DOMAIN_GTT)
		wmb();

	/* Unconditionally invalidate gpu caches and ensure that we do flush
	 * any residual writes from the previous batch.
	 */
	return intel_ring_invalidate_all_caches(req);
}

static bool
i915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)
{
	if (exec->flags & __I915_EXEC_UNKNOWN_FLAGS)
		return false;

#ifdef __linux__
	/* Kernel clipping was a DRI1 misfeature */
	if (exec->num_cliprects || exec->cliprects_ptr)
		return false;

	if (exec->DR4 == 0xffffffff) {
		DRM_DEBUG("UXA submitting garbage DR4, fixing up\n");
		exec->DR4 = 0;
	}
	if (exec->DR1 || exec->DR4)
		return false;
#endif

	if ((exec->batch_start_offset | exec->batch_len) & 0x7)
		return false;

	return true;
}

static int
validate_exec_list(struct drm_device *dev,
		   struct drm_i915_gem_exec_object2 *exec,
		   int count)
{
	unsigned relocs_total = 0;
	unsigned relocs_max = UINT_MAX / sizeof(struct drm_i915_gem_relocation_entry);
	unsigned invalid_flags;
	int i;

	invalid_flags = __EXEC_OBJECT_UNKNOWN_FLAGS;
	if (USES_FULL_PPGTT(dev))
		invalid_flags |= EXEC_OBJECT_NEEDS_GTT;

	for (i = 0; i < count; i++) {
		char __user *ptr = to_user_ptr(exec[i].relocs_ptr);
		int length; /* limited by fault_in_pages_readable() */

		if (exec[i].flags & invalid_flags)
			return -EINVAL;

		if (exec[i].alignment && !is_power_of_2(exec[i].alignment))
			return -EINVAL;

		/* First check for malicious input causing overflow in
		 * the worst case where we need to allocate the entire
		 * relocation tree as a single array.
		 */
		if (exec[i].relocation_count > relocs_max - relocs_total)
			return -EINVAL;
		relocs_total += exec[i].relocation_count;

		length = exec[i].relocation_count *
			sizeof(struct drm_i915_gem_relocation_entry);
		/*
		 * We must check that the entire relocation array is safe
		 * to read, but since we may need to update the presumed
		 * offsets during execution, check for full write access.
		 */
		if (!access_ok(VERIFY_WRITE, ptr, length))
			return -EFAULT;

#ifdef __linux__
		if (likely(!i915.prefault_disable)) {
			if (fault_in_multipages_readable(ptr, length))
				return -EFAULT;
		}
#endif
	}

	return 0;
}

static struct intel_context *
i915_gem_validate_context(struct drm_device *dev, struct drm_file *file,
			  struct intel_engine_cs *ring, const u32 ctx_id)
{
	struct intel_context *ctx = NULL;
	struct i915_ctx_hang_stats *hs;

	if (ring->id != RCS && ctx_id != DEFAULT_CONTEXT_HANDLE)
		return ERR_PTR(-EINVAL);

	ctx = i915_gem_context_get(file->driver_priv, ctx_id);
	if (IS_ERR(ctx))
		return ctx;

	hs = &ctx->hang_stats;
	if (hs->banned) {
		DRM_DEBUG("Context %u tried to submit while banned\n", ctx_id);
		return ERR_PTR(-EIO);
	}

	if (i915.enable_execlists && !ctx->engine[ring->id].state) {
		int ret = intel_lr_context_deferred_alloc(ctx, ring);
		if (ret) {
			DRM_DEBUG("Could not create LRC %u: %d\n", ctx_id, ret);
			return ERR_PTR(ret);
		}
	}

	return ctx;
}

void
i915_gem_execbuffer_move_to_active(struct list_head *vmas,
				   struct drm_i915_gem_request *req)
{
	struct intel_engine_cs *ring = i915_gem_request_get_ring(req);
	struct i915_vma *vma;

	list_for_each_entry(vma, vmas, exec_list) {
		struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
		struct drm_i915_gem_object *obj = vma->obj;
		u32 old_read = obj->base.read_domains;
		u32 old_write = obj->base.write_domain;

		obj->dirty = 1; /* be paranoid  */
		obj->base.write_domain = obj->base.pending_write_domain;
		if (obj->base.write_domain == 0)
			obj->base.pending_read_domains |= obj->base.read_domains;
		obj->base.read_domains = obj->base.pending_read_domains;

		i915_vma_move_to_active(vma, req);
		if (obj->base.write_domain) {
			i915_gem_request_assign(&obj->last_write_req, req);

			intel_fb_obj_invalidate(obj, ORIGIN_CS);

			/* update for the implicit flush after a batch */
			obj->base.write_domain &= ~I915_GEM_GPU_DOMAINS;
		}
		if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
			i915_gem_request_assign(&obj->last_fenced_req, req);
			if (entry->flags & __EXEC_OBJECT_HAS_FENCE) {
				struct drm_i915_private *dev_priv = to_i915(ring->dev);
				list_move_tail(&dev_priv->fence_regs[obj->fence_reg].lru_list,
					       &dev_priv->mm.fence_list);
			}
		}

		trace_i915_gem_object_change_domain(obj, old_read, old_write);
	}
}

void
i915_gem_execbuffer_retire_commands(struct i915_execbuffer_params *params)
{
	/* Unconditionally force add_request to emit a full flush. */
	params->ring->gpu_caches_dirty = true;

	/* Add a breadcrumb for the completion of the batch buffer */
	__i915_add_request(params->request, params->batch_obj, true);
}

static int
i915_reset_gen7_sol_offsets(struct drm_device *dev,
			    struct drm_i915_gem_request *req)
{
	struct intel_engine_cs *ring = req->ring;
	struct drm_i915_private *dev_priv = dev->dev_private;
	int ret, i;

	if (!IS_GEN7(dev) || ring != &dev_priv->ring[RCS]) {
		DRM_DEBUG("sol reset is gen7/rcs only\n");
		return -EINVAL;
	}

	ret = intel_ring_begin(req, 4 * 3);
	if (ret)
		return ret;

	for (i = 0; i < 4; i++) {
		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
		intel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));
		intel_ring_emit(ring, 0);
	}

	intel_ring_advance(ring);

	return 0;
}

static struct drm_i915_gem_object*
i915_gem_execbuffer_parse(struct intel_engine_cs *ring,
			  struct drm_i915_gem_exec_object2 *shadow_exec_entry,
			  struct eb_vmas *eb,
			  struct drm_i915_gem_object *batch_obj,
			  u32 batch_start_offset,
			  u32 batch_len,
			  bool is_master)
{
	struct drm_i915_gem_object *shadow_batch_obj;
	struct i915_vma *vma;
	int ret;

	shadow_batch_obj = i915_gem_batch_pool_get(&ring->batch_pool,
						   PAGE_ALIGN(batch_len));
	if (IS_ERR(shadow_batch_obj))
		return shadow_batch_obj;

	ret = i915_parse_cmds(ring,
			      batch_obj,
			      shadow_batch_obj,
			      batch_start_offset,
			      batch_len,
			      is_master);
	if (ret)
		goto err;

	ret = i915_gem_obj_ggtt_pin(shadow_batch_obj, 0, 0);
	if (ret)
		goto err;

	i915_gem_object_unpin_pages(shadow_batch_obj);

	memset(shadow_exec_entry, 0, sizeof(*shadow_exec_entry));

	vma = i915_gem_obj_to_ggtt(shadow_batch_obj);
	vma->exec_entry = shadow_exec_entry;
	vma->exec_entry->flags = __EXEC_OBJECT_HAS_PIN;
	drm_gem_object_reference(&shadow_batch_obj->base);
	list_add_tail(&vma->exec_list, &eb->vmas);

	shadow_batch_obj->base.pending_read_domains = I915_GEM_DOMAIN_COMMAND;

	return shadow_batch_obj;

err:
	i915_gem_object_unpin_pages(shadow_batch_obj);
	if (ret == -EACCES) /* unhandled chained batch */
		return batch_obj;
	else
		return ERR_PTR(ret);
}

int
i915_gem_ringbuffer_submission(struct i915_execbuffer_params *params,
			       struct drm_i915_gem_execbuffer2 *args,
			       struct list_head *vmas)
{
	struct drm_device *dev = params->dev;
	struct intel_engine_cs *ring = params->ring;
	struct drm_i915_private *dev_priv = dev->dev_private;
	u64 exec_start, exec_len;
	int instp_mode;
	u32 instp_mask;
	int ret;

	ret = i915_gem_execbuffer_move_to_gpu(params->request, vmas);
	if (ret)
		return ret;

	ret = i915_switch_context(params->request);
	if (ret)
		return ret;

	WARN(params->ctx->ppgtt && params->ctx->ppgtt->pd_dirty_rings & (1<<ring->id),
	     "%s didn't clear reload\n", ring->name);

	instp_mode = args->flags & I915_EXEC_CONSTANTS_MASK;
	instp_mask = I915_EXEC_CONSTANTS_MASK;
	switch (instp_mode) {
	case I915_EXEC_CONSTANTS_REL_GENERAL:
	case I915_EXEC_CONSTANTS_ABSOLUTE:
	case I915_EXEC_CONSTANTS_REL_SURFACE:
		if (instp_mode != 0 && ring != &dev_priv->ring[RCS]) {
			DRM_DEBUG("non-0 rel constants mode on non-RCS\n");
			return -EINVAL;
		}

		if (instp_mode != dev_priv->relative_constants_mode) {
			if (INTEL_INFO(dev)->gen < 4) {
				DRM_DEBUG("no rel constants on pre-gen4\n");
				return -EINVAL;
			}

			if (INTEL_INFO(dev)->gen > 5 &&
			    instp_mode == I915_EXEC_CONSTANTS_REL_SURFACE) {
				DRM_DEBUG("rel surface constants mode invalid on gen5+\n");
				return -EINVAL;
			}

			/* The HW changed the meaning on this bit on gen6 */
			if (INTEL_INFO(dev)->gen >= 6)
				instp_mask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;
		}
		break;
	default:
		DRM_DEBUG("execbuf with unknown constants: %d\n", instp_mode);
		return -EINVAL;
	}

	if (ring == &dev_priv->ring[RCS] &&
	    instp_mode != dev_priv->relative_constants_mode) {
		ret = intel_ring_begin(params->request, 4);
		if (ret)
			return ret;

		intel_ring_emit(ring, MI_NOOP);
		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
		intel_ring_emit(ring, INSTPM);
		intel_ring_emit(ring, instp_mask << 16 | instp_mode);
		intel_ring_advance(ring);

		dev_priv->relative_constants_mode = instp_mode;
	}

	if (args->flags & I915_EXEC_GEN7_SOL_RESET) {
		ret = i915_reset_gen7_sol_offsets(dev, params->request);
		if (ret)
			return ret;
	}

	exec_len   = args->batch_len;
	exec_start = params->batch_obj_vm_offset +
		     params->args_batch_start_offset;

	ret = ring->dispatch_execbuffer(params->request,
					exec_start, exec_len,
					params->dispatch_flags);
	if (ret)
		return ret;

	trace_i915_gem_ring_dispatch(params->request, params->dispatch_flags);

	i915_gem_execbuffer_move_to_active(vmas, params->request);
	i915_gem_execbuffer_retire_commands(params);

	return 0;
}

/**
 * Find one BSD ring to dispatch the corresponding BSD command.
 * The Ring ID is returned.
 */
static int gen8_dispatch_bsd_ring(struct drm_device *dev,
				  struct drm_file *file)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct drm_i915_file_private *file_priv = file->driver_priv;

	/* Check whether the file_priv is using one ring */
	if (file_priv->bsd_ring)
		return file_priv->bsd_ring->id;
	else {
		/* If no, use the ping-pong mechanism to select one ring */
		int ring_id;

		mutex_lock(&dev->struct_mutex);
		if (dev_priv->mm.bsd_ring_dispatch_index == 0) {
			ring_id = VCS;
			dev_priv->mm.bsd_ring_dispatch_index = 1;
		} else {
			ring_id = VCS2;
			dev_priv->mm.bsd_ring_dispatch_index = 0;
		}
		file_priv->bsd_ring = &dev_priv->ring[ring_id];
		mutex_unlock(&dev->struct_mutex);
		return ring_id;
	}
}

static struct drm_i915_gem_object *
eb_get_batch(struct eb_vmas *eb)
{
	struct i915_vma *vma = list_entry(eb->vmas.prev, typeof(*vma), exec_list);

	/*
	 * SNA is doing fancy tricks with compressing batch buffers, which leads
	 * to negative relocation deltas. Usually that works out ok since the
	 * relocate address is still positive, except when the batch is placed
	 * very low in the GTT. Ensure this doesn't happen.
	 *
	 * Note that actual hangs have only been observed on gen7, but for
	 * paranoia do it everywhere.
	 */
	vma->exec_entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;

	return vma->obj;
}

static int
i915_gem_do_execbuffer(struct drm_device *dev, void *data,
		       struct drm_file *file,
		       struct drm_i915_gem_execbuffer2 *args,
		       struct drm_i915_gem_exec_object2 *exec)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	struct eb_vmas *eb;
	struct drm_i915_gem_object *batch_obj;
	struct drm_i915_gem_exec_object2 shadow_exec_entry;
	struct intel_engine_cs *ring;
	struct intel_context *ctx;
	struct i915_address_space *vm;
	struct i915_execbuffer_params params_master; /* XXX: will be removed later */
	struct i915_execbuffer_params *params = &params_master;
	const u32 ctx_id = i915_execbuffer2_get_context_id(*args);
	u32 dispatch_flags;
	int ret;
	bool need_relocs;

	if (!i915_gem_check_execbuffer(args))
		return -EINVAL;

	ret = validate_exec_list(dev, exec, args->buffer_count);
	if (ret)
		return ret;

	dispatch_flags = 0;
	if (args->flags & I915_EXEC_SECURE) {
		if (!file->is_master || !capable(CAP_SYS_ADMIN))
		    return -EPERM;

		dispatch_flags |= I915_DISPATCH_SECURE;
	}
	if (args->flags & I915_EXEC_IS_PINNED)
		dispatch_flags |= I915_DISPATCH_PINNED;

	if ((args->flags & I915_EXEC_RING_MASK) > LAST_USER_RING) {
		DRM_DEBUG("execbuf with unknown ring: %d\n",
			  (int)(args->flags & I915_EXEC_RING_MASK));
		return -EINVAL;
	}

	if (((args->flags & I915_EXEC_RING_MASK) != I915_EXEC_BSD) &&
	    ((args->flags & I915_EXEC_BSD_MASK) != 0)) {
		DRM_DEBUG("execbuf with non bsd ring but with invalid "
			"bsd dispatch flags: %d\n", (int)(args->flags));
		return -EINVAL;
	} 

	if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_DEFAULT)
		ring = &dev_priv->ring[RCS];
	else if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_BSD) {
		if (HAS_BSD2(dev)) {
			int ring_id;

			switch (args->flags & I915_EXEC_BSD_MASK) {
			case I915_EXEC_BSD_DEFAULT:
				ring_id = gen8_dispatch_bsd_ring(dev, file);
				ring = &dev_priv->ring[ring_id];
				break;
			case I915_EXEC_BSD_RING1:
				ring = &dev_priv->ring[VCS];
				break;
			case I915_EXEC_BSD_RING2:
				ring = &dev_priv->ring[VCS2];
				break;
			default:
				DRM_DEBUG("execbuf with unknown bsd ring: %d\n",
					  (int)(args->flags & I915_EXEC_BSD_MASK));
				return -EINVAL;
			}
		} else
			ring = &dev_priv->ring[VCS];
	} else
		ring = &dev_priv->ring[(args->flags & I915_EXEC_RING_MASK) - 1];

	if (!intel_ring_initialized(ring)) {
		DRM_DEBUG("execbuf with invalid ring: %d\n",
			  (int)(args->flags & I915_EXEC_RING_MASK));
		return -EINVAL;
	}

	if (args->buffer_count < 1) {
		DRM_DEBUG("execbuf with %d buffers\n", args->buffer_count);
		return -EINVAL;
	}

	if (args->flags & I915_EXEC_RESOURCE_STREAMER) {
		if (!HAS_RESOURCE_STREAMER(dev)) {
			DRM_DEBUG("RS is only allowed for Haswell, Gen8 and above\n");
			return -EINVAL;
		}
		if (ring->id != RCS) {
			DRM_DEBUG("RS is not available on %s\n",
				 ring->name);
			return -EINVAL;
		}

		dispatch_flags |= I915_DISPATCH_RS;
	}

	intel_runtime_pm_get(dev_priv);

	ret = i915_mutex_lock_interruptible(dev);
	if (ret)
		goto pre_mutex_err;

	ctx = i915_gem_validate_context(dev, file, ring, ctx_id);
	if (IS_ERR(ctx)) {
		mutex_unlock(&dev->struct_mutex);
		ret = PTR_ERR(ctx);
		goto pre_mutex_err;
	}

	i915_gem_context_reference(ctx);

	if (ctx->ppgtt)
		vm = &ctx->ppgtt->base;
	else
		vm = &dev_priv->gtt.base;

	memset(&params_master, 0x00, sizeof(params_master));

	eb = eb_create(args);
	if (eb == NULL) {
		i915_gem_context_unreference(ctx);
		mutex_unlock(&dev->struct_mutex);
		ret = -ENOMEM;
		goto pre_mutex_err;
	}

	/* Look up object handles */
	ret = eb_lookup_vmas(eb, exec, args, vm, file);
	if (ret)
		goto err;

	/* take note of the batch buffer before we might reorder the lists */
	batch_obj = eb_get_batch(eb);

	/* Move the objects en-masse into the GTT, evicting if necessary. */
	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, ctx, &need_relocs);
	if (ret)
		goto err;

	/* The objects are in their final locations, apply the relocations. */
	if (need_relocs)
		ret = i915_gem_execbuffer_relocate(eb);
	if (ret) {
		if (ret == -EFAULT) {
			ret = i915_gem_execbuffer_relocate_slow(dev, args, file, ring,
								eb, exec, ctx);
			BUG_ON(!mutex_is_locked(&dev->struct_mutex));
		}
		if (ret)
			goto err;
	}

	/* Set the pending read domains for the batch buffer to COMMAND */
	if (batch_obj->base.pending_write_domain) {
		DRM_DEBUG("Attempting to use self-modifying batch buffer\n");
		ret = -EINVAL;
		goto err;
	}

	params->args_batch_start_offset = args->batch_start_offset;
	if (i915_needs_cmd_parser(ring) && args->batch_len) {
		struct drm_i915_gem_object *parsed_batch_obj;

		parsed_batch_obj = i915_gem_execbuffer_parse(ring,
						      &shadow_exec_entry,
						      eb,
						      batch_obj,
						      args->batch_start_offset,
						      args->batch_len,
						      file->is_master);
		if (IS_ERR(parsed_batch_obj)) {
			ret = PTR_ERR(parsed_batch_obj);
			goto err;
		}

		/*
		 * parsed_batch_obj == batch_obj means batch not fully parsed:
		 * Accept, but don't promote to secure.
		 */

		if (parsed_batch_obj != batch_obj) {
			/*
			 * Batch parsed and accepted:
			 *
			 * Set the DISPATCH_SECURE bit to remove the NON_SECURE
			 * bit from MI_BATCH_BUFFER_START commands issued in
			 * the dispatch_execbuffer implementations. We
			 * specifically don't want that set on batches the
			 * command parser has accepted.
			 */
			dispatch_flags |= I915_DISPATCH_SECURE;
			params->args_batch_start_offset = 0;
			batch_obj = parsed_batch_obj;
		}
	}

	batch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;

	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
	 * batch" bit. Hence we need to pin secure batches into the global gtt.
	 * hsw should have this fixed, but bdw mucks it up again. */
	if (dispatch_flags & I915_DISPATCH_SECURE) {
		/*
		 * So on first glance it looks freaky that we pin the batch here
		 * outside of the reservation loop. But:
		 * - The batch is already pinned into the relevant ppgtt, so we
		 *   already have the backing storage fully allocated.
		 * - No other BO uses the global gtt (well contexts, but meh),
		 *   so we don't really have issues with multiple objects not
		 *   fitting due to fragmentation.
		 * So this is actually safe.
		 */
		ret = i915_gem_obj_ggtt_pin(batch_obj, 0, 0);
		if (ret)
			goto err;

		params->batch_obj_vm_offset = i915_gem_obj_ggtt_offset(batch_obj);
	} else
		params->batch_obj_vm_offset = i915_gem_obj_offset(batch_obj, vm);

	/* Allocate a request for this batch buffer nice and early. */
	ret = i915_gem_request_alloc(ring, ctx, &params->request);
	if (ret)
		goto err_batch_unpin;

	ret = i915_gem_request_add_to_client(params->request, file);
	if (ret)
		goto err_batch_unpin;

	/*
	 * Save assorted stuff away to pass through to *_submission().
	 * NB: This data should be 'persistent' and not local as it will
	 * kept around beyond the duration of the IOCTL once the GPU
	 * scheduler arrives.
	 */
	params->dev                     = dev;
	params->file                    = file;
	params->ring                    = ring;
	params->dispatch_flags          = dispatch_flags;
	params->batch_obj               = batch_obj;
	params->ctx                     = ctx;

	ret = dev_priv->gt.execbuf_submit(params, args, &eb->vmas);

err_batch_unpin:
	/*
	 * FIXME: We crucially rely upon the active tracking for the (ppgtt)
	 * batch vma for correctness. For less ugly and less fragility this
	 * needs to be adjusted to also track the ggtt batch vma properly as
	 * active.
	 */
	if (dispatch_flags & I915_DISPATCH_SECURE)
		i915_gem_object_ggtt_unpin(batch_obj);

err:
	/* the request owns the ref now */
	i915_gem_context_unreference(ctx);
	eb_destroy(eb);

	/*
	 * If the request was created but not successfully submitted then it
	 * must be freed again. If it was submitted then it is being tracked
	 * on the active request list and no clean up is required here.
	 */
	if (ret && params->request)
		i915_gem_request_cancel(params->request);

	mutex_unlock(&dev->struct_mutex);

pre_mutex_err:
	/* intel_gpu_busy should also get a ref, so it will free when the device
	 * is really idle. */
	intel_runtime_pm_put(dev_priv);
	return ret;
}

#ifdef __linux__
/*
 * Legacy execbuffer just creates an exec2 list from the original exec object
 * list array and passes it to the real function.
 */
int
i915_gem_execbuffer(struct drm_device *dev, void *data,
		    struct drm_file *file)
{
	struct drm_i915_gem_execbuffer *args = data;
	struct drm_i915_gem_execbuffer2 exec2;
	struct drm_i915_gem_exec_object *exec_list = NULL;
	struct drm_i915_gem_exec_object2 *exec2_list = NULL;
	int ret, i;

	if (args->buffer_count < 1) {
		DRM_DEBUG("execbuf with %d buffers\n", args->buffer_count);
		return -EINVAL;
	}

	/* Copy in the exec list from userland */
	exec_list = drm_malloc_ab(sizeof(*exec_list), args->buffer_count);
	exec2_list = drm_malloc_ab(sizeof(*exec2_list), args->buffer_count);
	if (exec_list == NULL || exec2_list == NULL) {
		DRM_DEBUG("Failed to allocate exec list for %d buffers\n",
			  args->buffer_count);
		drm_free_large(exec_list);
		drm_free_large(exec2_list);
		return -ENOMEM;
	}
	ret = copy_from_user(exec_list,
			     to_user_ptr(args->buffers_ptr),
			     sizeof(*exec_list) * args->buffer_count);
	if (ret != 0) {
		DRM_DEBUG("copy %d exec entries failed %d\n",
			  args->buffer_count, ret);
		drm_free_large(exec_list);
		drm_free_large(exec2_list);
		return -EFAULT;
	}

	for (i = 0; i < args->buffer_count; i++) {
		exec2_list[i].handle = exec_list[i].handle;
		exec2_list[i].relocation_count = exec_list[i].relocation_count;
		exec2_list[i].relocs_ptr = exec_list[i].relocs_ptr;
		exec2_list[i].alignment = exec_list[i].alignment;
		exec2_list[i].offset = exec_list[i].offset;
		if (INTEL_INFO(dev)->gen < 4)
			exec2_list[i].flags = EXEC_OBJECT_NEEDS_FENCE;
		else
			exec2_list[i].flags = 0;
	}

	exec2.buffers_ptr = args->buffers_ptr;
	exec2.buffer_count = args->buffer_count;
	exec2.batch_start_offset = args->batch_start_offset;
	exec2.batch_len = args->batch_len;
	exec2.DR1 = args->DR1;
	exec2.DR4 = args->DR4;
	exec2.num_cliprects = args->num_cliprects;
	exec2.cliprects_ptr = args->cliprects_ptr;
	exec2.flags = I915_EXEC_RENDER;
	i915_execbuffer2_set_context_id(exec2, 0);

	ret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);
	if (!ret) {
		struct drm_i915_gem_exec_object __user *user_exec_list =
			to_user_ptr(args->buffers_ptr);

		/* Copy the new buffer offsets back to the user's exec list. */
		for (i = 0; i < args->buffer_count; i++) {
			ret = __copy_to_user(&user_exec_list[i].offset,
					     &exec2_list[i].offset,
					     sizeof(user_exec_list[i].offset));
			if (ret) {
				ret = -EFAULT;
				DRM_DEBUG("failed to copy %d exec entries "
					  "back to user (%d)\n",
					  args->buffer_count, ret);
				break;
			}
		}
	}

	drm_free_large(exec_list);
	drm_free_large(exec2_list);
	return ret;
}
#endif /* __linux__ */

int
i915_gem_execbuffer2(struct drm_device *dev, void *data,
		     struct drm_file *file)
{
	struct drm_i915_gem_execbuffer2 *args = data;
	struct drm_i915_gem_exec_object2 *exec2_list = NULL;
	int ret;

	if (args->buffer_count < 1 ||
	    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {
		DRM_DEBUG("execbuf2 with %d buffers\n", args->buffer_count);
		return -EINVAL;
	}

	if (args->rsvd2 != 0) {
		DRM_DEBUG("dirty rvsd2 field\n");
		return -EINVAL;
	}

	exec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,
			     GFP_TEMPORARY | __GFP_NOWARN | __GFP_NORETRY);
	if (exec2_list == NULL)
		exec2_list = drm_malloc_ab(sizeof(*exec2_list),
					   args->buffer_count);
	if (exec2_list == NULL) {
		DRM_DEBUG("Failed to allocate exec list for %d buffers\n",
			  args->buffer_count);
		return -ENOMEM;
	}
	ret = copy_from_user(exec2_list,
			     to_user_ptr(args->buffers_ptr),
			     sizeof(*exec2_list) * args->buffer_count);
	if (ret != 0) {
		DRM_DEBUG("copy %d exec entries failed %d\n",
			  args->buffer_count, ret);
		drm_free_large(exec2_list);
		return -EFAULT;
	}

	ret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);
	if (!ret) {
		/* Copy the new buffer offsets back to the user's exec list. */
		struct drm_i915_gem_exec_object2 __user *user_exec_list =
				   to_user_ptr(args->buffers_ptr);
		int i;

		for (i = 0; i < args->buffer_count; i++) {
			ret = __copy_to_user(&user_exec_list[i].offset,
					     &exec2_list[i].offset,
					     sizeof(user_exec_list[i].offset));
			if (ret) {
				ret = -EFAULT;
				DRM_DEBUG("failed to copy %d exec entries "
					  "back to user\n",
					  args->buffer_count);
				break;
			}
		}
	}

	drm_free_large(exec2_list);
	return ret;
}
@


1.41
log
@Get rid of some infrastrcuture that is now obsolete and synchronize some of
the data structures in drmP.h with Linux 3.14.

ok jsg@@
@
text
@a0 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.40 2016/04/05 20:46:45 kettenis Exp $	*/
a29 1
#include <dev/pci/drm/drm.h>
d34 4
d41 4
d227 1
a227 1
		i915_gem_object_unpin(obj);
a250 1
		!obj->map_and_fenceable ||
d256 2
a257 1
		   struct drm_i915_gem_relocation_entry *reloc)
d261 1
d271 1
a271 1
	*(uint32_t *)(vaddr + page_offset) = reloc->delta;
d282 1
a282 1
		*(uint32_t *)(vaddr + page_offset) = 0;
d292 2
a293 1
		   struct drm_i915_gem_relocation_entry *reloc)
d297 2
a298 1
	uint32_t __iomem *reloc_entry;
d312 2
a313 1
	reloc->offset += i915_gem_obj_ggtt_offset(obj);
d316 1
a316 1
			reloc->offset & PAGE_MASK);
d318 1
a318 1
	agp_map_atomic(dev_priv->agph, trunc_page(reloc->offset), &bsh);
d321 1
a321 3
	reloc_entry = (uint32_t __iomem *)
		(reloc_page + offset_in_page(reloc->offset));
	iowrite32(reloc->delta, reloc_entry);
d324 1
a324 1
		reloc_entry += 1;
d326 1
a326 1
		if (offset_in_page(reloc->offset + sizeof(uint32_t)) == 0) {
d329 3
a331 3
			reloc_page = io_mapping_map_atomic_wc(
					dev_priv->gtt.mappable,
					reloc->offset + sizeof(uint32_t));
d334 1
a334 1
			agp_map_atomic(dev_priv->agph, reloc->offset + sizeof(uint32_t), &bsh);
a336 1
			reloc_entry = reloc_page;
d339 2
a340 1
		iowrite32(0, reloc_entry);
d352 45
d400 1
a400 2
				   struct drm_i915_gem_relocation_entry *reloc,
				   struct i915_address_space *vm)
d406 1
a406 1
	uint32_t target_offset;
d422 5
a426 4
	    reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION &&
	    !target_i915_obj->has_global_gtt_mapping)) {
		i915_gem_gtt_bind_object(target_i915_obj,
					 target_i915_obj->cache_level);
d480 1
a480 1
	if (obj->active && in_atomic())
a482 1
	reloc->delta += target_offset;
d484 9
a492 3
		ret = relocate_entry_cpu(obj, reloc);
	else
		ret = relocate_entry_gtt(obj, reloc);
d529 1
a529 2
			ret = i915_gem_execbuffer_relocate_entry(vma->obj, eb, r,
								 vma->vm);
d558 1
a558 2
		ret = i915_gem_execbuffer_relocate_entry(vma->obj, eb, &relocs[i],
							 vma->vm);
d590 1
a590 2
static int
need_reloc_mappable(struct i915_vma *vma)
d592 2
a593 3
	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
	return entry->relocation_count && !use_cpu_reloc(vma->obj) &&
		i915_is_ggtt(vma->vm);
d598 1
a598 1
				struct intel_ring_buffer *ring,
d601 1
a601 1
	struct drm_i915_private *dev_priv = ring->dev->dev_private;
d603 1
a603 3
	bool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;
	bool need_fence, need_mappable;
	struct drm_i915_gem_object *obj = vma->obj;
d606 24
a629 8
	need_fence =
		has_fenced_gpu_access &&
		entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
		obj->tiling_mode != I915_TILING_NONE;
	need_mappable = need_fence || need_reloc_mappable(vma);

	ret = i915_gem_object_pin(obj, vma->vm, entry->alignment, need_mappable,
				  false);
d635 4
a638 8
	if (has_fenced_gpu_access) {
		if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
			ret = i915_gem_object_get_fence(obj);
			if (ret)
				return ret;

			if (i915_gem_object_pin_fence(obj))
				entry->flags |= __EXEC_OBJECT_HAS_FENCE;
d640 2
a641 10
			obj->pending_fenced_gpu_access = true;
		}
	}

	/* Ensure ppgtt mapping exists if needed */
	if (dev_priv->mm.aliasing_ppgtt && !obj->has_aliasing_ppgtt_mapping) {
		i915_ppgtt_bind_object(dev_priv->mm.aliasing_ppgtt,
				       obj, obj->cache_level);

		obj->has_aliasing_ppgtt_mapping = 1;
d654 23
a676 3
	if (entry->flags & EXEC_OBJECT_NEEDS_GTT &&
	    !obj->has_global_gtt_mapping)
		i915_gem_gtt_bind_object(obj, obj->cache_level);
d678 26
a703 1
	return 0;
d707 1
a707 1
i915_gem_execbuffer_reserve(struct intel_ring_buffer *ring,
d709 1
d719 1
a719 2
	if (list_empty(vmas))
		return 0;
d732 5
a737 1
			has_fenced_gpu_access &&
d742 2
a743 1
		if (need_mappable)
d745 1
a745 1
		else
a749 1
		obj->pending_fenced_gpu_access = false;
a770 5
			struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
			bool need_fence, need_mappable;

			obj = vma->obj;

d774 1
a774 12
			need_fence =
				has_fenced_gpu_access &&
				entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
				obj->tiling_mode != I915_TILING_NONE;
			need_mappable = need_fence || need_reloc_mappable(vma);

			WARN_ON((need_mappable || need_fence) &&
			       !i915_is_ggtt(vma->vm));

			if ((entry->alignment &&
			     vma->node.start & (entry->alignment - 1)) ||
			    (need_mappable && !obj->map_and_fenceable))
d810 1
a810 1
				  struct intel_ring_buffer *ring,
d812 2
a813 1
				  struct drm_i915_gem_exec_object2 *exec)
a822 3
	if (WARN_ON(list_empty(&eb->vmas)))
		return 0;

d899 1
a899 1
	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
d924 1
a924 1
i915_gem_execbuffer_move_to_gpu(struct intel_ring_buffer *ring,
d927 1
d935 6
a940 3
		ret = i915_gem_object_sync(obj, ring);
		if (ret)
			return ret;
d949 1
a949 1
		i915_gem_chipset_flush(ring->dev);
d957 1
a957 1
	return intel_ring_invalidate_all_caches(ring);
d966 17
a982 1
	return ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;
d986 2
a987 1
validate_exec_list(struct drm_i915_gem_exec_object2 *exec,
a989 1
	int i;
d992 6
d1003 4
a1006 1
		if (exec[i].flags & __EXEC_OBJECT_UNKNOWN_FLAGS)
d1028 1
a1028 1
		if (likely(!i915_prefault_disable)) {
d1038 1
a1038 1
static int
d1040 1
a1040 1
			  const u32 ctx_id)
d1042 1
d1045 6
a1050 3
	hs = i915_gem_context_get_hang_stats(dev, file, ctx_id);
	if (IS_ERR(hs))
		return PTR_ERR(hs);
d1052 1
d1055 1
a1055 1
		return -EIO;
d1058 9
a1066 1
	return 0;
d1069 1
a1069 1
static void
d1071 1
a1071 1
				   struct intel_ring_buffer *ring)
d1073 1
d1077 1
d1082 1
a1086 1
		obj->fenced_gpu_access = obj->pending_fenced_gpu_access;
d1088 1
a1088 1
		i915_vma_move_to_active(vma, ring);
d1090 14
a1103 4
			obj->dirty = 1;
			obj->last_write_seqno = intel_ring_get_seqno(ring);
			if (obj->pin_count) /* check for potential scanout */
				intel_mark_fb_busy(obj, ring);
d1110 2
a1111 5
static void
i915_gem_execbuffer_retire_commands(struct drm_device *dev,
				    struct drm_file *file,
				    struct intel_ring_buffer *ring,
				    struct drm_i915_gem_object *obj)
d1114 1
a1114 1
	ring->gpu_caches_dirty = true;
d1117 1
a1117 1
	(void)__i915_add_request(ring, file, obj, NULL);
d1122 1
a1122 1
			    struct intel_ring_buffer *ring)
d1124 2
a1125 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1128 4
a1131 2
	if (!IS_GEN7(dev) || ring != &dev_priv->ring[RCS])
		return 0;
d1133 1
a1133 1
	ret = intel_ring_begin(ring, 4 * 3);
d1148 199
d1351 1
a1351 2
		       struct drm_i915_gem_exec_object2 *exec,
		       struct i915_address_space *vm)
d1353 1
a1353 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d1356 6
a1361 2
	struct drm_clip_rect *cliprects = NULL;
	struct intel_ring_buffer *ring;
d1363 2
a1364 3
	u32 exec_start, exec_len;
	u32 mask, flags;
	int ret, mode;
d1370 1
a1370 1
	ret = validate_exec_list(exec, args->buffer_count);
d1374 1
a1374 1
	flags = 0;
d1379 1
a1379 1
		flags |= I915_DISPATCH_SECURE;
d1382 1
a1382 31
		flags |= I915_DISPATCH_PINNED;

	switch (args->flags & I915_EXEC_RING_MASK) {
	case I915_EXEC_DEFAULT:
	case I915_EXEC_RENDER:
		ring = &dev_priv->ring[RCS];
		break;
	case I915_EXEC_BSD:
		ring = &dev_priv->ring[VCS];
		if (ctx_id != DEFAULT_CONTEXT_ID) {
			DRM_DEBUG("Ring %s doesn't support contexts\n",
				  ring->name);
			return -EPERM;
		}
		break;
	case I915_EXEC_BLT:
		ring = &dev_priv->ring[BCS];
		if (ctx_id != DEFAULT_CONTEXT_ID) {
			DRM_DEBUG("Ring %s doesn't support contexts\n",
				  ring->name);
			return -EPERM;
		}
		break;
	case I915_EXEC_VEBOX:
		ring = &dev_priv->ring[VECS];
		if (ctx_id != DEFAULT_CONTEXT_ID) {
			DRM_DEBUG("Ring %s doesn't support contexts\n",
				  ring->name);
			return -EPERM;
		}
		break;
d1384 1
a1384 1
	default:
d1389 5
a1393 3
	if (!intel_ring_initialized(ring)) {
		DRM_DEBUG("execbuf with invalid ring: %d\n",
			  (int)(args->flags & I915_EXEC_RING_MASK));
d1395 1
a1395 1
	}
d1397 20
a1416 9
	mode = args->flags & I915_EXEC_CONSTANTS_MASK;
	mask = I915_EXEC_CONSTANTS_MASK;
	switch (mode) {
	case I915_EXEC_CONSTANTS_REL_GENERAL:
	case I915_EXEC_CONSTANTS_ABSOLUTE:
	case I915_EXEC_CONSTANTS_REL_SURFACE:
		if (ring == &dev_priv->ring[RCS] &&
		    mode != dev_priv->relative_constants_mode) {
			if (INTEL_INFO(dev)->gen < 4)
d1418 5
d1424 3
a1426 11
			if (INTEL_INFO(dev)->gen > 5 &&
			    mode == I915_EXEC_CONSTANTS_REL_SURFACE)
				return -EINVAL;

			/* The HW changed the meaning on this bit on gen6 */
			if (INTEL_INFO(dev)->gen >= 6)
				mask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;
		}
		break;
	default:
		DRM_DEBUG("execbuf with unknown constants: %d\n", mode);
d1435 3
a1437 4
#ifdef __linux__
	if (args->num_cliprects != 0) {
		if (ring != &dev_priv->ring[RCS]) {
			DRM_DEBUG("clip rectangles are only valid with the render ring\n");
d1440 3
a1442 3

		if (INTEL_INFO(dev)->gen >= 5) {
			DRM_DEBUG("clip rectangles are only valid on pre-gen5\n");
d1446 1
a1446 20
		if (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {
			DRM_DEBUG("execbuf with %u cliprects\n",
				  args->num_cliprects);
			return -EINVAL;
		}

		cliprects = kcalloc(args->num_cliprects,
				    sizeof(*cliprects),
				    GFP_KERNEL);
		if (cliprects == NULL) {
			ret = -ENOMEM;
			goto pre_mutex_err;
		}

		if (copy_from_user(cliprects,
				   to_user_ptr(args->cliprects_ptr),
				   sizeof(*cliprects)*args->num_cliprects)) {
			ret = -EFAULT;
			goto pre_mutex_err;
		}
a1447 1
#endif
d1455 2
a1456 1
	if (dev_priv->ums.mm_suspended) {
d1458 1
a1458 1
		ret = -EBUSY;
d1462 8
a1469 5
	ret = i915_gem_validate_context(dev, file, ctx_id);
	if (ret) {
		mutex_unlock(&dev->struct_mutex);
		goto pre_mutex_err;
	}
d1473 1
d1485 1
a1485 1
	batch_obj = list_entry(eb->vmas.prev, struct i915_vma, exec_list)->obj;
d1489 1
a1489 1
	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
d1499 1
a1499 1
								eb, exec);
d1512 38
d1555 18
a1572 2
	if (flags & I915_DISPATCH_SECURE && !batch_obj->has_global_gtt_mapping)
		i915_gem_gtt_bind_object(batch_obj, batch_obj->cache_level);
d1574 2
a1575 1
	ret = i915_gem_execbuffer_move_to_gpu(ring, &eb->vmas);
d1577 1
a1577 1
		goto err;
d1579 1
a1579 1
	ret = i915_switch_context(ring, file, ctx_id);
d1581 1
a1581 1
		goto err;
d1583 12
a1594 5
	if (ring == &dev_priv->ring[RCS] &&
	    mode != dev_priv->relative_constants_mode) {
		ret = intel_ring_begin(ring, 4);
		if (ret)
				goto err;
d1596 1
a1596 5
		intel_ring_emit(ring, MI_NOOP);
		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
		intel_ring_emit(ring, INSTPM);
		intel_ring_emit(ring, mask << 16 | mode);
		intel_ring_advance(ring);
d1598 9
a1606 41
		dev_priv->relative_constants_mode = mode;
	}

	if (args->flags & I915_EXEC_GEN7_SOL_RESET) {
		ret = i915_reset_gen7_sol_offsets(dev, ring);
		if (ret)
			goto err;
	}

	exec_start = i915_gem_obj_offset(batch_obj, vm) +
		args->batch_start_offset;
	exec_len = args->batch_len;
#ifdef __linux_
	if (cliprects) {
		for (i = 0; i < args->num_cliprects; i++) {
			ret = i915_emit_box(dev, &cliprects[i],
					    args->DR1, args->DR4);
			if (ret)
				goto err;

			ret = ring->dispatch_execbuffer(ring,
							exec_start, exec_len,
							flags);
			if (ret)
				goto err;
		}
	} else {
#endif
		ret = ring->dispatch_execbuffer(ring,
						exec_start, exec_len,
						flags);
		if (ret)
			goto err;
#ifdef __linux__
	}
#endif

	trace_i915_gem_ring_dispatch(ring, intel_ring_get_seqno(ring), flags);

	i915_gem_execbuffer_move_to_active(&eb->vmas, ring);
	i915_gem_execbuffer_retire_commands(dev, file, ring, batch_obj);
d1609 2
d1613 8
a1623 2
	kfree(cliprects);

a1638 1
	struct drm_i915_private *dev_priv = dev->dev_private;
d1694 1
a1694 2
	ret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list,
				     &dev_priv->gtt.base);
a1723 1
	struct drm_i915_private *dev_priv = dev->dev_private;
d1734 5
d1759 1
a1759 2
	ret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list,
				     &dev_priv->gtt.base);
d1762 1
a1762 1
		struct drm_i915_gem_exec_object2 *user_exec_list =
@


1.40
log
@Now that we the "idr" API, we can get rid of a lot of

#ifdef __linux__
...
#else
...
#endif

code in the generic drm GEM code.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.39 2015/09/23 23:12:12 kettenis Exp $	*/
d1034 1
a1034 1
		if (!file->master || !capable(CAP_SYS_ADMIN))
@


1.39
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d104 1
a104 1
		obj = to_intel_bo(drm_gem_object_find(file, exec[i].handle));
@


1.38
log
@Add a few missing trace functions, and "use" them.  Add back the WATCH_GTT
code (that isn't actually compiled in).  Use dev_priv->dev in one more place
now that we have it, and add set_normalized_timespec() and use it.
@
text
@d1 1
a1 16
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.37 2015/04/11 04:36:10 jsg Exp $	*/
/*
 * Copyright (c) 2008-2009 Owain G. Ainsworth <oga@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
d37 2
a38 1
#include <machine/pmap.h>
d40 8
a47 2
#include <sys/queue.h>
#include <sys/task.h>
d49 4
a52 1
static inline struct vm_page *i915_gem_object_get_page(struct drm_i915_gem_object *, int);
d54 6
a59 4
struct eb_objects {
	u_long hashmask;
	LIST_HEAD(, drm_i915_gem_object) *buckets;
};
d61 15
a75 4
static struct eb_objects *
eb_create(int size)
{
	struct eb_objects *eb;
d77 1
a77 2
	eb = malloc(sizeof(*eb), M_DRM, M_WAITOK | M_ZERO);
	eb->buckets = hashinit(size, M_DRM, M_WAITOK, &eb->hashmask);
d82 1
a82 1
eb_reset(struct eb_objects *eb)
d84 2
a85 4
	int i;

	for (i = 0; i <= eb->hashmask; i++)
		LIST_INIT(&eb->buckets[i]);
d88 6
a93 2
static void
eb_add_object(struct eb_objects *eb, struct drm_i915_gem_object *obj)
d95 87
a181 2
	LIST_INSERT_HEAD(&eb->buckets[obj->exec_handle & eb->hashmask],
	    obj, exec_node);
d184 1
a184 2
static struct drm_i915_gem_object *
eb_get_object(struct eb_objects *eb, unsigned long handle)
d186 7
a192 1
	struct drm_i915_gem_object *obj;
d194 9
a202 3
	LIST_FOREACH(obj, &eb->buckets[handle & eb->hashmask], exec_node) {
		if (obj->exec_handle == handle)
			return (obj);
a203 1
	return (NULL);
d207 1
a207 1
eb_destroy(struct eb_objects *eb)
d209 29
a237 1
	free(eb->buckets, M_DRM, 0);
d243 2
a244 1
	return (obj->base.write_domain == I915_GEM_DOMAIN_CPU ||
d250 95
d346 3
a348 2
				   struct eb_objects *eb,
				   struct drm_i915_gem_relocation_entry *reloc)
d353 1
d355 1
a355 1
	int ret = -EINVAL;
d358 2
a359 2
	target_obj = &eb_get_object(eb, reloc->target_handle)->base;
	if (unlikely(target_obj == NULL))
d361 2
d364 1
a364 2
	target_i915_obj = to_intel_bo(target_obj);
	target_offset = target_i915_obj->gtt_offset;
d385 1
a385 1
		return ret;
d396 1
a396 12
		return ret;
	}
	if (unlikely(reloc->write_domain && target_obj->pending_write_domain &&
		     reloc->write_domain != target_obj->pending_write_domain)) {
		DRM_DEBUG("Write domain conflict: "
			  "obj %p target %d offset %d "
			  "new %08x old %08x\n",
			  obj, reloc->target_handle,
			  (int) reloc->offset,
			  reloc->write_domain,
			  target_obj->pending_write_domain);
		return ret;
d409 2
a410 1
	if (unlikely(reloc->offset > obj->base.size - 4)) {
d416 1
a416 1
		return ret;
d423 1
a423 1
		return ret;
d431 4
a434 3
	if (use_cpu_reloc(obj)) {
		uint32_t page_offset = reloc->offset & PAGE_MASK;
		char *vaddr;
d436 2
a437 27
		ret = i915_gem_object_set_to_cpu_domain(obj, 1);
		if (ret)
			return ret;

		vaddr = kmap_atomic(i915_gem_object_get_page(obj,
							     reloc->offset >> PAGE_SHIFT));
		*(uint32_t *)(vaddr + page_offset) = reloc->delta;
		kunmap_atomic(vaddr);
	} else {
		struct drm_i915_private *dev_priv = dev->dev_private;
		bus_space_handle_t bsh;

		ret = i915_gem_object_set_to_gtt_domain(obj, true);
		if (ret)
			return ret;

		ret = i915_gem_object_put_fence(obj);
		if (ret)
			return ret;

		/* Map the page containing the relocation we're going to perform.  */
		reloc->offset += obj->gtt_offset;
		agp_map_atomic(dev_priv->agph, trunc_page(reloc->offset), &bsh);
		bus_space_write_4(dev_priv->bst, bsh, reloc->offset & PAGE_MASK,	
		    reloc->delta);
		agp_unmap_atomic(dev_priv->agph, bsh);
	}
d446 2
a447 2
i915_gem_execbuffer_relocate_object(struct drm_i915_gem_object *obj,
				    struct eb_objects *eb)
d452 1
a452 1
	struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;
d455 1
a455 1
	user_relocs = (void __user *)(uintptr_t)entry->relocs_ptr;
d471 2
a472 1
			ret = i915_gem_execbuffer_relocate_entry(obj, eb, r);
d493 3
a495 3
i915_gem_execbuffer_relocate_object_slow(struct drm_i915_gem_object *obj,
					 struct eb_objects *eb,
					 struct drm_i915_gem_relocation_entry *relocs)
d497 1
a497 1
	const struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;
d501 2
a502 1
		ret = i915_gem_execbuffer_relocate_entry(obj, eb, &relocs[i]);
d511 1
a511 3
i915_gem_execbuffer_relocate(struct drm_device *dev,
			     struct eb_objects *eb,
			     struct list_head *objects)
d513 1
a513 1
	struct drm_i915_gem_object *obj;
d524 2
a525 2
	list_for_each_entry(obj, objects, exec_list) {
		ret = i915_gem_execbuffer_relocate_object(obj, eb);
a533 3
#define  __EXEC_OBJECT_HAS_PIN (1<<31)
#define  __EXEC_OBJECT_HAS_FENCE (1<<30)

d535 1
a535 1
need_reloc_mappable(struct drm_i915_gem_object *obj)
d537 3
a539 2
	struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;
	return entry->relocation_count && !use_cpu_reloc(obj);
d543 3
a545 2
i915_gem_execbuffer_reserve_object(struct drm_i915_gem_object *obj,
				   struct intel_ring_buffer *ring)
d547 2
a548 4
#ifdef notyet
	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
#endif
	struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;
d551 1
d558 1
a558 1
	need_mappable = need_fence || need_reloc_mappable(obj);
d560 2
a561 1
	ret = i915_gem_object_pin(obj, entry->alignment, need_mappable, false);
a579 1
#ifdef notyet
a586 1
#endif
d588 4
a591 3
	entry->offset = obj->gtt_offset;
	return 0;
}
d593 4
a596 4
static void
i915_gem_execbuffer_unreserve_object(struct drm_i915_gem_object *obj)
{
	struct drm_i915_gem_exec_object2 *entry;
d598 3
a600 2
	if (!obj->gtt_space)
		return;
d602 1
a602 9
	entry = obj->exec_entry;

	if (entry->flags & __EXEC_OBJECT_HAS_FENCE)
		i915_gem_object_unpin_fence(obj);

	if (entry->flags & __EXEC_OBJECT_HAS_PIN)
		i915_gem_object_unpin(obj);

	entry->flags &= ~(__EXEC_OBJECT_HAS_FENCE | __EXEC_OBJECT_HAS_PIN);
d607 2
a608 2
			    struct drm_file *file,
			    struct list_head *objects)
d611 3
a613 1
	struct list_head ordered_objects;
d617 7
a623 2
	INIT_LIST_HEAD(&ordered_objects);
	while (!list_empty(objects)) {
d627 3
a629 4
		obj = list_first_entry(objects,
				       struct drm_i915_gem_object,
				       exec_list);
		entry = obj->exec_entry;
d635 1
a635 1
		need_mappable = need_fence || need_reloc_mappable(obj);
d638 1
a638 1
			list_move(&obj->exec_list, &ordered_objects);
d640 1
a640 1
			list_move_tail(&obj->exec_list, &ordered_objects);
d642 1
a642 1
		obj->base.pending_read_domains = 0;
d646 1
a646 1
	list_splice(&ordered_objects, objects);
d665 2
a666 2
		list_for_each_entry(obj, objects, exec_list) {
			struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;
d669 3
a671 1
			if (!obj->gtt_space)
d678 4
a681 1
			need_mappable = need_fence || need_reloc_mappable(obj);
d683 2
a684 1
			if ((entry->alignment && obj->gtt_offset & (entry->alignment - 1)) ||
d686 1
a686 1
				ret = i915_gem_object_unbind(obj);
d688 1
a688 1
				ret = i915_gem_execbuffer_reserve_object(obj, ring);
d694 2
a695 2
		list_for_each_entry(obj, objects, exec_list) {
			if (obj->gtt_space)
d698 1
a698 1
			ret = i915_gem_execbuffer_reserve_object(obj, ring);
d703 1
a703 4
err:		/* Decrement pin count for bound objects */
		list_for_each_entry(obj, objects, exec_list)
			i915_gem_execbuffer_unreserve_object(obj);

d707 5
a711 1
		ret = i915_gem_evict_everything(ring->dev);
d719 1
d722 2
a723 4
				  struct list_head *objects,
				  struct eb_objects *eb,
				  struct drm_i915_gem_exec_object2 *exec,
				  int count)
d726 3
a728 1
	struct drm_i915_gem_object *obj;
d731 6
d739 5
a743 6
	while (!list_empty(objects)) {
		obj = list_first_entry(objects,
				       struct drm_i915_gem_object,
				       exec_list);
		list_del_init(&obj->exec_list);
		drm_gem_object_unreference(&obj->base);
d767 1
a767 1
		user_relocs = (void __user *)(uintptr_t)exec[i].relocs_ptr;
d786 3
a788 3
			if (copy_to_user(&user_relocs[j].presumed_offset,
					 &invalid_offset,
					 sizeof(invalid_offset))) {
d807 3
a809 15
	for (i = 0; i < count; i++) {
		obj = to_intel_bo(drm_gem_object_lookup(dev, file,
							exec[i].handle));
		if (&obj->base == NULL) {
			DRM_DEBUG("Invalid object handle %d at index %d\n",
				   exec[i].handle, i);
			ret = -ENOENT;
			goto err;
		}

		list_add_tail(&obj->exec_list, objects);
		obj->exec_handle = exec[i].handle;
		obj->exec_entry = &exec[i];
		eb_add_object(eb, obj);
	}
d811 2
a812 1
	ret = i915_gem_execbuffer_reserve(ring, file, objects);
d816 4
a819 4
	list_for_each_entry(obj, objects, exec_list) {
		int offset = obj->exec_entry - exec;
		ret = i915_gem_execbuffer_relocate_object_slow(obj, eb,
							       reloc + reloc_offset[offset]);
a836 32
i915_gem_execbuffer_wait_for_flips(struct intel_ring_buffer *ring, u32 flips)
{
	u32 plane, flip_mask;
	int ret;

	/* Check for any pending flips. As we only maintain a flip queue depth
	 * of 1, we can simply insert a WAIT for the next display flip prior
	 * to executing the batch and avoid stalling the CPU.
	 */

	for (plane = 0; flips >> plane; plane++) {
		if (((flips >> plane) & 1) == 0)
			continue;

		if (plane)
			flip_mask = MI_WAIT_FOR_PLANE_B_FLIP;
		else
			flip_mask = MI_WAIT_FOR_PLANE_A_FLIP;

		ret = intel_ring_begin(ring, 2);
		if (ret)
			return ret;

		intel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);
		intel_ring_emit(ring, MI_NOOP);
		intel_ring_advance(ring);
	}

	return 0;
}

static int
d838 1
a838 1
				struct list_head *objects)
d840 1
a840 1
	struct drm_i915_gem_object *obj;
d842 1
a842 1
	uint32_t flips = 0;
d845 2
a846 1
	list_for_each_entry(obj, objects, exec_list) {
d852 1
a852 4
			i915_gem_clflush_object(obj);

		if (obj->base.pending_write_domain)
			flips |= atomic_read(&obj->pending_flip);
d857 1
a857 7
	if (flips) {
		ret = i915_gem_execbuffer_wait_for_flips(ring, flips);
		if (ret)
			return ret;
	}

	if (flush_domains & I915_GEM_DOMAIN_CPU)
d872 3
d883 2
a884 2
	int relocs_total = 0;
	int relocs_max = INT_MAX / sizeof(struct drm_i915_gem_relocation_entry);
d887 1
a887 3
#ifdef notyet
		char __user *ptr = (char __user *)(uintptr_t)exec[i].relocs_ptr;
#endif
d890 3
d903 5
a907 5
#ifdef notyet
		if (!access_ok(VERIFY_READ, ptr, length))
			return -EFAULT;

		/* we may also need to update the presumed offsets */
d911 5
a915 2
		if (fault_in_multipages_readable(ptr, length))
			return -EFAULT;
d922 18
d941 1
a941 1
i915_gem_execbuffer_move_to_active(struct list_head *objects,
d944 1
a944 1
	struct drm_i915_gem_object *obj;
d946 2
a947 1
	list_for_each_entry(obj, objects, exec_list) {
d951 3
a954 1
		obj->base.write_domain = obj->base.pending_write_domain;
d957 1
a957 1
		i915_gem_object_move_to_active(obj, ring);
d962 1
a962 1
				intel_mark_fb_busy(obj);
d972 2
a973 1
				    struct intel_ring_buffer *ring)
d979 1
a979 1
	(void)i915_add_request(ring, file, NULL);
d1011 2
a1012 1
		       struct drm_i915_gem_exec_object2 *exec)
d1015 1
a1015 2
	struct list_head objects;
	struct eb_objects *eb;
a1016 1
#ifdef __linux__
a1017 1
#endif
d1019 1
a1019 1
	u32 ctx_id = i915_execbuffer2_get_context_id(*args);
d1021 3
a1023 3
	u32 mask;
	u32 flags;
	int ret, mode, i;
d1025 1
a1025 2
	if (!i915_gem_check_execbuffer(args)) {
		DRM_DEBUG("execbuf with invalid offset/length\n");
a1026 1
	}
d1034 1
a1034 1
		if (!DRM_SUSER(curproc))
d1049 1
a1049 1
		if (ctx_id != 0) {
d1057 1
a1057 1
		if (ctx_id != 0) {
d1063 9
d1131 2
a1132 1
		cliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),
d1140 2
a1141 3
				     (struct drm_clip_rect __user *)(uintptr_t)
				     args->cliprects_ptr,
				     sizeof(*cliprects)*args->num_cliprects)) {
d1148 2
d1154 1
a1154 1
	if (dev_priv->mm.suspended) {
d1160 7
a1166 1
	eb = eb_create(args->buffer_count);
d1174 3
a1176 26
	INIT_LIST_HEAD(&objects);
	for (i = 0; i < args->buffer_count; i++) {
		struct drm_i915_gem_object *obj;

		obj = to_intel_bo(drm_gem_object_lookup(dev, file,
							exec[i].handle));
		if (&obj->base == NULL) {
			DRM_DEBUG("Invalid object handle %d at index %d\n",
				   exec[i].handle, i);
			/* prevent error path from reading uninitialized data */
			ret = -ENOENT;
			goto err;
		}

		if (!list_empty(&obj->exec_list)) {
			DRM_DEBUG("Object %p [handle %d, index %d] appears more than once in object list\n",
				   obj, exec[i].handle, i);
			ret = -EINVAL;
			goto err;
		}

		list_add_tail(&obj->exec_list, &objects);
		obj->exec_handle = exec[i].handle;
		obj->exec_entry = &exec[i];
		eb_add_object(eb, obj);
	}
d1179 1
a1179 3
	batch_obj = list_entry(objects.prev,
			       struct drm_i915_gem_object,
			       exec_list);
d1182 2
a1183 1
	ret = i915_gem_execbuffer_reserve(ring, file, &objects);
d1188 2
a1189 1
	ret = i915_gem_execbuffer_relocate(dev, eb, &objects);
d1192 2
a1193 4
			ret = i915_gem_execbuffer_relocate_slow(dev, file, ring,
								&objects, eb,
								exec,
								args->buffer_count);
d1210 1
a1210 2
	 * hsw should have this fixed, but let's be paranoid and do it
	 * unconditionally for now. */
d1214 1
a1214 1
	ret = i915_gem_execbuffer_move_to_gpu(ring, &objects);
d1243 2
a1244 1
	exec_start = batch_obj->gtt_offset + args->batch_start_offset;
d1246 1
a1246 1
#ifdef __linux__
d1273 2
a1274 2
	i915_gem_execbuffer_move_to_active(&objects, ring);
	i915_gem_execbuffer_retire_commands(dev, file, ring);
a1277 9
	while (!list_empty(&objects)) {
		struct drm_i915_gem_object *obj;

		obj = list_first_entry(&objects,
				       struct drm_i915_gem_object,
				       exec_list);
		list_del_init(&obj->exec_list);
		drm_gem_object_unreference(&obj->base);
	}
a1281 1
#ifdef __linux
d1283 4
a1286 1
#endif
d1299 1
d1322 1
a1322 1
			     (void __user *)(uintptr_t)args->buffers_ptr,
d1355 2
a1356 1
	ret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);
d1358 3
d1362 11
a1372 11
		for (i = 0; i < args->buffer_count; i++)
			exec_list[i].offset = exec2_list[i].offset;
		/* ... and back out to userspace */
		ret = copy_to_user((void __user *)(uintptr_t)args->buffers_ptr,
				   exec_list,
				   sizeof(*exec_list) * args->buffer_count);
		if (ret) {
			ret = -EFAULT;
			DRM_DEBUG("failed to copy %d exec entries "
				  "back to user (%d)\n",
				  args->buffer_count, ret);
d1386 1
d1398 1
a1398 1
			     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);
d1408 1
a1408 2
			     (struct drm_i915_relocation_entry __user *)
			     (uintptr_t) args->buffers_ptr,
d1417 2
a1418 1
	ret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);
d1421 15
a1435 8
		ret = copy_to_user((void __user *)(uintptr_t)args->buffers_ptr,
				   exec2_list,
				   sizeof(*exec2_list) * args->buffer_count);
		if (ret) {
			ret = -EFAULT;
			DRM_DEBUG("failed to copy %d exec entries "
				  "back to user (%d)\n",
				  args->buffer_count, ret);
a1440 6
}

static inline struct vm_page *
i915_gem_object_get_page(struct drm_i915_gem_object *obj, int n)
{
	return (obj->pages[n]);
@


1.37
log
@change back to drm_free_large/drm_malloc_ab
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.36 2015/04/05 11:53:53 kettenis Exp $	*/
d1096 1
a1098 1
#endif
@


1.36
log
@Another round of reducing diffs with Linux.  This one moves the various
copy_to_user and copy_from_user functions into drm_linux.h and uses them
instead of copyin/copyout and DRM_COPY_*.  Also move the timespec functions,
and put i915_gem_object_is_purgable() where it belongs.

Uncovered a bug where the arguments to copyout() were in the wrong order.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.35 2015/02/12 08:48:32 jsg Exp $	*/
d545 2
a546 3
	reloc_offset = mallocarray(count, sizeof(*reloc_offset), M_DRM,
	    M_WAITOK);
	reloc = mallocarray(total, sizeof(*reloc), M_DRM, M_WAITOK);
d548 2
a549 2
		drm_free(reloc);
		drm_free(reloc_offset);
d635 2
a636 2
	drm_free(reloc);
	drm_free(reloc_offset);
d1227 3
d1242 1
a1242 1
		drm_free(exec2_list);
d1260 1
a1260 1
	drm_free(exec2_list);
@


1.35
log
@Add and use macros for linux memory barriers.  Fix the call in
i915_gem_object_flush_fence() to be mb() not wmb() while here.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.34 2015/02/12 06:52:11 jsg Exp $	*/
d268 1
a268 1
		if (DRM_COPY_FROM_USER(r, user_relocs, count*sizeof(r[0])))
d279 1
a279 1
			    DRM_COPY_TO_USER(&user_relocs->presumed_offset,
d563 1
a563 1
		if (DRM_COPY_FROM_USER(reloc+total, user_relocs,
d580 1
a580 1
			if (DRM_COPY_TO_USER(&user_relocs[j].presumed_offset,
d1233 1
a1233 1
	ret = DRM_COPY_FROM_USER(exec2_list,
d1247 1
a1247 1
		ret = DRM_COPY_TO_USER((void __user *)(uintptr_t)args->buffers_ptr,
@


1.34
log
@switch some free calls back to kfree
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.33 2015/02/12 02:12:02 kettenis Exp $	*/
d706 1
a706 1
		DRM_WRITEMEMORYBARRIER();
@


1.33
log
@Add mutex_is_locked and use it wherever linux uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.32 2015/02/10 01:39:32 jsg Exp $	*/
d106 1
a106 1
	free(eb, M_DRM, 0);
@


1.32
log
@Remove DRM_LOCK macros, rename dev_lock to struct_mutex and directly
call linux style lock functions where these macros were used.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.31 2014/12/09 07:05:06 doug Exp $	*/
d1022 1
a1022 1
			rw_assert_wrlock(&dev->struct_mutex);
@


1.31
log
@More malloc() -> mallocarray() in the kernel.

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.30 2014/09/20 16:15:16 kettenis Exp $	*/
d539 1
a539 1
	DRM_UNLOCK();
d551 1
a551 1
		DRM_LOCK();
d566 1
a566 1
			DRM_LOCK();
d584 1
a584 1
				DRM_LOCK();
d595 1
a595 1
		DRM_LOCK();
d964 1
a964 1
		DRM_UNLOCK();
d971 1
a971 1
		DRM_UNLOCK();
d1022 1
a1022 1
			rw_assert_wrlock(&dev->dev_lock);
d1116 1
a1116 1
	DRM_UNLOCK();
@


1.30
log
@On i386, agp_map_subregion might sleep, which is not allowed in some of
the inteldrm code.  Fix this by adding new interfaces that can map a single
page without sleeping and use that in the execbuffer fast path that needs
this "atomic" behaviour.  Should fix the panic I've seen under memory pressure
on i386.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.29 2014/07/12 18:48:52 tedu Exp $	*/
d545 3
a547 2
	reloc_offset = malloc(count * sizeof(*reloc_offset), M_DRM, M_WAITOK);
	reloc = malloc(total * sizeof(*reloc), M_DRM, M_WAITOK);
@


1.29
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.28 2014/04/01 20:16:50 kettenis Exp $	*/
d236 1
a236 6
		if ((ret = agp_map_subregion(dev_priv->agph,
		    trunc_page(reloc->offset), PAGE_SIZE, &bsh)) != 0) {
			DRM_ERROR("map failed...\n");
			return -ret;
		}

d239 1
a239 2

		agp_unmap_subregion(dev_priv->agph, bsh, PAGE_SIZE);
@


1.28
log
@Move some duplicated code implementing Linux compatibility APIs and stick it
in a seperate header file.  This will become a dumping ground for similar code.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.27 2014/03/17 22:15:24 kettenis Exp $	*/
d105 2
a106 2
	free(eb->buckets, M_DRM);
	free(eb, M_DRM);
@


1.27
log
@Use the ci_inatomic flag here as well to "disable" page faults and provide
the same implementations of kmap_atomic() and kunmap_atomic() as we have
in i915_gem.c.  These will be unified later.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.26 2014/01/21 08:57:22 kettenis Exp $	*/
a55 45

static inline void
pagefault_disable(void)
{
	KASSERT(curcpu()->ci_inatomic == 0);
	curcpu()->ci_inatomic = 1;
}

static inline void
pagefault_enable(void)
{
	KASSERT(curcpu()->ci_inatomic == 1);
	curcpu()->ci_inatomic = 0;
}

static inline int
in_atomic(void)
{
	return curcpu()->ci_inatomic;
}

static inline void *
kmap_atomic(struct vm_page *pg)
{
	vaddr_t va;

#if defined (__HAVE_PMAP_DIRECT)
	va = pmap_map_direct(pg);
#else
	extern vaddr_t pmap_tmpmap_pa(paddr_t);
	va = pmap_tmpmap_pa(VM_PAGE_TO_PHYS(pg));
#endif
	return (void *)va;
}

static inline void
kunmap_atomic(void *addr)
{
#if defined (__HAVE_PMAP_DIRECT)
	pmap_unmap_direct((vaddr_t)addr);
#else
	extern void pmap_tmpunmap_pa(void);
	pmap_tmpunmap_pa();
#endif
}
@


1.26
log
@Use Linux compat functions to do kernel memory allocations in the bits of code
that are shared with Linux.  Use OpenBSD functions in the few sports where we
have our own implementation of bits.

discussed with jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.25 2013/12/11 20:31:43 kettenis Exp $	*/
a56 2
int pagefault_disabled;

d60 2
a61 2
	KASSERT(pagefault_disabled == 0);
	pagefault_disabled = 1;
d67 2
a68 2
	KASSERT(pagefault_disabled == 1);
	pagefault_disabled = 0;
d74 26
a99 1
	return pagefault_disabled;
a101 2
static void *kmap_atomic(struct vm_page *);
static void kunmap_atomic(void *);
a1309 31
}

static void *
kmap_atomic(struct vm_page *pg)
{
	vaddr_t va;

#if defined (__HAVE_PMAP_DIRECT)
	va = pmap_map_direct(pg);
#else
	va = uvm_km_valloc(kernel_map, PAGE_SIZE);
	if (va == 0)
		return (NULL);
	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW);
	pmap_update(pmap_kernel());
#endif
	return (void *)va;
}

static void
kunmap_atomic(void *addr)
{
	vaddr_t va = (vaddr_t)addr;

#if defined (__HAVE_PMAP_DIRECT)
	pmap_unmap_direct(va);
#else
	pmap_kremove(va, PAGE_SIZE);
	pmap_update(pmap_kernel());
	uvm_km_free(kernel_map, va, PAGE_SIZE);
#endif
@


1.25
log
@Make obj->pages a simple array instead of an array of bus_dma_segment_t's.
Simplifies things a bit and reduces the diffs with Linux a bit too.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.24 2013/12/07 10:48:35 kettenis Exp $	*/
d1255 2
a1256 2
	exec2_list = malloc(sizeof(*exec2_list)*args->buffer_count,
			    M_DRM,  M_WAITOK);
@


1.24
log
@Now that we properly flush caches, we can enable cpu relocations.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.23 2013/12/07 10:46:26 kettenis Exp $	*/
d1325 1
a1325 7
	bus_dma_segment_t *segp;
	struct vm_page *pg;

	segp = &obj->pages[n];
	pg = PHYS_TO_VM_PAGE(segp->ds_addr);

	return (pg);
@


1.23
log
@Enable fast path for relocations.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.22 2013/12/05 13:29:56 kettenis Exp $	*/
a134 1
#ifdef notyet
a137 3
#else
	return 0;
#endif
@


1.22
log
@Rename 'struct drm_obj' to 'struct drm_gem_object' to reduce the diffs with
Linux.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.21 2013/11/30 20:13:36 kettenis Exp $	*/
d57 22
a233 1
#ifdef notyet
a235 1
#endif
a352 3
	/* XXX fastpath not currently used on OpenBSD */
	return -EFAULT;

a359 1
#ifdef notyet
a360 1
#endif
a365 1
#ifdef notyet
a366 1
#endif
@


1.21
log
@Oops!  Only intended to commit the i915_dma.c changes in the previous commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.19 2013/11/27 22:20:19 kettenis Exp $	*/
d128 1
a128 1
	struct drm_obj *target_obj;
@


1.20
log
@Reorder some case statements to reduce the diffs with Linux.
@
text
@d113 1
d117 3
@


1.19
log
@Reduce diffs with Linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.18 2013/11/19 19:14:09 kettenis Exp $	*/
a112 1
#ifdef notyet
a115 3
#else
	return 0;
#endif
@


1.18
log
@Move the GTT management into the inteldrm driver.  It is really obvious now
that this is necessary as on some hardware we need guard pages between
regions that have different cache attributes.  Even if this appears to cause
regressions on some hardware, this change is a necessary (but not sufficient)
step to fix the cache coherency problems on the affected hardware.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.17 2013/11/16 18:24:59 kettenis Exp $	*/
d424 1
a424 1
	if (obj->gtt_space == NULL)
d496 1
a496 1
			if (obj->gtt_space == NULL)
d516 1
a516 1
			if (obj->gtt_space != NULL)
@


1.17
log
@PAGE_MASK has exactly the opposite meaning in Linux and OpenBSD.  Adjust its
usage in a currently disabled codepath to prevent future surprised.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.16 2013/10/29 06:30:57 jsg Exp $	*/
d147 1
a147 1
		i915_gem_gtt_rebind_object(target_i915_obj,
d424 1
a424 1
	if (obj->dmamap == NULL)
d496 1
a496 1
			if (obj->dmamap == NULL)
d516 1
a516 1
			if (obj->dmamap != NULL)
d1061 1
a1061 1
		i915_gem_gtt_rebind_object(batch_obj, batch_obj->cache_level);
@


1.16
log
@Move most of the uses of workqs in drm to the new task/taskq api.
Prevents unintended multiple additions to workqs that was causing
hangs on radeon, and lets us remove tasks more closely matching
the behaviour of the original linux code.

ok kettenis@@
cause of the ttm/radeon hangs debugged by claudio@@ and kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.15 2013/10/05 07:30:06 jsg Exp $	*/
d219 1
a219 1
		uint32_t page_offset = reloc->offset & ~PAGE_MASK;
@


1.15
log
@add and use gtt mapping flags, further reduces the diff to linux
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.14 2013/09/18 08:50:28 jsg Exp $	*/
d55 1
a55 1
#include <sys/workq.h>
@


1.14
log
@sync the execbuffer relocation code with linux 3.8.13
with the fastpath and cpu relocs disabled for now.
eb_* functions based on code in FreeBSD.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.13 2013/08/13 10:23:49 jsg Exp $	*/
a143 1
#ifdef notyet
d147 1
a147 1
		i915_gem_gtt_bind_object(target_i915_obj,
a149 1
#endif
a1059 1
#ifdef notyet
d1061 1
a1061 2
		i915_gem_gtt_bind_object(batch_obj, batch_obj->cache_level);
#endif
@


1.13
log
@add static back to functions that originally had it
reduces the diff to linux and makes ddb hangman a little easier
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.12 2013/08/09 08:14:55 jsg Exp $	*/
d49 1
d57 4
a60 1
#ifdef notyet
d62 2
a63 2
	int and;
	struct hlist_head buckets[0];
a69 9
	int count = PAGE_SIZE / sizeof(struct hlist_head) / 2;
	BUILD_BUG_ON_NOT_POWER_OF_2(PAGE_SIZE / sizeof(struct hlist_head));
	while (count > size)
		count >>= 1;
	eb = kzalloc(count*sizeof(struct hlist_head) +
		     sizeof(struct eb_objects),
		     GFP_KERNEL);
	if (eb == NULL)
		return eb;
d71 2
a72 1
	eb->and = count - 1;
d79 4
a82 1
	memset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));
d88 2
a89 2
	hlist_add_head(&obj->exec_node,
		       &eb->buckets[obj->exec_handle & eb->and]);
a94 2
	struct hlist_head *head;
	struct hlist_node *node;
d97 1
a97 3
	head = &eb->buckets[handle & eb->and];
	hlist_for_each(node, head) {
		obj = hlist_entry(node, struct drm_i915_gem_object, exec_node);
d99 1
a99 1
			return obj;
d101 1
a101 2

	return NULL;
d107 2
a108 1
	kfree(eb);
a109 1
#endif /* notyet */
d113 1
d117 3
a121 1
#ifdef notyet
d128 1
a128 1
	struct drm_gem_object *target_obj;
d144 1
d151 1
d214 1
d217 1
d234 1
a234 2
		uint32_t __iomem *reloc_entry;
		void __iomem *reloc_page;
d246 10
a255 6
		reloc_page = io_mapping_map_atomic_wc(dev_priv->mm.gtt_mapping,
						      reloc->offset & PAGE_MASK);
		reloc_entry = (uint32_t __iomem *)
			(reloc_page + (reloc->offset & ~PAGE_MASK));
		iowrite32(reloc->delta, reloc_entry);
		io_mapping_unmap_atomic(reloc_page);
d284 1
a284 1
		if (__copy_from_user_inatomic(r, user_relocs, count*sizeof(r[0])))
d295 1
a295 1
			    __copy_to_user_inatomic(&user_relocs->presumed_offset,
d335 3
d345 1
d347 1
d353 1
d355 1
a358 1
#endif /* notyet */
a362 1
#ifdef notyet
d374 1
d376 1
d562 1
a562 1
	mutex_unlock(&dev->struct_mutex);
d568 2
a569 2
	reloc_offset = drm_malloc_ab(count, sizeof(*reloc_offset));
	reloc = drm_malloc_ab(total, sizeof(*reloc));
d571 3
a573 3
		drm_free_large(reloc);
		drm_free_large(reloc_offset);
		mutex_lock(&dev->struct_mutex);
d585 1
a585 1
		if (copy_from_user(reloc+total, user_relocs,
d588 1
a588 1
			mutex_lock(&dev->struct_mutex);
d602 1
a602 1
			if (copy_to_user(&user_relocs[j].presumed_offset,
d606 1
a606 1
				mutex_lock(&dev->struct_mutex);
d617 1
a617 1
		mutex_lock(&dev->struct_mutex);
d658 2
a659 2
	drm_free_large(reloc);
	drm_free_large(reloc_offset);
a661 1
#endif /* notyet */
d697 1
a697 1
   struct drm_obj **object_list, int buffer_count)
d702 1
a702 1
	int ret, i;
d704 1
a704 2
	for (i = 0; i < buffer_count; i++) {
		obj = to_intel_bo(object_list[i]);
a735 1
#ifdef notyet
d751 1
d753 1
d766 1
d776 1
a780 1
#endif /* notyet */
d783 2
a784 2
i915_gem_execbuffer_move_to_active(struct drm_obj **object_list,
    int buffer_count, struct intel_ring_buffer *ring)
a786 1
	int i;
d788 1
a788 3
	for (i = 0; i < buffer_count; i++) {
		obj = to_intel_bo(object_list[i]);
#if 0
a790 1
#endif
d804 1
a804 1
//		trace_i915_gem_object_change_domain(obj, old_read, old_write);
d817 1
a817 1
	i915_add_request(ring, file, NULL);
d845 5
a849 6
// i915_gem_do_execbuffer
// i915_gem_execbuffer

int
i915_gem_execbuffer2(struct drm_device *dev, void *data,
    struct drm_file *file_priv)
d851 13
a863 28
	struct inteldrm_softc			*dev_priv = dev->dev_private;
	struct drm_i915_gem_execbuffer2		*args = data;
	struct drm_i915_gem_exec_object2	*exec_list = NULL;
	struct drm_i915_gem_relocation_entry	*relocs = NULL;
	struct drm_i915_gem_object		*batch_obj_priv;
	struct drm_obj				**object_list = NULL;
	struct drm_obj				*batch_obj, *obj;
	struct intel_ring_buffer		*ring;
	uint32_t				 ctx_id = i915_execbuffer2_get_context_id(*args);
	size_t					 oflow;
	int					 ret, ret2, i;
	int					 pinned = 0, pin_tries;
	uint32_t				 reloc_index;
	uint32_t				 flags;
	uint32_t				 exec_start, exec_len;
	uint32_t				 mask;
	int					 mode;

	/*
	 * Check for valid execbuffer offset. We can do this early because
	 * bound object are always page aligned, so only the start offset
	 * matters. Also check for integer overflow in the batch offset and size
	 */
	 if ((args->batch_start_offset | args->batch_len) & 0x7 ||
	    args->batch_start_offset + args->batch_len < args->batch_len ||
	    args->batch_start_offset + args->batch_len <
	    args->batch_start_offset)
		return -EINVAL;
d865 2
a866 2
	if (args->buffer_count < 1) {
		DRM_ERROR("execbuf with %d buffers\n", args->buffer_count);
d870 4
d877 1
a877 1
			return -EPERM;
d906 2
a907 2
		printf("unknown ring %d\n",
		    (int)(args->flags & I915_EXEC_RING_MASK));
d941 2
a942 3
	/* Copy in the exec list from userland, check for overflow */
	oflow = SIZE_MAX / args->buffer_count;
	if (oflow < sizeof(*exec_list) || oflow < sizeof(*object_list))
a943 5
	exec_list = drm_alloc(sizeof(*exec_list) * args->buffer_count);
	object_list = drm_alloc(sizeof(*object_list) * args->buffer_count);
	if (exec_list == NULL || object_list == NULL) {
		ret = -ENOMEM;
		goto pre_mutex_err;
a944 4
	ret = -copyin((void *)(uintptr_t)args->buffers_ptr, exec_list,
	    sizeof(*exec_list) * args->buffer_count);
	if (ret != 0)
		goto pre_mutex_err;
d946 34
a979 4
	ret = -i915_gem_get_relocs_from_user(exec_list, args->buffer_count,
	    &relocs);
	if (ret != 0)
		goto pre_mutex_err;
a983 1
	inteldrm_verify_inactive(dev_priv, __FILE__, __LINE__);
d985 4
a988 4
	/* XXX check these before we copyin... but we do need the lock */
	if (dev_priv->mm.wedged) {
		ret = -EIO;
		goto unlock;
d991 5
a995 3
	if (dev_priv->mm.suspended) {
		ret = -EBUSY;
		goto unlock;
d999 1
d1001 8
a1008 6
		object_list[i] = drm_gem_object_lookup(dev, file_priv,
		    exec_list[i].handle);
		obj = object_list[i];
		if (obj == NULL) {
			DRM_ERROR("Invalid object handle %d at index %d\n",
				   exec_list[i].handle, i);
d1012 4
a1015 3
		if (obj->do_flags & I915_IN_EXEC) {
			DRM_ERROR("Object %p appears more than once in object_list\n",
			    object_list[i]);
d1019 5
a1023 1
		atomic_setbits_int(&obj->do_flags, I915_IN_EXEC);
d1026 4
a1029 23
	/* Pin and relocate */
	for (pin_tries = 0; ; pin_tries++) {
		ret = pinned = 0;
		reloc_index = 0;

		for (i = 0; i < args->buffer_count; i++) {
			object_list[i]->pending_read_domains = 0;
			object_list[i]->pending_write_domain = 0;
			to_intel_bo(object_list[i])->pending_fenced_gpu_access = false;
			drm_hold_object(object_list[i]);
			to_intel_bo(object_list[i])->exec_entry = &exec_list[i];
			ret = i915_gem_object_pin_and_relocate(object_list[i],
			    file_priv, &exec_list[i], &relocs[reloc_index]);
			if (ret) {
				drm_unhold_object(object_list[i]);
				break;
			}
			pinned++;
			reloc_index += exec_list[i].relocation_count;
		}
		/* success */
		if (ret == 0)
			break;
d1031 4
a1034 3
		/* error other than GTT full, or we've already tried again */
		if (ret != -ENOSPC || pin_tries >= 1)
			goto err;
d1036 9
a1044 11
		/*
		 * unpin all of our buffers and unhold them so they can be
		 * unbound so we can try and refit everything in the aperture.
		 */
		for (i = 0; i < pinned; i++) {
			if (object_list[i]->do_flags & __EXEC_OBJECT_HAS_FENCE) {
				i915_gem_object_unpin_fence(to_intel_bo(object_list[i]));
				object_list[i]->do_flags &= ~__EXEC_OBJECT_HAS_FENCE;
			}
			i915_gem_object_unpin(to_intel_bo(object_list[i]));
			drm_unhold_object(object_list[i]);
a1045 3
		pinned = 0;
		/* evict everyone we can from the aperture */
		ret = i915_gem_evict_everything(dev);
d1050 3
a1052 10
	/* If we get here all involved objects are referenced, pinned, relocated
	 * and held. Now we can finish off the exec processing.
	 *
	 * First, set the pending read domains for the batch buffer to
	 * command.
	 */
	batch_obj = object_list[args->buffer_count - 1];
	batch_obj_priv = to_intel_bo(batch_obj);
	if (args->batch_start_offset + args->batch_len > batch_obj->size ||
	    batch_obj->pending_write_domain) {
d1056 10
a1065 1
	batch_obj->pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
d1067 1
a1067 2
	ret = i915_gem_execbuffer_move_to_gpu(ring, object_list,
	    args->buffer_count);
d1071 1
a1071 1
	ret = i915_switch_context(ring, file_priv, ctx_id);
d1096 1
a1096 6
	/* Exec the batchbuffer */
	/*
	 * XXX make sure that this may never fail by preallocating the request.
	 */

	exec_start = batch_obj_priv->gtt_offset + args->batch_start_offset;
d1098 7
d1106 15
a1120 3
	ret = ring->dispatch_execbuffer(ring, exec_start, exec_len, flags);
	if (ret)
		goto err;
d1122 2
a1123 2
	i915_gem_execbuffer_move_to_active(object_list, args->buffer_count, ring);
	i915_gem_execbuffer_retire_commands(dev, file_priv, ring);
d1125 2
a1126 2
	ret = -copyout(exec_list, (void *)(uintptr_t)args->buffers_ptr,
	    sizeof(*exec_list) * args->buffer_count);
d1129 61
d1191 10
a1200 2
		if (object_list[i] == NULL)
			break;
d1202 25
a1226 3
		if (object_list[i]->do_flags & __EXEC_OBJECT_HAS_FENCE) {
			i915_gem_object_unpin_fence(to_intel_bo(object_list[i]));
			object_list[i]->do_flags &= ~__EXEC_OBJECT_HAS_FENCE;
d1228 21
d1250 29
a1278 7
		atomic_clearbits_int(&object_list[i]->do_flags, I915_IN_EXEC |
		    I915_EXEC_NEEDS_FENCE);
		if (i < pinned) {
			i915_gem_object_unpin(to_intel_bo(object_list[i]));
			drm_unhold_and_unref(object_list[i]);
		} else {
			drm_unref(&object_list[i]->uobj);
d1282 34
a1315 2
unlock:
	DRM_UNLOCK();
d1317 5
a1321 6
pre_mutex_err:
	/* update userlands reloc state. */
	ret2 = -i915_gem_put_relocs_to_user(exec_list,
	    args->buffer_count, relocs);
	if (ret2 != 0 && ret == 0)
		ret = ret2;
d1323 2
a1324 2
	drm_free(object_list);
	drm_free(exec_list);
d1326 1
a1326 1
	return ret;
@


1.12
log
@match linux and only allow multiple contexts on the render ring
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.11 2013/08/09 07:55:42 jsg Exp $	*/
a55 13
int	i915_reset_gen7_sol_offsets(struct drm_device *,
	    struct intel_ring_buffer *);
int	i915_gem_execbuffer_wait_for_flips(struct intel_ring_buffer *, u32);
int	i915_gem_execbuffer_move_to_gpu(struct intel_ring_buffer *,
	    struct drm_obj **, int);
void	i915_gem_execbuffer_move_to_active(struct drm_obj **, int,
	    struct intel_ring_buffer *);
void	i915_gem_execbuffer_retire_commands(struct drm_device *,
	    struct drm_file *, struct intel_ring_buffer *);
int	need_reloc_mappable(struct drm_i915_gem_object *);
int	i915_gem_execbuffer_reserve(struct intel_ring_buffer *,
	    struct drm_file *, struct list_head *);

d353 2
a354 1
int
d361 1
a361 1
int
a364 1
#ifdef notyet
a365 1
#endif
d410 1
a410 1
void
d429 1
a429 1
int
a527 1
#ifdef notyet
d653 1
a653 1
int
d685 1
a685 1
int
d771 1
a771 1
void
d801 1
a801 1
void
d813 1
a813 1
int
@


1.11
log
@add commented out versions of unused functions present in the original
files to reduce the diff to linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.10 2013/08/08 21:35:56 kettenis Exp $	*/
d911 5
d919 5
@


1.10
log
@Fix sign of errno values to match Linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.9 2013/08/07 19:49:07 kettenis Exp $	*/
d69 60
a128 6
// struct eb_objects {
// eb_create
// eb_reset
// eb_add_object
// eb_get_object
// eb_destroy
d137 225
a361 4
// i915_gem_execbuffer_relocate_entry
// i915_gem_execbuffer_relocate_object
// i915_gem_execbuffer_relocate_object_slow
// i915_gem_execbuffer_relocate
d542 126
d742 43
a784 2
// i915_gem_check_execbuffer
// validate_exec_list
a826 2

// i915_gem_fix_mi_batchbuffer_end
@


1.9
log
@Another major overhaul of the inteldrm(4) GEM code, bringing us considerably
closer to the Linux 3.8.13 codebase.  Almost certainly squashes a few more
bugs.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.8 2013/08/07 00:04:28 jsg Exp $	*/
d521 1
a521 1
	ret = copyin((void *)(uintptr_t)args->buffers_ptr, exec_list,
d526 1
a526 1
	ret = i915_gem_get_relocs_from_user(exec_list, args->buffer_count,
d674 1
a674 1
	ret = copyout(exec_list, (void *)(uintptr_t)args->buffers_ptr,
d702 1
a702 1
	ret2 = i915_gem_put_relocs_to_user(exec_list,
@


1.8
log
@add support for hardware contexts on recent intel hardware
based on the code in linux 3.8.13
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.7 2013/05/05 13:55:36 kettenis Exp $	*/
d447 1
a447 1
		return (EINVAL);
d451 1
a451 1
		return (EINVAL);
d457 1
a457 1
			return (EPERM);
d478 1
a478 1
		return (EINVAL);
d483 1
a483 1
		return (EINVAL);
d495 1
a495 1
				return EINVAL;
d499 1
a499 1
				return EINVAL;
d508 1
a508 1
		return EINVAL;
d514 1
a514 1
		return (EINVAL);
d518 1
a518 1
		ret = ENOMEM;
d538 1
a538 1
		ret = EIO;
d543 1
a543 1
		ret = EBUSY;
d555 1
a555 1
			ret = ENOENT;
d561 1
a561 1
			ret = EINVAL;
d592 1
a592 1
		if (ret != ENOSPC || pin_tries >= 1)
d624 1
a624 1
		ret = EINVAL;
@


1.7
log
@Add nonblocking argument to i915_gem_object_pin() and
i915_gem_object_bind_to_gtt().
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.6 2013/04/17 20:04:04 kettenis Exp $	*/
d428 1
d631 4
@


1.6
log
@Another round of reducing diffs with Linux code.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.5 2013/04/03 19:57:17 kettenis Exp $	*/
d116 1
a116 1
	ret = i915_gem_object_pin(obj, entry->alignment, need_mappable);
@


1.5
log
@Return ENOENT instead of EBADF if looking up a gem object fails.
Return EINVAL instead of EBADF if the same object is specified twice in
an execbuffer2 call.
This is what Linux does.
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.4 2013/03/30 04:57:53 jsg Exp $	*/
d261 1
a261 1
		ret = i915_gem_evict_everything(ring->dev->dev_private);
d395 1
a395 1
	if (!IS_GEN7(dev) || ring != &dev_priv->rings[RCS])
d466 1
a466 1
		ring = &dev_priv->rings[RCS];
d469 1
a469 1
		ring = &dev_priv->rings[VCS];
d472 1
a472 1
		ring = &dev_priv->rings[BCS];
d491 1
a491 1
		if (ring == &dev_priv->rings[RCS] &&
d608 1
a608 1
		ret = i915_gem_evict_everything(dev_priv);
d633 1
a633 1
	if (ring == &dev_priv->rings[RCS] &&
@


1.4
log
@go back to the old method of execbuffer pinning
should fix problems noticed by Ralf Horstmann and bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.3 2013/03/28 19:38:53 kettenis Exp $	*/
d554 1
a554 1
			ret = EBADF;
d560 1
a560 1
			ret = EBADF;
@


1.3
log
@Call intel_mark_busy() in the right place, and call intel_mark_fb_busy()
where we used to call intel_mark_busy().
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.2 2013/03/28 11:51:05 jsg Exp $	*/
d578 1
a578 1
			    file_priv, &exec_list[i], &relocs[reloc_index], ring);
@


1.2
log
@add i915_gem_execbuffer_reserve_object and friends and move
the execbuffer pinning closer to linux
@
text
@d1 1
a1 1
/*	$OpenBSD: i915_gem_execbuffer.c,v 1.1 2013/03/18 12:36:52 jsg Exp $	*/
d366 2
a367 1
			intel_mark_busy(ring->dev);
@


1.1
log
@Significantly increase the wordlist for ddb hangman,
and update our device independent DRM code and the Intel DRM code
to be mostly in sync with Linux 3.8.3.  Among other things this
brings support for kernel modesetting and enables use of
the rings on gen6+ Intel hardware.

Based on some earlier work from matthieu@@ with some hints from FreeBSD
and with lots of help from kettenis@@ (including a beautiful accelerated
wscons framebuffer console!)

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d65 3
d75 8
d87 179
a265 3
// pin_and_fence_object
// i915_gem_execbuffer_reserve
// i915_gem_execbuffer_relocate_slow
d575 1
d577 1
a577 1
			    file_priv, &exec_list[i], &relocs[reloc_index]);
@

