head	1.20;
access;
symbols
	OPENBSD_6_2_BASE:1.20
	OPENBSD_6_1:1.19.0.8
	OPENBSD_6_1_BASE:1.19
	OPENBSD_6_0:1.19.0.6
	OPENBSD_6_0_BASE:1.19
	OPENBSD_5_9:1.19.0.2
	OPENBSD_5_9_BASE:1.19
	OPENBSD_5_8:1.16.0.4
	OPENBSD_5_8_BASE:1.16
	OPENBSD_5_7:1.13.0.4
	OPENBSD_5_7_BASE:1.13
	OPENBSD_5_6:1.8.0.4
	OPENBSD_5_6_BASE:1.8
	OPENBSD_5_5:1.7.0.4
	OPENBSD_5_5_BASE:1.7;
locks; strict;
comment	@ * @;


1.20
date	2017.06.04.14.02.24;	author kettenis;	state Exp;
branches;
next	1.19;
commitid	P8388KbYVi0mp6ZI;

1.19
date	2015.10.23.08.21.58;	author kettenis;	state Exp;
branches;
next	1.18;
commitid	FlsNNetLRJXhglbE;

1.18
date	2015.09.27.11.09.26;	author jsg;	state Exp;
branches;
next	1.17;
commitid	OkoKp05dU7tP7DK3;

1.17
date	2015.09.23.23.12.12;	author kettenis;	state Exp;
branches;
next	1.16;
commitid	lQlppvmETCN49oZe;

1.16
date	2015.07.11.04.00.46;	author jsg;	state Exp;
branches;
next	1.15;
commitid	eVKv6xUxvZe6BPUh;

1.15
date	2015.04.12.03.54.10;	author jsg;	state Exp;
branches;
next	1.14;
commitid	uVTyY1h8Sggc8pFj;

1.14
date	2015.04.06.05.35.29;	author jsg;	state Exp;
branches;
next	1.13;
commitid	oeVBooRupIYToF2n;

1.13
date	2015.02.11.07.01.37;	author jsg;	state Exp;
branches;
next	1.12;
commitid	dLgISW35NAmGN8Xl;

1.12
date	2015.02.10.10.50.49;	author jsg;	state Exp;
branches;
next	1.11;
commitid	aHLMSW1RfE1rmMw9;

1.11
date	2015.02.10.06.19.36;	author jsg;	state Exp;
branches;
next	1.10;
commitid	0OQNG9STPII6jEb4;

1.10
date	2015.01.27.03.17.36;	author dlg;	state Exp;
branches;
next	1.9;
commitid	MyKPm9Q3dQu92BiX;

1.9
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.8;
commitid	yv0ECmCdICvq576h;

1.8
date	2014.04.12.06.08.22;	author jsg;	state Exp;
branches;
next	1.7;

1.7
date	2014.02.10.02.35.09;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2014.02.09.10.57.26;	author jsg;	state Exp;
branches;
next	1.5;

1.5
date	2013.12.08.07.54.06;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2013.10.30.02.11.33;	author dlg;	state Exp;
branches;
next	1.3;

1.3
date	2013.10.29.06.30.57;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2013.09.02.07.14.22;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2013.08.12.04.11.53;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.20
log
@Switch the TTM code over to the generic DRM VMA manager.

ok jsg@@
@
text
@/*	$OpenBSD: ttm_bo.c,v 1.19 2015/10/23 08:21:58 kettenis Exp $	*/
/**************************************************************************
 *
 * Copyright (c) 2006-2009 VMware, Inc., Palo Alto, CA., USA
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/
/*
 * Authors: Thomas Hellstrom <thellstrom-at-vmware-dot-com>
 */

#define pr_fmt(fmt) "[TTM] " fmt

#include <dev/pci/drm/ttm/ttm_module.h>
#include <dev/pci/drm/ttm/ttm_bo_driver.h>
#include <dev/pci/drm/ttm/ttm_placement.h>

#define TTM_ASSERT_LOCKED(param)
#define TTM_DEBUG(fmt, arg...)
#define TTM_BO_HASH_ORDER 13

static int ttm_bo_setup_vm(struct ttm_buffer_object *bo);
static int ttm_bo_swapout(struct ttm_mem_shrink *shrink);
static void ttm_bo_global_kobj_release(struct kobject *kobj);

int ttm_bo_move_buffer(struct ttm_buffer_object *, struct ttm_placement *,
    bool, bool);

#ifdef notyet
static struct attribute ttm_bo_count = {
	.name = "bo_count",
	.mode = S_IRUGO
};
#endif

struct kobject *
ttm_get_kobj(void)
{
	return (NULL);
}

static inline int ttm_mem_type_from_flags(uint32_t flags, uint32_t *mem_type)
{
	int i;

	for (i = 0; i <= TTM_PL_PRIV5; i++)
		if (flags & (1 << i)) {
			*mem_type = i;
			return 0;
		}
	return -EINVAL;
}

static void ttm_mem_type_debug(struct ttm_bo_device *bdev, int mem_type)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem_type];

	pr_err("    has_type: %d\n", man->has_type);
	pr_err("    use_type: %d\n", man->use_type);
	pr_err("    flags: 0x%08X\n", man->flags);
	pr_err("    gpu_offset: 0x%08lX\n", man->gpu_offset);
	pr_err("    size: %llu\n", man->size);
	pr_err("    available_caching: 0x%08X\n", man->available_caching);
	pr_err("    default_caching: 0x%08X\n", man->default_caching);
	if (mem_type != TTM_PL_SYSTEM)
		(*man->func->debug)(man, TTM_PFX);
}

static void ttm_bo_mem_space_debug(struct ttm_buffer_object *bo,
					struct ttm_placement *placement)
{
	int i, ret, mem_type;

	pr_err("No space for %p (%lu pages, %luK, %luM)\n",
	       bo, bo->mem.num_pages, bo->mem.size >> 10,
	       bo->mem.size >> 20);
	for (i = 0; i < placement->num_placement; i++) {
		ret = ttm_mem_type_from_flags(placement->placement[i],
						&mem_type);
		if (ret)
			return;
		pr_err("  placement[%d]=0x%08X (%d)\n",
		       i, placement->placement[i], mem_type);
		ttm_mem_type_debug(bo->bdev, mem_type);
	}
}

#ifdef notyet
static ssize_t ttm_bo_global_show(struct kobject *kobj,
				  struct attribute *attr,
				  char *buffer)
{
	struct ttm_bo_global *glob =
		container_of(kobj, struct ttm_bo_global, kobj);

	return snprintf(buffer, PAGE_SIZE, "%lu\n",
			(unsigned long) atomic_read(&glob->bo_count));
}

static struct attribute *ttm_bo_global_attrs[] = {
	&ttm_bo_count,
	NULL
};

static const struct sysfs_ops ttm_bo_global_ops = {
	.show = &ttm_bo_global_show
};
#endif

static struct kobj_type ttm_bo_glob_kobj_type  = {
	.release = &ttm_bo_global_kobj_release,
#ifdef __linux__
	.sysfs_ops = &ttm_bo_global_ops,
	.default_attrs = ttm_bo_global_attrs
#endif
};

static inline uint32_t ttm_bo_type_flags(unsigned type)
{
	return 1 << (type);
}

static void ttm_bo_release_list(struct kref *list_kref)
{
	struct ttm_buffer_object *bo =
	    container_of(list_kref, struct ttm_buffer_object, list_kref);
	struct ttm_bo_device *bdev = bo->bdev;
	size_t acc_size = bo->acc_size;

	BUG_ON(atomic_read(&bo->list_kref.refcount));
	BUG_ON(atomic_read(&bo->kref.refcount));
	BUG_ON(atomic_read(&bo->cpu_writers));
	BUG_ON(bo->sync_obj != NULL);
	BUG_ON(bo->mem.mm_node != NULL);
	BUG_ON(!list_empty(&bo->lru));
	BUG_ON(!list_empty(&bo->ddestroy));

	if (bo->ttm)
		ttm_tt_destroy(bo->ttm);
	atomic_dec(&bo->glob->bo_count);
	if (bo->destroy)
		bo->destroy(bo);
	else {
		kfree(bo);
	}
	ttm_mem_global_free(bdev->glob->mem_glob, acc_size);
}

int ttm_bo_wait_unreserved(struct ttm_buffer_object *bo, bool interruptible)
{
	int ret = 0;

	while (ret == 0) {
		if (!ttm_bo_is_reserved(bo))
			break;
		ret = -tsleep(&bo->event_queue,
		    PZERO | (interruptible ? PCATCH : 0), "ttmwt", 0);
		
	}

	return (ret);
}
EXPORT_SYMBOL(ttm_bo_wait_unreserved);

void ttm_bo_add_to_lru(struct ttm_buffer_object *bo)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_type_manager *man;

	BUG_ON(!ttm_bo_is_reserved(bo));

	if (!(bo->mem.placement & TTM_PL_FLAG_NO_EVICT)) {

		BUG_ON(!list_empty(&bo->lru));

		man = &bdev->man[bo->mem.mem_type];
		list_add_tail(&bo->lru, &man->lru);
		kref_get(&bo->list_kref);

		if (bo->ttm != NULL) {
			list_add_tail(&bo->swap, &bo->glob->swap_lru);
			kref_get(&bo->list_kref);
		}
	}
}

int ttm_bo_del_from_lru(struct ttm_buffer_object *bo)
{
	int put_count = 0;

	if (!list_empty(&bo->swap)) {
		list_del_init(&bo->swap);
		++put_count;
	}
	if (!list_empty(&bo->lru)) {
		list_del_init(&bo->lru);
		++put_count;
	}

	/*
	 * TODO: Add a driver hook to delete from
	 * driver-specific LRU's here.
	 */

	return put_count;
}

int ttm_bo_reserve_locked(struct ttm_buffer_object *bo,
			  bool interruptible,
			  bool no_wait, bool use_sequence, uint32_t sequence)
{
	struct ttm_bo_global *glob = bo->glob;
	int ret;

	while (unlikely(atomic_read(&bo->reserved) != 0)) {
		/**
		 * Deadlock avoidance for multi-bo reserving.
		 */
		if (use_sequence && bo->seq_valid) {
			/**
			 * We've already reserved this one.
			 */
			if (unlikely(sequence == bo->val_seq))
				return -EDEADLK;
			/**
			 * Already reserved by a thread that will not back
			 * off for us. We need to back off.
			 */
			if (unlikely(sequence - bo->val_seq < (1 << 31)))
				return -EAGAIN;
		}

		if (no_wait)
			return -EBUSY;

		spin_unlock(&glob->lru_lock);
		ret = ttm_bo_wait_unreserved(bo, interruptible);
		spin_lock(&glob->lru_lock);

		if (unlikely(ret))
			return ret;
	}

	atomic_set(&bo->reserved, 1);
	if (use_sequence) {
		/**
		 * Wake up waiters that may need to recheck for deadlock,
		 * if we decreased the sequence number.
		 */
		if (unlikely((bo->val_seq - sequence < (1 << 31))
			     || !bo->seq_valid))
			wake_up_all(&bo->event_queue);

		bo->val_seq = sequence;
		bo->seq_valid = true;
	} else {
		bo->seq_valid = false;
	}

	return 0;
}
EXPORT_SYMBOL(ttm_bo_reserve);

static void ttm_bo_ref_bug(struct kref *list_kref)
{
	BUG();
}

void ttm_bo_list_ref_sub(struct ttm_buffer_object *bo, int count,
			 bool never_free)
{
	kref_sub(&bo->list_kref, count,
		 (never_free) ? ttm_bo_ref_bug : ttm_bo_release_list);
}

int ttm_bo_reserve(struct ttm_buffer_object *bo,
		   bool interruptible,
		   bool no_wait, bool use_sequence, uint32_t sequence)
{
	struct ttm_bo_global *glob = bo->glob;
	int put_count = 0;
	int ret;

	spin_lock(&glob->lru_lock);
	ret = ttm_bo_reserve_locked(bo, interruptible, no_wait, use_sequence,
				    sequence);
	if (likely(ret == 0))
		put_count = ttm_bo_del_from_lru(bo);
	spin_unlock(&glob->lru_lock);

	ttm_bo_list_ref_sub(bo, put_count, true);

	return ret;
}

void ttm_bo_unreserve_locked(struct ttm_buffer_object *bo)
{
	ttm_bo_add_to_lru(bo);
	atomic_set(&bo->reserved, 0);
	wake_up_all(&bo->event_queue);
}

void ttm_bo_unreserve(struct ttm_buffer_object *bo)
{
	struct ttm_bo_global *glob = bo->glob;

	spin_lock(&glob->lru_lock);
	ttm_bo_unreserve_locked(bo);
	spin_unlock(&glob->lru_lock);
}
EXPORT_SYMBOL(ttm_bo_unreserve);

/*
 * Call bo->mutex locked.
 */
static int ttm_bo_add_ttm(struct ttm_buffer_object *bo, bool zero_alloc)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_bo_global *glob = bo->glob;
	int ret = 0;
	uint32_t page_flags = 0;

#ifdef notyet
	rw_assert_wrlock(&bo->mutex);
#endif
	bo->ttm = NULL;

	if (bdev->need_dma32)
		page_flags |= TTM_PAGE_FLAG_DMA32;

	switch (bo->type) {
	case ttm_bo_type_device:
		if (zero_alloc)
			page_flags |= TTM_PAGE_FLAG_ZERO_ALLOC;
	case ttm_bo_type_kernel:
		bo->ttm = bdev->driver->ttm_tt_create(bdev, bo->num_pages << PAGE_SHIFT,
						      page_flags, glob->dummy_read_page);
		if (unlikely(bo->ttm == NULL))
			ret = -ENOMEM;
		break;
	case ttm_bo_type_sg:
		bo->ttm = bdev->driver->ttm_tt_create(bdev, bo->num_pages << PAGE_SHIFT,
						      page_flags | TTM_PAGE_FLAG_SG,
						      glob->dummy_read_page);
		if (unlikely(bo->ttm == NULL)) {
			ret = -ENOMEM;
			break;
		}
		bo->ttm->sg = bo->sg;
		break;
	default:
		pr_err("Illegal buffer object type\n");
		ret = -EINVAL;
		break;
	}

	return ret;
}

static int ttm_bo_handle_move_mem(struct ttm_buffer_object *bo,
				  struct ttm_mem_reg *mem,
				  bool evict, bool interruptible,
				  bool no_wait_gpu)
{
	struct ttm_bo_device *bdev = bo->bdev;
	bool old_is_pci = ttm_mem_reg_is_pci(bdev, &bo->mem);
	bool new_is_pci = ttm_mem_reg_is_pci(bdev, mem);
	struct ttm_mem_type_manager *old_man = &bdev->man[bo->mem.mem_type];
	struct ttm_mem_type_manager *new_man = &bdev->man[mem->mem_type];
	int ret = 0;

	if (old_is_pci || new_is_pci ||
	    ((mem->placement & bo->mem.placement & TTM_PL_MASK_CACHING) == 0)) {
		ret = ttm_mem_io_lock(old_man, true);
		if (unlikely(ret != 0))
			goto out_err;
		ttm_bo_unmap_virtual_locked(bo);
		ttm_mem_io_unlock(old_man);
	}

	/*
	 * Create and bind a ttm if required.
	 */

	if (!(new_man->flags & TTM_MEMTYPE_FLAG_FIXED)) {
		if (bo->ttm == NULL) {
			bool zero = !(old_man->flags & TTM_MEMTYPE_FLAG_FIXED);
			ret = ttm_bo_add_ttm(bo, zero);
			if (ret)
				goto out_err;
		}

		ret = ttm_tt_set_placement_caching(bo->ttm, mem->placement);
		if (ret)
			goto out_err;

		if (mem->mem_type != TTM_PL_SYSTEM) {
			ret = ttm_tt_bind(bo->ttm, mem);
			if (ret)
				goto out_err;
		}

		if (bo->mem.mem_type == TTM_PL_SYSTEM) {
			if (bdev->driver->move_notify)
				bdev->driver->move_notify(bo, mem);
			bo->mem = *mem;
			mem->mm_node = NULL;
			goto moved;
		}
	}

	if (bdev->driver->move_notify)
		bdev->driver->move_notify(bo, mem);

	if (!(old_man->flags & TTM_MEMTYPE_FLAG_FIXED) &&
	    !(new_man->flags & TTM_MEMTYPE_FLAG_FIXED))
		ret = ttm_bo_move_ttm(bo, evict, no_wait_gpu, mem);
	else if (bdev->driver->move)
		ret = bdev->driver->move(bo, evict, interruptible,
					 no_wait_gpu, mem);
	else
		ret = ttm_bo_move_memcpy(bo, evict, no_wait_gpu, mem);

	if (ret) {
		if (bdev->driver->move_notify) {
			struct ttm_mem_reg tmp_mem = *mem;
			*mem = bo->mem;
			bo->mem = tmp_mem;
			bdev->driver->move_notify(bo, mem);
			bo->mem = *mem;
			*mem = tmp_mem;
		}

		goto out_err;
	}

moved:
	if (bo->evicted) {
		if (bdev->driver->invalidate_caches) {
			ret = bdev->driver->invalidate_caches(bdev, bo->mem.placement);
			if (ret)
				pr_err("Can not flush read caches\n");
		}
		bo->evicted = false;
	}

	if (bo->mem.mm_node) {
		bo->offset = (bo->mem.start << PAGE_SHIFT) +
		    bdev->man[bo->mem.mem_type].gpu_offset;
		bo->cur_placement = bo->mem.placement;
	} else
		bo->offset = 0;

	return 0;

out_err:
	new_man = &bdev->man[bo->mem.mem_type];
	if ((new_man->flags & TTM_MEMTYPE_FLAG_FIXED) && bo->ttm) {
		ttm_tt_unbind(bo->ttm);
		ttm_tt_destroy(bo->ttm);
		bo->ttm = NULL;
	}

	return ret;
}

/**
 * Call bo::reserved.
 * Will release GPU memory type usage on destruction.
 * This is the place to put in driver specific hooks to release
 * driver private resources.
 * Will release the bo::reserved lock.
 */

static void ttm_bo_cleanup_memtype_use(struct ttm_buffer_object *bo)
{
	if (bo->bdev->driver->move_notify)
		bo->bdev->driver->move_notify(bo, NULL);

	if (bo->ttm) {
		ttm_tt_unbind(bo->ttm);
		ttm_tt_destroy(bo->ttm);
		bo->ttm = NULL;
	}
	ttm_bo_mem_put(bo, &bo->mem);

	atomic_set(&bo->reserved, 0);
	wake_up_all(&bo->event_queue);

	/*
	 * Since the final reference to this bo may not be dropped by
	 * the current task we have to put a memory barrier here to make
	 * sure the changes done in this function are always visible.
	 *
	 * This function only needs protection against the final kref_put.
	 */
	smp_mb__before_atomic_dec();
}

static void ttm_bo_cleanup_refs_or_queue(struct ttm_buffer_object *bo)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_bo_global *glob = bo->glob;
	struct ttm_bo_driver *driver = bdev->driver;
	void *sync_obj = NULL;
	int put_count;
	int ret;

	spin_lock(&glob->lru_lock);
	ret = ttm_bo_reserve_locked(bo, false, true, false, 0);

	spin_lock(&bdev->fence_lock);
	(void) ttm_bo_wait(bo, false, false, true);
	if (!ret && !bo->sync_obj) {
		spin_unlock(&bdev->fence_lock);
		put_count = ttm_bo_del_from_lru(bo);

		spin_unlock(&glob->lru_lock);
		ttm_bo_cleanup_memtype_use(bo);

		ttm_bo_list_ref_sub(bo, put_count, true);

		return;
	}
	if (bo->sync_obj)
		sync_obj = driver->sync_obj_ref(bo->sync_obj);
	spin_unlock(&bdev->fence_lock);

	if (!ret) {
		atomic_set(&bo->reserved, 0);
		wake_up_all(&bo->event_queue);
	}

	kref_get(&bo->list_kref);
	list_add_tail(&bo->ddestroy, &bdev->ddestroy);
	spin_unlock(&glob->lru_lock);

	if (sync_obj) {
		driver->sync_obj_flush(sync_obj);
		driver->sync_obj_unref(&sync_obj);
	}
	schedule_delayed_work(&bdev->wq,
			      ((HZ / 100) < 1) ? 1 : HZ / 100);
}

/**
 * function ttm_bo_cleanup_refs_and_unlock
 * If bo idle, remove from delayed- and lru lists, and unref.
 * If not idle, do nothing.
 *
 * Must be called with lru_lock and reservation held, this function
 * will drop both before returning.
 *
 * @@interruptible         Any sleeps should occur interruptibly.
 * @@no_wait_gpu           Never wait for gpu. Return -EBUSY instead.
 */

static int ttm_bo_cleanup_refs_and_unlock(struct ttm_buffer_object *bo,
					  bool interruptible,
					  bool no_wait_gpu)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_bo_driver *driver = bdev->driver;
	struct ttm_bo_global *glob = bo->glob;
	int put_count;
	int ret;

	spin_lock(&bdev->fence_lock);
	ret = ttm_bo_wait(bo, false, false, true);

	if (ret && !no_wait_gpu) {
		void *sync_obj;

		/*
		 * Take a reference to the fence and unreserve,
		 * at this point the buffer should be dead, so
		 * no new sync objects can be attached.
		 */
		sync_obj = driver->sync_obj_ref(bo->sync_obj);
		spin_unlock(&bdev->fence_lock);

		atomic_set(&bo->reserved, 0);
		wake_up_all(&bo->event_queue);
		spin_unlock(&glob->lru_lock);

		ret = driver->sync_obj_wait(sync_obj, false, interruptible);
		driver->sync_obj_unref(&sync_obj);
		if (ret)
			return ret;

		/*
		 * remove sync_obj with ttm_bo_wait, the wait should be
		 * finished, and no new wait object should have been added.
		 */
		spin_lock(&bdev->fence_lock);
		ret = ttm_bo_wait(bo, false, false, true);
		WARN_ON(ret);
		spin_unlock(&bdev->fence_lock);
		if (ret)
			return ret;

		spin_lock(&glob->lru_lock);
		ret = ttm_bo_reserve_locked(bo, false, true, false, 0);

		/*
		 * We raced, and lost, someone else holds the reservation now,
		 * and is probably busy in ttm_bo_cleanup_memtype_use.
		 *
		 * Even if it's not the case, because we finished waiting any
		 * delayed destruction would succeed, so just return success
		 * here.
		 */
		if (ret) {
			spin_unlock(&glob->lru_lock);
			return 0;
		}
	} else
		spin_unlock(&bdev->fence_lock);

	if (ret || unlikely(list_empty(&bo->ddestroy))) {
		atomic_set(&bo->reserved, 0);
		wake_up_all(&bo->event_queue);
		spin_unlock(&glob->lru_lock);
		return ret;
	}

	put_count = ttm_bo_del_from_lru(bo);
	list_del_init(&bo->ddestroy);
	++put_count;

	spin_unlock(&glob->lru_lock);
	ttm_bo_cleanup_memtype_use(bo);

	ttm_bo_list_ref_sub(bo, put_count, true);

	return 0;
}

/**
 * Traverse the delayed list, and call ttm_bo_cleanup_refs on all
 * encountered buffers.
 */

static int ttm_bo_delayed_delete(struct ttm_bo_device *bdev, bool remove_all)
{
	struct ttm_bo_global *glob = bdev->glob;
	struct ttm_buffer_object *entry = NULL;
	int ret = 0;

	spin_lock(&glob->lru_lock);
	if (list_empty(&bdev->ddestroy))
		goto out_unlock;

	entry = list_first_entry(&bdev->ddestroy,
		struct ttm_buffer_object, ddestroy);
	kref_get(&entry->list_kref);

	for (;;) {
		struct ttm_buffer_object *nentry = NULL;

		if (entry->ddestroy.next != &bdev->ddestroy) {
			nentry = list_first_entry(&entry->ddestroy,
				struct ttm_buffer_object, ddestroy);
			kref_get(&nentry->list_kref);
		}

		ret = ttm_bo_reserve_locked(entry, false, !remove_all, false, 0);
		if (!ret)
			ret = ttm_bo_cleanup_refs_and_unlock(entry, false,
							     !remove_all);
		else
			spin_unlock(&glob->lru_lock);

		kref_put(&entry->list_kref, ttm_bo_release_list);
		entry = nentry;

		if (ret || !entry)
			goto out;

		spin_lock(&glob->lru_lock);
		if (list_empty(&entry->ddestroy))
			break;
	}

out_unlock:
	spin_unlock(&glob->lru_lock);
out:
	if (entry)
		kref_put(&entry->list_kref, ttm_bo_release_list);
	return ret;
}

static void ttm_bo_delayed_workqueue(struct work_struct *work)
{
	struct ttm_bo_device *bdev =
	    container_of(work, struct ttm_bo_device, wq.work);

	if (ttm_bo_delayed_delete(bdev, false)) {
		schedule_delayed_work(&bdev->wq,
				      ((HZ / 100) < 1) ? 1 : HZ / 100);
	}
}

static void ttm_bo_release(struct kref *kref)
{
	struct ttm_buffer_object *bo =
	    container_of(kref, struct ttm_buffer_object, kref);
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_type_manager *man = &bdev->man[bo->mem.mem_type];

	drm_vma_offset_remove(&bdev->vma_manager, &bo->vma_node);
	ttm_mem_io_lock(man, false);
	ttm_mem_io_free_vm(bo);
	ttm_mem_io_unlock(man);
	ttm_bo_cleanup_refs_or_queue(bo);
	kref_put(&bo->list_kref, ttm_bo_release_list);
}

void ttm_bo_unref(struct ttm_buffer_object **p_bo)
{
	struct ttm_buffer_object *bo = *p_bo;

	*p_bo = NULL;
	kref_put(&bo->kref, ttm_bo_release);
}
EXPORT_SYMBOL(ttm_bo_unref);

int ttm_bo_lock_delayed_workqueue(struct ttm_bo_device *bdev)
{
	return cancel_delayed_work_sync(&bdev->wq);
}
EXPORT_SYMBOL(ttm_bo_lock_delayed_workqueue);

void ttm_bo_unlock_delayed_workqueue(struct ttm_bo_device *bdev, int resched)
{
	if (resched)
		schedule_delayed_work(&bdev->wq,
				      ((HZ / 100) < 1) ? 1 : HZ / 100);
}
EXPORT_SYMBOL(ttm_bo_unlock_delayed_workqueue);

static int ttm_bo_evict(struct ttm_buffer_object *bo, bool interruptible,
			bool no_wait_gpu)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_reg evict_mem;
	struct ttm_placement placement;
	int ret = 0;

	spin_lock(&bdev->fence_lock);
	ret = ttm_bo_wait(bo, false, interruptible, no_wait_gpu);
	spin_unlock(&bdev->fence_lock);

	if (unlikely(ret != 0)) {
		if (ret != -ERESTARTSYS) {
			pr_err("Failed to expire sync object before buffer eviction\n");
		}
		goto out;
	}

	BUG_ON(!ttm_bo_is_reserved(bo));

	evict_mem = bo->mem;
	evict_mem.mm_node = NULL;
	evict_mem.bus.io_reserved_vm = false;
	evict_mem.bus.io_reserved_count = 0;

	placement.fpfn = 0;
	placement.lpfn = 0;
	placement.num_placement = 0;
	placement.num_busy_placement = 0;
	bdev->driver->evict_flags(bo, &placement);
	ret = ttm_bo_mem_space(bo, &placement, &evict_mem, interruptible,
				no_wait_gpu);
	if (ret) {
		if (ret != -ERESTARTSYS) {
			pr_err("Failed to find memory space for buffer 0x%p eviction\n",
			       bo);
			ttm_bo_mem_space_debug(bo, &placement);
		}
		goto out;
	}

	ret = ttm_bo_handle_move_mem(bo, &evict_mem, true, interruptible,
				     no_wait_gpu);
	if (ret) {
		if (ret != -ERESTARTSYS)
			pr_err("Buffer eviction failed\n");
		ttm_bo_mem_put(bo, &evict_mem);
		goto out;
	}
	bo->evicted = true;
out:
	return ret;
}

static int ttm_mem_evict_first(struct ttm_bo_device *bdev,
				uint32_t mem_type,
				bool interruptible,
				bool no_wait_gpu)
{
	struct ttm_bo_global *glob = bdev->glob;
	struct ttm_mem_type_manager *man = &bdev->man[mem_type];
	struct ttm_buffer_object *bo;
	int ret = -EBUSY, put_count;

	spin_lock(&glob->lru_lock);
	list_for_each_entry(bo, &man->lru, lru) {
		ret = ttm_bo_reserve_locked(bo, false, true, false, 0);
		if (!ret)
			break;
	}

	if (ret) {
		spin_unlock(&glob->lru_lock);
		return ret;
	}

	kref_get(&bo->list_kref);

	if (!list_empty(&bo->ddestroy)) {
		ret = ttm_bo_cleanup_refs_and_unlock(bo, interruptible,
						     no_wait_gpu);
		kref_put(&bo->list_kref, ttm_bo_release_list);
		return ret;
	}

	put_count = ttm_bo_del_from_lru(bo);
	spin_unlock(&glob->lru_lock);

	BUG_ON(ret != 0);

	ttm_bo_list_ref_sub(bo, put_count, true);

	ret = ttm_bo_evict(bo, interruptible, no_wait_gpu);
	ttm_bo_unreserve(bo);

	kref_put(&bo->list_kref, ttm_bo_release_list);
	return ret;
}

void ttm_bo_mem_put(struct ttm_buffer_object *bo, struct ttm_mem_reg *mem)
{
	struct ttm_mem_type_manager *man = &bo->bdev->man[mem->mem_type];

	if (mem->mm_node)
		(*man->func->put_node)(man, mem);
}
EXPORT_SYMBOL(ttm_bo_mem_put);

/**
 * Repeatedly evict memory from the LRU for @@mem_type until we create enough
 * space, or we've evicted everything and there isn't enough space.
 */
static int ttm_bo_mem_force_space(struct ttm_buffer_object *bo,
					uint32_t mem_type,
					struct ttm_placement *placement,
					struct ttm_mem_reg *mem,
					bool interruptible,
					bool no_wait_gpu)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_type_manager *man = &bdev->man[mem_type];
	int ret;

	do {
		ret = (*man->func->get_node)(man, bo, placement, mem);
		if (unlikely(ret != 0))
			return ret;
		if (mem->mm_node)
			break;
		ret = ttm_mem_evict_first(bdev, mem_type,
					  interruptible, no_wait_gpu);
		if (unlikely(ret != 0))
			return ret;
	} while (1);
	if (mem->mm_node == NULL)
		return -ENOMEM;
	mem->mem_type = mem_type;
	return 0;
}

static uint32_t ttm_bo_select_caching(struct ttm_mem_type_manager *man,
				      uint32_t cur_placement,
				      uint32_t proposed_placement)
{
	uint32_t caching = proposed_placement & TTM_PL_MASK_CACHING;
	uint32_t result = proposed_placement & ~TTM_PL_MASK_CACHING;

	/**
	 * Keep current caching if possible.
	 */

	if ((cur_placement & caching) != 0)
		result |= (cur_placement & caching);
	else if ((man->default_caching & caching) != 0)
		result |= man->default_caching;
	else if ((TTM_PL_FLAG_CACHED & caching) != 0)
		result |= TTM_PL_FLAG_CACHED;
	else if ((TTM_PL_FLAG_WC & caching) != 0)
		result |= TTM_PL_FLAG_WC;
	else if ((TTM_PL_FLAG_UNCACHED & caching) != 0)
		result |= TTM_PL_FLAG_UNCACHED;

	return result;
}

static bool ttm_bo_mt_compatible(struct ttm_mem_type_manager *man,
				 uint32_t mem_type,
				 uint32_t proposed_placement,
				 uint32_t *masked_placement)
{
	uint32_t cur_flags = ttm_bo_type_flags(mem_type);

	if ((cur_flags & proposed_placement & TTM_PL_MASK_MEM) == 0)
		return false;

	if ((proposed_placement & man->available_caching) == 0)
		return false;

	cur_flags |= (proposed_placement & man->available_caching);

	*masked_placement = cur_flags;
	return true;
}

/**
 * Creates space for memory region @@mem according to its type.
 *
 * This function first searches for free space in compatible memory types in
 * the priority order defined by the driver.  If free space isn't found, then
 * ttm_bo_mem_force_space is attempted in priority order to evict and find
 * space.
 */
int ttm_bo_mem_space(struct ttm_buffer_object *bo,
			struct ttm_placement *placement,
			struct ttm_mem_reg *mem,
			bool interruptible,
			bool no_wait_gpu)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_type_manager *man;
	uint32_t mem_type = TTM_PL_SYSTEM;
	uint32_t cur_flags = 0;
	bool type_found = false;
	bool type_ok = false;
	bool has_erestartsys = false;
	int i, ret;

	mem->mm_node = NULL;
	for (i = 0; i < placement->num_placement; ++i) {
		ret = ttm_mem_type_from_flags(placement->placement[i],
						&mem_type);
		if (ret)
			return ret;
		man = &bdev->man[mem_type];

		type_ok = ttm_bo_mt_compatible(man,
						mem_type,
						placement->placement[i],
						&cur_flags);

		if (!type_ok)
			continue;

		cur_flags = ttm_bo_select_caching(man, bo->mem.placement,
						  cur_flags);
		/*
		 * Use the access and other non-mapping-related flag bits from
		 * the memory placement flags to the current flags
		 */
		ttm_flag_masked(&cur_flags, placement->placement[i],
				~TTM_PL_MASK_MEMTYPE);

		if (mem_type == TTM_PL_SYSTEM)
			break;

		if (man->has_type && man->use_type) {
			type_found = true;
			ret = (*man->func->get_node)(man, bo, placement, mem);
			if (unlikely(ret))
				return ret;
		}
		if (mem->mm_node)
			break;
	}

	if ((type_ok && (mem_type == TTM_PL_SYSTEM)) || mem->mm_node) {
		mem->mem_type = mem_type;
		mem->placement = cur_flags;
		return 0;
	}

	if (!type_found)
		return -EINVAL;

	for (i = 0; i < placement->num_busy_placement; ++i) {
		ret = ttm_mem_type_from_flags(placement->busy_placement[i],
						&mem_type);
		if (ret)
			return ret;
		man = &bdev->man[mem_type];
		if (!man->has_type)
			continue;
		if (!ttm_bo_mt_compatible(man,
						mem_type,
						placement->busy_placement[i],
						&cur_flags))
			continue;

		cur_flags = ttm_bo_select_caching(man, bo->mem.placement,
						  cur_flags);
		/*
		 * Use the access and other non-mapping-related flag bits from
		 * the memory placement flags to the current flags
		 */
		ttm_flag_masked(&cur_flags, placement->busy_placement[i],
				~TTM_PL_MASK_MEMTYPE);


		if (mem_type == TTM_PL_SYSTEM) {
			mem->mem_type = mem_type;
			mem->placement = cur_flags;
			mem->mm_node = NULL;
			return 0;
		}

		ret = ttm_bo_mem_force_space(bo, mem_type, placement, mem,
						interruptible, no_wait_gpu);
		if (ret == 0 && mem->mm_node) {
			mem->placement = cur_flags;
			return 0;
		}
		if (ret == -ERESTART)
			has_erestartsys = true;
	}
	ret = (has_erestartsys) ? -ERESTART: -ENOMEM;
	return ret;
}
EXPORT_SYMBOL(ttm_bo_mem_space);

int ttm_bo_move_buffer(struct ttm_buffer_object *bo,
			struct ttm_placement *placement,
			bool interruptible,
			bool no_wait_gpu)
{
	int ret = 0;
	struct ttm_mem_reg mem;
	struct ttm_bo_device *bdev = bo->bdev;

	BUG_ON(!ttm_bo_is_reserved(bo));

	/*
	 * FIXME: It's possible to pipeline buffer moves.
	 * Have the driver move function wait for idle when necessary,
	 * instead of doing it here.
	 */
	spin_lock(&bdev->fence_lock);
	ret = ttm_bo_wait(bo, false, interruptible, no_wait_gpu);
	spin_unlock(&bdev->fence_lock);
	if (ret)
		return ret;
	mem.num_pages = bo->num_pages;
	mem.size = mem.num_pages << PAGE_SHIFT;
	mem.page_alignment = bo->mem.page_alignment;
	mem.bus.io_reserved_vm = false;
	mem.bus.io_reserved_count = 0;
	/*
	 * Determine where to move the buffer.
	 */
	ret = ttm_bo_mem_space(bo, placement, &mem,
			       interruptible, no_wait_gpu);
	if (ret)
		goto out_unlock;
	ret = ttm_bo_handle_move_mem(bo, &mem, false,
				     interruptible, no_wait_gpu);
out_unlock:
	if (ret && mem.mm_node)
		ttm_bo_mem_put(bo, &mem);
	return ret;
}

static bool ttm_bo_mem_compat(struct ttm_placement *placement,
			      struct ttm_mem_reg *mem,
			      uint32_t *new_flags)
{
	int i;

	if (mem->mm_node && placement->lpfn != 0 &&
	    (mem->start < placement->fpfn ||
	     mem->start + mem->num_pages > placement->lpfn))
		return false;

	for (i = 0; i < placement->num_placement; i++) {
		*new_flags = placement->placement[i];
		if ((*new_flags & mem->placement & TTM_PL_MASK_CACHING) &&
		    (*new_flags & mem->placement & TTM_PL_MASK_MEM))
			return true;
	}

	for (i = 0; i < placement->num_busy_placement; i++) {
		*new_flags = placement->busy_placement[i];
		if ((*new_flags & mem->placement & TTM_PL_MASK_CACHING) &&
		    (*new_flags & mem->placement & TTM_PL_MASK_MEM))
			return true;
	}

	return false;
}

int ttm_bo_validate(struct ttm_buffer_object *bo,
			struct ttm_placement *placement,
			bool interruptible,
			bool no_wait_gpu)
{
	int ret;
	uint32_t new_flags;

	BUG_ON(!ttm_bo_is_reserved(bo));
	/* Check that range is valid */
	if (placement->lpfn || placement->fpfn)
		if (placement->fpfn > placement->lpfn ||
			(placement->lpfn - placement->fpfn) < bo->num_pages)
			return -EINVAL;
	/*
	 * Check whether we need to move buffer.
	 */
	if (!ttm_bo_mem_compat(placement, &bo->mem, &new_flags)) {
		ret = ttm_bo_move_buffer(bo, placement, interruptible,
					 no_wait_gpu);
		if (ret)
			return ret;
	} else {
		/*
		 * Use the access and other non-mapping-related flag bits from
		 * the compatible memory placement flags to the active flags
		 */
		ttm_flag_masked(&bo->mem.placement, new_flags,
				~TTM_PL_MASK_MEMTYPE);
	}
	/*
	 * We might need to add a TTM.
	 */
	if (bo->mem.mem_type == TTM_PL_SYSTEM && bo->ttm == NULL) {
		ret = ttm_bo_add_ttm(bo, true);
		if (ret)
			return ret;
	}
	return 0;
}
EXPORT_SYMBOL(ttm_bo_validate);

int ttm_bo_check_placement(struct ttm_buffer_object *bo,
				struct ttm_placement *placement)
{
	BUG_ON((placement->fpfn || placement->lpfn) &&
	       (bo->mem.num_pages > (placement->lpfn - placement->fpfn)));

	return 0;
}

int ttm_bo_init(struct ttm_bo_device *bdev,
		struct ttm_buffer_object *bo,
		unsigned long size,
		enum ttm_bo_type type,
		struct ttm_placement *placement,
		uint32_t page_alignment,
		bool interruptible,
		struct uvm_object *persistent_swap_storage,
		size_t acc_size,
		struct sg_table *sg,
		void (*destroy) (struct ttm_buffer_object *))
{
	int ret = 0;
	unsigned long num_pages;
	struct ttm_mem_global *mem_glob = bdev->glob->mem_glob;

	ret = ttm_mem_global_alloc(mem_glob, acc_size, false, false);
	if (ret) {
		pr_err("Out of kernel memory\n");
		if (destroy)
			(*destroy)(bo);
		else
			kfree(bo);
		return -ENOMEM;
	}

	num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
	if (num_pages == 0) {
		pr_err("Illegal buffer object size\n");
		if (destroy)
			(*destroy)(bo);
		else
			kfree(bo);
		ttm_mem_global_free(mem_glob, acc_size);
		return -EINVAL;
	}
	bo->destroy = destroy;

	uvm_objinit(&bo->uobj, NULL, 0);
	kref_init(&bo->kref);
	kref_init(&bo->list_kref);
	atomic_set(&bo->cpu_writers, 0);
	atomic_set(&bo->reserved, 1);
	init_waitqueue_head(&bo->event_queue);
	INIT_LIST_HEAD(&bo->lru);
	INIT_LIST_HEAD(&bo->ddestroy);
	INIT_LIST_HEAD(&bo->swap);
	INIT_LIST_HEAD(&bo->io_reserve_lru);
	bo->bdev = bdev;
	bo->glob = bdev->glob;
	bo->type = type;
	bo->num_pages = num_pages;
	bo->mem.size = num_pages << PAGE_SHIFT;
	bo->mem.mem_type = TTM_PL_SYSTEM;
	bo->mem.num_pages = bo->num_pages;
	bo->mem.mm_node = NULL;
	bo->mem.page_alignment = page_alignment;
	bo->mem.bus.io_reserved_vm = false;
	bo->mem.bus.io_reserved_count = 0;
	bo->priv_flags = 0;
	bo->mem.placement = (TTM_PL_FLAG_SYSTEM | TTM_PL_FLAG_CACHED);
	bo->seq_valid = false;
	bo->persistent_swap_storage = persistent_swap_storage;
	bo->acc_size = acc_size;
	bo->sg = sg;
	atomic_inc(&bo->glob->bo_count);
	drm_vma_node_reset(&bo->vma_node);

	ret = ttm_bo_check_placement(bo, placement);
	if (unlikely(ret != 0))
		goto out_err;

	/*
	 * For ttm_bo_type_device buffers, allocate
	 * address space from the device.
	 */
	if (bo->type == ttm_bo_type_device ||
	    bo->type == ttm_bo_type_sg) {
		ret = ttm_bo_setup_vm(bo);
		if (ret)
			goto out_err;
	}

	ret = ttm_bo_validate(bo, placement, interruptible, false);
	if (ret)
		goto out_err;

	ttm_bo_unreserve(bo);
	return 0;

out_err:
	ttm_bo_unreserve(bo);
	ttm_bo_unref(&bo);

	return ret;
}
EXPORT_SYMBOL(ttm_bo_init);

size_t ttm_bo_acc_size(struct ttm_bo_device *bdev,
		       unsigned long bo_size,
		       unsigned struct_size)
{
	unsigned npages = (PAGE_ALIGN(bo_size)) >> PAGE_SHIFT;
	size_t size = 0;

	size += ttm_round_pot(struct_size);
	size += PAGE_ALIGN(npages * sizeof(void *));
	size += ttm_round_pot(sizeof(struct ttm_tt));
	return size;
}
EXPORT_SYMBOL(ttm_bo_acc_size);

size_t ttm_bo_dma_acc_size(struct ttm_bo_device *bdev,
			   unsigned long bo_size,
			   unsigned struct_size)
{
	unsigned npages = (PAGE_ALIGN(bo_size)) >> PAGE_SHIFT;
	size_t size = 0;

	size += ttm_round_pot(struct_size);
	size += PAGE_ALIGN(npages * sizeof(void *));
	size += PAGE_ALIGN(npages * sizeof(bus_addr_t));
	size += ttm_round_pot(sizeof(struct ttm_dma_tt));
	return size;
}
EXPORT_SYMBOL(ttm_bo_dma_acc_size);

int ttm_bo_create(struct ttm_bo_device *bdev,
			unsigned long size,
			enum ttm_bo_type type,
			struct ttm_placement *placement,
			uint32_t page_alignment,
			bool interruptible,
			struct uvm_object *persistent_swap_storage,
			struct ttm_buffer_object **p_bo)
{
	struct ttm_buffer_object *bo;
	size_t acc_size;
	int ret;

	bo = kzalloc(sizeof(*bo), GFP_KERNEL);
	if (unlikely(bo == NULL))
		return -ENOMEM;

	acc_size = ttm_bo_acc_size(bdev, size, sizeof(struct ttm_buffer_object));
	ret = ttm_bo_init(bdev, bo, size, type, placement, page_alignment,
			  interruptible, persistent_swap_storage, acc_size,
			  NULL, NULL);
	if (likely(ret == 0))
		*p_bo = bo;

	return ret;
}
EXPORT_SYMBOL(ttm_bo_create);

static int ttm_bo_force_list_clean(struct ttm_bo_device *bdev,
					unsigned mem_type, bool allow_errors)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem_type];
	struct ttm_bo_global *glob = bdev->glob;
	int ret;

	/*
	 * Can't use standard list traversal since we're unlocking.
	 */

	spin_lock(&glob->lru_lock);
	while (!list_empty(&man->lru)) {
		spin_unlock(&glob->lru_lock);
		ret = ttm_mem_evict_first(bdev, mem_type, false, false);
		if (ret) {
			if (allow_errors) {
				return ret;
			} else {
				pr_err("Cleanup eviction failed\n");
			}
		}
		spin_lock(&glob->lru_lock);
	}
	spin_unlock(&glob->lru_lock);
	return 0;
}

int ttm_bo_clean_mm(struct ttm_bo_device *bdev, unsigned mem_type)
{
	struct ttm_mem_type_manager *man;
	int ret = -EINVAL;

	if (mem_type >= TTM_NUM_MEM_TYPES) {
		pr_err("Illegal memory type %d\n", mem_type);
		return ret;
	}
	man = &bdev->man[mem_type];

	if (!man->has_type) {
		pr_err("Trying to take down uninitialized memory manager type %u\n",
		       mem_type);
		return ret;
	}

	man->use_type = false;
	man->has_type = false;

	ret = 0;
	if (mem_type > 0) {
		ttm_bo_force_list_clean(bdev, mem_type, false);

		ret = (*man->func->takedown)(man);
	}

	return ret;
}
EXPORT_SYMBOL(ttm_bo_clean_mm);

int ttm_bo_evict_mm(struct ttm_bo_device *bdev, unsigned mem_type)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem_type];

	if (mem_type == 0 || mem_type >= TTM_NUM_MEM_TYPES) {
		pr_err("Illegal memory manager memory type %u\n", mem_type);
		return -EINVAL;
	}

	if (!man->has_type) {
		pr_err("Memory type %u has not been initialized\n", mem_type);
		return 0;
	}

	return ttm_bo_force_list_clean(bdev, mem_type, true);
}
EXPORT_SYMBOL(ttm_bo_evict_mm);

int ttm_bo_init_mm(struct ttm_bo_device *bdev, unsigned type,
			unsigned long p_size)
{
	int ret = -EINVAL;
	struct ttm_mem_type_manager *man;

	BUG_ON(type >= TTM_NUM_MEM_TYPES);
	man = &bdev->man[type];
	BUG_ON(man->has_type);
	man->io_reserve_fastpath = true;
	man->use_io_reserve_lru = false;
	rw_init(&man->io_reserve_mutex, "ttm_iores");
	INIT_LIST_HEAD(&man->io_reserve_lru);

	ret = bdev->driver->init_mem_type(bdev, type, man);
	if (ret)
		return ret;
	man->bdev = bdev;

	ret = 0;
	if (type != TTM_PL_SYSTEM) {
		ret = (*man->func->init)(man, p_size);
		if (ret)
			return ret;
	}
	man->has_type = true;
	man->use_type = true;
	man->size = p_size;

	INIT_LIST_HEAD(&man->lru);

	return 0;
}
EXPORT_SYMBOL(ttm_bo_init_mm);

static void ttm_bo_global_kobj_release(struct kobject *kobj)
{
	struct ttm_bo_global *glob =
		container_of(kobj, struct ttm_bo_global, kobj);

	ttm_mem_unregister_shrink(glob->mem_glob, &glob->shrink);
	km_free(glob->dummy_read_page, PAGE_SIZE, &kv_any, &kp_dma_zero);
	kfree(glob);
}

void ttm_bo_global_release(struct drm_global_reference *ref)
{
	struct ttm_bo_global *glob = ref->object;

	kobject_del(&glob->kobj);
	kobject_put(&glob->kobj);
}
EXPORT_SYMBOL(ttm_bo_global_release);

int ttm_bo_global_init(struct drm_global_reference *ref)
{
	struct ttm_bo_global_ref *bo_ref =
		container_of(ref, struct ttm_bo_global_ref, ref);
	struct ttm_bo_global *glob = ref->object;
	int ret;

	rw_init(&glob->device_list_mutex, "ttm_devlist");
	mtx_init(&glob->lru_lock, IPL_NONE);
	glob->mem_glob = bo_ref->mem_glob;
	glob->dummy_read_page = km_alloc(PAGE_SIZE, &kv_any, &kp_dma_zero,
	    &kd_waitok);

	if (unlikely(glob->dummy_read_page == NULL)) {
		ret = -ENOMEM;
		goto out_no_drp;
	}

	INIT_LIST_HEAD(&glob->swap_lru);
	INIT_LIST_HEAD(&glob->device_list);

	ttm_mem_init_shrink(&glob->shrink, ttm_bo_swapout);
	ret = ttm_mem_register_shrink(glob->mem_glob, &glob->shrink);
	if (unlikely(ret != 0)) {
		pr_err("Could not register buffer object swapout\n");
		goto out_no_shrink;
	}

	atomic_set(&glob->bo_count, 0);

	ret = kobject_init_and_add(
		&glob->kobj, &ttm_bo_glob_kobj_type, ttm_get_kobj(), "buffer_objects");
	if (unlikely(ret != 0))
		kobject_put(&glob->kobj);
	return ret;
out_no_shrink:
	km_free(glob->dummy_read_page, PAGE_SIZE, &kv_any, &kp_dma_zero);
out_no_drp:
	kfree(glob);
	return ret;
}
EXPORT_SYMBOL(ttm_bo_global_init);


int ttm_bo_device_release(struct ttm_bo_device *bdev)
{
	int ret = 0;
	unsigned i = TTM_NUM_MEM_TYPES;
	struct ttm_mem_type_manager *man;
	struct ttm_bo_global *glob = bdev->glob;

	while (i--) {
		man = &bdev->man[i];
		if (man->has_type) {
			man->use_type = false;
			if ((i != TTM_PL_SYSTEM) && ttm_bo_clean_mm(bdev, i)) {
				ret = -EBUSY;
				pr_err("DRM memory manager type %d is not clean\n",
				       i);
			}
			man->has_type = false;
		}
	}

	mutex_lock(&glob->device_list_mutex);
	list_del(&bdev->device_list);
	mutex_unlock(&glob->device_list_mutex);

	cancel_delayed_work_sync(&bdev->wq);

	while (ttm_bo_delayed_delete(bdev, true))
		;

	spin_lock(&glob->lru_lock);
	if (list_empty(&bdev->ddestroy))
		TTM_DEBUG("Delayed destroy list was clean\n");

	if (list_empty(&bdev->man[0].lru))
		TTM_DEBUG("Swap list was clean\n");
	spin_unlock(&glob->lru_lock);

	drm_vma_offset_manager_destroy(&bdev->vma_manager);

	return ret;
}
EXPORT_SYMBOL(ttm_bo_device_release);

int ttm_bo_device_init(struct ttm_bo_device *bdev,
		       struct ttm_bo_global *glob,
		       struct ttm_bo_driver *driver,
		       uint64_t file_page_offset,
		       bool need_dma32)
{
	int ret = -EINVAL;

	bdev->driver = driver;

	memset(bdev->man, 0, sizeof(bdev->man));

	/*
	 * Initialize the system memory buffer type.
	 * Other types need to be driver / IOCTL initialized.
	 */
	ret = ttm_bo_init_mm(bdev, TTM_PL_SYSTEM, 0);
	if (unlikely(ret != 0))
		goto out_no_sys;

	drm_vma_offset_manager_init(&bdev->vma_manager, file_page_offset,
				    0x10000000);
	INIT_DELAYED_WORK(&bdev->wq, ttm_bo_delayed_workqueue);
	INIT_LIST_HEAD(&bdev->ddestroy);
	bdev->dev_mapping = NULL;
	bdev->glob = glob;
	bdev->need_dma32 = need_dma32;
	bdev->val_seq = 0;
	mtx_init(&bdev->fence_lock, IPL_NONE);
	mutex_lock(&glob->device_list_mutex);
	list_add_tail(&bdev->device_list, &glob->device_list);
	mutex_unlock(&glob->device_list_mutex);

	return 0;
out_no_sys:
	return ret;
}
EXPORT_SYMBOL(ttm_bo_device_init);

/*
 * buffer object vm functions.
 */

bool ttm_mem_reg_is_pci(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];

	if (!(man->flags & TTM_MEMTYPE_FLAG_FIXED)) {
		if (mem->mem_type == TTM_PL_SYSTEM)
			return false;

		if (man->flags & TTM_MEMTYPE_FLAG_CMA)
			return false;

		if (mem->placement & TTM_PL_FLAG_CACHED)
			return false;
	}
	return true;
}

void ttm_bo_unmap_virtual_locked(struct ttm_buffer_object *bo)
{
	struct ttm_tt *ttm = bo->ttm;
	struct vm_page *page;
	bus_addr_t addr;
	paddr_t paddr;
	int i;

	if (bo->mem.bus.is_iomem) {
		for (i = 0; i < bo->mem.num_pages; ++i) {
			addr = bo->mem.bus.base + bo->mem.bus.offset;
			paddr = bus_space_mmap(bo->bdev->memt, addr,    
					       i << PAGE_SHIFT, 0, 0);
			page = PHYS_TO_VM_PAGE(paddr);
			if (unlikely(page == NULL))
				continue;
			pmap_page_protect(page, PROT_NONE);
		}
	} else if (ttm) {
		for (i = 0; i < ttm->num_pages; ++i) {
			page = ttm->pages[i];
			if (unlikely(page == NULL))
				continue;
			pmap_page_protect(page, PROT_NONE);
		}
	}
	ttm_mem_io_free_vm(bo);
}

void ttm_bo_unmap_virtual(struct ttm_buffer_object *bo)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_type_manager *man = &bdev->man[bo->mem.mem_type];

	ttm_mem_io_lock(man, false);
	ttm_bo_unmap_virtual_locked(bo);
	ttm_mem_io_unlock(man);
}


EXPORT_SYMBOL(ttm_bo_unmap_virtual);

/**
 * ttm_bo_setup_vm:
 *
 * @@bo: the buffer to allocate address space for
 *
 * Allocate address space in the drm device so that applications
 * can mmap the buffer and access the contents. This only
 * applies to ttm_bo_type_device objects as others are not
 * placed in the drm device address space.
 */

static int ttm_bo_setup_vm(struct ttm_buffer_object *bo)
{
	struct ttm_bo_device *bdev = bo->bdev;

	return drm_vma_offset_add(&bdev->vma_manager, &bo->vma_node,
				  bo->mem.num_pages);
}

int ttm_bo_wait(struct ttm_buffer_object *bo,
		bool lazy, bool interruptible, bool no_wait)
{
	struct ttm_bo_driver *driver = bo->bdev->driver;
	struct ttm_bo_device *bdev = bo->bdev;
	void *sync_obj;
	int ret = 0;

	if (likely(bo->sync_obj == NULL))
		return 0;

	while (bo->sync_obj) {

		if (driver->sync_obj_signaled(bo->sync_obj)) {
			void *tmp_obj = bo->sync_obj;
			bo->sync_obj = NULL;
			clear_bit(TTM_BO_PRIV_FLAG_MOVING, &bo->priv_flags);
			spin_unlock(&bdev->fence_lock);
			driver->sync_obj_unref(&tmp_obj);
			spin_lock(&bdev->fence_lock);
			continue;
		}

		if (no_wait)
			return -EBUSY;

		sync_obj = driver->sync_obj_ref(bo->sync_obj);
		spin_unlock(&bdev->fence_lock);
		ret = driver->sync_obj_wait(sync_obj,
					    lazy, interruptible);
		if (unlikely(ret != 0)) {
			driver->sync_obj_unref(&sync_obj);
			spin_lock(&bdev->fence_lock);
			return ret;
		}
		spin_lock(&bdev->fence_lock);
		if (likely(bo->sync_obj == sync_obj)) {
			void *tmp_obj = bo->sync_obj;
			bo->sync_obj = NULL;
			clear_bit(TTM_BO_PRIV_FLAG_MOVING,
				  &bo->priv_flags);
			spin_unlock(&bdev->fence_lock);
			driver->sync_obj_unref(&sync_obj);
			driver->sync_obj_unref(&tmp_obj);
			spin_lock(&bdev->fence_lock);
		} else {
			spin_unlock(&bdev->fence_lock);
			driver->sync_obj_unref(&sync_obj);
			spin_lock(&bdev->fence_lock);
		}
	}
	return 0;
}
EXPORT_SYMBOL(ttm_bo_wait);

int ttm_bo_synccpu_write_grab(struct ttm_buffer_object *bo, bool no_wait)
{
	struct ttm_bo_device *bdev = bo->bdev;
	int ret = 0;

	/*
	 * Using ttm_bo_reserve makes sure the lru lists are updated.
	 */

	ret = ttm_bo_reserve(bo, true, no_wait, false, 0);
	if (unlikely(ret != 0))
		return ret;
	spin_lock(&bdev->fence_lock);
	ret = ttm_bo_wait(bo, false, true, no_wait);
	spin_unlock(&bdev->fence_lock);
	if (likely(ret == 0))
		atomic_inc(&bo->cpu_writers);
	ttm_bo_unreserve(bo);
	return ret;
}
EXPORT_SYMBOL(ttm_bo_synccpu_write_grab);

void ttm_bo_synccpu_write_release(struct ttm_buffer_object *bo)
{
	atomic_dec(&bo->cpu_writers);
}
EXPORT_SYMBOL(ttm_bo_synccpu_write_release);

/**
 * A buffer object shrink method that tries to swap out the first
 * buffer object on the bo_global::swap_lru list.
 */

static int ttm_bo_swapout(struct ttm_mem_shrink *shrink)
{
	struct ttm_bo_global *glob =
	    container_of(shrink, struct ttm_bo_global, shrink);
	struct ttm_buffer_object *bo;
	int ret = -EBUSY;
	int put_count;
	uint32_t swap_placement = (TTM_PL_FLAG_CACHED | TTM_PL_FLAG_SYSTEM);

	spin_lock(&glob->lru_lock);
	list_for_each_entry(bo, &glob->swap_lru, swap) {
		ret = ttm_bo_reserve_locked(bo, false, true, false, 0);
		if (!ret)
			break;
	}

	if (ret) {
		spin_unlock(&glob->lru_lock);
		return ret;
	}

	kref_get(&bo->list_kref);

	if (!list_empty(&bo->ddestroy)) {
		ret = ttm_bo_cleanup_refs_and_unlock(bo, false, false);
		kref_put(&bo->list_kref, ttm_bo_release_list);
		return ret;
	}

	put_count = ttm_bo_del_from_lru(bo);
	spin_unlock(&glob->lru_lock);

	ttm_bo_list_ref_sub(bo, put_count, true);

	/**
	 * Wait for GPU, then move to system cached.
	 */

	spin_lock(&bo->bdev->fence_lock);
	ret = ttm_bo_wait(bo, false, false, false);
	spin_unlock(&bo->bdev->fence_lock);

	if (unlikely(ret != 0))
		goto out;

	if ((bo->mem.placement & swap_placement) != swap_placement) {
		struct ttm_mem_reg evict_mem;

		evict_mem = bo->mem;
		evict_mem.mm_node = NULL;
		evict_mem.placement = TTM_PL_FLAG_SYSTEM | TTM_PL_FLAG_CACHED;
		evict_mem.mem_type = TTM_PL_SYSTEM;

		ret = ttm_bo_handle_move_mem(bo, &evict_mem, true,
					     false, false);
		if (unlikely(ret != 0))
			goto out;
	}

	ttm_bo_unmap_virtual(bo);

	/**
	 * Swap out. Buffer will be swapped in again as soon as
	 * anyone tries to access a ttm page.
	 */

	if (bo->bdev->driver->swap_notify)
		bo->bdev->driver->swap_notify(bo);

	ret = ttm_tt_swapout(bo->ttm, bo->persistent_swap_storage);
out:

	/**
	 *
	 * Unreserve without putting on LRU to avoid swapping out an
	 * already swapped buffer.
	 */

	atomic_set(&bo->reserved, 0);
	wake_up_all(&bo->event_queue);
	kref_put(&bo->list_kref, ttm_bo_release_list);
	return ret;
}

void ttm_bo_swapout_all(struct ttm_bo_device *bdev)
{
	while (ttm_bo_swapout(&bdev->glob->shrink) == 0)
		;
}
EXPORT_SYMBOL(ttm_bo_swapout_all);
@


1.19
log
@Fix uvm_object reference counting.  While these reference counts aren't reaaly
used (ttm bo's have their own reference counts), we can't let the reference
count go negative as this will freak out the upper uvm layers.  Since the
uvm_object reference count is still a useful debugging tool (ddb will display
it for example), adjust it such that the uvm_object reference count represents
the number of references held by the uvm layer.

tested by matthieu@@
ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.18 2015/09/27 11:09:26 jsg Exp $	*/
d731 1
a731 8
	write_lock(&bdev->vm_lock);
	if (likely(bo->vm_node != NULL)) {
		RB_REMOVE(ttm_bo_device_buffer_objects,
		    &bdev->addr_space_rb, bo);
		drm_mm_put_block(bo->vm_node);
		bo->vm_node = NULL;
	}
	write_unlock(&bdev->vm_lock);
d1248 1
d1549 1
a1549 4
	BUG_ON(!drm_mm_clean(&bdev->addr_space_mm));
	write_lock(&bdev->vm_lock);
	drm_mm_takedown(&bdev->addr_space_mm);
	write_unlock(&bdev->vm_lock);
a1562 1
	rw_init(&bdev->vm_lock, "ttmvm");
d1575 2
a1576 3
	RB_INIT(&bdev->addr_space_rb);
	drm_mm_init(&bdev->addr_space_mm, file_page_offset, 0x10000000);

a1656 8
static void ttm_bo_vm_insert_rb(struct ttm_buffer_object *bo)
{
	struct ttm_bo_device *bdev = bo->bdev;

	/* The caller acquired bdev->vm_lock. */
	RB_INSERT(ttm_bo_device_buffer_objects, &bdev->addr_space_rb, bo);
}

a1670 10
	int ret;

retry_pre_get:
	ret = drm_mm_pre_get(&bdev->addr_space_mm);
	if (unlikely(ret != 0))
		return ret;

	write_lock(&bdev->vm_lock);
	bo->vm_node = drm_mm_search_free(&bdev->addr_space_mm,
					 bo->mem.num_pages, 0, 0);
d1672 2
a1673 21
	if (unlikely(bo->vm_node == NULL)) {
		ret = -ENOMEM;
		goto out_unlock;
	}

	bo->vm_node = drm_mm_get_block_atomic(bo->vm_node,
					      bo->mem.num_pages, 0);

	if (unlikely(bo->vm_node == NULL)) {
		write_unlock(&bdev->vm_lock);
		goto retry_pre_get;
	}

	ttm_bo_vm_insert_rb(bo);
	write_unlock(&bdev->vm_lock);
	bo->addr_space_offset = ((uint64_t) bo->vm_node->start) << PAGE_SHIFT;

	return 0;
out_unlock:
	write_unlock(&bdev->vm_lock);
	return ret;
@


1.18
log
@Switch remaining users of the FreeBSD refcount apis back to the original
linux kref/kobject use.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.17 2015/09/23 23:12:12 kettenis Exp $	*/
d1227 1
a1227 1
	uvm_objinit(&bo->uobj, NULL, 1);
@


1.17
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.16 2015/07/11 04:00:46 jsg Exp $	*/
a36 1
#include <dev/pci/drm/refcount.h>
d44 1
a44 1
static void ttm_bo_global_kobj_release(struct ttm_bo_global *glob);
d56 6
d128 1
d132 1
d135 1
a136 2
#endif

d143 1
a143 1
static void ttm_bo_release_list(struct ttm_buffer_object *bo)
d145 2
d150 2
a151 2
	BUG_ON(atomic_read(&bo->list_kref));
	BUG_ON(atomic_read(&bo->kref));
d198 1
a198 1
		refcount_acquire(&bo->list_kref);
d202 1
a202 1
			refcount_acquire(&bo->list_kref);
d284 5
d292 2
a293 8
	u_int old;

	old = atomic_fetchadd_int(&bo->list_kref, -count);
	if (old <= count) {
		if (never_free)
			panic("ttm_bo_ref_buf");
		ttm_bo_release_list(bo);
	}
d554 1
a554 1
	refcount_acquire(&bo->list_kref);
d676 1
a676 1
	refcount_acquire(&entry->list_kref);
d684 1
a684 1
			refcount_acquire(&nentry->list_kref);
d694 1
a694 2
		if (refcount_release(&entry->list_kref))
			ttm_bo_release_list(entry);
d708 2
a709 2
	if (entry && refcount_release(&entry->list_kref))
		ttm_bo_release_list(entry);
d724 1
a724 1
static void ttm_bo_release(struct ttm_buffer_object *bo)
d726 2
d743 1
a743 2
	if (refcount_release(&bo->list_kref))
		ttm_bo_release_list(bo);
d751 1
a751 2
	if (refcount_release(&bo->kref))
		ttm_bo_release(bo);
d846 1
a846 1
	refcount_acquire(&bo->list_kref);
d851 1
a851 2
		if (refcount_release(&bo->list_kref))
			ttm_bo_release_list(bo);
d865 1
a865 2
	if (refcount_release(&bo->list_kref))
		ttm_bo_release_list(bo);
d1228 2
a1229 2
	refcount_init(&bo->kref, 1);
	refcount_init(&bo->list_kref, 1);
d1455 1
a1455 1
static void ttm_bo_global_kobj_release(struct ttm_bo_global *glob)
d1457 2
d1469 2
a1470 2
	if (refcount_release(&glob->kobj_ref))
		ttm_bo_global_kobj_release(glob);
d1504 5
a1508 3
	refcount_init(&glob->kobj_ref, 1);
	return (0);

d1833 1
a1833 1
	refcount_acquire(&bo->list_kref);
d1837 1
a1837 2
		if (refcount_release(&bo->list_kref))
			ttm_bo_release_list(bo);
d1892 1
a1892 2
	if (refcount_release(&bo->list_kref))
		ttm_bo_release_list(bo);
@


1.16
log
@Make use of recent drm_linux.h additions to further reduce the
diff to linux.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.15 2015/04/12 03:54:10 jsg Exp $	*/
d1578 1
a1578 3
	ret = drm_mm_init(&bdev->addr_space_mm, file_page_offset, 0x10000000);
	if (unlikely(ret != 0))
		goto out_no_addr_mm;
a1591 2
out_no_addr_mm:
	ttm_bo_clean_mm(bdev, 0);
@


1.15
log
@make wait_queue_head a struct with a mutex
better matches linux behaviour
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.14 2015/04/06 05:35:29 jsg Exp $	*/
a45 1
void	 ttm_bo_delayed_workqueue(void *);
d555 2
a556 2
	timeout_add(&bdev->to,
			      ((hz / 100) < 1) ? 1 : hz / 100);
d707 1
a707 1
static void ttm_bo_delayed_tick(void *arg)
d709 2
a710 9
	struct ttm_bo_device *bdev = arg;

	task_add(systq, &bdev->task);
}

void
ttm_bo_delayed_workqueue(void *arg1)
{
	struct ttm_bo_device *bdev = arg1;
d713 2
a714 2
		timeout_add(&bdev->to,
				      ((hz / 100) < 1) ? 1 : hz / 100);
d751 1
a751 3
	timeout_del(&bdev->to);
	task_del(systq, &bdev->task);
	return 0;
d758 2
a759 2
		timeout_add(&bdev->to,
				      ((hz / 100) < 1) ? 1 : hz / 100);
d1534 1
a1534 2
	timeout_del(&bdev->to);
	task_del(systq, &bdev->task);
d1582 1
a1582 2
	task_set(&bdev->task, ttm_bo_delayed_workqueue, bdev);
	timeout_set(&bdev->to, ttm_bo_delayed_tick, bdev);
@


1.14
log
@add and use macros for wake_up/wake_up_all/wake_up_all_locked
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.13 2015/02/11 07:01:37 jsg Exp $	*/
a1237 1
#ifdef notyet
a1238 1
#endif
@


1.13
log
@Switch most printf style functions calls back to linux function names
and move DRM_INFO/pr_info/dev_info messages under DRMDEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.12 2015/02/10 10:50:49 jsg Exp $	*/
d265 1
a265 1
			wakeup(&bo->event_queue);
d314 1
a314 1
	wakeup(&bo->event_queue);
d502 1
a502 1
	wakeup(&bo->event_queue);
d545 1
a545 1
		wakeup(&bo->event_queue);
d597 1
a597 1
		wakeup(&bo->event_queue);
d636 1
a636 1
		wakeup(&bo->event_queue);
d1902 1
a1902 1
	wakeup(&bo->event_queue);
@


1.12
log
@switch most mtx_* calls back to linux spinlocks
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.11 2015/02/10 06:19:36 jsg Exp $	*/
d74 7
a80 7
	printf("    has_type: %d\n", man->has_type);
	printf("    use_type: %d\n", man->use_type);
	printf("    flags: 0x%08X\n", man->flags);
	printf("    gpu_offset: 0x%08lX\n", man->gpu_offset);
	printf("    size: %llu\n", man->size);
	printf("    available_caching: 0x%08X\n", man->available_caching);
	printf("    default_caching: 0x%08X\n", man->default_caching);
d90 1
a90 1
	printf("No space for %p (%lu pages, %luK, %luM)\n",
d98 1
a98 1
		printf("  placement[%d]=0x%08X (%d)\n",
d366 1
a366 1
		printf("Illegal buffer object type\n");
d456 1
a456 1
				printf("Can not flush read caches\n");
d786 2
a787 2
		if (ret != -ERESTART) {
			printf("Failed to expire sync object before buffer eviction\n");
d807 2
a808 2
		if (ret != -ERESTART) {
			printf("Failed to find memory space for buffer 0x%p eviction\n",
d818 2
a819 2
		if (ret != -ERESTART)
			printf("Buffer eviction failed\n");
d1213 1
a1213 1
		printf("Out of kernel memory\n");
d1223 1
a1223 1
		printf("Illegal buffer object size\n");
d1370 1
a1370 1
				printf("Cleanup eviction failed\n");
d1385 1
a1385 1
		printf("Illegal memory type %d\n", mem_type);
d1391 1
a1391 1
		printf("Trying to take down uninitialized memory manager type %u\n",
d1415 1
a1415 1
		printf("Illegal memory manager memory type %u\n", mem_type);
d1420 1
a1420 1
		printf("Memory type %u has not been initialized\n", mem_type);
d1504 1
a1504 1
		printf("Could not register buffer object swapout\n");
d1535 1
a1535 1
				printf("DRM memory manager type %d is not clean\n",
@


1.11
log
@switch most rwlock calls back to their linux equivalents
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.10 2015/01/27 03:17:36 dlg Exp $	*/
d249 1
a249 1
		mtx_leave(&glob->lru_lock);
d251 1
a251 1
		mtx_enter(&glob->lru_lock);
d298 1
a298 1
	mtx_enter(&glob->lru_lock);
d303 1
a303 1
	mtx_leave(&glob->lru_lock);
d321 1
a321 1
	mtx_enter(&glob->lru_lock);
d323 1
a323 1
	mtx_leave(&glob->lru_lock);
d523 1
a523 1
	mtx_enter(&glob->lru_lock);
d526 1
a526 1
	mtx_enter(&bdev->fence_lock);
d529 1
a529 1
		mtx_leave(&bdev->fence_lock);
d532 1
a532 1
		mtx_leave(&glob->lru_lock);
d541 1
a541 1
	mtx_leave(&bdev->fence_lock);
d550 1
a550 1
	mtx_leave(&glob->lru_lock);
d582 1
a582 1
	mtx_enter(&bdev->fence_lock);
d594 1
a594 1
		mtx_leave(&bdev->fence_lock);
d598 1
a598 1
		mtx_leave(&glob->lru_lock);
d609 1
a609 1
		mtx_enter(&bdev->fence_lock);
d612 1
a612 1
		mtx_leave(&bdev->fence_lock);
d616 1
a616 1
		mtx_enter(&glob->lru_lock);
d628 1
a628 1
			mtx_leave(&glob->lru_lock);
d632 1
a632 1
		mtx_leave(&bdev->fence_lock);
d637 1
a637 1
		mtx_leave(&glob->lru_lock);
d645 1
a645 1
	mtx_leave(&glob->lru_lock);
d664 1
a664 1
	mtx_enter(&glob->lru_lock);
d686 1
a686 1
			mtx_leave(&glob->lru_lock);
d695 1
a695 1
		mtx_enter(&glob->lru_lock);
d701 1
a701 1
	mtx_leave(&glob->lru_lock);
d781 1
a781 1
	mtx_enter(&bdev->fence_lock);
d783 1
a783 1
	mtx_leave(&bdev->fence_lock);
d838 1
a838 1
	mtx_enter(&glob->lru_lock);
d846 1
a846 1
		mtx_leave(&glob->lru_lock);
d861 1
a861 1
	mtx_leave(&glob->lru_lock);
d1091 1
a1091 1
	mtx_enter(&bdev->fence_lock);
d1093 1
a1093 1
	mtx_leave(&bdev->fence_lock);
d1362 1
a1362 1
	mtx_enter(&glob->lru_lock);
d1364 1
a1364 1
		mtx_leave(&glob->lru_lock);
d1373 1
a1373 1
		mtx_enter(&glob->lru_lock);
d1375 1
a1375 1
	mtx_leave(&glob->lru_lock);
d1552 1
a1552 1
	mtx_enter(&glob->lru_lock);
d1558 1
a1558 1
	mtx_leave(&glob->lru_lock);
d1751 1
a1751 1
			mtx_leave(&bdev->fence_lock);
d1753 1
a1753 1
			mtx_enter(&bdev->fence_lock);
d1761 1
a1761 1
		mtx_leave(&bdev->fence_lock);
d1766 1
a1766 1
			mtx_enter(&bdev->fence_lock);
d1769 1
a1769 1
		mtx_enter(&bdev->fence_lock);
d1775 1
a1775 1
			mtx_leave(&bdev->fence_lock);
d1778 1
a1778 1
			mtx_enter(&bdev->fence_lock);
d1780 1
a1780 1
			mtx_leave(&bdev->fence_lock);
d1782 1
a1782 1
			mtx_enter(&bdev->fence_lock);
d1801 1
a1801 1
	mtx_enter(&bdev->fence_lock);
d1803 1
a1803 1
	mtx_leave(&bdev->fence_lock);
d1831 1
a1831 1
	mtx_enter(&glob->lru_lock);
d1839 1
a1839 1
		mtx_leave(&glob->lru_lock);
d1853 1
a1853 1
	mtx_leave(&glob->lru_lock);
d1861 1
a1861 1
	mtx_enter(&bo->bdev->fence_lock);
d1863 1
a1863 1
	mtx_leave(&bo->bdev->fence_lock);
@


1.10
log
@remove the second void * argument on tasks.

when workqs were introduced, we provided a second argument so you
could pass a thing and some context to work on it in. there were
very few things that took advantage of the second argument, so when
i introduced pools i suggested removing it. since tasks were meant
to replace workqs, it was requested that we keep the second argument
to make porting from workqs to tasks easier.

now that workqs are gone, i had a look at the use of the second
argument again and found only one good use of it (vdsp(4) on sparc64
if you're interested) and a tiny handful of questionable uses. the
vast majority of tasks only used a single argument. i have since
modified all tasks that used two args to only use one, so now we
can remove the second argument.

so this is a mechanical change. all tasks only passed NULL as their
second argument, so we can just remove it.

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.9 2014/11/16 12:31:00 deraadt Exp $	*/
d328 1
a328 1
 * Call bo->rwlock locked.
d338 1
a338 1
	rw_assert_wrlock(&bo->rwlock);
d731 1
a731 1
	rw_enter_write(&bdev->vm_lock);
d738 1
a738 1
	rw_exit_write(&bdev->vm_lock);
d1439 1
a1439 1
	rw_init(&man->io_reserve_rwlock, "ttm_iores");
d1487 1
a1487 1
	rw_init(&glob->device_list_rwlock, "ttm_devlist");
d1542 1
a1542 1
	rw_enter_write(&glob->device_list_rwlock);
d1544 1
a1544 1
	rw_exit_write(&glob->device_list_rwlock);
d1561 1
a1561 1
	rw_enter_write(&bdev->vm_lock);
d1563 1
a1563 1
	rw_exit_write(&bdev->vm_lock);
d1603 1
a1603 1
	rw_enter_write(&glob->device_list_rwlock);
d1605 1
a1605 1
	rw_exit_write(&glob->device_list_rwlock);
d1707 1
a1707 1
	rw_enter_write(&bdev->vm_lock);
d1720 1
a1720 1
		rw_exit_write(&bdev->vm_lock);
d1725 1
a1725 1
	rw_exit_write(&bdev->vm_lock);
d1730 1
a1730 1
	rw_exit_write(&bdev->vm_lock);
@


1.9
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.8 2014/04/12 06:08:22 jsg Exp $	*/
d46 1
a46 1
void	 ttm_bo_delayed_workqueue(void *, void *);
d716 1
a716 1
ttm_bo_delayed_workqueue(void *arg1, void *arg2)
d1595 1
a1595 1
	task_set(&bdev->task, ttm_bo_delayed_workqueue, bdev, NULL);
@


1.8
log
@drm/ttm: don't oops if no invalidate_caches()

From Rob Clark
8af4707afcb6d191b9cc75dfd73b9550e8276253 in ubuntu 3.8
9ef7506f7eff3fc42724269f62e30164c141661f in mainline linux
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.7 2014/02/10 02:35:09 jsg Exp $	*/
d1652 1
a1652 1
			pmap_page_protect(page, VM_PROT_NONE);
d1659 1
a1659 1
			pmap_page_protect(page, VM_PROT_NONE);
@


1.7
log
@drm/ttm: Fix memory type compatibility check

From Thomas Hellstrom
04623382c8e829413c2b69f740a9455e2e764ab7 in ubuntu 3.8
59c8e66378fb78adbcd05f0d09783dde6fef282b in mainline linux
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.6 2014/02/09 10:57:26 jsg Exp $	*/
d453 5
a457 3
		ret = bdev->driver->invalidate_caches(bdev, bo->mem.placement);
		if (ret)
			printf("Can not flush read caches\n");
@


1.6
log
@use linux style memory allocations in ttm
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.5 2013/12/08 07:54:06 jsg Exp $	*/
d1114 3
a1116 2
static int ttm_bo_mem_compat(struct ttm_placement *placement,
			     struct ttm_mem_reg *mem)
d1123 1
a1123 1
		return -1;
d1126 11
a1136 5
		if ((placement->placement[i] & mem->placement &
			TTM_PL_MASK_CACHING) &&
			(placement->placement[i] & mem->placement &
			TTM_PL_MASK_MEM))
			return i;
d1138 2
a1139 1
	return -1;
d1148 1
d1159 1
a1159 2
	ret = ttm_bo_mem_compat(placement, &bo->mem);
	if (ret < 0) {
d1169 1
a1169 1
		ttm_flag_masked(&bo->mem.placement, placement->placement[ret],
@


1.5
log
@add static back to the ttm functions
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.4 2013/10/30 02:11:33 dlg Exp $	*/
d157 1
a157 1
		free(bo, M_DRM);
d1207 1
a1207 1
			free(bo, M_DRM);
d1217 1
a1217 1
			free(bo, M_DRM);
d1326 1
a1326 1
	bo = malloc(sizeof(*bo), M_DRM, M_WAITOK | M_ZERO);
d1458 1
a1458 1
	drm_free(glob);
d1506 1
a1506 1
	free(glob, M_DRM);
@


1.4
log
@deprecate taskq_systq() and replace it with extern struct taskq
*const systq defined in task.h

this reduces the cost of using the system taskq and looks less ugly.

requested by and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.3 2013/10/29 06:30:57 jsg Exp $	*/
d43 4
a46 4
int ttm_bo_setup_vm(struct ttm_buffer_object *bo);
int ttm_bo_swapout(struct ttm_mem_shrink *shrink);

void ttm_bo_global_kobj_release(struct ttm_bo_global *glob);
d48 2
a49 28
int	 ttm_bo_move_buffer(struct ttm_buffer_object *,
	     struct ttm_placement *, bool, bool);
int	 ttm_bo_cleanup_refs_and_unlock(struct ttm_buffer_object *,
	     bool, bool);
int	 ttm_bo_evict(struct ttm_buffer_object *, bool, bool);
void	 ttm_bo_vm_insert_rb(struct ttm_buffer_object *);
void	 ttm_mem_type_debug(struct ttm_bo_device *, int);
void	 ttm_bo_mem_space_debug(struct ttm_buffer_object *,
	     struct ttm_placement *);
void	 ttm_bo_release_list(struct ttm_buffer_object *);
int	 ttm_bo_add_ttm(struct ttm_buffer_object *, bool);
int	 ttm_bo_handle_move_mem(struct ttm_buffer_object *,
	     struct ttm_mem_reg *, bool, bool, bool);
void	 ttm_bo_cleanup_memtype_use(struct ttm_buffer_object *);
void	 ttm_bo_cleanup_refs_or_queue(struct ttm_buffer_object *);
int	 ttm_bo_delayed_delete(struct ttm_bo_device *, bool);
void	 ttm_bo_release(struct ttm_buffer_object *);
int	 ttm_mem_evict_first(struct ttm_bo_device *, uint32_t, bool, bool);
int	 ttm_bo_mem_force_space(struct ttm_buffer_object *, uint32_t,
	     struct ttm_placement *, struct ttm_mem_reg *, bool, bool);
uint32_t ttm_bo_select_caching(struct ttm_mem_type_manager *, uint32_t,
	     uint32_t);
bool	 ttm_bo_mt_compatible(struct ttm_mem_type_manager *, uint32_t,
	     uint32_t, uint32_t *);
int	 ttm_bo_mem_compat(struct ttm_placement *, struct ttm_mem_reg *);
int	 ttm_bo_force_list_clean(struct ttm_bo_device *, unsigned, bool);
void	 ttm_bo_delayed_tick(void *);
void	 ttm_bo_delayed_workqueue(void *, void *);
d58 1
a58 2
static inline int
ttm_mem_type_from_flags(uint32_t flags, uint32_t *mem_type)
d70 1
a70 2
void
ttm_mem_type_debug(struct ttm_bo_device *bdev, int mem_type)
d85 1
a85 2
void
ttm_bo_mem_space_debug(struct ttm_buffer_object *bo,
d105 1
a105 2
ssize_t
ttm_bo_global_show(struct kobject *kobj,
d133 1
a133 2
static inline uint32_t
ttm_bo_type_flags(unsigned type)
d138 1
a138 2
void
ttm_bo_release_list(struct ttm_buffer_object *bo)
d162 1
a162 2
int
ttm_bo_wait_unreserved(struct ttm_buffer_object *bo, bool interruptible)
d178 1
a178 2
void
ttm_bo_add_to_lru(struct ttm_buffer_object *bo)
d200 1
a200 2
int
ttm_bo_del_from_lru(struct ttm_buffer_object *bo)
d221 1
a221 2
int
ttm_bo_reserve_locked(struct ttm_buffer_object *bo,
d277 1
a277 2
void
ttm_bo_list_ref_sub(struct ttm_buffer_object *bo, int count,
d290 1
a290 2
int
ttm_bo_reserve(struct ttm_buffer_object *bo,
d310 1
a310 2
void
ttm_bo_unreserve_locked(struct ttm_buffer_object *bo)
d317 1
a317 2
void
ttm_bo_unreserve(struct ttm_buffer_object *bo)
d330 1
a330 2
int
ttm_bo_add_ttm(struct ttm_buffer_object *bo, bool zero_alloc)
d374 1
a374 2
int
ttm_bo_handle_move_mem(struct ttm_buffer_object *bo,
d487 1
a487 2
void
ttm_bo_cleanup_memtype_use(struct ttm_buffer_object *bo)
d512 1
a512 2
void
ttm_bo_cleanup_refs_or_queue(struct ttm_buffer_object *bo)
d570 1
a570 2
int
ttm_bo_cleanup_refs_and_unlock(struct ttm_buffer_object *bo,
d656 1
a656 2
int
ttm_bo_delayed_delete(struct ttm_bo_device *bdev, bool remove_all)
d706 1
a706 2
void
ttm_bo_delayed_tick(void *arg)
d724 1
a724 2
void
ttm_bo_release(struct ttm_buffer_object *bo)
d745 1
a745 2
void
ttm_bo_unref(struct ttm_buffer_object **p_bo)
d755 1
a755 2
int
ttm_bo_lock_delayed_workqueue(struct ttm_bo_device *bdev)
d763 1
a763 2
void
ttm_bo_unlock_delayed_workqueue(struct ttm_bo_device *bdev, int resched)
d771 1
a771 2
int
ttm_bo_evict(struct ttm_buffer_object *bo, bool interruptible,
d826 1
a826 2
int
ttm_mem_evict_first(struct ttm_bo_device *bdev,
d873 1
a873 2
void
ttm_bo_mem_put(struct ttm_buffer_object *bo, struct ttm_mem_reg *mem)
d886 1
a886 2
int
ttm_bo_mem_force_space(struct ttm_buffer_object *bo,
d914 1
a914 2
uint32_t
ttm_bo_select_caching(struct ttm_mem_type_manager *man,
d939 1
a939 2
bool
ttm_bo_mt_compatible(struct ttm_mem_type_manager *man,
d966 1
a966 2
int
ttm_bo_mem_space(struct ttm_buffer_object *bo,
d1073 1
a1073 2
int
ttm_bo_move_buffer(struct ttm_buffer_object *bo,
d1114 1
a1114 2
int
ttm_bo_mem_compat(struct ttm_placement *placement,
d1134 1
a1134 2
int
ttm_bo_validate(struct ttm_buffer_object *bo,
d1176 1
a1176 2
int
ttm_bo_check_placement(struct ttm_buffer_object *bo,
d1185 1
a1185 2
int
ttm_bo_init(struct ttm_bo_device *bdev,
d1284 1
a1284 2
size_t
ttm_bo_acc_size(struct ttm_bo_device *bdev,
d1298 1
a1298 2
size_t
ttm_bo_dma_acc_size(struct ttm_bo_device *bdev,
d1313 1
a1313 2
int
ttm_bo_create(struct ttm_bo_device *bdev,
d1341 1
a1341 2
int
ttm_bo_force_list_clean(struct ttm_bo_device *bdev,
d1369 1
a1369 2
int
ttm_bo_clean_mm(struct ttm_bo_device *bdev, unsigned mem_type)
d1400 1
a1400 2
int
ttm_bo_evict_mm(struct ttm_bo_device *bdev, unsigned mem_type)
d1418 1
a1418 2
int
ttm_bo_init_mm(struct ttm_bo_device *bdev, unsigned type,
d1453 1
a1453 2
void
ttm_bo_global_kobj_release(struct ttm_bo_global *glob)
d1461 1
a1461 2
void
ttm_bo_global_release(struct drm_global_reference *ref)
d1470 1
a1470 2
int
ttm_bo_global_init(struct drm_global_reference *ref)
d1512 1
a1512 2
int
ttm_bo_device_release(struct ttm_bo_device *bdev)
d1559 1
a1559 2
int
ttm_bo_device_init(struct ttm_bo_device *bdev,
d1609 1
a1609 2
bool
ttm_mem_reg_is_pci(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)
d1626 1
a1626 2
void
ttm_bo_unmap_virtual_locked(struct ttm_buffer_object *bo)
d1655 1
a1655 2
void
ttm_bo_unmap_virtual(struct ttm_buffer_object *bo)
d1668 1
a1668 2
void
ttm_bo_vm_insert_rb(struct ttm_buffer_object *bo)
d1687 1
a1687 2
int
ttm_bo_setup_vm(struct ttm_buffer_object *bo)
d1724 1
a1724 2
int
ttm_bo_wait(struct ttm_buffer_object *bo,
d1779 1
a1779 2
int
ttm_bo_synccpu_write_grab(struct ttm_buffer_object *bo, bool no_wait)
d1801 1
a1801 2
void
ttm_bo_synccpu_write_release(struct ttm_buffer_object *bo)
d1812 1
a1812 2
int
ttm_bo_swapout(struct ttm_mem_shrink *shrink)
d1898 1
a1898 2
void
ttm_bo_swapout_all(struct ttm_bo_device *bdev)
@


1.3
log
@Move most of the uses of workqs in drm to the new task/taskq api.
Prevents unintended multiple additions to workqs that was causing
hangs on radeon, and lets us remove tasks more closely matching
the behaviour of the original linux code.

ok kettenis@@
cause of the ttm/radeon hangs debugged by claudio@@ and kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.2 2013/09/02 07:14:22 jsg Exp $	*/
d757 1
a757 1
	task_add(taskq_systq(), &bdev->task);
d808 1
a808 1
	task_del(taskq_systq(), &bdev->task);
d1611 1
a1611 1
	task_del(taskq_systq(), &bdev->task);
@


1.2
log
@use DRM_MEMORYBARRIER() for smp_mb__*
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo.c,v 1.1 2013/08/12 04:11:53 jsg Exp $	*/
d757 1
a757 2
	workq_queue_task(NULL, &bdev->task, 0,
	    ttm_bo_delayed_workqueue, bdev, NULL);
d808 1
d1611 1
d1660 1
@


1.1
log
@Add a port of the TTM and Radeon DRM code from Linux 3.8.13.
Includes kernel modesetting, framebuffer console and support
for newer hardware.

Firmware needs to be present for acceleration and in some cases
modesetting to work.  It can be installed via fw_update
or manually via pkg_add.

With lots of help from kettenis@@ some macppc bits from mpi@@
and some ttm refcount/queue bits from FreeBSD.

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d552 1
a552 1
	DRM_MEMORYBARRIER();
@

