head	1.15;
access;
symbols
	OPENBSD_6_1:1.15.0.4
	OPENBSD_6_1_BASE:1.15
	OPENBSD_6_0:1.15.0.6
	OPENBSD_6_0_BASE:1.15
	OPENBSD_5_9:1.15.0.2
	OPENBSD_5_9_BASE:1.15
	OPENBSD_5_8:1.14.0.4
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.9.0.4
	OPENBSD_5_7_BASE:1.9
	OPENBSD_5_6:1.5.0.6
	OPENBSD_5_6_BASE:1.5
	OPENBSD_5_5:1.5.0.4
	OPENBSD_5_5_BASE:1.5;
locks; strict;
comment	@ * @;


1.15
date	2015.09.27.11.09.26;	author jsg;	state Exp;
branches;
next	1.14;
commitid	OkoKp05dU7tP7DK3;

1.14
date	2015.04.12.05.31.23;	author jsg;	state Exp;
branches;
next	1.13;
commitid	09jvnnyFAQrVbK5f;

1.13
date	2015.04.12.03.54.10;	author jsg;	state Exp;
branches;
next	1.12;
commitid	uVTyY1h8Sggc8pFj;

1.12
date	2015.04.08.04.03.06;	author jsg;	state Exp;
branches;
next	1.11;
commitid	yFnN7rFafIBD7cOc;

1.11
date	2015.04.08.02.28.13;	author jsg;	state Exp;
branches;
next	1.10;
commitid	pBZw25gbMMahiUV2;

1.10
date	2015.04.06.12.25.10;	author jsg;	state Exp;
branches;
next	1.9;
commitid	CN1fAwudhSb2ckyl;

1.9
date	2015.02.12.08.48.32;	author jsg;	state Exp;
branches;
next	1.8;
commitid	b7XA83Agvw5SEHRi;

1.8
date	2015.02.10.10.50.49;	author jsg;	state Exp;
branches;
next	1.7;
commitid	aHLMSW1RfE1rmMw9;

1.7
date	2015.02.10.06.19.36;	author jsg;	state Exp;
branches;
next	1.6;
commitid	0OQNG9STPII6jEb4;

1.6
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.5;
commitid	yv0ECmCdICvq576h;

1.5
date	2014.02.10.02.24.05;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2014.02.10.02.15.25;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2014.02.09.10.57.26;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2013.12.08.07.54.06;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2013.08.12.04.11.53;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.15
log
@Switch remaining users of the FreeBSD refcount apis back to the original
linux kref/kobject use.

ok kettenis@@
@
text
@/*	$OpenBSD: ttm_bo_util.c,v 1.14 2015/04/12 05:31:23 jsg Exp $	*/
/**************************************************************************
 *
 * Copyright (c) 2007-2009 VMware, Inc., Palo Alto, CA., USA
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/
/*
 * Authors: Thomas Hellstrom <thellstrom-at-vmware-dot-com>
 */

#include <dev/pci/drm/ttm/ttm_bo_driver.h>
#include <dev/pci/drm/ttm/ttm_placement.h>

int	 ttm_mem_reg_ioremap(struct ttm_bo_device *, struct ttm_mem_reg *,
	     void **);
void	 ttm_mem_reg_iounmap(struct ttm_bo_device *, struct ttm_mem_reg *,
	     void *);

void ttm_bo_free_old_node(struct ttm_buffer_object *bo)
{
	ttm_bo_mem_put(bo, &bo->mem);
}

int ttm_bo_move_ttm(struct ttm_buffer_object *bo,
		    bool evict,
		    bool no_wait_gpu, struct ttm_mem_reg *new_mem)
{
	struct ttm_tt *ttm = bo->ttm;
	struct ttm_mem_reg *old_mem = &bo->mem;
	int ret;

	if (old_mem->mem_type != TTM_PL_SYSTEM) {
		ttm_tt_unbind(ttm);
		ttm_bo_free_old_node(bo);
		ttm_flag_masked(&old_mem->placement, TTM_PL_FLAG_SYSTEM,
				TTM_PL_MASK_MEM);
		old_mem->mem_type = TTM_PL_SYSTEM;
	}

	ret = ttm_tt_set_placement_caching(ttm, new_mem->placement);
	if (unlikely(ret != 0))
		return ret;

	if (new_mem->mem_type != TTM_PL_SYSTEM) {
		ret = ttm_tt_bind(ttm, new_mem);
		if (unlikely(ret != 0))
			return ret;
	}

	*old_mem = *new_mem;
	new_mem->mm_node = NULL;

	return 0;
}
EXPORT_SYMBOL(ttm_bo_move_ttm);

int ttm_mem_io_lock(struct ttm_mem_type_manager *man, bool interruptible)
{
	if (likely(man->io_reserve_fastpath))
		return 0;

	if (interruptible)
		return mutex_lock_interruptible(&man->io_reserve_mutex);

	mutex_lock(&man->io_reserve_mutex);
	return 0;
}

void ttm_mem_io_unlock(struct ttm_mem_type_manager *man)
{
	if (likely(man->io_reserve_fastpath))
		return;

	mutex_unlock(&man->io_reserve_mutex);
}

static int ttm_mem_io_evict(struct ttm_mem_type_manager *man)
{
	struct ttm_buffer_object *bo;

	if (!man->use_io_reserve_lru || list_empty(&man->io_reserve_lru))
		return -EAGAIN;

	bo = list_first_entry(&man->io_reserve_lru,
			      struct ttm_buffer_object,
			      io_reserve_lru);
	list_del_init(&bo->io_reserve_lru);
	ttm_bo_unmap_virtual_locked(bo);

	return 0;
}

static int ttm_mem_io_reserve(struct ttm_bo_device *bdev,
			      struct ttm_mem_reg *mem)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];
	int ret = 0;

	if (!bdev->driver->io_mem_reserve)
		return 0;
	if (likely(man->io_reserve_fastpath))
		return bdev->driver->io_mem_reserve(bdev, mem);

	if (bdev->driver->io_mem_reserve &&
	    mem->bus.io_reserved_count++ == 0) {
retry:
		ret = bdev->driver->io_mem_reserve(bdev, mem);
		if (ret == -EAGAIN) {
			ret = ttm_mem_io_evict(man);
			if (ret == 0)
				goto retry;
		}
	}
	return ret;
}

static void ttm_mem_io_free(struct ttm_bo_device *bdev,
			    struct ttm_mem_reg *mem)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];

	if (likely(man->io_reserve_fastpath))
		return;

	if (bdev->driver->io_mem_reserve &&
	    --mem->bus.io_reserved_count == 0 &&
	    bdev->driver->io_mem_free)
		bdev->driver->io_mem_free(bdev, mem);

}

int ttm_mem_io_reserve_vm(struct ttm_buffer_object *bo)
{
	struct ttm_mem_reg *mem = &bo->mem;
	int ret;

	if (!mem->bus.io_reserved_vm) {
		struct ttm_mem_type_manager *man =
			&bo->bdev->man[mem->mem_type];

		ret = ttm_mem_io_reserve(bo->bdev, mem);
		if (unlikely(ret != 0))
			return ret;
		mem->bus.io_reserved_vm = true;
		if (man->use_io_reserve_lru)
			list_add_tail(&bo->io_reserve_lru,
				      &man->io_reserve_lru);
	}
	return 0;
}

void ttm_mem_io_free_vm(struct ttm_buffer_object *bo)
{
	struct ttm_mem_reg *mem = &bo->mem;

	if (mem->bus.io_reserved_vm) {
		mem->bus.io_reserved_vm = false;
		list_del_init(&bo->io_reserve_lru);
		ttm_mem_io_free(bo->bdev, mem);
	}
}

int ttm_mem_reg_ioremap(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem,
			void **virtual)
{
	struct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];
	int ret;
	void *addr;
	int flags;

	*virtual = NULL;
	(void) ttm_mem_io_lock(man, false);
	ret = ttm_mem_io_reserve(bdev, mem);
	ttm_mem_io_unlock(man);
	if (ret || !mem->bus.is_iomem)
		return ret;

	if (mem->bus.addr) {
		addr = mem->bus.addr;
	} else {
		if (mem->placement & TTM_PL_FLAG_WC)
			flags = BUS_SPACE_MAP_PREFETCHABLE;
		else
			flags = 0;

		if (bus_space_map(bdev->memt, mem->bus.base + mem->bus.offset,
		    mem->bus.size, BUS_SPACE_MAP_LINEAR | flags, &mem->bus.bsh)) {
			printf("%s bus_space_map failed\n", __func__);
			return -ENOMEM;
		}

		addr = bus_space_vaddr(bdev->memt, mem->bus.bsh);

		if (!addr) {
			(void) ttm_mem_io_lock(man, false);
			ttm_mem_io_free(bdev, mem);
			ttm_mem_io_unlock(man);
			return -ENOMEM;
		}
	}
	*virtual = addr;
	return 0;
}

void ttm_mem_reg_iounmap(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem,
			 void *virtual)
{
	struct ttm_mem_type_manager *man;

	man = &bdev->man[mem->mem_type];

	if (virtual && mem->bus.addr == NULL)
		bus_space_unmap(bdev->memt, mem->bus.bsh, mem->bus.size);
	(void) ttm_mem_io_lock(man, false);
	ttm_mem_io_free(bdev, mem);
	ttm_mem_io_unlock(man);
}

static int ttm_copy_io_page(void *dst, void *src, unsigned long page)
{
	uint32_t *dstP =
	    (uint32_t *) ((unsigned long)dst + (page << PAGE_SHIFT));
	uint32_t *srcP =
	    (uint32_t *) ((unsigned long)src + (page << PAGE_SHIFT));

	int i;
	for (i = 0; i < PAGE_SIZE / sizeof(uint32_t); ++i)
		iowrite32(ioread32(srcP++), dstP++);
	return 0;
}

static int ttm_copy_io_ttm_page(struct ttm_tt *ttm, void *src,
				unsigned long page,
				pgprot_t prot)
{
	struct vm_page *d = ttm->pages[page];
	void *dst;

	if (!d)
		return -ENOMEM;

	src = (void *)((unsigned long)src + (page << PAGE_SHIFT));

	if (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))
		dst = vmap(&d, 1, 0, prot);
	else
		dst = kmap(d);
	if (!dst)
		return -ENOMEM;

	memcpy_fromio(dst, src, PAGE_SIZE);

	if (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))
		vunmap(dst, PAGE_SIZE);
	else
		kunmap(d);

	return 0;
}

static int ttm_copy_ttm_io_page(struct ttm_tt *ttm, void *dst,
				unsigned long page,
				vm_prot_t prot)
{
	struct vm_page *s = ttm->pages[page];
	void *src;

	if (!s)
		return -ENOMEM;

	dst = (void *)((unsigned long)dst + (page << PAGE_SHIFT));
	if (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))
		src = vmap(&s, 1, 0, prot);
	else
		src = kmap(s);
	if (!src)
		return -ENOMEM;

	memcpy_toio(dst, src, PAGE_SIZE);

	if (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))
		vunmap(src, PAGE_SIZE);
	else
		kunmap(s);

	return 0;
}

int ttm_bo_move_memcpy(struct ttm_buffer_object *bo,
		       bool evict, bool no_wait_gpu,
		       struct ttm_mem_reg *new_mem)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_mem_type_manager *man = &bdev->man[new_mem->mem_type];
	struct ttm_tt *ttm = bo->ttm;
	struct ttm_mem_reg *old_mem = &bo->mem;
	struct ttm_mem_reg old_copy = *old_mem;
	void *old_iomap;
	void *new_iomap;
	int ret;
	unsigned long i;
	unsigned long page;
	unsigned long add = 0;
	int dir;

	ret = ttm_mem_reg_ioremap(bdev, old_mem, &old_iomap);
	if (ret)
		return ret;
	ret = ttm_mem_reg_ioremap(bdev, new_mem, &new_iomap);
	if (ret)
		goto out;

	/*
	 * Single TTM move. NOP.
	 */
	if (old_iomap == NULL && new_iomap == NULL)
		goto out2;

	/*
	 * Move nonexistent data. NOP.
	 */
	if (old_iomap == NULL && ttm == NULL)
		goto out2;

	/*
	 * TTM might be null for moves within the same region.
	 */
	if (ttm && ttm->state == tt_unpopulated) {
		ret = ttm->bdev->driver->ttm_tt_populate(ttm);
		if (ret)
			goto out1;
	}

	add = 0;
	dir = 1;

	if ((old_mem->mem_type == new_mem->mem_type) &&
	    (new_mem->start < old_mem->start + old_mem->size)) {
		dir = -1;
		add = new_mem->num_pages - 1;
	}

	for (i = 0; i < new_mem->num_pages; ++i) {
		page = i * dir + add;
		if (old_iomap == NULL) {
			pgprot_t prot = ttm_io_prot(old_mem->placement,
						    PAGE_KERNEL);
			ret = ttm_copy_ttm_io_page(ttm, new_iomap, page,
						   prot);
		} else if (new_iomap == NULL) {
			pgprot_t prot = ttm_io_prot(new_mem->placement,
						    PAGE_KERNEL);
			ret = ttm_copy_io_ttm_page(ttm, old_iomap, page,
						   prot);
		} else
			ret = ttm_copy_io_page(new_iomap, old_iomap, page);
		if (ret)
			goto out1;
	}
	mb();
out2:
	old_copy = *old_mem;
	*old_mem = *new_mem;
	new_mem->mm_node = NULL;

	if ((man->flags & TTM_MEMTYPE_FLAG_FIXED) && (ttm != NULL)) {
		ttm_tt_unbind(ttm);
		ttm_tt_destroy(ttm);
		bo->ttm = NULL;
	}

out1:
	ttm_mem_reg_iounmap(bdev, old_mem, new_iomap);
out:
	ttm_mem_reg_iounmap(bdev, &old_copy, old_iomap);

	/*
	 * On error, keep the mm node!
	 */
	if (!ret)
		ttm_bo_mem_put(bo, &old_copy);
	return ret;
}
EXPORT_SYMBOL(ttm_bo_move_memcpy);

static void ttm_transfered_destroy(struct ttm_buffer_object *bo)
{
	kfree(bo);
}

/**
 * ttm_buffer_object_transfer
 *
 * @@bo: A pointer to a struct ttm_buffer_object.
 * @@new_obj: A pointer to a pointer to a newly created ttm_buffer_object,
 * holding the data of @@bo with the old placement.
 *
 * This is a utility function that may be called after an accelerated move
 * has been scheduled. A new buffer object is created as a placeholder for
 * the old data while it's being copied. When that buffer object is idle,
 * it can be destroyed, releasing the space of the old placement.
 * Returns:
 * !0: Failure.
 */

static int ttm_buffer_object_transfer(struct ttm_buffer_object *bo,
				      struct ttm_buffer_object **new_obj)
{
	struct ttm_buffer_object *fbo;
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_bo_driver *driver = bdev->driver;

	fbo = kmalloc(sizeof(*fbo), GFP_KERNEL);
	if (!fbo)
		return -ENOMEM;

	*fbo = *bo;

	/**
	 * Fix up members that we shouldn't copy directly:
	 * TODO: Explicit member copy would probably be better here.
	 */

	init_waitqueue_head(&fbo->event_queue);
	INIT_LIST_HEAD(&fbo->ddestroy);
	INIT_LIST_HEAD(&fbo->lru);
	INIT_LIST_HEAD(&fbo->swap);
	INIT_LIST_HEAD(&fbo->io_reserve_lru);
	fbo->vm_node = NULL;
	atomic_set(&fbo->cpu_writers, 0);

	spin_lock(&bdev->fence_lock);
	if (bo->sync_obj)
		fbo->sync_obj = driver->sync_obj_ref(bo->sync_obj);
	else
		fbo->sync_obj = NULL;
	spin_unlock(&bdev->fence_lock);
	kref_init(&fbo->list_kref);
	kref_init(&fbo->kref);
	fbo->destroy = &ttm_transfered_destroy;
	fbo->acc_size = 0;

	*new_obj = fbo;
	return 0;
}

pgprot_t ttm_io_prot(uint32_t caching_flags, pgprot_t tmp)
{
#ifdef PMAP_WC
	if (caching_flags & TTM_PL_FLAG_WC)
		return PMAP_WC;
	else
#endif
		return PMAP_NOCACHE;
}
EXPORT_SYMBOL(ttm_io_prot);

static int ttm_bo_ioremap(struct ttm_buffer_object *bo,
			  unsigned long offset,
			  unsigned long size,
			  struct ttm_bo_kmap_obj *map)
{
	struct ttm_mem_reg *mem = &bo->mem;
	int flags;

	if (bo->mem.bus.addr) {
		map->bo_kmap_type = ttm_bo_map_premapped;
		map->virtual = (void *)(((u8 *)bo->mem.bus.addr) + offset);
	} else {
		map->bo_kmap_type = ttm_bo_map_iomap;
		if (mem->placement & TTM_PL_FLAG_WC)
			flags = BUS_SPACE_MAP_PREFETCHABLE;
		else
			flags = 0;

		if (bus_space_map(bo->bdev->memt,
		    mem->bus.base + bo->mem.bus.offset + offset,
		    size, BUS_SPACE_MAP_LINEAR | flags,
		    &bo->mem.bus.bsh)) {
			printf("%s bus_space_map failed\n", __func__);
			map->virtual = 0;
		} else
			map->virtual = bus_space_vaddr(bo->bdev->memt,
			    bo->mem.bus.bsh);
	}
	return (!map->virtual) ? -ENOMEM : 0;
}

static int ttm_bo_kmap_ttm(struct ttm_buffer_object *bo,
			   unsigned long start_page,
			   unsigned long num_pages,
			   struct ttm_bo_kmap_obj *map)
{
	struct ttm_mem_reg *mem = &bo->mem; pgprot_t prot;
	struct ttm_tt *ttm = bo->ttm;
	int ret;

	BUG_ON(!ttm);

	if (ttm->state == tt_unpopulated) {
		ret = ttm->bdev->driver->ttm_tt_populate(ttm);
		if (ret)
			return ret;
	}

	if (num_pages == 1 && (mem->placement & TTM_PL_FLAG_CACHED)) {
		/*
		 * We're mapping a single page, and the desired
		 * page protection is consistent with the bo.
		 */

		map->bo_kmap_type = ttm_bo_map_kmap;
		map->page = ttm->pages[start_page];
		map->virtual = kmap(map->page);
	} else {
		/*
		 * We need to use vmap to get the desired page protection
		 * or to make the buffer object look contiguous.
		 */
		prot = (mem->placement & TTM_PL_FLAG_CACHED) ?
			PAGE_KERNEL :
			ttm_io_prot(mem->placement, PAGE_KERNEL);
		map->bo_kmap_type = ttm_bo_map_vmap;
		map->virtual = vmap(ttm->pages + start_page, num_pages,
				    0, prot);
	}
	return (!map->virtual) ? -ENOMEM : 0;
}

int ttm_bo_kmap(struct ttm_buffer_object *bo,
		unsigned long start_page, unsigned long num_pages,
		struct ttm_bo_kmap_obj *map)
{
	struct ttm_mem_type_manager *man =
		&bo->bdev->man[bo->mem.mem_type];
	unsigned long offset, size;
	int ret;

	BUG_ON(!list_empty(&bo->swap));
	map->virtual = NULL;
	map->bo = bo;
	if (num_pages > bo->num_pages)
		return -EINVAL;
	if (start_page > bo->num_pages)
		return -EINVAL;
#if 0
	if (num_pages > 1 && !DRM_SUSER(DRM_CURPROC))
		return -EPERM;
#endif
	(void) ttm_mem_io_lock(man, false);
	ret = ttm_mem_io_reserve(bo->bdev, &bo->mem);
	ttm_mem_io_unlock(man);
	if (ret)
		return ret;
	if (!bo->mem.bus.is_iomem) {
		return ttm_bo_kmap_ttm(bo, start_page, num_pages, map);
	} else {
		offset = start_page << PAGE_SHIFT;
		size = num_pages << PAGE_SHIFT;
		return ttm_bo_ioremap(bo, offset, size, map);
	}
}
EXPORT_SYMBOL(ttm_bo_kmap);

void ttm_bo_kunmap(struct ttm_bo_kmap_obj *map)
{
	struct ttm_buffer_object *bo = map->bo;
	struct ttm_mem_type_manager *man =
		&bo->bdev->man[bo->mem.mem_type];

	if (!map->virtual)
		return;
	switch (map->bo_kmap_type) {
	case ttm_bo_map_iomap:
		bus_space_unmap(bo->bdev->memt, bo->mem.bus.bsh,
		    bo->mem.bus.size);
		break;
	case ttm_bo_map_vmap:
		vunmap(map->virtual, bo->mem.bus.size);
		break;
	case ttm_bo_map_kmap:
		kunmap(map->virtual);
		break;
	case ttm_bo_map_premapped:
		break;
	default:
		BUG();
	}
	(void) ttm_mem_io_lock(man, false);
	ttm_mem_io_free(map->bo->bdev, &map->bo->mem);
	ttm_mem_io_unlock(man);
	map->virtual = NULL;
	map->page = NULL;
}
EXPORT_SYMBOL(ttm_bo_kunmap);

int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,
			      void *sync_obj,
			      bool evict,
			      bool no_wait_gpu,
			      struct ttm_mem_reg *new_mem)
{
	struct ttm_bo_device *bdev = bo->bdev;
	struct ttm_bo_driver *driver = bdev->driver;
	struct ttm_mem_type_manager *man = &bdev->man[new_mem->mem_type];
	struct ttm_mem_reg *old_mem = &bo->mem;
	int ret;
	struct ttm_buffer_object *ghost_obj;
	void *tmp_obj = NULL;

	spin_lock(&bdev->fence_lock);
	if (bo->sync_obj) {
		tmp_obj = bo->sync_obj;
		bo->sync_obj = NULL;
	}
	bo->sync_obj = driver->sync_obj_ref(sync_obj);
	if (evict) {
		ret = ttm_bo_wait(bo, false, false, false);
		spin_unlock(&bdev->fence_lock);
		if (tmp_obj)
			driver->sync_obj_unref(&tmp_obj);
		if (ret)
			return ret;

		if ((man->flags & TTM_MEMTYPE_FLAG_FIXED) &&
		    (bo->ttm != NULL)) {
			ttm_tt_unbind(bo->ttm);
			ttm_tt_destroy(bo->ttm);
			bo->ttm = NULL;
		}
		ttm_bo_free_old_node(bo);
	} else {
		/**
		 * This should help pipeline ordinary buffer moves.
		 *
		 * Hang old buffer memory on a new buffer object,
		 * and leave it to be released when the GPU
		 * operation has completed.
		 */

		set_bit(TTM_BO_PRIV_FLAG_MOVING, &bo->priv_flags);
		spin_unlock(&bdev->fence_lock);
		if (tmp_obj)
			driver->sync_obj_unref(&tmp_obj);

		ret = ttm_buffer_object_transfer(bo, &ghost_obj);
		if (ret)
			return ret;

		/**
		 * If we're not moving to fixed memory, the TTM object
		 * needs to stay alive. Otherwhise hang it on the ghost
		 * bo to be unbound and destroyed.
		 */

		if (!(man->flags & TTM_MEMTYPE_FLAG_FIXED))
			ghost_obj->ttm = NULL;
		else
			bo->ttm = NULL;

		ttm_bo_unreserve(ghost_obj);
		ttm_bo_unref(&ghost_obj);
	}

	*old_mem = *new_mem;
	new_mem->mm_node = NULL;

	return 0;
}
EXPORT_SYMBOL(ttm_bo_move_accel_cleanup);
@


1.14
log
@Switch back to ioread32 and iowrite32 for cases where bus_space_vaddr is
used instead of bus_space_read/bus_space_write.
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.13 2015/04/12 03:54:10 jsg Exp $	*/
a33 1
#include <dev/pci/drm/refcount.h>
d459 2
a460 2
	refcount_init(&fbo->list_kref, 1);
	refcount_init(&fbo->kref, 1);
@


1.13
log
@make wait_queue_head a struct with a mutex
better matches linux behaviour
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.12 2015/04/08 04:03:06 jsg Exp $	*/
a249 1
#ifdef notyet
a250 3
#else
		*dstP++ = *srcP++;
#endif
@


1.12
log
@change back to memcpy_toio/memcpy_fromio/memset_io
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.11 2015/04/08 02:28:13 jsg Exp $	*/
a449 1
#ifdef notyet
a450 1
#endif
@


1.11
log
@ttm has it's own version of kmap/kunmap that uses
kernel_map/uvm_km_valloc and i915 has a version that uses
phys_map/uvm_km_valloc_wait as calling code assumes kmap would
sleep if no memory is available.

Move these and ttm's vmap/vunmap into the linux compat files
and make them all use phys_map/uvm_km_valloc_wait.

looks good kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.10 2015/04/06 12:25:10 jsg Exp $	*/
d277 1
a277 1
	memcpy(dst, src, PAGE_SIZE);
@


1.10
log
@move some inline linux compat into the dedicated files
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.9 2015/02/12 08:48:32 jsg Exp $	*/
a40 5
void	*kmap(struct vm_page *);
void	 kunmap(void *addr);
void	*vmap(struct vm_page **, unsigned int, unsigned long, pgprot_t);
void	 vunmap(void *, size_t);

a514 63
}

void *
kmap(struct vm_page *pg)
{
	vaddr_t va;

#if defined (__HAVE_PMAP_DIRECT)
	va = pmap_map_direct(pg);
#else
	va = uvm_km_valloc(kernel_map, PAGE_SIZE);
	if (va == 0)
		return (NULL);
	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());
#endif
	return (void *)va;
}

void
kunmap(void *addr)
{
	vaddr_t va = (vaddr_t)addr;

#if defined (__HAVE_PMAP_DIRECT)
	pmap_unmap_direct(va);
#else
	pmap_kremove(va, PAGE_SIZE);
	pmap_update(pmap_kernel());
	uvm_km_free(kernel_map, va, PAGE_SIZE);
#endif
}

void *
vmap(struct vm_page **pages, unsigned int npages, unsigned long flags,
     pgprot_t prot)
{
	vaddr_t va;
	paddr_t pa;
	int i;

	va = uvm_km_valloc(kernel_map, PAGE_SIZE * npages);
	if (va == 0)
		return NULL;
	for (i = 0; i < npages; i++) {
		pa = VM_PAGE_TO_PHYS(pages[i]) | prot;
		pmap_enter(pmap_kernel(), va + (i * PAGE_SIZE), pa,
		    PROT_READ | PROT_WRITE,
		    PROT_READ | PROT_WRITE | PMAP_WIRED);
		pmap_update(pmap_kernel());
	}

	return (void *)va;
}

void
vunmap(void *addr, size_t size)
{
	vaddr_t va = (vaddr_t)addr;

	pmap_remove(pmap_kernel(), va, va + size);
	pmap_update(pmap_kernel());
	uvm_km_free(kernel_map, va, size);
@


1.9
log
@Add and use macros for linux memory barriers.  Fix the call in
i915_gem_object_flush_fence() to be mb() not wmb() while here.
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.8 2015/02/10 10:50:49 jsg Exp $	*/
a309 1
#define memcpy_toio(d, s, n) memcpy(d, s, n)
@


1.8
log
@switch most mtx_* calls back to linux spinlocks
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.7 2015/02/10 06:19:36 jsg Exp $	*/
d392 1
a392 1
	DRM_MEMORYBARRIER();
@


1.7
log
@switch most rwlock calls back to their linux equivalents
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.6 2014/11/16 12:31:00 deraadt Exp $	*/
d466 1
a466 1
	mtx_enter(&bdev->fence_lock);
d471 1
a471 1
	mtx_leave(&bdev->fence_lock);
d708 1
a708 1
	mtx_enter(&bdev->fence_lock);
d716 1
a716 1
		mtx_leave(&bdev->fence_lock);
d739 1
a739 1
		mtx_leave(&bdev->fence_lock);
@


1.6
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.5 2014/02/10 02:24:05 jsg Exp $	*/
d90 1
a90 1
		return rw_enter(&man->io_reserve_rwlock, RW_WRITE | RW_INTR);
d92 1
a92 1
	rw_enter_write(&man->io_reserve_rwlock);
d101 1
a101 1
	rw_exit_write(&man->io_reserve_rwlock);
@


1.5
log
@drm/ttm: Fix ttm_bo_move_memcpy

From Thomas Hellstrom
1fe70122dd9b926e84b14d1603202ae2ef2c5edd in ubuntu 3.8
da95c788ef0c645378ffccb7060a0df1a33aee38 in mainline linux
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.4 2014/02/10 02:15:25 jsg Exp $	*/
d534 1
a534 1
	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW);
d568 2
a569 2
		    VM_PROT_READ | VM_PROT_WRITE,
		    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
@


1.4
log
@drm/ttm: Handle in-memory region copies

From Jakob Bornecrantz
f8f0599d88fb76646bdd8b735dc2574ad80c625d in ubuntu 3.8
9a0599ddeae012a771bba5e23393fc52d8a59d89 in mainline linux
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.3 2014/02/09 10:57:26 jsg Exp $	*/
d345 3
d350 4
d357 2
a358 1
	/* TTM might be null for moves within the same region.
d362 1
a362 4
		if (ret) {
			/* if we fail here don't nuke the mm node
			 * as the bo still owns it */
			old_copy.mm_node = NULL;
a363 1
		}
d389 1
a389 3
		if (ret) {
			/* failing here, means keep old copy as-is */
			old_copy.mm_node = NULL;
a390 1
		}
d408 6
a413 1
	ttm_bo_mem_put(bo, &old_copy);
@


1.3
log
@use linux style memory allocations in ttm
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.2 2013/12/08 07:54:06 jsg Exp $	*/
d350 3
a352 1
	if (ttm->state == tt_unpopulated) {
@


1.2
log
@add static back to the ttm functions
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ttm_bo_util.c,v 1.1 2013/08/12 04:11:53 jsg Exp $	*/
d412 1
a412 1
	free(bo, M_DRM);
d437 1
a437 1
	fbo = malloc(sizeof(*fbo), M_DRM, M_WAITOK);
@


1.1
log
@Add a port of the TTM and Radeon DRM code from Linux 3.8.13.
Includes kernel modesetting, framebuffer console and support
for newer hardware.

Firmware needs to be present for acceleration and in some cases
modesetting to work.  It can be installed via fw_update
or manually via pkg_add.

With lots of help from kettenis@@ some macppc bits from mpi@@
and some ttm refcount/queue bits from FreeBSD.

Thanks to M:Tier and the OpenBSD Foundation for sponsoring this work.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a39 2
int	 ttm_copy_io_page(void *, void *, unsigned long);
void	 ttm_transfered_destroy(struct ttm_buffer_object *);
d246 1
a246 2
int
ttm_copy_io_page(void *dst, void *src, unsigned long page)
d410 1
a410 2
void
ttm_transfered_destroy(struct ttm_buffer_object *bo)
@

