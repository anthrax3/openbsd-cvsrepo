head	1.119;
access;
symbols
	OPENBSD_6_1_BASE:1.119
	OPENBSD_6_0:1.117.0.4
	OPENBSD_6_0_BASE:1.117
	OPENBSD_5_9:1.116.0.2
	OPENBSD_5_9_BASE:1.116
	OPENBSD_5_8:1.111.0.4
	OPENBSD_5_8_BASE:1.111
	OPENBSD_5_7:1.108.0.4
	OPENBSD_5_7_BASE:1.108
	OPENBSD_5_6:1.105.0.4
	OPENBSD_5_6_BASE:1.105
	OPENBSD_5_5:1.104.0.4
	OPENBSD_5_5_BASE:1.104
	OPENBSD_5_4:1.101.0.2
	OPENBSD_5_4_BASE:1.101
	OPENBSD_5_3:1.100.0.2
	OPENBSD_5_3_BASE:1.100
	OPENBSD_5_2:1.98.0.6
	OPENBSD_5_2_BASE:1.98
	OPENBSD_5_1_BASE:1.98
	OPENBSD_5_1:1.98.0.4
	OPENBSD_5_0:1.98.0.2
	OPENBSD_5_0_BASE:1.98
	OPENBSD_4_9:1.97.0.2
	OPENBSD_4_9_BASE:1.97
	OPENBSD_4_8:1.91.0.2
	OPENBSD_4_8_BASE:1.91
	OPENBSD_4_7:1.89.0.2
	OPENBSD_4_7_BASE:1.89
	OPENBSD_4_6:1.89.0.4
	OPENBSD_4_6_BASE:1.89
	OPENBSD_4_5:1.87.0.2
	OPENBSD_4_5_BASE:1.87
	OPENBSD_4_4:1.79.0.2
	OPENBSD_4_4_BASE:1.79
	OPENBSD_4_3:1.77.0.2
	OPENBSD_4_3_BASE:1.77
	OPENBSD_4_2:1.69.0.4
	OPENBSD_4_2_BASE:1.69
	OPENBSD_4_1:1.69.0.2
	OPENBSD_4_1_BASE:1.69
	OPENBSD_4_0:1.64.0.2
	OPENBSD_4_0_BASE:1.64
	OPENBSD_3_9:1.52.0.2
	OPENBSD_3_9_BASE:1.52;
locks; strict;
comment	@ * @;


1.119
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.118;
commitid	VyLWTsbepAOk7VQM;

1.118
date	2016.11.29.10.22.30;	author jsg;	state Exp;
branches;
next	1.117;
commitid	ZQetSMB5ilG2z10X;

1.117
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.116;
commitid	8YSL8ByWzGeIGBiJ;

1.116
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.115;
commitid	B0kwmVGiD5DVx4kv;

1.115
date	2015.11.24.17.11.39;	author mpi;	state Exp;
branches;
next	1.114;
commitid	5gdEnqVoJuTuwdTu;

1.114
date	2015.11.20.03.35.23;	author dlg;	state Exp;
branches;
next	1.113;
commitid	eYnPulzvLjDImPCa;

1.113
date	2015.11.14.17.54.57;	author mpi;	state Exp;
branches;
next	1.112;
commitid	Waft2RDjXAxr4qZ9;

1.112
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.111;
commitid	hPF95ClMUQfeqQDX;

1.111
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.110;
commitid	MVWrtktB46JRxFWT;

1.110
date	2015.03.20.18.42.25;	author mpi;	state Exp;
branches;
next	1.109;
commitid	5RMfBLaX9iOEJNQU;

1.109
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.108;
commitid	p4LJxGKbi0BU2cG6;

1.108
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.107;
commitid	yM2VFFhpDTeFQlve;

1.107
date	2014.08.20.23.56.57;	author dlg;	state Exp;
branches;
next	1.106;
commitid	LKds0X7ar6JW3YqG;

1.106
date	2014.08.20.01.02.50;	author dlg;	state Exp;
branches;
next	1.105;
commitid	lSPrbeSeISePZs1C;

1.105
date	2014.07.22.13.12.11;	author mpi;	state Exp;
branches;
next	1.104;
commitid	TGHgrLxu6sxZoiFt;

1.104
date	2013.12.28.03.34.54;	author deraadt;	state Exp;
branches;
next	1.103;

1.103
date	2013.12.06.21.03.04;	author deraadt;	state Exp;
branches;
next	1.102;

1.102
date	2013.08.07.01.06.36;	author bluhm;	state Exp;
branches;
next	1.101;

1.101
date	2013.04.01.06.40.40;	author brad;	state Exp;
branches;
next	1.100;

1.100
date	2012.11.29.21.10.32;	author brad;	state Exp;
branches;
next	1.99;

1.99
date	2012.08.31.12.41.17;	author stsp;	state Exp;
branches;
next	1.98;

1.98
date	2011.04.05.18.01.21;	author henning;	state Exp;
branches;
next	1.97;

1.97
date	2011.01.10.16.18.03;	author kettenis;	state Exp;
branches;
next	1.96;

1.96
date	2010.09.07.16.21.45;	author deraadt;	state Exp;
branches;
next	1.95;

1.95
date	2010.08.31.17.13.44;	author deraadt;	state Exp;
branches;
next	1.94;

1.94
date	2010.08.31.16.27.09;	author deraadt;	state Exp;
branches;
next	1.93;

1.93
date	2010.08.27.19.56.23;	author deraadt;	state Exp;
branches;
next	1.92;

1.92
date	2010.08.27.17.08.00;	author jsg;	state Exp;
branches;
next	1.91;

1.91
date	2010.08.06.03.02.24;	author mlarkin;	state Exp;
branches;
next	1.90;

1.90
date	2010.05.19.15.27.35;	author oga;	state Exp;
branches;
next	1.89;

1.89
date	2009.06.18.08.19.03;	author jsg;	state Exp;
branches;
next	1.88;

1.88
date	2009.03.29.21.53.52;	author sthen;	state Exp;
branches;
next	1.87;

1.87
date	2008.11.28.02.44.18;	author brad;	state Exp;
branches;
next	1.86;

1.86
date	2008.11.09.15.08.26;	author naddy;	state Exp;
branches;
next	1.85;

1.85
date	2008.10.28.05.53.20;	author brad;	state Exp;
branches;
next	1.84;

1.84
date	2008.10.28.05.09.43;	author brad;	state Exp;
branches;
next	1.83;

1.83
date	2008.10.16.19.18.03;	author naddy;	state Exp;
branches;
next	1.82;

1.82
date	2008.10.02.20.21.14;	author brad;	state Exp;
branches;
next	1.81;

1.81
date	2008.09.10.14.01.22;	author blambert;	state Exp;
branches;
next	1.80;

1.80
date	2008.08.09.21.00.52;	author brad;	state Exp;
branches;
next	1.79;

1.79
date	2008.05.23.08.49.27;	author brad;	state Exp;
branches;
next	1.78;

1.78
date	2008.05.19.01.12.41;	author fgsch;	state Exp;
branches;
next	1.77;

1.77
date	2008.02.05.16.52.50;	author brad;	state Exp;
branches;
next	1.76;

1.76
date	2008.01.02.03.43.54;	author brad;	state Exp;
branches;
next	1.75;

1.75
date	2007.12.11.23.08.09;	author mikeb;	state Exp;
branches;
next	1.74;

1.74
date	2007.12.05.08.30.33;	author jsg;	state Exp;
branches;
next	1.73;

1.73
date	2007.11.17.15.52.23;	author jsg;	state Exp;
branches;
next	1.72;

1.72
date	2007.09.12.00.42.04;	author jsg;	state Exp;
branches;
next	1.71;

1.71
date	2007.09.07.19.05.05;	author damien;	state Exp;
branches;
next	1.70;

1.70
date	2007.09.01.19.19.39;	author ckuethe;	state Exp;
branches;
next	1.69;

1.69
date	2007.03.02.00.16.59;	author jsg;	state Exp;
branches;
next	1.68;

1.68
date	2007.01.08.18.39.27;	author damien;	state Exp;
branches;
next	1.67;

1.67
date	2006.11.15.02.24.37;	author brad;	state Exp;
branches;
next	1.66;

1.66
date	2006.11.10.20.46.58;	author damien;	state Exp;
branches;
next	1.65;

1.65
date	2006.11.05.20.15.37;	author brad;	state Exp;
branches;
next	1.64;

1.64
date	2006.07.23.02.12.12;	author brad;	state Exp;
branches
	1.64.2.1;
next	1.63;

1.63
date	2006.06.17.18.00.43;	author brad;	state Exp;
branches;
next	1.62;

1.62
date	2006.05.29.01.00.02;	author brad;	state Exp;
branches;
next	1.61;

1.61
date	2006.05.28.00.20.21;	author brad;	state Exp;
branches;
next	1.60;

1.60
date	2006.05.28.00.04.24;	author jason;	state Exp;
branches;
next	1.59;

1.59
date	2006.05.27.10.03.15;	author brad;	state Exp;
branches;
next	1.58;

1.58
date	2006.05.20.03.47.56;	author brad;	state Exp;
branches;
next	1.57;

1.57
date	2006.04.26.02.07.29;	author jsg;	state Exp;
branches;
next	1.56;

1.56
date	2006.04.26.01.33.38;	author brad;	state Exp;
branches;
next	1.55;

1.55
date	2006.04.26.01.20.28;	author brad;	state Exp;
branches;
next	1.54;

1.54
date	2006.04.07.12.38.12;	author jsg;	state Exp;
branches;
next	1.53;

1.53
date	2006.03.25.22.41.45;	author djm;	state Exp;
branches;
next	1.52;

1.52
date	2006.03.02.09.04.00;	author jsg;	state Exp;
branches
	1.52.2.1;
next	1.51;

1.51
date	2006.02.26.19.25.41;	author damien;	state Exp;
branches;
next	1.50;

1.50
date	2006.02.26.15.58.27;	author krw;	state Exp;
branches;
next	1.49;

1.49
date	2006.02.26.11.02.29;	author jsg;	state Exp;
branches;
next	1.48;

1.48
date	2006.02.24.04.58.25;	author brad;	state Exp;
branches;
next	1.47;

1.47
date	2006.02.22.21.05.13;	author damien;	state Exp;
branches;
next	1.46;

1.46
date	2006.02.22.19.23.44;	author damien;	state Exp;
branches;
next	1.45;

1.45
date	2006.02.22.03.19.11;	author brad;	state Exp;
branches;
next	1.44;

1.44
date	2006.02.21.20.52.15;	author damien;	state Exp;
branches;
next	1.43;

1.43
date	2006.02.20.20.19.47;	author damien;	state Exp;
branches;
next	1.42;

1.42
date	2006.02.19.13.57.02;	author damien;	state Exp;
branches;
next	1.41;

1.41
date	2006.02.16.17.35.51;	author damien;	state Exp;
branches;
next	1.40;

1.40
date	2006.02.15.20.21.27;	author brad;	state Exp;
branches;
next	1.39;

1.39
date	2006.02.15.20.08.59;	author damien;	state Exp;
branches;
next	1.38;

1.38
date	2006.02.15.19.53.17;	author damien;	state Exp;
branches;
next	1.37;

1.37
date	2006.02.15.19.36.46;	author damien;	state Exp;
branches;
next	1.36;

1.36
date	2006.02.13.08.54.54;	author brad;	state Exp;
branches;
next	1.35;

1.35
date	2006.02.13.06.15.32;	author brad;	state Exp;
branches;
next	1.34;

1.34
date	2006.02.12.15.53.20;	author damien;	state Exp;
branches;
next	1.33;

1.33
date	2006.02.12.13.28.30;	author damien;	state Exp;
branches;
next	1.32;

1.32
date	2006.02.12.13.25.01;	author damien;	state Exp;
branches;
next	1.31;

1.31
date	2006.02.12.13.08.42;	author damien;	state Exp;
branches;
next	1.30;

1.30
date	2006.02.12.10.28.07;	author damien;	state Exp;
branches;
next	1.29;

1.29
date	2006.02.11.20.25.21;	author brad;	state Exp;
branches;
next	1.28;

1.28
date	2006.02.11.11.51.30;	author damien;	state Exp;
branches;
next	1.27;

1.27
date	2006.02.11.09.40.36;	author damien;	state Exp;
branches;
next	1.26;

1.26
date	2006.02.11.09.26.15;	author damien;	state Exp;
branches;
next	1.25;

1.25
date	2006.02.11.09.18.56;	author damien;	state Exp;
branches;
next	1.24;

1.24
date	2006.02.11.09.15.57;	author damien;	state Exp;
branches;
next	1.23;

1.23
date	2006.02.10.03.54.54;	author brad;	state Exp;
branches;
next	1.22;

1.22
date	2006.02.08.13.28.32;	author jsg;	state Exp;
branches;
next	1.21;

1.21
date	2006.02.08.09.28.46;	author jsg;	state Exp;
branches;
next	1.20;

1.20
date	2006.02.07.08.55.37;	author jsg;	state Exp;
branches;
next	1.19;

1.19
date	2006.02.05.23.37.21;	author brad;	state Exp;
branches;
next	1.18;

1.18
date	2006.02.05.23.32.06;	author brad;	state Exp;
branches;
next	1.17;

1.17
date	2006.02.05.10.09.39;	author jsg;	state Exp;
branches;
next	1.16;

1.16
date	2006.02.05.10.01.23;	author damien;	state Exp;
branches;
next	1.15;

1.15
date	2006.02.05.09.38.29;	author damien;	state Exp;
branches;
next	1.14;

1.14
date	2006.02.05.09.14.28;	author damien;	state Exp;
branches;
next	1.13;

1.13
date	2006.02.04.21.48.34;	author damien;	state Exp;
branches;
next	1.12;

1.12
date	2006.02.04.16.51.15;	author damien;	state Exp;
branches;
next	1.11;

1.11
date	2006.02.04.10.32.56;	author damien;	state Exp;
branches;
next	1.10;

1.10
date	2006.02.04.09.46.48;	author damien;	state Exp;
branches;
next	1.9;

1.9
date	2006.01.22.21.35.08;	author damien;	state Exp;
branches;
next	1.8;

1.8
date	2006.01.20.22.02.03;	author brad;	state Exp;
branches;
next	1.7;

1.7
date	2006.01.18.20.44.51;	author damien;	state Exp;
branches;
next	1.6;

1.6
date	2006.01.15.10.55.06;	author damien;	state Exp;
branches;
next	1.5;

1.5
date	2006.01.14.04.33.35;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2005.12.17.11.12.54;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2005.12.17.09.03.14;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2005.12.14.22.08.20;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2005.12.14.21.54.57;	author jsg;	state Exp;
branches;
next	;

1.52.2.1
date	2006.05.03.03.49.50;	author brad;	state Exp;
branches;
next	1.52.2.2;

1.52.2.2
date	2006.11.14.00.01.53;	author brad;	state Exp;
branches;
next	;

1.64.2.1
date	2006.11.13.23.41.38;	author brad;	state Exp;
branches;
next	;


desc
@@


1.119
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@/*	$OpenBSD: if_nfe.c,v 1.118 2016/11/29 10:22:30 jsg Exp $	*/

/*-
 * Copyright (c) 2006, 2007 Damien Bergamini <damien.bergamini@@free.fr>
 * Copyright (c) 2005, 2006 Jonathan Gray <jsg@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/* Driver for NVIDIA nForce MCP Fast Ethernet and Gigabit Ethernet */

#include "bpfilter.h"
#include "vlan.h"

#include <sys/param.h>
#include <sys/endian.h>
#include <sys/systm.h>
#include <sys/types.h>
#include <sys/sockio.h>
#include <sys/mbuf.h>
#include <sys/queue.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/timeout.h>
#include <sys/socket.h>

#include <machine/bus.h>

#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#include <dev/mii/miivar.h>

#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>
#include <dev/pci/pcidevs.h>

#include <dev/pci/if_nfereg.h>
#include <dev/pci/if_nfevar.h>

int	nfe_match(struct device *, void *, void *);
void	nfe_attach(struct device *, struct device *, void *);
int	nfe_activate(struct device *, int);
void	nfe_miibus_statchg(struct device *);
int	nfe_miibus_readreg(struct device *, int, int);
void	nfe_miibus_writereg(struct device *, int, int, int);
int	nfe_intr(void *);
int	nfe_ioctl(struct ifnet *, u_long, caddr_t);
void	nfe_txdesc32_sync(struct nfe_softc *, struct nfe_desc32 *, int);
void	nfe_txdesc64_sync(struct nfe_softc *, struct nfe_desc64 *, int);
void	nfe_txdesc32_rsync(struct nfe_softc *, int, int, int);
void	nfe_txdesc64_rsync(struct nfe_softc *, int, int, int);
void	nfe_rxdesc32_sync(struct nfe_softc *, struct nfe_desc32 *, int);
void	nfe_rxdesc64_sync(struct nfe_softc *, struct nfe_desc64 *, int);
void	nfe_rxeof(struct nfe_softc *);
void	nfe_txeof(struct nfe_softc *);
int	nfe_encap(struct nfe_softc *, struct mbuf *);
void	nfe_start(struct ifnet *);
void	nfe_watchdog(struct ifnet *);
int	nfe_init(struct ifnet *);
void	nfe_stop(struct ifnet *, int);
int	nfe_alloc_rx_ring(struct nfe_softc *, struct nfe_rx_ring *);
void	nfe_reset_rx_ring(struct nfe_softc *, struct nfe_rx_ring *);
void	nfe_free_rx_ring(struct nfe_softc *, struct nfe_rx_ring *);
int	nfe_alloc_tx_ring(struct nfe_softc *, struct nfe_tx_ring *);
void	nfe_reset_tx_ring(struct nfe_softc *, struct nfe_tx_ring *);
void	nfe_free_tx_ring(struct nfe_softc *, struct nfe_tx_ring *);
int	nfe_ifmedia_upd(struct ifnet *);
void	nfe_ifmedia_sts(struct ifnet *, struct ifmediareq *);
void	nfe_iff(struct nfe_softc *);
void	nfe_get_macaddr(struct nfe_softc *, uint8_t *);
void	nfe_set_macaddr(struct nfe_softc *, const uint8_t *);
void	nfe_tick(void *);
#ifndef SMALL_KERNEL
int	nfe_wol(struct ifnet*, int);
#endif

struct cfattach nfe_ca = {
	sizeof (struct nfe_softc), nfe_match, nfe_attach, NULL,
	nfe_activate
};

struct cfdriver nfe_cd = {
	NULL, "nfe", DV_IFNET
};

#ifdef NFE_DEBUG
int nfedebug = 0;
#define DPRINTF(x)	do { if (nfedebug) printf x; } while (0)
#define DPRINTFN(n,x)	do { if (nfedebug >= (n)) printf x; } while (0)
#else
#define DPRINTF(x)
#define DPRINTFN(n,x)
#endif

const struct pci_matchid nfe_devices[] = {
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE_LAN },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE2_LAN },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE3_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE3_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE3_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE3_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_NFORCE3_LAN5 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_CK804_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_CK804_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP04_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP04_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP51_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP51_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP55_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP55_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP61_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP61_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP61_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP61_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP65_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP65_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP65_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP65_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP67_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP67_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP67_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP67_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP73_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP73_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP73_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP73_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP77_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP77_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP77_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP77_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP79_LAN1 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP79_LAN2 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP79_LAN3 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP79_LAN4 },
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP89_LAN }
};

int
nfe_match(struct device *dev, void *match, void *aux)
{
	return pci_matchbyid((struct pci_attach_args *)aux, nfe_devices,
	    sizeof (nfe_devices) / sizeof (nfe_devices[0]));
}

int
nfe_activate(struct device *self, int act)
{
	struct nfe_softc *sc = (struct nfe_softc *)self;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	int rv = 0;

	switch (act) {
	case DVACT_SUSPEND:
		if (ifp->if_flags & IFF_RUNNING)
			nfe_stop(ifp, 0);
		rv = config_activate_children(self, act);
		break;
	case DVACT_RESUME:
		if (ifp->if_flags & IFF_UP)
			nfe_init(ifp);
		break;
	default:
		rv = config_activate_children(self, act);
		break;
	}
	return (rv);
}


void
nfe_attach(struct device *parent, struct device *self, void *aux)
{
	struct nfe_softc *sc = (struct nfe_softc *)self;
	struct pci_attach_args *pa = aux;
	pci_chipset_tag_t pc = pa->pa_pc;
	pci_intr_handle_t ih;
	const char *intrstr;
	struct ifnet *ifp;
	bus_size_t memsize;
	pcireg_t memtype;

	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, NFE_PCI_BA);
	if (pci_mapreg_map(pa, NFE_PCI_BA, memtype, 0, &sc->sc_memt,
	    &sc->sc_memh, NULL, &memsize, 0)) {
		printf(": can't map mem space\n");
		return;
	}

	if (pci_intr_map(pa, &ih) != 0) {
		printf(": can't map interrupt\n");
		return;
	}

	intrstr = pci_intr_string(pc, ih);
	sc->sc_ih = pci_intr_establish(pc, ih, IPL_NET, nfe_intr, sc,
	    sc->sc_dev.dv_xname);
	if (sc->sc_ih == NULL) {
		printf(": could not establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		return;
	}
	printf(": %s", intrstr);

	sc->sc_dmat = pa->pa_dmat;
	sc->sc_flags = 0;

	switch (PCI_PRODUCT(pa->pa_id)) {
	case PCI_PRODUCT_NVIDIA_NFORCE3_LAN2:
	case PCI_PRODUCT_NVIDIA_NFORCE3_LAN3:
	case PCI_PRODUCT_NVIDIA_NFORCE3_LAN4:
	case PCI_PRODUCT_NVIDIA_NFORCE3_LAN5:
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_HW_CSUM;
		break;
	case PCI_PRODUCT_NVIDIA_MCP51_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP51_LAN2:
		sc->sc_flags |= NFE_40BIT_ADDR | NFE_PWR_MGMT;
		break;
	case PCI_PRODUCT_NVIDIA_MCP61_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP61_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP61_LAN3:
	case PCI_PRODUCT_NVIDIA_MCP61_LAN4:
	case PCI_PRODUCT_NVIDIA_MCP67_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP67_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP67_LAN3:
	case PCI_PRODUCT_NVIDIA_MCP67_LAN4:
	case PCI_PRODUCT_NVIDIA_MCP73_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP73_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP73_LAN3:
	case PCI_PRODUCT_NVIDIA_MCP73_LAN4:
		sc->sc_flags |= NFE_40BIT_ADDR | NFE_CORRECT_MACADDR |
		    NFE_PWR_MGMT;
		break;
	case PCI_PRODUCT_NVIDIA_MCP77_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP77_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP77_LAN3:
	case PCI_PRODUCT_NVIDIA_MCP77_LAN4:
		sc->sc_flags |= NFE_40BIT_ADDR | NFE_HW_CSUM |
		    NFE_CORRECT_MACADDR | NFE_PWR_MGMT;
		break;
	case PCI_PRODUCT_NVIDIA_MCP79_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP79_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP79_LAN3:
	case PCI_PRODUCT_NVIDIA_MCP79_LAN4:
	case PCI_PRODUCT_NVIDIA_MCP89_LAN:
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR | NFE_HW_CSUM |
		    NFE_CORRECT_MACADDR | NFE_PWR_MGMT;
		break;
	case PCI_PRODUCT_NVIDIA_CK804_LAN1:
	case PCI_PRODUCT_NVIDIA_CK804_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP04_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP04_LAN2:
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR | NFE_HW_CSUM;
		break;
	case PCI_PRODUCT_NVIDIA_MCP65_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP65_LAN2:
	case PCI_PRODUCT_NVIDIA_MCP65_LAN3:
	case PCI_PRODUCT_NVIDIA_MCP65_LAN4:
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR |
		    NFE_CORRECT_MACADDR | NFE_PWR_MGMT;
		break;
	case PCI_PRODUCT_NVIDIA_MCP55_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP55_LAN2:
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR | NFE_HW_CSUM |
		    NFE_HW_VLAN | NFE_PWR_MGMT;
		break;
	}

	if (sc->sc_flags & NFE_PWR_MGMT) {
		NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_RESET | NFE_RXTX_BIT2);
		NFE_WRITE(sc, NFE_MAC_RESET, NFE_MAC_RESET_MAGIC);
		DELAY(100);
		NFE_WRITE(sc, NFE_MAC_RESET, 0);
		DELAY(100);
		NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_BIT2);
		NFE_WRITE(sc, NFE_PWR2_CTL,
		    NFE_READ(sc, NFE_PWR2_CTL) & ~NFE_PWR2_WAKEUP_MASK);
	}

	nfe_get_macaddr(sc, sc->sc_arpcom.ac_enaddr);
	printf(", address %s\n", ether_sprintf(sc->sc_arpcom.ac_enaddr));

	/*
	 * Allocate Tx and Rx rings.
	 */
	if (nfe_alloc_tx_ring(sc, &sc->txq) != 0) {
		printf("%s: could not allocate Tx ring\n",
		    sc->sc_dev.dv_xname);
		return;
	}

	if (nfe_alloc_rx_ring(sc, &sc->rxq) != 0) {
		printf("%s: could not allocate Rx ring\n",
		    sc->sc_dev.dv_xname);
		nfe_free_tx_ring(sc, &sc->txq);
		return;
	}

	ifp = &sc->sc_arpcom.ac_if;
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = nfe_ioctl;
	ifp->if_start = nfe_start;
	ifp->if_watchdog = nfe_watchdog;
	IFQ_SET_MAXLEN(&ifp->if_snd, NFE_IFQ_MAXLEN);
	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, IFNAMSIZ);

	ifp->if_capabilities = IFCAP_VLAN_MTU;

#ifndef SMALL_KERNEL
	ifp->if_capabilities |= IFCAP_WOL;
	ifp->if_wol = nfe_wol;
	nfe_wol(ifp, 0);
#endif

#if NVLAN > 0
	if (sc->sc_flags & NFE_HW_VLAN)
		ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

	if (sc->sc_flags & NFE_HW_CSUM) {
		ifp->if_capabilities |= IFCAP_CSUM_IPv4 | IFCAP_CSUM_TCPv4 |
		    IFCAP_CSUM_UDPv4;
	}

	sc->sc_mii.mii_ifp = ifp;
	sc->sc_mii.mii_readreg = nfe_miibus_readreg;
	sc->sc_mii.mii_writereg = nfe_miibus_writereg;
	sc->sc_mii.mii_statchg = nfe_miibus_statchg;

	ifmedia_init(&sc->sc_mii.mii_media, 0, nfe_ifmedia_upd,
	    nfe_ifmedia_sts);
	mii_attach(self, &sc->sc_mii, 0xffffffff, MII_PHY_ANY, 0, 0);
	if (LIST_FIRST(&sc->sc_mii.mii_phys) == NULL) {
		printf("%s: no PHY found!\n", sc->sc_dev.dv_xname);
		ifmedia_add(&sc->sc_mii.mii_media, IFM_ETHER | IFM_MANUAL,
		    0, NULL);
		ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER | IFM_MANUAL);
	} else
		ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER | IFM_AUTO);

	if_attach(ifp);
	ether_ifattach(ifp);

	timeout_set(&sc->sc_tick_ch, nfe_tick, sc);
}

void
nfe_miibus_statchg(struct device *dev)
{
	struct nfe_softc *sc = (struct nfe_softc *)dev;
	struct mii_data *mii = &sc->sc_mii;
	uint32_t phy, seed, misc = NFE_MISC1_MAGIC, link = NFE_MEDIA_SET;

	phy = NFE_READ(sc, NFE_PHY_IFACE);
	phy &= ~(NFE_PHY_HDX | NFE_PHY_100TX | NFE_PHY_1000T);

	seed = NFE_READ(sc, NFE_RNDSEED);
	seed &= ~NFE_SEED_MASK;

	if ((mii->mii_media_active & IFM_GMASK) == IFM_HDX) {
		phy  |= NFE_PHY_HDX;	/* half-duplex */
		misc |= NFE_MISC1_HDX;
	}

	switch (IFM_SUBTYPE(mii->mii_media_active)) {
	case IFM_1000_T:	/* full-duplex only */
		link |= NFE_MEDIA_1000T;
		seed |= NFE_SEED_1000T;
		phy  |= NFE_PHY_1000T;
		break;
	case IFM_100_TX:
		link |= NFE_MEDIA_100TX;
		seed |= NFE_SEED_100TX;
		phy  |= NFE_PHY_100TX;
		break;
	case IFM_10_T:
		link |= NFE_MEDIA_10T;
		seed |= NFE_SEED_10T;
		break;
	}

	NFE_WRITE(sc, NFE_RNDSEED, seed);	/* XXX: gigabit NICs only? */

	NFE_WRITE(sc, NFE_PHY_IFACE, phy);
	NFE_WRITE(sc, NFE_MISC1, misc);
	NFE_WRITE(sc, NFE_LINKSPEED, link);
}

int
nfe_miibus_readreg(struct device *dev, int phy, int reg)
{
	struct nfe_softc *sc = (struct nfe_softc *)dev;
	uint32_t val;
	int ntries;

	NFE_WRITE(sc, NFE_PHY_STATUS, 0xf);

	if (NFE_READ(sc, NFE_PHY_CTL) & NFE_PHY_BUSY) {
		NFE_WRITE(sc, NFE_PHY_CTL, NFE_PHY_BUSY);
		DELAY(100);
	}

	NFE_WRITE(sc, NFE_PHY_CTL, (phy << NFE_PHYADD_SHIFT) | reg);

	for (ntries = 0; ntries < 1000; ntries++) {
		DELAY(100);
		if (!(NFE_READ(sc, NFE_PHY_CTL) & NFE_PHY_BUSY))
			break;
	}
	if (ntries == 1000) {
		DPRINTFN(2, ("%s: timeout waiting for PHY\n",
		    sc->sc_dev.dv_xname));
		return 0;
	}

	if (NFE_READ(sc, NFE_PHY_STATUS) & NFE_PHY_ERROR) {
		DPRINTFN(2, ("%s: could not read PHY\n",
		    sc->sc_dev.dv_xname));
		return 0;
	}

	val = NFE_READ(sc, NFE_PHY_DATA);
	if (val != 0xffffffff && val != 0)
		sc->mii_phyaddr = phy;

	DPRINTFN(2, ("%s: mii read phy %d reg 0x%x ret 0x%x\n",
	    sc->sc_dev.dv_xname, phy, reg, val));

	return val;
}

void
nfe_miibus_writereg(struct device *dev, int phy, int reg, int val)
{
	struct nfe_softc *sc = (struct nfe_softc *)dev;
	uint32_t ctl;
	int ntries;

	NFE_WRITE(sc, NFE_PHY_STATUS, 0xf);

	if (NFE_READ(sc, NFE_PHY_CTL) & NFE_PHY_BUSY) {
		NFE_WRITE(sc, NFE_PHY_CTL, NFE_PHY_BUSY);
		DELAY(100);
	}

	NFE_WRITE(sc, NFE_PHY_DATA, val);
	ctl = NFE_PHY_WRITE | (phy << NFE_PHYADD_SHIFT) | reg;
	NFE_WRITE(sc, NFE_PHY_CTL, ctl);

	for (ntries = 0; ntries < 1000; ntries++) {
		DELAY(100);
		if (!(NFE_READ(sc, NFE_PHY_CTL) & NFE_PHY_BUSY))
			break;
	}
#ifdef NFE_DEBUG
	if (nfedebug >= 2 && ntries == 1000)
		printf("could not write to PHY\n");
#endif
}

int
nfe_intr(void *arg)
{
	struct nfe_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	uint32_t r;

	if ((r = NFE_READ(sc, NFE_IRQ_STATUS) & NFE_IRQ_WANTED) == 0)
		return 0;	/* not for us */
	NFE_WRITE(sc, NFE_IRQ_STATUS, r);

	DPRINTFN(5, ("nfe_intr: interrupt register %x\n", r));

	if (r & NFE_IRQ_LINK) {
		NFE_READ(sc, NFE_PHY_STATUS);
		NFE_WRITE(sc, NFE_PHY_STATUS, 0xf);
		DPRINTF(("%s: link state changed\n", sc->sc_dev.dv_xname));
	}

	if (ifp->if_flags & IFF_RUNNING) {
		/* check Rx ring */
		nfe_rxeof(sc);

		/* check Tx ring */
		nfe_txeof(sc);
	}

	return 1;
}

int
nfe_ioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct nfe_softc *sc = ifp->if_softc;
	struct ifreq *ifr = (struct ifreq *)data;
	int s, error = 0;

	s = splnet();

	switch (cmd) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			nfe_init(ifp);
		break;

	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				nfe_init(ifp);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				nfe_stop(ifp, 1);
		}
		break;

	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &sc->sc_mii.mii_media, cmd);
		break;

	default:
		error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data);
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			nfe_iff(sc);
		error = 0;
	}

	splx(s);
	return error;
}

void
nfe_txdesc32_sync(struct nfe_softc *sc, struct nfe_desc32 *desc32, int ops)
{
	bus_dmamap_sync(sc->sc_dmat, sc->txq.map,
	    (caddr_t)desc32 - (caddr_t)sc->txq.desc32,
	    sizeof (struct nfe_desc32), ops);
}

void
nfe_txdesc64_sync(struct nfe_softc *sc, struct nfe_desc64 *desc64, int ops)
{
	bus_dmamap_sync(sc->sc_dmat, sc->txq.map,
	    (caddr_t)desc64 - (caddr_t)sc->txq.desc64,
	    sizeof (struct nfe_desc64), ops);
}

void
nfe_txdesc32_rsync(struct nfe_softc *sc, int start, int end, int ops)
{
	if (end > start) {
		bus_dmamap_sync(sc->sc_dmat, sc->txq.map,
		    (caddr_t)&sc->txq.desc32[start] - (caddr_t)sc->txq.desc32,
		    (caddr_t)&sc->txq.desc32[end] -
		    (caddr_t)&sc->txq.desc32[start], ops);
		return;
	}
	/* sync from 'start' to end of ring */
	bus_dmamap_sync(sc->sc_dmat, sc->txq.map,
	    (caddr_t)&sc->txq.desc32[start] - (caddr_t)sc->txq.desc32,
	    (caddr_t)&sc->txq.desc32[NFE_TX_RING_COUNT] -
	    (caddr_t)&sc->txq.desc32[start], ops);

	/* sync from start of ring to 'end' */
	bus_dmamap_sync(sc->sc_dmat, sc->txq.map, 0,
	    (caddr_t)&sc->txq.desc32[end] - (caddr_t)sc->txq.desc32, ops);
}

void
nfe_txdesc64_rsync(struct nfe_softc *sc, int start, int end, int ops)
{
	if (end > start) {
		bus_dmamap_sync(sc->sc_dmat, sc->txq.map,
		    (caddr_t)&sc->txq.desc64[start] - (caddr_t)sc->txq.desc64,
		    (caddr_t)&sc->txq.desc64[end] -
		    (caddr_t)&sc->txq.desc64[start], ops);
		return;
	}
	/* sync from 'start' to end of ring */
	bus_dmamap_sync(sc->sc_dmat, sc->txq.map,
	    (caddr_t)&sc->txq.desc64[start] - (caddr_t)sc->txq.desc64,
	    (caddr_t)&sc->txq.desc64[NFE_TX_RING_COUNT] -
	    (caddr_t)&sc->txq.desc64[start], ops);

	/* sync from start of ring to 'end' */
	bus_dmamap_sync(sc->sc_dmat, sc->txq.map, 0,
	    (caddr_t)&sc->txq.desc64[end] - (caddr_t)sc->txq.desc64, ops);
}

void
nfe_rxdesc32_sync(struct nfe_softc *sc, struct nfe_desc32 *desc32, int ops)
{
	bus_dmamap_sync(sc->sc_dmat, sc->rxq.map,
	    (caddr_t)desc32 - (caddr_t)sc->rxq.desc32,
	    sizeof (struct nfe_desc32), ops);
}

void
nfe_rxdesc64_sync(struct nfe_softc *sc, struct nfe_desc64 *desc64, int ops)
{
	bus_dmamap_sync(sc->sc_dmat, sc->rxq.map,
	    (caddr_t)desc64 - (caddr_t)sc->rxq.desc64,
	    sizeof (struct nfe_desc64), ops);
}

void
nfe_rxeof(struct nfe_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct nfe_desc32 *desc32;
	struct nfe_desc64 *desc64;
	struct nfe_rx_data *data;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *m, *mnew;
	bus_addr_t physaddr;
#if NVLAN > 0
	uint32_t vtag;
#endif
	uint16_t flags;
	int error, len;

	for (;;) {
		data = &sc->rxq.data[sc->rxq.cur];

		if (sc->sc_flags & NFE_40BIT_ADDR) {
			desc64 = &sc->rxq.desc64[sc->rxq.cur];
			nfe_rxdesc64_sync(sc, desc64, BUS_DMASYNC_POSTREAD);

			flags = letoh16(desc64->flags);
			len = letoh16(desc64->length) & 0x3fff;
#if NVLAN > 0
			vtag = letoh32(desc64->physaddr[1]);
#endif
		} else {
			desc32 = &sc->rxq.desc32[sc->rxq.cur];
			nfe_rxdesc32_sync(sc, desc32, BUS_DMASYNC_POSTREAD);

			flags = letoh16(desc32->flags);
			len = letoh16(desc32->length) & 0x3fff;
		}

		if (flags & NFE_RX_READY)
			break;

		if ((sc->sc_flags & (NFE_JUMBO_SUP | NFE_40BIT_ADDR)) == 0) {
			if (!(flags & NFE_RX_VALID_V1))
				goto skip;

			if ((flags & NFE_RX_FIXME_V1) == NFE_RX_FIXME_V1) {
				flags &= ~NFE_RX_ERROR;
				len--;	/* fix buffer length */
			}
		} else {
			if (!(flags & NFE_RX_VALID_V2))
				goto skip;

			if ((flags & NFE_RX_FIXME_V2) == NFE_RX_FIXME_V2) {
				flags &= ~NFE_RX_ERROR;
				len--;	/* fix buffer length */
			}
		}

		if (flags & NFE_RX_ERROR) {
			ifp->if_ierrors++;
			goto skip;
		}

		/*
		 * Try to allocate a new mbuf for this ring element and load
		 * it before processing the current mbuf. If the ring element
		 * cannot be loaded, drop the received packet and reuse the
		 * old mbuf. In the unlikely case that the old mbuf can't be
		 * reloaded either, explicitly panic.
		 */
		mnew = MCLGETI(NULL, MCLBYTES, NULL, M_DONTWAIT);
		if (mnew == NULL) {
			ifp->if_ierrors++;
			goto skip;
		}
		mnew->m_pkthdr.len = mnew->m_len = MCLBYTES;

		bus_dmamap_sync(sc->sc_dmat, data->map, 0,
		    data->map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, data->map);

		error = bus_dmamap_load_mbuf(sc->sc_dmat, data->map, mnew,
		    BUS_DMA_READ | BUS_DMA_NOWAIT);
		if (error != 0) {
			m_freem(mnew);

			/* try to reload the old mbuf */
			error = bus_dmamap_load_mbuf(sc->sc_dmat, data->map,
			    m, BUS_DMA_READ | BUS_DMA_NOWAIT);
			if (error != 0) {
				/* very unlikely that it will fail.. */
				panic("%s: could not load old rx mbuf",
				    sc->sc_dev.dv_xname);
			}
			ifp->if_ierrors++;
			goto skip;
		}
		physaddr = data->map->dm_segs[0].ds_addr;

		/*
		 * New mbuf successfully loaded, update Rx ring and continue
		 * processing.
		 */
		m = data->m;
		data->m = mnew;

		/* finalize mbuf */
		m->m_pkthdr.len = m->m_len = len;

		if ((sc->sc_flags & NFE_HW_CSUM) &&
		    (flags & NFE_RX_IP_CSUMOK)) {
			m->m_pkthdr.csum_flags |= M_IPV4_CSUM_IN_OK;
			if (flags & NFE_RX_UDP_CSUMOK)
				m->m_pkthdr.csum_flags |= M_UDP_CSUM_IN_OK;
			if (flags & NFE_RX_TCP_CSUMOK)
				m->m_pkthdr.csum_flags |= M_TCP_CSUM_IN_OK;
		}

#if NVLAN > 0
		if ((vtag & NFE_RX_VTAG) && (sc->sc_flags & NFE_HW_VLAN)) {
			m->m_pkthdr.ether_vtag = vtag & 0xffff;
			m->m_flags |= M_VLANTAG;
		}
#endif

		ml_enqueue(&ml, m);

		/* update mapping address in h/w descriptor */
		if (sc->sc_flags & NFE_40BIT_ADDR) {
#if defined(__LP64__)
			desc64->physaddr[0] = htole32(physaddr >> 32);
#endif
			desc64->physaddr[1] = htole32(physaddr & 0xffffffff);
		} else {
			desc32->physaddr = htole32(physaddr);
		}

skip:		if (sc->sc_flags & NFE_40BIT_ADDR) {
			desc64->length = htole16(sc->rxq.bufsz);
			desc64->flags = htole16(NFE_RX_READY);

			nfe_rxdesc64_sync(sc, desc64, BUS_DMASYNC_PREWRITE);
		} else {
			desc32->length = htole16(sc->rxq.bufsz);
			desc32->flags = htole16(NFE_RX_READY);

			nfe_rxdesc32_sync(sc, desc32, BUS_DMASYNC_PREWRITE);
		}

		sc->rxq.cur = (sc->rxq.cur + 1) % NFE_RX_RING_COUNT;
	}
	if_input(ifp, &ml);
}

void
nfe_txeof(struct nfe_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct nfe_desc32 *desc32;
	struct nfe_desc64 *desc64;
	struct nfe_tx_data *data = NULL;
	uint16_t flags;

	while (sc->txq.next != sc->txq.cur) {
		if (sc->sc_flags & NFE_40BIT_ADDR) {
			desc64 = &sc->txq.desc64[sc->txq.next];
			nfe_txdesc64_sync(sc, desc64, BUS_DMASYNC_POSTREAD);

			flags = letoh16(desc64->flags);
		} else {
			desc32 = &sc->txq.desc32[sc->txq.next];
			nfe_txdesc32_sync(sc, desc32, BUS_DMASYNC_POSTREAD);

			flags = letoh16(desc32->flags);
		}

		if (flags & NFE_TX_VALID)
			break;

		data = &sc->txq.data[sc->txq.next];

		if ((sc->sc_flags & (NFE_JUMBO_SUP | NFE_40BIT_ADDR)) == 0) {
			if (!(flags & NFE_TX_LASTFRAG_V1) && data->m == NULL)
				goto skip;

			if ((flags & NFE_TX_ERROR_V1) != 0) {
				printf("%s: tx v1 error %b\n",
				    sc->sc_dev.dv_xname, flags, NFE_V1_TXERR);
				ifp->if_oerrors++;
			}
		} else {
			if (!(flags & NFE_TX_LASTFRAG_V2) && data->m == NULL)
				goto skip;

			if ((flags & NFE_TX_ERROR_V2) != 0) {
				printf("%s: tx v2 error %b\n",
				    sc->sc_dev.dv_xname, flags, NFE_V2_TXERR);
				ifp->if_oerrors++;
			}
		}

		if (data->m == NULL) {	/* should not get there */
			printf("%s: last fragment bit w/o associated mbuf!\n",
			    sc->sc_dev.dv_xname);
			goto skip;
		}

		/* last fragment of the mbuf chain transmitted */
		bus_dmamap_sync(sc->sc_dmat, data->active, 0,
		    data->active->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, data->active);
		m_freem(data->m);
		data->m = NULL;

		ifp->if_timer = 0;

skip:		sc->txq.queued--;
		sc->txq.next = (sc->txq.next + 1) % NFE_TX_RING_COUNT;
	}

	if (data != NULL) {	/* at least one slot freed */
		ifq_clr_oactive(&ifp->if_snd);
		nfe_start(ifp);
	}
}

int
nfe_encap(struct nfe_softc *sc, struct mbuf *m0)
{
	struct nfe_desc32 *desc32;
	struct nfe_desc64 *desc64;
	struct nfe_tx_data *data;
	bus_dmamap_t map;
	uint16_t flags = 0;
	uint32_t vtag = 0;
	int error, i, first = sc->txq.cur;

	map = sc->txq.data[first].map;

	error = bus_dmamap_load_mbuf(sc->sc_dmat, map, m0, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: can't map mbuf (error %d)\n",
		    sc->sc_dev.dv_xname, error);
		return error;
	}

	if (sc->txq.queued + map->dm_nsegs >= NFE_TX_RING_COUNT - 1) {
		bus_dmamap_unload(sc->sc_dmat, map);
		return ENOBUFS;
	}

#if NVLAN > 0
	/* setup h/w VLAN tagging */
	if (m0->m_flags & M_VLANTAG)
		vtag = NFE_TX_VTAG | m0->m_pkthdr.ether_vtag;
#endif
	if (m0->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
		flags |= NFE_TX_IP_CSUM;
	if (m0->m_pkthdr.csum_flags & (M_TCP_CSUM_OUT | M_UDP_CSUM_OUT))
		flags |= NFE_TX_TCP_UDP_CSUM;

	for (i = 0; i < map->dm_nsegs; i++) {
		data = &sc->txq.data[sc->txq.cur];

		if (sc->sc_flags & NFE_40BIT_ADDR) {
			desc64 = &sc->txq.desc64[sc->txq.cur];
#if defined(__LP64__)
			desc64->physaddr[0] =
			    htole32(map->dm_segs[i].ds_addr >> 32);
#endif
			desc64->physaddr[1] =
			    htole32(map->dm_segs[i].ds_addr & 0xffffffff);
			desc64->length = htole16(map->dm_segs[i].ds_len - 1);
			desc64->flags = htole16(flags);
			desc64->vtag = htole32(vtag);
		} else {
			desc32 = &sc->txq.desc32[sc->txq.cur];

			desc32->physaddr = htole32(map->dm_segs[i].ds_addr);
			desc32->length = htole16(map->dm_segs[i].ds_len - 1);
			desc32->flags = htole16(flags);
		}

		if (map->dm_nsegs > 1) {
			/*
			 * Checksum flags and vtag belong to the first fragment
			 * only.
			 */
			flags &= ~(NFE_TX_IP_CSUM | NFE_TX_TCP_UDP_CSUM);
			vtag = 0;

			/*
			 * Setting of the valid bit in the first descriptor is
			 * deferred until the whole chain is fully setup.
			 */
			flags |= NFE_TX_VALID;
		}

		sc->txq.queued++;
		sc->txq.cur = (sc->txq.cur + 1) % NFE_TX_RING_COUNT;
	}

	/* the whole mbuf chain has been setup */
	if (sc->sc_flags & NFE_40BIT_ADDR) {
		/* fix last descriptor */
		flags |= NFE_TX_LASTFRAG_V2;
		desc64->flags = htole16(flags);

		/* finally, set the valid bit in the first descriptor */
		sc->txq.desc64[first].flags |= htole16(NFE_TX_VALID);
	} else {
		/* fix last descriptor */
		if (sc->sc_flags & NFE_JUMBO_SUP)
			flags |= NFE_TX_LASTFRAG_V2;
		else
			flags |= NFE_TX_LASTFRAG_V1;
		desc32->flags = htole16(flags);

		/* finally, set the valid bit in the first descriptor */
		sc->txq.desc32[first].flags |= htole16(NFE_TX_VALID);
	}

	data->m = m0;
	data->active = map;

	bus_dmamap_sync(sc->sc_dmat, map, 0, map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	return 0;
}

void
nfe_start(struct ifnet *ifp)
{
	struct nfe_softc *sc = ifp->if_softc;
	int old = sc->txq.cur;
	struct mbuf *m0;

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;

	for (;;) {
		m0 = ifq_deq_begin(&ifp->if_snd);
		if (m0 == NULL)
			break;

		if (nfe_encap(sc, m0) != 0) {
			ifq_deq_rollback(&ifp->if_snd, m0);
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

		/* packet put in h/w queue, remove from s/w queue */
		ifq_deq_commit(&ifp->if_snd, m0);

#if NBPFILTER > 0
		if (ifp->if_bpf != NULL)
			bpf_mtap_ether(ifp->if_bpf, m0, BPF_DIRECTION_OUT);
#endif
	}
	if (sc->txq.cur == old)	/* nothing sent */
		return;

	if (sc->sc_flags & NFE_40BIT_ADDR)
		nfe_txdesc64_rsync(sc, old, sc->txq.cur, BUS_DMASYNC_PREWRITE);
	else
		nfe_txdesc32_rsync(sc, old, sc->txq.cur, BUS_DMASYNC_PREWRITE);

	/* kick Tx */
	NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_KICKTX | sc->rxtxctl);

	/*
	 * Set a timeout in case the chip goes out to lunch.
	 */
	ifp->if_timer = 5;
}

void
nfe_watchdog(struct ifnet *ifp)
{
	struct nfe_softc *sc = ifp->if_softc;

	printf("%s: watchdog timeout\n", sc->sc_dev.dv_xname);

	nfe_init(ifp);

	ifp->if_oerrors++;
}

int
nfe_init(struct ifnet *ifp)
{
	struct nfe_softc *sc = ifp->if_softc;
	uint32_t tmp;

	nfe_stop(ifp, 0);

	NFE_WRITE(sc, NFE_TX_UNK, 0);
	NFE_WRITE(sc, NFE_STATUS, 0);

	sc->rxtxctl = NFE_RXTX_BIT2;
	if (sc->sc_flags & NFE_40BIT_ADDR)
		sc->rxtxctl |= NFE_RXTX_V3MAGIC;
	else if (sc->sc_flags & NFE_JUMBO_SUP)
		sc->rxtxctl |= NFE_RXTX_V2MAGIC;

	if (sc->sc_flags & NFE_HW_CSUM)
		sc->rxtxctl |= NFE_RXTX_RXCSUM;
	if (ifp->if_capabilities & IFCAP_VLAN_HWTAGGING)
		sc->rxtxctl |= NFE_RXTX_VTAG_INSERT | NFE_RXTX_VTAG_STRIP;

	NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_RESET | sc->rxtxctl);
	DELAY(10);
	NFE_WRITE(sc, NFE_RXTX_CTL, sc->rxtxctl);

	if (ifp->if_capabilities & IFCAP_VLAN_HWTAGGING)
		NFE_WRITE(sc, NFE_VTAG_CTL, NFE_VTAG_ENABLE);
	else
		NFE_WRITE(sc, NFE_VTAG_CTL, 0);

	NFE_WRITE(sc, NFE_SETUP_R6, 0);

	/* set MAC address */
	nfe_set_macaddr(sc, sc->sc_arpcom.ac_enaddr);

	/* tell MAC where rings are in memory */
#ifdef __LP64__
	NFE_WRITE(sc, NFE_RX_RING_ADDR_HI, sc->rxq.physaddr >> 32);
#endif
	NFE_WRITE(sc, NFE_RX_RING_ADDR_LO, sc->rxq.physaddr & 0xffffffff);
#ifdef __LP64__
	NFE_WRITE(sc, NFE_TX_RING_ADDR_HI, sc->txq.physaddr >> 32);
#endif
	NFE_WRITE(sc, NFE_TX_RING_ADDR_LO, sc->txq.physaddr & 0xffffffff);

	NFE_WRITE(sc, NFE_RING_SIZE,
	    (NFE_RX_RING_COUNT - 1) << 16 |
	    (NFE_TX_RING_COUNT - 1));

	NFE_WRITE(sc, NFE_RXBUFSZ, sc->rxq.bufsz);

	/* force MAC to wakeup */
	tmp = NFE_READ(sc, NFE_PWR_STATE);
	NFE_WRITE(sc, NFE_PWR_STATE, tmp | NFE_PWR_WAKEUP);
	DELAY(10);
	tmp = NFE_READ(sc, NFE_PWR_STATE);
	NFE_WRITE(sc, NFE_PWR_STATE, tmp | NFE_PWR_VALID);

#if 1
	/* configure interrupts coalescing/mitigation */
	NFE_WRITE(sc, NFE_IMTIMER, NFE_IM_DEFAULT);
#else
	/* no interrupt mitigation: one interrupt per packet */
	NFE_WRITE(sc, NFE_IMTIMER, 970);
#endif

	NFE_WRITE(sc, NFE_SETUP_R1, NFE_R1_MAGIC);
	NFE_WRITE(sc, NFE_SETUP_R2, NFE_R2_MAGIC);
	NFE_WRITE(sc, NFE_SETUP_R6, NFE_R6_MAGIC);

	/* update MAC knowledge of PHY; generates a NFE_IRQ_LINK interrupt */
	NFE_WRITE(sc, NFE_STATUS, sc->mii_phyaddr << 24 | NFE_STATUS_MAGIC);

	NFE_WRITE(sc, NFE_SETUP_R4, NFE_R4_MAGIC);

	sc->rxtxctl &= ~NFE_RXTX_BIT2;
	NFE_WRITE(sc, NFE_RXTX_CTL, sc->rxtxctl);
	DELAY(10);
	NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_BIT1 | sc->rxtxctl);

	/* program promiscuous mode and multicast filters */
	nfe_iff(sc);

	nfe_ifmedia_upd(ifp);

	/* enable Rx */
	NFE_WRITE(sc, NFE_RX_CTL, NFE_RX_START);

	/* enable Tx */
	NFE_WRITE(sc, NFE_TX_CTL, NFE_TX_START);

	NFE_WRITE(sc, NFE_PHY_STATUS, 0xf);

	/* enable interrupts */
	NFE_WRITE(sc, NFE_IRQ_MASK, NFE_IRQ_WANTED);

	timeout_add_sec(&sc->sc_tick_ch, 1);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	return 0;
}

void
nfe_stop(struct ifnet *ifp, int disable)
{
	struct nfe_softc *sc = ifp->if_softc;

	timeout_del(&sc->sc_tick_ch);

	ifp->if_timer = 0;
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	mii_down(&sc->sc_mii);

	/* abort Tx */
	NFE_WRITE(sc, NFE_TX_CTL, 0);

	if ((sc->sc_flags & NFE_WOL) == 0) {
		/* disable Rx */
		NFE_WRITE(sc, NFE_RX_CTL, 0);

		/* disable interrupts */
		NFE_WRITE(sc, NFE_IRQ_MASK, 0);
	}

	/* reset Tx and Rx rings */
	nfe_reset_tx_ring(sc, &sc->txq);
	nfe_reset_rx_ring(sc, &sc->rxq);
}

int
nfe_alloc_rx_ring(struct nfe_softc *sc, struct nfe_rx_ring *ring)
{
	struct nfe_desc32 *desc32;
	struct nfe_desc64 *desc64;
	struct nfe_rx_data *data;
	void **desc;
	bus_addr_t physaddr;
	int i, nsegs, error, descsize;

	if (sc->sc_flags & NFE_40BIT_ADDR) {
		desc = (void **)&ring->desc64;
		descsize = sizeof (struct nfe_desc64);
	} else {
		desc = (void **)&ring->desc32;
		descsize = sizeof (struct nfe_desc32);
	}

	ring->cur = ring->next = 0;
	ring->bufsz = MCLBYTES;

	error = bus_dmamap_create(sc->sc_dmat, NFE_RX_RING_COUNT * descsize, 1,
	    NFE_RX_RING_COUNT * descsize, 0, BUS_DMA_NOWAIT, &ring->map);
	if (error != 0) {
		printf("%s: could not create desc DMA map\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamem_alloc(sc->sc_dmat, NFE_RX_RING_COUNT * descsize,
	    PAGE_SIZE, 0, &ring->seg, 1, &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error != 0) {
		printf("%s: could not allocate DMA memory\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamem_map(sc->sc_dmat, &ring->seg, nsegs,
	    NFE_RX_RING_COUNT * descsize, (caddr_t *)desc, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: can't map desc DMA memory\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamap_load(sc->sc_dmat, ring->map, *desc,
	    NFE_RX_RING_COUNT * descsize, NULL, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: could not load desc DMA map\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}
	ring->physaddr = ring->map->dm_segs[0].ds_addr;

	/*
	 * Pre-allocate Rx buffers and populate Rx ring.
	 */
	for (i = 0; i < NFE_RX_RING_COUNT; i++) {
		data = &sc->rxq.data[i];

		data->m = MCLGETI(NULL, MCLBYTES, NULL, M_DONTWAIT);
		if (data->m == NULL) {
			printf("%s: could not allocate rx mbuf\n",
			    sc->sc_dev.dv_xname);
			error = ENOMEM;
			goto fail;
		}
		data->m->m_pkthdr.len = data->m->m_len = MCLBYTES;

		error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1,
		    MCLBYTES, 0, BUS_DMA_NOWAIT, &data->map);
		if (error != 0) {
			printf("%s: could not create DMA map\n",
			    sc->sc_dev.dv_xname);
			goto fail;
		}

		error = bus_dmamap_load_mbuf(sc->sc_dmat, data->map, data->m,
		    BUS_DMA_READ | BUS_DMA_NOWAIT);
		if (error != 0) {
			printf("%s: could not load rx buf DMA map",
			    sc->sc_dev.dv_xname);
			goto fail;
		}
		physaddr = data->map->dm_segs[0].ds_addr;

		if (sc->sc_flags & NFE_40BIT_ADDR) {
			desc64 = &sc->rxq.desc64[i];
#if defined(__LP64__)
			desc64->physaddr[0] = htole32(physaddr >> 32);
#endif
			desc64->physaddr[1] = htole32(physaddr & 0xffffffff);
			desc64->length = htole16(sc->rxq.bufsz);
			desc64->flags = htole16(NFE_RX_READY);
		} else {
			desc32 = &sc->rxq.desc32[i];
			desc32->physaddr = htole32(physaddr);
			desc32->length = htole16(sc->rxq.bufsz);
			desc32->flags = htole16(NFE_RX_READY);
		}
	}

	bus_dmamap_sync(sc->sc_dmat, ring->map, 0, ring->map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	return 0;

fail:	nfe_free_rx_ring(sc, ring);
	return error;
}

void
nfe_reset_rx_ring(struct nfe_softc *sc, struct nfe_rx_ring *ring)
{
	int i;

	for (i = 0; i < NFE_RX_RING_COUNT; i++) {
		if (sc->sc_flags & NFE_40BIT_ADDR) {
			ring->desc64[i].length = htole16(ring->bufsz);
			ring->desc64[i].flags = htole16(NFE_RX_READY);
		} else {
			ring->desc32[i].length = htole16(ring->bufsz);
			ring->desc32[i].flags = htole16(NFE_RX_READY);
		}
	}

	bus_dmamap_sync(sc->sc_dmat, ring->map, 0, ring->map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	ring->cur = ring->next = 0;
}

void
nfe_free_rx_ring(struct nfe_softc *sc, struct nfe_rx_ring *ring)
{
	struct nfe_rx_data *data;
	void *desc;
	int i, descsize;

	if (sc->sc_flags & NFE_40BIT_ADDR) {
		desc = ring->desc64;
		descsize = sizeof (struct nfe_desc64);
	} else {
		desc = ring->desc32;
		descsize = sizeof (struct nfe_desc32);
	}

	if (desc != NULL) {
		bus_dmamap_sync(sc->sc_dmat, ring->map, 0,
		    ring->map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, ring->map);
		bus_dmamem_unmap(sc->sc_dmat, (caddr_t)desc,
		    NFE_RX_RING_COUNT * descsize);
		bus_dmamem_free(sc->sc_dmat, &ring->seg, 1);
	}

	for (i = 0; i < NFE_RX_RING_COUNT; i++) {
		data = &ring->data[i];

		if (data->map != NULL) {
			bus_dmamap_sync(sc->sc_dmat, data->map, 0,
			    data->map->dm_mapsize, BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(sc->sc_dmat, data->map);
			bus_dmamap_destroy(sc->sc_dmat, data->map);
		}
		m_freem(data->m);
	}
}

int
nfe_alloc_tx_ring(struct nfe_softc *sc, struct nfe_tx_ring *ring)
{
	int i, nsegs, error;
	void **desc;
	int descsize;

	if (sc->sc_flags & NFE_40BIT_ADDR) {
		desc = (void **)&ring->desc64;
		descsize = sizeof (struct nfe_desc64);
	} else {
		desc = (void **)&ring->desc32;
		descsize = sizeof (struct nfe_desc32);
	}

	ring->queued = 0;
	ring->cur = ring->next = 0;

	error = bus_dmamap_create(sc->sc_dmat, NFE_TX_RING_COUNT * descsize, 1,
	    NFE_TX_RING_COUNT * descsize, 0, BUS_DMA_NOWAIT, &ring->map);

	if (error != 0) {
		printf("%s: could not create desc DMA map\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamem_alloc(sc->sc_dmat, NFE_TX_RING_COUNT * descsize,
	    PAGE_SIZE, 0, &ring->seg, 1, &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error != 0) {
		printf("%s: could not allocate DMA memory\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamem_map(sc->sc_dmat, &ring->seg, nsegs,
	    NFE_TX_RING_COUNT * descsize, (caddr_t *)desc, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: can't map desc DMA memory\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamap_load(sc->sc_dmat, ring->map, *desc,
	    NFE_TX_RING_COUNT * descsize, NULL, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: could not load desc DMA map\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}
	ring->physaddr = ring->map->dm_segs[0].ds_addr;

	for (i = 0; i < NFE_TX_RING_COUNT; i++) {
		error = bus_dmamap_create(sc->sc_dmat, NFE_JBYTES,
		    NFE_MAX_SCATTER, NFE_JBYTES, 0, BUS_DMA_NOWAIT,
		    &ring->data[i].map);
		if (error != 0) {
			printf("%s: could not create DMA map\n",
			    sc->sc_dev.dv_xname);
			goto fail;
		}
	}

	return 0;

fail:	nfe_free_tx_ring(sc, ring);
	return error;
}

void
nfe_reset_tx_ring(struct nfe_softc *sc, struct nfe_tx_ring *ring)
{
	struct nfe_tx_data *data;
	int i;

	for (i = 0; i < NFE_TX_RING_COUNT; i++) {
		if (sc->sc_flags & NFE_40BIT_ADDR)
			ring->desc64[i].flags = 0;
		else
			ring->desc32[i].flags = 0;

		data = &ring->data[i];

		if (data->m != NULL) {
			bus_dmamap_sync(sc->sc_dmat, data->active, 0,
			    data->active->dm_mapsize, BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, data->active);
			m_freem(data->m);
			data->m = NULL;
		}
	}

	bus_dmamap_sync(sc->sc_dmat, ring->map, 0, ring->map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	ring->queued = 0;
	ring->cur = ring->next = 0;
}

void
nfe_free_tx_ring(struct nfe_softc *sc, struct nfe_tx_ring *ring)
{
	struct nfe_tx_data *data;
	void *desc;
	int i, descsize;

	if (sc->sc_flags & NFE_40BIT_ADDR) {
		desc = ring->desc64;
		descsize = sizeof (struct nfe_desc64);
	} else {
		desc = ring->desc32;
		descsize = sizeof (struct nfe_desc32);
	}

	if (desc != NULL) {
		bus_dmamap_sync(sc->sc_dmat, ring->map, 0,
		    ring->map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, ring->map);
		bus_dmamem_unmap(sc->sc_dmat, (caddr_t)desc,
		    NFE_TX_RING_COUNT * descsize);
		bus_dmamem_free(sc->sc_dmat, &ring->seg, 1);
	}

	for (i = 0; i < NFE_TX_RING_COUNT; i++) {
		data = &ring->data[i];

		if (data->m != NULL) {
			bus_dmamap_sync(sc->sc_dmat, data->active, 0,
			    data->active->dm_mapsize, BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, data->active);
			m_freem(data->m);
		}
	}

	/* ..and now actually destroy the DMA mappings */
	for (i = 0; i < NFE_TX_RING_COUNT; i++) {
		data = &ring->data[i];
		if (data->map == NULL)
			continue;
		bus_dmamap_destroy(sc->sc_dmat, data->map);
	}
}

int
nfe_ifmedia_upd(struct ifnet *ifp)
{
	struct nfe_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_mii;
	struct mii_softc *miisc;

	if (mii->mii_instance != 0) {
		LIST_FOREACH(miisc, &mii->mii_phys, mii_list)
			mii_phy_reset(miisc);
	}
	return mii_mediachg(mii);
}

void
nfe_ifmedia_sts(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct nfe_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_mii;

	mii_pollstat(mii);
	ifmr->ifm_status = mii->mii_media_status;
	ifmr->ifm_active = mii->mii_media_active;
}

void
nfe_iff(struct nfe_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct arpcom *ac = &sc->sc_arpcom;
	struct ether_multi *enm;
	struct ether_multistep step;
	uint8_t addr[ETHER_ADDR_LEN], mask[ETHER_ADDR_LEN];
	uint32_t filter;
	int i;

	filter = NFE_RXFILTER_MAGIC;
	ifp->if_flags &= ~IFF_ALLMULTI;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0) {
		ifp->if_flags |= IFF_ALLMULTI;
		if (ifp->if_flags & IFF_PROMISC)
			filter |= NFE_PROMISC;
		else
			filter |= NFE_U2M;
		bzero(addr, ETHER_ADDR_LEN);
		bzero(mask, ETHER_ADDR_LEN);
	} else {
		filter |= NFE_U2M;

		bcopy(etherbroadcastaddr, addr, ETHER_ADDR_LEN);
		bcopy(etherbroadcastaddr, mask, ETHER_ADDR_LEN);

		ETHER_FIRST_MULTI(step, ac, enm);
		while (enm != NULL) {
			for (i = 0; i < ETHER_ADDR_LEN; i++) {
				addr[i] &=  enm->enm_addrlo[i];
				mask[i] &= ~enm->enm_addrlo[i];
			}

			ETHER_NEXT_MULTI(step, enm);
		}

		for (i = 0; i < ETHER_ADDR_LEN; i++)
			mask[i] |= addr[i];
	}

	addr[0] |= 0x01;	/* make sure multicast bit is set */

	NFE_WRITE(sc, NFE_MULTIADDR_HI,
	    addr[3] << 24 | addr[2] << 16 | addr[1] << 8 | addr[0]);
	NFE_WRITE(sc, NFE_MULTIADDR_LO,
	    addr[5] <<  8 | addr[4]);
	NFE_WRITE(sc, NFE_MULTIMASK_HI,
	    mask[3] << 24 | mask[2] << 16 | mask[1] << 8 | mask[0]);
	NFE_WRITE(sc, NFE_MULTIMASK_LO,
	    mask[5] <<  8 | mask[4]);
	NFE_WRITE(sc, NFE_RXFILTER, filter);
}

void
nfe_get_macaddr(struct nfe_softc *sc, uint8_t *addr)
{
	uint32_t tmp;

	if (sc->sc_flags & NFE_CORRECT_MACADDR) {
		tmp = NFE_READ(sc, NFE_MACADDR_HI);
		addr[0] = (tmp & 0xff);
		addr[1] = (tmp >>  8) & 0xff;
		addr[2] = (tmp >> 16) & 0xff;
		addr[3] = (tmp >> 24) & 0xff;

		tmp = NFE_READ(sc, NFE_MACADDR_LO);
		addr[4] = (tmp & 0xff);
		addr[5] = (tmp >> 8) & 0xff;

	} else {
		tmp = NFE_READ(sc, NFE_MACADDR_LO);
		addr[0] = (tmp >> 8) & 0xff;
		addr[1] = (tmp & 0xff);

		tmp = NFE_READ(sc, NFE_MACADDR_HI);
		addr[2] = (tmp >> 24) & 0xff;
		addr[3] = (tmp >> 16) & 0xff;
		addr[4] = (tmp >>  8) & 0xff;
		addr[5] = (tmp & 0xff);
	}
}

void
nfe_set_macaddr(struct nfe_softc *sc, const uint8_t *addr)
{
	NFE_WRITE(sc, NFE_MACADDR_LO,
	    addr[5] <<  8 | addr[4]);
	NFE_WRITE(sc, NFE_MACADDR_HI,
	    addr[3] << 24 | addr[2] << 16 | addr[1] << 8 | addr[0]);
}

void
nfe_tick(void *arg)
{
	struct nfe_softc *sc = arg;
	int s;

	s = splnet();
	mii_tick(&sc->sc_mii);
	splx(s);

	timeout_add_sec(&sc->sc_tick_ch, 1);
}

#ifndef SMALL_KERNEL
int
nfe_wol(struct ifnet *ifp, int enable)
{
	struct nfe_softc *sc = ifp->if_softc;

	if (enable) {
		sc->sc_flags |= NFE_WOL;
		NFE_WRITE(sc, NFE_WOL_CTL, NFE_WOL_ENABLE);
	} else {
		sc->sc_flags &= ~NFE_WOL;
		NFE_WRITE(sc, NFE_WOL_CTL, 0);
	}

	return 0;
}
#endif
@


1.118
log
@m_free() and m_freem() test for NULL.  Simplify callers which had their own
NULL tests.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.117 2016/04/13 10:34:32 mpi Exp $	*/
d820 1
a820 2
			} else
				ifp->if_opackets++;
d829 1
a829 2
			} else
				ifp->if_opackets++;
@


1.117
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.116 2015/11/25 03:09:59 dlg Exp $	*/
d1321 1
a1321 2
		if (data->m != NULL)
			m_freem(data->m);
@


1.116
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.115 2015/11/24 17:11:39 mpi Exp $	*/
a324 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.115
log
@You only need <net/if_dl.h> if you're using LLADDR() or a sockaddr_dl.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.114 2015/11/20 03:35:23 dlg Exp $	*/
d855 1
a855 1
		ifp->if_flags &= ~IFF_OACTIVE;
d972 1
a972 1
	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
d982 1
a982 1
			ifp->if_flags |= IFF_OACTIVE;
d1123 1
a1123 1
	ifp->if_flags &= ~IFF_OACTIVE;
d1136 2
a1137 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
@


1.114
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.113 2015/11/14 17:54:57 mpi Exp $	*/
a39 1
#include <net/if_dl.h>
@


1.113
log
@Do not include <net/if_vlan_var.h> when it's not necessary.

Because of the VLAN hacks in mpw(4) this file still contains the definition
of "struct ifvlan" which depends on <sys/refcnt.h> which in turns pull
<sys/atomic.h>...
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.112 2015/10/25 13:04:28 mpi Exp $	*/
d977 1
a977 1
		IFQ_POLL(&ifp->if_snd, m0);
d982 1
d988 1
a988 1
		IFQ_DEQUEUE(&ifp->if_snd, m0);
@


1.112
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.111 2015/06/24 09:40:54 mpi Exp $	*/
a44 5

#if NVLAN > 0
#include <net/if_types.h>
#include <net/if_vlan_var.h>
#endif
@


1.111
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.110 2015/03/20 18:42:25 mpi Exp $	*/
a521 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a531 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->sc_arpcom, ifa);
@


1.110
log
@Convert to if_input(), thanks to krw@@ for testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.109 2015/03/14 03:38:48 jsg Exp $	*/
a765 1
		ifp->if_ipackets++;
@


1.109
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.108 2014/12/22 02:28:52 tedu Exp $	*/
d649 1
a748 1
		m->m_pkthdr.rcvif = ifp;
a765 4
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_IN);
#endif
d767 1
a767 1
		ether_input_mbuf(ifp, m);
d793 1
@


1.108
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.107 2014/08/20 23:56:57 dlg Exp $	*/
a54 1
#include <dev/mii/mii.h>
@


1.107
log
@after allocating an mbuf and cluster you still need to init the length
fields.

found by steven roberts, who also tested this fix for me
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.106 2014/08/20 01:02:50 dlg Exp $	*/
a42 1
#ifdef INET
a44 1
#endif
a533 1
#ifdef INET
a535 1
#endif
@


1.106
log
@remove the custom jumbo allocator. its never been enabled or used.

putting this into the tree to make it easier to test.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.105 2014/07/22 13:12:11 mpi Exp $	*/
d720 1
d1240 1
@


1.105
log
@Fewer <netinet/in_systm.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.104 2013/12/28 03:34:54 deraadt Exp $	*/
a87 4
struct	nfe_jbuf *nfe_jalloc(struct nfe_softc *);
void	nfe_jfree(caddr_t, u_int, void *);
int	nfe_jpool_alloc(struct nfe_softc *);
void	nfe_jpool_free(struct nfe_softc *);
a307 6
#ifdef notyet
	/* enable jumbo frames for adapters that support it */
	if (sc->sc_flags & NFE_JUMBO_SUP)
		sc->sc_flags |= NFE_USE_JUMBO;
#endif

a344 3
	if (sc->sc_flags & NFE_USE_JUMBO)
		ifp->if_hardmtu = NFE_JUMBO_MTU;

a653 1
	struct nfe_jbuf *jbuf;
d715 1
a715 1
		MGETHDR(mnew, M_DONTWAIT, MT_DATA);
d721 12
a732 28
		if (sc->sc_flags & NFE_USE_JUMBO) {
			if ((jbuf = nfe_jalloc(sc)) == NULL) {
				m_freem(mnew);
				ifp->if_ierrors++;
				goto skip;
			}
			MEXTADD(mnew, jbuf->buf, NFE_JBYTES, 0, nfe_jfree, sc);

			bus_dmamap_sync(sc->sc_dmat, sc->rxq.jmap,
			    mtod(data->m, caddr_t) - sc->rxq.jpool, NFE_JBYTES,
			    BUS_DMASYNC_POSTREAD);

			physaddr = jbuf->physaddr;
		} else {
			MCLGET(mnew, M_DONTWAIT);
			if (!(mnew->m_flags & M_EXT)) {
				m_freem(mnew);
				ifp->if_ierrors++;
				goto skip;
			}

			bus_dmamap_sync(sc->sc_dmat, data->map, 0,
			    data->map->dm_mapsize, BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(sc->sc_dmat, data->map);

			error = bus_dmamap_load(sc->sc_dmat, data->map,
			    mtod(mnew, void *), MCLBYTES, NULL,
			    BUS_DMA_READ | BUS_DMA_NOWAIT);
d734 3
a736 13
				m_freem(mnew);

				/* try to reload the old mbuf */
				error = bus_dmamap_load(sc->sc_dmat, data->map,
				    mtod(data->m, void *), MCLBYTES, NULL,
				    BUS_DMA_READ | BUS_DMA_NOWAIT);
				if (error != 0) {
					/* very unlikely that it will fail.. */
					panic("%s: could not load old rx mbuf",
					    sc->sc_dev.dv_xname);
				}
				ifp->if_ierrors++;
				goto skip;
d738 2
a739 1
			physaddr = data->map->dm_segs[0].ds_addr;
d741 1
a1177 1
	struct nfe_jbuf *jbuf;
a1225 9
	if (sc->sc_flags & NFE_USE_JUMBO) {
		ring->bufsz = NFE_JBYTES;
		if ((error = nfe_jpool_alloc(sc)) != 0) {
			printf("%s: could not allocate jumbo frames\n",
			    sc->sc_dev.dv_xname);
			goto fail;
		}
	}

d1232 1
a1232 1
		MGETHDR(data->m, M_DONTWAIT, MT_DATA);
d1240 7
a1246 8
		if (sc->sc_flags & NFE_USE_JUMBO) {
			if ((jbuf = nfe_jalloc(sc)) == NULL) {
				printf("%s: could not allocate jumbo buffer\n",
				    sc->sc_dev.dv_xname);
				goto fail;
			}
			MEXTADD(data->m, jbuf->buf, NFE_JBYTES, 0, nfe_jfree,
			    sc);
d1248 6
a1253 26
			physaddr = jbuf->physaddr;
		} else {
			error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1,
			    MCLBYTES, 0, BUS_DMA_NOWAIT, &data->map);
			if (error != 0) {
				printf("%s: could not create DMA map\n",
				    sc->sc_dev.dv_xname);
				goto fail;
			}
			MCLGET(data->m, M_DONTWAIT);
			if (!(data->m->m_flags & M_EXT)) {
				printf("%s: could not allocate mbuf cluster\n",
				    sc->sc_dev.dv_xname);
				error = ENOMEM;
				goto fail;
			}

			error = bus_dmamap_load(sc->sc_dmat, data->map,
			    mtod(data->m, void *), MCLBYTES, NULL,
			    BUS_DMA_READ | BUS_DMA_NOWAIT);
			if (error != 0) {
				printf("%s: could not load rx buf DMA map",
				    sc->sc_dev.dv_xname);
				goto fail;
			}
			physaddr = data->map->dm_segs[0].ds_addr;
d1255 1
a1337 121
	}
}

struct nfe_jbuf *
nfe_jalloc(struct nfe_softc *sc)
{
	struct nfe_jbuf *jbuf;

	jbuf = SLIST_FIRST(&sc->rxq.jfreelist);
	if (jbuf == NULL)
		return NULL;
	SLIST_REMOVE_HEAD(&sc->rxq.jfreelist, jnext);
	return jbuf;
}

/*
 * This is called automatically by the network stack when the mbuf is freed.
 * Caution must be taken that the NIC might be reset by the time the mbuf is
 * freed.
 */
void
nfe_jfree(caddr_t buf, u_int size, void *arg)
{
	struct nfe_softc *sc = arg;
	struct nfe_jbuf *jbuf;
	int i;

	/* find the jbuf from the base pointer */
	i = (buf - sc->rxq.jpool) / NFE_JBYTES;
	if (i < 0 || i >= NFE_JPOOL_COUNT) {
		printf("%s: request to free a buffer (%p) not managed by us\n",
		    sc->sc_dev.dv_xname, buf);
		return;
	}
	jbuf = &sc->rxq.jbuf[i];

	/* ..and put it back in the free list */
	SLIST_INSERT_HEAD(&sc->rxq.jfreelist, jbuf, jnext);
}

int
nfe_jpool_alloc(struct nfe_softc *sc)
{
	struct nfe_rx_ring *ring = &sc->rxq;
	struct nfe_jbuf *jbuf;
	bus_addr_t physaddr;
	caddr_t buf;
	int i, nsegs, error;

	/*
	 * Allocate a big chunk of DMA'able memory.
	 */
	error = bus_dmamap_create(sc->sc_dmat, NFE_JPOOL_SIZE, 1,
	    NFE_JPOOL_SIZE, 0, BUS_DMA_NOWAIT, &ring->jmap);
	if (error != 0) {
		printf("%s: could not create jumbo DMA map\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamem_alloc(sc->sc_dmat, NFE_JPOOL_SIZE, PAGE_SIZE, 0,
	    &ring->jseg, 1, &nsegs, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s could not allocate jumbo DMA memory\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamem_map(sc->sc_dmat, &ring->jseg, nsegs, NFE_JPOOL_SIZE,
	    &ring->jpool, BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: can't map jumbo DMA memory\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	error = bus_dmamap_load(sc->sc_dmat, ring->jmap, ring->jpool,
	    NFE_JPOOL_SIZE, NULL, BUS_DMA_READ | BUS_DMA_NOWAIT);
	if (error != 0) {
		printf("%s: could not load jumbo DMA map\n",
		    sc->sc_dev.dv_xname);
		goto fail;
	}

	/* ..and split it into 9KB chunks */
	SLIST_INIT(&ring->jfreelist);

	buf = ring->jpool;
	physaddr = ring->jmap->dm_segs[0].ds_addr;
	for (i = 0; i < NFE_JPOOL_COUNT; i++) {
		jbuf = &ring->jbuf[i];

		jbuf->buf = buf;
		jbuf->physaddr = physaddr;

		SLIST_INSERT_HEAD(&ring->jfreelist, jbuf, jnext);

		buf += NFE_JBYTES;
		physaddr += NFE_JBYTES;
	}

	return 0;

fail:	nfe_jpool_free(sc);
	return error;
}

void
nfe_jpool_free(struct nfe_softc *sc)
{
	struct nfe_rx_ring *ring = &sc->rxq;

	if (ring->jmap != NULL) {
		bus_dmamap_sync(sc->sc_dmat, ring->jmap, 0,
		    ring->jmap->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, ring->jmap);
		bus_dmamap_destroy(sc->sc_dmat, ring->jmap);
	}
	if (ring->jpool != NULL) {
		bus_dmamem_unmap(sc->sc_dmat, ring->jpool, NFE_JPOOL_SIZE);
		bus_dmamem_free(sc->sc_dmat, &ring->jseg, 1);
@


1.104
log
@The few network drivers that called their children's (ie. mii PHY
drivers) activate functions at DVACT_RESUME time do not need to do
so, since their PHYs are repaired by IFF_UP.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.103 2013/12/06 21:03:04 deraadt Exp $	*/
a44 2
#include <netinet/in_systm.h>
#include <netinet/ip.h>
@


1.103
log
@Add a DVACT_WAKEUP op to the *_activate() API.  This is called after the
kernel resumes normal (non-cold, able to run processes, etc) operation.
Previously we were relying on specific DVACT_RESUME op's in drivers
creating callback/threads themselves, but that has become too common,
indicating the need for a built-in mechanism.
ok dlg kettenis, tested by a sufficient amount of people
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.102 2013/08/07 01:06:36 bluhm Exp $	*/
a191 1
		rv = config_activate_children(self, act);
@


1.102
log
@Most network drivers include netinet/in_var.h, but apparently they
don't have to.  Just remove these include lines.
Compiled on amd64 i386 sparc64; OK henning@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.101 2013/04/01 06:40:40 brad Exp $	*/
a185 3
	case DVACT_QUIESCE:
		rv = config_activate_children(self, act);
		break;
d195 3
@


1.101
log
@Rewrite receive filter and ioctl handling code.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.100 2012/11/29 21:10:32 brad Exp $	*/
a45 1
#include <netinet/in_var.h>
@


1.100
log
@Remove setting an initial assumed baudrate upon driver attach which is not
necessarily correct, there might not even be a link when attaching.

ok mikeb@@ reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.99 2012/08/31 12:41:17 stsp Exp $	*/
d103 1
a103 1
void	nfe_setmulti(struct nfe_softc *);
d561 4
a564 13
			/*
			 * If only the PROMISC or ALLMULTI flag changes, then
			 * don't do a full re-init of the chip, just update
			 * the Rx filter.
			 */
			if ((ifp->if_flags & IFF_RUNNING) &&
			    ((ifp->if_flags ^ sc->sc_if_flags) &
			     (IFF_ALLMULTI | IFF_PROMISC)) != 0) {
				nfe_setmulti(sc);
			} else {
				if (!(ifp->if_flags & IFF_RUNNING))
					nfe_init(ifp);
			}
a568 1
		sc->sc_if_flags = ifp->if_flags;
d582 1
a582 1
			nfe_setmulti(sc);
d1162 2
a1163 2
	/* set Rx filter */
	nfe_setmulti(sc);
d1703 1
a1703 1
nfe_setmulti(struct nfe_softc *sc)
d1705 1
a1706 1
	struct ifnet *ifp = &ac->ac_if;
d1710 1
a1710 1
	uint32_t filter = NFE_RXFILTER_MAGIC;
d1713 9
a1721 1
	if ((ifp->if_flags & (IFF_ALLMULTI | IFF_PROMISC)) != 0) {
d1724 5
a1728 2
		goto done;
	}
d1730 6
a1735 2
	bcopy(etherbroadcastaddr, addr, ETHER_ADDR_LEN);
	bcopy(etherbroadcastaddr, mask, ETHER_ADDR_LEN);
d1737 1
a1737 7
	ETHER_FIRST_MULTI(step, ac, enm);
	while (enm != NULL) {
		if (bcmp(enm->enm_addrlo, enm->enm_addrhi, ETHER_ADDR_LEN)) {
			ifp->if_flags |= IFF_ALLMULTI;
			bzero(addr, ETHER_ADDR_LEN);
			bzero(mask, ETHER_ADDR_LEN);
			goto done;
d1739 3
a1741 5
		for (i = 0; i < ETHER_ADDR_LEN; i++) {
			addr[i] &=  enm->enm_addrlo[i];
			mask[i] &= ~enm->enm_addrlo[i];
		}
		ETHER_NEXT_MULTI(step, enm);
a1742 2
	for (i = 0; i < ETHER_ADDR_LEN; i++)
		mask[i] |= addr[i];
a1743 1
done:
a1753 2

	filter |= (ifp->if_flags & IFF_PROMISC) ? NFE_PROMISC : NFE_U2M;
@


1.99
log
@Wake on LAN support for nfe(4). Tested by me, russel on misc@@, and jsg.
ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.98 2011/04/05 18:01:21 henning Exp $	*/
a346 1
	ifp->if_baudrate = IF_Gbps(1);
@


1.98
log
@mechanic rename M_{TCP|UDP}V4_CSUM_OUT -> M_{TCP|UDP}_CSUM_OUT
ok claudio krw
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.97 2011/01/10 16:18:03 kettenis Exp $	*/
d107 3
d354 6
a1166 1
	NFE_WRITE(sc, NFE_WOL_CTL, NFE_WOL_ENABLE);
d1212 3
a1214 2
	/* disable Rx */
	NFE_WRITE(sc, NFE_RX_CTL, 0);
d1216 3
a1218 2
	/* disable interrupts */
	NFE_WRITE(sc, NFE_IRQ_MASK, 0);
d1816 18
@


1.97
log
@Some nfe(4)/rlphy(4) combos don't work, because the PHY responds to all
addresses on the mii bus.  As a countereasure, only attach the first PHY we
encounter.  It is very unlikely we're going to ever see nfe(4) with multiple
PHYs.  The same is probably true for any modern NIC.

ok mikeb@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.96 2010/09/07 16:21:45 deraadt Exp $	*/
d953 1
a953 1
	if (m0->m_pkthdr.csum_flags & (M_TCPV4_CSUM_OUT | M_UDPV4_CSUM_OUT))
@


1.96
log
@remove the powerhook code.  All architectures now use the ca_activate tree
traversal code to suspend/resume
ok oga kettenis blambert
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.95 2010/08/31 17:13:44 deraadt Exp $	*/
d371 1
a371 2
	mii_attach(self, &sc->sc_mii, 0xffffffff, MII_PHY_ANY,
	    MII_OFFSET_ANY, 0);
@


1.95
log
@Add DVACT_QUIECE support.  This is called before splhigh() and before
DVACT_SUSPEND, therefore DVACT_QUIECE can do standard sleeping operations
to get ready.
Discussed quite a while back with kettenis and jakemsr, oga suddenly needed
it as well and wrote half of it, so it was time to finish it.
proofread by miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.94 2010/08/31 16:27:09 deraadt Exp $	*/
a72 1
void	nfe_powerhook(int, void *);
a384 8

	sc->sc_powerhook = powerhook_establish(nfe_powerhook, sc);
}

void
nfe_powerhook(int why, void *arg)
{
	nfe_activate(arg, why);
@


1.94
log
@activate function should return result of config_activate_children
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.93 2010/08/27 19:56:23 deraadt Exp $	*/
d185 3
@


1.93
log
@Make the powerhook use the activate functions, which actually do the full
job.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.92 2010/08/27 17:08:00 jsg Exp $	*/
d182 1
d188 1
a188 1
		config_activate_children(self, act);
d191 1
a191 1
		config_activate_children(self, act);
d196 1
a196 1
	return (0);
@


1.92
log
@remove the unused if_init callback in struct ifnet
ok deraadt@@ henning@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.91 2010/08/06 03:02:24 mlarkin Exp $	*/
d73 1
a73 1
void	nfe_power(int, void *);
d383 1
a383 1
	sc->sc_powerhook = powerhook_establish(nfe_power, sc);
d387 1
a387 1
nfe_power(int why, void *arg)
d389 1
a389 11
	struct nfe_softc *sc = arg;
	struct ifnet *ifp;

	if (why == PWR_RESUME) {
		ifp = &sc->sc_arpcom.ac_if;
		if (ifp->if_flags & IFF_UP) {
			nfe_init(ifp);
			if (ifp->if_flags & IFF_RUNNING)
				nfe_start(ifp);
		}
	}
@


1.91
log
@

ca_activate function for nfe(4) for suspend/resume
tested on nvidia mcp51

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.90 2010/05/19 15:27:35 oga Exp $	*/
a340 1
	ifp->if_init = nfe_init;
@


1.90
log
@BUS_DMA_ZERO instead of alloc, map, bzero.

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.89 2009/06/18 08:19:03 jsg Exp $	*/
d72 1
d110 2
a111 1
	sizeof (struct nfe_softc), nfe_match, nfe_attach
d176 22
@


1.89
log
@Remove support for cancelled NVIDIA MCP7B and add initial support for
MCP89.

From Brad based on information from Peer Chen @@ NVIDIA via Linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.88 2009/03/29 21:53:52 sthen Exp $	*/
d1239 1
a1239 1
	    PAGE_SIZE, 0, &ring->seg, 1, &nsegs, BUS_DMA_NOWAIT);
a1260 2

	bzero(*desc, NFE_RX_RING_COUNT * descsize);
d1556 1
a1556 1
	    PAGE_SIZE, 0, &ring->seg, 1, &nsegs, BUS_DMA_NOWAIT);
a1577 2

	bzero(*desc, NFE_TX_RING_COUNT * descsize);
@


1.88
log
@make various strings ("can't map mem space" and similar) more consistent
between instances, saving space in the kernel. feedback from many (some
incorporated, some left for future work).

ok deraadt, kettenis, "why not" miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.87 2008/11/28 02:44:18 brad Exp $	*/
d164 2
a165 1
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP79_LAN4 }
d251 1
@


1.87
log
@Eliminate the redundant bits of code for MTU and multicast handling
from the individual drivers now that ether_ioctl() handles this.

Shrinks the i386 kernels by..
RAMDISK - 2176 bytes
RAMDISKB - 1504 bytes
RAMDISKC - 736 bytes

Tested by naddy@@/okan@@/sthen@@/brad@@/todd@@/jmc@@ and lots of users.
Build tested on almost all archs by todd@@/brad@@

ok naddy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.86 2008/11/09 15:08:26 naddy Exp $	*/
d189 1
a189 1
		printf(": could not map mem space\n");
d194 1
a194 1
		printf(": could not map interrupt\n");
d927 1
a927 1
		printf("%s: could not map mbuf (error %d)\n",
d1247 1
a1247 1
		printf("%s: could not map desc DMA memory\n",
d1475 1
a1475 1
		printf("%s: could not map jumbo DMA memory\n",
d1566 1
a1566 1
		printf("%s: could not map desc DMA memory\n",
@


1.86
log
@Introduce bpf_mtap_ether(), which for the benefit of bpf listeners
creates the VLAN encapsulation from the tag stored in the mbuf
header.  Idea from FreeBSD, input from claudio@@ and canacar@@.

Switch all hardware VLAN enabled drivers to the new function.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.85 2008/10/28 05:53:20 brad Exp $	*/
d525 1
a526 1
	struct ifaddr *ifa = (struct ifaddr *)data;
d541 1
a541 6
	case SIOCSIFMTU:
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ifp->if_hardmtu)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu)
			ifp->if_mtu = ifr->ifr_mtu;
		break;
a562 5
	case SIOCADDMULTI:
	case SIOCDELMULTI:
		error = (cmd == SIOCADDMULTI) ?
		    ether_addmulti(ifr, &sc->sc_arpcom) :
		    ether_delmulti(ifr, &sc->sc_arpcom);
a563 6
		if (error == ENETRESET) {
			if (ifp->if_flags & IFF_RUNNING)
				nfe_setmulti(sc);
			error = 0;
		}
		break;
d568 1
d571 6
@


1.85
log
@Remove #if NVLAN.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.84 2008/10/28 05:09:43 brad Exp $	*/
d816 1
a816 1
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
d1051 1
a1051 1
			bpf_mtap(ifp->if_bpf, m0, BPF_DIRECTION_OUT);
@


1.84
log
@Re-add support for RX VLAN tag stripping.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.83 2008/10/16 19:18:03 naddy Exp $	*/
d1109 1
a1109 2
#if NVLAN
	if (sc->sc_flags & NFE_HW_VLAN)
d1111 2
a1112 1
#endif
@


1.83
log
@Switch the existing TX VLAN hardware support over to having the
tag in the header.  Convert TX tagging in the drivers.

Help and ok brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.82 2008/10/02 20:21:14 brad Exp $	*/
d330 1
d676 3
d691 3
d807 7
a928 1
#if NVLAN > 0
a929 1
#endif
d949 1
a949 1
		vtag = NFE_TX_VTAG | htons(m0->m_pkthdr.ether_vtag);
a968 1
#if NVLAN > 0
a969 1
#endif
a983 1
#if NVLAN > 0
d985 1
a985 1
#endif
d1099 1
d1102 3
a1104 9
#if NVLAN > 0
	/*
	 * Although the adapter is capable of stripping VLAN tags from received
	 * frames (NFE_RXTX_VTAG_STRIP), we do not enable this functionality on
	 * purpose.  This will be done in software by our network stack.
	 */
	if (sc->sc_flags & NFE_HW_VLAN)
		sc->rxtxctl |= NFE_RXTX_VTAG_INSERT;
#endif
@


1.82
log
@First step towards cleaning up the Ethernet driver ioctl handling.
Move calling ether_ioctl() from the top of the ioctl function, which
at the moment does absolutely nothing, to the default switch case.
Thus allowing drivers to define their own ioctl handlers and then
falling back on ether_ioctl(). The only functional change this results
in at the moment is having all Ethernet drivers returning the proper
errno of ENOTTY instead of EINVAL/ENXIO when encountering unknown
ioctl's.

Shrinks the i386 kernels by..
RAMDISK - 1024 bytes
RAMDISKB -  1120 bytes
RAMDISKC - 832 bytes

Tested by martin@@/jsing@@/todd@@/brad@@
Build tested on almost all archs by todd@@/brad@@

ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.81 2008/09/10 14:01:22 blambert Exp $	*/
d936 2
a937 5
	if ((m0->m_flags & (M_PROTO1 | M_PKTHDR)) == (M_PROTO1 | M_PKTHDR) &&
	    m0->m_pkthdr.rcvif != NULL) {
		struct ifvlan *ifv = m0->m_pkthdr.rcvif->if_softc;
		vtag = NFE_TX_VTAG | htons(ifv->ifv_tag);
	}
@


1.81
log
@Convert timeout_add() calls using multiples of hz to timeout_add_sec()

Really just the low-hanging fruit of (hopefully) forthcoming timeout
conversions.

ok art@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.80 2008/08/09 21:00:52 brad Exp $	*/
a529 5
	if ((error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data)) > 0) {
		splx(s);
		return error;
	}

d584 1
a584 1
		error = ENOTTY;
a587 1

@


1.80
log
@MCP79 are also capable of Jumbo frames. Add the Jumbo support flag.

From: Linux forcedeth
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.79 2008/05/23 08:49:27 brad Exp $	*/
d1186 1
a1186 1
	timeout_add(&sc->sc_tick_ch, hz);
d1813 1
a1813 1
	timeout_add(&sc->sc_tick_ch, hz);
@


1.79
log
@Simplify the combination use of pci_mapreg_type()/pci_mapreg_map() as
suggested by dlg@@ awhile ago.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.78 2008/05/19 01:12:41 fgsch Exp $	*/
d243 3
d250 1
a250 1
		sc->sc_flags |= NFE_40BIT_ADDR | NFE_HW_CSUM |
@


1.78
log
@correct format after %x -> %b convertion. deraadt jsg ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.77 2008/02/05 16:52:50 brad Exp $	*/
d187 2
a188 8
	switch (memtype) {
	case PCI_MAPREG_TYPE_MEM | PCI_MAPREG_MEM_TYPE_32BIT:
	case PCI_MAPREG_TYPE_MEM | PCI_MAPREG_MEM_TYPE_64BIT:
		if (pci_mapreg_map(pa, NFE_PCI_BA, memtype, 0, &sc->sc_memt,
		    &sc->sc_memh, NULL, &memsize, 0) == 0)
			break;
		/* FALLTHROUGH */
	default:
@


1.77
log
@Add the NFE_CORRECT_MACADDR flag for MCP77/79 chipsets.

ok jsg@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.76 2008/01/02 03:43:54 brad Exp $	*/
d874 1
a874 1
				printf("%s: tx v1 error 0x%04b\n",
d884 1
a884 1
				printf("%s: tx v2 error 0x%04b\n",
@


1.76
log
@add the NVIDIA MCP77/79 ids.

ok jsg@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.75 2007/12/11 23:08:09 mikeb Exp $	*/
d253 2
a254 1
		sc->sc_flags |= NFE_40BIT_ADDR | NFE_HW_CSUM | NFE_PWR_MGMT;
@


1.75
log
@Fix check for pending interrupts as some other device is causing
the status register to change.

From form@@.

ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.74 2007/12/05 08:30:33 jsg Exp $	*/
d156 9
a164 1
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP73_LAN4 }
d244 10
@


1.74
log
@Make sure newer adapters are not in powerdown mode.
From Oleg Safiullin <form@@pdp-11.org.ru> similiar to changes
in Linux/FreeBSD driver.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.73 2007/11/17 15:52:23 jsg Exp $	*/
d481 1
a481 1
	if ((r = NFE_READ(sc, NFE_IRQ_STATUS)) == 0)
@


1.73
log
@Disable jumbo allocator until the low number of buffers case
is better handled.

This will likely resolve stalls some people are seeing under high
load.

ok damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.72 2007/09/12 00:42:04 jsg Exp $	*/
d220 1
a220 1
		sc->sc_flags |= NFE_40BIT_ADDR;
d234 2
a235 1
		sc->sc_flags |= NFE_40BIT_ADDR | NFE_CORRECT_MACADDR;
d247 2
a248 1
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR | NFE_CORRECT_MACADDR;
d253 1
a253 1
		    NFE_HW_VLAN;
d255 11
@


1.72
log
@In nfe_start() do a fast return if IFF_OACTIVE is set, in
this case we need a Tx interrupt to clean up the DMA ring
before if_start can be properly called.

Diff based on a comment by & ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.71 2007/09/07 19:05:05 damien Exp $	*/
d255 1
d259 1
@


1.71
log
@use new malloc M_ZERO flag to shrink kernel.
remove <malloc.h> from files where malloc is not used.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.70 2007/09/01 19:19:39 ckuethe Exp $	*/
d1001 3
@


1.70
log
@Correct the backwards ethernet address that some NVidia MACs have.
diff from brad. "commit this" jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.69 2007/03/02 00:16:59 jsg Exp $	*/
a31 1
#include <sys/malloc.h>
@


1.69
log
@MCP65 has no hardware checksum support.
Diff from brad after he noticed recent changes
in the Linux driver by an NVIDIA employee.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.68 2007/01/08 18:39:27 damien Exp $	*/
d153 5
a157 1
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP67_LAN4 }
a209 4

	nfe_get_macaddr(sc, sc->sc_arpcom.ac_enaddr);
	printf(", address %s\n", ether_sprintf(sc->sc_arpcom.ac_enaddr));

d221 2
d231 5
a235 1
		sc->sc_flags |= NFE_40BIT_ADDR;
d247 1
a247 1
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR;
d260 3
d1737 22
a1758 9
	tmp = NFE_READ(sc, NFE_MACADDR_LO);
	addr[0] = (tmp >> 8) & 0xff;
	addr[1] = (tmp & 0xff);

	tmp = NFE_READ(sc, NFE_MACADDR_HI);
	addr[2] = (tmp >> 24) & 0xff;
	addr[3] = (tmp >> 16) & 0xff;
	addr[4] = (tmp >>  8) & 0xff;
	addr[5] = (tmp & 0xff);
@


1.68
log
@Add support for HW TCP/IP checksum offload for adapters that support it.

Tested by many (IP/UDP/TCP):

Jason McIntyre <jmc@@>
Chris Kuethe <chris.kuethe AT gmail.com>
Alf Schlichting <a.schlichting AT lemarit.com>
Rodolfo Gouveia <rgouveia AT cosmico.net>
Peter Stromberg <wilfried@@>

Has been in snaps for weeks too with noone complaining so far.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.67 2006/11/15 02:24:37 brad Exp $	*/
d235 2
d241 1
a241 1
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR | NFE_HW_CSUM;
@


1.67
log
@Correct the WOL magic value and rename NFE_WOL_MAGIC to NFE_WOL_ENABLE.

WOL magic value from Peer Chen@@NVIDIA via FreeBSD.

ok jsg@@ damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.66 2006/11/10 20:46:58 damien Exp $	*/
d4 1
a4 1
 * Copyright (c) 2006 Damien Bergamini <damien.bergamini@@free.fr>
a288 1
#ifdef NFE_CSUM
a292 1
#endif
d756 3
a758 4
#ifdef notyet
		if (sc->sc_flags & NFE_HW_CSUM) {
			if (flags & NFE_RX_IP_CSUMOK)
				m->m_pkthdr.csum_flags |= M_IPV4_CSUM_IN_OK;
a763 4
#elif defined(NFE_CSUM)
		if ((sc->sc_flags & NFE_HW_CSUM) && (flags & NFE_RX_CSUMOK))
			m->m_pkthdr.csum_flags = M_IPV4_CSUM_IN_OK;
#endif
a906 1
#ifdef NFE_CSUM
d910 1
a910 2
		flags |= NFE_TX_TCP_CSUM;
#endif
d941 1
a941 1
			flags &= ~(NFE_TX_IP_CSUM | NFE_TX_TCP_CSUM);
a1054 1
#ifdef NFE_CSUM
a1056 1
#endif
@


1.66
log
@Defer setting of the valid bit in the first TX descriptor after
all descriptors have been setup.  Otherwise, hardware may start
processing descriptors faster than us and crap out.
Fixes "watchdog timeout" errors.

Original idea from Matthew Dillon @@DragonFly.

ok deraadt@@ jsg@@ wim@@
tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.65 2006/11/05 20:15:37 brad Exp $	*/
d1130 1
a1130 1
	NFE_WRITE(sc, NFE_WOL_CTL, NFE_WOL_MAGIC);
@


1.65
log
@add the NVIDIA MCP67 LAN PCI ids.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.64 2006/07/23 02:12:12 brad Exp $	*/
d886 1
a886 1
	uint16_t flags = NFE_TX_VALID;
d890 1
a890 1
	int error, i;
d892 1
a892 1
	map = sc->txq.data[sc->txq.cur].map;
a944 1
		/* csum flags and vtag belong to the first fragment only */
d946 4
d954 5
d965 1
a965 1
	/* the whole mbuf chain has been DMA mapped, fix last descriptor */
d967 1
d970 3
d974 1
d980 3
@


1.64
log
@add NVidia MCP61/65 ids.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.63 2006/06/17 18:00:43 brad Exp $	*/
d149 5
a153 1
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP65_LAN4 }
d225 4
@


1.64.2.1
log
@MFC:
Fix by damien@@

Defer setting of the valid bit in the first TX descriptor after
all descriptors have been setup. Otherwise, hardware may start
processing descriptors faster than us and crap out.
Fixes "watchdog timeout" errors.

Original idea from Matthew Dillon @@DragonFly.

ok jsg@@ damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.64 2006/07/23 02:12:12 brad Exp $	*/
d878 1
a878 1
	uint16_t flags = 0;
d882 1
a882 1
	int error, i, first = sc->txq.cur;
d884 1
a884 1
	map = sc->txq.data[first].map;
d937 1
a938 4
			/*
			 * Checksum flags and vtag belong to the first fragment
			 * only.
			 */
a942 5
			/*
			 * Setting of the valid bit in the first descriptor is
			 * deferred until the whole chain is fully setup.
			 */
			flags |= NFE_TX_VALID;
d949 1
a949 1
	/* the whole mbuf chain has been setup */
a950 1
		/* fix last descriptor */
a952 3

		/* finally, set the valid bit in the first descriptor */
		sc->txq.desc64[first].flags |= htole16(NFE_TX_VALID);
a953 1
		/* fix last descriptor */
a958 3

		/* finally, set the valid bit in the first descriptor */
		sc->txq.desc32[first].flags |= htole16(NFE_TX_VALID);
@


1.63
log
@add sys/timeout.h
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.62 2006/05/29 01:00:02 brad Exp $	*/
d141 9
a149 1
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP55_LAN2 }
d217 4
d227 4
@


1.62
log
@garbage collect NFE_NO_JUMBO.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.61 2006/05/28 00:20:21 brad Exp $	*/
d35 1
@


1.61
log
@- remove ETHER_MAX_LEN_JUMBO and ETHERMTU_JUMBO.
- use if_hardmtu for MTU ioctl handlers.

ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.60 2006/05/28 00:04:24 jason Exp $	*/
a115 2
/*#define NFE_NO_JUMBO*/

a222 1
#ifndef NFE_NO_JUMBO
a225 1
#endif
@


1.60
log
@unknown ioctl is ENOTTY not EINVAL
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.59 2006/05/27 10:03:15 brad Exp $	*/
d262 1
a262 1
		ifp->if_hardmtu = ETHERMTU_JUMBO;
d486 1
a486 5
		if (ifr->ifr_mtu < ETHERMIN ||
		    ((sc->sc_flags & NFE_USE_JUMBO) &&
		    ifr->ifr_mtu > ETHERMTU_JUMBO) ||
		    (!(sc->sc_flags & NFE_USE_JUMBO) &&
		    ifr->ifr_mtu > ETHERMTU))
@


1.59
log
@remove IFCAP_JUMBO_MTU interface capabilities flag and set if_hardmtu in a few
more drivers.

ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.58 2006/05/20 03:47:56 brad Exp $	*/
d533 1
a533 1
		error = EINVAL;
@


1.58
log
@set if_jumbo_mtu and the IFCAP_JUMBO_MTU capabilities flag where
appropriate.

ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.57 2006/04/26 02:07:29 jsg Exp $	*/
d262 1
a262 1
		ifp->if_capabilities |= IFCAP_JUMBO_MTU;
@


1.57
log
@Use %b in error flag printfs to describe meaning of error bits.
requested by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.56 2006/04/26 01:33:38 brad Exp $	*/
d260 4
@


1.56
log
@And commit the diff from the right system.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.55 2006/04/26 01:20:28 brad Exp $	*/
d816 2
a817 2
				printf("%s: tx v1 error 0x%04x\n",
				    sc->sc_dev.dv_xname, flags);
d826 2
a827 2
				printf("%s: tx v2 error 0x%04x\n",
				    sc->sc_dev.dv_xname, flags);
@


1.55
log
@check for IFF_RUNNING being set before calling nfe_init().

ok damien@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.54 2006/04/07 12:38:12 jsg Exp $	*/
a479 2
			break;
		}
@


1.54
log
@Add work around for mbuf leak in the tx path until we
can come up with a better guess as to how the hardware works.
From Chuck Silvers.
ok damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.53 2006/03/25 22:41:45 djm Exp $	*/
a304 1
			ifp->if_flags &= ~IFF_RUNNING;
d474 2
a475 2
		nfe_init(ifp);
		switch (ifa->ifa_addr->sa_family) {
d477 1
a477 1
		case AF_INET:
a478 1
			break;
a479 1
		default:
d502 1
a502 1
			     (IFF_ALLMULTI | IFF_PROMISC)) != 0)
d504 4
a507 2
			else
				nfe_init(ifp);
a1007 1
	ifp->if_flags &= ~IFF_RUNNING;
a1017 3

	if (ifp->if_flags & IFF_RUNNING)
		return 0;
@


1.53
log
@allow bpf(4) to ignore packets based on their direction (inbound or
outbound), using a new BIOCSDIRFILT ioctl;
guidance, feedback and ok canacar@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.52 2006/03/02 09:04:00 jsg Exp $	*/
d815 1
a815 1
			if (!(flags & NFE_TX_LASTFRAG_V1))
d825 1
a825 1
			if (!(flags & NFE_TX_LASTFRAG_V2))
@


1.52
log
@Put the PHY update/reset call in nfe_init back to the spot it was at
before Rx/Tx/Interrupts are enabled.
This makes <fredd at cse.sc.edu>'s nfe+icsphy setup work again.

Tested on nfe+eephy by otto@@ and myself, nfe+ciphy by otto@@ and
nfe+rlphy by wilfried@@

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.51 2006/02/26 19:25:41 damien Exp $	*/
d756 1
a756 1
			bpf_mtap(ifp->if_bpf, m);
d982 1
a982 1
			bpf_mtap(ifp->if_bpf, m0);
@


1.52.2.1
log
@MFC:
Fix by jsg@@

Add work around for mbuf leak in the tx path until we
can come up with a better guess as to how the hardware works.
From Chuck Silvers.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.52 2006/03/02 09:04:00 jsg Exp $	*/
d815 1
a815 1
			if (!(flags & NFE_TX_LASTFRAG_V1) && data->m == NULL)
d825 1
a825 1
			if (!(flags & NFE_TX_LASTFRAG_V2) && data->m == NULL)
@


1.52.2.2
log
@MFC:
Fix by damien@@

Defer setting of the valid bit in the first TX descriptor after
all descriptors have been setup. Otherwise, hardware may start
processing descriptors faster than us and crap out.
Fixes "watchdog timeout" errors.

Original idea from Matthew Dillon @@DragonFly.

ok jsg@@ damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.52.2.1 2006/05/03 03:49:50 brad Exp $	*/
d868 1
a868 1
	uint16_t flags = 0;
d872 1
a872 1
	int error, i, first = sc->txq.cur;
d874 1
a874 1
	map = sc->txq.data[first].map;
d927 1
a928 4
			/*
			 * Checksum flags and vtag belong to the first fragment
			 * only.
			 */
a932 5
			/*
			 * Setting of the valid bit in the first descriptor is
			 * deferred until the whole chain is fully setup.
			 */
			flags |= NFE_TX_VALID;
d939 1
a939 1
	/* the whole mbuf chain has been setup */
a940 1
		/* fix last descriptor */
a942 3

		/* finally, set the valid bit in the first descriptor */
		sc->txq.desc64[first].flags |= htole16(NFE_TX_VALID);
a943 1
		/* fix last descriptor */
a948 3

		/* finally, set the valid bit in the first descriptor */
		sc->txq.desc32[first].flags |= htole16(NFE_TX_VALID);
@


1.51
log
@use sc->sc_dev.dv_xname consistently.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.50 2006/02/26 15:58:27 krw Exp $	*/
d1110 2
a1121 2

	nfe_ifmedia_upd(ifp);
@


1.50
log
@Let if_nfe.c compile again by putting declaration of ifp inside #ifdef
NFE_DEBUG.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.49 2006/02/26 11:02:29 jsg Exp $	*/
a358 3
#ifdef NFE_DEBUG
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
#endif	
d377 2
a378 1
		DPRINTFN(2, ("%s: timeout waiting for PHY\n", ifp->if_xname));
d383 2
a384 1
		DPRINTFN(2, ("%s: could not read PHY\n", ifp->if_xname));
d392 2
a393 2
	DPRINTFN(2, ("%s: mii read phy %d reg 0x%x ret 0x%x\n", ifp->if_xname,
	    phy, reg, val));
d443 1
a443 1
		DPRINTF(("%s: link state changed\n", ifp->if_xname));
d819 2
a820 2
				printf("%s: tx error 0x%04x\n", ifp->if_xname,
				    flags);
d829 2
a830 2
				printf("%s: tx error 0x%04x\n", ifp->if_xname,
				    flags);
d837 2
a838 2
			DPRINTF(("%s: last fragment bit w/o associated mbuf!\n",
			    ifp->if_xname));
d890 1
a890 1
	if ((m0->m_flags & (M_PROTO1|M_PKTHDR)) == (M_PROTO1|M_PKTHDR) &&
@


1.49
log
@Don't define NFE_DEBUG by default.
Set default debug level to 0.
Make sure to include interface or function name in debug strings.
Print Tx errors from the MAC when debug is not on.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.48 2006/02/24 04:58:25 brad Exp $	*/
d359 1
d361 1
@


1.48
log
@check for M_PKTHDR.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.47 2006/02/22 21:05:13 damien Exp $	*/
a115 1
#define NFE_DEBUG
d119 1
a119 1
int nfedebug = 1;
d359 1
d378 1
a378 1
		DPRINTFN(2, ("timeout waiting for PHY\n"));
d383 1
a383 1
		DPRINTFN(2, ("could not read PHY\n"));
d391 2
a392 1
	DPRINTFN(2, ("mii read phy %d reg 0x%x ret 0x%x\n", phy, reg, val));
d442 1
a442 1
		DPRINTF(("link state changed\n"));
d818 2
a819 1
				DPRINTF(("tx error 0x%04x\n", flags));
d828 2
a829 1
				DPRINTF(("tx error 0x%04x\n", flags));
d836 2
a837 1
			DPRINTF(("last fragment bit w/o associated mbuf!\n"));
@


1.47
log
@fix nfe_txeof() to reset the wathdog timeout only when a full tx frame
has been sent.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.46 2006/02/22 19:23:44 damien Exp $	*/
d885 2
a886 1
	if ((m0->m_flags & M_PROTO1) && m0->m_pkthdr.rcvif != NULL) {
@


1.46
log
@re-enable interrupt mitigation and mask out NFE_IRQ_TIMER that was causing
interrupts flood.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.45 2006/02/22 03:19:11 brad Exp $	*/
d791 1
a791 1
	struct nfe_tx_data *data;
a794 2
		data = &sc->txq.data[sc->txq.next];

d810 2
d844 2
d850 4
a853 3
	ifp->if_timer = 0;
	ifp->if_flags &= ~IFF_OACTIVE;
	nfe_start(ifp);
@


1.45
log
@update the media settings after MAC setup.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.44 2006/02/21 20:52:15 damien Exp $	*/
d1075 1
a1075 1
#if 0
@


1.44
log
@disable interrupt mitigation until i figure out why we're spending 10% of
CPU time in interrupts with it.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.43 2006/02/20 20:19:47 damien Exp $	*/
a1016 2
	nfe_ifmedia_upd(ifp);

d1111 2
@


1.43
log
@- remove mbuf linearization code. it is broken in this context and it is very
  unlikely that we will see mbuf chains with 62 fragments anytime soon
  (anyway, it would not crash, it would just freeze TX).
- fix max scatter value so we don't end up filling the ring with one mbuf
  chain.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.42 2006/02/19 13:57:02 damien Exp $	*/
d1077 1
a1077 1
#if 1
@


1.42
log
@- fix h/w VLAN tagging and enable it for adapters that support it (VLAN tag
  stripping job is left to the network stack).
- enable interrupt mitigation by default.
- add some magic to the initialization sequence in the hope that it will fix
  TX issues seen on some adapters.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.41 2006/02/16 17:35:51 damien Exp $	*/
d557 1
a557 1
	if (end >= start) {
d578 1
a578 1
	if (end >= start) {
a858 1
	struct mbuf *mnew;
d869 1
a869 1
	if (error != 0 && error != EFBIG) {
a872 30
	}
	if (error != 0) {
		/* too many fragments, linearize */

		MGETHDR(mnew, M_DONTWAIT, MT_DATA);
		if (mnew == NULL)
			return ENOBUFS;

		M_DUP_PKTHDR(mnew, m0);
		if (m0->m_pkthdr.len > MHLEN) {
			MCLGET(mnew, M_DONTWAIT);
			if (!(mnew->m_flags & M_EXT)) {
				m_freem(mnew);
				return ENOBUFS;
			}
		}

		m_copydata(m0, 0, m0->m_pkthdr.len, mtod(mnew, caddr_t));
		m_freem(m0);
		mnew->m_len = mnew->m_pkthdr.len;
		m0 = mnew;

		error = bus_dmamap_load_mbuf(sc->sc_dmat, map, m0,
		    BUS_DMA_NOWAIT);
		if (error != 0) {
			printf("%s: could not map mbuf (error %d)\n",
			    sc->sc_dev.dv_xname, error);
			m_freem(m0);
			return error;
		}
@


1.41
log
@- stop enabling/disabling interrupts in nfe_intr().
- store RX/TX settings in nfe_softc so we don't recompute them all the time.
- fix h/w VLAN tagging flags.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.40 2006/02/15 20:21:27 brad Exp $	*/
d222 1
a222 1
		    0/*NFE_HW_VLAN*/;
d389 1
a389 1
		sc->phyaddr = phy;
d862 3
d911 7
d938 3
d949 2
a950 2
		/* csum flags belong to the first fragment only */
		if (map->dm_nsegs > 1)
d952 4
a972 12
#if NVLAN > 0
	if (sc->sc_flags & NFE_HW_VLAN) {
		/* setup h/w VLAN tagging */
		if ((m0->m_flags & M_PROTO1) && m0->m_pkthdr.rcvif != NULL) {
			struct ifvlan *ifv = m0->m_pkthdr.rcvif->if_softc;
			desc64->vtag = htole32(NFE_TX_VTAG |
			    htons(ifv->ifv_tag));
		} else
			desc64->vtag = 0;
	}
#endif

d1051 1
d1062 9
a1070 1

d1075 5
d1108 1
a1108 1
#ifdef notyet
d1118 4
@


1.40
log
@move setting IFCAP_VLAN_MTU back to where it is supposed to be.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.39 2006/02/15 20:08:59 damien Exp $	*/
a260 1

d432 2
a433 4
	/* disable interrupts */
	NFE_WRITE(sc, NFE_IRQ_MASK, 0);

	r = NFE_READ(sc, NFE_IRQ_STATUS);
a435 6
	if (r == 0) {
		/* re-enable interrupts */
		NFE_WRITE(sc, NFE_IRQ_MASK, NFE_IRQ_WANTED);
		return 0;
	}

a451 3
	/* re-enable interrupts */
	NFE_WRITE(sc, NFE_IRQ_MASK, NFE_IRQ_WANTED);

a982 1
	uint32_t txctl;
a1009 10
	txctl = NFE_RXTX_KICKTX;
	if (sc->sc_flags & NFE_40BIT_ADDR)
		txctl |= NFE_RXTX_V3MAGIC;
	else if (sc->sc_flags & NFE_JUMBO_SUP)
		txctl |= NFE_RXTX_V2MAGIC;
#ifdef NFE_CSUM
	if (sc->sc_flags & NFE_HW_CSUM)
		txctl |= NFE_RXTX_RXCHECK;
#endif

d1011 1
a1011 1
	NFE_WRITE(sc, NFE_RXTX_CTL, txctl);
d1036 1
a1036 1
	uint32_t tmp, rxtxctl;
d1047 1
a1047 1
	rxtxctl = NFE_RXTX_BIT2;
d1049 1
a1049 1
		rxtxctl |= NFE_RXTX_V3MAGIC;
d1051 1
a1051 1
		rxtxctl |= NFE_RXTX_V2MAGIC;
d1054 1
a1054 1
		rxtxctl |= NFE_RXTX_RXCHECK;
d1057 1
a1057 1
	NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_RESET | rxtxctl);
d1059 1
a1059 1
	NFE_WRITE(sc, NFE_RXTX_CTL, rxtxctl);
d1103 2
a1104 2
	rxtxctl &= ~NFE_RXTX_BIT2;
	NFE_WRITE(sc, NFE_RXTX_CTL, rxtxctl);
d1106 1
a1106 1
	NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_BIT1 | rxtxctl);
@


1.39
log
@add support for 64bit rings base addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.38 2006/02/15 19:53:17 damien Exp $	*/
d260 2
a262 1
	ifp->if_capabilities |= IFCAP_VLAN_MTU;
@


1.38
log
@actually disable h/w VLAN tagging for now.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.37 2006/02/15 19:36:46 damien Exp $	*/
d1089 8
a1096 2
	NFE_WRITE(sc, NFE_RX_RING_ADDR, sc->rxq.physaddr);
	NFE_WRITE(sc, NFE_TX_RING_ADDR, sc->txq.physaddr);
@


1.37
log
@- enable jumbo frames for adapters that support it.
- extend TX DMA mappings size from MCLBYTES to NFE_JBYTES.
- add initial (disabled) bits for interrupts mitigation.
- add initial (disabled) bits for h/w VLAN tagging.
- did some consistency tweaks while i'm here.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.36 2006/02/13 08:54:54 brad Exp $	*/
d222 1
a222 1
		    NFE_HW_VLAN;
@


1.36
log
@use pci_mapreg_type().

ok damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.35 2006/02/13 06:15:32 brad Exp $	*/
d117 1
a117 1
#define NFE_NO_JUMBO
d169 2
a170 3
		if (pci_mapreg_map(pa, NFE_PCI_BA,
		    memtype, 0, &sc->sc_memt, &sc->sc_memh,
		    NULL, &memsize, 0) == 0)
d172 1
d174 1
a174 1
		printf(": can't map mem space\n");
d179 1
a179 1
		printf(": couldn't map interrupt\n");
d187 1
a187 1
		printf(": couldn't establish interrupt");
d217 2
d221 2
a222 1
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR | NFE_HW_CSUM;
d260 5
a264 1
	ifp->if_capabilities = IFCAP_VLAN_MTU;
d967 12
d1105 8
a1114 1
	NFE_WRITE(sc, NFE_TIMER_INT, 970);	/* XXX Magic */
d1362 1
a1362 1
		if (data->m != NULL) {
d1364 1
a1364 2
			    data->map->dm_mapsize,
			    BUS_DMASYNC_POSTREAD);
d1366 3
a1369 4
		}

		if (data->map != NULL)
			bus_dmamap_destroy(sc->sc_dmat, data->map);
d1549 2
a1550 2
		error = bus_dmamap_create(sc->sc_dmat, MCLBYTES,
		    NFE_MAX_SCATTER, MCLBYTES, 0, BUS_DMA_NOWAIT,
d1624 1
a1624 2
			    data->active->dm_mapsize,
			    BUS_DMASYNC_POSTWRITE);
@


1.35
log
@nVidia/nvidia -> NVIDIA
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.34 2006/02/12 15:53:20 damien Exp $	*/
d163 1
d165 9
a173 2
	if (pci_mapreg_map(pa, NFE_PCI_BA, PCI_MAPREG_TYPE_MEM, 0,
	    &sc->sc_memt, &sc->sc_memh, NULL, &memsize, 0) != 0) {
@


1.34
log
@fix a printf
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.33 2006/02/12 13:28:30 damien Exp $	*/
d20 1
a20 1
/* Driver for nvidia nForce Ethernet */
@


1.33
log
@don't print shared interrupts when debugging is on.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.32 2006/02/12 13:25:01 damien Exp $	*/
d1369 1
a1369 1
		    buf, sc->sc_dev.dv_xname);
@


1.32
log
@use BUS_DMA_READ flag when mapping RX buffers.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.31 2006/02/12 13:08:42 damien Exp $	*/
a422 2
	DPRINTFN(5, ("nfe_intr: interrupt register %x\n", r));

d428 2
@


1.31
log
@reduce the number of DMA sync operations by sincing multiple TX descriptors
at once.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.30 2006/02/12 10:28:07 damien Exp $	*/
d704 1
a704 1
			    BUS_DMA_NOWAIT);
d711 1
a711 1
				    BUS_DMA_NOWAIT);
d1246 1
a1246 1
			    BUS_DMA_NOWAIT);
d1415 1
a1415 1
	    NFE_JPOOL_SIZE, NULL, BUS_DMA_NOWAIT);
@


1.30
log
@initial jumbo frames support (disabled for now).
#define'ing NFE_NO_JUMBO can save a few hundred KB of wired memory.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.29 2006/02/11 20:25:21 brad Exp $	*/
d80 2
d551 42
a923 2

			nfe_txdesc64_sync(sc, desc64, BUS_DMASYNC_PREWRITE);
a929 2

			nfe_txdesc32_sync(sc, desc32, BUS_DMASYNC_PREWRITE);
a942 1

a943 1
		nfe_txdesc64_sync(sc, desc64, BUS_DMASYNC_PREWRITE);
a948 1

a949 1
		nfe_txdesc32_sync(sc, desc32, BUS_DMASYNC_PREWRITE);
d965 1
a967 1
	int pkts = 0;
a980 1
		pkts++;
d987 1
a987 1
	if (pkts == 0)
d989 5
@


1.29
log
@set IFCAP_VLAN_MTU in the capabilites field.

ok damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.28 2006/02/11 11:51:30 damien Exp $	*/
d31 1
d89 4
d115 1
d213 6
a247 1

d479 5
a483 1
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ETHERMTU)
d571 1
d573 1
d633 11
a643 6
		MCLGET(mnew, M_DONTWAIT);
		if (!(mnew->m_flags & M_EXT)) {
			m_freem(mnew);
			ifp->if_ierrors++;
			goto skip;
		}
d645 8
a652 3
		bus_dmamap_sync(sc->sc_dmat, data->map, 0,
		    data->map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, data->map);
d654 3
a656 4
		error = bus_dmamap_load(sc->sc_dmat, data->map,
		    mtod(mnew, void *), MCLBYTES, NULL, BUS_DMA_NOWAIT);
		if (error != 0) {
			m_freem(mnew);
a657 1
			/* try to reload the old mbuf */
d659 1
a659 1
			    mtod(data->m, void *), MCLBYTES, NULL,
d662 13
a674 3
				/* very unlikely that it will fail... */
				panic("%s: could not load old rx mbuf",
				    sc->sc_dev.dv_xname);
d676 1
a676 2
			ifp->if_ierrors++;
			goto skip;
d711 2
a712 1
skip:		if (sc->sc_flags & NFE_40BIT_ADDR) {
d714 1
a714 2
			desc64->physaddr[0] =
			    htole32(data->map->dm_segs->ds_addr >> 32);
d716 7
a722 2
			desc64->physaddr[1] =
			    htole32(data->map->dm_segs->ds_addr & 0xffffffff);
a723 1
			desc64->length = htole16(MCLBYTES);
d727 1
a727 2
			desc32->physaddr =
			    htole32(data->map->dm_segs->ds_addr);
a728 1
			desc32->length = htole16(MCLBYTES);
d1029 1
a1029 1
	NFE_WRITE(sc, NFE_RXBUFSZ, MCLBYTES);
a1100 1
	struct nfe_rx_data *data;
d1103 2
d1106 1
d1118 1
d1153 10
a1162 1
	ring->physaddr = ring->map->dm_segs->ds_addr;
a1169 8
		error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1, MCLBYTES,
		    0, BUS_DMA_NOWAIT, &data->map);
		if (error != 0) {
			printf("%s: could not create DMA map\n",
			    sc->sc_dev.dv_xname);
			goto fail;
		}

d1178 25
a1202 7
		MCLGET(data->m, M_DONTWAIT);
		if (!(data->m->m_flags & M_EXT)) {
			printf("%s: could not allocate rx mbuf cluster\n",
			    sc->sc_dev.dv_xname);
			error = ENOMEM;
			goto fail;
		}
d1204 9
a1212 6
		error = bus_dmamap_load(sc->sc_dmat, data->map,
		    mtod(data->m, void *), MCLBYTES, NULL, BUS_DMA_NOWAIT);
		if (error != 0) {
			printf("%s: could not load rx buf DMA map",
			    sc->sc_dev.dv_xname);
			goto fail;
d1218 1
a1218 2
			desc64->physaddr[0] =
			    htole32(data->map->dm_segs->ds_addr >> 32);
d1220 2
a1221 3
			desc64->physaddr[1] =
			    htole32(data->map->dm_segs->ds_addr & 0xffffffff);
			desc64->length = htole16(MCLBYTES);
d1225 2
a1226 3
			desc32->physaddr =
			    htole32(data->map->dm_segs->ds_addr);
			desc32->length = htole16(MCLBYTES);
d1247 1
a1247 1
			ring->desc64[i].length = htole16(MCLBYTES);
d1250 1
a1250 1
			ring->desc32[i].length = htole16(MCLBYTES);
d1301 121
d1474 1
a1474 1
	ring->physaddr = ring->map->dm_segs->ds_addr;
@


1.28
log
@- call nfe_ifmedia_upd() early in nfe_init() to reset the PHY.
- set RX buffer size register properly
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.27 2006/02/11 09:40:36 damien Exp $	*/
d235 2
d239 1
a239 1
		ifp->if_capabilities = IFCAP_CSUM_IPv4 | IFCAP_CSUM_TCPv4 |
@


1.27
log
@fix nfe_free_tx_ring() and nfe_reset_tx_ring() to unmap the good items.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.26 2006/02/11 09:26:15 damien Exp $	*/
d960 2
d991 2
a1010 3

	/* configure media */
	mii_mediachg(&sc->sc_mii);
@


1.26
log
@don't free the mbuf in nfe_encap() if it can't be DMA'mapped since it is
left in if_snd.  prettify nfe_start() a bit while i'm here.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.25 2006/02/11 09:18:56 damien Exp $	*/
d1326 3
a1328 3
			bus_dmamap_sync(sc->sc_dmat, data->map, 0,
			    data->map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, data->map);
d1369 2
a1370 2
			bus_dmamap_sync(sc->sc_dmat, data->map, 0,
			    data->map->dm_mapsize,
d1372 1
a1372 1
			bus_dmamap_unload(sc->sc_dmat, data->map);
d1375 1
d1377 6
a1382 2
		if (data->map != NULL)
			bus_dmamap_destroy(sc->sc_dmat, data->map);
@


1.25
log
@MCP51 boards don't support jumbo frames..
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.24 2006/02/11 09:15:57 damien Exp $	*/
a783 1
		m_freem(m0);
d790 2
a791 4
		if (mnew == NULL) {
			m_freem(m0);
			return ENOMEM;
		}
a796 1
				m_freem(m0);
d798 1
a798 1
				return ENOMEM;
d892 1
a892 1
	uint32_t txctl = NFE_RXTX_KICKTX;
d917 1
a921 1

@


1.24
log
@force a wakeup of the MAC in nfe_init().
this makes my MCP51 board working.

committed over a nfe.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.23 2006/02/10 03:54:54 brad Exp $	*/
d195 1
a195 1
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR;
@


1.23
log
@Only kick the TX engine once in nfe_start() and only if there are
packets to be transmitted, after iterating through the queue and
queuing up as many packets as possible as opposed to kicking
it each time through nfe_encap().

ok damien@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.22 2006/02/08 13:28:32 jsg Exp $	*/
d957 1
a957 1
	uint32_t rxtxctl;
d992 7
@


1.22
log
@Add back minimal debugging to help track down tx errors some MACs
seem to be reporting.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.21 2006/02/08 09:28:46 jsg Exp $	*/
a774 1
	uint32_t txctl = NFE_RXTX_KICKTX;
a867 1
		txctl |= NFE_RXTX_V3MAGIC;
d873 1
a873 2
		if (sc->sc_flags & NFE_JUMBO_SUP) {
			txctl |= NFE_RXTX_V2MAGIC;
d875 1
a875 1
		} else
a881 5
#ifdef NFE_CSUM
	if (sc->sc_flags & NFE_HW_CSUM)
		txctl |= NFE_RXTX_RXCHECK;
#endif

a887 3
	/* kick Tx */
	NFE_WRITE(sc, NFE_RXTX_CTL, txctl);

d896 2
d911 1
d917 13
d931 7
a937 3
		/* start watchdog timer */
		ifp->if_timer = 5;
	}
@


1.21
log
@Use __LP64__ instead of __amd64__ for portability.
Pointed out by miod@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.20 2006/02/07 08:55:37 jsg Exp $	*/
d109 2
d112 1
a112 1
int nfedebug = 0;
@


1.20
log
@Disable checksum offload for the moment as it appears to not
work for some people.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.19 2006/02/05 23:37:21 brad Exp $	*/
d674 1
a674 1
#if defined(__amd64__)
d837 1
a837 1
#if defined(__amd64__)
d1139 1
a1139 1
#if defined(__amd64__)
@


1.19
log
@Simplify SIOCSIFADDR switch case a bit.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.18 2006/02/05 23:32:06 brad Exp $	*/
d233 1
d238 1
d661 1
a661 1
#else
d825 1
d830 1
d883 1
d886 1
d960 1
d963 1
@


1.18
log
@Eliminate a reset when configuring the IP address.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.17 2006/02/05 10:09:39 jsg Exp $	*/
d450 1
a453 1
			nfe_init(ifp);
a457 1
			nfe_init(ifp);
@


1.17
log
@Don't force NFE_DEBUG, set default debug level to 0.
Committed via an nfe.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.16 2006/02/05 10:01:23 damien Exp $	*/
d272 1
d942 3
@


1.16
log
@quiet nfe_intr().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.15 2006/02/05 09:38:29 damien Exp $	*/
a108 2
#define NFE_DEBUG

d110 1
a110 1
int nfedebug = 1;
@


1.15
log
@unbreak the tree.

pointed at by Peter Stromberg.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.14 2006/02/05 09:14:28 damien Exp $	*/
d405 2
a425 2

	DPRINTF(("nfe_intr: interrupt register %x", r));
@


1.14
log
@fix nfe_txeof().
with these changes, the connection seems to work just fine.

with help from jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.13 2006/02/04 21:48:34 damien Exp $	*/
d706 1
a706 1
	while (txq->next != txq->cur) {
@


1.13
log
@- start MII timer in nfe_init() and move timer initialization in nfe_attach()
- set RX filter before enabling RX in nfe_init()
- call mii_down() in nfe_stop()
- fix setting of full/half-duplex mode
- call mii_phy_reset() for each PHY attached and call mii_mediachg() in
  nfe_ifmedia_upd()
- some cleaning while i'm here
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.12 2006/02/04 16:51:15 damien Exp $	*/
d706 1
a706 2
/* XXX: should limit # iterations to NFE_TX_RING_COUNT */
	for (;;) {
d721 1
a721 1
		if (!(flags & NFE_TX_VALID))
@


1.12
log
@fix clearing of h/w csum flags in all but the first fragment
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.11 2006/02/04 10:32:56 damien Exp $	*/
d94 2
a95 2
int	nfe_mediachange(struct ifnet *);
void	nfe_mediastatus(struct ifnet *, struct ifmediareq *);
a161 3
	/*
	 * Allocate interrupt.
	 */
d245 2
a246 2
	ifmedia_init(&sc->sc_mii.mii_media, 0, nfe_mediachange,
	    nfe_mediastatus);
d260 2
d286 1
a286 1
	uint32_t reg;
d288 10
a297 1
	reg = NFE_READ(sc, NFE_PHY_INT);
d300 4
a303 2
	case IFM_1000_T:
		reg |= NFE_PHY_1000T;
d306 7
a312 1
		reg |= NFE_PHY_100TX;
d316 5
a320 1
	NFE_WRITE(sc, NFE_PHY_INT, reg);
d986 6
a997 2
	nfe_setmulti(sc);

d1003 1
a1003 3
	mii_mediachg(&sc->sc_mii);

	timeout_set(&sc->sc_timeout, nfe_tick, sc);
d1016 1
a1016 1
	timeout_del(&sc->sc_timeout);
d1021 2
d1360 1
a1360 1
nfe_mediachange(struct ifnet *ifp)
d1363 2
a1364 12
	struct mii_data	*mii = &sc->sc_mii;
	uint32_t val;

	DPRINTF(("nfe_mediachange\n"));
#if 0
	if ((mii->mii_media_active & IFM_GMASK) == IFM_FDX)
		/* XXX? */
	else
#endif
		val = 0;

	val |= NFE_MEDIA_SET;
d1366 3
a1368 10
	switch (IFM_SUBTYPE(mii->mii_media_active)) {
	case IFM_1000_T:
		val |= NFE_MEDIA_1000T;
		break;
	case IFM_100_TX:
		val |= NFE_MEDIA_100TX;
		break;
	case IFM_10_T:
		val |= NFE_MEDIA_10T;
		break;
d1370 1
a1370 5

	DPRINTF(("nfe_miibus_statchg: val=0x%x\n", val));
	NFE_WRITE(sc, NFE_LINKSPEED, val);

	return 0;
d1374 1
a1374 1
nfe_mediastatus(struct ifnet *ifp, struct ifmediareq *ifmr)
d1377 1
d1379 3
a1381 3
	mii_pollstat(&sc->sc_mii);
	ifmr->ifm_status = sc->sc_mii.mii_media_status;
	ifmr->ifm_active = sc->sc_mii.mii_media_active;
d1472 1
a1472 1
	timeout_add(&sc->sc_timeout, hz);
@


1.11
log
@handle link state change interrupts but do nothing for now
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.10 2006/02/04 09:46:48 damien Exp $	*/
a805 1
	/* h/w checksum (XXX only if HW_CSUM?) */
d838 1
a838 1
			flags &= ~(M_TCPV4_CSUM_OUT | M_UDPV4_CSUM_OUT);
@


1.10
log
@- add support for multicast filters.
- fix setting of if_capabilities flags for chips supporting checksum offload.
- fix dmesg output in case we can't establish the intr handler.
- fix a call to bus_dmamap_unload() in nfe_encap().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.9 2006/01/22 21:35:08 damien Exp $	*/
d389 6
@


1.9
log
@fixes miibus_{read,write}reg routines.
this resolves the ghost ukphy problem.
did some cleanup while i'm here.

tested by and ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.8 2006/01/20 22:02:03 brad Exp $	*/
a98 1
void	nfe_update_promisc(struct nfe_softc *);
d135 1
a135 1
	{ PCI_VENDOR_NVIDIA, PCI_PRODUCT_NVIDIA_MCP55_LAN2 },
d177 1
d233 1
a233 1
	ifp->if_baudrate = 1000000000;
d238 5
d445 9
a453 2
			if (ifp->if_flags & IFF_RUNNING)
				nfe_update_promisc(sc);
d460 1
d631 1
a631 1
				m->m_pkthdr.csum_flags |= M_UDPV4_CSUM_OUT;
d633 1
a633 1
				m->m_pkthdr.csum_flags |= M_TCPV4_CSUM_OUT;
d796 1
a796 1
		bus_dmamap_unload(sc->sc_dmat, data->active);
d1378 48
a1425 9
	NFE_WRITE(sc, NFE_MULT_ADDR1, 0x01);
	NFE_WRITE(sc, NFE_MULT_ADDR2, 0);
	NFE_WRITE(sc, NFE_MULT_MASK1, 0);
	NFE_WRITE(sc, NFE_MULT_MASK2, 0);
#ifdef notyet
	NFE_WRITE(sc, NFE_MULTI_FLAGS, NFE_MC_ALWAYS | NFE_MC_MYADDR);
#else
	NFE_WRITE(sc, NFE_MULTI_FLAGS, NFE_MC_ALWAYS | NFE_MC_PROMISC);
#endif
a1450 5
}

void
nfe_update_promisc(struct nfe_softc *sc)
{
@


1.8
log
@remove redundant code from nfe_attach().

ok jsg@@
@
text
@d1 3
a3 2
/*	$OpenBSD: if_nfe.c,v 1.7 2006/01/18 20:44:51 damien Exp $	*/
/*
d71 4
a74 1

d76 1
a76 7
int	nfe_alloc_rx_ring(struct nfe_softc *, struct nfe_rx_ring *);
void	nfe_reset_rx_ring(struct nfe_softc *, struct nfe_rx_ring *);
void	nfe_free_rx_ring(struct nfe_softc *, struct nfe_rx_ring *);
int	nfe_alloc_tx_ring(struct nfe_softc *, struct nfe_tx_ring *);
void	nfe_reset_tx_ring(struct nfe_softc *, struct nfe_tx_ring *);
void	nfe_free_tx_ring(struct nfe_softc *, struct nfe_tx_ring *);

a83 2

int	nfe_ioctl(struct ifnet *, u_long, caddr_t);
a84 1
void	nfe_stop(struct ifnet *, int);
d87 9
a101 6
int	nfe_miibus_readreg(struct device *, int, int);
void	nfe_miibus_writereg(struct device *, int, int, int);
void	nfe_miibus_statchg(struct device *);
int	nfe_mediachange(struct ifnet *);
void	nfe_mediastatus(struct ifnet *, struct ifmediareq *);

d103 1
a103 3
	sizeof (struct nfe_softc),
	nfe_match,
	nfe_attach
d107 1
a107 1
	0, "nfe", DV_IFNET
d110 1
a156 3
	/*
	 * Map control/status registers.
	 */
d163 3
a165 1
	/* Allocate interrupt */
a235 2

	/* Set interface name */
a242 1
	/* XXX always seem to get a ghost ukphy along with eephy on nf4u */
d258 17
a274 1
	/* XXX powerhook */
d281 1
a281 1
	struct mii_data	*mii = &sc->sc_mii;
d302 4
a305 1
	uint32_t r;
d307 1
a307 2
	r = NFE_READ(sc, NFE_PHY_CTL);
	if (r & NFE_PHY_BUSY) {
d309 1
a309 1
		delay(100);
d312 19
a330 4
	NFE_WRITE(sc, NFE_PHY_CTL, reg | (phy << NFE_PHYADD_SHIFT));
	delay(1000);
	r = NFE_READ(sc, NFE_PHY_DATA);
	if (r != 0xffffffff && r != 0)
d333 1
a333 1
	DPRINTFN(2, ("nfe mii read phy %d reg 0x%x ret 0x%x\n", phy, reg, r));
d335 1
a335 1
	return r;
d339 1
a339 1
nfe_miibus_writereg(struct device *dev, int phy, int reg, int data)
d342 4
a345 1
	uint32_t r;
d347 1
a347 2
	r = NFE_READ(sc, NFE_PHY_CTL);
	if (r & NFE_PHY_BUSY) {
d349 1
a349 1
		delay(100);
d352 13
a364 3
	NFE_WRITE(sc, NFE_PHY_DATA, data);
	r = reg | (phy << NFE_PHYADD_SHIFT) | NFE_PHY_WRITE;
	NFE_WRITE(sc, NFE_PHY_CTL, r);
d520 1
a520 1
		if (sc->sc_flags & NFE_40BIT_ADDR) {	/* const condition */
d540 1
d548 1
d603 1
a603 1
	 	 * New mbuf successfully loaded, update Rx ring and continue
d613 10
d625 1
d634 1
a634 1
skip:		if (sc->sc_flags & NFE_40BIT_ADDR) {	/* const condition */
d671 1
a671 1
		if (sc->sc_flags & NFE_40BIT_ADDR) {	/* const condition */
d686 19
a704 2
		if (data->m == NULL)
			goto skip;	/* skip intermediate fragments */
d706 4
a709 4
		if (flags & NFE_TX_ERROR)
			ifp->if_oerrors++;
		else
			ifp->if_opackets++;
d782 5
a793 1

d796 1
a796 1
		if (sc->sc_flags & NFE_40BIT_ADDR) {	/* const condition */
d818 1
a818 1
		/* csum flags belong to the first frament only */
a900 24
void
nfe_stop(struct ifnet *ifp, int disable)
{
	struct nfe_softc *sc = ifp->if_softc;

	timeout_del(&sc->sc_timeout);

	ifp->if_timer = 0;
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);

	/* abort Tx */
	NFE_WRITE(sc, NFE_TX_CTL, 0);

	/* disable Rx */
	NFE_WRITE(sc, NFE_RX_CTL, 0);

	/* disable interrupts */
	NFE_WRITE(sc, NFE_IRQ_MASK, 0);

	/* reset Tx and Rx rings */
	nfe_reset_tx_ring(sc, &sc->txq);
	nfe_reset_rx_ring(sc, &sc->rxq);
}

d920 1
a920 1
	delay(10);
d938 1
a938 1
	NFE_WRITE(sc, NFE_TIMER_INT, 970);		/* XXX Magic */
d945 1
a945 1
	delay(10);
d956 2
d971 24
a1051 1

d1396 1
a1396 1
	    addr[0] <<  8 | addr[1]);
d1398 1
a1398 1
	    addr[2] << 24 | addr[3] << 16 | addr[4] << 8 | addr[5]);
@


1.7
log
@initial Tx/Rx bits.  not working yet.

joint work with jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.6 2006/01/15 10:55:06 damien Exp $	*/
a158 1
	pcireg_t command;
a162 11
	command = pci_conf_read(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG);
	command |= PCI_COMMAND_MEM_ENABLE | PCI_COMMAND_MASTER_ENABLE;

	pci_conf_write(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG, command);
	command = pci_conf_read(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG);

	if (!(command & PCI_COMMAND_MEM_ENABLE)) {
		printf(": mem space not enabled\n");
		return;
	}

@


1.6
log
@fix dma mapping of tx ring.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.5 2006/01/14 04:33:35 jsg Exp $	*/
d3 2
a4 1
 * Copyright (c) 2005 Jonathan Gray <jsg@@openbsd.org>
a74 1
int	nfe_rxintr(struct nfe_softc *);
d78 8
a85 1
int	nfe_txintr(struct nfe_softc *);
a91 1
void	nfe_reset(struct nfe_softc *);
d93 2
d105 1
a105 1
	sizeof(struct nfe_softc),
d116 1
a116 1
int	nfedebug = 1;
d145 2
a146 2
	return (pci_matchbyid((struct pci_attach_args *)aux, nfe_devices,
	    sizeof(nfe_devices)/sizeof(nfe_devices[0])));
d152 8
a159 9
	struct nfe_softc	*sc = (struct nfe_softc *)self;	
	struct pci_attach_args	*pa = aux;
	pci_chipset_tag_t	pc = pa->pa_pc;
	pci_intr_handle_t	ih;
	const char		*intrstr = NULL;
	struct ifnet		*ifp;
	bus_size_t		iosize;
	pcireg_t		command;
	int			i;
d166 1
a166 1
	    
d170 1
a170 1
	if ((command & PCI_COMMAND_MEM_ENABLE) == 0) {
d176 1
a176 1
	    &sc->sc_iot, &sc->sc_ioh, NULL, &iosize, 0)) {
d182 1
a182 1
	if (pci_intr_map(pa, &ih)) {
d186 1
d200 2
a201 4
	i = betoh16(NFE_READ(sc, NFE_MACADDR_LO) & 0xffff);
	memcpy((char *)sc->sc_arpcom.ac_enaddr, &i, 2);
	i = betoh32(NFE_READ(sc, NFE_MACADDR_HI));
	memcpy(&(sc->sc_arpcom.ac_enaddr[2]), &i, 4);
d203 1
a203 2
	printf(", address %s\n",
	    ether_sprintf(sc->sc_arpcom.ac_enaddr));
d205 1
a205 1
	switch(PCI_PRODUCT(pa->pa_id)) {
d207 1
a207 1
	case PCI_PRODUCT_NVIDIA_NFORCE3_LAN3: 
d210 5
a214 1
		sc->sc_flags |= NFE_JUMBO_SUP;
a219 2
	case PCI_PRODUCT_NVIDIA_MCP51_LAN1:
	case PCI_PRODUCT_NVIDIA_MCP51_LAN2:
d222 1
a222 1
		sc->sc_flags |= NFE_JUMBO_SUP | NFE_40BIT_ADDR;
a223 2
	default:
		sc->sc_flags = 0;
d232 1
a232 1
		goto fail1;
d238 2
a239 1
		goto fail2;
a241 3
	NFE_WRITE(sc, NFE_RING_SIZE, NFE_RX_RING_COUNT << 16 |
	    NFE_TX_RING_COUNT);

d262 2
a263 2
	ifmedia_init(&sc->sc_mii.mii_media, 0,
	    nfe_mediachange, nfe_mediastatus);
d268 1
a268 1
		ifmedia_add(&sc->sc_mii.mii_media, IFM_ETHER|IFM_MANUAL,
d270 1
a270 1
		ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER|IFM_MANUAL);
d272 1
a272 1
		ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER|IFM_AUTO);
a277 8
	return;

fail2:
	nfe_free_tx_ring(sc, &sc->txq);
fail1:
	nfe_free_rx_ring(sc, &sc->rxq);

	return;
d283 1
a283 1
	struct nfe_softc *sc = (struct nfe_softc *) dev;
d297 1
d304 1
a304 1
	struct nfe_softc *sc = (struct nfe_softc *) dev;
d321 1
a321 1
	return (r);
d327 1
a327 1
	struct nfe_softc *sc = (struct nfe_softc *) dev;
d345 1
d350 1
a350 1
	
d355 3
a357 2
		NFE_WRITE(sc, NFE_IRQ_MASK, NFE_IRQ_WANTED | NFE_IRQ_TIMER);
		return(0);
d360 3
a362 2
	if (r & NFE_IRQ_RX)
		nfe_rxintr(sc);
d364 3
a366 2
	if (r & NFE_IRQ_TX_DONE)
		nfe_txintr(sc);
d371 3
a373 3
	NFE_WRITE(sc, NFE_IRQ_MASK, NFE_IRQ_WANTED | NFE_IRQ_TIMER);
	
	return (1);
d381 1
a381 1
	struct ifaddr *ifa = (struct ifaddr *) data;
d388 1
a388 1
		return (error);
a391 10
	case SIOCSIFMTU:
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ETHERMTU)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu)
			ifp->if_mtu = ifr->ifr_mtu;
		break;
	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &sc->sc_mii.mii_media, cmd);
		break;
d406 5
a410 12
	case SIOCADDMULTI:
	case SIOCDELMULTI:
		if (cmd == SIOCADDMULTI)
			error = ether_addmulti(ifr, &sc->sc_arpcom);
		else
			error = ether_delmulti(ifr, &sc->sc_arpcom);

		if (error == ENETRESET) {
			if (ifp->if_flags & IFF_RUNNING)
				nfe_setmulti(sc);
			error = 0;
		}
d423 16
d442 1
d445 352
a796 1
	return (error);
d802 37
a864 5
void
nfe_watchdog(struct ifnet *ifp)
{
}

d868 2
a869 2
	struct nfe_softc	*sc = ifp->if_softc;
	int r;
d875 1
a875 4
	NFE_WRITE(sc, NFE_RXTX_CTL, NFE_RXTX_RESET | NFE_RXTX_BIT2);
	delay(10);

	r = NFE_RXTX_BIT2;
d877 1
a877 1
		r |= NFE_RXTX_V3MAGIC|NFE_RXTX_RXCHECK;
d879 3
a881 1
		r |= NFE_RXTX_V2MAGIC|NFE_RXTX_RXCHECK;
d883 3
a885 1
	NFE_WRITE(sc, NFE_RXTX_CTL, r);
d889 2
a890 1
	/* XXX set MAC address */
d892 1
a892 1
	/* Tell MAC where rings are in memory */
d896 4
d907 5
d915 3
d921 1
a921 3
	NFE_WRITE(sc, NFE_IRQ_MASK, NFE_IRQ_RXERR | NFE_IRQ_RX |
	    NFE_IRQ_RX_NOBUF | NFE_IRQ_TXERR | NFE_IRQ_TX_DONE | NFE_IRQ_LINK |
	    NFE_IRQ_TXERR2);
d930 1
a930 7
	return (0);
}

void
nfe_reset(struct nfe_softc *sc)
{
	printf("nfe_reset!\n");
d937 2
a938 2
	struct nfe_desc *desc_v1;
	struct nfe_desc_v3 *desc_v3;
d943 2
a944 2
		desc = (void **)&ring->desc_v3;
		descsize = sizeof(struct nfe_desc_v3);
d946 2
a947 2
		desc = (void **)&ring->desc_v1;	
		descsize = sizeof(struct nfe_desc);
d984 1
a984 1
	memset(*desc, 0, NFE_RX_RING_COUNT * descsize);
d990 1
a990 1
	
d1027 4
a1030 4
			desc_v3 = &sc->rxq.desc_v3[i];
			desc_v3->physaddr[0] =
#if 0
			(htole64(data->map->dm_segs->ds_addr) >> 32) & 0xffffffff;
d1032 4
a1035 6
			    0;
			desc_v3->physaddr[1] =
			    htole64(data->map->dm_segs->ds_addr) & 0xffffffff;

			desc_v3->length = htole16(MCLBYTES);
			desc_v3->flags = htole16(NFE_RX_READY);
d1037 2
a1038 2
			desc_v1 = &sc->rxq.desc_v1[i];
			desc_v1->physaddr =
d1040 2
a1041 2
			desc_v1->length = htole16(MCLBYTES);
			desc_v1->flags = htole16(NFE_RX_READY);
d1061 2
a1062 2
			ring->desc_v3[i].length = htole16(MCLBYTES);
			ring->desc_v3[i].flags = htole16(NFE_RX_READY);
d1064 2
a1065 2
			ring->desc_v1[i].length = htole16(MCLBYTES);
			ring->desc_v1[i].flags = htole16(NFE_RX_READY);
d1083 2
a1084 2
		desc = ring->desc_v3;
		descsize = sizeof(struct nfe_desc_v3);
d1086 2
a1087 2
		desc = ring->desc_v1;	
		descsize = sizeof(struct nfe_desc);
a1115 7
nfe_rxintr(struct nfe_softc *sc)
{
	printf("nfe_rxintr!\n");
	return (0);
}

int
d1123 2
a1124 2
		desc = (void **)&ring->desc_v3;
		descsize = sizeof(struct nfe_desc_v3);
d1126 2
a1127 2
		desc = (void **)&ring->desc_v1;	
		descsize = sizeof(struct nfe_desc);
d1135 1
a1135 1
	    
d1166 1
a1166 1
	memset(*desc, 0, NFE_TX_RING_COUNT * descsize);
a1188 1
	void *desc;
d1194 1
a1194 1
			desc = &ring->desc_v3[i];
d1196 1
a1196 1
			desc = &ring->desc_v1[i];	
a1206 5

		if (sc->sc_flags & NFE_40BIT_ADDR)
			((struct nfe_desc_v3 *)desc)->flags = 0;
		else
			((struct nfe_desc *)desc)->flags = 0;
d1224 2
a1225 2
		desc = ring->desc_v3;
		descsize = sizeof(struct nfe_desc_v3);
d1227 2
a1228 2
		desc = ring->desc_v1;	
		descsize = sizeof(struct nfe_desc);
a1256 7
nfe_txintr(struct nfe_softc *sc)
{
	printf("nfe_txintr!\n");
	return (0);
}

int 
d1261 1
a1261 1
	int val;
d1287 2
a1288 1
	return (0);
d1308 1
a1308 1
#ifdef notyet 
d1316 25
d1351 1
a1351 1
	s = splnet();	
@


1.5
log
@Make sure interrupt is properly ack'd.
From damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.4 2005/12/17 11:12:54 jsg Exp $	*/
d771 1
a771 2
	    NFE_TX_RING_COUNT * sizeof(struct nfe_desc), (caddr_t *)desc,
	    BUS_DMA_NOWAIT);
@


1.4
log
@Don't dynamically assign ring size, simplifies ring structures
and code that deals with rings.  We can get away with this
as we only have one type of rx and one type of tx ring at a time
unlike ral(4).

Suggested by and ok damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.3 2005/12/17 09:03:14 jsg Exp $	*/
d354 2
a355 1
	r = NFE_READ(sc, NFE_IRQ_STATUS) & 0x1ff;
@


1.3
log
@Split length and flags up into seperate variables in
descriptors and make use of MCLBYTES for length setting.
Sugested by damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.2 2005/12/14 22:08:20 jsg Exp $	*/
d71 1
a71 1
int	nfe_alloc_rx_ring(struct nfe_softc *, struct nfe_rx_ring *, int);
d75 1
a75 1
int	nfe_alloc_tx_ring(struct nfe_softc *, struct nfe_tx_ring *, int);
d224 1
a224 1
	if (nfe_alloc_tx_ring(sc, &sc->txq, NFE_TX_RING_COUNT) != 0) {
d230 1
a230 1
	if (nfe_alloc_rx_ring(sc, &sc->rxq, NFE_RX_RING_COUNT) != 0) {
d236 2
a237 1
	NFE_WRITE(sc, NFE_RING_SIZE, sc->rxq.count << 16 | sc->txq.count);
d544 1
a544 1
nfe_alloc_rx_ring(struct nfe_softc *sc, struct nfe_rx_ring *ring, int count)
a559 1
	ring->count = count;
d562 2
a563 2
	error = bus_dmamap_create(sc->sc_dmat, count * descsize, 1,
	    count * descsize, 0, BUS_DMA_NOWAIT, &ring->map);
d570 1
a570 1
	error = bus_dmamem_alloc(sc->sc_dmat, count * descsize,
d579 1
a579 1
	    count * descsize, (caddr_t *)desc, BUS_DMA_NOWAIT);
d587 1
a587 1
	    count * descsize, NULL, BUS_DMA_NOWAIT);
d594 1
a594 1
	memset(*desc, 0, count * descsize);
a596 9
	ring->data = malloc(count * sizeof (struct nfe_rx_data), M_DEVBUF,
	    M_NOWAIT);
	if (ring->data == NULL) {
		printf("%s: could not allocate soft data\n",
		    sc->sc_dev.dv_xname);
		error = ENOMEM;
		goto fail;
	}

d600 2
a601 2
	memset(ring->data, 0, count * sizeof (struct nfe_rx_data));
	for (i = 0; i < count; i++) {
d671 1
a671 1
	for (i = 0; i < ring->count; i++) {
d707 1
a707 1
		    ring->count * descsize);
d711 2
a712 11
	if (ring->data != NULL) {
		for (i = 0; i < ring->count; i++) {
			data = &ring->data[i];

			if (data->m != NULL) {
				bus_dmamap_sync(sc->sc_dmat, data->map, 0,
				    data->map->dm_mapsize,
				    BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(sc->sc_dmat, data->map);
				m_freem(data->m);
			}
d714 6
a719 2
			if (data->map != NULL)
				bus_dmamap_destroy(sc->sc_dmat, data->map);
d721 3
a723 1
		free(ring->data, M_DEVBUF);
d735 1
a735 1
nfe_alloc_tx_ring(struct nfe_softc *sc, struct nfe_tx_ring *ring, int count)
a748 1
	ring->count = count;
d752 2
a753 2
	error = bus_dmamap_create(sc->sc_dmat, count * descsize, 1,
	    count * descsize, 0, BUS_DMA_NOWAIT, &ring->map);
d761 1
a761 1
	error = bus_dmamem_alloc(sc->sc_dmat, count * descsize,
d770 1
a770 1
	    count * sizeof(struct nfe_desc), (caddr_t *)desc,
d779 1
a779 1
	    count * descsize, NULL, BUS_DMA_NOWAIT);
d786 1
a786 1
	memset(desc, 0, count * sizeof(struct nfe_desc));
d789 1
a789 11
	ring->data = malloc(count * sizeof(struct nfe_tx_data), M_DEVBUF,
	    M_NOWAIT);
	if (ring->data == NULL) {
		printf("%s: could not allocate soft data\n",
		    sc->sc_dev.dv_xname);
		error = ENOMEM;
		goto fail;
	}

	memset(ring->data, 0, count * sizeof (struct nfe_tx_data));
	for (i = 0; i < count; i++) {
d813 1
a813 1
	for (i = 0; i < ring->count; i++) {
d862 1
a862 1
		    ring->count * descsize);
d866 2
a867 11
	if (ring->data != NULL) {
		for (i = 0; i < ring->count; i++) {
			data = &ring->data[i];

			if (data->m != NULL) {
				bus_dmamap_sync(sc->sc_dmat, data->map, 0,
				    data->map->dm_mapsize,
				    BUS_DMASYNC_POSTWRITE);
				bus_dmamap_unload(sc->sc_dmat, data->map);
				m_freem(data->m);
			}
d869 6
a874 2
			if (data->map != NULL)
				bus_dmamap_destroy(sc->sc_dmat, data->map);
d876 3
a878 1
		free(ring->data, M_DEVBUF);
@


1.2
log
@We aren't likely to need to keep track of crypted/decrypted ring
bits any time soon... pointed out by damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_nfe.c,v 1.1 2005/12/14 21:54:57 jsg Exp $	*/
a544 2
	struct arpcom *ac = &sc->sc_arpcom;
	struct ifnet *ifp = &ac->ac_if;
a549 1
	int bufsz = ifp->if_mtu + 64;
d654 3
a656 1
			desc_v3->flags = htole32(bufsz | NFE_RX_READY);
d661 2
a662 1
			desc_v1->flags = htole32(bufsz | NFE_RX_READY);
d678 1
a678 3
	struct arpcom *ac = &sc->sc_arpcom;
	struct ifnet *ifp = &ac->ac_if;
	int i, bufsz = ifp->if_mtu + 64;
d682 2
a683 2
			ring->desc_v3[i].flags =
			    htole32(bufsz | NFE_RX_READY);
d685 2
a686 2
			ring->desc_v1[i].flags =
			    htole32(bufsz | NFE_RX_READY);
@


1.1
log
@Initial bits for an nvidia nforce Ethernet driver.
bus_dma usage modelled after ral.  Does not yet see rx interrupts
when testing with ck804.

Nvidia won't give out documentation for this, various "free" operating
systems include a closed source driver, and the Linux people who reverse
engineered it to create a specification won't give it out.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a563 1
	ring->cur_decrypt = 0;
a695 1
	ring->cur_decrypt = 0;
a765 1
	ring->cur_encrypt = ring->next_encrypt = 0;
a864 1
	ring->cur_encrypt = ring->next_encrypt = 0;
@

