head	1.150;
access;
symbols
	OPENBSD_6_1_BASE:1.150
	OPENBSD_6_0:1.132.0.4
	OPENBSD_6_0_BASE:1.132
	OPENBSD_5_9:1.131.0.2
	OPENBSD_5_9_BASE:1.131
	OPENBSD_5_8:1.121.0.4
	OPENBSD_5_8_BASE:1.121
	OPENBSD_5_7:1.117.0.4
	OPENBSD_5_7_BASE:1.117
	OPENBSD_5_6:1.96.0.4
	OPENBSD_5_6_BASE:1.96
	OPENBSD_5_5:1.93.0.4
	OPENBSD_5_5_BASE:1.93
	OPENBSD_5_4:1.89.0.2
	OPENBSD_5_4_BASE:1.89
	OPENBSD_5_3:1.87.0.2
	OPENBSD_5_3_BASE:1.87
	OPENBSD_5_2:1.65.0.2
	OPENBSD_5_2_BASE:1.65
	OPENBSD_5_1_BASE:1.60
	OPENBSD_5_1:1.60.0.2
	OPENBSD_5_0:1.54.0.2
	OPENBSD_5_0_BASE:1.54
	OPENBSD_4_9:1.46.0.2
	OPENBSD_4_9_BASE:1.46
	OPENBSD_4_8:1.42.0.2
	OPENBSD_4_8_BASE:1.42
	OPENBSD_4_7:1.38.0.2
	OPENBSD_4_7_BASE:1.38
	OPENBSD_4_6:1.23.0.4
	OPENBSD_4_6_BASE:1.23
	OPENBSD_4_5:1.15.0.2
	OPENBSD_4_5_BASE:1.15
	OPENBSD_4_4:1.7.0.2
	OPENBSD_4_4_BASE:1.7;
locks; strict;
comment	@ * @;


1.150
date	2017.01.24.03.57.35;	author dlg;	state Exp;
branches;
next	1.149;
commitid	PERtGPXCvlLRRBr8;

1.149
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.148;
commitid	VyLWTsbepAOk7VQM;

1.148
date	2016.12.09.17.41.39;	author mikeb;	state Exp;
branches;
next	1.147;
commitid	KXDS4n2to8ifzq4G;

1.147
date	2016.12.06.16.13.01;	author mikeb;	state Exp;
branches;
next	1.146;
commitid	6jP4hByrGqLFUvnM;

1.146
date	2016.12.02.15.10.53;	author mikeb;	state Exp;
branches;
next	1.145;
commitid	NCV1BCvONEWzevvh;

1.145
date	2016.11.30.16.15.44;	author mikeb;	state Exp;
branches;
next	1.144;
commitid	ebEqqIfY6dCXQyAf;

1.144
date	2016.11.24.17.39.49;	author mikeb;	state Exp;
branches;
next	1.143;
commitid	CSMhgwhQh4b9XznH;

1.143
date	2016.11.24.11.50.58;	author mikeb;	state Exp;
branches;
next	1.142;
commitid	EVRdcKGvdvEnNdFA;

1.142
date	2016.11.21.17.21.33;	author mikeb;	state Exp;
branches;
next	1.141;
commitid	1ZsCHVQzk7OidRw1;

1.141
date	2016.11.21.16.46.29;	author mikeb;	state Exp;
branches;
next	1.140;
commitid	AYjVVaxmrRzPuEOh;

1.140
date	2016.11.21.12.37.16;	author jsg;	state Exp;
branches;
next	1.139;
commitid	gb6Dqolm5NnuQ9Jh;

1.139
date	2016.11.21.12.05.28;	author mikeb;	state Exp;
branches;
next	1.138;
commitid	hkiHVNNfYrxphjhz;

1.138
date	2016.11.18.20.03.30;	author mikeb;	state Exp;
branches;
next	1.137;
commitid	Ag09nzkKHoSPqToI;

1.137
date	2016.11.18.19.49.21;	author mikeb;	state Exp;
branches;
next	1.136;
commitid	bw3Zx09BzFptNI4f;

1.136
date	2016.11.18.14.16.10;	author mikeb;	state Exp;
branches;
next	1.135;
commitid	9bYhS913EosfjXBD;

1.135
date	2016.11.18.13.45.57;	author mikeb;	state Exp;
branches;
next	1.134;
commitid	0ADnhIdNpZEhKmhP;

1.134
date	2016.11.18.13.37.00;	author mikeb;	state Exp;
branches;
next	1.133;
commitid	qrtoz4LP1sEe1tOk;

1.133
date	2016.10.27.03.06.53;	author dlg;	state Exp;
branches;
next	1.132;
commitid	tMxqIWVTjiRzeFoN;

1.132
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.131;
commitid	8YSL8ByWzGeIGBiJ;

1.131
date	2015.12.31.19.07.37;	author kettenis;	state Exp;
branches;
next	1.130;
commitid	pcDqoxBN9zeO7DHP;

1.130
date	2015.12.18.22.47.18;	author kettenis;	state Exp;
branches;
next	1.129;
commitid	eruQqs4DApDRRWk4;

1.129
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.128;
commitid	B0kwmVGiD5DVx4kv;

1.128
date	2015.11.20.03.35.23;	author dlg;	state Exp;
branches;
next	1.127;
commitid	eYnPulzvLjDImPCa;

1.127
date	2015.11.04.00.20.35;	author dlg;	state Exp;
branches;
next	1.126;
commitid	jhKHSqAqYIuLV5XY;

1.126
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.125;
commitid	hPF95ClMUQfeqQDX;

1.125
date	2015.09.11.12.09.10;	author claudio;	state Exp;
branches;
next	1.124;
commitid	CCeURkmrLTdsL7lJ;

1.124
date	2015.09.02.16.08.49;	author deraadt;	state Exp;
branches;
next	1.123;
commitid	eg6oejPsZN8JiDRh;

1.123
date	2015.09.01.07.09.55;	author deraadt;	state Exp;
branches;
next	1.122;
commitid	VvLv8PeakqoJLqr3;

1.122
date	2015.08.29.17.40.09;	author kettenis;	state Exp;
branches;
next	1.121;
commitid	jluYW9qUFg2qYU9N;

1.121
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.120;
commitid	MVWrtktB46JRxFWT;

1.120
date	2015.05.21.07.39.52;	author gerhard;	state Exp;
branches;
next	1.119;
commitid	QbSVrkI23yflKYNH;

1.119
date	2015.04.30.14.17.26;	author jsg;	state Exp;
branches;
next	1.118;
commitid	kHonDANUeRfBUHle;

1.118
date	2015.03.20.10.41.15;	author mikeb;	state Exp;
branches;
next	1.117;
commitid	nc2boy9G0tluSJV8;

1.117
date	2015.02.12.04.58.59;	author dlg;	state Exp;
branches;
next	1.116;
commitid	vz5uDBe25d792SjC;

1.116
date	2015.01.20.12.56.50;	author kettenis;	state Exp;
branches;
next	1.115;
commitid	Pgdm1Ig3jgE9N2J9;

1.115
date	2015.01.12.10.40.51;	author mikeb;	state Exp;
branches;
next	1.114;
commitid	GciQdfBD1QeqRhMI;

1.114
date	2015.01.11.03.06.19;	author deraadt;	state Exp;
branches;
next	1.113;
commitid	rJEvQQYge6G5b2Nl;

1.113
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.112;
commitid	yM2VFFhpDTeFQlve;

1.112
date	2014.12.13.21.05.33;	author doug;	state Exp;
branches;
next	1.111;
commitid	20ZyHa9gTJxHxhwD;

1.111
date	2014.11.27.15.49.13;	author brad;	state Exp;
branches;
next	1.110;
commitid	lCFpCQzqJwkkqpGr;

1.110
date	2014.11.27.07.54.06;	author kettenis;	state Exp;
branches;
next	1.109;
commitid	4rTXbYVU1B1v2EdX;

1.109
date	2014.11.26.00.09.27;	author dlg;	state Exp;
branches;
next	1.108;
commitid	t9FfQHyifN7TDZad;

1.108
date	2014.11.20.03.35.56;	author brad;	state Exp;
branches;
next	1.107;
commitid	sz4W4Kn16lHd7EIu;

1.107
date	2014.11.12.16.06.47;	author mikeb;	state Exp;
branches;
next	1.106;
commitid	iwiZZRLVhfz0hPwH;

1.106
date	2014.11.10.17.53.43;	author mikeb;	state Exp;
branches;
next	1.105;
commitid	43QmpLnrdVerUeLk;

1.105
date	2014.11.10.17.53.10;	author mikeb;	state Exp;
branches;
next	1.104;
commitid	TFJT2967LY1vVzAc;

1.104
date	2014.11.10.17.07.52;	author mikeb;	state Exp;
branches;
next	1.103;
commitid	lxoW7c5AJF8sNcmS;

1.103
date	2014.11.10.16.35.06;	author mikeb;	state Exp;
branches;
next	1.102;
commitid	SdooySLd82ZPzttz;

1.102
date	2014.11.10.16.01.18;	author mikeb;	state Exp;
branches;
next	1.101;
commitid	FTgR7fl3gd5X02O3;

1.101
date	2014.11.10.15.58.32;	author mikeb;	state Exp;
branches;
next	1.100;
commitid	fEGe9isBECvsktRN;

1.100
date	2014.09.08.02.39.57;	author chris;	state Exp;
branches;
next	1.99;
commitid	NEDaPrisdrolvDTJ;

1.99
date	2014.08.26.11.01.22;	author mikeb;	state Exp;
branches;
next	1.98;
commitid	NTysWaOWh0v3MRJC;

1.98
date	2014.08.25.14.26.25;	author mikeb;	state Exp;
branches;
next	1.97;
commitid	8deZACRIBvIf8lft;

1.97
date	2014.08.20.10.06.31;	author mikeb;	state Exp;
branches;
next	1.96;
commitid	btSYgQLFkTEyMUfL;

1.96
date	2014.07.13.23.10.23;	author deraadt;	state Exp;
branches
	1.96.4.1;
next	1.95;
commitid	JtO5uXxVcnZfhUkR;

1.95
date	2014.07.12.18.48.51;	author tedu;	state Exp;
branches;
next	1.94;
commitid	OBNa5kfxQ2UXoiIw;

1.94
date	2014.07.08.05.35.18;	author dlg;	state Exp;
branches;
next	1.93;
commitid	0QJleeeWqZmC5anF;

1.93
date	2013.12.09.19.48.04;	author mikeb;	state Exp;
branches;
next	1.92;

1.92
date	2013.08.23.19.44.14;	author mikeb;	state Exp;
branches;
next	1.91;

1.91
date	2013.08.21.13.52.57;	author mpi;	state Exp;
branches;
next	1.90;

1.90
date	2013.08.05.19.58.05;	author mikeb;	state Exp;
branches;
next	1.89;

1.89
date	2013.06.14.16.25.54;	author mikeb;	state Exp;
branches;
next	1.88;

1.88
date	2013.06.09.18.09.55;	author deraadt;	state Exp;
branches;
next	1.87;

1.87
date	2012.12.20.17.34.54;	author mikeb;	state Exp;
branches;
next	1.86;

1.86
date	2012.12.20.17.07.37;	author mikeb;	state Exp;
branches;
next	1.85;

1.85
date	2012.12.17.18.30.28;	author mikeb;	state Exp;
branches;
next	1.84;

1.84
date	2012.12.17.14.39.28;	author mikeb;	state Exp;
branches;
next	1.83;

1.83
date	2012.12.17.14.23.48;	author mikeb;	state Exp;
branches;
next	1.82;

1.82
date	2012.12.17.14.03.03;	author mikeb;	state Exp;
branches;
next	1.81;

1.81
date	2012.12.17.13.46.23;	author mikeb;	state Exp;
branches;
next	1.80;

1.80
date	2012.12.17.12.28.06;	author mikeb;	state Exp;
branches;
next	1.79;

1.79
date	2012.12.17.12.03.16;	author mikeb;	state Exp;
branches;
next	1.78;

1.78
date	2012.12.05.12.21.12;	author mikeb;	state Exp;
branches;
next	1.77;

1.77
date	2012.11.29.21.10.32;	author brad;	state Exp;
branches;
next	1.76;

1.76
date	2012.11.29.13.23.00;	author mikeb;	state Exp;
branches;
next	1.75;

1.75
date	2012.11.23.04.34.11;	author brad;	state Exp;
branches;
next	1.74;

1.74
date	2012.11.08.09.18.37;	author mikeb;	state Exp;
branches;
next	1.73;

1.73
date	2012.11.06.17.29.39;	author mikeb;	state Exp;
branches;
next	1.72;

1.72
date	2012.10.28.12.21.57;	author brad;	state Exp;
branches;
next	1.71;

1.71
date	2012.08.13.13.14.50;	author mikeb;	state Exp;
branches;
next	1.70;

1.70
date	2012.08.11.06.53.31;	author mikeb;	state Exp;
branches;
next	1.69;

1.69
date	2012.08.10.10.56.22;	author mikeb;	state Exp;
branches;
next	1.68;

1.68
date	2012.08.08.14.44.13;	author mikeb;	state Exp;
branches;
next	1.67;

1.67
date	2012.08.06.21.07.52;	author mikeb;	state Exp;
branches;
next	1.66;

1.66
date	2012.07.29.13.49.03;	author mikeb;	state Exp;
branches;
next	1.65;

1.65
date	2012.07.06.11.08.44;	author mikeb;	state Exp;
branches;
next	1.64;

1.64
date	2012.07.05.14.47.28;	author mikeb;	state Exp;
branches;
next	1.63;

1.63
date	2012.07.05.14.36.22;	author mikeb;	state Exp;
branches;
next	1.62;

1.62
date	2012.02.26.16.22.37;	author mikeb;	state Exp;
branches;
next	1.61;

1.61
date	2012.02.26.16.12.34;	author mikeb;	state Exp;
branches;
next	1.60;

1.60
date	2012.01.20.14.48.49;	author mikeb;	state Exp;
branches;
next	1.59;

1.59
date	2012.01.13.09.38.23;	author mikeb;	state Exp;
branches;
next	1.58;

1.58
date	2011.12.12.20.45.30;	author mikeb;	state Exp;
branches;
next	1.57;

1.57
date	2011.12.09.11.43.41;	author mikeb;	state Exp;
branches;
next	1.56;

1.56
date	2011.11.27.16.14.31;	author mikeb;	state Exp;
branches;
next	1.55;

1.55
date	2011.11.27.16.10.23;	author mikeb;	state Exp;
branches;
next	1.54;

1.54
date	2011.06.18.21.19.44;	author claudio;	state Exp;
branches;
next	1.53;

1.53
date	2011.06.15.00.03.00;	author dlg;	state Exp;
branches;
next	1.52;

1.52
date	2011.06.10.12.46.35;	author claudio;	state Exp;
branches;
next	1.51;

1.51
date	2011.04.15.15.12.27;	author chl;	state Exp;
branches;
next	1.50;

1.50
date	2011.04.13.00.14.18;	author dlg;	state Exp;
branches;
next	1.49;

1.49
date	2011.04.07.15.30.16;	author miod;	state Exp;
branches;
next	1.48;

1.48
date	2011.04.05.18.01.21;	author henning;	state Exp;
branches;
next	1.47;

1.47
date	2011.04.03.15.36.02;	author jasper;	state Exp;
branches;
next	1.46;

1.46
date	2010.11.10.15.23.25;	author claudio;	state Exp;
branches;
next	1.45;

1.45
date	2010.10.27.20.48.27;	author deraadt;	state Exp;
branches;
next	1.44;

1.44
date	2010.08.27.08.24.53;	author deraadt;	state Exp;
branches;
next	1.43;

1.43
date	2010.08.11.11.35.09;	author jsg;	state Exp;
branches;
next	1.42;

1.42
date	2010.04.20.14.54.58;	author jsg;	state Exp;
branches;
next	1.41;

1.41
date	2010.03.22.17.20.27;	author jsg;	state Exp;
branches;
next	1.40;

1.40
date	2010.03.22.17.09.27;	author jsg;	state Exp;
branches;
next	1.39;

1.39
date	2010.03.19.13.08.56;	author jsg;	state Exp;
branches;
next	1.38;

1.38
date	2010.03.16.22.48.43;	author kettenis;	state Exp;
branches;
next	1.37;

1.37
date	2010.02.25.10.56.07;	author jsg;	state Exp;
branches;
next	1.36;

1.36
date	2010.02.23.18.43.15;	author jsg;	state Exp;
branches;
next	1.35;

1.35
date	2010.02.19.18.55.12;	author jsg;	state Exp;
branches;
next	1.34;

1.34
date	2010.01.19.13.48.13;	author reyk;	state Exp;
branches;
next	1.33;

1.33
date	2010.01.11.02.04.25;	author reyk;	state Exp;
branches;
next	1.32;

1.32
date	2010.01.09.05.44.41;	author reyk;	state Exp;
branches;
next	1.31;

1.31
date	2010.01.09.05.30.19;	author reyk;	state Exp;
branches;
next	1.30;

1.30
date	2009.08.13.14.24.47;	author jasper;	state Exp;
branches;
next	1.29;

1.29
date	2009.08.12.20.02.42;	author dlg;	state Exp;
branches;
next	1.28;

1.28
date	2009.08.12.16.56.59;	author jsg;	state Exp;
branches;
next	1.27;

1.27
date	2009.08.12.14.39.05;	author dlg;	state Exp;
branches;
next	1.26;

1.26
date	2009.08.10.19.41.05;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	2009.08.09.11.40.56;	author deraadt;	state Exp;
branches;
next	1.24;

1.24
date	2009.07.10.12.00.52;	author dlg;	state Exp;
branches;
next	1.23;

1.23
date	2009.06.29.16.40.46;	author jsg;	state Exp;
branches;
next	1.22;

1.22
date	2009.06.28.22.20.20;	author jsg;	state Exp;
branches;
next	1.21;

1.21
date	2009.06.28.22.05.36;	author jsg;	state Exp;
branches;
next	1.20;

1.20
date	2009.06.25.17.01.32;	author deraadt;	state Exp;
branches;
next	1.19;

1.19
date	2009.06.24.13.36.56;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2009.06.04.22.27.31;	author jsg;	state Exp;
branches;
next	1.17;

1.17
date	2009.04.29.13.18.58;	author jsg;	state Exp;
branches;
next	1.16;

1.16
date	2009.04.24.12.54.15;	author jsg;	state Exp;
branches;
next	1.15;

1.15
date	2008.11.28.02.44.18;	author brad;	state Exp;
branches;
next	1.14;

1.14
date	2008.11.09.15.08.26;	author naddy;	state Exp;
branches;
next	1.13;

1.13
date	2008.10.28.05.39.18;	author brad;	state Exp;
branches;
next	1.12;

1.12
date	2008.10.16.19.18.03;	author naddy;	state Exp;
branches;
next	1.11;

1.11
date	2008.10.16.19.16.21;	author naddy;	state Exp;
branches;
next	1.10;

1.10
date	2008.10.11.20.31.50;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2008.10.02.20.21.14;	author brad;	state Exp;
branches;
next	1.8;

1.8
date	2008.09.10.14.01.22;	author blambert;	state Exp;
branches;
next	1.7;

1.7
date	2008.06.19.08.43.55;	author reyk;	state Exp;
branches;
next	1.6;

1.6
date	2008.06.09.07.07.16;	author djm;	state Exp;
branches;
next	1.5;

1.5
date	2008.06.08.21.15.34;	author reyk;	state Exp;
branches;
next	1.4;

1.4
date	2008.06.08.20.58.42;	author reyk;	state Exp;
branches;
next	1.3;

1.3
date	2008.06.08.20.36.34;	author reyk;	state Exp;
branches;
next	1.2;

1.2
date	2008.06.08.20.33.51;	author reyk;	state Exp;
branches;
next	1.1;

1.1
date	2008.06.08.20.01.02;	author reyk;	state Exp;
branches;
next	;

1.96.4.1
date	2014.09.07.03.15.42;	author jsg;	state Exp;
branches;
next	;
commitid	WD5M7v5ollO6mSXH;


desc
@@


1.150
log
@add support for multiple transmit ifqueues per network interface.

an ifq to transmit a packet is picked by the current traffic
conditioner (ie, priq or hfsc) by providing an index into an array
of ifqs. by default interfaces get a single ifq but can ask for
more using if_attach_queues().

the vast majority of our drivers still think there's a 1:1 mapping
between interfaces and transmit queues, so their if_start routines
take an ifnet pointer instead of a pointer to the ifqueue struct.
instead of changing all the drivers in the tree, drivers can opt
into using an if_qstart routine and setting the IFXF_MPSAFE flag.
the stack provides a compatability wrapper from the new if_qstart
handler to the previous if_start handlers if IFXF_MPSAFE isnt set.

enabling hfsc on an interface configures it to transmit everything
through the first ifq. any other ifqs are left configured as priq,
but unused, when hfsc is enabled.

getting this in now so everyone can kick the tyres.

ok mpi@@ visa@@ (who provided some tweaks for cnmac).
@
text
@/*	$OpenBSD: if_ix.c,v 1.149 2017/01/22 10:17:38 dlg Exp $	*/

/******************************************************************************

  Copyright (c) 2001-2013, Intel Corporation
  All rights reserved.

  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions are met:

   1. Redistributions of source code must retain the above copyright notice,
      this list of conditions and the following disclaimer.

   2. Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.

   3. Neither the name of the Intel Corporation nor the names of its
      contributors may be used to endorse or promote products derived from
      this software without specific prior written permission.

  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  POSSIBILITY OF SUCH DAMAGE.

******************************************************************************/
/* FreeBSD: src/sys/dev/ixgbe/ixgbe.c 251964 Jun 18 21:28:19 2013 UTC */

#include <dev/pci/if_ix.h>
#include <dev/pci/ixgbe_type.h>

/*********************************************************************
 *  Driver version
 *********************************************************************/
/* char ixgbe_driver_version[] = "2.5.13"; */

/*********************************************************************
 *  PCI Device ID Table
 *
 *  Used by probe to select devices to load on
 *********************************************************************/

const struct pci_matchid ixgbe_devices[] = {
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598_BX },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598AF_DUAL },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598AF },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598AT },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598AT2 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598AT_DUAL },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598EB_CX4 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598EB_CX4_DUAL },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598EB_XF_LR },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598EB_SFP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598_SR_DUAL_EM },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598_DA_DUAL },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_KX4 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_KX4_MEZZ },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_XAUI },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_COMBO_BP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_BPLANE_FCOE },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_CX4 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_T3_LOM },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP_EM },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP_SF_QP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP_SF2 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP_FCOE },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599EN_SFP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_QSFP_SF_QP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X540T },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X540T1 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550T },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550T1 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550EM_X_KX4 },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550EM_X_KR },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550EM_X_SFP },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550EM_X_10G_T },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_X550EM_X_1G_T },
};

/*********************************************************************
 *  Function prototypes
 *********************************************************************/
int	ixgbe_probe(struct device *, void *, void *);
void	ixgbe_attach(struct device *, struct device *, void *);
int	ixgbe_detach(struct device *, int);
void	ixgbe_start(struct ifqueue *);
int	ixgbe_ioctl(struct ifnet *, u_long, caddr_t);
int	ixgbe_rxrinfo(struct ix_softc *, struct if_rxrinfo *);
void	ixgbe_watchdog(struct ifnet *);
void	ixgbe_init(void *);
void	ixgbe_stop(void *);
void	ixgbe_media_status(struct ifnet *, struct ifmediareq *);
int	ixgbe_media_change(struct ifnet *);
void	ixgbe_identify_hardware(struct ix_softc *);
int	ixgbe_allocate_pci_resources(struct ix_softc *);
int	ixgbe_allocate_legacy(struct ix_softc *);
int	ixgbe_allocate_queues(struct ix_softc *);
void	ixgbe_free_pci_resources(struct ix_softc *);
void	ixgbe_local_timer(void *);
void	ixgbe_setup_interface(struct ix_softc *);
void	ixgbe_config_gpie(struct ix_softc *);
void	ixgbe_config_delay_values(struct ix_softc *);
void	ixgbe_add_media_types(struct ix_softc *);
void	ixgbe_config_link(struct ix_softc *);

int	ixgbe_allocate_transmit_buffers(struct tx_ring *);
int	ixgbe_setup_transmit_structures(struct ix_softc *);
int	ixgbe_setup_transmit_ring(struct tx_ring *);
void	ixgbe_initialize_transmit_units(struct ix_softc *);
void	ixgbe_free_transmit_structures(struct ix_softc *);
void	ixgbe_free_transmit_buffers(struct tx_ring *);

int	ixgbe_allocate_receive_buffers(struct rx_ring *);
int	ixgbe_setup_receive_structures(struct ix_softc *);
int	ixgbe_setup_receive_ring(struct rx_ring *);
void	ixgbe_initialize_receive_units(struct ix_softc *);
void	ixgbe_free_receive_structures(struct ix_softc *);
void	ixgbe_free_receive_buffers(struct rx_ring *);
void	ixgbe_initialize_rss_mapping(struct ix_softc *);
int	ixgbe_rxfill(struct rx_ring *);
void	ixgbe_rxrefill(void *);

void	ixgbe_enable_intr(struct ix_softc *);
void	ixgbe_disable_intr(struct ix_softc *);
void	ixgbe_update_stats_counters(struct ix_softc *);
int	ixgbe_txeof(struct tx_ring *);
int	ixgbe_rxeof(struct ix_queue *);
void	ixgbe_rx_checksum(uint32_t, struct mbuf *, uint32_t);
void	ixgbe_iff(struct ix_softc *);
#ifdef IX_DEBUG
void	ixgbe_print_hw_stats(struct ix_softc *);
#endif
void	ixgbe_update_link_status(struct ix_softc *);
int	ixgbe_get_buf(struct rx_ring *, int);
int	ixgbe_encap(struct tx_ring *, struct mbuf *);
int	ixgbe_dma_malloc(struct ix_softc *, bus_size_t,
		    struct ixgbe_dma_alloc *, int);
void	ixgbe_dma_free(struct ix_softc *, struct ixgbe_dma_alloc *);
int	ixgbe_tx_ctx_setup(struct tx_ring *, struct mbuf *, uint32_t *,
	    uint32_t *);
int	ixgbe_tso_setup(struct tx_ring *, struct mbuf *, uint32_t *,
	    uint32_t *);
void	ixgbe_set_ivar(struct ix_softc *, uint8_t, uint8_t, int8_t);
void	ixgbe_configure_ivars(struct ix_softc *);
uint8_t	*ixgbe_mc_array_itr(struct ixgbe_hw *, uint8_t **, uint32_t *);

void	ixgbe_setup_vlan_hw_support(struct ix_softc *);

/* Support for pluggable optic modules */
void	ixgbe_setup_optics(struct ix_softc *);
void	ixgbe_handle_mod(struct ix_softc *);
void	ixgbe_handle_msf(struct ix_softc *);
void	ixgbe_handle_phy(struct ix_softc *);

/* Legacy (single vector interrupt handler */
int	ixgbe_intr(void *);
void	ixgbe_enable_queue(struct ix_softc *, uint32_t);
void	ixgbe_disable_queue(struct ix_softc *, uint32_t);
void	ixgbe_rearm_queue(struct ix_softc *, uint32_t);

/*********************************************************************
 *  OpenBSD Device Interface Entry Points
 *********************************************************************/

struct cfdriver ix_cd = {
	NULL, "ix", DV_IFNET
};

struct cfattach ix_ca = {
	sizeof(struct ix_softc), ixgbe_probe, ixgbe_attach, ixgbe_detach
};

int ixgbe_smart_speed = ixgbe_smart_speed_on;

/*********************************************************************
 *  Device identification routine
 *
 *  ixgbe_probe determines if the driver should be loaded on
 *  adapter based on PCI vendor/device id of the adapter.
 *
 *  return 0 on success, positive on failure
 *********************************************************************/

int
ixgbe_probe(struct device *parent, void *match, void *aux)
{
	INIT_DEBUGOUT("ixgbe_probe: begin");

	return (pci_matchbyid((struct pci_attach_args *)aux, ixgbe_devices,
	    nitems(ixgbe_devices)));
}

/*********************************************************************
 *  Device initialization routine
 *
 *  The attach entry point is called when the driver is being loaded.
 *  This routine identifies the type of hardware, allocates all resources
 *  and initializes the hardware.
 *
 *  return 0 on success, positive on failure
 *********************************************************************/

void
ixgbe_attach(struct device *parent, struct device *self, void *aux)
{
	struct pci_attach_args	*pa = (struct pci_attach_args *)aux;
	struct ix_softc		*sc = (struct ix_softc *)self;
	int			 error = 0;
	uint16_t		 csum;
	uint32_t			 ctrl_ext;
	struct ixgbe_hw		*hw = &sc->hw;

	INIT_DEBUGOUT("ixgbe_attach: begin");

	sc->osdep.os_sc = sc;
	sc->osdep.os_pa = *pa;

	/* Set up the timer callout */
	timeout_set(&sc->timer, ixgbe_local_timer, sc);
	timeout_set(&sc->rx_refill, ixgbe_rxrefill, sc);

	/* Determine hardware revision */
	ixgbe_identify_hardware(sc);

	/* Indicate to RX setup to use Jumbo Clusters */
	sc->num_tx_desc = DEFAULT_TXD;
	sc->num_rx_desc = DEFAULT_RXD;

	/* Do base PCI setup - map BAR0 */
	if (ixgbe_allocate_pci_resources(sc))
		goto err_out;

	/* Allocate our TX/RX Queues */
	if (ixgbe_allocate_queues(sc))
		goto err_out;

	/* Allocate multicast array memory. */
	sc->mta = mallocarray(IXGBE_ETH_LENGTH_OF_ADDRESS,
	    MAX_NUM_MULTICAST_ADDRESSES, M_DEVBUF, M_NOWAIT);
	if (sc->mta == NULL) {
		printf(": Can not allocate multicast setup array\n");
		goto err_late;
	}

	/* Initialize the shared code */
	error = ixgbe_init_shared_code(hw);
	if (error) {
		printf(": Unable to initialize the shared code\n");
		goto err_late;
	}

	/* Make sure we have a good EEPROM before we read from it */
	if (sc->hw.eeprom.ops.validate_checksum(&sc->hw, &csum) < 0) {
		printf(": The EEPROM Checksum Is Not Valid\n");
		goto err_late;
	}

	error = ixgbe_init_hw(hw);
	if (error == IXGBE_ERR_EEPROM_VERSION) {
		printf(": This device is a pre-production adapter/"
		    "LOM.  Please be aware there may be issues associated "
		    "with your hardware.\nIf you are experiencing problems "
		    "please contact your Intel or hardware representative "
		    "who provided you with this hardware.\n");
	} else if (error && (error != IXGBE_ERR_SFP_NOT_PRESENT &&
	    error != IXGBE_ERR_SFP_NOT_SUPPORTED)) {
		printf(": Hardware Initialization Failure\n");
		goto err_late;
	}

	/* Detect and set physical type */
	ixgbe_setup_optics(sc);

	bcopy(sc->hw.mac.addr, sc->arpcom.ac_enaddr,
	    IXGBE_ETH_LENGTH_OF_ADDRESS);

	error = ixgbe_allocate_legacy(sc);
	if (error)
		goto err_late;

	/* Enable the optics for 82599 SFP+ fiber */
	if (sc->hw.mac.ops.enable_tx_laser)
		sc->hw.mac.ops.enable_tx_laser(&sc->hw);

	/* Enable power to the phy */
	if (hw->phy.ops.set_phy_power)
		hw->phy.ops.set_phy_power(&sc->hw, TRUE);

	/* Setup OS specific network interface */
	ixgbe_setup_interface(sc);

	/* Initialize statistics */
	ixgbe_update_stats_counters(sc);

	/* Get the PCI-E bus info and determine LAN ID */
	hw->mac.ops.get_bus_info(hw);

	/* Set an initial default flow control value */
	sc->fc = ixgbe_fc_full;

	/* let hardware know driver is loaded */
	ctrl_ext = IXGBE_READ_REG(&sc->hw, IXGBE_CTRL_EXT);
	ctrl_ext |= IXGBE_CTRL_EXT_DRV_LOAD;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_CTRL_EXT, ctrl_ext);

	printf(", address %s\n", ether_sprintf(sc->hw.mac.addr));

	INIT_DEBUGOUT("ixgbe_attach: end");
	return;

err_late:
	ixgbe_free_transmit_structures(sc);
	ixgbe_free_receive_structures(sc);
err_out:
	ixgbe_free_pci_resources(sc);
	free(sc->mta, M_DEVBUF, IXGBE_ETH_LENGTH_OF_ADDRESS *
	    MAX_NUM_MULTICAST_ADDRESSES);
}

/*********************************************************************
 *  Device removal routine
 *
 *  The detach entry point is called when the driver is being removed.
 *  This routine stops the adapter and deallocates all the resources
 *  that were allocated for driver operation.
 *
 *  return 0 on success, positive on failure
 *********************************************************************/

int
ixgbe_detach(struct device *self, int flags)
{
	struct ix_softc *sc = (struct ix_softc *)self;
	struct ifnet *ifp = &sc->arpcom.ac_if;
	uint32_t	ctrl_ext;

	INIT_DEBUGOUT("ixgbe_detach: begin");

	ixgbe_stop(sc);

	/* let hardware know driver is unloading */
	ctrl_ext = IXGBE_READ_REG(&sc->hw, IXGBE_CTRL_EXT);
	ctrl_ext &= ~IXGBE_CTRL_EXT_DRV_LOAD;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_CTRL_EXT, ctrl_ext);

	ether_ifdetach(ifp);
	if_detach(ifp);

	timeout_del(&sc->timer);
	timeout_del(&sc->rx_refill);
	ixgbe_free_pci_resources(sc);

	ixgbe_free_transmit_structures(sc);
	ixgbe_free_receive_structures(sc);
	free(sc->mta, M_DEVBUF, IXGBE_ETH_LENGTH_OF_ADDRESS *
	    MAX_NUM_MULTICAST_ADDRESSES);

	return (0);
}

/*********************************************************************
 *  Transmit entry point
 *
 *  ixgbe_start is called by the stack to initiate a transmit.
 *  The driver will remain in this routine as long as there are
 *  packets to transmit and transmit resources are available.
 *  In case resources are not available stack is notified and
 *  the packet is requeued.
 **********************************************************************/

void
ixgbe_start(struct ifqueue *ifq)
{
	struct ifnet		*ifp = ifq->ifq_if;
	struct ix_softc		*sc = ifp->if_softc;
	struct tx_ring		*txr = sc->tx_rings;
	struct mbuf  		*m_head;
	int			 post = 0;

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(ifq))
		return;
	if (!sc->link_up)
		return;

	bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map, 0,
	    txr->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);

	for (;;) {
		/* Check that we have the minimal number of TX descriptors. */
		if (txr->tx_avail <= IXGBE_TX_OP_THRESHOLD) {
			ifq_set_oactive(ifq);
			break;
		}

		m_head = ifq_dequeue(ifq);
		if (m_head == NULL)
			break;

		if (ixgbe_encap(txr, m_head)) {
			m_freem(m_head);
			continue;
		}

#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
#endif

		/* Set timeout in case hardware has problems transmitting */
		txr->watchdog_timer = IXGBE_TX_TIMEOUT;
		ifp->if_timer = IXGBE_TX_TIMEOUT;

		post = 1;
	}

	bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
	    0, txr->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	/*
	 * Advance the Transmit Descriptor Tail (Tdt), this tells the
	 * hardware that this frame is available to transmit.
	 */
	if (post)
		IXGBE_WRITE_REG(&sc->hw, IXGBE_TDT(txr->me),
		    txr->next_avail_desc);
}

/*********************************************************************
 *  Ioctl entry point
 *
 *  ixgbe_ioctl is called when the user wants to configure the
 *  interface.
 *
 *  return 0 on success, positive on failure
 **********************************************************************/

int
ixgbe_ioctl(struct ifnet * ifp, u_long command, caddr_t data)
{
	struct ix_softc	*sc = ifp->if_softc;
	struct ifreq	*ifr = (struct ifreq *) data;
	int		s, error = 0;

	s = splnet();

	switch (command) {
	case SIOCSIFADDR:
		IOCTL_DEBUGOUT("ioctl: SIOCxIFADDR (Get/Set Interface Addr)");
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			ixgbe_init(sc);
		break;

	case SIOCSIFFLAGS:
		IOCTL_DEBUGOUT("ioctl: SIOCSIFFLAGS (Set Interface Flags)");
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				ixgbe_init(sc);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				ixgbe_stop(sc);
		}
		break;

	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		IOCTL_DEBUGOUT("ioctl: SIOCxIFMEDIA (Get/Set Interface Media)");
		error = ifmedia_ioctl(ifp, ifr, &sc->media, command);
		break;

	case SIOCGIFRXR:
		error = ixgbe_rxrinfo(sc, (struct if_rxrinfo *)ifr->ifr_data);
		break;

	default:
		error = ether_ioctl(ifp, &sc->arpcom, command, data);
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING) {
			ixgbe_disable_intr(sc);
			ixgbe_iff(sc);
			ixgbe_enable_intr(sc);
		}
		error = 0;
	}

	splx(s);
	return (error);
}

int
ixgbe_rxrinfo(struct ix_softc *sc, struct if_rxrinfo *ifri)
{
	struct if_rxring_info *ifr, ifr1;
	struct rx_ring *rxr;
	int error, i;
	u_int n = 0;

	if (sc->num_queues > 1) {
		if ((ifr = mallocarray(sc->num_queues, sizeof(*ifr), M_DEVBUF,
		    M_WAITOK | M_ZERO)) == NULL)
			return (ENOMEM);
	} else
		ifr = &ifr1;

	for (i = 0; i < sc->num_queues; i++) {
		rxr = &sc->rx_rings[i];
		ifr[n].ifr_size = sc->rx_mbuf_sz;
		snprintf(ifr[n].ifr_name, sizeof(ifr[n].ifr_name), "/%d", i);
		ifr[n].ifr_info = rxr->rx_ring;
		n++;
	}

	error = if_rxr_info_ioctl(ifri, sc->num_queues, ifr);

	if (sc->num_queues > 1)
		free(ifr, M_DEVBUF, sc->num_queues * sizeof(*ifr));
	return (error);
}

/*********************************************************************
 *  Watchdog entry point
 *
 **********************************************************************/

void
ixgbe_watchdog(struct ifnet * ifp)
{
	struct ix_softc *sc = (struct ix_softc *)ifp->if_softc;
	struct tx_ring *txr = sc->tx_rings;
	struct ixgbe_hw *hw = &sc->hw;
	int		tx_hang = FALSE;
	int		i;

	/*
	 * The timer is set to 5 every time ixgbe_start() queues a packet.
	 * Anytime all descriptors are clean the timer is set to 0.
	 */
	for (i = 0; i < sc->num_queues; i++, txr++) {
		if (txr->watchdog_timer == 0 || --txr->watchdog_timer)
			continue;
		else {
			tx_hang = TRUE;
			break;
		}
	}
	if (tx_hang == FALSE)
		return;

	/*
	 * If we are in this routine because of pause frames, then don't
	 * reset the hardware.
	 */
	if (!(IXGBE_READ_REG(hw, IXGBE_TFCS) & IXGBE_TFCS_TXON)) {
		for (i = 0; i < sc->num_queues; i++, txr++)
			txr->watchdog_timer = IXGBE_TX_TIMEOUT;
		ifp->if_timer = IXGBE_TX_TIMEOUT;
		return;
	}


	printf("%s: Watchdog timeout -- resetting\n", ifp->if_xname);
	for (i = 0; i < sc->num_queues; i++, txr++) {
		printf("%s: Queue(%d) tdh = %d, hw tdt = %d\n", ifp->if_xname, i,
		    IXGBE_READ_REG(hw, IXGBE_TDH(i)),
		    IXGBE_READ_REG(hw, IXGBE_TDT(i)));
		printf("%s: TX(%d) desc avail = %d, Next TX to Clean = %d\n", ifp->if_xname,
		    i, txr->tx_avail, txr->next_to_clean);
	}
	ifp->if_flags &= ~IFF_RUNNING;
	sc->watchdog_events++;

	ixgbe_init(sc);
}

/*********************************************************************
 *  Init entry point
 *
 *  This routine is used in two ways. It is used by the stack as
 *  init entry point in network interface structure. It is also used
 *  by the driver as a hw/sw initialization routine to get to a
 *  consistent state.
 *
 *  return 0 on success, positive on failure
 **********************************************************************/
#define IXGBE_MHADD_MFS_SHIFT 16

void
ixgbe_init(void *arg)
{
	struct ix_softc	*sc = (struct ix_softc *)arg;
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct rx_ring	*rxr = sc->rx_rings;
	uint32_t	 k, txdctl, rxdctl, rxctrl, mhadd, itr;
	int		 i, s, err;

	INIT_DEBUGOUT("ixgbe_init: begin");

	s = splnet();

	ixgbe_stop(sc);

	/* reprogram the RAR[0] in case user changed it. */
	ixgbe_set_rar(&sc->hw, 0, sc->hw.mac.addr, 0, IXGBE_RAH_AV);

	/* Get the latest mac address, User can use a LAA */
	bcopy(sc->arpcom.ac_enaddr, sc->hw.mac.addr,
	      IXGBE_ETH_LENGTH_OF_ADDRESS);
	ixgbe_set_rar(&sc->hw, 0, sc->hw.mac.addr, 0, 1);
	sc->hw.addr_ctrl.rar_used_count = 1;

	/* Prepare transmit descriptors and buffers */
	if (ixgbe_setup_transmit_structures(sc)) {
		printf("%s: Could not setup transmit structures\n",
		    ifp->if_xname);
		ixgbe_stop(sc);
		splx(s);
		return;
	}

	ixgbe_init_hw(&sc->hw);
	ixgbe_initialize_transmit_units(sc);

	/* Use 2k clusters, even for jumbo frames */
	sc->rx_mbuf_sz = MCLBYTES + ETHER_ALIGN;

	/* Prepare receive descriptors and buffers */
	if (ixgbe_setup_receive_structures(sc)) {
		printf("%s: Could not setup receive structures\n",
		    ifp->if_xname);
		ixgbe_stop(sc);
		splx(s);
		return;
	}

	/* Configure RX settings */
	ixgbe_initialize_receive_units(sc);

	/* Enable SDP & MSIX interrupts based on adapter */
	ixgbe_config_gpie(sc);

	/* Program promiscuous mode and multicast filters. */
	ixgbe_iff(sc);

	/* Set MRU size */
	mhadd = IXGBE_READ_REG(&sc->hw, IXGBE_MHADD);
	mhadd &= ~IXGBE_MHADD_MFS_MASK;
	mhadd |= sc->max_frame_size << IXGBE_MHADD_MFS_SHIFT;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_MHADD, mhadd);

	/* Now enable all the queues */
	for (i = 0; i < sc->num_queues; i++) {
		txdctl = IXGBE_READ_REG(&sc->hw, IXGBE_TXDCTL(i));
		txdctl |= IXGBE_TXDCTL_ENABLE;
		/* Set WTHRESH to 8, burst writeback */
		txdctl |= (8 << 16);
		/*
		 * When the internal queue falls below PTHRESH (16),
		 * start prefetching as long as there are at least
		 * HTHRESH (1) buffers ready.
		 */
		txdctl |= (16 << 0) | (1 << 8);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_TXDCTL(i), txdctl);
	}

	for (i = 0; i < sc->num_queues; i++) {
		rxdctl = IXGBE_READ_REG(&sc->hw, IXGBE_RXDCTL(i));
		if (sc->hw.mac.type == ixgbe_mac_82598EB) {
			/*
			 * PTHRESH = 21
			 * HTHRESH = 4
			 * WTHRESH = 8
			 */
			rxdctl &= ~0x3FFFFF;
			rxdctl |= 0x080420;
		}
		rxdctl |= IXGBE_RXDCTL_ENABLE;
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RXDCTL(i), rxdctl);
		for (k = 0; k < 10; k++) {
			if (IXGBE_READ_REG(&sc->hw, IXGBE_RXDCTL(i)) &
			    IXGBE_RXDCTL_ENABLE)
				break;
			else
				msec_delay(1);
		}
		IXGBE_WRITE_FLUSH(&sc->hw);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(i), rxr->last_desc_filled);
	}

	/* Set up VLAN support and filter */
	ixgbe_setup_vlan_hw_support(sc);

	/* Enable Receive engine */
	rxctrl = IXGBE_READ_REG(&sc->hw, IXGBE_RXCTRL);
	if (sc->hw.mac.type == ixgbe_mac_82598EB)
		rxctrl |= IXGBE_RXCTRL_DMBYPS;
	rxctrl |= IXGBE_RXCTRL_RXEN;
	sc->hw.mac.ops.enable_rx_dma(&sc->hw, rxctrl);

	timeout_add_sec(&sc->timer, 1);

	/* Set up MSI/X routing */
	if (sc->msix > 1) {
		ixgbe_configure_ivars(sc);
		/* Set up auto-mask */
		if (sc->hw.mac.type == ixgbe_mac_82598EB)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAM, IXGBE_EICS_RTX_QUEUE);
		else {
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAM_EX(0), 0xFFFFFFFF);
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAM_EX(1), 0xFFFFFFFF);
		}
	} else {  /* Simple settings for Legacy/MSI */
		ixgbe_set_ivar(sc, 0, 0, 0);
		ixgbe_set_ivar(sc, 0, 0, 1);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAM, IXGBE_EICS_RTX_QUEUE);
	}

	/* Check on any SFP devices that need to be kick-started */
	if (sc->hw.phy.type == ixgbe_phy_none) {
		err = sc->hw.phy.ops.identify(&sc->hw);
		if (err == IXGBE_ERR_SFP_NOT_SUPPORTED) {
			printf("Unsupported SFP+ module type was detected.\n");
			splx(s);
			return;
		}
	}

	/* Setup interrupt moderation */
	itr = (4000000 / IXGBE_INTS_PER_SEC) & 0xff8;
	if (sc->hw.mac.type != ixgbe_mac_82598EB)
		itr |= IXGBE_EITR_LLI_MOD | IXGBE_EITR_CNT_WDIS;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_EITR(0), itr);

	/* Enable power to the phy */
	if (sc->hw.phy.ops.set_phy_power)
		sc->hw.phy.ops.set_phy_power(&sc->hw, TRUE);

	/* Config/Enable Link */
	ixgbe_config_link(sc);

	/* Hardware Packet Buffer & Flow Control setup */
	ixgbe_config_delay_values(sc);

	/* Initialize the FC settings */
	sc->hw.mac.ops.start_hw(&sc->hw);

	/* And now turn on interrupts */
	ixgbe_enable_intr(sc);

	/* Now inform the stack we're ready */
	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	splx(s);
}

void
ixgbe_config_gpie(struct ix_softc *sc)
{
	struct ixgbe_hw	*hw = &sc->hw;
	uint32_t gpie;

	gpie = IXGBE_READ_REG(&sc->hw, IXGBE_GPIE);

	/* Fan Failure Interrupt */
	if (hw->device_id == IXGBE_DEV_ID_82598AT)
		gpie |= IXGBE_SDP1_GPIEN;

	if (sc->hw.mac.type == ixgbe_mac_82599EB) {
		/* Add for Module detection */
		gpie |= IXGBE_SDP2_GPIEN;

		/* Media ready */
		if (hw->device_id != IXGBE_DEV_ID_82599_QSFP_SF_QP)
			gpie |= IXGBE_SDP1_GPIEN;

		/*
		 * Set LL interval to max to reduce the number of low latency
		 * interrupts hitting the card when the ring is getting full.
		 */
		gpie |= 0xf << IXGBE_GPIE_LLI_DELAY_SHIFT;
	}

	if (sc->hw.mac.type == ixgbe_mac_X540 ||
	    hw->device_id == IXGBE_DEV_ID_X550EM_X_SFP ||
	    hw->device_id == IXGBE_DEV_ID_X550EM_X_10G_T) {
		/*
		 * Thermal Failure Detection (X540)
		 * Link Detection (X552 SFP+, X552/X557-AT)
		 */
		gpie |= IXGBE_SDP0_GPIEN_X540;

		/*
		 * Set LL interval to max to reduce the number of low latency
		 * interrupts hitting the card when the ring is getting full.
		 */
		gpie |= 0xf << IXGBE_GPIE_LLI_DELAY_SHIFT;
	}

	if (sc->msix > 1) {
		/* Enable Enhanced MSIX mode */
		gpie |= IXGBE_GPIE_MSIX_MODE;
		gpie |= IXGBE_GPIE_EIAME | IXGBE_GPIE_PBA_SUPPORT |
		    IXGBE_GPIE_OCD;
	}

	IXGBE_WRITE_REG(&sc->hw, IXGBE_GPIE, gpie);
}

/*
 * Requires sc->max_frame_size to be set.
 */
void
ixgbe_config_delay_values(struct ix_softc *sc)
{
	struct ixgbe_hw *hw = &sc->hw;
	uint32_t rxpb, frame, size, tmp;

	frame = sc->max_frame_size;

	/* Calculate High Water */
	switch (hw->mac.type) {
	case ixgbe_mac_X540:
	case ixgbe_mac_X550:
	case ixgbe_mac_X550EM_x:
		tmp = IXGBE_DV_X540(frame, frame);
		break;
	default:
		tmp = IXGBE_DV(frame, frame);
		break;
	}
	size = IXGBE_BT2KB(tmp);
	rxpb = IXGBE_READ_REG(hw, IXGBE_RXPBSIZE(0)) >> 10;
	hw->fc.high_water[0] = rxpb - size;

	/* Now calculate Low Water */
	switch (hw->mac.type) {
	case ixgbe_mac_X540:
	case ixgbe_mac_X550:
	case ixgbe_mac_X550EM_x:
		tmp = IXGBE_LOW_DV_X540(frame);
		break;
	default:
		tmp = IXGBE_LOW_DV(frame);
		break;
	}
	hw->fc.low_water[0] = IXGBE_BT2KB(tmp);

	hw->fc.requested_mode = sc->fc;
	hw->fc.pause_time = IXGBE_FC_PAUSE;
	hw->fc.send_xon = TRUE;
}

/*
 * MSIX Interrupt Handlers
 */
void
ixgbe_enable_queue(struct ix_softc *sc, uint32_t vector)
{
	uint64_t queue = 1ULL << vector;
	uint32_t mask;

	if (sc->hw.mac.type == ixgbe_mac_82598EB) {
		mask = (IXGBE_EIMS_RTX_QUEUE & queue);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMS, mask);
	} else {
		mask = (queue & 0xFFFFFFFF);
		if (mask)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMS_EX(0), mask);
		mask = (queue >> 32);
		if (mask)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMS_EX(1), mask);
	}
}

void
ixgbe_disable_queue(struct ix_softc *sc, uint32_t vector)
{
	uint64_t queue = 1ULL << vector;
	uint32_t mask;

	if (sc->hw.mac.type == ixgbe_mac_82598EB) {
		mask = (IXGBE_EIMS_RTX_QUEUE & queue);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC, mask);
	} else {
		mask = (queue & 0xFFFFFFFF);
		if (mask)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC_EX(0), mask);
		mask = (queue >> 32);
		if (mask)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC_EX(1), mask);
	}
}

/*********************************************************************
 *
 *  Legacy Interrupt Service routine
 *
 **********************************************************************/

int
ixgbe_intr(void *arg)
{
	struct ix_softc	*sc = (struct ix_softc *)arg;
	struct ix_queue *que = sc->queues;
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct tx_ring	*txr = sc->tx_rings;
	struct ixgbe_hw	*hw = &sc->hw;
	uint32_t	 reg_eicr, mod_mask, msf_mask;
	int		 i, refill = 0;

	reg_eicr = IXGBE_READ_REG(&sc->hw, IXGBE_EICR);
	if (reg_eicr == 0) {
		ixgbe_enable_intr(sc);
		return (0);
	}

	if (ISSET(ifp->if_flags, IFF_RUNNING)) {
		ixgbe_rxeof(que);
		ixgbe_txeof(txr);
		refill = 1;
	}

	if (refill) {
		if (ixgbe_rxfill(que->rxr)) {
			/* Advance the Rx Queue "Tail Pointer" */
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(que->rxr->me),
			    que->rxr->last_desc_filled);
		} else
			timeout_add(&sc->rx_refill, 1);
	}

	/* Link status change */
	if (reg_eicr & IXGBE_EICR_LSC) {
		KERNEL_LOCK();
		ixgbe_update_link_status(sc);
		KERNEL_UNLOCK();
		ifq_start(&ifp->if_snd);
	}

	if (hw->mac.type != ixgbe_mac_82598EB) {
		if (reg_eicr & IXGBE_EICR_ECC) {
			printf("%s: CRITICAL: ECC ERROR!! "
			    "Please Reboot!!\n", sc->dev.dv_xname);
			IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_ECC);
		}
		/* Check for over temp condition */
		if (reg_eicr & IXGBE_EICR_TS) {
			printf("%s: CRITICAL: OVER TEMP!! "
			    "PHY IS SHUT DOWN!!\n", ifp->if_xname);
			IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_TS);
		}
	}

	/* Pluggable optics-related interrupt */
	if (ixgbe_is_sfp(hw)) {
		if (hw->device_id == IXGBE_DEV_ID_X550EM_X_SFP) {
			mod_mask = IXGBE_EICR_GPI_SDP0_X540;
			msf_mask = IXGBE_EICR_GPI_SDP1_X540;
		} else if (hw->mac.type == ixgbe_mac_X540 ||
		    hw->mac.type == ixgbe_mac_X550 ||
		    hw->mac.type == ixgbe_mac_X550EM_x) {
			mod_mask = IXGBE_EICR_GPI_SDP2_X540;
			msf_mask = IXGBE_EICR_GPI_SDP1_X540;
		} else {
			mod_mask = IXGBE_EICR_GPI_SDP2;
			msf_mask = IXGBE_EICR_GPI_SDP1;
		}
		if (reg_eicr & mod_mask) {
			/* Clear the interrupt */
			IXGBE_WRITE_REG(hw, IXGBE_EICR, mod_mask);
			KERNEL_LOCK();
			ixgbe_handle_mod(sc);
			KERNEL_UNLOCK();
		} else if ((hw->phy.media_type != ixgbe_media_type_copper) &&
		    (reg_eicr & msf_mask)) {
			/* Clear the interrupt */
			IXGBE_WRITE_REG(hw, IXGBE_EICR, msf_mask);
			KERNEL_LOCK();
			ixgbe_handle_msf(sc);
			KERNEL_UNLOCK();
		}
	}

	/* Check for fan failure */
	if ((hw->device_id == IXGBE_DEV_ID_82598AT) &&
	    (reg_eicr & IXGBE_EICR_GPI_SDP1)) {
		printf("%s: CRITICAL: FAN FAILURE!! "
		    "REPLACE IMMEDIATELY!!\n", ifp->if_xname);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP1);
	}

	/* External PHY interrupt */
	if (hw->device_id == IXGBE_DEV_ID_X550EM_X_10G_T &&
	    (reg_eicr & IXGBE_EICR_GPI_SDP0_X540)) {
		/* Clear the interrupt */
		IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP0_X540);
		KERNEL_LOCK();
		ixgbe_handle_phy(sc);
		KERNEL_UNLOCK();
	}

	for (i = 0; i < sc->num_queues; i++, que++)
		ixgbe_enable_queue(sc, que->msix);

	return (1);
}

/*********************************************************************
 *
 *  Media Ioctl callback
 *
 *  This routine is called whenever the user queries the status of
 *  the interface using ifconfig.
 *
 **********************************************************************/
void
ixgbe_media_status(struct ifnet * ifp, struct ifmediareq *ifmr)
{
	struct ix_softc *sc = ifp->if_softc;

	ifmr->ifm_active = IFM_ETHER;
	ifmr->ifm_status = IFM_AVALID;

	INIT_DEBUGOUT("ixgbe_media_status: begin");
	ixgbe_update_link_status(sc);

	if (LINK_STATE_IS_UP(ifp->if_link_state)) {
		ifmr->ifm_status |= IFM_ACTIVE;

		switch (sc->link_speed) {
		case IXGBE_LINK_SPEED_100_FULL:
			ifmr->ifm_active |= IFM_100_TX | IFM_FDX;
			break;
		case IXGBE_LINK_SPEED_1GB_FULL:
			switch (sc->optics) {
			case IFM_10G_SR: /* multi-speed fiber */
				ifmr->ifm_active |= IFM_1000_SX | IFM_FDX;
				break;
			case IFM_10G_LR: /* multi-speed fiber */
				ifmr->ifm_active |= IFM_1000_LX | IFM_FDX;
				break;
			default:
				ifmr->ifm_active |= sc->optics | IFM_FDX;
				break;
			}
			break;
		case IXGBE_LINK_SPEED_10GB_FULL:
			ifmr->ifm_active |= sc->optics | IFM_FDX;
			break;
		}

		switch (sc->hw.fc.current_mode) {
		case ixgbe_fc_tx_pause:
			ifmr->ifm_active |= IFM_FLOW | IFM_ETH_TXPAUSE;
			break;
		case ixgbe_fc_rx_pause:
			ifmr->ifm_active |= IFM_FLOW | IFM_ETH_RXPAUSE;
			break;
		case ixgbe_fc_full:
			ifmr->ifm_active |= IFM_FLOW | IFM_ETH_RXPAUSE |
			    IFM_ETH_TXPAUSE;
			break;
		default:
			ifmr->ifm_active &= ~(IFM_FLOW | IFM_ETH_RXPAUSE |
			    IFM_ETH_TXPAUSE);
			break;
		}
	}
}

/*********************************************************************
 *
 *  Media Ioctl callback
 *
 *  This routine is called when the user changes speed/duplex using
 *  media/mediopt option with ifconfig.
 *
 **********************************************************************/
int
ixgbe_media_change(struct ifnet *ifp)
{
	struct ix_softc	*sc = ifp->if_softc;
	struct ixgbe_hw	*hw = &sc->hw;
	struct ifmedia	*ifm = &sc->media;
	ixgbe_link_speed speed = 0;

	if (IFM_TYPE(ifm->ifm_media) != IFM_ETHER)
		return (EINVAL);

	if (hw->phy.media_type == ixgbe_media_type_backplane)
		return (ENODEV);

	switch (IFM_SUBTYPE(ifm->ifm_media)) {
		case IFM_AUTO:
		case IFM_10G_T:
			speed |= IXGBE_LINK_SPEED_100_FULL;
		case IFM_10G_SR: /* KR, too */
		case IFM_10G_LR:
		case IFM_10G_CX4: /* KX4 */
			speed |= IXGBE_LINK_SPEED_1GB_FULL;
		case IFM_10G_SFP_CU:
			speed |= IXGBE_LINK_SPEED_10GB_FULL;
			break;
		case IFM_1000_T:
			speed |= IXGBE_LINK_SPEED_100_FULL;
		case IFM_1000_LX:
		case IFM_1000_SX:
		case IFM_1000_CX: /* KX */
			speed |= IXGBE_LINK_SPEED_1GB_FULL;
			break;
		case IFM_100_TX:
			speed |= IXGBE_LINK_SPEED_100_FULL;
			break;
		default:
			return (EINVAL);
	}

	hw->mac.autotry_restart = TRUE;
	hw->mac.ops.setup_link(hw, speed, TRUE);

	return (0);
}

/*********************************************************************
 *
 *  This routine maps the mbufs to tx descriptors, allowing the
 *  TX engine to transmit the packets.
 *  	- return 0 on success, positive on failure
 *
 **********************************************************************/

int
ixgbe_encap(struct tx_ring *txr, struct mbuf *m_head)
{
	struct ix_softc *sc = txr->sc;
	uint32_t	olinfo_status = 0, cmd_type_len;
	int             i, j, error;
	int		first, last = 0;
	bus_dmamap_t	map;
	struct ixgbe_tx_buf *txbuf;
	union ixgbe_adv_tx_desc *txd = NULL;

	/* Basic descriptor defines */
	cmd_type_len = (IXGBE_ADVTXD_DTYP_DATA |
	    IXGBE_ADVTXD_DCMD_IFCS | IXGBE_ADVTXD_DCMD_DEXT);

#if NVLAN > 0
	if (m_head->m_flags & M_VLANTAG)
		cmd_type_len |= IXGBE_ADVTXD_DCMD_VLE;
#endif

	/*
	 * Important to capture the first descriptor
	 * used because it will contain the index of
	 * the one we tell the hardware to report back
	 */
	first = txr->next_avail_desc;
	txbuf = &txr->tx_buffers[first];
	map = txbuf->map;

	/*
	 * Map the packet for DMA.
	 */
	error = bus_dmamap_load_mbuf(txr->txdma.dma_tag, map, m_head,
	    BUS_DMA_NOWAIT);
	switch (error) {
	case 0:
		break;
	case EFBIG:
		if (m_defrag(m_head, M_NOWAIT) == 0 &&
		    (error = bus_dmamap_load_mbuf(txr->txdma.dma_tag, map,
		     m_head, BUS_DMA_NOWAIT)) == 0)
			break;
		/* FALLTHROUGH */
	default:
		sc->no_tx_dma_setup++;
		return (error);
	}

	/* Make certain there are enough descriptors */
	KASSERT(map->dm_nsegs <= txr->tx_avail - 2);

	/*
	 * Set the appropriate offload context
	 * this will becomes the first descriptor.
	 */
	error = ixgbe_tx_ctx_setup(txr, m_head, &cmd_type_len, &olinfo_status);
	if (error)
		goto xmit_fail;

	i = txr->next_avail_desc;
	for (j = 0; j < map->dm_nsegs; j++) {
		txbuf = &txr->tx_buffers[i];
		txd = &txr->tx_base[i];

		txd->read.buffer_addr = htole64(map->dm_segs[j].ds_addr);
		txd->read.cmd_type_len = htole32(txr->txd_cmd |
		    cmd_type_len | map->dm_segs[j].ds_len);
		txd->read.olinfo_status = htole32(olinfo_status);
		last = i; /* descriptor that will get completion IRQ */

		if (++i == sc->num_tx_desc)
			i = 0;

		txbuf->m_head = NULL;
		txbuf->eop_index = -1;
	}

	txd->read.cmd_type_len |=
	    htole32(IXGBE_TXD_CMD_EOP | IXGBE_TXD_CMD_RS);

	txbuf->m_head = m_head;
	/*
	 * Here we swap the map so the last descriptor,
	 * which gets the completion interrupt has the
	 * real map, and the first descriptor gets the
	 * unused map from this descriptor.
	 */
	txr->tx_buffers[first].map = txbuf->map;
	txbuf->map = map;
	bus_dmamap_sync(txr->txdma.dma_tag, map, 0, map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	/* Set the index of the descriptor that will be marked done */
	txbuf = &txr->tx_buffers[first];
	txbuf->eop_index = last;

	membar_producer();

	atomic_sub_int(&txr->tx_avail, map->dm_nsegs);
	txr->next_avail_desc = i;

	++txr->tx_packets;
	return (0);

xmit_fail:
	bus_dmamap_unload(txr->txdma.dma_tag, txbuf->map);
	return (error);
}

void
ixgbe_iff(struct ix_softc *sc)
{
	struct ifnet *ifp = &sc->arpcom.ac_if;
	struct arpcom *ac = &sc->arpcom;
	uint32_t	fctrl;
	uint8_t	*mta;
	uint8_t	*update_ptr;
	struct ether_multi *enm;
	struct ether_multistep step;
	int	mcnt = 0;

	IOCTL_DEBUGOUT("ixgbe_iff: begin");

	mta = sc->mta;
	bzero(mta, sizeof(uint8_t) * IXGBE_ETH_LENGTH_OF_ADDRESS *
	    MAX_NUM_MULTICAST_ADDRESSES);

	fctrl = IXGBE_READ_REG(&sc->hw, IXGBE_FCTRL);
	fctrl &= ~(IXGBE_FCTRL_MPE | IXGBE_FCTRL_UPE);
	ifp->if_flags &= ~IFF_ALLMULTI;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0 ||
	    ac->ac_multicnt > MAX_NUM_MULTICAST_ADDRESSES) {
		ifp->if_flags |= IFF_ALLMULTI;
		fctrl |= IXGBE_FCTRL_MPE;
		if (ifp->if_flags & IFF_PROMISC)
			fctrl |= IXGBE_FCTRL_UPE;
	} else {
		ETHER_FIRST_MULTI(step, &sc->arpcom, enm);
		while (enm != NULL) {
			bcopy(enm->enm_addrlo,
			    &mta[mcnt * IXGBE_ETH_LENGTH_OF_ADDRESS],
			    IXGBE_ETH_LENGTH_OF_ADDRESS);
			mcnt++;

			ETHER_NEXT_MULTI(step, enm);
		}

		update_ptr = mta;
		sc->hw.mac.ops.update_mc_addr_list(&sc->hw, update_ptr, mcnt,
		    ixgbe_mc_array_itr, TRUE);
	}

	IXGBE_WRITE_REG(&sc->hw, IXGBE_FCTRL, fctrl);
}

/*
 * This is an iterator function now needed by the multicast
 * shared code. It simply feeds the shared code routine the
 * addresses in the array of ixgbe_iff() one by one.
 */
uint8_t *
ixgbe_mc_array_itr(struct ixgbe_hw *hw, uint8_t **update_ptr, uint32_t *vmdq)
{
	uint8_t *addr = *update_ptr;
	uint8_t *newptr;
	*vmdq = 0;

	newptr = addr + IXGBE_ETH_LENGTH_OF_ADDRESS;
	*update_ptr = newptr;
	return addr;
}

void
ixgbe_local_timer(void *arg)
{
	struct ix_softc *sc = arg;
#ifdef IX_DEBUG
	struct ifnet	*ifp = &sc->arpcom.ac_if;
#endif
	int		 s;

	s = splnet();

	ixgbe_update_stats_counters(sc);

#ifdef IX_DEBUG
	if ((ifp->if_flags & (IFF_RUNNING|IFF_DEBUG)) ==
	    (IFF_RUNNING|IFF_DEBUG))
		ixgbe_print_hw_stats(sc);
#endif

	timeout_add_sec(&sc->timer, 1);

	splx(s);
}

void
ixgbe_update_link_status(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	int		link_state = LINK_STATE_DOWN;

	ixgbe_check_link(&sc->hw, &sc->link_speed, &sc->link_up, 0);

	ifp->if_baudrate = 0;
	if (sc->link_up) {
		link_state = LINK_STATE_FULL_DUPLEX;

		switch (sc->link_speed) {
		case IXGBE_LINK_SPEED_UNKNOWN:
			ifp->if_baudrate = 0;
			break;
		case IXGBE_LINK_SPEED_100_FULL:
			ifp->if_baudrate = IF_Mbps(100);
			break;
		case IXGBE_LINK_SPEED_1GB_FULL:
			ifp->if_baudrate = IF_Gbps(1);
			break;
		case IXGBE_LINK_SPEED_10GB_FULL:
			ifp->if_baudrate = IF_Gbps(10);
			break;
		}

		/* Update any Flow Control changes */
		sc->hw.mac.ops.fc_enable(&sc->hw);
	}
	if (ifp->if_link_state != link_state) {
		ifp->if_link_state = link_state;
		if_link_state_change(ifp);
	}
}


/*********************************************************************
 *
 *  This routine disables all traffic on the adapter by issuing a
 *  global reset on the MAC and deallocates TX/RX buffers.
 *
 **********************************************************************/

void
ixgbe_stop(void *arg)
{
	struct ix_softc *sc = arg;
	struct ifnet   *ifp = &sc->arpcom.ac_if;

	/* Tell the stack that the interface is no longer active */
	ifp->if_flags &= ~IFF_RUNNING;

	INIT_DEBUGOUT("ixgbe_stop: begin\n");
	ixgbe_disable_intr(sc);

	sc->hw.mac.ops.reset_hw(&sc->hw);
	sc->hw.adapter_stopped = FALSE;
	sc->hw.mac.ops.stop_adapter(&sc->hw);
	if (sc->hw.mac.type == ixgbe_mac_82599EB)
		sc->hw.mac.ops.stop_mac_link_on_d3(&sc->hw);
	/* Turn off the laser */
	if (sc->hw.mac.ops.disable_tx_laser)
		sc->hw.mac.ops.disable_tx_laser(&sc->hw);
	timeout_del(&sc->timer);
	timeout_del(&sc->rx_refill);

	/* reprogram the RAR[0] in case user changed it. */
	ixgbe_set_rar(&sc->hw, 0, sc->hw.mac.addr, 0, IXGBE_RAH_AV);

	ifq_barrier(&ifp->if_snd);
	intr_barrier(sc->tag);

	KASSERT((ifp->if_flags & IFF_RUNNING) == 0);

	ifq_clr_oactive(&ifp->if_snd);

	/* Should we really clear all structures on stop? */
	ixgbe_free_transmit_structures(sc);
	ixgbe_free_receive_structures(sc);
}


/*********************************************************************
 *
 *  Determine hardware revision.
 *
 **********************************************************************/
void
ixgbe_identify_hardware(struct ix_softc *sc)
{
	struct ixgbe_osdep	*os = &sc->osdep;
	struct pci_attach_args	*pa = &os->os_pa;
	uint32_t		 reg;

	/* Save off the information about this board */
	sc->hw.vendor_id = PCI_VENDOR(pa->pa_id);
	sc->hw.device_id = PCI_PRODUCT(pa->pa_id);

	reg = pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_CLASS_REG);
	sc->hw.revision_id = PCI_REVISION(reg);

	reg = pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_SUBSYS_ID_REG);
	sc->hw.subsystem_vendor_id = PCI_VENDOR(reg);
	sc->hw.subsystem_device_id = PCI_PRODUCT(reg);

	/* We need this here to set the num_segs below */
	ixgbe_set_mac_type(&sc->hw);

	/* Pick up the 82599 and VF settings */
	if (sc->hw.mac.type != ixgbe_mac_82598EB)
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
	sc->num_segs = IXGBE_82599_SCATTER;
}

/*********************************************************************
 *
 *  Determine optic type
 *
 **********************************************************************/
void
ixgbe_setup_optics(struct ix_softc *sc)
{
	struct ixgbe_hw *hw = &sc->hw;
	int		layer;

	layer = hw->mac.ops.get_supported_physical_layer(hw);

	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_T)
		sc->optics = IFM_10G_T;
	else if (layer & IXGBE_PHYSICAL_LAYER_1000BASE_T)
		sc->optics = IFM_1000_T;
	else if (layer & IXGBE_PHYSICAL_LAYER_100BASE_TX)
		sc->optics = IFM_100_TX;
	else if (layer & IXGBE_PHYSICAL_LAYER_SFP_PLUS_CU ||
	    layer & IXGBE_PHYSICAL_LAYER_SFP_ACTIVE_DA)
		sc->optics = IFM_10G_SFP_CU;
	else if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_LR ||
	    layer & IXGBE_PHYSICAL_LAYER_10GBASE_LRM)
		sc->optics = IFM_10G_LR;
	else if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_SR)
		sc->optics = IFM_10G_SR;
	else if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_KX4 ||
	    layer & IXGBE_PHYSICAL_LAYER_10GBASE_CX4)
		sc->optics = IFM_10G_CX4;
	else if (layer & IXGBE_PHYSICAL_LAYER_1000BASE_SX)
		sc->optics = IFM_1000_SX;
	else if (layer & IXGBE_PHYSICAL_LAYER_1000BASE_LX)
		sc->optics = IFM_1000_LX;
	else
		sc->optics = IFM_AUTO;
}

/*********************************************************************
 *
 *  Setup the Legacy or MSI Interrupt handler
 *
 **********************************************************************/
int
ixgbe_allocate_legacy(struct ix_softc *sc)
{
	struct ixgbe_osdep	*os = &sc->osdep;
	struct pci_attach_args	*pa = &os->os_pa;
	const char		*intrstr = NULL;
	pci_chipset_tag_t	pc = pa->pa_pc;
	pci_intr_handle_t	ih;

	/* We allocate a single interrupt resource */
	if (pci_intr_map_msi(pa, &ih) != 0 &&
	    pci_intr_map(pa, &ih) != 0) {
		printf(": couldn't map interrupt\n");
		return (ENXIO);
	}

#if 0
	/* XXX */
	/* Tasklets for Link, SFP and Multispeed Fiber */
	TASK_INIT(&sc->link_task, 0, ixgbe_handle_link, sc);
	TASK_INIT(&sc->mod_task, 0, ixgbe_handle_mod, sc);
	TASK_INIT(&sc->msf_task, 0, ixgbe_handle_msf, sc);
#endif

	intrstr = pci_intr_string(pc, ih);
	sc->tag = pci_intr_establish(pc, ih, IPL_NET | IPL_MPSAFE,
	    ixgbe_intr, sc, sc->dev.dv_xname);
	if (sc->tag == NULL) {
		printf(": couldn't establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		return (ENXIO);
	}
	printf(": %s", intrstr);

	/* For simplicity in the handlers */
	sc->que_mask = IXGBE_EIMS_ENABLE_MASK;

	return (0);
}

int
ixgbe_allocate_pci_resources(struct ix_softc *sc)
{
	struct ixgbe_osdep	*os = &sc->osdep;
	struct pci_attach_args	*pa = &os->os_pa;
	int			 val;

	val = pci_conf_read(pa->pa_pc, pa->pa_tag, PCIR_BAR(0));
	if (PCI_MAPREG_TYPE(val) != PCI_MAPREG_TYPE_MEM ||
	    PCI_MAPREG_MEM_TYPE(val) != PCI_MAPREG_MEM_TYPE_64BIT) {
		printf(": mmba is not mem space\n");
		return (ENXIO);
	}

	if (pci_mapreg_map(pa, PCIR_BAR(0), PCI_MAPREG_MEM_TYPE(val), 0,
	    &os->os_memt, &os->os_memh, &os->os_membase, &os->os_memsize, 0)) {
		printf(": cannot find mem space\n");
		return (ENXIO);
	}
	sc->hw.hw_addr = (uint8_t *)os->os_membase;

	/* Legacy defaults */
	sc->num_queues = 1;
	sc->hw.back = os;

#ifdef notyet
	/* Now setup MSI or MSI/X, return us the number of supported vectors. */
	sc->msix = ixgbe_setup_msix(sc);
#endif

	return (0);
}

void
ixgbe_free_pci_resources(struct ix_softc * sc)
{
	struct ixgbe_osdep	*os = &sc->osdep;
	struct pci_attach_args	*pa = &os->os_pa;
	struct ix_queue *que = sc->queues;
	int i;

	/* Release all msix queue resources: */
	for (i = 0; i < sc->num_queues; i++, que++) {
		if (que->tag)
			pci_intr_disestablish(pa->pa_pc, que->tag);
		que->tag = NULL;
	}

	if (sc->tag)
		pci_intr_disestablish(pa->pa_pc, sc->tag);
	sc->tag = NULL;
	if (os->os_membase != 0)
		bus_space_unmap(os->os_memt, os->os_memh, os->os_memsize);
	os->os_membase = 0;
}

/*********************************************************************
 *
 *  Setup networking device structure and register an interface.
 *
 **********************************************************************/
void
ixgbe_setup_interface(struct ix_softc *sc)
{
	struct ifnet   *ifp = &sc->arpcom.ac_if;

	strlcpy(ifp->if_xname, sc->dev.dv_xname, IFNAMSIZ);
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_xflags = IFXF_MPSAFE;
	ifp->if_ioctl = ixgbe_ioctl;
	ifp->if_qstart = ixgbe_start;
	ifp->if_timer = 0;
	ifp->if_watchdog = ixgbe_watchdog;
	ifp->if_hardmtu = IXGBE_MAX_FRAME_SIZE -
	    ETHER_HDR_LEN - ETHER_CRC_LEN;
	IFQ_SET_MAXLEN(&ifp->if_snd, sc->num_tx_desc - 1);

	ifp->if_capabilities = IFCAP_VLAN_MTU;

#if NVLAN > 0
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

#ifdef IX_CSUM_OFFLOAD
	ifp->if_capabilities |= IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;
#endif

	/*
	 * Specify the media types supported by this sc and register
	 * callbacks to update media and link information
	 */
	ifmedia_init(&sc->media, IFM_IMASK, ixgbe_media_change,
	    ixgbe_media_status);
	ixgbe_add_media_types(sc);
	ifmedia_set(&sc->media, IFM_ETHER | IFM_AUTO);

	if_attach(ifp);
	ether_ifattach(ifp);

	sc->max_frame_size = IXGBE_MAX_FRAME_SIZE;
}

void
ixgbe_add_media_types(struct ix_softc *sc)
{
	struct ixgbe_hw	*hw = &sc->hw;
	int		layer;

	layer = hw->mac.ops.get_supported_physical_layer(hw);

	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_T)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_T, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_1000BASE_T)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_T, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_100BASE_TX)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_100_TX, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_SFP_PLUS_CU ||
	    layer & IXGBE_PHYSICAL_LAYER_SFP_ACTIVE_DA)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_SFP_CU, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_LR) {
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_LR, 0, NULL);
		if (hw->phy.multispeed_fiber)
			ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_LX, 0,
			    NULL);
	}
	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_SR) {
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_SR, 0, NULL);
		if (hw->phy.multispeed_fiber)
			ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_SX, 0,
			    NULL);
	} else if (layer & IXGBE_PHYSICAL_LAYER_1000BASE_SX)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_SX, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_CX4)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_CX4, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_KR)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_SR, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_10GBASE_KX4)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_CX4, 0, NULL);
	if (layer & IXGBE_PHYSICAL_LAYER_1000BASE_KX)
		ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_CX, 0, NULL);

	if (hw->device_id == IXGBE_DEV_ID_82598AT) {
		ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_T | IFM_FDX, 0,
		    NULL);
		ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_T, 0, NULL);
	}

	ifmedia_add(&sc->media, IFM_ETHER | IFM_AUTO, 0, NULL);
}

void
ixgbe_config_link(struct ix_softc *sc)
{
	uint32_t	autoneg, err = 0;
	bool		negotiate;

	if (ixgbe_is_sfp(&sc->hw)) {
		if (sc->hw.phy.multispeed_fiber) {
			sc->hw.mac.ops.setup_sfp(&sc->hw);
			if (sc->hw.mac.ops.enable_tx_laser)
				sc->hw.mac.ops.enable_tx_laser(&sc->hw);
			ixgbe_handle_msf(sc);
		} else
			ixgbe_handle_mod(sc);
	} else {
		if (sc->hw.mac.ops.check_link)
			err = sc->hw.mac.ops.check_link(&sc->hw, &autoneg,
			    &sc->link_up, FALSE);
		if (err)
			return;
		autoneg = sc->hw.phy.autoneg_advertised;
		if ((!autoneg) && (sc->hw.mac.ops.get_link_capabilities))
			err = sc->hw.mac.ops.get_link_capabilities(&sc->hw,
			    &autoneg, &negotiate);
		if (err)
			return;
		if (sc->hw.mac.ops.setup_link)
			sc->hw.mac.ops.setup_link(&sc->hw,
			    autoneg, sc->link_up);
	}
}

/********************************************************************
 * Manage DMA'able memory.
  *******************************************************************/
int
ixgbe_dma_malloc(struct ix_softc *sc, bus_size_t size,
		struct ixgbe_dma_alloc *dma, int mapflags)
{
	struct ifnet		*ifp = &sc->arpcom.ac_if;
	struct ixgbe_osdep	*os = &sc->osdep;
	int			 r;

	dma->dma_tag = os->os_pa.pa_dmat;
	r = bus_dmamap_create(dma->dma_tag, size, 1,
	    size, 0, BUS_DMA_NOWAIT, &dma->dma_map);
	if (r != 0) {
		printf("%s: ixgbe_dma_malloc: bus_dmamap_create failed; "
		       "error %u\n", ifp->if_xname, r);
		goto fail_0;
	}

	r = bus_dmamem_alloc(dma->dma_tag, size, PAGE_SIZE, 0, &dma->dma_seg,
	    1, &dma->dma_nseg, BUS_DMA_NOWAIT);
	if (r != 0) {
		printf("%s: ixgbe_dma_malloc: bus_dmamem_alloc failed; "
		       "error %u\n", ifp->if_xname, r);
		goto fail_1;
	}

	r = bus_dmamem_map(dma->dma_tag, &dma->dma_seg, dma->dma_nseg, size,
	    &dma->dma_vaddr, BUS_DMA_NOWAIT);
	if (r != 0) {
		printf("%s: ixgbe_dma_malloc: bus_dmamem_map failed; "
		       "error %u\n", ifp->if_xname, r);
		goto fail_2;
	}

	r = bus_dmamap_load(dma->dma_tag, dma->dma_map, dma->dma_vaddr,
	    size, NULL, mapflags | BUS_DMA_NOWAIT);
	if (r != 0) {
		printf("%s: ixgbe_dma_malloc: bus_dmamap_load failed; "
		       "error %u\n", ifp->if_xname, r);
		goto fail_3;
	}

	dma->dma_size = size;
	return (0);
fail_3:
	bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, size);
fail_2:
	bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nseg);
fail_1:
	bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
fail_0:
	dma->dma_map = NULL;
	dma->dma_tag = NULL;
	return (r);
}

void
ixgbe_dma_free(struct ix_softc *sc, struct ixgbe_dma_alloc *dma)
{
	if (dma->dma_tag == NULL)
		return;

	if (dma->dma_map != NULL) {
		bus_dmamap_sync(dma->dma_tag, dma->dma_map, 0,
		    dma->dma_map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(dma->dma_tag, dma->dma_map);
		bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, dma->dma_size);
		bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nseg);
		bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
		dma->dma_map = NULL;
	}
}


/*********************************************************************
 *
 *  Allocate memory for the transmit and receive rings, and then
 *  the descriptors associated with each, called only once at attach.
 *
 **********************************************************************/
int
ixgbe_allocate_queues(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct ix_queue *que;
	struct tx_ring *txr;
	struct rx_ring *rxr;
	int rsize, tsize;
	int txconf = 0, rxconf = 0, i;

	/* First allocate the top level queue structs */
	if (!(sc->queues = mallocarray(sc->num_queues,
	    sizeof(struct ix_queue), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate queue memory\n", ifp->if_xname);
		goto fail;
	}

	/* Then allocate the TX ring struct memory */
	if (!(sc->tx_rings = mallocarray(sc->num_queues,
	    sizeof(struct tx_ring), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate TX ring memory\n", ifp->if_xname);
		goto fail;
	}

	/* Next allocate the RX */
	if (!(sc->rx_rings = mallocarray(sc->num_queues,
	    sizeof(struct rx_ring), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate RX ring memory\n", ifp->if_xname);
		goto rx_fail;
	}

	/* For the ring itself */
	tsize = roundup2(sc->num_tx_desc *
	    sizeof(union ixgbe_adv_tx_desc), DBA_ALIGN);

	/*
	 * Now set up the TX queues, txconf is needed to handle the
	 * possibility that things fail midcourse and we need to
	 * undo memory gracefully
	 */
	for (i = 0; i < sc->num_queues; i++, txconf++) {
		/* Set up some basics */
		txr = &sc->tx_rings[i];
		txr->sc = sc;
		txr->me = i;

		if (ixgbe_dma_malloc(sc, tsize,
		    &txr->txdma, BUS_DMA_NOWAIT)) {
			printf("%s: Unable to allocate TX Descriptor memory\n",
			    ifp->if_xname);
			goto err_tx_desc;
		}
		txr->tx_base = (union ixgbe_adv_tx_desc *)txr->txdma.dma_vaddr;
		bzero((void *)txr->tx_base, tsize);
	}

	/*
	 * Next the RX queues...
	 */
	rsize = roundup2(sc->num_rx_desc *
	    sizeof(union ixgbe_adv_rx_desc), 4096);
	for (i = 0; i < sc->num_queues; i++, rxconf++) {
		rxr = &sc->rx_rings[i];
		/* Set up some basics */
		rxr->sc = sc;
		rxr->me = i;

		if (ixgbe_dma_malloc(sc, rsize,
			&rxr->rxdma, BUS_DMA_NOWAIT)) {
			printf("%s: Unable to allocate RxDescriptor memory\n",
			    ifp->if_xname);
			goto err_rx_desc;
		}
		rxr->rx_base = (union ixgbe_adv_rx_desc *)rxr->rxdma.dma_vaddr;
		bzero((void *)rxr->rx_base, rsize);
	}

	/*
	 * Finally set up the queue holding structs
	 */
	for (i = 0; i < sc->num_queues; i++) {
		que = &sc->queues[i];
		que->sc = sc;
		que->txr = &sc->tx_rings[i];
		que->rxr = &sc->rx_rings[i];
	}

	return (0);

err_rx_desc:
	for (rxr = sc->rx_rings; rxconf > 0; rxr++, rxconf--)
		ixgbe_dma_free(sc, &rxr->rxdma);
err_tx_desc:
	for (txr = sc->tx_rings; txconf > 0; txr++, txconf--)
		ixgbe_dma_free(sc, &txr->txdma);
	free(sc->rx_rings, M_DEVBUF, sc->num_queues * sizeof(struct rx_ring));
	sc->rx_rings = NULL;
rx_fail:
	free(sc->tx_rings, M_DEVBUF, sc->num_queues * sizeof(struct tx_ring));
	sc->tx_rings = NULL;
fail:
	return (ENOMEM);
}

/*********************************************************************
 *
 *  Allocate memory for tx_buffer structures. The tx_buffer stores all
 *  the information needed to transmit a packet on the wire. This is
 *  called only once at attach, setup is done every reset.
 *
 **********************************************************************/
int
ixgbe_allocate_transmit_buffers(struct tx_ring *txr)
{
	struct ix_softc 	*sc = txr->sc;
	struct ifnet		*ifp = &sc->arpcom.ac_if;
	struct ixgbe_tx_buf	*txbuf;
	int			 error, i;

	if (!(txr->tx_buffers = mallocarray(sc->num_tx_desc,
	    sizeof(struct ixgbe_tx_buf), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate tx_buffer memory\n",
		    ifp->if_xname);
		error = ENOMEM;
		goto fail;
	}
	txr->txtag = txr->txdma.dma_tag;

	/* Create the descriptor buffer dma maps */
	for (i = 0; i < sc->num_tx_desc; i++) {
		txbuf = &txr->tx_buffers[i];
		error = bus_dmamap_create(txr->txdma.dma_tag, IXGBE_TSO_SIZE,
			    sc->num_segs, PAGE_SIZE, 0,
			    BUS_DMA_NOWAIT, &txbuf->map);

		if (error != 0) {
			printf("%s: Unable to create TX DMA map\n",
			    ifp->if_xname);
			goto fail;
		}
	}

	return 0;
fail:
	return (error);
}

/*********************************************************************
 *
 *  Initialize a transmit ring.
 *
 **********************************************************************/
int
ixgbe_setup_transmit_ring(struct tx_ring *txr)
{
	struct ix_softc		*sc = txr->sc;
	int			 error;

	/* Now allocate transmit buffers for the ring */
	if ((error = ixgbe_allocate_transmit_buffers(txr)) != 0)
		return (error);

	/* Clear the old ring contents */
	bzero((void *)txr->tx_base,
	      (sizeof(union ixgbe_adv_tx_desc)) * sc->num_tx_desc);

	/* Reset indices */
	txr->next_avail_desc = 0;
	txr->next_to_clean = 0;

	/* Set number of descriptors available */
	txr->tx_avail = sc->num_tx_desc;

	bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
	    0, txr->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	return (0);
}

/*********************************************************************
 *
 *  Initialize all transmit rings.
 *
 **********************************************************************/
int
ixgbe_setup_transmit_structures(struct ix_softc *sc)
{
	struct tx_ring *txr = sc->tx_rings;
	int		i, error;

	for (i = 0; i < sc->num_queues; i++, txr++) {
		if ((error = ixgbe_setup_transmit_ring(txr)) != 0)
			goto fail;
	}

	return (0);
fail:
	ixgbe_free_transmit_structures(sc);
	return (error);
}

/*********************************************************************
 *
 *  Enable transmit unit.
 *
 **********************************************************************/
void
ixgbe_initialize_transmit_units(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct tx_ring	*txr;
	struct ixgbe_hw	*hw = &sc->hw;
	int		 i;
	uint64_t	 tdba;
	uint32_t	 txctrl;

	/* Setup the Base and Length of the Tx Descriptor Ring */

	for (i = 0; i < sc->num_queues; i++) {
		txr = &sc->tx_rings[i];

		/* Setup descriptor base address */
		tdba = txr->txdma.dma_map->dm_segs[0].ds_addr;
		IXGBE_WRITE_REG(hw, IXGBE_TDBAL(i),
		       (tdba & 0x00000000ffffffffULL));
		IXGBE_WRITE_REG(hw, IXGBE_TDBAH(i), (tdba >> 32));
		IXGBE_WRITE_REG(hw, IXGBE_TDLEN(i),
		    sc->num_tx_desc * sizeof(struct ixgbe_legacy_tx_desc));

		/* Setup the HW Tx Head and Tail descriptor pointers */
		IXGBE_WRITE_REG(hw, IXGBE_TDH(i), 0);
		IXGBE_WRITE_REG(hw, IXGBE_TDT(i), 0);

		/* Setup Transmit Descriptor Cmd Settings */
		txr->txd_cmd = IXGBE_TXD_CMD_IFCS;
		txr->queue_status = IXGBE_QUEUE_IDLE;
		txr->watchdog_timer = 0;

		/* Disable Head Writeback */
		switch (hw->mac.type) {
		case ixgbe_mac_82598EB:
			txctrl = IXGBE_READ_REG(hw, IXGBE_DCA_TXCTRL(i));
			break;
		case ixgbe_mac_82599EB:
		case ixgbe_mac_X540:
		default:
			txctrl = IXGBE_READ_REG(hw, IXGBE_DCA_TXCTRL_82599(i));
			break;
		}
		txctrl &= ~IXGBE_DCA_TXCTRL_DESC_WRO_EN;
		switch (hw->mac.type) {
		case ixgbe_mac_82598EB:
			IXGBE_WRITE_REG(hw, IXGBE_DCA_TXCTRL(i), txctrl);
			break;
		case ixgbe_mac_82599EB:
		case ixgbe_mac_X540:
		default:
			IXGBE_WRITE_REG(hw, IXGBE_DCA_TXCTRL_82599(i), txctrl);
			break;
		}
	}
	ifp->if_timer = 0;

	if (hw->mac.type != ixgbe_mac_82598EB) {
		uint32_t dmatxctl, rttdcs;
		dmatxctl = IXGBE_READ_REG(hw, IXGBE_DMATXCTL);
		dmatxctl |= IXGBE_DMATXCTL_TE;
		IXGBE_WRITE_REG(hw, IXGBE_DMATXCTL, dmatxctl);
		/* Disable arbiter to set MTQC */
		rttdcs = IXGBE_READ_REG(hw, IXGBE_RTTDCS);
		rttdcs |= IXGBE_RTTDCS_ARBDIS;
		IXGBE_WRITE_REG(hw, IXGBE_RTTDCS, rttdcs);
		IXGBE_WRITE_REG(hw, IXGBE_MTQC, IXGBE_MTQC_64Q_1PB);
		rttdcs &= ~IXGBE_RTTDCS_ARBDIS;
		IXGBE_WRITE_REG(hw, IXGBE_RTTDCS, rttdcs);
	}
}

/*********************************************************************
 *
 *  Free all transmit rings.
 *
 **********************************************************************/
void
ixgbe_free_transmit_structures(struct ix_softc *sc)
{
	struct tx_ring *txr = sc->tx_rings;
	int		i;

	for (i = 0; i < sc->num_queues; i++, txr++)
		ixgbe_free_transmit_buffers(txr);
}

/*********************************************************************
 *
 *  Free transmit ring related data structures.
 *
 **********************************************************************/
void
ixgbe_free_transmit_buffers(struct tx_ring *txr)
{
	struct ix_softc *sc = txr->sc;
	struct ixgbe_tx_buf *tx_buffer;
	int             i;

	INIT_DEBUGOUT("free_transmit_ring: begin");

	if (txr->tx_buffers == NULL)
		return;

	tx_buffer = txr->tx_buffers;
	for (i = 0; i < sc->num_tx_desc; i++, tx_buffer++) {
		if (tx_buffer->map != NULL && tx_buffer->map->dm_nsegs > 0) {
			bus_dmamap_sync(txr->txdma.dma_tag, tx_buffer->map,
			    0, tx_buffer->map->dm_mapsize,
			    BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(txr->txdma.dma_tag,
			    tx_buffer->map);
		}
		if (tx_buffer->m_head != NULL) {
			m_freem(tx_buffer->m_head);
			tx_buffer->m_head = NULL;
		}
		if (tx_buffer->map != NULL) {
			bus_dmamap_destroy(txr->txdma.dma_tag,
			    tx_buffer->map);
			tx_buffer->map = NULL;
		}
	}

	if (txr->tx_buffers != NULL)
		free(txr->tx_buffers, M_DEVBUF,
		    sc->num_tx_desc * sizeof(struct ixgbe_tx_buf));
	txr->tx_buffers = NULL;
	txr->txtag = NULL;
}

/*********************************************************************
 *
 *  Advanced Context Descriptor setup for VLAN or CSUM
 *
 **********************************************************************/

int
ixgbe_tx_ctx_setup(struct tx_ring *txr, struct mbuf *mp,
    uint32_t *cmd_type_len, uint32_t *olinfo_status)
{
	struct ix_softc *sc = txr->sc;
	struct ixgbe_adv_tx_context_desc *TXD;
	struct ixgbe_tx_buf *tx_buffer;
#if NVLAN > 0
	struct ether_vlan_header *eh;
#else
	struct ether_header *eh;
#endif
	struct ip *ip;
#ifdef notyet
	struct ip6_hdr *ip6;
#endif
	struct mbuf *m;
	int	ipoff;
	uint32_t vlan_macip_lens = 0, type_tucmd_mlhl = 0;
	int 	ehdrlen, ip_hlen = 0;
	uint16_t etype;
	uint8_t	ipproto = 0;
	int	offload = TRUE;
	int	ctxd = txr->next_avail_desc;
#if NVLAN > 0
	uint16_t vtag = 0;
#endif

#if notyet
	/* First check if TSO is to be used */
	if (mp->m_pkthdr.csum_flags & CSUM_TSO)
		return (ixgbe_tso_setup(txr, mp, cmd_type_len, olinfo_status));
#endif

	if ((mp->m_pkthdr.csum_flags & (M_TCP_CSUM_OUT | M_UDP_CSUM_OUT)) == 0)
		offload = FALSE;

	/* Indicate the whole packet as payload when not doing TSO */
	*olinfo_status |= mp->m_pkthdr.len << IXGBE_ADVTXD_PAYLEN_SHIFT;

	/* Now ready a context descriptor */
	TXD = (struct ixgbe_adv_tx_context_desc *) &txr->tx_base[ctxd];
	tx_buffer = &txr->tx_buffers[ctxd];

	/*
	 * In advanced descriptors the vlan tag must
	 * be placed into the descriptor itself. Hence
	 * we need to make one even if not doing offloads.
	 */
#if NVLAN > 0
	if (mp->m_flags & M_VLANTAG) {
		vtag = mp->m_pkthdr.ether_vtag;
		vlan_macip_lens |= (vtag << IXGBE_ADVTXD_VLAN_SHIFT);
	} else
#endif
	if (offload == FALSE)
		return (0);	/* No need for CTX */

	/*
	 * Determine where frame payload starts.
	 * Jump over vlan headers if already present,
	 * helpful for QinQ too.
	 */
	if (mp->m_len < sizeof(struct ether_header))
		return (1);
#if NVLAN > 0
	eh = mtod(mp, struct ether_vlan_header *);
	if (eh->evl_encap_proto == htons(ETHERTYPE_VLAN)) {
		if (mp->m_len < sizeof(struct ether_vlan_header))
			return (1);
		etype = ntohs(eh->evl_proto);
		ehdrlen = ETHER_HDR_LEN + ETHER_VLAN_ENCAP_LEN;
	} else {
		etype = ntohs(eh->evl_encap_proto);
		ehdrlen = ETHER_HDR_LEN;
	}
#else
	eh = mtod(mp, struct ether_header *);
	etype = ntohs(eh->ether_type);
	ehdrlen = ETHER_HDR_LEN;
#endif

	/* Set the ether header length */
	vlan_macip_lens |= ehdrlen << IXGBE_ADVTXD_MACLEN_SHIFT;

	switch (etype) {
	case ETHERTYPE_IP:
		if (mp->m_pkthdr.len < ehdrlen + sizeof(*ip))
			return (1);
		m = m_getptr(mp, ehdrlen, &ipoff);
		KASSERT(m != NULL && m->m_len - ipoff >= sizeof(*ip));
		ip = (struct ip *)(m->m_data + ipoff);
		ip_hlen = ip->ip_hl << 2;
		ipproto = ip->ip_p;
		type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV4;
		break;
#ifdef notyet
	case ETHERTYPE_IPV6:
		if (mp->m_pkthdr.len < ehdrlen + sizeof(*ip6))
			return (1);
		m = m_getptr(mp, ehdrlen, &ipoff);
		KASSERT(m != NULL && m->m_len - ipoff >= sizeof(*ip6));
		ip6 = (struct ip6 *)(m->m_data + ipoff);
		ip_hlen = sizeof(*ip6);
		/* XXX-BZ this will go badly in case of ext hdrs. */
		ipproto = ip6->ip6_nxt;
		type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV6;
		break;
#endif
	default:
		offload = FALSE;
		break;
	}

	vlan_macip_lens |= ip_hlen;
	type_tucmd_mlhl |= IXGBE_ADVTXD_DCMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;

	switch (ipproto) {
	case IPPROTO_TCP:
		if (mp->m_pkthdr.csum_flags & M_TCP_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_TCP;
		break;
	case IPPROTO_UDP:
		if (mp->m_pkthdr.csum_flags & M_UDP_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_UDP;
		break;
	default:
		offload = FALSE;
		break;
	}

	if (offload) /* For the TX descriptor setup */
		*olinfo_status |= IXGBE_TXD_POPTS_TXSM << 8;

	/* Now copy bits into descriptor */
	TXD->vlan_macip_lens = htole32(vlan_macip_lens);
	TXD->type_tucmd_mlhl = htole32(type_tucmd_mlhl);
	TXD->seqnum_seed = htole32(0);
	TXD->mss_l4len_idx = htole32(0);

	tx_buffer->m_head = NULL;
	tx_buffer->eop_index = -1;

	membar_producer();

	/* We've consumed the first desc, adjust counters */
	if (++ctxd == sc->num_tx_desc)
		ctxd = 0;
	txr->next_avail_desc = ctxd;
	atomic_dec_int(&txr->tx_avail);

	return (0);
}

/**********************************************************************
 *
 *  Examine each tx_buffer in the used queue. If the hardware is done
 *  processing the packet then free associated resources. The
 *  tx_buffer is put back on the free queue.
 *
 **********************************************************************/
int
ixgbe_txeof(struct tx_ring *txr)
{
	struct ix_softc			*sc = txr->sc;
	struct ifnet			*ifp = &sc->arpcom.ac_if;
	uint32_t			 first, last, done, processed;
	uint32_t			 num_avail;
	struct ixgbe_tx_buf		*tx_buffer;
	struct ixgbe_legacy_tx_desc *tx_desc, *eop_desc;

	if (!ISSET(ifp->if_flags, IFF_RUNNING))
		return FALSE;

	if (txr->tx_avail == sc->num_tx_desc) {
		txr->queue_status = IXGBE_QUEUE_IDLE;
		return FALSE;
	}

	membar_consumer();

	processed = 0;
	first = txr->next_to_clean;
	/* was the txt queue cleaned up in the meantime */
	if (txr->tx_buffers == NULL)
		return FALSE;
	tx_buffer = &txr->tx_buffers[first];
	/* For cleanup we just use legacy struct */
	tx_desc = (struct ixgbe_legacy_tx_desc *)&txr->tx_base[first];
	last = tx_buffer->eop_index;
	if (last == -1)
		return FALSE;
	eop_desc = (struct ixgbe_legacy_tx_desc *)&txr->tx_base[last];

	/*
	 * Get the index of the first descriptor
	 * BEYOND the EOP and call that 'done'.
	 * I do this so the comparison in the
	 * inner while loop below can be simple
	 */
	if (++last == sc->num_tx_desc) last = 0;
	done = last;

	bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
	    0, txr->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD);

	while (eop_desc->upper.fields.status & IXGBE_TXD_STAT_DD) {
		/* We clean the range of the packet */
		while (first != done) {
			tx_desc->upper.data = 0;
			tx_desc->lower.data = 0;
			tx_desc->buffer_addr = 0;
			++processed;

			if (tx_buffer->m_head) {
				bus_dmamap_sync(txr->txdma.dma_tag,
				    tx_buffer->map,
				    0, tx_buffer->map->dm_mapsize,
				    BUS_DMASYNC_POSTWRITE);
				bus_dmamap_unload(txr->txdma.dma_tag,
				    tx_buffer->map);
				m_freem(tx_buffer->m_head);
				tx_buffer->m_head = NULL;
			}
			tx_buffer->eop_index = -1;

			if (++first == sc->num_tx_desc)
				first = 0;

			tx_buffer = &txr->tx_buffers[first];
			tx_desc = (struct ixgbe_legacy_tx_desc *)
			    &txr->tx_base[first];
		}
		++txr->packets;
		/* See if there is more work now */
		last = tx_buffer->eop_index;
		if (last != -1) {
			eop_desc =
			    (struct ixgbe_legacy_tx_desc *)&txr->tx_base[last];
			/* Get next done point */
			if (++last == sc->num_tx_desc) last = 0;
			done = last;
		} else
			break;
	}

	bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
	    0, txr->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	txr->next_to_clean = first;

	num_avail = atomic_add_int_nv(&txr->tx_avail, processed);

	/* All clean, turn off the timer */
	if (num_avail == sc->num_tx_desc)
		ifp->if_timer = 0;

	if (ifq_is_oactive(&ifp->if_snd))
		ifq_restart(&ifp->if_snd);

	return TRUE;
}

/*********************************************************************
 *
 *  Get a buffer from system mbuf buffer pool.
 *
 **********************************************************************/
int
ixgbe_get_buf(struct rx_ring *rxr, int i)
{
	struct ix_softc		*sc = rxr->sc;
	struct ixgbe_rx_buf	*rxbuf;
	struct mbuf		*mp;
	int			error;
	union ixgbe_adv_rx_desc	*rxdesc;
	size_t			 dsize = sizeof(union ixgbe_adv_rx_desc);

	rxbuf = &rxr->rx_buffers[i];
	rxdesc = &rxr->rx_base[i];
	if (rxbuf->buf) {
		printf("%s: ixgbe_get_buf: slot %d already has an mbuf\n",
		    sc->dev.dv_xname, i);
		return (ENOBUFS);
	}

	/* needed in any case so prealocate since this one will fail for sure */
	mp = MCLGETI(NULL, M_DONTWAIT, NULL, sc->rx_mbuf_sz);
	if (!mp)
		return (ENOBUFS);

	mp->m_len = mp->m_pkthdr.len = sc->rx_mbuf_sz;
	m_adj(mp, ETHER_ALIGN);

	error = bus_dmamap_load_mbuf(rxr->rxdma.dma_tag, rxbuf->map,
	    mp, BUS_DMA_NOWAIT);
	if (error) {
		m_freem(mp);
		return (error);
	}

	bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map,
	    0, rxbuf->map->dm_mapsize, BUS_DMASYNC_PREREAD);
	rxbuf->buf = mp;

	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
	    dsize * i, dsize, BUS_DMASYNC_POSTWRITE);

	rxdesc->read.pkt_addr = htole64(rxbuf->map->dm_segs[0].ds_addr);

	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
	    dsize * i, dsize, BUS_DMASYNC_PREWRITE);

	return (0);
}

/*********************************************************************
 *
 *  Allocate memory for rx_buffer structures. Since we use one
 *  rx_buffer per received packet, the maximum number of rx_buffer's
 *  that we'll need is equal to the number of receive descriptors
 *  that we've allocated.
 *
 **********************************************************************/
int
ixgbe_allocate_receive_buffers(struct rx_ring *rxr)
{
	struct ix_softc		*sc = rxr->sc;
	struct ifnet		*ifp = &sc->arpcom.ac_if;
	struct ixgbe_rx_buf 	*rxbuf;
	int			i, error;

	if (!(rxr->rx_buffers = mallocarray(sc->num_rx_desc,
	    sizeof(struct ixgbe_rx_buf), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate rx_buffer memory\n",
		    ifp->if_xname);
		error = ENOMEM;
		goto fail;
	}

	rxbuf = rxr->rx_buffers;
	for (i = 0; i < sc->num_rx_desc; i++, rxbuf++) {
		error = bus_dmamap_create(rxr->rxdma.dma_tag, 16 * 1024, 1,
		    16 * 1024, 0, BUS_DMA_NOWAIT, &rxbuf->map);
		if (error) {
			printf("%s: Unable to create Pack DMA map\n",
			    ifp->if_xname);
			goto fail;
		}
	}
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map, 0,
	    rxr->rxdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	return (0);

fail:
	return (error);
}

/*********************************************************************
 *
 *  Initialize a receive ring and its buffers.
 *
 **********************************************************************/
int
ixgbe_setup_receive_ring(struct rx_ring *rxr)
{
	struct ix_softc		*sc = rxr->sc;
	struct ifnet		*ifp = &sc->arpcom.ac_if;
	int			 rsize, error;

	rsize = roundup2(sc->num_rx_desc *
	    sizeof(union ixgbe_adv_rx_desc), 4096);
	/* Clear the ring contents */
	bzero((void *)rxr->rx_base, rsize);

	if ((error = ixgbe_allocate_receive_buffers(rxr)) != 0)
		return (error);

	/* Setup our descriptor indices */
	rxr->next_to_check = 0;
	rxr->last_desc_filled = sc->num_rx_desc - 1;

	if_rxr_init(&rxr->rx_ring, 2 * ((ifp->if_hardmtu / MCLBYTES) + 1),
	    sc->num_rx_desc);

	ixgbe_rxfill(rxr);
	if (if_rxr_inuse(&rxr->rx_ring) == 0) {
		printf("%s: unable to fill any rx descriptors\n",
		    sc->dev.dv_xname);
		return (ENOBUFS);
	}

	return (0);
}

int
ixgbe_rxfill(struct rx_ring *rxr)
{
	struct ix_softc *sc = rxr->sc;
	int		 post = 0;
	u_int		 slots;
	int		 i;

	i = rxr->last_desc_filled;
	for (slots = if_rxr_get(&rxr->rx_ring, sc->num_rx_desc);
	    slots > 0; slots--) {
		if (++i == sc->num_rx_desc)
			i = 0;

		if (ixgbe_get_buf(rxr, i) != 0)
			break;

		rxr->last_desc_filled = i;
		post = 1;
	}

	if_rxr_put(&rxr->rx_ring, slots);

	return (post);
}

void
ixgbe_rxrefill(void *xsc)
{
	struct ix_softc *sc = xsc;
	struct ix_queue *que = sc->queues;
	int s;

	s = splnet();
	if (ixgbe_rxfill(que->rxr)) {
		/* Advance the Rx Queue "Tail Pointer" */
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(que->rxr->me),
		    que->rxr->last_desc_filled);
	} else
		timeout_add(&sc->rx_refill, 1);
	splx(s);
}

/*********************************************************************
 *
 *  Initialize all receive rings.
 *
 **********************************************************************/
int
ixgbe_setup_receive_structures(struct ix_softc *sc)
{
	struct rx_ring *rxr = sc->rx_rings;
	int i;

	for (i = 0; i < sc->num_queues; i++, rxr++)
		if (ixgbe_setup_receive_ring(rxr))
			goto fail;

	return (0);
fail:
	ixgbe_free_receive_structures(sc);
	return (ENOBUFS);
}

/*********************************************************************
 *
 *  Setup receive registers and features.
 *
 **********************************************************************/
#define IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT 2

void
ixgbe_initialize_receive_units(struct ix_softc *sc)
{
	struct rx_ring	*rxr = sc->rx_rings;
	struct ixgbe_hw	*hw = &sc->hw;
	uint32_t	bufsz, fctrl, srrctl, rxcsum;
	uint32_t	hlreg;
	int		i;

	/*
	 * Make sure receives are disabled while
	 * setting up the descriptor ring
	 */
	ixgbe_disable_rx(hw);

	/* Enable broadcasts */
	fctrl = IXGBE_READ_REG(hw, IXGBE_FCTRL);
	fctrl |= IXGBE_FCTRL_BAM;
	if (sc->hw.mac.type == ixgbe_mac_82598EB) {
		fctrl |= IXGBE_FCTRL_DPF;
		fctrl |= IXGBE_FCTRL_PMCF;
	}
	IXGBE_WRITE_REG(hw, IXGBE_FCTRL, fctrl);

	/* Always enable jumbo frame reception */
	hlreg = IXGBE_READ_REG(hw, IXGBE_HLREG0);
	hlreg |= IXGBE_HLREG0_JUMBOEN;
	IXGBE_WRITE_REG(hw, IXGBE_HLREG0, hlreg);

	bufsz = (sc->rx_mbuf_sz - ETHER_ALIGN) >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;

	for (i = 0; i < sc->num_queues; i++, rxr++) {
		uint64_t rdba = rxr->rxdma.dma_map->dm_segs[0].ds_addr;

		/* Setup the Base and Length of the Rx Descriptor Ring */
		IXGBE_WRITE_REG(hw, IXGBE_RDBAL(i),
			       (rdba & 0x00000000ffffffffULL));
		IXGBE_WRITE_REG(hw, IXGBE_RDBAH(i), (rdba >> 32));
		IXGBE_WRITE_REG(hw, IXGBE_RDLEN(i),
		    sc->num_rx_desc * sizeof(union ixgbe_adv_rx_desc));

		/* Set up the SRRCTL register */
		srrctl = bufsz | IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;
		IXGBE_WRITE_REG(hw, IXGBE_SRRCTL(i), srrctl);

		/* Setup the HW Rx Head and Tail Descriptor Pointers */
		IXGBE_WRITE_REG(hw, IXGBE_RDH(i), 0);
		IXGBE_WRITE_REG(hw, IXGBE_RDT(i), 0);
	}

	if (sc->hw.mac.type != ixgbe_mac_82598EB) {
		uint32_t psrtype = IXGBE_PSRTYPE_TCPHDR |
			      IXGBE_PSRTYPE_UDPHDR |
			      IXGBE_PSRTYPE_IPV4HDR |
			      IXGBE_PSRTYPE_IPV6HDR;
		IXGBE_WRITE_REG(hw, IXGBE_PSRTYPE(0), psrtype);
	}

	rxcsum = IXGBE_READ_REG(hw, IXGBE_RXCSUM);
	rxcsum &= ~IXGBE_RXCSUM_PCSD;

	/* Setup RSS */
	if (sc->num_queues > 1) {
		ixgbe_initialize_rss_mapping(sc);

		/* RSS and RX IPP Checksum are mutually exclusive */
		rxcsum |= IXGBE_RXCSUM_PCSD;
	}

	/* This is useful for calculating UDP/IP fragment checksums */
	if (!(rxcsum & IXGBE_RXCSUM_PCSD))
		rxcsum |= IXGBE_RXCSUM_IPPCSE;

	IXGBE_WRITE_REG(hw, IXGBE_RXCSUM, rxcsum);
}

void
ixgbe_initialize_rss_mapping(struct ix_softc *sc)
{
	struct ixgbe_hw	*hw = &sc->hw;
	uint32_t reta = 0, mrqc, rss_key[10];
	int i, j, queue_id, table_size, index_mult;

	/* set up random bits */
	arc4random_buf(&rss_key, sizeof(rss_key));

	/* Set multiplier for RETA setup and table size based on MAC */
	index_mult = 0x1;
	table_size = 128;
	switch (sc->hw.mac.type) {
	case ixgbe_mac_82598EB:
		index_mult = 0x11;
		break;
	case ixgbe_mac_X550:
	case ixgbe_mac_X550EM_x:
		table_size = 512;
		break;
	default:
		break;
	}

	/* Set up the redirection table */
	for (i = 0, j = 0; i < table_size; i++, j++) {
		if (j == sc->num_queues) j = 0;
		queue_id = (j * index_mult);
		/*
		 * The low 8 bits are for hash value (n+0);
		 * The next 8 bits are for hash value (n+1), etc.
		 */
		reta = reta >> 8;
		reta = reta | ( ((uint32_t) queue_id) << 24);
		if ((i & 3) == 3) {
			if (i < 128)
				IXGBE_WRITE_REG(hw, IXGBE_RETA(i >> 2), reta);
			else
				IXGBE_WRITE_REG(hw, IXGBE_ERETA((i >> 2) - 32),
				    reta);
			reta = 0;
		}
	}

	/* Now fill our hash function seeds */
	for (i = 0; i < 10; i++)
		IXGBE_WRITE_REG(hw, IXGBE_RSSRK(i), rss_key[i]);

	/*
	 * Disable UDP - IP fragments aren't currently being handled
	 * and so we end up with a mix of 2-tuple and 4-tuple
	 * traffic.
	 */
	mrqc = IXGBE_MRQC_RSSEN
	     | IXGBE_MRQC_RSS_FIELD_IPV4
	     | IXGBE_MRQC_RSS_FIELD_IPV4_TCP
	     | IXGBE_MRQC_RSS_FIELD_IPV6_EX_TCP
	     | IXGBE_MRQC_RSS_FIELD_IPV6_EX
	     | IXGBE_MRQC_RSS_FIELD_IPV6
	     | IXGBE_MRQC_RSS_FIELD_IPV6_TCP
	;
	IXGBE_WRITE_REG(hw, IXGBE_MRQC, mrqc);
}

/*********************************************************************
 *
 *  Free all receive rings.
 *
 **********************************************************************/
void
ixgbe_free_receive_structures(struct ix_softc *sc)
{
	struct rx_ring *rxr;
	int		i;

	for (i = 0, rxr = sc->rx_rings; i < sc->num_queues; i++, rxr++)
		if_rxr_init(&rxr->rx_ring, 0, 0);

	for (i = 0, rxr = sc->rx_rings; i < sc->num_queues; i++, rxr++)
		ixgbe_free_receive_buffers(rxr);
}

/*********************************************************************
 *
 *  Free receive ring data structures
 *
 **********************************************************************/
void
ixgbe_free_receive_buffers(struct rx_ring *rxr)
{
	struct ix_softc		*sc;
	struct ixgbe_rx_buf	*rxbuf;
	int			 i;

	sc = rxr->sc;
	if (rxr->rx_buffers != NULL) {
		for (i = 0; i < sc->num_rx_desc; i++) {
			rxbuf = &rxr->rx_buffers[i];
			if (rxbuf->buf != NULL) {
				bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map,
				    0, rxbuf->map->dm_mapsize,
				    BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(rxr->rxdma.dma_tag,
				    rxbuf->map);
				m_freem(rxbuf->buf);
				rxbuf->buf = NULL;
			}
			bus_dmamap_destroy(rxr->rxdma.dma_tag, rxbuf->map);
			rxbuf->map = NULL;
		}
		free(rxr->rx_buffers, M_DEVBUF,
		    sc->num_rx_desc * sizeof(struct ixgbe_rx_buf));
		rxr->rx_buffers = NULL;
	}
}

/*********************************************************************
 *
 *  This routine executes in interrupt context. It replenishes
 *  the mbufs in the descriptor and sends data which has been
 *  dma'ed into host memory to upper layer.
 *
 *********************************************************************/
int
ixgbe_rxeof(struct ix_queue *que)
{
	struct ix_softc 	*sc = que->sc;
	struct rx_ring		*rxr = que->rxr;
	struct ifnet   		*ifp = &sc->arpcom.ac_if;
	struct mbuf_list	 ml = MBUF_LIST_INITIALIZER();
	struct mbuf    		*mp, *sendmp;
	uint8_t		    	 eop = 0;
	uint16_t		 len, vtag;
	uint32_t		 staterr = 0, ptype;
	struct ixgbe_rx_buf	*rxbuf, *nxbuf;
	union ixgbe_adv_rx_desc	*rxdesc;
	size_t			 dsize = sizeof(union ixgbe_adv_rx_desc);
	int			 i, nextp;

	if (!ISSET(ifp->if_flags, IFF_RUNNING))
		return FALSE;

	i = rxr->next_to_check;
	while (if_rxr_inuse(&rxr->rx_ring) > 0) {
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
		    dsize * i, dsize, BUS_DMASYNC_POSTREAD);

		rxdesc = &rxr->rx_base[i];
		staterr = letoh32(rxdesc->wb.upper.status_error);
		if (!ISSET(staterr, IXGBE_RXD_STAT_DD)) {
			bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
			    dsize * i, dsize,
			    BUS_DMASYNC_PREREAD);
			break;
		}

		/* Zero out the receive descriptors status  */
		rxdesc->wb.upper.status_error = 0;
		rxbuf = &rxr->rx_buffers[i];

		/* pull the mbuf off the ring */
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map, 0,
		    rxbuf->map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->map);

		mp = rxbuf->buf;
		len = letoh16(rxdesc->wb.upper.length);
		ptype = letoh32(rxdesc->wb.lower.lo_dword.data) &
		    IXGBE_RXDADV_PKTTYPE_MASK;
		vtag = letoh16(rxdesc->wb.upper.vlan);
		eop = ((staterr & IXGBE_RXD_STAT_EOP) != 0);

		if (staterr & IXGBE_RXDADV_ERR_FRAME_ERR_MASK) {
			sc->dropped_pkts++;

			if (rxbuf->fmp) {
				m_freem(rxbuf->fmp);
				rxbuf->fmp = NULL;
			}

			m_freem(mp);
			rxbuf->buf = NULL;
			goto next_desc;
		}

		if (mp == NULL) {
			panic("%s: ixgbe_rxeof: NULL mbuf in slot %d "
			    "(nrx %d, filled %d)", sc->dev.dv_xname,
			    i, if_rxr_inuse(&rxr->rx_ring),
			    rxr->last_desc_filled);
		}

		/* Currently no HW RSC support of 82599 */
		if (!eop) {
			/*
			 * Figure out the next descriptor of this frame.
			 */
			nextp = i + 1;
			if (nextp == sc->num_rx_desc)
				nextp = 0;
			nxbuf = &rxr->rx_buffers[nextp];
			/* prefetch(nxbuf); */
		}

		/*
		 * Rather than using the fmp/lmp global pointers
		 * we now keep the head of a packet chain in the
		 * buffer struct and pass this along from one
		 * descriptor to the next, until we get EOP.
		 */
		mp->m_len = len;
		/*
		 * See if there is a stored head
		 * that determines what we are
		 */
		sendmp = rxbuf->fmp;
		rxbuf->buf = rxbuf->fmp = NULL;

		if (sendmp != NULL) /* secondary frag */
			sendmp->m_pkthdr.len += mp->m_len;
		else {
			/* first desc of a non-ps chain */
			sendmp = mp;
			sendmp->m_pkthdr.len = mp->m_len;
#if NVLAN > 0
			if (staterr & IXGBE_RXD_STAT_VP) {
				sendmp->m_pkthdr.ether_vtag = vtag;
				sendmp->m_flags |= M_VLANTAG;
			}
#endif
		}

		/* Pass the head pointer on */
		if (eop == 0) {
			nxbuf->fmp = sendmp;
			sendmp = NULL;
			mp->m_next = nxbuf->buf;
		} else { /* Sending this frame? */
			rxr->rx_packets++;
			/* capture data for AIM */
			rxr->bytes += sendmp->m_pkthdr.len;
			rxr->rx_bytes += sendmp->m_pkthdr.len;

			ixgbe_rx_checksum(staterr, sendmp, ptype);

			ml_enqueue(&ml, sendmp);
		}
next_desc:
		if_rxr_put(&rxr->rx_ring, 1);
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
		    dsize * i, dsize,
		    BUS_DMASYNC_PREREAD);

		/* Advance our pointers to the next descriptor. */
		if (++i == sc->num_rx_desc)
			i = 0;
	}
	rxr->next_to_check = i;

	if_input(ifp, &ml);

	if (!(staterr & IXGBE_RXD_STAT_DD))
		return FALSE;

	return TRUE;
}

/*********************************************************************
 *
 *  Verify that the hardware indicated that the checksum is valid.
 *  Inform the stack about the status of checksum so that stack
 *  doesn't spend time verifying the checksum.
 *
 *********************************************************************/
void
ixgbe_rx_checksum(uint32_t staterr, struct mbuf * mp, uint32_t ptype)
{
	uint16_t status = (uint16_t) staterr;
	uint8_t  errors = (uint8_t) (staterr >> 24);

	if (status & IXGBE_RXD_STAT_IPCS) {
		if (!(errors & IXGBE_RXD_ERR_IPE)) {
			/* IP Checksum Good */
			mp->m_pkthdr.csum_flags = M_IPV4_CSUM_IN_OK;
		} else
			mp->m_pkthdr.csum_flags = 0;
	}
	if (status & IXGBE_RXD_STAT_L4CS) {
		if (!(errors & IXGBE_RXD_ERR_TCPE))
			mp->m_pkthdr.csum_flags |=
				M_TCP_CSUM_IN_OK | M_UDP_CSUM_IN_OK;
	}
}

void
ixgbe_setup_vlan_hw_support(struct ix_softc *sc)
{
	uint32_t	ctrl;
	int		i;

	/*
	 * A soft reset zero's out the VFTA, so
	 * we need to repopulate it now.
	 */
	for (i = 0; i < IXGBE_VFTA_SIZE; i++) {
		if (sc->shadow_vfta[i] != 0)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_VFTA(i),
			    sc->shadow_vfta[i]);
	}

	ctrl = IXGBE_READ_REG(&sc->hw, IXGBE_VLNCTRL);
#if 0
	/* Enable the Filter Table if enabled */
	if (ifp->if_capenable & IFCAP_VLAN_HWFILTER) {
		ctrl &= ~IXGBE_VLNCTRL_CFIEN;
		ctrl |= IXGBE_VLNCTRL_VFE;
	}
#endif
	if (sc->hw.mac.type == ixgbe_mac_82598EB)
		ctrl |= IXGBE_VLNCTRL_VME;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_VLNCTRL, ctrl);

	/* On 82599 the VLAN enable is per/queue in RXDCTL */
	if (sc->hw.mac.type != ixgbe_mac_82598EB) {
		for (i = 0; i < sc->num_queues; i++) {
			ctrl = IXGBE_READ_REG(&sc->hw, IXGBE_RXDCTL(i));
			ctrl |= IXGBE_RXDCTL_VME;
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RXDCTL(i), ctrl);
		}
	}
}

void
ixgbe_enable_intr(struct ix_softc *sc)
{
	struct ixgbe_hw *hw = &sc->hw;
	struct ix_queue *que = sc->queues;
	uint32_t	mask, fwsm;
	int i;

	mask = (IXGBE_EIMS_ENABLE_MASK & ~IXGBE_EIMS_RTX_QUEUE);
	/* Enable Fan Failure detection */
	if (hw->device_id == IXGBE_DEV_ID_82598AT)
		    mask |= IXGBE_EIMS_GPI_SDP1;

	switch (sc->hw.mac.type) {
	case ixgbe_mac_82599EB:
		mask |= IXGBE_EIMS_ECC;
		/* Temperature sensor on some adapters */
		mask |= IXGBE_EIMS_GPI_SDP0;
		/* SFP+ (RX_LOS_N & MOD_ABS_N) */
		mask |= IXGBE_EIMS_GPI_SDP1;
		mask |= IXGBE_EIMS_GPI_SDP2;
		break;
	case ixgbe_mac_X540:
		mask |= IXGBE_EIMS_ECC;
		/* Detect if Thermal Sensor is enabled */
		fwsm = IXGBE_READ_REG(hw, IXGBE_FWSM);
		if (fwsm & IXGBE_FWSM_TS_ENABLED)
			mask |= IXGBE_EIMS_TS;
		break;
	case ixgbe_mac_X550:
	case ixgbe_mac_X550EM_x:
		mask |= IXGBE_EIMS_ECC;
		/* MAC thermal sensor is automatically enabled */
		mask |= IXGBE_EIMS_TS;
		/* Some devices use SDP0 for important information */
		if (hw->device_id == IXGBE_DEV_ID_X550EM_X_SFP ||
		    hw->device_id == IXGBE_DEV_ID_X550EM_X_10G_T)
			mask |= IXGBE_EIMS_GPI_SDP0_X540;
	default:
		break;
	}

	IXGBE_WRITE_REG(hw, IXGBE_EIMS, mask);

	/* With MSI-X we use auto clear */
	if (sc->msix > 1) {
		mask = IXGBE_EIMS_ENABLE_MASK;
		/* Don't autoclear Link */
		mask &= ~IXGBE_EIMS_OTHER;
		mask &= ~IXGBE_EIMS_LSC;
		IXGBE_WRITE_REG(hw, IXGBE_EIAC, mask);
	}

	/*
	 * Now enable all queues, this is done separately to
	 * allow for handling the extended (beyond 32) MSIX
	 * vectors that can be used by 82599
	 */
	for (i = 0; i < sc->num_queues; i++, que++)
		ixgbe_enable_queue(sc, que->msix);

	IXGBE_WRITE_FLUSH(hw);
}

void
ixgbe_disable_intr(struct ix_softc *sc)
{
	if (sc->msix > 1)
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAC, 0);
	if (sc->hw.mac.type == ixgbe_mac_82598EB) {
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC, ~0);
	} else {
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC, 0xFFFF0000);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC_EX(0), ~0);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC_EX(1), ~0);
	}
	IXGBE_WRITE_FLUSH(&sc->hw);
}

uint16_t
ixgbe_read_pci_cfg(struct ixgbe_hw *hw, uint32_t reg)
{
	struct pci_attach_args	*pa;
	uint32_t value;
	int high = 0;

	if (reg & 0x2) {
		high = 1;
		reg &= ~0x2;
	}
	pa = &((struct ixgbe_osdep *)hw->back)->os_pa;
	value = pci_conf_read(pa->pa_pc, pa->pa_tag, reg);

	if (high)
		value >>= 16;

	return (value & 0xffff);
}

void
ixgbe_write_pci_cfg(struct ixgbe_hw *hw, uint32_t reg, uint16_t value)
{
	struct pci_attach_args	*pa;
	uint32_t rv;
	int high = 0;

	/* Need to do read/mask/write... because 16 vs 32 bit!!! */
	if (reg & 0x2) {
		high = 1;
		reg &= ~0x2;
	}
	pa = &((struct ixgbe_osdep *)hw->back)->os_pa;
	rv = pci_conf_read(pa->pa_pc, pa->pa_tag, reg);
	if (!high)
		rv = (rv & 0xffff0000) | value;
	else
		rv = (rv & 0xffff) | ((uint32_t)value << 16);
	pci_conf_write(pa->pa_pc, pa->pa_tag, reg, rv);
}

/*
 * Setup the correct IVAR register for a particular MSIX interrupt
 *   (yes this is all very magic and confusing :)
 *  - entry is the register array entry
 *  - vector is the MSIX vector for this queue
 *  - type is RX/TX/MISC
 */
void
ixgbe_set_ivar(struct ix_softc *sc, uint8_t entry, uint8_t vector, int8_t type)
{
	struct ixgbe_hw *hw = &sc->hw;
	uint32_t ivar, index;

	vector |= IXGBE_IVAR_ALLOC_VAL;

	switch (hw->mac.type) {

	case ixgbe_mac_82598EB:
		if (type == -1)
			entry = IXGBE_IVAR_OTHER_CAUSES_INDEX;
		else
			entry += (type * 64);
		index = (entry >> 2) & 0x1F;
		ivar = IXGBE_READ_REG(hw, IXGBE_IVAR(index));
		ivar &= ~(0xFF << (8 * (entry & 0x3)));
		ivar |= (vector << (8 * (entry & 0x3)));
		IXGBE_WRITE_REG(&sc->hw, IXGBE_IVAR(index), ivar);
		break;

	case ixgbe_mac_82599EB:
	case ixgbe_mac_X540:
	case ixgbe_mac_X550:
	case ixgbe_mac_X550EM_x:
		if (type == -1) { /* MISC IVAR */
			index = (entry & 1) * 8;
			ivar = IXGBE_READ_REG(hw, IXGBE_IVAR_MISC);
			ivar &= ~(0xFF << index);
			ivar |= (vector << index);
			IXGBE_WRITE_REG(hw, IXGBE_IVAR_MISC, ivar);
		} else {	/* RX/TX IVARS */
			index = (16 * (entry & 1)) + (8 * type);
			ivar = IXGBE_READ_REG(hw, IXGBE_IVAR(entry >> 1));
			ivar &= ~(0xFF << index);
			ivar |= (vector << index);
			IXGBE_WRITE_REG(hw, IXGBE_IVAR(entry >> 1), ivar);
		}

	default:
		break;
	}
}

void
ixgbe_configure_ivars(struct ix_softc *sc)
{
#if notyet
	struct ix_queue *que = sc->queues;
	uint32_t newitr;
	int i;

	if (ixgbe_max_interrupt_rate > 0)
		newitr = (4000000 / ixgbe_max_interrupt_rate) & 0x0FF8;
	else
		newitr = 0;

	for (i = 0; i < sc->num_queues; i++, que++) {
		/* First the RX queue entry */
		ixgbe_set_ivar(sc, i, que->msix, 0);
		/* ... and the TX */
		ixgbe_set_ivar(sc, i, que->msix, 1);
		/* Set an Initial EITR value */
		IXGBE_WRITE_REG(&sc->hw,
		    IXGBE_EITR(que->msix), newitr);
	}

	/* For the Link interrupt */
	ixgbe_set_ivar(sc, 1, sc->linkvec, -1);
#endif
}

/*
 * SFP module interrupts handler
 */
void
ixgbe_handle_mod(struct ix_softc *sc)
{
	struct ixgbe_hw *hw = &sc->hw;
	uint32_t err;

	err = hw->phy.ops.identify_sfp(hw);
	if (err == IXGBE_ERR_SFP_NOT_SUPPORTED) {
		printf("%s: Unsupported SFP+ module type was detected!\n",
		    sc->dev.dv_xname);
		return;
	}
	err = hw->mac.ops.setup_sfp(hw);
	if (err == IXGBE_ERR_SFP_NOT_SUPPORTED) {
		printf("%s: Setup failure - unsupported SFP+ module type!\n",
		    sc->dev.dv_xname);
		return;
	}
	/* Set the optics type so system reports correctly */
	ixgbe_setup_optics(sc);

	ixgbe_handle_msf(sc);
}


/*
 * MSF (multispeed fiber) interrupts handler
 */
void
ixgbe_handle_msf(struct ix_softc *sc)
{
	struct ixgbe_hw *hw = &sc->hw;
	uint32_t autoneg;
	bool negotiate;

	autoneg = hw->phy.autoneg_advertised;
	if ((!autoneg) && (hw->mac.ops.get_link_capabilities)) {
		if (hw->mac.ops.get_link_capabilities(hw, &autoneg, &negotiate))
			return;
	}
	if (hw->mac.ops.setup_link)
		hw->mac.ops.setup_link(hw, autoneg, TRUE);

	ifmedia_delete_instance(&sc->media, IFM_INST_ANY);
	ixgbe_add_media_types(sc);
	ifmedia_set(&sc->media, IFM_ETHER | IFM_AUTO);
}

/*
 * External PHY interrupts handler
 */
void
ixgbe_handle_phy(struct ix_softc *sc)
{
	struct ixgbe_hw *hw = &sc->hw;
	int error;

	error = hw->phy.ops.handle_lasi(hw);
	if (error == IXGBE_ERR_OVERTEMP)
		printf("%s: CRITICAL: EXTERNAL PHY OVER TEMP!! "
		    " PHY will downshift to lower power state!\n",
		    sc->dev.dv_xname);
	else if (error)
		printf("%s: Error handling LASI interrupt: %d\n",
		    sc->dev.dv_xname, error);

}

/**********************************************************************
 *
 *  Update the board statistics counters.
 *
 **********************************************************************/
void
ixgbe_update_stats_counters(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct ixgbe_hw	*hw = &sc->hw;
	uint64_t	total_missed_rx = 0;
#ifdef IX_DEBUG
	uint32_t	missed_rx = 0, bprc, lxon, lxoff, total;
	int		i;
#endif

	sc->stats.crcerrs += IXGBE_READ_REG(hw, IXGBE_CRCERRS);
	sc->stats.rlec += IXGBE_READ_REG(hw, IXGBE_RLEC);

#ifdef IX_DEBUG
	for (i = 0; i < 8; i++) {
		uint32_t mp;
		mp = IXGBE_READ_REG(hw, IXGBE_MPC(i));
		/* missed_rx tallies misses for the gprc workaround */
		missed_rx += mp;
		/* global total per queue */
		sc->stats.mpc[i] += mp;
		/* running comprehensive total for stats display */
		total_missed_rx += sc->stats.mpc[i];
		if (hw->mac.type == ixgbe_mac_82598EB)
			sc->stats.rnbc[i] += IXGBE_READ_REG(hw, IXGBE_RNBC(i));
	}

	/* Hardware workaround, gprc counts missed packets */
	sc->stats.gprc += IXGBE_READ_REG(hw, IXGBE_GPRC);
	sc->stats.gprc -= missed_rx;

	if (hw->mac.type != ixgbe_mac_82598EB) {
		sc->stats.gorc += IXGBE_READ_REG(hw, IXGBE_GORCL) +
		    ((uint64_t)IXGBE_READ_REG(hw, IXGBE_GORCH) << 32);
		sc->stats.gotc += IXGBE_READ_REG(hw, IXGBE_GOTCL) +
		    ((uint64_t)IXGBE_READ_REG(hw, IXGBE_GOTCH) << 32);
		sc->stats.tor += IXGBE_READ_REG(hw, IXGBE_TORL) +
		    ((uint64_t)IXGBE_READ_REG(hw, IXGBE_TORH) << 32);
		sc->stats.lxonrxc += IXGBE_READ_REG(hw, IXGBE_LXONRXCNT);
		sc->stats.lxoffrxc += IXGBE_READ_REG(hw, IXGBE_LXOFFRXCNT);
	} else {
		sc->stats.lxonrxc += IXGBE_READ_REG(hw, IXGBE_LXONRXC);
		sc->stats.lxoffrxc += IXGBE_READ_REG(hw, IXGBE_LXOFFRXC);
		/* 82598 only has a counter in the high register */
		sc->stats.gorc += IXGBE_READ_REG(hw, IXGBE_GORCH);
		sc->stats.gotc += IXGBE_READ_REG(hw, IXGBE_GOTCH);
		sc->stats.tor += IXGBE_READ_REG(hw, IXGBE_TORH);
	}

	/*
	 * Workaround: mprc hardware is incorrectly counting
	 * broadcasts, so for now we subtract those.
	 */
	bprc = IXGBE_READ_REG(hw, IXGBE_BPRC);
	sc->stats.bprc += bprc;
	sc->stats.mprc += IXGBE_READ_REG(hw, IXGBE_MPRC);
	if (hw->mac.type == ixgbe_mac_82598EB)
		sc->stats.mprc -= bprc;

	sc->stats.roc += IXGBE_READ_REG(hw, IXGBE_ROC);
	sc->stats.prc64 += IXGBE_READ_REG(hw, IXGBE_PRC64);
	sc->stats.prc127 += IXGBE_READ_REG(hw, IXGBE_PRC127);
	sc->stats.prc255 += IXGBE_READ_REG(hw, IXGBE_PRC255);
	sc->stats.prc511 += IXGBE_READ_REG(hw, IXGBE_PRC511);
	sc->stats.prc1023 += IXGBE_READ_REG(hw, IXGBE_PRC1023);
	sc->stats.prc1522 += IXGBE_READ_REG(hw, IXGBE_PRC1522);

	lxon = IXGBE_READ_REG(hw, IXGBE_LXONTXC);
	sc->stats.lxontxc += lxon;
	lxoff = IXGBE_READ_REG(hw, IXGBE_LXOFFTXC);
	sc->stats.lxofftxc += lxoff;
	total = lxon + lxoff;

	sc->stats.gptc += IXGBE_READ_REG(hw, IXGBE_GPTC);
	sc->stats.mptc += IXGBE_READ_REG(hw, IXGBE_MPTC);
	sc->stats.ptc64 += IXGBE_READ_REG(hw, IXGBE_PTC64);
	sc->stats.gptc -= total;
	sc->stats.mptc -= total;
	sc->stats.ptc64 -= total;
	sc->stats.gotc -= total * ETHER_MIN_LEN;

	sc->stats.ruc += IXGBE_READ_REG(hw, IXGBE_RUC);
	sc->stats.rfc += IXGBE_READ_REG(hw, IXGBE_RFC);
	sc->stats.rjc += IXGBE_READ_REG(hw, IXGBE_RJC);
	sc->stats.tpr += IXGBE_READ_REG(hw, IXGBE_TPR);
	sc->stats.ptc127 += IXGBE_READ_REG(hw, IXGBE_PTC127);
	sc->stats.ptc255 += IXGBE_READ_REG(hw, IXGBE_PTC255);
	sc->stats.ptc511 += IXGBE_READ_REG(hw, IXGBE_PTC511);
	sc->stats.ptc1023 += IXGBE_READ_REG(hw, IXGBE_PTC1023);
	sc->stats.ptc1522 += IXGBE_READ_REG(hw, IXGBE_PTC1522);
	sc->stats.bptc += IXGBE_READ_REG(hw, IXGBE_BPTC);
#endif

	/* Fill out the OS statistics structure */
	ifp->if_collisions = 0;
	ifp->if_oerrors = sc->watchdog_events;
	ifp->if_ierrors = total_missed_rx + sc->stats.crcerrs + sc->stats.rlec;
}

#ifdef IX_DEBUG
/**********************************************************************
 *
 *  This routine is called only when ixgbe_display_debug_stats is enabled.
 *  This routine provides a way to take a look at important statistics
 *  maintained by the driver and hardware.
 *
 **********************************************************************/
void
ixgbe_print_hw_stats(struct ix_softc * sc)
{
	struct ifnet   *ifp = &sc->arpcom.ac_if;;

	printf("%s: missed pkts %llu, rx len errs %llu, crc errs %llu, "
	    "dropped pkts %lu, watchdog timeouts %ld, "
	    "XON rx %llu, XON tx %llu, XOFF rx %llu, XOFF tx %llu, "
	    "total pkts rx %llu, good pkts rx %llu, good pkts tx %llu, "
	    "tso tx %lu\n",
	    ifp->if_xname,
	    (long long)sc->stats.mpc[0],
	    (long long)sc->stats.roc + (long long)sc->stats.ruc,
	    (long long)sc->stats.crcerrs,
	    sc->dropped_pkts,
	    sc->watchdog_events,
	    (long long)sc->stats.lxonrxc,
	    (long long)sc->stats.lxontxc,
	    (long long)sc->stats.lxoffrxc,
	    (long long)sc->stats.lxofftxc,
	    (long long)sc->stats.tpr,
	    (long long)sc->stats.gprc,
	    (long long)sc->stats.gptc,
	    sc->tso_tx);
}
#endif
@


1.149
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.148 2016/12/09 17:41:39 mikeb Exp $	*/
d96 1
a96 1
void	ixgbe_start(struct ifnet *);
d382 1
a382 1
ixgbe_start(struct ifnet * ifp)
d384 1
d390 1
a390 1
	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
d402 1
a402 1
			ifq_set_oactive(&ifp->if_snd);
d406 1
a406 1
		m_head = ifq_dequeue(&ifp->if_snd);
d1616 1
a1616 1
	ifp->if_start = ixgbe_start;
@


1.148
log
@Update the media as the last step in the SFP module configuration

The problem noticed, fix tested and OK procter@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.147 2016/12/06 16:13:01 mikeb Exp $	*/
a2388 1
		++ifp->if_opackets;
@


1.147
log
@Improve error handling and don't fail if SFP module is not present;
tested by Hrvoje Popovski, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.146 2016/12/02 15:10:53 mikeb Exp $	*/
a3257 4
	ifmedia_delete_instance(&sc->media, IFM_INST_ANY);
	ixgbe_add_media_types(sc);
	ifmedia_set(&sc->media, IFM_ETHER | IFM_AUTO);

d3279 4
@


1.146
log
@Disable the TX laser when interface is going down for all fiber modules

Previously only multi-rate fiber modules would disable the TX laser, but
newer Intel driver does it for single rate modules as well.  Reminded by
kettenis@@, tested by procter@@ and Hrvoje Popovski.  Thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.145 2016/11/30 16:15:44 mikeb Exp $	*/
d275 2
a276 5
	} else if (error == IXGBE_ERR_SFP_NOT_SUPPORTED) {
		printf(": Unsupported SFP+ Module\n");
	}

	if (error) {
@


1.145
log
@Update media types upon SFP module change

Tested by Hrvoje Popovski and myself.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.144 2016/11/24 17:39:49 mikeb Exp $	*/
d295 1
a295 1
	if (sc->hw.phy.multispeed_fiber && sc->hw.mac.ops.enable_tx_laser)
d1409 1
a1409 1
	if (sc->hw.phy.multispeed_fiber && sc->hw.mac.ops.disable_tx_laser)
d1745 1
a1745 1
		printf("%s: ixgbe_dma_malloc: bus_dma_tag_create failed; "
@


1.144
log
@Enable support for the X550 family of 10 Gigabit controllers

Code was obtained from FreeBSD.  Make release testing by tb@@ on i386
and mikeb@@ on amd64 and sparc64.  X552 SFP tested by Hrvoje Popovski,
HUGE thanks!  X550T tested by mikeb@@ on amd64 and sparc64.

ok kettenis, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.143 2016/11/24 11:50:58 mikeb Exp $	*/
d3260 4
@


1.143
log
@Fixup active media reporting for multi-speed fiber modules

A gigabit fiber connection was mistakenly reported as 1000baseT
when a mutli-speed 10GbaseSR/1000baseSX fiber optics module was
set to the gigabit mode.

Reported by and fix tested by Hrvoje Popovski <hrvoje at srce ! hr>,
HUGE thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.142 2016/11/21 17:21:33 mikeb Exp $	*/
d74 1
d78 1
d80 8
@


1.142
log
@Add ability to change media type

Tested with a X540 interconnected with a X550 via a CAT6 twisted
pair cable, but is expected to work on multi-speed fiber modules
as well to select between 10GbaseLR and 1000baseLX or 10GbaseSR
and 1000baseSX, etc.

This is largely required because X550 doesn't provide support for
auto-negotiation and requires manual configuration.

Obtained from FreeBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.141 2016/11/21 16:46:29 mikeb Exp $	*/
d1044 5
a1048 3
			case IFM_1000_SX:
			case IFM_1000_LX:
				ifmr->ifm_active |= sc->optics | IFM_FDX;
d1051 1
a1051 1
				ifmr->ifm_active |= IFM_1000_T | IFM_FDX;
@


1.141
log
@Factor out RSS initialization into a separate function

No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.140 2016/11/21 12:37:16 jsg Exp $	*/
d103 1
d1089 1
d1091 1
d1096 3
d1100 22
a1121 8
	case IFM_AUTO:
		sc->hw.phy.autoneg_advertised =
		    IXGBE_LINK_SPEED_100_FULL |
		    IXGBE_LINK_SPEED_1GB_FULL |
		    IXGBE_LINK_SPEED_10GB_FULL;
		break;
	default:
		return (EINVAL);
d1124 3
d1469 2
a1470 2
	else if (layer & (IXGBE_PHYSICAL_LAYER_SFP_PLUS_CU |
			  IXGBE_PHYSICAL_LAYER_SFP_ACTIVE_DA))
d1472 2
a1473 2
	else if (layer & (IXGBE_PHYSICAL_LAYER_10GBASE_LR |
			  IXGBE_PHYSICAL_LAYER_10GBASE_LRM))
d1477 2
a1478 2
	else if (layer & (IXGBE_PHYSICAL_LAYER_10GBASE_KX4 |
			  IXGBE_PHYSICAL_LAYER_10GBASE_CX4))
d1629 1
a1629 4
	if (sc->optics)
		ifmedia_add(&sc->media, IFM_ETHER | sc->optics |
		    IFM_FDX, 0, NULL);
	ifmedia_add(&sc->media, IFM_ETHER | IFM_AUTO, 0, NULL);
d1636 48
@


1.140
log
@Correct the test for requiring a 64 bit mem bar.
ok mikeb@@ who tested on 82599, x540 and x550.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.139 2016/11/21 12:05:28 mikeb Exp $	*/
d118 1
d2554 1
a2554 2
	uint32_t	reta, mrqc = 0, hlreg;
	uint32_t	random[10];
d2611 1
a2611 30
		int j;
		reta = 0;
		/* set up random bits */
		arc4random_buf(&random, sizeof(random));

		/* Set up the redirection table */
		for (i = 0, j = 0; i < 128; i++, j++) {
			if (j == sc->num_queues)
				j = 0;
			reta = (reta << 8) | (j * 0x11);
			if ((i & 3) == 3)
				IXGBE_WRITE_REG(&sc->hw, IXGBE_RETA(i >> 2), reta);
		}

		/* Now fill our hash function seeds */
		for (i = 0; i < 10; i++)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RSSRK(i), random[i]);

		/* Perform hash on these packet types */
		mrqc = IXGBE_MRQC_RSSEN
		    | IXGBE_MRQC_RSS_FIELD_IPV4
		    | IXGBE_MRQC_RSS_FIELD_IPV4_TCP
		    | IXGBE_MRQC_RSS_FIELD_IPV4_UDP
		    | IXGBE_MRQC_RSS_FIELD_IPV6_EX_TCP
		    | IXGBE_MRQC_RSS_FIELD_IPV6_EX
		    | IXGBE_MRQC_RSS_FIELD_IPV6
		    | IXGBE_MRQC_RSS_FIELD_IPV6_TCP
		    | IXGBE_MRQC_RSS_FIELD_IPV6_UDP
		    | IXGBE_MRQC_RSS_FIELD_IPV6_EX_UDP;
		IXGBE_WRITE_REG(&sc->hw, IXGBE_MRQC, mrqc);
d2622 65
@


1.139
log
@Turn on the PHY power during attach

After a cold boot the PHY power might be disabled by another OS:
https://svnweb.freebsd.org/base?view=revision&revision=295093
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.138 2016/11/18 20:03:30 mikeb Exp $	*/
d1519 2
a1520 2
	if (PCI_MAPREG_TYPE(val) != PCI_MAPREG_TYPE_MEM &&
	    PCI_MAPREG_TYPE(val) != PCI_MAPREG_MEM_TYPE_64BIT) {
@


1.138
log
@Sync some changes for ixgbe_initialize_receive_units

Call a chip specific method to disable RX unit;
DPF (Drop [Unicast] Pause Frames) and PMCF (Pass MAC Control Frames)
bits should only be enabled on 82598 since others don't document them.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.137 2016/11/18 19:49:21 mikeb Exp $	*/
d282 8
d738 4
@


1.137
log
@Recognize active SPF+ DA modules as IFM_10G_SFP_CU

Reported and tested by Hrvoje Popovski, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.136 2016/11/18 14:16:10 mikeb Exp $	*/
d2538 1
a2538 1
	struct	rx_ring	*rxr = sc->rx_rings;
d2540 1
a2540 1
	uint32_t	bufsz, rxctrl, fctrl, srrctl, rxcsum;
d2549 1
a2549 3
	rxctrl = IXGBE_READ_REG(&sc->hw, IXGBE_RXCTRL);
	IXGBE_WRITE_REG(&sc->hw, IXGBE_RXCTRL,
	    rxctrl & ~IXGBE_RXCTRL_RXEN);
d2554 4
a2557 2
	fctrl |= IXGBE_FCTRL_DPF;
	fctrl |= IXGBE_FCTRL_PMCF;
@


1.136
log
@Support for new GPI signals, including X550 external PHY interrupt

This factors out the code configuring General Purpose Interrupts into
a separate function and provides LASI (Link Alarm Status Interrupt)
handler used by controllers in the X550 family lacking integrated PHY.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.135 2016/11/18 13:45:57 mikeb Exp $	*/
d1011 1
a1011 1
ixgbe_media_status(struct ifnet * ifp, struct ifmediareq * ifmr)
d1433 2
a1434 1
	else if (layer & IXGBE_PHYSICAL_LAYER_SFP_PLUS_CU)
d1448 2
a1449 2
	/* If we get here just set the default */
	sc->optics = IFM_ETHER | IFM_AUTO;
@


1.135
log
@Reduce difference in ixgbe_initialize_receive_units to FreeBSD;
no binary change.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.134 2016/11/18 13:37:00 mikeb Exp $	*/
d101 1
d151 1
d591 1
a591 1
	uint32_t	 k, txdctl, rxdctl, rxctrl, mhadd, gpie, itr;
d636 3
a641 35
	gpie = IXGBE_READ_REG(&sc->hw, IXGBE_GPIE);

	/* Enable Fan Failure Interrupt */
	gpie |= IXGBE_SDP1_GPIEN;

	if (sc->hw.mac.type == ixgbe_mac_82599EB) {
		/* Add for Module detection */
		gpie |= IXGBE_SDP2_GPIEN;

		/*
		 * Set LL interval to max to reduce the number of low latency
		 * interrupts hitting the card when the ring is getting full.
		 */
		gpie |= 0xf << IXGBE_GPIE_LLI_DELAY_SHIFT;
	}

	if (sc->hw.mac.type == ixgbe_mac_X540) {
		/* Thermal Failure Detection */
		gpie |= IXGBE_SDP0_GPIEN;

		/*
		 * Set LL interval to max to reduce the number of low latency
		 * interrupts hitting the card when the ring is getting full.
		 */
		gpie |= 0xf << IXGBE_GPIE_LLI_DELAY_SHIFT;
	}

	if (sc->msix > 1) {
		/* Enable Enhanced MSIX mode */
		gpie |= IXGBE_GPIE_MSIX_MODE;
		gpie |= IXGBE_GPIE_EIAME | IXGBE_GPIE_PBA_SUPPORT |
		    IXGBE_GPIE_OCD;
	}
	IXGBE_WRITE_REG(&sc->hw, IXGBE_GPIE, gpie);

d750 53
d902 1
a902 1
	uint32_t	 reg_eicr;
a933 1
	/* ... more link status change */
d935 28
a962 1
		if (reg_eicr & IXGBE_EICR_GPI_SDP2) {
d964 1
a964 1
			IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP2);
d969 1
a969 1
		    (reg_eicr & IXGBE_EICR_GPI_SDP1)) {
d971 1
a971 1
			IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP1);
d986 8
a993 6
	/* Check for over temp condition */
	if ((hw->mac.type == ixgbe_mac_X540) &&
	    (reg_eicr & IXGBE_EICR_TS)) {
		printf("%s: CRITICAL: OVER TEMP!! "
		    "PHY IS SHUT DOWN!!\n", ifp->if_xname);
		IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_TS);
d2924 1
d2926 1
d2937 9
d2952 1
a2952 1
	/* With RSS we use auto clear */
d2958 1
a2958 1
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAC, mask);
d3059 2
d3153 20
@


1.134
log
@Minor code restructuring

- separate functions for delay value calculation and figuring out
  whether or not we're doing SFP.
- MAC type detection is now done by ixgbe_set_mac_type;
- call {enable,disable}_tx_laser conditionally;
- unused TSO code bites the dust;
- default to "IFM_ETHER | IFM_AUTO" when we can't select any other
  media type.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.133 2016/10/27 03:06:53 dlg Exp $	*/
d2487 1
d2502 1
a2502 1
	fctrl = IXGBE_READ_REG(&sc->hw, IXGBE_FCTRL);
d2506 1
a2506 1
	IXGBE_WRITE_REG(&sc->hw, IXGBE_FCTRL, fctrl);
d2509 1
a2509 1
	hlreg = IXGBE_READ_REG(&sc->hw, IXGBE_HLREG0);
d2511 1
a2511 1
	IXGBE_WRITE_REG(&sc->hw, IXGBE_HLREG0, hlreg);
d2519 1
a2519 1
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDBAL(i),
d2521 2
a2522 2
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDBAH(i), (rdba >> 32));
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDLEN(i),
d2527 1
a2527 1
		IXGBE_WRITE_REG(&sc->hw, IXGBE_SRRCTL(i), srrctl);
d2530 2
a2531 2
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDH(i), 0);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(i), 0);
d2539 1
a2539 1
		IXGBE_WRITE_REG(&sc->hw, IXGBE_PSRTYPE(0), psrtype);
d2542 1
a2542 1
	rxcsum = IXGBE_READ_REG(&sc->hw, IXGBE_RXCSUM);
d2582 1
d2586 1
a2586 1
	IXGBE_WRITE_REG(&sc->hw, IXGBE_RXCSUM, rxcsum);
@


1.133
log
@tell ix and em to use 2k+ETHER_ALIGN clusters for rx on all archs.

this means that the ethernet header and therefore its payload will
be aligned correctly for the stack. without this em and ix are
sufferring a 30 to 40 percent hit in forwarding performance because
the ethernet stack expects to be able to prepend 8 bytes for an
ethernet header so it can gaurantee its alignment. because em and
ix only had 6 bytes where the ethernet header was, it always prepends
an mbuf which turns out to be expensive. this way the prepend will
be cheap because the 8 byte space will exist.

2k+ETHER_ALIGN clusters will end up using the newly created mcl2k2
pool.

the regression was isolated and the fix tested by hrvoje popovski.
ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.132 2016/04/13 10:34:32 mpi Exp $	*/
d101 2
a102 1
void	ixgbe_config_link(struct ix_softc *sc);
d258 1
a258 1
		    "with your hardware.\n If you are experiencing problems "
a678 1

d765 1
a765 4
	{
		uint32_t rxpb, frame, size, tmp;

		frame = sc->max_frame_size;
a766 20
		/* Calculate High Water */
		if (sc->hw.mac.type == ixgbe_mac_X540)
			tmp = IXGBE_DV_X540(frame, frame);
		else
			tmp = IXGBE_DV(frame, frame);
		size = IXGBE_BT2KB(tmp);
		rxpb = IXGBE_READ_REG(&sc->hw, IXGBE_RXPBSIZE(0)) >> 10;
		sc->hw.fc.high_water[0] = rxpb - size;

		/* Now calculate Low Water */
		if (sc->hw.mac.type == ixgbe_mac_X540)
			tmp = IXGBE_LOW_DV_X540(frame);
		else
			tmp = IXGBE_LOW_DV(frame);
		sc->hw.fc.low_water[0] = IXGBE_BT2KB(tmp);

		sc->hw.fc.requested_mode = sc->fc;
		sc->hw.fc.pause_time = IXGBE_FC_PAUSE;
		sc->hw.fc.send_xon = TRUE;
	}
d781 44
d1310 1
a1310 1
	if (sc->hw.phy.multispeed_fiber)
d1354 2
a1355 39
	switch (sc->hw.device_id) {
	case PCI_PRODUCT_INTEL_82598:
	case PCI_PRODUCT_INTEL_82598AF_DUAL:
	case PCI_PRODUCT_INTEL_82598_DA_DUAL:
	case PCI_PRODUCT_INTEL_82598AF:
	case PCI_PRODUCT_INTEL_82598_SR_DUAL_EM:
	case PCI_PRODUCT_INTEL_82598EB_SFP:
	case PCI_PRODUCT_INTEL_82598EB_CX4_DUAL:
	case PCI_PRODUCT_INTEL_82598EB_CX4:
	case PCI_PRODUCT_INTEL_82598EB_XF_LR:
	case PCI_PRODUCT_INTEL_82598AT:
	case PCI_PRODUCT_INTEL_82598AT2:
	case PCI_PRODUCT_INTEL_82598AT_DUAL:
	case PCI_PRODUCT_INTEL_82598_BX:
		sc->hw.mac.type = ixgbe_mac_82598EB;
		break;
	case PCI_PRODUCT_INTEL_82599EN_SFP:
	case PCI_PRODUCT_INTEL_82599_SFP:
	case PCI_PRODUCT_INTEL_82599_SFP_EM:
	case PCI_PRODUCT_INTEL_82599_SFP_FCOE:
	case PCI_PRODUCT_INTEL_82599_SFP_SF2:
	case PCI_PRODUCT_INTEL_82599_KX4:
	case PCI_PRODUCT_INTEL_82599_KX4_MEZZ:
	case PCI_PRODUCT_INTEL_82599_CX4:
	case PCI_PRODUCT_INTEL_82599_T3_LOM:
	case PCI_PRODUCT_INTEL_82599_XAUI:
	case PCI_PRODUCT_INTEL_82599_COMBO_BP:
	case PCI_PRODUCT_INTEL_82599_BPLANE_FCOE:
		sc->hw.mac.type = ixgbe_mac_82599EB;
		break;
	case PCI_PRODUCT_INTEL_82599VF:
		sc->hw.mac.type = ixgbe_mac_82599_vf;
		break;
	case PCI_PRODUCT_INTEL_X540T:
		sc->hw.mac.type = ixgbe_mac_X540;
		break;
	default:
		break;
	}
d1396 2
d1557 1
a1557 15
	bool		sfp, negotiate;

	switch (sc->hw.phy.type) {
	case ixgbe_phy_sfp_avago:
	case ixgbe_phy_sfp_ftl:
	case ixgbe_phy_sfp_intel:
	case ixgbe_phy_sfp_unknown:
	case ixgbe_phy_sfp_passive_tyco:
	case ixgbe_phy_sfp_passive_unknown:
		sfp = TRUE;
		break;
	default:
		sfp = FALSE;
		break;
	}
d1559 1
a1559 1
	if (sfp) {
d1562 2
a1563 1
			sc->hw.mac.ops.enable_tx_laser(&sc->hw);
d1580 1
a1580 1
			err = sc->hw.mac.ops.setup_link(&sc->hw,
a2165 123
#ifdef notyet
/**********************************************************************
 *
 *  Setup work for hardware segmentation offload (TSO) on
 *  adapters using advanced tx descriptors
 *
 **********************************************************************/
int
ixgbe_tso_setup(struct tx_ring *txr, struct mbuf *mp,
    uint32_t *cmd_type_len, uint32_t *olinfo_status)
{
	struct ix_softc *sc = txr->sc;
	struct ixgbe_adv_tx_context_desc *TXD;
	uint32_t vlan_macip_lens = 0, type_tucmd_mlhl = 0;
	uint32_t mss_l4len_idx = 0, paylen;
	int ctxd, ehdrlen, ip_hlen, tcp_hlen;
	uint16_t etype;
#if NVLAN > 0
	uint16_t vtag = 0;
	struct ether_vlan_header *eh;
#else
	struct ether_header *eh;
#endif
	struct ip *ip;
	struct ip6_hdr *ip6;
	struct tcphdr *th;

	/*
	 * Determine where frame payload starts.
	 * Jump over vlan headers if already present
	 */
#if NVLAN > 0
	eh = mtod(mp, struct ether_vlan_header *);
	if (eh->evl_encap_proto == htons(ETHERTYPE_VLAN)) {
		etype = ntohs(eh->evl_proto);
		ehdrlen = ETHER_HDR_LEN + ETHER_VLAN_ENCAP_LEN;
	} else {
		etype = ntohs(eh->evl_encap_proto);
		ehdrlen = ETHER_HDR_LEN;
	}
#else
	eh = mtod(mp, struct ether_header *);
	etype = ntohs(eh->ether_type);
	ehdrlen = ETHER_HDR_LEN;
#endif

	switch (etype) {
	case ETHERTYPE_IPV6:
		ip6 = (struct ip6_hdr *)(mp->m_data + ehdrlen);
		/* XXX-BZ For now we do not pretend to support ext. hdrs. */
		if (ip6->ip6_nxt != IPPROTO_TCP)
			return (ENXIO);
		ip_hlen = sizeof(struct ip6_hdr);
		ip6 = (struct ip6_hdr *)(mp->m_data + ehdrlen);
		th = (struct tcphdr *)((caddr_t)ip6 + ip_hlen);
		th->th_sum = in6_cksum_pseudo(ip6, 0, IPPROTO_TCP, 0);
		type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV6;
		break;
	case ETHERTYPE_IP:
		ip = (struct ip *)(mp->m_data + ehdrlen);
		if (ip->ip_p != IPPROTO_TCP)
			return (ENXIO);
		ip->ip_sum = 0;
		ip_hlen = ip->ip_hl << 2;
		th = (struct tcphdr *)((caddr_t)ip + ip_hlen);
		th->th_sum = in_pseudo(ip->ip_src.s_addr,
		    ip->ip_dst.s_addr, htons(IPPROTO_TCP));
		type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV4;
		/* Tell transmit desc to also do IPv4 checksum. */
		*olinfo_status |= IXGBE_TXD_POPTS_IXSM << 8;
		break;
	default:
		panic("%s: CSUM_TSO but no supported IP version (0x%04x)",
		    __func__, ntohs(etype));
		break;
	}

	ctxd = txr->next_avail_desc;
	TXD = (struct ixgbe_adv_tx_context_desc *) &txr->tx_base[ctxd];

	tcp_hlen = th->th_off << 2;

	/* This is used in the transmit desc in encap */
	paylen = mp->m_pkthdr.len - ehdrlen - ip_hlen - tcp_hlen;

#if NVLAN > 0
	/* VLAN MACLEN IPLEN */
	if (mp->m_flags & M_VLANTAG) {
		vtag = mp->m_pkthdr.ether_vtag;
		vlan_macip_lens |= (vtag << IXGBE_ADVTXD_VLAN_SHIFT);
	}
#endif

	vlan_macip_lens |= ehdrlen << IXGBE_ADVTXD_MACLEN_SHIFT;
	vlan_macip_lens |= ip_hlen;
	TXD->vlan_macip_lens = htole32(vlan_macip_lens);

	/* ADV DTYPE TUCMD */
	type_tucmd_mlhl |= IXGBE_ADVTXD_DCMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;
	type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_TCP;
	TXD->type_tucmd_mlhl = htole32(type_tucmd_mlhl);

	/* MSS L4LEN IDX */
	mss_l4len_idx |= (mp->m_pkthdr.tso_segsz << IXGBE_ADVTXD_MSS_SHIFT);
	mss_l4len_idx |= (tcp_hlen << IXGBE_ADVTXD_L4LEN_SHIFT);
	TXD->mss_l4len_idx = htole32(mss_l4len_idx);

	TXD->seqnum_seed = htole32(0);

	membar_producer();

	if (++ctxd == sc->num_tx_desc)
		ctxd = 0;

	atomic_dec_int(&txr->tx_avail);
	txr->next_avail_desc = ctxd;
	*cmd_type_len |= IXGBE_ADVTXD_DCMD_TSE;
	*olinfo_status |= IXGBE_TXD_POPTS_TXSM << 8;
	*olinfo_status |= paylen << IXGBE_ADVTXD_PAYLEN_SHIFT;
	return TRUE;
}
#endif

d2345 1
a2345 1
	int			i, bsize, error;
a2346 1
	bsize = sizeof(struct ixgbe_rx_buf) * sc->num_rx_desc;
a2353 1
	bsize = sizeof(struct ixgbe_rx_buf) * sc->num_rx_desc;
@


1.132
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.131 2015/12/31 19:07:37 kettenis Exp $	*/
a618 1
#ifdef __STRICT_ALIGNMENT
a619 3
#else
	sc->rx_mbuf_sz = MCLBYTES;
#endif
a2456 1
#ifdef __STRICT_ALIGNMENT
a2457 1
#endif
a2663 1
#ifdef __STRICT_ALIGNMENT
a2664 3
#else
	bufsz = sc->rx_mbuf_sz >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;
#endif
@


1.131
log
@Make ixgbe_start() mpsafe.  This means the driver will no longer grab the
kernel lock in the rx and tx path anymore.

While there seems to be a small decrease in forwarding performance with our
default network stack settings, Performance whiel receiving manymore packets
than we can handle is better.  And this change opens the road for future
improvements in the network stack.

ok dlg@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.130 2015/12/18 22:47:18 kettenis Exp $	*/
a1541 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.130
log
@Make ix(4) mpsafer.  Take advantage of intr_barrier() to eliminate the mutex
introduced in the previous step, and use atomic instructions to make the
tx completion path mpsafe as well.

ok claudio@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.129 2015/11/25 03:09:59 dlg Exp $	*/
d379 7
a385 1
		m_head = ifq_deq_begin(&ifp->if_snd);
d390 2
a391 13
			ifq_deq_rollback(&ifp->if_snd, m_head);
			ifq_set_oactive(&ifp->if_snd);
			/*
			 *  Make sure there are still packets on the
			 *  ring.  The interrupt handler may have
			 *  cleaned up the ring before we were able to
			 *  set the IF_OACTIVE flag.
			 */
			if (txr->tx_avail == sc->num_tx_desc) {
				ifq_clr_oactive(&ifp->if_snd);
				continue;
			}
			break;
a393 2
		ifq_deq_commit(&ifp->if_snd, m_head);

a889 2
		if (!IFQ_IS_EMPTY(&ifp->if_snd))
			ixgbe_start(ifp);
d891 1
a1053 4
	/* Check that we have least the minimal number of TX descriptors. */
	if (txr->tx_avail <= IXGBE_TX_OP_THRESHOLD)
		return (ENOBUFS);

d1083 1
a1083 4
	if (map->dm_nsegs > txr->tx_avail - 2) {
		error = ENOBUFS;
		goto xmit_fail;
	}
a1140 1

a1282 1
	ifq_clr_oactive(&ifp->if_snd);
d1301 1
d1306 2
d1378 1
a1378 1
	if (sc->hw.mac.type != ixgbe_mac_82598EB) {
d1380 1
a1380 3
		sc->num_segs = IXGBE_82599_SCATTER;
	} else
		sc->num_segs = IXGBE_82598_SCATTER;
d1534 1
d2427 2
a2428 11
	/*
	 * If we have enough room, clear IFF_OACTIVE to tell the stack that
	 * it is OK to send packets.
	 */
	if (ifq_is_oactive(&ifp->if_snd) &&
	    num_avail > IXGBE_TX_OP_THRESHOLD) {
		ifq_clr_oactive(&ifp->if_snd);
		KERNEL_LOCK();
		ixgbe_start(ifp);
		KERNEL_UNLOCK();
	}
@


1.129
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.128 2015/11/20 03:35:23 dlg Exp $	*/
a212 2
	mtx_init(&sc->rx_mtx, IPL_NET);

d386 10
d538 1
a538 4
	 * Then ixgbe_txeof() keeps resetting to 5 as long as it cleans at
	 * least one descriptor.
	 * Finally, anytime all descriptors are clean the timer is
	 * set to 0.
d870 1
a870 1
	int		 i, refill = 0, was_active = 0;
d880 1
a881 4

		if (ifq_is_oactive(&ifp->if_snd))
			was_active = 1;
		ixgbe_txeof(txr);
d897 2
a919 6
	if (was_active) {
		KERNEL_LOCK();
		ixgbe_start(ifp);
		KERNEL_UNLOCK();
	}

d1062 3
a1064 11
	/*
	 * Force a cleanup if number of TX descriptors
	 * available is below the threshold. If it fails
	 * to get above, then abort transmit.
	 */
	if (txr->tx_avail <= IXGBE_TX_CLEANUP_THRESHOLD) {
		ixgbe_txeof(txr);
		/* Make sure things have improved */
		if (txr->tx_avail <= IXGBE_TX_OP_THRESHOLD)
			return (ENOBUFS);
	}
a1127 2
	txr->tx_avail -= map->dm_nsegs;
	txr->next_avail_desc = i;
d1145 5
d1318 4
d2202 2
d2208 1
a2208 1
	--txr->tx_avail;
d2322 2
d2327 1
a2327 1
	txr->tx_avail--;
d2349 1
d2361 1
a2361 1
	KERNEL_LOCK();
d2366 1
a2366 2
	if (txr->tx_buffers == NULL) {
		KERNEL_UNLOCK();
a2367 1
	}
d2372 1
a2372 2
	if (last == -1) {
		KERNEL_UNLOCK();
a2373 1
	}
a2394 1
			++txr->tx_avail;
d2436 6
d2444 1
a2444 3
	 * it is OK to send packets. If there are no pending descriptors,
	 * clear the timeout. Otherwise, if some descriptors have been freed,
	 * restart the timeout.
d2446 2
a2447 1
	if (txr->tx_avail > IXGBE_TX_CLEANUP_THRESHOLD) {
d2449 3
a2451 13

		/* If all are clean turn off the timer */
		if (txr->tx_avail == sc->num_tx_desc) {
			ifp->if_timer = 0;
			txr->watchdog_timer = 0;
			KERNEL_UNLOCK();
			return FALSE;
		}
		/* Some were cleaned, so reset timer */
		else if (processed) {
			ifp->if_timer = IXGBE_TX_TIMEOUT;
			txr->watchdog_timer = IXGBE_TX_TIMEOUT;
		}
a2453 2
	KERNEL_UNLOCK();

a2602 2
	mtx_enter(&sc->rx_mtx);

a2617 2
	mtx_leave(&sc->rx_mtx);

a2784 1
	mtx_enter(&sc->rx_mtx);
a2786 1
	mtx_leave(&sc->rx_mtx);
a2839 1
	struct mbuf_list	 free_ml = MBUF_LIST_INITIALIZER();
a2851 1
	mtx_enter(&sc->rx_mtx);
d2886 1
a2886 1
				ml_enqueue(&free_ml, rxbuf->fmp);
d2890 1
a2890 1
			ml_enqueue(&free_ml, mp);
a2967 3
	mtx_leave(&sc->rx_mtx);

	ml_purge(&free_ml);
@


1.128
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.127 2015/11/04 00:20:35 dlg Exp $	*/
d371 1
a371 1
	if ((ifp->if_flags & (IFF_RUNNING|IFF_OACTIVE)) != IFF_RUNNING)
d387 1
a387 1
			ifp->if_flags |= IFF_OACTIVE;
d804 1
a804 1
	ifp->if_flags &= ~IFF_OACTIVE;
d877 1
a877 1
		if (ISSET(ifp->if_flags, IFF_OACTIVE))
d1305 2
a1306 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
d2446 1
a2446 1
		ifp->if_flags &= ~IFF_OACTIVE;
@


1.127
log
@replace while (ml_dequeue()) m_freem(); with ml_purge();
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.126 2015/10/25 13:04:28 mpi Exp $	*/
d381 1
a381 1
		IFQ_POLL(&ifp->if_snd, m_head);
d386 1
d391 1
a391 1
		IFQ_DEQUEUE(&ifp->if_snd, m_head);
@


1.126
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.125 2015/09/11 12:09:10 claudio Exp $	*/
d2986 1
a2986 2
	while ((mp = ml_dequeue(&free_ml)))
		m_freem(mp);
@


1.125
log
@First step at making ix(4) MPSAVE. This is largely based on the em(4)
changes done by kettenis@@. Tested by Hrvoje Popovski and chris@@
dlg@@, mpi@@ and kettenis@@ agree on developping this further in tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.124 2015/09/02 16:08:49 deraadt Exp $	*/
a429 1
	struct ifaddr	*ifa = (struct ifaddr *) data;
a440 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->arpcom, ifa);
@


1.124
log
@correct sizes for free(), sigh
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.123 2015/09/01 07:09:55 deraadt Exp $	*/
d213 2
d867 1
a867 1
	int		 i, refill = 0;
d875 18
d894 2
a895 1
	if (reg_eicr & IXGBE_EICR_LSC)
d897 2
d900 1
d905 1
d907 1
d912 1
d914 1
d918 6
a939 18
	if (ifp->if_flags & IFF_RUNNING) {
		ixgbe_rxeof(que);
		ixgbe_txeof(txr);
		refill = 1;
	}

	if (refill) {
		if (ixgbe_rxfill(que->rxr)) {
			/* Advance the Rx Queue "Tail Pointer" */
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(que->rxr->me),
			    que->rxr->last_desc_filled);
		} else
			timeout_add(&sc->rx_refill, 1);
	}

	if (ifp->if_flags & IFF_RUNNING && !IFQ_IS_EMPTY(&ifp->if_snd))
		ixgbe_start(ifp);

d1468 1
a1468 1
	sc->tag = pci_intr_establish(pc, ih, IPL_NET,
d2352 3
d2360 2
d2364 5
d2373 2
a2374 1
	if (last == -1)
d2376 1
d2453 1
d2463 2
d2614 2
d2631 2
d2797 1
a2797 1
	struct rx_ring *rxr = sc->rx_rings;
d2800 6
a2805 1
	for (i = 0; i < sc->num_queues; i++, rxr++)
d2857 1
d2870 1
d2905 1
a2905 1
				m_freem(rxbuf->fmp);
d2909 1
a2909 1
			m_freem(mp);
d2987 4
@


1.123
log
@sizes for free(), mostly related to firmwares.
ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.122 2015/08/29 17:40:09 kettenis Exp $	*/
d2036 1
a2036 1
		    sc->num_queues * sizeof(struct tx_ring));
d2798 1
a2798 1
		    sc->num_queues * sizeof(struct rx_ring));
@


1.122
log
@Set the rx mbuf size to MCLBYTES + ETHER_ALIGN for strict alignment
architectures to communicate better what size is needed to mclgeti.
Makes ix(4) consistent with em(4).

ok mikeb@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.121 2015/06/24 09:40:54 mpi Exp $	*/
d306 2
a307 1
	free(sc->mta, M_DEVBUF, 0);
d345 2
a346 1
	free(sc->mta, M_DEVBUF, 0);
d1798 1
a1798 1
	free(sc->rx_rings, M_DEVBUF, 0);
d1801 1
a1801 1
	free(sc->tx_rings, M_DEVBUF, 0);
d2035 2
a2036 1
		free(txr->tx_buffers, M_DEVBUF, 0);
d2797 2
a2798 1
		free(rxr->rx_buffers, M_DEVBUF, 0);
@


1.121
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.120 2015/05/21 07:39:52 gerhard Exp $	*/
d618 1
d620 1
a620 2
	/* Use 4k clusters, even for jumbo frames */
	sc->rx_mbuf_sz = 4096;
a621 1
	/* Use 2k clusters, even for jumbo frames */
@


1.120
log
@Access to uninitialized variable fixed.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.119 2015/04/30 14:17:26 jsg Exp $	*/
a2921 1
			ifp->if_ipackets++;
@


1.119
log
@Set the correct media type for 1000baseLX SFPs.
Tested by/ok sthen@@, ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.118 2015/03/20 10:41:15 mikeb Exp $	*/
d3239 1
a3239 1
	uint32_t autoneg, err;
d3243 4
a3246 5
	if ((!autoneg) && (hw->mac.ops.get_link_capabilities))
		err = hw->mac.ops.get_link_capabilities(hw, &autoneg,
		    &negotiate);
	if (err)
		return;
@


1.118
log
@Re-apply -r1.115 that got accidentally reverted and brought to my
attention and fix re-tested by Kapetanakis Giannis.  Thanks a lot!

Original commit message:

When setting up advanced TX descriptor use m_getptr to locate the IP
or IPv6 header instead of assuming contiguousness of the target buffer
across Ethernet and IP/IPv6 headers.

Tested by Kapetanakis Giannis <bilias at edu ! physics ! uoc ! gr>,
thanks!  Problem analysis and initial diff by dlg@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.115 2015/01/12 10:40:51 mikeb Exp $	*/
d956 9
a964 2
			ifmr->ifm_active |= ((sc->optics == IFM_1000_SX) ?
			    IFM_1000_SX : IFM_1000_T) | IFM_FDX;
d1417 2
@


1.117
log
@convert to if_input.

ok mpi@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.116 2015/01/20 12:56:50 kettenis Exp $	*/
d2052 2
d2099 2
d2104 2
d2123 5
a2127 1
		ip = (struct ip *)(mp->m_data + ehdrlen);
d2134 6
a2139 2
		ip6 = (struct ip6_hdr *)(mp->m_data + ehdrlen);
		ip_hlen = sizeof(struct ip6_hdr);
@


1.116
log
@Make ix(4) work on strict alignment architectures.  The Intel networking
hardware is fairly retarded.  While it allows receive buffers with an
ETHER_ALIGN offset, it only allows the size of the buffers to be specified in
multiples of 1K.  This means that if we want to use standard mbuf clusters
we will waste 1024 - ETHER_ALIGN bytes per cluster, which is a lot for the
2K clusters we use now.  Compromise a bit by using 4K clusters on strict
alignment architectures and tell the hardware to use 3K of those, reducing
the spillage a bit.  While this isn't optimal, at least on sparc64 where we
have 8K pages, the pool page allocation overhead should be the same as on
amd64/i386 where we have 4K pages and continue to use 2K mbuf clusters.

ok mikeb@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.114 2015/01/11 03:06:19 deraadt Exp $	*/
d2790 1
a2898 1
			sendmp->m_pkthdr.rcvif = ifp;
d2907 1
a2907 7
#if NBPFILTER > 0
			if (ifp->if_bpf)
				bpf_mtap_ether(ifp->if_bpf, sendmp,
				    BPF_DIRECTION_IN);
#endif

			ether_input_mbuf(ifp, sendmp);
d2920 2
@


1.115
log
@When setting up advanced TX descriptor use m_getptr to locate the IP
or IPv6 header instead of assuming contiguousness of the target buffer
across Ethernet and IP/IPv6 headers.

Tested by Kapetanakis Giannis <bilias at edu ! physics ! uoc ! gr>,
thanks!  Problem analysis and initial diff by dlg@@.
@
text
@d499 1
a499 1
		ifr[n].ifr_size = MCLBYTES;
d618 4
d624 1
a2051 2
	struct mbuf *m;
	int ipoff;
a2096 2
	if (mp->m_len < sizeof(struct ether_header))
		return (1);
a2099 2
		if (mp->m_len < sizeof(struct ether_vlan_header))
			return (1);
d2117 1
a2117 5
		if (mp->m_pkthdr.len < ehdrlen + sizeof(*ip))
			return (1);
		m = m_getptr(mp, ehdrlen, &ipoff);
		KASSERT(m != NULL && m->m_len - ipoff >= sizeof(*ip));
		ip = (struct ip *)(m->m_data + ipoff);
d2124 2
a2125 6
		if (mp->m_pkthdr.len < ehdrlen + sizeof(*ip6))
			return (1);
		m = m_getptr(mp, ehdrlen, &ipoff);
		KASSERT(m != NULL && m->m_len - ipoff >= sizeof(*ip6));
		ip6 = (struct ip6 *)(m->m_data + ipoff);
		ip_hlen = sizeof(*ip6);
d2441 3
a2443 2
	if (sc->max_frame_size <= (sc->rx_mbuf_sz - ETHER_ALIGN))
		m_adj(mp, ETHER_ALIGN);
d2650 3
d2654 1
a2851 1
		/* XXX ixgbe_realign() STRICT_ALIGN */
@


1.114
log
@mallocarray() for the rx_buffer memory
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.113 2014/12/22 02:28:52 tedu Exp $	*/
d2047 2
d2094 2
d2099 2
d2118 5
a2122 1
		ip = (struct ip *)(mp->m_data + ehdrlen);
d2129 6
a2134 2
		ip6 = (struct ip6_hdr *)(mp->m_data + ehdrlen);
		ip_hlen = sizeof(struct ip6_hdr);
@


1.113
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.112 2014/12/13 21:05:33 doug Exp $	*/
d233 1
a233 1
	sc->mta = malloc(sizeof(uint8_t) * IXGBE_ETH_LENGTH_OF_ADDRESS *
d2478 2
a2479 2
	if (!(rxr->rx_buffers = (struct ixgbe_rx_buf *) malloc(bsize,
	    M_DEVBUF, M_NOWAIT | M_ZERO))) {
d2485 1
@


1.112
log
@yet more mallocarray() changes.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.111 2014/11/27 15:49:13 brad Exp $	*/
a437 1
#ifdef INET
a439 1
#endif
@


1.111
log
@Another spot where the VLAN tag doesn't need swapping in the currently
unused TSO code.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.110 2014/11/27 07:54:06 kettenis Exp $	*/
d493 1
a493 1
		if ((ifr = malloc(sc->num_queues * sizeof(*ifr), M_DEVBUF,
@


1.110
log
@Use dv_xname instead of if_xname to establish our interrupt.  The latter is
still unset when at this point, and some MD variants of pci_intr_establish(9)
make a copy of the string instead of storing a pointer.

Makes vmstat -i properly print the device name on sparc64.

ok mikeb@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.109 2014/11/26 00:09:27 dlg Exp $	*/
d2259 1
a2259 1
		vtag = htole16(mp->m_pkthdr.ether_vtag);
@


1.109
log
@dont swap the vlan tag twice on big endian archs for transmit.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.108 2014/11/20 03:35:56 brad Exp $	*/
a1416 1
	struct ifnet		*ifp = &sc->arpcom.ac_if;
d1440 1
a1440 1
	    ixgbe_intr, sc, ifp->if_xname);
@


1.108
log
@- Remove some unused #if 0'd code which does not pertain to OpenBSD
- Remove a bogus if_ierrors++ which if the counter was incremented it
  would be overwritten by ixgbe_update_stats_counters()

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.107 2014/11/12 16:06:47 mikeb Exp $	*/
d2083 1
a2083 1
		vtag = htole16(mp->m_pkthdr.ether_vtag);
@


1.107
log
@Remove SIOCSIFMTU handling and misuse of if_mtu values for MRU

Since there's now no way to select maximum receive unit size the
hardware is programmed to accept frame sizes up to 9216 which is
now the maximum (down from 15.5K since this is supposed to work
in all advanced configurations and gives slightly better flow
control watermark ranges) and split all frames larger 2K into
multiple fragments (code was already there but wasn't enabled).

Tested on 82599 (SFP+) and X540 (10GBaseT).
With input from dlg@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.106 2014/11/10 17:53:43 mikeb Exp $	*/
a2824 1
			ifp->if_ierrors++;
a3324 1
#if 0
a3325 6
	ifp->if_ipackets = sc->stats.gprc;
	ifp->if_opackets = sc->stats.gptc;
	ifp->if_ibytes = sc->stats.gorc;
	ifp->if_obytes = sc->stats.gotc;
	ifp->if_imcasts = sc->stats.mprc;
#endif
@


1.106
log
@add an additional error check into the ixgbe_handle_msf
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.105 2014/11/10 17:53:10 mikeb Exp $	*/
a443 12
	case SIOCSIFMTU:
		IOCTL_DEBUGOUT("ioctl: SIOCSIFMTU (Set Interface MTU)");
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ifp->if_hardmtu)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu) {
			ifp->if_mtu = ifr->ifr_mtu;
			sc->max_frame_size =
				ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN;
			ixgbe_init(sc);
		}
		break;

d620 2
a621 9
	/* Determine the correct mbuf pool for doing jumbo frames */
	if (sc->max_frame_size <= 2048)
		sc->rx_mbuf_sz = MCLBYTES;
	else if (sc->max_frame_size <= 4096)
		sc->rx_mbuf_sz = 4096;
	else if (sc->max_frame_size <= 9216)
		sc->rx_mbuf_sz = 9216;
	else
		sc->rx_mbuf_sz = 16 * 1024;
d673 5
a677 7
	/* Set MTU size */
	if (ifp->if_mtu > ETHERMTU) {
		mhadd = IXGBE_READ_REG(&sc->hw, IXGBE_MHADD);
		mhadd &= ~IXGBE_MHADD_MFS_MASK;
		mhadd |= sc->max_frame_size << IXGBE_MHADD_MFS_SHIFT;
		IXGBE_WRITE_REG(&sc->hw, IXGBE_MHADD, mhadd);
	}
d1560 1
a1560 2
	sc->max_frame_size =
	    ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN;
a2620 1
	struct ifnet   *ifp = &sc->arpcom.ac_if;
d2641 1
a2641 1
	/* Set for Jumbo Frames? */
d2643 1
a2643 4
	if (ifp->if_mtu > ETHERMTU)
		hlreg |= IXGBE_HLREG0_JUMBOEN;
	else
		hlreg &= ~IXGBE_HLREG0_JUMBOEN;
@


1.105
log
@remove pointless timeout_del/add dance in the interrupt handler
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.104 2014/11/10 17:07:52 mikeb Exp $	*/
d3240 1
a3240 1
	uint32_t autoneg;
d3245 4
a3248 1
		hw->mac.ops.get_link_capabilities(hw, &autoneg, &negotiate);
@


1.104
log
@Inadvertent ampersand has made the check to always yield truth

This change fixes up SFP+ module detection during "ifconfig up"
after the machine has been booted without the modules plugged in.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.103 2014/11/10 16:35:06 mikeb Exp $	*/
d891 1
a891 2
	if (reg_eicr & IXGBE_EICR_LSC) {
		timeout_del(&sc->timer);
a892 2
		timeout_add_sec(&sc->timer, 1);
	}
@


1.103
log
@Gather full statistics only when IX_DEBUG is defined
since most of them can't be retrieved otherwise.  This
comes with a slight but measurable performance increase
as well.

Also since the hardware has a single counter for missed
packets including those caused by the insufficient DMA
buffers available, this makes it hard to decipher actual
errors when used with Rx ring length limiting mechanisms
like if_rxr or mclgeti.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.102 2014/11/10 16:01:18 mikeb Exp $	*/
d1609 1
a1609 1
		if (&sc->hw.phy.multispeed_fiber) {
@


1.102
log
@don't try to update the link status every second
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.101 2014/11/10 15:58:32 mikeb Exp $	*/
d3263 2
a3265 1
	uint64_t	total_missed_rx = 0;
d3267 1
d3270 1
d3272 1
a3324 1
	sc->stats.rlec += IXGBE_READ_REG(hw, IXGBE_RLEC);
d3350 1
@


1.101
log
@remove ixgbe_sfp_probe since it's not called anyways
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.100 2014/09/08 02:39:57 chris Exp $	*/
a1239 1
	ixgbe_update_link_status(sc);
@


1.100
log
@Match 82599 as found on SuperMicro AOC-STGN-I1S

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.99 2014/08/26 11:01:22 mikeb Exp $	*/
a145 1
bool	ixgbe_sfp_probe(struct ix_softc *);
a1228 9

/*********************************************************************
 *  Timer routine
 *
 *  This routine checks for link status,updates statistics,
 *  and runs the watchdog check.
 *
 **********************************************************************/

a1239 5
	/* Check for pluggable optics */
	if (sc->sfp_probe)
		if (!ixgbe_sfp_probe(sc))
			goto out; /* Nothing to do */

a1242 1
out:
d1248 1
a3206 30
}

/*
 * ixgbe_sfp_probe - called in the local timer to
 * determine if a port had optics inserted.
 */
bool
ixgbe_sfp_probe(struct ix_softc *sc)
{
	bool result = FALSE;

	if ((sc->hw.phy.type == ixgbe_phy_nl) &&
	    (sc->hw.phy.sfp_type == ixgbe_sfp_type_not_present)) {
		int32_t  ret = sc->hw.phy.ops.identify_sfp(&sc->hw);
		if (ret)
			goto out;
		ret = sc->hw.phy.ops.reset(&sc->hw);
		if (ret == IXGBE_ERR_SFP_NOT_SUPPORTED) {
			printf("%s: Unsupported SFP+ module detected!",
			    sc->dev.dv_xname);
			goto out;
		}
		/* We now have supported optics */
		sc->sfp_probe = FALSE;
		/* Set the optics type so system reports correctly */
		ixgbe_setup_optics(sc);
		result = TRUE;
	}
out:
	return (result);
@


1.99
log
@Revert part of the if_rxr diff that incorrectly moves RX ring tail
index update code from the buf_get success path to the do it all
the time code path.  Tested by millert;  ok dlg, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.98 2014/08/25 14:26:25 mikeb Exp $	*/
d76 1
d1384 1
@


1.98
log
@We have never limited the definition of "supported SPF modules"
to the vendor/make whitelist maintained by Intel so there's no
reason to start doing it now.

When syncing the driver to the FreeBSD codebase I have decided
to take this chunk as is but it appears that it breaks cheap
chinese SFP+ fiber optics modules that we all love.  And while
there's still a lot of places where we check for the vendor
OUI, most of these checks are not necessary.

Issue reported and fix tested by Tony Sarendal.  Thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.97 2014/08/20 10:06:31 mikeb Exp $	*/
d2602 1
d2605 1
a2606 2

	rxr->last_desc_filled = i;
@


1.97
log
@Implement rxrinfo ioctl for cluster usage statistics
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.96 2014/07/13 23:10:23 deraadt Exp $	*/
d261 1
a261 1
		printf("Unsupported SFP+ Module\n");
@


1.96
log
@Some reallocarray() use; review Jean-Philippe Ouellet, patrick keshishian
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.95 2014/07/12 18:48:51 tedu Exp $	*/
d87 1
d475 4
d493 30
@


1.96.4.1
log
@OpenBSD 5.6 errata 1: Incorrect RX ring computation leads to panics
under load with bge(4), em(4) and ix(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.96 2014/07/13 23:10:23 deraadt Exp $	*/
a2566 1
		rxr->last_desc_filled = i;
d2569 1
d2571 1
a2571 1
	if_rxr_put(&rxr->rx_ring, slots);
@


1.95
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.94 2014/07/08 05:35:18 dlg Exp $	*/
d1706 2
a1707 3
	if (!(sc->queues =
	    (struct ix_queue *) malloc(sizeof(struct ix_queue) *
	    sc->num_queues, M_DEVBUF, M_NOWAIT | M_ZERO))) {
d1713 2
a1714 3
	if (!(sc->tx_rings =
	    (struct tx_ring *) malloc(sizeof(struct tx_ring) *
	    sc->num_queues, M_DEVBUF, M_NOWAIT | M_ZERO))) {
d1720 2
a1721 3
	if (!(sc->rx_rings =
	    (struct rx_ring *) malloc(sizeof(struct rx_ring) *
	    sc->num_queues, M_DEVBUF, M_NOWAIT | M_ZERO))) {
d1814 2
a1815 3
	if (!(txr->tx_buffers =
	    (struct ixgbe_tx_buf *) malloc(sizeof(struct ixgbe_tx_buf) *
	    sc->num_tx_desc, M_DEVBUF, M_NOWAIT | M_ZERO))) {
@


1.94
log
@cut things that relied on mclgeti for rx ring accounting/restriction over
to using if_rxr.

cut the reporting systat did over to the rxr ioctl.

tested as much as i can on alpha, amd64, and sparc64.
mpi@@ has run it on macppc.
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.93 2013/12/09 19:48:04 mikeb Exp $	*/
d305 1
a305 1
	free(sc->mta, M_DEVBUF);
d343 1
a343 1
	free(sc->mta, M_DEVBUF);
d1793 1
a1793 1
	free(sc->rx_rings, M_DEVBUF);
d1796 1
a1796 1
	free(sc->tx_rings, M_DEVBUF);
d2031 1
a2031 1
		free(txr->tx_buffers, M_DEVBUF);
d2776 1
a2776 1
		free(rxr->rx_buffers, M_DEVBUF);
@


1.93
log
@initialize staterr, no functional change;  from david hill
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.92 2013/08/23 19:44:14 mikeb Exp $	*/
a1537 2
	m_clsetwms(ifp, MCLBYTES, 4, sc->num_rx_desc);

d2442 1
a2442 1
	mp = MCLGETI(NULL, M_DONTWAIT, &sc->arpcom.ac_if, sc->rx_mbuf_sz);
a2468 2
	rxr->rx_ndescs++;

d2526 1
d2540 3
a2542 1
	rxr->rx_ndescs = 0;
d2545 1
a2545 1
	if (rxr->rx_ndescs < 1) {
d2559 1
d2563 2
a2564 1
	while (rxr->rx_ndescs < sc->num_rx_desc) {
a2570 1
		rxr->last_desc_filled = i;
d2573 3
d2807 1
a2807 1
	while (rxr->rx_ndescs > 0) {
d2853 1
a2853 1
			    i, rxr->rx_ndescs,
a2903 2
			m_cluncount(sendmp, 1);

d2922 1
a2922 1
		rxr->rx_ndescs--;
@


1.92
log
@don't call if_link_state_change if link state is not changed
and fix minor discrepancies with link state handling
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.91 2013/08/21 13:52:57 mpi Exp $	*/
d2794 1
a2794 1
	uint32_t		 staterr, ptype;
@


1.91
log
@Fix build without vlan.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.90 2013/08/05 19:58:05 mikeb Exp $	*/
a948 1
	}
d950 16
a965 15
	switch (sc->hw.fc.current_mode) {
	case ixgbe_fc_tx_pause:
		ifmr->ifm_active |= IFM_FLOW | IFM_ETH_TXPAUSE;
		break;
	case ixgbe_fc_rx_pause:
		ifmr->ifm_active |= IFM_FLOW | IFM_ETH_RXPAUSE;
		break;
	case ixgbe_fc_full:
		ifmr->ifm_active |= IFM_FLOW | IFM_ETH_RXPAUSE |
		    IFM_ETH_TXPAUSE;
		break;
	default:
		ifmr->ifm_active &= ~(IFM_FLOW | IFM_ETH_RXPAUSE |
		    IFM_ETH_TXPAUSE);
		break;
d1241 2
a1242 1
	if (sc->link_up)
d1244 1
a1244 4
	if (ifp->if_link_state == link_state)
		return;
	ifp->if_baudrate = 0;
	if (link_state != LINK_STATE_DOWN) {
d1259 7
a1266 4
	if (sc->link_up) /* Update any Flow Control changes */
		sc->hw.mac.ops.fc_enable(&sc->hw);
	ifp->if_link_state = link_state;
	if_link_state_change(ifp);
a1409 2
	else
		sc->optics = IFM_ETHER | IFM_AUTO;
d1555 1
a1555 1
		     ixgbe_media_status);
@


1.90
log
@First stab at updating this monster to the Intel/FreeBSD current version.
This syncs PHY and chip dependent parts as well as brings support for the
flow control and additional (untested) bits for 1G fiber versions.

Tested by Hrvoje Popovski <hrvoje at srce ! hr> on 82599/SFP+DA, florian@@
and benno@@ on 82599/SFP+SR and on 82598/CX4-BP, 82599/SFP+DA and X540/RJ45
by me.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.89 2013/06/14 16:25:54 mikeb Exp $	*/
d2067 1
d2069 1
@


1.89
log
@Correct interrupt moderation setting for 82598; tested on the CX4 version
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.88 2013/06/09 18:09:55 deraadt Exp $	*/
d5 1
a5 1
  Copyright (c) 2001-2008, Intel Corporation
d35 1
a35 1
/*$FreeBSD: src/sys/dev/ixgbe/ixgbe.c,v 1.5 2008/05/16 18:46:30 jfv Exp $*/
d43 1
a43 2

#define IXGBE_DRIVER_VERSION	"1.4.4"
a76 3
#if 0
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599VF }
#endif
d144 1
a144 1
int	ixgbe_sfp_probe(struct ix_softc *);
d150 1
a150 1
int	ixgbe_legacy_irq(void *);
a153 1
void	ixgbe_handle_que(void *, int);
a211 3
	/* Core Lock Init*/
	mtx_init(&sc->core_mtx, IPL_NET);

d234 1
a234 1
	if (sc->mta == 0) {
d240 2
a241 31
	switch (hw->mac.type) {
	case ixgbe_mac_82598EB:
		error = ixgbe_init_ops_82598(hw);
		break;
	case ixgbe_mac_82599EB:
		error = ixgbe_init_ops_82599(hw);
		break;
#if 0
	case ixgbe_mac_82599_vf:
		error = ixgbe_init_ops_vf(hw);
		break;
#endif
	case ixgbe_mac_X540:
		error = ixgbe_init_ops_X540(hw);
		break;
	default:
		error = IXGBE_ERR_DEVICE_NOT_SUPPORTED;
		break;
	}
	if (error == IXGBE_ERR_SFP_NOT_PRESENT) {
		/*
		 * No optics in this port, set up
		 * so the timer routine will probe
		 * for later insertion.
		 */
		sc->sfp_probe = TRUE;
		error = 0;
	} else if (error == IXGBE_ERR_SFP_NOT_SUPPORTED) {
		printf(": Unsupported SFP+ module detected!\n");
		goto err_late;
	} else if (error) {
d252 1
a252 8
	/* Get Hardware Flow Control setting */
	hw->fc.requested_mode = ixgbe_fc_full;
	hw->fc.pause_time = IXGBE_FC_PAUSE;
	hw->fc.low_water = IXGBE_FC_LO;
	hw->fc.high_water = IXGBE_FC_HI;
	hw->fc.send_xon = TRUE;

	error = sc->hw.mac.ops.init_hw(hw);
a273 1
	/* XXX sc->msix > 1 && ixgbe_allocate_msix() */
d284 1
a284 1
	/* Print PCIE bus type/speed/width info */
d287 3
a363 1
	uint32_t		 queue = 0;
a370 14
#if 0
	/*
	 * This is really just here for testing
	 * TX multiqueue, ultimately what is
	 * needed is the flow support in the stack
	 * and appropriate logic here to deal with
	 * it. -jfv
	 */
	if (sc->num_queues > 1)
		queue = (curcpu % sc->num_queues);
#endif

	txr = &sc->tx_rings[queue];

a493 3
 *  This routine is called by the local timer
 *  to detect hardware hangs .
 *
d527 1
a527 1
	if (IXGBE_READ_REG(hw, IXGBE_TFCS) & IXGBE_TFCS_TXOFF) {
d577 1
a577 1
	ixgbe_hw(&sc->hw, set_rar, 0, sc->hw.mac.addr, 0, IXGBE_RAH_AV);
d582 1
a582 1
	ixgbe_hw(&sc->hw, set_rar, 0, sc->hw.mac.addr, 0, 1);
d594 1
a594 1
	ixgbe_hw0(&sc->hw, init_hw);
d597 1
a597 1
	/* Determine the correct buffer size for jumbo */
d701 1
a701 1
		/* XXX wmb() : memory barrier */
d713 1
a713 1
	ixgbe_hw(&sc->hw, enable_rx_dma, rxctrl);
a716 1
#ifdef MSI
d718 1
a718 1
	if (ixgbe_enable_msix) {
d727 1
a727 3
	} else  /* Simple settings for Legacy/MSI */
#else
	{
a731 1
#endif
d752 29
a831 45
void
ixgbe_rearm_queue(struct ix_softc *sc, uint32_t vector)
{
	uint64_t queue = 1ULL << vector;
	uint32_t mask;

	if (sc->hw.mac.type == ixgbe_mac_82598EB) {
		mask = (IXGBE_EIMS_RTX_QUEUE & queue);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EICS, mask);
	} else {
		mask = (queue & 0xFFFFFFFF);
		if (mask)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EICS_EX(0), mask);
		mask = (queue >> 32);
		if (mask)
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EICS_EX(1), mask);
	}
}

void
ixgbe_handle_que(void *context, int pending)
{
	struct ix_queue *que = context;
	struct ix_softc *sc = que->sc;
	struct tx_ring	*txr = que->txr;
	struct ifnet	*ifp = &que->sc->arpcom.ac_if;

	if (ifp->if_flags & IFF_RUNNING) {
		ixgbe_rxeof(que);
		ixgbe_txeof(txr);

		if (ixgbe_rxfill(que->rxr)) {
			/* Advance the Rx Queue "Tail Pointer" */
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(que->rxr->me),
			    que->rxr->last_desc_filled);
		}

		if (!IFQ_IS_EMPTY(&ifp->if_snd))
			ixgbe_start(ifp);
	}

	/* Reenable this interrupt */
	ixgbe_enable_queue(que->sc, que->msix);
}

d839 1
a839 1
ixgbe_legacy_irq(void *arg)
a854 17
	/* Check for fan failure */
	if ((hw->phy.media_type == ixgbe_media_type_copper) &&
	    (reg_eicr & IXGBE_EICR_GPI_SDP1)) {
		printf("%s: CRITICAL: FAN FAILURE!! "
		    "REPLACE IMMEDIATELY!!\n", ifp->if_xname);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMS,
		    IXGBE_EICR_GPI_SDP1);
	}

	/* Check for over temp condition */
	if ((hw->mac.type == ixgbe_mac_X540) &&
	    (reg_eicr & IXGBE_EICR_GPI_SDP0)) {
		printf("%s: CRITICAL: OVER TEMP!! "
		    "PHY IS SHUT DOWN!!\n", ifp->if_xname);
		IXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP0);
	}

d867 1
a867 7
		}
#if 0
		/*
		 * XXX: Processing of SDP1 (multispeed fiber) interrupts is
		 *      disabled due to the lack of testing
		 */
		else if ((hw->phy.media_type != ixgbe_media_type_copper) &&
d873 16
a888 1
#endif
d942 2
a943 1
			ifmr->ifm_active |= IFM_1000_T | IFM_FDX;
d978 1
a978 1
ixgbe_media_change(struct ifnet * ifp)
d980 17
a996 1
	/* ignore */
d1002 3
a1004 3
 *  This routine maps the mbufs to tx descriptors.
 *    WARNING: while this code is using an MQ style infrastructure,
 *    it would NOT work as is with more than 1 queue.
a1005 1
 *  return 0 on success, positive on failure
d1012 1
a1012 1
	uint32_t	olinfo_status = 0, cmd_type_len = 0;
d1020 2
a1021 2
	cmd_type_len |= IXGBE_ADVTXD_DTYP_DATA |
	    IXGBE_ADVTXD_DCMD_IFCS | IXGBE_ADVTXD_DCMD_DEXT;
d1052 12
a1063 3
	error = bus_dmamap_load_mbuf(txr->txdma.dma_tag, map,
	    m_head, BUS_DMA_NOWAIT);
	if (error != 0) {
d1076 1
a1076 1
	 * this becomes the first descriptor.
d1078 2
a1079 2
	if ((error = ixgbe_tx_ctx_setup(txr, m_head, &cmd_type_len,
	    &olinfo_status)) != 0)
d1106 6
a1111 1
	/* swap maps because last tx descriptor is tracking all the data */
d1170 2
a1171 2
		ixgbe_hw(&sc->hw, update_mc_addr_list,
		    update_ptr, mcnt, ixgbe_mc_array_itr);
d1199 1
a1199 1
 *  and runs the watchdog timer.
d1239 1
a1239 1
	ixgbe_hw(&sc->hw, check_link, &sc->link_speed, &sc->link_up, 0);
d1262 2
a1268 1

d1271 1
a1271 1
 *  This routine disables all traffic on the sc by issuing a
d1288 1
a1288 1
	ixgbe_hw0(&sc->hw, reset_hw);
d1290 3
a1292 1
	ixgbe_hw0(&sc->hw, stop_adapter);
d1295 1
a1295 1
		ixgbe_hw0(&sc->hw, disable_tx_laser);
d1300 1
a1300 1
	ixgbe_hw(&sc->hw, set_rar, 0, sc->hw.mac.addr, 0, IXGBE_RAH_AV);
a1337 2
		sc->hw.mac.type = ixgbe_mac_82598EB;
		break;
a1339 2
		sc->hw.mac.type = ixgbe_mac_82598EB;
		break;
a1340 2
		sc->hw.mac.type = ixgbe_mac_82598EB;
		break;
a1343 2
		sc->hw.mac.type = ixgbe_mac_82598EB;
		break;
a1350 3
		sc->hw.mac.type = ixgbe_mac_82599EB;
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
		break;
a1353 3
		sc->hw.mac.type = ixgbe_mac_82599EB;
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
		break;
a1354 3
		sc->hw.mac.type = ixgbe_mac_82599EB;
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
		break;
a1358 1
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
a1361 1
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
a1364 1
		sc->hw.phy.smart_speed = ixgbe_smart_speed;
d1369 7
d1389 1
a1389 1
	layer = ixgbe_hw(hw, get_supported_physical_layer);
d1395 4
a1403 2
	else if (layer & IXGBE_PHYSICAL_LAYER_SFP_PLUS_CU)
		sc->optics = IFM_10G_SFP_CU;
d1407 2
d1445 1
a1445 1
	    ixgbe_legacy_irq, sc, ifp->if_xname);
a1524 1
	struct ixgbe_hw *hw = &sc->hw;
a1525 1
	INIT_DEBUGOUT("ixgbe_setup_interface: begin");
d1557 3
a1559 9
	ifmedia_add(&sc->media, IFM_ETHER | sc->optics |
	    IFM_FDX, 0, NULL);
	if ((hw->device_id == PCI_PRODUCT_INTEL_82598AT) ||
	    (hw->device_id == PCI_PRODUCT_INTEL_82598AT_DUAL)) {
		ifmedia_add(&sc->media,
		    IFM_ETHER | IFM_1000_T | IFM_FDX, 0, NULL);
		ifmedia_add(&sc->media,
		    IFM_ETHER | IFM_1000_T, 0, NULL);
	}
d1574 1
a1574 1
	int		sfp, negotiate = FALSE;
d1583 1
a1583 1
		sfp = 1;
d1586 1
a1586 1
		sfp = 0;
d1593 4
a1596 4
			ixgbe_hw0(&sc->hw, enable_tx_laser);
			/* XXX taskqueue_enqueue(sc->tq, &sc->msf_task); */
		} /* else */
			/* XXX taskqueue_enqueue(sc->tq, &sc->mod_task); */
d1603 1
a1603 4
		/* XXX: must be changeable in ixgbe_media_change */
		autoneg = IXGBE_LINK_SPEED_100_FULL |
			  IXGBE_LINK_SPEED_1GB_FULL |
			  IXGBE_LINK_SPEED_10GB_FULL;
d1610 2
a1611 2
			err = sc->hw.mac.ops.setup_link(&sc->hw, autoneg,
			    negotiate, sc->link_up);
a1614 1

d1651 2
a1652 3
	r = bus_dmamap_load(dma->dma_tag, dma->dma_map,
	    dma->dma_vaddr, size, NULL,
	    mapflags | BUS_DMA_NOWAIT);
d1734 1
a1734 1
	    sizeof(union ixgbe_adv_tx_desc), 4096);
a1746 3
		/* Initialize the TX side lock */
		mtx_init(&txr->tx_mtx, IPL_NET);

a1767 3
		/* Initialize the TX side lock */
		mtx_init(&rxr->rx_mtx, IPL_NET);

d1794 1
a1794 1
	for (txr = sc->tx_rings; txconf > 0; txr++, txconf--) {
a1795 1
	}
d1815 2
a1816 3
	struct ix_softc 	*sc;
	struct ixgbe_osdep	*os;
	struct ifnet		*ifp;
a1818 10
	int			 max_segs;

	sc = txr->sc;
	os = &sc->osdep;
	ifp = &sc->arpcom.ac_if;

	if (sc->hw.mac.type == ixgbe_mac_82598EB)
		max_segs = IXGBE_82598_SCATTER;
	else
		max_segs = IXGBE_82599_SCATTER;
d1834 1
a1834 1
			    max_segs, PAGE_SIZE, 0,
d1952 1
a1952 1
		txctrl &= ~IXGBE_DCA_TXCTRL_TX_WB_RO_EN;
d2185 1
a2185 1
 *  scs using advanced tx descriptors
d2276 1
a2276 1
	TXD->vlan_macip_lens |= htole32(vlan_macip_lens);
d2604 1
a2604 1
	for (i = 0; i < sc->num_queues; i++, rxr++) {
a2606 1
	}
a2608 1

d2616 1
a2616 1
 *  Enable receive unit.
a2950 1
		/* Did it pass? */
a2956 1

a2957 1
		/* Did it pass? */
d3007 1
a3007 1
	uint32_t mask = IXGBE_EIMS_ENABLE_MASK & ~IXGBE_EIMS_RTX_QUEUE;
d3010 1
d3014 3
a3016 1
	else {
d3021 10
d3036 1
a3036 1
	if (sc->msix) {
d3038 1
a3038 1
		/* Dont autoclear Link */
d3058 1
a3058 1
	if (sc->msix)
d3164 1
a3168 1
#if 0
d3170 1
a3170 1
		newitr = (8000000 / ixgbe_max_interrupt_rate) & 0x0FF8;
a3171 1
#endif
d3186 1
d3193 1
a3193 1
int
d3196 1
a3196 1
	int result = FALSE;
d3255 1
a3255 1
	int negotiate;
d3261 1
a3261 1
		hw->mac.ops.setup_link(hw, autoneg, negotiate, TRUE);
d3297 17
a3313 3
	sc->stats.gorc += IXGBE_READ_REG(hw, IXGBE_GORCH);
	sc->stats.gotc += IXGBE_READ_REG(hw, IXGBE_GOTCH);
	sc->stats.tor += IXGBE_READ_REG(hw, IXGBE_TORH);
a3332 3

	sc->stats.lxonrxc += IXGBE_READ_REG(hw, IXGBE_LXONRXC);
	sc->stats.lxoffrxc += IXGBE_READ_REG(hw, IXGBE_LXOFFRXC);
@


1.88
log
@adapt to a pcidevs name change
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.87 2012/12/20 17:34:54 mikeb Exp $	*/
d808 2
a809 4
	if (sc->hw.mac.type == ixgbe_mac_82598EB)
		itr = (8000000 / IXGBE_INTS_PER_SEC) & 0xff8;
	else {
		itr = (4000000 / IXGBE_INTS_PER_SEC) & 0xff8;
a810 1
	}
@


1.87
log
@report flow control mode in the media status callback
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.86 2012/12/20 17:07:37 mikeb Exp $	*/
d69 1
a69 1
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_COMBO_BACKPLANE },
d1431 1
a1431 1
	case PCI_PRODUCT_INTEL_82599_COMBO_BACKPLANE:
@


1.86
log
@max_frame_size must be set after ether_ifattach updates if_mtu;
verified with the upstream driver
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.85 2012/12/17 18:30:28 mikeb Exp $	*/
d1039 17
@


1.85
log
@Sync up advanced transmitter descriptor setup code to the upstream.
There are a number of bugs that this commit fixes.  The main issue
is that ixgbe_tx_ctx_setup sets up a descriptor for TCP and UDP
checksum calculation, but not for IP, therefore it must use TXSM
instead of IXSM.  Which is what FreeBSD does now.  I've tested this
on 82599 with TCP, UDP, ICMP, ICMP6, VLANs and OSPF with TCP and
UDP checksums turned on against oce(4).  ixgbe_tso_setup is updated
as well, albeit remains disabled (as well as checksums themselves).
Parts of the change were obtained from brad's diff he sent me a
while ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.84 2012/12/17 14:39:28 mikeb Exp $	*/
a1602 3
	sc->max_frame_size =
	    ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN;

d1623 3
a2528 1
	/* only adjust if this is not a split header */
@


1.84
log
@sync some comments and variable names in rxeof with freebsd
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.83 2012/12/17 14:23:48 mikeb Exp $	*/
d137 4
a140 2
int	ixgbe_tx_ctx_setup(struct tx_ring *, struct mbuf *);
int	ixgbe_tso_setup(struct tx_ring *, struct mbuf *, uint32_t *);
a1075 1
	uint32_t	paylen = 0;
d1078 2
a1079 2
	cmd_type_len |= IXGBE_ADVTXD_DTYP_DATA;
	cmd_type_len |= IXGBE_ADVTXD_DCMD_IFCS | IXGBE_ADVTXD_DCMD_DEXT;
d1125 1
a1125 2
	 * this becomes the first descriptor of
	 * a packet.
d1127 3
a1129 16
#ifdef notyet
	if (ixgbe_tso_setup(txr, m_head, &paylen)) {
		cmd_type_len |= IXGBE_ADVTXD_DCMD_TSE;
		olinfo_status |= IXGBE_TXD_POPTS_IXSM << 8;
		olinfo_status |= IXGBE_TXD_POPTS_TXSM << 8;
		olinfo_status |= paylen << IXGBE_ADVTXD_PAYLEN_SHIFT;
		++sc->tso_tx;
	} else
#endif
	if (ixgbe_tx_ctx_setup(txr, m_head))
		olinfo_status |= IXGBE_TXD_POPTS_IXSM << 8;

	/* Record payload length */
	if (paylen == 0)
		olinfo_status |= m_head->m_pkthdr.len <<
		    IXGBE_ADVTXD_PAYLEN_SHIFT;
d1600 1
a1600 2
	ifp->if_capabilities |= IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4 |
	    IFCAP_CSUM_IPv4;
d2127 2
a2128 1
ixgbe_tx_ctx_setup(struct tx_ring *txr, struct mbuf *mp)
a2130 1
	struct ifnet	*ifp = &sc->arpcom.ac_if;
d2132 6
a2137 2
	struct ixgbe_tx_buf        *tx_buffer;
	uint32_t vlan_macip_lens = 0, type_tucmd_mlhl = 0;
d2142 2
a2143 2
	uint8_t ipproto = 0;
	int  ehdrlen, ip_hlen = 0;
d2145 9
a2153 6
	int offload = TRUE;
	int ctxd = txr->next_avail_desc;
#if NVLAN > 0
	struct ether_vlan_header *eh;
#else
	struct ether_header *eh;
a2154 1
	uint16_t vtag = 0;
d2156 1
a2156 1
	if ((ifp->if_capabilities & IFCAP_CSUM_IPv4) == 0)
d2159 5
a2164 1
	TXD = (struct ixgbe_adv_tx_context_desc *) &txr->tx_base[ctxd];
d2168 2
a2169 1
	 * be placed into the descriptor itself.
d2178 1
a2178 1
		return FALSE;	/* No need for CTX */
a2206 2
		if (mp->m_len < ehdrlen + ip_hlen)
			return FALSE; /* failure */
d2208 1
a2208 2
		if (mp->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV4;
d2214 1
a2214 2
		if (mp->m_len < ehdrlen + ip_hlen)
			return FALSE; /* failure */
d2216 1
a2216 2
		if (mp->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV6;
d2236 3
d2241 3
d2245 2
a2246 2
	TXD->vlan_macip_lens |= htole32(vlan_macip_lens);
	TXD->type_tucmd_mlhl |= htole32(type_tucmd_mlhl);
d2259 1
a2259 1
	return (offload);
d2270 2
a2271 1
ixgbe_tso_setup(struct tx_ring *txr, struct mbuf *mp, uint32_t *paylen)
a2274 1
	struct ixgbe_tx_buf        *tx_buffer;
d2276 3
a2278 2
	uint32_t mss_l4len_idx = 0;
	int ctxd, ehdrlen,  hdrlen, ip_hlen, tcp_hlen;
d2286 1
a2288 4
	if (((mp->m_pkthdr.csum_flags & CSUM_TSO) == 0) ||
	    (mp->m_pkthdr.len <= IXGBE_TX_BUFFER_SIZE))
		return FALSE;

d2295 2
a2296 1
	if (eh->evl_encap_proto == htons(ETHERTYPE_VLAN))
d2298 2
a2299 1
	else
d2301 1
d2304 1
d2308 30
a2337 3
	/* Ensure we have at least the IP+TCP header in the first mbuf. */
	if (mp->m_len < ehdrlen + sizeof(struct ip) + sizeof(struct tcphdr))
		return FALSE;
a2339 1
	tx_buffer = &txr->tx_buffers[ctxd];
a2341 9
	ip = (struct ip *)(mp->m_data + ehdrlen);
	if (ip->ip_p != IPPROTO_TCP)
		return FALSE;   /* 0 */
	ip->ip_len = 0;
	ip->ip_sum = 0;
	ip_hlen = ip->ip_hl << 2;
	th = (struct tcphdr *)((caddr_t)ip + ip_hlen);
	th->th_sum = in_pseudo(ip->ip_src.s_addr,
	    ip->ip_dst.s_addr, htons(IPPROTO_TCP));
d2343 1
a2343 1
	hdrlen = ehdrlen + ip_hlen + tcp_hlen;
d2345 1
a2345 1
	*paylen = mp->m_pkthdr.len - hdrlen;
d2362 1
a2362 3
	type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV4;
	TXD->type_tucmd_mlhl |= htole32(type_tucmd_mlhl);

a2369 1
	tx_buffer->m_head = NULL;
d2376 3
a2379 8
}

#else
/* This makes it easy to keep the code common */
int
ixgbe_tso_setup(struct tx_ring *txr, struct mbuf *mp, uint32_t *paylen)
{
	return (FALSE);
@


1.83
log
@Catch up with upstream, where rxbuf->m_pack was renamed to rxbuf->buf
and rxbuf->pmap to rxbuf->map.  Tested on 82599 and X540.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.82 2012/12/17 14:03:03 mikeb Exp $	*/
d2867 1
a2867 1
	uint16_t		 plen, hdr, vtag;
d2901 1
a2901 1
		plen = letoh16(rxdesc->wb.upper.length);
a2903 1
		hdr = letoh16(rxdesc->wb.lower.lo_dword.hs_rss.hdr_info);
d2941 7
a2947 1
		mp->m_len = plen;
d2969 1
a2970 1
			/* Pass the head pointer on */
d2974 1
a2974 3
		} else {
			/* Sending this frame? */

@


1.82
log
@Don't forget to decrement a number of clusters on the ring in case
of an error.  Previous change made the problem evident.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.81 2012/12/17 13:46:23 mikeb Exp $	*/
d655 1
a655 1
	/* Determine the correct buffer size for jumbo/headersplit */
d2508 1
a2508 1
	if (rxbuf->m_pack) {
d2524 1
a2524 1
	error = bus_dmamap_load_mbuf(rxr->rxdma.dma_tag, rxbuf->pmap,
d2531 3
a2533 3
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->pmap,
	    0, rxbuf->pmap->dm_mapsize, BUS_DMASYNC_PREREAD);
	rxbuf->m_pack = mp;
d2538 1
a2538 1
	rxdesc->read.pkt_addr = htole64(rxbuf->pmap->dm_segs[0].ds_addr);
d2576 1
a2576 1
		    16 * 1024, 0, BUS_DMA_NOWAIT, &rxbuf->pmap);
d2835 3
a2837 3
			if (rxbuf->m_pack != NULL) {
				bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->pmap,
				    0, rxbuf->pmap->dm_mapsize,
d2840 3
a2842 3
				    rxbuf->pmap);
				m_freem(rxbuf->m_pack);
				rxbuf->m_pack = NULL;
d2844 2
a2845 2
			bus_dmamap_destroy(rxr->rxdma.dma_tag, rxbuf->pmap);
			rxbuf->pmap = NULL;
d2896 3
a2898 3
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->pmap, 0,
		    rxbuf->pmap->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->pmap);
d2900 1
a2900 1
		mp = rxbuf->m_pack;
d2918 1
a2918 1
			rxbuf->m_pack = NULL;
d2948 1
a2948 1
		rxbuf->m_pack = rxbuf->fmp = NULL;
d2968 1
a2968 1
			mp->m_next = nxbuf->m_pack;
@


1.81
log
@Get rid of the split header code as it was never used in
OpenBSD and now is finally removed from the upstream.
No real functional change (we've lost some weight though).
Tested on 82599.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.80 2012/12/17 12:28:06 mikeb Exp $	*/
d2991 1
a2991 1

a2992 1
next_desc:
@


1.80
log
@Implement SFP+ module hot-plug support for 82599 obtained
from FreeBSD.  This also adds untested and hence disabled
support for multispeed fiber interrupts.  With input from
and ok jsg.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.79 2012/12/17 12:03:16 mikeb Exp $	*/
d2501 1
a2501 1
	struct mbuf		*mp, *mh = NULL;
d2508 1
a2508 1
	if (rxbuf->m_head != NULL || rxbuf->m_pack) {
a2518 28
	if (rxr->hdr_split == FALSE)
		goto no_split;

	mh = m_gethdr(M_DONTWAIT, MT_DATA);
	if (mh == NULL) {
		m_freem(mp);
		return (ENOBUFS);
	}

	mh->m_pkthdr.len = mh->m_len = MHLEN;
	mh->m_len = MHLEN;
	/* always offset header buffers */
	m_adj(mh, ETHER_ALIGN);

	error = bus_dmamap_load_mbuf(rxr->rxdma.dma_tag, rxbuf->hmap,
	    mh, BUS_DMA_NOWAIT);
	if (error) {
		m_freem(mp);
		m_freem(mh);
		return (error);
	}
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->hmap,
	    0, rxbuf->hmap->dm_mapsize, BUS_DMASYNC_PREREAD);
	rxbuf->m_head = mh;

	rxdesc->read.hdr_addr = htole64(rxbuf->hmap->dm_segs[0].ds_addr);

no_split:
d2521 1
a2521 2
	if (rxr->hdr_split == FALSE &&
	    sc->max_frame_size <= (sc->rx_mbuf_sz - ETHER_ALIGN))
a2526 5
		if (mh) {
			bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->hmap);
			rxbuf->m_head = NULL;
			m_freem(mh);
		}
a2574 7
		error = bus_dmamap_create(rxr->rxdma.dma_tag, MSIZE, 1,
		    MSIZE, 0, BUS_DMA_NOWAIT, &rxbuf->hmap);
		if (error) {
			printf("%s: Unable to create Head DMA map\n",
			    ifp->if_xname);
			goto fail;
		}
d2742 1
a2742 9
		srrctl = bufsz;
		if (rxr->hdr_split) {
			/* Use a standard mbuf for the header */
			srrctl |= ((IXGBE_RX_HDR <<
			    IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT)
			    & IXGBE_SRRCTL_BSIZEHDR_MASK);
			srrctl |= IXGBE_SRRCTL_DESCTYPE_HDR_SPLIT_ALWAYS;
		} else
			srrctl |= IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;
a2834 9
			if (rxbuf->m_head != NULL) {
				bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->hmap,
				    0, rxbuf->hmap->dm_mapsize,
				    BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(rxr->rxdma.dma_tag,
				    rxbuf->hmap);
				m_freem(rxbuf->m_head);
				rxbuf->m_head = NULL;
			}
a2843 1
			bus_dmamap_destroy(rxr->rxdma.dma_tag, rxbuf->hmap);
a2844 1
			rxbuf->hmap = NULL;
d2865 1
a2865 1
	struct mbuf    		*mh, *mp, *sendmp;
d2867 1
a2867 1
	uint16_t		 hlen, plen, hdr, vtag;
a2895 3
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->hmap, 0,
		    rxbuf->hmap->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->hmap);
a2899 1
		mh = rxbuf->m_head;
a2916 1
			m_freem(mh);
a2917 1
			rxbuf->m_head = NULL;
d2941 2
d2944 2
a2945 9
		 * The header mbuf is ONLY used when header
		 * split is enabled, otherwise we get normal
		 * behavior, ie, both header and payload
		 * are DMA'd into the payload buffer.
		 *
		 * Rather than using the fmp/lmp global pointers
		 * we now keep the head of a packet chain in the
		 * buffer struct and pass this along from one
		 * descriptor to the next, until we get EOP.
d2947 13
a2959 23
		if (rxr->hdr_split && (rxbuf->fmp == NULL)) {
			/* This must be an initial descriptor */
			hlen = (hdr & IXGBE_RXDADV_HDRBUFLEN_MASK) >>
			    IXGBE_RXDADV_HDRBUFLEN_SHIFT;
			if (hlen > IXGBE_RX_HDR)
				hlen = IXGBE_RX_HDR;
			mh->m_len = hlen;
			mh->m_pkthdr.len = mh->m_len;
			rxbuf->m_head = NULL;
			/*
			 * Check the payload length, this could be zero if
			 * its a small packet.
			 */
			if (plen > 0) {
				mp->m_len = plen;
				mp->m_flags &= ~M_PKTHDR;
				mh->m_next = mp;
				mh->m_pkthdr.len += mp->m_len;
				rxbuf->m_pack = NULL;
				rxr->rx_split_packets++;
			} else {
				m_freem(mp);
				rxbuf->m_pack = NULL;
a2960 17
			/* Now create the forward chain. */
			if (eop == 0) {
				/* stash the chain head */
				nxbuf->fmp = mh;
				/* Make forward chain */
				if (plen)
					mp->m_next = nxbuf->m_pack;
				else
					mh->m_next = nxbuf->m_pack;
			} else {
				/* Singlet, prepare to send */
				sendmp = mh;
#if NVLAN > 0
				if (staterr & IXGBE_RXD_STAT_VP) {
					sendmp->m_pkthdr.ether_vtag = vtag;
					sendmp->m_flags |= M_VLANTAG;
				}
d2962 7
a2968 1
			}
d2970 1
a2970 12
			/*
			 * Either no header split, or a
			 * secondary piece of a fragmented
			 * split packet.
			 */
			mp->m_len = plen;
			/*
			 * See if there is a stored head
			 * that determines what we are
			 */
			sendmp = rxbuf->fmp;
			rxbuf->m_pack = rxbuf->fmp = NULL;
a2971 23
			if (sendmp != NULL) /* secondary frag */
				sendmp->m_pkthdr.len += mp->m_len;
			else {
				 /* first desc of a non-ps chain */
				 sendmp = mp;
				 sendmp->m_pkthdr.len = mp->m_len;
#if NVLAN > 0
				if (staterr & IXGBE_RXD_STAT_VP) {
					sendmp->m_pkthdr.ether_vtag = vtag;
					sendmp->m_flags |= M_VLANTAG;
				}
#endif
			}
			/* Pass the head pointer on */
			if (eop == 0) {
				nxbuf->fmp = sendmp;
				sendmp = NULL;
				mp->m_next = nxbuf->m_pack;
			}
		}
		rxr->rx_ndescs--;
		/* Sending this frame? */
		if (eop) {
d2991 2
a3037 1

@


1.79
log
@Fix a link autonegotiation bug on 10GbaseT controllers and improve
link information reporting in general.  Obtained for the most part
from FreeBSD, tested by mxb at alumni ! chalmers ! se on X540 and
me on 82598 (XAUI, KR4), 82599 (SFP+) and X540 (baseT); ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.78 2012/12/05 12:21:12 mikeb Exp $	*/
d148 2
a935 6
	if (ifp->if_flags & IFF_RUNNING) {
		ixgbe_rxeof(que);
		ixgbe_txeof(txr);
		refill = 1;
	}

d960 26
d3405 45
@


1.78
log
@fix from freebsd to correctly account rx errors;  problem reported
and fix tested on 82599 by tony sarendal tony@@polarcap.org, tested
on X540 and 82598 by me, ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.77 2012/11/29 21:10:32 brad Exp $	*/
d308 3
d407 1
a407 1
	if (!sc->link_active)
d1271 2
a1272 5
	int link_up = FALSE;
	struct ifnet *ifp = &sc->arpcom.ac_if;
	struct tx_ring *txr = sc->tx_rings;
	int		link_state;
	int		i;
d1274 1
a1274 1
	ixgbe_hw(&sc->hw, check_link, &sc->link_speed, &link_up, 0);
d1276 6
a1281 9
	link_state = link_up ? LINK_STATE_FULL_DUPLEX : LINK_STATE_DOWN;

	if (ifp->if_link_state != link_state) {
		sc->link_active = link_up;
		ifp->if_link_state = link_state;
		if_link_state_change(ifp);
	}

	if (LINK_STATE_IS_UP(ifp->if_link_state)) {
a1295 5
	} else {
		ifp->if_baudrate = 0;
		ifp->if_timer = 0;
		for (i = 0; i < sc->num_queues; i++)
			txr[i].watchdog_timer = FALSE;
d1297 2
a1370 1
		sc->optics = IFM_10G_SR;
a1374 1
		sc->optics = IFM_10G_CX4;
a1377 1
		sc->optics = IFM_10G_LR;
a1382 1
		sc->optics = IFM_10G_T;
a1385 1
		sc->optics = IFM_AUTO;
a1391 1
		sc->optics = IFM_10G_SR;
a1397 1
		sc->optics = IFM_10G_CX4;
a1401 1
		sc->optics = IFM_10G_T;
a1407 1
		sc->optics = IFM_AUTO;
a1411 1
		sc->optics = IFM_AUTO;
a1415 1
		sc->optics = IFM_10G_T;
a1418 1
		sc->optics = IFM_AUTO;
d1435 17
a1451 29
	switch (layer) {
		case IXGBE_PHYSICAL_LAYER_10GBASE_T:
			sc->optics = IFM_10G_T;
			break;
		case IXGBE_PHYSICAL_LAYER_1000BASE_T:
			sc->optics = IFM_1000_T;
			break;
		case IXGBE_PHYSICAL_LAYER_10GBASE_LR:
		case IXGBE_PHYSICAL_LAYER_10GBASE_LRM:
			sc->optics = IFM_10G_LR;
			break;
		case IXGBE_PHYSICAL_LAYER_10GBASE_SR:
			sc->optics = IFM_10G_SR;
			break;
		case IXGBE_PHYSICAL_LAYER_10GBASE_KX4:
		case IXGBE_PHYSICAL_LAYER_10GBASE_CX4:
			sc->optics = IFM_10G_CX4;
			break;
		case IXGBE_PHYSICAL_LAYER_SFP_PLUS_CU:
			sc->optics = IFM_10G_SFP_CU;
			break;
		case IXGBE_PHYSICAL_LAYER_1000BASE_KX:
		case IXGBE_PHYSICAL_LAYER_10GBASE_KR:
		case IXGBE_PHYSICAL_LAYER_10GBASE_XAUI:
		case IXGBE_PHYSICAL_LAYER_UNKNOWN:
		default:
			sc->optics = IFM_ETHER | IFM_AUTO;
			break;
	}
d1653 4
@


1.77
log
@Remove setting an initial assumed baudrate upon driver attach which is not
necessarily correct, there might not even be a link when attaching.

ok mikeb@@ reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.76 2012/11/29 13:23:00 mikeb Exp $	*/
d3419 5
a3423 4
	struct ifnet   *ifp = &sc->arpcom.ac_if;
	struct ixgbe_hw *hw = &sc->hw;
	uint32_t  missed_rx = 0, bprc, lxon, lxoff, total;
	int	i;
d3428 1
a3428 1
		int mp;
d3430 1
d3432 1
d3434 2
d3455 2
a3456 1
	sc->stats.mprc -= bprc;
d3505 1
a3505 1
	ifp->if_ierrors = missed_rx + sc->stats.crcerrs + sc->stats.rlec;
@


1.76
log
@make ix complile with IX_DEBUG defined, based on the patch from
mxb at alumni ! chalmers ! se.  thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.75 2012/11/23 04:34:11 brad Exp $	*/
a1600 1
	ifp->if_baudrate = IF_Gbps(10);
@


1.75
log
@- Remove return's at the end of void functions
- Some minor tweaking of the use of braces in two spots

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.74 2012/11/08 09:18:37 mikeb Exp $	*/
d2521 1
a2521 2
	if (!mp) {
		sc->mbuf_packet_failed++;
a2522 1
	}
d3516 1
a3516 2
	printf("%s: mbuf alloc failed %lu, mbuf cluster failed %lu, "
	    "missed pkts %llu, rx len errs %llu, crc errs %llu, "
a3521 2
	    sc->mbuf_alloc_failed,
	    sc->mbuf_cluster_failed,
@


1.74
log
@Do not depend on IFCAP_CSUM flags when configuring rx checksumming
and correct an incorrect usage of IXGBE_RXCSUM_PCSD.  ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.73 2012/11/06 17:29:39 mikeb Exp $	*/
a599 1
	return;
a1304 3


	return;
a1481 1
	return;
a1572 1

a1585 2

	return;
a1648 3


	return;
a1692 1
	return;
a2078 2

	return;
d2092 1
a2092 1
	for (i = 0; i < sc->num_queues; i++, txr++) {
a2093 1
	}
d2725 1
a2725 1
	for (i = 0; i < sc->num_queues; i++, rxr++)
d2728 1
a2857 2

	return;
d2871 1
a2871 1
	for (i = 0; i < sc->num_queues; i++, rxr++) {
a2872 1
	}
a3247 2

	return;
a3262 1
	return;
@


1.73
log
@Remove Flow Director code that is not used, is outdated and tends
to get in the way.  ok krw, brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.72 2012/10/28 12:21:57 brad Exp $	*/
d2829 1
a2866 3

	if (ifp->if_capabilities & IFCAP_CSUM_IPv4)
		rxcsum |= IXGBE_RXCSUM_PCSD;
@


1.72
log
@Add braces to some code in ixgbe_setup_vlan_hw_support().

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.71 2012/08/13 13:14:50 mikeb Exp $	*/
d791 1
a791 10
#ifdef IXGBE_FDIR
	/* Init Flow director */
	if (sc->hw.mac.type != ixgbe_mac_82598EB)
		ixgbe_init_fdir_signature_82599(&sc->hw, fdir_pballoc);
#endif

	/*
	 * Check on any SFP devices that
	 * need to be kick-started
	 */
@


1.71
log
@sync a comment with reality and remove an error path duplicate; from brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.70 2012/08/11 06:53:31 mikeb Exp $	*/
d3210 1
a3210 1
	for (i = 0; i < IXGBE_VFTA_SIZE; i++)
d3214 1
d3229 1
a3229 1
	if (sc->hw.mac.type != ixgbe_mac_82598EB)
d3235 1
a3235 1

@


1.70
log
@Add support for another 82599 SFP+ card.  Original diff and tests by
Chris Maxwell <chris ! maxwell () hootsuite.com>
ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.69 2012/08/10 10:56:22 mikeb Exp $	*/
d1096 1
a1096 5
	/* XXX EFBIG */
	if (error == ENOMEM) {
		sc->no_tx_dma_setup++;
		return (error);
	} else if (error != 0) {
a2951 3
 *
 *  We loop at most count times if count is > 0, or until done if
 *  count < 0.
@


1.69
log
@cleanup ixgbe_start routine; from brad, ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.68 2012/08/08 14:44:13 mikeb Exp $	*/
d75 1
d1418 1
@


1.68
log
@We're not going to loop in the rxeof refilling our ring as it would
work against the mclgeti algorithm and besides it doesn't make any
difference if [repaired and] enabled since interrupt mitigation was
fixed some time ago.  So remove the leftovers altogether so that
nobody would be tempted to use them.

ok claudio, jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.67 2012/08/06 21:07:52 mikeb Exp $	*/
a88 1
void	ixgbe_start_locked(struct tx_ring *, struct ifnet *);
d393 1
a393 1
ixgbe_start_locked(struct tx_ring *txr, struct ifnet * ifp)
d395 2
d398 1
a398 1
	struct ix_softc		*sc = txr->sc;
a402 1

d406 14
a460 28

void
ixgbe_start(struct ifnet *ifp)
{
	struct ix_softc *sc = ifp->if_softc;
	struct tx_ring	*txr = sc->tx_rings;
	uint32_t queue = 0;

#if 0
	/*
	 * This is really just here for testing
	 * TX multiqueue, ultimately what is
	 * needed is the flow support in the stack
	 * and appropriate logic here to deal with
	 * it. -jfv
	 */
	if (sc->num_queues > 1)
		queue = (curcpu % sc->num_queues);
#endif

	txr = &sc->tx_rings[queue];

	if (ifp->if_flags & IFF_RUNNING)
		ixgbe_start_locked(txr, ifp);

	return;
}

d910 1
a910 1
			ixgbe_start_locked(txr, ifp);
d980 1
a980 1
		ixgbe_start_locked(txr, ifp);
@


1.67
log
@Add support for 10Gb ethernet cards based on the Intel X540 chipset.
The code was obtained from FreeBSD and tested on the hardware kindly
donated by Tony Sarendal <tony () polarcap ! org>.  Thanks a lot!

ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.66 2012/07/29 13:49:03 mikeb Exp $	*/
d125 1
a125 1
int	ixgbe_rxeof(struct ix_queue *, int);
a225 1
	sc->rx_process_limit = 100;	// XXX
d914 1
a914 1
		ixgbe_rxeof(que, -1 /* XXX sc->rx_process_limit */);
a953 1
	++que->irqs;
d955 1
a955 1
		ixgbe_rxeof(que, -1);
d2974 1
a2974 1
ixgbe_rxeof(struct ix_queue *que, int count)
d2992 1
a2992 1
	while (count != 0 && rxr->rx_ndescs > 0) {
@


1.66
log
@whitespace cleanup, no binary change
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.65 2012/07/06 11:08:44 mikeb Exp $	*/
d75 2
a76 1
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP_FCOE }
d257 3
d696 1
a696 1
		/* Add for Thermal detection */
d706 11
d965 1
a965 1
		printf("\n%s: CRITICAL: FAN FAILURE!! "
d971 8
d1027 3
d1461 5
d1720 7
a1726 2
			if (err)
				return;
d2086 1
d2097 1
d2105 1
a2105 1
	if (hw->mac.type == ixgbe_mac_82599EB) {
d3271 1
d3387 1
@


1.65
log
@rewrite the receive filter programming to look similar to other
drivers - from brad;  while here initialized a stack variable
before usage as pointed out by david hill.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.64 2012/07/05 14:47:28 mikeb Exp $	*/
d5 1
a5 1
  Copyright (c) 2001-2008, Intel Corporation 
d7 2
a8 2
  
  Redistribution and use in source and binary forms, with or without 
d10 2
a11 2
  
   1. Redistributions of source code must retain the above copyright notice, 
d13 3
a15 3
  
   2. Redistributions in binary form must reproduce the above copyright 
      notice, this list of conditions and the following disclaimer in the 
d17 3
a19 3
  
   3. Neither the name of the Intel Corporation nor the names of its 
      contributors may be used to endorse or promote products derived from 
d21 1
a21 1
  
d23 8
a30 8
  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE 
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE 
  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS 
  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN 
  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) 
d263 1
a263 1
		 * so the timer routine will probe 
d309 2
a310 2
	error = ixgbe_allocate_legacy(sc); 
	if (error) 
d431 3
a433 3
        bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
	    0, txr->txdma.dma_map->dm_mapsize, 
            BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d568 7
a574 7
        /*
         * The timer is set to 5 every time ixgbe_start() queues a packet.
         * Then ixgbe_txeof() keeps resetting to 5 as long as it cleans at
         * least one descriptor.
         * Finally, anytime all descriptors are clean the timer is
         * set to 0.
         */
d576 2
a577 2
        	if (txr->watchdog_timer == 0 || --txr->watchdog_timer)
                	continue;
d858 1
a858 1
	
d872 1
a872 1
void    
d875 1
a875 1
        uint64_t queue = 1ULL << vector;
d894 1
a894 1
        struct ix_queue *que = context;
d950 1
a950 1
                printf("\n%s: CRITICAL: FAN FAILURE!! "
d959 1
a959 1
	        ixgbe_update_link_status(sc);
d1051 2
a1052 2
        cmd_type_len |= IXGBE_ADVTXD_DTYP_DATA;
        cmd_type_len |= IXGBE_ADVTXD_DCMD_IFCS | IXGBE_ADVTXD_DCMD_DEXT;
d1071 6
a1076 6
        /*
         * Important to capture the first descriptor
         * used because it will contain the index of
         * the one we tell the hardware to report back
         */
        first = txr->next_avail_desc;
d1102 1
a1102 1
	 * this becomes the first descriptor of 
d1152 2
a1153 2
        /* Set the index of the descriptor that will be marked done */
        txbuf = &txr->tx_buffers[first];
d1246 1
a1246 1
	
d1451 1
a1451 1
	
d1616 1
a1616 1
	
d1736 1
a1736 1
	    dma->dma_vaddr, size, NULL, 
d1825 1
a1825 1
	 */ 
d1847 1
a1847 1
	 */ 
d1933 1
a1933 1
        /* Create the descriptor buffer dma maps */
d2180 1
a2180 1
	 * In advanced descriptors the vlan tag must 
d2270 1
a2270 1
        return (offload);
d2300 1
a2300 1
	        return FALSE;
d2308 1
a2308 1
	if (eh->evl_encap_proto == htons(ETHERTYPE_VLAN)) 
d2317 2
a2318 2
        /* Ensure we have at least the IP+TCP header in the first mbuf. */
        if (mp->m_len < ehdrlen + sizeof(struct ip) + sizeof(struct tcphdr))
d2423 1
a2423 1
        bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
d2425 1
a2425 1
            BUS_DMASYNC_POSTREAD);
d2551 1
a2551 1
        bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->hmap,
d2576 1
a2576 1
        bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->pmap,
d2590 1
a2590 1
        return (0);
d2607 1
a2607 1
	int             	i, bsize, error;
d2811 1
a2811 1
		              IXGBE_PSRTYPE_UDPHDR |
d3014 1
a3014 1
		/* Currently no HW RSC support of 82599 */ 
d3381 1
a3381 1
        for (i = 0; i < sc->num_queues; i++, que++) {
d3392 1
a3392 1
        ixgbe_set_ivar(sc, 1, sc->linkvec, -1);
d3444 1
a3444 1
        	sc->stats.mpc[i] += mp;
@


1.64
log
@set the prefetch threshold for 99'er as well.  provides some additional
performance improvement.  obtained from freebsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.63 2012/07/05 14:36:22 mikeb Exp $	*/
d126 1
a126 3
void	ixgbe_set_promisc(struct ix_softc *);
void	ixgbe_disable_promisc(struct ix_softc *);
void	ixgbe_set_multi(struct ix_softc *);
d518 3
a520 7
			if ((ifp->if_flags & IFF_RUNNING)) {
				if ((ifp->if_flags ^ sc->if_flags) &
				    (IFF_PROMISC | IFF_ALLMULTI)) {
					ixgbe_disable_promisc(sc);
					ixgbe_set_promisc(sc);
                                }
			} else
d522 1
a522 1
		} else
d525 1
a525 1
		sc->if_flags = ifp->if_flags;
d541 1
a541 1
			ixgbe_set_multi(sc);
a660 3
	/* Setup Multicast table */
	ixgbe_set_multi(sc);

d683 3
d1166 1
a1166 1
ixgbe_set_promisc(struct ix_softc *sc)
a1167 2

	uint32_t       reg_rctl;
d1169 1
a1169 36

	reg_rctl = IXGBE_READ_REG(&sc->hw, IXGBE_FCTRL);

	if (ifp->if_flags & IFF_PROMISC) {
		reg_rctl |= (IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);
		IXGBE_WRITE_REG(&sc->hw, IXGBE_FCTRL, reg_rctl);
	} else if (ifp->if_flags & IFF_ALLMULTI) {
		reg_rctl |= IXGBE_FCTRL_MPE;
		reg_rctl &= ~IXGBE_FCTRL_UPE;
		IXGBE_WRITE_REG(&sc->hw, IXGBE_FCTRL, reg_rctl);
	}
	return;
}

void
ixgbe_disable_promisc(struct ix_softc * sc)
{
	uint32_t       reg_rctl;

	reg_rctl = IXGBE_READ_REG(&sc->hw, IXGBE_FCTRL);
	reg_rctl &= ~(IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);
	IXGBE_WRITE_REG(&sc->hw, IXGBE_FCTRL, reg_rctl);

	return;
}


/*********************************************************************
 *  Multicast Update
 *
 *  This routine is called whenever multicast address list is updated.
 *
 **********************************************************************/
void
ixgbe_set_multi(struct ix_softc *sc)
{
a1175 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1177 1
a1177 1
	IOCTL_DEBUGOUT("ixgbe_set_multi: begin");
d1184 6
a1189 4
	fctrl |= (IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);
	if (ifp->if_flags & IFF_PROMISC)
		fctrl |= (IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);
	else if (ifp->if_flags & IFF_ALLMULTI) {
d1191 9
a1199 5
		fctrl &= ~IXGBE_FCTRL_UPE;
	} else
		fctrl &= ~(IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);
	
	IXGBE_WRITE_REG(&sc->hw, IXGBE_FCTRL, fctrl);
d1201 1
a1201 5
	ETHER_FIRST_MULTI(step, &sc->arpcom, enm);
	while (enm != NULL) {
		if (bcmp(enm->enm_addrlo, enm->enm_addrhi, ETHER_ADDR_LEN)) {
			ifp->if_flags |= IFF_ALLMULTI;
			mcnt = MAX_NUM_MULTICAST_ADDRESSES;
d1203 4
a1206 7
		if (mcnt == MAX_NUM_MULTICAST_ADDRESSES)
			break;
		bcopy(enm->enm_addrlo,
		    &mta[mcnt * IXGBE_ETH_LENGTH_OF_ADDRESS],
		    IXGBE_ETH_LENGTH_OF_ADDRESS);
		mcnt++;
		ETHER_NEXT_MULTI(step, enm);
d1209 1
a1209 5
	update_ptr = mta;
	ixgbe_hw(&sc->hw, update_mc_addr_list,
	    update_ptr, mcnt, ixgbe_mc_array_itr);

	return;
d1215 1
a1215 1
 * addresses in the array of ixgbe_set_multi() one by one.
d1662 1
a1662 1
	int		sfp, negotiate;
@


1.63
log
@enable the code that forces a cleanup if number of tx descriptors
is below the threshold.  noticed by brad.  tweak threshold values
to get a bit of a performance increase.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.62 2012/02/26 16:22:37 mikeb Exp $	*/
d731 6
@


1.62
log
@rather than driving the card with 16k intr/s of low latency interrupts,
rely on regular rx/tx queue interrupts moderated to 8k intr/s achieving
best performance/latency ratio.  this effectively doubles performance
on 82599.  tested on 82598 as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.61 2012/02/26 16:12:34 mikeb Exp $	*/
a1058 1
#if 0
d1067 1
a1067 2
		if (txr->tx_avail <= IXGBE_TX_OP_THRESHOLD) {
			txr->no_desc_avail++;
a1068 1
		}
a1069 1
#endif
a1095 1
		txr->no_desc_avail++;
@


1.61
log
@there's no need to rearm interrupts, we only need to reenable queues;
no objections from claudio and deraadt; tested on 82598 and 82599
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.60 2012/01/20 14:48:49 mikeb Exp $	*/
d637 2
a638 2
	uint32_t	 k, txdctl, rxdctl, rxctrl, mhadd, gpie;
	int		 i, s, err, llimode = 0;
a705 1
		llimode = IXGBE_EITR_LLI_MOD;
d809 8
a816 3
	/* Set moderation on the Link interrupt */
	IXGBE_WRITE_REG(&sc->hw, IXGBE_EITR(sc->linkvec),
	    IXGBE_LINK_ITR | llimode);
d2841 1
a2841 4
		srrctl = IXGBE_READ_REG(&sc->hw, IXGBE_SRRCTL(i));
		srrctl &= ~IXGBE_SRRCTL_BSIZEHDR_MASK;
		srrctl &= ~IXGBE_SRRCTL_BSIZEPKT_MASK;
		srrctl |= bufsz;
@


1.60
log
@save content of the pci attach args, not the pointer; from Christian Ehrhardt
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.59 2012/01/13 09:38:23 mikeb Exp $	*/
d928 1
a928 1
	int		 refill = 0;
d971 3
a973 1
	ixgbe_enable_intr(sc);
@


1.59
log
@Repair hw vlan tagging and stripping on 82599. Previously receiver
would prepend another vlan tag 0 to all frames containing 802.1Q
tags rendering vlans unusable.

ok jsg, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.58 2011/12/12 20:45:30 mikeb Exp $	*/
d212 1
a212 1
	sc->osdep.os_pa = pa;
d1403 1
a1403 1
	struct pci_attach_args	*pa = os->os_pa;
d1537 1
a1537 1
	struct pci_attach_args	*pa = os->os_pa;
d1579 1
a1579 1
	struct pci_attach_args	*pa = os->os_pa;
d1612 1
a1612 1
	struct pci_attach_args	*pa = os->os_pa;
d1752 1
a1752 1
	dma->dma_tag = os->os_pa->pa_dmat;
d3333 1
a3333 1
	pa = ((struct ixgbe_osdep *)hw->back)->os_pa;
d3354 1
a3354 1
	pa = ((struct ixgbe_osdep *)hw->back)->os_pa;
@


1.58
log
@add missing m_freem's into the error code paths.  there's no change
in behavior since we don't do split headers;  ok dlg, kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.57 2011/12/09 11:43:41 mikeb Exp $	*/
d3118 1
a3118 2
				if ((sc->num_vlans) &&
				    (staterr & IXGBE_RXD_STAT_VP)) {
d3145 1
a3145 2
				if ((sc->num_vlans) &&
				    (staterr & IXGBE_RXD_STAT_VP)) {
a3232 9

	/*
	 * We get here thru ixgbe_init, meaning
	 * a soft reset, this has already cleared
	 * the VFTA and other state, so if there
	 * have been no vlan's registered do nothing.
	 */
	if (sc->num_vlans == 0)
		return;
@


1.57
log
@we need to always schedule another rx ring refill callout
in case we fail to do it at the spot.  prevents rx ring
lockups under high load.  ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.56 2011/11/27 16:14:31 mikeb Exp $	*/
d2552 1
a2552 1
	struct mbuf		*mh, *mp;
d2576 2
a2577 1
	if (mh == NULL)
d2579 1
d2589 1
d2609 5
@


1.56
log
@Bring back low latency interrupt moderation for 82599
lost during update.  Originally from claudio, rev1.46.

ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.55 2011/11/27 16:10:23 mikeb Exp $	*/
d964 2
a965 2
		} else if (que->rxr->rx_ndescs < 2)
			timeout_add(&sc->rx_refill, 0);
d2747 1
a2747 1
	} else if (que->rxr->rx_ndescs < 2)
@


1.55
log
@Checksum fix from reyk (rev1.31) that was lost during update:

Fix the IP ckecksum offloading logic that disables and breaks offloading
if the packet is neither TCP nor UDP because of an erroneous "default" case.
No functional change in the default build because IP checksum offloading
is currently disabled in ix(4).

Tested on 82598 and 82599 (though checksums are still disabled by default).

ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.54 2011/06/18 21:19:44 claudio Exp $	*/
d638 1
a638 2
	int		 err;
	int		 i, s;
d697 2
a698 2
	/* Add for Thermal detection */
	if (sc->hw.mac.type == ixgbe_mac_82599EB)
d701 8
d724 1
a724 1
	
d811 2
a812 1
	IXGBE_WRITE_REG(&sc->hw, IXGBE_EITR(sc->linkvec), IXGBE_LINK_ITR);
@


1.54
log
@Use MSI interrupts when available. Tested by deraadt@@ and jsg@@.
Removing a leftover M from my tree makes also my machine happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.53 2011/06/15 00:03:00 dlg Exp $	*/
a2285 3
		break;
	default:
		offload = FALSE;
@


1.53
log
@if the system runs out of mbufs and cannot provide any when filling
the rx ring, schedule a timeout to keep trying until it gets some.

a timeout is used here cos the chip doesnt have a way of reporting
if its out of descriptors or when there's a ring overflow. we have
to manage that ourselves in software.

mikeb reported this issue on an ipsec gateway. ipsec would consume
all the mbufs while they were stuck in crypto waiting for the cpu
to catch up, by which time it was too late to give more to the
hardware. without any rx descriptors the chip would never interrupt
and we'd never try to fill the ring again.

the fix was tested by and is ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.52 2011/06/10 12:46:35 claudio Exp $	*/
d1535 1
a1535 1
	if (/* pci_intr_map_msi(pa, &ih) != 0 && */
@


1.52
log
@Monster update of ix(4). This brings ix(4) close to what is currently
in FreeBSD. This seems to fix a lot of problems on 82599 based cards
including the VLAN problems and the corrupted receives.
Putting this in now to work on it in tree since a few additional things
need to be merged. Tested by myself, deraadt@@ and jsg@@ on both 98er and
99er cards.
OK jsg@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.51 2011/04/15 15:12:27 chl Exp $	*/
d118 1
d219 1
d372 1
d951 7
a957 4
	if (refill && ixgbe_rxfill(que->rxr)) {
		/* Advance the Rx Queue "Tail Pointer" */
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(que->rxr->me),
		    que->rxr->last_desc_filled);
d1375 1
d2728 17
@


1.51
log
@Remove dead assignments and one newly created unused variable.

Found by LLVM/Clang Static Analyzer.

ok henning@@ krw@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.50 2011/04/13 00:14:18 dlg Exp $	*/
d58 1
d70 1
d72 1
d74 5
a78 1
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP_EM }
a100 1
int	ixgbe_hardware_init(struct ix_softc *);
d102 1
d123 2
a124 2
int	ixgbe_rxeof(struct rx_ring *, int);
void	ixgbe_rx_checksum(struct ix_softc *, uint32_t, struct mbuf *);
a133 1
void	ixgbe_enable_hw_vlans(struct ix_softc * sc);
d143 6
d151 4
d174 1
a174 1
 *  sc based on PCI vendor/device id of the sc.
a222 1
	sc->bigbufs = FALSE;
d235 8
d244 16
a259 4
	if (hw->mac.type == ixgbe_mac_82598EB)
		error = ixgbe_init_ops_82598(&sc->hw);
	else
		error = ixgbe_init_ops_82599(&sc->hw);
a269 1
		error = EIO;
a272 1
		error = EIO;
a278 1
		error = EIO;
a281 4
	/* Pick up the smart speed setting */
	if (sc->hw.mac.type == ixgbe_mac_82599EB)
		sc->hw.phy.smart_speed = ixgbe_smart_speed;

d319 3
d337 1
d374 1
d440 1
a440 1
		    txr->next_avail_tx_desc);
d459 2
a460 2
	if (sc->num_tx_queues > 1)
		queue = (curcpu % sc->num_tx_queues);
d578 1
a578 1
	for (i = 0; i < sc->num_tx_queues; i++, txr++) {
d594 1
a594 1
		for (i = 0; i < sc->num_tx_queues; i++, txr++)
d602 1
a602 1
	for (i = 0; i < sc->num_tx_queues; i++, txr++) {
d607 1
a607 1
		    i, txr->tx_avail, txr->next_tx_to_clean);
d634 1
a634 1
	uint32_t	 k, txdctl, rxdctl, mhadd, gpie;
d644 3
a652 6
	/* Do a warm reset */
	sc->hw.mac.ops.reset_hw(&sc->hw);

	if (ifp->if_capabilities & IFCAP_VLAN_HWTAGGING)
		ixgbe_enable_hw_vlans(sc);

d662 1
d668 9
a676 6
	/*
	 * If we are resetting MTU smaller than 2K
	 * drop to small RX buffers
	 */
	if (sc->max_frame_size <= MCLBYTES)
		sc->bigbufs = FALSE;
d680 2
a681 1
		printf("%s: Could not setup receive structures\n", ifp->if_xname);
d692 5
a696 2
	if (sc->hw.mac.type == ixgbe_mac_82599EB) {
		gpie |= IXGBE_SDP1_GPIEN;
a697 6
		/*
		 * Set LL interval to max to reduce the number of low latency
		 * interrupts hitting the card when the ring is getting full.
		 */
		gpie |= 0xf << IXGBE_GPIE_LLI_DELAY_SHIFT;
	}
d699 1
a699 4
	/* Enable Fan Failure Interrupt */
	if (sc->hw.phy.media_type == ixgbe_media_type_copper)
		gpie |= IXGBE_SDP1_GPIEN;
	if (sc->msix) {
d717 1
a717 1
	for (i = 0; i < sc->num_tx_queues; i++) {
d725 1
a725 1
	for (i = 0; i < sc->num_rx_queues; i++) {
d728 7
a734 2
			/* PTHRESH set to 32 */
			rxdctl |= 0x0020;
d745 2
a746 1
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(i), rxr->last_rx_desc_filled);
d749 10
d763 1
a763 1
	if (ixgbe_enable_msix)
d765 8
a772 1
	else  /* Simple settings for Legacy/MSI */
d777 1
d781 6
d791 14
a804 6
	err = sc->hw.phy.ops.identify(&sc->hw);
	if (err == IXGBE_ERR_SFP_NOT_SUPPORTED) {
		printf("Unsupported SFP+ module type was detected.\n");
		splx(s);
		return;
        }
d806 1
d816 86
d912 1
a914 1
	struct rx_ring	*rxr = sc->rx_rings;
d920 2
a921 1
	if (reg_eicr == 0)
d923 1
d925 1
d927 1
a927 1
		ixgbe_rxeof(rxr, -1);
d935 1
a935 1
                printf("%s: \nCRITICAL: FAN FAILURE!! "
d948 1
a948 1
	if (refill && ixgbe_rxfill(rxr)) {
d950 2
a951 2
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(rxr->me),
		    rxr->last_rx_desc_filled);
d957 1
d1024 1
a1024 1
	int		first;
d1026 1
a1026 1
	struct ixgbe_tx_buf *txbuf, *txbuf_mapped;
d1039 1
d1049 1
a1049 1
			txr->no_tx_desc_avail++;
d1053 1
d1060 1
a1060 1
        first = txr->next_avail_tx_desc;
a1061 1
	txbuf_mapped = txbuf;
d1069 1
a1069 1

d1080 1
a1080 1
		txr->no_tx_desc_avail++;
d1107 1
a1107 1
	i = txr->next_avail_tx_desc;
d1116 1
d1122 1
d1128 1
a1128 1
	txr->next_avail_tx_desc = i;
d1131 2
a1132 1
	txbuf_mapped->map = txbuf->map;
d1139 1
d1176 1
a1176 3

	reg_rctl &= (~IXGBE_FCTRL_UPE);
	reg_rctl &= (~IXGBE_FCTRL_MPE);
a1188 2
#define IXGBE_RAR_ENTRIES 16

d1193 1
a1193 1
	uint8_t	mta[MAX_NUM_MULTICAST_ADDRESSES * IXGBE_ETH_LENGTH_OF_ADDRESS];
d1202 4
d1277 5
d1285 1
a1290 1

d1333 1
a1333 1
		for (i = 0; i < sc->num_tx_queues; i++)
d1365 3
d1373 1
d1421 2
a1423 1
	case PCI_PRODUCT_INTEL_82598AT:
d1433 1
d1436 1
d1443 6
d1452 1
d1455 6
d1470 44
a1526 4
	/* Legacy RID at 0 */
	if (sc->msix == 0)
		sc->rid[0] = 0;

d1528 2
a1529 1
	if (pci_intr_map(pa, &ih)) {
d1534 8
d1543 1
a1543 1
	sc->tag[0] = pci_intr_establish(pc, ih, IPL_NET,
d1545 1
a1545 1
	if (sc->tag[0] == NULL) {
d1554 3
d1565 1
a1565 1
	int			 val, i;
a1580 9
	/*
	 * Init the resource arrays
	 */
	for (i = 0; i < IXGBE_MSGS; i++) {
		sc->rid[i] = i + 1; /* MSI/X RID starts at 1 */   
		sc->tag[i] = NULL;
		sc->res[i] = NULL;
	}

d1582 2
a1583 2
	sc->num_tx_queues = 1;
	sc->num_rx_queues = 1;
d1586 1
a1586 1
	/* Now setup MSI or MSI/X */
a1588 1
	sc->hw.back = os;
d1598 10
d1609 3
a1611 3
	if (sc->tag[0])
		pci_intr_disestablish(pa->pa_pc, sc->tag[0]);
	sc->tag[0] = NULL;
a1620 46
 *  Initialize the hardware to a configuration as specified by the
 *  sc structure. The controller is reset, the EEPROM is
 *  verified, the MAC address is set, then the shared initialization
 *  routines are called.
 *
 **********************************************************************/
int
ixgbe_hardware_init(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	uint16_t csum;

	csum = 0;
	/* Issue a global reset */
	sc->hw.adapter_stopped = FALSE;
	ixgbe_hw0(&sc->hw, stop_adapter);

	/* Make sure we have a good EEPROM before we read from it */
	if (ixgbe_ee(&sc->hw, validate_checksum, &csum) < 0) {
		printf("%s: The EEPROM Checksum Is Not Valid\n", ifp->if_xname);
		return (EIO);
	}

	/* Pick up the smart speed setting */
	if (sc->hw.mac.type == ixgbe_mac_82599EB)
		sc->hw.phy.smart_speed = ixgbe_smart_speed;

	/* Get Hardware Flow Control setting */
	sc->hw.fc.requested_mode = ixgbe_fc_full;
	sc->hw.fc.pause_time = IXGBE_FC_PAUSE;
	sc->hw.fc.low_water = IXGBE_FC_LO;
	sc->hw.fc.high_water = IXGBE_FC_HI;
	sc->hw.fc.send_xon = TRUE;

	if (ixgbe_hw0(&sc->hw, init_hw) != 0) {
		printf("%s: Hardware Initialization Failed", ifp->if_xname);
		return (EIO);
	}
	bcopy(sc->hw.mac.addr, sc->arpcom.ac_enaddr,
	      IXGBE_ETH_LENGTH_OF_ADDRESS);

	return (0);
}

/*********************************************************************
 *
d1685 44
d1814 1
d1817 1
a1817 1
	int rsize, tsize, error = IXGBE_SUCCESS;
d1820 9
a1828 1
	/* First allocate the TX ring struct memory */
d1831 1
a1831 1
	    sc->num_tx_queues, M_DEVBUF, M_NOWAIT | M_ZERO))) {
a1832 1
		error = ENOMEM;
d1839 1
a1839 1
	    sc->num_rx_queues, M_DEVBUF, M_NOWAIT | M_ZERO))) {
a1840 1
		error = ENOMEM;
d1853 1
a1853 1
	for (i = 0; i < sc->num_tx_queues; i++, txconf++) {
a1865 1
			error = ENOMEM;
a1869 10

		if (ixgbe_dma_malloc(sc, sizeof(uint32_t),
		    &txr->txwbdma, BUS_DMA_NOWAIT)) {
			printf("%s: Unable to allocate TX Write Back memory\n",
			    ifp->if_xname);
			error = ENOMEM;
			goto err_tx_desc;
		}
		txr->tx_hwb = (uint32_t *)txr->txwbdma.dma_vaddr;
		*txr->tx_hwb = 0;
d1877 1
a1877 1
	for (i = 0; i < sc->num_rx_queues; i++, rxconf++) {
a1889 1
			error = ENOMEM;
d1896 10
a1913 1
		ixgbe_dma_free(sc, &txr->txwbdma);
d1921 1
a1921 1
	return (error);
d1999 2
a2000 2
	txr->next_avail_tx_desc = 0;
	txr->next_tx_to_clean = 0;
d2023 1
a2023 1
	for (i = 0; i < sc->num_tx_queues; i++, txr++) {
d2046 1
a2046 1
	uint64_t	 tdba, txhwb;
d2051 1
a2051 1
	for (i = 0; i < sc->num_tx_queues; i++) {
d2062 8
a2069 7
		/* Setup for Head WriteBack */
		txhwb = txr->txwbdma.dma_map->dm_segs[0].ds_addr;
		txhwb |= IXGBE_TDWBAL_HEAD_WB_ENABLE;
		IXGBE_WRITE_REG(hw, IXGBE_TDWBAL(i),
		    (txhwb & 0x00000000ffffffffULL));
		IXGBE_WRITE_REG(hw, IXGBE_TDWBAH(i),
		    (txhwb >> 32));
a2090 9

		/* Setup the HW Tx Head and Tail descriptor pointers */
		IXGBE_WRITE_REG(hw, IXGBE_TDH(i), 0);
		IXGBE_WRITE_REG(hw, IXGBE_TDT(i), 0);

		/* Setup Transmit Descriptor Cmd Settings */
		txr->txd_cmd = IXGBE_TXD_CMD_IFCS;

		txr->watchdog_timer = 0;
d2122 1
a2122 1
	for (i = 0; i < sc->num_tx_queues; i++, txr++) {
d2164 1
a2164 1
	if (txr->tx_buffers != NULL) {
a2165 2
		txr->tx_buffers = NULL;
	}
d2192 1
a2192 1
	int ctxd = txr->next_avail_tx_desc;
d2198 1
d2212 2
a2213 2
		vlan_macip_lens |=
		    htole16(mp->m_pkthdr.ether_vtag) << IXGBE_ADVTXD_VLAN_SHIFT;
a2240 1
	type_tucmd_mlhl |= IXGBE_ADVTXD_DCMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;
d2242 10
a2251 11
	if (offload == TRUE) {
		switch (etype) {
		case ETHERTYPE_IP:
			ip = (struct ip *)(mp->m_data + ehdrlen);
			ip_hlen = ip->ip_hl << 2;
			if (mp->m_len < ehdrlen + ip_hlen)
				return FALSE; /* failure */
			ipproto = ip->ip_p;
			if (mp->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
				type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV4;
			break;
d2253 9
a2261 9
		case ETHERTYPE_IPV6:
			ip6 = (struct ip6_hdr *)(mp->m_data + ehdrlen);
			ip_hlen = sizeof(struct ip6_hdr);
			if (mp->m_len < ehdrlen + ip_hlen)
				return FALSE; /* failure */
			ipproto = ip6->ip6_nxt;
			if (mp->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
				type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_IPV6;
			break;
d2263 4
a2266 4
		default:
			offload = FALSE;
			break;
		}
d2268 2
a2269 1
		vlan_macip_lens |= ip_hlen;
d2271 12
a2282 10
		switch (ipproto) {
		case IPPROTO_TCP:
			if (mp->m_pkthdr.csum_flags & M_TCP_CSUM_OUT)
				type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_TCP;
			break;
		case IPPROTO_UDP:
			if (mp->m_pkthdr.csum_flags & M_UDP_CSUM_OUT)
				type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_UDP;
			break;
		}
d2292 1
d2297 1
a2297 1
	txr->next_avail_tx_desc = ctxd;
d2351 1
a2351 1
	ctxd = txr->next_avail_tx_desc;
d2400 1
a2400 1
	txr->next_avail_tx_desc = ctxd;
d2425 1
a2425 1
	uint				 first, last, done, num_avail;
d2427 1
a2427 1
	struct ixgbe_legacy_tx_desc *tx_desc;
d2429 2
a2430 1
	if (txr->tx_avail == sc->num_tx_desc)
d2432 1
d2434 2
a2435 3
	num_avail = txr->tx_avail;
	first = txr->next_tx_to_clean;

a2436 1

d2439 4
d2444 8
a2451 5
	/* Get the HWB */
        bus_dmamap_sync(txr->txwbdma.dma_tag, txr->txwbdma.dma_map,
	    0, txr->txwbdma.dma_map->dm_mapsize,
            BUS_DMASYNC_POSTREAD);
        done = *txr->tx_hwb;
d2457 2
a2458 2
	while (TRUE) {
		/* We clean the range til last head write back */
d2463 2
a2464 1
			num_avail++;
a2466 1
				ifp->if_opackets++;
d2476 1
d2485 2
d2488 8
a2495 6
		last = done;
	        bus_dmamap_sync(txr->txwbdma.dma_tag, txr->txwbdma.dma_map,
		    0, txr->txwbdma.dma_map->dm_mapsize,
	            BUS_DMASYNC_POSTREAD);
        	done = *txr->tx_hwb;
		if (last == done)
d2503 1
a2503 1
	txr->next_tx_to_clean = first;
d2511 1
a2511 1
	if (num_avail > IXGBE_TX_CLEANUP_THRESHOLD) {
d2515 1
a2515 1
		if (num_avail == sc->num_tx_desc) {
a2517 1
			txr->tx_avail = num_avail;
d2521 1
a2521 1
		else if (num_avail != txr->tx_avail) {
a2526 2
	txr->tx_avail = num_avail;

d2539 2
a2540 1
	struct mbuf		*m;
a2541 2
	int			size = MCLBYTES;
	struct ixgbe_rx_buf	*rxbuf;
d2547 1
a2547 2

	if (rxbuf->m_head != NULL) {
d2553 12
a2564 3
	m = MCLGETI(NULL, M_DONTWAIT, &sc->arpcom.ac_if, size);
	if (!m) {
		sc->mbuf_cluster_failed++;
d2566 11
d2578 12
a2589 3
	m->m_len = m->m_pkthdr.len = size;
	if (sc->max_frame_size <= (size - ETHER_ALIGN))
		m_adj(m, ETHER_ALIGN);
d2591 2
a2592 2
	error = bus_dmamap_load_mbuf(rxr->rxdma.dma_tag, rxbuf->map,
	    m, BUS_DMA_NOWAIT);
d2594 1
a2594 1
		m_freem(m);
d2598 3
a2600 3
        bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map,
	    0, rxbuf->map->dm_mapsize, BUS_DMASYNC_PREREAD);
	rxbuf->m_head = m;
d2605 1
a2605 2
	bzero(rxdesc, dsize);
	rxdesc->read.pkt_addr = htole64(rxbuf->map->dm_segs[0].ds_addr);
d2629 1
a2629 1
	int             	i, bsize, error, size = MCLBYTES;
a2638 1
	rxr->rxtag = rxr->rxdma.dma_tag;
d2642 9
a2650 2
		error = bus_dmamap_create(rxr->rxdma.dma_tag, size, 1,
		    size, 0, BUS_DMA_NOWAIT, &rxbuf->map);
d2652 1
a2652 1
			printf("%s: Unable to create Rx DMA map\n",
a2655 1
		rxbuf->m_head = NULL;
d2688 1
a2688 1
	rxr->last_rx_desc_filled = sc->num_rx_desc - 1;
d2708 1
a2708 1
	i = rxr->last_rx_desc_filled;
d2716 1
a2716 1
		rxr->last_rx_desc_filled = i;
d2734 1
a2734 1
	for (i = 0; i < sc->num_rx_queues; i++, rxr++)
d2750 2
d2757 2
a2758 2
	uint32_t	rxctrl, fctrl, srrctl, rxcsum;
	uint32_t	reta, mrqc, hlreg, linkvec;
a2759 1
	uint32_t	llimod = 0;
d2773 2
d2777 1
d2785 1
a2785 9
	srrctl = IXGBE_READ_REG(&sc->hw, IXGBE_SRRCTL(0));
	srrctl &= ~IXGBE_SRRCTL_BSIZEHDR_MASK;
	srrctl &= ~IXGBE_SRRCTL_BSIZEPKT_MASK;
	if (sc->bigbufs)
		srrctl |= 4096 >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;
	else
		srrctl |= 2048 >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;
	srrctl |= IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_SRRCTL(0), srrctl);
d2787 2
a2788 9
	/* Set Queue moderation rate */
	if (sc->hw.mac.type == ixgbe_mac_82599EB)
		llimod = IXGBE_EITR_LLI_MOD;
	for (i = 0; i < IXGBE_MSGS; i++)
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EITR(i), DEFAULT_ITR | llimod);

	/* Set Link moderation lower */
	linkvec = sc->num_tx_queues + sc->num_rx_queues;
	IXGBE_WRITE_REG(&sc->hw, IXGBE_EITR(linkvec), LINK_ITR);
a2789 2
	for (i = 0; i < sc->num_rx_queues; i++, rxr++) {
		uint64_t rdba = rxr->rxdma.dma_map->dm_segs[0].ds_addr;
d2797 15
d2814 9
a2822 2
		IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(i),
		    rxr->last_rx_desc_filled);
d2827 4
a2830 1
	if (sc->num_rx_queues > 1) {
a2832 11
		switch (sc->num_rx_queues) {
			case 8:
			case 4:
				reta = 0x00010203;
				break;
			case 2:
				reta = 0x00010001;
				break;
			default:
				reta = 0x00000000;
		}
d2835 6
a2840 7
		for (i = 0; i < 32; i++) {
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RETA(i), reta);
			if (sc->num_rx_queues > 4) {
				++i;
				IXGBE_WRITE_REG(&sc->hw,
				    IXGBE_RETA(i), 0x04050607);
			}
d2845 1
a2845 2
			IXGBE_WRITE_REG_ARRAY(&sc->hw,
			    IXGBE_RSSRK(0), i, random[i]);
d2847 1
a2848 1
		    /* Perform hash on these packet types */
a2871 7
	/* Enable Receive engine */
	rxctrl = IXGBE_READ_REG(&sc->hw, IXGBE_RXCTRL);
	if (sc->hw.mac.type == ixgbe_mac_82598EB)
		rxctrl |= IXGBE_RXCTRL_DMBYPS;
	rxctrl |= IXGBE_RXCTRL_RXEN;
	sc->hw.mac.ops.enable_rx_dma(&sc->hw, rxctrl);

d2886 1
a2886 1
	for (i = 0; i < sc->num_rx_queues; i++, rxr++) {
d2899 2
a2900 2
	struct ix_softc		*sc = NULL;
	struct ixgbe_rx_buf	*rxbuf = NULL;
a2902 1
	INIT_DEBUGOUT("free_receive_buffers: begin");
d2905 2
a2906 2
		rxbuf = rxr->rx_buffers;
		for (i = 0; i < sc->num_rx_desc; i++, rxbuf++) {
d2908 2
a2909 2
				bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map,
				    0, rxbuf->map->dm_mapsize,
d2911 2
a2912 1
				bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->map);
d2916 13
a2928 2
			bus_dmamap_destroy(rxr->rxdma.dma_tag, rxbuf->map);
			rxbuf->map = NULL;
a2929 2
	}
	if (rxr->rx_buffers != NULL) {
a2932 9

	if (rxr->rxtag != NULL)
		rxr->rxtag = NULL;

	if (rxr->fmp != NULL) {
		m_freem(rxr->fmp);
		rxr->fmp = NULL;
		rxr->lmp = NULL;
	}
d2946 1
a2946 1
ixgbe_rxeof(struct rx_ring *rxr, int count)
d2948 2
a2949 1
	struct ix_softc 	*sc = rxr->sc;
d2951 1
a2951 2
	struct mbuf    		*m;
	uint8_t			 accept_frame = 0;
d2953 3
a2955 3
	uint16_t		 len, desc_len, prev_len_adj;
	uint32_t		 staterr;
	struct ixgbe_rx_buf	*rxbuf;
d2958 1
a2958 1
	int			 i;
a2963 1

d2965 2
a2966 1
		m = NULL;
a2968 6
		rxbuf = &rxr->rx_buffers[i];

		bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
		    dsize * i, dsize,
		    BUS_DMASYNC_POSTREAD);

d2977 4
d2982 31
a3012 6
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map, 0,
		    rxbuf->map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->map);
		m = rxbuf->m_head;
		rxbuf->m_head = NULL;
d3014 1
a3014 1
		if (m == NULL) {
d3018 1
a3018 1
			    rxr->last_rx_desc_filled);
d3021 11
a3031 12
		m_cluncount(m, 1);
		rxr->rx_ndescs--;

		accept_frame = 1;
		prev_len_adj = 0;
		desc_len = letoh16(rxdesc->wb.upper.length);

		if (staterr & IXGBE_RXD_STAT_EOP) {
			count--;
			eop = 1;
		} else {
			eop = 0;
d3033 44
a3076 14
		len = desc_len;

		if (staterr & IXGBE_RXDADV_ERR_FRAME_ERR_MASK)
			accept_frame = 0;

		if (accept_frame) {
			m->m_len = len;

			/* XXX ixgbe_realign() STRICT_ALIGN */

			if (rxr->fmp == NULL) {
				m->m_pkthdr.len = m->m_len;
				rxr->fmp = m; /* Store the first mbuf */
				rxr->lmp = m;
d3078 7
a3084 11
				/* Chain mbuf's together */
				m->m_flags &= ~M_PKTHDR;
#if 0
				/*
				 * Adjust length of previous mbuf in chain if
				 * we received less than 4 bytes in the last
				 * descriptor.
				 */
				if (prev_len_adj > 0) {
					rxr->lmp->m_len -= prev_len_adj;
					rxr->fmp->m_pkthdr.len -= prev_len_adj;
a3086 3
				rxr->lmp->m_next = m;
				rxr->lmp = m;
				rxr->fmp->m_pkthdr.len += m->m_len;
d3088 20
a3107 12

			if (eop) {
				ifp->if_ipackets++;

				m = rxr->fmp;
				m->m_pkthdr.rcvif = ifp;

				rxr->packet_count++;
				rxr->byte_count += rxr->fmp->m_pkthdr.len;

				ixgbe_rx_checksum(sc, staterr, m);

d3109 4
a3112 4
				if (staterr & IXGBE_RXD_STAT_VP) {
					m->m_pkthdr.ether_vtag =
					    letoh16(rxdesc->wb.upper.vlan);
					m->m_flags |= M_VLANTAG;
a3114 10
#if NBPFILTER > 0
				if (ifp->if_bpf)
					bpf_mtap_ether(ifp->if_bpf, m,
					    BPF_DIRECTION_IN);
#endif

				ether_input_mbuf(ifp, m);

				rxr->fmp = NULL;
				rxr->lmp = NULL;
d3116 5
a3120 7
		} else {
			sc->dropped_pkts++;

			if (rxr->fmp != NULL) {
				m_freem(rxr->fmp);
				rxr->fmp = NULL;
				rxr->lmp = NULL;
d3122 12
d3135 1
a3135 2
			m_freem(m);
		}
d3137 5
a3141 2
		/* Zero out the receive descriptors status  */
		rxdesc->wb.upper.status_error = 0;
d3143 3
d3170 1
a3170 2
ixgbe_rx_checksum(struct ix_softc *sc,
    uint32_t staterr, struct mbuf * mp)
d3194 1
a3194 1
ixgbe_enable_hw_vlans(struct ix_softc *sc)
d3197 19
a3216 1
	ixgbe_disable_intr(sc);
d3218 7
a3226 2
	ctrl &= ~IXGBE_VLNCTRL_CFIEN;
	ctrl &= ~IXGBE_VLNCTRL_VFE;
d3228 9
a3236 1
	ixgbe_enable_intr(sc);
d3243 3
a3245 1
	uint32_t mask = IXGBE_EIMS_ENABLE_MASK;
d3248 1
a3248 1
	if (hw->phy.media_type == ixgbe_media_type_copper)
d3250 1
a3250 3

	/* 82599 specific interrupts */
	if (sc->hw.mac.type == ixgbe_mac_82599EB) {
d3256 2
d3259 2
a3260 1
	if (sc->msix_mem) {
d3264 1
a3264 2
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EIAC,
		    sc->eims_mask | mask);
d3267 8
a3274 1
	IXGBE_WRITE_REG(hw, IXGBE_EIMS, mask);
d3283 1
a3283 1
	if (sc->msix_mem)
d3300 2
a3301 1
	uint16_t value;
d3303 4
d3308 1
d3310 2
a3311 2
	/* Should we do read/mask/write...?  16 vs 32 bit!!! */
	value = pci_conf_read(pa->pa_pc, pa->pa_tag, reg) & 0xffff;
d3313 1
a3313 1
	return (value);
d3320 2
d3323 5
d3329 6
a3334 3

	/* Should we do read/mask/write...?  16 vs 32 bit!!! */
	pci_conf_write(pa->pa_pc, pa->pa_tag, reg, value);
d3389 2
a3390 2
	struct  tx_ring *txr = sc->tx_rings;
	struct  rx_ring *rxr = sc->rx_rings;
d3393 6
a3398 2
        for (i = 0; i < sc->num_rx_queues; i++, rxr++)
                ixgbe_set_ivar(sc, i, rxr->msix, 0);
d3400 9
a3408 2
        for (i = 0; i < sc->num_tx_queues; i++, txr++)
		ixgbe_set_ivar(sc, i, txr->msix, 1);
d3412 30
@


1.50
log
@modify the interrupt handler so it only processes the rings once, rather
than looping over them until it runs out of work to do.

in my testing i have found that under what i consider high pps
(>160kpps) ix would loop 4 or 5 times in the interrupt handler,
where each loop does a bus_space_read and the mclgeti loop (ie, rx
dequeue followed by rx ring fill).

looping in the isr is bad for several reasons:

firstly, the chip does interrupt mitigation so you have a
decent/predictable amount of work to do in the isr. your first loop
will do that chunk of work (ie, it pulls off 50ish packets), and
then the successive looping aggressively pull one or two packets
off the rx ring. these extra loops work against the benefit that
interrupt mitigation provides.

bus space reads are slow. we should avoid doing them where possible
(but we should always do them when necessary).

doing the loop 5 times per isr works against the mclgeti semantics.
it knows a nic is busy and therefore needs more rx descriptors by
watching to see when the nic uses all of its descriptors between
interrupts. if we're aggressively pulling packets off by looping
in the isr then we're skewing this check.

ok deraadt@@ claudio@@
testing by phessler@@ bluhm@@ and me in production
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.49 2011/04/07 15:30:16 miod Exp $	*/
d866 1
a866 1
	int		first, last = 0;
a956 1
		last = i; /* Next descriptor that will get completed */
a1588 1
	txr = sc->tx_rings;
a1597 1
	rxr = sc->rx_rings;
@


1.49
log
@Do not use NULL in integer comparisons. No functional change.
ok matthew@@ tedu@@, also eyeballed by at least krw@@ oga@@ kettenis@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.48 2011/04/05 18:01:21 henning Exp $	*/
a758 1
	uint32_t	 reg_eicr;
d762 2
a763 24
	int		 claimed = 0, refill = 0;

	for (;;) {
		reg_eicr = IXGBE_READ_REG(&sc->hw, IXGBE_EICR);
		if (reg_eicr == 0)
			break;

		claimed = 1;
		refill = 0;

		if (ifp->if_flags & IFF_RUNNING) {
			ixgbe_rxeof(rxr, -1);
			ixgbe_txeof(txr);
			refill = 1;
		}

		/* Check for fan failure */
		if ((hw->phy.media_type == ixgbe_media_type_copper) &&
		    (reg_eicr & IXGBE_EICR_GPI_SDP1)) {
	                printf("%s: \nCRITICAL: FAN FAILURE!! "
			    "REPLACE IMMEDIATELY!!\n", ifp->if_xname);
			IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMS,
			    IXGBE_EICR_GPI_SDP1);
		}
d765 30
a794 12
		/* Link status change */
		if (reg_eicr & IXGBE_EICR_LSC) {
			timeout_del(&sc->timer);
		        ixgbe_update_link_status(sc);
			timeout_add_sec(&sc->timer, 1);
		}

		if (refill && ixgbe_rxfill(rxr)) {
			/* Advance the Rx Queue "Tail Pointer" */
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(rxr->me),
			    rxr->last_rx_desc_filled);
		}
d800 1
a800 1
	return (claimed);
@


1.48
log
@mechanic rename M_{TCP|UDP}V4_CSUM_OUT -> M_{TCP|UDP}_CSUM_OUT
ok claudio krw
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.47 2011/04/03 15:36:02 jasper Exp $	*/
d1378 1
a1378 1
	if (os->os_membase != NULL)
d1380 1
a1380 1
	os->os_membase = NULL;
@


1.47
log
@use nitems(); no binary change for drivers that are compiled on amd64.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.46 2010/11/10 15:23:25 claudio Exp $	*/
d2049 1
a2049 1
			if (mp->m_pkthdr.csum_flags & M_TCPV4_CSUM_OUT)
d2053 1
a2053 1
			if (mp->m_pkthdr.csum_flags & M_UDPV4_CSUM_OUT)
@


1.46
log
@Enable low latency interrupt moderation and set the LL interval
to the maximum value to reduce the number of low latency interrupts
hitting the card when the ring is getting full.
Tested at least by deraadt@@ on 99 and myself on 99 and 98 ix(4).
OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.45 2010/10/27 20:48:27 deraadt Exp $	*/
d169 1
a169 1
	    sizeof(ixgbe_devices)/sizeof(ixgbe_devices[0])));
@


1.45
log
@fix double ;;
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.44 2010/08/27 08:24:53 deraadt Exp $	*/
d660 5
d2491 1
d2525 2
d2528 1
a2528 1
		IXGBE_WRITE_REG(&sc->hw, IXGBE_EITR(i), DEFAULT_ITR);
@


1.44
log
@These do not need powerhook functions.
ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.43 2010/08/11 11:35:09 jsg Exp $	*/
d3031 1
a3031 1
	struct ifnet   *ifp = &sc->arpcom.ac_if;;
@


1.43
log
@Use the correct offsets when reading/writing to DCA_TXCTRL in the 82599
case.  Adapted from Intel code in FreeBSD and tested on 82598/82599.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.42 2010/04/20 14:54:58 jsg Exp $	*/
a79 1
void	ixgbe_power(int, void *);
a295 2
	sc->powerhook = powerhook_establish(ixgbe_power, sc);

a343 13
}

void
ixgbe_power(int why, void *arg)
{
	struct ix_softc *sc = (struct ix_softc *)arg;
	struct ifnet *ifp;

	if (why == PWR_RESUME) {
		ifp = &sc->arpcom.ac_if;
		if (ifp->if_flags & IFF_UP)
			ixgbe_init(sc);
	}
@


1.42
log
@Don't ask for ipv6 checksum offloading as we aren't ready for it.
Due to the messy context setup code this was breaking ipv6 forwarding
when ipv4 offloading was enabled.  All checksum offloading remains
disabled for now.

Debugged with and ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.41 2010/03/22 17:20:27 jsg Exp $	*/
d1846 11
a1856 1
		txctrl = IXGBE_READ_REG(hw, IXGBE_DCA_TXCTRL(i));
d1858 9
a1866 1
		IXGBE_WRITE_REG(hw, IXGBE_DCA_TXCTRL(i), txctrl);
@


1.41
log
@There is a workaround for a 82599 specific errata that could hang the rx dma
unit, it just wasn't called.
Problem is present in the FreeBSD driver (but not the Linux one).
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.40 2010/03/22 17:09:27 jsg Exp $	*/
d1954 1
d1956 1
d2022 1
d2032 1
@


1.40
log
@Use the correct number of max scatter gather segments for 82599,
adapted from FreeBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.39 2010/03/19 13:08:56 jsg Exp $	*/
d2597 1
a2597 1
	IXGBE_WRITE_REG(&sc->hw, IXGBE_RXCTRL, rxctrl);
@


1.39
log
@Remove support for preproduction 82598 revision A0,
it was removed from the equivalent FreeBSD code over a year ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.38 2010/03/16 22:48:43 kettenis Exp $	*/
d1716 1
d1722 5
d1741 1
a1741 1
			    IXGBE_MAX_SCATTER, PAGE_SIZE, 0,
@


1.38
log
@Set rx_ndescs to zero when initializing the rx ring.  Otherwise we'll
effectively lose receive descriptors each time we reset the interface, until
we run out of descriptors and panic.  Should fix the "em_rxeof: NULL mbuf in
slot 0 (nrx 256, filled 255)" panic on em(4).

ok jsing@@ (for the em(4) bits), jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.37 2010/02/25 10:56:07 jsg Exp $	*/
a140 4
#ifndef NO_82598_A0_SUPPORT
void	desc_flip(void *);
#endif

a2047 5
#ifndef NO_82598_A0_SUPPORT
	if (sc->hw.revision_id == 0)
		desc_flip(TXD);
#endif

a2151 5
#ifndef NO_82598_A0_SUPPORT
	if (sc->hw.revision_id == 0)
		desc_flip(TXD);
#endif

a2327 17
#ifndef NO_82598_A0_SUPPORT
        /* A0 needs to One's Compliment descriptors */
	if (sc->hw.revision_id == 0) {
        	struct dhack {
			uint32_t a1;
			uint32_t a2;
			uint32_t b1;
			uint32_t b2;
		};
        	struct dhack *d;   

        	d = (struct dhack *)rxdesc;
        	d->a1 = ~(d->a1);
        	d->a2 = ~(d->a2);
	}
#endif

a3138 23

#ifndef NO_82598_A0_SUPPORT
/*
 * A0 Workaround: invert descriptor for hardware
 */
void
desc_flip(void *desc)
{
        struct dhack {uint32_t a1; uint32_t a2; uint32_t b1; uint32_t b2;};
        struct dhack *d;

        d = (struct dhack *)desc;
        d->a1 = ~(d->a1);
        d->a2 = ~(d->a2);
        d->b1 = ~(d->b1);
        d->b2 = ~(d->b2);
        d->b2 &= 0xFFFFFFF0;
        d->b1 &= ~IXGBE_ADVTXD_DCMD_RS;
}
#endif



@


1.37
log
@Add support for the 82599 ExpressModule (X520-P2) card.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.36 2010/02/23 18:43:15 jsg Exp $	*/
d2436 1
@


1.36
log
@Add support for 82599 devices based on changes to the FreeBSD driver.

Tested by deraadt on a HotLava card and myself with an Intel X520
and a CX4 82598.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.35 2010/02/19 18:55:12 jsg Exp $	*/
d70 2
a71 1
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82599_SFP }
d1280 1
@


1.35
log
@Partial sync to the latest version of ixgbe in FreeBSD leaving things
like if_ix.c mostly untouched for now.  This brings in support for
newer 82598 parts and adds several things that will be needed for 82599.

Initially from claudio with some additions by me.

Tested by claudio, dlg (earlier version) and myself on different cards
and media types.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.34 2010/01/19 13:48:13 reyk Exp $	*/
d38 1
d64 7
a70 1
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598_DA_DUAL }
d133 1
a133 1
void	ixgbe_set_ivar(struct ix_softc *, uint16_t, uint8_t);
d156 1
a156 2
/* Total number of Interfaces - need for config sanity check */
static int ixgbe_total_ports;
d192 1
d194 1
d225 19
a243 3
	sc->hw.mac.type = ixgbe_mac_82598EB;
	if (ixgbe_init_ops_82598(&sc->hw) != 0) {
		printf(": failed to init the 82598EB\n");
d247 4
a250 3
	/* Initialize the hardware */
	if (ixgbe_hardware_init(sc)) {
		printf(": unable to initialize the hardware\n");
d254 30
d619 3
a621 1
	uint32_t	 txdctl, rxdctl, mhadd, gpie;
d636 2
a637 7
	/* Initialize the hardware */
	if (ixgbe_hardware_init(sc)) {
		printf("%s: Unable to initialize the hardware\n",
		    ifp->if_xname);
		splx(s);
		return;
	}
d675 6
d712 4
a715 2
		/* PTHRESH set to 32 */
		rxdctl |= 0x0020;
d718 8
d730 1
d732 20
a751 1
	ixgbe_configure_ivars(sc);
a888 1
#ifdef notyet
a889 1
#endif
d962 5
a981 12

		/*
		 * we have to do this inside the loop right now
		 * because of the hardware workaround.
		 */
		if (j == (map->dm_nsegs -1)) /* Last descriptor gets EOP and RS */
			txd->read.cmd_type_len |=
			    htole32(IXGBE_TXD_CMD_EOP | IXGBE_TXD_CMD_RS);
#ifndef NO_82598_A0_SUPPORT
		if (sc->hw.revision_id == 0)
			desc_flip(txd);
#endif
d984 2
a1249 9
	ixgbe_total_ports++;
	switch (sc->hw.device_id) {
	case PCI_PRODUCT_INTEL_82598AF_DUAL:
	case PCI_PRODUCT_INTEL_82598EB_CX4_DUAL:
	case PCI_PRODUCT_INTEL_82598AT_DUAL:
		ixgbe_total_ports++;
		break;
	}

d1251 1
d1253 1
d1255 3
d1262 1
d1266 1
d1271 1
d1274 19
d1423 4
d1857 14
d2615 4
a2618 1
	rxctrl |= (IXGBE_RXCTRL_RXEN | IXGBE_RXCTRL_DMBYPS);
d2892 2
a2893 1
	ctrl |= IXGBE_VLNCTRL_VME;
d2909 8
d2937 7
a2943 1
	IXGBE_WRITE_REG(&sc->hw, IXGBE_EIMC, ~0);
d2973 7
d2981 1
a2981 1
ixgbe_set_ivar(struct ix_softc *sc, uint16_t entry, uint8_t vector)
d2983 1
d2987 33
a3019 5
	index = (entry >> 2) & 0x1F;
	ivar = IXGBE_READ_REG(&sc->hw, IXGBE_IVAR(index));
	ivar &= ~(0xFF << (8 * (entry & 0x3)));
	ivar |= (vector << (8 * (entry & 0x3)));
	IXGBE_WRITE_REG(&sc->hw, IXGBE_IVAR(index), ivar);
d3027 1
a3027 1
	int		 i;
d3029 2
a3030 4
        for (i = 0; i < sc->num_rx_queues; i++, rxr++) {
                ixgbe_set_ivar(sc, IXGBE_IVAR_RX_QUEUE(i), rxr->msix);
		sc->eims_mask |= rxr->eims;
	}
d3032 2
a3033 4
        for (i = 0; i < sc->num_tx_queues; i++, txr++) {
		ixgbe_set_ivar(sc, IXGBE_IVAR_TX_QUEUE(i), txr->msix);
		sc->eims_mask |= txr->eims;
	}
d3036 1
a3036 3
        ixgbe_set_ivar(sc, IXGBE_IVAR_OTHER_CAUSES_INDEX,
	    sc->linkvec);
	sc->eims_mask |= IXGBE_IVAR_OTHER_CAUSES_INDEX;
d3059 2
a3060 1
		sc->stats.rnbc[i] += IXGBE_READ_REG(hw, IXGBE_RNBC(i));
@


1.34
log
@IP checksum is still broken with fragments, turn it off for now.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.33 2010/01/11 02:04:25 reyk Exp $	*/
d52 2
d56 1
d61 3
a63 1
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82598AT }
d263 1
a263 1
 *  This routine stops the sc and deallocates all the resources
d1289 2
a1290 1
	pci_intr_disestablish(pa->pa_pc, sc->tag[0]);
d1325 1
a1325 1
	sc->hw.fc.type = ixgbe_fc_full;
a1381 10
	if ((hw->device_id == PCI_PRODUCT_INTEL_82598AT) ||
	    (hw->device_id == PCI_PRODUCT_INTEL_82598AT_DUAL))
		ixgbe_hw(hw, setup_link_speed,
		    IXGBE_LINK_SPEED_10GB_FULL |
		    IXGBE_LINK_SPEED_1GB_FULL, TRUE, TRUE);
	else
		ixgbe_hw(hw, setup_link_speed,
		    IXGBE_LINK_SPEED_10GB_FULL,
		    TRUE, FALSE);

d2825 11
@


1.33
log
@Enable IP checksum offloading in ix(4).

Note: it did not work before because the checksum offloading was
taking care about TCP and UDP but forgot about IP fragments and other
IP protocols.  We need to take care that IP fragments are handled
correctly when we do IP/TCP/UDP offloading.

ok jsg@@ deraadt@@, discussed with others
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.32 2010/01/09 05:44:41 reyk Exp $	*/
d1362 1
a1362 1
	ifp->if_capabilities = IFCAP_VLAN_MTU | IFCAP_CSUM_IPv4;
d1369 2
a1370 1
	ifp->if_capabilities |= IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;
@


1.32
log
@replace an #ifdef IX_CSUM_OFFLOAD with an IFCAP_CSUM_IPv4 capability check,
this allows to enable/disable checksum offloading at one point in the driver
without the need to care about stupid #defines.  no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.31 2010/01/09 05:30:19 reyk Exp $	*/
d1362 1
a1362 1
	ifp->if_capabilities = IFCAP_VLAN_MTU;
d1369 1
a1369 2
	ifp->if_capabilities |= IFCAP_CSUM_IPv4 | IFCAP_CSUM_TCPv4 |
				IFCAP_CSUM_UDPv4;
@


1.31
log
@Fix the IP ckecksum offloading logic that disables and breakes offloading
if the packet is neither TCP nor UDP because of an erroneous "default" case.
No functional change in the default build because IP checksum offloading
is currently disabled in ix(4).

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.30 2009/08/13 14:24:47 jasper Exp $	*/
d2493 3
a2495 3
#if defined(IX_CSUM_OFFLOAD)
	rxcsum |= IXGBE_RXCSUM_PCSD;
#endif
@


1.30
log
@- consistify cfdriver for the ethernet drivers (0 -> NULL)

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.29 2009/08/12 20:02:42 dlg Exp $	*/
a1926 3
			break;
		default:
			offload = FALSE;
@


1.29
log
@revert my change to m_cluncount which tries to prevent the system
running out of mbufs for rx rings.

if the system low watermark is lower than a rx rings low watermark,
we'll never send a packet up the stack, we'll always recycle it.

found by thib@@ on a bge
sadface
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.28 2009/08/12 16:56:59 jsg Exp $	*/
d137 1
a137 1
	0, "ix", DV_IFNET
@


1.28
log
@ix currently relies on a gcc extension that removes a comma
if no arguments are passed to a variadic macro.

Create a seperate non variadic macro for this fixed argument
case. No binary change.

ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.27 2009/08/12 14:39:05 dlg Exp $	*/
d2634 1
a2635 4
		if (m_cluncount(m) == 0)
			accept_frame = 1;
		else
			accept_frame = 0;
d2637 1
@


1.27
log
@if we get dangerously low on clusters during interrupts, we need
to free some for use on the rx rings on network cards.

this modifies m_cluncount to advise callers when we're in such a
situation, and makes them responsible for freeing up the cluster
for allocation by MCLGETI later.

fixes an awesome lockup with sis(4) henning has been experiencing.
this is not the best fix, but it is better than the current situation.

yep deraadt@@ tested by henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.26 2009/08/10 19:41:05 deraadt Exp $	*/
d1128 1
a1128 1
	ixgbe_hw(&sc->hw, reset_hw);
d1130 1
a1130 1
	ixgbe_hw(&sc->hw, stop_adapter);
d1310 1
a1310 1
	ixgbe_hw(&sc->hw, stop_adapter);
d1325 1
a1325 1
	if (ixgbe_hw(&sc->hw, init_hw) != 0) {
@


1.26
log
@A few more simple cases of shutdown hooks which only call xxstop, when
we now know the interface has already been stopped
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.25 2009/08/09 11:40:56 deraadt Exp $	*/
a2633 1
		m_cluncount(m, 1);
d2635 4
a2639 1
		accept_frame = 1;
@


1.25
log
@MCLGETI() will now allocate a mbuf header if it is not provided, thus
reducing the amount of splnet/splx dancing required.. especially in the
worst case (of m_cldrop)
ok dlg kettenis damien
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.24 2009/07/10 12:00:52 dlg Exp $	*/
a67 1
void	ixgbe_shutdown(void *);
a240 1
	sc->shutdownhook = shutdownhook_establish(ixgbe_shutdown, sc);
a302 14
}

/*********************************************************************
 *
 *  Shutdown entry point
 *
 **********************************************************************/

void
ixgbe_shutdown(void *arg)
{
	struct ix_softc *sc = (struct ix_softc *)arg;

	ixgbe_stop(sc);
@


1.24
log
@rework link state handling a bit. this reports missing link correctly. the
old code showed the nic as active all the time, which makes it suck as
part of a trunk.

testing and ok by reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.22 2009/06/28 22:20:20 jsg Exp $	*/
d2220 2
a2221 8
	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL) {
		sc->mbuf_alloc_failed++;
		return (ENOBUFS);
	}
	MCLGETI(m, M_DONTWAIT, &sc->arpcom.ac_if, size);
	if ((m->m_flags & M_EXT) == 0) {
		m_freem(m);
@


1.23
log
@Bring back rev 1.17, enable hardware vlan tagging/stripping.

ok deraadt@@
@
text
@d758 3
d764 2
a765 2
	ifmr->ifm_status = IFM_AVALID;
	ifmr->ifm_active = IFM_ETHER;
d767 8
a774 14
	if (!sc->link_active) {
		ifmr->ifm_status |= IFM_NONE;
		return;
	}

	ifmr->ifm_status |= IFM_ACTIVE;

	switch (sc->link_speed) {
	case IXGBE_LINK_SPEED_1GB_FULL:
		ifmr->ifm_active |= IFM_1000_T | IFM_FDX;
		break;
	case IXGBE_LINK_SPEED_10GB_FULL:
		ifmr->ifm_active |= sc->optics | IFM_FDX;
		break;
d789 1
a789 19
	struct ix_softc *sc = ifp->if_softc;
	struct ifmedia *ifm = &sc->media;

	INIT_DEBUGOUT("ixgbe_media_change: begin");

	if (IFM_TYPE(ifm->ifm_media) != IFM_ETHER)
		return (EINVAL);

        switch (IFM_SUBTYPE(ifm->ifm_media)) {
        case IFM_AUTO:
                sc->hw.mac.autoneg = TRUE;
                sc->hw.phy.autoneg_advertised =
		    IXGBE_LINK_SPEED_1GB_FULL | IXGBE_LINK_SPEED_10GB_FULL;
                break;
        default:
                printf("%s: Only auto media type\n", ifp->if_xname);
		return (EINVAL);
        }

d1084 1
d1089 6
a1094 13
	switch (sc->link_speed) {
	case IXGBE_LINK_SPEED_UNKNOWN:
		ifp->if_baudrate = 0;
		break;
	case IXGBE_LINK_SPEED_100_FULL:
		ifp->if_baudrate = IF_Mbps(100);
		break;
	case IXGBE_LINK_SPEED_1GB_FULL:
		ifp->if_baudrate = IF_Gbps(1);
		break;
	case IXGBE_LINK_SPEED_10GB_FULL:
		ifp->if_baudrate = IF_Gbps(10);
		break;
d1097 3
a1099 8
	if (link_up){ 
		if (sc->link_active == FALSE) {
			sc->link_active = TRUE;
			ifp->if_link_state = LINK_STATE_FULL_DUPLEX;
			if_link_state_change(ifp);
		}
	} else { /* Link down */
		if (sc->link_active == TRUE) {
d1101 10
a1110 7
			ifp->if_link_state = LINK_STATE_DOWN;
			if_link_state_change(ifp);
			sc->link_active = FALSE;
			for (i = 0; i < sc->num_tx_queues;
			    i++, txr++)
				txr->watchdog_timer = FALSE;
			ifp->if_timer = 0;
d1112 5
d1118 1
@


1.22
log
@Now the tx dma mapping problem it was exposing is fixed
bring back rev 1.16, em style MCLGETI based on a diff from reyk with
critical fixes by me.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.21 2009/06/28 22:05:36 jsg Exp $	*/
d120 2
a121 4
#ifdef IX_CSUM_OFFLOAD
int ixgbe_tx_ctx_setup(struct tx_ring *, struct mbuf *);
int ixgbe_tso_setup(struct tx_ring *, struct mbuf *, uint32_t *);
#endif
d833 1
a833 1
#ifdef IX_CSUM_OFFLOAD
a890 1
#ifdef IX_CSUM_OFFLOAD
d896 1
d903 3
a905 1
	} else if (ixgbe_tx_ctx_setup(txr, m_head))
a906 1
#endif
d1403 1
a1403 1
#ifdef IX_VLAN_HWTAGGING
a1860 1
#ifdef IX_CSUM_OFFLOAD
d1877 1
a1879 1
	uint8_t ipproto = 0;
d1929 1
d1931 2
a1932 1
	switch (etype) {
d1954 1
a1954 1
	}
d1956 1
a1956 2
	vlan_macip_lens |= ip_hlen;
	type_tucmd_mlhl |= IXGBE_ADVTXD_DCMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;
d1958 13
a1970 12
	switch (ipproto) {
	case IPPROTO_TCP:
		if (mp->m_pkthdr.csum_flags & M_TCPV4_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_TCP;
		break;
	case IPPROTO_UDP:
		if (mp->m_pkthdr.csum_flags & M_UDPV4_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_UDP;
		break;
	default:
		offload = FALSE;
		break;
d2101 1
a2101 1
#else	/* For 6.2 RELEASE */
a2108 1
#endif
d2826 1
@


1.21
log
@Properly swap tx dma maps so we don't use invalid maps,
leak memory, lose maps or cause double frees.
Problem courtesy of our good friends at Intel in the original
FreeBSD driver.

"awesome" dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.20 2009/06/25 17:01:32 deraadt Exp $	*/
d88 1
a88 1
void	ixgbe_setup_transmit_ring(struct tx_ring *);
d99 1
d114 1
a114 1
int	ixgbe_get_buf(struct rx_ring *, int, struct mbuf *);
d702 1
a702 1
	int		 claimed = 0;
d710 1
d715 1
d733 6
d875 1
a875 1
	error = bus_dmamap_load_mbuf(txr->txtag, map,
d944 1
a944 1
	bus_dmamap_sync(txr->txtag, map, 0, map->dm_mapsize,
d954 1
a954 1
	bus_dmamap_unload(txr->txtag, txbuf->map);
d1162 3
a1167 3
	/* Tell the stack that the interface is no longer active */
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);

d1175 3
d1399 2
d1460 2
a1461 2
	r = bus_dmamap_create(dma->dma_tag, size, 1, size, 0,
	    BUS_DMA_NOWAIT, &dma->dma_map);
d1468 2
a1469 2
	r = bus_dmamem_alloc(dma->dma_tag, size, PAGE_SIZE, 0,
	    &dma->dma_seg, 1, &dma->dma_nseg, BUS_DMA_NOWAIT);
d1475 1
d1483 1
d1492 1
d1521 1
a1597 9

        	/* Now allocate transmit buffers for the ring */
        	if (ixgbe_allocate_transmit_buffers(txr)) {
			printf("%s: Critical Failure setting up transmit buffers\n",
			    ifp->if_xname);
			error = ENOMEM;
			goto err_tx_desc;
        	}

a1622 8

        	/* Allocate receive buffers for the ring*/
		if (ixgbe_allocate_receive_buffers(rxr)) {
			printf("%s: Critical Failure setting up receive buffers\n",
			    ifp->if_xname);
			error = ENOMEM;
			goto err_rx_desc;
		}
d1636 1
d1639 1
d1654 9
a1662 5
	struct ix_softc *sc = txr->sc;
	struct ixgbe_osdep *os = &sc->osdep;
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct ixgbe_tx_buf *txbuf;
	int error, i;
a1663 1
	txr->txtag = os->os_pa->pa_dmat;
d1667 2
a1668 1
		printf("%s: Unable to allocate tx_buffer memory\n", ifp->if_xname);
d1672 1
d1675 3
a1677 3
	txbuf = txr->tx_buffers;
	for (i = 0; i < sc->num_tx_desc; i++, txbuf++) {
		error = bus_dmamap_create(txr->txtag, IXGBE_TSO_SIZE,
d1680 1
d1682 2
a1683 1
			printf("%s: Unable to create TX DMA map\n", ifp->if_xname);
a1689 2
	/* We free all, it handles case where we are in the middle */
	ixgbe_free_transmit_structures(sc);
d1698 1
a1698 1
void
d1701 6
a1706 3
	struct ix_softc *sc = txr->sc;
	struct ixgbe_tx_buf *txbuf;
	int i;
d1711 1
a1715 13
	/* Free any existing tx buffers. */
        txbuf = txr->tx_buffers;
	for (i = 0; i < sc->num_tx_desc; i++, txbuf++) {
		if (txbuf->m_head != NULL) {
			bus_dmamap_sync(txr->txtag, txbuf->map,
			    0, txbuf->map->dm_mapsize,
			    BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(txr->txtag, txbuf->map);
			m_freem(txbuf->m_head);
		}
		txbuf->m_head = NULL;
        }

d1723 1
d1735 1
a1735 1
	int	i;
d1737 4
a1740 2
	for (i = 0; i < sc->num_tx_queues; i++, txr++)
		ixgbe_setup_transmit_ring(txr);
d1743 3
a1813 2
		ixgbe_dma_free(sc, &txr->txdma);
		ixgbe_dma_free(sc, &txr->txwbdma);
a1814 1
	free(sc->tx_rings, M_DEVBUF);
d1836 2
a1837 2
		if (tx_buffer->m_head != NULL) {
			bus_dmamap_sync(txr->txtag, tx_buffer->map,
d1840 1
a1840 1
			bus_dmamap_unload(txr->txtag,
d1842 2
d1845 4
a1848 8
			if (tx_buffer->map != NULL) {
				bus_dmamap_destroy(txr->txtag,
				    tx_buffer->map);
			}
		} else if (tx_buffer->map != NULL) {
			bus_dmamap_unload(txr->txtag,
			    tx_buffer->map);
			bus_dmamap_destroy(txr->txtag,
d1850 1
a1851 2
		tx_buffer->m_head = NULL;
		tx_buffer->map = NULL;
d1854 1
a1854 1
	if (txr->tx_buffers != NULL)
d1856 2
d2158 1
a2158 1
				bus_dmamap_sync(txr->txtag,
d2162 1
a2162 1
				bus_dmamap_unload(txr->txtag,
d2225 1
a2225 1
ixgbe_get_buf(struct rx_ring *rxr, int i, struct mbuf *nmp)
d2227 4
a2230 5
	struct ix_softc	*sc = rxr->sc;
	struct mbuf	*mp = nmp;
	bus_dmamap_t	map;
	int		error, old, s = 0;
	int		size = MCLBYTES;
d2232 5
d2238 5
a2242 6
#ifdef notyet
	/* Are we going to Jumbo clusters? */
	if (sc->bigbufs) {
		size = MJUMPAGESIZE;
		s = 1;
	};
d2244 2
a2245 2
	mp = m_getjcl(M_DONTWAIT, MT_DATA, M_PKTHDR, size);
	if (mp == NULL) {
d2249 5
a2253 19
#endif

	if (mp == NULL) {
		MGETHDR(mp, M_DONTWAIT, MT_DATA);
		if (mp == NULL) {
			sc->mbuf_alloc_failed++;
			return (ENOBUFS);
		}
		MCLGET(mp, M_DONTWAIT);
		if ((mp->m_flags & M_EXT) == 0) {
			m_freem(mp);
			sc->mbuf_cluster_failed++;
			return (ENOBUFS);
		}
		mp->m_len = mp->m_pkthdr.len = size;
	} else {
		mp->m_len = mp->m_pkthdr.len = size;
		mp->m_data = mp->m_ext.ext_buf;
		mp->m_next = NULL;
d2255 3
d2259 2
a2260 9
	if (sc->max_frame_size <= (MCLBYTES - ETHER_ALIGN))
		m_adj(mp, ETHER_ALIGN);

	/*
	 * Using memory from the mbuf cluster pool, invoke the bus_dma
	 * machinery to arrange the memory mapping.
	 */
	error = bus_dmamap_load_mbuf(rxr->rxtag[s], rxr->spare_map[s],
	    mp, BUS_DMA_NOWAIT);
d2262 1
a2262 1
		m_freem(mp);
d2266 3
a2268 11
	/* Now check our target buffer for existing mapping */
	rxbuf = &rxr->rx_buffers[i];
	old = rxbuf->bigbuf;
	if (rxbuf->m_head != NULL)
		bus_dmamap_unload(rxr->rxtag[old], rxbuf->map[old]);

        map = rxbuf->map[old];
        rxbuf->map[s] = rxr->spare_map[s];
        rxr->spare_map[old] = map;
        rxbuf->m_head = mp;
        rxbuf->bigbuf = s;
d2270 2
a2271 2
        rxr->rx_base[i].read.pkt_addr =
	    htole64(rxbuf->map[s]->dm_segs[0].ds_addr);
d2273 2
a2274 2
        bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
	    0, rxbuf->map[s]->dm_mapsize, BUS_DMASYNC_PREREAD);
d2279 6
a2284 1
        	struct dhack {uint32_t a1; uint32_t a2; uint32_t b1; uint32_t b2;};
d2287 1
a2287 1
        	d = (struct dhack *)&rxr->rx_base[i];
d2293 5
a2313 1
	struct ixgbe_osdep	*os = &sc->osdep;
d2315 1
a2315 1
	int             	i, bsize, error;
d2318 1
a2318 2
	if (!(rxr->rx_buffers =
	    (struct ixgbe_rx_buf *) malloc(bsize,
d2320 2
a2321 1
		printf("%s: Unable to allocate rx_buffer memory\n", ifp->if_xname);
d2325 1
a2325 17
	rxr->rxtag[0] = rxr->rxtag[1] = os->os_pa->pa_dmat;

	/* Create the spare maps (used by getbuf) */
        error = bus_dmamap_create(rxr->rxtag[0],
	    MCLBYTES, 1, MCLBYTES, 0, BUS_DMA_NOWAIT, &rxr->spare_map[0]);
	if (error) {
		printf("%s: %s: bus_dmamap_create failed: %d\n", ifp->if_xname,
		    __func__, error);
		goto fail;
	}
        error = bus_dmamap_create(rxr->rxtag[1],
	    MJUMPAGESIZE, 1, MJUMPAGESIZE, 0, BUS_DMA_NOWAIT, &rxr->spare_map[1]);
	if (error) {
		printf("%s: %s: bus_dmamap_create failed: %d\n", ifp->if_xname,
		    __func__, error);
		goto fail;
	}
d2327 1
d2329 2
a2330 3
		rxbuf = &rxr->rx_buffers[i];
		error = bus_dmamap_create(rxr->rxtag[0], MCLBYTES, 1, MCLBYTES,
		    0, BUS_DMA_NOWAIT, &rxbuf->map[0]);
d2332 2
a2333 7
			printf("%s: Unable to create Small RX DMA map\n", ifp->if_xname);
			goto fail;
		}
		error = bus_dmamap_create(rxr->rxtag[1], MJUMPAGESIZE, 1, MJUMPAGESIZE,
		    0, BUS_DMA_NOWAIT, &rxbuf->map[1]);
		if (error) {
			printf("%s: Unable to create Large RX DMA map\n", ifp->if_xname);
d2336 1
d2338 3
a2344 2
	/* Frees all, but can handle partial completion */
	ixgbe_free_receive_structures(sc);
d2357 1
a2357 2
	struct ixgbe_rx_buf	*rxbuf;
	int			j, rsize, s = 0, i;
d2364 2
a2365 27
	/*
	** Free current RX buffers: the size buffer
	** that is loaded is indicated by the buffer
	** bigbuf value.
	*/
	for (i = 0; i < sc->num_rx_desc; i++) {
		rxbuf = &rxr->rx_buffers[i];
		s = rxbuf->bigbuf;
		if (rxbuf->m_head != NULL) {
			bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
			    0, rxbuf->map[s]->dm_mapsize,
			    BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
			m_freem(rxbuf->m_head);
			rxbuf->m_head = NULL;
		}
	}

	for (j = 0; j < sc->num_rx_desc; j++) {
		if (ixgbe_get_buf(rxr, j, NULL) == ENOBUFS) {
			rxr->rx_buffers[j].m_head = NULL;
			rxr->rx_base[j].read.pkt_addr = 0;
			/* If we fail some may have change size */
			s = sc->bigbufs;
			goto fail;
		}
	}
d2369 1
a2369 1
	rxr->last_cleaned = 0;
d2371 6
a2376 3
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
	    0, rxr->rxdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d2379 19
a2397 16
fail:
	/*
	 * We need to clean up any buffers allocated so far
	 * 'j' is the failing index, decrement it to get the
	 * last success.
	 */
	for (--j; j < 0; j--) {
		rxbuf = &rxr->rx_buffers[j];
		if (rxbuf->m_head != NULL) {
			bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
			    0, rxbuf->map[s]->dm_mapsize,
			    BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
			m_freem(rxbuf->m_head);
			rxbuf->m_head = NULL;
		}
d2399 2
a2400 1
	return (ENOBUFS);
d2412 1
a2412 1
	int i, j, s;
d2419 1
d2421 1
a2421 22
	/*
	 * Free RX buffers allocated so far, we will only handle
	 * the rings that completed, the failing case will have
	 * cleaned up for itself. The value of 'i' will be the
	 * failed ring so we must pre-decrement it.
	 */
	rxr = sc->rx_rings;
	for (--i; i > 0; i--, rxr++) {
		for (j = 0; j < sc->num_rx_desc; j++) {
			struct ixgbe_rx_buf *rxbuf;
			rxbuf = &rxr->rx_buffers[j];
			s = rxbuf->bigbuf;
			if (rxbuf->m_head != NULL) {
				bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
				    0, rxbuf->map[s]->dm_mapsize, BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
				m_freem(rxbuf->m_head);
				rxbuf->m_head = NULL;
			}
		}
	}

d2490 1
a2490 1
		    sc->num_rx_desc - 1);
a2569 2
		/* Free the ring memory as well */
		ixgbe_dma_free(sc, &rxr->rxdma);
a2570 2

	free(sc->rx_rings, M_DEVBUF);
d2583 1
a2583 1
	int			 i, s;
d2588 2
a2589 7
		rxbuf = &rxr->rx_buffers[0];
		for (i = 0; i < sc->num_rx_desc; i++) {
			int s = rxbuf->bigbuf;
			if (rxbuf->map != NULL) {
				bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
				bus_dmamap_destroy(rxr->rxtag[s], rxbuf->map[s]);
			}
d2591 4
d2596 1
d2598 2
a2599 2
			rxbuf->m_head = NULL;
			++rxbuf;
d2606 8
a2613 3
	for (s = 0; s < 2; s++) {
		if (rxr->rxtag[s] != NULL)
			rxr->rxtag[s] = NULL;
a2614 1
	return;
d2632 4
a2635 3
	struct mbuf    		*mp;
	int             	 len, i, eop = 0;
	uint8_t		 accept_frame = 0;
d2637 7
a2643 1
	union ixgbe_adv_rx_desc	*cur;
a2645 1
	cur = &rxr->rx_base[i];
d2647 9
a2655 2
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map, 0,
	    rxr->rxdma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
d2657 7
a2663 3
	staterr = cur->wb.upper.status_error;
	if (!(staterr & IXGBE_RXD_STAT_DD))
		return FALSE;
d2665 3
a2667 9
	while ((staterr & IXGBE_RXD_STAT_DD) && (count != 0) &&
	    (ifp->if_flags & IFF_RUNNING)) {
		struct mbuf *m = NULL;
		int s;

		mp = rxr->rx_buffers[i].m_head;
		s = rxr->rx_buffers[i].bigbuf;
		bus_dmamap_sync(rxr->rxtag[s], rxr->rx_buffers[i].map[s],
		    0, rxr->rx_buffers[i].map[s]->dm_mapsize,
d2669 13
a2681 2
		bus_dmamap_unload(rxr->rxtag[s],
		    rxr->rx_buffers[i].map[s]);
d2684 3
d2693 1
a2693 1
		len = cur->wb.upper.length;
d2699 1
a2699 5
			/* Get a fresh buffer */
			if (ixgbe_get_buf(rxr, i, NULL) != 0) {
				ifp->if_iqdrops++;
				goto discard;
			}
d2701 1
a2701 2
			/* Assign correct length to the current fragment */
			mp->m_len = len;
d2704 3
a2706 3
				mp->m_pkthdr.len = len;
				rxr->fmp = mp; /* Store the first mbuf */
				rxr->lmp = mp;
d2709 15
a2723 4
				mp->m_flags &= ~M_PKTHDR;
				rxr->lmp->m_next = mp;
				rxr->lmp = rxr->lmp->m_next;
				rxr->fmp->m_pkthdr.len += len;
a2726 1
				rxr->fmp->m_pkthdr.rcvif = ifp;
d2728 4
d2734 2
a2735 1
				ixgbe_rx_checksum(sc, staterr, rxr->fmp);
d2739 3
a2741 3
					rxr->fmp->m_pkthdr.ether_vtag =
					    letoh16(cur->wb.upper.vlan);
					rxr->fmp->m_flags |= M_VLANTAG;
d2744 7
a2751 1
				m = rxr->fmp;
d2756 2
a2757 2
discard:
			ixgbe_get_buf(rxr, i, mp);
d2763 2
a2764 1
			m = NULL;
d2768 2
a2769 1
		cur->wb.upper.status_error = 0;
d2771 2
a2772 4
		    0, rxr->rxdma.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		rxr->last_cleaned = i; /* for updating tail */
d2774 1
a2776 15

		/* Now send up to the stack */
                if (m != NULL) {
                        rxr->next_to_check = i;
#if NBPFILTER > 0
			if (ifp->if_bpf)
				bpf_mtap_ether(ifp->if_bpf, m,
				    BPF_DIRECTION_IN);
#endif
			ether_input_mbuf(ifp, m);
			i = rxr->next_to_check;
                }
		/* Get next descriptor */
		cur = &rxr->rx_base[i];
		staterr = cur->wb.upper.status_error;
a2778 3

	/* Advance the IXGB's Receive Queue "Tail Pointer" */
	IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(rxr->me), rxr->last_cleaned);
@


1.20
log
@Back out if_ix.c 1.16, 1.17 and 1.18 (and corresponding changes to to
if_ix.h) since it corrupts packets or the checksum flags or something
upwards and breaks nfs.  The 1.16 MCLGETI change does not cause this
but has a double free on reboot, and the 1.18 fixes that double free
but introduces the packet corruption.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.19 2009/06/24 13:36:56 deraadt Exp $	*/
d933 1
@


1.19
log
@like I did for em(4) before, doubled error messages are silly
from brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.18 2009/06/04 22:27:31 jsg Exp $	*/
d88 1
a88 1
int	ixgbe_setup_transmit_ring(struct tx_ring *);
a98 1
int	ixgbe_rxfill(struct rx_ring *);
d113 1
a113 1
int	ixgbe_get_buf(struct rx_ring *, int);
d119 4
a122 2
int	ixgbe_tx_ctx_setup(struct tx_ring *, struct mbuf *);
int	ixgbe_tso_setup(struct tx_ring *, struct mbuf *, uint32_t *);
a133 4
struct rwlock ix_tx_pool_lk = RWLOCK_INITIALIZER("ixplinit");
struct pool *ix_tx_pool = NULL;
void	ix_alloc_pkts(void *, void *);

a577 1
	int		 txpl = 1;
a580 15
	if (rw_enter(&ix_tx_pool_lk, RW_WRITE | RW_INTR) != 0)
		return;
	if (ix_tx_pool == NULL) {
		ix_tx_pool = malloc(sizeof(*ix_tx_pool), M_DEVBUF, M_WAITOK);
		if (ix_tx_pool != NULL) {
			pool_init(ix_tx_pool, sizeof(struct ix_pkt), 0, 0, 0,
			    "ixpkts", &pool_allocator_nointr);
		} else
			txpl = 0;
	}
	rw_exit(&ix_tx_pool_lk);

	if (!txpl)
		return;

d701 1
a701 1
	int		 claimed = 0, refill = 0;
a708 1
		refill = 0;
a712 1
			refill = 1;
a729 6

		if (refill && ixgbe_rxfill(rxr)) {
			/* Advance the Rx Queue "Tail Pointer" */
			IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(rxr->me),
			    rxr->last_rx_desc_filled);
		}
a819 1
	struct ix_pkt	*pkt;
d824 1
d826 1
a826 1
#ifdef notyet
a829 14
	mtx_enter(&txr->tx_pkt_mtx);
	pkt = TAILQ_FIRST(&txr->tx_free_pkts);
	if (pkt == NULL) {
		if (txr->tx_pkt_count <= sc->num_tx_desc &&
		    !ISSET(sc->ix_flags, IX_ALLOC_PKTS_FLAG) &&
		    workq_add_task(NULL, 0, ix_alloc_pkts, sc, NULL) == 0)
		        SET(sc->ix_flags, IX_ALLOC_PKTS_FLAG);

		mtx_leave(&txr->tx_pkt_mtx);
		return (ENOMEM);
	}
	TAILQ_REMOVE(&txr->tx_free_pkts, pkt, pkt_entry);
	mtx_leave(&txr->tx_pkt_mtx);

a838 1
#if 0
a851 1
#endif
d859 3
a861 1
	map = pkt->pkt_dmamap;
d866 1
a866 1
	error = bus_dmamap_load_mbuf(txr->txdma.dma_tag, map,
d868 2
a869 1
	if (error != 0) {
d871 4
a874 1
		goto maperr;
d884 1
a889 1
#ifdef notyet
d896 2
a897 1
	} else
a898 2
	if (ixgbe_tx_ctx_setup(txr, m_head))
		olinfo_status |= IXGBE_TXD_POPTS_IXSM << 8;
d902 1
d914 2
d932 3
a934 8
	pkt->pkt_mbuf = m_head;
	pkt->pkt_start_desc = first;

	mtx_enter(&txr->tx_pkt_mtx);
	TAILQ_INSERT_TAIL(&txr->tx_used_pkts, pkt, pkt_entry);
	mtx_leave(&txr->tx_pkt_mtx);

	bus_dmamap_sync(txr->txdma.dma_tag, map, 0, map->dm_mapsize,
a936 1
#if 0
a938 1
#endif
d944 3
a946 7
	bus_dmamap_unload(txr->txdma.dma_tag, map);
maperr:
	mtx_enter(&txr->tx_pkt_mtx);
	TAILQ_INSERT_TAIL(&txr->tx_free_pkts, pkt, pkt_entry);
	mtx_leave(&txr->tx_pkt_mtx);
	
	return (ENOMEM);
d1152 3
a1157 3
	INIT_DEBUGOUT("ixgbe_stop: begin\n");
	ixgbe_disable_intr(sc);

a1164 3

	ixgbe_free_transmit_structures(sc);
	ixgbe_free_receive_structures(sc);
a1385 2
	
	m_clsetwms(ifp, MCLBYTES, 4, sc->num_rx_desc);
d1389 1
a1389 1
#if NVLAN > 0
d1445 2
a1446 2
	r = bus_dmamap_create(dma->dma_tag, size, 1,
	    size, 0, BUS_DMA_NOWAIT, &dma->dma_map);
d1453 2
a1454 2
	r = bus_dmamem_alloc(dma->dma_tag, size, PAGE_SIZE, 0, &dma->dma_seg,
	    1, &dma->dma_nseg, BUS_DMA_NOWAIT);
a1459 1

a1466 1

a1474 1

a1502 1
		dma->dma_map = NULL;
d1579 9
d1613 8
a1633 1
	sc->rx_rings = NULL;
a1635 1
	sc->tx_rings = NULL;
d1650 14
a1663 15
	struct ix_softc 	*sc;
	struct ixgbe_osdep	*os;
	struct ifnet		*ifp;

	sc = txr->sc;
	os = &sc->osdep;
	ifp = &sc->arpcom.ac_if;

	txr->txtag = txr->txdma.dma_tag;

	/* Create lists to hold TX mbufs */
	TAILQ_INIT(&txr->tx_free_pkts);
	TAILQ_INIT(&txr->tx_used_pkts);
	txr->tx_pkt_count = 0;
	mtx_init(&txr->tx_pkt_mtx, IPL_NET);
d1665 11
a1675 2
	/* Force an allocate of some dmamaps for tx up front */
	ix_alloc_pkts(sc, NULL);
d1678 4
d1689 1
a1689 1
int
d1692 3
a1694 6
	struct ix_softc		*sc = txr->sc;
	int			 error;

	/* Now allocate transmit buffers for the ring */
	if ((error = ixgbe_allocate_transmit_buffers(txr)) != 0)
		return (error);
a1698 1

d1703 13
a1722 1
	return (0);
d1734 1
a1734 1
	int		i, error;
d1736 2
a1737 4
	for (i = 0; i < sc->num_tx_queues; i++, txr++) {
		if ((error = ixgbe_setup_transmit_ring(txr)) != 0)
			goto fail;
	}
a1739 3
fail:
	ixgbe_free_transmit_structures(sc);
	return (error);
d1808 2
d1811 1
d1822 4
a1825 1
	struct ix_pkt		*pkt;
d1828 2
a1829 10
	mtx_enter(&txr->tx_pkt_mtx);
	while ((pkt = TAILQ_FIRST(&txr->tx_used_pkts)) != NULL) {
		TAILQ_REMOVE(&txr->tx_used_pkts, pkt, pkt_entry);
		mtx_leave(&txr->tx_pkt_mtx);
	
		bus_dmamap_sync(txr->txdma.dma_tag, pkt->pkt_dmamap,
		    0, pkt->pkt_dmamap->dm_mapsize,
		    BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(txr->txdma.dma_tag, pkt->pkt_dmamap);
		m_freem(pkt->pkt_mbuf);
d1831 21
a1851 2
		mtx_enter(&txr->tx_pkt_mtx);
		TAILQ_INSERT_TAIL(&txr->tx_free_pkts, pkt, pkt_entry);
d1854 4
a1857 12
	/* Destroy all the dmamaps we allocated for TX */
	while ((pkt = TAILQ_FIRST(&txr->tx_free_pkts)) != NULL) {
		TAILQ_REMOVE(&txr->tx_free_pkts, pkt, pkt_entry);
		txr->tx_pkt_count--;
		mtx_leave(&txr->tx_pkt_mtx);

		bus_dmamap_destroy(txr->txdma.dma_tag, pkt->pkt_dmamap);
		pool_put(ix_tx_pool, pkt);

		mtx_enter(&txr->tx_pkt_mtx);
	}
	mtx_leave(&txr->tx_pkt_mtx);
d1860 1
d1873 1
a1876 1
	uint8_t ipproto = 0;
d1879 1
d1891 1
a1928 1
	type_tucmd_mlhl |= IXGBE_ADVTXD_DCMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;
d1930 1
a1930 2
	if (offload == TRUE) {
		switch (etype) {
d1952 1
a1952 1
		}
d1954 2
a1955 1
		vlan_macip_lens |= ip_hlen;
d1957 12
a1968 13
		switch (ipproto) {
		case IPPROTO_TCP:
			if (mp->m_pkthdr.csum_flags & M_TCPV4_CSUM_OUT)
				type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_TCP;
			break;
		case IPPROTO_UDP:
			if (mp->m_pkthdr.csum_flags & M_UDPV4_CSUM_OUT)
				type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_UDP;
			break;
		default:
			offload = FALSE;
			break;
		}
d1982 2
d2099 1
a2099 1
#else
d2107 1
d2122 2
a2123 3
	struct ixgbe_legacy_tx_desc	*tx_desc;
	struct ix_pkt			*pkt;
	bus_dmamap_t			 map;
d2131 2
d2154 5
a2158 11
			mtx_enter(&txr->tx_pkt_mtx);
			pkt = TAILQ_FIRST(&txr->tx_used_pkts);
			if (pkt != NULL) {
				TAILQ_REMOVE(&txr->tx_used_pkts, pkt, pkt_entry);
				mtx_leave(&txr->tx_pkt_mtx);
				/*
				 * Free the associated mbuf.
				 */
				map = pkt->pkt_dmamap;
				bus_dmamap_sync(txr->txdma.dma_tag, map,
				    0, map->dm_mapsize,
d2160 4
a2163 6
				bus_dmamap_unload(txr->txdma.dma_tag, map);
				m_freem(pkt->pkt_mbuf);
				ifp->if_opackets++;

				mtx_enter(&txr->tx_pkt_mtx);
				TAILQ_INSERT_TAIL(&txr->tx_free_pkts, pkt, pkt_entry);
a2164 1
			mtx_leave(&txr->tx_pkt_mtx);
d2169 1
d2223 1
a2223 1
ixgbe_get_buf(struct rx_ring *rxr, int i)
d2225 5
a2229 4
	struct ix_softc		*sc = rxr->sc;
	struct mbuf		*m;
	int			error;
	int			size = MCLBYTES;
a2230 2
	union ixgbe_adv_rx_desc	*rxdesc;
	size_t			 dsize = sizeof(union ixgbe_adv_rx_desc);
d2232 6
a2237 2
	rxbuf = &rxr->rx_buffers[i];
	rxdesc = &rxr->rx_base[i];
d2239 3
a2241 3
	if (rxbuf->m_head != NULL) {
		printf("%s: ixgbe_get_buf: slot %d already has an mbuf\n",
		    sc->dev.dv_xname, i);
d2244 1
d2246 17
a2262 4
	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL) {
		sc->mbuf_alloc_failed++;
		return (ENOBUFS);
a2263 9
	MCLGETI(m, M_DONTWAIT, &sc->arpcom.ac_if, size);
	if ((m->m_flags & M_EXT) == 0) {
		m_freem(m);
		sc->mbuf_cluster_failed++;
		return (ENOBUFS);
	}
	m->m_len = m->m_pkthdr.len = size;
	if (sc->max_frame_size <= (size - ETHER_ALIGN))
		m_adj(m, ETHER_ALIGN);
d2265 9
a2273 2
	error = bus_dmamap_load_mbuf(rxr->rxdma.dma_tag, rxbuf->map,
	    m, BUS_DMA_NOWAIT);
d2275 1
a2275 1
		m_freem(m);
d2279 11
a2289 3
        bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map,
	    0, rxbuf->map->dm_mapsize, BUS_DMASYNC_PREREAD);
	rxbuf->m_head = m;
d2291 2
a2292 2
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
	    dsize * i, dsize, BUS_DMASYNC_POSTWRITE);
d2294 2
a2295 2
	bzero(rxdesc, dsize);
	rxdesc->read.pkt_addr = htole64(rxbuf->map->dm_segs[0].ds_addr);
d2300 1
a2300 6
        	struct dhack {
			uint32_t a1;
			uint32_t a2;
			uint32_t b1;
			uint32_t b2;
		};
d2303 1
a2303 1
        	d = (struct dhack *)rxdesc;
a2308 5
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
	    dsize * i, dsize, BUS_DMASYNC_PREWRITE);

	rxr->rx_ndescs++;

a2311 45
void
ix_alloc_pkts(void *xsc, void *arg)
{
	struct ix_softc *sc = (struct ix_softc *)xsc;
	struct tx_ring	*txr = &sc->tx_rings[0];	/* XXX */
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct ix_pkt	*pkt;
	int		 i, s;

	for (i = 0; i < 4; i++) { /* magic! */
		pkt = pool_get(ix_tx_pool, PR_WAITOK);
		if (pkt == NULL)
			break;

		if (bus_dmamap_create(txr->txdma.dma_tag, IXGBE_TSO_SIZE,
		    IXGBE_MAX_SCATTER, PAGE_SIZE, 0,
		    BUS_DMA_WAITOK | BUS_DMA_ALLOCNOW, &pkt->pkt_dmamap) != 0)
			goto put;

		if (!ISSET(ifp->if_flags, IFF_UP))
			goto stopping;

		mtx_enter(&txr->tx_pkt_mtx);
		TAILQ_INSERT_TAIL(&txr->tx_free_pkts, pkt, pkt_entry);
		txr->tx_pkt_count++;
		mtx_leave(&txr->tx_pkt_mtx);
	}

	mtx_enter(&txr->tx_pkt_mtx);
	CLR(sc->ix_flags, IX_ALLOC_PKTS_FLAG);
	mtx_leave(&txr->tx_pkt_mtx);

	s = splnet();
	if (!IFQ_IS_EMPTY(&ifp->if_snd))
		ixgbe_start(ifp);
	splx(s);

	return;

stopping:
	bus_dmamap_destroy(txr->txdma.dma_tag, pkt->pkt_dmamap);
put:
	pool_put(ix_tx_pool, pkt);
}

d2325 1
d2327 1
a2327 1
	int             	i, bsize, error, size = MCLBYTES;
d2330 2
a2331 1
	if (!(rxr->rx_buffers = (struct ixgbe_rx_buf *) malloc(bsize,
d2333 1
a2333 2
		printf("%s: Unable to allocate rx_buffer memory\n",
		    ifp->if_xname);
d2337 17
a2353 1
	rxr->rxtag = rxr->rxdma.dma_tag;
a2354 1
	rxbuf = rxr->rx_buffers;
d2356 3
a2358 2
		error = bus_dmamap_create(rxr->rxdma.dma_tag, size, 1,
		    size, 0, BUS_DMA_NOWAIT, &rxbuf->map);
d2360 7
a2366 2
			printf("%s: Unable to create Rx DMA map\n",
			    ifp->if_xname);
a2368 1
		rxbuf->m_head = NULL;
a2369 3
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map, 0,
	    rxr->rxdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d2374 2
d2388 2
a2389 1
	int			 rsize, error;
d2396 27
a2422 2
	if ((error = ixgbe_allocate_receive_buffers(rxr)) != 0)
		return (error);
d2426 1
a2426 1
	rxr->last_rx_desc_filled = sc->num_rx_desc - 1;
d2428 3
a2430 6
	ixgbe_rxfill(rxr);
	if (rxr->rx_ndescs < 1) {
		printf("%s: unable to fill any rx descriptors\n",
		    sc->dev.dv_xname);
		return (ENOBUFS);
	}
d2433 16
a2448 19
}

int
ixgbe_rxfill(struct rx_ring *rxr)
{
	struct ix_softc *sc = rxr->sc;
	int		 post = 0;
	int		 i;

	i = rxr->last_rx_desc_filled;
	while (rxr->rx_ndescs < sc->num_rx_desc) {
		if (++i == sc->num_rx_desc)
			i = 0;

		if (ixgbe_get_buf(rxr, i) != 0)
			break;

		rxr->last_rx_desc_filled = i;
		post = 1;
d2450 1
a2450 2

	return (post);
d2462 1
a2462 1
	int i;
d2469 22
a2491 2
fail:
	ixgbe_free_receive_structures(sc);
d2560 1
a2560 1
		    rxr->last_rx_desc_filled);
d2640 2
d2643 2
d2657 1
a2657 1
	int			 i;
d2662 7
a2668 2
		rxbuf = rxr->rx_buffers;
		for (i = 0; i < sc->num_rx_desc; i++, rxbuf++) {
a2669 4
				bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map,
				    0, rxbuf->map->dm_mapsize,
				    BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->map);
a2670 1
				rxbuf->m_head = NULL;
d2672 2
a2673 2
			bus_dmamap_destroy(rxr->rxdma.dma_tag, rxbuf->map);
			rxbuf->map = NULL;
d2680 3
a2682 8

	if (rxr->rxtag != NULL)
		rxr->rxtag = NULL;

	if (rxr->fmp != NULL) {
		m_freem(rxr->fmp);
		rxr->fmp = NULL;
		rxr->lmp = NULL;
d2684 1
d2702 3
a2704 4
	struct mbuf    		*m;
	uint8_t			 accept_frame = 0;
	uint8_t		    	 eop = 0;
	uint16_t		 len, desc_len, prev_len_adj;
d2706 1
a2706 7
	struct ixgbe_rx_buf	*rxbuf;
	union ixgbe_adv_rx_desc	*rxdesc;
	size_t			 dsize = sizeof(union ixgbe_adv_rx_desc);
	int			 i;

	if (!ISSET(ifp->if_flags, IFF_RUNNING))
		return FALSE;
d2709 1
d2711 2
a2712 2
	while (count != 0 && rxr->rx_ndescs > 0) {
		m = NULL;
d2714 3
a2716 2
		rxdesc = &rxr->rx_base[i];
		rxbuf = &rxr->rx_buffers[i];
d2718 9
a2726 2
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
		    dsize * i, dsize,
d2728 2
a2729 26

		staterr = letoh32(rxdesc->wb.upper.status_error);
		if (!ISSET(staterr, IXGBE_RXD_STAT_DD)) {
			bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
			    dsize * i, dsize,
			    BUS_DMASYNC_PREREAD);
			break;
		}

		/* pull the mbuf off the ring */
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxbuf->map, 0,
		    rxbuf->map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(rxr->rxdma.dma_tag, rxbuf->map);
		m = rxbuf->m_head;
		rxbuf->m_head = NULL;

		if (m == NULL) {
			panic("%s: ixgbe_rxeof: NULL mbuf in slot %d "
			    "(nrx %d, filled %d)", sc->dev.dv_xname,
			    i, rxr->rx_ndescs,
			    rxr->last_rx_desc_filled);
		}

		m_cluncount(m, 1);
		rxr->rx_ndescs--;
a2731 3
		prev_len_adj = 0;
		desc_len = letoh16(rxdesc->wb.upper.length);

d2738 1
a2738 1
		len = desc_len;
d2744 5
a2748 1
			m->m_len = len;
d2750 2
a2751 1
			/* XXX ixgbe_realign() STRICT_ALIGN */
d2754 3
a2756 3
				m->m_pkthdr.len = m->m_len;
				rxr->fmp = m; /* Store the first mbuf */
				rxr->lmp = m;
d2759 4
a2762 15
				m->m_flags &= ~M_PKTHDR;
#if 0
				/*
				 * Adjust length of previous mbuf in chain if
				 * we received less than 4 bytes in the last
				 * descriptor.
				 */
				if (prev_len_adj > 0) {
					rxr->lmp->m_len -= prev_len_adj;
					rxr->fmp->m_pkthdr.len -= prev_len_adj;
				}
#endif
				rxr->lmp->m_next = m;
				rxr->lmp = m;
				rxr->fmp->m_pkthdr.len += m->m_len;
d2766 1
a2767 4

				m = rxr->fmp;
				m->m_pkthdr.rcvif = ifp;

d2770 1
a2770 2

				ixgbe_rx_checksum(sc, staterr, m);
d2774 3
a2776 3
					m->m_pkthdr.ether_vtag =
					    letoh16(rxdesc->wb.upper.vlan);
					m->m_flags |= M_VLANTAG;
a2778 7
#if NBPFILTER > 0
				if (ifp->if_bpf)
					bpf_mtap_ether(ifp->if_bpf, m,
					    BPF_DIRECTION_IN);
#endif

				ether_input_mbuf(ifp, m);
d2780 1
d2785 2
a2786 2
			sc->dropped_pkts++;

d2792 1
a2792 2

			m_freem(m);
d2796 4
a2799 1
		rxdesc->wb.upper.status_error = 0;
d2801 1
a2801 3
		bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
		    dsize * i, dsize,
		    BUS_DMASYNC_PREREAD);
a2802 1
		/* Advance our pointers to the next descriptor. */
d2805 15
d2823 3
a2872 1
	ctrl &= ~IXGBE_VLNCTRL_VFE;
@


1.18
log
@switch to a bnx style dynamic pool backed tx so we don't
have to allocate everything up front.

Requested by reyk@@, 'I'm fine with it' dlg@@, 'commit it' deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.17 2009/04/29 13:18:58 jsg Exp $	*/
d209 1
a209 2
	if (ixgbe_allocate_pci_resources(sc)) {
		printf(": allocation of PCI resources failed\n");
a210 1
	}
@


1.17
log
@Enable hardware vlan tagging/stripping and disable the
vlan filter so it will work properly.
ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.16 2009/04/24 12:54:15 jsg Exp $	*/
d133 4
d583 1
d587 15
d849 1
a853 1
	struct ixgbe_tx_buf *txbuf, *txbuf_mapped;
d859 14
d882 1
d896 1
d904 1
a904 3
	txbuf = &txr->tx_buffers[first];
	txbuf_mapped = txbuf;
	map = txbuf->map;
d911 1
a911 2

	if (error == ENOMEM) {
d913 1
a913 4
		return (error);
	} else if (error != 0) {
		sc->no_tx_dma_setup++;
		return (error);
a941 1
		txbuf = &txr->tx_buffers[i];
a952 2
		txbuf->m_head = NULL;

d969 7
a975 2
	txbuf->m_head = m_head;
	txbuf->map = map;
d979 1
d982 1
d988 7
a994 3
	bus_dmamap_unload(txr->txdma.dma_tag, txbuf->map);
	return (error);

a1694 2
	struct ixgbe_tx_buf	*txbuf;
	int			 error, i;
a1699 8
	if (!(txr->tx_buffers =
	    (struct ixgbe_tx_buf *) malloc(sizeof(struct ixgbe_tx_buf) *
	    sc->num_tx_desc, M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate tx_buffer memory\n",
		    ifp->if_xname);
		error = ENOMEM;
		goto fail;
	}
d1702 5
a1706 6
        /* Create the descriptor buffer dma maps */
	for (i = 0; i < sc->num_tx_desc; i++) {
		txbuf = &txr->tx_buffers[i];
		error = bus_dmamap_create(txr->txdma.dma_tag, IXGBE_TSO_SIZE,
			    IXGBE_MAX_SCATTER, PAGE_SIZE, 0,
			    BUS_DMA_NOWAIT, &txbuf->map);
d1708 2
a1709 6
		if (error != 0) {
			printf("%s: Unable to create TX DMA map\n",
			    ifp->if_xname);
			goto fail;
		}
	}
a1711 2
fail:
	return (error);
d1846 1
a1846 4
	struct ix_softc *sc = txr->sc;
	struct ixgbe_tx_buf *tx_buffer;
	int             i;

d1849 10
a1858 2
	if (txr->tx_buffers == NULL)
		return;
d1860 2
a1861 18
	tx_buffer = txr->tx_buffers;
	for (i = 0; i < sc->num_tx_desc; i++, tx_buffer++) {
		if (tx_buffer->map != NULL && tx_buffer->map->dm_nsegs > 0) {
			bus_dmamap_sync(txr->txdma.dma_tag, tx_buffer->map,
			    0, tx_buffer->map->dm_mapsize,
			    BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(txr->txdma.dma_tag,
			    tx_buffer->map);
		}
		if (tx_buffer->m_head != NULL) {
			m_freem(tx_buffer->m_head);
			tx_buffer->m_head = NULL;
		}
		if (tx_buffer->map != NULL) {
			bus_dmamap_destroy(txr->txdma.dma_tag,
			    tx_buffer->map);
			tx_buffer->map = NULL;
		}
d1864 10
a1873 3
	if (txr->tx_buffers != NULL) {
		free(txr->tx_buffers, M_DEVBUF);
		txr->tx_buffers = NULL;
d1875 1
a1875 2
	txr->tx_buffers = NULL;
	txr->txtag = NULL;
a1889 1
	struct ixgbe_tx_buf        *tx_buffer;
a1906 1
	tx_buffer = &txr->tx_buffers[ctxd];
a1998 2
	tx_buffer->m_head = NULL;

d2136 3
a2138 2
	struct ixgbe_tx_buf		*tx_buffer;
	struct ixgbe_legacy_tx_desc *tx_desc;
a2145 2
	tx_buffer = &txr->tx_buffers[first];

d2167 14
a2180 1
			if (tx_buffer->m_head) {
d2182 3
a2184 8
				bus_dmamap_sync(txr->txdma.dma_tag,
				    tx_buffer->map,
				    0, tx_buffer->map->dm_mapsize,
				    BUS_DMASYNC_POSTWRITE);
				bus_dmamap_unload(txr->txdma.dma_tag,
				    tx_buffer->map);
				m_freem(tx_buffer->m_head);
				tx_buffer->m_head = NULL;
d2186 1
a2190 1
			tx_buffer = &txr->tx_buffers[first];
d2318 45
@


1.16
log
@Switch ix over to em flavoured MCLGETI.
Initial diff from reyk with a bunch of critical fixes from me.
ok reyk@@, 'put it in when you're confident with it' dlg@@ on an earlier rev.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.15 2008/11/28 02:44:18 brad Exp $	*/
d120 2
a121 4
#ifdef IX_CSUM_OFFLOAD
int ixgbe_tx_ctx_setup(struct tx_ring *, struct mbuf *);
int ixgbe_tso_setup(struct tx_ring *, struct mbuf *, uint32_t *);
#endif
d835 1
a835 1
#ifdef IX_CSUM_OFFLOAD
a892 1
#ifdef IX_CSUM_OFFLOAD
d898 1
d905 3
a907 1
	} else if (ixgbe_tx_ctx_setup(txr, m_head))
a908 1
#endif
d1404 1
a1404 1
#ifdef IX_VLAN_HWTAGGING
a1861 1
#ifdef IX_CSUM_OFFLOAD
d1878 1
a1880 1
	uint8_t ipproto = 0;
d1930 1
d1932 2
a1933 1
	switch (etype) {
d1955 1
a1955 1
	}
d1957 1
a1957 2
	vlan_macip_lens |= ip_hlen;
	type_tucmd_mlhl |= IXGBE_ADVTXD_DCMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;
d1959 13
a1971 12
	switch (ipproto) {
	case IPPROTO_TCP:
		if (mp->m_pkthdr.csum_flags & M_TCPV4_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_TCP;
		break;
	case IPPROTO_UDP:
		if (mp->m_pkthdr.csum_flags & M_UDPV4_CSUM_OUT)
			type_tucmd_mlhl |= IXGBE_ADVTXD_TUCMD_L4T_UDP;
		break;
	default:
		offload = FALSE;
		break;
d2102 1
a2102 1
#else	/* For 6.2 RELEASE */
a2109 1
#endif
d2827 1
@


1.15
log
@Eliminate the redundant bits of code for MTU and multicast handling
from the individual drivers now that ether_ioctl() handles this.

Shrinks the i386 kernels by..
RAMDISK - 2176 bytes
RAMDISKB - 1504 bytes
RAMDISKC - 736 bytes

Tested by naddy@@/okan@@/sthen@@/brad@@/todd@@/jmc@@ and lots of users.
Build tested on almost all archs by todd@@/brad@@

ok naddy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.14 2008/11/09 15:08:26 naddy Exp $	*/
d88 1
a88 1
void	ixgbe_setup_transmit_ring(struct tx_ring *);
d99 1
d114 1
a114 1
int	ixgbe_get_buf(struct rx_ring *, int, struct mbuf *);
d704 1
a704 1
	int		 claimed = 0;
d712 1
d717 1
d735 6
d877 1
a877 1
	error = bus_dmamap_load_mbuf(txr->txtag, map,
d945 1
a945 1
	bus_dmamap_sync(txr->txtag, map, 0, map->dm_mapsize,
d955 1
a955 1
	bus_dmamap_unload(txr->txtag, txbuf->map);
d1163 3
a1168 3
	/* Tell the stack that the interface is no longer active */
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);

d1176 3
d1400 2
d1461 2
a1462 2
	r = bus_dmamap_create(dma->dma_tag, size, 1, size, 0,
	    BUS_DMA_NOWAIT, &dma->dma_map);
d1469 2
a1470 2
	r = bus_dmamem_alloc(dma->dma_tag, size, PAGE_SIZE, 0,
	    &dma->dma_seg, 1, &dma->dma_nseg, BUS_DMA_NOWAIT);
d1476 1
d1484 1
d1493 1
d1522 1
a1598 9

        	/* Now allocate transmit buffers for the ring */
        	if (ixgbe_allocate_transmit_buffers(txr)) {
			printf("%s: Critical Failure setting up transmit buffers\n",
			    ifp->if_xname);
			error = ENOMEM;
			goto err_tx_desc;
        	}

a1623 8

        	/* Allocate receive buffers for the ring*/
		if (ixgbe_allocate_receive_buffers(rxr)) {
			printf("%s: Critical Failure setting up receive buffers\n",
			    ifp->if_xname);
			error = ENOMEM;
			goto err_rx_desc;
		}
d1637 1
d1640 1
d1655 9
a1663 5
	struct ix_softc *sc = txr->sc;
	struct ixgbe_osdep *os = &sc->osdep;
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct ixgbe_tx_buf *txbuf;
	int error, i;
a1664 1
	txr->txtag = os->os_pa->pa_dmat;
d1668 2
a1669 1
		printf("%s: Unable to allocate tx_buffer memory\n", ifp->if_xname);
d1673 1
d1676 3
a1678 3
	txbuf = txr->tx_buffers;
	for (i = 0; i < sc->num_tx_desc; i++, txbuf++) {
		error = bus_dmamap_create(txr->txtag, IXGBE_TSO_SIZE,
d1681 1
d1683 2
a1684 1
			printf("%s: Unable to create TX DMA map\n", ifp->if_xname);
a1690 2
	/* We free all, it handles case where we are in the middle */
	ixgbe_free_transmit_structures(sc);
d1699 1
a1699 1
void
d1702 6
a1707 3
	struct ix_softc *sc = txr->sc;
	struct ixgbe_tx_buf *txbuf;
	int i;
d1712 1
a1716 13
	/* Free any existing tx buffers. */
        txbuf = txr->tx_buffers;
	for (i = 0; i < sc->num_tx_desc; i++, txbuf++) {
		if (txbuf->m_head != NULL) {
			bus_dmamap_sync(txr->txtag, txbuf->map,
			    0, txbuf->map->dm_mapsize,
			    BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(txr->txtag, txbuf->map);
			m_freem(txbuf->m_head);
		}
		txbuf->m_head = NULL;
        }

d1724 1
d1736 1
a1736 1
	int	i;
d1738 4
a1741 2
	for (i = 0; i < sc->num_tx_queues; i++, txr++)
		ixgbe_setup_transmit_ring(txr);
d1744 3
a1814 2
		ixgbe_dma_free(sc, &txr->txdma);
		ixgbe_dma_free(sc, &txr->txwbdma);
a1815 1
	free(sc->tx_rings, M_DEVBUF);
d1837 2
a1838 2
		if (tx_buffer->m_head != NULL) {
			bus_dmamap_sync(txr->txtag, tx_buffer->map,
d1841 1
a1841 1
			bus_dmamap_unload(txr->txtag,
d1843 2
d1846 4
a1849 8
			if (tx_buffer->map != NULL) {
				bus_dmamap_destroy(txr->txtag,
				    tx_buffer->map);
			}
		} else if (tx_buffer->map != NULL) {
			bus_dmamap_unload(txr->txtag,
			    tx_buffer->map);
			bus_dmamap_destroy(txr->txtag,
d1851 1
a1852 2
		tx_buffer->m_head = NULL;
		tx_buffer->map = NULL;
d1855 1
a1855 1
	if (txr->tx_buffers != NULL)
d1857 2
d2159 1
a2159 1
				bus_dmamap_sync(txr->txtag,
d2163 1
a2163 1
				bus_dmamap_unload(txr->txtag,
d2226 1
a2226 1
ixgbe_get_buf(struct rx_ring *rxr, int i, struct mbuf *nmp)
d2228 4
a2231 5
	struct ix_softc	*sc = rxr->sc;
	struct mbuf	*mp = nmp;
	bus_dmamap_t	map;
	int		error, old, s = 0;
	int		size = MCLBYTES;
d2233 5
d2239 5
a2243 6
#ifdef notyet
	/* Are we going to Jumbo clusters? */
	if (sc->bigbufs) {
		size = MJUMPAGESIZE;
		s = 1;
	};
d2245 2
a2246 2
	mp = m_getjcl(M_DONTWAIT, MT_DATA, M_PKTHDR, size);
	if (mp == NULL) {
d2250 5
a2254 19
#endif

	if (mp == NULL) {
		MGETHDR(mp, M_DONTWAIT, MT_DATA);
		if (mp == NULL) {
			sc->mbuf_alloc_failed++;
			return (ENOBUFS);
		}
		MCLGET(mp, M_DONTWAIT);
		if ((mp->m_flags & M_EXT) == 0) {
			m_freem(mp);
			sc->mbuf_cluster_failed++;
			return (ENOBUFS);
		}
		mp->m_len = mp->m_pkthdr.len = size;
	} else {
		mp->m_len = mp->m_pkthdr.len = size;
		mp->m_data = mp->m_ext.ext_buf;
		mp->m_next = NULL;
d2256 3
d2260 2
a2261 9
	if (sc->max_frame_size <= (MCLBYTES - ETHER_ALIGN))
		m_adj(mp, ETHER_ALIGN);

	/*
	 * Using memory from the mbuf cluster pool, invoke the bus_dma
	 * machinery to arrange the memory mapping.
	 */
	error = bus_dmamap_load_mbuf(rxr->rxtag[s], rxr->spare_map[s],
	    mp, BUS_DMA_NOWAIT);
d2263 1
a2263 1
		m_freem(mp);
d2267 3
a2269 11
	/* Now check our target buffer for existing mapping */
	rxbuf = &rxr->rx_buffers[i];
	old = rxbuf->bigbuf;
	if (rxbuf->m_head != NULL)
		bus_dmamap_unload(rxr->rxtag[old], rxbuf->map[old]);

        map = rxbuf->map[old];
        rxbuf->map[s] = rxr->spare_map[s];
        rxr->spare_map[old] = map;
        rxbuf->m_head = mp;
        rxbuf->bigbuf = s;
d2271 2
a2272 2
        rxr->rx_base[i].read.pkt_addr =
	    htole64(rxbuf->map[s]->dm_segs[0].ds_addr);
d2274 2
a2275 2
        bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
	    0, rxbuf->map[s]->dm_mapsize, BUS_DMASYNC_PREREAD);
d2280 6
a2285 1
        	struct dhack {uint32_t a1; uint32_t a2; uint32_t b1; uint32_t b2;};
d2288 1
a2288 1
        	d = (struct dhack *)&rxr->rx_base[i];
d2294 5
a2314 1
	struct ixgbe_osdep	*os = &sc->osdep;
d2316 1
a2316 1
	int             	i, bsize, error;
d2319 1
a2319 2
	if (!(rxr->rx_buffers =
	    (struct ixgbe_rx_buf *) malloc(bsize,
d2321 2
a2322 1
		printf("%s: Unable to allocate rx_buffer memory\n", ifp->if_xname);
d2326 1
a2326 17
	rxr->rxtag[0] = rxr->rxtag[1] = os->os_pa->pa_dmat;

	/* Create the spare maps (used by getbuf) */
        error = bus_dmamap_create(rxr->rxtag[0],
	    MCLBYTES, 1, MCLBYTES, 0, BUS_DMA_NOWAIT, &rxr->spare_map[0]);
	if (error) {
		printf("%s: %s: bus_dmamap_create failed: %d\n", ifp->if_xname,
		    __func__, error);
		goto fail;
	}
        error = bus_dmamap_create(rxr->rxtag[1],
	    MJUMPAGESIZE, 1, MJUMPAGESIZE, 0, BUS_DMA_NOWAIT, &rxr->spare_map[1]);
	if (error) {
		printf("%s: %s: bus_dmamap_create failed: %d\n", ifp->if_xname,
		    __func__, error);
		goto fail;
	}
d2328 1
d2330 2
a2331 3
		rxbuf = &rxr->rx_buffers[i];
		error = bus_dmamap_create(rxr->rxtag[0], MCLBYTES, 1, MCLBYTES,
		    0, BUS_DMA_NOWAIT, &rxbuf->map[0]);
d2333 2
a2334 7
			printf("%s: Unable to create Small RX DMA map\n", ifp->if_xname);
			goto fail;
		}
		error = bus_dmamap_create(rxr->rxtag[1], MJUMPAGESIZE, 1, MJUMPAGESIZE,
		    0, BUS_DMA_NOWAIT, &rxbuf->map[1]);
		if (error) {
			printf("%s: Unable to create Large RX DMA map\n", ifp->if_xname);
d2337 1
d2339 3
a2345 2
	/* Frees all, but can handle partial completion */
	ixgbe_free_receive_structures(sc);
d2358 1
a2358 2
	struct ixgbe_rx_buf	*rxbuf;
	int			j, rsize, s = 0, i;
d2365 2
a2366 27
	/*
	** Free current RX buffers: the size buffer
	** that is loaded is indicated by the buffer
	** bigbuf value.
	*/
	for (i = 0; i < sc->num_rx_desc; i++) {
		rxbuf = &rxr->rx_buffers[i];
		s = rxbuf->bigbuf;
		if (rxbuf->m_head != NULL) {
			bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
			    0, rxbuf->map[s]->dm_mapsize,
			    BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
			m_freem(rxbuf->m_head);
			rxbuf->m_head = NULL;
		}
	}

	for (j = 0; j < sc->num_rx_desc; j++) {
		if (ixgbe_get_buf(rxr, j, NULL) == ENOBUFS) {
			rxr->rx_buffers[j].m_head = NULL;
			rxr->rx_base[j].read.pkt_addr = 0;
			/* If we fail some may have change size */
			s = sc->bigbufs;
			goto fail;
		}
	}
d2370 1
a2370 1
	rxr->last_cleaned = 0;
d2372 6
a2377 3
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map,
	    0, rxr->rxdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d2380 19
a2398 16
fail:
	/*
	 * We need to clean up any buffers allocated so far
	 * 'j' is the failing index, decrement it to get the
	 * last success.
	 */
	for (--j; j < 0; j--) {
		rxbuf = &rxr->rx_buffers[j];
		if (rxbuf->m_head != NULL) {
			bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
			    0, rxbuf->map[s]->dm_mapsize,
			    BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
			m_freem(rxbuf->m_head);
			rxbuf->m_head = NULL;
		}
d2400 2
a2401 1
	return (ENOBUFS);
d2413 1
a2413 1
	int i, j, s;
d2420 1
d2422 1
a2422 22
	/*
	 * Free RX buffers allocated so far, we will only handle
	 * the rings that completed, the failing case will have
	 * cleaned up for itself. The value of 'i' will be the
	 * failed ring so we must pre-decrement it.
	 */
	rxr = sc->rx_rings;
	for (--i; i > 0; i--, rxr++) {
		for (j = 0; j < sc->num_rx_desc; j++) {
			struct ixgbe_rx_buf *rxbuf;
			rxbuf = &rxr->rx_buffers[j];
			s = rxbuf->bigbuf;
			if (rxbuf->m_head != NULL) {
				bus_dmamap_sync(rxr->rxtag[s], rxbuf->map[s],
				    0, rxbuf->map[s]->dm_mapsize, BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
				m_freem(rxbuf->m_head);
				rxbuf->m_head = NULL;
			}
		}
	}

d2491 1
a2491 1
		    sc->num_rx_desc - 1);
a2570 2
		/* Free the ring memory as well */
		ixgbe_dma_free(sc, &rxr->rxdma);
a2571 2

	free(sc->rx_rings, M_DEVBUF);
d2584 1
a2584 1
	int			 i, s;
d2589 2
a2590 7
		rxbuf = &rxr->rx_buffers[0];
		for (i = 0; i < sc->num_rx_desc; i++) {
			int s = rxbuf->bigbuf;
			if (rxbuf->map != NULL) {
				bus_dmamap_unload(rxr->rxtag[s], rxbuf->map[s]);
				bus_dmamap_destroy(rxr->rxtag[s], rxbuf->map[s]);
			}
d2592 4
d2597 1
d2599 2
a2600 2
			rxbuf->m_head = NULL;
			++rxbuf;
d2607 8
a2614 3
	for (s = 0; s < 2; s++) {
		if (rxr->rxtag[s] != NULL)
			rxr->rxtag[s] = NULL;
a2615 1
	return;
d2633 4
a2636 3
	struct mbuf    		*mp;
	int             	 len, i, eop = 0;
	uint8_t		 accept_frame = 0;
d2638 7
a2644 1
	union ixgbe_adv_rx_desc	*cur;
a2646 1
	cur = &rxr->rx_base[i];
d2648 9
a2656 2
	bus_dmamap_sync(rxr->rxdma.dma_tag, rxr->rxdma.dma_map, 0,
	    rxr->rxdma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
d2658 7
a2664 3
	staterr = cur->wb.upper.status_error;
	if (!(staterr & IXGBE_RXD_STAT_DD))
		return FALSE;
d2666 3
a2668 9
	while ((staterr & IXGBE_RXD_STAT_DD) && (count != 0) &&
	    (ifp->if_flags & IFF_RUNNING)) {
		struct mbuf *m = NULL;
		int s;

		mp = rxr->rx_buffers[i].m_head;
		s = rxr->rx_buffers[i].bigbuf;
		bus_dmamap_sync(rxr->rxtag[s], rxr->rx_buffers[i].map[s],
		    0, rxr->rx_buffers[i].map[s]->dm_mapsize,
d2670 13
a2682 2
		bus_dmamap_unload(rxr->rxtag[s],
		    rxr->rx_buffers[i].map[s]);
d2685 3
d2694 1
a2694 1
		len = cur->wb.upper.length;
d2700 1
a2700 5
			/* Get a fresh buffer */
			if (ixgbe_get_buf(rxr, i, NULL) != 0) {
				ifp->if_iqdrops++;
				goto discard;
			}
d2702 1
a2702 2
			/* Assign correct length to the current fragment */
			mp->m_len = len;
d2705 3
a2707 3
				mp->m_pkthdr.len = len;
				rxr->fmp = mp; /* Store the first mbuf */
				rxr->lmp = mp;
d2710 15
a2724 4
				mp->m_flags &= ~M_PKTHDR;
				rxr->lmp->m_next = mp;
				rxr->lmp = rxr->lmp->m_next;
				rxr->fmp->m_pkthdr.len += len;
a2727 1
				rxr->fmp->m_pkthdr.rcvif = ifp;
d2729 4
d2735 2
a2736 1
				ixgbe_rx_checksum(sc, staterr, rxr->fmp);
d2740 3
a2742 3
					rxr->fmp->m_pkthdr.ether_vtag =
					    letoh16(cur->wb.upper.vlan);
					rxr->fmp->m_flags |= M_VLANTAG;
d2745 7
a2752 1
				m = rxr->fmp;
d2757 2
a2758 2
discard:
			ixgbe_get_buf(rxr, i, mp);
d2764 2
a2765 1
			m = NULL;
d2769 2
a2770 1
		cur->wb.upper.status_error = 0;
d2772 2
a2773 4
		    0, rxr->rxdma.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		rxr->last_cleaned = i; /* for updating tail */
d2775 1
a2777 15

		/* Now send up to the stack */
                if (m != NULL) {
                        rxr->next_to_check = i;
#if NBPFILTER > 0
			if (ifp->if_bpf)
				bpf_mtap_ether(ifp->if_bpf, m,
				    BPF_DIRECTION_IN);
#endif
			ether_input_mbuf(ifp, m);
			i = rxr->next_to_check;
                }
		/* Get next descriptor */
		cur = &rxr->rx_base[i];
		staterr = cur->wb.upper.status_error;
a2779 3

	/* Advance the IXGB's Receive Queue "Tail Pointer" */
	IXGBE_WRITE_REG(&sc->hw, IXGBE_RDT(rxr->me), rxr->last_cleaned);
@


1.14
log
@Introduce bpf_mtap_ether(), which for the benefit of bpf listeners
creates the VLAN encapsulation from the tag stored in the mbuf
header.  Idea from FreeBSD, input from claudio@@ and canacar@@.

Switch all hardware VLAN enabled drivers to the new function.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.13 2008/10/28 05:39:18 brad Exp $	*/
d428 4
a431 4
	int             s, error = 0;
	struct ifreq   *ifr = (struct ifreq *) data;
	struct ifaddr   *ifa = (struct ifaddr *) data;
	struct ix_softc *sc = ifp->if_softc;
d446 1
d458 1
d475 1
a475 16
	case SIOCADDMULTI:
	case SIOCDELMULTI:
		IOCTL_DEBUGOUT("ioctl: SIOC(ADD|DEL)MULTI");
		error = (command == SIOCADDMULTI) ?
		    ether_addmulti(ifr, &sc->arpcom) :
		    ether_delmulti(ifr, &sc->arpcom);

		if (error == ENETRESET) {
			if (ifp->if_flags & IFF_RUNNING) {
				ixgbe_disable_intr(sc);
				ixgbe_set_multi(sc);
				ixgbe_enable_intr(sc);
			}
			error = 0;
		}
		break;
d481 1
d484 9
@


1.13
log
@Fix up some of the code for VLAN tagging/stripping and checksum offload
support.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.12 2008/10/16 19:18:03 naddy Exp $	*/
d365 1
a365 1
			bpf_mtap(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
d2816 2
a2817 1
				bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
@


1.12
log
@Switch the existing TX VLAN hardware support over to having the
tag in the header.  Convert TX tagging in the drivers.

Help and ok brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.11 2008/10/16 19:16:21 naddy Exp $	*/
a114 1
#if NVLAN > 0
a115 1
#endif
a603 1
#if NVLAN > 0
a605 1
#endif
d841 1
a841 1
        	cmd_type_len |= IXGBE_ADVTXD_DCMD_VLE;
d1392 6
a1397 1
	ifp->if_capabilities |= IFCAP_VLAN_MTU;
d1399 2
a1400 3
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
	ifp->if_capabilities |= IFCAP_CSUM_IPv4;
	ifp->if_capabilities |= IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;
a2774 2
				m = rxr->fmp;

d2777 1
a2777 1
#if NVLAN > 0 && defined(IX_CSUM_OFFLOAD)
d2779 1
a2779 1
					m->m_pkthdr.ether_vtag =
d2781 1
a2781 1
					m->m_flags |= M_VLANTAG;
d2784 2
a2846 1
	struct ifnet   	*ifp = &sc->arpcom.ac_if;
a2849 11
	/* Not offloading */
	if ((ifp->if_capabilities & IFCAP_CSUM_IPv4) == 0) {
		mp->m_pkthdr.csum_flags = 0;
		return;
	}

	// XXX
	printf("%s: status 0x%04x errors 0x%02x\n", ifp->if_xname,
	    status, errors);

	mp->m_pkthdr.csum_flags = 0;
d2852 1
a2852 1
		if (!(errors & IXGBE_RXD_ERR_IPE))
d2855 2
d2858 8
a2865 7
	/* Did it pass? */
	if (errors & IXGBE_RXD_ERR_TCPE)
		return;
	if (status & IXGBE_RXD_STAT_L4CS)
		mp->m_pkthdr.csum_flags |= M_TCP_CSUM_IN_OK;
	if (status & IXGBE_RXD_STAT_UDPCS)
		mp->m_pkthdr.csum_flags |= M_UDP_CSUM_IN_OK;
a2867 1
#if NVLAN > 0
a2879 1
#endif
@


1.11
log
@Convert RX tag stripping to storing the tag in the mbuf header and
enable RX tag stripping for re(4).

ok brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.10 2008/10/11 20:31:50 miod Exp $	*/
d844 1
a844 3
	/* VLAN tagging? */
	if ((m_head->m_flags & (M_PROTO1|M_PKTHDR)) == (M_PROTO1|M_PKTHDR) &&
	    m_head->m_pkthdr.rcvif != NULL)
a1888 5
	struct ifvlan		*ifv = NULL;

	if ((mp->m_flags & (M_PROTO1|M_PKTHDR)) == (M_PROTO1|M_PKTHDR) &&
	    mp->m_pkthdr.rcvif != NULL)
		ifv = mp->m_pkthdr.rcvif->if_softc;
d1904 1
a1904 1
	if (ifv != NULL) {
d1906 1
a1906 1
		    htole16(ifv->ifv_tag) << IXGBE_ADVTXD_VLAN_SHIFT;
a2016 6

	struct ifvlan		*ifv = NULL;

	if ((mp->m_flags & (M_PROTO1|M_PKTHDR)) == (M_PROTO1|M_PKTHDR) &&
	    mp->m_pkthdr.rcvif != NULL)
		ifv = mp->m_pkthdr.rcvif->if_softc;
d2066 1
a2066 1
	if (ifv != NULL) {
d2068 1
a2068 1
                vlan_macip_lens |= (ifv->ifv_tag << IXGBE_ADVTXD_VLAN_SHIFT);
@


1.10
log
@Fix various printf constructs either missing arguments or format specifiers.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.9 2008/10/02 20:21:14 brad Exp $	*/
d2794 1
a2794 8
					struct ether_vlan_header vh;

					if (m->m_pkthdr.len < ETHER_HDR_LEN)
						goto discard;
					m_copydata(m, 0,
					    ETHER_HDR_LEN, (caddr_t)&vh);
					vh.evl_proto = vh.evl_encap_proto;
					vh.evl_tag =
d2796 1
a2796 7
					vh.evl_encap_proto =
					    htons(ETHERTYPE_VLAN);
					m_adj(m, ETHER_HDR_LEN);
					M_PREPEND(m, sizeof(vh), M_DONTWAIT);
					if (m == NULL)
						goto discard;
					m_copyback(m, 0, sizeof(vh), &vh);
@


1.9
log
@First step towards cleaning up the Ethernet driver ioctl handling.
Move calling ether_ioctl() from the top of the ioctl function, which
at the moment does absolutely nothing, to the default switch case.
Thus allowing drivers to define their own ioctl handlers and then
falling back on ether_ioctl(). The only functional change this results
in at the moment is having all Ethernet drivers returning the proper
errno of ENOTTY instead of EINVAL/ENXIO when encountering unknown
ioctl's.

Shrinks the i386 kernels by..
RAMDISK - 1024 bytes
RAMDISKB -  1120 bytes
RAMDISKC - 832 bytes

Tested by martin@@/jsing@@/todd@@/brad@@
Build tested on almost all archs by todd@@/brad@@

ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.8 2008/09/10 14:01:22 blambert Exp $	*/
d1364 1
a1364 1
		printf("%s: Hardware Initialization Failed");
@


1.8
log
@Convert timeout_add() calls using multiples of hz to timeout_add_sec()

Really just the low-hanging fruit of (hopefully) forthcoming timeout
conversions.

ok art@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.7 2008/06/19 08:43:55 reyk Exp $	*/
a436 5
	if ((error = ether_ioctl(ifp, &sc->arpcom, command, data)) > 0) {
		splx(s);
		return (error);
	}

d497 1
a497 3
		IOCTL_DEBUGOUT1("ioctl: UNKNOWN (0x%X)\n", (int)command);
		error = ENOTTY;
		break;
@


1.7
log
@the 82598AT variant of ix(4) is 10GbaseT, change media type from AUTO.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.6 2008/06/09 07:07:16 djm Exp $	*/
d688 1
a688 1
	timeout_add(&sc->timer, hz);
d744 1
a744 1
			timeout_add(&sc->timer, hz);
d1103 1
a1103 1
	timeout_add(&sc->timer, hz);
@


1.6
log
@rename arc4random_bytes => arc4random_buf to match libc's nicer name;
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.5 2008/06/08 21:15:34 reyk Exp $	*/
d1232 2
@


1.5
log
@more cleanup, removed unused code. we don't do LRO/RSS yet, code can
be added later if we ever support it.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.4 2008/06/08 20:58:42 reyk Exp $	*/
d2590 1
a2590 1
		arc4random_bytes(&random, sizeof(random));
@


1.4
log
@dma sync the tx ring and post new packets to the chip once per call to
the start routine instead of once per packet.

From ixgb(4), also works with ix(4)
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.3 2008/06/08 20:36:34 reyk Exp $	*/
a117 3
#if 0
int	ixgbe_set_flowcntl(SYSCTL_HANDLER_ARGS);
#endif
d1312 1
a1313 1
#if 0
a1391 3
#if 0
	ifp->if_init = ixgbe_init;
#endif
d2010 1
a2010 1
#if 0
d2255 1
a2255 1
#if 0
a2411 4
#if 0
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct lro_ctrl		*lro = &rxr->lro;
#endif
a2454 13
#if 0
	/* Now set up the LRO interface */
	if (ixgbe_enable_lro) {
		int err = tcp_lro_init(lro);
		if (err) {
			printf("%s: LRO Initialization failed!\n", ifp->if_xname);
			goto fail;
		}
		printf("%s: RX LRO Initialized\n", ifp->if_xname);
		lro->ifp = &sc->arpcom.ac_if;
	}
#endif

d2635 2
a2636 3
#if 0
	if (ifp->if_capenable & IFCAP_RXCSUM)
		rxcsum |= IXGBE_RXCSUM_PCSD;
a2637 1

a2661 5
#if 0
		struct lro_ctrl		*lro = &rxr->lro;
		/* Free LRO memory */
		tcp_lro_free(lro);
#endif
a2724 4
#if 0
	struct lro_ctrl		*lro = &rxr->lro;
	struct lro_entry	*queued;
#endif
a2844 4
#ifdef notyet
			/* Use LRO if possible */
			if ((!lro->lro_cnt) || (tcp_lro_rx(lro, m, 0))) {
#endif
d2846 2
a2847 7
				if (ifp->if_bpf)
					bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
#endif
				ether_input_mbuf(ifp, m);
				i = rxr->next_to_check;
#ifdef notyet
			}
d2849 2
a2860 13
#if 0
	/*
	** Flush any outstanding LRO work
	** this may call into the stack and
	** must not hold a driver lock.
	*/
	while(!SLIST_EMPTY(&lro->lro_active)) {
		queued = SLIST_FIRST(&lro->lro_active);
		SLIST_REMOVE_HEAD(&lro->lro_active, next);
		tcp_lro_flush(lro, queued);
	}
#endif

a3131 35
}

/*
 * Set flow control using sysctl:
 * Flow control values:
 * 	0 - off
 *	1 - rx pause
 *	2 - tx pause
 *	3 - full
 */
int
ixgbe_set_flowcntl(SYSCTL_HANDLER_ARGS)
{
	int error;
	struct ix_softc *sc;

	error = sysctl_handle_int(oidp, &ixgbe_flow_control, 0, req);

	if (error)
		return (error);

	sc = (struct ix_softc *) arg1;
	switch (ixgbe_flow_control) {
		case ixgbe_fc_rx_pause:
		case ixgbe_fc_tx_pause:
		case ixgbe_fc_full:
			sc->hw.fc.type = ixgbe_flow_control;
			break;
		case ixgbe_fc_none:
		default:
			sc->hw.fc.type = ixgbe_fc_none;
	}

	ixgbe_setup_fc(&sc->hw, 0);
	return error;
@


1.3
log
@we don't support msi/msi-x, remove the code
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.2 2008/06/08 20:33:51 reyk Exp $	*/
d344 1
d352 4
a357 1

d376 2
d379 12
a390 1
	return;
a960 9
        bus_dmamap_sync(txr->txdma.dma_tag, txr->txdma.dma_map,
	    0, txr->txdma.dma_map->dm_mapsize, 
            BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	/*
	 * Advance the Transmit Descriptor Tail (Tdt), this tells the
	 * hardware that this frame is available to transmit.
	 */
	IXGBE_WRITE_REG(&sc->hw, IXGBE_TDT(txr->me), i);
@


1.2
log
@replace strange Linux-style u8/u16/u32/u64/s32 integer types with the
standard C99 uint*_t/int*_t types (i don't get why these drivers
always use their own types when there is a well-defined standard).
@
text
@d1 1
a1 1
/*	$OpenBSD: if_ix.c,v 1.1 2008/06/08 20:01:02 reyk Exp $	*/
a118 1
void	ixgbe_print_debug_info(struct ix_softc *);
a134 9
#if 0
/* The MSI/X Interrupt handlers */
int	ixgbe_allocate_msix(struct ix_softc *);
int	ixgbe_setup_msix(struct ix_softc *);
void	ixgbe_msix_tx(void *);
void	ixgbe_msix_rx(void *);
void	ixgbe_msix_link(void *);
#endif

a1272 150
#if 0
/*********************************************************************
 *
 *  Setup MSIX Interrupt resources and handlers 
 *
 **********************************************************************/
int
ixgbe_allocate_msix(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	struct 		tx_ring *txr = sc->tx_rings;
	struct		rx_ring *rxr = sc->rx_rings;
	int 		error, vector = 0, i;

	/* TX setup: the code is here for multi tx,
	   there are other parts of the driver not ready for it */
	for (i = 0; i < sc->num_tx_queues; i++, vector++, txr++) {
		sc->res[vector] = bus_alloc_resource_any(dev,
	    	    SYS_RES_IRQ, &sc->rid[vector],
		    RF_SHAREABLE | RF_ACTIVE);
		if (!sc->res[vector]) {
			printf("%s: Unable to allocate"
		    	    " bus resource: tx interrupt [%d]\n", ifp->if_xname, vector);
			return (ENXIO);
		}
		/* Set the handler function */
		error = bus_setup_intr(dev, sc->res[vector],
		    INTR_TYPE_NET | INTR_MPSAFE, NULL,
		    ixgbe_msix_tx, txr, &sc->tag[vector]);
		if (error) {
			sc->res[vector] = NULL;
			printf("%s: Failed to register TX handler");
			return (error);
		}
		txr->msix = vector;
		txr->eims = IXGBE_IVAR_TX_QUEUE(vector);
		/* Make tasklet for deferred handling - one per queue */
		txr->tx_task = ixgbe_handle_tx;
		txr->tq = workq_create("ix_txq", sc->num_tx_queues + 1);
	}

	/* RX setup */
	for (i = 0; i < sc->num_rx_queues; i++, vector++, rxr++) {
		sc->res[vector] = bus_alloc_resource_any(dev,
	    	    SYS_RES_IRQ, &sc->rid[vector],
		    RF_SHAREABLE | RF_ACTIVE);
		if (!sc->res[vector]) {
			printf("%s: Unable to allocate"
		    	    " bus resource: rx interrupt [%d],"
			    "rid = %d\n", ifp->if_xname, i, sc->rid[vector]);
			return (ENXIO);
		}
		/* Set the handler function */
		error = bus_setup_intr(dev, sc->res[vector],
		    INTR_TYPE_NET | INTR_MPSAFE, NULL, ixgbe_msix_rx,
		    rxr, &sc->tag[vector]);
		if (error) {
			sc->res[vector] = NULL;
			printf("%s: Failed to register RX handler");
			return (error);
		}
		rxr->msix = vector;
		rxr->eims = IXGBE_IVAR_RX_QUEUE(vector);
		rxr->rx_task = ixgbe_handle_rx;
		rxr->tq = workq_create("ix_rxq", sc->num_rx_queues + 1);
	}

	/* Now for Link changes */
	sc->res[vector] = bus_alloc_resource_any(dev,
    	    SYS_RES_IRQ, &sc->rid[vector], RF_SHAREABLE | RF_ACTIVE);
	if (!sc->res[vector]) {
		printf("%s: Unable to allocate"
    	    " bus resource: Link interrupt [%d]\n", ifp->if_xname, sc->rid[vector]);
		return (ENXIO);
	}
	/* Set the link handler function */
	error = bus_setup_intr(dev, sc->res[vector],
	    INTR_TYPE_NET | INTR_MPSAFE, NULL, ixgbe_msix_link,
	    sc, &sc->tag[vector]);
	if (error) {
		sc->res[vector] = NULL;
		printf("%s: Failed to register LINK handler");
		return (error);
	}
	sc->linkvec = vector;
	sc->link_task = ixgbe_handle_link;

	return (0);
}

/*
 * Setup Either MSI/X or MSI
 */
int
ixgbe_setup_msix(struct ix_softc *sc)
{
	struct ifnet	*ifp = &sc->arpcom.ac_if;
	int rid, want, queues, msgs;

	/* First try MSI/X */
	rid = PCIR_BAR(IXGBE_MSIX_BAR);
	sc->msix_mem = bus_alloc_resource_any(dev,
	    SYS_RES_MEMORY, &rid, RF_ACTIVE);
       	if (!sc->msix_mem) {
		/* May not be enabled */
		printf("%s: Unable to map MSIX table \n", ifp->if_xname);
		goto msi;
	}

	msgs = pci_msix_count(dev); 
	if (msgs == 0) { /* system has msix disabled */
		bus_release_resource(dev, SYS_RES_MEMORY,
		    PCIR_BAR(IXGBE_MSIX_BAR), sc->msix_mem);
		sc->msix_mem = NULL;
		goto msi;
	}

	/* Figure out a reasonable auto config value */
	queues = (mp_ncpus > ((msgs-1)/2)) ? (msgs-1)/2 : mp_ncpus;

	if (ixgbe_tx_queues == 0)
		ixgbe_tx_queues = queues;
	if (ixgbe_rx_queues == 0)
		ixgbe_rx_queues = queues;
	want = ixgbe_tx_queues + ixgbe_rx_queues + 1;
	if (msgs >= want)
		msgs = want;
	else {
		printf("%s: MSIX Configuration Problem, "
		    "%d vectors but %d queues wanted!\n", ifp->if_xname,
		    msgs, want);
		return (ENXIO);
	}
	if ((msgs) && pci_alloc_msix(dev, &msgs) == 0) {
		printf("%s: Using MSIX interrupts with %d vectors\n",
		    ifp->if_xname, msgs);
		sc->num_tx_queues = ixgbe_tx_queues;
		sc->num_rx_queues = ixgbe_rx_queues;
		return (msgs);
	}
msi:
       	msgs = pci_msi_count(dev);
       	if (msgs == 1 && pci_alloc_msi(dev, &msgs) == 0)
               	printf("%s: Using MSI interrupt\n", ifp->if_xname);
	return (msgs);
	/* MSI is not supported yet */
	return (0);
}
#endif

a1321 34
#if 0
	int			 i;

	/*
	 * Legacy has this set to 0, but we need
	 * to run this once, so reset it.
	 */
	if (sc->msix == 0)
		sc->msix = 1;

	/*
	 * First release all the interrupt resources:
	 * 	notice that since these are just kept
	 *	in an array we can do the same logic
	 * 	whether its MSIX or just legacy.
	 */
	for (i = 0; i < sc->msix; i++) {
		if (sc->tag[i] != NULL) {
			pci_intr_disestablish(pa->pa_pc, sc->tag[0]);
			sc->tag[i] = NULL;
		}
		if (sc->res[i] != NULL) {
			bus_release_resource(dev, SYS_RES_IRQ,
			    sc->rid[i], sc->res[i]);
		}
	}

	if (sc->msix)
		pci_release_msi(dev);

	if (sc->msix_mem != NULL)
		bus_release_resource(dev, SYS_RES_MEMORY,
		    PCIR_BAR(IXGBE_MSIX_BAR), sc->msix_mem);
#else
a1323 2
#endif

d1326 1
a1326 1
	os->os_membase = 0;
a1866 4
#if 0
	if (txr->txtag != NULL)
		bus_dma_tag_destroy(txr->txtag);
#endif
a1867 1
	return;
d2726 1
a2726 4
		if (rxr->rxtag[s] != NULL) {
#if 0
			bus_dma_tag_destroy(rxr->rxtag[s]);
#endif
a2727 1
		}
@


1.1
log
@Import ix, a driver for the Intel 82598 PCI-Express 10 Gig Ethernet Adapter,
based on Intel's ixgbe driver.

Done on borrowed hardware since Intel was too poor to give us a card.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d103 3
a105 3
bool	ixgbe_txeof(struct tx_ring *);
bool	ixgbe_rxeof(struct rx_ring *, int);
void	ixgbe_rx_checksum(struct ix_softc *, u32, struct mbuf *);
d126 2
a127 2
boolean_t ixgbe_tx_ctx_setup(struct tx_ring *, struct mbuf *);
boolean_t ixgbe_tso_setup(struct tx_ring *, struct mbuf *, u32 *);
d129 1
a129 1
void	ixgbe_set_ivar(struct ix_softc *, u16, u8);
d131 1
a131 1
u8	*ixgbe_mc_array_itr(struct ixgbe_hw *, u8 **, u32 *);
d198 1
a198 1
	u32			 ctrl_ext;
d289 1
a289 1
	u32	ctrl_ext;
d392 1
a392 1
	u32 queue = 0;
d521 1
a521 1
	bool		tx_hang = FALSE;
d586 1
a586 1
	u32		 txdctl, rxdctl, mhadd, gpie;
d709 1
a709 1
	u32       	 reg_eicr;
d831 2
a832 2
	struct ix_softc  *sc = txr->sc;
	u32		olinfo_status = 0, cmd_type_len = 0;
d839 1
a839 1
	u32		paylen = 0;
d976 1
a976 1
	u_int32_t       reg_rctl;
d995 1
a995 1
	u_int32_t       reg_rctl;
d1018 3
a1020 3
	u32	fctrl;
	u8	mta[MAX_NUM_MULTICAST_ADDRESSES * IXGBE_ETH_LENGTH_OF_ADDRESS];
	u8	*update_ptr;
d1067 2
a1068 2
u8 *
ixgbe_mc_array_itr(struct ixgbe_hw *hw, u8 **update_ptr, u32 *vmdq)
d1070 2
a1071 2
	u8 *addr = *update_ptr;
	u8 *newptr;
d1116 1
a1116 1
	boolean_t link_up = FALSE;
d1201 1
a1201 1
	u_int32_t		 reg;
d1452 1
a1452 1
	sc->hw.hw_addr = (u8 *)os->os_membase;
d1539 1
a1539 1
	u16 csum;
d1776 1
a1776 1
		if (ixgbe_dma_malloc(sc, sizeof(u_int32_t),
d1783 1
a1783 1
		txr->tx_hwb = (u_int32_t *)txr->txwbdma.dma_vaddr;
d1960 2
a1961 2
	u64		 tdba, txhwb;
	u32		 txctrl;
d2078 1
a2078 1
boolean_t
d2085 1
a2085 1
	u32 vlan_macip_lens = 0, type_tucmd_mlhl = 0;
d2089 3
a2091 3
	u16	etype;
	u8	ipproto = 0;
	bool	offload = TRUE;
d2216 2
a2217 2
boolean_t
ixgbe_tso_setup(struct tx_ring *txr, struct mbuf *mp, u32 *paylen)
d2222 2
a2223 2
	u32 vlan_macip_lens = 0, type_tucmd_mlhl = 0;
	u32 mss_l4len_idx = 0;
d2226 1
a2226 1
	u16 vtag = 0;
d2323 2
a2324 2
boolean_t
ixgbe_tso_setup(struct tx_ring *txr, struct mbuf *mp, u32 *paylen)
d2338 1
a2338 1
boolean_t
d2343 1
a2343 1
	u_int				 first, last, done, num_avail;
d2522 1
a2522 1
        	struct dhack {u32 a1; u32 a2; u32 b1; u32 b2;};
d2744 3
a2746 3
	u32		rxctrl, fctrl, srrctl, rxcsum;
	u32		reta, mrqc, hlreg, linkvec;
	u32		random[10];
a2748 1

d2788 1
a2788 1
		u64 rdba = rxr->rxdma.dma_map->dm_segs[0].ds_addr;
d2947 1
a2947 1
bool
d2950 1
a2950 1
	struct ix_softc 		*sc = rxr->sc;
d2957 3
a2959 3
	int             	len, i, eop = 0;
	u8			accept_frame = 0;
	u32			staterr;
d3127 1
a3127 1
    u32 staterr, struct mbuf * mp)
d3130 2
a3131 2
	u16 status = (u16) staterr;
	u8  errors = (u8) (staterr >> 24);
d3163 1
a3163 1
	u32        ctrl;
d3178 1
a3178 1
	u32 mask = IXGBE_EIMS_ENABLE_MASK;
d3208 2
a3209 2
u16
ixgbe_read_pci_cfg(struct ixgbe_hw *hw, u32 reg)
d3212 1
a3212 1
	u16 value;
d3223 1
a3223 1
ixgbe_set_ivar(struct ix_softc *sc, u16 entry, u8 vector)
d3225 1
a3225 1
	u32 ivar, index;
d3268 1
a3268 1
	u32  missed_rx = 0, bprc, lxon, lxoff, total;
d3428 1
a3428 1
        struct dhack {u32 a1; u32 a2; u32 b1; u32 b2;};
@

