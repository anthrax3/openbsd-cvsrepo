head	1.20;
access;
symbols
	OPENBSD_6_1:1.20.0.2
	OPENBSD_6_1_BASE:1.20
	OPENBSD_6_0:1.19.0.4
	OPENBSD_6_0_BASE:1.19
	OPENBSD_5_9:1.18.0.2
	OPENBSD_5_9_BASE:1.18
	OPENBSD_5_8:1.13.0.4
	OPENBSD_5_8_BASE:1.13
	OPENBSD_5_7:1.10.0.4
	OPENBSD_5_7_BASE:1.10
	OPENBSD_5_6:1.9.0.6
	OPENBSD_5_6_BASE:1.9
	OPENBSD_5_5:1.9.0.4
	OPENBSD_5_5_BASE:1.9
	OPENBSD_5_4:1.7.0.4
	OPENBSD_5_4_BASE:1.7
	OPENBSD_5_3:1.7.0.2
	OPENBSD_5_3_BASE:1.7
	OPENBSD_5_2:1.6.0.8
	OPENBSD_5_2_BASE:1.6
	OPENBSD_5_1_BASE:1.6
	OPENBSD_5_1:1.6.0.6
	OPENBSD_5_0:1.6.0.4
	OPENBSD_5_0_BASE:1.6
	OPENBSD_4_9:1.6.0.2
	OPENBSD_4_9_BASE:1.6
	OPENBSD_4_8:1.2.0.2
	OPENBSD_4_8_BASE:1.2;
locks; strict;
comment	@ * @;


1.20
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.19;
commitid	VyLWTsbepAOk7VQM;

1.19
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.18;
commitid	8YSL8ByWzGeIGBiJ;

1.18
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.17;
commitid	B0kwmVGiD5DVx4kv;

1.17
date	2015.11.24.17.11.39;	author mpi;	state Exp;
branches;
next	1.16;
commitid	5gdEnqVoJuTuwdTu;

1.16
date	2015.11.24.13.33.17;	author mpi;	state Exp;
branches;
next	1.15;
commitid	5DvsamK0GblTp8ww;

1.15
date	2015.11.20.03.35.23;	author dlg;	state Exp;
branches;
next	1.14;
commitid	eYnPulzvLjDImPCa;

1.14
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.13;
commitid	hPF95ClMUQfeqQDX;

1.13
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.12;
commitid	MVWrtktB46JRxFWT;

1.12
date	2015.04.30.07.51.07;	author mpi;	state Exp;
branches;
next	1.11;
commitid	H09AuNxNnUcYramX;

1.11
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.10;
commitid	p4LJxGKbi0BU2cG6;

1.10
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.9;
commitid	yM2VFFhpDTeFQlve;

1.9
date	2013.12.28.03.34.54;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	2013.12.06.21.03.04;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	2012.09.26.19.18.00;	author brad;	state Exp;
branches;
next	1.6;

1.6
date	2010.09.07.07.54.44;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2010.09.05.12.42.54;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2010.09.04.12.47.00;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2010.08.27.17.08.00;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2010.04.02.22.42.55;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2010.04.02.20.07.30;	author deraadt;	state Exp;
branches;
next	;


desc
@@


1.20
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@/*	$OpenBSD: if_se.c,v 1.19 2016/04/13 10:34:32 mpi Exp $	*/

/*-
 * Copyright (c) 2009, 2010 Christopher Zimmermann <madroach@@zakweb.de>
 * Copyright (c) 2008, 2009, 2010 Nikolay Denev <ndenev@@gmail.com>
 * Copyright (c) 2007, 2008 Alexander Pohoyda <alexander.pohoyda@@gmx.net>
 * Copyright (c) 1997, 1998, 1999
 *	Bill Paul <wpaul@@ctr.columbia.edu>.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Bill Paul.
 * 4. Neither the name of the author nor the names of any co-contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY Bill Paul AND CONTRIBUTORS ``AS IS''
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
 * PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL AUTHORS OR
 * THE VOICES IN THEIR HEADS BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * SiS 190/191 PCI Ethernet NIC driver.
 *
 * Adapted to SiS 190 NIC by Alexander Pohoyda based on the original
 * SiS 900 driver by Bill Paul, using SiS 190/191 Solaris driver by
 * Masayuki Murayama and SiS 190/191 GNU/Linux driver by K.M. Liu
 * <kmliu@@sis.com>.  Thanks to Pyun YongHyeon <pyunyh@@gmail.com> for
 * review and very useful comments.
 *
 * Ported to OpenBSD by Christopher Zimmermann 2009/10
 *
 * Adapted to SiS 191 NIC by Nikolay Denev with further ideas from the
 * Linux and Solaris drivers.
 */

#include "bpfilter.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/device.h>
#include <sys/ioctl.h>
#include <sys/kernel.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/sockio.h>
#include <sys/timeout.h>

#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#include <dev/mii/miivar.h>

#include <dev/pci/pcidevs.h>
#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>

#include <dev/pci/if_sereg.h>

#define SE_RX_RING_CNT		256 /* [8, 1024] */
#define SE_TX_RING_CNT		256 /* [8, 8192] */
#define	SE_RX_BUF_ALIGN		sizeof(uint64_t)

#define SE_RX_RING_SZ		(SE_RX_RING_CNT * sizeof(struct se_desc))
#define SE_TX_RING_SZ		(SE_TX_RING_CNT * sizeof(struct se_desc))

struct se_list_data {
	struct se_desc		*se_rx_ring;
	struct se_desc		*se_tx_ring;
	bus_dmamap_t		se_rx_dmamap;
	bus_dmamap_t		se_tx_dmamap;
};

struct se_chain_data {
	struct mbuf		*se_rx_mbuf[SE_RX_RING_CNT];
	struct mbuf		*se_tx_mbuf[SE_TX_RING_CNT];
	bus_dmamap_t		se_rx_map[SE_RX_RING_CNT];
	bus_dmamap_t		se_tx_map[SE_TX_RING_CNT];
	uint			se_rx_prod;
	uint			se_tx_prod;
	uint			se_tx_cons;
	uint			se_tx_cnt;
};

struct se_softc {
    	struct device		 sc_dev;
	void			*sc_ih;
	bus_space_tag_t		 sc_iot;
	bus_space_handle_t	 sc_ioh;
	bus_dma_tag_t		 sc_dmat;

	struct mii_data		 sc_mii;
	struct arpcom		 sc_ac;

	struct se_list_data	 se_ldata;
	struct se_chain_data	 se_cdata;

	struct timeout		 sc_tick_tmo;

	int			 sc_flags;
#define	SE_FLAG_FASTETHER	0x0001
#define	SE_FLAG_RGMII		0x0010
#define	SE_FLAG_LINK		0x8000
};

/*
 * Various supported device vendors/types and their names.
 */
const struct pci_matchid se_devices[] = {
	{ PCI_VENDOR_SIS, PCI_PRODUCT_SIS_190 },
	{ PCI_VENDOR_SIS, PCI_PRODUCT_SIS_191 }
};

int	se_match(struct device *, void *, void *);
void	se_attach(struct device *, struct device *, void *);
int	se_activate(struct device *, int);

const struct cfattach se_ca = {
	sizeof(struct se_softc),
	se_match, se_attach, NULL, se_activate
};

struct cfdriver se_cd = {
	0, "se", DV_IFNET
};

uint32_t
	se_miibus_cmd(struct se_softc *, uint32_t);
int	se_miibus_readreg(struct device *, int, int);
void	se_miibus_writereg(struct device *, int, int, int);
void	se_miibus_statchg(struct device *);

int	se_newbuf(struct se_softc *, uint);
void	se_discard_rxbuf(struct se_softc *, uint);
int	se_encap(struct se_softc *, struct mbuf *, uint *);
void	se_rxeof(struct se_softc *);
void	se_txeof(struct se_softc *);
int	se_intr(void *);
void	se_tick(void *);
void	se_start(struct ifnet *);
int	se_ioctl(struct ifnet *, u_long, caddr_t);
int	se_init(struct ifnet *);
void	se_stop(struct se_softc *);
void	se_watchdog(struct ifnet *);
int	se_ifmedia_upd(struct ifnet *);
void	se_ifmedia_sts(struct ifnet *, struct ifmediareq *);

int	se_pcib_match(struct pci_attach_args *);
int	se_get_mac_addr_apc(struct se_softc *, uint8_t *);
int	se_get_mac_addr_eeprom(struct se_softc *, uint8_t *);
uint16_t
	se_read_eeprom(struct se_softc *, int);

void	se_iff(struct se_softc *);
void	se_reset(struct se_softc *);
int	se_list_rx_init(struct se_softc *);
int	se_list_rx_free(struct se_softc *);
int	se_list_tx_init(struct se_softc *);
int	se_list_tx_free(struct se_softc *);

/*
 * Register space access macros.
 */

#define	CSR_WRITE_4(sc, reg, val) \
	bus_space_write_4((sc)->sc_iot, (sc)->sc_ioh, reg, val)
#define	CSR_WRITE_2(sc, reg, val) \
	bus_space_write_2((sc)->sc_iot, (sc)->sc_ioh, reg, val)
#define	CSR_WRITE_1(sc, reg, val) \
	bus_space_write_1((sc)->sc_iot, (sc)->sc_ioh, reg, val)

#define	CSR_READ_4(sc, reg) \
	bus_space_read_4((sc)->sc_iot, (sc)->sc_ioh, reg)
#define	CSR_READ_2(sc, reg) \
	bus_space_read_2((sc)->sc_iot, (sc)->sc_ioh, reg)
#define	CSR_READ_1(sc, reg) \
	bus_space_read_1((sc)->sc_iot, (sc)->sc_ioh, reg)

/*
 * Read a sequence of words from the EEPROM.
 */
uint16_t
se_read_eeprom(struct se_softc *sc, int offset)
{
	uint32_t val;
	int i;

	KASSERT(offset <= EI_OFFSET);

	CSR_WRITE_4(sc, ROMInterface,
	    EI_REQ | EI_OP_RD | (offset << EI_OFFSET_SHIFT));
	DELAY(500);
	for (i = 0; i < SE_TIMEOUT; i++) {
		val = CSR_READ_4(sc, ROMInterface);
		if ((val & EI_REQ) == 0)
			break;
		DELAY(100);
	}
	if (i == SE_TIMEOUT) {
		printf("%s: EEPROM read timeout: 0x%08x\n",
		    sc->sc_dev.dv_xname, val);
		return 0xffff;
	}

	return (val & EI_DATA) >> EI_DATA_SHIFT;
}

int
se_get_mac_addr_eeprom(struct se_softc *sc, uint8_t *dest)
{
	uint16_t val;
	int i;

	val = se_read_eeprom(sc, EEPROMSignature);
	if (val == 0xffff || val == 0x0000) {
		printf("%s: invalid EEPROM signature : 0x%04x\n",
		    sc->sc_dev.dv_xname, val);
		return (EINVAL);
	}

	for (i = 0; i < ETHER_ADDR_LEN; i += 2) {
		val = se_read_eeprom(sc, EEPROMMACAddr + i / 2);
		dest[i + 0] = (uint8_t)val;
		dest[i + 1] = (uint8_t)(val >> 8);
	}

	if ((se_read_eeprom(sc, EEPROMInfo) & 0x80) != 0)
		sc->sc_flags |= SE_FLAG_RGMII;
	return (0);
}

/*
 * For SiS96x, APC CMOS RAM is used to store Ethernet address.
 * APC CMOS RAM is accessed through ISA bridge.
 */
#if defined(__amd64__) || defined(__i386__)
int
se_pcib_match(struct pci_attach_args *pa)
{
	const struct pci_matchid apc_devices[] = {
		{ PCI_VENDOR_SIS, PCI_PRODUCT_SIS_965 },
		{ PCI_VENDOR_SIS, PCI_PRODUCT_SIS_966 },
		{ PCI_VENDOR_SIS, PCI_PRODUCT_SIS_968 }
	};

	return pci_matchbyid(pa, apc_devices, nitems(apc_devices));
}
#endif

int
se_get_mac_addr_apc(struct se_softc *sc, uint8_t *dest)
{
#if defined(__amd64__) || defined(__i386__)
	struct pci_attach_args pa;
	pcireg_t reg;
	bus_space_handle_t ioh;
	int rc, i;

	if (pci_find_device(&pa, se_pcib_match) == 0) {
		printf("\n%s: couldn't find PCI-ISA bridge\n",
		    sc->sc_dev.dv_xname);
		return EINVAL;
	}

	/* Enable port 0x78 and 0x79 to access APC registers. */
	reg = pci_conf_read(pa.pa_pc, pa.pa_tag, 0x48);
	pci_conf_write(pa.pa_pc, pa.pa_tag, 0x48, reg & ~0x02);
	DELAY(50);
	(void)pci_conf_read(pa.pa_pc, pa.pa_tag, 0x48);

	/* XXX this abuses bus_space implementation knowledge */
	rc = _bus_space_map(pa.pa_iot, 0x78, 2, 0, &ioh);
	if (rc == 0) {
		/* Read stored Ethernet address. */
		for (i = 0; i < ETHER_ADDR_LEN; i++) {
			bus_space_write_1(pa.pa_iot, ioh, 0, 0x09 + i);
			dest[i] = bus_space_read_1(pa.pa_iot, ioh, 1);
		}
		bus_space_write_1(pa.pa_iot, ioh, 0, 0x12);
		if ((bus_space_read_1(pa.pa_iot, ioh, 1) & 0x80) != 0)
			sc->sc_flags |= SE_FLAG_RGMII;
		_bus_space_unmap(pa.pa_iot, ioh, 2, NULL);
	} else
		rc = EINVAL;

	/* Restore access to APC registers. */
	pci_conf_write(pa.pa_pc, pa.pa_tag, 0x48, reg);

	return rc;
#endif
	return EINVAL;
}

uint32_t
se_miibus_cmd(struct se_softc *sc, uint32_t ctrl)
{
	int i;
	uint32_t val;

	CSR_WRITE_4(sc, GMIIControl, ctrl);
	DELAY(10);
	for (i = 0; i < SE_TIMEOUT; i++) {
		val = CSR_READ_4(sc, GMIIControl);
		if ((val & GMI_REQ) == 0)
			return val;
		DELAY(10);
	}

	return GMI_REQ;
}

int
se_miibus_readreg(struct device *self, int phy, int reg)
{
	struct se_softc *sc = (struct se_softc *)self;
	uint32_t ctrl, val;

	ctrl = (phy << GMI_PHY_SHIFT) | (reg << GMI_REG_SHIFT) |
	    GMI_OP_RD | GMI_REQ;
	val = se_miibus_cmd(sc, ctrl);
	if ((val & GMI_REQ) != 0) {
		printf("%s: PHY read timeout : %d\n",
		    sc->sc_dev.dv_xname, reg);
		return 0;
	}
	return (val & GMI_DATA) >> GMI_DATA_SHIFT;
}

void
se_miibus_writereg(struct device *self, int phy, int reg, int data)
{
	struct se_softc *sc = (struct se_softc *)self;
	uint32_t ctrl, val;

	ctrl = (phy << GMI_PHY_SHIFT) | (reg << GMI_REG_SHIFT) |
	    GMI_OP_WR | (data << GMI_DATA_SHIFT) | GMI_REQ;
	val = se_miibus_cmd(sc, ctrl);
	if ((val & GMI_REQ) != 0) {
		printf("%s: PHY write timeout : %d\n",
		    sc->sc_dev.dv_xname, reg);
	}
}

void
se_miibus_statchg(struct device *self)
{
	struct se_softc *sc = (struct se_softc *)self;
#ifdef SE_DEBUG
	struct ifnet *ifp = &sc->sc_ac.ac_if;
#endif
	struct mii_data *mii = &sc->sc_mii;
	uint32_t ctl, speed;

	speed = 0;
	sc->sc_flags &= ~SE_FLAG_LINK;
	if ((mii->mii_media_status & (IFM_ACTIVE | IFM_AVALID)) ==
	    (IFM_ACTIVE | IFM_AVALID)) {
		switch (IFM_SUBTYPE(mii->mii_media_active)) {
		case IFM_10_T:
#ifdef SE_DEBUG
			if (ifp->if_flags & IFF_DEBUG)
				printf("%s: 10baseT link\n", ifp->if_xname);
#endif
			sc->sc_flags |= SE_FLAG_LINK;
			speed = SC_SPEED_10;
			break;
		case IFM_100_TX:
#ifdef SE_DEBUG
			if (ifp->if_flags & IFF_DEBUG)
				printf("%s: 100baseTX link\n", ifp->if_xname);
#endif
			sc->sc_flags |= SE_FLAG_LINK;
			speed = SC_SPEED_100;
			break;
		case IFM_1000_T:
#ifdef SE_DEBUG
			if (ifp->if_flags & IFF_DEBUG)
				printf("%s: 1000baseT link\n", ifp->if_xname);
#endif
			if ((sc->sc_flags & SE_FLAG_FASTETHER) == 0) {
				sc->sc_flags |= SE_FLAG_LINK;
				speed = SC_SPEED_1000;
			}
			break;
		default:
			break;
		}
	}
	if ((sc->sc_flags & SE_FLAG_LINK) == 0) {
#ifdef SE_DEBUG
		if (ifp->if_flags & IFF_DEBUG)
			printf("%s: no link\n", ifp->if_xname);
#endif
		return;
	}
	/* Reprogram MAC to resolved speed/duplex/flow-control paramters. */
	ctl = CSR_READ_4(sc, StationControl);
	ctl &= ~(0x0f000000 | SC_FDX | SC_SPEED_MASK);
	if (speed == SC_SPEED_1000)
		ctl |= 0x07000000;
	else
		ctl |= 0x04000000;
#ifdef notyet
	if ((sc->sc_flags & SE_FLAG_GMII) != 0)
		ctl |= 0x03000000;
#endif
	ctl |= speed;
	if ((IFM_OPTIONS(mii->mii_media_active) & IFM_FDX) != 0)
		ctl |= SC_FDX;
	CSR_WRITE_4(sc, StationControl, ctl);
	if ((sc->sc_flags & SE_FLAG_RGMII) != 0) {
		CSR_WRITE_4(sc, RGMIIDelay, 0x0441);
		CSR_WRITE_4(sc, RGMIIDelay, 0x0440);
	}
}

void
se_iff(struct se_softc *sc)
{
	struct arpcom *ac = &sc->sc_ac;
	struct ifnet *ifp = &ac->ac_if;
	struct ether_multi *enm;
	struct ether_multistep step;
	uint32_t crc, hashes[2];
	uint16_t rxfilt;

	rxfilt = CSR_READ_2(sc, RxMacControl);
	rxfilt &= ~(AcceptAllPhys | AcceptBroadcast | AcceptMulticast);
	ifp->if_flags &= ~IFF_ALLMULTI;

	/*
	 * Always accept broadcast frames.
	 * Always accept frames destined to our station address.
	 */
	rxfilt |= AcceptBroadcast | AcceptMyPhys;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0) {
		ifp->if_flags |= IFF_ALLMULTI;
		if (ifp->if_flags & IFF_PROMISC)
			rxfilt |= AcceptAllPhys;
		rxfilt |= AcceptMulticast;
		hashes[0] = hashes[1] = 0xffffffff;
	} else {
		rxfilt |= AcceptMulticast;
		hashes[0] = hashes[1] = 0;

		ETHER_FIRST_MULTI(step, ac, enm);
		while (enm != NULL) {
			crc = ether_crc32_be(enm->enm_addrlo, ETHER_ADDR_LEN);

			hashes[crc >> 31] |= 1 << ((crc >> 26) & 0x1f);

			ETHER_NEXT_MULTI(step, enm);
		}
	}

	CSR_WRITE_2(sc, RxMacControl, rxfilt);
	CSR_WRITE_4(sc, RxHashTable, hashes[0]);
	CSR_WRITE_4(sc, RxHashTable2, hashes[1]);
}

void
se_reset(struct se_softc *sc)
{
	CSR_WRITE_4(sc, IntrMask, 0);
	CSR_WRITE_4(sc, IntrStatus, 0xffffffff);

	/* Soft reset. */
	CSR_WRITE_4(sc, IntrControl, 0x8000);
	CSR_READ_4(sc, IntrControl);
	DELAY(100);
	CSR_WRITE_4(sc, IntrControl, 0);
	/* Stop MAC. */
	CSR_WRITE_4(sc, TX_CTL, 0x1a00);
	CSR_WRITE_4(sc, RX_CTL, 0x1a00);

	CSR_WRITE_4(sc, IntrMask, 0);
	CSR_WRITE_4(sc, IntrStatus, 0xffffffff);

	CSR_WRITE_4(sc, GMIIControl, 0);
}

/*
 * Probe for an SiS chip. Check the PCI vendor and device
 * IDs against our list and return a device name if we find a match.
 */
int
se_match(struct device *parent, void *match, void *aux)
{
	struct pci_attach_args *pa = (struct pci_attach_args *)aux;

	return pci_matchbyid(pa, se_devices, nitems(se_devices));
}

/*
 * Attach the interface. Do ifmedia setup and ethernet/BPF attach.
 */
void
se_attach(struct device *parent, struct device *self, void *aux)
{
	struct se_softc *sc = (struct se_softc *)self;
	struct arpcom *ac = &sc->sc_ac;
	struct ifnet *ifp = &ac->ac_if;
	struct pci_attach_args *pa = (struct pci_attach_args *)aux;
	uint8_t eaddr[ETHER_ADDR_LEN];
	const char *intrstr;
	pci_intr_handle_t ih;
	bus_size_t iosize;
	bus_dma_segment_t seg;
	struct se_list_data *ld;
	struct se_chain_data *cd;
	int nseg;
	uint i;
	int rc;

	printf(": ");

	/*
	 * Map control/status registers.
	 */

	rc = pci_mapreg_map(pa, PCI_MAPREG_START, PCI_MAPREG_TYPE_MEM, 0,
	    &sc->sc_iot, &sc->sc_ioh, NULL, &iosize, 0);
	if (rc != 0) {
		printf("can't map i/o space\n");
		return;
	}

	if (pci_intr_map(pa, &ih)) {
		printf("can't map interrupt\n");
		goto fail1;
	}
	intrstr = pci_intr_string(pa->pa_pc, ih);
	sc->sc_ih = pci_intr_establish(pa->pa_pc, ih, IPL_NET, se_intr, sc,
	    self->dv_xname);
	if (sc->sc_ih == NULL) {
		printf("can't establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		goto fail1;
	}

	printf("%s", intrstr);

	if (pa->pa_id == PCI_ID_CODE(PCI_VENDOR_SIS, PCI_PRODUCT_SIS_190))
		sc->sc_flags |= SE_FLAG_FASTETHER;

	/* Reset the adapter. */
	se_reset(sc);

	/* Get MAC address from the EEPROM. */
	if ((pci_conf_read(pa->pa_pc, pa->pa_tag, 0x70) & (0x01 << 24)) != 0)
		se_get_mac_addr_apc(sc, eaddr);
	else
		se_get_mac_addr_eeprom(sc, eaddr);
	printf(", address %s\n", ether_sprintf(eaddr));
	bcopy(eaddr, ac->ac_enaddr, ETHER_ADDR_LEN);

	/*
	 * Now do all the DMA mapping stuff
	 */

	sc->sc_dmat = pa->pa_dmat;
	ld = &sc->se_ldata;
	cd = &sc->se_cdata;

	/* First create TX/RX busdma maps. */
	for (i = 0; i < SE_RX_RING_CNT; i++) {
		rc = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1, MCLBYTES,
		    0, BUS_DMA_NOWAIT, &cd->se_rx_map[i]);
		if (rc != 0) {
			printf("%s: cannot init the RX map array\n",
			    self->dv_xname);
			goto fail2;
		}
	}

	for (i = 0; i < SE_TX_RING_CNT; i++) {
		rc = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1, MCLBYTES,
		    0, BUS_DMA_NOWAIT, &cd->se_tx_map[i]);
		if (rc != 0) {
			printf("%s: cannot init the TX map array\n",
			    self->dv_xname);
			goto fail2;
		}
	}

	/*
	 * Now allocate a chunk of DMA-able memory for RX and TX ring
	 * descriptors, as a contiguous block of memory.
	 * XXX fix deallocation upon error
	 */

	/* RX */
	rc = bus_dmamem_alloc(sc->sc_dmat, SE_RX_RING_SZ, PAGE_SIZE, 0,
	    &seg, 1, &nseg, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: no memory for RX descriptors\n", self->dv_xname);
		goto fail2;
	}

	rc = bus_dmamem_map(sc->sc_dmat, &seg, nseg, SE_RX_RING_SZ,
	    (caddr_t *)&ld->se_rx_ring, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: can't map RX descriptors\n", self->dv_xname);
		goto fail2;
	}

	rc = bus_dmamap_create(sc->sc_dmat, SE_RX_RING_SZ, 1,
	    SE_RX_RING_SZ, 0, BUS_DMA_NOWAIT, &ld->se_rx_dmamap);
	if (rc != 0) {
		printf("%s: can't alloc RX DMA map\n", self->dv_xname);
		goto fail2;
	}

	rc = bus_dmamap_load(sc->sc_dmat, ld->se_rx_dmamap,
	    (caddr_t)ld->se_rx_ring, SE_RX_RING_SZ, NULL, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: can't load RX DMA map\n", self->dv_xname);
		bus_dmamem_unmap(sc->sc_dmat,
		    (caddr_t)ld->se_rx_ring, SE_RX_RING_SZ);
		bus_dmamap_destroy(sc->sc_dmat, ld->se_rx_dmamap);
		bus_dmamem_free(sc->sc_dmat, &seg, nseg);
		goto fail2;
	}

	/* TX */
	rc = bus_dmamem_alloc(sc->sc_dmat, SE_TX_RING_SZ, PAGE_SIZE, 0,
	    &seg, 1, &nseg, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: no memory for TX descriptors\n", self->dv_xname);
		goto fail2;
	}

	rc = bus_dmamem_map(sc->sc_dmat, &seg, nseg, SE_TX_RING_SZ,
	    (caddr_t *)&ld->se_tx_ring, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: can't map TX descriptors\n", self->dv_xname);
		goto fail2;
	}

	rc = bus_dmamap_create(sc->sc_dmat, SE_TX_RING_SZ, 1,
	    SE_TX_RING_SZ, 0, BUS_DMA_NOWAIT, &ld->se_tx_dmamap);
	if (rc != 0) {
		printf("%s: can't alloc TX DMA map\n", self->dv_xname);
		goto fail2;
	}

	rc = bus_dmamap_load(sc->sc_dmat, ld->se_tx_dmamap,
	    (caddr_t)ld->se_tx_ring, SE_TX_RING_SZ, NULL, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: can't load TX DMA map\n", self->dv_xname);
		bus_dmamem_unmap(sc->sc_dmat,
		    (caddr_t)ld->se_tx_ring, SE_TX_RING_SZ);
		bus_dmamap_destroy(sc->sc_dmat, ld->se_tx_dmamap);
		bus_dmamem_free(sc->sc_dmat, &seg, nseg);
		goto fail2;
	}

	timeout_set(&sc->sc_tick_tmo, se_tick, sc);

	ifp = &sc->sc_ac.ac_if;
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = se_ioctl;
	ifp->if_start = se_start;
	ifp->if_watchdog = se_watchdog;
	IFQ_SET_MAXLEN(&ifp->if_snd, SE_TX_RING_CNT - 1);
	bcopy(sc->sc_dev.dv_xname, ifp->if_xname, IFNAMSIZ);

	ifp->if_capabilities = IFCAP_VLAN_MTU;

	/*
	 * Do MII setup.
	 */

	sc->sc_mii.mii_ifp = ifp;
	sc->sc_mii.mii_readreg = se_miibus_readreg;
	sc->sc_mii.mii_writereg = se_miibus_writereg;
	sc->sc_mii.mii_statchg = se_miibus_statchg;
	ifmedia_init(&sc->sc_mii.mii_media, 0, se_ifmedia_upd,
	    se_ifmedia_sts);
	mii_attach(&sc->sc_dev, &sc->sc_mii, 0xffffffff, MII_PHY_ANY,
	    MII_OFFSET_ANY, 0);

	if (LIST_FIRST(&sc->sc_mii.mii_phys) == NULL) {
		/* No PHY attached */
		ifmedia_add(&sc->sc_mii.mii_media, IFM_ETHER | IFM_MANUAL,
		    0, NULL);
		ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER | IFM_MANUAL);
	} else
		ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER | IFM_AUTO);

	/*
	 * Call MI attach routine.
	 */
	if_attach(ifp);
	ether_ifattach(ifp);

	return;

fail2:
	pci_intr_disestablish(pa->pa_pc, sc->sc_ih);
fail1:
	bus_space_unmap(sc->sc_iot, sc->sc_ioh, iosize);
}

int
se_activate(struct device *self, int act)
{
	struct se_softc *sc = (struct se_softc *)self;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int rv = 0;

	switch (act) {
	case DVACT_SUSPEND:
		if (ifp->if_flags & IFF_RUNNING)
			se_stop(sc);
		rv = config_activate_children(self, act);
		break;
	case DVACT_RESUME:
		if (ifp->if_flags & IFF_UP)
			(void)se_init(ifp);
		break;
	default:
		rv = config_activate_children(self, act);
		break;
	}

	return (rv);
}

/*
 * Initialize the TX descriptors.
 */
int
se_list_tx_init(struct se_softc *sc)
{
	struct se_list_data *ld = &sc->se_ldata;
	struct se_chain_data *cd = &sc->se_cdata;

	bzero(ld->se_tx_ring, SE_TX_RING_SZ);
	ld->se_tx_ring[SE_TX_RING_CNT - 1].se_flags = htole32(RING_END);
	bus_dmamap_sync(sc->sc_dmat, ld->se_tx_dmamap, 0, SE_TX_RING_SZ,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	cd->se_tx_prod = 0;
	cd->se_tx_cons = 0;
	cd->se_tx_cnt = 0;

	return 0;
}

int
se_list_tx_free(struct se_softc *sc)
{
	struct se_chain_data *cd = &sc->se_cdata;
	uint i;

	for (i = 0; i < SE_TX_RING_CNT; i++) {
		if (cd->se_tx_mbuf[i] != NULL) {
			bus_dmamap_unload(sc->sc_dmat, cd->se_tx_map[i]);
			m_free(cd->se_tx_mbuf[i]);
			cd->se_tx_mbuf[i] = NULL;
		}
	}

	return 0;
}

/*
 * Initialize the RX descriptors and allocate mbufs for them.
 */
int
se_list_rx_init(struct se_softc *sc)
{
	struct se_list_data *ld = &sc->se_ldata;
	struct se_chain_data *cd = &sc->se_cdata;
	uint i;

	bzero(ld->se_rx_ring, SE_RX_RING_SZ);
	bus_dmamap_sync(sc->sc_dmat, ld->se_rx_dmamap, 0, SE_RX_RING_SZ,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	for (i = 0; i < SE_RX_RING_CNT; i++) {
		if (se_newbuf(sc, i) != 0)
			return ENOBUFS;
	}

	cd->se_rx_prod = 0;

	return 0;
}

int
se_list_rx_free(struct se_softc *sc)
{
	struct se_chain_data *cd = &sc->se_cdata;
	uint i;

	for (i = 0; i < SE_RX_RING_CNT; i++) {
		if (cd->se_rx_mbuf[i] != NULL) {
			bus_dmamap_unload(sc->sc_dmat, cd->se_rx_map[i]);
			m_free(cd->se_rx_mbuf[i]);
			cd->se_rx_mbuf[i] = NULL;
		}
	}

	return 0;
}

/*
 * Initialize an RX descriptor and attach an MBUF cluster.
 */
int
se_newbuf(struct se_softc *sc, uint i)
{
#ifdef SE_DEBUG
	struct ifnet *ifp = &sc->sc_ac.ac_if;
#endif
	struct se_list_data *ld = &sc->se_ldata;
	struct se_chain_data *cd = &sc->se_cdata;
	struct se_desc *desc;
	struct mbuf *m;
	int rc;

	m = MCLGETI(NULL, M_DONTWAIT, NULL, MCLBYTES);
	if (m == NULL) {
#ifdef SE_DEBUG
		if (ifp->if_flags & IFF_DEBUG)
			printf("%s: MCLGETI failed\n", ifp->if_xname);
#endif
		return ENOBUFS;
	}
	m->m_len = m->m_pkthdr.len = MCLBYTES;
	m_adj(m, SE_RX_BUF_ALIGN);

	rc = bus_dmamap_load_mbuf(sc->sc_dmat, cd->se_rx_map[i],
	    m, BUS_DMA_NOWAIT);
	KASSERT(cd->se_rx_map[i]->dm_nsegs == 1);
	if (rc != 0) {
		m_freem(m);
		return ENOBUFS;
	}
	bus_dmamap_sync(sc->sc_dmat, cd->se_rx_map[i], 0,
	    cd->se_rx_map[i]->dm_mapsize, BUS_DMASYNC_PREREAD);

	cd->se_rx_mbuf[i] = m;
	desc = &ld->se_rx_ring[i];
	desc->se_sts_size = 0;
	desc->se_cmdsts = htole32(RDC_OWN | RDC_INTR);
	desc->se_ptr = htole32((uint32_t)cd->se_rx_map[i]->dm_segs[0].ds_addr);
	desc->se_flags = htole32(cd->se_rx_map[i]->dm_segs[0].ds_len);
	if (i == SE_RX_RING_CNT - 1)
		desc->se_flags |= htole32(RING_END);
	bus_dmamap_sync(sc->sc_dmat, ld->se_rx_dmamap, i * sizeof(*desc),
	    sizeof(*desc), BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	return 0;
}

void
se_discard_rxbuf(struct se_softc *sc, uint i)
{
	struct se_list_data *ld = &sc->se_ldata;
	struct se_desc *desc;

	desc = &ld->se_rx_ring[i];
	desc->se_sts_size = 0;
	desc->se_cmdsts = htole32(RDC_OWN | RDC_INTR);
	desc->se_flags = htole32(MCLBYTES - SE_RX_BUF_ALIGN);
	if (i == SE_RX_RING_CNT - 1)
		desc->se_flags |= htole32(RING_END);
	bus_dmamap_sync(sc->sc_dmat, ld->se_rx_dmamap, i * sizeof(*desc),
	    sizeof(*desc), BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
}

/*
 * A frame has been uploaded: pass the resulting mbuf chain up to
 * the higher level protocols.
 */
void
se_rxeof(struct se_softc *sc)
{
	struct mbuf *m;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct se_list_data *ld = &sc->se_ldata;
	struct se_chain_data *cd = &sc->se_cdata;
	struct se_desc *cur_rx;
	uint32_t rxinfo, rxstat;
	uint i;

	bus_dmamap_sync(sc->sc_dmat, ld->se_rx_dmamap, 0, SE_RX_RING_SZ,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
	for (i = cd->se_rx_prod; ; SE_INC(i, SE_RX_RING_CNT)) {
		cur_rx = &ld->se_rx_ring[i];
		rxinfo = letoh32(cur_rx->se_cmdsts);
		if ((rxinfo & RDC_OWN) != 0)
			break;
		rxstat = letoh32(cur_rx->se_sts_size);

		/*
		 * If an error occurs, update stats, clear the
		 * status word and leave the mbuf cluster in place:
		 * it should simply get re-used next time this descriptor
		 * comes up in the ring.
		 */
		if ((rxstat & RDS_CRCOK) == 0 || SE_RX_ERROR(rxstat) != 0 ||
		    SE_RX_NSEGS(rxstat) != 1) {
			/* XXX We don't support multi-segment frames yet. */
			if (ifp->if_flags & IFF_DEBUG)
				printf("%s: rx error %b\n",
				    ifp->if_xname, rxstat, RX_ERR_BITS);
			se_discard_rxbuf(sc, i);
			ifp->if_ierrors++;
			continue;
		}

		/* No errors; receive the packet. */
		bus_dmamap_sync(sc->sc_dmat, cd->se_rx_map[i], 0,
		    cd->se_rx_map[i]->dm_mapsize, BUS_DMASYNC_POSTREAD);
		m = cd->se_rx_mbuf[i];
		if (se_newbuf(sc, i) != 0) {
			se_discard_rxbuf(sc, i);
			ifp->if_iqdrops++;
			continue;
		}
		/*
		 * Account for 10 bytes auto padding which is used
		 * to align IP header on a 32bit boundary.  Also note,
		 * CRC bytes are automatically removed by the hardware.
		 */
		m->m_data += SE_RX_PAD_BYTES;
		m->m_pkthdr.len = m->m_len =
		    SE_RX_BYTES(rxstat) - SE_RX_PAD_BYTES;

		ml_enqueue(&ml, m);
	}

	if_input(ifp, &ml);

	cd->se_rx_prod = i;
}

/*
 * A frame was downloaded to the chip. It's safe for us to clean up
 * the list buffers.
 */

void
se_txeof(struct se_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct se_list_data *ld = &sc->se_ldata;
	struct se_chain_data *cd = &sc->se_cdata;
	struct se_desc *cur_tx;
	uint32_t txstat;
	uint i;

	/*
	 * Go through our tx list and free mbufs for those
	 * frames that have been transmitted.
	 */
	bus_dmamap_sync(sc->sc_dmat, ld->se_tx_dmamap, 0, SE_TX_RING_SZ,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
	for (i = cd->se_tx_cons; cd->se_tx_cnt > 0;
	    cd->se_tx_cnt--, SE_INC(i, SE_TX_RING_CNT)) {
		cur_tx = &ld->se_tx_ring[i];
		txstat = letoh32(cur_tx->se_cmdsts);
		if ((txstat & TDC_OWN) != 0)
			break;

		ifq_clr_oactive(&ifp->if_snd);

		if (SE_TX_ERROR(txstat) != 0) {
			if (ifp->if_flags & IFF_DEBUG)
				printf("%s: tx error %b\n",
				    ifp->if_xname, txstat, TX_ERR_BITS);
			ifp->if_oerrors++;
			/* TODO: better error differentiation */
		}

		if (cd->se_tx_mbuf[i] != NULL) {
			bus_dmamap_sync(sc->sc_dmat, cd->se_tx_map[i], 0,
			    cd->se_tx_map[i]->dm_mapsize,
			    BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, cd->se_tx_map[i]);
			m_free(cd->se_tx_mbuf[i]);
			cd->se_tx_mbuf[i] = NULL;
		}

		cur_tx->se_sts_size = 0;
		cur_tx->se_cmdsts = 0;
		cur_tx->se_ptr = 0;
		cur_tx->se_flags &= htole32(RING_END);
		bus_dmamap_sync(sc->sc_dmat, ld->se_tx_dmamap,
		    i * sizeof(*cur_tx), sizeof(*cur_tx),
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	}

	cd->se_tx_cons = i;
	if (cd->se_tx_cnt == 0)
		ifp->if_timer = 0;
}

void
se_tick(void *xsc)
{
	struct se_softc *sc = xsc;
	struct mii_data *mii;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int s;

	s = splnet();
	mii = &sc->sc_mii;
	mii_tick(mii);
	if ((sc->sc_flags & SE_FLAG_LINK) == 0) {
		se_miibus_statchg(&sc->sc_dev);
		if ((sc->sc_flags & SE_FLAG_LINK) != 0 &&
		    !IFQ_IS_EMPTY(&ifp->if_snd))
			se_start(ifp);
	}
	splx(s);

	timeout_add_sec(&sc->sc_tick_tmo, 1);
}

int
se_intr(void *arg)
{
	struct se_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	uint32_t status;

	status = CSR_READ_4(sc, IntrStatus);
	if (status == 0xffffffff || (status & SE_INTRS) == 0) {
		/* Not ours. */
		return 0;
	}
	/* Ack interrupts/ */
	CSR_WRITE_4(sc, IntrStatus, status);
	/* Disable further interrupts. */
	CSR_WRITE_4(sc, IntrMask, 0);

	for (;;) {
		if ((ifp->if_flags & IFF_RUNNING) == 0)
			break;
		if ((status & (INTR_RX_DONE | INTR_RX_IDLE)) != 0) {
			se_rxeof(sc);
			/* Wakeup Rx MAC. */
			if ((status & INTR_RX_IDLE) != 0)
				CSR_WRITE_4(sc, RX_CTL,
				    0x1a00 | 0x000c | RX_CTL_POLL | RX_CTL_ENB);
		}
		if ((status & (INTR_TX_DONE | INTR_TX_IDLE)) != 0)
			se_txeof(sc);
		status = CSR_READ_4(sc, IntrStatus);
		if ((status & SE_INTRS) == 0)
			break;
		/* Ack interrupts. */
		CSR_WRITE_4(sc, IntrStatus, status);
	}

	if ((ifp->if_flags & IFF_RUNNING) != 0) {
		/* Re-enable interrupts */
		CSR_WRITE_4(sc, IntrMask, SE_INTRS);
		if (!IFQ_IS_EMPTY(&ifp->if_snd))
			se_start(ifp);
	}

	return 1;
}

/*
 * Encapsulate an mbuf chain in a descriptor by coupling the mbuf data
 * pointers to the fragment pointers.
 */
int
se_encap(struct se_softc *sc, struct mbuf *m_head, uint32_t *txidx)
{
#ifdef SE_DEBUG
	struct ifnet *ifp = &sc->sc_ac.ac_if;
#endif
	struct mbuf *m;
	struct se_list_data *ld = &sc->se_ldata;
	struct se_chain_data *cd = &sc->se_cdata;
	struct se_desc *desc;
	uint i, cnt = 0;
	int rc;

	/*
	 * If there's no way we can send any packets, return now.
	 */
	if (SE_TX_RING_CNT - cd->se_tx_cnt < 2) {
#ifdef SE_DEBUG
		if (ifp->if_flags & IFF_DEBUG)
			printf("%s: encap failed, not enough TX desc\n",
			    ifp->if_xname);
#endif
		return ENOBUFS;
	}

	if (m_defrag(m_head, M_DONTWAIT) != 0) {
#ifdef SE_DEBUG
		if (ifp->if_flags & IFF_DEBUG)
			printf("%s: m_defrag failed\n", ifp->if_xname);
#endif
		return ENOBUFS;	/* XXX should not be fatal */
	}

	/*
	 * Start packing the mbufs in this chain into
	 * the fragment pointers. Stop when we run out
	 * of fragments or hit the end of the mbuf chain.
	 */
	i = *txidx;

	for (m = m_head; m != NULL; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		if ((SE_TX_RING_CNT - (cd->se_tx_cnt + cnt)) < 2) {
#ifdef SE_DEBUG
			if (ifp->if_flags & IFF_DEBUG)
				printf("%s: encap failed, not enough TX desc\n",
				    ifp->if_xname);
#endif
			return ENOBUFS;
		}
		cd->se_tx_mbuf[i] = m;
		rc = bus_dmamap_load_mbuf(sc->sc_dmat, cd->se_tx_map[i],
		    m, BUS_DMA_NOWAIT);
		if (rc != 0)
			return ENOBUFS;
		KASSERT(cd->se_tx_map[i]->dm_nsegs == 1);
		bus_dmamap_sync(sc->sc_dmat, cd->se_tx_map[i], 0,
		    cd->se_tx_map[i]->dm_mapsize, BUS_DMASYNC_PREWRITE);

		desc = &ld->se_tx_ring[i];
		desc->se_sts_size = htole32(cd->se_tx_map[i]->dm_segs->ds_len);
		desc->se_ptr =
		    htole32((uint32_t)cd->se_tx_map[i]->dm_segs->ds_addr);
		desc->se_flags = htole32(cd->se_tx_map[i]->dm_segs->ds_len);
		if (i == SE_TX_RING_CNT - 1)
			desc->se_flags |= htole32(RING_END);
		desc->se_cmdsts = htole32(TDC_OWN | TDC_INTR | TDC_DEF |
		    TDC_CRC | TDC_PAD | TDC_BST);
		bus_dmamap_sync(sc->sc_dmat, ld->se_tx_dmamap,
		    i * sizeof(*desc), sizeof(*desc),
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		SE_INC(i, SE_TX_RING_CNT);
		cnt++;
	}

	/* can't happen */
	if (m != NULL)
		return ENOBUFS;

	cd->se_tx_cnt += cnt;
	*txidx = i;

	return 0;
}

/*
 * Main transmit routine. To avoid having to do mbuf copies, we put pointers
 * to the mbuf data regions directly in the transmit lists. We also save a
 * copy of the pointers since the transmit list fragment pointers are
 * physical addresses.
 */
void
se_start(struct ifnet *ifp)
{
	struct se_softc *sc = ifp->if_softc;
	struct mbuf *m_head = NULL;
	struct se_chain_data *cd = &sc->se_cdata;
	uint i, queued = 0;

	if ((sc->sc_flags & SE_FLAG_LINK) == 0 ||
	    !(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd)) {
#ifdef SE_DEBUG
		if (ifp->if_flags & IFF_DEBUG)
			printf("%s: can't tx, flags 0x%x 0x%04x\n",
			    ifp->if_xname, sc->sc_flags, (uint)ifp->if_flags);
#endif
		return;
	}

	i = cd->se_tx_prod;

	while (cd->se_tx_mbuf[i] == NULL) {
		m_head = ifq_deq_begin(&ifp->if_snd);
		if (m_head == NULL)
			break;

		if (se_encap(sc, m_head, &i) != 0) {
			ifq_deq_rollback(&ifp->if_snd, m_head);
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

		/* now we are committed to transmit the packet */
		ifq_deq_commit(&ifp->if_snd, m_head);
		queued++;

		/*
		 * If there's a BPF listener, bounce a copy of this frame
		 * to him.
		 */
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
#endif
	}

	if (queued > 0) {
		/* Transmit */
		cd->se_tx_prod = i;
		CSR_WRITE_4(sc, TX_CTL, 0x1a00 | TX_CTL_ENB | TX_CTL_POLL);
		ifp->if_timer = 5;
	}
}

int
se_init(struct ifnet *ifp)
{
	struct se_softc *sc = ifp->if_softc;
	uint16_t rxfilt;
	int i;

	splassert(IPL_NET);

	/*
	 * Cancel pending I/O and free all RX/TX buffers.
	 */
	se_stop(sc);
	se_reset(sc);

	/* Init circular RX list. */
	if (se_list_rx_init(sc) == ENOBUFS) {
		se_stop(sc);	/* XXX necessary? */
		return ENOBUFS;
	}

	/* Init TX descriptors. */
	se_list_tx_init(sc);

	/*
	 * Load the address of the RX and TX lists.
	 */
	CSR_WRITE_4(sc, TX_DESC,
	    (uint32_t)sc->se_ldata.se_tx_dmamap->dm_segs[0].ds_addr);
	CSR_WRITE_4(sc, RX_DESC,
	    (uint32_t)sc->se_ldata.se_rx_dmamap->dm_segs[0].ds_addr);

	CSR_WRITE_4(sc, TxMacControl, 0x60);
	CSR_WRITE_4(sc, RxWakeOnLan, 0);
	CSR_WRITE_4(sc, RxWakeOnLanData, 0);
	CSR_WRITE_2(sc, RxMPSControl, ETHER_MAX_LEN + ETHER_VLAN_ENCAP_LEN +
	    SE_RX_PAD_BYTES);

	for (i = 0; i < ETHER_ADDR_LEN; i++)
		CSR_WRITE_1(sc, RxMacAddr + i, sc->sc_ac.ac_enaddr[i]);
	/* Configure RX MAC. */
	rxfilt = RXMAC_STRIP_FCS | RXMAC_PAD_ENB | RXMAC_CSUM_ENB;
	CSR_WRITE_2(sc, RxMacControl, rxfilt);

	/* Program promiscuous mode and multicast filters. */
	se_iff(sc);

	/*
	 * Clear and enable interrupts.
	 */
	CSR_WRITE_4(sc, IntrStatus, 0xFFFFFFFF);
	CSR_WRITE_4(sc, IntrMask, SE_INTRS);

	/* Enable receiver and transmitter. */
	CSR_WRITE_4(sc, TX_CTL, 0x1a00 | TX_CTL_ENB);
	CSR_WRITE_4(sc, RX_CTL, 0x1a00 | 0x000c | RX_CTL_POLL | RX_CTL_ENB);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	sc->sc_flags &= ~SE_FLAG_LINK;
	mii_mediachg(&sc->sc_mii);
	timeout_add_sec(&sc->sc_tick_tmo, 1);

	return 0;
}

/*
 * Set media options.
 */
int
se_ifmedia_upd(struct ifnet *ifp)
{
	struct se_softc *sc = ifp->if_softc;
	struct mii_data *mii;

	mii = &sc->sc_mii;
	sc->sc_flags &= ~SE_FLAG_LINK;
	if (mii->mii_instance) {
		struct mii_softc *miisc;
		LIST_FOREACH(miisc, &mii->mii_phys, mii_list)
			mii_phy_reset(miisc);
	}
	return mii_mediachg(mii);
}

/*
 * Report current media status.
 */
void
se_ifmedia_sts(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct se_softc *sc = ifp->if_softc;
	struct mii_data *mii;

	mii = &sc->sc_mii;
	mii_pollstat(mii);
	ifmr->ifm_active = mii->mii_media_active;
	ifmr->ifm_status = mii->mii_media_status;
}

int
se_ioctl(struct ifnet *ifp, u_long command, caddr_t data)
{
	struct se_softc *sc = ifp->if_softc;
	struct ifreq *ifr = (struct ifreq *) data;
	int s, rc = 0;

	s = splnet();

	switch (command) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if ((ifp->if_flags & IFF_RUNNING) == 0)
			rc = se_init(ifp);
		break;
	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				rc = ENETRESET;
			else
				rc = se_init(ifp);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				se_stop(sc);
		}
		break;
	case SIOCGIFMEDIA:
	case SIOCSIFMEDIA:
		rc = ifmedia_ioctl(ifp, ifr, &sc->sc_mii.mii_media, command);
		break;
	default:
		rc = ether_ioctl(ifp, &sc->sc_ac, command, data);
		break;
	}

	if (rc == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			se_iff(sc);
		rc = 0;
	}

	splx(s);
	return rc;
}

void
se_watchdog(struct ifnet *ifp)
{
	struct se_softc *sc = ifp->if_softc;
	int s;

	printf("%s: watchdog timeout\n", sc->sc_dev.dv_xname);
	ifp->if_oerrors++;

	s = splnet();
	se_init(ifp);
	if (!IFQ_IS_EMPTY(&ifp->if_snd))
		se_start(ifp);
	splx(s);
}

/*
 * Stop the adapter and free any mbufs allocated to the
 * RX and TX lists.
 */
void
se_stop(struct se_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	ifp->if_timer = 0;
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	timeout_del(&sc->sc_tick_tmo);
	mii_down(&sc->sc_mii);

	CSR_WRITE_4(sc, IntrMask, 0);
	CSR_READ_4(sc, IntrMask);
	CSR_WRITE_4(sc, IntrStatus, 0xffffffff);
	/* Stop TX/RX MAC. */
	CSR_WRITE_4(sc, TX_CTL, 0x1a00);
	CSR_WRITE_4(sc, RX_CTL, 0x1a00);
	/* XXX Can we assume active DMA cycles gone? */
	DELAY(2000);
	CSR_WRITE_4(sc, IntrMask, 0);
	CSR_WRITE_4(sc, IntrStatus, 0xffffffff);

	sc->sc_flags &= ~SE_FLAG_LINK;
	se_list_rx_free(sc);
	se_list_tx_free(sc);
}
@


1.19
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.18 2015/11/25 03:09:59 dlg Exp $	*/
d1005 1
a1005 2
		} else
			ifp->if_opackets++;
@


1.18
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.17 2015/11/24 17:11:39 mpi Exp $	*/
a693 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.17
log
@You only need <net/if_dl.h> if you're using LLADDR() or a sockaddr_dl.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.16 2015/11/24 13:33:17 mpi Exp $	*/
d998 1
a998 1
		ifp->if_flags &= ~IFF_OACTIVE;
d1206 1
a1206 1
	    (ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING) {
d1224 1
a1224 1
			ifp->if_flags |= IFF_OACTIVE;
d1308 1
a1308 1
	ifp->if_flags &= ~IFF_OACTIVE;
d1422 2
a1423 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
@


1.16
log
@The only network driver needing <net/if_types.h> is upl(4) for IFT_OTHER.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.15 2015/11/20 03:35:23 dlg Exp $	*/
a66 1
#include <net/if_dl.h>
@


1.15
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.14 2015/10/25 13:04:28 mpi Exp $	*/
a68 1
#include <net/if_types.h>
@


1.14
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.13 2015/06/24 09:40:54 mpi Exp $	*/
d1220 1
a1220 1
		IFQ_POLL(&ifp->if_snd, m_head);
d1225 1
d1231 1
a1231 1
		IFQ_DEQUEUE(&ifp->if_snd, m_head);
@


1.13
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.12 2015/04/30 07:51:07 mpi Exp $	*/
a1356 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a1365 4
		if (rc == 0) {
			if (ifa->ifa_addr->sa_family == AF_INET)
				arp_ifinit(&sc->sc_ac, ifa);
		}
@


1.12
log
@Convert moar drivers to if_input().

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.11 2015/03/14 03:38:48 jsg Exp $	*/
a964 1
		ifp->if_ipackets++;
@


1.11
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.10 2014/12/22 02:28:52 tedu Exp $	*/
d912 1
d964 1
d966 1
a966 1
		m->m_pkthdr.rcvif = ifp;
d968 1
a968 6
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
#endif
		ether_input_mbuf(ifp, m);
	}
@


1.10
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.9 2013/12/28 03:34:54 deraadt Exp $	*/
a77 1
#include <dev/mii/mii.h>
@


1.9
log
@The few network drivers that called their children's (ie. mii PHY
drivers) activate functions at DVACT_RESUME time do not need to do
so, since their PHYs are repaired by IFF_UP.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.8 2013/12/06 21:03:04 deraadt Exp $	*/
a1361 1
#ifdef INET
a1362 1
#endif
a1372 1
#ifdef INET
a1374 1
#endif
@


1.8
log
@Add a DVACT_WAKEUP op to the *_activate() API.  This is called after the
kernel resumes normal (non-cold, able to run processes, etc) operation.
Previously we were relying on specific DVACT_RESUME op's in drivers
creating callback/threads themselves, but that has become too common,
indicating the need for a built-in mechanism.
ok dlg kettenis, tested by a sufficient amount of people
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.7 2012/09/26 19:18:00 brad Exp $	*/
a750 1
		rv = config_activate_children(self, act);
@


1.7
log
@Add support for VLAN sized frames.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.6 2010/09/07 07:54:44 miod Exp $	*/
d742 1
a742 1
	int rc = 0;
d748 1
a748 1
		rc = config_activate_children(self, act);
d751 1
a751 1
		rc = config_activate_children(self, act);
d755 3
d760 1
a760 1
	return rc;
@


1.6
log
@Bring the promiscuous/multicast handling code in line with the current OpenBSD
practice; from brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.5 2010/09/05 12:42:54 miod Exp $	*/
d700 2
d1289 2
a1290 1
	CSR_WRITE_2(sc, RxMPSControl, ETHER_MAX_LEN + SE_RX_PAD_BYTES);
@


1.5
log
@Properly invoke bus_dmamap_sync() around the ring descriptors. No functional
change.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.4 2010/09/04 12:47:00 miod Exp $	*/
d455 12
a466 6
	rxfilt &= ~(AcceptBroadcast | AcceptAllPhys | AcceptMulticast);
	rxfilt |= AcceptMyPhys;
	if ((ifp->if_flags & IFF_BROADCAST) != 0)
		rxfilt |= AcceptBroadcast;
	if ((ifp->if_flags & (IFF_PROMISC | IFF_ALLMULTI)) != 0) {
		if ((ifp->if_flags & IFF_PROMISC) != 0)
d472 2
a473 1
		/* Now program new ones. */
a474 1
		hashes[0] = hashes[1] = 0;
d477 1
d479 1
a1260 3
	if ((ifp->if_flags & IFF_RUNNING) != 0)
		return 0;

d1294 2
a1415 1
	ifp->if_flags &= ~IFF_RUNNING;
@


1.4
log
@Let se(4) support SiS191, and bring a lot of bugfixes and improvements from
FreeBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.3 2010/08/27 17:08:00 jsg Exp $	*/
d761 2
d788 1
a788 3
 * Initialize the RX descriptors and allocate mbufs for them. Note that
 * we arrange the descriptors in a closed ring, so that the last descriptor
 * has RING_END flag set.
d798 2
a804 1
	ld->se_rx_ring[SE_RX_RING_CNT - 1].se_flags |= htole32(RING_END);
d860 2
d871 2
a872 3

	bus_dmamap_sync(sc->sc_dmat, cd->se_rx_map[i], 0,
	    cd->se_rx_map[i]->dm_mapsize, BUS_DMASYNC_PREREAD);
d889 2
d908 2
a915 3
		bus_dmamap_sync(sc->sc_dmat, cd->se_rx_map[i], 0,
		    cd->se_rx_map[i]->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
d935 2
d984 2
d1016 4
a1019 1
		cur_tx->se_flags &= RING_END;
d1157 1
a1157 2
		    cd->se_tx_map[i]->dm_mapsize,
		    BUS_DMASYNC_PREWRITE);
d1168 3
@


1.3
log
@remove the unused if_init callback in struct ifnet
ok deraadt@@ henning@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_se.c,v 1.2 2010/04/02 22:42:55 jsg Exp $	*/
d5 1
a38 2
#include <sys/cdefs.h>

d40 1
a40 1
 * SiS 190 Fast Ethernet PCI NIC driver.
d50 2
a51 2
 * It should be easy to adapt this driver to SiS 191 Gigabit Ethernet
 * PCI NIC.
d54 2
d58 3
a60 1
#include <sys/sockio.h>
d63 2
d69 1
d74 1
d76 1
d78 1
d81 2
a83 1
#include <dev/pci/pcidevs.h>
d85 47
a131 1
#include "if_sereg.h"
d138 1
a138 2
	/* Gigabit variant not supported yet. */
	/*{ PCI_VENDOR_SIS, PCI_PRODUCT_SIS_191 }*/
d141 7
a147 5
int se_probe(struct device *, void *, void *);
void se_attach(struct device *, struct device *, void *);

struct cfattach se_ca = {
	sizeof(struct se_softc), se_probe, se_attach
d154 33
a186 38
int	se_get_mac_addr_cmos	(struct se_softc *, caddr_t);
int	se_get_mac_addr_eeprom	(struct se_softc *, caddr_t);
int	se_isabridge_match	(struct pci_attach_args *);
uint16_t se_read_eeprom	(struct se_softc *, int);
void	miibus_cmd		(struct se_softc *, u_int32_t);
int	se_miibus_readreg	(struct device *, int, int);
void	se_miibus_writereg	(struct device *, int, int, int);
void	se_miibus_statchg	(struct device *);

int	se_ifmedia_upd		(struct ifnet *);
void	se_ifmedia_sts		(struct ifnet *, struct ifmediareq *);

int	se_ioctl		(struct ifnet *, u_long, caddr_t);

void	se_setmulti		(struct se_softc *);
uint32_t se_mchash		(struct se_softc *, const uint8_t *);

int	se_newbuf		(struct se_softc *, u_int32_t,
 				struct mbuf *);
int	se_encap		(struct se_softc *,
 				struct mbuf *, u_int32_t *);
int	se_init		(struct ifnet *);
int	se_list_rx_init	(struct se_softc *);
int	se_list_rx_free	(struct se_softc *);
int	se_list_tx_init	(struct se_softc *);
int	se_list_tx_free	(struct se_softc *);

int	se_intr		(void *);
void	se_rxeof		(struct se_softc *);
void	se_txeof		(struct se_softc *);

void	se_tick		(void *);
void	se_watchdog		(struct ifnet *);

void	se_start		(struct ifnet *);
void	se_reset		(struct se_softc *);
void	se_stop		(struct se_softc *);
void	se_shutdown		(void *);
d188 17
d210 1
a210 3
se_read_eeprom(sc, offset)
	struct se_softc	*sc;
	int			offset;
d212 2
a213 2
	uint32_t	ret, val, i;
        int             s;
d217 2
a218 4
	s = splnet();

	val = EI_REQ | EI_OP_RD | (offset << EI_OFFSET_SHIFT);
	CSR_WRITE_4(sc, ROMInterface, val);
d220 10
a229 9

	for (i = 0; ((ret = CSR_READ_4(sc, ROMInterface)) & EI_REQ) != 0; i++) {
	    if (i > 1000) {
		/* timeout */
		printf("EEPROM read timeout %d\n", i);
		splx(s);
		return (0xffff);
	    }
	    DELAY(100);
d232 1
a232 2
	splx(s);
	return ((ret & EI_DATA) >> EI_DATA_SHIFT);
d236 1
a236 1
se_isabridge_match(struct pci_attach_args *pa)
d238 2
a239 5
	const struct pci_matchid device_ids[] = {
	    { PCI_VENDOR_SIS, PCI_PRODUCT_SIS_965},
	    { PCI_VENDOR_SIS, PCI_PRODUCT_SIS_966},
	    { PCI_VENDOR_SIS, PCI_PRODUCT_SIS_968},
	};
d241 5
a245 16
	return (pci_matchbyid(pa, device_ids,
		    sizeof(device_ids)/sizeof(device_ids[0])));
}

int
se_get_mac_addr_cmos(sc, dest)
	struct se_softc	*sc;
	caddr_t			dest;
{
	struct pci_attach_args	isa_bridge;
	u_int32_t reg, tmp;
	int i, s;

	if (!pci_find_device(&isa_bridge, se_isabridge_match)) {
	    printf("Could not find ISA bridge to retrieve MAC address.\n");
	    return -1;
d248 4
a251 13
	s = splnet();

	/* Enable port 78h & 79h to access APC Registers.
	 * Taken from linux driver. */
	tmp = pci_conf_read(isa_bridge.pa_pc, isa_bridge.pa_tag, 0x48);
	reg = tmp & ~0x02;
	pci_conf_write(isa_bridge.pa_pc, isa_bridge.pa_tag, 0x48, reg);
	delay(50);
	reg = pci_conf_read(isa_bridge.pa_pc, isa_bridge.pa_tag, 0x48);

	for (i = 0; i < ETHER_ADDR_LEN; i++) {
	    bus_space_write_1(isa_bridge.pa_iot, 0x0, 0x78, 0x9 + i);
	    *(dest + i) = bus_space_read_1(isa_bridge.pa_iot, 0x0, 0x79);
d254 4
a257 2
	bus_space_write_1(isa_bridge.pa_iot, 0x0, 0x78, 0x12);
	reg = bus_space_read_1(isa_bridge.pa_iot, 0x0, 0x79);
d259 13
a271 1
	pci_conf_write(isa_bridge.pa_pc, isa_bridge.pa_tag, 0x48, tmp);
d273 1
a273 3
	/* XXX: pci_dev_put(isa_bridge) ? */
	splx(s);
	return 0;
d275 1
d278 1
a278 3
se_get_mac_addr_eeprom(sc, dest)
	struct se_softc	*sc;
	caddr_t			dest;
d280 32
a311 1
	uint16_t	val, i;
d313 2
a314 3
	val = se_read_eeprom(sc, EEPROMSignature);
	if (val == 0xffff || val == 0x0000)
	    return (1);
d316 3
a318 7
	for (i = 0; i < ETHER_ADDR_LEN; i += 2) {
	    val = se_read_eeprom(sc, EEPROMMACAddr + i/2);
	    dest[i + 0] = (uint8_t) val;
	    dest[i + 1] = (uint8_t) (val >> 8);
	}

	return (0);
d321 2
a322 4
void
miibus_cmd(sc, ctl)
    	struct se_softc	*sc;
	u_int32_t		ctl;
d324 2
a325 2
	uint32_t	i;
        int             s;
d327 1
a327 2
	s = splnet();
	CSR_WRITE_4(sc, GMIIControl, ctl);
d329 6
d336 1
a336 12
	for (i = 0; (CSR_READ_4(sc, GMIIControl) & GMI_REQ) != 0; i++)
	{
	    if (i > 1000) {
		/* timeout */
		printf("MIIBUS timeout\n");
		splx(s);
		return;
	    }
	    DELAY(100);
	}
        splx(s);
        return;
d340 4
a343 9
se_miibus_readreg(self, phy, reg)
	struct device		*self;
	int			phy, reg;
{
	struct se_softc	*sc = (struct se_softc *)self;
	miibus_cmd(sc, (phy << GMI_PHY_SHIFT) |
		(reg << GMI_REG_SHIFT) | GMI_OP_RD | GMI_REQ);
	return ((CSR_READ_4(sc, GMIIControl) & GMI_DATA) >> GMI_DATA_SHIFT);
}
d345 9
a353 9
void
se_miibus_writereg(self, phy, reg, data)
	struct device		*self;
	int			phy, reg, data;
{
	struct se_softc	*sc = (struct se_softc *)self;
	miibus_cmd(sc, (((uint32_t) data) << GMI_DATA_SHIFT) |
		(((uint32_t) phy) << GMI_PHY_SHIFT) |
		(reg << GMI_REG_SHIFT) | GMI_OP_WR | GMI_REQ);
d357 1
a357 2
se_miibus_statchg(self)
	struct device		*self;
d359 2
a360 1
	struct se_softc	*sc = (struct se_softc *)self;
d362 7
a368 1
	se_init(sc->se_ifp);
d371 2
a372 4
u_int32_t
se_mchash(sc, addr)
	struct se_softc	*sc;
	const uint8_t		*addr;
d374 68
a441 1
	return (ether_crc32_be(addr, ETHER_ADDR_LEN) >> 26);
d445 1
a445 2
se_setmulti(sc)
	struct se_softc	*sc;
d447 17
a463 11
	struct ifnet		*ifp = sc->se_ifp;
	struct arpcom		*ac = &sc->arpcom;
	struct ether_multi	*enm;
	struct ether_multistep	step;
	uint32_t		hashes[2] = { 0, 0 }, rxfilt;
	//struct ifmultiaddr	*ifma;

	rxfilt = AcceptMyPhys | 0x0052;

	if (ifp->if_flags & IFF_PROMISC) {
	    rxfilt |= AcceptAllPhys;
d465 9
a473 32
	    rxfilt &= ~AcceptAllPhys;
	}

	if (ifp->if_flags & IFF_BROADCAST) {
	    rxfilt |= AcceptBroadcast;
	} else {
	    rxfilt &= ~AcceptBroadcast;
	}

	if (ifp->if_flags & IFF_ALLMULTI || ifp->if_flags & IFF_PROMISC) {
allmulti:
	    rxfilt |= AcceptMulticast;
	    CSR_WRITE_2(sc, RxMacControl, rxfilt);
	    CSR_WRITE_4(sc, RxHashTable, 0xFFFFFFFF);
	    CSR_WRITE_4(sc, RxHashTable2, 0xFFFFFFFF);
	    return;
	}

	/* now program new ones */
	//TAILQ_FOREACH(ifma, &ifp->if_multiaddrs, ifma_link) {
	ETHER_FIRST_MULTI(step, ac, enm);
	while (enm != NULL) {
	    if (bcmp(enm->enm_addrlo, enm->enm_addrhi, ETHER_ADDR_LEN)) {
		ifp->if_flags |= IFF_ALLMULTI;
		goto allmulti;
	    }

	    int bit_nr = se_mchash(sc, LLADDR((struct sockaddr_dl *)
				    enm->enm_addrlo));
	    hashes[bit_nr >> 5] |= 1 << (bit_nr & 31);
	    rxfilt |= AcceptMulticast;
	    ETHER_NEXT_MULTI(step, enm);
d482 1
a482 2
se_reset(sc)
	struct se_softc	*sc;
d487 1
a487 3
	CSR_WRITE_4(sc, TxControl, 0x00001a00);
	CSR_WRITE_4(sc, RxControl, 0x00001a1d);

d489 1
a489 1
	SE_PCI_COMMIT();
d491 4
a494 1
	CSR_WRITE_4(sc, IntrControl, 0x0);
d498 2
d507 1
a507 4
se_probe(parent, match, aux)
    	struct device		*parent;
	void			*match;
	void			*aux;
d509 3
a511 2
	return (pci_matchbyid((struct pci_attach_args *)aux, se_devices,
	    sizeof(se_devices)/sizeof(se_devices[0])));
d515 1
a515 2
 * Attach the interface. Allocate softc structures, do ifmedia
 * setup and ethernet/BPF attach.
d518 12
a529 15
se_attach(parent, self, aux)
	struct device		*parent;
	struct device		*self;
	void			*aux;
{
	const char		*intrstr = NULL;
	struct se_softc	*sc = (struct se_softc *)self;
	struct pci_attach_args	*pa = aux;
	pci_chipset_tag_t	pc = pa->pa_pc;
	pci_intr_handle_t	ih;
	bus_size_t		size;
	struct ifnet		*ifp;
	bus_dma_segment_t	seg;
	int			nseg;
	struct se_list_data	*ld;
d531 3
a533 6
	int			i, error = 0;

	ld = &sc->se_ldata;
	cd = &sc->se_cdata;

	/* TODO: What about power management ? */
d535 1
a535 44
	/*
	 * Handle power management nonsense.
	 */
#if 0
/* power management registers */
#define SIS_PCI_CAPID		0x50 /* 8 bits */
#define SIS_PCI_NEXTPTR		0x51 /* 8 bits */
#define SIS_PCI_PWRMGMTCAP	0x52 /* 16 bits */
#define SIS_PCI_PWRMGMTCTRL	0x54 /* 16 bits */

#define SIS_PSTATE_MASK		0x0003
#define SIS_PSTATE_D0		0x0000
#define SIS_PSTATE_D1		0x0001
#define SIS_PSTATE_D2		0x0002
#define SIS_PSTATE_D3		0x0003
#define SIS_PME_EN		0x0010
#define SIS_PME_STATUS		0x8000
#define SIS_PCI_INTLINE		0x3C
#define SIS_PCI_LOMEM		0x14
	command = pci_conf_read(pc, pa->pa_tag, SIS_PCI_CAPID) & 0x000000FF;
	if (command == 0x01) {

		command = pci_conf_read(pc, pa->pa_tag, SIS_PCI_PWRMGMTCTRL);
		if (command & SIS_PSTATE_MASK) {
			u_int32_t		iobase, membase, irq;

			/* Save important PCI config data. */
			iobase = pci_conf_read(pc, pa->pa_tag, SIS_PCI_LOIO);
			membase = pci_conf_read(pc, pa->pa_tag, SIS_PCI_LOMEM);
			irq = pci_conf_read(pc, pa->pa_tag, SIS_PCI_INTLINE);

			/* Reset the power state. */
			printf("%s: chip is in D%d power mode -- setting to D0\n",
			    sc->sc_dev.dv_xname, command & SIS_PSTATE_MASK);
			command &= 0xFFFFFFFC;
			pci_conf_write(pc, pa->pa_tag, SIS_PCI_PWRMGMTCTRL, command);

			/* Restore PCI config data. */
			pci_conf_write(pc, pa->pa_tag, SIS_PCI_LOIO, iobase);
			pci_conf_write(pc, pa->pa_tag, SIS_PCI_LOMEM, membase);
			pci_conf_write(pc, pa->pa_tag, SIS_PCI_INTLINE, irq);
		}
	}
#endif
d541 6
a546 14
	/* Map IO */
	if ((error = pci_mapreg_map(
			pa,
			SE_PCI_LOMEM,
			PCI_MAPREG_TYPE_MEM, 0,
			&sc->se_btag,
			&sc->se_bhandle,
			NULL,
			&size,
			0)))
	{
	    printf(": can't map i/o space (code %d)\n", error);
	    return;
 	}
a547 1
	/* Allocate interrupt */
d549 2
a550 2
	    printf(": couldn't map interrupt\n");
	    goto intfail;
d552 2
a553 2
	intrstr = pci_intr_string(pc, ih);
	sc->sc_ih = pci_intr_establish(pc, ih, IPL_NET, se_intr, sc,
d556 5
a560 5
	    printf(": couldn't establish interrupt");
	    if (intrstr != NULL)
		printf(" at %s", intrstr);
	    printf("\n");
	    goto intfail;
d563 5
d571 7
a577 10
	/*
	 * Get MAC address from the EEPROM.
	 */
	if (se_get_mac_addr_eeprom(sc, (caddr_t)&sc->arpcom.ac_enaddr) &&
	    se_get_mac_addr_cmos(sc, (caddr_t)&sc->arpcom.ac_enaddr)
	   )
	    goto fail;

	printf(": %s, address %s\n", intrstr,
	    ether_sprintf(sc->arpcom.ac_enaddr));
d583 3
a585 1
	sc->se_tag = pa->pa_dmat;
d589 1
a589 1
	    error = bus_dmamap_create(sc->se_tag, MCLBYTES, 1, MCLBYTES,
d591 5
a595 4
	    if (error) {
		printf("cannot init the RX map array!\n");
		goto fail;
	    }
d599 1
a599 1
	    error = bus_dmamap_create(sc->se_tag, MCLBYTES, 1, MCLBYTES,
d601 5
a605 4
	    if (error) {
		printf("cannot init the TX map array!\n");
		goto fail;
	    }
a607 1

d609 3
a611 5
	 * Now allocate a tag for the DMA descriptor lists and a chunk
	 * of DMA-able memory based on the tag.  Also obtain the physical
	 * addresses of the RX and TX ring, which we'll need later.
	 * All of our lists are allocated as a contiguous block
	 * of memory.
d615 26
a640 28

	error = bus_dmamem_alloc(sc->se_tag, SE_RX_RING_SZ, PAGE_SIZE, 0,
		&seg, 1, &nseg, BUS_DMA_NOWAIT);
	if (error) {
	    printf("no memory for rx list buffers!\n");
	    goto fail;
	}

	error = bus_dmamem_map(sc->se_tag, &seg, nseg, SE_RX_RING_SZ,
		(caddr_t *)&ld->se_rx_ring, BUS_DMA_NOWAIT);
	if (error) {
	    printf("can't map rx list buffers!\n");
	    goto fail;
	}

	error = bus_dmamap_create(sc->se_tag, SE_RX_RING_SZ, 1,
		SE_RX_RING_SZ, 0, BUS_DMA_NOWAIT, &ld->se_rx_dmamap);
	if (error) {
	    printf("can't alloc rx list map!\n");
	    goto fail;
	}

	error = bus_dmamap_load(sc->se_tag, ld->se_rx_dmamap,
		(caddr_t)ld->se_rx_ring, SE_RX_RING_SZ,
		NULL, BUS_DMA_NOWAIT);
	if (error) {
	    printf("can't load rx ring mapping!\n");
	    bus_dmamem_unmap(sc->se_tag,
d642 3
a644 3
	    bus_dmamap_destroy(sc->se_tag, ld->se_rx_dmamap);
	    bus_dmamem_free(sc->se_tag, &seg, nseg);
	    goto fail;
d648 26
a673 28

	error = bus_dmamem_alloc(sc->se_tag, SE_TX_RING_SZ, PAGE_SIZE, 0,
		&seg, 1, &nseg, BUS_DMA_NOWAIT);
	if (error) {
	    printf("no memory for tx list buffers!\n");
	    goto fail;
	}

	error = bus_dmamem_map(sc->se_tag, &seg, nseg, SE_TX_RING_SZ,
		(caddr_t *)&ld->se_tx_ring, BUS_DMA_NOWAIT);
	if (error) {
	    printf("can't map tx list buffers!\n");
	    goto fail;
	}

	error = bus_dmamap_create(sc->se_tag, SE_TX_RING_SZ, 1,
		SE_TX_RING_SZ, 0, BUS_DMA_NOWAIT, &ld->se_tx_dmamap);
	if (error) {
	    printf("can't alloc tx list map!\n");
	    goto fail;
	}

	error = bus_dmamap_load(sc->se_tag, ld->se_tx_dmamap,
		(caddr_t)ld->se_tx_ring, SE_TX_RING_SZ,
		NULL, BUS_DMA_NOWAIT);
	if (error) {
	    printf("can't load tx ring mapping!\n");
	    bus_dmamem_unmap(sc->se_tag,
d675 3
a677 3
	    bus_dmamap_destroy(sc->se_tag, ld->se_tx_dmamap);
	    bus_dmamem_free(sc->se_tag, &seg, nseg);
	    goto fail;
d680 1
a680 1
	timeout_set(&sc->se_timeout, se_tick, sc);
d682 1
a682 1
	sc->se_ifp = ifp = &sc->arpcom.ac_if;
a683 1
	ifp->if_mtu = ETHERMTU;
a687 1
	ifp->if_baudrate = IF_Mbps(100);
d700 4
a703 3
	ifmedia_init(&sc->sc_mii.mii_media, 0,
			se_ifmedia_upd,se_ifmedia_sts);
	mii_phy_probe(self, &sc->sc_mii, 0xffffffff);
d706 4
a709 2
	    ifmedia_add(&sc->sc_mii.mii_media, IFM_ETHER|IFM_NONE, 0, NULL);
	    ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER|IFM_NONE);
d711 1
a711 1
	    ifmedia_set(&sc->sc_mii.mii_media, IFM_ETHER|IFM_AUTO);
a718 2
	shutdownhook_establish(se_shutdown, sc);

d721 4
a724 5
fail:
	pci_intr_disestablish(pc, sc->sc_ih);

intfail:
	bus_space_unmap(sc->se_btag, sc->se_bhandle, size);
d727 2
a728 7
/*
 * Stop all chip I/O so that the kernel's probe routines don't
 * get confused by errant DMAs when rebooting.
 */
void
se_shutdown(v)
	void			*v;
d730 16
a745 1
	struct se_softc	*sc = (struct se_softc *)v;
d747 1
a747 2
	se_reset(sc);
	se_stop(sc);
a749 2


d754 1
a754 2
se_list_tx_init(sc)
	struct se_softc	*sc;
d756 2
a757 2
	struct se_list_data	*ld = &sc->se_ldata;
	struct se_chain_data	*cd = &sc->se_cdata;
d760 4
a763 2
	ld->se_tx_ring[SE_TX_RING_CNT - 1].se_flags |= RING_END;
	cd->se_tx_prod = cd->se_tx_cons = cd->se_tx_cnt = 0;
d765 1
a765 1
	return (0);
d769 1
a769 2
se_list_tx_free(sc)
	struct se_softc	*sc;
d771 2
a772 2
	struct se_chain_data	*cd = &sc->se_cdata;
	int		i;
d775 5
a779 5
	    if (cd->se_tx_mbuf[i] != NULL) {
		bus_dmamap_unload(sc->se_tag, cd->se_tx_map[i]);
		m_free(cd->se_tx_mbuf[i]);
		cd->se_tx_mbuf[i] = NULL;
	    }
d782 1
a782 1
	return (0);
d791 1
a791 2
se_list_rx_init(sc)
	struct se_softc	*sc;
d793 3
a795 3
	struct se_list_data	*ld = &sc->se_ldata;
	struct se_chain_data	*cd = &sc->se_cdata;
	int		i;
d799 2
a800 4
	    if (se_newbuf(sc, i, NULL) == ENOBUFS) {
		printf("unable to allocate MBUFs, %d\n", i);
		return (ENOBUFS);
	    }
d803 1
a803 1
	ld->se_rx_ring[SE_RX_RING_CNT - 1].se_flags |= RING_END;
d806 1
a806 1
	return (0);
d810 1
a810 2
se_list_rx_free(sc)
	struct se_softc	*sc;
d812 2
a813 2
	struct se_chain_data	*cd = &sc->se_cdata;
	int		i;
d816 5
a820 5
	    if (cd->se_rx_mbuf[i] != NULL) {
		bus_dmamap_unload(sc->se_tag, cd->se_rx_map[i]);
		m_free(cd->se_rx_mbuf[i]);
		cd->se_rx_mbuf[i] = NULL;
	    }
d823 1
a823 1
	return (0);
d830 10
a839 9
se_newbuf(sc, i, m)
	struct se_softc	*sc;
	u_int32_t		i;
	struct mbuf		*m;
{
	struct se_list_data	*ld = &sc->se_ldata;
	struct se_chain_data	*cd = &sc->se_cdata;
        /*struct ifnet		*ifp = sc->se_ifp;*/
	int			error, alloc;
d841 1
d843 5
a847 10
	    m = MCLGETI(NULL, M_DONTWAIT, NULL, MCLBYTES);
	    if (m == NULL) {
		printf("unable to get new MBUF\n");
		return (ENOBUFS);
	    }
	    cd->se_rx_mbuf[i] = m;
	    alloc = 1;
	} else {
	    m->m_data = m->m_ext.ext_buf;
	    alloc = 0;
a848 1

d850 1
d852 4
a855 5
	if (alloc) {
	    error = bus_dmamap_load_mbuf(sc->se_tag, cd->se_rx_map[i],
					 m, BUS_DMA_NOWAIT);
	    if (error) {
		printf("unable to map and load the MBUF\n");
d857 1
a857 2
		return (ENOBUFS);
	    }
d860 14
a873 11
	/* This is used both to initialize the newly created RX
	 * descriptor as well as for re-initializing it for reuse. */
	ld->se_rx_ring[i].se_sts_size = 0;
	ld->se_rx_ring[i].se_cmdsts =
		htole32(OWNbit | INTbit | IPbit | TCPbit | UDPbit);
	ld->se_rx_ring[i].se_ptr =
		htole32(cd->se_rx_map[i]->dm_segs[0].ds_addr);
	ld->se_rx_ring[i].se_flags =
		htole32(cd->se_rx_map[i]->dm_segs[0].ds_len)
		| (i == SE_RX_RING_CNT - 1 ? RING_END : 0);
	KASSERT(cd->se_rx_map[i]->dm_nsegs == 1);
d875 5
a879 3
	bus_dmamap_sync(sc->se_tag, cd->se_rx_map[i], 0,
			cd->se_rx_map[i]->dm_mapsize,
			BUS_DMASYNC_PREREAD);
d881 6
a886 1
	return (0);
d894 1
a894 2
se_rxeof(sc)
	struct se_softc	*sc;
d896 41
a936 49
        struct mbuf		*m, *m0;
        struct ifnet		*ifp = sc->se_ifp;
	struct se_list_data	*ld = &sc->se_ldata;
	struct se_chain_data	*cd = &sc->se_cdata;
	struct se_desc	*cur_rx;
	u_int32_t		i, rxstat, total_len = 0;

	for (i = cd->se_rx_prod; !SE_OWNDESC(&ld->se_rx_ring[i]);
	    SE_INC(i, SE_RX_RING_CNT))
	{
	    bus_dmamap_sync(sc->se_tag, cd->se_rx_map[i], 0,
			    cd->se_rx_map[i]->dm_mapsize,
			    BUS_DMASYNC_POSTREAD);

	    cur_rx = &ld->se_rx_ring[i];
	    rxstat = SE_RXSTATUS(cur_rx);
	    total_len = SE_RXSIZE(cur_rx);
	    m = cd->se_rx_mbuf[i];

	    /*
	     * If an error occurs, update stats, clear the
	     * status word and leave the mbuf cluster in place:
	     * it should simply get re-used next time this descriptor
	     * comes up in the ring.
	     */
	    if (rxstat & RX_ERR_BITS) {
		printf("error_bits=%#x\n", rxstat);
		ifp->if_ierrors++;
		/* TODO: better error differentiation */
		se_newbuf(sc, i, m);
		continue;
	    }

	    /* No errors; receive the packet. */
	    cd->se_rx_mbuf[i] = NULL; /* XXX neccessary? */
#ifndef __STRICT_ALIGNMENT
	    if (se_newbuf(sc, i, NULL) == 0) {
		m->m_pkthdr.len = m->m_len = total_len;
	    } else
#endif
	    {
		/* ETHER_ALIGN is 2 */
		m0 = m_devget(mtod(m, char *), total_len,
		    2, ifp, NULL);
		se_newbuf(sc, i, m);
		if (m0 == NULL) {
		    printf("unable to copy MBUF\n");
		    ifp->if_ierrors++;
		    continue;
d938 8
a945 2
		m = m0;
	    }
d947 2
a948 2
	    ifp->if_ipackets++;
	    m->m_pkthdr.rcvif = ifp;
d951 2
a952 2
	    if (ifp->if_bpf)
		bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
d954 1
a954 3

	    /* pass it on. */
	    ether_input_mbuf(ifp, m);
a959 1

d966 1
a966 2
se_txeof(sc)
	struct se_softc	*sc;
d968 6
a973 5
	struct ifnet		*ifp = sc->se_ifp;
	struct se_list_data	*ld = &sc->se_ldata;
	struct se_chain_data	*cd = &sc->se_cdata;
	struct se_desc	*cur_tx;
	u_int32_t		i, txstat;
d979 20
a998 7
	for (i = cd->se_tx_cons; cd->se_tx_cnt > 0 &&
	       !SE_OWNDESC(&ld->se_tx_ring[i]);
	    cd->se_tx_cnt--, SE_INC(i, SE_TX_RING_CNT))
	{
	    cur_tx = &ld->se_tx_ring[i];
	    txstat = letoh32(cur_tx->se_cmdsts);
	    bus_dmamap_sync(sc->se_tag, cd->se_tx_map[i], 0,
d1001 4
d1006 4
a1009 24
	    /* current slot is transferred now */

	    if (txstat & TX_ERR_BITS) {
		printf("error_bits=%#x\n", txstat);
		ifp->if_oerrors++;
		/* TODO: better error differentiation */
	    }

	    ifp->if_opackets++;
	    if (cd->se_tx_mbuf[i] != NULL) {
		bus_dmamap_unload(sc->se_tag, cd->se_tx_map[i]);
		m_free(cd->se_tx_mbuf[i]);
		cd->se_tx_mbuf[i] = NULL;
	    }
	    cur_tx->se_sts_size = 0;
	    cur_tx->se_cmdsts = 0;
	    cur_tx->se_ptr = 0;
	    cur_tx->se_flags &= RING_END;
	}

	if (i != cd->se_tx_cons) {
	    /* we freed up some buffers */
	    cd->se_tx_cons = i;
	    ifp->if_flags &= ~IFF_OACTIVE;
d1012 3
a1014 1
	sc->se_watchdog_timer = (cd->se_tx_cnt == 0) ? 0 : 5;
d1018 1
a1018 2
se_tick(xsc)
	void			*xsc;
d1020 4
a1023 4
	struct se_softc	*sc = xsc;
	struct mii_data		*mii;
	struct ifnet		*ifp = sc->se_ifp;
        int                     s;
a1025 3

	sc->in_tick = 1;

d1028 5
a1032 9

	se_watchdog(ifp);

	if (!sc->se_link && mii->mii_media_status & IFM_ACTIVE &&
	    IFM_SUBTYPE(mii->mii_media_active) != IFM_NONE)
	{
	    sc->se_link++;
	    if (!IFQ_IS_EMPTY(&ifp->if_snd))
		se_start(ifp);
d1034 1
d1036 1
a1036 6
	timeout_add_sec(&sc->se_timeout, 1);

	sc->in_tick = 0;

        splx(s);
        return;
d1040 1
a1040 2
se_intr(arg)
	void			*arg;
d1042 13
a1054 9
	struct se_softc	*sc = arg;
	struct ifnet		*ifp = sc->se_ifp;
	int			status;
	int			claimed = 0;

	if (sc->se_stopped)	/* Most likely shared interrupt */
	    return (claimed);

	DISABLE_INTERRUPTS(sc);
d1057 23
a1079 14
	    /* Reading the ISR register clears all interrupts. */
	    status = CSR_READ_4(sc, IntrStatus);
	    if ((status == 0xffffffff) || (status == 0x0))
		break;

	    claimed = 1; /* XXX just a guess to put this here */

	    CSR_WRITE_4(sc, IntrStatus, status);

	    if (status & TxQInt)
		se_txeof(sc);

	    if (status & RxQInt)
		se_rxeof(sc);
d1082 1
a1082 6
	ENABLE_INTERRUPTS(sc);

	if (!IFQ_IS_EMPTY(&ifp->if_snd))
	    se_start(ifp);

	return (claimed);
d1090 11
a1100 9
se_encap(sc, m_head, txidx)
	struct se_softc	*sc;
	struct mbuf		*m_head;
	u_int32_t		*txidx;
{
	struct mbuf		*m;
	struct se_list_data	*ld = &sc->se_ldata;
	struct se_chain_data	*cd = &sc->se_cdata;
	int			error, i, cnt = 0;
d1105 8
a1112 2
	if (SE_TX_RING_CNT - cd->se_tx_cnt < 2)
	    return (ENOBUFS);
d1114 6
a1119 4
#if 1
	if (m_defrag(m_head, M_DONTWAIT)) {
	    printf("unable to defragment MBUFs\n");
	    return (ENOBUFS);
d1121 1
a1121 17
#else
	m = MCLGETI(NULL, M_DONTWAIT, NULL, MCLBYTES);
	if (m == NULL) {
	    printf("se_encap: unable to allocate MBUF\n");
	    return (ENOBUFS);
	}
	m_copydata(m_head, 0, m_head->m_pkthdr.len, mtod(m, caddr_t));
	m->m_pkthdr.len = m->m_len = m_head->m_pkthdr.len;
	map = cd->se_tx_map[i];
	error = bus_dmamap_load_mbuf(sc->se_tag, map,
				     m, BUS_DMA_NOWAIT);
	if (error) {
	    printf("unable to load the MBUF\n");
	    return (ENOBUFS);
	}
#endif
	
d1123 1
a1123 1
 	 * Start packing the mbufs in this chain into
d1125 1
a1125 1
 	 * of fragments or hit the end of the mbuf chain.
d1130 37
a1166 33
	    if (m->m_len == 0)
		continue;
	    if ((SE_TX_RING_CNT - (cd->se_tx_cnt + cnt)) < 2)
		return (ENOBUFS);
	    cd->se_tx_mbuf[i] = m;
	    error = bus_dmamap_load_mbuf(sc->se_tag, cd->se_tx_map[i],
					 m, BUS_DMA_NOWAIT);
	    if (error) {
		printf("unable to load the MBUF\n");
		return (ENOBUFS);
	    }

	    ld->se_tx_ring[i].se_sts_size =
		    htole32(cd->se_tx_map[i]->dm_segs->ds_len);
	    ld->se_tx_ring[i].se_cmdsts =
		    htole32(OWNbit | INTbit | PADbit | CRCbit | DEFbit);
	    ld->se_tx_ring[i].se_ptr =
		    htole32(cd->se_tx_map[i]->dm_segs->ds_addr);
	    ld->se_tx_ring[i].se_flags |=
		    htole32(cd->se_tx_map[i]->dm_segs->ds_len);
	    KASSERT(cd->se_tx_map[i]->dm_nsegs == 1);

	    bus_dmamap_sync(sc->se_tag, cd->se_tx_map[i], 0,
			    cd->se_tx_map[i]->dm_mapsize,
			    BUS_DMASYNC_PREWRITE);
	    SE_INC(i, SE_TX_RING_CNT);
	    cnt++;
	}

	if (m != NULL) {
	    printf("unable to encap all MBUFs\n");
	    return (ENOBUFS);
	}
d1171 1
a1171 1
	return (0);
d1181 1
a1181 2
se_start(ifp)
	struct ifnet		*ifp;
d1183 13
a1195 11
	struct se_softc	*sc = ifp->if_softc;
	struct mbuf		*m_head = NULL;
	struct se_chain_data	*cd = &sc->se_cdata;
	u_int32_t		i, queued = 0;

	if (!sc->se_link) {
	    return;
	}

	if (ifp->if_flags & IFF_OACTIVE) {
	    return;
d1201 8
a1208 8
	    IFQ_POLL(&ifp->if_snd, m_head);
	    if (m_head == NULL)
		break;

	    if (se_encap(sc, m_head, &i)) {
		ifp->if_flags |= IFF_OACTIVE;
		break;
	    }
d1210 8
a1217 8
	    /* now we are committed to transmit the packet */
	    IFQ_DEQUEUE(&ifp->if_snd, m_head);
	    queued++;

	    /*
	     * If there's a BPF listener, bounce a copy of this frame
	     * to him.
	     */
d1219 2
a1220 2
	    if (ifp->if_bpf)
		bpf_mtap(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
d1224 5
a1228 9
	if (queued) {
	    /* Transmit */
	    cd->se_tx_prod = i;
	    SE_SETBIT(sc, TxControl, CmdReset);

	    /*
	     * Set a timeout in case the chip goes out to lunch.
	     */
	    sc->se_watchdog_timer = 5;
a1231 1
/* TODO: Find out right return codes */
d1233 1
a1233 2
se_init(ifp)
	struct ifnet		*ifp;
d1235 5
a1239 2
	struct se_softc	*sc = ifp->if_softc;
	int			s;
d1241 2
a1242 1
	s = splnet();
d1248 1
a1248 1
	sc->se_stopped = 0;
d1252 2
a1253 5
	    printf("initialization failed: no "
		   "memory for rx buffers\n");
	    se_stop(sc);
	    splx(s);
	    return 1;
a1258 2
	se_reset(sc);

d1262 16
a1277 11
	CSR_WRITE_4(sc, TxDescStartAddr,
			sc->se_ldata.se_tx_dmamap->dm_segs[0].ds_addr);
	CSR_WRITE_4(sc, RxDescStartAddr,
			sc->se_ldata.se_rx_dmamap->dm_segs[0].ds_addr);

/* 	CSR_WRITE_4(sc, PMControl, 0xffc00000); */
	CSR_WRITE_4(sc, Reserved2, 0);

	CSR_WRITE_4(sc, IntrStatus, 0xffffffff);
	DISABLE_INTERRUPTS(sc);

d1280 1
a1280 2
	 * Default is 100Mbps.
	 * A bit strange: 100Mbps is 0x1801 elsewhere -- FR 2005/06/09
d1282 2
a1283 26
	CSR_WRITE_4(sc, StationControl, 0x04001801); // 1901

	CSR_WRITE_4(sc, GMacIOCR, 0x0);
	CSR_WRITE_4(sc, GMacIOCTL, 0x0);

	CSR_WRITE_4(sc, TxMacControl, 0x2364);		/* 0x60 */
	CSR_WRITE_4(sc, TxMacTimeLimit, 0x000f);
	CSR_WRITE_4(sc, RGMIIDelay, 0x0);
	CSR_WRITE_4(sc, Reserved3, 0x0);

	CSR_WRITE_4(sc, RxWakeOnLan, 0x80ff0000);
	CSR_WRITE_4(sc, RxWakeOnLanData, 0x80ff0000);
	CSR_WRITE_4(sc, RxMPSControl, 0x0);
	CSR_WRITE_4(sc, Reserved4, 0x0);

	SE_PCI_COMMIT();

	/*
	 * Load the multicast filter.
	 */
	se_setmulti(sc);

	/*
	 * Enable interrupts.
	 */
	ENABLE_INTERRUPTS(sc);
d1286 2
a1287 2
	SE_SETBIT(sc, TxControl, CmdTxEnb | CmdReset);
	SE_SETBIT(sc, RxControl, CmdRxEnb | CmdReset);
d1292 3
a1294 2
	if (!sc->in_tick)
	    timeout_add_sec(&sc->se_timeout, 1);
a1295 1
	splx(s);
d1303 1
a1303 2
se_ifmedia_upd(ifp)
	struct ifnet		*ifp;
d1305 2
a1306 2
	struct se_softc	*sc = ifp->if_softc;
	struct mii_data		*mii;
d1309 1
a1309 1
	sc->se_link = 0;
d1311 3
a1313 3
	    struct mii_softc	*miisc;
	    LIST_FOREACH(miisc, &mii->mii_phys, mii_list)
		mii_phy_reset(miisc);
d1315 1
a1315 3
	mii_mediachg(mii);

	return (0);
d1322 1
a1322 3
se_ifmedia_sts(ifp, ifmr)
	struct ifnet		*ifp;
	struct ifmediareq	*ifmr;
d1324 2
a1325 2
	struct se_softc	*sc = ifp->if_softc;
	struct mii_data		*mii;
d1334 8
a1341 9
se_ioctl(ifp, command, data)
	struct ifnet		*ifp;
	u_long			command;
	caddr_t			data;
{
	struct se_softc	*sc = ifp->if_softc;
	struct ifreq		*ifr = (struct ifreq *) data;
	struct mii_data		*mii;
	int			s, error = 0;
d1346 11
d1358 10
a1367 11
	    if (ifp->if_flags & IFF_UP)
		se_init(ifp);
	    else if (ifp->if_flags & IFF_RUNNING)
		se_stop(sc);
	    error = 0;
	    break;
	case SIOCADDMULTI:
	case SIOCDELMULTI:
	    se_setmulti(sc);
	    error = 0;
	    break;
d1370 2
a1371 3
	    mii = &sc->sc_mii;
	    error = ifmedia_ioctl(ifp, ifr, &mii->mii_media, command);
	    break;
d1373 8
a1380 2
	    error = ether_ioctl(ifp, &sc->arpcom, command, data);
	    break;
d1384 1
a1384 1
	return (error);
d1388 1
a1388 2
se_watchdog(ifp)
	struct ifnet		*ifp;
d1390 2
a1391 5
	struct se_softc	*sc = ifp->if_softc;
        int                     s;

	if (sc->se_stopped)
	    return;
d1393 2
a1394 5
	if (sc->se_watchdog_timer == 0 || --sc->se_watchdog_timer >0)
	    return;

	printf("watchdog timeout\n");
	sc->se_ifp->if_oerrors++;
d1397 1
a1397 2
	se_stop(sc);
	se_reset(sc);
d1399 2
a1400 4

	if (!IFQ_IS_EMPTY(&sc->se_ifp->if_snd))
	    se_start(sc->se_ifp);

a1401 1
        return;
d1409 1
a1409 2
se_stop(sc)
	struct se_softc	*sc;
d1411 1
a1411 8
	struct ifnet		*ifp = sc->se_ifp;

	if (sc->se_stopped)
	    return;

	sc->se_watchdog_timer = 0;

	timeout_del(&sc->se_timeout);
d1413 1
d1415 2
d1418 10
a1427 14
	DISABLE_INTERRUPTS(sc);

	CSR_WRITE_4(sc, IntrControl, 0x8000);
	SE_PCI_COMMIT();
	DELAY(100);
	CSR_WRITE_4(sc, IntrControl, 0x0);

	SE_CLRBIT(sc, TxControl, CmdTxEnb);
	SE_CLRBIT(sc, RxControl, CmdRxEnb);
	DELAY(100);
	CSR_WRITE_4(sc, TxDescStartAddr, 0);
	CSR_WRITE_4(sc, RxDescStartAddr, 0);

	sc->se_link = 0;
d1429 1
a1429 3
	/*
	 * Free data in the RX lists.
	 */
a1430 4

	/*
	 * Free the TX list buffers.
	 */
a1431 2

	sc->se_stopped = 1;
@


1.2
log
@add $OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a630 1
	ifp->if_init = se_init;
@


1.1
log
@SiS 190 ethernet driver ported from FreeBSD by Christopher Zimmermann
<madroach@@zakweb.de>.
SiS 191 is not verified to work yet because he does not have the hardware;
if anyone has it, please contact him.
@
text
@d1 2
@

