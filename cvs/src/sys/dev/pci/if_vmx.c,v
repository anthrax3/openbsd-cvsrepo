head	1.45;
access;
symbols
	OPENBSD_6_0:1.44.0.4
	OPENBSD_6_0_BASE:1.44
	OPENBSD_5_9:1.43.0.2
	OPENBSD_5_9_BASE:1.43
	OPENBSD_5_8:1.30.0.4
	OPENBSD_5_8_BASE:1.30
	OPENBSD_5_7:1.24.0.4
	OPENBSD_5_7_BASE:1.24
	OPENBSD_5_6:1.19.0.4
	OPENBSD_5_6_BASE:1.19
	OPENBSD_5_5:1.16.0.4
	OPENBSD_5_5_BASE:1.16
	OPENBSD_5_4:1.11.0.2
	OPENBSD_5_4_BASE:1.11;
locks; strict;
comment	@ * @;


1.45
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.44;
commitid	VyLWTsbepAOk7VQM;

1.44
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.43;
commitid	8YSL8ByWzGeIGBiJ;

1.43
date	2016.01.26.10.23.19;	author reyk;	state Exp;
branches;
next	1.42;
commitid	pDMn8E9pNU7LUnZr;

1.42
date	2016.01.25.10.39.20;	author reyk;	state Exp;
branches;
next	1.41;
commitid	QgKBWiIagDejqt3w;

1.41
date	2016.01.04.16.16.56;	author mikeb;	state Exp;
branches;
next	1.40;
commitid	Tc9cmFiS5N3nulqh;

1.40
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.39;
commitid	B0kwmVGiD5DVx4kv;

1.39
date	2015.11.24.15.25.20;	author mpi;	state Exp;
branches;
next	1.38;
commitid	MfakaqIkeFe2uL7U;

1.38
date	2015.11.24.13.45.06;	author mpi;	state Exp;
branches;
next	1.37;
commitid	FuSD2mFDJWATHIDx;

1.37
date	2015.11.24.13.33.17;	author mpi;	state Exp;
branches;
next	1.36;
commitid	5DvsamK0GblTp8ww;

1.36
date	2015.11.23.10.52.43;	author mpi;	state Exp;
branches;
next	1.35;
commitid	UywgOyMnPH4kBv9g;

1.35
date	2015.11.14.17.54.57;	author mpi;	state Exp;
branches;
next	1.34;
commitid	Waft2RDjXAxr4qZ9;

1.34
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.33;
commitid	hPF95ClMUQfeqQDX;

1.33
date	2015.09.20.22.26.18;	author dlg;	state Exp;
branches;
next	1.32;
commitid	uU7c561gXFL2i7bf;

1.32
date	2015.09.20.02.01.22;	author dlg;	state Exp;
branches;
next	1.31;
commitid	GerdjaKm5JTd88aT;

1.31
date	2015.09.18.03.53.44;	author dlg;	state Exp;
branches;
next	1.30;
commitid	8X6W54QNvMwfumlL;

1.30
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.29;
commitid	MVWrtktB46JRxFWT;

1.29
date	2015.06.04.17.10.33;	author mikeb;	state Exp;
branches;
next	1.28;
commitid	Swva7MShsuc7HyhJ;

1.28
date	2015.05.29.00.37.10;	author uebayasi;	state Exp;
branches;
next	1.27;
commitid	fK1KUxmxCh2v4tEt;

1.27
date	2015.05.29.00.33.37;	author uebayasi;	state Exp;
branches;
next	1.26;
commitid	k9pN2wgTn5cUwDek;

1.26
date	2015.05.26.12.29.42;	author dlg;	state Exp;
branches;
next	1.25;
commitid	zEEPRG0GGJ0BbCQV;

1.25
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.24;
commitid	p4LJxGKbi0BU2cG6;

1.24
date	2015.02.10.02.57.32;	author pelikan;	state Exp;
branches;
next	1.23;
commitid	CsGpnijgOcAILIF6;

1.23
date	2015.02.09.11.06.52;	author pelikan;	state Exp;
branches;
next	1.22;
commitid	41w8CUq36FRMleSv;

1.22
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.21;
commitid	yM2VFFhpDTeFQlve;

1.21
date	2014.12.19.02.32.57;	author brad;	state Exp;
branches;
next	1.20;
commitid	gLeng0vjQv1fgsB0;

1.20
date	2014.08.26.23.55.28;	author dlg;	state Exp;
branches;
next	1.19;
commitid	olX69sJoyRBqmbYQ;

1.19
date	2014.07.22.13.12.11;	author mpi;	state Exp;
branches;
next	1.18;
commitid	TGHgrLxu6sxZoiFt;

1.18
date	2014.07.08.08.54.00;	author stsp;	state Exp;
branches;
next	1.17;
commitid	bCjp1PqsJZoAnCgD;

1.17
date	2014.07.08.05.35.19;	author dlg;	state Exp;
branches;
next	1.16;
commitid	0QJleeeWqZmC5anF;

1.16
date	2014.01.22.06.04.17;	author brad;	state Exp;
branches;
next	1.15;

1.15
date	2013.11.11.07.19.53;	author dlg;	state Exp;
branches;
next	1.14;

1.14
date	2013.11.08.22.36.22;	author dlg;	state Exp;
branches;
next	1.13;

1.13
date	2013.09.08.02.18.00;	author reyk;	state Exp;
branches;
next	1.12;

1.12
date	2013.08.28.10.19.19;	author reyk;	state Exp;
branches;
next	1.11;

1.11
date	2013.06.22.00.28.10;	author uebayasi;	state Exp;
branches;
next	1.10;

1.10
date	2013.06.21.07.52.22;	author uebayasi;	state Exp;
branches;
next	1.9;

1.9
date	2013.06.12.01.07.33;	author uebayasi;	state Exp;
branches;
next	1.8;

1.8
date	2013.06.12.00.19.36;	author uebayasi;	state Exp;
branches;
next	1.7;

1.7
date	2013.06.12.00.18.00;	author uebayasi;	state Exp;
branches;
next	1.6;

1.6
date	2013.06.10.00.49.26;	author brad;	state Exp;
branches;
next	1.5;

1.5
date	2013.06.08.17.07.31;	author brad;	state Exp;
branches;
next	1.4;

1.4
date	2013.06.05.02.04.07;	author dlg;	state Exp;
branches;
next	1.3;

1.3
date	2013.06.03.21.08.21;	author reyk;	state Exp;
branches;
next	1.2;

1.2
date	2013.06.03.15.05.29;	author reyk;	state Exp;
branches;
next	1.1;

1.1
date	2013.05.31.20.14.18;	author uebayasi;	state Exp;
branches;
next	;


desc
@@


1.45
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@/*	$OpenBSD: if_vmx.c,v 1.44 2016/04/13 10:34:32 mpi Exp $	*/

/*
 * Copyright (c) 2013 Tsubai Masanari
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include "bpfilter.h"

#include <sys/param.h>
#include <sys/device.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/sockio.h>
#include <sys/systm.h>
#include <sys/atomic.h>

#include <net/bpf.h>
#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>
#include <netinet/ip.h>
#include <netinet/tcp.h>
#include <netinet/udp.h>

#include <machine/bus.h>

#include <dev/pci/if_vmxreg.h>
#include <dev/pci/pcivar.h>
#include <dev/pci/pcidevs.h>

#define NRXQUEUE 1
#define NTXQUEUE 1

#define NTXDESC 512 /* tx ring size */
#define NTXSEGS 8 /* tx descriptors per packet */
#define NRXDESC 512
#define NTXCOMPDESC NTXDESC
#define NRXCOMPDESC (NRXDESC * 2)	/* ring1 + ring2 */

#define VMXNET3_DRIVER_VERSION 0x00010000

struct vmxnet3_txring {
	struct mbuf *m[NTXDESC];
	bus_dmamap_t dmap[NTXDESC];
	struct vmxnet3_txdesc *txd;
	u_int prod;
	u_int cons;
	u_int free;
	u_int8_t gen;
};

struct vmxnet3_rxring {
	struct mbuf *m[NRXDESC];
	bus_dmamap_t dmap[NRXDESC];
	struct if_rxring rxr;
	struct vmxnet3_rxdesc *rxd;
	u_int fill;
	u_int8_t gen;
	u_int8_t rid;
};

struct vmxnet3_comp_ring {
	union {
		struct vmxnet3_txcompdesc *txcd;
		struct vmxnet3_rxcompdesc *rxcd;
	};
	u_int next;
	u_int8_t gen;
};

struct vmxnet3_txqueue {
	struct vmxnet3_txring cmd_ring;
	struct vmxnet3_comp_ring comp_ring;
	struct vmxnet3_txq_shared *ts;
};

struct vmxnet3_rxqueue {
	struct vmxnet3_rxring cmd_ring[2];
	struct vmxnet3_comp_ring comp_ring;
	struct vmxnet3_rxq_shared *rs;
};

struct vmxnet3_softc {
	struct device sc_dev;
	struct arpcom sc_arpcom;
	struct ifmedia sc_media;

	bus_space_tag_t	sc_iot0;
	bus_space_tag_t	sc_iot1;
	bus_space_handle_t sc_ioh0;
	bus_space_handle_t sc_ioh1;
	bus_dma_tag_t sc_dmat;
	void *sc_ih;

	struct vmxnet3_txqueue sc_txq[NTXQUEUE];
	struct vmxnet3_rxqueue sc_rxq[NRXQUEUE];
	struct vmxnet3_driver_shared *sc_ds;
	u_int8_t *sc_mcast;
};

#define VMXNET3_STAT

#ifdef VMXNET3_STAT
struct {
	u_int ntxdesc;
	u_int nrxdesc;
	u_int txhead;
	u_int txdone;
	u_int maxtxlen;
	u_int rxdone;
	u_int rxfill;
	u_int intr;
} vmxstat = {
	NTXDESC, NRXDESC
};
#endif

#define JUMBO_LEN (1024 * 9)
#define DMAADDR(map) ((map)->dm_segs[0].ds_addr)

#define READ_BAR0(sc, reg) bus_space_read_4((sc)->sc_iot0, (sc)->sc_ioh0, reg)
#define READ_BAR1(sc, reg) bus_space_read_4((sc)->sc_iot1, (sc)->sc_ioh1, reg)
#define WRITE_BAR0(sc, reg, val) \
	bus_space_write_4((sc)->sc_iot0, (sc)->sc_ioh0, reg, val)
#define WRITE_BAR1(sc, reg, val) \
	bus_space_write_4((sc)->sc_iot1, (sc)->sc_ioh1, reg, val)
#define WRITE_CMD(sc, cmd) WRITE_BAR1(sc, VMXNET3_BAR1_CMD, cmd)
#define vtophys(va) 0		/* XXX ok? */

int vmxnet3_match(struct device *, void *, void *);
void vmxnet3_attach(struct device *, struct device *, void *);
int vmxnet3_dma_init(struct vmxnet3_softc *);
int vmxnet3_alloc_txring(struct vmxnet3_softc *, int);
int vmxnet3_alloc_rxring(struct vmxnet3_softc *, int);
void vmxnet3_txinit(struct vmxnet3_softc *, struct vmxnet3_txqueue *);
void vmxnet3_rxinit(struct vmxnet3_softc *, struct vmxnet3_rxqueue *);
void vmxnet3_txstop(struct vmxnet3_softc *, struct vmxnet3_txqueue *);
void vmxnet3_rxstop(struct vmxnet3_softc *, struct vmxnet3_rxqueue *);
void vmxnet3_link_state(struct vmxnet3_softc *);
void vmxnet3_enable_all_intrs(struct vmxnet3_softc *);
void vmxnet3_disable_all_intrs(struct vmxnet3_softc *);
int vmxnet3_intr(void *);
void vmxnet3_evintr(struct vmxnet3_softc *);
void vmxnet3_txintr(struct vmxnet3_softc *, struct vmxnet3_txqueue *);
void vmxnet3_rxintr(struct vmxnet3_softc *, struct vmxnet3_rxqueue *);
void vmxnet3_iff(struct vmxnet3_softc *);
void vmxnet3_rx_csum(struct vmxnet3_rxcompdesc *, struct mbuf *);
int vmxnet3_getbuf(struct vmxnet3_softc *, struct vmxnet3_rxring *);
void vmxnet3_stop(struct ifnet *);
void vmxnet3_reset(struct vmxnet3_softc *);
int vmxnet3_init(struct vmxnet3_softc *);
int vmxnet3_ioctl(struct ifnet *, u_long, caddr_t);
void vmxnet3_start(struct ifnet *);
int vmxnet3_load_mbuf(struct vmxnet3_softc *, struct vmxnet3_txring *,
    struct mbuf **);
void vmxnet3_watchdog(struct ifnet *);
void vmxnet3_media_status(struct ifnet *, struct ifmediareq *);
int vmxnet3_media_change(struct ifnet *);
void *vmxnet3_dma_allocmem(struct vmxnet3_softc *, u_int, u_int, bus_addr_t *);

const struct pci_matchid vmx_devices[] = {
	{ PCI_VENDOR_VMWARE, PCI_PRODUCT_VMWARE_NET_3 }
};

struct cfattach vmx_ca = {
	sizeof(struct vmxnet3_softc), vmxnet3_match, vmxnet3_attach
};

struct cfdriver vmx_cd = {
	NULL, "vmx", DV_IFNET
};

int
vmxnet3_match(struct device *parent, void *match, void *aux)
{
	return (pci_matchbyid(aux, vmx_devices, nitems(vmx_devices)));
}

void
vmxnet3_attach(struct device *parent, struct device *self, void *aux)
{
	struct vmxnet3_softc *sc = (void *)self;
	struct pci_attach_args *pa = aux;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	pci_intr_handle_t ih;
	const char *intrstr;
	u_int memtype, ver, macl, mach;
	u_char enaddr[ETHER_ADDR_LEN];

	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, 0x10);
	if (pci_mapreg_map(pa, 0x10, memtype, 0, &sc->sc_iot0, &sc->sc_ioh0,
	    NULL, NULL, 0)) {
		printf(": failed to map BAR0\n");
		return;
	}
	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, 0x14);
	if (pci_mapreg_map(pa, 0x14, memtype, 0, &sc->sc_iot1, &sc->sc_ioh1,
	    NULL, NULL, 0)) {
		printf(": failed to map BAR1\n");
		return;
	}

	ver = READ_BAR1(sc, VMXNET3_BAR1_VRRS);
	if ((ver & 0x1) == 0) {
		printf(": unsupported hardware version 0x%x\n", ver);
		return;
	}
	WRITE_BAR1(sc, VMXNET3_BAR1_VRRS, 1);

	ver = READ_BAR1(sc, VMXNET3_BAR1_UVRS);
	if ((ver & 0x1) == 0) {
		printf(": incompatiable UPT version 0x%x\n", ver);
		return;
	}
	WRITE_BAR1(sc, VMXNET3_BAR1_UVRS, 1);

	sc->sc_dmat = pa->pa_dmat;
	if (vmxnet3_dma_init(sc)) {
		printf(": failed to setup DMA\n");
		return;
	}

	if (pci_intr_map(pa, &ih)) {
		printf(": failed to map interrupt\n");
		return;
	}
	sc->sc_ih = pci_intr_establish(pa->pa_pc, ih, IPL_NET | IPL_MPSAFE,
	    vmxnet3_intr, sc, self->dv_xname);
	intrstr = pci_intr_string(pa->pa_pc, ih);
	if (intrstr)
		printf(": %s", intrstr);

	WRITE_CMD(sc, VMXNET3_CMD_GET_MACL);
	macl = READ_BAR1(sc, VMXNET3_BAR1_CMD);
	enaddr[0] = macl;
	enaddr[1] = macl >> 8;
	enaddr[2] = macl >> 16;
	enaddr[3] = macl >> 24;
	WRITE_CMD(sc, VMXNET3_CMD_GET_MACH);
	mach = READ_BAR1(sc, VMXNET3_BAR1_CMD);
	enaddr[4] = mach;
	enaddr[5] = mach >> 8;

	WRITE_BAR1(sc, VMXNET3_BAR1_MACL, macl);
	WRITE_BAR1(sc, VMXNET3_BAR1_MACH, mach);
	printf(", address %s\n", ether_sprintf(enaddr));

	bcopy(enaddr, sc->sc_arpcom.ac_enaddr, 6);
	strlcpy(ifp->if_xname, self->dv_xname, IFNAMSIZ);
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_MULTICAST | IFF_SIMPLEX;
	ifp->if_ioctl = vmxnet3_ioctl;
	ifp->if_start = vmxnet3_start;
	ifp->if_watchdog = vmxnet3_watchdog;
	ifp->if_hardmtu = VMXNET3_MAX_MTU;
	ifp->if_capabilities = IFCAP_VLAN_MTU;
	if (sc->sc_ds->upt_features & UPT1_F_CSUM)
		ifp->if_capabilities |= IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;
	if (sc->sc_ds->upt_features & UPT1_F_VLAN)
		ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;

	IFQ_SET_MAXLEN(&ifp->if_snd, NTXDESC);

	ifmedia_init(&sc->sc_media, IFM_IMASK, vmxnet3_media_change,
	    vmxnet3_media_status);
	ifmedia_add(&sc->sc_media, IFM_ETHER|IFM_AUTO, 0, NULL);
	ifmedia_add(&sc->sc_media, IFM_ETHER|IFM_10G_T|IFM_FDX, 0, NULL);
	ifmedia_add(&sc->sc_media, IFM_ETHER|IFM_10G_T, 0, NULL);
	ifmedia_add(&sc->sc_media, IFM_ETHER|IFM_1000_T|IFM_FDX, 0, NULL);
	ifmedia_add(&sc->sc_media, IFM_ETHER|IFM_1000_T, 0, NULL);
	ifmedia_set(&sc->sc_media, IFM_ETHER|IFM_AUTO);

	if_attach(ifp);
	ether_ifattach(ifp);
	vmxnet3_link_state(sc);
}

int
vmxnet3_dma_init(struct vmxnet3_softc *sc)
{
	struct vmxnet3_driver_shared *ds;
	struct vmxnet3_txq_shared *ts;
	struct vmxnet3_rxq_shared *rs;
	bus_addr_t ds_pa, qs_pa, mcast_pa;
	int i, queue, qs_len;
	u_int major, minor, release_code, rev;

	qs_len = NTXQUEUE * sizeof *ts + NRXQUEUE * sizeof *rs;
	ts = vmxnet3_dma_allocmem(sc, qs_len, VMXNET3_DMADESC_ALIGN, &qs_pa);
	if (ts == NULL)
		return -1;
	for (queue = 0; queue < NTXQUEUE; queue++)
		sc->sc_txq[queue].ts = ts++;
	rs = (void *)ts;
	for (queue = 0; queue < NRXQUEUE; queue++)
		sc->sc_rxq[queue].rs = rs++;

	for (queue = 0; queue < NTXQUEUE; queue++)
		if (vmxnet3_alloc_txring(sc, queue))
			return -1;
	for (queue = 0; queue < NRXQUEUE; queue++)
		if (vmxnet3_alloc_rxring(sc, queue))
			return -1;

	sc->sc_mcast = vmxnet3_dma_allocmem(sc, 682 * ETHER_ADDR_LEN, 32, &mcast_pa);
	if (sc->sc_mcast == NULL)
		return -1;

	ds = vmxnet3_dma_allocmem(sc, sizeof *sc->sc_ds, 8, &ds_pa);
	if (ds == NULL)
		return -1;
	sc->sc_ds = ds;
	ds->magic = VMXNET3_REV1_MAGIC;
	ds->version = VMXNET3_DRIVER_VERSION;

	/*
	 * XXX FreeBSD version uses following values:
	 * (Does the device behavior depend on them?)
	 *
	 * major = __FreeBSD_version / 100000;
	 * minor = (__FreeBSD_version / 1000) % 100;
	 * release_code = (__FreeBSD_version / 100) % 10;
	 * rev = __FreeBSD_version % 100;
	 */
	major = 0;
	minor = 0;
	release_code = 0;
	rev = 0;
#ifdef __LP64__
	ds->guest = release_code << 30 | rev << 22 | major << 14 | minor << 6
	    | VMXNET3_GOS_FREEBSD | VMXNET3_GOS_64BIT;
#else
	ds->guest = release_code << 30 | rev << 22 | major << 14 | minor << 6
	    | VMXNET3_GOS_FREEBSD | VMXNET3_GOS_32BIT;
#endif
	ds->vmxnet3_revision = 1;
	ds->upt_version = 1;
	ds->upt_features = UPT1_F_CSUM | UPT1_F_VLAN;
	ds->driver_data = vtophys(sc);
	ds->driver_data_len = sizeof(struct vmxnet3_softc);
	ds->queue_shared = qs_pa;
	ds->queue_shared_len = qs_len;
	ds->mtu = VMXNET3_MAX_MTU;
	ds->ntxqueue = NTXQUEUE;
	ds->nrxqueue = NRXQUEUE;
	ds->mcast_table = mcast_pa;
	ds->automask = 1;
	ds->nintr = VMXNET3_NINTR;
	ds->evintr = 0;
	ds->ictrl = VMXNET3_ICTRL_DISABLE_ALL;
	for (i = 0; i < VMXNET3_NINTR; i++)
		ds->modlevel[i] = UPT1_IMOD_ADAPTIVE;
	WRITE_BAR1(sc, VMXNET3_BAR1_DSL, ds_pa);
	WRITE_BAR1(sc, VMXNET3_BAR1_DSH, (u_int64_t)ds_pa >> 32);
	return 0;
}

int
vmxnet3_alloc_txring(struct vmxnet3_softc *sc, int queue)
{
	struct vmxnet3_txqueue *tq = &sc->sc_txq[queue];
	struct vmxnet3_txq_shared *ts;
	struct vmxnet3_txring *ring = &tq->cmd_ring;
	struct vmxnet3_comp_ring *comp_ring = &tq->comp_ring;
	bus_addr_t pa, comp_pa;
	int idx;

	ring->txd = vmxnet3_dma_allocmem(sc, NTXDESC * sizeof ring->txd[0], 512, &pa);
	if (ring->txd == NULL)
		return -1;
	comp_ring->txcd = vmxnet3_dma_allocmem(sc,
	    NTXCOMPDESC * sizeof comp_ring->txcd[0], 512, &comp_pa);
	if (comp_ring->txcd == NULL)
		return -1;

	for (idx = 0; idx < NTXDESC; idx++) {
		if (bus_dmamap_create(sc->sc_dmat, JUMBO_LEN, NTXSEGS,
		    JUMBO_LEN, 0, BUS_DMA_NOWAIT, &ring->dmap[idx]))
			return -1;
	}

	ts = tq->ts;
	bzero(ts, sizeof *ts);
	ts->npending = 0;
	ts->intr_threshold = 1;
	ts->cmd_ring = pa;
	ts->cmd_ring_len = NTXDESC;
	ts->comp_ring = comp_pa;
	ts->comp_ring_len = NTXCOMPDESC;
	ts->driver_data = vtophys(tq);
	ts->driver_data_len = sizeof *tq;
	ts->intr_idx = 0;
	ts->stopped = 1;
	ts->error = 0;
	return 0;
}

int
vmxnet3_alloc_rxring(struct vmxnet3_softc *sc, int queue)
{
	struct vmxnet3_rxqueue *rq = &sc->sc_rxq[queue];
	struct vmxnet3_rxq_shared *rs;
	struct vmxnet3_rxring *ring;
	struct vmxnet3_comp_ring *comp_ring;
	bus_addr_t pa[2], comp_pa;
	int i, idx;

	for (i = 0; i < 2; i++) {
		ring = &rq->cmd_ring[i];
		ring->rxd = vmxnet3_dma_allocmem(sc, NRXDESC * sizeof ring->rxd[0],
		    512, &pa[i]);
		if (ring->rxd == NULL)
			return -1;
	}
	comp_ring = &rq->comp_ring;
	comp_ring->rxcd = vmxnet3_dma_allocmem(sc,
	    NRXCOMPDESC * sizeof comp_ring->rxcd[0], 512, &comp_pa);
	if (comp_ring->rxcd == NULL)
		return -1;

	for (i = 0; i < 2; i++) {
		ring = &rq->cmd_ring[i];
		ring->rid = i;
		for (idx = 0; idx < NRXDESC; idx++) {
			if (bus_dmamap_create(sc->sc_dmat, JUMBO_LEN, 1,
			    JUMBO_LEN, 0, BUS_DMA_NOWAIT, &ring->dmap[idx]))
				return -1;
		}
	}

	rs = rq->rs;
	bzero(rs, sizeof *rs);
	rs->cmd_ring[0] = pa[0];
	rs->cmd_ring[1] = pa[1];
	rs->cmd_ring_len[0] = NRXDESC;
	rs->cmd_ring_len[1] = NRXDESC;
	rs->comp_ring = comp_pa;
	rs->comp_ring_len = NRXCOMPDESC;
	rs->driver_data = vtophys(rq);
	rs->driver_data_len = sizeof *rq;
	rs->intr_idx = 0;
	rs->stopped = 1;
	rs->error = 0;
	return 0;
}

void
vmxnet3_txinit(struct vmxnet3_softc *sc, struct vmxnet3_txqueue *tq)
{
	struct vmxnet3_txring *ring = &tq->cmd_ring;
	struct vmxnet3_comp_ring *comp_ring = &tq->comp_ring;

	ring->cons = ring->prod = 0;
	ring->free = NTXDESC;
	ring->gen = 1;
	comp_ring->next = 0;
	comp_ring->gen = 1;
	bzero(ring->txd, NTXDESC * sizeof ring->txd[0]);
	bzero(comp_ring->txcd, NTXCOMPDESC * sizeof comp_ring->txcd[0]);
}

void
vmxnet3_rxinit(struct vmxnet3_softc *sc, struct vmxnet3_rxqueue *rq)
{
	struct vmxnet3_rxring *ring;
	struct vmxnet3_comp_ring *comp_ring;
	int i;
	u_int slots;

	for (i = 0; i < 2; i++) {
		ring = &rq->cmd_ring[i];
		ring->fill = 0;
		ring->gen = 1;
		bzero(ring->rxd, NRXDESC * sizeof ring->rxd[0]);
		if_rxr_init(&ring->rxr, 2, NRXDESC - 1);
		for (slots = if_rxr_get(&ring->rxr, NRXDESC);
		    slots > 0; slots--) {
			if (vmxnet3_getbuf(sc, ring))
				break;
		}
		if_rxr_put(&ring->rxr, slots);
	}
	comp_ring = &rq->comp_ring;
	comp_ring->next = 0;
	comp_ring->gen = 1;
	bzero(comp_ring->rxcd, NRXCOMPDESC * sizeof comp_ring->rxcd[0]);
}

void
vmxnet3_txstop(struct vmxnet3_softc *sc, struct vmxnet3_txqueue *tq)
{
	struct vmxnet3_txring *ring = &tq->cmd_ring;
	int idx;

	for (idx = 0; idx < NTXDESC; idx++) {
		if (ring->m[idx]) {
			bus_dmamap_unload(sc->sc_dmat, ring->dmap[idx]);
			m_freem(ring->m[idx]);
			ring->m[idx] = NULL;
		}
	}
}

void
vmxnet3_rxstop(struct vmxnet3_softc *sc, struct vmxnet3_rxqueue *rq)
{
	struct vmxnet3_rxring *ring;
	int i, idx;

	for (i = 0; i < 2; i++) {
		ring = &rq->cmd_ring[i];
		for (idx = 0; idx < NRXDESC; idx++) {
			if (ring->m[idx]) {
				m_freem(ring->m[idx]);
				ring->m[idx] = NULL;
			}
		}
	}
}

void
vmxnet3_link_state(struct vmxnet3_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	u_int x, link, speed;

	WRITE_CMD(sc, VMXNET3_CMD_GET_LINK);
	x = READ_BAR1(sc, VMXNET3_BAR1_CMD);
	speed = x >> 16;
	if (x & 1) {
		ifp->if_baudrate = IF_Mbps(speed);
		link = LINK_STATE_UP;
	} else
		link = LINK_STATE_DOWN;

	if (ifp->if_link_state != link) {
		ifp->if_link_state = link;
		if_link_state_change(ifp);
	}
}

static inline void
vmxnet3_enable_intr(struct vmxnet3_softc *sc, int irq)
{
	WRITE_BAR0(sc, VMXNET3_BAR0_IMASK(irq), 0);
}

static inline void
vmxnet3_disable_intr(struct vmxnet3_softc *sc, int irq)
{
	WRITE_BAR0(sc, VMXNET3_BAR0_IMASK(irq), 1);
}

void
vmxnet3_enable_all_intrs(struct vmxnet3_softc *sc)
{
	int i;

	sc->sc_ds->ictrl &= ~VMXNET3_ICTRL_DISABLE_ALL;
	for (i = 0; i < VMXNET3_NINTR; i++)
		vmxnet3_enable_intr(sc, i);
}

void
vmxnet3_disable_all_intrs(struct vmxnet3_softc *sc)
{
	int i;

	sc->sc_ds->ictrl |= VMXNET3_ICTRL_DISABLE_ALL;
	for (i = 0; i < VMXNET3_NINTR; i++)
		vmxnet3_disable_intr(sc, i);
}

int
vmxnet3_intr(void *arg)
{
	struct vmxnet3_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;

	if (READ_BAR1(sc, VMXNET3_BAR1_INTR) == 0)
		return 0;

	if (sc->sc_ds->event) {
		KERNEL_LOCK();
		vmxnet3_evintr(sc);
		KERNEL_UNLOCK();
	}

	if (ifp->if_flags & IFF_RUNNING) {
		vmxnet3_rxintr(sc, &sc->sc_rxq[0]);
		vmxnet3_txintr(sc, &sc->sc_txq[0]);
		vmxnet3_enable_intr(sc, 0);
	}

	return 1;
}

void
vmxnet3_evintr(struct vmxnet3_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	u_int event = sc->sc_ds->event;
	struct vmxnet3_txq_shared *ts;
	struct vmxnet3_rxq_shared *rs;

	/* Clear events. */
	WRITE_BAR1(sc, VMXNET3_BAR1_EVENT, event);

	/* Link state change? */
	if (event & VMXNET3_EVENT_LINK)
		vmxnet3_link_state(sc);

	/* Queue error? */
	if (event & (VMXNET3_EVENT_TQERROR | VMXNET3_EVENT_RQERROR)) {
		WRITE_CMD(sc, VMXNET3_CMD_GET_STATUS);

		ts = sc->sc_txq[0].ts;
		if (ts->stopped)
			printf("%s: TX error 0x%x\n", ifp->if_xname, ts->error);
		rs = sc->sc_rxq[0].rs;
		if (rs->stopped)
			printf("%s: RX error 0x%x\n", ifp->if_xname, rs->error);
		vmxnet3_init(sc);
	}

	if (event & VMXNET3_EVENT_DIC)
		printf("%s: device implementation change event\n",
		    ifp->if_xname);
	if (event & VMXNET3_EVENT_DEBUG)
		printf("%s: debug event\n", ifp->if_xname);
}

void
vmxnet3_txintr(struct vmxnet3_softc *sc, struct vmxnet3_txqueue *tq)
{
	struct vmxnet3_txring *ring = &tq->cmd_ring;
	struct vmxnet3_comp_ring *comp_ring = &tq->comp_ring;
	struct vmxnet3_txcompdesc *txcd;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	bus_dmamap_t map;
	struct mbuf *m;
	u_int cons;
	u_int free = 0;

	cons = ring->cons;

	for (;;) {
		txcd = &comp_ring->txcd[comp_ring->next];

		if (letoh32((txcd->txc_word3 >> VMXNET3_TXC_GEN_S) &
		    VMXNET3_TXC_GEN_M) != comp_ring->gen)
			break;

		comp_ring->next++;
		if (comp_ring->next == NTXCOMPDESC) {
			comp_ring->next = 0;
			comp_ring->gen ^= 1;
		}

		m = ring->m[cons];
		ring->m[cons] = NULL;

		KASSERT(m != NULL);

		map = ring->dmap[cons];
		free += map->dm_nsegs;
		bus_dmamap_unload(sc->sc_dmat, map);
		m_freem(m);

		cons = (letoh32((txcd->txc_word0 >>
		    VMXNET3_TXC_EOPIDX_S) & VMXNET3_TXC_EOPIDX_M) + 1)
		    % NTXDESC;
	}

	ring->cons = cons;

	if (atomic_add_int_nv(&ring->free, free) == NTXDESC)
		ifp->if_timer = 0;

	if (ifq_is_oactive(&ifp->if_snd)) {
		KERNEL_LOCK();
		ifq_clr_oactive(&ifp->if_snd);
		vmxnet3_start(ifp);
		KERNEL_UNLOCK();
	}
}

void
vmxnet3_rxintr(struct vmxnet3_softc *sc, struct vmxnet3_rxqueue *rq)
{
	struct vmxnet3_comp_ring *comp_ring = &rq->comp_ring;
	struct vmxnet3_rxring *ring;
	struct vmxnet3_rxdesc *rxd;
	struct vmxnet3_rxcompdesc *rxcd;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *m;
	int idx, len;
	u_int slots;

	for (;;) {
		rxcd = &comp_ring->rxcd[comp_ring->next];
		if (letoh32((rxcd->rxc_word3 >> VMXNET3_RXC_GEN_S) &
		    VMXNET3_RXC_GEN_M) != comp_ring->gen)
			break;

		comp_ring->next++;
		if (comp_ring->next == NRXCOMPDESC) {
			comp_ring->next = 0;
			comp_ring->gen ^= 1;
		}

		idx = letoh32((rxcd->rxc_word0 >> VMXNET3_RXC_IDX_S) &
		    VMXNET3_RXC_IDX_M);
		if (letoh32((rxcd->rxc_word0 >> VMXNET3_RXC_QID_S) &
		    VMXNET3_RXC_QID_M) < NRXQUEUE)
			ring = &rq->cmd_ring[0];
		else
			ring = &rq->cmd_ring[1];
		rxd = &ring->rxd[idx];
		len = letoh32((rxcd->rxc_word2 >> VMXNET3_RXC_LEN_S) &
		    VMXNET3_RXC_LEN_M);
		m = ring->m[idx];
		ring->m[idx] = NULL;
		if_rxr_put(&ring->rxr, 1);
		bus_dmamap_unload(sc->sc_dmat, ring->dmap[idx]);

		if (m == NULL)
			panic("%s: NULL ring->m[%u]", __func__, idx);

		if (letoh32((rxd->rx_word2 >> VMXNET3_RX_BTYPE_S) &
		    VMXNET3_RX_BTYPE_M) != VMXNET3_BTYPE_HEAD) {
			m_freem(m);
			goto skip_buffer;
		}
		if (letoh32(rxcd->rxc_word2 & VMXNET3_RXC_ERROR)) {
			ifp->if_ierrors++;
			m_freem(m);
			goto skip_buffer;
		}
		if (len < VMXNET3_MIN_MTU) {
			m_freem(m);
			goto skip_buffer;
		}

		vmxnet3_rx_csum(rxcd, m);
		m->m_pkthdr.len = m->m_len = len;
		if (letoh32(rxcd->rxc_word2 & VMXNET3_RXC_VLAN)) {
			m->m_flags |= M_VLANTAG;
			m->m_pkthdr.ether_vtag = letoh32((rxcd->rxc_word2 >>
			    VMXNET3_RXC_VLANTAG_S) & VMXNET3_RXC_VLANTAG_M);
		}

		ml_enqueue(&ml, m);

skip_buffer:
#ifdef VMXNET3_STAT
		vmxstat.rxdone = idx;
#endif
		if (rq->rs->update_rxhead) {
			u_int qid = letoh32((rxcd->rxc_word0 >>
			    VMXNET3_RXC_QID_S) & VMXNET3_RXC_QID_M);

			idx = (idx + 1) % NRXDESC;
			if (qid < NRXQUEUE) {
				WRITE_BAR0(sc, VMXNET3_BAR0_RXH1(qid), idx);
			} else {
				qid -= NRXQUEUE;
				WRITE_BAR0(sc, VMXNET3_BAR0_RXH2(qid), idx);
			}
		}
	}

	if_input(ifp, &ml);

	/* XXX Should we (try to) allocate buffers for ring 2 too? */
	ring = &rq->cmd_ring[0];
	for (slots = if_rxr_get(&ring->rxr, NRXDESC); slots > 0; slots--) {
		if (vmxnet3_getbuf(sc, ring))
			break;
	}
	if_rxr_put(&ring->rxr, slots);
}

void
vmxnet3_iff(struct vmxnet3_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct arpcom *ac = &sc->sc_arpcom;
	struct vmxnet3_driver_shared *ds = sc->sc_ds;
	struct ether_multi *enm;
	struct ether_multistep step;
	u_int mode;
	u_int8_t *p;

	ds->mcast_tablelen = 0;
	CLR(ifp->if_flags, IFF_ALLMULTI);

	/*
	 * Always accept broadcast frames.
	 * Always accept frames destined to our station address.
	 */
	mode = VMXNET3_RXMODE_BCAST | VMXNET3_RXMODE_UCAST;

	if (ISSET(ifp->if_flags, IFF_PROMISC) || ac->ac_multirangecnt > 0 ||
	    ac->ac_multicnt > 682) {
		SET(ifp->if_flags, IFF_ALLMULTI);
		SET(mode, (VMXNET3_RXMODE_ALLMULTI | VMXNET3_RXMODE_MCAST));
		if (ifp->if_flags & IFF_PROMISC)
			SET(mode, VMXNET3_RXMODE_PROMISC);
	} else {
		p = sc->sc_mcast;
		ETHER_FIRST_MULTI(step, ac, enm);
		while (enm != NULL) {
			bcopy(enm->enm_addrlo, p, ETHER_ADDR_LEN);

			p += ETHER_ADDR_LEN;

			ETHER_NEXT_MULTI(step, enm);
		}

		if (ac->ac_multicnt > 0) {
			SET(mode, VMXNET3_RXMODE_MCAST);
			ds->mcast_tablelen = p - sc->sc_mcast;
		}
	}

	WRITE_CMD(sc, VMXNET3_CMD_SET_FILTER);
	ds->rxmode = mode;
	WRITE_CMD(sc, VMXNET3_CMD_SET_RXMODE);
}


void
vmxnet3_rx_csum(struct vmxnet3_rxcompdesc *rxcd, struct mbuf *m)
{
	if (letoh32(rxcd->rxc_word0 & VMXNET3_RXC_NOCSUM))
		return;

	if ((rxcd->rxc_word3 & (VMXNET3_RXC_IPV4 | VMXNET3_RXC_IPSUM_OK)) ==
	    (VMXNET3_RXC_IPV4 | VMXNET3_RXC_IPSUM_OK))
		m->m_pkthdr.csum_flags |= M_IPV4_CSUM_IN_OK;

	if (rxcd->rxc_word3 & VMXNET3_RXC_FRAGMENT)
		return;

	if (rxcd->rxc_word3 & (VMXNET3_RXC_TCP | VMXNET3_RXC_UDP)) {
		if (rxcd->rxc_word3 & VMXNET3_RXC_CSUM_OK)
			m->m_pkthdr.csum_flags |=
			    M_TCP_CSUM_IN_OK | M_UDP_CSUM_IN_OK;
	}
}

int
vmxnet3_getbuf(struct vmxnet3_softc *sc, struct vmxnet3_rxring *ring)
{
	int idx = ring->fill;
	struct vmxnet3_rxdesc *rxd = &ring->rxd[idx];
	struct mbuf *m;
	int btype;

	if (ring->m[idx])
		panic("vmxnet3_getbuf: buffer has mbuf");

#if 1
	/* XXX Don't allocate buffers for ring 2 for now. */
	if (ring->rid != 0)
		return -1;
	btype = VMXNET3_BTYPE_HEAD;
#else
	if (ring->rid == 0)
		btype = VMXNET3_BTYPE_HEAD;
	else
		btype = VMXNET3_BTYPE_BODY;
#endif

	m = MCLGETI(NULL, M_DONTWAIT, NULL, JUMBO_LEN);
	if (m == NULL)
		return -1;

	m->m_pkthdr.len = m->m_len = JUMBO_LEN;
	m_adj(m, ETHER_ALIGN);
	ring->m[idx] = m;

	if (bus_dmamap_load_mbuf(sc->sc_dmat, ring->dmap[idx], m,
	    BUS_DMA_NOWAIT))
		panic("load mbuf");
	rxd->rx_addr = htole64(DMAADDR(ring->dmap[idx]));
	rxd->rx_word2 = htole32(((m->m_pkthdr.len & VMXNET3_RX_LEN_M) <<
	    VMXNET3_RX_LEN_S) | ((btype & VMXNET3_RX_BTYPE_M) <<
	    VMXNET3_RX_BTYPE_S) | ((ring->gen & VMXNET3_RX_GEN_M) <<
	    VMXNET3_RX_GEN_S));
	idx++;
	if (idx == NRXDESC) {
		idx = 0;
		ring->gen ^= 1;
	}
	ring->fill = idx;
#ifdef VMXNET3_STAT
	vmxstat.rxfill = ring->fill;
#endif
	return 0;
}

void
vmxnet3_stop(struct ifnet *ifp)
{
	struct vmxnet3_softc *sc = ifp->if_softc;
	int queue;

	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	ifp->if_timer = 0;

	vmxnet3_disable_all_intrs(sc);

	WRITE_CMD(sc, VMXNET3_CMD_DISABLE);

	intr_barrier(sc->sc_ih);

	for (queue = 0; queue < NTXQUEUE; queue++)
		vmxnet3_txstop(sc, &sc->sc_txq[queue]);
	for (queue = 0; queue < NRXQUEUE; queue++)
		vmxnet3_rxstop(sc, &sc->sc_rxq[queue]);
}

void
vmxnet3_reset(struct vmxnet3_softc *sc)
{
	WRITE_CMD(sc, VMXNET3_CMD_RESET);
}

int
vmxnet3_init(struct vmxnet3_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	int queue;

	/*
	 * Cancel pending I/O and free all RX/TX buffers.
	 */
	vmxnet3_stop(ifp);

#if 0
	/* Put controller into known state. */
	vmxnet3_reset(sc);
#endif

	for (queue = 0; queue < NTXQUEUE; queue++)
		vmxnet3_txinit(sc, &sc->sc_txq[queue]);
	for (queue = 0; queue < NRXQUEUE; queue++)
		vmxnet3_rxinit(sc, &sc->sc_rxq[queue]);

	for (queue = 0; queue < NRXQUEUE; queue++) {
		WRITE_BAR0(sc, VMXNET3_BAR0_RXH1(queue), 0);
		WRITE_BAR0(sc, VMXNET3_BAR0_RXH2(queue), 0);
	}

	WRITE_CMD(sc, VMXNET3_CMD_ENABLE);
	if (READ_BAR1(sc, VMXNET3_BAR1_CMD)) {
		printf("%s: failed to initialize\n", ifp->if_xname);
		vmxnet3_stop(ifp);
		return EIO;
	}

	/* Program promiscuous mode and multicast filters. */
	vmxnet3_iff(sc);

	vmxnet3_enable_all_intrs(sc);

	vmxnet3_link_state(sc);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	return 0;
}

int
vmxnet3_ioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct vmxnet3_softc *sc = ifp->if_softc;
	struct ifreq *ifr = (struct ifreq *)data;
	int error = 0, s;

	s = splnet();

	switch (cmd) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if ((ifp->if_flags & IFF_RUNNING) == 0)
			error = vmxnet3_init(sc);
		break;
	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				error = vmxnet3_init(sc);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				vmxnet3_stop(ifp);
		}
		break;
	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &sc->sc_media, cmd);
		break;
	case SIOCGIFRXR:
		error = if_rxr_ioctl((struct if_rxrinfo *)ifr->ifr_data,
		    NULL, JUMBO_LEN, &sc->sc_rxq[0].cmd_ring[0].rxr);
		break;
	default:
		error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data);
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			vmxnet3_iff(sc);
		error = 0;
	}

	splx(s);
	return error;
}

void
vmxnet3_start(struct ifnet *ifp)
{
	struct vmxnet3_softc *sc = ifp->if_softc;
	struct vmxnet3_txqueue *tq = sc->sc_txq;
	struct vmxnet3_txring *ring = &tq->cmd_ring;
	struct vmxnet3_txdesc *txd;
	struct mbuf *m;
	u_int free, used;
	int n;

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;

	free = ring->free;
	used = 0;

	for (;;) {
		if (used + NTXSEGS > free) {
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

		IFQ_DEQUEUE(&ifp->if_snd, m);
		if (m == NULL)
			break;

		txd = &ring->txd[ring->prod];

		n = vmxnet3_load_mbuf(sc, ring, &m);
		if (n == -1) {
			ifp->if_oerrors++;
			continue;
		}

#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif

		/* Change the ownership by flipping the "generation" bit */
		txd->tx_word2 ^= htole32(VMXNET3_TX_GEN_M << VMXNET3_TX_GEN_S);

		used += n;
	}

	if (used > 0) {
		ifp->if_timer = 5;
		atomic_sub_int(&ring->free, used);
		WRITE_BAR0(sc, VMXNET3_BAR0_TXH(0), ring->prod);
	}
}

int
vmxnet3_load_mbuf(struct vmxnet3_softc *sc, struct vmxnet3_txring *ring,
    struct mbuf **mp)
{
	struct vmxnet3_txdesc *txd, *sop;
	struct mbuf *n, *m = *mp;
	bus_dmamap_t map;
	u_int hlen = ETHER_HDR_LEN, csum_off;
	u_int prod;
	int gen, i;

	prod = ring->prod;
	map = ring->dmap[prod];
#if 0
	if (m->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT) {
		printf("%s: IP checksum offloading is not supported\n",
		    sc->sc_dev.dv_xname);
		return -1;
	}
#endif
	if (m->m_pkthdr.csum_flags & (M_TCP_CSUM_OUT|M_UDP_CSUM_OUT)) {
		struct ip *ip;
		int offp;

		if (m->m_pkthdr.csum_flags & M_TCP_CSUM_OUT)
			csum_off = offsetof(struct tcphdr, th_sum);
		else
			csum_off = offsetof(struct udphdr, uh_sum);

		n = m_pulldown(m, hlen, sizeof(*ip), &offp);
		if (n == NULL)
			return (-1);

		ip = (struct ip *)(n->m_data + offp);
		hlen += ip->ip_hl << 2;

		*mp = m_pullup(m, hlen + csum_off + 2);
		if (*mp == NULL)
			return (-1);
		m = *mp;
	}

	switch (bus_dmamap_load_mbuf(sc->sc_dmat, map, m, BUS_DMA_NOWAIT)) {
	case 0:
		break;
	case EFBIG:
		if (m_defrag(m, M_DONTWAIT) == 0 &&
		    bus_dmamap_load_mbuf(sc->sc_dmat, map, m,
		     BUS_DMA_NOWAIT) == 0)
			break;

		/* FALLTHROUGH */
	default:
		m_freem(m);
		return -1;
	}

	ring->m[prod] = m;

	sop = &ring->txd[prod];
	gen = ring->gen ^ 1;		/* owned by cpu (yet) */

	for (i = 0; i < map->dm_nsegs; i++) {
		txd = &ring->txd[prod];
		txd->tx_addr = htole64(map->dm_segs[i].ds_addr);
		txd->tx_word2 = htole32(((map->dm_segs[i].ds_len &
		    VMXNET3_TX_LEN_M) << VMXNET3_TX_LEN_S) |
		    ((gen & VMXNET3_TX_GEN_M) << VMXNET3_TX_GEN_S));
		txd->tx_word3 = 0;

		if (++prod == NTXDESC) {
			prod = 0;
			ring->gen ^= 1;
		}

		gen = ring->gen;
	}
	txd->tx_word3 |= htole32(VMXNET3_TX_EOP | VMXNET3_TX_COMPREQ);

	if (m->m_flags & M_VLANTAG) {
		sop->tx_word3 |= htole32(VMXNET3_TX_VTAG_MODE);
		sop->tx_word3 |= htole32((m->m_pkthdr.ether_vtag &
		    VMXNET3_TX_VLANTAG_M) << VMXNET3_TX_VLANTAG_S);
	}
	if (m->m_pkthdr.csum_flags & (M_TCP_CSUM_OUT|M_UDP_CSUM_OUT)) {
		sop->tx_word2 |= htole32(((hlen + csum_off) &
		    VMXNET3_TX_OP_M) << VMXNET3_TX_OP_S);
		sop->tx_word3 |= htole32(((hlen & VMXNET3_TX_HLEN_M) <<
		    VMXNET3_TX_HLEN_S) | (VMXNET3_OM_CSUM << VMXNET3_TX_OM_S));
	}

	/* dmamap_sync map */

	ring->prod = prod;

	return (map->dm_nsegs);
}

void
vmxnet3_watchdog(struct ifnet *ifp)
{
	struct vmxnet3_softc *sc = ifp->if_softc;
	int s;

	printf("%s: device timeout\n", ifp->if_xname);
	s = splnet();
	vmxnet3_init(sc);
	splx(s);
}

void
vmxnet3_media_status(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct vmxnet3_softc *sc = ifp->if_softc;

	vmxnet3_link_state(sc);

	ifmr->ifm_status = IFM_AVALID;
	ifmr->ifm_active = IFM_ETHER;

	if (ifp->if_link_state != LINK_STATE_UP)
		return;

	ifmr->ifm_status |= IFM_ACTIVE;

	if (ifp->if_baudrate >= IF_Gbps(10))
		ifmr->ifm_active |= IFM_10G_T;
}

int
vmxnet3_media_change(struct ifnet *ifp)
{
	return 0;
}

void *
vmxnet3_dma_allocmem(struct vmxnet3_softc *sc, u_int size, u_int align, bus_addr_t *pa)
{
	bus_dma_tag_t t = sc->sc_dmat;
	bus_dma_segment_t segs[1];
	bus_dmamap_t map;
	caddr_t va;
	int n;

	if (bus_dmamem_alloc(t, size, align, 0, segs, 1, &n, BUS_DMA_NOWAIT))
		return NULL;
	if (bus_dmamem_map(t, segs, 1, size, &va, BUS_DMA_NOWAIT))
		return NULL;
	if (bus_dmamap_create(t, size, 1, size, 0, BUS_DMA_NOWAIT, &map))
		return NULL;
	if (bus_dmamap_load(t, map, va, size, NULL, BUS_DMA_NOWAIT))
		return NULL;
	bzero(va, size);
	*pa = DMAADDR(map);
	bus_dmamap_unload(t, map);
	bus_dmamap_destroy(t, map);
	return va;
}
@


1.44
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.43 2016/01/26 10:23:19 reyk Exp $	*/
a1083 1
		ifp->if_opackets++;
@


1.43
log
@Improve the previous fix: call vmxnet3_load_mbuf, bpf_mtap, and flip
the generation bit to pass the tx descriptor and mbuf to the "hardware".
This way bpf is not called if vmxnet3_load_mbuf dropped the mbuf.

Tested by me
OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.42 2016/01/25 10:39:20 reyk Exp $	*/
a276 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.42
log
@In vmxnet3_start(), do not send the mbuf to bpf after passing it to
the hardware.  This could have resulted in a page fault when the mbuf
has already been freed by the TX interrupt handler on another CPU.

This has the slight drawback that bpf can be called before the packet
is eventually dropped by vmxnet3_load_mbuf() - but I'm getting this
simple and verified fix in before doing further optimizations on the
start handler.

As discussed with mikeb@@ jsg@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.41 2016/01/04 16:16:56 mikeb Exp $	*/
d1048 1
d1069 1
a1069 4
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
d1077 8
a1189 3

	/* Change the ownership by flipping the "generation" bit */
	sop->tx_word2 ^= htole32(VMXNET3_TX_GEN_M << VMXNET3_TX_GEN_S);
@


1.41
log
@Record the modified mbuf chain after transmit checksum setup code

Keep the modified chain pointer and pass it back to the calling code
so that it will get properly accounted for.  Change it to m_pullup
since m_pulldown with a zero offset is just as good.

Tested by yasuoka@@, myself and mxb at alumni ! chalmers ! se, thanks!
ok yasuoka, mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.40 2015/11/25 03:09:59 dlg Exp $	*/
d1068 5
a1077 5

#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
@


1.40
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.39 2015/11/24 15:25:20 mpi Exp $	*/
d169 1
a169 1
    struct mbuf *);
d1068 1
a1068 1
		n = vmxnet3_load_mbuf(sc, ring, m);
d1092 1
a1092 1
    struct mbuf *m)
d1095 1
a1110 1
		struct mbuf *mp;
d1119 2
a1120 2
		mp = m_pulldown(m, hlen, sizeof(*ip), &offp);
		if (mp == NULL)
d1123 1
a1123 1
		ip = (struct ip *)(mp->m_data + offp);
d1126 2
a1127 2
		mp = m_pulldown(m, 0, hlen + csum_off + 2, &offp);
		if (mp == NULL)
d1129 1
@


1.39
log
@No need for "vlan.h" if you don't check for "#if NVLAN > 0".
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.38 2015/11/24 13:45:06 mpi Exp $	*/
d695 1
a695 1
	if (ISSET(ifp->if_flags, IFF_OACTIVE)) {
d697 1
a697 1
		CLR(ifp->if_flags, IFF_OACTIVE);
d926 2
a927 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
d989 1
a989 1
	ifp->if_flags &= ~IFF_OACTIVE;
d1052 1
a1052 1
	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
d1060 1
a1060 1
			ifp->if_flags |= IFF_OACTIVE;
@


1.38
log
@No need to include <net/if_arp.h>

This header is only needed because <netinet/if_ether.h> declares a
structure that needs it.  But it turns out that <net/if.h> already
includes it as workaround.

A proper solution would be to stop declarting "struct ether_arp"
there.  But no driver should need this header.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.37 2015/11/24 13:33:17 mpi Exp $	*/
a19 1
#include "vlan.h"
@


1.37
log
@The only network driver needing <net/if_types.h> is upl(4) for IFT_OTHER.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.36 2015/11/23 10:52:43 mpi Exp $	*/
a31 1
#include <net/if_arp.h>
@


1.36
log
@Include <sys/atomic.h> when atomic operations are used.

This has been masked because <sys/srp.h> is pulled unconditionally.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.35 2015/11/14 17:54:57 mpi Exp $	*/
a33 1
#include <net/if_types.h>
@


1.35
log
@Do not include <net/if_vlan_var.h> when it's not necessary.

Because of the VLAN hacks in mpw(4) this file still contains the definition
of "struct ifvlan" which depends on <sys/refcnt.h> which in turns pull
<sys/atomic.h>...
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.34 2015/10/25 13:04:28 mpi Exp $	*/
d28 1
@


1.34
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.33 2015/09/20 22:26:18 dlg Exp $	*/
a39 2

#include <net/if_vlan_var.h>
@


1.33
log
@brad points out i need bpf_mtap_ether to reconstruct vlan headers
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.32 2015/09/20 02:01:22 dlg Exp $	*/
a1001 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a1010 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->sc_arpcom, ifa);
@


1.32
log
@need to keep bpf in the tx path. got a bit ahead of myself there...

noticed by brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.31 2015/09/18 03:53:44 dlg Exp $	*/
d1082 1
a1082 1
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_OUT);
@


1.31
log
@make vmx(4) interrupts mpsafe.

the vmx rx path is only touched in the interrupt handler, so it is
already guaranteed to be accessed by only one cpu at a time.

the tx path has been massaged so the the producer is only touched
by the start routine, and the consumer is only touched by the
interrupt path, and can therefore be run concurrently. the only
interlock is a count of the free descriptors.

if txintr clears IFF_OACTIVE, it takes the kernel lock before running
the start routine.

other interrupts, eg, link state handling, take the kernel lock.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.30 2015/06/24 09:40:54 mpi Exp $	*/
d1079 5
@


1.30
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.29 2015/06/04 17:10:33 mikeb Exp $	*/
d64 3
a66 2
	u_int head;
	u_int next;
d111 1
d172 2
a173 1
int vmxnet3_load_mbuf(struct vmxnet3_softc *, struct mbuf *);
d245 2
a246 2
	pci_intr_establish(pa->pa_pc, ih, IPL_NET, vmxnet3_intr, sc,
	    self->dv_xname);
d472 2
a473 1
	ring->head = ring->next = 0;
d601 3
a603 1
	if (sc->sc_ds->event)
d605 3
a607 3
#ifdef VMXNET3_STAT
	vmxstat.intr++;
#endif
d613 1
d659 6
a664 1
	u_int sop;
d679 11
a689 7
		sop = ring->next;
		if (ring->m[sop] == NULL)
			panic("%s: NULL ring->m[%u]", __func__, sop);
		m_freem(ring->m[sop]);
		ring->m[sop] = NULL;
		bus_dmamap_unload(sc->sc_dmat, ring->dmap[sop]);
		ring->next = (letoh32((txcd->txc_word0 >>
d692 6
d699 5
a703 1
		ifp->if_flags &= ~IFF_OACTIVE;
a704 3
	if (ring->head == ring->next)
		ifp->if_timer = 0;
	vmxnet3_start(ifp);
d937 2
d972 5
a983 5
	for (queue = 0; queue < NRXQUEUE; queue++) {
		WRITE_BAR0(sc, VMXNET3_BAR0_RXH1(queue), 0);
		WRITE_BAR0(sc, VMXNET3_BAR0_RXH2(queue), 0);
	}

d1052 1
a1052 1
	struct vmxnet3_txqueue *tq = &sc->sc_txq[0];
d1055 2
a1056 1
	int n = 0;
d1061 3
d1065 1
a1065 4
		IFQ_POLL(&ifp->if_snd, m);
		if (m == NULL)
			break;
		if ((ring->next - ring->head - 1) % NTXDESC < NTXSEGS) {
d1071 5
a1075 1
		if (vmxnet3_load_mbuf(sc, m) != 0) {
d1079 1
a1079 5
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
		ifp->if_timer = 5;
d1081 1
a1081 1
		n++;
d1084 5
a1088 8
	if (n > 0)
		WRITE_BAR0(sc, VMXNET3_BAR0_TXH(0), ring->head);
#ifdef VMXNET3_STAT
	vmxstat.txhead = ring->head;
	vmxstat.txdone = ring->next;
	vmxstat.maxtxlen =
	    max(vmxstat.maxtxlen, (ring->head - ring->next) % NTXDESC);
#endif
d1092 2
a1093 1
vmxnet3_load_mbuf(struct vmxnet3_softc *sc, struct mbuf *m)
a1094 2
	struct vmxnet3_txqueue *tq = &sc->sc_txq[0];
	struct vmxnet3_txring *ring = &tq->cmd_ring;
d1096 1
a1096 3
	struct mbuf *mp;
	struct ip *ip;
	bus_dmamap_t map = ring->dmap[ring->head];
d1098 2
a1099 1
	int offp, gen, i;
d1101 2
d1111 4
d1147 3
a1149 2
	ring->m[ring->head] = m;
	sop = &ring->txd[ring->head];
d1151 1
d1153 1
a1153 1
		txd = &ring->txd[ring->head];
d1159 3
a1161 3
		ring->head++;
		if (ring->head == NTXDESC) {
			ring->head = 0;
d1164 1
d1181 4
d1188 1
a1188 1
	return (0);
@


1.29
log
@Check if interface was stopped before calling rx/tx interrupt routines.

Report & tests by mxb@@alumni.chalmers.se, thanks!
OK deraadt, chris
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.28 2015/05/29 00:37:10 uebayasi Exp $	*/
a769 1
	ifp->if_ipackets += ml_len(&ml);
@


1.28
log
@Revert unrelated changes in previous.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d593 1
a598 2
	vmxnet3_rxintr(sc, &sc->sc_rxq[0]);
	vmxnet3_txintr(sc, &sc->sc_txq[0]);
d602 5
a606 1
	vmxnet3_enable_intr(sc, 0);
@


1.27
log
@Initial addition of ``Patrol Read'' support in bio(4), biocto(8), and
mfi(4).  Based on FreeBSD, but done without mfiutil(8).

OK deraadt@@
@
text
@a153 1
void vmxnet3_rxdump(struct vmxnet3_softc *);
a519 22
vmxnet3_rxdump(struct vmxnet3_softc *sc)
{
#if 0
	int queue, i;

	for (queue = 0; queue < NRXQUEUE; queue++) {
		struct vmxnet3_rxqueue *rq = &sc->sc_rxq[queue];

		for (i = 0; i < 2; i++) {
			struct vmxnet3_rxring *ring = &rq->cmd_ring[i];

			struct if_rxring *r = &ring->rxr;
			printf("ring%d: "
			    "adjusted=%d alive=%u cwm=%u lwm=%u hwm=%u\n",
			    i,
			    r->rxr_adjusted, r->rxr_alive, r->rxr_cwm, r->rxr_lwm, r->rxr_hwm);
		}
	}
#endif
}

void
a915 1
	vmxnet3_rxdump(sc);
a943 1
	vmxnet3_rxdump(sc);
@


1.26
log
@bump the number of tx and rx descriptors from 128 up to 512.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.25 2015/03/14 03:38:48 jsg Exp $	*/
d154 1
d521 22
d939 1
d968 1
@


1.25
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.24 2015/02/10 02:57:32 pelikan Exp $	*/
d52 1
a52 1
#define NTXDESC 128 /* tx ring size */
d54 1
a54 1
#define NRXDESC 128
@


1.24
log
@convert VMXNET drivers to ml_enqueue + if_input

ok dlg reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.23 2015/02/09 11:06:52 pelikan Exp $	*/
a46 1
#include <dev/pci/pcireg.h>
@


1.23
log
@fix print/panic messages + remove superfluous if_ibytes addition

ok reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.22 2014/12/22 02:28:52 tedu Exp $	*/
d690 1
a739 2
		ifp->if_ipackets++;

a740 1
		m->m_pkthdr.rcvif = ifp;
d748 1
a748 5
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_IN);
#endif
		ether_input_mbuf(ifp, m);
d767 3
@


1.22
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.21 2014/12/19 02:32:57 brad Exp $	*/
d667 1
a667 1
			panic("vmxnet3_txintr");
d722 1
a722 1
			panic("NULL mbuf");
a734 1
			printf("%s: short packet (%d)\n", ifp->if_xname, len);
a739 1
		ifp->if_ibytes += len;
@


1.21
log
@Rearrange mostly vmxnet3_init() to look like other Ethernet drivers.

ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.20 2014/08/26 23:55:28 dlg Exp $	*/
a990 1
#ifdef INET
a992 1
#endif
@


1.20
log
@dont base the mru on the mtu. unconditionally make it what the
hardware can do (9k). implement the rxr ioctl while here.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.19 2014/07/22 13:12:11 mpi Exp $	*/
d167 1
a167 1
void vmxnet3_reset(struct ifnet *);
d633 1
a633 1
		vmxnet3_reset(ifp);
a910 1
	vmxnet3_disable_all_intrs(sc);
d914 2
d925 1
a925 1
vmxnet3_reset(struct ifnet *ifp)
a926 3
	struct vmxnet3_softc *sc = ifp->if_softc;

	vmxnet3_stop(ifp);
a927 1
	vmxnet3_init(sc);
d936 9
a944 2
	ifp->if_flags |= IFF_RUNNING;
	ifp->if_flags &= ~IFF_OACTIVE;
d963 1
d965 1
d967 1
d969 4
a1170 1
	vmxnet3_stop(ifp);
@


1.19
log
@Fewer <netinet/in_systm.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.18 2014/07/08 08:54:00 stsp Exp $	*/
d135 1
a135 1
#define JUMBO_LEN (1024 * 9 - ETHER_ALIGN)
a169 1
int vmxnet3_change_mtu(struct vmxnet3_softc *, int);
d360 1
a360 1
	ds->mtu = ETHERMTU;
a965 15
vmxnet3_change_mtu(struct vmxnet3_softc *sc, int mtu)
{
	struct vmxnet3_driver_shared *ds = sc->sc_ds;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	int error;

	if (mtu < VMXNET3_MIN_MTU || mtu > VMXNET3_MAX_MTU)
		return EINVAL;
	vmxnet3_stop(ifp);
	ifp->if_mtu = ds->mtu = mtu;
	error = vmxnet3_init(sc);
	return error;
}

int
a995 3
	case SIOCSIFMTU:
		error = vmxnet3_change_mtu(sc, ifr->ifr_mtu);
		break;
d999 4
@


1.18
log
@Remove left-over call to removed function m_clsetwms().
ok mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.17 2014/07/08 05:35:19 dlg Exp $	*/
a36 1
#include <netinet/in_systm.h>
@


1.17
log
@cut things that relied on mclgeti for rx ring accounting/restriction over
to using if_rxr.

cut the reporting systat did over to the rxr ioctl.

tested as much as i can on alpha, amd64, and sparc64.
mpi@@ has run it on macppc.
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.16 2014/01/22 06:04:17 brad Exp $	*/
a281 2

	m_clsetwms(ifp, JUMBO_LEN, 2, NRXDESC - 1);
@


1.16
log
@- Unconditionally set IFCAP_VLAN_MTU
- Bring the receive filter handling in line with other drivers
- Simplify the RX checksum code a bit and only set the flags when the
  RX checksum is Ok

ok uebayasi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.15 2013/11/11 07:19:53 dlg Exp $	*/
d74 1
d487 2
a488 1
	int i, idx;
d495 3
a497 1
		for (idx = 0; idx < NRXDESC; idx++) {
d501 1
d696 1
d722 1
d782 1
a782 4
	for (;;) {
		idx = ring->fill;
		if (ring->m[idx])
			return;
d784 1
a784 1
			return;
d786 1
a862 1
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
d881 1
a881 1
	m = MCLGETI(NULL, M_DONTWAIT, ifp, JUMBO_LEN);
@


1.15
log
@bump the ring sizes up.

ok yasuoka@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.14 2013/11/08 22:36:22 dlg Exp $	*/
d273 1
d277 1
a277 1
		ifp->if_capabilities |= IFCAP_VLAN_MTU | IFCAP_VLAN_HWTAGGING;
d795 8
a802 5
	mode = VMXNET3_RXMODE_UCAST;
	if (ISSET(ifp->if_flags, IFF_BROADCAST))
		SET(mode, VMXNET3_RXMODE_BCAST);
	if (ISSET(ifp->if_flags, IFF_PROMISC))
		SET(mode, VMXNET3_RXMODE_PROMISC);
a803 1
	CLR(ifp->if_flags, IFF_ALLMULTI);
d807 4
a810 4
		SET(mode, VMXNET3_RXMODE_MCAST | VMXNET3_RXMODE_ALLMULTI);
		ds->mcast_tablelen = 0;
	} else if (ISSET(ifp->if_flags, IFF_MULTICAST) &&
	    ac->ac_multicnt > 0) {
d815 1
d817 1
a819 1
		ds->mcast_tablelen = p - sc->sc_mcast;
d821 4
a824 1
		SET(mode, VMXNET3_RXMODE_MCAST);
d839 2
a840 2
	if ((rxcd->rxc_word3 & (VMXNET3_RXC_IPV4|VMXNET3_RXC_IPSUM_OK)) ==
	    (VMXNET3_RXC_IPV4|VMXNET3_RXC_IPSUM_OK))
d846 1
a846 7
	if (rxcd->rxc_word3 & VMXNET3_RXC_TCP) {
		if (rxcd->rxc_word3 & VMXNET3_RXC_CSUM_OK)
			m->m_pkthdr.csum_flags |= M_TCP_CSUM_IN_OK;
		else
			m->m_pkthdr.csum_flags |= M_TCP_CSUM_IN_BAD;
	}
	if (rxcd->rxc_word3 & VMXNET3_RXC_UDP) {
d848 2
a849 3
			m->m_pkthdr.csum_flags |= M_UDP_CSUM_IN_OK;
		else
			m->m_pkthdr.csum_flags |= M_UDP_CSUM_IN_BAD;
@


1.14
log
@advertise the ring sizes through the stack. specifically:

- set the sndq size to the tx ring size so tx mitigation can hopefully
kick in

- advertise the size of the rx ring to mclgeti

- only uses jumbo clusters. mtu is about what the ip stack will
do for transmitted packets, we should happily receive packets up
to the size the hardware supports. the point of mclgeti was to make
it possible to do so without wasting too much memory.

ok yasuoka@@ uebayasi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.13 2013/09/08 02:18:00 reyk Exp $	*/
d54 1
a54 1
#define NTXDESC 64 /* tx ring size */
d56 1
a56 1
#define NRXDESC 64
@


1.13
log
@Unbreak vmx(4) on i386.  The right shift of 32 bits for the DSH
register (driver shared address high) exceeded the width of the 32bit
bus address; casting the address to a 64bit type will correctly result
in a zero value on i386 and the high bits on amd64.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.12 2013/08/28 10:19:19 reyk Exp $	*/
d135 1
a135 1
#define JUMBO_LEN (1024*9)
d277 2
d281 2
d860 1
a860 1
	int size, btype;
d877 1
a877 5
	size = ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN;
#if NVLAN > 0
	size += ETHER_VLAN_ENCAP_LEN;
#endif
	m = MCLGETI(NULL, M_DONTWAIT, ifp, size);
d881 1
a881 1
	m->m_pkthdr.len = m->m_len = size;
@


1.12
log
@vmx(4) uses 4 different types of 128bit descriptors in little-endian
format for Rx and Tx.  Replace the bit fields in the descriptor
structs with 32bit words to access them with traditional bit
operations using shifts and masks.  We try to avoid bit fields in
OpenBSD.  For consistence with other drivers, this change also uses
letoh32/htole32 endianess conversions even if it is very unlikely that
vmx will ever run on a big-endian VM/host.

discussed with uebayasi@@ and deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.11 2013/06/22 00:28:10 uebayasi Exp $	*/
d369 1
a369 1
	WRITE_BAR1(sc, VMXNET3_BAR1_DSH, ds_pa >> 32);
@


1.11
log
@Prefix all functions.  Pointed out by brad@@.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d648 3
a650 1
		if (txcd->gen != comp_ring->gen)
d665 3
a667 1
		ring->next = (txcd->eop_idx + 1) % NTXDESC;
d689 2
a690 1
		if (rxcd->gen != comp_ring->gen)
d699 4
a702 2
		idx = rxcd->rxd_idx;
		if (rxcd->qid < NRXQUEUE)
d707 2
a708 1
		len = rxcd->len;
d716 2
a717 1
		if (rxd->btype != VMXNET3_BTYPE_HEAD) {
d721 1
a721 1
		if (rxcd->error) {
d738 1
a738 1
		if (rxcd->vlan) {
d740 2
a741 1
			m->m_pkthdr.ether_vtag = rxcd->vtag;
d755 2
a756 1
			u_int qid = rxcd->qid;
d825 1
a825 1
	if (rxcd->no_csum)
d828 2
a829 1
	if (rxcd->ipv4 && rxcd->ipcsum_ok)
d832 1
a832 1
	if (rxcd->fragment)
d835 2
a836 2
	if (rxcd->tcp) {
		if (rxcd->csum_ok)
d841 2
a842 2
	if (rxcd->udp) {
		if (rxcd->csum_ok)
d888 5
a892 4
	rxd->addr = DMAADDR(ring->dmap[idx]);
	rxd->btype = btype;
	rxd->len = m->m_pkthdr.len;
	rxd->gen = ring->gen;
d1134 5
a1138 8
		txd->addr = map->dm_segs[i].ds_addr;
		txd->len = map->dm_segs[i].ds_len;
		txd->gen = gen;
		txd->dtype = 0;
		txd->offload_mode = VMXNET3_OM_NONE;
		txd->offload_pos = txd->hlen = 0;
		txd->eop = txd->compreq = 0;
		txd->vtag_mode = txd->vtag = 0;
d1146 1
a1146 1
	txd->eop = txd->compreq = 1;
d1149 3
a1151 2
		sop->vtag_mode = 1;
		sop->vtag = m->m_pkthdr.ether_vtag;
d1154 4
a1157 3
		sop->offload_mode = VMXNET3_OM_CSUM;
		sop->hlen = hlen;
		sop->offload_pos = hlen + csum_off;
d1160 2
a1161 2
	/* Change the ownership. */
	sop->gen ^= 1;
@


1.10
log
@De-static.
@
text
@d176 1
a176 1
void *dma_allocmem(struct vmxnet3_softc *, u_int, u_int, bus_addr_t *);
d304 1
a304 1
	ts = dma_allocmem(sc, qs_len, VMXNET3_DMADESC_ALIGN, &qs_pa);
d320 1
a320 1
	sc->sc_mcast = dma_allocmem(sc, 682 * ETHER_ADDR_LEN, 32, &mcast_pa);
d324 1
a324 1
	ds = dma_allocmem(sc, sizeof *sc->sc_ds, 8, &ds_pa);
d383 1
a383 1
	ring->txd = dma_allocmem(sc, NTXDESC * sizeof ring->txd[0], 512, &pa);
d386 1
a386 1
	comp_ring->txcd = dma_allocmem(sc,
d425 1
a425 1
		ring->rxd = dma_allocmem(sc, NRXDESC * sizeof ring->rxd[0],
d431 1
a431 1
	comp_ring->rxcd = dma_allocmem(sc,
d1193 1
a1193 1
dma_allocmem(struct vmxnet3_softc *sc, u_int size, u_int align, bus_addr_t *pa)
@


1.9
log
@Don't rely on __attribute__((__aligned__(x))) GCC extension.  Explicitly pad descriptors.

No binary change.
@
text
@d157 2
d163 1
d166 1
d170 1
d176 1
a176 1
static void *dma_allocmem(struct vmxnet3_softc *, u_int, u_int, bus_addr_t *);
d564 1
a564 1
static void
d574 1
a574 1
static void
d768 1
a768 1
static void
d892 1
a892 1
static void
d952 1
a952 1
static int
@


1.8
log
@Consistenly use bpf_mtap_ether().  From tsubai@@.
@
text
@d299 1
a299 1
	ts = dma_allocmem(sc, qs_len, 128, &qs_pa);
@


1.7
log
@Almost identical diffs from brad@@ and dlg@@:

o OpenBSD'ify the vmx(4) receive filter handling code
  o IFF_ALLMULTI is like hte OACTIVE flag in that its only ever set and
    cleared by an interface driver. with that in mind, this reorders
    the config to do that and take advantage of it to conditionally
    configure the multicast filtering.
  o It also makes the code check if any multicast ranges have been
    configured, which every other driver interprets as "set ALLMULTI",
    so we do too now.
o Add the usual ifdef INET guard to the ioctl code.

OK yasuoka@@ dlg@@
@
text
@d731 1
a731 1
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
@


1.6
log
@Make use of pci_matchbyid().

ok yasuoka@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.5 2013/06/08 17:07:31 brad Exp $	*/
d115 1
a115 1
	void *sc_mcast;
d764 1
a764 1
vmxnet3_set_rx_filter(struct vmxnet3_softc *sc)
d767 1
a768 2
	u_int mode = VMXNET3_RXMODE_UCAST;
	struct arpcom *ac = &sc->sc_arpcom;
d771 2
a772 11
	int n;
	char *p;

	if (ifp->if_flags & IFF_MULTICAST)
		mode |= VMXNET3_RXMODE_MCAST;
	if (ifp->if_flags & IFF_ALLMULTI)
		mode |= VMXNET3_RXMODE_ALLMULTI;
	if (ifp->if_flags & IFF_BROADCAST)
		mode |= VMXNET3_RXMODE_BCAST;
	if (ifp->if_flags & IFF_PROMISC)
		mode |= VMXNET3_RXMODE_PROMISC | VMXNET3_RXMODE_ALLMULTI;
d774 11
a784 2
	if ((mode & (VMXNET3_RXMODE_ALLMULTI | VMXNET3_RXMODE_MCAST))
	    != VMXNET3_RXMODE_MCAST) {
d786 10
a795 2
		goto setit;
	}
d797 1
a797 10
	n = sc->sc_arpcom.ac_multicnt;
	if (n == 0) {
		mode &= ~VMXNET3_RXMODE_MCAST;
		ds->mcast_tablelen = 0;
		goto setit;
	}
	if (n > 682) {
		mode |= VMXNET3_RXMODE_ALLMULTI;
		ds->mcast_tablelen = 0;
		goto setit;
a799 10
	p = sc->sc_mcast;
	ETHER_FIRST_MULTI(step, ac, enm);
	while (enm) {
		bcopy(enm->enm_addrlo, p, ETHER_ADDR_LEN);
		p += ETHER_ADDR_LEN;
		ETHER_NEXT_MULTI(step, enm);
	}
	ds->mcast_tablelen = n * ETHER_ADDR_LEN;

setit:
d941 1
a941 1
	vmxnet3_set_rx_filter(sc);
d977 1
d980 1
d1006 1
a1006 1
			vmxnet3_set_rx_filter(sc);
@


1.5
log
@Remove redundant code setting PCI_COMMAND_MASTER_ENABLE as this is already
taken care of by pci_mapreg_map().

Ok yasuoka@@ uebayasi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.4 2013/06/05 02:04:07 dlg Exp $	*/
d173 4
d188 1
a188 7
	struct pci_attach_args *pa = aux;

	switch (pa->pa_id) {
	case PCI_ID_CODE(PCI_VENDOR_VMWARE, PCI_PRODUCT_VMWARE_NET_3):
		return 1;
	}
	return 0;
@


1.4
log
@tweak vmxnet3_load_mbuf to use m_pulldown to safely reach into the mbuf for
the tcp/udp headers and to make theyre contiguous for the hypervisors
offload to work correctly.

use m_defrag instead of handrolling a copy of a heavily fragmented mbuf
into a single mbuf or cluster.

ok reyk@@ yasuoka@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.3 2013/06/03 21:08:21 reyk Exp $	*/
d201 1
a201 1
	u_int csr, memtype, ver, macl, mach;
a202 4

	csr = pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_COMMAND_STATUS_REG);
	csr |= PCI_COMMAND_MASTER_ENABLE;
	pci_conf_write(pa->pa_pc, pa->pa_tag, PCI_COMMAND_STATUS_REG, csr);
@


1.3
log
@vmxnet3_load_mbuf() could replace the mbuf but did not return the new
one which caused the following Tx bpf call to panic.  Instead of
moving the bpf hook in front of the vmxnet3_load_mbuf() function, we
return the updated mbuf because we want don't want to see packets that
don't go on the wire/chipset.

Analyzed and discussed with yasuoka@@ uebayasi@@
OK yasuoka@@ uebayasi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.2 2013/06/03 15:05:29 reyk Exp $	*/
d54 2
a55 1
#define NTXDESC 64
d167 1
a167 1
int vmxnet3_load_mbuf(struct vmxnet3_softc *, struct mbuf **);
d393 1
a393 1
		if (bus_dmamap_create(sc->sc_dmat, JUMBO_LEN, 8,
d1046 1
a1046 1
		if ((ring->next - ring->head - 1) % NTXDESC < 8) {
d1050 1
d1052 1
a1052 2
		if (vmxnet3_load_mbuf(sc, &m) != 0) {
			m_freem(m);
d1054 1
a1054 1
			break;
d1076 1
a1076 1
vmxnet3_load_mbuf(struct vmxnet3_softc *sc, struct mbuf **m0)
d1081 1
a1081 1
	struct mbuf *m = *m0, *n = NULL;
d1084 2
a1085 1
	int hlen, csum_off, error, nsegs, gen, i;
d1099 11
a1109 6
		if (m->m_len < ETHER_HDR_LEN + 1)
			goto copy_chain;
		ip = (void *)(m->m_data + ETHER_HDR_LEN);
		hlen = ip->ip_hl << 2;
		if (m->m_len < ETHER_HDR_LEN + hlen + csum_off + 2)
			goto copy_chain;
d1112 1
a1112 2
	error = bus_dmamap_load_mbuf(sc->sc_dmat, map, m, BUS_DMA_NOWAIT);
	switch (error) {
d1115 7
a1122 16
	copy_error:
		printf("%s: bus_dmamap_load failed\n", sc->sc_dev.dv_xname);
		return -1;
	case EFBIG:
	copy_chain:
		n = MCLGETI(NULL, M_DONTWAIT, NULL, m->m_pkthdr.len);
		if (n == NULL) {
			printf("%s: mbuf chain is too long\n",
			    sc->sc_dev.dv_xname);
			return -1;
		}
		m_copydata(m, 0, m->m_pkthdr.len, mtod(n, caddr_t));
		n->m_flags |= m->m_flags & M_VLANTAG;
		n->m_pkthdr.len = n->m_len = m->m_pkthdr.len;
		n->m_pkthdr.ether_vtag = m->m_pkthdr.ether_vtag;
		n->m_pkthdr.csum_flags = m->m_pkthdr.csum_flags;
a1123 8
		m = *m0 = n;
		if (bus_dmamap_load_mbuf(sc->sc_dmat, map, m, BUS_DMA_NOWAIT))
			goto copy_error;
	}
	nsegs = map->dm_nsegs;
	if (nsegs > (ring->next - ring->head - 1) % NTXDESC) {
		struct ifnet *ifp = &sc->sc_arpcom.ac_if;
		ifp->if_flags |= IFF_OACTIVE;
d1130 1
a1130 1
	for (i = 0; i < nsegs; i++) {
d1155 2
a1156 2
		sop->hlen = ETHER_HDR_LEN + hlen;
		sop->offload_pos = ETHER_HDR_LEN + hlen + csum_off;
d1161 2
a1162 1
	return 0;
@


1.2
log
@Use IF_POLL to check for available transmit descriptors before IF_DEQUEUE.

ok uebayasi@@ yasuoka@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vmx.c,v 1.1 2013/05/31 20:14:18 uebayasi Exp $	*/
d166 1
a166 1
int vmxnet3_load_mbuf(struct vmxnet3_softc *, struct mbuf *);
d1050 1
a1050 1
		if (vmxnet3_load_mbuf(sc, m)) {
d1075 1
a1075 1
vmxnet3_load_mbuf(struct vmxnet3_softc *sc, struct mbuf *m)
d1080 1
a1080 1
	struct mbuf *n;
d1127 1
a1127 1
		m = n;
@


1.1
log
@Add vmx(4), driver for VMware's VMXNET3 ethernet controller, written for IIJ.

Tested by reyk@@, yasuoka@@

OK'ed by reyk@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1041 4
a1044 1
	while (IFQ_LEN(&ifp->if_snd) > 0) {
@

