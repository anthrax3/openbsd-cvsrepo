head	1.163;
access;
symbols
	OPENBSD_6_2_BASE:1.163
	OPENBSD_6_1:1.163.0.4
	OPENBSD_6_1_BASE:1.163
	OPENBSD_6_0:1.162.0.4
	OPENBSD_6_0_BASE:1.162
	OPENBSD_5_9:1.161.0.2
	OPENBSD_5_9_BASE:1.161
	OPENBSD_5_8:1.160.0.6
	OPENBSD_5_8_BASE:1.160
	OPENBSD_5_7:1.160.0.4
	OPENBSD_5_7_BASE:1.160
	OPENBSD_5_6:1.159.0.4
	OPENBSD_5_6_BASE:1.159
	OPENBSD_5_5:1.156.0.4
	OPENBSD_5_5_BASE:1.156
	OPENBSD_5_4:1.155.0.6
	OPENBSD_5_4_BASE:1.155
	OPENBSD_5_3:1.155.0.4
	OPENBSD_5_3_BASE:1.155
	OPENBSD_5_2:1.155.0.2
	OPENBSD_5_2_BASE:1.155
	OPENBSD_5_1_BASE:1.154
	OPENBSD_5_1:1.154.0.2
	OPENBSD_5_0:1.153.0.2
	OPENBSD_5_0_BASE:1.153
	OPENBSD_4_9:1.150.0.2
	OPENBSD_4_9_BASE:1.150
	OPENBSD_4_8:1.147.0.2
	OPENBSD_4_8_BASE:1.147
	OPENBSD_4_7:1.145.0.2
	OPENBSD_4_7_BASE:1.145
	OPENBSD_4_6:1.143.0.4
	OPENBSD_4_6_BASE:1.143
	OPENBSD_4_5:1.141.0.4
	OPENBSD_4_5_BASE:1.141
	OPENBSD_4_4:1.141.0.2
	OPENBSD_4_4_BASE:1.141
	OPENBSD_4_3:1.140.0.2
	OPENBSD_4_3_BASE:1.140
	OPENBSD_4_2:1.138.0.4
	OPENBSD_4_2_BASE:1.138
	OPENBSD_4_1:1.138.0.2
	OPENBSD_4_1_BASE:1.138
	OPENBSD_4_0:1.137.0.2
	OPENBSD_4_0_BASE:1.137
	OPENBSD_3_9:1.136.0.4
	OPENBSD_3_9_BASE:1.136
	OPENBSD_3_8:1.136.0.2
	OPENBSD_3_8_BASE:1.136
	OPENBSD_3_7:1.135.0.4
	OPENBSD_3_7_BASE:1.135
	OPENBSD_3_6:1.135.0.2
	OPENBSD_3_6_BASE:1.135
	SMP_SYNC_A:1.135
	SMP_SYNC_B:1.135
	OPENBSD_3_5:1.133.0.2
	OPENBSD_3_5_BASE:1.133
	OPENBSD_3_4:1.131.0.2
	OPENBSD_3_4_BASE:1.131
	UBC_SYNC_A:1.126
	OPENBSD_3_3:1.123.0.2
	OPENBSD_3_3_BASE:1.123
	OPENBSD_3_2:1.115.0.2
	OPENBSD_3_2_BASE:1.115
	OPENBSD_3_1:1.84.0.2
	OPENBSD_3_1_BASE:1.84
	UBC_SYNC_B:1.118
	UBC:1.76.0.2
	UBC_BASE:1.76
	OPENBSD_3_0:1.70.0.2
	OPENBSD_3_0_BASE:1.70
	SMP:1.49.0.2
	OPENBSD_2_9_BASE:1.44
	OPENBSD_2_9:1.44.0.2
	OPENBSD_2_8:1.33.0.2
	OPENBSD_2_8_BASE:1.33;
locks; strict;
comment	@ * @;


1.163
date	2017.02.07.17.25.46;	author patrick;	state Exp;
branches;
next	1.162;
commitid	dMJlqKWYCJoMV7JN;

1.162
date	2016.05.30.23.38.21;	author dlg;	state Exp;
branches;
next	1.161;
commitid	CZheUXZo6DeW439H;

1.161
date	2015.12.10.21.00.51;	author naddy;	state Exp;
branches;
next	1.160;
commitid	T0HbsCFlrwPATHlH;

1.160
date	2014.08.15.15.37.51;	author mikeb;	state Exp;
branches;
next	1.159;
commitid	1HNugRqq8KSFJPRm;

1.159
date	2014.07.13.23.10.23;	author deraadt;	state Exp;
branches;
next	1.158;
commitid	JtO5uXxVcnZfhUkR;

1.158
date	2014.07.12.18.48.52;	author tedu;	state Exp;
branches;
next	1.157;
commitid	OBNa5kfxQ2UXoiIw;

1.157
date	2014.05.04.20.09.15;	author sf;	state Exp;
branches;
next	1.156;

1.156
date	2013.12.22.11.05.58;	author sf;	state Exp;
branches;
next	1.155;

1.155
date	2012.03.14.17.08.17;	author mikeb;	state Exp;
branches;
next	1.154;

1.154
date	2012.01.13.09.53.24;	author mikeb;	state Exp;
branches;
next	1.153;

1.153
date	2011.05.06.17.55.00;	author mikeb;	state Exp;
branches;
next	1.152;

1.152
date	2011.04.05.11.48.28;	author blambert;	state Exp;
branches;
next	1.151;

1.151
date	2011.04.03.15.36.03;	author jasper;	state Exp;
branches;
next	1.150;

1.150
date	2011.01.12.20.55.22;	author deraadt;	state Exp;
branches;
next	1.149;

1.149
date	2011.01.11.15.42.05;	author deraadt;	state Exp;
branches;
next	1.148;

1.148
date	2010.12.15.23.34.23;	author mikeb;	state Exp;
branches;
next	1.147;

1.147
date	2010.07.02.02.40.16;	author blambert;	state Exp;
branches
	1.147.2.1;
next	1.146;

1.146
date	2010.04.08.00.23.53;	author tedu;	state Exp;
branches;
next	1.145;

1.145
date	2010.01.10.12.43.07;	author markus;	state Exp;
branches
	1.145.2.1;
next	1.144;

1.144
date	2009.09.13.14.42.52;	author krw;	state Exp;
branches;
next	1.143;

1.143
date	2009.03.27.13.31.30;	author reyk;	state Exp;
branches;
next	1.142;

1.142
date	2009.03.25.12.17.30;	author reyk;	state Exp;
branches;
next	1.141;

1.141
date	2008.06.09.07.07.16;	author djm;	state Exp;
branches;
next	1.140;

1.140
date	2007.10.01.15.34.48;	author krw;	state Exp;
branches;
next	1.139;

1.139
date	2007.09.18.22.02.18;	author djm;	state Exp;
branches;
next	1.138;

1.138
date	2006.12.29.13.04.37;	author pedro;	state Exp;
branches;
next	1.137;

1.137
date	2006.06.29.21.34.51;	author deraadt;	state Exp;
branches;
next	1.136;

1.136
date	2005.08.09.04.10.13;	author mickey;	state Exp;
branches;
next	1.135;

1.135
date	2004.05.07.14.42.26;	author millert;	state Exp;
branches;
next	1.134;

1.134
date	2004.05.04.16.59.31;	author grange;	state Exp;
branches;
next	1.133;

1.133
date	2004.02.03.17.17.33;	author deraadt;	state Exp;
branches;
next	1.132;

1.132
date	2004.01.09.21.32.24;	author brad;	state Exp;
branches;
next	1.131;

1.131
date	2003.09.03.15.55.41;	author jason;	state Exp;
branches;
next	1.130;

1.130
date	2003.08.14.15.20.29;	author jason;	state Exp;
branches;
next	1.129;

1.129
date	2003.08.08.02.55.17;	author jason;	state Exp;
branches;
next	1.128;

1.128
date	2003.08.01.17.55.54;	author deraadt;	state Exp;
branches;
next	1.127;

1.127
date	2003.06.04.14.04.58;	author jason;	state Exp;
branches;
next	1.126;

1.126
date	2003.04.19.22.08.04;	author jason;	state Exp;
branches;
next	1.125;

1.125
date	2003.04.19.21.59.08;	author jason;	state Exp;
branches;
next	1.124;

1.124
date	2003.04.02.22.33.13;	author jason;	state Exp;
branches;
next	1.123;

1.123
date	2003.02.14.01.28.20;	author jason;	state Exp;
branches;
next	1.122;

1.122
date	2002.12.06.22.03.26;	author jason;	state Exp;
branches;
next	1.121;

1.121
date	2002.12.05.22.40.41;	author jason;	state Exp;
branches;
next	1.120;

1.120
date	2002.11.21.19.34.25;	author jason;	state Exp;
branches;
next	1.119;

1.119
date	2002.11.19.18.40.17;	author jason;	state Exp;
branches;
next	1.118;

1.118
date	2002.10.12.01.09.44;	author krw;	state Exp;
branches;
next	1.117;

1.117
date	2002.10.10.17.46.35;	author jason;	state Exp;
branches;
next	1.116;

1.116
date	2002.10.05.00.21.02;	author jason;	state Exp;
branches;
next	1.115;

1.115
date	2002.09.24.18.33.26;	author jason;	state Exp;
branches;
next	1.114;

1.114
date	2002.09.19.17.58.38;	author jason;	state Exp;
branches;
next	1.113;

1.113
date	2002.09.12.03.27.20;	author jason;	state Exp;
branches;
next	1.112;

1.112
date	2002.09.11.22.40.31;	author jason;	state Exp;
branches;
next	1.111;

1.111
date	2002.09.04.15.37.29;	author jason;	state Exp;
branches;
next	1.110;

1.110
date	2002.09.04.04.21.19;	author jason;	state Exp;
branches;
next	1.109;

1.109
date	2002.09.03.20.10.00;	author jason;	state Exp;
branches;
next	1.108;

1.108
date	2002.07.08.19.41.29;	author jason;	state Exp;
branches;
next	1.107;

1.107
date	2002.07.05.21.21.17;	author jason;	state Exp;
branches;
next	1.106;

1.106
date	2002.07.05.21.03.46;	author jason;	state Exp;
branches;
next	1.105;

1.105
date	2002.07.03.00.24.29;	author jason;	state Exp;
branches;
next	1.104;

1.104
date	2002.07.03.00.19.33;	author jason;	state Exp;
branches;
next	1.103;

1.103
date	2002.06.17.08.05.47;	author deraadt;	state Exp;
branches;
next	1.102;

1.102
date	2002.05.16.16.34.13;	author jason;	state Exp;
branches;
next	1.101;

1.101
date	2002.05.16.02.54.02;	author jason;	state Exp;
branches;
next	1.100;

1.100
date	2002.05.15.15.15.41;	author jason;	state Exp;
branches;
next	1.99;

1.99
date	2002.05.13.22.28.56;	author jason;	state Exp;
branches;
next	1.98;

1.98
date	2002.05.08.23.05.27;	author jason;	state Exp;
branches;
next	1.97;

1.97
date	2002.05.06.20.53.05;	author jason;	state Exp;
branches;
next	1.96;

1.96
date	2002.05.06.15.42.23;	author jason;	state Exp;
branches;
next	1.95;

1.95
date	2002.05.02.19.03.35;	author jason;	state Exp;
branches;
next	1.94;

1.94
date	2002.05.02.18.28.24;	author jason;	state Exp;
branches;
next	1.93;

1.93
date	2002.05.02.18.20.50;	author jason;	state Exp;
branches;
next	1.92;

1.92
date	2002.05.01.21.35.08;	author jason;	state Exp;
branches;
next	1.91;

1.91
date	2002.04.30.00.26.10;	author jason;	state Exp;
branches;
next	1.90;

1.90
date	2002.04.28.17.08.18;	author jason;	state Exp;
branches;
next	1.89;

1.89
date	2002.04.26.17.54.01;	author deraadt;	state Exp;
branches;
next	1.88;

1.88
date	2002.04.26.05.08.49;	author jason;	state Exp;
branches;
next	1.87;

1.87
date	2002.04.26.05.06.03;	author jason;	state Exp;
branches;
next	1.86;

1.86
date	2002.04.26.04.24.17;	author jason;	state Exp;
branches;
next	1.85;

1.85
date	2002.04.22.21.24.36;	author jason;	state Exp;
branches;
next	1.84;

1.84
date	2002.04.08.17.49.42;	author jason;	state Exp;
branches;
next	1.83;

1.83
date	2002.03.14.01.27.00;	author millert;	state Exp;
branches;
next	1.82;

1.82
date	2002.01.28.17.10.11;	author jason;	state Exp;
branches;
next	1.81;

1.81
date	2002.01.28.16.26.21;	author jason;	state Exp;
branches;
next	1.80;

1.80
date	2002.01.28.15.44.36;	author jason;	state Exp;
branches;
next	1.79;

1.79
date	2002.01.24.03.13.43;	author jason;	state Exp;
branches;
next	1.78;

1.78
date	2002.01.19.21.15.37;	author jason;	state Exp;
branches;
next	1.77;

1.77
date	2002.01.02.20.35.40;	author deraadt;	state Exp;
branches;
next	1.76;

1.76
date	2001.12.07.18.08.20;	author jason;	state Exp;
branches
	1.76.2.1;
next	1.75;

1.75
date	2001.11.20.20.19.26;	author jason;	state Exp;
branches;
next	1.74;

1.74
date	2001.11.14.00.39.46;	author jason;	state Exp;
branches;
next	1.73;

1.73
date	2001.11.09.03.11.38;	author deraadt;	state Exp;
branches;
next	1.72;

1.72
date	2001.11.06.19.53.19;	author miod;	state Exp;
branches;
next	1.71;

1.71
date	2001.11.05.17.25.58;	author art;	state Exp;
branches;
next	1.70;

1.70
date	2001.08.27.22.02.37;	author jason;	state Exp;
branches
	1.70.2.1;
next	1.69;

1.69
date	2001.08.26.03.29.54;	author jason;	state Exp;
branches;
next	1.68;

1.68
date	2001.08.25.10.13.30;	author art;	state Exp;
branches;
next	1.67;

1.67
date	2001.08.12.20.03.49;	author mickey;	state Exp;
branches;
next	1.66;

1.66
date	2001.07.04.06.03.55;	author jason;	state Exp;
branches;
next	1.65;

1.65
date	2001.07.02.04.34.47;	author jason;	state Exp;
branches;
next	1.64;

1.64
date	2001.06.29.21.52.41;	author jason;	state Exp;
branches;
next	1.63;

1.63
date	2001.06.29.16.19.15;	author jason;	state Exp;
branches;
next	1.62;

1.62
date	2001.06.23.22.02.55;	author angelos;	state Exp;
branches;
next	1.61;

1.61
date	2001.06.23.20.59.42;	author angelos;	state Exp;
branches;
next	1.60;

1.60
date	2001.06.23.18.30.37;	author deraadt;	state Exp;
branches;
next	1.59;

1.59
date	2001.06.18.10.23.45;	author deraadt;	state Exp;
branches;
next	1.58;

1.58
date	2001.06.14.23.56.54;	author deraadt;	state Exp;
branches;
next	1.57;

1.57
date	2001.06.14.23.55.02;	author deraadt;	state Exp;
branches;
next	1.56;

1.56
date	2001.06.12.15.40.33;	author niklas;	state Exp;
branches;
next	1.55;

1.55
date	2001.06.08.01.59.32;	author jason;	state Exp;
branches;
next	1.54;

1.54
date	2001.05.30.02.26.14;	author jason;	state Exp;
branches;
next	1.53;

1.53
date	2001.05.23.14.42.52;	author jason;	state Exp;
branches;
next	1.52;

1.52
date	2001.05.23.04.46.41;	author jason;	state Exp;
branches;
next	1.51;

1.51
date	2001.05.22.23.07.39;	author jason;	state Exp;
branches;
next	1.50;

1.50
date	2001.05.22.22.53.38;	author jason;	state Exp;
branches;
next	1.49;

1.49
date	2001.05.14.16.51.48;	author deraadt;	state Exp;
branches
	1.49.2.1;
next	1.48;

1.48
date	2001.05.14.02.45.19;	author deraadt;	state Exp;
branches;
next	1.47;

1.47
date	2001.05.13.15.56.09;	author jason;	state Exp;
branches;
next	1.46;

1.46
date	2001.05.13.15.39.27;	author deraadt;	state Exp;
branches;
next	1.45;

1.45
date	2001.05.13.01.20.02;	author jason;	state Exp;
branches;
next	1.44;

1.44
date	2001.04.29.00.37.11;	author jason;	state Exp;
branches;
next	1.43;

1.43
date	2001.04.06.16.27.45;	author jason;	state Exp;
branches;
next	1.42;

1.42
date	2001.03.28.20.03.00;	author angelos;	state Exp;
branches;
next	1.41;

1.41
date	2001.03.25.07.59.25;	author csapuntz;	state Exp;
branches;
next	1.40;

1.40
date	2001.02.02.01.00.07;	author jason;	state Exp;
branches;
next	1.39;

1.39
date	2001.01.31.05.14.02;	author jason;	state Exp;
branches;
next	1.38;

1.38
date	2001.01.29.04.01.44;	author jason;	state Exp;
branches;
next	1.37;

1.37
date	2001.01.29.00.39.20;	author jason;	state Exp;
branches;
next	1.36;

1.36
date	2001.01.11.18.56.50;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	2001.01.11.18.52.53;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2000.11.17.05.18.41;	author angelos;	state Exp;
branches;
next	1.33;

1.33
date	2000.09.21.04.39.11;	author jason;	state Exp;
branches;
next	1.32;

1.32
date	2000.08.19.16.41.01;	author jason;	state Exp;
branches;
next	1.31;

1.31
date	2000.08.17.21.44.56;	author jason;	state Exp;
branches;
next	1.30;

1.30
date	2000.08.17.13.54.23;	author jason;	state Exp;
branches;
next	1.29;

1.29
date	2000.08.15.17.50.12;	author mickey;	state Exp;
branches;
next	1.28;

1.28
date	2000.08.15.17.39.19;	author jason;	state Exp;
branches;
next	1.27;

1.27
date	2000.08.15.17.27.56;	author jason;	state Exp;
branches;
next	1.26;

1.26
date	2000.08.15.01.00.47;	author jason;	state Exp;
branches;
next	1.25;

1.25
date	2000.08.13.22.07.11;	author deraadt;	state Exp;
branches;
next	1.24;

1.24
date	2000.08.13.22.06.47;	author deraadt;	state Exp;
branches;
next	1.23;

1.23
date	2000.08.13.22.03.09;	author deraadt;	state Exp;
branches;
next	1.22;

1.22
date	2000.08.13.21.58.06;	author deraadt;	state Exp;
branches;
next	1.21;

1.21
date	2000.08.12.06.29.08;	author deraadt;	state Exp;
branches;
next	1.20;

1.20
date	2000.08.11.19.38.15;	author deraadt;	state Exp;
branches;
next	1.19;

1.19
date	2000.07.31.21.57.49;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2000.07.29.23.42.00;	author jason;	state Exp;
branches;
next	1.17;

1.17
date	2000.07.20.21.45.19;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	2000.06.20.05.40.45;	author jason;	state Exp;
branches;
next	1.15;

1.15
date	2000.06.19.03.58.27;	author jason;	state Exp;
branches;
next	1.14;

1.14
date	2000.06.19.02.50.30;	author jason;	state Exp;
branches;
next	1.13;

1.13
date	2000.06.18.03.37.22;	author jason;	state Exp;
branches;
next	1.12;

1.12
date	2000.06.14.14.09.36;	author jason;	state Exp;
branches;
next	1.11;

1.11
date	2000.06.13.06.32.16;	author jason;	state Exp;
branches;
next	1.10;

1.10
date	2000.06.13.06.11.13;	author jason;	state Exp;
branches;
next	1.9;

1.9
date	2000.06.13.05.15.19;	author jason;	state Exp;
branches;
next	1.8;

1.8
date	2000.06.13.00.38.25;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	2000.06.12.19.50.35;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2000.06.10.05.09.37;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	2000.06.10.03.54.23;	author jason;	state Exp;
branches;
next	1.4;

1.4
date	2000.06.10.03.49.29;	author jason;	state Exp;
branches;
next	1.3;

1.3
date	2000.06.03.13.14.39;	author jason;	state Exp;
branches;
next	1.2;

1.2
date	2000.06.02.22.42.08;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	2000.05.18.01.25.19;	author jason;	state Exp;
branches;
next	;

1.49.2.1
date	2001.05.14.22.26.00;	author niklas;	state Exp;
branches;
next	1.49.2.2;

1.49.2.2
date	2001.07.04.10.43.12;	author niklas;	state Exp;
branches;
next	1.49.2.3;

1.49.2.3
date	2001.10.31.03.22.48;	author nate;	state Exp;
branches;
next	1.49.2.4;

1.49.2.4
date	2001.11.13.21.10.03;	author niklas;	state Exp;
branches;
next	1.49.2.5;

1.49.2.5
date	2001.12.05.00.43.30;	author niklas;	state Exp;
branches;
next	1.49.2.6;

1.49.2.6
date	2002.03.06.02.11.47;	author niklas;	state Exp;
branches;
next	1.49.2.7;

1.49.2.7
date	2002.03.28.15.35.59;	author niklas;	state Exp;
branches;
next	1.49.2.8;

1.49.2.8
date	2003.03.28.00.38.25;	author niklas;	state Exp;
branches;
next	1.49.2.9;

1.49.2.9
date	2003.05.13.19.35.09;	author ho;	state Exp;
branches;
next	1.49.2.10;

1.49.2.10
date	2003.06.07.11.02.31;	author ho;	state Exp;
branches;
next	1.49.2.11;

1.49.2.11
date	2004.02.19.10.56.30;	author niklas;	state Exp;
branches;
next	1.49.2.12;

1.49.2.12
date	2004.06.05.23.12.54;	author niklas;	state Exp;
branches;
next	;

1.70.2.1
date	2001.12.14.21.48.07;	author jason;	state Exp;
branches;
next	;

1.76.2.1
date	2002.01.31.22.55.36;	author niklas;	state Exp;
branches;
next	1.76.2.2;

1.76.2.2
date	2002.06.11.03.42.27;	author art;	state Exp;
branches;
next	1.76.2.3;

1.76.2.3
date	2002.10.29.00.33.30;	author art;	state Exp;
branches;
next	1.76.2.4;

1.76.2.4
date	2003.05.19.22.19.08;	author tedu;	state Exp;
branches;
next	;

1.145.2.1
date	2010.12.20.14.08.40;	author jasper;	state Exp;
branches;
next	;

1.147.2.1
date	2010.12.17.16.25.14;	author jasper;	state Exp;
branches;
next	;


desc
@@


1.163
log
@Reduce the per-packet allocation costs for crypto operations (cryptop)
by pre-allocating two cryptodesc objects and storing them in an array
instead of a linked list.  If more than two cryptodesc objects are
required use mallocarray to fetch them.  Adapt the drivers to the new
API.

This change results in one pool-get per ESP packet instead of three.
It also simplifies softraid crypto where more cryptodesc objects are
allocated than used.

From, with and ok markus@@, ok bluhm@@
"looks sane" mpi@@
@
text
@/*	$OpenBSD: ubsec.c,v 1.162 2016/05/30 23:38:21 dlg Exp $	*/

/*
 * Copyright (c) 2000 Jason L. Wright (jason@@thought.net)
 * Copyright (c) 2000 Theo de Raadt (deraadt@@openbsd.org)
 * Copyright (c) 2001 Patrik Lindergren (patrik@@ipunplugged.com)
 * 
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
 * ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 *
 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
 *
 */

#undef UBSEC_DEBUG

/*
 * uBsec 5[56]01, 58xx hardware crypto accelerator
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/timeout.h>
#include <sys/errno.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/mbuf.h>
#include <sys/device.h>
#include <sys/queue.h>

#include <crypto/cryptodev.h>
#include <crypto/cryptosoft.h>
#include <dev/rndvar.h>
#include <crypto/md5.h>
#include <crypto/sha1.h>

#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>
#include <dev/pci/pcidevs.h>

#include <dev/pci/ubsecreg.h>
#include <dev/pci/ubsecvar.h>

/*
 * Prototypes and count for the pci_device structure
 */
int ubsec_probe(struct device *, void *, void *);
void ubsec_attach(struct device *, struct device *, void *);
void ubsec_reset_board(struct ubsec_softc *);
void ubsec_init_board(struct ubsec_softc *);
void ubsec_init_pciregs(struct pci_attach_args *pa);
void ubsec_cleanchip(struct ubsec_softc *);
void ubsec_totalreset(struct ubsec_softc *);
int  ubsec_free_q(struct ubsec_softc*, struct ubsec_q *);

struct cfattach ubsec_ca = {
	sizeof(struct ubsec_softc), ubsec_probe, ubsec_attach,
};

struct cfdriver ubsec_cd = {
	0, "ubsec", DV_DULL
};

int	ubsec_intr(void *);
int	ubsec_newsession(u_int32_t *, struct cryptoini *);
int	ubsec_freesession(u_int64_t);
int	ubsec_process(struct cryptop *);
void	ubsec_callback(struct ubsec_softc *, struct ubsec_q *);
void	ubsec_feed(struct ubsec_softc *);
void	ubsec_callback2(struct ubsec_softc *, struct ubsec_q2 *);
void	ubsec_feed2(struct ubsec_softc *);
void	ubsec_feed4(struct ubsec_softc *);
void	ubsec_rng(void *);
int	ubsec_dma_malloc(struct ubsec_softc *, bus_size_t,
    struct ubsec_dma_alloc *, int);
void	ubsec_dma_free(struct ubsec_softc *, struct ubsec_dma_alloc *);
int	ubsec_dmamap_aligned(bus_dmamap_t);

#define	READ_REG(sc,r) \
	bus_space_read_4((sc)->sc_st, (sc)->sc_sh, (r))

#define WRITE_REG(sc,reg,val) \
	bus_space_write_4((sc)->sc_st, (sc)->sc_sh, reg, val)

#define	SWAP32(x) (x) = htole32(ntohl((x)))
#define	HTOLE32(x) (x) = htole32(x)


struct ubsec_stats ubsecstats;

const struct pci_matchid ubsec_devices[] = {
	{ PCI_VENDOR_BLUESTEEL, PCI_PRODUCT_BLUESTEEL_5501 },
	{ PCI_VENDOR_BLUESTEEL, PCI_PRODUCT_BLUESTEEL_5601 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5801 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5802 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5805 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5820 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5821 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5822 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5823 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5825 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5860 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5861 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5862 },
	{ PCI_VENDOR_SUN, PCI_PRODUCT_SUN_SCA1K },
	{ PCI_VENDOR_SUN, PCI_PRODUCT_SUN_5821 },
};

int
ubsec_probe(struct device *parent, void *match, void *aux)
{
	return (pci_matchbyid((struct pci_attach_args *)aux, ubsec_devices,
	    nitems(ubsec_devices)));
}

void
ubsec_attach(struct device *parent, struct device *self, void *aux)
{
	struct ubsec_softc *sc = (struct ubsec_softc *)self;
	struct pci_attach_args *pa = aux;
	pci_chipset_tag_t pc = pa->pa_pc;
	pci_intr_handle_t ih;
	pcireg_t memtype;
	const char *intrstr = NULL;
	struct ubsec_dma *dmap;
	bus_size_t iosize;
	u_int32_t i;
	int algs[CRYPTO_ALGORITHM_MAX + 1];

	SIMPLEQ_INIT(&sc->sc_queue);
	SIMPLEQ_INIT(&sc->sc_qchip);
	SIMPLEQ_INIT(&sc->sc_queue2);
	SIMPLEQ_INIT(&sc->sc_qchip2);
	SIMPLEQ_INIT(&sc->sc_queue4);
	SIMPLEQ_INIT(&sc->sc_qchip4);

	sc->sc_statmask = BS_STAT_MCR1_DONE | BS_STAT_DMAERR;
	sc->sc_maxaggr = UBS_MIN_AGGR;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601)
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5802 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805))
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822))
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY;

	if ((PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821) ||
	    (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_SUN &&
	     (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_SCA1K ||
	      PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_5821))) {
		sc->sc_statmask |= BS_STAT_MCR1_ALLEMPTY |
		    BS_STAT_MCR2_ALLEMPTY;
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY;
	}

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5823 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5825))
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY |
		    UBS_FLAGS_AES;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5860 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5861 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5862)) {
		sc->sc_maxaggr = UBS_MAX_AGGR;
		sc->sc_statmask |=
		    BS_STAT_MCR1_ALLEMPTY | BS_STAT_MCR2_ALLEMPTY |
		    BS_STAT_MCR3_ALLEMPTY | BS_STAT_MCR4_ALLEMPTY;
		sc->sc_flags |= UBS_FLAGS_MULTIMCR | UBS_FLAGS_HWNORM |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_AES |
		    UBS_FLAGS_KEY | UBS_FLAGS_BIGKEY;
#if 0
		/* The RNG is not yet supported */
		sc->sc_flags |= UBS_FLAGS_RNG | UBS_FLAGS_RNG4;
#endif
	}

	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, BS_BAR);
	if (pci_mapreg_map(pa, BS_BAR, memtype, 0,
	    &sc->sc_st, &sc->sc_sh, NULL, &iosize, 0)) {
		printf(": can't find mem space\n");
		return;
	}
	sc->sc_dmat = pa->pa_dmat;

	if (pci_intr_map(pa, &ih)) {
		printf(": couldn't map interrupt\n");
		bus_space_unmap(sc->sc_st, sc->sc_sh, iosize);
		return;
	}
	intrstr = pci_intr_string(pc, ih);
	sc->sc_ih = pci_intr_establish(pc, ih, IPL_NET, ubsec_intr, sc,
	    self->dv_xname);
	if (sc->sc_ih == NULL) {
		printf(": couldn't establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		bus_space_unmap(sc->sc_st, sc->sc_sh, iosize);
		return;
	}

	sc->sc_cid = crypto_get_driverid(0);
	if (sc->sc_cid < 0) {
		pci_intr_disestablish(pc, sc->sc_ih);
		bus_space_unmap(sc->sc_st, sc->sc_sh, iosize);
		return;
	}

	SIMPLEQ_INIT(&sc->sc_freequeue);
	dmap = sc->sc_dmaa;
	for (i = 0; i < UBS_MAX_NQUEUE; i++, dmap++) {
		struct ubsec_q *q;

		q = (struct ubsec_q *)malloc(sizeof(struct ubsec_q),
		    M_DEVBUF, M_NOWAIT);
		if (q == NULL) {
			printf(": can't allocate queue buffers\n");
			break;
		}

		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_dmachunk),
		    &dmap->d_alloc, 0)) {
			printf(": can't allocate dma buffers\n");
			free(q, M_DEVBUF, 0);
			break;
		}
		dmap->d_dma = (struct ubsec_dmachunk *)dmap->d_alloc.dma_vaddr;

		q->q_dma = dmap;
		sc->sc_queuea[i] = q;

		SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
	}

	bzero(algs, sizeof(algs));
	algs[CRYPTO_3DES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_MD5_HMAC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_SHA1_HMAC] = CRYPTO_ALG_FLAG_SUPPORTED;
	if (sc->sc_flags & UBS_FLAGS_AES)
		algs[CRYPTO_AES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
	crypto_register(sc->sc_cid, algs, ubsec_newsession,
	    ubsec_freesession, ubsec_process);

	/*
	 * Reset Broadcom chip
	 */
	ubsec_reset_board(sc);

	/*
	 * Init Broadcom specific PCI settings
	 */
	ubsec_init_pciregs(pa);

	/*
	 * Init Broadcom chip
	 */
	ubsec_init_board(sc);

	printf(": 3DES MD5 SHA1");
	if (sc->sc_flags & UBS_FLAGS_AES)
		printf(" AES");

#ifndef UBSEC_NO_RNG
	if (sc->sc_flags & UBS_FLAGS_RNG) {
		if (sc->sc_flags & UBS_FLAGS_RNG4)
			sc->sc_statmask |= BS_STAT_MCR4_DONE;
		else
			sc->sc_statmask |= BS_STAT_MCR2_DONE;

		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
		    &sc->sc_rng.rng_q.q_mcr, 0))
			goto skip_rng;

		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rngbypass),
		    &sc->sc_rng.rng_q.q_ctx, 0)) {
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_mcr);
			goto skip_rng;
		}

		if (ubsec_dma_malloc(sc, sizeof(u_int32_t) *
		    UBSEC_RNG_BUFSIZ, &sc->sc_rng.rng_buf, 0)) {
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_ctx);
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_mcr);
			goto skip_rng;
		}

		timeout_set(&sc->sc_rngto, ubsec_rng, sc);
		if (hz >= 100)
			sc->sc_rnghz = hz / 100;
		else
			sc->sc_rnghz = 1;
		timeout_add(&sc->sc_rngto, sc->sc_rnghz);
		printf(" RNG");
skip_rng:
	;
	}
#endif /* UBSEC_NO_RNG */

	if (sc->sc_flags & UBS_FLAGS_KEY) {
		sc->sc_statmask |= BS_STAT_MCR2_DONE;
	}

	printf(", %s\n", intrstr);
}

/*
 * UBSEC Interrupt routine
 */
int
ubsec_intr(void *arg)
{
	struct ubsec_softc *sc = arg;
	volatile u_int32_t stat;
	struct ubsec_q *q;
	struct ubsec_dma *dmap;
	u_int16_t flags;
	int npkts = 0, i;

	stat = READ_REG(sc, BS_STAT);

	if ((stat & (BS_STAT_MCR1_DONE|BS_STAT_MCR2_DONE|BS_STAT_MCR4_DONE|
	    BS_STAT_DMAERR)) == 0)
		return (0);

	stat &= sc->sc_statmask;
	WRITE_REG(sc, BS_STAT, stat);		/* IACK */

	/*
	 * Check to see if we have any packets waiting for us
	 */
	if ((stat & BS_STAT_MCR1_DONE)) {
		while (!SIMPLEQ_EMPTY(&sc->sc_qchip)) {
			q = SIMPLEQ_FIRST(&sc->sc_qchip);
			dmap = q->q_dma;

			if ((dmap->d_dma->d_mcr.mcr_flags & htole16(UBS_MCR_DONE)) == 0)
				break;

			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q_next);

			npkts = q->q_nstacked_mcrs;
			/*
			 * search for further sc_qchip ubsec_q's that share
			 * the same MCR, and complete them too, they must be
			 * at the top.
			 */
			for (i = 0; i < npkts; i++) {
				if(q->q_stacked_mcr[i])
					ubsec_callback(sc, q->q_stacked_mcr[i]);
				else
					break;
			}
			ubsec_callback(sc, q);
		}

		/*
		 * Don't send any more packet to chip if there has been
		 * a DMAERR.
		 */
		if (!(stat & BS_STAT_DMAERR))
			ubsec_feed(sc);
	}

	/*
	 * Check to see if we have any key setups/rng's waiting for us
	 */
	if ((sc->sc_flags & (UBS_FLAGS_KEY|UBS_FLAGS_RNG)) &&
	    (stat & BS_STAT_MCR2_DONE)) {
		struct ubsec_q2 *q2;
		struct ubsec_mcr *mcr;

		while (!SIMPLEQ_EMPTY(&sc->sc_qchip2)) {
			q2 = SIMPLEQ_FIRST(&sc->sc_qchip2);

			bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
			    0, q2->q_mcr.dma_map->dm_mapsize,
			    BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE);

			mcr = (struct ubsec_mcr *)q2->q_mcr.dma_vaddr;

			/* A bug in new devices requires to swap this field */
			if (sc->sc_flags & UBS_FLAGS_MULTIMCR)
				flags = swap16(mcr->mcr_flags);
			else
				flags = mcr->mcr_flags;
			if ((flags & htole16(UBS_MCR_DONE)) == 0) {
				bus_dmamap_sync(sc->sc_dmat,
				    q2->q_mcr.dma_map, 0,
				    q2->q_mcr.dma_map->dm_mapsize,
				    BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
				break;
			}
			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip2, q_next);
			ubsec_callback2(sc, q2);
			/*
			 * Don't send any more packet to chip if there has been
			 * a DMAERR.
			 */
			if (!(stat & BS_STAT_DMAERR))
				ubsec_feed2(sc);
		}
	}
	if ((sc->sc_flags & UBS_FLAGS_RNG4) && (stat & BS_STAT_MCR4_DONE)) {
		struct ubsec_q2 *q2;
		struct ubsec_mcr *mcr;

		while (!SIMPLEQ_EMPTY(&sc->sc_qchip4)) {
			q2 = SIMPLEQ_FIRST(&sc->sc_qchip4);

			bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
			    0, q2->q_mcr.dma_map->dm_mapsize,
			    BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE);

			mcr = (struct ubsec_mcr *)q2->q_mcr.dma_vaddr;

			/* A bug in new devices requires to swap this field */
			flags = swap16(mcr->mcr_flags);

			if ((flags & htole16(UBS_MCR_DONE)) == 0) {
				bus_dmamap_sync(sc->sc_dmat,
				    q2->q_mcr.dma_map, 0,
				    q2->q_mcr.dma_map->dm_mapsize,
				    BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
				break;
			}
			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip4, q_next);
			ubsec_callback2(sc, q2);
			/*
			 * Don't send any more packet to chip if there has been
			 * a DMAERR.
			 */
			if (!(stat & BS_STAT_DMAERR))
				ubsec_feed4(sc);
		}
	}

	/*
	 * Check to see if we got any DMA Error
	 */
	if (stat & BS_STAT_DMAERR) {
#ifdef UBSEC_DEBUG
		volatile u_int32_t a = READ_REG(sc, BS_ERR);

		printf("%s: dmaerr %s@@%08x\n", sc->sc_dv.dv_xname,
		    (a & BS_ERR_READ) ? "read" : "write", a & BS_ERR_ADDR);
#endif /* UBSEC_DEBUG */
		ubsecstats.hst_dmaerr++;
		ubsec_totalreset(sc);
		ubsec_feed(sc);
	}

	return (1);
}

/*
 * ubsec_feed() - aggregate and post requests to chip
 *		  It is assumed that the caller set splnet()
 */
void
ubsec_feed(struct ubsec_softc *sc)
{
#ifdef UBSEC_DEBUG
	static int max;
#endif /* UBSEC_DEBUG */
	struct ubsec_q *q, *q2;
	int npkts, i;
	void *v;
	u_int32_t stat;

	npkts = sc->sc_nqueue;
	if (npkts > sc->sc_maxaggr)
		npkts = sc->sc_maxaggr;
	if (npkts < 2)
		goto feed1;

	if ((stat = READ_REG(sc, BS_STAT)) & (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
		if(stat & BS_STAT_DMAERR) {
			ubsec_totalreset(sc);
			ubsecstats.hst_dmaerr++;
		}
		return;
	}

#ifdef UBSEC_DEBUG
	printf("merging %d records\n", npkts);

	/* XXX temporary aggregation statistics reporting code */
	if (max < npkts) {
		max = npkts;
		printf("%s: new max aggregate %d\n", sc->sc_dv.dv_xname, max);
	}
#endif /* UBSEC_DEBUG */

	q = SIMPLEQ_FIRST(&sc->sc_queue);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q_next);
	--sc->sc_nqueue;

	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	if (q->q_dst_map != NULL)
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	q->q_nstacked_mcrs = npkts - 1;		/* Number of packets stacked */

	for (i = 0; i < q->q_nstacked_mcrs; i++) {
		q2 = SIMPLEQ_FIRST(&sc->sc_queue);
		bus_dmamap_sync(sc->sc_dmat, q2->q_src_map,
		    0, q2->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
		if (q2->q_dst_map != NULL)
			bus_dmamap_sync(sc->sc_dmat, q2->q_dst_map,
			    0, q2->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q_next);
		--sc->sc_nqueue;

		v = ((char *)&q2->q_dma->d_dma->d_mcr) + sizeof(struct ubsec_mcr) -
		    sizeof(struct ubsec_mcr_add);
		bcopy(v, &q->q_dma->d_dma->d_mcradd[i], sizeof(struct ubsec_mcr_add));
		q->q_stacked_mcr[i] = q2;
	}
	q->q_dma->d_dma->d_mcr.mcr_pkts = htole16(npkts);
	SIMPLEQ_INSERT_TAIL(&sc->sc_qchip, q, q_next);
	bus_dmamap_sync(sc->sc_dmat, q->q_dma->d_alloc.dma_map,
	    0, q->q_dma->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	WRITE_REG(sc, BS_MCR1, q->q_dma->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_mcr));
	return;

feed1:
	while (!SIMPLEQ_EMPTY(&sc->sc_queue)) {
		if ((stat = READ_REG(sc, BS_STAT)) &
		    (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
			if(stat & BS_STAT_DMAERR) {
				ubsec_totalreset(sc);
				ubsecstats.hst_dmaerr++;
			}
			break;
		}

		q = SIMPLEQ_FIRST(&sc->sc_queue);

		bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
		    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
		if (q->q_dst_map != NULL)
			bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
			    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
		bus_dmamap_sync(sc->sc_dmat, q->q_dma->d_alloc.dma_map,
		    0, q->q_dma->d_alloc.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		WRITE_REG(sc, BS_MCR1, q->q_dma->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_mcr));
#ifdef UBSEC_DEBUG
		printf("feed: q->chip %p %08x\n", q,
		    (u_int32_t)q->q_dma->d_alloc.dma_paddr);
#endif /* UBSEC_DEBUG */
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q_next);
		--sc->sc_nqueue;
		SIMPLEQ_INSERT_TAIL(&sc->sc_qchip, q, q_next);
	}
}

/*
 * Allocate a new 'session' and return an encoded session id.  'sidp'
 * contains our registration id, and should contain an encoded session
 * id on successful allocation.
 */
int
ubsec_newsession(u_int32_t *sidp, struct cryptoini *cri)
{
	struct cryptoini *c, *encini = NULL, *macini = NULL;
	struct ubsec_softc *sc = NULL;
	struct ubsec_session *ses = NULL;
	MD5_CTX md5ctx;
	SHA1_CTX sha1ctx;
	int i, sesn;

	if (sidp == NULL || cri == NULL)
		return (EINVAL);

	for (i = 0; i < ubsec_cd.cd_ndevs; i++) {
		sc = ubsec_cd.cd_devs[i];
		if (sc == NULL || sc->sc_cid == (*sidp))
			break;
	}
	if (sc == NULL)
		return (EINVAL);

	for (c = cri; c != NULL; c = c->cri_next) {
		if (c->cri_alg == CRYPTO_MD5_HMAC ||
		    c->cri_alg == CRYPTO_SHA1_HMAC) {
			if (macini)
				return (EINVAL);
			macini = c;
		} else if (c->cri_alg == CRYPTO_3DES_CBC ||
		    c->cri_alg == CRYPTO_AES_CBC) {
			if (encini)
				return (EINVAL);
			encini = c;
		} else
			return (EINVAL);
	}
	if (encini == NULL && macini == NULL)
		return (EINVAL);

	if (encini && encini->cri_alg == CRYPTO_AES_CBC) {
		switch (encini->cri_klen) {
		case 128:
		case 192:
		case 256:
			break;
		default:
			return (EINVAL);
		}
	}

	if (sc->sc_sessions == NULL) {
		ses = sc->sc_sessions = (struct ubsec_session *)malloc(
		    sizeof(struct ubsec_session), M_DEVBUF, M_NOWAIT);
		if (ses == NULL)
			return (ENOMEM);
		sesn = 0;
		sc->sc_nsessions = 1;
	} else {
		for (sesn = 0; sesn < sc->sc_nsessions; sesn++) {
			if (sc->sc_sessions[sesn].ses_used == 0) {
				ses = &sc->sc_sessions[sesn];
				break;
			}
		}

		if (ses == NULL) {
			sesn = sc->sc_nsessions;
			ses = mallocarray((sesn + 1),
			    sizeof(struct ubsec_session), M_DEVBUF, M_NOWAIT);
			if (ses == NULL)
				return (ENOMEM);
			bcopy(sc->sc_sessions, ses, sesn *
			    sizeof(struct ubsec_session));
			explicit_bzero(sc->sc_sessions, sesn *
			    sizeof(struct ubsec_session));
			free(sc->sc_sessions, M_DEVBUF, 0);
			sc->sc_sessions = ses;
			ses = &sc->sc_sessions[sesn];
			sc->sc_nsessions++;
		}
	}

	bzero(ses, sizeof(struct ubsec_session));
	ses->ses_used = 1;
	if (encini) {
		/* Go ahead and compute key in ubsec's byte order */
		if (encini->cri_alg == CRYPTO_AES_CBC) {
			bcopy(encini->cri_key, ses->ses_key,
			    encini->cri_klen / 8);
		} else
			bcopy(encini->cri_key, ses->ses_key, 24);

		SWAP32(ses->ses_key[0]);
		SWAP32(ses->ses_key[1]);
		SWAP32(ses->ses_key[2]);
		SWAP32(ses->ses_key[3]);
		SWAP32(ses->ses_key[4]);
		SWAP32(ses->ses_key[5]);
		SWAP32(ses->ses_key[6]);
		SWAP32(ses->ses_key[7]);
	}

	if (macini) {
		for (i = 0; i < macini->cri_klen / 8; i++)
			macini->cri_key[i] ^= HMAC_IPAD_VAL;

		if (macini->cri_alg == CRYPTO_MD5_HMAC) {
			MD5Init(&md5ctx);
			MD5Update(&md5ctx, macini->cri_key,
			    macini->cri_klen / 8);
			MD5Update(&md5ctx, hmac_ipad_buffer,
			    HMAC_MD5_BLOCK_LEN - (macini->cri_klen / 8));
			bcopy(md5ctx.state, ses->ses_hminner,
			    sizeof(md5ctx.state));
		} else {
			SHA1Init(&sha1ctx);
			SHA1Update(&sha1ctx, macini->cri_key,
			    macini->cri_klen / 8);
			SHA1Update(&sha1ctx, hmac_ipad_buffer,
			    HMAC_SHA1_BLOCK_LEN - (macini->cri_klen / 8));
			bcopy(sha1ctx.state, ses->ses_hminner,
			    sizeof(sha1ctx.state));
		}

		for (i = 0; i < macini->cri_klen / 8; i++)
			macini->cri_key[i] ^= (HMAC_IPAD_VAL ^ HMAC_OPAD_VAL);

		if (macini->cri_alg == CRYPTO_MD5_HMAC) {
			MD5Init(&md5ctx);
			MD5Update(&md5ctx, macini->cri_key,
			    macini->cri_klen / 8);
			MD5Update(&md5ctx, hmac_opad_buffer,
			    HMAC_MD5_BLOCK_LEN - (macini->cri_klen / 8));
			bcopy(md5ctx.state, ses->ses_hmouter,
			    sizeof(md5ctx.state));
		} else {
			SHA1Init(&sha1ctx);
			SHA1Update(&sha1ctx, macini->cri_key,
			    macini->cri_klen / 8);
			SHA1Update(&sha1ctx, hmac_opad_buffer,
			    HMAC_SHA1_BLOCK_LEN - (macini->cri_klen / 8));
			bcopy(sha1ctx.state, ses->ses_hmouter,
			    sizeof(sha1ctx.state));
		}

		for (i = 0; i < macini->cri_klen / 8; i++)
			macini->cri_key[i] ^= HMAC_OPAD_VAL;
	}

	*sidp = UBSEC_SID(sc->sc_dv.dv_unit, sesn);
	return (0);
}

/*
 * Deallocate a session.
 */
int
ubsec_freesession(u_int64_t tid)
{
	struct ubsec_softc *sc;
	int card, session;
	u_int32_t sid = ((u_int32_t)tid) & 0xffffffff;

	card = UBSEC_CARD(sid);
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL)
		return (EINVAL);
	sc = ubsec_cd.cd_devs[card];
	session = UBSEC_SESSION(sid);
	explicit_bzero(&sc->sc_sessions[session], sizeof(sc->sc_sessions[session]));
	return (0);
}

int
ubsec_process(struct cryptop *crp)
{
	struct ubsec_q *q = NULL;
	int card, err = 0, i, j, s, nicealign;
	struct ubsec_softc *sc;
	struct cryptodesc *crd1, *crd2 = NULL, *maccrd, *enccrd;
	int encoffset = 0, macoffset = 0, cpskip, cpoffset;
	int sskip, dskip, stheend, dtheend;
	int16_t coffset;
	struct ubsec_session *ses, key;
	struct ubsec_dma *dmap = NULL;
	u_int16_t flags = 0;
	int ivlen = 0, keylen = 0;

	if (crp == NULL || crp->crp_callback == NULL) {
		ubsecstats.hst_invalid++;
		return (EINVAL);
	}
	card = UBSEC_CARD(crp->crp_sid);
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL) {
		ubsecstats.hst_invalid++;
		return (EINVAL);
	}

	sc = ubsec_cd.cd_devs[card];

	s = splnet();

	if (SIMPLEQ_EMPTY(&sc->sc_freequeue)) {
		ubsecstats.hst_queuefull++;
		splx(s);
		err = ENOMEM;
		goto errout2;
	}

	q = SIMPLEQ_FIRST(&sc->sc_freequeue);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_freequeue, q_next);
	splx(s);

	dmap = q->q_dma; /* Save dma pointer */
	bzero(q, sizeof(struct ubsec_q));
	bzero(&key, sizeof(key));

	q->q_sesn = UBSEC_SESSION(crp->crp_sid);
	q->q_dma = dmap;
	ses = &sc->sc_sessions[q->q_sesn];

	if (crp->crp_flags & CRYPTO_F_IMBUF) {
		q->q_src_m = (struct mbuf *)crp->crp_buf;
		q->q_dst_m = (struct mbuf *)crp->crp_buf;
	} else if (crp->crp_flags & CRYPTO_F_IOV) {
		q->q_src_io = (struct uio *)crp->crp_buf;
		q->q_dst_io = (struct uio *)crp->crp_buf;
	} else {
		err = EINVAL;
		goto errout;	/* XXX we don't handle contiguous blocks! */
	}

	bzero(&dmap->d_dma->d_mcr, sizeof(struct ubsec_mcr));

	dmap->d_dma->d_mcr.mcr_pkts = htole16(1);
	dmap->d_dma->d_mcr.mcr_flags = 0;
	q->q_crp = crp;

	if (crp->crp_ndesc < 1) {
		err = EINVAL;
		goto errout;
	}
	crd1 = &crp->crp_desc[0];
	if (crp->crp_ndesc >= 2)
		crd2 = &crp->crp_desc[1];

	if (crd2 == NULL) {
		if (crd1->crd_alg == CRYPTO_MD5_HMAC ||
		    crd1->crd_alg == CRYPTO_SHA1_HMAC) {
			maccrd = crd1;
			enccrd = NULL;
		} else if (crd1->crd_alg == CRYPTO_3DES_CBC ||
		    crd1->crd_alg == CRYPTO_AES_CBC) {
			maccrd = NULL;
			enccrd = crd1;
		} else {
			err = EINVAL;
			goto errout;
		}
	} else {
		if ((crd1->crd_alg == CRYPTO_MD5_HMAC ||
		    crd1->crd_alg == CRYPTO_SHA1_HMAC) &&
		    (crd2->crd_alg == CRYPTO_3DES_CBC ||
		    crd2->crd_alg == CRYPTO_AES_CBC) &&
		    ((crd2->crd_flags & CRD_F_ENCRYPT) == 0)) {
			maccrd = crd1;
			enccrd = crd2;
		} else if ((crd1->crd_alg == CRYPTO_3DES_CBC ||
		    crd1->crd_alg == CRYPTO_AES_CBC) &&
		    (crd2->crd_alg == CRYPTO_MD5_HMAC ||
		    crd2->crd_alg == CRYPTO_SHA1_HMAC) &&
		    (crd1->crd_flags & CRD_F_ENCRYPT)) {
			enccrd = crd1;
			maccrd = crd2;
		} else {
			/*
			 * We cannot order the ubsec as requested
			 */
			err = EINVAL;
			goto errout;
		}
	}

	if (enccrd) {
		if (enccrd->crd_alg == CRYPTO_AES_CBC) {
			if ((sc->sc_flags & UBS_FLAGS_AES) == 0) {
				err = EINVAL;
				goto errout;
			}
			flags |= htole16(UBS_PKTCTX_ENC_AES);
			switch (enccrd->crd_klen) {
			case 128:
			case 192:
			case 256:
				keylen = enccrd->crd_klen / 8;
				break;
			default:
				err = EINVAL;
				goto errout;
			}
			ivlen = 16;
		} else {
			flags |= htole16(UBS_PKTCTX_ENC_3DES);
			ivlen = 8;
			keylen = 24;
		}

		encoffset = enccrd->crd_skip;

		if (enccrd->crd_flags & CRD_F_ENCRYPT) {
			if (enccrd->crd_flags & CRD_F_IV_EXPLICIT)
				bcopy(enccrd->crd_iv, key.ses_iv, ivlen);
			else
				arc4random_buf(key.ses_iv, ivlen);

			if ((enccrd->crd_flags & CRD_F_IV_PRESENT) == 0) {
				if (crp->crp_flags & CRYPTO_F_IMBUF)
					err = m_copyback(q->q_src_m,
					    enccrd->crd_inject,
					    ivlen, key.ses_iv, M_NOWAIT);
				else if (crp->crp_flags & CRYPTO_F_IOV)
					cuio_copyback(q->q_src_io,
					    enccrd->crd_inject,
					    ivlen, key.ses_iv);
				if (err)
					goto errout;
			}
		} else {
			flags |= htole16(UBS_PKTCTX_INBOUND);

			if (enccrd->crd_flags & CRD_F_IV_EXPLICIT)
				bcopy(enccrd->crd_iv, key.ses_iv, ivlen);
			else if (crp->crp_flags & CRYPTO_F_IMBUF)
				m_copydata(q->q_src_m, enccrd->crd_inject,
				    ivlen, (caddr_t)key.ses_iv);
			else if (crp->crp_flags & CRYPTO_F_IOV)
				cuio_copydata(q->q_src_io,
				    enccrd->crd_inject, ivlen,
				    (caddr_t)key.ses_iv);
		}

		for (i = 0; i < (keylen / 4); i++)
			key.ses_key[i] = ses->ses_key[i];
		for (i = 0; i < (ivlen / 4); i++)
			SWAP32(key.ses_iv[i]);
	}

	if (maccrd) {
		macoffset = maccrd->crd_skip;

		if (maccrd->crd_alg == CRYPTO_MD5_HMAC)
			flags |= htole16(UBS_PKTCTX_AUTH_MD5);
		else
			flags |= htole16(UBS_PKTCTX_AUTH_SHA1);

		for (i = 0; i < 5; i++) {
			key.ses_hminner[i] = ses->ses_hminner[i];
			key.ses_hmouter[i] = ses->ses_hmouter[i];

			HTOLE32(key.ses_hminner[i]);
			HTOLE32(key.ses_hmouter[i]);
		}
	}

	if (enccrd && maccrd) {
		/*
		 * ubsec cannot handle packets where the end of encryption
		 * and authentication are not the same, or where the
		 * encrypted part begins before the authenticated part.
		 */
		if (((encoffset + enccrd->crd_len) !=
		    (macoffset + maccrd->crd_len)) ||
		    (enccrd->crd_skip < maccrd->crd_skip)) {
			err = EINVAL;
			goto errout;
		}
		sskip = maccrd->crd_skip;
		cpskip = dskip = enccrd->crd_skip;
		stheend = maccrd->crd_len;
		dtheend = enccrd->crd_len;
		coffset = enccrd->crd_skip - maccrd->crd_skip;
		cpoffset = cpskip + dtheend;
#ifdef UBSEC_DEBUG
		printf("mac: skip %d, len %d, inject %d\n",
 		    maccrd->crd_skip, maccrd->crd_len, maccrd->crd_inject);
		printf("enc: skip %d, len %d, inject %d\n",
		    enccrd->crd_skip, enccrd->crd_len, enccrd->crd_inject);
		printf("src: skip %d, len %d\n", sskip, stheend);
		printf("dst: skip %d, len %d\n", dskip, dtheend);
		printf("ubs: coffset %d, pktlen %d, cpskip %d, cpoffset %d\n",
		    coffset, stheend, cpskip, cpoffset);
#endif
	} else {
		cpskip = dskip = sskip = macoffset + encoffset;
		dtheend = stheend = (enccrd)?enccrd->crd_len:maccrd->crd_len;
		cpoffset = cpskip + dtheend;
		coffset = 0;
	}

	if (bus_dmamap_create(sc->sc_dmat, 0xfff0, UBS_MAX_SCATTER,
		0xfff0, 0, BUS_DMA_NOWAIT, &q->q_src_map) != 0) {
		err = ENOMEM;
		goto errout;
	}
	if (crp->crp_flags & CRYPTO_F_IMBUF) {
		if (bus_dmamap_load_mbuf(sc->sc_dmat, q->q_src_map,
		    q->q_src_m, BUS_DMA_NOWAIT) != 0) {
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
			q->q_src_map = NULL;
			err = ENOMEM;
			goto errout;
		}
	} else if (crp->crp_flags & CRYPTO_F_IOV) {
		if (bus_dmamap_load_uio(sc->sc_dmat, q->q_src_map,
		    q->q_src_io, BUS_DMA_NOWAIT) != 0) {
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
			q->q_src_map = NULL;
			err = ENOMEM;
			goto errout;
		}
	}
	nicealign = ubsec_dmamap_aligned(q->q_src_map);

	dmap->d_dma->d_mcr.mcr_pktlen = htole16(stheend);

#ifdef UBSEC_DEBUG
	printf("src skip: %d\n", sskip);
#endif
	for (i = j = 0; i < q->q_src_map->dm_nsegs; i++) {
		struct ubsec_pktbuf *pb;
		bus_size_t packl = q->q_src_map->dm_segs[i].ds_len;
		bus_addr_t packp = q->q_src_map->dm_segs[i].ds_addr;

		if (sskip >= packl) {
			sskip -= packl;
			continue;
		}

		packl -= sskip;
		packp += sskip;
		sskip = 0;

		if (packl > 0xfffc) {
			err = EIO;
			goto errout;
		}

		if (j == 0)
			pb = &dmap->d_dma->d_mcr.mcr_ipktbuf;
		else
			pb = &dmap->d_dma->d_sbuf[j - 1];

		pb->pb_addr = htole32(packp);

		if (stheend) {
			if (packl > stheend) {
				pb->pb_len = htole32(stheend);
				stheend = 0;
			} else {
				pb->pb_len = htole32(packl);
				stheend -= packl;
			}
		} else
			pb->pb_len = htole32(packl);

		if ((i + 1) == q->q_src_map->dm_nsegs)
			pb->pb_next = 0;
		else
			pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
			    offsetof(struct ubsec_dmachunk, d_sbuf[j]));
		j++;
	}

	if (enccrd == NULL && maccrd != NULL) {
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_addr = 0;
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_len = 0;
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_next =
		    htole32(dmap->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_macbuf[0]));
#ifdef UBSEC_DEBUG
		printf("opkt: %x %x %x\n",
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_addr,
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_len,
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_next);
#endif
	} else {
		if (crp->crp_flags & CRYPTO_F_IOV) {
			if (!nicealign) {
				err = EINVAL;
				goto errout;
			}
			if (bus_dmamap_create(sc->sc_dmat, 0xfff0,
			    UBS_MAX_SCATTER, 0xfff0, 0, BUS_DMA_NOWAIT,
			    &q->q_dst_map) != 0) {
				err = ENOMEM;
				goto errout;
			}
			if (bus_dmamap_load_uio(sc->sc_dmat, q->q_dst_map,
			    q->q_dst_io, BUS_DMA_NOWAIT) != 0) {
				bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
				q->q_dst_map = NULL;
				goto errout;
			}
		} else if (crp->crp_flags & CRYPTO_F_IMBUF) {
			if (nicealign) {
				q->q_dst_m = q->q_src_m;
				q->q_dst_map = q->q_src_map;
			} else {
				q->q_dst_m = m_dup_pkt(q->q_src_m, 0,
				    M_NOWAIT);
				if (q->q_dst_m == NULL) {
 					err = ENOMEM;
 					goto errout;
 				}
				if (bus_dmamap_create(sc->sc_dmat, 0xfff0,
				    UBS_MAX_SCATTER, 0xfff0, 0, BUS_DMA_NOWAIT,
				    &q->q_dst_map) != 0) {
					err = ENOMEM;
					goto errout;
				}
				if (bus_dmamap_load_mbuf(sc->sc_dmat,
				    q->q_dst_map, q->q_dst_m,
				    BUS_DMA_NOWAIT) != 0) {
					bus_dmamap_destroy(sc->sc_dmat,
					q->q_dst_map);
					q->q_dst_map = NULL;
					err = ENOMEM;
					goto errout;
				}
			}
		} else {
			err = EINVAL;
			goto errout;
		}

#ifdef UBSEC_DEBUG
		printf("dst skip: %d\n", dskip);
#endif
		for (i = j = 0; i < q->q_dst_map->dm_nsegs; i++) {
			struct ubsec_pktbuf *pb;
			bus_size_t packl = q->q_dst_map->dm_segs[i].ds_len;
			bus_addr_t packp = q->q_dst_map->dm_segs[i].ds_addr;

			if (dskip >= packl) {
				dskip -= packl;
				continue;
			}

			packl -= dskip;
			packp += dskip;
			dskip = 0;

			if (packl > 0xfffc) {
				err = EIO;
				goto errout;
			}

			if (j == 0)
				pb = &dmap->d_dma->d_mcr.mcr_opktbuf;
			else
				pb = &dmap->d_dma->d_dbuf[j - 1];

			pb->pb_addr = htole32(packp);

			if (dtheend) {
				if (packl > dtheend) {
					pb->pb_len = htole32(dtheend);
					dtheend = 0;
				} else {
					pb->pb_len = htole32(packl);
					dtheend -= packl;
				}
			} else
				pb->pb_len = htole32(packl);

			if ((i + 1) == q->q_dst_map->dm_nsegs) {
				if (maccrd)
					pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
					    offsetof(struct ubsec_dmachunk, d_macbuf[0]));
				else
					pb->pb_next = 0;
			} else
				pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
				    offsetof(struct ubsec_dmachunk, d_dbuf[j]));
			j++;
		}
	}

	dmap->d_dma->d_mcr.mcr_cmdctxp = htole32(dmap->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_ctx));

	if (enccrd && enccrd->crd_alg == CRYPTO_AES_CBC) {
		struct ubsec_pktctx_aes128	*aes128;
		struct ubsec_pktctx_aes192	*aes192;
		struct ubsec_pktctx_aes256	*aes256;
		struct ubsec_pktctx_hdr		*ph;
		u_int8_t			*ctx;

		ctx = (u_int8_t *)(dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx));

		ph = (struct ubsec_pktctx_hdr *)ctx;
		ph->ph_type = htole16(UBS_PKTCTX_TYPE_IPSEC_AES);
		ph->ph_flags = flags;
		ph->ph_offset = htole16(coffset >> 2);

		switch (enccrd->crd_klen) {
		case 128:
			aes128 = (struct ubsec_pktctx_aes128 *)ctx;
 			ph->ph_len = htole16(sizeof(*aes128));
			ph->ph_flags |= htole16(UBS_PKTCTX_KEYSIZE_128);
			for (i = 0; i < 4; i++)
				aes128->pc_aeskey[i] = key.ses_key[i];
			for (i = 0; i < 5; i++)
				aes128->pc_hminner[i] = key.ses_hminner[i];
			for (i = 0; i < 5; i++)
				aes128->pc_hmouter[i] = key.ses_hmouter[i];   
			for (i = 0; i < 4; i++)
				aes128->pc_iv[i] = key.ses_iv[i];
			break;
		case 192:
			aes192 = (struct ubsec_pktctx_aes192 *)ctx;
			ph->ph_len = htole16(sizeof(*aes192));
			ph->ph_flags |= htole16(UBS_PKTCTX_KEYSIZE_192);
			for (i = 0; i < 6; i++)
				aes192->pc_aeskey[i] = key.ses_key[i];
			for (i = 0; i < 5; i++)
				aes192->pc_hminner[i] = key.ses_hminner[i];
			for (i = 0; i < 5; i++)
				aes192->pc_hmouter[i] = key.ses_hmouter[i];   
			for (i = 0; i < 4; i++)
				aes192->pc_iv[i] = key.ses_iv[i];
			break;
		case 256:
			aes256 = (struct ubsec_pktctx_aes256 *)ctx;
			ph->ph_len = htole16(sizeof(*aes256));
			ph->ph_flags |= htole16(UBS_PKTCTX_KEYSIZE_256);
			for (i = 0; i < 8; i++)
				aes256->pc_aeskey[i] = key.ses_key[i];
			for (i = 0; i < 5; i++)
				aes256->pc_hminner[i] = key.ses_hminner[i];
			for (i = 0; i < 5; i++)
				aes256->pc_hmouter[i] = key.ses_hmouter[i];   
			for (i = 0; i < 4; i++)
				aes256->pc_iv[i] = key.ses_iv[i];
			break;
		}
	} else if (sc->sc_flags & UBS_FLAGS_LONGCTX) {
		struct ubsec_pktctx_3des	*ctx;
		struct ubsec_pktctx_hdr		*ph;

		ctx = (struct ubsec_pktctx_3des *)
		    (dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx));

		ph = (struct ubsec_pktctx_hdr *)ctx;
		ph->ph_len = htole16(sizeof(*ctx));
		ph->ph_type = htole16(UBS_PKTCTX_TYPE_IPSEC_3DES);
		ph->ph_flags = flags;
		ph->ph_offset = htole16(coffset >> 2);

		for (i = 0; i < 6; i++)
			ctx->pc_deskey[i] = key.ses_key[i];
		for (i = 0; i < 5; i++)
			ctx->pc_hminner[i] = key.ses_hminner[i];
		for (i = 0; i < 5; i++)
			ctx->pc_hmouter[i] = key.ses_hmouter[i]; 
		for (i = 0; i < 2; i++)
			ctx->pc_iv[i] = key.ses_iv[i];
	} else {
		struct ubsec_pktctx *ctx = (struct ubsec_pktctx *)
		    (dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx));

		ctx->pc_flags = flags;
		ctx->pc_offset = htole16(coffset >> 2);
		for (i = 0; i < 6; i++)
			ctx->pc_deskey[i] = key.ses_key[i];
		for (i = 0; i < 5; i++)
			ctx->pc_hminner[i] = key.ses_hminner[i];
		for (i = 0; i < 5; i++)
			ctx->pc_hmouter[i] = key.ses_hmouter[i];   
		for (i = 0; i < 2; i++)
			ctx->pc_iv[i] = key.ses_iv[i];
	}

	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue, q, q_next);
	sc->sc_nqueue++;
	ubsecstats.hst_ipackets++;
	ubsecstats.hst_ibytes += dmap->d_alloc.dma_map->dm_mapsize;
	ubsec_feed(sc);
	splx(s);
	explicit_bzero(&key, sizeof(key));
	return (0);

errout:
	if (q != NULL) {
		if ((q->q_dst_m != NULL) && (q->q_src_m != q->q_dst_m))
			m_freem(q->q_dst_m);

		if (q->q_dst_map != NULL && q->q_dst_map != q->q_src_map) {
			bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
			bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
		}
		if (q->q_src_map != NULL) {
			bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
		}

		s = splnet();
		SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
		splx(s);
	}
	if (err == EINVAL)
		ubsecstats.hst_invalid++;
	else
		ubsecstats.hst_nomem++;
errout2:
	crp->crp_etype = err;
	crypto_done(crp);
	explicit_bzero(&key, sizeof(key));
	return (0);
}

void
ubsec_callback(struct ubsec_softc *sc, struct ubsec_q *q)
{
	struct cryptop *crp = (struct cryptop *)q->q_crp;
	struct cryptodesc *crd;
	struct ubsec_dma *dmap = q->q_dma;
	u_int8_t *ctx = (u_int8_t *)(dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx));
	struct ubsec_pktctx_hdr *ph = (struct ubsec_pktctx_hdr *)ctx;
	int i;

	ubsecstats.hst_opackets++;
	ubsecstats.hst_obytes += dmap->d_alloc.dma_size;

	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
	    dmap->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE);
	if (q->q_dst_map != NULL && q->q_dst_map != q->q_src_map) {
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
		bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
	}
	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
	bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);

	explicit_bzero(ctx, ph->ph_len);

	if ((crp->crp_flags & CRYPTO_F_IMBUF) && (q->q_src_m != q->q_dst_m)) {
		m_freem(q->q_src_m);
		crp->crp_buf = (caddr_t)q->q_dst_m;
	}

	for (i = 0; i < crp->crp_ndesc; i++) {
		crd = &crp->crp_desc[i];
		if (crd->crd_alg != CRYPTO_MD5_HMAC &&
		    crd->crd_alg != CRYPTO_SHA1_HMAC)
			continue;
		if (crp->crp_flags & CRYPTO_F_IMBUF)
			crp->crp_etype = m_copyback((struct mbuf *)crp->crp_buf,
			    crd->crd_inject, 12,
			    dmap->d_dma->d_macbuf, M_NOWAIT);
		else if (crp->crp_flags & CRYPTO_F_IOV && crp->crp_mac)
			bcopy((caddr_t)dmap->d_dma->d_macbuf,
			    crp->crp_mac, 12);
		break;
	}
	SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
	crypto_done(crp);
}

/*
 * feed the key generator, must be called at splnet() or higher.
 */
void
ubsec_feed2(struct ubsec_softc *sc)
{
	struct ubsec_q2 *q;

	while (!SIMPLEQ_EMPTY(&sc->sc_queue2)) {
		if (READ_REG(sc, BS_STAT) & BS_STAT_MCR2_FULL)
			break;
		q = SIMPLEQ_FIRST(&sc->sc_queue2);

		bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map, 0,
		    q->q_mcr.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
		bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
		    q->q_ctx.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREWRITE);

		WRITE_REG(sc, BS_MCR2, q->q_mcr.dma_paddr);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue2, q_next);
		--sc->sc_nqueue2;
		SIMPLEQ_INSERT_TAIL(&sc->sc_qchip2, q, q_next);
	}
}

/*
 * feed the RNG (used instead of ubsec_feed2() on 5827+ devices)
 */
void
ubsec_feed4(struct ubsec_softc *sc)
{
	struct ubsec_q2 *q;

	while (!SIMPLEQ_EMPTY(&sc->sc_queue4)) {
		if (READ_REG(sc, BS_STAT) & BS_STAT_MCR4_FULL)
			break;
		q = SIMPLEQ_FIRST(&sc->sc_queue4);

		bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map, 0,
		    q->q_mcr.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
		bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
		    q->q_ctx.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREWRITE);

		WRITE_REG(sc, BS_MCR4, q->q_mcr.dma_paddr);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue4, q_next);
		--sc->sc_nqueue4;
		SIMPLEQ_INSERT_TAIL(&sc->sc_qchip4, q, q_next);
	}
}

/*
 * Callback for handling random numbers
 */
void
ubsec_callback2(struct ubsec_softc *sc, struct ubsec_q2 *q)
{
	struct ubsec_ctx_keyop *ctx;

	ctx = (struct ubsec_ctx_keyop *)q->q_ctx.dma_vaddr;
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
	    q->q_ctx.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);

	switch (q->q_type) {
#ifndef UBSEC_NO_RNG
	case UBS_CTXOP_RNGSHA1:
	case UBS_CTXOP_RNGBYPASS: {
		struct ubsec_q2_rng *rng = (struct ubsec_q2_rng *)q;
		u_int32_t *p;
		int i;

		bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
		    rng->rng_buf.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		p = (u_int32_t *)rng->rng_buf.dma_vaddr;
		for (i = 0; i < UBSEC_RNG_BUFSIZ; p++, i++)
			add_true_randomness(*p);
		rng->rng_used = 0;
		timeout_add(&sc->sc_rngto, sc->sc_rnghz);
		break;
	}
#endif
	default:
		printf("%s: unknown ctx op: %x\n", sc->sc_dv.dv_xname,
		    letoh16(ctx->ctx_op));
		break;
	}
}

#ifndef UBSEC_NO_RNG
void
ubsec_rng(void *vsc)
{
	struct ubsec_softc *sc = vsc;
	struct ubsec_q2_rng *rng = &sc->sc_rng;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_rngbypass *ctx;
	int s, *nqueue;

	s = splnet();
	if (rng->rng_used) {
		splx(s);
		return;
	}
	if (sc->sc_flags & UBS_FLAGS_RNG4)
		nqueue = &sc->sc_nqueue4;
	else
		nqueue = &sc->sc_nqueue2;

	(*nqueue)++;
	if (*nqueue >= UBS_MAX_NQUEUE)
		goto out;

	mcr = (struct ubsec_mcr *)rng->rng_q.q_mcr.dma_vaddr;
	ctx = (struct ubsec_ctx_rngbypass *)rng->rng_q.q_ctx.dma_vaddr;

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(rng->rng_q.q_ctx.dma_paddr);
	mcr->mcr_ipktbuf.pb_addr = mcr->mcr_ipktbuf.pb_next = 0;
	mcr->mcr_ipktbuf.pb_len = 0;
	mcr->mcr_reserved = mcr->mcr_pktlen = 0;
	mcr->mcr_opktbuf.pb_addr = htole32(rng->rng_buf.dma_paddr);
	mcr->mcr_opktbuf.pb_len = htole32(((sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ)) &
	    UBS_PKTBUF_LEN);
	mcr->mcr_opktbuf.pb_next = 0;

	ctx->rbp_len = htole16(sizeof(struct ubsec_ctx_rngbypass));
	ctx->rbp_op = htole16(UBS_CTXOP_RNGSHA1);
	rng->rng_q.q_type = UBS_CTXOP_RNGSHA1;

	bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
	    rng->rng_buf.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	if (sc->sc_flags & UBS_FLAGS_RNG4) {
		SIMPLEQ_INSERT_TAIL(&sc->sc_queue4, &rng->rng_q, q_next);
		rng->rng_used = 1;
		ubsec_feed4(sc);
	} else {
		SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rng->rng_q, q_next);
		rng->rng_used = 1;
		ubsec_feed2(sc);
	}
	splx(s);

	return;

out:
	/*
	 * Something weird happened, generate our own call back.
	 */
	(*nqueue)--;
	splx(s);
	timeout_add(&sc->sc_rngto, sc->sc_rnghz);
}
#endif /* UBSEC_NO_RNG */

int
ubsec_dma_malloc(struct ubsec_softc *sc, bus_size_t size,
    struct ubsec_dma_alloc *dma, int mapflags)
{
	int r;

	if ((r = bus_dmamem_alloc(sc->sc_dmat, size, PAGE_SIZE, 0,
	    &dma->dma_seg, 1, &dma->dma_nseg, BUS_DMA_NOWAIT)) != 0)
		goto fail_0;

	if ((r = bus_dmamem_map(sc->sc_dmat, &dma->dma_seg, dma->dma_nseg,
	    size, &dma->dma_vaddr, mapflags | BUS_DMA_NOWAIT)) != 0)
		goto fail_1;

	if ((r = bus_dmamap_create(sc->sc_dmat, size, 1, size, 0,
	    BUS_DMA_NOWAIT, &dma->dma_map)) != 0)
		goto fail_2;

	if ((r = bus_dmamap_load(sc->sc_dmat, dma->dma_map, dma->dma_vaddr,
	    size, NULL, BUS_DMA_NOWAIT)) != 0)
		goto fail_3;

	dma->dma_paddr = dma->dma_map->dm_segs[0].ds_addr;
	dma->dma_size = size;
	return (0);

fail_3:
	bus_dmamap_destroy(sc->sc_dmat, dma->dma_map);
fail_2:
	bus_dmamem_unmap(sc->sc_dmat, dma->dma_vaddr, size);
fail_1:
	bus_dmamem_free(sc->sc_dmat, &dma->dma_seg, dma->dma_nseg);
fail_0:
	dma->dma_map = NULL;
	return (r);
}

void
ubsec_dma_free(struct ubsec_softc *sc, struct ubsec_dma_alloc *dma)
{
	bus_dmamap_unload(sc->sc_dmat, dma->dma_map);
	bus_dmamem_unmap(sc->sc_dmat, dma->dma_vaddr, dma->dma_size);
	bus_dmamem_free(sc->sc_dmat, &dma->dma_seg, dma->dma_nseg);
	bus_dmamap_destroy(sc->sc_dmat, dma->dma_map);
}

/*
 * Resets the board.  Values in the regesters are left as is
 * from the reset (i.e. initial values are assigned elsewhere).
 */
void
ubsec_reset_board(struct ubsec_softc *sc)
{
	volatile u_int32_t ctrl;

	/* Reset the device */
	ctrl = READ_REG(sc, BS_CTRL);
	ctrl |= BS_CTRL_RESET;
	WRITE_REG(sc, BS_CTRL, ctrl);

	/*
	* Wait aprox. 30 PCI clocks = 900 ns = 0.9 us
	*/
	DELAY(10);

	/* Enable RNG and interrupts on newer devices */
	if (sc->sc_flags & UBS_FLAGS_MULTIMCR) {
		WRITE_REG(sc, BS_CFG, BS_CFG_RNG);
		WRITE_REG(sc, BS_INT, BS_INT_DMAINT);
	}
}

/*
 * Init Broadcom registers
 */
void
ubsec_init_board(struct ubsec_softc *sc)
{
	u_int32_t ctrl;

	ctrl = READ_REG(sc, BS_CTRL);
	ctrl &= ~(BS_CTRL_BE32 | BS_CTRL_BE64);
	ctrl |= BS_CTRL_LITTLE_ENDIAN | BS_CTRL_MCR1INT;

	if (sc->sc_flags & UBS_FLAGS_KEY)
		ctrl |= BS_CTRL_MCR2INT;
	else
		ctrl &= ~BS_CTRL_MCR2INT;

	if (sc->sc_flags & UBS_FLAGS_HWNORM)
		ctrl &= ~BS_CTRL_SWNORM;

	if (sc->sc_flags & UBS_FLAGS_MULTIMCR) {
		ctrl |= BS_CTRL_BSIZE240;
		ctrl &= ~BS_CTRL_MCR3INT; /* MCR3 is reserved for SSL */

		if (sc->sc_flags & UBS_FLAGS_RNG4)
			ctrl |= BS_CTRL_MCR4INT;
		else
			ctrl &= ~BS_CTRL_MCR4INT;
	}

	WRITE_REG(sc, BS_CTRL, ctrl);
}

/*
 * Init Broadcom PCI registers
 */
void
ubsec_init_pciregs(struct pci_attach_args *pa)
{
	pci_chipset_tag_t pc = pa->pa_pc;
	u_int32_t misc;

	/*
	 * This will set the cache line size to 1, this will
	 * force the BCM58xx chip just to do burst read/writes.
	 * Cache line read/writes are to slow
	 */
	misc = pci_conf_read(pc, pa->pa_tag, PCI_BHLC_REG);
	misc = (misc & ~(PCI_CACHELINE_MASK << PCI_CACHELINE_SHIFT))
	    | ((UBS_DEF_CACHELINE & 0xff) << PCI_CACHELINE_SHIFT);
	pci_conf_write(pc, pa->pa_tag, PCI_BHLC_REG, misc);
}

/*
 * Clean up after a chip crash.
 * It is assumed that the caller is in splnet()
 */
void
ubsec_cleanchip(struct ubsec_softc *sc)
{
	struct ubsec_q *q;

	while (!SIMPLEQ_EMPTY(&sc->sc_qchip)) {
		q = SIMPLEQ_FIRST(&sc->sc_qchip);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q_next);
		ubsec_free_q(sc, q);
	}
}

/*
 * free a ubsec_q
 * It is assumed that the caller is within splnet()
 */
int
ubsec_free_q(struct ubsec_softc *sc, struct ubsec_q *q)
{
	struct ubsec_q *q2;
	struct cryptop *crp;
	int npkts;
	int i;

	npkts = q->q_nstacked_mcrs;

	for (i = 0; i < npkts; i++) {
		if(q->q_stacked_mcr[i]) {
			q2 = q->q_stacked_mcr[i];

			if ((q2->q_dst_m != NULL) && (q2->q_src_m != q2->q_dst_m)) 
				m_freem(q2->q_dst_m);

			crp = (struct cryptop *)q2->q_crp;
			
			SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q2, q_next);
			
			crp->crp_etype = EFAULT;
			crypto_done(crp);
		} else {
			break;
		}
	}

	/*
	 * Free header MCR
	 */
	if ((q->q_dst_m != NULL) && (q->q_src_m != q->q_dst_m))
		m_freem(q->q_dst_m);

	crp = (struct cryptop *)q->q_crp;
	
	SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
	
	crp->crp_etype = EFAULT;
	crypto_done(crp);
	return(0);
}

/*
 * Routine to reset the chip and clean up.
 * It is assumed that the caller is in splnet()
 */
void
ubsec_totalreset(struct ubsec_softc *sc)
{
	ubsec_reset_board(sc);
	ubsec_init_board(sc);
	ubsec_cleanchip(sc);
}

int
ubsec_dmamap_aligned(bus_dmamap_t map)
{
	int i;

	for (i = 0; i < map->dm_nsegs; i++) {
		if (map->dm_segs[i].ds_addr & 3)
			return (0);
		if ((i != (map->dm_nsegs - 1)) &&
		    (map->dm_segs[i].ds_len & 3))
			return (0);
	}
	return (1);
}
@


1.162
log
@replace m_copym2 with m_dup_pkt.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.161 2015/12/10 21:00:51 naddy Exp $	*/
d779 1
a779 1
	struct cryptodesc *crd1, *crd2, *maccrd, *enccrd;
d838 1
a838 2
	crd1 = crp->crp_desc;
	if (crd1 == NULL) {
d842 3
a844 1
	crd2 = crd1->crd_next;
d1334 1
d1360 2
a1361 1
	for (crd = crp->crp_desc; crd; crd = crd->crd_next) {
@


1.161
log
@Remove plain DES from the kernel crypto framework, including the crypto
accelerator drivers.  No longer used by anything.  ok sthen@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.160 2014/08/15 15:37:51 mikeb Exp $	*/
d1107 1
a1107 1
				q->q_dst_m = m_copym2(q->q_src_m, 0, M_COPYALL,
@


1.160
log
@Remove support for public key operations
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.159 2014/07/13 23:10:23 deraadt Exp $	*/
a269 1
	algs[CRYPTO_DES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
d629 1
a629 2
		} else if (c->cri_alg == CRYPTO_DES_CBC ||
		    c->cri_alg == CRYPTO_3DES_CBC ||
a689 4
		} else if (encini->cri_alg == CRYPTO_DES_CBC) {
			bcopy(encini->cri_key, &ses->ses_key[0], 8);
			bcopy(encini->cri_key, &ses->ses_key[2], 8);
			bcopy(encini->cri_key, &ses->ses_key[4], 8);
d850 1
a850 2
		} else if (crd1->crd_alg == CRYPTO_DES_CBC ||
		    crd1->crd_alg == CRYPTO_3DES_CBC ||
d861 1
a861 2
		    (crd2->crd_alg == CRYPTO_DES_CBC ||
		    crd2->crd_alg == CRYPTO_3DES_CBC ||
d866 1
a866 2
		} else if ((crd1->crd_alg == CRYPTO_DES_CBC ||
		    crd1->crd_alg == CRYPTO_3DES_CBC ||
@


1.159
log
@Some reallocarray() use; review Jean-Philippe Ouellet, patrick keshishian
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.158 2014/07/12 18:48:52 tedu Exp $	*/
a98 15
int	ubsec_kprocess(struct cryptkop *);
struct ubsec_softc *ubsec_kfind(struct cryptkop *);
int	ubsec_kprocess_modexp_sw(struct ubsec_softc *, struct cryptkop *);
int	ubsec_kprocess_modexp_hw(struct ubsec_softc *, struct cryptkop *);
int	ubsec_kprocess_rsapriv(struct ubsec_softc *, struct cryptkop *);
void	ubsec_kfree(struct ubsec_softc *, struct ubsec_q2 *);
int	ubsec_ksigbits(struct crparam *);
void	ubsec_kshift_r(u_int, u_int8_t *, u_int, u_int8_t *, u_int);
void	ubsec_kshift_l(u_int, u_int8_t *, u_int, u_int8_t *, u_int);

/* DEBUG crap... */
void ubsec_dump_pb(struct ubsec_pktbuf *);
void ubsec_dump_mcr(struct ubsec_mcr *);
void ubsec_dump_ctx2(struct ubsec_ctx_keyop *);

a148 1
	int kalgs[CRK_ALGORITHM_MAX + 1];
a153 1
	SIMPLEQ_INIT(&sc->sc_q2free);
a334 9

		bzero(kalgs, sizeof(kalgs));
		kalgs[CRK_MOD_EXP] = CRYPTO_ALG_FLAG_SUPPORTED;
#if 0
		kalgs[CRK_MOD_EXP_CRT] = CRYPTO_ALG_FLAG_SUPPORTED;
#endif

		crypto_kregister(sc->sc_cid, kalgs, ubsec_kprocess);
		printf(" PK");
a1443 1
	struct cryptkop *krp;
a1467 69
	case UBS_CTXOP_MODEXP: {
		struct ubsec_q2_modexp *me = (struct ubsec_q2_modexp *)q;
		u_int rlen, clen;

		krp = me->me_krp;
		rlen = (me->me_modbits + 7) / 8;
		clen = (krp->krp_param[krp->krp_iparams].crp_nbits + 7) / 8;

		bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
		    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
		    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
		    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
		    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);

		if (clen < rlen)
			krp->krp_status = E2BIG;
		else {
			if (sc->sc_flags & UBS_FLAGS_HWNORM) {
				bzero(krp->krp_param[krp->krp_iparams].crp_p,
				    (krp->krp_param[krp->krp_iparams].crp_nbits
					+ 7) / 8);
				bcopy(me->me_C.dma_vaddr,
				    krp->krp_param[krp->krp_iparams].crp_p,
				    (me->me_modbits + 7) / 8);
			} else
				ubsec_kshift_l(me->me_shiftbits,
				    me->me_C.dma_vaddr, me->me_normbits,
				    krp->krp_param[krp->krp_iparams].crp_p,
				    krp->krp_param[krp->krp_iparams].crp_nbits);
		}
		crypto_kdone(krp);

		/* bzero all potentially sensitive data */
		explicit_bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
		explicit_bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
		explicit_bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
		explicit_bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);

		/* Can't free here, so put us on the free list. */
		SIMPLEQ_INSERT_TAIL(&sc->sc_q2free, &me->me_q, q_next);
		break;
	}
	case UBS_CTXOP_RSAPRIV: {
		struct ubsec_q2_rsapriv *rp = (struct ubsec_q2_rsapriv *)q;
		u_int len;

		krp = rp->rpr_krp;
		bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map, 0,
		    rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgout.dma_map, 0,
		    rp->rpr_msgout.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);

		len = (krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_nbits + 7) / 8;
		bcopy(rp->rpr_msgout.dma_vaddr,
		    krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_p, len);

		crypto_kdone(krp);

		explicit_bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
		explicit_bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
		explicit_bzero(rp->rpr_q.q_ctx.dma_vaddr, rp->rpr_q.q_ctx.dma_size);

		/* Can't free here, so put us on the free list. */
		SIMPLEQ_INSERT_TAIL(&sc->sc_q2free, &rp->rpr_q, q_next);
		break;
	}
a1755 810
}

struct ubsec_softc *
ubsec_kfind(struct cryptkop *krp)
{
	struct ubsec_softc *sc;
	int i;

	for (i = 0; i < ubsec_cd.cd_ndevs; i++) {
		sc = ubsec_cd.cd_devs[i];
		if (sc == NULL)
			continue;
		if (sc->sc_cid == krp->krp_hid)
			return (sc);
	}
	return (NULL);
}

void
ubsec_kfree(struct ubsec_softc *sc, struct ubsec_q2 *q)
{
	switch (q->q_type) {
	case UBS_CTXOP_MODEXP: {
		struct ubsec_q2_modexp *me = (struct ubsec_q2_modexp *)q;

		ubsec_dma_free(sc, &me->me_q.q_mcr);
		ubsec_dma_free(sc, &me->me_q.q_ctx);
		ubsec_dma_free(sc, &me->me_M);
		ubsec_dma_free(sc, &me->me_E);
		ubsec_dma_free(sc, &me->me_C);
		ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF, 0);
		break;
	}
	case UBS_CTXOP_RSAPRIV: {
		struct ubsec_q2_rsapriv *rp = (struct ubsec_q2_rsapriv *)q;

		ubsec_dma_free(sc, &rp->rpr_q.q_mcr);
		ubsec_dma_free(sc, &rp->rpr_q.q_ctx);
		ubsec_dma_free(sc, &rp->rpr_msgin);
		ubsec_dma_free(sc, &rp->rpr_msgout);
		free(rp, M_DEVBUF, 0);
		break;
	}
	default:
		printf("%s: invalid kfree 0x%x\n", sc->sc_dv.dv_xname,
		    q->q_type);
		break;
	}
}

int
ubsec_kprocess(struct cryptkop *krp)
{
	struct ubsec_softc *sc;
	int r;

	if (krp == NULL || krp->krp_callback == NULL)
		return (EINVAL);
	if ((sc = ubsec_kfind(krp)) == NULL)
		return (EINVAL);

	while (!SIMPLEQ_EMPTY(&sc->sc_q2free)) {
		struct ubsec_q2 *q;

		q = SIMPLEQ_FIRST(&sc->sc_q2free);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_q2free, q_next);
		ubsec_kfree(sc, q);
	}

	switch (krp->krp_op) {
	case CRK_MOD_EXP:
		if (sc->sc_flags & UBS_FLAGS_HWNORM)
			r = ubsec_kprocess_modexp_hw(sc, krp);
		else
			r = ubsec_kprocess_modexp_sw(sc, krp);
		break;
	case CRK_MOD_EXP_CRT:
		r = ubsec_kprocess_rsapriv(sc, krp);
		break;
	default:
		printf("%s: kprocess: invalid op 0x%x\n",
		    sc->sc_dv.dv_xname, krp->krp_op);
		krp->krp_status = EOPNOTSUPP;
		crypto_kdone(krp);
		r = 0;
	}
	return (r);
}

/*
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N] (sw normalization)
 */
int
ubsec_kprocess_modexp_sw(struct ubsec_softc *sc, struct cryptkop *krp)
{
	struct ubsec_q2_modexp *me;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_modexp *ctx;
	struct ubsec_pktbuf *epb;
	int err = 0, s;
	u_int nbits, normbits, mbits, shiftbits, ebits;

	me = malloc(sizeof *me, M_DEVBUF, M_NOWAIT | M_ZERO);
	if (me == NULL) {
		err = ENOMEM;
		goto errout;
	}
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;

	nbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_N]);
	if (nbits <= 512)
		normbits = 512;
	else if (nbits <= 768)
		normbits = 768;
	else if (nbits <= 1024)
		normbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 1536)
		normbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 2048)
		normbits = 2048;
	else {
		err = E2BIG;
		goto errout;
	}

	shiftbits = normbits - nbits;

	me->me_modbits = nbits;
	me->me_shiftbits = shiftbits;
	me->me_normbits = normbits;

	/* Sanity check: result bits must be >= true modulus bits. */
	if (krp->krp_param[krp->krp_iparams].crp_nbits < nbits) {
		err = ERANGE;
		goto errout;
	}

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &me->me_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_modexp),
	    &me->me_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}

	mbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_M]);
	if (mbits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_M, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_M].crp_p, mbits,
	    me->me_M.dma_vaddr, normbits);

	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_E, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_E].crp_p, ebits,
	    me->me_E.dma_vaddr, normbits);

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktbuf),
	    &me->me_epb, 0)) {
		err = ENOMEM;
		goto errout;
	}
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
	epb->pb_addr = htole32(me->me_E.dma_paddr);
	epb->pb_next = 0;
	epb->pb_len = htole32(normbits / 8);

#ifdef UBSEC_DEBUG
	printf("Epb ");
	ubsec_dump_pb(epb);
#endif

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(me->me_q.q_ctx.dma_paddr);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = 0;

	mcr->mcr_ipktbuf.pb_addr = htole32(me->me_M.dma_paddr);
	mcr->mcr_ipktbuf.pb_len = htole32(normbits / 8);
	mcr->mcr_ipktbuf.pb_next = htole32(me->me_epb.dma_paddr);

	mcr->mcr_opktbuf.pb_addr = htole32(me->me_C.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(normbits / 8);

#ifdef DIAGNOSTIC
	/* Misaligned output buffer will hang the chip. */
	if ((letoh32(mcr->mcr_opktbuf.pb_addr) & 3) != 0)
		panic("%s: modexp invalid addr 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_addr));
	if ((letoh32(mcr->mcr_opktbuf.pb_len) & 3) != 0)
		panic("%s: modexp invalid len 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_len));
#endif

	ctx = (struct ubsec_ctx_modexp *)me->me_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof(*ctx));
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_N].crp_p, nbits,
	    ctx->me_N, normbits);
	ctx->me_len = htole16((normbits / 8) + (4 * sizeof(u_int16_t)));
	ctx->me_op = htole16(UBS_CTXOP_MODEXP);
	ctx->me_E_len = htole16(nbits);
	ctx->me_N_len = htole16(nbits);

#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
	ubsec_dump_ctx2((struct ubsec_ctx_keyop *)ctx);
#endif

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
	    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
	    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
	    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
	    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &me->me_q, q_next);
	ubsec_feed2(sc);
	splx(s);

	return (0);

errout:
	if (me != NULL) {
		if (me->me_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_q.q_mcr);
		if (me->me_q.q_ctx.dma_map != NULL) {
			explicit_bzero(me->me_q.q_ctx.dma_vaddr,
			    me->me_q.q_ctx.dma_size);
			ubsec_dma_free(sc, &me->me_q.q_ctx);
		}
		if (me->me_M.dma_map != NULL) {
			explicit_bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
			ubsec_dma_free(sc, &me->me_M);
		}
		if (me->me_E.dma_map != NULL) {
			explicit_bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
			ubsec_dma_free(sc, &me->me_E);
		}
		if (me->me_C.dma_map != NULL) {
			explicit_bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
			ubsec_dma_free(sc, &me->me_C);
		}
		if (me->me_epb.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF, 0);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

/*
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N] (hw normalization)
 */
int
ubsec_kprocess_modexp_hw(struct ubsec_softc *sc, struct cryptkop *krp)
{
	struct ubsec_q2_modexp *me;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_modexp *ctx;
	struct ubsec_pktbuf *epb;
	int err = 0, s;
	u_int nbits, normbits, mbits, shiftbits, ebits;

	me = malloc(sizeof *me, M_DEVBUF, M_NOWAIT | M_ZERO);
	if (me == NULL) {
		err = ENOMEM;
		goto errout;
	}
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;

	nbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_N]);
	if (nbits <= 512)
		normbits = 512;
	else if (nbits <= 768)
		normbits = 768;
	else if (nbits <= 1024)
		normbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 1536)
		normbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 2048)
		normbits = 2048;
	else {
		err = E2BIG;
		goto errout;
	}

	shiftbits = normbits - nbits;

	/* XXX ??? */
	me->me_modbits = nbits;
	me->me_shiftbits = shiftbits;
	me->me_normbits = normbits;

	/* Sanity check: result bits must be >= true modulus bits. */
	if (krp->krp_param[krp->krp_iparams].crp_nbits < nbits) {
		err = ERANGE;
		goto errout;
	}

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &me->me_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_modexp),
	    &me->me_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}

	mbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_M]);
	if (mbits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_M, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_M.dma_vaddr, normbits / 8);
	bcopy(krp->krp_param[UBS_MODEXP_PAR_M].crp_p,
	    me->me_M.dma_vaddr, (mbits + 7) / 8);

	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_E, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_E.dma_vaddr, normbits / 8);
	bcopy(krp->krp_param[UBS_MODEXP_PAR_E].crp_p,
	    me->me_E.dma_vaddr, (ebits + 7) / 8);

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktbuf),
	    &me->me_epb, 0)) {
		err = ENOMEM;
		goto errout;
	}
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
	epb->pb_addr = htole32(me->me_E.dma_paddr);
	epb->pb_next = 0;
	epb->pb_len = htole32((ebits + 7) / 8);

#ifdef UBSEC_DEBUG
	printf("Epb ");
	ubsec_dump_pb(epb);
#endif

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(me->me_q.q_ctx.dma_paddr);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = 0;

	mcr->mcr_ipktbuf.pb_addr = htole32(me->me_M.dma_paddr);
	mcr->mcr_ipktbuf.pb_len = htole32(normbits / 8);
	mcr->mcr_ipktbuf.pb_next = htole32(me->me_epb.dma_paddr);

	mcr->mcr_opktbuf.pb_addr = htole32(me->me_C.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(normbits / 8);

#ifdef DIAGNOSTIC
	/* Misaligned output buffer will hang the chip. */
	if ((letoh32(mcr->mcr_opktbuf.pb_addr) & 3) != 0)
		panic("%s: modexp invalid addr 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_addr));
	if ((letoh32(mcr->mcr_opktbuf.pb_len) & 3) != 0)
		panic("%s: modexp invalid len 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_len));
#endif

	ctx = (struct ubsec_ctx_modexp *)me->me_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof(*ctx));
	bcopy(krp->krp_param[UBS_MODEXP_PAR_N].crp_p, ctx->me_N,
	    (nbits + 7) / 8);
	ctx->me_len = htole16((normbits / 8) + (4 * sizeof(u_int16_t)));
	ctx->me_op = htole16(UBS_CTXOP_MODEXP);
	ctx->me_E_len = htole16(ebits);
	ctx->me_N_len = htole16(nbits);

#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
	ubsec_dump_ctx2((struct ubsec_ctx_keyop *)ctx);
#endif

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
	    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
	    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
	    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
	    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &me->me_q, q_next);
	ubsec_feed2(sc);
	splx(s);

	return (0);

errout:
	if (me != NULL) {
		if (me->me_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_q.q_mcr);
		if (me->me_q.q_ctx.dma_map != NULL) {
			explicit_bzero(me->me_q.q_ctx.dma_vaddr,
			    me->me_q.q_ctx.dma_size);
			ubsec_dma_free(sc, &me->me_q.q_ctx);
		}
		if (me->me_M.dma_map != NULL) {
			explicit_bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
			ubsec_dma_free(sc, &me->me_M);
		}
		if (me->me_E.dma_map != NULL) {
			explicit_bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
			ubsec_dma_free(sc, &me->me_E);
		}
		if (me->me_C.dma_map != NULL) {
			explicit_bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
			ubsec_dma_free(sc, &me->me_C);
		}
		if (me->me_epb.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF, 0);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

int
ubsec_kprocess_rsapriv(struct ubsec_softc *sc, struct cryptkop *krp)
{
	struct ubsec_q2_rsapriv *rp = NULL;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_rsapriv *ctx;
	int err = 0, s;
	u_int padlen, msglen;

	msglen = ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_P]);
	padlen = ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_Q]);
	if (msglen > padlen)
		padlen = msglen;

	if (padlen <= 256)
		padlen = 256;
	else if (padlen <= 384)
		padlen = 384;
	else if (padlen <= 512)
		padlen = 512;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && padlen <= 768)
		padlen = 768;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && padlen <= 1024)
		padlen = 1024;
	else {
		err = E2BIG;
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_DP]) > padlen) {
		err = E2BIG;
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_DQ]) > padlen) {
		err = E2BIG;
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_PINV]) > padlen) {
		err = E2BIG;
		goto errout;
	}

	rp = malloc(sizeof *rp, M_DEVBUF, M_NOWAIT | M_ZERO);
	if (rp == NULL)
		return (ENOMEM);
	rp->rpr_krp = krp;
	rp->rpr_q.q_type = UBS_CTXOP_RSAPRIV;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &rp->rpr_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)rp->rpr_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rsapriv),
	    &rp->rpr_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ctx = (struct ubsec_ctx_rsapriv *)rp->rpr_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof *ctx);

	/* Copy in p */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_P].crp_p,
	    &ctx->rpr_buf[0 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_P].crp_nbits + 7) / 8);

	/* Copy in q */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_p,
	    &ctx->rpr_buf[1 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_nbits + 7) / 8);

	/* Copy in dp */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_p,
	    &ctx->rpr_buf[2 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_nbits + 7) / 8);

	/* Copy in dq */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_p,
	    &ctx->rpr_buf[3 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_nbits + 7) / 8);

	/* Copy in pinv */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_p,
	    &ctx->rpr_buf[4 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_nbits + 7) / 8);

	msglen = padlen * 2;

	/* Copy in input message (aligned buffer/length). */
	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_MSGIN]) > msglen) {
		/* Is this likely? */
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, (msglen + 7) / 8, &rp->rpr_msgin, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(rp->rpr_msgin.dma_vaddr, (msglen + 7) / 8);
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_p,
	    rp->rpr_msgin.dma_vaddr,
	    (krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_nbits + 7) / 8);

	/* Prepare space for output message (aligned buffer/length). */
	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT]) < msglen) {
		/* Is this likely? */
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, (msglen + 7) / 8, &rp->rpr_msgout, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(rp->rpr_msgout.dma_vaddr, (msglen + 7) / 8);

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(rp->rpr_q.q_ctx.dma_paddr);
	mcr->mcr_ipktbuf.pb_addr = htole32(rp->rpr_msgin.dma_paddr);
	mcr->mcr_ipktbuf.pb_next = 0;
	mcr->mcr_ipktbuf.pb_len = htole32(rp->rpr_msgin.dma_size);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = htole16(msglen);
	mcr->mcr_opktbuf.pb_addr = htole32(rp->rpr_msgout.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(rp->rpr_msgout.dma_size);

#ifdef DIAGNOSTIC
	if (rp->rpr_msgin.dma_paddr & 3 || rp->rpr_msgin.dma_size & 3) {
		panic("%s: rsapriv: invalid msgin %08x(0x%lx)",
		    sc->sc_dv.dv_xname, rp->rpr_msgin.dma_paddr,
		    rp->rpr_msgin.dma_size);
	}
	if (rp->rpr_msgout.dma_paddr & 3 || rp->rpr_msgout.dma_size & 3) {
		panic("%s: rsapriv: invalid msgout %08x(0x%lx)",
		    sc->sc_dv.dv_xname, rp->rpr_msgout.dma_paddr,
		    rp->rpr_msgout.dma_size);
	}
#endif

	ctx->rpr_len = (sizeof(u_int16_t) * 4) + (5 * (padlen / 8));
	ctx->rpr_op = htole16(UBS_CTXOP_RSAPRIV);
	ctx->rpr_q_len = htole16(padlen);
	ctx->rpr_p_len = htole16(padlen);

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map,
	    0, rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgout.dma_map,
	    0, rp->rpr_msgout.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rp->rpr_q, q_next);
	ubsec_feed2(sc);
	splx(s);
	return (0);

errout:
	if (rp != NULL) {
		if (rp->rpr_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &rp->rpr_q.q_mcr);
		if (rp->rpr_msgin.dma_map != NULL) {
			explicit_bzero(rp->rpr_msgin.dma_vaddr,
			    rp->rpr_msgin.dma_size);
			ubsec_dma_free(sc, &rp->rpr_msgin);
		}
		if (rp->rpr_msgout.dma_map != NULL) {
			explicit_bzero(rp->rpr_msgout.dma_vaddr,
			    rp->rpr_msgout.dma_size);
			ubsec_dma_free(sc, &rp->rpr_msgout);
		}
		free(rp, M_DEVBUF, 0);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

void
ubsec_dump_pb(struct ubsec_pktbuf *pb)
{
	printf("addr 0x%x (0x%x) next 0x%x\n",
	    pb->pb_addr, pb->pb_len, pb->pb_next);
}

void
ubsec_dump_ctx2(struct ubsec_ctx_keyop *c)
{
	printf("CTX (0x%x):\n", c->ctx_len);
	switch (letoh16(c->ctx_op)) {
	case UBS_CTXOP_RNGBYPASS:
	case UBS_CTXOP_RNGSHA1:
		break;
	case UBS_CTXOP_MODEXP:
	{
		struct ubsec_ctx_modexp *cx = (void *)c;
		int i, len;

		printf(" Elen %u, Nlen %u\n",
		    letoh16(cx->me_E_len), letoh16(cx->me_N_len));
		len = (cx->me_N_len + 7)/8;
		for (i = 0; i < len; i++)
			printf("%s%02x", (i == 0) ? " N: " : ":", cx->me_N[i]);
		printf("\n");
		break;
	}
	default:
		printf("unknown context: %x\n", c->ctx_op);
	}
	printf("END CTX\n");
}

void
ubsec_dump_mcr(struct ubsec_mcr *mcr)
{
	struct ubsec_mcr_add *ma;
	int i;

	printf("MCR:\n");
	printf(" pkts: %u, flags 0x%x\n",
	    letoh16(mcr->mcr_pkts), letoh16(mcr->mcr_flags));
	ma = (struct ubsec_mcr_add *)&mcr->mcr_cmdctxp;
	for (i = 0; i < letoh16(mcr->mcr_pkts); i++) {
		printf(" %d: ctx 0x%x len 0x%x rsvd 0x%x\n", i,
		    letoh32(ma->mcr_cmdctxp), letoh16(ma->mcr_pktlen),
		    letoh16(ma->mcr_reserved));
		printf(" %d: ipkt ", i);
		ubsec_dump_pb(&ma->mcr_ipktbuf);
		printf(" %d: opkt ", i);
		ubsec_dump_pb(&ma->mcr_opktbuf);
		ma++;
	}
	printf("END MCR\n");
}

/*
 * Return the number of significant bits of a big number.
 */
int
ubsec_ksigbits(struct crparam *cr)
{
	u_int plen = (cr->crp_nbits + 7) / 8;
	int i, sig = plen * 8;
	u_int8_t c, *p = cr->crp_p;

	for (i = plen - 1; i >= 0; i--) {
		c = p[i];
		if (c != 0) {
			while ((c & 0x80) == 0) {
				sig--;
				c <<= 1;
			}
			break;
		}
		sig -= 8;
	}
	return (sig);
}

void
ubsec_kshift_r(u_int shiftbits, u_int8_t *src, u_int srcbits,
    u_int8_t *dst, u_int dstbits)
{
	u_int slen, dlen;
	int i, si, di, n;

	slen = (srcbits + 7) / 8;
	dlen = (dstbits + 7) / 8;

	for (i = 0; i < slen; i++)
		dst[i] = src[i];
	for (i = 0; i < dlen - slen; i++)
		dst[slen + i] = 0;

	n = shiftbits / 8;
	if (n != 0) {
		si = dlen - n - 1;
		di = dlen - 1;
		while (si >= 0)
			dst[di--] = dst[si--];
		while (di >= 0)
			dst[di--] = 0;
	}

	n = shiftbits % 8;
	if (n != 0) {
		for (i = dlen - 1; i > 0; i--)
			dst[i] = (dst[i] << n) |
			    (dst[i - 1] >> (8 - n));
		dst[0] = dst[0] << n;
	}
}

void
ubsec_kshift_l(u_int shiftbits, u_int8_t *src, u_int srcbits,
    u_int8_t *dst, u_int dstbits)
{
	int slen, dlen, i, n;

	slen = (srcbits + 7) / 8;
	dlen = (dstbits + 7) / 8;

	n = shiftbits / 8;
	for (i = 0; i < slen; i++)
		dst[i] = src[i + n];
	for (i = 0; i < dlen - slen; i++)
		dst[slen + i] = 0;

	n = shiftbits % 8;
	if (n != 0) {
		for (i = 0; i < (dlen - 1); i++)
			dst[i] = (dst[i] >> n) | (dst[i + 1] << (8 - n));
		dst[dlen - 1] = dst[dlen - 1] >> n;
	}
@


1.158
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.157 2014/05/04 20:09:15 sf Exp $	*/
d696 1
a696 1
			ses = (struct ubsec_session *)malloc((sesn + 1) *
@


1.157
log
@format string fixes for bus_addr_t and bus_size_t

bus_addr_t and bus_size_t are u_long everywhere

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.156 2013/12/22 11:05:58 sf Exp $	*/
d274 1
a274 1
			free(q, M_DEVBUF);
d704 1
a704 1
			free(sc->sc_sessions, M_DEVBUF);
d1883 1
a1883 1
		free(me, M_DEVBUF);
d1893 1
a1893 1
		free(rp, M_DEVBUF);
d2134 1
a2134 1
		free(me, M_DEVBUF);
d2333 1
a2333 1
		free(me, M_DEVBUF);
d2519 1
a2519 1
		free(rp, M_DEVBUF);
@


1.156
log
@format string fix: %08x instead of %p for uint32_t
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.155 2012/03/14 17:08:17 mikeb Exp $	*/
d2473 1
a2473 1
		panic("%s: rsapriv: invalid msgin %08x(0x%x)",
d2478 1
a2478 1
		panic("%s: rsapriv: invalid msgout %08x(0x%x)",
@


1.155
log
@ubsec gets stuck after receiving a packet of a particular length
and stops processing any further packets. this is believed to be
a result of the inconsistency of the destination mbuf chain as a
a copy acquired via m_copym2 works fine. so far only one problem
was discovered with the optimized inline version of m_copym2: a
cluster has to be allocated if source chain had one.

unfortunately, this solves the problem only for some packet sizes
so apply a larger hammer and call m_copym2 for now.

the problem was reported and diffs were patiently tested by Joosep
<joosepm-at-gmail-dot-com>, thanks!

deraadt agrees to this temporary measure.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.154 2012/01/13 09:53:24 mikeb Exp $	*/
d2473 1
a2473 1
		panic("%s: rsapriv: invalid msgin %p(0x%x)",
d2478 1
a2478 1
		panic("%s: rsapriv: invalid msgout %p(0x%x)",
@


1.154
log
@handle m_copyback errors, this code is too sensitive for such
failures to be neglected;  ok markus
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.153 2011/05/06 17:55:00 mikeb Exp $	*/
a89 1
void	ubsec_mcopy(struct mbuf *, struct mbuf *, int, int);
d1142 6
a1147 55
				int totlen, len;
				struct mbuf *m, *top, **mp;

				totlen = q->q_src_map->dm_mapsize;
				if (q->q_src_m->m_flags & M_PKTHDR) {
					len = MHLEN;
					MGETHDR(m, M_DONTWAIT, MT_DATA);
				} else {
					len = MLEN;
					MGET(m, M_DONTWAIT, MT_DATA);
				}
				if (m == NULL) {
					err = ENOMEM;
					goto errout;
				}
				if (len == MHLEN) {
					err = m_dup_pkthdr(m, q->q_src_m,
					    M_DONTWAIT);
					if (err) {
						m_freem(m);
						goto errout;
					}
				}
				if (totlen >= MINCLSIZE) {
					MCLGET(m, M_DONTWAIT);
					if (m->m_flags & M_EXT)
						len = MCLBYTES;
				}
				m->m_len = len;
				top = NULL;
				mp = &top;

				while (totlen > 0) {
					if (top) {
						MGET(m, M_DONTWAIT, MT_DATA);
						if (m == NULL) {
							m_freem(top);
							err = ENOMEM;
							goto errout;
						}
						len = MLEN;
					}
					if (top && totlen >= MINCLSIZE) {
						MCLGET(m, M_DONTWAIT);
						if (m->m_flags & M_EXT)
							len = MCLBYTES;
					}
					m->m_len = len = min(totlen, len);
					totlen -= len;
					*mp = m;
					mp = &m->m_next;
				}
				q->q_dst_m = top;
				ubsec_mcopy(q->q_src_m, q->q_dst_m,
				    cpskip, cpoffset);
a1407 37
}

void
ubsec_mcopy(struct mbuf *srcm, struct mbuf *dstm, int hoffset, int toffset)
{
	int i, j, dlen, slen;
	caddr_t dptr, sptr;

	j = 0;
	sptr = srcm->m_data;
	slen = srcm->m_len;
	dptr = dstm->m_data;
	dlen = dstm->m_len;

	while (1) {
		for (i = 0; i < min(slen, dlen); i++) {
			if (j < hoffset || j >= toffset)
				*dptr++ = *sptr++;
			slen--;
			dlen--;
			j++;
		}
		if (slen == 0) {
			srcm = srcm->m_next;
			if (srcm == NULL)
				return;
			sptr = srcm->m_data;
			slen = srcm->m_len;
		}
		if (dlen == 0) {
			dstm = dstm->m_next;
			if (dstm == NULL)
				return;
			dptr = dstm->m_data;
			dlen = dstm->m_len;
		}
	}
@


1.153
log
@acknowledge only those interrupts that we can process;
with suggestions and ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.152 2011/04/05 11:48:28 blambert Exp $	*/
d952 1
a952 1
					m_copyback(q->q_src_m,
d959 2
d1448 1
a1448 1
			m_copyback((struct mbuf *)crp->crp_buf,
@


1.152
log
@Passing M_WAITOK to mbuf functions is supposed to be a contract between
the caller and the function that the function will not fail to allocate
memory and return a NULL pointer. However, m_dup_pkthdr() violates
this contract, making it possible for functions that pass M_WAITOK to
be surprised in ways that hurt.

Fix this by passing the wait flag all the way down the functions that
actually do the allocation for m_dup_pkthdr() so that we won't be
surprised.

man page update forthcoming

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.151 2011/04/03 15:36:03 jasper Exp $	*/
d382 2
a383 2
	stat &= sc->sc_statmask;
	if (stat == 0)
d386 1
@


1.151
log
@use nitems(); no binary change for drivers that are compiled on amd64.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.150 2011/01/12 20:55:22 deraadt Exp $	*/
d1156 2
a1157 1
					err = m_dup_pkthdr(m, q->q_src_m);
@


1.150
log
@A bunch more explicit_bzero calls for key material
ubsec tested by mikeb, hifn tested by kettenis
ok mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.149 2011/01/11 15:42:05 deraadt Exp $	*/
d149 1
a149 1
	    sizeof(ubsec_devices)/sizeof(ubsec_devices[0])));
@


1.149
log
@for key material that is being being discarded, convert bzero() to
explicit_bzero() where required
ok markus mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.148 2010/12/15 23:34:23 mikeb Exp $	*/
d702 1
a702 1
			bzero(sc->sc_sessions, sesn *
d801 1
a801 1
	bzero(&sc->sc_sessions[session], sizeof(sc->sc_sessions[session]));
d1373 1
d1401 1
d1411 3
d1432 2
d1639 3
a1641 3
		bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
		bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
		bzero(rp->rpr_q.q_ctx.dma_vaddr, rp->rpr_q.q_ctx.dma_size);
d2199 2
a2200 1
			bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
d2204 1
a2204 1
			bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
d2208 1
a2208 1
			bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
d2212 1
a2212 1
			bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
d2398 2
a2399 1
			bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
d2403 1
a2403 1
			bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
d2407 1
a2407 1
			bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
d2411 1
a2411 1
			bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
d2593 2
a2594 1
			bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
d2598 2
a2599 1
			bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
@


1.148
log
@Bring CBC oracle attack countermeasure from r1.32 of cryptosoft.c to
the hardware crypto accelerator land.  This fixes aes-ni, via xcrypt,
glxsb(4), hifn(4), safe(4) and ubsec(4) drivers.

Original commit message by angelos:

Don't keep the last blocksize-bytes of ciphertext for use as the next
plaintext's IV, in CBC mode. Use arc4random() to acquire fresh IVs per
message.

with and ok deraadt, ok markus, djm
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.147 2010/07/02 02:40:16 blambert Exp $	*/
d1607 4
a1610 4
		bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
		bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
		bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
		bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
@


1.147
log
@m_copyback can fail to allocate memory, but is a void fucntion so gymnastics
are required to detect that.

Change the function to take a wait argument (used in nfs server, but
M_NOWAIT everywhere else for now) and to return an error

ok claudio@@ henning@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.146 2010/04/08 00:23:53 tedu Exp $	*/
a713 3
		/* get an IV, network byte order */
		arc4random_buf(ses->ses_iv, sizeof(ses->ses_iv));

a943 2
			q->q_flags |= UBSEC_QFLAGS_COPYOUTIV;

d946 2
a947 4
			else {
				for (i = 0; i < (ivlen / 4); i++)
					key.ses_iv[i] = ses->ses_iv[i];
			}
a1429 20
	}

	/* copy out IV for future use */
	if (q->q_flags & UBSEC_QFLAGS_COPYOUTIV) {
		for (crd = crp->crp_desc; crd; crd = crd->crd_next) {
			if (crd->crd_alg != CRYPTO_DES_CBC &&
			    crd->crd_alg != CRYPTO_3DES_CBC &&
			    crd->crd_alg != CRYPTO_AES_CBC)
				continue;
			if (crp->crp_flags & CRYPTO_F_IMBUF)
				m_copydata((struct mbuf *)crp->crp_buf,
				    crd->crd_skip + crd->crd_len - 8, 8,
				    (caddr_t)sc->sc_sessions[q->q_sesn].ses_iv);
			else if (crp->crp_flags & CRYPTO_F_IOV) {
				cuio_copydata((struct uio *)crp->crp_buf,
				    crd->crd_skip + crd->crd_len - 8, 8,
				    (caddr_t)sc->sc_sessions[q->q_sesn].ses_iv);
			}
			break;
		}
@


1.147.2.1
log
@Backport from -current, original commit by mikeb@@:
------
Bring CBC oracle attack countermeasure from r1.32 of cryptosoft.c to
the hardware crypto accelerator land.  This fixes aes-ni, via xcrypt,
glxsb(4), hifn(4), safe(4) and ubsec(4) drivers.
------

ok deraadt@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.147 2010/07/02 02:40:16 blambert Exp $	*/
d714 3
d947 2
d951 4
a954 2
			else
				arc4random_buf(key.ses_iv, ivlen);
d1437 20
@


1.146
log
@these files don't need to include proc.h anymore.  ok oga for agp
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.145 2010/01/10 12:43:07 markus Exp $	*/
d960 1
a960 1
					    ivlen, key.ses_iv);
d1466 1
a1466 1
			    dmap->d_dma->d_macbuf);
@


1.145
log
@Fix two bugs in IPsec/HMAC-SHA2:
(1) use correct (message) block size of 128 byte (instead of 64
    bytes) for HMAC-SHA512/384 (RFC4634).
(2) RFC4868 specifies that HMAC-SHA-{256,384,512} is truncated to
    nnn/2 bits, while we still use 96 bits. 96 bits have been
    specified in draft-ietf-ipsec-ciph-sha-256-00 while
    draft-ietf-ipsec-ciph-sha-256-01 changed it to 128 bits.

WARNING: this change makes IPsec with SHA-256 (the default)
incompatible with older OpenBSD versions and other IPsec-implementations
that share this bug.

ok+tests naddy, fries; requested by reyk/deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.144 2009/09/13 14:42:52 krw Exp $	*/
d43 1
a43 1
#include <sys/proc.h>
@


1.145.2.1
log
@Backport from -current, original commit by mikeb@@:
------
Bring CBC oracle attack countermeasure from r1.32 of cryptosoft.c to
the hardware crypto accelerator land.  This fixes aes-ni, via xcrypt,
glxsb(4), hifn(4), safe(4) and ubsec(4) drivers.
------
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.145 2010/01/10 12:43:07 markus Exp $	*/
d714 3
d947 2
d951 4
a954 2
			else
				arc4random_buf(key.ses_iv, ivlen);
d1437 20
@


1.144
log
@M_DUP_PKTHDR() define -> m_dup_pkthdr() function to properly deal
with m_tag_copy_chain() failures.

Use m_defrag() to eliminate hand rolled defragging of mbufs and
some uses of M_DUP_PKTHDR().

Original diff from thib@@, claudio@@'s feedback integrated by me.

Tests kevlo@@ claudio@@, "reads ok" blambert@@

ok thib@@ claudio@@, "m_defrag() bits ok" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.143 2009/03/27 13:31:30 reyk Exp $	*/
d747 1
a747 1
			    HMAC_BLOCK_LEN - (macini->cri_klen / 8));
d755 1
a755 1
			    HMAC_BLOCK_LEN - (macini->cri_klen / 8));
d768 1
a768 1
			    HMAC_BLOCK_LEN - (macini->cri_klen / 8));
d776 1
a776 1
			    HMAC_BLOCK_LEN - (macini->cri_klen / 8));
@


1.143
log
@Add support for the BCM5825 and the next-generation BCM5860, 5861,
5862 Broadcom CryptoNetX IPSec/SSL Security Processors.  The 5825 is a
faster version of the already supported 5823, and the even faster 586x
series is a bit different and needed some more changes.  The RNG
engine on the 586x is not supported yet but I hope to fix it soon...

ubsec0 at pci4 dev 0 function 0 "Broadcom 5862" rev 0x01: 3DES MD5 SHA1 AES PK, apic 10 int 10 (irq 11)

tested by phessler@@ and me
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.140 2007/10/01 15:34:48 krw Exp $	*/
d1162 7
a1168 2
				if (len == MHLEN)
					M_DUP_PKTHDR(m, q->q_src_m);
@


1.142
log
@add support for AES-CBC with the BCM5823 (or newer, but we don't support newer
variants yet).

ok deraadt@@ dlg@@
@
text
@d93 1
d137 4
d159 1
d172 2
d176 1
d189 1
a189 2
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5823))
a192 4
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5823)
		sc->sc_flags |= UBS_FLAGS_AES;

d204 26
a229 1
	if (pci_mapreg_map(pa, BS_BAR, PCI_MAPREG_TYPE_MEM, 0,
d317 4
a320 1
		sc->sc_statmask |= BS_STAT_MCR2_DONE;
d377 1
d440 7
a446 1
			if ((mcr->mcr_flags & htole16(UBS_MCR_DONE)) == 0) {
d463 33
d531 2
a532 2
	if (npkts > UBS_MAX_AGGR)
		npkts = UBS_MAX_AGGR;
d1536 27
d1677 1
a1677 1
	int s;
d1684 7
a1690 2
	sc->sc_nqueue2++;
	if (sc->sc_nqueue2 >= UBS_MAX_NQUEUE)
d1714 9
a1722 3
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rng->rng_q, q_next);
	rng->rng_used = 1;
	ubsec_feed2(sc);
d1731 1
a1731 1
	sc->sc_nqueue2--;
d1790 1
a1790 1
    volatile u_int32_t ctrl;
d1792 15
a1806 8
    ctrl = READ_REG(sc, BS_CTRL);
    ctrl |= BS_CTRL_RESET;
    WRITE_REG(sc, BS_CTRL, ctrl);

    /*
     * Wait aprox. 30 PCI clocks = 900 ns = 0.9 us
     */
    DELAY(10);
d1828 10
@


1.141
log
@rename arc4random_bytes => arc4random_buf to match libc's nicer name;
ok deraadt@@
@
text
@d185 4
d262 2
d283 2
d585 2
a586 1
		    c->cri_alg == CRYPTO_3DES_CBC) {
d596 11
d646 7
a652 4
		if (encini->cri_alg == CRYPTO_DES_CBC) {
			bcopy(encini->cri_key, &ses->ses_deskey[0], 8);
			bcopy(encini->cri_key, &ses->ses_deskey[2], 8);
			bcopy(encini->cri_key, &ses->ses_deskey[4], 8);
d654 1
a654 1
			bcopy(encini->cri_key, ses->ses_deskey, 24);
d656 8
a663 6
		SWAP32(ses->ses_deskey[0]);
		SWAP32(ses->ses_deskey[1]);
		SWAP32(ses->ses_deskey[2]);
		SWAP32(ses->ses_deskey[3]);
		SWAP32(ses->ses_deskey[4]);
		SWAP32(ses->ses_deskey[5]);
d746 1
a746 2
	struct ubsec_session *ses;
	struct ubsec_pktctx ctx;
d748 2
d778 1
a778 1
	bzero(&ctx, sizeof(ctx));
d814 2
a815 1
		    crd1->crd_alg == CRYPTO_3DES_CBC) {
d826 2
a827 1
			crd2->crd_alg == CRYPTO_3DES_CBC) &&
d832 2
a833 1
		    crd1->crd_alg == CRYPTO_3DES_CBC) &&
d835 1
a835 1
			crd2->crd_alg == CRYPTO_SHA1_HMAC) &&
d849 23
a872 1
		ctx.pc_flags |= htole16(UBS_PKTCTX_ENC_3DES);
d878 1
a878 1
				bcopy(enccrd->crd_iv, ctx.pc_iv, 8);
d880 2
a881 2
				ctx.pc_iv[0] = ses->ses_iv[0];
				ctx.pc_iv[1] = ses->ses_iv[1];
d888 1
a888 1
					    8, ctx.pc_iv);
d892 1
a892 1
					    8, ctx.pc_iv);
d895 1
a895 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_INBOUND);
d898 1
a898 1
				bcopy(enccrd->crd_iv, ctx.pc_iv, 8);
d901 1
a901 1
				    8, (caddr_t)ctx.pc_iv);
d904 2
a905 2
				    enccrd->crd_inject, 8,
				    (caddr_t)ctx.pc_iv);
d908 4
a911 8
		ctx.pc_deskey[0] = ses->ses_deskey[0];
		ctx.pc_deskey[1] = ses->ses_deskey[1];
		ctx.pc_deskey[2] = ses->ses_deskey[2];
		ctx.pc_deskey[3] = ses->ses_deskey[3];
		ctx.pc_deskey[4] = ses->ses_deskey[4];
		ctx.pc_deskey[5] = ses->ses_deskey[5];
		SWAP32(ctx.pc_iv[0]);
		SWAP32(ctx.pc_iv[1]);
d918 1
a918 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_AUTH_MD5);
d920 1
a920 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_AUTH_SHA1);
d923 2
a924 2
			ctx.pc_hminner[i] = ses->ses_hminner[i];
			ctx.pc_hmouter[i] = ses->ses_hmouter[i];
d926 2
a927 2
			HTOLE32(ctx.pc_hminner[i]);
			HTOLE32(ctx.pc_hmouter[i]);
a964 1
	ctx.pc_offset = htole16(coffset >> 2);
d1201 6
a1206 2
	if (sc->sc_flags & UBS_FLAGS_LONGCTX) {
		struct ubsec_pktctx_long *ctxl;
d1208 1
a1208 1
		ctxl = (struct ubsec_pktctx_long *)(dmap->d_alloc.dma_vaddr +
d1210 76
a1285 6
		
		/* transform small context into long context */
		ctxl->pc_len = htole16(sizeof(struct ubsec_pktctx_long));
		ctxl->pc_type = htole16(UBS_PKTCTX_TYPE_IPSEC);
		ctxl->pc_flags = ctx.pc_flags;
		ctxl->pc_offset = ctx.pc_offset;
d1287 1
a1287 1
			ctxl->pc_deskey[i] = ctx.pc_deskey[i];
d1289 1
a1289 1
			ctxl->pc_hminner[i] = ctx.pc_hminner[i];
d1291 4
a1294 7
			ctxl->pc_hmouter[i] = ctx.pc_hmouter[i];   
		ctxl->pc_iv[0] = ctx.pc_iv[0];
		ctxl->pc_iv[1] = ctx.pc_iv[1];
	} else
		bcopy(&ctx, dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx),
		    sizeof(struct ubsec_pktctx));
d1366 2
a1367 1
			    crd->crd_alg != CRYPTO_3DES_CBC)
@


1.140
log
@More easy bzero() -> M_ZERO. Use 'p = malloc(sizeof(*p) ...' where
obvious.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.139 2007/09/18 22:02:18 djm Exp $	*/
d623 1
a623 1
		arc4random_bytes(ses->ses_iv, sizeof(ses->ses_iv));
@


1.139
log
@arc4random_bytes() is the preferred interface for generating nonces;
"looks ok" markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.138 2006/12/29 13:04:37 pedro Exp $	*/
d1808 1
a1808 1
	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
a1812 1
	bzero(me, sizeof *me);
d2006 1
a2006 1
	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
a2010 1
	bzero(me, sizeof *me);
d2235 1
a2235 1
	rp = (struct ubsec_q2_rsapriv *)malloc(sizeof *rp, M_DEVBUF, M_NOWAIT);
a2237 1
	bzero(rp, sizeof *rp);
@


1.138
log
@Avoid void * arithmetic, okay deraadt@@, suggestions from millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.137 2006/06/29 21:34:51 deraadt Exp $	*/
d623 1
a623 1
		get_random_bytes(ses->ses_iv, sizeof(ses->ses_iv));
@


1.137
log
@do not check for master/io/mem enables; ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.136 2005/08/09 04:10:13 mickey Exp $	*/
d496 1
a496 1
		v = ((void *)&q2->q_dma->d_dma->d_mcr) + sizeof(struct ubsec_mcr) -
@


1.136
log
@do not set PCI_COMMAND_MASTER_ENABLE explicitly as it's already set in pcisubmatch(); kettenis@@ testing; brad@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.135 2004/05/07 14:42:26 millert Exp $	*/
d157 1
a157 1
	u_int32_t cmd, i;
a193 12
	}

	cmd = pci_conf_read(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG);

	if (!(cmd & PCI_COMMAND_MEM_ENABLE)) {
		printf(": failed to enable memory mapping\n");
		return;
	}

	if (!(cmd & PCI_COMMAND_MASTER_ENABLE)) {
		printf(": failed to enable bus mastering\n");
		return;
@


1.135
log
@Replace RSA-derived md5 code with code derived from Colin Plumb's PD version.
This moves md5.c out of libkern and into sys/crypto where it belongs (as
requested by markus@@).  Note that md5.c is still mandatory (dev/rnd.c uses it).
Verified with IPsec + hmac-md5 and tcp md5sig. OK henning@@ and hshoexer@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.134 2004/05/04 16:59:31 grange Exp $	*/
a195 3
	cmd = pci_conf_read(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG);
	cmd |= PCI_COMMAND_MEM_ENABLE | PCI_COMMAND_MASTER_ENABLE;
	pci_conf_write(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG, cmd);
@


1.134
log
@Remove useless ``elm'' argument from the SIMPLEQ_REMOVE_HEAD macro.
This matches our SLIST behaviour and NetBSD's SIMPLEQ as well.

ok millert krw deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.133 2004/02/03 17:17:33 deraadt Exp $	*/
d54 1
a54 1
#include <sys/md5k.h>
@


1.133
log
@advertise features of our crypto chips better; ok tdeval
jason is being a slacker
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.132 2004/01/09 21:32:24 brad Exp $	*/
d373 1
a373 1
			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q, q_next);
d421 1
a421 1
			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip2, q2, q_next);
d490 1
a490 1
	SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
d508 1
a508 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q2, q_next);
d553 1
a553 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
d762 1
a762 1
	SIMPLEQ_REMOVE_HEAD(&sc->sc_freequeue, q, q_next);
d1354 1
a1354 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue2, q, q_next);
d1643 1
a1643 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q, q_next);
d1786 1
a1786 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_q2free, q, q_next);
@


1.132
log
@remove uvm_extern.h

tested on alpha, i386, powerpc, sparc64, m68k.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.131 2003/09/03 15:55:41 jason Exp $	*/
d291 1
a291 1
	printf(": %s", intrstr);
d320 1
a320 1
		printf(", rng");
d336 1
d339 1
a339 1
	printf("\n");
@


1.131
log
@support for bcm5823; based on patch from Jim Lambert, jlamber at futurex dot com; ok deraadt.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.130 2003/08/14 15:20:29 jason Exp $	*/
a49 2

#include <uvm/uvm_extern.h>
@


1.130
log
@kill unneeded caddr_t casts for *_copyback
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.129 2003/08/08 02:55:17 jason Exp $	*/
d137 1
d182 2
a183 1
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822))
@


1.129
log
@x is probably just as random as letoh32(x), don't bother byte swapping the numbers from the RNG.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.128 2003/08/01 17:55:54 deraadt Exp $	*/
d851 1
a851 1
					    8, (caddr_t)ctx.pc_iv);
d855 1
a855 1
					    8, (caddr_t)ctx.pc_iv);
d1285 1
a1285 1
			    (caddr_t)dmap->d_dma->d_macbuf);
@


1.128
log
@ansi
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.127 2003/06/04 14:04:58 jason Exp $	*/
d1384 1
a1384 1
			add_true_randomness(letoh32(*p));
@


1.127
log
@nuke clause 3 & 4 (ok with patrik)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.126 2003/04/19 22:08:04 jason Exp $	*/
d142 1
a142 4
ubsec_probe(parent, match, aux)
	struct device *parent;
	void *match;
	void *aux;
d149 1
a149 3
ubsec_attach(parent, self, aux)
	struct device *parent, *self;
	void *aux;
d345 1
a345 2
ubsec_intr(arg)
	void *arg;
d454 1
a454 2
ubsec_feed(sc)
	struct ubsec_softc *sc;
d526 2
a527 1
		if ((stat = READ_REG(sc, BS_STAT)) & (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
d564 1
a564 3
ubsec_newsession(sidp, cri)
	u_int32_t *sidp;
	struct cryptoini *cri;
d710 1
a710 2
ubsec_freesession(tid)
	u_int64_t tid;
d726 1
a726 2
ubsec_process(crp)
	struct cryptop *crp;
d1011 2
a1012 1
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_next = htole32(dmap->d_alloc.dma_paddr +
d1231 1
a1231 3
ubsec_callback(sc, q)
	struct ubsec_softc *sc;
	struct ubsec_q *q;
d1296 1
a1296 3
ubsec_mcopy(srcm, dstm, hoffset, toffset)
	struct mbuf *srcm, *dstm;
	int hoffset, toffset;
d1336 1
a1336 2
ubsec_feed2(sc)
	struct ubsec_softc *sc;
d1363 1
a1363 3
ubsec_callback2(sc, q)
	struct ubsec_softc *sc;
	struct ubsec_q2 *q;
d1468 1
a1468 2
ubsec_rng(vsc)
	void *vsc;
d1524 2
a1525 5
ubsec_dma_malloc(sc, size, dma, mapflags)
	struct ubsec_softc *sc;
	bus_size_t size;
	struct ubsec_dma_alloc *dma;
	int mapflags;
d1561 1
a1561 3
ubsec_dma_free(sc, dma)
	struct ubsec_softc *sc;
	struct ubsec_dma_alloc *dma;
d1574 1
a1574 2
ubsec_reset_board(sc)
	struct ubsec_softc *sc;
d1592 1
a1592 2
ubsec_init_board(sc)
	struct ubsec_softc *sc;
d1615 1
a1615 2
ubsec_init_pciregs(pa)
	struct pci_attach_args *pa;
d1633 1
a1633 1
 * It is assumed that the caller in splnet()
d1636 1
a1636 2
ubsec_cleanchip(sc)
	struct ubsec_softc *sc;
d1699 1
a1699 2
ubsec_totalreset(sc)
	struct ubsec_softc *sc;
d1707 1
a1707 2
ubsec_dmamap_aligned(map)
	bus_dmamap_t map;
d1722 1
a1722 2
ubsec_kfind(krp)
	struct cryptkop *krp;
d1738 1
a1738 3
ubsec_kfree(sc, q)
	struct ubsec_softc *sc;
	struct ubsec_q2 *q;
d1771 1
a1771 2
ubsec_kprocess(krp)
	struct cryptkop *krp;
d1813 1
a1813 3
ubsec_kprocess_modexp_sw(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
d2012 1
a2012 3
ubsec_kprocess_modexp_hw(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
d2208 1
a2208 3
ubsec_kprocess_rsapriv(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
d2453 1
a2453 2
ubsec_ksigbits(cr)
	struct crparam *cr;
d2474 2
a2475 3
ubsec_kshift_r(shiftbits, src, srcbits, dst, dstbits)
	u_int shiftbits, srcbits, dstbits;
	u_int8_t *src, *dst;
d2508 2
a2509 3
ubsec_kshift_l(shiftbits, src, srcbits, dst, dstbits)
	u_int shiftbits, srcbits, dstbits;
	u_int8_t *src, *dst;
@


1.126
log
@correct opacket/obytes statistics; from sam@@freebsd
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.125 2003/04/19 21:59:08 jason Exp $	*/
a7 2
 * All rights reserved.
 *
a15 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Jason L. Wright
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.125
log
@Make the ubsec_feed* routines void, noone looks at the return value (always zero anyway); from freebsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.124 2003/04/02 22:33:13 jason Exp $	*/
d394 1
a394 1
				if(q->q_stacked_mcr[i]) {
d396 1
a396 2
					ubsecstats.hst_opackets++;
				} else {
a397 1
				}
a399 1
			ubsecstats.hst_opackets++;
d1255 3
a1275 1
	ubsecstats.hst_obytes += ((struct mbuf *)crp->crp_buf)->m_len;
@


1.124
log
@add support for the other Sun Crypto 1000; from sam at errno dot com
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.123 2003/02/14 01:28:20 jason Exp $	*/
d98 1
a98 1
int	ubsec_feed(struct ubsec_softc *);
d101 1
a101 1
int	ubsec_feed2(struct ubsec_softc *);
d469 1
a469 1
int
d492 1
a492 1
		return (0);
d539 1
a539 1
	return (0);
a571 1
	return (0);
d1356 1
a1356 1
int
a1378 1
	return (0);
@


1.123
log
@Let UBSEC_DEBUG compile; based on diff from sam at errno dot com
(also nuke the last vtophys so it'll even work on sparc64)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.122 2002/12/06 22:03:26 jason Exp $	*/
d145 1
d200 2
a201 1
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_SCA1K)) {
@


1.122
log
@Don't use the RNG oscillator output directly, use the sha1'd version (the
the direct data does not pass 1/2 of the FIPS140-2 tests with any degree
of regularity).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.121 2002/12/05 22:40:41 jason Exp $	*/
d563 2
a564 1
		printf("feed: q->chip %p %08x\n", q, (u_int32_t)vtophys(&q->q_dma->d_dma->d_mcr));
d1034 3
a1036 3
		    dmap->d_mcr->mcr_opktbuf.pb_addr,
		    dmap->d_mcr->mcr_opktbuf.pb_len,
		    dmap->d_mcr->mcr_opktbuf.pb_next);
@


1.121
log
@Treat RNGSHA1 operations the same as RNGBYPASS for callback purposes (they
produce the same size/format data).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.120 2002/11/21 19:34:25 jason Exp $	*/
d1524 2
a1525 2
	ctx->rbp_op = htole16(UBS_CTXOP_RNGBYPASS);
	rng->rng_q.q_type = UBS_CTXOP_RNGBYPASS;
@


1.120
log
@From Angelos:
- simplistic load balancing across multiple cards
- simplified registration process
- a few style nits.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.119 2002/11/19 18:40:17 jason Exp $	*/
d1397 1
@


1.119
log
@Add a simplistic table driven lookup routine and use it where appropriate.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.118 2002/10/12 01:09:44 krw Exp $	*/
d170 2
d278 7
a284 5
	crypto_register(sc->sc_cid, CRYPTO_3DES_CBC, 0, 0,
	    ubsec_newsession, ubsec_freesession, ubsec_process);
	crypto_register(sc->sc_cid, CRYPTO_DES_CBC, 0, 0, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_MD5_HMAC, 0, 0, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_SHA1_HMAC, 0, 0, NULL, NULL, NULL);
d339 2
a340 1
		crypto_kregister(sc->sc_cid, CRK_MOD_EXP, 0, ubsec_kprocess);
d342 1
a342 1
		crypto_kregister(sc->sc_cid, CRK_MOD_EXP_CRT, 0, ubsec_kprocess);
d344 2
@


1.118
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.117 2002/10/10 17:46:35 jason Exp $	*/
d135 12
d153 2
a154 18
	struct pci_attach_args *pa = (struct pci_attach_args *)aux;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5501 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601))
		return (1);
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5801 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5802 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822))
		return (1);
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_SUN &&
	    PCI_PRODUCT(pa->pa_id) ==  PCI_PRODUCT_SUN_SCA1K)
		return (1);
	return (0);
@


1.117
log
@Match the Sun Crypto Adapter 1000, it appears to be a broadcom 5821.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.116 2002/10/05 00:21:02 jason Exp $	*/
d1973 1
a1973 1
		panic("%s: modexp invalid addr 0x%x\n",
d1976 1
a1976 1
		panic("%s: modexp invalid len 0x%x\n",
d2175 1
a2175 1
		panic("%s: modexp invalid addr 0x%x\n",
d2178 1
a2178 1
		panic("%s: modexp invalid len 0x%x\n",
@


1.116
log
@match 5822
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.115 2002/09/24 18:33:26 jason Exp $	*/
d155 3
d198 4
a201 2
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821)) {
@


1.115
log
@Don't use constants for the output parameter, use the iparam count as a pointer to the first result location.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.114 2002/09/19 17:58:38 jason Exp $	*/
d152 2
a153 1
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821))
@


1.114
log
@remember: bits is bits and bytes is bytes... use -byte- count for bcopy not
bits.  Also, the conversion between bits and bytes involves a division by
8 not 2.  (The latter pointed out by Francis Cianfrocca <vze32r6m@@verizon.net>)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.113 2002/09/12 03:27:20 jason Exp $	*/
d1409 1
a1409 1
		clen = (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits + 7) / 8;
d1424 2
a1425 2
				bzero(krp->krp_param[UBS_MODEXP_PAR_C].crp_p,
				    (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits
d1428 1
a1428 1
				    krp->krp_param[UBS_MODEXP_PAR_C].crp_p,
d1433 2
a1434 2
				    krp->krp_param[UBS_MODEXP_PAR_C].crp_p,
				    krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits);
d1885 1
a1885 1
	if (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits < nbits) {
d2087 1
a2087 1
	if (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits < nbits) {
@


1.113
log
@- Split out the hardware and software normalization versions of modexp...
I screwed something up when the function was trying to do both and it's
much easier to read this way (and heck, even works).
- Enable hardware normalization for chips that support it
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.112 2002/09/11 22:40:31 jason Exp $	*/
d1429 1
a1429 1
				    me->me_modbits);
d2179 1
a2179 1
	    (nbits + 7) / 2);
@


1.112
log
@- On reset, disable hardware normalization for 582x and make sure the chip is in little endian mode.
- since sw normalization is now the only option, simplify normalization handling
- remove some leftover #if 0 code
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.111 2002/09/04 15:37:29 jason Exp $	*/
d110 2
a111 1
int	ubsec_kprocess_modexp(struct ubsec_softc *, struct cryptkop *);
d1422 14
a1435 6
		else
			ubsec_kshift_l(me->me_shiftbits,
			    me->me_C.dma_vaddr, me->me_normbits,
			    krp->krp_param[UBS_MODEXP_PAR_C].crp_p,
			    krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits);

d1615 15
a1629 5
	/* Turn on appropriate interrupts and disable hardware normalization */
	WRITE_REG(sc, BS_CTRL, READ_REG(sc, BS_CTRL) |
	    BS_CTRL_MCR1INT | BS_CTRL_DMAERR | BS_CTRL_LITTLE_ENDIAN |
	    ((sc->sc_flags & UBS_FLAGS_KEY) ? BS_CTRL_MCR2INT : 0) |
	    ((sc->sc_flags & UBS_FLAGS_HWNORM) ? BS_CTRL_SWNORM : 0));
d1803 1
d1820 5
a1824 1
		return (ubsec_kprocess_modexp(sc, krp));
d1826 2
a1827 1
		return (ubsec_kprocess_rsapriv(sc, krp));
d1833 1
a1833 1
		return (0);
d1835 1
d1839 1
a1839 1
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N]
d1842 1
a1842 1
ubsec_kprocess_modexp(sc, krp)
d1982 201
@


1.111
log
@Treat 5822 the same as 5820
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.110 2002/09/04 04:21:19 jason Exp $	*/
d1606 5
a1610 3
	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    ((sc->sc_flags & UBS_FLAGS_KEY) ? BS_CTRL_MCR2INT : 0));
a1622 9
#if 0
	misc = pci_conf_read(pc, pa->pa_tag, BS_RTY_TOUT);
	misc = (misc & ~(UBS_PCI_RTY_MASK << UBS_PCI_RTY_SHIFT))
	    | ((UBS_DEF_RTY & 0xff) << UBS_PCI_RTY_SHIFT);
	misc = (misc & ~(UBS_PCI_TOUT_MASK << UBS_PCI_TOUT_SHIFT))
	    | ((UBS_DEF_TOUT & 0xff) << UBS_PCI_TOUT_SHIFT);
	pci_conf_write(pc, pa->pa_tag, BS_RTY_TOUT, misc);
#endif

d1852 1
a1852 4
	if (sc->sc_flags & UBS_FLAGS_HWNORM)
		shiftbits = 0;
	else
		shiftbits = normbits - nbits;
d1955 2
a1956 2
	ctx->me_E_len = htole16(normbits - shiftbits);
	ctx->me_N_len = htole16(normbits - shiftbits);
@


1.110
log
@5801 has no pk or rng support
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.109 2002/09/03 20:10:00 jason Exp $	*/
d188 2
a189 1
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820))
@


1.109
log
@add support for 5801 and 5802 which appear to be 5805's as far as sw is concerned
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.108 2002/07/08 19:41:29 jason Exp $	*/
d183 1
a183 2
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5801 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5802 ||
d332 1
d334 1
@


1.108
log
@5821 has two additional bits that must be ack'd (note they don't have
corresponding enable bits... they are always on... dain bramage).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.107 2002/07/05 21:21:17 jason Exp $	*/
d147 3
a149 1
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805 ||
d183 3
a185 1
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)
@


1.107
log
@Never call crp_callback directly, use crypto_done() instead
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.106 2002/07/05 21:03:46 jason Exp $	*/
d185 8
a192 2
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821))
d195 1
@


1.106
log
@KNF (no space after casts)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.105 2002/07/03 00:24:29 jason Exp $	*/
d1220 1
a1220 1
	crp->crp_callback(crp);
d1671 1
a1671 1
			crp->crp_callback(crp);
d1688 1
a1688 1
	crp->crp_callback(crp);
@


1.105
log
@But when denormalizing we need the normalized length, too as well as the destination length.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.104 2002/07/03 00:19:33 jason Exp $	*/
d708 1
a708 1
	u_int32_t sid = ((u_int32_t) tid) & 0xffffffff;
@


1.104
log
@result only needs to be as big as the number of real bits in the modulus
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.103 2002/06/17 08:05:47 deraadt Exp $	*/
d1410 1
a1410 1
			    me->me_C.dma_vaddr, me->me_modbits,
d1853 1
@


1.103
log
@remove noisy jason debug printf blatther.  bad p
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.102 2002/05/16 16:34:13 jason Exp $	*/
d1851 1
a1851 1
	me->me_modbits = normbits;
@


1.102
log
@Normalize the exponent too.  This allows exponents with bit lengths different
from base/modulus to work on 5805.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.101 2002/05/16 02:54:02 jason Exp $	*/
a2036 1
		printf("bad pad.\n");
a2041 1
		printf("bad p\n");
a2046 1
		printf("bad q\n");
a2051 1
		printf("bad pinv\n");
a2106 1
		printf("msginbuf...\n");
a2120 1
		printf("msgoutbuf\n");
@


1.101
log
@Re-enable RNG on the 5601 (it needs to be disabled on some 5805 variants,
but I'm not sure which revisions yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.100 2002/05/15 15:15:41 jason Exp $	*/
d1886 1
a1886 1
	if (ubsec_dma_malloc(sc, 2048 / 8, &me->me_E, 0)) {
d1890 1
d1892 6
a1897 1
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
d1901 3
a1903 1
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
a1910 8

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	bcopy(krp->krp_param[UBS_MODEXP_PAR_E].crp_p,
	    me->me_E.dma_vaddr, (ebits + 7) / 8);
d1913 1
a1913 1
	epb->pb_len = htole32((ebits + 7) / 8);
d1951 1
a1951 1
	ctx->me_E_len = htole16(ebits);
@


1.100
log
@Rework MODEXP:
5805 (and 5601) require the modulus and base to be normalized to the right of
one of several different register lengths.  The result is also normalized to
the same length.  Provide functions for shifting the bits back and forth
as appropriate.  Note: for consistencies sake the exponent is NOT normalized.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.99 2002/05/13 22:28:56 jason Exp $	*/
d140 1
a140 1
	struct pci_attach_args *pa = (struct pci_attach_args *) aux;
d178 1
a178 1
		sc->sc_flags |= UBS_FLAGS_KEY;
@


1.99
log
@add and use three more flags:
RNG: chip has usable rng (5805/5820/5821)
HWNORM: chip will automagically normalize bignums (5820/5821)
BIGKEY: chip supports "large keys" (5820/5821)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.98 2002/05/08 23:05:27 jason Exp $	*/
d113 3
a115 2
int	ubsec_kcopyin(struct crparam *, caddr_t, u_int, u_int *);
int	ubsec_norm_sigbits(const u_int8_t *, u_int);
d391 2
a392 1
	if ((sc->sc_flags & UBS_FLAGS_KEY) && (stat & BS_STAT_MCR2_DONE)) {
d1408 5
a1412 8
		else {
			caddr_t dst;

			krp->krp_status = 0;
			dst = krp->krp_param[UBS_MODEXP_PAR_C].crp_p;
			bcopy(me->me_C.dma_vaddr, dst, rlen);
			bzero(dst + rlen, clen - rlen);
		}
d1814 1
a1814 1
	struct ubsec_q2_modexp *me = NULL;
d1819 10
a1828 2
	u_int len;
	u_int modbits;
d1830 11
a1840 11
	modbits = krp->krp_param[UBS_MODEXP_PAR_N].crp_nbits;
	if (modbits <= 512)
		modbits = 512;
	else if (modbits <= 768)
		modbits = 768;
	else if (modbits <= 1024)
		modbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_LONGCTX && modbits <= 1536)
		modbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_LONGCTX && modbits <= 2048)
		modbits = 2048;
d1846 8
d1855 1
a1855 2
	if (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits <
	    krp->krp_param[UBS_MODEXP_PAR_N].crp_nbits) {
a1859 8
	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
	if (me == NULL)
		return (ENOMEM);
	bzero(me, sizeof *me);
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;
	me->me_modbits = modbits;

d1873 6
a1878 1
	if (ubsec_dma_malloc(sc, 2048 / 8, &me->me_M, 0)) {
d1882 3
d1891 1
a1891 1
	if (ubsec_dma_malloc(sc, modbits / 8, &me->me_C, 0)) {
d1904 3
a1906 3
	len = (krp->krp_param[UBS_MODEXP_PAR_E].crp_nbits + 7) / 8;
	if (len > me->me_E.dma_size) {
		err = EOPNOTSUPP;
d1909 2
a1910 1
	bcopy(krp->krp_param[UBS_MODEXP_PAR_E].crp_p, me->me_E.dma_vaddr, len);
d1913 2
a1914 1
	epb->pb_len = htole32(len);
a1925 5
	if (ubsec_kcopyin(&krp->krp_param[UBS_MODEXP_PAR_M], me->me_M.dma_vaddr,
	    1024 / 8, &len)) {
		err = EOPNOTSUPP;
		goto errout;
	}
d1927 1
a1927 1
	mcr->mcr_ipktbuf.pb_len = htole32(len);
d1932 1
a1932 1
	mcr->mcr_opktbuf.pb_len = htole32(modbits / 8);
d1946 4
a1949 9
	if (ubsec_kcopyin(&krp->krp_param[UBS_MODEXP_PAR_N], ctx->me_N,
	    1024 / 8, &len)) {
		err = EOPNOTSUPP;
		goto errout;
	}
	len = ((krp->krp_param[UBS_MODEXP_PAR_N].crp_nbits + 31) / 32) * 32;
	if (len < 512)
		len = 512;
	ctx->me_len = htole16((len / 8) + (4 * sizeof(u_int16_t)));
d1951 2
a1952 3
	ctx->me_E_len =
	    htole16(((krp->krp_param[UBS_MODEXP_PAR_E].crp_nbits + 7) / 8) * 8);
	ctx->me_N_len = htole16(len);
d2020 2
a2021 4
	msglen = ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_P].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_P].crp_nbits);
	padlen = ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_nbits);
d2031 1
a2031 1
	else if (sc->sc_flags & UBS_FLAGS_LONGCTX && padlen <= 768)
d2033 1
a2033 1
	else if (sc->sc_flags & UBS_FLAGS_LONGCTX && padlen <= 1024)
d2041 1
a2041 2
	if (ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_nbits) > padlen) {
d2047 1
a2047 2
	if (ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_nbits) > padlen) {
d2053 1
a2053 2
	if (ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_nbits) > padlen) {
d2109 1
a2109 2
	if (ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_nbits) > msglen) {
d2125 1
a2125 2
	if (ubsec_norm_sigbits(krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_p,
	    krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_nbits) < msglen) {
a2201 28
/*
 * Copy a key into a ubsec dma buffer, round up to multiple of 32 bits.
 */
int
ubsec_kcopyin(crpar, buf, bufsiz, reslen)
	struct crparam *crpar;
	caddr_t buf;
	u_int bufsiz, *reslen;
{
	u_int nbytes, npad;

	nbytes = (crpar->crp_nbits + 7) / 8;
	if ((nbytes & 3) != 0)
		npad = 4 - (nbytes & 3);
	else
		npad = 0;

	if ((bufsiz & 3) != 0)
		panic("ubsec_kcopyin: bad len");
	if ((nbytes + npad) > bufsiz)
		return (-1);

	bcopy(crpar->crp_p, buf, nbytes);
	bzero(buf + nbytes, npad);
	*reslen = nbytes + npad;
	return (0);
}

d2263 2
a2264 3
ubsec_norm_sigbits(p, pbits)
	const u_int8_t *p;
	u_int pbits;
d2266 1
a2266 1
	u_int plen = (pbits + 7) / 8;
d2268 1
a2268 1
	u_int8_t c;
d2284 58
@


1.98
log
@- Go ahead and register to handle CRK_MOD_EXP_CRT ops
- completely (almost) revamp kprocess_rsapriv to match what the chip expects
- add and use a function to compute the significant bits of a given number
(this will be necessary for normalization)
[Still doesn't reproduce the broadcom or provos test data, but it's closer...]
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.97 2002/05/06 20:53:05 jason Exp $	*/
d174 3
a176 4
	if ((PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601) ||
	    (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805))
d180 4
d186 2
a187 1
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_LONGCTX;
d285 1
a285 1
	if (sc->sc_flags & UBS_FLAGS_KEY) {
@


1.97
log
@basic infrastructure for handling RSA with CRT parameters.  Just need to
figure out how p, q, dp, dq, and pinv fit into the context.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.96 2002/05/06 15:42:23 jason Exp $	*/
d114 1
a316 1
#if 0
a317 1
#endif
d2009 45
a2053 1
	u_int len;
d2077 27
d2105 8
a2112 2
	len = (krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_nbits + 31) / 8;
	if (ubsec_dma_malloc(sc, len, &rp->rpr_msgin, 0)) {
d2116 1
a2116 1
	bzero(rp->rpr_msgin.dma_vaddr, len);
d2122 8
a2129 2
	len = (krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_nbits + 31) / 8;
	if (ubsec_dma_malloc(sc, len, &rp->rpr_msgout, 0)) {
d2133 1
a2133 1
	bzero(rp->rpr_msgout.dma_vaddr, len);
d2142 1
a2142 1
	/* XXX mcr_pktlen, 0? */
d2147 14
a2160 1
	/* XXX ctx->rpr_len = XXX; */
d2162 2
a2163 2
	/* XXX ctx->rpr_q_len; */
	/* XXX ctx->rpr_p_len; */
d2169 2
a2172 2
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map,
	    0, rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
d2284 27
@


1.96
log
@- Only copy the significant bits of the result out (and make sure the buffer
is long enough to handle it) and bzero the rest.
- Increase key buffer sizes to 2048 bits.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.95 2002/05/02 19:03:35 jason Exp $	*/
d111 1
d316 3
d719 1
a719 1
	int card, err, i, j, s, nicealign;
d1360 1
d1388 1
d1390 1
a1390 2
		clen = (me->me_krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits + 7)
		    / 8;
d1402 1
a1402 1
			me->me_krp->krp_status = E2BIG;
d1406 2
a1407 2
			me->me_krp->krp_status = 0;
			dst = me->me_krp->krp_param[UBS_MODEXP_PAR_C].crp_p;
d1412 1
a1412 1
		crypto_kdone(me->me_krp);
d1424 24
d1754 10
d1792 3
a1794 1
		return ubsec_kprocess_modexp(sc, krp);
a1819 7
#ifdef UBSEC_DEBUG
	for (s = 0; s < (krp->krp_iparams + krp->krp_oparams); s++) {
		printf("param %d: bits %d\n",
		    s, krp->krp_param[s].crp_nbits);
	}
#endif

d1995 104
@


1.95
log
@the exponent parameter is:
1. byte counted (not word count * 4 counted)
2. not normalized (we don't normalize anything yet, so no biggy)
Increase maximum size of exponent to 2048 bits.  (Better length checks coming soon)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.94 2002/05/02 18:28:24 jason Exp $	*/
a112 1
int	ubsec_kcopyout(struct crparam *crpar, caddr_t, u_int);
d1381 5
d1396 1
a1396 2
		if (ubsec_kcopyout(&me->me_krp->krp_param[UBS_MODEXP_PAR_C],
		    me->me_C.dma_vaddr, me->me_C.dma_size))
d1398 3
a1400 1
		else
d1402 4
d1815 1
d1830 1
a1830 1
	if (ubsec_dma_malloc(sc, 1024 / 8, &me->me_M, 0)) {
d1840 1
a1840 1
	if (ubsec_dma_malloc(sc, modbits/8, &me->me_C, 0)) {
a1963 21
	return (0);
}

int
ubsec_kcopyout(crpar, buf, bufsiz)
	struct crparam *crpar;
	caddr_t buf;
	u_int bufsiz;
{
	u_int nbytes = (crpar->crp_nbits + 7) / 8;
#ifdef UBSEC_DEBUG
	u_int i;

	for (i = 0; i < bufsiz; i++)
		printf("%s%02x", (i == 0) ? "ko " : ":", (u_int8_t)buf[i]);
	printf("\n");
#endif

	if (bufsiz > nbytes)
		return (-1);
	bcopy(buf, crpar->crp_p, bufsiz);
@


1.94
log
@More of previous (don't refer to parameters by index directly)
Also, add a sanity check that the result bits must be >= modulus bits.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.93 2002/05/02 18:20:50 jason Exp $	*/
d1825 1
a1825 1
	if (ubsec_dma_malloc(sc, 1024 / 8, &me->me_E, 0)) {
d1843 2
a1844 2
	if (ubsec_kcopyin(&krp->krp_param[UBS_MODEXP_PAR_E], me->me_E.dma_vaddr,
	    1024 / 8, &len)) {
d1848 1
d1898 2
a1899 1
	ctx->me_E_len = htole16(((krp->krp_param[UBS_MODEXP_PAR_E].crp_nbits + 31) / 32) * 32);
@


1.93
log
@Don't refer to the parameters directly by number, use macro's so it can be
changed later if necessary.
Also, don't bother bzero'n the mcr, all of the fields are initialized anyway
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.92 2002/05/01 21:35:08 jason Exp $	*/
d1755 1
a1755 1
 * Start computation of cr[3] = (cr[0] ^ cr[1]) mod cr[2]
d1790 7
@


1.92
log
@- make sure 'me' is initialized
- compute modulus bits early (if its too big, return E2BIG)
- modulus bits must be rounded to 512/768/1024 (and/or 1536/2048 for 5820)
- allocate the result based on modulus bits and bzero it
- add two diagnostic checks that will hang the chip: unaligned result/length
[score so far: 655 out of 1000 test cases work for modexp on 5820]
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.91 2002/04/30 00:26:10 jason Exp $	*/
d1392 1
a1392 1
		if (ubsec_kcopyout(&me->me_krp->krp_param[3],
d1777 1
a1777 1
	modbits = krp->krp_param[2].crp_nbits;
a1805 1
	bzero(mcr, sizeof(*mcr));
d1836 1
a1836 1
	if (ubsec_kcopyin(&krp->krp_param[1], me->me_E.dma_vaddr,
d1855 1
a1855 1
	if (ubsec_kcopyin(&krp->krp_param[0], me->me_M.dma_vaddr,
a1862 1
	mcr->mcr_reserved = 0;
d1880 1
a1880 1
	if (ubsec_kcopyin(&krp->krp_param[2], ctx->me_N,
d1885 1
a1885 1
	len = ((krp->krp_param[2].crp_nbits + 31) / 32) * 32;
d1890 1
a1890 1
	ctx->me_E_len = htole16(((krp->krp_param[1].crp_nbits + 31) / 32) * 32);
@


1.91
log
@- Output chain length must be equal to the byte count of the modulus length
- document parameter order (it may yet change)
- don't bother setting mcr_pktlen, it isn't used
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.90 2002/04/28 17:08:18 jason Exp $	*/
d1762 1
a1762 1
	struct ubsec_q2_modexp *me;
d1777 16
d1824 1
a1824 1
	if (ubsec_dma_malloc(sc, 1024 / 8, &me->me_C, 0)) {
d1828 1
d1868 11
a1878 1
	mcr->mcr_opktbuf.pb_len = htole32(me->me_C.dma_size);
a1881 1
	modbits = krp->krp_param[2].crp_nbits;
a1893 2

	mcr->mcr_opktbuf.pb_len = htole32(modbits / 8);
@


1.90
log
@make UBSEC_NO_RNG work correctly (ie. it means no RNG, not no MCR2 operations)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.89 2002/04/26 17:54:01 deraadt Exp $	*/
d1754 3
d1768 1
a1768 1
	u_int16_t ilen = 0;
a1780 1

d1789 2
d1818 1
a1824 1
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
a1827 1
	ilen += len;
a1832 2
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;
	bzero(mcr, sizeof(*mcr));
d1837 1
a1847 2
	ilen += len;
	mcr->mcr_pktlen = htole16(ilen);
a1851 3
#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
#endif
d1855 1
d1868 3
d1872 1
d1875 1
d1995 1
a1995 1
		printf(" Elen 0x%x, Nlen 0x%x\n",
@


1.89
log
@debug stuff inside #ifdef
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.88 2002/04/26 05:08:49 jason Exp $	*/
a380 1
#ifndef UBSEC_NO_RNG
d382 1
a382 1
	 * Check to see if we have any Random number waiting for us
a412 1
#endif /* UBSEC_NO_RNG */
a1319 1
#ifndef UBSEC_NO_RNG
d1364 1
d1379 1
d1417 1
@


1.88
log
@missed one; deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.87 2002/04/26 05:06:03 jason Exp $	*/
d1767 1
d1772 1
d1825 1
d1828 1
d1852 1
d1854 1
d1870 1
d1872 1
a1872 1

d1930 1
d1936 1
@


1.87
log
@1024 bit max
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.86 2002/04/26 04:24:17 jason Exp $	*/
d1814 1
a1814 1
	    2048 / 8, &len)) {
@


1.86
log
@register as supporting modular exponentiation
still contains much debugging code and isn't quite done, but its a start
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.85 2002/04/22 21:24:36 jason Exp $	*/
d1792 1
a1792 1
	if (ubsec_dma_malloc(sc, 2048 / 8, &me->me_M, 0)) {
d1797 1
a1797 1
	if (ubsec_dma_malloc(sc, 2048 / 8, &me->me_E, 0)) {
d1802 1
a1802 1
	if (ubsec_dma_malloc(sc, 2048 / 8, &me->me_C, 0)) {
d1834 1
a1834 1
	    2048 / 8, &len)) {
d1853 1
a1853 1
	    2048 / 8, &len)) {
@


1.85
log
@clean up attach messages
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.84 2002/04/08 17:49:42 jason Exp $	*/
d108 12
d170 1
d312 6
d1366 1
a1366 1
	switch (letoh16(ctx->ctx_op)) {
d1381 30
d1453 1
d1458 1
a1458 1
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, (struct ubsec_q2 *)rng, q_next);
d1550 2
a1551 2
   ubsec_init_board(sc)
   struct ubsec_softc *sc;
d1562 2
a1563 2
   ubsec_init_pciregs(pa)
   struct pci_attach_args *pa;
d1679 339
@


1.84
log
@Credit DARPA/USAF appropriately.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.83 2002/03/14 01:27:00 millert Exp $	*/
d225 1
a225 1
			printf("Warning can't allocate queue buffers for ubsec\n");
d231 1
a231 1
			printf("Warning can't allocate dma buffers for ubsec\n");
d264 2
a265 1
	
d299 1
a299 1
	printf(": %s\n", intrstr);
@


1.83
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.82 2002/01/28 17:10:11 jason Exp $	*/
d35 5
@


1.82
log
@Try to share a common src/dst map where possible, and cope with that
eventuality later in the code.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.81 2002/01/28 16:26:21 jason Exp $	*/
d71 8
a78 8
int ubsec_probe		__P((struct device *, void *, void *));
void ubsec_attach	__P((struct device *, struct device *, void *));
void ubsec_reset_board	__P((struct ubsec_softc *));
void ubsec_init_board	__P((struct ubsec_softc *));
void ubsec_init_pciregs	__P((struct pci_attach_args *pa));
void ubsec_cleanchip	__P((struct ubsec_softc *));
void ubsec_totalreset	__P((struct ubsec_softc *));
int  ubsec_free_q	__P((struct ubsec_softc*, struct ubsec_q *));
d88 14
a101 14
int	ubsec_intr __P((void *));
int	ubsec_newsession __P((u_int32_t *, struct cryptoini *));
int	ubsec_freesession __P((u_int64_t));
int	ubsec_process __P((struct cryptop *));
void	ubsec_callback __P((struct ubsec_softc *, struct ubsec_q *));
int	ubsec_feed __P((struct ubsec_softc *));
void	ubsec_mcopy __P((struct mbuf *, struct mbuf *, int, int));
void	ubsec_callback2 __P((struct ubsec_softc *, struct ubsec_q2 *));
int	ubsec_feed2 __P((struct ubsec_softc *));
void	ubsec_rng __P((void *));
int	ubsec_dma_malloc __P((struct ubsec_softc *, bus_size_t,
    struct ubsec_dma_alloc *, int));
void	ubsec_dma_free __P((struct ubsec_softc *, struct ubsec_dma_alloc *));
int	ubsec_dmamap_aligned __P((bus_dmamap_t));
@


1.81
log
@First round of post-bus_dma cleanups:
- remove the packl/packp arrays and rework handling appropriately
- destination map may or may not exist, cope.
- remove a redundant bus_dmamap_sync() in _process (real sync is in _feed)
- remove long deprecated q_{src,dst}pkt stuff from queue structure
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.80 2002/01/28 15:44:36 jason Exp $	*/
d984 4
a987 14
		if (!nicealign && (crp->crp_flags & CRYPTO_F_IOV)) {
			err = EINVAL;
			goto errout;
		} else if (!nicealign && (crp->crp_flags & CRYPTO_F_IMBUF)) {
			int totlen, len;
			struct mbuf *m, *top, **mp;

			totlen = q->q_src_map->dm_mapsize;
			if (q->q_src_m->m_flags & M_PKTHDR) {
				len = MHLEN;
				MGETHDR(m, M_DONTWAIT, MT_DATA);
			} else {
				len = MLEN;
				MGET(m, M_DONTWAIT, MT_DATA);
d989 3
a991 1
			if (m == NULL) {
d995 5
a999 6
			if (len == MHLEN)
				M_DUP_PKTHDR(m, q->q_src_m);
			if (totlen >= MINCLSIZE) {
				MCLGET(m, M_DONTWAIT);
				if (m->m_flags & M_EXT)
					len = MCLBYTES;
d1001 7
a1007 3
			m->m_len = len;
			top = NULL;
			mp = &top;
d1009 6
a1014 2
			while (totlen > 0) {
				if (top) {
a1015 6
					if (m == NULL) {
						m_freem(top);
						err = ENOMEM;
						goto errout;
					}
					len = MLEN;
d1017 7
a1023 1
				if (top && totlen >= MINCLSIZE) {
d1028 42
a1069 4
				m->m_len = len = min(totlen, len);
				totlen -= len;
				*mp = m;
				mp = &m->m_next;
d1071 2
a1072 8
			q->q_dst_m = top;
			ubsec_mcopy(q->q_src_m, q->q_dst_m, cpskip, cpoffset);
		} else
			q->q_dst_m = q->q_src_m;

		if (bus_dmamap_create(sc->sc_dmat, 0xfff0, UBS_MAX_SCATTER,
		    0xfff0, 0, BUS_DMA_NOWAIT, &q->q_dst_map) != 0) {
			err = ENOMEM;
a1074 16
		if (crp->crp_flags & CRYPTO_F_IMBUF) {
			if (bus_dmamap_load_mbuf(sc->sc_dmat, q->q_dst_map,
			    q->q_dst_m, BUS_DMA_NOWAIT) != 0) {
				bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
				q->q_dst_map = NULL;
				err = ENOMEM;
				goto errout;
			}
		} else if (crp->crp_flags & CRYPTO_F_IOV) {
			if (bus_dmamap_load_uio(sc->sc_dmat, q->q_dst_map,
			    q->q_dst_io, BUS_DMA_NOWAIT) != 0) {
				bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
				q->q_dst_map = NULL;
				goto errout;
			}
		}
d1170 9
a1186 9
	if (q->q_src_map != NULL) {
		bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
		bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
	}
	if (q->q_dst_map != NULL) {
		bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
		bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
	}

d1205 1
a1205 3
	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
	if (q->q_dst_map != NULL) {
d1208 2
a1209 1
		bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
d1211 4
a1214 1
	bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
@


1.80
log
@Quick (and a little dirty) conversion to bus_dma(9).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.79 2002/01/24 03:13:43 jason Exp $	*/
d455 3
a457 2
	bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
	    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
d465 3
a467 2
		bus_dmamap_sync(sc->sc_dmat, q2->q_dst_map,
		    0, q2->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
d499 3
a501 2
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
a919 6
	q->q_src_l = q->q_src_map->dm_mapsize;
	q->q_src_npa = q->q_src_map->dm_nsegs;
	for (i = 0; i < q->q_src_map->dm_nsegs; i++) {
		q->q_src_packp[i] = q->q_src_map->dm_segs[i].ds_addr;
		q->q_src_packl[i] = q->q_src_map->dm_segs[i].ds_len;
	}
d927 1
a927 1
	for (i = j = 0; i < q->q_src_npa; i++) {
d929 2
d932 3
a934 12
#ifdef UBSEC_DEBUG
		printf("  src[%d->%d]: %d@@%x\n", i, j,
		    q->q_src_packl[i], q->q_src_packp[i]);
#endif
		if (sskip) {
			if (sskip >= q->q_src_packl[i]) {
				sskip -= q->q_src_packl[i];
				continue;
			}
			q->q_src_packp[i] += sskip;
			q->q_src_packl[i] -= sskip;
			sskip = 0;
d937 5
a941 1
		if (q->q_src_packl[i] > 0xfffc) {
d951 2
a952 1
		pb->pb_addr = htole32(q->q_src_packp[i]);
d954 1
a954 1
			if (q->q_src_packl[i] > stheend) {
d958 2
a959 2
				pb->pb_len = htole32(q->q_src_packl[i]);
				stheend -= q->q_src_packl[i];
d962 1
a962 1
			pb->pb_len = htole32(q->q_src_packl[i]);
d964 1
a964 1
		if ((i + 1) == q->q_src_npa)
d991 1
a991 1
			totlen = q->q_dst_l = q->q_src_l;
a1060 7
		q->q_dst_l = q->q_dst_map->dm_mapsize;
		q->q_dst_npa = q->q_dst_map->dm_nsegs;
		for (i = 0; i < q->q_dst_map->dm_nsegs; i++) {
			q->q_dst_packp[i] = q->q_dst_map->dm_segs[i].ds_addr;
			q->q_dst_packl[i] = q->q_dst_map->dm_segs[i].ds_len;
		}

d1064 1
a1064 1
		for (i = j = 0; i < q->q_dst_npa; i++) {
d1066 2
d1069 3
a1071 12
#ifdef UBSEC_DEBUG
			printf("  dst[%d->%d]: %d@@%x\n", i, j,
			    q->q_dst_packl[i], q->q_dst_packp[i]);
#endif
			if (dskip) {
				if (dskip >= q->q_dst_packl[i]) {
					dskip -= q->q_dst_packl[i];
					continue;
				}
				q->q_dst_packp[i] += dskip;
				q->q_dst_packl[i] -= dskip;
				dskip = 0;
d1074 5
a1078 1
			if (q->q_dst_packl[i] > 0xfffc) {
d1088 1
a1088 1
			pb->pb_addr = htole32(q->q_dst_packp[i]);
d1091 1
a1091 1
				if (q->q_dst_packl[i] > dtheend) {
d1095 2
a1096 2
					pb->pb_len = htole32(q->q_dst_packl[i]);
					dtheend -= q->q_dst_packl[i];
d1099 1
a1099 1
				pb->pb_len = htole32(q->q_dst_packl[i]);
d1101 1
a1101 1
			if ((i + 1) == q->q_dst_npa) {
a1139 3
	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
	    dmap->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
d1192 5
a1196 3
	bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
	    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
	bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
@


1.79
log
@More cleaning
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.78 2002/01/19 21:15:37 jason Exp $	*/
d101 1
d453 5
d462 4
d476 3
d494 9
d895 2
a896 7
	if (crp->crp_flags & CRYPTO_F_IMBUF)
		q->q_src_l = mbuf2pages(q->q_src_m, &q->q_src_npa, q->q_src_packp,
		    q->q_src_packl, UBS_MAX_SCATTER, &nicealign);
	else if (crp->crp_flags & CRYPTO_F_IOV)
		q->q_src_l = iov2pages(q->q_src_io, &q->q_src_npa,
		    q->q_src_packp, q->q_src_packl, UBS_MAX_SCATTER, &nicealign);
	if (q->q_src_l == 0) {
d900 22
a921 3
	if (q->q_src_l > 0xfffc) {
		err = EIO;
		goto errout;
d923 1
d1044 2
a1045 8
		if (crp->crp_flags & CRYPTO_F_IMBUF)
			q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa,
			    q->q_dst_packp, q->q_dst_packl, UBS_MAX_SCATTER, NULL);
		else if (crp->crp_flags & CRYPTO_F_IOV)
			q->q_dst_l = iov2pages(q->q_dst_io, &q->q_dst_npa,
			    q->q_dst_packp, q->q_dst_packl, UBS_MAX_SCATTER, NULL);

		if (q->q_dst_l == 0) {
d1049 22
a1070 3
		if (q->q_dst_l > 0xfffc) {
			err = ENOMEM;
			goto errout;
d1181 9
d1208 6
d1606 16
@


1.78
log
@From Patrik Lindergren (patrik@@ipunplugged.com):
* make the driver big-endian aware
* handling for DMA errors
* move some allocations to attach
From me:
whitespace clean up and vtophys removal (almost works on sparc64)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.77 2002/01/02 20:35:40 deraadt Exp $	*/
a465 3
#if 0
	WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(&q->q_dma->d_dma->d_mcr));
#else
a467 1
#endif
a480 3
#if 0
		WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(&q->q_dma->d_dma->d_mcr));
#else
a482 1
#endif
@


1.77
log
@at least ; required after label or case; openbsd@@davidkrause.com
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.76 2001/12/07 18:08:20 jason Exp $	*/
d6 2
d40 1
a40 1
 * uBsec 5[56]01, 580x hardware crypto accelerator
d73 6
d108 5
a112 1
#define	SWAP32(x) (x) = swap32((x))
d211 1
a211 1
	SIMPLEQ_INIT(&sc->sc_dma);
d214 9
d224 3
a226 1
		    &dmap->d_alloc, 0))
d228 1
d230 5
a234 1
		SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);
d243 17
d290 1
a290 4

	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    ((sc->sc_flags & UBS_FLAGS_KEY) ? BS_CTRL_MCR2INT : 0));
d295 3
d303 1
a303 1
	volatile u_int32_t stat, a;
d305 1
a305 2
	struct ubsec_q2 *q2;
	struct ubsec_mcr *mcr;
d316 3
d322 3
a324 5
#ifdef UBSEC_DEBUG
			printf("mcr_flags %x %x %x\n", q->q_mcr,
			    q->q_mcr->mcr_flags, READ_REG(sc, BS_ERR));
#endif
			if ((q->q_mcr->mcr_flags & UBS_MCR_DONE) == 0)
d326 1
a326 1
			npkts++;
a327 6
#ifdef UBSEC_DEBUG
			printf("intr: callback q %08x flags %04x\n", q,
			    q->q_mcr->mcr_flags);
#endif
			mcr = q->q_mcr;
			ubsec_callback(sc, q);
d329 1
d335 4
a338 10
			for (i = 1; i < mcr->mcr_pkts; i++) {
				q = SIMPLEQ_FIRST(&sc->sc_qchip);
				if (q && q->q_mcr == mcr) {
#ifdef UBSEC_DEBUG
					printf("found shared mcr %d out of %d\n",
					    i, mcr->mcr_pkts);
#endif
					SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip,
					    q, q_next);
					ubsec_callback(sc, q);
a339 1
					printf("HUH!\n");
d343 3
d347 6
a352 7
			free(mcr, M_DEVBUF);
		}
#ifdef UBSEC_DEBUG
		if (npkts > 1)
			printf("intr: %d pkts\n", npkts);
#endif
		ubsec_feed(sc);
d355 4
d360 3
d371 1
a371 1
			if ((mcr->mcr_flags & UBS_MCR_DONE) == 0) {
d380 6
a385 1
			ubsec_feed2(sc);
d388 1
d390 3
d394 3
a396 1
		a = READ_REG(sc, BS_ERR);
d398 5
a402 2
		    (a & BS_ERR_READ) ? "read" : "write",
		    a & BS_ERR_ADDR);
d408 4
d418 5
a422 5
#endif
	struct ubsec_q *q;
	struct ubsec_mcr *mcr;
	int npkts, i, l;
	void *v, *mcr2;
d430 5
a434 1
	if (READ_REG(sc, BS_STAT) & BS_STAT_MCR1_FULL)
d436 1
a436 5

	mcr = (struct ubsec_mcr *)malloc(sizeof(struct ubsec_mcr) +
	    (npkts-1) * sizeof(struct ubsec_mcr_add), M_DEVBUF, M_NOWAIT);
	if (mcr == NULL)
		goto feed1;
d446 7
a452 1
#endif
d454 3
a456 3
	for (mcr2 = mcr, i = 0; i < npkts; i++) {
		q = SIMPLEQ_FIRST(&sc->sc_queue);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
a457 1
		SIMPLEQ_INSERT_TAIL(&sc->sc_qchip, q, q_next);
d459 12
a470 14
		/*
		 * first packet contains a full mcr, others contain
		 * a shortened one
		 */
		if (i == 0) {
			v = q->q_mcr;
			l = sizeof(struct ubsec_mcr);
		} else {
			v = ((void *)q->q_mcr) + sizeof(struct ubsec_mcr) -
			    sizeof(struct ubsec_mcr_add);
			l = sizeof(struct ubsec_mcr_add);
		}
#ifdef UBSEC_DEBUG
		printf("copying %d from %x (mcr %x)\n", l, v, q->q_mcr);
a471 8
		bcopy(v, mcr2, l);
		mcr2 += l;

		free(q->q_mcr, M_DEVBUF);
		q->q_mcr = mcr;
	}
	mcr->mcr_pkts = npkts;
	WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(mcr));
d476 5
a480 1
		if (READ_REG(sc, BS_STAT) & BS_STAT_MCR1_FULL)
d482 2
d485 9
a493 1
		WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(q->q_mcr));
d686 2
a687 1
	if (crp == NULL || crp->crp_callback == NULL)
d689 1
a689 1

d691 2
a692 1
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL)
d694 1
d699 3
a701 1
	if (sc->sc_nqueue >= UBS_MAX_NQUEUE) {
d704 1
a704 1
		goto errout;
d706 3
a708 7
	if (SIMPLEQ_EMPTY(&sc->sc_dma)) {
		splx(s);
		err = ENOMEM;
		goto errout;
	}
	dmap = SIMPLEQ_FIRST(&sc->sc_dma);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_dma, dmap, d_next);
d711 1
a711 6
	q = (struct ubsec_q *)malloc(sizeof(struct ubsec_q),
	    M_DEVBUF, M_NOWAIT);
	if (q == NULL) {
		err = ENOMEM;
		goto errout;
	}
d730 1
a730 7
	q->q_mcr = (struct ubsec_mcr *)malloc(sizeof(struct ubsec_mcr),
	    M_DEVBUF, M_NOWAIT);
	if (q->q_mcr == NULL) {
		err = ENOMEM;
		goto errout;
	}
	bzero(q->q_mcr, sizeof(struct ubsec_mcr));
d732 2
a733 2
	q->q_mcr->mcr_pkts = 1;
	q->q_mcr->mcr_flags = 0;
d782 1
a782 1
		ctx.pc_flags |= UBS_PKTCTX_ENC_3DES;
d805 1
a805 1
			ctx.pc_flags |= UBS_PKTCTX_INBOUND;
d832 1
a832 1
			ctx.pc_flags |= UBS_PKTCTX_AUTH_MD5;
d834 1
a834 1
			ctx.pc_flags |= UBS_PKTCTX_AUTH_SHA1;
d839 3
d879 1
a879 1
	ctx.pc_offset = coffset >> 2;
d896 1
a896 1
	q->q_mcr->mcr_pktlen = stheend;
d924 1
a924 1
			pb = &q->q_mcr->mcr_ipktbuf;
d928 1
a928 1
		pb->pb_addr = q->q_src_packp[i];
d931 1
a931 1
				pb->pb_len = stheend;
d934 2
a935 2
				pb->pb_len = q->q_src_packl[i];
				stheend -= pb->pb_len;
d938 1
a938 1
			pb->pb_len = q->q_src_packl[i];
d943 2
a944 2
			pb->pb_next = dmap->d_alloc.dma_paddr +
			    offsetof(struct ubsec_dmachunk, d_sbuf[j]);
d949 4
a952 4
		q->q_mcr->mcr_opktbuf.pb_addr = 0;
		q->q_mcr->mcr_opktbuf.pb_len = 0;
		q->q_mcr->mcr_opktbuf.pb_next = dmap->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_macbuf[0]);
d955 3
a957 3
		    q->q_mcr->mcr_opktbuf.pb_addr,
		    q->q_mcr->mcr_opktbuf.pb_len,
		    q->q_mcr->mcr_opktbuf.pb_next);
d1057 1
a1057 1
				pb = &q->q_mcr->mcr_opktbuf;
d1061 1
a1061 1
			pb->pb_addr = q->q_dst_packp[i];
d1065 1
a1065 1
					pb->pb_len = dtheend;
d1068 2
a1069 2
					pb->pb_len = q->q_dst_packl[i];
					dtheend -= pb->pb_len;
d1072 1
a1072 1
				pb->pb_len = q->q_dst_packl[i];
d1076 2
a1077 2
					pb->pb_next = dmap->d_alloc.dma_paddr +
					    offsetof(struct ubsec_dmachunk, d_macbuf[0]);
d1081 2
a1082 2
				pb->pb_next = dmap->d_alloc.dma_paddr +
				    offsetof(struct ubsec_dmachunk, d_dbuf[j]);
d1087 2
a1088 2
	q->q_mcr->mcr_cmdctxp = dmap->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_ctx);
d1097 2
a1098 2
		ctxl->pc_len = sizeof(struct ubsec_pktctx_long);
		ctxl->pc_type = UBS_PKTCTX_TYPE_IPSEC;
d1120 2
a1127 7
		if (q->q_mcr != NULL)
			free(q->q_mcr, M_DEVBUF);
		if (dmap != NULL) {
			s = splnet();
			SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);
			splx(s);
		}
d1130 4
a1133 1
		free(q, M_DEVBUF);
d1135 5
d1162 1
d1196 1
a1196 8

	SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);

	/*
	 * note that q->q_mcr is not freed, because ubsec_intr() has to
	 * deal with possible sharing
	 */
	free(q, M_DEVBUF);
d1239 1
d1269 3
d1283 1
a1283 1
	switch (ctx->ctx_op) {
d1293 1
a1293 1
			add_true_randomness(*p);
d1300 1
a1300 1
		    ctx->ctx_op);
d1327 1
a1327 1
	mcr->mcr_pkts = 1;
d1329 1
a1329 1
	mcr->mcr_cmdctxp = rng->rng_q.q_ctx.dma_paddr;
d1333 3
a1335 3
	mcr->mcr_opktbuf.pb_addr = rng->rng_buf.dma_paddr;
	mcr->mcr_opktbuf.pb_len = ((sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ)) &
	    UBS_PKTBUF_LEN;
d1338 2
a1339 2
	ctx->rbp_len = sizeof(struct ubsec_ctx_rngbypass);
	ctx->rbp_op = UBS_CTXOP_RNGBYPASS;
d1359 1
d1410 139
@


1.76
log
@Don't disable TRDY/RETRY, this doesn't have the expected behavior.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.75 2001/11/20 20:19:26 jason Exp $	*/
d243 1
@


1.76.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.82 2002/01/28 17:10:11 jason Exp $	*/
a5 2
 * Copyright (c) 2001 Patrik Lindergren (patrik@@ipunplugged.com)
 * 
d38 1
a38 1
 * uBsec 5[56]01, 58xx hardware crypto accelerator
a70 6
void ubsec_reset_board	__P((struct ubsec_softc *));
void ubsec_init_board	__P((struct ubsec_softc *));
void ubsec_init_pciregs	__P((struct pci_attach_args *pa));
void ubsec_cleanchip	__P((struct ubsec_softc *));
void ubsec_totalreset	__P((struct ubsec_softc *));
int  ubsec_free_q	__P((struct ubsec_softc*, struct ubsec_q *));
a92 1
int	ubsec_dmamap_aligned __P((bus_dmamap_t));
d100 1
a100 5
#define	SWAP32(x) (x) = htole32(ntohl((x)))
#define	HTOLE32(x) (x) = htole32(x)


struct ubsec_stats ubsecstats;
d199 1
a199 1
	SIMPLEQ_INIT(&sc->sc_freequeue);
a201 9
		struct ubsec_q *q;

		q = (struct ubsec_q *)malloc(sizeof(struct ubsec_q),
		    M_DEVBUF, M_NOWAIT);
		if (q == NULL) {
			printf("Warning can't allocate queue buffers for ubsec\n");
			break;
		}

d203 1
a203 3
		    &dmap->d_alloc, 0)) {
			printf("Warning can't allocate dma buffers for ubsec\n");
			free(q, M_DEVBUF);
a204 1
		}
d206 1
a206 5

		q->q_dma = dmap;
		sc->sc_queuea[i] = q;

		SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
a214 17
	/*
	 * Reset Broadcom chip
	 */
	ubsec_reset_board(sc);

	/*
	 * Init Broadcom specific PCI settings
	 */
	ubsec_init_pciregs(pa);

	/*
	 * Init Broadcom chip
	 */
	ubsec_init_board(sc);

	
#ifndef UBSEC_NO_RNG
a242 1
	;
d244 4
a247 1
#endif /* UBSEC_NO_RNG */
a251 3
/*
 * UBSEC Interrupt routine
 */
d257 1
a257 1
	volatile u_int32_t stat;
d259 2
a260 1
	struct ubsec_dma *dmap;
a270 3
	/*
	 * Check to see if we have any packets waiting for us
	 */
d274 5
a278 3
			dmap = q->q_dma;

			if ((dmap->d_dma->d_mcr.mcr_flags & htole16(UBS_MCR_DONE)) == 0)
d280 1
a280 1

d282 6
a288 1
			npkts = q->q_nstacked_mcrs;
d294 10
a303 4
			for (i = 0; i < npkts; i++) {
				if(q->q_stacked_mcr[i]) {
					ubsec_callback(sc, q->q_stacked_mcr[i]);
					ubsecstats.hst_opackets++;
d305 1
d309 2
a310 2
			ubsec_callback(sc, q);
			ubsecstats.hst_opackets++;
d312 5
a316 7

		/*
		 * Don't send any more packet to chip if there has been
		 * a DMAERR.
		 */
		if (!(stat & BS_STAT_DMAERR))
			ubsec_feed(sc);
a318 4
#ifndef UBSEC_NO_RNG
	/*
	 * Check to see if we have any Random number waiting for us
	 */
a319 3
		struct ubsec_q2 *q2;
		struct ubsec_mcr *mcr;

d328 1
a328 1
			if ((mcr->mcr_flags & htole16(UBS_MCR_DONE)) == 0) {
d337 1
a337 6
			/*
			 * Don't send any more packet to chip if there has been
			 * a DMAERR.
			 */
			if (!(stat & BS_STAT_DMAERR))
				ubsec_feed2(sc);
a339 1
#endif /* UBSEC_NO_RNG */
a340 3
	/*
	 * Check to see if we got any DMA Error
	 */
d342 1
a342 3
#ifdef UBSEC_DEBUG
		volatile u_int32_t a = READ_REG(sc, BS_ERR);

d344 2
a345 5
		    (a & BS_ERR_READ) ? "read" : "write", a & BS_ERR_ADDR);
#endif /* UBSEC_DEBUG */
		ubsecstats.hst_dmaerr++;
		ubsec_totalreset(sc);
		ubsec_feed(sc);
a350 4
/*
 * ubsec_feed() - aggregate and post requests to chip
 *		  It is assumed that the caller set splnet()
 */
d357 5
a361 5
#endif /* UBSEC_DEBUG */
	struct ubsec_q *q, *q2;
	int npkts, i;
	void *v;
	u_int32_t stat;
d369 1
a369 5
	if ((stat = READ_REG(sc, BS_STAT)) & (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
		if(stat & BS_STAT_DMAERR) {
			ubsec_totalreset(sc);
			ubsecstats.hst_dmaerr++;
		}
d371 5
a375 1
	}
d385 1
a385 1
#endif /* UBSEC_DEBUG */
d387 3
a389 20
	q = SIMPLEQ_FIRST(&sc->sc_queue);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
	--sc->sc_nqueue;

	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	if (q->q_dst_map != NULL)
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	q->q_nstacked_mcrs = npkts - 1;		/* Number of packets stacked */

	for (i = 0; i < q->q_nstacked_mcrs; i++) {
		q2 = SIMPLEQ_FIRST(&sc->sc_queue);
		bus_dmamap_sync(sc->sc_dmat, q2->q_src_map,
		    0, q2->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
		if (q2->q_dst_map != NULL)
			bus_dmamap_sync(sc->sc_dmat, q2->q_dst_map,
			    0, q2->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q2, q_next);
d391 1
d393 23
a415 12
		v = ((void *)&q2->q_dma->d_dma->d_mcr) + sizeof(struct ubsec_mcr) -
		    sizeof(struct ubsec_mcr_add);
		bcopy(v, &q->q_dma->d_dma->d_mcradd[i], sizeof(struct ubsec_mcr_add));
		q->q_stacked_mcr[i] = q2;
	}
	q->q_dma->d_dma->d_mcr.mcr_pkts = htole16(npkts);
	SIMPLEQ_INSERT_TAIL(&sc->sc_qchip, q, q_next);
	bus_dmamap_sync(sc->sc_dmat, q->q_dma->d_alloc.dma_map,
	    0, q->q_dma->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	WRITE_REG(sc, BS_MCR1, q->q_dma->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_mcr));
d420 1
a420 5
		if ((stat = READ_REG(sc, BS_STAT)) & (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
			if(stat & BS_STAT_DMAERR) {
				ubsec_totalreset(sc);
				ubsecstats.hst_dmaerr++;
			}
a421 2
		}

d423 1
a423 15

		bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
		    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
		if (q->q_dst_map != NULL)
			bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
			    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
		bus_dmamap_sync(sc->sc_dmat, q->q_dma->d_alloc.dma_map,
		    0, q->q_dma->d_alloc.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		WRITE_REG(sc, BS_MCR1, q->q_dma->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_mcr));
#ifdef UBSEC_DEBUG
		printf("feed: q->chip %p %08x\n", q, (u_int32_t)vtophys(&q->q_dma->d_dma->d_mcr));
#endif /* UBSEC_DEBUG */
d616 1
a616 2
	if (crp == NULL || crp->crp_callback == NULL) {
		ubsecstats.hst_invalid++;
d618 1
a618 1
	}
d620 1
a620 2
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL) {
		ubsecstats.hst_invalid++;
a621 1
	}
d626 6
a631 3

	if (SIMPLEQ_EMPTY(&sc->sc_freequeue)) {
		ubsecstats.hst_queuefull++;
d634 1
a634 1
		goto errout2;
d636 2
a637 3

	q = SIMPLEQ_FIRST(&sc->sc_freequeue);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_freequeue, q, q_next);
d640 6
a645 1
	dmap = q->q_dma; /* Save dma pointer */
d664 7
a670 1
	bzero(&dmap->d_dma->d_mcr, sizeof(struct ubsec_mcr));
d672 2
a673 2
	dmap->d_dma->d_mcr.mcr_pkts = htole16(1);
	dmap->d_dma->d_mcr.mcr_flags = 0;
d722 1
a722 1
		ctx.pc_flags |= htole16(UBS_PKTCTX_ENC_3DES);
d745 1
a745 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_INBOUND);
d772 1
a772 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_AUTH_MD5);
d774 1
a774 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_AUTH_SHA1);
a778 3

			HTOLE32(ctx.pc_hminner[i]);
			HTOLE32(ctx.pc_hmouter[i]);
d816 1
a816 1
	ctx.pc_offset = htole16(coffset >> 2);
d818 7
a824 2
	if (bus_dmamap_create(sc->sc_dmat, 0xfff0, UBS_MAX_SCATTER,
		0xfff0, 0, BUS_DMA_NOWAIT, &q->q_src_map) != 0) {
d828 3
a830 16
	if (crp->crp_flags & CRYPTO_F_IMBUF) {
		if (bus_dmamap_load_mbuf(sc->sc_dmat, q->q_src_map,
		    q->q_src_m, BUS_DMA_NOWAIT) != 0) {
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
			q->q_src_map = NULL;
			err = ENOMEM;
			goto errout;
		}
	} else if (crp->crp_flags & CRYPTO_F_IOV) {
		if (bus_dmamap_load_uio(sc->sc_dmat, q->q_src_map,
		    q->q_src_io, BUS_DMA_NOWAIT) != 0) {
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
			q->q_src_map = NULL;
			err = ENOMEM;
			goto errout;
		}
a831 1
	nicealign = ubsec_dmamap_aligned(q->q_src_map);
d833 1
a833 1
	dmap->d_dma->d_mcr.mcr_pktlen = htole16(stheend);
d838 1
a838 1
	for (i = j = 0; i < q->q_src_map->dm_nsegs; i++) {
a839 2
		bus_size_t packl = q->q_src_map->dm_segs[i].ds_len;
		bus_addr_t packp = q->q_src_map->dm_segs[i].ds_addr;
d841 12
a852 3
		if (sskip >= packl) {
			sskip -= packl;
			continue;
d855 1
a855 5
		packl -= sskip;
		packp += sskip;
		sskip = 0;

		if (packl > 0xfffc) {
d861 1
a861 1
			pb = &dmap->d_dma->d_mcr.mcr_ipktbuf;
d865 1
a865 2
		pb->pb_addr = htole32(packp);

d867 2
a868 2
			if (packl > stheend) {
				pb->pb_len = htole32(stheend);
d871 2
a872 2
				pb->pb_len = htole32(packl);
				stheend -= packl;
d875 1
a875 1
			pb->pb_len = htole32(packl);
d877 1
a877 1
		if ((i + 1) == q->q_src_map->dm_nsegs)
d880 2
a881 2
			pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
			    offsetof(struct ubsec_dmachunk, d_sbuf[j]));
d886 4
a889 4
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_addr = 0;
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_len = 0;
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_next = htole32(dmap->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_macbuf[0]));
d892 3
a894 3
		    dmap->d_mcr->mcr_opktbuf.pb_addr,
		    dmap->d_mcr->mcr_opktbuf.pb_len,
		    dmap->d_mcr->mcr_opktbuf.pb_next);
d897 14
a910 4
		if (crp->crp_flags & CRYPTO_F_IOV) {
			if (!nicealign) {
				err = EINVAL;
				goto errout;
d912 1
a912 3
			if (bus_dmamap_create(sc->sc_dmat, 0xfff0,
			    UBS_MAX_SCATTER, 0xfff0, 0, BUS_DMA_NOWAIT,
			    &q->q_dst_map) != 0) {
d916 6
a921 5
			if (bus_dmamap_load_uio(sc->sc_dmat, q->q_dst_map,
			    q->q_dst_io, BUS_DMA_NOWAIT) != 0) {
				bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
				q->q_dst_map = NULL;
				goto errout;
d923 3
a925 7
		} else if (crp->crp_flags & CRYPTO_F_IMBUF) {
			if (nicealign) {
				q->q_dst_m = q->q_src_m;
				q->q_dst_map = q->q_src_map;
			} else {
				int totlen, len;
				struct mbuf *m, *top, **mp;
d927 8
a934 5
				totlen = q->q_src_map->dm_mapsize;
				if (q->q_src_m->m_flags & M_PKTHDR) {
					len = MHLEN;
					MGETHDR(m, M_DONTWAIT, MT_DATA);
				} else {
a935 1
					MGET(m, M_DONTWAIT, MT_DATA);
d937 1
a937 7
				if (m == NULL) {
					err = ENOMEM;
					goto errout;
				}
				if (len == MHLEN)
					M_DUP_PKTHDR(m, q->q_src_m);
				if (totlen >= MINCLSIZE) {
d942 4
a945 42
				m->m_len = len;
				top = NULL;
				mp = &top;

				while (totlen > 0) {
					if (top) {
						MGET(m, M_DONTWAIT, MT_DATA);
						if (m == NULL) {
							m_freem(top);
							err = ENOMEM;
							goto errout;
						}
						len = MLEN;
					}
					if (top && totlen >= MINCLSIZE) {
						MCLGET(m, M_DONTWAIT);
						if (m->m_flags & M_EXT)
							len = MCLBYTES;
					}
					m->m_len = len = min(totlen, len);
					totlen -= len;
					*mp = m;
					mp = &m->m_next;
				}
				q->q_dst_m = top;
				ubsec_mcopy(q->q_src_m, q->q_dst_m,
				    cpskip, cpoffset);
				if (bus_dmamap_create(sc->sc_dmat, 0xfff0,
				    UBS_MAX_SCATTER, 0xfff0, 0, BUS_DMA_NOWAIT,
				    &q->q_dst_map) != 0) {
					err = ENOMEM;
					goto errout;
				}
				if (bus_dmamap_load_mbuf(sc->sc_dmat,
				    q->q_dst_map, q->q_dst_m,
				    BUS_DMA_NOWAIT) != 0) {
					bus_dmamap_destroy(sc->sc_dmat,
					q->q_dst_map);
					q->q_dst_map = NULL;
					err = ENOMEM;
					goto errout;
				}
d947 18
a964 2
		} else {
			err = EINVAL;
d971 1
a971 1
		for (i = j = 0; i < q->q_dst_map->dm_nsegs; i++) {
a972 2
			bus_size_t packl = q->q_dst_map->dm_segs[i].ds_len;
			bus_addr_t packp = q->q_dst_map->dm_segs[i].ds_addr;
d974 12
a985 3
			if (dskip >= packl) {
				dskip -= packl;
				continue;
d988 1
a988 5
			packl -= dskip;
			packp += dskip;
			dskip = 0;

			if (packl > 0xfffc) {
d994 1
a994 1
				pb = &dmap->d_dma->d_mcr.mcr_opktbuf;
d998 1
a998 1
			pb->pb_addr = htole32(packp);
d1001 2
a1002 2
				if (packl > dtheend) {
					pb->pb_len = htole32(dtheend);
d1005 2
a1006 2
					pb->pb_len = htole32(packl);
					dtheend -= packl;
d1009 1
a1009 1
				pb->pb_len = htole32(packl);
d1011 1
a1011 1
			if ((i + 1) == q->q_dst_map->dm_nsegs) {
d1013 2
a1014 2
					pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
					    offsetof(struct ubsec_dmachunk, d_macbuf[0]));
d1018 2
a1019 2
				pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
				    offsetof(struct ubsec_dmachunk, d_dbuf[j]));
d1024 2
a1025 2
	dmap->d_dma->d_mcr.mcr_cmdctxp = htole32(dmap->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_ctx));
d1034 2
a1035 2
		ctxl->pc_len = htole16(sizeof(struct ubsec_pktctx_long));
		ctxl->pc_type = htole16(UBS_PKTCTX_TYPE_IPSEC);
d1050 3
a1056 2
	ubsecstats.hst_ipackets++;
	ubsecstats.hst_ibytes += dmap->d_alloc.dma_map->dm_mapsize;
d1063 7
d1072 1
a1072 13

		if (q->q_dst_map != NULL && q->q_dst_map != q->q_src_map) {
			bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
			bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
		}
		if (q->q_src_map != NULL) {
			bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
		}

		s = splnet();
		SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
		splx(s);
a1073 5
	if (err == EINVAL)
		ubsecstats.hst_invalid++;
	else
		ubsecstats.hst_nomem++;
errout2:
a1090 10
	if (q->q_dst_map != NULL && q->q_dst_map != q->q_src_map) {
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
		bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
	}
	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
	bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
a1095 1
	ubsecstats.hst_obytes += ((struct mbuf *)crp->crp_buf)->m_len;
d1129 8
a1136 1
	SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
a1178 1
#ifndef UBSEC_NO_RNG
a1207 3
/*
 * Callback for handling random numbers
 */
d1219 1
a1219 1
	switch (letoh16(ctx->ctx_op)) {
d1229 1
a1229 1
			add_true_randomness(letoh32(*p));
d1236 1
a1236 1
		    letoh16(ctx->ctx_op));
d1263 1
a1263 1
	mcr->mcr_pkts = htole16(1);
d1265 1
a1265 1
	mcr->mcr_cmdctxp = htole32(rng->rng_q.q_ctx.dma_paddr);
d1269 3
a1271 3
	mcr->mcr_opktbuf.pb_addr = htole32(rng->rng_buf.dma_paddr);
	mcr->mcr_opktbuf.pb_len = htole32(((sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ)) &
	    UBS_PKTBUF_LEN);
d1274 2
a1275 2
	ctx->rbp_len = htole16(sizeof(struct ubsec_ctx_rngbypass));
	ctx->rbp_op = htole16(UBS_CTXOP_RNGBYPASS);
a1294 1
#endif /* UBSEC_NO_RNG */
a1344 155
}

/*
 * Resets the board.  Values in the regesters are left as is
 * from the reset (i.e. initial values are assigned elsewhere).
 */
void
ubsec_reset_board(sc)
	struct ubsec_softc *sc;
{
    volatile u_int32_t ctrl;

    ctrl = READ_REG(sc, BS_CTRL);
    ctrl |= BS_CTRL_RESET;
    WRITE_REG(sc, BS_CTRL, ctrl);

    /*
     * Wait aprox. 30 PCI clocks = 900 ns = 0.9 us
     */
    DELAY(10);
}

/*
 * Init Broadcom registers
 */
void
   ubsec_init_board(sc)
   struct ubsec_softc *sc;
{
	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    ((sc->sc_flags & UBS_FLAGS_KEY) ? BS_CTRL_MCR2INT : 0));
}

/*
 * Init Broadcom PCI registers
 */
void
   ubsec_init_pciregs(pa)
   struct pci_attach_args *pa;
{
	pci_chipset_tag_t pc = pa->pa_pc;
	u_int32_t misc;

#if 0
	misc = pci_conf_read(pc, pa->pa_tag, BS_RTY_TOUT);
	misc = (misc & ~(UBS_PCI_RTY_MASK << UBS_PCI_RTY_SHIFT))
	    | ((UBS_DEF_RTY & 0xff) << UBS_PCI_RTY_SHIFT);
	misc = (misc & ~(UBS_PCI_TOUT_MASK << UBS_PCI_TOUT_SHIFT))
	    | ((UBS_DEF_TOUT & 0xff) << UBS_PCI_TOUT_SHIFT);
	pci_conf_write(pc, pa->pa_tag, BS_RTY_TOUT, misc);
#endif

	/*
	 * This will set the cache line size to 1, this will
	 * force the BCM58xx chip just to do burst read/writes.
	 * Cache line read/writes are to slow
	 */
	misc = pci_conf_read(pc, pa->pa_tag, PCI_BHLC_REG);
	misc = (misc & ~(PCI_CACHELINE_MASK << PCI_CACHELINE_SHIFT))
	    | ((UBS_DEF_CACHELINE & 0xff) << PCI_CACHELINE_SHIFT);
	pci_conf_write(pc, pa->pa_tag, PCI_BHLC_REG, misc);
}

/*
 * Clean up after a chip crash.
 * It is assumed that the caller in splnet()
 */
void
ubsec_cleanchip(sc)
	struct ubsec_softc *sc;
{
	struct ubsec_q *q;

	while (!SIMPLEQ_EMPTY(&sc->sc_qchip)) {
		q = SIMPLEQ_FIRST(&sc->sc_qchip);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q, q_next);
		ubsec_free_q(sc, q);
	}
}

/*
 * free a ubsec_q
 * It is assumed that the caller is within splnet()
 */
int
ubsec_free_q(struct ubsec_softc *sc, struct ubsec_q *q)
{
	struct ubsec_q *q2;
	struct cryptop *crp;
	int npkts;
	int i;

	npkts = q->q_nstacked_mcrs;

	for (i = 0; i < npkts; i++) {
		if(q->q_stacked_mcr[i]) {
			q2 = q->q_stacked_mcr[i];

			if ((q2->q_dst_m != NULL) && (q2->q_src_m != q2->q_dst_m)) 
				m_freem(q2->q_dst_m);

			crp = (struct cryptop *)q2->q_crp;
			
			SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q2, q_next);
			
			crp->crp_etype = EFAULT;
			crp->crp_callback(crp);
		} else {
			break;
		}
	}

	/*
	 * Free header MCR
	 */
	if ((q->q_dst_m != NULL) && (q->q_src_m != q->q_dst_m))
		m_freem(q->q_dst_m);

	crp = (struct cryptop *)q->q_crp;
	
	SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
	
	crp->crp_etype = EFAULT;
	crp->crp_callback(crp);
	return(0);
}

/*
 * Routine to reset the chip and clean up.
 * It is assumed that the caller is in splnet()
 */
void
ubsec_totalreset(sc)
	struct ubsec_softc *sc;
{
	ubsec_reset_board(sc);
	ubsec_init_board(sc);
	ubsec_cleanchip(sc);
}

int
ubsec_dmamap_aligned(map)
	bus_dmamap_t map;
{
	int i;

	for (i = 0; i < map->dm_nsegs; i++) {
		if (map->dm_segs[i].ds_addr & 3)
			return (0);
		if ((i != (map->dm_nsegs - 1)) &&
		    (map->dm_segs[i].ds_len & 3))
			return (0);
	}
	return (1);
@


1.76.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.76.2.1 2002/01/31 22:55:36 niklas Exp $	*/
a34 5
 *
 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
 *
d71 8
a78 8
int ubsec_probe(struct device *, void *, void *);
void ubsec_attach(struct device *, struct device *, void *);
void ubsec_reset_board(struct ubsec_softc *);
void ubsec_init_board(struct ubsec_softc *);
void ubsec_init_pciregs(struct pci_attach_args *pa);
void ubsec_cleanchip(struct ubsec_softc *);
void ubsec_totalreset(struct ubsec_softc *);
int  ubsec_free_q(struct ubsec_softc*, struct ubsec_q *);
d88 14
a101 28
int	ubsec_intr(void *);
int	ubsec_newsession(u_int32_t *, struct cryptoini *);
int	ubsec_freesession(u_int64_t);
int	ubsec_process(struct cryptop *);
void	ubsec_callback(struct ubsec_softc *, struct ubsec_q *);
int	ubsec_feed(struct ubsec_softc *);
void	ubsec_mcopy(struct mbuf *, struct mbuf *, int, int);
void	ubsec_callback2(struct ubsec_softc *, struct ubsec_q2 *);
int	ubsec_feed2(struct ubsec_softc *);
void	ubsec_rng(void *);
int	ubsec_dma_malloc(struct ubsec_softc *, bus_size_t,
    struct ubsec_dma_alloc *, int);
void	ubsec_dma_free(struct ubsec_softc *, struct ubsec_dma_alloc *);
int	ubsec_dmamap_aligned(bus_dmamap_t);

int	ubsec_kprocess(struct cryptkop *);
struct ubsec_softc *ubsec_kfind(struct cryptkop *);
int	ubsec_kprocess_modexp(struct ubsec_softc *, struct cryptkop *);
int	ubsec_kprocess_rsapriv(struct ubsec_softc *, struct cryptkop *);
void	ubsec_kfree(struct ubsec_softc *, struct ubsec_q2 *);
int	ubsec_ksigbits(struct crparam *);
void	ubsec_kshift_r(u_int, u_int8_t *, u_int, u_int8_t *, u_int);
void	ubsec_kshift_l(u_int, u_int8_t *, u_int, u_int8_t *, u_int);

/* DEBUG crap... */
void ubsec_dump_pb(struct ubsec_pktbuf *);
void ubsec_dump_mcr(struct ubsec_mcr *);
void ubsec_dump_ctx2(struct ubsec_ctx_keyop *);
d121 1
a121 1
	struct pci_attach_args *pa = (struct pci_attach_args *)aux;
a152 1
	SIMPLEQ_INIT(&sc->sc_q2free);
d155 5
a159 8

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601)
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG;
d164 1
a164 2
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY;
d220 1
a220 1
			printf(": can't allocate queue buffers\n");
d226 1
a226 1
			printf(": can't allocate dma buffers\n");
d259 1
a259 2
	printf(": %s", intrstr);

d261 1
a261 1
	if (sc->sc_flags & UBS_FLAGS_RNG) {
d293 1
a293 8
	if (sc->sc_flags & UBS_FLAGS_KEY) {
		sc->sc_statmask |= BS_STAT_MCR2_DONE;

		crypto_kregister(sc->sc_cid, CRK_MOD_EXP, 0, ubsec_kprocess);
		crypto_kregister(sc->sc_cid, CRK_MOD_EXP_CRT, 0, ubsec_kprocess);
	}

	printf("\n");
d356 1
d358 1
a358 1
	 * Check to see if we have any key setups/rng's waiting for us
d360 1
a360 2
	if ((sc->sc_flags & (UBS_FLAGS_KEY|UBS_FLAGS_RNG)) &&
	    (stat & BS_STAT_MCR2_DONE)) {
d389 1
d693 1
a693 1
	int card, err = 0, i, j, s, nicealign;
d1297 1
a1334 1
	struct cryptkop *krp;
d1341 1
a1341 2
	switch (q->q_type) {
#ifndef UBSEC_NO_RNG
a1355 62
#endif
	case UBS_CTXOP_MODEXP: {
		struct ubsec_q2_modexp *me = (struct ubsec_q2_modexp *)q;
		u_int rlen, clen;

		krp = me->me_krp;
		rlen = (me->me_modbits + 7) / 8;
		clen = (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits + 7) / 8;

		bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
		    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
		    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
		    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
		    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);

		if (clen < rlen)
			krp->krp_status = E2BIG;
		else
			ubsec_kshift_l(me->me_shiftbits,
			    me->me_C.dma_vaddr, me->me_modbits,
			    krp->krp_param[UBS_MODEXP_PAR_C].crp_p,
			    krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits);

		crypto_kdone(krp);

		/* bzero all potentially sensitive data */
		bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
		bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
		bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
		bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);

		/* Can't free here, so put us on the free list. */
		SIMPLEQ_INSERT_TAIL(&sc->sc_q2free, &me->me_q, q_next);
		break;
	}
	case UBS_CTXOP_RSAPRIV: {
		struct ubsec_q2_rsapriv *rp = (struct ubsec_q2_rsapriv *)q;
		u_int len;

		krp = rp->rpr_krp;
		bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map, 0,
		    rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgout.dma_map, 0,
		    rp->rpr_msgout.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);

		len = (krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_nbits + 7) / 8;
		bcopy(rp->rpr_msgout.dma_vaddr,
		    krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_p, len);

		crypto_kdone(krp);

		bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
		bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
		bzero(rp->rpr_q.q_ctx.dma_vaddr, rp->rpr_q.q_ctx.dma_size);

		/* Can't free here, so put us on the free list. */
		SIMPLEQ_INSERT_TAIL(&sc->sc_q2free, &rp->rpr_q, q_next);
		break;
	}
a1362 1
#ifndef UBSEC_NO_RNG
a1397 1
	rng->rng_q.q_type = UBS_CTXOP_RNGBYPASS;
d1402 1
a1402 1
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rng->rng_q, q_next);
d1494 2
a1495 2
ubsec_init_board(sc)
	struct ubsec_softc *sc;
d1506 2
a1507 2
ubsec_init_pciregs(pa)
	struct pci_attach_args *pa;
a1622 622
}

struct ubsec_softc *
ubsec_kfind(krp)
	struct cryptkop *krp;
{
	struct ubsec_softc *sc;
	int i;

	for (i = 0; i < ubsec_cd.cd_ndevs; i++) {
		sc = ubsec_cd.cd_devs[i];
		if (sc == NULL)
			continue;
		if (sc->sc_cid == krp->krp_hid)
			return (sc);
	}
	return (NULL);
}

void
ubsec_kfree(sc, q)
	struct ubsec_softc *sc;
	struct ubsec_q2 *q;
{
	switch (q->q_type) {
	case UBS_CTXOP_MODEXP: {
		struct ubsec_q2_modexp *me = (struct ubsec_q2_modexp *)q;

		ubsec_dma_free(sc, &me->me_q.q_mcr);
		ubsec_dma_free(sc, &me->me_q.q_ctx);
		ubsec_dma_free(sc, &me->me_M);
		ubsec_dma_free(sc, &me->me_E);
		ubsec_dma_free(sc, &me->me_C);
		ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF);
		break;
	}
	case UBS_CTXOP_RSAPRIV: {
		struct ubsec_q2_rsapriv *rp = (struct ubsec_q2_rsapriv *)q;

		ubsec_dma_free(sc, &rp->rpr_q.q_mcr);
		ubsec_dma_free(sc, &rp->rpr_q.q_ctx);
		ubsec_dma_free(sc, &rp->rpr_msgin);
		ubsec_dma_free(sc, &rp->rpr_msgout);
		free(rp, M_DEVBUF);
		break;
	}
	default:
		printf("%s: invalid kfree 0x%x\n", sc->sc_dv.dv_xname,
		    q->q_type);
		break;
	}
}

int
ubsec_kprocess(krp)
	struct cryptkop *krp;
{
	struct ubsec_softc *sc;

	if (krp == NULL || krp->krp_callback == NULL)
		return (EINVAL);
	if ((sc = ubsec_kfind(krp)) == NULL)
		return (EINVAL);

	while (!SIMPLEQ_EMPTY(&sc->sc_q2free)) {
		struct ubsec_q2 *q;

		q = SIMPLEQ_FIRST(&sc->sc_q2free);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_q2free, q, q_next);
		ubsec_kfree(sc, q);
	}

	switch (krp->krp_op) {
	case CRK_MOD_EXP:
		return (ubsec_kprocess_modexp(sc, krp));
	case CRK_MOD_EXP_CRT:
		return (ubsec_kprocess_rsapriv(sc, krp));
	default:
		printf("%s: kprocess: invalid op 0x%x\n",
		    sc->sc_dv.dv_xname, krp->krp_op);
		krp->krp_status = EOPNOTSUPP;
		crypto_kdone(krp);
		return (0);
	}
}

/*
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N]
 */
int
ubsec_kprocess_modexp(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
{
	struct ubsec_q2_modexp *me;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_modexp *ctx;
	struct ubsec_pktbuf *epb;
	int err = 0, s;
	u_int nbits, normbits, mbits, shiftbits, ebits;

	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
	if (me == NULL) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me, sizeof *me);
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;

	nbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_N]);
	if (nbits <= 512)
		normbits = 512;
	else if (nbits <= 768)
		normbits = 768;
	else if (nbits <= 1024)
		normbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 1536)
		normbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 2048)
		normbits = 2048;
	else {
		err = E2BIG;
		goto errout;
	}

	if (sc->sc_flags & UBS_FLAGS_HWNORM)
		shiftbits = 0;
	else
		shiftbits = normbits - nbits;

	me->me_modbits = normbits;
	me->me_shiftbits = shiftbits;

	/* Sanity check: result bits must be >= true modulus bits. */
	if (krp->krp_param[UBS_MODEXP_PAR_C].crp_nbits < nbits) {
		err = ERANGE;
		goto errout;
	}

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &me->me_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_modexp),
	    &me->me_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}

	mbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_M]);
	if (mbits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_M, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_M].crp_p, mbits,
	    me->me_M.dma_vaddr, normbits);

	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_E, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_E].crp_p, ebits,
	    me->me_E.dma_vaddr, normbits);

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktbuf),
	    &me->me_epb, 0)) {
		err = ENOMEM;
		goto errout;
	}
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
	epb->pb_addr = htole32(me->me_E.dma_paddr);
	epb->pb_next = 0;
	epb->pb_len = htole32(normbits / 8);

#ifdef UBSEC_DEBUG
	printf("Epb ");
	ubsec_dump_pb(epb);
#endif

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(me->me_q.q_ctx.dma_paddr);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = 0;

	mcr->mcr_ipktbuf.pb_addr = htole32(me->me_M.dma_paddr);
	mcr->mcr_ipktbuf.pb_len = htole32(normbits / 8);
	mcr->mcr_ipktbuf.pb_next = htole32(me->me_epb.dma_paddr);

	mcr->mcr_opktbuf.pb_addr = htole32(me->me_C.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(normbits / 8);

#ifdef DIAGNOSTIC
	/* Misaligned output buffer will hang the chip. */
	if ((letoh32(mcr->mcr_opktbuf.pb_addr) & 3) != 0)
		panic("%s: modexp invalid addr 0x%x\n",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_addr));
	if ((letoh32(mcr->mcr_opktbuf.pb_len) & 3) != 0)
		panic("%s: modexp invalid len 0x%x\n",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_len));
#endif

	ctx = (struct ubsec_ctx_modexp *)me->me_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof(*ctx));
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_N].crp_p, nbits,
	    ctx->me_N, normbits);
	ctx->me_len = htole16((normbits / 8) + (4 * sizeof(u_int16_t)));
	ctx->me_op = htole16(UBS_CTXOP_MODEXP);
	ctx->me_E_len = htole16(normbits - shiftbits);
	ctx->me_N_len = htole16(normbits - shiftbits);

#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
	ubsec_dump_ctx2((struct ubsec_ctx_keyop *)ctx);
#endif

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
	    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
	    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
	    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
	    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &me->me_q, q_next);
	ubsec_feed2(sc);
	splx(s);

	return (0);

errout:
	if (me != NULL) {
		if (me->me_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_q.q_mcr);
		if (me->me_q.q_ctx.dma_map != NULL) {
			bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
			ubsec_dma_free(sc, &me->me_q.q_ctx);
		}
		if (me->me_M.dma_map != NULL) {
			bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
			ubsec_dma_free(sc, &me->me_M);
		}
		if (me->me_E.dma_map != NULL) {
			bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
			ubsec_dma_free(sc, &me->me_E);
		}
		if (me->me_C.dma_map != NULL) {
			bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
			ubsec_dma_free(sc, &me->me_C);
		}
		if (me->me_epb.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

int
ubsec_kprocess_rsapriv(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
{
	struct ubsec_q2_rsapriv *rp = NULL;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_rsapriv *ctx;
	int err = 0, s;
	u_int padlen, msglen;

	msglen = ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_P]);
	padlen = ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_Q]);
	if (msglen > padlen)
		padlen = msglen;

	if (padlen <= 256)
		padlen = 256;
	else if (padlen <= 384)
		padlen = 384;
	else if (padlen <= 512)
		padlen = 512;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && padlen <= 768)
		padlen = 768;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && padlen <= 1024)
		padlen = 1024;
	else {
		err = E2BIG;
		printf("bad pad.\n");
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_DP]) > padlen) {
		err = E2BIG;
		printf("bad p\n");
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_DQ]) > padlen) {
		err = E2BIG;
		printf("bad q\n");
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_PINV]) > padlen) {
		err = E2BIG;
		printf("bad pinv\n");
		goto errout;
	}

	rp = (struct ubsec_q2_rsapriv *)malloc(sizeof *rp, M_DEVBUF, M_NOWAIT);
	if (rp == NULL)
		return (ENOMEM);
	bzero(rp, sizeof *rp);
	rp->rpr_krp = krp;
	rp->rpr_q.q_type = UBS_CTXOP_RSAPRIV;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &rp->rpr_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)rp->rpr_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rsapriv),
	    &rp->rpr_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ctx = (struct ubsec_ctx_rsapriv *)rp->rpr_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof *ctx);

	/* Copy in p */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_P].crp_p,
	    &ctx->rpr_buf[0 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_P].crp_nbits + 7) / 8);

	/* Copy in q */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_p,
	    &ctx->rpr_buf[1 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_nbits + 7) / 8);

	/* Copy in dp */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_p,
	    &ctx->rpr_buf[2 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_nbits + 7) / 8);

	/* Copy in dq */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_p,
	    &ctx->rpr_buf[3 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_nbits + 7) / 8);

	/* Copy in pinv */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_p,
	    &ctx->rpr_buf[4 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_nbits + 7) / 8);

	msglen = padlen * 2;

	/* Copy in input message (aligned buffer/length). */
	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_MSGIN]) > msglen) {
		/* Is this likely? */
		printf("msginbuf...\n");
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, (msglen + 7) / 8, &rp->rpr_msgin, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(rp->rpr_msgin.dma_vaddr, (msglen + 7) / 8);
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_p,
	    rp->rpr_msgin.dma_vaddr,
	    (krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_nbits + 7) / 8);

	/* Prepare space for output message (aligned buffer/length). */
	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT]) < msglen) {
		printf("msgoutbuf\n");
		/* Is this likely? */
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, (msglen + 7) / 8, &rp->rpr_msgout, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(rp->rpr_msgout.dma_vaddr, (msglen + 7) / 8);

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(rp->rpr_q.q_ctx.dma_paddr);
	mcr->mcr_ipktbuf.pb_addr = htole32(rp->rpr_msgin.dma_paddr);
	mcr->mcr_ipktbuf.pb_next = 0;
	mcr->mcr_ipktbuf.pb_len = htole32(rp->rpr_msgin.dma_size);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = htole16(msglen);
	mcr->mcr_opktbuf.pb_addr = htole32(rp->rpr_msgout.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(rp->rpr_msgout.dma_size);

#ifdef DIAGNOSTIC
	if (rp->rpr_msgin.dma_paddr & 3 || rp->rpr_msgin.dma_size & 3) {
		panic("%s: rsapriv: invalid msgin %p(0x%x)",
		    sc->sc_dv.dv_xname, rp->rpr_msgin.dma_paddr,
		    rp->rpr_msgin.dma_size);
	}
	if (rp->rpr_msgout.dma_paddr & 3 || rp->rpr_msgout.dma_size & 3) {
		panic("%s: rsapriv: invalid msgout %p(0x%x)",
		    sc->sc_dv.dv_xname, rp->rpr_msgout.dma_paddr,
		    rp->rpr_msgout.dma_size);
	}
#endif

	ctx->rpr_len = (sizeof(u_int16_t) * 4) + (5 * (padlen / 8));
	ctx->rpr_op = htole16(UBS_CTXOP_RSAPRIV);
	ctx->rpr_q_len = htole16(padlen);
	ctx->rpr_p_len = htole16(padlen);

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map,
	    0, rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgout.dma_map,
	    0, rp->rpr_msgout.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rp->rpr_q, q_next);
	ubsec_feed2(sc);
	splx(s);
	return (0);

errout:
	if (rp != NULL) {
		if (rp->rpr_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &rp->rpr_q.q_mcr);
		if (rp->rpr_msgin.dma_map != NULL) {
			bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
			ubsec_dma_free(sc, &rp->rpr_msgin);
		}
		if (rp->rpr_msgout.dma_map != NULL) {
			bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
			ubsec_dma_free(sc, &rp->rpr_msgout);
		}
		free(rp, M_DEVBUF);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

void
ubsec_dump_pb(struct ubsec_pktbuf *pb)
{
	printf("addr 0x%x (0x%x) next 0x%x\n",
	    pb->pb_addr, pb->pb_len, pb->pb_next);
}

void
ubsec_dump_ctx2(struct ubsec_ctx_keyop *c)
{
	printf("CTX (0x%x):\n", c->ctx_len);
	switch (letoh16(c->ctx_op)) {
	case UBS_CTXOP_RNGBYPASS:
	case UBS_CTXOP_RNGSHA1:
		break;
	case UBS_CTXOP_MODEXP:
	{
		struct ubsec_ctx_modexp *cx = (void *)c;
		int i, len;

		printf(" Elen %u, Nlen %u\n",
		    letoh16(cx->me_E_len), letoh16(cx->me_N_len));
		len = (cx->me_N_len + 7)/8;
		for (i = 0; i < len; i++)
			printf("%s%02x", (i == 0) ? " N: " : ":", cx->me_N[i]);
		printf("\n");
		break;
	}
	default:
		printf("unknown context: %x\n", c->ctx_op);
	}
	printf("END CTX\n");
}

void
ubsec_dump_mcr(struct ubsec_mcr *mcr)
{
	struct ubsec_mcr_add *ma;
	int i;

	printf("MCR:\n");
	printf(" pkts: %u, flags 0x%x\n",
	    letoh16(mcr->mcr_pkts), letoh16(mcr->mcr_flags));
	ma = (struct ubsec_mcr_add *)&mcr->mcr_cmdctxp;
	for (i = 0; i < letoh16(mcr->mcr_pkts); i++) {
		printf(" %d: ctx 0x%x len 0x%x rsvd 0x%x\n", i,
		    letoh32(ma->mcr_cmdctxp), letoh16(ma->mcr_pktlen),
		    letoh16(ma->mcr_reserved));
		printf(" %d: ipkt ", i);
		ubsec_dump_pb(&ma->mcr_ipktbuf);
		printf(" %d: opkt ", i);
		ubsec_dump_pb(&ma->mcr_opktbuf);
		ma++;
	}
	printf("END MCR\n");
}

/*
 * Return the number of significant bits of a big number.
 */
int
ubsec_ksigbits(cr)
	struct crparam *cr;
{
	u_int plen = (cr->crp_nbits + 7) / 8;
	int i, sig = plen * 8;
	u_int8_t c, *p = cr->crp_p;

	for (i = plen - 1; i >= 0; i--) {
		c = p[i];
		if (c != 0) {
			while ((c & 0x80) == 0) {
				sig--;
				c <<= 1;
			}
			break;
		}
		sig -= 8;
	}
	return (sig);
}

void
ubsec_kshift_r(shiftbits, src, srcbits, dst, dstbits)
	u_int shiftbits, srcbits, dstbits;
	u_int8_t *src, *dst;
{
	u_int slen, dlen;
	int i, si, di, n;

	slen = (srcbits + 7) / 8;
	dlen = (dstbits + 7) / 8;

	for (i = 0; i < slen; i++)
		dst[i] = src[i];
	for (i = 0; i < dlen - slen; i++)
		dst[slen + i] = 0;

	n = shiftbits / 8;
	if (n != 0) {
		si = dlen - n - 1;
		di = dlen - 1;
		while (si >= 0)
			dst[di--] = dst[si--];
		while (di >= 0)
			dst[di--] = 0;
	}

	n = shiftbits % 8;
	if (n != 0) {
		for (i = dlen - 1; i > 0; i--)
			dst[i] = (dst[i] << n) |
			    (dst[i - 1] >> (8 - n));
		dst[0] = dst[0] << n;
	}
}

void
ubsec_kshift_l(shiftbits, src, srcbits, dst, dstbits)
	u_int shiftbits, srcbits, dstbits;
	u_int8_t *src, *dst;
{
	int slen, dlen, i, n;

	slen = (srcbits + 7) / 8;
	dlen = (dstbits + 7) / 8;

	n = shiftbits / 8;
	for (i = 0; i < slen; i++)
		dst[i] = src[i + n];
	for (i = 0; i < dlen - slen; i++)
		dst[slen + i] = 0;

	n = shiftbits % 8;
	if (n != 0) {
		for (i = 0; i < (dlen - 1); i++)
			dst[i] = (dst[i] >> n) | (dst[i + 1] << (8 - n));
		dst[dlen - 1] = dst[dlen - 1] >> n;
	}
@


1.76.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.76.2.2 2002/06/11 03:42:27 art Exp $	*/
d110 1
a110 2
int	ubsec_kprocess_modexp_sw(struct ubsec_softc *, struct cryptkop *);
int	ubsec_kprocess_modexp_hw(struct ubsec_softc *, struct cryptkop *);
d147 1
a147 3
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5801 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5802 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805 ||
d149 1
a149 5
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822))
		return (1);
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_SUN &&
	    PCI_PRODUCT(pa->pa_id) ==  PCI_PRODUCT_SUN_SCA1K)
d181 1
a181 2
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5802 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805))
d186 1
a186 1
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822))
a189 10
	if ((PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821) ||
	    (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_SUN &&
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_SCA1K)) {
		sc->sc_statmask |= BS_STAT_MCR1_ALLEMPTY |
		    BS_STAT_MCR2_ALLEMPTY;
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY;
	}

a321 1
#if 0
a322 1
#endif
d708 1
a708 1
	u_int32_t sid = ((u_int32_t)tid) & 0xffffffff;
d1220 1
a1220 1
	crypto_done(crp);
d1395 1
a1395 1
		clen = (krp->krp_param[krp->krp_iparams].crp_nbits + 7) / 8;
d1408 6
a1413 14
		else {
			if (sc->sc_flags & UBS_FLAGS_HWNORM) {
				bzero(krp->krp_param[krp->krp_iparams].crp_p,
				    (krp->krp_param[krp->krp_iparams].crp_nbits
					+ 7) / 8);
				bcopy(me->me_C.dma_vaddr,
				    krp->krp_param[krp->krp_iparams].crp_p,
				    (me->me_modbits + 7) / 8);
			} else
				ubsec_kshift_l(me->me_shiftbits,
				    me->me_C.dma_vaddr, me->me_normbits,
				    krp->krp_param[krp->krp_iparams].crp_p,
				    krp->krp_param[krp->krp_iparams].crp_nbits);
		}
d1593 3
a1595 15
	u_int32_t ctrl;

	ctrl = READ_REG(sc, BS_CTRL);
	ctrl &= ~(BS_CTRL_BE32 | BS_CTRL_BE64);
	ctrl |= BS_CTRL_LITTLE_ENDIAN | BS_CTRL_MCR1INT;

	if (sc->sc_flags & UBS_FLAGS_KEY)
		ctrl |= BS_CTRL_MCR2INT;
	else
		ctrl &= ~BS_CTRL_MCR2INT;

	if (sc->sc_flags & UBS_FLAGS_HWNORM)
		ctrl &= ~BS_CTRL_SWNORM;

	WRITE_REG(sc, BS_CTRL, ctrl);
d1608 9
d1671 1
a1671 1
			crypto_done(crp);
d1688 1
a1688 1
	crypto_done(crp);
a1777 1
	int r;
d1794 1
a1794 5
		if (sc->sc_flags & UBS_FLAGS_HWNORM)
			r = ubsec_kprocess_modexp_hw(sc, krp);
		else
			r = ubsec_kprocess_modexp_sw(sc, krp);
		break;
d1796 1
a1796 2
		r = ubsec_kprocess_rsapriv(sc, krp);
		break;
d1802 1
a1802 1
		r = 0;
a1803 1
	return (r);
d1807 1
a1807 1
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N] (sw normalization)
d1810 1
a1810 1
ubsec_kprocess_modexp_sw(sc, krp)
d1846 4
a1849 1
	shiftbits = normbits - nbits;
d1851 1
a1851 1
	me->me_modbits = nbits;
a1852 1
	me->me_normbits = normbits;
d1855 1
a1855 1
	if (krp->krp_param[krp->krp_iparams].crp_nbits < nbits) {
d1937 1
a1937 1
		panic("%s: modexp invalid addr 0x%x",
d1940 1
a1940 1
		panic("%s: modexp invalid len 0x%x",
d1951 2
a1952 203
	ctx->me_E_len = htole16(nbits);
	ctx->me_N_len = htole16(nbits);

#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
	ubsec_dump_ctx2((struct ubsec_ctx_keyop *)ctx);
#endif

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
	    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
	    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
	    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
	    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &me->me_q, q_next);
	ubsec_feed2(sc);
	splx(s);

	return (0);

errout:
	if (me != NULL) {
		if (me->me_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_q.q_mcr);
		if (me->me_q.q_ctx.dma_map != NULL) {
			bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
			ubsec_dma_free(sc, &me->me_q.q_ctx);
		}
		if (me->me_M.dma_map != NULL) {
			bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
			ubsec_dma_free(sc, &me->me_M);
		}
		if (me->me_E.dma_map != NULL) {
			bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
			ubsec_dma_free(sc, &me->me_E);
		}
		if (me->me_C.dma_map != NULL) {
			bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
			ubsec_dma_free(sc, &me->me_C);
		}
		if (me->me_epb.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

/*
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N] (hw normalization)
 */
int
ubsec_kprocess_modexp_hw(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
{
	struct ubsec_q2_modexp *me;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_modexp *ctx;
	struct ubsec_pktbuf *epb;
	int err = 0, s;
	u_int nbits, normbits, mbits, shiftbits, ebits;

	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
	if (me == NULL) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me, sizeof *me);
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;

	nbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_N]);
	if (nbits <= 512)
		normbits = 512;
	else if (nbits <= 768)
		normbits = 768;
	else if (nbits <= 1024)
		normbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 1536)
		normbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 2048)
		normbits = 2048;
	else {
		err = E2BIG;
		goto errout;
	}

	shiftbits = normbits - nbits;

	/* XXX ??? */
	me->me_modbits = nbits;
	me->me_shiftbits = shiftbits;
	me->me_normbits = normbits;

	/* Sanity check: result bits must be >= true modulus bits. */
	if (krp->krp_param[krp->krp_iparams].crp_nbits < nbits) {
		err = ERANGE;
		goto errout;
	}

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &me->me_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_modexp),
	    &me->me_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}

	mbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_M]);
	if (mbits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_M, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_M.dma_vaddr, normbits / 8);
	bcopy(krp->krp_param[UBS_MODEXP_PAR_M].crp_p,
	    me->me_M.dma_vaddr, (mbits + 7) / 8);

	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_E, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_E.dma_vaddr, normbits / 8);
	bcopy(krp->krp_param[UBS_MODEXP_PAR_E].crp_p,
	    me->me_E.dma_vaddr, (ebits + 7) / 8);

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktbuf),
	    &me->me_epb, 0)) {
		err = ENOMEM;
		goto errout;
	}
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
	epb->pb_addr = htole32(me->me_E.dma_paddr);
	epb->pb_next = 0;
	epb->pb_len = htole32((ebits + 7) / 8);

#ifdef UBSEC_DEBUG
	printf("Epb ");
	ubsec_dump_pb(epb);
#endif

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(me->me_q.q_ctx.dma_paddr);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = 0;

	mcr->mcr_ipktbuf.pb_addr = htole32(me->me_M.dma_paddr);
	mcr->mcr_ipktbuf.pb_len = htole32(normbits / 8);
	mcr->mcr_ipktbuf.pb_next = htole32(me->me_epb.dma_paddr);

	mcr->mcr_opktbuf.pb_addr = htole32(me->me_C.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(normbits / 8);

#ifdef DIAGNOSTIC
	/* Misaligned output buffer will hang the chip. */
	if ((letoh32(mcr->mcr_opktbuf.pb_addr) & 3) != 0)
		panic("%s: modexp invalid addr 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_addr));
	if ((letoh32(mcr->mcr_opktbuf.pb_len) & 3) != 0)
		panic("%s: modexp invalid len 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_len));
#endif

	ctx = (struct ubsec_ctx_modexp *)me->me_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof(*ctx));
	bcopy(krp->krp_param[UBS_MODEXP_PAR_N].crp_p, ctx->me_N,
	    (nbits + 7) / 8);
	ctx->me_len = htole16((normbits / 8) + (4 * sizeof(u_int16_t)));
	ctx->me_op = htole16(UBS_CTXOP_MODEXP);
	ctx->me_E_len = htole16(ebits);
	ctx->me_N_len = htole16(nbits);
d2037 1
d2043 1
d2049 1
d2055 1
d2111 1
d2126 1
@


1.76.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d98 1
a98 1
void	ubsec_feed(struct ubsec_softc *);
d101 1
a101 1
void	ubsec_feed2(struct ubsec_softc *);
a134 13
const struct pci_matchid ubsec_devices[] = {
	{ PCI_VENDOR_BLUESTEEL, PCI_PRODUCT_BLUESTEEL_5501 },
	{ PCI_VENDOR_BLUESTEEL, PCI_PRODUCT_BLUESTEEL_5601 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5801 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5802 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5805 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5820 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5821 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5822 },
	{ PCI_VENDOR_SUN, PCI_PRODUCT_SUN_SCA1K },
	{ PCI_VENDOR_SUN, PCI_PRODUCT_SUN_5821 },
};

d141 18
a158 2
	return (pci_matchbyid((struct pci_attach_args *)aux, ubsec_devices,
	    sizeof(ubsec_devices)/sizeof(ubsec_devices[0])));
a173 2
	int algs[CRYPTO_ALGORITHM_MAX + 1];
	int kalgs[CRK_ALGORITHM_MAX + 1];
d201 1
a201 2
	     (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_SCA1K ||
	      PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_5821))) {
d280 5
a284 7
	bzero(algs, sizeof(algs));
	algs[CRYPTO_3DES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_DES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_MD5_HMAC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_SHA1_HMAC] = CRYPTO_ALG_FLAG_SUPPORTED;
	crypto_register(sc->sc_cid, algs, ubsec_newsession,
	    ubsec_freesession, ubsec_process);
d339 1
a339 2
		bzero(kalgs, sizeof(kalgs));
		kalgs[CRK_MOD_EXP] = CRYPTO_ALG_FLAG_SUPPORTED;
d341 1
a341 1
		kalgs[CRK_MOD_EXP_CRT] = CRYPTO_ALG_FLAG_SUPPORTED;
a342 2

		crypto_kregister(sc->sc_cid, kalgs, ubsec_kprocess);
d389 1
a389 1
				if(q->q_stacked_mcr[i])
d391 2
a392 1
				else
d394 1
d397 1
d464 1
a464 1
void
d487 1
a487 1
		return;
d534 1
a534 1
	return;
d560 1
a560 2
		printf("feed: q->chip %p %08x\n", q,
		    (u_int32_t)q->q_dma->d_alloc.dma_paddr);
d566 1
d1030 3
a1032 3
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_addr,
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_len,
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_next);
a1252 3
	ubsecstats.hst_opackets++;
	ubsecstats.hst_obytes += dmap->d_alloc.dma_size;

d1271 1
d1351 1
a1351 1
void
d1374 1
a1393 1
	case UBS_CTXOP_RNGSHA1:
d1520 2
a1521 2
	ctx->rbp_op = htole16(UBS_CTXOP_RNGSHA1);
	rng->rng_q.q_type = UBS_CTXOP_RNGSHA1;
@


1.75
log
@Match bcom 5821 (this is untested and is based on the datasheet's claim that
the 5821 is "register and software compatible with Broadcom 5820").
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.74 2001/11/14 00:39:46 jason Exp $	*/
a173 5

	/* Disable TRDY timeout and RETRY timeout */
	cmd = pci_conf_read(pc, pa->pa_tag, BS_TRDY_TIMEOUT);
	cmd &= 0xffff0000;
	pci_conf_write(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG, cmd);
@


1.74
log
@- Move rng buffer allocation (bus_dmamem_map/unmap) to attach instead of during
the first timeout which is the wrong time to be calling bus_dmamem_map/unmap
- Make sure that mastering is really enabled.
- remove some debugging stuff that would be a pain to get working on sparc64.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.73 2001/11/09 03:11:38 deraadt Exp $	*/
d116 2
a117 1
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820))
d149 2
a150 1
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820)
d174 5
@


1.73
log
@be way more sure that software cannot be used
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.72 2001/11/06 19:53:19 miod Exp $	*/
a36 1

d161 5
d215 18
d240 1
a421 3
#ifdef UBSEC_DEBUG
		printf("feed: q->chip %08x %08x\n", q, (u_int32_t)vtophys(q->q_mcr));
#endif
a881 6
#ifdef UBSEC_DEBUG
	printf("  buf[%x]: %d@@%x -> %x\n", vtophys(q->q_mcr),
	    q->q_mcr->mcr_ipktbuf.pb_len,
	    q->q_mcr->mcr_ipktbuf.pb_addr,
	    q->q_mcr->mcr_ipktbuf.pb_next);
#endif
a1019 7
#ifdef UBSEC_DEBUG
		printf("  buf[%d, %x]: %d@@%x -> %x\n", 0,
		    vtophys(q->q_mcr),
		    q->q_mcr->mcr_opktbuf.pb_len,
		    q->q_mcr->mcr_opktbuf.pb_addr,
		    q->q_mcr->mcr_opktbuf.pb_next);
#endif
a1256 17

	if (rng->rng_q.q_mcr.dma_map == NULL) {
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
		    &rng->rng_q.q_mcr, 0))
			goto out;
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rngbypass),
		    &rng->rng_q.q_ctx, 0)) {
			ubsec_dma_free(sc, &rng->rng_q.q_mcr);
			goto out;
		}
		if (ubsec_dma_malloc(sc, sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ,
		    &rng->rng_buf, 0)) {
			ubsec_dma_free(sc, &rng->rng_q.q_ctx);
			ubsec_dma_free(sc, &rng->rng_q.q_mcr);
			goto out;
		}
	}
@


1.72
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.71 2001/11/05 17:25:58 art Exp $	*/
d186 1
a186 1
	sc->sc_cid = crypto_get_driverid();
@


1.71
log
@Switch everything to the new bus_dmamap_sync API.
Most work by Wilbern Cobb <vedge@@csoft.org> with some fixes from me, mickey@@
and drahn@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.70 2001/08/27 22:02:37 jason Exp $	*/
d52 1
a52 1
#include <vm/vm.h>
@


1.70
log
@reverse read/write directions in bus_dmamap_sync calls since I misunderstood the API
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.69 2001/08/26 03:29:54 jason Exp $	*/
a102 8
#ifdef __HAS_NEW_BUS_DMAMAP_SYNC
#define ubsec_bus_dmamap_sync(t, m, o, l, f) \
    bus_dmamap_sync((t), (m), (o), (l), (f))
#else
#define ubsec_bus_dmamap_sync(t, m, o, l, f) \
    bus_dmamap_sync((t), (m), (f))
#endif

d298 1
a298 1
			ubsec_bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
d304 1
a304 1
				ubsec_bus_dmamap_sync(sc->sc_dmat,
d1041 1
a1041 1
	ubsec_bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
d1079 1
a1079 1
	ubsec_bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
d1184 1
a1184 1
		ubsec_bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map, 0,
d1187 1
a1187 1
		ubsec_bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
d1207 1
a1207 1
	ubsec_bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
d1216 1
a1216 1
		ubsec_bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
d1285 1
a1285 1
	ubsec_bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
@


1.70.2.1
log
@Pull in patch from current:
Fix (deraadt):
be way more sure that software cannot be used
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.70 2001/08/27 22:02:37 jason Exp $	*/
d194 1
a194 1
	sc->sc_cid = crypto_get_driverid(0);
@


1.69
log
@deal with 5 arg form of bus_dmamap_sync() if available
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.68 2001/08/25 10:13:30 art Exp $	*/
d1197 1
a1197 1
		    BUS_DMASYNC_PREREAD);
d1216 1
a1216 1
	    q->q_ctx.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
d1225 1
a1225 1
		    rng->rng_buf.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
d1294 1
a1294 1
	    rng->rng_buf.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
@


1.68
log
@Change pci_intr_map to take pci_attach_args as an argument.
All callers actually took all arguments to pci_intr_map from pci_attach_args
structs, so this simplifies code.
This also allows more complicated interrupt assignment schemes like the one
on sparc64.

This makes sparc64 pci interrupts work.

Inspired by the same change in NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.67 2001/08/12 20:03:49 mickey Exp $	*/
d103 8
d306 3
a308 2
			bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
			    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
d312 4
a315 2
				bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
				    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d1049 3
a1051 2
	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map,
	    BUS_DMASYNC_PREREAD);
d1087 3
a1089 2
	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map,
	    BUS_DMASYNC_POSTREAD);
d1192 2
a1193 1
		bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map,
d1195 2
a1196 1
		bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map,
d1215 2
a1216 1
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, BUS_DMASYNC_POSTREAD);
d1224 2
a1225 2
		bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map,
		    BUS_DMASYNC_POSTWRITE);
d1293 2
a1294 2
	bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map,
	    BUS_DMASYNC_PREWRITE);
@


1.67
log
@remove redundant vm includes
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.66 2001/07/04 06:03:55 jason Exp $	*/
d169 1
a169 2
	if (pci_intr_map(pc, pa->pa_intrtag, pa->pa_intrpin,
	    pa->pa_intrline, &ih)) {
@


1.66
log
@Add tests for segment lengths and total lengths bigger than the chip can handle.
Also, add a missing test for *2pages failure on destination buffers.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.65 2001/07/02 04:34:47 jason Exp $	*/
a48 4
#include <vm/vm.h>
#include <vm/vm_extern.h>
#include <vm/pmap.h>
#include <machine/pmap.h>
d51 2
@


1.65
log
@- More vtophys death: packet buffer lists are is bus_dma(9) memory as is
the mac buffer
- also, reenable aggregation code (accidentally removed)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.64 2001/06/29 21:52:41 jason Exp $	*/
d806 5
d833 5
d943 9
d970 5
@


1.64
log
@move definitions around a bit and define a few more constants
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.63 2001/06/29 16:19:15 jason Exp $	*/
d199 2
a200 2
		if (ubsec_dma_malloc(sc, MAX(sizeof(struct ubsec_pktctx_long),
		    sizeof(struct ubsec_pktctx)), &dmap->d_ctx, 0))
d202 1
a342 1
	goto feed1;
d831 1
a831 1
			pb = &q->q_srcpkt[j - 1];
a832 3
#ifdef UBSEC_DEBUG
		printf("  pb v %08x p %08x\n", pb, vtophys(pb));
#endif
d848 2
a849 1
			pb->pb_next = vtophys(&q->q_srcpkt[j]);
a856 6
	for (i = 0; i < j - 1; i++) {
		printf("  buf[%x]: %d@@%x -> %x\n", vtophys(&q->q_srcpkt[i]),
		    q->q_srcpkt[i].pb_len,
		    q->q_srcpkt[i].pb_addr,
		    q->q_srcpkt[i].pb_next);
	}
d862 2
a863 2
		q->q_mcr->mcr_opktbuf.pb_next =
		    (u_int32_t)vtophys(q->q_macbuf);
d956 1
a956 1
				pb = &q->q_dstpkt[j - 1];
d973 2
a974 1
					pb->pb_next = vtophys(q->q_macbuf);
d978 2
a979 1
				pb->pb_next = vtophys(&q->q_dstpkt[j]);
a987 7
		for (i = 0; i < j - 1; i++) {
			printf("  buf[%d, %x]: %d@@%x -> %x\n", i+1,
			    vtophys(&q->q_dstpkt[i]),
			    q->q_dstpkt[i].pb_len,
			    q->q_dstpkt[i].pb_addr,
			    q->q_dstpkt[i].pb_next);
		}
d991 2
a992 1
	q->q_mcr->mcr_cmdctxp = dmap->d_ctx.dma_paddr;
d997 2
a998 1
		ctxl = (struct ubsec_pktctx_long *)dmap->d_ctx.dma_vaddr;
d1014 2
a1015 1
		bcopy(&ctx, dmap->d_ctx.dma_vaddr,
d1017 2
a1018 1
	bus_dmamap_sync(sc->sc_dmat, dmap->d_ctx.dma_map, BUS_DMASYNC_PREREAD);
d1054 2
a1055 2
	bus_dmamap_sync(sc->sc_dmat, dmap->d_ctx.dma_map, BUS_DMASYNC_POSTREAD);
	SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);
d1087 2
a1088 1
			    crd->crd_inject, 12, (caddr_t)q->q_macbuf);
d1090 2
a1091 1
			bcopy((caddr_t)q->q_macbuf, crp->crp_mac, 12);
d1094 2
@


1.63
log
@allocate contexts during attach since we can't mess with them during
interrupts (the space allocation is wasteful, but more data will be
moved into the allocation soon).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.62 2001/06/23 22:02:55 angelos Exp $	*/
d338 2
a339 2
	if (npkts > 5)
		npkts = 5;
d798 1
a798 1
		    q->q_src_packl, MAX_SCATTER, &nicealign);
d801 1
a801 1
		    q->q_src_packp, q->q_src_packl, MAX_SCATTER, &nicealign);
d936 1
a936 1
			    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
d939 1
a939 1
			    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
a965 3
#ifdef UBSEC_DEBUG
			printf("  pb v %08x p %08x\n", pb, vtophys(pb));
#endif
@


1.62
log
@Correctly handle the IV_PRESENT flag.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.61 2001/06/23 20:59:42 angelos Exp $	*/
d134 1
d136 1
a136 1
	u_int32_t cmd;
d196 9
d592 1
d609 7
d628 1
d1007 1
a1007 6
	if (ubsec_dma_malloc(sc, MAX(sizeof(struct ubsec_pktctx_long),
	    sizeof(struct ubsec_pktctx)), &q->q_ctx_dma, 0)) {
		err = ENOMEM;
		goto errout;
	}	
	q->q_mcr->mcr_cmdctxp = q->q_ctx_dma.dma_paddr;
d1012 1
a1012 1
		ctxl = (struct ubsec_pktctx_long *)q->q_ctx_dma.dma_vaddr;
d1028 3
a1030 2
		bcopy(&ctx, q->q_ctx_dma.dma_vaddr, sizeof(struct ubsec_pktctx));
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx_dma.dma_map, BUS_DMASYNC_PREREAD);
d1043 5
a1047 2
		if (q->q_ctx_dma.dma_map != NULL)
			ubsec_dma_free(sc, &q->q_ctx_dma);
d1064 1
d1066 2
a1067 2
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx_dma.dma_map, BUS_DMASYNC_POSTREAD);
	ubsec_dma_free(sc, &q->q_ctx_dma);
@


1.61
log
@Conform to new prototype for crypto_register()
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.60 2001/06/23 18:30:37 deraadt Exp $	*/
d620 1
a620 1
		goto errout;	/* XXX only handle mbufs right now */
d695 6
a700 1
					m_copyback(q->q_src_m, enccrd->crd_inject,
a701 7
				else if (crp->crp_flags & CRYPTO_F_IOV) {
					if (crp->crp_iv == NULL)
						bzero((caddr_t)ctx.pc_iv, 8);
					else
						bcopy(crp->crp_iv,
						    (caddr_t)ctx.pc_iv, 8);
				}
d711 4
a714 7
			else if (crp->crp_flags & CRYPTO_F_IOV) {
				if (crp->crp_iv == NULL)
					bzero((caddr_t)ctx.pc_iv, 8);
				else
					bcopy(crp->crp_iv,
					    (caddr_t)ctx.pc_iv, 8);
			}
@


1.60
log
@merge crypto/crypto{dev,}.h to crypto/cryptodev.h, to avoid name conflicts inside OpenSSL codebase
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.59 2001/06/18 10:23:45 deraadt Exp $	*/
d195 1
a195 1
	crypto_register(sc->sc_cid, CRYPTO_3DES_CBC,
d197 3
a199 3
	crypto_register(sc->sc_cid, CRYPTO_DES_CBC, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_MD5_HMAC, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_SHA1_HMAC, NULL, NULL, NULL);
@


1.59
log
@kill a debug message that makes UBSEC_DEBUG useless
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.58 2001/06/14 23:56:54 deraadt Exp $	*/
d56 1
a56 1
#include <crypto/crypto.h>
@


1.58
log
@OOPS
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.57 2001/06/14 23:55:02 deraadt Exp $	*/
a237 3
#ifdef UBSEC_DEBUG
	printf("ubsec intr %x\n", stat);
#endif
@


1.57
log
@hackish auto-IV mode for IOV operations
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.56 2001/06/12 15:40:33 niklas Exp $	*/
d706 1
@


1.56
log
@Make pci_mapreg_map take an extra argument where we can
put a size limitation of the PCI region to map.  That makes the PERC 3/Di
raid controller usable, as it publishes too much PCI memory for us to map
in the kernel virtual memory.  As we only access the first 256 byte it is
of no use to map 128MB of kvm.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.55 2001/06/08 01:59:32 jason Exp $	*/
d701 5
a705 7
					if (crp->crp_iv == NULL) {
						err = EINVAL;
						goto errout;
					}
					bcopy(crp->crp_iv,
					    (caddr_t)ctx.pc_iv, 8);
				}
d716 5
a720 5
				if (crp->crp_iv == NULL) {
					err = EINVAL;
					goto errout;
				}
				bcopy(crp->crp_iv, (caddr_t)ctx.pc_iv, 8);
@


1.55
log
@Put back bus_dmaification of context's (seems to have been fixed by
recent changes to cryptosoft.c).  So, mcr and pktbuf handling to go...
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.54 2001/05/30 02:26:14 jason Exp $	*/
d164 1
a164 1
	    &sc->sc_st, &sc->sc_sh, NULL, &iosize)) {
@


1.54
log
@Un-bus_dma(9) mcr1 context and mcr operations (causes problems under load).
Reimplement mcr2 bus_dma handling.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.53 2001/05/23 14:42:52 jason Exp $	*/
d584 1
d610 1
d684 1
a684 1
		q->q_ctx.pc_flags |= UBS_PKTCTX_ENC_3DES;
d690 1
a690 1
				bcopy(enccrd->crd_iv, q->q_ctx.pc_iv, 8);
d692 2
a693 2
				q->q_ctx.pc_iv[0] = ses->ses_iv[0];
				q->q_ctx.pc_iv[1] = ses->ses_iv[1];
d699 1
a699 1
					    8, (caddr_t)q->q_ctx.pc_iv);
d706 1
a706 1
					    (caddr_t)q->q_ctx.pc_iv, 8);
d710 1
a710 1
			q->q_ctx.pc_flags |= UBS_PKTCTX_INBOUND;
d713 1
a713 1
				bcopy(enccrd->crd_iv, q->q_ctx.pc_iv, 8);
d716 1
a716 1
				    8, (caddr_t)q->q_ctx.pc_iv);
d722 1
a722 1
				bcopy(crp->crp_iv, (caddr_t)q->q_ctx.pc_iv, 8);
d726 8
a733 8
		q->q_ctx.pc_deskey[0] = ses->ses_deskey[0];
		q->q_ctx.pc_deskey[1] = ses->ses_deskey[1];
		q->q_ctx.pc_deskey[2] = ses->ses_deskey[2];
		q->q_ctx.pc_deskey[3] = ses->ses_deskey[3];
		q->q_ctx.pc_deskey[4] = ses->ses_deskey[4];
		q->q_ctx.pc_deskey[5] = ses->ses_deskey[5];
		SWAP32(q->q_ctx.pc_iv[0]);
		SWAP32(q->q_ctx.pc_iv[1]);
d740 1
a740 1
			q->q_ctx.pc_flags |= UBS_PKTCTX_AUTH_MD5;
d742 1
a742 1
			q->q_ctx.pc_flags |= UBS_PKTCTX_AUTH_SHA1;
d745 2
a746 2
			q->q_ctx.pc_hminner[i] = ses->ses_hminner[i];
			q->q_ctx.pc_hmouter[i] = ses->ses_hmouter[i];
d784 1
a784 1
	q->q_ctx.pc_offset = coffset >> 2;
d997 7
d1005 3
a1007 1
		q->q_mcr->mcr_cmdctxp = vtophys(&q->q_ctxl);
d1010 4
a1013 4
		q->q_ctxl.pc_len = sizeof(struct ubsec_pktctx_long);
		q->q_ctxl.pc_type = UBS_PKTCTX_TYPE_IPSEC;
		q->q_ctxl.pc_flags = q->q_ctx.pc_flags;
		q->q_ctxl.pc_offset = q->q_ctx.pc_offset;
d1015 1
a1015 1
			q->q_ctxl.pc_deskey[i] = q->q_ctx.pc_deskey[i];
d1017 1
a1017 1
			q->q_ctxl.pc_hminner[i] = q->q_ctx.pc_hminner[i];
d1019 3
a1021 3
			q->q_ctxl.pc_hmouter[i] = q->q_ctx.pc_hmouter[i];   
		q->q_ctxl.pc_iv[0] = q->q_ctx.pc_iv[0];
		q->q_ctxl.pc_iv[1] = q->q_ctx.pc_iv[1];
d1023 2
a1024 1
		q->q_mcr->mcr_cmdctxp = vtophys(&q->q_ctx);
d1037 2
d1055 3
@


1.53
log
@- relax alignment constraints
- make sure dma_map is set to NULL on failure
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.52 2001/05/23 04:46:41 jason Exp $	*/
d87 1
a87 1
void	ubsec_callback __P((struct ubsec_q *));
a92 1
void	ubsec_dma_free __P((struct ubsec_softc *, struct ubsec_dma_alloc *));
d95 1
a104 4
#ifdef __alpha__
#define	vtophys(va)	alpha_XXX_dmamap((vm_offset_t)(va))
#endif

d203 2
a204 1
		if (hz > 100)
a207 2

		timeout_set(&sc->sc_rngto, ubsec_rng, sc);
a227 1
	struct ubsec_dma_alloc mcr_dma;
a243 8

			bus_dmamap_sync(sc->sc_dmat, q->q_mcr_dma.dma_map,
			    BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE);
			if ((q->q_mcr->mcr_flags & UBS_MCR_DONE) == 0) {
				bus_dmamap_sync(sc->sc_dmat, q->q_mcr_dma.dma_map,
				    BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
				break;
			}
d248 2
d257 1
a257 2
			mcr_dma = q->q_mcr_dma;
			ubsec_callback(q);
d273 1
a273 1
					ubsec_callback(q);
d280 1
a280 1
			ubsec_dma_free(sc, &mcr_dma);
a326 1
	struct ubsec_dma_alloc mcr_dma;
d328 1
a328 1
	u_int8_t *v, *mcr2;
d335 1
d340 3
a342 2
	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr) +
	    ((npkts-1) * sizeof(struct ubsec_mcr_add)), &mcr_dma, 0))
a343 1
	mcr = (struct ubsec_mcr *)mcr_dma.dma_vaddr;
d355 1
a355 1
	for (mcr2 = (u_int8_t *)mcr, i = 0; i < npkts; i++) {
d366 1
a366 1
			v = (u_int8_t *)q->q_mcr;
d369 1
a369 1
			v = ((u_int8_t *)q->q_mcr) + sizeof(struct ubsec_mcr) -
d379 1
a379 2
		ubsec_dma_free(sc, &q->q_mcr_dma);
		bcopy(&mcr_dma, &q->q_mcr_dma, sizeof(mcr_dma));
d383 1
a383 3
	bus_dmamap_sync(sc->sc_dmat, mcr_dma.dma_map,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	WRITE_REG(sc, BS_MCR1, mcr_dma.dma_map->dm_segs[0].ds_addr);
d391 1
a391 1
		WRITE_REG(sc, BS_MCR1, q->q_mcr_dma.dma_map->dm_segs[0].ds_addr);
d393 1
a393 2
		printf("feed: q->chip %08x %08x\n", q,
		    q->q_mcr_dma.dma_map->dm_segs[0].ds_addr);
a583 1
	struct ubsec_pktctx ctx;
d589 2
a590 4
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL) {
		err = EINVAL;
		goto errout;
	}
d624 3
a626 1
	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr), &q->q_mcr_dma, 0)) {
a629 1
	q->q_mcr = (struct ubsec_mcr *)q->q_mcr_dma.dma_vaddr;
a630 1
	bzero(&ctx, sizeof(ctx));
a633 1
	q->q_sc = sc;
d682 1
a682 1
		ctx.pc_flags |= UBS_PKTCTX_ENC_3DES;
d688 1
a688 1
				bcopy(enccrd->crd_iv, ctx.pc_iv, 8);
d690 2
a691 2
				ctx.pc_iv[0] = ses->ses_iv[0];
				ctx.pc_iv[1] = ses->ses_iv[1];
d697 1
a697 1
					    8, (caddr_t)ctx.pc_iv);
d704 1
a704 1
					    (caddr_t)ctx.pc_iv, 8);
d708 1
a708 1
			ctx.pc_flags |= UBS_PKTCTX_INBOUND;
d711 1
a711 1
				bcopy(enccrd->crd_iv, ctx.pc_iv, 8);
d714 1
a714 1
				    8, (caddr_t)ctx.pc_iv);
d720 1
a720 1
				bcopy(crp->crp_iv, (caddr_t)ctx.pc_iv, 8);
d724 8
a731 2
		SWAP32(ctx.pc_iv[0]);
		SWAP32(ctx.pc_iv[1]);
d738 1
a738 1
			ctx.pc_flags |= UBS_PKTCTX_AUTH_MD5;
d740 6
a745 1
			ctx.pc_flags |= UBS_PKTCTX_AUTH_SHA1;
d782 1
a782 1
	ctx.pc_offset = coffset >> 2;
d788 2
a789 2
		q->q_src_l = iov2pages(q->q_src_io, &q->q_src_npa, q->q_src_packp,
		    q->q_src_packl, MAX_SCATTER, &nicealign);
d843 1
a843 2
	printf("  buf[%x]: %d@@%x -> %x\n",
	    q->q_mcr_dma.dma_map->dm_segs[0].ds_addr,
d981 1
a981 1
		    q->q_mcr_dma.dma_map->dm_segs[0].ds_addr,
a994 9
	/*
	 * Put computed context into dma safe memory
	 */
	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktctx_long),
	    &q->q_ctx_dma, 0)) {
		err = ENOMEM;
		goto errout;
	}
	q->q_mcr->mcr_cmdctxp = q->q_ctx_dma.dma_map->dm_segs[0].ds_addr;
d996 17
a1012 35
		struct ubsec_pktctx_long *cx;

		cx = (struct ubsec_pktctx_long *)q->q_ctx_dma.dma_vaddr;
		cx->pc_len = sizeof(struct ubsec_pktctx_long);
		cx->pc_type = UBS_PKTCTX_TYPE_IPSEC;
		cx->pc_flags = ctx.pc_flags;
		cx->pc_offset = ctx.pc_offset;

		if (enccrd)
			for (i = 0; i < 6; i++)
				cx->pc_deskey[i] = ses->ses_deskey[i];
		if (maccrd) {
			for (i = 0; i < 5; i++) {
				cx->pc_hminner[i] = ses->ses_hminner[i];
				cx->pc_hmouter[i] = ses->ses_hmouter[i];
			}
		}
		cx->pc_iv[0] = ctx.pc_iv[0];
		cx->pc_iv[1] = ctx.pc_iv[1];
	} else {
		struct ubsec_pktctx *cx;

		cx = (struct ubsec_pktctx *)q->q_ctx_dma.dma_vaddr;
		bcopy(&ctx, q->q_ctx_dma.dma_vaddr, sizeof(ctx));
		if (enccrd)
			for (i = 0; i < 6; i++)
				cx->pc_deskey[i] = ses->ses_deskey[i];
		if (maccrd) {
			for (i = 0; i < 5; i++) {
				ctx.pc_hminner[i] = ses->ses_hminner[i];
				ctx.pc_hmouter[i] = ses->ses_hmouter[i];
			}
		}
	}
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx_dma.dma_map, BUS_DMASYNC_PREREAD);
d1023 3
a1025 5
		if (q->q_ctx_dma.dma_map != NULL)
			ubsec_dma_free(sc, &q->q_ctx_dma);
		if (q->q_mcr_dma.dma_map != NULL)
			ubsec_dma_free(sc, &q->q_mcr_dma);
		if (q->q_dst_m && q->q_src_m != q->q_dst_m)
d1035 2
a1036 1
ubsec_callback(q)
a1041 4
	bus_dmamap_sync(q->q_sc->sc_dmat, q->q_ctx_dma.dma_map,
	    BUS_DMASYNC_POSTREAD);
	ubsec_dma_free(q->q_sc, &q->q_ctx_dma);

d1056 2
a1057 2
				    (caddr_t)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
			else if (crp->crp_flags & CRYPTO_F_IOV)
d1060 2
a1061 1
				    (caddr_t)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
d1138 7
a1144 1
		WRITE_REG(sc, BS_MCR2, q->q_mcr.dma_map->dm_segs[0].ds_addr);
d1203 1
a1203 1
	if (rng->rng_q.q_mcr.dma_vaddr == NULL) {
a1209 1
			rng->rng_q.q_mcr.dma_vaddr = NULL;
a1215 1
			rng->rng_q.q_mcr.dma_vaddr = NULL;
d1225 1
a1225 1
	mcr->mcr_cmdctxp = rng->rng_q.q_ctx.dma_map->dm_segs[0].ds_addr;
d1229 1
a1229 1
	mcr->mcr_opktbuf.pb_addr = rng->rng_buf.dma_map->dm_segs[0].ds_addr;
a1236 4
	bus_dmamap_sync(sc->sc_dmat, rng->rng_q.q_mcr.dma_map,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, rng->rng_q.q_ctx.dma_map,
	    BUS_DMASYNC_PREREAD);
d1263 1
a1263 1
        int r;
d1265 2
a1266 2
	if ((r = bus_dmamem_alloc(sc->sc_dmat, size, 4, 0,
	    &dma->dma_seg, 1, &dma->dma_nseg, 0)) != 0)
d1282 1
d1302 1
a1302 1
	bus_dmamem_unmap(sc->sc_dmat, dma->dma_vaddr, dma->dma_map->dm_mapsize);
@


1.52
log
@bus_dmaify mcr handling (just leaves buffer lists and mac buffer to go...)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.51 2001/05/22 23:07:39 jason Exp $	*/
a647 1

d1061 1
a1061 1
		if (q->q_ctx_dma.dma_map)
d1063 1
a1063 1
		if (q->q_mcr)
d1307 1
a1307 1
	if ((r = bus_dmamem_alloc(sc->sc_dmat, size, PAGE_SIZE, 0,
d1333 1
@


1.51
log
@- delay allocation of rng handling structures
- if an rng operation is already in progress, don't setup a new timeout.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.50 2001/05/22 22:53:38 jason Exp $	*/
d233 1
d250 8
a261 2
			if ((q->q_mcr->mcr_flags & UBS_MCR_DONE) == 0)
				break;
d269 1
d293 1
a293 1
			free(mcr, M_DEVBUF);
d340 1
d342 1
a342 1
	void *v, *mcr2;
d353 2
a354 3
	mcr = (struct ubsec_mcr *)malloc(sizeof(struct ubsec_mcr) +
	    (npkts-1) * sizeof(struct ubsec_mcr_add), M_DEVBUF, M_NOWAIT);
	if (mcr == NULL)
d356 1
d368 1
a368 1
	for (mcr2 = mcr, i = 0; i < npkts; i++) {
d379 1
a379 1
			v = q->q_mcr;
d382 1
a382 1
			v = ((void *)q->q_mcr) + sizeof(struct ubsec_mcr) -
d392 2
a393 1
		free(q->q_mcr, M_DEVBUF);
d397 3
a399 1
	WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(mcr));
d407 1
a407 1
		WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(q->q_mcr));
d409 2
a410 1
		printf("feed: q->chip %08x %08x\n", q, (u_int32_t)vtophys(q->q_mcr));
d644 1
a644 3
	q->q_mcr = (struct ubsec_mcr *)malloc(sizeof(struct ubsec_mcr),
	    M_DEVBUF, M_NOWAIT);
	if (q->q_mcr == NULL) {
d648 2
a756 1

d854 2
a855 1
	printf("  buf[%x]: %d@@%x -> %x\n", vtophys(q->q_mcr),
d993 1
a993 1
		    vtophys(q->q_mcr),
d1065 1
a1065 1
			free(q->q_mcr, M_DEVBUF);
@


1.50
log
@- Don't poll the rng more than 100 times a second
- bus_dmaify mcr2 operations (rng)
- start bus_dma of mcr1 operations (context; mcr and pktbufs to go)
... more to come ...
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.49 2001/05/14 16:51:48 deraadt Exp $	*/
a211 18
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
		    &sc->sc_rng.rng_q.q_mcr, 0)) {
			printf(": rng mcr alloc failed\n");
			return;
		}
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rngbypass),
		    &sc->sc_rng.rng_q.q_ctx, 0)) {
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_mcr);
			printf(": rng ctx alloc failed\n");
			return;
		}
		if (ubsec_dma_malloc(sc, sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ,
		    &sc->sc_rng.rng_buf, 0)) {
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_ctx);
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_mcr);
			printf(": rng buf alloc failed\n");
			return;
		}
d1219 4
a1244 3
		
	if (rng->rng_used)
		goto out;
@


1.49
log
@kill agregate messages; reported by stephen@@etunnels.com
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.48 2001/05/14 02:45:19 deraadt Exp $	*/
d90 1
a90 1
void	ubsec_callback2 __P((struct ubsec_q2 *));
d93 3
d105 4
d207 23
d231 1
a231 1
		timeout_add(&sc->sc_rngto, 1);
d316 7
a322 1
			if ((q2->q_mcr->mcr_flags & UBS_MCR_DONE) == 0)
d324 1
d326 1
a326 1
			ubsec_callback2(q2);
d606 1
d642 2
a643 2
		q->q_src = (struct uio *)crp->crp_buf;
		q->q_dst = (struct uio *)crp->crp_buf;
d656 1
d709 1
a709 1
		q->q_ctx.pc_flags |= UBS_PKTCTX_ENC_3DES;
d712 2
d715 1
a715 1
				bcopy(enccrd->crd_iv, q->q_ctx.pc_iv, 8);
d717 2
a718 2
				q->q_ctx.pc_iv[0] = ses->ses_iv[0];
				q->q_ctx.pc_iv[1] = ses->ses_iv[1];
d724 1
a724 1
					    8, (caddr_t)q->q_ctx.pc_iv);
d731 1
a731 1
					    (caddr_t)q->q_ctx.pc_iv, 8);
d735 1
a735 1
			q->q_ctx.pc_flags |= UBS_PKTCTX_INBOUND;
d738 1
a738 1
				bcopy(enccrd->crd_iv, q->q_ctx.pc_iv, 8);
d741 1
a741 1
				    8, (caddr_t)q->q_ctx.pc_iv);
d747 1
a747 1
				bcopy(crp->crp_iv, (caddr_t)q->q_ctx.pc_iv, 8);
d751 2
a752 8
		q->q_ctx.pc_deskey[0] = ses->ses_deskey[0];
		q->q_ctx.pc_deskey[1] = ses->ses_deskey[1];
		q->q_ctx.pc_deskey[2] = ses->ses_deskey[2];
		q->q_ctx.pc_deskey[3] = ses->ses_deskey[3];
		q->q_ctx.pc_deskey[4] = ses->ses_deskey[4];
		q->q_ctx.pc_deskey[5] = ses->ses_deskey[5];
		SWAP32(q->q_ctx.pc_iv[0]);
		SWAP32(q->q_ctx.pc_iv[1]);
d759 1
a759 1
			q->q_ctx.pc_flags |= UBS_PKTCTX_AUTH_MD5;
d761 1
a761 1
			q->q_ctx.pc_flags |= UBS_PKTCTX_AUTH_SHA1;
a762 4
		for (i = 0; i < 5; i++) {
			q->q_ctx.pc_hminner[i] = ses->ses_hminner[i];
			q->q_ctx.pc_hmouter[i] = ses->ses_hmouter[i];
		}
d799 1
a799 1
	q->q_ctx.pc_offset = coffset >> 2;
d805 1
a805 1
		q->q_src_l = iov2pages(q->q_src, &q->q_src_npa, q->q_src_packp,
d943 1
a943 1
			q->q_dst_l = iov2pages(q->q_dst, &q->q_dst_npa,
d1012 9
d1022 35
a1056 16
		/* transform small context into long context */
		q->q_mcr->mcr_cmdctxp = vtophys(&q->q_ctxl);
		q->q_ctxl.pc_len = sizeof(struct ubsec_pktctx_long);
		q->q_ctxl.pc_type = UBS_PKTCTX_TYPE_IPSEC;
		q->q_ctxl.pc_flags = q->q_ctx.pc_flags;
		q->q_ctxl.pc_offset = q->q_ctx.pc_offset;
		for (i = 0; i < 6; i++)
			q->q_ctxl.pc_deskey[i] = q->q_ctx.pc_deskey[i];
		for (i = 0; i < 5; i++)
			q->q_ctxl.pc_hminner[i] = q->q_ctx.pc_hminner[i];
		for (i = 0; i < 5; i++)
			q->q_ctxl.pc_hmouter[i] = q->q_ctx.pc_hmouter[i];   
		q->q_ctxl.pc_iv[0] = q->q_ctx.pc_iv[0];
		q->q_ctxl.pc_iv[1] = q->q_ctx.pc_iv[1];
	} else
		q->q_mcr->mcr_cmdctxp = vtophys(&q->q_ctx);
d1067 2
d1087 4
d1097 1
a1097 2
	if ((q->q_ctx.pc_flags & (UBS_PKTCTX_ENC_3DES | UBS_PKTCTX_INBOUND)) ==
	    UBS_PKTCTX_ENC_3DES) {
d1106 1
a1106 1
			else if (crp->crp_flags & CRYPTO_F_IOV) {
a1109 1
			}
d1186 1
a1186 1
		WRITE_REG(sc, BS_MCR2, (u_int32_t)vtophys(q->q_mcr));
d1195 2
a1196 1
ubsec_callback2(q)
d1199 4
a1202 2
	struct ubsec_softc *sc = q->q_sc;
	struct ubsec_keyctx *ctx = q->q_ctx;
d1206 2
a1207 2
		struct ubsec_rng *rng = q->q_private;
		volatile u_int32_t *dat;
d1210 7
a1216 6
		dat = rng->rng_buf;
		for (i = 0; i < UBS_RNGBUFSZ; i++, dat++)
			add_true_randomness(*dat);
		free(rng, M_DEVBUF);
		free(q, M_DEVBUF);
		timeout_add(&sc->sc_rngto, 1);
d1231 3
a1233 2
	struct ubsec_q2 *q;
	struct ubsec_rng *rng;
d1238 1
a1238 2
	if (sc->sc_nqueue2 >= UBS_MAX_NQUEUE) {
		splx(s);
d1240 18
d1259 2
a1260 10
	splx(s);
	
	q = (struct ubsec_q2 *)malloc(sizeof(struct ubsec_q2), M_DEVBUF, M_NOWAIT);
	if (q == NULL)
		goto out;

	rng = (struct ubsec_rng *)malloc(sizeof(struct ubsec_rng),
	    M_DEVBUF, M_NOWAIT);
	if (rng == NULL) {
		free(q, M_DEVBUF);
a1261 6
	}

	q->q_mcr = &rng->rng_mcr;
	q->q_ctx = &rng->rng_ctx;
	q->q_private = rng;
	q->q_sc = sc;
d1263 2
a1264 1
	bzero(rng, sizeof(*rng));
d1266 20
a1285 7
	rng->rng_mcr.mcr_pkts = 1;
	rng->rng_mcr.mcr_cmdctxp = (u_int32_t)vtophys(&rng->rng_ctx);
	rng->rng_ctx.rbp_len = sizeof(struct ubsec_rngbypass_ctx);
	rng->rng_ctx.rbp_op = UBS_CTXOP_RNGBYPASS;
	rng->rng_mcr.mcr_opktbuf.pb_addr = (u_int32_t)vtophys(&rng->rng_buf[0]);
	rng->rng_mcr.mcr_opktbuf.pb_len = ((sizeof(u_int32_t) * UBS_RNGBUFSZ))
	    & UBS_PKTBUF_LEN;
d1287 2
a1288 2
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, q, q_next);
a1297 1
	s = splnet();
d1300 50
a1349 1
	timeout_add(&sc->sc_rngto, 1);
@


1.49.2.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.43 2001/04/06 16:27:45 jason Exp $	*/
d111 2
a112 4
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5501)
		return (1);
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601)
d115 2
a116 1
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)
d143 7
a149 5
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)) {
		sc->sc_statmask |= BS_STAT_MCR2_DONE;
		sc->sc_5601 = 1;
	}
	
d198 2
a199 5
	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    (sc->sc_5601 ? BS_CTRL_MCR2INT : 0));

	if (sc->sc_5601) {
d202 1
d205 4
d282 1
a282 1
	if (sc->sc_5601 && (stat & BS_STAT_MCR2_DONE)) {
d308 1
d310 1
a331 1
#endif
d338 1
d453 1
a453 1
			bcopy(sc->sc_sessions, ses, (sesn + 1) *
d603 3
a620 1
	q->q_mcr->mcr_cmdctxp = vtophys(&q->q_ctx);
d680 13
a692 3
			if ((enccrd->crd_flags & CRD_F_IV_PRESENT) == 0)
				m_copyback(q->q_src_m, enccrd->crd_inject,
				    8, (caddr_t)q->q_ctx.pc_iv);
d698 1
a698 1
			else
d701 7
d770 6
a775 2
	q->q_src_l = mbuf2pages(q->q_src_m, &q->q_src_npa, q->q_src_packp,
	    q->q_src_packl, MAX_SCATTER, &nicealign);
d853 4
a856 1
		if (!nicealign) {
d908 6
a913 2
		q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa,
		    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
d981 18
d1010 1
a1010 1
		if (q->q_src_m != q->q_dst_m)
d1038 9
a1046 3
			m_copydata((struct mbuf *)crp->crp_buf,
			    crd->crd_skip + crd->crd_len - 8, 8,
			    (caddr_t)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
d1055 5
a1059 2
		m_copyback((struct mbuf *)crp->crp_buf,
		    crd->crd_inject, 12, (caddr_t)q->q_macbuf);
d1169 1
a1203 1
	sc->sc_nqueue2++;
d1213 3
@


1.49.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.49.2.1 2001/05/14 22:26:00 niklas Exp $	*/
d56 1
a56 1
#include <crypto/cryptodev.h>
d87 1
a87 1
void	ubsec_callback __P((struct ubsec_softc *, struct ubsec_q *));
d90 1
a90 1
void	ubsec_callback2 __P((struct ubsec_softc *, struct ubsec_q2 *));
a92 3
int	ubsec_dma_malloc __P((struct ubsec_softc *, bus_size_t,
    struct ubsec_dma_alloc *, int));
void	ubsec_dma_free __P((struct ubsec_softc *, struct ubsec_dma_alloc *));
d111 4
a114 2
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5501 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601))
d117 1
a117 2
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820))
a131 1
	struct ubsec_dma *dmap;
d133 1
a133 1
	u_int32_t cmd, i;
d144 5
a148 7
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805))
		sc->sc_flags |= UBS_FLAGS_KEY;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820)
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_LONGCTX;

d160 1
a160 1
	    &sc->sc_st, &sc->sc_sh, NULL, &iosize, 0)) {
d191 5
a195 9
	SIMPLEQ_INIT(&sc->sc_dma);
	dmap = sc->sc_dmaa;
	for (i = 0; i < UBS_MAX_NQUEUE; i++, dmap++) {
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_dmachunk),
		    &dmap->d_alloc, 0))
			break;
		dmap->d_dma = (struct ubsec_dmachunk *)dmap->d_alloc.dma_vaddr;
		SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);
	}
d197 3
a199 5
	crypto_register(sc->sc_cid, CRYPTO_3DES_CBC, 0, 0,
	    ubsec_newsession, ubsec_freesession, ubsec_process);
	crypto_register(sc->sc_cid, CRYPTO_DES_CBC, 0, 0, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_MD5_HMAC, 0, 0, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_SHA1_HMAC, 0, 0, NULL, NULL, NULL);
d201 1
a201 2
	if (sc->sc_flags & UBS_FLAGS_KEY) {
		sc->sc_statmask |= BS_STAT_MCR2_DONE;
d203 1
a203 6
		if (hz >= 100)
			sc->sc_rnghz = hz / 100;
		else
			sc->sc_rnghz = 1;
		timeout_add(&sc->sc_rngto, sc->sc_rnghz);
		printf(", rng");
a205 4
	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    ((sc->sc_flags & UBS_FLAGS_KEY) ? BS_CTRL_MCR2INT : 0));

d228 3
d247 1
a247 1
			ubsec_callback(sc, q);
d263 1
a263 1
					ubsec_callback(sc, q);
d279 1
a279 1
	if ((sc->sc_flags & UBS_FLAGS_KEY) && (stat & BS_STAT_MCR2_DONE)) {
d283 1
a283 7
			bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
			    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);

			mcr = (struct ubsec_mcr *)q2->q_mcr.dma_vaddr;
			if ((mcr->mcr_flags & UBS_MCR_DONE) == 0) {
				bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
				    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
a284 1
			}
d286 1
a286 1
			ubsec_callback2(sc, q2);
a304 1
#ifdef UBSEC_DEBUG
a305 1
#endif
d312 2
a313 2
	if (npkts > UBS_MAX_AGGR)
		npkts = UBS_MAX_AGGR;
d327 1
a333 1
#endif
d448 1
a448 1
			bcopy(sc->sc_sessions, ses, sesn *
a563 2
	struct ubsec_pktctx ctx;
	struct ubsec_dma *dmap = NULL;
d569 4
a572 2
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL)
		return (EINVAL);
a581 7
	if (SIMPLEQ_EMPTY(&sc->sc_dma)) {
		splx(s);
		err = ENOMEM;
		goto errout;
	}
	dmap = SIMPLEQ_FIRST(&sc->sc_dma);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_dma, dmap, d_next);
a590 1
	bzero(&ctx, sizeof(ctx));
a592 1
	q->q_dma = dmap;
a597 3
	} else if (crp->crp_flags & CRYPTO_F_IOV) {
		q->q_src_io = (struct uio *)crp->crp_buf;
		q->q_dst_io = (struct uio *)crp->crp_buf;
d600 1
a600 1
		goto errout;	/* XXX we don't handle contiguous blocks! */
d613 2
d663 1
a663 1
		ctx.pc_flags |= UBS_PKTCTX_ENC_3DES;
a665 2
			q->q_flags |= UBSEC_QFLAGS_COPYOUTIV;

d667 1
a667 1
				bcopy(enccrd->crd_iv, ctx.pc_iv, 8);
d669 2
a670 2
				ctx.pc_iv[0] = ses->ses_iv[0];
				ctx.pc_iv[1] = ses->ses_iv[1];
d673 3
a675 10
			if ((enccrd->crd_flags & CRD_F_IV_PRESENT) == 0) {
				if (crp->crp_flags & CRYPTO_F_IMBUF)
					m_copyback(q->q_src_m,
					    enccrd->crd_inject,
					    8, (caddr_t)ctx.pc_iv);
				else if (crp->crp_flags & CRYPTO_F_IOV)
					cuio_copyback(q->q_src_io,
					    enccrd->crd_inject,
					    8, (caddr_t)ctx.pc_iv);
			}
d677 1
a677 1
			ctx.pc_flags |= UBS_PKTCTX_INBOUND;
d680 2
a681 2
				bcopy(enccrd->crd_iv, ctx.pc_iv, 8);
			else if (crp->crp_flags & CRYPTO_F_IMBUF)
d683 1
a683 5
				    8, (caddr_t)ctx.pc_iv);
			else if (crp->crp_flags & CRYPTO_F_IOV)
				cuio_copydata(q->q_src_io,
				    enccrd->crd_inject, 8,
				    (caddr_t)ctx.pc_iv);
d686 8
a693 8
		ctx.pc_deskey[0] = ses->ses_deskey[0];
		ctx.pc_deskey[1] = ses->ses_deskey[1];
		ctx.pc_deskey[2] = ses->ses_deskey[2];
		ctx.pc_deskey[3] = ses->ses_deskey[3];
		ctx.pc_deskey[4] = ses->ses_deskey[4];
		ctx.pc_deskey[5] = ses->ses_deskey[5];
		SWAP32(ctx.pc_iv[0]);
		SWAP32(ctx.pc_iv[1]);
d700 1
a700 1
			ctx.pc_flags |= UBS_PKTCTX_AUTH_MD5;
d702 1
a702 1
			ctx.pc_flags |= UBS_PKTCTX_AUTH_SHA1;
d705 2
a706 2
			ctx.pc_hminner[i] = ses->ses_hminner[i];
			ctx.pc_hmouter[i] = ses->ses_hmouter[i];
d744 1
a744 1
	ctx.pc_offset = coffset >> 2;
d746 2
a747 6
	if (crp->crp_flags & CRYPTO_F_IMBUF)
		q->q_src_l = mbuf2pages(q->q_src_m, &q->q_src_npa, q->q_src_packp,
		    q->q_src_packl, UBS_MAX_SCATTER, &nicealign);
	else if (crp->crp_flags & CRYPTO_F_IOV)
		q->q_src_l = iov2pages(q->q_src_io, &q->q_src_npa,
		    q->q_src_packp, q->q_src_packl, UBS_MAX_SCATTER, &nicealign);
d777 1
a777 1
			pb = &dmap->d_dma->d_sbuf[j - 1];
d779 3
d797 1
a797 2
			pb->pb_next = dmap->d_alloc.dma_paddr +
			    offsetof(struct ubsec_dmachunk, d_sbuf[j]);
d805 6
d816 2
a817 2
		q->q_mcr->mcr_opktbuf.pb_next = dmap->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_macbuf[0]);
d825 1
a825 4
		if (!nicealign && (crp->crp_flags & CRYPTO_F_IOV)) {
			err = EINVAL;
			goto errout;
		} else if (!nicealign && (crp->crp_flags & CRYPTO_F_IMBUF)) {
d877 2
a878 6
		if (crp->crp_flags & CRYPTO_F_IMBUF)
			q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa,
			    q->q_dst_packp, q->q_dst_packl, UBS_MAX_SCATTER, NULL);
		else if (crp->crp_flags & CRYPTO_F_IOV)
			q->q_dst_l = iov2pages(q->q_dst_io, &q->q_dst_npa,
			    q->q_dst_packp, q->q_dst_packl, UBS_MAX_SCATTER, NULL);
d903 1
a903 1
				pb = &dmap->d_dma->d_dbuf[j - 1];
d905 3
d923 1
a923 2
					pb->pb_next = dmap->d_alloc.dma_paddr +
					    offsetof(struct ubsec_dmachunk, d_macbuf[0]);
d927 1
a927 2
				pb->pb_next = dmap->d_alloc.dma_paddr +
				    offsetof(struct ubsec_dmachunk, d_dbuf[j]);
d936 7
a945 29
	q->q_mcr->mcr_cmdctxp = dmap->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_ctx);

	if (sc->sc_flags & UBS_FLAGS_LONGCTX) {
		struct ubsec_pktctx_long *ctxl;

		ctxl = (struct ubsec_pktctx_long *)(dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx));
		
		/* transform small context into long context */
		ctxl->pc_len = sizeof(struct ubsec_pktctx_long);
		ctxl->pc_type = UBS_PKTCTX_TYPE_IPSEC;
		ctxl->pc_flags = ctx.pc_flags;
		ctxl->pc_offset = ctx.pc_offset;
		for (i = 0; i < 6; i++)
			ctxl->pc_deskey[i] = ctx.pc_deskey[i];
		for (i = 0; i < 5; i++)
			ctxl->pc_hminner[i] = ctx.pc_hminner[i];
		for (i = 0; i < 5; i++)
			ctxl->pc_hmouter[i] = ctx.pc_hmouter[i];   
		ctxl->pc_iv[0] = ctx.pc_iv[0];
		ctxl->pc_iv[1] = ctx.pc_iv[1];
	} else
		bcopy(&ctx, dmap->d_alloc.dma_vaddr +
		    offsetof(struct ubsec_dmachunk, d_ctx),
		    sizeof(struct ubsec_pktctx));
	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map,
	    BUS_DMASYNC_PREREAD);

d955 1
a955 1
		if (q->q_mcr != NULL)
d957 1
a957 6
		if (dmap != NULL) {
			s = splnet();
			SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);
			splx(s);
		}
		if ((q->q_dst_m != NULL) && (q->q_src_m != q->q_dst_m))
d967 1
a967 2
ubsec_callback(sc, q)
	struct ubsec_softc *sc;
a971 4
	struct ubsec_dma *dmap = q->q_dma;

	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map,
	    BUS_DMASYNC_POSTREAD);
d979 2
a980 1
	if (q->q_flags & UBSEC_QFLAGS_COPYOUTIV) {
d985 3
a987 9
			if (crp->crp_flags & CRYPTO_F_IMBUF)
				m_copydata((struct mbuf *)crp->crp_buf,
				    crd->crd_skip + crd->crd_len - 8, 8,
				    (caddr_t)sc->sc_sessions[q->q_sesn].ses_iv);
			else if (crp->crp_flags & CRYPTO_F_IOV) {
				cuio_copydata((struct uio *)crp->crp_buf,
				    crd->crd_skip + crd->crd_len - 8, 8,
				    (caddr_t)sc->sc_sessions[q->q_sesn].ses_iv);
			}
d996 2
a997 7
		if (crp->crp_flags & CRYPTO_F_IMBUF)
			m_copyback((struct mbuf *)crp->crp_buf,
			    crd->crd_inject, 12,
			    (caddr_t)dmap->d_dma->d_macbuf);
		else if (crp->crp_flags & CRYPTO_F_IOV && crp->crp_mac)
			bcopy((caddr_t)dmap->d_dma->d_macbuf,
			    crp->crp_mac, 12);
a1000 2
	SIMPLEQ_INSERT_TAIL(&sc->sc_dma, dmap, d_next);

d1061 1
a1061 7

		bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
		bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map,
		    BUS_DMASYNC_PREREAD);

		WRITE_REG(sc, BS_MCR2, q->q_mcr.dma_paddr);
d1070 1
a1070 2
ubsec_callback2(sc, q)
	struct ubsec_softc *sc;
d1073 2
a1074 4
	struct ubsec_ctx_keyop *ctx;

	ctx = (struct ubsec_ctx_keyop *)q->q_ctx.dma_vaddr;
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, BUS_DMASYNC_POSTREAD);
d1078 2
a1079 2
		struct ubsec_q2_rng *rng = (struct ubsec_q2_rng *)q;
		u_int32_t *p;
d1082 6
a1087 7
		bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map,
		    BUS_DMASYNC_POSTWRITE);
		p = (u_int32_t *)rng->rng_buf.dma_vaddr;
		for (i = 0; i < UBSEC_RNG_BUFSIZ; p++, i++)
			add_true_randomness(*p);
		rng->rng_used = 0;
		timeout_add(&sc->sc_rngto, sc->sc_rnghz);
d1102 2
a1103 3
	struct ubsec_q2_rng *rng = &sc->sc_rng;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_rngbypass *ctx;
d1107 1
a1107 1
	if (rng->rng_used) {
d1109 1
a1109 1
		return;
d1111 4
a1114 2
	sc->sc_nqueue2++;
	if (sc->sc_nqueue2 >= UBS_MAX_NQUEUE)
d1117 5
a1121 15
	if (rng->rng_q.q_mcr.dma_map == NULL) {
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
		    &rng->rng_q.q_mcr, 0))
			goto out;
		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rngbypass),
		    &rng->rng_q.q_ctx, 0)) {
			ubsec_dma_free(sc, &rng->rng_q.q_mcr);
			goto out;
		}
		if (ubsec_dma_malloc(sc, sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ,
		    &rng->rng_buf, 0)) {
			ubsec_dma_free(sc, &rng->rng_q.q_ctx);
			ubsec_dma_free(sc, &rng->rng_q.q_mcr);
			goto out;
		}
d1124 14
a1137 2
	mcr = (struct ubsec_mcr *)rng->rng_q.q_mcr.dma_vaddr;
	ctx = (struct ubsec_ctx_rngbypass *)rng->rng_q.q_ctx.dma_vaddr;
d1139 3
a1141 19
	mcr->mcr_pkts = 1;
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = rng->rng_q.q_ctx.dma_paddr;
	mcr->mcr_ipktbuf.pb_addr = mcr->mcr_ipktbuf.pb_next = 0;
	mcr->mcr_ipktbuf.pb_len = 0;
	mcr->mcr_reserved = mcr->mcr_pktlen = 0;
	mcr->mcr_opktbuf.pb_addr = rng->rng_buf.dma_paddr;
	mcr->mcr_opktbuf.pb_len = ((sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ)) &
	    UBS_PKTBUF_LEN;
	mcr->mcr_opktbuf.pb_next = 0;

	ctx->rbp_len = sizeof(struct ubsec_ctx_rngbypass);
	ctx->rbp_op = UBS_CTXOP_RNGBYPASS;

	bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map,
	    BUS_DMASYNC_PREWRITE);

	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, (struct ubsec_q2 *)rng, q_next);
	rng->rng_used = 1;
d1151 1
a1151 54
	sc->sc_nqueue2--;
	splx(s);
	timeout_add(&sc->sc_rngto, sc->sc_rnghz);
}

int
ubsec_dma_malloc(sc, size, dma, mapflags)
	struct ubsec_softc *sc;
	bus_size_t size;
	struct ubsec_dma_alloc *dma;
	int mapflags;
{
	int r;

	if ((r = bus_dmamem_alloc(sc->sc_dmat, size, PAGE_SIZE, 0,
	    &dma->dma_seg, 1, &dma->dma_nseg, BUS_DMA_NOWAIT)) != 0)
		goto fail_0;

	if ((r = bus_dmamem_map(sc->sc_dmat, &dma->dma_seg, dma->dma_nseg,
	    size, &dma->dma_vaddr, mapflags | BUS_DMA_NOWAIT)) != 0)
		goto fail_1;

	if ((r = bus_dmamap_create(sc->sc_dmat, size, 1, size, 0,
	    BUS_DMA_NOWAIT, &dma->dma_map)) != 0)
		goto fail_2;

	if ((r = bus_dmamap_load(sc->sc_dmat, dma->dma_map, dma->dma_vaddr,
	    size, NULL, BUS_DMA_NOWAIT)) != 0)
		goto fail_3;

	dma->dma_paddr = dma->dma_map->dm_segs[0].ds_addr;
	dma->dma_size = size;
	return (0);

fail_3:
	bus_dmamap_destroy(sc->sc_dmat, dma->dma_map);
fail_2:
	bus_dmamem_unmap(sc->sc_dmat, dma->dma_vaddr, size);
fail_1:
	bus_dmamem_free(sc->sc_dmat, &dma->dma_seg, dma->dma_nseg);
fail_0:
	dma->dma_map = NULL;
	return (r);
}

void
ubsec_dma_free(sc, dma)
	struct ubsec_softc *sc;
	struct ubsec_dma_alloc *dma;
{
	bus_dmamap_unload(sc->sc_dmat, dma->dma_map);
	bus_dmamem_unmap(sc->sc_dmat, dma->dma_vaddr, dma->dma_size);
	bus_dmamem_free(sc->sc_dmat, &dma->dma_seg, dma->dma_nseg);
	bus_dmamap_destroy(sc->sc_dmat, dma->dma_map);
@


1.49.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.49.2.2 2001/07/04 10:43:12 niklas Exp $	*/
d49 4
a55 2
#include <vm/vm.h>

a104 8
#ifdef __HAS_NEW_BUS_DMAMAP_SYNC
#define ubsec_bus_dmamap_sync(t, m, o, l, f) \
    bus_dmamap_sync((t), (m), (o), (l), (f))
#else
#define ubsec_bus_dmamap_sync(t, m, o, l, f) \
    bus_dmamap_sync((t), (m), (f))
#endif

d171 2
a172 1
	if (pci_intr_map(pa, &ih)) {
d301 2
a302 3
			ubsec_bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
			    0, q2->q_mcr.dma_map->dm_mapsize,
			    BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE);
d306 2
a307 4
				ubsec_bus_dmamap_sync(sc->sc_dmat,
				    q2->q_mcr.dma_map, 0,
				    q2->q_mcr.dma_map->dm_mapsize,
				    BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
a805 5
	if (q->q_src_l > 0xfffc) {
		err = EIO;
		goto errout;
	}

a827 5
		if (q->q_src_packl[i] > 0xfffc) {
			err = EIO;
			goto errout;
		}

a932 9
		if (q->q_dst_l == 0) {
			err = ENOMEM;
			goto errout;
		}
		if (q->q_dst_l > 0xfffc) {
			err = ENOMEM;
			goto errout;
		}

a952 5
			if (q->q_dst_packl[i] > 0xfffc) {
				err = EIO;
				goto errout;
			}

d1017 2
a1018 3
	ubsec_bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
	    dmap->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
d1054 2
a1055 3
	ubsec_bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
	    dmap->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE);
d1158 1
a1158 2
		ubsec_bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map, 0,
		    q->q_mcr.dma_map->dm_mapsize,
d1160 2
a1161 3
		ubsec_bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
		    q->q_ctx.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREWRITE);
d1179 1
a1179 2
	ubsec_bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
	    q->q_ctx.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
d1187 2
a1188 2
		ubsec_bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
		    rng->rng_buf.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
d1256 2
a1257 2
	ubsec_bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
	    rng->rng_buf.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
@


1.49.2.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d52 1
a52 1
#include <uvm/uvm_extern.h>
d103 8
d194 1
a194 1
	sc->sc_cid = crypto_get_driverid(0);
d306 1
a306 1
			bus_dmamap_sync(sc->sc_dmat, q2->q_mcr.dma_map,
d312 1
a312 1
				bus_dmamap_sync(sc->sc_dmat,
d1049 1
a1049 1
	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
d1087 1
a1087 1
	bus_dmamap_sync(sc->sc_dmat, dmap->d_alloc.dma_map, 0,
d1192 1
a1192 1
		bus_dmamap_sync(sc->sc_dmat, q->q_mcr.dma_map, 0,
d1195 1
a1195 1
		bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
d1215 1
a1215 1
	bus_dmamap_sync(sc->sc_dmat, q->q_ctx.dma_map, 0,
d1224 1
a1224 1
		bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
d1293 1
a1293 1
	bus_dmamap_sync(sc->sc_dmat, rng->rng_buf.dma_map, 0,
@


1.49.2.5
log
@Merge in -current
@
text
@d37 1
d117 1
a117 2
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821))
d149 1
a149 2
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5820 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821))
a161 5
	if (!(cmd & PCI_COMMAND_MASTER_ENABLE)) {
		printf(": failed to enable bus mastering\n");
		return;
	}

a168 5
	/* Disable TRDY timeout and RETRY timeout */
	cmd = pci_conf_read(pc, pa->pa_tag, BS_TRDY_TIMEOUT);
	cmd &= 0xffff0000;
	pci_conf_write(pc, pa->pa_tag, PCI_COMMAND_STATUS_REG, cmd);

a210 18

		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
		    &sc->sc_rng.rng_q.q_mcr, 0))
			goto skip_rng;

		if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rngbypass),
		    &sc->sc_rng.rng_q.q_ctx, 0)) {
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_mcr);
			goto skip_rng;
		}

		if (ubsec_dma_malloc(sc, sizeof(u_int32_t) *
		    UBSEC_RNG_BUFSIZ, &sc->sc_rng.rng_buf, 0)) {
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_ctx);
			ubsec_dma_free(sc, &sc->sc_rng.rng_q.q_mcr);
			goto skip_rng;
		}

a217 1
skip_rng:
d399 3
d862 6
d1006 7
d1250 17
@


1.49.2.6
log
@Merge in trunk
@
text
@a5 2
 * Copyright (c) 2001 Patrik Lindergren (patrik@@ipunplugged.com)
 * 
d38 1
a38 1
 * uBsec 5[56]01, 58xx hardware crypto accelerator
a70 6
void ubsec_reset_board	__P((struct ubsec_softc *));
void ubsec_init_board	__P((struct ubsec_softc *));
void ubsec_init_pciregs	__P((struct pci_attach_args *pa));
void ubsec_cleanchip	__P((struct ubsec_softc *));
void ubsec_totalreset	__P((struct ubsec_softc *));
int  ubsec_free_q	__P((struct ubsec_softc*, struct ubsec_q *));
a92 1
int	ubsec_dmamap_aligned __P((bus_dmamap_t));
d100 1
a100 5
#define	SWAP32(x) (x) = htole32(ntohl((x)))
#define	HTOLE32(x) (x) = htole32(x)


struct ubsec_stats ubsecstats;
d175 5
d204 1
a204 1
	SIMPLEQ_INIT(&sc->sc_freequeue);
a206 9
		struct ubsec_q *q;

		q = (struct ubsec_q *)malloc(sizeof(struct ubsec_q),
		    M_DEVBUF, M_NOWAIT);
		if (q == NULL) {
			printf("Warning can't allocate queue buffers for ubsec\n");
			break;
		}

d208 1
a208 3
		    &dmap->d_alloc, 0)) {
			printf("Warning can't allocate dma buffers for ubsec\n");
			free(q, M_DEVBUF);
a209 1
		}
d211 1
a211 5

		q->q_dma = dmap;
		sc->sc_queuea[i] = q;

		SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
a219 17
	/*
	 * Reset Broadcom chip
	 */
	ubsec_reset_board(sc);

	/*
	 * Init Broadcom specific PCI settings
	 */
	ubsec_init_pciregs(pa);

	/*
	 * Init Broadcom chip
	 */
	ubsec_init_board(sc);

	
#ifndef UBSEC_NO_RNG
a247 1
	;
d249 4
a252 1
#endif /* UBSEC_NO_RNG */
a256 3
/*
 * UBSEC Interrupt routine
 */
d262 1
a262 1
	volatile u_int32_t stat;
d264 2
a265 1
	struct ubsec_dma *dmap;
a275 3
	/*
	 * Check to see if we have any packets waiting for us
	 */
d279 5
a283 3
			dmap = q->q_dma;

			if ((dmap->d_dma->d_mcr.mcr_flags & htole16(UBS_MCR_DONE)) == 0)
d285 1
a285 1

d287 6
a293 1
			npkts = q->q_nstacked_mcrs;
d299 10
a308 4
			for (i = 0; i < npkts; i++) {
				if(q->q_stacked_mcr[i]) {
					ubsec_callback(sc, q->q_stacked_mcr[i]);
					ubsecstats.hst_opackets++;
d310 1
d314 2
a315 2
			ubsec_callback(sc, q);
			ubsecstats.hst_opackets++;
d317 5
a321 7

		/*
		 * Don't send any more packet to chip if there has been
		 * a DMAERR.
		 */
		if (!(stat & BS_STAT_DMAERR))
			ubsec_feed(sc);
a323 4
#ifndef UBSEC_NO_RNG
	/*
	 * Check to see if we have any Random number waiting for us
	 */
a324 3
		struct ubsec_q2 *q2;
		struct ubsec_mcr *mcr;

d333 1
a333 1
			if ((mcr->mcr_flags & htole16(UBS_MCR_DONE)) == 0) {
d342 1
a342 6
			/*
			 * Don't send any more packet to chip if there has been
			 * a DMAERR.
			 */
			if (!(stat & BS_STAT_DMAERR))
				ubsec_feed2(sc);
a344 1
#endif /* UBSEC_NO_RNG */
a345 3
	/*
	 * Check to see if we got any DMA Error
	 */
d347 1
a347 3
#ifdef UBSEC_DEBUG
		volatile u_int32_t a = READ_REG(sc, BS_ERR);

d349 2
a350 5
		    (a & BS_ERR_READ) ? "read" : "write", a & BS_ERR_ADDR);
#endif /* UBSEC_DEBUG */
		ubsecstats.hst_dmaerr++;
		ubsec_totalreset(sc);
		ubsec_feed(sc);
a355 4
/*
 * ubsec_feed() - aggregate and post requests to chip
 *		  It is assumed that the caller set splnet()
 */
d362 5
a366 5
#endif /* UBSEC_DEBUG */
	struct ubsec_q *q, *q2;
	int npkts, i;
	void *v;
	u_int32_t stat;
d374 1
a374 5
	if ((stat = READ_REG(sc, BS_STAT)) & (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
		if(stat & BS_STAT_DMAERR) {
			ubsec_totalreset(sc);
			ubsecstats.hst_dmaerr++;
		}
d376 5
a380 1
	}
d390 1
a390 1
#endif /* UBSEC_DEBUG */
d392 3
a394 20
	q = SIMPLEQ_FIRST(&sc->sc_queue);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
	--sc->sc_nqueue;

	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	if (q->q_dst_map != NULL)
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	q->q_nstacked_mcrs = npkts - 1;		/* Number of packets stacked */

	for (i = 0; i < q->q_nstacked_mcrs; i++) {
		q2 = SIMPLEQ_FIRST(&sc->sc_queue);
		bus_dmamap_sync(sc->sc_dmat, q2->q_src_map,
		    0, q2->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
		if (q2->q_dst_map != NULL)
			bus_dmamap_sync(sc->sc_dmat, q2->q_dst_map,
			    0, q2->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q2, q_next);
d396 19
d416 5
a420 12
		v = ((void *)&q2->q_dma->d_dma->d_mcr) + sizeof(struct ubsec_mcr) -
		    sizeof(struct ubsec_mcr_add);
		bcopy(v, &q->q_dma->d_dma->d_mcradd[i], sizeof(struct ubsec_mcr_add));
		q->q_stacked_mcr[i] = q2;
	}
	q->q_dma->d_dma->d_mcr.mcr_pkts = htole16(npkts);
	SIMPLEQ_INSERT_TAIL(&sc->sc_qchip, q, q_next);
	bus_dmamap_sync(sc->sc_dmat, q->q_dma->d_alloc.dma_map,
	    0, q->q_dma->d_alloc.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	WRITE_REG(sc, BS_MCR1, q->q_dma->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_mcr));
d425 1
a425 5
		if ((stat = READ_REG(sc, BS_STAT)) & (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
			if(stat & BS_STAT_DMAERR) {
				ubsec_totalreset(sc);
				ubsecstats.hst_dmaerr++;
			}
a426 2
		}

d428 1
a428 15

		bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
		    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
		if (q->q_dst_map != NULL)
			bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
			    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_PREREAD);
		bus_dmamap_sync(sc->sc_dmat, q->q_dma->d_alloc.dma_map,
		    0, q->q_dma->d_alloc.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		WRITE_REG(sc, BS_MCR1, q->q_dma->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_mcr));
#ifdef UBSEC_DEBUG
		printf("feed: q->chip %p %08x\n", q, (u_int32_t)vtophys(&q->q_dma->d_dma->d_mcr));
#endif /* UBSEC_DEBUG */
d621 1
a621 2
	if (crp == NULL || crp->crp_callback == NULL) {
		ubsecstats.hst_invalid++;
d623 1
a623 1
	}
d625 1
a625 2
	if (card >= ubsec_cd.cd_ndevs || ubsec_cd.cd_devs[card] == NULL) {
		ubsecstats.hst_invalid++;
a626 1
	}
d631 6
a636 3

	if (SIMPLEQ_EMPTY(&sc->sc_freequeue)) {
		ubsecstats.hst_queuefull++;
d639 1
a639 1
		goto errout2;
d641 2
a642 3

	q = SIMPLEQ_FIRST(&sc->sc_freequeue);
	SIMPLEQ_REMOVE_HEAD(&sc->sc_freequeue, q, q_next);
d645 6
a650 1
	dmap = q->q_dma; /* Save dma pointer */
d669 7
a675 1
	bzero(&dmap->d_dma->d_mcr, sizeof(struct ubsec_mcr));
d677 2
a678 2
	dmap->d_dma->d_mcr.mcr_pkts = htole16(1);
	dmap->d_dma->d_mcr.mcr_flags = 0;
d727 1
a727 1
		ctx.pc_flags |= htole16(UBS_PKTCTX_ENC_3DES);
d750 1
a750 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_INBOUND);
d777 1
a777 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_AUTH_MD5);
d779 1
a779 1
			ctx.pc_flags |= htole16(UBS_PKTCTX_AUTH_SHA1);
a783 3

			HTOLE32(ctx.pc_hminner[i]);
			HTOLE32(ctx.pc_hmouter[i]);
d821 1
a821 1
	ctx.pc_offset = htole16(coffset >> 2);
d823 7
a829 2
	if (bus_dmamap_create(sc->sc_dmat, 0xfff0, UBS_MAX_SCATTER,
		0xfff0, 0, BUS_DMA_NOWAIT, &q->q_src_map) != 0) {
d833 3
a835 16
	if (crp->crp_flags & CRYPTO_F_IMBUF) {
		if (bus_dmamap_load_mbuf(sc->sc_dmat, q->q_src_map,
		    q->q_src_m, BUS_DMA_NOWAIT) != 0) {
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
			q->q_src_map = NULL;
			err = ENOMEM;
			goto errout;
		}
	} else if (crp->crp_flags & CRYPTO_F_IOV) {
		if (bus_dmamap_load_uio(sc->sc_dmat, q->q_src_map,
		    q->q_src_io, BUS_DMA_NOWAIT) != 0) {
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
			q->q_src_map = NULL;
			err = ENOMEM;
			goto errout;
		}
a836 1
	nicealign = ubsec_dmamap_aligned(q->q_src_map);
d838 1
a838 1
	dmap->d_dma->d_mcr.mcr_pktlen = htole16(stheend);
d843 1
a843 1
	for (i = j = 0; i < q->q_src_map->dm_nsegs; i++) {
a844 2
		bus_size_t packl = q->q_src_map->dm_segs[i].ds_len;
		bus_addr_t packp = q->q_src_map->dm_segs[i].ds_addr;
d846 12
a857 3
		if (sskip >= packl) {
			sskip -= packl;
			continue;
d860 1
a860 5
		packl -= sskip;
		packp += sskip;
		sskip = 0;

		if (packl > 0xfffc) {
d866 1
a866 1
			pb = &dmap->d_dma->d_mcr.mcr_ipktbuf;
d870 1
a870 2
		pb->pb_addr = htole32(packp);

d872 2
a873 2
			if (packl > stheend) {
				pb->pb_len = htole32(stheend);
d876 2
a877 2
				pb->pb_len = htole32(packl);
				stheend -= packl;
d880 1
a880 1
			pb->pb_len = htole32(packl);
d882 1
a882 1
		if ((i + 1) == q->q_src_map->dm_nsegs)
d885 2
a886 2
			pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
			    offsetof(struct ubsec_dmachunk, d_sbuf[j]));
d891 4
a894 4
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_addr = 0;
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_len = 0;
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_next = htole32(dmap->d_alloc.dma_paddr +
		    offsetof(struct ubsec_dmachunk, d_macbuf[0]));
d897 3
a899 3
		    dmap->d_mcr->mcr_opktbuf.pb_addr,
		    dmap->d_mcr->mcr_opktbuf.pb_len,
		    dmap->d_mcr->mcr_opktbuf.pb_next);
d902 14
a915 4
		if (crp->crp_flags & CRYPTO_F_IOV) {
			if (!nicealign) {
				err = EINVAL;
				goto errout;
d917 1
a917 3
			if (bus_dmamap_create(sc->sc_dmat, 0xfff0,
			    UBS_MAX_SCATTER, 0xfff0, 0, BUS_DMA_NOWAIT,
			    &q->q_dst_map) != 0) {
d921 6
a926 5
			if (bus_dmamap_load_uio(sc->sc_dmat, q->q_dst_map,
			    q->q_dst_io, BUS_DMA_NOWAIT) != 0) {
				bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
				q->q_dst_map = NULL;
				goto errout;
d928 3
a930 7
		} else if (crp->crp_flags & CRYPTO_F_IMBUF) {
			if (nicealign) {
				q->q_dst_m = q->q_src_m;
				q->q_dst_map = q->q_src_map;
			} else {
				int totlen, len;
				struct mbuf *m, *top, **mp;
d932 8
a939 5
				totlen = q->q_src_map->dm_mapsize;
				if (q->q_src_m->m_flags & M_PKTHDR) {
					len = MHLEN;
					MGETHDR(m, M_DONTWAIT, MT_DATA);
				} else {
a940 5
					MGET(m, M_DONTWAIT, MT_DATA);
				}
				if (m == NULL) {
					err = ENOMEM;
					goto errout;
d942 1
a942 3
				if (len == MHLEN)
					M_DUP_PKTHDR(m, q->q_src_m);
				if (totlen >= MINCLSIZE) {
d947 4
a950 42
				m->m_len = len;
				top = NULL;
				mp = &top;

				while (totlen > 0) {
					if (top) {
						MGET(m, M_DONTWAIT, MT_DATA);
						if (m == NULL) {
							m_freem(top);
							err = ENOMEM;
							goto errout;
						}
						len = MLEN;
					}
					if (top && totlen >= MINCLSIZE) {
						MCLGET(m, M_DONTWAIT);
						if (m->m_flags & M_EXT)
							len = MCLBYTES;
					}
					m->m_len = len = min(totlen, len);
					totlen -= len;
					*mp = m;
					mp = &m->m_next;
				}
				q->q_dst_m = top;
				ubsec_mcopy(q->q_src_m, q->q_dst_m,
				    cpskip, cpoffset);
				if (bus_dmamap_create(sc->sc_dmat, 0xfff0,
				    UBS_MAX_SCATTER, 0xfff0, 0, BUS_DMA_NOWAIT,
				    &q->q_dst_map) != 0) {
					err = ENOMEM;
					goto errout;
				}
				if (bus_dmamap_load_mbuf(sc->sc_dmat,
				    q->q_dst_map, q->q_dst_m,
				    BUS_DMA_NOWAIT) != 0) {
					bus_dmamap_destroy(sc->sc_dmat,
					q->q_dst_map);
					q->q_dst_map = NULL;
					err = ENOMEM;
					goto errout;
				}
d952 18
a969 2
		} else {
			err = EINVAL;
d976 1
a976 1
		for (i = j = 0; i < q->q_dst_map->dm_nsegs; i++) {
a977 2
			bus_size_t packl = q->q_dst_map->dm_segs[i].ds_len;
			bus_addr_t packp = q->q_dst_map->dm_segs[i].ds_addr;
d979 12
a990 3
			if (dskip >= packl) {
				dskip -= packl;
				continue;
d993 1
a993 5
			packl -= dskip;
			packp += dskip;
			dskip = 0;

			if (packl > 0xfffc) {
d999 1
a999 1
				pb = &dmap->d_dma->d_mcr.mcr_opktbuf;
d1003 1
a1003 1
			pb->pb_addr = htole32(packp);
d1006 2
a1007 2
				if (packl > dtheend) {
					pb->pb_len = htole32(dtheend);
d1010 2
a1011 2
					pb->pb_len = htole32(packl);
					dtheend -= packl;
d1014 1
a1014 1
				pb->pb_len = htole32(packl);
d1016 1
a1016 1
			if ((i + 1) == q->q_dst_map->dm_nsegs) {
d1018 2
a1019 2
					pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
					    offsetof(struct ubsec_dmachunk, d_macbuf[0]));
d1023 2
a1024 2
				pb->pb_next = htole32(dmap->d_alloc.dma_paddr +
				    offsetof(struct ubsec_dmachunk, d_dbuf[j]));
d1029 2
a1030 2
	dmap->d_dma->d_mcr.mcr_cmdctxp = htole32(dmap->d_alloc.dma_paddr +
	    offsetof(struct ubsec_dmachunk, d_ctx));
d1039 2
a1040 2
		ctxl->pc_len = htole16(sizeof(struct ubsec_pktctx_long));
		ctxl->pc_type = htole16(UBS_PKTCTX_TYPE_IPSEC);
d1055 3
a1061 2
	ubsecstats.hst_ipackets++;
	ubsecstats.hst_ibytes += dmap->d_alloc.dma_map->dm_mapsize;
d1068 7
d1077 1
a1077 13

		if (q->q_dst_map != NULL && q->q_dst_map != q->q_src_map) {
			bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
			bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
		}
		if (q->q_src_map != NULL) {
			bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
			bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
		}

		s = splnet();
		SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
		splx(s);
a1078 5
	if (err == EINVAL)
		ubsecstats.hst_invalid++;
	else
		ubsecstats.hst_nomem++;
errout2:
a1095 10
	if (q->q_dst_map != NULL && q->q_dst_map != q->q_src_map) {
		bus_dmamap_sync(sc->sc_dmat, q->q_dst_map,
		    0, q->q_dst_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, q->q_dst_map);
		bus_dmamap_destroy(sc->sc_dmat, q->q_dst_map);
	}
	bus_dmamap_sync(sc->sc_dmat, q->q_src_map,
	    0, q->q_src_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(sc->sc_dmat, q->q_src_map);
	bus_dmamap_destroy(sc->sc_dmat, q->q_src_map);
a1100 1
	ubsecstats.hst_obytes += ((struct mbuf *)crp->crp_buf)->m_len;
d1134 8
a1141 1
	SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
a1183 1
#ifndef UBSEC_NO_RNG
a1212 3
/*
 * Callback for handling random numbers
 */
d1224 1
a1224 1
	switch (letoh16(ctx->ctx_op)) {
d1234 1
a1234 1
			add_true_randomness(letoh32(*p));
d1241 1
a1241 1
		    letoh16(ctx->ctx_op));
d1268 1
a1268 1
	mcr->mcr_pkts = htole16(1);
d1270 1
a1270 1
	mcr->mcr_cmdctxp = htole32(rng->rng_q.q_ctx.dma_paddr);
d1274 3
a1276 3
	mcr->mcr_opktbuf.pb_addr = htole32(rng->rng_buf.dma_paddr);
	mcr->mcr_opktbuf.pb_len = htole32(((sizeof(u_int32_t) * UBSEC_RNG_BUFSIZ)) &
	    UBS_PKTBUF_LEN);
d1279 2
a1280 2
	ctx->rbp_len = htole16(sizeof(struct ubsec_ctx_rngbypass));
	ctx->rbp_op = htole16(UBS_CTXOP_RNGBYPASS);
a1299 1
#endif /* UBSEC_NO_RNG */
a1349 155
}

/*
 * Resets the board.  Values in the regesters are left as is
 * from the reset (i.e. initial values are assigned elsewhere).
 */
void
ubsec_reset_board(sc)
	struct ubsec_softc *sc;
{
    volatile u_int32_t ctrl;

    ctrl = READ_REG(sc, BS_CTRL);
    ctrl |= BS_CTRL_RESET;
    WRITE_REG(sc, BS_CTRL, ctrl);

    /*
     * Wait aprox. 30 PCI clocks = 900 ns = 0.9 us
     */
    DELAY(10);
}

/*
 * Init Broadcom registers
 */
void
   ubsec_init_board(sc)
   struct ubsec_softc *sc;
{
	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    ((sc->sc_flags & UBS_FLAGS_KEY) ? BS_CTRL_MCR2INT : 0));
}

/*
 * Init Broadcom PCI registers
 */
void
   ubsec_init_pciregs(pa)
   struct pci_attach_args *pa;
{
	pci_chipset_tag_t pc = pa->pa_pc;
	u_int32_t misc;

#if 0
	misc = pci_conf_read(pc, pa->pa_tag, BS_RTY_TOUT);
	misc = (misc & ~(UBS_PCI_RTY_MASK << UBS_PCI_RTY_SHIFT))
	    | ((UBS_DEF_RTY & 0xff) << UBS_PCI_RTY_SHIFT);
	misc = (misc & ~(UBS_PCI_TOUT_MASK << UBS_PCI_TOUT_SHIFT))
	    | ((UBS_DEF_TOUT & 0xff) << UBS_PCI_TOUT_SHIFT);
	pci_conf_write(pc, pa->pa_tag, BS_RTY_TOUT, misc);
#endif

	/*
	 * This will set the cache line size to 1, this will
	 * force the BCM58xx chip just to do burst read/writes.
	 * Cache line read/writes are to slow
	 */
	misc = pci_conf_read(pc, pa->pa_tag, PCI_BHLC_REG);
	misc = (misc & ~(PCI_CACHELINE_MASK << PCI_CACHELINE_SHIFT))
	    | ((UBS_DEF_CACHELINE & 0xff) << PCI_CACHELINE_SHIFT);
	pci_conf_write(pc, pa->pa_tag, PCI_BHLC_REG, misc);
}

/*
 * Clean up after a chip crash.
 * It is assumed that the caller in splnet()
 */
void
ubsec_cleanchip(sc)
	struct ubsec_softc *sc;
{
	struct ubsec_q *q;

	while (!SIMPLEQ_EMPTY(&sc->sc_qchip)) {
		q = SIMPLEQ_FIRST(&sc->sc_qchip);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q, q_next);
		ubsec_free_q(sc, q);
	}
}

/*
 * free a ubsec_q
 * It is assumed that the caller is within splnet()
 */
int
ubsec_free_q(struct ubsec_softc *sc, struct ubsec_q *q)
{
	struct ubsec_q *q2;
	struct cryptop *crp;
	int npkts;
	int i;

	npkts = q->q_nstacked_mcrs;

	for (i = 0; i < npkts; i++) {
		if(q->q_stacked_mcr[i]) {
			q2 = q->q_stacked_mcr[i];

			if ((q2->q_dst_m != NULL) && (q2->q_src_m != q2->q_dst_m)) 
				m_freem(q2->q_dst_m);

			crp = (struct cryptop *)q2->q_crp;
			
			SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q2, q_next);
			
			crp->crp_etype = EFAULT;
			crp->crp_callback(crp);
		} else {
			break;
		}
	}

	/*
	 * Free header MCR
	 */
	if ((q->q_dst_m != NULL) && (q->q_src_m != q->q_dst_m))
		m_freem(q->q_dst_m);

	crp = (struct cryptop *)q->q_crp;
	
	SIMPLEQ_INSERT_TAIL(&sc->sc_freequeue, q, q_next);
	
	crp->crp_etype = EFAULT;
	crp->crp_callback(crp);
	return(0);
}

/*
 * Routine to reset the chip and clean up.
 * It is assumed that the caller is in splnet()
 */
void
ubsec_totalreset(sc)
	struct ubsec_softc *sc;
{
	ubsec_reset_board(sc);
	ubsec_init_board(sc);
	ubsec_cleanchip(sc);
}

int
ubsec_dmamap_aligned(map)
	bus_dmamap_t map;
{
	int i;

	for (i = 0; i < map->dm_nsegs; i++) {
		if (map->dm_segs[i].ds_addr & 3)
			return (0);
		if ((i != (map->dm_nsegs - 1)) &&
		    (map->dm_segs[i].ds_len & 3))
			return (0);
	}
	return (1);
@


1.49.2.7
log
@Merge in -current from roughly a week ago
@
text
@d71 8
a78 8
int ubsec_probe(struct device *, void *, void *);
void ubsec_attach(struct device *, struct device *, void *);
void ubsec_reset_board(struct ubsec_softc *);
void ubsec_init_board(struct ubsec_softc *);
void ubsec_init_pciregs(struct pci_attach_args *pa);
void ubsec_cleanchip(struct ubsec_softc *);
void ubsec_totalreset(struct ubsec_softc *);
int  ubsec_free_q(struct ubsec_softc*, struct ubsec_q *);
d88 14
a101 14
int	ubsec_intr(void *);
int	ubsec_newsession(u_int32_t *, struct cryptoini *);
int	ubsec_freesession(u_int64_t);
int	ubsec_process(struct cryptop *);
void	ubsec_callback(struct ubsec_softc *, struct ubsec_q *);
int	ubsec_feed(struct ubsec_softc *);
void	ubsec_mcopy(struct mbuf *, struct mbuf *, int, int);
void	ubsec_callback2(struct ubsec_softc *, struct ubsec_q2 *);
int	ubsec_feed2(struct ubsec_softc *);
void	ubsec_rng(void *);
int	ubsec_dma_malloc(struct ubsec_softc *, bus_size_t,
    struct ubsec_dma_alloc *, int);
void	ubsec_dma_free(struct ubsec_softc *, struct ubsec_dma_alloc *);
int	ubsec_dmamap_aligned(bus_dmamap_t);
@


1.49.2.8
log
@Sync the SMP branch with 3.3
@
text
@a34 5
 *
 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
 *
a102 15
int	ubsec_kprocess(struct cryptkop *);
struct ubsec_softc *ubsec_kfind(struct cryptkop *);
int	ubsec_kprocess_modexp_sw(struct ubsec_softc *, struct cryptkop *);
int	ubsec_kprocess_modexp_hw(struct ubsec_softc *, struct cryptkop *);
int	ubsec_kprocess_rsapriv(struct ubsec_softc *, struct cryptkop *);
void	ubsec_kfree(struct ubsec_softc *, struct ubsec_q2 *);
int	ubsec_ksigbits(struct crparam *);
void	ubsec_kshift_r(u_int, u_int8_t *, u_int, u_int8_t *, u_int);
void	ubsec_kshift_l(u_int, u_int8_t *, u_int, u_int8_t *, u_int);

/* DEBUG crap... */
void ubsec_dump_pb(struct ubsec_pktbuf *);
void ubsec_dump_mcr(struct ubsec_mcr *);
void ubsec_dump_ctx2(struct ubsec_ctx_keyop *);

a114 12
const struct pci_matchid ubsec_devices[] = {
	{ PCI_VENDOR_BLUESTEEL, PCI_PRODUCT_BLUESTEEL_5501 },
	{ PCI_VENDOR_BLUESTEEL, PCI_PRODUCT_BLUESTEEL_5601 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5801 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5802 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5805 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5820 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5821 },
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5822 },
	{ PCI_VENDOR_SUN, PCI_PRODUCT_SUN_SCA1K },
};

d121 12
a132 2
	return (pci_matchbyid((struct pci_attach_args *)aux, ubsec_devices,
	    sizeof(ubsec_devices)/sizeof(ubsec_devices[0])));
a147 2
	int algs[CRYPTO_ALGORITHM_MAX + 1];
	int kalgs[CRK_ALGORITHM_MAX + 1];
a152 1
	SIMPLEQ_INIT(&sc->sc_q2free);
d155 5
a159 9

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601)
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG;

	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	    (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5802 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805))
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG;
d163 2
a164 13
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822))
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY;

	if ((PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BROADCOM &&
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5821) ||
	    (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_SUN &&
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_SCA1K)) {
		sc->sc_statmask |= BS_STAT_MCR1_ALLEMPTY |
		    BS_STAT_MCR2_ALLEMPTY;
		sc->sc_flags |= UBS_FLAGS_KEY | UBS_FLAGS_RNG |
		    UBS_FLAGS_LONGCTX | UBS_FLAGS_HWNORM | UBS_FLAGS_BIGKEY;
	}
d220 1
a220 1
			printf(": can't allocate queue buffers\n");
d226 1
a226 1
			printf(": can't allocate dma buffers\n");
d238 5
a242 7
	bzero(algs, sizeof(algs));
	algs[CRYPTO_3DES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_DES_CBC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_MD5_HMAC] = CRYPTO_ALG_FLAG_SUPPORTED;
	algs[CRYPTO_SHA1_HMAC] = CRYPTO_ALG_FLAG_SUPPORTED;
	crypto_register(sc->sc_cid, algs, ubsec_newsession,
	    ubsec_freesession, ubsec_process);
d259 1
a259 2
	printf(": %s", intrstr);

d261 1
a261 1
	if (sc->sc_flags & UBS_FLAGS_RNG) {
d293 1
a293 13
	if (sc->sc_flags & UBS_FLAGS_KEY) {
		sc->sc_statmask |= BS_STAT_MCR2_DONE;

		bzero(kalgs, sizeof(kalgs));
		kalgs[CRK_MOD_EXP] = CRYPTO_ALG_FLAG_SUPPORTED;
#if 0
		kalgs[CRK_MOD_EXP_CRT] = CRYPTO_ALG_FLAG_SUPPORTED;
#endif

		crypto_kregister(sc->sc_cid, kalgs, ubsec_kprocess);
	}

	printf("\n");
d356 1
d358 1
a358 1
	 * Check to see if we have any key setups/rng's waiting for us
d360 1
a360 2
	if ((sc->sc_flags & (UBS_FLAGS_KEY|UBS_FLAGS_RNG)) &&
	    (stat & BS_STAT_MCR2_DONE)) {
d389 1
d509 1
a509 2
		printf("feed: q->chip %p %08x\n", q,
		    (u_int32_t)q->q_dma->d_alloc.dma_paddr);
d677 1
a677 1
	u_int32_t sid = ((u_int32_t)tid) & 0xffffffff;
d693 1
a693 1
	int card, err = 0, i, j, s, nicealign;
d979 3
a981 3
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_addr,
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_len,
		    dmap->d_dma->d_mcr.mcr_opktbuf.pb_next);
d1189 1
a1189 1
	crypto_done(crp);
d1297 1
a1334 1
	struct cryptkop *krp;
d1341 1
a1341 3
	switch (q->q_type) {
#ifndef UBSEC_NO_RNG
	case UBS_CTXOP_RNGSHA1:
a1355 70
#endif
	case UBS_CTXOP_MODEXP: {
		struct ubsec_q2_modexp *me = (struct ubsec_q2_modexp *)q;
		u_int rlen, clen;

		krp = me->me_krp;
		rlen = (me->me_modbits + 7) / 8;
		clen = (krp->krp_param[krp->krp_iparams].crp_nbits + 7) / 8;

		bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
		    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
		    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
		    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
		    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);

		if (clen < rlen)
			krp->krp_status = E2BIG;
		else {
			if (sc->sc_flags & UBS_FLAGS_HWNORM) {
				bzero(krp->krp_param[krp->krp_iparams].crp_p,
				    (krp->krp_param[krp->krp_iparams].crp_nbits
					+ 7) / 8);
				bcopy(me->me_C.dma_vaddr,
				    krp->krp_param[krp->krp_iparams].crp_p,
				    (me->me_modbits + 7) / 8);
			} else
				ubsec_kshift_l(me->me_shiftbits,
				    me->me_C.dma_vaddr, me->me_normbits,
				    krp->krp_param[krp->krp_iparams].crp_p,
				    krp->krp_param[krp->krp_iparams].crp_nbits);
		}
		crypto_kdone(krp);

		/* bzero all potentially sensitive data */
		bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
		bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
		bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
		bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);

		/* Can't free here, so put us on the free list. */
		SIMPLEQ_INSERT_TAIL(&sc->sc_q2free, &me->me_q, q_next);
		break;
	}
	case UBS_CTXOP_RSAPRIV: {
		struct ubsec_q2_rsapriv *rp = (struct ubsec_q2_rsapriv *)q;
		u_int len;

		krp = rp->rpr_krp;
		bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map, 0,
		    rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgout.dma_map, 0,
		    rp->rpr_msgout.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);

		len = (krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_nbits + 7) / 8;
		bcopy(rp->rpr_msgout.dma_vaddr,
		    krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT].crp_p, len);

		crypto_kdone(krp);

		bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
		bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
		bzero(rp->rpr_q.q_ctx.dma_vaddr, rp->rpr_q.q_ctx.dma_size);

		/* Can't free here, so put us on the free list. */
		SIMPLEQ_INSERT_TAIL(&sc->sc_q2free, &rp->rpr_q, q_next);
		break;
	}
a1362 1
#ifndef UBSEC_NO_RNG
d1397 1
a1397 2
	ctx->rbp_op = htole16(UBS_CTXOP_RNGSHA1);
	rng->rng_q.q_type = UBS_CTXOP_RNGSHA1;
d1402 1
a1402 1
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rng->rng_q, q_next);
d1494 2
a1495 2
ubsec_init_board(sc)
	struct ubsec_softc *sc;
d1497 3
a1499 15
	u_int32_t ctrl;

	ctrl = READ_REG(sc, BS_CTRL);
	ctrl &= ~(BS_CTRL_BE32 | BS_CTRL_BE64);
	ctrl |= BS_CTRL_LITTLE_ENDIAN | BS_CTRL_MCR1INT;

	if (sc->sc_flags & UBS_FLAGS_KEY)
		ctrl |= BS_CTRL_MCR2INT;
	else
		ctrl &= ~BS_CTRL_MCR2INT;

	if (sc->sc_flags & UBS_FLAGS_HWNORM)
		ctrl &= ~BS_CTRL_SWNORM;

	WRITE_REG(sc, BS_CTRL, ctrl);
d1506 2
a1507 2
ubsec_init_pciregs(pa)
	struct pci_attach_args *pa;
d1512 9
d1575 1
a1575 1
			crypto_done(crp);
d1592 1
a1592 1
	crypto_done(crp);
a1622 822
}

struct ubsec_softc *
ubsec_kfind(krp)
	struct cryptkop *krp;
{
	struct ubsec_softc *sc;
	int i;

	for (i = 0; i < ubsec_cd.cd_ndevs; i++) {
		sc = ubsec_cd.cd_devs[i];
		if (sc == NULL)
			continue;
		if (sc->sc_cid == krp->krp_hid)
			return (sc);
	}
	return (NULL);
}

void
ubsec_kfree(sc, q)
	struct ubsec_softc *sc;
	struct ubsec_q2 *q;
{
	switch (q->q_type) {
	case UBS_CTXOP_MODEXP: {
		struct ubsec_q2_modexp *me = (struct ubsec_q2_modexp *)q;

		ubsec_dma_free(sc, &me->me_q.q_mcr);
		ubsec_dma_free(sc, &me->me_q.q_ctx);
		ubsec_dma_free(sc, &me->me_M);
		ubsec_dma_free(sc, &me->me_E);
		ubsec_dma_free(sc, &me->me_C);
		ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF);
		break;
	}
	case UBS_CTXOP_RSAPRIV: {
		struct ubsec_q2_rsapriv *rp = (struct ubsec_q2_rsapriv *)q;

		ubsec_dma_free(sc, &rp->rpr_q.q_mcr);
		ubsec_dma_free(sc, &rp->rpr_q.q_ctx);
		ubsec_dma_free(sc, &rp->rpr_msgin);
		ubsec_dma_free(sc, &rp->rpr_msgout);
		free(rp, M_DEVBUF);
		break;
	}
	default:
		printf("%s: invalid kfree 0x%x\n", sc->sc_dv.dv_xname,
		    q->q_type);
		break;
	}
}

int
ubsec_kprocess(krp)
	struct cryptkop *krp;
{
	struct ubsec_softc *sc;
	int r;

	if (krp == NULL || krp->krp_callback == NULL)
		return (EINVAL);
	if ((sc = ubsec_kfind(krp)) == NULL)
		return (EINVAL);

	while (!SIMPLEQ_EMPTY(&sc->sc_q2free)) {
		struct ubsec_q2 *q;

		q = SIMPLEQ_FIRST(&sc->sc_q2free);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_q2free, q, q_next);
		ubsec_kfree(sc, q);
	}

	switch (krp->krp_op) {
	case CRK_MOD_EXP:
		if (sc->sc_flags & UBS_FLAGS_HWNORM)
			r = ubsec_kprocess_modexp_hw(sc, krp);
		else
			r = ubsec_kprocess_modexp_sw(sc, krp);
		break;
	case CRK_MOD_EXP_CRT:
		r = ubsec_kprocess_rsapriv(sc, krp);
		break;
	default:
		printf("%s: kprocess: invalid op 0x%x\n",
		    sc->sc_dv.dv_xname, krp->krp_op);
		krp->krp_status = EOPNOTSUPP;
		crypto_kdone(krp);
		r = 0;
	}
	return (r);
}

/*
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N] (sw normalization)
 */
int
ubsec_kprocess_modexp_sw(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
{
	struct ubsec_q2_modexp *me;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_modexp *ctx;
	struct ubsec_pktbuf *epb;
	int err = 0, s;
	u_int nbits, normbits, mbits, shiftbits, ebits;

	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
	if (me == NULL) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me, sizeof *me);
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;

	nbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_N]);
	if (nbits <= 512)
		normbits = 512;
	else if (nbits <= 768)
		normbits = 768;
	else if (nbits <= 1024)
		normbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 1536)
		normbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 2048)
		normbits = 2048;
	else {
		err = E2BIG;
		goto errout;
	}

	shiftbits = normbits - nbits;

	me->me_modbits = nbits;
	me->me_shiftbits = shiftbits;
	me->me_normbits = normbits;

	/* Sanity check: result bits must be >= true modulus bits. */
	if (krp->krp_param[krp->krp_iparams].crp_nbits < nbits) {
		err = ERANGE;
		goto errout;
	}

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &me->me_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_modexp),
	    &me->me_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}

	mbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_M]);
	if (mbits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_M, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_M].crp_p, mbits,
	    me->me_M.dma_vaddr, normbits);

	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_E, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_E].crp_p, ebits,
	    me->me_E.dma_vaddr, normbits);

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktbuf),
	    &me->me_epb, 0)) {
		err = ENOMEM;
		goto errout;
	}
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
	epb->pb_addr = htole32(me->me_E.dma_paddr);
	epb->pb_next = 0;
	epb->pb_len = htole32(normbits / 8);

#ifdef UBSEC_DEBUG
	printf("Epb ");
	ubsec_dump_pb(epb);
#endif

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(me->me_q.q_ctx.dma_paddr);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = 0;

	mcr->mcr_ipktbuf.pb_addr = htole32(me->me_M.dma_paddr);
	mcr->mcr_ipktbuf.pb_len = htole32(normbits / 8);
	mcr->mcr_ipktbuf.pb_next = htole32(me->me_epb.dma_paddr);

	mcr->mcr_opktbuf.pb_addr = htole32(me->me_C.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(normbits / 8);

#ifdef DIAGNOSTIC
	/* Misaligned output buffer will hang the chip. */
	if ((letoh32(mcr->mcr_opktbuf.pb_addr) & 3) != 0)
		panic("%s: modexp invalid addr 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_addr));
	if ((letoh32(mcr->mcr_opktbuf.pb_len) & 3) != 0)
		panic("%s: modexp invalid len 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_len));
#endif

	ctx = (struct ubsec_ctx_modexp *)me->me_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof(*ctx));
	ubsec_kshift_r(shiftbits,
	    krp->krp_param[UBS_MODEXP_PAR_N].crp_p, nbits,
	    ctx->me_N, normbits);
	ctx->me_len = htole16((normbits / 8) + (4 * sizeof(u_int16_t)));
	ctx->me_op = htole16(UBS_CTXOP_MODEXP);
	ctx->me_E_len = htole16(nbits);
	ctx->me_N_len = htole16(nbits);

#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
	ubsec_dump_ctx2((struct ubsec_ctx_keyop *)ctx);
#endif

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
	    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
	    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
	    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
	    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &me->me_q, q_next);
	ubsec_feed2(sc);
	splx(s);

	return (0);

errout:
	if (me != NULL) {
		if (me->me_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_q.q_mcr);
		if (me->me_q.q_ctx.dma_map != NULL) {
			bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
			ubsec_dma_free(sc, &me->me_q.q_ctx);
		}
		if (me->me_M.dma_map != NULL) {
			bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
			ubsec_dma_free(sc, &me->me_M);
		}
		if (me->me_E.dma_map != NULL) {
			bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
			ubsec_dma_free(sc, &me->me_E);
		}
		if (me->me_C.dma_map != NULL) {
			bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
			ubsec_dma_free(sc, &me->me_C);
		}
		if (me->me_epb.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

/*
 * Start computation of cr[C] = (cr[M] ^ cr[E]) mod cr[N] (hw normalization)
 */
int
ubsec_kprocess_modexp_hw(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
{
	struct ubsec_q2_modexp *me;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_modexp *ctx;
	struct ubsec_pktbuf *epb;
	int err = 0, s;
	u_int nbits, normbits, mbits, shiftbits, ebits;

	me = (struct ubsec_q2_modexp *)malloc(sizeof *me, M_DEVBUF, M_NOWAIT);
	if (me == NULL) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me, sizeof *me);
	me->me_krp = krp;
	me->me_q.q_type = UBS_CTXOP_MODEXP;

	nbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_N]);
	if (nbits <= 512)
		normbits = 512;
	else if (nbits <= 768)
		normbits = 768;
	else if (nbits <= 1024)
		normbits = 1024;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 1536)
		normbits = 1536;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && nbits <= 2048)
		normbits = 2048;
	else {
		err = E2BIG;
		goto errout;
	}

	shiftbits = normbits - nbits;

	/* XXX ??? */
	me->me_modbits = nbits;
	me->me_shiftbits = shiftbits;
	me->me_normbits = normbits;

	/* Sanity check: result bits must be >= true modulus bits. */
	if (krp->krp_param[krp->krp_iparams].crp_nbits < nbits) {
		err = ERANGE;
		goto errout;
	}

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &me->me_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)me->me_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_modexp),
	    &me->me_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}

	mbits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_M]);
	if (mbits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_M, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_M.dma_vaddr, normbits / 8);
	bcopy(krp->krp_param[UBS_MODEXP_PAR_M].crp_p,
	    me->me_M.dma_vaddr, (mbits + 7) / 8);

	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_C, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_C.dma_vaddr, me->me_C.dma_size);

	ebits = ubsec_ksigbits(&krp->krp_param[UBS_MODEXP_PAR_E]);
	if (ebits > nbits) {
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, normbits / 8, &me->me_E, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(me->me_E.dma_vaddr, normbits / 8);
	bcopy(krp->krp_param[UBS_MODEXP_PAR_E].crp_p,
	    me->me_E.dma_vaddr, (ebits + 7) / 8);

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_pktbuf),
	    &me->me_epb, 0)) {
		err = ENOMEM;
		goto errout;
	}
	epb = (struct ubsec_pktbuf *)me->me_epb.dma_vaddr;
	epb->pb_addr = htole32(me->me_E.dma_paddr);
	epb->pb_next = 0;
	epb->pb_len = htole32((ebits + 7) / 8);

#ifdef UBSEC_DEBUG
	printf("Epb ");
	ubsec_dump_pb(epb);
#endif

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(me->me_q.q_ctx.dma_paddr);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = 0;

	mcr->mcr_ipktbuf.pb_addr = htole32(me->me_M.dma_paddr);
	mcr->mcr_ipktbuf.pb_len = htole32(normbits / 8);
	mcr->mcr_ipktbuf.pb_next = htole32(me->me_epb.dma_paddr);

	mcr->mcr_opktbuf.pb_addr = htole32(me->me_C.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(normbits / 8);

#ifdef DIAGNOSTIC
	/* Misaligned output buffer will hang the chip. */
	if ((letoh32(mcr->mcr_opktbuf.pb_addr) & 3) != 0)
		panic("%s: modexp invalid addr 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_addr));
	if ((letoh32(mcr->mcr_opktbuf.pb_len) & 3) != 0)
		panic("%s: modexp invalid len 0x%x",
		    sc->sc_dv.dv_xname, letoh32(mcr->mcr_opktbuf.pb_len));
#endif

	ctx = (struct ubsec_ctx_modexp *)me->me_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof(*ctx));
	bcopy(krp->krp_param[UBS_MODEXP_PAR_N].crp_p, ctx->me_N,
	    (nbits + 7) / 8);
	ctx->me_len = htole16((normbits / 8) + (4 * sizeof(u_int16_t)));
	ctx->me_op = htole16(UBS_CTXOP_MODEXP);
	ctx->me_E_len = htole16(ebits);
	ctx->me_N_len = htole16(nbits);

#ifdef UBSEC_DEBUG
	ubsec_dump_mcr(mcr);
	ubsec_dump_ctx2((struct ubsec_ctx_keyop *)ctx);
#endif

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, me->me_M.dma_map,
	    0, me->me_M.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_E.dma_map,
	    0, me->me_E.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, me->me_C.dma_map,
	    0, me->me_C.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	bus_dmamap_sync(sc->sc_dmat, me->me_epb.dma_map,
	    0, me->me_epb.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &me->me_q, q_next);
	ubsec_feed2(sc);
	splx(s);

	return (0);

errout:
	if (me != NULL) {
		if (me->me_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_q.q_mcr);
		if (me->me_q.q_ctx.dma_map != NULL) {
			bzero(me->me_q.q_ctx.dma_vaddr, me->me_q.q_ctx.dma_size);
			ubsec_dma_free(sc, &me->me_q.q_ctx);
		}
		if (me->me_M.dma_map != NULL) {
			bzero(me->me_M.dma_vaddr, me->me_M.dma_size);
			ubsec_dma_free(sc, &me->me_M);
		}
		if (me->me_E.dma_map != NULL) {
			bzero(me->me_E.dma_vaddr, me->me_E.dma_size);
			ubsec_dma_free(sc, &me->me_E);
		}
		if (me->me_C.dma_map != NULL) {
			bzero(me->me_C.dma_vaddr, me->me_C.dma_size);
			ubsec_dma_free(sc, &me->me_C);
		}
		if (me->me_epb.dma_map != NULL)
			ubsec_dma_free(sc, &me->me_epb);
		free(me, M_DEVBUF);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

int
ubsec_kprocess_rsapriv(sc, krp)
	struct ubsec_softc *sc;
	struct cryptkop *krp;
{
	struct ubsec_q2_rsapriv *rp = NULL;
	struct ubsec_mcr *mcr;
	struct ubsec_ctx_rsapriv *ctx;
	int err = 0, s;
	u_int padlen, msglen;

	msglen = ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_P]);
	padlen = ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_Q]);
	if (msglen > padlen)
		padlen = msglen;

	if (padlen <= 256)
		padlen = 256;
	else if (padlen <= 384)
		padlen = 384;
	else if (padlen <= 512)
		padlen = 512;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && padlen <= 768)
		padlen = 768;
	else if (sc->sc_flags & UBS_FLAGS_BIGKEY && padlen <= 1024)
		padlen = 1024;
	else {
		err = E2BIG;
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_DP]) > padlen) {
		err = E2BIG;
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_DQ]) > padlen) {
		err = E2BIG;
		goto errout;
	}

	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_PINV]) > padlen) {
		err = E2BIG;
		goto errout;
	}

	rp = (struct ubsec_q2_rsapriv *)malloc(sizeof *rp, M_DEVBUF, M_NOWAIT);
	if (rp == NULL)
		return (ENOMEM);
	bzero(rp, sizeof *rp);
	rp->rpr_krp = krp;
	rp->rpr_q.q_type = UBS_CTXOP_RSAPRIV;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_mcr),
	    &rp->rpr_q.q_mcr, 0)) {
		err = ENOMEM;
		goto errout;
	}
	mcr = (struct ubsec_mcr *)rp->rpr_q.q_mcr.dma_vaddr;

	if (ubsec_dma_malloc(sc, sizeof(struct ubsec_ctx_rsapriv),
	    &rp->rpr_q.q_ctx, 0)) {
		err = ENOMEM;
		goto errout;
	}
	ctx = (struct ubsec_ctx_rsapriv *)rp->rpr_q.q_ctx.dma_vaddr;
	bzero(ctx, sizeof *ctx);

	/* Copy in p */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_P].crp_p,
	    &ctx->rpr_buf[0 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_P].crp_nbits + 7) / 8);

	/* Copy in q */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_p,
	    &ctx->rpr_buf[1 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_Q].crp_nbits + 7) / 8);

	/* Copy in dp */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_p,
	    &ctx->rpr_buf[2 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_DP].crp_nbits + 7) / 8);

	/* Copy in dq */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_p,
	    &ctx->rpr_buf[3 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_DQ].crp_nbits + 7) / 8);

	/* Copy in pinv */
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_p,
	    &ctx->rpr_buf[4 * (padlen / 8)],
	    (krp->krp_param[UBS_RSAPRIV_PAR_PINV].crp_nbits + 7) / 8);

	msglen = padlen * 2;

	/* Copy in input message (aligned buffer/length). */
	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_MSGIN]) > msglen) {
		/* Is this likely? */
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, (msglen + 7) / 8, &rp->rpr_msgin, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(rp->rpr_msgin.dma_vaddr, (msglen + 7) / 8);
	bcopy(krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_p,
	    rp->rpr_msgin.dma_vaddr,
	    (krp->krp_param[UBS_RSAPRIV_PAR_MSGIN].crp_nbits + 7) / 8);

	/* Prepare space for output message (aligned buffer/length). */
	if (ubsec_ksigbits(&krp->krp_param[UBS_RSAPRIV_PAR_MSGOUT]) < msglen) {
		/* Is this likely? */
		err = E2BIG;
		goto errout;
	}
	if (ubsec_dma_malloc(sc, (msglen + 7) / 8, &rp->rpr_msgout, 0)) {
		err = ENOMEM;
		goto errout;
	}
	bzero(rp->rpr_msgout.dma_vaddr, (msglen + 7) / 8);

	mcr->mcr_pkts = htole16(1);
	mcr->mcr_flags = 0;
	mcr->mcr_cmdctxp = htole32(rp->rpr_q.q_ctx.dma_paddr);
	mcr->mcr_ipktbuf.pb_addr = htole32(rp->rpr_msgin.dma_paddr);
	mcr->mcr_ipktbuf.pb_next = 0;
	mcr->mcr_ipktbuf.pb_len = htole32(rp->rpr_msgin.dma_size);
	mcr->mcr_reserved = 0;
	mcr->mcr_pktlen = htole16(msglen);
	mcr->mcr_opktbuf.pb_addr = htole32(rp->rpr_msgout.dma_paddr);
	mcr->mcr_opktbuf.pb_next = 0;
	mcr->mcr_opktbuf.pb_len = htole32(rp->rpr_msgout.dma_size);

#ifdef DIAGNOSTIC
	if (rp->rpr_msgin.dma_paddr & 3 || rp->rpr_msgin.dma_size & 3) {
		panic("%s: rsapriv: invalid msgin %p(0x%x)",
		    sc->sc_dv.dv_xname, rp->rpr_msgin.dma_paddr,
		    rp->rpr_msgin.dma_size);
	}
	if (rp->rpr_msgout.dma_paddr & 3 || rp->rpr_msgout.dma_size & 3) {
		panic("%s: rsapriv: invalid msgout %p(0x%x)",
		    sc->sc_dv.dv_xname, rp->rpr_msgout.dma_paddr,
		    rp->rpr_msgout.dma_size);
	}
#endif

	ctx->rpr_len = (sizeof(u_int16_t) * 4) + (5 * (padlen / 8));
	ctx->rpr_op = htole16(UBS_CTXOP_RSAPRIV);
	ctx->rpr_q_len = htole16(padlen);
	ctx->rpr_p_len = htole16(padlen);

	/*
	 * ubsec_feed2 will sync mcr and ctx, we just need to sync
	 * everything else.
	 */
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgin.dma_map,
	    0, rp->rpr_msgin.dma_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, rp->rpr_msgout.dma_map,
	    0, rp->rpr_msgout.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD);

	/* Enqueue and we're done... */
	s = splnet();
	SIMPLEQ_INSERT_TAIL(&sc->sc_queue2, &rp->rpr_q, q_next);
	ubsec_feed2(sc);
	splx(s);
	return (0);

errout:
	if (rp != NULL) {
		if (rp->rpr_q.q_mcr.dma_map != NULL)
			ubsec_dma_free(sc, &rp->rpr_q.q_mcr);
		if (rp->rpr_msgin.dma_map != NULL) {
			bzero(rp->rpr_msgin.dma_vaddr, rp->rpr_msgin.dma_size);
			ubsec_dma_free(sc, &rp->rpr_msgin);
		}
		if (rp->rpr_msgout.dma_map != NULL) {
			bzero(rp->rpr_msgout.dma_vaddr, rp->rpr_msgout.dma_size);
			ubsec_dma_free(sc, &rp->rpr_msgout);
		}
		free(rp, M_DEVBUF);
	}
	krp->krp_status = err;
	crypto_kdone(krp);
	return (0);
}

void
ubsec_dump_pb(struct ubsec_pktbuf *pb)
{
	printf("addr 0x%x (0x%x) next 0x%x\n",
	    pb->pb_addr, pb->pb_len, pb->pb_next);
}

void
ubsec_dump_ctx2(struct ubsec_ctx_keyop *c)
{
	printf("CTX (0x%x):\n", c->ctx_len);
	switch (letoh16(c->ctx_op)) {
	case UBS_CTXOP_RNGBYPASS:
	case UBS_CTXOP_RNGSHA1:
		break;
	case UBS_CTXOP_MODEXP:
	{
		struct ubsec_ctx_modexp *cx = (void *)c;
		int i, len;

		printf(" Elen %u, Nlen %u\n",
		    letoh16(cx->me_E_len), letoh16(cx->me_N_len));
		len = (cx->me_N_len + 7)/8;
		for (i = 0; i < len; i++)
			printf("%s%02x", (i == 0) ? " N: " : ":", cx->me_N[i]);
		printf("\n");
		break;
	}
	default:
		printf("unknown context: %x\n", c->ctx_op);
	}
	printf("END CTX\n");
}

void
ubsec_dump_mcr(struct ubsec_mcr *mcr)
{
	struct ubsec_mcr_add *ma;
	int i;

	printf("MCR:\n");
	printf(" pkts: %u, flags 0x%x\n",
	    letoh16(mcr->mcr_pkts), letoh16(mcr->mcr_flags));
	ma = (struct ubsec_mcr_add *)&mcr->mcr_cmdctxp;
	for (i = 0; i < letoh16(mcr->mcr_pkts); i++) {
		printf(" %d: ctx 0x%x len 0x%x rsvd 0x%x\n", i,
		    letoh32(ma->mcr_cmdctxp), letoh16(ma->mcr_pktlen),
		    letoh16(ma->mcr_reserved));
		printf(" %d: ipkt ", i);
		ubsec_dump_pb(&ma->mcr_ipktbuf);
		printf(" %d: opkt ", i);
		ubsec_dump_pb(&ma->mcr_opktbuf);
		ma++;
	}
	printf("END MCR\n");
}

/*
 * Return the number of significant bits of a big number.
 */
int
ubsec_ksigbits(cr)
	struct crparam *cr;
{
	u_int plen = (cr->crp_nbits + 7) / 8;
	int i, sig = plen * 8;
	u_int8_t c, *p = cr->crp_p;

	for (i = plen - 1; i >= 0; i--) {
		c = p[i];
		if (c != 0) {
			while ((c & 0x80) == 0) {
				sig--;
				c <<= 1;
			}
			break;
		}
		sig -= 8;
	}
	return (sig);
}

void
ubsec_kshift_r(shiftbits, src, srcbits, dst, dstbits)
	u_int shiftbits, srcbits, dstbits;
	u_int8_t *src, *dst;
{
	u_int slen, dlen;
	int i, si, di, n;

	slen = (srcbits + 7) / 8;
	dlen = (dstbits + 7) / 8;

	for (i = 0; i < slen; i++)
		dst[i] = src[i];
	for (i = 0; i < dlen - slen; i++)
		dst[slen + i] = 0;

	n = shiftbits / 8;
	if (n != 0) {
		si = dlen - n - 1;
		di = dlen - 1;
		while (si >= 0)
			dst[di--] = dst[si--];
		while (di >= 0)
			dst[di--] = 0;
	}

	n = shiftbits % 8;
	if (n != 0) {
		for (i = dlen - 1; i > 0; i--)
			dst[i] = (dst[i] << n) |
			    (dst[i - 1] >> (8 - n));
		dst[0] = dst[0] << n;
	}
}

void
ubsec_kshift_l(shiftbits, src, srcbits, dst, dstbits)
	u_int shiftbits, srcbits, dstbits;
	u_int8_t *src, *dst;
{
	int slen, dlen, i, n;

	slen = (srcbits + 7) / 8;
	dlen = (dstbits + 7) / 8;

	n = shiftbits / 8;
	for (i = 0; i < slen; i++)
		dst[i] = src[i + n];
	for (i = 0; i < dlen - slen; i++)
		dst[slen + i] = 0;

	n = shiftbits % 8;
	if (n != 0) {
		for (i = 0; i < (dlen - 1); i++)
			dst[i] = (dst[i] >> n) | (dst[i + 1] << (8 - n));
		dst[dlen - 1] = dst[dlen - 1] >> n;
	}
@


1.49.2.9
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.49.2.8 2003/03/28 00:38:25 niklas Exp $	*/
d98 1
a98 1
void	ubsec_feed(struct ubsec_softc *);
d101 1
a101 1
void	ubsec_feed2(struct ubsec_softc *);
a144 1
	{ PCI_VENDOR_SUN, PCI_PRODUCT_SUN_5821 },
d199 1
a199 2
	     (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_SCA1K ||
	      PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_SUN_5821))) {
d392 1
a392 1
				if(q->q_stacked_mcr[i])
d394 2
a395 1
				else
d397 1
d400 1
d467 1
a467 1
void
d490 1
a490 1
		return;
d537 1
a537 1
	return;
d570 1
a1256 3
	ubsecstats.hst_opackets++;
	ubsecstats.hst_obytes += dmap->d_alloc.dma_size;

d1275 1
d1355 1
a1355 1
void
d1378 1
@


1.49.2.10
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.49.2.9 2003/05/13 19:35:09 ho Exp $	*/
d8 2
d18 5
@


1.49.2.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d51 2
a136 1
	{ PCI_VENDOR_BROADCOM, PCI_PRODUCT_BROADCOM_5823 },
d142 4
a145 1
ubsec_probe(struct device *parent, void *match, void *aux)
d152 3
a154 1
ubsec_attach(struct device *parent, struct device *self, void *aux)
d186 1
a186 2
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5822 ||
	     PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5823))
d350 2
a351 1
ubsec_intr(void *arg)
d460 2
a461 1
ubsec_feed(struct ubsec_softc *sc)
d533 1
a533 2
		if ((stat = READ_REG(sc, BS_STAT)) &
		    (BS_STAT_MCR1_FULL | BS_STAT_DMAERR)) {
d570 3
a572 1
ubsec_newsession(u_int32_t *sidp, struct cryptoini *cri)
d718 2
a719 1
ubsec_freesession(u_int64_t tid)
d735 2
a736 1
ubsec_process(struct cryptop *crp)
d861 1
a861 1
					    8, ctx.pc_iv);
d865 1
a865 1
					    8, ctx.pc_iv);
d1021 1
a1021 2
		dmap->d_dma->d_mcr.mcr_opktbuf.pb_next =
		    htole32(dmap->d_alloc.dma_paddr +
d1240 3
a1242 1
ubsec_callback(struct ubsec_softc *sc, struct ubsec_q *q)
d1296 1
a1296 1
			    dmap->d_dma->d_macbuf);
d1307 3
a1309 1
ubsec_mcopy(struct mbuf *srcm, struct mbuf *dstm, int hoffset, int toffset)
d1349 2
a1350 1
ubsec_feed2(struct ubsec_softc *sc)
d1377 3
a1379 1
ubsec_callback2(struct ubsec_softc *sc, struct ubsec_q2 *q)
d1400 1
a1400 1
			add_true_randomness(*p);
d1484 2
a1485 1
ubsec_rng(void *vsc)
d1541 5
a1545 2
ubsec_dma_malloc(struct ubsec_softc *sc, bus_size_t size,
    struct ubsec_dma_alloc *dma, int mapflags)
d1581 3
a1583 1
ubsec_dma_free(struct ubsec_softc *sc, struct ubsec_dma_alloc *dma)
d1596 2
a1597 1
ubsec_reset_board(struct ubsec_softc *sc)
d1615 2
a1616 1
ubsec_init_board(struct ubsec_softc *sc)
d1639 2
a1640 1
ubsec_init_pciregs(struct pci_attach_args *pa)
d1658 1
a1658 1
 * It is assumed that the caller is in splnet()
d1661 2
a1662 1
ubsec_cleanchip(struct ubsec_softc *sc)
d1725 2
a1726 1
ubsec_totalreset(struct ubsec_softc *sc)
d1734 2
a1735 1
ubsec_dmamap_aligned(bus_dmamap_t map)
d1750 2
a1751 1
ubsec_kfind(struct cryptkop *krp)
d1767 3
a1769 1
ubsec_kfree(struct ubsec_softc *sc, struct ubsec_q2 *q)
d1802 2
a1803 1
ubsec_kprocess(struct cryptkop *krp)
d1845 3
a1847 1
ubsec_kprocess_modexp_sw(struct ubsec_softc *sc, struct cryptkop *krp)
d2046 3
a2048 1
ubsec_kprocess_modexp_hw(struct ubsec_softc *sc, struct cryptkop *krp)
d2244 3
a2246 1
ubsec_kprocess_rsapriv(struct ubsec_softc *sc, struct cryptkop *krp)
d2491 2
a2492 1
ubsec_ksigbits(struct crparam *cr)
d2513 3
a2515 2
ubsec_kshift_r(u_int shiftbits, u_int8_t *src, u_int srcbits,
    u_int8_t *dst, u_int dstbits)
d2548 3
a2550 2
ubsec_kshift_l(u_int shiftbits, u_int8_t *src, u_int srcbits,
    u_int8_t *dst, u_int dstbits)
@


1.49.2.12
log
@Merge with the trunk
@
text
@d54 1
a54 1
#include <crypto/md5.h>
d291 1
a291 1
	printf(": 3DES MD5 SHA1");
d320 1
a320 1
		printf(" RNG");
a335 1
		printf(" PK");
d338 1
a338 1
	printf(", %s\n", intrstr);
d372 1
a372 1
			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q_next);
d420 1
a420 1
			SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip2, q_next);
d489 1
a489 1
	SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q_next);
d507 1
a507 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q_next);
d552 1
a552 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q_next);
d761 1
a761 1
	SIMPLEQ_REMOVE_HEAD(&sc->sc_freequeue, q_next);
d1353 1
a1353 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue2, q_next);
d1642 1
a1642 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q_next);
d1785 1
a1785 1
		SIMPLEQ_REMOVE_HEAD(&sc->sc_q2free, q_next);
@


1.48
log
@use real uio
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.47 2001/05/13 15:56:09 jason Exp $	*/
d308 1
d310 1
a331 1
#endif
d338 1
@


1.47
log
@use criov_copydata for grabbing iv for next packet
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.46 2001/05/13 15:39:27 deraadt Exp $	*/
d602 2
a603 2
		q->q_src = (struct criov *)crp->crp_buf;
		q->q_dst = (struct criov *)crp->crp_buf;
d1041 1
a1041 1
				criov_copydata((struct criov *)crp->crp_buf,
@


1.46
log
@initial cut at /dev/crypto support.  takes original mbuf "try, and discard
if we fail" semantics and extends to two varients of data movement: mbuf,
or an iovec style block.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.45 2001/05/13 01:20:02 jason Exp $	*/
d1041 3
a1043 3
				/* XXX need last 8 bytes of encrypted data, and shove
				 * it into ses_iv */
				/* MISSING bcopy */
@


1.45
log
@Initial support for Broadcom 5820, which is very much like the 5805 except
that the packet context structure for ipsec has changed (added two fields
and, annoyingly, rearranged several of them).  The MCR2 operations (only
RNG is used at this point) are supported, too.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.44 2001/04/29 00:37:11 jason Exp $	*/
d601 3
d678 13
a690 3
			if ((enccrd->crd_flags & CRD_F_IV_PRESENT) == 0)
				m_copyback(q->q_src_m, enccrd->crd_inject,
				    8, (caddr_t)q->q_ctx.pc_iv);
d696 1
a696 1
			else
d699 7
d768 6
a773 2
	q->q_src_l = mbuf2pages(q->q_src_m, &q->q_src_npa, q->q_src_packp,
	    q->q_src_packl, MAX_SCATTER, &nicealign);
d851 4
a854 1
		if (!nicealign) {
d906 6
a911 2
		q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa,
		    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
d1008 1
a1008 1
		if (q->q_src_m != q->q_dst_m)
d1036 9
a1044 3
			m_copydata((struct mbuf *)crp->crp_buf,
			    crd->crd_skip + crd->crd_len - 8, 8,
			    (caddr_t)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
d1053 5
a1057 2
		m_copyback((struct mbuf *)crp->crp_buf,
		    crd->crd_inject, 12, (caddr_t)q->q_macbuf);
@


1.44
log
@When expanding the session table, only copy the number of sessions already
there to the new table; from stephen@@etunnels.com (PR 1801).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.43 2001/04/06 16:27:45 jason Exp $	*/
d111 2
a112 4
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5501)
		return (1);
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601)
d115 2
a116 1
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)
d143 7
a149 5
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)) {
		sc->sc_statmask |= BS_STAT_MCR2_DONE;
		sc->sc_5601 = 1;
	}
	
d198 2
a199 5
	WRITE_REG(sc, BS_CTRL,
	    READ_REG(sc, BS_CTRL) | BS_CTRL_MCR1INT | BS_CTRL_DMAERR |
	    (sc->sc_5601 ? BS_CTRL_MCR2INT : 0));

	if (sc->sc_5601) {
d202 1
d205 4
d282 1
a282 1
	if (sc->sc_5601 && (stat & BS_STAT_MCR2_DONE)) {
a615 1
	q->q_mcr->mcr_cmdctxp = vtophys(&q->q_ctx);
d948 18
d1127 1
a1161 1
	sc->sc_nqueue2++;
d1171 3
@


1.43
log
@typo in error message
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.42 2001/03/28 20:03:00 angelos Exp $	*/
d448 1
a448 1
			bcopy(sc->sc_sessions, ses, (sesn + 1) *
@


1.42
log
@Allow tdbi's to appear in mbufs throughout the stack; this allows
security properties of the packets to be pushed up to the application
(not done yet). Eventually, this will be turned into a packet
attributes framework.

Make sure tdbi's are free'd/cleared properly whenever drivers (or NFS)
does weird things with mbufs.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.41 2001/03/25 07:59:25 csapuntz Exp $	*/
d176 1
a176 1
		printf(": couldn't establish interrupt\n");
@


1.41
log
@Fix potential dangling mbuf and potential double free. Thanks to Dawson and team
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.40 2001/02/02 01:00:07 jason Exp $	*/
d842 1
a842 1
				M_COPY_PKTHDR(m, q->q_src_m);
@


1.40
log
@The read/write indication bit in DMAERR reg is bit 1, not bit 0
also, add a mask for the address portion of DMAERR and use it
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.39 2001/01/31 05:14:02 jason Exp $	*/
d595 8
a605 1
		free(q, M_DEVBUF);
a615 8

	if (crp->crp_flags & CRYPTO_F_IMBUF) {
		q->q_src_m = (struct mbuf *)crp->crp_buf;
		q->q_dst_m = (struct mbuf *)crp->crp_buf;
	} else {
		err = EINVAL;
		goto errout;	/* XXX only handle mbufs right now */
	}
@


1.39
log
@before copying the packet header, make sure we actually got the mbuf
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.38 2001/01/29 04:01:44 jason Exp $	*/
d295 1
a295 1
		    a & ~BS_ERR_READ);
@


1.38
log
@grab rng stuff more often (now 6400bytes/sec)
document the other mcr2 operations
and fix a printf (luckily it's never been called =)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.37 2001/01/29 00:39:20 jason Exp $	*/
d832 1
a833 2
				M_COPY_PKTHDR(m, q->q_src_m);
				len = MHLEN;
d835 1
a836 1
				len = MLEN;
d842 2
@


1.37
log
@- add infrastructure for dealing with the key generator (MCR2)
- add support for the onboard rng using that structure
- add a interrupt status mask (differs for 5501 and 5601)
- reorganize slightly to take into account that MCR1 isn't the only reason
  for interrupts.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.36 2001/01/11 18:56:50 deraadt Exp $	*/
d203 1
a203 1
		timeout_add(&sc->sc_rngto, hz);
d1073 1
d1087 1
d1091 2
a1092 1
		printf("%s: unknown ctx op: %x\n", ctx->ctx_op);
d1145 2
d1148 4
a1151 1
	timeout_add(&sc->sc_rngto, hz);
@


1.36
log
@oops
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.34 2000/11/17 05:18:41 angelos Exp $	*/
d90 3
d137 2
d140 1
d144 2
a145 1
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805))
d147 2
a148 1

d201 5
d216 1
d222 1
a222 1
	stat &= (BS_STAT_MCR1_DONE | BS_STAT_MCR2_DONE | BS_STAT_DMAERR);
d276 13
a295 1
		panic("to let theo see things");
a297 1
	ubsec_feed(sc);
d577 1
a577 1
	if (sc->sc_nqueue == UBS_MAX_NQUEUE) {
d1046 98
@


1.35
log
@move ich to auich at mickey's request
@
text
@a587 3
	} if (crp->crp_flags & CRYPTO_F_IOV) {
		q->q_src = (struct criov *)crp->crp_buf;
		q->q_dst = (struct criov *)crp->crp_buf;
d649 3
a651 14
			if ((enccrd->crd_flags & CRD_F_IV_PRESENT) == 0) {
				if (crp->crp_flags & CRYPTO_F_IMBUF)
					m_copyback(q->q_src_m, enccrd->crd_inject,
					    8, (caddr_t)q->q_ctx.pc_iv);
				else if (crp->crp_flags & CRYPTO_F_IOV) {
					if (crp->crp_iv == NULL) {
						err = EINVAL;
						goto errout;
					}
					bcopy(crp->crp_iv,
					    (caddr_t)q->q_ctx.pc_iv, 8);
				}
			}
			
d657 1
a657 1
			else if (crp->crp_flags & CRYPTO_F_IMBUF)
a659 7
			else if (crp->crp_flags & CRYPTO_F_IOV) {
				if (crp->crp_iv == NULL) {
					err = EINVAL;
					goto errout;
				}
				bcopy(crp->crp_iv, (caddr_t)q->q_ctx.pc_iv, 8);
			}
d722 2
a723 6
	if (crp->crp_flags & CRYPTO_F_IMBUF)
		q->q_src_l = mbuf2pages(q->q_src_m, &q->q_src_npa, q->q_src_packp,
		    q->q_src_packl, MAX_SCATTER, &nicealign);
	else if (crp->crp_flags & CRYPTO_F_IOV)
		q->q_src_l = iov2pages(q->q_src, &q->q_src_npa, q->q_src_packp,
		    q->q_src_packl, MAX_SCATTER, &nicealign);
d801 1
a801 4
		if (!nicealign && (crp->crp_flags & CRYPTO_F_IOV)) {
			err = EINVAL;
			goto errout;
		} else if (!nicealign && (crp->crp_flags & CRYPTO_F_IMBUF)) {
d852 2
a853 6
		if (crp->crp_flags & CRYPTO_F_IMBUF)
			q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa,
			    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
		else if (crp->crp_flags & CRYPTO_F_IOV)
			q->q_dst_l = iov2pages(q->q_dst, &q->q_dst_npa,
			    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
d932 1
a932 1
		if (q->q_dst_m && q->q_src_m != q->q_dst_m)
d960 3
a962 9
			if (crp->crp_flags & CRYPTO_F_IMBUF)
				m_copydata((struct mbuf *)crp->crp_buf,
				    crd->crd_skip + crd->crd_len - 8, 8,
				    (caddr_t)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
			else if (crp->crp_flags & CRYPTO_F_IOV) {
				/* XXX need last 8 bytes of encrypted data, and shove
				 * it into ses_iv */
				/* MISSING bcopy */
			}
d971 2
a972 5
		if (crp->crp_flags & CRYPTO_F_IMBUF)
			m_copyback((struct mbuf *)crp->crp_buf,
			    crd->crd_inject, 12, (caddr_t)q->q_macbuf);
		else if (crp->crp_flags & CRYPTO_F_IOV && crp->crp_mac)
			bcopy((caddr_t)q->q_macbuf, crp->crp_mac, 12);
@


1.34
log
@*HMAC96->*HMAC

Bear in mind, you will need to recompile both isakmpd/ipsecadm and
your kernel --- otherwise things won't work together.

Naturally, all these changes will not be folded into -STABLE, since
they would break binary compatibility.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.33 2000/09/21 04:39:11 jason Exp $	*/
d588 3
d652 14
a665 3
			if ((enccrd->crd_flags & CRD_F_IV_PRESENT) == 0)
				m_copyback(q->q_src_m, enccrd->crd_inject,
				    8, (caddr_t)q->q_ctx.pc_iv);
d671 1
a671 1
			else
d674 7
d743 6
a748 2
	q->q_src_l = mbuf2pages(q->q_src_m, &q->q_src_npa, q->q_src_packp,
	    q->q_src_packl, MAX_SCATTER, &nicealign);
d826 4
a829 1
		if (!nicealign) {
d880 6
a885 2
		q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa,
		    q->q_dst_packp, q->q_dst_packl, MAX_SCATTER, NULL);
d964 1
a964 1
		if (q->q_src_m != q->q_dst_m)
d992 9
a1000 3
			m_copydata((struct mbuf *)crp->crp_buf,
			    crd->crd_skip + crd->crd_len - 8, 8,
			    (caddr_t)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
d1009 5
a1013 2
		m_copyback((struct mbuf *)crp->crp_buf,
		    crd->crd_inject, 12, (caddr_t)q->q_macbuf);
@


1.33
log
@style
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.32 2000/08/19 16:41:01 jason Exp $	*/
d186 2
a187 2
	crypto_register(sc->sc_cid, CRYPTO_MD5_HMAC96, NULL, NULL, NULL);
	crypto_register(sc->sc_cid, CRYPTO_SHA1_HMAC96, NULL, NULL, NULL);
d386 2
a387 2
		if (c->cri_alg == CRYPTO_MD5_HMAC96 ||
		    c->cri_alg == CRYPTO_SHA1_HMAC96) {
d460 1
a460 1
		if (macini->cri_alg == CRYPTO_MD5_HMAC96) {
d481 1
a481 1
		if (macini->cri_alg == CRYPTO_MD5_HMAC96) {
d601 2
a602 2
		if (crd1->crd_alg == CRYPTO_MD5_HMAC96 ||
		    crd1->crd_alg == CRYPTO_SHA1_HMAC96) {
d614 2
a615 2
		if ((crd1->crd_alg == CRYPTO_MD5_HMAC96 ||
		    crd1->crd_alg == CRYPTO_SHA1_HMAC96) &&
d623 2
a624 2
		    (crd2->crd_alg == CRYPTO_MD5_HMAC96 ||
			crd2->crd_alg == CRYPTO_SHA1_HMAC96) &&
d675 1
a675 1
		if (maccrd->crd_alg == CRYPTO_MD5_HMAC96)
d968 2
a969 2
		if (crd->crd_alg != CRYPTO_MD5_HMAC96 &&
		    crd->crd_alg != CRYPTO_SHA1_HMAC96)
@


1.32
log
@- introduce new function ubsec_mcopy() which copies the header and trailer
of one mbuf into another since ubsec doesn't copy data through
- get all of the offsets right for the !nicealign case
- style fixes
Authentication/Encryption on the same packet now appears to work reliably
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.31 2000/08/17 21:44:56 jason Exp $	*/
d267 2
a268 2
		       (a & BS_ERR_READ) ? "read" : "write",
		       a & ~BS_ERR_READ);
d446 1
a446 1
			bcopy(encini->cri_key, &ses->ses_deskey[0], 24);
d656 1
a656 1
				bcopy(enccrd->crd_iv, &q->q_ctx.pc_iv[0], 8);
d712 1
a712 1
		       coffset, stheend, cpskip, cpoffset);
d793 1
a793 1
		    (u_int32_t)vtophys(&q->q_macbuf[0]);
d898 1
a898 2
					pb->pb_next =
					    vtophys(&q->q_macbuf[0]);
@


1.31
log
@use destination lengths when trimming the output packet (cut and pasto)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.30 2000/08/17 13:54:23 jason Exp $	*/
d89 1
d447 1
d457 1
a457 1
                for (i = 0; i < macini->cri_klen / 8; i++)
d535 2
a536 1
	int encoffset = 0, macoffset = 0, sskip, dskip, stheend, dtheend;
a687 1
		 *
d699 1
a699 1
		dskip = enccrd->crd_skip;
d703 1
d711 2
a712 1
		printf("ubs: coffset %d, pktlen %d\n", coffset, stheend);
d715 1
a715 1
		dskip = sskip = macoffset + encoffset;
d717 1
d848 1
d983 39
@


1.30
log
@- auth+enc sorta works: fixup all of the offsets for encryption/authentication
- also check for cases that ubsec cannot handle (and should not happen anyway)
- actually use the destination length to trim the packet in the
output descriptor
- don't copy the IV into the packet unless it doesn't already have one (old
debugging code...)
- style fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.29 2000/08/15 17:50:12 mickey Exp $	*/
d880 1
a880 1
				if (q->q_src_packl[i] > dtheend) {
d884 1
a884 1
					pb->pb_len = q->q_src_packl[i];
d888 1
a888 1
				pb->pb_len = q->q_src_packl[i];
@


1.29
log
@use pci_mapreg_map, deallocate resources on failure in attachment; jason@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.28 2000/08/15 17:39:19 jason Exp $	*/
d640 1
a640 1
				bcopy(enccrd->crd_iv, &q->q_ctx.pc_iv[0], 8);
a645 3
			m_copyback(q->q_src_m, enccrd->crd_inject, 8,
			    (caddr_t)&q->q_ctx.pc_iv);

d648 1
a648 1
				    8, (caddr_t)&q->q_ctx.pc_iv[0]);
d656 1
a656 1
				    8, (caddr_t)&q->q_ctx.pc_iv[0]);
d684 26
a709 9
		dskip = sskip = (macoffset > encoffset) ? encoffset : macoffset;
		coffset = macoffset - encoffset;
		if (coffset < 0)
			coffset = -coffset;
		if ((encoffset + enccrd->crd_len) >
		    (macoffset + maccrd->crd_len))
			stheend = dtheend = enccrd->crd_len;
		else
			stheend = dtheend = maccrd->crd_len;
d878 11
a888 1
			pb->pb_len = q->q_dst_packl[i];
d957 1
a957 1
			    (u_int8_t *)q->q_sc->sc_sessions[q->q_sesn].ses_iv);
d967 1
a967 1
		    crd->crd_inject, 12, (u_int8_t *)&q->q_macbuf[0]);
@


1.28
log
@Put the bcopy back in place for the decryption CRD_F_EXPLICIT_IV case (missed
this yesterday when fixing the encryption side of this case).  Only used for
old IPSec xforms anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.27 2000/08/15 17:27:56 jason Exp $	*/
a127 1
	bus_addr_t iobase;
d150 2
a151 1
	if (pci_mem_find(pc, pa->pa_tag, BS_BAR, &iobase, &iosize, NULL)) {
a154 6
	if (bus_space_map(pa->pa_memt, iobase, iosize, 0, &sc->sc_sh)) {
		printf(": can't map mem space\n");
		return;
	}
	sc->sc_st = pa->pa_memt;

d160 1
d171 1
d176 3
a178 1
	if (sc->sc_cid < 0)
d180 1
@


1.27
log
@don't forget the initial swizzling with IPAD for inner state (fixes
auth-only mode which was broken with new session code).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.26 2000/08/15 01:00:47 jason Exp $	*/
d656 3
a658 4
			if (enccrd->crd_flags & CRD_F_IV_EXPLICIT) {
				q->q_ctx.pc_iv[0] = enccrd->crd_iv[0];
				q->q_ctx.pc_iv[1] = enccrd->crd_iv[1];
			} else
@


1.26
log
@- Can't avoid one of those bcopy's so easily (only happens with old ESP
transforms... those with CRD_F_IV_EXPLICIT, so we can take the function
call overhead there).
- >> instead of << for coffset (auth+enc still isn't working, but this was
an obvious bug).
- #ifdef UBSEC_DEBUG out the auth-only debugging code
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.25 2000/08/13 22:07:11 deraadt Exp $	*/
d454 1
d456 3
d676 1
d681 1
@


1.25
log
@do iv copying by hand, to avoid bcopy overhead
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.24 2000/08/13 22:06:47 deraadt Exp $	*/
d636 3
a638 4
			if (enccrd->crd_flags & CRD_F_IV_EXPLICIT) {
				q->q_ctx.pc_iv[0] = enccrd->crd_iv[0];
				q->q_ctx.pc_iv[1] = enccrd->crd_iv[1];
			} else {
d697 1
a697 1
	q->q_ctx.pc_offset = coffset << 2;
d771 1
d776 1
@


1.24
log
@fix session code
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.22 2000/08/13 21:58:06 deraadt Exp $	*/
d636 7
a642 4
			if (enccrd->crd_flags & CRD_F_IV_EXPLICIT)
				bcopy(enccrd->crd_iv, &q->q_ctx.pc_iv[0], 8);
			else
				bcopy(&ses->ses_iv[0], &q->q_ctx.pc_iv[0], 8);
d653 4
a656 3
			if (enccrd->crd_flags & CRD_F_IV_EXPLICIT)
				bcopy(enccrd->crd_iv, &q->q_ctx.pc_iv[0], 8);
			else
@


1.23
log
@not completely working session code from jason
@
text
@d287 2
a288 2
	if (npkts > 20)
		npkts = 20;
d369 1
a369 1
	struct ubsec_session *ses;
d372 1
a372 1
	int i;
d404 1
a404 1
			sizeof(struct ubsec_session), M_DEVBUF, M_NOWAIT);
d407 1
a407 1
		i = 0;
d410 3
a412 3
		for (i = 0; i < sc->sc_nsessions; i++) {
			if (sc->sc_sessions[i].ses_used == 0) {
				ses = &sc->sc_sessions[i];
d418 2
a419 2
			ses = (struct ubsec_session *)malloc(
			    (sc->sc_nsessions + 1) *
d423 4
a426 6
			for (i = 0; i < sc->sc_nsessions; i++) {
				bcopy(&sc->sc_sessions[i], &ses[i],
				    sizeof(struct ubsec_session));
				bzero(&sc->sc_sessions[i],
				    sizeof(struct ubsec_session));
			}
d429 1
a429 1
			ses = &sc->sc_sessions[sc->sc_nsessions];
d445 1
a445 2
		}
		else 
d498 1
a498 2
	*sidp = UBSEC_SID(sc->sc_dv.dv_unit, i);

d561 2
a562 1
	ses = q->q_ses = &sc->sc_sessions[UBSEC_SESSION(crp->crp_sid)];
d924 1
a924 1
			    (u_int8_t *)q->q_ses->ses_iv);
@


1.22
log
@indent
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.21 2000/08/12 06:29:08 deraadt Exp $	*/
a360 1
 * XXX No allocation actually done here, all sessions are the same.
d367 1
a367 1
	struct cryptoini *c;
d369 3
a371 1
	char mac = 0, cry = 0;
d388 1
a388 1
			if (mac)
d390 1
a390 1
			mac = 1;
d393 1
a393 1
			if (cry)
d395 1
a395 1
			cry = 1;
d399 1
a399 1
	if (mac == 0 && cry == 0)
d402 100
a501 1
	*sidp = UBSEC_SID(sc->sc_dv.dv_unit, 0);
a507 1
 * XXX Nothing to do yet.
d510 2
a511 2
ubsec_freesession(sid)
	u_int64_t sid;
d513 10
a529 2
	MD5_CTX md5ctx;
	SHA1_CTX sha1ctx;
d536 1
d565 2
d642 1
a642 1
				get_random_bytes(&q->q_ctx.pc_iv[0], 8);
d660 6
a665 8
		if (enccrd->crd_alg == CRYPTO_DES_CBC) {
			/* Cheat: des == 3des with two of the keys the same */
			bcopy(enccrd->crd_key, &q->q_ctx.pc_deskey[0], 8);
			bcopy(enccrd->crd_key, &q->q_ctx.pc_deskey[2], 8);
			bcopy(enccrd->crd_key, &q->q_ctx.pc_deskey[4], 8);
		} else
			bcopy(enccrd->crd_key, &q->q_ctx.pc_deskey[0], 24);

a667 6
		SWAP32(q->q_ctx.pc_deskey[0]);
		SWAP32(q->q_ctx.pc_deskey[1]);
		SWAP32(q->q_ctx.pc_deskey[2]);
		SWAP32(q->q_ctx.pc_deskey[3]);
		SWAP32(q->q_ctx.pc_deskey[4]);
		SWAP32(q->q_ctx.pc_deskey[5]);
d672 1
a672 5

		for (i = 0; i < maccrd->crd_klen / 8; i++)
			maccrd->crd_key[i] ^= HMAC_IPAD_VAL;

		if (maccrd->crd_alg == CRYPTO_MD5_HMAC96) {
d674 1
a674 8
			MD5Init(&md5ctx);
			MD5Update(&md5ctx, maccrd->crd_key,
			    maccrd->crd_klen / 8);
			MD5Update(&md5ctx, hmac_ipad_buffer,
			    HMAC_BLOCK_LEN - (maccrd->crd_klen / 8));
			bcopy(md5ctx.state, q->q_ctx.pc_hminner,
			    sizeof(md5ctx.state));
		} else {
d676 3
a678 7
			SHA1Init(&sha1ctx);
			SHA1Update(&sha1ctx, maccrd->crd_key,
			    maccrd->crd_klen / 8);
			SHA1Update(&sha1ctx, hmac_ipad_buffer,
			    HMAC_BLOCK_LEN - (maccrd->crd_klen / 8));
			bcopy(sha1ctx.state, q->q_ctx.pc_hminner,
			    sizeof(sha1ctx.state));
a679 24

		for (i = 0; i < maccrd->crd_klen / 8; i++)
			maccrd->crd_key[i] ^= (HMAC_IPAD_VAL ^ HMAC_OPAD_VAL);

		if (maccrd->crd_alg == CRYPTO_MD5_HMAC96) {
			MD5Init(&md5ctx);
			MD5Update(&md5ctx, maccrd->crd_key,
			    maccrd->crd_klen / 8);
			MD5Update(&md5ctx, hmac_opad_buffer,
			    HMAC_BLOCK_LEN - (maccrd->crd_klen / 8));
			bcopy(md5ctx.state, q->q_ctx.pc_hmouter,
			    sizeof(md5ctx.state));
		} else {
			SHA1Init(&sha1ctx);
			SHA1Update(&sha1ctx, maccrd->crd_key,
			    maccrd->crd_klen / 8);
			SHA1Update(&sha1ctx, hmac_opad_buffer,
			    HMAC_BLOCK_LEN - (maccrd->crd_klen / 8));
			bcopy(sha1ctx.state, q->q_ctx.pc_hmouter,
			    sizeof(sha1ctx.state));
		}

		for (i = 0; i < maccrd->crd_klen / 8; i++)
			maccrd->crd_key[i] ^= HMAC_OPAD_VAL;
d916 14
@


1.21
log
@mcr aggregation now works, best i have seen yet is 5 (huh?)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.20 2000/08/11 19:38:15 deraadt Exp $	*/
d39 1
a39 1
 * uBsec 5[56]01 hardware crypto accelerator
d118 1
a118 1
void 
d196 1
a196 1
int 
d285 1
a285 1
	
d799 1
a799 1
					pb->pb_next = 
@


1.20
log
@move mcr out of q; and write prelim mcr aggregation code, which does not yet
work for some reason or another, so it is currently disabled.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.19 2000/07/31 21:57:49 deraadt Exp $	*/
d243 4
a246 1
					printf("found a share\n");
d280 1
d282 3
a284 2
	struct ubsec_mcr *mcr, *mcr2;
	int npkts, i;
a288 1
#ifdef not_working_yet
a290 3
#else
	goto feed1;
#endif
d300 1
d302 7
d321 2
a322 2
			bcopy(q->q_mcr, mcr2, sizeof(struct ubsec_mcr));
			mcr2 += sizeof(struct ubsec_mcr);
a323 2
			void *v;

d326 1
a326 2
			bcopy(v, mcr2, sizeof(struct ubsec_mcr_add));
			mcr2 += sizeof(struct ubsec_mcr_add);
d328 6
@


1.19
log
@since byte order bugs are gone, interrupts work ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.18 2000/07/29 23:42:00 jason Exp $	*/
d203 2
a204 1
	int npkts = 0;
d221 2
a222 2
			printf("mcr_flags %x %x %x\n", &q->q_mcr, q->q_mcr.mcr_flags,
			    READ_REG(sc, BS_ERR));
d224 1
a224 1
			if ((q->q_mcr.mcr_flags & UBS_MCR_DONE) == 0)
d230 1
a230 1
			    q->q_mcr.mcr_flags);
d232 1
d234 20
d266 1
d278 50
d329 1
d334 1
a334 1
		WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(&q->q_mcr));
d336 1
a336 1
		printf("feed: q->chip %08x %08x\n", q, (u_int32_t)vtophys(&q->q_mcr));
d445 12
a456 3
	q->q_mcr.mcr_pkts = 1;
	q->q_mcr.mcr_flags = 0;
	q->q_mcr.mcr_cmdctxp = vtophys(&q->q_ctx);
d481 1
a481 1
			 crd1->crd_alg == CRYPTO_3DES_CBC) {
d630 1
a630 1
	q->q_mcr.mcr_pktlen = stheend;
d653 1
a653 1
			pb = &q->q_mcr.mcr_ipktbuf;
d679 4
a682 4
	printf("  buf[%x]: %d@@%x -> %x\n", vtophys(&q->q_mcr),
	    q->q_mcr.mcr_ipktbuf.pb_len,
	    q->q_mcr.mcr_ipktbuf.pb_addr,
	    q->q_mcr.mcr_ipktbuf.pb_next);
d692 3
a694 3
		q->q_mcr.mcr_opktbuf.pb_addr = 0;
		q->q_mcr.mcr_opktbuf.pb_len = 0;
		q->q_mcr.mcr_opktbuf.pb_next =
d697 3
a699 3
		    q->q_mcr.mcr_opktbuf.pb_addr,
		    q->q_mcr.mcr_opktbuf.pb_len,
		    q->q_mcr.mcr_opktbuf.pb_next);
d775 1
a775 1
				pb = &q->q_mcr.mcr_opktbuf;
d797 4
a800 4
		    vtophys(&q->q_mcr),
		    q->q_mcr.mcr_opktbuf.pb_len,
		    q->q_mcr.mcr_opktbuf.pb_addr,
		    q->q_mcr.mcr_opktbuf.pb_next);
d820 2
d852 4
@


1.18
log
@remove the (unused) sc_intrmask, and be more careful about initializing
the BS_CTRL register... The BE32 & BE64 bits do NOT do what they imply,
so leave the bits set, and add the ones we want.  This allows ubsec to
interop with our software implementation (at least for encryption).
More work to be done in this driver though...  Many thanks to Jimmy Ruane
at Broadcom for the pointer about BE32 & BE64!
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.17 2000/07/20 21:45:19 deraadt Exp $	*/
d35 1
a35 1
#define UBSEC_DEBUG
d213 3
a215 1
printf("ubsec intr %x\n", stat);
d219 1
d222 2
a223 2
			if ((q->q_mcr.mcr_flags & UBS_MCR_DONE) == 0 &&
			    READ_REG(sc, BS_ERR) != vtophys(&q->q_mcr))
@


1.17
log
@work around broken A0 5805 silicon that fails to set the status result word in the mcr
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.16 2000/06/20 05:40:45 jason Exp $	*/
a133 1
	sc->sc_intrmask = BS_CTRL_MCR1INT | BS_CTRL_DMAERR;
d138 1
a138 2
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BROADCOM_5805)) {
		sc->sc_intrmask |= BS_CTRL_MCR2INT;
a139 1
	}
d189 3
a191 1
	WRITE_REG(sc, BS_CTRL, BS_CTRL_MCR1INT | BS_CTRL_DMAERR);
@


1.16
log
@call crypto_done()
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.15 2000/06/19 03:58:27 jason Exp $	*/
d35 3
d214 2
a215 1
	if (stat & BS_STAT_MCR1_DONE) {
d218 4
a221 1
			if ((q->q_mcr.mcr_flags & UBS_MCR_DONE) == 0)
@


1.15
log
@oops, nuke unused variable
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.14 2000/06/19 02:50:30 jason Exp $	*/
d759 1
a759 1
	crp->crp_callback(crp);
@


1.14
log
@add authentication-only handling (easy setup).
remove most of the parameters from ubsec_callback() since they can be found
later.
ok, so ubsec can now authenticate to itself (doesn't like software, tho)
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.13 2000/06/18 03:37:22 jason Exp $	*/
a742 1
	int i;
@


1.13
log
@Use the same field data types as the reference code and adjust offsets
appropriately.
Byte swap key/iv fields because they are given to us as "network order",
but the chip operates as little endian.
coffset is in WORDS not bytes
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.12 2000/06/14 14:09:36 jason Exp $	*/
d54 1
d56 2
d84 1
a84 1
void	ubsec_callback __P((struct ubsec_softc *, struct ubsec_q *, u_int8_t *));
d222 1
a222 1
			ubsec_callback(sc, q, NULL);
d326 2
d329 1
a329 1
	int card, err, i, j, s;
d332 1
a332 1
	int encoffset = 0, macoffset = 0, sskip, dskip;
d466 5
a470 1
		if (maccrd->crd_alg == CRYPTO_MD5_HMAC96)
d472 8
a479 1
		else
d481 29
d511 2
a512 4
		/* XXX not right */
		bcopy(maccrd->crd_key, &q->q_ctx.pc_hminner[0],
		    maccrd->crd_klen >> 5);

d520 5
d527 1
d533 1
a533 1
	    q->q_src_packl, MAX_SCATTER, &err);
d538 1
a538 55

	if (err == 0) {
		int totlen, len;
		struct mbuf *m, *top, **mp;

		totlen = q->q_dst_l = q->q_src_l;
		if (q->q_src_m->m_flags & M_PKTHDR) {
			MGETHDR(m, M_DONTWAIT, MT_DATA);
			M_COPY_PKTHDR(m, q->q_src_m);
			len = MHLEN;
		} else {
			MGET(m, M_DONTWAIT, MT_DATA);
			len = MLEN;
		}
		if (m == NULL) {
			err = ENOMEM;
			goto errout;
		}
		if (totlen >= MINCLSIZE) {
			MCLGET(m, M_DONTWAIT);
			if (m->m_flags & M_EXT)
				len = MCLBYTES;
		}
		m->m_len = len;
		top = NULL;
		mp = &top;

		while (totlen > 0) {
			if (top) {
				MGET(m, M_DONTWAIT, MT_DATA);
				if (m == NULL) {
					m_freem(top);
					err = ENOMEM;
					goto errout;
				}
				len = MLEN;
			}
			if (top && totlen >= MINCLSIZE) {
				MCLGET(m, M_DONTWAIT);
				if (m->m_flags & M_EXT)
					len = MCLBYTES;
			}
			m->m_len = len = min(totlen, len);
			totlen -= len;
			*mp = m;
			mp = &m->m_next;
		}
		q->q_dst_m = top;
	} else
		q->q_dst_m = q->q_src_m;

	q->q_dst_l = mbuf2pages(q->q_dst_m, &q->q_dst_npa, q->q_dst_packp,
	    q->q_dst_packl, MAX_SCATTER, NULL);

	q->q_mcr.mcr_pktlen = q->q_dst_l - dskip;
d569 10
a578 1
		pb->pb_len = q->q_src_packl[i];
d599 63
d663 1
a663 1
	printf("dst skip: %d\n", dskip);
d665 2
a666 2
	for (i = j = 0; i < q->q_dst_npa; i++) {
		struct ubsec_pktbuf *pb;
d669 2
a670 2
		printf("  dst[%d->%d]: %d@@%x\n", i, j,
		    q->q_dst_packl[i], q->q_dst_packp[i]);
d672 8
a679 4
		if (dskip) {
			if (dskip >= q->q_dst_packl[i]) {
				dskip -= q->q_dst_packl[i];
				continue;
a680 4
			q->q_dst_packp[i] += dskip;
			q->q_dst_packl[i] -= dskip;
			dskip = 0;
		}
d682 4
a685 4
		if (j == 0)
			pb = &q->q_mcr.mcr_opktbuf;
		else
			pb = &q->q_dstpkt[j - 1];
d688 1
a688 1
		printf("  pb v %08x p %08x\n", pb, vtophys(pb));
d690 2
a691 2
		pb->pb_addr = q->q_dst_packp[i];
		pb->pb_len = q->q_dst_packl[i];
d693 24
a716 18
		if ((i + 1) == q->q_dst_npa)
			pb->pb_next = 0;
		else
			pb->pb_next = vtophys(&q->q_dstpkt[j]);
		j++;
	}
#ifdef UBSEC_DEBUG
	printf("  buf[%d, %x]: %d@@%x -> %x\n", 0,
	    vtophys(&q->q_mcr),
	    q->q_mcr.mcr_opktbuf.pb_len,
	    q->q_mcr.mcr_opktbuf.pb_addr,
	    q->q_mcr.mcr_opktbuf.pb_next);
	for (i = 0; i < j - 1; i++) {
		printf("  buf[%d, %x]: %d@@%x -> %x\n", i+1,
		    vtophys(&q->q_dstpkt[i]),
		    q->q_dstpkt[i].pb_len,
		    q->q_dstpkt[i].pb_addr,
		    q->q_dstpkt[i].pb_next);
a717 1
#endif
d738 1
a738 2
ubsec_callback(sc, q, macbuf)
	struct ubsec_softc *sc;
a739 1
	u_int8_t *macbuf;
d743 1
d750 7
a756 10
	if (macbuf != NULL) {
		printf("copying macbuf\n");
		for (crd = crp->crp_desc; crd; crd = crd->crd_next) {
			if (crd->crd_alg != CRYPTO_MD5_HMAC96 &&
			    crd->crd_alg != CRYPTO_SHA1_HMAC96)
				continue;
			m_copyback((struct mbuf *)crp->crp_buf,
			    crd->crd_inject, 12, macbuf);
			break;
		}
@


1.12
log
@readd queue limiting code that was backed out yesterday.  (ip_esp.c change
seems to have fixed this).
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.10 2000/06/13 06:11:13 jason Exp $	*/
d90 2
d421 1
a421 2
				bcopy(enccrd->crd_iv, q->q_ctx.pc_iv,
				    sizeof(q->q_ctx.pc_iv));
d423 4
a426 2
				get_random_bytes(q->q_ctx.pc_iv,
				    sizeof(q->q_ctx.pc_iv));
d430 1
a430 1
				    sizeof(q->q_ctx.pc_iv), q->q_ctx.pc_iv);
d435 1
a435 2
				bcopy(enccrd->crd_iv, q->q_ctx.pc_iv,
				    sizeof(q->q_ctx.pc_iv));
d438 1
a438 1
				    sizeof(q->q_ctx.pc_iv), q->q_ctx.pc_iv);
d444 2
a445 2
			bcopy(enccrd->crd_key, &q->q_ctx.pc_deskey[8], 8);
			bcopy(enccrd->crd_key, &q->q_ctx.pc_deskey[16], 8);
d449 8
d468 1
a468 1
		    maccrd->crd_klen >> 3);
d481 1
a481 1
	q->q_ctx.pc_flags |= (coffset << 16);
d543 1
a543 1
	q->q_mcr.mcr_pktlen = q->q_dst_l - sskip;
@


1.11
log
@backout previous 2 changes... causes panics down the line.
@
text
@d321 1
a321 1
	struct ubsec_q *q;
d338 8
@


1.10
log
@Oops: initialize q to avoid using the unitialized value when the queue fills
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.9 2000/06/13 05:15:19 jason Exp $	*/
d321 1
a321 1
	struct ubsec_q *q = NULL;
a337 8

	s = splnet();
	if (sc->sc_nqueue == UBS_MAX_NQUEUE) {
		splx(s);
		err = ENOMEM;
		goto errout;
	}
	splx(s);
@


1.9
log
@put an upperbound on queue length
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.8 2000/06/13 00:38:25 deraadt Exp $	*/
d321 1
a321 1
	struct ubsec_q *q;
@


1.8
log
@correct 5805 test
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.7 2000/06/12 19:50:35 deraadt Exp $	*/
d338 8
@


1.7
log
@ESP 3des now works, after squishing 4 bugs
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.6 2000/06/10 05:09:37 deraadt Exp $	*/
d130 2
a131 2
	    (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601)) {
@


1.6
log
@correct SIMPLEQ bugs, ack interrupt.  8 packets have crypted, but kernel
memory got corrupted.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.5 2000/06/10 03:54:23 jason Exp $	*/
d5 1
d82 1
a82 1
int	ubsec_crypto __P((struct ubsec_softc *, struct ubsec_q *q));
d104 3
d128 4
a131 2
	if (PCI_VENDOR(pa->pa_id) == PCI_VENDOR_BLUESTEEL &&
	    PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_BLUESTEEL_5601) {
d194 1
a194 1
	u_int32_t stat;
d196 1
d204 2
d207 10
a216 4
		q = SIMPLEQ_FIRST(&sc->sc_qchip);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_qchip, q, q_next);
		if (q) {
			/* XXX must generate macbuf ... */
d219 4
a224 6
#if 0
	if (stat & BS_STAT_MCR2_DONE) {
		...
	}
#endif

d226 4
a229 1
		printf("%s: dmaerr\n", sc->sc_dv.dv_xname);
d232 1
a232 12
	/* if MCR is non-full, put a new command in it */
	if ((stat & BS_STAT_MCR1_FULL) == 0) {
		if (!SIMPLEQ_EMPTY(&sc->sc_queue)) {
			q = SIMPLEQ_FIRST(&sc->sc_queue);
			SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
			--sc->sc_nqueue;
			SIMPLEQ_INSERT_TAIL(&sc->sc_qchip, q, q_next);
			WRITE_REG(sc, BS_MCR1, (u_int32_t)vtophys(&q->q_mcr));
		}
	}

	WRITE_REG(sc, BS_STAT, stat);
d237 1
a237 1
ubsec_crypto(sc, q)
d239 1
a240 3
{
	int s;
	u_int32_t stat;
d242 4
a245 3
	s = splnet();
	stat = READ_REG(sc, BS_STAT);
	if ((stat & BS_STAT_MCR1_FULL) == 0) {
d247 5
a252 3
	} else {
		SIMPLEQ_INSERT_TAIL(&sc->sc_queue, q, q_next);
		sc->sc_nqueue++;
a253 1
	splx(s);
d322 1
a322 1
	int card, err, i, j;
d347 2
a348 1
	q->q_mcr.mcr_flags = 1 & UBS_MCR_PACKETS;
d477 8
a484 1
		MGETHDR(m, M_DONTWAIT, MT_DATA);
a488 1
		len = MHLEN;
d525 6
a530 2
	j = 0;
	for (i = 0; i < q->q_src_npa; i++) {
d533 4
d552 3
d564 12
d577 4
a580 2
	j = 0;
	for (i = 0; i < q->q_dst_npa; i++) {
d583 4
d602 3
d614 14
d629 5
a633 2
	/* queues it, or sends it to the chip */
	ubsec_crypto(sc, q);
d662 1
@


1.5
log
@another vaddr used as paddr
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.4 2000/06/10 03:49:29 jason Exp $	*/
d193 4
d198 1
d206 6
d218 4
a221 3
		SIMPLEQ_REMOVE_HEAD(&sc->sc_queue, q, q_next);
		--sc->sc_nqueue;
		if (q) {
d227 2
a228 1
	return (stat ? 1 : 0);
d453 1
a453 2
	}
	else {
@


1.4
log
@paddr not vaddr for mcr1
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.3 2000/06/03 13:14:39 jason Exp $	*/
d211 1
a211 1
			WRITE_REG(sc, BS_MCR1, (u_int32_t)&q->q_mcr);
@


1.3
log
@Move everything to where is supposed to be (reg definitions, etc).
Add some of the skip logic.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.2 2000/06/02 22:42:08 deraadt Exp $	*/
d229 1
a229 1
		WRITE_REG(sc, BS_MCR1, (u_int32_t)&q->q_mcr);
@


1.2
log
@squeeze basic framework into place.  process generates SIMPLEQ of requests,
they get fed in, irq recovers old one, feeds new one in, callback schreds
and calls back to crypto(9)... mac result buffers and packet offsets need
work.
@
text
@d1 1
a1 1
/*	$OpenBSD: ubsec.c,v 1.1 2000/05/18 01:25:19 jason Exp $	*/
d59 2
a60 114
/*
 * Register definitions for 5601 BlueSteel Networks Ubiquitous Broadband
 * Security "uBSec" chip.  Definitions from revision 2.8 of the product
 * datasheet.
 */

#define BS_BAR		0x10	/* DMA and status base address register */

/*
 * DMA Control & Status Registers (offset from BS_BAR)
 */
#define	BS_MCR1		0x00	/* DMA Master Command Record 1 */
#define	BS_CTRL		0x04	/* DMA Control */
#define	BS_STAT		0x08	/* DMA Status */
#define	BS_ERR		0x0c	/* DMA Error Address */
#define	BS_MCR2		0x10	/* DMA Master Command Record 2 */

/* BS_CTRL - DMA Control */
#define	BS_CTRL_MCR2INT		0x40000000	/* enable intr MCR for MCR2 */
#define	BS_CTRL_MCR1INT		0x20000000	/* enable intr MCR for MCR1 */
#define	BS_CTRL_OFM		0x10000000	/* Output fragment mode */
#define	BS_CTRL_BE32		0x08000000	/* big-endian, 32bit bytes */
#define	BS_CTRL_BE64		0x04000000	/* big-endian, 64bit bytes */
#define	BS_CTRL_DMAERR		0x02000000	/* enable intr DMA error */
#define	BS_CTRL_RNG_M		0x01800000	/* RND mode */
#define	BS_CTRL_RNG_1		0x00000000	/* 1bit rn/one slow clock */
#define	BS_CTRL_RNG_4		0x00800000	/* 1bit rn/four slow clocks */
#define	BS_CTRL_RNG_8		0x01000000	/* 1bit rn/eight slow clocks */
#define	BS_CTRL_RNG_16		0x01800000	/* 1bit rn/16 slow clocks */
#define	BS_CTRL_FRAG_M		0x0000ffff	/* output fragment size mask */

/* BS_STAT - DMA Status */
#define	BS_STAT_MCR1_BUSY	0x80000000	/* MCR1 is busy */
#define	BS_STAT_MCR1_FULL	0x40000000	/* MCR1 is full */
#define	BS_STAT_MCR1_DONE	0x20000000	/* MCR1 is done */
#define	BS_STAT_DMAERR		0x10000000	/* DMA error */
#define	BS_STAT_MCR2_FULL	0x08000000	/* MCR2 is full */
#define	BS_STAT_MCR2_DONE	0x04000000	/* MCR2 is done */

/* BS_ERR - DMA Error Address */
#define	BS_ERR_READ		0x00000001	/* fault was on read */

#define UBSEC_CARD(sid)		(((sid) & 0xf0000000) >> 28)
#define UBSEC_SID(crd,ses)	(((crd) << 28) | ((ses) & 0x7ff))
#define	MAX_SCATTER		10

struct ubsec_pktctx {
	u_int8_t	pc_deskey[24];		/* 3DES key */
	u_int8_t	pc_hminner[20];		/* hmac inner state */
	u_int8_t	pc_hmouter[20];		/* hmac outer state */
	u_int8_t	pc_iv[8];		/* 3DES iv */
	u_int32_t	pc_flags;
};
#define	UBS_PKTCTX_COFFSET	0xffff0000	/* cryto to mac offset */
#define	UBS_PKTCTX_ENC_3DES	0x00008000	/* use 3des */
#define	UBS_PKTCTX_ENC_NONE	0x00000000	/* no encryption */
#define	UBS_PKTCTX_INBOUND	0x00004000	/* inbound packet */
#define	UBS_PKTCTX_AUTH		0x00003000	/* authentication mask */
#define	UBS_PKTCTX_AUTH_NONE	0x00000000	/* no authentication */
#define	UBS_PKTCTX_AUTH_MD5	0x00001000	/* use hmac-md5 */
#define	UBS_PKTCTX_AUTH_SHA1	0x00002000	/* use hmac-sha1 */

struct ubsec_pktbuf {
	u_int32_t	pb_addr;		/* address of buffer start */
	u_int32_t	pb_next;		/* pointer to next pktbuf */
	u_int32_t	pb_len;
};
#define	UBS_PKTBUF_LEN		0x0000ffff	/* length mask */

struct ubsec_mcr {
	u_int32_t		mcr_flags;	/* flags/packet count */

	u_int32_t		mcr_cmdctxp;	/* command ctx pointer */
	struct ubsec_pktbuf	mcr_ipktbuf;	/* input chain header */
	struct ubsec_pktbuf	mcr_opktbuf;	/* output chain header */
};
#define	UBS_MCR_PACKETS		0x0000ffff	/* packets in this mcr */
#define	UBS_MCR_DONE		0x00010000	/* mcr has been processed */
#define	UBS_MCR_ERROR		0x00020000	/* error in processing */
#define	UBS_MCR_ERRORCODE	0xff000000	/* error type */

struct ubsec_q {
	SIMPLEQ_ENTRY(ubsec_q)		q_next;
	struct ubsec_softc		*q_sc;
	struct cryptop			*q_crp;
	struct ubsec_mcr		q_mcr;
	struct ubsec_pktctx		q_ctx;

	struct mbuf *		      	q_src_m;
	long				q_src_packp[MAX_SCATTER];
	int				q_src_packl[MAX_SCATTER];
	int				q_src_npa, q_src_l;
	struct ubsec_pktbuf		q_srcpkt[MAX_SCATTER-1];

	struct mbuf *			q_dst_m;
	long				q_dst_packp[MAX_SCATTER];
	int				q_dst_packl[MAX_SCATTER];
	int				q_dst_npa, q_dst_l;
	struct ubsec_pktbuf		q_dstpkt[MAX_SCATTER-1];
};

struct ubsec_softc {
	struct	device		sc_dv;		/* generic device */
	void			*sc_ih;		/* interrupt handler cookie */
	bus_space_handle_t	sc_sh;		/* memory handle */
	bus_space_tag_t		sc_st;		/* memory tag */
	bus_dma_tag_t		sc_dmat;	/* dma tag */
	int			sc_5601;	/* device is 5601 */
	int32_t			sc_cid;		/* crypto tag */
	u_int32_t		sc_intrmask;	/* interrupt mask */
	SIMPLEQ_HEAD(,ubsec_q)	sc_queue;	/* packet queue */
	int			sc_nqueue;	/* count enqueued */
	SIMPLEQ_HEAD(,ubsec_q)	sc_qchip;	/* on chip */
};
d83 6
d178 2
a182 6
#define	READ_REG(sc,r) \
	bus_space_read_4((sc)->sc_st, (sc)->sc_sh, (r))

#define WRITE_REG(sc,reg,val) \
	bus_space_write_4((sc)->sc_st, (sc)->sc_sh, reg, val)

d304 1
a304 1
	int card, err, i;
d307 2
d387 1
d423 1
d435 12
d501 1
a501 8
#if 0
	/* XXX time to incorporate the information in crd_skip... */

	cmd->crypt_header_skip = enccrd->crd_skip;
	cmd->crypt_process_len = enccrd->crd_len;
	cmd->mac_header_skip = maccrd->crd_skip;
	cmd->mac_process_len = maccrd->crd_len;
#endif
d505 11
a515 1
		if (i == 0)
d518 1
a518 1
			pb = &q->q_srcpkt[i - 1];
d526 2
a527 1
			pb->pb_next = vtophys(&q->q_srcpkt[i]);
d530 1
d534 11
a544 1
		if (i == 0)
d547 1
a547 1
			pb = &q->q_dstpkt[i - 1];
d555 2
a556 1
			pb->pb_next = vtophys(&q->q_dstpkt[i]);
@


1.1
log
@work in progress: driver for BlueSteel (Broadcom) 5[56]01 crypto accelerator
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a104 6
struct ubsec_command {
	struct mbuf *src_m;
	struct mbuf *dst_m;
	long private_data;
};

d140 2
a141 2
struct ubsec_queue {
	LIST_ENTRY(ubsec_queue)		q_next;
d169 1
a169 1
	LIST_HEAD(,ubsec_queue)	sc_queue;	/* packet queue */
d171 1
d192 2
a193 1
void	ubsec_callback __P((struct ubsec_softc *, struct ubsec_command *, u_int8_t *));
d226 2
a227 1
	LIST_INIT(&sc->sc_queue);
d290 3
d299 1
d301 23
a323 3
	stat = READ_REG(sc, BS_STAT) & sc->sc_intrmask;
	if (stat == 0)
		return (stat);
d325 22
a346 1
	return (1);
d382 1
a382 2
		}
		else if (c->cri_alg == CRYPTO_DES_CBC ||
d387 1
a387 2
		}
		else
d413 1
a413 1
	struct ubsec_queue *q;
d429 1
a429 1
	q = (struct ubsec_queue *)malloc(sizeof(struct ubsec_queue),
d435 1
a435 1
	bzero(q, sizeof(struct ubsec_queue));
d445 1
a445 2
	}
	else {
d462 1
a462 2
		}
		else if (crd1->crd_alg == CRYPTO_DES_CBC ||
d466 1
a466 2
		}
		else {
d470 1
a470 2
	}
	else {
d478 1
a478 2
		}
		else if ((crd1->crd_alg == CRYPTO_DES_CBC ||
d485 1
a485 2
		}
		else {
d508 1
a508 2
		}
		else {
d524 1
a524 2
		}
		else
a526 4
#if 0
		cmd->crypt_header_skip = enccrd->crd_skip;
		cmd->crypt_process_len = enccrd->crd_len;
#endif
a538 4
#if 0
		cmd->mac_header_skip = maccrd->crd_skip;
		cmd->mac_process_len = maccrd->crd_len;
#endif
d589 1
a589 2
	}
	else
d595 8
d637 3
a639 1
	err = ENOMEM;
d653 1
a653 1
ubsec_callback(sc, cmd, macbuf)
d655 1
a655 1
	struct ubsec_command *cmd;
d658 1
a658 1
	struct cryptop *crp = (struct cryptop *)cmd->private_data;
d661 3
a663 3
	if ((crp->crp_flags & CRYPTO_F_IMBUF) && (cmd->src_m != cmd->dst_m)) {
		m_freem(cmd->src_m);
		crp->crp_buf = (caddr_t)cmd->dst_m;
d677 1
a677 1
	free(cmd, M_DEVBUF);
@

