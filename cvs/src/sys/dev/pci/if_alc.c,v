head	1.42;
access;
symbols
	OPENBSD_6_2:1.42.0.4
	OPENBSD_6_2_BASE:1.42
	OPENBSD_6_1:1.41.0.4
	OPENBSD_6_1_BASE:1.41
	OPENBSD_6_0:1.39.0.4
	OPENBSD_6_0_BASE:1.39
	OPENBSD_5_9:1.37.0.2
	OPENBSD_5_9_BASE:1.37
	OPENBSD_5_8:1.33.0.4
	OPENBSD_5_8_BASE:1.33
	OPENBSD_5_7:1.30.0.4
	OPENBSD_5_7_BASE:1.30
	OPENBSD_5_6:1.27.0.4
	OPENBSD_5_6_BASE:1.27
	OPENBSD_5_5:1.26.0.4
	OPENBSD_5_5_BASE:1.26
	OPENBSD_5_4:1.22.0.4
	OPENBSD_5_4_BASE:1.22
	OPENBSD_5_3:1.22.0.2
	OPENBSD_5_3_BASE:1.22
	OPENBSD_5_2:1.21.0.4
	OPENBSD_5_2_BASE:1.21
	OPENBSD_5_1_BASE:1.21
	OPENBSD_5_1:1.21.0.2
	OPENBSD_5_0:1.15.0.2
	OPENBSD_5_0_BASE:1.15
	OPENBSD_4_9:1.10.0.2
	OPENBSD_4_9_BASE:1.10
	OPENBSD_4_8:1.6.0.2
	OPENBSD_4_8_BASE:1.6
	OPENBSD_4_7:1.4.0.2
	OPENBSD_4_7_BASE:1.4;
locks; strict;
comment	@ * @;


1.42
date	2017.09.08.05.36.52;	author deraadt;	state Exp;
branches;
next	1.41;
commitid	uRv5pa9QDlZaYgwD;

1.41
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.40;
commitid	VyLWTsbepAOk7VQM;

1.40
date	2016.11.29.10.22.30;	author jsg;	state Exp;
branches;
next	1.39;
commitid	ZQetSMB5ilG2z10X;

1.39
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.38;
commitid	8YSL8ByWzGeIGBiJ;

1.38
date	2016.03.15.16.45.52;	author naddy;	state Exp;
branches;
next	1.37;
commitid	X5t9omeXAp1mh2AJ;

1.37
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.36;
commitid	B0kwmVGiD5DVx4kv;

1.36
date	2015.11.09.00.29.06;	author dlg;	state Exp;
branches;
next	1.35;
commitid	rcVfmI5b2gosTZgD;

1.35
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.34;
commitid	hPF95ClMUQfeqQDX;

1.34
date	2015.09.11.13.02.28;	author stsp;	state Exp;
branches;
next	1.33;
commitid	6vhYvh5CxZAHMnsN;

1.33
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.32;
commitid	MVWrtktB46JRxFWT;

1.32
date	2015.03.20.16.48.13;	author mpi;	state Exp;
branches;
next	1.31;
commitid	GJTGiFiMwDfzEJqv;

1.31
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.30;
commitid	p4LJxGKbi0BU2cG6;

1.30
date	2014.12.22.02.28.51;	author tedu;	state Exp;
branches;
next	1.29;
commitid	yM2VFFhpDTeFQlve;

1.29
date	2014.11.27.14.52.04;	author brad;	state Exp;
branches;
next	1.28;
commitid	r35l6cwyikjEN054;

1.28
date	2014.11.18.02.37.30;	author tedu;	state Exp;
branches;
next	1.27;
commitid	Z1vcFtHO8wRH0yRt;

1.27
date	2014.07.22.13.12.11;	author mpi;	state Exp;
branches;
next	1.26;
commitid	TGHgrLxu6sxZoiFt;

1.26
date	2013.12.28.03.34.53;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	2013.12.06.21.03.03;	author deraadt;	state Exp;
branches;
next	1.24;

1.24
date	2013.11.21.16.16.08;	author mpi;	state Exp;
branches;
next	1.23;

1.23
date	2013.08.07.01.06.33;	author bluhm;	state Exp;
branches;
next	1.22;

1.22
date	2012.11.29.21.10.32;	author brad;	state Exp;
branches;
next	1.21;

1.21
date	2011.10.19.05.23.44;	author kevlo;	state Exp;
branches;
next	1.20;

1.20
date	2011.09.21.07.09.19;	author kevlo;	state Exp;
branches;
next	1.19;

1.19
date	2011.09.13.08.12.51;	author kevlo;	state Exp;
branches;
next	1.18;

1.18
date	2011.09.03.14.33.51;	author kevlo;	state Exp;
branches;
next	1.17;

1.17
date	2011.08.26.07.49.04;	author kevlo;	state Exp;
branches;
next	1.16;

1.16
date	2011.08.21.15.25.20;	author kevlo;	state Exp;
branches;
next	1.15;

1.15
date	2011.06.17.07.16.42;	author kevlo;	state Exp;
branches;
next	1.14;

1.14
date	2011.05.27.07.45.44;	author kevlo;	state Exp;
branches;
next	1.13;

1.13
date	2011.05.25.02.22.20;	author kevlo;	state Exp;
branches;
next	1.12;

1.12
date	2011.05.18.14.20.27;	author sthen;	state Exp;
branches;
next	1.11;

1.11
date	2011.04.05.18.01.21;	author henning;	state Exp;
branches;
next	1.10;

1.10
date	2011.02.18.17.20.15;	author mikeb;	state Exp;
branches;
next	1.9;

1.9
date	2011.01.29.08.13.46;	author kevlo;	state Exp;
branches;
next	1.8;

1.8
date	2010.08.31.17.13.44;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	2010.08.27.17.08.00;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2010.07.27.22.39.59;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	2010.04.08.00.23.53;	author tedu;	state Exp;
branches;
next	1.4;

1.4
date	2010.02.27.08.19.47;	author kevlo;	state Exp;
branches;
next	1.3;

1.3
date	2010.01.07.12.26.06;	author sthen;	state Exp;
branches;
next	1.2;

1.2
date	2009.09.13.14.42.52;	author krw;	state Exp;
branches;
next	1.1;

1.1
date	2009.08.08.09.31.13;	author kevlo;	state Exp;
branches;
next	;


desc
@@


1.42
log
@If you use sys/param.h, you don't need sys/types.h
@
text
@/*	$OpenBSD: if_alc.c,v 1.41 2017/01/22 10:17:38 dlg Exp $	*/
/*-
 * Copyright (c) 2009, Pyun YongHyeon <yongari@@FreeBSD.org>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice unmodified, this list of conditions, and the following
 *    disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

/* Driver for Atheros AR8131/AR8132 PCIe Ethernet. */

#include "bpfilter.h"
#include "vlan.h"

#include <sys/param.h>
#include <sys/endian.h>
#include <sys/systm.h>
#include <sys/sockio.h>
#include <sys/mbuf.h>
#include <sys/queue.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/timeout.h>
#include <sys/socket.h>

#include <machine/bus.h>

#include <net/if.h>
#include <net/if_dl.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#include <dev/mii/mii.h>
#include <dev/mii/miivar.h>

#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>
#include <dev/pci/pcidevs.h>

#include <dev/pci/if_alcreg.h>

int	alc_match(struct device *, void *, void *);
void	alc_attach(struct device *, struct device *, void *);
int	alc_detach(struct device *, int);
int	alc_activate(struct device *, int);

int	alc_init(struct ifnet *);
void	alc_start(struct ifnet *);
int	alc_ioctl(struct ifnet *, u_long, caddr_t);
void	alc_watchdog(struct ifnet *);
int	alc_mediachange(struct ifnet *);
void	alc_mediastatus(struct ifnet *, struct ifmediareq *);

void	alc_aspm(struct alc_softc *, uint64_t);
void	alc_disable_l0s_l1(struct alc_softc *);
int	alc_dma_alloc(struct alc_softc *);
void	alc_dma_free(struct alc_softc *);
int	alc_encap(struct alc_softc *, struct mbuf *);
void	alc_get_macaddr(struct alc_softc *);
void	alc_init_cmb(struct alc_softc *);
void	alc_init_rr_ring(struct alc_softc *);
int	alc_init_rx_ring(struct alc_softc *);
void	alc_init_smb(struct alc_softc *);
void	alc_init_tx_ring(struct alc_softc *);
int	alc_intr(void *);
void	alc_mac_config(struct alc_softc *);
int	alc_miibus_readreg(struct device *, int, int);
void	alc_miibus_statchg(struct device *);
void	alc_miibus_writereg(struct device *, int, int, int);
int	alc_newbuf(struct alc_softc *, struct alc_rxdesc *);
void	alc_phy_down(struct alc_softc *);
void	alc_phy_reset(struct alc_softc *);
void	alc_reset(struct alc_softc *);
void	alc_rxeof(struct alc_softc *, struct rx_rdesc *);
void	alc_rxintr(struct alc_softc *);
void	alc_iff(struct alc_softc *);
void	alc_rxvlan(struct alc_softc *);
void	alc_start_queue(struct alc_softc *);
void	alc_stats_clear(struct alc_softc *);
void	alc_stats_update(struct alc_softc *);
void	alc_stop(struct alc_softc *);
void	alc_stop_mac(struct alc_softc *);
void	alc_stop_queue(struct alc_softc *);
void	alc_tick(void *);
void	alc_txeof(struct alc_softc *);

uint32_t alc_dma_burst[] = { 128, 256, 512, 1024, 2048, 4096, 0 };

const struct pci_matchid alc_devices[] = {
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L1C },
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L2C },
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L1D },
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L1D_1 },
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L2C_1 },
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L2C_2 }
};

struct cfattach alc_ca = {
	sizeof (struct alc_softc), alc_match, alc_attach, NULL,
	alc_activate
};

struct cfdriver alc_cd = {
	NULL, "alc", DV_IFNET
};

int alcdebug = 0;
#define	DPRINTF(x)	do { if (alcdebug) printf x; } while (0)

#define ALC_CSUM_FEATURES	(M_TCP_CSUM_OUT | M_UDP_CSUM_OUT)

int
alc_miibus_readreg(struct device *dev, int phy, int reg)
{
	struct alc_softc *sc = (struct alc_softc *)dev;
	uint32_t v;
	int i;

	if (phy != sc->alc_phyaddr)
		return (0);

	/*
	 * For AR8132 fast ethernet controller, do not report 1000baseT
	 * capability to mii(4). Even though AR8132 uses the same
	 * model/revision number of F1 gigabit PHY, the PHY has no
	 * ability to establish 1000baseT link.
	 */
	if ((sc->alc_flags & ALC_FLAG_FASTETHER) != 0 &&
	    reg == MII_EXTSR)
		return (0);

	CSR_WRITE_4(sc, ALC_MDIO, MDIO_OP_EXECUTE | MDIO_OP_READ |
	    MDIO_SUP_PREAMBLE | MDIO_CLK_25_4 | MDIO_REG_ADDR(reg));
	for (i = ALC_PHY_TIMEOUT; i > 0; i--) {
		DELAY(5);
		v = CSR_READ_4(sc, ALC_MDIO);
		if ((v & (MDIO_OP_EXECUTE | MDIO_OP_BUSY)) == 0)
			break;
	}

	if (i == 0) {
		printf("%s: phy read timeout: phy %d, reg %d\n",
		    sc->sc_dev.dv_xname, phy, reg);
		return (0);
	}

	return ((v & MDIO_DATA_MASK) >> MDIO_DATA_SHIFT);
}

void
alc_miibus_writereg(struct device *dev, int phy, int reg, int val)
{
	struct alc_softc *sc = (struct alc_softc *)dev;
	uint32_t v;
	int i;

	if (phy != sc->alc_phyaddr)
		return;

	CSR_WRITE_4(sc, ALC_MDIO, MDIO_OP_EXECUTE | MDIO_OP_WRITE |
	    (val & MDIO_DATA_MASK) << MDIO_DATA_SHIFT |
	    MDIO_SUP_PREAMBLE | MDIO_CLK_25_4 | MDIO_REG_ADDR(reg));
	for (i = ALC_PHY_TIMEOUT; i > 0; i--) {
		DELAY(5);
		v = CSR_READ_4(sc, ALC_MDIO);
		if ((v & (MDIO_OP_EXECUTE | MDIO_OP_BUSY)) == 0)
			break;
	}

	if (i == 0)
		printf("%s: phy write timeout: phy %d, reg %d\n",
		    sc->sc_dev.dv_xname, phy, reg);
}

void
alc_miibus_statchg(struct device *dev)
{
	struct alc_softc *sc = (struct alc_softc *)dev;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct mii_data *mii = &sc->sc_miibus;
	uint32_t reg;

	if ((ifp->if_flags & IFF_RUNNING) == 0)
		return;

	sc->alc_flags &= ~ALC_FLAG_LINK;
	if ((mii->mii_media_status & (IFM_ACTIVE | IFM_AVALID)) ==
	    (IFM_ACTIVE | IFM_AVALID)) {
		switch (IFM_SUBTYPE(mii->mii_media_active)) {
		case IFM_10_T:
		case IFM_100_TX:
			sc->alc_flags |= ALC_FLAG_LINK;
			break;
		case IFM_1000_T:
			if ((sc->alc_flags & ALC_FLAG_FASTETHER) == 0)
				sc->alc_flags |= ALC_FLAG_LINK;
			break;
		default:
			break;
		}
	}
	alc_stop_queue(sc);
	/* Stop Rx/Tx MACs. */
	alc_stop_mac(sc);

	/* Program MACs with resolved speed/duplex/flow-control. */
	if ((sc->alc_flags & ALC_FLAG_LINK) != 0) {
		alc_start_queue(sc);
		alc_mac_config(sc);
		/* Re-enable Tx/Rx MACs. */
		reg = CSR_READ_4(sc, ALC_MAC_CFG);
		reg |= MAC_CFG_TX_ENB | MAC_CFG_RX_ENB;
		CSR_WRITE_4(sc, ALC_MAC_CFG, reg);
		alc_aspm(sc, IFM_SUBTYPE(mii->mii_media_active));
	}
}

void
alc_mediastatus(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct alc_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_miibus;

	if ((ifp->if_flags & IFF_UP) == 0)
		return;

	mii_pollstat(mii);
	ifmr->ifm_status = mii->mii_media_status;
	ifmr->ifm_active = mii->mii_media_active;
}

int
alc_mediachange(struct ifnet *ifp)
{
	struct alc_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_miibus;
	int error;

	if (mii->mii_instance != 0) {
		struct mii_softc *miisc;

		LIST_FOREACH(miisc, &mii->mii_phys, mii_list)
			mii_phy_reset(miisc);
	}
	error = mii_mediachg(mii);

	return (error);
}

int
alc_match(struct device *dev, void *match, void *aux)
{
	return pci_matchbyid((struct pci_attach_args *)aux, alc_devices,
	    nitems(alc_devices));
}

void
alc_get_macaddr(struct alc_softc *sc)
{
	uint32_t ea[2], opt;
	uint16_t val;
	int eeprom, i;

	eeprom = 0;
	opt = CSR_READ_4(sc, ALC_OPT_CFG);
	if ((CSR_READ_4(sc, ALC_MASTER_CFG) & MASTER_OTP_SEL) != 0 &&
	    (CSR_READ_4(sc, ALC_TWSI_DEBUG) & TWSI_DEBUG_DEV_EXIST) != 0) {
		/*
		 * EEPROM found, let TWSI reload EEPROM configuration.
		 * This will set ethernet address of controller.
		 */
		eeprom++;
		switch (sc->sc_product) {
		case PCI_PRODUCT_ATTANSIC_L1C:
		case PCI_PRODUCT_ATTANSIC_L2C:
			if ((opt & OPT_CFG_CLK_ENB) == 0) {
				opt |= OPT_CFG_CLK_ENB;
				CSR_WRITE_4(sc, ALC_OPT_CFG, opt);
				CSR_READ_4(sc, ALC_OPT_CFG);
				DELAY(1000);
			}
			break;
		case PCI_PRODUCT_ATTANSIC_L1D:
		case PCI_PRODUCT_ATTANSIC_L1D_1:
		case PCI_PRODUCT_ATTANSIC_L2C_1:
		case PCI_PRODUCT_ATTANSIC_L2C_2:
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_ADDR, 0x00);
			val = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA);
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA, val & 0xFF7F);
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_ADDR, 0x3B);
			val = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA);
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA, val | 0x0008);
			DELAY(20);
			break;
		}

		CSR_WRITE_4(sc, ALC_LTSSM_ID_CFG,
		    CSR_READ_4(sc, ALC_LTSSM_ID_CFG) & ~LTSSM_ID_WRO_ENB);
		CSR_WRITE_4(sc, ALC_WOL_CFG, 0);
		CSR_READ_4(sc, ALC_WOL_CFG);

		CSR_WRITE_4(sc, ALC_TWSI_CFG, CSR_READ_4(sc, ALC_TWSI_CFG) |
		    TWSI_CFG_SW_LD_START);
		for (i = 100; i > 0; i--) {
			DELAY(1000);
			if ((CSR_READ_4(sc, ALC_TWSI_CFG) &
			    TWSI_CFG_SW_LD_START) == 0)
				break;
		}
		if (i == 0)
			printf("%s: reloading EEPROM timeout!\n", 
			    sc->sc_dev.dv_xname);
	} else {
		if (alcdebug)
			printf("%s: EEPROM not found!\n", sc->sc_dev.dv_xname);
	}
	if (eeprom != 0) {
		switch (sc->sc_product) {
		case PCI_PRODUCT_ATTANSIC_L1C:
		case PCI_PRODUCT_ATTANSIC_L2C:
			if ((opt & OPT_CFG_CLK_ENB) != 0) {
				opt &= ~OPT_CFG_CLK_ENB;
				CSR_WRITE_4(sc, ALC_OPT_CFG, opt);
				CSR_READ_4(sc, ALC_OPT_CFG);
				DELAY(1000);
			}
			break;
		case PCI_PRODUCT_ATTANSIC_L1D:
		case PCI_PRODUCT_ATTANSIC_L1D_1:
		case PCI_PRODUCT_ATTANSIC_L2C_1:
		case PCI_PRODUCT_ATTANSIC_L2C_2:
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_ADDR, 0x00);
			val = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA);
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA, val | 0x0080);
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_ADDR, 0x3B);
			val = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA);
			alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
			    ALC_MII_DBG_DATA, val & 0xFFF7);
			DELAY(20);
			break;
		}
	}

	ea[0] = CSR_READ_4(sc, ALC_PAR0);
	ea[1] = CSR_READ_4(sc, ALC_PAR1);
	sc->alc_eaddr[0] = (ea[1] >> 8) & 0xFF;
	sc->alc_eaddr[1] = (ea[1] >> 0) & 0xFF;
	sc->alc_eaddr[2] = (ea[0] >> 24) & 0xFF;
	sc->alc_eaddr[3] = (ea[0] >> 16) & 0xFF;
	sc->alc_eaddr[4] = (ea[0] >> 8) & 0xFF;
	sc->alc_eaddr[5] = (ea[0] >> 0) & 0xFF;
}

void
alc_disable_l0s_l1(struct alc_softc *sc)
{
	uint32_t pmcfg;

	/* Another magic from vendor. */
	pmcfg = CSR_READ_4(sc, ALC_PM_CFG);
	pmcfg &= ~(PM_CFG_L1_ENTRY_TIMER_MASK | PM_CFG_CLK_SWH_L1 |
	    PM_CFG_ASPM_L0S_ENB | PM_CFG_ASPM_L1_ENB | PM_CFG_MAC_ASPM_CHK |
	    PM_CFG_SERDES_PD_EX_L1);
	pmcfg |= PM_CFG_SERDES_BUDS_RX_L1_ENB | PM_CFG_SERDES_PLL_L1_ENB |
	    PM_CFG_SERDES_L1_ENB;
	CSR_WRITE_4(sc, ALC_PM_CFG, pmcfg);
}

void
alc_phy_reset(struct alc_softc *sc)
{
	uint16_t data;

	/* Reset magic from Linux. */
	CSR_WRITE_2(sc, ALC_GPHY_CFG, GPHY_CFG_SEL_ANA_RESET);
	CSR_READ_2(sc, ALC_GPHY_CFG);
	DELAY(10 * 1000);

	CSR_WRITE_2(sc, ALC_GPHY_CFG, GPHY_CFG_EXT_RESET |
	    GPHY_CFG_SEL_ANA_RESET);
	CSR_READ_2(sc, ALC_GPHY_CFG);
	DELAY(10 * 1000);

	/* DSP fixup, Vendor magic. */
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1) {
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_ADDR, 0x000A);
		data = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_DATA);
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_DATA, data & 0xDFFF);
	}
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_2) {
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_ADDR, 0x003B);
		data = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_DATA);
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_DATA, data & 0xFFF7);
		DELAY(20 * 1000);
	}
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D) {
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_ADDR, 0x0029);
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_DATA, 0x929D);
	}
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1C ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_2) {
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_ADDR, 0x0029);
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    ALC_MII_DBG_DATA, 0xB6DD);
	}

	/* Load DSP codes, vendor magic. */
	data = ANA_LOOP_SEL_10BT | ANA_EN_MASK_TB | ANA_EN_10BT_IDLE |
	    ((1 << ANA_INTERVAL_SEL_TIMER_SHIFT) & ANA_INTERVAL_SEL_TIMER_MASK);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_ADDR, MII_ANA_CFG18);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA, data);

	data = ((2 << ANA_SERDES_CDR_BW_SHIFT) & ANA_SERDES_CDR_BW_MASK) |
	    ANA_SERDES_EN_DEEM | ANA_SERDES_SEL_HSP | ANA_SERDES_EN_PLL |
	    ANA_SERDES_EN_LCKDT;
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_ADDR, MII_ANA_CFG5);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA, data);

	data = ((44 << ANA_LONG_CABLE_TH_100_SHIFT) &
	    ANA_LONG_CABLE_TH_100_MASK) |
	    ((33 << ANA_SHORT_CABLE_TH_100_SHIFT) &
	    ANA_SHORT_CABLE_TH_100_SHIFT) |
	    ANA_BP_BAD_LINK_ACCUM | ANA_BP_SMALL_BW;
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_ADDR, MII_ANA_CFG54);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA, data);

	data = ((11 << ANA_IECHO_ADJ_3_SHIFT) & ANA_IECHO_ADJ_3_MASK) |
	    ((11 << ANA_IECHO_ADJ_2_SHIFT) & ANA_IECHO_ADJ_2_MASK) |
	    ((8 << ANA_IECHO_ADJ_1_SHIFT) & ANA_IECHO_ADJ_1_MASK) |
	    ((8 << ANA_IECHO_ADJ_0_SHIFT) & ANA_IECHO_ADJ_0_MASK);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_ADDR, MII_ANA_CFG4);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA, data);

	data = ((7 & ANA_MANUL_SWICH_ON_SHIFT) & ANA_MANUL_SWICH_ON_MASK) |
	    ANA_RESTART_CAL | ANA_MAN_ENABLE | ANA_SEL_HSP | ANA_EN_HB |
	    ANA_OEN_125M;
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_ADDR, MII_ANA_CFG0);
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA, data);
	DELAY(1000);

	/* Disable hibernation. */
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr, ALC_MII_DBG_ADDR,
	    0x0029);
	data = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA);
	data &= ~0x8000;
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr, ALC_MII_DBG_DATA,
	    data);

	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr, ALC_MII_DBG_ADDR,
	    0x000B);
	data = alc_miibus_readreg(&sc->sc_dev, sc->alc_phyaddr,
	    ALC_MII_DBG_DATA);
	data &= ~0x8000;
	alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr, ALC_MII_DBG_DATA,
	    data);
}

void
alc_phy_down(struct alc_softc *sc)
{
	switch (sc->sc_product) {
	case PCI_PRODUCT_ATTANSIC_L1D:
	case PCI_PRODUCT_ATTANSIC_L1D_1:
		/*
		 * GPHY power down caused more problems on AR8151 v2.0.
		 * When driver is reloaded after GPHY power down,
		 * accesses to PHY/MAC registers hung the system. Only
		 * cold boot recovered from it.  I'm not sure whether
		 * AR8151 v1.0 also requires this one though.  I don't
		 * have AR8151 v1.0 controller in hand.
		 * The only option left is to isolate the PHY and
		 * initiates power down the PHY which in turn saves
		 * more power when driver is unloaded.
		 */
		alc_miibus_writereg(&sc->sc_dev, sc->alc_phyaddr,
		    MII_BMCR, BMCR_ISO | BMCR_PDOWN);
		break;
	default:
		/* Force PHY down. */
		CSR_WRITE_2(sc, ALC_GPHY_CFG, GPHY_CFG_EXT_RESET |
		    GPHY_CFG_SEL_ANA_RESET | GPHY_CFG_PHY_IDDQ | 
		    GPHY_CFG_PWDOWN_HW);
		DELAY(1000);
		break;
	}
}

void
alc_aspm(struct alc_softc *sc, uint64_t media)
{
	uint32_t pmcfg;
	uint16_t linkcfg;

	pmcfg = CSR_READ_4(sc, ALC_PM_CFG);
	if ((sc->alc_flags & (ALC_FLAG_APS | ALC_FLAG_PCIE)) ==
	    (ALC_FLAG_APS | ALC_FLAG_PCIE))
		linkcfg = CSR_READ_2(sc, sc->alc_expcap +
		    PCI_PCIE_LCSR);
	else
		linkcfg = 0;
	pmcfg &= ~PM_CFG_SERDES_PD_EX_L1;
	pmcfg &= ~(PM_CFG_L1_ENTRY_TIMER_MASK | PM_CFG_LCKDET_TIMER_MASK);
	pmcfg |= PM_CFG_MAC_ASPM_CHK;
	pmcfg |= (PM_CFG_LCKDET_TIMER_DEFAULT << PM_CFG_LCKDET_TIMER_SHIFT);
	pmcfg &= ~(PM_CFG_ASPM_L1_ENB | PM_CFG_ASPM_L0S_ENB);

	if ((sc->alc_flags & ALC_FLAG_APS) != 0) {
		/* Disable extended sync except AR8152 B v1.0 */
		linkcfg &= ~0x80;
		if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1 &&
		    sc->alc_rev == ATHEROS_AR8152_B_V10)
			linkcfg |= 0x80;
		CSR_WRITE_2(sc, sc->alc_expcap + PCI_PCIE_LCSR,
		    linkcfg);
		pmcfg &= ~(PM_CFG_EN_BUFS_RX_L0S | PM_CFG_SA_DLY_ENB |
		    PM_CFG_HOTRST);
		pmcfg |= (PM_CFG_L1_ENTRY_TIMER_DEFAULT <<
		    PM_CFG_L1_ENTRY_TIMER_SHIFT);
		pmcfg &= ~PM_CFG_PM_REQ_TIMER_MASK;
		pmcfg |= (PM_CFG_PM_REQ_TIMER_DEFAULT <<
		    PM_CFG_PM_REQ_TIMER_SHIFT);
		pmcfg |= PM_CFG_SERDES_PD_EX_L1 | PM_CFG_PCIE_RECV;
	}

	if ((sc->alc_flags & ALC_FLAG_LINK) != 0) {
		if ((sc->alc_flags & ALC_FLAG_L0S) != 0)
			pmcfg |= PM_CFG_ASPM_L0S_ENB;
		if ((sc->alc_flags & ALC_FLAG_L1S) != 0)
			pmcfg |= PM_CFG_ASPM_L1_ENB;
		if ((sc->alc_flags & ALC_FLAG_APS) != 0) {
			if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1)
				pmcfg &= ~PM_CFG_ASPM_L0S_ENB;
			pmcfg &= ~(PM_CFG_SERDES_L1_ENB |
			    PM_CFG_SERDES_PLL_L1_ENB |
			    PM_CFG_SERDES_BUDS_RX_L1_ENB);
			pmcfg |= PM_CFG_CLK_SWH_L1;
			if (media == IFM_100_TX || media == IFM_1000_T) {
				pmcfg &= ~PM_CFG_L1_ENTRY_TIMER_MASK;
				switch (sc->sc_product) {
				case PCI_PRODUCT_ATTANSIC_L2C_1:
					pmcfg |= (7 <<
					    PM_CFG_L1_ENTRY_TIMER_SHIFT);
					break;
				case PCI_PRODUCT_ATTANSIC_L1D_1:
				case PCI_PRODUCT_ATTANSIC_L2C_2:
					pmcfg |= (4 <<
					    PM_CFG_L1_ENTRY_TIMER_SHIFT);
					break;
				default:
					pmcfg |= (15 <<
					    PM_CFG_L1_ENTRY_TIMER_SHIFT);
					break;
				}
			}
		} else {
			pmcfg |= PM_CFG_SERDES_L1_ENB |
			    PM_CFG_SERDES_PLL_L1_ENB |
			    PM_CFG_SERDES_BUDS_RX_L1_ENB;
			pmcfg &= ~(PM_CFG_CLK_SWH_L1 |
			    PM_CFG_ASPM_L1_ENB | PM_CFG_ASPM_L0S_ENB);
		}
	} else {
		pmcfg &= ~(PM_CFG_SERDES_BUDS_RX_L1_ENB | PM_CFG_SERDES_L1_ENB |
		    PM_CFG_SERDES_PLL_L1_ENB);
		pmcfg |= PM_CFG_CLK_SWH_L1;
		if ((sc->alc_flags & ALC_FLAG_L1S) != 0)
			pmcfg |= PM_CFG_ASPM_L1_ENB;
	}
	CSR_WRITE_4(sc, ALC_PM_CFG, pmcfg);
}

void
alc_attach(struct device *parent, struct device *self, void *aux)
{

	struct alc_softc *sc = (struct alc_softc *)self;
	struct pci_attach_args *pa = aux;
	pci_chipset_tag_t pc = pa->pa_pc;
	pci_intr_handle_t ih;
	const char *intrstr;
	struct ifnet *ifp;
	pcireg_t memtype;
	char *aspm_state[] = { "L0s/L1", "L0s", "L1", "L0s/L1" };
	uint16_t burst;
	int base, state, error = 0;
	uint32_t cap, ctl, val;

	/*
	 * Allocate IO memory
	 */
	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, ALC_PCIR_BAR);
	if (pci_mapreg_map(pa, ALC_PCIR_BAR, memtype, 0, &sc->sc_mem_bt,
	    &sc->sc_mem_bh, NULL, &sc->sc_mem_size, 0)) {
		printf(": can't map mem space\n");
		return;
	}

	if (pci_intr_map_msi(pa, &ih) != 0 && pci_intr_map(pa, &ih) != 0) {
		printf(": can't map interrupt\n");
		goto fail;
	}

	/*
	 * Allocate IRQ
	 */
	intrstr = pci_intr_string(pc, ih);
	sc->sc_irq_handle = pci_intr_establish(pc, ih, IPL_NET, alc_intr, sc,
	    sc->sc_dev.dv_xname);
	if (sc->sc_irq_handle == NULL) {
		printf(": could not establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		goto fail;
	}
	printf(": %s", intrstr);

	sc->sc_dmat = pa->pa_dmat;
	sc->sc_pct = pa->pa_pc;
	sc->sc_pcitag = pa->pa_tag;
	
	/* Set PHY address. */
	sc->alc_phyaddr = ALC_PHY_ADDR;

	/* Get PCI and chip id/revision. */
	sc->sc_product = PCI_PRODUCT(pa->pa_id);
	sc->alc_rev = PCI_REVISION(pa->pa_class);

	/* Initialize DMA parameters. */
	sc->alc_dma_rd_burst = 0;
	sc->alc_dma_wr_burst = 0;
	sc->alc_rcb = DMA_CFG_RCB_64;
	if (pci_get_capability(pc, pa->pa_tag, PCI_CAP_PCIEXPRESS,
	    &base, NULL)) {
		sc->alc_flags |= ALC_FLAG_PCIE;
		sc->alc_expcap = base;
		burst = pci_conf_read(sc->sc_pct, sc->sc_pcitag,
		    base + PCI_PCIE_DCSR) >> 16;
		sc->alc_dma_rd_burst = (burst & 0x7000) >> 12;
		sc->alc_dma_wr_burst = (burst & 0x00e0) >> 5;
		if (alcdebug) {
			printf("%s: Read request size : %u bytes.\n",
			    sc->sc_dev.dv_xname, 
			    alc_dma_burst[sc->alc_dma_rd_burst]);
			printf("%s: TLP payload size : %u bytes.\n",
			    sc->sc_dev.dv_xname,
			    alc_dma_burst[sc->alc_dma_wr_burst]);
		}
		if (alc_dma_burst[sc->alc_dma_rd_burst] > 1024)
			sc->alc_dma_rd_burst = 3;
		if (alc_dma_burst[sc->alc_dma_wr_burst] > 1024)
			sc->alc_dma_wr_burst = 3;
		/* Clear data link and flow-control protocol error. */
		val = CSR_READ_4(sc, ALC_PEX_UNC_ERR_SEV);
		val &= ~(PEX_UNC_ERR_SEV_DLP | PEX_UNC_ERR_SEV_FCP);
		CSR_WRITE_4(sc, ALC_PEX_UNC_ERR_SEV, val);
		CSR_WRITE_4(sc, ALC_LTSSM_ID_CFG,
		    CSR_READ_4(sc, ALC_LTSSM_ID_CFG) & ~LTSSM_ID_WRO_ENB);
		CSR_WRITE_4(sc, ALC_PCIE_PHYMISC,
		    CSR_READ_4(sc, ALC_PCIE_PHYMISC) |
		    PCIE_PHYMISC_FORCE_RCV_DET);
		if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1 &&
		    sc->alc_rev == ATHEROS_AR8152_B_V10) {
			val = CSR_READ_4(sc, ALC_PCIE_PHYMISC2);
			val &= ~(PCIE_PHYMISC2_SERDES_CDR_MASK |
			    PCIE_PHYMISC2_SERDES_TH_MASK);
			val |= 3 << PCIE_PHYMISC2_SERDES_CDR_SHIFT;
			val |= 3 << PCIE_PHYMISC2_SERDES_TH_SHIFT;
			CSR_WRITE_4(sc, ALC_PCIE_PHYMISC2, val);
		}
		/* Disable ASPM L0S and L1. */
		cap = pci_conf_read(sc->sc_pct, sc->sc_pcitag,
		    base + PCI_PCIE_LCAP) >> 16;
		if ((cap & 0x00000c00) != 0) {
			ctl = pci_conf_read(sc->sc_pct, sc->sc_pcitag,
			    base + PCI_PCIE_LCSR) >> 16;
			if ((ctl & 0x08) != 0)
				sc->alc_rcb = DMA_CFG_RCB_128;
			if (alcdebug)
				printf("%s: RCB %u bytes\n",
				    sc->sc_dev.dv_xname,
				    sc->alc_rcb == DMA_CFG_RCB_64 ? 64 : 128);
			state = ctl & 0x03;
			if (state & 0x01)
				sc->alc_flags |= ALC_FLAG_L0S;
			if (state & 0x02)
				sc->alc_flags |= ALC_FLAG_L1S;
			if (alcdebug)
				printf("%s: ASPM %s %s\n",
				    sc->sc_dev.dv_xname,
				    aspm_state[state],
				    state == 0 ? "disabled" : "enabled");
			alc_disable_l0s_l1(sc);
		}
	}

	/* Reset PHY. */
	alc_phy_reset(sc);

	/* Reset the ethernet controller. */
	alc_reset(sc);

	/*
	 * One odd thing is AR8132 uses the same PHY hardware(F1
	 * gigabit PHY) of AR8131. So atphy(4) of AR8132 reports
	 * the PHY supports 1000Mbps but that's not true. The PHY
	 * used in AR8132 can't establish gigabit link even if it
	 * shows the same PHY model/revision number of AR8131.
	 */
	switch (sc->sc_product) {
	case PCI_PRODUCT_ATTANSIC_L2C_1:
	case PCI_PRODUCT_ATTANSIC_L2C_2:
		sc->alc_flags |= ALC_FLAG_APS;
		/* FALLTHROUGH */
	case PCI_PRODUCT_ATTANSIC_L2C:
		sc->alc_flags |= ALC_FLAG_FASTETHER;
		break;
	case PCI_PRODUCT_ATTANSIC_L1D:
	case PCI_PRODUCT_ATTANSIC_L1D_1:
		sc->alc_flags |= ALC_FLAG_APS;
		/* FALLTHROUGH */
	default:
		break;
	}
	sc->alc_flags |= ALC_FLAG_ASPM_MON | ALC_FLAG_JUMBO;

	switch (sc->sc_product) {
	case PCI_PRODUCT_ATTANSIC_L1C:
	case PCI_PRODUCT_ATTANSIC_L2C:
		sc->alc_max_framelen = 9 * 1024;
		break;
	case PCI_PRODUCT_ATTANSIC_L1D:
	case PCI_PRODUCT_ATTANSIC_L1D_1:
	case PCI_PRODUCT_ATTANSIC_L2C_1:
	case PCI_PRODUCT_ATTANSIC_L2C_2:
		sc->alc_max_framelen = 6 * 1024;
		break;
	}

	/*
	 * It seems that AR813x/AR815x has silicon bug for SMB. In
	 * addition, Atheros said that enabling SMB wouldn't improve
	 * performance. However I think it's bad to access lots of
	 * registers to extract MAC statistics.
	 */
	sc->alc_flags |= ALC_FLAG_SMB_BUG;
	/*
	 * Don't use Tx CMB. It is known to have silicon bug.
	 */
	sc->alc_flags |= ALC_FLAG_CMB_BUG;

	sc->alc_chip_rev = CSR_READ_4(sc, ALC_MASTER_CFG) >>
	    MASTER_CHIP_REV_SHIFT;
	if (alcdebug) {
		printf("%s: PCI device revision : 0x%04x\n",
		    sc->sc_dev.dv_xname, sc->alc_rev);
		printf("%s: Chip id/revision : 0x%04x\n",
		    sc->sc_dev.dv_xname, sc->alc_chip_rev);
		printf("%s: %u Tx FIFO, %u Rx FIFO\n", sc->sc_dev.dv_xname,
		    CSR_READ_4(sc, ALC_SRAM_TX_FIFO_LEN) * 8,
		    CSR_READ_4(sc, ALC_SRAM_RX_FIFO_LEN) * 8);
	}

	error = alc_dma_alloc(sc);
	if (error)
		goto fail;

	/* Load station address. */
	alc_get_macaddr(sc);

	ifp = &sc->sc_arpcom.ac_if;
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = alc_ioctl;
	ifp->if_start = alc_start;
	ifp->if_watchdog = alc_watchdog;
	IFQ_SET_MAXLEN(&ifp->if_snd, ALC_TX_RING_CNT - 1);
	bcopy(sc->alc_eaddr, sc->sc_arpcom.ac_enaddr, ETHER_ADDR_LEN);
	bcopy(sc->sc_dev.dv_xname, ifp->if_xname, IFNAMSIZ);

	ifp->if_capabilities = IFCAP_VLAN_MTU;

#ifdef ALC_CHECKSUM
	ifp->if_capabilities |= IFCAP_CSUM_IPv4 | IFCAP_CSUM_TCPv4 |
	    IFCAP_CSUM_UDPv4;
#endif

#if NVLAN > 0
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

	printf(", address %s\n", ether_sprintf(sc->sc_arpcom.ac_enaddr));

	/* Set up MII bus. */
	sc->sc_miibus.mii_ifp = ifp;
	sc->sc_miibus.mii_readreg = alc_miibus_readreg;
	sc->sc_miibus.mii_writereg = alc_miibus_writereg;
	sc->sc_miibus.mii_statchg = alc_miibus_statchg;

	ifmedia_init(&sc->sc_miibus.mii_media, 0, alc_mediachange,
	    alc_mediastatus);
	mii_attach(self, &sc->sc_miibus, 0xffffffff, MII_PHY_ANY,
		MII_OFFSET_ANY, MIIF_DOPAUSE);

	if (LIST_FIRST(&sc->sc_miibus.mii_phys) == NULL) {
		printf("%s: no PHY found!\n", sc->sc_dev.dv_xname);
		ifmedia_add(&sc->sc_miibus.mii_media, IFM_ETHER | IFM_MANUAL,
		    0, NULL);
		ifmedia_set(&sc->sc_miibus.mii_media, IFM_ETHER | IFM_MANUAL);
	} else 
		ifmedia_set(&sc->sc_miibus.mii_media, IFM_ETHER | IFM_AUTO);

	if_attach(ifp);
	ether_ifattach(ifp);

	timeout_set(&sc->alc_tick_ch, alc_tick, sc);

	return;
fail:
	alc_dma_free(sc);
	if (sc->sc_irq_handle != NULL)
		pci_intr_disestablish(pc, sc->sc_irq_handle);
	if (sc->sc_mem_size)
		bus_space_unmap(sc->sc_mem_bt, sc->sc_mem_bh, sc->sc_mem_size);
}

int
alc_detach(struct device *self, int flags)
{
	struct alc_softc *sc = (struct alc_softc *)self;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	int s;

	s = splnet();
	alc_stop(sc);
	splx(s);

	mii_detach(&sc->sc_miibus, MII_PHY_ANY, MII_OFFSET_ANY);

	/* Delete all remaining media. */
	ifmedia_delete_instance(&sc->sc_miibus.mii_media, IFM_INST_ANY);

	ether_ifdetach(ifp);
	if_detach(ifp);
	alc_dma_free(sc);

	alc_phy_down(sc);
	if (sc->sc_irq_handle != NULL) {
		pci_intr_disestablish(sc->sc_pct, sc->sc_irq_handle);
		sc->sc_irq_handle = NULL;
	}

	return (0);
}

int
alc_activate(struct device *self, int act)
{
	struct alc_softc *sc = (struct alc_softc *)self;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	int rv = 0;

	switch (act) {
	case DVACT_SUSPEND:
		if (ifp->if_flags & IFF_RUNNING)
			alc_stop(sc);
		rv = config_activate_children(self, act);
		break;
	case DVACT_RESUME:
		if (ifp->if_flags & IFF_UP)
			alc_init(ifp);
		break;
	default:
		rv = config_activate_children(self, act);
		break;
	}
	return (rv);
}

int
alc_dma_alloc(struct alc_softc *sc)
{
	struct alc_txdesc *txd;
	struct alc_rxdesc *rxd;
	int nsegs, error, i;

	/*
	 * Create DMA stuffs for TX ring
	 */
	error = bus_dmamap_create(sc->sc_dmat, ALC_TX_RING_SZ, 1,
	    ALC_TX_RING_SZ, 0, BUS_DMA_NOWAIT, &sc->alc_cdata.alc_tx_ring_map);
	if (error)
		return (ENOBUFS);

	/* Allocate DMA'able memory for TX ring */
	error = bus_dmamem_alloc(sc->sc_dmat, ALC_TX_RING_SZ,
	    ETHER_ALIGN, 0, &sc->alc_rdata.alc_tx_ring_seg, 1,
	    &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error) {
		printf("%s: could not allocate DMA'able memory for Tx ring.\n",
		    sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->alc_rdata.alc_tx_ring_seg,
	    nsegs, ALC_TX_RING_SZ, (caddr_t *)&sc->alc_rdata.alc_tx_ring,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/* Load the DMA map for Tx ring. */
	error = bus_dmamap_load(sc->sc_dmat, sc->alc_cdata.alc_tx_ring_map,
	    sc->alc_rdata.alc_tx_ring, ALC_TX_RING_SZ, NULL, BUS_DMA_WAITOK);
	if (error) {
		printf("%s: could not load DMA'able memory for Tx ring.\n",
		    sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat, 
		    (bus_dma_segment_t *)&sc->alc_rdata.alc_tx_ring, 1);
		return error;
	}

	sc->alc_rdata.alc_tx_ring_paddr = 
	    sc->alc_cdata.alc_tx_ring_map->dm_segs[0].ds_addr;

	/*
	 * Create DMA stuffs for RX ring
	 */
	error = bus_dmamap_create(sc->sc_dmat, ALC_RX_RING_SZ, 1,
	    ALC_RX_RING_SZ, 0, BUS_DMA_NOWAIT, &sc->alc_cdata.alc_rx_ring_map);
	if (error)
		return (ENOBUFS);
	
	/* Allocate DMA'able memory for RX ring */
	error = bus_dmamem_alloc(sc->sc_dmat, ALC_RX_RING_SZ,
	    ETHER_ALIGN, 0, &sc->alc_rdata.alc_rx_ring_seg, 1,
	    &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error) {
		printf("%s: could not allocate DMA'able memory for Rx ring.\n",
		    sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->alc_rdata.alc_rx_ring_seg,
	    nsegs, ALC_RX_RING_SZ, (caddr_t *)&sc->alc_rdata.alc_rx_ring,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/* Load the DMA map for Rx ring. */
	error = bus_dmamap_load(sc->sc_dmat, sc->alc_cdata.alc_rx_ring_map,
	    sc->alc_rdata.alc_rx_ring, ALC_RX_RING_SZ, NULL, BUS_DMA_WAITOK);
	if (error) {
		printf("%s: could not load DMA'able memory for Rx ring.\n",
		    sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)sc->alc_rdata.alc_rx_ring, 1);
		return error;
	}

	sc->alc_rdata.alc_rx_ring_paddr =
	    sc->alc_cdata.alc_rx_ring_map->dm_segs[0].ds_addr;

	/*
	 * Create DMA stuffs for RX return ring
	 */
	error = bus_dmamap_create(sc->sc_dmat, ALC_RR_RING_SZ, 1, 
	    ALC_RR_RING_SZ, 0, BUS_DMA_NOWAIT, &sc->alc_cdata.alc_rr_ring_map);
	if (error)
		return (ENOBUFS);

	/* Allocate DMA'able memory for RX return ring */
	error = bus_dmamem_alloc(sc->sc_dmat, ALC_RR_RING_SZ, 
	    ETHER_ALIGN, 0, &sc->alc_rdata.alc_rr_ring_seg, 1, 
	    &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error) {
		printf("%s: could not allocate DMA'able memory for Rx "
		    "return ring.\n", sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->alc_rdata.alc_rr_ring_seg,
	    nsegs, ALC_RR_RING_SZ, (caddr_t *)&sc->alc_rdata.alc_rr_ring,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/*  Load the DMA map for Rx return ring. */
	error = bus_dmamap_load(sc->sc_dmat, sc->alc_cdata.alc_rr_ring_map,
	    sc->alc_rdata.alc_rr_ring, ALC_RR_RING_SZ, NULL, BUS_DMA_WAITOK);
	if (error) {
		printf("%s: could not load DMA'able memory for Rx return ring."
		    "\n", sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)&sc->alc_rdata.alc_rr_ring, 1);
		return error;
	}

	sc->alc_rdata.alc_rr_ring_paddr = 
	    sc->alc_cdata.alc_rr_ring_map->dm_segs[0].ds_addr;

	/*
	 * Create DMA stuffs for CMB block 
	 */
	error = bus_dmamap_create(sc->sc_dmat, ALC_CMB_SZ, 1, 
	    ALC_CMB_SZ, 0, BUS_DMA_NOWAIT, 
	    &sc->alc_cdata.alc_cmb_map);
	if (error) 
		return (ENOBUFS);

	/* Allocate DMA'able memory for CMB block */
	error = bus_dmamem_alloc(sc->sc_dmat, ALC_CMB_SZ, 
	    ETHER_ALIGN, 0, &sc->alc_rdata.alc_cmb_seg, 1, 
	    &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error) {
		printf("%s: could not allocate DMA'able memory for "
		    "CMB block\n", sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->alc_rdata.alc_cmb_seg,
	    nsegs, ALC_CMB_SZ, (caddr_t *)&sc->alc_rdata.alc_cmb,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/*  Load the DMA map for CMB block. */
	error = bus_dmamap_load(sc->sc_dmat, sc->alc_cdata.alc_cmb_map,
	    sc->alc_rdata.alc_cmb, ALC_CMB_SZ, NULL, 
	    BUS_DMA_WAITOK);
	if (error) {
		printf("%s: could not load DMA'able memory for CMB block\n",
		    sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)&sc->alc_rdata.alc_cmb, 1);
		return error;
	}

	sc->alc_rdata.alc_cmb_paddr = 
	    sc->alc_cdata.alc_cmb_map->dm_segs[0].ds_addr;

	/*
	 * Create DMA stuffs for SMB block
	 */
	error = bus_dmamap_create(sc->sc_dmat, ALC_SMB_SZ, 1, 
	    ALC_SMB_SZ, 0, BUS_DMA_NOWAIT, 
	    &sc->alc_cdata.alc_smb_map);
	if (error)
		return (ENOBUFS);

	/* Allocate DMA'able memory for SMB block */
	error = bus_dmamem_alloc(sc->sc_dmat, ALC_SMB_SZ, 
	    ETHER_ALIGN, 0, &sc->alc_rdata.alc_smb_seg, 1, 
	    &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (error) {
		printf("%s: could not allocate DMA'able memory for "
		    "SMB block\n", sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->alc_rdata.alc_smb_seg,
	    nsegs, ALC_SMB_SZ, (caddr_t *)&sc->alc_rdata.alc_smb,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/*  Load the DMA map for SMB block */
	error = bus_dmamap_load(sc->sc_dmat, sc->alc_cdata.alc_smb_map,
	    sc->alc_rdata.alc_smb, ALC_SMB_SZ, NULL, 
	    BUS_DMA_WAITOK);
	if (error) {
		printf("%s: could not load DMA'able memory for SMB block\n",
		    sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)&sc->alc_rdata.alc_smb, 1);
		return error;
	}

	sc->alc_rdata.alc_smb_paddr = 
	    sc->alc_cdata.alc_smb_map->dm_segs[0].ds_addr;


	/* Create DMA maps for Tx buffers. */
	for (i = 0; i < ALC_TX_RING_CNT; i++) {
		txd = &sc->alc_cdata.alc_txdesc[i];
		txd->tx_m = NULL;
		txd->tx_dmamap = NULL;
		error = bus_dmamap_create(sc->sc_dmat, ALC_TSO_MAXSIZE,
		    ALC_MAXTXSEGS, ALC_TSO_MAXSEGSIZE, 0, BUS_DMA_NOWAIT,
		    &txd->tx_dmamap);
		if (error) {
			printf("%s: could not create Tx dmamap.\n",
			    sc->sc_dev.dv_xname);
			return error;
		}
	}

	/* Create DMA maps for Rx buffers. */
	error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1, MCLBYTES, 0,
	    BUS_DMA_NOWAIT, &sc->alc_cdata.alc_rx_sparemap);
	if (error) {
		printf("%s: could not create spare Rx dmamap.\n",
		    sc->sc_dev.dv_xname);
		return error;
	}

	for (i = 0; i < ALC_RX_RING_CNT; i++) {
		rxd = &sc->alc_cdata.alc_rxdesc[i];
		rxd->rx_m = NULL;
		rxd->rx_dmamap = NULL;
		error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1,
		    MCLBYTES, 0, BUS_DMA_NOWAIT, &rxd->rx_dmamap);
		if (error) {
			printf("%s: could not create Rx dmamap.\n",
			    sc->sc_dev.dv_xname);
			return error;
		}
	}

	return (0);
}


void
alc_dma_free(struct alc_softc *sc)
{
	struct alc_txdesc *txd;
	struct alc_rxdesc *rxd;
	int i;

	/* Tx buffers */
	for (i = 0; i < ALC_TX_RING_CNT; i++) {
		txd = &sc->alc_cdata.alc_txdesc[i];
		if (txd->tx_dmamap != NULL) {
			bus_dmamap_destroy(sc->sc_dmat, txd->tx_dmamap);
			txd->tx_dmamap = NULL;
		}
	}
	/* Rx buffers */
	for (i = 0; i < ALC_RX_RING_CNT; i++) {
		rxd = &sc->alc_cdata.alc_rxdesc[i];
		if (rxd->rx_dmamap != NULL) {
			bus_dmamap_destroy(sc->sc_dmat, rxd->rx_dmamap);
			rxd->rx_dmamap = NULL;
		}
	}
	if (sc->alc_cdata.alc_rx_sparemap != NULL) {
		bus_dmamap_destroy(sc->sc_dmat, sc->alc_cdata.alc_rx_sparemap);
		sc->alc_cdata.alc_rx_sparemap = NULL;
	}

	/* Tx ring. */
	if (sc->alc_cdata.alc_tx_ring_map != NULL)
		bus_dmamap_unload(sc->sc_dmat, sc->alc_cdata.alc_tx_ring_map);
	if (sc->alc_cdata.alc_tx_ring_map != NULL &&
	    sc->alc_rdata.alc_tx_ring != NULL)
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)sc->alc_rdata.alc_tx_ring, 1);
	sc->alc_rdata.alc_tx_ring = NULL;
	sc->alc_cdata.alc_tx_ring_map = NULL;

	/* Rx ring. */
	if (sc->alc_cdata.alc_rx_ring_map != NULL) 
		bus_dmamap_unload(sc->sc_dmat, sc->alc_cdata.alc_rx_ring_map);
	if (sc->alc_cdata.alc_rx_ring_map != NULL &&
	    sc->alc_rdata.alc_rx_ring != NULL)
		bus_dmamem_free(sc->sc_dmat, 
		    (bus_dma_segment_t *)sc->alc_rdata.alc_rx_ring, 1);
	sc->alc_rdata.alc_rx_ring = NULL;
	sc->alc_cdata.alc_rx_ring_map = NULL;

	/* Rx return ring. */
	if (sc->alc_cdata.alc_rr_ring_map != NULL)
		bus_dmamap_unload(sc->sc_dmat, sc->alc_cdata.alc_rr_ring_map);
	if (sc->alc_cdata.alc_rr_ring_map != NULL &&
	    sc->alc_rdata.alc_rr_ring != NULL)
		bus_dmamem_free(sc->sc_dmat, 
		    (bus_dma_segment_t *)sc->alc_rdata.alc_rr_ring, 1);
	sc->alc_rdata.alc_rr_ring = NULL;
	sc->alc_cdata.alc_rr_ring_map = NULL;

	/* CMB block */
	if (sc->alc_cdata.alc_cmb_map != NULL)
		bus_dmamap_unload(sc->sc_dmat, sc->alc_cdata.alc_cmb_map);
	if (sc->alc_cdata.alc_cmb_map != NULL &&
	    sc->alc_rdata.alc_cmb != NULL)
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)sc->alc_rdata.alc_cmb, 1);
	sc->alc_rdata.alc_cmb = NULL;
	sc->alc_cdata.alc_cmb_map = NULL;

	/* SMB block */
	if (sc->alc_cdata.alc_smb_map != NULL)
		bus_dmamap_unload(sc->sc_dmat, sc->alc_cdata.alc_smb_map);
	if (sc->alc_cdata.alc_smb_map != NULL &&
	    sc->alc_rdata.alc_smb != NULL)
		bus_dmamem_free(sc->sc_dmat, 
		    (bus_dma_segment_t *)sc->alc_rdata.alc_smb, 1);
	sc->alc_rdata.alc_smb = NULL;
	sc->alc_cdata.alc_smb_map = NULL;
}

int
alc_encap(struct alc_softc *sc, struct mbuf *m)
{
	struct alc_txdesc *txd, *txd_last;
	struct tx_desc *desc;
	bus_dmamap_t map;
	uint32_t cflags, poff, vtag;
	int error, idx, prod;

	cflags = vtag = 0;
	poff = 0;

	prod = sc->alc_cdata.alc_tx_prod;
	txd = &sc->alc_cdata.alc_txdesc[prod];
	txd_last = txd;
	map = txd->tx_dmamap;

	error = bus_dmamap_load_mbuf(sc->sc_dmat, map, m, BUS_DMA_NOWAIT);
	if (error != 0 && error != EFBIG)
		goto drop;
	if (error != 0) {
		if (m_defrag(m, M_DONTWAIT)) {
			error = ENOBUFS;
			goto drop;
		}
		error = bus_dmamap_load_mbuf(sc->sc_dmat, map, m,
		    BUS_DMA_NOWAIT);
		if (error != 0)
			goto drop;
	}

	bus_dmamap_sync(sc->sc_dmat, map, 0, map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	desc = NULL;
	idx = 0;
#if NVLAN > 0
	/* Configure VLAN hardware tag insertion. */
	if (m->m_flags & M_VLANTAG) {
		vtag = htons(m->m_pkthdr.ether_vtag);
		vtag = (vtag << TD_VLAN_SHIFT) & TD_VLAN_MASK;
		cflags |= TD_INS_VLAN_TAG;
	}
#endif
	/* Configure Tx checksum offload. */
	if ((m->m_pkthdr.csum_flags & ALC_CSUM_FEATURES) != 0) {
		cflags |= TD_CUSTOM_CSUM;
		/* Set checksum start offset. */
		cflags |= ((poff >> 1) << TD_PLOAD_OFFSET_SHIFT) &
		    TD_PLOAD_OFFSET_MASK;
	}

	for (; idx < map->dm_nsegs; idx++) {
		desc = &sc->alc_rdata.alc_tx_ring[prod];
		desc->len =
		    htole32(TX_BYTES(map->dm_segs[idx].ds_len) | vtag);
		desc->flags = htole32(cflags);
		desc->addr = htole64(map->dm_segs[idx].ds_addr);
		sc->alc_cdata.alc_tx_cnt++;
		ALC_DESC_INC(prod, ALC_TX_RING_CNT);
	}

	/* Update producer index. */
	sc->alc_cdata.alc_tx_prod = prod;

	/* Finally set EOP on the last descriptor. */
	prod = (prod + ALC_TX_RING_CNT - 1) % ALC_TX_RING_CNT;
	desc = &sc->alc_rdata.alc_tx_ring[prod];
	desc->flags |= htole32(TD_EOP);

	/* Swap dmamap of the first and the last. */
	txd = &sc->alc_cdata.alc_txdesc[prod];
	map = txd_last->tx_dmamap;
	txd_last->tx_dmamap = txd->tx_dmamap;
	txd->tx_dmamap = map;
	txd->tx_m = m;

	return (0);

 drop:
	m_freem(m);
	return (error);
}

void
alc_start(struct ifnet *ifp)
{
	struct alc_softc *sc = ifp->if_softc;
	struct mbuf *m;
	int enq = 0;

	/* Reclaim transmitted frames. */
	if (sc->alc_cdata.alc_tx_cnt >= ALC_TX_DESC_HIWAT)
		alc_txeof(sc);

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;
	if ((sc->alc_flags & ALC_FLAG_LINK) == 0)
		return;
	if (IFQ_IS_EMPTY(&ifp->if_snd))
		return;

	for (;;) {
		if (sc->alc_cdata.alc_tx_cnt + ALC_MAXTXSEGS >=
		    ALC_TX_RING_CNT - 3) {
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

		IFQ_DEQUEUE(&ifp->if_snd, m);
		if (m == NULL)
			break;

		if (alc_encap(sc, m) != 0) {
			ifp->if_oerrors++;
			continue;
		}
		enq++;
		
#if NBPFILTER > 0
		/*
		 * If there's a BPF listener, bounce a copy of this frame
		 * to him.
		 */
		if (ifp->if_bpf != NULL)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
	}

	if (enq > 0) {
		/* Sync descriptors. */
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_tx_ring_map, 0,
		    sc->alc_cdata.alc_tx_ring_map->dm_mapsize, 
		    BUS_DMASYNC_PREWRITE);
		/* Kick. Assume we're using normal Tx priority queue. */
		CSR_WRITE_4(sc, ALC_MBOX_TD_PROD_IDX,
		    (sc->alc_cdata.alc_tx_prod <<
		    MBOX_TD_PROD_LO_IDX_SHIFT) &
		    MBOX_TD_PROD_LO_IDX_MASK);
		/* Set a timeout in case the chip goes out to lunch. */
		ifp->if_timer = ALC_TX_TIMEOUT;
	}
}

void
alc_watchdog(struct ifnet *ifp)
{
	struct alc_softc *sc = ifp->if_softc;

	if ((sc->alc_flags & ALC_FLAG_LINK) == 0) {
		printf("%s: watchdog timeout (missed link)\n",
		    sc->sc_dev.dv_xname);
		ifp->if_oerrors++;
		alc_init(ifp);
		return;
	}

	printf("%s: watchdog timeout\n", sc->sc_dev.dv_xname);
	ifp->if_oerrors++;
	alc_init(ifp);
	alc_start(ifp);
}

int
alc_ioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct alc_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_miibus;
	struct ifreq *ifr = (struct ifreq *)data;
	int s, error = 0;

	s = splnet();

	switch (cmd) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			alc_init(ifp);
		break;

	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				alc_init(ifp);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				alc_stop(sc);
		}
		break;

	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &mii->mii_media, cmd);
		break;

	default:
		error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data);
		break;
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			alc_iff(sc);
		error = 0;
	}

	splx(s);
	return (error);
}

void
alc_mac_config(struct alc_softc *sc)
{
	struct mii_data *mii;
	uint32_t reg;

	mii = &sc->sc_miibus;
	reg = CSR_READ_4(sc, ALC_MAC_CFG);
	reg &= ~(MAC_CFG_FULL_DUPLEX | MAC_CFG_TX_FC | MAC_CFG_RX_FC |
	    MAC_CFG_SPEED_MASK);
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_2)
		reg |= MAC_CFG_HASH_ALG_CRC32 | MAC_CFG_SPEED_MODE_SW;
	/* Reprogram MAC with resolved speed/duplex. */
	switch (IFM_SUBTYPE(mii->mii_media_active)) {
	case IFM_10_T:
	case IFM_100_TX:
		reg |= MAC_CFG_SPEED_10_100;
		break;
	case IFM_1000_T:
		reg |= MAC_CFG_SPEED_1000;
		break;
	}
	if ((IFM_OPTIONS(mii->mii_media_active) & IFM_FDX) != 0) {
		reg |= MAC_CFG_FULL_DUPLEX;
		if ((IFM_OPTIONS(mii->mii_media_active) & IFM_ETH_TXPAUSE) != 0)
			reg |= MAC_CFG_TX_FC;
		if ((IFM_OPTIONS(mii->mii_media_active) & IFM_ETH_RXPAUSE) != 0)
			reg |= MAC_CFG_RX_FC;
	}
	CSR_WRITE_4(sc, ALC_MAC_CFG, reg);
}

void
alc_stats_clear(struct alc_softc *sc)
{
	struct smb sb, *smb;
	uint32_t *reg;
	int i;

	if ((sc->alc_flags & ALC_FLAG_SMB_BUG) == 0) {
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_smb_map, 0,
		    sc->alc_cdata.alc_smb_map->dm_mapsize, 
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		smb = sc->alc_rdata.alc_smb;
		/* Update done, clear. */
		smb->updated = 0;
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_smb_map, 0,
		    sc->alc_cdata.alc_smb_map->dm_mapsize, 
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	} else {
		for (reg = &sb.rx_frames, i = 0; reg <= &sb.rx_pkts_filtered;
		    reg++) {
			CSR_READ_4(sc, ALC_RX_MIB_BASE + i);
			i += sizeof(uint32_t);
		}
		/* Read Tx statistics. */
		for (reg = &sb.tx_frames, i = 0; reg <= &sb.tx_mcast_bytes;
		    reg++) {
			CSR_READ_4(sc, ALC_TX_MIB_BASE + i);
			i += sizeof(uint32_t);
		}
	}
}

void
alc_stats_update(struct alc_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct alc_hw_stats *stat;
	struct smb sb, *smb;
	uint32_t *reg;
	int i;

	stat = &sc->alc_stats;
	if ((sc->alc_flags & ALC_FLAG_SMB_BUG) == 0) {
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_smb_map, 0,
		    sc->alc_cdata.alc_smb_map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		smb = sc->alc_rdata.alc_smb;
		if (smb->updated == 0)
			return;
	} else {
		smb = &sb;
		/* Read Rx statistics. */
		for (reg = &sb.rx_frames, i = 0; reg <= &sb.rx_pkts_filtered;
		    reg++) {
			*reg = CSR_READ_4(sc, ALC_RX_MIB_BASE + i);
			i += sizeof(uint32_t);
		}
		/* Read Tx statistics. */
		for (reg = &sb.tx_frames, i = 0; reg <= &sb.tx_mcast_bytes;
		    reg++) {
			*reg = CSR_READ_4(sc, ALC_TX_MIB_BASE + i);
			i += sizeof(uint32_t);
		}
	}

	/* Rx stats. */
	stat->rx_frames += smb->rx_frames;
	stat->rx_bcast_frames += smb->rx_bcast_frames;
	stat->rx_mcast_frames += smb->rx_mcast_frames;
	stat->rx_pause_frames += smb->rx_pause_frames;
	stat->rx_control_frames += smb->rx_control_frames;
	stat->rx_crcerrs += smb->rx_crcerrs;
	stat->rx_lenerrs += smb->rx_lenerrs;
	stat->rx_bytes += smb->rx_bytes;
	stat->rx_runts += smb->rx_runts;
	stat->rx_fragments += smb->rx_fragments;
	stat->rx_pkts_64 += smb->rx_pkts_64;
	stat->rx_pkts_65_127 += smb->rx_pkts_65_127;
	stat->rx_pkts_128_255 += smb->rx_pkts_128_255;
	stat->rx_pkts_256_511 += smb->rx_pkts_256_511;
	stat->rx_pkts_512_1023 += smb->rx_pkts_512_1023;
	stat->rx_pkts_1024_1518 += smb->rx_pkts_1024_1518;
	stat->rx_pkts_1519_max += smb->rx_pkts_1519_max;
	stat->rx_pkts_truncated += smb->rx_pkts_truncated;
	stat->rx_fifo_oflows += smb->rx_fifo_oflows;
	stat->rx_rrs_errs += smb->rx_rrs_errs;
	stat->rx_alignerrs += smb->rx_alignerrs;
	stat->rx_bcast_bytes += smb->rx_bcast_bytes;
	stat->rx_mcast_bytes += smb->rx_mcast_bytes;
	stat->rx_pkts_filtered += smb->rx_pkts_filtered;

	/* Tx stats. */
	stat->tx_frames += smb->tx_frames;
	stat->tx_bcast_frames += smb->tx_bcast_frames;
	stat->tx_mcast_frames += smb->tx_mcast_frames;
	stat->tx_pause_frames += smb->tx_pause_frames;
	stat->tx_excess_defer += smb->tx_excess_defer;
	stat->tx_control_frames += smb->tx_control_frames;
	stat->tx_deferred += smb->tx_deferred;
	stat->tx_bytes += smb->tx_bytes;
	stat->tx_pkts_64 += smb->tx_pkts_64;
	stat->tx_pkts_65_127 += smb->tx_pkts_65_127;
	stat->tx_pkts_128_255 += smb->tx_pkts_128_255;
	stat->tx_pkts_256_511 += smb->tx_pkts_256_511;
	stat->tx_pkts_512_1023 += smb->tx_pkts_512_1023;
	stat->tx_pkts_1024_1518 += smb->tx_pkts_1024_1518;
	stat->tx_pkts_1519_max += smb->tx_pkts_1519_max;
	stat->tx_single_colls += smb->tx_single_colls;
	stat->tx_multi_colls += smb->tx_multi_colls;
	stat->tx_late_colls += smb->tx_late_colls;
	stat->tx_excess_colls += smb->tx_excess_colls;
	stat->tx_underrun += smb->tx_underrun;
	stat->tx_desc_underrun += smb->tx_desc_underrun;
	stat->tx_lenerrs += smb->tx_lenerrs;
	stat->tx_pkts_truncated += smb->tx_pkts_truncated;
	stat->tx_bcast_bytes += smb->tx_bcast_bytes;
	stat->tx_mcast_bytes += smb->tx_mcast_bytes;

	ifp->if_collisions += smb->tx_single_colls +
	    smb->tx_multi_colls * 2 + smb->tx_late_colls +
	    smb->tx_excess_colls * HDPX_CFG_RETRY_DEFAULT;

	ifp->if_oerrors += smb->tx_late_colls + smb->tx_excess_colls +
	    smb->tx_underrun + smb->tx_pkts_truncated;

	ifp->if_ierrors += smb->rx_crcerrs + smb->rx_lenerrs +
	    smb->rx_runts + smb->rx_pkts_truncated +
	    smb->rx_fifo_oflows + smb->rx_rrs_errs +
	    smb->rx_alignerrs;

	if ((sc->alc_flags & ALC_FLAG_SMB_BUG) == 0) {
		/* Update done, clear. */
		smb->updated = 0;
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_smb_map, 0,
		    sc->alc_cdata.alc_smb_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	}
}

int
alc_intr(void *arg)
{
	struct alc_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	uint32_t status;
	int claimed = 0;

	status = CSR_READ_4(sc, ALC_INTR_STATUS);
	if ((status & ALC_INTRS) == 0)
		return (0);

	/* Disable interrupts. */
	CSR_WRITE_4(sc, ALC_INTR_STATUS, INTR_DIS_INT);

	status = CSR_READ_4(sc, ALC_INTR_STATUS);
	if ((status & ALC_INTRS) == 0)
		goto back;

	/* Acknowledge and disable interrupts. */
	CSR_WRITE_4(sc, ALC_INTR_STATUS, status | INTR_DIS_INT);

	if (ifp->if_flags & IFF_RUNNING) {
		if (status & INTR_RX_PKT)
			alc_rxintr(sc);

		if (status & (INTR_DMA_RD_TO_RST | INTR_DMA_WR_TO_RST |
		    INTR_TXQ_TO_RST)) {
			if (status & INTR_DMA_RD_TO_RST)
				printf("%s: DMA read error! -- resetting\n",
				    sc->sc_dev.dv_xname);
			if (status & INTR_DMA_WR_TO_RST)
				printf("%s: DMA write error! -- resetting\n",
				    sc->sc_dev.dv_xname);
			if (status & INTR_TXQ_TO_RST)
				printf("%s: TxQ reset! -- resetting\n",
				    sc->sc_dev.dv_xname);
			alc_init(ifp);
			return (0);
		}

		if (status & INTR_TX_PKT)
			alc_txeof(sc);

		alc_start(ifp);
	}

	claimed = 1;
back:
	/* Re-enable interrupts. */
	CSR_WRITE_4(sc, ALC_INTR_STATUS, 0x7FFFFFFF);
	return (claimed);
}

void
alc_txeof(struct alc_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct alc_txdesc *txd;
	uint32_t cons, prod;
	int prog;

	if (sc->alc_cdata.alc_tx_cnt == 0)
		return;
	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_tx_ring_map, 0,
	    sc->alc_cdata.alc_tx_ring_map->dm_mapsize,
	    BUS_DMASYNC_POSTWRITE);
	if ((sc->alc_flags & ALC_FLAG_CMB_BUG) == 0) {
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_cmb_map, 0,
		    sc->alc_cdata.alc_cmb_map->dm_mapsize, 
		    BUS_DMASYNC_POSTREAD);
		prod = sc->alc_rdata.alc_cmb->cons;
	} else
		prod = CSR_READ_4(sc, ALC_MBOX_TD_CONS_IDX);
	/* Assume we're using normal Tx priority queue. */
	prod = (prod & MBOX_TD_CONS_LO_IDX_MASK) >>
	    MBOX_TD_CONS_LO_IDX_SHIFT;
	cons = sc->alc_cdata.alc_tx_cons;
	/*
	 * Go through our Tx list and free mbufs for those
	 * frames which have been transmitted.
	 */
	for (prog = 0; cons != prod; prog++,
	    ALC_DESC_INC(cons, ALC_TX_RING_CNT)) {
		if (sc->alc_cdata.alc_tx_cnt <= 0)
			break;
		prog++;
		ifq_clr_oactive(&ifp->if_snd);
		sc->alc_cdata.alc_tx_cnt--;
		txd = &sc->alc_cdata.alc_txdesc[cons];
		if (txd->tx_m != NULL) {
			/* Reclaim transmitted mbufs. */
			bus_dmamap_sync(sc->sc_dmat, txd->tx_dmamap, 0,
			    txd->tx_dmamap->dm_mapsize, BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, txd->tx_dmamap);
			m_freem(txd->tx_m);
			txd->tx_m = NULL;
		}
	}

	if ((sc->alc_flags & ALC_FLAG_CMB_BUG) == 0)
	    bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_cmb_map, 0,
	        sc->alc_cdata.alc_cmb_map->dm_mapsize, BUS_DMASYNC_PREREAD);
	sc->alc_cdata.alc_tx_cons = cons;
	/*
	 * Unarm watchdog timer only when there is no pending
	 * frames in Tx queue.
	 */
	if (sc->alc_cdata.alc_tx_cnt == 0)
		ifp->if_timer = 0;
}

int
alc_newbuf(struct alc_softc *sc, struct alc_rxdesc *rxd)
{
	struct mbuf *m;
	bus_dmamap_t map;
	int error;

	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL)
		return (ENOBUFS);
	MCLGET(m, M_DONTWAIT);
	if (!(m->m_flags & M_EXT)) {
		m_freem(m);
		return (ENOBUFS);
	}

	m->m_len = m->m_pkthdr.len = RX_BUF_SIZE_MAX;

	error = bus_dmamap_load_mbuf(sc->sc_dmat,
	    sc->alc_cdata.alc_rx_sparemap, m, BUS_DMA_NOWAIT);

	if (error != 0) {
		m_freem(m);
		printf("%s: can't load RX mbuf\n", sc->sc_dev.dv_xname);
		return (error);
	}

	if (rxd->rx_m != NULL) {
		bus_dmamap_sync(sc->sc_dmat, rxd->rx_dmamap, 0,
		    rxd->rx_dmamap->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, rxd->rx_dmamap);
	}
	map = rxd->rx_dmamap;
	rxd->rx_dmamap = sc->alc_cdata.alc_rx_sparemap;
	sc->alc_cdata.alc_rx_sparemap = map;
	bus_dmamap_sync(sc->sc_dmat, rxd->rx_dmamap, 0, rxd->rx_dmamap->dm_mapsize,
	    BUS_DMASYNC_PREREAD);
	rxd->rx_m = m;
	rxd->rx_desc->addr = htole64(rxd->rx_dmamap->dm_segs[0].ds_addr);
	return (0);
}

void
alc_rxintr(struct alc_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct rx_rdesc *rrd;
	uint32_t nsegs, status;
	int rr_cons, prog;

	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_rr_ring_map, 0,
	    sc->alc_cdata.alc_rr_ring_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_rx_ring_map, 0,
	    sc->alc_cdata.alc_rx_ring_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
	rr_cons = sc->alc_cdata.alc_rr_cons;
	for (prog = 0; (ifp->if_flags & IFF_RUNNING) != 0;) {
		rrd = &sc->alc_rdata.alc_rr_ring[rr_cons];
		status = letoh32(rrd->status);
		if ((status & RRD_VALID) == 0)
			break;
		nsegs = RRD_RD_CNT(letoh32(rrd->rdinfo));
		if (nsegs == 0) {
			/* This should not happen! */
			if (alcdebug)
				printf("%s: unexpected segment count -- "
				    "resetting\n", sc->sc_dev.dv_xname);
			break;
		}
		alc_rxeof(sc, rrd);
		/* Clear Rx return status. */
		rrd->status = 0;
		ALC_DESC_INC(rr_cons, ALC_RR_RING_CNT);
		sc->alc_cdata.alc_rx_cons += nsegs;
		sc->alc_cdata.alc_rx_cons %= ALC_RR_RING_CNT;
		prog += nsegs;
	}

	if (prog > 0) {
		/* Update the consumer index. */
		sc->alc_cdata.alc_rr_cons = rr_cons;
		/* Sync Rx return descriptors. */
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_rr_ring_map, 0,
		    sc->alc_cdata.alc_rr_ring_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
		/*
		 * Sync updated Rx descriptors such that controller see
		 * modified buffer addresses.
		 */
		bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_rx_ring_map, 0,
		    sc->alc_cdata.alc_rx_ring_map->dm_mapsize,
		    BUS_DMASYNC_PREWRITE);
		/*
		 * Let controller know availability of new Rx buffers.
		 * Since alc(4) use RXQ_CFG_RD_BURST_DEFAULT descriptors
		 * it may be possible to update ALC_MBOX_RD0_PROD_IDX
		 * only when Rx buffer pre-fetching is required. In
		 * addition we already set ALC_RX_RD_FREE_THRESH to
		 * RX_RD_FREE_THRESH_LO_DEFAULT descriptors. However
		 * it still seems that pre-fetching needs more
		 * experimentation.
		 */
		CSR_WRITE_4(sc, ALC_MBOX_RD0_PROD_IDX,
		    sc->alc_cdata.alc_rx_cons);
	}
}

/* Receive a frame. */
void
alc_rxeof(struct alc_softc *sc, struct rx_rdesc *rrd)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct alc_rxdesc *rxd;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *mp, *m;
	uint32_t rdinfo, status;
	int count, nsegs, rx_cons;

	status = letoh32(rrd->status);
	rdinfo = letoh32(rrd->rdinfo);
	rx_cons = RRD_RD_IDX(rdinfo);
	nsegs = RRD_RD_CNT(rdinfo);

	sc->alc_cdata.alc_rxlen = RRD_BYTES(status);
	if (status & (RRD_ERR_SUM | RRD_ERR_LENGTH)) {
		/*
		 * We want to pass the following frames to upper
		 * layer regardless of error status of Rx return
		 * ring.
		 *
		 *  o IP/TCP/UDP checksum is bad.
		 *  o frame length and protocol specific length
		 *     does not match.
		 *
		 *  Force network stack compute checksum for
		 *  errored frames.
		 */
		if ((status & (RRD_ERR_CRC | RRD_ERR_ALIGN |
		    RRD_ERR_TRUNC | RRD_ERR_RUNT)) != 0)
			return;
	}

	for (count = 0; count < nsegs; count++,
	    ALC_DESC_INC(rx_cons, ALC_RX_RING_CNT)) {
		rxd = &sc->alc_cdata.alc_rxdesc[rx_cons];
		mp = rxd->rx_m;
		/* Add a new receive buffer to the ring. */
		if (alc_newbuf(sc, rxd) != 0) {
			ifp->if_iqdrops++;
			/* Reuse Rx buffers. */
			m_freem(sc->alc_cdata.alc_rxhead);
			break;
		}

		/*
		 * Assume we've received a full sized frame.
		 * Actual size is fixed when we encounter the end of
		 * multi-segmented frame.
		 */
		mp->m_len = sc->alc_buf_size;

		/* Chain received mbufs. */
		if (sc->alc_cdata.alc_rxhead == NULL) {
			sc->alc_cdata.alc_rxhead = mp;
			sc->alc_cdata.alc_rxtail = mp;
		} else {
			mp->m_flags &= ~M_PKTHDR;
			sc->alc_cdata.alc_rxprev_tail =
			    sc->alc_cdata.alc_rxtail;
			sc->alc_cdata.alc_rxtail->m_next = mp;
			sc->alc_cdata.alc_rxtail = mp;
		}

		if (count == nsegs - 1) {
			/* Last desc. for this frame. */
			m = sc->alc_cdata.alc_rxhead;
			m->m_flags |= M_PKTHDR;
			/*
			 * It seems that L1C/L2C controller has no way
			 * to tell hardware to strip CRC bytes.
			 */
			m->m_pkthdr.len =
			    sc->alc_cdata.alc_rxlen - ETHER_CRC_LEN;
			if (nsegs > 1) {
				/* Set last mbuf size. */
				mp->m_len = sc->alc_cdata.alc_rxlen -
				    (nsegs - 1) * sc->alc_buf_size;
				/* Remove the CRC bytes in chained mbufs. */
				if (mp->m_len <= ETHER_CRC_LEN) {
					sc->alc_cdata.alc_rxtail =
					    sc->alc_cdata.alc_rxprev_tail;
					sc->alc_cdata.alc_rxtail->m_len -=
					    (ETHER_CRC_LEN - mp->m_len);
					sc->alc_cdata.alc_rxtail->m_next = NULL;
					m_freem(mp);
				} else {
					mp->m_len -= ETHER_CRC_LEN;
				}
			} else
				m->m_len = m->m_pkthdr.len;
			/*
			 * Due to hardware bugs, Rx checksum offloading
			 * was intentionally disabled.
			 */
#if NVLAN > 0
			if (status & RRD_VLAN_TAG) {
				u_int32_t vtag = RRD_VLAN(letoh32(rrd->vtag));
				m->m_pkthdr.ether_vtag = ntohs(vtag);
				m->m_flags |= M_VLANTAG;
			}
#endif


			ml_enqueue(&ml, m);
		}
	}
	if_input(ifp, &ml);

	/* Reset mbuf chains. */
	ALC_RXCHAIN_RESET(sc);
}

void
alc_tick(void *xsc)
{
	struct alc_softc *sc = xsc;
	struct mii_data *mii = &sc->sc_miibus;
	int s;

	s = splnet();
	mii_tick(mii);
	alc_stats_update(sc);

	timeout_add_sec(&sc->alc_tick_ch, 1);
	splx(s);
}

void
alc_reset(struct alc_softc *sc)
{
	uint32_t reg;
	int i;

	reg = CSR_READ_4(sc, ALC_MASTER_CFG) & 0xFFFF;
	reg |= MASTER_OOB_DIS_OFF | MASTER_RESET;
	CSR_WRITE_4(sc, ALC_MASTER_CFG, reg);
	for (i = ALC_RESET_TIMEOUT; i > 0; i--) {
		DELAY(10);
		if ((CSR_READ_4(sc, ALC_MASTER_CFG) & MASTER_RESET) == 0)
			break;
	}
	if (i == 0)
		printf("%s: master reset timeout!\n", sc->sc_dev.dv_xname);

	for (i = ALC_RESET_TIMEOUT; i > 0; i--) {
		if ((reg = CSR_READ_4(sc, ALC_IDLE_STATUS)) == 0)
			break;
		DELAY(10);
	}

	if (i == 0)
		printf("%s: reset timeout(0x%08x)!\n", sc->sc_dev.dv_xname, 
		    reg);
}

int
alc_init(struct ifnet *ifp)
{
	struct alc_softc *sc = ifp->if_softc;
	struct mii_data *mii;
	uint8_t eaddr[ETHER_ADDR_LEN];
	bus_addr_t paddr;
	uint32_t reg, rxf_hi, rxf_lo;
	int error;

	/*
	 * Cancel any pending I/O.
	 */
	alc_stop(sc);
	/*
	 * Reset the chip to a known state.
	 */
	alc_reset(sc);

	/* Initialize Rx descriptors. */
	error = alc_init_rx_ring(sc);
	if (error != 0) {
		printf("%s: no memory for Rx buffers.\n", sc->sc_dev.dv_xname);
		alc_stop(sc);
		return (error);
	}
	alc_init_rr_ring(sc);
	alc_init_tx_ring(sc);
	alc_init_cmb(sc);
	alc_init_smb(sc);

	/* Enable all clocks. */
	CSR_WRITE_4(sc, ALC_CLK_GATING_CFG, 0);

	/* Reprogram the station address. */
	bcopy(LLADDR(ifp->if_sadl), eaddr, ETHER_ADDR_LEN);
	CSR_WRITE_4(sc, ALC_PAR0,
	    eaddr[2] << 24 | eaddr[3] << 16 | eaddr[4] << 8 | eaddr[5]);
	CSR_WRITE_4(sc, ALC_PAR1, eaddr[0] << 8 | eaddr[1]);
	/*
	 * Clear WOL status and disable all WOL feature as WOL
	 * would interfere Rx operation under normal environments.
	 */
	CSR_READ_4(sc, ALC_WOL_CFG);
	CSR_WRITE_4(sc, ALC_WOL_CFG, 0);
	/* Set Tx descriptor base addresses. */
	paddr = sc->alc_rdata.alc_tx_ring_paddr;
	CSR_WRITE_4(sc, ALC_TX_BASE_ADDR_HI, ALC_ADDR_HI(paddr));
	CSR_WRITE_4(sc, ALC_TDL_HEAD_ADDR_LO, ALC_ADDR_LO(paddr));
	/* We don't use high priority ring. */
	CSR_WRITE_4(sc, ALC_TDH_HEAD_ADDR_LO, 0);
	/* Set Tx descriptor counter. */
	CSR_WRITE_4(sc, ALC_TD_RING_CNT,
	    (ALC_TX_RING_CNT << TD_RING_CNT_SHIFT) & TD_RING_CNT_MASK);
	/* Set Rx descriptor base addresses. */
	paddr = sc->alc_rdata.alc_rx_ring_paddr;
	CSR_WRITE_4(sc, ALC_RX_BASE_ADDR_HI, ALC_ADDR_HI(paddr));
	CSR_WRITE_4(sc, ALC_RD0_HEAD_ADDR_LO, ALC_ADDR_LO(paddr));
	/* We use one Rx ring. */
	CSR_WRITE_4(sc, ALC_RD1_HEAD_ADDR_LO, 0);
	CSR_WRITE_4(sc, ALC_RD2_HEAD_ADDR_LO, 0);
	CSR_WRITE_4(sc, ALC_RD3_HEAD_ADDR_LO, 0);
	/* Set Rx descriptor counter. */
	CSR_WRITE_4(sc, ALC_RD_RING_CNT,
	    (ALC_RX_RING_CNT << RD_RING_CNT_SHIFT) & RD_RING_CNT_MASK);

	/*
	 * Let hardware split jumbo frames into alc_max_buf_sized chunks.
	 * if it do not fit the buffer size. Rx return descriptor holds
	 * a counter that indicates how many fragments were made by the
	 * hardware. The buffer size should be multiple of 8 bytes.
	 * Since hardware has limit on the size of buffer size, always
	 * use the maximum value.
	 * For strict-alignment architectures make sure to reduce buffer
	 * size by 8 bytes to make room for alignment fixup.
	 */
	sc->alc_buf_size = RX_BUF_SIZE_MAX;
	CSR_WRITE_4(sc, ALC_RX_BUF_SIZE, sc->alc_buf_size);

	paddr = sc->alc_rdata.alc_rr_ring_paddr;
	/* Set Rx return descriptor base addresses. */
	CSR_WRITE_4(sc, ALC_RRD0_HEAD_ADDR_LO, ALC_ADDR_LO(paddr));
	/* We use one Rx return ring. */
	CSR_WRITE_4(sc, ALC_RRD1_HEAD_ADDR_LO, 0);
	CSR_WRITE_4(sc, ALC_RRD2_HEAD_ADDR_LO, 0);
	CSR_WRITE_4(sc, ALC_RRD3_HEAD_ADDR_LO, 0);
	/* Set Rx return descriptor counter. */
	CSR_WRITE_4(sc, ALC_RRD_RING_CNT,
	    (ALC_RR_RING_CNT << RRD_RING_CNT_SHIFT) & RRD_RING_CNT_MASK);
	paddr = sc->alc_rdata.alc_cmb_paddr;
	CSR_WRITE_4(sc, ALC_CMB_BASE_ADDR_LO, ALC_ADDR_LO(paddr));
	paddr = sc->alc_rdata.alc_smb_paddr;
	CSR_WRITE_4(sc, ALC_SMB_BASE_ADDR_HI, ALC_ADDR_HI(paddr));
	CSR_WRITE_4(sc, ALC_SMB_BASE_ADDR_LO, ALC_ADDR_LO(paddr));

	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1) {
		/* Reconfigure SRAM - Vendor magic. */
		CSR_WRITE_4(sc, ALC_SRAM_RX_FIFO_LEN, 0x000002A0);
		CSR_WRITE_4(sc, ALC_SRAM_TX_FIFO_LEN, 0x00000100);
		CSR_WRITE_4(sc, ALC_SRAM_RX_FIFO_ADDR, 0x029F0000);
		CSR_WRITE_4(sc, ALC_SRAM_RD0_ADDR, 0x02BF02A0);
		CSR_WRITE_4(sc, ALC_SRAM_TX_FIFO_ADDR, 0x03BF02C0);
		CSR_WRITE_4(sc, ALC_SRAM_TD_ADDR, 0x03DF03C0);
		CSR_WRITE_4(sc, ALC_TXF_WATER_MARK, 0x00000000);
		CSR_WRITE_4(sc, ALC_RD_DMA_CFG, 0x00000000);
	}

	/* Tell hardware that we're ready to load DMA blocks. */
	CSR_WRITE_4(sc, ALC_DMA_BLOCK, DMA_BLOCK_LOAD);

	/* Configure interrupt moderation timer. */
	sc->alc_int_rx_mod = ALC_IM_RX_TIMER_DEFAULT;
	sc->alc_int_tx_mod = ALC_IM_TX_TIMER_DEFAULT;
	reg = ALC_USECS(sc->alc_int_rx_mod) << IM_TIMER_RX_SHIFT;
	reg |= ALC_USECS(sc->alc_int_tx_mod) << IM_TIMER_TX_SHIFT;
	CSR_WRITE_4(sc, ALC_IM_TIMER, reg);
	/*
	 * We don't want to automatic interrupt clear as task queue
	 * for the interrupt should know interrupt status.
	 */
	reg = MASTER_SA_TIMER_ENB;
	if (ALC_USECS(sc->alc_int_rx_mod) != 0)
		reg |= MASTER_IM_RX_TIMER_ENB;
	if (ALC_USECS(sc->alc_int_tx_mod) != 0)
		reg |= MASTER_IM_TX_TIMER_ENB;
	CSR_WRITE_4(sc, ALC_MASTER_CFG, reg);
	/*
	 * Disable interrupt re-trigger timer. We don't want automatic
	 * re-triggering of un-ACKed interrupts.
	 */
	CSR_WRITE_4(sc, ALC_INTR_RETRIG_TIMER, ALC_USECS(0));
	/* Configure CMB. */
	if ((sc->alc_flags & ALC_FLAG_CMB_BUG) == 0) {
		CSR_WRITE_4(sc, ALC_CMB_TD_THRESH, 4);
		CSR_WRITE_4(sc, ALC_CMB_TX_TIMER, ALC_USECS(5000));
	} else
		CSR_WRITE_4(sc, ALC_CMB_TX_TIMER, ALC_USECS(0));
	/*
	 * Hardware can be configured to issue SMB interrupt based
	 * on programmed interval. Since there is a callout that is
	 * invoked for every hz in driver we use that instead of
	 * relying on periodic SMB interrupt.
	 */
	CSR_WRITE_4(sc, ALC_SMB_STAT_TIMER, ALC_USECS(0));
	/* Clear MAC statistics. */
	alc_stats_clear(sc);

	/*
	 * Always use maximum frame size that controller can support.
	 * Otherwise received frames that has larger frame length
	 * than alc(4) MTU would be silently dropped in hardware. This
	 * would make path-MTU discovery hard as sender wouldn't get
	 * any responses from receiver. alc(4) supports
	 * multi-fragmented frames on Rx path so it has no issue on
	 * assembling fragmented frames. Using maximum frame size also
	 * removes the need to reinitialize hardware when interface
	 * MTU configuration was changed.
	 *
	 * Be conservative in what you do, be liberal in what you
	 * accept from others - RFC 793.
	 */
	CSR_WRITE_4(sc, ALC_FRAME_SIZE, sc->alc_max_framelen);

	/* Disable header split(?) */
	CSR_WRITE_4(sc, ALC_HDS_CFG, 0);

	/* Configure IPG/IFG parameters. */
	CSR_WRITE_4(sc, ALC_IPG_IFG_CFG,
	    ((IPG_IFG_IPGT_DEFAULT << IPG_IFG_IPGT_SHIFT) & IPG_IFG_IPGT_MASK) |
	    ((IPG_IFG_MIFG_DEFAULT << IPG_IFG_MIFG_SHIFT) & IPG_IFG_MIFG_MASK) |
	    ((IPG_IFG_IPG1_DEFAULT << IPG_IFG_IPG1_SHIFT) & IPG_IFG_IPG1_MASK) |
	    ((IPG_IFG_IPG2_DEFAULT << IPG_IFG_IPG2_SHIFT) & IPG_IFG_IPG2_MASK));
	/* Set parameters for half-duplex media. */
	CSR_WRITE_4(sc, ALC_HDPX_CFG,
	    ((HDPX_CFG_LCOL_DEFAULT << HDPX_CFG_LCOL_SHIFT) &
	    HDPX_CFG_LCOL_MASK) |
	    ((HDPX_CFG_RETRY_DEFAULT << HDPX_CFG_RETRY_SHIFT) &
	    HDPX_CFG_RETRY_MASK) | HDPX_CFG_EXC_DEF_EN |
	    ((HDPX_CFG_ABEBT_DEFAULT << HDPX_CFG_ABEBT_SHIFT) &
	    HDPX_CFG_ABEBT_MASK) |
	    ((HDPX_CFG_JAMIPG_DEFAULT << HDPX_CFG_JAMIPG_SHIFT) &
	    HDPX_CFG_JAMIPG_MASK));
	/*
	 * Set TSO/checksum offload threshold. For frames that is
	 * larger than this threshold, hardware wouldn't do
	 * TSO/checksum offloading.
	 */
	CSR_WRITE_4(sc, ALC_TSO_OFFLOAD_THRESH,
	    (sc->alc_max_framelen >> TSO_OFFLOAD_THRESH_UNIT_SHIFT) &
	    TSO_OFFLOAD_THRESH_MASK);
	/* Configure TxQ. */
	reg = (alc_dma_burst[sc->alc_dma_rd_burst] <<
	    TXQ_CFG_TX_FIFO_BURST_SHIFT) & TXQ_CFG_TX_FIFO_BURST_MASK;
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_2)
		reg >>= 1;
	reg |= (TXQ_CFG_TD_BURST_DEFAULT << TXQ_CFG_TD_BURST_SHIFT) &
	    TXQ_CFG_TD_BURST_MASK;
	CSR_WRITE_4(sc, ALC_TXQ_CFG, reg | TXQ_CFG_ENHANCED_MODE);

	/* Configure Rx free descriptor pre-fetching. */
	CSR_WRITE_4(sc, ALC_RX_RD_FREE_THRESH,
	    ((RX_RD_FREE_THRESH_HI_DEFAULT << RX_RD_FREE_THRESH_HI_SHIFT) &
	    RX_RD_FREE_THRESH_HI_MASK) |
	    ((RX_RD_FREE_THRESH_LO_DEFAULT << RX_RD_FREE_THRESH_LO_SHIFT) &
	    RX_RD_FREE_THRESH_LO_MASK));

	/*
	 * Configure flow control parameters.
	 * XON  : 80% of Rx FIFO
	 * XOFF : 30% of Rx FIFO
	 */
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1C ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C) {
		reg = CSR_READ_4(sc, ALC_SRAM_RX_FIFO_LEN);
		rxf_hi = (reg * 8) / 10;
		rxf_lo = (reg * 3) / 10;
		CSR_WRITE_4(sc, ALC_RX_FIFO_PAUSE_THRESH,
		    ((rxf_lo << RX_FIFO_PAUSE_THRESH_LO_SHIFT) &
		    RX_FIFO_PAUSE_THRESH_LO_MASK) |
		    ((rxf_hi << RX_FIFO_PAUSE_THRESH_HI_SHIFT) &
		    RX_FIFO_PAUSE_THRESH_HI_MASK));
	}
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_1)
		CSR_WRITE_4(sc, ALC_SERDES_LOCK,
		    CSR_READ_4(sc, ALC_SERDES_LOCK) | SERDES_MAC_CLK_SLOWDOWN |
		    SERDES_PHY_CLK_SLOWDOWN);

	/* Disable RSS until I understand L1C/L2C's RSS logic. */
	CSR_WRITE_4(sc, ALC_RSS_IDT_TABLE0, 0);
	CSR_WRITE_4(sc, ALC_RSS_CPU, 0);

	/* Configure RxQ. */
	reg = (RXQ_CFG_RD_BURST_DEFAULT << RXQ_CFG_RD_BURST_SHIFT) &
	    RXQ_CFG_RD_BURST_MASK;
	reg |= RXQ_CFG_RSS_MODE_DIS;
	if ((sc->alc_flags & ALC_FLAG_ASPM_MON) != 0)
		reg |= RXQ_CFG_ASPM_THROUGHPUT_LIMIT_1M;
	CSR_WRITE_4(sc, ALC_RXQ_CFG, reg);

	/* Configure DMA parameters. */
	reg = DMA_CFG_OUT_ORDER | DMA_CFG_RD_REQ_PRI;
	reg |= sc->alc_rcb;
	if ((sc->alc_flags & ALC_FLAG_CMB_BUG) == 0)
		reg |= DMA_CFG_CMB_ENB;
	if ((sc->alc_flags & ALC_FLAG_SMB_BUG) == 0)
		reg |= DMA_CFG_SMB_ENB;
	else
		reg |= DMA_CFG_SMB_DIS;
	reg |= (sc->alc_dma_rd_burst & DMA_CFG_RD_BURST_MASK) <<
	    DMA_CFG_RD_BURST_SHIFT;
	reg |= (sc->alc_dma_wr_burst & DMA_CFG_WR_BURST_MASK) <<
	    DMA_CFG_WR_BURST_SHIFT;
	reg |= (DMA_CFG_RD_DELAY_CNT_DEFAULT << DMA_CFG_RD_DELAY_CNT_SHIFT) &
	    DMA_CFG_RD_DELAY_CNT_MASK;
	reg |= (DMA_CFG_WR_DELAY_CNT_DEFAULT << DMA_CFG_WR_DELAY_CNT_SHIFT) &
	    DMA_CFG_WR_DELAY_CNT_MASK;
	CSR_WRITE_4(sc, ALC_DMA_CFG, reg);

	/*
	 * Configure Tx/Rx MACs.
	 *  - Auto-padding for short frames.
	 *  - Enable CRC generation.
	 *  Actual reconfiguration of MAC for resolved speed/duplex
	 *  is followed after detection of link establishment.
	 *  AR813x/AR815x always does checksum computation regardless
	 *  of MAC_CFG_RXCSUM_ENB bit. Also the controller is known to
	 *  have bug in protocol field in Rx return structure so
	 *  these controllers can't handle fragmented frames. Disable
	 *  Rx checksum offloading until there is a newer controller
	 *  that has sane implementation.
	 */
	reg = MAC_CFG_TX_CRC_ENB | MAC_CFG_TX_AUTO_PAD | MAC_CFG_FULL_DUPLEX |
	    ((MAC_CFG_PREAMBLE_DEFAULT << MAC_CFG_PREAMBLE_SHIFT) &
	    MAC_CFG_PREAMBLE_MASK);
	if (sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L1D_1 ||
	    sc->sc_product == PCI_PRODUCT_ATTANSIC_L2C_2)
		reg |= MAC_CFG_HASH_ALG_CRC32 | MAC_CFG_SPEED_MODE_SW;
	if ((sc->alc_flags & ALC_FLAG_FASTETHER) != 0)
		reg |= MAC_CFG_SPEED_10_100;
	else
		reg |= MAC_CFG_SPEED_1000;
	CSR_WRITE_4(sc, ALC_MAC_CFG, reg);

	/* Set up the receive filter. */
	alc_iff(sc);

	alc_rxvlan(sc);

	/* Acknowledge all pending interrupts and clear it. */
	CSR_WRITE_4(sc, ALC_INTR_MASK, ALC_INTRS);
	CSR_WRITE_4(sc, ALC_INTR_STATUS, 0xFFFFFFFF);
	CSR_WRITE_4(sc, ALC_INTR_STATUS, 0);

	sc->alc_flags &= ~ALC_FLAG_LINK;
	/* Switch to the current media. */
	mii = &sc->sc_miibus;
	mii_mediachg(mii);

	timeout_add_sec(&sc->alc_tick_ch, 1);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	return (0);
}

void
alc_stop(struct alc_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct alc_txdesc *txd;
	struct alc_rxdesc *rxd;
	uint32_t reg;
	int i;

	/*
	 * Mark the interface down and cancel the watchdog timer.
	 */
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	ifp->if_timer = 0;

	timeout_del(&sc->alc_tick_ch);
	sc->alc_flags &= ~ALC_FLAG_LINK;

	alc_stats_update(sc);

	/* Disable interrupts. */
	CSR_WRITE_4(sc, ALC_INTR_MASK, 0);
	CSR_WRITE_4(sc, ALC_INTR_STATUS, 0xFFFFFFFF);
	alc_stop_queue(sc);

	/* Disable DMA. */
	reg = CSR_READ_4(sc, ALC_DMA_CFG);
	reg &= ~(DMA_CFG_CMB_ENB | DMA_CFG_SMB_ENB);
	reg |= DMA_CFG_SMB_DIS;
	CSR_WRITE_4(sc, ALC_DMA_CFG, reg);
	DELAY(1000);

	/* Stop Rx/Tx MACs. */
	alc_stop_mac(sc);

	/* Disable interrupts which might be touched in taskq handler. */
	CSR_WRITE_4(sc, ALC_INTR_STATUS, 0xFFFFFFFF);

	/* Reclaim Rx buffers that have been processed. */
	m_freem(sc->alc_cdata.alc_rxhead);
	ALC_RXCHAIN_RESET(sc);
	/*
	 * Free Tx/Rx mbufs still in the queues.
	 */
	for (i = 0; i < ALC_RX_RING_CNT; i++) {
		rxd = &sc->alc_cdata.alc_rxdesc[i];
		if (rxd->rx_m != NULL) {
			bus_dmamap_sync(sc->sc_dmat, rxd->rx_dmamap, 0,
			    rxd->rx_dmamap->dm_mapsize, BUS_DMASYNC_POSTREAD);
			bus_dmamap_unload(sc->sc_dmat, rxd->rx_dmamap);
			m_freem(rxd->rx_m);
			rxd->rx_m = NULL;
		}
	}
	for (i = 0; i < ALC_TX_RING_CNT; i++) {
		txd = &sc->alc_cdata.alc_txdesc[i];
		if (txd->tx_m != NULL) {
			bus_dmamap_sync(sc->sc_dmat, txd->tx_dmamap, 0,
			    txd->tx_dmamap->dm_mapsize, BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, txd->tx_dmamap);
			m_freem(txd->tx_m);
			txd->tx_m = NULL;
		}
	}
}

void
alc_stop_mac(struct alc_softc *sc)
{
	uint32_t reg;
	int i;

	/* Disable Rx/Tx MAC. */
	reg = CSR_READ_4(sc, ALC_MAC_CFG);
	if ((reg & (MAC_CFG_TX_ENB | MAC_CFG_RX_ENB)) != 0) {
		reg &= ~(MAC_CFG_TX_ENB | MAC_CFG_RX_ENB);
		CSR_WRITE_4(sc, ALC_MAC_CFG, reg);
	}
	for (i = ALC_TIMEOUT; i > 0; i--) {
		reg = CSR_READ_4(sc, ALC_IDLE_STATUS);
		if (reg == 0)
			break;
		DELAY(10);
	}
	if (i == 0)
		printf("%s: could not disable Rx/Tx MAC(0x%08x)!\n",
		    sc->sc_dev.dv_xname, reg);
}

void
alc_start_queue(struct alc_softc *sc)
{
	uint32_t qcfg[] = {
		0,
		RXQ_CFG_QUEUE0_ENB,
		RXQ_CFG_QUEUE0_ENB | RXQ_CFG_QUEUE1_ENB,
		RXQ_CFG_QUEUE0_ENB | RXQ_CFG_QUEUE1_ENB | RXQ_CFG_QUEUE2_ENB,
		RXQ_CFG_ENB
	};
	uint32_t cfg;

	/* Enable RxQ. */
	cfg = CSR_READ_4(sc, ALC_RXQ_CFG);
	cfg &= ~RXQ_CFG_ENB;
	cfg |= qcfg[1];
	CSR_WRITE_4(sc, ALC_RXQ_CFG, cfg);
	/* Enable TxQ. */
	cfg = CSR_READ_4(sc, ALC_TXQ_CFG);
	cfg |= TXQ_CFG_ENB;
	CSR_WRITE_4(sc, ALC_TXQ_CFG, cfg);
}

void
alc_stop_queue(struct alc_softc *sc)
{
	uint32_t reg;
	int i;

	/* Disable RxQ. */
	reg = CSR_READ_4(sc, ALC_RXQ_CFG);
	if ((reg & RXQ_CFG_ENB) != 0) {
		reg &= ~RXQ_CFG_ENB;
		CSR_WRITE_4(sc, ALC_RXQ_CFG, reg);
	}
	/* Disable TxQ. */
	reg = CSR_READ_4(sc, ALC_TXQ_CFG);
	if ((reg & TXQ_CFG_ENB) != 0) {
		reg &= ~TXQ_CFG_ENB;
		CSR_WRITE_4(sc, ALC_TXQ_CFG, reg);
	}
	for (i = ALC_TIMEOUT; i > 0; i--) {
		reg = CSR_READ_4(sc, ALC_IDLE_STATUS);
		if ((reg & (IDLE_STATUS_RXQ | IDLE_STATUS_TXQ)) == 0)
			break;
		DELAY(10);
	}
	if (i == 0)
		printf("%s: could not disable RxQ/TxQ (0x%08x)!\n",
		    sc->sc_dev.dv_xname, reg);
}

void
alc_init_tx_ring(struct alc_softc *sc)
{
	struct alc_ring_data *rd;
	struct alc_txdesc *txd;
	int i;

	sc->alc_cdata.alc_tx_prod = 0;
	sc->alc_cdata.alc_tx_cons = 0;
	sc->alc_cdata.alc_tx_cnt = 0;

	rd = &sc->alc_rdata;
	bzero(rd->alc_tx_ring, ALC_TX_RING_SZ);
	for (i = 0; i < ALC_TX_RING_CNT; i++) {
		txd = &sc->alc_cdata.alc_txdesc[i];
		txd->tx_m = NULL;
	}

	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_tx_ring_map, 0,
	    sc->alc_cdata.alc_tx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
}

int
alc_init_rx_ring(struct alc_softc *sc)
{
	struct alc_ring_data *rd;
	struct alc_rxdesc *rxd;
	int i;

	sc->alc_cdata.alc_rx_cons = ALC_RX_RING_CNT - 1;
	rd = &sc->alc_rdata;
	bzero(rd->alc_rx_ring, ALC_RX_RING_SZ);
	for (i = 0; i < ALC_RX_RING_CNT; i++) {
		rxd = &sc->alc_cdata.alc_rxdesc[i];
		rxd->rx_m = NULL;
		rxd->rx_desc = &rd->alc_rx_ring[i];
		if (alc_newbuf(sc, rxd) != 0)
			return (ENOBUFS);
	}

	/*
	 * Since controller does not update Rx descriptors, driver
	 * does have to read Rx descriptors back so BUS_DMASYNC_PREWRITE
	 * is enough to ensure coherence.
	 */
	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_rx_ring_map, 0,
	    sc->alc_cdata.alc_rx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	/* Let controller know availability of new Rx buffers. */
	CSR_WRITE_4(sc, ALC_MBOX_RD0_PROD_IDX, sc->alc_cdata.alc_rx_cons);

	return (0);
}

void
alc_init_rr_ring(struct alc_softc *sc)
{
	struct alc_ring_data *rd;

	sc->alc_cdata.alc_rr_cons = 0;
	ALC_RXCHAIN_RESET(sc);

	rd = &sc->alc_rdata;
	bzero(rd->alc_rr_ring, ALC_RR_RING_SZ);
	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_rr_ring_map, 0,
	    sc->alc_cdata.alc_rr_ring_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
}

void
alc_init_cmb(struct alc_softc *sc)
{
	struct alc_ring_data *rd;

	rd = &sc->alc_rdata;
	bzero(rd->alc_cmb, ALC_CMB_SZ);
	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_cmb_map, 0,
	    sc->alc_cdata.alc_cmb_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
}

void
alc_init_smb(struct alc_softc *sc)
{
	struct alc_ring_data *rd;

	rd = &sc->alc_rdata;
	bzero(rd->alc_smb, ALC_SMB_SZ);
	bus_dmamap_sync(sc->sc_dmat, sc->alc_cdata.alc_smb_map, 0,
	    sc->alc_cdata.alc_smb_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
}

void
alc_rxvlan(struct alc_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	uint32_t reg;

	reg = CSR_READ_4(sc, ALC_MAC_CFG);
	if (ifp->if_capabilities & IFCAP_VLAN_HWTAGGING)
		reg |= MAC_CFG_VLAN_TAG_STRIP;
	else
		reg &= ~MAC_CFG_VLAN_TAG_STRIP;
	CSR_WRITE_4(sc, ALC_MAC_CFG, reg);
}

void
alc_iff(struct alc_softc *sc)
{
	struct arpcom *ac = &sc->sc_arpcom;
	struct ifnet *ifp = &ac->ac_if;
	struct ether_multi *enm;
	struct ether_multistep step;
	uint32_t crc;
	uint32_t mchash[2];
	uint32_t rxcfg;

	rxcfg = CSR_READ_4(sc, ALC_MAC_CFG);
	rxcfg &= ~(MAC_CFG_ALLMULTI | MAC_CFG_BCAST | MAC_CFG_PROMISC);
	ifp->if_flags &= ~IFF_ALLMULTI;

	/*
	 * Always accept broadcast frames.
	 */
	rxcfg |= MAC_CFG_BCAST;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0) {
		ifp->if_flags |= IFF_ALLMULTI;
		if (ifp->if_flags & IFF_PROMISC)
			rxcfg |= MAC_CFG_PROMISC;
		else
			rxcfg |= MAC_CFG_ALLMULTI;
		mchash[0] = mchash[1] = 0xFFFFFFFF;
	} else {
		/* Program new filter. */
		bzero(mchash, sizeof(mchash));

		ETHER_FIRST_MULTI(step, ac, enm);
		while (enm != NULL) {
			crc = ether_crc32_be(enm->enm_addrlo, ETHER_ADDR_LEN);

			mchash[crc >> 31] |= 1 << ((crc >> 26) & 0x1f);

			ETHER_NEXT_MULTI(step, enm);
		}
	}

	CSR_WRITE_4(sc, ALC_MAR0, mchash[0]);
	CSR_WRITE_4(sc, ALC_MAR1, mchash[1]);
	CSR_WRITE_4(sc, ALC_MAC_CFG, rxcfg);
}
@


1.41
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.40 2016/11/29 10:22:30 jsg Exp $	*/
a36 1
#include <sys/types.h>
@


1.40
log
@m_free() and m_freem() test for NULL.  Simplify callers which had their own
NULL tests.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.39 2016/04/13 10:34:32 mpi Exp $	*/
a1626 3

	/* Update counters in ifnet. */
	ifp->if_opackets += smb->tx_frames;
@


1.39
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.38 2016/03/15 16:45:52 naddy Exp $	*/
d1915 1
a1915 2
			if (sc->alc_cdata.alc_rxhead != NULL)
				m_freem(sc->alc_cdata.alc_rxhead);
d2379 1
a2379 2
	if (sc->alc_cdata.alc_rxhead != NULL)
		m_freem(sc->alc_cdata.alc_rxhead);
@


1.38
log
@Ethernet drivers no longer need to include if_vlan_var.h for the VLAN
definitions; ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.37 2015/11/25 03:09:59 dlg Exp $	*/
a839 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.37
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.36 2015/11/09 00:29:06 dlg Exp $	*/
a53 2

#include <net/if_vlan_var.h>
@


1.36
log
@rework the start routines to avoid IF_PREPEND.

IF_PREPEND assumes the underlying send queue is priq, while hfsc may be
enabled on it.

the previous code pattern to DEQUEUE, try and encap the mbuf on the
ring, and if that failed cos there was no space it would PREPEND
it.

now it checks for space on the ring before it attempts to DEQUEUE.
failure to encap means the mbuf is now unconditionally dropped.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.35 2015/10/25 13:04:28 mpi Exp $	*/
d1362 1
a1362 1
	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
d1372 1
a1372 1
			ifp->if_flags |= IFF_OACTIVE;
d1742 1
a1742 1
		ifp->if_flags &= ~IFF_OACTIVE;
d2338 1
a2338 1
	ifp->if_flags &= ~IFF_OACTIVE;
d2355 2
a2356 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
@


1.35
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.34 2015/09/11 13:02:28 stsp Exp $	*/
d86 1
a86 1
int	alc_encap(struct alc_softc *, struct mbuf **);
d1268 1
a1268 1
alc_encap(struct alc_softc *sc, struct mbuf **m_head)
a1271 1
	struct mbuf *m;
a1275 1
	m = *m_head;
d1284 1
a1284 1
	error = bus_dmamap_load_mbuf(sc->sc_dmat, map, *m_head, BUS_DMA_NOWAIT);
d1288 1
a1288 1
		if (m_defrag(*m_head, M_DONTWAIT)) {
d1292 1
a1292 1
		error = bus_dmamap_load_mbuf(sc->sc_dmat, map, *m_head,
a1297 6
	/* Check descriptor overrun. */
	if (sc->alc_cdata.alc_tx_cnt + map->dm_nsegs >= ALC_TX_RING_CNT - 3) {
		bus_dmamap_unload(sc->sc_dmat, map);
		return (ENOBUFS);
	}

a1300 1
	m = *m_head;
d1347 1
a1347 2
	m_freem(*m_head);
	*m_head = NULL;
d1355 1
a1355 1
	struct mbuf *m_head;
d1370 3
a1372 2
		IFQ_DEQUEUE(&ifp->if_snd, m_head);
		if (m_head == NULL)
d1374 1
d1376 2
a1377 12
		/*
		 * Pack the data into the transmit ring. If we
		 * don't have room, set the OACTIVE flag and wait
		 * for the NIC to drain the ring.
		 */
		if (alc_encap(sc, &m_head)) {
			if (m_head == NULL)
				ifp->if_oerrors++;
			else {
				IF_PREPEND(&ifp->if_snd, m_head);
				ifp->if_flags |= IFF_OACTIVE;
			}
d1379 4
d1392 1
a1392 1
			bpf_mtap_ether(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
@


1.34
log
@Make room for media types of the future. Extend the ifmedia word to 64 bits.
This changes numbers of the SIOCSIFMEDIA and SIOCGIFMEDIA ioctls and
grows struct ifmediareq.

Old ifconfig and dhclient binaries can still assign addresses, however
the 'media' subcommand stops working. Recompiling ifconfig and dhclient
with new headers before a reboot should not be necessary unless in very
special circumstances where non-default media settings must be used to
get link and console access is not available.

There may be some MD fallout but that will be cleared up later.

ok deraadt miod
with help and suggestions from several sharks attending l2k15
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.33 2015/06/24 09:40:54 mpi Exp $	*/
a1448 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a1458 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->sc_arpcom, ifa);
@


1.33
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.32 2015/03/20 16:48:13 mpi Exp $	*/
d82 1
a82 1
void	alc_aspm(struct alc_softc *, int);
d553 1
a553 1
alc_aspm(struct alc_softc *sc, int media)
@


1.32
log
@Convert to if_input(), thanks to krw@@ for testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.31 2015/03/14 03:38:48 jsg Exp $	*/
a1656 2

	ifp->if_ipackets += smb->rx_frames;
@


1.31
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.30 2014/12/22 02:28:51 tedu Exp $	*/
d1900 1
a1987 1
			m->m_pkthdr.rcvif = ifp;
a1999 5
#if NBPFILTER > 0
			if (ifp->if_bpf)
				bpf_mtap_ether(ifp->if_bpf, m,
				    BPF_DIRECTION_IN);
#endif
d2001 1
a2001 4
			{
			/* Pass it on. */
			ether_input_mbuf(ifp, m);
			}
d2004 2
@


1.30
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.29 2014/11/27 14:52:04 brad Exp $	*/
a54 1
#include <net/if_types.h>
@


1.29
log
@Fix a long standing bug in MAC statistics register access.  One
additional register was erroneously added in the MAC register set
such that 7 TX statistics counters were wrong.

From FreeBSD

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.28 2014/11/18 02:37:30 tedu Exp $	*/
a51 1
#ifdef INET
a53 1
#endif
a1460 1
#ifdef INET
a1462 1
#endif
@


1.28
log
@move arc4random prototype to systm.h. more appropriate for most code
to include that than rdnvar.h. ok deraadt dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.27 2014/07/22 13:12:11 mpi Exp $	*/
a1645 1
	stat->tx_abort += smb->tx_abort;
d1658 1
a1658 1
	    smb->tx_abort * HDPX_CFG_RETRY_DEFAULT;
d1660 2
a1661 9
	/*
	 * XXX
	 * tx_pkts_truncated counter looks suspicious. It constantly
	 * increments with no sign of Tx errors. This may indicate
	 * the counter name is not correct one so I've removed the
	 * counter in output errors.
	 */
	ifp->if_oerrors += smb->tx_abort + smb->tx_late_colls +
	    smb->tx_underrun;
@


1.27
log
@Fewer <netinet/in_systm.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.26 2013/12/28 03:34:53 deraadt Exp $	*/
a62 2

#include <dev/rndvar.h>
@


1.26
log
@The few network drivers that called their children's (ie. mii PHY
drivers) activate functions at DVACT_RESUME time do not need to do
so, since their PHYs are repaired by IFF_UP.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.25 2013/12/06 21:03:03 deraadt Exp $	*/
a53 2
#include <netinet/in_systm.h>
#include <netinet/ip.h>
@


1.25
log
@Add a DVACT_WAKEUP op to the *_activate() API.  This is called after the
kernel resumes normal (non-cold, able to run processes, etc) operation.
Previously we were relying on specific DVACT_RESUME op's in drivers
creating callback/threads themselves, but that has become too common,
indicating the need for a built-in mechanism.
ok dlg kettenis, tested by a sufficient amount of people
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.24 2013/11/21 16:16:08 mpi Exp $	*/
a941 1
		rv = config_activate_children(self, act);
@


1.24
log
@Remove unneeded include.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.23 2013/08/07 01:06:33 bluhm Exp $	*/
a935 3
	case DVACT_QUIESCE:
		rv = config_activate_children(self, act);
		break;
d945 3
@


1.23
log
@Most network drivers include netinet/in_var.h, but apparently they
don't have to.  Just remove these include lines.
Compiled on amd64 i386 sparc64; OK henning@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.22 2012/11/29 21:10:32 brad Exp $	*/
a49 1
#include <net/if_llc.h>
@


1.22
log
@Remove setting an initial assumed baudrate upon driver attach which is not
necessarily correct, there might not even be a link when attaching.

ok mikeb@@ reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.21 2011/10/19 05:23:44 kevlo Exp $	*/
a55 1
#include <netinet/in_var.h>
@


1.21
log
@Add some missing bus_dmamap_sync()'s and sync the others with
the FreeBSD code.

age(4) tested by Thomas Pfaff; alc(4) tested by Gabriel Linder;
ale(4) tested by Johan Torin.

From Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.20 2011/09/21 07:09:19 kevlo Exp $	*/
a849 1
	ifp->if_baudrate = IF_Gbps(1);
@


1.20
log
@Some minor clean up to the _start funtions to make the code read a little
better. No functional change.

From Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.19 2011/09/13 08:12:51 kevlo Exp $	*/
d1553 1
a1553 1
		    BUS_DMASYNC_POSTREAD);
d1559 1
a1559 1
		    BUS_DMASYNC_PREWRITE);
d1588 1
a1588 1
		    BUS_DMASYNC_POSTREAD);
d1690 2
a1691 1
		sc->alc_cdata.alc_smb_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
d1787 2
d1797 1
a1797 1
	        sc->alc_cdata.alc_cmb_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
d1842 2
d1858 2
a1859 1
	    sc->alc_cdata.alc_rr_ring_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
d1861 2
a1862 1
	    sc->alc_cdata.alc_rx_ring_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
d1892 1
a1892 1
		    BUS_DMASYNC_PREWRITE);
d2437 2
d2447 2
d2595 2
a2596 1
	    sc->alc_cdata.alc_rr_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
d2607 2
a2608 1
	    sc->alc_cdata.alc_cmb_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
d2619 2
a2620 1
	    sc->alc_cdata.alc_smb_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
@


1.19
log
@Fix up alc_encap() / alc_start():

- Remove unnecessary nsegs variable from alc_encap() and
  use map->dm_nsegs. Also remove unnecessary FreeBSD check
  for 0 DMA segments check.
- Remove printfs in alc_encap() failure paths that shouldn't
  be there.
- Add missing IF_PREPEND() from failure path coming off of
  alc_encap() within alc_start().
- Fix error handling within alc_encap(). Previously alc_encap()
  was attempting to unload a DMA map upon failure from
  bus_dmamap_load_mbuf() even though one wasn't loaded at that
  point and then always forcing mbufs through the EFBIG path.

Tested by Gabriel Linder.

From Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.18 2011/09/03 14:33:51 kevlo Exp $	*/
d1401 1
a1401 1
			if (m_head == NULL) {
d1403 3
a1405 1
				break;
a1406 2
			IF_PREPEND(&ifp->if_snd, m_head);
			ifp->if_flags |= IFF_OACTIVE;
@


1.18
log
@Disable PHY hibernation. The PHY will go into sleep state when it
detects no established link and it will re-establish link when the
cable is plugged in.  Previously it failed to re-establish link
when the cable was plugged in such that it required turning the
interface down and then up to make it work. This was caused by
incorrectly programmed hibernation parameters. Further details
regarding PHY setup are necessary to be able to re-enable this
feature.

Tested by Matteo Filippetto and Gabriel Linder

From FreeBSD via Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.17 2011/08/26 07:49:04 kevlo Exp $	*/
d1286 1
a1286 1
	int error, idx, nsegs, prod;
d1298 2
a1299 1

a1300 4
		bus_dmamap_unload(sc->sc_dmat, map);
		error = EFBIG;
	}
	if (error == EFBIG) {
d1302 2
a1303 5
			printf("%s: can't defrag TX mbuf\n",
			    sc->sc_dev.dv_xname);
			m_freem(*m_head);
			*m_head = NULL;
			return (ENOBUFS);
d1307 2
a1308 18
		if (error != 0) {
			printf("%s: could not load defragged TX mbuf\n",
			    sc->sc_dev.dv_xname);
			m_freem(*m_head);
			*m_head = NULL;
			return (error);
		}
	} else if (error) {
		printf("%s: could not load TX mbuf\n", sc->sc_dev.dv_xname);
		return (error);
	}

	nsegs = map->dm_nsegs;

	if (nsegs == 0) {
		m_freem(*m_head);
		*m_head = NULL;
		return (EIO);
d1312 1
a1312 1
	if (sc->alc_cdata.alc_tx_cnt + nsegs >= ALC_TX_RING_CNT - 3) {
d1316 1
d1337 3
a1339 2
	} 
	for (; idx < nsegs; idx++) {
d1348 1
d1365 5
d1401 2
a1402 1
			if (m_head == NULL)
d1404 2
@


1.17
log
@Move the comment outside of the VLAN section of code and above that
chunk to where it should be.

From Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.16 2011/08/21 15:25:20 kevlo Exp $	*/
d423 1
a423 2
	CSR_WRITE_2(sc, ALC_GPHY_CFG,
	    GPHY_CFG_HIB_EN | GPHY_CFG_HIB_PULSE | GPHY_CFG_SEL_ANA_RESET);
d427 1
a427 2
	CSR_WRITE_2(sc, ALC_GPHY_CFG,
	    GPHY_CFG_EXT_RESET | GPHY_CFG_HIB_EN | GPHY_CFG_HIB_PULSE |
d512 17
d553 1
a553 2
		CSR_WRITE_2(sc, ALC_GPHY_CFG,
		    GPHY_CFG_EXT_RESET | GPHY_CFG_HIB_EN | GPHY_CFG_HIB_PULSE |
@


1.16
log
@Help with the watchdog timeouts seen when unplugging the cable from
the alc(4) NIC while running or the NIC not working if the cable is
not plugged in upon boot up.

From Brad; tested by matteo filippetto, Gabriel Linder and edd@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.15 2011/06/17 07:16:42 kevlo Exp $	*/
a1998 1
#if NVLAN > 0
d2003 1
@


1.15
log
@Enable MSI support; tested by krw@@, Mark Peoples, and
Abel Abraham Camarillo Ojeda.

From Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.14 2011/05/27 07:45:44 kevlo Exp $	*/
d217 1
a217 1
	struct mii_data *mii;
a222 2
	mii = &sc->sc_miibus;

d261 3
d1385 4
d1449 1
a1449 3

	if (!IFQ_IS_EMPTY(&ifp->if_snd))
		alc_start(ifp);
d1732 1
a1732 1
		if (status & INTR_TX_PKT) {
d1734 2
a1735 3
		    if (!IFQ_IS_EMPTY(&ifp->if_snd))
			alc_start(ifp);
		}
d1737 1
@


1.14
log
@Whitespace nits; from Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.13 2011/05/25 02:22:20 kevlo Exp $	*/
d656 1
a656 1
	if (pci_intr_map(pa, &ih) != 0) {
@


1.13
log
@- For AR8132 fast ethernet controller, do not report 1000baseT
  capability to mii(4).
- Limit DMA burst size to be less than or equal to 1024 bytes.
  Controller does not seem to support more than 1024 bytes DMA burst.
- Do not touch CMB TX threshold register when CMB is not used.

From FreeBSD via Brad

- Properly initialize sc_product and alc_rev early enough in
  alc_attach() with the PCI product id and PCI revision.
- Further sync if_alcvar.h changes from FreeBSD for L2C/L1D
  addition commit which in OpenBSD has been merged into if_alcreg.h

From Brad; tested by Gabriel Linder and edd@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.12 2011/05/18 14:20:27 sthen Exp $	*/
d1446 1
a1446 1
		 alc_start(ifp);
d2163 2
a2164 2
		/* Reconfigure SRAM - Vendor magic. */ 
		CSR_WRITE_4(sc, ALC_SRAM_RX_FIFO_LEN, 0x000002A0); 
@


1.12
log
@Support additional L2C variants and L1D (AR813x/AR815x chips).
Tested on L1C by Abel Abraham Camarillo Ojeda, thank you.

From FreeBSD via kevlo@@, ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.11 2011/04/05 18:01:21 henning Exp $	*/
d159 10
d683 4
d707 4
a767 1
	sc->sc_product = PCI_PRODUCT(pa->pa_id);
d809 1
a809 1
	sc->alc_rev = PCI_REVISION(pa->pa_class);
d2199 2
a2200 2
	CSR_WRITE_4(sc, ALC_CMB_TD_THRESH, 4);
	if ((sc->alc_flags & ALC_FLAG_CMB_BUG) == 0)
d2202 1
a2202 1
	else
@


1.11
log
@mechanic rename M_{TCP|UDP}V4_CSUM_OUT -> M_{TCP|UDP}_CSUM_OUT
ok claudio krw
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.10 2011/02/18 17:20:15 mikeb Exp $	*/
d91 1
a91 1
void	alc_aspm(struct alc_softc *);
d112 1
a112 1
int	alc_rxintr(struct alc_softc *);
d128 5
a132 1
	{ PCI_VENDOR_ATTANSIC, PCI_PRODUCT_ATTANSIC_L2C }
d243 1
a244 1
	alc_aspm(sc);
d287 2
a288 1
	int i;
d290 1
d292 2
a293 1
	if ((CSR_READ_4(sc, ALC_TWSI_DEBUG) & TWSI_DEBUG_DEV_EXIST) != 0) {
d298 29
a326 5
		if ((opt & OPT_CFG_CLK_ENB) == 0) {
			opt |= OPT_CFG_CLK_ENB;
			CSR_WRITE_4(sc, ALC_OPT_CFG, opt);
			CSR_READ_4(sc, ALC_OPT_CFG);
			DELAY(1000);
d328 6
d349 30
a378 5
	if ((opt & OPT_CFG_CLK_ENB) != 0) {
		opt &= ~OPT_CFG_CLK_ENB;
		CSR_WRITE_4(sc, ALC_OPT_CFG, opt);
		CSR_READ_4(sc, ALC_OPT_CFG);
		DELAY(1000);
d423 37
d508 26
a533 6

	/* Force PHY down. */
	CSR_WRITE_2(sc, ALC_GPHY_CFG,
	    GPHY_CFG_EXT_RESET | GPHY_CFG_HIB_EN | GPHY_CFG_HIB_PULSE |
	    GPHY_CFG_SEL_ANA_RESET | GPHY_CFG_PHY_IDDQ | GPHY_CFG_PWDOWN_HW);
	DELAY(1000);
d537 1
a537 1
alc_aspm(struct alc_softc *sc)
d540 1
d543 6
d550 1
a550 3
	pmcfg |= PM_CFG_SERDES_BUDS_RX_L1_ENB;
	pmcfg |= PM_CFG_SERDES_L1_ENB;
	pmcfg &= ~PM_CFG_L1_ENTRY_TIMER_MASK;
d552 21
d574 36
a609 4
		pmcfg |= PM_CFG_SERDES_PLL_L1_ENB;
		pmcfg &= ~PM_CFG_CLK_SWH_L1;
		pmcfg &= ~PM_CFG_ASPM_L1_ENB;
		pmcfg &= ~PM_CFG_ASPM_L0S_ENB;
d611 2
a612 1
		pmcfg &= ~PM_CFG_SERDES_PLL_L1_ENB;
d614 2
a615 2
		pmcfg &= ~PM_CFG_ASPM_L1_ENB;
		pmcfg &= ~PM_CFG_ASPM_L0S_ENB;
d631 1
a631 1
	char *aspm_state[] = { "L0s/L1", "L0s", "L1", "L0s/l1" };
d633 1
a633 1
	int base, mii_flags, state, error = 0;
d680 1
d697 14
d724 4
d733 1
a733 2
			if (state != 0)
				alc_disable_l0s_l1(sc);
d750 31
a780 4
	if (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_ATTANSIC_L2C)
		sc->alc_flags |= ALC_FLAG_FASTETHER | ALC_FLAG_JUMBO;
	else
		sc->alc_flags |= ALC_FLAG_JUMBO | ALC_FLAG_ASPM_MON;
d782 1
a782 1
	 * It seems that AR8131/AR8132 has silicon bug for SMB. In
a844 3
	mii_flags = 0;
	if ((sc->alc_flags & ALC_FLAG_JUMBO) != 0)
		mii_flags |= MIIF_DOPAUSE;
d846 1
a846 1
		MII_OFFSET_ANY, mii_flags);
d1359 1
a1359 4
	int enq;

	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
		return;
d1365 3
a1367 1
	enq = 0;
d1384 1
a1384 1
		enq = 1;
d1396 1
a1396 1
	if (enq) {
d1496 4
d1677 1
d1683 7
d1694 2
a1695 9
		if (status & INTR_RX_PKT) {
			int error;

			error = alc_rxintr(sc);
			if (error) {
				alc_init(ifp);
				return (0);
			}
		}
d1712 3
a1714 2
		alc_txeof(sc);
		if (!IFQ_IS_EMPTY(&ifp->if_snd))
d1716 1
d1718 2
a1719 1

d1722 1
a1722 1
	return (1);
d1737 1
a1737 1
	    BUS_DMASYNC_POSTREAD);
d1821 1
a1821 1
int
d1845 1
a1845 1
			return (EIO);
a1882 2

	return (0);
d1914 2
a1915 3
		status |= RRD_TCP_UDPCSUM_NOK | RRD_IPCSUM_NOK;
		if ((RRD_ERR_CRC | RRD_ERR_ALIGN | RRD_ERR_TRUNC |
		    RRD_ERR_RUNT) != 0)
d2028 3
a2030 1
	CSR_WRITE_4(sc, ALC_MASTER_CFG, MASTER_RESET);
d2081 3
d2145 12
a2165 2
	reg = CSR_READ_4(sc, ALC_MASTER_CFG);
	reg &= ~(MASTER_CHIP_REV_MASK | MASTER_CHIP_ID_MASK);
d2170 1
a2170 2
	reg &= ~MASTER_INTR_RD_CLR;
	reg &= ~(MASTER_IM_RX_TIMER_ENB | MASTER_IM_TX_TIMER_ENB);
d2211 1
a2211 1
	CSR_WRITE_4(sc, ALC_FRAME_SIZE, ALC_JUMBO_FRAMELEN);
d2238 1
a2238 1
	    (ALC_JUMBO_FRAMELEN >> TSO_OFFLOAD_THRESH_UNIT_SHIFT) &
d2243 3
d2262 16
a2277 8
	reg = CSR_READ_4(sc, ALC_SRAM_RX_FIFO_LEN);
	rxf_hi = (reg * 8) / 10;
	rxf_lo = (reg * 3)/ 10;
	CSR_WRITE_4(sc, ALC_RX_FIFO_PAUSE_THRESH,
	    ((rxf_lo << RX_FIFO_PAUSE_THRESH_LO_SHIFT) &
	    RX_FIFO_PAUSE_THRESH_LO_MASK) |
	    ((rxf_hi << RX_FIFO_PAUSE_THRESH_HI_SHIFT) &
	     RX_FIFO_PAUSE_THRESH_HI_MASK));
d2288 1
a2288 1
		reg |= RXQ_CFG_ASPM_THROUGHPUT_LIMIT_100M;
a2290 6
	/* Configure Rx DMAW request thresold. */
	CSR_WRITE_4(sc, ALC_RD_DMA_CFG,
	    ((RD_DMA_CFG_THRESH_DEFAULT << RD_DMA_CFG_THRESH_SHIFT) &
	    RD_DMA_CFG_THRESH_MASK) |
	    ((ALC_RD_DMA_CFG_USECS(0) << RD_DMA_CFG_TIMER_SHIFT) &
	    RD_DMA_CFG_TIMER_MASK));
d2316 1
a2316 1
	 *  AR8131/AR8132 always does checksum computation regardless
d2326 4
@


1.10
log
@alc_newbuf is always called from the interrupt context so it can't sleep;
tested by Gabriel Linder, ok kevlo, miod
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.9 2011/01/29 08:13:46 kevlo Exp $	*/
d143 1
a143 1
#define ALC_CSUM_FEATURES	(M_TCPV4_CSUM_OUT | M_UDPV4_CSUM_OUT)
@


1.9
log
@Fix two logic errors:
- "could not disable Rx/Tx MAC" from FreeBSD
- "could not disable RxQ/TxQ" from Gabriel Linder
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.8 2010/08/31 17:13:44 deraadt Exp $	*/
d107 1
a107 1
int	alc_newbuf(struct alc_softc *, struct alc_rxdesc *, int);
d1555 1
a1555 1
alc_newbuf(struct alc_softc *sc, struct alc_rxdesc *rxd, int init)
d1561 1
a1561 1
	MGETHDR(m, init ? M_WAITOK : M_DONTWAIT, MT_DATA);
d1564 1
a1564 1
	MCLGET(m, init ? M_WAITOK : M_DONTWAIT);
a1575 7
		if (!error) {
			bus_dmamap_unload(sc->sc_dmat,
			    sc->alc_cdata.alc_rx_sparemap);
			error = EFBIG;
			printf("%s: too many segments?!\n",
			    sc->sc_dev.dv_xname);
		}
d1577 1
a1577 4

		if (init)
			printf("%s: can't load RX mbuf\n", sc->sc_dev.dv_xname);

d1700 1
a1700 1
		if (alc_newbuf(sc, rxd, 0) != 0) {
d2286 1
a2286 1
		if (alc_newbuf(sc, rxd, 1) != 0)
@


1.8
log
@Add DVACT_QUIECE support.  This is called before splhigh() and before
DVACT_SUSPEND, therefore DVACT_QUIECE can do standard sleeping operations
to get ready.
Discussed quite a while back with kettenis and jakemsr, oga suddenly needed
it as well and wrote half of it, so it was time to finish it.
proofread by miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.7 2010/08/27 17:08:00 jsg Exp $	*/
d2194 1
a2194 1
		reg &= ~MAC_CFG_TX_ENB | MAC_CFG_RX_ENB;
d2245 1
a2245 1
	if ((reg & TXQ_CFG_ENB) == 0) {
@


1.7
log
@remove the unused if_init callback in struct ifnet
ok deraadt@@ henning@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.6 2010/07/27 22:39:59 deraadt Exp $	*/
d687 3
d701 1
a701 1
	return rv;
@


1.6
log
@ca_activate handler for suspend/resume.  untested -- if someone tests
this let me know.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.5 2010/04/08 00:23:53 tedu Exp $	*/
a591 1
	ifp->if_init = alc_init;
@


1.5
log
@these files don't need to include proc.h anymore.  ok oga for agp
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.4 2010/02/27 08:19:47 kevlo Exp $	*/
d82 1
d132 2
a133 1
	sizeof (struct alc_softc), alc_match, alc_attach
d678 22
@


1.4
log
@Fix multicast handling. All Atheros controllers use big-endian form
when computing multicast hash.

From Brad via FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.3 2010/01/07 12:26:06 sthen Exp $	*/
a34 1
#include <sys/proc.h>
@


1.3
log
@Rename _rxfilter functions to _iff for consistency. From Brad, ok kevlo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.2 2009/09/13 14:42:52 krw Exp $	*/
d2371 2
a2372 1
			crc = ether_crc32_le(enm->enm_addrlo, ETHER_ADDR_LEN);
d2374 1
@


1.2
log
@M_DUP_PKTHDR() define -> m_dup_pkthdr() function to properly deal
with m_tag_copy_chain() failures.

Use m_defrag() to eliminate hand rolled defragging of mbufs and
some uses of M_DUP_PKTHDR().

Original diff from thib@@, claudio@@'s feedback integrated by me.

Tests kevlo@@ claudio@@, "reads ok" blambert@@

ok thib@@ claudio@@, "m_defrag() bits ok" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_alc.c,v 1.1 2009/08/08 09:31:13 kevlo Exp $	*/
d113 1
a113 1
void	alc_rxfilter(struct alc_softc *);
d1234 1
a1234 1
			alc_rxfilter(sc);
d2075 2
a2076 1
	alc_rxfilter(sc);
d2339 1
a2339 1
alc_rxfilter(struct alc_softc *sc)
@


1.1
log
@alc(4) is a driver for the Atheros AR8131/AR8132 ethernet chip.
this driver was written by Pyun YongHyeon from FreeBSD.

"go ahead" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1027 1
a1027 4
		error = 0;

		MGETHDR(m, M_DONTWAIT, MT_DATA);
		if (m == NULL) {
a1033 17

		M_DUP_PKTHDR(m, *m_head);
		if ((*m_head)->m_pkthdr.len > MHLEN) {
			MCLGET(m, M_DONTWAIT);
			if (!(m->m_flags & M_EXT)) {
				m_freem(*m_head);
				m_freem(m);
				*m_head = NULL;
				return (ENOBUFS);
			}
		}
		m_copydata(*m_head, 0, (*m_head)->m_pkthdr.len,
		    mtod(m, caddr_t));
		m_freem(*m_head);
		m->m_len = m->m_pkthdr.len;
		*m_head = m;

a1035 1

a1038 4
			if (!error) {
				bus_dmamap_unload(sc->sc_dmat, map);
				error = EFBIG;
			}
@

