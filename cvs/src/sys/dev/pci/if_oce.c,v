head	1.99;
access;
symbols
	OPENBSD_6_1:1.99.0.2
	OPENBSD_6_1_BASE:1.99
	OPENBSD_6_0:1.95.0.4
	OPENBSD_6_0_BASE:1.95
	OPENBSD_5_9:1.92.0.2
	OPENBSD_5_9_BASE:1.92
	OPENBSD_5_8:1.85.0.4
	OPENBSD_5_8_BASE:1.85
	OPENBSD_5_7:1.81.0.4
	OPENBSD_5_7_BASE:1.81
	OPENBSD_5_6:1.77.0.4
	OPENBSD_5_6_BASE:1.77
	OPENBSD_5_5:1.74.0.4
	OPENBSD_5_5_BASE:1.74
	OPENBSD_5_4:1.69.0.4
	OPENBSD_5_4_BASE:1.69
	OPENBSD_5_3:1.69.0.2
	OPENBSD_5_3_BASE:1.69;
locks; strict;
comment	@ * @;


1.99
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.98;
commitid	VyLWTsbepAOk7VQM;

1.98
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.97;
commitid	RlO92XR575sygHqm;

1.97
date	2016.09.14.19.04.54;	author mikeb;	state Exp;
branches;
next	1.96;
commitid	6cSafhdF8jBx6vby;

1.96
date	2016.08.24.10.38.34;	author dlg;	state Exp;
branches;
next	1.95;
commitid	WDsFfiVI6qe7io3Y;

1.95
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.94;
commitid	8YSL8ByWzGeIGBiJ;

1.94
date	2016.03.14.01.16.39;	author mikeb;	state Exp;
branches;
next	1.93;
commitid	FFfWSclnOGIMApek;

1.93
date	2016.03.04.00.04.48;	author deraadt;	state Exp;
branches;
next	1.92;
commitid	PvWQ3PPbOZBc1fEt;

1.92
date	2016.01.06.06.41.57;	author mikeb;	state Exp;
branches;
next	1.91;
commitid	cCxZQCbcCIwH3Ewr;

1.91
date	2015.12.11.16.07.02;	author mpi;	state Exp;
branches;
next	1.90;
commitid	fbhqfhfdKxBcsetK;

1.90
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.89;
commitid	B0kwmVGiD5DVx4kv;

1.89
date	2015.11.14.17.54.57;	author mpi;	state Exp;
branches;
next	1.88;
commitid	Waft2RDjXAxr4qZ9;

1.88
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.87;
commitid	hPF95ClMUQfeqQDX;

1.87
date	2015.09.29.17.04.20;	author chris;	state Exp;
branches;
next	1.86;
commitid	yUo24wNQhgXVvtpT;

1.86
date	2015.09.11.13.02.28;	author stsp;	state Exp;
branches;
next	1.85;
commitid	6vhYvh5CxZAHMnsN;

1.85
date	2015.06.29.18.58.04;	author mikeb;	state Exp;
branches;
next	1.84;
commitid	QfnJr92HmOeWG3Nu;

1.84
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.83;
commitid	MVWrtktB46JRxFWT;

1.83
date	2015.04.30.07.51.07;	author mpi;	state Exp;
branches;
next	1.82;
commitid	H09AuNxNnUcYramX;

1.82
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.81;
commitid	p4LJxGKbi0BU2cG6;

1.81
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.80;
commitid	yM2VFFhpDTeFQlve;

1.80
date	2014.12.13.21.05.33;	author doug;	state Exp;
branches;
next	1.79;
commitid	20ZyHa9gTJxHxhwD;

1.79
date	2014.08.30.09.48.23;	author dlg;	state Exp;
branches;
next	1.78;
commitid	OKXgTMKhJ8SVfHJf;

1.78
date	2014.08.14.09.52.03;	author mikeb;	state Exp;
branches;
next	1.77;
commitid	6JIybVPYvVLJcco3;

1.77
date	2014.07.22.13.12.11;	author mpi;	state Exp;
branches;
next	1.76;
commitid	TGHgrLxu6sxZoiFt;

1.76
date	2014.07.12.18.48.52;	author tedu;	state Exp;
branches;
next	1.75;
commitid	OBNa5kfxQ2UXoiIw;

1.75
date	2014.07.08.05.35.18;	author dlg;	state Exp;
branches;
next	1.74;
commitid	0QJleeeWqZmC5anF;

1.74
date	2014.01.20.17.21.22;	author chris;	state Exp;
branches;
next	1.73;

1.73
date	2014.01.20.17.17.08;	author chris;	state Exp;
branches;
next	1.72;

1.72
date	2014.01.20.17.14.56;	author chris;	state Exp;
branches;
next	1.71;

1.71
date	2013.08.23.19.16.17;	author mikeb;	state Exp;
branches;
next	1.70;

1.70
date	2013.08.07.01.06.36;	author bluhm;	state Exp;
branches;
next	1.69;

1.69
date	2013.01.17.00.48.04;	author henning;	state Exp;
branches;
next	1.68;

1.68
date	2012.12.10.17.36.10;	author mikeb;	state Exp;
branches;
next	1.67;

1.67
date	2012.11.27.18.08.21;	author gsoares;	state Exp;
branches;
next	1.66;

1.66
date	2012.11.26.19.03.59;	author mikeb;	state Exp;
branches;
next	1.65;

1.65
date	2012.11.26.18.58.11;	author mikeb;	state Exp;
branches;
next	1.64;

1.64
date	2012.11.23.18.46.03;	author mikeb;	state Exp;
branches;
next	1.63;

1.63
date	2012.11.21.11.24.16;	author mikeb;	state Exp;
branches;
next	1.62;

1.62
date	2012.11.20.18.43.19;	author mikeb;	state Exp;
branches;
next	1.61;

1.61
date	2012.11.14.17.25.46;	author mikeb;	state Exp;
branches;
next	1.60;

1.60
date	2012.11.13.19.17.39;	author mikeb;	state Exp;
branches;
next	1.59;

1.59
date	2012.11.13.19.13.08;	author mikeb;	state Exp;
branches;
next	1.58;

1.58
date	2012.11.13.17.52.11;	author mikeb;	state Exp;
branches;
next	1.57;

1.57
date	2012.11.13.17.40.41;	author mikeb;	state Exp;
branches;
next	1.56;

1.56
date	2012.11.12.20.31.32;	author mikeb;	state Exp;
branches;
next	1.55;

1.55
date	2012.11.09.21.11.42;	author mikeb;	state Exp;
branches;
next	1.54;

1.54
date	2012.11.09.20.31.43;	author mikeb;	state Exp;
branches;
next	1.53;

1.53
date	2012.11.09.18.58.17;	author mikeb;	state Exp;
branches;
next	1.52;

1.52
date	2012.11.09.18.56.31;	author mikeb;	state Exp;
branches;
next	1.51;

1.51
date	2012.11.09.18.53.04;	author mikeb;	state Exp;
branches;
next	1.50;

1.50
date	2012.11.09.18.40.12;	author mikeb;	state Exp;
branches;
next	1.49;

1.49
date	2012.11.08.19.48.37;	author mikeb;	state Exp;
branches;
next	1.48;

1.48
date	2012.11.08.18.56.54;	author mikeb;	state Exp;
branches;
next	1.47;

1.47
date	2012.11.08.18.26.17;	author mikeb;	state Exp;
branches;
next	1.46;

1.46
date	2012.11.08.17.59.08;	author mikeb;	state Exp;
branches;
next	1.45;

1.45
date	2012.11.08.17.48.20;	author mikeb;	state Exp;
branches;
next	1.44;

1.44
date	2012.11.07.19.43.33;	author mikeb;	state Exp;
branches;
next	1.43;

1.43
date	2012.11.07.12.46.49;	author mikeb;	state Exp;
branches;
next	1.42;

1.42
date	2012.11.07.12.43.35;	author mikeb;	state Exp;
branches;
next	1.41;

1.41
date	2012.11.05.20.05.39;	author mikeb;	state Exp;
branches;
next	1.40;

1.40
date	2012.11.03.00.23.25;	author mikeb;	state Exp;
branches;
next	1.39;

1.39
date	2012.11.03.00.05.41;	author brynet;	state Exp;
branches;
next	1.38;

1.38
date	2012.11.02.23.34.57;	author mikeb;	state Exp;
branches;
next	1.37;

1.37
date	2012.10.31.20.15.43;	author mikeb;	state Exp;
branches;
next	1.36;

1.36
date	2012.10.30.17.38.23;	author mikeb;	state Exp;
branches;
next	1.35;

1.35
date	2012.10.29.22.33.20;	author mikeb;	state Exp;
branches;
next	1.34;

1.34
date	2012.10.29.22.16.45;	author mikeb;	state Exp;
branches;
next	1.33;

1.33
date	2012.10.29.18.36.42;	author mikeb;	state Exp;
branches;
next	1.32;

1.32
date	2012.10.29.18.17.39;	author mikeb;	state Exp;
branches;
next	1.31;

1.31
date	2012.10.29.18.14.28;	author mikeb;	state Exp;
branches;
next	1.30;

1.30
date	2012.10.26.23.35.09;	author mikeb;	state Exp;
branches;
next	1.29;

1.29
date	2012.10.26.18.05.50;	author mikeb;	state Exp;
branches;
next	1.28;

1.28
date	2012.10.26.17.56.24;	author mikeb;	state Exp;
branches;
next	1.27;

1.27
date	2012.10.25.17.42.16;	author mikeb;	state Exp;
branches;
next	1.26;

1.26
date	2012.10.25.17.26.42;	author mikeb;	state Exp;
branches;
next	1.25;

1.25
date	2012.10.25.17.24.11;	author mikeb;	state Exp;
branches;
next	1.24;

1.24
date	2012.10.25.16.47.30;	author mikeb;	state Exp;
branches;
next	1.23;

1.23
date	2012.10.22.02.49.03;	author brad;	state Exp;
branches;
next	1.22;

1.22
date	2012.10.18.09.31.07;	author mikeb;	state Exp;
branches;
next	1.21;

1.21
date	2012.10.15.19.30.39;	author mikeb;	state Exp;
branches;
next	1.20;

1.20
date	2012.10.15.19.23.23;	author mikeb;	state Exp;
branches;
next	1.19;

1.19
date	2012.10.12.18.24.31;	author mikeb;	state Exp;
branches;
next	1.18;

1.18
date	2012.10.12.17.41.40;	author mikeb;	state Exp;
branches;
next	1.17;

1.17
date	2012.10.12.15.16.45;	author mikeb;	state Exp;
branches;
next	1.16;

1.16
date	2012.10.11.16.38.10;	author mikeb;	state Exp;
branches;
next	1.15;

1.15
date	2012.10.11.16.33.57;	author mikeb;	state Exp;
branches;
next	1.14;

1.14
date	2012.08.09.19.29.03;	author mikeb;	state Exp;
branches;
next	1.13;

1.13
date	2012.08.09.19.23.35;	author mikeb;	state Exp;
branches;
next	1.12;

1.12
date	2012.08.09.19.15.47;	author mikeb;	state Exp;
branches;
next	1.11;

1.11
date	2012.08.09.19.03.14;	author mikeb;	state Exp;
branches;
next	1.10;

1.10
date	2012.08.09.18.49.57;	author mikeb;	state Exp;
branches;
next	1.9;

1.9
date	2012.08.09.18.41.45;	author mikeb;	state Exp;
branches;
next	1.8;

1.8
date	2012.08.09.12.43.46;	author mikeb;	state Exp;
branches;
next	1.7;

1.7
date	2012.08.08.17.50.24;	author mikeb;	state Exp;
branches;
next	1.6;

1.6
date	2012.08.08.09.50.15;	author mikeb;	state Exp;
branches;
next	1.5;

1.5
date	2012.08.07.17.16.26;	author mikeb;	state Exp;
branches;
next	1.4;

1.4
date	2012.08.07.09.23.13;	author mikeb;	state Exp;
branches;
next	1.3;

1.3
date	2012.08.06.21.55.31;	author mikeb;	state Exp;
branches;
next	1.2;

1.2
date	2012.08.02.22.14.31;	author mikeb;	state Exp;
branches;
next	1.1;

1.1
date	2012.08.02.17.35.52;	author mikeb;	state Exp;
branches;
next	;


desc
@@


1.99
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@/*	$OpenBSD: if_oce.c,v 1.98 2016/09/15 02:00:17 dlg Exp $	*/

/*
 * Copyright (c) 2012 Mike Belopuhov
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*-
 * Copyright (C) 2012 Emulex
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *    this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the Emulex Corporation nor the names of its
 *    contributors may be used to endorse or promote products derived from
 *    this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 *
 * Contact Information:
 * freebsd-drivers@@emulex.com
 *
 * Emulex
 * 3333 Susan Street
 * Costa Mesa, CA 92626
 */

#include "bpfilter.h"
#include "vlan.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/sockio.h>
#include <sys/mbuf.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/socket.h>
#include <sys/queue.h>
#include <sys/timeout.h>
#include <sys/pool.h>

#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#ifdef INET6
#include <netinet/ip6.h>
#endif

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>
#include <dev/pci/pcidevs.h>

#include <dev/pci/if_ocereg.h>

#ifndef TRUE
#define TRUE			1
#endif
#ifndef FALSE
#define FALSE			0
#endif

#define OCE_MBX_TIMEOUT		5

#define OCE_MAX_PAYLOAD		65536

#define OCE_TX_RING_SIZE	512
#define OCE_RX_RING_SIZE	1024

/* This should be powers of 2. Like 2,4,8 & 16 */
#define OCE_MAX_RSS		4 /* TODO: 8 */
#define OCE_MAX_RQ		OCE_MAX_RSS + 1 /* one default queue */
#define OCE_MAX_WQ		8

#define OCE_MAX_EQ		32
#define OCE_MAX_CQ		OCE_MAX_RQ + OCE_MAX_WQ + 1 /* one MCC queue */
#define OCE_MAX_CQ_EQ		8 /* Max CQ that can attached to an EQ */

#define OCE_DEFAULT_EQD		80

#define OCE_MIN_MTU		256
#define OCE_MAX_MTU		9000

#define OCE_MAX_RQ_COMPL	64
#define OCE_MAX_RQ_POSTS	255
#define OCE_RX_BUF_SIZE		2048

#define OCE_MAX_TX_ELEMENTS	29
#define OCE_MAX_TX_DESC		1024
#define OCE_MAX_TX_SIZE		65535

#define OCE_MEM_KVA(_m)		((void *)((_m)->vaddr))
#define OCE_MEM_DVA(_m)		((_m)->paddr)

#define OCE_WQ_FOREACH(sc, wq, i) 	\
	for (i = 0, wq = sc->sc_wq[0]; i < sc->sc_nwq; i++, wq = sc->sc_wq[i])
#define OCE_RQ_FOREACH(sc, rq, i) 	\
	for (i = 0, rq = sc->sc_rq[0]; i < sc->sc_nrq; i++, rq = sc->sc_rq[i])
#define OCE_EQ_FOREACH(sc, eq, i) 	\
	for (i = 0, eq = sc->sc_eq[0]; i < sc->sc_neq; i++, eq = sc->sc_eq[i])
#define OCE_CQ_FOREACH(sc, cq, i) 	\
	for (i = 0, cq = sc->sc_cq[0]; i < sc->sc_ncq; i++, cq = sc->sc_cq[i])
#define OCE_RING_FOREACH(_r, _v, _c)	\
	for ((_v) = oce_ring_first(_r); _c; (_v) = oce_ring_next(_r))

static inline int
ilog2(unsigned int v)
{
	int r = 0;

	while (v >>= 1)
		r++;
	return (r);
}

struct oce_pkt {
	struct mbuf *		mbuf;
	bus_dmamap_t		map;
	int			nsegs;
	SIMPLEQ_ENTRY(oce_pkt)	entry;
};
SIMPLEQ_HEAD(oce_pkt_list, oce_pkt);

struct oce_dma_mem {
	bus_dma_tag_t		tag;
	bus_dmamap_t		map;
	bus_dma_segment_t	segs;
	int			nsegs;
	bus_size_t		size;
	caddr_t			vaddr;
	bus_addr_t		paddr;
};

struct oce_ring {
	int			index;
	int			nitems;
	int			nused;
	int			isize;
	struct oce_dma_mem	dma;
};

struct oce_softc;

enum cq_len {
	CQ_LEN_256  = 256,
	CQ_LEN_512  = 512,
	CQ_LEN_1024 = 1024
};

enum eq_len {
	EQ_LEN_256  = 256,
	EQ_LEN_512  = 512,
	EQ_LEN_1024 = 1024,
	EQ_LEN_2048 = 2048,
	EQ_LEN_4096 = 4096
};

enum eqe_size {
	EQE_SIZE_4  = 4,
	EQE_SIZE_16 = 16
};

enum qtype {
	QTYPE_EQ,
	QTYPE_MQ,
	QTYPE_WQ,
	QTYPE_RQ,
	QTYPE_CQ,
	QTYPE_RSS
};

struct oce_eq {
	struct oce_softc *	sc;
	struct oce_ring *	ring;
	enum qtype		type;
	int			id;

	struct oce_cq *		cq[OCE_MAX_CQ_EQ];
	int			cq_valid;

	int			nitems;
	int			isize;
	int			delay;
};

struct oce_cq {
	struct oce_softc *	sc;
	struct oce_ring *	ring;
	enum qtype		type;
	int			id;

	struct oce_eq *		eq;

	void			(*cq_intr)(void *);
	void *			cb_arg;

	int			nitems;
	int			nodelay;
	int			eventable;
	int			ncoalesce;
};

struct oce_mq {
	struct oce_softc *	sc;
	struct oce_ring *	ring;
	enum qtype		type;
	int			id;

	struct oce_cq *		cq;

	int			nitems;
};

struct oce_wq {
	struct oce_softc *	sc;
	struct oce_ring *	ring;
	enum qtype		type;
	int			id;

	struct oce_cq *		cq;

	struct oce_pkt_list	pkt_list;
	struct oce_pkt_list	pkt_free;

	int			nitems;
};

struct oce_rq {
	struct oce_softc *	sc;
	struct oce_ring *	ring;
	enum qtype		type;
	int			id;

	struct oce_cq *		cq;

	struct if_rxring	rxring;
	struct oce_pkt_list	pkt_list;
	struct oce_pkt_list	pkt_free;

	uint32_t		rss_cpuid;

#ifdef OCE_LRO
	struct lro_ctrl		lro;
	int			lro_pkts_queued;
#endif

	int			nitems;
	int			fragsize;
	int			mtu;
	int			rss;
};

struct oce_softc {
	struct device		sc_dev;

	uint			sc_flags;
#define  OCE_F_BE2		 0x00000001
#define  OCE_F_BE3		 0x00000002
#define  OCE_F_XE201		 0x00000008
#define  OCE_F_BE3_NATIVE	 0x00000100
#define  OCE_F_RESET_RQD	 0x00001000
#define  OCE_F_MBOX_ENDIAN_RQD	 0x00002000

	bus_dma_tag_t		sc_dmat;

	bus_space_tag_t		sc_cfg_iot;
	bus_space_handle_t	sc_cfg_ioh;
	bus_size_t		sc_cfg_size;

	bus_space_tag_t		sc_csr_iot;
	bus_space_handle_t	sc_csr_ioh;
	bus_size_t		sc_csr_size;

	bus_space_tag_t		sc_db_iot;
	bus_space_handle_t	sc_db_ioh;
	bus_size_t		sc_db_size;

	void *			sc_ih;

	struct arpcom		sc_ac;
	struct ifmedia		sc_media;
	ushort			sc_link_up;
	ushort			sc_link_speed;
	uint64_t		sc_fc;

	struct oce_dma_mem	sc_mbx;
	struct oce_dma_mem	sc_pld;

	uint			sc_port;
	uint			sc_fmode;

	struct oce_wq *		sc_wq[OCE_MAX_WQ];	/* TX work queues */
	struct oce_rq *		sc_rq[OCE_MAX_RQ];	/* RX work queues */
	struct oce_cq *		sc_cq[OCE_MAX_CQ];	/* Completion queues */
	struct oce_eq *		sc_eq[OCE_MAX_EQ];	/* Event queues */
	struct oce_mq *		sc_mq;			/* Mailbox queue */

	ushort			sc_neq;
	ushort			sc_ncq;
	ushort			sc_nrq;
	ushort			sc_nwq;
	ushort			sc_nintr;

	ushort			sc_tx_ring_size;
	ushort			sc_rx_ring_size;
	ushort			sc_rss_enable;

	uint32_t		sc_if_id;	/* interface ID */
	uint32_t		sc_pmac_id;	/* PMAC id */
	char			sc_macaddr[ETHER_ADDR_LEN];

	uint32_t		sc_pvid;

	uint64_t		sc_rx_errors;
	uint64_t		sc_tx_errors;

	struct timeout		sc_tick;
	struct timeout		sc_rxrefill;

	void *			sc_statcmd;
};

#define IS_BE(sc)		ISSET((sc)->sc_flags, OCE_F_BE2 | OCE_F_BE3)
#define IS_XE201(sc)		ISSET((sc)->sc_flags, OCE_F_XE201)

#define ADDR_HI(x)		((uint32_t)((uint64_t)(x) >> 32))
#define ADDR_LO(x)		((uint32_t)((uint64_t)(x) & 0xffffffff))

#define IF_LRO_ENABLED(ifp)	ISSET((ifp)->if_capabilities, IFCAP_LRO)

int 	oce_match(struct device *, void *, void *);
void	oce_attach(struct device *, struct device *, void *);
int 	oce_pci_alloc(struct oce_softc *, struct pci_attach_args *);
void	oce_attachhook(struct device *);
void	oce_attach_ifp(struct oce_softc *);
int 	oce_ioctl(struct ifnet *, u_long, caddr_t);
int	oce_rxrinfo(struct oce_softc *, struct if_rxrinfo *);
void	oce_iff(struct oce_softc *);
void	oce_link_status(struct oce_softc *);
void	oce_media_status(struct ifnet *, struct ifmediareq *);
int 	oce_media_change(struct ifnet *);
void	oce_tick(void *);
void	oce_init(void *);
void	oce_stop(struct oce_softc *);
void	oce_watchdog(struct ifnet *);
void	oce_start(struct ifnet *);
int	oce_encap(struct oce_softc *, struct mbuf **, int wqidx);
#ifdef OCE_TSO
struct mbuf *
	oce_tso(struct oce_softc *, struct mbuf **);
#endif
int 	oce_intr(void *);
void	oce_intr_wq(void *);
void	oce_txeof(struct oce_wq *);
void	oce_intr_rq(void *);
void	oce_rxeof(struct oce_rq *, struct oce_nic_rx_cqe *);
void	oce_rxeoc(struct oce_rq *, struct oce_nic_rx_cqe *);
int 	oce_vtp_valid(struct oce_softc *, struct oce_nic_rx_cqe *);
int 	oce_port_valid(struct oce_softc *, struct oce_nic_rx_cqe *);
#ifdef OCE_LRO
void	oce_flush_lro(struct oce_rq *);
int 	oce_init_lro(struct oce_softc *);
void	oce_free_lro(struct oce_softc *);
#endif
int	oce_get_buf(struct oce_rq *);
int	oce_alloc_rx_bufs(struct oce_rq *);
void	oce_refill_rx(void *);
void	oce_free_posted_rxbuf(struct oce_rq *);
void	oce_intr_mq(void *);
void	oce_link_event(struct oce_softc *,
	    struct oce_async_cqe_link_state *);

int 	oce_init_queues(struct oce_softc *);
void	oce_release_queues(struct oce_softc *);
struct oce_wq *oce_create_wq(struct oce_softc *, struct oce_eq *);
void	oce_drain_wq(struct oce_wq *);
void	oce_destroy_wq(struct oce_wq *);
struct oce_rq *
	oce_create_rq(struct oce_softc *, struct oce_eq *, int rss);
void	oce_drain_rq(struct oce_rq *);
void	oce_destroy_rq(struct oce_rq *);
struct oce_eq *
	oce_create_eq(struct oce_softc *);
static inline void
	oce_arm_eq(struct oce_eq *, int neqe, int rearm, int clearint);
void	oce_drain_eq(struct oce_eq *);
void	oce_destroy_eq(struct oce_eq *);
struct oce_mq *
	oce_create_mq(struct oce_softc *, struct oce_eq *);
void	oce_drain_mq(struct oce_mq *);
void	oce_destroy_mq(struct oce_mq *);
struct oce_cq *
	oce_create_cq(struct oce_softc *, struct oce_eq *, int nitems,
	    int isize, int eventable, int nodelay, int ncoalesce);
static inline void
	oce_arm_cq(struct oce_cq *, int ncqe, int rearm);
void	oce_destroy_cq(struct oce_cq *);

int	oce_dma_alloc(struct oce_softc *, bus_size_t, struct oce_dma_mem *);
void	oce_dma_free(struct oce_softc *, struct oce_dma_mem *);
#define	oce_dma_sync(d, f) \
	    bus_dmamap_sync((d)->tag, (d)->map, 0, (d)->map->dm_mapsize, f)

struct oce_ring *
	oce_create_ring(struct oce_softc *, int nitems, int isize, int maxseg);
void	oce_destroy_ring(struct oce_softc *, struct oce_ring *);
int	oce_load_ring(struct oce_softc *, struct oce_ring *,
	    struct oce_pa *, int max_segs);
static inline void *
	oce_ring_get(struct oce_ring *);
static inline void *
	oce_ring_first(struct oce_ring *);
static inline void *
	oce_ring_next(struct oce_ring *);
struct oce_pkt *
	oce_pkt_alloc(struct oce_softc *, size_t size, int nsegs,
	    int maxsegsz);
void	oce_pkt_free(struct oce_softc *, struct oce_pkt *);
static inline struct oce_pkt *
	oce_pkt_get(struct oce_pkt_list *);
static inline void
	oce_pkt_put(struct oce_pkt_list *, struct oce_pkt *);

int	oce_init_fw(struct oce_softc *);
int	oce_mbox_init(struct oce_softc *);
int	oce_mbox_dispatch(struct oce_softc *);
int	oce_cmd(struct oce_softc *, int subsys, int opcode, int version,
	    void *payload, int length);
void	oce_first_mcc(struct oce_softc *);

int	oce_get_fw_config(struct oce_softc *);
int	oce_check_native_mode(struct oce_softc *);
int	oce_create_iface(struct oce_softc *, uint8_t *macaddr);
int	oce_config_vlan(struct oce_softc *, struct normal_vlan *vtags,
	    int nvtags, int untagged, int promisc);
int	oce_set_flow_control(struct oce_softc *, uint64_t);
int	oce_config_rss(struct oce_softc *, int enable);
int	oce_update_mcast(struct oce_softc *, uint8_t multi[][ETHER_ADDR_LEN],
	    int naddr);
int	oce_set_promisc(struct oce_softc *, int enable);
int	oce_get_link_status(struct oce_softc *);

void	oce_macaddr_set(struct oce_softc *);
int	oce_macaddr_get(struct oce_softc *, uint8_t *macaddr);
int	oce_macaddr_add(struct oce_softc *, uint8_t *macaddr, uint32_t *pmac);
int	oce_macaddr_del(struct oce_softc *, uint32_t pmac);

int	oce_new_rq(struct oce_softc *, struct oce_rq *);
int	oce_new_wq(struct oce_softc *, struct oce_wq *);
int	oce_new_mq(struct oce_softc *, struct oce_mq *);
int	oce_new_eq(struct oce_softc *, struct oce_eq *);
int	oce_new_cq(struct oce_softc *, struct oce_cq *);

int	oce_init_stats(struct oce_softc *);
int	oce_update_stats(struct oce_softc *);
int	oce_stats_be2(struct oce_softc *, uint64_t *, uint64_t *);
int	oce_stats_be3(struct oce_softc *, uint64_t *, uint64_t *);
int	oce_stats_xe(struct oce_softc *, uint64_t *, uint64_t *);

struct pool *oce_pkt_pool;

struct cfdriver oce_cd = {
	NULL, "oce", DV_IFNET
};

struct cfattach oce_ca = {
	sizeof(struct oce_softc), oce_match, oce_attach, NULL, NULL
};

const struct pci_matchid oce_devices[] = {
	{ PCI_VENDOR_SERVERENGINES, PCI_PRODUCT_SERVERENGINES_BE2 },
	{ PCI_VENDOR_SERVERENGINES, PCI_PRODUCT_SERVERENGINES_BE3 },
	{ PCI_VENDOR_SERVERENGINES, PCI_PRODUCT_SERVERENGINES_OCBE2 },
	{ PCI_VENDOR_SERVERENGINES, PCI_PRODUCT_SERVERENGINES_OCBE3 },
	{ PCI_VENDOR_EMULEX, PCI_PRODUCT_EMULEX_XE201 },
};

int
oce_match(struct device *parent, void *match, void *aux)
{
	return (pci_matchbyid(aux, oce_devices, nitems(oce_devices)));
}

void
oce_attach(struct device *parent, struct device *self, void *aux)
{
	struct pci_attach_args *pa = (struct pci_attach_args *)aux;
	struct oce_softc *sc = (struct oce_softc *)self;
	const char *intrstr = NULL;
	pci_intr_handle_t ih;

	switch (PCI_PRODUCT(pa->pa_id)) {
	case PCI_PRODUCT_SERVERENGINES_BE2:
	case PCI_PRODUCT_SERVERENGINES_OCBE2:
		SET(sc->sc_flags, OCE_F_BE2);
		break;
	case PCI_PRODUCT_SERVERENGINES_BE3:
	case PCI_PRODUCT_SERVERENGINES_OCBE3:
		SET(sc->sc_flags, OCE_F_BE3);
		break;
	case PCI_PRODUCT_EMULEX_XE201:
		SET(sc->sc_flags, OCE_F_XE201);
		break;
	}

	sc->sc_dmat = pa->pa_dmat;
	if (oce_pci_alloc(sc, pa))
		return;

	sc->sc_tx_ring_size = OCE_TX_RING_SIZE;
	sc->sc_rx_ring_size = OCE_RX_RING_SIZE;

	/* create the bootstrap mailbox */
	if (oce_dma_alloc(sc, sizeof(struct oce_bmbx), &sc->sc_mbx)) {
		printf(": failed to allocate mailbox memory\n");
		return;
	}
	if (oce_dma_alloc(sc, OCE_MAX_PAYLOAD, &sc->sc_pld)) {
		printf(": failed to allocate payload memory\n");
		goto fail_1;
	}

	if (oce_init_fw(sc))
		goto fail_2;

	if (oce_mbox_init(sc)) {
		printf(": failed to initialize mailbox\n");
		goto fail_2;
	}

	if (oce_get_fw_config(sc)) {
		printf(": failed to get firmware configuration\n");
		goto fail_2;
	}

	if (ISSET(sc->sc_flags, OCE_F_BE3)) {
		if (oce_check_native_mode(sc))
			goto fail_2;
	}

	if (oce_macaddr_get(sc, sc->sc_macaddr)) {
		printf(": failed to fetch MAC address\n");
		goto fail_2;
	}
	memcpy(sc->sc_ac.ac_enaddr, sc->sc_macaddr, ETHER_ADDR_LEN);

	if (oce_pkt_pool == NULL) {
		oce_pkt_pool = malloc(sizeof(struct pool), M_DEVBUF, M_NOWAIT);
		if (oce_pkt_pool == NULL) {
			printf(": unable to allocate descriptor pool\n");
			goto fail_2;
		}
		pool_init(oce_pkt_pool, sizeof(struct oce_pkt), 0, IPL_NET,
		    0, "ocepkts", NULL);
	}

	/* We allocate a single interrupt resource */
	sc->sc_nintr = 1;
	if (pci_intr_map_msi(pa, &ih) != 0 &&
	    pci_intr_map(pa, &ih) != 0) {
		printf(": couldn't map interrupt\n");
		goto fail_2;
	}

	intrstr = pci_intr_string(pa->pa_pc, ih);
	sc->sc_ih = pci_intr_establish(pa->pa_pc, ih, IPL_NET, oce_intr, sc,
	    sc->sc_dev.dv_xname);
	if (sc->sc_ih == NULL) {
		printf(": couldn't establish interrupt\n");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		goto fail_2;
	}
	printf(": %s", intrstr);

	if (oce_init_stats(sc))
		goto fail_3;

	if (oce_init_queues(sc))
		goto fail_3;

	oce_attach_ifp(sc);

#ifdef OCE_LRO
	if (oce_init_lro(sc))
		goto fail_4;
#endif

	timeout_set(&sc->sc_tick, oce_tick, sc);
	timeout_set(&sc->sc_rxrefill, oce_refill_rx, sc);

	config_mountroot(self, oce_attachhook);

	printf(", address %s\n", ether_sprintf(sc->sc_ac.ac_enaddr));

	return;

#ifdef OCE_LRO
fail_4:
	oce_free_lro(sc);
	ether_ifdetach(&sc->sc_ac.ac_if);
	if_detach(&sc->sc_ac.ac_if);
	oce_release_queues(sc);
#endif
fail_3:
	pci_intr_disestablish(pa->pa_pc, sc->sc_ih);
fail_2:
	oce_dma_free(sc, &sc->sc_pld);
fail_1:
	oce_dma_free(sc, &sc->sc_mbx);
}

int
oce_pci_alloc(struct oce_softc *sc, struct pci_attach_args *pa)
{
	pcireg_t memtype, reg;

	/* setup the device config region */
	if (ISSET(sc->sc_flags, OCE_F_BE2))
		reg = OCE_BAR_CFG_BE2;
	else
		reg = OCE_BAR_CFG;

	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, reg);
	if (pci_mapreg_map(pa, reg, memtype, 0, &sc->sc_cfg_iot,
	    &sc->sc_cfg_ioh, NULL, &sc->sc_cfg_size,
	    IS_BE(sc) ? 0 : 32768)) {
		printf(": can't find cfg mem space\n");
		return (ENXIO);
	}

	/*
	 * Read the SLI_INTF register and determine whether we
	 * can use this port and its features
	 */
	reg = pci_conf_read(pa->pa_pc, pa->pa_tag, OCE_INTF_REG_OFFSET);
	if (OCE_SLI_SIGNATURE(reg) != OCE_INTF_VALID_SIG) {
		printf(": invalid signature\n");
		goto fail_1;
	}
	if (OCE_SLI_REVISION(reg) != OCE_INTF_SLI_REV4) {
		printf(": unsupported SLI revision\n");
		goto fail_1;
	}
	if (OCE_SLI_IFTYPE(reg) == OCE_INTF_IF_TYPE_1)
		SET(sc->sc_flags, OCE_F_MBOX_ENDIAN_RQD);
	if (OCE_SLI_HINT1(reg) == OCE_INTF_FUNC_RESET_REQD)
		SET(sc->sc_flags, OCE_F_RESET_RQD);

	/* Lancer has one BAR (CFG) but BE3 has three (CFG, CSR, DB) */
	if (IS_BE(sc)) {
		/* set up CSR region */
		reg = OCE_BAR_CSR;
		memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, reg);
		if (pci_mapreg_map(pa, reg, memtype, 0, &sc->sc_csr_iot,
		    &sc->sc_csr_ioh, NULL, &sc->sc_csr_size, 0)) {
			printf(": can't find csr mem space\n");
			goto fail_1;
		}

		/* set up DB doorbell region */
		reg = OCE_BAR_DB;
		memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, reg);
		if (pci_mapreg_map(pa, reg, memtype, 0, &sc->sc_db_iot,
		    &sc->sc_db_ioh, NULL, &sc->sc_db_size, 0)) {
			printf(": can't find csr mem space\n");
			goto fail_2;
		}
	} else {
		sc->sc_csr_iot = sc->sc_db_iot = sc->sc_cfg_iot;
		sc->sc_csr_ioh = sc->sc_db_ioh = sc->sc_cfg_ioh;
	}

	return (0);

fail_2:
	bus_space_unmap(sc->sc_csr_iot, sc->sc_csr_ioh, sc->sc_csr_size);
fail_1:
	bus_space_unmap(sc->sc_cfg_iot, sc->sc_cfg_ioh, sc->sc_cfg_size);
	return (ENXIO);
}

static inline uint32_t
oce_read_cfg(struct oce_softc *sc, bus_size_t off)
{
	bus_space_barrier(sc->sc_cfg_iot, sc->sc_cfg_ioh, off, 4,
	    BUS_SPACE_BARRIER_READ);
	return (bus_space_read_4(sc->sc_cfg_iot, sc->sc_cfg_ioh, off));
}

static inline uint32_t
oce_read_csr(struct oce_softc *sc, bus_size_t off)
{
	bus_space_barrier(sc->sc_csr_iot, sc->sc_csr_ioh, off, 4,
	    BUS_SPACE_BARRIER_READ);
	return (bus_space_read_4(sc->sc_csr_iot, sc->sc_csr_ioh, off));
}

static inline uint32_t
oce_read_db(struct oce_softc *sc, bus_size_t off)
{
	bus_space_barrier(sc->sc_db_iot, sc->sc_db_ioh, off, 4,
	    BUS_SPACE_BARRIER_READ);
	return (bus_space_read_4(sc->sc_db_iot, sc->sc_db_ioh, off));
}

static inline void
oce_write_cfg(struct oce_softc *sc, bus_size_t off, uint32_t val)
{
	bus_space_write_4(sc->sc_cfg_iot, sc->sc_cfg_ioh, off, val);
	bus_space_barrier(sc->sc_cfg_iot, sc->sc_cfg_ioh, off, 4,
	    BUS_SPACE_BARRIER_WRITE);
}

static inline void
oce_write_csr(struct oce_softc *sc, bus_size_t off, uint32_t val)
{
	bus_space_write_4(sc->sc_csr_iot, sc->sc_csr_ioh, off, val);
	bus_space_barrier(sc->sc_csr_iot, sc->sc_csr_ioh, off, 4,
	    BUS_SPACE_BARRIER_WRITE);
}

static inline void
oce_write_db(struct oce_softc *sc, bus_size_t off, uint32_t val)
{
	bus_space_write_4(sc->sc_db_iot, sc->sc_db_ioh, off, val);
	bus_space_barrier(sc->sc_db_iot, sc->sc_db_ioh, off, 4,
	    BUS_SPACE_BARRIER_WRITE);
}

static inline void
oce_intr_enable(struct oce_softc *sc)
{
	uint32_t reg;

	reg = oce_read_cfg(sc, PCI_INTR_CTRL);
	oce_write_cfg(sc, PCI_INTR_CTRL, reg | HOSTINTR_MASK);
}

static inline void
oce_intr_disable(struct oce_softc *sc)
{
	uint32_t reg;

	reg = oce_read_cfg(sc, PCI_INTR_CTRL);
	oce_write_cfg(sc, PCI_INTR_CTRL, reg & ~HOSTINTR_MASK);
}

void
oce_attachhook(struct device *self)
{
	struct oce_softc *sc = (struct oce_softc *)self;

	oce_get_link_status(sc);

	oce_arm_cq(sc->sc_mq->cq, 0, TRUE);

	/*
	 * We need to get MCC async events. So enable intrs and arm
	 * first EQ, Other EQs will be armed after interface is UP
	 */
	oce_intr_enable(sc);
	oce_arm_eq(sc->sc_eq[0], 0, TRUE, FALSE);

	/*
	 * Send first mcc cmd and after that we get gracious
	 * MCC notifications from FW
	 */
	oce_first_mcc(sc);
}

void
oce_attach_ifp(struct oce_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	ifmedia_init(&sc->sc_media, IFM_IMASK, oce_media_change,
	    oce_media_status);
	ifmedia_add(&sc->sc_media, IFM_ETHER | IFM_AUTO, 0, NULL);
	ifmedia_set(&sc->sc_media, IFM_ETHER | IFM_AUTO);

	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, IFNAMSIZ);
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = oce_ioctl;
	ifp->if_start = oce_start;
	ifp->if_watchdog = oce_watchdog;
	ifp->if_hardmtu = OCE_MAX_MTU;
	ifp->if_softc = sc;
	IFQ_SET_MAXLEN(&ifp->if_snd, sc->sc_tx_ring_size - 1);

	ifp->if_capabilities = IFCAP_VLAN_MTU | IFCAP_CSUM_IPv4 |
	    IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;

#if NVLAN > 0
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

#ifdef OCE_TSO
	ifp->if_capabilities |= IFCAP_TSO;
	ifp->if_capabilities |= IFCAP_VLAN_HWTSO;
#endif
#ifdef OCE_LRO
	ifp->if_capabilities |= IFCAP_LRO;
#endif

	if_attach(ifp);
	ether_ifattach(ifp);
}

int
oce_ioctl(struct ifnet *ifp, u_long command, caddr_t data)
{
	struct oce_softc *sc = ifp->if_softc;
	struct ifreq *ifr = (struct ifreq *)data;
	int s, error = 0;

	s = splnet();

	switch (command) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			oce_init(sc);
		break;
	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				oce_init(sc);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				oce_stop(sc);
		}
		break;
	case SIOCGIFMEDIA:
	case SIOCSIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &sc->sc_media, command);
		break;
	case SIOCGIFRXR:
		error = oce_rxrinfo(sc, (struct if_rxrinfo *)ifr->ifr_data);
		break;
	default:
		error = ether_ioctl(ifp, &sc->sc_ac, command, data);
		break;
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			oce_iff(sc);
		error = 0;
	}

	splx(s);

	return (error);
}

int
oce_rxrinfo(struct oce_softc *sc, struct if_rxrinfo *ifri)
{
	struct if_rxring_info *ifr, ifr1;
	struct oce_rq *rq;
	int error, i;
	u_int n = 0;

	if (sc->sc_nrq > 1) {
		if ((ifr = mallocarray(sc->sc_nrq, sizeof(*ifr), M_DEVBUF,
		    M_WAITOK | M_ZERO)) == NULL)
			return (ENOMEM);
	} else
		ifr = &ifr1;

	OCE_RQ_FOREACH(sc, rq, i) {
		ifr[n].ifr_size = MCLBYTES;
		snprintf(ifr[n].ifr_name, sizeof(ifr[n].ifr_name), "/%d", i);
		ifr[n].ifr_info = rq->rxring;
		n++;
	}

	error = if_rxr_info_ioctl(ifri, sc->sc_nrq, ifr);

	if (sc->sc_nrq > 1)
		free(ifr, M_DEVBUF, sc->sc_nrq * sizeof(*ifr));
	return (error);
}


void
oce_iff(struct oce_softc *sc)
{
	uint8_t multi[OCE_MAX_MC_FILTER_SIZE][ETHER_ADDR_LEN];
	struct arpcom *ac = &sc->sc_ac;
	struct ifnet *ifp = &ac->ac_if;
	struct ether_multi *enm;
	struct ether_multistep step;
	int naddr = 0, promisc = 0;

	ifp->if_flags &= ~IFF_ALLMULTI;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0 ||
	    ac->ac_multicnt >= OCE_MAX_MC_FILTER_SIZE) {
		ifp->if_flags |= IFF_ALLMULTI;
		promisc = 1;
	} else {
		ETHER_FIRST_MULTI(step, &sc->sc_ac, enm);
		while (enm != NULL) {
			memcpy(multi[naddr++], enm->enm_addrlo, ETHER_ADDR_LEN);
			ETHER_NEXT_MULTI(step, enm);
		}
		oce_update_mcast(sc, multi, naddr);
	}

	oce_set_promisc(sc, promisc);
}

void
oce_link_status(struct oce_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int link_state = LINK_STATE_DOWN;

	ifp->if_baudrate = 0;
	if (sc->sc_link_up) {
		link_state = LINK_STATE_FULL_DUPLEX;

		switch (sc->sc_link_speed) {
		case 1:
			ifp->if_baudrate = IF_Mbps(10);
			break;
		case 2:
			ifp->if_baudrate = IF_Mbps(100);
			break;
		case 3:
			ifp->if_baudrate = IF_Gbps(1);
			break;
		case 4:
			ifp->if_baudrate = IF_Gbps(10);
			break;
		}
	}
	if (ifp->if_link_state != link_state) {
		ifp->if_link_state = link_state;
		if_link_state_change(ifp);
	}
}

void
oce_media_status(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct oce_softc *sc = ifp->if_softc;

	ifmr->ifm_status = IFM_AVALID;
	ifmr->ifm_active = IFM_ETHER;

	if (oce_get_link_status(sc) == 0)
		oce_link_status(sc);

	if (!sc->sc_link_up) {
		ifmr->ifm_active |= IFM_NONE;
		return;
	}

	ifmr->ifm_status |= IFM_ACTIVE;

	switch (sc->sc_link_speed) {
	case 1: /* 10 Mbps */
		ifmr->ifm_active |= IFM_10_T | IFM_FDX;
		break;
	case 2: /* 100 Mbps */
		ifmr->ifm_active |= IFM_100_TX | IFM_FDX;
		break;
	case 3: /* 1 Gbps */
		ifmr->ifm_active |= IFM_1000_T | IFM_FDX;
		break;
	case 4: /* 10 Gbps */
		ifmr->ifm_active |= IFM_10G_SR | IFM_FDX;
		break;
	}

	if (sc->sc_fc & IFM_ETH_RXPAUSE)
		ifmr->ifm_active |= IFM_FLOW | IFM_ETH_RXPAUSE;
	if (sc->sc_fc & IFM_ETH_TXPAUSE)
		ifmr->ifm_active |= IFM_FLOW | IFM_ETH_TXPAUSE;
}

int
oce_media_change(struct ifnet *ifp)
{
	return (0);
}

void
oce_tick(void *arg)
{
	struct oce_softc *sc = arg;
	int s;

	s = splnet();

	if (oce_update_stats(sc) == 0)
		timeout_add_sec(&sc->sc_tick, 1);

	splx(s);
}

void
oce_init(void *arg)
{
	struct oce_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct oce_eq *eq;
	struct oce_rq *rq;
	struct oce_wq *wq;
	int i;

	oce_stop(sc);

	DELAY(10);

	oce_macaddr_set(sc);

	oce_iff(sc);

	/* Enable VLAN promiscuous mode */
	if (oce_config_vlan(sc, NULL, 0, 1, 1))
		goto error;

	if (oce_set_flow_control(sc, IFM_ETH_RXPAUSE | IFM_ETH_TXPAUSE))
		goto error;

	OCE_RQ_FOREACH(sc, rq, i) {
		rq->mtu = ifp->if_hardmtu + ETHER_HDR_LEN + ETHER_CRC_LEN +
		    ETHER_VLAN_ENCAP_LEN;
		if (oce_new_rq(sc, rq)) {
			printf("%s: failed to create rq\n",
			    sc->sc_dev.dv_xname);
			goto error;
		}
		rq->ring->index	 = 0;

		/* oce splits jumbos into 2k chunks... */
		if_rxr_init(&rq->rxring, 8, rq->nitems);

		if (!oce_alloc_rx_bufs(rq)) {
			printf("%s: failed to allocate rx buffers\n",
			    sc->sc_dev.dv_xname);
			goto error;
		}
	}

#ifdef OCE_RSS
	/* RSS config */
	if (sc->sc_rss_enable) {
		if (oce_config_rss(sc, (uint8_t)sc->sc_if_id, 1)) {
			printf("%s: failed to configure RSS\n",
			    sc->sc_dev.dv_xname);
			goto error;
		}
	}
#endif

	OCE_RQ_FOREACH(sc, rq, i)
		oce_arm_cq(rq->cq, 0, TRUE);

	OCE_WQ_FOREACH(sc, wq, i)
		oce_arm_cq(wq->cq, 0, TRUE);

	oce_arm_cq(sc->sc_mq->cq, 0, TRUE);

	OCE_EQ_FOREACH(sc, eq, i)
		oce_arm_eq(eq, 0, TRUE, FALSE);

	if (oce_get_link_status(sc) == 0)
		oce_link_status(sc);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	timeout_add_sec(&sc->sc_tick, 1);

	oce_intr_enable(sc);

	return;
error:
	oce_stop(sc);
}

void
oce_stop(struct oce_softc *sc)
{
	struct mbx_delete_nic_rq cmd;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct oce_rq *rq;
	struct oce_wq *wq;
	struct oce_eq *eq;
	int i;

	timeout_del(&sc->sc_tick);
	timeout_del(&sc->sc_rxrefill);

	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	/* Stop intrs and finish any bottom halves pending */
	oce_intr_disable(sc);

	/* Invalidate any pending cq and eq entries */
	OCE_EQ_FOREACH(sc, eq, i)
		oce_drain_eq(eq);
	OCE_RQ_FOREACH(sc, rq, i) {
		/* destroy the work queue in the firmware */
		memset(&cmd, 0, sizeof(cmd));
		cmd.params.req.rq_id = htole16(rq->id);
		oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_DELETE_RQ,
		    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
		DELAY(1000);
		oce_drain_rq(rq);
		oce_free_posted_rxbuf(rq);
	}
	OCE_WQ_FOREACH(sc, wq, i)
		oce_drain_wq(wq);
}

void
oce_watchdog(struct ifnet *ifp)
{
	printf("%s: watchdog timeout -- resetting\n", ifp->if_xname);

	oce_init(ifp->if_softc);

	ifp->if_oerrors++;
}

void
oce_start(struct ifnet *ifp)
{
	struct oce_softc *sc = ifp->if_softc;
	struct mbuf *m;
	int pkts = 0;

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;

	for (;;) {
		IFQ_DEQUEUE(&ifp->if_snd, m);
		if (m == NULL)
			break;

		if (oce_encap(sc, &m, 0)) {
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
		pkts++;
	}

	/* Set a timeout in case the chip goes out to lunch */
	if (pkts)
		ifp->if_timer = 5;
}

int
oce_encap(struct oce_softc *sc, struct mbuf **mpp, int wqidx)
{
	struct mbuf *m = *mpp;
	struct oce_wq *wq = sc->sc_wq[wqidx];
	struct oce_pkt *pkt = NULL;
	struct oce_nic_hdr_wqe *nhe;
	struct oce_nic_frag_wqe *nfe;
	int i, nwqe, err;

#ifdef OCE_TSO
	if (m->m_pkthdr.csum_flags & CSUM_TSO) {
		/* consolidate packet buffers for TSO/LSO segment offload */
		m = oce_tso(sc, mpp);
		if (m == NULL)
			goto error;
	}
#endif

	if ((pkt = oce_pkt_get(&wq->pkt_free)) == NULL)
		goto error;

	err = bus_dmamap_load_mbuf(sc->sc_dmat, pkt->map, m, BUS_DMA_NOWAIT);
	if (err == EFBIG) {
		if (m_defrag(m, M_DONTWAIT) ||
		    bus_dmamap_load_mbuf(sc->sc_dmat, pkt->map, m,
			BUS_DMA_NOWAIT))
			goto error;
		*mpp = m;
	} else if (err != 0)
		goto error;

	pkt->nsegs = pkt->map->dm_nsegs;

	nwqe = pkt->nsegs + 1;
	if (IS_BE(sc)) {
		/* BE2 and BE3 require even number of WQEs */
		if (nwqe & 1)
			nwqe++;
	}

	/* Fail if there's not enough free WQEs */
	if (nwqe >= wq->ring->nitems - wq->ring->nused) {
		bus_dmamap_unload(sc->sc_dmat, pkt->map);
		goto error;
	}

	bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);
	pkt->mbuf = m;

	/* TX work queue entry for the header */
	nhe = oce_ring_get(wq->ring);
	memset(nhe, 0, sizeof(*nhe));

	nhe->u0.s.complete = 1;
	nhe->u0.s.event = 1;
	nhe->u0.s.crc = 1;
	nhe->u0.s.forward = 0;
	nhe->u0.s.ipcs = (m->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT) ? 1 : 0;
	nhe->u0.s.udpcs = (m->m_pkthdr.csum_flags & M_UDP_CSUM_OUT) ? 1 : 0;
	nhe->u0.s.tcpcs = (m->m_pkthdr.csum_flags & M_TCP_CSUM_OUT) ? 1 : 0;
	nhe->u0.s.num_wqe = nwqe;
	nhe->u0.s.total_length = m->m_pkthdr.len;

#if NVLAN > 0
	if (m->m_flags & M_VLANTAG) {
		nhe->u0.s.vlan = 1; /* Vlan present */
		nhe->u0.s.vlan_tag = m->m_pkthdr.ether_vtag;
	}
#endif

#ifdef OCE_TSO
	if (m->m_pkthdr.csum_flags & CSUM_TSO) {
		if (m->m_pkthdr.tso_segsz) {
			nhe->u0.s.lso = 1;
			nhe->u0.s.lso_mss  = m->m_pkthdr.tso_segsz;
		}
		if (!IS_BE(sc))
			nhe->u0.s.ipcs = 1;
	}
#endif

	oce_dma_sync(&wq->ring->dma, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);

	wq->ring->nused++;

	/* TX work queue entries for data chunks */
	for (i = 0; i < pkt->nsegs; i++) {
		nfe = oce_ring_get(wq->ring);
		memset(nfe, 0, sizeof(*nfe));
		nfe->u0.s.frag_pa_hi = ADDR_HI(pkt->map->dm_segs[i].ds_addr);
		nfe->u0.s.frag_pa_lo = ADDR_LO(pkt->map->dm_segs[i].ds_addr);
		nfe->u0.s.frag_len = pkt->map->dm_segs[i].ds_len;
		wq->ring->nused++;
	}
	if (nwqe > (pkt->nsegs + 1)) {
		nfe = oce_ring_get(wq->ring);
		memset(nfe, 0, sizeof(*nfe));
		wq->ring->nused++;
		pkt->nsegs++;
	}

	oce_pkt_put(&wq->pkt_list, pkt);

	oce_dma_sync(&wq->ring->dma, BUS_DMASYNC_POSTREAD |
	    BUS_DMASYNC_POSTWRITE);

	oce_write_db(sc, PD_TXULP_DB, wq->id | (nwqe << 16));

	return (0);

error:
	if (pkt)
		oce_pkt_put(&wq->pkt_free, pkt);
	m_freem(*mpp);
	*mpp = NULL;
	return (1);
}

#ifdef OCE_TSO
struct mbuf *
oce_tso(struct oce_softc *sc, struct mbuf **mpp)
{
	struct mbuf *m;
	struct ip *ip;
#ifdef INET6
	struct ip6_hdr *ip6;
#endif
	struct ether_vlan_header *eh;
	struct tcphdr *th;
	uint16_t etype;
	int total_len = 0, ehdrlen = 0;

	m = *mpp;

	if (M_WRITABLE(m) == 0) {
		m = m_dup(*mpp, M_DONTWAIT);
		if (!m)
			return (NULL);
		m_freem(*mpp);
		*mpp = m;
	}

	eh = mtod(m, struct ether_vlan_header *);
	if (eh->evl_encap_proto == htons(ETHERTYPE_VLAN)) {
		etype = ntohs(eh->evl_proto);
		ehdrlen = ETHER_HDR_LEN + ETHER_VLAN_ENCAP_LEN;
	} else {
		etype = ntohs(eh->evl_encap_proto);
		ehdrlen = ETHER_HDR_LEN;
	}

	switch (etype) {
	case ETHERTYPE_IP:
		ip = (struct ip *)(m->m_data + ehdrlen);
		if (ip->ip_p != IPPROTO_TCP)
			return (NULL);
		th = (struct tcphdr *)((caddr_t)ip + (ip->ip_hl << 2));

		total_len = ehdrlen + (ip->ip_hl << 2) + (th->th_off << 2);
		break;
#ifdef INET6
	case ETHERTYPE_IPV6:
		ip6 = (struct ip6_hdr *)(m->m_data + ehdrlen);
		if (ip6->ip6_nxt != IPPROTO_TCP)
			return NULL;
		th = (struct tcphdr *)((caddr_t)ip6 + sizeof(struct ip6_hdr));

		total_len = ehdrlen + sizeof(struct ip6_hdr) +
		    (th->th_off << 2);
		break;
#endif
	default:
		return (NULL);
	}

	m = m_pullup(m, total_len);
	if (!m)
		return (NULL);
	*mpp = m;
	return (m);

}
#endif /* OCE_TSO */

int
oce_intr(void *arg)
{
	struct oce_softc *sc = arg;
	struct oce_eq *eq = sc->sc_eq[0];
	struct oce_eqe *eqe;
	struct oce_cq *cq = NULL;
	int i, neqe = 0;

	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTREAD);

	OCE_RING_FOREACH(eq->ring, eqe, eqe->evnt != 0) {
		eqe->evnt = 0;
		neqe++;
	}

	/* Spurious? */
	if (!neqe) {
		oce_arm_eq(eq, 0, TRUE, FALSE);
		return (0);
	}

	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_PREWRITE);

 	/* Clear EQ entries, but dont arm */
	oce_arm_eq(eq, neqe, FALSE, TRUE);

	/* Process TX, RX and MCC completion queues */
	for (i = 0; i < eq->cq_valid; i++) {
		cq = eq->cq[i];
		(*cq->cq_intr)(cq->cb_arg);
		oce_arm_cq(cq, 0, TRUE);
	}

	oce_arm_eq(eq, 0, TRUE, FALSE);
	return (1);
}

/* Handle the Completion Queue for transmit */
void
oce_intr_wq(void *arg)
{
	struct oce_wq *wq = (struct oce_wq *)arg;
	struct oce_cq *cq = wq->cq;
	struct oce_nic_tx_cqe *cqe;
	struct oce_softc *sc = wq->sc;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, WQ_CQE_VALID(cqe)) {
		oce_txeof(wq);
		WQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);

	if (ifq_is_oactive(&ifp->if_snd)) {
		if (wq->ring->nused < (wq->ring->nitems / 2)) {
			ifq_clr_oactive(&ifp->if_snd);
			oce_start(ifp);
		}
	}
	if (wq->ring->nused == 0)
		ifp->if_timer = 0;

	if (ncqe)
		oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_txeof(struct oce_wq *wq)
{
	struct oce_softc *sc = wq->sc;
	struct oce_pkt *pkt;
	struct mbuf *m;

	if ((pkt = oce_pkt_get(&wq->pkt_list)) == NULL) {
		printf("%s: missing descriptor in txeof\n",
		    sc->sc_dev.dv_xname);
		return;
	}

	wq->ring->nused -= pkt->nsegs + 1;
	bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
	    BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(sc->sc_dmat, pkt->map);

	m = pkt->mbuf;
	m_freem(m);
	pkt->mbuf = NULL;
	oce_pkt_put(&wq->pkt_free, pkt);
}

/* Handle the Completion Queue for receive */
void
oce_intr_rq(void *arg)
{
	struct oce_rq *rq = (struct oce_rq *)arg;
	struct oce_cq *cq = rq->cq;
	struct oce_softc *sc = rq->sc;
	struct oce_nic_rx_cqe *cqe;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int maxrx, ncqe = 0;

	maxrx = IS_XE201(sc) ? 8 : OCE_MAX_RQ_COMPL;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);

	OCE_RING_FOREACH(cq->ring, cqe, RQ_CQE_VALID(cqe) && ncqe <= maxrx) {
		if (cqe->u0.s.error == 0) {
			if (cqe->u0.s.pkt_size == 0)
				/* partial DMA workaround for Lancer */
				oce_rxeoc(rq, cqe);
			else
				oce_rxeof(rq, cqe);
		} else {
			ifp->if_ierrors++;
			if (IS_XE201(sc))
				/* Lancer A0 no buffer workaround */
				oce_rxeoc(rq, cqe);
			else
				/* Post L3/L4 errors to stack.*/
				oce_rxeof(rq, cqe);
		}
#ifdef OCE_LRO
		if (IF_LRO_ENABLED(ifp) && rq->lro_pkts_queued >= 16)
			oce_flush_lro(rq);
#endif
		RQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);

#ifdef OCE_LRO
	if (IF_LRO_ENABLED(ifp))
		oce_flush_lro(rq);
#endif

	if (ncqe) {
		oce_arm_cq(cq, ncqe, FALSE);
		if (!oce_alloc_rx_bufs(rq))
			timeout_add(&sc->sc_rxrefill, 1);
	}
}

void
oce_rxeof(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe)
{
	struct oce_softc *sc = rq->sc;
	struct oce_pkt *pkt = NULL;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *m = NULL, *tail = NULL;
	int i, len, frag_len;
	uint16_t vtag;

	len = cqe->u0.s.pkt_size;

	 /* Get vlan_tag value */
	if (IS_BE(sc))
		vtag = ntohs(cqe->u0.s.vlan_tag);
	else
		vtag = cqe->u0.s.vlan_tag;

	for (i = 0; i < cqe->u0.s.num_fragments; i++) {
		if ((pkt = oce_pkt_get(&rq->pkt_list)) == NULL) {
			printf("%s: missing descriptor in rxeof\n",
			    sc->sc_dev.dv_xname);
			goto exit;
		}

		bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, pkt->map);
		if_rxr_put(&rq->rxring, 1);

		frag_len = (len > rq->fragsize) ? rq->fragsize : len;
		pkt->mbuf->m_len = frag_len;

		if (tail != NULL) {
			/* additional fragments */
			pkt->mbuf->m_flags &= ~M_PKTHDR;
			tail->m_next = pkt->mbuf;
			tail = pkt->mbuf;
		} else {
			/* first fragment, fill out most of the header */
			pkt->mbuf->m_pkthdr.len = len;
			pkt->mbuf->m_pkthdr.csum_flags = 0;
			if (cqe->u0.s.ip_cksum_pass) {
				if (!cqe->u0.s.ip_ver) { /* IPV4 */
					pkt->mbuf->m_pkthdr.csum_flags =
					    M_IPV4_CSUM_IN_OK;
				}
			}
			if (cqe->u0.s.l4_cksum_pass) {
				pkt->mbuf->m_pkthdr.csum_flags |=
				    M_TCP_CSUM_IN_OK | M_UDP_CSUM_IN_OK;
			}
			m = tail = pkt->mbuf;
		}
		pkt->mbuf = NULL;
		oce_pkt_put(&rq->pkt_free, pkt);
		len -= frag_len;
	}

	if (m) {
		if (!oce_port_valid(sc, cqe)) {
			 m_freem(m);
			 goto exit;
		}

#if NVLAN > 0
		/* This determines if vlan tag is valid */
		if (oce_vtp_valid(sc, cqe)) {
			if (sc->sc_fmode & FNM_FLEX10_MODE) {
				/* FLEX10. If QnQ is not set, neglect VLAN */
				if (cqe->u0.s.qnq) {
					m->m_pkthdr.ether_vtag = vtag;
					m->m_flags |= M_VLANTAG;
				}
			} else if (sc->sc_pvid != (vtag & VLAN_VID_MASK))  {
				/*
				 * In UMC mode generally pvid will be striped.
				 * But in some cases we have seen it comes
				 * with pvid. So if pvid == vlan, neglect vlan.
				 */
				m->m_pkthdr.ether_vtag = vtag;
				m->m_flags |= M_VLANTAG;
			}
		}
#endif

#ifdef OCE_LRO
		/* Try to queue to LRO */
		if (IF_LRO_ENABLED(ifp) && !(m->m_flags & M_VLANTAG) &&
		    cqe->u0.s.ip_cksum_pass && cqe->u0.s.l4_cksum_pass &&
		    !cqe->u0.s.ip_ver && rq->lro.lro_cnt != 0) {

			if (tcp_lro_rx(&rq->lro, m, 0) == 0) {
				rq->lro_pkts_queued ++;
				goto exit;
			}
			/* If LRO posting fails then try to post to STACK */
		}
#endif

		ml_enqueue(&ml, m);
	}
exit:
	if_input(ifp, &ml);
}

void
oce_rxeoc(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe)
{
	struct oce_softc *sc = rq->sc;
	struct oce_pkt *pkt;
	int i, num_frags = cqe->u0.s.num_fragments;

	if (IS_XE201(sc) && cqe->u0.s.error) {
		/*
		 * Lancer A0 workaround:
		 * num_frags will be 1 more than actual in case of error
		 */
		if (num_frags)
			num_frags--;
	}
	for (i = 0; i < num_frags; i++) {
		if ((pkt = oce_pkt_get(&rq->pkt_list)) == NULL) {
			printf("%s: missing descriptor in rxeoc\n",
			    sc->sc_dev.dv_xname);
			return;
		}
		bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, pkt->map);
		if_rxr_put(&rq->rxring, 1);
		m_freem(pkt->mbuf);
		oce_pkt_put(&rq->pkt_free, pkt);
	}
}

int
oce_vtp_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe)
{
	struct oce_nic_rx_cqe_v1 *cqe_v1;

	if (IS_BE(sc) && ISSET(sc->sc_flags, OCE_F_BE3_NATIVE)) {
		cqe_v1 = (struct oce_nic_rx_cqe_v1 *)cqe;
		return (cqe_v1->u0.s.vlan_tag_present);
	}
	return (cqe->u0.s.vlan_tag_present);
}

int
oce_port_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe)
{
	struct oce_nic_rx_cqe_v1 *cqe_v1;

	if (IS_BE(sc) && ISSET(sc->sc_flags, OCE_F_BE3_NATIVE)) {
		cqe_v1 = (struct oce_nic_rx_cqe_v1 *)cqe;
		if (sc->sc_port != cqe_v1->u0.s.port)
			return (0);
	}
	return (1);
}

#ifdef OCE_LRO
void
oce_flush_lro(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct lro_ctrl	*lro = &rq->lro;
	struct lro_entry *queued;

	if (!IF_LRO_ENABLED(ifp))
		return;

	while ((queued = SLIST_FIRST(&lro->lro_active)) != NULL) {
		SLIST_REMOVE_HEAD(&lro->lro_active, next);
		tcp_lro_flush(lro, queued);
	}
	rq->lro_pkts_queued = 0;
}

int
oce_init_lro(struct oce_softc *sc)
{
	struct lro_ctrl *lro = NULL;
	int i = 0, rc = 0;

	for (i = 0; i < sc->sc_nrq; i++) {
		lro = &sc->sc_rq[i]->lro;
		rc = tcp_lro_init(lro);
		if (rc != 0) {
			printf("%s: LRO init failed\n",
			    sc->sc_dev.dv_xname);
			return rc;
		}
		lro->ifp = &sc->sc_ac.ac_if;
	}

	return (rc);
}

void
oce_free_lro(struct oce_softc *sc)
{
	struct lro_ctrl *lro = NULL;
	int i = 0;

	for (i = 0; i < sc->sc_nrq; i++) {
		lro = &sc->sc_rq[i]->lro;
		if (lro)
			tcp_lro_free(lro);
	}
}
#endif /* OCE_LRO */

int
oce_get_buf(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;
	struct oce_pkt *pkt;
	struct oce_nic_rqe *rqe;

	if ((pkt = oce_pkt_get(&rq->pkt_free)) == NULL)
		return (0);

	pkt->mbuf = MCLGETI(NULL, M_DONTWAIT, NULL, MCLBYTES);
	if (pkt->mbuf == NULL) {
		oce_pkt_put(&rq->pkt_free, pkt);
		return (0);
	}

	pkt->mbuf->m_len = pkt->mbuf->m_pkthdr.len = MCLBYTES;
#ifdef __STRICT_ALIGNMENT
	m_adj(pkt->mbuf, ETHER_ALIGN);
#endif

	if (bus_dmamap_load_mbuf(sc->sc_dmat, pkt->map, pkt->mbuf,
	    BUS_DMA_NOWAIT)) {
		m_freem(pkt->mbuf);
		pkt->mbuf = NULL;
		oce_pkt_put(&rq->pkt_free, pkt);
		return (0);
	}

	bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
	    BUS_DMASYNC_PREREAD);

	oce_dma_sync(&rq->ring->dma, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);

	rqe = oce_ring_get(rq->ring);
	rqe->u0.s.frag_pa_hi = ADDR_HI(pkt->map->dm_segs[0].ds_addr);
	rqe->u0.s.frag_pa_lo = ADDR_LO(pkt->map->dm_segs[0].ds_addr);

	oce_dma_sync(&rq->ring->dma, BUS_DMASYNC_POSTREAD |
	    BUS_DMASYNC_POSTWRITE);

	oce_pkt_put(&rq->pkt_list, pkt);

	return (1);
}

int
oce_alloc_rx_bufs(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;
	int i, nbufs = 0;
	u_int slots;

	for (slots = if_rxr_get(&rq->rxring, rq->nitems); slots > 0; slots--) {
		if (oce_get_buf(rq) == 0)
			break;

		nbufs++;
	}
	if_rxr_put(&rq->rxring, slots);

	if (!nbufs)
		return (0);
	for (i = nbufs / OCE_MAX_RQ_POSTS; i > 0; i--) {
		oce_write_db(sc, PD_RXULP_DB, rq->id |
		    (OCE_MAX_RQ_POSTS << 24));
		nbufs -= OCE_MAX_RQ_POSTS;
	}
	if (nbufs > 0)
		oce_write_db(sc, PD_RXULP_DB, rq->id | (nbufs << 24));
	return (1);
}

void
oce_refill_rx(void *arg)
{
	struct oce_softc *sc = arg;
	struct oce_rq *rq;
	int i, s;

	s = splnet();
	OCE_RQ_FOREACH(sc, rq, i) {
		if (!oce_alloc_rx_bufs(rq))
			timeout_add(&sc->sc_rxrefill, 5);
	}
	splx(s);
}

/* Handle the Completion Queue for the Mailbox/Async notifications */
void
oce_intr_mq(void *arg)
{
	struct oce_mq *mq = (struct oce_mq *)arg;
	struct oce_softc *sc = mq->sc;
	struct oce_cq *cq = mq->cq;
	struct oce_mq_cqe *cqe;
	struct oce_async_cqe_link_state *acqe;
	struct oce_async_event_grp5_pvid_state *gcqe;
	int evtype, optype, ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);

	OCE_RING_FOREACH(cq->ring, cqe, MQ_CQE_VALID(cqe)) {
		if (cqe->u0.s.async_event) {
			evtype = cqe->u0.s.event_type;
			optype = cqe->u0.s.async_type;
			if (evtype  == ASYNC_EVENT_CODE_LINK_STATE) {
				/* Link status evt */
				acqe = (struct oce_async_cqe_link_state *)cqe;
				oce_link_event(sc, acqe);
			} else if ((evtype == ASYNC_EVENT_GRP5) &&
				   (optype == ASYNC_EVENT_PVID_STATE)) {
				/* GRP5 PVID */
				gcqe =
				(struct oce_async_event_grp5_pvid_state *)cqe;
				if (gcqe->enabled)
					sc->sc_pvid =
					    gcqe->tag & VLAN_VID_MASK;
				else
					sc->sc_pvid = 0;
			}
		}
		MQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);

	if (ncqe)
		oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_link_event(struct oce_softc *sc, struct oce_async_cqe_link_state *acqe)
{
	/* Update Link status */
	sc->sc_link_up = ((acqe->u0.s.link_status & ~ASYNC_EVENT_LOGICAL) ==
	    ASYNC_EVENT_LINK_UP);
	/* Update speed */
	sc->sc_link_speed = acqe->u0.s.speed;
	oce_link_status(sc);
}

int
oce_init_queues(struct oce_softc *sc)
{
	struct oce_wq *wq;
	struct oce_rq *rq;
	int i;

	sc->sc_nrq = 1;
	sc->sc_nwq = 1;

	/* Create network interface on card */
	if (oce_create_iface(sc, sc->sc_macaddr))
		goto error;

	/* create all of the event queues */
	for (i = 0; i < sc->sc_nintr; i++) {
		sc->sc_eq[i] = oce_create_eq(sc);
		if (!sc->sc_eq[i])
			goto error;
	}

	/* alloc tx queues */
	OCE_WQ_FOREACH(sc, wq, i) {
		sc->sc_wq[i] = oce_create_wq(sc, sc->sc_eq[i]);
		if (!sc->sc_wq[i])
			goto error;
	}

	/* alloc rx queues */
	OCE_RQ_FOREACH(sc, rq, i) {
		sc->sc_rq[i] = oce_create_rq(sc, sc->sc_eq[i > 0 ? i - 1 : 0],
		    i > 0 ? sc->sc_rss_enable : 0);
		if (!sc->sc_rq[i])
			goto error;
	}

	/* alloc mailbox queue */
	sc->sc_mq = oce_create_mq(sc, sc->sc_eq[0]);
	if (!sc->sc_mq)
		goto error;

	return (0);
error:
	oce_release_queues(sc);
	return (1);
}

void
oce_release_queues(struct oce_softc *sc)
{
	struct oce_wq *wq;
	struct oce_rq *rq;
	struct oce_eq *eq;
	int i;

	OCE_RQ_FOREACH(sc, rq, i) {
		if (rq)
			oce_destroy_rq(sc->sc_rq[i]);
	}

	OCE_WQ_FOREACH(sc, wq, i) {
		if (wq)
			oce_destroy_wq(sc->sc_wq[i]);
	}

	if (sc->sc_mq)
		oce_destroy_mq(sc->sc_mq);

	OCE_EQ_FOREACH(sc, eq, i) {
		if (eq)
			oce_destroy_eq(sc->sc_eq[i]);
	}
}

/**
 * @@brief 		Function to create a WQ for NIC Tx
 * @@param sc 		software handle to the device
 * @@returns		the pointer to the WQ created or NULL on failure
 */
struct oce_wq *
oce_create_wq(struct oce_softc *sc, struct oce_eq *eq)
{
	struct oce_wq *wq;
	struct oce_cq *cq;
	struct oce_pkt *pkt;
	int i;

	if (sc->sc_tx_ring_size < 256 || sc->sc_tx_ring_size > 2048)
		return (NULL);

	wq = malloc(sizeof(struct oce_wq), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (!wq)
		return (NULL);

	wq->ring = oce_create_ring(sc, sc->sc_tx_ring_size, NIC_WQE_SIZE, 8);
	if (!wq->ring) {
		free(wq, M_DEVBUF, 0);
		return (NULL);
	}

	cq = oce_create_cq(sc, eq, CQ_LEN_512, sizeof(struct oce_nic_tx_cqe),
	    1, 0, 3);
	if (!cq) {
		oce_destroy_ring(sc, wq->ring);
		free(wq, M_DEVBUF, 0);
		return (NULL);
	}

	wq->id = -1;
	wq->sc = sc;

	wq->cq = cq;
	wq->nitems = sc->sc_tx_ring_size;

	SIMPLEQ_INIT(&wq->pkt_free);
	SIMPLEQ_INIT(&wq->pkt_list);

	for (i = 0; i < sc->sc_tx_ring_size / 2; i++) {
		pkt = oce_pkt_alloc(sc, OCE_MAX_TX_SIZE, OCE_MAX_TX_ELEMENTS,
		    PAGE_SIZE);
		if (pkt == NULL) {
			oce_destroy_wq(wq);
			return (NULL);
		}
		oce_pkt_put(&wq->pkt_free, pkt);
	}

	if (oce_new_wq(sc, wq)) {
		oce_destroy_wq(wq);
		return (NULL);
	}

	eq->cq[eq->cq_valid] = cq;
	eq->cq_valid++;
	cq->cb_arg = wq;
	cq->cq_intr = oce_intr_wq;

	return (wq);
}

void
oce_drain_wq(struct oce_wq *wq)
{
	struct oce_cq *cq = wq->cq;
	struct oce_nic_tx_cqe *cqe;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, WQ_CQE_VALID(cqe)) {
		WQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_destroy_wq(struct oce_wq *wq)
{
	struct mbx_delete_nic_wq cmd;
	struct oce_softc *sc = wq->sc;
	struct oce_pkt *pkt;

	if (wq->id >= 0) {
		memset(&cmd, 0, sizeof(cmd));
		cmd.params.req.wq_id = htole16(wq->id);
		oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_DELETE_WQ, OCE_MBX_VER_V0,
		    &cmd, sizeof(cmd));
	}
	if (wq->cq != NULL)
		oce_destroy_cq(wq->cq);
	if (wq->ring != NULL)
		oce_destroy_ring(sc, wq->ring);
	while ((pkt = oce_pkt_get(&wq->pkt_free)) != NULL)
		oce_pkt_free(sc, pkt);
	free(wq, M_DEVBUF, 0);
}

/**
 * @@brief 		function to allocate receive queue resources
 * @@param sc		software handle to the device
 * @@param eq		pointer to associated event queue
 * @@param rss		is-rss-queue flag
 * @@returns		the pointer to the RQ created or NULL on failure
 */
struct oce_rq *
oce_create_rq(struct oce_softc *sc, struct oce_eq *eq, int rss)
{
	struct oce_rq *rq;
	struct oce_cq *cq;
	struct oce_pkt *pkt;
	int i;

	/* Hardware doesn't support any other value */
	if (sc->sc_rx_ring_size != 1024)
		return (NULL);

	rq = malloc(sizeof(struct oce_rq), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (!rq)
		return (NULL);

	rq->ring = oce_create_ring(sc, sc->sc_rx_ring_size,
	    sizeof(struct oce_nic_rqe), 2);
	if (!rq->ring) {
		free(rq, M_DEVBUF, 0);
		return (NULL);
	}

	cq = oce_create_cq(sc, eq, CQ_LEN_1024, sizeof(struct oce_nic_rx_cqe),
	    1, 0, 3);
	if (!cq) {
		oce_destroy_ring(sc, rq->ring);
		free(rq, M_DEVBUF, 0);
		return (NULL);
	}

	rq->id = -1;
	rq->sc = sc;

	rq->nitems = sc->sc_rx_ring_size;
	rq->fragsize = OCE_RX_BUF_SIZE;
	rq->rss = rss;

	SIMPLEQ_INIT(&rq->pkt_free);
	SIMPLEQ_INIT(&rq->pkt_list);

	for (i = 0; i < sc->sc_rx_ring_size; i++) {
		pkt = oce_pkt_alloc(sc, OCE_RX_BUF_SIZE, 1, OCE_RX_BUF_SIZE);
		if (pkt == NULL) {
			oce_destroy_rq(rq);
			return (NULL);
		}
		oce_pkt_put(&rq->pkt_free, pkt);
	}

	rq->cq = cq;
	eq->cq[eq->cq_valid] = cq;
	eq->cq_valid++;
	cq->cb_arg = rq;
	cq->cq_intr = oce_intr_rq;

	/* RX queue is created in oce_init */

	return (rq);
}

void
oce_drain_rq(struct oce_rq *rq)
{
	struct oce_nic_rx_cqe *cqe;
	struct oce_cq *cq = rq->cq;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, RQ_CQE_VALID(cqe)) {
		RQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_destroy_rq(struct oce_rq *rq)
{
	struct mbx_delete_nic_rq cmd;
	struct oce_softc *sc = rq->sc;
	struct oce_pkt *pkt;

	if (rq->id >= 0) {
		memset(&cmd, 0, sizeof(cmd));
		cmd.params.req.rq_id = htole16(rq->id);
		oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_DELETE_RQ, OCE_MBX_VER_V0,
		    &cmd, sizeof(cmd));
	}
	if (rq->cq != NULL)
		oce_destroy_cq(rq->cq);
	if (rq->ring != NULL)
		oce_destroy_ring(sc, rq->ring);
	while ((pkt = oce_pkt_get(&rq->pkt_free)) != NULL)
		oce_pkt_free(sc, pkt);
	free(rq, M_DEVBUF, 0);
}

struct oce_eq *
oce_create_eq(struct oce_softc *sc)
{
	struct oce_eq *eq;

	/* allocate an eq */
	eq = malloc(sizeof(struct oce_eq), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (eq == NULL)
		return (NULL);

	eq->ring = oce_create_ring(sc, EQ_LEN_1024, EQE_SIZE_4, 8);
	if (!eq->ring) {
		free(eq, M_DEVBUF, 0);
		return (NULL);
	}

	eq->id = -1;
	eq->sc = sc;
	eq->nitems = EQ_LEN_1024;	/* length of event queue */
	eq->isize = EQE_SIZE_4; 	/* size of a queue item */
	eq->delay = OCE_DEFAULT_EQD;	/* event queue delay */

	if (oce_new_eq(sc, eq)) {
		oce_destroy_ring(sc, eq->ring);
		free(eq, M_DEVBUF, 0);
		return (NULL);
	}

	return (eq);
}

/**
 * @@brief		Function to arm an EQ so that it can generate events
 * @@param eq		pointer to event queue structure
 * @@param neqe		number of EQEs to arm
 * @@param rearm		rearm bit enable/disable
 * @@param clearint	bit to clear the interrupt condition because of which
 *			EQEs are generated
 */
static inline void
oce_arm_eq(struct oce_eq *eq, int neqe, int rearm, int clearint)
{
	oce_write_db(eq->sc, PD_EQ_DB, eq->id | PD_EQ_DB_EVENT |
	    (clearint << 9) | (neqe << 16) | (rearm << 29));
}

void
oce_drain_eq(struct oce_eq *eq)
{
	struct oce_eqe *eqe;
	int neqe = 0;

	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(eq->ring, eqe, eqe->evnt != 0) {
		eqe->evnt = 0;
		neqe++;
	}
	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_eq(eq, neqe, FALSE, TRUE);
}

void
oce_destroy_eq(struct oce_eq *eq)
{
	struct mbx_destroy_common_eq cmd;
	struct oce_softc *sc = eq->sc;

	if (eq->id >= 0) {
		memset(&cmd, 0, sizeof(cmd));
		cmd.params.req.id = htole16(eq->id);
		oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_DESTROY_EQ,
		    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	}
	if (eq->ring != NULL)
		oce_destroy_ring(sc, eq->ring);
	free(eq, M_DEVBUF, 0);
}

struct oce_mq *
oce_create_mq(struct oce_softc *sc, struct oce_eq *eq)
{
	struct oce_mq *mq = NULL;
	struct oce_cq *cq;

	/* allocate the mq */
	mq = malloc(sizeof(struct oce_mq), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (!mq)
		return (NULL);

	mq->ring = oce_create_ring(sc, 128, sizeof(struct oce_mbx), 8);
	if (!mq->ring) {
		free(mq, M_DEVBUF, 0);
		return (NULL);
	}

	cq = oce_create_cq(sc, eq, CQ_LEN_256, sizeof(struct oce_mq_cqe),
	    1, 0, 0);
	if (!cq) {
		oce_destroy_ring(sc, mq->ring);
		free(mq, M_DEVBUF, 0);
		return (NULL);
	}

	mq->id = -1;
	mq->sc = sc;
	mq->cq = cq;

	mq->nitems = 128;

	if (oce_new_mq(sc, mq)) {
		oce_destroy_cq(mq->cq);
		oce_destroy_ring(sc, mq->ring);
		free(mq, M_DEVBUF, 0);
		return (NULL);
	}

	eq->cq[eq->cq_valid] = cq;
	eq->cq_valid++;
	mq->cq->eq = eq;
	mq->cq->cb_arg = mq;
	mq->cq->cq_intr = oce_intr_mq;

	return (mq);
}

void
oce_drain_mq(struct oce_mq *mq)
{
	struct oce_cq *cq = mq->cq;
	struct oce_mq_cqe *cqe;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, MQ_CQE_VALID(cqe)) {
		MQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_destroy_mq(struct oce_mq *mq)
{
	struct mbx_destroy_common_mq cmd;
	struct oce_softc *sc = mq->sc;

	if (mq->id >= 0) {
		memset(&cmd, 0, sizeof(cmd));
		cmd.params.req.id = htole16(mq->id);
		oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_DESTROY_MQ,
		    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	}
	if (mq->ring != NULL)
		oce_destroy_ring(sc, mq->ring);
	if (mq->cq != NULL)
		oce_destroy_cq(mq->cq);
	free(mq, M_DEVBUF, 0);
}

/**
 * @@brief		Function to create a completion queue
 * @@param sc		software handle to the device
 * @@param eq		optional eq to be associated with to the cq
 * @@param nitems	length of completion queue
 * @@param isize		size of completion queue items
 * @@param eventable	event table
 * @@param nodelay	no delay flag
 * @@param ncoalesce	no coalescence flag
 * @@returns 		pointer to the cq created, NULL on failure
 */
struct oce_cq *
oce_create_cq(struct oce_softc *sc, struct oce_eq *eq, int nitems, int isize,
    int eventable, int nodelay, int ncoalesce)
{
	struct oce_cq *cq = NULL;

	cq = malloc(sizeof(struct oce_cq), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (!cq)
		return (NULL);

	cq->ring = oce_create_ring(sc, nitems, isize, 4);
	if (!cq->ring) {
		free(cq, M_DEVBUF, 0);
		return (NULL);
	}

	cq->sc = sc;
	cq->eq = eq;
	cq->nitems = nitems;
	cq->nodelay = nodelay;
	cq->ncoalesce = ncoalesce;
	cq->eventable = eventable;

	if (oce_new_cq(sc, cq)) {
		oce_destroy_ring(sc, cq->ring);
		free(cq, M_DEVBUF, 0);
		return (NULL);
	}

	sc->sc_cq[sc->sc_ncq++] = cq;

	return (cq);
}

void
oce_destroy_cq(struct oce_cq *cq)
{
	struct mbx_destroy_common_cq cmd;
	struct oce_softc *sc = cq->sc;

	if (cq->id >= 0) {
		memset(&cmd, 0, sizeof(cmd));
		cmd.params.req.id = htole16(cq->id);
		oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_DESTROY_CQ,
		    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	}
	if (cq->ring != NULL)
		oce_destroy_ring(sc, cq->ring);
	free(cq, M_DEVBUF, 0);
}

/**
 * @@brief		Function to arm a CQ with CQEs
 * @@param cq		pointer to the completion queue structure
 * @@param ncqe		number of CQEs to arm
 * @@param rearm		rearm bit enable/disable
 */
static inline void
oce_arm_cq(struct oce_cq *cq, int ncqe, int rearm)
{
	oce_write_db(cq->sc, PD_CQ_DB, cq->id | (ncqe << 16) | (rearm << 29));
}

void
oce_free_posted_rxbuf(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;
	struct oce_pkt *pkt;

	while ((pkt = oce_pkt_get(&rq->pkt_list)) != NULL) {
		bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, pkt->map);
		if (pkt->mbuf != NULL) {
			m_freem(pkt->mbuf);
			pkt->mbuf = NULL;
		}
		oce_pkt_put(&rq->pkt_free, pkt);
		if_rxr_put(&rq->rxring, 1);
	}
}

int
oce_dma_alloc(struct oce_softc *sc, bus_size_t size, struct oce_dma_mem *dma)
{
	int rc;

	memset(dma, 0, sizeof(struct oce_dma_mem));

	dma->tag = sc->sc_dmat;
	rc = bus_dmamap_create(dma->tag, size, 1, size, 0, BUS_DMA_NOWAIT,
	    &dma->map);
	if (rc != 0) {
		printf("%s: failed to allocate DMA handle",
		    sc->sc_dev.dv_xname);
		goto fail_0;
	}

	rc = bus_dmamem_alloc(dma->tag, size, PAGE_SIZE, 0, &dma->segs, 1,
	    &dma->nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (rc != 0) {
		printf("%s: failed to allocate DMA memory",
		    sc->sc_dev.dv_xname);
		goto fail_1;
	}

	rc = bus_dmamem_map(dma->tag, &dma->segs, dma->nsegs, size,
	    &dma->vaddr, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: failed to map DMA memory", sc->sc_dev.dv_xname);
		goto fail_2;
	}

	rc = bus_dmamap_load(dma->tag, dma->map, dma->vaddr, size, NULL,
	    BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: failed to load DMA memory", sc->sc_dev.dv_xname);
		goto fail_3;
	}

	bus_dmamap_sync(dma->tag, dma->map, 0, dma->map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	dma->paddr = dma->map->dm_segs[0].ds_addr;
	dma->size = size;

	return (0);

fail_3:
	bus_dmamem_unmap(dma->tag, dma->vaddr, size);
fail_2:
	bus_dmamem_free(dma->tag, &dma->segs, dma->nsegs);
fail_1:
	bus_dmamap_destroy(dma->tag, dma->map);
fail_0:
	return (rc);
}

void
oce_dma_free(struct oce_softc *sc, struct oce_dma_mem *dma)
{
	if (dma->tag == NULL)
		return;

	if (dma->map != NULL) {
		oce_dma_sync(dma, BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(dma->tag, dma->map);

		if (dma->vaddr != 0) {
			bus_dmamem_free(dma->tag, &dma->segs, dma->nsegs);
			dma->vaddr = 0;
		}

		bus_dmamap_destroy(dma->tag, dma->map);
		dma->map = NULL;
		dma->tag = NULL;
	}
}

struct oce_ring *
oce_create_ring(struct oce_softc *sc, int nitems, int isize, int maxsegs)
{
	struct oce_dma_mem *dma;
	struct oce_ring *ring;
	bus_size_t size = nitems * isize;
	int rc;

	if (size > maxsegs * PAGE_SIZE)
		return (NULL);

	ring = malloc(sizeof(struct oce_ring), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (ring == NULL)
		return (NULL);

	ring->isize = isize;
	ring->nitems = nitems;

	dma = &ring->dma;
	dma->tag = sc->sc_dmat;
	rc = bus_dmamap_create(dma->tag, size, maxsegs, PAGE_SIZE, 0,
	    BUS_DMA_NOWAIT, &dma->map);
	if (rc != 0) {
		printf("%s: failed to allocate DMA handle",
		    sc->sc_dev.dv_xname);
		goto fail_0;
	}

	rc = bus_dmamem_alloc(dma->tag, size, 0, 0, &dma->segs, maxsegs,
	    &dma->nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
	if (rc != 0) {
		printf("%s: failed to allocate DMA memory",
		    sc->sc_dev.dv_xname);
		goto fail_1;
	}

	rc = bus_dmamem_map(dma->tag, &dma->segs, dma->nsegs, size,
	    &dma->vaddr, BUS_DMA_NOWAIT);
	if (rc != 0) {
		printf("%s: failed to map DMA memory", sc->sc_dev.dv_xname);
		goto fail_2;
	}

	bus_dmamap_sync(dma->tag, dma->map, 0, dma->map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	dma->paddr = 0;
	dma->size = size;

	return (ring);

fail_2:
	bus_dmamem_free(dma->tag, &dma->segs, dma->nsegs);
fail_1:
	bus_dmamap_destroy(dma->tag, dma->map);
fail_0:
	free(ring, M_DEVBUF, 0);
	return (NULL);
}

void
oce_destroy_ring(struct oce_softc *sc, struct oce_ring *ring)
{
	oce_dma_free(sc, &ring->dma);
	free(ring, M_DEVBUF, 0);
}

int
oce_load_ring(struct oce_softc *sc, struct oce_ring *ring,
    struct oce_pa *pa, int maxsegs)
{
	struct oce_dma_mem *dma = &ring->dma;
	int i;

	if (bus_dmamap_load(dma->tag, dma->map, dma->vaddr,
	    ring->isize * ring->nitems, NULL, BUS_DMA_NOWAIT)) {
		printf("%s: failed to load a ring map\n", sc->sc_dev.dv_xname);
		return (0);
	}

	if (dma->map->dm_nsegs > maxsegs) {
		printf("%s: too many segments\n", sc->sc_dev.dv_xname);
		return (0);
	}

	bus_dmamap_sync(dma->tag, dma->map, 0, dma->map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	for (i = 0; i < dma->map->dm_nsegs; i++)
		pa[i].addr = dma->map->dm_segs[i].ds_addr;

	return (dma->map->dm_nsegs);
}

static inline void *
oce_ring_get(struct oce_ring *ring)
{
	int index = ring->index;

	if (++ring->index == ring->nitems)
		ring->index = 0;
	return ((void *)(ring->dma.vaddr + index * ring->isize));
}

static inline void *
oce_ring_first(struct oce_ring *ring)
{
	return ((void *)(ring->dma.vaddr + ring->index * ring->isize));
}

static inline void *
oce_ring_next(struct oce_ring *ring)
{
	if (++ring->index == ring->nitems)
		ring->index = 0;
	return ((void *)(ring->dma.vaddr + ring->index * ring->isize));
}

struct oce_pkt *
oce_pkt_alloc(struct oce_softc *sc, size_t size, int nsegs, int maxsegsz)
{
	struct oce_pkt *pkt;

	if ((pkt = pool_get(oce_pkt_pool, PR_NOWAIT | PR_ZERO)) == NULL)
		return (NULL);

	if (bus_dmamap_create(sc->sc_dmat, size, nsegs, maxsegsz, 0,
	    BUS_DMA_NOWAIT | BUS_DMA_ALLOCNOW, &pkt->map)) {
		pool_put(oce_pkt_pool, pkt);
		return (NULL);
	}

	return (pkt);
}

void
oce_pkt_free(struct oce_softc *sc, struct oce_pkt *pkt)
{
	if (pkt->map) {
		bus_dmamap_unload(sc->sc_dmat, pkt->map);
		bus_dmamap_destroy(sc->sc_dmat, pkt->map);
	}
	pool_put(oce_pkt_pool, pkt);
}

static inline struct oce_pkt *
oce_pkt_get(struct oce_pkt_list *lst)
{
	struct oce_pkt *pkt;

	pkt = SIMPLEQ_FIRST(lst);
	if (pkt == NULL)
		return (NULL);

	SIMPLEQ_REMOVE_HEAD(lst, entry);

	return (pkt);
}

static inline void
oce_pkt_put(struct oce_pkt_list *lst, struct oce_pkt *pkt)
{
	SIMPLEQ_INSERT_TAIL(lst, pkt, entry);
}

/**
 * @@brief Wait for FW to become ready and reset it
 * @@param sc		software handle to the device
 */
int
oce_init_fw(struct oce_softc *sc)
{
	struct ioctl_common_function_reset cmd;
	uint32_t reg;
	int err = 0, tmo = 60000;

	/* read semaphore CSR */
	reg = oce_read_csr(sc, MPU_EP_SEMAPHORE(sc));

	/* if host is ready then wait for fw ready else send POST */
	if ((reg & MPU_EP_SEM_STAGE_MASK) <= POST_STAGE_AWAITING_HOST_RDY) {
		reg = (reg & ~MPU_EP_SEM_STAGE_MASK) | POST_STAGE_CHIP_RESET;
		oce_write_csr(sc, MPU_EP_SEMAPHORE(sc), reg);
	}

	/* wait for FW to become ready */
	for (;;) {
		if (--tmo == 0)
			break;

		DELAY(1000);

		reg = oce_read_csr(sc, MPU_EP_SEMAPHORE(sc));
		if (reg & MPU_EP_SEM_ERROR) {
			printf(": POST failed: %#x\n", reg);
			return (ENXIO);
		}
		if ((reg & MPU_EP_SEM_STAGE_MASK) == POST_STAGE_ARMFW_READY) {
			/* reset FW */
			if (ISSET(sc->sc_flags, OCE_F_RESET_RQD)) {
				memset(&cmd, 0, sizeof(cmd));
				err = oce_cmd(sc, SUBSYS_COMMON,
				    OPCODE_COMMON_FUNCTION_RESET,
				    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
			}
			return (err);
		}
	}

	printf(": POST timed out: %#x\n", reg);

	return (ENXIO);
}

static inline int
oce_mbox_wait(struct oce_softc *sc)
{
	int i;

	for (i = 0; i < 20000; i++) {
		if (oce_read_db(sc, PD_MPU_MBOX_DB) & PD_MPU_MBOX_DB_READY)
			return (0);
		DELAY(100);
	}
	return (ETIMEDOUT);
}

/**
 * @@brief Mailbox dispatch
 * @@param sc		software handle to the device
 */
int
oce_mbox_dispatch(struct oce_softc *sc)
{
	uint32_t pa, reg;
	int err;

	pa = (uint32_t)((uint64_t)OCE_MEM_DVA(&sc->sc_mbx) >> 34);
	reg = PD_MPU_MBOX_DB_HI | (pa << PD_MPU_MBOX_DB_ADDR_SHIFT);

	if ((err = oce_mbox_wait(sc)) != 0)
		goto out;

	oce_write_db(sc, PD_MPU_MBOX_DB, reg);

	pa = (uint32_t)((uint64_t)OCE_MEM_DVA(&sc->sc_mbx) >> 4) & 0x3fffffff;
	reg = pa << PD_MPU_MBOX_DB_ADDR_SHIFT;

	if ((err = oce_mbox_wait(sc)) != 0)
		goto out;

	oce_write_db(sc, PD_MPU_MBOX_DB, reg);

	oce_dma_sync(&sc->sc_mbx, BUS_DMASYNC_POSTWRITE);

	if ((err = oce_mbox_wait(sc)) != 0)
		goto out;

out:
	oce_dma_sync(&sc->sc_mbx, BUS_DMASYNC_PREREAD);
	return (err);
}

/**
 * @@brief Function to initialize the hw with host endian information
 * @@param sc		software handle to the device
 * @@returns		0 on success, ETIMEDOUT on failure
 */
int
oce_mbox_init(struct oce_softc *sc)
{
	struct oce_bmbx *bmbx = OCE_MEM_KVA(&sc->sc_mbx);
	uint8_t *ptr = (uint8_t *)&bmbx->mbx;

	if (!ISSET(sc->sc_flags, OCE_F_MBOX_ENDIAN_RQD))
		return (0);

	/* Endian Signature */
	*ptr++ = 0xff;
	*ptr++ = 0x12;
	*ptr++ = 0x34;
	*ptr++ = 0xff;
	*ptr++ = 0xff;
	*ptr++ = 0x56;
	*ptr++ = 0x78;
	*ptr = 0xff;

	return (oce_mbox_dispatch(sc));
}

int
oce_cmd(struct oce_softc *sc, int subsys, int opcode, int version,
    void *payload, int length)
{
	struct oce_bmbx *bmbx = OCE_MEM_KVA(&sc->sc_mbx);
	struct oce_mbx *mbx = &bmbx->mbx;
	struct mbx_hdr *hdr;
	caddr_t epayload = NULL;
	int err;

	if (length > OCE_MBX_PAYLOAD)
		epayload = OCE_MEM_KVA(&sc->sc_pld);
	if (length > OCE_MAX_PAYLOAD)
		return (EINVAL);

	oce_dma_sync(&sc->sc_mbx, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	memset(mbx, 0, sizeof(struct oce_mbx));

	mbx->payload_length = length;

	if (epayload) {
		mbx->flags = OCE_MBX_F_SGE;
		oce_dma_sync(&sc->sc_pld, BUS_DMASYNC_PREREAD);
		memcpy(epayload, payload, length);
		mbx->pld.sgl[0].addr = OCE_MEM_DVA(&sc->sc_pld);
		mbx->pld.sgl[0].length = length;
		hdr = (struct mbx_hdr *)epayload;
	} else {
		mbx->flags = OCE_MBX_F_EMBED;
		memcpy(mbx->pld.data, payload, length);
		hdr = (struct mbx_hdr *)&mbx->pld.data;
	}

	hdr->subsys = subsys;
	hdr->opcode = opcode;
	hdr->version = version;
	hdr->length = length - sizeof(*hdr);
	if (opcode == OPCODE_COMMON_FUNCTION_RESET)
		hdr->timeout = 2 * OCE_MBX_TIMEOUT;
	else
		hdr->timeout = OCE_MBX_TIMEOUT;

	if (epayload)
		oce_dma_sync(&sc->sc_pld, BUS_DMASYNC_PREWRITE);

	err = oce_mbox_dispatch(sc);
	if (err == 0) {
		if (epayload) {
			oce_dma_sync(&sc->sc_pld, BUS_DMASYNC_POSTWRITE);
			memcpy(payload, epayload, length);
		} else
			memcpy(payload, &mbx->pld.data, length);
	} else
		printf("%s: mailbox timeout, subsys %d op %d ver %d "
		    "%spayload lenght %d\n", sc->sc_dev.dv_xname, subsys,
		    opcode, version, epayload ? "ext " : "",
		    length);
	return (err);
}

/**
 * @@brief	Firmware will send gracious notifications during
 *		attach only after sending first mcc commnad. We
 *		use MCC queue only for getting async and mailbox
 *		for sending cmds. So to get gracious notifications
 *		atleast send one dummy command on mcc.
 */
void
oce_first_mcc(struct oce_softc *sc)
{
	struct oce_mbx *mbx;
	struct oce_mq *mq = sc->sc_mq;
	struct mbx_hdr *hdr;
	struct mbx_get_common_fw_version *cmd;

	mbx = oce_ring_get(mq->ring);
	memset(mbx, 0, sizeof(struct oce_mbx));

	cmd = (struct mbx_get_common_fw_version *)&mbx->pld.data;

	hdr = &cmd->hdr;
	hdr->subsys = SUBSYS_COMMON;
	hdr->opcode = OPCODE_COMMON_GET_FW_VERSION;
	hdr->version = OCE_MBX_VER_V0;
	hdr->timeout = OCE_MBX_TIMEOUT;
	hdr->length = sizeof(*cmd) - sizeof(*hdr);

	mbx->flags = OCE_MBX_F_EMBED;
	mbx->payload_length = sizeof(*cmd);
	oce_dma_sync(&mq->ring->dma, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);
	oce_write_db(sc, PD_MQ_DB, mq->id | (1 << 16));
}

int
oce_get_fw_config(struct oce_softc *sc)
{
	struct mbx_common_query_fw_config cmd;
	int err;

	memset(&cmd, 0, sizeof(cmd));

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_QUERY_FIRMWARE_CONFIG,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	sc->sc_port = cmd.params.rsp.port_id;
	sc->sc_fmode = cmd.params.rsp.function_mode;

	return (0);
}

int
oce_check_native_mode(struct oce_softc *sc)
{
	struct mbx_common_set_function_cap cmd;
	int err;

	memset(&cmd, 0, sizeof(cmd));

	cmd.params.req.valid_capability_flags = CAP_SW_TIMESTAMPS |
	    CAP_BE3_NATIVE_ERX_API;
	cmd.params.req.capability_flags = CAP_BE3_NATIVE_ERX_API;

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_FUNCTIONAL_CAPS,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	if (cmd.params.rsp.capability_flags & CAP_BE3_NATIVE_ERX_API)
		SET(sc->sc_flags, OCE_F_BE3_NATIVE);

	return (0);
}

/**
 * @@brief Function for creating a network interface.
 * @@param sc		software handle to the device
 * @@returns		0 on success, error otherwise
 */
int
oce_create_iface(struct oce_softc *sc, uint8_t *macaddr)
{
	struct mbx_create_common_iface cmd;
	uint32_t caps, caps_en;
	int err = 0;

	/* interface capabilities to give device when creating interface */
	caps = MBX_RX_IFACE_BROADCAST | MBX_RX_IFACE_UNTAGGED |
	    MBX_RX_IFACE_PROMISC | MBX_RX_IFACE_MCAST_PROMISC |
	    MBX_RX_IFACE_RSS;

	/* capabilities to enable by default (others set dynamically) */
	caps_en = MBX_RX_IFACE_BROADCAST | MBX_RX_IFACE_UNTAGGED;

	if (!IS_XE201(sc)) {
		/* LANCER A0 workaround */
		caps |= MBX_RX_IFACE_PASS_L3L4_ERR;
		caps_en |= MBX_RX_IFACE_PASS_L3L4_ERR;
	}

	/* enable capabilities controlled via driver startup parameters */
	if (sc->sc_rss_enable)
		caps_en |= MBX_RX_IFACE_RSS;

	memset(&cmd, 0, sizeof(cmd));

	cmd.params.req.version = 0;
	cmd.params.req.cap_flags = htole32(caps);
	cmd.params.req.enable_flags = htole32(caps_en);
	if (macaddr != NULL) {
		memcpy(&cmd.params.req.mac_addr[0], macaddr, ETHER_ADDR_LEN);
		cmd.params.req.mac_invalid = 0;
	} else
		cmd.params.req.mac_invalid = 1;

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_CREATE_IFACE,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	sc->sc_if_id = letoh32(cmd.params.rsp.if_id);

	if (macaddr != NULL)
		sc->sc_pmac_id = letoh32(cmd.params.rsp.pmac_id);

	return (0);
}

/**
 * @@brief Function to send the mbx command to configure vlan
 * @@param sc 		software handle to the device
 * @@param vtags		array of vlan tags
 * @@param nvtags	number of elements in array
 * @@param untagged	boolean TRUE/FLASE
 * @@param promisc	flag to enable/disable VLAN promiscuous mode
 * @@returns		0 on success, EIO on failure
 */
int
oce_config_vlan(struct oce_softc *sc, struct normal_vlan *vtags, int nvtags,
    int untagged, int promisc)
{
	struct mbx_common_config_vlan cmd;

	memset(&cmd, 0, sizeof(cmd));

	cmd.params.req.if_id = sc->sc_if_id;
	cmd.params.req.promisc = promisc;
	cmd.params.req.untagged = untagged;
	cmd.params.req.num_vlans = nvtags;

	if (!promisc)
		memcpy(cmd.params.req.tags.normal_vlans, vtags,
			nvtags * sizeof(struct normal_vlan));

	return (oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_CONFIG_IFACE_VLAN,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd)));
}

/**
 * @@brief Function to set flow control capability in the hardware
 * @@param sc 		software handle to the device
 * @@param flags		flow control flags to set
 * @@returns		0 on success, EIO on failure
 */
int
oce_set_flow_control(struct oce_softc *sc, uint64_t flags)
{
	struct mbx_common_get_set_flow_control cmd;
	int err;

	memset(&cmd, 0, sizeof(cmd));

	cmd.rx_flow_control = flags & IFM_ETH_RXPAUSE ? 1 : 0;
	cmd.tx_flow_control = flags & IFM_ETH_TXPAUSE ? 1 : 0;

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_FLOW_CONTROL,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	memset(&cmd, 0, sizeof(cmd));

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_GET_FLOW_CONTROL,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	sc->sc_fc  = cmd.rx_flow_control ? IFM_ETH_RXPAUSE : 0;
	sc->sc_fc |= cmd.tx_flow_control ? IFM_ETH_TXPAUSE : 0;

	return (0);
}

#ifdef OCE_RSS
/**
 * @@brief Function to set flow control capability in the hardware
 * @@param sc 		software handle to the device
 * @@param enable	0=disable, OCE_RSS_xxx flags otherwise
 * @@returns		0 on success, EIO on failure
 */
int
oce_config_rss(struct oce_softc *sc, int enable)
{
	struct mbx_config_nic_rss cmd;
	uint8_t *tbl = &cmd.params.req.cputable;
	int i, j;

	memset(&cmd, 0, sizeof(cmd));

	if (enable)
		cmd.params.req.enable_rss = RSS_ENABLE_IPV4 | RSS_ENABLE_IPV6 |
		    RSS_ENABLE_TCP_IPV4 | RSS_ENABLE_TCP_IPV6;
	cmd.params.req.flush = OCE_FLUSH;
	cmd.params.req.if_id = htole32(sc->sc_if_id);

	arc4random_buf(cmd.params.req.hash, sizeof(cmd.params.req.hash));

	/*
	 * Initialize the RSS CPU indirection table.
	 *
	 * The table is used to choose the queue to place incoming packets.
	 * Incoming packets are hashed.  The lowest bits in the hash result
	 * are used as the index into the CPU indirection table.
	 * Each entry in the table contains the RSS CPU-ID returned by the NIC
	 * create.  Based on the CPU ID, the receive completion is routed to
	 * the corresponding RSS CQs.  (Non-RSS packets are always completed
	 * on the default (0) CQ).
	 */
	for (i = 0, j = 0; j < sc->sc_nrq; j++) {
		if (sc->sc_rq[j]->cfg.is_rss_queue)
			tbl[i++] = sc->sc_rq[j]->rss_cpuid;
	}
	if (i > 0)
		cmd->params.req.cpu_tbl_sz_log2 = htole16(ilog2(i));
	else
		return (ENXIO);

	return (oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_CONFIG_RSS, OCE_MBX_VER_V0,
	    &cmd, sizeof(cmd)));
}
#endif	/* OCE_RSS */

/**
 * @@brief Function for hardware update multicast filter
 * @@param sc		software handle to the device
 * @@param multi		table of multicast addresses
 * @@param naddr		number of multicast addresses in the table
 */
int
oce_update_mcast(struct oce_softc *sc,
    uint8_t multi[][ETHER_ADDR_LEN], int naddr)
{
	struct mbx_set_common_iface_multicast cmd;

	memset(&cmd, 0, sizeof(cmd));

	memcpy(&cmd.params.req.mac[0], &multi[0], naddr * ETHER_ADDR_LEN);
	cmd.params.req.num_mac = htole16(naddr);
	cmd.params.req.if_id = sc->sc_if_id;

	return (oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_IFACE_MULTICAST,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd)));
}

/**
 * @@brief RXF function to enable/disable device promiscuous mode
 * @@param sc		software handle to the device
 * @@param enable	enable/disable flag
 * @@returns		0 on success, EIO on failure
 * @@note
 *	The OPCODE_NIC_CONFIG_PROMISCUOUS command deprecated for Lancer.
 *	This function uses the COMMON_SET_IFACE_RX_FILTER command instead.
 */
int
oce_set_promisc(struct oce_softc *sc, int enable)
{
	struct mbx_set_common_iface_rx_filter cmd;
	struct iface_rx_filter_ctx *req;

	memset(&cmd, 0, sizeof(cmd));

	req = &cmd.params.req;
	req->if_id = sc->sc_if_id;

	if (enable)
		req->iface_flags = req->iface_flags_mask =
		    MBX_RX_IFACE_PROMISC | MBX_RX_IFACE_VLAN_PROMISC;

	return (oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_IFACE_RX_FILTER,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd)));
}

/**
 * @@brief Function to query the link status from the hardware
 * @@param sc 		software handle to the device
 * @@param[out] link	pointer to the structure returning link attributes
 * @@returns		0 on success, EIO on failure
 */
int
oce_get_link_status(struct oce_softc *sc)
{
	struct mbx_query_common_link_config cmd;
	int err;

	memset(&cmd, 0, sizeof(cmd));

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_QUERY_LINK_CONFIG,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	sc->sc_link_up = (letoh32(cmd.params.rsp.logical_link_status) ==
	    NTWK_LOGICAL_LINK_UP);

	if (cmd.params.rsp.mac_speed < 5)
		sc->sc_link_speed = cmd.params.rsp.mac_speed;
	else
		sc->sc_link_speed = 0;

	return (0);
}

void
oce_macaddr_set(struct oce_softc *sc)
{
	uint32_t old_pmac_id = sc->sc_pmac_id;
	int status = 0;

	if (!memcmp(sc->sc_macaddr, sc->sc_ac.ac_enaddr, ETHER_ADDR_LEN))
		return;

	status = oce_macaddr_add(sc, sc->sc_ac.ac_enaddr, &sc->sc_pmac_id);
	if (!status)
		status = oce_macaddr_del(sc, old_pmac_id);
	else
		printf("%s: failed to set MAC address\n", sc->sc_dev.dv_xname);
}

int
oce_macaddr_get(struct oce_softc *sc, uint8_t *macaddr)
{
	struct mbx_query_common_iface_mac cmd;
	int err;

	memset(&cmd, 0, sizeof(cmd));

	cmd.params.req.type = MAC_ADDRESS_TYPE_NETWORK;
	cmd.params.req.permanent = 1;

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_QUERY_IFACE_MAC,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err == 0)
		memcpy(macaddr, &cmd.params.rsp.mac.mac_addr[0],
		    ETHER_ADDR_LEN);
	return (err);
}

int
oce_macaddr_add(struct oce_softc *sc, uint8_t *enaddr, uint32_t *pmac)
{
	struct mbx_add_common_iface_mac cmd;
	int err;

	memset(&cmd, 0, sizeof(cmd));

	cmd.params.req.if_id = htole16(sc->sc_if_id);
	memcpy(cmd.params.req.mac_address, enaddr, ETHER_ADDR_LEN);

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_ADD_IFACE_MAC,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err == 0)
		*pmac = letoh32(cmd.params.rsp.pmac_id);
	return (err);
}

int
oce_macaddr_del(struct oce_softc *sc, uint32_t pmac)
{
	struct mbx_del_common_iface_mac cmd;

	memset(&cmd, 0, sizeof(cmd));

	cmd.params.req.if_id = htole16(sc->sc_if_id);
	cmd.params.req.pmac_id = htole32(pmac);

	return (oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_DEL_IFACE_MAC,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd)));
}

int
oce_new_rq(struct oce_softc *sc, struct oce_rq *rq)
{
	struct mbx_create_nic_rq cmd;
	int err, npages;

	memset(&cmd, 0, sizeof(cmd));

	npages = oce_load_ring(sc, rq->ring, &cmd.params.req.pages[0],
	    nitems(cmd.params.req.pages));
	if (!npages) {
		printf("%s: failed to load the rq ring\n", __func__);
		return (1);
	}

	if (IS_XE201(sc)) {
		cmd.params.req.frag_size = rq->fragsize / 2048;
		cmd.params.req.page_size = 1;
	} else
		cmd.params.req.frag_size = ilog2(rq->fragsize);
	cmd.params.req.num_pages = npages;
	cmd.params.req.cq_id = rq->cq->id;
	cmd.params.req.if_id = htole32(sc->sc_if_id);
	cmd.params.req.max_frame_size = htole16(rq->mtu);
	cmd.params.req.is_rss_queue = htole32(rq->rss);

	err = oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_CREATE_RQ,
	    IS_XE201(sc) ? OCE_MBX_VER_V1 : OCE_MBX_VER_V0, &cmd,
	    sizeof(cmd));
	if (err)
		return (err);

	rq->id = letoh16(cmd.params.rsp.rq_id);
	rq->rss_cpuid = cmd.params.rsp.rss_cpuid;

	return (0);
}

int
oce_new_wq(struct oce_softc *sc, struct oce_wq *wq)
{
	struct mbx_create_nic_wq cmd;
	int err, npages;

	memset(&cmd, 0, sizeof(cmd));

	npages = oce_load_ring(sc, wq->ring, &cmd.params.req.pages[0],
	    nitems(cmd.params.req.pages));
	if (!npages) {
		printf("%s: failed to load the wq ring\n", __func__);
		return (1);
	}

	if (IS_XE201(sc))
		cmd.params.req.if_id = sc->sc_if_id;
	cmd.params.req.nic_wq_type = NIC_WQ_TYPE_STANDARD;
	cmd.params.req.num_pages = npages;
	cmd.params.req.wq_size = ilog2(wq->nitems) + 1;
	cmd.params.req.cq_id = htole16(wq->cq->id);
	cmd.params.req.ulp_num = 1;

	err = oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_CREATE_WQ,
	    IS_XE201(sc) ? OCE_MBX_VER_V1 : OCE_MBX_VER_V0, &cmd,
	    sizeof(cmd));
	if (err)
		return (err);

	wq->id = letoh16(cmd.params.rsp.wq_id);

	return (0);
}

int
oce_new_mq(struct oce_softc *sc, struct oce_mq *mq)
{
	struct mbx_create_common_mq_ex cmd;
	union oce_mq_ext_ctx *ctx;
	int err, npages;

	memset(&cmd, 0, sizeof(cmd));

	npages = oce_load_ring(sc, mq->ring, &cmd.params.req.pages[0],
	    nitems(cmd.params.req.pages));
	if (!npages) {
		printf("%s: failed to load the mq ring\n", __func__);
		return (-1);
	}

	ctx = &cmd.params.req.context;
	ctx->v0.num_pages = npages;
	ctx->v0.cq_id = mq->cq->id;
	ctx->v0.ring_size = ilog2(mq->nitems) + 1;
	ctx->v0.valid = 1;
	/* Subscribe to Link State and Group 5 Events(bits 1 and 5 set) */
	ctx->v0.async_evt_bitmap = 0xffffffff;

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_CREATE_MQ_EXT,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	mq->id = letoh16(cmd.params.rsp.mq_id);

	return (0);
}

int
oce_new_eq(struct oce_softc *sc, struct oce_eq *eq)
{
	struct mbx_create_common_eq cmd;
	int err, npages;

	memset(&cmd, 0, sizeof(cmd));

	npages = oce_load_ring(sc, eq->ring, &cmd.params.req.pages[0],
	    nitems(cmd.params.req.pages));
	if (!npages) {
		printf("%s: failed to load the eq ring\n", __func__);
		return (-1);
	}

	cmd.params.req.ctx.num_pages = htole16(npages);
	cmd.params.req.ctx.valid = 1;
	cmd.params.req.ctx.size = (eq->isize == 4) ? 0 : 1;
	cmd.params.req.ctx.count = ilog2(eq->nitems / 256);
	cmd.params.req.ctx.armed = 0;
	cmd.params.req.ctx.delay_mult = htole32(eq->delay);

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_CREATE_EQ,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	eq->id = letoh16(cmd.params.rsp.eq_id);

	return (0);
}

int
oce_new_cq(struct oce_softc *sc, struct oce_cq *cq)
{
	struct mbx_create_common_cq cmd;
	union oce_cq_ctx *ctx;
	int err, npages;

	memset(&cmd, 0, sizeof(cmd));

	npages = oce_load_ring(sc, cq->ring, &cmd.params.req.pages[0],
	    nitems(cmd.params.req.pages));
	if (!npages) {
		printf("%s: failed to load the cq ring\n", __func__);
		return (-1);
	}

	ctx = &cmd.params.req.cq_ctx;

	if (IS_XE201(sc)) {
		ctx->v2.num_pages = htole16(npages);
		ctx->v2.page_size = 1; /* for 4K */
		ctx->v2.eventable = cq->eventable;
		ctx->v2.valid = 1;
		ctx->v2.count = ilog2(cq->nitems / 256);
		ctx->v2.nodelay = cq->nodelay;
		ctx->v2.coalesce_wm = cq->ncoalesce;
		ctx->v2.armed = 0;
		ctx->v2.eq_id = cq->eq->id;
		if (ctx->v2.count == 3) {
			if (cq->nitems > (4*1024)-1)
				ctx->v2.cqe_count = (4*1024)-1;
			else
				ctx->v2.cqe_count = cq->nitems;
		}
	} else {
		ctx->v0.num_pages = htole16(npages);
		ctx->v0.eventable = cq->eventable;
		ctx->v0.valid = 1;
		ctx->v0.count = ilog2(cq->nitems / 256);
		ctx->v0.nodelay = cq->nodelay;
		ctx->v0.coalesce_wm = cq->ncoalesce;
		ctx->v0.armed = 0;
		ctx->v0.eq_id = cq->eq->id;
	}

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_CREATE_CQ,
	    IS_XE201(sc) ? OCE_MBX_VER_V2 : OCE_MBX_VER_V0, &cmd,
	    sizeof(cmd));
	if (err)
		return (err);

	cq->id = letoh16(cmd.params.rsp.cq_id);

	return (0);
}

int
oce_init_stats(struct oce_softc *sc)
{
	union cmd {
		struct mbx_get_nic_stats_v0	_be2;
		struct mbx_get_nic_stats	_be3;
		struct mbx_get_pport_stats	_xe201;
	};

	sc->sc_statcmd = malloc(sizeof(union cmd), M_DEVBUF, M_ZERO | M_NOWAIT);
	if (sc->sc_statcmd == NULL) {
		printf("%s: failed to allocate statistics command block\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}
	return (0);
}

int
oce_update_stats(struct oce_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	uint64_t rxe, txe;
	int err;

	if (ISSET(sc->sc_flags, OCE_F_BE2))
		err = oce_stats_be2(sc, &rxe, &txe);
	else if (ISSET(sc->sc_flags, OCE_F_BE3))
		err = oce_stats_be3(sc, &rxe, &txe);
	else
		err = oce_stats_xe(sc, &rxe, &txe);
	if (err)
		return (err);

	ifp->if_ierrors += (rxe > sc->sc_rx_errors) ?
	    rxe - sc->sc_rx_errors : sc->sc_rx_errors - rxe;
	sc->sc_rx_errors = rxe;
	ifp->if_oerrors += (txe > sc->sc_tx_errors) ?
	    txe - sc->sc_tx_errors : sc->sc_tx_errors - txe;
	sc->sc_tx_errors = txe;

	return (0);
}

int
oce_stats_be2(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe)
{
	struct mbx_get_nic_stats_v0 *cmd = sc->sc_statcmd;
	struct oce_pmem_stats *ms;
	struct oce_rxf_stats_v0 *rs;
	struct oce_port_rxf_stats_v0 *ps;
	int err;

	memset(cmd, 0, sizeof(*cmd));

	err = oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_GET_STATS, OCE_MBX_VER_V0,
	    cmd, sizeof(*cmd));
	if (err)
		return (err);

	ms = &cmd->params.rsp.stats.pmem;
	rs = &cmd->params.rsp.stats.rxf;
	ps = &rs->port[sc->sc_port];

	*rxe = ps->rx_crc_errors + ps->rx_in_range_errors +
	    ps->rx_frame_too_long + ps->rx_dropped_runt +
	    ps->rx_ip_checksum_errs + ps->rx_tcp_checksum_errs +
	    ps->rx_udp_checksum_errs + ps->rxpp_fifo_overflow_drop +
	    ps->rx_dropped_tcp_length + ps->rx_dropped_too_small +
	    ps->rx_dropped_too_short + ps->rx_out_range_errors +
	    ps->rx_dropped_header_too_small + ps->rx_input_fifo_overflow_drop +
	    ps->rx_alignment_symbol_errors;
	if (sc->sc_if_id)
		*rxe += rs->port1_jabber_events;
	else
		*rxe += rs->port0_jabber_events;
	*rxe += ms->eth_red_drops;

	*txe = 0; /* hardware doesn't provide any extra tx error statistics */

	return (0);
}

int
oce_stats_be3(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe)
{
	struct mbx_get_nic_stats *cmd = sc->sc_statcmd;
	struct oce_pmem_stats *ms;
	struct oce_rxf_stats_v1 *rs;
	struct oce_port_rxf_stats_v1 *ps;
	int err;

	memset(cmd, 0, sizeof(*cmd));

	err = oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_GET_STATS, OCE_MBX_VER_V1,
	    cmd, sizeof(*cmd));
	if (err)
		return (err);

	ms = &cmd->params.rsp.stats.pmem;
	rs = &cmd->params.rsp.stats.rxf;
	ps = &rs->port[sc->sc_port];

	*rxe = ps->rx_crc_errors + ps->rx_in_range_errors +
	    ps->rx_frame_too_long + ps->rx_dropped_runt +
	    ps->rx_ip_checksum_errs + ps->rx_tcp_checksum_errs +
	    ps->rx_udp_checksum_errs + ps->rxpp_fifo_overflow_drop +
	    ps->rx_dropped_tcp_length + ps->rx_dropped_too_small +
	    ps->rx_dropped_too_short + ps->rx_out_range_errors +
	    ps->rx_dropped_header_too_small + ps->rx_input_fifo_overflow_drop +
	    ps->rx_alignment_symbol_errors + ps->jabber_events;
	*rxe += ms->eth_red_drops;

	*txe = 0; /* hardware doesn't provide any extra tx error statistics */

	return (0);
}

int
oce_stats_xe(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe)
{
	struct mbx_get_pport_stats *cmd = sc->sc_statcmd;
	struct oce_pport_stats *pps;
	int err;

	memset(cmd, 0, sizeof(*cmd));

	cmd->params.req.reset_stats = 0;
	cmd->params.req.port_number = sc->sc_if_id;

	err = oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_GET_PPORT_STATS,
	    OCE_MBX_VER_V0, cmd, sizeof(*cmd));
	if (err)
		return (err);

	pps = &cmd->params.rsp.pps;

	*rxe = pps->rx_discards + pps->rx_errors + pps->rx_crc_errors +
	    pps->rx_alignment_errors + pps->rx_symbol_errors +
	    pps->rx_frames_too_long + pps->rx_internal_mac_errors +
	    pps->rx_undersize_pkts + pps->rx_oversize_pkts + pps->rx_jabbers +
	    pps->rx_control_frames_unknown_opcode + pps->rx_in_range_errors +
	    pps->rx_out_of_range_errors + pps->rx_ip_checksum_errors +
	    pps->rx_tcp_checksum_errors + pps->rx_udp_checksum_errors +
	    pps->rx_fifo_overflow + pps->rx_input_fifo_overflow +
	    pps->rx_drops_too_many_frags + pps->rx_drops_mtu;

	*txe = pps->tx_discards + pps->tx_errors + pps->tx_internal_mac_errors;

	return (0);
}
@


1.98
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.97 2016/09/14 19:04:54 mikeb Exp $	*/
a1206 1
	struct ifnet *ifp = &sc->sc_ac.ac_if;
a1308 2

	ifp->if_opackets++;
@


1.97
log
@Declare the type instead of a on-stack variable for sizeof purposes

(Un?)surprisingly the compiler is smart enough to produce the same
code in both cases, but this conveys the intention better.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.96 2016/08/24 10:38:34 dlg Exp $	*/
d589 2
a590 3
		pool_init(oce_pkt_pool, sizeof(struct oce_pkt), 0, 0, 0,
		    "ocepkts", NULL);
		pool_setipl(oce_pkt_pool, IPL_NET);
@


1.96
log
@pool_setipl for oce(4)

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.95 2016/04/13 10:34:32 mpi Exp $	*/
d3454 1
a3454 1
	union {
d3458 1
a3458 1
	} cmd;
d3460 1
a3460 1
	sc->sc_statcmd = malloc(sizeof(cmd), M_DEVBUF, M_ZERO | M_NOWAIT);
@


1.95
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.94 2016/03/14 01:16:39 mikeb Exp $	*/
d591 1
@


1.94
log
@Allocate statistics command from the heap, pointed out by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.93 2016/03/04 00:04:48 deraadt Exp $	*/
a826 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.93
log
@extra ) not needed, spotted by tiago silva
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.92 2016/01/06 06:41:57 mikeb Exp $	*/
d355 2
d490 2
a491 2
static inline int
	oce_update_stats(struct oce_softc *);
d613 3
d3451 19
a3469 1
static inline int
d3498 1
a3498 1
	struct mbx_get_nic_stats_v0 cmd;
d3504 1
a3504 1
	memset(&cmd, 0, sizeof(cmd));
d3507 1
a3507 1
	    &cmd, sizeof(cmd));
d3511 2
a3512 2
	ms = &cmd.params.rsp.stats.pmem;
	rs = &cmd.params.rsp.stats.rxf;
d3537 1
a3537 1
	struct mbx_get_nic_stats cmd;
d3543 1
a3543 1
	memset(&cmd, 0, sizeof(cmd));
d3546 1
a3546 1
	    &cmd, sizeof(cmd));
d3550 2
a3551 2
	ms = &cmd.params.rsp.stats.pmem;
	rs = &cmd.params.rsp.stats.rxf;
d3572 1
a3572 1
	struct mbx_get_pport_stats cmd;
d3576 1
a3576 1
	memset(&cmd, 0, sizeof(cmd));
d3578 2
a3579 2
	cmd.params.req.reset_stats = 0;
	cmd.params.req.port_number = sc->sc_if_id;
d3582 1
a3582 1
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
d3586 1
a3586 1
	pps = &cmd.params.rsp.pps;
@


1.92
log
@revert 1.87, more work is needed here
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.86 2015/09/11 13:02:28 stsp Exp $	*/
d3074 1
a3074 1
		    RSS_ENABLE_TCP_IPV4 | RSS_ENABLE_TCP_IPV6);
@


1.91
log
@Replace mountroothook_establish(9) by config_mountroot(9) a narrower API
similar to config_defer(9).

ok mikeb@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.90 2015/11/25 03:09:59 dlg Exp $	*/
d600 2
a601 2
	sc->sc_ih = pci_intr_establish(pa->pa_pc, ih, IPL_NET | IPL_MPSAFE,
	    oce_intr, sc, sc->sc_dev.dv_xname);
a1140 4
	intr_barrier(sc->sc_ih);

	KASSERT((ifp->if_flags & IFF_RUNNING) == 0);

a1438 1
	KERNEL_LOCK();
a1455 2
	KERNEL_UNLOCK();

a1547 3
	if (if_rxr_inuse(&rq->rxring) == 0)
		return;

a1887 1
	KERNEL_LOCK();
a1893 1
	KERNEL_UNLOCK();
@


1.90
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.89 2015/11/14 17:54:57 mpi Exp $	*/
d368 1
a368 1
void	oce_attachhook(void *);
d624 1
a624 1
	mountroothook_establish(oce_attachhook, sc);
d782 1
a782 1
oce_attachhook(void *arg)
d784 1
a784 1
	struct oce_softc *sc = arg;
@


1.89
log
@Do not include <net/if_vlan_var.h> when it's not necessary.

Because of the VLAN hacks in mpw(4) this file still contains the definition
of "struct ifvlan" which depends on <sys/refcnt.h> which in turns pull
<sys/atomic.h>...
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.88 2015/10/25 13:04:28 mpi Exp $	*/
d1111 1
a1111 1
	ifp->if_flags &= ~IFF_OACTIVE;
d1135 2
a1136 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
d1179 1
a1179 1
	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
d1188 1
a1188 1
			ifp->if_flags |= IFF_OACTIVE;
d1452 1
a1452 1
	if (ifp->if_flags & IFF_OACTIVE) {
d1454 1
a1454 1
			ifp->if_flags &= ~IFF_OACTIVE;
@


1.88
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.87 2015/09/29 17:04:20 chris Exp $	*/
a83 5
#endif

#if NVLAN > 0
#include <net/if_types.h>
#include <net/if_vlan_var.h>
@


1.87
log
@Unlock interrupt handler rx path with intr_barrier

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.86 2015/09/11 13:02:28 stsp Exp $	*/
a851 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a861 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->sc_ac, ifa);
@


1.86
log
@Make room for media types of the future. Extend the ifmedia word to 64 bits.
This changes numbers of the SIOCSIFMEDIA and SIOCGIFMEDIA ioctls and
grows struct ifmediareq.

Old ifconfig and dhclient binaries can still assign addresses, however
the 'media' subcommand stops working. Recompiling ifconfig and dhclient
with new headers before a reboot should not be necessary unless in very
special circumstances where non-default media settings must be used to
get link and console access is not available.

There may be some MD fallout but that will be cleared up later.

ok deraadt miod
with help and suggestions from several sharks attending l2k15
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.85 2015/06/29 18:58:04 mikeb Exp $	*/
d605 2
a606 2
	sc->sc_ih = pci_intr_establish(pa->pa_pc, ih, IPL_NET, oce_intr, sc,
	    sc->sc_dev.dv_xname);
d1148 4
d1450 1
d1468 2
d1562 3
d1905 1
d1912 1
@


1.85
log
@Hide ETHER_ALIGN mbuf adjustment under "#ifdef __STRICT_ALIGNMENT"
for now to get jumbo frames working.  oce(4) will need the same
treatment as ix(4) when sparc64 support will be implemented.

Tested by Pedro Caetano <pedrocaetano at binaryflows ! com>, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.84 2015/06/24 09:40:54 mpi Exp $	*/
d325 1
a325 1
	uint			sc_fc;
d475 1
a475 1
int	oce_set_flow_control(struct oce_softc *, uint flags);
d3035 1
a3035 1
oce_set_flow_control(struct oce_softc *sc, uint flags)
@


1.84
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.83 2015/04/30 07:51:07 mpi Exp $	*/
d1776 1
d1778 1
@


1.83
log
@Convert moar drivers to if_input().

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.82 2015/03/14 03:38:48 jsg Exp $	*/
a1629 2

		ifp->if_ipackets++;
@


1.82
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.81 2014/12/22 02:28:52 tedu Exp $	*/
d1550 1
a1609 2
		m->m_pkthdr.rcvif = ifp;

d1647 1
a1647 6
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_IN);
#endif

		ether_input_mbuf(ifp, m);
d1650 1
a1650 1
	return;
@


1.81
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.80 2014/12/13 21:05:33 doug Exp $	*/
a72 1
#include <net/if_dl.h>
@


1.80
log
@yet more mallocarray() changes.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.79 2014/08/30 09:48:23 dlg Exp $	*/
a75 1
#ifdef INET
a77 1
#endif
a863 1
#ifdef INET
a865 1
#endif
a1336 1
#ifdef INET
a1337 1
#endif
a1365 1
#ifdef INET
a1373 1
#endif
@


1.79
log
@let the mru always be what the chip can do, not what the mtu implies.

tested by and ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.78 2014/08/14 09:52:03 mikeb Exp $	*/
d914 1
a914 1
		if ((ifr = malloc(sc->sc_nrq * sizeof(*ifr), M_DEVBUF,
@


1.78
log
@Implement rxrinfo ioctl for cluster usage statistics
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.77 2014/07/22 13:12:11 mpi Exp $	*/
a881 8
	case SIOCSIFMTU:
		if (ifr->ifr_mtu < OCE_MIN_MTU || ifr->ifr_mtu > OCE_MAX_MTU)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu) {
			ifp->if_mtu = ifr->ifr_mtu;
			oce_init(sc);
		}
		break;
d1079 1
a1079 1
		rq->mtu = ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN +
@


1.77
log
@Fewer <netinet/in_systm.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.76 2014/07/12 18:48:52 tedu Exp $	*/
d379 1
d894 3
d912 30
@


1.76
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.75 2014/07/08 05:35:18 dlg Exp $	*/
a77 2
#include <netinet/in_systm.h>
#include <netinet/ip.h>
@


1.75
log
@cut things that relied on mclgeti for rx ring accounting/restriction over
to using if_rxr.

cut the reporting systat did over to the rxr ioctl.

tested as much as i can on alpha, amd64, and sparc64.
mpi@@ has run it on macppc.
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.74 2014/01/20 17:21:22 chris Exp $	*/
d1990 1
a1990 1
		free(wq, M_DEVBUF);
d1998 1
a1998 1
		free(wq, M_DEVBUF);
d2069 1
a2069 1
	free(wq, M_DEVBUF);
d2098 1
a2098 1
		free(rq, M_DEVBUF);
d2106 1
a2106 1
		free(rq, M_DEVBUF);
d2175 1
a2175 1
	free(rq, M_DEVBUF);
d2190 1
a2190 1
		free(eq, M_DEVBUF);
d2202 1
a2202 1
		free(eq, M_DEVBUF);
d2253 1
a2253 1
	free(eq, M_DEVBUF);
d2269 1
a2269 1
		free(mq, M_DEVBUF);
d2277 1
a2277 1
		free(mq, M_DEVBUF);
d2290 1
a2290 1
		free(mq, M_DEVBUF);
d2335 1
a2335 1
	free(mq, M_DEVBUF);
d2361 1
a2361 1
		free(cq, M_DEVBUF);
d2374 1
a2374 1
		free(cq, M_DEVBUF);
d2397 1
a2397 1
	free(cq, M_DEVBUF);
d2564 1
a2564 1
	free(ring, M_DEVBUF);
d2572 1
a2572 1
	free(ring, M_DEVBUF);
@


1.74
log
@bcopy to memcpy

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.73 2014/01/20 17:17:08 chris Exp $	*/
d282 1
a285 2
	uint32_t		pending;

a832 3
	/* oce splits jumbos into 2k chunks... */
	m_clsetwms(ifp, MCLBYTES, 8, sc->sc_rx_ring_size);

a1061 1
		rq->pending	 = 0;
d1064 3
d1524 1
a1524 1
		if (rq->nitems - rq->pending > 1 && !oce_alloc_rx_bufs(rq))
d1557 1
a1557 1
		rq->pending--;
a1593 3
		/* Account for jumbo chains */
		m_cluncount(m, 1);

d1668 1
a1668 1
		rq->pending--;
a1755 1
	struct ifnet *ifp = &sc->sc_ac.ac_if;
d1762 1
a1762 1
	pkt->mbuf = MCLGETI(NULL, M_DONTWAIT, ifp, MCLBYTES);
a1787 1
	rq->pending++;
d1802 5
a1807 1
	while (oce_get_buf(rq))
d1809 3
d2427 1
a2427 1
		rq->pending--;
@


1.73
log
@bcmp to memcmp

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.72 2014/01/20 17:14:56 chris Exp $	*/
d589 1
a589 1
	bcopy(sc->sc_macaddr, sc->sc_ac.ac_enaddr, ETHER_ADDR_LEN);
d934 1
a934 1
			bcopy(enm->enm_addrlo, multi[naddr++], ETHER_ADDR_LEN);
d2822 1
a2822 1
		bcopy(payload, epayload, length);
d2828 1
a2828 1
		bcopy(payload, mbx->pld.data, length);
d2848 1
a2848 1
			bcopy(epayload, payload, length);
d2850 1
a2850 1
			bcopy(&mbx->pld.data, payload, length);
d2971 1
a2971 1
		bcopy(macaddr, &cmd.params.req.mac_addr[0], ETHER_ADDR_LEN);
d3012 1
a3012 1
		bcopy(vtags, cmd.params.req.tags.normal_vlans,
d3117 1
a3117 1
	bcopy(&multi[0], &cmd.params.req.mac[0], naddr * ETHER_ADDR_LEN);
d3213 1
a3213 1
		bcopy(&cmd.params.rsp.mac.mac_addr[0], macaddr,
d3227 1
a3227 1
	bcopy(enaddr, cmd.params.req.mac_address, ETHER_ADDR_LEN);
@


1.72
log
@bzero to memset

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.71 2013/08/23 19:16:17 mikeb Exp $	*/
d3189 1
a3189 1
	if (!bcmp(sc->sc_macaddr, sc->sc_ac.ac_enaddr, ETHER_ADDR_LEN))
@


1.71
log
@don't call if_link_state_change if link state is not changed
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.70 2013/08/07 01:06:36 bluhm Exp $	*/
d1136 1
a1136 1
		bzero(&cmd, sizeof(cmd));
d1244 1
a1244 1
	bzero(nhe, sizeof(*nhe));
d1282 1
a1282 1
		bzero(nfe, sizeof(*nfe));
d1290 1
a1290 1
		bzero(nfe, sizeof(*nfe));
d2058 1
a2058 1
		bzero(&cmd, sizeof(cmd));
d2164 1
a2164 1
		bzero(&cmd, sizeof(cmd));
d2246 1
a2246 1
		bzero(&cmd, sizeof(cmd));
d2326 1
a2326 1
		bzero(&cmd, sizeof(cmd));
d2390 1
a2390 1
		bzero(&cmd, sizeof(cmd));
d2436 1
a2436 1
	bzero(dma, sizeof(struct oce_dma_mem));
d2708 1
a2708 1
				bzero(&cmd, sizeof(cmd));
d2815 1
a2815 1
	bzero(mbx, sizeof(struct oce_mbx));
d2875 1
a2875 1
	bzero(mbx, sizeof(struct oce_mbx));
d2899 1
a2899 1
	bzero(&cmd, sizeof(cmd));
d2918 1
a2918 1
	bzero(&cmd, sizeof(cmd));
d2965 1
a2965 1
	bzero(&cmd, sizeof(cmd));
d3004 1
a3004 1
	bzero(&cmd, sizeof(cmd));
d3031 1
a3031 1
	bzero(&cmd, sizeof(cmd));
d3041 1
a3041 1
	bzero(&cmd, sizeof(cmd));
d3068 1
a3068 1
	bzero(&cmd, sizeof(cmd));
d3115 1
a3115 1
	bzero(&cmd, sizeof(cmd));
d3140 1
a3140 1
	bzero(&cmd, sizeof(cmd));
d3165 1
a3165 1
	bzero(&cmd, sizeof(cmd));
d3205 1
a3205 1
	bzero(&cmd, sizeof(cmd));
d3224 1
a3224 1
	bzero(&cmd, sizeof(cmd));
d3241 1
a3241 1
	bzero(&cmd, sizeof(cmd));
d3256 1
a3256 1
	bzero(&cmd, sizeof(cmd));
d3294 1
a3294 1
	bzero(&cmd, sizeof(cmd));
d3329 1
a3329 1
	bzero(&cmd, sizeof(cmd));
d3362 1
a3362 1
	bzero(&cmd, sizeof(cmd));
d3395 1
a3395 1
	bzero(&cmd, sizeof(cmd));
d3479 1
a3479 1
	bzero(&cmd, sizeof(cmd));
d3518 1
a3518 1
	bzero(&cmd, sizeof(cmd));
d3551 1
a3551 1
	bzero(&cmd, sizeof(cmd));
@


1.70
log
@Most network drivers include netinet/in_var.h, but apparently they
don't have to.  Just remove these include lines.
Compiled on amd64 i386 sparc64; OK henning@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.69 2013/01/17 00:48:04 henning Exp $	*/
d949 2
a950 1
	if (sc->sc_link_up)
d952 1
a952 4
	if (ifp->if_link_state == link_state)
		return;
	ifp->if_baudrate = 0;
	if (link_state != LINK_STATE_DOWN) {
d968 4
a971 2
	ifp->if_link_state = link_state;
	if_link_state_change(ifp);
@


1.69
log
@first or second coming, commie or not commie, one m in coming is sufficient
ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.67 2012/11/27 18:08:21 gsoares Exp $	*/
a78 1
#include <netinet/in_var.h>
@


1.68
log
@adjust mbuf chain data pointer so that ip header would appear
word aligned; remove pool constraints insanity while here
@
text
@d3082 2
a3083 2
	 * The table is used to choose the queue to place incomming packets.
	 * Incomming packets are hashed.  The lowest bits in the hash result
@


1.67
log
@fix format string; OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.66 2012/11/26 19:03:59 mikeb Exp $	*/
a71 2
#include <uvm/uvm_extern.h>

d462 1
a462 1
	    int maxsegs);
a505 6
extern struct uvm_constraint_range no_constraint;
const struct kmem_pa_mode kp_contig = {
	.kp_constraint = &no_constraint,
	.kp_maxseg = 1,
	.kp_zero = 1
};
a599 1
		pool_set_constraints(oce_pkt_pool, &kp_contig);
d1776 1
d2628 1
a2628 1
oce_pkt_alloc(struct oce_softc *sc, size_t size, int nsegs, int maxsegs)
d2632 1
a2632 1
	if ((pkt = pool_get(oce_pkt_pool, PR_NOWAIT)) == NULL)
d2635 1
a2635 1
	if (bus_dmamap_create(sc->sc_dmat, size, nsegs, maxsegs, 0,
@


1.66
log
@shorten MBX_RX_IFACE_* defines
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.65 2012/11/26 18:58:11 mikeb Exp $	*/
d1743 2
a1744 1
			printf("%s: LRO init failed\n");
@


1.65
log
@get rid of some useless bitfields in oce_mbx and mbx_hdr
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.64 2012/11/23 18:46:03 mikeb Exp $	*/
d2956 3
a2958 3
	caps = MBX_RX_IFACE_FLAGS_BROADCAST | MBX_RX_IFACE_FLAGS_UNTAGGED |
	    MBX_RX_IFACE_FLAGS_PROMISC | MBX_RX_IFACE_FLAGS_MCAST_PROMISC |
	    MBX_RX_IFACE_FLAGS_RSS;
d2961 1
a2961 1
	caps_en = MBX_RX_IFACE_FLAGS_BROADCAST | MBX_RX_IFACE_FLAGS_UNTAGGED;
d2965 2
a2966 2
		caps |= MBX_RX_IFACE_FLAGS_PASS_L3L4_ERR;
		caps_en |= MBX_RX_IFACE_FLAGS_PASS_L3L4_ERR;
d2971 1
a2971 1
		caps_en |= MBX_RX_IFACE_FLAGS_RSS;
d3155 1
a3155 2
		    MBX_RX_IFACE_FLAGS_PROMISC |
		    MBX_RX_IFACE_FLAGS_VLAN_PROMISC;
@


1.64
log
@better way to set baudrate to 0; pointed out by gsoares@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.63 2012/11/21 11:24:16 mikeb Exp $	*/
d455 1
a455 1
	    struct phys_addr *, int max_segs);
a952 1

d2585 1
a2585 1
    struct phys_addr *pa_list, int maxsegs)
d2597 1
a2597 1
		printf("%s: too many segments", sc->sc_dev.dv_xname);
d2604 2
a2605 4
	for (i = 0; i < dma->map->dm_nsegs; i++) {
		pa_list[i].lo = ADDR_LO(dma->map->dm_segs[i].ds_addr);
		pa_list[i].hi = ADDR_HI(dma->map->dm_segs[i].ds_addr);
	}
d2828 1
a2828 1
		mbx->u0.s.sge_count = 1;
d2831 2
a2832 2
		mbx->payload.u0.u1.sgl[0].paddr = OCE_MEM_DVA(&sc->sc_pld);
		mbx->payload.u0.u1.sgl[0].length = length;
d2835 3
a2837 3
		mbx->u0.s.embedded = 1;
		bcopy(payload, &mbx->payload, length);
		hdr = (struct mbx_hdr *)&mbx->payload;
d2840 4
a2843 4
	hdr->u0.req.opcode = opcode;
	hdr->u0.req.subsystem = subsys;
	hdr->u0.req.request_length = length - sizeof(*hdr);
	hdr->u0.req.version = version;
d2845 1
a2845 1
		hdr->u0.req.timeout = 2 * OCE_MBX_TIMEOUT;
d2847 1
a2847 1
		hdr->u0.req.timeout = OCE_MBX_TIMEOUT;
d2858 1
a2858 1
			bcopy(&mbx->payload, payload, length);
d2885 1
a2885 1
	cmd = (struct mbx_get_common_fw_version *)&mbx->payload;
d2888 5
a2892 5
	hdr->u0.req.subsystem = SUBSYS_COMMON;
	hdr->u0.req.opcode = OPCODE_COMMON_GET_FW_VERSION;
	hdr->u0.req.version = OCE_MBX_VER_V0;
	hdr->u0.req.timeout = OCE_MBX_TIMEOUT;
	hdr->u0.req.request_length = sizeof(*cmd) - sizeof(*hdr);
d2894 1
a2894 1
	mbx->u0.s.embedded = 1;
@


1.63
log
@Don't forget to delete an rx refill timeout when bringing
an interface down (noticed by dlg@@ in the other diff).
While here, do some minor cleanup in the interrupt handler.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.62 2012/11/20 18:43:19 mikeb Exp $	*/
d964 1
d980 1
a980 2
	} else
		ifp->if_baudrate = 0;
@


1.62
log
@fix typo
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.61 2012/11/14 17:25:46 mikeb Exp $	*/
d1135 1
d1402 1
a1402 1
	int i, claimed = 0, neqe = 0;
d1411 5
a1415 2
	if (!neqe)
		goto eq_arm; /* Spurious */
a1418 2
	claimed = 1;

a1425 5
	}

	/* Arm all CQs connected to this EQ */
	for (i = 0; i < eq->cq_valid; i++) {
		cq = eq->cq[i];
a1428 1
eq_arm:
d1430 1
a1430 1
	return (claimed);
d1842 2
a1843 3
		oce_alloc_rx_bufs(rq);
		if (!rq->pending)
			timeout_add(&sc->sc_rxrefill, 1);
@


1.61
log
@allocate a mailbox payload dma memory upfront instead of per request
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.60 2012/11/13 19:17:39 mikeb Exp $	*/
d379 1
a379 1
int 	oce_probe(struct device *, void *, void *);
d520 1
a520 1
	sizeof(struct oce_softc), oce_probe, oce_attach, NULL, NULL
d532 1
a532 1
oce_probe(struct device *parent, void *match, void *aux)
a663 1

@


1.60
log
@do an OACTIVE/if_start dance only once per tx interrupt
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.59 2012/11/13 19:13:08 mikeb Exp $	*/
d114 2
d142 1
d337 1
d571 4
d577 1
a577 1
		goto fail_1;
d581 1
a581 1
		goto fail_1;
d586 1
a586 1
		goto fail_1;
d591 1
a591 1
			goto fail_1;
d596 1
a596 1
		goto fail_1;
d604 1
a604 1
			goto fail_1;
d616 1
a616 1
		goto fail_1;
d627 1
a627 1
		goto fail_1;
d632 1
a632 1
		goto fail_2;
d638 1
a638 1
		goto fail_3;
d651 1
a651 1
fail_3:
d657 2
d660 1
a660 1
	pci_intr_disestablish(pa->pa_pc, sc->sc_ih);
d2762 1
a2762 1
	pa = (uint32_t)((uint64_t)sc->sc_mbx.paddr >> 34);
d2770 1
a2770 1
	pa = (uint32_t)((uint64_t)sc->sc_mbx.paddr >> 4) & 0x3fffffff;
a2820 1
	struct oce_dma_mem sgl;
d2825 4
a2828 5
	if (length > OCE_MBX_PAYLOAD) {
		if (oce_dma_alloc(sc, length, &sgl))
			return (-1);
		epayload = OCE_MEM_KVA(&sgl);
	}
d2838 1
a2838 1
		oce_dma_sync(&sgl, BUS_DMASYNC_PREWRITE);
d2840 1
a2840 1
		mbx->payload.u0.u1.sgl[0].paddr = sgl.paddr;
d2842 1
a2842 1
		hdr = OCE_MEM_KVA(&sgl);
d2858 3
d2864 1
a2864 1
			oce_dma_sync(&sgl, BUS_DMASYNC_POSTWRITE);
a2872 2
	if (epayload)
		oce_dma_free(sc, &sgl);
@


1.59
log
@enable hardware tx checksum offloading as oce doesn't
seem to require an initialized pseudo-header checksum
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.58 2012/11/13 17:52:11 mikeb Exp $	*/
d1435 2
d1446 10
a1464 1
	struct ifnet *ifp = &sc->sc_ac.ac_if;
a1481 9

	if (ifp->if_flags & IFF_OACTIVE) {
		if (wq->ring->nused < (wq->ring->nitems / 2)) {
			ifp->if_flags &= ~IFF_OACTIVE;
			oce_start(ifp);
		}
	}
	if (wq->ring->nused == 0)
		ifp->if_timer = 0;
@


1.58
log
@more cleanup missed in the previous commit
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.57 2012/11/13 17:40:41 mikeb Exp $	*/
d838 2
a839 1
	ifp->if_capabilities = IFCAP_VLAN_MTU;
@


1.57
log
@major cleanup; get rid of the oce_destroy_queue
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.56 2012/11/12 20:31:32 mikeb Exp $	*/
d86 4
d375 16
a390 16
int 	oce_probe(struct device *parent, void *match, void *aux);
void	oce_attach(struct device *parent, struct device *self, void *aux);
int 	oce_pci_alloc(struct oce_softc *sc, struct pci_attach_args *pa);
void	oce_attachhook(void *arg);
void	oce_attach_ifp(struct oce_softc *sc);
int 	oce_ioctl(struct ifnet *ifp, u_long command, caddr_t data);
void	oce_iff(struct oce_softc *sc);
void	oce_link_status(struct oce_softc *sc);
void	oce_media_status(struct ifnet *ifp, struct ifmediareq *ifmr);
int 	oce_media_change(struct ifnet *ifp);
void	oce_tick(void *arg);
void	oce_init(void *xsc);
void	oce_stop(struct oce_softc *sc);
void	oce_watchdog(struct ifnet *ifp);
void	oce_start(struct ifnet *ifp);
int	oce_encap(struct oce_softc *sc, struct mbuf **mpp, int wq_index);
d393 1
a393 1
	oce_tso(struct oce_softc *sc, struct mbuf **mpp);
d395 8
a402 8
int 	oce_intr(void *arg);
void	oce_intr_wq(void *arg);
void	oce_txeof(struct oce_wq *wq);
void	oce_intr_rq(void *arg);
void	oce_rxeof(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe);
void	oce_rxeoc(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe);
int 	oce_vtp_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe);
int 	oce_port_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe);
d404 11
a414 11
void	oce_flush_lro(struct oce_rq *rq);
int 	oce_init_lro(struct oce_softc *sc);
void	oce_free_lro(struct oce_softc *sc);
#endif
int	oce_get_buf(struct oce_rq *rq);
int	oce_alloc_rx_bufs(struct oce_rq *rq);
void	oce_refill_rx(void *arg);
void	oce_free_posted_rxbuf(struct oce_rq *rq);
void	oce_intr_mq(void *arg);
void	oce_link_event(struct oce_softc *sc,
	    struct oce_async_cqe_link_state *acqe);
d416 2
a417 2
int 	oce_init_queues(struct oce_softc *sc);
void	oce_release_queues(struct oce_softc *sc);
d1191 1
a1191 1
oce_encap(struct oce_softc *sc, struct mbuf **mpp, int wq_index)
d1195 1
a1195 1
	struct oce_wq *wq = sc->sc_wq[wq_index];
d1314 1
a1314 1
#if OCE_TSO
d1335 1
a1335 1
			return NULL;
d1354 1
a1354 1
			return NULL;
d1372 1
a1372 1
		return NULL;
d1377 1
a1377 1
		return NULL;
d1379 1
a1379 1
	return m;
d1382 1
a1382 1
#endif
a1723 2

	return;
d1742 1
a1742 1
	return rc;
d2370 1
a2370 1
	cq->nodelay = (uint8_t) nodelay;
@


1.56
log
@move some stuff around, do minor cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.55 2012/11/09 21:11:42 mikeb Exp $	*/
d371 16
a386 43
int  oce_probe(struct device *parent, void *match, void *aux);
void oce_attach(struct device *parent, struct device *self, void *aux);
int  oce_pci_alloc(struct oce_softc *sc, struct pci_attach_args *pa);
void oce_attachhook(void *arg);
void oce_attach_ifp(struct oce_softc *sc);
int  oce_ioctl(struct ifnet *ifp, u_long command, caddr_t data);
void oce_init(void *xsc);
void oce_stop(struct oce_softc *sc);
void oce_iff(struct oce_softc *sc);
int  oce_intr(void *arg);

void oce_media_status(struct ifnet *ifp, struct ifmediareq *ifmr);
int  oce_media_change(struct ifnet *ifp);
void oce_link_status(struct oce_softc *sc);
void oce_link_event(struct oce_softc *sc,
    struct oce_async_cqe_link_state *acqe);

int  oce_get_buf(struct oce_rq *rq);
int  oce_alloc_rx_bufs(struct oce_rq *rq);
void oce_refill_rx(void *arg);

void oce_watchdog(struct ifnet *ifp);
void oce_start(struct ifnet *ifp);
int  oce_encap(struct oce_softc *sc, struct mbuf **mpp, int wq_index);
void oce_txeof(struct oce_wq *wq);

void oce_discard_rx_comp(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe);
int  oce_vtp_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe);
int  oce_port_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe);
void oce_rxeof(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe);
int  oce_start_rq(struct oce_rq *rq);
void oce_stop_rq(struct oce_rq *rq);
void oce_free_posted_rxbuf(struct oce_rq *rq);

int  oce_vid_config(struct oce_softc *sc);
void oce_set_macaddr(struct oce_softc *sc);
void oce_tick(void *arg);

#ifdef OCE_LRO
void oce_flush_lro(struct oce_rq *rq);
int  oce_init_lro(struct oce_softc *sc);
void oce_free_lro(struct oce_softc *sc);
#endif
d388 2
a389 1
struct mbuf *oce_tso(struct oce_softc *sc, struct mbuf **mpp);
d391 71
d463 35
a497 86
void oce_intr_mq(void *arg);
void oce_intr_wq(void *arg);
void oce_intr_rq(void *arg);

int  oce_init_queues(struct oce_softc *sc);
void oce_release_queues(struct oce_softc *sc);

struct oce_wq *oce_create_wq(struct oce_softc *sc, struct oce_eq *eq);
void oce_drain_wq(struct oce_wq *wq);
void oce_destroy_wq(struct oce_wq *wq);

struct oce_rq *oce_create_rq(struct oce_softc *sc, struct oce_eq *eq, int rss);
void oce_drain_rq(struct oce_rq *rq);
void oce_destroy_rq(struct oce_rq *rq);

struct oce_eq *oce_create_eq(struct oce_softc *sc);
static inline void oce_arm_eq(struct oce_eq *eq, int neqe, int rearm,
    int clearint);
void oce_drain_eq(struct oce_eq *eq);
void oce_destroy_eq(struct oce_eq *eq);

struct oce_mq *oce_create_mq(struct oce_softc *sc, struct oce_eq *eq);
void oce_drain_mq(struct oce_mq *mq);
void oce_destroy_mq(struct oce_mq *mq);

struct oce_cq *oce_create_cq(struct oce_softc *sc, struct oce_eq *eq,
    uint32_t q_len, uint32_t item_size, uint32_t is_eventable,
    uint32_t nodelay, uint32_t ncoalesce);
static inline void oce_arm_cq(struct oce_cq *cq, int ncqe, int rearm);
void oce_destroy_cq(struct oce_cq *cq);

int oce_dma_alloc(struct oce_softc *sc, bus_size_t size,
    struct oce_dma_mem *dma);
void oce_dma_free(struct oce_softc *sc, struct oce_dma_mem *dma);
#define oce_dma_sync(d, f) \
	bus_dmamap_sync((d)->tag, (d)->map, 0, (d)->map->dm_mapsize, f)

struct oce_ring *oce_create_ring(struct oce_softc *sc, int nitems,
    int isize, int maxsegs);
void oce_destroy_ring(struct oce_softc *sc, struct oce_ring *ring);
int oce_load_ring(struct oce_softc *sc, struct oce_ring *ring,
    struct phys_addr *pa_list, int max_segs);
static inline void *oce_ring_get(struct oce_ring *ring);
static inline void *oce_ring_first(struct oce_ring *ring);
static inline void *oce_ring_next(struct oce_ring *ring);

struct oce_pkt *oce_pkt_alloc(struct oce_softc *sc, size_t size, int nsegs,
    int maxsegs);
void oce_pkt_free(struct oce_softc *sc, struct oce_pkt *pkt);
static inline struct oce_pkt *oce_pkt_get(struct oce_pkt_list *lst);
static inline void oce_pkt_put(struct oce_pkt_list *lst, struct oce_pkt *pkt);

int oce_init_fw(struct oce_softc *sc);
int oce_mbox_init(struct oce_softc *sc);
int oce_mbox_dispatch(struct oce_softc *sc);
int oce_cmd(struct oce_softc *sc, int subsys, int opcode, int version,
    void *payload, int length);
void oce_first_mcc(struct oce_softc *sc);

int oce_get_fw_config(struct oce_softc *sc);
int oce_check_native_mode(struct oce_softc *sc);
int oce_create_iface(struct oce_softc *sc, uint8_t *macaddr);
int oce_config_vlan(struct oce_softc *sc, struct normal_vlan *vtags,
    int nvtags, int untagged, int promisc);
int oce_set_flow_control(struct oce_softc *sc, uint flags);
int oce_config_rss(struct oce_softc *sc, int enable);
int oce_update_mcast(struct oce_softc *sc,
    uint8_t multi[][ETHER_ADDR_LEN], int naddr);
int oce_set_promisc(struct oce_softc *sc, int enable);
int oce_get_link_status(struct oce_softc *sc);

int oce_macaddr_get(struct oce_softc *sc, uint8_t *macaddr);
int oce_macaddr_add(struct oce_softc *sc, uint8_t *macaddr, uint32_t *pmac);
int oce_macaddr_del(struct oce_softc *sc, uint32_t pmac);

int oce_new_rq(struct oce_softc *sc, struct oce_rq *rq);
int oce_new_wq(struct oce_softc *sc, struct oce_wq *wq);
int oce_new_mq(struct oce_softc *sc, struct oce_mq *mq);
int oce_new_eq(struct oce_softc *sc, struct oce_eq *eq);
int oce_new_cq(struct oce_softc *sc, struct oce_cq *cq);
int oce_destroy_queue(struct oce_softc *sc, enum qtype qtype, uint32_t qid);

static inline int oce_update_stats(struct oce_softc *sc);
int oce_stats_be2(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe);
int oce_stats_be3(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe);
int oce_stats_xe(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe);
a938 42
int
oce_intr(void *arg)
{
	struct oce_softc *sc = arg;
	struct oce_eq *eq = sc->sc_eq[0];
	struct oce_eqe *eqe;
	struct oce_cq *cq = NULL;
	int i, claimed = 0, neqe = 0;

	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTREAD);

	OCE_RING_FOREACH(eq->ring, eqe, eqe->evnt != 0) {
		eqe->evnt = 0;
		neqe++;
	}

	if (!neqe)
		goto eq_arm; /* Spurious */

	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_PREWRITE);

	claimed = 1;

 	/* Clear EQ entries, but dont arm */
	oce_arm_eq(eq, neqe, FALSE, TRUE);

	/* Process TX, RX and MCC completion queues */
	for (i = 0; i < eq->cq_valid; i++) {
		cq = eq->cq[i];
		(*cq->cq_intr)(cq->cb_arg);
	}

	/* Arm all CQs connected to this EQ */
	for (i = 0; i < eq->cq_valid; i++) {
		cq = eq->cq[i];
		oce_arm_cq(cq, 0, TRUE);
	}

eq_arm:
	oce_arm_eq(eq, 0, TRUE, FALSE);
	return (claimed);
}
d1016 170
d1310 3
a1312 2
void
oce_txeof(struct oce_wq *wq)
a1313 3
	struct oce_softc *sc = wq->sc;
	struct oce_pkt *pkt;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
d1315 10
d1326 1
a1326 5
	if ((pkt = oce_pkt_get(&wq->pkt_list)) == NULL) {
		printf("%s: missing descriptor in txeof\n",
		    sc->sc_dev.dv_xname);
		return;
	}
d1328 5
a1332 44
	wq->ring->nused -= pkt->nsegs + 1;
	bus_dmamap_sync(sc->sc_dmat, pkt->map, 0, pkt->map->dm_mapsize,
	    BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(sc->sc_dmat, pkt->map);

	m = pkt->mbuf;
	m_freem(m);
	pkt->mbuf = NULL;
	oce_pkt_put(&wq->pkt_free, pkt);

	if (ifp->if_flags & IFF_OACTIVE) {
		if (wq->ring->nused < (wq->ring->nitems / 2)) {
			ifp->if_flags &= ~IFF_OACTIVE;
			oce_start(ifp);
		}
	}
	if (wq->ring->nused == 0)
		ifp->if_timer = 0;
}

#if OCE_TSO
#if defined(INET6) || defined(INET)
struct mbuf *
oce_tso(struct oce_softc *sc, struct mbuf **mpp)
{
	struct mbuf *m;
#ifdef INET
	struct ip *ip;
#endif
#ifdef INET6
	struct ip6_hdr *ip6;
#endif
	struct ether_vlan_header *eh;
	struct tcphdr *th;
	uint16_t etype;
	int total_len = 0, ehdrlen = 0;

	m = *mpp;

	if (M_WRITABLE(m) == 0) {
		m = m_dup(*mpp, M_DONTWAIT);
		if (!m)
			return NULL;
		m_freem(*mpp);
a1377 1
#endif /* INET6 || INET */
d1380 2
a1381 2
void
oce_watchdog(struct ifnet *ifp)
d1383 7
a1389 1
	printf("%s: watchdog timeout -- resetting\n", ifp->if_xname);
d1391 4
a1394 1
	oce_init(ifp->if_softc);
d1396 2
a1397 2
	ifp->if_oerrors++;
}
d1399 1
a1399 6
void
oce_start(struct ifnet *ifp)
{
	struct oce_softc *sc = ifp->if_softc;
	struct mbuf *m;
	int pkts = 0;
d1401 1
a1401 2
	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
		return;
d1403 2
a1404 4
	for (;;) {
		IFQ_DEQUEUE(&ifp->if_snd, m);
		if (m == NULL)
			break;
d1406 5
a1410 4
		if (oce_encap(sc, &m, 0)) {
			ifp->if_flags |= IFF_OACTIVE;
			break;
		}
d1412 4
a1415 5
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
		pkts++;
d1418 3
a1420 3
	/* Set a timeout in case the chip goes out to lunch */
	if (pkts)
		ifp->if_timer = 5;
d1444 87
a1540 5
	if (!len) {
		/* partial DMA workaround for Lancer */
		oce_discard_rx_comp(rq, cqe);
		goto exit;
	}
d1649 1
a1649 1
oce_discard_rx_comp(struct oce_rq *rq, struct oce_nic_rx_cqe *cqe)
d1665 1
a1665 1
			printf("%s: missing descriptor in discard_rx_comp\n",
d1798 1
a1798 219
	oce_pkt_put(&rq->pkt_list, pkt);

	return (1);
}

int
oce_alloc_rx_bufs(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;
	int i, nbufs = 0;

	while (oce_get_buf(rq))
		nbufs++;
	if (!nbufs)
		return (0);
	for (i = nbufs / OCE_MAX_RQ_POSTS; i > 0; i--) {
		oce_write_db(sc, PD_RXULP_DB, rq->id |
		    (OCE_MAX_RQ_POSTS << 24));
		nbufs -= OCE_MAX_RQ_POSTS;
	}
	if (nbufs > 0)
		oce_write_db(sc, PD_RXULP_DB, rq->id | (nbufs << 24));
	return (1);
}

void
oce_refill_rx(void *arg)
{
	struct oce_softc *sc = arg;
	struct oce_rq *rq;
	int i, s;

	s = splnet();
	OCE_RQ_FOREACH(sc, rq, i) {
		oce_alloc_rx_bufs(rq);
		if (!rq->pending)
			timeout_add(&sc->sc_rxrefill, 1);
	}
	splx(s);
}

/* Handle the Completion Queue for receive */
void
oce_intr_rq(void *arg)
{
	struct oce_rq *rq = (struct oce_rq *)arg;
	struct oce_cq *cq = rq->cq;
	struct oce_softc *sc = rq->sc;
	struct oce_nic_rx_cqe *cqe;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int maxrx, ncqe = 0;

	maxrx = IS_XE201(sc) ? 8 : OCE_MAX_RQ_COMPL;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);

	OCE_RING_FOREACH(cq->ring, cqe, RQ_CQE_VALID(cqe) && ncqe <= maxrx) {
		if (cqe->u0.s.error == 0) {
			oce_rxeof(rq, cqe);
		} else {
			ifp->if_ierrors++;
			if (IS_XE201(sc))
				/* Lancer A0 no buffer workaround */
				oce_discard_rx_comp(rq, cqe);
			else
				/* Post L3/L4 errors to stack.*/
				oce_rxeof(rq, cqe);
		}
#ifdef OCE_LRO
		if (IF_LRO_ENABLED(ifp) && rq->lro_pkts_queued >= 16)
			oce_flush_lro(rq);
#endif
		RQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);

#ifdef OCE_LRO
	if (IF_LRO_ENABLED(ifp))
		oce_flush_lro(rq);
#endif

	if (ncqe) {
		oce_arm_cq(cq, ncqe, FALSE);
		if (rq->nitems - rq->pending > 1 && !oce_alloc_rx_bufs(rq))
			timeout_add(&sc->sc_rxrefill, 1);
	}
}

void
oce_set_macaddr(struct oce_softc *sc)
{
	uint32_t old_pmac_id = sc->sc_pmac_id;
	int status = 0;

	if (!bcmp(sc->sc_macaddr, sc->sc_ac.ac_enaddr, ETHER_ADDR_LEN))
		return;

	status = oce_macaddr_add(sc, sc->sc_ac.ac_enaddr, &sc->sc_pmac_id);
	if (!status)
		status = oce_macaddr_del(sc, old_pmac_id);
	else
		printf("%s: failed to set MAC address\n", sc->sc_dev.dv_xname);
}

void
oce_tick(void *arg)
{
	struct oce_softc *sc = arg;
	int s;

	s = splnet();

	if (oce_update_stats(sc) == 0)
		timeout_add_sec(&sc->sc_tick, 1);

	splx(s);
}

void
oce_stop(struct oce_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct oce_rq *rq;
	struct oce_wq *wq;
	struct oce_eq *eq;
	int i;

	timeout_del(&sc->sc_tick);

	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);

	/* Stop intrs and finish any bottom halves pending */
	oce_intr_disable(sc);

	/* Invalidate any pending cq and eq entries */
	OCE_EQ_FOREACH(sc, eq, i)
		oce_drain_eq(eq);
	OCE_RQ_FOREACH(sc, rq, i) {
		oce_destroy_queue(sc, QTYPE_RQ, rq->id);
		DELAY(1000);
		oce_drain_rq(rq);
		oce_free_posted_rxbuf(rq);
	}
	OCE_WQ_FOREACH(sc, wq, i)
		oce_drain_wq(wq);
}

void
oce_init(void *arg)
{
	struct oce_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct oce_eq *eq;
	struct oce_rq *rq;
	struct oce_wq *wq;
	int i;

	oce_stop(sc);

	DELAY(10);

	oce_set_macaddr(sc);

	oce_iff(sc);

	/* Enable VLAN promiscuous mode */
	if (oce_config_vlan(sc, NULL, 0, 1, 1))
		goto error;

	if (oce_set_flow_control(sc, IFM_ETH_RXPAUSE | IFM_ETH_TXPAUSE))
		goto error;

	OCE_RQ_FOREACH(sc, rq, i) {
		rq->mtu = ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN +
		    ETHER_VLAN_ENCAP_LEN;
		if (oce_new_rq(sc, rq)) {
			printf("%s: failed to create rq\n",
			    sc->sc_dev.dv_xname);
			goto error;
		}
		rq->pending	 = 0;
		rq->ring->index	 = 0;

		if (!oce_alloc_rx_bufs(rq)) {
			printf("%s: failed to allocate rx buffers\n",
			    sc->sc_dev.dv_xname);
			goto error;
		}
	}

#ifdef OCE_RSS
	/* RSS config */
	if (sc->sc_rss_enable) {
		if (oce_config_rss(sc, (uint8_t)sc->sc_if_id, 1)) {
			printf("%s: failed to configure RSS\n",
			    sc->sc_dev.dv_xname);
			goto error;
		}
	}
#endif

	OCE_RQ_FOREACH(sc, rq, i)
		oce_arm_cq(rq->cq, 0, TRUE);

	OCE_WQ_FOREACH(sc, wq, i)
		oce_arm_cq(wq->cq, 0, TRUE);

	oce_arm_cq(sc->sc_mq->cq, 0, TRUE);

	OCE_EQ_FOREACH(sc, eq, i)
		oce_arm_eq(eq, 0, TRUE, FALSE);

	if (oce_get_link_status(sc) == 0)
		oce_link_status(sc);

	ifp->if_flags |= IFF_RUNNING;
	ifp->if_flags &= ~IFF_OACTIVE;
d1800 2
a1801 1
	timeout_add_sec(&sc->sc_tick, 1);
d1803 5
a1807 1
	oce_intr_enable(sc);
d1809 12
a1820 3
	return;
error:
	oce_stop(sc);
d1824 1
a1824 1
oce_link_event(struct oce_softc *sc, struct oce_async_cqe_link_state *acqe)
d1826 11
a1836 6
	/* Update Link status */
	sc->sc_link_up = ((acqe->u0.s.link_status & ~ASYNC_EVENT_LOGICAL) ==
	    ASYNC_EVENT_LINK_UP);
	/* Update speed */
	sc->sc_link_speed = acqe->u0.s.speed;
	oce_link_status(sc);
d1883 11
d2035 16
d2053 1
d2057 6
a2062 2
	if (wq->id >= 0)
		oce_destroy_queue(sc, QTYPE_WQ, wq->id);
d2141 16
d2159 1
d2163 6
a2168 2
	if (rq->id >= 0)
		oce_destroy_queue(sc, QTYPE_RQ, rq->id);
d2209 30
d2242 1
d2245 6
a2250 2
	if (eq->id >= 0)
		oce_destroy_queue(sc, QTYPE_EQ, eq->id);
d2304 16
d2322 1
d2325 6
a2330 2
	if (mq->id >= 0)
		oce_destroy_queue(sc, QTYPE_MQ, mq->id);
d2342 3
a2344 3
 * @@param q_len		length of completion queue
 * @@param item_size	size of completion queue items
 * @@param is_eventable	event table
d2350 2
a2351 3
oce_create_cq(struct oce_softc *sc, struct oce_eq *eq, uint32_t q_len,
    uint32_t item_size, uint32_t eventable, uint32_t nodelay,
    uint32_t ncoalesce)
d2359 1
a2359 1
	cq->ring = oce_create_ring(sc, q_len, item_size, 4);
d2367 1
a2367 1
	cq->nitems = q_len;
d2386 1
d2389 7
a2395 2
	if (cq->ring != NULL) {
		oce_destroy_queue(sc, QTYPE_CQ, cq->id);
a2396 1
	}
a2400 15
 * @@brief		Function to arm an EQ so that it can generate events
 * @@param eq		pointer to event queue structure
 * @@param neqe		number of EQEs to arm
 * @@param rearm		rearm bit enable/disable
 * @@param clearint	bit to clear the interrupt condition because of which
 *			EQEs are generated
 */
static inline void
oce_arm_eq(struct oce_eq *eq, int neqe, int rearm, int clearint)
{
	oce_write_db(eq->sc, PD_EQ_DB, eq->id | PD_EQ_DB_EVENT |
	    (clearint << 9) | (neqe << 16) | (rearm << 29));
}

/**
a2411 68
/**
 * @@brief		function to cleanup the eqs used during stop
 * @@param eq		pointer to event queue structure
 * @@returns		the number of EQs processed
 */
void
oce_drain_eq(struct oce_eq *eq)
{
	struct oce_eqe *eqe;
	int neqe = 0;

	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(eq->ring, eqe, eqe->evnt != 0) {
		eqe->evnt = 0;
		neqe++;
	}
	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_eq(eq, neqe, FALSE, TRUE);
}

void
oce_drain_wq(struct oce_wq *wq)
{
	struct oce_cq *cq = wq->cq;
	struct oce_nic_tx_cqe *cqe;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, WQ_CQE_VALID(cqe)) {
		WQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_drain_mq(struct oce_mq *mq)
{
	struct oce_cq *cq = mq->cq;
	struct oce_mq_cqe *cqe;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, MQ_CQE_VALID(cqe)) {
		MQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_cq(cq, ncqe, FALSE);
}

void
oce_drain_rq(struct oce_rq *rq)
{
	struct oce_nic_rx_cqe *cqe;
	struct oce_cq *cq = rq->cq;
	int ncqe = 0;

	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTREAD);
	OCE_RING_FOREACH(cq->ring, cqe, RQ_CQE_VALID(cqe)) {
		RQ_CQE_INVALIDATE(cqe);
		ncqe++;
	}
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
	oce_arm_cq(cq, ncqe, FALSE);
}

d3187 16
a3445 39
}

int
oce_destroy_queue(struct oce_softc *sc, enum qtype qtype, uint32_t qid)
{
	struct mbx_destroy_common_mq cmd;
	int opcode, subsys;

	switch (qtype) {
	case QTYPE_CQ:
		opcode = OPCODE_COMMON_DESTROY_CQ;
		subsys = SUBSYS_COMMON;
		break;
	case QTYPE_EQ:
		opcode = OPCODE_COMMON_DESTROY_EQ;
		subsys = SUBSYS_COMMON;
		break;
	case QTYPE_MQ:
		opcode = OPCODE_COMMON_DESTROY_MQ;
		subsys = SUBSYS_COMMON;
		break;
	case QTYPE_RQ:
		opcode = OPCODE_NIC_DELETE_RQ;
		subsys = SUBSYS_NIC;
		break;
	case QTYPE_WQ:
		opcode = OPCODE_NIC_DELETE_WQ;
		subsys = SUBSYS_NIC;
		break;
	default:
		return (EINVAL);
	}

	bzero(&cmd, sizeof(cmd));

	cmd.params.req.id = htole16(qid);

	return (oce_cmd(sc, subsys, opcode, OCE_MBX_VER_V0, &cmd,
	    sizeof(cmd)));
@


1.55
log
@don't sync dma memory for the whole ring when updating a single
entry but rather sync the whole ring once done with individual
entries; use proper dma sync flags as well
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.54 2012/11/09 20:31:43 mikeb Exp $	*/
d373 1
a379 2

int  oce_pci_alloc(struct oce_softc *sc, struct pci_attach_args *pa);
a380 2
void oce_intr_enable(struct oce_softc *sc);
void oce_intr_disable(struct oce_softc *sc);
d398 2
a399 2
int  oce_cqe_vtp_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe);
int  oce_cqe_portid_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe);
a408 1
#if defined(INET6) || defined(INET)
d410 1
a412 1
void oce_rx_flush_lro(struct oce_rq *rq);
d415 1
a415 2
struct mbuf *oce_tso_setup(struct oce_softc *sc, struct mbuf **mpp);
#endif
d657 137
a845 1
#if defined(INET6) || defined(INET)
a852 1
#endif
a987 136
int
oce_pci_alloc(struct oce_softc *sc, struct pci_attach_args *pa)
{
	pcireg_t memtype, reg;

	/* setup the device config region */
	if (ISSET(sc->sc_flags, OCE_F_BE2))
		reg = OCE_BAR_CFG_BE2;
	else
		reg = OCE_BAR_CFG;

	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, reg);
	if (pci_mapreg_map(pa, reg, memtype, 0, &sc->sc_cfg_iot,
	    &sc->sc_cfg_ioh, NULL, &sc->sc_cfg_size,
	    IS_BE(sc) ? 0 : 32768)) {
		printf(": can't find cfg mem space\n");
		return (ENXIO);
	}

	/*
	 * Read the SLI_INTF register and determine whether we
	 * can use this port and its features
	 */
	reg = pci_conf_read(pa->pa_pc, pa->pa_tag, OCE_INTF_REG_OFFSET);
	if (OCE_SLI_SIGNATURE(reg) != OCE_INTF_VALID_SIG) {
		printf(": invalid signature\n");
		goto fail_1;
	}
	if (OCE_SLI_REVISION(reg) != OCE_INTF_SLI_REV4) {
		printf(": unsupported SLI revision\n");
		goto fail_1;
	}
	if (OCE_SLI_IFTYPE(reg) == OCE_INTF_IF_TYPE_1)
		SET(sc->sc_flags, OCE_F_MBOX_ENDIAN_RQD);
	if (OCE_SLI_HINT1(reg) == OCE_INTF_FUNC_RESET_REQD)
		SET(sc->sc_flags, OCE_F_RESET_RQD);

	/* Lancer has one BAR (CFG) but BE3 has three (CFG, CSR, DB) */
	if (IS_BE(sc)) {
		/* set up CSR region */
		reg = OCE_BAR_CSR;
		memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, reg);
		if (pci_mapreg_map(pa, reg, memtype, 0, &sc->sc_csr_iot,
		    &sc->sc_csr_ioh, NULL, &sc->sc_csr_size, 0)) {
			printf(": can't find csr mem space\n");
			goto fail_1;
		}

		/* set up DB doorbell region */
		reg = OCE_BAR_DB;
		memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, reg);
		if (pci_mapreg_map(pa, reg, memtype, 0, &sc->sc_db_iot,
		    &sc->sc_db_ioh, NULL, &sc->sc_db_size, 0)) {
			printf(": can't find csr mem space\n");
			goto fail_2;
		}
	} else {
		sc->sc_csr_iot = sc->sc_db_iot = sc->sc_cfg_iot;
		sc->sc_csr_ioh = sc->sc_db_ioh = sc->sc_cfg_ioh;
	}

	return (0);

fail_2:
	bus_space_unmap(sc->sc_csr_iot, sc->sc_csr_ioh, sc->sc_csr_size);
fail_1:
	bus_space_unmap(sc->sc_cfg_iot, sc->sc_cfg_ioh, sc->sc_cfg_size);
	return (ENXIO);
}

static inline uint32_t
oce_read_cfg(struct oce_softc *sc, bus_size_t off)
{
	bus_space_barrier(sc->sc_cfg_iot, sc->sc_cfg_ioh, off, 4,
	    BUS_SPACE_BARRIER_READ);
	return (bus_space_read_4(sc->sc_cfg_iot, sc->sc_cfg_ioh, off));
}

static inline uint32_t
oce_read_csr(struct oce_softc *sc, bus_size_t off)
{
	bus_space_barrier(sc->sc_csr_iot, sc->sc_csr_ioh, off, 4,
	    BUS_SPACE_BARRIER_READ);
	return (bus_space_read_4(sc->sc_csr_iot, sc->sc_csr_ioh, off));
}

static inline uint32_t
oce_read_db(struct oce_softc *sc, bus_size_t off)
{
	bus_space_barrier(sc->sc_db_iot, sc->sc_db_ioh, off, 4,
	    BUS_SPACE_BARRIER_READ);
	return (bus_space_read_4(sc->sc_db_iot, sc->sc_db_ioh, off));
}

static inline void
oce_write_cfg(struct oce_softc *sc, bus_size_t off, uint32_t val)
{
	bus_space_write_4(sc->sc_cfg_iot, sc->sc_cfg_ioh, off, val);
	bus_space_barrier(sc->sc_cfg_iot, sc->sc_cfg_ioh, off, 4,
	    BUS_SPACE_BARRIER_WRITE);
}

static inline void
oce_write_csr(struct oce_softc *sc, bus_size_t off, uint32_t val)
{
	bus_space_write_4(sc->sc_csr_iot, sc->sc_csr_ioh, off, val);
	bus_space_barrier(sc->sc_csr_iot, sc->sc_csr_ioh, off, 4,
	    BUS_SPACE_BARRIER_WRITE);
}

static inline void
oce_write_db(struct oce_softc *sc, bus_size_t off, uint32_t val)
{
	bus_space_write_4(sc->sc_db_iot, sc->sc_db_ioh, off, val);
	bus_space_barrier(sc->sc_db_iot, sc->sc_db_ioh, off, 4,
	    BUS_SPACE_BARRIER_WRITE);
}

void
oce_intr_enable(struct oce_softc *sc)
{
	uint32_t reg;

	reg = oce_read_cfg(sc, PCI_INTR_CTRL);
	oce_write_cfg(sc, PCI_INTR_CTRL, reg | HOSTINTR_MASK);
}

void
oce_intr_disable(struct oce_softc *sc)
{
	uint32_t reg;

	reg = oce_read_cfg(sc, PCI_INTR_CTRL);
	oce_write_cfg(sc, PCI_INTR_CTRL, reg & ~HOSTINTR_MASK);
}

d1078 1
a1078 5
#if defined(INET6) || defined(INET)
		m = oce_tso_setup(sc, mpp);
#else
		m = NULL;	/* Huh? */
#endif
d1225 1
a1225 1
oce_tso_setup(struct oce_softc *sc, struct mbuf **mpp)
d1421 1
a1421 1
		if (!oce_cqe_portid_valid(sc, cqe)) {
d1433 1
a1433 1
		if (oce_cqe_vtp_valid(sc, cqe)) {
a1454 1
#if defined(INET6) || defined(INET)
a1466 1
#endif /* OCE_LRO */
d1510 1
a1510 1
oce_cqe_vtp_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe)
d1522 1
a1522 1
oce_cqe_portid_valid(struct oce_softc *sc, struct oce_nic_rx_cqe *cqe)
a1534 1
#if defined(INET6) || defined(INET)
d1536 1
a1536 1
oce_rx_flush_lro(struct oce_rq *rq)
a1585 1
#endif /* INET6 || INET */
a1697 1
#if defined(INET6) || defined(INET)
d1699 1
a1699 2
			oce_rx_flush_lro(rq);
#endif
a1707 1
#if defined(INET6) || defined(INET)
d1709 1
a1709 2
		oce_rx_flush_lro(rq);
#endif
@


1.54
log
@merge if_ocevar.h and if_oce.c; do some minor cleanup while here
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.53 2012/11/09 18:58:17 mikeb Exp $	*/
a369 1
#define IF_LSO_ENABLED(ifp)	ISSET((ifp)->if_capabilities, IFCAP_TSO4)
a565 1
	sc->sc_rss_enable 	 = 0;
a790 1
	struct ifnet *ifp = &sc->sc_ac.ac_if;
d792 1
a827 1
		oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_PREWRITE);
d834 2
a1357 1

a1358 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
d1361 1
a1710 1

a1716 1

a1717 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
d1721 2
a1921 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_PREWRITE);
d1925 2
a2375 1
		oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTWRITE);
d2378 1
a2391 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d2394 1
a2407 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d2410 1
a2423 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d2426 1
@


1.53
log
@cleanup oce_encap
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.52 2012/11/09 18:56:31 mikeb Exp $	*/
d100 271
a370 1
#include <dev/pci/if_ocevar.h>
a382 1
int  oce_alloc_intr(struct oce_softc *sc, struct pci_attach_args *pa);
d458 2
d546 2
d552 1
a552 1
		SET(sc->flags, OCE_F_BE2);
d556 1
a556 1
		SET(sc->flags, OCE_F_BE3);
d559 1
a559 1
		SET(sc->flags, OCE_F_XE201);
d563 1
a563 1
	sc->dmat = pa->pa_dmat;
d567 3
a569 4
	sc->rss_enable 	 = 0;
	sc->tx_ring_size = OCE_TX_RING_SIZE;
	sc->rx_ring_size = OCE_RX_RING_SIZE;
	sc->rx_frag_size = OCE_RQ_BUF_SIZE;
d572 1
a572 1
	if (oce_dma_alloc(sc, sizeof(struct oce_bmbx), &sc->bsmbx)) {
d590 1
a590 1
	if (ISSET(sc->flags, OCE_F_BE3)) {
d595 1
a595 1
	if (oce_macaddr_get(sc, sc->macaddr)) {
d599 1
a599 1
	bcopy(sc->macaddr, sc->arpcom.ac_enaddr, ETHER_ADDR_LEN);
d612 16
a627 1
	if (oce_alloc_intr(sc, pa))
d629 2
d633 1
a633 1
		goto fail_1;
d639 1
a639 1
		goto fail_2;
d642 2
a643 2
	timeout_set(&sc->timer, oce_tick, sc);
	timeout_set(&sc->rxrefill, oce_refill_rx, sc);
d647 1
a647 1
	printf(", address %s\n", ether_sprintf(sc->arpcom.ac_enaddr));
d652 1
a652 1
fail_2:
d654 2
a655 2
	ether_ifdetach(&sc->arpcom.ac_if);
	if_detach(&sc->arpcom.ac_if);
d658 2
d661 1
a661 1
	oce_dma_free(sc, &sc->bsmbx);
d671 1
a671 1
	oce_arm_cq(sc->mq->cq, 0, TRUE);
d678 1
a678 1
	oce_arm_eq(sc->eq[0], 0, TRUE, FALSE);
d690 1
a690 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d692 4
a695 3
	ifmedia_init(&sc->media, IFM_IMASK, oce_media_change, oce_media_status);
	ifmedia_add(&sc->media, IFM_ETHER | IFM_AUTO, 0, NULL);
	ifmedia_set(&sc->media, IFM_ETHER | IFM_AUTO);
d697 1
a697 1
	strlcpy(ifp->if_xname, sc->dev.dv_xname, IFNAMSIZ);
d704 1
a704 1
	IFQ_SET_MAXLEN(&ifp->if_snd, sc->tx_ring_size - 1);
d708 1
a708 1
	m_clsetwms(ifp, MCLBYTES, 8, sc->rx_ring_size);
d747 1
a747 1
			arp_ifinit(&sc->arpcom, ifa);
d771 1
a771 1
		error = ifmedia_ioctl(ifp, ifr, &sc->media, command);
d774 1
a774 1
		error = ether_ioctl(ifp, &sc->arpcom, command, data);
d786 1
a786 1
	return error;
d793 2
a794 2
	struct ifnet *ifp = &sc->arpcom.ac_if;
	struct arpcom *ac = &sc->arpcom;
d806 1
a806 1
		ETHER_FIRST_MULTI(step, &sc->arpcom, enm);
d821 1
a821 1
	struct oce_eq *eq = sc->eq[0];
d865 1
a865 1
	if (ISSET(sc->flags, OCE_F_BE2))
d871 2
a872 2
	if (pci_mapreg_map(pa, reg, memtype, 0, &sc->cfg_iot,
	    &sc->cfg_ioh, NULL, &sc->cfg_size,
d892 1
a892 1
		SET(sc->flags, OCE_F_MBOX_ENDIAN_RQD);
d894 1
a894 1
		SET(sc->flags, OCE_F_RESET_RQD);
d901 2
a902 2
		if (pci_mapreg_map(pa, reg, memtype, 0, &sc->csr_iot,
		    &sc->csr_ioh, NULL, &sc->csr_size, 0)) {
d910 2
a911 2
		if (pci_mapreg_map(pa, reg, memtype, 0, &sc->db_iot,
		    &sc->db_ioh, NULL, &sc->db_size, 0)) {
d916 2
a917 2
		sc->csr_iot = sc->db_iot = sc->cfg_iot;
		sc->csr_ioh = sc->db_ioh = sc->cfg_ioh;
d923 1
a923 1
	bus_space_unmap(sc->csr_iot, sc->csr_ioh, sc->csr_size);
d925 1
a925 1
	bus_space_unmap(sc->cfg_iot, sc->cfg_ioh, sc->cfg_size);
d932 1
a932 1
	bus_space_barrier(sc->cfg_iot, sc->cfg_ioh, off, 4,
d934 1
a934 1
	return (bus_space_read_4(sc->cfg_iot, sc->cfg_ioh, off));
d940 1
a940 1
	bus_space_barrier(sc->csr_iot, sc->csr_ioh, off, 4,
d942 1
a942 1
	return (bus_space_read_4(sc->csr_iot, sc->csr_ioh, off));
d948 1
a948 1
	bus_space_barrier(sc->db_iot, sc->db_ioh, off, 4,
d950 1
a950 1
	return (bus_space_read_4(sc->db_iot, sc->db_ioh, off));
d956 2
a957 2
	bus_space_write_4(sc->cfg_iot, sc->cfg_ioh, off, val);
	bus_space_barrier(sc->cfg_iot, sc->cfg_ioh, off, 4,
d964 2
a965 2
	bus_space_write_4(sc->csr_iot, sc->csr_ioh, off, val);
	bus_space_barrier(sc->csr_iot, sc->csr_ioh, off, 4,
d972 2
a973 2
	bus_space_write_4(sc->db_iot, sc->db_ioh, off, val);
	bus_space_barrier(sc->db_iot, sc->db_ioh, off, 4,
a976 29
int
oce_alloc_intr(struct oce_softc *sc, struct pci_attach_args *pa)
{
	const char *intrstr = NULL;
	pci_intr_handle_t ih;

	sc->intr_count = 1;

	/* We allocate a single interrupt resource */
	if (pci_intr_map_msi(pa, &ih) != 0 &&
	    pci_intr_map(pa, &ih) != 0) {
		printf(": couldn't map interrupt\n");
		return (ENXIO);
	}

	intrstr = pci_intr_string(pa->pa_pc, ih);
	if (pci_intr_establish(pa->pa_pc, ih, IPL_NET, oce_intr, sc,
	    sc->dev.dv_xname) == NULL) {
		printf(": couldn't establish interrupt\n");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		return (ENXIO);
	}
	printf(": %s", intrstr);

	return (0);
}

d998 1
a998 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1001 1
a1001 1
	if (sc->link_up)
d1006 1
a1006 1
		switch (sc->link_speed) {
d1037 1
a1037 1
	if (!sc->link_up) {
d1044 1
a1044 1
	switch (sc->link_speed) {
d1059 1
a1059 1
	if (sc->flow_flags & IFM_ETH_RXPAUSE)
d1061 1
a1061 1
	if (sc->flow_flags & IFM_ETH_TXPAUSE)
d1074 1
a1074 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1076 1
a1076 1
	struct oce_wq *wq = sc->wq[wq_index];
d1078 3
a1080 3
	struct oce_nic_hdr_wqe *nichdr;
	struct oce_nic_frag_wqe *nicfrag;
	int i, nwqe, rc;
d1098 2
a1099 2
	rc = bus_dmamap_load_mbuf(sc->dmat, pkt->map, m, BUS_DMA_NOWAIT);
	if (rc == EFBIG) {
d1101 2
a1102 1
		    bus_dmamap_load_mbuf(sc->dmat, pkt->map, m, BUS_DMA_NOWAIT))
d1105 1
a1105 1
	} else if (rc != 0)
d1119 1
a1119 1
		bus_dmamap_unload(sc->dmat, pkt->map);
d1123 1
a1123 1
	bus_dmamap_sync(sc->dmat, pkt->map, 0, pkt->map->dm_mapsize,
d1127 13
a1139 12
	nichdr = oce_ring_get(wq->ring);
	bzero(nichdr, sizeof(*nichdr));

	nichdr->u0.s.complete = 1;
	nichdr->u0.s.event = 1;
	nichdr->u0.s.crc = 1;
	nichdr->u0.s.forward = 0;
	nichdr->u0.s.ipcs = (m->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT) ? 1 : 0;
	nichdr->u0.s.udpcs = (m->m_pkthdr.csum_flags & M_UDP_CSUM_OUT) ? 1 : 0;
	nichdr->u0.s.tcpcs = (m->m_pkthdr.csum_flags & M_TCP_CSUM_OUT) ? 1 : 0;
	nichdr->u0.s.num_wqe = nwqe;
	nichdr->u0.s.total_length = m->m_pkthdr.len;
d1143 2
a1144 2
		nichdr->u0.s.vlan = 1; /* Vlan present */
		nichdr->u0.s.vlan_tag = m->m_pkthdr.ether_vtag;
d1151 2
a1152 2
			nichdr->u0.s.lso = 1;
			nichdr->u0.s.lso_mss  = m->m_pkthdr.tso_segsz;
d1155 1
a1155 1
			nichdr->u0.s.ipcs = 1;
d1164 1
d1166 5
a1170 5
		nicfrag = oce_ring_get(wq->ring);
		bzero(nicfrag, sizeof(*nicfrag));
		nicfrag->u0.s.frag_pa_hi = ADDR_HI(pkt->map->dm_segs[i].ds_addr);
		nicfrag->u0.s.frag_pa_lo = ADDR_LO(pkt->map->dm_segs[i].ds_addr);
		nicfrag->u0.s.frag_len = pkt->map->dm_segs[i].ds_len;
d1174 2
a1175 2
		nicfrag = oce_ring_get(wq->ring);
		bzero(nicfrag, sizeof(*nicfrag));
d1204 1
a1204 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1208 2
a1209 1
		printf("%s: missing descriptor in txeof\n", sc->dev.dv_xname);
d1214 1
a1214 1
	bus_dmamap_sync(sc->dmat, pkt->map, 0, pkt->map->dm_mapsize,
d1216 1
a1216 1
	bus_dmamap_unload(sc->dmat, pkt->map);
d1287 2
a1288 1
		total_len = ehdrlen + sizeof(struct ip6_hdr) + (th->th_off << 2);
a1357 2
		DW_SWAP((uint32_t *) cqe, sizeof(oce_wq_cqe));

d1373 1
a1373 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1387 1
a1387 1
		vtag = BSWAP_16(cqe->u0.s.vlan_tag);
d1394 1
a1394 1
			    sc->dev.dv_xname);
d1398 1
a1398 1
		bus_dmamap_sync(sc->dmat, pkt->map, 0, pkt->map->dm_mapsize,
d1400 1
a1400 1
		bus_dmamap_unload(sc->dmat, pkt->map);
d1412 1
a1412 1
			/* first fragment, fill out much of the packet header */
d1446 1
a1446 1
			if (sc->function_mode & FNM_FLEX10_MODE) {
d1452 1
a1452 1
			} else if (sc->pvid != (vtag & VLAN_VID_MASK))  {
d1454 2
a1455 2
				 * In UMC mode generally pvid will be striped by
				 * hw. But in some cases we have seen it comes
d1511 1
a1511 1
			    sc->dev.dv_xname);
d1514 1
a1514 1
		bus_dmamap_sync(sc->dmat, pkt->map, 0, pkt->map->dm_mapsize,
d1516 1
a1516 1
		bus_dmamap_unload(sc->dmat, pkt->map);
d1528 1
a1528 1
	if (IS_BE(sc) && ISSET(sc->flags, OCE_F_BE3_NATIVE)) {
d1540 1
a1540 1
	if (IS_BE(sc) && ISSET(sc->flags, OCE_F_BE3_NATIVE)) {
d1542 1
a1542 1
		if (sc->port_id != cqe_v1->u0.s.port)
d1554 1
a1554 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1576 2
a1577 2
	for (i = 0; i < sc->nrqs; i++) {
		lro = &sc->rq[i]->lro;
d1583 1
a1583 1
		lro->ifp = &sc->arpcom.ac_if;
d1595 2
a1596 2
	for (i = 0; i < sc->nrqs; i++) {
		lro = &sc->rq[i]->lro;
d1608 1
a1608 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1623 1
a1623 1
	if (bus_dmamap_load_mbuf(sc->dmat, pkt->map, pkt->mbuf,
d1631 1
a1631 1
	bus_dmamap_sync(sc->dmat, pkt->map, 0, pkt->map->dm_mapsize,
a1639 1
	DW_SWAP(u32ptr(rqe), sizeof(struct oce_nic_rqe));
d1678 1
a1678 1
	for_all_rq_queues(sc, rq, i) {
d1681 1
a1681 1
			timeout_add(&sc->rxrefill, 1);
d1694 2
a1695 2
	struct ifnet *ifp = &sc->arpcom.ac_if;
	int max_rsp, ncqe = 0;
d1697 1
a1697 1
	max_rsp = IS_XE201(sc) ? 8 : OCE_MAX_RSP_HANDLED;
d1701 1
a1701 3
	OCE_RING_FOREACH(cq->ring, cqe, RQ_CQE_VALID(cqe) && ncqe <= max_rsp) {
		DW_SWAP((uint32_t *)cqe, sizeof(oce_rq_cqe));

d1736 1
a1736 1
			timeout_add(&sc->rxrefill, 1);
d1743 1
a1743 1
	uint32_t old_pmac_id = sc->pmac_id;
d1746 1
a1746 1
	if (!bcmp(sc->macaddr, sc->arpcom.ac_enaddr, ETHER_ADDR_LEN))
d1749 1
a1749 1
	status = oce_macaddr_add(sc, sc->arpcom.ac_enaddr, &sc->pmac_id);
d1753 1
a1753 1
		printf("%s: failed to set MAC address\n", sc->dev.dv_xname);
d1765 1
a1765 1
		timeout_add_sec(&sc->timer, 1);
d1773 1
a1773 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1779 1
a1779 1
	timeout_del(&sc->timer);
d1787 1
a1787 1
	for_all_eq_queues(sc, eq, i)
d1789 1
a1789 1
	for_all_rq_queues(sc, rq, i) {
d1795 1
a1795 1
	for_all_wq_queues(sc, wq, i)
d1803 1
a1803 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d1824 1
a1824 1
	for_all_rq_queues(sc, rq, i) {
d1828 2
a1829 1
			printf("%s: failed to create rq\n", sc->dev.dv_xname);
d1837 1
a1837 1
			    sc->dev.dv_xname);
d1844 2
a1845 2
	if (sc->rss_enable) {
		if (oce_config_rss(sc, (uint8_t)sc->if_id, 1)) {
d1847 1
a1847 1
			    sc->dev.dv_xname);
d1853 1
a1853 1
	for_all_rq_queues(sc, rq, i)
d1856 1
a1856 1
	for_all_wq_queues(sc, wq, i)
d1859 1
a1859 1
	oce_arm_cq(sc->mq->cq, 0, TRUE);
d1861 1
a1861 1
	for_all_eq_queues(sc, eq, i)
d1870 1
a1870 1
	timeout_add_sec(&sc->timer, 1);
d1883 1
a1883 1
	sc->link_up = ((acqe->u0.s.link_status & ~ASYNC_EVENT_LOGICAL) ==
d1886 1
a1886 1
	sc->link_speed = acqe->u0.s.speed;
a1904 1
		DW_SWAP((uint32_t *) cqe, sizeof(oce_mq_cqe));
d1918 2
a1919 1
					sc->pvid = gcqe->tag & VLAN_VID_MASK;
d1921 1
a1921 1
					sc->pvid = 0;
d1930 1
a1930 1
		oce_arm_cq(cq, ncqe, FALSE /* TRUE */);
d1940 2
a1941 2
	sc->nrqs = 1;
	sc->nwqs = 1;
d1944 1
a1944 1
	if (oce_create_iface(sc, sc->macaddr))
d1948 3
a1950 3
	for (i = 0; i < sc->intr_count; i++) {
		sc->eq[i] = oce_create_eq(sc);
		if (!sc->eq[i])
d1955 3
a1957 3
	for_all_wq_queues(sc, wq, i) {
		sc->wq[i] = oce_create_wq(sc, sc->eq[i]);
		if (!sc->wq[i])
d1962 4
a1965 4
	for_all_rq_queues(sc, rq, i) {
		sc->rq[i] = oce_create_rq(sc, sc->eq[i > 0 ? i - 1 : 0],
		    i > 0 ? sc->rss_enable : 0);
		if (!sc->rq[i])
d1970 2
a1971 2
	sc->mq = oce_create_mq(sc, sc->eq[0]);
	if (!sc->mq)
d1988 1
a1988 1
	for_all_rq_queues(sc, rq, i) {
d1990 1
a1990 1
			oce_destroy_rq(sc->rq[i]);
d1993 1
a1993 1
	for_all_wq_queues(sc, wq, i) {
d1995 1
a1995 1
			oce_destroy_wq(sc->wq[i]);
d1998 2
a1999 2
	if (sc->mq)
		oce_destroy_mq(sc->mq);
d2001 1
a2001 1
	for_all_eq_queues(sc, eq, i) {
d2003 1
a2003 1
			oce_destroy_eq(sc->eq[i]);
d2020 1
a2020 1
	if (sc->tx_ring_size < 256 || sc->tx_ring_size > 2048)
d2027 1
a2027 1
	wq->ring = oce_create_ring(sc, sc->tx_ring_size, NIC_WQE_SIZE, 8);
d2045 1
a2045 1
	wq->nitems = sc->tx_ring_size;
d2050 1
a2050 1
	for (i = 0; i < sc->tx_ring_size / 2; i++) {
a2104 3
	if (ilog2(sc->rx_frag_size) <= 0)
		return (NULL);

d2106 1
a2106 1
	if (sc->rx_ring_size != 1024)
d2113 1
a2113 1
	rq->ring = oce_create_ring(sc, sc->rx_ring_size,
d2131 2
a2132 2
	rq->nitems = sc->rx_ring_size;
	rq->fragsize = sc->rx_frag_size;
d2138 2
a2139 2
	for (i = 0; i < sc->rx_ring_size; i++) {
		pkt = oce_pkt_alloc(sc, sc->rx_frag_size, 1, sc->rx_frag_size);
d2320 1
a2320 1
	sc->cq[sc->ncqs++] = cq;
d2439 1
a2439 1
		bus_dmamap_sync(sc->dmat, pkt->map, 0, pkt->map->dm_mapsize,
d2441 1
a2441 1
		bus_dmamap_unload(sc->dmat, pkt->map);
d2458 1
a2458 1
	dma->tag = sc->dmat;
d2462 2
a2463 1
		printf("%s: failed to allocate DMA handle", sc->dev.dv_xname);
d2470 2
a2471 1
		printf("%s: failed to allocate DMA memory", sc->dev.dv_xname);
d2478 1
a2478 1
		printf("%s: failed to map DMA memory", sc->dev.dv_xname);
d2485 1
a2485 1
		printf("%s: failed to load DMA memory", sc->dev.dv_xname);
d2547 1
a2547 1
	dma->tag = sc->dmat;
d2551 2
a2552 1
		printf("%s: failed to allocate DMA handle", sc->dev.dv_xname);
d2559 2
a2560 1
		printf("%s: failed to allocate DMA memory", sc->dev.dv_xname);
d2567 1
a2567 1
		printf("%s: failed to map DMA memory", sc->dev.dv_xname);
d2604 1
a2604 1
		printf("%s: failed to load a ring map\n", sc->dev.dv_xname);
d2609 1
a2609 1
		printf("%s: too many segments", sc->dev.dv_xname);
d2656 1
a2656 1
	if (bus_dmamap_create(sc->dmat, size, nsegs, maxsegs, 0,
d2669 2
a2670 2
		bus_dmamap_unload(sc->dmat, pkt->map);
		bus_dmamap_destroy(sc->dmat, pkt->map);
d2729 1
a2729 1
			if (ISSET(sc->flags, OCE_F_RESET_RQD)) {
d2767 1
a2767 1
	pa = (uint32_t)((uint64_t)sc->bsmbx.paddr >> 34);
d2775 1
a2775 1
	pa = (uint32_t)((uint64_t)sc->bsmbx.paddr >> 4) & 0x3fffffff;
d2783 1
a2783 1
	oce_dma_sync(&sc->bsmbx, BUS_DMASYNC_POSTWRITE);
d2789 1
a2789 1
	oce_dma_sync(&sc->bsmbx, BUS_DMASYNC_PREREAD);
d2801 1
a2801 1
	struct oce_bmbx *bmbx = OCE_MEM_KVA(&sc->bsmbx);
d2804 1
a2804 1
	if (!ISSET(sc->flags, OCE_F_MBOX_ENDIAN_RQD))
d2824 1
a2824 1
	struct oce_bmbx *bmbx = OCE_MEM_KVA(&sc->bsmbx);
d2837 1
a2837 1
	oce_dma_sync(&sc->bsmbx, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d2861 1
a2861 1
		hdr->u0.req.timeout = 2 * MBX_TIMEOUT_SEC;
d2863 1
a2863 1
		hdr->u0.req.timeout = MBX_TIMEOUT_SEC;
d2874 1
a2874 1
		    "%spayload lenght %d\n", sc->dev.dv_xname, subsys,
d2893 1
a2893 1
	struct oce_mq *mq = sc->mq;
d2906 1
a2906 1
	hdr->u0.req.timeout = MBX_TIMEOUT_SEC;
d2929 2
a2930 2
	sc->port_id	  = cmd.params.rsp.port_id;
	sc->function_mode = cmd.params.rsp.function_mode;
d2953 1
a2953 1
		SET(sc->flags, OCE_F_BE3_NATIVE);
d2985 1
a2985 1
	if (sc->rss_enable)
d3004 1
a3004 1
	sc->if_id = letoh32(cmd.params.rsp.if_id);
d3007 1
a3007 1
		sc->pmac_id = letoh32(cmd.params.rsp.pmac_id);
d3029 1
a3029 1
	cmd.params.req.if_id = sc->if_id;
d3071 2
a3072 2
	sc->flow_flags  = cmd.rx_flow_control ? IFM_ETH_RXPAUSE : 0;
	sc->flow_flags |= cmd.tx_flow_control ? IFM_ETH_TXPAUSE : 0;
d3097 1
a3097 1
	cmd.params.req.if_id = htole32(sc->if_id);
d3112 3
a3114 3
	for (i = 0, j = 0; j < sc->nrqs; j++) {
		if (sc->rq[j]->cfg.is_rss_queue)
			tbl[i++] = sc->rq[j]->rss_cpuid;
d3142 1
a3142 1
	cmd.params.req.if_id = sc->if_id;
d3166 1
a3166 1
	req->if_id = sc->if_id;
d3196 1
a3196 1
	sc->link_up = (letoh32(cmd.params.rsp.logical_link_status) ==
d3200 1
a3200 1
		sc->link_speed = cmd.params.rsp.mac_speed;
d3202 1
a3202 1
		sc->link_speed = 0;
d3234 1
a3234 1
	cmd.params.req.if_id = htole16(sc->if_id);
d3251 1
a3251 1
	cmd.params.req.if_id = htole16(sc->if_id);
d3280 1
a3280 1
	cmd.params.req.if_id = htole32(sc->if_id);
d3312 1
a3312 1
		cmd.params.req.if_id = sc->if_id;
d3494 1
a3494 1
	struct ifnet *ifp = &sc->arpcom.ac_if;
d3498 1
a3498 1
	if (ISSET(sc->flags, OCE_F_BE2))
d3500 1
a3500 1
	else if (ISSET(sc->flags, OCE_F_BE3))
d3507 6
a3512 6
	ifp->if_ierrors += (rxe > sc->rx_errors) ?
	    rxe - sc->rx_errors : sc->rx_errors - rxe;
	sc->rx_errors = rxe;
	ifp->if_oerrors += (txe > sc->tx_errors) ?
	    txe - sc->tx_errors : sc->tx_errors - txe;
	sc->tx_errors = txe;
d3535 1
a3535 1
	ps = &rs->port[sc->port_id];
d3545 1
a3545 1
	if (sc->if_id)
d3574 1
a3574 1
	ps = &rs->port[sc->port_id];
d3601 1
a3601 1
	cmd.params.req.port_number = sc->if_id;
@


1.52
log
@stop passing if_id around
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.51 2012/11/09 18:53:04 mikeb Exp $	*/
d848 1
a848 1
		/*Dummy required only for BE3.*/
d852 3
a854 1
	if (nwqe >= RING_NUM_FREE(wq->ring)) {
d864 1
a864 4
	nichdr->u0.dw[0] = 0;
	nichdr->u0.dw[1] = 0;
	nichdr->u0.dw[2] = 0;
	nichdr->u0.dw[3] = 0;
d901 1
a901 1
		nicfrag->u0.s.rsvd0 = 0;
d909 1
a909 4
		nicfrag->u0.dw[0] = 0;
		nicfrag->u0.dw[1] = 0;
		nicfrag->u0.dw[2] = 0;
		nicfrag->u0.dw[3] = 0;
@


1.51
log
@improve flow control code
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.50 2012/11/09 18:40:12 mikeb Exp $	*/
d166 1
a166 2
struct oce_rq *oce_create_rq(struct oce_softc *sc, struct oce_eq *eq,
    uint32_t if_id, uint32_t rss);
d218 1
a218 1
int oce_config_rss(struct oce_softc *sc, uint32_t if_id, int enable);
d225 2
a226 4
int oce_macaddr_add(struct oce_softc *sc, uint8_t *macaddr,
    uint32_t if_id, uint32_t *pmac_id);
int oce_macaddr_del(struct oce_softc *sc, uint32_t if_id,
    uint32_t pmac_id);
d1490 1
a1490 2
	status = oce_macaddr_add(sc, sc->arpcom.ac_enaddr, sc->if_id,
	    &sc->pmac_id);
d1492 1
a1492 1
		status = oce_macaddr_del(sc, sc->if_id, old_pmac_id);
d1703 2
a1704 2
		sc->rq[i] = oce_create_rq(sc, sc->eq[i == 0 ? 0 : i - 1],
		    sc->if_id, (i == 0) ? 0 : sc->rss_enable);
a1833 1
 * @@param if_id		interface identifier index
d1838 1
a1838 2
oce_create_rq(struct oce_softc *sc, struct oce_eq *eq, uint32_t if_id,
    uint32_t rss)
a2819 1
 * @@param if_id 	interface id to read the address from
d2824 1
a2824 1
oce_config_rss(struct oce_softc *sc, uint32_t if_id, int enable)
d2836 1
a2836 1
	cmd.params.req.if_id = htole32(if_id);
d2966 1
a2966 2
oce_macaddr_add(struct oce_softc *sc, uint8_t *enaddr, uint32_t if_id,
    uint32_t *pmac_id)
d2973 1
a2973 1
	cmd.params.req.if_id = htole16(if_id);
d2979 1
a2979 1
		*pmac_id = letoh32(cmd.params.rsp.pmac_id);
d2984 1
a2984 1
oce_macaddr_del(struct oce_softc *sc, uint32_t if_id, uint32_t pmac_id)
d2990 2
a2991 2
	cmd.params.req.if_id = htole16(if_id);
	cmd.params.req.pmac_id = htole32(pmac_id);
@


1.50
log
@To be able to receive ethernet packets with VLAN tags oce_set_promisc
should not disable VLAN promiscuous mode set up by oce_config_vlan.
Move VLAN and Flow Control configuration to oce_init so that it would
be rerun every time we plumb the interface.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.49 2012/11/08 19:48:37 mikeb Exp $	*/
d218 1
a218 1
int oce_set_flow_control(struct oce_softc *sc, uint32_t flow_control);
a300 1
	sc->flow_control = OCE_FC_TX | OCE_FC_RX;
d799 3
a801 1
	if (sc->flow_control & OCE_FC_TX)
a802 2
	if (sc->flow_control & OCE_FC_RX)
		ifmr->ifm_active |= IFM_FLOW | IFM_ETH_RXPAUSE;
d1566 1
a1566 1
	if (oce_set_flow_control(sc, sc->flow_control))
d2790 1
a2790 1
 * @@param flow_control	flow control flags to set
d2794 1
a2794 1
oce_set_flow_control(struct oce_softc *sc, uint32_t flow_control)
d2797 11
d2811 7
a2817 4
	if (flow_control & OCE_FC_TX)
		cmd.tx_flow_control = 1;
	if (flow_control & OCE_FC_RX)
		cmd.rx_flow_control = 1;
d2819 1
a2819 2
	return (oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_FLOW_CONTROL,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd)));
@


1.49
log
@make link state update code more comprehensible by using some ideas from myx(4)
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.48 2012/11/08 18:56:54 mikeb Exp $	*/
d216 2
a217 2
int oce_config_vlan(struct oce_softc *sc, uint32_t if_id,
    struct normal_vlan *vtag_arr, int vtag_cnt, int untagged, int promisc);
d1563 7
d2713 1
a2713 1
	uint32_t capab_flags, capab_en_flags;
d2717 3
a2719 1
	capab_flags = OCE_CAPAB_FLAGS;
d2722 1
a2722 1
	capab_en_flags = OCE_CAPAB_ENABLE;
d2724 1
a2724 1
	if (IS_XE201(sc)) {
d2726 2
a2727 2
		capab_en_flags &= ~MBX_RX_IFACE_FLAGS_PASS_L3L4_ERR;
		capab_flags &= ~MBX_RX_IFACE_FLAGS_PASS_L3L4_ERR;
d2732 1
a2732 5
		capab_en_flags |= MBX_RX_IFACE_FLAGS_RSS;
	else {
		capab_en_flags &= ~MBX_RX_IFACE_FLAGS_RSS;
		capab_flags &= ~MBX_RX_IFACE_FLAGS_RSS;
	}
d2737 2
a2738 2
	cmd.params.req.cap_flags = htole32(capab_flags);
	cmd.params.req.enable_flags = htole32(capab_en_flags);
a2754 14
	sc->nifs++;

	sc->if_cap_flags = capab_en_flags;

	/* Enable VLAN Promisc on HW */
	err = oce_config_vlan(sc, (uint8_t)sc->if_id, NULL, 0, 1, 1);
	if (err)
		return (err);

	/* set default flow control */
	err = oce_set_flow_control(sc, sc->flow_control);
	if (err)
		return (err);

d2761 2
a2762 3
 * @@param if_id 	interface identifier index
 * @@param vtag_arr	array of vlan tags
 * @@param vtag_cnt	number of elements in array
d2768 2
a2769 2
oce_config_vlan(struct oce_softc *sc, uint32_t if_id,
    struct normal_vlan *vtag_arr, int vtag_cnt, int untagged, int promisc)
d2775 1
a2775 1
	cmd.params.req.if_id = if_id;
d2778 1
a2778 1
	cmd.params.req.num_vlans = vtag_cnt;
d2781 2
a2782 2
		bcopy(vtag_arr, cmd.params.req.tags.normal_vlans,
			vtag_cnt * sizeof(struct normal_vlan));
d2901 1
a2901 2
	req->iface_flags_mask = MBX_RX_IFACE_FLAGS_PROMISC |
				MBX_RX_IFACE_FLAGS_VLAN_PROMISC;
d2903 3
a2905 1
		req->iface_flags = req->iface_flags_mask;
@


1.48
log
@hardware supports mtu values from 256 up to 9000;
figured out the hard way, linux driver agrees
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.47 2012/11/08 18:26:17 mikeb Exp $	*/
d119 1
a119 1
void oce_update_link_status(struct oce_softc *sc);
d737 1
a737 1
oce_update_link_status(struct oce_softc *sc)
d740 1
a740 1
	int speed = 0;
d742 18
a759 18
	if (sc->link_status) {
		if (sc->link_active == 0) {
			switch (sc->link_speed) {
			case 1: /* 10 Mbps */
				speed = 10;
				break;
			case 2: /* 100 Mbps */
				speed = 100;
				break;
			case 3: /* 1 Gbps */
				speed = 1000;
				break;
			case 4: /* 10 Gbps */
				speed = 10000;
				break;
			}
			sc->link_active = 1;
			ifp->if_baudrate = speed * 1000000ULL;
d761 4
a764 14
		if (!LINK_STATE_IS_UP(ifp->if_link_state)) {
			ifp->if_link_state = LINK_STATE_FULL_DUPLEX;
			if_link_state_change(ifp);
		}
	} else {
		if (sc->link_active == 1) {
			ifp->if_baudrate = 0;
			sc->link_active = 0;
		}
		if (ifp->if_link_state != LINK_STATE_DOWN) {
			ifp->if_link_state = LINK_STATE_DOWN;
			if_link_state_change(ifp);
		}
	}
d776 1
a776 1
		oce_update_link_status(sc);
d778 1
a778 1
	if (!sc->link_status) {
d1603 1
a1603 1
		oce_update_link_status(sc);
d1621 2
a1622 6
	if ((acqe->u0.s.link_status & ~ASYNC_EVENT_LOGICAL) ==
	     ASYNC_EVENT_LINK_UP)
		sc->link_status = ASYNC_EVENT_LINK_UP;
	else
		sc->link_status = ASYNC_EVENT_LINK_DOWN;

d1625 1
a1625 3
	sc->qos_link_speed = (uint32_t) acqe->u0.s.qos_link_speed * 10;

	oce_update_link_status(sc);
a2929 1
	struct link_status link;
d2939 2
a2940 3
	bcopy(&cmd.params.rsp, &link, sizeof(struct link_status));
	link.logical_link_status = letoh32(link.logical_link_status);
	link.qos_link_speed = letoh16(link.qos_link_speed);
d2942 2
a2943 7
	if (link.logical_link_status == NTWK_LOGICAL_LINK_UP)
		sc->link_status = NTWK_LOGICAL_LINK_UP;
	else
		sc->link_status = NTWK_LOGICAL_LINK_DOWN;

	if (link.mac_speed > 0 && link.mac_speed < 5)
		sc->link_speed = link.mac_speed;
a2945 4

	sc->duplex = link.mac_duplex;

	sc->qos_link_speed = (uint32_t )link.qos_link_speed * 10;
@


1.47
log
@When halting the rx engine wait 1ms after destroying the queue in
firmware then drain the completion queue and only afterwards deal
with posted buffers so that the firmware wouldn't decide to DMA
something into the freed cluster.  Logic from the Linux driver.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.46 2012/11/08 17:59:08 mikeb Exp $	*/
d474 1
a474 1
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ifp->if_hardmtu)
@


1.46
log
@minor style cleanup, improve the mailbox timeout printf
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.45 2012/11/08 17:48:20 mikeb Exp $	*/
d1547 2
a1549 1
		oce_drain_rq(rq);
@


1.45
log
@Hide stats calculation ugliness inside oce_update_stats and
don't schedule another update if the one at hand fails.
s/oce_local_timer/oce_tick/ while here (:
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.44 2012/11/07 19:43:33 mikeb Exp $	*/
d1448 1
a1448 1
	int max_rsp, ncqe = 0, rq_buffers_used = 0;
d1490 1
a1490 2
		rq_buffers_used = rq->nitems - rq->pending;
		if (rq_buffers_used > 1 && !oce_alloc_rx_bufs(rq))
d1653 1
a1653 1
	int evt_type, optype, ncqe = 0;
d1660 1
a1660 1
			evt_type = cqe->u0.s.event_type;
d1662 1
a1662 1
			if (evt_type  == ASYNC_EVENT_CODE_LINK_STATE) {
d1666 1
a1666 1
			} else if ((evt_type == ASYNC_EVENT_GRP5) &&
a2543 2
	if (err)
		printf("%s: mailbox timeout\n", sc->dev.dv_xname);
d2627 4
a2630 1
		printf("%s: mailbox error %d\n", sc->dev.dv_xname, err);
@


1.44
log
@minor tweaks to the ioctl code
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.43 2012/11/07 12:46:49 mikeb Exp $	*/
d142 1
a142 1
void oce_local_timer(void *arg);
d238 1
a238 1
int oce_update_stats(struct oce_softc *sc, u_int64_t *rxe, u_int64_t *txe);
d357 1
a357 1
	timeout_set(&sc->timer, oce_local_timer, sc);
d1514 1
a1514 1
oce_local_timer(void *arg)
a1516 2
	struct ifnet *ifp = &sc->arpcom.ac_if;
	u_int64_t rxe, txe;
d1521 2
a1522 8
	if (!(oce_update_stats(sc, &rxe, &txe))) {
		ifp->if_ierrors += (rxe > sc->rx_errors) ?
		    rxe - sc->rx_errors : sc->rx_errors - rxe;
		sc->rx_errors = rxe;
		ifp->if_oerrors += (txe > sc->tx_errors) ?
		    txe - sc->tx_errors : sc->tx_errors - txe;
		sc->tx_errors = txe;
	}
a1524 2

	timeout_add_sec(&sc->timer, 1);
d3261 26
a3392 10
}

int
oce_update_stats(struct oce_softc *sc, uint64_t *rxe, uint64_t *txe)
{
	if (ISSET(sc->flags, OCE_F_BE2))
		return (oce_stats_be2(sc, rxe, txe));
	if (ISSET(sc->flags, OCE_F_BE3))
		return (oce_stats_be3(sc, rxe, txe));
	return (oce_stats_xe(sc, rxe, txe));
@


1.43
log
@do not depend on IFCAP_CSUM flags set when reading rx checksumming
results from the hardware
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.42 2012/11/07 12:43:35 mikeb Exp $	*/
a461 11
	case SIOCGIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &sc->media, command);
		break;
	case SIOCSIFMTU:
		if (ifr->ifr_mtu > OCE_MAX_MTU)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu) {
			ifp->if_mtu = ifr->ifr_mtu;
			oce_init(sc);
		}
		break;
d472 12
@


1.42
log
@we still need to query the firmware for a couple of values after all
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.41 2012/11/05 20:05:39 mikeb Exp $	*/
d1166 4
a1169 10
			if (IF_CSUM_ENABLED(ifp)) {
				if (cqe->u0.s.ip_cksum_pass) {
					if (!cqe->u0.s.ip_ver) { /* IPV4 */
						pkt->mbuf->m_pkthdr.csum_flags =
						    M_IPV4_CSUM_IN_OK;
					}
				}
				if (cqe->u0.s.l4_cksum_pass) {
					pkt->mbuf->m_pkthdr.csum_flags |=
					    M_TCP_CSUM_IN_OK | M_UDP_CSUM_IN_OK;
d1171 4
@


1.41
log
@Steal SIMPLEQ-based packet descriptor managing code from myx(4)
to simplify a whole bunch of things. And despite this being the
main purpose of the commit I'm also sneaking in loads of minor
and unrelated cleanup since separating it out would be just too
much work. Enjoy!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.40 2012/11/03 00:23:25 mikeb Exp $	*/
d213 1
d317 5
d2681 42
a3036 23
}

int
oce_check_native_mode(struct oce_softc *sc)
{
	struct mbx_common_set_function_cap cmd;
	int err;

	bzero(&cmd, sizeof(cmd));

	cmd.params.req.valid_capability_flags = CAP_SW_TIMESTAMPS |
	    CAP_BE3_NATIVE_ERX_API;
	cmd.params.req.capability_flags = CAP_BE3_NATIVE_ERX_API;

	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_FUNCTIONAL_CAPS,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	if (err)
		return (err);

	if (cmd.params.rsp.capability_flags & CAP_BE3_NATIVE_ERX_API)
		SET(sc->flags, OCE_F_BE3_NATIVE);

	return (0);
@


1.40
log
@s/OCE_DMAPTR/OCE_MEM_KVA/ and don't require a type
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.39 2012/11/03 00:05:41 brynet Exp $	*/
d68 1
d72 2
d111 1
a111 1
int  oce_pci_alloc(struct oce_softc *sc);
d113 1
a113 1
int  oce_alloc_intr(struct oce_softc *sc);
d151 1
a151 1
struct mbuf * oce_tso_setup(struct oce_softc *sc, struct mbuf **mpp);
d162 1
a162 2
struct oce_wq *oce_create_wq(struct oce_softc *sc, struct oce_eq *eq,
    uint32_t q_len);
d167 1
a167 1
    uint32_t if_id, uint32_t q_len, uint32_t frag_size, uint32_t rss);
d191 2
a192 2
struct oce_ring *oce_create_ring(struct oce_softc *sc, int q_len,
    int num_entries, int max_segs);
d200 6
a210 1

d242 8
d292 2
a293 2
	sc->pa = *pa;
	if (oce_pci_alloc(sc))
d299 1
a299 1
	sc->rq_frag_size = OCE_RQ_BUF_SIZE;
d327 10
a336 3
	sc->nrqs = 1;
	sc->nwqs = 1;
	sc->intr_count = 1;
d338 1
a338 1
	if (oce_alloc_intr(sc))
d362 1
d565 1
a565 1
oce_pci_alloc(struct oce_softc *sc)
a566 1
	struct pci_attach_args *pa = &sc->pa;
d683 1
a683 1
oce_alloc_intr(struct oce_softc *sc)
a685 1
	struct pci_attach_args *pa = &sc->pa;
d688 2
d821 1
a821 1
	struct oce_packet_desc *pd;
d824 1
a824 1
	int i, nwqe, out, rc;
d839 1
a839 4
	out = wq->packets_out + 1;
	if (out == OCE_WQ_PACKET_ARRAY_SIZE)
		out = 0;
	if (out == wq->packets_in)
d842 1
a842 3
	pd = &wq->pckts[wq->packets_out];

	rc = bus_dmamap_load_mbuf(wq->tag, pd->map, m, BUS_DMA_NOWAIT);
d845 1
a845 1
		    bus_dmamap_load_mbuf(wq->tag, pd->map, m, BUS_DMA_NOWAIT))
d851 1
a851 1
	pd->nsegs = pd->map->dm_nsegs;
d853 1
a853 1
	nwqe = pd->nsegs + 1;
d860 1
a860 1
		bus_dmamap_unload(wq->tag, pd->map);
d864 1
a864 1
	bus_dmamap_sync(wq->tag, pd->map, 0, pd->map->dm_mapsize,
d866 1
a866 2
	pd->mbuf = m;
	wq->packets_out = out;
d907 1
a907 1
	for (i = 0; i < pd->nsegs; i++) {
d910 3
a912 3
		nicfrag->u0.s.frag_pa_hi = ADDR_HI(pd->map->dm_segs[i].ds_addr);
		nicfrag->u0.s.frag_pa_lo = ADDR_LO(pd->map->dm_segs[i].ds_addr);
		nicfrag->u0.s.frag_len = pd->map->dm_segs[i].ds_len;
d915 1
a915 1
	if (nwqe > (pd->nsegs + 1)) {
d922 1
a922 1
		pd->nsegs++;
d925 2
d937 2
d947 2
a948 2
	struct oce_softc *sc = (struct oce_softc *) wq->sc;
	struct oce_packet_desc *pd;
a950 1
	uint32_t in;
d952 4
a955 2
	if (wq->packets_out == wq->packets_in)
		printf("%s: WQ transmit descriptor missing\n");
d957 2
a958 8
	in = wq->packets_in + 1;
	if (in == OCE_WQ_PACKET_ARRAY_SIZE)
		in = 0;

	pd = &wq->pckts[wq->packets_in];
	wq->packets_in = in;
	wq->ring->nused -= (pd->nsegs + 1);
	bus_dmamap_sync(wq->tag, pd->map, 0, pd->map->dm_mapsize,
d960 1
a960 1
	bus_dmamap_unload(wq->tag, pd->map);
d962 1
a962 1
	m = pd->mbuf;
d964 2
a965 1
	pd->mbuf = NULL;
d1116 2
a1117 2
	struct oce_softc *sc = (struct oce_softc *)rq->sc;
	struct oce_packet_desc *pd;
a1120 1
	uint32_t out;
d1137 2
a1138 2
		if (rq->packets_out == rq->packets_in) {
			printf("%s: RQ transmit descriptor missing\n",
d1140 1
a1141 5
		out = rq->packets_out + 1;
		if (out == OCE_RQ_PACKET_ARRAY_SIZE)
			out = 0;
		pd = &rq->pckts[rq->packets_out];
		rq->packets_out = out;
d1143 1
a1143 1
		bus_dmamap_sync(rq->tag, pd->map, 0, pd->map->dm_mapsize,
d1145 1
a1145 1
		bus_dmamap_unload(rq->tag, pd->map);
d1148 2
a1149 2
		frag_len = (len > rq->cfg.frag_size) ? rq->cfg.frag_size : len;
		pd->mbuf->m_len = frag_len;
d1153 3
a1155 3
			pd->mbuf->m_flags &= ~M_PKTHDR;
			tail->m_next = pd->mbuf;
			tail = pd->mbuf;
d1158 2
a1159 2
			pd->mbuf->m_pkthdr.len = len;
			pd->mbuf->m_pkthdr.csum_flags = 0;
d1163 1
a1163 1
						pd->mbuf->m_pkthdr.csum_flags =
d1168 1
a1168 1
					pd->mbuf->m_pkthdr.csum_flags |=
d1172 1
a1172 1
			m = tail = pd->mbuf;
d1174 2
a1175 1
		pd->mbuf = NULL;
d1216 3
a1218 6
		if (IF_LRO_ENABLED(sc) &&
		    !(m->m_flags & M_VLANTAG) &&
		    (cqe->u0.s.ip_cksum_pass) &&
		    (cqe->u0.s.l4_cksum_pass) &&
		    (!cqe->u0.s.ip_ver)       &&
		    (rq->lro.lro_cnt != 0)) {
d1243 3
a1245 4
	uint32_t out, i = 0;
	struct oce_packet_desc *pd;
	struct oce_softc *sc = (struct oce_softc *) rq->sc;
	int num_frags = cqe->u0.s.num_fragments;
d1256 2
a1257 2
		if (rq->packets_out == rq->packets_in) {
			printf("%s: RQ transmit descriptor missing\n",
d1259 1
d1261 1
a1261 7
		out = rq->packets_out + 1;
		if (out == OCE_RQ_PACKET_ARRAY_SIZE)
			out = 0;
		pd = &rq->pckts[rq->packets_out];
		rq->packets_out = out;

		bus_dmamap_sync(rq->tag, pd->map, 0, pd->map->dm_mapsize,
d1263 1
a1263 1
		bus_dmamap_unload(rq->tag, pd->map);
d1265 2
a1266 1
		m_freem(pd->mbuf);
d1300 2
a1303 1
	struct oce_softc *sc = (struct oce_softc *) rq->sc;
d1305 1
a1305 1
	if (!IF_LRO_ENABLED(sc))
d1354 1
a1354 1
	struct oce_softc *sc = (struct oce_softc *)rq->sc;
d1356 1
a1356 1
	struct oce_packet_desc *pd;
a1357 1
	int in = rq->packets_in + 1;
d1359 2
a1360 6
	if (in == OCE_RQ_PACKET_ARRAY_SIZE)
		in = 0;
	if (in == rq->packets_out)
		return (0);	/* no more room */

	pd = &rq->pckts[rq->packets_in];
d1362 3
a1364 2
	pd->mbuf = MCLGETI(NULL, M_DONTWAIT, ifp, MCLBYTES);
	if (pd->mbuf == NULL)
d1366 1
d1368 1
a1368 1
	pd->mbuf->m_len = pd->mbuf->m_pkthdr.len = MCLBYTES;
d1370 5
a1374 3
	if (bus_dmamap_load_mbuf(rq->tag, pd->map, pd->mbuf, BUS_DMA_NOWAIT)) {
		m_freem(pd->mbuf);
		pd->mbuf = NULL;
d1378 1
a1378 3
	rq->packets_in = in;

	bus_dmamap_sync(rq->tag, pd->map, 0, pd->map->dm_mapsize,
d1385 2
a1386 2
	rqe->u0.s.frag_pa_hi = ADDR_HI(pd->map->dm_segs[0].ds_addr);
	rqe->u0.s.frag_pa_lo = ADDR_LO(pd->map->dm_segs[0].ds_addr);
d1393 2
d1401 1
a1401 1
	struct oce_softc *sc = (struct oce_softc *)rq->sc;
d1466 1
a1466 1
		if (IF_LRO_ENABLED(sc) && rq->lro_pkts_queued >= 16)
d1478 1
a1478 1
	if (IF_LRO_ENABLED(sc))
d1485 1
a1485 1
		rq_buffers_used = OCE_RQ_PACKET_ARRAY_SIZE - rq->pending;
d1579 1
a1579 1
		rq->cfg.mtu = ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN +
a1586 2
		rq->packets_in	 = 0;
		rq->packets_out	 = 0;
d1699 3
d1715 1
a1715 1
		sc->wq[i] = oce_create_wq(sc, sc->eq[i], sc->tx_ring_size);
d1723 1
a1723 2
		    sc->if_id, sc->rx_ring_size, sc->rq_frag_size,
		    (i == 0) ? 0 : sc->rss_enable);
a1741 1
	int i = 0;
d1745 1
a1768 1
 * @@param qlen		number of entries in the queue
d1772 1
a1772 1
oce_create_wq(struct oce_softc *sc, struct oce_eq *eq, uint32_t q_len)
d1776 1
d1779 1
a1779 1
	if (q_len < 256 || q_len > 2048)
d1786 1
a1786 1
	wq->ring = oce_create_ring(sc, q_len, NIC_WQE_SIZE, 8);
a1801 1
	wq->tag = sc->pa.pa_dmat;
d1804 9
a1812 9
	wq->cfg.q_len = q_len;
	wq->cfg.wq_type = NIC_WQ_TYPE_STANDARD;
	wq->cfg.eqd = OCE_DEFAULT_EQD;
	wq->cfg.nbufs = 2 * wq->cfg.q_len;

	for (i = 0; i < OCE_WQ_PACKET_ARRAY_SIZE; i++) {
		if (bus_dmamap_create(wq->tag, OCE_MAX_TX_SIZE,
		    OCE_MAX_TX_ELEMENTS, PAGE_SIZE, 0, BUS_DMA_NOWAIT,
		    &wq->pckts[i].map)) {
d1816 1
d1836 1
a1836 1
	int i;
d1844 2
a1845 6
	for (i = 0; i < OCE_WQ_PACKET_ARRAY_SIZE; i++) {
		if (wq->pckts[i].map != NULL) {
			bus_dmamap_unload(wq->tag, wq->pckts[i].map);
			bus_dmamap_destroy(wq->tag, wq->pckts[i].map);
		}
	}
a1853 2
 * @@param q_len		length of receive queue
 * @@param frag_size	size of an receive queue fragment
d1859 1
a1859 1
    uint32_t q_len, uint32_t frag_size, uint32_t rss)
d1863 1
d1866 1
a1866 1
	if (ilog2(frag_size) <= 0)
d1870 1
a1870 1
	if (q_len != 1024)
d1877 2
a1878 1
	rq->ring = oce_create_ring(sc, q_len, sizeof(struct oce_nic_rqe), 2);
a1893 1
	rq->tag = sc->pa.pa_dmat;
d1895 10
a1904 8
	rq->cfg.if_id = if_id;
	rq->cfg.q_len = q_len;
	rq->cfg.frag_size = frag_size;
	rq->cfg.is_rss_queue = rss;

	for (i = 0; i < OCE_RQ_PACKET_ARRAY_SIZE; i++) {
		if (bus_dmamap_create(rq->tag, frag_size, 1, frag_size, 0,
		    BUS_DMA_NOWAIT, &rq->pckts[i].map)) {
d1908 1
d1926 1
a1926 1
	int i;
d1934 2
a1935 8
	for (i = 0; i < OCE_RQ_PACKET_ARRAY_SIZE; i++) {
		if (rq->pckts[i].map != NULL) {
			bus_dmamap_unload(rq->tag, rq->pckts[i].map);
			bus_dmamap_destroy(rq->tag, rq->pckts[i].map);
		}
		if (rq->pckts[i].mbuf)
			m_freem(rq->pckts[i].mbuf);
	}
d1957 3
a1959 3
	eq->cfg.q_len = EQ_LEN_1024;	/* length of event queue */
	eq->cfg.item_size = EQE_SIZE_4; /* size of a queue item */
	eq->cfg.eqd = OCE_DEFAULT_EQD;	/* event queue delay */
d2011 1
a2011 1
	mq->cfg.q_len = 128;
d2073 4
a2076 5
	cq->cfg.q_len = q_len;
	cq->cfg.item_size = item_size;
	cq->cfg.nodelay = (uint8_t) nodelay;
	cq->cfg.ncoalesce = ncoalesce;
	cq->cfg.eventable = eventable;
d2199 2
a2200 1
	struct oce_packet_desc *pd;
d2202 2
a2203 3
	while (rq->pending) {
		pd = &rq->pckts[rq->packets_out];
		bus_dmamap_sync(rq->tag, pd->map, 0, pd->map->dm_mapsize,
d2205 4
a2208 4
		bus_dmamap_unload(rq->tag, pd->map);
		if (pd->mbuf != NULL) {
			m_freem(pd->mbuf);
			pd->mbuf = NULL;
d2210 2
a2211 7

		if ((rq->packets_out + 1) == OCE_RQ_PACKET_ARRAY_SIZE)
			rq->packets_out = 0;
		else
			rq->packets_out++;

                rq->pending--;
d2222 1
a2222 1
	dma->tag = sc->pa.pa_dmat;
d2291 1
a2291 1
oce_create_ring(struct oce_softc *sc, int q_len, int item_size, int max_segs)
d2295 1
a2295 1
	bus_size_t size = q_len * item_size;
d2298 1
a2298 1
	if (size > max_segs * PAGE_SIZE)
d2305 2
a2306 2
	ring->isize = item_size;
	ring->nitems = q_len;
d2309 2
a2310 2
	dma->tag = sc->pa.pa_dmat;
	rc = bus_dmamap_create(dma->tag, size, max_segs, PAGE_SIZE, 0,
d2317 1
a2317 1
	rc = bus_dmamem_alloc(dma->tag, size, 0, 0, &dma->segs, max_segs,
d2357 1
a2357 1
    struct phys_addr *pa_list, int max_segs)
d2360 1
a2360 2
	bus_dma_segment_t *segs;
	int i, nsegs;
d2368 1
a2368 3
	segs = dma->map->dm_segs;
	nsegs = dma->map->dm_nsegs;
	if (nsegs > max_segs) {
d2376 3
a2378 3
	for (i = 0; i < nsegs; i++) {
		pa_list[i].lo = ADDR_LO(segs[i].ds_addr);
		pa_list[i].hi = ADDR_HI(segs[i].ds_addr);
d2381 1
a2381 1
	return (nsegs);
d2408 47
d3030 1
a3030 1
		cmd.params.req.frag_size = rq->cfg.frag_size / 2048;
d3033 1
a3033 1
		cmd.params.req.frag_size = ilog2(rq->cfg.frag_size);
d3037 2
a3038 2
	cmd.params.req.max_frame_size = htole16(rq->cfg.mtu);
	cmd.params.req.is_rss_queue = htole32(rq->cfg.is_rss_queue);
d3069 1
a3069 1
	cmd.params.req.nic_wq_type = wq->cfg.wq_type;
d3071 1
a3071 1
	cmd.params.req.wq_size = ilog2(wq->cfg.q_len) + 1;
d3105 1
a3105 1
	ctx->v0.ring_size = ilog2(mq->cfg.q_len) + 1;
d3137 2
a3138 2
	cmd.params.req.ctx.size = (eq->cfg.item_size == 4) ? 0 : 1;
	cmd.params.req.ctx.count = ilog2(eq->cfg.q_len / 256);
d3140 1
a3140 1
	cmd.params.req.ctx.delay_mult = htole32(eq->cfg.eqd);
d3173 1
a3173 1
		ctx->v2.eventable = cq->cfg.eventable;
d3175 3
a3177 3
		ctx->v2.count = ilog2(cq->cfg.q_len / 256);
		ctx->v2.nodelay = cq->cfg.nodelay;
		ctx->v2.coalesce_wm = cq->cfg.ncoalesce;
d3181 1
a3181 1
			if (cq->cfg.q_len > (4*1024)-1)
d3184 1
a3184 1
				ctx->v2.cqe_count = cq->cfg.q_len;
d3188 1
a3188 1
		ctx->v0.eventable = cq->cfg.eventable;
d3190 3
a3192 3
		ctx->v0.count = ilog2(cq->cfg.q_len / 256);
		ctx->v0.nodelay = cq->cfg.nodelay;
		ctx->v0.coalesce_wm = cq->cfg.ncoalesce;
@


1.39
log
@Obligatory second oce commit for tonight. No binary change.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.38 2012/11/02 23:34:57 mikeb Exp $	*/
d1960 3
a1962 3
	eq->cfg.q_len = EQ_LEN_1024;		/* length of event queue */
	eq->cfg.item_size = EQE_SIZE_4; 	/* size of a queue item */
	eq->cfg.cur_eqd = OCE_DEFAULT_EQD;	/* event queue delay */
d2528 1
a2528 1
	struct oce_bmbx *bmbx = OCE_DMAPTR(&sc->bsmbx, struct oce_bmbx);
d2551 1
a2551 1
	struct oce_bmbx *bmbx = OCE_DMAPTR(&sc->bsmbx, struct oce_bmbx);
d2561 1
a2561 1
		epayload = OCE_DMAPTR(&sgl, char);
d2576 1
a2576 1
		hdr = OCE_DMAPTR(&sgl, struct mbx_hdr);
d3105 1
a3105 1
	cmd.params.req.ctx.delay_mult = htole32(eq->cfg.cur_eqd);
@


1.38
log
@Introduce better and simpler producer/consumer queue iterator
implementation that is usable for both producer (rq, wq, mq)
and consumer (eq, cq) rings.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.37 2012/10/31 20:15:43 mikeb Exp $	*/
d1178 1
a1178 1
		/* This determies if vlan tag is valid */
@


1.37
log
@minor style cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.36 2012/10/30 17:38:23 mikeb Exp $	*/
d188 1
a188 1
void oce_destroy_ring(struct oce_softc *sc, struct oce_ring *ring);
d191 1
d194 3
d510 1
a510 4
	for (;;) {
		eqe = RING_GET_CONSUMER_ITEM_VA(eq->ring, struct oce_eqe);
		if (eqe->evnt == 0)
			break;
a512 1
		RING_GET(eq->ring, 1);
d524 1
a524 1
	/* Process TX, RX and MCC. But dont arm CQ */
d851 1
a851 1
	nichdr = RING_GET_PRODUCER_ITEM_VA(wq->ring, struct oce_nic_hdr_wqe);
d888 1
a888 2
	RING_PUT(wq->ring, 1);
	wq->ring->num_used++;
d891 1
a891 2
		nicfrag = RING_GET_PRODUCER_ITEM_VA(wq->ring,
			struct oce_nic_frag_wqe);
d896 1
a896 3
		pd->wqe_idx = wq->ring->pidx;
		RING_PUT(wq->ring, 1);
		wq->ring->num_used++;
d899 1
a899 2
		nicfrag = RING_GET_PRODUCER_ITEM_VA(wq->ring,
			struct oce_nic_frag_wqe);
d904 1
a904 3
		pd->wqe_idx = wq->ring->pidx;
		RING_PUT(wq->ring, 1);
		wq->ring->num_used++;
d941 1
a941 1
	wq->ring->num_used -= (pd->nsegs + 1);
d951 1
a951 1
		if (wq->ring->num_used < (wq->ring->num_items / 2)) {
d956 1
a956 1
	if (wq->ring->num_used == 0)
d1083 1
a1083 2
	cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_nic_tx_cqe);
	while (cqe->u0.dw[3]) {
a1085 4
		wq->ring->cidx = cqe->u0.s.wqe_index + 1;
		if (wq->ring->cidx >= wq->ring->num_items)
			wq->ring->cidx -= wq->ring->num_items;

d1088 1
a1088 2
		cqe->u0.dw[3] = 0;
		RING_GET(cq->ring, 1);
a1089 2
		cqe =
		    RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_nic_tx_cqe);
a1091 1

d1381 1
a1381 1
	rqe = RING_GET_PRODUCER_ITEM_VA(rq->ring, struct oce_nic_rqe);
a1384 1
	RING_PUT(rq->ring, 1);
d1438 3
a1440 1
	int ncqe = 0, rq_buffers_used = 0;
d1443 2
a1444 2
	cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_nic_rx_cqe);
	while (cqe->u0.dw[2]) {
a1446 1
		RING_GET(rq->ring, 1);
a1457 1
		cqe->u0.dw[2] = 0;
d1466 1
a1466 1
		RING_GET(cq->ring, 1);
a1467 2
		cqe =
		    RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_nic_rx_cqe);
a1468 2
		if (ncqe >= (IS_XE201(sc) ? 8 : OCE_MAX_RSP_HANDLED))
			break;
d1581 1
a1581 2
		rq->ring->cidx	 = 0;
		rq->ring->pidx	 = 0;
d1659 2
a1660 2
	cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_mq_cqe);
	while (cqe->u0.dw[3]) {
d1680 1
a1680 2
		cqe->u0.dw[3] = 0;
		RING_GET(cq->ring, 1);
a1681 1
		cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_mq_cqe);
a1818 3
	wq->ring->cidx = 0;
	wq->ring->pidx = 0;

d2144 1
a2144 5

	for (;;) {
		eqe = RING_GET_CONSUMER_ITEM_VA(eq->ring, struct oce_eqe);
		if (eqe->evnt == 0)
			break;
a2146 1
		RING_GET(eq->ring, 1);
a2148 1

d2160 2
a2161 6

	for (;;) {
		cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_nic_tx_cqe);
		if (cqe->u0.dw[3] == 0)
			break;
		cqe->u0.dw[3] = 0;
a2162 1
		RING_GET(cq->ring, 1);
a2164 1

d2171 11
a2181 1
	/* TODO: additional code. */
d2192 1
a2192 4

	cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring, struct oce_nic_rx_cqe);
	/* dequeue till you reach an invalid cqe */
	while (RQ_CQE_VALID(cqe)) {
d2194 1
a2194 3
		RING_GET(cq->ring, 1);
		cqe = RING_GET_CONSUMER_ITEM_VA(cq->ring,
		    struct oce_nic_rx_cqe);
a2196 1

d2300 1
a2300 2
oce_create_ring(struct oce_softc *sc, int q_len, int item_size,
    int max_segs)
d2302 2
a2304 1
	struct oce_ring *ring;
d2314 2
a2315 2
	ring->item_size = item_size;
	ring->num_items = q_len;
d2317 4
a2320 3
	ring->dma.tag = sc->pa.pa_dmat;
	rc = bus_dmamap_create(ring->dma.tag, size, max_segs, PAGE_SIZE, 0,
	    BUS_DMA_NOWAIT, &ring->dma.map);
d2326 2
a2327 2
	rc = bus_dmamem_alloc(ring->dma.tag, size, 0, 0, &ring->dma.segs,
	    max_segs, &ring->dma.nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO);
d2333 2
a2334 2
	rc = bus_dmamem_map(ring->dma.tag, &ring->dma.segs, ring->dma.nsegs,
	    size, &ring->dma.vaddr, BUS_DMA_NOWAIT);
d2340 2
a2341 3
	bus_dmamap_sync(ring->dma.tag, ring->dma.map, 0,
	    ring->dma.map->dm_mapsize, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);
d2343 2
a2344 2
	ring->dma.paddr = 0;
	ring->dma.size = size;
d2349 1
a2349 1
	bus_dmamem_free(ring->dma.tag, &ring->dma.segs, ring->dma.nsegs);
d2351 1
a2351 1
	bus_dmamap_destroy(ring->dma.tag, ring->dma.map);
d2373 1
a2373 1
	    ring->item_size * ring->num_items, NULL, BUS_DMA_NOWAIT)) {
d2396 24
d2621 1
a2621 1
	mbx = RING_GET_PRODUCER_ITEM_VA(mq->ring, struct oce_mbx);
a2636 1
	RING_PUT(mq->ring, 1);
@


1.36
log
@introduce specialized register read and write functions
oce_{read,write}_{cfg,csr,db} for different pci bars
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.35 2012/10/29 22:33:20 mikeb Exp $	*/
d102 1
a102 1
int  oce_attach_ifp(struct oce_softc *sc);
d209 1
a209 1
    uint8_t multi[][ETH_ADDR_LEN], int naddr);
d262 1
a262 1
		sc->flags |= OCE_FLAGS_BE2;
d266 1
a266 1
		sc->flags |= OCE_FLAGS_BE3;
d269 1
a269 1
		sc->flags |= OCE_FLAGS_XE201;
d281 1
a281 1
	sc->flow_control = OCE_DEFAULT_FLOW_CONTROL;
d297 1
a297 1
	if (IS_BE(sc) && (sc->flags & OCE_FLAGS_BE3)) {
d300 1
a300 2
	} else
		sc->be3_native = 0;
d306 1
a306 1
	bcopy(sc->macaddr, sc->arpcom.ac_enaddr, ETH_ADDR_LEN);
d318 1
a318 2
	if (oce_attach_ifp(sc))
		goto fail_2;
d322 1
a322 1
		goto fail_3;
d335 1
a335 4
fail_4:
	oce_free_lro(sc);
fail_3:
#endif
a337 1
fail_2:
d339 1
d367 1
a367 1
int
a406 2

	return 0;
a428 1

a431 1

a439 1

a450 1

d470 1
a470 1
	uint8_t multi[OCE_MAX_MC_FILTER_SIZE][ETH_ADDR_LEN];
d486 1
a486 2
			bcopy(enm->enm_addrlo, multi[naddr++], ETH_ADDR_LEN);

a488 1

d530 1
a530 1
	/* Arm all cqs connected to this EQ */
d548 1
a548 1
	if (IS_BE(sc) && (sc->flags & OCE_FLAGS_BE2))
d575 1
a575 1
		sc->flags |= OCE_FLAGS_MBOX_ENDIAN_RQD;
d577 1
a577 3
		sc->flags |= OCE_FLAGS_RESET_RQD;
	if (OCE_SLI_FUNCTION(reg) == OCE_INTF_VIRT_FUNC)
		sc->flags |= OCE_FLAGS_VIRTUAL_PORT;
d677 1
a677 1
		printf(": couldn't establish interrupt");
d789 1
a789 1
	return 0;
d1255 2
a1256 1
		/* Lancer A0 workaround
d1260 1
a1260 1
			num_frags -= 1;
a1284 1
	int vtp = 0;
d1286 1
a1286 1
	if (sc->be3_native) {
d1288 3
a1290 6
		vtp = cqe_v1->u0.s.vlan_tag_present;
	} else
		vtp = cqe->u0.s.vlan_tag_present;

	return vtp;

d1298 1
a1298 1
	if (sc->be3_native && IS_BE(sc)) {
d1301 3
a1303 6
			return 0;
	} else
		;/* For BE3 legacy and Lancer this is dummy */

	return 1;

d1373 1
a1373 1
		return 0;	/* no more room */
d1379 1
a1379 1
		return 0;
d1386 1
a1386 1
		return 0;
d1407 1
a1407 1
	return 1;
d1419 1
a1419 1
		return 0;
d1427 1
a1427 1
	return 1;
d1513 1
a1513 1
	if (!bcmp(sc->macaddr, sc->arpcom.ac_enaddr, ETH_ADDR_LEN))
d1617 1
a1617 1
		if (oce_config_rss(sc, (uint8_t)sc->if_id, RSS_ENABLE)) {
d1826 1
a1826 1
	wq->cfg.eqd = OCE_DEFAULT_WQ_EQD;
d1987 3
a1989 3
	eq->cfg.q_len = EQ_LEN_1024;	/* length of event queue */
	eq->cfg.item_size = EQE_SIZE_4; /* size of a queue item */
	eq->cfg.cur_eqd = (uint8_t)80;	/* event queue delay */
d2093 1
a2093 1
		return NULL;
d2098 1
a2098 1
		return NULL;
d2301 1
a2301 1
	return 0;
d2310 1
a2310 1
	return rc;
d2343 1
a2343 1
		return NULL;
d2347 1
a2347 1
		return NULL;
d2465 1
a2465 1
			if (sc->flags & OCE_FLAGS_RESET_RQD) {
d2542 2
a2543 10
	if (sc->flags & OCE_FLAGS_MBOX_ENDIAN_RQD) {
		/* Endian Signature */
		*ptr++ = 0xff;
		*ptr++ = 0x12;
		*ptr++ = 0x34;
		*ptr++ = 0xff;
		*ptr++ = 0xff;
		*ptr++ = 0x56;
		*ptr++ = 0x78;
		*ptr = 0xff;
d2545 9
a2553 2
		return (oce_mbox_dispatch(sc));
	}
d2555 1
a2555 1
	return (0);
d2690 1
a2690 1
		bcopy(macaddr, &cmd.params.req.mac_addr[0], ETH_ADDR_LEN);
d2719 1
a2719 1
	return 0;
d2780 1
a2780 1
 * @@param enable	0=disable, RSS_ENABLE_xxx flags otherwise
d2793 2
a2794 3
		cmd.params.req.enable_rss = RSS_ENABLE_IPV4 |
		    RSS_ENABLE_TCP_IPV4 | RSS_ENABLE_IPV6 |
		    RSS_ENABLE_TCP_IPV6);
d2833 1
a2833 1
    uint8_t multi[][ETH_ADDR_LEN], int naddr)
d2839 1
a2839 1
	bcopy(&multi[0], &cmd.params.req.mac[0], naddr * ETH_ADDR_LEN);
d2930 2
a2931 1
		bcopy(&cmd.params.rsp.mac.mac_addr[0], macaddr, ETH_ADDR_LEN);
d2945 1
a2945 1
	bcopy(enaddr, cmd.params.req.mac_address, ETH_ADDR_LEN);
d2985 2
a2986 2
	sc->be3_native = cmd.params.rsp.capability_flags &
	    CAP_BE3_NATIVE_ERX_API;
d3335 4
a3338 6
	if (IS_BE(sc)) {
		if (sc->flags & OCE_FLAGS_BE2)
			return (oce_stats_be2(sc, rxe, txe));
		else
			return (oce_stats_be3(sc, rxe, txe));
	}
@


1.35
log
@prefer to return oce_cmd and save on the error variable
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.34 2012/10/29 22:16:45 mikeb Exp $	*/
d613 3
d627 48
d708 2
a709 3
	reg = OCE_READ_REG32(sc, cfg, PCI_INTR_CTRL);
	reg |= HOSTINTR_MASK;
	OCE_WRITE_REG32(sc, cfg, PCI_INTR_CTRL, reg);
d717 2
a718 3
	reg = OCE_READ_REG32(sc, cfg, PCI_INTR_CTRL);
	reg &= ~HOSTINTR_MASK;
	OCE_WRITE_REG32(sc, cfg, PCI_INTR_CTRL, reg);
a815 1
	uint32_t txdb;
d935 1
a935 2
	txdb = wq->id | (nwqe << 16);
	OCE_WRITE_REG32(sc, db, PD_TXULP_DB, txdb);
a1434 1
	uint32_t rxdb;
d1442 2
a1443 3
		DELAY(1);
		rxdb = rq->id | (OCE_MAX_RQ_POSTS << 24);
		OCE_WRITE_REG32(sc, db, PD_RXULP_DB, rxdb);
d1446 2
a1447 5
	if (nbufs > 0) {
		DELAY(1);
		rxdb = rq->id | (nbufs << 24);
		OCE_WRITE_REG32(sc, db, PD_RXULP_DB, rxdb);
	}
d2164 2
a2165 5
	uint32_t eqdb;

	eqdb = eq->id | (clearint << 9) | (neqe << 16) | (rearm << 29) |
	    PD_EQ_DB_EVENT;
	OCE_WRITE_REG32(eq->sc, db, PD_EQ_DB, eqdb);
d2177 1
a2177 4
	uint32_t cqdb;

	cqdb = cq->id | (ncqe << 16) | (rearm << 29);
	OCE_WRITE_REG32(cq->sc, db, PD_CQ_DB, cqdb);
d2464 1
a2464 1
	reg = OCE_READ_REG32(sc, csr, MPU_EP_SEMAPHORE(sc));
d2469 1
a2469 1
		OCE_WRITE_REG32(sc, csr, MPU_EP_SEMAPHORE(sc), reg);
d2479 1
a2479 1
		reg = OCE_READ_REG32(sc, csr, MPU_EP_SEMAPHORE(sc));
d2507 1
a2507 2
		if (OCE_READ_REG32(sc, db, PD_MPU_MBOX_DB) &
		    PD_MPU_MBOX_DB_READY)
d2530 1
a2530 1
	OCE_WRITE_REG32(sc, db, PD_MPU_MBOX_DB, reg);
d2538 1
a2538 1
	OCE_WRITE_REG32(sc, db, PD_MPU_MBOX_DB, reg);
a2652 1
	uint32_t reg_value;
d2671 1
a2671 2
	reg_value = (1 << 16) | mq->id;
	OCE_WRITE_REG32(sc, db, PD_MQ_DB, reg_value);
@


1.34
log
@shorten fwcmd to cmd
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.33 2012/10/29 18:36:42 mikeb Exp $	*/
a2725 1
	int err;
d2738 2
a2739 3
	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_CONFIG_IFACE_VLAN,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	return (err);
a2751 1
	int err;
d2760 2
a2761 3
	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_FLOW_CONTROL,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	return (err);
d2777 1
a2777 1
	int i, j, err;
d2810 2
a2811 3
	err = oce_cmd(sc, SUBSYS_NIC, OPCODE_NIC_CONFIG_RSS,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	return (err);
a2825 1
	int err;
d2833 2
a2834 3
	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_IFACE_MULTICAST,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	return (err);
a2850 1
	int rc;
d2861 2
a2862 4
	rc = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_SET_IFACE_RX_FILTER,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));

	return rc;
a2946 1
	int err;
d2953 2
a2954 3
	err = oce_cmd(sc, SUBSYS_COMMON, OPCODE_COMMON_DEL_IFACE_MAC,
	    OCE_MBX_VER_V0, &cmd, sizeof(cmd));
	return (err);
d2969 2
a2970 3
	err = oce_cmd(sc, SUBSYS_COMMON,
	    OPCODE_COMMON_SET_FUNCTIONAL_CAPS, OCE_MBX_VER_V0, &cmd,
	    sizeof(cmd));
d3178 1
a3178 1
	int opcode, subsys, err;
d3209 2
a3210 3
	err = oce_cmd(sc, subsys, opcode, OCE_MBX_VER_V0, &cmd,
	    sizeof(cmd));
	return (err);
@


1.33
log
@merge oce.c into if_oce.c and rename oce{reg,var}.h to if_oce{reg,var}.h
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.32 2012/10/29 18:17:39 mikeb Exp $	*/
d2423 1
a2423 1
	struct ioctl_common_function_reset fwcmd;
d2451 1
a2451 1
				bzero(&fwcmd, sizeof(fwcmd));
d2454 1
a2454 1
				    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2617 1
a2617 1
	struct mbx_get_common_fw_version *fwcmd;
d2623 1
a2623 1
	fwcmd = (struct mbx_get_common_fw_version *)&mbx->payload;
d2625 1
a2625 1
	hdr = &fwcmd->hdr;
d2630 1
a2630 1
	hdr->u0.req.request_length = sizeof(*fwcmd) - sizeof(*hdr);
d2633 1
a2633 1
	mbx->payload_length = sizeof(*fwcmd);
d2649 1
a2649 1
	struct mbx_create_common_iface fwcmd;
d2673 1
a2673 1
	bzero(&fwcmd, sizeof(fwcmd));
d2675 3
a2677 3
	fwcmd.params.req.version = 0;
	fwcmd.params.req.cap_flags = htole32(capab_flags);
	fwcmd.params.req.enable_flags = htole32(capab_en_flags);
d2679 2
a2680 2
		bcopy(macaddr, &fwcmd.params.req.mac_addr[0], ETH_ADDR_LEN);
		fwcmd.params.req.mac_invalid = 0;
d2682 1
a2682 1
		fwcmd.params.req.mac_invalid = 1;
d2685 1
a2685 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2689 1
a2689 1
	sc->if_id = letoh32(fwcmd.params.rsp.if_id);
d2692 1
a2692 1
		sc->pmac_id = letoh32(fwcmd.params.rsp.pmac_id);
d2725 1
a2725 1
	struct mbx_common_config_vlan fwcmd;
d2728 1
a2728 1
	bzero(&fwcmd, sizeof(fwcmd));
d2730 4
a2733 4
	fwcmd.params.req.if_id = if_id;
	fwcmd.params.req.promisc = promisc;
	fwcmd.params.req.untagged = untagged;
	fwcmd.params.req.num_vlans = vtag_cnt;
d2736 1
a2736 1
		bcopy(vtag_arr, fwcmd.params.req.tags.normal_vlans,
d2740 1
a2740 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2753 1
a2753 1
	struct mbx_common_get_set_flow_control fwcmd;
d2756 1
a2756 1
	bzero(&fwcmd, sizeof(fwcmd));
d2759 1
a2759 1
		fwcmd.tx_flow_control = 1;
d2761 1
a2761 1
		fwcmd.rx_flow_control = 1;
d2764 1
a2764 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2779 2
a2780 2
	struct mbx_config_nic_rss fwcmd;
	uint8_t *tbl = &fwcmd.params.req.cputable;
d2783 1
a2783 1
	bzero(&fwcmd, sizeof(fwcmd));
d2786 1
a2786 1
		fwcmd.params.req.enable_rss = RSS_ENABLE_IPV4 |
d2789 2
a2790 2
	fwcmd.params.req.flush = OCE_FLUSH;
	fwcmd.params.req.if_id = htole32(if_id);
d2792 1
a2792 1
	arc4random_buf(fwcmd.params.req.hash, sizeof(fwcmd.params.req.hash));
d2810 1
a2810 1
		fwcmd->params.req.cpu_tbl_sz_log2 = htole16(ilog2(i));
d2815 1
a2815 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2830 1
a2830 1
	struct mbx_set_common_iface_multicast fwcmd;
d2833 1
a2833 1
	bzero(&fwcmd, sizeof(fwcmd));
d2835 3
a2837 3
	bcopy(&multi[0], &fwcmd.params.req.mac[0], naddr * ETH_ADDR_LEN);
	fwcmd.params.req.num_mac = htole16(naddr);
	fwcmd.params.req.if_id = sc->if_id;
d2840 1
a2840 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2856 1
a2856 1
	struct mbx_set_common_iface_rx_filter fwcmd;
d2860 1
a2860 1
	bzero(&fwcmd, sizeof(fwcmd));
d2862 1
a2862 1
	req = &fwcmd.params.req;
d2870 1
a2870 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2884 1
a2884 1
	struct mbx_query_common_link_config fwcmd;
d2888 1
a2888 1
	bzero(&fwcmd, sizeof(fwcmd));
d2891 1
a2891 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2895 1
a2895 1
	bcopy(&fwcmd.params.rsp, &link, sizeof(struct link_status));
d2919 1
a2919 1
	struct mbx_query_common_iface_mac fwcmd;
d2922 1
a2922 1
	bzero(&fwcmd, sizeof(fwcmd));
d2924 2
a2925 2
	fwcmd.params.req.type = MAC_ADDRESS_TYPE_NETWORK;
	fwcmd.params.req.permanent = 1;
d2928 1
a2928 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2930 1
a2930 1
		bcopy(&fwcmd.params.rsp.mac.mac_addr[0], macaddr, ETH_ADDR_LEN);
d2938 1
a2938 1
	struct mbx_add_common_iface_mac fwcmd;
d2941 1
a2941 1
	bzero(&fwcmd, sizeof(fwcmd));
d2943 2
a2944 2
	fwcmd.params.req.if_id = htole16(if_id);
	bcopy(enaddr, fwcmd.params.req.mac_address, ETH_ADDR_LEN);
d2947 1
a2947 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2949 1
a2949 1
		*pmac_id = letoh32(fwcmd.params.rsp.pmac_id);
d2956 1
a2956 1
	struct mbx_del_common_iface_mac fwcmd;
d2959 1
a2959 1
	bzero(&fwcmd, sizeof(fwcmd));
d2961 2
a2962 2
	fwcmd.params.req.if_id = htole16(if_id);
	fwcmd.params.req.pmac_id = htole32(pmac_id);
d2965 1
a2965 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d2972 1
a2972 1
	struct mbx_common_set_function_cap fwcmd;
d2975 1
a2975 1
	bzero(&fwcmd, sizeof(fwcmd));
d2977 1
a2977 1
	fwcmd.params.req.valid_capability_flags = CAP_SW_TIMESTAMPS |
d2979 1
a2979 1
	fwcmd.params.req.capability_flags = CAP_BE3_NATIVE_ERX_API;
d2982 2
a2983 2
	    OPCODE_COMMON_SET_FUNCTIONAL_CAPS, OCE_MBX_VER_V0, &fwcmd,
	    sizeof(fwcmd));
d2987 1
a2987 1
	sc->be3_native = fwcmd.params.rsp.capability_flags &
d2996 1
a2996 1
	struct mbx_create_nic_rq fwcmd;
d2999 1
a2999 1
	bzero(&fwcmd, sizeof(fwcmd));
d3001 2
a3002 2
	npages = oce_load_ring(sc, rq->ring, &fwcmd.params.req.pages[0],
	    nitems(fwcmd.params.req.pages));
d3009 2
a3010 2
		fwcmd.params.req.frag_size = rq->cfg.frag_size / 2048;
		fwcmd.params.req.page_size = 1;
d3012 6
a3017 6
		fwcmd.params.req.frag_size = ilog2(rq->cfg.frag_size);
	fwcmd.params.req.num_pages = npages;
	fwcmd.params.req.cq_id = rq->cq->id;
	fwcmd.params.req.if_id = htole32(sc->if_id);
	fwcmd.params.req.max_frame_size = htole16(rq->cfg.mtu);
	fwcmd.params.req.is_rss_queue = htole32(rq->cfg.is_rss_queue);
d3020 2
a3021 2
	    IS_XE201(sc) ? OCE_MBX_VER_V1 : OCE_MBX_VER_V0, &fwcmd,
	    sizeof(fwcmd));
d3025 2
a3026 2
	rq->id = letoh16(fwcmd.params.rsp.rq_id);
	rq->rss_cpuid = fwcmd.params.rsp.rss_cpuid;
d3034 1
a3034 1
	struct mbx_create_nic_wq fwcmd;
d3037 1
a3037 1
	bzero(&fwcmd, sizeof(fwcmd));
d3039 2
a3040 2
	npages = oce_load_ring(sc, wq->ring, &fwcmd.params.req.pages[0],
	    nitems(fwcmd.params.req.pages));
d3047 6
a3052 6
		fwcmd.params.req.if_id = sc->if_id;
	fwcmd.params.req.nic_wq_type = wq->cfg.wq_type;
	fwcmd.params.req.num_pages = npages;
	fwcmd.params.req.wq_size = ilog2(wq->cfg.q_len) + 1;
	fwcmd.params.req.cq_id = htole16(wq->cq->id);
	fwcmd.params.req.ulp_num = 1;
d3055 2
a3056 2
	    IS_XE201(sc) ? OCE_MBX_VER_V1 : OCE_MBX_VER_V0, &fwcmd,
	    sizeof(fwcmd));
d3060 1
a3060 1
	wq->id = letoh16(fwcmd.params.rsp.wq_id);
d3068 1
a3068 1
	struct mbx_create_common_mq_ex fwcmd;
d3072 1
a3072 1
	bzero(&fwcmd, sizeof(fwcmd));
d3074 2
a3075 2
	npages = oce_load_ring(sc, mq->ring, &fwcmd.params.req.pages[0],
	    nitems(fwcmd.params.req.pages));
d3081 1
a3081 1
	ctx = &fwcmd.params.req.context;
d3090 1
a3090 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d3094 1
a3094 1
	mq->id = letoh16(fwcmd.params.rsp.mq_id);
d3102 1
a3102 1
	struct mbx_create_common_eq fwcmd;
d3105 1
a3105 1
	bzero(&fwcmd, sizeof(fwcmd));
d3107 2
a3108 2
	npages = oce_load_ring(sc, eq->ring, &fwcmd.params.req.pages[0],
	    nitems(fwcmd.params.req.pages));
d3114 6
a3119 6
	fwcmd.params.req.ctx.num_pages = htole16(npages);
	fwcmd.params.req.ctx.valid = 1;
	fwcmd.params.req.ctx.size = (eq->cfg.item_size == 4) ? 0 : 1;
	fwcmd.params.req.ctx.count = ilog2(eq->cfg.q_len / 256);
	fwcmd.params.req.ctx.armed = 0;
	fwcmd.params.req.ctx.delay_mult = htole32(eq->cfg.cur_eqd);
d3122 1
a3122 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d3126 1
a3126 1
	eq->id = letoh16(fwcmd.params.rsp.eq_id);
d3134 1
a3134 1
	struct mbx_create_common_cq fwcmd;
d3138 1
a3138 1
	bzero(&fwcmd, sizeof(fwcmd));
d3140 2
a3141 2
	npages = oce_load_ring(sc, cq->ring, &fwcmd.params.req.pages[0],
	    nitems(fwcmd.params.req.pages));
d3147 1
a3147 1
	ctx = &fwcmd.params.req.cq_ctx;
d3177 2
a3178 2
	    IS_XE201(sc) ? OCE_MBX_VER_V2 : OCE_MBX_VER_V0, &fwcmd,
	    sizeof(fwcmd));
d3182 1
a3182 1
	cq->id = letoh16(fwcmd.params.rsp.cq_id);
d3190 1
a3190 1
	struct mbx_destroy_common_mq fwcmd;
d3218 1
a3218 1
	bzero(&fwcmd, sizeof(fwcmd));
d3220 1
a3220 1
	fwcmd.params.req.id = htole16(qid);
d3222 2
a3223 2
	err = oce_cmd(sc, subsys, opcode, OCE_MBX_VER_V0, &fwcmd,
	    sizeof(fwcmd));
d3230 1
a3230 1
	struct mbx_get_nic_stats_v0 fwcmd;
d3236 1
a3236 1
	bzero(&fwcmd, sizeof(fwcmd));
d3239 1
a3239 1
	    &fwcmd, sizeof(fwcmd));
d3243 2
a3244 2
	ms = &fwcmd.params.rsp.stats.pmem;
	rs = &fwcmd.params.rsp.stats.rxf;
d3269 1
a3269 1
	struct mbx_get_nic_stats fwcmd;
d3275 1
a3275 1
	bzero(&fwcmd, sizeof(fwcmd));
d3278 1
a3278 1
	    &fwcmd, sizeof(fwcmd));
d3282 2
a3283 2
	ms = &fwcmd.params.rsp.stats.pmem;
	rs = &fwcmd.params.rsp.stats.rxf;
d3304 1
a3304 1
	struct mbx_get_pport_stats fwcmd;
d3308 1
a3308 1
	bzero(&fwcmd, sizeof(fwcmd));
d3310 2
a3311 2
	fwcmd.params.req.reset_stats = 0;
	fwcmd.params.req.port_number = sc->if_id;
d3314 1
a3314 1
	    OCE_MBX_VER_V0, &fwcmd, sizeof(fwcmd));
d3318 1
a3318 1
	pps = &fwcmd.params.rsp.pps;
@


1.32
log
@rearrange the function layout a bit
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.31 2012/10/29 18:14:28 mikeb Exp $	*/
d96 2
a97 2
#include <dev/pci/ocereg.h>
#include <dev/pci/ocevar.h>
d185 46
d2414 931
@


1.31
log
@oce_get_fw_config is useless; ditch it
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.30 2012/10/26 23:35:09 mikeb Exp $	*/
d257 1
a257 1
	if (oce_read_macaddr(sc, sc->macaddr)) {
@


1.30
log
@oce_first_mcc_cmd doesn't return anything of value so make it void;
shorten it to oce_first_mcc while here
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.29 2012/10/26 18:05:50 mikeb Exp $	*/
a247 5
		goto fail_1;
	}

	if (oce_get_fw_config(sc)) {
		printf(": failed to fetch fw configuration\n");
@


1.29
log
@cleanup oce_init_fw; use less bitfields
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.28 2012/10/26 17:56:24 mikeb Exp $	*/
d328 1
a328 1
	oce_first_mcc_cmd(sc);
@


1.28
log
@rename oce_config_nic_rss to oce_config_rss and merge oce_rss_itbl_init in;
cleanup oce_set_promisc and oce_config_vlan function arguments, a bunch of
defines and prototypes while at it.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.27 2012/10/25 17:42:16 mikeb Exp $	*/
d549 1
a549 1
		sc->flags |= OCE_FLAGS_FUNCRESET_RQD;
@


1.27
log
@simplify/unify writes to the rx and tx doorbell registers
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.26 2012/10/25 17:26:42 mikeb Exp $	*/
d1555 1
a1555 1
		if (oce_config_nic_rss(sc, (uint8_t)sc->if_id, RSS_ENABLE)) {
@


1.26
log
@don't use bitfields in oce_pci_alloc; kill some unused structures
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.25 2012/10/25 17:24:11 mikeb Exp $	*/
d726 1
a726 1
	uint32_t reg;
d846 2
a847 2
	reg = (nwqe << 16) | wq->id;
	OCE_WRITE_REG32(sc, db, PD_TXULP_DB, reg);
d1347 1
a1347 1
	pd_rxulp_db_t rxdb_reg;
d1356 2
a1357 4
		bzero(&rxdb_reg, sizeof(rxdb_reg));
		rxdb_reg.bits.num_posted = OCE_MAX_RQ_POSTS;
		rxdb_reg.bits.qid = rq->id;
		OCE_WRITE_REG32(sc, db, PD_RXULP_DB, rxdb_reg.dw0);
d1362 2
a1363 4
		bzero(&rxdb_reg, sizeof(rxdb_reg));
		rxdb_reg.bits.qid = rq->id;
		rxdb_reg.bits.num_posted = nbufs;
		OCE_WRITE_REG32(sc, db, PD_RXULP_DB, rxdb_reg.dw0);
@


1.25
log
@cleanup interrupt register defines
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.24 2012/10/25 16:47:30 mikeb Exp $	*/
a516 1
	pci_sli_intf_t intf;
d533 2
a534 1
	/* Read the SLI_INTF register and determine whether we
d537 2
a538 3
	intf.dw0 = pci_conf_read(pa->pa_pc, pa->pa_tag, OCE_INTF_REG_OFFSET);

	if (intf.bits.sli_valid != OCE_INTF_VALID_SIG) {
d542 2
a543 4

	if (intf.bits.sli_rev != OCE_INTF_SLI_REV4) {
		printf(": adapter doesnt support SLI revision %d\n",
		    intf.bits.sli_rev);
d546 1
a546 2

	if (intf.bits.sli_if_type == OCE_INTF_IF_TYPE_1)
d548 1
a548 2

	if (intf.bits.sli_hint1 == OCE_INTF_FUNC_RESET_REQD)
d550 1
a550 2

	if (intf.bits.sli_func_type == OCE_INTF_VIRT_FUNC)
@


1.24
log
@simplify oce_arm_eq and oce_arm_cq
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.23 2012/10/22 02:49:03 brad Exp $	*/
d622 1
a622 1
	reg = OCE_READ_REG32(sc, cfg, PCICFG_INTR_CTRL);
d624 1
a624 1
	OCE_WRITE_REG32(sc, cfg, PCICFG_INTR_CTRL, reg);
d632 1
a632 1
	reg = OCE_READ_REG32(sc, cfg, PCICFG_INTR_CTRL);
d634 1
a634 1
	OCE_WRITE_REG32(sc, cfg, PCICFG_INTR_CTRL, reg);
@


1.23
log
@Fix the multicast filter full size check.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.22 2012/10/18 09:31:07 mikeb Exp $	*/
d170 2
a171 1
void oce_arm_eq(struct oce_eq *eq, int npopped, int rearm, int clearint);
d182 1
a182 1
void oce_arm_cq(struct oce_cq *cq, int npopped, int rearm);
d2083 1
a2083 1
 * @@param npopped	number of EQEs to arm
d2088 2
a2089 2
void
oce_arm_eq(struct oce_eq *eq, int npopped, int rearm, int clearint)
d2091 1
a2091 2
	struct oce_softc *sc = eq->sc;
	eq_db_t eq_db = { 0 };
d2093 3
a2095 6
	eq_db.bits.rearm = rearm;
	eq_db.bits.event = 1;
	eq_db.bits.num_popped = npopped;
	eq_db.bits.clrint = clearint;
	eq_db.bits.qid = eq->id;
	OCE_WRITE_REG32(sc, db, PD_EQ_DB, eq_db.dw0);
d2101 1
a2101 1
 * @@param npopped	number of CQEs to arm
d2104 2
a2105 2
void
oce_arm_cq(struct oce_cq *cq, int npopped, int rearm)
d2107 1
a2107 2
	struct oce_softc *sc = cq->sc;
	cq_db_t cq_db = { 0 };
d2109 2
a2110 5
	cq_db.bits.rearm = rearm;
	cq_db.bits.num_popped = npopped;
	cq_db.bits.event = 0;
	cq_db.bits.qid = cq->id;
	OCE_WRITE_REG32(sc, db, PD_CQ_DB, cq_db.dw0);
@


1.22
log
@make oce_arm_{eq,cq} functions look like the other queue
manupulation functions in this file; do some minor style
cleanup while here.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.21 2012/10/15 19:30:39 mikeb Exp $	*/
d449 1
a449 1
	    ac->ac_multicnt > OCE_MAX_MC_FILTER_SIZE) {
a454 4
			if (naddr == OCE_MAX_MC_FILTER_SIZE) {
				promisc = 1;
				break;
			}
d456 1
d459 1
@


1.21
log
@eliminate a couple of gotos
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.20 2012/10/15 19:23:23 mikeb Exp $	*/
d170 1
a170 2
void oce_arm_eq(struct oce_softc *sc, int16_t qid, int npopped,
    uint32_t rearm, uint32_t clearint);
d181 1
a181 2
void oce_arm_cq(struct oce_softc *sc, int16_t qid, int npopped,
    uint32_t rearm);
d314 1
a314 1
	oce_arm_cq(sc->mq->sc, sc->mq->cq->id, 0, TRUE);
d321 1
a321 1
	oce_arm_eq(sc, sc->eq[0]->id, 0, TRUE, FALSE);
d455 1
a455 1
			if (naddr >= OCE_MAX_MC_FILTER_SIZE) {
d475 1
a475 1
	int i, claimed = 0, num_eqes = 0;
d486 1
a486 1
		num_eqes++;
d489 1
a489 1
	if (!num_eqes)
d495 1
a495 1
	oce_arm_eq(sc, eq->id, num_eqes, FALSE, TRUE);
d506 1
a506 1
		oce_arm_cq(sc, cq->id, 0, TRUE);
d510 1
a510 1
	oce_arm_eq(sc, eq->id, 0, TRUE, FALSE);
a1018 1
	struct oce_softc *sc = wq->sc;
d1021 1
a1021 1
	int num_cqes = 0;
d1039 1
a1039 1
		num_cqes++;
d1042 2
a1043 2
	if (num_cqes)
		oce_arm_cq(sc, cq->id, num_cqes, FALSE);
d1404 1
a1404 1
	int num_cqes = 0, rq_buffers_used = 0;
d1436 2
a1437 2
		num_cqes++;
		if (num_cqes >= (IS_XE201(sc) ? 8 : OCE_MAX_RSP_HANDLED))
d1448 2
a1449 2
	if (num_cqes) {
		oce_arm_cq(sc, cq->id, num_cqes, FALSE);
a1534 2
	splassert(IPL_NET);

d1575 1
a1575 1
		oce_arm_cq(rq->sc, rq->cq->id, 0, TRUE);
d1578 1
a1578 1
		oce_arm_cq(wq->sc, wq->cq->id, 0, TRUE);
d1580 1
a1580 1
	oce_arm_cq(sc->mq->sc, sc->mq->cq->id, 0, TRUE);
d1583 1
a1583 1
		oce_arm_eq(sc, eq->id, 0, TRUE, FALSE);
d1627 1
a1627 1
	int evt_type, optype, num_cqes = 0;
d1655 1
a1655 1
		num_cqes++;
d1658 2
a1659 2
	if (num_cqes)
		oce_arm_cq(sc, cq->id, num_cqes, FALSE /* TRUE */);
d2083 1
a2083 2
 * @@param sc		software handle to the device
 * @@param qid		id of the EQ returned by the fw at the time of creation
d2090 1
a2090 2
oce_arm_eq(struct oce_softc *sc, int16_t qid, int npopped, uint32_t rearm,
    uint32_t clearint)
d2092 1
d2099 1
a2099 1
	eq_db.bits.qid = qid;
d2105 1
a2105 2
 * @@param sc		software handle to the device
 * @@param qid		id of the CQ returned by the fw at the time of creation
d2110 1
a2110 1
oce_arm_cq(struct oce_softc *sc, int16_t qid, int npopped, uint32_t rearm)
d2112 1
d2118 1
a2118 1
	cq_db.bits.qid = qid;
d2122 1
a2122 1
/*
a2129 1
	struct oce_softc *sc = eq->sc;
d2145 1
a2145 1
	oce_arm_eq(sc, eq->id, neqe, FALSE, TRUE);
a2150 1
	struct oce_softc *sc = wq->sc;
d2167 1
a2167 1
	oce_arm_cq(sc, cq->id, ncqe, FALSE);
a2169 6
/*
 * @@brief		function to drain a MCQ and process its CQEs
 * @@param dev		software handle to the device
 * @@param cq		pointer to the cq to drain
 * @@returns		the number of CQEs processed
 */
a2175 5
/**
 * @@brief		function to process a Recieve queue
 * @@param arg		pointer to the RQ to charge
 * @@return		number of cqes processed
 */
a2178 1
	struct oce_softc *sc = rq->sc;
d2195 1
a2195 1
	oce_arm_cq(sc, cq->id, ncqe, FALSE);
d2344 1
a2344 1
	return ring;
d2352 1
a2352 2
	ring = NULL;
	return NULL;
d2373 1
a2373 1
		return 0;
d2380 1
a2380 1
		return 0;
d2390 2
a2391 1
	return nsegs;
@


1.20
log
@major cleanup of the queue allocation code;  we need only half of
those functions.  fixup some dma syncs, not tested yet though.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.19 2012/10/12 18:24:31 mikeb Exp $	*/
d1786 4
a1789 2
		    &wq->pckts[i].map))
			goto free_wq;
d1792 4
a1795 2
	if (oce_new_wq(sc, wq))
		goto free_wq;
a1805 4

free_wq:
	oce_destroy_wq(wq);
	return NULL;
d1876 1
d1883 4
a1886 2
		    BUS_DMA_NOWAIT, &rq->pckts[i].map))
			goto free_rq;
a1889 1
	rq->cfg.if_id = if_id;
d1897 1
a1897 5
	return rq;

free_rq:
	oce_destroy_rq(rq);
	return NULL;
@


1.19
log
@no need to fetch and print the fw revision
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.18 2012/10/12 17:41:40 mikeb Exp $	*/
d127 1
a127 1
void oce_txeof(struct oce_wq *wq, uint32_t wqe_idx, uint32_t status);
d152 3
a154 3
void oce_mq_handler(void *arg);
void oce_wq_handler(void *arg);
void oce_rq_handler(void *arg);
d158 14
a171 4
void oce_arm_eq(struct oce_softc *sc, int16_t qid, int npopped, uint32_t rearm,
    uint32_t clearint);
void oce_arm_cq(struct oce_softc *sc, int16_t qid, int npopped,
    uint32_t rearm);
d173 7
a179 18
void oce_drain_mq(void *arg);
void oce_drain_rq(struct oce_rq *rq);
void oce_drain_wq(struct oce_wq *wq);
struct oce_wq *oce_wq_init(struct oce_softc *sc, uint32_t q_len,
    uint32_t wq_type);
int  oce_wq_create(struct oce_wq *wq, struct oce_eq *eq);
void oce_wq_destroy(struct oce_wq *wq);
struct oce_rq *oce_rq_init(struct oce_softc *sc, uint32_t q_len,
    uint32_t frag_size, uint32_t rss);
int  oce_rq_create(struct oce_rq *rq, uint32_t if_id, struct oce_eq *eq);
void oce_rq_destroy(struct oce_rq *rq);
struct oce_eq *oce_eq_create(struct oce_softc *sc, uint32_t q_len,
    uint32_t item_size, uint32_t eq_delay);
void oce_eq_destroy(struct oce_eq *eq);
struct oce_mq *oce_mq_create(struct oce_softc *sc, struct oce_eq *eq,
    uint32_t q_len);
void oce_mq_destroy(struct oce_mq *mq);
struct oce_cq *oce_cq_create(struct oce_softc *sc, struct oce_eq *eq,
d182 3
a184 1
void oce_cq_destroy(struct oce_cq *cq);
d479 1
a479 1
	oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTWRITE);
d486 1
a486 1
		oce_dma_sync(&eq->ring->dma, BUS_DMASYNC_POSTWRITE);
d502 1
a502 1
		(*cq->cq_handler)(cq->cb_arg);
a515 1

a728 3
	int rc = 0, i, retry_cnt = 0;
	bus_dma_segment_t *segs;
	struct mbuf *m;
d730 1
a732 1
	uint32_t out;
d735 2
a736 11
	int num_wqes;
	uint32_t reg_value;

	m = *mpp;
	if (!m)
		return EINVAL;

	if (!(m->m_flags & M_PKTHDR)) {
		rc = ENXIO;
		goto free_ret;
	}
d744 1
a744 1
		m = NULL;
d746 2
a747 4
		if (m == NULL) {
			rc = ENXIO;
			goto free_ret;
		}
d755 1
a755 1
		return EBUSY;
d758 1
a758 1
retry:
d760 7
a766 17
	if (rc == EFBIG)	{
		if (retry_cnt == 0) {
			if (m_defrag(m, M_DONTWAIT) != 0)
				goto free_ret;
			*mpp = m;
			retry_cnt = retry_cnt + 1;
			goto retry;
		} else
			goto free_ret;
	} else if (rc == ENOMEM) {
		printf("%s: failed to load mbuf: out of memory",
		    sc->dev.dv_xname);
		return rc;
	} else if (rc) {
		printf("%s: failed to load mbuf: %d", sc->dev.dv_xname, rc);
		goto free_ret;
	}
a767 1
	segs = pd->map->dm_segs;
d770 1
a770 1
	num_wqes = pd->nsegs + 1;
d773 2
a774 2
		if (num_wqes & 1)
			num_wqes++;
d776 1
a776 1
	if (num_wqes >= RING_NUM_FREE(wq->ring)) {
d778 1
a778 1
		return EBUSY;
d799 1
a799 1
	nichdr->u0.s.num_wqe = num_wqes;
d820 3
d830 3
a832 3
		nicfrag->u0.s.frag_pa_hi = ADDR_HI(segs[i].ds_addr);
		nicfrag->u0.s.frag_pa_lo = ADDR_LO(segs[i].ds_addr);
		nicfrag->u0.s.frag_len = segs[i].ds_len;
d837 1
a837 1
	if (num_wqes > (pd->nsegs + 1)) {
d852 5
a856 4
	oce_dma_sync(&wq->ring->dma, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);
	reg_value = (num_wqes << 16) | wq->id;
	OCE_WRITE_REG32(sc, db, PD_TXULP_DB, reg_value);
d858 1
a858 1
	return 0;
d860 1
a860 1
free_ret:
d863 1
a863 1
	return rc;
d867 1
a867 1
oce_txeof(struct oce_wq *wq, uint32_t wqe_idx, uint32_t status)
d995 1
a995 1
		IFQ_POLL(&ifp->if_snd, m);
d1000 1
a1000 2
			if (m)
				ifp->if_flags |= IFF_OACTIVE;
a1003 2
		IFQ_DEQUEUE(&ifp->if_snd, m);

d1018 1
a1018 1
oce_wq_handler(void *arg)
d1026 1
a1026 1
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d1035 1
a1035 1
		oce_txeof(wq, cqe->u0.s.wqe_index, cqe->u0.s.status);
d1039 1
a1039 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d1062 1
a1062 1
		/*partial DMA workaround for Lancer*/
d1085 1
a1085 1
		    BUS_DMASYNC_POSTWRITE);
d1131 1
a1131 1
		/* This deternies if vlan tag is Valid */
d1140 5
a1144 4
				/* In UMC mode generally pvid will be striped by
				   hw. But in some cases we have seen it comes
				   with pvid. So if pvid == vlan, neglect vlan.
				*/
d1193 1
a1193 1
		* num_frags will be 1 more than actual in case of error
d1210 1
a1210 1
		    BUS_DMASYNC_POSTWRITE);
d1311 1
a1311 3
	bus_dma_segment_t *segs;
	int nsegs;
	int in, rc;
a1312 1
	in = rq->packets_in + 1;
d1326 1
a1326 4
	rc = bus_dmamap_load_mbuf(rq->tag, pd->map, pd->mbuf,
	    BUS_DMA_NOWAIT);
	if (rc) {
		printf("%s: failed to load an mbuf", sc->dev.dv_xname);
d1328 1
a1328 6
		return 0;
	}
	segs = pd->map->dm_segs;
	nsegs = pd->map->dm_nsegs;
	if (nsegs != 1) {
		printf("%s: too many DMA segments", sc->dev.dv_xname);
d1333 1
d1337 3
d1341 2
a1342 2
	rqe->u0.s.frag_pa_hi = ADDR_HI(segs[0].ds_addr);
	rqe->u0.s.frag_pa_lo = ADDR_LO(segs[0].ds_addr);
d1346 4
d1400 1
a1400 1
oce_rq_handler(void *arg)
d1409 1
a1409 1
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d1436 1
a1436 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d1520 1
a1520 1
		oce_stop_rq(rq);
d1551 1
a1551 1
		if (oce_start_rq(rq)) {
d1555 6
a1567 2
	DELAY(10);

d1624 1
a1624 1
oce_mq_handler(void *arg)
d1634 1
a1634 1
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d1658 1
a1658 1
		oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d1672 1
a1672 16
	int i, rc = 0;

	/* alloc TX/RX queues */
	for_all_wq_queues(sc, wq, i) {
		sc->wq[i] = oce_wq_init(sc, sc->tx_ring_size,
		    NIC_WQ_TYPE_STANDARD);
		if (!sc->wq[i])
			goto error;
	}

	for_all_rq_queues(sc, rq, i) {
		sc->rq[i] = oce_rq_init(sc, sc->rx_ring_size, sc->rq_frag_size,
		    (i == 0) ? 0 : sc->rss_enable);
		if (!sc->rq[i])
			goto error;
	}
d1680 1
a1680 1
		sc->eq[i] = oce_eq_create(sc, EQ_LEN_1024, EQE_SIZE_4, 80);
d1685 1
a1685 1
	/* create Tx, Rx and mcc queues */
d1687 2
a1688 2
		rc = oce_wq_create(wq, sc->eq[i]);
		if (rc)
a1689 1
		wq->queue_index = i;
d1692 1
d1694 4
a1697 2
		rc = oce_rq_create(rq, sc->if_id, sc->eq[i == 0 ? 0 : i - 1]);
		if (rc)
a1698 1
		rq->queue_index = i;
d1701 2
a1702 1
	sc->mq = oce_mq_create(sc, sc->eq[0], 128);
d1706 1
a1706 2
	return rc;

d1709 1
a1709 1
	return 1;
d1722 1
a1722 1
			oce_rq_destroy(sc->rq[i]);
d1727 1
a1727 1
			oce_wq_destroy(sc->wq[i]);
d1731 1
a1731 1
		oce_mq_destroy(sc->mq);
d1735 1
a1735 1
			oce_eq_destroy(sc->eq[i]);
a1742 1
 * @@param wq_type	work queue type
d1746 1
a1746 1
oce_wq_init(struct oce_softc *sc, uint32_t q_len, uint32_t wq_type)
d1749 2
a1750 1
	int rc = 0, i;
d1752 2
a1753 6
	/* q_len must be min 256 and max 2k */
	if (q_len < 256 || q_len > 2048) {
		printf("%s: Invalid q length. Must be [256, 2000]: 0x%x\n",
		    sc->dev.dv_xname, q_len);
		return NULL;
	}
d1757 19
a1775 1
		return NULL;
d1777 1
d1779 1
a1779 1
	wq->cfg.wq_type = (uint8_t) wq_type;
a1782 3
	wq->sc = sc;
	wq->tag = sc->pa.pa_dmat;

d1784 1
a1784 1
		rc = bus_dmamap_create(wq->tag, OCE_MAX_TX_SIZE,
d1786 1
a1786 2
		    &wq->pckts[i].map);
		if (rc)
d1790 1
a1790 2
	wq->ring = oce_create_ring(sc, q_len, NIC_WQE_SIZE, 8);
	if (!wq->ring)
d1793 9
a1801 1
	return wq;
d1804 1
a1804 1
	oce_wq_destroy(wq);
d1809 1
a1809 1
oce_wq_destroy(struct oce_wq *wq)
d1814 1
a1814 1
	if (wq->state == QCREATED)
a1815 1

d1817 1
a1817 2
		oce_cq_destroy(wq->cq);

a1819 1

a1825 1

a1829 38
 * @@brief 		Create a work queue
 * @@param wq		pointer to work queue
 * @@param eq		pointer to associated event queue
 */
int
oce_wq_create(struct oce_wq *wq, struct oce_eq *eq)
{
	struct oce_softc *sc = wq->sc;
	struct oce_cq *cq;
	int rc = 0;

	cq = oce_cq_create(sc, eq, CQ_LEN_512, sizeof(struct oce_nic_tx_cqe),
	    1, 0, 3);
	if (!cq)
		return ENXIO;

	wq->cq = cq;

	rc = oce_create_wq(sc, wq);
	if (rc)
		goto error;

	wq->state = QCREATED;
	wq->ring->cidx = 0;
	wq->ring->pidx = 0;

	eq->cq[eq->cq_valid] = cq;
	eq->cq_valid++;
	cq->cb_arg = wq;
	cq->cq_handler = oce_wq_handler;

	return 0;
error:
	oce_wq_destroy(wq);
	return rc;
}

/**
d1832 2
a1835 1
 * @@param mtu		maximum transmission unit
d1840 2
a1841 2
oce_rq_init(struct oce_softc *sc, uint32_t q_len, uint32_t frag_size,
    uint32_t rss)
d1844 2
a1845 1
	int rc = 0, i;
d1848 1
a1848 1
		return NULL;
d1852 1
a1852 1
		return NULL;
d1856 19
a1874 1
		return NULL;
a1879 3
	rq->sc = sc;
	rq->tag = sc->pa.pa_dmat;

d1881 2
a1882 3
		rc = bus_dmamap_create(rq->tag, frag_size, 1, frag_size, 0,
		    BUS_DMA_NOWAIT, &rq->pckts[i].map);
		if (rc)
d1886 8
a1893 3
	rq->ring = oce_create_ring(sc, q_len, sizeof(struct oce_nic_rqe), 2);
	if (!rq->ring)
		goto free_rq;
d1896 1
d1898 1
a1898 1
	oce_rq_destroy(rq);
d1903 1
a1903 1
oce_rq_destroy(struct oce_rq *rq)
d1908 1
a1908 1
	if (rq->state == QCREATED)
a1909 1

d1911 1
a1911 2
		oce_cq_destroy(rq->cq);

a1913 1

a1921 1

a1924 69
/**
 * @@brief 		Create a receive queue
 * @@param rq 		receive queue
 * @@param if_id		interface identifier index
 * @@param eq		pointer to event queue
 */
int
oce_rq_create(struct oce_rq *rq, uint32_t if_id, struct oce_eq *eq)
{
	struct oce_softc *sc = rq->sc;
	struct oce_cq *cq;

	cq = oce_cq_create(sc, eq, CQ_LEN_1024, sizeof(struct oce_nic_rx_cqe),
	    1, 0, 3);
	if (!cq)
		return ENXIO;

	rq->cq = cq;
	rq->cfg.if_id = if_id;
	eq->cq[eq->cq_valid] = cq;
	eq->cq_valid++;
	cq->cb_arg = rq;
	cq->cq_handler = oce_rq_handler;

	/* RX queue is created in oce_init */

	return 0;
}

int
oce_start_rq(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;

	if (rq->state != QCREATED) {
		if (oce_create_rq(sc, rq))
			return 1;
		rq->state 	 = QCREATED;
		rq->pending	 = 0;
		rq->ring->cidx	 = 0;
		rq->ring->pidx	 = 0;
		rq->packets_in	 = 0;
		rq->packets_out	 = 0;
		DELAY(10);
	}
	return 0;
}

void
oce_stop_rq(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->sc;

	if (rq->state == QCREATED) {
		oce_destroy_queue(sc, QTYPE_RQ, rq->id);
		rq->state = QDELETED;
		DELAY(10);
	}
}

/**
 * @@brief		function to create an event queue
 * @@param sc		software handle to the device
 * @@param q_len		length of event queue
 * @@param item_size	size of an event queue item
 * @@param eq_delay	event queue delay
 * @@retval eq	   	success, pointer to event queue
 * @@retval NULL		failure
 */
d1926 1
a1926 2
oce_eq_create(struct oce_softc *sc, uint32_t q_len, uint32_t item_size,
    uint32_t eq_delay)
a1928 1
	int rc = 0;
d1933 1
a1933 1
		return NULL;
d1935 1
a1935 1
	eq->ring = oce_create_ring(sc, q_len, item_size, 8);
d1938 1
a1938 1
		return NULL;
d1941 1
d1943 11
a1953 13
	eq->id = 0xffff;
	eq->cfg.q_len = q_len;
	eq->cfg.item_size = item_size;
	eq->cfg.cur_eqd = (uint8_t)eq_delay;

	rc = oce_create_eq(sc, eq);
	if (rc)
		goto free_eq;

	return eq;
free_eq:
	oce_eq_destroy(eq);
	return NULL;
d1957 1
a1957 1
oce_eq_destroy(struct oce_eq *eq)
d1961 1
a1961 1
	if (eq->id != 0xffff)
a1962 1

a1964 1

a1967 7
/**
 * @@brief		Function to create an MQ
 * @@param sc		software handle to the device
 * @@param eq		the EQ to associate with the MQ for event notification
 * @@param q_len		the number of entries to create in the MQ
 * @@returns		pointer to the created MQ, failure otherwise
 */
d1969 1
a1969 1
oce_mq_create(struct oce_softc *sc, struct oce_eq *eq, uint32_t q_len)
a1971 1
	int rc = 0;
d1977 7
a1983 1
		return NULL;
d1985 1
a1985 1
	cq = oce_cq_create(sc, eq, CQ_LEN_256, sizeof(struct oce_mq_cqe),
d1988 1
d1990 1
a1990 1
		return NULL;
d1993 1
d1997 8
a2004 9
	mq->ring = oce_create_ring(sc, q_len, sizeof(struct oce_mbx), 8);
	if (!mq->ring)
		goto free_mq;

	mq->cfg.q_len = (uint8_t)q_len;

	rc = oce_create_mq(sc, mq);
	if (rc)
		goto free_mq;
a2008 1
	mq->state = QCREATED;
d2010 1
a2010 1
	mq->cq->cq_handler = oce_mq_handler;
d2012 1
a2012 4
	return mq;
free_mq:
	oce_mq_destroy(mq);
	return NULL;
d2016 1
a2016 1
oce_mq_destroy(struct oce_mq *mq)
d2020 1
a2020 1
	if (mq->state == QCREATED)
a2021 1

a2023 1

d2025 1
a2025 2
		oce_cq_destroy(mq->cq);

d2041 1
a2041 1
oce_cq_create(struct oce_softc *sc, struct oce_eq *eq, uint32_t q_len,
a2045 1
	int rc = 0;
d2065 5
a2069 3
	rc = oce_create_cq(sc, cq);
	if (rc)
		goto free_cq;
d2073 1
a2073 4
	return cq;
free_cq:
	oce_cq_destroy(cq);
	return NULL;
d2077 1
a2077 1
oce_cq_destroy(struct oce_cq *cq)
a2084 1

d2138 1
d2140 3
a2142 2
	uint16_t num_eqe = 0;
	struct oce_softc *sc = eq->sc;
a2149 1
		num_eqe++;
d2151 1
d2154 1
a2154 1
	oce_arm_eq(sc, eq->id, num_eqe, FALSE, TRUE);
d2163 1
a2163 1
	int num_cqes = 0;
d2165 1
a2165 1
	oce_dma_sync(&cq->ring->dma, BUS_DMASYNC_POSTWRITE);
d2174 1
a2174 1
		num_cqes++;
d2177 1
a2177 1
	oce_arm_cq(sc, cq->id, num_cqes, FALSE);
d2187 1
a2187 1
oce_drain_mq(void *arg)
d2200 1
d2202 4
a2205 3
	uint16_t num_cqe = 0;
	struct oce_cq  *cq;
	struct oce_softc *sc;
a2206 2
	sc = rq->sc;
	cq = rq->cq;
d2214 1
a2214 1
		num_cqe++;
d2217 1
a2217 1
	oce_arm_cq(sc, cq->id, num_cqe, FALSE);
d2228 1
a2228 1
		    BUS_DMASYNC_POSTWRITE);
d2362 1
@


1.18
log
@major cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.17 2012/10/12 15:16:45 mikeb Exp $	*/
d251 1
a251 1
	if (oce_get_fw_version(sc) || oce_get_fw_config(sc)) {
a291 1
	printf("%s: %s ASIC %d\n", sc->dev.dv_xname, sc->fw_version, sc->asic_revision);
@


1.17
log
@a bit of janitoring
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.16 2012/10/11 16:38:10 mikeb Exp $	*/
d102 1
d108 1
d111 2
a136 1
int  oce_attach_ifp(struct oce_softc *sc);
d138 1
a138 1
void oce_mac_addr_set(struct oce_softc *sc);
a211 2
	int rc = 0;
	uint16_t devid;
d213 1
a213 2
	devid = PCI_PRODUCT(pa->pa_id);
	switch (devid) {
d228 1
a228 1
	if (oce_hw_pci_alloc(sc))
d237 3
a239 3
	/* initialise the hardware */
	rc = oce_hw_init(sc);
	if (rc)
d241 26
d272 2
a273 3
	rc = oce_alloc_intr(sc);
	if (rc)
		goto dma_free;
d275 2
a276 3
	rc = oce_init_queues(sc);
	if (rc)
		goto dma_free;
d278 2
a279 5
	bcopy(sc->macaddr.mac_addr, sc->arpcom.ac_enaddr, ETH_ADDR_LEN);

	rc = oce_attach_ifp(sc);
	if (rc)
		goto queues_free;
d282 2
a283 3
	rc = oce_init_lro(sc);
	if (rc)
		goto ifp_free;
d291 2
a292 1
	printf(", address %s\n", ether_sprintf(sc->macaddr.mac_addr));
d297 1
a297 1
lro_free:
d299 1
a299 1
ifp_free:
d303 1
a303 1
queues_free:
d305 1
a305 1
dma_free:
d316 1
a316 1
	oce_arm_cq(sc->mq->parent, sc->mq->cq->cq_id, 0, TRUE);
d322 2
a323 2
	oce_hw_intr_enable(sc);
	oce_arm_eq(sc, sc->eq[0]->eq_id, 0, TRUE, FALSE);
d333 44
d497 1
a497 1
	oce_arm_eq(sc, eq->eq_id, num_eqes, FALSE, TRUE);
d508 1
a508 1
		oce_arm_cq(sc, cq->cq_id, 0, TRUE);
d512 1
a512 1
	oce_arm_eq(sc, eq->eq_id, 0, TRUE, FALSE);
d516 77
d622 20
d877 1
a877 1
	reg_value = (num_wqes << 16) | wq->wq_id;
d891 1
a891 1
	struct oce_softc *sc = (struct oce_softc *) wq->parent;
d1046 1
a1046 1
	struct oce_softc *sc = wq->parent;
d1071 1
a1071 1
		oce_arm_cq(sc, cq->cq_id, num_cqes, FALSE);
d1077 1
a1077 1
	struct oce_softc *sc = (struct oce_softc *)rq->parent;
d1212 1
a1212 1
	struct oce_softc *sc = (struct oce_softc *) rq->parent;
d1280 1
a1280 1
	struct oce_softc *sc = (struct oce_softc *) rq->parent;
d1331 1
a1331 1
	struct oce_softc *sc = (struct oce_softc *)rq->parent;
d1383 1
a1383 1
	struct oce_softc *sc = (struct oce_softc *)rq->parent;
d1395 1
a1395 1
		rxdb_reg.bits.qid = rq->rq_id;
d1402 1
a1402 1
		rxdb_reg.bits.qid = rq->rq_id;
d1431 1
a1431 1
	struct oce_softc *sc = rq->parent;
d1479 1
a1479 1
		oce_arm_cq(sc, cq->cq_id, num_cqes, FALSE);
a1485 44
int
oce_attach_ifp(struct oce_softc *sc)
{
	struct ifnet *ifp = &sc->arpcom.ac_if;

	ifmedia_init(&sc->media, IFM_IMASK, oce_media_change, oce_media_status);
	ifmedia_add(&sc->media, IFM_ETHER | IFM_AUTO, 0, NULL);
	ifmedia_set(&sc->media, IFM_ETHER | IFM_AUTO);

	strlcpy(ifp->if_xname, sc->dev.dv_xname, IFNAMSIZ);
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = oce_ioctl;
	ifp->if_start = oce_start;
	ifp->if_watchdog = oce_watchdog;
	ifp->if_hardmtu = OCE_MAX_MTU;
	ifp->if_softc = sc;
	IFQ_SET_MAXLEN(&ifp->if_snd, sc->tx_ring_size - 1);
	IFQ_SET_READY(&ifp->if_snd);

	/* oce splits jumbos into 2k chunks... */
	m_clsetwms(ifp, MCLBYTES, 8, sc->rx_ring_size);

	ifp->if_capabilities = IFCAP_VLAN_MTU;

#if NVLAN > 0
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

#if defined(INET6) || defined(INET)
#ifdef OCE_TSO
	ifp->if_capabilities |= IFCAP_TSO;
	ifp->if_capabilities |= IFCAP_VLAN_HWTSO;
#endif
#ifdef OCE_LRO
	ifp->if_capabilities |= IFCAP_LRO;
#endif
#endif

	if_attach(ifp);
	ether_ifattach(ifp);

	return 0;
}

d1487 1
a1487 1
oce_mac_addr_set(struct oce_softc *sc)
d1492 1
a1492 1
	if (!bcmp(sc->arpcom.ac_enaddr, sc->macaddr.mac_addr, ETH_ADDR_LEN))
d1495 1
a1495 1
	status = oce_mbox_macaddr_add(sc, sc->arpcom.ac_enaddr, sc->if_id,
d1497 4
a1500 6
	if (!status) {
		status = oce_mbox_macaddr_del(sc, sc->if_id, old_pmac_id);
		bcopy(sc->arpcom.ac_enaddr, sc->macaddr.mac_addr,
		    sc->macaddr.size_of_struct);
	} else
		printf("%s: Failed to update MAC address\n", sc->dev.dv_xname);
d1541 1
a1541 1
	oce_hw_intr_disable(sc);
d1571 1
a1571 1
	oce_mac_addr_set(sc);
d1603 1
a1603 1
		oce_arm_cq(rq->parent, rq->cq->cq_id, 0, TRUE);
d1606 1
a1606 1
		oce_arm_cq(wq->parent, wq->cq->cq_id, 0, TRUE);
d1608 1
a1608 1
	oce_arm_cq(sc->mq->parent, sc->mq->cq->cq_id, 0, TRUE);
d1611 1
a1611 1
		oce_arm_eq(sc, eq->eq_id, 0, TRUE, FALSE);
d1621 1
a1621 1
	oce_hw_intr_enable(sc);
d1650 1
a1650 1
	struct oce_softc *sc = mq->parent;
d1687 1
a1687 1
		oce_arm_cq(sc, cq->cq_id, num_cqes, FALSE /* TRUE */);
d1713 1
a1713 1
	if (oce_create_iface(sc))
a1803 1
	wq->cfg.nhdl = 2 * wq->cfg.q_len;
d1805 1
a1805 1
	wq->parent = sc;
d1830 1
a1830 1
	struct oce_softc *sc = wq->parent;
d1833 2
a1834 2
	if (wq->qstate == QCREATED)
		oce_mbox_destroy_q(sc, QTYPE_WQ, wq->wq_id);
d1860 1
a1860 1
	struct oce_softc *sc = wq->parent;
d1871 1
a1871 1
	rc = oce_mbox_create_wq(wq);
d1875 1
a1875 2
	wq->qstate = QCREATED;
	wq->wq_free = wq->cfg.q_len;
d1921 1
a1921 1
	rq->parent = sc;
d1944 1
a1944 1
	struct oce_softc *sc = rq->parent;
d1947 2
a1948 2
	if (rq->qstate == QCREATED)
		oce_mbox_destroy_q(sc, QTYPE_RQ, rq->rq_id);
d1977 1
a1977 1
	struct oce_softc *sc = rq->parent;
d2000 4
a2003 2
	if (rq->qstate != QCREATED) {
		if (oce_mbox_create_rq(rq))
d2005 1
a2005 1
		rq->qstate 	 = QCREATED;
d2019 1
a2019 1
	struct oce_softc *sc = rq->parent;
d2021 3
a2023 3
	if (rq->qstate == QCREATED) {
		oce_mbox_destroy_q(sc, QTYPE_RQ, rq->rq_id);
		rq->qstate = QDELETED;
d2055 5
a2059 5
	eq->parent = sc;
	eq->eq_id = 0xffff;
	eq->eq_cfg.q_len = q_len;
	eq->eq_cfg.item_size = item_size;
	eq->eq_cfg.cur_eqd = (uint8_t)eq_delay;
d2061 1
a2061 1
	rc = oce_mbox_create_eq(eq);
d2074 1
a2074 1
	struct oce_softc *sc = eq->parent;
d2076 2
a2077 2
	if (eq->eq_id != 0xffff)
		oce_mbox_destroy_q(sc, QTYPE_EQ, eq->eq_id);
d2111 1
a2111 1
	mq->parent = sc;
d2120 1
a2120 1
	rc = oce_mbox_create_mq(mq);
d2127 1
a2127 1
	mq->qstate = QCREATED;
d2140 1
a2140 1
	struct oce_softc *sc = mq->parent;
d2142 2
a2143 2
	if (mq->qstate == QCREATED)
		oce_mbox_destroy_q(sc, QTYPE_MQ, mq->mq_id);
d2167 1
a2167 1
    uint32_t item_size, uint32_t is_eventable, uint32_t nodelay,
d2183 1
a2183 1
	cq->parent = sc;
d2185 5
a2189 3
	cq->cq_cfg.q_len = q_len;
	cq->cq_cfg.item_size = item_size;
	cq->cq_cfg.nodelay = (uint8_t) nodelay;
d2191 1
a2191 1
	rc = oce_mbox_create_cq(cq, ncoalesce, is_eventable);
d2206 1
a2206 1
	struct oce_softc *sc = cq->parent;
d2209 1
a2209 1
		oce_mbox_destroy_q(sc, QTYPE_CQ, cq->cq_id);
d2268 1
a2268 1
	struct oce_softc *sc = eq->parent;
d2280 1
a2280 1
	oce_arm_eq(sc, eq->eq_id, num_eqe, FALSE, TRUE);
d2286 1
a2286 1
	struct oce_softc *sc = wq->parent;
d2303 1
a2303 1
	oce_arm_cq(sc, cq->cq_id, num_cqes, FALSE);
d2331 1
a2331 1
	sc = rq->parent;
d2343 1
a2343 1
	oce_arm_cq(sc, cq->cq_id, num_cqe, FALSE);
a2444 7
void
oce_destroy_ring(struct oce_softc *sc, struct oce_ring *ring)
{
	oce_dma_free(sc, &ring->dma);
	free(ring, M_DEVBUF);
}

d2501 7
@


1.16
log
@rework the firmware interface and incorporate all the guts into
one single function instead of spreading them across 10 others.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.15 2012/10/11 16:33:57 mikeb Exp $	*/
d153 2
a154 1
int  oce_queue_init_all(struct oce_softc *sc);
a156 1
void oce_queue_release_all(struct oce_softc *sc);
d160 3
a162 3
void oce_drain_mq_cq(void *arg);
void oce_drain_rq_cq(struct oce_rq *rq);
void oce_drain_wq_cq(struct oce_wq *wq);
d166 1
a166 2
void oce_wq_free(struct oce_wq *wq);
void oce_wq_del(struct oce_wq *wq);
d170 1
a170 2
void oce_rq_free(struct oce_rq *rq);
void oce_rq_del(struct oce_rq *rq);
d173 1
a173 1
void oce_eq_del(struct oce_eq *eq);
d176 1
a176 1
void oce_mq_free(struct oce_mq *mq);
d180 1
a180 1
void oce_cq_del(struct oce_softc *sc, struct oce_cq *cq);
d250 1
a250 1
	rc = oce_queue_init_all(sc);
d283 1
a283 1
	oce_queue_release_all(sc);
d293 1
a293 2
	if (oce_get_link_status(sc))
		goto error;
a308 11

	return;

 error:
	timeout_del(&sc->rxrefill);
	timeout_del(&sc->timer);
	oce_hw_intr_disable(sc);
	ether_ifdetach(&sc->arpcom.ac_if);
	if_detach(&sc->arpcom.ac_if);
	oce_queue_release_all(sc);
	oce_dma_free(sc, &sc->bsmbx);
d416 1
a416 1
	do {
d424 1
a424 2

	} while (TRUE);
d1432 2
a1433 1
		oce_drain_rq_cq(rq);
d1436 1
a1436 3
		oce_drain_wq_cq(wq);

	DELAY(10);
d1453 2
a1573 5
/**
 * @@brief	Create and initialize all the queues on the board
 * @@param sc	software handle to the device
 * @@returns 0	if successful, or error
 **/
d1575 1
a1575 1
oce_queue_init_all(struct oce_softc *sc)
d1629 1
a1629 1
	oce_queue_release_all(sc);
a1632 4
/**
 * @@brief Releases all mailbox queues created
 * @@param sc		software handle to the device
 */
d1634 1
a1634 1
oce_queue_release_all(struct oce_softc *sc)
d1642 2
a1643 4
		if (rq) {
			oce_rq_del(sc->rq[i]);
			oce_rq_free(sc->rq[i]);
		}
d1647 2
a1648 4
		if (wq) {
			oce_wq_del(sc->wq[i]);
			oce_wq_free(sc->wq[i]);
		}
d1652 1
a1652 1
		oce_mq_free(sc->mq);
d1656 1
a1656 1
			oce_eq_del(sc->eq[i]);
d1690 1
a1690 1
	wq->parent = (void *)sc;
d1708 1
a1708 2
	printf("%s: Create WQ failed\n", sc->dev.dv_xname);
	oce_wq_free(wq);
a1711 4
/**
 * @@brief 		Frees the work queue
 * @@param wq		pointer to work queue to free
 */
d1713 1
a1713 1
oce_wq_free(struct oce_wq *wq)
d1715 1
a1715 1
	struct oce_softc *sc = (struct oce_softc *) wq->parent;
d1718 7
a1724 1
	if (wq->ring != NULL) {
a1725 2
		wq->ring = NULL;
	}
a1730 1
			wq->pckts[i].map = NULL;
a1733 2
	wq->tag = NULL;

d1772 1
a1772 2
	printf("%s: failed to create wq\n", sc->dev.dv_xname);
	oce_wq_del(wq);
a1776 19
 * @@brief 		Delete a work queue
 * @@param wq		pointer to work queue
 */
void
oce_wq_del(struct oce_wq *wq)
{
	struct oce_softc *sc = (struct oce_softc *) wq->parent;

	if (wq->qstate == QCREATED)
		oce_mbox_destroy_q(sc, QTYPE_WQ, wq->wq_id);
	wq->qstate = QDELETED;

	if (wq->cq != NULL) {
		oce_cq_del(sc, wq->cq);
		wq->cq = NULL;
	}
}

/**
d1807 1
a1807 1
	rq->parent = (void *)sc;
a1821 1

d1823 1
a1823 2
	printf("%s: failed to create rq\n", sc->dev.dv_xname);
	oce_rq_free(rq);
a1826 4
/**
 * @@brief 		Free a receive queue
 * @@param rq		pointer to receive queue
 */
d1828 1
a1828 1
oce_rq_free(struct oce_rq *rq)
d1830 8
a1837 2
	struct oce_softc *sc = (struct oce_softc *) rq->parent;
	int i = 0 ;
d1839 1
a1839 1
	if (rq->ring != NULL) {
d1841 1
a1841 2
		rq->ring = NULL;
	}
a1845 1
			rq->pckts[i].map = NULL;
d1847 1
a1847 1
		if (rq->pckts[i].mbuf) {
a1848 2
			rq->pckts[i].mbuf = NULL;
		}
a1850 2
	rq->tag = NULL;

a1872 1

d1883 17
a1899 4
/**
 * @@brief 		Delete a receive queue
 * @@param rq		receive queue
 */
d1901 1
a1901 1
oce_rq_del(struct oce_rq *rq)
d1903 1
a1903 1
	struct oce_softc *sc = (struct oce_softc *)rq->parent;
d1905 1
a1905 1
	if (rq->qstate == QCREATED)
d1907 2
a1908 5
	rq->qstate = QDELETED;

	if (rq->cq != NULL) {
		oce_cq_del(sc, rq->cq);
		rq->cq = NULL;
d1933 6
a1940 5

	eq->ring = oce_create_ring(sc, q_len, item_size, 8);
	if (!eq->ring)
		goto free_eq;

d1951 1
a1951 2
	printf("%s: failed to create eq\n", __func__);
	oce_eq_del(eq);
a1954 4
/**
 * @@brief 		Function to delete an event queue
 * @@param eq		pointer to an event queue
 */
d1956 1
a1956 1
oce_eq_del(struct oce_eq *eq)
d1958 1
a1958 1
	struct oce_softc *sc = (struct oce_softc *)eq->parent;
d1963 1
a1963 1
	if (eq->ring != NULL) {
a1964 2
		eq->ring = NULL;
	}
d2000 1
a2000 1
		goto error;
d2006 1
a2006 1
		goto error;
d2016 3
a2018 5
error:
	printf("%s: failed to create mq\n", sc->dev.dv_xname);
	oce_mq_free(mq);
	mq = NULL;
	return mq;
a2020 4
/**
 * @@brief		Function to free a mailbox queue
 * @@param mq		pointer to a mailbox queue
 */
d2022 1
a2022 1
oce_mq_free(struct oce_mq *mq)
d2024 1
a2024 1
	struct oce_softc *sc = (struct oce_softc *) mq->parent;
d2026 2
a2027 2
	if (!mq)
		return;
d2029 1
a2029 1
	if (mq->ring != NULL) {
a2030 5
		mq->ring = NULL;
		if (mq->qstate == QCREATED)
			oce_mbox_destroy_q(sc, QTYPE_MQ, mq->mq_id);
		mq->qstate = QDELETED;
	}
d2032 2
a2033 4
	if (mq->cq != NULL) {
		oce_cq_del(sc, mq->cq);
		mq->cq = NULL;
	}
a2035 1
	mq = NULL;
d2062 4
a2065 2
	if (!cq->ring)
		goto error;
d2075 1
a2075 1
		goto error;
d2080 2
a2081 3
error:
	printf("%s: failed to create cq\n", sc->dev.dv_xname);
	oce_cq_del(sc, cq);
a2084 5
/**
 * @@brief		Deletes the completion queue
 * @@param sc		software handle to the device
 * @@param cq		pointer to a completion queue
 */
d2086 1
a2086 1
oce_cq_del(struct oce_softc *sc, struct oce_cq *cq)
d2088 2
a2091 1
		/*NOW destroy the ring */
a2092 1
		cq->ring = NULL;
a2095 1
	cq = NULL;
d2152 1
a2152 1
	do {
d2160 1
a2160 2

	} while (TRUE);
d2166 1
a2166 1
oce_drain_wq_cq(struct oce_wq *wq)
d2175 1
a2175 1
	do {
d2183 1
a2183 2

	} while (TRUE);
d2195 1
a2195 1
oce_drain_mq_cq(void *arg)
d2206 1
a2206 1
oce_drain_rq_cq(struct oce_rq *rq)
a2251 19
void
oce_stop_rq(struct oce_rq *rq)
{
	struct oce_softc *sc = rq->parent;

	if (rq->qstate == QCREATED) {
		/* Delete rxq in firmware */

		oce_mbox_destroy_q(sc, QTYPE_RQ, rq->rq_id);

		rq->qstate = QDELETED;

		DELAY(1);

		/* Free posted RX buffers that are not used */
		oce_free_posted_rxbuf(rq);
	}
}

d2253 1
a2253 30
oce_start_rq(struct oce_rq *rq)
{
	if (rq->qstate == QCREATED)
		return 0;
	if (oce_mbox_create_rq(rq))
		return 1;
	/* reset queue pointers */
	rq->qstate 	 = QCREATED;
	rq->pending	 = 0;
	rq->ring->cidx	 = 0;
	rq->ring->pidx	 = 0;
	rq->packets_in	 = 0;
	rq->packets_out	 = 0;

	DELAY(10);

	return 0;
}

/**
 * @@brief		Allocate DMA memory
 * @@param sc		software handle to the device
 * @@param size		bus size
 * @@param dma		dma memory area
 * @@param flags		creation flags
 * @@returns		0 on success, error otherwize
 */
int
oce_dma_alloc(struct oce_softc *sc, bus_size_t size, struct oce_dma_mem *dma,
    int flags)
d2268 1
a2268 1
	    &dma->nsegs, BUS_DMA_NOWAIT);
d2282 1
a2282 1
	    flags | BUS_DMA_NOWAIT);
a2305 5
/**
 * @@brief		Free DMA memory
 * @@param sc		software handle to the device
 * @@param dma		dma area to free
 */
a2326 5
/**
 * @@brief		Destroy a ring buffer
 * @@param sc		software handle to the device
 * @@param ring		ring buffer
 */
d2392 2
a2393 8
/**
 * @@brief		Load bus dma map for a ring buffer
 * @@param ring		ring buffer pointer
 * @@param pa_list	physical address list
 * @@returns		number entries
 */
uint32_t
oce_page_list(struct oce_softc *sc, struct oce_ring *ring,
d2402 1
a2402 1
		printf("%s: oce_page_list failed to load\n", sc->dev.dv_xname);
@


1.15
log
@better integer log2 implementation, checked with what linux is doing
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.14 2012/08/09 19:29:03 mikeb Exp $	*/
a178 2
int  oce_destroy_q(struct oce_softc *sc, struct oce_mbx *mbx, size_t req_size,
    enum qtype qtype);
a267 4
	rc = oce_stats_init(sc);
	if (rc)
		goto stats_free;

a276 2
stats_free:
	oce_stats_free(sc);
a317 1
	oce_stats_free(sc);
d390 1
d393 3
a395 1
	int promisc = 0;
d403 12
a414 2
	} else
		oce_hw_update_multicast(sc);
d416 1
a416 1
	oce_rxf_set_promiscuous(sc, promisc);
d1412 1
a1412 1
	if (!(oce_stats_get(sc, &rxe, &txe))) {
d1616 1
a1616 1
	if (oce_create_nw_interface(sc))
a1814 2
	struct oce_mbx mbx;
	struct mbx_delete_nic_wq *fwcmd;
d1817 3
a1819 9
	if (wq->qstate == QCREATED) {
		bzero(&mbx, sizeof(struct oce_mbx));
		/* now fill the command */
		fwcmd = (struct mbx_delete_nic_wq *)&mbx.payload;
		fwcmd->params.req.wq_id = wq->wq_id;
		(void)oce_destroy_q(sc, &mbx,
				sizeof(struct mbx_delete_nic_wq), QTYPE_WQ);
		wq->qstate = QDELETED;
	}
d1948 1
a1948 3
	struct oce_softc *sc = (struct oce_softc *) rq->parent;
	struct oce_mbx mbx;
	struct mbx_delete_nic_rq *fwcmd;
d1950 3
a1952 9
	if (rq->qstate == QCREATED) {
		bzero(&mbx, sizeof(mbx));

		fwcmd = (struct mbx_delete_nic_rq *)&mbx.payload;
		fwcmd->params.req.rq_id = rq->rq_id;
		(void)oce_destroy_q(sc, &mbx,
				sizeof(struct mbx_delete_nic_rq), QTYPE_RQ);
		rq->qstate = QDELETED;
	}
d2010 4
a2013 11
	struct oce_mbx mbx;
	struct mbx_destroy_common_eq *fwcmd;
	struct oce_softc *sc = (struct oce_softc *) eq->parent;

	if (eq->eq_id != 0xffff) {
		bzero(&mbx, sizeof(mbx));
		fwcmd = (struct mbx_destroy_common_eq *)&mbx.payload;
		fwcmd->params.req.id = eq->eq_id;
		(void)oce_destroy_q(sc, &mbx,
			sizeof(struct mbx_destroy_common_eq), QTYPE_EQ);
	}
a2020 1

a2084 2
	struct oce_mbx mbx;
	struct mbx_destroy_common_mq *fwcmd;
d2092 2
a2093 8
		if (mq->qstate == QCREATED) {
			bzero(&mbx, sizeof (struct oce_mbx));
			fwcmd = (struct mbx_destroy_common_mq *)&mbx.payload;
			fwcmd->params.req.id = mq->mq_id;
			(void)oce_destroy_q(sc, &mbx,
				sizeof (struct mbx_destroy_common_mq),
				QTYPE_MQ);
		}
a2106 59
 * @@brief		Function to delete a EQ, CQ, MQ, WQ or RQ
 * @@param sc		sofware handle to the device
 * @@param mbx		mailbox command to send to the fw to delete the queue
 *			(mbx contains the queue information to delete)
 * @@param req_size	the size of the mbx payload dependent on the qtype
 * @@param qtype		the type of queue i.e. EQ, CQ, MQ, WQ or RQ
 * @@returns 		0 on success, failure otherwise
 */
int
oce_destroy_q(struct oce_softc *sc, struct oce_mbx *mbx, size_t req_size,
    enum qtype qtype)
{
	struct mbx_hdr *hdr = (struct mbx_hdr *)&mbx->payload;
	int opcode;
	int subsys;
	int rc = 0;

	switch (qtype) {
	case QTYPE_EQ:
		opcode = OPCODE_COMMON_DESTROY_EQ;
		subsys = MBX_SUBSYSTEM_COMMON;
		break;
	case QTYPE_CQ:
		opcode = OPCODE_COMMON_DESTROY_CQ;
		subsys = MBX_SUBSYSTEM_COMMON;
		break;
	case QTYPE_MQ:
		opcode = OPCODE_COMMON_DESTROY_MQ;
		subsys = MBX_SUBSYSTEM_COMMON;
		break;
	case QTYPE_WQ:
		opcode = OPCODE_NIC_DELETE_WQ;
		subsys = MBX_SUBSYSTEM_NIC;
		break;
	case QTYPE_RQ:
		opcode = OPCODE_NIC_DELETE_RQ;
		subsys = MBX_SUBSYSTEM_NIC;
		break;
	default:
		return EINVAL;
	}

	mbx_common_req_hdr_init(hdr, 0, 0, subsys,
				opcode, MBX_TIMEOUT_SEC, req_size,
				OCE_MBX_VER_V0);

	mbx->u0.s.embedded = 1;
	mbx->payload_length = (uint32_t) req_size;
	DW_SWAP(u32ptr(mbx), mbx->payload_length + OCE_BMBX_RHDR_SZ);

	rc = oce_mbox_post(sc, mbx, NULL);

	if (rc != 0)
		printf("%s: Failed to del q\n", sc->dev.dv_xname);

	return rc;
}

/**
a2159 3
	struct oce_mbx mbx;
	struct mbx_destroy_common_cq *fwcmd;

d2161 1
a2161 7

		bzero(&mbx, sizeof(struct oce_mbx));
		/* now fill the command */
		fwcmd = (struct mbx_destroy_common_cq *)&mbx.payload;
		fwcmd->params.req.id = cq->cq_id;
		(void)oce_destroy_q(sc, &mbx,
			sizeof(struct mbx_destroy_common_cq), QTYPE_CQ);
a2330 2
	struct oce_mbx mbx;
	struct mbx_delete_nic_rq *fwcmd;
d2335 1
a2335 6
		bzero(&mbx, sizeof(mbx));
		fwcmd = (struct mbx_delete_nic_rq *)&mbx.payload;
		fwcmd->params.req.rq_id = rq->rq_id;

		(void)oce_destroy_q(sc, &mbx,
		    sizeof(struct mbx_delete_nic_rq), QTYPE_RQ);
@


1.14
log
@don't compile rss functions in unless OCE_RSS is specified
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.13 2012/08/09 19:23:35 mikeb Exp $	*/
d1847 1
a1847 1
	if (OCE_LOG2(frag_size) <= 0)
@


1.13
log
@oops. missed these conflicts in the previous commit
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.12 2012/08/09 19:15:47 mikeb Exp $	*/
d1485 1
d1494 1
@


1.12
log
@schedule a rx refill if the ring is empty, sprinkle some dma syncs
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.11 2012/08/09 19:03:14 mikeb Exp $	*/
d1402 2
d1408 8
a1415 6
	oce_refresh_nic_stats(sc);

#if 0
	/* TX Watchdog */
	oce_start(ifp);
#endif
@


1.11
log
@remove internal queue stats, use if_* counters wherever possible
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.10 2012/08/09 18:49:57 mikeb Exp $	*/
d1267 3
a1269 2
		if (!oce_alloc_rx_bufs(rq))
			; /* timeout_add(&sc->rxrefill, 10); */
d2508 3
d2604 3
a2606 1
	oce_dma_sync(&ring->dma, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d2648 3
@


1.10
log
@respect the mtu value that ifconfig sets
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.9 2012/08/09 18:41:45 mikeb Exp $	*/
a284 1
	oce_hw_intr_disable(sc);
d534 1
a534 1
	struct oce_softc *sc = (struct oce_softc *)ifp->if_softc;
d582 1
d722 1
a722 5
	sc->arpcom.ac_if.if_opackets++;
	wq->tx_stats.tx_reqs++;
	wq->tx_stats.tx_wrbs += num_wqes;
	wq->tx_stats.tx_bytes += m->m_pkthdr.len;
	wq->tx_stats.tx_pkts++;
d910 1
a910 1
		wq->tx_stats.tx_compl++;
d942 1
a942 1
	if(IS_BE(sc))
d1038 1
a1038 1
				goto post_done;
a1050 14

#ifdef OCE_LRO
#if defined(INET6) || defined(INET)
post_done:
#endif
#endif
		/* Update rx stats per queue */
		rq->rx_stats.rx_pkts++;
		rq->rx_stats.rx_bytes += cqe->u0.s.pkt_size;
		rq->rx_stats.rx_frags += cqe->u0.s.num_fragments;
		if (cqe->u0.s.pkt_type == OCE_MULTICAST_PACKET)
			rq->rx_stats.rx_mcast_pkts++;
		if (cqe->u0.s.pkt_type == OCE_UNICAST_PACKET)
			rq->rx_stats.rx_ucast_pkts++;
d1098 1
a1098 1
		vtp =  cqe_v1->u0.s.vlan_tag_present;
a1109 1
	int port_id = 0;
d1113 1
a1113 2
		port_id =  cqe_v1->u0.s.port;
		if (sc->port_id != port_id)
d1281 1
d1293 1
a1293 2
			rq->rx_stats.rxcp_err++;
			sc->arpcom.ac_if.if_ierrors++;
a1300 1
		rq->rx_stats.rx_compl++;
d1305 1
a1305 1
		if (IF_LRO_ENABLED(sc) && rq->lro_pkts_queued >= 16) {
a1306 1
		}
a1405 1
	oce_refresh_queue_stats(sc);
@


1.9
log
@set a timeout in case the chip goes out to lunch
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.8 2012/08/09 12:43:46 mikeb Exp $	*/
d129 2
a130 2
int  oce_start_rx(struct oce_softc *sc);
void oce_stop_rx(struct oce_softc *sc);
d169 1
a169 1
    uint32_t frag_size, uint32_t mtu, uint32_t rss);
a1455 2
	oce_stop_rx(sc);

d1459 2
a1460 1
	for_all_rq_queues(sc, rq, i)
d1462 1
a1486 5
	if (oce_start_rx(sc)) {
		printf("%s: failed to create rq\n", sc->dev.dv_xname);
		goto error;
	}

d1488 6
d1622 1
a1622 1
		    OCE_MAX_JUMBO_FRAME_SIZE, (i == 0) ? 0 : sc->rss_enable);
d1858 1
a1858 1
    uint32_t mtu, uint32_t rss)
a1875 1
	rq->cfg.mtu = mtu;
d2440 1
a2440 1
oce_stop_rx(struct oce_softc *sc)
d2442 1
a2444 2
	struct oce_rq *rq;
	int i;
d2446 2
a2447 3
	for_all_rq_queues(sc, rq, i) {
		if (rq->qstate == QCREATED) {
			/* Delete rxq in firmware */
d2449 3
a2451 3
			bzero(&mbx, sizeof(mbx));
			fwcmd = (struct mbx_delete_nic_rq *)&mbx.payload;
			fwcmd->params.req.rq_id = rq->rq_id;
d2453 2
a2454 2
			(void)oce_destroy_q(sc, &mbx,
				sizeof(struct mbx_delete_nic_rq), QTYPE_RQ);
d2456 1
a2456 1
			rq->qstate = QDELETED;
d2458 1
a2458 1
			DELAY(1);
d2460 2
a2461 3
			/* Free posted RX buffers that are not used */
			oce_free_posted_rxbuf(rq);
		}
d2466 1
a2466 1
oce_start_rx(struct oce_softc *sc)
d2468 11
a2478 17
	struct oce_rq *rq;
	int rc = 0, i;

	for_all_rq_queues(sc, rq, i) {
		if (rq->qstate == QCREATED)
			continue;
		rc = oce_mbox_create_rq(rq);
		if (rc)
			return rc;
		/* reset queue pointers */
		rq->qstate 	 = QCREATED;
		rq->pending	 = 0;
		rq->ring->cidx	 = 0;
		rq->ring->pidx	 = 0;
		rq->packets_in	 = 0;
		rq->packets_out	 = 0;
	}
d2482 1
a2482 1
	return rc;
@


1.8
log
@remove oce_dmamap_sync wrapper
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.7 2012/08/08 17:50:24 mikeb Exp $	*/
d120 1
d770 1
a770 1
			ifp->if_flags &= ~(IFF_OACTIVE);
d774 2
d850 10
d864 1
a864 2
	int rc = 0;
	int def_q = 0; /* Defualt tx queue is 0 */
d869 1
a869 1
	do {
d874 2
a875 4
		rc = oce_encap(sc, &m, def_q);
		if (rc) {
			if (m != NULL) {
				sc->wq[def_q]->tx_stats.tx_stops++;
a876 1
			}
d886 2
a887 1
	} while (TRUE);
d889 3
a891 1
	return;
d1369 1
@


1.7
log
@call m_cluncount to account correctly for cluster chains the driver
builds upon receiving a jumbo frame.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.6 2012/08/08 09:50:15 mikeb Exp $	*/
d655 2
a656 1
	oce_dmamap_sync(wq->tag, pd->map, BUS_DMASYNC_PREWRITE);
d759 2
a760 1
	oce_dmamap_sync(wq->tag, pd->map, BUS_DMASYNC_POSTWRITE);
d950 2
a951 1
		oce_dmamap_sync(rq->tag, pd->map, BUS_DMASYNC_POSTWRITE);
d1088 2
a1089 1
		oce_dmamap_sync(rq->tag, pd->map, BUS_DMASYNC_POSTWRITE);
d1225 2
a1226 1
	oce_dmamap_sync(rq->tag, pd->map, BUS_DMASYNC_PREREAD);
a2401 1

d2409 2
a2410 1
		oce_dmamap_sync(rq->tag, pd->map, BUS_DMASYNC_POSTWRITE);
@


1.6
log
@remove rx debugging code
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.5 2012/08/07 17:16:26 mikeb Exp $	*/
d988 3
d1035 2
a1036 3
			if (ifp->if_bpf)
				bpf_mtap_ether(ifp->if_bpf, m,
				    BPF_DIRECTION_IN);
@


1.5
log
@a bit of cleanup and a tx delay tweak
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.4 2012/08/07 09:23:13 mikeb Exp $	*/
a1273 21
#ifdef OCE_DEBUG
void oce_inspect_rxring(struct oce_softc *sc, struct oce_ring *ring);

void
oce_inspect_rxring(struct oce_softc *sc, struct oce_ring *ring)
{
	struct oce_nic_rx_cqe *cqe;
	int i;

	printf("%s: cidx %d pidx %d used %d from %d\n", sc->dev.dv_xname,
	    ring->cidx, ring->pidx, ring->num_used, ring->num_items);

	for (i = 0; i < ring->num_items; i++) {
		cqe = OCE_DMAPTR(&ring->dma, struct oce_nic_rx_cqe) + i;
		if (cqe->u0.dw[0] || cqe->u0.dw[1] || cqe->u0.dw[2])
			printf("%s: cqe %d dw0=%#x dw1=%#x dw2=%#x\n", sc->dev.dv_xname,
			    i, cqe->u0.dw[0], cqe->u0.dw[1], cqe->u0.dw[2]);
	}
}
#endif

a1284 5

#ifdef OCE_DEBUG
	oce_inspect_rxring(sc, cq->ring);
#endif

a1285 6

#ifdef OCE_DEBUG
	printf("%s: %s %x %x %x\n", sc->dev.dv_xname, __func__,
	    cqe->u0.dw[0], cqe->u0.dw[1], cqe->u0.dw[2]);
#endif

@


1.4
log
@don't forget to set IFF_ALLMULTI;  from brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.3 2012/08/06 21:55:31 mikeb Exp $	*/
d315 4
a318 3
	/* Send first mcc cmd and after that we get gracious
	   MCC notifications from FW
	*/
a1404 2

	ifp->if_baudrate = IF_Gbps(10UL);
@


1.3
log
@kill sc->promisc
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.2 2012/08/02 22:14:31 mikeb Exp $	*/
d405 2
a406 1
	    ac->ac_multicnt > OCE_MAX_MC_FILTER_SIZE)
d408 1
a408 1
	else
@


1.2
log
@get rid of some "capabilities" leftovers
@
text
@d1 1
a1 1
/*	$OpenBSD: if_oce.c,v 1.1 2012/08/02 17:35:52 mikeb Exp $	*/
a238 1
	sc->promisc	 = OCE_DEFAULT_PROMISCUOUS;
d399 2
d404 7
a410 10
	if ((ifp->if_flags & IFF_PROMISC) && !sc->promisc) {
		sc->promisc = TRUE;
		oce_rxf_set_promiscuous(sc, sc->promisc);
	} else if (!(ifp->if_flags & IFF_PROMISC) && sc->promisc) {
		sc->promisc = FALSE;
		oce_rxf_set_promiscuous(sc, sc->promisc);
	}
	if (oce_hw_update_multicast(sc))
		printf("%s: Update multicast address failed\n",
		    sc->dev.dv_xname);
@


1.1
log
@Add a driver for Emulex OneConnect 10Gb Ethernet obtained from FreeBSD
but heavily massaged to look like other BSD network drivers. Support is
provided for cards based on the following controllers:

 o  ServerEngines BladeEngine 2
 o  ServerEngines BladeEngine 3
 o  Emulex Lancer
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d234 1
a234 1
	sc->rss_enable 	 = OCE_MODCAP_RSS;
a247 1
	sc->rss_enable = 0;
@

