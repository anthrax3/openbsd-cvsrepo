head	1.8;
access;
symbols
	OPENBSD_5_1_BASE:1.7
	OPENBSD_5_1:1.7.0.18
	OPENBSD_5_0:1.7.0.16
	OPENBSD_5_0_BASE:1.7
	OPENBSD_4_9:1.7.0.14
	OPENBSD_4_9_BASE:1.7
	OPENBSD_4_8:1.7.0.12
	OPENBSD_4_8_BASE:1.7
	OPENBSD_4_7:1.7.0.8
	OPENBSD_4_7_BASE:1.7
	OPENBSD_4_6:1.7.0.10
	OPENBSD_4_6_BASE:1.7
	OPENBSD_4_5:1.7.0.6
	OPENBSD_4_5_BASE:1.7
	OPENBSD_4_4:1.7.0.4
	OPENBSD_4_4_BASE:1.7
	OPENBSD_4_3:1.7.0.2
	OPENBSD_4_3_BASE:1.7
	OPENBSD_4_2:1.6.0.6
	OPENBSD_4_2_BASE:1.6
	OPENBSD_4_1:1.6.0.4
	OPENBSD_4_1_BASE:1.6
	OPENBSD_4_0:1.6.0.2
	OPENBSD_4_0_BASE:1.6
	OPENBSD_3_9:1.5.0.14
	OPENBSD_3_9_BASE:1.5
	OPENBSD_3_8:1.5.0.12
	OPENBSD_3_8_BASE:1.5
	OPENBSD_3_7:1.5.0.10
	OPENBSD_3_7_BASE:1.5
	OPENBSD_3_6:1.5.0.8
	OPENBSD_3_6_BASE:1.5
	SMP_SYNC_A:1.5
	SMP_SYNC_B:1.5
	OPENBSD_3_5:1.5.0.6
	OPENBSD_3_5_BASE:1.5
	OPENBSD_3_4:1.5.0.4
	OPENBSD_3_4_BASE:1.5
	UBC_SYNC_A:1.5
	OPENBSD_3_3:1.5.0.2
	OPENBSD_3_3_BASE:1.5
	OPENBSD_3_2:1.4.0.16
	OPENBSD_3_2_BASE:1.4
	OPENBSD_3_1:1.4.0.14
	OPENBSD_3_1_BASE:1.4
	UBC_SYNC_B:1.4
	UBC:1.4.0.12
	UBC_BASE:1.4
	OPENBSD_3_0:1.4.0.10
	OPENBSD_3_0_BASE:1.4
	OPENBSD_2_9_BASE:1.4
	OPENBSD_2_9:1.4.0.8
	OPENBSD_2_8:1.4.0.6
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.4
	OPENBSD_2_7_BASE:1.4
	SMP:1.4.0.2
	SMP_BASE:1.4
	kame_19991208:1.2
	OPENBSD_2_6:1.2.0.4
	OPENBSD_2_6_BASE:1.2
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.8
date	2012.04.06.15.53.58;	author jsing;	state dead;
branches;
next	1.7;

1.7
date	2007.11.25.16.40.04;	author jmc;	state Exp;
branches;
next	1.6;

1.6
date	2006.07.09.22.10.05;	author mk;	state Exp;
branches;
next	1.5;

1.5
date	2002.12.16.07.01.03;	author tdeval;	state Exp;
branches;
next	1.4;

1.4
date	2000.01.11.18.02.20;	author peter;	state Exp;
branches
	1.4.2.1
	1.4.12.1;
next	1.3;

1.3
date	2000.01.07.14.50.20;	author peter;	state Exp;
branches;
next	1.2;

1.2
date	99.02.16.00.02.29;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	99.01.11.14.29.07;	author niklas;	state Exp;
branches;
next	;

1.4.2.1
date	2003.03.28.00.38.27;	author niklas;	state Exp;
branches;
next	;

1.4.12.1
date	2003.05.19.22.21.51;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.8
log
@Put raidframe in the attic.
@
text
@/*	$OpenBSD: rf_dagdegwr.c,v 1.7 2007/11/25 16:40:04 jmc Exp $	*/
/*	$NetBSD: rf_dagdegwr.c,v 1.5 2000/01/07 03:40:57 oster Exp $	*/

/*
 * Copyright (c) 1995 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Author: Mark Holland, Daniel Stodolsky, William V. Courtright II
 *
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 *
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * rf_dagdegwr.c
 *
 * Code for creating degraded write DAGs.
 *
 */

#include "rf_types.h"
#include "rf_raid.h"
#include "rf_dag.h"
#include "rf_dagutils.h"
#include "rf_dagfuncs.h"
#include "rf_debugMem.h"
#include "rf_memchunk.h"
#include "rf_general.h"
#include "rf_dagdegwr.h"


/*****************************************************************************
 *
 * General comments on DAG creation:
 *
 * All DAGs in this file use roll-away error recovery. Each DAG has a single
 * commit node, usually called "Cmt". If an error occurs before the Cmt node
 * is reached, the execution engine will halt forward execution and work
 * backward through the graph, executing the undo functions. Assuming that
 * each node in the graph prior to the Cmt node are undoable and atomic - or -
 * does not make changes to permanent state, the graph will fail atomically.
 * If an error occurs after the Cmt node executes, the engine will roll-forward
 * through the graph, blindly executing nodes until it reaches the end.
 * If a graph reaches the end, it is assumed to have completed successfully.
 *
 * A graph has only 1 Cmt node.
 *
 *****************************************************************************/


/*****************************************************************************
 *
 * The following wrappers map the standard DAG creation interface to the
 * DAG creation routines. Additionally, these wrappers enable experimentation
 * with new DAG structures by providing an extra level of indirection, allowing
 * the DAG creation routines to be replaced at this single point.
 *
 *****************************************************************************/

RF_CREATE_DAG_FUNC_DECL(rf_CreateSimpleDegradedWriteDAG)
{
	rf_CommonCreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp,
	    flags, allocList, 1, rf_RecoveryXorFunc, RF_TRUE);
}

void
rf_CreateDegradedWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
{
	RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
	RF_PhysDiskAddr_t *failedPDA = asmap->failedPDAs[0];

	RF_ASSERT(asmap->numDataFailed == 1);
	dag_h->creator = "DegradedWriteDAG";

	/*
	 * If the access writes only a portion of the failed unit, and also
	 * writes some portion of at least one surviving unit, we create two
	 * DAGs, one for the failed component and one for the non-failed
	 * component, and do them sequentially. Note that the fact that we're
	 * accessing only a portion of the failed unit indicates that the
	 * access either starts or ends in the failed unit, and hence we need
	 * create only two dags. This is inefficient in that the same data or
	 * parity can get read and written twice using this structure. I need
	 * to fix this to do the access all at once.
	 */
	RF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 &&
	    failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));
	rf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp,
	    flags, allocList);
}



/*****************************************************************************
 *
 * DAG creation code begins here.
 *
 *****************************************************************************/


/*****************************************************************************
 *
 * CommonCreateSimpleDegradedWriteDAG -- creates a DAG to do a degraded-mode
 * write, which is as follows
 *
 *                                        / {Wnq} --\
 * hdr -> blockNode ->  Rod -> Xor -> Cmt -> Wnp ----> unblock -> term
 *                  \  {Rod} /            |  Wnd ---/
 *                                        \ {Wnd} -/
 *
 * Commit nodes: Xor, Wnd
 *
 * IMPORTANT:
 * This DAG generator does not work for double-degraded archs since it does not
 * generate Q.
 *
 * This dag is essentially identical to the large-write dag, except that the
 * write to the failed data unit is suppressed.
 *
 * IMPORTANT:  this dag does not work in the case where the access writes only
 * a portion of the failed unit, and also writes some portion of at least one
 * surviving SU. this case is handled in CreateDegradedWriteDAG above.
 *
 * The block & unblock nodes are leftovers from a previous version. They
 * do nothing, but I haven't deleted them because it would be a tremendous
 * effort to put them back in.
 *
 * This dag is used whenever one of the data units in a write has failed.
 * If it is the parity unit that failed, the nonredundant write dag (below)
 * is used.
 *
 *****************************************************************************/

void
rf_CommonCreateSimpleDegradedWriteDAG(RF_Raid_t *raidPtr,
    RF_AccessStripeMap_t *asmap, RF_DagHeader_t *dag_h, void *bp,
    RF_RaidAccessFlags_t flags, RF_AllocListElem_t *allocList, int nfaults,
    int (*redFunc) (RF_DagNode_t *), int allowBufferRecycle)
{
	int nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum,
	    rdnodesFaked;
	RF_DagNode_t *blockNode, *unblockNode, *wnpNode, *wnqNode, *termNode;
	RF_DagNode_t *nodes, *wndNodes, *rrdNodes, *xorNode, *commitNode;
	RF_SectorCount_t sectorsPerSU;
	RF_ReconUnitNum_t which_ru;
	char *xorTargetBuf = NULL;	/*
					 * The target buffer for the XOR
					 * operation.
					 */
	char *overlappingPDAs;		/* A temporary array of flags. */
	RF_AccessStripeMapHeader_t *new_asm_h[2];
	RF_PhysDiskAddr_t *pda, *parityPDA;
	RF_StripeNum_t parityStripeID;
	RF_PhysDiskAddr_t *failedPDA;
	RF_RaidLayout_t *layoutPtr;

	layoutPtr = &(raidPtr->Layout);
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,
	    asmap->raidAddress, &which_ru);
	sectorsPerSU = layoutPtr->sectorsPerStripeUnit;
	/*
	 * failedPDA points to the pda within the asm that targets
	 * the failed disk.
	 */
	failedPDA = asmap->failedPDAs[0];

	if (rf_dagDebug)
		printf("[Creating degraded-write DAG]\n");

	RF_ASSERT(asmap->numDataFailed == 1);
	dag_h->creator = "SimpleDegradedWriteDAG";

	/*
	 * Generate two ASMs identifying the surviving data
	 * we need in order to recover the lost data.
	 */
	/* overlappingPDAs array must be zero'd */
	RF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed,
	    sizeof(char), (char *));
	rf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h,
	    new_asm_h, &nXorBufs, NULL, overlappingPDAs, allocList);

	/* Create all the nodes at once. */
	nWndNodes = asmap->numStripeUnitsAccessed - 1;	/*
							 * No access is
							 * generated for the
							 * failed pda.
							 */

	nRrdNodes = ((new_asm_h[0]) ?
	    new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +
	    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed
			    : 0);
	/*
	 * XXX
	 *
	 * There's a bug with a complete stripe overwrite- that means 0 reads
	 * of old data, and the rest of the DAG generation code doesn't like
	 * that. A release is coming, and I don't wanna risk breaking a
	 * critical DAG generator, so here's what I'm gonna do- if there's
	 * no read nodes, I'm gonna fake there being a read node, and I'm
	 * gonna swap in a no-op node in its place (to make all the link-up
	 * code happy).
	 * This should be fixed at some point. --jimz
	 */
	if (nRrdNodes == 0) {
		nRrdNodes = 1;
		rdnodesFaked = 1;
	} else {
		rdnodesFaked = 0;
	}
	/* Lock, unlock, xor, Wnd, Rrd, W(nfaults). */
	nNodes = 5 + nfaults + nWndNodes + nRrdNodes;
	RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
	i = 0;
	blockNode = &nodes[i];
	i += 1;
	commitNode = &nodes[i];
	i += 1;
	unblockNode = &nodes[i];
	i += 1;
	termNode = &nodes[i];
	i += 1;
	xorNode = &nodes[i];
	i += 1;
	wnpNode = &nodes[i];
	i += 1;
	wndNodes = &nodes[i];
	i += nWndNodes;
	rrdNodes = &nodes[i];
	i += nRrdNodes;
	if (nfaults == 2) {
		wnqNode = &nodes[i];
		i += 1;
	} else {
		wnqNode = NULL;
	}
	RF_ASSERT(i == nNodes);

	/*
	 * This dag can not commit until all rrd and xor Nodes have
	 * completed.
	 */
	dag_h->numCommitNodes = 1;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	RF_ASSERT(nRrdNodes > 0);
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nRrdNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nWndNodes + nfaults, 1, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, nWndNodes + nfaults, 0, 0,
	    dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nRrdNodes, 2 * nXorBufs + 2, nfaults, dag_h, "Xrc",
	    allocList);

	/*
	 * Fill in the Rrd nodes. If any of the rrd buffers are the same size
	 * as the failed buffer, save a pointer to it so we can use it as the
	 * target of the XOR. The pdas in the rrd nodes have been range-
	 * restricted, so if a buffer is the same size as the failed buffer,
	 * it must also be at the same alignment within the SU.
	 */
	i = 0;
	if (new_asm_h[0]) {
		for (i = 0, pda = new_asm_h[0]->stripeMap->physInfo;
		    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;
		    i++, pda = pda->next) {
			rf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Rrd", allocList);
			RF_ASSERT(pda);
			rrdNodes[i].params[0].p = pda;
			rrdNodes[i].params[1].p = pda->bufPtr;
			rrdNodes[i].params[2].v = parityStripeID;
			rrdNodes[i].params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		}
	}
	/* i now equals the number of stripe units accessed in new_asm_h[0]. */
	if (new_asm_h[1]) {
		for (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;
		    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;
		    j++, pda = pda->next) {
			rf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Rrd", allocList);
			RF_ASSERT(pda);
			rrdNodes[i + j].params[0].p = pda;
			rrdNodes[i + j].params[1].p = pda->bufPtr;
			rrdNodes[i + j].params[2].v = parityStripeID;
			rrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
			if (allowBufferRecycle &&
			    (pda->numSector == failedPDA->numSector))
				xorTargetBuf = pda->bufPtr;
		}
	}
	if (rdnodesFaked) {
		/*
		 * This is where we'll init that fake noop read node.
		 * (XXX should the wakeup func be different ?)
		 */
		rf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 1, 0, 0, dag_h, "RrN",
		    allocList);
	}
	/*
	 * Make a PDA for the parity unit. The parity PDA should start at
	 * the same offset into the SU as the failed PDA.
	 */
	/*
	 * Danner comment: I don't think this copy is really necessary. We are
	 * in one of two cases here.
	 * (1) The entire failed unit is written. Then asmap->parityInfo will
	 *     describe the entire parity.
	 * (2) We are only writing a subset of the failed unit and nothing else.
	 *     Then the asmap->parityInfo describes the failed unit and the copy
	 *     can also be avoided.
	 */

	RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),
	    (RF_PhysDiskAddr_t *), allocList);
	parityPDA->row = asmap->parityInfo->row;
	parityPDA->col = asmap->parityInfo->col;
	parityPDA->startSector = ((asmap->parityInfo->startSector /
	    sectorsPerSU) * sectorsPerSU) + (failedPDA->startSector %
	    sectorsPerSU);
	parityPDA->numSector = failedPDA->numSector;

	if (!xorTargetBuf) {
		RF_CallocAndAdd(xorTargetBuf, 1, rf_RaidAddressToByte(raidPtr,
		    failedPDA->numSector), (char *), allocList);
	}
	/* Init the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
	wnpNode->params[0].p = parityPDA;
	wnpNode->params[1].p = xorTargetBuf;
	wnpNode->params[2].v = parityStripeID;
	wnpNode->params[3].v = RF_CREATE_PARAM3(
	    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);

	/* Fill in the Wnq Node. */
	if (nfaults == 2) {
		{
			RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
			parityPDA->row = asmap->qInfo->row;
			parityPDA->col = asmap->qInfo->col;
			parityPDA->startSector = ((asmap->qInfo->startSector /
			    sectorsPerSU) * sectorsPerSU) +
			    (failedPDA->startSector % sectorsPerSU);
			parityPDA->numSector = failedPDA->numSector;

			rf_InitNode(wnqNode, rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wnq", allocList);
			wnqNode->params[0].p = parityPDA;
			RF_CallocAndAdd(xorNode->results[1], 1,
			    rf_RaidAddressToByte(raidPtr, failedPDA->numSector),
			    (char *), allocList);
			wnqNode->params[1].p = xorNode->results[1];
			wnqNode->params[2].v = parityStripeID;
			wnqNode->params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		}
	}
	/* Fill in the Wnd nodes. */
	for (pda = asmap->physInfo, i = 0; i < nWndNodes;
	     i++, pda = pda->next) {
		if (pda == failedPDA) {
			i--;
			continue;
		}
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
		RF_ASSERT(pda);
		wndNodes[i].params[0].p = pda;
		wndNodes[i].params[1].p = pda->bufPtr;
		wndNodes[i].params[2].v = parityStripeID;
		wndNodes[i].params[3].v = RF_CREATE_PARAM3(
		    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	}

	/* Fill in the results of the xor node. */
	xorNode->results[0] = xorTargetBuf;

	/* Fill in the params of the xor node. */

	paramNum = 0;
	if (rdnodesFaked == 0) {
		for (i = 0; i < nRrdNodes; i++) {
			/* All the Rrd nodes need to be xored together. */
			xorNode->params[paramNum++] = rrdNodes[i].params[0];
			xorNode->params[paramNum++] = rrdNodes[i].params[1];
		}
	}
	for (i = 0; i < nWndNodes; i++) {
		/*
		 * Any Wnd nodes that overlap the failed access need to be
		 * xored in.
		 */
		if (overlappingPDAs[i]) {
			RF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
			bcopy((char *) wndNodes[i].params[0].p, (char *) pda,
			    sizeof(RF_PhysDiskAddr_t));
			rf_RangeRestrictPDA(raidPtr, failedPDA, pda,
			    RF_RESTRICT_DOBUFFER, 0);
			xorNode->params[paramNum++].p = pda;
			xorNode->params[paramNum++].p = pda->bufPtr;
		}
	}
	RF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));

	/*
	 * Install the failed PDA into the xor param list so that the
	 * new data gets xor'd in.
	 */
	xorNode->params[paramNum++].p = failedPDA;
	xorNode->params[paramNum++].p = failedPDA->bufPtr;

	/*
	 * The last 2 params to the recovery xor node are always the failed
	 * PDA and the raidPtr. Install the failedPDA even though we have just
	 * done so above. This allows us to use the same XOR function for both
	 * degraded reads and degraded writes.
	 */
	xorNode->params[paramNum++].p = failedPDA;
	xorNode->params[paramNum++].p = raidPtr;
	RF_ASSERT(paramNum == 2 * nXorBufs + 2);

	/*
	 * Code to link nodes begins here.
	 */

	/* Link header to block node. */
	RF_ASSERT(blockNode->numAntecedents == 0);
	dag_h->succedents[0] = blockNode;

	/* Link block node to rd nodes. */
	RF_ASSERT(blockNode->numSuccedents == nRrdNodes);
	for (i = 0; i < nRrdNodes; i++) {
		RF_ASSERT(rrdNodes[i].numAntecedents == 1);
		blockNode->succedents[i] = &rrdNodes[i];
		rrdNodes[i].antecedents[0] = blockNode;
		rrdNodes[i].antType[0] = rf_control;
	}

	/* Link read nodes to xor node. */
	RF_ASSERT(xorNode->numAntecedents == nRrdNodes);
	for (i = 0; i < nRrdNodes; i++) {
		RF_ASSERT(rrdNodes[i].numSuccedents == 1);
		rrdNodes[i].succedents[0] = xorNode;
		xorNode->antecedents[i] = &rrdNodes[i];
		xorNode->antType[i] = rf_trueData;
	}

	/* Link xor node to commit node. */
	RF_ASSERT(xorNode->numSuccedents == 1);
	RF_ASSERT(commitNode->numAntecedents == 1);
	xorNode->succedents[0] = commitNode;
	commitNode->antecedents[0] = xorNode;
	commitNode->antType[0] = rf_control;

	/* Link commit node to wnd nodes. */
	RF_ASSERT(commitNode->numSuccedents == nfaults + nWndNodes);
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNodes[i].numAntecedents == 1);
		commitNode->succedents[i] = &wndNodes[i];
		wndNodes[i].antecedents[0] = commitNode;
		wndNodes[i].antType[0] = rf_control;
	}

	/* Link the commit node to wnp, wnq nodes. */
	RF_ASSERT(wnpNode->numAntecedents == 1);
	commitNode->succedents[nWndNodes] = wnpNode;
	wnpNode->antecedents[0] = commitNode;
	wnpNode->antType[0] = rf_control;
	if (nfaults == 2) {
		RF_ASSERT(wnqNode->numAntecedents == 1);
		commitNode->succedents[nWndNodes + 1] = wnqNode;
		wnqNode->antecedents[0] = commitNode;
		wnqNode->antType[0] = rf_control;
	}
	/* Link write new data nodes to unblock node. */
	RF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nfaults));
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNodes[i].numSuccedents == 1);
		wndNodes[i].succedents[0] = unblockNode;
		unblockNode->antecedents[i] = &wndNodes[i];
		unblockNode->antType[i] = rf_control;
	}

	/* Link write new parity node to unblock node. */
	RF_ASSERT(wnpNode->numSuccedents == 1);
	wnpNode->succedents[0] = unblockNode;
	unblockNode->antecedents[nWndNodes] = wnpNode;
	unblockNode->antType[nWndNodes] = rf_control;

	/* Link write new q node to unblock node. */
	if (nfaults == 2) {
		RF_ASSERT(wnqNode->numSuccedents == 1);
		wnqNode->succedents[0] = unblockNode;
		unblockNode->antecedents[nWndNodes + 1] = wnqNode;
		unblockNode->antType[nWndNodes + 1] = rf_control;
	}
	/* Link unblock node to term node. */
	RF_ASSERT(unblockNode->numSuccedents == 1);
	RF_ASSERT(termNode->numAntecedents == 1);
	RF_ASSERT(termNode->numSuccedents == 0);
	unblockNode->succedents[0] = termNode;
	termNode->antecedents[0] = unblockNode;
	termNode->antType[0] = rf_control;
}

#define	CONS_PDA(if,start,num)	do {					\
	pda_p->row = asmap->if->row;					\
	pda_p->col = asmap->if->col;					\
	pda_p->startSector = ((asmap->if->startSector / secPerSU) *	\
	    secPerSU) + start;						\
	pda_p->numSector = num;						\
	pda_p->next = NULL;						\
	RF_MallocAndAdd(pda_p->bufPtr,					\
	    rf_RaidAddressToByte(raidPtr,num),(char *), allocList);	\
} while (0)

void
rf_WriteGenerateFailedAccessASMs(RF_Raid_t *raidPtr,
    RF_AccessStripeMap_t *asmap, RF_PhysDiskAddr_t **pdap, int *nNodep,
    RF_PhysDiskAddr_t **pqpdap, int *nPQNodep, RF_AllocListElem_t *allocList)
{
	RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
	int PDAPerDisk, i;
	RF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;
	int numDataCol = layoutPtr->numDataCol;
	int state;
	unsigned napdas;
	RF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end;
	RF_PhysDiskAddr_t *fone = asmap->failedPDAs[0];
	RF_PhysDiskAddr_t *ftwo = asmap->failedPDAs[1];
	RF_PhysDiskAddr_t *pda_p;
	RF_RaidAddr_t sosAddr;

	/*
	 * Determine how many pda's we will have to generate per unaccessed
	 * stripe. If there is only one failed data unit, it is one; if two,
	 * possibly two, depending whether they overlap.
	 */

	fone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);
	fone_end = fone_start + fone->numSector;

	if (asmap->numDataFailed == 1) {
		PDAPerDisk = 1;
		state = 1;
		RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t),
		    (RF_PhysDiskAddr_t *), allocList);
		pda_p = *pqpdap;
		/* Build p. */
		CONS_PDA(parityInfo, fone_start, fone->numSector);
		pda_p->type = RF_PDA_TYPE_PARITY;
		pda_p++;
		/* Build q. */
		CONS_PDA(qInfo, fone_start, fone->numSector);
		pda_p->type = RF_PDA_TYPE_Q;
	} else {
		ftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);
		ftwo_end = ftwo_start + ftwo->numSector;
		if (fone->numSector + ftwo->numSector > secPerSU) {
			PDAPerDisk = 1;
			state = 2;
			RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
			pda_p = *pqpdap;
			CONS_PDA(parityInfo, 0, secPerSU);
			pda_p->type = RF_PDA_TYPE_PARITY;
			pda_p++;
			CONS_PDA(qInfo, 0, secPerSU);
			pda_p->type = RF_PDA_TYPE_Q;
		} else {
			PDAPerDisk = 2;
			state = 3;
			/* Four of them, fone, then ftwo. */
			RF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
			pda_p = *pqpdap;
			CONS_PDA(parityInfo, fone_start, fone->numSector);
			pda_p->type = RF_PDA_TYPE_PARITY;
			pda_p++;
			CONS_PDA(qInfo, fone_start, fone->numSector);
			pda_p->type = RF_PDA_TYPE_Q;
			pda_p++;
			CONS_PDA(parityInfo, ftwo_start, ftwo->numSector);
			pda_p->type = RF_PDA_TYPE_PARITY;
			pda_p++;
			CONS_PDA(qInfo, ftwo_start, ftwo->numSector);
			pda_p->type = RF_PDA_TYPE_Q;
		}
	}
	/* Figure out number of nonaccessed pda. */
	napdas = PDAPerDisk * (numDataCol - 2);
	*nPQNodep = PDAPerDisk;

	*nNodep = napdas;
	if (napdas == 0)
		return;		/* Short circuit. */

	/* Allocate up our list of pda's. */

	RF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t),
	    (RF_PhysDiskAddr_t *), allocList);
	*pdap = pda_p;

	/* Link them together. */
	for (i = 0; i < (napdas - 1); i++)
		pda_p[i].next = pda_p + (i + 1);

	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
	    asmap->raidAddress);
	for (i = 0; i < numDataCol; i++) {
		if ((pda_p - (*pdap)) == napdas)
			continue;
		pda_p->type = RF_PDA_TYPE_DATA;
		pda_p->raidAddress = sosAddr + (i * secPerSU);
		(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress,
		    &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
		/* Skip over dead disks. */
		if (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))
			continue;
		switch (state) {
		case 1:	/* Fone. */
			pda_p->numSector = fone->numSector;
			pda_p->raidAddress += fone_start;
			pda_p->startSector += fone_start;
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
			break;
		case 2:	/* Full stripe. */
			pda_p->numSector = secPerSU;
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, secPerSU), (char *), allocList);
			break;
		case 3:	/* Two slabs. */
			pda_p->numSector = fone->numSector;
			pda_p->raidAddress += fone_start;
			pda_p->startSector += fone_start;
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
			pda_p++;
			pda_p->type = RF_PDA_TYPE_DATA;
			pda_p->raidAddress = sosAddr + (i * secPerSU);
			(raidPtr->Layout.map->MapSector) (raidPtr,
			    pda_p->raidAddress, &(pda_p->row), &(pda_p->col),
			    &(pda_p->startSector), 0);
			pda_p->numSector = ftwo->numSector;
			pda_p->raidAddress += ftwo_start;
			pda_p->startSector += ftwo_start;
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
			break;
		default:
			RF_PANIC();
		}
		pda_p++;
	}

	RF_ASSERT(pda_p - *pdap == napdas);
	return;
}

#define	DISK_NODE_PDA(node)	((node)->params[0].p)

#define	DISK_NODE_PARAMS(_node_,_p_)	do {				\
	(_node_).params[0].p = _p_ ;					\
	(_node_).params[1].p = (_p_)->bufPtr;				\
	(_node_).params[2].v = parityStripeID;				\
	(_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,	\
	    0, 0, which_ru);						\
} while (0)

void
rf_DoubleDegSmallWrite(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, char *redundantReadNodeName,
    char *redundantWriteNodeName, char *recoveryNodeName,
    int (*recovFunc) (RF_DagNode_t *))
{
	RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
	RF_DagNode_t *nodes, *wudNodes, *rrdNodes, *recoveryNode, *blockNode,
	    *unblockNode, *rpNodes, *rqNodes, *wpNodes, *wqNodes, *termNode;
	RF_PhysDiskAddr_t *pda, *pqPDAs;
	RF_PhysDiskAddr_t *npdas;
	int nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;
	RF_ReconUnitNum_t which_ru;
	int nPQNodes;
	RF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(
	    layoutPtr, asmap->raidAddress, &which_ru);

	/*
	 * Simple small write case - First part looks like a reconstruct-read
	 * of the failed data units. Then a write of all data units not
	 * failed.
	 */


	/*
	 * Hdr | ------Block- /  /         \   Rrd  Rrd ...  Rrd  Rp Rq \  \
	 * /  -------PQ----- /   \   \ Wud   Wp  WQ	     \    |   /
	 * --Unblock- | T
	 *
	 * Rrd = read recovery data (potentially none)
	 * Wud = write user data (not incl. failed disks)
	 * Wp = Write P (could be two)
	 * Wq = Write Q (could be two)
	 *
	 */

	rf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes,
	    &pqPDAs, &nPQNodes, allocList);

	RF_ASSERT(asmap->numDataFailed == 1);

	nWudNodes = asmap->numStripeUnitsAccessed - (asmap->numDataFailed);
	nReadNodes = nRrdNodes + 2 * nPQNodes;
	nWriteNodes = nWudNodes + 2 * nPQNodes;
	nNodes = 4 + nReadNodes + nWriteNodes;

	RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),
	    allocList);
	blockNode = nodes;
	unblockNode = blockNode + 1;
	termNode = unblockNode + 1;
	recoveryNode = termNode + 1;
	rrdNodes = recoveryNode + 1;
	rpNodes = rrdNodes + nRrdNodes;
	rqNodes = rpNodes + nPQNodes;
	wudNodes = rqNodes + nPQNodes;
	wpNodes = wudNodes + nWudNodes;
	wqNodes = wpNodes + nPQNodes;

	dag_h->creator = "PQ_DDSimpleSmallWrite";
	dag_h->numSuccedents = 1;
	dag_h->succedents[0] = blockNode;
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
	termNode->antecedents[0] = unblockNode;
	termNode->antType[0] = rf_control;

	/* Init the block and unblock nodes. */
	/* The block node has all the read nodes as successors. */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);
	for (i = 0; i < nReadNodes; i++)
		blockNode->succedents[i] = rrdNodes + i;

	/* The unblock node has all the writes as successors. */
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h,
	    "Nil", allocList);
	for (i = 0; i < nWriteNodes; i++) {
		unblockNode->antecedents[i] = wudNodes + i;
		unblockNode->antType[i] = rf_control;
	}
	unblockNode->succedents[0] = termNode;

#define	INIT_READ_NODE(node,name)	do {				\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc,		\
	    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = recoveryNode;				\
	(node)->antecedents[0] = blockNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)

	/* Build the read nodes. */
	pda = npdas;
	for (i = 0; i < nRrdNodes; i++, pda = pda->next) {
		INIT_READ_NODE(rrdNodes + i, "rrd");
		DISK_NODE_PARAMS(rrdNodes[i], pda);
	}

	/* Read redundancy pdas. */
	pda = pqPDAs;
	INIT_READ_NODE(rpNodes, "Rp");
	RF_ASSERT(pda);
	DISK_NODE_PARAMS(rpNodes[0], pda);
	pda++;
	INIT_READ_NODE(rqNodes, redundantReadNodeName);
	RF_ASSERT(pda);
	DISK_NODE_PARAMS(rqNodes[0], pda);
	if (nPQNodes == 2) {
		pda++;
		INIT_READ_NODE(rpNodes + 1, "Rp");
		RF_ASSERT(pda);
		DISK_NODE_PARAMS(rpNodes[1], pda);
		pda++;
		INIT_READ_NODE(rqNodes + 1, redundantReadNodeName);
		RF_ASSERT(pda);
		DISK_NODE_PARAMS(rqNodes[1], pda);
	}
	/*
	 * The recovery node has all reads as precedessors and all writes as
	 * successors. It generates a result for every write P or write Q
	 * node. As parameters, it takes a pda per read and a pda per stripe
	 * of user data written. It also takes as the last params the raidPtr
	 * and asm. For results, it takes PDA for P & Q.
	 */

	rf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc,
	    rf_NullNodeUndoFunc, NULL,
	    nWriteNodes,		/* successors */
	    nReadNodes,			/* preds */
	    nReadNodes + nWudNodes + 3,	/* params */
	    2 * nPQNodes,		/* results */
	    dag_h, recoveryNodeName, allocList);



	for (i = 0; i < nReadNodes; i++) {
		recoveryNode->antecedents[i] = rrdNodes + i;
		recoveryNode->antType[i] = rf_control;
		recoveryNode->params[i].p = DISK_NODE_PDA(rrdNodes + i);
	}
	for (i = 0; i < nWudNodes; i++) {
		recoveryNode->succedents[i] = wudNodes + i;
	}
	recoveryNode->params[nReadNodes + nWudNodes].p = asmap->failedPDAs[0];
	recoveryNode->params[nReadNodes + nWudNodes + 1].p = raidPtr;
	recoveryNode->params[nReadNodes + nWudNodes + 2].p = asmap;

	for (; i < nWriteNodes; i++)
		recoveryNode->succedents[i] = wudNodes + i;

	pda = pqPDAs;
	recoveryNode->results[0] = pda;
	pda++;
	recoveryNode->results[1] = pda;
	if (nPQNodes == 2) {
		pda++;
		recoveryNode->results[2] = pda;
		pda++;
		recoveryNode->results[3] = pda;
	}
	/* Fill writes. */
#define	INIT_WRITE_NODE(node,name)	do {				\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskWriteFunc,		\
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = unblockNode;				\
	(node)->antecedents[0] = recoveryNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)

	pda = asmap->physInfo;
	for (i = 0; i < nWudNodes; i++) {
		INIT_WRITE_NODE(wudNodes + i, "Wd");
		DISK_NODE_PARAMS(wudNodes[i], pda);
		recoveryNode->params[nReadNodes + i].p =
		    DISK_NODE_PDA(wudNodes + i);
		pda = pda->next;
	}
	/* Write redundancy pdas. */
	pda = pqPDAs;
	INIT_WRITE_NODE(wpNodes, "Wp");
	RF_ASSERT(pda);
	DISK_NODE_PARAMS(wpNodes[0], pda);
	pda++;
	INIT_WRITE_NODE(wqNodes, "Wq");
	RF_ASSERT(pda);
	DISK_NODE_PARAMS(wqNodes[0], pda);
	if (nPQNodes == 2) {
		pda++;
		INIT_WRITE_NODE(wpNodes + 1, "Wp");
		RF_ASSERT(pda);
		DISK_NODE_PARAMS(wpNodes[1], pda);
		pda++;
		INIT_WRITE_NODE(wqNodes + 1, "Wq");
		RF_ASSERT(pda);
		DISK_NODE_PARAMS(wqNodes[1], pda);
	}
}
@


1.7
log
@spelling fixes, from Martynas Venckus;
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_dagdegwr.c,v 1.6 2006/07/09 22:10:05 mk Exp $	*/
@


1.6
log
@I don't think we're talking about animals in here, so
s/wether/whether/g.
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_dagdegwr.c,v 1.5 2002/12/16 07:01:03 tdeval Exp $	*/
d846 1
a846 1
	    nWriteNodes,		/* succesors */
@


1.5
log
@Major KNF.  Incentive from Tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_dagdegwr.c,v 1.4 2000/01/11 18:02:20 peter Exp $	*/
d581 1
a581 1
	 * possibly two, depending wether they overlap.
@


1.4
log
@sync with NetBSD

- removed threadid stuff
- removed unused files
- general tidyup
- you can no longer configure the same unit twice (without
de-configuring first of course).

Again, this has only been tested locally on IDE disks. Further testing
and feedback would be appreciated.
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_dagdegwr.c,v 1.3 2000/01/07 14:50:20 peter Exp $	*/
d3 1
d34 1
a34 1
 * code for creating degraded write DAGs
d49 1
a49 1
/******************************************************************************
d53 2
a54 2
 * All DAGs in this file use roll-away error recovery.  Each DAG has a single
 * commit node, usually called "Cmt."  If an error occurs before the Cmt node
d56 1
a56 1
 * backward through the graph, executing the undo functions.  Assuming that
d65 1
a65 1
 */
d68 1
a68 1
/******************************************************************************
d71 1
a71 1
 * DAG creation routines.  Additionally, these wrappers enable experimentation
d74 2
a75 1
 */
a76 1
static 
d83 4
a86 8
void 
rf_CreateDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList)
	RF_Raid_t *raidPtr;
	RF_AccessStripeMap_t *asmap;
	RF_DagHeader_t *dag_h;
	void   *bp;
	RF_RaidAccessFlags_t flags;
	RF_AllocListElem_t *allocList;
d94 2
a95 1
	/* if the access writes only a portion of the failed unit, and also
d98 1
a98 1
	 * component, and do them sequentially.  Note that the fact that we're
d101 8
a108 5
	 * create only two dags.  This is inefficient in that the same data or
	 * parity can get read and written twice using this structure.  I need
	 * to fix this to do the access all at once. */
	RF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 && failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));
	rf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList);
d113 3
a115 1
/******************************************************************************
d117 1
a117 3
 * DAG creation code begins here
 */

d120 1
a120 1
/******************************************************************************
d127 1
a127 1
 *                  \  {Rod} /            \  Wnd ---/
d130 1
a130 1
 * commit nodes: Xor, Wnd
d134 1
a134 1
 * generate Q
d141 1
a141 1
 * surviving SU.  this case is handled in CreateDegradedWriteDAG above.
d143 1
a143 1
 * The block & unblock nodes are leftovers from a previous version.  They
d147 1
a147 1
 * This dag is used whenever a one of the data units in a write has failed.
d150 1
d153 5
a157 12
void 
rf_CommonCreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags,
    allocList, nfaults, redFunc, allowBufferRecycle)
	RF_Raid_t *raidPtr;
	RF_AccessStripeMap_t *asmap;
	RF_DagHeader_t *dag_h;
	void   *bp;
	RF_RaidAccessFlags_t flags;
	RF_AllocListElem_t *allocList;
	int     nfaults;
	int     (*redFunc) (RF_DagNode_t *);
	int     allowBufferRecycle;
d159 2
a160 2
	int     nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum,
	        rdnodesFaked;
d165 5
a169 3
	char   *xorTargetBuf = NULL;	/* the target buffer for the XOR
					 * operation */
	char   *overlappingPDAs;/* a temporary array of flags */
d177 2
a178 2
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,
	    &which_ru);
d180 4
a183 2
	/* failedPDA points to the pda within the asm that targets the failed
	 * disk */
d193 3
a195 3
         * Generate two ASMs identifying the surviving data
         * we need in order to recover the lost data.
         */
d197 8
a204 6
	RF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));
	rf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h,
	    &nXorBufs, NULL, overlappingPDAs, allocList);

	/* create all the nodes at once */
	nWndNodes = asmap->numStripeUnitsAccessed - 1;	/* no access is
d206 2
a207 1
							 * failed pda */
d209 4
a212 2
	nRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +
	    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);
d214 11
a224 10
         * XXX
         *
         * There's a bug with a complete stripe overwrite- that means 0 reads
         * of old data, and the rest of the DAG generation code doesn't like
         * that. A release is coming, and I don't wanna risk breaking a critical
         * DAG generator, so here's what I'm gonna do- if there's no read nodes,
         * I'm gonna fake there being a read node, and I'm gonna swap in a
         * no-op node in its place (to make all the link-up code happy).
         * This should be fixed at some point.  --jimz
         */
d231 1
a231 1
	/* lock, unlock, xor, Wnd, Rrd, W(nfaults) */
d260 4
a263 1
	/* this dag can not commit until all rrd and xor Nodes have completed */
d269 22
a290 18
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, nRrdNodes, 0, 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, nWndNodes + nfaults, 1, 0, 0, dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nWndNodes + nfaults, 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,
	    NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,
	    nRrdNodes, 2 * nXorBufs + 2, nfaults, dag_h, "Xrc", allocList);

	/*
         * Fill in the Rrd nodes. If any of the rrd buffers are the same size as
         * the failed buffer, save a pointer to it so we can use it as the target
         * of the XOR. The pdas in the rrd nodes have been range-restricted, so if
         * a buffer is the same size as the failed buffer, it must also be at the
         * same alignment within the SU.
         */
d296 4
a299 2
			rf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Rrd", allocList);
d304 2
a305 1
			rrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d308 1
a308 1
	/* i now equals the number of stripe units accessed in new_asm_h[0] */
d313 4
a316 2
			rf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Rrd", allocList);
d321 4
a324 2
			rrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
			if (allowBufferRecycle && (pda->numSector == failedPDA->numSector))
d330 20
a349 16
	         * This is where we'll init that fake noop read node
	         * (XXX should the wakeup func be different?)
	         */
		rf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
		    NULL, 1, 1, 0, 0, dag_h, "RrN", allocList);
	}
	/*
         * Make a PDA for the parity unit.  The parity PDA should start at
         * the same offset into the SU as the failed PDA.
         */
	/* Danner comment: I don't think this copy is really necessary. We are
	 * in one of two cases here. (1) The entire failed unit is written.
	 * Then asmap->parityInfo will describe the entire parity. (2) We are
	 * only writing a subset of the failed unit and nothing else. Then the
	 * asmap->parityInfo describes the failed unit and the copy can also
	 * be avoided. */
d351 2
a352 1
	RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
d355 3
a357 2
	parityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)
	    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);
d361 2
a362 2
		RF_CallocAndAdd(xorTargetBuf, 1,
		    rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);
d364 4
a367 3
	/* init the Wnp node */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
	    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnp", allocList);
d371 2
a372 1
	wnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d374 1
a374 1
	/* fill in the Wnq Node */
d381 3
a383 2
			parityPDA->startSector = ((asmap->qInfo->startSector / sectorsPerSU)
			    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);
d386 4
a389 2
			rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnq", allocList);
d392 2
a393 1
			    rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);
d396 2
a397 1
			wnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d400 3
a402 2
	/* fill in the Wnd nodes */
	for (pda = asmap->physInfo, i = 0; i < nWndNodes; i++, pda = pda->next) {
d407 3
a409 2
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
d414 2
a415 1
		wndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d418 1
a418 1
	/* fill in the results of the xor node */
d421 1
a421 1
	/* fill in the params of the xor node */
d426 1
a426 1
			/* all the Rrd nodes need to be xored together */
d432 4
a435 2
		/* any Wnd nodes that overlap the failed access need to be
		 * xored in */
d437 6
a442 3
			RF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
			bcopy((char *) wndNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));
			rf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);
d450 3
a452 3
         * Install the failed PDA into the xor param list so that the
         * new data gets xor'd in.
         */
d457 5
a461 5
         * The last 2 params to the recovery xor node are always the failed
         * PDA and the raidPtr. install the failedPDA even though we have just
         * done so above. This allows us to use the same XOR function for both
         * degraded reads and degraded writes.
         */
d467 2
a468 2
         * Code to link nodes begins here
         */
d470 1
a470 1
	/* link header to block node */
d474 1
a474 1
	/* link block node to rd nodes */
d483 1
a483 1
	/* link read nodes to xor node */
d492 1
a492 1
	/* link xor node to commit node */
d499 1
a499 1
	/* link commit node to wnd nodes */
d508 1
a508 1
	/* link the commit node to wnp, wnq nodes */
d519 1
a519 1
	/* link write new data nodes to unblock node */
d528 1
a528 1
	/* link write new parity node to unblock node */
d534 1
a534 1
	/* link write new q node to unblock node */
d541 1
a541 1
	/* link unblock node to term node */
d549 16
a564 16
#define CONS_PDA(if,start,num) \
  pda_p->row = asmap->if->row;    pda_p->col = asmap->if->col; \
  pda_p->startSector = ((asmap->if->startSector / secPerSU) * secPerSU) + start; \
  pda_p->numSector = num; \
  pda_p->next = NULL; \
  RF_MallocAndAdd(pda_p->bufPtr,rf_RaidAddressToByte(raidPtr,num),(char *), allocList)

void 
rf_WriteGenerateFailedAccessASMs(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_PhysDiskAddr_t ** pdap,
    int *nNodep,
    RF_PhysDiskAddr_t ** pqpdap,
    int *nPQNodep,
    RF_AllocListElem_t * allocList)
d567 1
a567 1
	int     PDAPerDisk, i;
d569 2
a570 2
	int     numDataCol = layoutPtr->numDataCol;
	int     state;
d573 2
a574 1
	RF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];
d578 2
a579 1
	/* determine how many pda's we will have to generate per unaccess
d581 2
a582 1
	 * possibly two, depending wether they overlap. */
d590 2
a591 1
		RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
d593 1
a593 1
		/* build p */
d597 1
a597 1
		/* build q */
d606 2
a607 1
			RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
d617 3
a619 2
			/* four of them, fone, then ftwo */
			RF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
d634 1
a634 1
	/* figure out number of nonaccessed pda */
d640 1
a640 1
		return;		/* short circuit */
d642 1
a642 1
	/* allocate up our list of pda's */
d644 2
a645 1
	RF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
d648 1
a648 1
	/* linkem together */
d652 2
a653 1
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);
d659 3
a661 2
		(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
		/* skip over dead disks */
d665 1
a665 1
		case 1:	/* fone */
d669 2
a670 1
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);
d672 1
a672 1
		case 2:	/* full stripe */
d674 2
a675 1
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);
d677 1
a677 1
		case 3:	/* two slabs */
d681 2
a682 1
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);
d686 3
a688 1
			(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
d692 2
a693 1
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);
a703 1
#define DISK_NODE_PDA(node)  ((node)->params[0].p)
d705 15
a719 17
#define DISK_NODE_PARAMS(_node_,_p_) \
  (_node_).params[0].p = _p_ ; \
  (_node_).params[1].p = (_p_)->bufPtr; \
  (_node_).params[2].v = parityStripeID; \
  (_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru)

void 
rf_DoubleDegSmallWrite(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    char *redundantReadNodeName,
    char *redundantWriteNodeName,
    char *recoveryNodeName,
d724 1
a724 1
	       *unblockNode, *rpNodes, *rqNodes, *wpNodes, *wqNodes, *termNode;
d727 1
a727 1
	int     nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;
d729 3
a731 2
	int     nPQNodes;
	RF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress, &which_ru);
d733 2
a734 1
	/* simple small write case - First part looks like a reconstruct-read
d736 2
a737 1
	 * failed. */
d740 2
a741 1
	/* Hdr | ------Block- /  /         \   Rrd  Rrd ...  Rrd  Rp Rq \  \
d744 6
a749 5
	 * 
	 * Rrd = read recovery data  (potentially none) Wud = write user data
	 * (not incl. failed disks) Wp = Write P (could be two) Wq = Write Q
	 * (could be two)
	 * 
d752 2
a753 1
	rf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes, &pqPDAs, &nPQNodes, allocList);
d762 2
a763 1
	RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d778 2
a779 1
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d783 5
a787 3
	/* init the block and unblock nodes */
	/* The block node has all the read nodes as successors */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h, "Nil", allocList);
d791 4
a794 2
	/* The unblock node has all the writes as successors */
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h, "Nil", allocList);
d801 8
a808 5
#define INIT_READ_NODE(node,name) \
  rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, name, allocList); \
  (node)->succedents[0] = recoveryNode; \
  (node)->antecedents[0] = blockNode; \
  (node)->antType[0] = rf_control;
d810 1
a810 1
	/* build the read nodes */
d817 1
a817 1
	/* read redundancy pdas */
d836 2
a837 1
	/* the recovery node has all reads as precedessors and all writes as
d841 2
a842 2
	 * and asm. For results, it takes PDA for P & Q. */

d844 4
a847 3
	rf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc, rf_NullNodeUndoFunc, NULL,
	    nWriteNodes,	/* succesors */
	    nReadNodes,		/* preds */
d849 1
a849 1
	    2 * nPQNodes,	/* results */
d879 9
a887 6
	/* fill writes */
#define INIT_WRITE_NODE(node,name) \
  rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, name, allocList); \
    (node)->succedents[0] = unblockNode; \
    (node)->antecedents[0] = recoveryNode; \
    (node)->antType[0] = rf_control;
d893 2
a894 1
		recoveryNode->params[nReadNodes + i].p = DISK_NODE_PDA(wudNodes + i);
d897 1
a897 1
	/* write redundancy pdas */
@


1.4.12.1
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 1

d33 1
a33 1
 * Code for creating degraded write DAGs.
d48 1
a48 1
/*****************************************************************************
d52 2
a53 2
 * All DAGs in this file use roll-away error recovery. Each DAG has a single
 * commit node, usually called "Cmt". If an error occurs before the Cmt node
d55 1
a55 1
 * backward through the graph, executing the undo functions. Assuming that
d64 1
a64 1
 *****************************************************************************/
d67 1
a67 1
/*****************************************************************************
d70 1
a70 1
 * DAG creation routines. Additionally, these wrappers enable experimentation
d73 1
a73 2
 *
 *****************************************************************************/
d75 1
d82 8
a89 4
void
rf_CreateDegradedWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d97 1
a97 2
	/*
	 * If the access writes only a portion of the failed unit, and also
d100 1
a100 1
	 * component, and do them sequentially. Note that the fact that we're
d103 5
a107 8
	 * create only two dags. This is inefficient in that the same data or
	 * parity can get read and written twice using this structure. I need
	 * to fix this to do the access all at once.
	 */
	RF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 &&
	    failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));
	rf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp,
	    flags, allocList);
d112 1
a112 1
/*****************************************************************************
d114 3
a116 3
 * DAG creation code begins here.
 *
 *****************************************************************************/
d119 1
a119 1
/*****************************************************************************
d126 1
a126 1
 *                  \  {Rod} /            |  Wnd ---/
d129 1
a129 1
 * Commit nodes: Xor, Wnd
d133 1
a133 1
 * generate Q.
d140 1
a140 1
 * surviving SU. this case is handled in CreateDegradedWriteDAG above.
d142 1
a142 1
 * The block & unblock nodes are leftovers from a previous version. They
d146 1
a146 1
 * This dag is used whenever one of the data units in a write has failed.
a148 1
 *
d151 12
a162 5
void
rf_CommonCreateSimpleDegradedWriteDAG(RF_Raid_t *raidPtr,
    RF_AccessStripeMap_t *asmap, RF_DagHeader_t *dag_h, void *bp,
    RF_RaidAccessFlags_t flags, RF_AllocListElem_t *allocList, int nfaults,
    int (*redFunc) (RF_DagNode_t *), int allowBufferRecycle)
d164 2
a165 2
	int nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum,
	    rdnodesFaked;
d170 3
a172 5
	char *xorTargetBuf = NULL;	/*
					 * The target buffer for the XOR
					 * operation.
					 */
	char *overlappingPDAs;		/* A temporary array of flags. */
d180 2
a181 2
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,
	    asmap->raidAddress, &which_ru);
d183 2
a184 4
	/*
	 * failedPDA points to the pda within the asm that targets
	 * the failed disk.
	 */
d194 3
a196 3
	 * Generate two ASMs identifying the surviving data
	 * we need in order to recover the lost data.
	 */
d198 6
a203 8
	RF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed,
	    sizeof(char), (char *));
	rf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h,
	    new_asm_h, &nXorBufs, NULL, overlappingPDAs, allocList);

	/* Create all the nodes at once. */
	nWndNodes = asmap->numStripeUnitsAccessed - 1;	/*
							 * No access is
d205 1
a205 2
							 * failed pda.
							 */
d207 2
a208 4
	nRrdNodes = ((new_asm_h[0]) ?
	    new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +
	    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed
			    : 0);
d210 10
a219 11
	 * XXX
	 *
	 * There's a bug with a complete stripe overwrite- that means 0 reads
	 * of old data, and the rest of the DAG generation code doesn't like
	 * that. A release is coming, and I don't wanna risk breaking a
	 * critical DAG generator, so here's what I'm gonna do- if there's
	 * no read nodes, I'm gonna fake there being a read node, and I'm
	 * gonna swap in a no-op node in its place (to make all the link-up
	 * code happy).
	 * This should be fixed at some point. --jimz
	 */
d226 1
a226 1
	/* Lock, unlock, xor, Wnd, Rrd, W(nfaults). */
d255 1
a255 4
	/*
	 * This dag can not commit until all rrd and xor Nodes have
	 * completed.
	 */
d261 18
a278 22
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nRrdNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nWndNodes + nfaults, 1, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, nWndNodes + nfaults, 0, 0,
	    dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nRrdNodes, 2 * nXorBufs + 2, nfaults, dag_h, "Xrc",
	    allocList);

	/*
	 * Fill in the Rrd nodes. If any of the rrd buffers are the same size
	 * as the failed buffer, save a pointer to it so we can use it as the
	 * target of the XOR. The pdas in the rrd nodes have been range-
	 * restricted, so if a buffer is the same size as the failed buffer,
	 * it must also be at the same alignment within the SU.
	 */
d284 2
a285 4
			rf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Rrd", allocList);
d290 1
a290 2
			rrdNodes[i].params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d293 1
a293 1
	/* i now equals the number of stripe units accessed in new_asm_h[0]. */
d298 2
a299 4
			rf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Rrd", allocList);
d304 2
a305 4
			rrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
			if (allowBufferRecycle &&
			    (pda->numSector == failedPDA->numSector))
d311 16
a326 20
		 * This is where we'll init that fake noop read node.
		 * (XXX should the wakeup func be different ?)
		 */
		rf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 1, 0, 0, dag_h, "RrN",
		    allocList);
	}
	/*
	 * Make a PDA for the parity unit. The parity PDA should start at
	 * the same offset into the SU as the failed PDA.
	 */
	/*
	 * Danner comment: I don't think this copy is really necessary. We are
	 * in one of two cases here.
	 * (1) The entire failed unit is written. Then asmap->parityInfo will
	 *     describe the entire parity.
	 * (2) We are only writing a subset of the failed unit and nothing else.
	 *     Then the asmap->parityInfo describes the failed unit and the copy
	 *     can also be avoided.
	 */
d328 1
a328 2
	RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),
	    (RF_PhysDiskAddr_t *), allocList);
d331 2
a332 3
	parityPDA->startSector = ((asmap->parityInfo->startSector /
	    sectorsPerSU) * sectorsPerSU) + (failedPDA->startSector %
	    sectorsPerSU);
d336 2
a337 2
		RF_CallocAndAdd(xorTargetBuf, 1, rf_RaidAddressToByte(raidPtr,
		    failedPDA->numSector), (char *), allocList);
d339 3
a341 4
	/* Init the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
d345 1
a345 2
	wnpNode->params[3].v = RF_CREATE_PARAM3(
	    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d347 1
a347 1
	/* Fill in the Wnq Node. */
d354 2
a355 3
			parityPDA->startSector = ((asmap->qInfo->startSector /
			    sectorsPerSU) * sectorsPerSU) +
			    (failedPDA->startSector % sectorsPerSU);
d358 2
a359 4
			rf_InitNode(wnqNode, rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wnq", allocList);
d362 1
a362 2
			    rf_RaidAddressToByte(raidPtr, failedPDA->numSector),
			    (char *), allocList);
d365 1
a365 2
			wnqNode->params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d368 2
a369 3
	/* Fill in the Wnd nodes. */
	for (pda = asmap->physInfo, i = 0; i < nWndNodes;
	     i++, pda = pda->next) {
d374 2
a375 3
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
d380 1
a380 2
		wndNodes[i].params[3].v = RF_CREATE_PARAM3(
		    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d383 1
a383 1
	/* Fill in the results of the xor node. */
d386 1
a386 1
	/* Fill in the params of the xor node. */
d391 1
a391 1
			/* All the Rrd nodes need to be xored together. */
d397 2
a398 4
		/*
		 * Any Wnd nodes that overlap the failed access need to be
		 * xored in.
		 */
d400 3
a402 6
			RF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
			bcopy((char *) wndNodes[i].params[0].p, (char *) pda,
			    sizeof(RF_PhysDiskAddr_t));
			rf_RangeRestrictPDA(raidPtr, failedPDA, pda,
			    RF_RESTRICT_DOBUFFER, 0);
d410 3
a412 3
	 * Install the failed PDA into the xor param list so that the
	 * new data gets xor'd in.
	 */
d417 5
a421 5
	 * The last 2 params to the recovery xor node are always the failed
	 * PDA and the raidPtr. Install the failedPDA even though we have just
	 * done so above. This allows us to use the same XOR function for both
	 * degraded reads and degraded writes.
	 */
d427 2
a428 2
	 * Code to link nodes begins here.
	 */
d430 1
a430 1
	/* Link header to block node. */
d434 1
a434 1
	/* Link block node to rd nodes. */
d443 1
a443 1
	/* Link read nodes to xor node. */
d452 1
a452 1
	/* Link xor node to commit node. */
d459 1
a459 1
	/* Link commit node to wnd nodes. */
d468 1
a468 1
	/* Link the commit node to wnp, wnq nodes. */
d479 1
a479 1
	/* Link write new data nodes to unblock node. */
d488 1
a488 1
	/* Link write new parity node to unblock node. */
d494 1
a494 1
	/* Link write new q node to unblock node. */
d501 1
a501 1
	/* Link unblock node to term node. */
d509 16
a524 16

#define	CONS_PDA(if,start,num)	do {					\
	pda_p->row = asmap->if->row;					\
	pda_p->col = asmap->if->col;					\
	pda_p->startSector = ((asmap->if->startSector / secPerSU) *	\
	    secPerSU) + start;						\
	pda_p->numSector = num;						\
	pda_p->next = NULL;						\
	RF_MallocAndAdd(pda_p->bufPtr,					\
	    rf_RaidAddressToByte(raidPtr,num),(char *), allocList);	\
} while (0)

void
rf_WriteGenerateFailedAccessASMs(RF_Raid_t *raidPtr,
    RF_AccessStripeMap_t *asmap, RF_PhysDiskAddr_t **pdap, int *nNodep,
    RF_PhysDiskAddr_t **pqpdap, int *nPQNodep, RF_AllocListElem_t *allocList)
d527 1
a527 1
	int PDAPerDisk, i;
d529 2
a530 2
	int numDataCol = layoutPtr->numDataCol;
	int state;
d533 1
a533 2
	RF_PhysDiskAddr_t *fone = asmap->failedPDAs[0];
	RF_PhysDiskAddr_t *ftwo = asmap->failedPDAs[1];
d537 1
a537 2
	/*
	 * Determine how many pda's we will have to generate per unaccessed
d539 1
a539 2
	 * possibly two, depending wether they overlap.
	 */
d547 1
a547 2
		RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t),
		    (RF_PhysDiskAddr_t *), allocList);
d549 1
a549 1
		/* Build p. */
d553 1
a553 1
		/* Build q. */
d562 1
a562 2
			RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
d572 2
a573 3
			/* Four of them, fone, then ftwo. */
			RF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
d588 1
a588 1
	/* Figure out number of nonaccessed pda. */
d594 1
a594 1
		return;		/* Short circuit. */
d596 1
a596 1
	/* Allocate up our list of pda's. */
d598 1
a598 2
	RF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t),
	    (RF_PhysDiskAddr_t *), allocList);
d601 1
a601 1
	/* Link them together. */
d605 1
a605 2
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
	    asmap->raidAddress);
d611 2
a612 3
		(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress,
		    &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
		/* Skip over dead disks. */
d616 1
a616 1
		case 1:	/* Fone. */
d620 1
a620 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
d622 1
a622 1
		case 2:	/* Full stripe. */
d624 1
a624 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, secPerSU), (char *), allocList);
d626 1
a626 1
		case 3:	/* Two slabs. */
d630 1
a630 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
d634 1
a634 3
			(raidPtr->Layout.map->MapSector) (raidPtr,
			    pda_p->raidAddress, &(pda_p->row), &(pda_p->col),
			    &(pda_p->startSector), 0);
d638 1
a638 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
d649 1
d651 17
a667 15
#define	DISK_NODE_PDA(node)	((node)->params[0].p)

#define	DISK_NODE_PARAMS(_node_,_p_)	do {				\
	(_node_).params[0].p = _p_ ;					\
	(_node_).params[1].p = (_p_)->bufPtr;				\
	(_node_).params[2].v = parityStripeID;				\
	(_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,	\
	    0, 0, which_ru);						\
} while (0)

void
rf_DoubleDegSmallWrite(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, char *redundantReadNodeName,
    char *redundantWriteNodeName, char *recoveryNodeName,
d672 1
a672 1
	    *unblockNode, *rpNodes, *rqNodes, *wpNodes, *wqNodes, *termNode;
d675 1
a675 1
	int nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;
d677 2
a678 3
	int nPQNodes;
	RF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(
	    layoutPtr, asmap->raidAddress, &which_ru);
d680 1
a680 2
	/*
	 * Simple small write case - First part looks like a reconstruct-read
d682 1
a682 2
	 * failed.
	 */
d685 1
a685 2
	/*
	 * Hdr | ------Block- /  /         \   Rrd  Rrd ...  Rrd  Rp Rq \  \
d688 5
a692 6
	 *
	 * Rrd = read recovery data (potentially none)
	 * Wud = write user data (not incl. failed disks)
	 * Wp = Write P (could be two)
	 * Wq = Write Q (could be two)
	 *
d695 1
a695 2
	rf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes,
	    &pqPDAs, &nPQNodes, allocList);
d704 1
a704 2
	RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),
	    allocList);
d719 1
a719 2
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d723 3
a725 5
	/* Init the block and unblock nodes. */
	/* The block node has all the read nodes as successors. */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);
d729 2
a730 4
	/* The unblock node has all the writes as successors. */
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h,
	    "Nil", allocList);
d737 5
a741 8
#define	INIT_READ_NODE(node,name)	do {				\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc,		\
	    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = recoveryNode;				\
	(node)->antecedents[0] = blockNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)
d743 1
a743 1
	/* Build the read nodes. */
d750 1
a750 1
	/* Read redundancy pdas. */
d769 1
a769 2
	/*
	 * The recovery node has all reads as precedessors and all writes as
d773 2
a774 2
	 * and asm. For results, it takes PDA for P & Q.
	 */
d776 3
a778 4
	rf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc,
	    rf_NullNodeUndoFunc, NULL,
	    nWriteNodes,		/* succesors */
	    nReadNodes,			/* preds */
d780 1
a780 1
	    2 * nPQNodes,		/* results */
d810 6
a815 9
	/* Fill writes. */
#define	INIT_WRITE_NODE(node,name)	do {				\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskWriteFunc,		\
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = unblockNode;				\
	(node)->antecedents[0] = recoveryNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)
d821 1
a821 2
		recoveryNode->params[nReadNodes + i].p =
		    DISK_NODE_PDA(wudNodes + i);
d824 1
a824 1
	/* Write redundancy pdas. */
@


1.4.2.1
log
@Sync the SMP branch with 3.3
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 1

d33 1
a33 1
 * Code for creating degraded write DAGs.
d48 1
a48 1
/*****************************************************************************
d52 2
a53 2
 * All DAGs in this file use roll-away error recovery. Each DAG has a single
 * commit node, usually called "Cmt". If an error occurs before the Cmt node
d55 1
a55 1
 * backward through the graph, executing the undo functions. Assuming that
d64 1
a64 1
 *****************************************************************************/
d67 1
a67 1
/*****************************************************************************
d70 1
a70 1
 * DAG creation routines. Additionally, these wrappers enable experimentation
d73 1
a73 2
 *
 *****************************************************************************/
d75 1
d82 8
a89 4
void
rf_CreateDegradedWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d97 1
a97 2
	/*
	 * If the access writes only a portion of the failed unit, and also
d100 1
a100 1
	 * component, and do them sequentially. Note that the fact that we're
d103 5
a107 8
	 * create only two dags. This is inefficient in that the same data or
	 * parity can get read and written twice using this structure. I need
	 * to fix this to do the access all at once.
	 */
	RF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 &&
	    failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));
	rf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp,
	    flags, allocList);
d112 1
a112 1
/*****************************************************************************
d114 3
a116 3
 * DAG creation code begins here.
 *
 *****************************************************************************/
d119 1
a119 1
/*****************************************************************************
d126 1
a126 1
 *                  \  {Rod} /            |  Wnd ---/
d129 1
a129 1
 * Commit nodes: Xor, Wnd
d133 1
a133 1
 * generate Q.
d140 1
a140 1
 * surviving SU. this case is handled in CreateDegradedWriteDAG above.
d142 1
a142 1
 * The block & unblock nodes are leftovers from a previous version. They
d146 1
a146 1
 * This dag is used whenever one of the data units in a write has failed.
a148 1
 *
d151 12
a162 5
void
rf_CommonCreateSimpleDegradedWriteDAG(RF_Raid_t *raidPtr,
    RF_AccessStripeMap_t *asmap, RF_DagHeader_t *dag_h, void *bp,
    RF_RaidAccessFlags_t flags, RF_AllocListElem_t *allocList, int nfaults,
    int (*redFunc) (RF_DagNode_t *), int allowBufferRecycle)
d164 2
a165 2
	int nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum,
	    rdnodesFaked;
d170 3
a172 5
	char *xorTargetBuf = NULL;	/*
					 * The target buffer for the XOR
					 * operation.
					 */
	char *overlappingPDAs;		/* A temporary array of flags. */
d180 2
a181 2
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,
	    asmap->raidAddress, &which_ru);
d183 2
a184 4
	/*
	 * failedPDA points to the pda within the asm that targets
	 * the failed disk.
	 */
d194 3
a196 3
	 * Generate two ASMs identifying the surviving data
	 * we need in order to recover the lost data.
	 */
d198 6
a203 8
	RF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed,
	    sizeof(char), (char *));
	rf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h,
	    new_asm_h, &nXorBufs, NULL, overlappingPDAs, allocList);

	/* Create all the nodes at once. */
	nWndNodes = asmap->numStripeUnitsAccessed - 1;	/*
							 * No access is
d205 1
a205 2
							 * failed pda.
							 */
d207 2
a208 4
	nRrdNodes = ((new_asm_h[0]) ?
	    new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +
	    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed
			    : 0);
d210 10
a219 11
	 * XXX
	 *
	 * There's a bug with a complete stripe overwrite- that means 0 reads
	 * of old data, and the rest of the DAG generation code doesn't like
	 * that. A release is coming, and I don't wanna risk breaking a
	 * critical DAG generator, so here's what I'm gonna do- if there's
	 * no read nodes, I'm gonna fake there being a read node, and I'm
	 * gonna swap in a no-op node in its place (to make all the link-up
	 * code happy).
	 * This should be fixed at some point. --jimz
	 */
d226 1
a226 1
	/* Lock, unlock, xor, Wnd, Rrd, W(nfaults). */
d255 1
a255 4
	/*
	 * This dag can not commit until all rrd and xor Nodes have
	 * completed.
	 */
d261 18
a278 22
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nRrdNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nWndNodes + nfaults, 1, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, nWndNodes + nfaults, 0, 0,
	    dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nRrdNodes, 2 * nXorBufs + 2, nfaults, dag_h, "Xrc",
	    allocList);

	/*
	 * Fill in the Rrd nodes. If any of the rrd buffers are the same size
	 * as the failed buffer, save a pointer to it so we can use it as the
	 * target of the XOR. The pdas in the rrd nodes have been range-
	 * restricted, so if a buffer is the same size as the failed buffer,
	 * it must also be at the same alignment within the SU.
	 */
d284 2
a285 4
			rf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Rrd", allocList);
d290 1
a290 2
			rrdNodes[i].params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d293 1
a293 1
	/* i now equals the number of stripe units accessed in new_asm_h[0]. */
d298 2
a299 4
			rf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Rrd", allocList);
d304 2
a305 4
			rrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
			if (allowBufferRecycle &&
			    (pda->numSector == failedPDA->numSector))
d311 16
a326 20
		 * This is where we'll init that fake noop read node.
		 * (XXX should the wakeup func be different ?)
		 */
		rf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 1, 0, 0, dag_h, "RrN",
		    allocList);
	}
	/*
	 * Make a PDA for the parity unit. The parity PDA should start at
	 * the same offset into the SU as the failed PDA.
	 */
	/*
	 * Danner comment: I don't think this copy is really necessary. We are
	 * in one of two cases here.
	 * (1) The entire failed unit is written. Then asmap->parityInfo will
	 *     describe the entire parity.
	 * (2) We are only writing a subset of the failed unit and nothing else.
	 *     Then the asmap->parityInfo describes the failed unit and the copy
	 *     can also be avoided.
	 */
d328 1
a328 2
	RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),
	    (RF_PhysDiskAddr_t *), allocList);
d331 2
a332 3
	parityPDA->startSector = ((asmap->parityInfo->startSector /
	    sectorsPerSU) * sectorsPerSU) + (failedPDA->startSector %
	    sectorsPerSU);
d336 2
a337 2
		RF_CallocAndAdd(xorTargetBuf, 1, rf_RaidAddressToByte(raidPtr,
		    failedPDA->numSector), (char *), allocList);
d339 3
a341 4
	/* Init the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
d345 1
a345 2
	wnpNode->params[3].v = RF_CREATE_PARAM3(
	    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d347 1
a347 1
	/* Fill in the Wnq Node. */
d354 2
a355 3
			parityPDA->startSector = ((asmap->qInfo->startSector /
			    sectorsPerSU) * sectorsPerSU) +
			    (failedPDA->startSector % sectorsPerSU);
d358 2
a359 4
			rf_InitNode(wnqNode, rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wnq", allocList);
d362 1
a362 2
			    rf_RaidAddressToByte(raidPtr, failedPDA->numSector),
			    (char *), allocList);
d365 1
a365 2
			wnqNode->params[3].v = RF_CREATE_PARAM3(
			    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d368 2
a369 3
	/* Fill in the Wnd nodes. */
	for (pda = asmap->physInfo, i = 0; i < nWndNodes;
	     i++, pda = pda->next) {
d374 2
a375 3
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
d380 1
a380 2
		wndNodes[i].params[3].v = RF_CREATE_PARAM3(
		    RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d383 1
a383 1
	/* Fill in the results of the xor node. */
d386 1
a386 1
	/* Fill in the params of the xor node. */
d391 1
a391 1
			/* All the Rrd nodes need to be xored together. */
d397 2
a398 4
		/*
		 * Any Wnd nodes that overlap the failed access need to be
		 * xored in.
		 */
d400 3
a402 6
			RF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
			bcopy((char *) wndNodes[i].params[0].p, (char *) pda,
			    sizeof(RF_PhysDiskAddr_t));
			rf_RangeRestrictPDA(raidPtr, failedPDA, pda,
			    RF_RESTRICT_DOBUFFER, 0);
d410 3
a412 3
	 * Install the failed PDA into the xor param list so that the
	 * new data gets xor'd in.
	 */
d417 5
a421 5
	 * The last 2 params to the recovery xor node are always the failed
	 * PDA and the raidPtr. Install the failedPDA even though we have just
	 * done so above. This allows us to use the same XOR function for both
	 * degraded reads and degraded writes.
	 */
d427 2
a428 2
	 * Code to link nodes begins here.
	 */
d430 1
a430 1
	/* Link header to block node. */
d434 1
a434 1
	/* Link block node to rd nodes. */
d443 1
a443 1
	/* Link read nodes to xor node. */
d452 1
a452 1
	/* Link xor node to commit node. */
d459 1
a459 1
	/* Link commit node to wnd nodes. */
d468 1
a468 1
	/* Link the commit node to wnp, wnq nodes. */
d479 1
a479 1
	/* Link write new data nodes to unblock node. */
d488 1
a488 1
	/* Link write new parity node to unblock node. */
d494 1
a494 1
	/* Link write new q node to unblock node. */
d501 1
a501 1
	/* Link unblock node to term node. */
d509 16
a524 16

#define	CONS_PDA(if,start,num)	do {					\
	pda_p->row = asmap->if->row;					\
	pda_p->col = asmap->if->col;					\
	pda_p->startSector = ((asmap->if->startSector / secPerSU) *	\
	    secPerSU) + start;						\
	pda_p->numSector = num;						\
	pda_p->next = NULL;						\
	RF_MallocAndAdd(pda_p->bufPtr,					\
	    rf_RaidAddressToByte(raidPtr,num),(char *), allocList);	\
} while (0)

void
rf_WriteGenerateFailedAccessASMs(RF_Raid_t *raidPtr,
    RF_AccessStripeMap_t *asmap, RF_PhysDiskAddr_t **pdap, int *nNodep,
    RF_PhysDiskAddr_t **pqpdap, int *nPQNodep, RF_AllocListElem_t *allocList)
d527 1
a527 1
	int PDAPerDisk, i;
d529 2
a530 2
	int numDataCol = layoutPtr->numDataCol;
	int state;
d533 1
a533 2
	RF_PhysDiskAddr_t *fone = asmap->failedPDAs[0];
	RF_PhysDiskAddr_t *ftwo = asmap->failedPDAs[1];
d537 1
a537 2
	/*
	 * Determine how many pda's we will have to generate per unaccessed
d539 1
a539 2
	 * possibly two, depending wether they overlap.
	 */
d547 1
a547 2
		RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t),
		    (RF_PhysDiskAddr_t *), allocList);
d549 1
a549 1
		/* Build p. */
d553 1
a553 1
		/* Build q. */
d562 1
a562 2
			RF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
d572 2
a573 3
			/* Four of them, fone, then ftwo. */
			RF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t),
			    (RF_PhysDiskAddr_t *), allocList);
d588 1
a588 1
	/* Figure out number of nonaccessed pda. */
d594 1
a594 1
		return;		/* Short circuit. */
d596 1
a596 1
	/* Allocate up our list of pda's. */
d598 1
a598 2
	RF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t),
	    (RF_PhysDiskAddr_t *), allocList);
d601 1
a601 1
	/* Link them together. */
d605 1
a605 2
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
	    asmap->raidAddress);
d611 2
a612 3
		(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress,
		    &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
		/* Skip over dead disks. */
d616 1
a616 1
		case 1:	/* Fone. */
d620 1
a620 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
d622 1
a622 1
		case 2:	/* Full stripe. */
d624 1
a624 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, secPerSU), (char *), allocList);
d626 1
a626 1
		case 3:	/* Two slabs. */
d630 1
a630 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
d634 1
a634 3
			(raidPtr->Layout.map->MapSector) (raidPtr,
			    pda_p->raidAddress, &(pda_p->row), &(pda_p->col),
			    &(pda_p->startSector), 0);
d638 1
a638 2
			RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(
			    raidPtr, pda_p->numSector), (char *), allocList);
d649 1
d651 17
a667 15
#define	DISK_NODE_PDA(node)	((node)->params[0].p)

#define	DISK_NODE_PARAMS(_node_,_p_)	do {				\
	(_node_).params[0].p = _p_ ;					\
	(_node_).params[1].p = (_p_)->bufPtr;				\
	(_node_).params[2].v = parityStripeID;				\
	(_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,	\
	    0, 0, which_ru);						\
} while (0)

void
rf_DoubleDegSmallWrite(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, char *redundantReadNodeName,
    char *redundantWriteNodeName, char *recoveryNodeName,
d672 1
a672 1
	    *unblockNode, *rpNodes, *rqNodes, *wpNodes, *wqNodes, *termNode;
d675 1
a675 1
	int nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;
d677 2
a678 3
	int nPQNodes;
	RF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(
	    layoutPtr, asmap->raidAddress, &which_ru);
d680 1
a680 2
	/*
	 * Simple small write case - First part looks like a reconstruct-read
d682 1
a682 2
	 * failed.
	 */
d685 1
a685 2
	/*
	 * Hdr | ------Block- /  /         \   Rrd  Rrd ...  Rrd  Rp Rq \  \
d688 5
a692 6
	 *
	 * Rrd = read recovery data (potentially none)
	 * Wud = write user data (not incl. failed disks)
	 * Wp = Write P (could be two)
	 * Wq = Write Q (could be two)
	 *
d695 1
a695 2
	rf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes,
	    &pqPDAs, &nPQNodes, allocList);
d704 1
a704 2
	RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),
	    allocList);
d719 1
a719 2
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d723 3
a725 5
	/* Init the block and unblock nodes. */
	/* The block node has all the read nodes as successors. */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);
d729 2
a730 4
	/* The unblock node has all the writes as successors. */
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h,
	    "Nil", allocList);
d737 5
a741 8
#define	INIT_READ_NODE(node,name)	do {				\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc,		\
	    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = recoveryNode;				\
	(node)->antecedents[0] = blockNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)
d743 1
a743 1
	/* Build the read nodes. */
d750 1
a750 1
	/* Read redundancy pdas. */
d769 1
a769 2
	/*
	 * The recovery node has all reads as precedessors and all writes as
d773 2
a774 2
	 * and asm. For results, it takes PDA for P & Q.
	 */
d776 3
a778 4
	rf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc,
	    rf_NullNodeUndoFunc, NULL,
	    nWriteNodes,		/* succesors */
	    nReadNodes,			/* preds */
d780 1
a780 1
	    2 * nPQNodes,		/* results */
d810 6
a815 9
	/* Fill writes. */
#define	INIT_WRITE_NODE(node,name)	do {				\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskWriteFunc,		\
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = unblockNode;				\
	(node)->antecedents[0] = recoveryNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)
d821 1
a821 2
		recoveryNode->params[nReadNodes + i].p =
		    DISK_NODE_PDA(wudNodes + i);
d824 1
a824 1
	/* Write redundancy pdas. */
@


1.3
log
@sync with work by Greg Oster on NetBSD

Please note: This update has *only* been tested on i386 with IDE
disks. Could someone with a spare box please make sure all is OK with
SCSI and maybe other arches ? sparc testing will follow locally.

* remove rf_sys.h
* many changes to make it more stable
* some performance increases
* All raid threads now get their own kernel process and the calling
  raidctl(8) program will show status progress through a meter.
* In theory FFS_SOFTUPDATES and RAIDframe will now work together - NOT
  TESTED YET

See http://www.cs.usask.ca/staff/oster/raid.html

This updates include Greg's changes to Jan 4th 2000.

TODO:
* some odd behaviour when running raictl -c on an already config'ed
  raid set - problem founf, fix being done
* progress meter is in raidctl(8) - seperate commit, but could do with
  sync'ing with OpenBSD ftp version
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_dagdegwr.c,v 1.2 1999/02/16 00:02:29 niklas Exp $	*/
/*	$NetBSD: rf_dagdegwr.c,v 1.4 1999/08/13 03:41:53 oster Exp $	*/
a41 1
#include "rf_threadid.h"
@


1.2
log
@Merge from NetBSD, mostly indentation
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_dagdegwr.c,v 1.1 1999/01/11 14:29:07 niklas Exp $	*/
/*	$NetBSD: rf_dagdegwr.c,v 1.3 1999/02/05 00:06:07 oster Exp $	*/
a46 1
#include "rf_sys.h"
@


1.1
log
@Import of CMU's RAIDframe via NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_dagdegwr.c,v 1.1 1998/11/13 04:20:27 oster Exp $	*/
/*	$NetBSD: rf_dagdegwr.c,v 1.1 1998/11/13 04:20:27 oster Exp $	*/
a34 102
 * :  
 * Log: rf_dagdegwr.c,v 
 * Revision 1.23  1996/11/05 21:10:40  jimz
 * failed pda generalization
 *
 * Revision 1.22  1996/08/23  14:49:48  jimz
 * remove bogus assert from small write double deg DAG generator
 *
 * Revision 1.21  1996/08/21  05:09:44  jimz
 * get rid of bogus fakery in DoubleDegSmallWrite
 *
 * Revision 1.20  1996/08/21  04:14:35  jimz
 * cleanup doubledegsmallwrite
 * NOTE: we need doubledeglargewrite
 *
 * Revision 1.19  1996/08/19  21:39:38  jimz
 * CommonCreateSimpleDegradedWriteDAG() was unable to correctly create DAGs for
 * complete stripe overwrite accesses- it assumed the necessity to read old
 * data. Rather than do the "right" thing, and risk breaking a critical DAG so
 * close to release, I made a no-op read node to stick in and link up in this
 * case. Seems to work.
 *
 * Revision 1.18  1996/07/31  15:35:34  jimz
 * evenodd changes; bugfixes for double-degraded archs, generalize
 * some formerly PQ-only functions
 *
 * Revision 1.17  1996/07/28  20:31:39  jimz
 * i386netbsd port
 * true/false fixup
 *
 * Revision 1.16  1996/07/27  23:36:08  jimz
 * Solaris port of simulator
 *
 * Revision 1.15  1996/07/27  16:30:19  jimz
 * cleanup sweep
 *
 * Revision 1.14  1996/07/22  19:52:16  jimz
 * switched node params to RF_DagParam_t, a union of
 * a 64-bit int and a void *, for better portability
 * attempted hpux port, but failed partway through for
 * lack of a single C compiler capable of compiling all
 * source files
 *
 * Revision 1.13  1996/06/09  02:36:46  jimz
 * lots of little crufty cleanup- fixup whitespace
 * issues, comment #ifdefs, improve typing in some
 * places (esp size-related)
 *
 * Revision 1.12  1996/06/07  22:26:27  jimz
 * type-ify which_ru (RF_ReconUnitNum_t)
 *
 * Revision 1.11  1996/06/07  21:33:04  jimz
 * begin using consistent types for sector numbers,
 * stripe numbers, row+col numbers, recon unit numbers
 *
 * Revision 1.10  1996/05/31  22:26:54  jimz
 * fix a lot of mapping problems, memory allocation problems
 * found some weird lock issues, fixed 'em
 * more code cleanup
 *
 * Revision 1.9  1996/05/30  11:29:41  jimz
 * Numerous bug fixes. Stripe lock release code disagreed with the taking code
 * about when stripes should be locked (I made it consistent: no parity, no lock)
 * There was a lot of extra serialization of I/Os which I've removed- a lot of
 * it was to calculate values for the cache code, which is no longer with us.
 * More types, function, macro cleanup. Added code to properly quiesce the array
 * on shutdown. Made a lot of stuff array-specific which was (bogusly) general
 * before. Fixed memory allocation, freeing bugs.
 *
 * Revision 1.8  1996/05/27  18:56:37  jimz
 * more code cleanup
 * better typing
 * compiles in all 3 environments
 *
 * Revision 1.7  1996/05/24  22:17:04  jimz
 * continue code + namespace cleanup
 * typed a bunch of flags
 *
 * Revision 1.6  1996/05/24  04:28:55  jimz
 * release cleanup ckpt
 *
 * Revision 1.5  1996/05/23  21:46:35  jimz
 * checkpoint in code cleanup (release prep)
 * lots of types, function names have been fixed
 *
 * Revision 1.4  1996/05/23  00:33:23  jimz
 * code cleanup: move all debug decls to rf_options.c, all extern
 * debug decls to rf_options.h, all debug vars preceded by rf_
 *
 * Revision 1.3  1996/05/18  19:51:34  jimz
 * major code cleanup- fix syntax, make some types consistent,
 * add prototypes, clean out dead code, et cetera
 *
 * Revision 1.2  1996/05/08  21:01:24  jimz
 * fixed up enum type names that were conflicting with other
 * enums and function names (ie, "panic")
 * future naming trends will be towards RF_ and rf_ for
 * everything raidframe-related
 *
 * Revision 1.1  1996/05/03  19:21:50  wvcii
 * Initial revision
 *
d53 1
a53 1
 * 
d77 2
a78 1
static RF_CREATE_DAG_FUNC_DECL(rf_CreateSimpleDegradedWriteDAG)
d80 2
a81 2
  rf_CommonCreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp,
    flags, allocList,1, rf_RecoveryXorFunc, RF_TRUE);
d84 8
a91 7
void rf_CreateDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList)
  RF_Raid_t             *raidPtr;
  RF_AccessStripeMap_t  *asmap;
  RF_DagHeader_t        *dag_h;
  void                  *bp;
  RF_RaidAccessFlags_t   flags;
  RF_AllocListElem_t    *allocList;
d93 17
a109 17
  RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
  RF_PhysDiskAddr_t *failedPDA = asmap->failedPDAs[0];
  
  RF_ASSERT( asmap->numDataFailed == 1 );
  dag_h->creator = "DegradedWriteDAG";

  /* if the access writes only a portion of the failed unit, and also writes
   * some portion of at least one surviving unit, we create two DAGs, one for
   * the failed component and one for the non-failed component, and do them
   * sequentially.  Note that the fact that we're accessing only a portion of
   * the failed unit indicates that the access either starts or ends in the
   * failed unit, and hence we need create only two dags.  This is inefficient
   * in that the same data or parity can get read and written twice using this
   * structure.  I need to fix this to do the access all at once.
   */
  RF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 && failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));
  rf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList);
d153 12
a164 11
void rf_CommonCreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	allocList, nfaults, redFunc, allowBufferRecycle)
  RF_Raid_t              *raidPtr;
  RF_AccessStripeMap_t   *asmap;
  RF_DagHeader_t         *dag_h;
  void                   *bp;
  RF_RaidAccessFlags_t    flags;
  RF_AllocListElem_t     *allocList;
  int                     nfaults;
  int                   (*redFunc)(RF_DagNode_t *);
  int                     allowBufferRecycle;
d166 344
a509 343
  int nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum, rdnodesFaked;
  RF_DagNode_t *blockNode, *unblockNode, *wnpNode, *wnqNode, *termNode;
  RF_DagNode_t *nodes, *wndNodes, *rrdNodes, *xorNode, *commitNode;
  RF_SectorCount_t sectorsPerSU;
  RF_ReconUnitNum_t which_ru;
  char *xorTargetBuf = NULL; /* the target buffer for the XOR operation */
  char *overlappingPDAs; /* a temporary array of flags */
  RF_AccessStripeMapHeader_t *new_asm_h[2];
  RF_PhysDiskAddr_t *pda, *parityPDA;
  RF_StripeNum_t parityStripeID;
  RF_PhysDiskAddr_t *failedPDA;
  RF_RaidLayout_t *layoutPtr;

  layoutPtr = &(raidPtr->Layout);
  parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,
    &which_ru);
  sectorsPerSU = layoutPtr->sectorsPerStripeUnit;
  /* failedPDA points to the pda within the asm that targets the failed disk */
  failedPDA = asmap->failedPDAs[0];

  if (rf_dagDebug)
    printf("[Creating degraded-write DAG]\n");

  RF_ASSERT( asmap->numDataFailed == 1 );
  dag_h->creator = "SimpleDegradedWriteDAG";

  /*
   * Generate two ASMs identifying the surviving data
   * we need in order to recover the lost data.
   */
  /* overlappingPDAs array must be zero'd */
  RF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));
  rf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h,
    &nXorBufs, NULL, overlappingPDAs, allocList);

  /* create all the nodes at once */
  nWndNodes = asmap->numStripeUnitsAccessed - 1;   /* no access is generated
                                                  * for the failed pda */

  nRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +
              ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);
  /*
   * XXX
   *
   * There's a bug with a complete stripe overwrite- that means 0 reads
   * of old data, and the rest of the DAG generation code doesn't like
   * that. A release is coming, and I don't wanna risk breaking a critical
   * DAG generator, so here's what I'm gonna do- if there's no read nodes,
   * I'm gonna fake there being a read node, and I'm gonna swap in a
   * no-op node in its place (to make all the link-up code happy).
   * This should be fixed at some point.  --jimz
   */
  if (nRrdNodes == 0) {
    nRrdNodes = 1;
    rdnodesFaked = 1;
  }
  else {
    rdnodesFaked = 0;
  }
  /* lock, unlock, xor, Wnd, Rrd, W(nfaults) */
  nNodes = 5 + nfaults + nWndNodes + nRrdNodes;
  RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t),
    (RF_DagNode_t *), allocList);
  i = 0;
  blockNode   = &nodes[i]; i += 1;
  commitNode    = &nodes[i]; i += 1;
  unblockNode = &nodes[i]; i += 1;
  termNode    = &nodes[i]; i += 1;
  xorNode     = &nodes[i]; i += 1;
  wnpNode     = &nodes[i]; i += 1;
  wndNodes    = &nodes[i]; i += nWndNodes;
  rrdNodes    = &nodes[i]; i += nRrdNodes;
  if (nfaults == 2) {
    wnqNode   = &nodes[i]; i += 1;
  }
  else {
    wnqNode = NULL;
  }
  RF_ASSERT(i == nNodes);

  /* this dag can not commit until all rrd and xor Nodes have completed */
  dag_h->numCommitNodes = 1;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  RF_ASSERT( nRrdNodes > 0 );
  rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, nRrdNodes, 0, 0, 0, dag_h, "Nil", allocList);
  rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, nWndNodes + nfaults, 1, 0, 0, dag_h, "Cmt", allocList);
  rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, 1, nWndNodes + nfaults, 0, 0, dag_h, "Nil", allocList);
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,
    NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
  rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,
    nRrdNodes, 2*nXorBufs+2, nfaults, dag_h, "Xrc", allocList);

  /*
   * Fill in the Rrd nodes. If any of the rrd buffers are the same size as
   * the failed buffer, save a pointer to it so we can use it as the target
   * of the XOR. The pdas in the rrd nodes have been range-restricted, so if
   * a buffer is the same size as the failed buffer, it must also be at the
   * same alignment within the SU.
   */
  i = 0;
  if (new_asm_h[0]) {
    for (i=0, pda=new_asm_h[0]->stripeMap->physInfo;
        i<new_asm_h[0]->stripeMap->numStripeUnitsAccessed;
        i++, pda=pda->next)
    {
      rf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
        rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Rrd", allocList);
      RF_ASSERT(pda);
      rrdNodes[i].params[0].p = pda;
      rrdNodes[i].params[1].p = pda->bufPtr;
      rrdNodes[i].params[2].v = parityStripeID;
      rrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
    }
  }
  /* i now equals the number of stripe units accessed in new_asm_h[0] */
  if (new_asm_h[1]) {
    for (j=0,pda=new_asm_h[1]->stripeMap->physInfo;
        j<new_asm_h[1]->stripeMap->numStripeUnitsAccessed;
        j++, pda=pda->next)
    {
      rf_InitNode(&rrdNodes[i+j], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
        rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Rrd", allocList);
      RF_ASSERT(pda);
      rrdNodes[i+j].params[0].p = pda;
      rrdNodes[i+j].params[1].p = pda->bufPtr;
      rrdNodes[i+j].params[2].v = parityStripeID;
      rrdNodes[i+j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      if (allowBufferRecycle && (pda->numSector == failedPDA->numSector))
        xorTargetBuf = pda->bufPtr;
    }
  }
  if (rdnodesFaked) {
    /*
     * This is where we'll init that fake noop read node
     * (XXX should the wakeup func be different?)
     */
    rf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
      NULL, 1, 1, 0, 0, dag_h, "RrN", allocList);
  }

  /*
   * Make a PDA for the parity unit.  The parity PDA should start at
   * the same offset into the SU as the failed PDA.
   */
  /* 
   * Danner comment:
   * I don't think this copy is really necessary.
   * We are in one of two cases here.
   *   (1) The entire failed unit is written. Then asmap->parityInfo will
   *       describe the entire parity.
   *   (2) We are only writing a subset of the failed unit and nothing
   *       else. Then the asmap->parityInfo describes the failed unit and
   *       the copy can also be avoided.
   */

  RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
  parityPDA->row = asmap->parityInfo->row;
  parityPDA->col = asmap->parityInfo->col;
  parityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)
    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);
  parityPDA->numSector = failedPDA->numSector;

  if (!xorTargetBuf) {
    RF_CallocAndAdd(xorTargetBuf, 1,
      rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);
  }

  /* init the Wnp node */
  rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnp", allocList);
  wnpNode->params[0].p = parityPDA;
  wnpNode->params[1].p = xorTargetBuf;
  wnpNode->params[2].v = parityStripeID;
  wnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);

  /* fill in the Wnq Node */
  if (nfaults == 2) {
    {
      RF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),
        (RF_PhysDiskAddr_t *), allocList);
      parityPDA->row = asmap->qInfo->row;
      parityPDA->col = asmap->qInfo->col;
      parityPDA->startSector = ((asmap->qInfo->startSector / sectorsPerSU)
        * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);
      parityPDA->numSector = failedPDA->numSector;

      rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
        rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnq", allocList);
      wnqNode->params[0].p = parityPDA;
      RF_CallocAndAdd(xorNode->results[1], 1,
        rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList); 
      wnqNode->params[1].p = xorNode->results[1];
      wnqNode->params[2].v = parityStripeID;
      wnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
    }
  }

  /* fill in the Wnd nodes */
  for (pda=asmap->physInfo, i=0; i<nWndNodes; i++, pda=pda->next) {
    if (pda == failedPDA) {
      i--;
      continue;
    }
    rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
      rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
    RF_ASSERT(pda);
    wndNodes[i].params[0].p = pda;
    wndNodes[i].params[1].p = pda->bufPtr;
    wndNodes[i].params[2].v = parityStripeID;
    wndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
  }

  /* fill in the results of the xor node */
  xorNode->results[0] = xorTargetBuf;

  /* fill in the params of the xor node */

  paramNum=0;
  if (rdnodesFaked == 0) {
    for (i=0; i<nRrdNodes; i++) {
      /* all the Rrd nodes need to be xored together */
      xorNode->params[paramNum++] = rrdNodes[i].params[0];
      xorNode->params[paramNum++] = rrdNodes[i].params[1];
    }
  }
  for (i=0; i < nWndNodes; i++) {
    /* any Wnd nodes that overlap the failed access need to be xored in */
    if (overlappingPDAs[i]) {
      RF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
      bcopy((char *)wndNodes[i].params[0].p, (char *)pda, sizeof(RF_PhysDiskAddr_t));
      rf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);
      xorNode->params[paramNum++].p = pda;
      xorNode->params[paramNum++].p = pda->bufPtr;
    }
  }
  RF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));

  /*
   * Install the failed PDA into the xor param list so that the
   * new data gets xor'd in.
   */
  xorNode->params[paramNum++].p = failedPDA;
  xorNode->params[paramNum++].p = failedPDA->bufPtr;

  /*
   * The last 2 params to the recovery xor node are always the failed
   * PDA and the raidPtr. install the failedPDA even though we have just
   * done so above. This allows us to use the same XOR function for both
   * degraded reads and degraded writes.
   */
  xorNode->params[paramNum++].p = failedPDA;
  xorNode->params[paramNum++].p = raidPtr;
  RF_ASSERT( paramNum == 2*nXorBufs+2 );

  /*
   * Code to link nodes begins here
   */

  /* link header to block node */
  RF_ASSERT(blockNode->numAntecedents == 0);
  dag_h->succedents[0] = blockNode;

  /* link block node to rd nodes */
  RF_ASSERT(blockNode->numSuccedents == nRrdNodes);
  for (i = 0; i < nRrdNodes; i++) {
    RF_ASSERT(rrdNodes[i].numAntecedents == 1);
    blockNode->succedents[i] = &rrdNodes[i];
    rrdNodes[i].antecedents[0] = blockNode;
    rrdNodes[i].antType[0] = rf_control;
  }

  /* link read nodes to xor node*/
  RF_ASSERT(xorNode->numAntecedents == nRrdNodes);
  for (i = 0; i < nRrdNodes; i++) {
    RF_ASSERT(rrdNodes[i].numSuccedents == 1);
    rrdNodes[i].succedents[0] = xorNode;
    xorNode->antecedents[i] = &rrdNodes[i];
    xorNode->antType[i] = rf_trueData;
  }

  /* link xor node to commit node */
  RF_ASSERT(xorNode->numSuccedents == 1);
  RF_ASSERT(commitNode->numAntecedents == 1);
  xorNode->succedents[0] = commitNode;
  commitNode->antecedents[0] = xorNode;
  commitNode->antType[0] = rf_control;

  /* link commit node to wnd nodes */
  RF_ASSERT(commitNode->numSuccedents == nfaults + nWndNodes);
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNodes[i].numAntecedents == 1);
    commitNode->succedents[i] = &wndNodes[i];
    wndNodes[i].antecedents[0] = commitNode;
    wndNodes[i].antType[0] = rf_control;
  }

  /* link the commit node to wnp, wnq nodes */
  RF_ASSERT(wnpNode->numAntecedents == 1);
  commitNode->succedents[nWndNodes] = wnpNode;
  wnpNode->antecedents[0] = commitNode;
  wnpNode->antType[0] = rf_control;
  if (nfaults == 2) {
    RF_ASSERT(wnqNode->numAntecedents == 1);
    commitNode->succedents[nWndNodes + 1] = wnqNode;
    wnqNode->antecedents[0] = commitNode;
    wnqNode->antType[0] = rf_control;
  }

  /* link write new data nodes to unblock node */
  RF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nfaults));
  for(i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNodes[i].numSuccedents == 1);
    wndNodes[i].succedents[0] = unblockNode;
    unblockNode->antecedents[i] = &wndNodes[i];
    unblockNode->antType[i] = rf_control;
  }

  /* link write new parity node to unblock node */
  RF_ASSERT(wnpNode->numSuccedents == 1);
  wnpNode->succedents[0] = unblockNode;
  unblockNode->antecedents[nWndNodes] = wnpNode;
  unblockNode->antType[nWndNodes] = rf_control;

  /* link write new q node to unblock node */
  if (nfaults == 2) {
    RF_ASSERT(wnqNode->numSuccedents == 1);
    wnqNode->succedents[0] = unblockNode;
    unblockNode->antecedents[nWndNodes+1] = wnqNode;
    unblockNode->antType[nWndNodes+1] = rf_control;
  }

  /* link unblock node to term node */
  RF_ASSERT(unblockNode->numSuccedents == 1);
  RF_ASSERT(termNode->numAntecedents == 1);
  RF_ASSERT(termNode->numSuccedents == 0);
  unblockNode->succedents[0] = termNode;
  termNode->antecedents[0]  = unblockNode;
  termNode->antType[0] = rf_control;
a510 1

d518 9
a526 8
void rf_WriteGenerateFailedAccessASMs(
  RF_Raid_t              *raidPtr,
  RF_AccessStripeMap_t   *asmap,
  RF_PhysDiskAddr_t     **pdap,
  int                    *nNodep,
  RF_PhysDiskAddr_t     **pqpdap,
  int                    *nPQNodep,
  RF_AllocListElem_t     *allocList)
d528 118
a645 123
  RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
  int PDAPerDisk,i;
  RF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;
  int numDataCol = layoutPtr->numDataCol;
  int state;
  unsigned napdas;
  RF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end;
  RF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];
  RF_PhysDiskAddr_t *pda_p;
  RF_RaidAddr_t sosAddr;

  /* determine how many pda's we will have to generate per unaccess stripe.
     If there is only one failed data unit, it is one; if two, possibly two,
     depending wether they overlap. */

  fone_start = rf_StripeUnitOffset(layoutPtr,fone->startSector);
  fone_end = fone_start + fone->numSector;

  if (asmap->numDataFailed==1)
    {
      PDAPerDisk = 1;
      state = 1;
      RF_MallocAndAdd(*pqpdap,2*sizeof(RF_PhysDiskAddr_t),(RF_PhysDiskAddr_t *), allocList);
      pda_p = *pqpdap;
      /* build p */
      CONS_PDA(parityInfo,fone_start,fone->numSector);
      pda_p->type = RF_PDA_TYPE_PARITY;
      pda_p++;
      /* build q */
      CONS_PDA(qInfo,fone_start,fone->numSector);
      pda_p->type = RF_PDA_TYPE_Q;
    }
  else
    {
      ftwo_start = rf_StripeUnitOffset(layoutPtr,ftwo->startSector);
      ftwo_end = ftwo_start + ftwo->numSector;    
      if (fone->numSector + ftwo->numSector > secPerSU)
	{
	  PDAPerDisk = 1;
	  state = 2;
	  RF_MallocAndAdd(*pqpdap,2*sizeof(RF_PhysDiskAddr_t),(RF_PhysDiskAddr_t *), allocList);
	  pda_p = *pqpdap;
	  CONS_PDA(parityInfo,0,secPerSU);
	  pda_p->type = RF_PDA_TYPE_PARITY;
	  pda_p++;
	  CONS_PDA(qInfo,0,secPerSU);
	  pda_p->type = RF_PDA_TYPE_Q;
	}
      else
	{
	  PDAPerDisk = 2;
	  state = 3;
	  /* four of them, fone, then ftwo */
	  RF_MallocAndAdd(*pqpdap,4*sizeof(RF_PhysDiskAddr_t),(RF_PhysDiskAddr_t *), allocList);
	  pda_p = *pqpdap;
	  CONS_PDA(parityInfo,fone_start,fone->numSector);
	  pda_p->type = RF_PDA_TYPE_PARITY;
	  pda_p++;
	  CONS_PDA(qInfo,fone_start,fone->numSector);
	  pda_p->type = RF_PDA_TYPE_Q;
	  pda_p++;
	  CONS_PDA(parityInfo,ftwo_start,ftwo->numSector);
	  pda_p->type = RF_PDA_TYPE_PARITY;
	  pda_p++;
	  CONS_PDA(qInfo,ftwo_start,ftwo->numSector);
	  pda_p->type = RF_PDA_TYPE_Q;
	}
    }
  /* figure out number of nonaccessed pda */
  napdas = PDAPerDisk * (numDataCol - 2);
  *nPQNodep = PDAPerDisk;

  *nNodep = napdas;
  if (napdas == 0) return; /* short circuit */

  /* allocate up our list of pda's */
     
  RF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);
  *pdap = pda_p;
    
  /* linkem together */
  for (i=0; i < (napdas-1); i++)
    pda_p[i].next = pda_p+(i+1);

  sosAddr      = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);
  for (i=0; i < numDataCol; i++)
    {
      if ((pda_p - (*pdap)) == napdas) 
	continue;
      pda_p->type = RF_PDA_TYPE_DATA;
      pda_p->raidAddress = sosAddr + (i * secPerSU);
      (raidPtr->Layout.map->MapSector)(raidPtr,pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
      /* skip over dead disks */
      if (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status)) 
	continue;
      switch (state)
	{
	case 1: /* fone */
	  pda_p->numSector = fone->numSector;
	  pda_p->raidAddress += fone_start;
	  pda_p->startSector += fone_start;
	  RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr,pda_p->numSector), (char *), allocList);
	  break;
	case 2: /* full stripe */
	  pda_p->numSector = secPerSU;
	  RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr,secPerSU), (char *), allocList);
	  break;
	case 3: /* two slabs */
	  pda_p->numSector = fone->numSector;
	  pda_p->raidAddress += fone_start;
	  pda_p->startSector += fone_start;
	  RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr,pda_p->numSector), (char *), allocList);
	  pda_p++;
          pda_p->type = RF_PDA_TYPE_DATA;
          pda_p->raidAddress = sosAddr + (i * secPerSU);
          (raidPtr->Layout.map->MapSector)(raidPtr,pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);
	  pda_p->numSector = ftwo->numSector;
	  pda_p->raidAddress += ftwo_start;
	  pda_p->startSector += ftwo_start;
	  RF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr,pda_p->numSector), (char *), allocList);
	  break;
	default:
	  RF_PANIC();
a646 2
      pda_p++;
    }
d648 2
a649 2
  RF_ASSERT  (pda_p - *pdap == napdas);
  return;
a650 1

d659 12
a670 11
void rf_DoubleDegSmallWrite(
  RF_Raid_t              *raidPtr,
  RF_AccessStripeMap_t   *asmap,
  RF_DagHeader_t         *dag_h,
  void                   *bp,
  RF_RaidAccessFlags_t    flags,
  RF_AllocListElem_t     *allocList,
  char                   *redundantReadNodeName,
  char                   *redundantWriteNodeName,
  char                   *recoveryNodeName,
  int                   (*recovFunc)(RF_DagNode_t *))
d672 66
a737 77
  RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
  RF_DagNode_t *nodes, *wudNodes, *rrdNodes, *recoveryNode, *blockNode, *unblockNode, *rpNodes,*rqNodes, *wpNodes, *wqNodes, *termNode;
  RF_PhysDiskAddr_t *pda, *pqPDAs;
  RF_PhysDiskAddr_t *npdas;
  int nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;
  RF_ReconUnitNum_t which_ru;
  int nPQNodes;
  RF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress, &which_ru);

  /* simple small write case -
     First part looks like a reconstruct-read of the failed data units.
     Then a write of all data units not failed. */


  /* 
             Hdr
	      |
       ------Block- 
      /  /         \   
    Rrd  Rrd ...  Rrd  Rp Rq
      \  \          /  
      -------PQ-----
            /   \   \
          Wud   Wp  WQ	     
           \    |   /
           --Unblock-
                |
                T

   Rrd = read recovery data  (potentially none)
   Wud = write user data (not incl. failed disks)
   Wp = Write P (could be two)
   Wq = Write Q (could be two)

   */
  
  rf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes, &pqPDAs, &nPQNodes,allocList);

  RF_ASSERT(asmap->numDataFailed == 1);
  
  nWudNodes = asmap->numStripeUnitsAccessed - (asmap->numDataFailed);
  nReadNodes = nRrdNodes + 2*nPQNodes;
  nWriteNodes = nWudNodes+ 2*nPQNodes;
  nNodes = 4 + nReadNodes + nWriteNodes;

  RF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
  blockNode = nodes;
  unblockNode = blockNode+1;
  termNode = unblockNode+1;
  recoveryNode = termNode+1;
  rrdNodes = recoveryNode+1;
  rpNodes = rrdNodes + nRrdNodes;
  rqNodes = rpNodes + nPQNodes;
  wudNodes = rqNodes + nPQNodes;
  wpNodes = wudNodes + nWudNodes;
  wqNodes = wpNodes + nPQNodes;
  
  dag_h->creator = "PQ_DDSimpleSmallWrite";
  dag_h->numSuccedents = 1;
  dag_h->succedents[0] = blockNode;
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
  termNode->antecedents[0]  = unblockNode;
  termNode->antType[0] = rf_control;

  /* init the block and unblock nodes */
  /* The block node has all the read nodes as successors */
  rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h, "Nil", allocList);
  for (i=0; i < nReadNodes; i++)
    blockNode->succedents[i] = rrdNodes+i;

  /* The unblock node has all the writes as successors */
  rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h, "Nil", allocList);
  for (i=0; i < nWriteNodes; i++) {
    unblockNode->antecedents[i] = wudNodes+i;
    unblockNode->antType[i] = rf_control;
  }
  unblockNode->succedents[0] = termNode;
a743 73
  
  /* build the read nodes */
  pda = npdas;
  for (i=0; i < nRrdNodes; i++, pda = pda->next) {
    INIT_READ_NODE(rrdNodes+i,"rrd");
    DISK_NODE_PARAMS(rrdNodes[i],pda);
  }

  /* read redundancy pdas */
  pda = pqPDAs;
  INIT_READ_NODE(rpNodes,"Rp");
  RF_ASSERT(pda);
  DISK_NODE_PARAMS(rpNodes[0],pda);
  pda++;
  INIT_READ_NODE(rqNodes, redundantReadNodeName );
  RF_ASSERT(pda);
  DISK_NODE_PARAMS(rqNodes[0],pda);
  if (nPQNodes==2)
    {
      pda++;
      INIT_READ_NODE(rpNodes+1,"Rp");
      RF_ASSERT(pda);
      DISK_NODE_PARAMS(rpNodes[1],pda);
      pda++;
      INIT_READ_NODE(rqNodes+1,redundantReadNodeName );
      RF_ASSERT(pda);
      DISK_NODE_PARAMS(rqNodes[1],pda);
    }

  /* the recovery node has all reads as precedessors and all writes as successors.
     It generates a result for every write P or write Q node. 
     As parameters, it takes a pda per read and a pda per stripe of user data written. 
     It also takes as the last params the raidPtr and asm.
     For results, it takes PDA for P & Q. */


    rf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc, rf_NullNodeUndoFunc, NULL,
           nWriteNodes, /* succesors */
           nReadNodes, /* preds */
           nReadNodes + nWudNodes + 3, /* params */
           2 * nPQNodes, /* results */
           dag_h, recoveryNodeName, allocList);
 


  for (i=0; i < nReadNodes; i++ )
    {
      recoveryNode->antecedents[i] = rrdNodes+i;
      recoveryNode->antType[i] = rf_control;
      recoveryNode->params[i].p = DISK_NODE_PDA(rrdNodes+i);
    }
  for (i=0; i < nWudNodes; i++)
    {
      recoveryNode->succedents[i] = wudNodes+i;
    }
  recoveryNode->params[nReadNodes+nWudNodes].p = asmap->failedPDAs[0];
  recoveryNode->params[nReadNodes+nWudNodes+1].p = raidPtr;
  recoveryNode->params[nReadNodes+nWudNodes+2].p = asmap;
  
  for ( ; i < nWriteNodes; i++)
      recoveryNode->succedents[i] = wudNodes+i;

  pda = pqPDAs;
  recoveryNode->results[0] = pda;
  pda++;
  recoveryNode->results[1] = pda;
  if ( nPQNodes == 2)
    {
      pda++;
      recoveryNode->results[2] = pda;
      pda++;
      recoveryNode->results[3] = pda;
    }
d745 68
a812 1
  /* fill writes */
d819 26
a844 28
  pda = asmap->physInfo;
  for (i=0; i < nWudNodes; i++)
    {
      INIT_WRITE_NODE(wudNodes+i,"Wd");
      DISK_NODE_PARAMS(wudNodes[i],pda);
      recoveryNode->params[nReadNodes+i].p = DISK_NODE_PDA(wudNodes+i);
      pda = pda->next;
    }
  /* write redundancy pdas */
  pda = pqPDAs;
  INIT_WRITE_NODE(wpNodes,"Wp");
  RF_ASSERT(pda);
  DISK_NODE_PARAMS(wpNodes[0],pda);
  pda++;
  INIT_WRITE_NODE(wqNodes,"Wq");
  RF_ASSERT(pda);
  DISK_NODE_PARAMS(wqNodes[0],pda);
  if (nPQNodes==2)
    {
      pda++;
      INIT_WRITE_NODE(wpNodes+1,"Wp");
      RF_ASSERT(pda);
      DISK_NODE_PARAMS(wpNodes[1],pda);
      pda++;
      INIT_WRITE_NODE(wqNodes+1,"Wq");
      RF_ASSERT(pda);
      DISK_NODE_PARAMS(wqNodes[1],pda);
    }
@

