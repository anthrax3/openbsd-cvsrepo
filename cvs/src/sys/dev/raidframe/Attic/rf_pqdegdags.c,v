head	1.6;
access;
symbols
	OPENBSD_5_1_BASE:1.5
	OPENBSD_5_1:1.5.0.38
	OPENBSD_5_0:1.5.0.36
	OPENBSD_5_0_BASE:1.5
	OPENBSD_4_9:1.5.0.34
	OPENBSD_4_9_BASE:1.5
	OPENBSD_4_8:1.5.0.32
	OPENBSD_4_8_BASE:1.5
	OPENBSD_4_7:1.5.0.28
	OPENBSD_4_7_BASE:1.5
	OPENBSD_4_6:1.5.0.30
	OPENBSD_4_6_BASE:1.5
	OPENBSD_4_5:1.5.0.26
	OPENBSD_4_5_BASE:1.5
	OPENBSD_4_4:1.5.0.24
	OPENBSD_4_4_BASE:1.5
	OPENBSD_4_3:1.5.0.22
	OPENBSD_4_3_BASE:1.5
	OPENBSD_4_2:1.5.0.20
	OPENBSD_4_2_BASE:1.5
	OPENBSD_4_1:1.5.0.18
	OPENBSD_4_1_BASE:1.5
	OPENBSD_4_0:1.5.0.16
	OPENBSD_4_0_BASE:1.5
	OPENBSD_3_9:1.5.0.14
	OPENBSD_3_9_BASE:1.5
	OPENBSD_3_8:1.5.0.12
	OPENBSD_3_8_BASE:1.5
	OPENBSD_3_7:1.5.0.10
	OPENBSD_3_7_BASE:1.5
	OPENBSD_3_6:1.5.0.8
	OPENBSD_3_6_BASE:1.5
	SMP_SYNC_A:1.5
	SMP_SYNC_B:1.5
	OPENBSD_3_5:1.5.0.6
	OPENBSD_3_5_BASE:1.5
	OPENBSD_3_4:1.5.0.4
	OPENBSD_3_4_BASE:1.5
	UBC_SYNC_A:1.5
	OPENBSD_3_3:1.5.0.2
	OPENBSD_3_3_BASE:1.5
	OPENBSD_3_2:1.4.0.16
	OPENBSD_3_2_BASE:1.4
	OPENBSD_3_1:1.4.0.14
	OPENBSD_3_1_BASE:1.4
	UBC_SYNC_B:1.4
	UBC:1.4.0.12
	UBC_BASE:1.4
	OPENBSD_3_0:1.4.0.10
	OPENBSD_3_0_BASE:1.4
	OPENBSD_2_9_BASE:1.4
	OPENBSD_2_9:1.4.0.8
	OPENBSD_2_8:1.4.0.6
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.4
	OPENBSD_2_7_BASE:1.4
	SMP:1.4.0.2
	SMP_BASE:1.4
	kame_19991208:1.3
	OPENBSD_2_6:1.2.0.4
	OPENBSD_2_6_BASE:1.2
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.6
date	2012.04.06.15.53.59;	author jsing;	state dead;
branches;
next	1.5;

1.5
date	2002.12.16.07.01.04;	author tdeval;	state Exp;
branches;
next	1.4;

1.4
date	2000.01.07.14.50.22;	author peter;	state Exp;
branches
	1.4.2.1
	1.4.12.1;
next	1.3;

1.3
date	99.10.29.08.57.18;	author todd;	state Exp;
branches;
next	1.2;

1.2
date	99.02.16.00.03.11;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	99.01.11.14.29.40;	author niklas;	state Exp;
branches;
next	;

1.4.2.1
date	2003.03.28.00.38.29;	author niklas;	state Exp;
branches;
next	;

1.4.12.1
date	2003.05.19.22.21.53;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.6
log
@Put raidframe in the attic.
@
text
@/*	$OpenBSD: rf_pqdegdags.c,v 1.5 2002/12/16 07:01:04 tdeval Exp $	*/
/*	$NetBSD: rf_pqdegdags.c,v 1.5 1999/08/15 02:36:40 oster Exp $	*/

/*
 * Copyright (c) 1995 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Author: Daniel Stodolsky
 *
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 *
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * rf_pqdegdags.c
 * Degraded mode dags for double fault cases.
 */


#include "rf_archs.h"

#if	(RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0)

#include "rf_types.h"
#include "rf_raid.h"
#include "rf_dag.h"
#include "rf_dagdegrd.h"
#include "rf_dagdegwr.h"
#include "rf_dagfuncs.h"
#include "rf_dagutils.h"
#include "rf_etimer.h"
#include "rf_acctrace.h"
#include "rf_general.h"
#include "rf_pqdegdags.h"
#include "rf_pq.h"

void rf_applyPDA(RF_Raid_t *, RF_PhysDiskAddr_t *, RF_PhysDiskAddr_t *,
	RF_PhysDiskAddr_t *, void *);

/*
 * Two data drives have failed, and we are doing a read that covers one of them.
 * We may also be reading some of the surviving drives.
 */


/*****************************************************************************
 *
 * Creates a DAG to perform a degraded-mode read of data within one stripe.
 * This DAG is as follows:
 *
 *			                Hdr
 *			                 |
 *			               Block
 *			 /         /           \         \     \   \
 *			Rud  ...  Rud         Rrd  ...  Rrd    Rp  Rq
 *			| \       | \         | \       | \    | \ | \
 *
 *			           |                 |
 *			        Unblock              X
 *			            \               /
 *			             ------ T ------
 *
 * Each R node is a successor of the L node.
 * One successor arc from each R node goes to U, and the other to X.
 * There is one Rud for each chunk of surviving user data requested by the
 * user, and one Rrd for each chunk of surviving user data _not_ being read
 * by the user.
 * R = read, ud = user data, rd = recovery (surviving) data, p = P data,
 * q = Qdata, X = pq recovery node, T = terminate
 *
 * The block & unblock nodes are leftovers from a previous version. They
 * do nothing, but I haven't deleted them because it would be a tremendous
 * effort to put them back in.
 *
 * Note:  The target buffer for the XOR node is set to the actual user buffer
 * where the failed data is supposed to end up. This buffer is zero'd by the
 * code here. Thus, if you create a degraded read dag, use it, and then
 * re-use. You have to be sure to zero the target buffer prior to the re-use.
 *
 * Every buffer read is passed to the pq recovery node, whose job it is to
 * sort out what's needed and what's not.
 *****************************************************************************/

/* Init a disk node with 2 successors and one predecessor. */
#define	INIT_DISK_NODE(node,name)					\
do {									\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc,		\
	    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 2, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = unblockNode;				\
	(node)->succedents[1] = recoveryNode;				\
	(node)->antecedents[0] = blockNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)

#define	DISK_NODE_PARAMS(_node_,_p_)					\
do {									\
	(_node_).params[0].p = _p_ ;					\
	(_node_).params[1].p = (_p_)->bufPtr;				\
	(_node_).params[2].v = parityStripeID;				\
	(_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,	\
	    0, 0, which_ru);						\
} while (0)

#define	DISK_NODE_PDA(node)	((node)->params[0].p)

RF_CREATE_DAG_FUNC_DECL(rf_PQ_DoubleDegRead)
{
	rf_DoubleDegRead(raidPtr, asmap, dag_h, bp, flags, allocList,
	    "Rq", "PQ Recovery", rf_PQDoubleRecoveryFunc);
}

void
rf_applyPDA(RF_Raid_t *raidPtr, RF_PhysDiskAddr_t *pda,
    RF_PhysDiskAddr_t *ppda, RF_PhysDiskAddr_t *qpda, void *bp)
{
	RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
	RF_RaidAddr_t s0off = rf_StripeUnitOffset(layoutPtr, ppda->startSector);
	RF_SectorCount_t s0len = ppda->numSector, len;
	RF_SectorNum_t suoffset;
	unsigned coeff;
	char *pbuf = ppda->bufPtr;
	char *qbuf = qpda->bufPtr;
	char *buf;
	int delta;

	suoffset = rf_StripeUnitOffset(layoutPtr, pda->startSector);
	len = pda->numSector;
	/* See if pda intersects a recovery pda. */
	if ((suoffset < s0off + s0len) && (suoffset + len > s0off)) {
		buf = pda->bufPtr;
		coeff = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
		    pda->raidAddress);
		coeff = (coeff % raidPtr->Layout.numDataCol);

		if (suoffset < s0off) {
			delta = s0off - suoffset;
			buf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
			suoffset = s0off;
			len -= delta;
		}
		if (suoffset > s0off) {
			delta = suoffset - s0off;
			pbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
			qbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
		}
		if ((suoffset + len) > (s0len + s0off))
			len = s0len + s0off - suoffset;

		/* Src, dest, len. */
		rf_bxor(buf, pbuf, rf_RaidAddressToByte(raidPtr, len), bp);

		/* Dest, src, len, coeff. */
		rf_IncQ((unsigned long *) qbuf, (unsigned long *) buf,
		    rf_RaidAddressToByte(raidPtr, len), coeff);
	}
}


/*
 * Recover data in the case of a double failure. There can be two
 * result buffers, one for each chunk of data trying to be recovered.
 * The params are pda's that have not been range restricted or otherwise
 * politely massaged - this should be done here. The last params are the
 * pdas of P and Q, followed by the raidPtr. The list can look like
 *
 *   pda, pda, ..., p pda, q pda, raidptr, asm
 *
 * or
 *
 *   pda, pda, ..., p_1 pda, p_2 pda, q_1 pda, q_2 pda, raidptr, asm
 *
 * depending on whether two chunks of recovery data were required.
 *
 * The second condition only arises if there are two failed buffers
 * whose lengths do not add up a stripe unit.
 */

int
rf_PQDoubleRecoveryFunc(RF_DagNode_t *node)
{
	int np = node->numParams;
	RF_AccessStripeMap_t *asmap =
	    (RF_AccessStripeMap_t *) node->params[np - 1].p;
	RF_Raid_t *raidPtr = (RF_Raid_t *) node->params[np - 2].p;
	RF_RaidLayout_t *layoutPtr = (RF_RaidLayout_t *) & (raidPtr->Layout);
	int d, i;
	unsigned coeff;
	RF_RaidAddr_t sosAddr, suoffset;
	RF_SectorCount_t len, secPerSU = layoutPtr->sectorsPerStripeUnit;
	int two = 0;
	RF_PhysDiskAddr_t *ppda, *ppda2, *qpda, *qpda2, *pda, npda;
	char *buf;
	int numDataCol = layoutPtr->numDataCol;
	RF_Etimer_t timer;
	RF_AccTraceEntry_t *tracerec = node->dagHdr->tracerec;

	RF_ETIMER_START(timer);

	if (asmap->failedPDAs[1] &&
	    (asmap->failedPDAs[1]->numSector +
	     asmap->failedPDAs[0]->numSector < secPerSU)) {
		RF_ASSERT(0);
		ppda = node->params[np - 6].p;
		ppda2 = node->params[np - 5].p;
		qpda = node->params[np - 4].p;
		qpda2 = node->params[np - 3].p;
		d = (np - 6);
		two = 1;
	} else {
		ppda = node->params[np - 4].p;
		qpda = node->params[np - 3].p;
		d = (np - 4);
	}

	for (i = 0; i < d; i++) {
		pda = node->params[i].p;
		buf = pda->bufPtr;
		suoffset = rf_StripeUnitOffset(layoutPtr, pda->startSector);
		len = pda->numSector;
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr,
		    pda->raidAddress);
		/* Compute the data unit offset within the column. */
		coeff = (coeff % raidPtr->Layout.numDataCol);
		/* See if pda intersects a recovery pda. */
		rf_applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
		if (two)
			rf_applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
	}

	/*
	 * Ok, we got the parity back to the point where we can recover. We
	 * now need to determine the coeff of the columns that need to be
	 * recovered. We can also only need to recover a single stripe unit.
	 */

	if (asmap->failedPDAs[1] == NULL) {	/*
						 * Only a single stripe unit
						 * to recover.
						 */
		pda = asmap->failedPDAs[0];
		sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
		    asmap->raidAddress);
		/* Need to determine the column of the other failed disk. */
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr,
		    pda->raidAddress);
		/* Compute the data unit offset within the column. */
		coeff = (coeff % raidPtr->Layout.numDataCol);
		for (i = 0; i < numDataCol; i++) {
			npda.raidAddress = sosAddr + (i * secPerSU);
			(raidPtr->Layout.map->MapSector) (raidPtr,
			    npda.raidAddress, &(npda.row), &(npda.col),
			    &(npda.startSector), 0);
			/* Skip over dead disks. */
			if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col]
			    .status))
				if (i != coeff)
					break;
		}
		RF_ASSERT(i < numDataCol);
		RF_ASSERT(two == 0);
		/*
		 * Recover the data. Since we need only to recover one
		 * column, we overwrite the parity with the other one.
		 */
		if (coeff < i)	/* Recovering 'a'. */
			rf_PQ_recover((unsigned long *) ppda->bufPtr,
			    (unsigned long *) qpda->bufPtr,
			    (unsigned long *) pda->bufPtr,
			    (unsigned long *) ppda->bufPtr,
			    rf_RaidAddressToByte(raidPtr, pda->numSector),
			    coeff, i);
		else		/* Recovering 'b'. */
			rf_PQ_recover((unsigned long *) ppda->bufPtr,
			    (unsigned long *) qpda->bufPtr,
			    (unsigned long *) ppda->bufPtr,
			    (unsigned long *) pda->bufPtr,
			    rf_RaidAddressToByte(raidPtr, pda->numSector),
			    i, coeff);
	} else
		RF_PANIC();

	RF_ETIMER_STOP(timer);
	RF_ETIMER_EVAL(timer);
	if (tracerec)
		tracerec->q_us += RF_ETIMER_VAL_US(timer);
	rf_GenericWakeupFunc(node, 0);
	return (0);
}

int
rf_PQWriteDoubleRecoveryFunc(RF_DagNode_t *node)
{
	/*
	 * The situation:
	 *
	 * We are doing a write that hits only one failed data unit. The other
	 * failed data unit is not being overwritten, so we need to generate
	 * it.
	 *
	 * For the moment, we assume all the nonfailed data being written is in
	 * the shadow of the failed data unit. (i.e., either a single data
	 * unit write or the entire failed stripe unit is being overwritten.)
	 *
	 * Recovery strategy: apply the recovery data to the parity and Q.
	 * Use P & Q to recover the second failed data unit in P. Zero fill
	 * Q, then apply the recovered data to P. Then apply the data being
	 * written to the failed drive. Then walk through the surviving drives,
	 * applying new data when it exists, othewise the recovery data.
	 * Quite a mess.
	 *
	 *
	 * The params:
	 *
	 *   read pda0, read pda1, ..., read pda (numDataCol-3),
	 *   write pda0, ..., write pda (numStripeUnitAccess - numDataFailed),
	 *   failed pda, raidPtr, asmap
	 */

	int np = node->numParams;
	RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *)
	    node->params[np - 1].p;
	RF_Raid_t *raidPtr = (RF_Raid_t *) node->params[np - 2].p;
	RF_RaidLayout_t *layoutPtr = (RF_RaidLayout_t *) & (raidPtr->Layout);
	int i;
	RF_RaidAddr_t sosAddr;
	unsigned coeff;
	RF_StripeCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;
	RF_PhysDiskAddr_t *ppda, *qpda, *pda, npda;
	int numDataCol = layoutPtr->numDataCol;
	RF_Etimer_t timer;
	RF_AccTraceEntry_t *tracerec = node->dagHdr->tracerec;

	RF_ASSERT(node->numResults == 2);
	RF_ASSERT(asmap->failedPDAs[1] == NULL);
	RF_ETIMER_START(timer);
	ppda = node->results[0];
	qpda = node->results[1];
	/* apply the recovery data */
	for (i = 0; i < numDataCol - 2; i++)
		rf_applyPDA(raidPtr, node->params[i].p, ppda, qpda,
		    node->dagHdr->bp);

	/* Determine the other failed data unit. */
	pda = asmap->failedPDAs[0];
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
	    asmap->raidAddress);
	/* Need to determine the column of the other failed disk. */
	coeff = rf_RaidAddressToStripeUnitID(layoutPtr, pda->raidAddress);
	/* Compute the data unit offset within the column. */
	coeff = (coeff % raidPtr->Layout.numDataCol);
	for (i = 0; i < numDataCol; i++) {
		npda.raidAddress = sosAddr + (i * secPerSU);
		(raidPtr->Layout.map->MapSector) (raidPtr, npda.raidAddress,
		    &(npda.row), &(npda.col), &(npda.startSector), 0);
		/* Skip over dead disks. */
		if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col].status))
			if (i != coeff)
				break;
	}
	RF_ASSERT(i < numDataCol);
	/*
	 * Recover the data. The column we want to recover, we write over the
	 * parity. The column we don't care about, we dump in q.
	 */
	if (coeff < i)		/* Recovering 'a'. */
		rf_PQ_recover((unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    rf_RaidAddressToByte(raidPtr, pda->numSector), coeff, i);
	else			/* Recovering 'b'. */
		rf_PQ_recover((unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) ppda->bufPtr,
		    rf_RaidAddressToByte(raidPtr, pda->numSector), i, coeff);

	/* OK. The valid data is in P. Zero fill Q, then inc it into it. */
	bzero(qpda->bufPtr, rf_RaidAddressToByte(raidPtr, qpda->numSector));
	rf_IncQ((unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr,
	    rf_RaidAddressToByte(raidPtr, qpda->numSector), i);

	/* Now apply all the write data to the buffer. */
	/*
	 * Single stripe unit write case: The failed data is the only thing
	 * we are writing.
	 */
	RF_ASSERT(asmap->numStripeUnitsAccessed == 1);
	/* Dest, src, len, coeff. */
	rf_IncQ((unsigned long *) qpda->bufPtr,
	    (unsigned long *) asmap->failedPDAs[0]->bufPtr,
	    rf_RaidAddressToByte(raidPtr, qpda->numSector), coeff);
	rf_bxor(asmap->failedPDAs[0]->bufPtr, ppda->bufPtr,
	    rf_RaidAddressToByte(raidPtr, ppda->numSector), node->dagHdr->bp);

	/* Now apply all the recovery data. */
	for (i = 0; i < numDataCol - 2; i++)
		rf_applyPDA(raidPtr, node->params[i].p, ppda, qpda,
		    node->dagHdr->bp);

	RF_ETIMER_STOP(timer);
	RF_ETIMER_EVAL(timer);
	if (tracerec)
		tracerec->q_us += RF_ETIMER_VAL_US(timer);

	rf_GenericWakeupFunc(node, 0);
	return (0);
}

RF_CREATE_DAG_FUNC_DECL(rf_PQ_DDLargeWrite)
{
	RF_PANIC();
}


/*
 * Two lost data unit write case.
 *
 * There are really two cases here:
 *
 * (1) The write completely covers the two lost data units.
 *     In that case, a reconstruct write that doesn't write the
 *     failed data units will do the correct thing. So in this case,
 *     the dag looks like
 *
 *	   Full stripe read of surviving data units (not being overwritten)
 *	   Write new data (ignoring failed units)
 *	   Compute P&Q
 *	   Write P&Q
 *
 *
 * (2) The write does not completely cover both failed data units
 *     (but touches at least one of them). Then we need to do the
 *     equivalent of a reconstruct read to recover the missing data
 *     unit from the other stripe.
 *
 *     For any data we are writing that is not in the "shadow"
 *     of the failed units, we need to do a four cycle update.
 *     PANIC on this case. For now.
 *
 */

RF_CREATE_DAG_FUNC_DECL(rf_PQ_200_CreateWriteDAG)
{
	RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
	RF_SectorCount_t sectorsPerSU = layoutPtr->sectorsPerStripeUnit;
	int sum;
	int nf = asmap->numDataFailed;

	sum = asmap->failedPDAs[0]->numSector;
	if (nf == 2)
		sum += asmap->failedPDAs[1]->numSector;

	if ((nf == 2) && (sum == (2 * sectorsPerSU))) {
		/* Large write case. */
		rf_PQ_DDLargeWrite(raidPtr, asmap, dag_h, bp, flags, allocList);
		return;
	}
	if ((nf == asmap->numStripeUnitsAccessed) || (sum >= sectorsPerSU)) {
		/* Small write case, no user data not in shadow. */
		rf_PQ_DDSimpleSmallWrite(raidPtr, asmap, dag_h, bp, flags,
		    allocList);
		return;
	}
	RF_PANIC();
}

RF_CREATE_DAG_FUNC_DECL(rf_PQ_DDSimpleSmallWrite)
{
	rf_DoubleDegSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList,
	    "Rq", "Wq", "PQ Recovery", rf_PQWriteDoubleRecoveryFunc);
}

#endif	/* (RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0) */
@


1.5
log
@Major KNF.  Incentive from Tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_pqdegdags.c,v 1.4 2000/01/07 14:50:22 peter Exp $	*/
@


1.4
log
@sync with work by Greg Oster on NetBSD

Please note: This update has *only* been tested on i386 with IDE
disks. Could someone with a spare box please make sure all is OK with
SCSI and maybe other arches ? sparc testing will follow locally.

* remove rf_sys.h
* many changes to make it more stable
* some performance increases
* All raid threads now get their own kernel process and the calling
  raidctl(8) program will show status progress through a meter.
* In theory FFS_SOFTUPDATES and RAIDframe will now work together - NOT
  TESTED YET

See http://www.cs.usask.ca/staff/oster/raid.html

This updates include Greg's changes to Jan 4th 2000.

TODO:
* some odd behaviour when running raictl -c on an already config'ed
  raid set - problem founf, fix being done
* progress meter is in raidctl(8) - seperate commit, but could do with
  sync'ing with OpenBSD ftp version
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_pqdegdags.c,v 1.3 1999/10/29 08:57:18 todd Exp $	*/
d3 1
d34 1
a34 1
*/
d39 1
a39 1
#if (RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0)
d54 2
a55 3
static void 
applyPDA(RF_Raid_t * raidPtr, RF_PhysDiskAddr_t * pda, RF_PhysDiskAddr_t * ppda,
    RF_PhysDiskAddr_t * qpda, void *bp);
d58 3
a60 2
   Two data drives have failed, and we are doing a read that covers one of them.
   We may also be reading some of the surviving drives.
d63 1
a63 1
 *****************************************************************************************
d65 1
a65 1
 * creates a DAG to perform a degraded-mode read of data within one stripe.
d68 19
a86 18
 *                                      Hdr
 *                                       |
 *                                     Block
 *                       /         /           \         \     \   \
 *                      Rud  ...  Rud         Rrd  ...  Rrd    Rp  Rq
 *                      | \       | \         | \       | \    | \ | \
 *
 *                                 |                 |
 *                              Unblock              X
 *                                  \               /
 *                                   ------ T ------
 *
 * Each R node is a successor of the L node
 * One successor arc from each R node goes to U, and the other to X
 * There is one Rud for each chunk of surviving user data requested by the user,
 * and one Rrd for each chunk of surviving user data _not_ being read by the user
 * R = read, ud = user data, rd = recovery (surviving) data, p = P data, q = Qdata
 * X = pq recovery node, T = terminate
d88 1
a88 1
 * The block & unblock nodes are leftovers from a previous version.  They
d92 29
a120 21
 * Note:  The target buffer for the XOR node is set to the actual user buffer where the
 * failed data is supposed to end up.  This buffer is zero'd by the code here.  Thus,
 * if you create a degraded read dag, use it, and then re-use, you have to be sure to
 * zero the target buffer prior to the re-use.
 *
 * Every buffer read is passed to the pq recovery node, whose job it is to sort out whats
 * needs and what's not.
 ****************************************************************************************/
/*   init a disk node with 2 successors and one predecessor */
#define INIT_DISK_NODE(node,name) \
rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 2,1,4,0, dag_h, name, allocList); \
(node)->succedents[0] = unblockNode; \
(node)->succedents[1] = recoveryNode; \
(node)->antecedents[0] = blockNode; \
(node)->antType[0] = rf_control

#define DISK_NODE_PARAMS(_node_,_p_) \
  (_node_).params[0].p = _p_ ; \
  (_node_).params[1].p = (_p_)->bufPtr; \
  (_node_).params[2].v = parityStripeID; \
  (_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru)
d122 1
a122 1
#define DISK_NODE_PDA(node)  ((node)->params[0].p)
d130 3
a132 7
static void 
applyPDA(raidPtr, pda, ppda, qpda, bp)
	RF_Raid_t *raidPtr;
	RF_PhysDiskAddr_t *pda;
	RF_PhysDiskAddr_t *ppda;
	RF_PhysDiskAddr_t *qpda;
	void   *bp;
d139 4
a142 4
	char   *pbuf = ppda->bufPtr;
	char   *qbuf = qpda->bufPtr;
	char   *buf;
	int     delta;
d146 1
a146 1
	/* see if pda intersects a recovery pda */
d149 2
a150 1
		coeff = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), pda->raidAddress);
d155 2
a156 1
			buf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), delta);
d162 4
a165 2
			pbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), delta);
			qbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), delta);
d170 1
a170 1
		/* src, dest, len */
d173 3
a175 2
		/* dest, src, len, coeff */
		rf_IncQ((unsigned long *) qbuf, (unsigned long *) buf, rf_RaidAddressToByte(raidPtr, len), coeff);
a177 6
/*
   Recover data in the case of a double failure. There can be two
   result buffers, one for each chunk of data trying to be recovered.
   The params are pda's that have not been range restricted or otherwise
   politely massaged - this should be done here. The last params are the
   pdas of P and Q, followed by the raidPtr. The list can look like
a178 1
   pda, pda, ... , p pda, q pda, raidptr, asm
d180 18
a197 5
   or

   pda, pda, ... , p_1 pda, p_2 pda, q_1 pda, q_2 pda, raidptr, asm

   depending on wether two chunks of recovery data were required.
d199 2
a200 8
   The second condition only arises if there are two failed buffers
   whose lengths do not add up a stripe unit.
*/


int 
rf_PQDoubleRecoveryFunc(node)
	RF_DagNode_t *node;
d202 3
a204 2
	int     np = node->numParams;
	RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *) node->params[np - 1].p;
d207 1
a207 1
	int     d, i;
d211 1
a211 1
	int     two = 0;
d213 2
a214 2
	char   *buf;
	int     numDataCol = layoutPtr->numDataCol;
d221 2
a222 1
	    (asmap->failedPDAs[1]->numSector + asmap->failedPDAs[0]->numSector < secPerSU)) {
d241 3
a243 2
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr, pda->raidAddress);
		/* compute the data unit offset within the column */
d245 2
a246 2
		/* see if pda intersects a recovery pda */
		applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
d248 1
a248 1
			applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
d251 2
a252 1
	/* ok, we got the parity back to the point where we can recover. We
d254 2
a255 1
	 * recovered. We can also only need to recover a single stripe unit. */
d257 4
a260 2
	if (asmap->failedPDAs[1] == NULL) {	/* only a single stripe unit
						 * to recover. */
d262 6
a267 4
		sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);
		/* need to determine the column of the other failed disk */
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr, pda->raidAddress);
		/* compute the data unit offset within the column */
d271 6
a276 3
			(raidPtr->Layout.map->MapSector) (raidPtr, npda.raidAddress, &(npda.row), &(npda.col), &(npda.startSector), 0);
			/* skip over dead disks */
			if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col].status))
d282 18
a299 6
		/* recover the data. Since we need only want to recover one
		 * column, we overwrite the parity with the other one. */
		if (coeff < i)	/* recovering 'a' */
			rf_PQ_recover((unsigned long *) ppda->bufPtr, (unsigned long *) qpda->bufPtr, (unsigned long *) pda->bufPtr, (unsigned long *) ppda->bufPtr, rf_RaidAddressToByte(raidPtr, pda->numSector), coeff, i);
		else		/* recovering 'b' */
			rf_PQ_recover((unsigned long *) ppda->bufPtr, (unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr, (unsigned long *) pda->bufPtr, rf_RaidAddressToByte(raidPtr, pda->numSector), i, coeff);
d311 2
a312 3
int 
rf_PQWriteDoubleRecoveryFunc(node)
	RF_DagNode_t *node;
d314 3
a316 2
	/* The situation:
	 * 
d320 1
a320 1
	 * 
d322 21
a342 18
	 * the shadow of the failed data unit. (i.e,, either a single data
	 * unit write or the entire failed stripe unit is being overwritten. )
	 * 
	 * Recovery strategy: apply the recovery data to the parity and q. Use P
	 * & Q to recover the second failed data unit in P. Zero fill Q, then
	 * apply the recovered data to p. Then apply the data being written to
	 * the failed drive. Then walk through the surviving drives, applying
	 * new data when it exists, othewise the recovery data. Quite a mess.
	 * 
	 * 
	 * The params
	 * 
	 * read pda0, read pda1, ... read pda (numDataCol-3), write pda0, ... ,
	 * write pda (numStripeUnitAccess - numDataFailed), failed pda,
	 * raidPtr, asmap */

	int     np = node->numParams;
	RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *) node->params[np - 1].p;
d345 1
a345 1
	int     i;
d350 1
a350 1
	int     numDataCol = layoutPtr->numDataCol;
d361 2
a362 1
		applyPDA(raidPtr, node->params[i].p, ppda, qpda, node->dagHdr->bp);
d364 1
a364 1
	/* determine the other failed data unit */
d366 3
a368 2
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);
	/* need to determine the column of the other failed disk */
d370 1
a370 1
	/* compute the data unit offset within the column */
d374 3
a376 2
		(raidPtr->Layout.map->MapSector) (raidPtr, npda.raidAddress, &(npda.row), &(npda.col), &(npda.startSector), 0);
		/* skip over dead disks */
d382 16
a397 6
	/* recover the data. The column we want to recover we write over the
	 * parity. The column we don't care about we dump in q. */
	if (coeff < i)		/* recovering 'a' */
		rf_PQ_recover((unsigned long *) ppda->bufPtr, (unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr, (unsigned long *) qpda->bufPtr, rf_RaidAddressToByte(raidPtr, pda->numSector), coeff, i);
	else			/* recovering 'b' */
		rf_PQ_recover((unsigned long *) ppda->bufPtr, (unsigned long *) qpda->bufPtr, (unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr, rf_RaidAddressToByte(raidPtr, pda->numSector), i, coeff);
d401 2
a402 1
	rf_IncQ((unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr, rf_RaidAddressToByte(raidPtr, qpda->numSector), i);
d404 5
a408 3
	/* now apply all the write data to the buffer */
	/* single stripe unit write case: the failed data is only thing we are
	 * writing. */
d410 6
a415 3
	/* dest, src, len, coeff */
	rf_IncQ((unsigned long *) qpda->bufPtr, (unsigned long *) asmap->failedPDAs[0]->bufPtr, rf_RaidAddressToByte(raidPtr, qpda->numSector), coeff);
	rf_bxor(asmap->failedPDAs[0]->bufPtr, ppda->bufPtr, rf_RaidAddressToByte(raidPtr, ppda->numSector), node->dagHdr->bp);
d417 1
a417 1
	/* now apply all the recovery data */
d419 2
a420 1
		applyPDA(raidPtr, node->params[i].p, ppda, qpda, node->dagHdr->bp);
d430 1
a434 2
/*
   Two lost data unit write case.
a435 1
   There are really two cases here:
d437 26
a462 20
   (1) The write completely covers the two lost data units.
       In that case, a reconstruct write that doesn't write the
       failed data units will do the correct thing. So in this case,
       the dag looks like

            full stripe read of surviving data units (not being overwritten)
	    write new data (ignoring failed units)   compute P&Q
	                                             write P&Q


   (2) The write does not completely cover both failed data units
       (but touches at least one of them). Then we need to do the
       equivalent of a reconstruct read to recover the missing data
       unit from the other stripe.

       For any data we are writing that is not in the "shadow"
       of the failed units, we need to do a four cycle update.
       PANIC on this case. for now

*/
d468 2
a469 2
	int     sum;
	int     nf = asmap->numDataFailed;
d476 1
a476 1
		/* large write case */
d481 3
a483 2
		/* small write case, no user data not in shadow */
		rf_PQ_DDSimpleSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList);
d488 1
d491 2
a492 1
	rf_DoubleDegSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList, "Rq", "Wq", "PQ Recovery", rf_PQWriteDoubleRecoveryFunc);
d494 2
a495 2
#endif				/* (RF_INCLUDE_DECL_PQ > 0) ||
				 * (RF_INCLUDE_RAID6 > 0) */
@


1.4.12.1
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 1

d33 1
a33 1
 */
d38 1
a38 1
#if	(RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0)
d53 3
a55 2
void rf_applyPDA(RF_Raid_t *, RF_PhysDiskAddr_t *, RF_PhysDiskAddr_t *,
	RF_PhysDiskAddr_t *, void *);
d58 2
a59 3
 * Two data drives have failed, and we are doing a read that covers one of them.
 * We may also be reading some of the surviving drives.
 */
d62 1
a62 1
/*****************************************************************************
d64 1
a64 1
 * Creates a DAG to perform a degraded-mode read of data within one stripe.
d67 18
a84 19
 *			                Hdr
 *			                 |
 *			               Block
 *			 /         /           \         \     \   \
 *			Rud  ...  Rud         Rrd  ...  Rrd    Rp  Rq
 *			| \       | \         | \       | \    | \ | \
 *
 *			           |                 |
 *			        Unblock              X
 *			            \               /
 *			             ------ T ------
 *
 * Each R node is a successor of the L node.
 * One successor arc from each R node goes to U, and the other to X.
 * There is one Rud for each chunk of surviving user data requested by the
 * user, and one Rrd for each chunk of surviving user data _not_ being read
 * by the user.
 * R = read, ud = user data, rd = recovery (surviving) data, p = P data,
 * q = Qdata, X = pq recovery node, T = terminate
d86 1
a86 1
 * The block & unblock nodes are leftovers from a previous version. They
d90 21
a110 29
 * Note:  The target buffer for the XOR node is set to the actual user buffer
 * where the failed data is supposed to end up. This buffer is zero'd by the
 * code here. Thus, if you create a degraded read dag, use it, and then
 * re-use. You have to be sure to zero the target buffer prior to the re-use.
 *
 * Every buffer read is passed to the pq recovery node, whose job it is to
 * sort out what's needed and what's not.
 *****************************************************************************/

/* Init a disk node with 2 successors and one predecessor. */
#define	INIT_DISK_NODE(node,name)					\
do {									\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc,		\
	    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 2, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = unblockNode;				\
	(node)->succedents[1] = recoveryNode;				\
	(node)->antecedents[0] = blockNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)

#define	DISK_NODE_PARAMS(_node_,_p_)					\
do {									\
	(_node_).params[0].p = _p_ ;					\
	(_node_).params[1].p = (_p_)->bufPtr;				\
	(_node_).params[2].v = parityStripeID;				\
	(_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,	\
	    0, 0, which_ru);						\
} while (0)
d112 1
a112 1
#define	DISK_NODE_PDA(node)	((node)->params[0].p)
d120 7
a126 3
void
rf_applyPDA(RF_Raid_t *raidPtr, RF_PhysDiskAddr_t *pda,
    RF_PhysDiskAddr_t *ppda, RF_PhysDiskAddr_t *qpda, void *bp)
d133 4
a136 4
	char *pbuf = ppda->bufPtr;
	char *qbuf = qpda->bufPtr;
	char *buf;
	int delta;
d140 1
a140 1
	/* See if pda intersects a recovery pda. */
d143 1
a143 2
		coeff = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
		    pda->raidAddress);
d148 1
a148 2
			buf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
d154 2
a155 4
			pbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
			qbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
d160 1
a160 1
		/* Src, dest, len. */
d163 2
a164 3
		/* Dest, src, len, coeff. */
		rf_IncQ((unsigned long *) qbuf, (unsigned long *) buf,
		    rf_RaidAddressToByte(raidPtr, len), coeff);
d167 8
d176 9
a185 18
/*
 * Recover data in the case of a double failure. There can be two
 * result buffers, one for each chunk of data trying to be recovered.
 * The params are pda's that have not been range restricted or otherwise
 * politely massaged - this should be done here. The last params are the
 * pdas of P and Q, followed by the raidPtr. The list can look like
 *
 *   pda, pda, ..., p pda, q pda, raidptr, asm
 *
 * or
 *
 *   pda, pda, ..., p_1 pda, p_2 pda, q_1 pda, q_2 pda, raidptr, asm
 *
 * depending on whether two chunks of recovery data were required.
 *
 * The second condition only arises if there are two failed buffers
 * whose lengths do not add up a stripe unit.
 */
d187 3
a189 2
int
rf_PQDoubleRecoveryFunc(RF_DagNode_t *node)
d191 2
a192 3
	int np = node->numParams;
	RF_AccessStripeMap_t *asmap =
	    (RF_AccessStripeMap_t *) node->params[np - 1].p;
d195 1
a195 1
	int d, i;
d199 1
a199 1
	int two = 0;
d201 2
a202 2
	char *buf;
	int numDataCol = layoutPtr->numDataCol;
d209 1
a209 2
	    (asmap->failedPDAs[1]->numSector +
	     asmap->failedPDAs[0]->numSector < secPerSU)) {
d228 2
a229 3
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr,
		    pda->raidAddress);
		/* Compute the data unit offset within the column. */
d231 2
a232 2
		/* See if pda intersects a recovery pda. */
		rf_applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
d234 1
a234 1
			rf_applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
d237 1
a237 2
	/*
	 * Ok, we got the parity back to the point where we can recover. We
d239 1
a239 2
	 * recovered. We can also only need to recover a single stripe unit.
	 */
d241 2
a242 4
	if (asmap->failedPDAs[1] == NULL) {	/*
						 * Only a single stripe unit
						 * to recover.
						 */
d244 4
a247 6
		sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
		    asmap->raidAddress);
		/* Need to determine the column of the other failed disk. */
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr,
		    pda->raidAddress);
		/* Compute the data unit offset within the column. */
d251 3
a253 6
			(raidPtr->Layout.map->MapSector) (raidPtr,
			    npda.raidAddress, &(npda.row), &(npda.col),
			    &(npda.startSector), 0);
			/* Skip over dead disks. */
			if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col]
			    .status))
d259 6
a264 18
		/*
		 * Recover the data. Since we need only to recover one
		 * column, we overwrite the parity with the other one.
		 */
		if (coeff < i)	/* Recovering 'a'. */
			rf_PQ_recover((unsigned long *) ppda->bufPtr,
			    (unsigned long *) qpda->bufPtr,
			    (unsigned long *) pda->bufPtr,
			    (unsigned long *) ppda->bufPtr,
			    rf_RaidAddressToByte(raidPtr, pda->numSector),
			    coeff, i);
		else		/* Recovering 'b'. */
			rf_PQ_recover((unsigned long *) ppda->bufPtr,
			    (unsigned long *) qpda->bufPtr,
			    (unsigned long *) ppda->bufPtr,
			    (unsigned long *) pda->bufPtr,
			    rf_RaidAddressToByte(raidPtr, pda->numSector),
			    i, coeff);
d276 3
a278 2
int
rf_PQWriteDoubleRecoveryFunc(RF_DagNode_t *node)
d280 2
a281 3
	/*
	 * The situation:
	 *
d285 1
a285 1
	 *
d287 18
a304 21
	 * the shadow of the failed data unit. (i.e., either a single data
	 * unit write or the entire failed stripe unit is being overwritten.)
	 *
	 * Recovery strategy: apply the recovery data to the parity and Q.
	 * Use P & Q to recover the second failed data unit in P. Zero fill
	 * Q, then apply the recovered data to P. Then apply the data being
	 * written to the failed drive. Then walk through the surviving drives,
	 * applying new data when it exists, othewise the recovery data.
	 * Quite a mess.
	 *
	 *
	 * The params:
	 *
	 *   read pda0, read pda1, ..., read pda (numDataCol-3),
	 *   write pda0, ..., write pda (numStripeUnitAccess - numDataFailed),
	 *   failed pda, raidPtr, asmap
	 */

	int np = node->numParams;
	RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *)
	    node->params[np - 1].p;
d307 1
a307 1
	int i;
d312 1
a312 1
	int numDataCol = layoutPtr->numDataCol;
d323 1
a323 2
		rf_applyPDA(raidPtr, node->params[i].p, ppda, qpda,
		    node->dagHdr->bp);
d325 1
a325 1
	/* Determine the other failed data unit. */
d327 2
a328 3
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
	    asmap->raidAddress);
	/* Need to determine the column of the other failed disk. */
d330 1
a330 1
	/* Compute the data unit offset within the column. */
d334 2
a335 3
		(raidPtr->Layout.map->MapSector) (raidPtr, npda.raidAddress,
		    &(npda.row), &(npda.col), &(npda.startSector), 0);
		/* Skip over dead disks. */
d341 6
a346 16
	/*
	 * Recover the data. The column we want to recover, we write over the
	 * parity. The column we don't care about, we dump in q.
	 */
	if (coeff < i)		/* Recovering 'a'. */
		rf_PQ_recover((unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    rf_RaidAddressToByte(raidPtr, pda->numSector), coeff, i);
	else			/* Recovering 'b'. */
		rf_PQ_recover((unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) ppda->bufPtr,
		    rf_RaidAddressToByte(raidPtr, pda->numSector), i, coeff);
d350 1
a350 2
	rf_IncQ((unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr,
	    rf_RaidAddressToByte(raidPtr, qpda->numSector), i);
d352 3
a354 5
	/* Now apply all the write data to the buffer. */
	/*
	 * Single stripe unit write case: The failed data is the only thing
	 * we are writing.
	 */
d356 3
a358 6
	/* Dest, src, len, coeff. */
	rf_IncQ((unsigned long *) qpda->bufPtr,
	    (unsigned long *) asmap->failedPDAs[0]->bufPtr,
	    rf_RaidAddressToByte(raidPtr, qpda->numSector), coeff);
	rf_bxor(asmap->failedPDAs[0]->bufPtr, ppda->bufPtr,
	    rf_RaidAddressToByte(raidPtr, ppda->numSector), node->dagHdr->bp);
d360 1
a360 1
	/* Now apply all the recovery data. */
d362 1
a362 2
		rf_applyPDA(raidPtr, node->params[i].p, ppda, qpda,
		    node->dagHdr->bp);
a371 1

d376 14
d391 4
d396 5
a400 26
/*
 * Two lost data unit write case.
 *
 * There are really two cases here:
 *
 * (1) The write completely covers the two lost data units.
 *     In that case, a reconstruct write that doesn't write the
 *     failed data units will do the correct thing. So in this case,
 *     the dag looks like
 *
 *	   Full stripe read of surviving data units (not being overwritten)
 *	   Write new data (ignoring failed units)
 *	   Compute P&Q
 *	   Write P&Q
 *
 *
 * (2) The write does not completely cover both failed data units
 *     (but touches at least one of them). Then we need to do the
 *     equivalent of a reconstruct read to recover the missing data
 *     unit from the other stripe.
 *
 *     For any data we are writing that is not in the "shadow"
 *     of the failed units, we need to do a four cycle update.
 *     PANIC on this case. For now.
 *
 */
d406 2
a407 2
	int sum;
	int nf = asmap->numDataFailed;
d414 1
a414 1
		/* Large write case. */
d419 2
a420 3
		/* Small write case, no user data not in shadow. */
		rf_PQ_DDSimpleSmallWrite(raidPtr, asmap, dag_h, bp, flags,
		    allocList);
a424 1

d427 1
a427 2
	rf_DoubleDegSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList,
	    "Rq", "Wq", "PQ Recovery", rf_PQWriteDoubleRecoveryFunc);
d429 2
a430 2

#endif	/* (RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0) */
@


1.4.2.1
log
@Sync the SMP branch with 3.3
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 1

d33 1
a33 1
 */
d38 1
a38 1
#if	(RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0)
d53 3
a55 2
void rf_applyPDA(RF_Raid_t *, RF_PhysDiskAddr_t *, RF_PhysDiskAddr_t *,
	RF_PhysDiskAddr_t *, void *);
d58 2
a59 3
 * Two data drives have failed, and we are doing a read that covers one of them.
 * We may also be reading some of the surviving drives.
 */
d62 1
a62 1
/*****************************************************************************
d64 1
a64 1
 * Creates a DAG to perform a degraded-mode read of data within one stripe.
d67 18
a84 19
 *			                Hdr
 *			                 |
 *			               Block
 *			 /         /           \         \     \   \
 *			Rud  ...  Rud         Rrd  ...  Rrd    Rp  Rq
 *			| \       | \         | \       | \    | \ | \
 *
 *			           |                 |
 *			        Unblock              X
 *			            \               /
 *			             ------ T ------
 *
 * Each R node is a successor of the L node.
 * One successor arc from each R node goes to U, and the other to X.
 * There is one Rud for each chunk of surviving user data requested by the
 * user, and one Rrd for each chunk of surviving user data _not_ being read
 * by the user.
 * R = read, ud = user data, rd = recovery (surviving) data, p = P data,
 * q = Qdata, X = pq recovery node, T = terminate
d86 1
a86 1
 * The block & unblock nodes are leftovers from a previous version. They
d90 21
a110 29
 * Note:  The target buffer for the XOR node is set to the actual user buffer
 * where the failed data is supposed to end up. This buffer is zero'd by the
 * code here. Thus, if you create a degraded read dag, use it, and then
 * re-use. You have to be sure to zero the target buffer prior to the re-use.
 *
 * Every buffer read is passed to the pq recovery node, whose job it is to
 * sort out what's needed and what's not.
 *****************************************************************************/

/* Init a disk node with 2 successors and one predecessor. */
#define	INIT_DISK_NODE(node,name)					\
do {									\
	rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc,		\
	    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 2, 1, 4, 0,	\
	    dag_h, name, allocList);					\
	(node)->succedents[0] = unblockNode;				\
	(node)->succedents[1] = recoveryNode;				\
	(node)->antecedents[0] = blockNode;				\
	(node)->antType[0] = rf_control;				\
} while (0)

#define	DISK_NODE_PARAMS(_node_,_p_)					\
do {									\
	(_node_).params[0].p = _p_ ;					\
	(_node_).params[1].p = (_p_)->bufPtr;				\
	(_node_).params[2].v = parityStripeID;				\
	(_node_).params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,	\
	    0, 0, which_ru);						\
} while (0)
d112 1
a112 1
#define	DISK_NODE_PDA(node)	((node)->params[0].p)
d120 7
a126 3
void
rf_applyPDA(RF_Raid_t *raidPtr, RF_PhysDiskAddr_t *pda,
    RF_PhysDiskAddr_t *ppda, RF_PhysDiskAddr_t *qpda, void *bp)
d133 4
a136 4
	char *pbuf = ppda->bufPtr;
	char *qbuf = qpda->bufPtr;
	char *buf;
	int delta;
d140 1
a140 1
	/* See if pda intersects a recovery pda. */
d143 1
a143 2
		coeff = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
		    pda->raidAddress);
d148 1
a148 2
			buf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
d154 2
a155 4
			pbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
			qbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),
			    delta);
d160 1
a160 1
		/* Src, dest, len. */
d163 2
a164 3
		/* Dest, src, len, coeff. */
		rf_IncQ((unsigned long *) qbuf, (unsigned long *) buf,
		    rf_RaidAddressToByte(raidPtr, len), coeff);
d167 8
d176 9
a185 18
/*
 * Recover data in the case of a double failure. There can be two
 * result buffers, one for each chunk of data trying to be recovered.
 * The params are pda's that have not been range restricted or otherwise
 * politely massaged - this should be done here. The last params are the
 * pdas of P and Q, followed by the raidPtr. The list can look like
 *
 *   pda, pda, ..., p pda, q pda, raidptr, asm
 *
 * or
 *
 *   pda, pda, ..., p_1 pda, p_2 pda, q_1 pda, q_2 pda, raidptr, asm
 *
 * depending on whether two chunks of recovery data were required.
 *
 * The second condition only arises if there are two failed buffers
 * whose lengths do not add up a stripe unit.
 */
d187 3
a189 2
int
rf_PQDoubleRecoveryFunc(RF_DagNode_t *node)
d191 2
a192 3
	int np = node->numParams;
	RF_AccessStripeMap_t *asmap =
	    (RF_AccessStripeMap_t *) node->params[np - 1].p;
d195 1
a195 1
	int d, i;
d199 1
a199 1
	int two = 0;
d201 2
a202 2
	char *buf;
	int numDataCol = layoutPtr->numDataCol;
d209 1
a209 2
	    (asmap->failedPDAs[1]->numSector +
	     asmap->failedPDAs[0]->numSector < secPerSU)) {
d228 2
a229 3
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr,
		    pda->raidAddress);
		/* Compute the data unit offset within the column. */
d231 2
a232 2
		/* See if pda intersects a recovery pda. */
		rf_applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
d234 1
a234 1
			rf_applyPDA(raidPtr, pda, ppda, qpda, node->dagHdr->bp);
d237 1
a237 2
	/*
	 * Ok, we got the parity back to the point where we can recover. We
d239 1
a239 2
	 * recovered. We can also only need to recover a single stripe unit.
	 */
d241 2
a242 4
	if (asmap->failedPDAs[1] == NULL) {	/*
						 * Only a single stripe unit
						 * to recover.
						 */
d244 4
a247 6
		sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
		    asmap->raidAddress);
		/* Need to determine the column of the other failed disk. */
		coeff = rf_RaidAddressToStripeUnitID(layoutPtr,
		    pda->raidAddress);
		/* Compute the data unit offset within the column. */
d251 3
a253 6
			(raidPtr->Layout.map->MapSector) (raidPtr,
			    npda.raidAddress, &(npda.row), &(npda.col),
			    &(npda.startSector), 0);
			/* Skip over dead disks. */
			if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col]
			    .status))
d259 6
a264 18
		/*
		 * Recover the data. Since we need only to recover one
		 * column, we overwrite the parity with the other one.
		 */
		if (coeff < i)	/* Recovering 'a'. */
			rf_PQ_recover((unsigned long *) ppda->bufPtr,
			    (unsigned long *) qpda->bufPtr,
			    (unsigned long *) pda->bufPtr,
			    (unsigned long *) ppda->bufPtr,
			    rf_RaidAddressToByte(raidPtr, pda->numSector),
			    coeff, i);
		else		/* Recovering 'b'. */
			rf_PQ_recover((unsigned long *) ppda->bufPtr,
			    (unsigned long *) qpda->bufPtr,
			    (unsigned long *) ppda->bufPtr,
			    (unsigned long *) pda->bufPtr,
			    rf_RaidAddressToByte(raidPtr, pda->numSector),
			    i, coeff);
d276 3
a278 2
int
rf_PQWriteDoubleRecoveryFunc(RF_DagNode_t *node)
d280 2
a281 3
	/*
	 * The situation:
	 *
d285 1
a285 1
	 *
d287 18
a304 21
	 * the shadow of the failed data unit. (i.e., either a single data
	 * unit write or the entire failed stripe unit is being overwritten.)
	 *
	 * Recovery strategy: apply the recovery data to the parity and Q.
	 * Use P & Q to recover the second failed data unit in P. Zero fill
	 * Q, then apply the recovered data to P. Then apply the data being
	 * written to the failed drive. Then walk through the surviving drives,
	 * applying new data when it exists, othewise the recovery data.
	 * Quite a mess.
	 *
	 *
	 * The params:
	 *
	 *   read pda0, read pda1, ..., read pda (numDataCol-3),
	 *   write pda0, ..., write pda (numStripeUnitAccess - numDataFailed),
	 *   failed pda, raidPtr, asmap
	 */

	int np = node->numParams;
	RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *)
	    node->params[np - 1].p;
d307 1
a307 1
	int i;
d312 1
a312 1
	int numDataCol = layoutPtr->numDataCol;
d323 1
a323 2
		rf_applyPDA(raidPtr, node->params[i].p, ppda, qpda,
		    node->dagHdr->bp);
d325 1
a325 1
	/* Determine the other failed data unit. */
d327 2
a328 3
	sosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr,
	    asmap->raidAddress);
	/* Need to determine the column of the other failed disk. */
d330 1
a330 1
	/* Compute the data unit offset within the column. */
d334 2
a335 3
		(raidPtr->Layout.map->MapSector) (raidPtr, npda.raidAddress,
		    &(npda.row), &(npda.col), &(npda.startSector), 0);
		/* Skip over dead disks. */
d341 6
a346 16
	/*
	 * Recover the data. The column we want to recover, we write over the
	 * parity. The column we don't care about, we dump in q.
	 */
	if (coeff < i)		/* Recovering 'a'. */
		rf_PQ_recover((unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    rf_RaidAddressToByte(raidPtr, pda->numSector), coeff, i);
	else			/* Recovering 'b'. */
		rf_PQ_recover((unsigned long *) ppda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) qpda->bufPtr,
		    (unsigned long *) ppda->bufPtr,
		    rf_RaidAddressToByte(raidPtr, pda->numSector), i, coeff);
d350 1
a350 2
	rf_IncQ((unsigned long *) qpda->bufPtr, (unsigned long *) ppda->bufPtr,
	    rf_RaidAddressToByte(raidPtr, qpda->numSector), i);
d352 3
a354 5
	/* Now apply all the write data to the buffer. */
	/*
	 * Single stripe unit write case: The failed data is the only thing
	 * we are writing.
	 */
d356 3
a358 6
	/* Dest, src, len, coeff. */
	rf_IncQ((unsigned long *) qpda->bufPtr,
	    (unsigned long *) asmap->failedPDAs[0]->bufPtr,
	    rf_RaidAddressToByte(raidPtr, qpda->numSector), coeff);
	rf_bxor(asmap->failedPDAs[0]->bufPtr, ppda->bufPtr,
	    rf_RaidAddressToByte(raidPtr, ppda->numSector), node->dagHdr->bp);
d360 1
a360 1
	/* Now apply all the recovery data. */
d362 1
a362 2
		rf_applyPDA(raidPtr, node->params[i].p, ppda, qpda,
		    node->dagHdr->bp);
a371 1

d376 14
d391 4
d396 5
a400 26
/*
 * Two lost data unit write case.
 *
 * There are really two cases here:
 *
 * (1) The write completely covers the two lost data units.
 *     In that case, a reconstruct write that doesn't write the
 *     failed data units will do the correct thing. So in this case,
 *     the dag looks like
 *
 *	   Full stripe read of surviving data units (not being overwritten)
 *	   Write new data (ignoring failed units)
 *	   Compute P&Q
 *	   Write P&Q
 *
 *
 * (2) The write does not completely cover both failed data units
 *     (but touches at least one of them). Then we need to do the
 *     equivalent of a reconstruct read to recover the missing data
 *     unit from the other stripe.
 *
 *     For any data we are writing that is not in the "shadow"
 *     of the failed units, we need to do a four cycle update.
 *     PANIC on this case. For now.
 *
 */
d406 2
a407 2
	int sum;
	int nf = asmap->numDataFailed;
d414 1
a414 1
		/* Large write case. */
d419 2
a420 3
		/* Small write case, no user data not in shadow. */
		rf_PQ_DDSimpleSmallWrite(raidPtr, asmap, dag_h, bp, flags,
		    allocList);
a424 1

d427 1
a427 2
	rf_DoubleDegSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList,
	    "Rq", "Wq", "PQ Recovery", rf_PQWriteDoubleRecoveryFunc);
d429 2
a430 2

#endif	/* (RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0) */
@


1.3
log
@writen->written
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_pqdegdags.c,v 1.2 1999/02/16 00:03:11 niklas Exp $	*/
/*	$NetBSD: rf_pqdegdags.c,v 1.3 1999/02/05 00:06:15 oster Exp $	*/
d43 2
a51 1
#include "rf_sys.h"
@


1.2
log
@Merge from NetBSD, mostly indentation
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_pqdegdags.c,v 1.1 1999/01/11 14:29:40 niklas Exp $	*/
d385 1
a385 1
            full stripe read of surviving data units (not being overwriten)
@


1.1
log
@Import of CMU's RAIDframe via NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_pqdegdags.c,v 1.1 1998/11/13 04:20:32 oster Exp $	*/
/*	$NetBSD: rf_pqdegdags.c,v 1.1 1998/11/13 04:20:32 oster Exp $	*/
d32 1
a32 1
 * Degraded mode dags for double fault cases. 
a34 109
/*
 * :  
 * Log: rf_pqdegdags.c,v 
 * Revision 1.31  1996/11/05 21:10:40  jimz
 * failed pda generalization
 *
 * Revision 1.30  1996/07/31  16:30:05  jimz
 * asm/asmap fix
 *
 * Revision 1.29  1996/07/31  15:35:15  jimz
 * evenodd changes; bugfixes for double-degraded archs, generalize
 * some formerly PQ-only functions
 *
 * Revision 1.28  1996/07/28  20:31:39  jimz
 * i386netbsd port
 * true/false fixup
 *
 * Revision 1.27  1996/07/27  23:36:08  jimz
 * Solaris port of simulator
 *
 * Revision 1.26  1996/07/22  19:52:16  jimz
 * switched node params to RF_DagParam_t, a union of
 * a 64-bit int and a void *, for better portability
 * attempted hpux port, but failed partway through for
 * lack of a single C compiler capable of compiling all
 * source files
 *
 * Revision 1.25  1996/06/09  02:36:46  jimz
 * lots of little crufty cleanup- fixup whitespace
 * issues, comment #ifdefs, improve typing in some
 * places (esp size-related)
 *
 * Revision 1.24  1996/06/07  22:26:27  jimz
 * type-ify which_ru (RF_ReconUnitNum_t)
 *
 * Revision 1.23  1996/06/07  21:33:04  jimz
 * begin using consistent types for sector numbers,
 * stripe numbers, row+col numbers, recon unit numbers
 *
 * Revision 1.22  1996/06/02  17:31:48  jimz
 * Moved a lot of global stuff into array structure, where it belongs.
 * Fixed up paritylogging, pss modules in this manner. Some general
 * code cleanup. Removed lots of dead code, some dead files.
 *
 * Revision 1.21  1996/05/31  22:26:54  jimz
 * fix a lot of mapping problems, memory allocation problems
 * found some weird lock issues, fixed 'em
 * more code cleanup
 *
 * Revision 1.20  1996/05/30  12:59:18  jimz
 * make etimer happier, more portable
 *
 * Revision 1.19  1996/05/30  11:29:41  jimz
 * Numerous bug fixes. Stripe lock release code disagreed with the taking code
 * about when stripes should be locked (I made it consistent: no parity, no lock)
 * There was a lot of extra serialization of I/Os which I've removed- a lot of
 * it was to calculate values for the cache code, which is no longer with us.
 * More types, function, macro cleanup. Added code to properly quiesce the array
 * on shutdown. Made a lot of stuff array-specific which was (bogusly) general
 * before. Fixed memory allocation, freeing bugs.
 *
 * Revision 1.18  1996/05/27  18:56:37  jimz
 * more code cleanup
 * better typing
 * compiles in all 3 environments
 *
 * Revision 1.17  1996/05/24  22:17:04  jimz
 * continue code + namespace cleanup
 * typed a bunch of flags
 *
 * Revision 1.16  1996/05/24  04:28:55  jimz
 * release cleanup ckpt
 *
 * Revision 1.15  1996/05/23  21:46:35  jimz
 * checkpoint in code cleanup (release prep)
 * lots of types, function names have been fixed
 *
 * Revision 1.14  1996/05/23  00:33:23  jimz
 * code cleanup: move all debug decls to rf_options.c, all extern
 * debug decls to rf_options.h, all debug vars preceded by rf_
 *
 * Revision 1.13  1996/05/18  19:51:34  jimz
 * major code cleanup- fix syntax, make some types consistent,
 * add prototypes, clean out dead code, et cetera
 *
 * Revision 1.12  1996/05/08  21:01:24  jimz
 * fixed up enum type names that were conflicting with other
 * enums and function names (ie, "panic")
 * future naming trends will be towards RF_ and rf_ for
 * everything raidframe-related
 *
 * Revision 1.11  1996/05/03  19:47:50  wvcii
 * removed include of rf_redstripe.h
 *
 * Revision 1.10  1995/12/12  18:10:06  jimz
 * MIN -> RF_MIN, MAX -> RF_MAX, ASSERT -> RF_ASSERT
 * fix 80-column brain damage in comments
 *
 * Revision 1.9  1995/11/30  16:17:57  wvcii
 * added copyright info
 *
 * Revision 1.8  1995/11/07  15:33:25  wvcii
 * dag creation routines now generate term node
 * added asserts
 * encoded commit point nodes, antecedence types into dags
 * didn't add commit barrier - the code is a mess and needs to
 * be cleand up first
 *
 */
d52 3
a54 2
static void applyPDA(RF_Raid_t *raidPtr, RF_PhysDiskAddr_t *pda, RF_PhysDiskAddr_t *ppda,
	RF_PhysDiskAddr_t *qpda, void *bp);
d57 3
a59 3
   Two data drives have failed, and we are doing a read that covers one of them. 
   We may also be reading some of the surviving drives. 
   
d115 2
a116 2
  rf_DoubleDegRead(raidPtr, asmap, dag_h, bp, flags, allocList,
    "Rq", "PQ Recovery", rf_PQDoubleRecoveryFunc);
d118 8
a125 7
  
static void applyPDA(raidPtr,pda,ppda,qpda, bp)
  RF_Raid_t          *raidPtr;
  RF_PhysDiskAddr_t  *pda;
  RF_PhysDiskAddr_t  *ppda;
  RF_PhysDiskAddr_t  *qpda;
  void               *bp;
d127 37
a163 31
  RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
  RF_RaidAddr_t s0off = rf_StripeUnitOffset(layoutPtr, ppda->startSector);
  RF_SectorCount_t s0len = ppda->numSector, len;
  RF_SectorNum_t suoffset;
  unsigned coeff;
  char *pbuf = ppda->bufPtr;
  char *qbuf = qpda->bufPtr;
  char *buf;
  int delta;

  suoffset = rf_StripeUnitOffset(layoutPtr, pda->startSector);
  len = pda->numSector;
  /* see if pda intersects a recovery pda */
  if ((suoffset < s0off+s0len) && ( suoffset+len > s0off))
    {
      buf = pda->bufPtr;
      coeff = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),pda->raidAddress);
      coeff = (coeff % raidPtr->Layout.numDataCol);

      if (suoffset < s0off)
	{
	  delta = s0off - suoffset;
	  buf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),delta);
	  suoffset = s0off;
	  len -= delta;
	}
      if (suoffset > s0off)
	{
	  delta = suoffset - s0off;
	  pbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),delta);
	  qbuf += rf_RaidAddressToStripeUnitID(&(raidPtr->Layout),delta);
a164 9
      if ((suoffset + len) > (s0len + s0off))
	len = s0len + s0off - suoffset;

      /* src, dest, len */
      rf_bxor(buf,pbuf,rf_RaidAddressToByte(raidPtr,len), bp);
	  
      /* dest, src, len, coeff */
      rf_IncQ((unsigned long *)qbuf,(unsigned long *)buf,rf_RaidAddressToByte(raidPtr,len),coeff);
    }
d174 1
a174 1
   
d186 3
a188 2
int rf_PQDoubleRecoveryFunc(node)
  RF_DagNode_t  *node;
d190 30
a219 70
  int np = node->numParams;
  RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *) node->params[np-1].p;
  RF_Raid_t *raidPtr = (RF_Raid_t *) node->params[np-2].p;
  RF_RaidLayout_t *layoutPtr = (RF_RaidLayout_t *) &(raidPtr->Layout);
  int d, i;
  unsigned coeff;
  RF_RaidAddr_t sosAddr, suoffset;
  RF_SectorCount_t len, secPerSU = layoutPtr->sectorsPerStripeUnit;
  int two = 0;
  RF_PhysDiskAddr_t *ppda,*ppda2,*qpda,*qpda2,*pda,npda;
  char *buf;
  int numDataCol = layoutPtr->numDataCol;
  RF_Etimer_t timer;
  RF_AccTraceEntry_t *tracerec = node->dagHdr->tracerec;

  RF_ETIMER_START(timer);

  if (asmap->failedPDAs[1] && 
      (asmap->failedPDAs[1]->numSector + asmap->failedPDAs[0]->numSector < secPerSU))
    {
      RF_ASSERT(0);
      ppda  = node->params[np-6].p;
      ppda2 = node->params[np-5].p;
      qpda  = node->params[np-4].p;
      qpda2 = node->params[np-3].p;
      d = (np-6);
      two = 1;
    }
  else
    {
      ppda = node->params[np-4].p;
      qpda = node->params[np-3].p;
      d = (np-4);
    }

  for (i=0; i < d; i++)
    {
      pda = node->params[i].p;
      buf = pda->bufPtr;
      suoffset = rf_StripeUnitOffset(layoutPtr, pda->startSector);
      len = pda->numSector;
      coeff = rf_RaidAddressToStripeUnitID(layoutPtr,pda->raidAddress);
      /* compute the data unit offset within the column */
      coeff = (coeff % raidPtr->Layout.numDataCol);
      /* see if pda intersects a recovery pda */
      applyPDA(raidPtr,pda,ppda,qpda,node->dagHdr->bp);
      if (two)
	applyPDA(raidPtr,pda,ppda,qpda,node->dagHdr->bp);
    }

  /* ok, we got the parity back to the point where we can recover.
     We now need to determine the coeff of the columns that need to be
     recovered. We can also only need to recover a single stripe unit.
     */
  
  if (asmap->failedPDAs[1] == NULL)
    { /* only a single stripe unit to recover. */
      pda = asmap->failedPDAs[0];
      sosAddr      = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);
      /* need to determine the column of the other failed disk */
      coeff = rf_RaidAddressToStripeUnitID(layoutPtr,pda->raidAddress);
      /* compute the data unit offset within the column */
      coeff = (coeff % raidPtr->Layout.numDataCol);
      for (i=0; i < numDataCol; i++)
	{
	  npda.raidAddress = sosAddr + (i * secPerSU);
	  (raidPtr->Layout.map->MapSector)(raidPtr,npda.raidAddress, &(npda.row), &(npda.col), &(npda.startSector), 0);
	  /* skip over dead disks */
	  if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col].status))
	    if (i != coeff) break;
d221 52
a272 18
      RF_ASSERT (i < numDataCol);
      RF_ASSERT (two==0);
      /* recover the data. Since we need only want to recover one column, we overwrite the
	 parity with the other one. */
      if (coeff < i) /* recovering 'a' */
	rf_PQ_recover((unsigned long *)ppda->bufPtr,(unsigned long *)qpda->bufPtr,(unsigned long *)pda->bufPtr,(unsigned long *)ppda->bufPtr,rf_RaidAddressToByte(raidPtr,pda->numSector), coeff, i);
      else /* recovering 'b' */
	rf_PQ_recover((unsigned long *)ppda->bufPtr,(unsigned long *)qpda->bufPtr,(unsigned long *)ppda->bufPtr,(unsigned long *)pda->bufPtr,rf_RaidAddressToByte(raidPtr,pda->numSector), i, coeff);
    }
  else
    RF_PANIC();

  RF_ETIMER_STOP(timer);
  RF_ETIMER_EVAL(timer);
  if (tracerec)
    tracerec->q_us += RF_ETIMER_VAL_US(timer);
  rf_GenericWakeupFunc(node,0);
  return(0);
d275 3
a277 2
int rf_PQWriteDoubleRecoveryFunc(node)
  RF_DagNode_t  *node;
d279 88
a366 1
  /* The situation:
d368 2
a369 93
         We are doing a write that hits only one
	 failed data unit.
	 The other failed data unit is not being overwritten, so
	 we need to generate it.
	 
	 For the moment, we assume all the nonfailed data being
	 written is in the shadow of the failed data unit.
	 (i.e,, either a single data unit write or the entire
	 failed stripe unit is being overwritten. )
	 
	 Recovery strategy: 
	     apply the recovery data to the parity and q.
	     Use P & Q to recover the second failed data unit in P.
	     Zero fill Q, then apply the recovered data to p.
	     Then apply the data being written to the failed drive.
	     Then walk through the surviving drives, applying new data
	     when it exists, othewise the recovery data. Quite a mess.


	The params

	read pda0, read pda1, ... read pda (numDataCol-3), 
	write pda0, ... , write pda (numStripeUnitAccess - numDataFailed),
	failed pda, raidPtr, asmap
   */

  int np = node->numParams;
  RF_AccessStripeMap_t *asmap = (RF_AccessStripeMap_t *) node->params[np-1].p;
  RF_Raid_t *raidPtr = (RF_Raid_t *) node->params[np-2].p;
  RF_RaidLayout_t *layoutPtr = (RF_RaidLayout_t *) &(raidPtr->Layout);
  int i;
  RF_RaidAddr_t sosAddr;
  unsigned coeff;
  RF_StripeCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;
  RF_PhysDiskAddr_t *ppda,*qpda,*pda,npda;
  int numDataCol = layoutPtr->numDataCol;
  RF_Etimer_t timer;
  RF_AccTraceEntry_t *tracerec = node->dagHdr->tracerec;

  RF_ASSERT(node->numResults == 2);
  RF_ASSERT(asmap->failedPDAs[1] == NULL);
  RF_ETIMER_START(timer);
  ppda = node->results[0];
  qpda = node->results[1];
  /* apply the recovery data */
  for (i=0; i < numDataCol-2; i++)
    applyPDA(raidPtr,node->params[i].p,ppda,qpda, node->dagHdr->bp);

  /* determine the other failed data unit */
  pda = asmap->failedPDAs[0];
  sosAddr      = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);
  /* need to determine the column of the other failed disk */
  coeff = rf_RaidAddressToStripeUnitID(layoutPtr,pda->raidAddress);
  /* compute the data unit offset within the column */
  coeff = (coeff % raidPtr->Layout.numDataCol);
  for (i=0; i < numDataCol; i++)
    {
      npda.raidAddress = sosAddr + (i * secPerSU);
      (raidPtr->Layout.map->MapSector)(raidPtr,npda.raidAddress, &(npda.row), &(npda.col), &(npda.startSector), 0);
      /* skip over dead disks */
      if (RF_DEAD_DISK(raidPtr->Disks[npda.row][npda.col].status))
	if (i != coeff) break;
    }
  RF_ASSERT (i < numDataCol);
  /* recover the data. The column we want to recover we write over the parity.
     The column we don't care about we dump in q. */
  if (coeff < i) /* recovering 'a' */
    rf_PQ_recover((unsigned long *)ppda->bufPtr,(unsigned long *)qpda->bufPtr,(unsigned long *)ppda->bufPtr,(unsigned long *)qpda->bufPtr,rf_RaidAddressToByte(raidPtr,pda->numSector), coeff, i);
  else /* recovering 'b' */
    rf_PQ_recover((unsigned long *)ppda->bufPtr,(unsigned long *)qpda->bufPtr,(unsigned long *)qpda->bufPtr,(unsigned long *)ppda->bufPtr,rf_RaidAddressToByte(raidPtr,pda->numSector), i, coeff);
  
  /* OK. The valid data is in P. Zero fill Q, then inc it into it. */
  bzero(qpda->bufPtr,rf_RaidAddressToByte(raidPtr,qpda->numSector));
  rf_IncQ((unsigned long *)qpda->bufPtr,(unsigned long *)ppda->bufPtr,rf_RaidAddressToByte(raidPtr,qpda->numSector),i);

  /* now apply all the write data to the buffer */
  /* single stripe unit write case: the failed data is only thing we are writing. */
  RF_ASSERT(asmap->numStripeUnitsAccessed == 1);
  /* dest, src, len, coeff */
  rf_IncQ((unsigned long *)qpda->bufPtr,(unsigned long *)asmap->failedPDAs[0]->bufPtr,rf_RaidAddressToByte(raidPtr,qpda->numSector),coeff);
  rf_bxor(asmap->failedPDAs[0]->bufPtr,ppda->bufPtr,rf_RaidAddressToByte(raidPtr,ppda->numSector),node->dagHdr->bp);
  
  /* now apply all the recovery data */
  for (i=0; i < numDataCol-2; i++)
    applyPDA(raidPtr,node->params[i].p,ppda,qpda, node->dagHdr->bp);

  RF_ETIMER_STOP(timer);
  RF_ETIMER_EVAL(timer);
  if (tracerec)
    tracerec->q_us += RF_ETIMER_VAL_US(timer);
	
  rf_GenericWakeupFunc(node,0);
  return(0);
a370 1

d373 1
a373 1
  RF_PANIC();
a374 1

d380 1
a380 1
   (1) The write completely covers the two lost data units. 
d391 1
a391 1
       (but touches at least one of them). Then we need to do the 
d393 2
a394 2
       unit from the other stripe. 
     
d403 20
a422 24
  RF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);
  RF_SectorCount_t sectorsPerSU = layoutPtr->sectorsPerStripeUnit;
  int sum;
  int nf = asmap->numDataFailed;

  sum = asmap->failedPDAs[0]->numSector;
  if (nf == 2)
    sum += asmap->failedPDAs[1]->numSector;

  if ((nf == 2) && ( sum == (2*sectorsPerSU)))
    {
      /* large write case */
      rf_PQ_DDLargeWrite(raidPtr, asmap, dag_h, bp, flags, allocList);
      return;
    }

  
  if ((nf == asmap->numStripeUnitsAccessed) || (sum >= sectorsPerSU))
    {
      /* small write case, no user data not in shadow */
      rf_PQ_DDSimpleSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList);
      return;
    }
  RF_PANIC();
a423 1

d426 1
a426 1
  rf_DoubleDegSmallWrite(raidPtr, asmap, dag_h, bp, flags, allocList, "Rq", "Wq", "PQ Recovery", rf_PQWriteDoubleRecoveryFunc);
d428 2
a429 2

#endif /* (RF_INCLUDE_DECL_PQ > 0) || (RF_INCLUDE_RAID6 > 0) */
@

