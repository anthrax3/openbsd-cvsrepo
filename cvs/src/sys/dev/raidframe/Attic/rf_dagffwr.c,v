head	1.6;
access;
symbols
	OPENBSD_5_1_BASE:1.5
	OPENBSD_5_1:1.5.0.38
	OPENBSD_5_0:1.5.0.36
	OPENBSD_5_0_BASE:1.5
	OPENBSD_4_9:1.5.0.34
	OPENBSD_4_9_BASE:1.5
	OPENBSD_4_8:1.5.0.32
	OPENBSD_4_8_BASE:1.5
	OPENBSD_4_7:1.5.0.28
	OPENBSD_4_7_BASE:1.5
	OPENBSD_4_6:1.5.0.30
	OPENBSD_4_6_BASE:1.5
	OPENBSD_4_5:1.5.0.26
	OPENBSD_4_5_BASE:1.5
	OPENBSD_4_4:1.5.0.24
	OPENBSD_4_4_BASE:1.5
	OPENBSD_4_3:1.5.0.22
	OPENBSD_4_3_BASE:1.5
	OPENBSD_4_2:1.5.0.20
	OPENBSD_4_2_BASE:1.5
	OPENBSD_4_1:1.5.0.18
	OPENBSD_4_1_BASE:1.5
	OPENBSD_4_0:1.5.0.16
	OPENBSD_4_0_BASE:1.5
	OPENBSD_3_9:1.5.0.14
	OPENBSD_3_9_BASE:1.5
	OPENBSD_3_8:1.5.0.12
	OPENBSD_3_8_BASE:1.5
	OPENBSD_3_7:1.5.0.10
	OPENBSD_3_7_BASE:1.5
	OPENBSD_3_6:1.5.0.8
	OPENBSD_3_6_BASE:1.5
	SMP_SYNC_A:1.5
	SMP_SYNC_B:1.5
	OPENBSD_3_5:1.5.0.6
	OPENBSD_3_5_BASE:1.5
	OPENBSD_3_4:1.5.0.4
	OPENBSD_3_4_BASE:1.5
	UBC_SYNC_A:1.5
	OPENBSD_3_3:1.5.0.2
	OPENBSD_3_3_BASE:1.5
	OPENBSD_3_2:1.4.0.16
	OPENBSD_3_2_BASE:1.4
	OPENBSD_3_1:1.4.0.14
	OPENBSD_3_1_BASE:1.4
	UBC_SYNC_B:1.4
	UBC:1.4.0.12
	UBC_BASE:1.4
	OPENBSD_3_0:1.4.0.10
	OPENBSD_3_0_BASE:1.4
	OPENBSD_2_9_BASE:1.4
	OPENBSD_2_9:1.4.0.8
	OPENBSD_2_8:1.4.0.6
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.4
	OPENBSD_2_7_BASE:1.4
	SMP:1.4.0.2
	SMP_BASE:1.4
	kame_19991208:1.2
	OPENBSD_2_6:1.2.0.4
	OPENBSD_2_6_BASE:1.2
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.6
date	2012.04.06.15.53.58;	author jsing;	state dead;
branches;
next	1.5;

1.5
date	2002.12.16.07.01.03;	author tdeval;	state Exp;
branches;
next	1.4;

1.4
date	2000.01.11.18.02.21;	author peter;	state Exp;
branches
	1.4.2.1
	1.4.12.1;
next	1.3;

1.3
date	2000.01.07.14.50.20;	author peter;	state Exp;
branches;
next	1.2;

1.2
date	99.02.16.00.02.31;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	99.01.11.14.29.09;	author niklas;	state Exp;
branches;
next	;

1.4.2.1
date	2003.03.28.00.38.27;	author niklas;	state Exp;
branches;
next	;

1.4.12.1
date	2003.05.19.22.21.51;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.6
log
@Put raidframe in the attic.
@
text
@/*	$OpenBSD: rf_dagffwr.c,v 1.5 2002/12/16 07:01:03 tdeval Exp $	*/
/*	$NetBSD: rf_dagffwr.c,v 1.5 2000/01/07 03:40:58 oster Exp $	*/

/*
 * Copyright (c) 1995 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Author: Mark Holland, Daniel Stodolsky, William V. Courtright II
 *
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 *
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * rf_dagff.c
 *
 * Code for creating fault-free DAGs.
 *
 */

#include "rf_types.h"
#include "rf_raid.h"
#include "rf_dag.h"
#include "rf_dagutils.h"
#include "rf_dagfuncs.h"
#include "rf_debugMem.h"
#include "rf_dagffrd.h"
#include "rf_memchunk.h"
#include "rf_general.h"
#include "rf_dagffwr.h"

/*****************************************************************************
 *
 * General comments on DAG creation:
 *
 * All DAGs in this file use roll-away error recovery. Each DAG has a single
 * commit node, usually called "Cmt."  If an error occurs before the Cmt node
 * is reached, the execution engine will halt forward execution and work
 * backward through the graph, executing the undo functions. Assuming that
 * each node in the graph prior to the Cmt node are undoable and atomic - or -
 * does not make changes to permanent state, the graph will fail atomically.
 * If an error occurs after the Cmt node executes, the engine will roll-forward
 * through the graph, blindly executing nodes until it reaches the end.
 * If a graph reaches the end, it is assumed to have completed successfully.
 *
 * A graph has only 1 Cmt node.
 *
 *****************************************************************************/


/*****************************************************************************
 *
 * The following wrappers map the standard DAG creation interface to the
 * DAG creation routines. Additionally, these wrappers enable experimentation
 * with new DAG structures by providing an extra level of indirection, allowing
 * the DAG creation routines to be replaced at this single point.
 *
 *****************************************************************************/


void
rf_CreateNonRedundantWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_IoType_t type)
{
	rf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
	    RF_IO_TYPE_WRITE);
}

void
rf_CreateRAID0WriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_IoType_t type)
{
	rf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
	    RF_IO_TYPE_WRITE);
}

void
rf_CreateSmallWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
{
	/* "normal" rollaway. */
	rf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	    allocList, &rf_xorFuncs, NULL);
}

void
rf_CreateLargeWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
{
	/* "normal" rollaway. */
	rf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	    allocList, 1, rf_RegularXorFunc, RF_TRUE);
}


/*****************************************************************************
 *
 * DAG creation code begins here.
 *
 *****************************************************************************/


/*****************************************************************************
 *
 * creates a DAG to perform a large-write operation:
 *
 *           / Rod \           / Wnd \
 * H -- block- Rod - Xor - Cmt - Wnd --- T
 *           \ Rod /          \  Wnp /
 *                             \[Wnq]/
 *
 * The XOR node also does the Q calculation in the P+Q architecture.
 * All nodes that are before the commit node (Cmt) are assumed to be atomic
 * and undoable - or - they make no changes to permanent state.
 *
 * Rod = read old data
 * Cmt = commit node
 * Wnp = write new parity
 * Wnd = write new data
 * Wnq = write new "q"
 * [] denotes optional segments in the graph.
 *
 * Parameters:  raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		nfaults	  - number of faults array can tolerate
 *			    (equal to # redundancy units in stripe)
 *		redfuncs  - list of redundancy generating functions
 *
 *****************************************************************************/

void
rf_CommonCreateLargeWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, int nfaults, int (*redFunc) (RF_DagNode_t *),
    int allowBufferRecycle)
{
	RF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;
	RF_DagNode_t *wnqNode, *blockNode, *commitNode, *termNode;
	int nWndNodes, nRodNodes, i, nodeNum, asmNum;
	RF_AccessStripeMapHeader_t *new_asm_h[2];
	RF_StripeNum_t parityStripeID;
	char *sosBuffer, *eosBuffer;
	RF_ReconUnitNum_t which_ru;
	RF_RaidLayout_t *layoutPtr;
	RF_PhysDiskAddr_t *pda;

	layoutPtr = &(raidPtr->Layout);
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,
	    asmap->raidAddress, &which_ru);

	if (rf_dagDebug) {
		printf("[Creating large-write DAG]\n");
	}
	dag_h->creator = "LargeWriteDAG";

	dag_h->numCommitNodes = 1;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	/* Alloc the nodes: Wnd, xor, commit, block, term, and  Wnp. */
	nWndNodes = asmap->numStripeUnitsAccessed;
	RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
	i = 0;
	wndNodes = &nodes[i];
	i += nWndNodes;
	xorNode = &nodes[i];
	i += 1;
	wnpNode = &nodes[i];
	i += 1;
	blockNode = &nodes[i];
	i += 1;
	commitNode = &nodes[i];
	i += 1;
	termNode = &nodes[i];
	i += 1;
	if (nfaults == 2) {
		wnqNode = &nodes[i];
		i += 1;
	} else {
		wnqNode = NULL;
	}
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h,
	    new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
	if (nRodNodes > 0) {
		RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),
		    (RF_DagNode_t *), allocList);
	} else {
		rodNodes = NULL;
	}

	/* Begin node initialization. */
	if (nRodNodes > 0) {
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h,
		    "Nil", allocList);
	} else {
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil",
		    allocList);
	}

	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nWndNodes + nfaults, 1, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0,
	    dag_h, "Trm", allocList);

	/* Initialize the Rod nodes. */
	for (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {
		if (new_asm_h[asmNum]) {
			pda = new_asm_h[asmNum]->stripeMap->physInfo;
			while (pda) {
				rf_InitNode(&rodNodes[nodeNum], rf_wait,
				    RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
				    1, 1, 4, 0, dag_h, "Rod", allocList);
				rodNodes[nodeNum].params[0].p = pda;
				rodNodes[nodeNum].params[1].p = pda->bufPtr;
				rodNodes[nodeNum].params[2].v = parityStripeID;
				rodNodes[nodeNum].params[3].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, 0, which_ru);
				nodeNum++;
				pda = pda->next;
			}
		}
	}
	RF_ASSERT(nodeNum == nRodNodes);

	/* Initialize the wnd nodes. */
	pda = asmap->physInfo;
	for (i = 0; i < nWndNodes; i++) {
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
		RF_ASSERT(pda != NULL);
		wndNodes[i].params[0].p = pda;
		wndNodes[i].params[1].p = pda->bufPtr;
		wndNodes[i].params[2].v = parityStripeID;
		wndNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		pda = pda->next;
	}

	/* Initialize the redundancy node. */
	if (nRodNodes > 0) {
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc,
		    rf_NullNodeUndoFunc, NULL, 1, nRodNodes,
		    2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
		    "Xr ", allocList);
	} else {
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 1,
		    2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
		    "Xr ", allocList);
	}
	xorNode->flags |= RF_DAGNODE_FLAG_YIELD;
	for (i = 0; i < nWndNodes; i++) {
		xorNode->params[2 * i + 0] =
		    wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] =
		    wndNodes[i].params[1];	/* buf ptr */
	}
	for (i = 0; i < nRodNodes; i++) {
		xorNode->params[2 * (nWndNodes + i) + 0] =
		    rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] =
		    rodNodes[i].params[1];	/* buf ptr */
	}
	/* Xor node needs to get at RAID information. */
	xorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;

	/*
	 * Look for an Rod node that reads a complete SU. If none, alloc
	 * a buffer to receive the parity info. Note that we can't use a
	 * new data buffer because it will not have gotten written when
	 * the xor occurs.
	 */
	if (allowBufferRecycle) {
		for (i = 0; i < nRodNodes; i++) {
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)
			    ->numSector == raidPtr->Layout.sectorsPerStripeUnit)
				break;
		}
	}
	if ((!allowBufferRecycle) || (i == nRodNodes)) {
		RF_CallocAndAdd(xorNode->results[0], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
	} else {
		xorNode->results[0] = rodNodes[i].params[1].p;
	}

	/* Initialize the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
	wnpNode->params[0].p = asmap->parityInfo;
	wnpNode->params[1].p = xorNode->results[0];
	wnpNode->params[2].v = parityStripeID;
	wnpNode->params[3].v =
	    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit. */
	RF_ASSERT(asmap->parityInfo->next == NULL);

	if (nfaults == 2) {
		/*
		 * We never try to recycle a buffer for the Q calculation
		 * in addition to the parity. This would cause two buffers
		 * to get smashed during the P and Q calculation, guaranteeing
		 * one would be wrong.
		 */
		RF_CallocAndAdd(xorNode->results[1], 1,
		    rf_RaidAddressToByte(raidPtr,
		     raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnq", allocList);
		wnqNode->params[0].p = asmap->qInfo;
		wnqNode->params[1].p = xorNode->results[1];
		wnqNode->params[2].v = parityStripeID;
		wnqNode->params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit. */
		RF_ASSERT(asmap->parityInfo->next == NULL);
	}
	/*
	 * Connect nodes to form graph.
	 */

	/* Connect dag header to block node. */
	RF_ASSERT(blockNode->numAntecedents == 0);
	dag_h->succedents[0] = blockNode;

	if (nRodNodes > 0) {
		/* Connect the block node to the Rod nodes. */
		RF_ASSERT(blockNode->numSuccedents == nRodNodes);
		RF_ASSERT(xorNode->numAntecedents == nRodNodes);
		for (i = 0; i < nRodNodes; i++) {
			RF_ASSERT(rodNodes[i].numAntecedents == 1);
			blockNode->succedents[i] = &rodNodes[i];
			rodNodes[i].antecedents[0] = blockNode;
			rodNodes[i].antType[0] = rf_control;

			/* Connect the Rod nodes to the Xor node. */
			RF_ASSERT(rodNodes[i].numSuccedents == 1);
			rodNodes[i].succedents[0] = xorNode;
			xorNode->antecedents[i] = &rodNodes[i];
			xorNode->antType[i] = rf_trueData;
		}
	} else {
		/* Connect the block node to the Xor node. */
		RF_ASSERT(blockNode->numSuccedents == 1);
		RF_ASSERT(xorNode->numAntecedents == 1);
		blockNode->succedents[0] = xorNode;
		xorNode->antecedents[0] = blockNode;
		xorNode->antType[0] = rf_control;
	}

	/* Connect the xor node to the commit node. */
	RF_ASSERT(xorNode->numSuccedents == 1);
	RF_ASSERT(commitNode->numAntecedents == 1);
	xorNode->succedents[0] = commitNode;
	commitNode->antecedents[0] = xorNode;
	commitNode->antType[0] = rf_control;

	/* Connect the commit node to the write nodes. */
	RF_ASSERT(commitNode->numSuccedents == nWndNodes + nfaults);
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNodes->numAntecedents == 1);
		commitNode->succedents[i] = &wndNodes[i];
		wndNodes[i].antecedents[0] = commitNode;
		wndNodes[i].antType[0] = rf_control;
	}
	RF_ASSERT(wnpNode->numAntecedents == 1);
	commitNode->succedents[nWndNodes] = wnpNode;
	wnpNode->antecedents[0] = commitNode;
	wnpNode->antType[0] = rf_trueData;
	if (nfaults == 2) {
		RF_ASSERT(wnqNode->numAntecedents == 1);
		commitNode->succedents[nWndNodes + 1] = wnqNode;
		wnqNode->antecedents[0] = commitNode;
		wnqNode->antType[0] = rf_trueData;
	}
	/* Connect the write nodes to the term node. */
	RF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);
	RF_ASSERT(termNode->numSuccedents == 0);
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNodes->numSuccedents == 1);
		wndNodes[i].succedents[0] = termNode;
		termNode->antecedents[i] = &wndNodes[i];
		termNode->antType[i] = rf_control;
	}
	RF_ASSERT(wnpNode->numSuccedents == 1);
	wnpNode->succedents[0] = termNode;
	termNode->antecedents[nWndNodes] = wnpNode;
	termNode->antType[nWndNodes] = rf_control;
	if (nfaults == 2) {
		RF_ASSERT(wnqNode->numSuccedents == 1);
		wnqNode->succedents[0] = termNode;
		termNode->antecedents[nWndNodes + 1] = wnqNode;
		termNode->antType[nWndNodes + 1] = rf_control;
	}
}
/*****************************************************************************
 *
 * Create a DAG to perform a small-write operation (either raid 5 or pq),
 * which is as follows:
 *
 * Hdr -> Nil -> Rop -> Xor -> Cmt ----> Wnp [Unp] --> Trm
 *            \- Rod X      /     \----> Wnd [Und]-/
 *           [\- Rod X     /       \---> Wnd [Und]-/]
 *           [\- Roq -> Q /         \--> Wnq [Unq]-/]
 *
 * Rop = read old parity
 * Rod = read old data
 * Roq = read old "q"
 * Cmt = commit node
 * Und = unlock data disk
 * Unp = unlock parity disk
 * Unq = unlock q disk
 * Wnp = write new parity
 * Wnd = write new data
 * Wnq = write new "q"
 * [ ] denotes optional segments in the graph.
 *
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		pfuncs	  - list of parity generating functions
 *		qfuncs	  - list of q generating functions
 *
 * A null qfuncs indicates single fault tolerant.
 *****************************************************************************/

void
rf_CommonCreateSmallWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_RedFuncs_t *pfuncs, RF_RedFuncs_t *qfuncs)
{
	RF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;
	RF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;
	RF_DagNode_t *xorNodes, *qNodes, *blockNode, *commitNode, *nodes;
	RF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;
	int i, j, nNodes, totalNumNodes, lu_flag;
	RF_ReconUnitNum_t which_ru;
	int (*func) (RF_DagNode_t *);
	int (*undoFunc) (RF_DagNode_t *);
	int (*qfunc) (RF_DagNode_t *);
	int numDataNodes, numParityNodes;
	RF_StripeNum_t parityStripeID;
	RF_PhysDiskAddr_t *pda;
	char *name, *qname;
	long nfaults;

	nfaults = qfuncs ? 2 : 1;
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* Lock/unlock flag. */

	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
	pda = asmap->physInfo;
	numDataNodes = asmap->numStripeUnitsAccessed;
	numParityNodes = (asmap->parityInfo->next) ? 2 : 1;

	if (rf_dagDebug) {
		printf("[Creating small-write DAG]\n");
	}
	RF_ASSERT(numDataNodes > 0);
	dag_h->creator = "SmallWriteDAG";

	dag_h->numCommitNodes = 1;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	/*
	 * DAG creation occurs in four steps:
	 * 1. Count the number of nodes in the DAG.
	 * 2. Create the nodes.
	 * 3. Initialize the nodes.
	 * 4. Connect the nodes.
	 */

	/*
	 * Step 1. Compute number of nodes in the graph.
	 */

	/*
	 * Number of nodes: a read and write for each data unit, a redundancy
	 * computation node for each parity node (nfaults * nparity), a read
	 * and write for each parity unit, a block and commit node (2), a
	 * terminate node if atomic RMW, an unlock node for each
	 * data/redundancy unit.
	 */
	totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)
	    + (nfaults * 2 * numParityNodes) + 3;
	if (lu_flag) {
		totalNumNodes += (numDataNodes + (nfaults * numParityNodes));
	}
	/*
	 * Step 2. Create the nodes.
	 */
	RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
	i = 0;
	blockNode = &nodes[i];
	i += 1;
	commitNode = &nodes[i];
	i += 1;
	readDataNodes = &nodes[i];
	i += numDataNodes;
	readParityNodes = &nodes[i];
	i += numParityNodes;
	writeDataNodes = &nodes[i];
	i += numDataNodes;
	writeParityNodes = &nodes[i];
	i += numParityNodes;
	xorNodes = &nodes[i];
	i += numParityNodes;
	termNode = &nodes[i];
	i += 1;
	if (lu_flag) {
		unlockDataNodes = &nodes[i];
		i += numDataNodes;
		unlockParityNodes = &nodes[i];
		i += numParityNodes;
	} else {
		unlockDataNodes = unlockParityNodes = NULL;
	}
	if (nfaults == 2) {
		readQNodes = &nodes[i];
		i += numParityNodes;
		writeQNodes = &nodes[i];
		i += numParityNodes;
		qNodes = &nodes[i];
		i += numParityNodes;
		if (lu_flag) {
			unlockQNodes = &nodes[i];
			i += numParityNodes;
		} else {
			unlockQNodes = NULL;
		}
	} else {
		readQNodes = writeQNodes = qNodes = unlockQNodes = NULL;
	}
	RF_ASSERT(i == totalNumNodes);

	/*
	 * Step 3. Initialize the nodes.
	 */
	/* Initialize block node (Nil). */
	nNodes = numDataNodes + (nfaults * numParityNodes);
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);

	/* Initialize commit node (Cmt). */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, (nfaults * numParityNodes),
	    0, 0, dag_h, "Cmt", allocList);

	/* Initialize terminate node (Trm). */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h,
	    "Trm", allocList);

	/* Initialize nodes which read old data (Rod). */
	for (i = 0; i < numDataNodes; i++) {
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    (nfaults * numParityNodes), 1, 4, 0, dag_h, "Rod",
		    allocList);
		RF_ASSERT(pda != NULL);
		/* Physical disk addr desc. */
		readDataNodes[i].params[0].p = pda;
		/* Buffer to hold old data. */
		readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
		    dag_h, pda, allocList);
		readDataNodes[i].params[2].v = parityStripeID;
		readDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
		pda = pda->next;
		for (j = 0; j < readDataNodes[i].numSuccedents; j++) {
			readDataNodes[i].propList[j] = NULL;
		}
	}

	/* Initialize nodes which read old parity (Rop). */
	pda = asmap->parityInfo;
	i = 0;
	for (i = 0; i < numParityNodes; i++) {
		RF_ASSERT(pda != NULL);
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
		readParityNodes[i].params[0].p = pda;
		/* Buffer to hold old parity. */
		readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
		    dag_h, pda, allocList);
		readParityNodes[i].params[2].v = parityStripeID;
		readParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
		pda = pda->next;
		for (j = 0; j < readParityNodes[i].numSuccedents; j++) {
			readParityNodes[i].propList[0] = NULL;
		}
	}

	/* Initialize nodes which read old Q (Roq). */
	if (nfaults == 2) {
		pda = asmap->qInfo;
		for (i = 0; i < numParityNodes; i++) {
			RF_ASSERT(pda != NULL);
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes,
			    1, 4, 0, dag_h, "Roq", allocList);
			readQNodes[i].params[0].p = pda;
			/* Buffer to hold old Q. */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
			    dag_h, pda, allocList);
			readQNodes[i].params[2].v = parityStripeID;
			readQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    lu_flag, 0, which_ru);
			pda = pda->next;
			for (j = 0; j < readQNodes[i].numSuccedents; j++) {
				readQNodes[i].propList[0] = NULL;
			}
		}
	}
	/* Initialize nodes which write new data (Wnd). */
	pda = asmap->physInfo;
	for (i = 0; i < numDataNodes; i++) {
		RF_ASSERT(pda != NULL);
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
		    "Wnd", allocList);
		/* Physical disk addr desc. */
		writeDataNodes[i].params[0].p = pda;
		/* Buffer holding new data to be written. */
		writeDataNodes[i].params[1].p = pda->bufPtr;
		writeDataNodes[i].params[2].v = parityStripeID;
		writeDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		if (lu_flag) {
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Und", allocList);
			/* Physical disk addr desc. */
			unlockDataNodes[i].params[0].p = pda;
			unlockDataNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
		}
		pda = pda->next;
	}

	/*
	 * Initialize nodes which compute new parity and Q.
	 */
	/*
	 * We use the simple XOR func in the double-XOR case, and when
	 * we're accessing only a portion of one stripe unit.
	 * The distinction between the two is that the regular XOR func
	 * assumes that the targbuf is a full SU in size, and examines
	 * the pda associated with the buffer to decide where within
	 * the buffer to XOR the data, whereas the simple XOR func just
	 * XORs the data into the start of the buffer.
	 */
	if ((numParityNodes == 2) || ((numDataNodes == 1) &&
	    (asmap->totalSectorsAccessed <
	     raidPtr->Layout.sectorsPerStripeUnit))) {
		func = pfuncs->simple;
		undoFunc = rf_NullNodeUndoFunc;
		name = pfuncs->SimpleName;
		if (qfuncs) {
			qfunc = qfuncs->simple;
			qname = qfuncs->SimpleName;
		} else {
			qfunc = NULL;
			qname = NULL;
		}
	} else {
		func = pfuncs->regular;
		undoFunc = rf_NullNodeUndoFunc;
		name = pfuncs->RegularName;
		if (qfuncs) {
			qfunc = qfuncs->regular;
			qname = qfuncs->RegularName;
		} else {
			qfunc = NULL;
			qname = NULL;
		}
	}
	/*
	 * Initialize the xor nodes: params are {pda,buf}.
	 * From {Rod,Wnd,Rop} nodes, and raidPtr.
	 */
	if (numParityNodes == 2) {
		/* Double-xor case. */
		for (i = 0; i < numParityNodes; i++) {
			/* Note: no wakeup func for xor. */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func,
			    undoFunc, NULL, 1, (numDataNodes + numParityNodes),
			    7, 1, dag_h, name, allocList);
			xorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;
			xorNodes[i].params[0] = readDataNodes[i].params[0];
			xorNodes[i].params[1] = readDataNodes[i].params[1];
			xorNodes[i].params[2] = readParityNodes[i].params[0];
			xorNodes[i].params[3] = readParityNodes[i].params[1];
			xorNodes[i].params[4] = writeDataNodes[i].params[0];
			xorNodes[i].params[5] = writeDataNodes[i].params[1];
			xorNodes[i].params[6].p = raidPtr;
			/* Use old parity buf as target buf. */
			xorNodes[i].results[0] = readParityNodes[i].params[1].p;
			if (nfaults == 2) {
				/* Note: no wakeup func for qor. */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE,
				    qfunc, undoFunc, NULL, 1,
				    (numDataNodes + numParityNodes), 7, 1,
				    dag_h, qname, allocList);
				qNodes[i].params[0] =
				    readDataNodes[i].params[0];
				qNodes[i].params[1] =
				    readDataNodes[i].params[1];
				qNodes[i].params[2] = readQNodes[i].params[0];
				qNodes[i].params[3] = readQNodes[i].params[1];
				qNodes[i].params[4] =
				    writeDataNodes[i].params[0];
				qNodes[i].params[5] =
				    writeDataNodes[i].params[1];
				qNodes[i].params[6].p = raidPtr;
				/* Use old Q buf as target buf. */
				qNodes[i].results[0] =
				    readQNodes[i].params[1].p;
			}
		}
	} else {
		/* There is only one xor node in this case. */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc,
		    NULL, 1, (numDataNodes + numParityNodes),
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1,
		    dag_h, name, allocList);
		xorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;
		for (i = 0; i < numDataNodes + 1; i++) {
			/* Set up params related to Rod and Rop nodes. */
			xorNodes[0].params[2 * i + 0] =
			    readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] =
			    readDataNodes[i].params[1];	/* buffer ptr */
		}
		for (i = 0; i < numDataNodes; i++) {
			/* Set up params related to Wnd and Wnp nodes. */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =
			    writeDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =
			    writeDataNodes[i].params[1];	/* buffer ptr */
		}
		/* Xor node needs to get at RAID information. */
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p =
		    raidPtr;
		xorNodes[0].results[0] = readParityNodes[0].params[1].p;
		if (nfaults == 2) {
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc,
			    undoFunc, NULL, 1, (numDataNodes + numParityNodes),
			    (2 * (numDataNodes + numDataNodes + 1) + 1), 1,
			    dag_h, qname, allocList);
			for (i = 0; i < numDataNodes; i++) {
				/* Set up params related to Rod. */
				qNodes[0].params[2 * i + 0] =
				    readDataNodes[i].params[0];	/* pda */
				qNodes[0].params[2 * i + 1] =
				    readDataNodes[i].params[1];	/* buffer ptr */
			}
			/* And read old q. */
			qNodes[0].params[2 * numDataNodes + 0] =
			    readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] =
			    readQNodes[0].params[1];	/* buffer ptr */
			for (i = 0; i < numDataNodes; i++) {
				/* Set up params related to Wnd nodes. */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 0] =
				    /* pda */
				    writeDataNodes[i].params[0];
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 1] =
				    /* buffer ptr */
				    writeDataNodes[i].params[1];
			}
			/* Xor node needs to get at RAID information. */
			qNodes[0].params
			    [2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;
			qNodes[0].results[0] = readQNodes[0].params[1].p;
		}
	}

	/* Initialize nodes which write new parity (Wnp). */
	pda = asmap->parityInfo;
	for (i = 0; i < numParityNodes; i++) {
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
		    "Wnp", allocList);
		RF_ASSERT(pda != NULL);
		/* Param 1 (bufPtr) filled in by xor node. */
		writeParityNodes[i].params[0].p = pda;
		/* Buffer pointer for parity write operation. */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];
		writeParityNodes[i].params[2].v = parityStripeID;
		writeParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		if (lu_flag) {
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Unp", allocList);
			/* Physical disk addr desc. */
			unlockParityNodes[i].params[0].p = pda;
			unlockParityNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
		}
		pda = pda->next;
	}

	/* Initialize nodes which write new Q (Wnq). */
	if (nfaults == 2) {
		pda = asmap->qInfo;
		for (i = 0; i < numParityNodes; i++) {
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wnq", allocList);
			RF_ASSERT(pda != NULL);
			/* Param 1 (bufPtr) filled in by xor node. */
			writeQNodes[i].params[0].p = pda;
			writeQNodes[i].params[1].p = qNodes[i].results[0];
			/* Buffer pointer for parity write operation. */
			writeQNodes[i].params[2].v = parityStripeID;
			writeQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
			if (lu_flag) {
				/* Initialize node to unlock the disk queue. */
				rf_InitNode(&unlockQNodes[i], rf_wait,
				    RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc,
				    rf_GenericWakeupFunc, 1, 1, 2, 0,
				    dag_h, "Unq", allocList);
				/* Physical disk addr desc. */
				unlockQNodes[i].params[0].p = pda;
				unlockQNodes[i].params[1].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, lu_flag, which_ru);
			}
			pda = pda->next;
		}
	}
	/*
	 * Step 4. Connect the nodes.
	 */

	/* Connect header to block node. */
	dag_h->succedents[0] = blockNode;

	/* Connect block node to read old data nodes. */
	RF_ASSERT(blockNode->numSuccedents ==
	    (numDataNodes + (numParityNodes * nfaults)));
	for (i = 0; i < numDataNodes; i++) {
		blockNode->succedents[i] = &readDataNodes[i];
		RF_ASSERT(readDataNodes[i].numAntecedents == 1);
		readDataNodes[i].antecedents[0] = blockNode;
		readDataNodes[i].antType[0] = rf_control;
	}

	/* Connect block node to read old parity nodes. */
	for (i = 0; i < numParityNodes; i++) {
		blockNode->succedents[numDataNodes + i] = &readParityNodes[i];
		RF_ASSERT(readParityNodes[i].numAntecedents == 1);
		readParityNodes[i].antecedents[0] = blockNode;
		readParityNodes[i].antType[0] = rf_control;
	}

	/* Connect block node to read old Q nodes. */
	if (nfaults == 2) {
		for (i = 0; i < numParityNodes; i++) {
			blockNode->succedents[numDataNodes + numParityNodes + i]
			    = &readQNodes[i];
			RF_ASSERT(readQNodes[i].numAntecedents == 1);
			readQNodes[i].antecedents[0] = blockNode;
			readQNodes[i].antType[0] = rf_control;
		}
	}
	/* Connect read old data nodes to xor nodes. */
	for (i = 0; i < numDataNodes; i++) {
		RF_ASSERT(readDataNodes[i].numSuccedents ==
		    (nfaults * numParityNodes));
		for (j = 0; j < numParityNodes; j++) {
			RF_ASSERT(xorNodes[j].numAntecedents ==
			    numDataNodes + numParityNodes);
			readDataNodes[i].succedents[j] = &xorNodes[j];
			xorNodes[j].antecedents[i] = &readDataNodes[i];
			xorNodes[j].antType[i] = rf_trueData;
		}
	}

	/* Connect read old data nodes to q nodes. */
	if (nfaults == 2) {
		for (i = 0; i < numDataNodes; i++) {
			for (j = 0; j < numParityNodes; j++) {
				RF_ASSERT(qNodes[j].numAntecedents ==
				    numDataNodes + numParityNodes);
				readDataNodes[i].succedents[numParityNodes + j]
				    = &qNodes[j];
				qNodes[j].antecedents[i] = &readDataNodes[i];
				qNodes[j].antType[i] = rf_trueData;
			}
		}
	}
	/* Connect read old parity nodes to xor nodes. */
	for (i = 0; i < numParityNodes; i++) {
		RF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);
		for (j = 0; j < numParityNodes; j++) {
			readParityNodes[i].succedents[j] = &xorNodes[j];
			xorNodes[j].antecedents[numDataNodes + i] =
			    &readParityNodes[i];
			xorNodes[j].antType[numDataNodes + i] = rf_trueData;
		}
	}

	/* Connect read old q nodes to q nodes. */
	if (nfaults == 2) {
		for (i = 0; i < numParityNodes; i++) {
			RF_ASSERT(readParityNodes[i].numSuccedents ==
			    numParityNodes);
			for (j = 0; j < numParityNodes; j++) {
				readQNodes[i].succedents[j] = &qNodes[j];
				qNodes[j].antecedents[numDataNodes + i] =
				    &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] =
				    rf_trueData;
			}
		}
	}
	/* Connect xor nodes to commit node. */
	RF_ASSERT(commitNode->numAntecedents == (nfaults * numParityNodes));
	for (i = 0; i < numParityNodes; i++) {
		RF_ASSERT(xorNodes[i].numSuccedents == 1);
		xorNodes[i].succedents[0] = commitNode;
		commitNode->antecedents[i] = &xorNodes[i];
		commitNode->antType[i] = rf_control;
	}

	/* Connect q nodes to commit node. */
	if (nfaults == 2) {
		for (i = 0; i < numParityNodes; i++) {
			RF_ASSERT(qNodes[i].numSuccedents == 1);
			qNodes[i].succedents[0] = commitNode;
			commitNode->antecedents[i + numParityNodes] =
			    &qNodes[i];
			commitNode->antType[i + numParityNodes] = rf_control;
		}
	}
	/* Connect commit node to write nodes. */
	RF_ASSERT(commitNode->numSuccedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
	for (i = 0; i < numDataNodes; i++) {
		RF_ASSERT(writeDataNodes[i].numAntecedents == 1);
		commitNode->succedents[i] = &writeDataNodes[i];
		writeDataNodes[i].antecedents[0] = commitNode;
		writeDataNodes[i].antType[0] = rf_trueData;
	}
	for (i = 0; i < numParityNodes; i++) {
		RF_ASSERT(writeParityNodes[i].numAntecedents == 1);
		commitNode->succedents[i + numDataNodes] = &writeParityNodes[i];
		writeParityNodes[i].antecedents[0] = commitNode;
		writeParityNodes[i].antType[0] = rf_trueData;
	}
	if (nfaults == 2) {
		for (i = 0; i < numParityNodes; i++) {
			RF_ASSERT(writeQNodes[i].numAntecedents == 1);
			commitNode->succedents
			    [i + numDataNodes + numParityNodes] =
			    &writeQNodes[i];
			writeQNodes[i].antecedents[0] = commitNode;
			writeQNodes[i].antType[0] = rf_trueData;
		}
	}
	RF_ASSERT(termNode->numAntecedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
	RF_ASSERT(termNode->numSuccedents == 0);
	for (i = 0; i < numDataNodes; i++) {
		if (lu_flag) {
			/* Connect write new data nodes to unlock nodes. */
			RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
			RF_ASSERT(unlockDataNodes[i].numAntecedents == 1);
			writeDataNodes[i].succedents[0] = &unlockDataNodes[i];
			unlockDataNodes[i].antecedents[0] = &writeDataNodes[i];
			unlockDataNodes[i].antType[0] = rf_control;

			/* Connect unlock nodes to term node. */
			RF_ASSERT(unlockDataNodes[i].numSuccedents == 1);
			unlockDataNodes[i].succedents[0] = termNode;
			termNode->antecedents[i] = &unlockDataNodes[i];
			termNode->antType[i] = rf_control;
		} else {
			/* Connect write new data nodes to term node. */
			RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
			RF_ASSERT(termNode->numAntecedents ==
			    (numDataNodes + (nfaults * numParityNodes)));
			writeDataNodes[i].succedents[0] = termNode;
			termNode->antecedents[i] = &writeDataNodes[i];
			termNode->antType[i] = rf_control;
		}
	}

	for (i = 0; i < numParityNodes; i++) {
		if (lu_flag) {
			/* Connect write new parity nodes to unlock nodes. */
			RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
			RF_ASSERT(unlockParityNodes[i].numAntecedents == 1);
			writeParityNodes[i].succedents[0] =
			    &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] =
			    &writeParityNodes[i];
			unlockParityNodes[i].antType[0] = rf_control;

			/* Connect unlock nodes to term node. */
			RF_ASSERT(unlockParityNodes[i].numSuccedents == 1);
			unlockParityNodes[i].succedents[0] = termNode;
			termNode->antecedents[numDataNodes + i] =
			    &unlockParityNodes[i];
			termNode->antType[numDataNodes + i] = rf_control;
		} else {
			RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
			writeParityNodes[i].succedents[0] = termNode;
			termNode->antecedents[numDataNodes + i] =
			    &writeParityNodes[i];
			termNode->antType[numDataNodes + i] = rf_control;
		}
	}

	if (nfaults == 2) {
		for (i = 0; i < numParityNodes; i++) {
			if (lu_flag) {
				/* Connect write new Q nodes to unlock nodes. */
				RF_ASSERT(writeQNodes[i].numSuccedents == 1);
				RF_ASSERT(unlockQNodes[i].numAntecedents == 1);
				writeQNodes[i].succedents[0] = &unlockQNodes[i];
				unlockQNodes[i].antecedents[0] =
				    &writeQNodes[i];
				unlockQNodes[i].antType[0] = rf_control;

				/* Connect unlock nodes to unblock node. */
				RF_ASSERT(unlockQNodes[i].numSuccedents == 1);
				unlockQNodes[i].succedents[0] = termNode;
				termNode->antecedents
				    [numDataNodes + numParityNodes + i] =
				    &unlockQNodes[i];
				termNode->antType
				    [numDataNodes + numParityNodes + i] =
				    rf_control;
			} else {
				RF_ASSERT(writeQNodes[i].numSuccedents == 1);
				writeQNodes[i].succedents[0] = termNode;
				termNode->antecedents
				    [numDataNodes + numParityNodes + i] =
				    &writeQNodes[i];
				termNode->antType
				    [numDataNodes + numParityNodes + i] =
				    rf_control;
			}
		}
	}
}


/*****************************************************************************
 * Create a write graph (fault-free or degraded) for RAID level 1.
 *
 * Hdr -> Commit -> Wpd -> Nil -> Trm
 *		 -> Wsd ->
 *
 * The "Wpd" node writes data to the primary copy in the mirror pair.
 * The "Wsd" node writes data to the secondary copy in the mirror pair.
 *
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *****************************************************************************/

void
rf_CreateRaidOneWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
{
	RF_DagNode_t *unblockNode, *termNode, *commitNode;
	RF_DagNode_t *nodes, *wndNode, *wmirNode;
	int nWndNodes, nWmirNodes, i;
	RF_ReconUnitNum_t which_ru;
	RF_PhysDiskAddr_t *pda, *pdaP;
	RF_StripeNum_t parityStripeID;

	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
	if (rf_dagDebug) {
		printf("[Creating RAID level 1 write DAG]\n");
	}
	dag_h->creator = "RaidOneWriteDAG";

	/* 2 implies access not SU aligned. */
	nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;
	nWndNodes = (asmap->physInfo->next) ? 2 : 1;

	/* Alloc the Wnd nodes and the Wmir node. */
	if (asmap->numDataFailed == 1)
		nWndNodes--;
	if (asmap->numParityFailed == 1)
		nWmirNodes--;

	/*
	 * Total number of nodes = nWndNodes + nWmirNodes
	 * + (commit + unblock + terminator)
	 */
	RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
	i = 0;
	wndNode = &nodes[i];
	i += nWndNodes;
	wmirNode = &nodes[i];
	i += nWmirNodes;
	commitNode = &nodes[i];
	i += 1;
	unblockNode = &nodes[i];
	i += 1;
	termNode = &nodes[i];
	i += 1;
	RF_ASSERT(i == (nWndNodes + nWmirNodes + 3));

	/* This dag can commit immediately. */
	dag_h->numCommitNodes = 1;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	/* Initialize the commit, unblock, and term nodes. */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0,
	    dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);

	/* Initialize the wnd nodes. */
	if (nWndNodes > 0) {
		pda = asmap->physInfo;
		for (i = 0; i < nWndNodes; i++) {
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wpd", allocList);
			RF_ASSERT(pda != NULL);
			wndNode[i].params[0].p = pda;
			wndNode[i].params[1].p = pda->bufPtr;
			wndNode[i].params[2].v = parityStripeID;
			wndNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
			pda = pda->next;
		}
		RF_ASSERT(pda == NULL);
	}
	/* Initialize the mirror nodes. */
	if (nWmirNodes > 0) {
		pda = asmap->physInfo;
		pdaP = asmap->parityInfo;
		for (i = 0; i < nWmirNodes; i++) {
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wsd", allocList);
			RF_ASSERT(pda != NULL);
			wmirNode[i].params[0].p = pdaP;
			wmirNode[i].params[1].p = pda->bufPtr;
			wmirNode[i].params[2].v = parityStripeID;
			wmirNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
			pda = pda->next;
			pdaP = pdaP->next;
		}
		RF_ASSERT(pda == NULL);
		RF_ASSERT(pdaP == NULL);
	}
	/* Link the header node to the commit node. */
	RF_ASSERT(dag_h->numSuccedents == 1);
	RF_ASSERT(commitNode->numAntecedents == 0);
	dag_h->succedents[0] = commitNode;

	/* Link the commit node to the write nodes. */
	RF_ASSERT(commitNode->numSuccedents == (nWndNodes + nWmirNodes));
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNode[i].numAntecedents == 1);
		commitNode->succedents[i] = &wndNode[i];
		wndNode[i].antecedents[0] = commitNode;
		wndNode[i].antType[0] = rf_control;
	}
	for (i = 0; i < nWmirNodes; i++) {
		RF_ASSERT(wmirNode[i].numAntecedents == 1);
		commitNode->succedents[i + nWndNodes] = &wmirNode[i];
		wmirNode[i].antecedents[0] = commitNode;
		wmirNode[i].antType[0] = rf_control;
	}

	/* Link the write nodes to the unblock node. */
	RF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNode[i].numSuccedents == 1);
		wndNode[i].succedents[0] = unblockNode;
		unblockNode->antecedents[i] = &wndNode[i];
		unblockNode->antType[i] = rf_control;
	}
	for (i = 0; i < nWmirNodes; i++) {
		RF_ASSERT(wmirNode[i].numSuccedents == 1);
		wmirNode[i].succedents[0] = unblockNode;
		unblockNode->antecedents[i + nWndNodes] = &wmirNode[i];
		unblockNode->antType[i + nWndNodes] = rf_control;
	}

	/* Link the unblock node to the term node. */
	RF_ASSERT(unblockNode->numSuccedents == 1);
	RF_ASSERT(termNode->numAntecedents == 1);
	RF_ASSERT(termNode->numSuccedents == 0);
	unblockNode->succedents[0] = termNode;
	termNode->antecedents[0] = unblockNode;
	termNode->antType[0] = rf_control;
}



/*
 * DAGs that have no commit points.
 *
 * The following DAGs are used in forward and backward error recovery
 * experiments.
 * They are identical to the DAGs above this comment with the exception that
 * the commit points have been removed.
 */


void
rf_CommonCreateLargeWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, int nfaults, int (*redFunc) (RF_DagNode_t *),
    int allowBufferRecycle)
{
	RF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;
	RF_DagNode_t *wnqNode, *blockNode, *syncNode, *termNode;
	int nWndNodes, nRodNodes, i, nodeNum, asmNum;
	RF_AccessStripeMapHeader_t *new_asm_h[2];
	RF_StripeNum_t parityStripeID;
	char *sosBuffer, *eosBuffer;
	RF_ReconUnitNum_t which_ru;
	RF_RaidLayout_t *layoutPtr;
	RF_PhysDiskAddr_t *pda;

	layoutPtr = &(raidPtr->Layout);
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);

	if (rf_dagDebug)
		printf("[Creating large-write DAG]\n");
	dag_h->creator = "LargeWriteDAGFwd";

	dag_h->numCommitNodes = 0;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	/* Alloc the nodes: Wnd, xor, commit, block, term, and  Wnp. */
	nWndNodes = asmap->numStripeUnitsAccessed;
	RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
	i = 0;
	wndNodes = &nodes[i];
	i += nWndNodes;
	xorNode = &nodes[i];
	i += 1;
	wnpNode = &nodes[i];
	i += 1;
	blockNode = &nodes[i];
	i += 1;
	syncNode = &nodes[i];
	i += 1;
	termNode = &nodes[i];
	i += 1;
	if (nfaults == 2) {
		wnqNode = &nodes[i];
		i += 1;
	} else {
		wnqNode = NULL;
	}
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h,
	    new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
	if (nRodNodes > 0) {
		RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),
		    (RF_DagNode_t *), allocList);
	} else {
		rodNodes = NULL;
	}

	/* Begin node initialization. */
	if (nRodNodes > 0) {
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h,
		    "Nil", allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0,
		    dag_h, "Nil", allocList);
	} else {
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil",
		    allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h,
		    "Nil", allocList);
	}

	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0,
	    dag_h, "Trm", allocList);

	/* Initialize the Rod nodes. */
	for (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {
		if (new_asm_h[asmNum]) {
			pda = new_asm_h[asmNum]->stripeMap->physInfo;
			while (pda) {
				rf_InitNode(&rodNodes[nodeNum], rf_wait,
				    RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
				    1, 1, 4, 0, dag_h, "Rod", allocList);
				rodNodes[nodeNum].params[0].p = pda;
				rodNodes[nodeNum].params[1].p = pda->bufPtr;
				rodNodes[nodeNum].params[2].v = parityStripeID;
				rodNodes[nodeNum].params[3].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, 0, which_ru);
				nodeNum++;
				pda = pda->next;
			}
		}
	}
	RF_ASSERT(nodeNum == nRodNodes);

	/* Initialize the wnd nodes. */
	pda = asmap->physInfo;
	for (i = 0; i < nWndNodes; i++) {
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
		RF_ASSERT(pda != NULL);
		wndNodes[i].params[0].p = pda;
		wndNodes[i].params[1].p = pda->bufPtr;
		wndNodes[i].params[2].v = parityStripeID;
		wndNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		pda = pda->next;
	}

	/* Initialize the redundancy node. */
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
	    "Xr ", allocList);
	xorNode->flags |= RF_DAGNODE_FLAG_YIELD;
	for (i = 0; i < nWndNodes; i++) {
		xorNode->params[2 * i + 0] =
		    wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] =
		    wndNodes[i].params[1];	/* buf ptr */
	}
	for (i = 0; i < nRodNodes; i++) {
		xorNode->params[2 * (nWndNodes + i) + 0] =
		    rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] =
		    rodNodes[i].params[1];	/* buf ptr */
	}
	/* Xor node needs to get at RAID information. */
	xorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;

	/*
	 * Look for an Rod node that reads a complete SU. If none, alloc a
	 * buffer to receive the parity info. Note that we can't use a new
	 * data buffer because it will not have gotten written when the xor
	 * occurs.
	 */
	if (allowBufferRecycle) {
		for (i = 0; i < nRodNodes; i++)
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)
			    ->numSector == raidPtr->Layout.sectorsPerStripeUnit)
				break;
	}
	if ((!allowBufferRecycle) || (i == nRodNodes)) {
		RF_CallocAndAdd(xorNode->results[0], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
	} else
		xorNode->results[0] = rodNodes[i].params[1].p;

	/* Initialize the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
	wnpNode->params[0].p = asmap->parityInfo;
	wnpNode->params[1].p = xorNode->results[0];
	wnpNode->params[2].v = parityStripeID;
	wnpNode->params[3].v =
	    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit. */
	RF_ASSERT(asmap->parityInfo->next == NULL);

	if (nfaults == 2) {
		/*
		 * Never try to recycle a buffer for the Q calcuation in
		 * addition to the parity. This would cause two buffers to
		 * get smashed during the P and Q calculation, guaranteeing
		 * one would be wrong.
		 */
		RF_CallocAndAdd(xorNode->results[1], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnq", allocList);
		wnqNode->params[0].p = asmap->qInfo;
		wnqNode->params[1].p = xorNode->results[1];
		wnqNode->params[2].v = parityStripeID;
		wnqNode->params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit. */
		RF_ASSERT(asmap->parityInfo->next == NULL);
	}

	/* Connect nodes to form graph. */

	/* Connect dag header to block node. */
	RF_ASSERT(blockNode->numAntecedents == 0);
	dag_h->succedents[0] = blockNode;

	if (nRodNodes > 0) {
		/* Connect the block node to the Rod nodes. */
		RF_ASSERT(blockNode->numSuccedents == nRodNodes);
		RF_ASSERT(syncNode->numAntecedents == nRodNodes);
		for (i = 0; i < nRodNodes; i++) {
			RF_ASSERT(rodNodes[i].numAntecedents == 1);
			blockNode->succedents[i] = &rodNodes[i];
			rodNodes[i].antecedents[0] = blockNode;
			rodNodes[i].antType[0] = rf_control;

			/* Connect the Rod nodes to the Nil node. */
			RF_ASSERT(rodNodes[i].numSuccedents == 1);
			rodNodes[i].succedents[0] = syncNode;
			syncNode->antecedents[i] = &rodNodes[i];
			syncNode->antType[i] = rf_trueData;
		}
	} else {
		/* Connect the block node to the Nil node. */
		RF_ASSERT(blockNode->numSuccedents == 1);
		RF_ASSERT(syncNode->numAntecedents == 1);
		blockNode->succedents[0] = syncNode;
		syncNode->antecedents[0] = blockNode;
		syncNode->antType[0] = rf_control;
	}

	/* Connect the sync node to the Wnd nodes. */
	RF_ASSERT(syncNode->numSuccedents == (1 + nWndNodes));
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNodes->numAntecedents == 1);
		syncNode->succedents[i] = &wndNodes[i];
		wndNodes[i].antecedents[0] = syncNode;
		wndNodes[i].antType[0] = rf_control;
	}

	/* Connect the sync node to the Xor node. */
	RF_ASSERT(xorNode->numAntecedents == 1);
	syncNode->succedents[nWndNodes] = xorNode;
	xorNode->antecedents[0] = syncNode;
	xorNode->antType[0] = rf_control;

	/* Connect the xor node to the write parity node. */
	RF_ASSERT(xorNode->numSuccedents == nfaults);
	RF_ASSERT(wnpNode->numAntecedents == 1);
	xorNode->succedents[0] = wnpNode;
	wnpNode->antecedents[0] = xorNode;
	wnpNode->antType[0] = rf_trueData;
	if (nfaults == 2) {
		RF_ASSERT(wnqNode->numAntecedents == 1);
		xorNode->succedents[1] = wnqNode;
		wnqNode->antecedents[0] = xorNode;
		wnqNode->antType[0] = rf_trueData;
	}
	/* Connect the write nodes to the term node. */
	RF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);
	RF_ASSERT(termNode->numSuccedents == 0);
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNodes->numSuccedents == 1);
		wndNodes[i].succedents[0] = termNode;
		termNode->antecedents[i] = &wndNodes[i];
		termNode->antType[i] = rf_control;
	}
	RF_ASSERT(wnpNode->numSuccedents == 1);
	wnpNode->succedents[0] = termNode;
	termNode->antecedents[nWndNodes] = wnpNode;
	termNode->antType[nWndNodes] = rf_control;
	if (nfaults == 2) {
		RF_ASSERT(wnqNode->numSuccedents == 1);
		wnqNode->succedents[0] = termNode;
		termNode->antecedents[nWndNodes + 1] = wnqNode;
		termNode->antType[nWndNodes + 1] = rf_control;
	}
}


/*****************************************************************************
 *
 * Create a DAG to perform a small-write operation (either raid 5 or pq),
 * which is as follows:
 *
 * Hdr -> Nil -> Rop - Xor - Wnp [Unp] -- Trm
 *            \- Rod X- Wnd [Und] -------/
 *           [\- Rod X- Wnd [Und] ------/]
 *           [\- Roq - Q --> Wnq [Unq]-/]
 *
 * Rop = read old parity
 * Rod = read old data
 * Roq = read old "q"
 * Cmt = commit node
 * Und = unlock data disk
 * Unp = unlock parity disk
 * Unq = unlock q disk
 * Wnp = write new parity
 * Wnd = write new data
 * Wnq = write new "q"
 * [ ] denotes optional segments in the graph.
 *
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		pfuncs	  - list of parity generating functions
 *		qfuncs	  - list of q generating functions
 *
 * A null qfuncs indicates single fault tolerant.
 *****************************************************************************/

void
rf_CommonCreateSmallWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_RedFuncs_t *pfuncs, RF_RedFuncs_t *qfuncs)
{
	RF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;
	RF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;
	RF_DagNode_t *xorNodes, *qNodes, *blockNode, *nodes;
	RF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;
	int i, j, nNodes, totalNumNodes, lu_flag;
	RF_ReconUnitNum_t which_ru;
	int (*func) (RF_DagNode_t *);
	int (*undoFunc) (RF_DagNode_t *);
	int (*qfunc) (RF_DagNode_t *);
	int numDataNodes, numParityNodes;
	RF_StripeNum_t parityStripeID;
	RF_PhysDiskAddr_t *pda;
	char *name, *qname;
	long nfaults;

	nfaults = qfuncs ? 2 : 1;
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* Lock/unlock flag. */

	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
	pda = asmap->physInfo;
	numDataNodes = asmap->numStripeUnitsAccessed;
	numParityNodes = (asmap->parityInfo->next) ? 2 : 1;

	if (rf_dagDebug)
		printf("[Creating small-write DAG]\n");
	RF_ASSERT(numDataNodes > 0);
	dag_h->creator = "SmallWriteDAGFwd";

	dag_h->numCommitNodes = 0;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	qfunc = NULL;
	qname = NULL;

	/*
	 * DAG creation occurs in four steps:
	 * 1. Count the number of nodes in the DAG.
	 * 2. Create the nodes.
	 * 3. Initialize the nodes.
	 * 4. Connect the nodes.
	 */

	/* Step 1. Compute number of nodes in the graph. */

	/*
	 * Number of nodes: a read and write for each data unit, a redundancy
	 * computation node for each parity node (nfaults * nparity), a read
	 * and write for each parity unit, a block node, a terminate node if
	 * atomic RMW, an unlock node for each data/redundancy unit.
	 */
	totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)
	    + (nfaults * 2 * numParityNodes) + 2;
	if (lu_flag)
		totalNumNodes += (numDataNodes + (nfaults * numParityNodes));


	/* Step 2. Create the nodes. */
	RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
	i = 0;
	blockNode = &nodes[i];
	i += 1;
	readDataNodes = &nodes[i];
	i += numDataNodes;
	readParityNodes = &nodes[i];
	i += numParityNodes;
	writeDataNodes = &nodes[i];
	i += numDataNodes;
	writeParityNodes = &nodes[i];
	i += numParityNodes;
	xorNodes = &nodes[i];
	i += numParityNodes;
	termNode = &nodes[i];
	i += 1;
	if (lu_flag) {
		unlockDataNodes = &nodes[i];
		i += numDataNodes;
		unlockParityNodes = &nodes[i];
		i += numParityNodes;
	} else {
		unlockDataNodes = unlockParityNodes = NULL;
	}
	if (nfaults == 2) {
		readQNodes = &nodes[i];
		i += numParityNodes;
		writeQNodes = &nodes[i];
		i += numParityNodes;
		qNodes = &nodes[i];
		i += numParityNodes;
		if (lu_flag) {
			unlockQNodes = &nodes[i];
			i += numParityNodes;
		} else {
			unlockQNodes = NULL;
		}
	} else {
		readQNodes = writeQNodes = qNodes = unlockQNodes = NULL;
	}
	RF_ASSERT(i == totalNumNodes);

	/* Step 3. Initialize the nodes. */
	/* Initialize block node (Nil). */
	nNodes = numDataNodes + (nfaults * numParityNodes);
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);

	/* Initialize terminate node (Trm). */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h,
	    "Trm", allocList);

	/* Initialize nodes which read old data (Rod). */
	for (i = 0; i < numDataNodes; i++) {
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h,
		    "Rod", allocList);
		RF_ASSERT(pda != NULL);
		/* Physical disk addr desc. */
		readDataNodes[i].params[0].p = pda;
		/* Buffer to hold old data. */
		readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h,
		    pda, allocList);
		readDataNodes[i].params[2].v = parityStripeID;
		readDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
		pda = pda->next;
		for (j = 0; j < readDataNodes[i].numSuccedents; j++)
			readDataNodes[i].propList[j] = NULL;
	}

	/* Initialize nodes which read old parity (Rop). */
	pda = asmap->parityInfo;
	i = 0;
	for (i = 0; i < numParityNodes; i++) {
		RF_ASSERT(pda != NULL);
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
		readParityNodes[i].params[0].p = pda;
		/* Buffer to hold old parity. */
		readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
		    dag_h, pda, allocList);
		readParityNodes[i].params[2].v = parityStripeID;
		readParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
		for (j = 0; j < readParityNodes[i].numSuccedents; j++)
			readParityNodes[i].propList[0] = NULL;
		pda = pda->next;
	}

	/* Initialize nodes which read old Q (Roq). */
	if (nfaults == 2) {
		pda = asmap->qInfo;
		for (i = 0; i < numParityNodes; i++) {
			RF_ASSERT(pda != NULL);
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes, 1, 4, 0,
			    dag_h, "Roq", allocList);
			readQNodes[i].params[0].p = pda;
			/* Buffer to hold old Q. */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
			    dag_h, pda, allocList);
			readQNodes[i].params[2].v = parityStripeID;
			readQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    lu_flag, 0, which_ru);
			for (j = 0; j < readQNodes[i].numSuccedents; j++)
				readQNodes[i].propList[0] = NULL;
			pda = pda->next;
		}
	}
	/* Initialize nodes which write new data (Wnd). */
	pda = asmap->physInfo;
	for (i = 0; i < numDataNodes; i++) {
		RF_ASSERT(pda != NULL);
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
		/* Physical disk addr desc. */
		writeDataNodes[i].params[0].p = pda;
		/* Buffer holding new data to be written. */
		writeDataNodes[i].params[1].p = pda->bufPtr;
		writeDataNodes[i].params[2].v = parityStripeID;
		writeDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);

		if (lu_flag) {
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Und", allocList);
			/* Physical disk addr desc. */
			unlockDataNodes[i].params[0].p = pda;
			unlockDataNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
		}
		pda = pda->next;
	}


	/* Initialize nodes which compute new parity and Q. */
	/*
	 * Use the simple XOR func in the double-XOR case, and when
	 * accessing only a portion of one stripe unit. The distinction
	 * between the two is that the regular XOR func assumes that the
	 * targbuf is a full SU in size, and examines the pda associated with
	 * the buffer to decide where within the buffer to XOR the data,
	 * whereas the simple XOR func just XORs the data into the start of
	 * the buffer.
	 */
	if ((numParityNodes == 2) || ((numDataNodes == 1) &&
	    (asmap->totalSectorsAccessed <
	     raidPtr->Layout.sectorsPerStripeUnit))) {
		func = pfuncs->simple;
		undoFunc = rf_NullNodeUndoFunc;
		name = pfuncs->SimpleName;
		if (qfuncs) {
			qfunc = qfuncs->simple;
			qname = qfuncs->SimpleName;
		}
	} else {
		func = pfuncs->regular;
		undoFunc = rf_NullNodeUndoFunc;
		name = pfuncs->RegularName;
		if (qfuncs) {
			qfunc = qfuncs->regular;
			qname = qfuncs->RegularName;
		}
	}
	/*
	 * Initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}
	 * nodes, and raidPtr.
	 */
	if (numParityNodes == 2) {	/* Double-xor case. */
		for (i = 0; i < numParityNodes; i++) {
			/* No wakeup func for xor. */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func,
			    undoFunc, NULL, numParityNodes, numParityNodes +
			    numDataNodes, 7, 1, dag_h, name, allocList);
			xorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;
			xorNodes[i].params[0] = readDataNodes[i].params[0];
			xorNodes[i].params[1] = readDataNodes[i].params[1];
			xorNodes[i].params[2] = readParityNodes[i].params[0];
			xorNodes[i].params[3] = readParityNodes[i].params[1];
			xorNodes[i].params[4] = writeDataNodes[i].params[0];
			xorNodes[i].params[5] = writeDataNodes[i].params[1];
			xorNodes[i].params[6].p = raidPtr;
			/* Use old parity buf as target buf. */
			xorNodes[i].results[0] = readParityNodes[i].params[1].p;
			if (nfaults == 2) {
				/* No wakeup func for xor. */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE,
				    qfunc, undoFunc, NULL, numParityNodes,
				    numParityNodes + numDataNodes, 7, 1,
				    dag_h, qname, allocList);
				qNodes[i].params[0] =
				    readDataNodes[i].params[0];
				qNodes[i].params[1] =
				    readDataNodes[i].params[1];
				qNodes[i].params[2] = readQNodes[i].params[0];
				qNodes[i].params[3] = readQNodes[i].params[1];
				qNodes[i].params[4] =
				    writeDataNodes[i].params[0];
				qNodes[i].params[5] =
				    writeDataNodes[i].params[1];
				qNodes[i].params[6].p = raidPtr;
				/* Use old Q buf as target buf. */
				qNodes[i].results[0] =
				    readQNodes[i].params[1].p;
			}
		}
	} else {
		/* There is only one xor node in this case. */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc,
		    NULL, numParityNodes, numParityNodes + numDataNodes,
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,
		    name, allocList);
		xorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;
		for (i = 0; i < numDataNodes + 1; i++) {
			/* Set up params related to Rod and Rop nodes. */
			xorNodes[0].params[2 * i + 0] =
			    readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] =
			    readDataNodes[i].params[1];	/* buffer pointer */
		}
		for (i = 0; i < numDataNodes; i++) {
			/* Set up params related to Wnd and Wnp nodes. */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =
			    writeDataNodes[i].params[0]; /* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =
			    writeDataNodes[i].params[1]; /* buffer pointer */
		}
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p =
		    raidPtr;	/* xor node needs to get at RAID information */
		xorNodes[0].results[0] = readParityNodes[0].params[1].p;
		if (nfaults == 2) {
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc,
			    undoFunc, NULL, numParityNodes,
			    numParityNodes + numDataNodes,
			    (2 * (numDataNodes + numDataNodes + 1) + 1),
			    1, dag_h, qname, allocList);
			for (i = 0; i < numDataNodes; i++) {
				/* Set up params related to Rod. */
				/* pda */
				qNodes[0].params[2 * i + 0] =
				    readDataNodes[i].params[0];
				/* buffer pointer */
				qNodes[0].params[2 * i + 1] =
				    readDataNodes[i].params[1];
			}
			/* And read old q. */
			qNodes[0].params[2 * numDataNodes + 0] =
			    readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] =
			    readQNodes[0].params[1];	/* buffer pointer */
			for (i = 0; i < numDataNodes; i++) {
				/* Set up params related to Wnd nodes. */
				/* pda */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 0] =
				    writeDataNodes[i].params[0];
				/* buffer pointer */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 1] =
				    writeDataNodes[i].params[1];
			}
			/* Xor node needs to get at RAID information. */
			qNodes[0].params
			    [2 * (numDataNodes + numDataNodes + 1)].p =
			    raidPtr;
			qNodes[0].results[0] = readQNodes[0].params[1].p;
		}
	}

	/* Initialize nodes which write new parity (Wnp). */
	pda = asmap->parityInfo;
	for (i = 0; i < numParityNodes; i++) {
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, numParityNodes,
		    4, 0, dag_h, "Wnp", allocList);
		RF_ASSERT(pda != NULL);
		/* Param 1 (bufPtr) filled in by xor node. */
		writeParityNodes[i].params[0].p = pda;
		/* Buffer pointer for parity write operation. */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];
		writeParityNodes[i].params[2].v = parityStripeID;
		writeParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);

		if (lu_flag) {
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Unp", allocList);
			unlockParityNodes[i].params[0].p =
			    pda;	/* Physical disk addr desc. */
			unlockParityNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
		}
		pda = pda->next;
	}

	/* Initialize nodes which write new Q (Wnq). */
	if (nfaults == 2) {
		pda = asmap->qInfo;
		for (i = 0; i < numParityNodes; i++) {
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, numParityNodes,
			    4, 0, dag_h, "Wnq", allocList);
			RF_ASSERT(pda != NULL);
			/* Param 1 (bufPtr) filled in by xor node. */
			writeQNodes[i].params[0].p = pda;
			/* Buffer pointer for parity write operation. */
			writeQNodes[i].params[1].p = qNodes[i].results[0];
			writeQNodes[i].params[2].v = parityStripeID;
			writeQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);

			if (lu_flag) {
				/* Initialize node to unlock the disk queue. */
				rf_InitNode(&unlockQNodes[i], rf_wait,
				    RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc,
				    rf_GenericWakeupFunc, 1, 1, 2, 0,
				    dag_h, "Unq", allocList);
				/* Physical disk addr desc. */
				unlockQNodes[i].params[0].p = pda;
				unlockQNodes[i].params[1].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, lu_flag, which_ru);
			}
			pda = pda->next;
		}
	}
	/* Step 4. Connect the nodes. */

	/* Connect header to block node. */
	dag_h->succedents[0] = blockNode;

	/* Connect block node to read old data nodes. */
	RF_ASSERT(blockNode->numSuccedents ==
	    (numDataNodes + (numParityNodes * nfaults)));
	for (i = 0; i < numDataNodes; i++) {
		blockNode->succedents[i] = &readDataNodes[i];
		RF_ASSERT(readDataNodes[i].numAntecedents == 1);
		readDataNodes[i].antecedents[0] = blockNode;
		readDataNodes[i].antType[0] = rf_control;
	}

	/* Connect block node to read old parity nodes. */
	for (i = 0; i < numParityNodes; i++) {
		blockNode->succedents[numDataNodes + i] = &readParityNodes[i];
		RF_ASSERT(readParityNodes[i].numAntecedents == 1);
		readParityNodes[i].antecedents[0] = blockNode;
		readParityNodes[i].antType[0] = rf_control;
	}

	/* Connect block node to read old Q nodes. */
	if (nfaults == 2)
		for (i = 0; i < numParityNodes; i++) {
			blockNode->succedents[numDataNodes +
			    numParityNodes + i] = &readQNodes[i];
			RF_ASSERT(readQNodes[i].numAntecedents == 1);
			readQNodes[i].antecedents[0] = blockNode;
			readQNodes[i].antType[0] = rf_control;
		}

	/* Connect read old data nodes to write new data nodes. */
	for (i = 0; i < numDataNodes; i++) {
		RF_ASSERT(readDataNodes[i].numSuccedents ==
		    ((nfaults * numParityNodes) + 1));
		RF_ASSERT(writeDataNodes[i].numAntecedents == 1);
		readDataNodes[i].succedents[0] = &writeDataNodes[i];
		writeDataNodes[i].antecedents[0] = &readDataNodes[i];
		writeDataNodes[i].antType[0] = rf_antiData;
	}

	/* Connect read old data nodes to xor nodes. */
	for (i = 0; i < numDataNodes; i++) {
		for (j = 0; j < numParityNodes; j++) {
			RF_ASSERT(xorNodes[j].numAntecedents ==
			    numDataNodes + numParityNodes);
			readDataNodes[i].succedents[1 + j] = &xorNodes[j];
			xorNodes[j].antecedents[i] = &readDataNodes[i];
			xorNodes[j].antType[i] = rf_trueData;
		}
	}

	/* Connect read old data nodes to q nodes. */
	if (nfaults == 2)
		for (i = 0; i < numDataNodes; i++)
			for (j = 0; j < numParityNodes; j++) {
				RF_ASSERT(qNodes[j].numAntecedents ==
				    numDataNodes + numParityNodes);
				readDataNodes[i].succedents
				    [1 + numParityNodes + j] = &qNodes[j];
				qNodes[j].antecedents[i] = &readDataNodes[i];
				qNodes[j].antType[i] = rf_trueData;
			}

	/* Connect read old parity nodes to xor nodes. */
	for (i = 0; i < numParityNodes; i++) {
		for (j = 0; j < numParityNodes; j++) {
			RF_ASSERT(readParityNodes[i].numSuccedents ==
			    numParityNodes);
			readParityNodes[i].succedents[j] = &xorNodes[j];
			xorNodes[j].antecedents[numDataNodes + i] =
			    &readParityNodes[i];
			xorNodes[j].antType[numDataNodes + i] = rf_trueData;
		}
	}

	/* Connect read old q nodes to q nodes. */
	if (nfaults == 2)
		for (i = 0; i < numParityNodes; i++) {
			for (j = 0; j < numParityNodes; j++) {
				RF_ASSERT(readQNodes[i].numSuccedents ==
				    numParityNodes);
				readQNodes[i].succedents[j] = &qNodes[j];
				qNodes[j].antecedents[numDataNodes + i] =
				    &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] =
				    rf_trueData;
			}
		}

	/* Connect xor nodes to the write new parity nodes. */
	for (i = 0; i < numParityNodes; i++) {
		RF_ASSERT(writeParityNodes[i].numAntecedents == numParityNodes);
		for (j = 0; j < numParityNodes; j++) {
			RF_ASSERT(xorNodes[j].numSuccedents == numParityNodes);
			xorNodes[i].succedents[j] = &writeParityNodes[j];
			writeParityNodes[j].antecedents[i] = &xorNodes[i];
			writeParityNodes[j].antType[i] = rf_trueData;
		}
	}

	/* Connect q nodes to the write new q nodes. */
	if (nfaults == 2)
		for (i = 0; i < numParityNodes; i++) {
			RF_ASSERT(writeQNodes[i].numAntecedents ==
			    numParityNodes);
			for (j = 0; j < numParityNodes; j++) {
				RF_ASSERT(qNodes[j].numSuccedents == 1);
				qNodes[i].succedents[j] = &writeQNodes[j];
				writeQNodes[j].antecedents[i] = &qNodes[i];
				writeQNodes[j].antType[i] = rf_trueData;
			}
		}

	RF_ASSERT(termNode->numAntecedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
	RF_ASSERT(termNode->numSuccedents == 0);
	for (i = 0; i < numDataNodes; i++) {
		if (lu_flag) {
			/* Connect write new data nodes to unlock nodes. */
			RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
			RF_ASSERT(unlockDataNodes[i].numAntecedents == 1);
			writeDataNodes[i].succedents[0] = &unlockDataNodes[i];
			unlockDataNodes[i].antecedents[0] = &writeDataNodes[i];
			unlockDataNodes[i].antType[0] = rf_control;

			/* Connect unlock nodes to term nodes. */
			RF_ASSERT(unlockDataNodes[i].numSuccedents == 1);
			unlockDataNodes[i].succedents[0] = termNode;
			termNode->antecedents[i] = &unlockDataNodes[i];
			termNode->antType[i] = rf_control;
		} else {
			/* Connect write new data nodes to term node. */
			RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
			RF_ASSERT(termNode->numAntecedents ==
			    (numDataNodes + (nfaults * numParityNodes)));
			writeDataNodes[i].succedents[0] = termNode;
			termNode->antecedents[i] = &writeDataNodes[i];
			termNode->antType[i] = rf_control;
		}
	}

	for (i = 0; i < numParityNodes; i++) {
		if (lu_flag) {
			/* Connect write new parity nodes to unlock nodes. */
			RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
			RF_ASSERT(unlockParityNodes[i].numAntecedents == 1);
			writeParityNodes[i].succedents[0] =
			    &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] =
			    &writeParityNodes[i];
			unlockParityNodes[i].antType[0] = rf_control;

			/* Connect unlock nodes to term node. */
			RF_ASSERT(unlockParityNodes[i].numSuccedents == 1);
			unlockParityNodes[i].succedents[0] = termNode;
			termNode->antecedents[numDataNodes + i] =
			    &unlockParityNodes[i];
			termNode->antType[numDataNodes + i] = rf_control;
		} else {
			RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
			writeParityNodes[i].succedents[0] = termNode;
			termNode->antecedents[numDataNodes + i] =
			    &writeParityNodes[i];
			termNode->antType[numDataNodes + i] = rf_control;
		}
	}

	if (nfaults == 2)
		for (i = 0; i < numParityNodes; i++) {
			if (lu_flag) {
				/* Connect write new Q nodes to unlock nodes. */
				RF_ASSERT(writeQNodes[i].numSuccedents == 1);
				RF_ASSERT(unlockQNodes[i].numAntecedents == 1);
				writeQNodes[i].succedents[0] = &unlockQNodes[i];
				unlockQNodes[i].antecedents[0] =
				    &writeQNodes[i];
				unlockQNodes[i].antType[0] = rf_control;

				/* Connect unlock nodes to unblock node. */
				RF_ASSERT(unlockQNodes[i].numSuccedents == 1);
				unlockQNodes[i].succedents[0] = termNode;
				termNode->antecedents[numDataNodes +
				    numParityNodes + i] = &unlockQNodes[i];
				termNode->antType[numDataNodes +
				    numParityNodes + i] = rf_control;
			} else {
				RF_ASSERT(writeQNodes[i].numSuccedents == 1);
				writeQNodes[i].succedents[0] = termNode;
				termNode->antecedents[numDataNodes +
				    numParityNodes + i] = &writeQNodes[i];
				termNode->antType[numDataNodes +
				    numParityNodes + i] = rf_control;
			}
		}
}



/*****************************************************************************
 * Create a write graph (fault-free or degraded) for RAID level 1.
 *
 * Hdr  Nil -> Wpd -> Nil -> Trm
 *	Nil -> Wsd ->
 *
 * The "Wpd" node writes data to the primary copy in the mirror pair.
 * The "Wsd" node writes data to the secondary copy in the mirror pair.
 *
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *****************************************************************************/

void
rf_CreateRaidOneWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
{
	RF_DagNode_t *blockNode, *unblockNode, *termNode;
	RF_DagNode_t *nodes, *wndNode, *wmirNode;
	int nWndNodes, nWmirNodes, i;
	RF_ReconUnitNum_t which_ru;
	RF_PhysDiskAddr_t *pda, *pdaP;
	RF_StripeNum_t parityStripeID;

	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
	if (rf_dagDebug) {
		printf("[Creating RAID level 1 write DAG]\n");
	}
	/* 2 implies access not SU aligned. */
	nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;
	nWndNodes = (asmap->physInfo->next) ? 2 : 1;

	/* Alloc the Wnd nodes and the Wmir node. */
	if (asmap->numDataFailed == 1)
		nWndNodes--;
	if (asmap->numParityFailed == 1)
		nWmirNodes--;

	/*
	 * Total number of nodes = nWndNodes + nWmirNodes +
	 *			   (block + unblock + terminator)
	 */
	RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3,
	    sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
	i = 0;
	wndNode = &nodes[i];
	i += nWndNodes;
	wmirNode = &nodes[i];
	i += nWmirNodes;
	blockNode = &nodes[i];
	i += 1;
	unblockNode = &nodes[i];
	i += 1;
	termNode = &nodes[i];
	i += 1;
	RF_ASSERT(i == (nWndNodes + nWmirNodes + 3));

	/* This dag can commit immediately. */
	dag_h->numCommitNodes = 0;
	dag_h->numCommits = 0;
	dag_h->numSuccedents = 1;

	/* Initialize the unblock and term nodes. */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes),
	    0, 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes),
	    0, 0, dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);

	/* Initialize the wnd nodes. */
	if (nWndNodes > 0) {
		pda = asmap->physInfo;
		for (i = 0; i < nWndNodes; i++) {
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wpd", allocList);
			RF_ASSERT(pda != NULL);
			wndNode[i].params[0].p = pda;
			wndNode[i].params[1].p = pda->bufPtr;
			wndNode[i].params[2].v = parityStripeID;
			wndNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
			pda = pda->next;
		}
		RF_ASSERT(pda == NULL);
	}
	/* Initialize the mirror nodes. */
	if (nWmirNodes > 0) {
		pda = asmap->physInfo;
		pdaP = asmap->parityInfo;
		for (i = 0; i < nWmirNodes; i++) {
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wsd", allocList);
			RF_ASSERT(pda != NULL);
			wmirNode[i].params[0].p = pdaP;
			wmirNode[i].params[1].p = pda->bufPtr;
			wmirNode[i].params[2].v = parityStripeID;
			wmirNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
			pda = pda->next;
			pdaP = pdaP->next;
		}
		RF_ASSERT(pda == NULL);
		RF_ASSERT(pdaP == NULL);
	}
	/* Link the header node to the block node. */
	RF_ASSERT(dag_h->numSuccedents == 1);
	RF_ASSERT(blockNode->numAntecedents == 0);
	dag_h->succedents[0] = blockNode;

	/* Link the block node to the write nodes. */
	RF_ASSERT(blockNode->numSuccedents == (nWndNodes + nWmirNodes));
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNode[i].numAntecedents == 1);
		blockNode->succedents[i] = &wndNode[i];
		wndNode[i].antecedents[0] = blockNode;
		wndNode[i].antType[0] = rf_control;
	}
	for (i = 0; i < nWmirNodes; i++) {
		RF_ASSERT(wmirNode[i].numAntecedents == 1);
		blockNode->succedents[i + nWndNodes] = &wmirNode[i];
		wmirNode[i].antecedents[0] = blockNode;
		wmirNode[i].antType[0] = rf_control;
	}

	/* Link the write nodes to the unblock node. */
	RF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));
	for (i = 0; i < nWndNodes; i++) {
		RF_ASSERT(wndNode[i].numSuccedents == 1);
		wndNode[i].succedents[0] = unblockNode;
		unblockNode->antecedents[i] = &wndNode[i];
		unblockNode->antType[i] = rf_control;
	}
	for (i = 0; i < nWmirNodes; i++) {
		RF_ASSERT(wmirNode[i].numSuccedents == 1);
		wmirNode[i].succedents[0] = unblockNode;
		unblockNode->antecedents[i + nWndNodes] = &wmirNode[i];
		unblockNode->antType[i + nWndNodes] = rf_control;
	}

	/* Link the unblock node to the term node. */
	RF_ASSERT(unblockNode->numSuccedents == 1);
	RF_ASSERT(termNode->numAntecedents == 1);
	RF_ASSERT(termNode->numSuccedents == 0);
	unblockNode->succedents[0] = termNode;
	termNode->antecedents[0] = unblockNode;
	termNode->antType[0] = rf_control;

	return;
}
@


1.5
log
@Major KNF.  Incentive from Tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_dagffwr.c,v 1.4 2000/01/11 18:02:21 peter Exp $	*/
@


1.4
log
@sync with NetBSD

- removed threadid stuff
- removed unused files
- general tidyup
- you can no longer configure the same unit twice (without
de-configuring first of course).

Again, this has only been tested locally on IDE disks. Further testing
and feedback would be appreciated.
@
text
@d1 1
a1 1
/*	$OpenBSD: rf_dagffwr.c,v 1.3 2000/01/07 14:50:20 peter Exp $	*/
d3 1
d34 1
a34 1
 * code for creating fault-free DAGs
d49 1
a49 1
/******************************************************************************
d53 1
a53 1
 * All DAGs in this file use roll-away error recovery.  Each DAG has a single
d56 1
a56 1
 * backward through the graph, executing the undo functions.  Assuming that
d65 1
a65 1
 */
d68 1
a68 1
/******************************************************************************
d71 1
a71 1
 * DAG creation routines.  Additionally, these wrappers enable experimentation
d74 2
a75 1
 */
d78 4
a81 9
void 
rf_CreateNonRedundantWriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    RF_IoType_t type)
d87 4
a90 9
void 
rf_CreateRAID0WriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    RF_IoType_t type)
d96 4
a99 8
void 
rf_CreateSmallWriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList)
d101 3
a103 3
	/* "normal" rollaway */
	rf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
	    &rf_xorFuncs, NULL);
d106 4
a109 8
void 
rf_CreateLargeWriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList)
d111 3
a113 3
	/* "normal" rollaway */
	rf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
	    1, rf_RegularXorFunc, RF_TRUE);
d117 3
a119 1
/******************************************************************************
d121 1
a121 2
 * DAG creation code begins here
 */
d124 1
a124 1
/******************************************************************************
d134 2
a135 2
 * All nodes are before the commit node (Cmt) are assumed to be atomic and
 * undoable - or - they make no changes to permanent state.
d142 1
a142 1
 * [] denotes optional segments in the graph
d144 8
a151 8
 * Parameters:  raidPtr   - description of the physical array
 *              asmap     - logical & physical addresses for this access
 *              bp        - buffer ptr (holds write data)
 *              flags     - general flags (e.g. disk locking)
 *              allocList - list of memory allocated in DAG creation
 *              nfaults   - number of faults array can tolerate
 *                          (equal to # redundancy units in stripe)
 *              redfuncs  - list of redundancy generating functions
d155 4
a158 10
void 
rf_CommonCreateLargeWriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    int nfaults,
    int (*redFunc) (RF_DagNode_t *),
d163 1
a163 1
	int     nWndNodes, nRodNodes, i, nodeNum, asmNum;
d166 1
a166 1
	char   *sosBuffer, *eosBuffer;
d172 2
a173 2
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,
	    &which_ru);
d184 1
a184 1
	/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */
d207 2
a208 2
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h,
	    &nRodNodes, &sosBuffer, &eosBuffer, allocList);
d216 1
a216 1
	/* begin node initialization */
d218 3
a220 2
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
		    NULL, nRodNodes, 0, 0, 0, dag_h, "Nil", allocList);
d222 3
a224 2
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
		    NULL, 1, 0, 0, 0, dag_h, "Nil", allocList);
d227 6
a232 4
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL,
	    nWndNodes + nfaults, 1, 0, 0, dag_h, "Cmt", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL,
	    0, nWndNodes + nfaults, 0, 0, dag_h, "Trm", allocList);
d234 1
a234 1
	/* initialize the Rod nodes */
d239 4
a242 3
				rf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
				    "Rod", allocList);
d246 2
a247 1
				rodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d256 1
a256 1
	/* initialize the wnd nodes */
d259 3
a261 2
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
d266 2
a267 1
		wndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d271 1
a271 1
	/* initialize the redundancy node */
d273 3
a275 2
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,
		    nRodNodes, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
d278 4
a281 2
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,
		    1, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, "Xr ", allocList);
d285 4
a288 2
		xorNode->params[2 * i + 0] = wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] = wndNodes[i].params[1];	/* buf ptr */
d291 4
a294 2
		xorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];	/* buf ptr */
d296 1
a296 1
	/* xor node needs to get at RAID information */
d300 5
a304 4
         * Look for an Rod node that reads a complete SU. If none, alloc a buffer
         * to receive the parity info. Note that we can't use a new data buffer
         * because it will not have gotten written when the xor occurs.
         */
d307 2
a308 1
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)
d314 2
a315 1
		    rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),
d321 4
a324 3
	/* initialize the Wnp node */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
	    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnp", allocList);
d328 3
a330 2
	wnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit */
d335 5
a339 5
	         * We never try to recycle a buffer for the Q calcuation
	         * in addition to the parity. This would cause two buffers
	         * to get smashed during the P and Q calculation, guaranteeing
	         * one would be wrong.
	         */
d341 2
a342 1
		    rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),
d344 3
a346 2
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnq", allocList);
d350 3
a352 2
		wnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit */
d356 2
a357 2
         * Connect nodes to form graph.
         */
d359 1
a359 1
	/* connect dag header to block node */
d364 1
a364 1
		/* connect the block node to the Rod nodes */
d373 1
a373 1
			/* connect the Rod nodes to the Xor node */
d380 1
a380 1
		/* connect the block node to the Xor node */
d388 1
a388 1
	/* connect the xor node to the commit node */
d395 1
a395 1
	/* connect the commit node to the write nodes */
d413 1
a413 1
	/* connect the write nodes to the term node */
d433 1
a433 1
/******************************************************************************
d435 1
a435 1
 * creates a DAG to perform a small-write operation (either raid 5 or pq),
d453 1
a453 1
 * [ ] denotes optional segments in the graph
d455 7
a461 7
 * Parameters:  raidPtr   - description of the physical array
 *              asmap     - logical & physical addresses for this access
 *              bp        - buffer ptr (holds write data)
 *              flags     - general flags (e.g. disk locking)
 *              allocList - list of memory allocated in DAG creation
 *              pfuncs    - list of parity generating functions
 *              qfuncs    - list of q generating functions
d463 1
a463 1
 * A null qfuncs indicates single fault tolerant
d466 4
a469 10
void 
rf_CommonCreateSmallWriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    RF_RedFuncs_t * pfuncs,
    RF_RedFuncs_t * qfuncs)
d475 1
a475 1
	int     i, j, nNodes, totalNumNodes, lu_flag;
d477 4
a480 3
	int     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);
	int     (*qfunc) (RF_DagNode_t *);
	int     numDataNodes, numParityNodes;
d483 2
a484 2
	char   *name, *qname;
	long    nfaults;
d487 1
a487 1
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* lock/unlock flag */
d506 6
a511 6
         * DAG creation occurs in four steps:
         * 1. count the number of nodes in the DAG
         * 2. create the nodes
         * 3. initialize the nodes
         * 4. connect the nodes
         */
d514 2
a515 2
         * Step 1. compute number of nodes in the graph
         */
d517 7
a523 5
	/* number of nodes: a read and write for each data unit a redundancy
	 * computation node for each parity node (nfaults * nparity) a read
	 * and write for each parity unit a block and commit node (2) a
	 * terminate node if atomic RMW an unlock node for each data unit,
	 * redundancy unit */
d530 2
a531 2
         * Step 2. create the nodes
         */
d578 3
a580 3
         * Step 3. initialize the nodes
         */
	/* initialize block node (Nil) */
d582 13
a594 10
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, nNodes, 0, 0, 0, dag_h, "Nil", allocList);

	/* initialize commit node (Cmt) */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, nNodes, (nfaults * numParityNodes), 0, 0, dag_h, "Cmt", allocList);

	/* initialize terminate node (Trm) */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,
	    NULL, 0, nNodes, 0, 0, dag_h, "Trm", allocList);
d596 1
a596 1
	/* initialize nodes which read old data (Rod) */
d598 4
a601 3
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
		    rf_GenericWakeupFunc, (nfaults * numParityNodes), 1, 4, 0, dag_h,
		    "Rod", allocList);
d603 1
a603 1
		/* physical disk addr desc */
d605 1
a605 1
		/* buffer to hold old data */
d609 2
a610 1
		readDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d618 1
a618 1
	/* initialize nodes which read old parity (Rop) */
d623 3
a625 3
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,
		    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4,
		    0, dag_h, "Rop", allocList);
d627 1
a627 1
		/* buffer to hold old parity */
d631 2
a632 1
		readParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d640 1
a640 1
	/* initialize nodes which read old Q (Roq) */
d645 4
a648 2
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, "Roq", allocList);
d650 3
a652 3
			/* buffer to hold old Q */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda,
			    allocList);
d654 2
a655 1
			readQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d663 1
a663 1
	/* initialize nodes which write new data (Wnd) */
d667 3
a669 2
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d671 1
a671 1
		/* physical disk addr desc */
d673 1
a673 1
		/* buffer holding new data to be written */
d676 2
a677 2
		writeDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    0, 0, which_ru);
d679 4
a682 3
			/* initialize node to unlock the disk queue */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,
			    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
d684 1
a684 1
			/* physical disk addr desc */
d686 2
a687 1
			unlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d694 2
a695 2
         * Initialize nodes which compute new parity and Q.
         */
d697 11
a707 9
         * We use the simple XOR func in the double-XOR case, and when
         * we're accessing only a portion of one stripe unit. The distinction
         * between the two is that the regular XOR func assumes that the targbuf
         * is a full SU in size, and examines the pda associated with the buffer
         * to decide where within the buffer to XOR the data, whereas
         * the simple XOR func just XORs the data into the start of the buffer.
         */
	if ((numParityNodes == 2) || ((numDataNodes == 1)
		&& (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {
d731 3
a733 3
         * Initialize the xor nodes: params are {pda,buf}
         * from {Rod,Wnd,Rop} nodes, and raidPtr
         */
d735 1
a735 1
		/* double-xor case */
d737 4
a740 3
			/* note: no wakeup func for xor */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL,
			    1, (numDataNodes + numParityNodes), 7, 1, dag_h, name, allocList);
d749 1
a749 1
			/* use old parity buf as target buf */
d752 9
a760 5
				/* note: no wakeup func for qor */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,
				    (numDataNodes + numParityNodes), 7, 1, dag_h, qname, allocList);
				qNodes[i].params[0] = readDataNodes[i].params[0];
				qNodes[i].params[1] = readDataNodes[i].params[1];
d763 4
a766 2
				qNodes[i].params[4] = writeDataNodes[i].params[0];
				qNodes[i].params[5] = writeDataNodes[i].params[1];
d768 3
a770 2
				/* use old Q buf as target buf */
				qNodes[i].results[0] = readQNodes[i].params[1].p;
d774 5
a778 4
		/* there is only one xor node in this case */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, 1,
		    (numDataNodes + numParityNodes),
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);
d781 5
a785 3
			/* set up params related to Rod and Rop nodes */
			xorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];	/* buffer ptr */
d788 9
a796 8
			/* set up params related to Wnd and Wnp nodes */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =	/* pda */
			    writeDataNodes[i].params[0];
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =	/* buffer ptr */
			    writeDataNodes[i].params[1];
		}
		/* xor node needs to get at RAID information */
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;
d799 4
a802 4
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,
			    (numDataNodes + numParityNodes),
			    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,
			    qname, allocList);
d804 5
a808 3
				/* set up params related to Rod */
				qNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];	/* pda */
				qNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];	/* buffer ptr */
d810 5
a814 5
			/* and read old q */
			qNodes[0].params[2 * numDataNodes + 0] =	/* pda */
			    readQNodes[0].params[0];
			qNodes[0].params[2 * numDataNodes + 1] =	/* buffer ptr */
			    readQNodes[0].params[1];
d816 4
a819 2
				/* set up params related to Wnd nodes */
				qNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =	/* pda */
d821 3
a823 1
				qNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =	/* buffer ptr */
d826 3
a828 2
			/* xor node needs to get at RAID information */
			qNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;
d833 1
a833 1
	/* initialize nodes which write new parity (Wnp) */
d836 3
a838 2
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d841 4
a844 5
		writeParityNodes[i].params[0].p = pda;	/* param 1 (bufPtr)
							 * filled in by xor node */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];	/* buffer pointer for
										 * parity write
										 * operation */
d846 2
a847 2
		writeParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    0, 0, which_ru);
d849 4
a852 3
			/* initialize node to unlock the disk queue */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,
			    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
d854 4
a857 3
			unlockParityNodes[i].params[0].p = pda;	/* physical disk addr
								 * desc */
			unlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d863 1
a863 1
	/* initialize nodes which write new Q (Wnq) */
d867 3
a869 2
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
			    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d872 4
a875 5
			writeQNodes[i].params[0].p = pda;	/* param 1 (bufPtr)
								 * filled in by xor node */
			writeQNodes[i].params[1].p = qNodes[i].results[0];	/* buffer pointer for
										 * parity write
										 * operation */
d877 2
a878 1
			writeQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d881 10
a890 7
				/* initialize node to unlock the disk queue */
				rf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
				    "Unq", allocList);
				unlockQNodes[i].params[0].p = pda;	/* physical disk addr
									 * desc */
				unlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d897 2
a898 2
         * Step 4. connect the nodes.
         */
d900 1
a900 1
	/* connect header to block node */
d903 3
a905 2
	/* connect block node to read old data nodes */
	RF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));
d913 1
a913 1
	/* connect block node to read old parity nodes */
d921 1
a921 1
	/* connect block node to read old Q nodes */
d924 2
a925 1
			blockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];
d931 1
a931 1
	/* connect read old data nodes to xor nodes */
d933 2
a934 1
		RF_ASSERT(readDataNodes[i].numSuccedents == (nfaults * numParityNodes));
d936 2
a937 1
			RF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);
d944 1
a944 1
	/* connect read old data nodes to q nodes */
d948 4
a951 2
				RF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);
				readDataNodes[i].succedents[numParityNodes + j] = &qNodes[j];
d957 1
a957 1
	/* connect read old parity nodes to xor nodes */
d962 2
a963 1
			xorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];
d968 1
a968 1
	/* connect read old q nodes to q nodes */
d971 2
a972 1
			RF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);
d975 4
a978 2
				qNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] = rf_trueData;
d982 1
a982 1
	/* connect xor nodes to commit node */
d991 1
a991 1
	/* connect q nodes to commit node */
d996 2
a997 1
			commitNode->antecedents[i + numParityNodes] = &qNodes[i];
d1001 3
a1003 2
	/* connect commit node to write nodes */
	RF_ASSERT(commitNode->numSuccedents == (numDataNodes + (nfaults * numParityNodes)));
d1019 3
a1021 1
			commitNode->succedents[i + numDataNodes + numParityNodes] = &writeQNodes[i];
d1026 2
a1027 1
	RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
d1031 1
a1031 1
			/* connect write new data nodes to unlock nodes */
d1038 1
a1038 1
			/* connect unlock nodes to term node */
d1044 1
a1044 1
			/* connect write new data nodes to term node */
d1046 2
a1047 1
			RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
d1056 1
a1056 1
			/* connect write new parity nodes to unlock nodes */
d1059 4
a1062 2
			writeParityNodes[i].succedents[0] = &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] = &writeParityNodes[i];
d1065 1
a1065 1
			/* connect unlock nodes to term node */
d1068 2
a1069 1
			termNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];
d1074 2
a1075 1
			termNode->antecedents[numDataNodes + i] = &writeParityNodes[i];
d1083 1
a1083 1
				/* connect write new Q nodes to unlock nodes */
d1087 2
a1088 1
				unlockQNodes[i].antecedents[0] = &writeQNodes[i];
d1091 1
a1091 1
				/* connect unlock nodes to unblock node */
d1094 6
a1099 2
				termNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];
				termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
d1103 6
a1108 2
				termNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];
				termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
d1115 2
a1116 2
/******************************************************************************
 * create a write graph (fault-free or degraded) for RAID level 1
d1119 1
a1119 1
 *               -> Wsd ->
d1121 2
a1122 2
 * The "Wpd" node writes data to the primary copy in the mirror pair
 * The "Wsd" node writes data to the secondary copy in the mirror pair
d1124 5
a1128 5
 * Parameters:  raidPtr   - description of the physical array
 *              asmap     - logical & physical addresses for this access
 *              bp        - buffer ptr (holds write data)
 *              flags     - general flags (e.g. disk locking)
 *              allocList - list of memory allocated in DAG creation
d1131 4
a1134 8
void 
rf_CreateRaidOneWriteDAG(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList)
d1138 1
a1138 1
	int     nWndNodes, nWmirNodes, i;
d1150 1
a1150 1
	/* 2 implies access not SU aligned */
d1154 1
a1154 1
	/* alloc the Wnd nodes and the Wmir node */
d1160 4
a1163 2
	/* total number of nodes = nWndNodes + nWmirNodes + (commit + unblock
	 * + terminator) */
d1179 1
a1179 1
	/* this dag can commit immediately */
d1184 9
a1192 7
	/* initialize the commit, unblock, and term nodes */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
	    NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,
	    NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d1194 1
a1194 1
	/* initialize the wnd nodes */
d1198 4
a1201 2
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wpd", allocList);
d1206 3
a1208 1
			wndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1213 1
a1213 1
	/* initialize the mirror nodes */
d1218 4
a1221 2
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wsd", allocList);
d1226 3
a1228 1
			wmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1235 1
a1235 1
	/* link the header node to the commit node */
d1240 1
a1240 1
	/* link the commit node to the write nodes */
d1255 1
a1255 1
	/* link the write nodes to the unblock node */
d1270 1
a1270 1
	/* link the unblock node to the term node */
d1281 2
a1282 1
/* DAGs which have no commit points.
d1284 3
a1286 2
 * The following DAGs are used in forward and backward error recovery experiments.
 * They are identical to the DAGs above this comment with the exception that the
d1291 4
a1294 11

void 
rf_CommonCreateLargeWriteDAGFwd(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    int nfaults,
    int (*redFunc) (RF_DagNode_t *),
d1299 1
a1299 1
	int     nWndNodes, nRodNodes, i, nodeNum, asmNum;
d1302 1
a1302 1
	char   *sosBuffer, *eosBuffer;
d1308 2
a1309 1
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);
d1319 1
a1319 1
	/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */
d1321 2
a1322 1
	RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d1342 2
a1343 1
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
d1345 2
a1346 1
		RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d1351 1
a1351 1
	/* begin node initialization */
d1353 6
a1358 2
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h, "Nil", allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0, dag_h, "Nil", allocList);
d1360 6
a1365 2
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil", allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h, "Nil", allocList);
d1368 3
a1370 1
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0, dag_h, "Trm", allocList);
d1372 1
a1372 1
	/* initialize the Rod nodes */
d1377 4
a1380 1
				rf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Rod", allocList);
d1384 3
a1386 1
				rodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1394 1
a1394 1
	/* initialize the wnd nodes */
d1397 3
a1399 1
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
d1404 2
a1405 1
		wndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1409 4
a1412 2
	/* initialize the redundancy node */
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, "Xr ", allocList);
d1415 4
a1418 2
		xorNode->params[2 * i + 0] = wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] = wndNodes[i].params[1];	/* buf ptr */
d1421 4
a1424 2
		xorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];	/* buf ptr */
d1426 2
a1427 2
	xorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;	/* xor node needs to get
									 * at RAID information */
d1429 2
a1430 1
	/* look for an Rod node that reads a complete SU.  If none, alloc a
d1433 2
a1434 1
	 * occurs. */
d1437 2
a1438 1
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)
d1442 4
a1445 1
		RF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);
d1449 4
a1452 2
	/* initialize the Wnp node */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnp", allocList);
d1456 4
a1459 4
	wnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	RF_ASSERT(asmap->parityInfo->next == NULL);	/* parityInfo must
							 * describe entire
							 * parity unit */
d1462 13
a1474 6
		/* we never try to recycle a buffer for the Q calcuation in
		 * addition to the parity. This would cause two buffers to get
		 * smashed during the P and Q calculation, guaranteeing one
		 * would be wrong. */
		RF_CallocAndAdd(xorNode->results[1], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnq", allocList);
d1478 4
a1481 4
		wnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		RF_ASSERT(asmap->parityInfo->next == NULL);	/* parityInfo must
								 * describe entire
								 * parity unit */
a1482 1
	/* connect nodes to form graph */
d1484 3
a1486 1
	/* connect dag header to block node */
d1491 1
a1491 1
		/* connect the block node to the Rod nodes */
d1500 1
a1500 1
			/* connect the Rod nodes to the Nil node */
d1507 1
a1507 1
		/* connect the block node to the Nil node */
d1515 1
a1515 1
	/* connect the sync node to the Wnd nodes */
d1524 1
a1524 1
	/* connect the sync node to the Xor node */
d1530 1
a1530 1
	/* connect the xor node to the write parity node */
d1542 1
a1542 1
	/* connect the write nodes to the term node */
d1564 1
a1564 1
/******************************************************************************
d1566 1
a1566 1
 * creates a DAG to perform a small-write operation (either raid 5 or pq),
d1584 1
a1584 1
 * [ ] denotes optional segments in the graph
d1586 7
a1592 7
 * Parameters:  raidPtr   - description of the physical array
 *              asmap     - logical & physical addresses for this access
 *              bp        - buffer ptr (holds write data)
 *              flags     - general flags (e.g. disk locking)
 *              allocList - list of memory allocated in DAG creation
 *              pfuncs    - list of parity generating functions
 *              qfuncs    - list of q generating functions
d1594 1
a1594 1
 * A null qfuncs indicates single fault tolerant
d1597 4
a1600 10
void 
rf_CommonCreateSmallWriteDAGFwd(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList,
    RF_RedFuncs_t * pfuncs,
    RF_RedFuncs_t * qfuncs)
d1606 1
a1606 1
	int     i, j, nNodes, totalNumNodes, lu_flag;
d1608 4
a1611 3
	int     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);
	int     (*qfunc) (RF_DagNode_t *);
	int     numDataNodes, numParityNodes;
d1614 2
a1615 2
	char   *name, *qname;
	long    nfaults;
d1618 1
a1618 1
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* lock/unlock flag */
d1620 2
a1621 1
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);
d1638 18
a1655 11
	/* DAG creation occurs in four steps: 1. count the number of nodes in
	 * the DAG 2. create the nodes 3. initialize the nodes 4. connect the
	 * nodes */

	/* Step 1. compute number of nodes in the graph */

	/* number of nodes: a read and write for each data unit a redundancy
	 * computation node for each parity node (nfaults * nparity) a read
	 * and write for each parity unit a block node a terminate node if
	 * atomic RMW an unlock node for each data unit, redundancy unit */
	totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes) + (nfaults * 2 * numParityNodes) + 2;
d1660 3
a1662 2
	/* Step 2. create the nodes */
	RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d1704 2
a1705 2
	/* Step 3. initialize the nodes */
	/* initialize block node (Nil) */
d1707 8
a1714 1
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, "Nil", allocList);
d1716 1
a1716 4
	/* initialize terminate node (Trm) */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h, "Trm", allocList);

	/* initialize nodes which read old data (Rod) */
d1718 4
a1721 1
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h, "Rod", allocList);
d1723 5
a1727 4
		readDataNodes[i].params[0].p = pda;	/* physical disk addr
							 * desc */
		readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);	/* buffer to hold old
												 * data */
d1729 3
a1731 1
		readDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);
d1737 1
a1737 1
	/* initialize nodes which read old parity (Rop) */
d1742 3
a1744 1
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
d1746 3
a1748 2
		readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);	/* buffer to hold old
													 * parity */
d1750 3
a1752 1
		readParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);
d1758 1
a1758 1
	/* initialize nodes which read old Q (Roq) */
d1763 4
a1766 1
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, "Roq", allocList);
d1768 3
a1770 1
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);	/* buffer to hold old Q */
d1772 3
a1774 1
			readQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);
d1780 1
a1780 1
	/* initialize nodes which write new data (Wnd) */
d1784 8
a1791 5
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
		writeDataNodes[i].params[0].p = pda;	/* physical disk addr
							 * desc */
		writeDataNodes[i].params[1].p = pda->bufPtr;	/* buffer holding new
								 * data to be written */
d1793 2
a1794 1
		writeDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1797 10
a1806 5
			/* initialize node to unlock the disk queue */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, "Und", allocList);
			unlockDataNodes[i].params[0].p = pda;	/* physical disk addr
								 * desc */
			unlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);
d1812 4
a1815 3
	/* initialize nodes which compute new parity and Q */
	/* we use the simple XOR func in the double-XOR case, and when we're
	 * accessing only a portion of one stripe unit. the distinction
d1820 5
a1824 2
	 * the buffer. */
	if ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {
d1841 5
a1845 3
	/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}
	 * nodes, and raidPtr  */
	if (numParityNodes == 2) {	/* double-xor case */
d1847 4
a1850 2
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, name, allocList);	/* no wakeup func for
																						 * xor */
d1859 2
a1860 2
			xorNodes[i].results[0] = readParityNodes[i].params[1].p;	/* use old parity buf as
											 * target buf */
d1862 9
a1870 4
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, qname, allocList);	/* no wakeup func for
																							 * xor */
				qNodes[i].params[0] = readDataNodes[i].params[0];
				qNodes[i].params[1] = readDataNodes[i].params[1];
d1873 4
a1876 2
				qNodes[i].params[4] = writeDataNodes[i].params[0];
				qNodes[i].params[5] = writeDataNodes[i].params[1];
d1878 3
a1880 2
				qNodes[i].results[0] = readQNodes[i].params[1].p;	/* use old Q buf as
											 * target buf */
d1884 5
a1888 2
		/* there is only one xor node in this case */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);
d1891 5
a1895 3
			/* set up params related to Rod and Rop nodes */
			xorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];	/* buffer pointer */
d1898 5
a1902 3
			/* set up params related to Wnd and Wnp nodes */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];	/* buffer pointer */
d1904 2
a1905 2
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;	/* xor node needs to get
											 * at RAID information */
d1908 5
a1912 1
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, qname, allocList);
d1914 7
a1920 3
				/* set up params related to Rod */
				qNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];	/* pda */
				qNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];	/* buffer pointer */
d1922 5
a1926 3
			/* and read old q */
			qNodes[0].params[2 * numDataNodes + 0] = readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] = readQNodes[0].params[1];	/* buffer pointer */
d1928 9
a1936 3
				/* set up params related to Wnd nodes */
				qNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];	/* pda */
				qNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];	/* buffer pointer */
d1938 4
a1941 2
			qNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;	/* xor node needs to get
												 * at RAID information */
d1946 1
a1946 1
	/* initialize nodes which write new parity (Wnp) */
d1949 4
a1952 1
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, "Wnp", allocList);
d1954 4
a1957 5
		writeParityNodes[i].params[0].p = pda;	/* param 1 (bufPtr)
							 * filled in by xor node */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];	/* buffer pointer for
										 * parity write
										 * operation */
d1959 2
a1960 1
		writeParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1963 10
a1972 5
			/* initialize node to unlock the disk queue */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, "Unp", allocList);
			unlockParityNodes[i].params[0].p = pda;	/* physical disk addr
								 * desc */
			unlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);
d1977 1
a1977 1
	/* initialize nodes which write new Q (Wnq) */
d1981 4
a1984 1
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, "Wnq", allocList);
d1986 4
a1989 5
			writeQNodes[i].params[0].p = pda;	/* param 1 (bufPtr)
								 * filled in by xor node */
			writeQNodes[i].params[1].p = qNodes[i].results[0];	/* buffer pointer for
										 * parity write
										 * operation */
d1991 3
a1993 1
			writeQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1996 11
a2006 5
				/* initialize node to unlock the disk queue */
				rf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, "Unq", allocList);
				unlockQNodes[i].params[0].p = pda;	/* physical disk addr
									 * desc */
				unlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);
d2011 1
a2011 1
	/* Step 4. connect the nodes */
d2013 1
a2013 1
	/* connect header to block node */
d2016 3
a2018 2
	/* connect block node to read old data nodes */
	RF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));
d2026 1
a2026 1
	/* connect block node to read old parity nodes */
d2034 1
a2034 1
	/* connect block node to read old Q nodes */
d2037 2
a2038 1
			blockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];
d2044 1
a2044 1
	/* connect read old data nodes to write new data nodes */
d2046 2
a2047 1
		RF_ASSERT(readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1));
d2054 1
a2054 1
	/* connect read old data nodes to xor nodes */
d2057 2
a2058 1
			RF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);
d2065 1
a2065 1
	/* connect read old data nodes to q nodes */
d2069 4
a2072 2
				RF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);
				readDataNodes[i].succedents[1 + numParityNodes + j] = &qNodes[j];
d2077 1
a2077 1
	/* connect read old parity nodes to xor nodes */
d2080 2
a2081 1
			RF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);
d2083 2
a2084 1
			xorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];
d2089 1
a2089 1
	/* connect read old q nodes to q nodes */
d2093 2
a2094 1
				RF_ASSERT(readQNodes[i].numSuccedents == numParityNodes);
d2096 4
a2099 2
				qNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] = rf_trueData;
d2103 1
a2103 1
	/* connect xor nodes to the write new parity nodes */
d2114 1
a2114 1
	/* connect q nodes to the write new q nodes */
d2117 2
a2118 1
			RF_ASSERT(writeQNodes[i].numAntecedents == numParityNodes);
d2127 2
a2128 1
	RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
d2132 1
a2132 1
			/* connect write new data nodes to unlock nodes */
d2139 1
a2139 1
			/* connect unlock nodes to term node */
d2145 1
a2145 1
			/* connect write new data nodes to term node */
d2147 2
a2148 1
			RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
d2157 1
a2157 1
			/* connect write new parity nodes to unlock nodes */
d2160 4
a2163 2
			writeParityNodes[i].succedents[0] = &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] = &writeParityNodes[i];
d2166 1
a2166 1
			/* connect unlock nodes to term node */
d2169 2
a2170 1
			termNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];
d2175 2
a2176 1
			termNode->antecedents[numDataNodes + i] = &writeParityNodes[i];
d2184 1
a2184 1
				/* connect write new Q nodes to unlock nodes */
d2188 2
a2189 1
				unlockQNodes[i].antecedents[0] = &writeQNodes[i];
d2192 1
a2192 1
				/* connect unlock nodes to unblock node */
d2195 4
a2198 2
				termNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];
				termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
d2202 4
a2205 2
				termNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];
				termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
d2212 2
a2213 2
/******************************************************************************
 * create a write graph (fault-free or degraded) for RAID level 1
d2216 1
a2216 1
 *      Nil -> Wsd ->
d2218 2
a2219 2
 * The "Wpd" node writes data to the primary copy in the mirror pair
 * The "Wsd" node writes data to the secondary copy in the mirror pair
d2221 5
a2225 5
 * Parameters:  raidPtr   - description of the physical array
 *              asmap     - logical & physical addresses for this access
 *              bp        - buffer ptr (holds write data)
 *              flags     - general flags (e.g. disk locking)
 *              allocList - list of memory allocated in DAG creation
d2228 4
a2231 8
void 
rf_CreateRaidOneWriteDAGFwd(
    RF_Raid_t * raidPtr,
    RF_AccessStripeMap_t * asmap,
    RF_DagHeader_t * dag_h,
    void *bp,
    RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t * allocList)
d2235 1
a2235 1
	int     nWndNodes, nWmirNodes, i;
d2245 2
a2246 2
	nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;	/* 2 implies access not
							 * SU aligned */
d2249 1
a2249 1
	/* alloc the Wnd nodes and the Wmir node */
d2255 6
a2260 3
	/* total number of nodes = nWndNodes + nWmirNodes + (block + unblock +
	 * terminator) */
	RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d2274 1
a2274 1
	/* this dag can commit immediately */
d2279 9
a2287 4
	/* initialize the unblock and term nodes */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d2289 1
a2289 1
	/* initialize the wnd nodes */
d2293 4
a2296 1
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wpd", allocList);
d2301 3
a2303 1
			wndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d2308 1
a2308 1
	/* initialize the mirror nodes */
d2313 4
a2316 1
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wsd", allocList);
d2321 3
a2323 1
			wmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d2330 1
a2330 1
	/* link the header node to the block node */
d2335 1
a2335 1
	/* link the block node to the write nodes */
d2350 1
a2350 1
	/* link the write nodes to the unblock node */
d2365 1
a2365 1
	/* link the unblock node to the term node */
@


1.4.12.1
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 1

d33 1
a33 1
 * Code for creating fault-free DAGs.
d48 1
a48 1
/*****************************************************************************
d52 1
a52 1
 * All DAGs in this file use roll-away error recovery. Each DAG has a single
d55 1
a55 1
 * backward through the graph, executing the undo functions. Assuming that
d64 1
a64 1
 *****************************************************************************/
d67 1
a67 1
/*****************************************************************************
d70 1
a70 1
 * DAG creation routines. Additionally, these wrappers enable experimentation
d73 1
a73 2
 *
 *****************************************************************************/
d76 9
a84 4
void
rf_CreateNonRedundantWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_IoType_t type)
d90 9
a98 4
void
rf_CreateRAID0WriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_IoType_t type)
d104 8
a111 4
void
rf_CreateSmallWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d113 3
a115 3
	/* "normal" rollaway. */
	rf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	    allocList, &rf_xorFuncs, NULL);
d118 8
a125 4
void
rf_CreateLargeWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d127 3
a129 3
	/* "normal" rollaway. */
	rf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	    allocList, 1, rf_RegularXorFunc, RF_TRUE);
d133 1
a133 1
/*****************************************************************************
d135 2
a136 3
 * DAG creation code begins here.
 *
 *****************************************************************************/
d139 1
a139 1
/*****************************************************************************
d149 2
a150 2
 * All nodes that are before the commit node (Cmt) are assumed to be atomic
 * and undoable - or - they make no changes to permanent state.
d157 1
a157 1
 * [] denotes optional segments in the graph.
d159 8
a166 8
 * Parameters:  raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		nfaults	  - number of faults array can tolerate
 *			    (equal to # redundancy units in stripe)
 *		redfuncs  - list of redundancy generating functions
d170 10
a179 4
void
rf_CommonCreateLargeWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, int nfaults, int (*redFunc) (RF_DagNode_t *),
d184 1
a184 1
	int nWndNodes, nRodNodes, i, nodeNum, asmNum;
d187 1
a187 1
	char *sosBuffer, *eosBuffer;
d193 2
a194 2
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,
	    asmap->raidAddress, &which_ru);
d205 1
a205 1
	/* Alloc the nodes: Wnd, xor, commit, block, term, and  Wnp. */
d228 2
a229 2
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h,
	    new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
d237 1
a237 1
	/* Begin node initialization. */
d239 2
a240 3
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h,
		    "Nil", allocList);
d242 2
a243 3
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil",
		    allocList);
d246 4
a249 6
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nWndNodes + nfaults, 1, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0,
	    dag_h, "Trm", allocList);
d251 1
a251 1
	/* Initialize the Rod nodes. */
d256 3
a258 4
				rf_InitNode(&rodNodes[nodeNum], rf_wait,
				    RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
				    1, 1, 4, 0, dag_h, "Rod", allocList);
d262 1
a262 2
				rodNodes[nodeNum].params[3].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d271 1
a271 1
	/* Initialize the wnd nodes. */
d274 2
a275 3
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
d280 1
a280 2
		wndNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d284 1
a284 1
	/* Initialize the redundancy node. */
d286 2
a287 3
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc,
		    rf_NullNodeUndoFunc, NULL, 1, nRodNodes,
		    2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
d290 2
a291 4
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 1,
		    2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
		    "Xr ", allocList);
d295 2
a296 4
		xorNode->params[2 * i + 0] =
		    wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] =
		    wndNodes[i].params[1];	/* buf ptr */
d299 2
a300 4
		xorNode->params[2 * (nWndNodes + i) + 0] =
		    rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] =
		    rodNodes[i].params[1];	/* buf ptr */
d302 1
a302 1
	/* Xor node needs to get at RAID information. */
d306 4
a309 5
	 * Look for an Rod node that reads a complete SU. If none, alloc
	 * a buffer to receive the parity info. Note that we can't use a
	 * new data buffer because it will not have gotten written when
	 * the xor occurs.
	 */
d312 1
a312 2
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)
			    ->numSector == raidPtr->Layout.sectorsPerStripeUnit)
d318 1
a318 2
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
d324 3
a326 4
	/* Initialize the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
d330 2
a331 3
	wnpNode->params[3].v =
	    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit. */
d336 5
a340 5
		 * We never try to recycle a buffer for the Q calculation
		 * in addition to the parity. This would cause two buffers
		 * to get smashed during the P and Q calculation, guaranteeing
		 * one would be wrong.
		 */
d342 1
a342 2
		    rf_RaidAddressToByte(raidPtr,
		     raidPtr->Layout.sectorsPerStripeUnit),
d344 2
a345 3
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnq", allocList);
d349 2
a350 3
		wnqNode->params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit. */
d354 2
a355 2
	 * Connect nodes to form graph.
	 */
d357 1
a357 1
	/* Connect dag header to block node. */
d362 1
a362 1
		/* Connect the block node to the Rod nodes. */
d371 1
a371 1
			/* Connect the Rod nodes to the Xor node. */
d378 1
a378 1
		/* Connect the block node to the Xor node. */
d386 1
a386 1
	/* Connect the xor node to the commit node. */
d393 1
a393 1
	/* Connect the commit node to the write nodes. */
d411 1
a411 1
	/* Connect the write nodes to the term node. */
d431 1
a431 1
/*****************************************************************************
d433 1
a433 1
 * Create a DAG to perform a small-write operation (either raid 5 or pq),
d451 1
a451 1
 * [ ] denotes optional segments in the graph.
d453 7
a459 7
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		pfuncs	  - list of parity generating functions
 *		qfuncs	  - list of q generating functions
d461 1
a461 1
 * A null qfuncs indicates single fault tolerant.
d464 10
a473 4
void
rf_CommonCreateSmallWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_RedFuncs_t *pfuncs, RF_RedFuncs_t *qfuncs)
d479 1
a479 1
	int i, j, nNodes, totalNumNodes, lu_flag;
d481 3
a483 4
	int (*func) (RF_DagNode_t *);
	int (*undoFunc) (RF_DagNode_t *);
	int (*qfunc) (RF_DagNode_t *);
	int numDataNodes, numParityNodes;
d486 2
a487 2
	char *name, *qname;
	long nfaults;
d490 1
a490 1
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* Lock/unlock flag. */
d509 6
a514 6
	 * DAG creation occurs in four steps:
	 * 1. Count the number of nodes in the DAG.
	 * 2. Create the nodes.
	 * 3. Initialize the nodes.
	 * 4. Connect the nodes.
	 */
d517 2
a518 2
	 * Step 1. Compute number of nodes in the graph.
	 */
d520 5
a524 7
	/*
	 * Number of nodes: a read and write for each data unit, a redundancy
	 * computation node for each parity node (nfaults * nparity), a read
	 * and write for each parity unit, a block and commit node (2), a
	 * terminate node if atomic RMW, an unlock node for each
	 * data/redundancy unit.
	 */
d531 2
a532 2
	 * Step 2. Create the nodes.
	 */
d579 3
a581 3
	 * Step 3. Initialize the nodes.
	 */
	/* Initialize block node (Nil). */
d583 10
a592 13
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);

	/* Initialize commit node (Cmt). */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, (nfaults * numParityNodes),
	    0, 0, dag_h, "Cmt", allocList);

	/* Initialize terminate node (Trm). */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h,
	    "Trm", allocList);
d594 1
a594 1
	/* Initialize nodes which read old data (Rod). */
d596 3
a598 4
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    (nfaults * numParityNodes), 1, 4, 0, dag_h, "Rod",
		    allocList);
d600 1
a600 1
		/* Physical disk addr desc. */
d602 1
a602 1
		/* Buffer to hold old data. */
d606 1
a606 2
		readDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d614 1
a614 1
	/* Initialize nodes which read old parity (Rop). */
d619 3
a621 3
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
d623 1
a623 1
		/* Buffer to hold old parity. */
d627 1
a627 2
		readParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d635 1
a635 1
	/* Initialize nodes which read old Q (Roq). */
d640 2
a641 4
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes,
			    1, 4, 0, dag_h, "Roq", allocList);
d643 3
a645 3
			/* Buffer to hold old Q. */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
			    dag_h, pda, allocList);
d647 1
a647 2
			readQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d655 1
a655 1
	/* Initialize nodes which write new data (Wnd). */
d659 2
a660 3
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d662 1
a662 1
		/* Physical disk addr desc. */
d664 1
a664 1
		/* Buffer holding new data to be written. */
d667 2
a668 2
		writeDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d670 3
a672 4
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
d674 1
a674 1
			/* Physical disk addr desc. */
d676 1
a676 2
			unlockDataNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d683 2
a684 2
	 * Initialize nodes which compute new parity and Q.
	 */
d686 9
a694 11
	 * We use the simple XOR func in the double-XOR case, and when
	 * we're accessing only a portion of one stripe unit.
	 * The distinction between the two is that the regular XOR func
	 * assumes that the targbuf is a full SU in size, and examines
	 * the pda associated with the buffer to decide where within
	 * the buffer to XOR the data, whereas the simple XOR func just
	 * XORs the data into the start of the buffer.
	 */
	if ((numParityNodes == 2) || ((numDataNodes == 1) &&
	    (asmap->totalSectorsAccessed <
	     raidPtr->Layout.sectorsPerStripeUnit))) {
d718 3
a720 3
	 * Initialize the xor nodes: params are {pda,buf}.
	 * From {Rod,Wnd,Rop} nodes, and raidPtr.
	 */
d722 1
a722 1
		/* Double-xor case. */
d724 3
a726 4
			/* Note: no wakeup func for xor. */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func,
			    undoFunc, NULL, 1, (numDataNodes + numParityNodes),
			    7, 1, dag_h, name, allocList);
d735 1
a735 1
			/* Use old parity buf as target buf. */
d738 5
a742 9
				/* Note: no wakeup func for qor. */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE,
				    qfunc, undoFunc, NULL, 1,
				    (numDataNodes + numParityNodes), 7, 1,
				    dag_h, qname, allocList);
				qNodes[i].params[0] =
				    readDataNodes[i].params[0];
				qNodes[i].params[1] =
				    readDataNodes[i].params[1];
d745 2
a746 4
				qNodes[i].params[4] =
				    writeDataNodes[i].params[0];
				qNodes[i].params[5] =
				    writeDataNodes[i].params[1];
d748 2
a749 3
				/* Use old Q buf as target buf. */
				qNodes[i].results[0] =
				    readQNodes[i].params[1].p;
d753 4
a756 5
		/* There is only one xor node in this case. */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc,
		    NULL, 1, (numDataNodes + numParityNodes),
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1,
		    dag_h, name, allocList);
d759 3
a761 5
			/* Set up params related to Rod and Rop nodes. */
			xorNodes[0].params[2 * i + 0] =
			    readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] =
			    readDataNodes[i].params[1];	/* buffer ptr */
d764 8
a771 9
			/* Set up params related to Wnd and Wnp nodes. */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =
			    writeDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =
			    writeDataNodes[i].params[1];	/* buffer ptr */
		}
		/* Xor node needs to get at RAID information. */
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p =
		    raidPtr;
d774 4
a777 4
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc,
			    undoFunc, NULL, 1, (numDataNodes + numParityNodes),
			    (2 * (numDataNodes + numDataNodes + 1) + 1), 1,
			    dag_h, qname, allocList);
d779 3
a781 5
				/* Set up params related to Rod. */
				qNodes[0].params[2 * i + 0] =
				    readDataNodes[i].params[0];	/* pda */
				qNodes[0].params[2 * i + 1] =
				    readDataNodes[i].params[1];	/* buffer ptr */
d783 5
a787 5
			/* And read old q. */
			qNodes[0].params[2 * numDataNodes + 0] =
			    readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] =
			    readQNodes[0].params[1];	/* buffer ptr */
d789 2
a790 4
				/* Set up params related to Wnd nodes. */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 0] =
				    /* pda */
d792 1
a792 3
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 1] =
				    /* buffer ptr */
d795 2
a796 3
			/* Xor node needs to get at RAID information. */
			qNodes[0].params
			    [2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;
d801 1
a801 1
	/* Initialize nodes which write new parity (Wnp). */
d804 2
a805 3
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d808 5
a812 4
		/* Param 1 (bufPtr) filled in by xor node. */
		writeParityNodes[i].params[0].p = pda;
		/* Buffer pointer for parity write operation. */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];
d814 2
a815 2
		writeParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d817 3
a819 4
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
d821 3
a823 4
			/* Physical disk addr desc. */
			unlockParityNodes[i].params[0].p = pda;
			unlockParityNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d829 1
a829 1
	/* Initialize nodes which write new Q (Wnq). */
d833 2
a834 3
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d837 5
a841 4
			/* Param 1 (bufPtr) filled in by xor node. */
			writeQNodes[i].params[0].p = pda;
			writeQNodes[i].params[1].p = qNodes[i].results[0];
			/* Buffer pointer for parity write operation. */
d843 1
a843 2
			writeQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d846 7
a852 10
				/* Initialize node to unlock the disk queue. */
				rf_InitNode(&unlockQNodes[i], rf_wait,
				    RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc,
				    rf_GenericWakeupFunc, 1, 1, 2, 0,
				    dag_h, "Unq", allocList);
				/* Physical disk addr desc. */
				unlockQNodes[i].params[0].p = pda;
				unlockQNodes[i].params[1].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d859 2
a860 2
	 * Step 4. Connect the nodes.
	 */
d862 1
a862 1
	/* Connect header to block node. */
d865 2
a866 3
	/* Connect block node to read old data nodes. */
	RF_ASSERT(blockNode->numSuccedents ==
	    (numDataNodes + (numParityNodes * nfaults)));
d874 1
a874 1
	/* Connect block node to read old parity nodes. */
d882 1
a882 1
	/* Connect block node to read old Q nodes. */
d885 1
a885 2
			blockNode->succedents[numDataNodes + numParityNodes + i]
			    = &readQNodes[i];
d891 1
a891 1
	/* Connect read old data nodes to xor nodes. */
d893 1
a893 2
		RF_ASSERT(readDataNodes[i].numSuccedents ==
		    (nfaults * numParityNodes));
d895 1
a895 2
			RF_ASSERT(xorNodes[j].numAntecedents ==
			    numDataNodes + numParityNodes);
d902 1
a902 1
	/* Connect read old data nodes to q nodes. */
d906 2
a907 4
				RF_ASSERT(qNodes[j].numAntecedents ==
				    numDataNodes + numParityNodes);
				readDataNodes[i].succedents[numParityNodes + j]
				    = &qNodes[j];
d913 1
a913 1
	/* Connect read old parity nodes to xor nodes. */
d918 1
a918 2
			xorNodes[j].antecedents[numDataNodes + i] =
			    &readParityNodes[i];
d923 1
a923 1
	/* Connect read old q nodes to q nodes. */
d926 1
a926 2
			RF_ASSERT(readParityNodes[i].numSuccedents ==
			    numParityNodes);
d929 2
a930 4
				qNodes[j].antecedents[numDataNodes + i] =
				    &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] =
				    rf_trueData;
d934 1
a934 1
	/* Connect xor nodes to commit node. */
d943 1
a943 1
	/* Connect q nodes to commit node. */
d948 1
a948 2
			commitNode->antecedents[i + numParityNodes] =
			    &qNodes[i];
d952 2
a953 3
	/* Connect commit node to write nodes. */
	RF_ASSERT(commitNode->numSuccedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
d969 1
a969 3
			commitNode->succedents
			    [i + numDataNodes + numParityNodes] =
			    &writeQNodes[i];
d974 1
a974 2
	RF_ASSERT(termNode->numAntecedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
d978 1
a978 1
			/* Connect write new data nodes to unlock nodes. */
d985 1
a985 1
			/* Connect unlock nodes to term node. */
d991 1
a991 1
			/* Connect write new data nodes to term node. */
d993 1
a993 2
			RF_ASSERT(termNode->numAntecedents ==
			    (numDataNodes + (nfaults * numParityNodes)));
d1002 1
a1002 1
			/* Connect write new parity nodes to unlock nodes. */
d1005 2
a1006 4
			writeParityNodes[i].succedents[0] =
			    &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] =
			    &writeParityNodes[i];
d1009 1
a1009 1
			/* Connect unlock nodes to term node. */
d1012 1
a1012 2
			termNode->antecedents[numDataNodes + i] =
			    &unlockParityNodes[i];
d1017 1
a1017 2
			termNode->antecedents[numDataNodes + i] =
			    &writeParityNodes[i];
d1025 1
a1025 1
				/* Connect write new Q nodes to unlock nodes. */
d1029 1
a1029 2
				unlockQNodes[i].antecedents[0] =
				    &writeQNodes[i];
d1032 1
a1032 1
				/* Connect unlock nodes to unblock node. */
d1035 2
a1036 6
				termNode->antecedents
				    [numDataNodes + numParityNodes + i] =
				    &unlockQNodes[i];
				termNode->antType
				    [numDataNodes + numParityNodes + i] =
				    rf_control;
d1040 2
a1041 6
				termNode->antecedents
				    [numDataNodes + numParityNodes + i] =
				    &writeQNodes[i];
				termNode->antType
				    [numDataNodes + numParityNodes + i] =
				    rf_control;
d1048 2
a1049 2
/*****************************************************************************
 * Create a write graph (fault-free or degraded) for RAID level 1.
d1052 1
a1052 1
 *		 -> Wsd ->
d1054 2
a1055 2
 * The "Wpd" node writes data to the primary copy in the mirror pair.
 * The "Wsd" node writes data to the secondary copy in the mirror pair.
d1057 5
a1061 5
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
d1064 8
a1071 4
void
rf_CreateRaidOneWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d1075 1
a1075 1
	int nWndNodes, nWmirNodes, i;
d1087 1
a1087 1
	/* 2 implies access not SU aligned. */
d1091 1
a1091 1
	/* Alloc the Wnd nodes and the Wmir node. */
d1097 2
a1098 4
	/*
	 * Total number of nodes = nWndNodes + nWmirNodes
	 * + (commit + unblock + terminator)
	 */
d1114 1
a1114 1
	/* This dag can commit immediately. */
d1119 7
a1125 9
	/* Initialize the commit, unblock, and term nodes. */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0,
	    dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d1127 1
a1127 1
	/* Initialize the wnd nodes. */
d1131 2
a1132 4
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wpd", allocList);
d1137 1
a1137 3
			wndNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d1142 1
a1142 1
	/* Initialize the mirror nodes. */
d1147 2
a1148 4
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wsd", allocList);
d1153 1
a1153 3
			wmirNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d1160 1
a1160 1
	/* Link the header node to the commit node. */
d1165 1
a1165 1
	/* Link the commit node to the write nodes. */
d1180 1
a1180 1
	/* Link the write nodes to the unblock node. */
d1195 1
a1195 1
	/* Link the unblock node to the term node. */
d1206 1
a1206 2
/*
 * DAGs that have no commit points.
d1208 2
a1209 3
 * The following DAGs are used in forward and backward error recovery
 * experiments.
 * They are identical to the DAGs above this comment with the exception that
d1214 11
a1224 4
void
rf_CommonCreateLargeWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, int nfaults, int (*redFunc) (RF_DagNode_t *),
d1229 1
a1229 1
	int nWndNodes, nRodNodes, i, nodeNum, asmNum;
d1232 1
a1232 1
	char *sosBuffer, *eosBuffer;
d1238 1
a1238 2
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
d1248 1
a1248 1
	/* Alloc the nodes: Wnd, xor, commit, block, term, and  Wnp. */
d1250 1
a1250 2
	RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
d1270 1
a1270 2
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h,
	    new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
d1272 1
a1272 2
		RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),
		    (RF_DagNode_t *), allocList);
d1277 1
a1277 1
	/* Begin node initialization. */
d1279 2
a1280 6
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h,
		    "Nil", allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0,
		    dag_h, "Nil", allocList);
d1282 2
a1283 6
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil",
		    allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h,
		    "Nil", allocList);
d1286 1
a1286 3
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0,
	    dag_h, "Trm", allocList);
d1288 1
a1288 1
	/* Initialize the Rod nodes. */
d1293 1
a1293 4
				rf_InitNode(&rodNodes[nodeNum], rf_wait,
				    RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
				    1, 1, 4, 0, dag_h, "Rod", allocList);
d1297 1
a1297 3
				rodNodes[nodeNum].params[3].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, 0, which_ru);
d1305 1
a1305 1
	/* Initialize the wnd nodes. */
d1308 1
a1308 3
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
d1313 1
a1313 2
		wndNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1317 2
a1318 4
	/* Initialize the redundancy node. */
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
	    "Xr ", allocList);
d1321 2
a1322 4
		xorNode->params[2 * i + 0] =
		    wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] =
		    wndNodes[i].params[1];	/* buf ptr */
d1325 2
a1326 4
		xorNode->params[2 * (nWndNodes + i) + 0] =
		    rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] =
		    rodNodes[i].params[1];	/* buf ptr */
d1328 2
a1329 2
	/* Xor node needs to get at RAID information. */
	xorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;
d1331 1
a1331 2
	/*
	 * Look for an Rod node that reads a complete SU. If none, alloc a
d1334 1
a1334 2
	 * occurs.
	 */
d1337 1
a1337 2
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)
			    ->numSector == raidPtr->Layout.sectorsPerStripeUnit)
d1341 1
a1341 4
		RF_CallocAndAdd(xorNode->results[0], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
d1345 2
a1346 4
	/* Initialize the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
d1350 4
a1353 4
	wnpNode->params[3].v =
	    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit. */
	RF_ASSERT(asmap->parityInfo->next == NULL);
d1356 6
a1361 13
		/*
		 * Never try to recycle a buffer for the Q calcuation in
		 * addition to the parity. This would cause two buffers to
		 * get smashed during the P and Q calculation, guaranteeing
		 * one would be wrong.
		 */
		RF_CallocAndAdd(xorNode->results[1], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnq", allocList);
d1365 4
a1368 4
		wnqNode->params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit. */
		RF_ASSERT(asmap->parityInfo->next == NULL);
d1370 1
d1372 1
a1372 3
	/* Connect nodes to form graph. */

	/* Connect dag header to block node. */
d1377 1
a1377 1
		/* Connect the block node to the Rod nodes. */
d1386 1
a1386 1
			/* Connect the Rod nodes to the Nil node. */
d1393 1
a1393 1
		/* Connect the block node to the Nil node. */
d1401 1
a1401 1
	/* Connect the sync node to the Wnd nodes. */
d1410 1
a1410 1
	/* Connect the sync node to the Xor node. */
d1416 1
a1416 1
	/* Connect the xor node to the write parity node. */
d1428 1
a1428 1
	/* Connect the write nodes to the term node. */
d1450 1
a1450 1
/*****************************************************************************
d1452 1
a1452 1
 * Create a DAG to perform a small-write operation (either raid 5 or pq),
d1470 1
a1470 1
 * [ ] denotes optional segments in the graph.
d1472 7
a1478 7
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		pfuncs	  - list of parity generating functions
 *		qfuncs	  - list of q generating functions
d1480 1
a1480 1
 * A null qfuncs indicates single fault tolerant.
d1483 10
a1492 4
void
rf_CommonCreateSmallWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_RedFuncs_t *pfuncs, RF_RedFuncs_t *qfuncs)
d1498 1
a1498 1
	int i, j, nNodes, totalNumNodes, lu_flag;
d1500 3
a1502 4
	int (*func) (RF_DagNode_t *);
	int (*undoFunc) (RF_DagNode_t *);
	int (*qfunc) (RF_DagNode_t *);
	int numDataNodes, numParityNodes;
d1505 2
a1506 2
	char *name, *qname;
	long nfaults;
d1509 1
a1509 1
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* Lock/unlock flag. */
d1511 1
a1511 2
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
d1528 11
a1538 18
	/*
	 * DAG creation occurs in four steps:
	 * 1. Count the number of nodes in the DAG.
	 * 2. Create the nodes.
	 * 3. Initialize the nodes.
	 * 4. Connect the nodes.
	 */

	/* Step 1. Compute number of nodes in the graph. */

	/*
	 * Number of nodes: a read and write for each data unit, a redundancy
	 * computation node for each parity node (nfaults * nparity), a read
	 * and write for each parity unit, a block node, a terminate node if
	 * atomic RMW, an unlock node for each data/redundancy unit.
	 */
	totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)
	    + (nfaults * 2 * numParityNodes) + 2;
d1543 2
a1544 3
	/* Step 2. Create the nodes. */
	RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
d1586 2
a1587 2
	/* Step 3. Initialize the nodes. */
	/* Initialize block node (Nil). */
d1589 1
a1589 8
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);

	/* Initialize terminate node (Trm). */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h,
	    "Trm", allocList);
d1591 4
a1594 1
	/* Initialize nodes which read old data (Rod). */
d1596 1
a1596 4
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h,
		    "Rod", allocList);
d1598 4
a1601 5
		/* Physical disk addr desc. */
		readDataNodes[i].params[0].p = pda;
		/* Buffer to hold old data. */
		readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h,
		    pda, allocList);
d1603 1
a1603 3
		readDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
d1609 1
a1609 1
	/* Initialize nodes which read old parity (Rop). */
d1614 1
a1614 3
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
d1616 2
a1617 3
		/* Buffer to hold old parity. */
		readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
		    dag_h, pda, allocList);
d1619 1
a1619 3
		readParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
d1625 1
a1625 1
	/* Initialize nodes which read old Q (Roq). */
d1630 1
a1630 4
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes, 1, 4, 0,
			    dag_h, "Roq", allocList);
d1632 1
a1632 3
			/* Buffer to hold old Q. */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
			    dag_h, pda, allocList);
d1634 1
a1634 3
			readQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    lu_flag, 0, which_ru);
d1640 1
a1640 1
	/* Initialize nodes which write new data (Wnd). */
d1644 5
a1648 8
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
		/* Physical disk addr desc. */
		writeDataNodes[i].params[0].p = pda;
		/* Buffer holding new data to be written. */
		writeDataNodes[i].params[1].p = pda->bufPtr;
d1650 1
a1650 2
		writeDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1653 5
a1657 10
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Und", allocList);
			/* Physical disk addr desc. */
			unlockDataNodes[i].params[0].p = pda;
			unlockDataNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
d1663 3
a1665 4
	/* Initialize nodes which compute new parity and Q. */
	/*
	 * Use the simple XOR func in the double-XOR case, and when
	 * accessing only a portion of one stripe unit. The distinction
d1670 2
a1671 5
	 * the buffer.
	 */
	if ((numParityNodes == 2) || ((numDataNodes == 1) &&
	    (asmap->totalSectorsAccessed <
	     raidPtr->Layout.sectorsPerStripeUnit))) {
d1688 3
a1690 5
	/*
	 * Initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}
	 * nodes, and raidPtr.
	 */
	if (numParityNodes == 2) {	/* Double-xor case. */
d1692 2
a1693 4
			/* No wakeup func for xor. */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func,
			    undoFunc, NULL, numParityNodes, numParityNodes +
			    numDataNodes, 7, 1, dag_h, name, allocList);
d1702 2
a1703 2
			/* Use old parity buf as target buf. */
			xorNodes[i].results[0] = readParityNodes[i].params[1].p;
d1705 4
a1708 9
				/* No wakeup func for xor. */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE,
				    qfunc, undoFunc, NULL, numParityNodes,
				    numParityNodes + numDataNodes, 7, 1,
				    dag_h, qname, allocList);
				qNodes[i].params[0] =
				    readDataNodes[i].params[0];
				qNodes[i].params[1] =
				    readDataNodes[i].params[1];
d1711 2
a1712 4
				qNodes[i].params[4] =
				    writeDataNodes[i].params[0];
				qNodes[i].params[5] =
				    writeDataNodes[i].params[1];
d1714 2
a1715 3
				/* Use old Q buf as target buf. */
				qNodes[i].results[0] =
				    readQNodes[i].params[1].p;
d1719 2
a1720 5
		/* There is only one xor node in this case. */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc,
		    NULL, numParityNodes, numParityNodes + numDataNodes,
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,
		    name, allocList);
d1723 3
a1725 5
			/* Set up params related to Rod and Rop nodes. */
			xorNodes[0].params[2 * i + 0] =
			    readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] =
			    readDataNodes[i].params[1];	/* buffer pointer */
d1728 3
a1730 5
			/* Set up params related to Wnd and Wnp nodes. */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =
			    writeDataNodes[i].params[0]; /* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =
			    writeDataNodes[i].params[1]; /* buffer pointer */
d1732 2
a1733 2
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p =
		    raidPtr;	/* xor node needs to get at RAID information */
d1736 1
a1736 5
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc,
			    undoFunc, NULL, numParityNodes,
			    numParityNodes + numDataNodes,
			    (2 * (numDataNodes + numDataNodes + 1) + 1),
			    1, dag_h, qname, allocList);
d1738 3
a1740 7
				/* Set up params related to Rod. */
				/* pda */
				qNodes[0].params[2 * i + 0] =
				    readDataNodes[i].params[0];
				/* buffer pointer */
				qNodes[0].params[2 * i + 1] =
				    readDataNodes[i].params[1];
d1742 3
a1744 5
			/* And read old q. */
			qNodes[0].params[2 * numDataNodes + 0] =
			    readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] =
			    readQNodes[0].params[1];	/* buffer pointer */
d1746 3
a1748 9
				/* Set up params related to Wnd nodes. */
				/* pda */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 0] =
				    writeDataNodes[i].params[0];
				/* buffer pointer */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 1] =
				    writeDataNodes[i].params[1];
d1750 2
a1751 4
			/* Xor node needs to get at RAID information. */
			qNodes[0].params
			    [2 * (numDataNodes + numDataNodes + 1)].p =
			    raidPtr;
d1756 1
a1756 1
	/* Initialize nodes which write new parity (Wnp). */
d1759 1
a1759 4
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, numParityNodes,
		    4, 0, dag_h, "Wnp", allocList);
d1761 5
a1765 4
		/* Param 1 (bufPtr) filled in by xor node. */
		writeParityNodes[i].params[0].p = pda;
		/* Buffer pointer for parity write operation. */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];
d1767 1
a1767 2
		writeParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1770 5
a1774 10
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Unp", allocList);
			unlockParityNodes[i].params[0].p =
			    pda;	/* Physical disk addr desc. */
			unlockParityNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
d1779 1
a1779 1
	/* Initialize nodes which write new Q (Wnq). */
d1783 1
a1783 4
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, numParityNodes,
			    4, 0, dag_h, "Wnq", allocList);
d1785 5
a1789 4
			/* Param 1 (bufPtr) filled in by xor node. */
			writeQNodes[i].params[0].p = pda;
			/* Buffer pointer for parity write operation. */
			writeQNodes[i].params[1].p = qNodes[i].results[0];
d1791 1
a1791 3
			writeQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d1794 5
a1798 11
				/* Initialize node to unlock the disk queue. */
				rf_InitNode(&unlockQNodes[i], rf_wait,
				    RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc,
				    rf_GenericWakeupFunc, 1, 1, 2, 0,
				    dag_h, "Unq", allocList);
				/* Physical disk addr desc. */
				unlockQNodes[i].params[0].p = pda;
				unlockQNodes[i].params[1].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, lu_flag, which_ru);
d1803 1
a1803 1
	/* Step 4. Connect the nodes. */
d1805 1
a1805 1
	/* Connect header to block node. */
d1808 2
a1809 3
	/* Connect block node to read old data nodes. */
	RF_ASSERT(blockNode->numSuccedents ==
	    (numDataNodes + (numParityNodes * nfaults)));
d1817 1
a1817 1
	/* Connect block node to read old parity nodes. */
d1825 1
a1825 1
	/* Connect block node to read old Q nodes. */
d1828 1
a1828 2
			blockNode->succedents[numDataNodes +
			    numParityNodes + i] = &readQNodes[i];
d1834 1
a1834 1
	/* Connect read old data nodes to write new data nodes. */
d1836 1
a1836 2
		RF_ASSERT(readDataNodes[i].numSuccedents ==
		    ((nfaults * numParityNodes) + 1));
d1843 1
a1843 1
	/* Connect read old data nodes to xor nodes. */
d1846 1
a1846 2
			RF_ASSERT(xorNodes[j].numAntecedents ==
			    numDataNodes + numParityNodes);
d1853 1
a1853 1
	/* Connect read old data nodes to q nodes. */
d1857 2
a1858 4
				RF_ASSERT(qNodes[j].numAntecedents ==
				    numDataNodes + numParityNodes);
				readDataNodes[i].succedents
				    [1 + numParityNodes + j] = &qNodes[j];
d1863 1
a1863 1
	/* Connect read old parity nodes to xor nodes. */
d1866 1
a1866 2
			RF_ASSERT(readParityNodes[i].numSuccedents ==
			    numParityNodes);
d1868 1
a1868 2
			xorNodes[j].antecedents[numDataNodes + i] =
			    &readParityNodes[i];
d1873 1
a1873 1
	/* Connect read old q nodes to q nodes. */
d1877 1
a1877 2
				RF_ASSERT(readQNodes[i].numSuccedents ==
				    numParityNodes);
d1879 2
a1880 4
				qNodes[j].antecedents[numDataNodes + i] =
				    &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] =
				    rf_trueData;
d1884 1
a1884 1
	/* Connect xor nodes to the write new parity nodes. */
d1895 1
a1895 1
	/* Connect q nodes to the write new q nodes. */
d1898 1
a1898 2
			RF_ASSERT(writeQNodes[i].numAntecedents ==
			    numParityNodes);
d1907 1
a1907 2
	RF_ASSERT(termNode->numAntecedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
d1911 1
a1911 1
			/* Connect write new data nodes to unlock nodes. */
d1918 1
a1918 1
			/* Connect unlock nodes to term nodes. */
d1924 1
a1924 1
			/* Connect write new data nodes to term node. */
d1926 1
a1926 2
			RF_ASSERT(termNode->numAntecedents ==
			    (numDataNodes + (nfaults * numParityNodes)));
d1935 1
a1935 1
			/* Connect write new parity nodes to unlock nodes. */
d1938 2
a1939 4
			writeParityNodes[i].succedents[0] =
			    &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] =
			    &writeParityNodes[i];
d1942 1
a1942 1
			/* Connect unlock nodes to term node. */
d1945 1
a1945 2
			termNode->antecedents[numDataNodes + i] =
			    &unlockParityNodes[i];
d1950 1
a1950 2
			termNode->antecedents[numDataNodes + i] =
			    &writeParityNodes[i];
d1958 1
a1958 1
				/* Connect write new Q nodes to unlock nodes. */
d1962 1
a1962 2
				unlockQNodes[i].antecedents[0] =
				    &writeQNodes[i];
d1965 1
a1965 1
				/* Connect unlock nodes to unblock node. */
d1968 2
a1969 4
				termNode->antecedents[numDataNodes +
				    numParityNodes + i] = &unlockQNodes[i];
				termNode->antType[numDataNodes +
				    numParityNodes + i] = rf_control;
d1973 2
a1974 4
				termNode->antecedents[numDataNodes +
				    numParityNodes + i] = &writeQNodes[i];
				termNode->antType[numDataNodes +
				    numParityNodes + i] = rf_control;
d1981 2
a1982 2
/*****************************************************************************
 * Create a write graph (fault-free or degraded) for RAID level 1.
d1985 1
a1985 1
 *	Nil -> Wsd ->
d1987 2
a1988 2
 * The "Wpd" node writes data to the primary copy in the mirror pair.
 * The "Wsd" node writes data to the secondary copy in the mirror pair.
d1990 5
a1994 5
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
d1997 8
a2004 4
void
rf_CreateRaidOneWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d2008 1
a2008 1
	int nWndNodes, nWmirNodes, i;
d2018 2
a2019 2
	/* 2 implies access not SU aligned. */
	nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;
d2022 1
a2022 1
	/* Alloc the Wnd nodes and the Wmir node. */
d2028 3
a2030 6
	/*
	 * Total number of nodes = nWndNodes + nWmirNodes +
	 *			   (block + unblock + terminator)
	 */
	RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3,
	    sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d2044 1
a2044 1
	/* This dag can commit immediately. */
d2049 4
a2052 9
	/* Initialize the unblock and term nodes. */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes),
	    0, 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes),
	    0, 0, dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d2054 1
a2054 1
	/* Initialize the wnd nodes. */
d2058 1
a2058 4
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wpd", allocList);
d2063 1
a2063 3
			wndNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d2068 1
a2068 1
	/* Initialize the mirror nodes. */
d2073 1
a2073 4
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wsd", allocList);
d2078 1
a2078 3
			wmirNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d2085 1
a2085 1
	/* Link the header node to the block node. */
d2090 1
a2090 1
	/* Link the block node to the write nodes. */
d2105 1
a2105 1
	/* Link the write nodes to the unblock node. */
d2120 1
a2120 1
	/* Link the unblock node to the term node. */
@


1.4.2.1
log
@Sync the SMP branch with 3.3
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 1

d33 1
a33 1
 * Code for creating fault-free DAGs.
d48 1
a48 1
/*****************************************************************************
d52 1
a52 1
 * All DAGs in this file use roll-away error recovery. Each DAG has a single
d55 1
a55 1
 * backward through the graph, executing the undo functions. Assuming that
d64 1
a64 1
 *****************************************************************************/
d67 1
a67 1
/*****************************************************************************
d70 1
a70 1
 * DAG creation routines. Additionally, these wrappers enable experimentation
d73 1
a73 2
 *
 *****************************************************************************/
d76 9
a84 4
void
rf_CreateNonRedundantWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_IoType_t type)
d90 9
a98 4
void
rf_CreateRAID0WriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_IoType_t type)
d104 8
a111 4
void
rf_CreateSmallWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d113 3
a115 3
	/* "normal" rollaway. */
	rf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	    allocList, &rf_xorFuncs, NULL);
d118 8
a125 4
void
rf_CreateLargeWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d127 3
a129 3
	/* "normal" rollaway. */
	rf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags,
	    allocList, 1, rf_RegularXorFunc, RF_TRUE);
d133 1
a133 1
/*****************************************************************************
d135 2
a136 3
 * DAG creation code begins here.
 *
 *****************************************************************************/
d139 1
a139 1
/*****************************************************************************
d149 2
a150 2
 * All nodes that are before the commit node (Cmt) are assumed to be atomic
 * and undoable - or - they make no changes to permanent state.
d157 1
a157 1
 * [] denotes optional segments in the graph.
d159 8
a166 8
 * Parameters:  raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		nfaults	  - number of faults array can tolerate
 *			    (equal to # redundancy units in stripe)
 *		redfuncs  - list of redundancy generating functions
d170 10
a179 4
void
rf_CommonCreateLargeWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, int nfaults, int (*redFunc) (RF_DagNode_t *),
d184 1
a184 1
	int nWndNodes, nRodNodes, i, nodeNum, asmNum;
d187 1
a187 1
	char *sosBuffer, *eosBuffer;
d193 2
a194 2
	parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,
	    asmap->raidAddress, &which_ru);
d205 1
a205 1
	/* Alloc the nodes: Wnd, xor, commit, block, term, and  Wnp. */
d228 2
a229 2
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h,
	    new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
d237 1
a237 1
	/* Begin node initialization. */
d239 2
a240 3
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h,
		    "Nil", allocList);
d242 2
a243 3
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil",
		    allocList);
d246 4
a249 6
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nWndNodes + nfaults, 1, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0,
	    dag_h, "Trm", allocList);
d251 1
a251 1
	/* Initialize the Rod nodes. */
d256 3
a258 4
				rf_InitNode(&rodNodes[nodeNum], rf_wait,
				    RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
				    1, 1, 4, 0, dag_h, "Rod", allocList);
d262 1
a262 2
				rodNodes[nodeNum].params[3].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d271 1
a271 1
	/* Initialize the wnd nodes. */
d274 2
a275 3
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
d280 1
a280 2
		wndNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d284 1
a284 1
	/* Initialize the redundancy node. */
d286 2
a287 3
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc,
		    rf_NullNodeUndoFunc, NULL, 1, nRodNodes,
		    2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
d290 2
a291 4
		rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 1,
		    2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
		    "Xr ", allocList);
d295 2
a296 4
		xorNode->params[2 * i + 0] =
		    wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] =
		    wndNodes[i].params[1];	/* buf ptr */
d299 2
a300 4
		xorNode->params[2 * (nWndNodes + i) + 0] =
		    rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] =
		    rodNodes[i].params[1];	/* buf ptr */
d302 1
a302 1
	/* Xor node needs to get at RAID information. */
d306 4
a309 5
	 * Look for an Rod node that reads a complete SU. If none, alloc
	 * a buffer to receive the parity info. Note that we can't use a
	 * new data buffer because it will not have gotten written when
	 * the xor occurs.
	 */
d312 1
a312 2
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)
			    ->numSector == raidPtr->Layout.sectorsPerStripeUnit)
d318 1
a318 2
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
d324 3
a326 4
	/* Initialize the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
d330 2
a331 3
	wnpNode->params[3].v =
	    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit. */
d336 5
a340 5
		 * We never try to recycle a buffer for the Q calculation
		 * in addition to the parity. This would cause two buffers
		 * to get smashed during the P and Q calculation, guaranteeing
		 * one would be wrong.
		 */
d342 1
a342 2
		    rf_RaidAddressToByte(raidPtr,
		     raidPtr->Layout.sectorsPerStripeUnit),
d344 2
a345 3
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnq", allocList);
d349 2
a350 3
		wnqNode->params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit. */
d354 2
a355 2
	 * Connect nodes to form graph.
	 */
d357 1
a357 1
	/* Connect dag header to block node. */
d362 1
a362 1
		/* Connect the block node to the Rod nodes. */
d371 1
a371 1
			/* Connect the Rod nodes to the Xor node. */
d378 1
a378 1
		/* Connect the block node to the Xor node. */
d386 1
a386 1
	/* Connect the xor node to the commit node. */
d393 1
a393 1
	/* Connect the commit node to the write nodes. */
d411 1
a411 1
	/* Connect the write nodes to the term node. */
d431 1
a431 1
/*****************************************************************************
d433 1
a433 1
 * Create a DAG to perform a small-write operation (either raid 5 or pq),
d451 1
a451 1
 * [ ] denotes optional segments in the graph.
d453 7
a459 7
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		pfuncs	  - list of parity generating functions
 *		qfuncs	  - list of q generating functions
d461 1
a461 1
 * A null qfuncs indicates single fault tolerant.
d464 10
a473 4
void
rf_CommonCreateSmallWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_RedFuncs_t *pfuncs, RF_RedFuncs_t *qfuncs)
d479 1
a479 1
	int i, j, nNodes, totalNumNodes, lu_flag;
d481 3
a483 4
	int (*func) (RF_DagNode_t *);
	int (*undoFunc) (RF_DagNode_t *);
	int (*qfunc) (RF_DagNode_t *);
	int numDataNodes, numParityNodes;
d486 2
a487 2
	char *name, *qname;
	long nfaults;
d490 1
a490 1
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* Lock/unlock flag. */
d509 6
a514 6
	 * DAG creation occurs in four steps:
	 * 1. Count the number of nodes in the DAG.
	 * 2. Create the nodes.
	 * 3. Initialize the nodes.
	 * 4. Connect the nodes.
	 */
d517 2
a518 2
	 * Step 1. Compute number of nodes in the graph.
	 */
d520 5
a524 7
	/*
	 * Number of nodes: a read and write for each data unit, a redundancy
	 * computation node for each parity node (nfaults * nparity), a read
	 * and write for each parity unit, a block and commit node (2), a
	 * terminate node if atomic RMW, an unlock node for each
	 * data/redundancy unit.
	 */
d531 2
a532 2
	 * Step 2. Create the nodes.
	 */
d579 3
a581 3
	 * Step 3. Initialize the nodes.
	 */
	/* Initialize block node (Nil). */
d583 10
a592 13
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);

	/* Initialize commit node (Cmt). */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, (nfaults * numParityNodes),
	    0, 0, dag_h, "Cmt", allocList);

	/* Initialize terminate node (Trm). */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h,
	    "Trm", allocList);
d594 1
a594 1
	/* Initialize nodes which read old data (Rod). */
d596 3
a598 4
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    (nfaults * numParityNodes), 1, 4, 0, dag_h, "Rod",
		    allocList);
d600 1
a600 1
		/* Physical disk addr desc. */
d602 1
a602 1
		/* Buffer to hold old data. */
d606 1
a606 2
		readDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d614 1
a614 1
	/* Initialize nodes which read old parity (Rop). */
d619 3
a621 3
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
d623 1
a623 1
		/* Buffer to hold old parity. */
d627 1
a627 2
		readParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d635 1
a635 1
	/* Initialize nodes which read old Q (Roq). */
d640 2
a641 4
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes,
			    1, 4, 0, dag_h, "Roq", allocList);
d643 3
a645 3
			/* Buffer to hold old Q. */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
			    dag_h, pda, allocList);
d647 1
a647 2
			readQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d655 1
a655 1
	/* Initialize nodes which write new data (Wnd). */
d659 2
a660 3
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d662 1
a662 1
		/* Physical disk addr desc. */
d664 1
a664 1
		/* Buffer holding new data to be written. */
d667 2
a668 2
		writeDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d670 3
a672 4
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
d674 1
a674 1
			/* Physical disk addr desc. */
d676 1
a676 2
			unlockDataNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d683 2
a684 2
	 * Initialize nodes which compute new parity and Q.
	 */
d686 9
a694 11
	 * We use the simple XOR func in the double-XOR case, and when
	 * we're accessing only a portion of one stripe unit.
	 * The distinction between the two is that the regular XOR func
	 * assumes that the targbuf is a full SU in size, and examines
	 * the pda associated with the buffer to decide where within
	 * the buffer to XOR the data, whereas the simple XOR func just
	 * XORs the data into the start of the buffer.
	 */
	if ((numParityNodes == 2) || ((numDataNodes == 1) &&
	    (asmap->totalSectorsAccessed <
	     raidPtr->Layout.sectorsPerStripeUnit))) {
d718 3
a720 3
	 * Initialize the xor nodes: params are {pda,buf}.
	 * From {Rod,Wnd,Rop} nodes, and raidPtr.
	 */
d722 1
a722 1
		/* Double-xor case. */
d724 3
a726 4
			/* Note: no wakeup func for xor. */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func,
			    undoFunc, NULL, 1, (numDataNodes + numParityNodes),
			    7, 1, dag_h, name, allocList);
d735 1
a735 1
			/* Use old parity buf as target buf. */
d738 5
a742 9
				/* Note: no wakeup func for qor. */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE,
				    qfunc, undoFunc, NULL, 1,
				    (numDataNodes + numParityNodes), 7, 1,
				    dag_h, qname, allocList);
				qNodes[i].params[0] =
				    readDataNodes[i].params[0];
				qNodes[i].params[1] =
				    readDataNodes[i].params[1];
d745 2
a746 4
				qNodes[i].params[4] =
				    writeDataNodes[i].params[0];
				qNodes[i].params[5] =
				    writeDataNodes[i].params[1];
d748 2
a749 3
				/* Use old Q buf as target buf. */
				qNodes[i].results[0] =
				    readQNodes[i].params[1].p;
d753 4
a756 5
		/* There is only one xor node in this case. */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc,
		    NULL, 1, (numDataNodes + numParityNodes),
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1,
		    dag_h, name, allocList);
d759 3
a761 5
			/* Set up params related to Rod and Rop nodes. */
			xorNodes[0].params[2 * i + 0] =
			    readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] =
			    readDataNodes[i].params[1];	/* buffer ptr */
d764 8
a771 9
			/* Set up params related to Wnd and Wnp nodes. */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =
			    writeDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =
			    writeDataNodes[i].params[1];	/* buffer ptr */
		}
		/* Xor node needs to get at RAID information. */
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p =
		    raidPtr;
d774 4
a777 4
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc,
			    undoFunc, NULL, 1, (numDataNodes + numParityNodes),
			    (2 * (numDataNodes + numDataNodes + 1) + 1), 1,
			    dag_h, qname, allocList);
d779 3
a781 5
				/* Set up params related to Rod. */
				qNodes[0].params[2 * i + 0] =
				    readDataNodes[i].params[0];	/* pda */
				qNodes[0].params[2 * i + 1] =
				    readDataNodes[i].params[1];	/* buffer ptr */
d783 5
a787 5
			/* And read old q. */
			qNodes[0].params[2 * numDataNodes + 0] =
			    readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] =
			    readQNodes[0].params[1];	/* buffer ptr */
d789 2
a790 4
				/* Set up params related to Wnd nodes. */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 0] =
				    /* pda */
d792 1
a792 3
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 1] =
				    /* buffer ptr */
d795 2
a796 3
			/* Xor node needs to get at RAID information. */
			qNodes[0].params
			    [2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;
d801 1
a801 1
	/* Initialize nodes which write new parity (Wnp). */
d804 2
a805 3
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d808 5
a812 4
		/* Param 1 (bufPtr) filled in by xor node. */
		writeParityNodes[i].params[0].p = pda;
		/* Buffer pointer for parity write operation. */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];
d814 2
a815 2
		writeParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d817 3
a819 4
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
d821 3
a823 4
			/* Physical disk addr desc. */
			unlockParityNodes[i].params[0].p = pda;
			unlockParityNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d829 1
a829 1
	/* Initialize nodes which write new Q (Wnq). */
d833 2
a834 3
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
d837 5
a841 4
			/* Param 1 (bufPtr) filled in by xor node. */
			writeQNodes[i].params[0].p = pda;
			writeQNodes[i].params[1].p = qNodes[i].results[0];
			/* Buffer pointer for parity write operation. */
d843 1
a843 2
			writeQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d846 7
a852 10
				/* Initialize node to unlock the disk queue. */
				rf_InitNode(&unlockQNodes[i], rf_wait,
				    RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc,
				    rf_GenericWakeupFunc, 1, 1, 2, 0,
				    dag_h, "Unq", allocList);
				/* Physical disk addr desc. */
				unlockQNodes[i].params[0].p = pda;
				unlockQNodes[i].params[1].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
d859 2
a860 2
	 * Step 4. Connect the nodes.
	 */
d862 1
a862 1
	/* Connect header to block node. */
d865 2
a866 3
	/* Connect block node to read old data nodes. */
	RF_ASSERT(blockNode->numSuccedents ==
	    (numDataNodes + (numParityNodes * nfaults)));
d874 1
a874 1
	/* Connect block node to read old parity nodes. */
d882 1
a882 1
	/* Connect block node to read old Q nodes. */
d885 1
a885 2
			blockNode->succedents[numDataNodes + numParityNodes + i]
			    = &readQNodes[i];
d891 1
a891 1
	/* Connect read old data nodes to xor nodes. */
d893 1
a893 2
		RF_ASSERT(readDataNodes[i].numSuccedents ==
		    (nfaults * numParityNodes));
d895 1
a895 2
			RF_ASSERT(xorNodes[j].numAntecedents ==
			    numDataNodes + numParityNodes);
d902 1
a902 1
	/* Connect read old data nodes to q nodes. */
d906 2
a907 4
				RF_ASSERT(qNodes[j].numAntecedents ==
				    numDataNodes + numParityNodes);
				readDataNodes[i].succedents[numParityNodes + j]
				    = &qNodes[j];
d913 1
a913 1
	/* Connect read old parity nodes to xor nodes. */
d918 1
a918 2
			xorNodes[j].antecedents[numDataNodes + i] =
			    &readParityNodes[i];
d923 1
a923 1
	/* Connect read old q nodes to q nodes. */
d926 1
a926 2
			RF_ASSERT(readParityNodes[i].numSuccedents ==
			    numParityNodes);
d929 2
a930 4
				qNodes[j].antecedents[numDataNodes + i] =
				    &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] =
				    rf_trueData;
d934 1
a934 1
	/* Connect xor nodes to commit node. */
d943 1
a943 1
	/* Connect q nodes to commit node. */
d948 1
a948 2
			commitNode->antecedents[i + numParityNodes] =
			    &qNodes[i];
d952 2
a953 3
	/* Connect commit node to write nodes. */
	RF_ASSERT(commitNode->numSuccedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
d969 1
a969 3
			commitNode->succedents
			    [i + numDataNodes + numParityNodes] =
			    &writeQNodes[i];
d974 1
a974 2
	RF_ASSERT(termNode->numAntecedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
d978 1
a978 1
			/* Connect write new data nodes to unlock nodes. */
d985 1
a985 1
			/* Connect unlock nodes to term node. */
d991 1
a991 1
			/* Connect write new data nodes to term node. */
d993 1
a993 2
			RF_ASSERT(termNode->numAntecedents ==
			    (numDataNodes + (nfaults * numParityNodes)));
d1002 1
a1002 1
			/* Connect write new parity nodes to unlock nodes. */
d1005 2
a1006 4
			writeParityNodes[i].succedents[0] =
			    &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] =
			    &writeParityNodes[i];
d1009 1
a1009 1
			/* Connect unlock nodes to term node. */
d1012 1
a1012 2
			termNode->antecedents[numDataNodes + i] =
			    &unlockParityNodes[i];
d1017 1
a1017 2
			termNode->antecedents[numDataNodes + i] =
			    &writeParityNodes[i];
d1025 1
a1025 1
				/* Connect write new Q nodes to unlock nodes. */
d1029 1
a1029 2
				unlockQNodes[i].antecedents[0] =
				    &writeQNodes[i];
d1032 1
a1032 1
				/* Connect unlock nodes to unblock node. */
d1035 2
a1036 6
				termNode->antecedents
				    [numDataNodes + numParityNodes + i] =
				    &unlockQNodes[i];
				termNode->antType
				    [numDataNodes + numParityNodes + i] =
				    rf_control;
d1040 2
a1041 6
				termNode->antecedents
				    [numDataNodes + numParityNodes + i] =
				    &writeQNodes[i];
				termNode->antType
				    [numDataNodes + numParityNodes + i] =
				    rf_control;
d1048 2
a1049 2
/*****************************************************************************
 * Create a write graph (fault-free or degraded) for RAID level 1.
d1052 1
a1052 1
 *		 -> Wsd ->
d1054 2
a1055 2
 * The "Wpd" node writes data to the primary copy in the mirror pair.
 * The "Wsd" node writes data to the secondary copy in the mirror pair.
d1057 5
a1061 5
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
d1064 8
a1071 4
void
rf_CreateRaidOneWriteDAG(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d1075 1
a1075 1
	int nWndNodes, nWmirNodes, i;
d1087 1
a1087 1
	/* 2 implies access not SU aligned. */
d1091 1
a1091 1
	/* Alloc the Wnd nodes and the Wmir node. */
d1097 2
a1098 4
	/*
	 * Total number of nodes = nWndNodes + nWmirNodes
	 * + (commit + unblock + terminator)
	 */
d1114 1
a1114 1
	/* This dag can commit immediately. */
d1119 7
a1125 9
	/* Initialize the commit, unblock, and term nodes. */
	rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0,
	    dag_h, "Cmt", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0,
	    dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d1127 1
a1127 1
	/* Initialize the wnd nodes. */
d1131 2
a1132 4
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wpd", allocList);
d1137 1
a1137 3
			wndNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d1142 1
a1142 1
	/* Initialize the mirror nodes. */
d1147 2
a1148 4
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wsd", allocList);
d1153 1
a1153 3
			wmirNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d1160 1
a1160 1
	/* Link the header node to the commit node. */
d1165 1
a1165 1
	/* Link the commit node to the write nodes. */
d1180 1
a1180 1
	/* Link the write nodes to the unblock node. */
d1195 1
a1195 1
	/* Link the unblock node to the term node. */
d1206 1
a1206 2
/*
 * DAGs that have no commit points.
d1208 2
a1209 3
 * The following DAGs are used in forward and backward error recovery
 * experiments.
 * They are identical to the DAGs above this comment with the exception that
d1214 11
a1224 4
void
rf_CommonCreateLargeWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, int nfaults, int (*redFunc) (RF_DagNode_t *),
d1229 1
a1229 1
	int nWndNodes, nRodNodes, i, nodeNum, asmNum;
d1232 1
a1232 1
	char *sosBuffer, *eosBuffer;
d1238 1
a1238 2
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
d1248 1
a1248 1
	/* Alloc the nodes: Wnd, xor, commit, block, term, and  Wnp. */
d1250 1
a1250 2
	RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
d1270 1
a1270 2
	rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h,
	    new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
d1272 1
a1272 2
		RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),
		    (RF_DagNode_t *), allocList);
d1277 1
a1277 1
	/* Begin node initialization. */
d1279 2
a1280 6
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h,
		    "Nil", allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0,
		    dag_h, "Nil", allocList);
d1282 2
a1283 6
		rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil",
		    allocList);
		rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
		    rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h,
		    "Nil", allocList);
d1286 1
a1286 3
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0,
	    dag_h, "Trm", allocList);
d1288 1
a1288 1
	/* Initialize the Rod nodes. */
d1293 1
a1293 4
				rf_InitNode(&rodNodes[nodeNum], rf_wait,
				    RF_FALSE, rf_DiskReadFunc,
				    rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
				    1, 1, 4, 0, dag_h, "Rod", allocList);
d1297 1
a1297 3
				rodNodes[nodeNum].params[3].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, 0, which_ru);
d1305 1
a1305 1
	/* Initialize the wnd nodes. */
d1308 1
a1308 3
		rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
d1313 1
a1313 2
		wndNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1317 2
a1318 4
	/* Initialize the redundancy node. */
	rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc,
	    NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,
	    "Xr ", allocList);
d1321 2
a1322 4
		xorNode->params[2 * i + 0] =
		    wndNodes[i].params[0];	/* pda */
		xorNode->params[2 * i + 1] =
		    wndNodes[i].params[1];	/* buf ptr */
d1325 2
a1326 4
		xorNode->params[2 * (nWndNodes + i) + 0] =
		    rodNodes[i].params[0];	/* pda */
		xorNode->params[2 * (nWndNodes + i) + 1] =
		    rodNodes[i].params[1];	/* buf ptr */
d1328 2
a1329 2
	/* Xor node needs to get at RAID information. */
	xorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;
d1331 1
a1331 2
	/*
	 * Look for an Rod node that reads a complete SU. If none, alloc a
d1334 1
a1334 2
	 * occurs.
	 */
d1337 1
a1337 2
			if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)
			    ->numSector == raidPtr->Layout.sectorsPerStripeUnit)
d1341 1
a1341 4
		RF_CallocAndAdd(xorNode->results[0], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
d1345 2
a1346 4
	/* Initialize the Wnp node. */
	rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
	    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
	    dag_h, "Wnp", allocList);
d1350 4
a1353 4
	wnpNode->params[3].v =
	    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	/* parityInfo must describe entire parity unit. */
	RF_ASSERT(asmap->parityInfo->next == NULL);
d1356 6
a1361 13
		/*
		 * Never try to recycle a buffer for the Q calcuation in
		 * addition to the parity. This would cause two buffers to
		 * get smashed during the P and Q calculation, guaranteeing
		 * one would be wrong.
		 */
		RF_CallocAndAdd(xorNode->results[1], 1,
		    rf_RaidAddressToByte(raidPtr,
		    raidPtr->Layout.sectorsPerStripeUnit),
		    (void *), allocList);
		rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc,
		    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnq", allocList);
d1365 4
a1368 4
		wnqNode->params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
		/* parityInfo must describe entire parity unit. */
		RF_ASSERT(asmap->parityInfo->next == NULL);
d1370 1
d1372 1
a1372 3
	/* Connect nodes to form graph. */

	/* Connect dag header to block node. */
d1377 1
a1377 1
		/* Connect the block node to the Rod nodes. */
d1386 1
a1386 1
			/* Connect the Rod nodes to the Nil node. */
d1393 1
a1393 1
		/* Connect the block node to the Nil node. */
d1401 1
a1401 1
	/* Connect the sync node to the Wnd nodes. */
d1410 1
a1410 1
	/* Connect the sync node to the Xor node. */
d1416 1
a1416 1
	/* Connect the xor node to the write parity node. */
d1428 1
a1428 1
	/* Connect the write nodes to the term node. */
d1450 1
a1450 1
/*****************************************************************************
d1452 1
a1452 1
 * Create a DAG to perform a small-write operation (either raid 5 or pq),
d1470 1
a1470 1
 * [ ] denotes optional segments in the graph.
d1472 7
a1478 7
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
 *		pfuncs	  - list of parity generating functions
 *		qfuncs	  - list of q generating functions
d1480 1
a1480 1
 * A null qfuncs indicates single fault tolerant.
d1483 10
a1492 4
void
rf_CommonCreateSmallWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList, RF_RedFuncs_t *pfuncs, RF_RedFuncs_t *qfuncs)
d1498 1
a1498 1
	int i, j, nNodes, totalNumNodes, lu_flag;
d1500 3
a1502 4
	int (*func) (RF_DagNode_t *);
	int (*undoFunc) (RF_DagNode_t *);
	int (*qfunc) (RF_DagNode_t *);
	int numDataNodes, numParityNodes;
d1505 2
a1506 2
	char *name, *qname;
	long nfaults;
d1509 1
a1509 1
	lu_flag = (rf_enableAtomicRMW) ? 1 : 0;	/* Lock/unlock flag. */
d1511 1
a1511 2
	parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
	    asmap->raidAddress, &which_ru);
d1528 11
a1538 18
	/*
	 * DAG creation occurs in four steps:
	 * 1. Count the number of nodes in the DAG.
	 * 2. Create the nodes.
	 * 3. Initialize the nodes.
	 * 4. Connect the nodes.
	 */

	/* Step 1. Compute number of nodes in the graph. */

	/*
	 * Number of nodes: a read and write for each data unit, a redundancy
	 * computation node for each parity node (nfaults * nparity), a read
	 * and write for each parity unit, a block node, a terminate node if
	 * atomic RMW, an unlock node for each data/redundancy unit.
	 */
	totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)
	    + (nfaults * 2 * numParityNodes) + 2;
d1543 2
a1544 3
	/* Step 2. Create the nodes. */
	RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),
	    (RF_DagNode_t *), allocList);
d1586 2
a1587 2
	/* Step 3. Initialize the nodes. */
	/* Initialize block node (Nil). */
d1589 1
a1589 8
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h,
	    "Nil", allocList);

	/* Initialize terminate node (Trm). */
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h,
	    "Trm", allocList);
d1591 4
a1594 1
	/* Initialize nodes which read old data (Rod). */
d1596 1
a1596 4
		rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h,
		    "Rod", allocList);
d1598 4
a1601 5
		/* Physical disk addr desc. */
		readDataNodes[i].params[0].p = pda;
		/* Buffer to hold old data. */
		readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h,
		    pda, allocList);
d1603 1
a1603 3
		readDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
d1609 1
a1609 1
	/* Initialize nodes which read old parity (Rop). */
d1614 1
a1614 3
		rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc,
		    numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
d1616 2
a1617 3
		/* Buffer to hold old parity. */
		readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
		    dag_h, pda, allocList);
d1619 1
a1619 3
		readParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
		    lu_flag, 0, which_ru);
d1625 1
a1625 1
	/* Initialize nodes which read old Q (Roq). */
d1630 1
a1630 4
			rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskReadFunc, rf_DiskReadUndoFunc,
			    rf_GenericWakeupFunc, numParityNodes, 1, 4, 0,
			    dag_h, "Roq", allocList);
d1632 1
a1632 3
			/* Buffer to hold old Q. */
			readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
			    dag_h, pda, allocList);
d1634 1
a1634 3
			readQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    lu_flag, 0, which_ru);
d1640 1
a1640 1
	/* Initialize nodes which write new data (Wnd). */
d1644 5
a1648 8
		rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, 1, 4, 0,
		    dag_h, "Wnd", allocList);
		/* Physical disk addr desc. */
		writeDataNodes[i].params[0].p = pda;
		/* Buffer holding new data to be written. */
		writeDataNodes[i].params[1].p = pda->bufPtr;
d1650 1
a1650 2
		writeDataNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1653 5
a1657 10
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Und", allocList);
			/* Physical disk addr desc. */
			unlockDataNodes[i].params[0].p = pda;
			unlockDataNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
d1663 3
a1665 4
	/* Initialize nodes which compute new parity and Q. */
	/*
	 * Use the simple XOR func in the double-XOR case, and when
	 * accessing only a portion of one stripe unit. The distinction
d1670 2
a1671 5
	 * the buffer.
	 */
	if ((numParityNodes == 2) || ((numDataNodes == 1) &&
	    (asmap->totalSectorsAccessed <
	     raidPtr->Layout.sectorsPerStripeUnit))) {
d1688 3
a1690 5
	/*
	 * Initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}
	 * nodes, and raidPtr.
	 */
	if (numParityNodes == 2) {	/* Double-xor case. */
d1692 2
a1693 4
			/* No wakeup func for xor. */
			rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func,
			    undoFunc, NULL, numParityNodes, numParityNodes +
			    numDataNodes, 7, 1, dag_h, name, allocList);
d1702 2
a1703 2
			/* Use old parity buf as target buf. */
			xorNodes[i].results[0] = readParityNodes[i].params[1].p;
d1705 4
a1708 9
				/* No wakeup func for xor. */
				rf_InitNode(&qNodes[i], rf_wait, RF_FALSE,
				    qfunc, undoFunc, NULL, numParityNodes,
				    numParityNodes + numDataNodes, 7, 1,
				    dag_h, qname, allocList);
				qNodes[i].params[0] =
				    readDataNodes[i].params[0];
				qNodes[i].params[1] =
				    readDataNodes[i].params[1];
d1711 2
a1712 4
				qNodes[i].params[4] =
				    writeDataNodes[i].params[0];
				qNodes[i].params[5] =
				    writeDataNodes[i].params[1];
d1714 2
a1715 3
				/* Use old Q buf as target buf. */
				qNodes[i].results[0] =
				    readQNodes[i].params[1].p;
d1719 2
a1720 5
		/* There is only one xor node in this case. */
		rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc,
		    NULL, numParityNodes, numParityNodes + numDataNodes,
		    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,
		    name, allocList);
d1723 3
a1725 5
			/* Set up params related to Rod and Rop nodes. */
			xorNodes[0].params[2 * i + 0] =
			    readDataNodes[i].params[0];	/* pda */
			xorNodes[0].params[2 * i + 1] =
			    readDataNodes[i].params[1];	/* buffer pointer */
d1728 3
a1730 5
			/* Set up params related to Wnd and Wnp nodes. */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =
			    writeDataNodes[i].params[0]; /* pda */
			xorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =
			    writeDataNodes[i].params[1]; /* buffer pointer */
d1732 2
a1733 2
		xorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p =
		    raidPtr;	/* xor node needs to get at RAID information */
d1736 1
a1736 5
			rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc,
			    undoFunc, NULL, numParityNodes,
			    numParityNodes + numDataNodes,
			    (2 * (numDataNodes + numDataNodes + 1) + 1),
			    1, dag_h, qname, allocList);
d1738 3
a1740 7
				/* Set up params related to Rod. */
				/* pda */
				qNodes[0].params[2 * i + 0] =
				    readDataNodes[i].params[0];
				/* buffer pointer */
				qNodes[0].params[2 * i + 1] =
				    readDataNodes[i].params[1];
d1742 3
a1744 5
			/* And read old q. */
			qNodes[0].params[2 * numDataNodes + 0] =
			    readQNodes[0].params[0];	/* pda */
			qNodes[0].params[2 * numDataNodes + 1] =
			    readQNodes[0].params[1];	/* buffer pointer */
d1746 3
a1748 9
				/* Set up params related to Wnd nodes. */
				/* pda */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 0] =
				    writeDataNodes[i].params[0];
				/* buffer pointer */
				qNodes[0].params
				    [2 * (numDataNodes + 1 + i) + 1] =
				    writeDataNodes[i].params[1];
d1750 2
a1751 4
			/* Xor node needs to get at RAID information. */
			qNodes[0].params
			    [2 * (numDataNodes + numDataNodes + 1)].p =
			    raidPtr;
d1756 1
a1756 1
	/* Initialize nodes which write new parity (Wnp). */
d1759 1
a1759 4
		rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE,
		    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
		    rf_GenericWakeupFunc, 1, numParityNodes,
		    4, 0, dag_h, "Wnp", allocList);
d1761 5
a1765 4
		/* Param 1 (bufPtr) filled in by xor node. */
		writeParityNodes[i].params[0].p = pda;
		/* Buffer pointer for parity write operation. */
		writeParityNodes[i].params[1].p = xorNodes[i].results[0];
d1767 1
a1767 2
		writeParityNodes[i].params[3].v =
		    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1770 5
a1774 10
			/* Initialize node to unlock the disk queue. */
			rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE,
			    rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
			    "Unp", allocList);
			unlockParityNodes[i].params[0].p =
			    pda;	/* Physical disk addr desc. */
			unlockParityNodes[i].params[1].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, lu_flag, which_ru);
d1779 1
a1779 1
	/* Initialize nodes which write new Q (Wnq). */
d1783 1
a1783 4
			rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, numParityNodes,
			    4, 0, dag_h, "Wnq", allocList);
d1785 5
a1789 4
			/* Param 1 (bufPtr) filled in by xor node. */
			writeQNodes[i].params[0].p = pda;
			/* Buffer pointer for parity write operation. */
			writeQNodes[i].params[1].p = qNodes[i].results[0];
d1791 1
a1791 3
			writeQNodes[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d1794 5
a1798 11
				/* Initialize node to unlock the disk queue. */
				rf_InitNode(&unlockQNodes[i], rf_wait,
				    RF_FALSE, rf_DiskUnlockFunc,
				    rf_DiskUnlockUndoFunc,
				    rf_GenericWakeupFunc, 1, 1, 2, 0,
				    dag_h, "Unq", allocList);
				/* Physical disk addr desc. */
				unlockQNodes[i].params[0].p = pda;
				unlockQNodes[i].params[1].v =
				    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
				    0, lu_flag, which_ru);
d1803 1
a1803 1
	/* Step 4. Connect the nodes. */
d1805 1
a1805 1
	/* Connect header to block node. */
d1808 2
a1809 3
	/* Connect block node to read old data nodes. */
	RF_ASSERT(blockNode->numSuccedents ==
	    (numDataNodes + (numParityNodes * nfaults)));
d1817 1
a1817 1
	/* Connect block node to read old parity nodes. */
d1825 1
a1825 1
	/* Connect block node to read old Q nodes. */
d1828 1
a1828 2
			blockNode->succedents[numDataNodes +
			    numParityNodes + i] = &readQNodes[i];
d1834 1
a1834 1
	/* Connect read old data nodes to write new data nodes. */
d1836 1
a1836 2
		RF_ASSERT(readDataNodes[i].numSuccedents ==
		    ((nfaults * numParityNodes) + 1));
d1843 1
a1843 1
	/* Connect read old data nodes to xor nodes. */
d1846 1
a1846 2
			RF_ASSERT(xorNodes[j].numAntecedents ==
			    numDataNodes + numParityNodes);
d1853 1
a1853 1
	/* Connect read old data nodes to q nodes. */
d1857 2
a1858 4
				RF_ASSERT(qNodes[j].numAntecedents ==
				    numDataNodes + numParityNodes);
				readDataNodes[i].succedents
				    [1 + numParityNodes + j] = &qNodes[j];
d1863 1
a1863 1
	/* Connect read old parity nodes to xor nodes. */
d1866 1
a1866 2
			RF_ASSERT(readParityNodes[i].numSuccedents ==
			    numParityNodes);
d1868 1
a1868 2
			xorNodes[j].antecedents[numDataNodes + i] =
			    &readParityNodes[i];
d1873 1
a1873 1
	/* Connect read old q nodes to q nodes. */
d1877 1
a1877 2
				RF_ASSERT(readQNodes[i].numSuccedents ==
				    numParityNodes);
d1879 2
a1880 4
				qNodes[j].antecedents[numDataNodes + i] =
				    &readQNodes[i];
				qNodes[j].antType[numDataNodes + i] =
				    rf_trueData;
d1884 1
a1884 1
	/* Connect xor nodes to the write new parity nodes. */
d1895 1
a1895 1
	/* Connect q nodes to the write new q nodes. */
d1898 1
a1898 2
			RF_ASSERT(writeQNodes[i].numAntecedents ==
			    numParityNodes);
d1907 1
a1907 2
	RF_ASSERT(termNode->numAntecedents ==
	    (numDataNodes + (nfaults * numParityNodes)));
d1911 1
a1911 1
			/* Connect write new data nodes to unlock nodes. */
d1918 1
a1918 1
			/* Connect unlock nodes to term nodes. */
d1924 1
a1924 1
			/* Connect write new data nodes to term node. */
d1926 1
a1926 2
			RF_ASSERT(termNode->numAntecedents ==
			    (numDataNodes + (nfaults * numParityNodes)));
d1935 1
a1935 1
			/* Connect write new parity nodes to unlock nodes. */
d1938 2
a1939 4
			writeParityNodes[i].succedents[0] =
			    &unlockParityNodes[i];
			unlockParityNodes[i].antecedents[0] =
			    &writeParityNodes[i];
d1942 1
a1942 1
			/* Connect unlock nodes to term node. */
d1945 1
a1945 2
			termNode->antecedents[numDataNodes + i] =
			    &unlockParityNodes[i];
d1950 1
a1950 2
			termNode->antecedents[numDataNodes + i] =
			    &writeParityNodes[i];
d1958 1
a1958 1
				/* Connect write new Q nodes to unlock nodes. */
d1962 1
a1962 2
				unlockQNodes[i].antecedents[0] =
				    &writeQNodes[i];
d1965 1
a1965 1
				/* Connect unlock nodes to unblock node. */
d1968 2
a1969 4
				termNode->antecedents[numDataNodes +
				    numParityNodes + i] = &unlockQNodes[i];
				termNode->antType[numDataNodes +
				    numParityNodes + i] = rf_control;
d1973 2
a1974 4
				termNode->antecedents[numDataNodes +
				    numParityNodes + i] = &writeQNodes[i];
				termNode->antType[numDataNodes +
				    numParityNodes + i] = rf_control;
d1981 2
a1982 2
/*****************************************************************************
 * Create a write graph (fault-free or degraded) for RAID level 1.
d1985 1
a1985 1
 *	Nil -> Wsd ->
d1987 2
a1988 2
 * The "Wpd" node writes data to the primary copy in the mirror pair.
 * The "Wsd" node writes data to the secondary copy in the mirror pair.
d1990 5
a1994 5
 * Parameters:	raidPtr	  - description of the physical array
 *		asmap	  - logical & physical addresses for this access
 *		bp	  - buffer ptr (holds write data)
 *		flags	  - general flags (e.g. disk locking)
 *		allocList - list of memory allocated in DAG creation
d1997 8
a2004 4
void
rf_CreateRaidOneWriteDAGFwd(RF_Raid_t *raidPtr, RF_AccessStripeMap_t *asmap,
    RF_DagHeader_t *dag_h, void *bp, RF_RaidAccessFlags_t flags,
    RF_AllocListElem_t *allocList)
d2008 1
a2008 1
	int nWndNodes, nWmirNodes, i;
d2018 2
a2019 2
	/* 2 implies access not SU aligned. */
	nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;
d2022 1
a2022 1
	/* Alloc the Wnd nodes and the Wmir node. */
d2028 3
a2030 6
	/*
	 * Total number of nodes = nWndNodes + nWmirNodes +
	 *			   (block + unblock + terminator)
	 */
	RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3,
	    sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
d2044 1
a2044 1
	/* This dag can commit immediately. */
d2049 4
a2052 9
	/* Initialize the unblock and term nodes. */
	rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes),
	    0, 0, 0, dag_h, "Nil", allocList);
	rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc,
	    rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes),
	    0, 0, dag_h, "Nil", allocList);
	rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc,
	    rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);
d2054 1
a2054 1
	/* Initialize the wnd nodes. */
d2058 1
a2058 4
			rf_InitNode(&wndNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wpd", allocList);
d2063 1
a2063 3
			wndNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d2068 1
a2068 1
	/* Initialize the mirror nodes. */
d2073 1
a2073 4
			rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE,
			    rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
			    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
			    "Wsd", allocList);
d2078 1
a2078 3
			wmirNode[i].params[3].v =
			    RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
			    0, 0, which_ru);
d2085 1
a2085 1
	/* Link the header node to the block node. */
d2090 1
a2090 1
	/* Link the block node to the write nodes. */
d2105 1
a2105 1
	/* Link the write nodes to the unblock node. */
d2120 1
a2120 1
	/* Link the unblock node to the term node. */
@


1.3
log
@sync with work by Greg Oster on NetBSD

Please note: This update has *only* been tested on i386 with IDE
disks. Could someone with a spare box please make sure all is OK with
SCSI and maybe other arches ? sparc testing will follow locally.

* remove rf_sys.h
* many changes to make it more stable
* some performance increases
* All raid threads now get their own kernel process and the calling
  raidctl(8) program will show status progress through a meter.
* In theory FFS_SOFTUPDATES and RAIDframe will now work together - NOT
  TESTED YET

See http://www.cs.usask.ca/staff/oster/raid.html

This updates include Greg's changes to Jan 4th 2000.

TODO:
* some odd behaviour when running raictl -c on an already config'ed
  raid set - problem founf, fix being done
* progress meter is in raidctl(8) - seperate commit, but could do with
  sync'ing with OpenBSD ftp version
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_dagffwr.c,v 1.2 1999/02/16 00:02:31 niklas Exp $	*/
/*	$NetBSD: rf_dagffwr.c,v 1.4 1999/08/26 02:40:28 oster Exp $	*/
a41 1
#include "rf_threadid.h"
@


1.2
log
@Merge from NetBSD, mostly indentation
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_dagffwr.c,v 1.1 1999/01/11 14:29:09 niklas Exp $	*/
/*	$NetBSD: rf_dagffwr.c,v 1.3 1999/02/05 00:06:07 oster Exp $	*/
a113 8
#if RF_FORWARD > 0
	rf_CommonCreateSmallWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
	    &rf_xorFuncs, NULL);
#else				/* RF_FORWARD > 0 */
#if RF_BACKWARD > 0
	rf_CommonCreateSmallWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
	    &rf_xorFuncs, NULL);
#else				/* RF_BACKWARD > 0 */
a116 2
#endif				/* RF_BACKWARD > 0 */
#endif				/* RF_FORWARD > 0 */
a127 8
#if RF_FORWARD > 0
	rf_CommonCreateLargeWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
	    1, rf_RegularXorFunc, RF_TRUE);
#else				/* RF_FORWARD > 0 */
#if RF_BACKWARD > 0
	rf_CommonCreateLargeWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
	    1, rf_RegularXorFunc, RF_TRUE);
#else				/* RF_BACKWARD > 0 */
a130 2
#endif				/* RF_BACKWARD > 0 */
#endif				/* RF_FORWARD > 0 */
@


1.1
log
@Import of CMU's RAIDframe via NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: rf_dagffwr.c,v 1.1 1998/11/13 04:20:27 oster Exp $	*/
/*	$NetBSD: rf_dagffwr.c,v 1.1 1998/11/13 04:20:27 oster Exp $	*/
a34 94
 * :  
 * Log: rf_dagffwr.c,v 
 * Revision 1.19  1996/07/31 15:35:24  jimz
 * evenodd changes; bugfixes for double-degraded archs, generalize
 * some formerly PQ-only functions
 *
 * Revision 1.18  1996/07/28  20:31:39  jimz
 * i386netbsd port
 * true/false fixup
 *
 * Revision 1.17  1996/07/27  18:40:24  jimz
 * cleanup sweep
 *
 * Revision 1.16  1996/07/22  19:52:16  jimz
 * switched node params to RF_DagParam_t, a union of
 * a 64-bit int and a void *, for better portability
 * attempted hpux port, but failed partway through for
 * lack of a single C compiler capable of compiling all
 * source files
 *
 * Revision 1.15  1996/06/11  01:27:50  jimz
 * Fixed bug where diskthread shutdown would crash or hang. This
 * turned out to be two distinct bugs:
 * (1) [crash] The thread shutdown code wasn't properly waiting for
 * all the diskthreads to complete. This caused diskthreads that were
 * exiting+cleaning up to unlock a destroyed mutex.
 * (2) [hang] TerminateDiskQueues wasn't locking, and DiskIODequeue
 * only checked for termination _after_ a wakeup if the queues were
 * empty. This was a race where the termination wakeup could be lost
 * by the dequeueing thread, and the system would hang waiting for the
 * thread to exit, while the thread waited for an I/O or a signal to
 * check the termination flag.
 *
 * Revision 1.14  1996/06/10  22:24:01  wvcii
 * added write dags which do not have a commit node and are
 * used in forward and backward error recovery experiments.
 *
 * Revision 1.13  1996/06/07  22:26:27  jimz
 * type-ify which_ru (RF_ReconUnitNum_t)
 *
 * Revision 1.12  1996/06/07  21:33:04  jimz
 * begin using consistent types for sector numbers,
 * stripe numbers, row+col numbers, recon unit numbers
 *
 * Revision 1.11  1996/05/31  22:26:54  jimz
 * fix a lot of mapping problems, memory allocation problems
 * found some weird lock issues, fixed 'em
 * more code cleanup
 *
 * Revision 1.10  1996/05/30  11:29:41  jimz
 * Numerous bug fixes. Stripe lock release code disagreed with the taking code
 * about when stripes should be locked (I made it consistent: no parity, no lock)
 * There was a lot of extra serialization of I/Os which I've removed- a lot of
 * it was to calculate values for the cache code, which is no longer with us.
 * More types, function, macro cleanup. Added code to properly quiesce the array
 * on shutdown. Made a lot of stuff array-specific which was (bogusly) general
 * before. Fixed memory allocation, freeing bugs.
 *
 * Revision 1.9  1996/05/27  18:56:37  jimz
 * more code cleanup
 * better typing
 * compiles in all 3 environments
 *
 * Revision 1.8  1996/05/24  22:17:04  jimz
 * continue code + namespace cleanup
 * typed a bunch of flags
 *
 * Revision 1.7  1996/05/24  04:28:55  jimz
 * release cleanup ckpt
 *
 * Revision 1.6  1996/05/23  21:46:35  jimz
 * checkpoint in code cleanup (release prep)
 * lots of types, function names have been fixed
 *
 * Revision 1.5  1996/05/23  00:33:23  jimz
 * code cleanup: move all debug decls to rf_options.c, all extern
 * debug decls to rf_options.h, all debug vars preceded by rf_
 *
 * Revision 1.4  1996/05/18  19:51:34  jimz
 * major code cleanup- fix syntax, make some types consistent,
 * add prototypes, clean out dead code, et cetera
 *
 * Revision 1.3  1996/05/15  23:23:12  wvcii
 * fixed bug in small write read old q node succedent initialization
 *
 * Revision 1.2  1996/05/08  21:01:24  jimz
 * fixed up enum type names that were conflicting with other
 * enums and function names (ie, "panic")
 * future naming trends will be towards RF_ and rf_ for
 * everything raidframe-related
 *
 * Revision 1.1  1996/05/03  19:20:45  wvcii
 * Initial revision
 *
d52 1
a52 1
 * 
d77 9
a85 8
void rf_CreateNonRedundantWriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList,
  RF_IoType_t            type)
d87 2
a88 2
  rf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
    RF_IO_TYPE_WRITE);
d91 9
a99 8
void rf_CreateRAID0WriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList,
  RF_IoType_t            type)
d101 2
a102 2
  rf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
    RF_IO_TYPE_WRITE);
d105 8
a112 7
void rf_CreateSmallWriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList)
d115 3
a117 3
  rf_CommonCreateSmallWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
    &rf_xorFuncs, NULL);
#else /* RF_FORWARD > 0 */
d119 8
a126 8
  rf_CommonCreateSmallWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
    &rf_xorFuncs, NULL);
#else /* RF_BACKWARD > 0 */
  /* "normal" rollaway */
  rf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
    &rf_xorFuncs, NULL);
#endif /* RF_BACKWARD > 0 */
#endif /* RF_FORWARD > 0 */
d129 8
a136 7
void rf_CreateLargeWriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList)
d139 3
a141 3
  rf_CommonCreateLargeWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
    1, rf_RegularXorFunc, RF_TRUE);
#else /* RF_FORWARD > 0 */
d143 8
a150 8
  rf_CommonCreateLargeWriteDAGFwd(raidPtr, asmap, dag_h, bp, flags, allocList,
    1, rf_RegularXorFunc, RF_TRUE);
#else /* RF_BACKWARD > 0 */
  /* "normal" rollaway */
  rf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,
    1, rf_RegularXorFunc, RF_TRUE);
#endif /* RF_BACKWARD > 0 */
#endif /* RF_FORWARD > 0 */
d183 1
a183 1
 *              flags     - general flags (e.g. disk locking) 
d191 11
a201 10
void rf_CommonCreateLargeWriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList,
  int                    nfaults,
  int                  (*redFunc)(RF_DagNode_t *),
  int                    allowBufferRecycle)
d203 248
a450 249
  RF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;
  RF_DagNode_t *wnqNode, *blockNode, *commitNode, *termNode;
  int nWndNodes, nRodNodes, i, nodeNum, asmNum;
  RF_AccessStripeMapHeader_t *new_asm_h[2];
  RF_StripeNum_t parityStripeID;
  char *sosBuffer, *eosBuffer;
  RF_ReconUnitNum_t which_ru;
  RF_RaidLayout_t *layoutPtr;
  RF_PhysDiskAddr_t *pda;

  layoutPtr = &(raidPtr->Layout);
  parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,
    &which_ru);

  if (rf_dagDebug) {
    printf("[Creating large-write DAG]\n");
  }
  dag_h->creator = "LargeWriteDAG";

  dag_h->numCommitNodes = 1;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  /* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */
  nWndNodes = asmap->numStripeUnitsAccessed;
  RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),
    (RF_DagNode_t *), allocList);
  i = 0;
  wndNodes    = &nodes[i]; i += nWndNodes;
  xorNode     = &nodes[i]; i += 1;
  wnpNode     = &nodes[i]; i += 1;
  blockNode   = &nodes[i]; i += 1;
  commitNode  = &nodes[i]; i += 1;
  termNode    = &nodes[i]; i += 1;
  if (nfaults == 2) {
    wnqNode   = &nodes[i]; i += 1;
  }
  else {
    wnqNode = NULL;
  }
  rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h,
    &nRodNodes, &sosBuffer, &eosBuffer, allocList);
  if (nRodNodes > 0) {
    RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),
      (RF_DagNode_t *), allocList);
  }
  else {
    rodNodes = NULL;
  }

  /* begin node initialization */
  if (nRodNodes > 0) {
    rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
      NULL, nRodNodes, 0, 0, 0, dag_h, "Nil", allocList);
  }
  else {
    rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
      NULL, 1, 0, 0, 0, dag_h, "Nil", allocList);
  }

  rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL,
    nWndNodes + nfaults, 1, 0, 0, dag_h, "Cmt", allocList);
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL,
    0, nWndNodes + nfaults, 0, 0, dag_h, "Trm", allocList);

  /* initialize the Rod nodes */
  for (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {
    if (new_asm_h[asmNum]) {
      pda = new_asm_h[asmNum]->stripeMap->physInfo;
      while (pda) {
        rf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc,
          rf_DiskReadUndoFunc,rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
          "Rod", allocList);
        rodNodes[nodeNum].params[0].p = pda;
        rodNodes[nodeNum].params[1].p = pda->bufPtr;
        rodNodes[nodeNum].params[2].v = parityStripeID;
        rodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
          0, 0, which_ru);
        nodeNum++;
        pda = pda->next;
      }
    }
  }
  RF_ASSERT(nodeNum == nRodNodes);

  /* initialize the wnd nodes */
  pda = asmap->physInfo;
  for (i=0; i < nWndNodes; i++) {
    rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
      rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
    RF_ASSERT(pda != NULL);
    wndNodes[i].params[0].p = pda;
    wndNodes[i].params[1].p = pda->bufPtr;
    wndNodes[i].params[2].v = parityStripeID;
    wndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
    pda = pda->next;
  }

  /* initialize the redundancy node */
  if (nRodNodes > 0) {
    rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,
      nRodNodes, 2 * (nWndNodes+nRodNodes) + 1, nfaults, dag_h,
      "Xr ", allocList);
  }
  else {
    rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,
      1, 2 * (nWndNodes+nRodNodes) + 1, nfaults, dag_h, "Xr ", allocList);
  }
  xorNode->flags |= RF_DAGNODE_FLAG_YIELD;
  for (i=0; i < nWndNodes; i++) {
    xorNode->params[2*i+0] = wndNodes[i].params[0];         /* pda */
    xorNode->params[2*i+1] = wndNodes[i].params[1];         /* buf ptr */
  }
  for (i=0; i < nRodNodes; i++) {
    xorNode->params[2*(nWndNodes+i)+0] = rodNodes[i].params[0];  /* pda */
    xorNode->params[2*(nWndNodes+i)+1] = rodNodes[i].params[1];  /* buf ptr */
  }
  /* xor node needs to get at RAID information */
  xorNode->params[2*(nWndNodes+nRodNodes)].p = raidPtr;
  
  /*
   * Look for an Rod node that reads a complete SU. If none, alloc a buffer
   * to receive the parity info. Note that we can't use a new data buffer
   * because it will not have gotten written when the xor occurs.
   */
  if (allowBufferRecycle) {
    for (i = 0; i < nRodNodes; i++) {
      if (((RF_PhysDiskAddr_t *)rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)
        break;
    }
  }
  if ((!allowBufferRecycle) || (i == nRodNodes)) {
    RF_CallocAndAdd(xorNode->results[0], 1,
      rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),
      (void *), allocList);
  }
  else {
    xorNode->results[0] = rodNodes[i].params[1].p;
  }

  /* initialize the Wnp node */
  rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnp", allocList);
  wnpNode->params[0].p = asmap->parityInfo;
  wnpNode->params[1].p = xorNode->results[0];
  wnpNode->params[2].v = parityStripeID;
  wnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
  /* parityInfo must describe entire parity unit */
  RF_ASSERT(asmap->parityInfo->next == NULL);

  if (nfaults == 2) {
      /*
       * We never try to recycle a buffer for the Q calcuation
       * in addition to the parity. This would cause two buffers
       * to get smashed during the P and Q calculation, guaranteeing
       * one would be wrong.
       */
      RF_CallocAndAdd(xorNode->results[1], 1,
        rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),
        (void *),allocList); 
      rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
        rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnq", allocList);
      wnqNode->params[0].p = asmap->qInfo;
      wnqNode->params[1].p = xorNode->results[1];
      wnqNode->params[2].v = parityStripeID;
      wnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      /* parityInfo must describe entire parity unit */
      RF_ASSERT(asmap->parityInfo->next == NULL);
  }

  /*
   * Connect nodes to form graph.
   */

  /* connect dag header to block node */
  RF_ASSERT(blockNode->numAntecedents == 0);
  dag_h->succedents[0] = blockNode;

  if (nRodNodes > 0) {
    /* connect the block node to the Rod nodes */
    RF_ASSERT(blockNode->numSuccedents == nRodNodes);
    RF_ASSERT(xorNode->numAntecedents == nRodNodes);
    for (i = 0; i < nRodNodes; i++) {
      RF_ASSERT(rodNodes[i].numAntecedents == 1);
      blockNode->succedents[i] = &rodNodes[i];
      rodNodes[i].antecedents[0] = blockNode;
      rodNodes[i].antType[0] = rf_control;

      /* connect the Rod nodes to the Xor node */
      RF_ASSERT(rodNodes[i].numSuccedents == 1);
      rodNodes[i].succedents[0] = xorNode;
      xorNode->antecedents[i] = &rodNodes[i];
      xorNode->antType[i] = rf_trueData;
    }
  }
  else {
    /* connect the block node to the Xor node */
    RF_ASSERT(blockNode->numSuccedents == 1);
    RF_ASSERT(xorNode->numAntecedents == 1);
    blockNode->succedents[0] = xorNode;
    xorNode->antecedents[0] = blockNode;
    xorNode->antType[0] = rf_control;
  }

  /* connect the xor node to the commit node */
  RF_ASSERT(xorNode->numSuccedents == 1);
  RF_ASSERT(commitNode->numAntecedents == 1);
  xorNode->succedents[0] = commitNode;
  commitNode->antecedents[0] = xorNode;
  commitNode->antType[0] = rf_control;

  /* connect the commit node to the write nodes */
  RF_ASSERT(commitNode->numSuccedents == nWndNodes + nfaults);
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNodes->numAntecedents == 1);
    commitNode->succedents[i] = &wndNodes[i];
    wndNodes[i].antecedents[0] = commitNode;
    wndNodes[i].antType[0] = rf_control;
  }
  RF_ASSERT(wnpNode->numAntecedents == 1);
  commitNode->succedents[nWndNodes] = wnpNode;
  wnpNode->antecedents[0]= commitNode;
  wnpNode->antType[0] = rf_trueData;
  if (nfaults == 2) {
    RF_ASSERT(wnqNode->numAntecedents == 1);
    commitNode->succedents[nWndNodes + 1] = wnqNode;
    wnqNode->antecedents[0] = commitNode;
    wnqNode->antType[0] = rf_trueData;
  }

  /* connect the write nodes to the term node */
  RF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);
  RF_ASSERT(termNode->numSuccedents == 0);
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNodes->numSuccedents == 1);
    wndNodes[i].succedents[0] = termNode;
    termNode->antecedents[i] = &wndNodes[i];
    termNode->antType[i] = rf_control;
  }
  RF_ASSERT(wnpNode->numSuccedents == 1);
  wnpNode->succedents[0] = termNode;
  termNode->antecedents[nWndNodes] = wnpNode;
  termNode->antType[nWndNodes] = rf_control;
  if (nfaults == 2) {
    RF_ASSERT(wnqNode->numSuccedents == 1);
    wnqNode->succedents[0] = termNode;
    termNode->antecedents[nWndNodes + 1] = wnqNode;
    termNode->antType[nWndNodes + 1] = rf_control;
  }
a451 1

d477 1
a477 1
 *              flags     - general flags (e.g. disk locking) 
d485 10
a494 9
void rf_CommonCreateSmallWriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList,
  RF_RedFuncs_t         *pfuncs,
  RF_RedFuncs_t         *qfuncs)
d496 570
a1065 568
  RF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;
  RF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;
  RF_DagNode_t *xorNodes, *qNodes, *blockNode, *commitNode, *nodes;
  RF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;
  int i, j, nNodes, totalNumNodes, lu_flag;
  RF_ReconUnitNum_t which_ru;
  int (*func)(RF_DagNode_t *), (*undoFunc)(RF_DagNode_t *);
  int (*qfunc)(RF_DagNode_t *);
  int numDataNodes, numParityNodes;
  RF_StripeNum_t parityStripeID;
  RF_PhysDiskAddr_t *pda;
  char *name, *qname;
  long nfaults;

  nfaults = qfuncs ? 2 : 1;
  lu_flag = (rf_enableAtomicRMW) ? 1 : 0; /* lock/unlock flag */

  parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
    asmap->raidAddress, &which_ru);
  pda = asmap->physInfo;
  numDataNodes = asmap->numStripeUnitsAccessed;
  numParityNodes = (asmap->parityInfo->next) ? 2 : 1;

  if (rf_dagDebug) {
    printf("[Creating small-write DAG]\n");
  }
  RF_ASSERT(numDataNodes > 0);
  dag_h->creator = "SmallWriteDAG";

  dag_h->numCommitNodes = 1;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  /*
   * DAG creation occurs in four steps:
   * 1. count the number of nodes in the DAG
   * 2. create the nodes
   * 3. initialize the nodes
   * 4. connect the nodes
   */

  /*
   * Step 1. compute number of nodes in the graph
   */

  /* number of nodes:
   *  a read and write for each data unit
   *  a redundancy computation node for each parity node (nfaults * nparity)
   *  a read and write for each parity unit
   *  a block and commit node (2)
   *  a terminate node
   *  if atomic RMW
   *    an unlock node for each data unit, redundancy unit
   */
  totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)
    + (nfaults * 2 * numParityNodes) + 3;
  if (lu_flag) {
    totalNumNodes += (numDataNodes + (nfaults * numParityNodes));
  }

  /*
   * Step 2. create the nodes
   */
  RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),
    (RF_DagNode_t *), allocList);
  i = 0;
  blockNode        = &nodes[i]; i += 1;
  commitNode       = &nodes[i]; i += 1;
  readDataNodes    = &nodes[i]; i += numDataNodes;
  readParityNodes  = &nodes[i]; i += numParityNodes;
  writeDataNodes   = &nodes[i]; i += numDataNodes;
  writeParityNodes = &nodes[i]; i += numParityNodes;
  xorNodes         = &nodes[i]; i += numParityNodes;
  termNode         = &nodes[i]; i += 1;
  if (lu_flag) {
    unlockDataNodes   = &nodes[i]; i += numDataNodes;
    unlockParityNodes = &nodes[i]; i += numParityNodes;
  }
  else {
    unlockDataNodes = unlockParityNodes = NULL;
  }
  if (nfaults == 2) {
    readQNodes     = &nodes[i]; i += numParityNodes;
    writeQNodes    = &nodes[i]; i += numParityNodes;
    qNodes         = &nodes[i]; i += numParityNodes;
    if (lu_flag) {
      unlockQNodes    = &nodes[i]; i += numParityNodes;
    }
    else {
      unlockQNodes = NULL;
    }
  }
  else {
    readQNodes = writeQNodes = qNodes = unlockQNodes = NULL;
  }
  RF_ASSERT(i == totalNumNodes);
  
  /*
   * Step 3. initialize the nodes
   */
  /* initialize block node (Nil) */
  nNodes     = numDataNodes + (nfaults * numParityNodes);
  rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, nNodes, 0, 0, 0, dag_h, "Nil", allocList);

  /* initialize commit node (Cmt) */
  rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, nNodes, (nfaults * numParityNodes), 0, 0, dag_h, "Cmt", allocList);

  /* initialize terminate node (Trm) */
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,
    NULL, 0, nNodes, 0, 0, dag_h, "Trm", allocList);

  /* initialize nodes which read old data (Rod) */
  for (i = 0; i < numDataNodes; i++) {
    rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
      rf_GenericWakeupFunc, (nfaults * numParityNodes), 1, 4, 0, dag_h,
      "Rod", allocList);
    RF_ASSERT(pda != NULL);
    /* physical disk addr desc */
    readDataNodes[i].params[0].p = pda;
    /* buffer to hold old data */
    readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
      dag_h, pda, allocList);
    readDataNodes[i].params[2].v = parityStripeID;
    readDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
      lu_flag, 0, which_ru);
    pda = pda->next;
    for (j = 0; j < readDataNodes[i].numSuccedents; j++) {
      readDataNodes[i].propList[j] = NULL;
    }
  }

  /* initialize nodes which read old parity (Rop) */
  pda = asmap->parityInfo; i = 0;
  for (i = 0; i < numParityNodes; i++) {
    RF_ASSERT(pda != NULL);
    rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,
      rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4,
      0, dag_h, "Rop", allocList);
    readParityNodes[i].params[0].p = pda;
    /* buffer to hold old parity */
    readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,
      dag_h, pda, allocList);
    readParityNodes[i].params[2].v = parityStripeID;
    readParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
      lu_flag, 0, which_ru);
    pda = pda->next;
    for (j = 0; j < readParityNodes[i].numSuccedents; j++) {
      readParityNodes[i].propList[0] = NULL;
    }
  }

  /* initialize nodes which read old Q (Roq) */
  if (nfaults == 2) {
    pda = asmap->qInfo; 
    for (i = 0; i < numParityNodes; i++) {
      RF_ASSERT(pda != NULL);
      rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,
        rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, "Roq", allocList);
      readQNodes[i].params[0].p = pda;
      /* buffer to hold old Q */
      readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda,
        allocList);
      readQNodes[i].params[2].v = parityStripeID;
      readQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
        lu_flag, 0, which_ru);
      pda = pda->next;
      for (j = 0; j < readQNodes[i].numSuccedents; j++) {
        readQNodes[i].propList[0] = NULL;
      }
    }
  }

  /* initialize nodes which write new data (Wnd) */
  pda = asmap->physInfo;
  for (i=0; i < numDataNodes; i++) {
    RF_ASSERT(pda != NULL);
    rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
      rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
      "Wnd", allocList);
    /* physical disk addr desc */
    writeDataNodes[i].params[0].p = pda;
    /* buffer holding new data to be written */
    writeDataNodes[i].params[1].p = pda->bufPtr;
    writeDataNodes[i].params[2].v = parityStripeID;
    writeDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
      0, 0, which_ru);
    if (lu_flag) {
      /* initialize node to unlock the disk queue */
      rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,
        rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
        "Und", allocList);
      /* physical disk addr desc */
      unlockDataNodes[i].params[0].p = pda;
      unlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
        0, lu_flag, which_ru);
    }
    pda = pda->next;
  }

  /*
   * Initialize nodes which compute new parity and Q.
   */
  /*
   * We use the simple XOR func in the double-XOR case, and when
   * we're accessing only a portion of one stripe unit. The distinction
   * between the two is that the regular XOR func assumes that the targbuf
   * is a full SU in size, and examines the pda associated with the buffer
   * to decide where within the buffer to XOR the data, whereas
   * the simple XOR func just XORs the data into the start of the buffer.
   */
  if ((numParityNodes==2) || ((numDataNodes == 1)
    && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit)))
  {
    func = pfuncs->simple; undoFunc = rf_NullNodeUndoFunc; name = pfuncs->SimpleName;
    if (qfuncs) {
      qfunc = qfuncs->simple;
      qname = qfuncs->SimpleName;
    }
    else {
      qfunc = NULL;
      qname = NULL;
    }
  }
  else {
    func = pfuncs->regular;
    undoFunc = rf_NullNodeUndoFunc;
    name = pfuncs->RegularName;
    if (qfuncs) {
      qfunc = qfuncs->regular;
      qname = qfuncs->RegularName;
    }
    else {
      qfunc = NULL;
      qname = NULL;
    }
  }
  /*
   * Initialize the xor nodes: params are {pda,buf}
   * from {Rod,Wnd,Rop} nodes, and raidPtr
   */
  if (numParityNodes==2) {
    /* double-xor case */
    for (i=0; i < numParityNodes; i++) {
      /* note: no wakeup func for xor */
      rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL,
        1, (numDataNodes + numParityNodes), 7, 1, dag_h, name, allocList);
      xorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;
      xorNodes[i].params[0]   = readDataNodes[i].params[0];
      xorNodes[i].params[1]   = readDataNodes[i].params[1];
      xorNodes[i].params[2]   = readParityNodes[i].params[0];
      xorNodes[i].params[3]   = readParityNodes[i].params[1];
      xorNodes[i].params[4]   = writeDataNodes[i].params[0];
      xorNodes[i].params[5]   = writeDataNodes[i].params[1];
      xorNodes[i].params[6].p = raidPtr;
      /* use old parity buf as target buf */
      xorNodes[i].results[0] = readParityNodes[i].params[1].p;
      if (nfaults == 2) {
        /* note: no wakeup func for qor */
        rf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,
          (numDataNodes + numParityNodes), 7, 1, dag_h, qname, allocList);
        qNodes[i].params[0]   = readDataNodes[i].params[0];
        qNodes[i].params[1]   = readDataNodes[i].params[1];
        qNodes[i].params[2]   = readQNodes[i].params[0];
        qNodes[i].params[3]   = readQNodes[i].params[1];
        qNodes[i].params[4]   = writeDataNodes[i].params[0];
        qNodes[i].params[5]   = writeDataNodes[i].params[1];
        qNodes[i].params[6].p = raidPtr;
        /* use old Q buf as target buf */
        qNodes[i].results[0] = readQNodes[i].params[1].p;
      }
    }
  }
  else {
    /* there is only one xor node in this case */
    rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, 1,
      (numDataNodes + numParityNodes),
      (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);
    xorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;
    for (i=0; i < numDataNodes + 1; i++) {
      /* set up params related to Rod and Rop nodes */
      xorNodes[0].params[2*i+0] = readDataNodes[i].params[0]; /* pda */
      xorNodes[0].params[2*i+1] = readDataNodes[i].params[1]; /* buffer ptr */
    }
    for (i=0; i < numDataNodes; i++) {
      /* set up params related to Wnd and Wnp nodes */
      xorNodes[0].params[2*(numDataNodes+1+i)+0] = /* pda */
        writeDataNodes[i].params[0];
      xorNodes[0].params[2*(numDataNodes+1+i)+1] = /* buffer ptr */
       writeDataNodes[i].params[1];
    }
    /* xor node needs to get at RAID information */
    xorNodes[0].params[2*(numDataNodes+numDataNodes+1)].p = raidPtr;
    xorNodes[0].results[0] = readParityNodes[0].params[1].p;
    if (nfaults == 2)  {
      rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,
        (numDataNodes + numParityNodes),
        (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,
        qname, allocList);
      for (i=0; i<numDataNodes; i++) {
        /* set up params related to Rod */
        qNodes[0].params[2*i+0] = readDataNodes[i].params[0]; /* pda */
        qNodes[0].params[2*i+1] = readDataNodes[i].params[1]; /* buffer ptr */
      }
      /* and read old q */
      qNodes[0].params[2*numDataNodes + 0] = /* pda */
        readQNodes[0].params[0];
      qNodes[0].params[2*numDataNodes + 1] = /* buffer ptr */
        readQNodes[0].params[1];
      for (i=0; i < numDataNodes; i++) {
        /* set up params related to Wnd nodes */
        qNodes[0].params[2*(numDataNodes+1+i)+0] = /* pda */
          writeDataNodes[i].params[0];
        qNodes[0].params[2*(numDataNodes+1+i)+1] = /* buffer ptr */
          writeDataNodes[i].params[1];
      }
      /* xor node needs to get at RAID information */
      qNodes[0].params[2*(numDataNodes+numDataNodes+1)].p = raidPtr;
      qNodes[0].results[0] = readQNodes[0].params[1].p;
    }
  }

  /* initialize nodes which write new parity (Wnp) */
  pda = asmap->parityInfo;
  for (i=0;  i < numParityNodes; i++) {
    rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
      rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
      "Wnp", allocList);
    RF_ASSERT(pda != NULL);
    writeParityNodes[i].params[0].p = pda; /* param 1 (bufPtr) filled in by xor node */
    writeParityNodes[i].params[1].p = xorNodes[i].results[0]; /* buffer pointer for parity write operation */
    writeParityNodes[i].params[2].v = parityStripeID;
    writeParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
      0, 0, which_ru);
    if (lu_flag) {
      /* initialize node to unlock the disk queue */
      rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,
        rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
        "Unp", allocList);
      unlockParityNodes[i].params[0].p = pda; /* physical disk addr desc */
      unlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
        0, lu_flag, which_ru);
    }
    pda = pda->next;
  }

  /* initialize nodes which write new Q (Wnq) */
  if (nfaults == 2) {
    pda = asmap->qInfo;
    for (i=0;  i < numParityNodes; i++) {
      rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,
        rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,
        "Wnq", allocList);
      RF_ASSERT(pda != NULL);
      writeQNodes[i].params[0].p = pda; /* param 1 (bufPtr) filled in by xor node */
      writeQNodes[i].params[1].p = qNodes[i].results[0]; /* buffer pointer for parity write operation */
      writeQNodes[i].params[2].v = parityStripeID;
      writeQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
        0, 0, which_ru);
      if (lu_flag) {
        /* initialize node to unlock the disk queue */
        rf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,
          rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,
          "Unq", allocList);
        unlockQNodes[i].params[0].p = pda; /* physical disk addr desc */
        unlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,
          0, lu_flag, which_ru);
      }
      pda = pda->next;
    }
  }

  /*
   * Step 4. connect the nodes.
   */

  /* connect header to block node */
  dag_h->succedents[0] = blockNode;

  /* connect block node to read old data nodes */
  RF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));
  for (i = 0; i < numDataNodes; i++) {
    blockNode->succedents[i] = &readDataNodes[i];
    RF_ASSERT(readDataNodes[i].numAntecedents == 1);
    readDataNodes[i].antecedents[0]= blockNode;
    readDataNodes[i].antType[0] = rf_control;
  }

  /* connect block node to read old parity nodes */
  for (i = 0; i < numParityNodes; i++) {
    blockNode->succedents[numDataNodes + i] = &readParityNodes[i];
    RF_ASSERT(readParityNodes[i].numAntecedents == 1);
    readParityNodes[i].antecedents[0] = blockNode;
    readParityNodes[i].antType[0] = rf_control;
  }

  /* connect block node to read old Q nodes */
  if (nfaults == 2) {
    for (i = 0; i < numParityNodes; i++) {
      blockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];
      RF_ASSERT(readQNodes[i].numAntecedents == 1);
      readQNodes[i].antecedents[0] = blockNode;
      readQNodes[i].antType[0] = rf_control;
    }
  }

  /* connect read old data nodes to xor nodes */
  for (i = 0; i < numDataNodes; i++) {
    RF_ASSERT(readDataNodes[i].numSuccedents == (nfaults * numParityNodes));
    for (j = 0; j < numParityNodes; j++){
      RF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);
      readDataNodes[i].succedents[j] = &xorNodes[j];
      xorNodes[j].antecedents[i] = &readDataNodes[i];
      xorNodes[j].antType[i] = rf_trueData;
    }
  }

  /* connect read old data nodes to q nodes */
  if (nfaults == 2) {
    for (i = 0; i < numDataNodes; i++) {
      for (j = 0; j < numParityNodes; j++) {
        RF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);
        readDataNodes[i].succedents[numParityNodes + j] = &qNodes[j];
        qNodes[j].antecedents[i] = &readDataNodes[i];
        qNodes[j].antType[i] = rf_trueData;
      }
    }
  }

  /* connect read old parity nodes to xor nodes */
  for (i = 0; i < numParityNodes; i++) {
    RF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);
    for (j = 0; j < numParityNodes; j++) {
      readParityNodes[i].succedents[j] = &xorNodes[j];
      xorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];
      xorNodes[j].antType[numDataNodes + i] = rf_trueData;
    }
  }

  /* connect read old q nodes to q nodes */
  if (nfaults == 2) {
    for (i = 0; i < numParityNodes; i++) {
      RF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);
      for (j = 0; j < numParityNodes; j++) {
        readQNodes[i].succedents[j] = &qNodes[j];
        qNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];
        qNodes[j].antType[numDataNodes + i] = rf_trueData;
      }
    }
  }

  /* connect xor nodes to commit node */
  RF_ASSERT(commitNode->numAntecedents == (nfaults * numParityNodes));
  for (i = 0; i < numParityNodes; i++) {
    RF_ASSERT(xorNodes[i].numSuccedents == 1);
    xorNodes[i].succedents[0] = commitNode;
    commitNode->antecedents[i] = &xorNodes[i];
    commitNode->antType[i] = rf_control;
  }

  /* connect q nodes to commit node */
  if (nfaults == 2) {
    for (i = 0; i < numParityNodes; i++) {
      RF_ASSERT(qNodes[i].numSuccedents == 1);
      qNodes[i].succedents[0] = commitNode;
      commitNode->antecedents[i + numParityNodes] = &qNodes[i];
      commitNode->antType[i + numParityNodes] = rf_control;
    }
  }

  /* connect commit node to write nodes */
  RF_ASSERT(commitNode->numSuccedents == (numDataNodes + (nfaults * numParityNodes)));
  for (i = 0; i < numDataNodes; i++) {
    RF_ASSERT(writeDataNodes[i].numAntecedents == 1);
    commitNode->succedents[i] = &writeDataNodes[i];
    writeDataNodes[i].antecedents[0] = commitNode;
    writeDataNodes[i].antType[0] = rf_trueData;
  }
  for (i = 0; i < numParityNodes; i++) {
    RF_ASSERT(writeParityNodes[i].numAntecedents == 1);
    commitNode->succedents[i + numDataNodes] = &writeParityNodes[i];
    writeParityNodes[i].antecedents[0] = commitNode;
    writeParityNodes[i].antType[0] = rf_trueData;
  }
  if (nfaults == 2) {
    for (i = 0; i < numParityNodes; i++) {
      RF_ASSERT(writeQNodes[i].numAntecedents == 1);
      commitNode->succedents[i + numDataNodes + numParityNodes] = &writeQNodes[i];
      writeQNodes[i].antecedents[0] = commitNode;
      writeQNodes[i].antType[0] = rf_trueData;
    }
  }

  RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
  RF_ASSERT(termNode->numSuccedents == 0);
  for (i = 0; i < numDataNodes; i++) {
    if (lu_flag) {
      /* connect write new data nodes to unlock nodes */
      RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
      RF_ASSERT(unlockDataNodes[i].numAntecedents == 1);
      writeDataNodes[i].succedents[0] = &unlockDataNodes[i];
      unlockDataNodes[i].antecedents[0] = &writeDataNodes[i];
      unlockDataNodes[i].antType[0] = rf_control;

      /* connect unlock nodes to term node */
      RF_ASSERT(unlockDataNodes[i].numSuccedents == 1);
      unlockDataNodes[i].succedents[0] = termNode;
      termNode->antecedents[i] = &unlockDataNodes[i];
      termNode->antType[i] = rf_control;
    }
    else {
      /* connect write new data nodes to term node */
      RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
      RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
      writeDataNodes[i].succedents[0] = termNode;
      termNode->antecedents[i] = &writeDataNodes[i];
      termNode->antType[i] = rf_control;
    }
  }

  for (i = 0; i < numParityNodes; i++) {
    if (lu_flag) {
      /* connect write new parity nodes to unlock nodes */
      RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
      RF_ASSERT(unlockParityNodes[i].numAntecedents == 1);
      writeParityNodes[i].succedents[0] = &unlockParityNodes[i];
      unlockParityNodes[i].antecedents[0] = &writeParityNodes[i];
      unlockParityNodes[i].antType[0] = rf_control;

      /* connect unlock nodes to term node */
      RF_ASSERT(unlockParityNodes[i].numSuccedents == 1);
      unlockParityNodes[i].succedents[0] = termNode;
      termNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];
      termNode->antType[numDataNodes + i] = rf_control;
    }
    else {
      RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
      writeParityNodes[i].succedents[0] = termNode;
      termNode->antecedents[numDataNodes + i] = &writeParityNodes[i];
      termNode->antType[numDataNodes + i] = rf_control;
    }
  }

  if (nfaults == 2) {
    for (i = 0; i < numParityNodes; i++) {
      if (lu_flag) {
        /* connect write new Q nodes to unlock nodes */
        RF_ASSERT(writeQNodes[i].numSuccedents == 1);
        RF_ASSERT(unlockQNodes[i].numAntecedents == 1);
        writeQNodes[i].succedents[0] = &unlockQNodes[i];
        unlockQNodes[i].antecedents[0] = &writeQNodes[i];
        unlockQNodes[i].antType[0] = rf_control;

        /* connect unlock nodes to unblock node */
        RF_ASSERT(unlockQNodes[i].numSuccedents == 1);
        unlockQNodes[i].succedents[0] = termNode;
        termNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];
        termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
      }
      else {
        RF_ASSERT(writeQNodes[i].numSuccedents == 1);
        writeQNodes[i].succedents[0] = termNode;
        termNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];
        termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
      }
    }
  }
d1081 1
a1081 1
 *              flags     - general flags (e.g. disk locking) 
d1085 8
a1092 7
void rf_CreateRaidOneWriteDAG(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList)
d1094 129
a1222 125
  RF_DagNode_t *unblockNode, *termNode, *commitNode;
  RF_DagNode_t *nodes, *wndNode, *wmirNode;
  int nWndNodes, nWmirNodes, i;
  RF_ReconUnitNum_t which_ru;
  RF_PhysDiskAddr_t *pda, *pdaP;
  RF_StripeNum_t parityStripeID;

  parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
    asmap->raidAddress, &which_ru);
  if (rf_dagDebug) {
    printf("[Creating RAID level 1 write DAG]\n");
  }
  dag_h->creator = "RaidOneWriteDAG";

  /* 2 implies access not SU aligned */
  nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;
  nWndNodes =  (asmap->physInfo->next) ? 2 : 1;

  /* alloc the Wnd nodes and the Wmir node */
  if (asmap->numDataFailed == 1)
    nWndNodes--;
  if (asmap->numParityFailed == 1)
    nWmirNodes--;

  /* total number of nodes = nWndNodes + nWmirNodes + (commit + unblock + terminator) */
  RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t),
    (RF_DagNode_t *), allocList);
  i = 0;
  wndNode     = &nodes[i]; i += nWndNodes;
  wmirNode    = &nodes[i]; i += nWmirNodes;
  commitNode   = &nodes[i]; i += 1;
  unblockNode = &nodes[i]; i += 1;
  termNode = &nodes[i]; i += 1;
  RF_ASSERT(i == (nWndNodes + nWmirNodes + 3));

  /* this dag can commit immediately */
  dag_h->numCommitNodes = 1;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  /* initialize the commit, unblock, and term nodes */
  rf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, "Cmt", allocList);
  rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,
    NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, "Nil", allocList);
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,
    NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);

  /* initialize the wnd nodes */
  if (nWndNodes > 0) {
    pda = asmap->physInfo;
    for (i = 0; i < nWndNodes; i++) {
      rf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
        rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wpd", allocList);
      RF_ASSERT(pda != NULL);
      wndNode[i].params[0].p = pda;
      wndNode[i].params[1].p = pda->bufPtr;
      wndNode[i].params[2].v = parityStripeID;
      wndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      pda = pda->next;
    }
    RF_ASSERT(pda == NULL);
  }

  /* initialize the mirror nodes */
  if (nWmirNodes > 0) {
    pda = asmap->physInfo;
    pdaP = asmap->parityInfo;
    for (i = 0; i < nWmirNodes; i++) {
      rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,
        rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wsd", allocList);
      RF_ASSERT(pda != NULL);
      wmirNode[i].params[0].p = pdaP;
      wmirNode[i].params[1].p = pda->bufPtr;
      wmirNode[i].params[2].v = parityStripeID;
      wmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      pda = pda->next;
      pdaP = pdaP->next;
    }
    RF_ASSERT(pda == NULL);
    RF_ASSERT(pdaP == NULL);
  }

  /* link the header node to the commit node */
  RF_ASSERT(dag_h->numSuccedents == 1);
  RF_ASSERT(commitNode->numAntecedents == 0);
  dag_h->succedents[0] = commitNode;

  /* link the commit node to the write nodes */
  RF_ASSERT(commitNode->numSuccedents == (nWndNodes + nWmirNodes));
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNode[i].numAntecedents == 1);
    commitNode->succedents[i] = &wndNode[i];
    wndNode[i].antecedents[0] = commitNode;
    wndNode[i].antType[0] = rf_control;
  }
  for (i = 0; i < nWmirNodes; i++) {
    RF_ASSERT(wmirNode[i].numAntecedents == 1);
    commitNode->succedents[i + nWndNodes] = &wmirNode[i];
    wmirNode[i].antecedents[0] = commitNode;
    wmirNode[i].antType[0] = rf_control;
  }

  /* link the write nodes to the unblock node */
  RF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNode[i].numSuccedents == 1);
    wndNode[i].succedents[0] = unblockNode;
    unblockNode->antecedents[i] = &wndNode[i];
    unblockNode->antType[i] = rf_control;
  }
  for (i = 0; i < nWmirNodes; i++) {
    RF_ASSERT(wmirNode[i].numSuccedents == 1);
    wmirNode[i].succedents[0] = unblockNode;
    unblockNode->antecedents[i + nWndNodes] = &wmirNode[i];
    unblockNode->antType[i + nWndNodes] = rf_control;
  }

  /* link the unblock node to the term node */
  RF_ASSERT(unblockNode->numSuccedents == 1);
  RF_ASSERT(termNode->numAntecedents == 1);
  RF_ASSERT(termNode->numSuccedents == 0);
  unblockNode->succedents[0] = termNode;
  termNode->antecedents[0] = unblockNode;
  termNode->antType[0] = rf_control;
d1236 11
a1246 10
void rf_CommonCreateLargeWriteDAGFwd(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList,
  int                    nfaults,
  int                  (*redFunc)(RF_DagNode_t *),
  int                    allowBufferRecycle)
d1248 220
a1467 216
  RF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;
  RF_DagNode_t *wnqNode, *blockNode, *syncNode, *termNode;
  int nWndNodes, nRodNodes, i, nodeNum, asmNum;
  RF_AccessStripeMapHeader_t *new_asm_h[2];
  RF_StripeNum_t parityStripeID;
  char *sosBuffer, *eosBuffer;
  RF_ReconUnitNum_t which_ru;
  RF_RaidLayout_t *layoutPtr;
  RF_PhysDiskAddr_t *pda;

  layoutPtr = &(raidPtr->Layout);
  parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);

  if (rf_dagDebug)
    printf("[Creating large-write DAG]\n");
  dag_h->creator = "LargeWriteDAGFwd";

  dag_h->numCommitNodes = 0;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  /* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */
  nWndNodes = asmap->numStripeUnitsAccessed;
  RF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
  i = 0;
  wndNodes    = &nodes[i]; i += nWndNodes;
  xorNode     = &nodes[i]; i += 1;
  wnpNode     = &nodes[i]; i += 1;
  blockNode   = &nodes[i]; i += 1;
  syncNode  = &nodes[i]; i += 1;
  termNode    = &nodes[i]; i += 1;
  if (nfaults == 2) {
    wnqNode   = &nodes[i]; i += 1;
  }
  else {
    wnqNode = NULL;
  }
  rf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);
  if (nRodNodes > 0) {
    RF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
  }
  else {
    rodNodes = NULL;
  }

  /* begin node initialization */
  if (nRodNodes > 0) {
    rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h, "Nil", allocList);
    rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0, dag_h, "Nil", allocList);
  }
  else {
    rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, "Nil", allocList);
    rf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h, "Nil", allocList);
  }

  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0, dag_h, "Trm", allocList);

  /* initialize the Rod nodes */
  for (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {
    if (new_asm_h[asmNum]) {
      pda = new_asm_h[asmNum]->stripeMap->physInfo;
      while (pda) {
	rf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Rod", allocList);
	rodNodes[nodeNum].params[0].p = pda;
	rodNodes[nodeNum].params[1].p = pda->bufPtr;
	rodNodes[nodeNum].params[2].v = parityStripeID;
	rodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
	nodeNum++;
	pda=pda->next;
      }
    }
  }
  RF_ASSERT(nodeNum == nRodNodes);

  /* initialize the wnd nodes */
  pda = asmap->physInfo;
  for (i=0; i < nWndNodes; i++) {
    rf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
    RF_ASSERT(pda != NULL);
    wndNodes[i].params[0].p = pda;
    wndNodes[i].params[1].p = pda->bufPtr;
    wndNodes[i].params[2].v = parityStripeID;
    wndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
    pda = pda->next;
  }

  /* initialize the redundancy node */
  rf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, "Xr ", allocList);
  xorNode->flags |= RF_DAGNODE_FLAG_YIELD;
  for (i=0; i < nWndNodes; i++) {
    xorNode->params[2*i+0] = wndNodes[i].params[0];         /* pda */
    xorNode->params[2*i+1] = wndNodes[i].params[1];         /* buf ptr */
  }
  for (i=0; i < nRodNodes; i++) {
    xorNode->params[2*(nWndNodes+i)+0] = rodNodes[i].params[0];         /* pda */
    xorNode->params[2*(nWndNodes+i)+1] = rodNodes[i].params[1];         /* buf ptr */
  }
  xorNode->params[2*(nWndNodes+nRodNodes)].p = raidPtr; /* xor node needs to get at RAID information */
  
  /* look for an Rod node that reads a complete SU.  If none, alloc a buffer to receive the parity info.
   * Note that we can't use a new data buffer because it will not have gotten written when the xor occurs.
   */
  if (allowBufferRecycle) {
    for (i = 0; i < nRodNodes; i++)
      if (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)
        break;
  }
  if ((!allowBufferRecycle) || (i == nRodNodes)) {
    RF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);
  }
  else
    xorNode->results[0] = rodNodes[i].params[1].p;

  /* initialize the Wnp node */
  rf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnp", allocList);
  wnpNode->params[0].p = asmap->parityInfo;
  wnpNode->params[1].p = xorNode->results[0];
  wnpNode->params[2].v = parityStripeID;
  wnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
  RF_ASSERT(asmap->parityInfo->next == NULL);        /* parityInfo must describe entire parity unit */

  if (nfaults == 2)
    {
      /* we never try to recycle a buffer for the Q calcuation in addition to the parity.
	 This would cause two buffers to get smashed during the P and Q calculation, 
	 guaranteeing one would be wrong.
      */
      RF_CallocAndAdd(xorNode->results[1], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList); 
      rf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnq", allocList);
      wnqNode->params[0].p = asmap->qInfo;
      wnqNode->params[1].p = xorNode->results[1];
      wnqNode->params[2].v = parityStripeID;
      wnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      RF_ASSERT(asmap->parityInfo->next == NULL);        /* parityInfo must describe entire parity unit */
    }


  /* connect nodes to form graph */

  /* connect dag header to block node */
  RF_ASSERT(blockNode->numAntecedents == 0);
  dag_h->succedents[0] = blockNode;

  if (nRodNodes > 0) {
    /* connect the block node to the Rod nodes */
    RF_ASSERT(blockNode->numSuccedents == nRodNodes);
    RF_ASSERT(syncNode->numAntecedents == nRodNodes);
    for (i = 0; i < nRodNodes; i++) {
      RF_ASSERT(rodNodes[i].numAntecedents == 1);
      blockNode->succedents[i] = &rodNodes[i];
      rodNodes[i].antecedents[0] = blockNode;
      rodNodes[i].antType[0] = rf_control;

      /* connect the Rod nodes to the Nil node */
      RF_ASSERT(rodNodes[i].numSuccedents == 1);
      rodNodes[i].succedents[0] = syncNode;
      syncNode->antecedents[i] = &rodNodes[i];
      syncNode->antType[i] = rf_trueData;
    }
  }
  else {
    /* connect the block node to the Nil node */
    RF_ASSERT(blockNode->numSuccedents == 1);
    RF_ASSERT(syncNode->numAntecedents == 1);
    blockNode->succedents[0] = syncNode;
    syncNode->antecedents[0] = blockNode;
    syncNode->antType[0] = rf_control;
  }

  /* connect the sync node to the Wnd nodes */
  RF_ASSERT(syncNode->numSuccedents == (1 + nWndNodes));
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNodes->numAntecedents == 1);
    syncNode->succedents[i] = &wndNodes[i];
    wndNodes[i].antecedents[0] = syncNode;
    wndNodes[i].antType[0] = rf_control;
  }

  /* connect the sync node to the Xor node */
  RF_ASSERT(xorNode->numAntecedents == 1);
  syncNode->succedents[nWndNodes] = xorNode;
  xorNode->antecedents[0] = syncNode;
  xorNode->antType[0] = rf_control;

  /* connect the xor node to the write parity node */
  RF_ASSERT(xorNode->numSuccedents == nfaults);
  RF_ASSERT(wnpNode->numAntecedents == 1);
  xorNode->succedents[0] = wnpNode;
  wnpNode->antecedents[0]= xorNode;
  wnpNode->antType[0] = rf_trueData;
  if (nfaults == 2) {
    RF_ASSERT(wnqNode->numAntecedents == 1);
    xorNode->succedents[1] = wnqNode;
    wnqNode->antecedents[0] = xorNode;
    wnqNode->antType[0] = rf_trueData;
  }

  /* connect the write nodes to the term node */
  RF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);
  RF_ASSERT(termNode->numSuccedents == 0);
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNodes->numSuccedents == 1);
    wndNodes[i].succedents[0] = termNode;
    termNode->antecedents[i] = &wndNodes[i];
    termNode->antType[i] = rf_control;
  }
  RF_ASSERT(wnpNode->numSuccedents == 1);
  wnpNode->succedents[0] = termNode;
  termNode->antecedents[nWndNodes] = wnpNode;
  termNode->antType[nWndNodes] = rf_control;
  if (nfaults == 2) {
    RF_ASSERT(wnqNode->numSuccedents == 1);
    wnqNode->succedents[0] = termNode;
    termNode->antecedents[nWndNodes + 1] = wnqNode;
    termNode->antType[nWndNodes + 1] = rf_control;
  }
d1496 1
a1496 1
 *              flags     - general flags (e.g. disk locking) 
d1504 10
a1513 9
void rf_CommonCreateSmallWriteDAGFwd(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList,
  RF_RedFuncs_t         *pfuncs,
  RF_RedFuncs_t         *qfuncs)
d1515 130
a1644 274
  RF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;
  RF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;
  RF_DagNode_t *xorNodes, *qNodes, *blockNode, *nodes;
  RF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;
  int i, j, nNodes, totalNumNodes, lu_flag;
  RF_ReconUnitNum_t which_ru;
  int (*func)(RF_DagNode_t *), (*undoFunc)(RF_DagNode_t *);
  int (*qfunc)(RF_DagNode_t *);
  int numDataNodes, numParityNodes;
  RF_StripeNum_t parityStripeID;
  RF_PhysDiskAddr_t *pda;
  char *name, *qname;
  long nfaults;

  nfaults = qfuncs ? 2 : 1;
  lu_flag = (rf_enableAtomicRMW) ? 1 : 0;          /* lock/unlock flag */

  parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);
  pda = asmap->physInfo;
  numDataNodes = asmap->numStripeUnitsAccessed;
  numParityNodes = (asmap->parityInfo->next) ? 2 : 1;

  if (rf_dagDebug) printf("[Creating small-write DAG]\n");
  RF_ASSERT(numDataNodes > 0);
  dag_h->creator = "SmallWriteDAGFwd";

  dag_h->numCommitNodes = 0;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  qfunc = NULL;
  qname = NULL;

  /* DAG creation occurs in four steps:
     1. count the number of nodes in the DAG
     2. create the nodes
     3. initialize the nodes
     4. connect the nodes
   */

  /* Step 1. compute number of nodes in the graph */

  /* number of nodes:
      a read and write for each data unit
      a redundancy computation node for each parity node (nfaults * nparity)
      a read and write for each parity unit
      a block node
      a terminate node
      if atomic RMW
        an unlock node for each data unit, redundancy unit
  */
  totalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes) + (nfaults * 2 * numParityNodes) + 2;
  if (lu_flag)
    totalNumNodes += (numDataNodes + (nfaults * numParityNodes));


  /* Step 2. create the nodes */
  RF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
  i = 0;
  blockNode        = &nodes[i]; i += 1;
  readDataNodes    = &nodes[i]; i += numDataNodes;
  readParityNodes  = &nodes[i]; i += numParityNodes;
  writeDataNodes   = &nodes[i]; i += numDataNodes;
  writeParityNodes = &nodes[i]; i += numParityNodes;
  xorNodes         = &nodes[i]; i += numParityNodes;
  termNode         = &nodes[i]; i += 1;
  if (lu_flag) {
    unlockDataNodes   = &nodes[i]; i += numDataNodes;
    unlockParityNodes = &nodes[i]; i += numParityNodes;
  }
  else {
    unlockDataNodes = unlockParityNodes = NULL;
  }
  if (nfaults == 2) {
    readQNodes     = &nodes[i]; i += numParityNodes;
    writeQNodes    = &nodes[i]; i += numParityNodes;
    qNodes         = &nodes[i]; i += numParityNodes;
    if (lu_flag) {
      unlockQNodes    = &nodes[i]; i += numParityNodes;
    }
    else {
      unlockQNodes = NULL;
    }
  }
  else {
    readQNodes = writeQNodes = qNodes = unlockQNodes = NULL;
  }
  RF_ASSERT(i == totalNumNodes);
  
  /* Step 3. initialize the nodes */
  /* initialize block node (Nil) */
  nNodes     = numDataNodes + (nfaults * numParityNodes);
  rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, "Nil", allocList);

  /* initialize terminate node (Trm) */
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h, "Trm", allocList);

  /* initialize nodes which read old data (Rod) */
  for (i = 0; i < numDataNodes; i++) {
    rf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h, "Rod", allocList);
    RF_ASSERT(pda != NULL);
    readDataNodes[i].params[0].p = pda;  /* physical disk addr desc */
    readDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);  /* buffer to hold old data */
    readDataNodes[i].params[2].v = parityStripeID;
    readDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);
    pda=pda->next;
    for (j = 0; j < readDataNodes[i].numSuccedents; j++)
      readDataNodes[i].propList[j] = NULL;
  }

  /* initialize nodes which read old parity (Rop) */
  pda = asmap->parityInfo; i = 0;
  for (i = 0; i < numParityNodes; i++) {
    RF_ASSERT(pda != NULL);
    rf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, "Rop", allocList);
    readParityNodes[i].params[0].p = pda;
    readParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);    /* buffer to hold old parity */
    readParityNodes[i].params[2].v = parityStripeID;
    readParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);
    for (j = 0; j < readParityNodes[i].numSuccedents; j++)
      readParityNodes[i].propList[0] = NULL;
    pda=pda->next;
  }

  /* initialize nodes which read old Q (Roq) */
  if (nfaults == 2)
    {
      pda = asmap->qInfo; 
      for (i = 0; i < numParityNodes; i++) {
	RF_ASSERT(pda != NULL);
	rf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, "Roq", allocList);
	readQNodes[i].params[0].p = pda;
	readQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList); /* buffer to hold old Q */
	readQNodes[i].params[2].v = parityStripeID;
	readQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);
	for (j = 0; j < readQNodes[i].numSuccedents; j++)
	  readQNodes[i].propList[0] = NULL;
	pda=pda->next;
      }
    }

  /* initialize nodes which write new data (Wnd) */
  pda = asmap->physInfo;
  for (i=0; i < numDataNodes; i++) {
    RF_ASSERT(pda != NULL);
    rf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wnd", allocList);
    writeDataNodes[i].params[0].p = pda;                    /* physical disk addr desc */
    writeDataNodes[i].params[1].p = pda->bufPtr;   /* buffer holding new data to be written */
    writeDataNodes[i].params[2].v = parityStripeID;
    writeDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);

    if (lu_flag) {
      /* initialize node to unlock the disk queue */
      rf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, "Und", allocList);
      unlockDataNodes[i].params[0].p = pda;                    /* physical disk addr desc */
      unlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);
    }

    pda = pda->next;
  }


  /* initialize nodes which compute new parity and Q */
  /* we use the simple XOR func in the double-XOR case, and when we're accessing only a portion of one stripe unit.
   * the distinction between the two is that the regular XOR func assumes that the targbuf is a full SU in size,
   * and examines the pda associated with the buffer to decide where within the buffer to XOR the data, whereas
   * the simple XOR func just XORs the data into the start of the buffer.
   */
  if ((numParityNodes==2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {
    func = pfuncs->simple; undoFunc = rf_NullNodeUndoFunc; name = pfuncs->SimpleName;
    if (qfuncs) {
      qfunc = qfuncs->simple;
      qname = qfuncs->SimpleName;
    }
  }
  else {
    func = pfuncs->regular; undoFunc = rf_NullNodeUndoFunc; name = pfuncs->RegularName;
    if (qfuncs) { qfunc = qfuncs->regular; qname = qfuncs->RegularName;}
  }
  /* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop} nodes, and raidPtr  */
  if (numParityNodes==2) {        /* double-xor case */
    for (i=0; i < numParityNodes; i++) {
      rf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, name, allocList);  /* no wakeup func for xor */
      xorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;
      xorNodes[i].params[0]   = readDataNodes[i].params[0];
      xorNodes[i].params[1]   = readDataNodes[i].params[1];
      xorNodes[i].params[2]   = readParityNodes[i].params[0];
      xorNodes[i].params[3]   = readParityNodes[i].params[1];
      xorNodes[i].params[4]   = writeDataNodes[i].params[0];
      xorNodes[i].params[5]   = writeDataNodes[i].params[1];
      xorNodes[i].params[6].p = raidPtr;
      xorNodes[i].results[0] = readParityNodes[i].params[1].p;   /* use old parity buf as target buf */
      if (nfaults==2)
	{
	  rf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, qname, allocList);  /* no wakeup func for xor */
	  qNodes[i].params[0]   = readDataNodes[i].params[0];
	  qNodes[i].params[1]   = readDataNodes[i].params[1];
	  qNodes[i].params[2]   = readQNodes[i].params[0];
	  qNodes[i].params[3]   = readQNodes[i].params[1];
	  qNodes[i].params[4]   = writeDataNodes[i].params[0];
	  qNodes[i].params[5]   = writeDataNodes[i].params[1];
	  qNodes[i].params[6].p = raidPtr;
	  qNodes[i].results[0] = readQNodes[i].params[1].p;   /* use old Q buf as target buf */
	}
    }
  }
  else {
    /* there is only one xor node in this case */
    rf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);
    xorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;
    for (i=0; i < numDataNodes + 1; i++) {
      /* set up params related to Rod and Rop nodes */
      xorNodes[0].params[2*i+0] = readDataNodes[i].params[0];    /* pda */
      xorNodes[0].params[2*i+1] = readDataNodes[i].params[1];    /* buffer pointer */
    }
    for (i=0; i < numDataNodes; i++) {
      /* set up params related to Wnd and Wnp nodes */
      xorNodes[0].params[2*(numDataNodes+1+i)+0] = writeDataNodes[i].params[0]; /* pda */
      xorNodes[0].params[2*(numDataNodes+1+i)+1] = writeDataNodes[i].params[1]; /* buffer pointer */
    }
    xorNodes[0].params[2*(numDataNodes+numDataNodes+1)].p = raidPtr;  /* xor node needs to get at RAID information */
    xorNodes[0].results[0] = readParityNodes[0].params[1].p;
    if (nfaults==2) 
      {
	rf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, qname, allocList);
	for (i=0; i<numDataNodes; i++) {
	  /* set up params related to Rod */
	  qNodes[0].params[2*i+0] = readDataNodes[i].params[0];    /* pda */
	  qNodes[0].params[2*i+1] = readDataNodes[i].params[1];    /* buffer pointer */
	}
	/* and read old q */
	qNodes[0].params[2*numDataNodes + 0] = readQNodes[0].params[0];    /* pda */
	qNodes[0].params[2*numDataNodes + 1] = readQNodes[0].params[1];    /* buffer pointer */
	for (i=0; i < numDataNodes; i++) {
	  /* set up params related to Wnd nodes */
	  qNodes[0].params[2*(numDataNodes+1+i)+0] = writeDataNodes[i].params[0]; /* pda */
	  qNodes[0].params[2*(numDataNodes+1+i)+1] = writeDataNodes[i].params[1]; /* buffer pointer */
	}	
	qNodes[0].params[2*(numDataNodes+numDataNodes+1)].p = raidPtr;  /* xor node needs to get at RAID information */
	qNodes[0].results[0] = readQNodes[0].params[1].p;
      }
  }

  /* initialize nodes which write new parity (Wnp) */
  pda = asmap->parityInfo;
  for (i=0;  i < numParityNodes; i++) {
    rf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, "Wnp", allocList);
    RF_ASSERT(pda != NULL);
    writeParityNodes[i].params[0].p = pda;                  /* param 1 (bufPtr) filled in by xor node */
    writeParityNodes[i].params[1].p = xorNodes[i].results[0];     /* buffer pointer for parity write operation */
    writeParityNodes[i].params[2].v = parityStripeID;
    writeParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);

    if (lu_flag) {
      /* initialize node to unlock the disk queue */
      rf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, "Unp", allocList);
      unlockParityNodes[i].params[0].p = pda;                    /* physical disk addr desc */
      unlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);
    }

    pda = pda->next;
  }
  
  /* initialize nodes which write new Q (Wnq) */
  if (nfaults == 2)
    {
      pda = asmap->qInfo;
      for (i=0;  i < numParityNodes; i++) {
	rf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, "Wnq", allocList);
	RF_ASSERT(pda != NULL);
	writeQNodes[i].params[0].p = pda;                  /* param 1 (bufPtr) filled in by xor node */
	writeQNodes[i].params[1].p = qNodes[i].results[0];     /* buffer pointer for parity write operation */
	writeQNodes[i].params[2].v = parityStripeID;
	writeQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
d1646 352
a1997 188
	if (lu_flag) {
	  /* initialize node to unlock the disk queue */
	  rf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, "Unq", allocList);
	  unlockQNodes[i].params[0].p = pda;                    /* physical disk addr desc */
	  unlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);
	}

	pda = pda->next;
      }
    }

  /* Step 4. connect the nodes */

  /* connect header to block node */
  dag_h->succedents[0] = blockNode;

  /* connect block node to read old data nodes */
  RF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));
  for (i = 0; i < numDataNodes; i++) {
    blockNode->succedents[i] = &readDataNodes[i];
    RF_ASSERT(readDataNodes[i].numAntecedents == 1);
    readDataNodes[i].antecedents[0]= blockNode;
    readDataNodes[i].antType[0] = rf_control;
  }
  
  /* connect block node to read old parity nodes */
  for (i = 0; i < numParityNodes; i++) {
    blockNode->succedents[numDataNodes + i] = &readParityNodes[i];
    RF_ASSERT(readParityNodes[i].numAntecedents == 1);
    readParityNodes[i].antecedents[0] = blockNode;
    readParityNodes[i].antType[0] = rf_control;
  }

  /* connect block node to read old Q nodes */
  if (nfaults == 2)
    for (i = 0; i < numParityNodes; i++) {
      blockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];
      RF_ASSERT(readQNodes[i].numAntecedents == 1);
      readQNodes[i].antecedents[0] = blockNode;
      readQNodes[i].antType[0] = rf_control;
    }

  /* connect read old data nodes to write new data nodes */
  for (i = 0; i < numDataNodes; i++) {
    RF_ASSERT(readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1));
    RF_ASSERT(writeDataNodes[i].numAntecedents == 1);
    readDataNodes[i].succedents[0] = &writeDataNodes[i];
    writeDataNodes[i].antecedents[0] = &readDataNodes[i];
    writeDataNodes[i].antType[0] = rf_antiData;
  }
  
  /* connect read old data nodes to xor nodes */
  for (i = 0; i < numDataNodes; i++) {
    for (j = 0; j < numParityNodes; j++){
      RF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);
      readDataNodes[i].succedents[1 + j] = &xorNodes[j];
      xorNodes[j].antecedents[i] = &readDataNodes[i];
      xorNodes[j].antType[i] = rf_trueData;
    }
  }

  /* connect read old data nodes to q nodes */
  if (nfaults == 2)
    for (i = 0; i < numDataNodes; i++)
      for (j = 0; j < numParityNodes; j++){
	RF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);
	readDataNodes[i].succedents[1 + numParityNodes + j] = &qNodes[j];
	qNodes[j].antecedents[i] = &readDataNodes[i];
	qNodes[j].antType[i] = rf_trueData;
      }

  /* connect read old parity nodes to xor nodes */
  for (i = 0; i < numParityNodes; i++) {
    for (j = 0; j < numParityNodes; j++) {
      RF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);
      readParityNodes[i].succedents[j] = &xorNodes[j];
      xorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];
      xorNodes[j].antType[numDataNodes + i] = rf_trueData;
    }
  }

  /* connect read old q nodes to q nodes */
  if (nfaults == 2)
    for (i = 0; i < numParityNodes; i++) {
      for (j = 0; j < numParityNodes; j++) {
	RF_ASSERT(readQNodes[i].numSuccedents == numParityNodes);
	readQNodes[i].succedents[j] = &qNodes[j];
	qNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];
	qNodes[j].antType[numDataNodes + i] = rf_trueData;
      }
    }
  
  /* connect xor nodes to the write new parity nodes */
  for (i = 0; i < numParityNodes; i++) {
    RF_ASSERT(writeParityNodes[i].numAntecedents == numParityNodes);
    for (j = 0; j < numParityNodes; j++) {
      RF_ASSERT(xorNodes[j].numSuccedents == numParityNodes);
      xorNodes[i].succedents[j] = &writeParityNodes[j];
      writeParityNodes[j].antecedents[i] = &xorNodes[i];
      writeParityNodes[j].antType[i] = rf_trueData;
    }
  }

  /* connect q nodes to the write new q nodes */
  if (nfaults == 2)
    for (i = 0; i < numParityNodes; i++) {
      RF_ASSERT(writeQNodes[i].numAntecedents == numParityNodes);
      for (j = 0; j < numParityNodes; j++) {
	RF_ASSERT(qNodes[j].numSuccedents == 1);
	qNodes[i].succedents[j] = &writeQNodes[j];
	writeQNodes[j].antecedents[i] = &qNodes[i];
	writeQNodes[j].antType[i] = rf_trueData;
      }
    }
  
  RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
  RF_ASSERT(termNode->numSuccedents == 0);
  for (i = 0; i < numDataNodes; i++) {
    if (lu_flag) {
      /* connect write new data nodes to unlock nodes */
      RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
      RF_ASSERT(unlockDataNodes[i].numAntecedents == 1);
      writeDataNodes[i].succedents[0] = &unlockDataNodes[i];
      unlockDataNodes[i].antecedents[0] = &writeDataNodes[i];
      unlockDataNodes[i].antType[0] = rf_control;

      /* connect unlock nodes to term node */
      RF_ASSERT(unlockDataNodes[i].numSuccedents == 1);
      unlockDataNodes[i].succedents[0] = termNode;
      termNode->antecedents[i] = &unlockDataNodes[i];
      termNode->antType[i] = rf_control;
    }
    else {
      /* connect write new data nodes to term node */
      RF_ASSERT(writeDataNodes[i].numSuccedents == 1);
      RF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));
      writeDataNodes[i].succedents[0] = termNode;
      termNode->antecedents[i] = &writeDataNodes[i];
      termNode->antType[i] = rf_control;
    }
  }

  for (i = 0; i < numParityNodes; i++) {
    if (lu_flag) {
      /* connect write new parity nodes to unlock nodes */
      RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
      RF_ASSERT(unlockParityNodes[i].numAntecedents == 1);
      writeParityNodes[i].succedents[0] = &unlockParityNodes[i];
      unlockParityNodes[i].antecedents[0] = &writeParityNodes[i];
      unlockParityNodes[i].antType[0] = rf_control;

      /* connect unlock nodes to term node */
      RF_ASSERT(unlockParityNodes[i].numSuccedents == 1);
      unlockParityNodes[i].succedents[0] = termNode;
      termNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];
      termNode->antType[numDataNodes + i] = rf_control;
    }
    else {
      RF_ASSERT(writeParityNodes[i].numSuccedents == 1);
      writeParityNodes[i].succedents[0] = termNode;
      termNode->antecedents[numDataNodes + i] = &writeParityNodes[i];
      termNode->antType[numDataNodes + i] = rf_control;
    }
  }

  if (nfaults == 2)
    for (i = 0; i < numParityNodes; i++) {
      if (lu_flag) {
	/* connect write new Q nodes to unlock nodes */
	RF_ASSERT(writeQNodes[i].numSuccedents == 1);
	RF_ASSERT(unlockQNodes[i].numAntecedents == 1);
	writeQNodes[i].succedents[0] = &unlockQNodes[i];
	unlockQNodes[i].antecedents[0] = &writeQNodes[i];
	unlockQNodes[i].antType[0] = rf_control;

	/* connect unlock nodes to unblock node */
	RF_ASSERT(unlockQNodes[i].numSuccedents == 1);
	unlockQNodes[i].succedents[0] = termNode;
	termNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];
	termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
      }
      else {
	RF_ASSERT(writeQNodes[i].numSuccedents == 1);
	writeQNodes[i].succedents[0] = termNode;
	termNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];
	termNode->antType[numDataNodes + numParityNodes + i] = rf_control;
      }
    }
d2014 1
a2014 1
 *              flags     - general flags (e.g. disk locking) 
d2018 8
a2025 7
void rf_CreateRaidOneWriteDAGFwd(
  RF_Raid_t             *raidPtr,
  RF_AccessStripeMap_t  *asmap,
  RF_DagHeader_t        *dag_h,
  void                  *bp,
  RF_RaidAccessFlags_t   flags,
  RF_AllocListElem_t    *allocList)
d2027 121
a2147 117
  RF_DagNode_t *blockNode, *unblockNode, *termNode;
  RF_DagNode_t *nodes, *wndNode, *wmirNode;
  int nWndNodes, nWmirNodes, i;
  RF_ReconUnitNum_t which_ru;
  RF_PhysDiskAddr_t *pda, *pdaP;
  RF_StripeNum_t parityStripeID;

  parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),
    asmap->raidAddress, &which_ru);
  if (rf_dagDebug) {
    printf("[Creating RAID level 1 write DAG]\n");
  }

  nWmirNodes = (asmap->parityInfo->next) ? 2 : 1;  /* 2 implies access not SU aligned */
  nWndNodes =  (asmap->physInfo->next) ? 2 : 1;

  /* alloc the Wnd nodes and the Wmir node */
  if (asmap->numDataFailed == 1)
    nWndNodes--;
  if (asmap->numParityFailed == 1)
    nWmirNodes--;

  /* total number of nodes = nWndNodes + nWmirNodes + (block + unblock + terminator) */
  RF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);
  i = 0;
  wndNode     = &nodes[i]; i += nWndNodes;
  wmirNode    = &nodes[i]; i += nWmirNodes;
  blockNode   = &nodes[i]; i += 1;
  unblockNode = &nodes[i]; i += 1;
  termNode = &nodes[i]; i += 1;
  RF_ASSERT(i == (nWndNodes + nWmirNodes + 3));

  /* this dag can commit immediately */
  dag_h->numCommitNodes = 0;
  dag_h->numCommits = 0;
  dag_h->numSuccedents = 1;

  /* initialize the unblock and term nodes */
  rf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, "Nil", allocList);
  rf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, "Nil", allocList);
  rf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, "Trm", allocList);

  /* initialize the wnd nodes */
  if (nWndNodes > 0) {
    pda = asmap->physInfo;
    for (i = 0; i < nWndNodes; i++) {
      rf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wpd", allocList);
      RF_ASSERT(pda != NULL);
      wndNode[i].params[0].p = pda;
      wndNode[i].params[1].p = pda->bufPtr;
      wndNode[i].params[2].v = parityStripeID;
      wndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      pda = pda->next;
    }
    RF_ASSERT(pda == NULL);
  }

  /* initialize the mirror nodes */
  if (nWmirNodes > 0) {
    pda = asmap->physInfo;
    pdaP = asmap->parityInfo;
    for (i = 0; i < nWmirNodes; i++) {
      rf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, "Wsd", allocList);
      RF_ASSERT(pda != NULL);
      wmirNode[i].params[0].p = pdaP;
      wmirNode[i].params[1].p = pda->bufPtr;
      wmirNode[i].params[2].v = parityStripeID;
      wmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);
      pda = pda->next;
      pdaP = pdaP->next;
    }
    RF_ASSERT(pda == NULL);
    RF_ASSERT(pdaP == NULL);
  }

  /* link the header node to the block node */
  RF_ASSERT(dag_h->numSuccedents == 1);
  RF_ASSERT(blockNode->numAntecedents == 0);
  dag_h->succedents[0] = blockNode;

  /* link the block node to the write nodes */
  RF_ASSERT(blockNode->numSuccedents == (nWndNodes + nWmirNodes));
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNode[i].numAntecedents == 1);
    blockNode->succedents[i] = &wndNode[i];
    wndNode[i].antecedents[0] = blockNode;
    wndNode[i].antType[0] = rf_control;
  }
  for (i = 0; i < nWmirNodes; i++) {
    RF_ASSERT(wmirNode[i].numAntecedents == 1);
    blockNode->succedents[i + nWndNodes] = &wmirNode[i];
    wmirNode[i].antecedents[0] = blockNode;
    wmirNode[i].antType[0] = rf_control;
  }

  /* link the write nodes to the unblock node */
  RF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));
  for (i = 0; i < nWndNodes; i++) {
    RF_ASSERT(wndNode[i].numSuccedents == 1);
    wndNode[i].succedents[0] = unblockNode;
    unblockNode->antecedents[i] = &wndNode[i];
    unblockNode->antType[i] = rf_control;
  }
  for (i = 0; i < nWmirNodes; i++) {
    RF_ASSERT(wmirNode[i].numSuccedents == 1);
    wmirNode[i].succedents[0] = unblockNode;
    unblockNode->antecedents[i + nWndNodes] = &wmirNode[i];
    unblockNode->antType[i + nWndNodes] = rf_control;
  }

  /* link the unblock node to the term node */
  RF_ASSERT(unblockNode->numSuccedents == 1);
  RF_ASSERT(termNode->numAntecedents == 1);
  RF_ASSERT(termNode->numSuccedents == 0);
  unblockNode->succedents[0] = termNode;
  termNode->antecedents[0] = unblockNode;
  termNode->antType[0] = rf_control;
d2149 1
a2149 1
  return;
@

