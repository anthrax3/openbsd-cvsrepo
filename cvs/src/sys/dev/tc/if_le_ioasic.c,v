head	1.17;
access;
symbols
	OPENBSD_6_2_BASE:1.17
	OPENBSD_6_1:1.17.0.12
	OPENBSD_6_1_BASE:1.17
	OPENBSD_6_0:1.17.0.8
	OPENBSD_6_0_BASE:1.17
	OPENBSD_5_9:1.17.0.2
	OPENBSD_5_9_BASE:1.17
	OPENBSD_5_8:1.17.0.6
	OPENBSD_5_8_BASE:1.17
	OPENBSD_5_7:1.17.0.4
	OPENBSD_5_7_BASE:1.17
	OPENBSD_5_6:1.16.0.6
	OPENBSD_5_6_BASE:1.16
	OPENBSD_5_5:1.16.0.4
	OPENBSD_5_5_BASE:1.16
	OPENBSD_5_4:1.15.0.20
	OPENBSD_5_4_BASE:1.15
	OPENBSD_5_3:1.15.0.18
	OPENBSD_5_3_BASE:1.15
	OPENBSD_5_2:1.15.0.16
	OPENBSD_5_2_BASE:1.15
	OPENBSD_5_1_BASE:1.15
	OPENBSD_5_1:1.15.0.14
	OPENBSD_5_0:1.15.0.12
	OPENBSD_5_0_BASE:1.15
	OPENBSD_4_9:1.15.0.10
	OPENBSD_4_9_BASE:1.15
	OPENBSD_4_8:1.15.0.8
	OPENBSD_4_8_BASE:1.15
	OPENBSD_4_7:1.15.0.4
	OPENBSD_4_7_BASE:1.15
	OPENBSD_4_6:1.15.0.6
	OPENBSD_4_6_BASE:1.15
	OPENBSD_4_5:1.15.0.2
	OPENBSD_4_5_BASE:1.15
	OPENBSD_4_4:1.14.0.4
	OPENBSD_4_4_BASE:1.14
	OPENBSD_4_3:1.14.0.2
	OPENBSD_4_3_BASE:1.14
	OPENBSD_4_2:1.13.0.2
	OPENBSD_4_2_BASE:1.13
	OPENBSD_4_1:1.11.0.20
	OPENBSD_4_1_BASE:1.11
	OPENBSD_4_0:1.11.0.18
	OPENBSD_4_0_BASE:1.11
	OPENBSD_3_9:1.11.0.16
	OPENBSD_3_9_BASE:1.11
	OPENBSD_3_8:1.11.0.14
	OPENBSD_3_8_BASE:1.11
	OPENBSD_3_7:1.11.0.12
	OPENBSD_3_7_BASE:1.11
	OPENBSD_3_6:1.11.0.10
	OPENBSD_3_6_BASE:1.11
	SMP_SYNC_A:1.11
	SMP_SYNC_B:1.11
	OPENBSD_3_5:1.11.0.8
	OPENBSD_3_5_BASE:1.11
	OPENBSD_3_4:1.11.0.6
	OPENBSD_3_4_BASE:1.11
	UBC_SYNC_A:1.11
	OPENBSD_3_3:1.11.0.4
	OPENBSD_3_3_BASE:1.11
	OPENBSD_3_2:1.11.0.2
	OPENBSD_3_2_BASE:1.11
	OPENBSD_3_1:1.10.0.2
	OPENBSD_3_1_BASE:1.10
	UBC_SYNC_B:1.11
	UBC:1.9.0.2
	UBC_BASE:1.9
	OPENBSD_3_0:1.6.0.2
	OPENBSD_3_0_BASE:1.6
	OPENBSD_2_9_BASE:1.5
	OPENBSD_2_9:1.5.0.2
	OPENBSD_2_8:1.4.0.12
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.10
	OPENBSD_2_7_BASE:1.4
	SMP:1.4.0.8
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.6
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.4.0.4
	OPENBSD_2_5_BASE:1.4
	OPENBSD_2_4:1.4.0.2
	OPENBSD_2_4_BASE:1.4
	OPENBSD_2_3:1.3.0.2
	OPENBSD_2_3_BASE:1.3
	OPENBSD_2_2:1.2.0.6
	OPENBSD_2_2_BASE:1.2
	OPENBSD_2_1:1.2.0.4
	OPENBSD_2_1_BASE:1.2
	OPENBSD_2_0:1.2.0.2
	OPENBSD_2_0_BASE:1.2;
locks; strict;
comment	@ * @;


1.17
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.16;
commitid	yM2VFFhpDTeFQlve;

1.16
date	2013.09.24.20.11.05;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2008.08.09.16.42.30;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2007.11.06.18.20.07;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2007.06.18.21.24.43;	author jasper;	state Exp;
branches;
next	1.12;

1.12
date	2007.06.17.21.20.47;	author jasper;	state Exp;
branches;
next	1.11;

1.11
date	2002.05.02.22.56.06;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2002.03.14.01.27.03;	author millert;	state Exp;
branches;
next	1.9;

1.9
date	2001.12.08.02.24.07;	author art;	state Exp;
branches
	1.9.2.1;
next	1.8;

1.8
date	2001.11.30.17.24.19;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.11.06.19.53.20;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2001.06.27.04.45.59;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2000.11.08.14.39.42;	author art;	state Exp;
branches;
next	1.4;

1.4
date	98.09.16.22.41.22;	author jason;	state Exp;
branches
	1.4.8.1;
next	1.3;

1.3
date	97.11.07.08.07.49;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	96.05.10.12.41.29;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	96.05.02.13.51.49;	author deraadt;	state Exp;
branches;
next	;

1.4.8.1
date	2001.05.14.22.26.17;	author niklas;	state Exp;
branches;
next	1.4.8.2;

1.4.8.2
date	2001.07.04.10.43.37;	author niklas;	state Exp;
branches;
next	1.4.8.3;

1.4.8.3
date	2001.11.13.21.10.04;	author niklas;	state Exp;
branches;
next	1.4.8.4;

1.4.8.4
date	2001.12.05.01.02.38;	author niklas;	state Exp;
branches;
next	1.4.8.5;

1.4.8.5
date	2002.03.06.02.11.47;	author niklas;	state Exp;
branches;
next	1.4.8.6;

1.4.8.6
date	2002.03.28.15.09.09;	author niklas;	state Exp;
branches;
next	1.4.8.7;

1.4.8.7
date	2003.03.28.00.38.30;	author niklas;	state Exp;
branches;
next	;

1.9.2.1
date	2002.06.11.03.42.29;	author art;	state Exp;
branches;
next	;


desc
@@


1.17
log
@unifdef INET
@
text
@/*	$OpenBSD: if_le_ioasic.c,v 1.16 2013/09/24 20:11:05 miod Exp $	*/
/*	$NetBSD: if_le_ioasic.c,v 1.18 2001/11/13 06:26:10 lukem Exp $	*/

/*
 * Copyright (c) 1996 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Author: Chris G. Demetriou
 *
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 *
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * LANCE on DEC IOCTL ASIC.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/mbuf.h>
#include <sys/syslog.h>
#include <sys/socket.h>
#include <sys/device.h>

#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#include <dev/ic/lancereg.h>
#include <dev/ic/lancevar.h>
#include <dev/ic/am7990reg.h>
#include <dev/ic/am7990var.h>

#include <dev/tc/if_levar.h>
#include <dev/tc/tcvar.h>
#include <dev/tc/ioasicreg.h>
#include <dev/tc/ioasicvar.h>

struct le_ioasic_softc {
	struct	am7990_softc sc_am7990;	/* glue to MI code */
	struct	lereg1 *sc_r1;		/* LANCE registers */
	/* XXX must match with le_softc of if_levar.h XXX */

	bus_dma_tag_t sc_dmat;		/* bus dma tag */
	bus_dmamap_t sc_dmamap;		/* bus dmamap */
};

int  le_ioasic_match(struct device *, void *, void *);
void le_ioasic_attach(struct device *, struct device *, void *);

struct cfattach le_ioasic_ca = {
	sizeof(struct le_softc), le_ioasic_match, le_ioasic_attach
};

void le_ioasic_copytobuf_gap2(struct lance_softc *, void *, int, int);
void le_ioasic_copyfrombuf_gap2(struct lance_softc *, void *, int, int);
void le_ioasic_copytobuf_gap16(struct lance_softc *, void *, int, int);
void le_ioasic_copyfrombuf_gap16(struct lance_softc *, void *, int, int);
void le_ioasic_zerobuf_gap16(struct lance_softc *, int, int);

int
le_ioasic_match(struct device *parent, void *match, void *aux)
{
	struct ioasicdev_attach_args *d = aux;

	if (strncmp("PMAD-BA ", d->iada_modname, TC_ROM_LLEN) != 0)
		return 0;

	return 1;
}

/* IOASIC LANCE DMA needs 128KB boundary aligned 128KB chunk */
#define	LE_IOASIC_MEMSIZE	(128*1024)
#define	LE_IOASIC_MEMALIGN	(128*1024)

void
le_ioasic_attach(struct device *parent, struct device *self, void *aux)
{
	struct le_ioasic_softc *sc = (void *)self;
	struct ioasicdev_attach_args *d = aux;
	struct lance_softc *le = &sc->sc_am7990.lsc;
	bus_space_tag_t ioasic_bst;
	bus_space_handle_t ioasic_bsh;
	bus_dma_tag_t dmat;
	bus_dma_segment_t seg;
	tc_addr_t tca;
	u_int32_t ssr;
	int rseg;
	caddr_t le_iomem;

	ioasic_bst = ((struct ioasic_softc *)parent)->sc_bst;
	ioasic_bsh = ((struct ioasic_softc *)parent)->sc_bsh;
	dmat = sc->sc_dmat = ((struct ioasic_softc *)parent)->sc_dmat;
	/*
	 * Allocate a DMA area for the chip.
	 */
	if (bus_dmamem_alloc(dmat, LE_IOASIC_MEMSIZE, LE_IOASIC_MEMALIGN,
	    0, &seg, 1, &rseg, BUS_DMA_NOWAIT)) {
		printf("can't allocate DMA area for LANCE\n");
		return;
	}
	if (bus_dmamem_map(dmat, &seg, rseg, LE_IOASIC_MEMSIZE,
	    &le_iomem, BUS_DMA_NOWAIT|BUS_DMA_COHERENT)) {
		printf("can't map DMA area for LANCE\n");
		bus_dmamem_free(dmat, &seg, rseg);
		return;
	}
	/*
	 * Create and load the DMA map for the DMA area.
	 */
	if (bus_dmamap_create(dmat, LE_IOASIC_MEMSIZE, 1,
	    LE_IOASIC_MEMSIZE, 0, BUS_DMA_NOWAIT, &sc->sc_dmamap)) {
		printf("can't create DMA map\n");
		goto bad;
	}
	if (bus_dmamap_load(dmat, sc->sc_dmamap,
	    le_iomem, LE_IOASIC_MEMSIZE, NULL, BUS_DMA_NOWAIT)) {
		printf("can't load DMA map\n");
		goto bad;
	}
	/*
	 * Bind 128KB buffer with IOASIC DMA.
	 */
	tca = IOASIC_DMA_ADDR(sc->sc_dmamap->dm_segs[0].ds_addr);
	bus_space_write_4(ioasic_bst, ioasic_bsh, IOASIC_LANCE_DMAPTR, tca);
	ssr = bus_space_read_4(ioasic_bst, ioasic_bsh, IOASIC_CSR);
	ssr |= IOASIC_CSR_DMAEN_LANCE;
	bus_space_write_4(ioasic_bst, ioasic_bsh, IOASIC_CSR, ssr);

	sc->sc_r1 = (struct lereg1 *)
		TC_DENSE_TO_SPARSE(TC_PHYS_TO_UNCACHED(d->iada_addr));
	le->sc_mem = (void *)TC_PHYS_TO_UNCACHED(le_iomem);
	le->sc_copytodesc = le_ioasic_copytobuf_gap2;
	le->sc_copyfromdesc = le_ioasic_copyfrombuf_gap2;
	le->sc_copytobuf = le_ioasic_copytobuf_gap16;
	le->sc_copyfrombuf = le_ioasic_copyfrombuf_gap16;
	le->sc_zerobuf = le_ioasic_zerobuf_gap16;

	dec_le_common_attach(&sc->sc_am7990,
	    (u_char *)((struct ioasic_softc *)parent)->sc_base
	        + IOASIC_SLOT_2_START);

	ioasic_intr_establish(parent, d->iada_cookie, IPL_NET,
	    am7990_intr, sc, self->dv_xname);
	return;

 bad:
	bus_dmamem_unmap(dmat, le_iomem, LE_IOASIC_MEMSIZE);
	bus_dmamem_free(dmat, &seg, rseg);
}

/*
 * Special memory access functions needed by ioasic-attached LANCE
 * chips.
 */

/*
 * gap2: two bytes of data followed by two bytes of pad.
 *
 * Buffers must be 4-byte aligned.  The code doesn't worry about
 * doing an extra byte.
 */

void
le_ioasic_copytobuf_gap2(struct lance_softc *sc, void *fromv,
    int boff, int len)
{
	volatile caddr_t buf = sc->sc_mem;
	caddr_t from = fromv;
	volatile u_int16_t *bptr;  

	if (boff & 0x1) {
		/* handle unaligned first byte */
		bptr = ((volatile u_int16_t *)buf) + (boff - 1);
		*bptr = (*from++ << 8) | (*bptr & 0xff);
		bptr += 2;  
		len--;
	} else
		bptr = ((volatile u_int16_t *)buf) + boff;
	while (len > 1) {
		*bptr = (from[1] << 8) | (from[0] & 0xff);
		bptr += 2;
		from += 2;
		len -= 2;
	}
	if (len == 1)
		*bptr = (u_int16_t)*from;
}

void
le_ioasic_copyfrombuf_gap2(struct lance_softc *sc, void *tov,
    int boff, int len)
{
	volatile caddr_t buf = sc->sc_mem;
	caddr_t to = tov;
	volatile u_int16_t *bptr;
	u_int16_t tmp;

	if (boff & 0x1) {
		/* handle unaligned first byte */
		bptr = ((volatile u_int16_t *)buf) + (boff - 1);
		*to++ = (*bptr >> 8) & 0xff;
		bptr += 2;
		len--;
	} else
		bptr = ((volatile u_int16_t *)buf) + boff;
	while (len > 1) {
		tmp = *bptr;
		*to++ = tmp & 0xff;
		*to++ = (tmp >> 8) & 0xff;
		bptr += 2;
		len -= 2;
	}
	if (len == 1)
		*to = *bptr & 0xff;
}

/*
 * gap16: 16 bytes of data followed by 16 bytes of pad.
 *
 * Buffers must be 32-byte aligned.
 */

void
le_ioasic_copytobuf_gap16(struct lance_softc *sc, void *fromv,
    int boff, int len)
{
	volatile caddr_t buf = sc->sc_mem;
	caddr_t from = fromv;
	caddr_t bptr;

	bptr = buf + ((boff << 1) & ~0x1f);
	boff &= 0xf;

	/*
	 * Dispose of boff so destination of subsequent copies is
	 * 16-byte aligned.
	 */
	if (boff) {
		int xfer;
		xfer = min(len, 16 - boff);
		bcopy(from, bptr + boff, xfer);
		from += xfer;
		bptr += 32;
		len -= xfer;
	}

	/* Destination of  copies is now 16-byte aligned. */
	if (len >= 16)
		switch ((u_long)from & (sizeof(u_int32_t) -1)) {
		case 2:
			/*  Ethernet headers make this the dominant case. */
		do {
			u_int32_t *dst = (u_int32_t*)bptr;
			u_int16_t t0;
			u_int32_t t1,  t2, t3, t4;

			/* read from odd-16-bit-aligned, cached src */
			t0 = *(u_int16_t*)from;
			t1 = *(u_int32_t*)(from+2);
			t2 = *(u_int32_t*)(from+6);
			t3 = *(u_int32_t*)(from+10);
			t4 = *(u_int16_t*)(from+14);

			/* DMA buffer is uncached on mips */
			dst[0] =         t0 |  (t1 << 16);
			dst[1] = (t1 >> 16) |  (t2 << 16);
			dst[2] = (t2 >> 16) |  (t3 << 16);
			dst[3] = (t3 >> 16) |  (t4 << 16);

			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;

		case 0:
		do {
			u_int32_t *src = (u_int32_t*)from;
			u_int32_t *dst = (u_int32_t*)bptr;
			u_int32_t t0, t1, t2, t3;

			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];
			dst[0] = t0; dst[1] = t1; dst[2] = t2; dst[3] = t3;

			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;

		default: 
		/* Does odd-aligned case ever happen? */
		do {
			bcopy(from, bptr, 16);
			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;
	}
	if (len)
		bcopy(from, bptr, len);
}

void
le_ioasic_copyfrombuf_gap16(struct lance_softc *sc, void *tov,
    int boff, int len)
{
	volatile caddr_t buf = sc->sc_mem;
	caddr_t to = tov;
	caddr_t bptr;

	bptr = buf + ((boff << 1) & ~0x1f);
	boff &= 0xf;

	/* Dispose of boff. source of copy is subsequently 16-byte aligned. */
	if (boff) {
		int xfer;
		xfer = min(len, 16 - boff);
		bcopy(bptr+boff, to, xfer);
		to += xfer;
		bptr += 32;
		len -= xfer;
	}
	if (len >= 16)
	switch ((u_long)to & (sizeof(u_int32_t) -1)) {
	case 2:
		/*
		 * to is aligned to an odd 16-bit boundary.  Ethernet headers
		 * make this the dominant case (98% or more).
		 */
		do {
			u_int32_t *src = (u_int32_t*)bptr;
			u_int32_t t0, t1, t2, t3;

			/* read from uncached aligned DMA buf */
			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];

			/* write to odd-16-bit-word aligned dst */
			*(u_int16_t *) (to+0)  = (u_short)  t0;
			*(u_int32_t *) (to+2)  = (t0 >> 16) |  (t1 << 16);
			*(u_int32_t *) (to+6)  = (t1 >> 16) |  (t2 << 16);
			*(u_int32_t *) (to+10) = (t2 >> 16) |  (t3 << 16);
			*(u_int16_t *) (to+14) = (t3 >> 16);
			bptr += 32;
			to += 16;
			len -= 16;
		} while (len > 16);
		break;
	case 0:
		/* 32-bit aligned aligned copy. Rare. */
		do {
			u_int32_t *src = (u_int32_t*)bptr;
			u_int32_t *dst = (u_int32_t*)to;
			u_int32_t t0, t1, t2, t3;

			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];
			dst[0] = t0; dst[1] = t1; dst[2] = t2; dst[3] = t3;
			to += 16;
			bptr += 32;
			len -= 16;
		} while (len  > 16);
		break;

	/* XXX Does odd-byte-aligned case ever happen? */
	default:
		do {
			bcopy(bptr, to, 16);
			to += 16;
			bptr += 32;
			len -= 16;
		} while (len  > 16);
		break;
	}
	if (len)
		bcopy(bptr, to, len);
}

void
le_ioasic_zerobuf_gap16(struct lance_softc *sc, int boff, int len)
{
	volatile caddr_t buf = sc->sc_mem;
	caddr_t bptr;
	int xfer;

	bptr = buf + ((boff << 1) & ~0x1f);
	boff &= 0xf;
	xfer = min(len, 16 - boff);
	while (len > 0) {
		bzero(bptr + boff, xfer);
		bptr += 32;
		boff = 0;
		len -= xfer;
		xfer = min(len, 16);
	}
}
@


1.16
log
@Sync the MI LANCE code ( le(4) ) with NetBSD, except for the following:
- the am7990_get() - now lance_get() - is unchanged.
- the interrupt acknowledge logic is unchanged, and will disable interrupts,
  then acknowledge all interrupt conditions.

Add ILACC (79900) support (from NetBSD).

Both LANCE (am7990.c) and ILACC (am79900.c) code share as much common code
(lance.c) as possible. This affects all le(4) attachments, but the changes
are mostly mechanical, to split am7990-specific parts from lance-agnostic
parts.

Compile tested on all affected platforms. Tested on alpha, hp300, luna88k,
mvme88k, sparc, sparc64 and vax.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.15 2008/08/09 16:42:30 miod Exp $	*/
a44 1
#ifdef INET
a46 1
#endif
@


1.15
log
@Pass a device name to {tc,tcds,ioasic}_intr_establish in order to get
meaningful names associated to the interrupt counters.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.14 2007/11/06 18:20:07 miod Exp $	*/
d50 2
d76 5
a80 9
void le_ioasic_copytobuf_gap2(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copyfrombuf_gap2(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copytobuf_gap16(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copyfrombuf_gap16(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_zerobuf_gap16(struct am7990_softc *, int, int);
d102 1
a102 1
	struct am7990_softc *le = &sc->sc_am7990;
d186 1
a186 1
le_ioasic_copytobuf_gap2(struct am7990_softc *sc, void *fromv,
d212 1
a212 1
le_ioasic_copyfrombuf_gap2(struct am7990_softc *sc, void *tov,
d246 1
a246 1
le_ioasic_copytobuf_gap16(struct am7990_softc *sc, void *fromv,
d328 1
a328 1
le_ioasic_copyfrombuf_gap16(struct am7990_softc *sc, void *tov,
d402 1
a402 1
le_ioasic_zerobuf_gap16(struct am7990_softc *sc, int boff, int len)
@


1.14
log
@Get rid of TC_IPL_xxx values and tc_intrlevel_t, and use IPL_xxx and int.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.13 2007/06/18 21:24:43 jasper Exp $	*/
d167 1
a167 1
	    am7990_intr, sc);
@


1.13
log
@fix pasto's
from janjaap@@stack.nl
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.12 2007/06/17 21:20:47 jasper Exp $	*/
d166 1
a166 1
	ioasic_intr_establish(parent, d->iada_cookie, TC_IPL_NET,
@


1.12
log
@ansify/de-register

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.11 2002/05/02 22:56:06 miod Exp $	*/
d85 1
a85 1
le_ioasic_match(struct device parent, void *match, void *aux)
@


1.11
log
@Big TURBOchannel support catchup from NetBSD, part 1.
A few local changes and tweaks remain.

This bring DEC 3000 machines back in the game, but framebuffers are still
not supported at the moment.

Thanks to ericj@@ and nate@@ for supplying me a DEC 3000 for testing.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d85 1
a85 3
le_ioasic_match(parent, match, aux)
	struct device *parent;
	void *match, *aux;
d100 1
a100 3
le_ioasic_attach(parent, self, aux)
	struct device *parent, *self;
	void *aux;
d188 2
a189 5
le_ioasic_copytobuf_gap2(sc, fromv, boff, len)
	struct am7990_softc *sc;  
	void *fromv;
	int boff;
	int len;
d214 2
a215 4
le_ioasic_copyfrombuf_gap2(sc, tov, boff, len)
	struct am7990_softc *sc;
	void *tov;
	int boff, len;
d248 2
a249 5
le_ioasic_copytobuf_gap16(sc, fromv, boff, len)
	struct am7990_softc *sc;
	void *fromv;
	int boff;
	int len;
d330 2
a331 4
le_ioasic_copyfrombuf_gap16(sc, tov, boff, len)
	struct am7990_softc *sc;
	void *tov;
	int boff, len;
d404 1
a404 3
le_ioasic_zerobuf_gap16(sc, boff, len)
	struct am7990_softc *sc;
	int boff, len;
@


1.10
log
@First round of __P removal in sys
@
text
@d1 2
a2 2
/*	$OpenBSD: if_le_ioasic.c,v 1.9 2001/12/08 02:24:07 art Exp $	*/
/*	$NetBSD: if_le_ioasic.c,v 1.2 1996/05/07 02:24:56 thorpej Exp $	*/
a44 2
#include <uvm/uvm_extern.h>

d55 1
d58 4
a61 2
int	le_ioasic_match(struct device *, void *, void *);
void	le_ioasic_attach(struct device *, struct device *, void *);
d63 3
a65 4
hide void le_ioasic_copytobuf_gap2(struct am7990_softc *, void *,
	    int, int);
hide void le_ioasic_copyfrombuf_gap2(struct am7990_softc *, void *,
	    int, int);
d67 2
a68 5
hide void le_ioasic_copytobuf_gap16(struct am7990_softc *, void *,
	    int, int);
hide void le_ioasic_copyfrombuf_gap16(struct am7990_softc *, void *,
	    int, int);
hide void le_ioasic_zerobuf_gap16(struct am7990_softc *, int, int);
d74 10
d91 2
a92 4
	if (!ioasic_submatch(match, aux))
		return (0);
	if (strncmp("lance", d->iada_modname, TC_ROM_LLEN))
		return (0);
d94 1
a94 1
	return (1);
d97 4
a100 2
#define LE_IOASIC_MEMSIZE	(128*1024)
#define LE_IOASIC_MEMALIGN	(128*1024)
d106 1
d108 8
a115 2
	struct le_softc *lesc = (void *)self;
	struct am7990_softc *sc = &lesc->sc_am7990;
a116 4
	struct pglist pglist;
	struct vm_page *pg;
	vaddr_t va;
	vsize_t size;
d118 17
d136 1
a136 1
	 * XXX - this vm juggling is so wrong. use bus_dma instead!
d138 9
a146 12
	size = round_page(LE_IOASIC_MEMSIZE);
	if (uvm_pglistalloc(size, 0, 0, LE_IOASIC_MEMALIGN, 0, &pglist, 1, 0) ||
	    uvm_map(kernel_map, &va, size, NULL, UVM_UNKNOWN_OFFSET, 0,
		UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
			UVM_ADV_RANDOM, 0)))
		panic("aha_init: could not allocate mailbox");

	le_iomem = (caddr_t)va;
	for (pg = TAILQ_FIRST(&pglist); pg != NULL;pg = TAILQ_NEXT(pg, pageq)) {
		pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg),
			VM_PROT_READ|VM_PROT_WRITE);
		va += PAGE_SIZE;
a147 1
	pmap_update(pmap_kernel());
d149 1
a149 1
	 * XXXEND
d151 5
d157 1
a157 1
	lesc->sc_r1 = (struct lereg1 *)
d159 10
a168 11
	sc->sc_mem = (void *)TC_PHYS_TO_UNCACHED(le_iomem);

	sc->sc_copytodesc = le_ioasic_copytobuf_gap2;
	sc->sc_copyfromdesc = le_ioasic_copyfrombuf_gap2;
	sc->sc_copytobuf = le_ioasic_copytobuf_gap16;
	sc->sc_copyfrombuf = le_ioasic_copyfrombuf_gap16;
	sc->sc_zerobuf = le_ioasic_zerobuf_gap16;

	ioasic_lance_dma_setup(le_iomem);	/* XXX more thought */

	dec_le_common_attach(sc, ioasic_lance_ether_address());
d172 5
d196 1
a196 1
	register int len;
d199 2
a200 2
	register caddr_t from = fromv;
	register volatile u_int16_t *bptr;  
d227 3
a229 3
	register caddr_t to = tov;
	register volatile u_int16_t *bptr;
	register u_int16_t tmp;
d261 1
a261 1
	register int len;
d264 2
a265 3
	register caddr_t from = fromv;
	register caddr_t bptr;
	register int xfer;
d269 8
a276 2
	xfer = min(len, 16 - boff);
	while (len > 0) {
a279 1
		boff = 0;
a280 1
		xfer = min(len, 16);
d282 57
d348 2
a349 3
	register caddr_t to = tov;
	register caddr_t bptr;
	register int xfer;
d353 6
a358 3
	xfer = min(len, 16 - boff);
	while (len > 0) {
		bcopy(bptr + boff, to, xfer);
a360 1
		boff = 0;
a361 1
		xfer = min(len, 16);
d363 52
d423 2
a424 2
	register caddr_t bptr;
	register int xfer;
@


1.9
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.8 2001/11/30 17:24:19 art Exp $	*/
d59 2
a60 2
int	le_ioasic_match __P((struct device *, void *, void *));
void	le_ioasic_attach __P((struct device *, struct device *, void *));
d62 10
a71 10
hide void le_ioasic_copytobuf_gap2 __P((struct am7990_softc *, void *,
	    int, int));
hide void le_ioasic_copyfrombuf_gap2 __P((struct am7990_softc *, void *,
	    int, int));

hide void le_ioasic_copytobuf_gap16 __P((struct am7990_softc *, void *,
	    int, int));
hide void le_ioasic_copyfrombuf_gap16 __P((struct am7990_softc *, void *,
	    int, int));
hide void le_ioasic_zerobuf_gap16 __P((struct am7990_softc *, int, int));
@


1.9.2.1
log
@Sync UBC branch to -current
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: if_le_ioasic.c,v 1.18 2001/11/13 06:26:10 lukem Exp $	*/
d45 2
a56 1
#include <dev/tc/ioasicreg.h>
d59 2
a60 8
struct le_ioasic_softc {
	struct	am7990_softc sc_am7990;	/* glue to MI code */
	struct	lereg1 *sc_r1;		/* LANCE registers */
	/* XXX must match with le_softc of if_levar.h XXX */

	bus_dma_tag_t sc_dmat;		/* bus dma tag */
	bus_dmamap_t sc_dmamap;		/* bus dmamap */
};
d62 10
a71 2
int  le_ioasic_match(struct device *, void *, void *);
void le_ioasic_attach(struct device *, struct device *, void *);
a76 10
void le_ioasic_copytobuf_gap2(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copyfrombuf_gap2(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copytobuf_gap16(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copyfrombuf_gap16(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_zerobuf_gap16(struct am7990_softc *, int, int);

d84 4
a87 2
	if (strncmp("PMAD-BA ", d->iada_modname, TC_ROM_LLEN) != 0)
		return 0;
d89 1
a89 1
	return 1;
d92 2
a93 4
/* IOASIC LANCE DMA needs 128KB boundary aligned 128KB chunk */
#define	LE_IOASIC_MEMSIZE	(128*1024)
#define	LE_IOASIC_MEMALIGN	(128*1024)

a98 1
	struct le_ioasic_softc *sc = (void *)self;
d100 2
a101 8
	struct am7990_softc *le = &sc->sc_am7990;
	bus_space_tag_t ioasic_bst;
	bus_space_handle_t ioasic_bsh;
	bus_dma_tag_t dmat;
	bus_dma_segment_t seg;
	tc_addr_t tca;
	u_int32_t ssr;
	int rseg;
d103 4
a107 3
	ioasic_bst = ((struct ioasic_softc *)parent)->sc_bst;
	ioasic_bsh = ((struct ioasic_softc *)parent)->sc_bsh;
	dmat = sc->sc_dmat = ((struct ioasic_softc *)parent)->sc_dmat;
d109 1
a109 1
	 * Allocate a DMA area for the chip.
d111 12
a122 10
	if (bus_dmamem_alloc(dmat, LE_IOASIC_MEMSIZE, LE_IOASIC_MEMALIGN,
	    0, &seg, 1, &rseg, BUS_DMA_NOWAIT)) {
		printf("can't allocate DMA area for LANCE\n");
		return;
	}
	if (bus_dmamem_map(dmat, &seg, rseg, LE_IOASIC_MEMSIZE,
	    &le_iomem, BUS_DMA_NOWAIT|BUS_DMA_COHERENT)) {
		printf("can't map DMA area for LANCE\n");
		bus_dmamem_free(dmat, &seg, rseg);
		return;
d124 1
d126 1
a126 1
	 * Create and load the DMA map for the DMA area.
a127 18
	if (bus_dmamap_create(dmat, LE_IOASIC_MEMSIZE, 1,
	    LE_IOASIC_MEMSIZE, 0, BUS_DMA_NOWAIT, &sc->sc_dmamap)) {
		printf("can't create DMA map\n");
		goto bad;
	}
	if (bus_dmamap_load(dmat, sc->sc_dmamap,
	    le_iomem, LE_IOASIC_MEMSIZE, NULL, BUS_DMA_NOWAIT)) {
		printf("can't load DMA map\n");
		goto bad;
	}
	/*
	 * Bind 128KB buffer with IOASIC DMA.
	 */
	tca = IOASIC_DMA_ADDR(sc->sc_dmamap->dm_segs[0].ds_addr);
	bus_space_write_4(ioasic_bst, ioasic_bsh, IOASIC_LANCE_DMAPTR, tca);
	ssr = bus_space_read_4(ioasic_bst, ioasic_bsh, IOASIC_CSR);
	ssr |= IOASIC_CSR_DMAEN_LANCE;
	bus_space_write_4(ioasic_bst, ioasic_bsh, IOASIC_CSR, ssr);
d129 1
a129 1
	sc->sc_r1 = (struct lereg1 *)
d131 11
a141 10
	le->sc_mem = (void *)TC_PHYS_TO_UNCACHED(le_iomem);
	le->sc_copytodesc = le_ioasic_copytobuf_gap2;
	le->sc_copyfromdesc = le_ioasic_copyfrombuf_gap2;
	le->sc_copytobuf = le_ioasic_copytobuf_gap16;
	le->sc_copyfrombuf = le_ioasic_copyfrombuf_gap16;
	le->sc_zerobuf = le_ioasic_zerobuf_gap16;

	dec_le_common_attach(&sc->sc_am7990,
	    (u_char *)((struct ioasic_softc *)parent)->sc_base
	        + IOASIC_SLOT_2_START);
a144 5
	return;

 bad:
	bus_dmamem_unmap(dmat, le_iomem, LE_IOASIC_MEMSIZE);
	bus_dmamem_free(dmat, &seg, rseg);
d164 1
a164 1
	int len;
d167 2
a168 2
	caddr_t from = fromv;
	volatile u_int16_t *bptr;  
d195 3
a197 3
	caddr_t to = tov;
	volatile u_int16_t *bptr;
	u_int16_t tmp;
d229 1
a229 1
	int len;
d232 3
a234 2
	caddr_t from = fromv;
	caddr_t bptr;
d238 2
a239 8

	/*
	 * Dispose of boff so destination of subsequent copies is
	 * 16-byte aligned.
	 */
	if (boff) {
		int xfer;
		xfer = min(len, 16 - boff);
d243 1
d245 1
a246 57

	/* Destination of  copies is now 16-byte aligned. */
	if (len >= 16)
		switch ((u_long)from & (sizeof(u_int32_t) -1)) {
		case 2:
			/*  Ethernet headers make this the dominant case. */
		do {
			u_int32_t *dst = (u_int32_t*)bptr;
			u_int16_t t0;
			u_int32_t t1,  t2, t3, t4;

			/* read from odd-16-bit-aligned, cached src */
			t0 = *(u_int16_t*)from;
			t1 = *(u_int32_t*)(from+2);
			t2 = *(u_int32_t*)(from+6);
			t3 = *(u_int32_t*)(from+10);
			t4 = *(u_int16_t*)(from+14);

			/* DMA buffer is uncached on mips */
			dst[0] =         t0 |  (t1 << 16);
			dst[1] = (t1 >> 16) |  (t2 << 16);
			dst[2] = (t2 >> 16) |  (t3 << 16);
			dst[3] = (t3 >> 16) |  (t4 << 16);

			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;

		case 0:
		do {
			u_int32_t *src = (u_int32_t*)from;
			u_int32_t *dst = (u_int32_t*)bptr;
			u_int32_t t0, t1, t2, t3;

			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];
			dst[0] = t0; dst[1] = t1; dst[2] = t2; dst[3] = t3;

			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;

		default: 
		/* Does odd-aligned case ever happen? */
		do {
			bcopy(from, bptr, 16);
			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;
	}
	if (len)
		bcopy(from, bptr, len);
d256 3
a258 2
	caddr_t to = tov;
	caddr_t bptr;
d262 3
a264 6

	/* Dispose of boff. source of copy is subsequently 16-byte aligned. */
	if (boff) {
		int xfer;
		xfer = min(len, 16 - boff);
		bcopy(bptr+boff, to, xfer);
d267 1
d269 1
a270 52
	if (len >= 16)
	switch ((u_long)to & (sizeof(u_int32_t) -1)) {
	case 2:
		/*
		 * to is aligned to an odd 16-bit boundary.  Ethernet headers
		 * make this the dominant case (98% or more).
		 */
		do {
			u_int32_t *src = (u_int32_t*)bptr;
			u_int32_t t0, t1, t2, t3;

			/* read from uncached aligned DMA buf */
			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];

			/* write to odd-16-bit-word aligned dst */
			*(u_int16_t *) (to+0)  = (u_short)  t0;
			*(u_int32_t *) (to+2)  = (t0 >> 16) |  (t1 << 16);
			*(u_int32_t *) (to+6)  = (t1 >> 16) |  (t2 << 16);
			*(u_int32_t *) (to+10) = (t2 >> 16) |  (t3 << 16);
			*(u_int16_t *) (to+14) = (t3 >> 16);
			bptr += 32;
			to += 16;
			len -= 16;
		} while (len > 16);
		break;
	case 0:
		/* 32-bit aligned aligned copy. Rare. */
		do {
			u_int32_t *src = (u_int32_t*)bptr;
			u_int32_t *dst = (u_int32_t*)to;
			u_int32_t t0, t1, t2, t3;

			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];
			dst[0] = t0; dst[1] = t1; dst[2] = t2; dst[3] = t3;
			to += 16;
			bptr += 32;
			len -= 16;
		} while (len  > 16);
		break;

	/* XXX Does odd-byte-aligned case ever happen? */
	default:
		do {
			bcopy(bptr, to, 16);
			to += 16;
			bptr += 32;
			len -= 16;
		} while (len  > 16);
		break;
	}
	if (len)
		bcopy(bptr, to, len);
d279 2
a280 2
	caddr_t bptr;
	int xfer;
@


1.8
log
@Kill uvm_pagealloc_contig. The two drivers that still used it should have
been converted to bus_dma ages ago, but since noone haven't bothered to do that
I haven't bothered to do more than to test that the kernel still builds
with those changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.7 2001/11/06 19:53:20 miod Exp $	*/
d124 1
@


1.7
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.6 2001/06/27 04:45:59 art Exp $	*/
d100 2
a101 2
	register struct le_softc *lesc = (void *)self;
	register struct am7990_softc *sc = &lesc->sc_am7990;
d103 24
a126 2

	le_iomem = (caddr_t)uvm_pagealloc_contig(LE_IOASIC_MEMSIZE, 0, 0, LE_IOASIC_MEMALIGN);
@


1.6
log
@zap old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.5 2000/11/08 14:39:42 art Exp $	*/
a44 1
#include <vm/vm.h>
@


1.5
log
@allocate le_iomem in le_ioasic_attach instead of MD code,
only for UVM.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.4 1998/09/16 22:41:22 jason Exp $	*/
a45 1
#ifdef UVM
a46 1
#endif
a102 1
#ifdef UVM
a103 3
#else
	extern caddr_t le_iomem;
#endif
a104 1
#ifdef UVM
a105 1
#endif
@


1.4
log
@o if_media'fied am7990
o if_media'fied sun4m le.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.3 1997/11/07 08:07:49 niklas Exp $	*/
d45 5
a61 2
extern caddr_t le_iomem;

d95 2
d105 9
@


1.4.8.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.5 2000/11/08 14:39:42 art Exp $	*/
a44 5
#include <vm/vm.h>
#ifdef UVM
#include <uvm/uvm_extern.h>
#endif

d57 2
a91 2
#define LE_IOASIC_MEMSIZE	(128*1024)
#define LE_IOASIC_MEMALIGN	(128*1024)
a99 9
#ifdef UVM
	caddr_t le_iomem;
#else
	extern caddr_t le_iomem;
#endif

#ifdef UVM
	le_iomem = (caddr_t)uvm_pagealloc_contig(LE_IOASIC_MEMSIZE, 0, 0, LE_IOASIC_MEMALIGN);
#endif
@


1.4.8.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.4.8.1 2001/05/14 22:26:17 niklas Exp $	*/
d46 1
d48 1
d105 1
d107 3
d111 1
d113 1
@


1.4.8.3
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d45 1
@


1.4.8.4
log
@Merge in -current
@
text
@d100 2
a101 2
	struct le_softc *lesc = (void *)self;
	struct am7990_softc *sc = &lesc->sc_am7990;
a102 4
	struct pglist pglist;
	struct vm_page *pg;
	vaddr_t va;
	vsize_t size;
d104 1
a104 19
	/*
	 * XXX - this vm juggling is so wrong. use bus_dma instead!
	 */
	size = round_page(LE_IOASIC_MEMSIZE);
	if (uvm_pglistalloc(size, 0, 0, LE_IOASIC_MEMALIGN, 0, &pglist, 1, 0) ||
	    uvm_map(kernel_map, &va, size, NULL, UVM_UNKNOWN_OFFSET, 0,
		UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
			UVM_ADV_RANDOM, 0)))
		panic("aha_init: could not allocate mailbox");

	le_iomem = (caddr_t)va;
	for (pg = TAILQ_FIRST(&pglist); pg != NULL;pg = TAILQ_NEXT(pg, pageq)) {
		pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg),
			VM_PROT_READ|VM_PROT_WRITE);
		va += PAGE_SIZE;
	}
	/*
	 * XXXEND
	 */
@


1.4.8.5
log
@Merge in trunk
@
text
@a123 1
	pmap_update(pmap_kernel());
@


1.4.8.6
log
@Merge in -current from roughly a week ago
@
text
@d59 2
a60 2
int	le_ioasic_match(struct device *, void *, void *);
void	le_ioasic_attach(struct device *, struct device *, void *);
d62 4
a65 4
hide void le_ioasic_copytobuf_gap2(struct am7990_softc *, void *,
	    int, int);
hide void le_ioasic_copyfrombuf_gap2(struct am7990_softc *, void *,
	    int, int);
d67 5
a71 5
hide void le_ioasic_copytobuf_gap16(struct am7990_softc *, void *,
	    int, int);
hide void le_ioasic_copyfrombuf_gap16(struct am7990_softc *, void *,
	    int, int);
hide void le_ioasic_zerobuf_gap16(struct am7990_softc *, int, int);
@


1.4.8.7
log
@Sync the SMP branch with 3.3
@
text
@d2 1
a2 1
/*	$NetBSD: if_le_ioasic.c,v 1.18 2001/11/13 06:26:10 lukem Exp $	*/
d45 2
a56 1
#include <dev/tc/ioasicreg.h>
d59 2
a60 4
struct le_ioasic_softc {
	struct	am7990_softc sc_am7990;	/* glue to MI code */
	struct	lereg1 *sc_r1;		/* LANCE registers */
	/* XXX must match with le_softc of if_levar.h XXX */
d62 4
a65 3
	bus_dma_tag_t sc_dmat;		/* bus dma tag */
	bus_dmamap_t sc_dmamap;		/* bus dmamap */
};
d67 5
a71 2
int  le_ioasic_match(struct device *, void *, void *);
void le_ioasic_attach(struct device *, struct device *, void *);
a76 10
void le_ioasic_copytobuf_gap2(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copyfrombuf_gap2(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copytobuf_gap16(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_copyfrombuf_gap16(struct am7990_softc *, void *,
	    int, int);
void le_ioasic_zerobuf_gap16(struct am7990_softc *, int, int);

d84 4
a87 2
	if (strncmp("PMAD-BA ", d->iada_modname, TC_ROM_LLEN) != 0)
		return 0;
d89 1
a89 1
	return 1;
d92 2
a93 4
/* IOASIC LANCE DMA needs 128KB boundary aligned 128KB chunk */
#define	LE_IOASIC_MEMSIZE	(128*1024)
#define	LE_IOASIC_MEMALIGN	(128*1024)

a98 1
	struct le_ioasic_softc *sc = (void *)self;
d100 2
a101 8
	struct am7990_softc *le = &sc->sc_am7990;
	bus_space_tag_t ioasic_bst;
	bus_space_handle_t ioasic_bsh;
	bus_dma_tag_t dmat;
	bus_dma_segment_t seg;
	tc_addr_t tca;
	u_int32_t ssr;
	int rseg;
d103 4
a107 17
	ioasic_bst = ((struct ioasic_softc *)parent)->sc_bst;
	ioasic_bsh = ((struct ioasic_softc *)parent)->sc_bsh;
	dmat = sc->sc_dmat = ((struct ioasic_softc *)parent)->sc_dmat;
	/*
	 * Allocate a DMA area for the chip.
	 */
	if (bus_dmamem_alloc(dmat, LE_IOASIC_MEMSIZE, LE_IOASIC_MEMALIGN,
	    0, &seg, 1, &rseg, BUS_DMA_NOWAIT)) {
		printf("can't allocate DMA area for LANCE\n");
		return;
	}
	if (bus_dmamem_map(dmat, &seg, rseg, LE_IOASIC_MEMSIZE,
	    &le_iomem, BUS_DMA_NOWAIT|BUS_DMA_COHERENT)) {
		printf("can't map DMA area for LANCE\n");
		bus_dmamem_free(dmat, &seg, rseg);
		return;
	}
d109 1
a109 1
	 * Create and load the DMA map for the DMA area.
d111 12
a122 9
	if (bus_dmamap_create(dmat, LE_IOASIC_MEMSIZE, 1,
	    LE_IOASIC_MEMSIZE, 0, BUS_DMA_NOWAIT, &sc->sc_dmamap)) {
		printf("can't create DMA map\n");
		goto bad;
	}
	if (bus_dmamap_load(dmat, sc->sc_dmamap,
	    le_iomem, LE_IOASIC_MEMSIZE, NULL, BUS_DMA_NOWAIT)) {
		printf("can't load DMA map\n");
		goto bad;
d124 1
d126 1
a126 1
	 * Bind 128KB buffer with IOASIC DMA.
a127 5
	tca = IOASIC_DMA_ADDR(sc->sc_dmamap->dm_segs[0].ds_addr);
	bus_space_write_4(ioasic_bst, ioasic_bsh, IOASIC_LANCE_DMAPTR, tca);
	ssr = bus_space_read_4(ioasic_bst, ioasic_bsh, IOASIC_CSR);
	ssr |= IOASIC_CSR_DMAEN_LANCE;
	bus_space_write_4(ioasic_bst, ioasic_bsh, IOASIC_CSR, ssr);
d129 1
a129 1
	sc->sc_r1 = (struct lereg1 *)
d131 11
a141 10
	le->sc_mem = (void *)TC_PHYS_TO_UNCACHED(le_iomem);
	le->sc_copytodesc = le_ioasic_copytobuf_gap2;
	le->sc_copyfromdesc = le_ioasic_copyfrombuf_gap2;
	le->sc_copytobuf = le_ioasic_copytobuf_gap16;
	le->sc_copyfrombuf = le_ioasic_copyfrombuf_gap16;
	le->sc_zerobuf = le_ioasic_zerobuf_gap16;

	dec_le_common_attach(&sc->sc_am7990,
	    (u_char *)((struct ioasic_softc *)parent)->sc_base
	        + IOASIC_SLOT_2_START);
a144 5
	return;

 bad:
	bus_dmamem_unmap(dmat, le_iomem, LE_IOASIC_MEMSIZE);
	bus_dmamem_free(dmat, &seg, rseg);
d164 1
a164 1
	int len;
d167 2
a168 2
	caddr_t from = fromv;
	volatile u_int16_t *bptr;  
d195 3
a197 3
	caddr_t to = tov;
	volatile u_int16_t *bptr;
	u_int16_t tmp;
d229 1
a229 1
	int len;
d232 3
a234 2
	caddr_t from = fromv;
	caddr_t bptr;
d238 2
a239 8

	/*
	 * Dispose of boff so destination of subsequent copies is
	 * 16-byte aligned.
	 */
	if (boff) {
		int xfer;
		xfer = min(len, 16 - boff);
d243 1
d245 1
a246 57

	/* Destination of  copies is now 16-byte aligned. */
	if (len >= 16)
		switch ((u_long)from & (sizeof(u_int32_t) -1)) {
		case 2:
			/*  Ethernet headers make this the dominant case. */
		do {
			u_int32_t *dst = (u_int32_t*)bptr;
			u_int16_t t0;
			u_int32_t t1,  t2, t3, t4;

			/* read from odd-16-bit-aligned, cached src */
			t0 = *(u_int16_t*)from;
			t1 = *(u_int32_t*)(from+2);
			t2 = *(u_int32_t*)(from+6);
			t3 = *(u_int32_t*)(from+10);
			t4 = *(u_int16_t*)(from+14);

			/* DMA buffer is uncached on mips */
			dst[0] =         t0 |  (t1 << 16);
			dst[1] = (t1 >> 16) |  (t2 << 16);
			dst[2] = (t2 >> 16) |  (t3 << 16);
			dst[3] = (t3 >> 16) |  (t4 << 16);

			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;

		case 0:
		do {
			u_int32_t *src = (u_int32_t*)from;
			u_int32_t *dst = (u_int32_t*)bptr;
			u_int32_t t0, t1, t2, t3;

			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];
			dst[0] = t0; dst[1] = t1; dst[2] = t2; dst[3] = t3;

			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;

		default: 
		/* Does odd-aligned case ever happen? */
		do {
			bcopy(from, bptr, 16);
			from += 16;
			bptr += 32;
			len -= 16;
		} while (len >= 16);
		break;
	}
	if (len)
		bcopy(from, bptr, len);
d256 3
a258 2
	caddr_t to = tov;
	caddr_t bptr;
d262 3
a264 6

	/* Dispose of boff. source of copy is subsequently 16-byte aligned. */
	if (boff) {
		int xfer;
		xfer = min(len, 16 - boff);
		bcopy(bptr+boff, to, xfer);
d267 1
d269 1
a270 52
	if (len >= 16)
	switch ((u_long)to & (sizeof(u_int32_t) -1)) {
	case 2:
		/*
		 * to is aligned to an odd 16-bit boundary.  Ethernet headers
		 * make this the dominant case (98% or more).
		 */
		do {
			u_int32_t *src = (u_int32_t*)bptr;
			u_int32_t t0, t1, t2, t3;

			/* read from uncached aligned DMA buf */
			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];

			/* write to odd-16-bit-word aligned dst */
			*(u_int16_t *) (to+0)  = (u_short)  t0;
			*(u_int32_t *) (to+2)  = (t0 >> 16) |  (t1 << 16);
			*(u_int32_t *) (to+6)  = (t1 >> 16) |  (t2 << 16);
			*(u_int32_t *) (to+10) = (t2 >> 16) |  (t3 << 16);
			*(u_int16_t *) (to+14) = (t3 >> 16);
			bptr += 32;
			to += 16;
			len -= 16;
		} while (len > 16);
		break;
	case 0:
		/* 32-bit aligned aligned copy. Rare. */
		do {
			u_int32_t *src = (u_int32_t*)bptr;
			u_int32_t *dst = (u_int32_t*)to;
			u_int32_t t0, t1, t2, t3;

			t0 = src[0]; t1 = src[1]; t2 = src[2]; t3 = src[3];
			dst[0] = t0; dst[1] = t1; dst[2] = t2; dst[3] = t3;
			to += 16;
			bptr += 32;
			len -= 16;
		} while (len  > 16);
		break;

	/* XXX Does odd-byte-aligned case ever happen? */
	default:
		do {
			bcopy(bptr, to, 16);
			to += 16;
			bptr += 32;
			len -= 16;
		} while (len  > 16);
		break;
	}
	if (len)
		bcopy(bptr, to, len);
d279 2
a280 2
	caddr_t bptr;
	int xfer;
@


1.3
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: if_le_ioasic.c,v 1.2 1996/05/07 02:24:56 thorpej Exp $	*/
d43 1
@


1.2
log
@if_name/if_unit -> if_xname/if_softc
@
text
@d1 1
@


1.1
log
@make these work together
@
text
@d1 1
a1 1
/*	$NetBSD: if_le_ioasic.c,v 1.1 1996/04/18 00:50:13 cgd Exp $	*/
a47 1
#include <dev/tc/if_levar.h>
a48 2
#define LE_NEED_BUF_GAP2
#define LE_NEED_BUF_GAP16
d51 1
d60 11
d96 2
a97 1
	register struct le_softc *sc = (void *)self;
d99 1
a99 1
	sc->sc_r1 = (struct lereg1 *)
d103 5
a107 5
	sc->sc_copytodesc = am7990_copytobuf_gap2;
	sc->sc_copyfromdesc = am7990_copyfrombuf_gap2;
	sc->sc_copytobuf = am7990_copytobuf_gap16;
	sc->sc_copyfrombuf = am7990_copyfrombuf_gap16;
	sc->sc_zerobuf = am7990_zerobuf_gap16;
d113 149
a261 1
	ioasic_intr_establish(parent, d->iada_cookie, TC_IPL_NET, leintr, sc);
@
