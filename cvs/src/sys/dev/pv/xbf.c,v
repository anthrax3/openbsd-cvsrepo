head	1.30;
access;
symbols
	OPENBSD_6_1:1.28.0.4
	OPENBSD_6_1_BASE:1.28;
locks; strict;
comment	@ * @;


1.30
date	2017.06.06.21.12.01;	author mikeb;	state Exp;
branches;
next	1.29;
commitid	AKPK9ZrGvu2Qa73C;

1.29
date	2017.06.06.20.33.28;	author mikeb;	state Exp;
branches;
next	1.28;
commitid	DQAutBqRUyz0GSfe;

1.28
date	2017.03.19.16.37.19;	author mikeb;	state Exp;
branches;
next	1.27;
commitid	jZZT6sWk2iGgy7TB;

1.27
date	2017.03.19.16.26.28;	author mikeb;	state Exp;
branches;
next	1.26;
commitid	VBxWiosWHMeX19dJ;

1.26
date	2017.03.13.01.10.03;	author mikeb;	state Exp;
branches;
next	1.25;
commitid	7qXoPSATw8fQxunw;

1.25
date	2017.03.13.01.00.15;	author mikeb;	state Exp;
branches;
next	1.24;
commitid	knAVpjVB7fgmCUNq;

1.24
date	2017.02.24.16.58.12;	author mikeb;	state Exp;
branches;
next	1.23;
commitid	dd60qwGHamkvdf8J;

1.23
date	2017.02.08.17.39.57;	author mikeb;	state Exp;
branches;
next	1.22;
commitid	27kniJvFSLa81Soc;

1.22
date	2017.02.08.17.32.45;	author mikeb;	state Exp;
branches;
next	1.21;
commitid	H7iwx9tX5SRaa6tM;

1.21
date	2017.02.08.17.23.46;	author mikeb;	state Exp;
branches;
next	1.20;
commitid	eg2fZj9T3887NMYR;

1.20
date	2017.02.08.16.51.26;	author mikeb;	state Exp;
branches;
next	1.19;
commitid	B1NDO0ofwOhW3QI2;

1.19
date	2017.02.08.16.29.00;	author mikeb;	state Exp;
branches;
next	1.18;
commitid	DYnDwos8Yjo11fRf;

1.18
date	2017.02.06.21.47.06;	author mikeb;	state Exp;
branches;
next	1.17;
commitid	pnmLP2d2eFJP6MM0;

1.17
date	2017.02.06.21.43.48;	author mikeb;	state Exp;
branches;
next	1.16;
commitid	NscjkKs6gyrHx1os;

1.16
date	2017.01.19.12.36.50;	author mikeb;	state Exp;
branches;
next	1.15;
commitid	ko24dzYHQKBLPWq1;

1.15
date	2017.01.18.22.18.47;	author mikeb;	state Exp;
branches;
next	1.14;
commitid	5Ct87jpbtRtxqTKt;

1.14
date	2016.12.23.12.52.12;	author mikeb;	state Exp;
branches;
next	1.13;
commitid	DRMwbQyjeQU9MCQn;

1.13
date	2016.12.15.03.41.15;	author jsg;	state Exp;
branches;
next	1.12;
commitid	s9iz8oPfa6JOoa5R;

1.12
date	2016.12.14.11.39.30;	author mikeb;	state Exp;
branches;
next	1.11;
commitid	9NeIe0e1jnDJsVOi;

1.11
date	2016.12.13.19.02.39;	author mikeb;	state Exp;
branches;
next	1.10;
commitid	KQeiperkjfYRpDRa;

1.10
date	2016.12.13.18.27.24;	author mikeb;	state Exp;
branches;
next	1.9;
commitid	JdAVJSGYsMvtftDX;

1.9
date	2016.12.10.19.41.31;	author mikeb;	state Exp;
branches;
next	1.8;
commitid	rW2jcoYKOtoFC9QW;

1.8
date	2016.12.10.19.38.50;	author mikeb;	state Exp;
branches;
next	1.7;
commitid	z1Z3yYcUDpT52d9m;

1.7
date	2016.12.09.17.35.13;	author mikeb;	state Exp;
branches;
next	1.6;
commitid	4aPqvfqWdX3ArZH9;

1.6
date	2016.12.08.19.30.44;	author mikeb;	state Exp;
branches;
next	1.5;
commitid	2dwQFPGEVLgXVTWB;

1.5
date	2016.12.07.21.08.55;	author mikeb;	state Exp;
branches;
next	1.4;
commitid	Kas90HdVqX0DPM7K;

1.4
date	2016.12.07.21.06.55;	author mikeb;	state Exp;
branches;
next	1.3;
commitid	C9B156Fi3FKXTUP8;

1.3
date	2016.12.07.19.17.52;	author mikeb;	state Exp;
branches;
next	1.2;
commitid	etcK4zzP5w2s1Pxz;

1.2
date	2016.12.07.18.33.45;	author mikeb;	state Exp;
branches;
next	1.1;
commitid	xUzkTD6M9IIbxAnX;

1.1
date	2016.12.07.15.26.43;	author mikeb;	state Exp;
branches;
next	;
commitid	nToduJIygUHdEuyQ;


desc
@@


1.30
log
@Rewrite the driver to handle 64kb transfers

Although several codepaths in the kernel such as coredump
and buffercache read-ahead feature assume that underlying
hardware is capable of handling 64kb transfers without any
issues, xbf was setup to rely on a single descriptor per
transfer which limited the maximum size of an individual
transfer to 11 4k segments amounting to 44k bytes.

To avoid overbooking, a metadata object is allocated for
each transfer to keep track of associated descriptors
limiting the maximum amount of outstanding transfers to
half the ring size.

The issue was reported by Dan Cross <crossd at gmail.com>,
thanks!
@
text
@/*	$OpenBSD: xbf.c,v 1.29 2017/06/06 20:33:28 mikeb Exp $	*/

/*
 * Copyright (c) 2016 Mike Belopuhov
 * Copyright (c) 2009, 2011 Mark Kettenis
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include "bio.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/atomic.h>
#include <sys/device.h>
#include <sys/kernel.h>
#include <sys/buf.h>
#include <sys/malloc.h>
#include <sys/task.h>

#include <machine/bus.h>

#include <dev/pv/xenreg.h>
#include <dev/pv/xenvar.h>

#include <scsi/scsi_all.h>
#include <scsi/cd.h>
#include <scsi/scsi_disk.h>
#include <scsi/scsiconf.h>

/* #define XBF_DEBUG */

#ifdef XBF_DEBUG
#define DPRINTF(x...)		printf(x)
#else
#define DPRINTF(x...)
#endif

#define XBF_OP_READ		0
#define XBF_OP_WRITE		1
#define XBF_OP_BARRIER		2 /* feature-barrier */
#define XBF_OP_FLUSH		3 /* feature-flush-cache */
#define XBF_OP_DISCARD		5 /* feature-discard */
#define XBF_OP_INDIRECT		6 /* feature-max-indirect-segments */

#define XBF_MAX_SGE		11
#define XBF_MAX_ISGE		8

#define XBF_SEC_SHIFT		9

#define XBF_CDROM		1
#define XBF_REMOVABLE		2
#define XBF_READONLY		4

#define XBF_OK			0
#define XBF_EIO			-1 /* generic failure */
#define XBF_EOPNOTSUPP		-2 /* only for XBF_OP_BARRIER */

struct xbf_sge {
	uint32_t		 sge_ref;
	uint8_t			 sge_first;
	uint8_t			 sge_last;
	uint16_t		 sge_pad;
} __packed;

/* Generic I/O request */
struct xbf_req {
	uint8_t			 req_op;
	uint8_t			 req_nsegs;
	uint16_t		 req_unit;
#ifdef __amd64__
	uint32_t		 req_pad;
#endif
	uint64_t		 req_id;
	uint64_t		 req_sector;
	struct xbf_sge		 req_sgl[XBF_MAX_SGE];
} __packed;

/* Indirect I/O request */
struct xbf_ireq {
	uint8_t			 req_op;
	uint8_t			 req_iop;
	uint16_t		 req_nsegs;
#ifdef __amd64__
	uint32_t		 req_pad;
#endif
	uint64_t		 req_id;
	uint64_t		 req_sector;
	uint16_t		 req_unit;
	uint32_t		 req_gref[XBF_MAX_ISGE];
#ifdef __i386__
	uint64_t		 req_pad;
#endif
} __packed;

struct xbf_rsp {
	uint64_t		 rsp_id;
	uint8_t			 rsp_op;
	uint8_t			 rsp_pad1;
	int16_t			 rsp_status;
#ifdef __amd64__
	uint32_t		 rsp_pad2;
#endif
} __packed;

union xbf_ring_desc {
	struct xbf_req	 	 xrd_req;
	struct xbf_ireq		 xrd_ireq;
	struct xbf_rsp	 	 xrd_rsp;
} __packed;

#define XBF_MIN_RING_SIZE	1
#define XBF_MAX_RING_SIZE	8
#define XBF_MAX_REQS		256 /* must be a power of 2 */

struct xbf_ring {
	volatile uint32_t	 xr_prod;
	volatile uint32_t	 xr_prod_event;
	volatile uint32_t	 xr_cons;
	volatile uint32_t	 xr_cons_event;
	uint32_t		 xr_reserved[12];
	union xbf_ring_desc	 xr_desc[0];
} __packed;

struct xbf_dma_mem {
	bus_size_t		 dma_size;
	bus_dma_tag_t		 dma_tag;
	bus_dmamap_t		 dma_map;
	bus_dma_segment_t	*dma_seg;
	int			 dma_nsegs; /* total amount */
	int			 dma_rsegs; /* used amount */
	caddr_t			 dma_vaddr;
};

struct xbf_ccb {
	struct scsi_xfer	*ccb_xfer;  /* associated transfer */
	bus_dmamap_t		 ccb_dmap;  /* transfer map */
	struct xbf_dma_mem	 ccb_bbuf;  /* bounce buffer */
	uint32_t		 ccb_first; /* first descriptor */
	uint32_t		 ccb_last;  /* last descriptor */
	uint16_t		 ccb_want;  /* expected chunks */
	uint16_t		 ccb_seen;  /* completed chunks */
	TAILQ_ENTRY(xbf_ccb)	 ccb_link;
};
TAILQ_HEAD(xbf_ccb_queue, xbf_ccb);

struct xbf_softc {
	struct device		 sc_dev;
	struct device		*sc_parent;
	char			 sc_node[XEN_MAX_NODE_LEN];
	char			 sc_backend[XEN_MAX_BACKEND_LEN];
	bus_dma_tag_t		 sc_dmat;
	int			 sc_domid;

	xen_intr_handle_t	 sc_xih;

	int			 sc_state;
#define  XBF_CONNECTED		  4
#define  XBF_CLOSING		  5

	int			 sc_caps;
#define  XBF_CAP_BARRIER	  0x0001
#define  XBF_CAP_FLUSH		  0x0002

	uint32_t		 sc_type;
	uint32_t		 sc_unit;
	char			 sc_dtype[16];
	char			 sc_prod[16];

	uint64_t		 sc_disk_size;
	uint32_t		 sc_block_size;

	/* Ring */
	struct xbf_ring		*sc_xr;
	uint32_t		 sc_xr_cons;
	uint32_t		 sc_xr_prod;
	uint32_t		 sc_xr_size; /* in pages */
	struct xbf_dma_mem	 sc_xr_dma;
	uint32_t		 sc_xr_ref[XBF_MAX_RING_SIZE];
	int			 sc_xr_ndesc;

	/* Maximum number of blocks that one descriptor may refer to */
	int			 sc_xrd_nblk;

	/* CCBs */
	int			 sc_nccb;
	struct xbf_ccb		*sc_ccbs;
	struct xbf_ccb_queue	 sc_ccb_fq; /* free queue */
	struct xbf_ccb_queue	 sc_ccb_sq; /* pending requests */
	struct mutex		 sc_ccb_fqlck;
	struct mutex		 sc_ccb_sqlck;

	struct scsi_iopool	 sc_iopool;
	struct scsi_adapter	 sc_switch;
	struct scsi_link         sc_link;
	struct device		*sc_scsibus;
};

int	xbf_match(struct device *, void *, void *);
void	xbf_attach(struct device *, struct device *, void *);
int	xbf_detach(struct device *, int);

struct cfdriver xbf_cd = {
	NULL, "xbf", DV_DULL
};

const struct cfattach xbf_ca = {
	sizeof(struct xbf_softc), xbf_match, xbf_attach, xbf_detach
};

void	xbf_intr(void *);

int	xbf_load_cmd(struct scsi_xfer *);
int	xbf_bounce_cmd(struct scsi_xfer *);
void	xbf_reclaim_cmd(struct scsi_xfer *);

void	xbf_scsi_cmd(struct scsi_xfer *);
int	xbf_submit_cmd(struct scsi_xfer *);
int	xbf_poll_cmd(struct scsi_xfer *);
void	xbf_complete_cmd(struct xbf_softc *, struct xbf_ccb_queue *, int);
int	xbf_dev_probe(struct scsi_link *);

void	xbf_scsi_inq(struct scsi_xfer *);
void	xbf_scsi_inquiry(struct scsi_xfer *);
void	xbf_scsi_capacity(struct scsi_xfer *);
void	xbf_scsi_capacity16(struct scsi_xfer *);
void	xbf_scsi_done(struct scsi_xfer *, int);

int	xbf_dma_alloc(struct xbf_softc *, struct xbf_dma_mem *,
	    bus_size_t, int, int);
void	xbf_dma_free(struct xbf_softc *, struct xbf_dma_mem *);

int	xbf_get_type(struct xbf_softc *);
int	xbf_init(struct xbf_softc *);
int	xbf_ring_create(struct xbf_softc *);
void	xbf_ring_destroy(struct xbf_softc *);
void	xbf_stop(struct xbf_softc *);

int	xbf_alloc_ccbs(struct xbf_softc *);
void	xbf_free_ccbs(struct xbf_softc *);
void	*xbf_get_ccb(void *);
void	xbf_put_ccb(void *, void *);

int
xbf_match(struct device *parent, void *match, void *aux)
{
	struct xen_attach_args *xa = aux;

	if (strcmp("vbd", xa->xa_name))
		return (0);

	return (1);
}

void
xbf_attach(struct device *parent, struct device *self, void *aux)
{
	struct xen_attach_args *xa = aux;
	struct xbf_softc *sc = (struct xbf_softc *)self;
	struct scsibus_attach_args saa;

	sc->sc_parent = parent;
	sc->sc_dmat = xa->xa_dmat;
	sc->sc_domid = xa->xa_domid;

	memcpy(sc->sc_node, xa->xa_node, XEN_MAX_NODE_LEN);
	memcpy(sc->sc_backend, xa->xa_backend, XEN_MAX_BACKEND_LEN);

	if (xbf_get_type(sc))
		return;

	if (xen_intr_establish(0, &sc->sc_xih, sc->sc_domid, xbf_intr, sc,
	    sc->sc_dev.dv_xname)) {
		printf(": failed to establish an interrupt\n");
		return;
	}
	xen_intr_mask(sc->sc_xih);

	printf(" backend %d channel %u: %s\n", sc->sc_domid, sc->sc_xih,
	    sc->sc_dtype);

	if (xbf_init(sc))
		goto error;

	if (xen_intr_unmask(sc->sc_xih)) {
		printf("%s: failed to enable interrupts\n",
		    sc->sc_dev.dv_xname);
		goto error;
	}

	sc->sc_switch.scsi_cmd = xbf_scsi_cmd;
	sc->sc_switch.scsi_minphys = scsi_minphys;
	sc->sc_switch.dev_probe = xbf_dev_probe;

	sc->sc_link.adapter = &sc->sc_switch;
	sc->sc_link.adapter_softc = self;
	sc->sc_link.adapter_buswidth = 2;
	sc->sc_link.luns = 1;
	sc->sc_link.adapter_target = 2;
	sc->sc_link.openings = sc->sc_nccb;
	sc->sc_link.pool = &sc->sc_iopool;

	bzero(&saa, sizeof(saa));
	saa.saa_sc_link = &sc->sc_link;
	sc->sc_scsibus = config_found(self, &saa, scsiprint);

	xen_unplug_emulated(parent, XEN_UNPLUG_IDE | XEN_UNPLUG_IDESEC);

	return;

 error:
	xen_intr_disestablish(sc->sc_xih);
}

int
xbf_detach(struct device *self, int flags)
{
	struct xbf_softc *sc = (struct xbf_softc *)self;
	int ostate = sc->sc_state;

	sc->sc_state = XBF_CLOSING;

	xen_intr_mask(sc->sc_xih);
	xen_intr_barrier(sc->sc_xih);

	if (ostate == XBF_CONNECTED) {
		xen_intr_disestablish(sc->sc_xih);
		xbf_stop(sc);
	}

	if (sc->sc_scsibus)
		return (config_detach(sc->sc_scsibus, flags | DETACH_FORCE));

	return (0);
}

void
xbf_intr(void *xsc)
{
	struct xbf_softc *sc = xsc;
	struct xbf_ring *xr = sc->sc_xr;
	struct xbf_dma_mem *dma = &sc->sc_xr_dma;
	struct xbf_ccb_queue cq;
	struct xbf_ccb *ccb, *nccb;
	uint32_t cons;
	int desc, s;

	TAILQ_INIT(&cq);

	for (;;) {
		bus_dmamap_sync(dma->dma_tag, dma->dma_map, 0, dma->dma_size,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);

		for (cons = sc->sc_xr_cons; cons != xr->xr_cons; cons++) {
			desc = cons & (sc->sc_xr_ndesc - 1);
			xbf_complete_cmd(sc, &cq, desc);
		}

		sc->sc_xr_cons = cons;

		if (TAILQ_EMPTY(&cq))
			break;

		s = splbio();
		KERNEL_LOCK();
		TAILQ_FOREACH_SAFE(ccb, &cq, ccb_link, nccb) {
			TAILQ_REMOVE(&cq, ccb, ccb_link);
			xbf_reclaim_cmd(ccb->ccb_xfer);
			scsi_done(ccb->ccb_xfer);
		}
		KERNEL_UNLOCK();
		splx(s);
	}
}

void
xbf_scsi_cmd(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;

	switch (xs->cmd->opcode) {
	case READ_BIG:
	case READ_COMMAND:
	case READ_12:
	case READ_16:
	case WRITE_BIG:
	case WRITE_COMMAND:
	case WRITE_12:
	case WRITE_16:
		if (sc->sc_state != XBF_CONNECTED) {
			xbf_scsi_done(xs, XS_SELTIMEOUT);
			return;
		}
		break;
	case SYNCHRONIZE_CACHE:
		if (!(sc->sc_caps & (XBF_CAP_BARRIER|XBF_CAP_FLUSH))) {
			xbf_scsi_done(xs, XS_NOERROR);
			return;
		}
		break;
	case INQUIRY:
		xbf_scsi_inq(xs);
		return;
	case READ_CAPACITY:
		xbf_scsi_capacity(xs);
		return;
	case READ_CAPACITY_16:
		xbf_scsi_capacity16(xs);
		return;
	case TEST_UNIT_READY:
	case START_STOP:
	case PREVENT_ALLOW:
		xbf_scsi_done(xs, XS_NOERROR);
		return;
	default:
		printf("%s cmd 0x%02x\n", __func__, xs->cmd->opcode);
	case MODE_SENSE:
	case MODE_SENSE_BIG:
	case REPORT_LUNS:
	case READ_TOC:
		xbf_scsi_done(xs, XS_DRIVER_STUFFUP);
		return;
	}

	if (xbf_submit_cmd(xs)) {
		xbf_scsi_done(xs, XS_DRIVER_STUFFUP);
		return;
	}

	if (ISSET(xs->flags, SCSI_POLL) && xbf_poll_cmd(xs)) {
		printf("%s: op %#x timed out\n", sc->sc_dev.dv_xname,
		    xs->cmd->opcode);
		if (sc->sc_state == XBF_CONNECTED) {
			xbf_reclaim_cmd(xs);
			xbf_scsi_done(xs, XS_TIMEOUT);
		}
		return;
	}
}

int
xbf_load_cmd(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct xbf_ccb *ccb = xs->io;
	struct xbf_sge *sge;
	union xbf_ring_desc *xrd;
	bus_dmamap_t map;
	int error, mapflags, nsg, seg;
	int desc, ndesc = 0;

	map = ccb->ccb_dmap;

	mapflags = (sc->sc_domid << 16);
	if (ISSET(xs->flags, SCSI_NOSLEEP))
		mapflags |= BUS_DMA_NOWAIT;
	else
		mapflags |= BUS_DMA_WAITOK;
	if (ISSET(xs->flags, SCSI_DATA_IN))
		mapflags |= BUS_DMA_READ;
	else
		mapflags |= BUS_DMA_WRITE;

	error = bus_dmamap_load(sc->sc_dmat, map, xs->data, xs->datalen,
	    NULL, mapflags);
	if (error) {
		printf("%s: failed to load %d bytes of data\n",
		    sc->sc_dev.dv_xname, xs->datalen);
		return (error);
	}

	xrd = &sc->sc_xr->xr_desc[ccb->ccb_first];
	/* seg is the segment map iterator, nsg is the s-g list iterator */
	for (seg = 0, nsg = 0; seg < map->dm_nsegs; seg++, nsg++) {
		if (nsg == XBF_MAX_SGE) {
			/* Number of segments so far */
			xrd->xrd_req.req_nsegs = nsg;
			/* Pick next descriptor */
			ndesc++;
			desc = (sc->sc_xr_prod + ndesc) & (sc->sc_xr_ndesc - 1);
			xrd = &sc->sc_xr->xr_desc[desc];
			nsg = 0;
		}
		sge = &xrd->xrd_req.req_sgl[nsg];
		sge->sge_ref = map->dm_segs[seg].ds_addr;
		sge->sge_first = nsg > 0 ? 0 :
		    (((vaddr_t)xs->data + ndesc * sc->sc_xrd_nblk *
			(1 << XBF_SEC_SHIFT)) & PAGE_MASK) >> XBF_SEC_SHIFT;
		sge->sge_last = sge->sge_first +
		    (map->dm_segs[seg].ds_len >> XBF_SEC_SHIFT) - 1;

		DPRINTF("%s:   seg %d/%d ref %lu len %lu first %u last %u\n",
		    sc->sc_dev.dv_xname, nsg + 1, map->dm_nsegs,
		    map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len,
		    sge->sge_first, sge->sge_last);

		KASSERT(sge->sge_last <= 7);
	}

	KASSERT(nsg > 0);
	xrd->xrd_req.req_nsegs = nsg;

	return (0);
}

int
xbf_bounce_cmd(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct xbf_ccb *ccb = xs->io;
	struct xbf_sge *sge;
	struct xbf_dma_mem *dma;
	union xbf_ring_desc *xrd;
	bus_dmamap_t map;
	bus_size_t size;
	int error, mapflags, nsg, seg;
	int desc, ndesc = 0;

	size = roundup(xs->datalen, PAGE_SIZE);
	if (size > MAXPHYS)
		return (EFBIG);

	mapflags = (sc->sc_domid << 16);
	if (ISSET(xs->flags, SCSI_NOSLEEP))
		mapflags |= BUS_DMA_NOWAIT;
	else
		mapflags |= BUS_DMA_WAITOK;
	if (ISSET(xs->flags, SCSI_DATA_IN))
		mapflags |= BUS_DMA_READ;
	else
		mapflags |= BUS_DMA_WRITE;

	dma = &ccb->ccb_bbuf;
	error = xbf_dma_alloc(sc, dma, size, size / PAGE_SIZE, mapflags);
	if (error) {
		DPRINTF("%s: failed to allocate a %lu byte bounce buffer\n",
		    sc->sc_dev.dv_xname, size);
		return (error);
	}

	map = dma->dma_map;

	DPRINTF("%s: bouncing %d bytes via %lu size map with %d segments\n",
	    sc->sc_dev.dv_xname, xs->datalen, size, map->dm_nsegs);

	if (ISSET(xs->flags, SCSI_DATA_OUT))
		memcpy(dma->dma_vaddr, xs->data, xs->datalen);

	xrd = &sc->sc_xr->xr_desc[ccb->ccb_first];
	/* seg is the map segment iterator, nsg is the s-g element iterator */
	for (seg = 0, nsg = 0; seg < map->dm_nsegs; seg++, nsg++) {
		if (nsg == XBF_MAX_SGE) {
			/* Number of segments so far */
			xrd->xrd_req.req_nsegs = nsg;
			/* Pick next descriptor */
			ndesc++;
			desc = (sc->sc_xr_prod + ndesc) & (sc->sc_xr_ndesc - 1);
			xrd = &sc->sc_xr->xr_desc[desc];
			nsg = 0;
		}
		sge = &xrd->xrd_req.req_sgl[nsg];
		sge->sge_ref = map->dm_segs[seg].ds_addr;
		sge->sge_first = nsg > 0 ? 0 :
		    (((vaddr_t)dma->dma_vaddr + ndesc * sc->sc_xrd_nblk *
			(1 << XBF_SEC_SHIFT)) & PAGE_MASK) >> XBF_SEC_SHIFT;
		sge->sge_last = sge->sge_first +
		    (map->dm_segs[seg].ds_len >> XBF_SEC_SHIFT) - 1;

		DPRINTF("%s:   seg %d/%d ref %lu len %lu first %u last %u\n",
		    sc->sc_dev.dv_xname, nsg + 1, map->dm_nsegs,
		    map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len,
		    sge->sge_first, sge->sge_last);

		KASSERT(sge->sge_last <= 7);
	}

	xrd->xrd_req.req_nsegs = nsg;

	return (0);
}

void
xbf_reclaim_cmd(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct xbf_ccb *ccb = xs->io;
	struct xbf_dma_mem *dma = &ccb->ccb_bbuf;

	if (dma->dma_size == 0)
		return;

	if (ISSET(xs->flags, SCSI_DATA_IN))
		memcpy(xs->data, (caddr_t)dma->dma_vaddr, xs->datalen);

	xbf_dma_free(sc, &ccb->ccb_bbuf);
}

int
xbf_submit_cmd(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct xbf_ccb *ccb = xs->io;
	union xbf_ring_desc *xrd;
	struct scsi_rw *rw;
	struct scsi_rw_big *rwb;
	struct scsi_rw_12 *rw12;
	struct scsi_rw_16 *rw16;
	uint64_t lba = 0;
	uint32_t nblk = 0;
	uint8_t operation = 0;
	unsigned int ndesc = 0;
	int desc, error;

	switch (xs->cmd->opcode) {
	case READ_BIG:
	case READ_COMMAND:
	case READ_12:
	case READ_16:
		operation = XBF_OP_READ;
		break;

	case WRITE_BIG:
	case WRITE_COMMAND:
	case WRITE_12:
	case WRITE_16:
		operation = XBF_OP_WRITE;
		break;

	case SYNCHRONIZE_CACHE:
		if (sc->sc_caps & XBF_CAP_FLUSH)
			operation = XBF_OP_FLUSH;
		else if (sc->sc_caps & XBF_CAP_BARRIER)
			operation = XBF_OP_BARRIER;
		break;
	}

	/*
	 * READ/WRITE/SYNCHRONIZE commands. SYNCHRONIZE CACHE
	 * has the same layout as 10-byte READ/WRITE commands.
	 */
	if (xs->cmdlen == 6) {
		rw = (struct scsi_rw *)xs->cmd;
		lba = _3btol(rw->addr) & (SRW_TOPADDR << 16 | 0xffff);
		nblk = rw->length ? rw->length : 0x100;
	} else if (xs->cmdlen == 10) {
		rwb = (struct scsi_rw_big *)xs->cmd;
		lba = _4btol(rwb->addr);
		nblk = _2btol(rwb->length);
	} else if (xs->cmdlen == 12) {
		rw12 = (struct scsi_rw_12 *)xs->cmd;
		lba = _4btol(rw12->addr);
		nblk = _4btol(rw12->length);
	} else if (xs->cmdlen == 16) {
		rw16 = (struct scsi_rw_16 *)xs->cmd;
		lba = _8btol(rw16->addr);
		nblk = _4btol(rw16->length);
	}

	ccb->ccb_want = ccb->ccb_seen = 0;

	do {
		desc = (sc->sc_xr_prod + ndesc) & (sc->sc_xr_ndesc - 1);
		if (ndesc == 0)
			ccb->ccb_first = desc;

		xrd = &sc->sc_xr->xr_desc[desc];
		xrd->xrd_req.req_op = operation;
		xrd->xrd_req.req_unit = (uint16_t)sc->sc_unit;
		xrd->xrd_req.req_sector = lba + ndesc * sc->sc_xrd_nblk;

		ccb->ccb_want |= 1 << ndesc;
		ndesc++;
	} while (ndesc * sc->sc_xrd_nblk < nblk);

	ccb->ccb_last = desc;

	if (operation == XBF_OP_READ || operation == XBF_OP_WRITE) {
		DPRINTF("%s: desc %u,%u %s%s lba %llu nsec %u "
		    "len %d\n", sc->sc_dev.dv_xname, ccb->ccb_first,
		    ccb->ccb_last, operation == XBF_OP_READ ? "read" :
		    "write", ISSET(xs->flags, SCSI_POLL) ? "-poll" : "",
		    lba, nblk, xs->datalen);

		if (((vaddr_t)xs->data & ((1 << XBF_SEC_SHIFT) - 1)) == 0)
			error = xbf_load_cmd(xs);
		else
			error = xbf_bounce_cmd(xs);
		if (error)
			return (-1);
	} else {
		KASSERT(nblk == 0);
		KASSERT(ndesc == 1);
		DPRINTF("%s: desc %u %s%s lba %llu\n", sc->sc_dev.dv_xname,
		    ccb->ccb_first, operation == XBF_OP_FLUSH ? "flush" :
		    "barrier", ISSET(xs->flags, SCSI_POLL) ? "-poll" : "",
		    lba);
		xrd->xrd_req.req_nsegs = 0;
	}

	ccb->ccb_xfer = xs;

	bus_dmamap_sync(sc->sc_dmat, ccb->ccb_dmap, 0,
	    ccb->ccb_dmap->dm_mapsize, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);

	mtx_enter(&sc->sc_ccb_sqlck);
	TAILQ_INSERT_TAIL(&sc->sc_ccb_sq, ccb, ccb_link);
	mtx_leave(&sc->sc_ccb_sqlck);

	sc->sc_xr_prod += ndesc;
	sc->sc_xr->xr_prod = sc->sc_xr_prod;
	sc->sc_xr->xr_cons_event = sc->sc_xr_prod;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_xr_dma.dma_map, 0,
	    sc->sc_xr_dma.dma_map->dm_mapsize, BUS_DMASYNC_PREREAD |
	    BUS_DMASYNC_PREWRITE);

	xen_intr_signal(sc->sc_xih);

	return (0);
}

int
xbf_poll_cmd(struct scsi_xfer *xs)
{
	int timo = 1000;

	do {
		if (ISSET(xs->flags, ITSDONE))
			break;
		if (ISSET(xs->flags, SCSI_NOSLEEP))
			delay(10);
		else
			tsleep(xs, PRIBIO, "xbfpoll", 1);
		xbf_intr(xs->sc_link->adapter_softc);
	} while(--timo > 0);

	return (0);
}

void
xbf_complete_cmd(struct xbf_softc *sc, struct xbf_ccb_queue *cq, int desc)
{
	struct xbf_ccb *ccb;
	union xbf_ring_desc *xrd;
	bus_dmamap_t map;
	uint32_t id, chunk;
	int error;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_xr_dma.dma_map, 0,
	    sc->sc_xr_dma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD |
	    BUS_DMASYNC_POSTWRITE);

	xrd = &sc->sc_xr->xr_desc[desc];
	error = xrd->xrd_rsp.rsp_status == XBF_OK ? XS_NOERROR :
	    XS_DRIVER_STUFFUP;

	id = (uint32_t)xrd->xrd_rsp.rsp_id;
	mtx_enter(&sc->sc_ccb_sqlck);
	TAILQ_FOREACH(ccb, &sc->sc_ccb_sq, ccb_link) {
		if (((id - ccb->ccb_first) & (sc->sc_xr_ndesc - 1)) <=
		    ((ccb->ccb_last - ccb->ccb_first) & (sc->sc_xr_ndesc - 1)))
			break;
	}
	mtx_leave(&sc->sc_ccb_sqlck);
	KASSERT(ccb != NULL);
	chunk = 1 << ((id - ccb->ccb_first) & (sc->sc_xr_ndesc - 1));
#ifdef XBF_DEBUG
	if ((ccb->ccb_want & chunk) == 0) {
		panic("%s: id %#x first %#x want %#x seen %#x chunk %#x",
		    sc->sc_dev.dv_xname, id, ccb->ccb_first, ccb->ccb_want,
		    ccb->ccb_seen, chunk);
	}
	if ((ccb->ccb_seen & chunk) != 0) {
		panic("%s: id %#x first %#x want %#x seen %#x chunk %#x",
		    sc->sc_dev.dv_xname, id, ccb->ccb_first, ccb->ccb_want,
		    ccb->ccb_seen, chunk);
	}
#endif
	ccb->ccb_seen |= chunk;

	if (ccb->ccb_bbuf.dma_size > 0)
		map = ccb->ccb_bbuf.dma_map;
	else
		map = ccb->ccb_dmap;

	bus_dmamap_sync(sc->sc_dmat, map, 0, map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(sc->sc_dmat, map);

	DPRINTF("%s: completing desc %d(%llu) op %u with error %d\n",
	    sc->sc_dev.dv_xname, desc, xrd->xrd_rsp.rsp_id,
	    xrd->xrd_rsp.rsp_op, xrd->xrd_rsp.rsp_status);

	memset(xrd, 0, sizeof(*xrd));
	xrd->xrd_req.req_id = desc;

	if (ccb->ccb_seen == ccb->ccb_want) {
		mtx_enter(&sc->sc_ccb_sqlck);
		TAILQ_REMOVE(&sc->sc_ccb_sq, ccb, ccb_link);
		mtx_leave(&sc->sc_ccb_sqlck);

		ccb->ccb_xfer->resid = 0;
		ccb->ccb_xfer->error = error;
		TAILQ_INSERT_TAIL(cq, ccb, ccb_link);
	}
}

void
xbf_scsi_inq(struct scsi_xfer *xs)
{
	struct scsi_inquiry *inq = (struct scsi_inquiry *)xs->cmd;

	if (ISSET(inq->flags, SI_EVPD))
		xbf_scsi_done(xs, XS_DRIVER_STUFFUP);
	else
		xbf_scsi_inquiry(xs);
}

void
xbf_scsi_inquiry(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct scsi_inquiry_data inq;
	/* char buf[5]; */

	bzero(&inq, sizeof(inq));

	switch (sc->sc_type) {
	case XBF_CDROM:
		inq.device = T_CDROM;
		break;
	default:
		inq.device = T_DIRECT;
		break;
	}

	inq.version = 0x05; /* SPC-3 */
	inq.response_format = 2;
	inq.additional_length = 32;
	inq.flags |= SID_CmdQue;
	bcopy("Xen     ", inq.vendor, sizeof(inq.vendor));
	bcopy(sc->sc_prod, inq.product, sizeof(inq.product));
	bcopy("0000", inq.revision, sizeof(inq.revision));

	bcopy(&inq, xs->data, MIN(sizeof(inq), xs->datalen));

	xbf_scsi_done(xs, XS_NOERROR);
}

void
xbf_scsi_capacity(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct scsi_read_cap_data rcd;
	uint64_t capacity;

	bzero(&rcd, sizeof(rcd));

	capacity = sc->sc_disk_size - 1;
	if (capacity > 0xffffffff)
		capacity = 0xffffffff;

	_lto4b(capacity, rcd.addr);
	_lto4b(sc->sc_block_size, rcd.length);

	bcopy(&rcd, xs->data, MIN(sizeof(rcd), xs->datalen));

	xbf_scsi_done(xs, XS_NOERROR);
}

void
xbf_scsi_capacity16(struct scsi_xfer *xs)
{
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
	struct scsi_read_cap_data_16 rcd;

	bzero(&rcd, sizeof(rcd));

	_lto8b(sc->sc_disk_size - 1, rcd.addr);
	_lto4b(sc->sc_block_size, rcd.length);

	bcopy(&rcd, xs->data, MIN(sizeof(rcd), xs->datalen));

	xbf_scsi_done(xs, XS_NOERROR);
}

void
xbf_scsi_done(struct scsi_xfer *xs, int error)
{
	int s;

	KERNEL_ASSERT_LOCKED();

	xs->error = error;

	s = splbio();
	scsi_done(xs);
	splx(s);
}

int
xbf_dev_probe(struct scsi_link *link)
{
	KASSERT(link->lun == 0);

	if (link->target == 0)
		return (0);

	return (ENODEV);
}

int
xbf_get_type(struct xbf_softc *sc)
{
	unsigned long long res;
	const char *prop;
	char val[32];
	int error;

	prop = "type";
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) != 0)
		goto errout;
	snprintf(sc->sc_prod, sizeof(sc->sc_prod), "%s", val);

	prop = "dev";
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) != 0)
		goto errout;
	snprintf(sc->sc_prod, sizeof(sc->sc_prod), "%s %s", sc->sc_prod, val);

	prop = "virtual-device";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_node, prop, &res)) != 0)
		goto errout;
	sc->sc_unit = (uint32_t)res;
	snprintf(sc->sc_prod, sizeof(sc->sc_prod), "%s %llu", sc->sc_prod, res);

	prop = "device-type";
	if ((error = xs_getprop(sc->sc_parent, sc->sc_node, prop,
	    sc->sc_dtype, sizeof(sc->sc_dtype))) != 0)
		goto errout;
	if (!strcmp(sc->sc_dtype, "cdrom"))
		sc->sc_type = XBF_CDROM;

	return (0);

 errout:
	printf("%s: failed to read \"%s\" property\n", sc->sc_dev.dv_xname,
	    prop);
	return (-1);
}

int
xbf_init(struct xbf_softc *sc)
{
	unsigned long long res;
	const char *action, *prop;
	char pbuf[sizeof("ring-refXX")];
	unsigned int i;
	int error;

	prop = "max-ring-page-order";
	error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res);
	if (error == 0)
		sc->sc_xr_size = 1 << res;
	if (error == ENOENT) {
		prop = "max-ring-pages";
		error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res);
		if (error == 0)
			sc->sc_xr_size = res;
	}
	/* Fallback to the known minimum */
	if (error)
		sc->sc_xr_size = XBF_MIN_RING_SIZE;

	if (sc->sc_xr_size < XBF_MIN_RING_SIZE)
		sc->sc_xr_size = XBF_MIN_RING_SIZE;
	if (sc->sc_xr_size > XBF_MAX_RING_SIZE)
		sc->sc_xr_size = XBF_MAX_RING_SIZE;
	if (!powerof2(sc->sc_xr_size))
		sc->sc_xr_size = 1 << (fls(sc->sc_xr_size) - 1);

	sc->sc_xr_ndesc = ((sc->sc_xr_size * PAGE_SIZE) -
	    sizeof(struct xbf_ring)) / sizeof(union xbf_ring_desc);
	if (!powerof2(sc->sc_xr_ndesc))
		sc->sc_xr_ndesc = 1 << (fls(sc->sc_xr_ndesc) - 1);
	if (sc->sc_xr_ndesc > XBF_MAX_REQS)
		sc->sc_xr_ndesc = XBF_MAX_REQS;

	DPRINTF("%s: %u ring pages, %d requests\n",
	    sc->sc_dev.dv_xname, sc->sc_xr_size, sc->sc_xr_ndesc);

	if (xbf_ring_create(sc))
		return (-1);

	action = "set";

	for (i = 0; i < sc->sc_xr_size; i++) {
		if (i == 0 && sc->sc_xr_size == 1)
			snprintf(pbuf, sizeof(pbuf), "ring-ref");
		else
			snprintf(pbuf, sizeof(pbuf), "ring-ref%d", i);
		prop = pbuf;
		if (xs_setnum(sc->sc_parent, sc->sc_node, prop,
		    sc->sc_xr_ref[i]))
			goto errout;
	}

	if (sc->sc_xr_size > 1) {
		prop = "num-ring-pages";
		if (xs_setnum(sc->sc_parent, sc->sc_node, prop,
		    sc->sc_xr_size))
			goto errout;
		prop = "ring-page-order";
		if (xs_setnum(sc->sc_parent, sc->sc_node, prop,
		    fls(sc->sc_xr_size) - 1))
			goto errout;
	}

	prop = "event-channel";
	if (xs_setnum(sc->sc_parent, sc->sc_node, prop, sc->sc_xih))
		goto errout;

	prop = "protocol";
#ifdef __amd64__
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, "x86_64-abi",
	    strlen("x86_64-abi")))
		goto errout;
#else
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, "x86_32-abi",
	    strlen("x86_32-abi")))
		goto errout;
#endif

	if (xs_setprop(sc->sc_parent, sc->sc_node, "state",
	    XEN_STATE_INITIALIZED, strlen(XEN_STATE_INITIALIZED))) {
		printf("%s: failed to set state to INITIALIZED\n",
		    sc->sc_dev.dv_xname);
		xbf_ring_destroy(sc);
		return (-1);
	}

	if (xs_await_transition(sc->sc_parent, sc->sc_backend, "state",
	    XEN_STATE_CONNECTED, 10000)) {
		printf("%s: timed out waiting for backend to connect\n",
		    sc->sc_dev.dv_xname);
		xbf_ring_destroy(sc);
		return (-1);
	}

	action = "read";

	prop = "sectors";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0)
		goto errout;
	sc->sc_disk_size = res;

	prop = "sector-size";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0)
		goto errout;
	sc->sc_block_size = res;

	prop = "feature-barrier";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps |= XBF_CAP_BARRIER;

	prop = "feature-flush-cache";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps |= XBF_CAP_FLUSH;

#ifdef XBF_DEBUG
	if (sc->sc_caps) {
		printf("%s: features:", sc->sc_dev.dv_xname);
		if (sc->sc_caps & XBF_CAP_BARRIER)
			printf(" BARRIER");
		if (sc->sc_caps & XBF_CAP_FLUSH)
			printf(" FLUSH");
		printf("\n");
	}
#endif

	if (xs_setprop(sc->sc_parent, sc->sc_node, "state",
	    XEN_STATE_CONNECTED, strlen(XEN_STATE_CONNECTED))) {
		printf("%s: failed to set state to CONNECTED\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}

	sc->sc_state = XBF_CONNECTED;

	return (0);

 errout:
	printf("%s: failed to %s \"%s\" property (%d)\n", sc->sc_dev.dv_xname,
	    action, prop, error);
	xbf_ring_destroy(sc);
	return (-1);
}

int
xbf_dma_alloc(struct xbf_softc *sc, struct xbf_dma_mem *dma,
    bus_size_t size, int nsegs, int mapflags)
{
	int error;

	dma->dma_tag = sc->sc_dmat;

	dma->dma_seg = mallocarray(nsegs, sizeof(bus_dma_segment_t), M_DEVBUF,
	    M_ZERO | M_NOWAIT);
	if (dma->dma_seg == NULL) {
		printf("%s: failed to allocate a segment array\n",
		    sc->sc_dev.dv_xname);
		return (ENOMEM);
	}

	error = bus_dmamap_create(dma->dma_tag, size, nsegs, PAGE_SIZE, 0,
	    BUS_DMA_NOWAIT, &dma->dma_map);
	if (error) {
		printf("%s: failed to create a memory map (%d)\n",
		    sc->sc_dev.dv_xname, error);
		goto errout;
	}

	error = bus_dmamem_alloc(dma->dma_tag, size, PAGE_SIZE, 0,
	    dma->dma_seg, nsegs, &dma->dma_rsegs, BUS_DMA_ZERO |
	    BUS_DMA_NOWAIT);
	if (error) {
		printf("%s: failed to allocate DMA memory (%d)\n",
		    sc->sc_dev.dv_xname, error);
		goto destroy;
	}

	error = bus_dmamem_map(dma->dma_tag, dma->dma_seg, dma->dma_rsegs,
	    size, &dma->dma_vaddr, BUS_DMA_NOWAIT);
	if (error) {
		printf("%s: failed to map DMA memory (%d)\n",
		    sc->sc_dev.dv_xname, error);
		goto free;
	}

	error = bus_dmamap_load(dma->dma_tag, dma->dma_map, dma->dma_vaddr,
	    size, NULL, mapflags | BUS_DMA_NOWAIT);
	if (error) {
		printf("%s: failed to load DMA memory (%d)\n",
		    sc->sc_dev.dv_xname, error);
		goto unmap;
	}

	dma->dma_size = size;
	dma->dma_nsegs = nsegs;
	return (0);

 unmap:
	bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, size);
 free:
	bus_dmamem_free(dma->dma_tag, dma->dma_seg, dma->dma_rsegs);
 destroy:
	bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
 errout:
	free(dma->dma_seg, M_DEVBUF, nsegs * sizeof(bus_dma_segment_t));
	dma->dma_map = NULL;
	dma->dma_tag = NULL;
	return (error);
}

void
xbf_dma_free(struct xbf_softc *sc, struct xbf_dma_mem *dma)
{
	if (dma->dma_tag == NULL || dma->dma_map == NULL)
		return;
	bus_dmamap_sync(dma->dma_tag, dma->dma_map, 0, dma->dma_size,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
	bus_dmamap_unload(dma->dma_tag, dma->dma_map);
	bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, dma->dma_size);
	bus_dmamem_free(dma->dma_tag, dma->dma_seg, dma->dma_rsegs);
	bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
	free(dma->dma_seg, M_DEVBUF, dma->dma_nsegs * sizeof(bus_dma_segment_t));
	dma->dma_seg = NULL;
	dma->dma_map = NULL;
	dma->dma_size = 0;
}

int
xbf_ring_create(struct xbf_softc *sc)
{
	int i;

	if (xbf_dma_alloc(sc, &sc->sc_xr_dma, sc->sc_xr_size * PAGE_SIZE,
	    sc->sc_xr_size, sc->sc_domid << 16))
		return (-1);
	for (i = 0; i < sc->sc_xr_dma.dma_map->dm_nsegs; i++)
		sc->sc_xr_ref[i] = sc->sc_xr_dma.dma_map->dm_segs[i].ds_addr;

	sc->sc_xr = (struct xbf_ring *)sc->sc_xr_dma.dma_vaddr;

	sc->sc_xr->xr_prod_event = sc->sc_xr->xr_cons_event = 1;

	for (i = 0; i < sc->sc_xr_ndesc; i++)
		sc->sc_xr->xr_desc[i].xrd_req.req_id = i;

	/* The number of contiguous blocks addressable by one descriptor */
	sc->sc_xrd_nblk = (PAGE_SIZE * XBF_MAX_SGE) / (1 << XBF_SEC_SHIFT);

	if (xbf_alloc_ccbs(sc)) {
		xbf_ring_destroy(sc);
		return (-1);
	}

	return (0);
}

void
xbf_ring_destroy(struct xbf_softc *sc)
{
	xbf_free_ccbs(sc);
	xbf_dma_free(sc, &sc->sc_xr_dma);
	sc->sc_xr = NULL;
}

void
xbf_stop(struct xbf_softc *sc)
{
	struct xbf_ccb *ccb, *nccb;
	bus_dmamap_t map;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_xr_dma.dma_map, 0,
	    sc->sc_xr_dma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD |
	    BUS_DMASYNC_POSTWRITE);

	TAILQ_FOREACH_SAFE(ccb, &sc->sc_ccb_sq, ccb_link, nccb) {
		TAILQ_REMOVE(&sc->sc_ccb_sq, ccb, ccb_link);

		if (ccb->ccb_bbuf.dma_size > 0)
			map = ccb->ccb_bbuf.dma_map;
		else
			map = ccb->ccb_dmap;
		bus_dmamap_sync(sc->sc_dmat, map, 0, map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, map);

		xbf_reclaim_cmd(ccb->ccb_xfer);
		xbf_scsi_done(ccb->ccb_xfer, XS_SELTIMEOUT);
	}

	xbf_ring_destroy(sc);
}

int
xbf_alloc_ccbs(struct xbf_softc *sc)
{
	int i, error;

	TAILQ_INIT(&sc->sc_ccb_fq);
	TAILQ_INIT(&sc->sc_ccb_sq);
	mtx_init(&sc->sc_ccb_fqlck, IPL_BIO);
	mtx_init(&sc->sc_ccb_sqlck, IPL_BIO);

	sc->sc_nccb = sc->sc_xr_ndesc / 2;

	sc->sc_ccbs = mallocarray(sc->sc_nccb, sizeof(struct xbf_ccb),
	    M_DEVBUF, M_ZERO | M_NOWAIT);
	if (sc->sc_ccbs == NULL) {
		printf("%s: failed to allocate CCBs\n", sc->sc_dev.dv_xname);
		return (-1);
	}

	for (i = 0; i < sc->sc_nccb; i++) {
		/*
		 * Each CCB is set up to use up to 2 descriptors and
		 * each descriptor can transfer XBF_MAX_SGE number of
		 * pages.
		 */
		error = bus_dmamap_create(sc->sc_dmat, MAXPHYS, 2 *
		    XBF_MAX_SGE, PAGE_SIZE, PAGE_SIZE, BUS_DMA_NOWAIT,
		    &sc->sc_ccbs[i].ccb_dmap);
		if (error) {
			printf("%s: failed to create a memory map for "
			    "the xfer %d (%d)\n", sc->sc_dev.dv_xname, i,
			    error);
			goto errout;
		}

		xbf_put_ccb(sc, &sc->sc_ccbs[i]);
	}

	scsi_iopool_init(&sc->sc_iopool, sc, xbf_get_ccb, xbf_put_ccb);

	return (0);

 errout:
	xbf_free_ccbs(sc);
	return (-1);
}

void
xbf_free_ccbs(struct xbf_softc *sc)
{
	struct xbf_ccb *ccb;
	int i;

	for (i = 0; i < sc->sc_nccb; i++) {
		ccb = &sc->sc_ccbs[i];
		if (ccb->ccb_dmap == NULL)
			continue;
		bus_dmamap_sync(sc->sc_dmat, ccb->ccb_dmap, 0, 0,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, ccb->ccb_dmap);
		bus_dmamap_destroy(sc->sc_dmat, ccb->ccb_dmap);
	}

	free(sc->sc_ccbs, M_DEVBUF, sc->sc_nccb * sizeof(struct xbf_ccb));
	sc->sc_ccbs = NULL;
	sc->sc_nccb = 0;
}

void *
xbf_get_ccb(void *xsc)
{
	struct xbf_softc *sc = xsc;
	struct xbf_ccb *ccb;

	if (sc->sc_state != XBF_CONNECTED &&
	    sc->sc_state != XBF_CLOSING)
		return (NULL);

	mtx_enter(&sc->sc_ccb_fqlck);
	ccb = TAILQ_FIRST(&sc->sc_ccb_fq);
	if (ccb != NULL)
		TAILQ_REMOVE(&sc->sc_ccb_fq, ccb, ccb_link);
	mtx_leave(&sc->sc_ccb_fqlck);

	return (ccb);
}

void
xbf_put_ccb(void *xsc, void *io)
{
	struct xbf_softc *sc = xsc;
	struct xbf_ccb *ccb = io;

	ccb->ccb_xfer = NULL;

	mtx_enter(&sc->sc_ccb_fqlck);
	TAILQ_INSERT_HEAD(&sc->sc_ccb_fq, ccb, ccb_link);
	mtx_leave(&sc->sc_ccb_fqlck);
}
@


1.29
log
@Call xbf_intr for polled transfers that can't sleep
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.28 2017/03/19 16:37:19 mikeb Exp $	*/
d145 12
a179 1
	uint32_t		 sc_maxphys;
d183 1
d192 10
a201 4
	struct scsi_xfer	**sc_xs;
	bus_dmamap_t		*sc_xs_map;
	int			 sc_xs_avail;
	struct xbf_dma_mem	*sc_xs_bb;
d223 3
a225 6
void	*xbf_io_get(void *);
void	xbf_io_put(void *, void *);

int	xbf_load_xs(struct scsi_xfer *, int);
int	xbf_bounce_xs(struct scsi_xfer *, int);
void	xbf_reclaim_xs(struct scsi_xfer *, int);
d229 2
a230 2
int	xbf_poll_cmd(struct scsi_xfer *, int, int);
void	xbf_complete_cmd(struct scsi_xfer *, int);
a232 1
void	xbf_scsi_minphys(struct buf *, struct scsi_link *);
d249 5
a291 2
	scsi_iopool_init(&sc->sc_iopool, sc, xbf_io_get, xbf_io_put);

d302 1
a302 1
	sc->sc_switch.scsi_minphys = xbf_scsi_minphys;
d310 1
a310 1
	sc->sc_link.openings = sc->sc_xr_ndesc - 1;
d353 2
a354 1
	struct scsi_xfer *xs;
d356 1
a356 1
	int desc;
d358 1
a358 2
	bus_dmamap_sync(dma->dma_tag, dma->dma_map, 0, dma->dma_size,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
d360 3
a362 6
	for (cons = sc->sc_xr_cons; cons != xr->xr_cons; cons++) {
		desc = cons & (sc->sc_xr_ndesc - 1);
		xs = sc->sc_xs[desc];
		if (xs != NULL)
			xbf_complete_cmd(xs, desc);
	}
d364 4
a367 2
	sc->sc_xr_cons = cons;
}
d369 1
a369 5
void *
xbf_io_get(void *xsc)
{
	struct xbf_softc *sc = xsc;
	void *rv = sc; /* just has to be !NULL */
d371 2
a372 3
	if (sc->sc_state != XBF_CONNECTED &&
	    sc->sc_state != XBF_CLOSING)
		rv = NULL;
d374 10
a383 12
	return (rv);
}

void
xbf_io_put(void *xsc, void *io)
{
#ifdef DIAGNOSTIC
	struct xbf_softc *sc = xsc;

	if (sc != io)
		panic("xbf_io_put: unexpected io");
#endif
a389 1
	int desc;
d435 1
a435 2
	desc = xbf_submit_cmd(xs);
	if (desc < 0) {
d440 1
a440 1
	if (ISSET(xs->flags, SCSI_POLL) && xbf_poll_cmd(xs, desc, 1000)) {
d444 1
a444 2
			sc->sc_xs[desc] = NULL;
			xbf_reclaim_xs(xs, desc);
d452 1
a452 1
xbf_load_xs(struct scsi_xfer *xs, int desc)
d455 1
d459 2
a460 1
	int i, error, mapflags;
d462 1
a462 2
	xrd = &sc->sc_xr->xr_desc[desc];
	map = sc->sc_xs_map[desc];
d477 1
a477 1
		DPRINTF("%s: failed to load %d bytes of data\n",
d482 17
a498 5
	for (i = 0; i < map->dm_nsegs; i++) {
		sge = &xrd->xrd_req.req_sgl[i];
		sge->sge_ref = map->dm_segs[i].ds_addr;
		sge->sge_first = i > 0 ? 0 :
		    ((vaddr_t)xs->data & PAGE_MASK) >> XBF_SEC_SHIFT;
d500 1
a500 1
		    (map->dm_segs[i].ds_len >> XBF_SEC_SHIFT) - 1;
d503 2
a504 2
		    sc->sc_dev.dv_xname, i + 1, map->dm_nsegs,
		    map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len,
d510 2
a511 1
	xrd->xrd_req.req_nsegs = map->dm_nsegs;
d517 1
a517 1
xbf_bounce_xs(struct scsi_xfer *xs, int desc)
d520 1
d526 2
a527 4
	int i, error, mapflags;

	xrd = &sc->sc_xr->xr_desc[desc];
	dma = &sc->sc_xs_bb[desc];
d530 1
a530 1
	if (size > sc->sc_maxphys)
d543 1
d559 17
a575 5
	for (i = 0; i < map->dm_nsegs; i++) {
		sge = &xrd->xrd_req.req_sgl[i];
		sge->sge_ref = map->dm_segs[i].ds_addr;
		sge->sge_first = i > 0 ? 0 :
		    ((vaddr_t)dma->dma_vaddr & PAGE_MASK) >> XBF_SEC_SHIFT;
d577 1
a577 1
		    (map->dm_segs[i].ds_len >> XBF_SEC_SHIFT) - 1;
d580 2
a581 2
		    sc->sc_dev.dv_xname, i + 1, map->dm_nsegs,
		    map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len,
d587 1
a587 1
	xrd->xrd_req.req_nsegs = map->dm_nsegs;
d593 1
a593 1
xbf_reclaim_xs(struct scsi_xfer *xs, int desc)
d596 2
a597 1
	struct xbf_dma_mem *dma;
a598 1
	dma = &sc->sc_xs_bb[desc];
d605 1
a605 1
	xbf_dma_free(sc, dma);
d612 1
a613 1
	bus_dmamap_t map;
d621 1
d669 15
a683 3
	desc = sc->sc_xr_prod & (sc->sc_xr_ndesc - 1);
	xrd = &sc->sc_xr->xr_desc[desc];
	map = sc->sc_xs_map[desc];
d685 1
a685 3
	xrd->xrd_req.req_op = operation;
	xrd->xrd_req.req_unit = (uint16_t)sc->sc_unit;
	xrd->xrd_req.req_sector = lba;
d688 5
a692 4
		DPRINTF("%s: desc %d %s%s lba %llu nsec %u len %d\n",
		    sc->sc_dev.dv_xname, desc, operation == XBF_OP_READ ?
		    "read" : "write", ISSET(xs->flags, SCSI_POLL) ? "-poll" :
		    "", lba, nblk, xs->datalen);
d695 1
a695 1
			error = xbf_load_xs(xs, desc);
d697 1
a697 1
			error = xbf_bounce_xs(xs, desc);
d701 6
a706 3
		DPRINTF("%s: desc %d %s%s lba %llu\n", sc->sc_dev.dv_xname,
		    desc, operation == XBF_OP_FLUSH ? "flush" : "barrier",
		    ISSET(xs->flags, SCSI_POLL) ? "-poll" : "", lba);
d710 5
a714 1
	sc->sc_xs[desc] = xs;
d716 3
a718 2
	bus_dmamap_sync(sc->sc_dmat, map, 0, map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d720 1
a720 1
	sc->sc_xr_prod++;
d730 1
a730 1
	return (desc);
d734 1
a734 1
xbf_poll_cmd(struct scsi_xfer *xs, int desc, int timo)
d736 2
d752 1
a752 1
xbf_complete_cmd(struct scsi_xfer *xs, int desc)
d754 1
a754 1
	struct xbf_softc *sc = xs->sc_link->adapter_softc;
d757 1
a757 1
	uint64_t id;
d768 26
a793 2
	if (sc->sc_xs_bb[desc].dma_size > 0)
		map = sc->sc_xs_bb[desc].dma_map;
d795 2
a796 1
		map = sc->sc_xs_map[desc];
a800 2
	sc->sc_xs[desc] = NULL;

a804 1
	id = xrd->xrd_rsp.rsp_id;
d806 1
a806 1
	xrd->xrd_req.req_id = id;
d808 9
a816 13
	xs->resid = 0;

	xbf_reclaim_xs(xs, desc);
	xbf_scsi_done(xs, error);
}

void
xbf_scsi_minphys(struct buf *bp, struct scsi_link *sl)
{
	struct xbf_softc *sc = sl->adapter_softc;

	if (bp->b_bcount > sc->sc_maxphys)
		bp->b_bcount = sc->sc_maxphys;
d903 2
d970 2
a971 1
	int i, error;
d1203 1
a1203 1
	int i, error, nsegs;
d1215 9
a1223 7
	/* SCSI transfer map */
	sc->sc_xs_map = mallocarray(sc->sc_xr_ndesc, sizeof(bus_dmamap_t),
	    M_DEVBUF, M_ZERO | M_NOWAIT);
	if (sc->sc_xs_map == NULL) {
		printf("%s: failed to allocate scsi transfer map\n",
		    sc->sc_dev.dv_xname);
		goto errout;
d1225 35
a1259 6
	sc->sc_xs = mallocarray(sc->sc_xr_ndesc, sizeof(struct scsi_xfer *),
	    M_DEVBUF, M_ZERO | M_NOWAIT);
	if (sc->sc_xs == NULL) {
		printf("%s: failed to allocate scsi transfer array\n",
		    sc->sc_dev.dv_xname);
		goto errout;
a1260 1
	sc->sc_xs_avail = sc->sc_xr_ndesc;
d1262 16
a1277 2
	/* Bounce buffer maps for unaligned buffers */
	sc->sc_xs_bb = mallocarray(sc->sc_xr_ndesc, sizeof(struct xbf_dma_mem),
d1279 3
a1281 4
	if (sc->sc_xs_bb == NULL) {
		printf("%s: failed to allocate bounce buffer maps\n",
		    sc->sc_dev.dv_xname);
		goto errout;
d1284 9
a1292 6
	nsegs = MIN(MAXPHYS / PAGE_SIZE, XBF_MAX_SGE);
	sc->sc_maxphys = nsegs * PAGE_SIZE;

	for (i = 0; i < sc->sc_xr_ndesc; i++) {
		error = bus_dmamap_create(sc->sc_dmat, sc->sc_maxphys, nsegs,
		    PAGE_SIZE, PAGE_SIZE, BUS_DMA_NOWAIT, &sc->sc_xs_map[i]);
d1299 2
a1300 1
		sc->sc_xr->xr_desc[i].xrd_req.req_id = i;
d1303 2
d1308 2
a1309 2
 	xbf_ring_destroy(sc);
 	return (-1);
d1313 1
a1313 1
xbf_ring_destroy(struct xbf_softc *sc)
d1315 1
d1318 3
a1320 4
	for (i = 0; i < sc->sc_xr_ndesc; i++) {
		if (sc->sc_xs_map == NULL)
			break;
		if (sc->sc_xs_map[i] == NULL)
d1322 1
a1322 1
		bus_dmamap_sync(sc->sc_dmat, sc->sc_xs_map[i], 0, 0,
d1324 24
a1347 16
		bus_dmamap_unload(sc->sc_dmat, sc->sc_xs_map[i]);
		bus_dmamap_destroy(sc->sc_dmat, sc->sc_xs_map[i]);
		sc->sc_xs_map[i] = NULL;
	}

	free(sc->sc_xs, M_DEVBUF, sc->sc_xr_ndesc *
	    sizeof(struct scsi_xfer *));
	sc->sc_xs = NULL;

	free(sc->sc_xs_map, M_DEVBUF, sc->sc_xr_ndesc *
	    sizeof(bus_dmamap_t));
	sc->sc_xs_map = NULL;

	free(sc->sc_xs_bb, M_DEVBUF, sc->sc_xr_ndesc *
	    sizeof(struct xbf_dma_mem));
	sc->sc_xs_bb = NULL;
d1349 1
a1349 2
	xbf_dma_free(sc, &sc->sc_xr_dma);
	sc->sc_xr = NULL;
d1353 1
a1353 1
xbf_stop(struct xbf_softc *sc)
d1355 2
a1356 8
	union xbf_ring_desc *xrd;
	struct scsi_xfer *xs;
	bus_dmamap_t map;
	int desc;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_xr_dma.dma_map, 0,
	    sc->sc_xr_dma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD |
	    BUS_DMASYNC_POSTWRITE);
d1358 1
a1358 19
	for (desc = 0; desc < sc->sc_xr_ndesc; desc++) {
		xs = sc->sc_xs[desc];
		if (xs == NULL)
			continue;
		xrd = &sc->sc_xr->xr_desc[desc];
		DPRINTF("%s: aborting desc %d(%llu) op %u\n",
		    sc->sc_dev.dv_xname, desc, xrd->xrd_rsp.rsp_id,
		    xrd->xrd_rsp.rsp_op);
		if (sc->sc_xs_bb[desc].dma_size > 0)
			map = sc->sc_xs_bb[desc].dma_map;
		else
			map = sc->sc_xs_map[desc];
		bus_dmamap_sync(sc->sc_dmat, map, 0, map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, map);
		xbf_reclaim_xs(xs, desc);
		xbf_scsi_done(xs, XS_SELTIMEOUT);
		sc->sc_xs[desc] = NULL;
	}
d1360 3
a1362 1
	xbf_ring_destroy(sc);
@


1.28
log
@Fixup starting block number calculation for bounced transfers

From Nathanael Rensen, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.27 2017/03/19 16:26:28 mikeb Exp $	*/
d691 1
@


1.27
log
@Fixup return values to properly handle transfer submission errors

From Nathanael Rensen, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.26 2017/03/13 01:10:03 mikeb Exp $	*/
d537 1
a537 1
		memcpy((caddr_t)dma->dma_vaddr, xs->data, xs->datalen);
d543 1
a543 1
		    ((vaddr_t)xs->data & PAGE_MASK) >> XBF_SEC_SHIFT;
@


1.26
log
@Fixup format strings in debug messages found by cppcheck
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.25 2017/03/13 01:00:15 mikeb Exp $	*/
d472 1
a472 1
		return (-1);
d655 1
a655 1
			return (error);
d678 1
a678 1
	return desc;
@


1.25
log
@Fixup format string and type issues found by cppcheck
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.24 2017/02/24 16:58:12 mikeb Exp $	*/
d470 1
a470 1
		DPRINTF("%s: failed to load %u bytes of data\n",
d533 1
a533 1
	DPRINTF("%s: bouncing %d bytes via %ld size map with %d segments\n",
d645 1
a645 1
		DPRINTF("%s: desc %u %s%s lba %llu nsec %u len %u\n",
d657 1
a657 1
		DPRINTF("%s: desc %u %s%s lba %llu\n", sc->sc_dev.dv_xname,
d723 1
a723 1
	DPRINTF("%s: completing desc %u(%llu) op %u with error %d\n",
d925 1
a925 1
	DPRINTF("%s: %u ring pages, %u requests\n",
d1237 1
a1237 1
		DPRINTF("%s: aborting desc %u(%llu) op %u\n",
@


1.24
log
@Update license

Some final touches before the release, increase the maximum
number of CAS iterations before we declare the grant table
entry lost forever.  This happens on older Xen 3.x versions
as reported by Kirill Miazine.
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.23 2017/02/08 17:39:57 mikeb Exp $	*/
d937 1
a937 1
			snprintf(pbuf, sizeof(pbuf), "ring-ref%u", i);
@


1.23
log
@Improve error handling for poll timeouts
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.22 2017/02/08 17:32:45 mikeb Exp $	*/
d270 1
a270 1
	printf(" backend %d chan %u: %s\n", sc->sc_domid, sc->sc_xih,
@


1.22
log
@Specify the read/write DMA flag for bounce buffers
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.21 2017/02/08 17:23:46 mikeb Exp $	*/
d434 7
a440 4
		DPRINTF("%s: desc %u timed out\n", sc->sc_dev.dv_xname, desc);
		sc->sc_xs[desc] = NULL;
		xbf_reclaim_xs(xs, desc);
		xbf_scsi_done(xs, XS_TIMEOUT);
@


1.21
log
@Cleanup the device removal path

When destroying the ring all transfers should be already gone so there
should be no need to repeat ourselves after xbf_stop has done its work.

Get rid of the yield() that was probably masking some issues that have
been since fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.20 2017/02/08 16:51:26 mikeb Exp $	*/
d516 4
@


1.20
log
@Switch to Xen interrupt barrier and improve state transitions
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.19 2017/02/08 16:29:00 mikeb Exp $	*/
a1194 3
		if (sc->sc_xs == NULL || sc->sc_xs[i] == NULL)
			continue;
		xbf_scsi_done(sc->sc_xs[i], XS_RESET);
a1209 1

a1243 3

	/* Give other processes a chance to run */
	yield();
@


1.19
log
@Abort transactions with non-retriable error when device is stopped
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.18 2017/02/06 21:47:06 mikeb Exp $	*/
d312 3
d317 1
d319 1
a319 3
	intr_barrier(&sc->sc_xih);

	if (sc->sc_state == XBF_CONNECTED) {
a978 2
	sc->sc_state = XBF_CONNECTED;

d1023 2
a1247 2

	sc->sc_state = XBF_CLOSING;
@


1.18
log
@Fixup a few errors, make detaching more robust
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.17 2017/02/06 21:43:48 mikeb Exp $	*/
d391 1
a391 1
			xbf_scsi_done(xs, XS_RESET);
d1243 1
a1243 1
		xbf_scsi_done(xs, XS_RESET);
@


1.17
log
@Use separate compile time debug flags for xen, xnf and xbf
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.16 2017/01/19 12:36:50 mikeb Exp $	*/
a313 1
	xen_intr_disestablish(sc->sc_xih);
d315 9
a323 1
	xbf_stop(sc);
d325 1
a325 1
	return (config_detach(sc->sc_scsibus, flags | DETACH_FORCE));
d517 1
a517 1
		DPRINTF("%s: failed to allocate a %u byte bounce buffer\n",
d965 1
d973 1
d1028 1
d1041 1
a1041 1
	    M_ZERO | BUS_DMA_NOWAIT);
d1057 2
a1058 1
	    dma->dma_seg, nsegs, &dma->dma_rsegs, BUS_DMA_NOWAIT);
d1189 1
a1189 1
		    BUS_DMASYNC_POSTWRITE);
@


1.16
log
@No need for conditionals around free(9)
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.15 2017/01/18 22:18:47 mikeb Exp $	*/
d41 8
d996 1
a996 1
#ifdef XEN_DEBUG
@


1.15
log
@Don't forget to free bounce buffer data when destroying the ring
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.14 2016/12/23 12:52:12 mikeb Exp $	*/
d1178 12
a1189 15
	if (sc->sc_xs) {
		free(sc->sc_xs, M_DEVBUF, sc->sc_xr_ndesc *
		    sizeof(struct scsi_xfer *));
		sc->sc_xs = NULL;
	}
	if (sc->sc_xs_map) {
		free(sc->sc_xs_map, M_DEVBUF, sc->sc_xr_ndesc *
		    sizeof(bus_dmamap_t));
		sc->sc_xs_map = NULL;
	}
	if (sc->sc_xs_bb) {
		free(sc->sc_xs_bb, M_DEVBUF, sc->sc_xr_ndesc *
		    sizeof(struct xbf_dma_mem));
		sc->sc_xs_bb = NULL;
	}
@


1.14
log
@Implement disk detaching
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.13 2016/12/15 03:41:15 jsg Exp $	*/
d1187 5
@


1.13
log
@fix build when DIAGNOSTIC is not defined
ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.12 2016/12/14 11:39:30 mikeb Exp $	*/
d149 1
d180 1
a180 1
	struct scsibus_softc	*sc_scsibus;
d185 1
d192 1
a192 1
	sizeof(struct xbf_softc), xbf_match, xbf_attach
a208 1
void	xbf_dev_free(struct scsi_link *);
d225 1
a225 1
int	xbf_capabilities(struct xbf_softc *);
a278 1
	sc->sc_switch.dev_free = xbf_dev_free;
d290 1
a290 1
	config_found(self, &saa, scsiprint);
d300 13
d342 2
a343 1
	if (sc->sc_state != XBF_CONNECTED)
d375 4
a379 1

a385 1

a394 1

a399 1

a823 6
void
xbf_dev_free(struct scsi_link *link)
{
	printf("%s\n", __func__);
}

d1192 40
@


1.12
log
@Various typos and minor cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.11 2016/12/13 19:02:39 mikeb Exp $	*/
d338 1
a340 1
#ifdef DIAGNOSTIC
@


1.11
log
@Bounce unaligned transfer data through a scratch buffer

Thanks to the detailed report from Nathanael Rensen, the issue
with unaligned transfer data became apparent: the backend expects
buffers be multiple of 512 bytes and to be 512 byte aligned, which
is not always satisfied.

This isn't an issue when requests are coming from the buffer cache,
but can happen with raw device access since physio(9) ensures the
former requirement is met by disallowing non-block sized reads, but
doesn't enforce the latter.  It remaps userland buffers into the
kernel virtual space which preserves the data offset within the
memory page and thus the original alignment.

Buffers with offsets under the block size can't be referenced by
Blkfront ring descriptors that measure data in blocks and must be
substituted with temporary buffers for the duration of the I/O
operation.
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.10 2016/12/13 18:27:24 mikeb Exp $	*/
d59 1
a59 1
#define XBF_EOPNOTSUPP		-2 /* only for XBF_OP_WRBAR */
a330 2
	else
		KASSERT(atomic_dec_int_nv(&sc->sc_xs_avail) >= 0);
d342 1
a342 1
		panic("vsdk_io_put: unexpected io");
a343 2

	KASSERT(atomic_inc_int_nv(&sc->sc_xs_avail) <= sc->sc_xr_ndesc);
d669 1
a669 1
	    BUS_DMASYNC_POSTREAD);
@


1.10
log
@Poll until the ITSDONE flag is set on the transfer
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.9 2016/12/10 19:41:31 mikeb Exp $	*/
d131 3
a133 2
	bus_dma_segment_t	 dma_seg;
	int			 dma_nsegs;
d174 1
d198 4
d409 1
d416 127
a546 1
	struct xbf_sge *sge;
d555 1
a555 2
	int mapflags;
	int i, desc, error;
d606 4
d611 1
a611 12
		mapflags = (sc->sc_domid << 16) | BUS_DMA_NOWAIT;
		mapflags |= operation == XBF_OP_READ ? BUS_DMA_READ :
		    BUS_DMA_WRITE;
		error = bus_dmamap_load(sc->sc_dmat, map, xs->data,
		    xs->datalen, NULL, mapflags);
		if (error) {
			DPRINTF("%s: failed to load %u bytes of data\n",
			    sc->sc_dev.dv_xname, xs->datalen);
			return (-1);
		}

		DPRINTF("%s: desc %u %s%s lba %llu nsec %u segs %u len %u\n",
d614 1
a614 1
		    "", lba, nblk, map->dm_nsegs, xs->datalen);
d616 6
a621 13
		for (i = 0; i < map->dm_nsegs; i++) {
			sge = &xrd->xrd_req.req_sgl[i];
			sge->sge_ref = map->dm_segs[i].ds_addr;
			sge->sge_first = i > 0 ? 0 :
			    ((vaddr_t)xs->data & PAGE_MASK) >> XBF_SEC_SHIFT;
			sge->sge_last = sge->sge_first +
			    (map->dm_segs[i].ds_len >> XBF_SEC_SHIFT) - 1;
			DPRINTF("%s:   seg %d ref %lu len %lu first %u "
			    "last %u\n", sc->sc_dev.dv_xname, i,
			    map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len,
			    sge->sge_first, sge->sge_last);
			KASSERT(sge->sge_last <= 7);
		}
d626 1
a626 1
		map->dm_nsegs = 0;
a630 5
	xrd->xrd_req.req_op = operation;
	xrd->xrd_req.req_nsegs = map->dm_nsegs;
	xrd->xrd_req.req_unit = (uint16_t)sc->sc_unit;
	xrd->xrd_req.req_sector = lba;

d679 4
a682 1
	map = sc->sc_xs_map[desc];
d698 2
d1012 1
a1012 1
    bus_size_t size, int nseg, int mapflags)
d1018 9
a1026 1
	error = bus_dmamap_create(dma->dma_tag, size, nseg, PAGE_SIZE, 0,
d1035 1
a1035 1
	    &dma->dma_seg, nseg, &dma->dma_nsegs, BUS_DMA_NOWAIT);
d1042 1
a1042 1
	error = bus_dmamem_map(dma->dma_tag, &dma->dma_seg, dma->dma_nsegs,
d1059 1
d1065 1
a1065 1
	bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nsegs);
d1069 1
d1084 1
a1084 1
	bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nsegs);
d1086 2
d1089 1
d1123 9
@


1.9
log
@Bring back the sector count variable as it's used in the debug printf
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.8 2016/12/10 19:38:50 mikeb Exp $	*/
d310 2
a311 2
		KASSERT(xs != NULL);
		xbf_complete_cmd(xs, desc);
d402 1
a536 2
	struct xbf_softc *sc = xs->sc_link->adapter_softc;

d538 1
a538 1
		if (sc->sc_xs[desc] == NULL)
d540 4
a543 1
		delay(1000);
@


1.8
log
@Don't fail if optional feature properties aren't found

Nathanael Rensen has noticed that the driver would fail to attach if
optional "feature-barrier" or "feature-flush-cache" properties cannot
be fetched and has provided a patch to solve the issue, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.7 2016/12/09 17:35:13 mikeb Exp $	*/
d418 2
a419 1
	u_int64_t lba = 0;
d454 1
d458 1
d462 1
d466 1
@


1.7
log
@Convert to the new xs_{get,set}num XenStore API
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.6 2016/12/08 19:30:44 mikeb Exp $	*/
d846 2
a847 1
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0)
d849 1
a849 1
	if (res == 1)
d853 2
a854 1
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0)
d856 1
a856 1
	if (res == 1)
@


1.6
log
@Silence scan-build; with prodding from jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.5 2016/12/07 21:08:55 mikeb Exp $	*/
a219 7
static int
	xbf_get_numval(struct xbf_softc *, int, const char *,
	    unsigned long long *);

static unsigned long long
	strtoull(const char *, int *);

d716 1
a716 1
	if ((error = xbf_get_numval(sc, 0, prop, &res)) != 0)
a741 1
	char val[32];
d745 1
a745 1
	error = xbf_get_numval(sc, 1, prop, &res);
d750 1
a750 1
		error = xbf_get_numval(sc, 1, prop, &res);
d786 2
a787 3
		snprintf(val, sizeof(val), "%u", sc->sc_xr_ref[i]);
		if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val,
		    strlen(val)))
d793 2
a794 3
		snprintf(val, sizeof(val), "%u", sc->sc_xr_size);
		if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val,
		    strlen(val)))
d797 2
a798 3
		snprintf(val, sizeof(val), "%u", fls(sc->sc_xr_size) - 1);
		if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val,
		    strlen(val)))
d803 1
a803 2
	snprintf(val, sizeof(val), "%u", sc->sc_xih);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d808 3
a810 1
	snprintf(val, sizeof(val), "%s", "x86_64-abi");
d812 3
a814 1
	snprintf(val, sizeof(val), "%s", "x86_32-abi");
a815 2
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
		goto errout;
d836 1
a836 1
	if ((error = xbf_get_numval(sc, 1, prop, &res)) != 0)
d841 1
a841 1
	if ((error = xbf_get_numval(sc, 1, prop, &res)) != 0)
d846 1
a846 1
	if ((error = xbf_get_numval(sc, 1, prop, &res)) != 0)
d852 1
a852 1
	if ((error = xbf_get_numval(sc, 1, prop, &res)) != 0)
a1038 53
}

static int
xbf_get_numval(struct xbf_softc *sc, int backend, const char *prop,
    unsigned long long *value)
{
	unsigned long long res;
	char buf[32];
	int error;

	error = xs_getprop(sc->sc_parent, backend ? sc->sc_backend :
	    sc->sc_node, prop, buf, sizeof(buf));
	if (error)
		return (error);

	res = strtoull(buf, &error);
	if (error)
		return (error);
	*value = res;
	return (0);
}

static unsigned long long
strtoull(const char *nptr, int *error)
{
	unsigned long long mul, ores, res;
	size_t len = 0;
	char *cp;
	int ch;

	res = 0;
	mul = 1;
	for (cp = (char *)nptr; *cp != 0; cp++)
		len++;
	for (cp--; len > 0; len--, cp--) {
		ch = *cp;
		if (ch < '0' || ch > '9') {
			if (error)
				*error = EINVAL; /* invalid char */
			return (res);
		}
		ores = res;
		res += (ch - '0') * mul;
		if (res < ores) {
			if (error)
				*error = ERANGE; /* overflow */
			return (res);
		}
		mul *= 10;
	}
	if (error)
		*error = 0;
	return (res);
@


1.5
log
@Add required padding to the response descriptor

Xen source code relies on the compiler to pad members of the structure
representing the descriptor layout in memory; we're however trying to
be more defensive and define packed structures.

Figured out the hard way with reyk@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.4 2016/12/07 21:06:55 mikeb Exp $	*/
a405 2
	if (!ISSET(xs->flags, SCSI_POLL))
		return;
d407 1
a407 1
	if (xbf_poll_cmd(xs, desc, 1000)) {
d425 2
a426 3
	u_int64_t lba;
	u_int32_t nblk;
	uint8_t operation;
a459 1
		nblk = rw->length ? rw->length : 0x100;
a462 1
		nblk = _2btol(rwb->length);
a465 1
		nblk = _4btol(rw12->length);
a468 1
		nblk = _4btol(rw16->length);
a555 2
	int16_t status;
	uint8_t op;
d573 4
a577 2
	op = xrd->xrd_rsp.rsp_op; /* TEMP */
	status = xrd->xrd_rsp.rsp_status; /* TEMP */
a580 3
	DPRINTF("%s: completing desc %u(%llu) op %u with error %d\n",
	    sc->sc_dev.dv_xname, desc, id, op, status);

a750 2

	action = "read";
@


1.4
log
@Response status field is signed; adjust the debug message
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.3 2016/12/07 19:17:52 mikeb Exp $	*/
d101 1
d103 3
@


1.3
log
@Fall back to the minimum amount of ring pages

Some AWS volumes lack both max-ring-page-order and max-ring-pages
properties so we have to default to a minimum amount of 1 ring page.
Noticed and fix tested by reyk@@ on EC2.
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.2 2016/12/07 18:33:45 mikeb Exp $	*/
d559 1
a559 1
	uint16_t status;
d584 1
a584 1
	DPRINTF("%s: completing desc %u(%llu) op %u with error %u\n",
@


1.2
log
@splbio isn't doing us any good here and prevents polling from working
@
text
@d1 1
a1 1
/*	$OpenBSD: xbf.c,v 1.1 2016/12/07 15:26:43 mikeb Exp $	*/
d770 1
d772 1
a772 1
		goto errout;
@


1.1
log
@A driver for Xen Blkfront heavily based on vdsk(4) from kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d351 1
a351 1
	int desc, s;
a396 1
	s = splbio();
a398 1
		splx(s);
d402 1
a402 3

	if (!ISSET(xs->flags, SCSI_POLL)) {
		splx(s);
a403 1
	}
d406 1
a406 2
		splx(s);
		printf("%s: desc %u timed out\n", sc->sc_dev.dv_xname, desc);
a409 1
	splx(s);
@

