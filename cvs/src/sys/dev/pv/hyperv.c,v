head	1.39;
access;
symbols
	OPENBSD_6_1:1.35.0.4
	OPENBSD_6_1_BASE:1.35
	OPENBSD_6_0:1.4.0.2
	OPENBSD_6_0_BASE:1.4;
locks; strict;
comment	@ * @;


1.39
date	2017.06.23.18.54.44;	author mikeb;	state Exp;
branches;
next	1.38;
commitid	g3MiBOL7MB0GMLnh;

1.38
date	2017.06.15.20.12.12;	author mikeb;	state Exp;
branches;
next	1.37;
commitid	kWisba78Jk3C8dUx;

1.37
date	2017.06.15.20.08.56;	author mikeb;	state Exp;
branches;
next	1.36;
commitid	lmldO8LtWL9twraM;

1.36
date	2017.06.14.09.47.37;	author mikeb;	state Exp;
branches;
next	1.35;
commitid	ZuMc1lf2ThkFIJSc;

1.35
date	2017.01.16.11.54.26;	author mikeb;	state Exp;
branches;
next	1.34;
commitid	1TtUb44BELaPrT6M;

1.34
date	2017.01.05.13.17.22;	author mikeb;	state Exp;
branches;
next	1.33;
commitid	5BJTIoglFDn7Rjmv;

1.33
date	2017.01.05.11.47.02;	author mikeb;	state Exp;
branches;
next	1.32;
commitid	AgK0ddEFWMG3bGCG;

1.32
date	2016.11.23.12.40.04;	author mikeb;	state Exp;
branches;
next	1.31;
commitid	vd72Ax9ifxTBeJZY;

1.31
date	2016.11.23.11.59.08;	author mikeb;	state Exp;
branches;
next	1.30;
commitid	ONaW7R7ZsGRAA7CU;

1.30
date	2016.11.23.09.37.43;	author mikeb;	state Exp;
branches;
next	1.29;
commitid	5XgIePKjlCwEuU2x;

1.29
date	2016.11.22.13.51.21;	author mikeb;	state Exp;
branches;
next	1.28;
commitid	hfYNPnlYib8u3ZTb;

1.28
date	2016.11.14.14.03.09;	author mikeb;	state Exp;
branches;
next	1.27;
commitid	9KgMaPdMFi7rts4Z;

1.27
date	2016.11.10.17.45.29;	author mikeb;	state Exp;
branches;
next	1.26;
commitid	o4FW44Pnwbi91Qb7;

1.26
date	2016.11.08.21.43.24;	author mikeb;	state Exp;
branches;
next	1.25;
commitid	TtcMBQVccgBJehQS;

1.25
date	2016.11.03.13.37.19;	author mikeb;	state Exp;
branches;
next	1.24;
commitid	afT2kKLnYILcEj3s;

1.24
date	2016.11.02.19.56.25;	author mikeb;	state Exp;
branches;
next	1.23;
commitid	0Kz6vIX3UERP0SmJ;

1.23
date	2016.11.02.19.06.05;	author mikeb;	state Exp;
branches;
next	1.22;
commitid	2GGfvIXUEj2dAM7N;

1.22
date	2016.10.27.14.04.09;	author mikeb;	state Exp;
branches;
next	1.21;
commitid	42EqlD4qFPBRp5xv;

1.21
date	2016.10.25.19.36.11;	author mikeb;	state Exp;
branches;
next	1.20;
commitid	tujUilLvS05dLN8J;

1.20
date	2016.10.14.18.32.54;	author mikeb;	state Exp;
branches;
next	1.19;
commitid	BUcvGWHE135w7XvH;

1.19
date	2016.10.11.11.46.49;	author mikeb;	state Exp;
branches;
next	1.18;
commitid	EdcKmtCuROaO2Xuz;

1.18
date	2016.10.10.21.08.39;	author mikeb;	state Exp;
branches;
next	1.17;
commitid	wZhE0zlV0nfBXn3M;

1.17
date	2016.10.04.09.46.31;	author kettenis;	state Exp;
branches;
next	1.16;
commitid	bCksURwv7BBZjYTT;

1.16
date	2016.09.20.10.27.14;	author mikeb;	state Exp;
branches;
next	1.15;
commitid	rY9brrie9yj7uMrM;

1.15
date	2016.08.18.09.20.01;	author mikeb;	state Exp;
branches;
next	1.14;
commitid	P4CAPkyuDtEZvJaA;

1.14
date	2016.08.17.17.39.41;	author mikeb;	state Exp;
branches;
next	1.13;
commitid	VRQF41lN3BVrqsJk;

1.13
date	2016.08.16.14.43.31;	author mikeb;	state Exp;
branches;
next	1.12;
commitid	gjDTTLYz8B3N4PzV;

1.12
date	2016.08.08.16.39.06;	author mikeb;	state Exp;
branches;
next	1.11;
commitid	IB26XCb51rdD01l0;

1.11
date	2016.08.05.18.16.04;	author mikeb;	state Exp;
branches;
next	1.10;
commitid	gJs47xQOJG7tGR4O;

1.10
date	2016.08.05.18.12.48;	author mikeb;	state Exp;
branches;
next	1.9;
commitid	61EJHDaLQ2W5ETAn;

1.9
date	2016.08.05.18.12.20;	author mikeb;	state Exp;
branches;
next	1.8;
commitid	asAXJY0qKdJk9ace;

1.8
date	2016.08.05.18.05.44;	author mikeb;	state Exp;
branches;
next	1.7;
commitid	Hj8Miv0tWl0D37xX;

1.7
date	2016.08.05.11.36.24;	author mikeb;	state Exp;
branches;
next	1.6;
commitid	l5QuHyuez5gc4Xkq;

1.6
date	2016.07.28.12.26.04;	author mikeb;	state Exp;
branches;
next	1.5;
commitid	IxtZ4H2tisVliWUK;

1.5
date	2016.07.28.12.08.14;	author mikeb;	state Exp;
branches;
next	1.4;
commitid	DybuCYH7FsRJfhfz;

1.4
date	2016.07.21.10.21.00;	author mikeb;	state Exp;
branches;
next	1.3;
commitid	oQ0uYlEM533yeyFr;

1.3
date	2016.06.27.13.06.41;	author jsg;	state Exp;
branches;
next	1.2;
commitid	wZFfZyV5YBVzuin3;

1.2
date	2016.06.27.12.32.39;	author jsg;	state Exp;
branches;
next	1.1;
commitid	Ww8u9Yck5ms7FC8I;

1.1
date	2016.06.22.01.13.07;	author mikeb;	state Exp;
branches;
next	;
commitid	qi2GLlgF1qtf4Kfm;


desc
@@


1.39
log
@s/membar_sync/virtio_membar_sync/
@
text
@/*-
 * Copyright (c) 2009-2012 Microsoft Corp.
 * Copyright (c) 2012 NetApp Inc.
 * Copyright (c) 2012 Citrix Inc.
 * Copyright (c) 2016 Mike Belopuhov <mike@@esdenera.com>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice unmodified, this list of conditions, and the following
 *    disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * The OpenBSD port was done under funding by Esdenera Networks GmbH.
 */

#include <sys/param.h>

/* Hyperv requires locked atomic operations */
#ifndef MULTIPROCESSOR
#define _HYPERVMPATOMICS
#define MULTIPROCESSOR
#endif
#include <sys/atomic.h>
#ifdef _HYPERVMPATOMICS
#undef MULTIPROCESSOR
#undef _HYPERVMPATOMICS
#endif

#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/signal.h>
#include <sys/signalvar.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/timetc.h>
#include <sys/task.h>
#include <sys/syslog.h>

#include <machine/bus.h>
#include <machine/cpu.h>
#include <machine/cpufunc.h>

#include <uvm/uvm_extern.h>

#include <machine/i82489var.h>

#include <dev/rndvar.h>

#include <dev/pv/pvvar.h>
#include <dev/pv/pvreg.h>
#include <dev/pv/hypervreg.h>
#include <dev/pv/hypervvar.h>

/* Command submission flags */
#define HCF_SLEEPOK	0x0001	/* M_WAITOK */
#define HCF_NOSLEEP	0x0002	/* M_NOWAIT */
#define HCF_NOREPLY	0x0004

struct hv_softc *hv_sc;

int 	hv_match(struct device *, void *, void *);
void	hv_attach(struct device *, struct device *, void *);
void	hv_set_version(struct hv_softc *);
u_int	hv_gettime(struct timecounter *);
int	hv_init_hypercall(struct hv_softc *);
uint64_t hv_hypercall(struct hv_softc *, uint64_t, void *, void *);
int	hv_init_interrupts(struct hv_softc *);
int	hv_init_synic(struct hv_softc *);
int	hv_cmd(struct hv_softc *, void *, size_t, void *, size_t, int);
int	hv_start(struct hv_softc *, struct hv_msg *);
int	hv_reply(struct hv_softc *, struct hv_msg *);
void	hv_wait(struct hv_softc *, int (*done)(struct hv_softc *,
	    struct hv_msg *), struct hv_msg *, void *, const char *);
uint16_t hv_intr_signal(struct hv_softc *, void *);
void	hv_intr(void);
void	hv_event_intr(struct hv_softc *);
void	hv_message_intr(struct hv_softc *);
int	hv_vmbus_connect(struct hv_softc *);
void	hv_channel_response(struct hv_softc *, struct vmbus_chanmsg_hdr *);
void	hv_channel_offer(struct hv_softc *, struct vmbus_chanmsg_hdr *);
void	hv_channel_delivered(struct hv_softc *, struct vmbus_chanmsg_hdr *);
int	hv_channel_scan(struct hv_softc *);
void	hv_process_offer(struct hv_softc *, struct hv_offer *);
struct hv_channel *
	hv_channel_lookup(struct hv_softc *, uint32_t);
int	hv_channel_ring_create(struct hv_channel *, uint32_t);
void	hv_channel_ring_destroy(struct hv_channel *);
extern void hv_attach_icdevs(struct hv_softc *);
int	hv_attach_devices(struct hv_softc *);

struct {
	int		  hmd_response;
	int		  hmd_request;
	void		(*hmd_handler)(struct hv_softc *,
			    struct vmbus_chanmsg_hdr *);
} hv_msg_dispatch[] = {
	{ 0,					0, NULL },
	{ VMBUS_CHANMSG_CHOFFER,		0, hv_channel_offer },
	{ VMBUS_CHANMSG_CHRESCIND,		0, NULL },
	{ VMBUS_CHANMSG_CHREQUEST,		VMBUS_CHANMSG_CHOFFER,
	  NULL },
	{ VMBUS_CHANMSG_CHOFFER_DONE,		0,
	  hv_channel_delivered },
	{ VMBUS_CHANMSG_CHOPEN,			0, NULL },
	{ VMBUS_CHANMSG_CHOPEN_RESP,		VMBUS_CHANMSG_CHOPEN,
	  hv_channel_response },
	{ VMBUS_CHANMSG_CHCLOSE,		0, NULL },
	{ VMBUS_CHANMSG_GPADL_CONN,		0, NULL },
	{ VMBUS_CHANMSG_GPADL_SUBCONN,		0, NULL },
	{ VMBUS_CHANMSG_GPADL_CONNRESP,		VMBUS_CHANMSG_GPADL_CONN,
	  hv_channel_response },
	{ VMBUS_CHANMSG_GPADL_DISCONN,		0, NULL },
	{ VMBUS_CHANMSG_GPADL_DISCONNRESP,	VMBUS_CHANMSG_GPADL_DISCONN,
	  hv_channel_response },
	{ VMBUS_CHANMSG_CHFREE,			0, NULL },
	{ VMBUS_CHANMSG_CONNECT,		0, NULL },
	{ VMBUS_CHANMSG_CONNECT_RESP,		VMBUS_CHANMSG_CONNECT,
	  hv_channel_response },
	{ VMBUS_CHANMSG_DISCONNECT,		0, NULL },
};

struct timecounter hv_timecounter = {
	hv_gettime, 0, 0xffffffff, 10000000, "hyperv", 9001
};

struct cfdriver hyperv_cd = {
	NULL, "hyperv", DV_DULL
};

const struct cfattach hyperv_ca = {
	sizeof(struct hv_softc), hv_match, hv_attach
};

const struct hv_guid hv_guid_network = {
	{ 0x63, 0x51, 0x61, 0xf8, 0x3e, 0xdf, 0xc5, 0x46,
	  0x91, 0x3f, 0xf2, 0xd2, 0xf9, 0x65, 0xed, 0x0e }
};

const struct hv_guid hv_guid_ide = {
	{ 0x32, 0x26, 0x41, 0x32, 0xcb, 0x86, 0xa2, 0x44,
	  0x9b, 0x5c, 0x50, 0xd1, 0x41, 0x73, 0x54, 0xf5 }
};

const struct hv_guid hv_guid_scsi = {
	{ 0xd9, 0x63, 0x61, 0xba, 0xa1, 0x04, 0x29, 0x4d,
	  0xb6, 0x05, 0x72, 0xe2, 0xff, 0xb1, 0xdc, 0x7f }
};

const struct hv_guid hv_guid_shutdown = {
	{ 0x31, 0x60, 0x0b, 0x0e, 0x13, 0x52, 0x34, 0x49,
	  0x81, 0x8b, 0x38, 0xd9, 0x0c, 0xed, 0x39, 0xdb }
};

const struct hv_guid hv_guid_timesync = {
	{ 0x30, 0xe6, 0x27, 0x95, 0xae, 0xd0, 0x7b, 0x49,
	  0xad, 0xce, 0xe8, 0x0a, 0xb0, 0x17, 0x5c, 0xaf }
};

const struct hv_guid hv_guid_heartbeat = {
	{ 0x39, 0x4f, 0x16, 0x57, 0x15, 0x91, 0x78, 0x4e,
	  0xab, 0x55, 0x38, 0x2f, 0x3b, 0xd5, 0x42, 0x2d }
};

const struct hv_guid hv_guid_kvp = {
	{ 0xe7, 0xf4, 0xa0, 0xa9, 0x45, 0x5a, 0x96, 0x4d,
	  0xb8, 0x27, 0x8a, 0x84, 0x1e, 0x8c, 0x03, 0xe6 }
};

#ifdef HYPERV_DEBUG
const struct hv_guid hv_guid_vss = {
	{ 0x29, 0x2e, 0xfa, 0x35, 0x23, 0xea, 0x36, 0x42,
	  0x96, 0xae, 0x3a, 0x6e, 0xba, 0xcb, 0xa4, 0x40 }
};

const struct hv_guid hv_guid_dynmem = {
	{ 0xdc, 0x74, 0x50, 0x52, 0x85, 0x89, 0xe2, 0x46,
	  0x80, 0x57, 0xa3, 0x07, 0xdc, 0x18, 0xa5, 0x02 }
};

const struct hv_guid hv_guid_mouse = {
	{ 0x9e, 0xb6, 0xa8, 0xcf, 0x4a, 0x5b, 0xc0, 0x4c,
	  0xb9, 0x8b, 0x8b, 0xa1, 0xa1, 0xf3, 0xf9, 0x5a }
};

const struct hv_guid hv_guid_kbd = {
	{ 0x6d, 0xad, 0x12, 0xf9, 0x17, 0x2b, 0xea, 0x48,
	  0xbd, 0x65, 0xf9, 0x27, 0xa6, 0x1c, 0x76, 0x84 }
};

const struct hv_guid hv_guid_video = {
	{ 0x02, 0x78, 0x0a, 0xda, 0x77, 0xe3, 0xac, 0x4a,
	  0x8e, 0x77, 0x05, 0x58, 0xeb, 0x10, 0x73, 0xf8 }
};

const struct hv_guid hv_guid_fc = {
	{ 0x4a, 0xcc, 0x9b, 0x2f, 0x69, 0x00, 0xf3, 0x4a,
	  0xb7, 0x6b, 0x6f, 0xd0, 0xbe, 0x52, 0x8c, 0xda }
};

const struct hv_guid hv_guid_fcopy = {
	{ 0xe3, 0x4b, 0xd1, 0x34, 0xe4, 0xde, 0xc8, 0x41,
	  0x9a, 0xe7, 0x6b, 0x17, 0x49, 0x77, 0xc1, 0x92 }
};

const struct hv_guid hv_guid_pcie = {
	{ 0x1d, 0xf6, 0xc4, 0x44, 0x44, 0x44, 0x00, 0x44,
	  0x9d, 0x52, 0x80, 0x2e, 0x27, 0xed, 0xe1, 0x9f }
};

const struct hv_guid hv_guid_netdir = {
	{ 0x3d, 0xaf, 0x2e, 0x8c, 0xa7, 0x32, 0x09, 0x4b,
	  0xab, 0x99, 0xbd, 0x1f, 0x1c, 0x86, 0xb5, 0x01 }
};

const struct hv_guid hv_guid_rdesktop = {
	{ 0xf4, 0xac, 0x6a, 0x27, 0x15, 0xac, 0x6c, 0x42,
	  0x98, 0xdd, 0x75, 0x21, 0xad, 0x3f, 0x01, 0xfe }
};

/* Automatic Virtual Machine Activation (AVMA) Services */
const struct hv_guid hv_guid_avma1 = {
	{ 0x55, 0xb2, 0x87, 0x44, 0x8c, 0xb8, 0x3f, 0x40,
	  0xbb, 0x51, 0xd1, 0xf6, 0x9c, 0xf1, 0x7f, 0x87 }
};

const struct hv_guid hv_guid_avma2 = {
	{ 0xf4, 0xba, 0x75, 0x33, 0x15, 0x9e, 0x30, 0x4b,
	  0xb7, 0x65, 0x67, 0xac, 0xb1, 0x0d, 0x60, 0x7b }
};

const struct hv_guid hv_guid_avma3 = {
	{ 0xa0, 0x1f, 0x22, 0x99, 0xad, 0x24, 0xe2, 0x11,
	  0xbe, 0x98, 0x00, 0x1a, 0xa0, 0x1b, 0xbf, 0x6e }
};

const struct hv_guid hv_guid_avma4 = {
	{ 0x16, 0x57, 0xe6, 0xf8, 0xb3, 0x3c, 0x06, 0x4a,
	  0x9a, 0x60, 0x18, 0x89, 0xc5, 0xcc, 0xca, 0xb5 }
};
#endif	/* HYPERV_DEBUG */

int
hv_match(struct device *parent, void *match, void *aux)
{
	struct pv_attach_args *pva = aux;
	struct pvbus_hv *hv = &pva->pva_hv[PVBUS_HYPERV];

	if ((hv->hv_major == 0 && hv->hv_minor == 0) || hv->hv_base == 0)
		return (0);

	return (1);
}

void
hv_attach(struct device *parent, struct device *self, void *aux)
{
	struct hv_softc *sc = (struct hv_softc *)self;
	struct pv_attach_args *pva = aux;
	struct pvbus_hv *hv = &pva->pva_hv[PVBUS_HYPERV];

	sc->sc_pvbus = hv;
	sc->sc_dmat = pva->pva_dmat;

	if (!(hv->hv_features & CPUID_HV_MSR_HYPERCALL) ||
	    !(hv->hv_features & CPUID_HV_MSR_SYNIC)) {
		printf(": not functional\n");
		return;
	}

	DPRINTF("\n");

	hv_set_version(sc);

	if (hv->hv_features & CPUID_HV_MSR_TIME_REFCNT)
		tc_init(&hv_timecounter);

	if (hv_init_hypercall(sc))
		return;

	/* Wire it up to the global */
	hv_sc = sc;

	if (hv_init_interrupts(sc))
		return;

	if (hv_vmbus_connect(sc))
		return;

	DPRINTF("%s", sc->sc_dev.dv_xname);
	printf(": protocol %d.%d, features %#x\n",
	    VMBUS_VERSION_MAJOR(sc->sc_proto),
	    VMBUS_VERSION_MINOR(sc->sc_proto),
	    hv->hv_features);

	if (hv_channel_scan(sc))
		return;

	/* Attach heartbeat, KVP and other "internal" services */
	hv_attach_icdevs(sc);

	/* Attach devices with external drivers */
	hv_attach_devices(sc);
}

void
hv_set_version(struct hv_softc *sc)
{
	uint64_t ver;

	/* OpenBSD build date */
	ver = MSR_HV_GUESTID_OSTYPE_OPENBSD;
	ver |= (uint64_t)OpenBSD << MSR_HV_GUESTID_VERSION_SHIFT;
	wrmsr(MSR_HV_GUEST_OS_ID, ver);
}

u_int
hv_gettime(struct timecounter *tc)
{
	u_int now = rdmsr(MSR_HV_TIME_REF_COUNT);

	return (now);
}

int
hv_init_hypercall(struct hv_softc *sc)
{
	extern void *hv_hypercall_page;
	uint64_t msr;
	paddr_t pa;

	sc->sc_hc = &hv_hypercall_page;

	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_hc, &pa)) {
		printf(": hypercall page PA extraction failed\n");
		return (-1);
	}

	msr = (atop(pa) << MSR_HV_HYPERCALL_PGSHIFT) | MSR_HV_HYPERCALL_ENABLE;
	wrmsr(MSR_HV_HYPERCALL, msr);

	if (!(rdmsr(MSR_HV_HYPERCALL) & MSR_HV_HYPERCALL_ENABLE)) {
		printf(": failed to set up a hypercall page\n");
		return (-1);
	}

	return (0);
}

uint64_t
hv_hypercall(struct hv_softc *sc, uint64_t control, void *input,
    void *output)
{
	paddr_t input_pa = 0, output_pa = 0;
	uint64_t status = 0;

	if (input != NULL &&
	    pmap_extract(pmap_kernel(), (vaddr_t)input, &input_pa) == 0) {
		printf("%s: hypercall input PA extraction failed\n",
		    sc->sc_dev.dv_xname);
		return (~HYPERCALL_STATUS_SUCCESS);
	}

	if (output != NULL &&
	    pmap_extract(pmap_kernel(), (vaddr_t)output, &output_pa) == 0) {
		printf("%s: hypercall output PA extraction failed\n",
		    sc->sc_dev.dv_xname);
		return (~HYPERCALL_STATUS_SUCCESS);
	}

#ifdef __amd64__
	__asm__ __volatile__ ("mov %0, %%r8" : : "r" (output_pa) : "r8");
	__asm__ __volatile__ ("call *%3" : "=a" (status) : "c" (control),
	    "d" (input_pa), "m" (sc->sc_hc));
#else  /* __i386__ */
	{
		uint32_t control_hi = control >> 32;
		uint32_t control_lo = control & 0xfffffffff;
		uint32_t status_hi = 1;
		uint32_t status_lo = 1;

		__asm__ __volatile__ ("call *%8" :
		    "=d" (status_hi), "=a"(status_lo) :
		    "d" (control_hi), "a" (control_lo),
		    "b" (0), "c" (input_pa), "D" (0), "S" (output_pa),
		    "m" (sc->sc_hc));

		status = status_lo | ((uint64_t)status_hi << 32);
	}
#endif	/* __amd64__ */

	return (status);
}

int
hv_init_interrupts(struct hv_softc *sc)
{
	struct cpu_info *ci = curcpu();
	int cpu = CPU_INFO_UNIT(ci);

	sc->sc_idtvec = LAPIC_HYPERV_VECTOR;

	TAILQ_INIT(&sc->sc_reqs);
	mtx_init(&sc->sc_reqlck, IPL_NET);

	TAILQ_INIT(&sc->sc_rsps);
	mtx_init(&sc->sc_rsplck, IPL_NET);

	sc->sc_simp[cpu] = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (sc->sc_simp[cpu] == NULL) {
		printf(": failed to allocate SIMP\n");
		return (-1);
	}

	sc->sc_siep[cpu] = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (sc->sc_siep[cpu] == NULL) {
		printf(": failed to allocate SIEP\n");
		km_free(sc->sc_simp[cpu], PAGE_SIZE, &kv_any, &kp_zero);
		return (-1);
	}

	sc->sc_proto = VMBUS_VERSION_WS2008;

	return (hv_init_synic(sc));
}

int
hv_init_synic(struct hv_softc *sc)
{
	struct cpu_info *ci = curcpu();
	int cpu = CPU_INFO_UNIT(ci);
	uint64_t simp, siefp, sctrl, sint;
	paddr_t pa;

	/*
	 * Setup the Synic's message page
	 */
	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_simp[cpu], &pa)) {
		printf(": SIMP PA extraction failed\n");
		return (-1);
	}
	simp = rdmsr(MSR_HV_SIMP);
	simp &= (1 << MSR_HV_SIMP_PGSHIFT) - 1;
	simp |= (atop(pa) << MSR_HV_SIMP_PGSHIFT);
	simp |= MSR_HV_SIMP_ENABLE;
	wrmsr(MSR_HV_SIMP, simp);

	/*
	 * Setup the Synic's event page
	 */
	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_siep[cpu], &pa)) {
		printf(": SIEP PA extraction failed\n");
		return (-1);
	}
	siefp = rdmsr(MSR_HV_SIEFP);
	siefp &= (1<<MSR_HV_SIEFP_PGSHIFT) - 1;
	siefp |= (atop(pa) << MSR_HV_SIEFP_PGSHIFT);
	siefp |= MSR_HV_SIEFP_ENABLE;
	wrmsr(MSR_HV_SIEFP, siefp);

	/*
	 * Configure and unmask SINT for message and event flags
	 */
	sint = rdmsr(MSR_HV_SINT0 + VMBUS_SINT_MESSAGE);
	sint = sc->sc_idtvec | MSR_HV_SINT_AUTOEOI |
	    (sint & MSR_HV_SINT_RSVD_MASK);
	wrmsr(MSR_HV_SINT0 + VMBUS_SINT_MESSAGE, sint);

	/* Enable the global synic bit */
	sctrl = rdmsr(MSR_HV_SCONTROL);
	sctrl |= MSR_HV_SCTRL_ENABLE;
	wrmsr(MSR_HV_SCONTROL, sctrl);

	sc->sc_vcpus[cpu] = rdmsr(MSR_HV_VP_INDEX);

	DPRINTF("vcpu%u: SIMP %#llx SIEFP %#llx SCTRL %#llx\n",
	    sc->sc_vcpus[cpu], simp, siefp, sctrl);

	return (0);
}

int
hv_cmd(struct hv_softc *sc, void *cmd, size_t cmdlen, void *rsp,
    size_t rsplen, int flags)
{
	struct hv_msg msg;
	int rv;

	if (cmdlen > VMBUS_MSG_DSIZE_MAX) {
		printf("%s: payload too large (%lu)\n", sc->sc_dev.dv_xname,
		    cmdlen);
		return (EMSGSIZE);
	}

	memset(&msg, 0, sizeof(msg));

	msg.msg_req.hc_dsize = cmdlen;
	memcpy(msg.msg_req.hc_data, cmd, cmdlen);

	if (!(flags & HCF_NOREPLY)) {
		msg.msg_rsp = rsp;
		msg.msg_rsplen = rsplen;
	} else
		msg.msg_flags |= MSGF_NOQUEUE;

	if (flags & HCF_NOSLEEP)
		msg.msg_flags |= MSGF_NOSLEEP;

	if ((rv = hv_start(sc, &msg)) != 0)
		return (rv);
	return (hv_reply(sc, &msg));
}

int
hv_start(struct hv_softc *sc, struct hv_msg *msg)
{
	const int delays[] = { 100, 100, 100, 500, 500, 5000, 5000, 5000 };
	const char *wchan = "hvstart";
	uint16_t status;
	int i, s;

	msg->msg_req.hc_connid = VMBUS_CONNID_MESSAGE;
	msg->msg_req.hc_msgtype = 1;

	if (!(msg->msg_flags & MSGF_NOQUEUE)) {
		mtx_enter(&sc->sc_reqlck);
		TAILQ_INSERT_TAIL(&sc->sc_reqs, msg, msg_entry);
		mtx_leave(&sc->sc_reqlck);
	}

	for (i = 0; i < nitems(delays); i++) {
		status = hv_hypercall(sc, HYPERCALL_POST_MESSAGE,
		    &msg->msg_req, NULL);
		if (status == HYPERCALL_STATUS_SUCCESS)
			break;
		if (msg->msg_flags & MSGF_NOSLEEP) {
			delay(delays[i]);
			s = splnet();
			hv_intr();
			splx(s);
		} else
			tsleep(wchan, PRIBIO, wchan, 1);
	}
	if (status != 0) {
		printf("%s: posting vmbus message failed with %d\n",
		    sc->sc_dev.dv_xname, status);
		if (!(msg->msg_flags & MSGF_NOQUEUE)) {
			mtx_enter(&sc->sc_reqlck);
			TAILQ_REMOVE(&sc->sc_reqs, msg, msg_entry);
			mtx_leave(&sc->sc_reqlck);
		}
		return (EIO);
	}

	return (0);
}

static int
hv_reply_done(struct hv_softc *sc, struct hv_msg *msg)
{
	struct hv_msg *m;

	mtx_enter(&sc->sc_rsplck);
	TAILQ_FOREACH(m, &sc->sc_rsps, msg_entry) {
		if (m == msg) {
			mtx_leave(&sc->sc_rsplck);
			return (1);
		}
	}
	mtx_leave(&sc->sc_rsplck);
	return (0);
}

int
hv_reply(struct hv_softc *sc, struct hv_msg *msg)
{
	if (msg->msg_flags & MSGF_NOQUEUE)
		return (0);

	hv_wait(sc, hv_reply_done, msg, msg, "hvreply");

	mtx_enter(&sc->sc_rsplck);
	TAILQ_REMOVE(&sc->sc_rsps, msg, msg_entry);
	mtx_leave(&sc->sc_rsplck);

	return (0);
}

void
hv_wait(struct hv_softc *sc, int (*cond)(struct hv_softc *, struct hv_msg *),
    struct hv_msg *msg, void *wchan, const char *wmsg)
{
	int s;

	KASSERT(cold ? msg->msg_flags & MSGF_NOSLEEP : 1);

	while (!cond(sc, msg)) {
		if (msg->msg_flags & MSGF_NOSLEEP) {
			delay(1000);
			s = splnet();
			hv_intr();
			splx(s);
		} else
			tsleep(wchan, PRIBIO, wmsg ? wmsg : "hvwait", 1);
	}
}

uint16_t
hv_intr_signal(struct hv_softc *sc, void *con)
{
	uint64_t status;

	status = hv_hypercall(sc, HYPERCALL_SIGNAL_EVENT, con, NULL);
	return ((uint16_t)status);
}

void
hv_intr(void)
{
	struct hv_softc *sc = hv_sc;

	hv_event_intr(sc);
	hv_message_intr(sc);
}

void
hv_event_intr(struct hv_softc *sc)
{
	struct vmbus_evtflags *evt;
	struct cpu_info *ci = curcpu();
	int cpu = CPU_INFO_UNIT(ci);
	int bit, row, maxrow, chanid;
	struct hv_channel *ch;
	u_long *revents, pending;

	evt = (struct vmbus_evtflags *)sc->sc_siep[cpu] +
	    VMBUS_SINT_MESSAGE;
	if ((sc->sc_proto == VMBUS_VERSION_WS2008) ||
	    (sc->sc_proto == VMBUS_VERSION_WIN7)) {
		if (!test_bit(0, &evt->evt_flags[0]))
			return;
		clear_bit(0, &evt->evt_flags[0]);
		maxrow = VMBUS_CHAN_MAX_COMPAT / VMBUS_EVTFLAG_LEN;
		/*
		 * receive size is 1/2 page and divide that by 4 bytes
		 */
		revents = sc->sc_revents;
	} else {
		maxrow = nitems(evt->evt_flags);
		/*
		 * On Host with Win8 or above, the event page can be
		 * checked directly to get the id of the channel
		 * that has the pending interrupt.
		 */
		revents = &evt->evt_flags[0];
	}

	for (row = 0; row < maxrow; row++) {
		if (revents[row] == 0)
			continue;
		pending = atomic_swap_ulong(&revents[row], 0);
		for (bit = 0; pending > 0; pending >>= 1, bit++) {
			if ((pending & 1) == 0)
				continue;
			chanid = (row * LONG_BIT) + bit;
			/* vmbus channel protocol message */
			if (chanid == 0)
				continue;
			ch = hv_channel_lookup(sc, chanid);
			if (ch == NULL) {
				printf("%s: unhandled event on %d\n",
				    sc->sc_dev.dv_xname, chanid);
				continue;
			}
			if (ch->ch_state != HV_CHANSTATE_OPENED) {
				printf("%s: channel %d is not active\n",
				    sc->sc_dev.dv_xname, chanid);
				continue;
			}
			ch->ch_evcnt.ec_count++;
			if (ch->ch_handler)
				ch->ch_handler(ch->ch_ctx);
		}
	}
}

void
hv_message_intr(struct hv_softc *sc)
{
	struct vmbus_message *msg;
	struct vmbus_chanmsg_hdr *hdr;
	struct cpu_info *ci = curcpu();
	int cpu = CPU_INFO_UNIT(ci);

	for (;;) {
		msg = (struct vmbus_message *)sc->sc_simp[cpu] +
		    VMBUS_SINT_MESSAGE;
		if (msg->msg_type == VMBUS_MSGTYPE_NONE)
			break;

		hdr = (struct vmbus_chanmsg_hdr *)msg->msg_data;
		if (hdr->chm_type >= VMBUS_CHANMSG_COUNT) {
			printf("%s: unhandled message type %u flags %#x\n",
			    sc->sc_dev.dv_xname, hdr->chm_type,
			    msg->msg_flags);
			goto skip;
		}
		if (hv_msg_dispatch[hdr->chm_type].hmd_handler)
			hv_msg_dispatch[hdr->chm_type].hmd_handler(sc, hdr);
		else
			printf("%s: unhandled message type %u\n",
			    sc->sc_dev.dv_xname, hdr->chm_type);
 skip:
		msg->msg_type = VMBUS_MSGTYPE_NONE;
		virtio_membar_sync();
		if (msg->msg_flags & VMBUS_MSGFLAG_PENDING)
			wrmsr(MSR_HV_EOM, 0);
	}
}

void
hv_channel_response(struct hv_softc *sc, struct vmbus_chanmsg_hdr *rsphdr)
{
	struct hv_msg *msg;
	struct vmbus_chanmsg_hdr *reqhdr;
	int req;

	req = hv_msg_dispatch[rsphdr->chm_type].hmd_request;
	mtx_enter(&sc->sc_reqlck);
	TAILQ_FOREACH(msg, &sc->sc_reqs, msg_entry) {
		reqhdr = (struct vmbus_chanmsg_hdr *)&msg->msg_req.hc_data;
		if (reqhdr->chm_type == req) {
			TAILQ_REMOVE(&sc->sc_reqs, msg, msg_entry);
			break;
		}
	}
	mtx_leave(&sc->sc_reqlck);
	if (msg != NULL) {
		memcpy(msg->msg_rsp, rsphdr, msg->msg_rsplen);
		mtx_enter(&sc->sc_rsplck);
		TAILQ_INSERT_TAIL(&sc->sc_rsps, msg, msg_entry);
		mtx_leave(&sc->sc_rsplck);
		wakeup(msg);
	}
}

void
hv_channel_offer(struct hv_softc *sc, struct vmbus_chanmsg_hdr *hdr)
{
	struct hv_offer *co;

	co = malloc(sizeof(*co), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (co == NULL) {
		printf("%s: failed to allocate an offer object\n",
		    sc->sc_dev.dv_xname);
		return;
	}

	memcpy(&co->co_chan, hdr, sizeof(co->co_chan));

	mtx_enter(&sc->sc_offerlck);
	SIMPLEQ_INSERT_TAIL(&sc->sc_offers, co, co_entry);
	mtx_leave(&sc->sc_offerlck);
}

void
hv_channel_delivered(struct hv_softc *sc, struct vmbus_chanmsg_hdr *hdr)
{
	atomic_setbits_int(&sc->sc_flags, HSF_OFFERS_DELIVERED);
	wakeup(&sc->sc_offers);
}

int
hv_vmbus_connect(struct hv_softc *sc)
{
	const uint32_t versions[] = {
		VMBUS_VERSION_WIN8_1, VMBUS_VERSION_WIN8,
		VMBUS_VERSION_WIN7, VMBUS_VERSION_WS2008
	};
	struct vmbus_chanmsg_connect cmd;
	struct vmbus_chanmsg_connect_resp rsp;
	paddr_t epa, mpa1, mpa2;
	int i;

	sc->sc_events = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (sc->sc_events == NULL) {
		printf(": failed to allocate channel port events page\n");
		goto errout;
	}
	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_events, &epa)) {
		printf(": channel port events page PA extraction failed\n");
		goto errout;
	}

	sc->sc_wevents = (u_long *)sc->sc_events;
	sc->sc_revents = (u_long *)((caddr_t)sc->sc_events + (PAGE_SIZE >> 1));

	sc->sc_monitor[0] = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (sc->sc_monitor[0] == NULL) {
		printf(": failed to allocate monitor page 1\n");
		goto errout;
	}
	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_monitor[0], &mpa1)) {
		printf(": monitor page 1 PA extraction failed\n");
		goto errout;
	}

	sc->sc_monitor[1] = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (sc->sc_monitor[1] == NULL) {
		printf(": failed to allocate monitor page 2\n");
		goto errout;
	}
	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_monitor[1], &mpa2)) {
		printf(": monitor page 2 PA extraction failed\n");
		goto errout;
	}

	memset(&cmd, 0, sizeof(cmd));
	cmd.chm_hdr.chm_type = VMBUS_CHANMSG_CONNECT;
	cmd.chm_evtflags = (uint64_t)epa;
	cmd.chm_mnf1 = (uint64_t)mpa1;
	cmd.chm_mnf2 = (uint64_t)mpa2;

	memset(&rsp, 0, sizeof(rsp));

	for (i = 0; i < nitems(versions); i++) {
		cmd.chm_ver = versions[i];
		if (hv_cmd(sc, &cmd, sizeof(cmd), &rsp, sizeof(rsp),
		    HCF_NOSLEEP)) {
			DPRINTF("%s: CONNECT failed\n",
			    sc->sc_dev.dv_xname);
			goto errout;
		}
		if (rsp.chm_done) {
			sc->sc_flags |= HSF_CONNECTED;
			sc->sc_proto = versions[i];
			sc->sc_handle = VMBUS_GPADL_START;
			break;
		}
	}
	if (i == nitems(versions)) {
		printf("%s: failed to negotiate protocol version\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}

	return (0);

 errout:
	if (sc->sc_events) {
		km_free(sc->sc_events, PAGE_SIZE, &kv_any, &kp_zero);
		sc->sc_events = NULL;
		sc->sc_wevents = NULL;
		sc->sc_revents = NULL;
	}
	if (sc->sc_monitor[0]) {
		km_free(sc->sc_monitor[0], PAGE_SIZE, &kv_any, &kp_zero);
		sc->sc_monitor[0] = NULL;
	}
	if (sc->sc_monitor[1]) {
		km_free(sc->sc_monitor[1], PAGE_SIZE, &kv_any, &kp_zero);
		sc->sc_monitor[1] = NULL;
	}
	return (-1);
}

#ifdef HYPERV_DEBUG
static inline char *
guidprint(struct hv_guid *a)
{
	/* 3     0  5  4 7 6  8 9  10        15 */
	/* 33221100-5544-7766-9988-FFEEDDCCBBAA */
	static char buf[16 * 2 + 4 + 1];
	int i, j = 0;

	for (i = 3; i != -1; i -= 1, j += 2)
		snprintf(&buf[j], 3, "%02x", (uint8_t)a->data[i]);
	buf[j++] = '-';
	for (i = 5; i != 3; i -= 1, j += 2)
		snprintf(&buf[j], 3, "%02x", (uint8_t)a->data[i]);
	buf[j++] = '-';
	for (i = 7; i != 5; i -= 1, j += 2)
		snprintf(&buf[j], 3, "%02x", (uint8_t)a->data[i]);
	buf[j++] = '-';
	for (i = 8; i < 10; i += 1, j += 2)
		snprintf(&buf[j], 3, "%02x", (uint8_t)a->data[i]);
	buf[j++] = '-';
	for (i = 10; i < 16; i += 1, j += 2)
		snprintf(&buf[j], 3, "%02x", (uint8_t)a->data[i]);
	return (&buf[0]);
}
#endif	/* HYPERV_DEBUG */

void
hv_guid_sprint(struct hv_guid *guid, char *str, size_t size)
{
	const struct {
		const struct hv_guid	*guid;
		const char		*ident;
	} map[] = {
		{ &hv_guid_network,	"network" },
		{ &hv_guid_ide,		"ide" },
		{ &hv_guid_scsi,	"scsi" },
		{ &hv_guid_shutdown,	"shutdown" },
		{ &hv_guid_timesync,	"timesync" },
		{ &hv_guid_heartbeat,	"heartbeat" },
		{ &hv_guid_kvp,		"kvp" },
#ifdef HYPERV_DEBUG
		{ &hv_guid_vss,		"vss" },
		{ &hv_guid_dynmem,	"dynamic-memory" },
		{ &hv_guid_mouse,	"mouse" },
		{ &hv_guid_kbd,		"keyboard" },
		{ &hv_guid_video,	"video" },
		{ &hv_guid_fc,		"fiber-channel" },
		{ &hv_guid_fcopy,	"file-copy" },
		{ &hv_guid_pcie,	"pcie-passthrough" },
		{ &hv_guid_netdir,	"network-direct" },
		{ &hv_guid_rdesktop,	"remote-desktop" },
		{ &hv_guid_avma1,	"avma-1" },
		{ &hv_guid_avma2,	"avma-2" },
		{ &hv_guid_avma3,	"avma-3" },
		{ &hv_guid_avma4,	"avma-4" },
#endif
	};
	int i;

	for (i = 0; i < nitems(map); i++) {
		if (memcmp(guid, map[i].guid, sizeof(*guid)) == 0) {
			strlcpy(str, map[i].ident, size);
			return;
		}
	}
#ifdef HYPERV_DEBUG
	strlcpy(str, guidprint(guid), size);
#endif
}

static int
hv_channel_scan_done(struct hv_softc *sc, struct hv_msg *msg __unused)
{
	return (sc->sc_flags & HSF_OFFERS_DELIVERED);
}

int
hv_channel_scan(struct hv_softc *sc)
{
	struct vmbus_chanmsg_hdr hdr;
	struct vmbus_chanmsg_choffer rsp;
	struct hv_offer *co;

	SIMPLEQ_INIT(&sc->sc_offers);
	mtx_init(&sc->sc_offerlck, IPL_NET);

	memset(&hdr, 0, sizeof(hdr));
	hdr.chm_type = VMBUS_CHANMSG_CHREQUEST;

	if (hv_cmd(sc, &hdr, sizeof(hdr), &rsp, sizeof(rsp),
	    HCF_NOSLEEP | HCF_NOREPLY)) {
		DPRINTF("%s: CHREQUEST failed\n", sc->sc_dev.dv_xname);
		return (-1);
	}

	hv_wait(sc, hv_channel_scan_done, (struct hv_msg *)&hdr,
	    &sc->sc_offers, "hvscan");

	TAILQ_INIT(&sc->sc_channels);
	mtx_init(&sc->sc_channelck, IPL_NET);

	mtx_enter(&sc->sc_offerlck);
	while (!SIMPLEQ_EMPTY(&sc->sc_offers)) {
		co = SIMPLEQ_FIRST(&sc->sc_offers);
		SIMPLEQ_REMOVE_HEAD(&sc->sc_offers, co_entry);
		mtx_leave(&sc->sc_offerlck);

		hv_process_offer(sc, co);
		free(co, M_DEVBUF, sizeof(*co));

		mtx_enter(&sc->sc_offerlck);
	}
	mtx_leave(&sc->sc_offerlck);

	return (0);
}

void
hv_process_offer(struct hv_softc *sc, struct hv_offer *co)
{
	struct hv_channel *ch, *nch;

	nch = malloc(sizeof(*nch), M_DEVBUF, M_ZERO | M_NOWAIT);
	if (nch == NULL) {
		printf("%s: failed to allocate memory for the channel\n",
		    sc->sc_dev.dv_xname);
		return;
	}
	nch->ch_sc = sc;
	hv_guid_sprint(&co->co_chan.chm_chtype, nch->ch_ident,
	    sizeof(nch->ch_ident));

	/*
	 * By default we setup state to enable batched reading.
	 * A specific service can choose to disable this prior
	 * to opening the channel.
	 */
	nch->ch_flags |= CHF_BATCHED;

	KASSERT((((vaddr_t)&nch->ch_monprm) & 0x7) == 0);
	memset(&nch->ch_monprm, 0, sizeof(nch->ch_monprm));
	nch->ch_monprm.mp_connid = VMBUS_CONNID_EVENT;

	if (sc->sc_proto != VMBUS_VERSION_WS2008)
		nch->ch_monprm.mp_connid = co->co_chan.chm_connid;

	if (co->co_chan.chm_flags1 & VMBUS_CHOFFER_FLAG1_HASMNF) {
		nch->ch_mgroup = co->co_chan.chm_montrig / VMBUS_MONTRIG_LEN;
		nch->ch_mindex = co->co_chan.chm_montrig % VMBUS_MONTRIG_LEN;
		nch->ch_flags |= CHF_MONITOR;
	}

	nch->ch_id = co->co_chan.chm_chanid;

	memcpy(&nch->ch_type, &co->co_chan.chm_chtype, sizeof(ch->ch_type));
	memcpy(&nch->ch_inst, &co->co_chan.chm_chinst, sizeof(ch->ch_inst));

	mtx_enter(&sc->sc_channelck);
	TAILQ_FOREACH(ch, &sc->sc_channels, ch_entry) {
		if (!memcmp(&ch->ch_type, &nch->ch_type, sizeof(ch->ch_type)) &&
		    !memcmp(&ch->ch_inst, &nch->ch_inst, sizeof(ch->ch_inst)))
			break;
	}
	if (ch != NULL) {
		if (co->co_chan.chm_subidx == 0) {
			printf("%s: unknown offer \"%s\"\n",
			    sc->sc_dev.dv_xname, nch->ch_ident);
			mtx_leave(&sc->sc_channelck);
			free(nch, M_DEVBUF, sizeof(*nch));
			return;
		}
#ifdef HYPERV_DEBUG
		printf("%s: subchannel %u for \"%s\"\n", sc->sc_dev.dv_xname,
		    co->co_chan.chm_subidx, ch->ch_ident);
#endif
		mtx_leave(&sc->sc_channelck);
		free(nch, M_DEVBUF, sizeof(*nch));
		return;
	}

	nch->ch_state = HV_CHANSTATE_OFFERED;

	TAILQ_INSERT_TAIL(&sc->sc_channels, nch, ch_entry);
	mtx_leave(&sc->sc_channelck);

#ifdef HYPERV_DEBUG
	printf("%s: channel %u: \"%s\"", sc->sc_dev.dv_xname, nch->ch_id,
	    nch->ch_ident);
	if (nch->ch_flags & CHF_MONITOR)
		printf(", monitor %u\n", co->co_chan.chm_montrig);
	else
		printf("\n");
#endif
}

struct hv_channel *
hv_channel_lookup(struct hv_softc *sc, uint32_t relid)
{
	struct hv_channel *ch;

	TAILQ_FOREACH(ch, &sc->sc_channels, ch_entry) {
		if (ch->ch_id == relid)
			return (ch);
	}
	return (NULL);
}

int
hv_channel_ring_create(struct hv_channel *ch, uint32_t buflen)
{
	struct hv_softc *sc = ch->ch_sc;

	buflen = roundup(buflen, PAGE_SIZE) + sizeof(struct vmbus_bufring);
	ch->ch_ring = km_alloc(2 * buflen, &kv_any, &kp_zero, cold ?
	    &kd_nowait : &kd_waitok);
	if (ch->ch_ring == NULL) {
		printf("%s: failed to allocate channel ring\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}
	ch->ch_ring_size = 2 * buflen;

	memset(&ch->ch_wrd, 0, sizeof(ch->ch_wrd));
	ch->ch_wrd.rd_ring = (struct vmbus_bufring *)ch->ch_ring;
	ch->ch_wrd.rd_size = buflen;
	ch->ch_wrd.rd_dsize = buflen - sizeof(struct vmbus_bufring);
	mtx_init(&ch->ch_wrd.rd_lock, IPL_NET);

	memset(&ch->ch_rrd, 0, sizeof(ch->ch_rrd));
	ch->ch_rrd.rd_ring = (struct vmbus_bufring *)((uint8_t *)ch->ch_ring +
	    buflen);
	ch->ch_rrd.rd_size = buflen;
	ch->ch_rrd.rd_dsize = buflen - sizeof(struct vmbus_bufring);
	mtx_init(&ch->ch_rrd.rd_lock, IPL_NET);

	if (hv_handle_alloc(ch, ch->ch_ring, 2 * buflen, &ch->ch_ring_gpadl)) {
		printf("%s: failed to obtain a PA handle for the ring\n",
		    sc->sc_dev.dv_xname);
		hv_channel_ring_destroy(ch);
		return (-1);
	}

	return (0);
}

void
hv_channel_ring_destroy(struct hv_channel *ch)
{
	km_free(ch->ch_ring, ch->ch_ring_size, &kv_any, &kp_zero);
	ch->ch_ring = NULL;
	hv_handle_free(ch, ch->ch_ring_gpadl);

	memset(&ch->ch_wrd, 0, sizeof(ch->ch_wrd));
	memset(&ch->ch_rrd, 0, sizeof(ch->ch_rrd));
}

int
hv_channel_open(struct hv_channel *ch, size_t buflen, void *udata,
    size_t udatalen, void (*handler)(void *), void *arg)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanmsg_chopen cmd;
	struct vmbus_chanmsg_chopen_resp rsp;
	int rv;

	if (ch->ch_ring == NULL &&
	    hv_channel_ring_create(ch, buflen)) {
		DPRINTF("%s: failed to create channel ring\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}

	memset(&cmd, 0, sizeof(cmd));
	cmd.chm_hdr.chm_type = VMBUS_CHANMSG_CHOPEN;
	cmd.chm_openid = ch->ch_id;
	cmd.chm_chanid = ch->ch_id;
	cmd.chm_gpadl = ch->ch_ring_gpadl;
	cmd.chm_txbr_pgcnt = ch->ch_wrd.rd_size >> PAGE_SHIFT;
	cmd.chm_vcpuid = ch->ch_vcpu;

	if (udata && udatalen > 0)
		memcpy(cmd.chm_udata, udata, udatalen);

	memset(&rsp, 0, sizeof(rsp));

	ch->ch_handler = handler;
	ch->ch_ctx = arg;

	ch->ch_state = HV_CHANSTATE_OPENED;

	rv = hv_cmd(sc, &cmd, sizeof(cmd), &rsp, sizeof(rsp),
	    cold ? HCF_NOSLEEP : HCF_SLEEPOK);
	if (rv) {
		hv_channel_ring_destroy(ch);
		DPRINTF("%s: CHOPEN failed with %d\n",
		    sc->sc_dev.dv_xname, rv);
		ch->ch_handler = NULL;
		ch->ch_ctx = NULL;
		ch->ch_state = HV_CHANSTATE_OFFERED;
		return (-1);
	}

	return (0);
}

int
hv_channel_close(struct hv_channel *ch)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanmsg_chclose cmd;
	int rv;

	memset(&cmd, 0, sizeof(cmd));
	cmd.chm_hdr.chm_type = VMBUS_CHANMSG_CHCLOSE;
	cmd.chm_chanid = ch->ch_id;

	ch->ch_state = HV_CHANSTATE_CLOSING;
	rv = hv_cmd(sc, &cmd, sizeof(cmd), NULL, 0, HCF_NOREPLY);
	if (rv) {
		DPRINTF("%s: CHCLOSE failed with %d\n",
		    sc->sc_dev.dv_xname, rv);
		return (-1);
	}
	ch->ch_state = HV_CHANSTATE_CLOSED;
	hv_channel_ring_destroy(ch);
	return (0);
}

static inline void
hv_channel_setevent(struct hv_softc *sc, struct hv_channel *ch)
{
	struct vmbus_mon_trig *mtg;

	/* Each uint32_t represents 32 channels */
	set_bit(ch->ch_id, sc->sc_wevents);
	if (ch->ch_flags & CHF_MONITOR) {
		mtg = &sc->sc_monitor[1]->mnf_trigs[ch->ch_mgroup];
		set_bit(ch->ch_mindex, &mtg->mt_pending);
	} else
		hv_intr_signal(sc, &ch->ch_monprm);
}

static inline void
hv_ring_put(struct hv_ring_data *wrd, uint8_t *data, uint32_t datalen)
{
	int left = MIN(datalen, wrd->rd_dsize - wrd->rd_prod);

	memcpy(&wrd->rd_ring->br_data[wrd->rd_prod], data, left);
	memcpy(&wrd->rd_ring->br_data[0], data + left, datalen - left);
	wrd->rd_prod += datalen;
	if (wrd->rd_prod >= wrd->rd_dsize)
		wrd->rd_prod -= wrd->rd_dsize;
}

static inline void
hv_ring_get(struct hv_ring_data *rrd, uint8_t *data, uint32_t datalen,
    int peek)
{
	int left = MIN(datalen, rrd->rd_dsize - rrd->rd_cons);

	memcpy(data, &rrd->rd_ring->br_data[rrd->rd_cons], left);
	memcpy(data + left, &rrd->rd_ring->br_data[0], datalen - left);
	if (!peek) {
		rrd->rd_cons += datalen;
		if (rrd->rd_cons >= rrd->rd_dsize)
			rrd->rd_cons -= rrd->rd_dsize;
	}
}

static inline void
hv_ring_avail(struct hv_ring_data *rd, uint32_t *towrite, uint32_t *toread)
{
	uint32_t ridx = rd->rd_ring->br_rindex;
	uint32_t widx = rd->rd_ring->br_windex;
	uint32_t r, w;

	if (widx >= ridx)
		w = rd->rd_dsize - (widx - ridx);
	else
		w = ridx - widx;
	r = rd->rd_dsize - w;
	if (towrite)
		*towrite = w;
	if (toread)
		*toread = r;
}

int
hv_ring_write(struct hv_ring_data *wrd, struct iovec *iov, int iov_cnt,
    int *needsig)
{
	uint64_t indices = 0;
	uint32_t avail, oprod, datalen = sizeof(indices);
	int i;

	for (i = 0; i < iov_cnt; i++)
		datalen += iov[i].iov_len;

	KASSERT(datalen <= wrd->rd_dsize);

	hv_ring_avail(wrd, &avail, NULL);
	if (avail <= datalen) {
		DPRINTF("%s: avail %u datalen %u\n", __func__, avail, datalen);
		return (EAGAIN);
	}

	oprod = wrd->rd_prod;

	for (i = 0; i < iov_cnt; i++)
		hv_ring_put(wrd, iov[i].iov_base, iov[i].iov_len);

	indices = (uint64_t)oprod << 32;
	hv_ring_put(wrd, (uint8_t *)&indices, sizeof(indices));

	virtio_membar_sync();
	wrd->rd_ring->br_windex = wrd->rd_prod;
	virtio_membar_sync();

	/* Signal when the ring transitions from being empty to non-empty */
	if (wrd->rd_ring->br_imask == 0 &&
	    wrd->rd_ring->br_rindex == oprod)
		*needsig = 1;
	else
		*needsig = 0;

	return (0);
}

int
hv_channel_send(struct hv_channel *ch, void *data, uint32_t datalen,
    uint64_t rid, int type, uint32_t flags)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanpkt cp;
	struct iovec iov[3];
	uint32_t pktlen, pktlen_aligned;
	uint64_t zeropad = 0;
	int rv, needsig = 0;

	pktlen = sizeof(cp) + datalen;
	pktlen_aligned = roundup(pktlen, sizeof(uint64_t));

	cp.cp_hdr.cph_type = type;
	cp.cp_hdr.cph_flags = flags;
	VMBUS_CHANPKT_SETLEN(cp.cp_hdr.cph_hlen, sizeof(cp));
	VMBUS_CHANPKT_SETLEN(cp.cp_hdr.cph_tlen, pktlen_aligned);
	cp.cp_hdr.cph_tid = rid;

	iov[0].iov_base = &cp;
	iov[0].iov_len = sizeof(cp);

	iov[1].iov_base = data;
	iov[1].iov_len = datalen;

	iov[2].iov_base = &zeropad;
	iov[2].iov_len = pktlen_aligned - pktlen;

	mtx_enter(&ch->ch_wrd.rd_lock);
	rv = hv_ring_write(&ch->ch_wrd, iov, 3, &needsig);
	mtx_leave(&ch->ch_wrd.rd_lock);
	if (rv == 0 && needsig)
		hv_channel_setevent(sc, ch);

	return (rv);
}

int
hv_channel_send_sgl(struct hv_channel *ch, struct vmbus_gpa *sgl,
    uint32_t nsge, void *data, uint32_t datalen, uint64_t rid)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanpkt_sglist cp;
	struct iovec iov[4];
	uint32_t buflen, pktlen, pktlen_aligned;
	uint64_t zeropad = 0;
	int rv, needsig = 0;

	buflen = sizeof(struct vmbus_gpa) * nsge;
	pktlen = sizeof(cp) + datalen + buflen;
	pktlen_aligned = roundup(pktlen, sizeof(uint64_t));

	cp.cp_hdr.cph_type = VMBUS_CHANPKT_TYPE_GPA;
	cp.cp_hdr.cph_flags = VMBUS_CHANPKT_FLAG_RC;
	VMBUS_CHANPKT_SETLEN(cp.cp_hdr.cph_hlen, sizeof(cp) + buflen);
	VMBUS_CHANPKT_SETLEN(cp.cp_hdr.cph_tlen, pktlen_aligned);
	cp.cp_hdr.cph_tid = rid;
	cp.cp_gpa_cnt = nsge;
	cp.cp_rsvd = 0;

	iov[0].iov_base = &cp;
	iov[0].iov_len = sizeof(cp);

	iov[1].iov_base = sgl;
	iov[1].iov_len = buflen;

	iov[2].iov_base = data;
	iov[2].iov_len = datalen;

	iov[3].iov_base = &zeropad;
	iov[3].iov_len = pktlen_aligned - pktlen;

	mtx_enter(&ch->ch_wrd.rd_lock);
	rv = hv_ring_write(&ch->ch_wrd, iov, 4, &needsig);
	mtx_leave(&ch->ch_wrd.rd_lock);
	if (rv == 0 && needsig)
		hv_channel_setevent(sc, ch);

	return (rv);
}

int
hv_channel_send_prpl(struct hv_channel *ch, struct vmbus_gpa_range *prpl,
    uint32_t nprp, void *data, uint32_t datalen, uint64_t rid)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanpkt_prplist cp;
	struct iovec iov[4];
	uint32_t buflen, pktlen, pktlen_aligned;
	uint64_t zeropad = 0;
	int rv, needsig = 0;

	buflen = sizeof(struct vmbus_gpa_range) * (nprp + 1);
	pktlen = sizeof(cp) + datalen + buflen;
	pktlen_aligned = roundup(pktlen, sizeof(uint64_t));

	cp.cp_hdr.cph_type = VMBUS_CHANPKT_TYPE_GPA;
	cp.cp_hdr.cph_flags = VMBUS_CHANPKT_FLAG_RC;
	VMBUS_CHANPKT_SETLEN(cp.cp_hdr.cph_hlen, sizeof(cp) + buflen);
	VMBUS_CHANPKT_SETLEN(cp.cp_hdr.cph_tlen, pktlen_aligned);
	cp.cp_hdr.cph_tid = rid;
	cp.cp_range_cnt = 1;
	cp.cp_rsvd = 0;

	iov[0].iov_base = &cp;
	iov[0].iov_len = sizeof(cp);

	iov[1].iov_base = prpl;
	iov[1].iov_len = buflen;

	iov[2].iov_base = data;
	iov[2].iov_len = datalen;

	iov[3].iov_base = &zeropad;
	iov[3].iov_len = pktlen_aligned - pktlen;

	mtx_enter(&ch->ch_wrd.rd_lock);
	rv = hv_ring_write(&ch->ch_wrd, iov, 4, &needsig);
	mtx_leave(&ch->ch_wrd.rd_lock);
	if (rv == 0 && needsig)
		hv_channel_setevent(sc, ch);

	return (rv);
}

int
hv_ring_peek(struct hv_ring_data *rrd, void *data, uint32_t datalen)
{
	uint32_t avail;

	KASSERT(datalen <= rrd->rd_dsize);

	hv_ring_avail(rrd, NULL, &avail);
	if (avail < datalen)
		return (EAGAIN);

	hv_ring_get(rrd, (uint8_t *)data, datalen, 1);
	return (0);
}

int
hv_ring_read(struct hv_ring_data *rrd, void *data, uint32_t datalen,
    uint32_t offset)
{
	uint64_t indices;
	uint32_t avail;

	KASSERT(datalen <= rrd->rd_dsize);

	hv_ring_avail(rrd, NULL, &avail);
	if (avail < datalen) {
		DPRINTF("%s: avail %u datalen %u\n", __func__, avail, datalen);
		return (EAGAIN);
	}

	if (offset) {
		rrd->rd_cons += offset;
		if (rrd->rd_cons >= rrd->rd_dsize)
			rrd->rd_cons -= rrd->rd_dsize;
	}

	hv_ring_get(rrd, (uint8_t *)data, datalen, 0);
	hv_ring_get(rrd, (uint8_t *)&indices, sizeof(indices), 0);

	virtio_membar_sync();
	rrd->rd_ring->br_rindex = rrd->rd_cons;

	return (0);
}

int
hv_channel_recv(struct hv_channel *ch, void *data, uint32_t datalen,
    uint32_t *rlen, uint64_t *rid, int raw)
{
	struct vmbus_chanpkt_hdr cph;
	uint32_t offset, pktlen;
	int rv;

	*rlen = 0;

	mtx_enter(&ch->ch_rrd.rd_lock);

	if ((rv = hv_ring_peek(&ch->ch_rrd, &cph, sizeof(cph))) != 0) {
		mtx_leave(&ch->ch_rrd.rd_lock);
		return (rv);
	}

	offset = raw ? 0 : VMBUS_CHANPKT_GETLEN(cph.cph_hlen);
	pktlen = VMBUS_CHANPKT_GETLEN(cph.cph_tlen) - offset;
	if (pktlen > datalen) {
		mtx_leave(&ch->ch_rrd.rd_lock);
		printf("%s: pktlen %u datalen %u\n", __func__, pktlen, datalen);
		return (EINVAL);
	}

	rv = hv_ring_read(&ch->ch_rrd, data, pktlen, offset);
	if (rv == 0) {
		*rlen = pktlen;
		*rid = cph.cph_tid;
	}

	mtx_leave(&ch->ch_rrd.rd_lock);

	return (rv);
}

/* How many PFNs can be referenced by the header */
#define HV_NPFNHDR	((VMBUS_MSG_DSIZE_MAX -	\
	  sizeof(struct vmbus_chanmsg_gpadl_conn)) / sizeof(uint64_t))

/* How many PFNs can be referenced by the body */
#define HV_NPFNBODY	((VMBUS_MSG_DSIZE_MAX -	\
	  sizeof(struct vmbus_chanmsg_gpadl_subconn)) / sizeof(uint64_t))

int
hv_handle_alloc(struct hv_channel *ch, void *buffer, uint32_t buflen,
    uint32_t *handle)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanmsg_gpadl_conn *hdr;
	struct vmbus_chanmsg_gpadl_subconn *cmd;
	struct vmbus_chanmsg_gpadl_connresp rsp;
	struct hv_msg *msg;
	int i, j, last, left, rv;
	int bodylen = 0, ncmds = 0, pfn = 0;
	int waitflag = cold ? M_NOWAIT : M_WAITOK;
	uint64_t *frames;
	paddr_t pa;
	caddr_t body;
	/* Total number of pages to reference */
	int total = atop(buflen);
	/* Number of pages that will fit the header */
	int inhdr = MIN(total, HV_NPFNHDR);

	KASSERT((buflen & (PAGE_SIZE - 1)) == 0);

	if ((msg = malloc(sizeof(*msg), M_DEVBUF, M_ZERO | waitflag)) == NULL)
		return (ENOMEM);

	/* Prepare array of frame addresses */
	if ((frames = mallocarray(total, sizeof(*frames), M_DEVBUF, M_ZERO |
	    waitflag)) == NULL) {
		free(msg, M_DEVBUF, sizeof(*msg));
		return (ENOMEM);
	}
	for (i = 0; i < total; i++) {
		if (!pmap_extract(pmap_kernel(), (vaddr_t)buffer +
		    PAGE_SIZE * i, &pa)) {
			free(msg, M_DEVBUF, sizeof(*msg));
			free(frames, M_DEVBUF, total * sizeof(*frames));
			return (EFAULT);
		}
		frames[i] = atop(pa);
	}

	msg->msg_req.hc_dsize = sizeof(struct vmbus_chanmsg_gpadl_conn) +
	    inhdr * sizeof(uint64_t);
	hdr = (struct vmbus_chanmsg_gpadl_conn *)msg->msg_req.hc_data;
	msg->msg_rsp = &rsp;
	msg->msg_rsplen = sizeof(rsp);
	if (waitflag == M_NOWAIT)
		msg->msg_flags = MSGF_NOSLEEP;

	left = total - inhdr;

	/* Allocate additional gpadl_body structures if required */
	if (left > 0) {
		ncmds = MAX(1, left / HV_NPFNBODY + left % HV_NPFNBODY);
		bodylen = ncmds * VMBUS_MSG_DSIZE_MAX;
		body = malloc(bodylen, M_DEVBUF, M_ZERO | waitflag);
		if (body == NULL) {
			free(msg, M_DEVBUF, sizeof(*msg));
			free(frames, M_DEVBUF, atop(buflen) * sizeof(*frames));
			return (ENOMEM);
		}
	}

	*handle = atomic_inc_int_nv(&sc->sc_handle);

	hdr->chm_hdr.chm_type = VMBUS_CHANMSG_GPADL_CONN;
	hdr->chm_chanid = ch->ch_id;
	hdr->chm_gpadl = *handle;

	/* Single range for a contiguous buffer */
	hdr->chm_range_cnt = 1;
	hdr->chm_range_len = sizeof(struct vmbus_gpa_range) + total *
	    sizeof(uint64_t);
	hdr->chm_range.gpa_ofs = 0;
	hdr->chm_range.gpa_len = buflen;

	/* Fit as many pages as possible into the header */
	for (i = 0; i < inhdr; i++)
		hdr->chm_range.gpa_page[i] = frames[pfn++];

	for (i = 0; i < ncmds; i++) {
		cmd = (struct vmbus_chanmsg_gpadl_subconn *)(body +
		    VMBUS_MSG_DSIZE_MAX * i);
		cmd->chm_hdr.chm_type = VMBUS_CHANMSG_GPADL_SUBCONN;
		cmd->chm_gpadl = *handle;
		last = MIN(left, HV_NPFNBODY);
		for (j = 0; j < last; j++)
			cmd->chm_gpa_page[j] = frames[pfn++];
		left -= last;
	}

	rv = hv_start(sc, msg);
	if (rv != 0) {
		DPRINTF("%s: GPADL_CONN failed\n", sc->sc_dev.dv_xname);
		goto out;
	}
	for (i = 0; i < ncmds; i++) {
		int cmdlen = sizeof(*cmd);
		cmd = (struct vmbus_chanmsg_gpadl_subconn *)(body +
		    VMBUS_MSG_DSIZE_MAX * i);
		/* Last element can be short */
		if (i == ncmds - 1)
			cmdlen += last * sizeof(uint64_t);
		else
			cmdlen += HV_NPFNBODY * sizeof(uint64_t);
		rv = hv_cmd(sc, cmd, cmdlen, NULL, 0, waitflag | HCF_NOREPLY);
		if (rv != 0) {
			DPRINTF("%s: GPADL_SUBCONN (iteration %d/%d) failed "
			    "with %d\n", sc->sc_dev.dv_xname, i, ncmds, rv);
			goto out;
		}
	}
	rv = hv_reply(sc, msg);
	if (rv != 0)
		DPRINTF("%s: GPADL allocation failed with %d\n",
		    sc->sc_dev.dv_xname, rv);

 out:
	free(msg, M_DEVBUF, sizeof(*msg));
	free(frames, M_DEVBUF, total * sizeof(*frames));
	if (bodylen > 0)
		free(body, M_DEVBUF, bodylen);
	if (rv != 0)
		return (rv);

	KASSERT(*handle == rsp.chm_gpadl);

	return (0);
}

void
hv_handle_free(struct hv_channel *ch, uint32_t handle)
{
	struct hv_softc *sc = ch->ch_sc;
	struct vmbus_chanmsg_gpadl_disconn cmd;
	struct vmbus_chanmsg_gpadl_disconn rsp;
	int rv;

	memset(&cmd, 0, sizeof(cmd));
	cmd.chm_hdr.chm_type = VMBUS_CHANMSG_GPADL_DISCONN;
	cmd.chm_chanid = ch->ch_id;
	cmd.chm_gpadl = handle;

	rv = hv_cmd(sc, &cmd, sizeof(cmd), &rsp, sizeof(rsp), cold ?
	    HCF_NOSLEEP : 0);
	if (rv)
		DPRINTF("%s: GPADL_DISCONN failed with %d\n",
		    sc->sc_dev.dv_xname, rv);
}

static int
hv_attach_print(void *aux, const char *name)
{
	struct hv_attach_args *aa = aux;

	if (name)
		printf("\"%s\" at %s", aa->aa_ident, name);

	return (UNCONF);
}

int
hv_attach_devices(struct hv_softc *sc)
{
	struct hv_dev *dv;
	struct hv_channel *ch;

	SLIST_INIT(&sc->sc_devs);
	mtx_init(&sc->sc_devlck, IPL_NET);

	TAILQ_FOREACH(ch, &sc->sc_channels, ch_entry) {
		if (ch->ch_state != HV_CHANSTATE_OFFERED)
			continue;
		if (!(ch->ch_flags & CHF_MONITOR))
			continue;
		dv = malloc(sizeof(*dv), M_DEVBUF, M_ZERO | M_NOWAIT);
		if (dv == NULL) {
			printf("%s: failed to allocate device object\n",
			    sc->sc_dev.dv_xname);
			return (-1);
		}
		dv->dv_aa.aa_parent = sc;
		dv->dv_aa.aa_type = &ch->ch_type;
		dv->dv_aa.aa_inst = &ch->ch_inst;
		dv->dv_aa.aa_ident = ch->ch_ident;
		dv->dv_aa.aa_chan = ch;
		dv->dv_aa.aa_dmat = sc->sc_dmat;
		mtx_enter(&sc->sc_devlck);
		SLIST_INSERT_HEAD(&sc->sc_devs, dv, dv_entry);
		mtx_leave(&sc->sc_devlck);
		config_found((struct device *)sc, &dv->dv_aa, hv_attach_print);
	}
	return (0);
}

void
hv_evcount_attach(struct hv_channel *ch, const char *name)
{
	struct hv_softc *sc = ch->ch_sc;

	evcount_attach(&ch->ch_evcnt, name, &sc->sc_idtvec);
}
@


1.38
log
@Don't defer attaching PV devices

Now that both hvn(4) and hvs(4) can perform VMBus channel operations
during autoconf, it's no longer necessary to defer their attachment.
@
text
@d733 1
a733 1
		membar_sync();
d1300 1
a1300 1
	membar_sync();
d1302 1
a1302 1
	membar_sync();
d1479 1
a1479 1
	membar_sync();
@


1.37
log
@Make sure we can free GPA handles when cold
@
text
@a81 1
void	hv_deferred(void *);
d320 2
a321 10
	startuphook_establish(hv_deferred, sc);
}

void
hv_deferred(void *arg)
{
	struct hv_softc *sc = arg;

	if (hv_attach_devices(sc))
		return;
@


1.36
log
@Add another type of a scatter-gather list operation used by StorVSC

Obtained from FreeBSD.
@
text
@d1681 2
a1682 1
	rv = hv_cmd(sc, &cmd, sizeof(cmd), &rsp, sizeof(rsp), 0);
@


1.35
log
@Make sure we're producing a 64-bit value when shifting left

Reported by brad and kettenis separately; Mark has even sent me a diff
and told to commit it with OK kettenis.
@
text
@d1406 44
@


1.34
log
@Microsoft has allocated us a SubID that we can use to identify OpenBSD
@
text
@d340 1
a340 1
	ver |= OpenBSD << MSR_HV_GUESTID_VERSION_SHIFT;
@


1.33
log
@Don't let producer index become equal with the consumer when writing

A nod to the re.c commit by dlg@@, perhaps pure paranoia, but works
nevertheless.  While here replace a more expensive modulo operation
with a subtraction.
@
text
@d338 3
a340 3
	/* OpenBSD 6.0 */
	ver = 0x8300ULL << 48;
	ver |= 6 << 16;
@


1.32
log
@Move as much code out of a startup hook as possible

To be able to perform asynchronous operations early we need to
have a robust polling mechanism with flexible condition check
semantics.  A new hv_wait interface provides such facilities.

Right now hvn(4) submits NVS commands and sleeps waiting for a
interrupt to fire and run the completion code and it will take
some effort to convert this code to be able to poll instead of
tsleep.  But VMBus attachment, channel scanning, etc can done
at an earlier stage.  Lets see if this sticks.
@
text
@d1245 2
a1246 1
	wrd->rd_prod %= wrd->rd_dsize;
d1259 2
a1260 1
		rrd->rd_cons %= rrd->rd_dsize;
d1296 1
a1296 1
	if (avail < datalen) {
d1437 2
a1438 1
		rrd->rd_cons %= rrd->rd_dsize;
@


1.31
log
@M_*WAIT* flags cannot be treated as boolean values
@
text
@d92 2
d290 1
a290 1
	printf("\n");
d306 15
a328 6
	if (hv_vmbus_connect(sc))
		return;

	if (hv_channel_scan(sc))
		return;

d585 16
a603 4
	const char *wchan = "hvreply";
	struct hv_msg *m, *tmp;
	int i, s;

d607 18
a624 11
	for (i = 0; i < 1000; i++) {
		mtx_enter(&sc->sc_rsplck);
		TAILQ_FOREACH_SAFE(m, &sc->sc_rsps, msg_entry, tmp) {
			if (m == msg) {
				TAILQ_REMOVE(&sc->sc_rsps, m, msg_entry);
				break;
			}
		}
		mtx_leave(&sc->sc_rsplck);
		if (m != NULL)
			return (0);
d626 1
a626 1
			delay(100000);
d630 2
a631 5
		} else {
			s = tsleep(&msg, PRIBIO | PCATCH, wchan, 1);
			if (s != EWOULDBLOCK)
				return (EINTR);
		}
a632 9
	mtx_enter(&sc->sc_rsplck);
	TAILQ_FOREACH_SAFE(m, &sc->sc_reqs, msg_entry, tmp) {
		if (m == msg) {
			TAILQ_REMOVE(&sc->sc_reqs, m, msg_entry);
			break;
		}
	}
	mtx_leave(&sc->sc_rsplck);
	return (ETIMEDOUT);
d751 1
a751 1
	struct hv_msg *msg, *tmp;
d757 1
a757 1
	TAILQ_FOREACH_SAFE(msg, &sc->sc_reqs, msg_entry, tmp) {
a864 2
			DPRINTF("%s: protocol version %#x\n",
			    sc->sc_dev.dv_xname, versions[i]);
d965 6
d984 2
a985 1
	if (hv_cmd(sc, &hdr, sizeof(hdr), &rsp, sizeof(rsp), HCF_NOREPLY)) {
d990 2
a991 2
	while ((sc->sc_flags & HSF_OFFERS_DELIVERED) == 0)
		tsleep(&sc->sc_offers, PRIBIO, "hvoffers", 1);
d1162 2
a1163 1
		DPRINTF(": failed to create channel ring\n");
d1185 2
a1186 1
	rv = hv_cmd(sc, &cmd, sizeof(cmd), &rsp, sizeof(rsp), 0);
a1658 3

	/* Attach heartbeat, KVP and other "internal" services */
	hv_attach_icdevs(sc);
@


1.30
log
@Disable debugging output
@
text
@d1477 1
a1477 1
	int waitok = cold ? M_NOWAIT : M_WAITOK;
d1488 1
a1488 1
	if ((msg = malloc(sizeof(*msg), M_DEVBUF, M_ZERO | waitok)) == NULL)
d1493 1
a1493 1
	    waitok)) == NULL) {
d1512 1
a1512 1
	if (!waitok)
d1521 1
a1521 1
		body = malloc(bodylen, M_DEVBUF, M_ZERO | waitok);
d1571 1
a1571 1
		rv = hv_cmd(sc, cmd, cmdlen, NULL, 0, waitok | HCF_NOREPLY);
@


1.29
log
@Use CPUID flags to determine working components

This adds a few checks to make sure we're not trying to use features
that are not advertised with CPUID feature flags and avoid attaching
when Xen viridium emulation is turned on.

Prompted by a report from Kirill Miazine <km at krot ! org>, thanks!
Discussed with reyk@@.
@
text
@d1270 1
a1270 1
		printf("%s: avail %u datalen %u\n", __func__, avail, datalen);
d1404 1
a1404 1
		printf("%s: avail %u datalen %u\n", __func__, avail, datalen);
@


1.28
log
@Minor cleanup, remove unused structure members
@
text
@d266 1
a266 1
	if (hv->hv_base == 0)
d282 6
d292 2
a293 1
	tc_init(&hv_timecounter);
@


1.27
log
@Set the reserved field to zero
@
text
@a1085 1
	ch->ch_ring_npg = ch->ch_ring_size >> PAGE_SHIFT;
d1090 1
a1090 1
	ch->ch_wrd.rd_data_size = buflen - sizeof(struct vmbus_bufring);
d1097 1
a1097 1
	ch->ch_rrd.rd_data_size = buflen - sizeof(struct vmbus_bufring);
d1113 1
a1113 2
	km_free(ch->ch_ring, ch->ch_wrd.rd_size + ch->ch_rrd.rd_size,
	    &kv_any, &kp_zero);
d1208 1
a1208 1
	int left = MIN(datalen, wrd->rd_data_size - wrd->rd_prod);
d1213 1
a1213 1
	wrd->rd_prod %= wrd->rd_data_size;
d1220 1
a1220 1
	int left = MIN(datalen, rrd->rd_data_size - rrd->rd_cons);
d1226 1
a1226 1
		rrd->rd_cons %= rrd->rd_data_size;
d1238 1
a1238 1
		w = rd->rd_data_size - (widx - ridx);
d1241 1
a1241 1
	r = rd->rd_data_size - w;
d1259 1
a1259 1
	KASSERT(datalen <= wrd->rd_data_size);
d1376 1
a1376 1
	KASSERT(datalen <= rrd->rd_data_size);
d1393 1
a1393 1
	KASSERT(datalen <= rrd->rd_data_size);
d1403 1
a1403 1
		rrd->rd_cons %= rrd->rd_data_size;
@


1.26
log
@Reserve extra space for the bufring header structure
@
text
@d1350 1
@


1.25
log
@Identify as an OSPlatformID 131 with a kernel version of 6
@
text
@d1077 1
a1077 1
	buflen = roundup(buflen, PAGE_SIZE);
@


1.24
log
@Inline the macro that is used only once
@
text
@d83 1
a83 1
void	hv_fake_version(struct hv_softc *);
d284 1
a284 1
	hv_fake_version(sc);
d316 1
a316 1
hv_fake_version(struct hv_softc *sc)
d320 3
a322 3
	/* FreeBSD 10 apparently */
	ver = 0x8200ULL << 48;
	ver |= 10 << 16;
@


1.23
log
@Fixup a wait channel used during VMBus channel discovery

Clang static analyser has found that a tsleep was using an uninitialised
pointer value as a wait channel.  An associated wakeup wasn't doing the
right thing either.
@
text
@a1231 3
#define	HV_BYTES_AVAIL_TO_WRITE(r, w, z)			\
	((w) >= (r)) ? ((z) - ((w) - (r))) : ((r) - (w))

d1239 4
a1242 1
	w =  HV_BYTES_AVAIL_TO_WRITE(ridx, widx, rd->rd_data_size);
d1502 1
a1502 1
	    /* sizeof(struct vmbus_gpa_range) */ + inhdr * sizeof(uint64_t);
@


1.22
log
@Improve channel ring locking strategy

Make sure to hold the ring buffer lock for the complete duration
of a channel ring I/O operation.  For read operations this means
peeking and reading is done uninterrupted and for both reads and
writes this places the buffer space availability check under the
same lock as well.
@
text
@d772 1
a772 1
	wakeup(hdr);
d946 1
a946 1
	struct vmbus_chanmsg_choffer rsp, *offer;
d961 1
a961 1
		tsleep(offer, PRIBIO, "hvoffers", 1);
@


1.21
log
@Channel API users should pick the bufring size

hvn(4) needs to be able to put up to 128 RNDIS data messages on the
channel ring each pointing up to 16 fragments in addition to a small
number of RNDIS control messages.  This simplifies accounting of
available TX descriptors in relation to the available space on the
channel ring and as a results prevents packet drops due to the ring
being full.
@
text
@a1268 2
	mtx_enter(&wrd->rd_lock);

d1279 1
a1279 2

	mtx_leave(&wrd->rd_lock);
d1320 1
d1322 1
d1363 1
d1365 1
a1382 1
	mtx_enter(&rrd->rd_lock);
a1383 1
	mtx_leave(&rrd->rd_lock);
a1401 2
	mtx_enter(&rrd->rd_lock);

a1412 2
	mtx_leave(&rrd->rd_lock);

d1426 4
a1429 1
	if ((rv = hv_ring_peek(&ch->ch_rrd, &cph, sizeof(cph))) != 0)
d1431 1
d1436 1
d1446 2
@


1.20
log
@The trailing producer index must point to the beginning of the message
@
text
@d104 1
a104 1
int	hv_channel_ring_create(struct hv_channel *, uint32_t, uint32_t);
d1073 1
a1073 2
hv_channel_ring_create(struct hv_channel *ch, uint32_t sndbuflen,
    uint32_t rcvbuflen)
d1077 3
a1079 4
	sndbuflen = roundup(sndbuflen, PAGE_SIZE);
	rcvbuflen = roundup(rcvbuflen, PAGE_SIZE);
	ch->ch_ring = km_alloc(sndbuflen + rcvbuflen, &kv_any, &kp_zero,
	    cold ? &kd_nowait : &kd_waitok);
d1085 1
a1085 1
	ch->ch_ring_size = sndbuflen + rcvbuflen;
d1090 2
a1091 2
	ch->ch_wrd.rd_size = sndbuflen;
	ch->ch_wrd.rd_data_size = sndbuflen - sizeof(struct vmbus_bufring);
d1096 3
a1098 3
	    sndbuflen);
	ch->ch_rrd.rd_size = rcvbuflen;
	ch->ch_rrd.rd_data_size = rcvbuflen - sizeof(struct vmbus_bufring);
d1101 1
a1101 2
	if (hv_handle_alloc(ch, ch->ch_ring, sndbuflen + rcvbuflen,
	    &ch->ch_ring_gpadl)) {
d1124 2
a1125 2
hv_channel_open(struct hv_channel *ch, void *udata, size_t udatalen,
    void (*handler)(void *), void *arg)
d1133 1
a1133 1
	    hv_channel_ring_create(ch, PAGE_SIZE * 4, PAGE_SIZE * 4)) {
@


1.19
log
@Move Hyper-V integration components into a separate source file

Input & OK reyk.
@
text
@d1279 1
a1279 1
	indices = (uint64_t)wrd->rd_prod << 32;
@


1.18
log
@Reshuffle GUID device ID table and add $ tags to headers
@
text
@d106 1
a106 9
void	hv_attach_internal(struct hv_softc *);
void	hv_heartbeat(void *);
void	hv_kvp_init(struct hv_channel *);
void	hv_kvp(void *);
int	hv_kvop(void *, int, char *, char *, size_t);
void	hv_shutdown_init(struct hv_channel *);
void	hv_shutdown(void *);
void	hv_timesync_init(struct hv_channel *);
void	hv_timesync(void *);
a310 2
	hv_attach_internal(sc);

a1609 267
const struct {
	const char		 *id_name;
	const struct hv_guid	 *id_type;
	void			(*id_init)(struct hv_channel *);
	void			(*id_handler)(void *);
} hv_internal_devs[] = {
	{ "heartbeat",	&hv_guid_heartbeat, NULL,		hv_heartbeat },
	{ "kvp",	&hv_guid_kvp,	    hv_kvp_init,	hv_kvp },
	{ "shutdown",	&hv_guid_shutdown,  hv_shutdown_init,	hv_shutdown },
	{ "timesync",	&hv_guid_timesync,  hv_timesync_init,	hv_timesync }
};

void
hv_attach_internal(struct hv_softc *sc)
{
	struct hv_channel *ch;
	int i;

	TAILQ_FOREACH(ch, &sc->sc_channels, ch_entry) {
		if (ch->ch_state != HV_CHANSTATE_OFFERED)
			continue;
		if (ch->ch_flags & CHF_MONITOR)
			continue;
		for (i = 0; i < nitems(hv_internal_devs); i++) {
			if (memcmp(hv_internal_devs[i].id_type, &ch->ch_type,
			    sizeof(ch->ch_type)) != 0)
				continue;
			/*
			 * These services are not performance critical and
			 * do not need batched reading. Furthermore, some
			 * services such as KVP can only handle one message
			 * from the host at a time.
			 */
			ch->ch_flags &= ~CHF_BATCHED;

			if (hv_internal_devs[i].id_init)
				hv_internal_devs[i].id_init(ch);

			ch->ch_buf = km_alloc(PAGE_SIZE, &kv_any, &kp_zero,
			    (cold ? &kd_nowait : &kd_waitok));
			if (ch->ch_buf == NULL) {
				hv_channel_ring_destroy(ch);
				printf("%s: failed to allocate channel data "
				    "buffer for \"%s\"", sc->sc_dev.dv_xname,
				    hv_internal_devs[i].id_name);
				continue;
			}
			ch->ch_buflen = PAGE_SIZE;

			if (hv_channel_open(ch, NULL, 0,
			    hv_internal_devs[i].id_handler, ch)) {
				km_free(ch->ch_buf, PAGE_SIZE, &kv_any,
				    &kp_zero);
				ch->ch_buf = NULL;
				ch->ch_buflen = 0;
				printf("%s: failed to open channel for \"%s\"\n",
				    sc->sc_dev.dv_xname,
				    hv_internal_devs[i].id_name);
			}
			evcount_attach(&ch->ch_evcnt,
			    hv_internal_devs[i].id_name, &sc->sc_idtvec);
			break;
		}
	}
}

int
hv_service_common(struct hv_channel *ch, uint32_t *rlen, uint64_t *rid,
    struct hv_icmsg_hdr **hdr)
{
	struct hv_icmsg_negotiate *msg;
	int rv;

	rv = hv_channel_recv(ch, ch->ch_buf, ch->ch_buflen, rlen, rid, 0);
	if (rv || *rlen == 0)
		return (rv);
	*hdr = (struct hv_icmsg_hdr *)&ch->ch_buf[sizeof(struct hv_pipe_hdr)];
	if ((*hdr)->icmsgtype == HV_ICMSGTYPE_NEGOTIATE) {
		msg = (struct hv_icmsg_negotiate *)(*hdr + 1);
		if (msg->icframe_vercnt >= 2 &&
		    msg->icversion_data[1].major == 3) {
			msg->icversion_data[0].major = 3;
			msg->icversion_data[0].minor = 0;
			msg->icversion_data[1].major = 3;
			msg->icversion_data[1].minor = 0;
		} else {
			msg->icversion_data[0].major = 1;
			msg->icversion_data[0].minor = 0;
			msg->icversion_data[1].major = 1;
			msg->icversion_data[1].minor = 0;
		}
		msg->icframe_vercnt = 1;
		msg->icmsg_vercnt = 1;
		(*hdr)->icmsgsize = 0x10;
	}
	return (0);
}

void
hv_heartbeat(void *arg)
{
	struct hv_channel *ch = arg;
	struct hv_softc *sc = ch->ch_sc;
	struct hv_icmsg_hdr *hdr;
	struct hv_heartbeat_msg *msg;
	uint64_t rid;
	uint32_t rlen;
	int rv;

	rv = hv_service_common(ch, &rlen, &rid, &hdr);
	if (rv || rlen == 0) {
		if (rv != EAGAIN)
			printf("heartbeat: rv=%d rlen=%u\n", rv, rlen);
		return;
	}
	if (hdr->icmsgtype == HV_ICMSGTYPE_HEARTBEAT) {
		msg = (struct hv_heartbeat_msg *)(hdr + 1);
		msg->seq_num += 1;
	} else if (hdr->icmsgtype != HV_ICMSGTYPE_NEGOTIATE) {
		printf("%s: unhandled heartbeat message type %u\n",
		    sc->sc_dev.dv_xname, hdr->icmsgtype);
	}
	hdr->icflags = HV_ICMSGHDRFLAG_TRANSACTION | HV_ICMSGHDRFLAG_RESPONSE;
	hv_channel_send(ch, ch->ch_buf, rlen, rid, VMBUS_CHANPKT_TYPE_INBAND, 0);
}

void
hv_kvp_init(struct hv_channel *ch)
{
	struct hv_softc *sc = ch->ch_sc;

	sc->sc_pvbus->hv_kvop = hv_kvop;
	sc->sc_pvbus->hv_arg = sc;
}

void
hv_kvp(void *arg)
{
}

int
hv_kvop(void *arg, int op, char *key, char *value, size_t valuelen)
{
	switch (op) {
	case PVBUS_KVWRITE:
	case PVBUS_KVREAD:
	default:
		return (EOPNOTSUPP);
	}
}

static void
hv_shutdown_task(void *arg)
{
	extern int allowpowerdown;

	if (allowpowerdown == 0)
		return;

	suspend_randomness();

	log(LOG_KERN | LOG_NOTICE, "Shutting down in response to "
	    "request from Hyper-V host\n");
	prsignal(initprocess, SIGUSR2);
}

void
hv_shutdown_init(struct hv_channel *ch)
{
	struct hv_softc *sc = ch->ch_sc;

	task_set(&sc->sc_sdtask, hv_shutdown_task, sc);
}

void
hv_shutdown(void *arg)
{
	struct hv_channel *ch = arg;
	struct hv_softc *sc = ch->ch_sc;
	struct hv_icmsg_hdr *hdr;
	struct hv_shutdown_msg *msg;
	uint64_t rid;
	uint32_t rlen;
	int rv, shutdown = 0;

	rv = hv_service_common(ch, &rlen, &rid, &hdr);
	if (rv || rlen == 0) {
		if (rv != EAGAIN)
			printf("shutdown: rv=%d rlen=%u\n", rv, rlen);
		return;
	}
	if (hdr->icmsgtype == HV_ICMSGTYPE_SHUTDOWN) {
		msg = (struct hv_shutdown_msg *)(hdr + 1);
		if (msg->flags == 0 || msg->flags == 1) {
			shutdown = 1;
			hdr->status = HV_ICMSG_STATUS_OK;
		} else
			hdr->status = HV_ICMSG_STATUS_FAIL;
	} else if (hdr->icmsgtype != HV_ICMSGTYPE_NEGOTIATE) {
		printf("%s: unhandled shutdown message type %u\n",
		    sc->sc_dev.dv_xname, hdr->icmsgtype);
	}

	hdr->icflags = HV_ICMSGHDRFLAG_TRANSACTION | HV_ICMSGHDRFLAG_RESPONSE;
	hv_channel_send(ch, ch->ch_buf, rlen, rid, VMBUS_CHANPKT_TYPE_INBAND, 0);

	if (shutdown)
		task_add(systq, &sc->sc_sdtask);
}

void
hv_timesync_init(struct hv_channel *ch)
{
	struct hv_softc *sc = ch->ch_sc;

	strlcpy(sc->sc_sensordev.xname, sc->sc_dev.dv_xname,
	    sizeof(sc->sc_sensordev.xname));

	sc->sc_sensor.type = SENSOR_TIMEDELTA;
	sc->sc_sensor.status = SENSOR_S_UNKNOWN;

	sensor_attach(&sc->sc_sensordev, &sc->sc_sensor);
	sensordev_install(&sc->sc_sensordev);
}

void
hv_timesync(void *arg)
{
	struct hv_channel *ch = arg;
	struct hv_softc *sc = ch->ch_sc;
	struct hv_icmsg_hdr *hdr;
	struct hv_timesync_msg *msg;
	struct timespec guest, host, diff;
	uint64_t tns;
	uint64_t rid;
	uint32_t rlen;
	int rv;

	rv = hv_service_common(ch, &rlen, &rid, &hdr);
	if (rv || rlen == 0) {
		if (rv != EAGAIN)
			printf("timesync: rv=%d rlen=%u\n", rv, rlen);
		return;
	}
	if (hdr->icmsgtype == HV_ICMSGTYPE_TIMESYNC) {
		msg = (struct hv_timesync_msg *)(hdr + 1);
		if (msg->flags == HV_TIMESYNC_SYNC ||
		    msg->flags == HV_TIMESYNC_SAMPLE) {
			microtime(&sc->sc_sensor.tv);
			nanotime(&guest);
			tns = (msg->parent_time - 116444736000000000LL) * 100;
			host.tv_sec = tns / 1000000000LL;
			host.tv_nsec = tns % 1000000000LL;
			timespecsub(&guest, &host, &diff);
			sc->sc_sensor.value = (int64_t)diff.tv_sec *
			    1000000000LL + diff.tv_nsec;
			sc->sc_sensor.status = SENSOR_S_OK;
		}
	} else if (hdr->icmsgtype != HV_ICMSGTYPE_NEGOTIATE) {
		printf("%s: unhandled timesync message type %u\n",
		    sc->sc_dev.dv_xname, hdr->icmsgtype);
	}

	hdr->icflags = HV_ICMSGHDRFLAG_TRANSACTION | HV_ICMSGHDRFLAG_RESPONSE;
	hv_channel_send(ch, ch->ch_buf, rlen, rid, VMBUS_CHANPKT_TYPE_INBAND, 0);
}

d1629 3
@


1.17
log
@Check the actual pointer returned by km_alloc(9) instead of the array.  The
latter will always be non-NULL, and therefore the test will always be false.

Caught by clang.

ok jca@@, mikeb@@, jsg@@
@
text
@d160 108
a880 35
const struct hv_guid hv_guid_network = {
	{ 0x63, 0x51, 0x61, 0xf8, 0x3e, 0xdf, 0xc5, 0x46,
	  0x91, 0x3f, 0xf2, 0xd2, 0xf9, 0x65, 0xed, 0x0e }
};

const struct hv_guid hv_guid_ide = {
	{ 0x32, 0x26, 0x41, 0x32, 0xcb, 0x86, 0xa2, 0x44,
	  0x9b, 0x5c, 0x50, 0xd1, 0x41, 0x73, 0x54, 0xf5 }
};

const struct hv_guid hv_guid_scsi = {
	{ 0xd9, 0x63, 0x61, 0xba, 0xa1, 0x04, 0x29, 0x4d,
	  0xb6, 0x05, 0x72, 0xe2, 0xff, 0xb1, 0xdc, 0x7f }
};

const struct hv_guid hv_guid_shutdown = {
	{ 0x31, 0x60, 0x0b, 0x0e, 0x13, 0x52, 0x34, 0x49,
	  0x81, 0x8b, 0x38, 0xd9, 0x0c, 0xed, 0x39, 0xdb }
};

const struct hv_guid hv_guid_timesync = {
	{ 0x30, 0xe6, 0x27, 0x95, 0xae, 0xd0, 0x7b, 0x49,
	  0xad, 0xce, 0xe8, 0x0a, 0xb0, 0x17, 0x5c, 0xaf }
};

const struct hv_guid hv_guid_heartbeat = {
	{ 0x39, 0x4f, 0x16, 0x57, 0x15, 0x91, 0x78, 0x4e,
	  0xab, 0x55, 0x38, 0x2f, 0x3b, 0xd5, 0x42, 0x2d }
};

const struct hv_guid hv_guid_kvp = {
	{ 0xe7, 0xf4, 0xa0, 0xa9, 0x45, 0x5a, 0x96, 0x4d,
	  0xb8, 0x27, 0x8a, 0x84, 0x1e, 0x8c, 0x03, 0xe6 }
};

a881 71
const struct hv_guid hv_guid_vss = {
	{ 0x29, 0x2e, 0xfa, 0x35, 0x23, 0xea, 0x36, 0x42,
	  0x96, 0xae, 0x3a, 0x6e, 0xba, 0xcb, 0xa4, 0x40 }
};

const struct hv_guid hv_guid_dynmem = {
	{ 0xdc, 0x74, 0x50, 0x52, 0x85, 0x89, 0xe2, 0x46,
	  0x80, 0x57, 0xa3, 0x07, 0xdc, 0x18, 0xa5, 0x02 }
};

const struct hv_guid hv_guid_mouse = {
	{ 0x9e, 0xb6, 0xa8, 0xcf, 0x4a, 0x5b, 0xc0, 0x4c,
	  0xb9, 0x8b, 0x8b, 0xa1, 0xa1, 0xf3, 0xf9, 0x5a }
};

const struct hv_guid hv_guid_kbd = {
	{ 0x6d, 0xad, 0x12, 0xf9, 0x17, 0x2b, 0xea, 0x48,
	  0xbd, 0x65, 0xf9, 0x27, 0xa6, 0x1c, 0x76, 0x84 }
};

const struct hv_guid hv_guid_video = {
	{ 0x02, 0x78, 0x0a, 0xda, 0x77, 0xe3, 0xac, 0x4a,
	  0x8e, 0x77, 0x05, 0x58, 0xeb, 0x10, 0x73, 0xf8 }
};

const struct hv_guid hv_guid_fc = {
	{ 0x4a, 0xcc, 0x9b, 0x2f, 0x69, 0x00, 0xf3, 0x4a,
	  0xb7, 0x6b, 0x6f, 0xd0, 0xbe, 0x52, 0x8c, 0xda }
};

const struct hv_guid hv_guid_fcopy = {
	{ 0xe3, 0x4b, 0xd1, 0x34, 0xe4, 0xde, 0xc8, 0x41,
	  0x9a, 0xe7, 0x6b, 0x17, 0x49, 0x77, 0xc1, 0x92 }
};

const struct hv_guid hv_guid_pcie = {
	{ 0x1d, 0xf6, 0xc4, 0x44, 0x44, 0x44, 0x00, 0x44,
	  0x9d, 0x52, 0x80, 0x2e, 0x27, 0xed, 0xe1, 0x9f }
};

const struct hv_guid hv_guid_netdir = {
	{ 0x3d, 0xaf, 0x2e, 0x8c, 0xa7, 0x32, 0x09, 0x4b,
	  0xab, 0x99, 0xbd, 0x1f, 0x1c, 0x86, 0xb5, 0x01 }
};

const struct hv_guid hv_guid_rdesktop = {
	{ 0xf4, 0xac, 0x6a, 0x27, 0x15, 0xac, 0x6c, 0x42,
	  0x98, 0xdd, 0x75, 0x21, 0xad, 0x3f, 0x01, 0xfe }
};

/* Automatic Virtual Machine Activation (AVMA) Services */
const struct hv_guid hv_guid_avma1 = {
	{ 0x55, 0xb2, 0x87, 0x44, 0x8c, 0xb8, 0x3f, 0x40,
	  0xbb, 0x51, 0xd1, 0xf6, 0x9c, 0xf1, 0x7f, 0x87 }
};

const struct hv_guid hv_guid_avma2 = {
	{ 0xf4, 0xba, 0x75, 0x33, 0x15, 0x9e, 0x30, 0x4b,
	  0xb7, 0x65, 0x67, 0xac, 0xb1, 0x0d, 0x60, 0x7b }
};

const struct hv_guid hv_guid_avma3 = {
	{ 0xa0, 0x1f, 0x22, 0x99, 0xad, 0x24, 0xe2, 0x11,
	  0xbe, 0x98, 0x00, 0x1a, 0xa0, 0x1b, 0xbf, 0x6e }
};

const struct hv_guid hv_guid_avma4 = {
	{ 0x16, 0x57, 0xe6, 0xf8, 0xb3, 0x3c, 0x06, 0x4a,
	  0x9a, 0x60, 0x18, 0x89, 0xc5, 0xcc, 0xca, 0xb5 }
};

d1462 8
@


1.16
log
@Add an API to hook up event counters
@
text
@d703 1
a703 1
	if (sc->sc_monitor == NULL) {
d713 1
a713 1
	if (sc->sc_monitor == NULL) {
@


1.15
log
@Clean up style and replace a magic value with a magic define
@
text
@d1921 8
@


1.14
log
@Sync up monitor trigger groups handling to the upstream

Replace hand rolled atomic bit operations and use MI ones from DRM
and convert event matrixes to arrays of longs.
@
text
@d741 1
a741 1
			sc->sc_handle = 0xe1e10 - 1; /* magic! */
@


1.13
log
@Update Hyper-V structure definitions to the 2016 version

Microsoft has performed a significant clean up and stylistic improvement
of Hyper-V structure definitions and updated FreeBSD port.  We update to
stay in sync with the upstream and improve maintainability of this code
in the future.
@
text
@d536 1
a536 1
	int bit, dword, maxdword, chanid;
d538 1
a538 1
	uint32_t *revents, pending;
d544 1
a544 1
		if (atomic_clearbit_ptr(&evt->evt_flags[0], 0) == 0)
d546 2
a547 1
		maxdword = VMBUS_CHAN_MAX_COMPAT >> 5;
d553 1
a553 1
		maxdword = nitems(evt->evt_flags);
d562 2
a563 2
	for (dword = 0; dword < maxdword; dword++) {
		if (revents[dword] == 0)
d565 1
a565 1
		pending = atomic_swap_uint(&revents[dword], 0);
d569 1
a569 1
			chanid = (dword << 5) + bit;
d699 2
a700 3
	sc->sc_wevents = (uint32_t *)sc->sc_events;
	sc->sc_revents = (uint32_t *)((caddr_t)sc->sc_events +
	    (PAGE_SIZE >> 1));
d1020 2
a1021 2
		nch->ch_mgroup = co->co_chan.chm_montrig >> 5;
		nch->ch_mindex = co->co_chan.chm_montrig & 0x1f;
d1210 1
a1210 2
	atomic_setbit_ptr((uint32_t *)sc->sc_wevents + (ch->ch_id >> 5),
	    ch->ch_id & 31);
d1213 1
a1213 1
		atomic_setbit_ptr((uint32_t *)&mtg->mt_pending, ch->ch_mindex);
@


1.12
log
@Fixup incorrect format specification bugs spotted by jsg@@
@
text
@d97 3
a99 3
void	hv_channel_response(struct hv_softc *, struct hv_channel_msg_header *);
void	hv_channel_offer(struct hv_softc *, struct hv_channel_msg_header *);
void	hv_channel_delivered(struct hv_softc *, struct hv_channel_msg_header *);
d121 1
a121 1
			    struct hv_channel_msg_header *);
d123 4
a126 4
	{ HV_CHANMSG_INVALID,			0, NULL },
	{ HV_CHANMSG_OFFER_CHANNEL,		0, hv_channel_offer },
	{ HV_CHANMSG_RESCIND_CHANNEL_OFFER,	0, NULL },
	{ HV_CHANMSG_REQUEST_OFFERS,		HV_CHANMSG_OFFER_CHANNEL,
d128 1
a128 1
	{ HV_CHANMSG_ALL_OFFERS_DELIVERED,	0,
d130 2
a131 2
	{ HV_CHANMSG_OPEN_CHANNEL,		0, NULL },
	{ HV_CHANMSG_OPEN_CHANNEL_RESULT,	HV_CHANMSG_OPEN_CHANNEL,
d133 4
a136 4
	{ HV_CHANMSG_CLOSE_CHANNEL,		0, NULL },
	{ HV_CHANMSG_GPADL_HEADER,		0, NULL },
	{ HV_CHANMSG_GPADL_BODY,		0, NULL },
	{ HV_CHANMSG_GPADL_CREATED,		HV_CHANMSG_GPADL_HEADER,
d138 2
a139 2
	{ HV_CHANMSG_GPADL_TEARDOWN,		0, NULL },
	{ HV_CHANMSG_GPADL_TORNDOWN,		HV_CHANMSG_GPADL_TEARDOWN,
d141 3
a143 3
	{ HV_CHANMSG_REL_ID_RELEASED,		0, NULL },
	{ HV_CHANMSG_INITIATED_CONTACT,		0, NULL },
	{ HV_CHANMSG_VERSION_RESPONSE,		HV_CHANMSG_INITIATED_CONTACT,
d145 1
a145 1
	{ HV_CHANMSG_UNLOAD,			0, NULL },
d225 1
a225 1
	wrmsr(HV_X64_MSR_GUEST_OS_ID, ver);
d231 1
a231 1
	u_int now = rdmsr(HV_X64_MSR_TIME_REF_COUNT);
d250 2
a251 3
	msr = (atop(pa) << HV_X64_MSR_HYPERCALL_PASHIFT) |
	    HV_X64_MSR_HYPERCALL_ENABLED;
	wrmsr(HV_X64_MSR_HYPERCALL, msr);
d253 1
a253 1
	if (!(rdmsr(HV_X64_MSR_HYPERCALL) & HV_X64_MSR_HYPERCALL_ENABLED)) {
d272 1
a272 1
		return (~HV_STATUS_SUCCESS);
d279 1
a279 1
		return (~HV_STATUS_SUCCESS);
d333 1
a333 1
	sc->sc_proto = HV_VMBUS_VERSION_WS2008;
d343 1
a343 1
	uint64_t simp, siefp, sctrl, sint, version;
a345 2
	version = rdmsr(HV_X64_MSR_SVERSION);

d353 5
a357 5
	simp = rdmsr(HV_X64_MSR_SIMP);
	simp &= (1 << HV_X64_MSR_SIMP_PASHIFT) - 1;
	simp |= (atop(pa) << HV_X64_MSR_SIMP_PASHIFT);
	simp |= HV_X64_MSR_SIMP_ENABLED;
	wrmsr(HV_X64_MSR_SIMP, simp);
d366 13
a378 9
	siefp = rdmsr(HV_X64_MSR_SIEFP);
	siefp &= (1<<HV_X64_MSR_SIEFP_PASHIFT) - 1;
	siefp |= (atop(pa) << HV_X64_MSR_SIEFP_PASHIFT);
	siefp |= HV_X64_MSR_SIEFP_ENABLED;
	wrmsr(HV_X64_MSR_SIEFP, siefp);

	/* HV_SHARED_SINT_IDT_VECTOR + 0x20 */
	sint = sc->sc_idtvec | HV_X64_MSR_SINT_AUTOEOI;
	wrmsr(HV_X64_MSR_SINT0 + HV_MESSAGE_SINT, sint);
d381 3
a383 3
	sctrl = rdmsr(HV_X64_MSR_SCONTROL);
	sctrl |= HV_X64_MSR_SCONTROL_ENABLED;
	wrmsr(HV_X64_MSR_SCONTROL, sctrl);
d385 1
a385 1
	sc->sc_vcpus[cpu] = rdmsr(HV_X64_MSR_VP_INDEX);
d400 1
a400 1
	if (cmdlen > HV_MESSAGE_PAYLOAD) {
d408 2
a409 2
	msg.msg_req.payload_size = cmdlen;
	memcpy(msg.msg_req.payload, cmd, cmdlen);
d433 2
a434 2
	msg->msg_req.connection_id = HV_MESSAGE_CONNECTION_ID;
	msg->msg_req.message_type = 1;
d443 1
a443 1
		status = hv_hypercall(sc, HV_CALL_POST_MESSAGE,
d445 1
a445 1
		if (status != HV_STATUS_INSUFFICIENT_BUFFERS)
d517 1
a517 1
	status = hv_hypercall(sc, HV_CALL_SIGNAL_EVENT, con, NULL);
d533 1
a533 1
	struct hv_synic_event_flags *evt;
d536 1
a536 1
	int bit, dword, maxdword, relid;
d540 5
a544 4
	evt = (struct hv_synic_event_flags *)sc->sc_siep[cpu] + HV_MESSAGE_SINT;
	if ((sc->sc_proto == HV_VMBUS_VERSION_WS2008) ||
	    (sc->sc_proto == HV_VMBUS_VERSION_WIN7)) {
		if (atomic_clearbit_ptr(&evt->flags[0], 0) == 0)
d546 1
a546 1
		maxdword = HV_MAX_NUM_CHANNELS_SUPPORTED >> 5;
d552 1
a552 1
		maxdword = nitems(evt->flags);
d558 1
a558 1
		revents = &evt->flags[0];
d568 1
a568 1
			relid = (dword << 5) + bit;
d570 1
a570 1
			if (relid == 0)
d572 1
a572 1
			ch = hv_channel_lookup(sc, relid);
d575 1
a575 1
				    sc->sc_dev.dv_xname, relid);
d580 1
a580 1
				    sc->sc_dev.dv_xname, relid);
d593 2
a594 2
	struct hv_vmbus_message *msg;
	struct hv_channel_msg_header *hdr;
d599 3
a601 3
		msg = (struct hv_vmbus_message *)sc->sc_simp[cpu] +
		    HV_MESSAGE_SINT;
		if (msg->header.message_type == HV_MESSAGE_TYPE_NONE)
d604 2
a605 2
		hdr = (struct hv_channel_msg_header *)msg->payload;
		if (hdr->message_type >= HV_CHANMSG_COUNT) {
d607 2
a608 2
			    sc->sc_dev.dv_xname, hdr->message_type,
			    msg->header.message_flags);
d611 2
a612 2
		if (hv_msg_dispatch[hdr->message_type].hmd_handler)
			hv_msg_dispatch[hdr->message_type].hmd_handler(sc, hdr);
d615 1
a615 1
			    sc->sc_dev.dv_xname, hdr->message_type);
d617 1
a617 1
		msg->header.message_type = HV_MESSAGE_TYPE_NONE;
d619 2
a620 2
		if (msg->header.message_flags & HV_SYNIC_MHF_PENDING)
			wrmsr(HV_X64_MSR_EOM, 0);
d625 1
a625 1
hv_channel_response(struct hv_softc *sc, struct hv_channel_msg_header *rsphdr)
d628 1
a628 1
	struct hv_channel_msg_header *reqhdr;
d631 1
a631 1
	req = hv_msg_dispatch[rsphdr->message_type].hmd_request;
d634 2
a635 2
		reqhdr = (struct hv_channel_msg_header *)&msg->msg_req.payload;
		if (reqhdr->message_type == req) {
d651 1
a651 1
hv_channel_offer(struct hv_softc *sc, struct hv_channel_msg_header *hdr)
d670 1
a670 1
hv_channel_delivered(struct hv_softc *sc, struct hv_channel_msg_header *hdr)
d679 3
a681 3
	const uint32_t versions[] = { HV_VMBUS_VERSION_WIN8_1,
	    HV_VMBUS_VERSION_WIN8, HV_VMBUS_VERSION_WIN7,
	    HV_VMBUS_VERSION_WS2008, HV_VMBUS_VERSION_INVALID
d683 2
a684 2
	struct hv_channel_initiate_contact cmd;
	struct hv_channel_version_response rsp;
d723 4
a726 4
	cmd.hdr.message_type = HV_CHANMSG_INITIATED_CONTACT;
	cmd.interrupt_page = (uint64_t)epa;
	cmd.monitor_page_1 = (uint64_t)mpa1;
	cmd.monitor_page_2 = (uint64_t)mpa2;
d730 2
a731 2
	for (i = 0; versions[i] != HV_VMBUS_VERSION_INVALID; i++) {
		cmd.vmbus_version_requested = versions[i];
d734 1
a734 1
			DPRINTF("%s: INITIATED_CONTACT failed\n",
d738 1
a738 1
		if (rsp.version_supported) {
d747 1
a747 1
	if (versions[i] == HV_VMBUS_VERSION_INVALID) {
d953 2
a954 2
	struct hv_channel_msg_header hdr;
	struct hv_channel_offer_channel rsp, *offer;
d960 2
a961 2
	hdr.message_type = HV_CHANMSG_REQUEST_OFFERS;
	hdr.padding = 0;
d964 1
a964 1
		DPRINTF("%s: REQUEST_OFFERS failed\n", sc->sc_dev.dv_xname);
d1002 1
a1002 1
	hv_guid_sprint(&co->co_chan.offer.interface_type, nch->ch_ident,
d1012 10
a1021 13
	KASSERT((((vaddr_t)&nch->ch_sigevt) & 0x7) == 0);
	memset(&nch->ch_sigevt, 0, sizeof(nch->ch_sigevt));
	nch->ch_sigevt.connection_id = HV_EVENT_CONNECTION_ID;

	if (sc->sc_proto != HV_VMBUS_VERSION_WS2008) {
		if (co->co_chan.is_dedicated_interrupt)
			nch->ch_flags |= CHF_DEDICATED;
		nch->ch_sigevt.connection_id = co->co_chan.connection_id;
	}

	if (co->co_chan.monitor_allocated) {
		nch->ch_mgroup = co->co_chan.monitor_id >> 5;
		nch->ch_mindex = co->co_chan.monitor_id & 0x1f;
d1025 1
a1025 1
	nch->ch_relid = co->co_chan.child_rel_id;
d1027 2
a1028 4
	memcpy(&nch->ch_type, &co->co_chan.offer.interface_type,
	    sizeof(ch->ch_type));
	memcpy(&nch->ch_inst, &co->co_chan.offer.interface_instance,
	    sizeof(ch->ch_inst));
d1037 1
a1037 1
		if (co->co_chan.offer.sub_channel_index == 0) {
d1046 1
a1046 1
		    co->co_chan.offer.sub_channel_index, ch->ch_ident);
d1059 1
a1059 1
	printf("%s: channel %u: \"%s\"", sc->sc_dev.dv_xname, nch->ch_relid,
d1061 2
a1062 2
	if (co->co_chan.monitor_allocated)
		printf(", monitor %u\n", co->co_chan.monitor_id);
d1074 1
a1074 1
		if (ch->ch_relid == relid)
d1099 1
a1099 1
	ch->ch_wrd.rd_ring = (struct hv_ring_buffer *)ch->ch_ring;
d1101 1
a1101 1
	ch->ch_wrd.rd_data_size = sndbuflen - sizeof(struct hv_ring_buffer);
d1105 1
a1105 1
	ch->ch_rrd.rd_ring = (struct hv_ring_buffer *)((uint8_t *)ch->ch_ring +
d1108 1
a1108 1
	ch->ch_rrd.rd_data_size = rcvbuflen - sizeof(struct hv_ring_buffer);
d1112 1
a1112 1
	    &ch->ch_ring_hndl)) {
d1128 1
a1128 1
	hv_handle_free(ch, ch->ch_ring_hndl);
d1139 2
a1140 2
	struct hv_channel_open cmd;
	struct hv_channel_open_result rsp;
d1150 6
a1155 7
	cmd.header.message_type = HV_CHANMSG_OPEN_CHANNEL;
	cmd.open_id = ch->ch_relid;
	cmd.child_rel_id = ch->ch_relid;
	cmd.ring_buffer_gpadl_handle = ch->ch_ring_hndl;
	cmd.downstream_ring_buffer_page_offset =
	    ch->ch_wrd.rd_size >> PAGE_SHIFT;
	cmd.target_vcpu = ch->ch_vcpu;
d1158 1
a1158 1
		memcpy(&cmd.user_data, udata, udatalen);
d1170 1
a1170 1
		DPRINTF("%s: OPEN_CHANNEL failed with %d\n",
d1185 1
a1185 1
	struct hv_channel_close cmd;
d1189 2
a1190 2
	cmd.header.message_type = HV_CHANMSG_CLOSE_CHANNEL;
	cmd.child_rel_id = ch->ch_relid;
d1195 1
a1195 1
		DPRINTF("%s: CLOSE_CHANNEL failed with %d\n",
d1207 1
a1207 1
	struct hv_monitor_trigger_group *mtg;
d1210 2
a1211 2
	atomic_setbit_ptr((uint32_t *)sc->sc_wevents + (ch->ch_relid >> 5),
	    ch->ch_relid & 31);
d1213 2
a1214 2
		mtg = &sc->sc_monitor[1]->trigger_group[ch->ch_mgroup];
		atomic_setbit_ptr((uint32_t *)&mtg->pending, ch->ch_mindex);
d1216 1
a1216 1
		hv_intr_signal(sc, &ch->ch_sigevt);
d1224 2
a1225 2
	memcpy(&wrd->rd_ring->buffer[wrd->rd_prod], data, left);
	memcpy(&wrd->rd_ring->buffer[0], data + left, datalen - left);
d1236 2
a1237 2
	memcpy(data, &rrd->rd_ring->buffer[rrd->rd_cons], left);
	memcpy(data + left, &rrd->rd_ring->buffer[0], datalen - left);
d1250 2
a1251 2
	uint32_t ridx = rd->rd_ring->read_index;
	uint32_t widx = rd->rd_ring->write_index;
d1292 1
a1292 1
	wrd->rd_ring->write_index = wrd->rd_prod;
d1297 2
a1298 2
	if (wrd->rd_ring->interrupt_mask == 0 &&
	    wrd->rd_ring->read_index == oprod)
d1311 1
a1311 1
	struct hv_pktdesc d;
d1317 1
a1317 1
	pktlen = sizeof(d) + datalen;
d1320 5
a1324 5
	d.type = type;
	d.flags = flags;
	d.offset = sizeof(d) >> 3;
	d.length = pktlen_aligned >> 3;
	d.tid = rid;
d1326 2
a1327 2
	iov[0].iov_base = &d;
	iov[0].iov_len = sizeof(d);
d1343 2
a1344 2
hv_channel_sendbuf(struct hv_channel *ch, struct hv_page_buffer *pb,
    uint32_t npb, void *data, uint32_t datalen, uint64_t rid)
d1347 1
a1347 1
	struct hv_gpadesc d;
d1353 2
a1354 2
	buflen = sizeof(struct hv_page_buffer) * npb;
	pktlen = sizeof(d) + datalen + buflen;
d1357 6
a1362 6
	d.type = HV_PKT_DATA_USING_GPA_DIRECT;
	d.flags = HV_PKTFLAG_COMPLETION_REQUESTED;
	d.offset = (sizeof(d) + buflen) >> 3;
	d.length = pktlen_aligned >> 3;
	d.tid = rid;
	d.range_count = npb;
d1364 2
a1365 2
	iov[0].iov_base = &d;
	iov[0].iov_len = sizeof(d);
d1367 1
a1367 1
	iov[1].iov_base = pb;
d1426 1
a1426 1
	rrd->rd_ring->read_index = rrd->rd_cons;
d1437 1
a1437 1
	struct hv_pktdesc d;
d1443 1
a1443 1
	if ((rv = hv_ring_peek(&ch->ch_rrd, &d, sizeof(d))) != 0)
d1446 2
a1447 2
	offset = raw ? 0 : (d.offset << 3);
	pktlen = (d.length << 3) - offset;
d1456 1
a1456 1
		*rid = d.tid;
d1467 3
a1469 3
	struct hv_gpadl_header *hdr;
	struct hv_gpadl_body *body, *cmd;
	struct hv_gpadl_created rsp;
d1476 1
d1503 3
a1505 3
	msg->msg_req.payload_size = sizeof(struct hv_gpadl_header) +
	    sizeof(struct hv_gpa_range) + inhdr * sizeof(uint64_t);
	hdr = (struct hv_gpadl_header *)msg->msg_req.payload;
d1516 1
a1516 1
		bodylen = ncmds * HV_MESSAGE_PAYLOAD;
d1527 3
a1529 3
	hdr->header.message_type = HV_CHANMSG_GPADL_HEADER;
	hdr->child_rel_id = ch->ch_relid;
	hdr->gpadl = *handle;
d1532 2
a1533 2
	hdr->range_count = 1;
	hdr->range_buf_len = sizeof(struct hv_gpa_range) + total *
d1535 2
a1536 2
	hdr->range[0].byte_offset = 0;
	hdr->range[0].byte_count = buflen;
d1540 1
a1540 1
		hdr->range[0].pfn_array[i] = frames[pfn++];
d1543 4
a1546 4
		cmd = (struct hv_gpadl_body *)((caddr_t)body +
		    HV_MESSAGE_PAYLOAD * i);
		cmd->header.message_type = HV_CHANMSG_GPADL_BODY;
		cmd->gpadl = *handle;
d1549 1
a1549 1
			cmd->pfn[j] = frames[pfn++];
d1555 1
a1555 1
		DPRINTF("%s: GPADL_HEADER failed\n", sc->sc_dev.dv_xname);
d1560 2
a1561 2
		cmd = (struct hv_gpadl_body *)((caddr_t)body +
		    HV_MESSAGE_PAYLOAD * i);
d1569 1
a1569 1
			DPRINTF("%s: GPADL_BODY (iteration %d/%d) failed "
d1587 1
a1587 1
	KASSERT(*handle == rsp.gpadl);
d1596 2
a1597 2
	struct hv_gpadl_teardown cmd;
	struct hv_gpadl_torndown rsp;
d1601 3
a1603 3
	cmd.header.message_type = HV_CHANMSG_GPADL_TEARDOWN;
	cmd.child_rel_id = ch->ch_relid;
	cmd.gpadl = handle;
d1607 1
a1607 1
		DPRINTF("%s: GPADL_TEARDOWN failed with %d\n",
d1734 1
a1734 1
	hv_channel_send(ch, ch->ch_buf, rlen, rid, HV_PKT_DATA_IN_BAND, 0);
d1806 1
a1806 1
			hdr->status = HV_S_OK;
d1808 1
a1808 1
			hdr->status = HV_E_FAIL;
d1815 1
a1815 1
	hv_channel_send(ch, ch->ch_buf, rlen, rid, HV_PKT_DATA_IN_BAND, 0);
d1875 1
a1875 1
	hv_channel_send(ch, ch->ch_buf, rlen, rid, HV_PKT_DATA_IN_BAND, 0);
@


1.11
log
@Switch pending event clearing to an atomic swap operation

Rather than performing an atomic bit clearing for every encountered
event bit set we can adjust the code to perform an atomic swap of a
single row of the events array and decrease the amount of expensive
atomic operations.

From FreeBSD.
@
text
@d400 1
a400 1
		printf("%s: payload too large (%ld)\n", sc->sc_dev.dv_xname,
d572 1
a572 1
				printf("%s: unhandled event on %u\n",
d577 1
a577 1
				printf("%s: channel %u is not active\n",
d604 1
a604 1
			printf("%s: unhandled message type %d flags %#x\n",
d612 1
a612 1
			printf("%s: unhandled message type %d\n",
@


1.10
log
@Fixup incorrect EAGAIN handling
@
text
@d537 1
a537 1
	uint32_t *revents;
d562 3
a564 2
		for (bit = 0; bit < 32; bit++) {
			if (!atomic_clearbit_ptr(&revents[dword], bit))
@


1.9
log
@Read events array offset must be calculated in bytes
@
text
@d1687 1
a1687 1
	if ((rv && rv != EAGAIN) || *rlen == 0)
@


1.8
log
@Remove incorrect optimization: ring data portion size is not power of 2
@
text
@d696 2
a697 1
	sc->sc_revents = (uint32_t *)sc->sc_events + (PAGE_SIZE >> 1);
@


1.7
log
@Fixup the condition that prevented heartbeat & co. from attaching
@
text
@d1229 1
a1229 1
	wrd->rd_prod &= wrd->rd_data_size - 1;
d1242 1
a1242 1
		rrd->rd_cons &= rrd->rd_data_size - 1;
d1421 1
a1421 1
		rrd->rd_cons &= rrd->rd_data_size - 1;
@


1.6
log
@Revert previous commit
@
text
@d1631 1
a1631 1
		if (ch->ch_state != HV_CHANSTATE_OPENED)
@


1.5
log
@Convert ifq_deq_{begin,rollback,commit} dance to a single ifq_dequeue
@
text
@d400 1
a400 1
		printf("%s: payload too large (%lu)\n", sc->sc_dev.dv_xname,
d571 1
a571 1
				printf("%s: unhandled event on %d\n",
d576 1
a576 1
				printf("%s: channel %d is not active\n",
d603 1
a603 1
			printf("%s: unhandled message type %u flags %#x\n",
d611 1
a611 1
			printf("%s: unhandled message type %u\n",
@


1.4
log
@Remove a few debugging leftovers
@
text
@d400 1
a400 1
		printf("%s: payload too large (%ld)\n", sc->sc_dev.dv_xname,
d571 1
a571 1
				printf("%s: unhandled event on %u\n",
d576 1
a576 1
				printf("%s: channel %u is not active\n",
d603 1
a603 1
			printf("%s: unhandled message type %d flags %#x\n",
d611 1
a611 1
			printf("%s: unhandled message type %d\n",
@


1.3
log
@Fix a memory leak in an error path.
ok mikeb@@
@
text
@a1901 6
#if 0
		if (strcmp(ch->ch_ident, "network") != 0 &&
		    strcmp(ch->ch_ident, "scsi") != 0 &&
		    strcmp(ch->ch_ident, "ide") != 0)
			continue;
#else
a1903 1
#endif
@


1.2
log
@Correct the order of memset arguments.
ok mikeb@@
@
text
@d1490 2
a1491 1
	    waitok)) == NULL)
d1493 1
@


1.1
log
@Main Hyper-V nexus driver, work in progress.

ok mlarkin, deraadt
@
text
@d718 1
a718 1
	memset(&cmd, sizeof(cmd), 0);
d724 1
a724 1
	memset(&rsp, sizeof(rsp), 0);
@

