head	1.54;
access;
symbols
	OPENBSD_6_1:1.54.0.4
	OPENBSD_6_1_BASE:1.54
	OPENBSD_6_0:1.22.0.2
	OPENBSD_6_0_BASE:1.22
	OPENBSD_5_9:1.17.0.2
	OPENBSD_5_9_BASE:1.17;
locks; strict;
comment	@ * @;


1.54
date	2017.03.13.01.10.03;	author mikeb;	state Exp;
branches;
next	1.53;
commitid	7qXoPSATw8fQxunw;

1.53
date	2017.03.13.01.00.15;	author mikeb;	state Exp;
branches;
next	1.52;
commitid	knAVpjVB7fgmCUNq;

1.52
date	2017.03.09.20.15.36;	author mikeb;	state Exp;
branches;
next	1.51;
commitid	Cnf3b1ZlxVmrPY0i;

1.51
date	2017.02.24.16.58.12;	author mikeb;	state Exp;
branches;
next	1.50;
commitid	dd60qwGHamkvdf8J;

1.50
date	2017.02.08.16.17.31;	author mikeb;	state Exp;
branches;
next	1.49;
commitid	X7IM6zKDuCCwg2Cs;

1.49
date	2017.02.06.21.43.48;	author mikeb;	state Exp;
branches;
next	1.48;
commitid	NscjkKs6gyrHx1os;

1.48
date	2017.01.24.03.57.35;	author dlg;	state Exp;
branches;
next	1.47;
commitid	PERtGPXCvlLRRBr8;

1.47
date	2017.01.22.10.17.39;	author dlg;	state Exp;
branches;
next	1.46;
commitid	VyLWTsbepAOk7VQM;

1.46
date	2017.01.05.13.23.51;	author mikeb;	state Exp;
branches;
next	1.45;
commitid	vrs26y2Cwl19Fr2D;

1.45
date	2016.12.19.21.08.57;	author mikeb;	state Exp;
branches;
next	1.44;
commitid	kSvjeXF1EGVSskvs;

1.44
date	2016.12.13.21.01.46;	author mikeb;	state Exp;
branches;
next	1.43;
commitid	kVppuip15myw1SHj;

1.43
date	2016.12.02.15.12.41;	author mikeb;	state Exp;
branches;
next	1.42;
commitid	WoU611RQ2I021Oof;

1.42
date	2016.11.29.14.55.04;	author mikeb;	state Exp;
branches;
next	1.41;
commitid	tsMjTSHgEUUjCy08;

1.41
date	2016.10.06.17.02.22;	author mikeb;	state Exp;
branches;
next	1.40;
commitid	Ku13PH4Zgy2Gcozt;

1.40
date	2016.10.06.17.00.25;	author mikeb;	state Exp;
branches;
next	1.39;
commitid	ZeSFkAmBHZTZgeoK;

1.39
date	2016.09.13.10.16.22;	author mikeb;	state Exp;
branches;
next	1.38;
commitid	Be6GfXLF5KWaU20A;

1.38
date	2016.09.12.18.55.18;	author mikeb;	state Exp;
branches;
next	1.37;
commitid	sknhpZpXtFhDy5lf;

1.37
date	2016.09.12.17.32.00;	author mikeb;	state Exp;
branches;
next	1.36;
commitid	XVz9Vq4CktGVU4hI;

1.36
date	2016.09.12.17.17.12;	author mikeb;	state Exp;
branches;
next	1.35;
commitid	eh6Zq3SO1pGYXgjA;

1.35
date	2016.09.12.17.15.53;	author mikeb;	state Exp;
branches;
next	1.34;
commitid	senu9Am4tNg9dimu;

1.34
date	2016.08.29.17.35.25;	author mikeb;	state Exp;
branches;
next	1.33;
commitid	xacvaCUCMVqv3chR;

1.33
date	2016.08.29.17.27.04;	author mikeb;	state Exp;
branches;
next	1.32;
commitid	uxrE9RhZkMIz58oW;

1.32
date	2016.08.29.14.12.58;	author mikeb;	state Exp;
branches;
next	1.31;
commitid	ZOp9cu7zWvd4BRBx;

1.31
date	2016.08.03.15.08.06;	author mikeb;	state Exp;
branches;
next	1.30;
commitid	HTxpufDWx27QKaEM;

1.30
date	2016.08.01.13.48.33;	author mikeb;	state Exp;
branches;
next	1.29;
commitid	pGz3I592V9eZzwGK;

1.29
date	2016.07.29.22.25.28;	author mikeb;	state Exp;
branches;
next	1.28;
commitid	9WXBGYd4q3M9zmU8;

1.28
date	2016.07.29.22.01.57;	author mikeb;	state Exp;
branches;
next	1.27;
commitid	noxmbg50Xfey9u9k;

1.27
date	2016.07.29.18.33.12;	author mikeb;	state Exp;
branches;
next	1.26;
commitid	fUYwvGoYmBqEtXCW;

1.26
date	2016.07.29.18.31.51;	author mikeb;	state Exp;
branches;
next	1.25;
commitid	hNcY0jobA4aRDAT5;

1.25
date	2016.07.29.18.31.22;	author mikeb;	state Exp;
branches;
next	1.24;
commitid	HNujFT4VOA9wkXZH;

1.24
date	2016.07.28.17.35.13;	author mikeb;	state Exp;
branches;
next	1.23;
commitid	YIRrHcamQkW8loua;

1.23
date	2016.07.28.12.08.14;	author mikeb;	state Exp;
branches;
next	1.22;
commitid	DybuCYH7FsRJfhfz;

1.22
date	2016.04.19.18.15.41;	author mikeb;	state Exp;
branches;
next	1.21;
commitid	3qNUUD4LouZDwEm4;

1.21
date	2016.04.19.14.19.44;	author mikeb;	state Exp;
branches;
next	1.20;
commitid	mAX2wP9YQZzgeS9t;

1.20
date	2016.04.19.13.55.19;	author mikeb;	state Exp;
branches;
next	1.19;
commitid	HxzkQ2JvETeHrLDy;

1.19
date	2016.04.19.12.39.31;	author mikeb;	state Exp;
branches;
next	1.18;
commitid	2egN98MvwADkTMbn;

1.18
date	2016.04.13.11.36.00;	author mpi;	state Exp;
branches;
next	1.17;
commitid	nHRUtEnkD6rbEjY0;

1.17
date	2016.02.05.10.34.52;	author mikeb;	state Exp;
branches;
next	1.16;
commitid	LujpJ0kPVX5exFic;

1.16
date	2016.01.29.18.49.06;	author mikeb;	state Exp;
branches;
next	1.15;
commitid	Pre5L4C8fKqWmy3c;

1.15
date	2016.01.26.17.01.01;	author mikeb;	state Exp;
branches;
next	1.14;
commitid	8AMv0eeBDBNQprT6;

1.14
date	2016.01.26.16.31.05;	author mikeb;	state Exp;
branches;
next	1.13;
commitid	09zgFJNOEQCVZ1td;

1.13
date	2016.01.26.16.13.32;	author mikeb;	state Exp;
branches;
next	1.12;
commitid	Tr7D3OPLlbHtIUcV;

1.12
date	2016.01.25.10.46.54;	author mikeb;	state Exp;
branches;
next	1.11;
commitid	BMMS4o4lFIrQaiNA;

1.11
date	2016.01.22.19.47.57;	author mikeb;	state Exp;
branches;
next	1.10;
commitid	OZmrXXTMwCIC5PcU;

1.10
date	2016.01.22.19.33.30;	author mikeb;	state Exp;
branches;
next	1.9;
commitid	V53tOaC9Xqw5QCRF;

1.9
date	2016.01.20.10.04.32;	author mikeb;	state Exp;
branches;
next	1.8;
commitid	G8WJcERBNcfGI9r4;

1.8
date	2016.01.19.17.16.19;	author mikeb;	state Exp;
branches;
next	1.7;
commitid	IAc3jSJFU8aLmYPx;

1.7
date	2016.01.19.17.13.22;	author mikeb;	state Exp;
branches;
next	1.6;
commitid	besXVwrXIGDmSPkr;

1.6
date	2016.01.15.14.27.08;	author mikeb;	state Exp;
branches;
next	1.5;
commitid	dQDkEWTtzV0PvUhk;

1.5
date	2016.01.13.20.27.18;	author mikeb;	state Exp;
branches;
next	1.4;
commitid	wseDR4E1jw1eGYUj;

1.4
date	2016.01.13.20.15.54;	author mikeb;	state Exp;
branches;
next	1.3;
commitid	UjYrWtNEaUpUxuIF;

1.3
date	2016.01.13.18.56.26;	author mikeb;	state Exp;
branches;
next	1.2;
commitid	dhpgKk62R9dDSOTB;

1.2
date	2016.01.08.14.59.37;	author reyk;	state Exp;
branches;
next	1.1;
commitid	T0WNPyqAvpbJTORb;

1.1
date	2016.01.07.11.13.19;	author mikeb;	state Exp;
branches;
next	;
commitid	gA9qUqkZDRFXzE4f;


desc
@@


1.54
log
@Fixup format strings in debug messages found by cppcheck
@
text
@/*	$OpenBSD: if_xnf.c,v 1.53 2017/03/13 01:00:15 mikeb Exp $	*/

/*
 * Copyright (c) 2015, 2016 Mike Belopuhov
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include "bpfilter.h"
#include "vlan.h"
#include "xen.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/atomic.h>
#include <sys/device.h>
#include <sys/kernel.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/pool.h>
#include <sys/queue.h>
#include <sys/socket.h>
#include <sys/sockio.h>
#include <sys/task.h>
#include <sys/timeout.h>

#include <machine/bus.h>

#include <dev/pv/xenreg.h>
#include <dev/pv/xenvar.h>

#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#ifdef INET6
#include <netinet/ip6.h>
#endif

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

/* #define XNF_DEBUG */

#ifdef XNF_DEBUG
#define DPRINTF(x...)		printf(x)
#else
#define DPRINTF(x...)
#endif

/*
 * Rx ring
 */

struct xnf_rx_req {
	uint16_t		 rxq_id;
	uint16_t		 rxq_pad;
	uint32_t		 rxq_ref;
} __packed;

struct xnf_rx_rsp {
	uint16_t		 rxp_id;
	uint16_t		 rxp_offset;
	uint16_t		 rxp_flags;
#define  XNF_RXF_CSUM_VALID	  0x0001
#define  XNF_RXF_CSUM_BLANK	  0x0002
#define  XNF_RXF_CHUNK		  0x0004
#define  XNF_RXF_MGMT		  0x0008
	int16_t			 rxp_status;
} __packed;

union xnf_rx_desc {
	struct xnf_rx_req	 rxd_req;
	struct xnf_rx_rsp	 rxd_rsp;
} __packed;

#define XNF_RX_DESC		256
#define XNF_MCLEN		PAGE_SIZE
#define XNF_RX_MIN		32

struct xnf_rx_ring {
	volatile uint32_t	 rxr_prod;
	volatile uint32_t	 rxr_prod_event;
	volatile uint32_t	 rxr_cons;
	volatile uint32_t	 rxr_cons_event;
	uint32_t		 rxr_reserved[12];
	union xnf_rx_desc	 rxr_desc[XNF_RX_DESC];
} __packed;


/*
 * Tx ring
 */

struct xnf_tx_req {
	uint32_t		 txq_ref;
	uint16_t		 txq_offset;
	uint16_t		 txq_flags;
#define  XNF_TXF_CSUM_BLANK	  0x0001
#define  XNF_TXF_CSUM_VALID	  0x0002
#define  XNF_TXF_CHUNK		  0x0004
#define  XNF_TXF_ETXRA		  0x0008
	uint16_t		 txq_id;
	uint16_t		 txq_size;
} __packed;

struct xnf_tx_rsp {
	uint16_t		 txp_id;
	int16_t			 txp_status;
} __packed;

union xnf_tx_desc {
	struct xnf_tx_req	 txd_req;
	struct xnf_tx_rsp	 txd_rsp;
} __packed;

#define XNF_TX_DESC		256
#define XNF_TX_FRAG		18

struct xnf_tx_ring {
	volatile uint32_t	 txr_prod;
	volatile uint32_t	 txr_prod_event;
	volatile uint32_t	 txr_cons;
	volatile uint32_t	 txr_cons_event;
	uint32_t		 txr_reserved[12];
	union xnf_tx_desc	 txr_desc[XNF_TX_DESC];
} __packed;


/* Management frame, "extra info" in Xen parlance */
struct xnf_mgmt {
	uint8_t			 mg_type;
#define  XNF_MGMT_MCAST_ADD	2
#define  XNF_MGMT_MCAST_DEL	3
	uint8_t			 mg_flags;
	union {
		uint8_t		 mgu_mcaddr[ETHER_ADDR_LEN];
		uint16_t	 mgu_pad[3];
	} u;
#define mg_mcaddr		 u.mgu_mcaddr
} __packed;


struct xnf_softc {
	struct device		 sc_dev;
	struct device		*sc_parent;
	char			 sc_node[XEN_MAX_NODE_LEN];
	char			 sc_backend[XEN_MAX_BACKEND_LEN];
	bus_dma_tag_t		 sc_dmat;
	int			 sc_domid;

	struct arpcom		 sc_ac;
	struct ifmedia		 sc_media;

	xen_intr_handle_t	 sc_xih;

	int			 sc_caps;
#define  XNF_CAP_SG		  0x0001
#define  XNF_CAP_CSUM4		  0x0002
#define  XNF_CAP_CSUM6		  0x0004
#define  XNF_CAP_MCAST		  0x0008
#define  XNF_CAP_SPLIT		  0x0010
#define  XNF_CAP_MULTIQ		  0x0020

	/* Rx ring */
	struct xnf_rx_ring	*sc_rx_ring;
	uint32_t		 sc_rx_cons;
	bus_dmamap_t		 sc_rx_rmap;		  /* map for the ring */
	bus_dma_segment_t	 sc_rx_seg;
	uint32_t		 sc_rx_ref;		  /* grant table ref */
	struct mbuf		*sc_rx_buf[XNF_RX_DESC];
	bus_dmamap_t		 sc_rx_dmap[XNF_RX_DESC]; /* maps for packets */
	struct mbuf		*sc_rx_cbuf[2];	  	  /* chain handling */

	/* Tx ring */
	struct xnf_tx_ring	*sc_tx_ring;
	uint32_t		 sc_tx_cons;
	bus_dmamap_t		 sc_tx_rmap;		  /* map for the ring */
	bus_dma_segment_t	 sc_tx_seg;
	uint32_t		 sc_tx_ref;		  /* grant table ref */
	int			 sc_tx_frags;
	struct mbuf		*sc_tx_buf[XNF_TX_DESC];
	bus_dmamap_t		 sc_tx_dmap[XNF_TX_DESC]; /* maps for packets */
};

int	xnf_match(struct device *, void *, void *);
void	xnf_attach(struct device *, struct device *, void *);
int	xnf_detach(struct device *, int);
int	xnf_lladdr(struct xnf_softc *);
int	xnf_ioctl(struct ifnet *, u_long, caddr_t);
int	xnf_media_change(struct ifnet *);
void	xnf_media_status(struct ifnet *, struct ifmediareq *);
int	xnf_iff(struct xnf_softc *);
void	xnf_init(struct xnf_softc *);
void	xnf_stop(struct xnf_softc *);
void	xnf_start(struct ifqueue *);
int	xnf_encap(struct xnf_softc *, struct mbuf *, uint32_t *);
void	xnf_intr(void *);
void	xnf_watchdog(struct ifnet *);
void	xnf_txeof(struct xnf_softc *);
void	xnf_rxeof(struct xnf_softc *);
int	xnf_rx_ring_fill(struct xnf_softc *);
int	xnf_rx_ring_create(struct xnf_softc *);
void	xnf_rx_ring_drain(struct xnf_softc *);
void	xnf_rx_ring_destroy(struct xnf_softc *);
int	xnf_tx_ring_create(struct xnf_softc *);
void	xnf_tx_ring_drain(struct xnf_softc *);
void	xnf_tx_ring_destroy(struct xnf_softc *);
int	xnf_capabilities(struct xnf_softc *sc);
int	xnf_init_backend(struct xnf_softc *);

struct cfdriver xnf_cd = {
	NULL, "xnf", DV_IFNET
};

const struct cfattach xnf_ca = {
	sizeof(struct xnf_softc), xnf_match, xnf_attach, xnf_detach
};

int
xnf_match(struct device *parent, void *match, void *aux)
{
	struct xen_attach_args *xa = aux;

	if (strcmp("vif", xa->xa_name))
		return (0);

	return (1);
}

void
xnf_attach(struct device *parent, struct device *self, void *aux)
{
	struct xen_attach_args *xa = aux;
	struct xnf_softc *sc = (struct xnf_softc *)self;
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	sc->sc_parent = parent;
	sc->sc_dmat = xa->xa_dmat;
	sc->sc_domid = xa->xa_domid;

	memcpy(sc->sc_node, xa->xa_node, XEN_MAX_NODE_LEN);
	memcpy(sc->sc_backend, xa->xa_backend, XEN_MAX_BACKEND_LEN);

	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, IFNAMSIZ);

	if (xnf_lladdr(sc)) {
		printf(": failed to obtain MAC address\n");
		return;
	}

	if (xen_intr_establish(0, &sc->sc_xih, sc->sc_domid, xnf_intr, sc,
	    ifp->if_xname)) {
		printf(": failed to establish an interrupt\n");
		return;
	}
	xen_intr_mask(sc->sc_xih);

	printf(" backend %d channel %u: address %s\n", sc->sc_domid,
	    sc->sc_xih, ether_sprintf(sc->sc_ac.ac_enaddr));

	if (xnf_capabilities(sc)) {
		xen_intr_disestablish(sc->sc_xih);
		return;
	}

	if (sc->sc_caps & XNF_CAP_SG)
		ifp->if_hardmtu = 9000;

	if (xnf_rx_ring_create(sc)) {
		xen_intr_disestablish(sc->sc_xih);
		return;
	}
	if (xnf_tx_ring_create(sc)) {
		xen_intr_disestablish(sc->sc_xih);
		xnf_rx_ring_destroy(sc);
		return;
	}
	if (xnf_init_backend(sc)) {
		xen_intr_disestablish(sc->sc_xih);
		xnf_rx_ring_destroy(sc);
		xnf_tx_ring_destroy(sc);
		return;
	}

	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_xflags = IFXF_MPSAFE;
	ifp->if_ioctl = xnf_ioctl;
	ifp->if_qstart = xnf_start;
	ifp->if_watchdog = xnf_watchdog;
	ifp->if_softc = sc;

	ifp->if_capabilities = IFCAP_VLAN_MTU;
	if (sc->sc_caps & XNF_CAP_CSUM4)
		ifp->if_capabilities |= IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;
	if (sc->sc_caps & XNF_CAP_CSUM6)
		ifp->if_capabilities |= IFCAP_CSUM_TCPv6 | IFCAP_CSUM_UDPv6;

	IFQ_SET_MAXLEN(&ifp->if_snd, XNF_TX_DESC - 1);

	ifmedia_init(&sc->sc_media, IFM_IMASK, xnf_media_change,
	    xnf_media_status);
	ifmedia_add(&sc->sc_media, IFM_ETHER | IFM_MANUAL, 0, NULL);
	ifmedia_set(&sc->sc_media, IFM_ETHER | IFM_MANUAL);

	if_attach(ifp);
	ether_ifattach(ifp);

	/* Kick out emulated em's and re's */
	xen_unplug_emulated(parent, XEN_UNPLUG_NIC);
}

int
xnf_detach(struct device *self, int flags)
{
	struct xnf_softc *sc = (struct xnf_softc *)self;
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	xnf_stop(sc);

	ether_ifdetach(ifp);
	if_detach(ifp);

	xen_intr_disestablish(sc->sc_xih);

	if (sc->sc_tx_ring)
		xnf_tx_ring_destroy(sc);
	if (sc->sc_rx_ring)
		xnf_rx_ring_destroy(sc);

	return (0);
}

static int
nibble(int ch)
{
	if (ch >= '0' && ch <= '9')
		return (ch - '0');
	if (ch >= 'A' && ch <= 'F')
		return (10 + ch - 'A');
	if (ch >= 'a' && ch <= 'f')
		return (10 + ch - 'a');
	return (-1);
}

int
xnf_lladdr(struct xnf_softc *sc)
{
	char enaddr[ETHER_ADDR_LEN];
	char mac[32];
	int i, j, lo, hi;

	if (xs_getprop(sc->sc_parent, sc->sc_backend, "mac", mac, sizeof(mac)))
		return (-1);

	for (i = 0, j = 0; j < ETHER_ADDR_LEN; i += 3, j++) {
		if ((hi = nibble(mac[i])) == -1 ||
		    (lo = nibble(mac[i+1])) == -1)
			return (-1);
		enaddr[j] = hi << 4 | lo;
	}

	memcpy(sc->sc_ac.ac_enaddr, enaddr, ETHER_ADDR_LEN);
	return (0);
}

int
xnf_ioctl(struct ifnet *ifp, u_long command, caddr_t data)
{
	struct xnf_softc *sc = ifp->if_softc;
	struct ifreq *ifr = (struct ifreq *)data;
	int s, error = 0;

	s = splnet();

	switch (command) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			xnf_init(sc);
		break;
	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				xnf_init(sc);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				xnf_stop(sc);
		}
		break;
	case SIOCGIFMEDIA:
	case SIOCSIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &sc->sc_media, command);
		break;
	default:
		error = ether_ioctl(ifp, &sc->sc_ac, command, data);
		break;
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			xnf_iff(sc);
		error = 0;
	}

	splx(s);

	return (error);
}

int
xnf_media_change(struct ifnet *ifp)
{
	return (0);
}

void
xnf_media_status(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	ifmr->ifm_status = IFM_ACTIVE | IFM_AVALID;
	ifmr->ifm_active = IFM_ETHER | IFM_MANUAL;
}

int
xnf_iff(struct xnf_softc *sc)
{
	return (0);
}

void
xnf_init(struct xnf_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	xnf_stop(sc);

	xnf_iff(sc);

	if (xen_intr_unmask(sc->sc_xih)) {
		printf("%s: failed to enable interrupts\n", ifp->if_xname);
		xnf_stop(sc);
		return;
	}

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
}

void
xnf_stop(struct xnf_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	ifp->if_flags &= ~IFF_RUNNING;

	xen_intr_mask(sc->sc_xih);

	ifp->if_timer = 0;

	ifq_barrier(&ifp->if_snd);
	xen_intr_barrier(sc->sc_xih);

	ifq_clr_oactive(&ifp->if_snd);

	if (sc->sc_tx_ring)
		xnf_tx_ring_drain(sc);
	if (sc->sc_rx_ring)
		xnf_rx_ring_drain(sc);
}

void
xnf_start(struct ifqueue *ifq)
{
	struct ifnet *ifp = ifq->ifq_if;
	struct xnf_softc *sc = ifp->if_softc;
	struct xnf_tx_ring *txr = sc->sc_tx_ring;
	struct mbuf *m;
	int pkts = 0;
	uint32_t prod, oprod;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
	    BUS_DMASYNC_POSTREAD);

	prod = oprod = txr->txr_prod;

	for (;;) {
		if ((XNF_TX_DESC - (prod - sc->sc_tx_cons)) <
		    sc->sc_tx_frags) {
			/* transient */
			ifq_set_oactive(ifq);
			break;
		}

		m = ifq_dequeue(ifq);
		if (m == NULL)
			break;

		if (xnf_encap(sc, m, &prod)) {
			/* the chain is too large */
			ifp->if_oerrors++;
			m_freem(m);
			continue;
		}

#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
		pkts++;
	}
	if (pkts > 0) {
		txr->txr_prod = prod;
		if (txr->txr_cons_event <= txr->txr_cons)
			txr->txr_cons_event = txr->txr_cons +
			    ((txr->txr_prod - txr->txr_cons) >> 1) + 1;
		bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
		if (prod - txr->txr_prod_event < prod - oprod)
			xen_intr_signal(sc->sc_xih);
		ifp->if_timer = 5;
	}
}

static inline int
xnf_fragcount(struct mbuf *m_head)
{
	struct mbuf *m;
	vaddr_t va, va0;
	int n = 0;

	for (m = m_head; m != NULL; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		     /* start of the buffer */
		for (va0 = va = mtod(m, vaddr_t);
		     /* does the buffer end on this page? */
		     va + (PAGE_SIZE - (va & PAGE_MASK)) < va0 + m->m_len;
		     /* move on to the next page */
		     va += PAGE_SIZE - (va & PAGE_MASK))
			n++;
		n++;
	}
	return (n);
}

int
xnf_encap(struct xnf_softc *sc, struct mbuf *m_head, uint32_t *prod)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct xnf_tx_ring *txr = sc->sc_tx_ring;
	union xnf_tx_desc *txd;
	struct mbuf *m;
	bus_dmamap_t dmap;
	uint32_t oprod = *prod;
	int i, id, flags, n;

	if ((xnf_fragcount(m_head) > sc->sc_tx_frags) &&
	    m_defrag(m_head, M_DONTWAIT))
		goto errout;

	for (m = m_head; m != NULL && m->m_len > 0; m = m->m_next) {
		i = *prod & (XNF_TX_DESC - 1);
		dmap = sc->sc_tx_dmap[i];
		txd = &txr->txr_desc[i];
		if (sc->sc_tx_buf[i])
			panic("%s: cons %u(%u) prod %u next %u seg %d/%d\n",
			    ifp->if_xname, txr->txr_cons, sc->sc_tx_cons,
			    txr->txr_prod, *prod, *prod - oprod,
			    xnf_fragcount(m_head));

		flags = (sc->sc_domid << 16) | BUS_DMA_WRITE | BUS_DMA_WAITOK;
		if (bus_dmamap_load(sc->sc_dmat, dmap, m->m_data, m->m_len,
		    NULL, flags)) {
			DPRINTF("%s: failed to load %u bytes @@%lu\n",
			    sc->sc_dev.dv_xname, m->m_len,
			    mtod(m, vaddr_t) & PAGE_MASK);
			goto unroll;
		}

		if (m == m_head) {
			if (m->m_pkthdr.csum_flags &
			    (M_TCP_CSUM_OUT | M_UDP_CSUM_OUT))
				txd->txd_req.txq_flags = XNF_TXF_CSUM_BLANK |
				    XNF_TXF_CSUM_VALID;
			txd->txd_req.txq_size = m->m_pkthdr.len;
		}
		for (n = 0; n < dmap->dm_nsegs; n++) {
			i = *prod & (XNF_TX_DESC - 1);
			txd = &txr->txr_desc[i];
			if (sc->sc_tx_buf[i])
				panic("%s: cons %u(%u) prod %u next %u "
				    "seg %d/%d\n", ifp->if_xname,
				    txr->txr_cons, sc->sc_tx_cons,
				    txr->txr_prod, *prod, *prod - oprod,
				    xnf_fragcount(m_head));

			/* Don't overwrite lenght of the very first one */
			if (!(m == m_head && n == 0))
				txd->txd_req.txq_size = dmap->dm_segs[n].ds_len;
			/* The chunk flag will be removed from the last one */
			txd->txd_req.txq_flags |= XNF_TXF_CHUNK;
			txd->txd_req.txq_ref = dmap->dm_segs[n].ds_addr;
			if (n == 0)
				txd->txd_req.txq_offset =
				    mtod(m, vaddr_t) & PAGE_MASK;
			(*prod)++;
		}
	}
	/* Clear the chunk flag from the last segment */
	txd->txd_req.txq_flags &= ~XNF_TXF_CHUNK;
	sc->sc_tx_buf[i] = m_head;
	bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
	    BUS_DMASYNC_PREWRITE);

	return (0);

 unroll:
	for (; *prod != oprod; (*prod)--) {
		i = (*prod - 1) & (XNF_TX_DESC - 1);
		dmap = sc->sc_tx_dmap[i];
		txd = &txr->txr_desc[i];

		id = txd->txd_rsp.txp_id;
		memset(txd, 0, sizeof(*txd));
		txd->txd_req.txq_id = id;

		bus_dmamap_sync(sc->sc_dmat, dmap, 0, 0,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, dmap);

		if (sc->sc_tx_buf[i])
			sc->sc_tx_buf[i] = NULL;
	}

 errout:
	return (ENOBUFS);
}

void
xnf_intr(void *arg)
{
	struct xnf_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	if (ifp->if_flags & IFF_RUNNING) {
		xnf_txeof(sc);
		xnf_rxeof(sc);
	}
}

void
xnf_watchdog(struct ifnet *ifp)
{
	struct xnf_softc *sc = ifp->if_softc;
	struct xnf_tx_ring *txr = sc->sc_tx_ring;

	printf("%s: tx stuck: prod %u cons %u,%u evt %u,%u\n",
	    ifp->if_xname, txr->txr_prod, txr->txr_cons, sc->sc_tx_cons,
	    txr->txr_prod_event, txr->txr_cons_event);
}

void
xnf_txeof(struct xnf_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct xnf_tx_ring *txr = sc->sc_tx_ring;
	union xnf_tx_desc *txd;
	bus_dmamap_t dmap;
	uint32_t cons;
	int i, id;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
	    BUS_DMASYNC_POSTWRITE);

	for (cons = sc->sc_tx_cons; cons != txr->txr_cons; cons++) {
		i = cons & (XNF_TX_DESC - 1);
		txd = &txr->txr_desc[i];
		dmap = sc->sc_tx_dmap[i];

		id = txd->txd_rsp.txp_id;
		memset(txd, 0, sizeof(*txd));
		txd->txd_req.txq_id = id;

		bus_dmamap_sync(sc->sc_dmat, dmap, 0, 0,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, dmap);

		if (sc->sc_tx_buf[i] != NULL) {
			m_freem(sc->sc_tx_buf[i]);
			sc->sc_tx_buf[i] = NULL;
		}
	}

	sc->sc_tx_cons = cons;
	txr->txr_cons_event = sc->sc_tx_cons +
	    ((txr->txr_prod - sc->sc_tx_cons) >> 1) + 1;
	bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	if (txr->txr_cons == txr->txr_prod)
		ifp->if_timer = 0;
	if (ifq_is_oactive(&ifp->if_snd))
		ifq_restart(&ifp->if_snd);
}

void
xnf_rxeof(struct xnf_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct xnf_rx_ring *rxr = sc->sc_rx_ring;
	union xnf_rx_desc *rxd;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *fmp = sc->sc_rx_cbuf[0];
	struct mbuf *lmp = sc->sc_rx_cbuf[1];
	struct mbuf *m;
	bus_dmamap_t dmap;
	uint32_t cons;
	int i, id, flags, len, offset;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_rx_rmap, 0, 0,
	    BUS_DMASYNC_POSTREAD);

	for (cons = sc->sc_rx_cons; cons != rxr->rxr_cons; cons++) {
		i = cons & (XNF_RX_DESC - 1);
		rxd = &rxr->rxr_desc[i];
		dmap = sc->sc_rx_dmap[i];

		len = rxd->rxd_rsp.rxp_status;
		flags = rxd->rxd_rsp.rxp_flags;
		offset = rxd->rxd_rsp.rxp_offset;
		id = rxd->rxd_rsp.rxp_id;
		memset(rxd, 0, sizeof(*rxd));
		rxd->rxd_req.rxq_id = id;

		bus_dmamap_sync(sc->sc_dmat, dmap, 0, 0,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, dmap);

		m = sc->sc_rx_buf[i];
		KASSERT(m != NULL);
		sc->sc_rx_buf[i] = NULL;

		if (flags & XNF_RXF_MGMT) {
			printf("%s: management data present\n",
			    ifp->if_xname);
			m_freem(m);
			continue;
		}

		if (flags & XNF_RXF_CSUM_VALID)
			m->m_pkthdr.csum_flags = M_TCP_CSUM_IN_OK |
			    M_UDP_CSUM_IN_OK;

		if (len < 0 || (len + offset > PAGE_SIZE)) {
			ifp->if_ierrors++;
			m_freem(m);
			continue;
		}

		m->m_len = len;
		m->m_data += offset;

		if (fmp == NULL) {
			m->m_pkthdr.len = len;
			fmp = m;
		} else {
			m->m_flags &= ~M_PKTHDR;
			lmp->m_next = m;
			fmp->m_pkthdr.len += m->m_len;
		}
		lmp = m;

		if (flags & XNF_RXF_CHUNK) {
			sc->sc_rx_cbuf[0] = fmp;
			sc->sc_rx_cbuf[1] = lmp;
			continue;
		}

		m = fmp;

		ml_enqueue(&ml, m);
		sc->sc_rx_cbuf[0] = sc->sc_rx_cbuf[1] = fmp = lmp = NULL;
	}

	sc->sc_rx_cons = cons;
	rxr->rxr_cons_event = sc->sc_rx_cons + 1;
	bus_dmamap_sync(sc->sc_dmat, sc->sc_rx_rmap, 0, 0,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	if_input(ifp, &ml);

	if (xnf_rx_ring_fill(sc) || (sc->sc_rx_cons != rxr->rxr_cons))
		xen_intr_schedule(sc->sc_xih);
}

int
xnf_rx_ring_fill(struct xnf_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct xnf_rx_ring *rxr = sc->sc_rx_ring;
	bus_dmamap_t dmap;
	struct mbuf *m;
	uint32_t cons, prod, oprod;
	int i, flags, resched = 0;

	cons = rxr->rxr_cons;
	prod = oprod = rxr->rxr_prod;

	while (prod - cons < XNF_RX_DESC) {
		i = prod & (XNF_RX_DESC - 1);
		if (sc->sc_rx_buf[i])
			break;
		m = MCLGETI(NULL, M_DONTWAIT, NULL, XNF_MCLEN);
		if (m == NULL)
			break;
		m->m_len = m->m_pkthdr.len = XNF_MCLEN;
		dmap = sc->sc_rx_dmap[i];
		flags = (sc->sc_domid << 16) | BUS_DMA_READ | BUS_DMA_NOWAIT;
		if (bus_dmamap_load_mbuf(sc->sc_dmat, dmap, m, flags)) {
			m_freem(m);
			break;
		}
		sc->sc_rx_buf[i] = m;
		rxr->rxr_desc[i].rxd_req.rxq_ref = dmap->dm_segs[0].ds_addr;
		bus_dmamap_sync(sc->sc_dmat, dmap, 0, 0, BUS_DMASYNC_PREWRITE);
		prod++;
	}

	rxr->rxr_prod = prod;
	bus_dmamap_sync(sc->sc_dmat, sc->sc_rx_rmap, 0, 0,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	if ((prod - cons < XNF_RX_MIN) && (ifp->if_flags & IFF_RUNNING))
		resched = 1;
	if (prod - rxr->rxr_prod_event < prod - oprod)
		xen_intr_signal(sc->sc_xih);

	return (resched);
}

int
xnf_rx_ring_create(struct xnf_softc *sc)
{
	int i, flags, rsegs;

	/* Allocate a page of memory for the ring */
	if (bus_dmamem_alloc(sc->sc_dmat, PAGE_SIZE, PAGE_SIZE, 0,
	    &sc->sc_rx_seg, 1, &rsegs, BUS_DMA_ZERO | BUS_DMA_NOWAIT)) {
		printf("%s: failed to allocate memory for the rx ring\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}
	/* Map in the allocated memory into the ring structure */
	if (bus_dmamem_map(sc->sc_dmat, &sc->sc_rx_seg, 1, PAGE_SIZE,
	    (caddr_t *)(&sc->sc_rx_ring), BUS_DMA_NOWAIT)) {
		printf("%s: failed to map memory for the rx ring\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}
	/* Create a map to load the ring memory into */
	if (bus_dmamap_create(sc->sc_dmat, PAGE_SIZE, 1, PAGE_SIZE, 0,
	    BUS_DMA_NOWAIT, &sc->sc_rx_rmap)) {
		printf("%s: failed to create a memory map for the rx ring\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}
	/* Load the ring into the ring map to extract the PA */
	flags = (sc->sc_domid << 16) | BUS_DMA_NOWAIT;
	if (bus_dmamap_load(sc->sc_dmat, sc->sc_rx_rmap, sc->sc_rx_ring,
	    PAGE_SIZE, NULL, flags)) {
		printf("%s: failed to load the rx ring map\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}
	sc->sc_rx_ref = sc->sc_rx_rmap->dm_segs[0].ds_addr;

	sc->sc_rx_ring->rxr_prod_event = sc->sc_rx_ring->rxr_cons_event = 1;

	for (i = 0; i < XNF_RX_DESC; i++) {
		if (bus_dmamap_create(sc->sc_dmat, XNF_MCLEN, 1, XNF_MCLEN,
		    PAGE_SIZE, BUS_DMA_NOWAIT, &sc->sc_rx_dmap[i])) {
			printf("%s: failed to create a memory map for the"
			    " rx slot %d\n", sc->sc_dev.dv_xname, i);
			goto errout;
		}
		sc->sc_rx_ring->rxr_desc[i].rxd_req.rxq_id = i;
	}

	xnf_rx_ring_fill(sc);

	return (0);

 errout:
	xnf_rx_ring_destroy(sc);
	return (-1);
}

void
xnf_rx_ring_drain(struct xnf_softc *sc)
{
	struct xnf_rx_ring *rxr = sc->sc_rx_ring;

	if (sc->sc_rx_cons != rxr->rxr_cons)
		xnf_rxeof(sc);
}

void
xnf_rx_ring_destroy(struct xnf_softc *sc)
{
	int i;

	for (i = 0; i < XNF_RX_DESC; i++) {
		if (sc->sc_rx_buf[i] == NULL)
			continue;
		bus_dmamap_sync(sc->sc_dmat, sc->sc_rx_dmap[i], 0, 0,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, sc->sc_rx_dmap[i]);
		m_freem(sc->sc_rx_buf[i]);
		sc->sc_rx_buf[i] = NULL;
	}

	for (i = 0; i < XNF_RX_DESC; i++) {
		if (sc->sc_rx_dmap[i] == NULL)
			continue;
		bus_dmamap_destroy(sc->sc_dmat, sc->sc_rx_dmap[i]);
		sc->sc_rx_dmap[i] = NULL;
	}
	if (sc->sc_rx_rmap) {
		bus_dmamap_sync(sc->sc_dmat, sc->sc_rx_rmap, 0, 0,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, sc->sc_rx_rmap);
		bus_dmamap_destroy(sc->sc_dmat, sc->sc_rx_rmap);
	}
	if (sc->sc_rx_ring) {
		bus_dmamem_unmap(sc->sc_dmat, (caddr_t)sc->sc_rx_ring,
		    PAGE_SIZE);
		bus_dmamem_free(sc->sc_dmat, &sc->sc_rx_seg, 1);
	}
	sc->sc_rx_ring = NULL;
	sc->sc_rx_rmap = NULL;
	sc->sc_rx_cons = 0;
}

int
xnf_tx_ring_create(struct xnf_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int i, flags, nsegs, rsegs;
	bus_size_t segsz;

	sc->sc_tx_frags = sc->sc_caps & XNF_CAP_SG ? XNF_TX_FRAG : 1;

	/* Allocate a page of memory for the ring */
	if (bus_dmamem_alloc(sc->sc_dmat, PAGE_SIZE, PAGE_SIZE, 0,
	    &sc->sc_tx_seg, 1, &rsegs, BUS_DMA_ZERO | BUS_DMA_NOWAIT)) {
		printf("%s: failed to allocate memory for the tx ring\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}
	/* Map in the allocated memory into the ring structure */
	if (bus_dmamem_map(sc->sc_dmat, &sc->sc_tx_seg, 1, PAGE_SIZE,
	    (caddr_t *)&sc->sc_tx_ring, BUS_DMA_NOWAIT)) {
		printf("%s: failed to map memory for the tx ring\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}
	/* Create a map to load the ring memory into */
	if (bus_dmamap_create(sc->sc_dmat, PAGE_SIZE, 1, PAGE_SIZE, 0,
	    BUS_DMA_NOWAIT, &sc->sc_tx_rmap)) {
		printf("%s: failed to create a memory map for the tx ring\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}
	/* Load the ring into the ring map to extract the PA */
	flags = (sc->sc_domid << 16) | BUS_DMA_NOWAIT;
	if (bus_dmamap_load(sc->sc_dmat, sc->sc_tx_rmap, sc->sc_tx_ring,
	    PAGE_SIZE, NULL, flags)) {
		printf("%s: failed to load the tx ring map\n",
		    sc->sc_dev.dv_xname);
		goto errout;
	}
	sc->sc_tx_ref = sc->sc_tx_rmap->dm_segs[0].ds_addr;

	sc->sc_tx_ring->txr_prod_event = sc->sc_tx_ring->txr_cons_event = 1;

	if (sc->sc_caps & XNF_CAP_SG) {
		nsegs = roundup(ifp->if_hardmtu, XNF_MCLEN) / XNF_MCLEN + 1;
		segsz = nsegs * XNF_MCLEN;
	} else {
		nsegs = 1;
		segsz = XNF_MCLEN;
	}
	for (i = 0; i < XNF_TX_DESC; i++) {
		if (bus_dmamap_create(sc->sc_dmat, segsz, nsegs, XNF_MCLEN,
		    PAGE_SIZE, BUS_DMA_NOWAIT, &sc->sc_tx_dmap[i])) {
			printf("%s: failed to create a memory map for the"
			    " tx slot %d\n", sc->sc_dev.dv_xname, i);
			goto errout;
		}
		sc->sc_tx_ring->txr_desc[i].txd_req.txq_id = i;
	}

	return (0);

 errout:
	xnf_tx_ring_destroy(sc);
	return (-1);
}

void
xnf_tx_ring_drain(struct xnf_softc *sc)
{
	struct xnf_tx_ring *txr = sc->sc_tx_ring;

	if (sc->sc_tx_cons != txr->txr_cons)
		xnf_txeof(sc);
}

void
xnf_tx_ring_destroy(struct xnf_softc *sc)
{
	int i;

	for (i = 0; i < XNF_TX_DESC; i++) {
		if (sc->sc_tx_dmap[i] == NULL)
			continue;
		bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_dmap[i], 0, 0,
		    BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, sc->sc_tx_dmap[i]);
		bus_dmamap_destroy(sc->sc_dmat, sc->sc_tx_dmap[i]);
		sc->sc_tx_dmap[i] = NULL;
		if (sc->sc_tx_buf[i] == NULL)
			continue;
		m_free(sc->sc_tx_buf[i]);
		sc->sc_tx_buf[i] = NULL;
	}
	if (sc->sc_tx_rmap) {
		bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(sc->sc_dmat, sc->sc_tx_rmap);
		bus_dmamap_destroy(sc->sc_dmat, sc->sc_tx_rmap);
	}
	if (sc->sc_tx_ring) {
		bus_dmamem_unmap(sc->sc_dmat, (caddr_t)sc->sc_tx_ring,
		    PAGE_SIZE);
		bus_dmamem_free(sc->sc_dmat, &sc->sc_tx_seg, 1);
	}
	sc->sc_tx_ring = NULL;
	sc->sc_tx_rmap = NULL;
}

int
xnf_capabilities(struct xnf_softc *sc)
{
	unsigned long long res;
	const char *prop;
	char val[32];
	int error;

	/* Query scatter-gather capability */
	prop = "feature-sg";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps |= XNF_CAP_SG;

	/* Query IPv4 checksum offloading capability, enabled by default */
	sc->sc_caps |= XNF_CAP_CSUM4;
	prop = "feature-no-csum-offload";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps &= ~XNF_CAP_CSUM4;

	/* Query IPv6 checksum offloading capability */
	prop = "feature-ipv6-csum-offload";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps |= XNF_CAP_CSUM6;

	/* Query multicast traffic contol capability */
	prop = "feature-multicast-control";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps |= XNF_CAP_MCAST;

	/* Query split Rx/Tx event channel capability */
	prop = "feature-split-event-channels";
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0 && res == 1)
		sc->sc_caps |= XNF_CAP_SPLIT;

	/* Query multiqueue capability */
	prop = "multi-queue-max-queues";
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) == 0)
	if ((error = xs_getnum(sc->sc_parent, sc->sc_backend, prop, &res)) != 0
	    && error != ENOENT)
		goto errout;
	if (error == 0)
		sc->sc_caps |= XNF_CAP_MULTIQ;

	DPRINTF("%s: capabilities %b\n", sc->sc_dev.dv_xname, sc->sc_caps,
	    "\20\006MULTIQ\005SPLIT\004MCAST\003CSUM6\002CSUM4\001SG");
	return (0);

 errout:
	printf("%s: failed to read \"%s\" property\n", sc->sc_dev.dv_xname,
	    prop);
	return (-1);
}

int
xnf_init_backend(struct xnf_softc *sc)
{
	const char *prop;

	/* Plumb the Rx ring */
	prop = "rx-ring-ref";
	if (xs_setnum(sc->sc_parent, sc->sc_node, prop, sc->sc_rx_ref))
		goto errout;
	/* Enable "copy" mode */
	prop = "request-rx-copy";
	if (xs_setnum(sc->sc_parent, sc->sc_node, prop, 1))
		goto errout;
	/* Enable notify mode */
	prop = "feature-rx-notify";
	if (xs_setnum(sc->sc_parent, sc->sc_node, prop, 1))
		goto errout;

	/* Plumb the Tx ring */
	prop = "tx-ring-ref";
	if (xs_setnum(sc->sc_parent, sc->sc_node, prop, sc->sc_tx_ref))
		goto errout;
	/* Enable scatter-gather mode */
	if (sc->sc_tx_frags > 1) {
		prop = "feature-sg";
		if (xs_setnum(sc->sc_parent, sc->sc_node, prop, 1))
			goto errout;
	}

	/* Enable IPv6 checksum offloading */
	if (sc->sc_caps & XNF_CAP_CSUM6) {
		prop = "feature-ipv6-csum-offload";
		if (xs_setnum(sc->sc_parent, sc->sc_node, prop, 1))
			goto errout;
	}

	/* Plumb the event channel port */
	prop = "event-channel";
	if (xs_setnum(sc->sc_parent, sc->sc_node, prop, sc->sc_xih))
		goto errout;

	/* Connect the device */
	prop = "state";
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, XEN_STATE_CONNECTED,
	    strlen(XEN_STATE_CONNECTED)))
		goto errout;

	return (0);

 errout:
	printf("%s: failed to set \"%s\" property\n", sc->sc_dev.dv_xname, prop);
	return (-1);
}
@


1.53
log
@Fixup format string and type issues found by cppcheck
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.52 2017/03/09 20:15:36 mikeb Exp $	*/
d589 1
a589 1
			DPRINTF("%s: failed to load %d bytes @@%lu\n",
@


1.52
log
@Fix an off by one when updating the TX consumer event index

The transmit completion notification is posted when the consumer index
becomes equal to the consumer event index. The code attempted to save
up on an update if the current value of the consumer index was below
its event index, but incorrectly handled the situation when they were
equal: the consumer event index wouldn't be advanced and the ring would
stall.

With help from Jan Schreiber who asked some good questions.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.51 2017/02/24 16:58:12 mikeb Exp $	*/
d180 1
a180 1
	int			 sc_rx_cons;
d190 1
a190 1
	int			 sc_tx_cons;
d369 1
a369 1
	for (i = 0, j = 0; j < ETHER_ADDR_LEN; i += 3) {
d373 1
a373 1
		enaddr[j++] = hi << 4 | lo;
d925 1
a925 1
	int i, slots = 0;
a934 1
		slots++;
@


1.51
log
@Update license

Some final touches before the release, increase the maximum
number of CAS iterations before we declare the grant table
entry lost forever.  This happens on older Xen 3.x versions
as reported by Kirill Miazine.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.50 2017/02/08 16:17:31 mikeb Exp $	*/
d528 1
a528 1
		if (txr->txr_cons_event < txr->txr_cons)
@


1.50
log
@Switch to Xen interrupt barrier
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.49 2017/02/06 21:43:48 mikeb Exp $	*/
d272 1
a272 1
	printf(" backend %d chan %u: address %s\n", sc->sc_domid,
@


1.49
log
@Use separate compile time debug flags for xen, xnf and xbf
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.48 2017/01/24 03:57:35 dlg Exp $	*/
d476 1
a476 1
	intr_barrier(&sc->sc_xih);
@


1.48
log
@add support for multiple transmit ifqueues per network interface.

an ifq to transmit a packet is picked by the current traffic
conditioner (ie, priq or hfsc) by providing an index into an array
of ifqs. by default interfaces get a single ifq but can ask for
more using if_attach_queues().

the vast majority of our drivers still think there's a 1:1 mapping
between interfaces and transmit queues, so their if_start routines
take an ifnet pointer instead of a pointer to the ifqueue struct.
instead of changing all the drivers in the tree, drivers can opt
into using an if_qstart routine and setting the IFXF_MPSAFE flag.
the stack provides a compatability wrapper from the new if_qstart
handler to the previous if_start handlers if IFXF_MPSAFE isnt set.

enabling hfsc on an interface configures it to transmit everything
through the first ifq. any other ifqs are left configured as priq,
but unused, when hfsc is enabled.

getting this in now so everyone can kick the tyres.

ok mpi@@ visa@@ (who provided some tweaks for cnmac).
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.47 2017/01/22 10:17:39 dlg Exp $	*/
d56 7
@


1.47
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.46 2017/01/05 13:23:51 mikeb Exp $	*/
d202 1
a202 1
void	xnf_start(struct ifnet *);
d295 1
a295 1
	ifp->if_start = xnf_start;
d480 1
a480 1
xnf_start(struct ifnet *ifp)
d482 1
a488 3
	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;

d498 1
a498 1
			ifq_set_oactive(&ifp->if_snd);
d501 2
a502 1
		m = ifq_dequeue(&ifp->if_snd);
@


1.46
log
@Checking whether mbuf list is empty is done by the if_input now
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.45 2016/12/19 21:08:57 mikeb Exp $	*/
a512 1
		ifp->if_opackets++;
@


1.45
log
@Implement interface detaching
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.44 2016/12/13 21:01:46 mikeb Exp $	*/
d800 1
a800 2
	if (!ml_empty(&ml))
		if_input(ifp, &ml);
@


1.44
log
@Use new xs_{get,set}num functions instead of hand-rolled weirdness
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.43 2016/12/02 15:12:41 mikeb Exp $	*/
d194 1
d223 1
a223 1
	sizeof(struct xnf_softc), xnf_match, xnf_attach
d317 21
@


1.43
log
@Sinc rings are created during attach memory allocations shouldn't sleep
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.42 2016/11/29 14:55:04 mikeb Exp $	*/
d264 1
a264 1
	printf(": backend %d, event channel %u, address %s\n", sc->sc_domid,
d1045 1
d1052 2
a1053 5
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) == 0) {
		if (val[0] == '1')
			sc->sc_caps |= XNF_CAP_SG;
	} else if (error != ENOENT)
d1055 2
d1061 2
a1062 5
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) == 0) {
		if (val[0] == '1')
			sc->sc_caps &= ~XNF_CAP_CSUM4;
	} else if (error != ENOENT)
d1064 2
d1069 2
a1070 5
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) == 0) {
		if (val[0] == '1')
			sc->sc_caps |= XNF_CAP_CSUM6;
	} else if (error != ENOENT)
d1072 2
d1077 2
a1078 5
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) == 0) {
		if (val[0] == '1')
			sc->sc_caps |= XNF_CAP_MCAST;
	} else if (error != ENOENT)
d1080 2
d1085 2
a1086 5
	if ((error = xs_getprop(sc->sc_parent, sc->sc_backend, prop, val,
	    sizeof(val))) == 0) {
		if (val[0] == '1')
			sc->sc_caps |= XNF_CAP_SPLIT;
	} else if (error != ENOENT)
d1088 2
d1095 4
a1099 2
	else if (error != ENOENT)
		goto errout;
a1114 1
	char val[32];
d1118 1
a1118 2
	snprintf(val, sizeof(val), "%u", sc->sc_rx_ref);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d1122 1
a1122 2
	snprintf(val, sizeof(val), "%u", 1);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d1126 1
a1126 2
	snprintf(val, sizeof(val), "%u", 1);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d1131 1
a1131 2
	snprintf(val, sizeof(val), "%u", sc->sc_tx_ref);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d1136 1
a1136 3
		snprintf(val, sizeof(val), "%u", 1);
		if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val,
		    strlen(val)))
d1143 1
a1143 3
		snprintf(val, sizeof(val), "%u", 1);
		if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val,
		    strlen(val)))
d1149 1
a1149 2
	snprintf(val, sizeof(val), "%u", sc->sc_xih);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d1154 2
a1155 2
	snprintf(val, sizeof(val), "%u", 4);
	if (xs_setprop(sc->sc_parent, sc->sc_node, prop, val, strlen(val)))
d1161 1
a1161 2
	printf("%s: failed to set \"%s\" property to \"%s\"\n",
	    sc->sc_dev.dv_xname, prop, val);
@


1.42
log
@Stop exposing xen_softc to PV devices directly
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.41 2016/10/06 17:02:22 mikeb Exp $	*/
d837 1
a837 1
	    &sc->sc_rx_seg, 1, &rsegs, BUS_DMA_ZERO | BUS_DMA_WAITOK)) {
d844 1
a844 1
	    (caddr_t *)(&sc->sc_rx_ring), BUS_DMA_WAITOK)) {
d851 1
a851 1
	    BUS_DMA_WAITOK, &sc->sc_rx_rmap)) {
d857 1
a857 1
	flags = (sc->sc_domid << 16) | BUS_DMA_WAITOK;
d870 1
a870 1
		    PAGE_SIZE, BUS_DMA_WAITOK, &sc->sc_rx_dmap[i])) {
d945 1
a945 1
	    &sc->sc_tx_seg, 1, &rsegs, BUS_DMA_ZERO | BUS_DMA_WAITOK)) {
d952 1
a952 1
	    (caddr_t *)&sc->sc_tx_ring, BUS_DMA_WAITOK)) {
d959 1
a959 1
	    BUS_DMA_WAITOK, &sc->sc_tx_rmap)) {
d965 1
a965 1
	flags = (sc->sc_domid << 16) | BUS_DMA_WAITOK;
d985 1
a985 1
		    PAGE_SIZE, BUS_DMA_WAITOK, &sc->sc_tx_dmap[i])) {
@


1.41
log
@Fold the bus_dmamap_destroy into the loop above
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.40 2016/10/06 17:00:25 mikeb Exp $	*/
d152 3
a154 2
	struct xen_attach_args	 sc_xa;
	struct xen_softc	*sc_xen;
d243 1
a243 2
	sc->sc_xa = *xa;
	sc->sc_xen = xa->xa_parent;
d247 3
d315 1
a315 1
	sc->sc_xen->sc_flags |= XSF_UNPLUG_NIC;
d337 1
a337 2
	if (xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend, "mac",
	    mac, sizeof(mac)))
d1051 2
a1052 2
	if ((error = xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend,
	    prop, val, sizeof(val))) == 0) {
d1061 2
a1062 2
	if ((error = xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend,
	    prop, val, sizeof(val))) == 0) {
d1070 2
a1071 2
	if ((error = xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend,
	    prop, val, sizeof(val))) == 0) {
d1079 2
a1080 2
	if ((error = xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend,
	    prop, val, sizeof(val))) == 0) {
d1088 2
a1089 2
	if ((error = xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend,
	    prop, val, sizeof(val))) == 0) {
d1097 2
a1098 2
	if ((error = xs_getprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_backend,
	    prop, val, sizeof(val))) == 0)
d1122 1
a1122 2
	if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop, val,
	    strlen(val)))
d1127 1
a1127 2
	if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop, val,
	    strlen(val)))
d1132 1
a1132 2
	if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop, val,
	    strlen(val)))
d1138 1
a1138 2
	if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop, val,
	    strlen(val)))
d1144 2
a1145 2
		if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop,
		    val, strlen(val)))
d1153 2
a1154 2
		if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop,
		    val, strlen(val)))
d1161 1
a1161 2
	if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop, val,
	    strlen(val)))
d1167 1
a1167 2
	if (xs_setprop(sc->sc_xa.xa_parent, sc->sc_xa.xa_node, prop, val,
	    strlen(val)))
@


1.40
log
@Remove _ds_boundary abuse (again)

The logic behind this change is this: a single mbuf may reference
only a contiguous chunk of memory.  When this chunk crosses a page
boundary only the first part of it has a non-zero offset while all
other chunks start at the beginning of the page.

We take advantage of this fact and calculate the offset of a first
chunk as a simple "mtod(m, vaddr_t) & PAGE_MASK".
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.39 2016/09/13 10:16:22 mikeb Exp $	*/
d1018 2
a1023 6
	}
	for (i = 0; i < XNF_TX_DESC; i++) {
		if (sc->sc_tx_dmap[i] == NULL)
			continue;
		bus_dmamap_destroy(sc->sc_dmat, sc->sc_tx_dmap[i]);
		sc->sc_tx_dmap[i] = NULL;
@


1.39
log
@Raise maximum supported MTU value to 9000; discussed with reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.38 2016/09/12 18:55:18 mikeb Exp $	*/
d589 3
a591 1
			txd->txd_req.txq_offset = dmap->dm_segs[n].ds_offset;
@


1.38
log
@Correctly account for fragments larger than a page size

Fix a subtle calculation bug that prevented larger fragments from
being properly broken down into pages and overflowing the counter
value.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.37 2016/09/12 17:32:00 mikeb Exp $	*/
d270 1
a270 1
		ifp->if_hardmtu = XNF_MCLEN - ETHER_HDR_LEN;
@


1.37
log
@Add support for packets spanning multiple pages.

The size of TX fragment DMA maps is increased to fit a contiguous
buffer of up to MTU size bytes spanning several pages, e.g. 2 for
4000, 4 for 9000 and so on.

This change also changes the function that calculates an mbuf
chain length so that it tells the driver exactly how many
descriptors will be required in order to provide references to
all buffers within the mbuf chain.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.36 2016/09/12 17:17:12 mikeb Exp $	*/
d514 1
a514 1
	vaddr_t va;
d521 1
a521 1
		for (va = mtod(m, vaddr_t);
d523 1
a523 1
		     va + (PAGE_SIZE - (va & PAGE_MASK)) < va + m->m_len;
@


1.36
log
@Record mbuf chain head rather than individual fragments
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.35 2016/09/12 17:15:53 mikeb Exp $	*/
d269 3
a294 3
	if (sc->sc_caps & XNF_CAP_SG)
		ifp->if_hardmtu = XNF_MCLEN - ETHER_HDR_LEN;

d511 1
a511 1
chainlen(struct mbuf *m_head)
d514 1
d517 10
a526 1
	for (m = m_head; m != NULL && m->m_len > 0; m = m->m_next)
d528 1
d541 1
a541 1
	int i, id, flags;
d543 1
a543 1
	if ((chainlen(m_head) > sc->sc_tx_frags) &&
a550 1

d555 1
a555 1
			    chainlen(m_head));
d559 4
a562 1
		    NULL, flags))
d564 1
d572 15
a586 4
		} else
			txd->txd_req.txq_size = dmap->dm_segs[0].ds_len;

		if (m->m_next != NULL)
d588 4
a591 4

		txd->txd_req.txq_ref = dmap->dm_segs[0].ds_addr;
		txd->txd_req.txq_offset = mtod(m, vaddr_t) & PAGE_MASK;
		(*prod)++;
d593 2
d933 3
a935 1
	int i, flags, rsegs;
d972 7
d980 1
a980 1
		if (bus_dmamap_create(sc->sc_dmat, XNF_MCLEN, 1, XNF_MCLEN,
@


1.35
log
@Skip empty mbuf fragments like bus_dmamap_load_mbuf does
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.34 2016/08/29 17:35:25 mikeb Exp $	*/
a565 1
		sc->sc_tx_buf[i] = m;
d568 1
a624 1
	struct mbuf *m;
a640 4
		m = sc->sc_tx_buf[i];
		KASSERT(m != NULL);
		sc->sc_tx_buf[i] = NULL;

d644 5
a648 1
		m_free(m);
@


1.34
log
@Set MTU size to ~4k until the TX path is ready to deal with larger packets
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.33 2016/08/29 17:27:04 mikeb Exp $	*/
d516 1
a516 1
	for (m = m_head; m != NULL; m = m->m_next)
d536 1
a536 1
	for (m = m_head; m != NULL; m = m->m_next) {
@


1.33
log
@Don't count output errors twice
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.32 2016/08/29 14:12:58 mikeb Exp $	*/
d293 1
a293 1
		ifp->if_hardmtu = 9000;
@


1.32
log
@Fixup packet fragment unrolling procedure

When bus_dmamap_load fails to load one of the buffers in the mbuf
chain, we need to revert all changes to transmit descriptors.  The
code to do that was prototyped but not tested.  However due to how
the Tx ring is set up in xnf(4) and generic lack of proper fragment
support in the Netfront design we're always limited to having 256
entries for distinct shared memory pages.  The mbuf chain is
traversed and attempt is made to load every data chunk into a 4k
sized DMA map segment which makes it impossible to reference a
buffer composed of multiple pages.  Current implementation lacks
support for this preventing reliable transmission of frames larger
than 4k.

Bug reported by Kirill Miazine <km at krot ! org>, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.31 2016/08/03 15:08:06 mikeb Exp $	*/
a592 1
	ifp->if_oerrors++;
@


1.31
log
@Remove the periodic timer and do rescheduling during Rx completion

This change adds a check into the Rx ring completion routine that
schedules an interrupt task to be executed immediately after if
consumer index has already advanced itself.  The benefit of doing
this compared to an additional loop after replenishing the ring
(as done in FreeBSD for example) is that first of all this goes
through the loop in the taskqueue thread with a yeild check to
prevent CPU hogging and second is that it triggers Tx completion
as well since interrupt handler runs both.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.30 2016/08/01 13:48:33 mikeb Exp $	*/
d530 1
a530 1
	int i, id, flags, n = 0;
d532 2
a533 5
	n = chainlen(m_head);
	if (n > sc->sc_tx_frags && m_defrag(m_head, M_DONTWAIT))
		goto errout;
	n = chainlen(m_head);
	if (n > sc->sc_tx_frags)
d544 2
a545 1
			    txr->txr_prod, *prod, n, dmap->dm_nsegs - 1);
d575 2
a576 2
	for (n = oprod; n < *prod; n++) {
		i = *prod & (XNF_TX_DESC - 1);
a583 3
		m = sc->sc_tx_buf[i];
		sc->sc_tx_buf[i] = NULL;

d587 3
a589 1
		m_free(m);
a590 1
	*prod = oprod;
@


1.30
log
@Mark shared producer and consumer indices volatile
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.29 2016/07/29 22:25:28 mikeb Exp $	*/
a160 1
	struct timeout		 sc_timer;
a202 1
void	xnf_timer(void *);
d204 3
a206 3
int	xnf_txeof(struct xnf_softc *);
int	xnf_rxeof(struct xnf_softc *);
void	xnf_rx_ring_fill(void *);
a310 2
	timeout_set(&sc->sc_timer, xnf_timer, sc);

a441 1
	timeout_del(&sc->sc_timer);
d608 1
a609 2
		xnf_txeof(sc);
		timeout_add(&sc->sc_timer, 10);
a613 10
xnf_timer(void *arg)
{
	struct xnf_softc *sc = arg;
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	if (ifp->if_flags & IFF_RUNNING)
		xen_intr_schedule(sc->sc_xih);
}

void
d624 1
a624 1
int
a666 2

	return (0);
d669 1
a669 1
int
a752 2
	xnf_rx_ring_fill(sc);

d756 2
a757 1
	return (0);
d760 2
a761 2
void
xnf_rx_ring_fill(void *arg)
a762 1
	struct xnf_softc *sc = arg;
d768 1
a768 1
	int i, flags;
d798 1
a798 1
		xen_intr_schedule(sc->sc_xih);
d801 2
@


1.29
log
@Add a periodic timer to workaround missing completion events
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.28 2016/07/29 22:01:57 mikeb Exp $	*/
d88 4
a91 4
	uint32_t		 rxr_prod;
	uint32_t		 rxr_prod_event;
	uint32_t		 rxr_cons;
	uint32_t		 rxr_cons_event;
d127 4
a130 4
	uint32_t		 txr_prod;
	uint32_t		 txr_prod_event;
	uint32_t		 txr_cons;
	uint32_t		 txr_cons_event;
@


1.28
log
@Disable receive ring slot accounting

In the Netfront case it appears that by having scheduler decide when
to run the workload we don't need to involve additional mechanisms
to artificially limit resource availability to achieve better
performance under heavy load.  On the contrary, by performing a
single pass through Rx and Tx completion rings and having scheduler
decide when to run the next attempt, we limit the amount of time
spent in the packet processing and achieve system responsiveness.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.27 2016/07/29 18:33:12 mikeb Exp $	*/
d161 1
a179 1
	struct timeout		 sc_rx_fill;
d204 1
d313 1
a313 1
	timeout_set(&sc->sc_rx_fill, xnf_rx_ring_fill, sc);
d446 1
a446 1
	timeout_del(&sc->sc_rx_fill);
d615 1
d620 10
d788 1
a788 3
	int i, flags, s;

	s = splnet();
d817 2
a818 2
	if ((prod - cons < XNF_RX_DESC) && (ifp->if_flags & IFF_RUNNING))
		timeout_add(&sc->sc_rx_fill, 10);
a820 2

	splx(s);
@


1.27
log
@Update TX completion event index when putting a packet on the ring
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.26 2016/07/29 18:31:51 mikeb Exp $	*/
a178 1
	struct if_rxring	 sc_rx_slots;
a381 4
	case SIOCGIFRXR:
		error = if_rxr_ioctl((struct if_rxrinfo *)ifr->ifr_data,
		    NULL, XNF_MCLEN, &sc->sc_rx_slots);
		break;
a711 2
		if_rxr_put(&sc->sc_rx_slots, 1);

d776 1
a776 1
	int i, flags, n, s;
d783 1
a783 11
	n = if_rxr_get(&sc->sc_rx_slots, XNF_RX_DESC);

	/* Less than XNF_RX_MIN slots available? */
	if (n == 0 && prod - cons < XNF_RX_MIN) {
		splx(s);
		if (ifp->if_flags & IFF_RUNNING)
			timeout_add(&sc->sc_rx_fill, 10);
		return;
	}

	for (; n > 0; prod++, n--) {
d800 1
a802 3
	if (n > 0)
		if_rxr_put(&sc->sc_rx_slots, n);

d807 2
a808 7
	if (prod - cons < XNF_RX_MIN) {
		splx(s);
		if (ifp->if_flags & IFF_RUNNING)
			timeout_add(&sc->sc_rx_fill, 10);
		return;
	}

a862 1
	if_rxr_init(&sc->sc_rx_slots, XNF_RX_MIN, XNF_RX_DESC);
a895 2

	if_rxr_put(&sc->sc_rx_slots, slots);
@


1.26
log
@Reduce the amount of sent RX producer notifications; from FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.25 2016/07/29 18:31:22 mikeb Exp $	*/
d508 3
@


1.25
log
@Reduce the amount of sent TX producer notifications; from FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.24 2016/07/28 17:35:13 mikeb Exp $	*/
d779 4
a782 3
	uint32_t cons, prod;
	static int timer = 0;
	int i, flags, n;
d785 1
a785 1
	prod = rxr->rxr_prod;
d791 1
d793 1
a793 3
			timeout_add(&sc->sc_rx_fill, 1 << timer);
		if (timer < 10)
			timer++;
d806 1
a806 1
		flags = (sc->sc_domid << 16) | BUS_DMA_READ |BUS_DMA_NOWAIT;
d823 11
a833 1
	xen_intr_signal(sc->sc_xih);
@


1.24
log
@Remove top level ring processing loops as too ambiguous
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.23 2016/07/28 12:08:14 mikeb Exp $	*/
d471 1
a471 1
	uint32_t prod;
d479 1
a479 1
	prod = txr->txr_prod;
d508 4
a511 1
		xen_intr_signal(sc->sc_xih);
@


1.23
log
@Convert ifq_deq_{begin,rollback,commit} dance to a single ifq_dequeue
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.22 2016/04/19 18:15:41 mikeb Exp $	*/
a634 1
	volatile uint32_t r;
d636 1
a636 1
	int i, id, pkts = 0;
d641 12
a652 20
	do {
		for (cons = sc->sc_tx_cons; cons != txr->txr_cons; cons++) {
			i = cons & (XNF_TX_DESC - 1);
			txd = &txr->txr_desc[i];
			dmap = sc->sc_tx_dmap[i];

			id = txd->txd_rsp.txp_id;
			memset(txd, 0, sizeof(*txd));
			txd->txd_req.txq_id = id;

			m = sc->sc_tx_buf[i];
			KASSERT(m != NULL);
			sc->sc_tx_buf[i] = NULL;

			bus_dmamap_sync(sc->sc_dmat, dmap, 0, 0,
			    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, dmap);
			m_free(m);
			pkts++;
		}
d654 5
a658 8
		if (pkts > 0) {
			sc->sc_tx_cons = cons;
			txr->txr_cons_event = cons +
			    ((txr->txr_prod - cons) >> 1) + 1;
			bus_dmamap_sync(sc->sc_dmat, sc->sc_tx_rmap, 0, 0,
			    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
			pkts = 0;
		}
d660 5
a664 2
		r = txr->txr_cons - sc->sc_tx_cons;
	} while (r > 0);
d666 2
a669 2
	else if (txr->txr_cons == txr->txr_prod)
		ifp->if_timer = 0;
a684 1
	volatile uint32_t r;
d686 1
a686 1
	int i, id, flags, len, offset, pkts = 0;
d691 27
a717 62
	do {
		for (cons = sc->sc_rx_cons; cons != rxr->rxr_cons; cons++) {
			i = cons & (XNF_RX_DESC - 1);
			rxd = &rxr->rxr_desc[i];
			dmap = sc->sc_rx_dmap[i];

			len = rxd->rxd_rsp.rxp_status;
			flags = rxd->rxd_rsp.rxp_flags;
			offset = rxd->rxd_rsp.rxp_offset;
			id = rxd->rxd_rsp.rxp_id;
			memset(rxd, 0, sizeof(*rxd));
			rxd->rxd_req.rxq_id = id;

			bus_dmamap_sync(sc->sc_dmat, dmap, 0, 0,
			    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
			bus_dmamap_unload(sc->sc_dmat, dmap);

			m = sc->sc_rx_buf[i];
			KASSERT(m != NULL);
			sc->sc_rx_buf[i] = NULL;

			if (flags & XNF_RXF_MGMT)
				printf("%s: management data present\n",
				    ifp->if_xname);

			if (flags & XNF_RXF_CSUM_VALID)
				m->m_pkthdr.csum_flags = M_TCP_CSUM_IN_OK |
				    M_UDP_CSUM_IN_OK;

			if_rxr_put(&sc->sc_rx_slots, 1);
			pkts++;

			if (len < 0 || (len + offset > PAGE_SIZE)) {
				ifp->if_ierrors++;
				m_freem(m);
				continue;
			}

			m->m_len = len;
			m->m_data += offset;

			if (fmp == NULL) {
				m->m_pkthdr.len = len;
				fmp = m;
			} else {
				m->m_flags &= ~M_PKTHDR;
				lmp->m_next = m;
				fmp->m_pkthdr.len += m->m_len;
			}
			lmp = m;

			if (flags & XNF_RXF_CHUNK) {
				sc->sc_rx_cbuf[0] = fmp;
				sc->sc_rx_cbuf[1] = lmp;
				continue;
			}

			m = fmp;

			ml_enqueue(&ml, m);
			sc->sc_rx_cbuf[0] = sc->sc_rx_cbuf[1] =
			    fmp = lmp = NULL;
d720 8
a727 6
		if (pkts > 0) {
			sc->sc_rx_cons = cons;
			rxr->rxr_cons_event = cons + 1;
			bus_dmamap_sync(sc->sc_dmat, sc->sc_rx_rmap, 0, 0,
			    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
			pkts = 0;
d730 12
a741 2
		r = rxr->rxr_cons - sc->sc_rx_cons;
	} while (r > 0);
d743 20
a762 1
	if (!ml_empty(&ml)) {
a763 2
		xnf_rx_ring_fill(sc);
	}
@


1.22
log
@Bind event channels to backend domains

This is another piece of the QubesOS "chained VM" puzzle reported by
Marco Peereboom.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.21 2016/04/19 14:19:44 mikeb Exp $	*/
d470 1
a470 1
	int error, pkts = 0;
d482 7
a488 1
		m = ifq_deq_begin(&ifp->if_snd);
d492 1
a492 7
		error = xnf_encap(sc, m, &prod);
		if (error == ENOENT) {
			/* transient */
			ifq_deq_rollback(&ifp->if_snd, m);
			ifq_set_oactive(&ifp->if_snd);
			break;
		} else if (error) {
a494 1
			ifq_deq_commit(&ifp->if_snd, m);
a498 1
		ifq_deq_commit(&ifp->if_snd, m);
a534 2
	if ((XNF_TX_DESC - (*prod - sc->sc_tx_cons)) < sc->sc_tx_frags)
		return (ENOENT);
@


1.21
log
@Allow to grant memory access to domains other than dom0.

Extend xen_grant_table_enter to take an additional "domain" argument
and extract it from the upper part of the bus_dmamap_load flags (sigh.)
to be able to punch it into the grant table entry.

Issue reported by Marco Peereboom who found that we wouldn't run under
QubesOS that "chains" VMs.  He also did the hard work getting the debug
data out of the aforementioned system.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.20 2016/04/19 13:55:19 mikeb Exp $	*/
d256 2
a257 1
	if (xen_intr_establish(0, &sc->sc_xih, xnf_intr, sc, ifp->if_xname)) {
@


1.20
log
@Pass down the backend-id property to children in the attach arguments
and pick it up in xnf(4) and print it in the dmesg line for now. We'll
need to pass it down to the Grant Table code.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.19 2016/04/19 12:39:31 mikeb Exp $	*/
d534 1
a534 1
	int i, id, n = 0;
d555 1
d557 1
a557 1
		    NULL, BUS_DMA_WRITE | BUS_DMA_NOWAIT))
d797 1
a797 1
	int i, n;
d822 2
a823 2
		if (bus_dmamap_load_mbuf(sc->sc_dmat, dmap, m, BUS_DMA_READ |
		    BUS_DMA_NOWAIT)) {
d845 1
a845 1
	int i, rsegs;
d869 1
d871 1
a871 1
	    PAGE_SIZE, NULL, BUS_DMA_WAITOK)) {
d952 1
a952 1
	int i, rsegs;
d978 1
d980 1
a980 1
	    PAGE_SIZE, NULL, BUS_DMA_WAITOK)) {
@


1.19
log
@Remove the ds_offset hack since object offset within a page
is the same for both virtual and physical addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.18 2016/04/13 11:36:00 mpi Exp $	*/
d155 1
d247 1
d262 2
a263 2
	printf(": event channel %u, address %s\n", sc->sc_xih,
	    ether_sprintf(sc->sc_ac.ac_enaddr));
@


1.18
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.17 2016/02/05 10:34:52 mikeb Exp $	*/
d570 1
a570 1
		txd->txd_req.txq_offset = dmap->dm_segs[0].ds_offset;
@


1.17
log
@Simple moderation of Tx completion notifications

Follow FreeBSD and schedule the next Tx completion event to fire
when about half of the packet segments scheduled for transmission
are consumed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.16 2016/01/29 18:49:06 mikeb Exp $	*/
a300 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.16
log
@Cleanup XenStore API

Turns out that we want to let devices choose whether they're issuing
XenStore requests to the backend or frontend.  This also unifies the
the API somewhat as providing the xen softcore structure is now
mandatory.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.15 2016/01/26 17:01:01 mikeb Exp $	*/
d666 2
a667 1
			txr->txr_cons_event = cons + 1;
@


1.15
log
@Convert membar_* operations to bus_dmamap_sync calls
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.14 2016/01/26 16:31:05 mikeb Exp $	*/
d26 2
d30 2
a31 2
#include <sys/kernel.h>
#include <sys/device.h>
d34 1
a34 1
#include <sys/queue.h>
a35 1
#include <sys/pool.h>
d336 2
a337 1
	if (xs_getprop(&sc->sc_xa, "mac", mac, sizeof(mac)))
d1056 2
a1057 1
	if ((error = xs_getprop(&sc->sc_xa, prop, val, sizeof(val))) == 0) {
d1066 2
a1067 1
	if ((error = xs_getprop(&sc->sc_xa, prop, val, sizeof(val))) == 0) {
d1075 2
a1076 1
	if ((error = xs_getprop(&sc->sc_xa, prop, val, sizeof(val))) == 0) {
d1084 2
a1085 1
	if ((error = xs_getprop(&sc->sc_xa, prop, val, sizeof(val))) == 0) {
d1093 2
a1094 1
	if ((error = xs_getprop(&sc->sc_xa, prop, val, sizeof(val))) == 0) {
d1102 2
a1103 1
	if ((error = xs_getprop(&sc->sc_xa, prop, val, sizeof(val))) == 0)
d1127 2
a1128 1
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1133 2
a1134 1
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1139 2
a1140 1
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1146 2
a1147 1
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1153 2
a1154 1
		if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1162 2
a1163 1
		if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1170 2
a1171 1
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
d1177 2
a1178 1
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
@


1.14
log
@Rewrite tx path to use flat transmit ring without fragment chains

Xen doesn't provide transmit fragment chains so initially they were
emulated but amount of grant table entries wasted in the process was
astronomical (9 times more than after this change).  So while code
readability was sacrificed a bit, the change comes with a very nice
transmit performance improvement and taxes grant table references
much less than before.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.13 2016/01/26 16:13:32 mikeb Exp $	*/
d472 3
a475 1
	membar_consumer();
d573 2
d591 2
d638 3
a642 1
			membar_consumer();
a649 1
			membar_producer();
d655 2
a663 1
			membar_producer();
d665 2
a670 1
		membar_consumer();
d696 3
a700 1
			membar_consumer();
a710 1
			membar_producer();
d712 2
a764 1
			membar_producer();
d766 2
a771 1
		membar_consumer();
d824 1
a829 1
	membar_producer();
d831 2
d911 2
d928 2
d1016 2
d1031 2
@


1.13
log
@Rename _{req,rsp}_evt descriptor structure members to _{prod,cons}_event

Setting rxr_ and/or txr_cons_event value allows domU to delay completion
notification for receive and/or transmit ring to specified values of
consumer index.  rxr_ and txr_prod_event values are updated by dom0 and
don't seem to hold any significance for us.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.12 2016/01/25 10:46:54 mikeb Exp $	*/
d488 1
d493 1
d509 11
d521 1
a521 1
xnf_encap(struct xnf_softc *sc, struct mbuf *m, uint32_t *prod)
d526 1
d528 2
a529 1
	int error, i, n = 0;
d533 5
a537 12

	i = *prod & (XNF_TX_DESC - 1);
	dmap = sc->sc_tx_dmap[i];

	error = bus_dmamap_load_mbuf(sc->sc_dmat, dmap, m, BUS_DMA_WRITE |
	    BUS_DMA_NOWAIT);
	if (error == EFBIG) {
		if (m_defrag(m, M_DONTWAIT) ||
		    bus_dmamap_load_mbuf(sc->sc_dmat, dmap, m, BUS_DMA_WRITE |
		     BUS_DMA_NOWAIT))
			goto errout;
	} else if (error)
d540 1
a540 1
	for (n = 0; n < dmap->dm_nsegs; n++, (*prod)++) {
d542 3
d549 6
a554 2
		txd = &txr->txr_desc[i];
		if (n == 0) {
d561 3
a563 2
			txd->txd_req.txq_size = dmap->dm_segs[n].ds_len;
		if (n != dmap->dm_nsegs - 1)
d565 3
a567 2
		txd->txd_req.txq_ref = dmap->dm_segs[n].ds_addr;
		txd->txd_req.txq_offset = dmap->dm_segs[n].ds_offset;
d569 1
a569 1
		m = m->m_next;
a571 1
	ifp->if_opackets++;
d574 18
d594 1
a594 1
	return (error);
d637 2
d643 7
a649 7
			if (sc->sc_tx_buf[i]) {
				dmap = sc->sc_tx_dmap[i];
				bus_dmamap_unload(sc->sc_dmat, dmap);
				m = sc->sc_tx_buf[i];
				sc->sc_tx_buf[i] = NULL;
				m_free(m);
			}
a763 1

d861 2
a862 2
		if (bus_dmamap_create(sc->sc_dmat, XNF_MCLEN, 1,
		    XNF_MCLEN, PAGE_SIZE, BUS_DMA_WAITOK, &sc->sc_rx_dmap[i])) {
d965 2
a966 2
		if (bus_dmamap_create(sc->sc_dmat, XNF_MCLEN, sc->sc_tx_frags,
		    XNF_MCLEN, PAGE_SIZE, BUS_DMA_WAITOK, &sc->sc_tx_dmap[i])) {
@


1.12
log
@Revert the minimum number of Rx ring slots back to 32

Figured out the hard way by Jonathon Sisson <openbsd at j3z ! org>,
thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.11 2016/01/22 19:47:57 mikeb Exp $	*/
d88 1
a88 1
	uint32_t		 rxr_req_evt;
d90 1
a90 1
	uint32_t		 rxr_rsp_evt;
d127 1
a127 1
	uint32_t		 txr_req_evt;
d129 1
a129 1
	uint32_t		 txr_rsp_evt;
d581 1
a581 1
	printf("%s: tx prod %u cons %u,%u evt %u,%u\n",
d583 1
a583 1
	    txr->txr_req_evt, txr->txr_rsp_evt);
d620 1
a620 1
			txr->txr_rsp_evt = cons + 1;
d718 1
a718 1
			rxr->rxr_rsp_evt = cons + 1;
d823 1
a823 1
	sc->sc_rx_ring->rxr_req_evt = sc->sc_rx_ring->rxr_rsp_evt = 1;
d927 1
a927 1
	sc->sc_tx_ring->txr_req_evt = sc->sc_tx_ring->txr_rsp_evt = 1;
@


1.11
log
@Setup interface features based on capabilities provided by the backend

Instead of just setting bits that we think we need, do a better job of
figuring out what's supported by the backend and what's not and what do
we really need.  The following improvements were implemented:
 o  fallback for when scatter gather I/O is not supported by Dom0;
 o  tcp/udp checksum offloading;
 o  larger mtu up to 9000: an experimental feature;
 o  stop requesting multicast control feature that we don't support.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.10 2016/01/22 19:33:30 mikeb Exp $	*/
d84 1
a84 1
#define XNF_RX_MIN		18
@


1.10
log
@Set minimum number of slots on the receive ring to 18

After some experimentation, discussions with Xen folks and pondering Linux
source code, it became clear that most versions of Xen require at least 18
slots available on the receive ring to send an event.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.9 2016/01/20 10:04:32 mikeb Exp $	*/
d70 2
a71 2
#define  XNF_RXF_CSUM		  0x0001
#define  XNF_RXF_BLANK		  0x0002
d104 2
a105 2
#define  XNF_TXF_CSUM		  0x0001
#define  XNF_TXF_VALID		  0x0002
d160 8
d186 1
d213 1
a214 1
int	xnf_stop_backend(struct xnf_softc *);
d262 5
d290 3
d294 4
d516 1
a516 1
	if ((XNF_TX_DESC - (*prod - sc->sc_tx_cons)) < XNF_TX_FRAG)
d540 4
a543 3
			if (0 && m->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
				txd->txd_req.txq_flags = XNF_TXF_CSUM |
				    XNF_TXF_VALID;
d676 3
a678 2
			if (flags & XNF_RXF_CSUM)
				m->m_pkthdr.csum_flags = M_IPV4_CSUM_IN_OK;
d895 2
d930 1
a930 1
		if (bus_dmamap_create(sc->sc_dmat, XNF_MCLEN, XNF_TX_FRAG,
d989 65
a1073 5
	/* Request multicast filtering */
	prop = "request-multicast-control";
	snprintf(val, sizeof(val), "%u", 1);
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
		goto errout;
d1080 7
a1086 5
	/* Enable transmit scatter-gather mode */
	prop = "feature-sg";
	snprintf(val, sizeof(val), "%u", 1);
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
		goto errout;
d1088 7
a1094 15
	/* Disable TCP/UDP checksum offload */
	prop = "feature-csum-offload";
	if (xs_setprop(&sc->sc_xa, prop, NULL, 0))
		goto errout;
	prop = "feature-no-csum-offload";
	snprintf(val, sizeof(val), "%u", 1);
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
		goto errout;
	prop = "feature-ipv6-csum-offload";
	if (xs_setprop(&sc->sc_xa, prop, NULL, 0))
		goto errout;
	prop = "feature-no-ipv6-csum-offload";
	snprintf(val, sizeof(val), "%u", 1);
	if (xs_setprop(&sc->sc_xa, prop, val, strlen(val)))
		goto errout;
@


1.9
log
@Don't bump output errors when when tx ring is full

Reported by Jonathon Sisson <openbsd at j3z ! org>, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.8 2016/01/19 17:16:19 mikeb Exp $	*/
d84 1
a84 1
#define XNF_RX_MIN		32
@


1.8
log
@Mask interrupts on boot, masking/unmasking is handled by stop/init
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.7 2016/01/19 17:13:22 mikeb Exp $	*/
d495 2
a496 4
	if ((XNF_TX_DESC - (*prod - sc->sc_tx_cons)) < XNF_TX_FRAG) {
		error = ENOENT;
		goto errout;
	}
@


1.7
log
@Fix a few issues in the xnf transmit path

A crash reported by Jonathon Sisson is caused by incorrect calculation
of available descriptors on the tx ring.  While here, split the mbuf
chain so that we won't unload the whole thing in the txeof before
removing grant table references from transmit descriptors.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.6 2016/01/15 14:27:08 mikeb Exp $	*/
d248 1
@


1.6
log
@Detach emulated network devices if Netfront driver is enabled

Xen doesn't provide a way for a guest to decide which model of
the interface is selected in the VM configuration and therefore
there's no simple way for Netfront and emulated devices to co-
exist on the same system.  Emulated em(4) or re(4) drivers will
take over if xnf(4) driver is disabled or not compiled in.

Idea and OK reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.5 2016/01/13 20:27:18 mikeb Exp $	*/
d494 1
a494 1
	if (((txr->txr_cons - *prod - 1) & (XNF_TX_DESC - 1)) < XNF_TX_FRAG) {
d515 3
a517 1
			panic("%s: save vs spell: %d\n", ifp->if_xname, i);
a519 1
			sc->sc_tx_buf[i] = m;
d530 2
d591 1
a591 1
				m_freem(m);
d942 1
a942 1
		m_freem(sc->sc_tx_buf[i]);
@


1.5
log
@Bump number of tx fragments to the stock value of 18
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.4 2016/01/13 20:15:54 mikeb Exp $	*/
a218 1
	char type[64];
d223 1
a223 5
	if (xs_getprop(xa, "type", type, sizeof(type)) == 0 &&
	    ((strcmp("vif", type) == 0) || (strcmp("front", type) == 0)))
		return (1);

	return (0);
d289 3
@


1.4
log
@Implement transmit watchdog for testing purposes
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.3 2016/01/13 18:56:26 mikeb Exp $	*/
d123 1
a123 1
#define XNF_TX_FRAG		8	/* down from 18 */
@


1.3
log
@Create rx and tx fragment maps with a page size boundary restriction

We need to ensure that rx and tx fragments do not cross page boundary
since grant table reference can only point to a complete page. Add a
couple of kernel assertions in the dma map loading code to catch these
problems early in the future.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_xnf.c,v 1.2 2016/01/08 14:59:37 reyk Exp $	*/
d194 1
d277 1
d427 1
d483 1
d553 11
d608 2
@


1.2
log
@mikeb@@ doesn't like RCS ids, so I add the OpenBSD one for him.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d787 1
a787 1
		    XNF_MCLEN, 0, BUS_DMA_WAITOK, &sc->sc_rx_dmap[i])) {
d889 1
a889 1
		    XNF_MCLEN, 0, BUS_DMA_WAITOK, &sc->sc_tx_dmap[i])) {
@


1.1
log
@Xen virtual networking interface (Netfront) driver.

Encouragement from deraadt@@, ok reyk
@
text
@d1 2
@

