head	1.90;
access;
symbols
	OPENBSD_6_2:1.90.0.2
	OPENBSD_6_2_BASE:1.90
	OPENBSD_6_1:1.81.0.4
	OPENBSD_6_1_BASE:1.81
	OPENBSD_6_0:1.56.0.2
	OPENBSD_6_0_BASE:1.56
	OPENBSD_5_9:1.50.0.2
	OPENBSD_5_9_BASE:1.50;
locks; strict;
comment	@ * @;


1.90
date	2017.08.10.20.13.57;	author mikeb;	state Exp;
branches;
next	1.89;
commitid	elpXgaDINEZf76nn;

1.89
date	2017.07.21.20.00.47;	author mikeb;	state Exp;
branches;
next	1.88;
commitid	iTN0bek0233vQM72;

1.88
date	2017.07.19.16.48.22;	author mikeb;	state Exp;
branches;
next	1.87;
commitid	mTNjBMSxbMdBvkdc;

1.87
date	2017.07.17.16.32.26;	author mikeb;	state Exp;
branches;
next	1.86;
commitid	MjL32NVFDos9WKj6;

1.86
date	2017.07.17.16.04.31;	author mikeb;	state Exp;
branches;
next	1.85;
commitid	4hniBGsx0bSVKgvr;

1.85
date	2017.07.14.21.53.34;	author mikeb;	state Exp;
branches;
next	1.84;
commitid	gvbpfm4mi1rLIrUm;

1.84
date	2017.07.14.20.08.46;	author mikeb;	state Exp;
branches;
next	1.83;
commitid	dmgdorwonrYN5Y8h;

1.83
date	2017.07.14.19.09.52;	author mikeb;	state Exp;
branches;
next	1.82;
commitid	HOzKysZJg74DQ6ei;

1.82
date	2017.06.02.20.25.50;	author mikeb;	state Exp;
branches;
next	1.81;
commitid	5rE6oCEiqnaicx0h;

1.81
date	2017.03.19.16.55.31;	author mikeb;	state Exp;
branches;
next	1.80;
commitid	KW2zOQ88cUXOE1wn;

1.80
date	2017.03.13.01.00.15;	author mikeb;	state Exp;
branches;
next	1.79;
commitid	knAVpjVB7fgmCUNq;

1.79
date	2017.02.24.16.58.12;	author mikeb;	state Exp;
branches;
next	1.78;
commitid	dd60qwGHamkvdf8J;

1.78
date	2017.02.08.16.15.52;	author mikeb;	state Exp;
branches;
next	1.77;
commitid	gxSiomnJZTQK4vmq;

1.77
date	2017.02.08.16.08.06;	author mikeb;	state Exp;
branches;
next	1.76;
commitid	eEMCKEKIrv4ZMyeb;

1.76
date	2017.02.06.21.58.29;	author mikeb;	state Exp;
branches;
next	1.75;
commitid	pnXIcMYhxO5V6vfF;

1.75
date	2017.02.06.21.52.02;	author mikeb;	state Exp;
branches;
next	1.74;
commitid	uLYJdmDVwCLofDEA;

1.74
date	2017.02.06.21.43.48;	author mikeb;	state Exp;
branches;
next	1.73;
commitid	NscjkKs6gyrHx1os;

1.73
date	2017.01.31.12.17.20;	author mikeb;	state Exp;
branches;
next	1.72;
commitid	ljguyjHBxfl8kTVW;

1.72
date	2017.01.20.16.57.38;	author mikeb;	state Exp;
branches;
next	1.71;
commitid	Cws62j7ebzc9JPcT;

1.71
date	2017.01.10.17.16.39;	author reyk;	state Exp;
branches;
next	1.70;
commitid	P8LvESj9qwBhTMCf;

1.70
date	2016.12.21.12.17.15;	author mikeb;	state Exp;
branches;
next	1.69;
commitid	UjXSBCfwZeftHnab;

1.69
date	2016.12.19.21.07.10;	author mikeb;	state Exp;
branches;
next	1.68;
commitid	Y4USgZxawPwLIGuG;

1.68
date	2016.12.09.17.29.48;	author mikeb;	state Exp;
branches;
next	1.67;
commitid	bUr4SXfFZTI6m6sh;

1.67
date	2016.12.07.15.11.41;	author mikeb;	state Exp;
branches;
next	1.66;
commitid	rCmQfXv5IzCHUyjJ;

1.66
date	2016.11.29.14.55.04;	author mikeb;	state Exp;
branches;
next	1.65;
commitid	tsMjTSHgEUUjCy08;

1.65
date	2016.11.29.13.55.33;	author mikeb;	state Exp;
branches;
next	1.64;
commitid	wUluX7TJ50t0yQ2U;

1.64
date	2016.10.06.17.00.25;	author mikeb;	state Exp;
branches;
next	1.63;
commitid	ZeSFkAmBHZTZgeoK;

1.63
date	2016.09.12.17.22.45;	author mikeb;	state Exp;
branches;
next	1.62;
commitid	X9qrKshOXVVM8uwX;

1.62
date	2016.08.17.17.18.38;	author mikeb;	state Exp;
branches;
next	1.61;
commitid	LrewzMGAAlhEne3g;

1.61
date	2016.08.05.18.31.21;	author mikeb;	state Exp;
branches;
next	1.60;
commitid	1FmFBV7JwFMwb6lh;

1.60
date	2016.08.03.17.14.41;	author mikeb;	state Exp;
branches;
next	1.59;
commitid	eQJxVHPRM6vI49O4;

1.59
date	2016.08.03.14.55.57;	author mikeb;	state Exp;
branches;
next	1.58;
commitid	YaeDVNUrMnvb9jV0;

1.58
date	2016.08.01.14.37.39;	author mikeb;	state Exp;
branches;
next	1.57;
commitid	1wZsb9WETDtQRckf;

1.57
date	2016.07.29.21.27.43;	author mikeb;	state Exp;
branches;
next	1.56;
commitid	qkYps3xdNFEayvHI;

1.56
date	2016.04.28.16.40.10;	author mikeb;	state Exp;
branches;
next	1.55;
commitid	PxHc69KFLJIbNCBp;

1.55
date	2016.04.19.18.15.41;	author mikeb;	state Exp;
branches;
next	1.54;
commitid	3qNUUD4LouZDwEm4;

1.54
date	2016.04.19.14.19.44;	author mikeb;	state Exp;
branches;
next	1.53;
commitid	mAX2wP9YQZzgeS9t;

1.53
date	2016.04.19.13.55.19;	author mikeb;	state Exp;
branches;
next	1.52;
commitid	HxzkQ2JvETeHrLDy;

1.52
date	2016.04.19.12.39.31;	author mikeb;	state Exp;
branches;
next	1.51;
commitid	2egN98MvwADkTMbn;

1.51
date	2016.04.01.15.41.12;	author mikeb;	state Exp;
branches;
next	1.50;
commitid	uRG3UJ3TgUmIrng7;

1.50
date	2016.02.05.10.30.37;	author mikeb;	state Exp;
branches;
next	1.49;
commitid	hC9DmZs6JEBo9krq;

1.49
date	2016.02.02.17.52.46;	author mikeb;	state Exp;
branches;
next	1.48;
commitid	9CqV2Pl3GitdAPFk;

1.48
date	2016.01.29.19.12.26;	author mikeb;	state Exp;
branches;
next	1.47;
commitid	edb9DtZGGKTN3qVe;

1.47
date	2016.01.29.19.04.30;	author mikeb;	state Exp;
branches;
next	1.46;
commitid	JnL9q61QcwW3tbUG;

1.46
date	2016.01.29.18.49.06;	author mikeb;	state Exp;
branches;
next	1.45;
commitid	Pre5L4C8fKqWmy3c;

1.45
date	2016.01.28.11.19.49;	author mikeb;	state Exp;
branches;
next	1.44;
commitid	7p4YI7YYjeebjgx6;

1.44
date	2016.01.27.18.04.42;	author mikeb;	state Exp;
branches;
next	1.43;
commitid	CkT7q5sXwlJlAHQ7;

1.43
date	2016.01.27.15.34.50;	author mikeb;	state Exp;
branches;
next	1.42;
commitid	apVoOQpQMj6Kzcxm;

1.42
date	2016.01.27.15.29.00;	author mikeb;	state Exp;
branches;
next	1.41;
commitid	XGeB963qMsKRfqIr;

1.41
date	2016.01.27.15.27.53;	author mikeb;	state Exp;
branches;
next	1.40;
commitid	8jgBN3WgpfCgaByF;

1.40
date	2016.01.27.09.04.19;	author reyk;	state Exp;
branches;
next	1.39;
commitid	xjeRSSQ0SXSd3nFz;

1.39
date	2016.01.26.15.51.07;	author mikeb;	state Exp;
branches;
next	1.38;
commitid	D3Fy8hdSYQBKz1Nt;

1.38
date	2016.01.26.15.35.21;	author mikeb;	state Exp;
branches;
next	1.37;
commitid	JqQDyNhU1b8Hyadh;

1.37
date	2016.01.26.15.31.02;	author mikeb;	state Exp;
branches;
next	1.36;
commitid	v17soaigz5UgwWCL;

1.36
date	2016.01.26.15.23.11;	author mikeb;	state Exp;
branches;
next	1.35;
commitid	C6BrGzbjkT9jb3XT;

1.35
date	2016.01.25.15.22.56;	author mikeb;	state Exp;
branches;
next	1.34;
commitid	fNd5qUc1O1ALVnQH;

1.34
date	2016.01.23.15.19.02;	author jsg;	state Exp;
branches;
next	1.33;
commitid	bplhh9QYi2SG5Sz3;

1.33
date	2016.01.22.19.28.16;	author mikeb;	state Exp;
branches;
next	1.32;
commitid	WtuQLs81Y1Kjvb0d;

1.32
date	2016.01.22.19.26.40;	author mikeb;	state Exp;
branches;
next	1.31;
commitid	8WKKDjpXSgA8QRPP;

1.31
date	2016.01.19.13.36.00;	author mikeb;	state Exp;
branches;
next	1.30;
commitid	KS9J7IDd2z2AO9va;

1.30
date	2016.01.18.19.09.09;	author mikeb;	state Exp;
branches;
next	1.29;
commitid	xYYXuU3YaxoWS4Uf;

1.29
date	2016.01.18.19.06.48;	author mikeb;	state Exp;
branches;
next	1.28;
commitid	Q6j68J9KFjTHDuDD;

1.28
date	2016.01.15.18.20.41;	author mikeb;	state Exp;
branches;
next	1.27;
commitid	iowr1UutJmouij7D;

1.27
date	2016.01.15.14.27.08;	author mikeb;	state Exp;
branches;
next	1.26;
commitid	dQDkEWTtzV0PvUhk;

1.26
date	2016.01.14.12.37.17;	author mikeb;	state Exp;
branches;
next	1.25;
commitid	FTgBoDZxiEwDKEY0;

1.25
date	2016.01.13.19.09.50;	author mikeb;	state Exp;
branches;
next	1.24;
commitid	hKsHhEWky6Xmpr7Q;

1.24
date	2016.01.13.18.56.26;	author mikeb;	state Exp;
branches;
next	1.23;
commitid	dhpgKk62R9dDSOTB;

1.23
date	2016.01.05.18.03.59;	author mikeb;	state Exp;
branches;
next	1.22;
commitid	OLRiYqTiVIeIpj2C;

1.22
date	2016.01.05.13.47.28;	author mikeb;	state Exp;
branches;
next	1.21;
commitid	nA7maMH5lF997jfw;

1.21
date	2016.01.04.16.07.52;	author mikeb;	state Exp;
branches;
next	1.20;
commitid	NQDMFDF8RF6Jgjdc;

1.20
date	2016.01.04.16.05.43;	author mikeb;	state Exp;
branches;
next	1.19;
commitid	gST5qrnvalmZGhHz;

1.19
date	2015.12.22.22.16.53;	author mikeb;	state Exp;
branches;
next	1.18;
commitid	6HhBROM8QMXkOoyd;

1.18
date	2015.12.21.19.43.16;	author mikeb;	state Exp;
branches;
next	1.17;
commitid	MMVctmSuoodXjEmx;

1.17
date	2015.12.21.18.17.36;	author mikeb;	state Exp;
branches;
next	1.16;
commitid	iEb7l4w8l1M3rzzp;

1.16
date	2015.12.21.18.13.44;	author mikeb;	state Exp;
branches;
next	1.15;
commitid	Boh7zJpHaTnfmHgi;

1.15
date	2015.12.19.09.11.14;	author mikeb;	state Exp;
branches;
next	1.14;
commitid	D1AAzmLsQ1wQBsBn;

1.14
date	2015.12.12.21.07.45;	author reyk;	state Exp;
branches;
next	1.13;
commitid	h86qK9FFdj1rQhW1;

1.13
date	2015.12.12.12.33.49;	author reyk;	state Exp;
branches;
next	1.12;
commitid	BGb3xaQZJ9ACCtNF;

1.12
date	2015.12.11.16.07.01;	author mpi;	state Exp;
branches;
next	1.11;
commitid	fbhqfhfdKxBcsetK;

1.11
date	2015.12.09.14.20.06;	author mikeb;	state Exp;
branches;
next	1.10;
commitid	NhDER9bNMCkf94t4;

1.10
date	2015.12.09.12.49.21;	author mikeb;	state Exp;
branches;
next	1.9;
commitid	XATrYocIqfeE4Scx;

1.9
date	2015.12.09.01.24.06;	author mikeb;	state Exp;
branches;
next	1.8;
commitid	aBQTV958b1NgJ8BA;

1.8
date	2015.12.08.22.23.30;	author mikeb;	state Exp;
branches;
next	1.7;
commitid	UfaLQxXE4MAwWIqK;

1.7
date	2015.12.08.22.18.21;	author mikeb;	state Exp;
branches;
next	1.6;
commitid	hWx75YZezpNFNogl;

1.6
date	2015.12.08.22.14.40;	author mikeb;	state Exp;
branches;
next	1.5;
commitid	TtYskjHMvQp6NRMk;

1.5
date	2015.12.08.20.33.30;	author mikeb;	state Exp;
branches;
next	1.4;
commitid	Z2250xyyYY3JunCq;

1.4
date	2015.12.08.20.07.04;	author mikeb;	state Exp;
branches;
next	1.3;
commitid	rfOMtvGygJd7etQG;

1.3
date	2015.12.08.19.29.22;	author mikeb;	state Exp;
branches;
next	1.2;
commitid	C8vFI0RNH9XPJUKs;

1.2
date	2015.12.08.19.17.00;	author mikeb;	state Exp;
branches;
next	1.1;
commitid	pq3FAYuwXteAsF4d;

1.1
date	2015.12.08.18.46.25;	author mikeb;	state Exp;
branches;
next	;
commitid	Ij2SOB19ATTH0yEx;


desc
@@


1.90
log
@Don't forget to call va_end in xen_hypercall

Coverity CID 1453343
@
text
@/*	$OpenBSD: xen.c,v 1.89 2017/07/21 20:00:47 mikeb Exp $	*/

/*
 * Copyright (c) 2015, 2016, 2017 Mike Belopuhov
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>

/* Xen requires locked atomic operations */
#ifndef MULTIPROCESSOR
#define _XENMPATOMICS
#define MULTIPROCESSOR
#endif
#include <sys/atomic.h>
#ifdef _XENMPATOMICS
#undef MULTIPROCESSOR
#undef _XENMPATOMICS
#endif

#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/signal.h>
#include <sys/signalvar.h>
#include <sys/refcnt.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/stdint.h>
#include <sys/device.h>
#include <sys/task.h>
#include <sys/syslog.h>

#include <machine/bus.h>
#include <machine/cpu.h>
#include <machine/cpufunc.h>

#include <uvm/uvm_extern.h>

#include <machine/i82489var.h>

#include <dev/rndvar.h>

#include <dev/pv/pvvar.h>
#include <dev/pv/pvreg.h>
#include <dev/pv/xenreg.h>
#include <dev/pv/xenvar.h>

/* #define XEN_DEBUG */

#ifdef XEN_DEBUG
#define DPRINTF(x...)		printf(x)
#else
#define DPRINTF(x...)
#endif

struct xen_softc *xen_sc;

int	xen_init_hypercall(struct xen_softc *);
int	xen_getfeatures(struct xen_softc *);
int	xen_init_info_page(struct xen_softc *);
int	xen_init_cbvec(struct xen_softc *);
int	xen_init_interrupts(struct xen_softc *);
void	xen_intr_dispatch(void *);
int	xen_init_grant_tables(struct xen_softc *);
struct xen_gntent *
	xen_grant_table_grow(struct xen_softc *);
int	xen_grant_table_alloc(struct xen_softc *, grant_ref_t *);
void	xen_grant_table_free(struct xen_softc *, grant_ref_t);
void	xen_grant_table_enter(struct xen_softc *, grant_ref_t, paddr_t,
	    int, int);
void	xen_grant_table_remove(struct xen_softc *, grant_ref_t);
void	xen_disable_emulated_devices(struct xen_softc *);

int 	xen_match(struct device *, void *, void *);
void	xen_attach(struct device *, struct device *, void *);
void	xen_deferred(struct device *);
void	xen_control(void *);
void	xen_hotplug(void *);
void	xen_resume(struct device *);
int	xen_activate(struct device *, int);
int	xen_attach_device(struct xen_softc *, struct xen_devlist *,
	    const char *, const char *);
int	xen_probe_devices(struct xen_softc *);

int	xen_bus_dmamap_create(bus_dma_tag_t, bus_size_t, int, bus_size_t,
	    bus_size_t, int, bus_dmamap_t *);
void	xen_bus_dmamap_destroy(bus_dma_tag_t, bus_dmamap_t);
int	xen_bus_dmamap_load(bus_dma_tag_t, bus_dmamap_t, void *, bus_size_t,
	    struct proc *, int);
int	xen_bus_dmamap_load_mbuf(bus_dma_tag_t, bus_dmamap_t, struct mbuf *,
	    int);
void	xen_bus_dmamap_unload(bus_dma_tag_t, bus_dmamap_t);
void	xen_bus_dmamap_sync(bus_dma_tag_t, bus_dmamap_t, bus_addr_t,
	    bus_size_t, int);

int	xs_attach(struct xen_softc *);

struct cfdriver xen_cd = {
	NULL, "xen", DV_DULL
};

const struct cfattach xen_ca = {
	sizeof(struct xen_softc), xen_match, xen_attach, NULL, xen_activate
};

struct bus_dma_tag xen_bus_dma_tag = {
	NULL,
	xen_bus_dmamap_create,
	xen_bus_dmamap_destroy,
	xen_bus_dmamap_load,
	xen_bus_dmamap_load_mbuf,
	NULL,
	NULL,
	xen_bus_dmamap_unload,
	xen_bus_dmamap_sync,
	_bus_dmamem_alloc,
	NULL,
	_bus_dmamem_free,
	_bus_dmamem_map,
	_bus_dmamem_unmap,
	NULL,
};

int
xen_match(struct device *parent, void *match, void *aux)
{
	struct pv_attach_args *pva = aux;
	struct pvbus_hv *hv = &pva->pva_hv[PVBUS_XEN];

	if (hv->hv_base == 0)
		return (0);

	return (1);
}

void
xen_attach(struct device *parent, struct device *self, void *aux)
{
	struct pv_attach_args *pva = (struct pv_attach_args *)aux;
	struct pvbus_hv *hv = &pva->pva_hv[PVBUS_XEN];
	struct xen_softc *sc = (struct xen_softc *)self;

	sc->sc_base = hv->hv_base;
	sc->sc_dmat = pva->pva_dmat;

	if (xen_init_hypercall(sc))
		return;

	/* Wire it up to the global */
	xen_sc = sc;

	if (xen_getfeatures(sc))
		return;

	if (xen_init_info_page(sc))
		return;

	xen_init_cbvec(sc);

	if (xen_init_interrupts(sc))
		return;

	if (xen_init_grant_tables(sc))
		return;

	if (xs_attach(sc))
		return;

	xen_probe_devices(sc);

	/* pvbus(4) key/value interface */
	hv->hv_kvop = xs_kvop;
	hv->hv_arg = sc;

	xen_disable_emulated_devices(sc);

	config_mountroot(self, xen_deferred);
}

void
xen_deferred(struct device *self)
{
	struct xen_softc *sc = (struct xen_softc *)self;

	if (!(sc->sc_flags & XSF_CBVEC)) {
		DPRINTF("%s: callback vector hasn't been established\n",
		    sc->sc_dev.dv_xname);
		return;
	}

	xen_intr_enable();

	if (xs_watch(sc, "control", "shutdown", &sc->sc_ctltsk,
	    xen_control, sc))
		printf("%s: failed to setup shutdown control watch\n",
		    sc->sc_dev.dv_xname);
}

void
xen_control(void *arg)
{
	struct xen_softc *sc = arg;
	struct xs_transaction xst;
	char action[128];
	int error;

	memset(&xst, 0, sizeof(xst));
	xst.xst_id = 0;
	xst.xst_cookie = sc->sc_xs;

	error = xs_getprop(sc, "control", "shutdown", action, sizeof(action));
	if (error) {
		if (error != ENOENT)
			printf("%s: failed to process control event\n",
			    sc->sc_dev.dv_xname);
		return;
	}

	if (strlen(action) == 0)
		return;

	/* Acknowledge the event */
	xs_setprop(sc, "control", "shutdown", "", 0);

	if (strcmp(action, "halt") == 0 || strcmp(action, "poweroff") == 0) {
		pvbus_shutdown(&sc->sc_dev);
	} else if (strcmp(action, "reboot") == 0) {
		pvbus_reboot(&sc->sc_dev);
	} else if (strcmp(action, "crash") == 0) {
		panic("xen told us to do this");
	} else if (strcmp(action, "suspend") == 0) {
		/* Not implemented yet */
	} else {
		printf("%s: unknown shutdown event \"%s\"\n",
		    sc->sc_dev.dv_xname, action);
	}
}

void
xen_resume(struct device *self)
{
}

int
xen_activate(struct device *self, int act)
{
	int rv = 0;

	switch (act) {
	case DVACT_RESUME:
		xen_resume(self);
		break;
	}
	return (rv);
}

int
xen_init_hypercall(struct xen_softc *sc)
{
	extern void *xen_hypercall_page;
	uint32_t regs[4];
	paddr_t pa;

	/* Get hypercall page configuration MSR */
	CPUID(sc->sc_base + CPUID_OFFSET_XEN_HYPERCALL,
	    regs[0], regs[1], regs[2], regs[3]);

	/* We don't support more than one hypercall page */
	if (regs[0] != 1) {
		printf(": requested %u hypercall pages\n", regs[0]);
		return (-1);
	}

	sc->sc_hc = &xen_hypercall_page;

	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_hc, &pa)) {
		printf(": hypercall page PA extraction failed\n");
		return (-1);
	}
	wrmsr(regs[1], pa);

	return (0);
}

int
xen_hypercall(struct xen_softc *sc, int op, int argc, ...)
{
	va_list ap;
	ulong argv[5];
	int i;

	if (argc < 0 || argc > 5)
		return (-1);
	va_start(ap, argc);
	for (i = 0; i < argc; i++)
		argv[i] = (ulong)va_arg(ap, ulong);
	va_end(ap);
	return (xen_hypercallv(sc, op, argc, argv));
}

int
xen_hypercallv(struct xen_softc *sc, int op, int argc, ulong *argv)
{
	ulong hcall;
	int rv = 0;

	hcall = (ulong)sc->sc_hc + op * 32;

#if defined(XEN_DEBUG) && disabled
	{
		int i;

		printf("hypercall %d", op);
		if (argc > 0) {
			printf(", args {");
			for (i = 0; i < argc; i++)
				printf(" %#lx", argv[i]);
			printf(" }\n");
		} else
			printf("\n");
	}
#endif

	switch (argc) {
	case 0: {
		HYPERCALL_RES1;
		__asm__ volatile (			\
			  HYPERCALL_LABEL		\
			: HYPERCALL_OUT1		\
			: HYPERCALL_PTR(hcall)		\
			: HYPERCALL_CLOBBER		\
		);
		HYPERCALL_RET(rv);
		break;
	}
	case 1: {
		HYPERCALL_RES1; HYPERCALL_RES2;
		HYPERCALL_ARG1(argv[0]);
		__asm__ volatile (			\
			  HYPERCALL_LABEL		\
			: HYPERCALL_OUT1 HYPERCALL_OUT2	\
			: HYPERCALL_IN1			\
			, HYPERCALL_PTR(hcall)		\
			: HYPERCALL_CLOBBER		\
		);
		HYPERCALL_RET(rv);
		break;
	}
	case 2: {
		HYPERCALL_RES1; HYPERCALL_RES2; HYPERCALL_RES3;
		HYPERCALL_ARG1(argv[0]); HYPERCALL_ARG2(argv[1]);
		__asm__ volatile (			\
			  HYPERCALL_LABEL		\
			: HYPERCALL_OUT1 HYPERCALL_OUT2	\
			  HYPERCALL_OUT3		\
			: HYPERCALL_IN1	HYPERCALL_IN2	\
			, HYPERCALL_PTR(hcall)		\
			: HYPERCALL_CLOBBER		\
		);
		HYPERCALL_RET(rv);
		break;
	}
	case 3: {
		HYPERCALL_RES1; HYPERCALL_RES2; HYPERCALL_RES3;
		HYPERCALL_RES4;
		HYPERCALL_ARG1(argv[0]); HYPERCALL_ARG2(argv[1]);
		HYPERCALL_ARG3(argv[2]);
		__asm__ volatile (			\
			  HYPERCALL_LABEL		\
			: HYPERCALL_OUT1 HYPERCALL_OUT2	\
			  HYPERCALL_OUT3 HYPERCALL_OUT4	\
			: HYPERCALL_IN1	HYPERCALL_IN2	\
			  HYPERCALL_IN3			\
			, HYPERCALL_PTR(hcall)		\
			: HYPERCALL_CLOBBER		\
		);
		HYPERCALL_RET(rv);
		break;
	}
	case 4: {
		HYPERCALL_RES1; HYPERCALL_RES2; HYPERCALL_RES3;
		HYPERCALL_RES4; HYPERCALL_RES5;
		HYPERCALL_ARG1(argv[0]); HYPERCALL_ARG2(argv[1]);
		HYPERCALL_ARG3(argv[2]); HYPERCALL_ARG4(argv[3]);
		__asm__ volatile (			\
			  HYPERCALL_LABEL		\
			: HYPERCALL_OUT1 HYPERCALL_OUT2	\
			  HYPERCALL_OUT3 HYPERCALL_OUT4	\
			  HYPERCALL_OUT5		\
			: HYPERCALL_IN1	HYPERCALL_IN2	\
			  HYPERCALL_IN3	HYPERCALL_IN4	\
			, HYPERCALL_PTR(hcall)		\
			: HYPERCALL_CLOBBER		\
		);
		HYPERCALL_RET(rv);
		break;
	}
	case 5: {
		HYPERCALL_RES1; HYPERCALL_RES2; HYPERCALL_RES3;
		HYPERCALL_RES4; HYPERCALL_RES5; HYPERCALL_RES6;
		HYPERCALL_ARG1(argv[0]); HYPERCALL_ARG2(argv[1]);
		HYPERCALL_ARG3(argv[2]); HYPERCALL_ARG4(argv[3]);
		HYPERCALL_ARG5(argv[4]);
		__asm__ volatile (			\
			  HYPERCALL_LABEL		\
			: HYPERCALL_OUT1 HYPERCALL_OUT2	\
			  HYPERCALL_OUT3 HYPERCALL_OUT4	\
			  HYPERCALL_OUT5 HYPERCALL_OUT6	\
			: HYPERCALL_IN1	HYPERCALL_IN2	\
			  HYPERCALL_IN3	HYPERCALL_IN4	\
			  HYPERCALL_IN5			\
			, HYPERCALL_PTR(hcall)		\
			: HYPERCALL_CLOBBER		\
		);
		HYPERCALL_RET(rv);
		break;
	}
	default:
		DPRINTF("%s: wrong number of arguments: %d\n", __func__, argc);
		rv = -1;
		break;
	}
	return (rv);
}

int
xen_getfeatures(struct xen_softc *sc)
{
	struct xen_feature_info xfi;

	memset(&xfi, 0, sizeof(xfi));
	if (xen_hypercall(sc, XC_VERSION, 2, XENVER_get_features, &xfi) < 0) {
		printf(": failed to fetch features\n");
		return (-1);
	}
	sc->sc_features = xfi.submap;
#ifdef XEN_DEBUG
	printf(": features %b", sc->sc_features,
	    "\20\014DOM0\013PIRQ\012PVCLOCK\011CBVEC\010GNTFLAGS\007HMA"
	    "\006PTUPD\005PAE4G\004SUPERVISOR\003AUTOPMAP\002WDT\001WPT");
#else
	printf(": features %#x", sc->sc_features);
#endif
	return (0);
}

#ifdef XEN_DEBUG
void
xen_print_info_page(void)
{
	struct xen_softc *sc = xen_sc;
	struct shared_info *s = sc->sc_ipg;
	struct vcpu_info *v;
	int i;

	virtio_membar_sync();
	for (i = 0; i < XEN_LEGACY_MAX_VCPUS; i++) {
		v = &s->vcpu_info[i];
		if (!v->evtchn_upcall_pending && !v->evtchn_upcall_mask &&
		    !v->evtchn_pending_sel && !v->time.version &&
		    !v->time.tsc_timestamp && !v->time.system_time &&
		    !v->time.tsc_to_system_mul && !v->time.tsc_shift)
			continue;
		printf("vcpu%d:\n"
		    "   upcall_pending=%02x upcall_mask=%02x pending_sel=%#lx\n"
		    "   time version=%u tsc=%llu system=%llu\n"
		    "   time mul=%u shift=%d\n",
		    i, v->evtchn_upcall_pending, v->evtchn_upcall_mask,
		    v->evtchn_pending_sel, v->time.version,
		    v->time.tsc_timestamp, v->time.system_time,
		    v->time.tsc_to_system_mul, v->time.tsc_shift);
	}
	printf("pending events: ");
	for (i = 0; i < nitems(s->evtchn_pending); i++) {
		if (s->evtchn_pending[i] == 0)
			continue;
		printf(" %d:%#lx", i, s->evtchn_pending[i]);
	}
	printf("\nmasked events: ");
	for (i = 0; i < nitems(s->evtchn_mask); i++) {
		if (s->evtchn_mask[i] == 0xffffffffffffffffULL)
			continue;
		printf(" %d:%#lx", i, s->evtchn_mask[i]);
	}
	printf("\nwc ver=%u sec=%u nsec=%u\n", s->wc_version, s->wc_sec,
	    s->wc_nsec);
	printf("arch maxpfn=%lu framelist=%lu nmi=%lu\n", s->arch.max_pfn,
	    s->arch.pfn_to_mfn_frame_list, s->arch.nmi_reason);
}
#endif	/* XEN_DEBUG */

int
xen_init_info_page(struct xen_softc *sc)
{
	struct xen_add_to_physmap xatp;
	paddr_t pa;

	sc->sc_ipg = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT | M_ZERO);
	if (sc->sc_ipg == NULL) {
		printf(": failed to allocate shared info page\n");
		return (-1);
	}
	if (!pmap_extract(pmap_kernel(), (vaddr_t)sc->sc_ipg, &pa)) {
		printf(": shared info page PA extraction failed\n");
		free(sc->sc_ipg, M_DEVBUF, PAGE_SIZE);
		return (-1);
	}
	xatp.domid = DOMID_SELF;
	xatp.idx = 0;
	xatp.space = XENMAPSPACE_shared_info;
	xatp.gpfn = atop(pa);
	if (xen_hypercall(sc, XC_MEMORY, 2, XENMEM_add_to_physmap, &xatp)) {
		printf(": failed to register shared info page\n");
		free(sc->sc_ipg, M_DEVBUF, PAGE_SIZE);
		return (-1);
	}
	return (0);
}

int
xen_init_cbvec(struct xen_softc *sc)
{
	struct xen_hvm_param xhp;

	if ((sc->sc_features & XENFEAT_CBVEC) == 0)
		return (ENOENT);

	xhp.domid = DOMID_SELF;
	xhp.index = HVM_PARAM_CALLBACK_IRQ;
	xhp.value = HVM_CALLBACK_VECTOR(LAPIC_XEN_VECTOR);
	if (xen_hypercall(sc, XC_HVM, 2, HVMOP_set_param, &xhp)) {
		/* Will retry with the xspd(4) PCI interrupt */
		return (ENOENT);
	}
	DPRINTF(", idtvec %d", LAPIC_XEN_VECTOR);

	sc->sc_flags |= XSF_CBVEC;

	return (0);
}

int
xen_init_interrupts(struct xen_softc *sc)
{
	int i;

	sc->sc_irq = LAPIC_XEN_VECTOR;

	/*
	 * Clear all pending events and mask all interrupts
	 */
	for (i = 0; i < nitems(sc->sc_ipg->evtchn_pending); i++) {
		sc->sc_ipg->evtchn_pending[i] = 0;
		sc->sc_ipg->evtchn_mask[i] = ~0UL;
	}

	SLIST_INIT(&sc->sc_intrs);

	mtx_init(&sc->sc_islck, IPL_NET);

	return (0);
}

static int
xen_evtchn_hypercall(struct xen_softc *sc, int cmd, void *arg, size_t len)
{
	struct evtchn_op compat;
	int error;

	error = xen_hypercall(sc, XC_EVTCHN, 2, cmd, arg);
	if (error == -ENOXENSYS) {
		memset(&compat, 0, sizeof(compat));
		compat.cmd = cmd;
		memcpy(&compat.u, arg, len);
		error = xen_hypercall(sc, XC_OEVTCHN, 1, &compat);
	}
	return (error);
}

static inline void
xen_intsrc_add(struct xen_softc *sc, struct xen_intsrc *xi)
{
	refcnt_init(&xi->xi_refcnt);
	mtx_enter(&sc->sc_islck);
	SLIST_INSERT_HEAD(&sc->sc_intrs, xi, xi_entry);
	mtx_leave(&sc->sc_islck);
}

static inline struct xen_intsrc *
xen_intsrc_acquire(struct xen_softc *sc, evtchn_port_t port)
{
	struct xen_intsrc *xi;

	mtx_enter(&sc->sc_islck);
	SLIST_FOREACH(xi, &sc->sc_intrs, xi_entry) {
		if (xi->xi_port == port) {
			refcnt_take(&xi->xi_refcnt);
			break;
		}
	}
	mtx_leave(&sc->sc_islck);
	return (xi);
}

static inline void
xen_intsrc_release(struct xen_softc *sc, struct xen_intsrc *xi)
{
	refcnt_rele_wake(&xi->xi_refcnt);
}

static inline struct xen_intsrc *
xen_intsrc_remove(struct xen_softc *sc, evtchn_port_t port)
{
	struct xen_intsrc *xi;

	mtx_enter(&sc->sc_islck);
	SLIST_FOREACH(xi, &sc->sc_intrs, xi_entry) {
		if (xi->xi_port == port) {
			SLIST_REMOVE(&sc->sc_intrs, xi, xen_intsrc, xi_entry);
			break;
		}
	}
	mtx_leave(&sc->sc_islck);
	if (xi != NULL)
		refcnt_finalize(&xi->xi_refcnt, "xenisrm");
	return (xi);
}

static inline void
xen_intr_mask_acquired(struct xen_softc *sc, struct xen_intsrc *xi)
{
	xi->xi_masked = 1;
	set_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]);
}

static inline int
xen_intr_unmask_release(struct xen_softc *sc, struct xen_intsrc *xi)
{
	struct evtchn_unmask eu;

	xi->xi_masked = 0;
	if (!test_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]))
		return (0);
	eu.port = xi->xi_port;
	xen_intsrc_release(sc, xi);
	return (xen_evtchn_hypercall(sc, EVTCHNOP_unmask, &eu, sizeof(eu)));
}

void
xen_intr_ack(void)
{
	struct xen_softc *sc = xen_sc;
	struct shared_info *s = sc->sc_ipg;
	struct cpu_info *ci = curcpu();
	struct vcpu_info *v = &s->vcpu_info[CPU_INFO_UNIT(ci)];

	v->evtchn_upcall_pending = 0;
	virtio_membar_sync();
}

void
xen_intr(void)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi;
	struct shared_info *s = sc->sc_ipg;
	struct cpu_info *ci = curcpu();
	struct vcpu_info *v = &s->vcpu_info[CPU_INFO_UNIT(ci)];
	ulong pending, selector;
	int port, bit, row;

	v->evtchn_upcall_pending = 0;
	selector = atomic_swap_ulong(&v->evtchn_pending_sel, 0);

	for (row = 0; selector > 0; selector >>= 1, row++) {
		if ((selector & 1) == 0)
			continue;
		if ((sc->sc_ipg->evtchn_pending[row] &
		    ~(sc->sc_ipg->evtchn_mask[row])) == 0)
			continue;
		pending = atomic_swap_ulong(&sc->sc_ipg->evtchn_pending[row],
		    0) & ~(sc->sc_ipg->evtchn_mask[row]);
		for (bit = 0; pending > 0; pending >>= 1, bit++) {
			if ((pending & 1) == 0)
				continue;
			port = (row * LONG_BIT) + bit;
			if ((xi = xen_intsrc_acquire(sc, port)) == NULL) {
				printf("%s: unhandled interrupt on port %d\n",
				    sc->sc_dev.dv_xname, port);
				continue;
			}
			xi->xi_evcnt.ec_count++;
			xen_intr_mask_acquired(sc, xi);
			task_add(xi->xi_taskq, &xi->xi_task);
		}
	}
}

void
xen_intr_schedule(xen_intr_handle_t xih)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi;

	if ((xi = xen_intsrc_acquire(sc, (evtchn_port_t)xih)) != NULL)
		task_add(xi->xi_taskq, &xi->xi_task);
}

static void
xen_barrier_task(void *arg)
{
	int *notdone = arg;

	*notdone = 0;
	wakeup_one(notdone);
}

/*
 * This code achieves two goals: 1) makes sure that *after* masking
 * the interrupt source we're not getting more task_adds: intr_barrier
 * will take care of that, and 2) makes sure that the interrupt task
 * has finished executing the current task and won't be called again:
 * it sets up a barrier task to await completion of the current task
 * and relies on the interrupt masking to prevent submission of new
 * tasks in the future.
 */
void
xen_intr_barrier(xen_intr_handle_t xih)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi;
	struct sleep_state sls;
	int notdone = 1;
	struct task t = TASK_INITIALIZER(xen_barrier_task, &notdone);

	/*
	 * XXX This will need to be revised once intr_barrier starts
	 * using its argument.
	 */
	intr_barrier(NULL);

	if ((xi = xen_intsrc_acquire(sc, (evtchn_port_t)xih)) != NULL) {
		task_add(xi->xi_taskq, &t);
		while (notdone) {
			sleep_setup(&sls, &notdone, PWAIT, "xenbar");
			sleep_finish(&sls, notdone);
		}
		xen_intsrc_release(sc, xi);
	}
}

void
xen_intr_signal(xen_intr_handle_t xih)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi;
	struct evtchn_send es;

	if ((xi = xen_intsrc_acquire(sc, (evtchn_port_t)xih)) != NULL) {
		es.port = xi->xi_port;
		xen_intsrc_release(sc, xi);
		xen_evtchn_hypercall(sc, EVTCHNOP_send, &es, sizeof(es));
	}
}

int
xen_intr_establish(evtchn_port_t port, xen_intr_handle_t *xih, int domain,
    void (*handler)(void *), void *arg, char *name)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi;
	struct evtchn_alloc_unbound eau;
#ifdef notyet
	struct evtchn_bind_vcpu ebv;
#endif
#if defined(XEN_DEBUG) && disabled
	struct evtchn_status es;
#endif

	if (port && (xi = xen_intsrc_acquire(sc, port)) != NULL) {
		xen_intsrc_release(sc, xi);
		DPRINTF("%s: interrupt handler has already been established "
		    "for port %u\n", sc->sc_dev.dv_xname, port);
		return (-1);
	}

	xi = malloc(sizeof(*xi), M_DEVBUF, M_NOWAIT | M_ZERO);
	if (xi == NULL)
		return (-1);

	xi->xi_port = (evtchn_port_t)*xih;

	xi->xi_handler = handler;
	xi->xi_ctx = arg;

	xi->xi_taskq = taskq_create(name, 1, IPL_NET, TASKQ_MPSAFE);
	if (!xi->xi_taskq) {
		printf("%s: failed to create interrupt task for %s\n",
		    sc->sc_dev.dv_xname, name);
		free(xi, M_DEVBUF, sizeof(*xi));
		return (-1);
	}
	task_set(&xi->xi_task, xen_intr_dispatch, xi);

	if (port == 0) {
		/* We're being asked to allocate a new event port */
		memset(&eau, 0, sizeof(eau));
		eau.dom = DOMID_SELF;
		eau.remote_dom = domain;
		if (xen_evtchn_hypercall(sc, EVTCHNOP_alloc_unbound, &eau,
		    sizeof(eau)) != 0) {
			DPRINTF("%s: failed to allocate new event port\n",
			    sc->sc_dev.dv_xname);
			free(xi, M_DEVBUF, sizeof(*xi));
			return (-1);
		}
		*xih = xi->xi_port = eau.port;
	} else {
		*xih = xi->xi_port = port;
		/*
		 * The Event Channel API didn't open this port, so it is not
		 * responsible for closing it automatically on unbind.
		 */
		xi->xi_noclose = 1;
	}

#ifdef notyet
	/* Bind interrupt to VCPU#0 */
	memset(&ebv, 0, sizeof(ebv));
	ebv.port = xi->xi_port;
	ebv.vcpu = 0;
	if (xen_evtchn_hypercall(sc, EVTCHNOP_bind_vcpu, &ebv, sizeof(ebv))) {
		printf("%s: failed to bind interrupt on port %u to vcpu%d\n",
		    sc->sc_dev.dv_xname, ebv.port, ebv.vcpu);
	}
#endif

	evcount_attach(&xi->xi_evcnt, name, &sc->sc_irq);

	xen_intsrc_add(sc, xi);

	/* Mask the event port */
	set_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]);

#if defined(XEN_DEBUG) && disabled
	memset(&es, 0, sizeof(es));
	es.dom = DOMID_SELF;
	es.port = xi->xi_port;
	if (xen_evtchn_hypercall(sc, EVTCHNOP_status, &es, sizeof(es))) {
		printf("%s: failed to obtain status for port %d\n",
		    sc->sc_dev.dv_xname, es.port);
	}
	printf("%s: port %u bound to vcpu%u", sc->sc_dev.dv_xname,
	    es.port, es.vcpu);
	if (es.status == EVTCHNSTAT_interdomain)
		printf(": domain %d port %u\n", es.u.interdomain.dom,
		    es.u.interdomain.port);
	else if (es.status == EVTCHNSTAT_unbound)
		printf(": domain %d\n", es.u.unbound.dom);
	else if (es.status == EVTCHNSTAT_pirq)
		printf(": pirq %u\n", es.u.pirq);
	else if (es.status == EVTCHNSTAT_virq)
		printf(": virq %u\n", es.u.virq);
	else
		printf("\n");
#endif

	return (0);
}

int
xen_intr_disestablish(xen_intr_handle_t xih)
{
	struct xen_softc *sc = xen_sc;
	evtchn_port_t port = (evtchn_port_t)xih;
	struct evtchn_close ec;
	struct xen_intsrc *xi;

	if ((xi = xen_intsrc_remove(sc, port)) == NULL)
		return (-1);

	evcount_detach(&xi->xi_evcnt);

	taskq_destroy(xi->xi_taskq);

	set_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]);
	clear_bit(xi->xi_port, &sc->sc_ipg->evtchn_pending[0]);

	if (!xi->xi_noclose) {
		ec.port = xi->xi_port;
		if (xen_evtchn_hypercall(sc, EVTCHNOP_close, &ec, sizeof(ec))) {
			DPRINTF("%s: failed to close event port %u\n",
			    sc->sc_dev.dv_xname, xi->xi_port);
		}
	}

	free(xi, M_DEVBUF, sizeof(*xi));
	return (0);
}

void
xen_intr_dispatch(void *arg)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi = arg;

	if (xi->xi_handler)
		xi->xi_handler(xi->xi_ctx);

	xen_intr_unmask_release(sc, xi);
}

void
xen_intr_enable(void)
{
	struct xen_softc *sc = xen_sc;
	struct xen_intsrc *xi;
	struct evtchn_unmask eu;

	mtx_enter(&sc->sc_islck);
	SLIST_FOREACH(xi, &sc->sc_intrs, xi_entry) {
		if (!xi->xi_masked) {
			eu.port = xi->xi_port;
			if (xen_evtchn_hypercall(sc, EVTCHNOP_unmask, &eu,
			    sizeof(eu)))
				printf("%s: unmasking port %u failed\n",
				    sc->sc_dev.dv_xname, xi->xi_port);
			virtio_membar_sync();
			if (test_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]))
				printf("%s: port %u is still masked\n",
				    sc->sc_dev.dv_xname, xi->xi_port);
		}
	}
	mtx_leave(&sc->sc_islck);
}

void
xen_intr_mask(xen_intr_handle_t xih)
{
	struct xen_softc *sc = xen_sc;
	evtchn_port_t port = (evtchn_port_t)xih;
	struct xen_intsrc *xi;

	if ((xi = xen_intsrc_acquire(sc, port)) != NULL) {
		xen_intr_mask_acquired(sc, xi);
		xen_intsrc_release(sc, xi);
	}
}

int
xen_intr_unmask(xen_intr_handle_t xih)
{
	struct xen_softc *sc = xen_sc;
	evtchn_port_t port = (evtchn_port_t)xih;
	struct xen_intsrc *xi;

	if ((xi = xen_intsrc_acquire(sc, port)) != NULL)
		return (xen_intr_unmask_release(sc, xi));

	return (0);
}

int
xen_init_grant_tables(struct xen_softc *sc)
{
	struct gnttab_query_size gqs;

	gqs.dom = DOMID_SELF;
	if (xen_hypercall(sc, XC_GNTTAB, 3, GNTTABOP_query_size, &gqs, 1)) {
		printf(": failed the query for grant table pages\n");
		return (-1);
	}
	if (gqs.nr_frames == 0 || gqs.nr_frames > gqs.max_nr_frames) {
		printf(": invalid number of grant table pages: %u/%u\n",
		    gqs.nr_frames, gqs.max_nr_frames);
		return (-1);
	}

	sc->sc_gntmax = gqs.max_nr_frames;

	sc->sc_gnt = mallocarray(sc->sc_gntmax + 1, sizeof(struct xen_gntent),
	    M_DEVBUF, M_ZERO | M_NOWAIT);
	if (sc->sc_gnt == NULL) {
		printf(": failed to allocate grant table lookup table\n");
		return (-1);
	}

	mtx_init(&sc->sc_gntlck, IPL_NET);

	if (xen_grant_table_grow(sc) == NULL) {
		free(sc->sc_gnt, M_DEVBUF, sc->sc_gntmax *
		    sizeof(struct xen_gntent));
		return (-1);
	}

	printf(", %d grant table frames", sc->sc_gntmax);

	xen_bus_dma_tag._cookie = sc;

	return (0);
}

struct xen_gntent *
xen_grant_table_grow(struct xen_softc *sc)
{
	struct xen_add_to_physmap xatp;
	struct xen_gntent *ge;
	void *va;
	paddr_t pa;

	if (sc->sc_gntcnt == sc->sc_gntmax) {
		printf("%s: grant table frame allotment limit reached\n",
		    sc->sc_dev.dv_xname);
		return (NULL);
	}

	va = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (va == NULL)
		return (NULL);
	if (!pmap_extract(pmap_kernel(), (vaddr_t)va, &pa)) {
		printf("%s: grant table page PA extraction failed\n",
		    sc->sc_dev.dv_xname);
		km_free(va, PAGE_SIZE, &kv_any, &kp_zero);
		return (NULL);
	}

	mtx_enter(&sc->sc_gntlck);

	ge = &sc->sc_gnt[sc->sc_gntcnt];
	ge->ge_table = va;

	xatp.domid = DOMID_SELF;
	xatp.idx = sc->sc_gntcnt;
	xatp.space = XENMAPSPACE_grant_table;
	xatp.gpfn = atop(pa);
	if (xen_hypercall(sc, XC_MEMORY, 2, XENMEM_add_to_physmap, &xatp)) {
		printf("%s: failed to add a grant table page\n",
		    sc->sc_dev.dv_xname);
		km_free(ge->ge_table, PAGE_SIZE, &kv_any, &kp_zero);
		mtx_leave(&sc->sc_gntlck);
		return (NULL);
	}
	ge->ge_start = sc->sc_gntcnt * GNTTAB_NEPG;
	/* First page has 8 reserved entries */
	ge->ge_reserved = ge->ge_start == 0 ? GNTTAB_NR_RESERVED_ENTRIES : 0;
	ge->ge_free = GNTTAB_NEPG - ge->ge_reserved;
	ge->ge_next = ge->ge_reserved;
	mtx_init(&ge->ge_lock, IPL_NET);

	sc->sc_gntcnt++;
	mtx_leave(&sc->sc_gntlck);

	return (ge);
}

int
xen_grant_table_alloc(struct xen_softc *sc, grant_ref_t *ref)
{
	struct xen_gntent *ge;
	int i;

	/* Start with a previously allocated table page */
	ge = &sc->sc_gnt[sc->sc_gntcnt - 1];
	if (ge->ge_free > 0) {
		mtx_enter(&ge->ge_lock);
		if (ge->ge_free > 0)
			goto search;
		mtx_leave(&ge->ge_lock);
	}

	/* Try other existing table pages */
	for (i = 0; i < sc->sc_gntcnt; i++) {
		ge = &sc->sc_gnt[i];
		if (ge->ge_free == 0)
			continue;
		mtx_enter(&ge->ge_lock);
		if (ge->ge_free > 0)
			goto search;
		mtx_leave(&ge->ge_lock);
	}

 alloc:
	/* Allocate a new table page */
	if ((ge = xen_grant_table_grow(sc)) == NULL)
		return (-1);

	mtx_enter(&ge->ge_lock);
	if (ge->ge_free == 0) {
		/* We were not fast enough... */
		mtx_leave(&ge->ge_lock);
		goto alloc;
	}

 search:
	for (i = ge->ge_next;
	     /* Math works here because GNTTAB_NEPG is a power of 2 */
	     i != ((ge->ge_next + GNTTAB_NEPG - 1) & (GNTTAB_NEPG - 1));
	     i++) {
		if (i == GNTTAB_NEPG)
			i = 0;
		if (ge->ge_reserved && i < ge->ge_reserved)
			continue;
		if (ge->ge_table[i].frame != 0)
			continue;
		*ref = ge->ge_start + i;
		ge->ge_table[i].flags = GTF_invalid;
		ge->ge_table[i].frame = 0xffffffff; /* Mark as taken */
		if ((ge->ge_next = i + 1) == GNTTAB_NEPG)
			ge->ge_next = ge->ge_reserved;
		ge->ge_free--;
		mtx_leave(&ge->ge_lock);
		return (0);
	}
	mtx_leave(&ge->ge_lock);

	panic("page full, sc %p gnt %p (%d) ge %p", sc, sc->sc_gnt,
	    sc->sc_gntcnt, ge);
	return (-1);
}

void
xen_grant_table_free(struct xen_softc *sc, grant_ref_t ref)
{
	struct xen_gntent *ge;

#ifdef XEN_DEBUG
	if (ref > sc->sc_gntcnt * GNTTAB_NEPG)
		panic("unmanaged ref %u sc %p gnt %p (%d)", ref, sc,
		    sc->sc_gnt, sc->sc_gntcnt);
#endif
	ge = &sc->sc_gnt[ref / GNTTAB_NEPG];
	mtx_enter(&ge->ge_lock);
#ifdef XEN_DEBUG
	if (ref < ge->ge_start || ref > ge->ge_start + GNTTAB_NEPG) {
		mtx_leave(&ge->ge_lock);
		panic("out of bounds ref %u ge %p start %u sc %p gnt %p",
		    ref, ge, ge->ge_start, sc, sc->sc_gnt);
	}
#endif
	ref -= ge->ge_start;
	if (ge->ge_table[ref].flags != GTF_invalid) {
		mtx_leave(&ge->ge_lock);
		panic("reference %u is still in use, flags %#x frame %#x",
		    ref + ge->ge_start, ge->ge_table[ref].flags,
		    ge->ge_table[ref].frame);
	}
	ge->ge_table[ref].frame = 0;
	ge->ge_next = ref;
	ge->ge_free++;
	mtx_leave(&ge->ge_lock);
}

void
xen_grant_table_enter(struct xen_softc *sc, grant_ref_t ref, paddr_t pa,
    int domain, int flags)
{
	struct xen_gntent *ge;

#ifdef XEN_DEBUG
	if (ref > sc->sc_gntcnt * GNTTAB_NEPG)
		panic("unmanaged ref %u sc %p gnt %p (%d)", ref, sc,
		    sc->sc_gnt, sc->sc_gntcnt);
#endif
	ge = &sc->sc_gnt[ref / GNTTAB_NEPG];
#ifdef XEN_DEBUG
	if (ref < ge->ge_start || ref > ge->ge_start + GNTTAB_NEPG) {
		panic("out of bounds ref %u ge %p start %u sc %p gnt %p",
		    ref, ge, ge->ge_start, sc, sc->sc_gnt);
	}
#endif
	ref -= ge->ge_start;
	if (ge->ge_table[ref].flags != GTF_invalid) {
		panic("reference %u is still in use, flags %#x frame %#x",
		    ref + ge->ge_start, ge->ge_table[ref].flags,
		    ge->ge_table[ref].frame);
	}
	ge->ge_table[ref].frame = atop(pa);
	ge->ge_table[ref].domid = domain;
	virtio_membar_sync();
	ge->ge_table[ref].flags = GTF_permit_access | flags;
	virtio_membar_sync();
}

void
xen_grant_table_remove(struct xen_softc *sc, grant_ref_t ref)
{
	struct xen_gntent *ge;
	uint32_t flags, *ptr;
	int loop;

#ifdef XEN_DEBUG
	if (ref > sc->sc_gntcnt * GNTTAB_NEPG)
		panic("unmanaged ref %u sc %p gnt %p (%d)", ref, sc,
		    sc->sc_gnt, sc->sc_gntcnt);
#endif
	ge = &sc->sc_gnt[ref / GNTTAB_NEPG];
#ifdef XEN_DEBUG
	if (ref < ge->ge_start || ref > ge->ge_start + GNTTAB_NEPG) {
		panic("out of bounds ref %u ge %p start %u sc %p gnt %p",
		    ref, ge, ge->ge_start, sc, sc->sc_gnt);
	}
#endif
	ref -= ge->ge_start;
	/* Invalidate the grant reference */
	virtio_membar_sync();
	ptr = (uint32_t *)&ge->ge_table[ref];
	flags = (ge->ge_table[ref].flags & ~(GTF_reading|GTF_writing)) |
	    (ge->ge_table[ref].domid << 16);
	loop = 0;
	while (atomic_cas_uint(ptr, flags, GTF_invalid) != flags) {
		if (loop++ > 10) {
			panic("%s: grant table reference %u is held "
			    "by domain %d: frame %#x flags %#x\n",
			    sc->sc_dev.dv_xname, ref + ge->ge_start,
			    ge->ge_table[ref].domid, ge->ge_table[ref].frame,
			    ge->ge_table[ref].flags);
		}
#if (defined(__amd64__) || defined(__i386__))
		__asm volatile("pause": : : "memory");
#endif
	}
	ge->ge_table[ref].frame = 0xffffffff;
}

int
xen_bus_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
    bus_size_t maxsegsz, bus_size_t boundary, int flags, bus_dmamap_t *dmamp)
{
	struct xen_softc *sc = t->_cookie;
	struct xen_gntmap *gm;
	int i, error;

	if (maxsegsz < PAGE_SIZE)
		return (EINVAL);

	/* Allocate a dma map structure */
	error = bus_dmamap_create(sc->sc_dmat, size, nsegments, maxsegsz,
	    boundary, flags, dmamp);
	if (error)
		return (error);
	/* Allocate an array of grant table pa<->ref maps */
	gm = mallocarray(nsegments, sizeof(struct xen_gntmap), M_DEVBUF,
	    M_ZERO | ((flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK));
	if (gm == NULL) {
		bus_dmamap_destroy(sc->sc_dmat, *dmamp);
		*dmamp = NULL;
		return (ENOMEM);
	}
	/* Wire it to the dma map */
	(*dmamp)->_dm_cookie = gm;
	/* Claim references from the grant table */
	for (i = 0; i < (*dmamp)->_dm_segcnt; i++) {
		if (xen_grant_table_alloc(sc, &gm[i].gm_ref)) {
			xen_bus_dmamap_destroy(t, *dmamp);
			*dmamp = NULL;
			return (ENOBUFS);
		}
	}
	return (0);
}

void
xen_bus_dmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{
	struct xen_softc *sc = t->_cookie;
	struct xen_gntmap *gm;
	int i;

	gm = map->_dm_cookie;
	for (i = 0; i < map->_dm_segcnt; i++) {
		if (gm[i].gm_ref == 0)
			continue;
		xen_grant_table_free(sc, gm[i].gm_ref);
	}
	free(gm, M_DEVBUF, map->_dm_segcnt * sizeof(struct xen_gntmap));
	bus_dmamap_destroy(sc->sc_dmat, map);
}

int
xen_bus_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags)
{
	struct xen_softc *sc = t->_cookie;
	struct xen_gntmap *gm = map->_dm_cookie;
	int i, domain, error;

	domain = flags >> 16;
	flags &= 0xffff;
	error = bus_dmamap_load(sc->sc_dmat, map, buf, buflen, p, flags);
	if (error)
		return (error);
	for (i = 0; i < map->dm_nsegs; i++) {
		xen_grant_table_enter(sc, gm[i].gm_ref, map->dm_segs[i].ds_addr,
		    domain, flags & BUS_DMA_WRITE ? GTF_readonly : 0);
		gm[i].gm_paddr = map->dm_segs[i].ds_addr;
		map->dm_segs[i].ds_addr = gm[i].gm_ref;
	}
	return (0);
}

int
xen_bus_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map, struct mbuf *m0,
    int flags)
{
	struct xen_softc *sc = t->_cookie;
	struct xen_gntmap *gm = map->_dm_cookie;
	int i, domain, error;

	domain = flags >> 16;
	flags &= 0xffff;
	error = bus_dmamap_load_mbuf(sc->sc_dmat, map, m0, flags);
	if (error)
		return (error);
	for (i = 0; i < map->dm_nsegs; i++) {
		xen_grant_table_enter(sc, gm[i].gm_ref, map->dm_segs[i].ds_addr,
		    domain, flags & BUS_DMA_WRITE ? GTF_readonly : 0);
		gm[i].gm_paddr = map->dm_segs[i].ds_addr;
		map->dm_segs[i].ds_addr = gm[i].gm_ref;
	}
	return (0);
}

void
xen_bus_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{
	struct xen_softc *sc = t->_cookie;
	struct xen_gntmap *gm = map->_dm_cookie;
	int i;

	for (i = 0; i < map->dm_nsegs; i++) {
		if (gm[i].gm_paddr == 0)
			continue;
		xen_grant_table_remove(sc, gm[i].gm_ref);
		map->dm_segs[i].ds_addr = gm[i].gm_paddr;
		gm[i].gm_paddr = 0;
	}
	bus_dmamap_unload(sc->sc_dmat, map);
}

void
xen_bus_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t addr,
    bus_size_t size, int op)
{
	if ((op == (BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE)) ||
	    (op == (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE)))
		virtio_membar_sync();
}

static int
xen_attach_print(void *aux, const char *name)
{
	struct xen_attach_args *xa = aux;

	if (name)
		printf("\"%s\" at %s: %s", xa->xa_name, name, xa->xa_node);

	return (UNCONF);
}

int
xen_attach_device(struct xen_softc *sc, struct xen_devlist *xdl,
    const char *name, const char *unit)
{
	struct xen_attach_args xa;
	struct xen_device *xdv;
	unsigned long long res;

	xa.xa_dmat = &xen_bus_dma_tag;

	strlcpy(xa.xa_name, name, sizeof(xa.xa_name));
	snprintf(xa.xa_node, sizeof(xa.xa_node), "device/%s/%s", name, unit);

	if (xs_getprop(sc, xa.xa_node, "backend", xa.xa_backend,
	    sizeof(xa.xa_backend))) {
		DPRINTF("%s: failed to identify \"backend\" for "
		    "\"%s\"\n", sc->sc_dev.dv_xname, xa.xa_node);
		return (EIO);
	}

	if (xs_getnum(sc, xa.xa_node, "backend-id", &res) || res > UINT16_MAX) {
		DPRINTF("%s: invalid \"backend-id\" for \"%s\"\n",
		    sc->sc_dev.dv_xname, xa.xa_node);
		return (EIO);
	}
	xa.xa_domid = (uint16_t)res;

	xdv = malloc(sizeof(struct xen_device), M_DEVBUF, M_ZERO | M_NOWAIT);
	if (xdv == NULL)
		return (ENOMEM);

	strlcpy(xdv->dv_unit, unit, sizeof(xdv->dv_unit));
	LIST_INSERT_HEAD(&xdl->dl_devs, xdv, dv_entry);

	xdv->dv_dev = config_found((struct device *)sc, &xa, xen_attach_print);

	return (0);
}

int
xen_probe_devices(struct xen_softc *sc)
{
	struct xen_devlist *xdl;
	struct xs_transaction xst;
	struct iovec *iovp1 = NULL, *iovp2 = NULL;
	int i, j, error, iov1_cnt = 0, iov2_cnt = 0;
	char path[256];

	memset(&xst, 0, sizeof(xst));
	xst.xst_id = 0;
	xst.xst_cookie = sc->sc_xs;

	if ((error = xs_cmd(&xst, XS_LIST, "device", &iovp1, &iov1_cnt)) != 0)
		return (error);

	for (i = 0; i < iov1_cnt; i++) {
		if (strcmp("suspend", (char *)iovp1[i].iov_base) == 0)
			continue;
		snprintf(path, sizeof(path), "device/%s",
		    (char *)iovp1[i].iov_base);
		if ((error = xs_cmd(&xst, XS_LIST, path, &iovp2,
		    &iov2_cnt)) != 0)
			goto out;
		if ((xdl = malloc(sizeof(struct xen_devlist), M_DEVBUF,
		    M_ZERO | M_NOWAIT)) == NULL) {
			error = ENOMEM;
			goto out;
		}
		xdl->dl_xen = sc;
		strlcpy(xdl->dl_node, (const char *)iovp1[i].iov_base,
		    XEN_MAX_NODE_LEN);
		for (j = 0; j < iov2_cnt; j++) {
			error = xen_attach_device(sc, xdl,
			    (const char *)iovp1[i].iov_base,
			    (const char *)iovp2[j].iov_base);
			if (error) {
				printf("%s: failed to attach \"%s/%s\"\n",
				    sc->sc_dev.dv_xname, path,
				    (const char *)iovp2[j].iov_base);
				goto out;
			}
		}
		/* Setup a watch for every device subtree */
		if (xs_watch(sc, "device", (char *)iovp1[i].iov_base,
		    &xdl->dl_task, xen_hotplug, xdl))
			printf("%s: failed to setup hotplug watch for \"%s\"\n",
			    sc->sc_dev.dv_xname, (char *)iovp1[i].iov_base);
		SLIST_INSERT_HEAD(&sc->sc_devlists, xdl, dl_entry);
		xs_resfree(&xst, iovp2, iov2_cnt);
		iovp2 = NULL;
		iov2_cnt = 0;
	}

 out:
	if (iovp2)
		xs_resfree(&xst, iovp2, iov2_cnt);
	xs_resfree(&xst, iovp1, iov1_cnt);
	return (error);
}

void
xen_hotplug(void *arg)
{
	struct xen_devlist *xdl = arg;
	struct xen_softc *sc = xdl->dl_xen;
	struct xen_device *xdv, *xvdn;
	struct xs_transaction xst;
	struct iovec *iovp = NULL;
	int error, i, keep, iov_cnt = 0;
	char path[256];
	int8_t *seen;

	memset(&xst, 0, sizeof(xst));
	xst.xst_id = 0;
	xst.xst_cookie = sc->sc_xs;

	snprintf(path, sizeof(path), "device/%s", xdl->dl_node);
	if ((error = xs_cmd(&xst, XS_LIST, path, &iovp, &iov_cnt)) != 0)
		return;

	seen = malloc(iov_cnt, M_TEMP, M_ZERO | M_WAITOK);

	/* Detect all removed and kept devices */
	LIST_FOREACH_SAFE(xdv, &xdl->dl_devs, dv_entry, xvdn) {
		for (i = 0, keep = 0; i < iov_cnt; i++) {
			if (!seen[i] &&
			    !strcmp(xdv->dv_unit, (char *)iovp[i].iov_base)) {
				seen[i]++;
				keep++;
				break;
			}
		}
		if (!keep) {
			DPRINTF("%s: removing \"%s/%s\"\n", sc->sc_dev.dv_xname,
			    xdl->dl_node, xdv->dv_unit);
			LIST_REMOVE(xdv, dv_entry);
			config_detach(xdv->dv_dev, 0);
			free(xdv, M_DEVBUF, sizeof(struct xen_device));
		}
	}

	/* Attach all new devices */
	for (i = 0; i < iov_cnt; i++) {
		if (seen[i])
			continue;
		DPRINTF("%s: attaching \"%s/%s\"\n", sc->sc_dev.dv_xname,
			    xdl->dl_node, (const char *)iovp[i].iov_base);
		error = xen_attach_device(sc, xdl, xdl->dl_node,
		    (const char *)iovp[i].iov_base);
		if (error) {
			printf("%s: failed to attach \"%s/%s\"\n",
			    sc->sc_dev.dv_xname, path,
			    (const char *)iovp[i].iov_base);
			continue;
		}
	}

	free(seen, M_TEMP, iov_cnt);

	xs_resfree(&xst, iovp, iov_cnt);
}

#include <machine/pio.h>

#define	XMI_PORT		0x10
#define XMI_MAGIC		0x49d2
#define XMI_UNPLUG_IDE		0x01
#define XMI_UNPLUG_NIC		0x02
#define XMI_UNPLUG_IDESEC	0x04

void
xen_disable_emulated_devices(struct xen_softc *sc)
{
#if defined(__i386__) || defined(__amd64__)
	ushort unplug = 0;

	if (inw(XMI_PORT) != XMI_MAGIC) {
		printf("%s: failed to disable emulated devices\n",
		    sc->sc_dev.dv_xname);
		return;
	}
	if (sc->sc_unplug & XEN_UNPLUG_IDE)
		unplug |= XMI_UNPLUG_IDE;
	if (sc->sc_unplug & XEN_UNPLUG_IDESEC)
		unplug |= XMI_UNPLUG_IDESEC;
	if (sc->sc_unplug & XEN_UNPLUG_NIC)
		unplug |= XMI_UNPLUG_NIC;
	if (unplug)
		outw(XMI_PORT, unplug);
#endif	/* __i386__ || __amd64__ */
}

void
xen_unplug_emulated(void *xsc, int what)
{
	struct xen_softc *sc = xsc;

	sc->sc_unplug |= what;
}
@


1.89
log
@Replace MD _bus_dmamap_* function calls with MI ones
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.88 2017/07/19 16:48:22 mikeb Exp $	*/
d308 1
@


1.88
log
@Turn this into a panic since there's no way to recover from it
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.87 2017/07/17 16:32:26 mikeb Exp $	*/
d155 1
d1247 2
a1248 2
	error = _bus_dmamap_create(t, size, nsegments, maxsegsz, boundary,
	    flags, dmamp);
d1255 1
a1255 1
		_bus_dmamap_destroy(t, *dmamp);
d1286 1
a1286 1
	_bus_dmamap_destroy(t, map);
d1299 1
a1299 1
	error = _bus_dmamap_load(t, map, buf, buflen, p, flags);
d1321 1
a1321 1
	error = _bus_dmamap_load_mbuf(t, map, m0, flags);
d1347 1
a1347 1
	_bus_dmamap_unload(t, map);
@


1.87
log
@Forbid overwriting a grant table entry currently in use
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.86 2017/07/17 16:04:31 mikeb Exp $	*/
d1221 5
a1225 4
			printf("%s: grant table reference %u is held "
			    "by domain %d\n", sc->sc_dev.dv_xname, ref +
			    ge->ge_start, ge->ge_table[ref].domid);
			return;
@


1.86
log
@Reduce amount of CAS attempts in a busy-wait loop by a factor of 100
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.85 2017/07/14 21:53:34 mikeb Exp $	*/
d1181 5
@


1.85
log
@Spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.84 2017/07/14 20:08:46 mikeb Exp $	*/
d1215 1
a1215 1
		if (loop++ > 1000) {
@


1.84
log
@Reduce the number of CAS loops from ludicrous to ridiculous

Now that the source of the delay with releasing grant table entries has
been identified and fixed the number of attempts to CAS entry flags can
be substantially reduced and while it's decreased by a factor of 100000,
it should go down at least a 100 more in the future.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.83 2017/07/14 19:09:52 mikeb Exp $	*/
d655 1
a655 2
	return (xen_evtchn_hypercall(sc, EVTCHNOP_unmask, &eu,
	    sizeof(eu)));
@


1.83
log
@Silence the interrupt source until the interrupt task has done its job

This small change significantly improves performance under load and halves
the number of received interrupts.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.82 2017/06/02 20:25:50 mikeb Exp $	*/
d1216 1
a1216 1
		if (loop++ > 100000000) {
@


1.82
log
@Perform grant table page allocation outside of the table mutex

witness(4) has found that km_alloc will trigger an rw_enter via uvm_map
and vm_map_lock.  While rw_enter is called with RW_SLEEPFAIL, there's
also an msleep in there, so it's easier to avoid getting in the middle
of that.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.81 2017/03/19 16:55:31 mikeb Exp $	*/
d74 1
d638 21
d703 1
a704 1
			xen_intsrc_release(sc, xi);
d715 1
a715 1
	if ((xi = xen_intsrc_acquire(sc, (evtchn_port_t)xih)) != NULL) {
a716 2
		xen_intsrc_release(sc, xi);
	}
d803 3
d813 1
a813 1
	task_set(&xi->xi_task, handler, arg);
d912 12
d955 1
a955 2
		xi->xi_masked = 1;
		set_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]);
a965 1
	struct evtchn_unmask eu;
d967 3
a969 9
	if ((xi = xen_intsrc_acquire(sc, port)) != NULL) {
		xi->xi_masked = 0;
		if (!test_bit(xi->xi_port, &sc->sc_ipg->evtchn_mask[0]))
			return (0);
		eu.port = xi->xi_port;
		xen_intsrc_release(sc, xi);
		return (xen_evtchn_hypercall(sc, EVTCHNOP_unmask, &eu,
		    sizeof(eu)));
	}
@


1.81
log
@Improve comments slightly
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.80 2017/03/13 01:00:15 mikeb Exp $	*/
d991 1
d1000 2
a1001 6
	mtx_enter(&sc->sc_gntlck);

	ge = &sc->sc_gnt[sc->sc_gntcnt];
	ge->ge_table = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_nowait);
	if (ge->ge_table == NULL) {
		mtx_leave(&sc->sc_gntlck);
d1003 1
a1003 2
	}
	if (!pmap_extract(pmap_kernel(), (vaddr_t)ge->ge_table, &pa)) {
d1006 1
a1006 2
		km_free(ge->ge_table, PAGE_SIZE, &kv_any, &kp_zero);
		mtx_leave(&sc->sc_gntlck);
d1009 6
@


1.80
log
@Fixup format string and type issues found by cppcheck
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.79 2017/02/24 16:58:12 mikeb Exp $	*/
d712 1
a712 1
 * is finished executing the current task and won't be called again:
d728 1
a728 1
	 * using an argument.
a1087 1
		/* XXX Mark as taken */
d1089 1
a1089 1
		ge->ge_table[i].frame = 0xffffffff;
@


1.79
log
@Update license

Some final touches before the release, increase the maximum
number of CAS iterations before we declare the grant table
entry lost forever.  This happens on older Xen 3.x versions
as reported by Kirill Miazine.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.78 2017/02/08 16:15:52 mikeb Exp $	*/
d279 1
a279 1
		printf(": requested %d hypercall pages\n", regs[0]);
d676 1
a676 1
				printf("%s: unhandled interrupt on port %u\n",
d979 1
a979 1
	printf(", %u grant table frames", sc->sc_gntmax);
@


1.78
log
@Introduce Xen interrupt barriers

intr_barrier(9) is useful to make sure that after an interrupt is
masked, the interrupt handler for the device has finished executing
before proceeding with further device configuration.

However, since Xen interrupt handlers run in the thread context, we
need to make sure that they have finished as well.  By scheduling a
xen_barrier_task modelled after (or rather copied ;) ifq_barrier_task
we can ensure that the interrupt handler is no longer running.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.77 2017/02/08 16:08:06 mikeb Exp $	*/
d4 1
a4 1
 * Copyright (c) 2015 Mike Belopuhov
d1189 1
a1189 1
		if (loop++ > 10000000) {
@


1.77
log
@Fixup incorrect test when allocating grant table entries

An xnf & xbf attach/detach loop has revealed that sometimes when we're
about to free a grant table entry that is still in use which is a grave
mistake code wise.  Turned out we could allocate an entry twice because
of an incorrect test that took flags value into account when making the
decision regarding availability of a given entry.  At the same time,
upon releasing the entry we explicitly CAS in 0 into the flags making
this check bogus.

While here be explicit about starting flags by initializing them to 0
and always panic when the "double free" condition is encountered.

rzalamena@@ has lent me his eyes and has double-checked the condition.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.76 2017/02/06 21:58:29 mikeb Exp $	*/
d695 43
@


1.76
log
@Add proper locking for the interrupt source list

Now that we can attach and detach devices, we need to make sure we
can do so while interrupts are running.  Thankfully, in the meantime
the refcnt_init(9) API came around to help us out.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.75 2017/02/06 21:52:02 mikeb Exp $	*/
d1042 1
a1042 2
		if (ge->ge_table[i].flags != GTF_invalid &&
		    ge->ge_table[i].frame != 0)
d1046 1
d1083 3
a1085 7
#ifdef XEN_DEBUG
		panic("ref %u is still in use, sc %p gnt %p", ref +
		    ge->ge_start, sc, sc->sc_gnt);
#else
		printf("%s: reference %u is still in use\n",
		    sc->sc_dev.dv_xname, ref + ge->ge_start);
#endif
@


1.75
log
@XST_POLL turned out to be pretty useless since it's only set when cold
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.74 2017/02/06 21:43:48 mikeb Exp $	*/
d36 1
d567 2
d588 9
d598 1
a598 1
xen_lookup_intsrc(struct xen_softc *sc, evtchn_port_t port)
d602 4
a605 2
	SLIST_FOREACH(xi, &sc->sc_intrs, xi_entry)
		if (xi->xi_port == port)
d607 27
d675 1
a675 1
			if ((xi = xen_lookup_intsrc(sc, port)) == NULL) {
d682 1
d693 1
a693 1
	if ((xi = xen_lookup_intsrc(sc, (evtchn_port_t)xih)) != NULL)
d695 2
d706 1
a706 1
	if ((xi = xen_lookup_intsrc(sc, (evtchn_port_t)xih)) != NULL) {
d708 1
d727 2
a728 1
	if (port && xen_lookup_intsrc(sc, port)) {
d784 1
a784 1
	SLIST_INSERT_HEAD(&sc->sc_intrs, xi, xi_entry);
d823 1
a823 1
	if ((xi = xen_lookup_intsrc(sc, port)) == NULL)
a827 3
	/* XXX not MP safe */
	SLIST_REMOVE(&sc->sc_intrs, xi, xen_intsrc, xi_entry);

d852 1
d866 1
d876 1
a876 1
	if ((xi = xen_lookup_intsrc(sc, port)) != NULL) {
d879 1
d891 1
a891 1
	if ((xi = xen_lookup_intsrc(sc, port)) != NULL) {
d896 1
d928 1
a928 1
	mtx_init(&sc->sc_gntmtx, IPL_NET);
d956 1
a956 1
	mtx_enter(&sc->sc_gntmtx);
d961 1
a961 1
		mtx_leave(&sc->sc_gntmtx);
d968 1
a968 1
		mtx_leave(&sc->sc_gntmtx);
d979 1
a979 1
		mtx_leave(&sc->sc_gntmtx);
d987 1
a987 1
	mtx_init(&ge->ge_mtx, IPL_NET);
d990 1
a990 1
	mtx_leave(&sc->sc_gntmtx);
d1004 1
a1004 1
		mtx_enter(&ge->ge_mtx);
d1007 1
a1007 1
		mtx_leave(&ge->ge_mtx);
d1015 1
a1015 1
		mtx_enter(&ge->ge_mtx);
d1018 1
a1018 1
		mtx_leave(&ge->ge_mtx);
d1026 1
a1026 1
	mtx_enter(&ge->ge_mtx);
d1029 1
a1029 1
		mtx_leave(&ge->ge_mtx);
d1051 1
a1051 1
		mtx_leave(&ge->ge_mtx);
d1054 1
a1054 1
	mtx_leave(&ge->ge_mtx);
d1072 1
a1072 1
	mtx_enter(&ge->ge_mtx);
d1075 1
a1075 1
		mtx_leave(&ge->ge_mtx);
d1082 1
a1082 1
		mtx_leave(&ge->ge_mtx);
d1094 1
a1094 1
	mtx_leave(&ge->ge_mtx);
@


1.74
log
@Use separate compile time debug flags for xen, xnf and xbf
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.73 2017/01/31 12:17:20 mikeb Exp $	*/
a1301 1
	xst.xst_flags |= XST_POLL;
@


1.73
log
@Issue a pause instruction in the busy-wait loop on SP kernels as well
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.72 2017/01/20 16:57:38 mikeb Exp $	*/
d57 8
@


1.72
log
@Merge two conditional expressions
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.71 2017/01/10 17:16:39 reyk Exp $	*/
d1101 3
a1103 1
		CPU_BUSY_CYCLE();
@


1.71
log
@Introduce pvbus_reboot() and pvbus_shutdown() to move the repeated
tasks from the PV drivers into a central place.  While here, we
figured out that it is not needed to check for allowpowerdown on the
hypervisor-initiated shutdown requests.

OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.70 2016/12/21 12:17:15 mikeb Exp $	*/
d1367 2
a1368 3
			if (seen[i])
				continue;
			if (!strcmp(xdv->dv_unit, (char *)iovp[i].iov_base)) {
@


1.70
log
@Remove the rwlock paranoia since we're under KERNEL_LOCK anyway
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.69 2016/12/19 21:07:10 mikeb Exp $	*/
d226 1
a226 10
		extern int allowpowerdown;

		if (allowpowerdown == 0)
			return;

		suspend_randomness();

		log(LOG_KERN | LOG_NOTICE, "Shutting down in response to "
		    "request from Xen host\n");
		prsignal(initprocess, SIGUSR2);
d228 1
a228 10
		extern int allowpowerdown;

		if (allowpowerdown == 0)
			return;

		suspend_randomness();

		log(LOG_KERN | LOG_NOTICE, "Rebooting in response to request "
		    "from Xen host\n");
		prsignal(initprocess, SIGINT);
@


1.69
log
@Add experimental support for device hot-plugging

We're installing watches on all nodes under "device/" and re-scan
the subtree every time the watch is triggered looking for changes
in the output.  Tested with xnf(4) and xbf(4), helpful hints from
Roger Pau Monne, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.68 2016/12/09 17:29:48 mikeb Exp $	*/
a1311 4
	rw_init(&sc->sc_devlck, "xenprobe");

	rw_enter_write(&sc->sc_devlck);

a1353 2
	rw_exit_write(&sc->sc_devlck);

a1375 2
	rw_enter_write(&sc->sc_devlck);

a1416 2

	rw_exit_write(&sc->sc_devlck);
@


1.68
log
@Convert to the new xs_{get,set}num XenStore API
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.67 2016/12/07 15:11:41 mikeb Exp $	*/
d38 1
d79 1
d82 2
d1260 39
d1301 1
a1301 1
	struct xen_attach_args xa;
a1304 1
	unsigned long long res;
d1312 4
d1325 6
a1330 3
		    &iov2_cnt)) != 0) {
			xs_resfree(&xst, iovp1, iov1_cnt);
			return (error);
d1332 3
d1336 8
a1343 11
			xa.xa_dmat = &xen_bus_dma_tag;
			strlcpy(xa.xa_name, (char *)iovp1[i].iov_base,
			    sizeof(xa.xa_name));
			snprintf(xa.xa_node, sizeof(xa.xa_node), "device/%s/%s",
			    (char *)iovp1[i].iov_base,
			    (char *)iovp2[j].iov_base);
			if (xs_getprop(sc, xa.xa_node, "backend", xa.xa_backend,
			    sizeof(xa.xa_backend))) {
				printf("%s: failed to identify \"backend\" for "
				    "\"%s\"\n", sc->sc_dev.dv_xname, xa.xa_node);
				return (ENODEV);
d1345 54
a1398 5
			if (xs_getnum(sc, xa.xa_node, "backend-id", &res) ||
			    res > 0xffffULL) {
				printf("%s: invalid \"backend-id\" for \"%s\"\n",
				    sc->sc_dev.dv_xname, xa.xa_node);
				return (ENODEV);
a1399 2
			xa.xa_domid = (uint16_t)res;
			config_found((struct device *)sc, &xa, xen_attach_print);
d1401 23
a1423 1
		xs_resfree(&xst, iovp2, iov2_cnt);
d1426 5
a1430 1
	return (0);
@


1.67
log
@Remove some leftovers from before the dynamic grant table code
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.66 2016/11/29 14:55:04 mikeb Exp $	*/
a1244 13
atoi(char *cp, int *res)
{
	*res = 0;
	do {
		if (*cp < '0' || *cp > '9')
			return (-1);
		*res *= 10;
		*res += *cp - '0';
	} while (*(++cp) != '\0');
	return (0);
}

static int
d1261 2
a1262 2
	int i, j, error = 0, iov1_cnt = 0, iov2_cnt = 0;
	char domid[16];
d1290 1
a1290 3
			if (xs_getprop(sc, xa.xa_node, "backend-id", domid,
			    sizeof(domid)) ||
			    xs_getprop(sc, xa.xa_node, "backend", xa.xa_backend,
d1292 9
a1300 7
				printf("%s: failed to identify \"backend\" "
				    "for \"%s\"\n", sc->sc_dev.dv_xname,
				    xa.xa_node);
			} else if (atoi(domid, &xa.xa_domid)) {
				printf("%s: non-numeric backend domain id "
				    "\"%s\" for \"%s\"\n", sc->sc_dev.dv_xname,
				    domid, xa.xa_node);
d1302 2
a1303 2
			config_found((struct device *)sc, &xa,
			    xen_attach_print);
d1308 1
a1308 1
	return (error);
@


1.66
log
@Stop exposing xen_softc to PV devices directly
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.65 2016/11/29 13:55:33 mikeb Exp $	*/
a919 1
		free(ge, M_DEVBUF, sizeof(*ge));
a926 1
		free(ge, M_DEVBUF, sizeof(*ge));
a937 1
		free(ge, M_DEVBUF, sizeof(*ge));
@


1.65
log
@Don't expose the xen_softc pointer in the XenStore transaction struct
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.64 2016/10/06 17:00:25 mikeb Exp $	*/
a1299 1
			xa.xa_parent = sc;
d1346 1
a1346 1
	if (sc->sc_flags & XSF_UNPLUG_IDE)
d1348 1
a1348 1
	if (sc->sc_flags & XSF_UNPLUG_IDESEC)
d1350 1
a1350 1
	if (sc->sc_flags & XSF_UNPLUG_NIC)
d1355 8
@


1.64
log
@Remove _ds_boundary abuse (again)

The logic behind this change is this: a single mbuf may reference
only a contiguous chunk of memory.  When this chunk crosses a page
boundary only the first part of it has a non-zero offset while all
other chunks start at the beginning of the page.

We take advantage of this fact and calculate the offset of a first
chunk as a simple "mtod(m, vaddr_t) & PAGE_MASK".
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.63 2016/09/12 17:22:45 mikeb Exp $	*/
d205 1
a205 1
	xst.xst_sc = sc->sc_xs;
d1283 1
a1283 1
	xst.xst_sc = sc->sc_xs;
@


1.63
log
@Bring back the code that cached DMA fragment offset.

It's required to handle mbuf fragments spanning multiple pages.
Original commit message said:

Memorize the DMA segment's data offset within the page

Grant table references don't convey the information about an actual
offset of the data within the page, however Xen needs to know it.
We (ab)use bus_dma_segment's _ds_boundary member to store it and can
get away with not restoring it's original value atm because neither
i386 nor amd64 bus_dmamap_unload(9) code needs it.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.62 2016/08/17 17:18:38 mikeb Exp $	*/
a1193 2
		map->dm_segs[i].ds_offset = map->dm_segs[i].ds_addr &
		    PAGE_MASK;
a1214 2
		map->dm_segs[i].ds_offset = map->dm_segs[i].ds_addr &
		    PAGE_MASK;
@


1.62
log
@Replace hand rolled atomic bit operations and use MI ones from DRM
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.61 2016/08/05 18:31:21 mikeb Exp $	*/
d1194 2
d1217 2
@


1.61
log
@Switch pending event clearing to an atomic swap operation

Rather than performing an atomic bit clearing for every encountered
event bit set we can adjust the code to perform an atomic swap of a
single row of the events array and decrease the amount of expensive
atomic operations.

Same optimization as for Hyper-V.  From FreeBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.60 2016/08/03 17:14:41 mikeb Exp $	*/
d747 1
a747 1
	atomic_setbit_ptr(&sc->sc_ipg->evtchn_mask[0], xi->xi_port);
d793 2
a794 2
	atomic_setbit_ptr(&sc->sc_ipg->evtchn_mask[0], xi->xi_port);
	atomic_clearbit_ptr(&sc->sc_ipg->evtchn_pending[0], xi->xi_port);
d823 1
a823 2
			if (isset((char *)&sc->sc_ipg->evtchn_mask[0],
			    xi->xi_port))
d839 1
a839 1
		atomic_setbit_ptr(&sc->sc_ipg->evtchn_mask[0], xi->xi_port);
d853 1
a853 1
		if (!isset((char *)&sc->sc_ipg->evtchn_mask[0], xi->xi_port))
@


1.60
log
@Use atomic operations to manipulate event masking bits
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.59 2016/08/03 14:55:57 mikeb Exp $	*/
d631 5
a635 2
		pending = sc->sc_ipg->evtchn_pending[row] &
		    ~(sc->sc_ipg->evtchn_mask[row]);
a638 2
			atomic_clearbit_ptr(&sc->sc_ipg->evtchn_pending[row],
			    bit);
@


1.59
log
@Use an atomic operation to clear pending event bits

Pending event bits are located in a shared memory and are potentially
accessed by multiple CPUs running dom0 and the guest VM.  It appears
that a failure to synchronize changes to this shared memory leads to
race conditions resulting in the guest missing out on notifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.58 2016/08/01 14:37:39 mikeb Exp $	*/
d746 1
a746 1
	setbit((char *)&sc->sc_ipg->evtchn_mask[0], xi->xi_port);
d792 2
a793 3
	setbit((char *)&sc->sc_ipg->evtchn_mask[0], xi->xi_port);
	clrbit((char *)&sc->sc_ipg->evtchn_pending[0], xi->xi_port);
	virtio_membar_sync();
d839 1
a839 2
		setbit((char *)&sc->sc_ipg->evtchn_mask[0], xi->xi_port);
		virtio_membar_sync();
@


1.58
log
@Don't forget to destroy the taskqueue on interrupt disestablish
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.57 2016/07/29 21:27:43 mikeb Exp $	*/
d636 2
a637 2
			sc->sc_ipg->evtchn_pending[row] &= ~(1 << bit);
			virtio_membar_producer();
@


1.57
log
@Move xen interrupt handlers to dedicated task queues

Handling receive and transmit for multiple networking interfaces
in a "shared interrupt" within normal interrupt vector code path
introduces too much delay from the hypervisor POV which prevents
it from injecting further completion event interrupts for Rx and
Tx queues.

Additionally, Netfront backend driver includes a mechanism to
detect Rx ring stalls and "turn the carrier off" when the guest
is not replenishing the ring (e.g. due to missing completion
interrupts) that relies on guest waking up periodically and making
sure that the Rx ring completion handling is progressing.

Having tried both task queue + timeout and interrupts + timeout
approaches, it appears that using  the task queue is more flexible
and provides superior performance under heavy network load.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.56 2016/04/28 16:40:10 mikeb Exp $	*/
d787 1
d789 2
@


1.56
log
@Preserve the domid when swapping 16 bit grant table entry flags

We use an atomic CMPXCHG on first 32 bits of the grant table entry
when revoking access to the memory page.  Target domain ID field is
part of these 32 bits, thus shouldn't be masked out for comparison.
This appears to be the last piece of the QubesOS VM chaining puzzle;
tested by Marco Peereboom, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.55 2016/04/19 18:15:41 mikeb Exp $	*/
d645 1
a645 2
			if (xi->xi_handler)
				xi->xi_handler(xi->xi_arg);
d651 10
a696 2
	xi->xi_handler = handler;
	xi->xi_arg = arg;
d698 9
@


1.55
log
@Bind event channels to backend domains

This is another piece of the QubesOS "chained VM" puzzle reported by
Marco Peereboom.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.54 2016/04/19 14:19:44 mikeb Exp $	*/
d1091 2
a1092 1
	flags = (ge->ge_table[ref].flags & ~(GTF_reading|GTF_writing));
@


1.54
log
@Allow to grant memory access to domains other than dom0.

Extend xen_grant_table_enter to take an additional "domain" argument
and extract it from the upper part of the bus_dmamap_load flags (sigh.)
to be able to punch it into the grant table entry.

Issue reported by Marco Peereboom who found that we wouldn't run under
QubesOS that "chains" VMs.  He also did the hard work getting the debug
data out of the aforementioned system.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.53 2016/04/19 13:55:19 mikeb Exp $	*/
d665 1
a665 1
xen_intr_establish(evtchn_port_t port, xen_intr_handle_t *xih,
d696 1
@


1.53
log
@Pass down the backend-id property to children in the attach arguments
and pick it up in xnf(4) and print it in the dmesg line for now. We'll
need to pass it down to the Grant Table code.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.52 2016/04/19 12:39:31 mikeb Exp $	*/
d69 2
a70 1
void	xen_grant_table_enter(struct xen_softc *, grant_ref_t, paddr_t, int);
d1043 1
a1043 1
    int flags)
d1061 1
a1061 1
	ge->ge_table[ref].domid = 0;
d1164 1
a1164 1
	int i, error;
d1166 2
d1173 1
a1173 1
		    flags & BUS_DMA_WRITE ? GTF_readonly : 0);
d1186 1
a1186 1
	int i, error;
d1188 2
d1195 1
a1195 1
		    flags & BUS_DMA_WRITE ? GTF_readonly : 0);
@


1.52
log
@Remove the ds_offset hack since object offset within a page
is the same for both virtual and physical addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.51 2016/04/01 15:41:12 mikeb Exp $	*/
d1224 13
d1254 1
d1283 3
a1285 1
			if (xs_getprop(sc, xa.xa_node, "backend", xa.xa_backend,
d1290 4
@


1.51
log
@Move atomics.h include dance to an earlier stage

Otherwise proc.h & friends pick it up before we manage to perform
our MULTIPROCESSOR dance.  This time I've made sure we get LOCK
prefixes with an objdump.  Bug reported by Evgeniy Sudyr, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.50 2016/02/05 10:30:37 mikeb Exp $	*/
a1171 4
		map->dm_segs[i].ds_offset = map->dm_segs[i].ds_addr &
		    PAGE_MASK;
		KASSERT(map->dm_segs[i].ds_offset +
		    map->dm_segs[i].ds_len <= PAGE_SIZE);
a1191 4
		map->dm_segs[i].ds_offset = map->dm_segs[i].ds_addr &
		    PAGE_MASK;
		KASSERT(map->dm_segs[i].ds_offset +
		    map->dm_segs[i].ds_len <= PAGE_SIZE);
@


1.50
log
@Silence warnings from static analyzers; found by jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.49 2016/02/02 17:52:46 mikeb Exp $	*/
d20 12
a55 11

/* Xen requires locked atomic operations */
#ifndef MULTIPROCESSOR
#define _XENMPATOMICS
#define MULTIPROCESSOR
#endif
#include <sys/atomic.h>
#ifdef _XENMPATOMICS
#undef MULTIPROCESSOR
#undef _XENMPATOMICS
#endif
@


1.49
log
@A few reliability improvements in the power management interface

Nathanael Rensen <nathanael at list ! polymorpheus ! com> came up with
a few improvements to the event watcher and power management interface,
namely:

 o Make sure to put our watcher on a list before issuing an XS_WATCH
   command since Xen will raise the event right after it's been set up.

 o The first time xen_control is called the "control/shutdown" node
   may not exist, so skip printing the error message in this case.

 o Acknowledge requests by writing back an empty string.

 o log(9) reboot and halt requests like vmt(4) does.

Huge thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.48 2016/01/29 19:12:26 mikeb Exp $	*/
d1246 2
a1247 2
	struct iovec *iovp1, *iovp2;
	int i, j, error = 0, iov1_cnt, iov2_cnt;
@


1.48
log
@Add support for "control/shutdown" power management facility

At the moment only "poweroff" and "reboot" actions are supported.
Suspend/resume requires additional changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.47 2016/01/29 19:04:30 mikeb Exp $	*/
d28 1
d38 2
d199 1
d205 5
a209 3
	if (xs_getprop(sc, "control", "shutdown", action, sizeof(action))) {
		printf("%s: failed to process control event\n",
		    sc->sc_dev.dv_xname);
d216 3
d222 8
a229 4
		if (allowpowerdown == 1) {
			allowpowerdown = 0;
			prsignal(initprocess, SIGUSR2);
		}
d233 8
a240 4
		if (allowpowerdown == 1) {
			allowpowerdown = 0;
			prsignal(initprocess, SIGINT);
		}
@


1.47
log
@Add support for XS_WATCH: XenStore notification facility

After configuring a watch for the node, XenStore will asynchronously
notify the system when the value of the specified node changes with
an event message.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.46 2016/01/29 18:49:06 mikeb Exp $	*/
d21 3
d72 1
a159 1

d183 49
@


1.46
log
@Cleanup XenStore API

Turns out that we want to let devices choose whether they're issuing
XenStore requests to the backend or frontend.  This also unifies the
the API somewhat as providing the xen softcore structure is now
mandatory.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.45 2016/01/28 11:19:49 mikeb Exp $	*/
d24 1
@


1.45
log
@Older Xen dom0's don't implement setting version of Grant Table entries

Remove leftover code that was used to set v2 of Grant Table entries.
From Nathanael Rensen <nathanael at list ! polymorpheus ! com>, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.44 2016/01/27 18:04:42 mikeb Exp $	*/
d1176 3
a1178 3
	struct iovec *iovp1, *iovp2, *iovp3;
	int i, j, error = 0, iov1_cnt, iov2_cnt, iov3_cnt;
	char path[128];
d1206 5
a1210 5
			snprintf(path, sizeof(path), "%s/backend", xa.xa_node);
			if (!xs_cmd(&xst, XS_READ, path, &iovp3, &iov3_cnt)) {
				strlcpy(xa.xa_backend, (char *)iovp3->iov_base,
				    sizeof(xa.xa_backend));
				xs_resfree(&xst, iovp3, iov3_cnt);
@


1.44
log
@Tighten up next reference calculation, printing fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.43 2016/01/27 15:34:50 mikeb Exp $	*/
a775 2
	struct gnttab_get_version ggv;
	struct gnttab_set_version gsv;
a784 9
		return (-1);
	}

	gsv.version = 1;
	ggv.dom = DOMID_SELF;
	if (xen_hypercall(sc, XC_GNTTAB, 3, GNTTABOP_set_version, &gsv, 1) ||
	    xen_hypercall(sc, XC_GNTTAB, 3, GNTTABOP_get_version, &ggv, 1) ||
	    ggv.version != 1) {
		printf(": failed to set grant tables API version\n");
@


1.43
log
@Reimplement Grant Table metadata linking and enable dynamic allocation

Instead of pre-allocating maximum number of Grant Table frames allotted by
the hypervisor we switch over to allocating them dynamically when the need
arises.  At the same time we no longer link metadata entries representing
individual Grant Table frames as a list and use a table instead to speed
up reference lookups when establishing and removing mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.42 2016/01/27 15:29:00 mikeb Exp $	*/
d869 1
a869 1
	ge->ge_next = ge->ge_reserved ? ge->ge_reserved + 1 : 0;
d932 1
a932 1
			ge->ge_next = ge->ge_reserved + 1;
d939 1
a939 1
	panic("page full, sc %p gnts %p (%d) ge %p", sc, sc->sc_gnt,
d959 1
a959 1
		panic("out of bounds ref %u ge %p start %u sc %p gtt %p",
d967 2
a968 2
		panic("ref %u is still in use, sc %p gnt %p", ref,
		    sc, sc->sc_gnt);
d971 1
a971 1
		    sc->sc_dev.dv_xname, ref);
d994 1
a994 1
		panic("out of bounds ref %u ge %p start %u sc %p gtt %p",
d1015 1
a1015 1
		panic("unmanaged ref %u sc %p gnts %p (%d)", ref, sc,
d1021 1
a1021 1
		panic("out of bounds ref %u ge %p start %u sc %p gtt %p",
@


1.42
log
@xen_bus_dma_init turned out to be unnecessary
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.41 2016/01/27 15:27:53 mikeb Exp $	*/
d61 1
a61 1
int	xen_grant_table_enter(struct xen_softc *, grant_ref_t, paddr_t, int);
a777 1
	int i;
d799 10
a808 1
	SLIST_INIT(&sc->sc_gnts);
d810 5
a814 3
	for (i = 0; i < gqs.max_nr_frames; i++)
		if (xen_grant_table_grow(sc) == NULL)
			break;
d816 1
a816 1
	printf(", %u grant table frames", sc->sc_gntcnt);
d830 2
a831 3
	ge = malloc(sizeof(*ge), M_DEVBUF, M_ZERO | M_NOWAIT);
	if (ge == NULL) {
		printf("%s: failed to allocate a grant table entry\n",
d835 4
d842 1
d850 1
d862 1
a870 1
	SLIST_INSERT_HEAD(&sc->sc_gnts, ge, ge_entry);
d873 1
d884 13
a896 2
	SLIST_FOREACH(ge, &sc->sc_gnts, ge_entry) {
		if (!ge->ge_free)
d899 35
a933 20
		for (i = ge->ge_next;
		     /* Math works here because GNTTAB_NEPG is a power of 2 */
		     i != ((ge->ge_next + GNTTAB_NEPG - 1) & (GNTTAB_NEPG - 1));
		     i++) {
			if (i == GNTTAB_NEPG)
				i = 0;
			if (ge->ge_reserved && i < ge->ge_reserved)
				continue;
			if (ge->ge_table[i].flags != GTF_invalid &&
			    ge->ge_table[i].frame != 0)
				continue;
			*ref = ge->ge_start + i;
			/* XXX Mark as taken */
			ge->ge_table[i].frame = 0xffffffff;
			if ((ge->ge_next = i + 1) == GNTTAB_NEPG)
				ge->ge_next = ge->ge_reserved + 1;
			ge->ge_free--;
			mtx_leave(&ge->ge_mtx);
			return (0);
		}
d935 1
d937 1
d939 2
a940 1
	/* We're out of entries */
d949 16
a964 12
	SLIST_FOREACH(ge, &sc->sc_gnts, ge_entry) {
		if (ref < ge->ge_start || ref > ge->ge_start + GNTTAB_NEPG)
			continue;
		ref -= ge->ge_start;
		mtx_enter(&ge->ge_mtx);
		if (ge->ge_table[ref].flags != GTF_invalid) {
			mtx_leave(&ge->ge_mtx);
			return;
		}
		ge->ge_table[ref].frame = 0;
		ge->ge_next = ref;
		ge->ge_free++;
d966 7
d974 4
d980 1
a980 1
int
d986 10
a995 10
	SLIST_FOREACH(ge, &sc->sc_gnts, ge_entry) {
		if (ref < ge->ge_start || ref > ge->ge_start + GNTTAB_NEPG)
			continue;
		ref -= ge->ge_start;
		ge->ge_table[ref].frame = atop(pa);
		ge->ge_table[ref].domid = 0;
		virtio_membar_sync();
		ge->ge_table[ref].flags = GTF_permit_access | flags;
		virtio_membar_sync();
		return (0);
d997 7
a1003 1
	return (ENOBUFS);
d1013 24
a1036 18
	SLIST_FOREACH(ge, &sc->sc_gnts, ge_entry) {
		if (ref < ge->ge_start || ref > ge->ge_start + GNTTAB_NEPG)
			continue;
		ref -= ge->ge_start;

		/* Invalidate the grant reference */
		virtio_membar_sync();
		ptr = (uint32_t *)&ge->ge_table[ref];
		flags = (ge->ge_table[ref].flags & ~(GTF_reading|GTF_writing));
		loop = 0;
		while (atomic_cas_uint(ptr, flags, GTF_invalid) != flags) {
			if (loop++ > 10000000) {
				printf("%s: grant table reference %u is held "
				    "by domain %d\n", sc->sc_dev.dv_xname, ref +
				    ge->ge_start, ge->ge_table[ref].domid);
				return;
			}
			CPU_BUSY_CYCLE();
d1038 1
a1038 2
		ge->ge_table[ref].frame = 0xffffffff;
		break;
d1040 1
d1109 2
a1110 7
		error = xen_grant_table_enter(sc, gm[i].gm_ref,
		    map->dm_segs[i].ds_addr, flags & BUS_DMA_WRITE ?
		    GTF_readonly : 0);
		if (error) {
			xen_bus_dmamap_unload(t, map);
			return (error);
		}
d1133 2
a1134 7
		error = xen_grant_table_enter(sc, gm[i].gm_ref,
		    map->dm_segs[i].ds_addr, flags & BUS_DMA_WRITE ?
		    GTF_readonly : 0);
		if (error) {
			xen_bus_dmamap_unload(t, map);
			return (error);
		}
@


1.41
log
@shorten a few long lines
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.40 2016/01/27 09:04:19 reyk Exp $	*/
a71 1
void	xen_bus_dma_init(struct xen_softc *);
d808 1
a808 1
	xen_bus_dma_init(sc);
a968 6
}

void
xen_bus_dma_init(struct xen_softc *sc)
{
	xen_bus_dma_tag._cookie = sc;
@


1.40
log
@Add a key-value interface to pvbus(4) that allows to get or set values
in the underlying information store of the host from the OpenBSD-VM's
userspace.  OpenBSD did not provide access to these stores before,
mostly because we did not want to add a custom tool and interface for
each hypervisor.  The pvbus(4) interface provides backends for
xen(4)'s XenStore and vmt(4)'s VMware Tools "guestinfo".  These
information stores are fairly different, XenStore is a "filesystem"
while vmt is a RPC, and the key-value abstraction limits them a bit
but provides the most wanted functionality.

Discussed with many
OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.39 2016/01/26 15:51:07 mikeb Exp $	*/
d956 1
a956 1
		flags = (ge->ge_table[ref].flags & ~(GTF_reading | GTF_writing));
d960 2
a961 2
				printf("%s: grant table reference %u is held by "
				    "domain %d\n", sc->sc_dev.dv_xname, ref +
@


1.39
log
@No need to take a grant table entry mutex in xen_grant_table_{enter,remove}

Grant table API is constructed in a way that once allocated grant table
entries are marked as used and cannot be given away again to some other
user.  At the same time xen_grant_table_enter and _remove do not operate
on the same grant reference at the same time, so there's no need for a
lock here.  Guard flag operations with memory fences to ensure correct
store/load order.  This provides some decent performance improvement as
well.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.38 2016/01/26 15:35:21 mikeb Exp $	*/
d158 4
@


1.38
log
@Add a grant table reference invalidation spin out check

This debugging check has been helpful in identifying and fixing
a few issues already.  Subject to removal in the future however.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.37 2016/01/26 15:31:02 mikeb Exp $	*/
a926 1
		mtx_enter(&ge->ge_mtx);
d931 1
a931 1
		mtx_leave(&ge->ge_mtx);
a948 1
		mtx_enter(&ge->ge_mtx);
d950 1
a955 1
				mtx_leave(&ge->ge_mtx);
a963 1
		mtx_leave(&ge->ge_mtx);
@


1.37
log
@Implement a rather conservative bus_dmamap_sync API
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.36 2016/01/26 15:23:11 mikeb Exp $	*/
d943 1
d954 9
a962 1
		while (atomic_cas_uint(ptr, flags, GTF_invalid) != flags)
d964 1
@


1.36
log
@Make sure to use locked atomic operations even on the SP kernel

When executed under the hypervisor we need to make sure that CAS
and other atomic operations are executed while locking the bus.

Problem reported by Imre Oolberg <imre at auul ! pri ! ee>, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.35 2016/01/25 15:22:56 mikeb Exp $	*/
d81 2
d103 1
a103 1
	_bus_dmamap_sync,
d1094 9
@


1.35
log
@Don't count the total number of Xen upcalls
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.34 2016/01/23 15:19:02 jsg Exp $	*/
a20 1
#include <sys/atomic.h>
d38 11
d1119 1
a1119 2
	if ((error = xs_cmd(&xst, XS_LIST, "device", &iovp1,
	    &iov1_cnt)) != 0)
@


1.34
log
@Fix some uses of sizeof where the size of a struct was intended, not
the size of the pointer to a struct.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.33 2016/01/22 19:28:16 mikeb Exp $	*/
a473 1
	evcount_attach(&sc->sc_evcnt, sc->sc_dev.dv_xname, &sc->sc_irq);
a536 2

	sc->sc_evcnt.ec_count++;
@


1.33
log
@To facilitate reading make sure to use a GTF_invalid flag by name
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.32 2016/01/22 19:26:40 mikeb Exp $	*/
d613 1
a613 1
		    sizeof(&eau)) != 0) {
d634 1
a634 1
	if (xen_evtchn_hypercall(sc, EVTCHNOP_bind_vcpu, &ebv, sizeof(&ebv))) {
@


1.32
log
@Convert membar_* calls into virtio_membar_sync where it matters

membar_* functions are defined only as compiler barriers on !MP
kernels, while we're trying to be conservative in our use of the
barriers.  Barriers are placed only where loads and stores might
get reordered and it matters at the same time.  Shared info page
operations are using atomic instructions on Linux, so they get
barriers as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.31 2016/01/19 13:36:00 mikeb Exp $	*/
d944 1
a944 1
		while (atomic_cas_uint(ptr, flags, 0) != flags)
@


1.31
log
@Cast evtchn_mask to a char pointer for an isset operation

When testing evtchn_mask bits we need to treat the array as a bit
matrix for an isset macro to test correct bits.  Reported by reyk@@
some time ago, Wei Liu <wei ! liu2 at citrix !com> figured out how
to reproduce the problem.  Thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.30 2016/01/18 19:09:09 mikeb Exp $	*/
d382 1
a382 1
	membar_sync();
a481 1
		membar_producer();
d525 1
d553 1
a553 1
			membar_producer();
d690 1
a690 1
	membar_producer();
d718 1
a718 1
			membar_sync();
d737 1
a737 1
		membar_producer();
d921 1
a921 1
		membar_producer();
@


1.30
log
@Log unhandled interrupts
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.29 2016/01/18 19:06:48 mikeb Exp $	*/
d587 1
a587 1
#if notyet
d719 2
a720 1
			if (isset(sc->sc_ipg->evtchn_mask, xi->xi_port))
d751 1
a751 1
		if (!isset(sc->sc_ipg->evtchn_mask, xi->xi_port))
@


1.29
log
@Provide a Xen v3 API compatible fallback for event channel hypercalls
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.28 2016/01/15 18:20:41 mikeb Exp $	*/
d555 3
a557 1
			if ((xi = xen_lookup_intsrc(sc, port)) == NULL)
d559 1
@


1.28
log
@Cleanup dmesg output, disable debugging; prodding and suggestions from reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.27 2016/01/15 14:27:08 mikeb Exp $	*/
d490 16
d573 1
a573 1
		xen_hypercall(sc, XC_EVTCHN, 2, EVTCHNOP_send, &es);
d609 2
a610 2
		if (xen_hypercall(sc, XC_EVTCHN, 2,
		    EVTCHNOP_alloc_unbound, &eau) != 0) {
d631 1
a631 1
	if (xen_hypercall(sc, XC_EVTCHN, 2, EVTCHNOP_bind_vcpu, &ebv)) {
d648 1
a648 1
	if (xen_hypercall(sc, XC_EVTCHN, 2, EVTCHNOP_status, &es)) {
d691 1
a691 2
		if (xen_hypercall(sc, XC_EVTCHN, 2, EVTCHNOP_close,
		    &ec)) {
d711 2
a712 3
			if (xen_hypercall(sc, XC_EVTCHN, 2,
			    EVTCHNOP_unmask, &eu) ||
			    isset(sc->sc_ipg->evtchn_mask, xi->xi_port))
d715 4
d750 2
a751 1
		return (xen_hypercall(sc, XC_EVTCHN, 2, EVTCHNOP_unmask, &eu));
@


1.27
log
@Detach emulated network devices if Netfront driver is enabled

Xen doesn't provide a way for a guest to decide which model of
the interface is selected in the VM configuration and therefore
there's no simple way for Netfront and emulated devices to co-
exist on the same system.  Emulated em(4) or re(4) drivers will
take over if xnf(4) driver is disabled or not compiled in.

Idea and OK reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.26 2016/01/14 12:37:17 mikeb Exp $	*/
a41 1
int	xen_getversion(struct xen_softc *);
a120 2
	printf("\n");

a126 2
	if (xen_getversion(sc))
		return;
d197 1
a197 2
		printf("%s: requested %d hypercall pages\n",
		    sc->sc_dev.dv_xname, regs[0]);
d204 1
a204 2
		printf("%s: hypercall page PA extraction failed\n",
		    sc->sc_dev.dv_xname);
a208 3
	DPRINTF("%s: hypercall page at va %p pa %#lx\n", sc->sc_dev.dv_xname,
	    sc->sc_hc, pa);

a352 23
xen_getversion(struct xen_softc *sc)
{
	char buf[16];
	int version;
	ulong argv[2] = { XENVER_extraversion, (ulong)&buf[0] };
	int argc = 2;

	memset(buf, 0, sizeof(buf));
	if ((version = xen_hypercall(sc, XC_VERSION, 1, XENVER_version)) < 0) {
		printf("%s: failed to fetch version\n", sc->sc_dev.dv_xname);
		return (-1);
	}
	if (xen_hypercallv(sc, XC_VERSION, argc, argv) < 0) {
		printf("%s: failed to fetch extended version\n",
		    sc->sc_dev.dv_xname);
		return (-1);
	}
	printf("%s: version %d.%d%s\n", sc->sc_dev.dv_xname,
	    version >> 16, version & 0xffff, buf);
	return (0);
}

int
a355 2
	ulong argv[2] = { XENVER_get_features, (ulong)&xfi };
	int argc = 2;
d358 2
a359 2
	if (xen_hypercallv(sc, XC_VERSION, argc, argv) < 0) {
		printf("%s: failed to fetch features\n", sc->sc_dev.dv_xname);
d363 2
a364 1
	printf("%s: features %b\n", sc->sc_dev.dv_xname, sc->sc_features,
d367 3
d426 1
a426 2
		printf("%s: failed to allocate shared info page\n",
		    sc->sc_dev.dv_xname);
d430 1
a430 2
		printf("%s: shared info page PA extraction failed\n",
		    sc->sc_dev.dv_xname);
d439 1
a439 2
		printf("%s: failed to register shared info page\n",
		    sc->sc_dev.dv_xname);
a442 2
	DPRINTF("%s: shared info page at va %p pa %#lx\n", sc->sc_dev.dv_xname,
	    sc->sc_ipg, pa);
d461 1
a461 2
	DPRINTF("%s: registered callback IDT vector %d\n",
	    sc->sc_dev.dv_xname, LAPIC_XEN_VECTOR);
d747 1
a747 2
		printf("%s: failed the query for grant table pages\n",
		    sc->sc_dev.dv_xname);
d751 2
a752 2
		printf("%s: invalid number of grant table pages: %u/%u\n",
		    sc->sc_dev.dv_xname, gqs.nr_frames, gqs.max_nr_frames);
d761 1
a761 2
		printf("%s: failed to set grant tables API version\n",
		    sc->sc_dev.dv_xname);
d771 1
a771 2
	DPRINTF("%s: grant table frames allocated %u/%u\n",
	    sc->sc_dev.dv_xname, sc->sc_gntcnt, gqs.max_nr_frames);
@


1.26
log
@Fixup a merge error
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.25 2016/01/13 19:09:50 mikeb Exp $	*/
a148 1
	xen_disable_emulated_devices(sc);
d152 2
d162 1
a162 1
	if (!sc->sc_cbvec) {
d500 1
a500 1
	sc->sc_cbvec = 1;
a1173 2
int xen_disable_pv_ide, xen_disable_pv_idesec, xen_disable_pv_nic;

d1181 2
a1182 1
		printf("%s: no magic!\n", sc->sc_dev.dv_xname);
d1185 1
a1185 1
	if (xen_disable_pv_ide)
d1187 1
a1187 1
	if (xen_disable_pv_idesec)
d1189 1
a1189 1
	if (xen_disable_pv_nic)
d1191 1
a1191 1
	if (unplug) {
a1192 2
		DPRINTF("%s: disabled emulated devices\n", sc->sc_dev.dv_xname);
	}
@


1.25
log
@spacing typos
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.24 2016/01/13 18:56:26 mikeb Exp $	*/
d1050 2
a1078 2
		KASSERT(map->dm_segs[i].ds_offset +
		    map->dm_segs[i].ds_len <= PAGE_SIZE);
@


1.24
log
@Create rx and tx fragment maps with a page size boundary restriction

We need to ensure that rx and tx fragments do not cross page boundary
since grant table reference can only point to a complete page. Add a
couple of kernel assertions in the dma map loading code to catch these
problems early in the future.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.23 2016/01/05 18:03:59 mikeb Exp $	*/
d423 2
a424 2
		    "   time mul=%u shift=%d\n"
		    , i, v->evtchn_upcall_pending, v->evtchn_upcall_mask,
d672 2
a673 2
	printf("%s: port %u bound to vcpu%u",
	    sc->sc_dev.dv_xname, es.port, es.vcpu);
d830 1
a830 2
	ge->ge_table = km_alloc(PAGE_SIZE, &kv_any, &kp_zero,
	    &kd_nowait);
d990 1
a990 1
	/* ALlocate an array of grant table pa<->ref maps */
@


1.23
log
@Move over to the v1 of Grant Table entries

In spite of comments in the Xen source code encouraging use of v2
entries in the new code, there's no benefit for us to do so at the
moment.  While v1 entries support only full page mappings, they're
half the size of their newer counterparts, increasing the number
of available grant table references from 8000 to 16000 within the
same allotment of grant table frames (up to 32).
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.22 2016/01/05 13:47:28 mikeb Exp $	*/
d1078 4
@


1.22
log
@Memorize the DMA segment's data offset within the page

Grant table references don't convey the information about an actual
offset of the data within the page, however Xen needs to know it.
We (ab)use bus_dma_segment's _ds_boundary member to store it and can
get away with not restoring it's original value atm because neither
i386 nor amd64 bus_dmamap_unload(9) code needs it.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.21 2016/01/04 16:07:52 mikeb Exp $	*/
d793 1
a793 1
	gsv.version = 2;
d797 1
a797 1
	    ggv.version != 2) {
d885 2
a886 2
			if (ge->ge_table[i].hdr.flags != GTF_invalid &&
			    ge->ge_table[i].full_page.frame != 0)
d890 1
a890 1
			ge->ge_table[i].full_page.frame = 0xffffffff;
d914 1
a914 1
		if (ge->ge_table[ref].hdr.flags != GTF_invalid) {
d918 1
a918 1
		ge->ge_table[ref].full_page.frame = 0;
d936 2
a937 2
		ge->ge_table[ref].full_page.frame = atop(pa);
		ge->ge_table[ref].full_page.hdr.domid = 0;
d939 1
a939 2
		ge->ge_table[ref].full_page.hdr.flags =
		    GTF_permit_access | flags;
d959 2
a960 3
		ptr = (uint32_t *)&ge->ge_table[ref].hdr;
		flags = (ge->ge_table[ref].hdr.flags &
		    ~(GTF_reading | GTF_writing));
d963 1
a963 1
		ge->ge_table[ref].full_page.frame = 0xffffffff;
@


1.21
log
@Skip "suspend" device node during probing
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.20 2016/01/04 16:05:43 mikeb Exp $	*/
d1051 2
d1078 2
@


1.20
log
@Preallocate Grant Table frames to simplify the code for now
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.19 2015/12/22 22:16:53 mikeb Exp $	*/
d1128 1
a1128 10
		/* Special handling */
		if (!strcmp("suspend", (char *)iovp1[i].iov_base)) {
			xa.xa_parent = sc;
			xa.xa_dmat = &xen_bus_dma_tag;
			strlcpy(xa.xa_name, (char *)iovp1[i].iov_base,
			    sizeof(xa.xa_name));
			snprintf(xa.xa_node, sizeof(xa.xa_node), "device/%s",
			    (char *)iovp1[i].iov_base);
			config_found((struct device *)sc, &xa,
			    xen_attach_print);
a1129 1
		}
@


1.19
log
@Implement a bus_dma(9) abstraction on top of Grant Table API
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.18 2015/12/21 19:43:16 mikeb Exp $	*/
a792 3
	DPRINTF("%s: grant table frames allocated %u, max %u\n",
	    sc->sc_dev.dv_xname, gqs.nr_frames, gqs.max_nr_frames);

d805 1
a805 1
	for (i = 0; i < gqs.nr_frames; i++)
d809 2
a810 1
	sc->sc_gntmax = gqs.max_nr_frames;
a861 3
	DPRINTF("%s: grant table frame %d start %d pa %#lx\n",
	    sc->sc_dev.dv_xname, sc->sc_gntcnt, ge->ge_start, pa);

a872 1
 retry:
d900 1
a900 7
	/* We're out of entries, try growing the table */
	if (sc->sc_gntcnt >= sc->sc_gntmax)
		return (-1);

	if (xen_grant_table_grow(sc) != NULL)
		goto retry;

@


1.18
log
@Introduce xen_intr_mask and xen_intr_unmask primitives

Mask the event port during xen_intr_establish, but don't set the
masked flag in the intsrc.  By providing mask and unmask routines
we allow the device to decide when to perform these actions.  The
port will still be unmasked during xen_intr_enable.  This allows
netfront to fulfil the intr_barrier pattern requirements fairly
easily and at the same time should be sufficient for diskfront
that doesn't need to fiddle with interrupt masking.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.17 2015/12/21 18:17:36 mikeb Exp $	*/
d47 7
d63 10
d83 18
d143 3
d773 337
d1143 1
d1161 1
@


1.17
log
@Cleanup hypercall subsystem type defines
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.16 2015/12/21 18:13:44 mikeb Exp $	*/
d623 3
d701 32
@


1.16
log
@Don't unmask the port in xen_intr_establish
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.15 2015/12/19 09:11:14 mikeb Exp $	*/
d332 1
a332 1
	if ((version = xen_hypercall(sc, xen_version, 1, XENVER_version)) < 0) {
d336 1
a336 1
	if (xen_hypercallv(sc, xen_version, argc, argv) < 0) {
d354 1
a354 1
	if (xen_hypercallv(sc, xen_version, argc, argv) < 0) {
d432 1
a432 1
	if (xen_hypercall(sc, memory_op, 2, XENMEM_add_to_physmap, &xatp)) {
d454 1
a454 1
	if (xen_hypercall(sc, hvm_op, 2, HVMOP_set_param, &xhp)) {
d555 1
a555 1
		xen_hypercall(sc, event_channel_op, 2, EVTCHNOP_send, &es);
d591 1
a591 1
		if (xen_hypercall(sc, event_channel_op, 2,
d613 1
a613 1
	if (xen_hypercall(sc, event_channel_op, 2, EVTCHNOP_bind_vcpu, &ebv)) {
d627 1
a627 1
	if (xen_hypercall(sc, event_channel_op, 2, EVTCHNOP_status, &es)) {
d670 1
a670 1
		if (xen_hypercall(sc, event_channel_op, 2, EVTCHNOP_close,
d691 1
a691 1
			if (xen_hypercall(sc, event_channel_op, 2,
@


1.15
log
@Fixup a few bugs in xen_intr_{establish,disestablish}

xen_intr_establish was using a variable that hasn't got its value
updated in the supplementary check for event channel port unmasking.

xen_intr_disestablish didn't save and correctly check the return value
of xen_lookup_intsrc call and was incorrectly indexing into the pending
event channel port bitmap.
@
text
@d1 1
a1 1
/*	$OpenBSD: xen.c,v 1.14 2015/12/12 21:07:45 reyk Exp $	*/
a565 1
	struct evtchn_unmask eu;
d569 1
a569 1
#ifdef XEN_DEBUG
d574 1
a574 1
		printf("%s: interrupt handler has already been established "
d623 1
a623 9
	if (!cold) {
		eu.port = xi->xi_port;
		if (xen_hypercall(sc, event_channel_op, 2, EVTCHNOP_unmask,
		    &eu) || isset(sc->sc_ipg->evtchn_mask, xi->xi_port))
			printf("%s: unmasking port %u failed\n",
			    sc->sc_dev.dv_xname, xi->xi_port);
	}

#ifdef XEN_DEBUG
@


1.14
log
@Add OpenBSD CVS/RCS Ids.

mikeb@@ doesn't like the Ids, "somebody else has to add them".
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a539 1

d627 1
a627 1
		    &eu) || isset(sc->sc_ipg->evtchn_mask, port))
d629 1
a629 1
			    sc->sc_dev.dv_xname, port);
d666 1
a666 3
	if (xen_lookup_intsrc(sc, port) != NULL) {
		DPRINTF("%s: failed to lookup an established interrupt handler "
		    "for port %u\n", sc->sc_dev.dv_xname, port);
a667 1
	}
d673 2
a674 2
	setbit((char *)sc->sc_ipg->evtchn_mask, xi->xi_port);
	clrbit((char *)sc->sc_ipg->evtchn_pending[0], xi->xi_port);
@


1.13
log
@Identify hypervisors before configuring other children of the mainbus
(bios, CPU, interrupt handlers, pvbus).  This splits the pvbus attach
function into two parts: pvbus_identify() to scan the CPUID registers
for supported hypervisors and pvbus_attach() to attach the bus, print
information, and configure the children.

This will be needed for Xen and KVM, as discussed with mikeb@@ and sf@@
OK mlarkin@@
@
text
@d1 2
@


1.12
log
@Replace mountroothook_establish(9) by config_mountroot(9) a narrower API
similar to config_defer(9).

ok mikeb@@, deraadt@@
@
text
@d33 1
@


1.11
log
@Store the backend node in the attach argument structure
@
text
@d48 1
a48 1
void	xen_deferred(void *);
d112 1
a112 1
	mountroothook_establish(xen_deferred, sc);
d116 1
a116 1
xen_deferred(void *arg)
d118 1
a118 1
	struct xen_softc *sc = arg;
@


1.10
log
@cfdriver can't be const...
@
text
@d726 3
a728 3
	struct iovec *iovp1, *iovp2;
	int error = 0, iov1_cnt, iov2_cnt, i, j;
	char path[64];
d765 6
@


1.9
log
@Don't expose XenStore ops we don't know how to deal with
@
text
@d55 1
a55 1
const struct cfdriver xen_cd = {
@


1.8
log
@Implements simple virtual device probing routine

Discussed with deraadt@@, kettenis@@, mpi@@, OK mlarkin, mpi
@
text
@d735 1
a735 1
	if ((error = xs_cmd(&xst, XS_DIRECTORY, "device", &iovp1,
d753 1
a753 1
		if ((error = xs_cmd(&xst, XS_DIRECTORY, path, &iovp2,
@


1.7
log
@Implement a function to detach emulated devices (such as an em(4)
network interface) in order to attach a paravirtualized drivers
(such as Xen Netfront).

OK mlarkin
@
text
@d51 1
d110 2
d708 64
@


1.6
log
@Driver for XenStore, the configuration storage

XenStore provides a hierarchical storage for Xen configuration
in a style of an OpenFirmware.  Itself it's an interrupt driven
producer/consumer interface with two 1kb rings for input and
output.

It's required in order to do virtual device discovery and device
configuration (fetch MAC address, various parameters).

With input from and OK mlarkin, reyk
@
text
@d44 1
d107 2
d705 33
@


1.5
log
@Add ability to establish virtual interrupts via Xen event
channel ports.

During boot, Xen will use polling mode, but once the system
enables interrupts after cpu_configure(), xen_intr_enable
will be called from the mountrook hook to unmask event ports.

OK mlarkin, mpi, reyk
@
text
@d51 2
d101 3
@


1.4
log
@Communicate the selected IDT vector to the Hypervisor

OK mlarkin, reyk
@
text
@d43 1
d47 1
d97 19
d452 44
d499 198
a696 1
	/* stub */
@


1.3
log
@Allocate and hook up a "shared info page"

This page provides a matrix of pending events and some other
information like hypervisor timecounter.

OK mlarkin, reyk
@
text
@d30 2
d42 1
d93 2
d406 29
@


1.2
log
@This brings in support for Xen hypercalls via an MI interface
and implements functions to fetch extended version and features.

OK mlarkin
@
text
@d39 1
d87 3
d322 78
@


1.1
log
@Xen basic infrastructure files and pvbus(4) attachment.

With input from and OK mpi, mlarkin, reyk
@
text
@d31 1
d36 4
d76 3
d81 5
d104 215
@

