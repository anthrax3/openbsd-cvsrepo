head	1.135;
access;
symbols
	OPENBSD_6_2:1.135.0.2
	OPENBSD_6_2_BASE:1.135
	OPENBSD_6_1:1.135.0.4
	OPENBSD_6_1_BASE:1.135
	OPENBSD_6_0:1.133.0.2
	OPENBSD_6_0_BASE:1.133
	OPENBSD_5_9:1.131.0.4
	OPENBSD_5_9_BASE:1.131
	OPENBSD_5_8:1.131.0.6
	OPENBSD_5_8_BASE:1.131
	OPENBSD_5_7:1.131.0.2
	OPENBSD_5_7_BASE:1.131
	OPENBSD_5_6:1.128.0.4
	OPENBSD_5_6_BASE:1.128
	OPENBSD_5_5:1.125.0.4
	OPENBSD_5_5_BASE:1.125
	OPENBSD_5_4:1.119.0.2
	OPENBSD_5_4_BASE:1.119
	OPENBSD_5_3:1.116.0.2
	OPENBSD_5_3_BASE:1.116
	OPENBSD_5_2:1.112.0.4
	OPENBSD_5_2_BASE:1.112
	OPENBSD_5_1_BASE:1.112
	OPENBSD_5_1:1.112.0.2
	OPENBSD_5_0:1.110.0.2
	OPENBSD_5_0_BASE:1.110
	OPENBSD_4_9:1.102.0.4
	OPENBSD_4_9_BASE:1.102
	OPENBSD_4_8:1.102.0.2
	OPENBSD_4_8_BASE:1.102
	OPENBSD_4_7:1.101.0.2
	OPENBSD_4_7_BASE:1.101
	OPENBSD_4_6:1.100.0.4
	OPENBSD_4_6_BASE:1.100
	OPENBSD_4_5:1.98.0.4
	OPENBSD_4_5_BASE:1.98
	OPENBSD_4_4:1.98.0.2
	OPENBSD_4_4_BASE:1.98
	OPENBSD_4_3:1.94.0.2
	OPENBSD_4_3_BASE:1.94
	OPENBSD_4_2:1.92.0.2
	OPENBSD_4_2_BASE:1.92
	OPENBSD_4_1:1.83.0.2
	OPENBSD_4_1_BASE:1.83
	OPENBSD_4_0:1.73.0.2
	OPENBSD_4_0_BASE:1.73
	OPENBSD_3_9:1.68.0.2
	OPENBSD_3_9_BASE:1.68
	OPENBSD_3_8:1.61.0.2
	OPENBSD_3_8_BASE:1.61
	OPENBSD_3_7:1.55.0.2
	OPENBSD_3_7_BASE:1.55
	OPENBSD_3_6:1.52.0.2
	OPENBSD_3_6_BASE:1.52
	SMP_SYNC_A:1.50
	SMP_SYNC_B:1.50
	OPENBSD_3_5:1.49.0.2
	OPENBSD_3_5_BASE:1.49
	OPENBSD_3_4:1.46.0.2
	OPENBSD_3_4_BASE:1.46
	UBC_SYNC_A:1.43
	OPENBSD_3_3:1.42.0.2
	OPENBSD_3_3_BASE:1.42
	OPENBSD_3_2:1.41.0.2
	OPENBSD_3_2_BASE:1.41
	OPENBSD_3_1:1.37.0.2
	OPENBSD_3_1_BASE:1.37
	UBC_SYNC_B:1.42
	UBC:1.30.0.2
	UBC_BASE:1.30
	OPENBSD_3_0:1.23.0.2
	OPENBSD_3_0_BASE:1.23
	OPENBSD_2_9_BASE:1.22
	OPENBSD_2_9:1.22.0.2
	OPENBSD_2_8:1.10.0.2
	OPENBSD_2_8_BASE:1.10
	OPENBSD_2_7:1.9.0.2
	OPENBSD_2_7_BASE:1.9
	SMP:1.8.0.2
	SMP_BASE:1.8
	kame_19991208:1.8
	OPENBSD_2_6:1.7.0.2
	OPENBSD_2_6_BASE:1.7
	OPENBSD_2_5:1.6.0.2
	OPENBSD_2_5_BASE:1.6
	OPENBSD_2_4:1.5.0.2
	OPENBSD_2_4_BASE:1.5
	OPENBSD_2_3:1.4.0.2
	OPENBSD_2_3_BASE:1.4;
locks; strict;
comment	@ * @;


1.135
date	2016.11.07.00.26.33;	author guenther;	state Exp;
branches;
next	1.134;
commitid	W7ztnDZwvjCaeQTS;

1.134
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.133;
commitid	RlO92XR575sygHqm;

1.133
date	2016.06.19.10.21.56;	author dlg;	state Exp;
branches;
next	1.132;
commitid	J5HRbCm5olgUBGNw;

1.132
date	2016.03.19.12.04.16;	author natano;	state Exp;
branches;
next	1.131;
commitid	gAjwyca5TfuoJAhn;

1.131
date	2015.01.09.05.01.57;	author tedu;	state Exp;
branches;
next	1.130;
commitid	KWogeIYA2sxG3IjB;

1.130
date	2014.12.23.01.53.34;	author tedu;	state Exp;
branches;
next	1.129;
commitid	6nkNTy1tuOQfzz0f;

1.129
date	2014.11.18.10.42.15;	author dlg;	state Exp;
branches;
next	1.128;
commitid	4DOHz2gKfCx7RejS;

1.128
date	2014.07.12.18.44.01;	author tedu;	state Exp;
branches;
next	1.127;
commitid	bDGgAR6yEQVcVl5u;

1.127
date	2014.05.22.02.02.39;	author guenther;	state Exp;
branches;
next	1.126;

1.126
date	2014.04.22.20.14.39;	author beck;	state Exp;
branches;
next	1.125;

1.125
date	2014.02.04.01.04.03;	author tedu;	state Exp;
branches;
next	1.124;

1.124
date	2013.12.12.19.00.10;	author tedu;	state Exp;
branches;
next	1.123;

1.123
date	2013.12.01.16.40.56;	author krw;	state Exp;
branches;
next	1.122;

1.122
date	2013.11.03.02.22.07;	author krw;	state Exp;
branches;
next	1.121;

1.121
date	2013.11.01.17.36.19;	author krw;	state Exp;
branches;
next	1.120;

1.120
date	2013.08.09.08.20.05;	author syl;	state Exp;
branches;
next	1.119;

1.119
date	2013.06.11.16.42.18;	author deraadt;	state Exp;
branches;
next	1.118;

1.118
date	2013.05.30.19.19.09;	author guenther;	state Exp;
branches;
next	1.117;

1.117
date	2013.04.04.17.29.36;	author beck;	state Exp;
branches;
next	1.116;

1.116
date	2013.02.17.17.39.29;	author miod;	state Exp;
branches;
next	1.115;

1.115
date	2013.02.09.20.56.35;	author miod;	state Exp;
branches;
next	1.114;

1.114
date	2013.01.16.22.41.47;	author beck;	state Exp;
branches;
next	1.113;

1.113
date	2012.12.10.22.58.04;	author beck;	state Exp;
branches;
next	1.112;

1.112
date	2011.09.18.23.20.28;	author bluhm;	state Exp;
branches;
next	1.111;

1.111
date	2011.09.18.11.18.28;	author miod;	state Exp;
branches;
next	1.110;

1.110
date	2011.08.17.15.48.22;	author thib;	state Exp;
branches;
next	1.109;

1.109
date	2011.08.16.14.36.39;	author thib;	state Exp;
branches;
next	1.108;

1.108
date	2011.08.03.20.21.19;	author beck;	state Exp;
branches;
next	1.107;

1.107
date	2011.07.04.20.35.35;	author deraadt;	state Exp;
branches;
next	1.106;

1.106
date	2011.07.04.04.30.41;	author tedu;	state Exp;
branches;
next	1.105;

1.105
date	2011.07.03.18.23.10;	author tedu;	state Exp;
branches;
next	1.104;

1.104
date	2011.06.29.12.15.26;	author tedu;	state Exp;
branches;
next	1.103;

1.103
date	2011.04.12.19.45.43;	author beck;	state Exp;
branches;
next	1.102;

1.102
date	2010.03.29.23.33.39;	author krw;	state Exp;
branches;
next	1.101;

1.101
date	2009.09.03.07.47.47;	author jasper;	state Exp;
branches;
next	1.100;

1.100
date	2009.06.25.15.49.26;	author thib;	state Exp;
branches;
next	1.99;

1.99
date	2009.06.06.18.06.22;	author art;	state Exp;
branches;
next	1.98;

1.98
date	2008.06.14.10.55.21;	author mk;	state Exp;
branches;
next	1.97;

1.97
date	2008.06.12.06.58.40;	author deraadt;	state Exp;
branches;
next	1.96;

1.96
date	2008.06.11.12.35.46;	author deraadt;	state Exp;
branches;
next	1.95;

1.95
date	2008.06.10.20.14.37;	author beck;	state Exp;
branches;
next	1.94;

1.94
date	2008.01.05.19.49.26;	author otto;	state Exp;
branches;
next	1.93;

1.93
date	2007.10.29.17.06.20;	author chl;	state Exp;
branches;
next	1.92;

1.92
date	2007.07.11.15.32.22;	author millert;	state Exp;
branches;
next	1.91;

1.91
date	2007.06.01.20.23.26;	author pedro;	state Exp;
branches;
next	1.90;

1.90
date	2007.06.01.18.54.27;	author pedro;	state Exp;
branches;
next	1.89;

1.89
date	2007.06.01.06.38.54;	author deraadt;	state Exp;
branches;
next	1.88;

1.88
date	2007.05.27.20.06.40;	author otto;	state Exp;
branches;
next	1.87;

1.87
date	2007.05.26.20.26.51;	author pedro;	state Exp;
branches;
next	1.86;

1.86
date	2007.04.15.10.48.35;	author pedro;	state Exp;
branches;
next	1.85;

1.85
date	2007.04.04.18.53.20;	author pedro;	state Exp;
branches;
next	1.84;

1.84
date	2007.03.15.10.22.30;	author art;	state Exp;
branches;
next	1.83;

1.83
date	2007.02.04.10.36.05;	author pedro;	state Exp;
branches;
next	1.82;

1.82
date	2007.01.17.16.43.36;	author pedro;	state Exp;
branches;
next	1.81;

1.81
date	2007.01.15.11.18.17;	author pedro;	state Exp;
branches;
next	1.80;

1.80
date	2007.01.15.11.05.53;	author pedro;	state Exp;
branches;
next	1.79;

1.79
date	2006.11.07.12.29.45;	author mickey;	state Exp;
branches;
next	1.78;

1.78
date	2006.10.20.13.02.55;	author mickey;	state Exp;
branches;
next	1.77;

1.77
date	2006.10.19.14.37.54;	author mickey;	state Exp;
branches;
next	1.76;

1.76
date	2006.09.30.14.47.52;	author mickey;	state Exp;
branches;
next	1.75;

1.75
date	2006.09.26.09.50.31;	author mickey;	state Exp;
branches;
next	1.74;

1.74
date	2006.09.26.09.26.36;	author mickey;	state Exp;
branches;
next	1.73;

1.73
date	2006.07.27.11.34.13;	author mickey;	state Exp;
branches;
next	1.72;

1.72
date	2006.07.11.21.17.58;	author mickey;	state Exp;
branches;
next	1.71;

1.71
date	2006.06.28.14.17.07;	author mickey;	state Exp;
branches;
next	1.70;

1.70
date	2006.06.21.10.01.10;	author mickey;	state Exp;
branches;
next	1.69;

1.69
date	2006.06.07.10.51.43;	author mickey;	state Exp;
branches;
next	1.68;

1.68
date	2006.01.03.23.34.39;	author pedro;	state Exp;
branches;
next	1.67;

1.67
date	2005.12.28.20.48.17;	author pedro;	state Exp;
branches;
next	1.66;

1.66
date	2005.12.17.13.56.01;	author pedro;	state Exp;
branches;
next	1.65;

1.65
date	2005.12.14.22.04.56;	author pedro;	state Exp;
branches;
next	1.64;

1.64
date	2005.10.25.20.12.52;	author pedro;	state Exp;
branches;
next	1.63;

1.63
date	2005.09.26.21.11.09;	author pedro;	state Exp;
branches;
next	1.62;

1.62
date	2005.09.06.17.02.09;	author pedro;	state Exp;
branches;
next	1.61;

1.61
date	2005.08.08.09.48.02;	author pedro;	state Exp;
branches;
next	1.60;

1.60
date	2005.07.20.16.30.34;	author pedro;	state Exp;
branches;
next	1.59;

1.59
date	2005.07.03.20.14.01;	author drahn;	state Exp;
branches;
next	1.58;

1.58
date	2005.06.18.18.09.43;	author millert;	state Exp;
branches;
next	1.57;

1.57
date	2005.06.10.17.37.40;	author pedro;	state Exp;
branches;
next	1.56;

1.56
date	2005.05.24.04.33.35;	author pedro;	state Exp;
branches;
next	1.55;

1.55
date	2004.12.09.17.41.53;	author millert;	state Exp;
branches;
next	1.54;

1.54
date	2004.12.08.16.17.52;	author millert;	state Exp;
branches;
next	1.53;

1.53
date	2004.12.07.04.37.28;	author tedu;	state Exp;
branches;
next	1.52;

1.52
date	2004.07.13.21.04.29;	author millert;	state Exp;
branches;
next	1.51;

1.51
date	2004.06.24.19.35.26;	author tholo;	state Exp;
branches;
next	1.50;

1.50
date	2004.05.07.01.40.16;	author tedu;	state Exp;
branches;
next	1.49;

1.49
date	2004.01.20.03.44.06;	author tedu;	state Exp;
branches;
next	1.48;

1.48
date	2003.11.19.03.29.31;	author mickey;	state Exp;
branches;
next	1.47;

1.47
date	2003.11.19.02.52.13;	author tedu;	state Exp;
branches;
next	1.46;

1.46
date	2003.08.25.23.26.55;	author tedu;	state Exp;
branches;
next	1.45;

1.45
date	2003.08.02.01.47.17;	author tedu;	state Exp;
branches;
next	1.44;

1.44
date	2003.05.26.18.33.17;	author tedu;	state Exp;
branches;
next	1.43;

1.43
date	2003.05.14.01.12.27;	author jason;	state Exp;
branches;
next	1.42;

1.42
date	2002.10.12.01.09.45;	author krw;	state Exp;
branches;
next	1.41;

1.41
date	2002.07.16.22.54.42;	author millert;	state Exp;
branches;
next	1.40;

1.40
date	2002.07.12.14.02.23;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2002.06.08.04.51.05;	author art;	state Exp;
branches;
next	1.38;

1.38
date	2002.06.08.04.44.01;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2002.03.14.01.27.14;	author millert;	state Exp;
branches;
next	1.36;

1.36
date	2002.02.22.20.37.46;	author drahn;	state Exp;
branches;
next	1.35;

1.35
date	2002.01.29.14.31.59;	author millert;	state Exp;
branches;
next	1.34;

1.34
date	2002.01.25.02.30.26;	author millert;	state Exp;
branches;
next	1.33;

1.33
date	2002.01.24.21.42.24;	author mickey;	state Exp;
branches;
next	1.32;

1.32
date	2002.01.23.21.24.02;	author millert;	state Exp;
branches;
next	1.31;

1.31
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.12.10.02.19.34;	author art;	state Exp;
branches
	1.30.2.1;
next	1.29;

1.29
date	2001.12.05.00.36.17;	author niklas;	state Exp;
branches;
next	1.28;

1.28
date	2001.12.04.15.05.56;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.28.01.18.10;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.13.14.19.24;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.06.19.53.21;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2001.06.23.02.07.54;	author csapuntz;	state Exp;
branches;
next	1.22;

1.22
date	2001.04.06.18.59.16;	author gluk;	state Exp;
branches;
next	1.21;

1.21
date	2001.04.04.20.19.03;	author gluk;	state Exp;
branches;
next	1.20;

1.20
date	2001.03.08.10.56.47;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.03.04.07.00.33;	author csapuntz;	state Exp;
branches;
next	1.18;

1.18
date	2001.03.04.06.46.31;	author csapuntz;	state Exp;
branches;
next	1.17;

1.17
date	2001.02.27.09.07.54;	author csapuntz;	state Exp;
branches;
next	1.16;

1.16
date	2001.02.26.17.54.07;	author csapuntz;	state Exp;
branches;
next	1.15;

1.15
date	2001.02.26.17.43.34;	author csapuntz;	state Exp;
branches;
next	1.14;

1.14
date	2001.02.24.19.07.09;	author csapuntz;	state Exp;
branches;
next	1.13;

1.13
date	2001.02.23.14.52.51;	author csapuntz;	state Exp;
branches;
next	1.12;

1.12
date	2001.02.21.23.24.31;	author csapuntz;	state Exp;
branches;
next	1.11;

1.11
date	2001.02.10.11.08.39;	author fgsch;	state Exp;
branches;
next	1.10;

1.10
date	2000.06.26.22.50.03;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2000.03.04.16.40.29;	author art;	state Exp;
branches;
next	1.8;

1.8
date	99.12.05.08.30.38;	author art;	state Exp;
branches
	1.8.2.1;
next	1.7;

1.7
date	99.04.28.09.28.17;	author art;	state Exp;
branches;
next	1.6;

1.6
date	98.11.12.04.30.03;	author csapuntz;	state Exp;
branches;
next	1.5;

1.5
date	98.08.29.00.02.51;	author csapuntz;	state Exp;
branches;
next	1.4;

1.4
date	98.03.15.20.37.25;	author millert;	state Exp;
branches;
next	1.3;

1.3
date	98.03.15.03.53.51;	author millert;	state Exp;
branches;
next	1.2;

1.2
date	97.11.07.10.28.37;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	97.11.06.18.22.22;	author csapuntz;	state Exp;
branches;
next	;

1.8.2.1
date	2000.03.24.09.09.46;	author niklas;	state Exp;
branches;
next	1.8.2.2;

1.8.2.2
date	2001.05.14.22.47.40;	author niklas;	state Exp;
branches;
next	1.8.2.3;

1.8.2.3
date	2001.07.04.11.00.49;	author niklas;	state Exp;
branches;
next	1.8.2.4;

1.8.2.4
date	2001.11.13.23.02.30;	author niklas;	state Exp;
branches;
next	1.8.2.5;

1.8.2.5
date	2001.12.05.01.02.41;	author niklas;	state Exp;
branches;
next	1.8.2.6;

1.8.2.6
date	2002.03.06.02.17.13;	author niklas;	state Exp;
branches;
next	1.8.2.7;

1.8.2.7
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.8.2.8;

1.8.2.8
date	2003.03.28.00.08.47;	author niklas;	state Exp;
branches;
next	1.8.2.9;

1.8.2.9
date	2003.05.16.00.29.45;	author niklas;	state Exp;
branches;
next	1.8.2.10;

1.8.2.10
date	2003.06.07.11.09.08;	author ho;	state Exp;
branches;
next	1.8.2.11;

1.8.2.11
date	2004.02.19.11.01.36;	author niklas;	state Exp;
branches;
next	1.8.2.12;

1.8.2.12
date	2004.06.05.23.13.11;	author niklas;	state Exp;
branches;
next	;

1.30.2.1
date	2002.01.31.22.55.49;	author niklas;	state Exp;
branches;
next	1.30.2.2;

1.30.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.30.2.3;

1.30.2.3
date	2002.06.11.03.32.50;	author art;	state Exp;
branches;
next	1.30.2.4;

1.30.2.4
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.30.2.5;

1.30.2.5
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	1.30.2.6;

1.30.2.6
date	2003.05.19.22.38.11;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.135
log
@Split PID from TID, giving processes a PID unrelated to the TID of their
initial thread

ok jsing@@ kettenis@@
@
text
@/*	$OpenBSD: ffs_softdep.c,v 1.134 2016/09/15 02:00:18 dlg Exp $	*/

/*
 * Copyright 1998, 2000 Marshall Kirk McKusick. All Rights Reserved.
 *
 * The soft updates code is derived from the appendix of a University
 * of Michigan technical report (Gregory R. Ganger and Yale N. Patt,
 * "Soft Updates: A Solution to the Metadata Update Problem in File
 * Systems", CSE-TR-254-95, August 1995).
 *
 * Further information about soft updates can be obtained from:
 *
 *	Marshall Kirk McKusick		http://www.mckusick.com/softdep/
 *	1614 Oxford Street		mckusick@@mckusick.com
 *	Berkeley, CA 94709-1608		+1-510-843-9542
 *	USA
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY MARSHALL KIRK MCKUSICK ``AS IS'' AND ANY
 * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL MARSHALL KIRK MCKUSICK BE LIABLE FOR
 * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	from: @@(#)ffs_softdep.c	9.59 (McKusick) 6/21/00
 * $FreeBSD: src/sys/ufs/ffs/ffs_softdep.c,v 1.86 2001/02/04 16:08:18 phk Exp $
 */

#include <sys/param.h>
#include <sys/buf.h>
#include <sys/kernel.h>
#include <sys/malloc.h>
#include <sys/mount.h>
#include <sys/proc.h>
#include <sys/pool.h>
#include <sys/syslog.h>
#include <sys/systm.h>
#include <sys/vnode.h>
#include <sys/specdev.h>
#include <crypto/siphash.h>
#include <ufs/ufs/dir.h>
#include <ufs/ufs/quota.h>
#include <ufs/ufs/inode.h>
#include <ufs/ufs/ufsmount.h>
#include <ufs/ffs/fs.h>
#include <ufs/ffs/softdep.h>
#include <ufs/ffs/ffs_extern.h>
#include <ufs/ufs/ufs_extern.h>

#define STATIC

/*
 * Mapping of dependency structure types to malloc types.
 */
#define	D_PAGEDEP	0
#define	D_INODEDEP	1
#define	D_NEWBLK	2
#define	D_BMSAFEMAP	3
#define	D_ALLOCDIRECT	4
#define	D_INDIRDEP	5
#define	D_ALLOCINDIR	6
#define	D_FREEFRAG	7
#define	D_FREEBLKS	8
#define	D_FREEFILE	9
#define	D_DIRADD	10
#define	D_MKDIR		11
#define	D_DIRREM	12
#define	D_NEWDIRBLK	13
#define	D_LAST		13
/*
 * Names of softdep types.
 */
const char *softdep_typenames[] = {
	"pagedep",
	"inodedep",
	"newblk",
	"bmsafemap",
	"allocdirect",
	"indirdep",
	"allocindir",
	"freefrag",
	"freeblks",
	"freefile",
	"diradd",
	"mkdir",
	"dirrem",
	"newdirblk",
};
#define	TYPENAME(type) \
	((unsigned)(type) <= D_LAST ? softdep_typenames[type] : "???")
/*
 * Finding the current process.
 */
#define CURPROC curproc
/*
 * End system adaptation definitions.
 */

/*
 * Internal function prototypes.
 */
STATIC	void softdep_error(char *, int);
STATIC	void drain_output(struct vnode *, int);
STATIC	int getdirtybuf(struct buf *, int);
STATIC	void clear_remove(struct proc *);
STATIC	void clear_inodedeps(struct proc *);
STATIC	int flush_pagedep_deps(struct vnode *, struct mount *,
	    struct diraddhd *);
STATIC	int flush_inodedep_deps(struct fs *, ufsino_t);
STATIC	int handle_written_filepage(struct pagedep *, struct buf *);
STATIC  void diradd_inode_written(struct diradd *, struct inodedep *);
STATIC	int handle_written_inodeblock(struct inodedep *, struct buf *);
STATIC	void handle_allocdirect_partdone(struct allocdirect *);
STATIC	void handle_allocindir_partdone(struct allocindir *);
STATIC	void initiate_write_filepage(struct pagedep *, struct buf *);
STATIC	void handle_written_mkdir(struct mkdir *, int);
STATIC	void initiate_write_inodeblock_ufs1(struct inodedep *, struct buf *);
#ifdef FFS2
STATIC	void initiate_write_inodeblock_ufs2(struct inodedep *, struct buf *);
#endif
STATIC	void handle_workitem_freefile(struct freefile *);
STATIC	void handle_workitem_remove(struct dirrem *);
STATIC	struct dirrem *newdirrem(struct buf *, struct inode *,
	    struct inode *, int, struct dirrem **);
STATIC	void free_diradd(struct diradd *);
STATIC	void free_allocindir(struct allocindir *, struct inodedep *);
STATIC	void free_newdirblk(struct newdirblk *);
STATIC	int indir_trunc(struct inode *, daddr_t, int, daddr_t, long *);
STATIC	void deallocate_dependencies(struct buf *, struct inodedep *);
STATIC	void free_allocdirect(struct allocdirectlst *,
	    struct allocdirect *, int);
STATIC	int check_inode_unwritten(struct inodedep *);
STATIC	int free_inodedep(struct inodedep *);
STATIC	void handle_workitem_freeblocks(struct freeblks *);
STATIC	void merge_inode_lists(struct inodedep *);
STATIC	void setup_allocindir_phase2(struct buf *, struct inode *,
	    struct allocindir *);
STATIC	struct allocindir *newallocindir(struct inode *, int, daddr_t,
	    daddr_t);
STATIC	void handle_workitem_freefrag(struct freefrag *);
STATIC	struct freefrag *newfreefrag(struct inode *, daddr_t, long);
STATIC	void allocdirect_merge(struct allocdirectlst *,
	    struct allocdirect *, struct allocdirect *);
STATIC	struct bmsafemap *bmsafemap_lookup(struct buf *);
STATIC	int newblk_lookup(struct fs *, daddr_t, int,
	    struct newblk **);
STATIC	int inodedep_lookup(struct fs *, ufsino_t, int, struct inodedep **);
STATIC	int pagedep_lookup(struct inode *, daddr_t, int, struct pagedep **);
STATIC	void pause_timer(void *);
STATIC	int request_cleanup(int, int);
STATIC	int process_worklist_item(struct mount *, int);
STATIC	void add_to_worklist(struct worklist *);

/*
 * Exported softdep operations.
 */
void softdep_disk_io_initiation(struct buf *);
void softdep_disk_write_complete(struct buf *);
void softdep_deallocate_dependencies(struct buf *);
void softdep_move_dependencies(struct buf *, struct buf *);
int softdep_count_dependencies(struct buf *bp, int, int);

/*
 * Locking primitives.
 *
 * For a uniprocessor, all we need to do is protect against disk
 * interrupts. For a multiprocessor, this lock would have to be
 * a mutex. A single mutex is used throughout this file, though
 * finer grain locking could be used if contention warranted it.
 *
 * For a multiprocessor, the sleep call would accept a lock and
 * release it after the sleep processing was complete. In a uniprocessor
 * implementation there is no such interlock, so we simple mark
 * the places where it needs to be done with the `interlocked' form
 * of the lock calls. Since the uniprocessor sleep already interlocks
 * the spl, there is nothing that really needs to be done.
 */
#ifndef /* NOT */ DEBUG
STATIC struct lockit {
	int	lkt_spl;
} lk = { 0 };
#define ACQUIRE_LOCK(lk)		(lk)->lkt_spl = splbio()
#define FREE_LOCK(lk)			splx((lk)->lkt_spl)
#define ACQUIRE_LOCK_INTERLOCKED(lk,s)	(lk)->lkt_spl = (s)
#define FREE_LOCK_INTERLOCKED(lk)	((lk)->lkt_spl)

#else /* DEBUG */
STATIC struct lockit {
	int	lkt_spl;
	pid_t	lkt_held;
	int     lkt_line;
} lk = { 0, -1 };
STATIC int lockcnt;

STATIC	void acquire_lock(struct lockit *, int);
STATIC	void free_lock(struct lockit *, int);
STATIC	void acquire_lock_interlocked(struct lockit *, int, int);
STATIC	int free_lock_interlocked(struct lockit *, int);

#define ACQUIRE_LOCK(lk)		acquire_lock(lk, __LINE__)
#define FREE_LOCK(lk)			free_lock(lk, __LINE__)
#define ACQUIRE_LOCK_INTERLOCKED(lk,s)	acquire_lock_interlocked(lk, (s), __LINE__)
#define FREE_LOCK_INTERLOCKED(lk)	free_lock_interlocked(lk, __LINE__)

STATIC void
acquire_lock(struct lockit *lk, int line)
{
	pid_t holder;
	int original_line;

	if (lk->lkt_held != -1) {
		holder = lk->lkt_held;
		original_line = lk->lkt_line;
		FREE_LOCK(lk);
		if (holder == CURPROC->p_tid)
			panic("softdep_lock: locking against myself, acquired at line %d, relocked at line %d", original_line, line);
		else
			panic("softdep_lock: lock held by %d, acquired at line %d, relocked at line %d", holder, original_line, line);
	}
	lk->lkt_spl = splbio();
	lk->lkt_held = CURPROC->p_tid;
	lk->lkt_line = line;
	lockcnt++;
}

STATIC void
free_lock(struct lockit *lk, int line)
{

	if (lk->lkt_held == -1)
		panic("softdep_unlock: lock not held at line %d", line);
	lk->lkt_held = -1;
	splx(lk->lkt_spl);
}

STATIC void
acquire_lock_interlocked(struct lockit *lk, int s, int line)
{
	pid_t holder;
	int original_line;

	if (lk->lkt_held != -1) {
		holder = lk->lkt_held;
		original_line = lk->lkt_line;
		FREE_LOCK_INTERLOCKED(lk);
		if (holder == CURPROC->p_tid)
			panic("softdep_lock: locking against myself, acquired at line %d, relocked at line %d", original_line, line);
		else
			panic("softdep_lock: lock held by %d, acquired at line %d, relocked at line %d", holder, original_line, line);
	}
	lk->lkt_held = CURPROC->p_tid;
	lk->lkt_line = line;
	lk->lkt_spl = s;
	lockcnt++;
}

STATIC int
free_lock_interlocked(struct lockit *lk, int line)
{

	if (lk->lkt_held == -1)
		panic("softdep_unlock_interlocked: lock not held at line %d", line);
	lk->lkt_held = -1;

	return (lk->lkt_spl);
}
#endif /* DEBUG */

/*
 * Place holder for real semaphores.
 */
struct sema {
	int	value;
	pid_t	holder;
	char	*name;
	int	prio;
	int	timo;
};
STATIC	void sema_init(struct sema *, char *, int, int);
STATIC	int sema_get(struct sema *, struct lockit *);
STATIC	void sema_release(struct sema *);

STATIC void
sema_init(struct sema *semap, char *name, int prio, int timo)
{

	semap->holder = -1;
	semap->value = 0;
	semap->name = name;
	semap->prio = prio;
	semap->timo = timo;
}

STATIC int
sema_get(struct sema *semap, struct lockit *interlock)
{
	int s;

	if (semap->value++ > 0) {
		if (interlock != NULL)
			s = FREE_LOCK_INTERLOCKED(interlock);
		tsleep((caddr_t)semap, semap->prio, semap->name, semap->timo);
		if (interlock != NULL) {
			ACQUIRE_LOCK_INTERLOCKED(interlock, s);
			FREE_LOCK(interlock);
		}
		return (0);
	}
	semap->holder = CURPROC->p_tid;
	if (interlock != NULL)
		FREE_LOCK(interlock);
	return (1);
}

STATIC void
sema_release(struct sema *semap)
{

	if (semap->value <= 0 || semap->holder != CURPROC->p_tid) {
#ifdef DEBUG
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
#endif
		panic("sema_release: not held");
	}
	if (--semap->value > 0) {
		semap->value = 0;
		wakeup(semap);
	}
	semap->holder = -1;
}

/*
 * Memory management.
 */
STATIC struct pool pagedep_pool;
STATIC struct pool inodedep_pool;
STATIC struct pool newblk_pool;
STATIC struct pool bmsafemap_pool;
STATIC struct pool allocdirect_pool;
STATIC struct pool indirdep_pool;
STATIC struct pool allocindir_pool;
STATIC struct pool freefrag_pool;
STATIC struct pool freeblks_pool;
STATIC struct pool freefile_pool;
STATIC struct pool diradd_pool;
STATIC struct pool mkdir_pool;
STATIC struct pool dirrem_pool;
STATIC struct pool newdirblk_pool;

static __inline void
softdep_free(struct worklist *item, int type)
{

	switch (type) {
	case D_PAGEDEP:
		pool_put(&pagedep_pool, item);
		break;

	case D_INODEDEP:
		pool_put(&inodedep_pool, item);
		break;

	case D_BMSAFEMAP:
		pool_put(&bmsafemap_pool, item);
		break;

	case D_ALLOCDIRECT:
		pool_put(&allocdirect_pool, item);
		break;

	case D_INDIRDEP:
		pool_put(&indirdep_pool, item);
		break;

	case D_ALLOCINDIR:
		pool_put(&allocindir_pool, item);
		break;

	case D_FREEFRAG:
		pool_put(&freefrag_pool, item);
		break;

	case D_FREEBLKS:
		pool_put(&freeblks_pool, item);
		break;

	case D_FREEFILE:
		pool_put(&freefile_pool, item);
		break;

	case D_DIRADD:
		pool_put(&diradd_pool, item);
		break;

	case D_MKDIR:
		pool_put(&mkdir_pool, item);
		break;

	case D_DIRREM:
		pool_put(&dirrem_pool, item);
		break;

	case D_NEWDIRBLK:
		pool_put(&newdirblk_pool, item);
		break;

	default:
#ifdef DEBUG
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
#endif
		panic("softdep_free: unknown type %d", type);
	}
}

struct workhead softdep_freequeue;

static __inline void
softdep_freequeue_add(struct worklist *item)
{
	int s;

	s = splbio();
	LIST_INSERT_HEAD(&softdep_freequeue, item, wk_list);
	splx(s);
}

static __inline void
softdep_freequeue_process(void)
{
	struct worklist *wk;

	splassert(IPL_BIO);

	while ((wk = LIST_FIRST(&softdep_freequeue)) != NULL) {
		LIST_REMOVE(wk, wk_list);
		FREE_LOCK(&lk);
		softdep_free(wk, wk->wk_type);
		ACQUIRE_LOCK(&lk);
	}
}

/*
 * Worklist queue management.
 * These routines require that the lock be held.
 */
#ifndef /* NOT */ DEBUG
#define WORKLIST_INSERT(head, item) do {	\
	(item)->wk_state |= ONWORKLIST;		\
	LIST_INSERT_HEAD(head, item, wk_list);	\
} while (0)
#define WORKLIST_REMOVE(item) do {		\
	(item)->wk_state &= ~ONWORKLIST;	\
	LIST_REMOVE(item, wk_list);		\
} while (0)
#define WORKITEM_FREE(item, type) softdep_freequeue_add((struct worklist *)item)

#else /* DEBUG */
STATIC	void worklist_insert(struct workhead *, struct worklist *);
STATIC	void worklist_remove(struct worklist *);
STATIC	void workitem_free(struct worklist *);

#define WORKLIST_INSERT(head, item) worklist_insert(head, item)
#define WORKLIST_REMOVE(item) worklist_remove(item)
#define WORKITEM_FREE(item, type) workitem_free((struct worklist *)item)

STATIC void
worklist_insert(struct workhead *head, struct worklist *item)
{

	if (lk.lkt_held == -1)
		panic("worklist_insert: lock not held");
	if (item->wk_state & ONWORKLIST) {
		FREE_LOCK(&lk);
		panic("worklist_insert: already on list");
	}
	item->wk_state |= ONWORKLIST;
	LIST_INSERT_HEAD(head, item, wk_list);
}

STATIC void
worklist_remove(struct worklist *item)
{

	if (lk.lkt_held == -1)
		panic("worklist_remove: lock not held");
	if ((item->wk_state & ONWORKLIST) == 0) {
		FREE_LOCK(&lk);
		panic("worklist_remove: not on list");
	}
	item->wk_state &= ~ONWORKLIST;
	LIST_REMOVE(item, wk_list);
}

STATIC void
workitem_free(struct worklist *item)
{

	if (item->wk_state & ONWORKLIST) {
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
		panic("workitem_free: still on list");
	}
	softdep_freequeue_add(item);
}
#endif /* DEBUG */

/*
 * Workitem queue management
 */
STATIC struct workhead softdep_workitem_pending;
STATIC struct worklist *worklist_tail;
STATIC int num_on_worklist;	/* number of worklist items to be processed */
STATIC int softdep_worklist_busy; /* 1 => trying to do unmount */
STATIC int softdep_worklist_req; /* serialized waiters */
STATIC int max_softdeps;	/* maximum number of structs before slowdown */
STATIC int tickdelay = 2;	/* number of ticks to pause during slowdown */
STATIC int proc_waiting;	/* tracks whether we have a timeout posted */
STATIC int *stat_countp;	/* statistic to count in proc_waiting timeout */
STATIC struct timeout proc_waiting_timeout; 
STATIC struct proc *filesys_syncer; /* proc of filesystem syncer process */
STATIC int req_clear_inodedeps;	/* syncer process flush some inodedeps */
#define FLUSH_INODES	1
STATIC int req_clear_remove;	/* syncer process flush some freeblks */
#define FLUSH_REMOVE	2
/*
 * runtime statistics
 */
STATIC int stat_worklist_push;	/* number of worklist cleanups */
STATIC int stat_blk_limit_push;	/* number of times block limit neared */
STATIC int stat_ino_limit_push;	/* number of times inode limit neared */
STATIC int stat_blk_limit_hit;	/* number of times block slowdown imposed */
STATIC int stat_ino_limit_hit;	/* number of times inode slowdown imposed */
STATIC int stat_sync_limit_hit;	/* number of synchronous slowdowns imposed */
STATIC int stat_indir_blk_ptrs;	/* bufs redirtied as indir ptrs not written */
STATIC int stat_inode_bitmap;	/* bufs redirtied as inode bitmap not written */
STATIC int stat_direct_blk_ptrs;/* bufs redirtied as direct ptrs not written */
STATIC int stat_dir_entry;	/* bufs redirtied as dir entry cannot write */

/*
 * Add an item to the end of the work queue.
 * This routine requires that the lock be held.
 * This is the only routine that adds items to the list.
 * The following routine is the only one that removes items
 * and does so in order from first to last.
 */
STATIC void
add_to_worklist(struct worklist *wk)
{

	if (wk->wk_state & ONWORKLIST) {
#ifdef DEBUG
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
#endif
		panic("add_to_worklist: already on list");
	}
	wk->wk_state |= ONWORKLIST;
	if (LIST_FIRST(&softdep_workitem_pending) == NULL)
		LIST_INSERT_HEAD(&softdep_workitem_pending, wk, wk_list);
	else
		LIST_INSERT_AFTER(worklist_tail, wk, wk_list);
	worklist_tail = wk;
	num_on_worklist += 1;
}

/*
 * Process that runs once per second to handle items in the background queue.
 *
 * Note that we ensure that everything is done in the order in which they
 * appear in the queue. The code below depends on this property to ensure
 * that blocks of a file are freed before the inode itself is freed. This
 * ordering ensures that no new <vfsid, inum, lbn> triples will be generated
 * until all the old ones have been purged from the dependency lists.
 */
int 
softdep_process_worklist(struct mount *matchmnt)
{
	struct proc *p = CURPROC;
	int matchcnt, loopcount;
	struct timeval starttime;

	/*
	 * First process any items on the delayed-free queue.
	 */
	ACQUIRE_LOCK(&lk);
	softdep_freequeue_process();
	FREE_LOCK(&lk);

	/*
	 * Record the process identifier of our caller so that we can give
	 * this process preferential treatment in request_cleanup below.
	 * We can't do this in softdep_initialize, because the syncer doesn't
	 * have to run then.
	 * NOTE! This function _could_ be called with a curproc != syncerproc.
	 */
	filesys_syncer = syncerproc;
	matchcnt = 0;

	/*
	 * There is no danger of having multiple processes run this
	 * code, but we have to single-thread it when softdep_flushfiles()
	 * is in operation to get an accurate count of the number of items
	 * related to its mount point that are in the list.
	 */
	if (matchmnt == NULL) {
		if (softdep_worklist_busy < 0)
			return(-1);
		softdep_worklist_busy += 1;
	}

	/*
	 * If requested, try removing inode or removal dependencies.
	 */
	if (req_clear_inodedeps) {
		clear_inodedeps(p);
		req_clear_inodedeps -= 1;
		wakeup_one(&proc_waiting);
	}
	if (req_clear_remove) {
		clear_remove(p);
		req_clear_remove -= 1;
		wakeup_one(&proc_waiting);
	}
	loopcount = 1;
	getmicrouptime(&starttime);
	while (num_on_worklist > 0) {
		matchcnt += process_worklist_item(matchmnt, 0);

		/*
		 * If a umount operation wants to run the worklist
		 * accurately, abort.
		 */
		if (softdep_worklist_req && matchmnt == NULL) {
			matchcnt = -1;
			break;
		}

		/*
		 * If requested, try removing inode or removal dependencies.
		 */
		if (req_clear_inodedeps) {
			clear_inodedeps(p);
			req_clear_inodedeps -= 1;
			wakeup_one(&proc_waiting);
		}
		if (req_clear_remove) {
			clear_remove(p);
			req_clear_remove -= 1;
			wakeup_one(&proc_waiting);
		}
		/*
		 * We do not generally want to stop for buffer space, but if
		 * we are really being a buffer hog, we will stop and wait.
		 */
#if 0
		if (loopcount++ % 128 == 0)
			bwillwrite();
#endif
		/*
		 * Never allow processing to run for more than one
		 * second. Otherwise the other syncer tasks may get
		 * excessively backlogged.
		 */
		{
			struct timeval diff;
			struct timeval tv;

			getmicrouptime(&tv);
			timersub(&tv, &starttime, &diff);
			if (diff.tv_sec != 0 && matchmnt == NULL) {
				matchcnt = -1;
				break;
			}
		}

		/*
		 * Process any new items on the delayed-free queue.
		 */
		ACQUIRE_LOCK(&lk);
		softdep_freequeue_process();
		FREE_LOCK(&lk);
	}
	if (matchmnt == NULL) {
		softdep_worklist_busy -= 1;
		if (softdep_worklist_req && softdep_worklist_busy == 0)
			wakeup(&softdep_worklist_req);
	}
	return (matchcnt);
}

/*
 * Process one item on the worklist.
 */
STATIC int
process_worklist_item(struct mount *matchmnt, int flags)
{
	struct worklist *wk, *wkend;
	struct dirrem *dirrem;
	struct mount *mp;
	struct vnode *vp;
	int matchcnt = 0;

	ACQUIRE_LOCK(&lk);
	/*
	 * Normally we just process each item on the worklist in order.
	 * However, if we are in a situation where we cannot lock any
	 * inodes, we have to skip over any dirrem requests whose
	 * vnodes are resident and locked.
	 */
	LIST_FOREACH(wk, &softdep_workitem_pending, wk_list) {
		if ((flags & LK_NOWAIT) == 0 || wk->wk_type != D_DIRREM)
			break;
		dirrem = WK_DIRREM(wk);
		vp = ufs_ihashlookup(VFSTOUFS(dirrem->dm_mnt)->um_dev,
		    dirrem->dm_oldinum);
		if (vp == NULL || !VOP_ISLOCKED(vp))
			break;
	}
	if (wk == NULL) {
		FREE_LOCK(&lk);
		return (0);
	}
	/*
	 * Remove the item to be processed. If we are removing the last
	 * item on the list, we need to recalculate the tail pointer.
	 * As this happens rarely and usually when the list is short,
	 * we just run down the list to find it rather than tracking it
	 * in the above loop.
	 */
	WORKLIST_REMOVE(wk);
	if (wk == worklist_tail) {
		LIST_FOREACH(wkend, &softdep_workitem_pending, wk_list)
			if (LIST_NEXT(wkend, wk_list) == NULL)
				break;
		worklist_tail = wkend;
	}
	num_on_worklist -= 1;
	FREE_LOCK(&lk);
	switch (wk->wk_type) {

	case D_DIRREM:
		/* removal of a directory entry */
		mp = WK_DIRREM(wk)->dm_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: dirrem on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_remove(WK_DIRREM(wk));
		break;

	case D_FREEBLKS:
		/* releasing blocks and/or fragments from a file */
		mp = WK_FREEBLKS(wk)->fb_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: freeblks on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_freeblocks(WK_FREEBLKS(wk));
		break;

	case D_FREEFRAG:
		/* releasing a fragment when replaced as a file grows */
		mp = WK_FREEFRAG(wk)->ff_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: freefrag on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_freefrag(WK_FREEFRAG(wk));
		break;

	case D_FREEFILE:
		/* releasing an inode when its link count drops to 0 */
		mp = WK_FREEFILE(wk)->fx_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: freefile on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_freefile(WK_FREEFILE(wk));
		break;

	default:
		panic("%s_process_worklist: Unknown type %s",
		    "softdep", TYPENAME(wk->wk_type));
		/* NOTREACHED */
	}
	return (matchcnt);
}

/*
 * Move dependencies from one buffer to another.
 */
void
softdep_move_dependencies(struct buf *oldbp, struct buf *newbp)
{
	struct worklist *wk, *wktail;

	if (LIST_FIRST(&newbp->b_dep) != NULL)
		panic("softdep_move_dependencies: need merge code");
	wktail = NULL;
	ACQUIRE_LOCK(&lk);
	while ((wk = LIST_FIRST(&oldbp->b_dep)) != NULL) {
		LIST_REMOVE(wk, wk_list);
		if (wktail == NULL)
			LIST_INSERT_HEAD(&newbp->b_dep, wk, wk_list);
		else
			LIST_INSERT_AFTER(wktail, wk, wk_list);
		wktail = wk;
	}
	FREE_LOCK(&lk);
}

/*
 * Purge the work list of all items associated with a particular mount point.
 */
int
softdep_flushworklist(struct mount *oldmnt, int *countp, struct proc *p)
{
	struct vnode *devvp;
	int count, error = 0;

	/*
	 * Await our turn to clear out the queue, then serialize access.
	 */
	while (softdep_worklist_busy) {
		softdep_worklist_req += 1;
		tsleep(&softdep_worklist_req, PRIBIO, "softflush", 0);
		softdep_worklist_req -= 1;
	}
	softdep_worklist_busy = -1;
	/*
	 * Alternately flush the block device associated with the mount
	 * point and process any dependencies that the flushing
	 * creates. We continue until no more worklist dependencies
	 * are found.
	 */
	*countp = 0;
	devvp = VFSTOUFS(oldmnt)->um_devvp;
	while ((count = softdep_process_worklist(oldmnt)) > 0) {
		*countp += count;
		vn_lock(devvp, LK_EXCLUSIVE | LK_RETRY, p);
		error = VOP_FSYNC(devvp, p->p_ucred, MNT_WAIT, p);
		VOP_UNLOCK(devvp, p);
		if (error)
			break;
	}
	softdep_worklist_busy = 0;
	if (softdep_worklist_req)
		wakeup(&softdep_worklist_req);
	return (error);
}

/*
 * Flush all vnodes and worklist items associated with a specified mount point.
 */
int
softdep_flushfiles(struct mount *oldmnt, int flags, struct proc *p)
{
	int error, count, loopcnt;

	/*
	 * Alternately flush the vnodes associated with the mount
	 * point and process any dependencies that the flushing
	 * creates. In theory, this loop can happen at most twice,
	 * but we give it a few extra just to be sure.
	 */
	for (loopcnt = 10; loopcnt > 0; loopcnt--) {
		/*
		 * Do another flush in case any vnodes were brought in
		 * as part of the cleanup operations.
		 */
		if ((error = ffs_flushfiles(oldmnt, flags, p)) != 0)
			break;
		if ((error = softdep_flushworklist(oldmnt, &count, p)) != 0 ||
		    count == 0)
			break;
	}
	/*
	 * If we are unmounting then it is an error to fail. If we
	 * are simply trying to downgrade to read-only, then filesystem
	 * activity can keep us busy forever, so we just fail with EBUSY.
	 */
	if (loopcnt == 0) {
		error = EBUSY;
	}
	return (error);
}

/*
 * Structure hashing.
 * 
 * There are three types of structures that can be looked up:
 *	1) pagedep structures identified by mount point, inode number,
 *	   and logical block.
 *	2) inodedep structures identified by mount point and inode number.
 *	3) newblk structures identified by mount point and
 *	   physical block number.
 *
 * The "pagedep" and "inodedep" dependency structures are hashed
 * separately from the file blocks and inodes to which they correspond.
 * This separation helps when the in-memory copy of an inode or
 * file block must be replaced. It also obviates the need to access
 * an inode or file page when simply updating (or de-allocating)
 * dependency structures. Lookup of newblk structures is needed to
 * find newly allocated blocks when trying to associate them with
 * their allocdirect or allocindir structure.
 *
 * The lookup routines optionally create and hash a new instance when
 * an existing entry is not found.
 */
#define DEPALLOC	0x0001	/* allocate structure if lookup fails */
#define NODELAY         0x0002  /* cannot do background work */

SIPHASH_KEY softdep_hashkey;

/*
 * Structures and routines associated with pagedep caching.
 */
LIST_HEAD(pagedep_hashhead, pagedep) *pagedep_hashtbl;
u_long	pagedep_hash;		/* size of hash table - 1 */
STATIC struct sema pagedep_in_progress;

/*
 * Look up a pagedep. Return 1 if found, 0 if not found or found
 * when asked to allocate but not associated with any buffer.
 * If not found, allocate if DEPALLOC flag is passed.
 * Found or allocated entry is returned in pagedeppp.
 * This routine must be called with splbio interrupts blocked.
 */
STATIC int
pagedep_lookup(struct inode *ip, daddr_t lbn, int flags,
    struct pagedep **pagedeppp)
{
	SIPHASH_CTX ctx;
	struct pagedep *pagedep;
	struct pagedep_hashhead *pagedephd;
	struct mount *mp;
	int i;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("pagedep_lookup: lock not held");
#endif
	mp = ITOV(ip)->v_mount;

	SipHash24_Init(&ctx, &softdep_hashkey);
	SipHash24_Update(&ctx, &mp, sizeof(mp));
	SipHash24_Update(&ctx, &ip->i_number, sizeof(ip->i_number));
	SipHash24_Update(&ctx, &lbn, sizeof(lbn));
	pagedephd = &pagedep_hashtbl[SipHash24_End(&ctx) & pagedep_hash];
top:
	LIST_FOREACH(pagedep, pagedephd, pd_hash)
		if (ip->i_number == pagedep->pd_ino &&
		    lbn == pagedep->pd_lbn &&
		    mp == pagedep->pd_mnt)
			break;
	if (pagedep) {
		*pagedeppp = pagedep;
		if ((flags & DEPALLOC) != 0 &&
		    (pagedep->pd_state & ONWORKLIST) == 0)
			return (0);
		return (1);
	}
	if ((flags & DEPALLOC) == 0) {
		*pagedeppp = NULL;
		return (0);
	}
	if (sema_get(&pagedep_in_progress, &lk) == 0) {
		ACQUIRE_LOCK(&lk);
		goto top;
	}
	pagedep = pool_get(&pagedep_pool, PR_WAITOK | PR_ZERO);
	pagedep->pd_list.wk_type = D_PAGEDEP;
	pagedep->pd_mnt = mp;
	pagedep->pd_ino = ip->i_number;
	pagedep->pd_lbn = lbn;
	LIST_INIT(&pagedep->pd_dirremhd);
	LIST_INIT(&pagedep->pd_pendinghd);
	for (i = 0; i < DAHASHSZ; i++)
		LIST_INIT(&pagedep->pd_diraddhd[i]);
	ACQUIRE_LOCK(&lk);
	LIST_INSERT_HEAD(pagedephd, pagedep, pd_hash);
	sema_release(&pagedep_in_progress);
	*pagedeppp = pagedep;
	return (0);
}

/*
 * Structures and routines associated with inodedep caching.
 */
LIST_HEAD(inodedep_hashhead, inodedep) *inodedep_hashtbl;
STATIC u_long	inodedep_hash;	/* size of hash table - 1 */
STATIC long	num_inodedep;	/* number of inodedep allocated */
STATIC struct sema inodedep_in_progress;

/*
 * Look up a inodedep. Return 1 if found, 0 if not found.
 * If not found, allocate if DEPALLOC flag is passed.
 * Found or allocated entry is returned in inodedeppp.
 * This routine must be called with splbio interrupts blocked.
 */
STATIC int
inodedep_lookup(struct fs *fs, ufsino_t inum, int flags,
    struct inodedep **inodedeppp)
{
	SIPHASH_CTX ctx;
	struct inodedep *inodedep;
	struct inodedep_hashhead *inodedephd;
	int firsttry;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("inodedep_lookup: lock not held");
#endif
	firsttry = 1;
	SipHash24_Init(&ctx, &softdep_hashkey);
	SipHash24_Update(&ctx, &fs, sizeof(fs));
	SipHash24_Update(&ctx, &inum, sizeof(inum));
	inodedephd = &inodedep_hashtbl[SipHash24_End(&ctx) & inodedep_hash];
top:
	LIST_FOREACH(inodedep, inodedephd, id_hash)
		if (inum == inodedep->id_ino && fs == inodedep->id_fs)
			break;
	if (inodedep) {
		*inodedeppp = inodedep;
		return (1);
	}
	if ((flags & DEPALLOC) == 0) {
		*inodedeppp = NULL;
		return (0);
	}
	/*
	 * If we are over our limit, try to improve the situation.
	 */
	if (num_inodedep > max_softdeps && firsttry && (flags & NODELAY) == 0 &&
	    request_cleanup(FLUSH_INODES, 1)) {
		firsttry = 0;
		goto top;
	}
	if (sema_get(&inodedep_in_progress, &lk) == 0) {
		ACQUIRE_LOCK(&lk);
		goto top;
	}
	num_inodedep += 1;
	inodedep = pool_get(&inodedep_pool, PR_WAITOK);
	inodedep->id_list.wk_type = D_INODEDEP;
	inodedep->id_fs = fs;
	inodedep->id_ino = inum;
	inodedep->id_state = ALLCOMPLETE;
	inodedep->id_nlinkdelta = 0;
	inodedep->id_savedino1 = NULL;
	inodedep->id_savedsize = -1;
	inodedep->id_buf = NULL;
	LIST_INIT(&inodedep->id_pendinghd);
	LIST_INIT(&inodedep->id_inowait);
	LIST_INIT(&inodedep->id_bufwait);
	TAILQ_INIT(&inodedep->id_inoupdt);
	TAILQ_INIT(&inodedep->id_newinoupdt);
	ACQUIRE_LOCK(&lk);
	LIST_INSERT_HEAD(inodedephd, inodedep, id_hash);
	sema_release(&inodedep_in_progress);
	*inodedeppp = inodedep;
	return (0);
}

/*
 * Structures and routines associated with newblk caching.
 */
LIST_HEAD(newblk_hashhead, newblk) *newblk_hashtbl;
u_long	newblk_hash;		/* size of hash table - 1 */
STATIC struct sema newblk_in_progress;

/*
 * Look up a newblk. Return 1 if found, 0 if not found.
 * If not found, allocate if DEPALLOC flag is passed.
 * Found or allocated entry is returned in newblkpp.
 */
STATIC int
newblk_lookup(struct fs *fs, daddr_t newblkno, int flags,
    struct newblk **newblkpp)
{
	SIPHASH_CTX ctx;
	struct newblk *newblk;
	struct newblk_hashhead *newblkhd;

	SipHash24_Init(&ctx, &softdep_hashkey);
	SipHash24_Update(&ctx, &fs, sizeof(fs));
	SipHash24_Update(&ctx, &newblkno, sizeof(newblkno));
	newblkhd = &newblk_hashtbl[SipHash24_End(&ctx) & newblk_hash];
top:
	LIST_FOREACH(newblk, newblkhd, nb_hash)
		if (newblkno == newblk->nb_newblkno && fs == newblk->nb_fs)
			break;
	if (newblk) {
		*newblkpp = newblk;
		return (1);
	}
	if ((flags & DEPALLOC) == 0) {
		*newblkpp = NULL;
		return (0);
	}
	if (sema_get(&newblk_in_progress, NULL) == 0)
		goto top;
	newblk = pool_get(&newblk_pool, PR_WAITOK);
	newblk->nb_state = 0;
	newblk->nb_fs = fs;
	newblk->nb_newblkno = newblkno;
	LIST_INSERT_HEAD(newblkhd, newblk, nb_hash);
	sema_release(&newblk_in_progress);
	*newblkpp = newblk;
	return (0);
}

/*
 * Executed during filesystem system initialization before
 * mounting any file systems.
 */
void 
softdep_initialize(void)
{

	bioops.io_start = softdep_disk_io_initiation;
	bioops.io_complete = softdep_disk_write_complete;
	bioops.io_deallocate = softdep_deallocate_dependencies;
	bioops.io_movedeps = softdep_move_dependencies;
	bioops.io_countdeps = softdep_count_dependencies;

	LIST_INIT(&mkdirlisthd);
	LIST_INIT(&softdep_workitem_pending);
#ifdef KMEMSTATS
	max_softdeps = min (initialvnodes * 8,
	    kmemstats[M_INODEDEP].ks_limit / (2 * sizeof(struct inodedep)));
#else
	max_softdeps = initialvnodes * 4;
#endif
	arc4random_buf(&softdep_hashkey, sizeof(softdep_hashkey));
	pagedep_hashtbl = hashinit(initialvnodes / 5, M_PAGEDEP, M_WAITOK,
	    &pagedep_hash);
	sema_init(&pagedep_in_progress, "pagedep", PRIBIO, 0);
	inodedep_hashtbl = hashinit(initialvnodes, M_INODEDEP, M_WAITOK,
	    &inodedep_hash);
	sema_init(&inodedep_in_progress, "inodedep", PRIBIO, 0);
	newblk_hashtbl = hashinit(64, M_NEWBLK, M_WAITOK, &newblk_hash);
	sema_init(&newblk_in_progress, "newblk", PRIBIO, 0);
	timeout_set(&proc_waiting_timeout, pause_timer, NULL);
	pool_init(&pagedep_pool, sizeof(struct pagedep), 0, IPL_NONE,
	    PR_WAITOK, "pagedep", NULL);
	pool_init(&inodedep_pool, sizeof(struct inodedep), 0, IPL_NONE,
	    PR_WAITOK, "inodedep", NULL);
	pool_init(&newblk_pool, sizeof(struct newblk), 0, IPL_NONE,
	    PR_WAITOK, "newblk", NULL);
	pool_init(&bmsafemap_pool, sizeof(struct bmsafemap), 0, IPL_NONE,
	    PR_WAITOK, "bmsafemap", NULL);
	pool_init(&allocdirect_pool, sizeof(struct allocdirect), 0, IPL_NONE,
	    PR_WAITOK, "allocdir", NULL);
	pool_init(&indirdep_pool, sizeof(struct indirdep), 0, IPL_NONE,
	    PR_WAITOK, "indirdep", NULL);
	pool_init(&allocindir_pool, sizeof(struct allocindir), 0, IPL_NONE,
	    PR_WAITOK, "allocindir", NULL);
	pool_init(&freefrag_pool, sizeof(struct freefrag), 0, IPL_NONE,
	    PR_WAITOK, "freefrag", NULL);
	pool_init(&freeblks_pool, sizeof(struct freeblks), 0, IPL_NONE,
	    PR_WAITOK, "freeblks", NULL);
	pool_init(&freefile_pool, sizeof(struct freefile), 0, IPL_NONE,
	    PR_WAITOK, "freefile", NULL);
	pool_init(&diradd_pool, sizeof(struct diradd), 0, IPL_NONE,
	    PR_WAITOK, "diradd", NULL);
	pool_init(&mkdir_pool, sizeof(struct mkdir), 0, IPL_NONE,
	    PR_WAITOK, "mkdir", NULL);
	pool_init(&dirrem_pool, sizeof(struct dirrem), 0, IPL_NONE,
	    PR_WAITOK, "dirrem", NULL);
	pool_init(&newdirblk_pool, sizeof(struct newdirblk), 0, IPL_NONE,
	    PR_WAITOK, "newdirblk", NULL);
}

/*
 * Called at mount time to notify the dependency code that a
 * filesystem wishes to use it.
 */
int
softdep_mount(struct vnode *devvp, struct mount *mp, struct fs *fs,
    struct ucred *cred)
{
	struct csum_total cstotal;
	struct cg *cgp;
	struct buf *bp;
	int error, cyl;

	/*
	 * When doing soft updates, the counters in the
	 * superblock may have gotten out of sync, so we have
	 * to scan the cylinder groups and recalculate them.
	 */
	if ((fs->fs_flags & FS_UNCLEAN) == 0)
		return (0);
	memset(&cstotal, 0, sizeof(cstotal));
	for (cyl = 0; cyl < fs->fs_ncg; cyl++) {
		if ((error = bread(devvp, fsbtodb(fs, cgtod(fs, cyl)),
		    fs->fs_cgsize, &bp)) != 0) {
			brelse(bp);
			return (error);
		}
		cgp = (struct cg *)bp->b_data;
		cstotal.cs_nffree += cgp->cg_cs.cs_nffree;
		cstotal.cs_nbfree += cgp->cg_cs.cs_nbfree;
		cstotal.cs_nifree += cgp->cg_cs.cs_nifree;
		cstotal.cs_ndir += cgp->cg_cs.cs_ndir;
		fs->fs_cs(fs, cyl) = cgp->cg_cs;
		brelse(bp);
	}
#ifdef DEBUG
	if (memcmp(&cstotal, &fs->fs_cstotal, sizeof(cstotal)))
		printf("ffs_mountfs: superblock updated for soft updates\n");
#endif
	memcpy(&fs->fs_cstotal, &cstotal, sizeof(cstotal));
	return (0);
}

/*
 * Protecting the freemaps (or bitmaps).
 * 
 * To eliminate the need to execute fsck before mounting a file system
 * after a power failure, one must (conservatively) guarantee that the
 * on-disk copy of the bitmaps never indicate that a live inode or block is
 * free.  So, when a block or inode is allocated, the bitmap should be
 * updated (on disk) before any new pointers.  When a block or inode is
 * freed, the bitmap should not be updated until all pointers have been
 * reset.  The latter dependency is handled by the delayed de-allocation
 * approach described below for block and inode de-allocation.  The former
 * dependency is handled by calling the following procedure when a block or
 * inode is allocated. When an inode is allocated an "inodedep" is created
 * with its DEPCOMPLETE flag cleared until its bitmap is written to disk.
 * Each "inodedep" is also inserted into the hash indexing structure so
 * that any additional link additions can be made dependent on the inode
 * allocation.
 * 
 * The ufs file system maintains a number of free block counts (e.g., per
 * cylinder group, per cylinder and per <cylinder, rotational position> pair)
 * in addition to the bitmaps.  These counts are used to improve efficiency
 * during allocation and therefore must be consistent with the bitmaps.
 * There is no convenient way to guarantee post-crash consistency of these
 * counts with simple update ordering, for two main reasons: (1) The counts
 * and bitmaps for a single cylinder group block are not in the same disk
 * sector.  If a disk write is interrupted (e.g., by power failure), one may
 * be written and the other not.  (2) Some of the counts are located in the
 * superblock rather than the cylinder group block. So, we focus our soft
 * updates implementation on protecting the bitmaps. When mounting a
 * filesystem, we recompute the auxiliary counts from the bitmaps.
 */

/*
 * Called just after updating the cylinder group block to allocate an inode.
 */
/* buffer for cylgroup block with inode map */
/* inode related to allocation */
/* new inode number being allocated */
void
softdep_setup_inomapdep(struct buf *bp, struct inode *ip, ufsino_t newinum)
{
	struct inodedep *inodedep;
	struct bmsafemap *bmsafemap;

	/*
	 * Create a dependency for the newly allocated inode.
	 * Panic if it already exists as something is seriously wrong.
	 * Otherwise add it to the dependency list for the buffer holding
	 * the cylinder group map from which it was allocated.
	 */
	ACQUIRE_LOCK(&lk);
	if (inodedep_lookup(ip->i_fs, newinum, DEPALLOC | NODELAY, &inodedep)
	    != 0) {
		FREE_LOCK(&lk);
		panic("softdep_setup_inomapdep: found inode");
	}
	inodedep->id_buf = bp;
	inodedep->id_state &= ~DEPCOMPLETE;
	bmsafemap = bmsafemap_lookup(bp);
	LIST_INSERT_HEAD(&bmsafemap->sm_inodedephd, inodedep, id_deps);
	FREE_LOCK(&lk);
}

/*
 * Called just after updating the cylinder group block to
 * allocate block or fragment.
 */
/* buffer for cylgroup block with block map */
/* filesystem doing allocation */
/* number of newly allocated block */
void
softdep_setup_blkmapdep(struct buf *bp, struct fs *fs, daddr_t newblkno)
{
	struct newblk *newblk;
	struct bmsafemap *bmsafemap;

	/*
	 * Create a dependency for the newly allocated block.
	 * Add it to the dependency list for the buffer holding
	 * the cylinder group map from which it was allocated.
	 */
	if (newblk_lookup(fs, newblkno, DEPALLOC, &newblk) != 0)
		panic("softdep_setup_blkmapdep: found block");
	ACQUIRE_LOCK(&lk);
	newblk->nb_bmsafemap = bmsafemap = bmsafemap_lookup(bp);
	LIST_INSERT_HEAD(&bmsafemap->sm_newblkhd, newblk, nb_deps);
	FREE_LOCK(&lk);
}

/*
 * Find the bmsafemap associated with a cylinder group buffer.
 * If none exists, create one. The buffer must be locked when
 * this routine is called and this routine must be called with
 * splbio interrupts blocked.
 */
STATIC struct bmsafemap *
bmsafemap_lookup(struct buf *bp)
{
	struct bmsafemap *bmsafemap;
	struct worklist *wk;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("bmsafemap_lookup: lock not held");
#endif
	LIST_FOREACH(wk, &bp->b_dep, wk_list)
		if (wk->wk_type == D_BMSAFEMAP)
			return (WK_BMSAFEMAP(wk));
	FREE_LOCK(&lk);
	bmsafemap = pool_get(&bmsafemap_pool, PR_WAITOK);
	bmsafemap->sm_list.wk_type = D_BMSAFEMAP;
	bmsafemap->sm_list.wk_state = 0;
	bmsafemap->sm_buf = bp;
	LIST_INIT(&bmsafemap->sm_allocdirecthd);
	LIST_INIT(&bmsafemap->sm_allocindirhd);
	LIST_INIT(&bmsafemap->sm_inodedephd);
	LIST_INIT(&bmsafemap->sm_newblkhd);
	ACQUIRE_LOCK(&lk);
	WORKLIST_INSERT(&bp->b_dep, &bmsafemap->sm_list);
	return (bmsafemap);
}

/*
 * Direct block allocation dependencies.
 * 
 * When a new block is allocated, the corresponding disk locations must be
 * initialized (with zeros or new data) before the on-disk inode points to
 * them.  Also, the freemap from which the block was allocated must be
 * updated (on disk) before the inode's pointer. These two dependencies are
 * independent of each other and are needed for all file blocks and indirect
 * blocks that are pointed to directly by the inode.  Just before the
 * "in-core" version of the inode is updated with a newly allocated block
 * number, a procedure (below) is called to setup allocation dependency
 * structures.  These structures are removed when the corresponding
 * dependencies are satisfied or when the block allocation becomes obsolete
 * (i.e., the file is deleted, the block is de-allocated, or the block is a
 * fragment that gets upgraded).  All of these cases are handled in
 * procedures described later.
 * 
 * When a file extension causes a fragment to be upgraded, either to a larger
 * fragment or to a full block, the on-disk location may change (if the
 * previous fragment could not simply be extended). In this case, the old
 * fragment must be de-allocated, but not until after the inode's pointer has
 * been updated. In most cases, this is handled by later procedures, which
 * will construct a "freefrag" structure to be added to the workitem queue
 * when the inode update is complete (or obsolete).  The main exception to
 * this is when an allocation occurs while a pending allocation dependency
 * (for the same block pointer) remains.  This case is handled in the main
 * allocation dependency setup procedure by immediately freeing the
 * unreferenced fragments.
 */ 
/* inode to which block is being added */
/* block pointer within inode */
/* disk block number being added */
/* previous block number, 0 unless frag */
/* size of new block */
/* size of new block */
/* bp for allocated block */
void 
softdep_setup_allocdirect(struct inode *ip, daddr_t lbn, daddr_t newblkno,
    daddr_t oldblkno, long newsize, long oldsize, struct buf *bp)
{
	struct allocdirect *adp, *oldadp;
	struct allocdirectlst *adphead;
	struct bmsafemap *bmsafemap;
	struct inodedep *inodedep;
	struct pagedep *pagedep;
	struct newblk *newblk;

	adp = pool_get(&allocdirect_pool, PR_WAITOK | PR_ZERO);
	adp->ad_list.wk_type = D_ALLOCDIRECT;
	adp->ad_lbn = lbn;
	adp->ad_newblkno = newblkno;
	adp->ad_oldblkno = oldblkno;
	adp->ad_newsize = newsize;
	adp->ad_oldsize = oldsize;
	adp->ad_state = ATTACHED;
	LIST_INIT(&adp->ad_newdirblk);
	if (newblkno == oldblkno)
		adp->ad_freefrag = NULL;
	else
		adp->ad_freefrag = newfreefrag(ip, oldblkno, oldsize);

	if (newblk_lookup(ip->i_fs, newblkno, 0, &newblk) == 0)
		panic("softdep_setup_allocdirect: lost block");

	ACQUIRE_LOCK(&lk);
	inodedep_lookup(ip->i_fs, ip->i_number, DEPALLOC | NODELAY, &inodedep);
	adp->ad_inodedep = inodedep;

	if (newblk->nb_state == DEPCOMPLETE) {
		adp->ad_state |= DEPCOMPLETE;
		adp->ad_buf = NULL;
	} else {
		bmsafemap = newblk->nb_bmsafemap;
		adp->ad_buf = bmsafemap->sm_buf;
		LIST_REMOVE(newblk, nb_deps);
		LIST_INSERT_HEAD(&bmsafemap->sm_allocdirecthd, adp, ad_deps);
	}
	LIST_REMOVE(newblk, nb_hash);
	pool_put(&newblk_pool, newblk);

	if (bp == NULL) {
		/*
		 * XXXUBC - Yes, I know how to fix this, but not right now.
		 */
		panic("softdep_setup_allocdirect: Bonk art in the head");
	}
	WORKLIST_INSERT(&bp->b_dep, &adp->ad_list);
	if (lbn >= NDADDR) {
		/* allocating an indirect block */
		if (oldblkno != 0) {
			FREE_LOCK(&lk);
			panic("softdep_setup_allocdirect: non-zero indir");
		}
	} else {
		/*
		 * Allocating a direct block.
		 *
		 * If we are allocating a directory block, then we must
		 * allocate an associated pagedep to track additions and
		 * deletions.
		 */
		if ((DIP(ip, mode) & IFMT) == IFDIR &&
		    pagedep_lookup(ip, lbn, DEPALLOC, &pagedep) == 0)
			WORKLIST_INSERT(&bp->b_dep, &pagedep->pd_list);
	}
	/*
	 * The list of allocdirects must be kept in sorted and ascending
	 * order so that the rollback routines can quickly determine the
	 * first uncommitted block (the size of the file stored on disk
	 * ends at the end of the lowest committed fragment, or if there
	 * are no fragments, at the end of the highest committed block).
	 * Since files generally grow, the typical case is that the new
	 * block is to be added at the end of the list. We speed this
	 * special case by checking against the last allocdirect in the
	 * list before laboriously traversing the list looking for the
	 * insertion point.
	 */
	adphead = &inodedep->id_newinoupdt;
	oldadp = TAILQ_LAST(adphead, allocdirectlst);
	if (oldadp == NULL || oldadp->ad_lbn <= lbn) {
		/* insert at end of list */
		TAILQ_INSERT_TAIL(adphead, adp, ad_next);
		if (oldadp != NULL && oldadp->ad_lbn == lbn)
			allocdirect_merge(adphead, adp, oldadp);
		FREE_LOCK(&lk);
		return;
	}
	TAILQ_FOREACH(oldadp, adphead, ad_next) {
		if (oldadp->ad_lbn >= lbn)
			break;
	}
	if (oldadp == NULL) {
		FREE_LOCK(&lk);
		panic("softdep_setup_allocdirect: lost entry");
	}
	/* insert in middle of list */
	TAILQ_INSERT_BEFORE(oldadp, adp, ad_next);
	if (oldadp->ad_lbn == lbn)
		allocdirect_merge(adphead, adp, oldadp);
	FREE_LOCK(&lk);
}

/*
 * Replace an old allocdirect dependency with a newer one.
 * This routine must be called with splbio interrupts blocked.
 */
/* head of list holding allocdirects */
/* allocdirect being added */
/* existing allocdirect being checked */
STATIC void
allocdirect_merge(struct allocdirectlst *adphead, struct allocdirect *newadp,
    struct allocdirect *oldadp)
{
	struct worklist *wk;
	struct freefrag *freefrag;
	struct newdirblk *newdirblk;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("allocdirect_merge: lock not held");
#endif
	if (newadp->ad_oldblkno != oldadp->ad_newblkno ||
	    newadp->ad_oldsize != oldadp->ad_newsize ||
	    newadp->ad_lbn >= NDADDR) {
		FREE_LOCK(&lk);
		panic("allocdirect_merge: old %lld != new %lld || lbn %lld >= "
		    "%d", (long long)newadp->ad_oldblkno,
		    (long long)oldadp->ad_newblkno, (long long)newadp->ad_lbn,
		    NDADDR);
	}
	newadp->ad_oldblkno = oldadp->ad_oldblkno;
	newadp->ad_oldsize = oldadp->ad_oldsize;
	/*
	 * If the old dependency had a fragment to free or had never
	 * previously had a block allocated, then the new dependency
	 * can immediately post its freefrag and adopt the old freefrag.
	 * This action is done by swapping the freefrag dependencies.
	 * The new dependency gains the old one's freefrag, and the
	 * old one gets the new one and then immediately puts it on
	 * the worklist when it is freed by free_allocdirect. It is
	 * not possible to do this swap when the old dependency had a
	 * non-zero size but no previous fragment to free. This condition
	 * arises when the new block is an extension of the old block.
	 * Here, the first part of the fragment allocated to the new
	 * dependency is part of the block currently claimed on disk by
	 * the old dependency, so cannot legitimately be freed until the
	 * conditions for the new dependency are fulfilled.
	 */
	if (oldadp->ad_freefrag != NULL || oldadp->ad_oldblkno == 0) {
		freefrag = newadp->ad_freefrag;
		newadp->ad_freefrag = oldadp->ad_freefrag;
		oldadp->ad_freefrag = freefrag;
	}
	/*
	 * If we are tracking a new directory-block allocation,
	 * move it from the old allocdirect to the new allocdirect.
	 */
	if ((wk = LIST_FIRST(&oldadp->ad_newdirblk)) != NULL) {
		newdirblk = WK_NEWDIRBLK(wk);
		WORKLIST_REMOVE(&newdirblk->db_list);
		if (LIST_FIRST(&oldadp->ad_newdirblk) != NULL)
			panic("allocdirect_merge: extra newdirblk");
		WORKLIST_INSERT(&newadp->ad_newdirblk, &newdirblk->db_list);
	}
	free_allocdirect(adphead, oldadp, 0);
}
		
/*
 * Allocate a new freefrag structure if needed.
 */
STATIC struct freefrag *
newfreefrag(struct inode *ip, daddr_t blkno, long size)
{
	struct freefrag *freefrag;
	struct fs *fs;

	if (blkno == 0)
		return (NULL);
	fs = ip->i_fs;
	if (fragnum(fs, blkno) + numfrags(fs, size) > fs->fs_frag)
		panic("newfreefrag: frag size");
	freefrag = pool_get(&freefrag_pool, PR_WAITOK);
	freefrag->ff_list.wk_type = D_FREEFRAG;
	freefrag->ff_state = DIP(ip, uid) & ~ONWORKLIST; /* used below */
	freefrag->ff_inum = ip->i_number;
	freefrag->ff_mnt = ITOV(ip)->v_mount;
	freefrag->ff_devvp = ip->i_devvp;
	freefrag->ff_blkno = blkno;
	freefrag->ff_fragsize = size;
	return (freefrag);
}

/*
 * This workitem de-allocates fragments that were replaced during
 * file block allocation.
 */
STATIC void 
handle_workitem_freefrag(struct freefrag *freefrag)
{
	struct inode tip;
	struct ufs1_dinode dtip1;

	tip.i_vnode = NULL;
	tip.i_din1 = &dtip1;
	tip.i_fs = VFSTOUFS(freefrag->ff_mnt)->um_fs;
	tip.i_ump = VFSTOUFS(freefrag->ff_mnt);
	tip.i_dev = freefrag->ff_devvp->v_rdev;
	tip.i_number = freefrag->ff_inum;
	tip.i_ffs1_uid = freefrag->ff_state & ~ONWORKLIST; /* set above */
	ffs_blkfree(&tip, freefrag->ff_blkno, freefrag->ff_fragsize);
	pool_put(&freefrag_pool, freefrag);
}

/*
 * Indirect block allocation dependencies.
 * 
 * The same dependencies that exist for a direct block also exist when
 * a new block is allocated and pointed to by an entry in a block of
 * indirect pointers. The undo/redo states described above are also
 * used here. Because an indirect block contains many pointers that
 * may have dependencies, a second copy of the entire in-memory indirect
 * block is kept. The buffer cache copy is always completely up-to-date.
 * The second copy, which is used only as a source for disk writes,
 * contains only the safe pointers (i.e., those that have no remaining
 * update dependencies). The second copy is freed when all pointers
 * are safe. The cache is not allowed to replace indirect blocks with
 * pending update dependencies. If a buffer containing an indirect
 * block with dependencies is written, these routines will mark it
 * dirty again. It can only be successfully written once all the
 * dependencies are removed. The ffs_fsync routine in conjunction with
 * softdep_sync_metadata work together to get all the dependencies
 * removed so that a file can be successfully written to disk. Three
 * procedures are used when setting up indirect block pointer
 * dependencies. The division is necessary because of the organization
 * of the "balloc" routine and because of the distinction between file
 * pages and file metadata blocks.
 */

/*
 * Allocate a new allocindir structure.
 */
/* inode for file being extended */
/* offset of pointer in indirect block */
/* disk block number being added */
/* previous block number, 0 if none */
STATIC struct allocindir *
newallocindir(struct inode *ip, int ptrno, daddr_t newblkno,
    daddr_t oldblkno)
{
	struct allocindir *aip;

	aip = pool_get(&allocindir_pool, PR_WAITOK | PR_ZERO);
	aip->ai_list.wk_type = D_ALLOCINDIR;
	aip->ai_state = ATTACHED;
	aip->ai_offset = ptrno;
	aip->ai_newblkno = newblkno;
	aip->ai_oldblkno = oldblkno;
	aip->ai_freefrag = newfreefrag(ip, oldblkno, ip->i_fs->fs_bsize);
	return (aip);
}

/*
 * Called just before setting an indirect block pointer
 * to a newly allocated file page.
 */
/* inode for file being extended */
/* allocated block number within file */
/* buffer with indirect blk referencing page */
/* offset of pointer in indirect block */
/* disk block number being added */
/* previous block number, 0 if none */
/* buffer holding allocated page */
void
softdep_setup_allocindir_page(struct inode *ip, daddr_t lbn, struct buf *bp,
    int ptrno, daddr_t newblkno, daddr_t oldblkno, struct buf *nbp)
{
	struct allocindir *aip;
	struct pagedep *pagedep;

	aip = newallocindir(ip, ptrno, newblkno, oldblkno);
	ACQUIRE_LOCK(&lk);
	/*
	 * If we are allocating a directory page, then we must
	 * allocate an associated pagedep to track additions and
	 * deletions.
	 */
	if ((DIP(ip, mode) & IFMT) == IFDIR &&
	    pagedep_lookup(ip, lbn, DEPALLOC, &pagedep) == 0)
		WORKLIST_INSERT(&nbp->b_dep, &pagedep->pd_list);
	if (nbp == NULL) {
		/*
		 * XXXUBC - Yes, I know how to fix this, but not right now.
		 */
		panic("softdep_setup_allocindir_page: Bonk art in the head");
	}
	WORKLIST_INSERT(&nbp->b_dep, &aip->ai_list);
	FREE_LOCK(&lk);
	setup_allocindir_phase2(bp, ip, aip);
}

/*
 * Called just before setting an indirect block pointer to a
 * newly allocated indirect block.
 */
/* newly allocated indirect block */
/* inode for file being extended */
/* indirect block referencing allocated block */
/* offset of pointer in indirect block */
/* disk block number being added */
void
softdep_setup_allocindir_meta(struct buf *nbp, struct inode *ip,
    struct buf *bp, int ptrno, daddr_t newblkno)
{
	struct allocindir *aip;

	aip = newallocindir(ip, ptrno, newblkno, 0);
	ACQUIRE_LOCK(&lk);
	WORKLIST_INSERT(&nbp->b_dep, &aip->ai_list);
	FREE_LOCK(&lk);
	setup_allocindir_phase2(bp, ip, aip);
}

/*
 * Called to finish the allocation of the "aip" allocated
 * by one of the two routines above.
 */
/* in-memory copy of the indirect block */
/* inode for file being extended */
/* allocindir allocated by the above routines */
STATIC void 
setup_allocindir_phase2(struct buf *bp, struct inode *ip,
    struct allocindir *aip)
{
	struct worklist *wk;
	struct indirdep *indirdep, *newindirdep;
	struct bmsafemap *bmsafemap;
	struct allocindir *oldaip;
	struct freefrag *freefrag;
	struct newblk *newblk;

	if (bp->b_lblkno >= 0)
		panic("setup_allocindir_phase2: not indir blk");
	for (indirdep = NULL, newindirdep = NULL; ; ) {
		ACQUIRE_LOCK(&lk);
		LIST_FOREACH(wk, &bp->b_dep, wk_list) {
			if (wk->wk_type != D_INDIRDEP)
				continue;
			indirdep = WK_INDIRDEP(wk);
			break;
		}
		if (indirdep == NULL && newindirdep) {
			indirdep = newindirdep;
			WORKLIST_INSERT(&bp->b_dep, &indirdep->ir_list);
			newindirdep = NULL;
		}
		FREE_LOCK(&lk);
		if (indirdep) {
			if (newblk_lookup(ip->i_fs, aip->ai_newblkno, 0,
			    &newblk) == 0)
				panic("setup_allocindir: lost block");
			ACQUIRE_LOCK(&lk);
			if (newblk->nb_state == DEPCOMPLETE) {
				aip->ai_state |= DEPCOMPLETE;
				aip->ai_buf = NULL;
			} else {
				bmsafemap = newblk->nb_bmsafemap;
				aip->ai_buf = bmsafemap->sm_buf;
				LIST_REMOVE(newblk, nb_deps);
				LIST_INSERT_HEAD(&bmsafemap->sm_allocindirhd,
				    aip, ai_deps);
			}
			LIST_REMOVE(newblk, nb_hash);
			pool_put(&newblk_pool, newblk);
			aip->ai_indirdep = indirdep;
			/*
			 * Check to see if there is an existing dependency
			 * for this block. If there is, merge the old
			 * dependency into the new one.
			 */
			if (aip->ai_oldblkno == 0)
				oldaip = NULL;
			else

				LIST_FOREACH(oldaip, &indirdep->ir_deplisthd, ai_next)
					if (oldaip->ai_offset == aip->ai_offset)
						break;
			freefrag = NULL;
			if (oldaip != NULL) {
				if (oldaip->ai_newblkno != aip->ai_oldblkno) {
					FREE_LOCK(&lk);
					panic("setup_allocindir_phase2: blkno");
				}
				aip->ai_oldblkno = oldaip->ai_oldblkno;
				freefrag = aip->ai_freefrag;
				aip->ai_freefrag = oldaip->ai_freefrag;
				oldaip->ai_freefrag = NULL;
				free_allocindir(oldaip, NULL);
			}
			LIST_INSERT_HEAD(&indirdep->ir_deplisthd, aip, ai_next);
			if (ip->i_ump->um_fstype == UM_UFS1)
				((int32_t *)indirdep->ir_savebp->b_data)
				    [aip->ai_offset] = aip->ai_oldblkno;
			else
				((int64_t *)indirdep->ir_savebp->b_data)
				    [aip->ai_offset] = aip->ai_oldblkno;
			FREE_LOCK(&lk);
			if (freefrag != NULL)
				handle_workitem_freefrag(freefrag);
		}
		if (newindirdep) {
			if (indirdep->ir_savebp != NULL)
				brelse(newindirdep->ir_savebp);
			WORKITEM_FREE(newindirdep, D_INDIRDEP);
		}
		if (indirdep)
			break;
		newindirdep = pool_get(&indirdep_pool, PR_WAITOK);
		newindirdep->ir_list.wk_type = D_INDIRDEP;
		newindirdep->ir_state = ATTACHED;
		if (ip->i_ump->um_fstype == UM_UFS1)
			newindirdep->ir_state |= UFS1FMT;
		LIST_INIT(&newindirdep->ir_deplisthd);
		LIST_INIT(&newindirdep->ir_donehd);
		if (bp->b_blkno == bp->b_lblkno) {
			VOP_BMAP(bp->b_vp, bp->b_lblkno, NULL, &bp->b_blkno,
				NULL);
		}
		newindirdep->ir_savebp =
		    getblk(ip->i_devvp, bp->b_blkno, bp->b_bcount, 0, 0);
#if 0
		BUF_KERNPROC(newindirdep->ir_savebp);
#endif
		memcpy(newindirdep->ir_savebp->b_data, bp->b_data, bp->b_bcount);
	}
}

/*
 * Block de-allocation dependencies.
 * 
 * When blocks are de-allocated, the on-disk pointers must be nullified before
 * the blocks are made available for use by other files.  (The true
 * requirement is that old pointers must be nullified before new on-disk
 * pointers are set.  We chose this slightly more stringent requirement to
 * reduce complexity.) Our implementation handles this dependency by updating
 * the inode (or indirect block) appropriately but delaying the actual block
 * de-allocation (i.e., freemap and free space count manipulation) until
 * after the updated versions reach stable storage.  After the disk is
 * updated, the blocks can be safely de-allocated whenever it is convenient.
 * This implementation handles only the common case of reducing a file's
 * length to zero. Other cases are handled by the conventional synchronous
 * write approach.
 *
 * The ffs implementation with which we worked double-checks
 * the state of the block pointers and file size as it reduces
 * a file's length.  Some of this code is replicated here in our
 * soft updates implementation.  The freeblks->fb_chkcnt field is
 * used to transfer a part of this information to the procedure
 * that eventually de-allocates the blocks.
 *
 * This routine should be called from the routine that shortens
 * a file's length, before the inode's size or block pointers
 * are modified. It will save the block pointer information for
 * later release and zero the inode so that the calling routine
 * can release it.
 */
/* The inode whose length is to be reduced */
/* The new length for the file */
void
softdep_setup_freeblocks(struct inode *ip, off_t length)
{
	struct freeblks *freeblks;
	struct inodedep *inodedep;
	struct allocdirect *adp;
	struct vnode *vp;
	struct buf *bp;
	struct fs *fs;
	int i, delay, error;

	fs = ip->i_fs;
	if (length != 0)
		panic("softdep_setup_freeblocks: non-zero length");
	freeblks = pool_get(&freeblks_pool, PR_WAITOK | PR_ZERO);
	freeblks->fb_list.wk_type = D_FREEBLKS;
	freeblks->fb_state = ATTACHED;
	freeblks->fb_uid = DIP(ip, uid);
	freeblks->fb_previousinum = ip->i_number;
	freeblks->fb_devvp = ip->i_devvp;
	freeblks->fb_mnt = ITOV(ip)->v_mount;
	freeblks->fb_oldsize = DIP(ip, size);
	freeblks->fb_newsize = length;
	freeblks->fb_chkcnt = DIP(ip, blocks);

	for (i = 0; i < NDADDR; i++) {
		freeblks->fb_dblks[i] = DIP(ip, db[i]);
		DIP_ASSIGN(ip, db[i], 0);
	}

	for (i = 0; i < NIADDR; i++) {
		freeblks->fb_iblks[i] = DIP(ip, ib[i]);
		DIP_ASSIGN(ip, ib[i], 0);
	}

	DIP_ASSIGN(ip, blocks, 0);
	DIP_ASSIGN(ip, size, 0);

	/*
	 * Push the zero'ed inode to to its disk buffer so that we are free
	 * to delete its dependencies below. Once the dependencies are gone
	 * the buffer can be safely released.
	 */
	if ((error = bread(ip->i_devvp,
	    fsbtodb(fs, ino_to_fsba(fs, ip->i_number)),
	    (int)fs->fs_bsize, &bp)) != 0)
		softdep_error("softdep_setup_freeblocks", error);

	if (ip->i_ump->um_fstype == UM_UFS1)
		*((struct ufs1_dinode *) bp->b_data +
		    ino_to_fsbo(fs, ip->i_number)) = *ip->i_din1;
	else
		*((struct ufs2_dinode *) bp->b_data +
		    ino_to_fsbo(fs, ip->i_number)) = *ip->i_din2;

	/*
	 * Find and eliminate any inode dependencies.
	 */
	ACQUIRE_LOCK(&lk);
	(void) inodedep_lookup(fs, ip->i_number, DEPALLOC, &inodedep);
	if ((inodedep->id_state & IOSTARTED) != 0) {
		FREE_LOCK(&lk);
		panic("softdep_setup_freeblocks: inode busy");
	}
	/*
	 * Add the freeblks structure to the list of operations that
	 * must await the zero'ed inode being written to disk. If we
	 * still have a bitmap dependency (delay == 0), then the inode
	 * has never been written to disk, so we can process the
	 * freeblks below once we have deleted the dependencies.
	 */
	delay = (inodedep->id_state & DEPCOMPLETE);
	if (delay)
		WORKLIST_INSERT(&inodedep->id_bufwait, &freeblks->fb_list);
	/*
	 * Because the file length has been truncated to zero, any
	 * pending block allocation dependency structures associated
	 * with this inode are obsolete and can simply be de-allocated.
	 * We must first merge the two dependency lists to get rid of
	 * any duplicate freefrag structures, then purge the merged list.
	 * If we still have a bitmap dependency, then the inode has never
	 * been written to disk, so we can free any fragments without delay.
	 */
	merge_inode_lists(inodedep);
	while ((adp = TAILQ_FIRST(&inodedep->id_inoupdt)) != NULL)
		free_allocdirect(&inodedep->id_inoupdt, adp, delay);
	FREE_LOCK(&lk);
	bdwrite(bp);
	/*
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 * Once they are all there, walk the list and get rid of
	 * any dependencies.
	 */
	vp = ITOV(ip);
	ACQUIRE_LOCK(&lk);
	drain_output(vp, 1);
	while ((bp = LIST_FIRST(&vp->v_dirtyblkhd))) {
		if (getdirtybuf(bp, MNT_WAIT) <= 0)
			break;
		(void) inodedep_lookup(fs, ip->i_number, 0, &inodedep);
		deallocate_dependencies(bp, inodedep);
		bp->b_flags |= B_INVAL | B_NOCACHE;
		FREE_LOCK(&lk);
		brelse(bp);
		ACQUIRE_LOCK(&lk);
	}
	if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) != 0)
		(void) free_inodedep(inodedep);

	if (delay) {
		freeblks->fb_state |= DEPCOMPLETE;
		/*
		 * If the inode with zeroed block pointers is now on disk we
		 * can start freeing blocks. Add freeblks to the worklist
		 * instead of calling handle_workitem_freeblocks() directly as
		 * it is more likely that additional IO is needed to complete
		 * the request than in the !delay case.
		 */
		if ((freeblks->fb_state & ALLCOMPLETE) == ALLCOMPLETE)
			add_to_worklist(&freeblks->fb_list);
	}

	FREE_LOCK(&lk);
	/*
	 * If the inode has never been written to disk (delay == 0),
	 * then we can process the freeblks now that we have deleted
	 * the dependencies.
	 */
	if (!delay)
		handle_workitem_freeblocks(freeblks);
}

/*
 * Reclaim any dependency structures from a buffer that is about to
 * be reallocated to a new vnode. The buffer must be locked, thus,
 * no I/O completion operations can occur while we are manipulating
 * its associated dependencies. The mutex is held so that other I/O's
 * associated with related dependencies do not occur.
 */
STATIC void
deallocate_dependencies(struct buf *bp, struct inodedep *inodedep)
{
	struct worklist *wk;
	struct indirdep *indirdep;
	struct allocindir *aip;
	struct pagedep *pagedep;
	struct dirrem *dirrem;
	struct diradd *dap;
	int i;

	while ((wk = LIST_FIRST(&bp->b_dep)) != NULL) {
		switch (wk->wk_type) {

		case D_INDIRDEP:
			indirdep = WK_INDIRDEP(wk);
			/*
			 * None of the indirect pointers will ever be visible,
			 * so they can simply be tossed. GOINGAWAY ensures
			 * that allocated pointers will be saved in the buffer
			 * cache until they are freed. Note that they will
			 * only be able to be found by their physical address
			 * since the inode mapping the logical address will
			 * be gone. The save buffer used for the safe copy
			 * was allocated in setup_allocindir_phase2 using
			 * the physical address so it could be used for this
			 * purpose. Hence we swap the safe copy with the real
			 * copy, allowing the safe copy to be freed and holding
			 * on to the real copy for later use in indir_trunc.
			 */
			if (indirdep->ir_state & GOINGAWAY) {
				FREE_LOCK(&lk);
				panic("deallocate_dependencies: already gone");
			}
			indirdep->ir_state |= GOINGAWAY;
			while ((aip = LIST_FIRST(&indirdep->ir_deplisthd)))
				free_allocindir(aip, inodedep);
			if (bp->b_lblkno >= 0 ||
			    bp->b_blkno != indirdep->ir_savebp->b_lblkno) {
				FREE_LOCK(&lk);
				panic("deallocate_dependencies: not indir");
			}
			memcpy(indirdep->ir_savebp->b_data, bp->b_data,
			    bp->b_bcount);
			WORKLIST_REMOVE(wk);
			WORKLIST_INSERT(&indirdep->ir_savebp->b_dep, wk);
			continue;

		case D_PAGEDEP:
			pagedep = WK_PAGEDEP(wk);
			/*
			 * None of the directory additions will ever be
			 * visible, so they can simply be tossed.
			 */
			for (i = 0; i < DAHASHSZ; i++)
				while ((dap =
				    LIST_FIRST(&pagedep->pd_diraddhd[i])))
					free_diradd(dap);
			while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)))
				free_diradd(dap);
			/*
			 * Copy any directory remove dependencies to the list
			 * to be processed after the zero'ed inode is written.
			 * If the inode has already been written, then they 
			 * can be dumped directly onto the work list.
			 */
			while ((dirrem = LIST_FIRST(&pagedep->pd_dirremhd))) {
				LIST_REMOVE(dirrem, dm_next);
				dirrem->dm_dirinum = pagedep->pd_ino;
				if (inodedep == NULL ||
				    (inodedep->id_state & ALLCOMPLETE) ==
				     ALLCOMPLETE)
					add_to_worklist(&dirrem->dm_list);
				else
					WORKLIST_INSERT(&inodedep->id_bufwait,
					    &dirrem->dm_list);
			}
			if ((pagedep->pd_state & NEWBLOCK) != 0) {
				LIST_FOREACH(wk, &inodedep->id_bufwait, wk_list)
					if (wk->wk_type == D_NEWDIRBLK &&
					    WK_NEWDIRBLK(wk)->db_pagedep ==
					    pagedep)
						break;
				if (wk != NULL) {
					WORKLIST_REMOVE(wk);
					free_newdirblk(WK_NEWDIRBLK(wk));
				} else {
					FREE_LOCK(&lk);
					panic("deallocate_dependencies: "
					    "lost pagedep");
					}
			}
			WORKLIST_REMOVE(&pagedep->pd_list);
			LIST_REMOVE(pagedep, pd_hash);
			WORKITEM_FREE(pagedep, D_PAGEDEP);
			continue;

		case D_ALLOCINDIR:
			free_allocindir(WK_ALLOCINDIR(wk), inodedep);
			continue;

		case D_ALLOCDIRECT:
		case D_INODEDEP:
			FREE_LOCK(&lk);
			panic("deallocate_dependencies: Unexpected type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */

		default:
			FREE_LOCK(&lk);
			panic("deallocate_dependencies: Unknown type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}
}

/*
 * Free an allocdirect. Generate a new freefrag work request if appropriate.
 * This routine must be called with splbio interrupts blocked.
 */
STATIC void
free_allocdirect(struct allocdirectlst *adphead, struct allocdirect *adp,
    int delay)
{
	struct newdirblk *newdirblk;
	struct worklist *wk;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("free_allocdirect: lock not held");
#endif
	if ((adp->ad_state & DEPCOMPLETE) == 0)
		LIST_REMOVE(adp, ad_deps);
	TAILQ_REMOVE(adphead, adp, ad_next);
	if ((adp->ad_state & COMPLETE) == 0)
		WORKLIST_REMOVE(&adp->ad_list);
	if (adp->ad_freefrag != NULL) {
		if (delay)
			WORKLIST_INSERT(&adp->ad_inodedep->id_bufwait,
			    &adp->ad_freefrag->ff_list);
		else
			add_to_worklist(&adp->ad_freefrag->ff_list);
	}
	if ((wk = LIST_FIRST(&adp->ad_newdirblk)) != NULL) {
		newdirblk = WK_NEWDIRBLK(wk);
		WORKLIST_REMOVE(&newdirblk->db_list);
		if (LIST_FIRST(&adp->ad_newdirblk) != NULL)
			panic("free_allocdirect: extra newdirblk");
		if (delay)
			WORKLIST_INSERT(&adp->ad_inodedep->id_bufwait,
			    &newdirblk->db_list);
		else
			free_newdirblk(newdirblk);
	}
	WORKITEM_FREE(adp, D_ALLOCDIRECT);
}

/*
 * Free a newdirblk. Clear the NEWBLOCK flag on its associated pagedep.
 * This routine must be called with splbio interrupts blocked.
 */
void
free_newdirblk(struct newdirblk *newdirblk)
{
	struct pagedep *pagedep;
	struct diradd *dap;
	int i;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("free_newdirblk: lock not held");
#endif
	/*
	 * If the pagedep is still linked onto the directory buffer
	 * dependency chain, then some of the entries on the
	 * pd_pendinghd list may not be committed to disk yet. In
	 * this case, we will simply clear the NEWBLOCK flag and
	 * let the pd_pendinghd list be processed when the pagedep
	 * is next written. If the pagedep is no longer on the buffer
	 * dependency chain, then all the entries on the pd_pending
	 * list are committed to disk and we can free them here.
	 */
	pagedep = newdirblk->db_pagedep;
	pagedep->pd_state &= ~NEWBLOCK;
	if ((pagedep->pd_state & ONWORKLIST) == 0)
		while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
			free_diradd(dap);
	/*
	 * If no dependencies remain, the pagedep will be freed.
	 */
	for (i = 0; i < DAHASHSZ; i++)
		if (LIST_FIRST(&pagedep->pd_diraddhd[i]) != NULL)
			break;
	if (i == DAHASHSZ && (pagedep->pd_state & ONWORKLIST) == 0) {
		LIST_REMOVE(pagedep, pd_hash);
		WORKITEM_FREE(pagedep, D_PAGEDEP);
	}
	WORKITEM_FREE(newdirblk, D_NEWDIRBLK);
}

/*
 * Prepare an inode to be freed. The actual free operation is not
 * done until the zero'ed inode has been written to disk.
 */
void
softdep_freefile(struct vnode *pvp, ufsino_t ino, mode_t mode)
{
	struct inode *ip = VTOI(pvp);
	struct inodedep *inodedep;
	struct freefile *freefile;

	/*
	 * This sets up the inode de-allocation dependency.
	 */
	freefile = pool_get(&freefile_pool, PR_WAITOK);
	freefile->fx_list.wk_type = D_FREEFILE;
	freefile->fx_list.wk_state = 0;
	freefile->fx_mode = mode;
	freefile->fx_oldinum = ino;
	freefile->fx_devvp = ip->i_devvp;
	freefile->fx_mnt = ITOV(ip)->v_mount;

	/*
	 * If the inodedep does not exist, then the zero'ed inode has
	 * been written to disk. If the allocated inode has never been
	 * written to disk, then the on-disk inode is zero'ed. In either
	 * case we can free the file immediately.
	 */
	ACQUIRE_LOCK(&lk);
	if (inodedep_lookup(ip->i_fs, ino, 0, &inodedep) == 0 ||
	    check_inode_unwritten(inodedep)) {
		FREE_LOCK(&lk);
		handle_workitem_freefile(freefile);
		return;
	}
	WORKLIST_INSERT(&inodedep->id_inowait, &freefile->fx_list);
	FREE_LOCK(&lk);
}

/*
 * Check to see if an inode has never been written to disk. If
 * so free the inodedep and return success, otherwise return failure.
 * This routine must be called with splbio interrupts blocked.
 *
 * If we still have a bitmap dependency, then the inode has never
 * been written to disk. Drop the dependency as it is no longer
 * necessary since the inode is being deallocated. We set the
 * ALLCOMPLETE flags since the bitmap now properly shows that the
 * inode is not allocated. Even if the inode is actively being
 * written, it has been rolled back to its zero'ed state, so we
 * are ensured that a zero inode is what is on the disk. For short
 * lived files, this change will usually result in removing all the
 * dependencies from the inode so that it can be freed immediately.
 */
STATIC int
check_inode_unwritten(struct inodedep *inodedep)
{
	splassert(IPL_BIO);

	if ((inodedep->id_state & DEPCOMPLETE) != 0 ||
	    LIST_FIRST(&inodedep->id_pendinghd) != NULL ||
	    LIST_FIRST(&inodedep->id_bufwait) != NULL ||
	    LIST_FIRST(&inodedep->id_inowait) != NULL ||
	    TAILQ_FIRST(&inodedep->id_inoupdt) != NULL ||
	    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL ||
	    inodedep->id_nlinkdelta != 0)
		return (0);
	inodedep->id_state |= ALLCOMPLETE;
	LIST_REMOVE(inodedep, id_deps);
	inodedep->id_buf = NULL;
	if (inodedep->id_state & ONWORKLIST)
		WORKLIST_REMOVE(&inodedep->id_list);
	if (inodedep->id_savedino1 != NULL) {
		free(inodedep->id_savedino1, M_INODEDEP, 0);
		inodedep->id_savedino1 = NULL;
	}
	if (free_inodedep(inodedep) == 0) {
		FREE_LOCK(&lk);
		panic("check_inode_unwritten: busy inode");
	}
	return (1);
}

/*
 * Try to free an inodedep structure. Return 1 if it could be freed.
 */
STATIC int
free_inodedep(struct inodedep *inodedep)
{

	if ((inodedep->id_state & ONWORKLIST) != 0 ||
	    (inodedep->id_state & ALLCOMPLETE) != ALLCOMPLETE ||
	    LIST_FIRST(&inodedep->id_pendinghd) != NULL ||
	    LIST_FIRST(&inodedep->id_bufwait) != NULL ||
	    LIST_FIRST(&inodedep->id_inowait) != NULL ||
	    TAILQ_FIRST(&inodedep->id_inoupdt) != NULL ||
	    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL ||
	    inodedep->id_nlinkdelta != 0 || inodedep->id_savedino1 != NULL)
		return (0);
	LIST_REMOVE(inodedep, id_hash);
	WORKITEM_FREE(inodedep, D_INODEDEP);
	num_inodedep -= 1;
	return (1);
}

/*
 * This workitem routine performs the block de-allocation.
 * The workitem is added to the pending list after the updated
 * inode block has been written to disk.  As mentioned above,
 * checks regarding the number of blocks de-allocated (compared
 * to the number of blocks allocated for the file) are also
 * performed in this function.
 */
STATIC void
handle_workitem_freeblocks(struct freeblks *freeblks)
{
	struct inode tip;
	daddr_t bn;
	union {
		struct ufs1_dinode di1;
		struct ufs2_dinode di2;
	} di;
	struct fs *fs;
	int i, level, bsize;
	long nblocks, blocksreleased = 0;
	int error, allerror = 0;
	daddr_t baselbns[NIADDR], tmpval;

	if (VFSTOUFS(freeblks->fb_mnt)->um_fstype == UM_UFS1)
		tip.i_din1 = &di.di1;
	else
		tip.i_din2 = &di.di2;

	tip.i_fs = fs = VFSTOUFS(freeblks->fb_mnt)->um_fs;
	tip.i_number = freeblks->fb_previousinum;
	tip.i_ump = VFSTOUFS(freeblks->fb_mnt);
	tip.i_dev = freeblks->fb_devvp->v_rdev;
	DIP_ASSIGN(&tip, size, freeblks->fb_oldsize);
	DIP_ASSIGN(&tip, uid, freeblks->fb_uid);
	tip.i_vnode = NULL;
	tmpval = 1;
	baselbns[0] = NDADDR;
	for (i = 1; i < NIADDR; i++) {
		tmpval *= NINDIR(fs);
		baselbns[i] = baselbns[i - 1] + tmpval;
	}
	nblocks = btodb(fs->fs_bsize);
	blocksreleased = 0;
	/*
	 * Indirect blocks first.
	 */
	for (level = (NIADDR - 1); level >= 0; level--) {
		if ((bn = freeblks->fb_iblks[level]) == 0)
			continue;
		if ((error = indir_trunc(&tip, fsbtodb(fs, bn), level,
		    baselbns[level], &blocksreleased)) != 0)
			allerror = error;
		ffs_blkfree(&tip, bn, fs->fs_bsize);
		blocksreleased += nblocks;
	}
	/*
	 * All direct blocks or frags.
	 */
	for (i = (NDADDR - 1); i >= 0; i--) {
		if ((bn = freeblks->fb_dblks[i]) == 0)
			continue;
		bsize = blksize(fs, &tip, i);
		ffs_blkfree(&tip, bn, bsize);
		blocksreleased += btodb(bsize);
	}

#ifdef DIAGNOSTIC
	if (freeblks->fb_chkcnt != blocksreleased)
		printf("handle_workitem_freeblocks: block count\n");
	if (allerror)
		softdep_error("handle_workitem_freeblks", allerror);
#endif /* DIAGNOSTIC */
	WORKITEM_FREE(freeblks, D_FREEBLKS);
}

/*
 * Release blocks associated with the inode ip and stored in the indirect
 * block dbn. If level is greater than SINGLE, the block is an indirect block
 * and recursive calls to indirtrunc must be used to cleanse other indirect
 * blocks.
 */
STATIC int
indir_trunc(struct inode *ip, daddr_t dbn, int level, daddr_t lbn,
    long *countp)
{
	struct buf *bp;
	int32_t *bap1 = NULL;
	int64_t nb, *bap2 = NULL;
	struct fs *fs;
	struct worklist *wk;
	struct indirdep *indirdep;
	int i, lbnadd, nblocks, ufs1fmt;
	int error, allerror = 0;

	fs = ip->i_fs;
	lbnadd = 1;
	for (i = level; i > 0; i--)
		lbnadd *= NINDIR(fs);
	/*
	 * Get buffer of block pointers to be freed. This routine is not
	 * called until the zero'ed inode has been written, so it is safe
	 * to free blocks as they are encountered. Because the inode has
	 * been zero'ed, calls to bmap on these blocks will fail. So, we
	 * have to use the on-disk address and the block device for the
	 * filesystem to look them up. If the file was deleted before its
	 * indirect blocks were all written to disk, the routine that set
	 * us up (deallocate_dependencies) will have arranged to leave
	 * a complete copy of the indirect block in memory for our use.
	 * Otherwise we have to read the blocks in from the disk.
	 */
	ACQUIRE_LOCK(&lk);
	if ((bp = incore(ip->i_devvp, dbn)) != NULL &&
	    (wk = LIST_FIRST(&bp->b_dep)) != NULL) {
		if (wk->wk_type != D_INDIRDEP ||
		    (indirdep = WK_INDIRDEP(wk))->ir_savebp != bp ||
		    (indirdep->ir_state & GOINGAWAY) == 0) {
			FREE_LOCK(&lk);
			panic("indir_trunc: lost indirdep");
		}
		WORKLIST_REMOVE(wk);
		WORKITEM_FREE(indirdep, D_INDIRDEP);
		if (LIST_FIRST(&bp->b_dep) != NULL) {
			FREE_LOCK(&lk);
			panic("indir_trunc: dangling dep");
		}
		FREE_LOCK(&lk);
	} else {
		FREE_LOCK(&lk);
		error = bread(ip->i_devvp, dbn, (int)fs->fs_bsize, &bp);
		if (error)
			return (error);
	}
	/*
	 * Recursively free indirect blocks.
	 */
	if (ip->i_ump->um_fstype == UM_UFS1) {
		ufs1fmt = 1;
		bap1 = (int32_t *)bp->b_data;
	} else {
		ufs1fmt = 0;
		bap2 = (int64_t *)bp->b_data;
	}
	nblocks = btodb(fs->fs_bsize);
	for (i = NINDIR(fs) - 1; i >= 0; i--) {
		if (ufs1fmt)
			nb = bap1[i];
		else
			nb = bap2[i];
		if (nb == 0)
			continue;
		if (level != 0) {
			if ((error = indir_trunc(ip, fsbtodb(fs, nb),
			     level - 1, lbn + (i * lbnadd), countp)) != 0)
				allerror = error;
		}
		ffs_blkfree(ip, nb, fs->fs_bsize);
		*countp += nblocks;
	}
	bp->b_flags |= B_INVAL | B_NOCACHE;
	brelse(bp);
	return (allerror);
}

/*
 * Free an allocindir.
 * This routine must be called with splbio interrupts blocked.
 */
STATIC void
free_allocindir(struct allocindir *aip, struct inodedep *inodedep)
{
	struct freefrag *freefrag;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("free_allocindir: lock not held");
#endif
	if ((aip->ai_state & DEPCOMPLETE) == 0)
		LIST_REMOVE(aip, ai_deps);
	if (aip->ai_state & ONWORKLIST)
		WORKLIST_REMOVE(&aip->ai_list);
	LIST_REMOVE(aip, ai_next);
	if ((freefrag = aip->ai_freefrag) != NULL) {
		if (inodedep == NULL)
			add_to_worklist(&freefrag->ff_list);
		else
			WORKLIST_INSERT(&inodedep->id_bufwait,
			    &freefrag->ff_list);
	}
	WORKITEM_FREE(aip, D_ALLOCINDIR);
}

/*
 * Directory entry addition dependencies.
 * 
 * When adding a new directory entry, the inode (with its incremented link
 * count) must be written to disk before the directory entry's pointer to it.
 * Also, if the inode is newly allocated, the corresponding freemap must be
 * updated (on disk) before the directory entry's pointer. These requirements
 * are met via undo/redo on the directory entry's pointer, which consists
 * simply of the inode number.
 * 
 * As directory entries are added and deleted, the free space within a
 * directory block can become fragmented.  The ufs file system will compact
 * a fragmented directory block to make space for a new entry. When this
 * occurs, the offsets of previously added entries change. Any "diradd"
 * dependency structures corresponding to these entries must be updated with
 * the new offsets.
 */

/*
 * This routine is called after the in-memory inode's link
 * count has been incremented, but before the directory entry's
 * pointer to the inode has been set.
 */
/* buffer containing directory block */
/* inode for directory */
/* offset of new entry in directory */
/* inode referenced by new directory entry */
/* non-NULL => contents of new mkdir */
/* entry is in a newly allocated block */
int 
softdep_setup_directory_add(struct buf *bp, struct inode *dp, off_t diroffset,
    long newinum, struct buf *newdirbp, int isnewblk)
{
	int offset;		/* offset of new entry within directory block */
	daddr_t lbn;		/* block in directory containing new entry */
	struct fs *fs;
	struct diradd *dap;
	struct allocdirect *adp;
	struct pagedep *pagedep;
	struct inodedep *inodedep;
	struct newdirblk *newdirblk = NULL;
	struct mkdir *mkdir1, *mkdir2;
	

	fs = dp->i_fs;
	lbn = lblkno(fs, diroffset);
	offset = blkoff(fs, diroffset);
	dap = pool_get(&diradd_pool, PR_WAITOK | PR_ZERO);
	dap->da_list.wk_type = D_DIRADD;
	dap->da_offset = offset;
	dap->da_newinum = newinum;
	dap->da_state = ATTACHED;
	if (isnewblk && lbn < NDADDR && fragoff(fs, diroffset) == 0) {
		newdirblk = pool_get(&newdirblk_pool, PR_WAITOK);
		newdirblk->db_list.wk_type = D_NEWDIRBLK;
		newdirblk->db_state = 0;
	}
	if (newdirbp == NULL) {
		dap->da_state |= DEPCOMPLETE;
		ACQUIRE_LOCK(&lk);
	} else {
		dap->da_state |= MKDIR_BODY | MKDIR_PARENT;
		mkdir1 = pool_get(&mkdir_pool, PR_WAITOK);
		mkdir1->md_list.wk_type = D_MKDIR;
		mkdir1->md_state = MKDIR_BODY;
		mkdir1->md_diradd = dap;
		mkdir2 = pool_get(&mkdir_pool, PR_WAITOK);
		mkdir2->md_list.wk_type = D_MKDIR;
		mkdir2->md_state = MKDIR_PARENT;
		mkdir2->md_diradd = dap;
		/*
		 * Dependency on "." and ".." being written to disk.
		 */
		mkdir1->md_buf = newdirbp;
		ACQUIRE_LOCK(&lk);
		LIST_INSERT_HEAD(&mkdirlisthd, mkdir1, md_mkdirs);
		WORKLIST_INSERT(&newdirbp->b_dep, &mkdir1->md_list);
		FREE_LOCK(&lk);
		bdwrite(newdirbp);
		/*
		 * Dependency on link count increase for parent directory
		 */
		ACQUIRE_LOCK(&lk);
		if (inodedep_lookup(fs, dp->i_number, 0, &inodedep) == 0
		    || (inodedep->id_state & ALLCOMPLETE) == ALLCOMPLETE) {
			dap->da_state &= ~MKDIR_PARENT;
			WORKITEM_FREE(mkdir2, D_MKDIR);
		} else {
			LIST_INSERT_HEAD(&mkdirlisthd, mkdir2, md_mkdirs);
			WORKLIST_INSERT(&inodedep->id_bufwait,&mkdir2->md_list);
		}
	}
	/*
	 * Link into parent directory pagedep to await its being written.
	 */
	if (pagedep_lookup(dp, lbn, DEPALLOC, &pagedep) == 0)
		WORKLIST_INSERT(&bp->b_dep, &pagedep->pd_list);
	dap->da_pagedep = pagedep;
	LIST_INSERT_HEAD(&pagedep->pd_diraddhd[DIRADDHASH(offset)], dap,
	    da_pdlist);
	/*
	 * Link into its inodedep. Put it on the id_bufwait list if the inode
	 * is not yet written. If it is written, do the post-inode write
	 * processing to put it on the id_pendinghd list.
	 */
	(void) inodedep_lookup(fs, newinum, DEPALLOC, &inodedep);
	if ((inodedep->id_state & ALLCOMPLETE) == ALLCOMPLETE)
		diradd_inode_written(dap, inodedep);
	else
		WORKLIST_INSERT(&inodedep->id_bufwait, &dap->da_list);
	if (isnewblk) {
		/*
		 * Directories growing into indirect blocks are rare
		 * enough and the frequency of new block allocation
		 * in those cases even more rare, that we choose not
		 * to bother tracking them. Rather we simply force the
		 * new directory entry to disk.
		 */
		if (lbn >= NDADDR) {
			FREE_LOCK(&lk);
			/*
			 * We only have a new allocation when at the
			 * beginning of a new block, not when we are
			 * expanding into an existing block.
			 */
			if (blkoff(fs, diroffset) == 0)
				return (1);
			return (0);
		}
		/*
		 * We only have a new allocation when at the beginning
		 * of a new fragment, not when we are expanding into an
		 * existing fragment. Also, there is nothing to do if we
		 * are already tracking this block.
		 */
		if (fragoff(fs, diroffset) != 0) {
			FREE_LOCK(&lk);
			return (0);
		}
			
		if ((pagedep->pd_state & NEWBLOCK) != 0) {
			WORKITEM_FREE(newdirblk, D_NEWDIRBLK);
			FREE_LOCK(&lk);
			return (0);
		}
		/*
		 * Find our associated allocdirect and have it track us.
		 */
		if (inodedep_lookup(fs, dp->i_number, 0, &inodedep) == 0)
			panic("softdep_setup_directory_add: lost inodedep");
		adp = TAILQ_LAST(&inodedep->id_newinoupdt, allocdirectlst);
		if (adp == NULL || adp->ad_lbn != lbn) {
			FREE_LOCK(&lk);
			panic("softdep_setup_directory_add: lost entry");
		}
		pagedep->pd_state |= NEWBLOCK;
		newdirblk->db_pagedep = pagedep;
		WORKLIST_INSERT(&adp->ad_newdirblk, &newdirblk->db_list);
	}
	FREE_LOCK(&lk);
	return (0);
}

/*
 * This procedure is called to change the offset of a directory
 * entry when compacting a directory block which must be owned
 * exclusively by the caller. Note that the actual entry movement
 * must be done in this procedure to ensure that no I/O completions
 * occur while the move is in progress.
 */
/* inode for directory */
/* address of dp->i_offset */
/* address of old directory location */
/* address of new directory location */
/* size of directory entry */
void 
softdep_change_directoryentry_offset(struct inode *dp, caddr_t base,
    caddr_t oldloc, caddr_t newloc, int entrysize)
{
	int offset, oldoffset, newoffset;
	struct pagedep *pagedep;
	struct diradd *dap;
	daddr_t lbn;

	ACQUIRE_LOCK(&lk);
	lbn = lblkno(dp->i_fs, dp->i_offset);
	offset = blkoff(dp->i_fs, dp->i_offset);
	if (pagedep_lookup(dp, lbn, 0, &pagedep) == 0)
		goto done;
	oldoffset = offset + (oldloc - base);
	newoffset = offset + (newloc - base);

	LIST_FOREACH(dap, &pagedep->pd_diraddhd[DIRADDHASH(oldoffset)], da_pdlist) {
		if (dap->da_offset != oldoffset)
			continue;
		dap->da_offset = newoffset;
		if (DIRADDHASH(newoffset) == DIRADDHASH(oldoffset))
			break;
		LIST_REMOVE(dap, da_pdlist);
		LIST_INSERT_HEAD(&pagedep->pd_diraddhd[DIRADDHASH(newoffset)],
		    dap, da_pdlist);
		break;
	}
	if (dap == NULL) {

		LIST_FOREACH(dap, &pagedep->pd_pendinghd, da_pdlist) {
			if (dap->da_offset == oldoffset) {
				dap->da_offset = newoffset;
				break;
			}
		}
	}
done:
	memmove(newloc, oldloc, entrysize);
	FREE_LOCK(&lk);
}

/*
 * Free a diradd dependency structure. This routine must be called
 * with splbio interrupts blocked.
 */
STATIC void
free_diradd(struct diradd *dap)
{
	struct dirrem *dirrem;
	struct pagedep *pagedep;
	struct inodedep *inodedep;
	struct mkdir *mkdir, *nextmd;

	splassert(IPL_BIO);

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("free_diradd: lock not held");
#endif
	WORKLIST_REMOVE(&dap->da_list);
	LIST_REMOVE(dap, da_pdlist);
	if ((dap->da_state & DIRCHG) == 0) {
		pagedep = dap->da_pagedep;
	} else {
		dirrem = dap->da_previous;
		pagedep = dirrem->dm_pagedep;
		dirrem->dm_dirinum = pagedep->pd_ino;
		add_to_worklist(&dirrem->dm_list);
	}
	if (inodedep_lookup(VFSTOUFS(pagedep->pd_mnt)->um_fs, dap->da_newinum,
	    0, &inodedep) != 0)
		(void) free_inodedep(inodedep);
	if ((dap->da_state & (MKDIR_PARENT | MKDIR_BODY)) != 0) {
		for (mkdir = LIST_FIRST(&mkdirlisthd); mkdir; mkdir = nextmd) {
			nextmd = LIST_NEXT(mkdir, md_mkdirs);
			if (mkdir->md_diradd != dap)
				continue;
			dap->da_state &= ~mkdir->md_state;
			WORKLIST_REMOVE(&mkdir->md_list);
			LIST_REMOVE(mkdir, md_mkdirs);
			WORKITEM_FREE(mkdir, D_MKDIR);
		}
		if ((dap->da_state & (MKDIR_PARENT | MKDIR_BODY)) != 0) {
			FREE_LOCK(&lk);
			panic("free_diradd: unfound ref");
		}
	}
	WORKITEM_FREE(dap, D_DIRADD);
}

/*
 * Directory entry removal dependencies.
 * 
 * When removing a directory entry, the entry's inode pointer must be
 * zero'ed on disk before the corresponding inode's link count is decremented
 * (possibly freeing the inode for re-use). This dependency is handled by
 * updating the directory entry but delaying the inode count reduction until
 * after the directory block has been written to disk. After this point, the
 * inode count can be decremented whenever it is convenient.
 */

/*
 * This routine should be called immediately after removing
 * a directory entry.  The inode's link count should not be
 * decremented by the calling procedure -- the soft updates
 * code will do this task when it is safe.
 */
/* buffer containing directory block */
/* inode for the directory being modified */
/* inode for directory entry being removed */
/* indicates if doing RMDIR */
void 
softdep_setup_remove(struct buf *bp, struct inode *dp, struct inode *ip,
    int isrmdir)
{
	struct dirrem *dirrem, *prevdirrem;

	/*
	 * Allocate a new dirrem if appropriate and ACQUIRE_LOCK.
	 */
	dirrem = newdirrem(bp, dp, ip, isrmdir, &prevdirrem);

	/*
	 * If the COMPLETE flag is clear, then there were no active
	 * entries and we want to roll back to a zeroed entry until
	 * the new inode is committed to disk. If the COMPLETE flag is
	 * set then we have deleted an entry that never made it to
	 * disk. If the entry we deleted resulted from a name change,
	 * then the old name still resides on disk. We cannot delete
	 * its inode (returned to us in prevdirrem) until the zeroed
	 * directory entry gets to disk. The new inode has never been
	 * referenced on the disk, so can be deleted immediately.
	 */
	if ((dirrem->dm_state & COMPLETE) == 0) {
		LIST_INSERT_HEAD(&dirrem->dm_pagedep->pd_dirremhd, dirrem,
		    dm_next);
		FREE_LOCK(&lk);
	} else {
		if (prevdirrem != NULL)
			LIST_INSERT_HEAD(&dirrem->dm_pagedep->pd_dirremhd,
			    prevdirrem, dm_next);
		dirrem->dm_dirinum = dirrem->dm_pagedep->pd_ino;
		FREE_LOCK(&lk);
		handle_workitem_remove(dirrem);
	}
}

STATIC long num_dirrem;		/* number of dirrem allocated */
/*
 * Allocate a new dirrem if appropriate and return it along with
 * its associated pagedep. Called without a lock, returns with lock.
 */
/* buffer containing directory block */
/* inode for the directory being modified */
/* inode for directory entry being removed */
/* indicates if doing RMDIR */
/* previously referenced inode, if any */
STATIC struct dirrem *
newdirrem(struct buf *bp, struct inode *dp, struct inode *ip, int isrmdir,
    struct dirrem **prevdirremp)
{
	int offset;
	daddr_t lbn;
	struct diradd *dap;
	struct dirrem *dirrem;
	struct pagedep *pagedep;

	/*
	 * Whiteouts have no deletion dependencies.
	 */
	if (ip == NULL)
		panic("newdirrem: whiteout");
	/*
	 * If we are over our limit, try to improve the situation.
	 * Limiting the number of dirrem structures will also limit
	 * the number of freefile and freeblks structures.
	 */
	if (num_dirrem > max_softdeps / 2)
		(void) request_cleanup(FLUSH_REMOVE, 0);
	num_dirrem += 1;
	dirrem = pool_get(&dirrem_pool, PR_WAITOK | PR_ZERO);
	dirrem->dm_list.wk_type = D_DIRREM;
	dirrem->dm_state = isrmdir ? RMDIR : 0;
	dirrem->dm_mnt = ITOV(ip)->v_mount;
	dirrem->dm_oldinum = ip->i_number;
	*prevdirremp = NULL;

	ACQUIRE_LOCK(&lk);
	lbn = lblkno(dp->i_fs, dp->i_offset);
	offset = blkoff(dp->i_fs, dp->i_offset);
	if (pagedep_lookup(dp, lbn, DEPALLOC, &pagedep) == 0)
		WORKLIST_INSERT(&bp->b_dep, &pagedep->pd_list);
	dirrem->dm_pagedep = pagedep;
	/*
	 * Check for a diradd dependency for the same directory entry.
	 * If present, then both dependencies become obsolete and can
	 * be de-allocated. Check for an entry on both the pd_dirraddhd
	 * list and the pd_pendinghd list.
	 */

	LIST_FOREACH(dap, &pagedep->pd_diraddhd[DIRADDHASH(offset)], da_pdlist)
		if (dap->da_offset == offset)
			break;
	if (dap == NULL) {

		LIST_FOREACH(dap, &pagedep->pd_pendinghd, da_pdlist)
			if (dap->da_offset == offset)
				break;
		if (dap == NULL)
			return (dirrem);
	}
	/*
	 * Must be ATTACHED at this point.
	 */
	if ((dap->da_state & ATTACHED) == 0) {
		FREE_LOCK(&lk);
		panic("newdirrem: not ATTACHED");
	}
	if (dap->da_newinum != ip->i_number) {
		FREE_LOCK(&lk);
		panic("newdirrem: inum %u should be %u",
		    ip->i_number, dap->da_newinum);
	}
	/*
	 * If we are deleting a changed name that never made it to disk,
	 * then return the dirrem describing the previous inode (which
	 * represents the inode currently referenced from this entry on disk).
	 */
	if ((dap->da_state & DIRCHG) != 0) {
		*prevdirremp = dap->da_previous;
		dap->da_state &= ~DIRCHG;
		dap->da_pagedep = pagedep;
	}
	/*
	 * We are deleting an entry that never made it to disk.
	 * Mark it COMPLETE so we can delete its inode immediately.
	 */
	dirrem->dm_state |= COMPLETE;
	free_diradd(dap);
	return (dirrem);
}

/*
 * Directory entry change dependencies.
 * 
 * Changing an existing directory entry requires that an add operation
 * be completed first followed by a deletion. The semantics for the addition
 * are identical to the description of adding a new entry above except
 * that the rollback is to the old inode number rather than zero. Once
 * the addition dependency is completed, the removal is done as described
 * in the removal routine above.
 */

/*
 * This routine should be called immediately after changing
 * a directory entry.  The inode's link count should not be
 * decremented by the calling procedure -- the soft updates
 * code will perform this task when it is safe.
 */
/* buffer containing directory block */
/* inode for the directory being modified */
/* inode for directory entry being removed */
/* new inode number for changed entry */
/* indicates if doing RMDIR */
void 
softdep_setup_directory_change(struct buf *bp, struct inode *dp,
    struct inode *ip, long newinum, int isrmdir)
{
	int offset;
	struct diradd *dap;
	struct dirrem *dirrem, *prevdirrem;
	struct pagedep *pagedep;
	struct inodedep *inodedep;

	offset = blkoff(dp->i_fs, dp->i_offset);
	dap = pool_get(&diradd_pool, PR_WAITOK | PR_ZERO);
	dap->da_list.wk_type = D_DIRADD;
	dap->da_state = DIRCHG | ATTACHED | DEPCOMPLETE;
	dap->da_offset = offset;
	dap->da_newinum = newinum;

	/*
	 * Allocate a new dirrem and ACQUIRE_LOCK.
	 */
	dirrem = newdirrem(bp, dp, ip, isrmdir, &prevdirrem);
	pagedep = dirrem->dm_pagedep;
	/*
	 * The possible values for isrmdir:
	 *	0 - non-directory file rename
	 *	1 - directory rename within same directory
	 *   inum - directory rename to new directory of given inode number
	 * When renaming to a new directory, we are both deleting and
	 * creating a new directory entry, so the link count on the new
	 * directory should not change. Thus we do not need the followup
	 * dirrem which is usually done in handle_workitem_remove. We set
	 * the DIRCHG flag to tell handle_workitem_remove to skip the 
	 * followup dirrem.
	 */
	if (isrmdir > 1)
		dirrem->dm_state |= DIRCHG;

	/*
	 * If the COMPLETE flag is clear, then there were no active
	 * entries and we want to roll back to the previous inode until
	 * the new inode is committed to disk. If the COMPLETE flag is
	 * set, then we have deleted an entry that never made it to disk.
	 * If the entry we deleted resulted from a name change, then the old
	 * inode reference still resides on disk. Any rollback that we do
	 * needs to be to that old inode (returned to us in prevdirrem). If
	 * the entry we deleted resulted from a create, then there is
	 * no entry on the disk, so we want to roll back to zero rather
	 * than the uncommitted inode. In either of the COMPLETE cases we
	 * want to immediately free the unwritten and unreferenced inode.
	 */
	if ((dirrem->dm_state & COMPLETE) == 0) {
		dap->da_previous = dirrem;
	} else {
		if (prevdirrem != NULL) {
			dap->da_previous = prevdirrem;
		} else {
			dap->da_state &= ~DIRCHG;
			dap->da_pagedep = pagedep;
		}
		dirrem->dm_dirinum = pagedep->pd_ino;
		add_to_worklist(&dirrem->dm_list);
	}
	/*
	 * Link into its inodedep. Put it on the id_bufwait list if the inode
	 * is not yet written. If it is written, do the post-inode write
	 * processing to put it on the id_pendinghd list.
	 */
	if (inodedep_lookup(dp->i_fs, newinum, DEPALLOC, &inodedep) == 0 ||
	    (inodedep->id_state & ALLCOMPLETE) == ALLCOMPLETE) {
		dap->da_state |= COMPLETE;
		LIST_INSERT_HEAD(&pagedep->pd_pendinghd, dap, da_pdlist);
		WORKLIST_INSERT(&inodedep->id_pendinghd, &dap->da_list);
	} else {
		LIST_INSERT_HEAD(&pagedep->pd_diraddhd[DIRADDHASH(offset)],
		    dap, da_pdlist);
		WORKLIST_INSERT(&inodedep->id_bufwait, &dap->da_list);
	}
	FREE_LOCK(&lk);
}

/*
 * Called whenever the link count on an inode is changed.
 * It creates an inode dependency so that the new reference(s)
 * to the inode cannot be committed to disk until the updated
 * inode has been written.
 */
/* the inode with the increased link count */
/* do background work or not */
void
softdep_change_linkcnt(struct inode *ip, int nodelay)
{
	struct inodedep *inodedep;
	int flags;

	/*
	 * If requested, do not allow background work to happen.
	 */
	flags = DEPALLOC;
	if (nodelay)
		flags |= NODELAY;

	ACQUIRE_LOCK(&lk);

	(void) inodedep_lookup(ip->i_fs, ip->i_number, flags, &inodedep);
	if (DIP(ip, nlink) < ip->i_effnlink) {
		FREE_LOCK(&lk);
		panic("softdep_change_linkcnt: bad delta");
	}

	inodedep->id_nlinkdelta = DIP(ip, nlink) - ip->i_effnlink;

	FREE_LOCK(&lk);
}

/*
 * This workitem decrements the inode's link count.
 * If the link count reaches zero, the file is removed.
 */
STATIC void 
handle_workitem_remove(struct dirrem *dirrem)
{
	struct proc *p = CURPROC;	/* XXX */
	struct inodedep *inodedep;
	struct vnode *vp;
	struct inode *ip;
	ufsino_t oldinum;
	int error;

	if ((error = VFS_VGET(dirrem->dm_mnt, dirrem->dm_oldinum, &vp)) != 0) {
		softdep_error("handle_workitem_remove: vget", error);
		return;
	}
	ip = VTOI(vp);
	ACQUIRE_LOCK(&lk);
	if ((inodedep_lookup(ip->i_fs, dirrem->dm_oldinum, 0, &inodedep)) 
	    == 0) {
		FREE_LOCK(&lk);
		panic("handle_workitem_remove: lost inodedep");
	}
	/*
	 * Normal file deletion.
	 */
	if ((dirrem->dm_state & RMDIR) == 0) {
		DIP_ADD(ip, nlink, -1);
		ip->i_flag |= IN_CHANGE;
		if (DIP(ip, nlink) < ip->i_effnlink) {
			FREE_LOCK(&lk);
			panic("handle_workitem_remove: bad file delta");
		}
		inodedep->id_nlinkdelta = DIP(ip, nlink) - ip->i_effnlink;
		FREE_LOCK(&lk);
		vput(vp);
		num_dirrem -= 1;
		WORKITEM_FREE(dirrem, D_DIRREM);
		return;
	}
	/*
	 * Directory deletion. Decrement reference count for both the
	 * just deleted parent directory entry and the reference for ".".
	 * Next truncate the directory to length zero. When the
	 * truncation completes, arrange to have the reference count on
	 * the parent decremented to account for the loss of "..".
	 */
	DIP_ADD(ip, nlink, -2);
	ip->i_flag |= IN_CHANGE;
	if (DIP(ip, nlink) < ip->i_effnlink)
		panic("handle_workitem_remove: bad dir delta");
	inodedep->id_nlinkdelta = DIP(ip, nlink) - ip->i_effnlink;
	FREE_LOCK(&lk);
	if ((error = UFS_TRUNCATE(ip, (off_t)0, 0, p->p_ucred)) != 0)
		softdep_error("handle_workitem_remove: truncate", error);
	/*
	 * Rename a directory to a new parent. Since, we are both deleting
	 * and creating a new directory entry, the link count on the new
	 * directory should not change. Thus we skip the followup dirrem.
	 */
	if (dirrem->dm_state & DIRCHG) {
		vput(vp);
		num_dirrem -= 1;
		WORKITEM_FREE(dirrem, D_DIRREM);
		return;
	}
	/*
	 * If the inodedep does not exist, then the zero'ed inode has
	 * been written to disk. If the allocated inode has never been
	 * written to disk, then the on-disk inode is zero'ed. In either
	 * case we can remove the file immediately.
	 */
	ACQUIRE_LOCK(&lk);
	dirrem->dm_state = 0;
	oldinum = dirrem->dm_oldinum;
	dirrem->dm_oldinum = dirrem->dm_dirinum;
	if (inodedep_lookup(ip->i_fs, oldinum, 0, &inodedep) == 0 ||
	    check_inode_unwritten(inodedep)) {
		FREE_LOCK(&lk);
		vput(vp);
		handle_workitem_remove(dirrem);
		return;
	}
	WORKLIST_INSERT(&inodedep->id_inowait, &dirrem->dm_list);
	FREE_LOCK(&lk);
	ip->i_flag |= IN_CHANGE;
	UFS_UPDATE(VTOI(vp), 0);
	vput(vp);
}

/*
 * Inode de-allocation dependencies.
 * 
 * When an inode's link count is reduced to zero, it can be de-allocated. We
 * found it convenient to postpone de-allocation until after the inode is
 * written to disk with its new link count (zero).  At this point, all of the
 * on-disk inode's block pointers are nullified and, with careful dependency
 * list ordering, all dependencies related to the inode will be satisfied and
 * the corresponding dependency structures de-allocated.  So, if/when the
 * inode is reused, there will be no mixing of old dependencies with new
 * ones.  This artificial dependency is set up by the block de-allocation
 * procedure above (softdep_setup_freeblocks) and completed by the
 * following procedure.
 */
STATIC void 
handle_workitem_freefile(struct freefile *freefile)
{
	struct fs *fs;
	struct vnode vp;
	struct inode tip;
#ifdef DEBUG
	struct inodedep *idp;
#endif
	int error;

	fs = VFSTOUFS(freefile->fx_mnt)->um_fs;
#ifdef DEBUG
	ACQUIRE_LOCK(&lk);
	error = inodedep_lookup(fs, freefile->fx_oldinum, 0, &idp);
	FREE_LOCK(&lk);
	if (error)
		panic("handle_workitem_freefile: inodedep survived");
#endif
	tip.i_ump = VFSTOUFS(freefile->fx_mnt);
	tip.i_dev = freefile->fx_devvp->v_rdev;
	tip.i_fs = fs;
	tip.i_vnode = &vp;
	vp.v_data = &tip;

	if ((error = ffs_freefile(&tip, freefile->fx_oldinum, 
		 freefile->fx_mode)) != 0) {
		softdep_error("handle_workitem_freefile", error);
	}
	WORKITEM_FREE(freefile, D_FREEFILE);
}

/*
 * Disk writes.
 * 
 * The dependency structures constructed above are most actively used when file
 * system blocks are written to disk.  No constraints are placed on when a
 * block can be written, but unsatisfied update dependencies are made safe by
 * modifying (or replacing) the source memory for the duration of the disk
 * write.  When the disk write completes, the memory block is again brought
 * up-to-date.
 *
 * In-core inode structure reclamation.
 * 
 * Because there are a finite number of "in-core" inode structures, they are
 * reused regularly.  By transferring all inode-related dependencies to the
 * in-memory inode block and indexing them separately (via "inodedep"s), we
 * can allow "in-core" inode structures to be reused at any time and avoid
 * any increase in contention.
 *
 * Called just before entering the device driver to initiate a new disk I/O.
 * The buffer must be locked, thus, no I/O completion operations can occur
 * while we are manipulating its associated dependencies.
 */
/* structure describing disk write to occur */
void 
softdep_disk_io_initiation(struct buf *bp)
{
	struct worklist *wk, *nextwk;
	struct indirdep *indirdep;
	struct inodedep *inodedep;
	struct buf *sbp;

	/*
	 * We only care about write operations. There should never
	 * be dependencies for reads.
	 */
	if (bp->b_flags & B_READ)
		panic("softdep_disk_io_initiation: read");

	ACQUIRE_LOCK(&lk);

	/*
	 * Do any necessary pre-I/O processing.
	 */
	for (wk = LIST_FIRST(&bp->b_dep); wk; wk = nextwk) {
		nextwk = LIST_NEXT(wk, wk_list);
		switch (wk->wk_type) {

		case D_PAGEDEP:
			initiate_write_filepage(WK_PAGEDEP(wk), bp);
			continue;

		case D_INODEDEP:
			inodedep = WK_INODEDEP(wk);
			if (inodedep->id_fs->fs_magic == FS_UFS1_MAGIC)
				initiate_write_inodeblock_ufs1(inodedep, bp);
#ifdef FFS2
			else
				initiate_write_inodeblock_ufs2(inodedep, bp);
#endif
			continue;

		case D_INDIRDEP:
			indirdep = WK_INDIRDEP(wk);
			if (indirdep->ir_state & GOINGAWAY)
				panic("disk_io_initiation: indirdep gone");
			/*
			 * If there are no remaining dependencies, this
			 * will be writing the real pointers, so the
			 * dependency can be freed.
			 */
			if (LIST_FIRST(&indirdep->ir_deplisthd) == NULL) {
				sbp = indirdep->ir_savebp;
				sbp->b_flags |= B_INVAL | B_NOCACHE;
				/* inline expand WORKLIST_REMOVE(wk); */
				wk->wk_state &= ~ONWORKLIST;
				LIST_REMOVE(wk, wk_list);
				WORKITEM_FREE(indirdep, D_INDIRDEP);
				FREE_LOCK(&lk);
				brelse(sbp);
				ACQUIRE_LOCK(&lk);
				continue;
			}
			/*
			 * Replace up-to-date version with safe version.
			 */
			FREE_LOCK(&lk);
			indirdep->ir_saveddata = malloc(bp->b_bcount,
			    M_INDIRDEP, M_WAITOK);
			ACQUIRE_LOCK(&lk);
			indirdep->ir_state &= ~ATTACHED;
			indirdep->ir_state |= UNDONE;
			memcpy(indirdep->ir_saveddata, bp->b_data, bp->b_bcount);
			memcpy(bp->b_data, indirdep->ir_savebp->b_data,
			    bp->b_bcount);
			continue;

		case D_MKDIR:
		case D_BMSAFEMAP:
		case D_ALLOCDIRECT:
		case D_ALLOCINDIR:
			continue;

		default:
			FREE_LOCK(&lk);
			panic("handle_disk_io_initiation: Unexpected type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}

	FREE_LOCK(&lk);
}

/*
 * Called from within the procedure above to deal with unsatisfied
 * allocation dependencies in a directory. The buffer must be locked,
 * thus, no I/O completion operations can occur while we are
 * manipulating its associated dependencies.
 */
STATIC void
initiate_write_filepage(struct pagedep *pagedep, struct buf *bp)
{
	struct diradd *dap;
	struct direct *ep;
	int i;

	if (pagedep->pd_state & IOSTARTED) {
		/*
		 * This can only happen if there is a driver that does not
		 * understand chaining. Here biodone will reissue the call
		 * to strategy for the incomplete buffers.
		 */
		printf("initiate_write_filepage: already started\n");
		return;
	}
	pagedep->pd_state |= IOSTARTED;
	for (i = 0; i < DAHASHSZ; i++) {
		LIST_FOREACH(dap, &pagedep->pd_diraddhd[i], da_pdlist) {
			ep = (struct direct *)
			    ((char *)bp->b_data + dap->da_offset);
			if (ep->d_ino != dap->da_newinum) {
				FREE_LOCK(&lk);
				panic("%s: dir inum %u != new %u",
				    "initiate_write_filepage",
				    ep->d_ino, dap->da_newinum);
			}
			if (dap->da_state & DIRCHG)
				ep->d_ino = dap->da_previous->dm_oldinum;
			else
				ep->d_ino = 0;
			dap->da_state &= ~ATTACHED;
			dap->da_state |= UNDONE;
		}
	}
}

/*
 * Called from within the procedure above to deal with unsatisfied
 * allocation dependencies in an inodeblock. The buffer must be
 * locked, thus, no I/O completion operations can occur while we
 * are manipulating its associated dependencies.
 */
/* The inode block */
STATIC void 
initiate_write_inodeblock_ufs1(struct inodedep *inodedep, struct buf *bp)
{
	struct allocdirect *adp, *lastadp;
	struct ufs1_dinode *dp;
	struct fs *fs;
#ifdef DIAGNOSTIC
	daddr_t prevlbn = 0;
	int32_t d1, d2;
#endif
	int i, deplist;

	if (inodedep->id_state & IOSTARTED) {
		FREE_LOCK(&lk);
		panic("initiate_write_inodeblock: already started");
	}
	inodedep->id_state |= IOSTARTED;
	fs = inodedep->id_fs;
	dp = (struct ufs1_dinode *)bp->b_data +
	    ino_to_fsbo(fs, inodedep->id_ino);
	/*
	 * If the bitmap is not yet written, then the allocated
	 * inode cannot be written to disk.
	 */
	if ((inodedep->id_state & DEPCOMPLETE) == 0) {
		if (inodedep->id_savedino1 != NULL) {
			FREE_LOCK(&lk);
			panic("initiate_write_inodeblock: already doing I/O");
		}
		FREE_LOCK(&lk);
		inodedep->id_savedino1 = malloc(sizeof(struct ufs1_dinode),
		    M_INODEDEP, M_WAITOK);
		ACQUIRE_LOCK(&lk);
		*inodedep->id_savedino1 = *dp;
		memset(dp, 0, sizeof(struct ufs1_dinode));
		return;
	}
	/*
	 * If no dependencies, then there is nothing to roll back.
	 */
	inodedep->id_savedsize = dp->di_size;
	if (TAILQ_FIRST(&inodedep->id_inoupdt) == NULL)
		return;
	/*
	 * Set the dependencies to busy.
	 */
	for (deplist = 0, adp = TAILQ_FIRST(&inodedep->id_inoupdt); adp;
	     adp = TAILQ_NEXT(adp, ad_next)) {
#ifdef DIAGNOSTIC
		if (deplist != 0 && prevlbn >= adp->ad_lbn) {
			FREE_LOCK(&lk);
			panic("softdep_write_inodeblock: lbn order");
		}
		prevlbn = adp->ad_lbn;
		if (adp->ad_lbn < NDADDR &&
		    (d1 = dp->di_db[adp->ad_lbn]) != (d2 = adp->ad_newblkno)) {
			FREE_LOCK(&lk);
			panic("%s: direct pointer #%lld mismatch %d != %d",
			    "softdep_write_inodeblock", (long long)adp->ad_lbn,
			    d1, d2);
		}
		if (adp->ad_lbn >= NDADDR &&
		    (d1 = dp->di_ib[adp->ad_lbn - NDADDR]) !=
		    (d2 = adp->ad_newblkno)) {
			FREE_LOCK(&lk);
			panic("%s: indirect pointer #%lld mismatch %d != %d",
			    "softdep_write_inodeblock", (long long)(adp->ad_lbn -
			    NDADDR), d1, d2);
		}
		deplist |= 1 << adp->ad_lbn;
		if ((adp->ad_state & ATTACHED) == 0) {
			FREE_LOCK(&lk);
			panic("softdep_write_inodeblock: Unknown state 0x%x",
			    adp->ad_state);
		}
#endif /* DIAGNOSTIC */
		adp->ad_state &= ~ATTACHED;
		adp->ad_state |= UNDONE;
	}
	/*
	 * The on-disk inode cannot claim to be any larger than the last
	 * fragment that has been written. Otherwise, the on-disk inode
	 * might have fragments that were not the last block in the file
	 * which would corrupt the filesystem.
	 */
	for (lastadp = NULL, adp = TAILQ_FIRST(&inodedep->id_inoupdt); adp;
	     lastadp = adp, adp = TAILQ_NEXT(adp, ad_next)) {
		if (adp->ad_lbn >= NDADDR)
			break;
		dp->di_db[adp->ad_lbn] = adp->ad_oldblkno;
		/* keep going until hitting a rollback to a frag */
		if (adp->ad_oldsize == 0 || adp->ad_oldsize == fs->fs_bsize)
			continue;
		dp->di_size = fs->fs_bsize * adp->ad_lbn + adp->ad_oldsize;
		for (i = adp->ad_lbn + 1; i < NDADDR; i++) {
#ifdef DIAGNOSTIC
			if (dp->di_db[i] != 0 && (deplist & (1 << i)) == 0) {
				FREE_LOCK(&lk);
				panic("softdep_write_inodeblock: lost dep1");
			}
#endif /* DIAGNOSTIC */
			dp->di_db[i] = 0;
		}
		for (i = 0; i < NIADDR; i++) {
#ifdef DIAGNOSTIC
			if (dp->di_ib[i] != 0 &&
			    (deplist & ((1 << NDADDR) << i)) == 0) {
				FREE_LOCK(&lk);
				panic("softdep_write_inodeblock: lost dep2");
			}
#endif /* DIAGNOSTIC */
			dp->di_ib[i] = 0;
		}
		return;
	}
	/*
	 * If we have zero'ed out the last allocated block of the file,
	 * roll back the size to the last currently allocated block.
	 * We know that this last allocated block is a full-sized as
	 * we already checked for fragments in the loop above.
	 */
	if (lastadp != NULL &&
	    dp->di_size <= (lastadp->ad_lbn + 1) * fs->fs_bsize) {
		for (i = lastadp->ad_lbn; i >= 0; i--)
			if (dp->di_db[i] != 0)
				break;
		dp->di_size = (i + 1) * fs->fs_bsize;
	}
	/*
	 * The only dependencies are for indirect blocks.
	 *
	 * The file size for indirect block additions is not guaranteed.
	 * Such a guarantee would be non-trivial to achieve. The conventional
	 * synchronous write implementation also does not make this guarantee.
	 * Fsck should catch and fix discrepancies. Arguably, the file size
	 * can be over-estimated without destroying integrity when the file
	 * moves into the indirect blocks (i.e., is large). If we want to
	 * postpone fsck, we are stuck with this argument.
	 */
	for (; adp; adp = TAILQ_NEXT(adp, ad_next))
		dp->di_ib[adp->ad_lbn - NDADDR] = 0;
}

#ifdef FFS2
/*
 * Version of initiate_write_inodeblock that handles FFS2 dinodes.
 */
/* The inode block */
STATIC void
initiate_write_inodeblock_ufs2(struct inodedep *inodedep, struct buf *bp)
{
	struct allocdirect *adp, *lastadp;
	struct ufs2_dinode *dp;
	struct fs *fs = inodedep->id_fs;
#ifdef DIAGNOSTIC
	daddr_t prevlbn = -1, d1, d2;
#endif
	int deplist, i;

	if (inodedep->id_state & IOSTARTED)
		panic("initiate_write_inodeblock_ufs2: already started");
	inodedep->id_state |= IOSTARTED;
	fs = inodedep->id_fs;
	dp = (struct ufs2_dinode *)bp->b_data +
	    ino_to_fsbo(fs, inodedep->id_ino);
	/*
	 * If the bitmap is not yet written, then the allocated
	 * inode cannot be written to disk.
	 */
	if ((inodedep->id_state & DEPCOMPLETE) == 0) {
		if (inodedep->id_savedino2 != NULL)
			panic("initiate_write_inodeblock_ufs2: I/O underway");
		inodedep->id_savedino2 = malloc(sizeof(struct ufs2_dinode),
		    M_INODEDEP, M_WAITOK);
		*inodedep->id_savedino2 = *dp;
		memset(dp, 0, sizeof(struct ufs2_dinode));
		return;
	}
	/*
	 * If no dependencies, then there is nothing to roll back.
	 */
	inodedep->id_savedsize = dp->di_size;
	if (TAILQ_FIRST(&inodedep->id_inoupdt) == NULL)
		return;

#ifdef notyet
	inodedep->id_savedextsize = dp->di_extsize;
	if (TAILQ_FIRST(&inodedep->id_inoupdt) == NULL &&
	    TAILQ_FIRST(&inodedep->id_extupdt) == NULL)
		return;
	/*
	 * Set the ext data dependencies to busy.
	 */
	for (deplist = 0, adp = TAILQ_FIRST(&inodedep->id_extupdt); adp;
	     adp = TAILQ_NEXT(adp, ad_next)) {
#ifdef DIAGNOSTIC
		if (deplist != 0 && prevlbn >= adp->ad_lbn) {
			FREE_LOCK(&lk);
			panic("softdep_write_inodeblock: lbn order");
		}
		prevlbn = adp->ad_lbn;
		if ((d1 = dp->di_extb[adp->ad_lbn]) !=
		    (d2 = adp->ad_newblkno)) {
			FREE_LOCK(&lk);
			panic("%s: direct pointer #%lld mismatch %lld != %lld",
			    "softdep_write_inodeblock", (long long)adp->ad_lbn,
			    d1, d2);
		}
		deplist |= 1 << adp->ad_lbn;
		if ((adp->ad_state & ATTACHED) == 0) {
			FREE_LOCK(&lk);
			panic("softdep_write_inodeblock: Unknown state 0x%x",
			    adp->ad_state);
		}
#endif /* DIAGNOSTIC */
		adp->ad_state &= ~ATTACHED;
		adp->ad_state |= UNDONE;
	}
	/*
	 * The on-disk inode cannot claim to be any larger than the last
	 * fragment that has been written. Otherwise, the on-disk inode
	 * might have fragments that were not the last block in the ext
	 * data which would corrupt the filesystem.
	 */
	for (lastadp = NULL, adp = TAILQ_FIRST(&inodedep->id_extupdt); adp;
	     lastadp = adp, adp = TAILQ_NEXT(adp, ad_next)) {
		dp->di_extb[adp->ad_lbn] = adp->ad_oldblkno;
		/* keep going until hitting a rollback to a frag */
		if (adp->ad_oldsize == 0 || adp->ad_oldsize == fs->fs_bsize)
			continue;
		dp->di_extsize = fs->fs_bsize * adp->ad_lbn + adp->ad_oldsize;
		for (i = adp->ad_lbn + 1; i < NXADDR; i++) {
#ifdef DIAGNOSTIC
			if (dp->di_extb[i] != 0 && (deplist & (1 << i)) == 0) {
				FREE_LOCK(&lk);
				panic("softdep_write_inodeblock: lost dep1");
			}
#endif /* DIAGNOSTIC */
			dp->di_extb[i] = 0;
		}
		lastadp = NULL;
		break;
	}
	/*
	 * If we have zero'ed out the last allocated block of the ext
	 * data, roll back the size to the last currently allocated block.
	 * We know that this last allocated block is a full-sized as
	 * we already checked for fragments in the loop above.
	 */
	if (lastadp != NULL &&
	    dp->di_extsize <= (lastadp->ad_lbn + 1) * fs->fs_bsize) {
		for (i = lastadp->ad_lbn; i >= 0; i--)
			if (dp->di_extb[i] != 0)
				break;
		dp->di_extsize = (i + 1) * fs->fs_bsize;
	}
#endif /* notyet */

	/*
	 * Set the file data dependencies to busy.
	 */
	for (deplist = 0, adp = TAILQ_FIRST(&inodedep->id_inoupdt); adp;
	     adp = TAILQ_NEXT(adp, ad_next)) {
#ifdef DIAGNOSTIC
		if (deplist != 0 && prevlbn >= adp->ad_lbn) {
			FREE_LOCK(&lk);
			panic("softdep_write_inodeblock: lbn order");
		}
		prevlbn = adp->ad_lbn;
		if (adp->ad_lbn < NDADDR &&
		    (d1 = dp->di_db[adp->ad_lbn]) != (d2 = adp->ad_newblkno)) {
			FREE_LOCK(&lk);
			panic("%s: direct pointer #%lld mismatch %lld != %lld",
			    "softdep_write_inodeblock", (long long)adp->ad_lbn,
			    d1, d2);
		}
		if (adp->ad_lbn >= NDADDR &&
		    (d1 = dp->di_ib[adp->ad_lbn - NDADDR]) !=
		    (d2 = adp->ad_newblkno)) {
			FREE_LOCK(&lk);
			panic("%s: indirect pointer #%lld mismatch %lld != %lld",
			    "softdep_write_inodeblock", (long long)(adp->ad_lbn -
			    NDADDR), d1, d2);
		}
		deplist |= 1 << adp->ad_lbn;
		if ((adp->ad_state & ATTACHED) == 0) {
			FREE_LOCK(&lk);
			panic("softdep_write_inodeblock: Unknown state 0x%x",
			    adp->ad_state);
		}
#endif /* DIAGNOSTIC */
		adp->ad_state &= ~ATTACHED;
		adp->ad_state |= UNDONE;
	}
	/*
	 * The on-disk inode cannot claim to be any larger than the last
	 * fragment that has been written. Otherwise, the on-disk inode
	 * might have fragments that were not the last block in the file
	 * which would corrupt the filesystem.
	 */
	for (lastadp = NULL, adp = TAILQ_FIRST(&inodedep->id_inoupdt); adp;
	     lastadp = adp, adp = TAILQ_NEXT(adp, ad_next)) {
		if (adp->ad_lbn >= NDADDR)
			break;
		dp->di_db[adp->ad_lbn] = adp->ad_oldblkno;
		/* keep going until hitting a rollback to a frag */
		if (adp->ad_oldsize == 0 || adp->ad_oldsize == fs->fs_bsize)
			continue;
		dp->di_size = fs->fs_bsize * adp->ad_lbn + adp->ad_oldsize;
		for (i = adp->ad_lbn + 1; i < NDADDR; i++) {
#ifdef DIAGNOSTIC
			if (dp->di_db[i] != 0 && (deplist & (1 << i)) == 0) {
				FREE_LOCK(&lk);
				panic("softdep_write_inodeblock: lost dep2");
			}
#endif /* DIAGNOSTIC */
			dp->di_db[i] = 0;
		}
		for (i = 0; i < NIADDR; i++) {
#ifdef DIAGNOSTIC
			if (dp->di_ib[i] != 0 &&
			    (deplist & ((1 << NDADDR) << i)) == 0) {
				FREE_LOCK(&lk);
				panic("softdep_write_inodeblock: lost dep3");
			}
#endif /* DIAGNOSTIC */
			dp->di_ib[i] = 0;
		}
		return;
	}
	/*
	 * If we have zero'ed out the last allocated block of the file,
	 * roll back the size to the last currently allocated block.
	 * We know that this last allocated block is a full-sized as
	 * we already checked for fragments in the loop above.
	 */
	if (lastadp != NULL &&
	    dp->di_size <= (lastadp->ad_lbn + 1) * fs->fs_bsize) {
		for (i = lastadp->ad_lbn; i >= 0; i--)
			if (dp->di_db[i] != 0)
				break;
		dp->di_size = (i + 1) * fs->fs_bsize;
	}
	/*
	 * The only dependencies are for indirect blocks.
	 *
	 * The file size for indirect block additions is not guaranteed.
	 * Such a guarantee would be non-trivial to achieve. The conventional
	 * synchronous write implementation also does not make this guarantee.
	 * Fsck should catch and fix discrepancies. Arguably, the file size
	 * can be over-estimated without destroying integrity when the file
	 * moves into the indirect blocks (i.e., is large). If we want to
	 * postpone fsck, we are stuck with this argument.
	 */
	for (; adp; adp = TAILQ_NEXT(adp, ad_next))
		dp->di_ib[adp->ad_lbn - NDADDR] = 0;
}
#endif /* FFS2 */

/*
 * This routine is called during the completion interrupt
 * service routine for a disk write (from the procedure called
 * by the device driver to inform the file system caches of
 * a request completion).  It should be called early in this
 * procedure, before the block is made available to other
 * processes or other routines are called.
 */
/* describes the completed disk write */
void 
softdep_disk_write_complete(struct buf *bp)
{
	struct worklist *wk;
	struct workhead reattach;
	struct newblk *newblk;
	struct allocindir *aip;
	struct allocdirect *adp;
	struct indirdep *indirdep;
	struct inodedep *inodedep;
	struct bmsafemap *bmsafemap;

	/*
	 * If an error occurred while doing the write, then the data
	 * has not hit the disk and the dependencies cannot be unrolled.
	 */
	if ((bp->b_flags & B_ERROR) && !(bp->b_flags & B_INVAL))
		return;

#ifdef DEBUG
	if (lk.lkt_held != -1)
		panic("softdep_disk_write_complete: lock is held");
	lk.lkt_held = -2;
#endif
	LIST_INIT(&reattach);
	while ((wk = LIST_FIRST(&bp->b_dep)) != NULL) {
		WORKLIST_REMOVE(wk);
		switch (wk->wk_type) {

		case D_PAGEDEP:
			if (handle_written_filepage(WK_PAGEDEP(wk), bp))
				WORKLIST_INSERT(&reattach, wk);
			continue;

		case D_INODEDEP:
			if (handle_written_inodeblock(WK_INODEDEP(wk), bp))
				WORKLIST_INSERT(&reattach, wk);
			continue;

		case D_BMSAFEMAP:
			bmsafemap = WK_BMSAFEMAP(wk);
			while ((newblk = LIST_FIRST(&bmsafemap->sm_newblkhd))) {
				newblk->nb_state |= DEPCOMPLETE;
				newblk->nb_bmsafemap = NULL;
				LIST_REMOVE(newblk, nb_deps);
			}
			while ((adp =
			   LIST_FIRST(&bmsafemap->sm_allocdirecthd))) {
				adp->ad_state |= DEPCOMPLETE;
				adp->ad_buf = NULL;
				LIST_REMOVE(adp, ad_deps);
				handle_allocdirect_partdone(adp);
			}
			while ((aip =
			    LIST_FIRST(&bmsafemap->sm_allocindirhd))) {
				aip->ai_state |= DEPCOMPLETE;
				aip->ai_buf = NULL;
				LIST_REMOVE(aip, ai_deps);
				handle_allocindir_partdone(aip);
			}
			while ((inodedep =
			     LIST_FIRST(&bmsafemap->sm_inodedephd)) != NULL) {
				inodedep->id_state |= DEPCOMPLETE;
				LIST_REMOVE(inodedep, id_deps);
				inodedep->id_buf = NULL;
			}
			WORKITEM_FREE(bmsafemap, D_BMSAFEMAP);
			continue;

		case D_MKDIR:
			handle_written_mkdir(WK_MKDIR(wk), MKDIR_BODY);
			continue;

		case D_ALLOCDIRECT:
			adp = WK_ALLOCDIRECT(wk);
			adp->ad_state |= COMPLETE;
			handle_allocdirect_partdone(adp);
			continue;

		case D_ALLOCINDIR:
			aip = WK_ALLOCINDIR(wk);
			aip->ai_state |= COMPLETE;
			handle_allocindir_partdone(aip);
			continue;

		case D_INDIRDEP:
			indirdep = WK_INDIRDEP(wk);
			if (indirdep->ir_state & GOINGAWAY)
				panic("disk_write_complete: indirdep gone");
			memcpy(bp->b_data, indirdep->ir_saveddata, bp->b_bcount);
			free(indirdep->ir_saveddata, M_INDIRDEP, 0);
			indirdep->ir_saveddata = NULL;
			indirdep->ir_state &= ~UNDONE;
			indirdep->ir_state |= ATTACHED;
			while ((aip = LIST_FIRST(&indirdep->ir_donehd))) {
				handle_allocindir_partdone(aip);
				if (aip == LIST_FIRST(&indirdep->ir_donehd))
					panic("disk_write_complete: not gone");
			}
			WORKLIST_INSERT(&reattach, wk);
			if ((bp->b_flags & B_DELWRI) == 0)
				stat_indir_blk_ptrs++;
			buf_dirty(bp);
			continue;

		default:
			panic("handle_disk_write_complete: Unknown type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}
	/*
	 * Reattach any requests that must be redone.
	 */
	while ((wk = LIST_FIRST(&reattach)) != NULL) {
		WORKLIST_REMOVE(wk);
		WORKLIST_INSERT(&bp->b_dep, wk);
	}
#ifdef DEBUG
	if (lk.lkt_held != -2)
		panic("softdep_disk_write_complete: lock lost");
	lk.lkt_held = -1;
#endif
}

/*
 * Called from within softdep_disk_write_complete above. Note that
 * this routine is always called from interrupt level with further
 * splbio interrupts blocked.
 */
/* the completed allocdirect */
STATIC void 
handle_allocdirect_partdone(struct allocdirect *adp)
{
	struct allocdirect *listadp;
	struct inodedep *inodedep;
	long bsize, delay;

	splassert(IPL_BIO);

	if ((adp->ad_state & ALLCOMPLETE) != ALLCOMPLETE)
		return;
	if (adp->ad_buf != NULL)
		panic("handle_allocdirect_partdone: dangling dep");

	/*
	 * The on-disk inode cannot claim to be any larger than the last
	 * fragment that has been written. Otherwise, the on-disk inode
	 * might have fragments that were not the last block in the file
	 * which would corrupt the filesystem. Thus, we cannot free any
	 * allocdirects after one whose ad_oldblkno claims a fragment as
	 * these blocks must be rolled back to zero before writing the inode.
	 * We check the currently active set of allocdirects in id_inoupdt.
	 */
	inodedep = adp->ad_inodedep;
	bsize = inodedep->id_fs->fs_bsize;
	TAILQ_FOREACH(listadp, &inodedep->id_inoupdt, ad_next) {
		/* found our block */
		if (listadp == adp)
			break;
		/* continue if ad_oldlbn is not a fragment */
		if (listadp->ad_oldsize == 0 ||
		    listadp->ad_oldsize == bsize)
			continue;
		/* hit a fragment */
		return;
	}
	/*
	 * If we have reached the end of the current list without
	 * finding the just finished dependency, then it must be
	 * on the future dependency list. Future dependencies cannot
	 * be freed until they are moved to the current list.
	 */
	if (listadp == NULL) {
#ifdef DEBUG
		TAILQ_FOREACH(listadp, &inodedep->id_newinoupdt, ad_next)
			/* found our block */
			if (listadp == adp)
				break;
		if (listadp == NULL)
			panic("handle_allocdirect_partdone: lost dep");
#endif /* DEBUG */
		return;
	}
	/*
	 * If we have found the just finished dependency, then free
	 * it along with anything that follows it that is complete.
	 * If the inode still has a bitmap dependency, then it has
	 * never been written to disk, hence the on-disk inode cannot
	 * reference the old fragment so we can free it without delay.
	 */
	delay = (inodedep->id_state & DEPCOMPLETE);
	for (; adp; adp = listadp) {
		listadp = TAILQ_NEXT(adp, ad_next);
		if ((adp->ad_state & ALLCOMPLETE) != ALLCOMPLETE)
			return;
		free_allocdirect(&inodedep->id_inoupdt, adp, delay);
	}
}

/*
 * Called from within softdep_disk_write_complete above. Note that
 * this routine is always called from interrupt level with further
 * splbio interrupts blocked.
 */
/* the completed allocindir */
STATIC void
handle_allocindir_partdone(struct allocindir *aip)
{
	struct indirdep *indirdep;

	splassert(IPL_BIO);

	if ((aip->ai_state & ALLCOMPLETE) != ALLCOMPLETE)
		return;
	if (aip->ai_buf != NULL)
		panic("handle_allocindir_partdone: dangling dependency");
	indirdep = aip->ai_indirdep;
	if (indirdep->ir_state & UNDONE) {
		LIST_REMOVE(aip, ai_next);
		LIST_INSERT_HEAD(&indirdep->ir_donehd, aip, ai_next);
		return;
	}
	if (indirdep->ir_state & UFS1FMT)
		((int32_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
		    aip->ai_newblkno;
	else
		((int64_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
		    aip->ai_newblkno;
	LIST_REMOVE(aip, ai_next);
	if (aip->ai_freefrag != NULL)
		add_to_worklist(&aip->ai_freefrag->ff_list);
	WORKITEM_FREE(aip, D_ALLOCINDIR);
}

/*
 * Called from within softdep_disk_write_complete above to restore
 * in-memory inode block contents to their most up-to-date state. Note
 * that this routine is always called from interrupt level with further
 * splbio interrupts blocked.
 */
/* buffer containing the inode block */
STATIC int 
handle_written_inodeblock(struct inodedep *inodedep, struct buf *bp)
{
	struct worklist *wk, *filefree;
	struct allocdirect *adp, *nextadp;
	struct ufs1_dinode *dp1 = NULL;
	struct ufs2_dinode *dp2 = NULL;
	int hadchanges, fstype;

	splassert(IPL_BIO);

	if ((inodedep->id_state & IOSTARTED) == 0)
		panic("handle_written_inodeblock: not started");
	inodedep->id_state &= ~IOSTARTED;

	if (inodedep->id_fs->fs_magic == FS_UFS1_MAGIC) {
		fstype = UM_UFS1;
		dp1 = (struct ufs1_dinode *) bp->b_data +
		    ino_to_fsbo(inodedep->id_fs, inodedep->id_ino);
	} else {
		fstype = UM_UFS2;
		dp2 = (struct ufs2_dinode *) bp->b_data +
		    ino_to_fsbo(inodedep->id_fs, inodedep->id_ino);
	}

	/*
	 * If we had to rollback the inode allocation because of
	 * bitmaps being incomplete, then simply restore it.
	 * Keep the block dirty so that it will not be reclaimed until
	 * all associated dependencies have been cleared and the
	 * corresponding updates written to disk.
	 */
	if (inodedep->id_savedino1 != NULL) {
		if (fstype == UM_UFS1)
			*dp1 = *inodedep->id_savedino1;
		else
			*dp2 = *inodedep->id_savedino2;
		free(inodedep->id_savedino1, M_INODEDEP, 0);
		inodedep->id_savedino1 = NULL;
		if ((bp->b_flags & B_DELWRI) == 0)
			stat_inode_bitmap++;
		buf_dirty(bp);
		return (1);
	}
	inodedep->id_state |= COMPLETE;
	/*
	 * Roll forward anything that had to be rolled back before 
	 * the inode could be updated.
	 */
	hadchanges = 0;
	for (adp = TAILQ_FIRST(&inodedep->id_inoupdt); adp; adp = nextadp) {
		nextadp = TAILQ_NEXT(adp, ad_next);
		if (adp->ad_state & ATTACHED)
			panic("handle_written_inodeblock: new entry");
		if (fstype == UM_UFS1) {
			if (adp->ad_lbn < NDADDR) {
				if (dp1->di_db[adp->ad_lbn] != adp->ad_oldblkno)
					 panic("%s: %s #%lld mismatch %d != "
					     "%lld",
					     "handle_written_inodeblock",
					     "direct pointer",
					     (long long)adp->ad_lbn,
					     dp1->di_db[adp->ad_lbn],
					     (long long)adp->ad_oldblkno);
				dp1->di_db[adp->ad_lbn] = adp->ad_newblkno;
			} else {
				if (dp1->di_ib[adp->ad_lbn - NDADDR] != 0)
					panic("%s: %s #%lld allocated as %d",
					    "handle_written_inodeblock",
					    "indirect pointer",
					    (long long)(adp->ad_lbn - NDADDR),
					    dp1->di_ib[adp->ad_lbn - NDADDR]);
				dp1->di_ib[adp->ad_lbn - NDADDR] =
				   adp->ad_newblkno;
			}
		} else {
			if (adp->ad_lbn < NDADDR) {
				if (dp2->di_db[adp->ad_lbn] != adp->ad_oldblkno)
					panic("%s: %s #%lld mismatch %lld != "
					    "%lld", "handle_written_inodeblock",
					    "direct pointer",
					    (long long)adp->ad_lbn,
					    dp2->di_db[adp->ad_lbn],
					    (long long)adp->ad_oldblkno);
				dp2->di_db[adp->ad_lbn] = adp->ad_newblkno;
			} else {
				if (dp2->di_ib[adp->ad_lbn - NDADDR] != 0)
					panic("%s: %s #%lld allocated as %lld",
					    "handle_written_inodeblock",
					    "indirect pointer",
					    (long long)(adp->ad_lbn - NDADDR),
					    dp2->di_ib[adp->ad_lbn - NDADDR]);
				dp2->di_ib[adp->ad_lbn - NDADDR] =
				    adp->ad_newblkno;
			}
		}
		adp->ad_state &= ~UNDONE;
		adp->ad_state |= ATTACHED;
		hadchanges = 1;
	}
	if (hadchanges && (bp->b_flags & B_DELWRI) == 0)
		stat_direct_blk_ptrs++;
	/*
	 * Reset the file size to its most up-to-date value.
	 */
	if (inodedep->id_savedsize == -1)
		panic("handle_written_inodeblock: bad size");
	
	if (fstype == UM_UFS1) {
		if (dp1->di_size != inodedep->id_savedsize) {
			dp1->di_size = inodedep->id_savedsize;
			hadchanges = 1;
		}
	} else {
		if (dp2->di_size != inodedep->id_savedsize) {
			dp2->di_size = inodedep->id_savedsize;
			hadchanges = 1;
		}
	}
	inodedep->id_savedsize = -1;
	/*
	 * If there were any rollbacks in the inode block, then it must be
	 * marked dirty so that its will eventually get written back in
	 * its correct form.
	 */
	if (hadchanges)
		buf_dirty(bp);
	/*
	 * Process any allocdirects that completed during the update.
	 */
	if ((adp = TAILQ_FIRST(&inodedep->id_inoupdt)) != NULL)
		handle_allocdirect_partdone(adp);
	/*
	 * Process deallocations that were held pending until the
	 * inode had been written to disk. Freeing of the inode
	 * is delayed until after all blocks have been freed to
	 * avoid creation of new <vfsid, inum, lbn> triples
	 * before the old ones have been deleted.
	 */
	filefree = NULL;
	while ((wk = LIST_FIRST(&inodedep->id_bufwait)) != NULL) {
		WORKLIST_REMOVE(wk);
		switch (wk->wk_type) {

		case D_FREEFILE:
			/*
			 * We defer adding filefree to the worklist until
			 * all other additions have been made to ensure
			 * that it will be done after all the old blocks
			 * have been freed.
			 */
			if (filefree != NULL)
				panic("handle_written_inodeblock: filefree");
			filefree = wk;
			continue;

		case D_MKDIR:
			handle_written_mkdir(WK_MKDIR(wk), MKDIR_PARENT);
			continue;

		case D_DIRADD:
			diradd_inode_written(WK_DIRADD(wk), inodedep);
			continue;

		case D_FREEBLKS:
			wk->wk_state |= COMPLETE;
			if ((wk->wk_state & ALLCOMPLETE) != ALLCOMPLETE)
				continue;
			/* FALLTHROUGH */
		case D_FREEFRAG:
		case D_DIRREM:
			add_to_worklist(wk);
			continue;

		case D_NEWDIRBLK:
			free_newdirblk(WK_NEWDIRBLK(wk));
			continue;

		default:
			panic("handle_written_inodeblock: Unknown type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}
	if (filefree != NULL) {
		if (free_inodedep(inodedep) == 0)
			panic("handle_written_inodeblock: live inodedep");
		add_to_worklist(filefree);
		return (0);
	}

	/*
	 * If no outstanding dependencies, free it.
	 */
	if (free_inodedep(inodedep) ||
	    TAILQ_FIRST(&inodedep->id_inoupdt) == NULL)
		return (0);
	return (hadchanges);
}

/*
 * Process a diradd entry after its dependent inode has been written.
 * This routine must be called with splbio interrupts blocked.
 */
STATIC void
diradd_inode_written(struct diradd *dap, struct inodedep *inodedep)
{
	struct pagedep *pagedep;

	splassert(IPL_BIO);

	dap->da_state |= COMPLETE;
	if ((dap->da_state & ALLCOMPLETE) == ALLCOMPLETE) {
		if (dap->da_state & DIRCHG)
			pagedep = dap->da_previous->dm_pagedep;
		else
			pagedep = dap->da_pagedep;
		LIST_REMOVE(dap, da_pdlist);
		LIST_INSERT_HEAD(&pagedep->pd_pendinghd, dap, da_pdlist);
	}
	WORKLIST_INSERT(&inodedep->id_pendinghd, &dap->da_list);
}

/*
 * Handle the completion of a mkdir dependency.
 */
STATIC void
handle_written_mkdir(struct mkdir *mkdir, int type)
{
	struct diradd *dap;
	struct pagedep *pagedep;

	splassert(IPL_BIO);

	if (mkdir->md_state != type)
		panic("handle_written_mkdir: bad type");
	dap = mkdir->md_diradd;
	dap->da_state &= ~type;
	if ((dap->da_state & (MKDIR_PARENT | MKDIR_BODY)) == 0)
		dap->da_state |= DEPCOMPLETE;
	if ((dap->da_state & ALLCOMPLETE) == ALLCOMPLETE) {
		if (dap->da_state & DIRCHG)
			pagedep = dap->da_previous->dm_pagedep;
		else
			pagedep = dap->da_pagedep;
		LIST_REMOVE(dap, da_pdlist);
		LIST_INSERT_HEAD(&pagedep->pd_pendinghd, dap, da_pdlist);
	}
	LIST_REMOVE(mkdir, md_mkdirs);
	WORKITEM_FREE(mkdir, D_MKDIR);
}

/*
 * Called from within softdep_disk_write_complete above.
 * A write operation was just completed. Removed inodes can
 * now be freed and associated block pointers may be committed.
 * Note that this routine is always called from interrupt level
 * with further splbio interrupts blocked.
 */
/* buffer containing the written page */
STATIC int 
handle_written_filepage(struct pagedep *pagedep, struct buf *bp)
{
	struct dirrem *dirrem;
	struct diradd *dap, *nextdap;
	struct direct *ep;
	int i, chgs;

	splassert(IPL_BIO);

	if ((pagedep->pd_state & IOSTARTED) == 0)
		panic("handle_written_filepage: not started");
	pagedep->pd_state &= ~IOSTARTED;
	/*
	 * Process any directory removals that have been committed.
	 */
	while ((dirrem = LIST_FIRST(&pagedep->pd_dirremhd)) != NULL) {
		LIST_REMOVE(dirrem, dm_next);
		dirrem->dm_dirinum = pagedep->pd_ino;
		add_to_worklist(&dirrem->dm_list);
	}
	/*
	 * Free any directory additions that have been committed.
	 * If it is a newly allocated block, we have to wait until
	 * the on-disk directory inode claims the new block.
	 */
	if ((pagedep->pd_state & NEWBLOCK) == 0)
		while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
			free_diradd(dap);
	/*
	 * Uncommitted directory entries must be restored.
	 */
	for (chgs = 0, i = 0; i < DAHASHSZ; i++) {
		for (dap = LIST_FIRST(&pagedep->pd_diraddhd[i]); dap;
		     dap = nextdap) {
			nextdap = LIST_NEXT(dap, da_pdlist);
			if (dap->da_state & ATTACHED)
				panic("handle_written_filepage: attached");
			ep = (struct direct *)
			    ((char *)bp->b_data + dap->da_offset);
			ep->d_ino = dap->da_newinum;
			dap->da_state &= ~UNDONE;
			dap->da_state |= ATTACHED;
			chgs = 1;
			/*
			 * If the inode referenced by the directory has
			 * been written out, then the dependency can be
			 * moved to the pending list.
			 */
			if ((dap->da_state & ALLCOMPLETE) == ALLCOMPLETE) {
				LIST_REMOVE(dap, da_pdlist);
				LIST_INSERT_HEAD(&pagedep->pd_pendinghd, dap,
				    da_pdlist);
			}
		}
	}
	/*
	 * If there were any rollbacks in the directory, then it must be
	 * marked dirty so that its will eventually get written back in
	 * its correct form.
	 */
	if (chgs) {
		if ((bp->b_flags & B_DELWRI) == 0)
			stat_dir_entry++;
		buf_dirty(bp);
		return (1);
	}
	/*
	 * If we are not waiting for a new directory block to be
	 * claimed by its inode, then the pagedep will be freed.
	 * Otherwise it will remain to track any new entries on
	 * the page in case they are fsync'ed.
	 */
	if ((pagedep->pd_state & NEWBLOCK) == 0) {
		LIST_REMOVE(pagedep, pd_hash);
		WORKITEM_FREE(pagedep, D_PAGEDEP);
	}
	return (0);
}

/*
 * Writing back in-core inode structures.
 * 
 * The file system only accesses an inode's contents when it occupies an
 * "in-core" inode structure.  These "in-core" structures are separate from
 * the page frames used to cache inode blocks.  Only the latter are
 * transferred to/from the disk.  So, when the updated contents of the
 * "in-core" inode structure are copied to the corresponding in-memory inode
 * block, the dependencies are also transferred.  The following procedure is
 * called when copying a dirty "in-core" inode to a cached inode block.
 */

/*
 * Called when an inode is loaded from disk. If the effective link count
 * differed from the actual link count when it was last flushed, then we
 * need to ensure that the correct effective link count is put back.
 */
/* the "in_core" copy of the inode */
void 
softdep_load_inodeblock(struct inode *ip)
{
	struct inodedep *inodedep;

	/*
	 * Check for alternate nlink count.
	 */
	ip->i_effnlink = DIP(ip, nlink);
	ACQUIRE_LOCK(&lk);
	if (inodedep_lookup(ip->i_fs, ip->i_number, 0, &inodedep) == 0) {
		FREE_LOCK(&lk);
		return;
	}
	ip->i_effnlink -= inodedep->id_nlinkdelta;
	FREE_LOCK(&lk);
}

/*
 * This routine is called just before the "in-core" inode
 * information is to be copied to the in-memory inode block.
 * Recall that an inode block contains several inodes. If
 * the force flag is set, then the dependencies will be
 * cleared so that the update can always be made. Note that
 * the buffer is locked when this routine is called, so we
 * will never be in the middle of writing the inode block 
 * to disk.
 */
/* the "in_core" copy of the inode */
/* the buffer containing the inode block */
/* nonzero => update must be allowed */
void 
softdep_update_inodeblock(struct inode *ip, struct buf *bp, int waitfor)
{
	struct inodedep *inodedep;
	struct worklist *wk;
	int error, gotit;

	/*
	 * If the effective link count is not equal to the actual link
	 * count, then we must track the difference in an inodedep while
	 * the inode is (potentially) tossed out of the cache. Otherwise,
	 * if there is no existing inodedep, then there are no dependencies
	 * to track.
	 */
	ACQUIRE_LOCK(&lk);
	if (inodedep_lookup(ip->i_fs, ip->i_number, 0, &inodedep) == 0) {
		FREE_LOCK(&lk);
		if (ip->i_effnlink != DIP(ip, nlink))
			panic("softdep_update_inodeblock: bad link count");
		return;
	}
	if (inodedep->id_nlinkdelta != DIP(ip, nlink) - ip->i_effnlink) {
		FREE_LOCK(&lk);
		panic("softdep_update_inodeblock: bad delta");
	}
	/*
	 * Changes have been initiated. Anything depending on these
	 * changes cannot occur until this inode has been written.
	 */
	inodedep->id_state &= ~COMPLETE;
	if ((inodedep->id_state & ONWORKLIST) == 0)
		WORKLIST_INSERT(&bp->b_dep, &inodedep->id_list);
	/*
	 * Any new dependencies associated with the incore inode must 
	 * now be moved to the list associated with the buffer holding
	 * the in-memory copy of the inode. Once merged process any
	 * allocdirects that are completed by the merger.
	 */
	merge_inode_lists(inodedep);
	if (TAILQ_FIRST(&inodedep->id_inoupdt) != NULL)
		handle_allocdirect_partdone(TAILQ_FIRST(&inodedep->id_inoupdt));
	/*
	 * Now that the inode has been pushed into the buffer, the
	 * operations dependent on the inode being written to disk
	 * can be moved to the id_bufwait so that they will be
	 * processed when the buffer I/O completes.
	 */
	while ((wk = LIST_FIRST(&inodedep->id_inowait)) != NULL) {
		WORKLIST_REMOVE(wk);
		WORKLIST_INSERT(&inodedep->id_bufwait, wk);
	}
	/*
	 * Newly allocated inodes cannot be written until the bitmap
	 * that allocates them have been written (indicated by
	 * DEPCOMPLETE being set in id_state). If we are doing a
	 * forced sync (e.g., an fsync on a file), we force the bitmap
	 * to be written so that the update can be done.
	 */
	do {
		if ((inodedep->id_state & DEPCOMPLETE) != 0 || waitfor == 0) {
			FREE_LOCK(&lk);
			return;
		}
		bp = inodedep->id_buf;
		gotit = getdirtybuf(bp, MNT_WAIT);
	} while (gotit == -1);
	FREE_LOCK(&lk);
	if (gotit && (error = bwrite(bp)) != 0)
		softdep_error("softdep_update_inodeblock: bwrite", error);
	if ((inodedep->id_state & DEPCOMPLETE) == 0)
		panic("softdep_update_inodeblock: update failed");
}

/*
 * Merge the new inode dependency list (id_newinoupdt) into the old
 * inode dependency list (id_inoupdt). This routine must be called
 * with splbio interrupts blocked.
 */
STATIC void
merge_inode_lists(struct inodedep *inodedep)
{
	struct allocdirect *listadp, *newadp;

	splassert(IPL_BIO);

	newadp = TAILQ_FIRST(&inodedep->id_newinoupdt);
	for (listadp = TAILQ_FIRST(&inodedep->id_inoupdt); listadp && newadp;) {
		if (listadp->ad_lbn < newadp->ad_lbn) {
			listadp = TAILQ_NEXT(listadp, ad_next);
			continue;
		}
		TAILQ_REMOVE(&inodedep->id_newinoupdt, newadp, ad_next);
		TAILQ_INSERT_BEFORE(listadp, newadp, ad_next);
		if (listadp->ad_lbn == newadp->ad_lbn) {
			allocdirect_merge(&inodedep->id_inoupdt, newadp,
			    listadp);
			listadp = newadp;
		}
		newadp = TAILQ_FIRST(&inodedep->id_newinoupdt);
	}
	while ((newadp = TAILQ_FIRST(&inodedep->id_newinoupdt)) != NULL) {
		TAILQ_REMOVE(&inodedep->id_newinoupdt, newadp, ad_next);
		TAILQ_INSERT_TAIL(&inodedep->id_inoupdt, newadp, ad_next);
	}
}

/*
 * If we are doing an fsync, then we must ensure that any directory
 * entries for the inode have been written after the inode gets to disk.
 */
/* the "in_core" copy of the inode */
int
softdep_fsync(struct vnode *vp)
{
	struct inodedep *inodedep;
	struct pagedep *pagedep;
	struct worklist *wk;
	struct diradd *dap;
	struct mount *mnt;
	struct vnode *pvp;
	struct inode *ip;
	struct inode *pip;
	struct buf *bp;
	struct fs *fs;
	struct proc *p = CURPROC;		/* XXX */
	int error, flushparent;
	ufsino_t parentino;
	daddr_t lbn;

	ip = VTOI(vp);
	fs = ip->i_fs;
	ACQUIRE_LOCK(&lk);
	if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) == 0) {
		FREE_LOCK(&lk);
		return (0);
	}
	if (LIST_FIRST(&inodedep->id_inowait) != NULL ||
	    LIST_FIRST(&inodedep->id_bufwait) != NULL ||
	    TAILQ_FIRST(&inodedep->id_inoupdt) != NULL ||
	    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL) {
		FREE_LOCK(&lk);
		panic("softdep_fsync: pending ops");
	}
	for (error = 0, flushparent = 0; ; ) {
		if ((wk = LIST_FIRST(&inodedep->id_pendinghd)) == NULL)
			break;
		if (wk->wk_type != D_DIRADD) {
			FREE_LOCK(&lk);
			panic("softdep_fsync: Unexpected type %s",
			    TYPENAME(wk->wk_type));
		}
		dap = WK_DIRADD(wk);
		/*
		 * Flush our parent if this directory entry has a MKDIR_PARENT
		 * dependency or is contained in a newly allocated block.
		 */
		if (dap->da_state & DIRCHG)
			pagedep = dap->da_previous->dm_pagedep;
		else
			pagedep = dap->da_pagedep;
		mnt = pagedep->pd_mnt;
		parentino = pagedep->pd_ino;
		lbn = pagedep->pd_lbn;
		if ((dap->da_state & (MKDIR_BODY | COMPLETE)) != COMPLETE) {
			FREE_LOCK(&lk);
			panic("softdep_fsync: dirty");
		}
		if ((dap->da_state & MKDIR_PARENT) ||
		    (pagedep->pd_state & NEWBLOCK))
			flushparent = 1;
		else
			flushparent = 0;
		/*
		 * If we are being fsync'ed as part of vgone'ing this vnode,
		 * then we will not be able to release and recover the
		 * vnode below, so we just have to give up on writing its
		 * directory entry out. It will eventually be written, just
		 * not now, but then the user was not asking to have it
		 * written, so we are not breaking any promises.
		 */
		if (vp->v_flag & VXLOCK)
			break;
		/*
		 * We prevent deadlock by always fetching inodes from the
		 * root, moving down the directory tree. Thus, when fetching
		 * our parent directory, we must unlock ourselves before
		 * requesting the lock on our parent. See the comment in
		 * ufs_lookup for details on possible races.
		 */
		FREE_LOCK(&lk);
		VOP_UNLOCK(vp, p);
		error = VFS_VGET(mnt, parentino, &pvp);
		vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, p);
		if (error != 0)
			return (error);
		/*
		 * All MKDIR_PARENT dependencies and all the NEWBLOCK pagedeps
		 * that are contained in direct blocks will be resolved by 
		 * doing a UFS_UPDATE. Pagedeps contained in indirect blocks
		 * may require a complete sync'ing of the directory. So, we
		 * try the cheap and fast UFS_UPDATE first, and if that fails,
		 * then we do the slower VOP_FSYNC of the directory.
		 */
		pip = VTOI(pvp);
		if (flushparent) {
			error = UFS_UPDATE(pip, 1);
			if (error) {
				vput(pvp);
				return (error);
			}
			if (pagedep->pd_state & NEWBLOCK) {
				error = VOP_FSYNC(pvp, p->p_ucred, MNT_WAIT, p);
				if (error) {
					vput(pvp);
					return (error);
				}
			}
		}
		/*
		 * Flush directory page containing the inode's name.
		 */
		error = bread(pvp, lbn, fs->fs_bsize, &bp);
		if (error == 0) {
			bp->b_bcount = blksize(fs, pip, lbn);
			error = bwrite(bp);
		} else
			brelse(bp);
		vput(pvp);
		if (error != 0)
			return (error);
		ACQUIRE_LOCK(&lk);
		if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) == 0)
			break;
	}
	FREE_LOCK(&lk);
	return (0);
}

/*
 * Flush all the dirty bitmaps associated with the block device
 * before flushing the rest of the dirty blocks so as to reduce
 * the number of dependencies that will have to be rolled back.
 */
void
softdep_fsync_mountdev(struct vnode *vp, int waitfor)
{
	struct buf *bp, *nbp;
	struct worklist *wk;

	if (!vn_isdisk(vp, NULL))
		panic("softdep_fsync_mountdev: vnode not a disk");
	ACQUIRE_LOCK(&lk);
	for (bp = LIST_FIRST(&vp->v_dirtyblkhd); bp; bp = nbp) {
		nbp = LIST_NEXT(bp, b_vnbufs);
		/* 
		 * If it is already scheduled, skip to the next buffer.
		 */
		splassert(IPL_BIO);
		if (bp->b_flags & B_BUSY)
			continue;

		if ((bp->b_flags & B_DELWRI) == 0) {
			FREE_LOCK(&lk);
			panic("softdep_fsync_mountdev: not dirty");
		}
		/*
		 * We are only interested in bitmaps with outstanding
		 * dependencies.
		 */
		if ((wk = LIST_FIRST(&bp->b_dep)) == NULL ||
		    wk->wk_type != D_BMSAFEMAP) {
			continue;
		}
		bremfree(bp);
		buf_acquire(bp);
		FREE_LOCK(&lk);
		(void) bawrite(bp);
		ACQUIRE_LOCK(&lk);
		/*
		 * Since we may have slept during the I/O, we need 
		 * to start from a known point.
		 */
		nbp = LIST_FIRST(&vp->v_dirtyblkhd);
	}
	if (waitfor == MNT_WAIT)
		drain_output(vp, 1);
	FREE_LOCK(&lk);
}

/*
 * This routine is called when we are trying to synchronously flush a
 * file. This routine must eliminate any filesystem metadata dependencies
 * so that the syncing routine can succeed by pushing the dirty blocks
 * associated with the file. If any I/O errors occur, they are returned.
 */
int
softdep_sync_metadata(struct vop_fsync_args *ap)
{
	struct vnode *vp = ap->a_vp;
	struct pagedep *pagedep;
	struct allocdirect *adp;
	struct allocindir *aip;
	struct buf *bp, *nbp;
	struct worklist *wk;
	int i, gotit, error, waitfor;

	/*
	 * Check whether this vnode is involved in a filesystem
	 * that is doing soft dependency processing.
	 */
	if (!vn_isdisk(vp, NULL)) {
		if (!DOINGSOFTDEP(vp))
			return (0);
	} else
		if (vp->v_specmountpoint == NULL ||
		    (vp->v_specmountpoint->mnt_flag & MNT_SOFTDEP) == 0)
			return (0);
	/*
	 * Ensure that any direct block dependencies have been cleared.
	 */
	ACQUIRE_LOCK(&lk);
	if ((error = flush_inodedep_deps(VTOI(vp)->i_fs, VTOI(vp)->i_number))) {
		FREE_LOCK(&lk);
		return (error);
	}
	/*
	 * For most files, the only metadata dependencies are the
	 * cylinder group maps that allocate their inode or blocks.
	 * The block allocation dependencies can be found by traversing
	 * the dependency lists for any buffers that remain on their
	 * dirty buffer list. The inode allocation dependency will
	 * be resolved when the inode is updated with MNT_WAIT.
	 * This work is done in two passes. The first pass grabs most
	 * of the buffers and begins asynchronously writing them. The
	 * only way to wait for these asynchronous writes is to sleep
	 * on the filesystem vnode which may stay busy for a long time
	 * if the filesystem is active. So, instead, we make a second
	 * pass over the dependencies blocking on each write. In the
	 * usual case we will be blocking against a write that we
	 * initiated, so when it is done the dependency will have been
	 * resolved. Thus the second pass is expected to end quickly.
	 */
	waitfor = MNT_NOWAIT;
top:
	/*
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 */
	drain_output(vp, 1);
	bp = LIST_FIRST(&vp->v_dirtyblkhd);
	gotit = getdirtybuf(bp, MNT_WAIT);
	if (gotit == 0) {
		FREE_LOCK(&lk);
		return (0);
	} else if (gotit == -1)
		goto top;
loop:
	/*
	 * As we hold the buffer locked, none of its dependencies
	 * will disappear.
	 */
	LIST_FOREACH(wk, &bp->b_dep, wk_list) {
		switch (wk->wk_type) {

		case D_ALLOCDIRECT:
			adp = WK_ALLOCDIRECT(wk);
			if (adp->ad_state & DEPCOMPLETE)
				break;
			nbp = adp->ad_buf;
			gotit = getdirtybuf(nbp, waitfor);
			if (gotit == 0)
				break;
			else if (gotit == -1)
				goto loop;
			FREE_LOCK(&lk);
			if (waitfor == MNT_NOWAIT) {
				bawrite(nbp);
			} else if ((error = VOP_BWRITE(nbp)) != 0) {
				bawrite(bp);
				return (error);
			}
			ACQUIRE_LOCK(&lk);
			break;

		case D_ALLOCINDIR:
			aip = WK_ALLOCINDIR(wk);
			if (aip->ai_state & DEPCOMPLETE)
				break;
			nbp = aip->ai_buf;
			gotit = getdirtybuf(nbp, waitfor);
			if (gotit == 0)
				break;
			else if (gotit == -1)
				goto loop;
			FREE_LOCK(&lk);
			if (waitfor == MNT_NOWAIT) {
				bawrite(nbp);
			} else if ((error = VOP_BWRITE(nbp)) != 0) {
				bawrite(bp);
				return (error);
			}
			ACQUIRE_LOCK(&lk);
			break;

		case D_INDIRDEP:
		restart:

			LIST_FOREACH(aip, &WK_INDIRDEP(wk)->ir_deplisthd, ai_next) {
				if (aip->ai_state & DEPCOMPLETE)
					continue;
				nbp = aip->ai_buf;
				if (getdirtybuf(nbp, MNT_WAIT) <= 0)
					goto restart;
				FREE_LOCK(&lk);
				if ((error = VOP_BWRITE(nbp)) != 0) {
					bawrite(bp);
					return (error);
				}
				ACQUIRE_LOCK(&lk);
				goto restart;
			}
			break;

		case D_INODEDEP:
			if ((error = flush_inodedep_deps(WK_INODEDEP(wk)->id_fs,
			    WK_INODEDEP(wk)->id_ino)) != 0) {
				FREE_LOCK(&lk);
				bawrite(bp);
				return (error);
			}
			break;

		case D_PAGEDEP:
			/*
			 * We are trying to sync a directory that may
			 * have dependencies on both its own metadata
			 * and/or dependencies on the inodes of any
			 * recently allocated files. We walk its diradd
			 * lists pushing out the associated inode.
			 */
			pagedep = WK_PAGEDEP(wk);
			for (i = 0; i < DAHASHSZ; i++) {
				if (LIST_FIRST(&pagedep->pd_diraddhd[i]) ==
				    NULL)
					continue;
				if ((error =
				    flush_pagedep_deps(vp, pagedep->pd_mnt,
						&pagedep->pd_diraddhd[i]))) {
					FREE_LOCK(&lk);
					bawrite(bp);
					return (error);
				}
			}
			break;

		case D_MKDIR:
			/*
			 * This case should never happen if the vnode has
			 * been properly sync'ed. However, if this function
			 * is used at a place where the vnode has not yet
			 * been sync'ed, this dependency can show up. So,
			 * rather than panic, just flush it.
			 */
			nbp = WK_MKDIR(wk)->md_buf;
			gotit = getdirtybuf(nbp, waitfor);
			if (gotit == 0)
				break;
			else if (gotit == -1)
				goto loop;
			FREE_LOCK(&lk);
			if (waitfor == MNT_NOWAIT) {
				bawrite(nbp);
			} else if ((error = VOP_BWRITE(nbp)) != 0) {
				bawrite(bp);
				return (error);
			}
			ACQUIRE_LOCK(&lk);
			break;

		case D_BMSAFEMAP:
			/*
			 * This case should never happen if the vnode has
			 * been properly sync'ed. However, if this function
			 * is used at a place where the vnode has not yet
			 * been sync'ed, this dependency can show up. So,
			 * rather than panic, just flush it.
			 */
			nbp = WK_BMSAFEMAP(wk)->sm_buf;
			gotit = getdirtybuf(nbp, waitfor);
			if (gotit == 0)
				break;
			else if (gotit == -1)
				goto loop;
			FREE_LOCK(&lk);
			if (waitfor == MNT_NOWAIT) {
				bawrite(nbp);
			} else if ((error = VOP_BWRITE(nbp)) != 0) {
				bawrite(bp);
				return (error);
			}
			ACQUIRE_LOCK(&lk);
			break;

		default:
			FREE_LOCK(&lk);
			panic("softdep_sync_metadata: Unknown type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}
	do {
		nbp = LIST_NEXT(bp, b_vnbufs);
		gotit = getdirtybuf(nbp, MNT_WAIT);
	} while (gotit == -1);
	FREE_LOCK(&lk);
	bawrite(bp);
	ACQUIRE_LOCK(&lk);
	if (nbp != NULL) {
		bp = nbp;
		goto loop;
	}
	/*
	 * The brief unlock is to allow any pent up dependency
	 * processing to be done. Then proceed with the second pass.
	 */
	if (waitfor == MNT_NOWAIT) {
		waitfor = MNT_WAIT;
		FREE_LOCK(&lk);
		ACQUIRE_LOCK(&lk);
		goto top;
	}

	/*
	 * If we have managed to get rid of all the dirty buffers,
	 * then we are done. For certain directories and block
	 * devices, we may need to do further work.
	 *
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 */
	drain_output(vp, 1);
	if (LIST_FIRST(&vp->v_dirtyblkhd) == NULL) {
		FREE_LOCK(&lk);
		return (0);
	}

	FREE_LOCK(&lk);
	/*
	 * If we are trying to sync a block device, some of its buffers may
	 * contain metadata that cannot be written until the contents of some
	 * partially written files have been written to disk. The only easy
	 * way to accomplish this is to sync the entire filesystem (luckily
	 * this happens rarely).
	 */
	if (vn_isdisk(vp, NULL) &&
	    vp->v_specmountpoint && !VOP_ISLOCKED(vp) &&
	    (error = VFS_SYNC(vp->v_specmountpoint, MNT_WAIT, ap->a_cred,
	     ap->a_p)) != 0)
		return (error);
	return (0);
}

/*
 * Flush the dependencies associated with an inodedep.
 * Called with splbio blocked.
 */
STATIC int
flush_inodedep_deps(struct fs *fs, ufsino_t ino)
{
	struct inodedep *inodedep;
	struct allocdirect *adp;
	int gotit, error, waitfor;
	struct buf *bp;

	splassert(IPL_BIO);

	/*
	 * This work is done in two passes. The first pass grabs most
	 * of the buffers and begins asynchronously writing them. The
	 * only way to wait for these asynchronous writes is to sleep
	 * on the filesystem vnode which may stay busy for a long time
	 * if the filesystem is active. So, instead, we make a second
	 * pass over the dependencies blocking on each write. In the
	 * usual case we will be blocking against a write that we
	 * initiated, so when it is done the dependency will have been
	 * resolved. Thus the second pass is expected to end quickly.
	 * We give a brief window at the top of the loop to allow
	 * any pending I/O to complete.
	 */
	for (waitfor = MNT_NOWAIT; ; ) {
	retry_ino:
		FREE_LOCK(&lk);
		ACQUIRE_LOCK(&lk);
		if (inodedep_lookup(fs, ino, 0, &inodedep) == 0)
			return (0);
		TAILQ_FOREACH(adp, &inodedep->id_inoupdt, ad_next) {
			if (adp->ad_state & DEPCOMPLETE)
				continue;
			bp = adp->ad_buf;
			gotit = getdirtybuf(bp, waitfor);
			if (gotit == 0) {
				if (waitfor == MNT_NOWAIT)
					continue;
				break;
			} else if (gotit == -1)
				goto retry_ino;
			FREE_LOCK(&lk);
			if (waitfor == MNT_NOWAIT) {
				bawrite(bp);
			} else if ((error = VOP_BWRITE(bp)) != 0) {
				ACQUIRE_LOCK(&lk);
				return (error);
			}
			ACQUIRE_LOCK(&lk);
			break;
		}
		if (adp != NULL)
			continue;
	retry_newino:
		TAILQ_FOREACH(adp, &inodedep->id_newinoupdt, ad_next) {
			if (adp->ad_state & DEPCOMPLETE)
				continue;
			bp = adp->ad_buf;
			gotit = getdirtybuf(bp, waitfor);
			if (gotit == 0) {
				if (waitfor == MNT_NOWAIT)
					continue;
				break;
			} else if (gotit == -1)
				goto retry_newino;
			FREE_LOCK(&lk);
			if (waitfor == MNT_NOWAIT) {
				bawrite(bp);
			} else if ((error = VOP_BWRITE(bp)) != 0) {
				ACQUIRE_LOCK(&lk);
				return (error);
			}
			ACQUIRE_LOCK(&lk);
			break;
		}
		if (adp != NULL)
			continue;
		/*
		 * If pass2, we are done, otherwise do pass 2.
		 */
		if (waitfor == MNT_WAIT)
			break;
		waitfor = MNT_WAIT;
	}
	/*
	 * Try freeing inodedep in case all dependencies have been removed.
	 */
	if (inodedep_lookup(fs, ino, 0, &inodedep) != 0)
		(void) free_inodedep(inodedep);
	return (0);
}

/*
 * Eliminate a pagedep dependency by flushing out all its diradd dependencies.
 * Called with splbio blocked.
 */
STATIC int
flush_pagedep_deps(struct vnode *pvp, struct mount *mp,
    struct diraddhd *diraddhdp)
{
	struct proc *p = CURPROC;	/* XXX */
	struct worklist *wk;
	struct inodedep *inodedep;
	struct ufsmount *ump;
	struct diradd *dap;
	struct vnode *vp;
	int gotit, error = 0;
	struct buf *bp;
	ufsino_t inum;

	splassert(IPL_BIO);

	ump = VFSTOUFS(mp);
	while ((dap = LIST_FIRST(diraddhdp)) != NULL) {
		/*
		 * Flush ourselves if this directory entry
		 * has a MKDIR_PARENT dependency.
		 */
		if (dap->da_state & MKDIR_PARENT) {
			FREE_LOCK(&lk);
			if ((error = UFS_UPDATE(VTOI(pvp), 1)))
				break;
			ACQUIRE_LOCK(&lk);
			/*
			 * If that cleared dependencies, go on to next.
			 */
			if (dap != LIST_FIRST(diraddhdp))
				continue;
			if (dap->da_state & MKDIR_PARENT) {
				FREE_LOCK(&lk);
				panic("flush_pagedep_deps: MKDIR_PARENT");
			}
		}
		/*
		 * A newly allocated directory must have its "." and
		 * ".." entries written out before its name can be
		 * committed in its parent. We do not want or need
		 * the full semantics of a synchronous VOP_FSYNC as
		 * that may end up here again, once for each directory
		 * level in the filesystem. Instead, we push the blocks
		 * and wait for them to clear. We have to fsync twice
		 * because the first call may choose to defer blocks
		 * that still have dependencies, but deferral will
		 * happen at most once.
		 */
		inum = dap->da_newinum;
		if (dap->da_state & MKDIR_BODY) {
			FREE_LOCK(&lk);
			if ((error = VFS_VGET(mp, inum, &vp)) != 0)
				break;
			if ((error=VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p)) ||
			    (error=VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p))) {
				vput(vp);
				break;
			}
			drain_output(vp, 0);
			/*
			 * If first block is still dirty with a D_MKDIR
			 * dependency then it needs to be written now.
			 */
			for (;;) {
				error = 0;
				ACQUIRE_LOCK(&lk);
				bp = incore(vp, 0);
				if (bp == NULL) {
					FREE_LOCK(&lk);
					break;
				}
				LIST_FOREACH(wk, &bp->b_dep, wk_list)
					if (wk->wk_type == D_MKDIR)
						break;
				if (wk) {
					gotit = getdirtybuf(bp, MNT_WAIT);
					FREE_LOCK(&lk);
					if (gotit == -1)
						continue;
					if (gotit && (error = bwrite(bp)) != 0)
						break;
				} else
					FREE_LOCK(&lk);
				break;
			}
			vput(vp);
			/* Flushing of first block failed */
			if (error)
				break;
			ACQUIRE_LOCK(&lk);
			/*
			 * If that cleared dependencies, go on to next.
			 */
			if (dap != LIST_FIRST(diraddhdp))
				continue;
			if (dap->da_state & MKDIR_BODY) {
				FREE_LOCK(&lk);
				panic("flush_pagedep_deps: MKDIR_BODY");
			}
		}
		/*
		 * Flush the inode on which the directory entry depends.
		 * Having accounted for MKDIR_PARENT and MKDIR_BODY above,
		 * the only remaining dependency is that the updated inode
		 * count must get pushed to disk. The inode has already
		 * been pushed into its inode buffer (via VOP_UPDATE) at
		 * the time of the reference count change. So we need only
		 * locate that buffer, ensure that there will be no rollback
		 * caused by a bitmap dependency, then write the inode buffer.
		 */
		if (inodedep_lookup(ump->um_fs, inum, 0, &inodedep) == 0) {
			FREE_LOCK(&lk);
			panic("flush_pagedep_deps: lost inode");
		}
		/*
		 * If the inode still has bitmap dependencies,
		 * push them to disk.
		 */
	retry:
		if ((inodedep->id_state & DEPCOMPLETE) == 0) {
			bp = inodedep->id_buf;
			gotit = getdirtybuf(bp, MNT_WAIT);
			if (gotit == -1)
				goto retry;
			FREE_LOCK(&lk);
			if (gotit && (error = bwrite(bp)) != 0)
				break;
			ACQUIRE_LOCK(&lk);
			if (dap != LIST_FIRST(diraddhdp))
				continue;
		}
		/*
		 * If the inode is still sitting in a buffer waiting
		 * to be written, push it to disk.
		 */
		FREE_LOCK(&lk);
		if ((error = bread(ump->um_devvp,
		    fsbtodb(ump->um_fs, ino_to_fsba(ump->um_fs, inum)),
		    (int)ump->um_fs->fs_bsize, &bp)) != 0) {
		    	brelse(bp);
			break;
		}
		if ((error = bwrite(bp)) != 0)
			break;
		ACQUIRE_LOCK(&lk);
		/*
		 * If we have failed to get rid of all the dependencies
		 * then something is seriously wrong.
		 */
		if (dap == LIST_FIRST(diraddhdp)) {
			FREE_LOCK(&lk);
			panic("flush_pagedep_deps: flush failed");
		}
	}
	if (error)
		ACQUIRE_LOCK(&lk);
	return (error);
}

/*
 * A large burst of file addition or deletion activity can drive the
 * memory load excessively high. First attempt to slow things down
 * using the techniques below. If that fails, this routine requests
 * the offending operations to fall back to running synchronously
 * until the memory load returns to a reasonable level.
 */
int
softdep_slowdown(struct vnode *vp)
{
	int max_softdeps_hard;

	max_softdeps_hard = max_softdeps * 11 / 10;
	if (num_dirrem < max_softdeps_hard / 2 &&
	    num_inodedep < max_softdeps_hard)
		return (0);
	stat_sync_limit_hit += 1;
	return (1);
}

/*
 * If memory utilization has gotten too high, deliberately slow things
 * down and speed up the I/O processing.
 */
STATIC int
request_cleanup(int resource, int islocked)
{
	struct proc *p = CURPROC;
	int s;

	/*
	 * We never hold up the filesystem syncer process.
	 */
	if (p == filesys_syncer || (p->p_flag & P_SOFTDEP))
		return (0);
	/*
	 * First check to see if the work list has gotten backlogged.
	 * If it has, co-opt this process to help clean up two entries.
	 * Because this process may hold inodes locked, we cannot
	 * handle any remove requests that might block on a locked
	 * inode as that could lead to deadlock. We set P_SOFTDEP
	 * to avoid recursively processing the worklist.
	 */
	if (num_on_worklist > max_softdeps / 10) {
		atomic_setbits_int(&p->p_flag, P_SOFTDEP);
		if (islocked)
			FREE_LOCK(&lk);
		process_worklist_item(NULL, LK_NOWAIT);
		process_worklist_item(NULL, LK_NOWAIT);
		atomic_clearbits_int(&p->p_flag, P_SOFTDEP);
		stat_worklist_push += 2;
		if (islocked)
			ACQUIRE_LOCK(&lk);
		return(1);
	}
	/*
	 * Next, we attempt to speed up the syncer process. If that
	 * is successful, then we allow the process to continue.
	 */
	if (speedup_syncer())
		return(0);
	/*
	 * If we are resource constrained on inode dependencies, try
	 * flushing some dirty inodes. Otherwise, we are constrained
	 * by file deletions, so try accelerating flushes of directories
	 * with removal dependencies. We would like to do the cleanup
	 * here, but we probably hold an inode locked at this point and 
	 * that might deadlock against one that we try to clean. So,
	 * the best that we can do is request the syncer daemon to do
	 * the cleanup for us.
	 */
	switch (resource) {

	case FLUSH_INODES:
		stat_ino_limit_push += 1;
		req_clear_inodedeps += 1;
		stat_countp = &stat_ino_limit_hit;
		break;

	case FLUSH_REMOVE:
		stat_blk_limit_push += 1;
		req_clear_remove += 1;
		stat_countp = &stat_blk_limit_hit;
		break;

	default:
		if (islocked)
			FREE_LOCK(&lk);
		panic("request_cleanup: unknown type");
	}
	/*
	 * Hopefully the syncer daemon will catch up and awaken us.
	 * We wait at most tickdelay before proceeding in any case.
	 */
	if (islocked == 0)
		ACQUIRE_LOCK(&lk);
	proc_waiting += 1;
	if (!timeout_pending(&proc_waiting_timeout))
		timeout_add(&proc_waiting_timeout, tickdelay > 2 ? tickdelay : 2);

	s = FREE_LOCK_INTERLOCKED(&lk);
	(void) tsleep((caddr_t)&proc_waiting, PPAUSE, "softupdate", 0);
	ACQUIRE_LOCK_INTERLOCKED(&lk, s);
	proc_waiting -= 1;
	if (islocked == 0)
		FREE_LOCK(&lk);
	return (1);
}

/*
 * Awaken processes pausing in request_cleanup and clear proc_waiting
 * to indicate that there is no longer a timer running.
 */
void
pause_timer(void *arg)
{

	*stat_countp += 1;
	wakeup_one(&proc_waiting);
	if (proc_waiting > 0)
		timeout_add(&proc_waiting_timeout, tickdelay > 2 ? tickdelay : 2);
}

/*
 * Flush out a directory with at least one removal dependency in an effort to
 * reduce the number of dirrem, freefile, and freeblks dependency structures.
 */
STATIC void
clear_remove(struct proc *p)
{
	struct pagedep_hashhead *pagedephd;
	struct pagedep *pagedep;
	static int next = 0;
	struct mount *mp;
	struct vnode *vp;
	int error, cnt;
	ufsino_t ino;

	ACQUIRE_LOCK(&lk);
	for (cnt = 0; cnt <= pagedep_hash; cnt++) {
		pagedephd = &pagedep_hashtbl[next++];
		if (next > pagedep_hash)
			next = 0;
		LIST_FOREACH(pagedep, pagedephd, pd_hash) {
			if (LIST_FIRST(&pagedep->pd_dirremhd) == NULL)
				continue;
			mp = pagedep->pd_mnt;
			ino = pagedep->pd_ino;
#if 0
			if (vn_start_write(NULL, &mp, V_NOWAIT) != 0)
				continue;
#endif
			FREE_LOCK(&lk);
			if ((error = VFS_VGET(mp, ino, &vp)) != 0) {
				softdep_error("clear_remove: vget", error);
#if 0
				vn_finished_write(mp);
#endif
				return;
			}
			if ((error = VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p)))
				softdep_error("clear_remove: fsync", error);
			drain_output(vp, 0);
			vput(vp);
#if 0
			vn_finished_write(mp);
#endif
			return;
		}
	}
	FREE_LOCK(&lk);
}

/*
 * Clear out a block of dirty inodes in an effort to reduce
 * the number of inodedep dependency structures.
 */
STATIC void
clear_inodedeps(struct proc *p)
{
	struct inodedep_hashhead *inodedephd;
	struct inodedep *inodedep = NULL;
	static int next = 0;
	struct mount *mp;
	struct vnode *vp;
	struct fs *fs;
	int error, cnt;
	ufsino_t firstino, lastino, ino;

	ACQUIRE_LOCK(&lk);
	/*
	 * Pick a random inode dependency to be cleared.
	 * We will then gather up all the inodes in its block 
	 * that have dependencies and flush them out.
	 */
	for (cnt = 0; cnt <= inodedep_hash; cnt++) {
		inodedephd = &inodedep_hashtbl[next++];
		if (next > inodedep_hash)
			next = 0;
		if ((inodedep = LIST_FIRST(inodedephd)) != NULL)
			break;
	}
	if (inodedep == NULL) {
		FREE_LOCK(&lk);
		return;
	}
	/*
	 * Ugly code to find mount point given pointer to superblock.
	 */
	fs = inodedep->id_fs;
	TAILQ_FOREACH(mp, &mountlist, mnt_list)
		if ((mp->mnt_flag & MNT_SOFTDEP) && fs == VFSTOUFS(mp)->um_fs)
			break;
	/*
	 * Find the last inode in the block with dependencies.
	 */
	firstino = inodedep->id_ino & ~(INOPB(fs) - 1);
	for (lastino = firstino + INOPB(fs) - 1; lastino > firstino; lastino--)
		if (inodedep_lookup(fs, lastino, 0, &inodedep) != 0)
			break;
	/*
	 * Asynchronously push all but the last inode with dependencies.
	 * Synchronously push the last inode with dependencies to ensure
	 * that the inode block gets written to free up the inodedeps.
	 */
	for (ino = firstino; ino <= lastino; ino++) {
		if (inodedep_lookup(fs, ino, 0, &inodedep) == 0)
			continue;
		FREE_LOCK(&lk);
#if 0
		if (vn_start_write(NULL, &mp, V_NOWAIT) != 0)
			continue;
#endif
		if ((error = VFS_VGET(mp, ino, &vp)) != 0) {
			softdep_error("clear_inodedeps: vget", error);
#if 0
			vn_finished_write(mp);
#endif
			return;
		}
		if (ino == lastino) {
			if ((error = VOP_FSYNC(vp, p->p_ucred, MNT_WAIT, p)))
				softdep_error("clear_inodedeps: fsync1", error);
		} else {
			if ((error = VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p)))
				softdep_error("clear_inodedeps: fsync2", error);
			drain_output(vp, 0);
		}
		vput(vp);
#if 0
		vn_finished_write(mp);
#endif
		ACQUIRE_LOCK(&lk);
	}
	FREE_LOCK(&lk);
}

/*
 * Function to determine if the buffer has outstanding dependencies
 * that will cause a roll-back if the buffer is written. If wantcount
 * is set, return number of dependencies, otherwise just yes or no.
 */
int
softdep_count_dependencies(struct buf *bp, int wantcount, int islocked)
{
	struct worklist *wk;
	struct inodedep *inodedep;
	struct indirdep *indirdep;
	struct allocindir *aip;
	struct pagedep *pagedep;
	struct diradd *dap;
	int i, retval;

	retval = 0;
	if (!islocked)
		ACQUIRE_LOCK(&lk);
	LIST_FOREACH(wk, &bp->b_dep, wk_list) {
		switch (wk->wk_type) {

		case D_INODEDEP:
			inodedep = WK_INODEDEP(wk);
			if ((inodedep->id_state & DEPCOMPLETE) == 0) {
				/* bitmap allocation dependency */
				retval += 1;
				if (!wantcount)
					goto out;
			}
			if (TAILQ_FIRST(&inodedep->id_inoupdt)) {
				/* direct block pointer dependency */
				retval += 1;
				if (!wantcount)
					goto out;
			}
			continue;

		case D_INDIRDEP:
			indirdep = WK_INDIRDEP(wk);

			LIST_FOREACH(aip, &indirdep->ir_deplisthd, ai_next) {
				/* indirect block pointer dependency */
				retval += 1;
				if (!wantcount)
					goto out;
			}
			continue;

		case D_PAGEDEP:
			pagedep = WK_PAGEDEP(wk);
			for (i = 0; i < DAHASHSZ; i++) {

				LIST_FOREACH(dap, &pagedep->pd_diraddhd[i], da_pdlist) {
					/* directory entry dependency */
					retval += 1;
					if (!wantcount)
						goto out;
				}
			}
			continue;

		case D_BMSAFEMAP:
		case D_ALLOCDIRECT:
		case D_ALLOCINDIR:
		case D_MKDIR:
			/* never a dependency on these blocks */
			continue;

		default:
			if (!islocked)
				FREE_LOCK(&lk);
			panic("softdep_check_for_rollback: Unexpected type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}
out:
	if (!islocked)
		FREE_LOCK(&lk);
	return retval;
}

/*
 * Acquire exclusive access to a buffer.
 * Must be called with splbio blocked.
 * Returns:
 * 1 if the buffer was acquired and is dirty;
 * 0 if the buffer was clean, or we would have slept but had MN_NOWAIT;
 * -1 if we slept and may try again (but not with this bp).
 */
STATIC int
getdirtybuf(struct buf *bp, int waitfor)
{
	int s;

	if (bp == NULL)
		return (0);

	splassert(IPL_BIO);

	if (bp->b_flags & B_BUSY) {
		if (waitfor != MNT_WAIT)
			return (0);
		bp->b_flags |= B_WANTED;
		s = FREE_LOCK_INTERLOCKED(&lk);
		tsleep((caddr_t)bp, PRIBIO + 1, "sdsdty", 0);
		ACQUIRE_LOCK_INTERLOCKED(&lk, s);
		return (-1);
	}
	if ((bp->b_flags & B_DELWRI) == 0)
		return (0);
	bremfree(bp);
	buf_acquire(bp);
	return (1);
}

/*
 * Wait for pending output on a vnode to complete.
 * Must be called with vnode locked.
 */
STATIC void
drain_output(struct vnode *vp, int islocked)
{
	int s;

	if (!islocked)
		ACQUIRE_LOCK(&lk);

	splassert(IPL_BIO);

	while (vp->v_numoutput) {
		vp->v_bioflag |= VBIOWAIT;
		s = FREE_LOCK_INTERLOCKED(&lk);
		tsleep((caddr_t)&vp->v_numoutput, PRIBIO + 1, "drain_output", 0);
		ACQUIRE_LOCK_INTERLOCKED(&lk, s);
	}
	if (!islocked)
		FREE_LOCK(&lk);
}

/*
 * Called whenever a buffer that is being invalidated or reallocated
 * contains dependencies. This should only happen if an I/O error has
 * occurred. The routine is called with the buffer locked.
 */ 
void
softdep_deallocate_dependencies(struct buf *bp)
{

	if ((bp->b_flags & B_ERROR) == 0)
		panic("softdep_deallocate_dependencies: dangling deps");
	softdep_error(bp->b_vp->v_mount->mnt_stat.f_mntonname, bp->b_error);
	panic("softdep_deallocate_dependencies: unrecovered I/O error");
}

/*
 * Function to handle asynchronous write errors in the filesystem.
 */
void
softdep_error(char *func, int error)
{

	/* XXX should do something better! */
	printf("%s: got error %d while accessing filesystem\n", func, error);
}

#ifdef DDB
#include <machine/db_machdep.h>
#include <ddb/db_interface.h>
#include <ddb/db_output.h>

void
softdep_print(struct buf *bp, int full,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct worklist *wk;

	(*pr)("  deps:\n");
	LIST_FOREACH(wk, &bp->b_dep, wk_list)
		worklist_print(wk, full, pr);
}

void
worklist_print(struct worklist *wk, int full,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct pagedep *pagedep;
	struct inodedep *inodedep;
	struct newblk *newblk;
	struct bmsafemap *bmsafemap;
	struct allocdirect *adp;
	struct indirdep *indirdep;
	struct allocindir *aip;
	struct freefrag *freefrag;
	struct freeblks *freeblks;
	struct freefile *freefile;
	struct diradd *dap;
	struct mkdir *mkdir;
	struct dirrem *dirrem;
	struct newdirblk *newdirblk;
	char prefix[33];
	int i;

	for (prefix[i = 2 * MIN(16, full)] = '\0'; i--; prefix[i] = ' ')
		;

	(*pr)("%s%s(%p) state %b\n%s", prefix, TYPENAME(wk->wk_type), wk,
	    wk->wk_state, DEP_BITS, prefix);
	switch (wk->wk_type) {
	case D_PAGEDEP:
		pagedep = WK_PAGEDEP(wk);
		(*pr)("mount %p ino %u lbn %lld\n", pagedep->pd_mnt,
		    pagedep->pd_ino, (long long)pagedep->pd_lbn);
		break;
	case D_INODEDEP:
		inodedep = WK_INODEDEP(wk);
		(*pr)("fs %p ino %u nlinkdelta %u dino %p\n"
		    "%s  bp %p savsz %lld\n", inodedep->id_fs,
		    inodedep->id_ino, inodedep->id_nlinkdelta,
		    inodedep->id_un.idu_savedino1,
		    prefix, inodedep->id_buf, inodedep->id_savedsize);
		break;
	case D_NEWBLK:
		newblk = WK_NEWBLK(wk);
		(*pr)("fs %p newblk %lld state %d bmsafemap %p\n",
		    newblk->nb_fs, (long long)newblk->nb_newblkno,
		    newblk->nb_state, newblk->nb_bmsafemap);
		break;
	case D_BMSAFEMAP:
		bmsafemap = WK_BMSAFEMAP(wk);
		(*pr)("buf %p\n", bmsafemap->sm_buf);
		break;
	case D_ALLOCDIRECT:
		adp = WK_ALLOCDIRECT(wk);
		(*pr)("lbn %lld newlbk %lld oldblk %lld newsize %ld olsize "
		    "%ld\n%s  bp %p inodedep %p freefrag %p\n",
		    (long long)adp->ad_lbn, (long long)adp->ad_newblkno,
		    (long long)adp->ad_oldblkno, adp->ad_newsize,
		    adp->ad_oldsize,
		    prefix, adp->ad_buf, adp->ad_inodedep, adp->ad_freefrag);
		break;
	case D_INDIRDEP:
		indirdep = WK_INDIRDEP(wk);
		(*pr)("savedata %p savebp %p\n", indirdep->ir_saveddata,
		    indirdep->ir_savebp);
		break;
	case D_ALLOCINDIR:
		aip = WK_ALLOCINDIR(wk);
		(*pr)("off %d newblk %lld oldblk %lld freefrag %p\n"
		    "%s  indirdep %p buf %p\n", aip->ai_offset,
		    (long long)aip->ai_newblkno, (long long)aip->ai_oldblkno,
		    aip->ai_freefrag, prefix, aip->ai_indirdep, aip->ai_buf);
		break;
	case D_FREEFRAG:
		freefrag = WK_FREEFRAG(wk);
		(*pr)("vnode %p mp %p blkno %lld fsize %ld ino %u\n",
		    freefrag->ff_devvp, freefrag->ff_mnt,
		    (long long)freefrag->ff_blkno, freefrag->ff_fragsize,
		    freefrag->ff_inum);
		break;
	case D_FREEBLKS:
		freeblks = WK_FREEBLKS(wk);
		(*pr)("previno %u devvp %p mp %p oldsz %lld newsz %lld\n"
		    "%s  chkcnt %d uid %d\n", freeblks->fb_previousinum,
		    freeblks->fb_devvp, freeblks->fb_mnt, freeblks->fb_oldsize,
		    freeblks->fb_newsize,
		    prefix, freeblks->fb_chkcnt, freeblks->fb_uid);
		break;
	case D_FREEFILE:
		freefile = WK_FREEFILE(wk);
		(*pr)("mode %x oldino %u vnode %p mp %p\n", freefile->fx_mode,
		    freefile->fx_oldinum, freefile->fx_devvp, freefile->fx_mnt);
		break;
	case D_DIRADD:
		dap = WK_DIRADD(wk);
		(*pr)("off %d ino %u da_un %p\n", dap->da_offset, 
		    dap->da_newinum, dap->da_un.dau_previous);
		break;
	case D_MKDIR:
		mkdir = WK_MKDIR(wk);
		(*pr)("diradd %p bp %p\n", mkdir->md_diradd, mkdir->md_buf);
		break;
	case D_DIRREM:
		dirrem = WK_DIRREM(wk);
		(*pr)("mp %p ino %u dm_un %p\n", dirrem->dm_mnt, 
		    dirrem->dm_oldinum, dirrem->dm_un.dmu_pagedep);
		break;
	case D_NEWDIRBLK:
		newdirblk = WK_NEWDIRBLK(wk);
		(*pr)("pagedep %p\n", newdirblk->db_pagedep);
		break;
	}
}
#endif
@


1.134
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.133 2016/06/19 10:21:56 dlg Exp $	*/
d230 1
a230 1
		if (holder == CURPROC->p_pid)
d236 1
a236 1
	lk->lkt_held = CURPROC->p_pid;
d261 1
a261 1
		if (holder == CURPROC->p_pid)
d266 1
a266 1
	lk->lkt_held = CURPROC->p_pid;
d324 1
a324 1
	semap->holder = CURPROC->p_pid;
d334 1
a334 1
	if (semap->value <= 0 || semap->holder != CURPROC->p_pid) {
@


1.133
log
@add pool_setipl on all pools.

ok tedu@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.132 2016/03/19 12:04:16 natano Exp $	*/
d1178 28
a1205 42
	pool_init(&pagedep_pool, sizeof(struct pagedep), 0, 0, PR_WAITOK,
	    "pagedep", NULL);
	pool_setipl(&pagedep_pool, IPL_NONE);
	pool_init(&inodedep_pool, sizeof(struct inodedep), 0, 0, PR_WAITOK,
	    "inodedep", NULL);
	pool_setipl(&inodedep_pool, IPL_NONE);
	pool_init(&newblk_pool, sizeof(struct newblk), 0, 0, PR_WAITOK,
	    "newblk", NULL);
	pool_setipl(&newblk_pool, IPL_NONE);
	pool_init(&bmsafemap_pool, sizeof(struct bmsafemap), 0, 0, PR_WAITOK,
	    "bmsafemap", NULL);
	pool_setipl(&bmsafemap_pool, IPL_NONE);
	pool_init(&allocdirect_pool, sizeof(struct allocdirect), 0, 0, PR_WAITOK,
	    "allocdir", NULL);
	pool_setipl(&allocdirect_pool, IPL_NONE);
	pool_init(&indirdep_pool, sizeof(struct indirdep), 0, 0, PR_WAITOK,
	    "indirdep", NULL);
	pool_setipl(&indirdep_pool, IPL_NONE);
	pool_init(&allocindir_pool, sizeof(struct allocindir), 0, 0, PR_WAITOK,
	    "allocindir", NULL);
	pool_setipl(&allocindir_pool, IPL_NONE);
	pool_init(&freefrag_pool, sizeof(struct freefrag), 0, 0, PR_WAITOK,
	    "freefrag", NULL);
	pool_setipl(&freefrag_pool, IPL_NONE);
	pool_init(&freeblks_pool, sizeof(struct freeblks), 0, 0, PR_WAITOK,
	    "freeblks", NULL);
	pool_setipl(&freeblks_pool, IPL_NONE);
	pool_init(&freefile_pool, sizeof(struct freefile), 0, 0, PR_WAITOK,
	    "freefile", NULL);
	pool_setipl(&freefile_pool, IPL_NONE);
	pool_init(&diradd_pool, sizeof(struct diradd), 0, 0, PR_WAITOK,
	    "diradd", NULL);
	pool_setipl(&diradd_pool, IPL_NONE);
	pool_init(&mkdir_pool, sizeof(struct mkdir), 0, 0, PR_WAITOK,
	    "mkdir", NULL);
	pool_setipl(&mkdir_pool, IPL_NONE);
	pool_init(&dirrem_pool, sizeof(struct dirrem), 0, 0, PR_WAITOK,
	    "dirrem", NULL);
	pool_setipl(&dirrem_pool, IPL_NONE);
	pool_init(&newdirblk_pool, sizeof(struct newdirblk), 0, 0, PR_WAITOK,
	    "newdirblk", NULL);
	pool_setipl(&newdirblk_pool, IPL_NONE);
@


1.132
log
@Remove the unused flags argument from VOP_UNLOCK().

torture tested on amd64, i386 and macppc
ok beck mpi stefan
"the change looks right" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.131 2015/01/09 05:01:57 tedu Exp $	*/
d1180 1
d1183 1
d1186 1
d1189 1
d1192 1
d1195 1
d1198 1
d1201 1
d1204 1
d1207 1
d1210 1
d1213 1
d1216 1
d1219 1
@


1.131
log
@rename desiredvnodes to initialvnodes. less of a lie. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.130 2014/12/23 01:53:34 tedu Exp $	*/
d871 1
a871 1
		VOP_UNLOCK(devvp, 0, p);
d4571 1
a4571 1
		VOP_UNLOCK(vp, 0, p);
@


1.130
log
@change pool_init allocator to NULL and pass PR_WAITOK in flags as a sign
that these don't need to support interrupts
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.129 2014/11/18 10:42:15 dlg Exp $	*/
d1163 1
a1163 1
	max_softdeps = min (desiredvnodes * 8,
d1166 1
a1166 1
	max_softdeps = desiredvnodes * 4;
d1169 1
a1169 1
	pagedep_hashtbl = hashinit(desiredvnodes / 5, M_PAGEDEP, M_WAITOK,
d1172 1
a1172 1
	inodedep_hashtbl = hashinit(desiredvnodes, M_INODEDEP, M_WAITOK,
@


1.129
log
@use siphash for key lookups in all the filesystem hashes.

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.128 2014/07/12 18:44:01 tedu Exp $	*/
d1178 28
a1205 28
	pool_init(&pagedep_pool, sizeof(struct pagedep), 0, 0, 0,
	    "pagedep", &pool_allocator_nointr);
	pool_init(&inodedep_pool, sizeof(struct inodedep), 0, 0, 0,
	    "inodedep", &pool_allocator_nointr);
	pool_init(&newblk_pool, sizeof(struct newblk), 0, 0, 0,
	    "newblk", &pool_allocator_nointr);
	pool_init(&bmsafemap_pool, sizeof(struct bmsafemap), 0, 0, 0,
	    "bmsafemap", &pool_allocator_nointr);
	pool_init(&allocdirect_pool, sizeof(struct allocdirect), 0, 0, 0,
	    "allocdir", &pool_allocator_nointr);
	pool_init(&indirdep_pool, sizeof(struct indirdep), 0, 0, 0,
	    "indirdep", &pool_allocator_nointr);
	pool_init(&allocindir_pool, sizeof(struct allocindir), 0, 0, 0,
	    "allocindir", &pool_allocator_nointr);
	pool_init(&freefrag_pool, sizeof(struct freefrag), 0, 0, 0,
	    "freefrag", &pool_allocator_nointr);
	pool_init(&freeblks_pool, sizeof(struct freeblks), 0, 0, 0,
	    "freeblks", &pool_allocator_nointr);
	pool_init(&freefile_pool, sizeof(struct freefile), 0, 0, 0,
	    "freefile", &pool_allocator_nointr);
	pool_init(&diradd_pool, sizeof(struct diradd), 0, 0, 0,
	    "diradd", &pool_allocator_nointr);
	pool_init(&mkdir_pool, sizeof(struct mkdir), 0, 0, 0,
	    "mkdir", &pool_allocator_nointr);
	pool_init(&dirrem_pool, sizeof(struct dirrem), 0, 0, 0,
	    "dirrem", &pool_allocator_nointr);
	pool_init(&newdirblk_pool, sizeof(struct newdirblk), 0, 0, 0,
	    "newdirblk", &pool_allocator_nointr);
@


1.128
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.127 2014/05/22 02:02:39 guenther Exp $	*/
d55 1
d942 2
a948 3
#define	PAGEDEP_HASH(mp, inum, lbn) \
	(&pagedep_hashtbl[((((register_t)(mp)) >> 13) + (inum) + (lbn)) & \
	    pagedep_hash])
d962 1
d975 6
a980 1
	pagedephd = PAGEDEP_HASH(mp, ip->i_number, lbn);
a1023 2
#define	INODEDEP_HASH(fs, inum) \
      (&inodedep_hashtbl[((((register_t)(fs)) >> 13) + (inum)) & inodedep_hash])
d1036 1
d1048 4
a1051 1
	inodedephd = INODEDEP_HASH(fs, inum);
a1102 2
#define	NEWBLK_HASH(fs, inum) \
	(&newblk_hashtbl[((((register_t)(fs)) >> 13) + (inum)) & newblk_hash])
d1114 1
d1118 4
a1121 1
	newblkhd = NEWBLK_HASH(fs, newblkno);
d1168 1
@


1.127
log
@From FreeBSD: the second argument to UFS_UPDATE/ffs_update is just a
boolean, not a MNT_* flag.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.126 2014/04/22 20:14:39 beck Exp $	*/
d2291 1
a2291 1
		free(inodedep->id_savedino1, M_INODEDEP);
d3829 1
a3829 1
			free(indirdep->ir_saveddata, M_INDIRDEP);
d4018 1
a4018 1
		free(inodedep->id_savedino1, M_INODEDEP);
@


1.126
log
@Fix issue where we could jump into getdirtybuf without splbio() on a retry
that probably crashed espie.
ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.125 2014/02/04 01:04:03 tedu Exp $	*/
d4575 1
a4575 1
			error = UFS_UPDATE(pip, MNT_WAIT);
d5055 1
a5055 1
			if ((error = UFS_UPDATE(VTOI(pvp), MNT_WAIT)))
@


1.125
log
@reduce the length of some pool names. ok deraadt guenther mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.124 2013/12/12 19:00:10 tedu Exp $	*/
a5153 1
			FREE_LOCK(&lk);
d5156 1
@


1.124
log
@replace old bcopy/bzero with standard functions. ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.123 2013/12/01 16:40:56 krw Exp $	*/
d1168 1
a1168 1
	    "pagedeppl", &pool_allocator_nointr);
d1170 1
a1170 1
	    "inodedeppl", &pool_allocator_nointr);
d1172 1
a1172 1
	    "newblkpl", &pool_allocator_nointr);
d1174 1
a1174 1
	    "bmsafemappl", &pool_allocator_nointr);
d1176 1
a1176 1
	    "allocdirectpl", &pool_allocator_nointr);
d1178 1
a1178 1
	    "indirdeppl", &pool_allocator_nointr);
d1180 1
a1180 1
	    "allocindirpl", &pool_allocator_nointr);
d1182 1
a1182 1
	    "freefragpl", &pool_allocator_nointr);
d1184 1
a1184 1
	    "freeblkspl", &pool_allocator_nointr);
d1186 1
a1186 1
	    "freefilepl", &pool_allocator_nointr);
d1188 1
a1188 1
	    "diraddpl", &pool_allocator_nointr);
d1190 1
a1190 1
	    "mkdirpl", &pool_allocator_nointr);
d1192 1
a1192 1
	    "dirrempl", &pool_allocator_nointr);
d1194 1
a1194 1
	    "newdirblkpl", &pool_allocator_nointr);
@


1.123
log
@Change 'mountlist' from CIRCLEQ to TAILQ. Be paranoid and
use TAILQ_*_SAFE more than might be needed.

Bulk ports build by sthen@@ showed nobody sticking their fingers
so deep into the kernel.

Feedback and suggestions from millert@@. ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.122 2013/11/03 02:22:07 krw Exp $	*/
d1217 1
a1217 1
	bzero(&cstotal, sizeof cstotal);
d1233 1
a1233 1
	if (bcmp(&cstotal, &fs->fs_cstotal, sizeof cstotal))
d1236 1
a1236 1
	bcopy(&cstotal, &fs->fs_cstotal, sizeof cstotal);
d1838 1
a1838 1
		bcopy(bp->b_data, newindirdep->ir_savebp->b_data, bp->b_bcount);
d2055 1
a2055 1
			bcopy(bp->b_data, indirdep->ir_savebp->b_data,
d2729 1
a2729 1
	bcopy(oldloc, newloc, entrysize);
d3299 2
a3300 2
			bcopy(bp->b_data, indirdep->ir_saveddata, bp->b_bcount);
			bcopy(indirdep->ir_savebp->b_data, bp->b_data,
d3405 1
a3405 1
		bzero((caddr_t)dp, sizeof(struct ufs1_dinode));
d3546 1
a3546 1
		bzero((caddr_t)dp, sizeof(struct ufs2_dinode));
d3828 1
a3828 1
			bcopy(indirdep->ir_saveddata, bp->b_data, bp->b_bcount);
@


1.122
log
@Add missing (long long) cast of variable, (long long) casts
for defines fragnum, fsbtodb, cgsblock, and cgdmin.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.121 2013/11/01 17:36:19 krw Exp $	*/
d5401 1
a5401 1
	CIRCLEQ_FOREACH(mp, &mountlist, mnt_list)
@


1.121
log
@Sprinkle (long long) casts where %lld is being used to print daddr_t
variables. Some random whitespace/knf repairs encountered on the way.

ok miod@@ on inspection, feedback & more suggestions from millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.120 2013/08/09 08:20:05 syl Exp $	*/
d4038 2
a4039 1
					 panic("%s: %s #%lld mismatch %d != %lld",
d4044 1
a4044 1
					     adp->ad_oldblkno);
@


1.120
log
@Uncomment another kprintf attribute.

ok miod@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.119 2013/06/11 16:42:18 deraadt Exp $	*/
d1531 3
a1533 2
		panic("allocdirect_merge: old %lld != new %lld || lbn %lld >= %d",
		    newadp->ad_oldblkno, oldadp->ad_newblkno, newadp->ad_lbn,
d3429 2
a3430 1
			    "softdep_write_inodeblock", adp->ad_lbn, d1, d2);
d3437 2
a3438 2
			    "softdep_write_inodeblock", adp->ad_lbn - NDADDR,
			    d1, d2);
d3576 2
a3577 1
			    "softdep_write_inodeblock", adp->ad_lbn, d1, d2);
d3644 2
a3645 1
			    "softdep_write_inodeblock", adp->ad_lbn, d1, d2);
d3652 2
a3653 2
			    "softdep_write_inodeblock", adp->ad_lbn - NDADDR,
			    d1, d2);
d4040 2
a4041 1
					     "direct pointer", adp->ad_lbn,
d4050 1
a4050 1
					    adp->ad_lbn - NDADDR,
d4058 4
a4061 3
					panic("%s: %s #%lld mismatch %lld != %lld",
					    "handle_written_inodeblock",
					    "direct pointer", adp->ad_lbn,
d4063 1
a4063 1
					    adp->ad_oldblkno);
d4070 1
a4070 1
					    adp->ad_lbn - NDADDR,
d5660 1
a5660 1
		    pagedep->pd_ino, pagedep->pd_lbn);
d5673 2
a5674 2
		    newblk->nb_fs, newblk->nb_newblkno, newblk->nb_state,
		    newblk->nb_bmsafemap);
d5682 4
a5685 3
		(*pr)("lbn %lld newlbk %lld oldblk %lld newsize %ld olsize %ld\n"
		    "%s  bp %p inodedep %p freefrag %p\n", adp->ad_lbn,
		    adp->ad_newblkno, adp->ad_oldblkno, adp->ad_newsize,
d5698 2
a5699 2
		    aip->ai_newblkno, aip->ai_oldblkno, aip->ai_freefrag,
		    prefix, aip->ai_indirdep, aip->ai_buf);
d5704 3
a5706 2
		    freefrag->ff_devvp, freefrag->ff_mnt, freefrag->ff_blkno,
		    freefrag->ff_fragsize, freefrag->ff_inum);
@


1.119
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.118 2013/05/30 19:19:09 guenther Exp $	*/
d5615 1
a5615 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
d5626 1
a5626 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
@


1.118
log
@UFS does't do inode numbers >2^32, so use a smaller type internally,
only using ino_t in the VFS layer APIs: vget, readdir, getattr.

otto wrote the original diff for libsa to keep bootblock from overflowing
ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.117 2013/04/04 17:29:36 beck Exp $	*/
d142 1
a142 1
STATIC	int indir_trunc(struct inode *, daddr64_t, int, daddr64_t, long *);
d152 2
a153 2
STATIC	struct allocindir *newallocindir(struct inode *, int, daddr64_t,
	    daddr64_t);
d155 1
a155 1
STATIC	struct freefrag *newfreefrag(struct inode *, daddr64_t, long);
d159 1
a159 1
STATIC	int newblk_lookup(struct fs *, daddr64_t, int,
d162 1
a162 1
STATIC	int pagedep_lookup(struct inode *, daddr64_t, int, struct pagedep **);
d959 1
a959 1
pagedep_lookup(struct inode *ip, daddr64_t lbn, int flags,
d1105 1
a1105 1
newblk_lookup(struct fs *fs, daddr64_t newblkno, int flags,
d1311 1
a1311 1
softdep_setup_blkmapdep(struct buf *bp, struct fs *fs, daddr64_t newblkno)
d1401 2
a1402 2
softdep_setup_allocdirect(struct inode *ip, daddr64_t lbn, daddr64_t newblkno,
    daddr64_t oldblkno, long newsize, long oldsize, struct buf *bp)
d1576 1
a1576 1
newfreefrag(struct inode *ip, daddr64_t blkno, long size)
d1651 2
a1652 2
newallocindir(struct inode *ip, int ptrno, daddr64_t newblkno,
    daddr64_t oldblkno)
d1678 2
a1679 2
softdep_setup_allocindir_page(struct inode *ip, daddr64_t lbn, struct buf *bp,
    int ptrno, daddr64_t newblkno, daddr64_t oldblkno, struct buf *nbp)
d1716 1
a1716 1
    struct buf *bp, int ptrno, daddr64_t newblkno)
d2334 1
a2334 1
	daddr64_t bn;
d2343 1
a2343 1
	daddr64_t baselbns[NIADDR], tmpval;
d2404 1
a2404 1
indir_trunc(struct inode *ip, daddr64_t dbn, int level, daddr64_t lbn,
d2549 1
a2549 1
	daddr64_t lbn;		/* block in directory containing new entry */
d2697 1
a2697 1
	daddr64_t lbn;
d2853 1
a2853 1
	daddr64_t lbn;
d3377 1
a3377 1
	daddr64_t prevlbn = 0;
d3524 1
a3524 1
	daddr64_t prevlbn = -1, d1, d2;
d4490 1
a4490 1
	daddr64_t lbn;
@


1.117
log
@Fix bug where clear_remove() and clear_inodedeps() would not iterate
over the entire pagedep and inodedep hash tables due to an off-by-one
mistake in loops.  Spotted by and diff from Pedro Martelletto. Sent
upstream to Kirk and also fixed in FreeBSD.
ok otto@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.116 2013/02/17 17:39:29 miod Exp $	*/
d123 1
a123 1
STATIC	int flush_inodedep_deps(struct fs *, ino_t);
d161 1
a161 1
STATIC	int inodedep_lookup(struct fs *, ino_t, int, struct inodedep **);
d1029 1
a1029 1
inodedep_lookup(struct fs *fs, ino_t inum, int flags,
d1279 1
a1279 1
softdep_setup_inomapdep(struct buf *bp, struct inode *ip, ino_t newinum)
d2222 1
a2222 1
softdep_freefile(struct vnode *pvp, ino_t ino, mode_t mode)
d3080 1
a3080 1
	ino_t oldinum;
d4489 1
a4489 1
	ino_t parentino;
d4930 1
a4930 1
flush_inodedep_deps(struct fs *fs, ino_t ino)
d5036 1
a5036 1
	ino_t inum;
d5320 1
a5320 1
	ino_t ino;
d5371 1
a5371 1
	ino_t firstino, lastino, ino;
@


1.116
log
@Comment out recently added __attribute__((__format__(__kprintf__))) annotations
in MI code; gcc 2.95 does not accept such annotation for function pointer
declarations, only function prototypes.
To be uncommented once gcc 2.95 bites the dust.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.115 2013/02/09 20:56:35 miod Exp $	*/
d5323 1
a5323 1
	for (cnt = 0; cnt < pagedep_hash; cnt++) {
d5325 1
a5325 1
		if (next >= pagedep_hash)
d5379 1
a5379 1
	for (cnt = 0; cnt < inodedep_hash; cnt++) {
d5381 1
a5381 1
		if (next >= inodedep_hash)
@


1.115
log
@Add explicit __attribute__ ((__format__(__kprintf__)))) to the functions and
function pointer arguments which are {used as,} wrappers around the kernel
printf function.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.114 2013/01/16 22:41:47 beck Exp $	*/
d5615 1
a5615 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
d5626 1
a5626 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
@


1.114
log
@re-backout the infamous softdep diff - originally committed to get around
the problem of the cleaner not being able to get softdep out of trouble
due to the removal of a reserve back in vienna.  Been run by me for a long
time and beat up, along with a bunch of others. Hopefully this nasty piece
of workaround can stay out for good now.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.113 2012/12/10 22:58:04 beck Exp $	*/
d5614 2
a5615 1
softdep_print(struct buf *bp, int full, int (*pr)(const char *, ...))
d5625 2
a5626 1
worklist_print(struct worklist *wk, int full, int (*pr)(const char *, ...))
@


1.113
log
@Fix potential for use-after-free of bufs in softdep.

Softdep uses it's own "getdirtybuf" which has (always) potentially
slept - this wasn't noticable when buffers were static and never freed,
however now that the bufs it was trying to busy is might have been
written out and freed by someone else we can see use-after free if we
fast-recycle buffers. We fix this by either re-trying operations
or ensuring we don't use the buf pointer after waiting.

ok guenther@@ (in coimbra)
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.112 2011/09/18 23:20:28 bluhm Exp $	*/
a528 2
STATIC int num_indirdep;	/* number of indirdep items to be processed */
STATIC int max_indirdep;	/* maximum number of indirdep items allowed */
a1142 2
	extern vsize_t bufkvm;
	max_indirdep = (int)bufkvm / MAXPHYS * 80 / 100;
a1817 1
			num_indirdep--;
a1823 1
		num_indirdep++;
a2442 1
		num_indirdep--;
a3283 1
				num_indirdep--;
@


1.112
log
@Fix more printf format string bugs in sys/ufs.
ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.111 2011/09/18 11:18:28 miod Exp $	*/
d1976 1
a1976 1
		if (!getdirtybuf(bp, MNT_WAIT))
d4429 8
a4436 6
	if ((inodedep->id_state & DEPCOMPLETE) != 0 || waitfor == 0) {
		FREE_LOCK(&lk);
		return;
	}
	bp = inodedep->id_buf;
	gotit = getdirtybuf(bp, MNT_WAIT);
d4675 1
a4675 1
	int i, error, waitfor;
d4721 2
a4722 1
	if (getdirtybuf(bp, MNT_WAIT) == 0) {
d4725 2
a4726 1
	}
d4740 2
a4741 1
			if (getdirtybuf(nbp, waitfor) == 0)
d4743 2
d4760 2
a4761 1
			if (getdirtybuf(nbp, waitfor) == 0)
d4763 2
d4782 1
a4782 1
				if (getdirtybuf(nbp, MNT_WAIT) == 0)
d4835 2
a4836 1
			if (getdirtybuf(nbp, waitfor) == 0)
d4838 2
d4859 2
a4860 1
			if (getdirtybuf(nbp, waitfor) == 0)
d4862 2
d4881 4
a4884 2
	nbp = LIST_NEXT(bp, b_vnbufs);
	getdirtybuf(nbp, MNT_WAIT);
d4942 1
a4942 1
	int error, waitfor;
d4961 1
d4970 2
a4971 1
			if (getdirtybuf(bp, waitfor) == 0) {
d4975 2
a4976 1
			}
d4989 1
d4994 2
a4995 1
			if (getdirtybuf(bp, waitfor) == 0) {
d4999 2
a5000 1
			}
d5110 2
d5151 1
d5156 2
d5535 4
a5538 1
 * Return 1 if buffer was acquired.
d5550 1
a5550 3
	for (;;) {
		if ((bp->b_flags & B_BUSY) == 0)
			break;
d5557 1
@


1.111
log
@Make sure daddr64_t values get printed with %lld.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.110 2011/08/17 15:48:22 thib Exp $	*/
d2918 1
a2918 1
		panic("newdirrem: inum %d should be %d",
d3357 1
a3357 1
				panic("%s: dir inum %d != new %d",
d3581 1
a3581 1
			panic("%s: direct pointer #%ld mismatch %ld != %ld",
d5651 1
a5651 1
		(*pr)("lbn %lld newlbk %d oldblk %d newsize %lu olsize %lu\n"
d5664 1
a5664 1
		(*pr)("off %d newblk %d oldblk %d freefrag %p\n"
d5690 1
a5690 1
		(*pr)("off %ld ino %u da_un %p\n", dap->da_offset, 
@


1.110
log
@remove the clamping of max_softdeps as it makes
softdeps almost totally unusable;
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.109 2011/08/16 14:36:39 thib Exp $	*/
d1535 1
a1535 1
		panic("allocdirect_merge: old %d != new %d || lbn %ld >= %d",
d3435 1
a3435 1
			panic("%s: direct pointer #%ld mismatch %d != %d",
d3442 1
a3442 1
			panic("%s: indirect pointer #%ld mismatch %d != %d",
d3648 1
a3648 1
			panic("%s: direct pointer #%ld mismatch %ld != %ld",
d3655 1
a3655 1
			panic("%s: indirect pointer #%ld mismatch %ld != %ld",
d4042 1
a4042 1
					 panic("%s: %s #%ld mismatch %d != %d",
d4050 1
a4050 1
					panic("%s: %s #%ld allocated as %d",
d4061 1
a4061 1
					panic("%s: %s #%ld mismatch %d != %d",
d4069 1
a4069 1
					panic("%s: %s #%ld allocated as %d",
@


1.109
log
@Put back the pedro diff for VOP_FSYNC of softdep when its
limit is reached.

Added twist, clamp the max_softdeps to 64 as it is still
possible to exhaust bufkva.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.108 2011/08/03 20:21:19 beck Exp $	*/
a1161 1
	max_softdeps = 64;
@


1.108
log
@back out pedro diff for VOP_FSYNC of softdep when limit is reached.

The basic analysys is correct, however, the problem in this case is that by forcing
softdept to synchornously flush everything across *all* softdep filesystems we cause a
huge performance problem when we take a 3 second pause and slam everything synchronously.

the right way to fix this is to fix the speedup_softdep code, not make the filesystem
go synchronous when we hit a limit - if we are doing that we may as well not run softdep
it will be faster.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.107 2011/07/04 20:35:35 deraadt Exp $	*/
d529 2
d1145 2
d1162 1
d1823 1
d1830 1
d2450 1
d3292 1
@


1.107
log
@move the specfs code to a place people can see it; ok guenther thib krw
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.106 2011/07/04 04:30:41 tedu Exp $	*/
a528 2
STATIC int num_indirdep;	/* number of indirdep items to be processed */
STATIC int max_indirdep;	/* maximum number of indirdep items allowed */
a1142 2
	extern vsize_t bufkvm;
	max_indirdep = (int)bufkvm / MAXPHYS * 80 / 100;
a1817 1
			num_indirdep--;
a1823 1
		num_indirdep++;
a2442 1
		num_indirdep--;
a3283 1
				num_indirdep--;
@


1.106
log
@bread does nothing with its ucred argument.  remove it.  ok matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.105 2011/07/03 18:23:10 tedu Exp $	*/
d54 1
a54 1
#include <miscfs/specfs/specdev.h>
@


1.105
log
@correctify NULL 0 confusion
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.104 2011/06/29 12:15:26 tedu Exp $	*/
d1224 1
a1224 1
		    fs->fs_cgsize, cred, &bp)) != 0) {
d1923 1
a1923 1
	    (int)fs->fs_bsize, NOCRED, &bp)) != 0)
d2457 1
a2457 1
		error = bread(ip->i_devvp, dbn, (int)fs->fs_bsize, NOCRED, &bp);
d4590 1
a4590 1
		error = bread(pvp, lbn, fs->fs_bsize, p->p_ucred, &bp);
d5142 1
a5142 1
		    (int)ump->um_fs->fs_bsize, NOCRED, &bp)) != 0) {
@


1.104
log
@ansi softdep, ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.103 2011/04/12 19:45:43 beck Exp $	*/
d737 1
a737 1
	if (wk == 0) {
d829 1
a829 1
	wktail = 0;
d833 1
a833 1
		if (wktail == 0)
d1126 1
a1126 1
	if (sema_get(&newblk_in_progress, 0) == 0)
d1170 1
a1170 1
	timeout_set(&proc_waiting_timeout, pause_timer, 0);
d1962 1
a1962 1
	while ((adp = TAILQ_FIRST(&inodedep->id_inoupdt)) != 0)
d2053 1
a2053 1
			while ((aip = LIST_FIRST(&indirdep->ir_deplisthd)) != 0)
d2076 1
a2076 1
			while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != 0)
d3834 1
a3834 1
			indirdep->ir_saveddata = 0;
d3837 1
a3837 1
			while ((aip = LIST_FIRST(&indirdep->ir_donehd)) != 0) {
d4176 2
a4177 1
	if (free_inodedep(inodedep) || TAILQ_FIRST(&inodedep->id_inoupdt) == 0)
d4803 2
a4804 1
				if (LIST_FIRST(&pagedep->pd_diraddhd[i]) == 0)
@


1.103
log
@Avoid kvm starvation due to softdeps waiting on too many mapped buffers,
thus possibly consuming all of our available kva mapping buffers for
deps. Diff and analysis actually comes from Pedro Martelleto (thanks!)
tested by me and thib

ok thib@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.102 2010/03/29 23:33:39 krw Exp $	*/
d220 1
a220 3
acquire_lock(lk, line)
	struct lockit *lk;
	int line;
d241 1
a241 3
free_lock(lk, line)
	struct lockit *lk;
	int line;
d251 1
a251 4
acquire_lock_interlocked(lk, s, line)
	struct lockit *lk;
	int s;
	int line;
d272 1
a272 3
free_lock_interlocked(lk, line)
	struct lockit *lk;
	int line;
d298 1
a298 4
sema_init(semap, name, prio, timo)
	struct sema *semap;
	char *name;
	int prio, timo;
d309 1
a309 3
sema_get(semap, interlock)
	struct sema *semap;
	struct lockit *interlock;
d330 1
a330 2
sema_release(semap)
	struct sema *semap;
d483 1
a483 3
worklist_insert(head, item)
	struct workhead *head;
	struct worklist *item;
d497 1
a497 2
worklist_remove(item)
	struct worklist *item;
d511 1
a511 2
workitem_free(item)
	struct worklist *item;
d565 1
a565 2
add_to_worklist(wk)
	struct worklist *wk;
d594 1
a594 2
softdep_process_worklist(matchmnt)
	struct mount *matchmnt;
d713 1
a713 3
process_worklist_item(matchmnt, flags)
	struct mount *matchmnt;
	int flags;
d823 1
a823 3
softdep_move_dependencies(oldbp, newbp)
	struct buf *oldbp;
	struct buf *newbp;
d846 1
a846 4
softdep_flushworklist(oldmnt, countp, p)
	struct mount *oldmnt;
	int *countp;
	struct proc *p;
d886 1
a886 4
softdep_flushfiles(oldmnt, flags, p)
	struct mount *oldmnt;
	int flags;
	struct proc *p;
d961 2
a962 5
pagedep_lookup(ip, lbn, flags, pagedeppp)
	struct inode *ip;
	daddr64_t lbn;
	int flags;
	struct pagedep **pagedeppp;
d1031 2
a1032 5
inodedep_lookup(fs, inum, flags, inodedeppp)
	struct fs *fs;
	ino_t inum;
	int flags;
	struct inodedep **inodedeppp;
d1107 2
a1108 5
newblk_lookup(fs, newblkno, flags, newblkpp)
	struct fs *fs;
	daddr64_t newblkno;
	int flags;
	struct newblk **newblkpp;
d1143 1
a1143 1
softdep_initialize()
d1206 2
a1207 5
softdep_mount(devvp, mp, fs, cred)
	struct vnode *devvp;
	struct mount *mp;
	struct fs *fs;
	struct ucred *cred;
d1279 3
d1283 1
a1283 4
softdep_setup_inomapdep(bp, ip, newinum)
	struct buf *bp;		/* buffer for cylgroup block with inode map */
	struct inode *ip;	/* inode related to allocation */
	ino_t newinum;		/* new inode number being allocated */
d1311 3
d1315 1
a1315 4
softdep_setup_blkmapdep(bp, fs, newblkno)
	struct buf *bp;		/* buffer for cylgroup block with block map */
	struct fs *fs;		/* filesystem doing allocation */
	daddr64_t newblkno;	/* number of newly allocated block */
d1340 1
a1340 2
bmsafemap_lookup(bp)
	struct buf *bp;
d1397 7
d1405 2
a1406 8
softdep_setup_allocdirect(ip, lbn, newblkno, oldblkno, newsize, oldsize, bp)
	struct inode *ip;	/* inode to which block is being added */
	daddr64_t lbn;		/* block pointer within inode */
	daddr64_t newblkno;	/* disk block number being added */
	daddr64_t oldblkno;	/* previous block number, 0 unless frag */
	long newsize;		/* size of new block */
	long oldsize;		/* size of new block */
	struct buf *bp;		/* bp for allocated block */
d1514 3
d1518 2
a1519 4
allocdirect_merge(adphead, newadp, oldadp)
	struct allocdirectlst *adphead;	/* head of list holding allocdirects */
	struct allocdirect *newadp;	/* allocdirect being added */
	struct allocdirect *oldadp;	/* existing allocdirect being checked */
d1580 1
a1580 4
newfreefrag(ip, blkno, size)
	struct inode *ip;
	daddr64_t blkno;
	long size;
d1606 1
a1606 2
handle_workitem_freefrag(freefrag)
	struct freefrag *freefrag;
d1650 4
d1655 2
a1656 5
newallocindir(ip, ptrno, newblkno, oldblkno)
	struct inode *ip;	/* inode for file being extended */
	int ptrno;		/* offset of pointer in indirect block */
	daddr64_t newblkno;	/* disk block number being added */
	daddr64_t oldblkno;	/* previous block number, 0 if none */
d1674 7
d1682 2
a1683 8
softdep_setup_allocindir_page(ip, lbn, bp, ptrno, newblkno, oldblkno, nbp)
	struct inode *ip;	/* inode for file being extended */
	daddr64_t lbn;		/* allocated block number within file */
	struct buf *bp;		/* buffer with indirect blk referencing page */
	int ptrno;		/* offset of pointer in indirect block */
	daddr64_t newblkno;	/* disk block number being added */
	daddr64_t oldblkno;	/* previous block number, 0 if none */
	struct buf *nbp;	/* buffer holding allocated page */
d1713 5
d1719 2
a1720 6
softdep_setup_allocindir_meta(nbp, ip, bp, ptrno, newblkno)
	struct buf *nbp;	/* newly allocated indirect block */
	struct inode *ip;	/* inode for file being extended */
	struct buf *bp;		/* indirect block referencing allocated block */
	int ptrno;		/* offset of pointer in indirect block */
	daddr64_t newblkno;	/* disk block number being added */
d1735 3
d1739 2
a1740 4
setup_allocindir_phase2(bp, ip, aip)
	struct buf *bp;		/* in-memory copy of the indirect block */
	struct inode *ip;	/* inode for file being extended */
	struct allocindir *aip;	/* allocindir allocated by the above routines */
d1876 2
d1879 1
a1879 3
softdep_setup_freeblocks(ip, length)
	struct inode *ip;	/* The inode whose length is to be reduced */
	off_t length;		/* The new length for the file */
d2019 1
a2019 3
deallocate_dependencies(bp, inodedep)
	struct buf *bp;
	struct inodedep *inodedep;
d2140 2
a2141 4
free_allocdirect(adphead, adp, delay)
	struct allocdirectlst *adphead;
	struct allocdirect *adp;
	int delay;
d2183 1
a2183 2
free_newdirblk(newdirblk)
	struct newdirblk *newdirblk;
d2228 1
a2228 4
softdep_freefile(pvp, ino, mode)
		struct vnode *pvp;
		ino_t ino;
		mode_t mode;
d2278 1
a2278 2
check_inode_unwritten(inodedep)
	struct inodedep *inodedep;
d2310 1
a2310 2
free_inodedep(inodedep)
	struct inodedep *inodedep;
d2337 1
a2337 2
handle_workitem_freeblocks(freeblks)
	struct freeblks *freeblks;
d2410 2
a2411 6
indir_trunc(ip, dbn, level, lbn, countp)
	struct inode *ip;
	daddr64_t dbn;
	int level;
	daddr64_t lbn;
	long *countp;
d2497 1
a2497 3
free_allocindir(aip, inodedep)
	struct allocindir *aip;
	struct inodedep *inodedep;
d2545 6
d2552 2
a2553 7
softdep_setup_directory_add(bp, dp, diroffset, newinum, newdirbp, isnewblk)
	struct buf *bp;		/* buffer containing directory block */
	struct inode *dp;	/* inode for directory */
	off_t diroffset;	/* offset of new entry in directory */
	long newinum;		/* inode referenced by new directory entry */
	struct buf *newdirbp;	/* non-NULL => contents of new mkdir */
	int isnewblk;		/* entry is in a newly allocated block */
d2692 5
d2698 2
a2699 6
softdep_change_directoryentry_offset(dp, base, oldloc, newloc, entrysize)
	struct inode *dp;	/* inode for directory */
	caddr_t base;		/* address of dp->i_offset */
	caddr_t oldloc;		/* address of old directory location */
	caddr_t newloc;		/* address of new directory location */
	int entrysize;		/* size of directory entry */
d2744 1
a2744 2
free_diradd(dap)
	struct diradd *dap;
d2805 4
d2810 2
a2811 5
softdep_setup_remove(bp, dp, ip, isrmdir)
	struct buf *bp;		/* buffer containing directory block */
	struct inode *dp;	/* inode for the directory being modified */
	struct inode *ip;	/* inode for directory entry being removed */
	int isrmdir;		/* indicates if doing RMDIR */
d2845 1
d2850 5
a2854 1
STATIC long num_dirrem;		/* number of dirrem allocated */
d2856 2
a2857 6
newdirrem(bp, dp, ip, isrmdir, prevdirremp)
	struct buf *bp;		/* buffer containing directory block */
	struct inode *dp;	/* inode for the directory being modified */
	struct inode *ip;	/* inode for directory entry being removed */
	int isrmdir;		/* indicates if doing RMDIR */
	struct dirrem **prevdirremp; /* previously referenced inode, if any */
d2957 5
d2963 2
a2964 6
softdep_setup_directory_change(bp, dp, ip, newinum, isrmdir)
	struct buf *bp;		/* buffer containing directory block */
	struct inode *dp;	/* inode for the directory being modified */
	struct inode *ip;	/* inode for directory entry being removed */
	long newinum;		/* new inode number for changed entry */
	int isrmdir;		/* indicates if doing RMDIR */
d3048 2
d3051 1
a3051 3
softdep_change_linkcnt(ip, nodelay)
	struct inode *ip;	/* the inode with the increased link count */
	int nodelay;		/* do background work or not */
d3081 1
a3081 2
handle_workitem_remove(dirrem)
	struct dirrem *dirrem;
d3183 1
a3183 2
handle_workitem_freefile(freefile)
	struct freefile *freefile;
d3236 1
d3238 1
a3238 2
softdep_disk_io_initiation(bp)
	struct buf *bp;		/* structure describing disk write to occur */
d3335 1
a3335 3
initiate_write_filepage(pagedep, bp)
	struct pagedep *pagedep;
	struct buf *bp;
d3377 1
d3379 1
a3379 3
initiate_write_inodeblock_ufs1(inodedep, bp)
	struct inodedep *inodedep;
	struct buf *bp;			/* The inode block */
d3524 1
d3526 1
a3526 3
initiate_write_inodeblock_ufs2(inodedep, bp)
	struct inodedep *inodedep;
	struct buf *bp;			/* The inode block */
d3742 1
d3744 1
a3744 2
softdep_disk_write_complete(bp)
	struct buf *bp;		/* describes the completed disk write */
d3873 1
d3875 1
a3875 2
handle_allocdirect_partdone(adp)
	struct allocdirect *adp;	/* the completed allocdirect */
d3948 1
d3950 1
a3950 2
handle_allocindir_partdone(aip)
	struct allocindir *aip;		/* the completed allocindir */
d3984 1
d3986 1
a3986 3
handle_written_inodeblock(inodedep, bp)
	struct inodedep *inodedep;
	struct buf *bp;		/* buffer containing the inode block */
d4186 1
a4186 3
diradd_inode_written(dap, inodedep)
	struct diradd *dap;
	struct inodedep *inodedep;
d4208 1
a4208 3
handle_written_mkdir(mkdir, type)
	struct mkdir *mkdir;
	int type;
d4240 1
d4242 1
a4242 3
handle_written_filepage(pagedep, bp)
	struct pagedep *pagedep;
	struct buf *bp;		/* buffer containing the written page */
d4338 1
d4340 1
a4340 2
softdep_load_inodeblock(ip)
	struct inode *ip;	/* the "in_core" copy of the inode */
d4367 3
d4371 1
a4371 4
softdep_update_inodeblock(ip, bp, waitfor)
	struct inode *ip;	/* the "in_core" copy of the inode */
	struct buf *bp;		/* the buffer containing the inode block */
	int waitfor;		/* nonzero => update must be allowed */
d4447 1
a4447 2
merge_inode_lists(inodedep)
	struct inodedep *inodedep;
d4478 1
d4480 1
a4480 2
softdep_fsync(vp)
	struct vnode *vp;	/* the "in_core" copy of the inode */
d4612 1
a4612 3
softdep_fsync_mountdev(vp, waitfor)
	struct vnode *vp;
	int waitfor;
d4664 1
a4664 7
softdep_sync_metadata(ap)
	struct vop_fsync_args /* {
		struct vnode *a_vp;
		struct ucred *a_cred;
		int a_waitfor;
		struct proc *a_p;
	} */ *ap;
d4918 1
a4918 3
flush_inodedep_deps(fs, ino)
	struct fs *fs;
	ino_t ino;
d5007 2
a5008 4
flush_pagedep_deps(pvp, mp, diraddhdp)
	struct vnode *pvp;
	struct mount *mp;
	struct diraddhd *diraddhdp;
d5169 1
a5169 2
softdep_slowdown(vp)
	struct vnode *vp;
d5186 1
a5186 3
request_cleanup(resource, islocked)
	int resource;
	int islocked;
d5275 1
a5275 2
pause_timer(arg)
	void *arg;
d5289 1
a5289 2
clear_remove(p)
	struct proc *p;
d5339 1
a5339 2
clear_inodedeps(p)
	struct proc *p;
d5424 1
a5424 4
softdep_count_dependencies(bp, wantcount, islocked)
	struct buf *bp;
	int wantcount;
	int islocked;
d5507 1
a5507 3
getdirtybuf(bp, waitfor)
	struct buf *bp;
	int waitfor;
d5538 1
a5538 3
drain_output(vp, islocked)
	struct vnode *vp;
	int islocked;
d5563 1
a5563 2
softdep_deallocate_dependencies(bp)
	struct buf *bp;
d5576 1
a5576 3
softdep_error(func, error)
	char *func;
	int error;
@


1.102
log
@Initialize various uninitialized variables. Found by jsg@@ via Clang.

Feedback from miod@@ and kettenis@@.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.101 2009/09/03 07:47:47 jasper Exp $	*/
d548 2
d1185 2
d1864 1
d1871 1
d2506 1
d3348 1
@


1.101
log
@- fix two more format strings wrt daddr64_t values

"sure" deraadt@@, ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.100 2009/06/25 15:49:26 thib Exp $	*/
d5420 1
a5420 1
	struct inodedep *inodedep;
@


1.100
log
@backout the buf_acquire() does the bremfree() since all callers
where doing bremfree() befure calling buf_acquire().

This is causing us headache pinning down a bug that showed up
when deraadt@@ too cvs to current, and will have to be done
anyway as a preperation for backouts.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.98 2008/06/14 10:55:21 mk Exp $	*/
d5727 1
a5727 1
		(*pr)("fs %p newblk %d state %d bmsafemap %p\n",
d5757 1
a5757 1
		(*pr)("vnode %p mp %p blkno %d fsize %ld ino %u\n",
@


1.99
log
@All caller of buf_acquire were doing bremfree before the call.
Just put it in the buf_acquire function.
oga@@ ok
@
text
@d4703 1
d5611 1
@


1.98
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.97 2008/06/12 06:58:40 deraadt Exp $	*/
a4702 1
		bremfree(bp);
a5609 1
	bremfree(bp);
@


1.97
log
@Bring biomem diff back into the tree after the nfs_bio.c fix went in.
ok thib beck art
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.95 2008/06/10 20:14:37 beck Exp $	*/
d1030 1
a1030 2
	pagedep = pool_get(&pagedep_pool, PR_WAITOK);
	bzero(pagedep, sizeof(struct pagedep));
d1454 1
a1454 2
	adp = pool_get(&allocdirect_pool, PR_WAITOK);
	bzero(adp, sizeof(struct allocdirect));
d1701 1
a1701 2
	aip = pool_get(&allocindir_pool, PR_WAITOK);
	bzero(aip,sizeof(struct allocindir));
d1928 1
a1928 2
	freeblks = pool_get(&freeblks_pool, PR_WAITOK);
	bzero(freeblks, sizeof(struct freeblks));
d2620 1
a2620 2
	dap = pool_get(&diradd_pool, PR_WAITOK);
	bzero(dap,sizeof(struct diradd));
d2927 1
a2927 2
	dirrem = pool_get(&dirrem_pool, PR_WAITOK);
	bzero(dirrem,sizeof(struct dirrem));
d3015 1
a3015 1
	struct diradd *dap = NULL;
d3021 1
a3021 2
	dap = pool_get(&diradd_pool, PR_WAITOK);
	bzero(dap,sizeof(struct diradd));
@


1.96
log
@back out biomem diff since it is not right yet.  Doing very large
file copies to nfsv2 causes the system to eventually peg the console.
On the console ^T indicates that the load is increasing rapidly, ddb
indicates many calls to getbuf, there is some very slow nfs traffic
making none (or extremely slow) progress.  Eventually some machines
seize up entirely.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.94 2008/01/05 19:49:26 otto Exp $	*/
d4694 1
a4696 1
		bp->b_flags |= B_BUSY;
a4707 1
			bp->b_flags &= ~B_BUSY;
d4711 1
d5619 1
a5619 1
	bp->b_flags |= B_BUSY;
@


1.95
log
@
Buffer cache revamp

1) remove multiple size queues, introduced as a stopgap.
2) decouple pages containing data from their mappings
3) only keep buffers mapped when they actually have to be mapped
  (right now, this is when buffers are B_BUSY)
4) New functions to make a buffer busy, and release the busy flag
   (buf_acquire and buf_release)
5) Move high/low water marks and statistics counters into a structure
6) Add a sysctl to retrieve buffer cache statistics

Tested in several variants and beat upon by bob and art for a year. run
accidentally on henning's nfs server for a few months...

ok deraadt@@, krw@@, art@@ - who promises to be around to deal with any fallout
@
text
@a4693 1
		splassert(IPL_BIO);
d4696 1
d4708 1
a4711 1
		buf_acquire(bp);
d5619 1
a5619 1
	buf_acquire(bp);
@


1.94
log
@Make the ffs code 64-bit disk block number clean.  Based on a diff
from Pedro Martelleto.  Two things remain: the on-disk quota
structures are still 32-bit and statfs does not do 64-bit numbers
yet. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.93 2007/10/29 17:06:20 chl Exp $	*/
d4694 1
a4696 1
		bp->b_flags |= B_BUSY;
a4707 1
			bp->b_flags &= ~B_BUSY;
d4711 1
d5619 1
a5619 1
	bp->b_flags |= B_BUSY;
@


1.93
log
@MALLOC/FREE -> malloc/free

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.92 2007/07/11 15:32:22 millert Exp $	*/
d142 1
a142 1
STATIC	int indir_trunc(struct inode *, daddr_t, int, daddr64_t, long *);
d152 2
a153 2
STATIC	struct allocindir *newallocindir(struct inode *, int, daddr_t,
	    daddr_t);
d155 1
a155 1
STATIC	struct freefrag *newfreefrag(struct inode *, daddr_t, long);
d159 1
a159 1
STATIC	int newblk_lookup(struct fs *, daddr_t, int,
d1145 1
a1145 1
	daddr_t newblkno;
d1355 1
a1355 1
	daddr_t newblkno;	/* number of newly allocated block */
d1442 2
a1443 2
	daddr_t newblkno;	/* disk block number being added */
	daddr_t oldblkno;	/* previous block number, 0 unless frag */
d1622 1
a1622 1
	daddr_t blkno;
d1698 2
a1699 2
	daddr_t newblkno;	/* disk block number being added */
	daddr_t oldblkno;	/* previous block number, 0 if none */
d1724 2
a1725 2
	daddr_t newblkno;	/* disk block number being added */
	daddr_t oldblkno;	/* previous block number, 0 if none */
d1762 1
a1762 1
	daddr_t newblkno;	/* disk block number being added */
d2391 1
a2391 1
	daddr_t bn;
d2463 1
a2463 1
	daddr_t dbn;
@


1.92
log
@Bring back the change in rev 1.70, it is safe now:
Propagate the wait flag from fsync down to softdep_fsync_mountdev()
and do not perform synchronous sync there is no wait requested by
skipping the drain_output() call.  This fixes a problem where
update kthread would sleep forever on some vnode since work is created
faster than it can be flushed.
OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.91 2007/06/01 20:23:26 pedro Exp $	*/
d2345 1
a2345 1
		FREE(inodedep->id_savedino1, M_INODEDEP);
d3467 2
a3468 2
		MALLOC(inodedep->id_savedino1, struct ufs1_dinode *,
		    sizeof(struct ufs1_dinode), M_INODEDEP, M_WAITOK);
d3609 2
a3610 2
		MALLOC(inodedep->id_savedino2, struct ufs2_dinode *,
		    sizeof(struct ufs2_dinode), M_INODEDEP, M_WAITOK);
d4083 1
a4083 1
		FREE(inodedep->id_savedino1, M_INODEDEP);
@


1.91
log
@Nuke 'ufs_lbn_t', okay otto@@ deraadt@@ krw@@ beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.90 2007/06/01 18:54:27 pedro Exp $	*/
d4721 2
a4722 1
	drain_output(vp, 1);
@


1.90
log
@cleanup of 'ufs1_daddr_t', first round, okay deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.89 2007/06/01 06:38:54 deraadt Exp $	*/
d142 1
a142 2
STATIC	int indir_trunc(struct inode *, daddr_t, int, ufs_lbn_t,
	    long *);
d162 1
a162 2
STATIC	int pagedep_lookup(struct inode *, ufs_lbn_t, int,
	    struct pagedep **);
d992 1
a992 1
	ufs_lbn_t lbn;
d1441 1
a1441 1
	ufs_lbn_t lbn;		/* block pointer within inode */
d1721 1
a1721 1
	ufs_lbn_t lbn;		/* allocated block number within file */
d2400 1
a2400 1
	ufs_lbn_t baselbns[NIADDR], tmpval;
d2465 1
a2465 1
	ufs_lbn_t lbn;
d2611 1
a2611 1
	ufs_lbn_t lbn;		/* block in directory containing new entry */
d2759 1
a2759 1
	ufs_lbn_t lbn;
d2914 1
a2914 1
	ufs_lbn_t lbn;
d3444 1
a3444 1
	ufs_lbn_t prevlbn = 0;
d4562 1
a4562 1
	ufs_lbn_t lbn;
@


1.89
log
@convert ufs2_daddr_t -> daddr64_t for greater clarity; ok pedro otto thib
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.88 2007/05/27 20:06:40 otto Exp $	*/
d1852 1
a1852 1
				((ufs1_daddr_t *)indirdep->ir_savebp->b_data)
d1855 1
a1855 1
				((daddr64_t *)indirdep->ir_savebp->b_data)
d2471 2
a2472 2
	ufs1_daddr_t *bap1 = NULL;
	daddr64_t nb, *bap2 = NULL;
d2522 1
a2522 1
		bap1 = (ufs1_daddr_t *) bp->b_data;
d2525 1
a2525 1
		bap2 = (daddr64_t *) bp->b_data;
d3447 1
a3447 1
	ufs1_daddr_t d1, d2;
d4029 1
a4029 1
		((ufs1_daddr_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
d4032 1
a4032 1
		((daddr64_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
@


1.88
log
@use struct csum_total for superblock summary info; ok pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.87 2007/05/26 20:26:51 pedro Exp $	*/
d1855 1
a1855 1
				((ufs2_daddr_t *)indirdep->ir_savebp->b_data)
d2472 1
a2472 1
	ufs2_daddr_t nb, *bap2 = NULL;
d2525 1
a2525 1
		bap2 = (ufs2_daddr_t *) bp->b_data;
d3594 1
a3594 1
	ufs2_daddr_t prevlbn = -1, d1, d2;
d4032 1
a4032 1
		((ufs2_daddr_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
@


1.87
log
@Dynamic buffer cache. Initial diff from mickey@@, okay art@@ beck@@ toby@@
deraadt@@ dlg@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.86 2007/04/15 10:48:35 pedro Exp $	*/
d1251 1
a1251 1
	struct csum cstotal;
@


1.86
log
@Save block pointers contents before releasing interrupts so that panic
messages are consistent, from mickey@@, okay pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.85 2007/04/04 18:53:20 pedro Exp $	*/
d4658 3
a4660 3
		error = bread(pvp, lbn, blksize(fs, pip, lbn), p->p_ucred,
		    &bp);
		if (error == 0)
d4662 1
a4662 1
		else
@


1.85
log
@Back out revision 1.70. By asynchronously writing the bitmaps to disk
upon unmount, we were generating rollbacks that wouldn't be taken care
of, as well as leaving dangling items in softdep's worklist.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.84 2007/03/15 10:22:30 art Exp $	*/
d3447 1
d3494 1
a3494 1
		    dp->di_db[adp->ad_lbn] != adp->ad_newblkno) {
d3497 1
a3497 2
			    "softdep_write_inodeblock", adp->ad_lbn,
			    dp->di_db[adp->ad_lbn], adp->ad_newblkno);
d3500 2
a3501 1
		    dp->di_ib[adp->ad_lbn - NDADDR] != adp->ad_newblkno) {
d3505 1
a3505 1
			    dp->di_ib[adp->ad_lbn - NDADDR], adp->ad_newblkno);
d3594 1
a3594 1
	ufs2_daddr_t prevlbn = -1;
d3640 2
a3641 1
		if (dp->di_extb[adp->ad_lbn] != adp->ad_newblkno) {
d3644 1
a3644 2
			    "softdep_write_inodeblock", adp->ad_lbn,
			    dp->di_extb[adp->ad_lbn], adp->ad_newblkno);
d3708 1
a3708 1
		    dp->di_db[adp->ad_lbn] != adp->ad_newblkno) {
d3711 1
a3711 2
			    "softdep_write_inodeblock", adp->ad_lbn,
			    dp->di_db[adp->ad_lbn], adp->ad_newblkno);
d3714 2
a3715 1
		    dp->di_ib[adp->ad_lbn - NDADDR] != adp->ad_newblkno) {
d3719 1
a3719 1
			   dp->di_ib[adp->ad_lbn - NDADDR], adp->ad_newblkno);
@


1.84
log
@Since p_flag is often manipulated in interrupts and without biglock
it's a good idea to use atomic.h operations on it. This mechanic
change updates all bit operations on p_flag to atomic_{set,clear}bits_int.

Only exception is that P_OWEUPC is set by MI code before calling
need_proftick and it's automatically cleared by ADDUPC. There's
no reason for MD handling of that flag since everyone handles it the
same way.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.83 2007/02/04 10:36:05 pedro Exp $	*/
d4722 1
a4722 2
	if (waitfor == MNT_WAIT)
		drain_output(vp, 1);
@


1.83
log
@Correctly fake dinode for the FFS2 case in handle_workitem_freeblocks()
Okay millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.82 2007/01/17 16:43:36 pedro Exp $	*/
d5288 1
a5288 1
		p->p_flag |= P_SOFTDEP;
d5293 1
a5293 1
		p->p_flag &= ~P_SOFTDEP;
@


1.82
log
@more ufs2 leftovers
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.81 2007/01/15 11:18:17 pedro Exp $	*/
a2392 1
	struct ufs1_dinode dtip1;
d2394 4
d2404 5
a2408 1
	tip.i_din1 = &dtip1;
d2413 2
a2414 2
	tip.i_ffs1_size = freeblks->fb_oldsize;
	tip.i_ffs1_uid = freeblks->fb_uid;
@


1.81
log
@Fix splbio() in initiate_write_inodeblock_ufs2()
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.80 2007/01/15 11:05:53 pedro Exp $	*/
d3575 1
a3575 1
 * Version of initiate_write_inodeblock that handles UFS2 dinodes.
d3786 1
a3786 1
#endif /* UFS2 */
@


1.80
log
@UFS2 -> FFS2
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.79 2006/11/07 12:29:45 mickey Exp $	*/
a3614 1
	ACQUIRE_LOCK(&lk);
a3623 1
	ACQUIRE_LOCK(&lk);
a3756 1
		FREE_LOCK(&lk);
a3784 1
	FREE_LOCK(&lk);
@


1.79
log
@a fix derived from freebsd 1.196 revision.
due to ffs_sync not be able to sync some buffers here is another
instance of softdep code that must ensure proper syncing.
try harder to flush MKDIR_BODY dependancy if such still exists
during pagedep flush (that is by syncing first block of the dir).
pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.78 2006/10/20 13:02:55 mickey Exp $	*/
d132 1
a132 1
#ifdef UFS2
d3321 1
a3321 1
#ifdef UFS2
d3573 1
a3573 1
#ifdef UFS2
@


1.78
log
@from freebsd rev1.128:
If an error occurs while writing a buffer, then the data will
not have hit the disk and the dependencies cannot be unrolled.
In this case, the system will mark the buffer as dirty again so
that the write can be retried in the future. When the write
succeeds or the system gives up on the buffer and marks it as
invalid (B_INVAL), the dependencies will be cleared.

pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.77 2006/10/19 14:37:54 mickey Exp $	*/
d5088 1
d5143 24
d5168 3
@


1.77
log
@one more missing brelse() on bread() error; pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.76 2006/09/30 14:47:52 mickey Exp $	*/
d3812 7
@


1.76
log
@simply getdirtybuf to take a plain buf* as there is no need otherwise; no functional change; pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.75 2006/09/26 09:50:31 mickey Exp $	*/
d4651 2
@


1.75
log
@accidental extra %s in ddb printf
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.74 2006/09/26 09:26:36 mickey Exp $	*/
d118 1
a118 1
STATIC	int getdirtybuf(struct buf **, int);
d2018 1
a2018 1
		if (!getdirtybuf(&bp, MNT_WAIT))
d4490 1
a4490 1
	gotit = getdirtybuf(&bp, MNT_WAIT);
d4782 1
a4782 1
	if (getdirtybuf(&bp, MNT_WAIT) == 0) {
d4799 1
a4799 1
			if (getdirtybuf(&nbp, waitfor) == 0)
d4816 1
a4816 1
			if (getdirtybuf(&nbp, waitfor) == 0)
d4835 1
a4835 1
				if (getdirtybuf(&nbp, MNT_WAIT) == 0)
d4887 1
a4887 1
			if (getdirtybuf(&nbp, waitfor) == 0)
d4908 1
a4908 1
			if (getdirtybuf(&nbp, waitfor) == 0)
d4928 1
a4928 1
	getdirtybuf(&nbp, MNT_WAIT);
d5015 1
a5015 1
			if (getdirtybuf(&bp, waitfor) == 0) {
d5036 1
a5036 1
			if (getdirtybuf(&bp, waitfor) == 0) {
d5165 1
a5165 1
			gotit = getdirtybuf(&bp, MNT_WAIT);
d5556 2
a5557 2
getdirtybuf(bpp, waitfor)
	struct buf **bpp;
a5559 1
	struct buf *bp;
d5562 3
a5567 2
		if ((bp = *bpp) == NULL)
			return (0);
@


1.74
log
@fix races w/ getdirtybuf() usage that can violate normal
softdep processing and temporarily create inconsistant
on-disk ffs state and as well abuse kmem.
pedro@@ deraadt@@ ok; tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.73 2006/07/27 11:34:13 mickey Exp $	*/
d5687 1
a5687 1
		(*pr)("fs %p ino %u nlinkdelta %u dino %p\n%s"
@


1.73
log
@missing \n in ddb printf
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.72 2006/07/11 21:17:58 mickey Exp $	*/
d2017 3
a2019 2
	while (getdirtybuf(&LIST_FIRST(&vp->v_dirtyblkhd), MNT_WAIT)) {
		bp = LIST_FIRST(&vp->v_dirtyblkhd);
d4489 2
a4490 1
	gotit = getdirtybuf(&inodedep->id_buf, MNT_WAIT);
d4492 1
a4492 2
	if (gotit &&
	    (error = bwrite(inodedep->id_buf)) != 0)
d4781 2
a4782 1
	if (getdirtybuf(&LIST_FIRST(&vp->v_dirtyblkhd), MNT_WAIT) == 0) {
a4785 1
	bp = LIST_FIRST(&vp->v_dirtyblkhd);
a4926 1
	(void) getdirtybuf(&LIST_NEXT(bp, b_vnbufs), MNT_WAIT);
d4928 1
d5164 2
a5165 1
			gotit = getdirtybuf(&inodedep->id_buf, MNT_WAIT);
d5167 1
a5167 2
			if (gotit &&
			    (error = bwrite(inodedep->id_buf)) != 0)
@


1.72
log
@add mount/vnode/buf and softdep printing commands; tested on a few archs and will make pedro happy too (;
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.71 2006/06/28 14:17:07 mickey Exp $	*/
d5687 1
a5687 1
		    "%s  bp %p savsz %lld", inodedep->id_fs,
@


1.71
log
@from freebsd 1.179:
When the softupdates worklist gets too long, threads that attempt to
add more work are forced to process two worklist items first.
However, processing an item may generate additional work, causing the
unlucky thread to recursively process the worklist.  Add a per-thread
flag to detect this situation and avoid the recursion.  This should
fix the stack overflows that could occur while removing large
directory trees.

pedro@@ tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.70 2006/06/21 10:01:10 mickey Exp $	*/
d5637 125
@


1.70
log
@propagate the wait flag from fsync down to softdep_fsync_mountdev()
and do not perform synchronous sync there is no wait requested by
skipping the drain_output() call. this fixes a problem where
update kthread would sleep forever on some vnode since work is created
faster than it can be flushed. pedro@@ ok; tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.69 2006/06/07 10:51:43 mickey Exp $	*/
d5236 1
a5236 1
	if (p == filesys_syncer)
d5243 2
a5244 1
	 * inode as that could lead to deadlock.
d5247 1
d5252 1
a5458 1
		drain_output(vp, 1);
@


1.69
log
@from freebsd r1.185 (tested by beck@@ and krw@@; pedro@@ ok):
After a rmdir()ed directory has been truncated, force an update of
the directory's inode after queuing the dirrem that will decrement
the parent directory's link count.  This will force the update of
the parent directory's actual link to actually be scheduled.  Without
this change the parent directory's actual link count would not be
updated until ufs_inactive() cleared the inode of the newly removed
directory, which might be deferred indefinitely.  ufs_inactive()
will not be called as long as any process holds a reference to the
removed directory, and ufs_inactive() will not clear the inode if
the link count is non-zero, which could be the result of an earlier
system crash.

If a background fsck is run before the update of the parent directory's
actual link count has been performed, or at least scheduled by
putting the dirrem on the leaf directory's inodedep id_bufwait list,
fsck will corrupt the file system by decrementing the parent
directory's effective link count, which was previously correct
because it already took the removal of the leaf directory into
account, and setting the actual link count to the same value as the
effective link count after the dangling, removed, leaf directory
has been removed.  This happens because fsck acts based on the
actual link count, which will be too high when fsck creates the
file system snapshot that it references.

This change has the fortunate side effect of more quickly cleaning
up the large number dirrem structures that linger for an extended
time after the removal of a large directory tree.  It also fixes a
potential problem with the shutdown of the syncer thread timing out
if the system is rebooted immediately after removing a large directory
tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.68 2006/01/03 23:34:39 pedro Exp $	*/
d4667 1
a4667 1
softdep_fsync_mountdev(vp)
d4669 1
d4709 2
a4710 1
	drain_output(vp, 1);
@


1.68
log
@Add UFS2 softdep bits, okay tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.67 2005/12/28 20:48:17 pedro Exp $	*/
d3213 2
@


1.67
log
@Use the DIP macros to uniformly access fields from UFS1 and UFS2 dinodes.
No functional change, okay tedu@@.
@
text
@d1 2
a2 1
/*	$OpenBSD: ffs_softdep.c,v 1.66 2005/12/17 13:56:01 pedro Exp $	*/
d131 4
a134 1
STATIC	void initiate_write_inodeblock(struct inodedep *, struct buf *);
d1115 1
a1115 1
	inodedep->id_savedino = NULL;
d1851 6
a1856 2
			((daddr_t *)indirdep->ir_savebp->b_data)
			    [aip->ai_offset] = aip->ai_oldblkno;
d1871 2
d1967 8
a1974 2
	*((struct ufs1_dinode *)bp->b_data + ino_to_fsbo(fs, ip->i_number)) =
	    *ip->i_din1;
d2345 3
a2347 3
	if (inodedep->id_savedino != NULL) {
		FREE(inodedep->id_savedino, M_INODEDEP);
		inodedep->id_savedino = NULL;
d2371 1
a2371 1
	    inodedep->id_nlinkdelta != 0 || inodedep->id_savedino != NULL)
d2463 2
a2464 2
	daddr_t *bap;
	daddr_t nb;
d2468 1
a2468 1
	int i, lbnadd, nblocks;
d2512 7
a2518 1
	bap = (daddr_t *)bp->b_data;
d2521 5
a2525 1
		if ((nb = bap[i]) == 0)
d3291 1
d3315 7
a3321 1
			initiate_write_inodeblock(WK_INODEDEP(wk), bp);
d3428 1
a3428 1
initiate_write_inodeblock(inodedep, bp)
d3453 1
a3453 1
		if (inodedep->id_savedino != NULL) {
d3458 1
a3458 1
		MALLOC(inodedep->id_savedino, struct ufs1_dinode *,
d3461 1
a3461 1
		*inodedep->id_savedino = *dp;
d3570 219
d4014 6
a4019 2
	((daddr_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
	    aip->ai_newblkno;
d4039 3
a4041 2
	struct ufs1_dinode *dp;
	int hadchanges;
d4048 11
a4058 2
	dp = (struct ufs1_dinode *)bp->b_data +
	    ino_to_fsbo(inodedep->id_fs, inodedep->id_ino);
d4066 7
a4072 4
	if (inodedep->id_savedino != NULL) {
		*dp = *inodedep->id_savedino;
		FREE(inodedep->id_savedino, M_INODEDEP);
		inodedep->id_savedino = NULL;
d4088 19
a4106 7
		if (adp->ad_lbn < NDADDR) {
			if (dp->di_db[adp->ad_lbn] != adp->ad_oldblkno)
				panic("%s: %s #%ld mismatch %d != %d",
				    "handle_written_inodeblock",
				    "direct pointer", adp->ad_lbn,
				    dp->di_db[adp->ad_lbn], adp->ad_oldblkno);
			dp->di_db[adp->ad_lbn] = adp->ad_newblkno;
d4108 18
a4125 6
			if (dp->di_ib[adp->ad_lbn - NDADDR] != 0)
				panic("%s: %s #%ld allocated as %d",
				    "handle_written_inodeblock",
				    "indirect pointer", adp->ad_lbn - NDADDR,
				    dp->di_ib[adp->ad_lbn - NDADDR]);
			dp->di_ib[adp->ad_lbn - NDADDR] = adp->ad_newblkno;
d4138 11
a4148 3
	if (dp->di_size != inodedep->id_savedsize) {
		dp->di_size = inodedep->id_savedsize;
		hadchanges = 1;
@


1.66
log
@Remove the 'on disk' inode (dinode) from the 'in memory' inode in UFS.
Instead of having the dinode inside the inode structure itself, we now
have just a pointer to it, and use a separate pool to allocate dinodes
as needed. Inspiration from FreeBSD, various testing for a while, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.65 2005/12/14 22:04:56 pedro Exp $	*/
d1508 1
a1508 1
		if ((ip->i_ffs_mode & IFMT) == IFDIR &&
d1633 1
a1633 1
	freefrag->ff_state = ip->i_ffs_uid & ~ONWORKLIST; /* used below */
d1659 1
a1659 1
	tip.i_ffs_uid = freefrag->ff_state & ~ONWORKLIST; /* set above */
d1736 1
a1736 1
	if ((ip->i_ffs_mode & IFMT) == IFDIR &&
d1927 1
a1927 1
	freeblks->fb_uid = ip->i_ffs_uid;
d1931 1
a1931 1
	freeblks->fb_oldsize = ip->i_ffs_size;
d1933 2
a1934 1
	freeblks->fb_chkcnt = ip->i_ffs_blocks;
d1936 2
a1937 2
		freeblks->fb_dblks[i] = ip->i_ffs_db[i];
		ip->i_ffs_db[i] = 0;
d1939 1
d1941 2
a1942 2
		freeblks->fb_iblks[i] = ip->i_ffs_ib[i];
		ip->i_ffs_ib[i] = 0;
d1944 4
a1947 2
	ip->i_ffs_blocks = 0;
	ip->i_ffs_size = 0;
d2389 2
a2390 2
	tip.i_ffs_size = freeblks->fb_oldsize;
	tip.i_ffs_uid = freeblks->fb_uid;
d3089 1
a3089 1
	if (ip->i_ffs_nlink < ip->i_effnlink) {
d3094 1
a3094 1
	inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
d3129 1
a3129 1
		ip->i_ffs_nlink--;
d3131 1
a3131 1
		if (ip->i_ffs_nlink < ip->i_effnlink) {
d3135 1
a3135 1
		inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
d3149 1
a3149 1
	ip->i_ffs_nlink -= 2;
d3151 1
a3151 1
	if (ip->i_ffs_nlink < ip->i_effnlink)
d3153 1
a3153 1
	inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
d4100 1
a4100 1
	ip->i_effnlink = ip->i_ffs_nlink;
d4140 1
a4140 1
		if (ip->i_effnlink != ip->i_ffs_nlink)
d4144 1
a4144 1
	if (inodedep->id_nlinkdelta != ip->i_ffs_nlink - ip->i_effnlink) {
@


1.65
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.64 2005/10/25 20:12:52 pedro Exp $	*/
d1633 1
a1633 1
	freefrag->ff_state = ip->i_ffs_uid & ~ONWORKLIST; /* XXX - used below */
d1651 1
d1654 1
d1659 1
a1659 1
	tip.i_ffs_uid = freefrag->ff_state & ~ONWORKLIST; /* XXX - set above */
d1954 1
a1954 1
	    ip->i_din1;
d2372 1
d2380 1
@


1.64
log
@Correctly remove elements from a pagedep dirrem list.
Issued noted and fix okay'd by otto@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.63 2005/09/26 21:11:09 pedro Exp $	*/
d109 1
a109 1
 * End system adaptaion definitions.
@


1.63
log
@bring in two changes from freebsd:

- keep track of free blocks dependencies, and only tell the syncer to
  release the blocks when there are no more dirty buffers associated
- don't mark the inode dependency as done if we had to rollback its
  allocation due to the bitmap being inconsistent

testing by krw, jsg, jmc and sturm, okay deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.62 2005/09/06 17:02:09 pedro Exp $	*/
d2105 1
a2105 1
			LIST_FOREACH(dirrem, &pagedep->pd_dirremhd, dm_next) {
@


1.62
log
@Enforce splbio() while traversing the worklist in
softdep_disk_io_initiation(). Testing by dtucker@@ and krw@@, okay
deraadt@@ long ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.61 2005/08/08 09:48:02 pedro Exp $	*/
d1924 1
d2006 14
a3782 1
	inodedep->id_state |= COMPLETE;
d3801 1
d3887 4
@


1.61
log
@Add a few splassert() checks, okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.60 2005/07/20 16:30:34 pedro Exp $	*/
d3242 1
d3250 3
d3278 2
a3279 2
				indirdep->ir_savebp->b_flags |= B_INVAL | B_NOCACHE;
				brelse(indirdep->ir_savebp);
d3284 3
d3292 1
a3300 1
			FREE_LOCK(&lk);
d3310 1
d3316 2
a3344 1
	ACQUIRE_LOCK(&lk);
a3362 1
	FREE_LOCK(&lk);
d3384 2
a3385 1
	if (inodedep->id_state & IOSTARTED)
d3387 1
d3397 2
a3398 1
		if (inodedep->id_savedino != NULL)
d3400 2
d3404 1
a3417 1
	ACQUIRE_LOCK(&lk);
a3483 1
		FREE_LOCK(&lk);
a3511 1
	FREE_LOCK(&lk);
@


1.60
log
@Reintroduce the changes made by tedu in revision 1.50 of ffs_softdep.c,
this time with a small tweak: when flushing the dependencies, don't
update the inode twice, but update once, and then, if needed, fsync it.
Doing so fixes the inode hangs some people were seeing.

Various testing for a while, especially krw@@ and millert@@, okay deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.59 2005/07/03 20:14:01 drahn Exp $	*/
d461 2
d999 2
d1072 2
d1384 2
d1563 2
d2154 2
d2198 2
d2293 1
d2502 2
d2752 2
d3643 2
d3716 2
d3752 2
d3902 2
d3927 2
d3964 2
d4167 2
d4647 2
d4743 2
d5217 2
d5248 1
a5248 1
	
d5251 3
@


1.59
log
@Extended Attributes was a piece to get to ACLs, however ACLs have not
been worked on, so EA is pointless. Also the code is not enabled
in GENERIC so it is not being tested or maintained.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.58 2005/06/18 18:09:43 millert Exp $	*/
d68 14
a81 13
#define	D_PAGEDEP	1
#define	D_INODEDEP	2
#define	D_NEWBLK	3
#define	D_BMSAFEMAP	4
#define	D_ALLOCDIRECT	5
#define	D_INDIRDEP	6
#define	D_ALLOCINDIR	7
#define	D_FREEFRAG	8
#define	D_FREEBLKS	9
#define	D_FREEFILE	10
#define	D_DIRADD	11
#define	D_MKDIR		12
#define	D_DIRREM	13
a86 1
	"invalid",
d100 1
d137 1
d376 1
d431 4
d979 2
a980 1
 * Look up a pagedep. Return 1 if found, 0 if not found.
d1011 3
d1226 2
d1454 1
d1551 1
d1553 1
d1563 1
a1563 1
		panic("allocdirect_check: old %d != new %d || lbn %ld >= %d",
d1590 11
d2091 15
d2141 2
d2160 11
d2175 44
d2529 2
a2530 2
void 
softdep_setup_directory_add(bp, dp, diroffset, newinum, newdirbp)
d2536 1
d2542 1
d2545 1
d2547 1
d2558 5
d2589 1
a2589 1
		if (inodedep_lookup(dp->i_fs, dp->i_number, 0, &inodedep) == 0
d2616 49
d2666 1
d3266 1
a3266 1
			MALLOC(indirdep->ir_saveddata, caddr_t, bp->b_bcount,
d3576 1
a3576 1
			FREE(indirdep->ir_saveddata, M_INDIRDEP);
d3841 4
d3948 2
d3951 3
a3953 2
	while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
		free_diradd(dap);
d3990 1
d3993 8
a4000 13
	 * If no dependencies remain, the pagedep will be freed.
	 * Otherwise it will remain to update the page before it
	 * is written back to disk.
	 */
	if (LIST_FIRST(&pagedep->pd_pendinghd) == 0) {
		for (i = 0; i < DAHASHSZ; i++)
			if (LIST_FIRST(&pagedep->pd_diraddhd[i]) != NULL)
				break;
		if (i == DAHASHSZ) {
			LIST_REMOVE(pagedep, pd_hash);
			WORKITEM_FREE(pagedep, D_PAGEDEP);
			return (0);
		}
d4002 1
a4002 1
	return (1);
d4172 1
d4204 2
a4205 2
		 * Flush our parent if this directory entry
		 * has a MKDIR_PARENT dependency.
d4218 5
a4222 1
		flushparent = dap->da_state & MKDIR_PARENT;
d4246 9
d4256 2
a4257 1
			if ((error = UFS_UPDATE(VTOI(pvp), MNT_WAIT))) {
d4261 7
d4272 1
a4272 1
		error = bread(pvp, lbn, blksize(fs, VTOI(pvp), lbn), p->p_ucred,
d4399 5
a4559 7
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 * Once they are all there, proceed with the second pass
	 * which will wait for the I/O as per above.
	 */
	drain_output(vp, 1);
	/*
d4561 1
a4561 1
	 * processing to be done.
d4574 3
d4578 1
d5076 1
@


1.58
log
@Remove remaining whiteout tentacles; OK deraadt@@ miod@@ weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.57 2005/06/10 17:37:40 pedro Exp $	*/
a53 1
#include <ufs/ufs/extattr.h>
@


1.57
log
@Tweak softdep_change_linkcnt() so we can specify whether we're willing
to be co-opted by softdep or not, and use this new interface to inform
softdep in ufs_inactive() that the inode mode has changed.

We don't want to be co-opted there as that might lead to undesired
circular dependencies such as a vput() depending on another vput() to
complete, or a process that is trying to free up a vnode being blocked
trying to acquire a new vnode.

Okay tedu@@ deraadt@@, thanks to all those who tested.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.56 2005/05/24 04:33:35 pedro Exp $	*/
a2446 9
	/*
	 * Whiteouts have no dependencies.
	 */
	if (newinum == WINO) {
		if (newdirbp != NULL)
			bdwrite(newdirbp);
		return;
	}

d2796 6
a2801 12

	/*
	 * Whiteouts do not need diradd dependencies.
	 */
	if (newinum != WINO) {
		dap = pool_get(&diradd_pool, PR_WAITOK);
		bzero(dap,sizeof(struct diradd));
		dap->da_list.wk_type = D_DIRADD;
		dap->da_state = DIRCHG | ATTACHED | DEPCOMPLETE;
		dap->da_offset = offset;
		dap->da_newinum = newinum;
	}
a2821 16

	/*
	 * Whiteouts have no additional dependencies,
	 * so just put the dirrem on the correct list.
	 */
	if (newinum == WINO) {
		if ((dirrem->dm_state & COMPLETE) == 0) {
			LIST_INSERT_HEAD(&pagedep->pd_dirremhd, dirrem,
			    dm_next);
		} else {
			dirrem->dm_dirinum = pagedep->pd_ino;
			add_to_worklist(&dirrem->dm_list);
		}
		FREE_LOCK(&lk);
		return;
	}
@


1.56
log
@brelse() on failed bread()
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.55 2004/12/09 17:41:53 millert Exp $	*/
d2904 1
a2904 1
softdep_change_linkcnt(ip)
d2906 1
d2909 8
d2919 2
a2920 1
	(void) inodedep_lookup(ip->i_fs, ip->i_number, DEPALLOC, &inodedep);
d2925 1
d2927 1
@


1.55
log
@When removing the last item from a non-empty worklist, the worklist
tail pointer must be updated; from FreeBSD (mckusick).
Appears to fix problems I was seeing with processes stuck with
WCHAN=inode in rev 1.50.  OK pedro@@ and tedu@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.54 2004/12/08 16:17:52 millert Exp $	*/
d4634 2
a4635 1
		    (int)ump->um_fs->fs_bsize, NOCRED, &bp)) != 0)
d4637 1
@


1.54
log
@Move a FREE_LOCK until after a potential continue so we don't try
to free the lock twice.  Doesn't actually make any difference for
us (no .o file diff) since the code block that contains the continue
is currently commented out but it makes sense to fix this now so
if/when that code is enabled we don't get bitten.  From FreeBSD, ok pedro@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.53 2004/12/07 04:37:28 tedu Exp $	*/
d536 1
a574 1
	static struct worklist *worklist_tail;
d726 1
a726 1
	struct worklist *wk;
d752 7
d760 6
@


1.53
log
@prev softdep merge could lead to inode wait deadlock for unknown reasons.
revert.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.52 2004/07/13 21:04:29 millert Exp $	*/
a4789 1
			FREE_LOCK(&lk);
d4794 1
@


1.52
log
@Change mode_t and nlink_t from 16bit to 32bit.  This allows us to
use mode_t in syscalls.master and to use mode_t in more places in
the kernel.  It also makes lint much more useful on kernel code.

I've also added a placeholder for st_birthtime to make a UFS2 import
easier at some future date.

Requested by and OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.51 2004/06/24 19:35:26 tholo Exp $	*/
d69 13
a81 14
#define	D_PAGEDEP	0
#define	D_INODEDEP	1
#define	D_NEWBLK	2
#define	D_BMSAFEMAP	3
#define	D_ALLOCDIRECT	4
#define	D_INDIRDEP	5
#define	D_ALLOCINDIR	6
#define	D_FREEFRAG	7
#define	D_FREEBLKS	8
#define	D_FREEFILE	9
#define	D_DIRADD	10
#define	D_MKDIR		11
#define	D_DIRREM	12
#define	D_NEWDIRBLK	13
d87 1
a100 1
	"newdirblk",
a136 1
STATIC	void free_newdirblk(struct newdirblk *);
a374 1
STATIC struct pool newdirblk_pool;
a428 4
	case D_NEWDIRBLK:
		pool_put(&newdirblk_pool, item);
		break;

d960 1
a960 2
 * Look up a pagedep. Return 1 if found, 0 if not found or found
 * when asked to allocate but not associated with any buffer.
a990 3
		if ((flags & DEPALLOC) != 0 &&
		    (pagedep->pd_state & ONWORKLIST) == 0)
			return (0);
a1202 2
	pool_init(&newdirblk_pool, sizeof(struct newdirblk), 0, 0, 0,
	    "newdirblkpl", &pool_allocator_nointr);
a1428 1
	LIST_INIT(&adp->ad_newdirblk);
a1524 1
	struct worklist *wk;
a1525 1
	struct newdirblk *newdirblk;
d1535 1
a1535 1
		panic("allocdirect_merge: old %d != new %d || lbn %ld >= %d",
a1561 11
	/*
	 * If we are tracking a new directory-block allocation,
	 * move it from the old allocdirect to the new allocdirect.
	 */
	if ((wk = LIST_FIRST(&oldadp->ad_newdirblk)) != NULL) {
		newdirblk = WK_NEWDIRBLK(wk);
		WORKLIST_REMOVE(&newdirblk->db_list);
		if (LIST_FIRST(&oldadp->ad_newdirblk) != NULL)
			panic("allocdirect_merge: extra newdirblk");
		WORKLIST_INSERT(&newadp->ad_newdirblk, &newdirblk->db_list);
	}
a2051 15
			if ((pagedep->pd_state & NEWBLOCK) != 0) {
				LIST_FOREACH(wk, &inodedep->id_bufwait, wk_list)
					if (wk->wk_type == D_NEWDIRBLK &&
					    WK_NEWDIRBLK(wk)->db_pagedep ==
					    pagedep)
						break;
				if (wk != NULL) {
					WORKLIST_REMOVE(wk);
					free_newdirblk(WK_NEWDIRBLK(wk));
				} else {
					FREE_LOCK(&lk);
					panic("deallocate_dependencies: "
					    "lost pagedep");
					}
			}
a2086 2
	struct newdirblk *newdirblk;
	struct worklist *wk;
a2103 11
	if ((wk = LIST_FIRST(&adp->ad_newdirblk)) != NULL) {
		newdirblk = WK_NEWDIRBLK(wk);
		WORKLIST_REMOVE(&newdirblk->db_list);
		if (LIST_FIRST(&adp->ad_newdirblk) != NULL)
			panic("free_allocdirect: extra newdirblk");
		if (delay)
			WORKLIST_INSERT(&adp->ad_inodedep->id_bufwait,
			    &newdirblk->db_list);
		else
			free_newdirblk(newdirblk);
	}
a2107 44
 * Free a newdirblk. Clear the NEWBLOCK flag on its associated pagedep.
 * This routine must be called with splbio interrupts blocked.
 */
void
free_newdirblk(newdirblk)
	struct newdirblk *newdirblk;
{
	struct pagedep *pagedep;
	struct diradd *dap;
	int i;

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("free_newdirblk: lock not held");
#endif
	/*
	 * If the pagedep is still linked onto the directory buffer
	 * dependency chain, then some of the entries on the
	 * pd_pendinghd list may not be committed to disk yet. In
	 * this case, we will simply clear the NEWBLOCK flag and
	 * let the pd_pendinghd list be processed when the pagedep
	 * is next written. If the pagedep is no longer on the buffer
	 * dependency chain, then all the entries on the pd_pending
	 * list are committed to disk and we can free them here.
	 */
	pagedep = newdirblk->db_pagedep;
	pagedep->pd_state &= ~NEWBLOCK;
	if ((pagedep->pd_state & ONWORKLIST) == 0)
		while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
			free_diradd(dap);
	/*
	 * If no dependencies remain, the pagedep will be freed.
	 */
	for (i = 0; i < DAHASHSZ; i++)
		if (LIST_FIRST(&pagedep->pd_diraddhd[i]) != NULL)
			break;
	if (i == DAHASHSZ && (pagedep->pd_state & ONWORKLIST) == 0) {
		LIST_REMOVE(pagedep, pd_hash);
		WORKITEM_FREE(pagedep, D_PAGEDEP);
	}
	WORKITEM_FREE(newdirblk, D_NEWDIRBLK);
}

/*
d2418 2
a2419 2
int 
softdep_setup_directory_add(bp, dp, diroffset, newinum, newdirbp, isnewblk)
a2424 1
	int isnewblk;		/* entry is in a newly allocated block */
a2429 1
	struct allocdirect *adp;
a2431 1
	struct newdirblk *newdirblk = NULL;
a2432 1
	
d2440 1
a2440 1
		return (0);
a2451 5
	if (isnewblk && lbn < NDADDR && fragoff(fs, diroffset) == 0) {
		newdirblk = pool_get(&newdirblk_pool, PR_WAITOK);
		newdirblk->db_list.wk_type = D_NEWDIRBLK;
		newdirblk->db_state = 0;
	}
d2478 1
a2478 1
		if (inodedep_lookup(fs, dp->i_number, 0, &inodedep) == 0
a2504 49
	if (isnewblk) {
		/*
		 * Directories growing into indirect blocks are rare
		 * enough and the frequency of new block allocation
		 * in those cases even more rare, that we choose not
		 * to bother tracking them. Rather we simply force the
		 * new directory entry to disk.
		 */
		if (lbn >= NDADDR) {
			FREE_LOCK(&lk);
			/*
			 * We only have a new allocation when at the
			 * beginning of a new block, not when we are
			 * expanding into an existing block.
			 */
			if (blkoff(fs, diroffset) == 0)
				return (1);
			return (0);
		}
		/*
		 * We only have a new allocation when at the beginning
		 * of a new fragment, not when we are expanding into an
		 * existing fragment. Also, there is nothing to do if we
		 * are already tracking this block.
		 */
		if (fragoff(fs, diroffset) != 0) {
			FREE_LOCK(&lk);
			return (0);
		}
			
		if ((pagedep->pd_state & NEWBLOCK) != 0) {
			WORKITEM_FREE(newdirblk, D_NEWDIRBLK);
			FREE_LOCK(&lk);
			return (0);
		}
		/*
		 * Find our associated allocdirect and have it track us.
		 */
		if (inodedep_lookup(fs, dp->i_number, 0, &inodedep) == 0)
			panic("softdep_setup_directory_add: lost inodedep");
		adp = TAILQ_LAST(&inodedep->id_newinoupdt, allocdirectlst);
		if (adp == NULL || adp->ad_lbn != lbn) {
			FREE_LOCK(&lk);
			panic("softdep_setup_directory_add: lost entry");
		}
		pagedep->pd_state |= NEWBLOCK;
		newdirblk->db_pagedep = pagedep;
		WORKLIST_INSERT(&adp->ad_newdirblk, &newdirblk->db_list);
	}
a2505 1
	return (0);
d3115 1
a3115 1
			indirdep->ir_saveddata = malloc(bp->b_bcount,
d3425 1
a3425 1
			free(indirdep->ir_saveddata, M_INDIRDEP);
a3689 4
		case D_NEWDIRBLK:
			free_newdirblk(WK_NEWDIRBLK(wk));
			continue;

a3792 2
	 * If it is a newly allocated block, we have to wait until
	 * the on-disk directory inode claims the new block.
d3794 2
a3795 3
	if ((pagedep->pd_state & NEWBLOCK) == 0)
		while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
			free_diradd(dap);
a3831 1
		return (1);
d3834 13
a3846 8
	 * If we are not waiting for a new directory block to be
	 * claimed by its inode, then the pagedep will be freed.
	 * Otherwise it will remain to track any new entries on
	 * the page in case they are fsync'ed.
	 */
	if ((pagedep->pd_state & NEWBLOCK) == 0) {
		LIST_REMOVE(pagedep, pd_hash);
		WORKITEM_FREE(pagedep, D_PAGEDEP);
d3848 1
a3848 1
	return (0);
a4017 1
	struct inode *pip;
d4049 2
a4050 2
		 * Flush our parent if this directory entry has a MKDIR_PARENT
		 * dependency or is contained in a newly allocated block.
d4063 1
a4063 5
		if ((dap->da_state & MKDIR_PARENT) ||
		    (pagedep->pd_state & NEWBLOCK))
			flushparent = 1;
		else
			flushparent = 0;
a4086 9
		/*
		 * All MKDIR_PARENT dependencies and all the NEWBLOCK pagedeps
		 * that are contained in direct blocks will be resolved by 
		 * doing a UFS_UPDATE. Pagedeps contained in indirect blocks
		 * may require a complete sync'ing of the directory. So, we
		 * try the cheap and fast UFS_UPDATE first, and if that fails,
		 * then we do the slower VOP_FSYNC of the directory.
		 */
		pip = VTOI(pvp);
d4088 1
a4088 6
			if ((error = UFS_UPDATE(pip, 1)) != 0) {
				vput(pvp);
				return (error);
			}
			if ((pagedep->pd_state & NEWBLOCK) &&
			    (error = UFS_UPDATE(pip, MNT_WAIT))) {
d4096 1
a4096 1
		error = bread(pvp, lbn, blksize(fs, pip, lbn), p->p_ucred,
a4222 5
	/*
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 */
	drain_output(vp, 1);
d4379 7
d4387 1
a4387 1
	 * processing to be done. Then proceed with the second pass.
a4399 3
	 *
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
a4400 1
	drain_output(vp, 1);
a4895 1
		drain_output(vp, 1);
@


1.51
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.50 2004/05/07 01:40:16 tedu Exp $	*/
d2214 1
a2214 1
		int mode;
@


1.50
log
@fatty softdep merge with freebsd.  fixes a variety of possible issues.
relevant changes to ffs_softdep.c were 1.95, 1.96, 1.97, 1.98, 1.103, and
1.107 in freebsd.  testing marc and otto.  ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.49 2004/01/20 03:44:06 tedu Exp $	*/
d659 1
a659 1
	starttime = time;
d700 1
d702 2
a703 1
			timersub(&time, &starttime, &diff);
@


1.49
log
@some pieces of ufs2.  help testing otto sturm
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.48 2003/11/19 03:29:31 mickey Exp $	*/
d69 14
a82 13
#define	D_PAGEDEP	1
#define	D_INODEDEP	2
#define	D_NEWBLK	3
#define	D_BMSAFEMAP	4
#define	D_ALLOCDIRECT	5
#define	D_INDIRDEP	6
#define	D_ALLOCINDIR	7
#define	D_FREEFRAG	8
#define	D_FREEBLKS	9
#define	D_FREEFILE	10
#define	D_DIRADD	11
#define	D_MKDIR		12
#define	D_DIRREM	13
a87 1
	"invalid",
d101 1
d138 1
d377 1
d432 4
d965 2
a966 1
 * Look up a pagedep. Return 1 if found, 0 if not found.
d997 3
d1212 2
d1440 1
d1537 1
d1539 1
d1549 1
a1549 1
		panic("allocdirect_check: old %d != new %d || lbn %ld >= %d",
d1576 11
d2077 15
d2127 2
d2146 11
d2161 44
d2515 2
a2516 2
void 
softdep_setup_directory_add(bp, dp, diroffset, newinum, newdirbp)
d2522 1
d2528 1
d2531 1
d2533 1
d2541 1
a2541 1
		return;
d2553 5
d2584 1
a2584 1
		if (inodedep_lookup(dp->i_fs, dp->i_number, 0, &inodedep) == 0
d2611 49
d2661 1
d3271 1
a3271 1
			MALLOC(indirdep->ir_saveddata, caddr_t, bp->b_bcount,
d3581 1
a3581 1
			FREE(indirdep->ir_saveddata, M_INDIRDEP);
d3846 4
d3953 2
d3956 3
a3958 2
	while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
		free_diradd(dap);
d3995 1
d3998 8
a4005 13
	 * If no dependencies remain, the pagedep will be freed.
	 * Otherwise it will remain to update the page before it
	 * is written back to disk.
	 */
	if (LIST_FIRST(&pagedep->pd_pendinghd) == 0) {
		for (i = 0; i < DAHASHSZ; i++)
			if (LIST_FIRST(&pagedep->pd_diraddhd[i]) != NULL)
				break;
		if (i == DAHASHSZ) {
			LIST_REMOVE(pagedep, pd_hash);
			WORKITEM_FREE(pagedep, D_PAGEDEP);
			return (0);
		}
d4007 1
a4007 1
	return (1);
d4177 1
d4209 2
a4210 2
		 * Flush our parent if this directory entry
		 * has a MKDIR_PARENT dependency.
d4223 5
a4227 1
		flushparent = dap->da_state & MKDIR_PARENT;
d4251 9
d4261 6
a4266 1
			if ((error = UFS_UPDATE(VTOI(pvp), MNT_WAIT))) {
d4274 1
a4274 1
		error = bread(pvp, lbn, blksize(fs, VTOI(pvp), lbn), p->p_ucred,
d4401 5
a4561 7
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 * Once they are all there, proceed with the second pass
	 * which will wait for the I/O as per above.
	 */
	drain_output(vp, 1);
	/*
d4563 1
a4563 1
	 * processing to be done.
d4576 3
d4580 1
d5076 1
@


1.48
log
@undo boolshit unapproved changes the author does not even care about to fix
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.46 2003/08/25 23:26:55 tedu Exp $	*/
d1603 1
a1603 1
	tip.i_devvp = freefrag->ff_devvp;
d2236 1
a2236 1
	tip.i_devvp = freeblks->fb_devvp;
d3029 1
a3029 1
	tip.i_devvp = freefile->fx_devvp;
@


1.47
log
@MALLOC is not meant for variable size allocations
@
text
@d3113 1
a3113 1
			indirdep->ir_saveddata = malloc(bp->b_bcount,
@


1.46
log
@rename struct dinode to ufs1_dinode.  clears the namespace and makes
way for some future work.  no function changes yet.
few other little cleanups.
help testing otto@@ and markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.45 2003/08/02 01:47:17 tedu Exp $	*/
d3113 1
a3113 1
			MALLOC(indirdep->ir_saveddata, caddr_t, bp->b_bcount,
@


1.45
log
@newline at end of printf
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.44 2003/05/26 18:33:17 tedu Exp $	*/
d1899 2
a1900 2
	*((struct dinode *)bp->b_data + ino_to_fsbo(fs, ip->i_number)) =
	    ip->i_din.ffs_din;
d3197 1
a3197 1
	struct dinode *dp;
d3208 1
a3208 1
	dp = (struct dinode *)bp->b_data +
d3217 2
a3218 2
		MALLOC(inodedep->id_savedino, struct dinode *,
		    sizeof(struct dinode), M_INODEDEP, M_WAITOK);
d3220 1
a3220 1
		bzero((caddr_t)dp, sizeof(struct dinode));
d3573 1
a3573 1
	struct dinode *dp;
d3580 1
a3580 1
	dp = (struct dinode *)bp->b_data +
@


1.44
log
@fiddle with some type names.  change most instances of ufs_daddr_t to
ufs1_daddr_t, a few to daddr_t.  ufs_daddr_t typedef is retained, but consider
it deprecated.  no functional changes.  inspired by freebsd.  ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.43 2003/05/14 01:12:27 jason Exp $	*/
d2274 1
a2274 1
		printf("handle_workitem_freeblocks: block count");
@


1.43
log
@remove the last of the MI commons
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.42 2002/10/12 01:09:45 krw Exp $	*/
d137 1
a137 1
STATIC	int indir_trunc(struct inode *, ufs_daddr_t, int, ufs_lbn_t,
d148 2
a149 2
STATIC	struct allocindir *newallocindir(struct inode *, int, ufs_daddr_t,
	    ufs_daddr_t);
d151 1
a151 1
STATIC	struct freefrag *newfreefrag(struct inode *, ufs_daddr_t, long);
d155 1
a155 1
STATIC	int newblk_lookup(struct fs *, ufs_daddr_t, int,
d1112 1
a1112 1
	ufs_daddr_t newblkno;
d1320 1
a1320 1
	ufs_daddr_t newblkno;	/* number of newly allocated block */
d1405 2
a1406 2
	ufs_daddr_t newblkno;	/* disk block number being added */
	ufs_daddr_t oldblkno;	/* previous block number, 0 unless frag */
d1569 1
a1569 1
	ufs_daddr_t blkno;
d1643 2
a1644 2
	ufs_daddr_t newblkno;	/* disk block number being added */
	ufs_daddr_t oldblkno;	/* previous block number, 0 if none */
d1669 2
a1670 2
	ufs_daddr_t newblkno;	/* disk block number being added */
	ufs_daddr_t oldblkno;	/* previous block number, 0 if none */
d1707 1
a1707 1
	ufs_daddr_t newblkno;	/* disk block number being added */
d1794 1
a1794 1
			((ufs_daddr_t *)indirdep->ir_savebp->b_data)
d2227 1
a2227 1
	ufs_daddr_t bn;
d2290 1
a2290 1
	ufs_daddr_t dbn;
d2296 2
a2297 2
	ufs_daddr_t *bap;
	ufs_daddr_t nb;
d2345 1
a2345 1
	bap = (ufs_daddr_t *)bp->b_data;
d3552 1
a3552 1
	((ufs_daddr_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
@


1.42
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.41 2002/07/16 22:54:42 millert Exp $	*/
a173 8
struct bio_ops bioops = {
	softdep_disk_io_initiation,		/* io_start */
	softdep_disk_write_complete,		/* io_complete */
	softdep_deallocate_dependencies,	/* io_deallocate */
	softdep_move_dependencies,		/* io_movedeps */
	softdep_count_dependencies,		/* io_countdeps */
};

d1151 6
@


1.41
log
@Fix typo which prevents diagnostic test from working; enami@@netbsd.org

Fix two problems with softdep_typenames (missing entry, wrong boundary check);
wiz@@netbsd.org

art@@ OK
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.40 2002/07/12 14:02:23 art Exp $	*/
d1457 1
a1457 1
		panic("softdep_setup_allocdirect: Bonk art in the head\n");
d1692 1
a1692 1
		panic("softdep_setup_allocindir_page: Bonk art in the head\n");
@


1.40
log
@Change the locking on the mountpoint slightly. Instead of using mnt_lock
to get shared locks for lookup and get the exclusive lock only with
LK_DRAIN on unmount and do the real exclusive locking with flags in
mnt_flags, we now use shared locks for lookup and an exclusive lock for
unmount.

This is accomplished by slightly changing the semantics of vfs_busy.
Old vfs_busy behavior:
 - with LK_NOWAIT set in flags, a shared lock was obtained if the
   mountpoint wasn't being unmounted, otherwise we just returned an error.
 - with no flags, a shared lock was obtained if the mountpoint was being
   unmounted, otherwise we slept until the unmount was done and returned
   an error.
LK_NOWAIT was used for sync(2) and some statistics code where it isn't really
critical that we get the correct results.
0 was used in fchdir and lookup where it's critical that we get the right
directory vnode for the filesystem root.

After this change vfs_busy keeps the same behavior for no flags and LK_NOWAIT.
But if some other flags are passed into it, they are passed directly
into lockmgr (actually LK_SLEEPFAIL is always added to those flags because
if we sleep for the lock, that means someone was holding the exclusive lock
and the exclusive lock is only held when the filesystem is being unmounted.

More changes:
 dounmount must now be called with the exclusive lock held. (before this
 the caller was supposed to hold the vfs_busy lock, but that wasn't always
 true).
 Zap some (now) unused mount flags.
And the highlight of this change:
 Add some vfs_busy calls to match some vfs_unbusy calls, especially in
 sys_mount. (lockmgr doesn't detect the case where we release a lock noone
 holds (it will do that soon)).

If you've seen hangs on reboot with mfs this should solve it (I repeat this
for the fourth time now, but this time I spent two months fixing and
redesigning this and reading the code so this time I must have gotten
this right).
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.39 2002/06/08 04:51:05 art Exp $	*/
d97 1
d103 1
a103 1
	((unsigned)(type) < D_LAST ? softdep_typenames[type] : "???")
d2258 1
a2258 1
		    baselbns[level], &blocksreleased)) == 0)
@


1.39
log
@Remove this horror DIAGNOSTIC define. It was just ugly when it
was introduced, but now it causes crashes when kernels are built
without DIAGNOSTIC. Instead of trying to debug this incorrect code,
I'm just removing this abomination.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.38 2002/06/08 04:44:01 art Exp $	*/
a923 2
		if (oldmnt->mnt_flag & MNT_UNMOUNT)
			panic("softdep_flushfiles: looping");
@


1.38
log
@Let this build when the internal DIAGNOSTIC define is removed.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.37 2002/03/14 01:27:14 millert Exp $	*/
a41 7

/*
 * For now we want the safety net that the DIAGNOSTIC flag provide.
 */
#ifndef DIAGNOSTIC
#define DIAGNOSTIC
#endif
@


1.37
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.36 2002/02/22 20:37:46 drahn Exp $	*/
d3209 1
d3211 1
@


1.36
log
@Extended Attribute support from FreeBSD/TrustedBSD ok art@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.35 2002/01/29 14:31:59 millert Exp $	*/
d121 49
a169 49
STATIC	void softdep_error __P((char *, int));
STATIC	void drain_output __P((struct vnode *, int));
STATIC	int getdirtybuf __P((struct buf **, int));
STATIC	void clear_remove __P((struct proc *));
STATIC	void clear_inodedeps __P((struct proc *));
STATIC	int flush_pagedep_deps __P((struct vnode *, struct mount *,
	    struct diraddhd *));
STATIC	int flush_inodedep_deps __P((struct fs *, ino_t));
STATIC	int handle_written_filepage __P((struct pagedep *, struct buf *));
STATIC  void diradd_inode_written __P((struct diradd *, struct inodedep *));
STATIC	int handle_written_inodeblock __P((struct inodedep *, struct buf *));
STATIC	void handle_allocdirect_partdone __P((struct allocdirect *));
STATIC	void handle_allocindir_partdone __P((struct allocindir *));
STATIC	void initiate_write_filepage __P((struct pagedep *, struct buf *));
STATIC	void handle_written_mkdir __P((struct mkdir *, int));
STATIC	void initiate_write_inodeblock __P((struct inodedep *, struct buf *));
STATIC	void handle_workitem_freefile __P((struct freefile *));
STATIC	void handle_workitem_remove __P((struct dirrem *));
STATIC	struct dirrem *newdirrem __P((struct buf *, struct inode *,
	    struct inode *, int, struct dirrem **));
STATIC	void free_diradd __P((struct diradd *));
STATIC	void free_allocindir __P((struct allocindir *, struct inodedep *));
STATIC	int indir_trunc __P((struct inode *, ufs_daddr_t, int, ufs_lbn_t,
	    long *));
STATIC	void deallocate_dependencies __P((struct buf *, struct inodedep *));
STATIC	void free_allocdirect __P((struct allocdirectlst *,
	    struct allocdirect *, int));
STATIC	int check_inode_unwritten __P((struct inodedep *));
STATIC	int free_inodedep __P((struct inodedep *));
STATIC	void handle_workitem_freeblocks __P((struct freeblks *));
STATIC	void merge_inode_lists __P((struct inodedep *));
STATIC	void setup_allocindir_phase2 __P((struct buf *, struct inode *,
	    struct allocindir *));
STATIC	struct allocindir *newallocindir __P((struct inode *, int, ufs_daddr_t,
	    ufs_daddr_t));
STATIC	void handle_workitem_freefrag __P((struct freefrag *));
STATIC	struct freefrag *newfreefrag __P((struct inode *, ufs_daddr_t, long));
STATIC	void allocdirect_merge __P((struct allocdirectlst *,
	    struct allocdirect *, struct allocdirect *));
STATIC	struct bmsafemap *bmsafemap_lookup __P((struct buf *));
STATIC	int newblk_lookup __P((struct fs *, ufs_daddr_t, int,
	    struct newblk **));
STATIC	int inodedep_lookup __P((struct fs *, ino_t, int, struct inodedep **));
STATIC	int pagedep_lookup __P((struct inode *, ufs_lbn_t, int,
	    struct pagedep **));
STATIC	void pause_timer __P((void *));
STATIC	int request_cleanup __P((int, int));
STATIC	int process_worklist_item __P((struct mount *, int));
STATIC	void add_to_worklist __P((struct worklist *));
d174 5
a178 5
void softdep_disk_io_initiation __P((struct buf *));
void softdep_disk_write_complete __P((struct buf *));
void softdep_deallocate_dependencies __P((struct buf *));
void softdep_move_dependencies __P((struct buf *, struct buf *));
int softdep_count_dependencies __P((struct buf *bp, int, int));
d220 4
a223 4
STATIC	void acquire_lock __P((struct lockit *, int));
STATIC	void free_lock __P((struct lockit *, int));
STATIC	void acquire_lock_interlocked __P((struct lockit *, int, int));
STATIC	int free_lock_interlocked __P((struct lockit *, int));
d313 3
a315 3
STATIC	void sema_init __P((struct sema *, char *, int, int));
STATIC	int sema_get __P((struct sema *, struct lockit *));
STATIC	void sema_release __P((struct sema *));
d493 3
a495 3
STATIC	void worklist_insert __P((struct workhead *, struct worklist *));
STATIC	void worklist_remove __P((struct worklist *));
STATIC	void workitem_free __P((struct worklist *));
@


1.35
log
@process the delayed-free queue more often; chs@@netbsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.34 2002/01/25 02:30:26 millert Exp $	*/
d61 1
@


1.34
log
@Move softdep sysctls from debug to vfs.ffs; art@@ OK
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.33 2002/01/24 21:42:24 mickey Exp $	*/
d451 1
a451 1
struct workhead softdep_tofree;
d454 1
a454 1
softdep_queuefree(struct worklist *item)
d459 1
a459 1
	LIST_INSERT_HEAD(&softdep_tofree, item, wk_list);
d463 13
d489 1
a489 1
#define WORKITEM_FREE(item, type) softdep_queuefree((struct worklist *)item)
d541 1
a541 1
	softdep_queuefree(item);
d619 1
a619 1
	int matchcnt, loopcount, s;
a620 1
	struct worklist *wk;
d623 1
a623 1
	 * First process any items on the delay-free queue.
d625 3
a627 6
	s = splbio();
	while ((wk = LIST_FIRST(&softdep_tofree)) != NULL) {
		LIST_REMOVE(wk, wk_list);
		softdep_free(wk, wk->wk_type);
	}
	splx(s);
d713 7
@


1.33
log
@sigh, nobody compiles before commit anymore
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.32 2002/01/23 21:24:02 millert Exp $	*/
a561 16
#ifdef DEBUG
#include <uvm/uvm_extern.h>
#include <sys/sysctl.h>
struct ctldebug debug20 = { "max_softdeps", &max_softdeps };
struct ctldebug debug21 = { "tickdelay", &tickdelay };
struct ctldebug debug22 = { "worklist_push", &stat_worklist_push };
struct ctldebug debug23 = { "blk_limit_push", &stat_blk_limit_push };
struct ctldebug debug24 = { "ino_limit_push", &stat_ino_limit_push };
struct ctldebug debug25 = { "blk_limit_hit", &stat_blk_limit_hit };
struct ctldebug debug26 = { "ino_limit_hit", &stat_ino_limit_hit };
struct ctldebug debug27 = { "sync_limit_hit", &stat_sync_limit_hit }; 
struct ctldebug debug28 = { "indir_blk_ptrs", &stat_indir_blk_ptrs };
struct ctldebug debug29 = { "inode_bitmap", &stat_inode_bitmap };
struct ctldebug debug30 = { "direct_blk_ptrs", &stat_direct_blk_ptrs };
struct ctldebug debug31 = { "dir_entry", &stat_dir_entry };
#endif /* DEBUG */
@


1.32
log
@Convert to pool; based on changes NetBSD
We no longer use the last arg to WORKITEM_FREE but I have not removed it
from the calls in order to simplify these diffs and further softdep merges.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.31 2001/12/19 08:58:07 art Exp $	*/
d481 1
a481 1
STATIC	void workitem_free __P((struct worklist *, int));
@


1.31
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.30 2001/12/10 02:19:34 art Exp $	*/
d56 1
a72 7
 * These definitions need to be adapted to the system to which
 * this file is being ported.
 */

#define M_SOFTDEP_FLAGS (M_WAITOK)

/*
d75 34
a108 19
#define	D_PAGEDEP	M_PAGEDEP
#define	D_INODEDEP	M_INODEDEP
#define	D_NEWBLK	M_NEWBLK
#define	D_BMSAFEMAP	M_BMSAFEMAP
#define	D_ALLOCDIRECT	M_ALLOCDIRECT
#define	D_INDIRDEP	M_INDIRDEP
#define	D_ALLOCINDIR	M_ALLOCINDIR
#define	D_FREEFRAG	M_FREEFRAG
#define	D_FREEBLKS	M_FREEBLKS
#define	D_FREEFILE	M_FREEFILE
#define	D_DIRADD	M_DIRADD
#define	D_MKDIR		M_MKDIR
#define	D_DIRREM	M_DIRREM
/*
 * Names of malloc types.
 */
extern char *memname[];
#define TYPENAME(type) ((unsigned)(type) < M_LAST ? memname[type] : "???")
#define DtoM(type) (type)
d373 91
d476 1
a476 1
#define WORKITEM_FREE(item, type) FREE(item, DtoM(type))
d485 1
a485 1
#define WORKITEM_FREE(item, type) workitem_free((struct worklist *)item, type)
d519 1
a519 1
workitem_free(item, type)
a520 1
	int type;
d528 1
a528 6
	if (item->wk_type != type) {
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
		panic("workitem_free: type mismatch");
	}
	FREE(item, DtoM(type));
d622 1
a622 1
	int matchcnt, loopcount;
d624 11
d1014 1
a1014 2
	MALLOC(pagedep, struct pagedep *, sizeof(struct pagedep), M_PAGEDEP,
		M_SOFTDEP_FLAGS);
d1089 1
a1089 2
	MALLOC(inodedep, struct inodedep *, sizeof(struct inodedep),
		M_INODEDEP, M_SOFTDEP_FLAGS);
d1149 1
a1149 2
	MALLOC(newblk, struct newblk *, sizeof(struct newblk),
		M_NEWBLK, M_SOFTDEP_FLAGS);
d1184 26
d1368 1
a1368 2
	MALLOC(bmsafemap, struct bmsafemap *, sizeof(struct bmsafemap),
		M_BMSAFEMAP, M_SOFTDEP_FLAGS);
d1427 1
a1427 2
	MALLOC(adp, struct allocdirect *, sizeof(struct allocdirect),
		M_ALLOCDIRECT, M_SOFTDEP_FLAGS);
d1458 1
a1458 1
	FREE(newblk, M_NEWBLK);
d1589 1
a1589 2
	MALLOC(freefrag, struct freefrag *, sizeof(struct freefrag),
		M_FREEFRAG, M_SOFTDEP_FLAGS);
d1617 1
a1617 1
	FREE(freefrag, M_FREEFRAG);
d1657 1
a1657 2
	MALLOC(aip, struct allocindir *, sizeof(struct allocindir),
		M_ALLOCINDIR, M_SOFTDEP_FLAGS);
d1776 1
a1776 1
			FREE(newblk, M_NEWBLK);
d1812 1
a1812 1
			WORKITEM_FREE((caddr_t)newindirdep, D_INDIRDEP);
d1816 1
a1816 2
		MALLOC(newindirdep, struct indirdep *, sizeof(struct indirdep),
			M_INDIRDEP, M_SOFTDEP_FLAGS);
d1879 1
a1879 2
	MALLOC(freeblks, struct freeblks *, sizeof(struct freeblks),
	    M_FREEBLKS, M_SOFTDEP_FLAGS);
d2131 1
a2131 2
	MALLOC(freefile, struct freefile *, sizeof(struct freefile),
		M_FREEFILE, M_SOFTDEP_FLAGS);
d2453 1
a2453 2
	MALLOC(dap, struct diradd *, sizeof(struct diradd), M_DIRADD,
		M_SOFTDEP_FLAGS);
d2464 1
a2464 2
		MALLOC(mkdir1, struct mkdir *, sizeof(struct mkdir), M_MKDIR,
		    M_SOFTDEP_FLAGS);
d2468 1
a2468 2
		MALLOC(mkdir2, struct mkdir *, sizeof(struct mkdir), M_MKDIR,
		    M_SOFTDEP_FLAGS);
d2704 1
a2704 2
	MALLOC(dirrem, struct dirrem *, sizeof(struct dirrem),
		M_DIRREM, M_SOFTDEP_FLAGS);
d2804 1
a2804 2
		MALLOC(dap, struct diradd *, sizeof(struct diradd),
		    M_DIRADD, M_SOFTDEP_FLAGS);
d3123 1
a3123 1
			    M_INDIRDEP, M_SOFTDEP_FLAGS);
d3225 1
a3225 1
		    sizeof(struct dinode), M_INODEDEP, M_SOFTDEP_FLAGS);
@


1.30
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.29 2001/12/05 00:36:17 niklas Exp $	*/
a55 1
#include <sys/pool.h>
a68 4
#include <uvm/uvm.h>
struct pool sdpcpool;
int softdep_lockedbufs;

a108 7
 * Definitions for page cache info hashtable.
 */
#define PCBPHASHSIZE 1024
LIST_HEAD(, buf) pcbphashhead[PCBPHASHSIZE];
#define PCBPHASH(vp, lbn) ((((vaddr_t)(vp) >> 8) ^ (lbn)) & (PCBPHASHSIZE - 1))

/*
a159 7
STATIC struct buf *softdep_setup_pagecache __P((struct inode *, ufs_lbn_t,
						long));
STATIC void softdep_collect_pagecache __P((struct inode *));
STATIC void softdep_free_pagecache __P((struct inode *));
STATIC struct vnode *softdep_lookupvp(struct fs *, ino_t);
STATIC struct buf *softdep_lookup_pcbp __P((struct vnode *, ufs_lbn_t));
void softdep_pageiodone __P((struct buf *));
a175 1
	softdep_pageiodone,			/* io_pagedone */
a1063 1
	int i;
a1081 5
	pool_init(&sdpcpool, sizeof(struct buf), 0, 0, 0, "sdpcpool",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_TEMP);
	for (i = 0; i < PCBPHASHSIZE; i++) {
		LIST_INIT(&pcbphashhead[i]);
	}
a1333 8
	/*
	 * If we were not passed a bp to attach the dep to,
	 * then this must be for a regular file.
	 * Allocate a buffer to represent the page cache pages
	 * that are the real dependency.  The pages themselves
	 * cannot refer to the dependency since we don't want to
	 * add a field to struct vm_page for this.
	 */
d1335 4
a1338 1
		bp = softdep_setup_pagecache(ip, lbn, newsize);
d1572 4
a1575 1
		nbp = softdep_setup_pagecache(ip, lbn, ip->i_fs->fs_bsize);
a1753 1
	vp = ITOV(ip);
a1812 5
	 * We must remove any pagecache markers from the pagecache
	 * hashtable first because any I/Os in flight will want to see
	 * dependencies attached to their pagecache markers.  We cannot
	 * free the pagecache markers until after we've freed all the
	 * dependencies that reference them later.
a1815 1
	softdep_collect_pagecache(ip);
d1827 1
a1838 1
	softdep_free_pagecache(ip);
d2907 1
d2925 2
a2926 1
	tip.i_vnode = NULL;
a4323 9
	struct vnode *vp;
	struct uvm_object *uobj;

	vp = softdep_lookupvp(fs, ino);
#ifdef DIAGNOSTIC
	if (vp == NULL)
		panic("flush_inodedep_deps: null vp");
#endif
	uobj = &vp->v_uobj;
a4342 20

		/*
		 * When file data was in the buffer cache,
		 * softdep_sync_metadata() would start i/o on
		 * file data buffers itself.  But now that
		 * we're using the page cache to hold file data,
		 * we need something else to trigger those flushes.
		 * let's just do it here.
		 */
		FREE_LOCK(&lk);
		simple_lock(&uobj->vmobjlock);
		(uobj->pgops->pgo_flush)(uobj, 0, 0,
		    PGO_ALLPAGES|PGO_CLEANIT|
		    (waitfor == MNT_NOWAIT ? 0: PGO_SYNCIO));
		simple_unlock(&uobj->vmobjlock);
		if (waitfor == MNT_WAIT) {
			drain_output(vp, 0);
		}
		ACQUIRE_LOCK(&lk);

a4956 191
}

/*
 * Allocate a buffer on which to attach a dependency.
 */
STATIC struct buf *
softdep_setup_pagecache(ip, lbn, size)
	struct inode *ip;
	ufs_lbn_t lbn;
	long size;
{
	struct vnode *vp = ITOV(ip);
	struct buf *bp;
	int s;

	/*
	 * Enter pagecache dependency buf in hash.
	 * Always reset b_resid to be the full amount of data in the block
	 * since the caller has the corresponding pages locked and dirty.
	 */

	bp = softdep_lookup_pcbp(vp, lbn);
	if (bp == NULL) {
		s = splbio();
		bp = pool_get(&sdpcpool, PR_WAITOK);
		splx(s);

		bp->b_vp = vp;
		bp->b_lblkno = lbn;
		LIST_INIT(&bp->b_dep);
		LIST_INSERT_HEAD(&pcbphashhead[PCBPHASH(vp, lbn)], bp, b_hash);
		LIST_INSERT_HEAD(&ip->i_pcbufhd, bp, b_vnbufs);
	}
	bp->b_bcount = bp->b_resid = size;
	return bp;
}

/*
 * softdep_collect_pagecache() and softdep_free_pagecache()
 * are used to remove page cache dependency buffers when
 * a file is being truncated to 0.
 */

STATIC void
softdep_collect_pagecache(ip)
	struct inode *ip;
{
	struct buf *bp;

	LIST_FOREACH(bp, &ip->i_pcbufhd, b_vnbufs) {
		LIST_REMOVE(bp, b_hash);
	}
}

STATIC void
softdep_free_pagecache(ip)
	struct inode *ip;
{
	struct buf *bp, *nextbp;

	for (bp = LIST_FIRST(&ip->i_pcbufhd); bp != NULL; bp = nextbp) {
		nextbp = LIST_NEXT(bp, b_vnbufs);
		LIST_REMOVE(bp, b_vnbufs);
		KASSERT(LIST_FIRST(&bp->b_dep) == NULL);
		pool_put(&sdpcpool, bp);
	}
}

STATIC struct vnode *
softdep_lookupvp(fs, ino)
	struct fs *fs;
	ino_t ino;
{
	struct mount *mp;
	extern struct vfsops ffs_vfsops;

	CIRCLEQ_FOREACH(mp, &mountlist, mnt_list) {
		if (mp->mnt_op == &ffs_vfsops &&
		    VFSTOUFS(mp)->um_fs == fs) {
			break;
		}
	}
	if (mp == NULL) {
		return NULL;
	}
	return ufs_ihashlookup(VFSTOUFS(mp)->um_dev, ino);
}

STATIC struct buf *
softdep_lookup_pcbp(vp, lbn)
	struct vnode *vp;
	ufs_lbn_t lbn;
{
	struct buf *bp;

	LIST_FOREACH(bp, &pcbphashhead[PCBPHASH(vp, lbn)], b_hash) {
		if (bp->b_vp == vp && bp->b_lblkno == lbn) {
			break;
		}
	}
	return bp;	     
}

/*
 * Do softdep i/o completion processing for page cache writes.
 */
 
void
softdep_pageiodone(bp)
	struct buf *bp;
{
	int npages = bp->b_bufsize >> PAGE_SHIFT;
	struct vnode *vp = bp->b_vp;
	struct vm_page *pg;
	struct buf *pcbp = NULL;
	struct allocdirect *adp;
	struct allocindir *aip;
	struct worklist *wk;
	ufs_lbn_t lbn;
	voff_t off;
	long iosize = bp->b_bcount;
	int size, asize, bshift, bsize;
	int i;

	KASSERT(!(bp->b_flags & B_READ));
	bshift = vp->v_mount->mnt_fs_bshift;
	bsize = 1 << bshift;
	asize = min(PAGE_SIZE, bsize);
	ACQUIRE_LOCK(&lk);
	for (i = 0; i < npages; i++) {
		pg = uvm_pageratop((vaddr_t)bp->b_data + (i << PAGE_SHIFT));
		if (pg == NULL) {
			continue;
		}

		for (off = pg->offset;
		     off < pg->offset + PAGE_SIZE;
		     off += bsize) {
			size = min(asize, iosize);
			iosize -= size;
			lbn = off >> bshift;
			if (pcbp == NULL || pcbp->b_lblkno != lbn) {
				pcbp = softdep_lookup_pcbp(vp, lbn);
			}
			if (pcbp == NULL) {
				continue;
			}
			pcbp->b_resid -= size;
			if (pcbp->b_resid < 0) {
				panic("softdep_pageiodone: "
				      "resid < 0, vp %p lbn 0x%lx pcbp %p",
				      vp, lbn, pcbp);
			}
			if (pcbp->b_resid > 0) {
				continue;
			}

			/*
			 * We've completed all the i/o for this block.
			 * mark the dep complete.
			 */

			KASSERT(LIST_FIRST(&pcbp->b_dep) != NULL);
			while ((wk = LIST_FIRST(&pcbp->b_dep))) {
				WORKLIST_REMOVE(wk);
				switch (wk->wk_type) {
				case D_ALLOCDIRECT:
					adp = WK_ALLOCDIRECT(wk);
					adp->ad_state |= COMPLETE;
					handle_allocdirect_partdone(adp);
					break;

				case D_ALLOCINDIR:
					aip = WK_ALLOCINDIR(wk);
					aip->ai_state |= COMPLETE;
					handle_allocindir_partdone(aip);
					break;

				default:
					panic("softdep_pageiodone: "
					      "bad type %d, pcbp %p wk %p",
					      wk->wk_type, pcbp, wk);
				}
			}
			LIST_REMOVE(pcbp, b_hash);
			LIST_REMOVE(pcbp, b_vnbufs);
			pool_put(&sdpcpool, pcbp);
			pcbp = NULL;
		}
	}
	FREE_LOCK(&lk);
@


1.30.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 2
/*	$OpenBSD: ffs_softdep.c,v 1.35 2002/01/29 14:31:59 millert Exp $	*/

d77 7
d86 19
a104 34
#define	D_PAGEDEP	1
#define	D_INODEDEP	2
#define	D_NEWBLK	3
#define	D_BMSAFEMAP	4
#define	D_ALLOCDIRECT	5
#define	D_INDIRDEP	6
#define	D_ALLOCINDIR	7
#define	D_FREEFRAG	8
#define	D_FREEBLKS	9
#define	D_FREEFILE	10
#define	D_DIRADD	11
#define	D_MKDIR		12
#define	D_DIRREM	13
#define	D_LAST		13
/*
 * Names of softdep types.
 */
const char *softdep_typenames[] = {
	"invalid",
	"pagedep",
	"inodedep",
	"newblk",
	"bmsafemap",
	"allocdirect",
	"indirdep",
	"allocindir",
	"freefrag",
	"freeblks",
	"diradd",
	"mkdir",
	"dirrem",
};
#define	TYPENAME(type) \
	((unsigned)(type) < D_LAST ? softdep_typenames[type] : "???")
a383 104
 * Memory management.
 */
STATIC struct pool pagedep_pool;
STATIC struct pool inodedep_pool;
STATIC struct pool newblk_pool;
STATIC struct pool bmsafemap_pool;
STATIC struct pool allocdirect_pool;
STATIC struct pool indirdep_pool;
STATIC struct pool allocindir_pool;
STATIC struct pool freefrag_pool;
STATIC struct pool freeblks_pool;
STATIC struct pool freefile_pool;
STATIC struct pool diradd_pool;
STATIC struct pool mkdir_pool;
STATIC struct pool dirrem_pool;

static __inline void
softdep_free(struct worklist *item, int type)
{

	switch (type) {
	case D_PAGEDEP:
		pool_put(&pagedep_pool, item);
		break;

	case D_INODEDEP:
		pool_put(&inodedep_pool, item);
		break;

	case D_BMSAFEMAP:
		pool_put(&bmsafemap_pool, item);
		break;

	case D_ALLOCDIRECT:
		pool_put(&allocdirect_pool, item);
		break;

	case D_INDIRDEP:
		pool_put(&indirdep_pool, item);
		break;

	case D_ALLOCINDIR:
		pool_put(&allocindir_pool, item);
		break;

	case D_FREEFRAG:
		pool_put(&freefrag_pool, item);
		break;

	case D_FREEBLKS:
		pool_put(&freeblks_pool, item);
		break;

	case D_FREEFILE:
		pool_put(&freefile_pool, item);
		break;

	case D_DIRADD:
		pool_put(&diradd_pool, item);
		break;

	case D_MKDIR:
		pool_put(&mkdir_pool, item);
		break;

	case D_DIRREM:
		pool_put(&dirrem_pool, item);
		break;

	default:
#ifdef DEBUG
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
#endif
		panic("softdep_free: unknown type %d", type);
	}
}

struct workhead softdep_freequeue;

static __inline void
softdep_freequeue_add(struct worklist *item)
{
	int s;

	s = splbio();
	LIST_INSERT_HEAD(&softdep_freequeue, item, wk_list);
	splx(s);
}

static __inline void
softdep_freequeue_process(void)
{
	struct worklist *wk;

	while ((wk = LIST_FIRST(&softdep_freequeue)) != NULL) {
		LIST_REMOVE(wk, wk_list);
		FREE_LOCK(&lk);
		softdep_free(wk, wk->wk_type);
		ACQUIRE_LOCK(&lk);
	}
}

/*
d396 1
a396 1
#define WORKITEM_FREE(item, type) softdep_freequeue_add((struct worklist *)item)
d401 1
a401 1
STATIC	void workitem_free __P((struct worklist *));
d405 1
a405 1
#define WORKITEM_FREE(item, type) workitem_free((struct worklist *)item)
d439 1
a439 1
workitem_free(item)
d441 1
d449 6
a454 1
	softdep_freequeue_add(item);
d488 16
a551 7
	 * First process any items on the delayed-free queue.
	 */
	ACQUIRE_LOCK(&lk);
	softdep_freequeue_process();
	FREE_LOCK(&lk);

	/*
a634 7

		/*
		 * Process any new items on the delayed-free queue.
		 */
		ACQUIRE_LOCK(&lk);
		softdep_freequeue_process();
		FREE_LOCK(&lk);
d929 2
a930 1
	pagedep = pool_get(&pagedep_pool, PR_WAITOK);
d1005 2
a1006 1
	inodedep = pool_get(&inodedep_pool, PR_WAITOK);
d1066 2
a1067 1
	newblk = pool_get(&newblk_pool, PR_WAITOK);
d1104 1
a1104 1
	    &pool_allocator_nointr);
a1107 26
	pool_init(&pagedep_pool, sizeof(struct pagedep), 0, 0, 0,
	    "pagedeppl", &pool_allocator_nointr);
	pool_init(&inodedep_pool, sizeof(struct inodedep), 0, 0, 0,
	    "inodedeppl", &pool_allocator_nointr);
	pool_init(&newblk_pool, sizeof(struct newblk), 0, 0, 0,
	    "newblkpl", &pool_allocator_nointr);
	pool_init(&bmsafemap_pool, sizeof(struct bmsafemap), 0, 0, 0,
	    "bmsafemappl", &pool_allocator_nointr);
	pool_init(&allocdirect_pool, sizeof(struct allocdirect), 0, 0, 0,
	    "allocdirectpl", &pool_allocator_nointr);
	pool_init(&indirdep_pool, sizeof(struct indirdep), 0, 0, 0,
	    "indirdeppl", &pool_allocator_nointr);
	pool_init(&allocindir_pool, sizeof(struct allocindir), 0, 0, 0,
	    "allocindirpl", &pool_allocator_nointr);
	pool_init(&freefrag_pool, sizeof(struct freefrag), 0, 0, 0,
	    "freefragpl", &pool_allocator_nointr);
	pool_init(&freeblks_pool, sizeof(struct freeblks), 0, 0, 0,
	    "freeblkspl", &pool_allocator_nointr);
	pool_init(&freefile_pool, sizeof(struct freefile), 0, 0, 0,
	    "freefilepl", &pool_allocator_nointr);
	pool_init(&diradd_pool, sizeof(struct diradd), 0, 0, 0,
	    "diraddpl", &pool_allocator_nointr);
	pool_init(&mkdir_pool, sizeof(struct mkdir), 0, 0, 0,
	    "mkdirpl", &pool_allocator_nointr);
	pool_init(&dirrem_pool, sizeof(struct dirrem), 0, 0, 0,
	    "dirrempl", &pool_allocator_nointr);
d1266 2
a1267 1
	bmsafemap = pool_get(&bmsafemap_pool, PR_WAITOK);
d1326 2
a1327 1
	adp = pool_get(&allocdirect_pool, PR_WAITOK);
d1358 1
a1358 1
	pool_put(&newblk_pool, newblk);
d1494 2
a1495 1
	freefrag = pool_get(&freefrag_pool, PR_WAITOK);
d1523 1
a1523 1
	pool_put(&freefrag_pool, freefrag);
d1563 2
a1564 1
	aip = pool_get(&allocindir_pool, PR_WAITOK);
d1680 1
a1680 1
			pool_put(&newblk_pool, newblk);
d1716 1
a1716 1
			WORKITEM_FREE(newindirdep, D_INDIRDEP);
d1720 2
a1721 1
		newindirdep = pool_get(&indirdep_pool, PR_WAITOK);
d1785 2
a1786 1
	freeblks = pool_get(&freeblks_pool, PR_WAITOK);
d2044 2
a2045 1
	freefile = pool_get(&freefile_pool, PR_WAITOK);
d2367 2
a2368 1
	dap = pool_get(&diradd_pool, PR_WAITOK);
d2379 2
a2380 1
		mkdir1 = pool_get(&mkdir_pool, PR_WAITOK);
d2384 2
a2385 1
		mkdir2 = pool_get(&mkdir_pool, PR_WAITOK);
d2621 2
a2622 1
	dirrem = pool_get(&dirrem_pool, PR_WAITOK);
d2722 2
a2723 1
		dap = pool_get(&diradd_pool, PR_WAITOK);
d3040 1
a3040 1
			    M_INDIRDEP, M_WAITOK);
d3142 1
a3142 1
		    sizeof(struct dinode), M_INODEDEP, M_WAITOK);
@


1.30.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.30.2.1 2002/01/31 22:55:49 niklas Exp $	*/
d4512 1
a4512 1
		error = (uobj->pgops->pgo_put)(uobj, 0, 0,
d4515 1
a4519 3
		if (error) {
			return error;
		}
@


1.30.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.30.2.2 2002/02/02 03:28:26 art Exp $	*/
d44 7
a61 1
#include <ufs/ufs/extattr.h>
d132 56
a187 55
STATIC	void softdep_error(char *, int);
STATIC	void drain_output(struct vnode *, int);
STATIC	int getdirtybuf(struct buf **, int);
STATIC	void clear_remove(struct proc *);
STATIC	void clear_inodedeps(struct proc *);
STATIC	int flush_pagedep_deps(struct vnode *, struct mount *,
	    struct diraddhd *);
STATIC	int flush_inodedep_deps(struct fs *, ino_t);
STATIC	int handle_written_filepage(struct pagedep *, struct buf *);
STATIC  void diradd_inode_written(struct diradd *, struct inodedep *);
STATIC	int handle_written_inodeblock(struct inodedep *, struct buf *);
STATIC	void handle_allocdirect_partdone(struct allocdirect *);
STATIC	void handle_allocindir_partdone(struct allocindir *);
STATIC	void initiate_write_filepage(struct pagedep *, struct buf *);
STATIC	void handle_written_mkdir(struct mkdir *, int);
STATIC	void initiate_write_inodeblock(struct inodedep *, struct buf *);
STATIC	void handle_workitem_freefile(struct freefile *);
STATIC	void handle_workitem_remove(struct dirrem *);
STATIC	struct dirrem *newdirrem(struct buf *, struct inode *,
	    struct inode *, int, struct dirrem **);
STATIC	void free_diradd(struct diradd *);
STATIC	void free_allocindir(struct allocindir *, struct inodedep *);
STATIC	int indir_trunc(struct inode *, ufs_daddr_t, int, ufs_lbn_t,
	    long *);
STATIC	void deallocate_dependencies(struct buf *, struct inodedep *);
STATIC	void free_allocdirect(struct allocdirectlst *,
	    struct allocdirect *, int);
STATIC	int check_inode_unwritten(struct inodedep *);
STATIC	int free_inodedep(struct inodedep *);
STATIC	void handle_workitem_freeblocks(struct freeblks *);
STATIC	void merge_inode_lists(struct inodedep *);
STATIC	void setup_allocindir_phase2(struct buf *, struct inode *,
	    struct allocindir *);
STATIC	struct allocindir *newallocindir(struct inode *, int, ufs_daddr_t,
	    ufs_daddr_t);
STATIC	void handle_workitem_freefrag(struct freefrag *);
STATIC	struct freefrag *newfreefrag(struct inode *, ufs_daddr_t, long);
STATIC	void allocdirect_merge(struct allocdirectlst *,
	    struct allocdirect *, struct allocdirect *);
STATIC	struct bmsafemap *bmsafemap_lookup(struct buf *);
STATIC	int newblk_lookup(struct fs *, ufs_daddr_t, int,
	    struct newblk **);
STATIC	int inodedep_lookup(struct fs *, ino_t, int, struct inodedep **);
STATIC	int pagedep_lookup(struct inode *, ufs_lbn_t, int,
	    struct pagedep **);
STATIC	void pause_timer(void *);
STATIC	int request_cleanup(int, int);
STATIC	int process_worklist_item(struct mount *, int);
STATIC	void add_to_worklist(struct worklist *);
STATIC	struct buf *softdep_setup_pagecache(struct inode *, ufs_lbn_t, long);
STATIC	void softdep_collect_pagecache(struct inode *);
STATIC	void softdep_free_pagecache(struct inode *);
STATIC	struct vnode *softdep_lookupvp(struct fs *, ino_t);
STATIC	struct buf *softdep_lookup_pcbp(struct vnode *, ufs_lbn_t);
void	softdep_pageiodone(struct buf *);
d192 5
a196 5
void softdep_disk_io_initiation(struct buf *);
void softdep_disk_write_complete(struct buf *);
void softdep_deallocate_dependencies(struct buf *);
void softdep_move_dependencies(struct buf *, struct buf *);
int softdep_count_dependencies(struct buf *bp, int, int);
d239 4
a242 4
STATIC	void acquire_lock(struct lockit *, int);
STATIC	void free_lock(struct lockit *, int);
STATIC	void acquire_lock_interlocked(struct lockit *, int, int);
STATIC	int free_lock_interlocked(struct lockit *, int);
d332 3
a334 3
STATIC	void sema_init(struct sema *, char *, int, int);
STATIC	int sema_get(struct sema *, struct lockit *);
STATIC	void sema_release(struct sema *);
d512 3
a514 3
STATIC	void worklist_insert(struct workhead *, struct worklist *);
STATIC	void worklist_remove(struct worklist *);
STATIC	void workitem_free(struct worklist *);
a3240 1
#ifdef DIAGNOSTIC
a3241 1
#endif
@


1.30.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.30.2.3 2002/06/11 03:32:50 art Exp $	*/
a101 1
	"freefile",
d107 1
a107 1
	((unsigned)(type) <= D_LAST ? softdep_typenames[type] : "???")
d943 2
d2293 1
a2293 1
		    baselbns[level], &blocksreleased)) != 0)
@


1.30.2.5
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.30.2.4 2002/10/29 00:36:50 art Exp $	*/
d4120 1
a4120 2
			VTOI(pvp)->i_flag |= IN_MODIFIED;
			if ((error = UFS_UPDATE(VTOI(pvp), UPDATE_WAIT))) {
a4209 1
	struct inodedep *inodedep;
d4255 4
a4258 2
	if (getdirtybuf(&LIST_FIRST(&vp->v_dirtyblkhd), MNT_WAIT) == 0)
		goto clean;
d4433 1
a4433 1
	if (LIST_FIRST(&vp->v_dirtyblkhd) != NULL) {
d4435 1
a4435 13
		/*
		 * If we are trying to sync a block device, some of its buffers
		 * may contain metadata that cannot be written until the
		 * contents of some partially written files have been written
		 * to disk. The only easy way to accomplish this is to sync the
		 * entire filesystem (luckily this happens rarely).
		 */
		if (vn_isdisk(vp, NULL) &&
		    vp->v_specmountpoint && !VOP_ISLOCKED(vp) &&
		    (error = VFS_SYNC(vp->v_specmountpoint, MNT_WAIT,
		     ap->a_cred, ap->a_p)) != 0)
			return (error);
		ACQUIRE_LOCK(&lk);
d4438 1
a4438 1
clean:
d4440 11
a4450 8
	 * If there is still an inodedep, we know that the inode has pending
	 * modifications, and we must force it to be flushed to disk.  We do
	 * this by explicitly setting IN_MODIFIED so that ffs_update() will
	 * see it.
	 */
	if (inodedep_lookup(VTOI(vp)->i_fs, VTOI(vp)->i_number, 0, &inodedep))
		VTOI(vp)->i_flag |= IN_MODIFIED;
	FREE_LOCK(&lk);
d4601 1
a4601 2
			VTOI(pvp)->i_flag |= IN_MODIFIED;
			if ((error = UFS_UPDATE(VTOI(pvp), UPDATE_WAIT)))
@


1.30.2.6
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d192 9
a1178 7

	bioops.io_start = softdep_disk_io_initiation;
	bioops.io_complete = softdep_disk_write_complete;
	bioops.io_deallocate = softdep_deallocate_dependencies;
	bioops.io_movedeps = softdep_move_dependencies;
	bioops.io_countdeps = softdep_count_dependencies;
	bioops.io_pageiodone = softdep_pageiodone;
@


1.29
log
@Make DEBUG not default, and make the default compile
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.28 2001/12/04 15:05:56 art Exp $	*/
d4365 1
a4365 1
	uobj = &vp->v_uvm.u_obj;
@


1.28
log
@FREE_LOCK_INTERLOCKED drops the lock owner without changing lock "state".
ACQUIRE_LOCK_INTERLOCKED set the owner back without validating that the state
is ok. This could lead to changed states and FREE_LOCK could set spl to
any random value.

Try to preserve the state of the lock.

XXX - all this locking in the code is insane. all this debugging code
breaks stuff.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.27 2001/11/28 01:18:10 art Exp $	*/
d44 1
a44 1
 * For now we want the safety net that the DIAGNOSTIC and DEBUG flags provide.
a48 3
#ifndef DEBUG
#define DEBUG
#endif
d370 1
d373 1
d519 1
d522 1
d2943 1
d2945 1
@


1.27
log
@Bug fixes from NetBSD.
In flush_inodedep_deps - release the lock before sleep.
When allocating a pagecache dependency buffer - make sure it's always
correctly initialized.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.26 2001/11/27 05:27:12 art Exp $	*/
d222 2
a223 2
#define ACQUIRE_LOCK_INTERLOCKED(lk)
#define FREE_LOCK_INTERLOCKED(lk)
d235 2
a236 2
STATIC	void acquire_lock_interlocked __P((struct lockit *, int));
STATIC	void free_lock_interlocked __P((struct lockit *, int));
d240 1
a240 1
#define ACQUIRE_LOCK_INTERLOCKED(lk)	acquire_lock_interlocked(lk, __LINE__)
d279 1
a279 1
acquire_lock_interlocked(lk, line)
d281 1
d298 1
d302 1
a302 1
STATIC void
d311 2
d349 1
d353 1
a353 1
			FREE_LOCK_INTERLOCKED(interlock);
d356 1
a356 1
			ACQUIRE_LOCK_INTERLOCKED(interlock);
d4619 1
d4688 1
a4688 1
	FREE_LOCK_INTERLOCKED(&lk);
d4690 1
a4690 1
	ACQUIRE_LOCK_INTERLOCKED(&lk);
d4945 1
d4955 1
a4955 1
		FREE_LOCK_INTERLOCKED(&lk);
d4957 1
a4957 1
		ACQUIRE_LOCK_INTERLOCKED(&lk);
d4975 1
d4981 1
a4981 1
		FREE_LOCK_INTERLOCKED(&lk);
d4983 1
a4983 1
		ACQUIRE_LOCK_INTERLOCKED(&lk);
@


1.26
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.25 2001/11/13 14:19:24 art Exp $	*/
d4350 8
d4386 8
a4393 10

		vp = softdep_lookupvp(fs, ino);
		if (vp) {
			struct uvm_object *uobj = &vp->v_uvm.u_obj;

			simple_lock(&uobj->vmobjlock);
			(uobj->pgops->pgo_flush)(uobj, 0, 0,
			    PGO_ALLPAGES|PGO_CLEANIT|
			    (waitfor == MNT_NOWAIT ? 0: PGO_SYNCIO));
			simple_unlock(&uobj->vmobjlock);
d4395 1
d5025 2
a5036 1
		bp->b_bcount = bp->b_resid = size;
a5039 4
	} else {
		KASSERT(size >= bp->b_bcount);
		bp->b_resid += size - bp->b_bcount;
		bp->b_bcount = size;
d5041 1
@


1.25
log
@Add two controlled panics instead of some code that I need to write later.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.24 2001/11/06 19:53:21 miod Exp $	*/
d59 1
d73 4
d117 7
d175 7
d198 1
d1078 1
d1097 5
d1354 8
d1363 1
a1363 4
		/*
		 * XXXUBC - Yes, I know how to fix this, but not right now.
		 */
		panic("softdep_setup_allocdirect: Bonk art in the head\n");
d1597 1
a1597 4
		/*
		 * XXXUBC - Yes, I know how to fix this, but not right now.
		 */
		panic("softdep_setup_allocindir_page: Bonk art in the head\n");
d1776 1
d1836 5
d1844 1
a1855 1
	vp = ITOV(ip);
d1867 1
a2935 1
	struct vnode vp;
d2951 1
a2951 2
	tip.i_vnode = &vp;
	vp.v_data = &tip;
d4349 1
d4369 21
d5001 193
@


1.24
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.23 2001/06/23 02:07:54 csapuntz Exp $	*/
d1328 6
d1565 6
@


1.23
log
@Privatize several vnode operations that are not used by the generic code.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.22 2001/04/06 18:59:16 gluk Exp $	*/
d465 1
a465 1
#include <vm/vm.h>
@


1.22
log
@Change softdep_count_dependencies interface so that it may be called
from interrupt at splbio.

costa@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.21 2001/04/04 20:19:03 gluk Exp $	*/
d2835 1
a2835 1
	if ((error = VOP_TRUNCATE(vp, (off_t)0, 0, p->p_ucred, p)) != 0)
a2906 2
	{
		struct vop_vfree_args  vargs;
d2908 3
a2910 6
		vargs.a_pvp = &vp;
		vargs.a_ino = freefile->fx_oldinum;
		vargs.a_mode = freefile->fx_mode;

		if ((error = ffs_freefile(&vargs)) != 0)
			softdep_error("handle_workitem_freefile", error);
a3892 1
	struct timespec ts;
d3957 1
a3957 2
			TIMEVAL_TO_TIMESPEC(&time, &ts);
			if ((error = VOP_UPDATE(pvp, &ts, &ts, MNT_WAIT))) {
a4393 1
	struct timespec ts;
a4405 1
			TIMEVAL_TO_TIMESPEC(&time, &ts);
d4407 1
a4407 1
			if ((error = VOP_UPDATE(pvp, &ts, &ts, MNT_WAIT)))
@


1.21
log
@Add "softdep" option to mount. Update from rw/async to softdep
and otherwise are disabled.

art@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.20 2001/03/08 10:56:47 art Exp $	*/
d171 1
a171 1
int softdep_count_dependencies __P((struct buf *bp, int));
d4783 1
a4783 1
softdep_count_dependencies(bp, wantcount)
d4786 1
d4797 2
a4798 1
	ACQUIRE_LOCK(&lk);
d4850 2
a4851 1
			FREE_LOCK(&lk);
d4858 2
a4859 1
	FREE_LOCK(&lk);
@


1.20
log
@Be more careful about who we treat as the syncer.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.19 2001/03/04 07:00:33 csapuntz Exp $	*/
a1093 2
	mp->mnt_flag &= ~MNT_ASYNC;
	mp->mnt_flag |= MNT_SOFTDEP;
d1099 1
a1099 1
	if (fs->fs_clean != 0)
@


1.19
log
@Better diagnostics for locking errors. Fix panic before panic in
acquire_lock_interlocked. Thanks to Grigoriy Orlov for triggering this
one.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.18 2001/03/04 06:46:31 csapuntz Exp $	*/
d528 3
d532 1
a532 1
	filesys_syncer = p;
@


1.18
log
@Fix use of timeouts. Thanks to Grigoriy Orlov (gluk)
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.17 2001/02/27 09:07:54 csapuntz Exp $	*/
d236 1
a236 1
			panic("softdep_lock: locking against myself, acquired at line %d", original_line);
d238 1
a238 1
			panic("softdep_lock: lock held by %d, acquired at line %d", holder, original_line);
d269 1
a269 1
		FREE_LOCK(lk);
d271 1
a271 1
			panic("softdep_lock: locking against myself, acquired at line %d", original_line);
d273 1
a273 1
			panic("softdep_lock: lock held by %d, acquired at line %d", holder, original_line);
@


1.17
log
@

Add wakeup_n and wakeup_one. wakeup_n will wakeup up to n sleeping processes
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.16 2001/02/26 17:54:07 csapuntz Exp $	*/
d1072 1
d4613 1
a4613 2
	if (!timeout_initialized(&proc_waiting_timeout)) {
		timeout_set(&proc_waiting_timeout, pause_timer, 0);
a4614 1
	}
a4637 2
	else
		timeout_del(&proc_waiting_timeout);
@


1.16
log
@

Move #define STATIC
Get rid of unnecessary comment to self
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.15 2001/02/26 17:43:34 csapuntz Exp $	*/
a479 9

void wakeup_one __P((void *));

void
wakeup_one(c) 
	void *c;
{
	wakeup(c);
}
@


1.15
log
@

Fix a couple panics caused by not freeing locks at the right times.
Thanks gluk for the patch

Free locks before calling panic
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.14 2001/02/24 19:07:09 csapuntz Exp $	*/
a50 1
#define STATIC
d72 1
a4448 1
		/* XXXCPS - figure out locking here */
@


1.14
log
@

Cleanup of vnode interface continues. Get rid of VHOLD/HOLDRELE.
Change VM/UVM to use buf_replacevnode to change the vnode associated
with a buffer.

Addition v_bioflag for flags written in interrupt handlers
(and read at splbio, though not strictly necessary)

Add vwaitforio and use it instead of a while loop of v_numoutput.

Fix race conditions when manipulation vnode free list
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.13 2001/02/23 14:52:51 csapuntz Exp $	*/
d40 1
a40 1
 * $FreeBSD: src/sys/ufs/ffs/ffs_softdep.c,v 1.84 2001/02/04 16:08:18 phk Exp $
d228 2
d232 5
a236 2
		if (lk->lkt_held == CURPROC->p_pid)
			panic("softdep_lock: locking against myself, acquired at line %d", lk->lkt_line);
d238 1
a238 1
			panic("softdep_lock: lock held by %d, acquired at line %d", lk->lkt_held, line);
d263 2
d267 5
a271 2
		if (lk->lkt_held == CURPROC->p_pid)
			panic("softdep_lock: locking against myself, acquired at line %d", lk->lkt_line);
d273 1
a273 1
			panic("softdep_lock: lock held by %d, acquired at line %d", lk->lkt_held, lk->lkt_line);
d347 3
a349 1
	if (semap->value <= 0 || semap->holder != CURPROC->p_pid)
d351 1
d391 2
a392 1
	if (item->wk_state & ONWORKLIST)
d394 1
d406 2
a407 1
	if ((item->wk_state & ONWORKLIST) == 0)
d409 1
d420 3
a422 1
	if (item->wk_state & ONWORKLIST)
d424 4
a427 1
	if (item->wk_type != type)
d429 1
d484 1
a484 1
wakeup_one(c)
d503 3
a505 1
	if (wk->wk_state & ONWORKLIST)
d507 1
d610 1
a610 1
			if (diff.tv_sec > 0 && matchmnt == NULL) {
d654 2
a655 1
	if (wk == 0)
d657 1
d1067 1
d1070 3
d1183 2
a1184 1
	    != 0)
d1186 1
d1338 2
a1339 1
		if (oldblkno != 0)
d1341 1
d1380 2
a1381 1
	if (oldadp == NULL)
d1383 1
d1409 2
a1410 1
	    newadp->ad_lbn >= NDADDR)
d1414 1
d1657 2
a1658 1
				if (oldaip->ai_newblkno != aip->ai_oldblkno)
d1660 1
d1782 2
a1783 1
	if ((inodedep->id_state & IOSTARTED) != 0)
d1785 1
d1879 2
a1880 1
			if (indirdep->ir_state & GOINGAWAY)
d1882 1
d1887 2
a1888 1
			    bp->b_blkno != indirdep->ir_savebp->b_lblkno)
d1890 1
d1937 1
d1943 1
d2061 2
a2062 1
	if (free_inodedep(inodedep) == 0)
d2064 1
d2202 2
a2203 1
		    (indirdep->ir_state & GOINGAWAY) == 0)
d2205 1
d2208 2
a2209 1
		if (LIST_FIRST(&bp->b_dep) != NULL)
d2211 1
d2479 2
a2480 1
		if ((dap->da_state & (MKDIR_PARENT | MKDIR_BODY)) != 0)
d2482 1
d2611 2
a2612 1
	if ((dap->da_state & ATTACHED) == 0)
d2614 3
a2616 1
	if (dap->da_newinum != ip->i_number)
d2619 1
d2778 2
a2779 1
	if (ip->i_ffs_nlink < ip->i_effnlink)
d2781 1
d2807 3
a2809 1
	if ((inodedep_lookup(ip->i_fs, dirrem->dm_oldinum, 0, &inodedep)) == 0)
d2811 1
d2818 2
a2819 1
		if (ip->i_ffs_nlink < ip->i_effnlink)
d2821 1
d2904 3
a2906 1
	if (inodedep_lookup(fs, freefile->fx_oldinum, 0, &idp))
a2907 1
	FREE_LOCK(&lk);
d3053 2
a3054 1
			if (ep->d_ino != dap->da_newinum)
d3058 1
d3119 2
a3120 1
		if (deplist != 0 && prevlbn >= adp->ad_lbn)
d3122 1
d3125 2
a3126 1
		    dp->di_db[adp->ad_lbn] != adp->ad_newblkno)
d3130 1
d3132 2
a3133 1
		    dp->di_ib[adp->ad_lbn - NDADDR] != adp->ad_newblkno)
d3137 1
d3139 2
a3140 1
		if ((adp->ad_state & ATTACHED) == 0)
d3143 1
d3165 2
a3166 1
			if (dp->di_db[i] != 0 && (deplist & (1 << i)) == 0)
d3168 1
d3175 2
a3176 1
			    (deplist & ((1 << NDADDR) << i)) == 0)
d3178 1
d3358 1
d3797 1
a3799 1
		FREE_LOCK(&lk);
d3802 2
a3803 1
	if (inodedep->id_nlinkdelta != ip->i_ffs_nlink - ip->i_effnlink)
d3805 1
d3917 2
a3918 1
	    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL)
d3920 1
d3924 2
a3925 1
		if (wk->wk_type != D_DIRADD)
d3928 1
d3941 2
a3942 1
		if ((dap->da_state & (MKDIR_BODY | COMPLETE)) != COMPLETE)
d3944 1
d4018 2
a4019 1
		if ((bp->b_flags & B_DELWRI) == 0)
d4021 1
d4246 1
d4431 2
a4432 1
			if (dap->da_state & MKDIR_PARENT)
d4434 1
d4449 1
d4467 2
a4468 1
			if (dap->da_state & MKDIR_BODY)
d4470 1
d4482 2
a4483 1
		if (inodedep_lookup(ump->um_fs, inum, 0, &inodedep) == 0)
d4485 1
d4516 2
a4517 1
		if (dap == LIST_FIRST(diraddhdp))
d4519 1
d4611 2
d4734 4
d4860 1
@


1.13
log
@

Change the B_DELWRI flag using buf_dirty and buf_undirty instead of
manually twiddling it. This allows the buffer cache to more easily
keep track of dirty buffers and decide when it is appropriate to speed
up the syncer.

Insipired by FreeBSD.
Look over by art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.12 2001/02/21 23:24:31 csapuntz Exp $	*/
d4799 1
a4799 1

d4803 1
a4803 1
		vp->v_flag |= VBWAIT;
d4805 1
a4805 1
		tsleep((caddr_t)&vp->v_numoutput, PRIBIO + 1, "drainvp", 0);
@


1.12
log
@

Latest soft updates from FreeBSD/Kirk McKusick

Snapshot-related code has been commented out.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.11 2001/02/10 11:08:39 fgsch Exp $	*/
d3234 1
a3234 1
			bdirty(bp);
d3393 1
a3393 1
		bdirty(bp);
d3442 1
a3442 1
		bdirty(bp);
d3627 1
a3627 1
		bdirty(bp);
@


1.11
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d3 1
a3 1
 * Copyright 1998 Marshall Kirk McKusick. All Rights Reserved.
d10 1
a10 5
 * The following are the copyrights and redistribution conditions that
 * apply to this copy of the soft update software. For a license
 * to use, redistribute or sell the soft update software under
 * conditions other than those described here, please contact the
 * author at one of the following addresses:
d12 3
a14 3
 *	Marshall Kirk McKusick		mckusick@@mckusick.com
 *	1614 Oxford Street		+1-510-843-9542
 *	Berkeley, CA 94709-1608
a25 13
 * 3. None of the names of McKusick, Ganger, Patt, or the University of
 *    Michigan may be used to endorse or promote products derived from
 *    this software without specific prior written permission.
 * 4. Redistributions in any form must be accompanied by information on
 *    how to obtain complete source code for any accompanying software
 *    that uses this software. This source code must either be included
 *    in the distribution or be available for no more than the cost of
 *    distribution plus a nominal fee, and must be freely redistributable
 *    under reasonable conditions. For an executable file, complete
 *    source code means the source code for all modules it contains.
 *    It does not mean source code for modules or files that typically
 *    accompany the operating system on which the executable file runs,
 *    e.g., standard library modules or system header files.
d39 2
a40 1
 *	@@(#)ffs_softdep.c	9.40 (McKusick) 6/15/99
d51 1
d73 1
d78 3
d114 6
a119 6
static	void softdep_error __P((char *, int));
static	void drain_output __P((struct vnode *, int));
static	int getdirtybuf __P((struct buf **, int));
static	void clear_remove __P((struct proc *));
static	void clear_inodedeps __P((struct proc *));
static	int flush_pagedep_deps __P((struct vnode *, struct mount *,
d121 16
a136 16
static	int flush_inodedep_deps __P((struct fs *, ino_t));
static	int handle_written_filepage __P((struct pagedep *, struct buf *));
static  void diradd_inode_written __P((struct diradd *, struct inodedep *));
static	int handle_written_inodeblock __P((struct inodedep *, struct buf *));
static	void handle_allocdirect_partdone __P((struct allocdirect *));
static	void handle_allocindir_partdone __P((struct allocindir *));
static	void initiate_write_filepage __P((struct pagedep *, struct buf *));
static	void handle_written_mkdir __P((struct mkdir *, int));
static	void initiate_write_inodeblock __P((struct inodedep *, struct buf *));
static	void handle_workitem_freefile __P((struct freefile *));
static	void handle_workitem_remove __P((struct dirrem *));
static	struct dirrem *newdirrem __P((struct buf *, struct inode *,
	    struct inode *, int));
static	void free_diradd __P((struct diradd *));
static	void free_allocindir __P((struct allocindir *, struct inodedep *));
static	int indir_trunc __P((struct inode *, ufs_daddr_t, int, ufs_lbn_t,
d138 2
a139 2
static	void deallocate_dependencies __P((struct buf *, struct inodedep *));
static	void free_allocdirect __P((struct allocdirectlst *,
d141 5
a145 4
static	int free_inodedep __P((struct inodedep *));
static	void handle_workitem_freeblocks __P((struct freeblks *));
static	void merge_inode_lists __P((struct inodedep *));
static	void setup_allocindir_phase2 __P((struct buf *, struct inode *,
d147 1
a147 1
static	struct allocindir *newallocindir __P((struct inode *, int, ufs_daddr_t,
d149 3
a151 3
static	void handle_workitem_freefrag __P((struct freefrag *));
static	struct freefrag *newfreefrag __P((struct inode *, ufs_daddr_t, long));
static	void allocdirect_merge __P((struct allocdirectlst *,
d153 2
a154 2
static	struct bmsafemap *bmsafemap_lookup __P((struct buf *));
static	int newblk_lookup __P((struct fs *, ufs_daddr_t, int,
d156 2
a157 2
static	int inodedep_lookup __P((struct fs *, ino_t, int, struct inodedep **));
static	int pagedep_lookup __P((struct inode *, ufs_lbn_t, int,
d159 4
a162 2
static	int request_cleanup __P((int, int));
static	void add_to_worklist __P((struct worklist *));
d167 6
d177 2
a178 2
	softdep_fsync,				/* io_fsync */
	softdep_process_worklist,		/* io_sync */
d197 1
a197 1
static struct lockit {
d206 1
a206 1
static struct lockit {
d209 1
d211 1
a211 1
static int lockcnt;
d213 9
a221 9
static	void acquire_lock __P((struct lockit *));
static	void free_lock __P((struct lockit *));
static	void acquire_lock_interlocked __P((struct lockit *));
static	void free_lock_interlocked __P((struct lockit *));

#define ACQUIRE_LOCK(lk)		acquire_lock(lk)
#define FREE_LOCK(lk)			free_lock(lk)
#define ACQUIRE_LOCK_INTERLOCKED(lk)	acquire_lock_interlocked(lk)
#define FREE_LOCK_INTERLOCKED(lk)	free_lock_interlocked(lk)
d223 2
a224 2
static void
acquire_lock(lk)
d226 1
d231 1
a231 1
			panic("softdep_lock: locking against myself");
d233 1
a233 1
			panic("softdep_lock: lock held by %d", lk->lkt_held);
d237 1
d241 2
a242 2
static void
free_lock(lk)
d244 1
d248 1
a248 1
		panic("softdep_unlock: lock not held");
d253 2
a254 2
static void
acquire_lock_interlocked(lk)
d256 1
d261 1
a261 1
			panic("softdep_lock_interlocked: locking against self");
d263 1
a263 2
			panic("softdep_lock_interlocked: lock held by %d",
			    lk->lkt_held);
d266 1
d270 2
a271 2
static void
free_lock_interlocked(lk)
d273 1
d277 1
a277 1
		panic("softdep_unlock_interlocked: lock not held");
d292 3
a294 3
static	void sema_init __P((struct sema *, char *, int, int));
static	int sema_get __P((struct sema *, struct lockit *));
static	void sema_release __P((struct sema *));
d296 1
a296 1
static void
d310 1
a310 1
static int
d332 1
a332 1
static void
d362 3
a364 3
static	void worklist_insert __P((struct workhead *, struct worklist *));
static	void worklist_remove __P((struct worklist *));
static	void workitem_free __P((struct worklist *, int));
d370 1
a370 1
static void
d384 1
a384 1
static void
d397 1
a397 1
static void
d414 11
a424 7
static struct workhead softdep_workitem_pending;
static int softdep_worklist_busy;
static int max_softdeps;	/* maximum number of structs before slowdown */
static int tickdelay = 2;	/* number of ticks to pause during slowdown */
static int proc_waiting;	/* tracks whether we have a timeout posted */
static struct proc *filesys_syncer; /* proc of filesystem syncer process */
static int req_clear_inodedeps;	/* syncer process flush some inodedeps */
d426 1
a426 1
static int req_clear_remove;	/* syncer process flush some freeblks */
d431 10
a440 8
static int stat_blk_limit_push;	/* number of times block limit neared */
static int stat_ino_limit_push;	/* number of times inode limit neared */
static int stat_blk_limit_hit;	/* number of times block slowdown imposed */
static int stat_ino_limit_hit;	/* number of times inode slowdown imposed */
static int stat_indir_blk_ptrs;	/* bufs redirtied as indir ptrs not written */
static int stat_inode_bitmap;	/* bufs redirtied as inode bitmap not written */
static int stat_direct_blk_ptrs;/* bufs redirtied as direct ptrs not written */
static int stat_dir_entry;	/* bufs redirtied as dir entry cannot write */
d446 1
d451 5
a455 4
struct ctldebug debug27 = { "indir_blk_ptrs", &stat_indir_blk_ptrs };
struct ctldebug debug28 = { "inode_bitmap", &stat_inode_bitmap };
struct ctldebug debug29 = { "direct_blk_ptrs", &stat_direct_blk_ptrs };
struct ctldebug debug30 = { "dir_entry", &stat_dir_entry };
d458 9
d474 1
a474 1
static void
d488 1
d505 2
a506 3
	struct worklist *wk;
	struct fs *matchfs;
	int matchcnt;
d514 1
a514 3
	matchfs = NULL;
	if (matchmnt != NULL)
		matchfs = VFSTOUFS(matchmnt)->um_fs;
d517 2
a518 2
	 * code. It is single threaded solely so that softdep_flushfiles
	 * (below) can get an accurate count of the number of items
d521 6
a526 2
	if (softdep_worklist_busy && matchmnt == NULL)
		return (-1);
d532 2
a533 2
		req_clear_inodedeps = 0;
		wakeup(&proc_waiting);
d537 2
a538 2
		req_clear_remove = 0;
		wakeup(&proc_waiting);
d540 4
a543 5
	ACQUIRE_LOCK(&lk);
	while ((wk = LIST_FIRST(&softdep_workitem_pending)) != 0) {
		WORKLIST_REMOVE(wk);
		FREE_LOCK(&lk);
		switch (wk->wk_type) {
d545 6
a550 19
		case D_DIRREM:
			/* removal of a directory entry */
			if (WK_DIRREM(wk)->dm_mnt == matchmnt)
				matchcnt += 1;
			handle_workitem_remove(WK_DIRREM(wk));
			break;

		case D_FREEBLKS:
			/* releasing blocks and/or fragments from a file */
			if (WK_FREEBLKS(wk)->fb_fs == matchfs)
				matchcnt += 1;
			handle_workitem_freeblocks(WK_FREEBLKS(wk));
			break;

		case D_FREEFRAG:
			/* releasing a fragment when replaced as a file grows */
			if (WK_FREEFRAG(wk)->ff_fs == matchfs)
				matchcnt += 1;
			handle_workitem_freefrag(WK_FREEFRAG(wk));
d552 1
a553 14
		case D_FREEFILE:
			/* releasing an inode when its link count drops to 0 */
			if (WK_FREEFILE(wk)->fx_fs == matchfs)
				matchcnt += 1;
			handle_workitem_freefile(WK_FREEFILE(wk));
			break;

		default:
			panic("%s_process_worklist: Unknown type %s",
			    "softdep", TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
		if (softdep_worklist_busy && matchmnt == NULL)
			return (-1);
d559 2
a560 2
			req_clear_inodedeps = 0;
			wakeup(&proc_waiting);
d564 2
a565 2
			req_clear_remove = 0;
			wakeup(&proc_waiting);
d567 60
a626 1
		ACQUIRE_LOCK(&lk);
d628 4
d633 59
d696 25
d724 1
a724 1
softdep_flushfiles(oldmnt, flags, p)
d726 1
a726 1
	int flags;
d730 1
a730 1
	int error, loopcnt;
d733 1
a733 1
	 * Await our turn to clear out the queue.
d735 4
a738 6
	while (softdep_worklist_busy)
		tsleep(&lbolt, PRIBIO, "softflush", 0);
	softdep_worklist_busy = 1;
	if ((error = ffs_flushfiles(oldmnt, flags, p)) != 0) {
		softdep_worklist_busy = 0;
		return (error);
d740 1
d744 2
a745 2
	 * creates. In theory, this loop can happen at most twice,
	 * but we give it a few extra just to be sure.
d747 1
d749 2
a750 14
	for (loopcnt = 10; loopcnt > 0; loopcnt--) {
		if (softdep_process_worklist(oldmnt) == 0) {
			/*
			 * Do another flush in case any vnodes were brought in
			 * as part of the cleanup operations.
			 */
			if ((error = ffs_flushfiles(oldmnt, flags, p)) != 0)
				break;
			/*
			 * If we still found nothing to do, we are really done.
			 */
			if (softdep_process_worklist(oldmnt) == 0)
				break;
		}
d758 33
d827 1
d837 1
a837 1
static struct sema pagedep_in_progress;
d845 1
a845 1
static int
d864 1
a864 2
	for (pagedep = LIST_FIRST(pagedephd); pagedep;
	     pagedep = LIST_NEXT(pagedep, pd_hash))
d882 1
a882 1
		M_WAITOK);
d903 2
a904 2
static u_long	inodedep_hash;	/* size of hash table - 1 */
static long	num_inodedep;	/* number of inodedep allocated */
d907 1
a907 1
static struct sema inodedep_in_progress;
d915 1
a915 1
static int
d933 1
a933 2
	for (inodedep = LIST_FIRST(inodedephd); inodedep;
	     inodedep = LIST_NEXT(inodedep, id_hash))
d947 1
a947 1
	if (num_inodedep > max_softdeps && firsttry && speedup_syncer() == 0 &&
d958 1
a958 1
		M_INODEDEP, M_WAITOK);
d986 1
a986 1
static struct sema newblk_in_progress;
d993 1
a993 1
static int
d1005 1
a1005 2
	for (newblk = LIST_FIRST(newblkhd); newblk;
	     newblk = LIST_NEXT(newblk, nb_hash))
d1019 1
a1019 1
		M_NEWBLK, M_WAITOK);
d1039 2
a1040 1
	max_softdeps = desiredvnodes * (16 / sizeof(register_t));
d1067 1
d1150 2
a1151 1
	if (inodedep_lookup(ip->i_fs, newinum, DEPALLOC, &inodedep) != 0)
d1192 1
a1192 1
static struct bmsafemap *
d1203 1
a1203 1
	for (wk = LIST_FIRST(&bp->b_dep); wk; wk = LIST_NEXT(wk, wk_list))
d1208 1
a1208 1
		M_BMSAFEMAP, M_WAITOK);
d1268 1
a1268 1
		M_ALLOCDIRECT, M_WAITOK);
d1286 1
a1286 1
	(void) inodedep_lookup(ip->i_fs, ip->i_number, DEPALLOC, &inodedep);
d1340 1
a1340 2
	for (oldadp = TAILQ_FIRST(adphead); oldadp;
	     oldadp = TAILQ_NEXT(oldadp, ad_next)) {
d1357 1
a1357 1
static void
d1404 1
a1404 1
static struct freefrag *
d1419 1
a1419 1
		M_FREEFRAG, M_WAITOK);
d1423 1
a1423 1
	freefrag->ff_fs = fs;
d1434 1
a1434 1
static void 
d1440 2
a1441 1
	tip.i_fs = freefrag->ff_fs;
d1478 1
a1478 1
static struct allocindir *
d1488 2
a1489 2
		M_ALLOCINDIR, M_WAITOK);
	bzero(aip, sizeof(struct allocindir));
d1556 1
a1556 1
static void 
d1573 1
a1573 2
		for (wk = LIST_FIRST(&bp->b_dep); wk;
		     wk = LIST_NEXT(wk, wk_list)) {
d1611 2
a1612 2
				for (oldaip=LIST_FIRST(&indirdep->ir_deplisthd);
				    oldaip; oldaip = LIST_NEXT(oldaip, ai_next))
d1615 1
d1620 3
a1622 3
				freefrag = oldaip->ai_freefrag;
				oldaip->ai_freefrag = aip->ai_freefrag;
				aip->ai_freefrag = freefrag;
d1629 2
d1640 1
a1640 1
			M_INDIRDEP, M_WAITOK);
d1645 4
a1648 1
		newindirdep->ir_saveddata = (ufs_daddr_t *)bp->b_data;
d1651 4
a1654 2
		bcopy((caddr_t)newindirdep->ir_saveddata,
		    newindirdep->ir_savebp->b_data, bp->b_bcount);
a1686 1
static long num_freeblks;	/* number of freeblks allocated */
d1698 1
a1698 1
	int i, error;
d1702 1
a1702 7
		panic("softde_setup_freeblocks: non-zero length");
	/*
	 * If we are over our limit, try to improve the situation.
	 */
	if (num_freeblks > max_softdeps / 2 && speedup_syncer() == 0)
		(void) request_cleanup(FLUSH_REMOVE, 0);
	num_freeblks += 1;
d1704 1
a1704 1
		M_FREEBLKS, M_WAITOK);
d1710 1
a1710 1
	freeblks->fb_fs = fs;
d1744 8
a1751 3
	 * must await the zero'ed inode being written to disk.
	 */
	WORKLIST_INSERT(&inodedep->id_bufwait, &freeblks->fb_list);
d1758 2
d1763 1
a1763 1
		free_allocdirect(&inodedep->id_inoupdt, adp, 1);
d1779 1
a1779 1
		bp->b_flags |= B_INVAL;
d1784 3
d1788 3
a1790 1
	 * Try freeing the inodedep in case that was the last dependency.
d1792 2
a1793 3
	if ((inodedep_lookup(fs, ip->i_number, 0, &inodedep)) != 0)
		(void) free_inodedep(inodedep);
	FREE_LOCK(&lk);
d1803 1
a1803 1
static void
d1867 1
a1867 2
			for (dirrem = LIST_FIRST(&pagedep->pd_dirremhd); dirrem;
			     dirrem = LIST_NEXT(dirrem, dm_next)) {
d1870 3
a1872 1
				if (inodedep == NULL)
d1905 1
a1905 1
static void
a1934 1
static long num_freefile;	/* number of freefile allocated */
d1936 4
a1939 6
softdep_freefile(ap)
	struct vop_vfree_args /* {
		struct vnode *a_pvp;
		ino_t a_ino;
		int a_mode;
	} */ *ap;
d1941 1
a1941 1
	struct inode *ip = VTOI(ap->a_pvp);
a1945 5
	 * If we are over our limit, try to improve the situation.
	 */
	if (num_freefile > max_softdeps / 2 && speedup_syncer() == 0)
		(void) request_cleanup(FLUSH_REMOVE, 0);
	/*
a1947 1
	num_freefile += 1;
d1949 1
a1949 1
		M_FREEFILE, M_WAITOK);
d1952 2
a1953 2
	freefile->fx_mode = ap->a_mode;
	freefile->fx_oldinum = ap->a_ino;
d1955 1
a1955 1
	freefile->fx_fs = ip->i_fs;
d1959 3
a1961 1
	 * been written to disk and we can free the file immediately.
d1964 2
a1965 2
	if (inodedep_lookup(ip->i_fs, ap->a_ino, 0, &inodedep) == 0) {
		add_to_worklist(&freefile->fx_list);
d1967 1
d1970 3
d1974 37
a2010 20
	/*
	 * If we still have a bitmap dependency, then the inode has never
	 * been written to disk. Drop the dependency as it is no longer
	 * necessary since the inode is being deallocated. We could process
	 * the freefile immediately, but then we would have to clear the
	 * id_inowait dependencies here and it is easier just to let the
	 * zero'ed inode be written and let them be cleaned up in the
	 * normal followup actions that follow the inode write.
	 */
	 if ((inodedep->id_state & DEPCOMPLETE) == 0) {
		inodedep->id_state |= DEPCOMPLETE;
		LIST_REMOVE(inodedep, id_deps);
		inodedep->id_buf = NULL;
	}
	/*
	 * If the inodedep has no dependencies associated with it,
	 * then we must free it here and free the file immediately.
	 * This case arises when an early allocation fails (for
	 * example, the user is over their file quota).
	 */
d2012 2
a2013 4
		WORKLIST_INSERT(&inodedep->id_inowait, &freefile->fx_list);
	else
		add_to_worklist(&freefile->fx_list);
	FREE_LOCK(&lk);
d2019 1
a2019 1
static int
d2047 1
a2047 1
static void
d2059 1
a2062 1
	tip.i_fs = freeblks->fb_fs;
d2065 1
a2065 1
	fs = freeblks->fb_fs;
d2099 1
a2099 1
		panic("handle_workitem_freeblocks: block count");
a2103 1
	num_freeblks -= 1;
d2112 1
a2112 1
static int
d2179 1
a2179 1
	bp->b_flags |= B_INVAL;
d2188 1
a2188 1
static void
d2265 3
a2267 2
	MALLOC(dap, struct diradd *, sizeof(struct diradd), M_DIRADD, M_WAITOK);
	bzero(dap, sizeof(struct diradd));
d2278 1
a2278 1
		    M_WAITOK);
d2283 1
a2283 1
		    M_WAITOK);
d2357 2
a2358 2
	for (dap = LIST_FIRST(&pagedep->pd_diraddhd[DIRADDHASH(oldoffset)]);
	     dap; dap = LIST_NEXT(dap, da_pdlist)) {
d2370 2
a2371 2
		for (dap = LIST_FIRST(&pagedep->pd_pendinghd);
		     dap; dap = LIST_NEXT(dap, da_pdlist)) {
d2387 1
a2387 1
static void
d2453 1
a2453 1
	struct dirrem *dirrem;
d2458 13
a2470 1
	dirrem = newdirrem(bp, dp, ip, isrmdir);
d2474 1
d2476 3
d2480 2
a2481 1
		add_to_worklist(&dirrem->dm_list);
a2482 1
	FREE_LOCK(&lk);
d2489 3
a2491 2
static struct dirrem *
newdirrem(bp, dp, ip, isrmdir)
d2496 1
d2509 8
d2518 2
a2519 2
		M_DIRREM, M_WAITOK);
	bzero(dirrem, sizeof(struct dirrem));
d2524 1
d2538 2
a2539 2
	for (dap = LIST_FIRST(&pagedep->pd_diraddhd[DIRADDHASH(offset)]);
	     dap; dap = LIST_NEXT(dap, da_pdlist))
d2543 2
a2544 2
		for (dap = LIST_FIRST(&pagedep->pd_pendinghd);
		     dap; dap = LIST_NEXT(dap, da_pdlist))
d2551 1
a2551 1
	 * Must be ATTACHED at this point, so just delete it.
d2558 15
a2573 1
	dirrem->dm_state |= COMPLETE;
d2604 1
a2604 1
	struct dirrem *dirrem;
d2615 2
a2616 2
		    M_DIRADD, M_WAITOK);
		bzero(dap, sizeof(struct diradd));
d2626 1
a2626 1
	dirrem = newdirrem(bp, dp, ip, isrmdir);
d2660 25
a2688 1
	dap->da_previous = dirrem;
a2698 12
	/*
	 * If the previous inode was never written or its previous directory
	 * entry was never written, then we do not want to roll back to this
	 * previous value. Instead we want to roll back to zero and immediately
	 * free the unwritten or unreferenced inode.
	 */
	if (dirrem->dm_state & COMPLETE) {
		dap->da_state &= ~DIRCHG;
		dap->da_pagedep = pagedep;
		dirrem->dm_dirinum = pagedep->pd_ino;
		add_to_worklist(&dirrem->dm_list);
	}
d2703 1
a2703 1
 * Called whenever the link count on an inode is increased.
d2709 1
a2709 1
softdep_increase_linkcnt(ip)
d2716 3
d2726 1
a2726 1
static void 
d2734 1
d2742 3
d2750 1
d2753 2
a2754 1
		ip->i_flag |= IN_CHANGE;
d2756 1
d2768 1
d2771 2
a2772 1
	ip->i_flag |= IN_CHANGE;
d2782 1
d2786 6
a2792 2
	(void) inodedep_lookup(ip->i_fs, dirrem->dm_oldinum, DEPALLOC,
	    &inodedep);
d2794 1
d2796 7
d2822 1
a2822 1
static void 
d2826 1
a2829 1
	struct vop_vfree_args args;
d2832 1
d2835 1
a2835 1
	if (inodedep_lookup(freefile->fx_fs, freefile->fx_oldinum, 0, &idp))
d2841 2
a2842 1
	tip.i_fs = freefile->fx_fs;
d2844 10
a2853 5
	args.a_pvp = &vp;
	args.a_ino = freefile->fx_oldinum;
	args.a_mode = freefile->fx_mode;
	if ((error = ffs_freefile(&args)) != 0)
		softdep_error("handle_workitem_freefile", error);
a2854 1
	num_freefile -= 1;
d2917 1
a2917 1
				indirdep->ir_savebp->b_flags |= B_INVAL;
d2928 2
d2933 3
a2935 1
			bp->b_data = indirdep->ir_savebp->b_data;
d2959 1
a2959 1
static void
d2980 1
a2980 2
		for (dap = LIST_FIRST(&pagedep->pd_diraddhd[i]); dap;
		     dap = LIST_NEXT(dap, da_pdlist)) {
d3004 1
a3004 1
static void 
d3029 1
a3029 1
		    sizeof(struct dinode), M_INODEDEP, M_WAITOK);
d3221 3
a3223 1
			bp->b_data = (caddr_t)indirdep->ir_saveddata;
d3262 1
a3262 1
static void 
d3268 1
a3268 1
	long bsize;
d3285 1
a3285 2
	for (listadp = TAILQ_FIRST(&inodedep->id_inoupdt); listadp;
	     listadp = TAILQ_NEXT(listadp, ad_next)) {
d3304 1
a3304 2
		for (listadp = TAILQ_FIRST(&inodedep->id_newinoupdt); listadp;
		     listadp = TAILQ_NEXT(listadp, ad_next))
d3316 3
d3320 1
d3325 1
a3325 1
		free_allocdirect(&inodedep->id_inoupdt, adp, 1);
d3334 1
a3334 1
static void
d3364 1
a3364 1
static int 
d3511 1
a3511 1
static void
d3533 1
a3533 1
static void
d3566 1
a3566 1
static int 
d3679 1
a3679 6
	if (inodedep->id_nlinkdelta != 0) {
		ip->i_effnlink -= inodedep->id_nlinkdelta;
		ip->i_flag |= IN_MODIFIED;
		inodedep->id_nlinkdelta = 0;
		(void) free_inodedep(inodedep);
	}
d3711 3
a3713 4
	if (ip->i_effnlink != ip->i_ffs_nlink) {
		(void) inodedep_lookup(ip->i_fs, ip->i_number, DEPALLOC,
		    &inodedep);
	} else if (inodedep_lookup(ip->i_fs, ip->i_number, 0, &inodedep) == 0) {
d3717 1
a3717 1
	if (ip->i_ffs_nlink < ip->i_effnlink)
a3718 1
	inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
d3758 2
a3759 1
	if (gotit && (error = VOP_BWRITE(inodedep->id_buf)) != 0)
d3770 1
a3770 1
static void
a3804 1
	struct diradd *dap, *olddap;
d3808 1
d3815 1
a3815 2
	int error, ret, flushparent;
	struct timespec ts;
d3818 1
d3822 11
a3832 9
	for (error = 0, flushparent = 0, olddap = NULL; ; ) {
		ACQUIRE_LOCK(&lk);
		if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) == 0)
			break;
		if (LIST_FIRST(&inodedep->id_inowait) != NULL ||
		    LIST_FIRST(&inodedep->id_bufwait) != NULL ||
		    TAILQ_FIRST(&inodedep->id_inoupdt) != NULL ||
		    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL)
			panic("softdep_fsync: pending ops");
a3839 7
		 * If we have failed to get rid of all the dependencies
		 * then something is seriously wrong.
		 */
		if (dap == olddap)
			panic("softdep_fsync: flush failed");
		olddap = dap;
		/*
d3872 3
a3874 2
		if ((error = VFS_VGET(mnt, parentino, &pvp)) != 0) {
			vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, p);
a3875 2
		}
		vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, p);
d3888 2
a3889 1
		ret = VOP_BWRITE(bp);
d3893 3
a3895 2
		if (ret != 0)
			return (ret);
d3913 2
a3914 2
	if (vp->v_type != VBLK)
		panic("softdep_fsync_mountdev: vnode not VBLK");
d3923 2
d3932 2
a3933 1
		    wk->wk_type != D_BMSAFEMAP)
d3935 1
a3936 1
		bp->b_flags |= B_BUSY;
d3977 1
a3977 1
	if (vp->v_type != VBLK) {
d4021 1
a4021 2
	for (wk = LIST_FIRST(&bp->b_dep); wk;
	     wk = LIST_NEXT(wk, wk_list)) {
d4060 2
a4061 2
			for (aip = LIST_FIRST(&WK_INDIRDEP(wk)->ir_deplisthd);
			     aip; aip = LIST_NEXT(aip, ai_next)) {
d4201 2
a4202 1
	if (vp->v_type == VBLK && vp->v_specmountpoint && !VOP_ISLOCKED(vp) &&
d4213 1
a4213 1
static int
d4241 1
a4241 2
		for (adp = TAILQ_FIRST(&inodedep->id_inoupdt); adp;
		     adp = TAILQ_NEXT(adp, ad_next)) {
d4262 1
a4262 2
		for (adp = TAILQ_FIRST(&inodedep->id_newinoupdt); adp;
		     adp = TAILQ_NEXT(adp, ad_next)) {
d4302 1
a4302 1
static int
d4336 1
a4336 1
				panic("flush_pagedep_deps: MKDIR");
d4339 10
a4348 8
		 * Flush the file on which the directory entry depends.
		 * If the inode has already been pushed out of the cache,
		 * then all the block dependencies will have been flushed
		 * leaving only inode dependencies (e.g., bitmaps). Thus,
		 * we do a ufs_ihashget to check for the vnode in the cache.
		 * If it is there, we do a full flush. If it is no longer
		 * there we need only dispose of any remaining bitmap
		 * dependencies and write the inode to disk.
d4351 11
a4361 2
		FREE_LOCK(&lk);
		if ((vp = ufs_ihashget(ump->um_dev, inum)) == NULL) {
a4362 3
			if (inodedep_lookup(ump->um_fs, inum, 0, &inodedep) == 0
			    && dap == LIST_FIRST(diraddhdp))
				panic("flush_pagedep_deps: flush 1 failed");
d4364 1
a4364 2
			 * If the inode still has bitmap dependencies,
			 * push them to disk.
a4365 8
			if ((inodedep->id_state & DEPCOMPLETE) == 0) {
				gotit = getdirtybuf(&inodedep->id_buf,MNT_WAIT);
				FREE_LOCK(&lk);
				if (gotit &&
				    (error = VOP_BWRITE(inodedep->id_buf)) != 0)
					break;
				ACQUIRE_LOCK(&lk);
			}
d4368 21
a4388 4
			/*
			 * If the inode is still sitting in a buffer waiting
			 * to be written, push it to disk.
			 */
d4390 2
a4391 5
			if ((error = bread(ump->um_devvp,
			    fsbtodb(ump->um_fs, ino_to_fsba(ump->um_fs, inum)),
			    (int)ump->um_fs->fs_bsize, NOCRED, &bp)) != 0)
				break;
			if ((error = VOP_BWRITE(bp)) != 0)
d4394 2
a4395 3
			if (dap == LIST_FIRST(diraddhdp))
				panic("flush_pagedep_deps: flush 2 failed");
			continue;
d4397 10
a4406 21
		if (vp->v_type == VDIR) {
			/*
			 * A newly allocated directory must have its "." and
			 * ".." entries written out before its name can be
			 * committed in its parent. We do not want or need
			 * the full semantics of a synchronous VOP_FSYNC as
			 * that may end up here again, once for each directory
			 * level in the filesystem. Instead, we push the blocks
			 * and wait for them to clear.
			 */
			if ((error =
			    VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p))) {
				vput(vp);
				break;
			}
			drain_output(vp, 0);
		}
		TIMEVAL_TO_TIMESPEC(&time, &ts);
		error = VOP_UPDATE(vp, &ts, &ts, MNT_WAIT);
		vput(vp);
		if (error)
d4408 1
d4414 1
a4414 2
			panic("flush_pagedep_deps: flush 3 failed");
		ACQUIRE_LOCK(&lk);
d4423 4
a4426 3
 * memory load excessively high. Therefore we deliberately slow things
 * down and speed up the I/O processing if we find ourselves with too
 * many dependencies in progress.
d4428 19
a4446 1
static int
a4451 1
	int error;
d4459 23
d4495 2
a4496 1
		req_clear_inodedeps = 1;
d4501 2
a4502 1
		req_clear_remove = 1;
d4514 5
d4521 1
a4521 2
	error = tsleep((caddr_t)&proc_waiting, PPAUSE | PCATCH, "softupdate", 
		       tickdelay > 2 ? tickdelay : 2);
d4523 1
a4523 12
	if (error == EWOULDBLOCK) {
		switch (resource) {

		case FLUSH_INODES:
			stat_ino_limit_hit += 1;
			break;

		case FLUSH_REMOVE:
			stat_blk_limit_hit += 1;
			break;
		}
	}
d4530 2
a4531 2
 * Flush out a directory with at least one removal dependency in an effort
 * to reduce the number of freefile and freeblks dependency structures.
d4533 18
a4550 1
static void
d4567 1
a4567 2
		for (pagedep = LIST_FIRST(pagedephd); pagedep;
		     pagedep = LIST_NEXT(pagedep, pd_hash)) {
d4573 4
d4579 3
d4588 3
d4601 1
a4601 1
static void
d4631 1
a4631 2
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != (void *)&mountlist;
	     mp = CIRCLEQ_NEXT(mp, mnt_list))
d4650 4
d4656 3
d4670 3
d4679 81
d4764 1
a4764 1
static int
d4794 1
a4794 1
static void
@


1.10
log
@Instead of using an ugly hack with timeouts to wake up the process
waiting for resources, use the built-in facilities in tsleep.
@
text
@d1 1
@


1.9
log
@Cut the number of dependencies to reduce problems on low-memory machines.
@
text
@a167 1
static	void pause_timer __P((void *));
d4160 1
d4198 1
a4198 4
	if (proc_waiting == 0) {
		proc_waiting = 1;
		timeout(pause_timer, NULL, tickdelay > 2 ? tickdelay : 2);
	}
d4200 2
a4201 1
	(void) tsleep((caddr_t)&proc_waiting, PPAUSE | PCATCH, "softupdate", 0);
d4203 1
a4203 4
	if (proc_waiting) {
		untimeout(pause_timer, NULL);
		proc_waiting = 0;
	} else {
a4217 13
}

/*
 * Awaken processes pausing in request_cleanup and clear proc_waiting
 * to indicate that there is no longer a timer running.
 */
void
pause_timer(arg)
	void *arg;
{

	proc_waiting = 0;
	wakeup(&proc_waiting);
@


1.8
log
@Integrate the latest soft updates code.
From mckusick@@mckusick.com.
@
text
@d877 1
a877 1
	max_softdeps = desiredvnodes * (32 / sizeof(register_t));
@


1.8.2.1
log
@Sync with -current
@
text
@d877 1
a877 1
	max_softdeps = desiredvnodes * (16 / sizeof(register_t));
@


1.8.2.2
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 1
/*	$OpenBSD: ffs_softdep.c,v 1.22 2001/04/06 18:59:16 gluk Exp $	*/
d2 1
a2 1
 * Copyright 1998, 2000 Marshall Kirk McKusick. All Rights Reserved.
d9 5
a13 1
 * Further information about soft updates can be obtained from:
d15 3
a17 3
 *	Marshall Kirk McKusick		http://www.mckusick.com/softdep/
 *	1614 Oxford Street		mckusick@@mckusick.com
 *	Berkeley, CA 94709-1608		+1-510-843-9542
d29 13
d55 1
a55 2
 *	from: @@(#)ffs_softdep.c	9.59 (McKusick) 6/21/00
 * $FreeBSD: src/sys/ufs/ffs/ffs_softdep.c,v 1.86 2001/02/04 16:08:18 phk Exp $
a86 2
#define STATIC

a90 3

#define M_SOFTDEP_FLAGS (M_WAITOK)

d124 6
a129 6
STATIC	void softdep_error __P((char *, int));
STATIC	void drain_output __P((struct vnode *, int));
STATIC	int getdirtybuf __P((struct buf **, int));
STATIC	void clear_remove __P((struct proc *));
STATIC	void clear_inodedeps __P((struct proc *));
STATIC	int flush_pagedep_deps __P((struct vnode *, struct mount *,
d131 16
a146 16
STATIC	int flush_inodedep_deps __P((struct fs *, ino_t));
STATIC	int handle_written_filepage __P((struct pagedep *, struct buf *));
STATIC  void diradd_inode_written __P((struct diradd *, struct inodedep *));
STATIC	int handle_written_inodeblock __P((struct inodedep *, struct buf *));
STATIC	void handle_allocdirect_partdone __P((struct allocdirect *));
STATIC	void handle_allocindir_partdone __P((struct allocindir *));
STATIC	void initiate_write_filepage __P((struct pagedep *, struct buf *));
STATIC	void handle_written_mkdir __P((struct mkdir *, int));
STATIC	void initiate_write_inodeblock __P((struct inodedep *, struct buf *));
STATIC	void handle_workitem_freefile __P((struct freefile *));
STATIC	void handle_workitem_remove __P((struct dirrem *));
STATIC	struct dirrem *newdirrem __P((struct buf *, struct inode *,
	    struct inode *, int, struct dirrem **));
STATIC	void free_diradd __P((struct diradd *));
STATIC	void free_allocindir __P((struct allocindir *, struct inodedep *));
STATIC	int indir_trunc __P((struct inode *, ufs_daddr_t, int, ufs_lbn_t,
d148 2
a149 2
STATIC	void deallocate_dependencies __P((struct buf *, struct inodedep *));
STATIC	void free_allocdirect __P((struct allocdirectlst *,
d151 4
a154 5
STATIC	int check_inode_unwritten __P((struct inodedep *));
STATIC	int free_inodedep __P((struct inodedep *));
STATIC	void handle_workitem_freeblocks __P((struct freeblks *));
STATIC	void merge_inode_lists __P((struct inodedep *));
STATIC	void setup_allocindir_phase2 __P((struct buf *, struct inode *,
d156 1
a156 1
STATIC	struct allocindir *newallocindir __P((struct inode *, int, ufs_daddr_t,
d158 3
a160 3
STATIC	void handle_workitem_freefrag __P((struct freefrag *));
STATIC	struct freefrag *newfreefrag __P((struct inode *, ufs_daddr_t, long));
STATIC	void allocdirect_merge __P((struct allocdirectlst *,
d162 2
a163 2
STATIC	struct bmsafemap *bmsafemap_lookup __P((struct buf *));
STATIC	int newblk_lookup __P((struct fs *, ufs_daddr_t, int,
d165 2
a166 2
STATIC	int inodedep_lookup __P((struct fs *, ino_t, int, struct inodedep **));
STATIC	int pagedep_lookup __P((struct inode *, ufs_lbn_t, int,
d168 3
a170 4
STATIC	void pause_timer __P((void *));
STATIC	int request_cleanup __P((int, int));
STATIC	int process_worklist_item __P((struct mount *, int));
STATIC	void add_to_worklist __P((struct worklist *));
a174 6
void softdep_disk_io_initiation __P((struct buf *));
void softdep_disk_write_complete __P((struct buf *));
void softdep_deallocate_dependencies __P((struct buf *));
void softdep_move_dependencies __P((struct buf *, struct buf *));
int softdep_count_dependencies __P((struct buf *bp, int, int));

d179 2
a180 2
	softdep_move_dependencies,		/* io_movedeps */
	softdep_count_dependencies,		/* io_countdeps */
d199 1
a199 1
STATIC struct lockit {
d208 1
a208 1
STATIC struct lockit {
a210 1
	int     lkt_line;
d212 1
a212 1
STATIC int lockcnt;
d214 9
a222 9
STATIC	void acquire_lock __P((struct lockit *, int));
STATIC	void free_lock __P((struct lockit *, int));
STATIC	void acquire_lock_interlocked __P((struct lockit *, int));
STATIC	void free_lock_interlocked __P((struct lockit *, int));

#define ACQUIRE_LOCK(lk)		acquire_lock(lk, __LINE__)
#define FREE_LOCK(lk)			free_lock(lk, __LINE__)
#define ACQUIRE_LOCK_INTERLOCKED(lk)	acquire_lock_interlocked(lk, __LINE__)
#define FREE_LOCK_INTERLOCKED(lk)	free_lock_interlocked(lk, __LINE__)
d224 2
a225 2
STATIC void
acquire_lock(lk, line)
a226 1
	int line;
a227 2
	pid_t holder;
	int original_line;
d230 2
a231 5
		holder = lk->lkt_held;
		original_line = lk->lkt_line;
		FREE_LOCK(lk);
		if (holder == CURPROC->p_pid)
			panic("softdep_lock: locking against myself, acquired at line %d, relocked at line %d", original_line, line);
d233 1
a233 1
			panic("softdep_lock: lock held by %d, acquired at line %d, relocked at line %d", holder, original_line, line);
a236 1
	lk->lkt_line = line;
d240 2
a241 2
STATIC void
free_lock(lk, line)
a242 1
	int line;
d246 1
a246 1
		panic("softdep_unlock: lock not held at line %d", line);
d251 2
a252 2
STATIC void
acquire_lock_interlocked(lk, line)
a253 1
	int line;
a254 2
	pid_t holder;
	int original_line;
d257 2
a258 5
		holder = lk->lkt_held;
		original_line = lk->lkt_line;
		FREE_LOCK_INTERLOCKED(lk);
		if (holder == CURPROC->p_pid)
			panic("softdep_lock: locking against myself, acquired at line %d, relocked at line %d", original_line, line);
d260 2
a261 1
			panic("softdep_lock: lock held by %d, acquired at line %d, relocked at line %d", holder, original_line, line);
a263 1
	lk->lkt_line = line;
d267 2
a268 2
STATIC void
free_lock_interlocked(lk, line)
a269 1
	int line;
d273 1
a273 1
		panic("softdep_unlock_interlocked: lock not held at line %d", line);
d288 3
a290 3
STATIC	void sema_init __P((struct sema *, char *, int, int));
STATIC	int sema_get __P((struct sema *, struct lockit *));
STATIC	void sema_release __P((struct sema *));
d292 1
a292 1
STATIC void
d306 1
a306 1
STATIC int
d328 1
a328 1
STATIC void
d333 1
a333 3
	if (semap->value <= 0 || semap->holder != CURPROC->p_pid) {
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
a334 1
	}
d358 3
a360 3
STATIC	void worklist_insert __P((struct workhead *, struct worklist *));
STATIC	void worklist_remove __P((struct worklist *));
STATIC	void workitem_free __P((struct worklist *, int));
d366 1
a366 1
STATIC void
d374 1
a374 2
	if (item->wk_state & ONWORKLIST) {
		FREE_LOCK(&lk);
a375 1
	}
d380 1
a380 1
STATIC void
d387 1
a387 2
	if ((item->wk_state & ONWORKLIST) == 0) {
		FREE_LOCK(&lk);
a388 1
	}
d393 1
a393 1
STATIC void
d399 1
a399 3
	if (item->wk_state & ONWORKLIST) {
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
d401 1
a401 4
	}
	if (item->wk_type != type) {
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
a402 1
	}
d410 7
a416 11
STATIC struct workhead softdep_workitem_pending;
STATIC int num_on_worklist;	/* number of worklist items to be processed */
STATIC int softdep_worklist_busy; /* 1 => trying to do unmount */
STATIC int softdep_worklist_req; /* serialized waiters */
STATIC int max_softdeps;	/* maximum number of structs before slowdown */
STATIC int tickdelay = 2;	/* number of ticks to pause during slowdown */
STATIC int proc_waiting;	/* tracks whether we have a timeout posted */
STATIC int *stat_countp;	/* statistic to count in proc_waiting timeout */
STATIC struct timeout proc_waiting_timeout; 
STATIC struct proc *filesys_syncer; /* proc of filesystem syncer process */
STATIC int req_clear_inodedeps;	/* syncer process flush some inodedeps */
d418 1
a418 1
STATIC int req_clear_remove;	/* syncer process flush some freeblks */
d423 8
a430 10
STATIC int stat_worklist_push;	/* number of worklist cleanups */
STATIC int stat_blk_limit_push;	/* number of times block limit neared */
STATIC int stat_ino_limit_push;	/* number of times inode limit neared */
STATIC int stat_blk_limit_hit;	/* number of times block slowdown imposed */
STATIC int stat_ino_limit_hit;	/* number of times inode slowdown imposed */
STATIC int stat_sync_limit_hit;	/* number of synchronous slowdowns imposed */
STATIC int stat_indir_blk_ptrs;	/* bufs redirtied as indir ptrs not written */
STATIC int stat_inode_bitmap;	/* bufs redirtied as inode bitmap not written */
STATIC int stat_direct_blk_ptrs;/* bufs redirtied as direct ptrs not written */
STATIC int stat_dir_entry;	/* bufs redirtied as dir entry cannot write */
a435 1
struct ctldebug debug22 = { "worklist_push", &stat_worklist_push };
d440 4
a443 5
struct ctldebug debug27 = { "sync_limit_hit", &stat_sync_limit_hit }; 
struct ctldebug debug28 = { "indir_blk_ptrs", &stat_indir_blk_ptrs };
struct ctldebug debug29 = { "inode_bitmap", &stat_inode_bitmap };
struct ctldebug debug30 = { "direct_blk_ptrs", &stat_direct_blk_ptrs };
struct ctldebug debug31 = { "dir_entry", &stat_dir_entry };
d453 1
a453 1
STATIC void
d459 1
a459 3
	if (wk->wk_state & ONWORKLIST) {
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
a460 1
	}
a466 1
	num_on_worklist += 1;
d483 3
a485 2
	int matchcnt, loopcount;
	struct timeval starttime;
a489 3
	 * We can't do this in softdep_initialize, because the syncer doesn't
	 * have to run then.
	 * NOTE! This function _could_ be called with a curproc != syncerproc.
d491 1
a491 1
	filesys_syncer = syncerproc;
d493 3
a495 1

d498 2
a499 2
	 * code, but we have to single-thread it when softdep_flushfiles()
	 * is in operation to get an accurate count of the number of items
d502 2
a503 6
	if (matchmnt == NULL) {
		if (softdep_worklist_busy < 0)
			return(-1);
		softdep_worklist_busy += 1;
	}

d509 2
a510 2
		req_clear_inodedeps -= 1;
		wakeup_one(&proc_waiting);
d514 2
a515 2
		req_clear_remove -= 1;
		wakeup_one(&proc_waiting);
d517 5
a521 4
	loopcount = 1;
	starttime = time;
	while (num_on_worklist > 0) {
		matchcnt += process_worklist_item(matchmnt, 0);
d523 26
a548 6
		/*
		 * If a umount operation wants to run the worklist
		 * accurately, abort.
		 */
		if (softdep_worklist_req && matchmnt == NULL) {
			matchcnt = -1;
d550 5
d556 2
a557 1

d563 2
a564 2
			req_clear_inodedeps -= 1;
			wakeup_one(&proc_waiting);
d568 2
a569 24
			req_clear_remove -= 1;
			wakeup_one(&proc_waiting);
		}
		/*
		 * We do not generally want to stop for buffer space, but if
		 * we are really being a buffer hog, we will stop and wait.
		 */
#if 0
		if (loopcount++ % 128 == 0)
			bwillwrite();
#endif
		/*
		 * Never allow processing to run for more than one
		 * second. Otherwise the other syncer tasks may get
		 * excessively backlogged.
		 */
		{
			struct timeval diff;

			timersub(&time, &starttime, &diff);
			if (diff.tv_sec != 0 && matchmnt == NULL) {
				matchcnt = -1;
				break;
			}
d571 1
a572 44
	if (matchmnt == NULL) {
		softdep_worklist_busy -= 1;
		if (softdep_worklist_req && softdep_worklist_busy == 0)
			wakeup(&softdep_worklist_req);
	}
	return (matchcnt);
}

/*
 * Process one item on the worklist.
 */
STATIC int
process_worklist_item(matchmnt, flags)
	struct mount *matchmnt;
	int flags;
{
	struct worklist *wk;
	struct dirrem *dirrem;
	struct mount *mp;
	struct vnode *vp;
	int matchcnt = 0;

	ACQUIRE_LOCK(&lk);
	/*
	 * Normally we just process each item on the worklist in order.
	 * However, if we are in a situation where we cannot lock any
	 * inodes, we have to skip over any dirrem requests whose
	 * vnodes are resident and locked.
	 */
	LIST_FOREACH(wk, &softdep_workitem_pending, wk_list) {
		if ((flags & LK_NOWAIT) == 0 || wk->wk_type != D_DIRREM)
			break;
		dirrem = WK_DIRREM(wk);
		vp = ufs_ihashlookup(VFSTOUFS(dirrem->dm_mnt)->um_dev,
		    dirrem->dm_oldinum);
		if (vp == NULL || !VOP_ISLOCKED(vp))
			break;
	}
	if (wk == 0) {
		FREE_LOCK(&lk);
		return (0);
	}
	WORKLIST_REMOVE(wk);
	num_on_worklist -= 1;
a573 59
	switch (wk->wk_type) {

	case D_DIRREM:
		/* removal of a directory entry */
		mp = WK_DIRREM(wk)->dm_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: dirrem on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_remove(WK_DIRREM(wk));
		break;

	case D_FREEBLKS:
		/* releasing blocks and/or fragments from a file */
		mp = WK_FREEBLKS(wk)->fb_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: freeblks on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_freeblocks(WK_FREEBLKS(wk));
		break;

	case D_FREEFRAG:
		/* releasing a fragment when replaced as a file grows */
		mp = WK_FREEFRAG(wk)->ff_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: freefrag on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_freefrag(WK_FREEFRAG(wk));
		break;

	case D_FREEFILE:
		/* releasing an inode when its link count drops to 0 */
		mp = WK_FREEFILE(wk)->fx_mnt;
#if 0
		if (vn_write_suspend_wait(NULL, mp, V_NOWAIT))
			panic("%s: freefile on suspended filesystem",
				"process_worklist_item");
#endif
		if (mp == matchmnt)
			matchcnt += 1;
		handle_workitem_freefile(WK_FREEFILE(wk));
		break;

	default:
		panic("%s_process_worklist: Unknown type %s",
		    "softdep", TYPENAME(wk->wk_type));
		/* NOTREACHED */
	}
a577 25
 * Move dependencies from one buffer to another.
 */
void
softdep_move_dependencies(oldbp, newbp)
	struct buf *oldbp;
	struct buf *newbp;
{
	struct worklist *wk, *wktail;

	if (LIST_FIRST(&newbp->b_dep) != NULL)
		panic("softdep_move_dependencies: need merge code");
	wktail = 0;
	ACQUIRE_LOCK(&lk);
	while ((wk = LIST_FIRST(&oldbp->b_dep)) != NULL) {
		LIST_REMOVE(wk, wk_list);
		if (wktail == 0)
			LIST_INSERT_HEAD(&newbp->b_dep, wk, wk_list);
		else
			LIST_INSERT_AFTER(wktail, wk, wk_list);
		wktail = wk;
	}
	FREE_LOCK(&lk);
}

/*
d581 1
a581 1
softdep_flushworklist(oldmnt, countp, p)
d583 1
a583 1
	int *countp;
d587 1
a587 1
	int count, error = 0;
d590 1
a590 1
	 * Await our turn to clear out the queue, then serialize access.
d592 6
a597 4
	while (softdep_worklist_busy) {
		softdep_worklist_req += 1;
		tsleep(&softdep_worklist_req, PRIBIO, "softflush", 0);
		softdep_worklist_req -= 1;
a598 1
	softdep_worklist_busy = -1;
d602 2
a603 2
	 * creates. We continue until no more worklist dependencies
	 * are found.
a604 1
	*countp = 0;
d606 14
a619 2
	while ((count = softdep_process_worklist(oldmnt)) > 0) {
		*countp += count;
a626 33
	if (softdep_worklist_req)
		wakeup(&softdep_worklist_req);
	return (error);
}

/*
 * Flush all vnodes and worklist items associated with a specified mount point.
 */
int
softdep_flushfiles(oldmnt, flags, p)
	struct mount *oldmnt;
	int flags;
	struct proc *p;
{
	int error, count, loopcnt;

	/*
	 * Alternately flush the vnodes associated with the mount
	 * point and process any dependencies that the flushing
	 * creates. In theory, this loop can happen at most twice,
	 * but we give it a few extra just to be sure.
	 */
	for (loopcnt = 10; loopcnt > 0; loopcnt--) {
		/*
		 * Do another flush in case any vnodes were brought in
		 * as part of the cleanup operations.
		 */
		if ((error = ffs_flushfiles(oldmnt, flags, p)) != 0)
			break;
		if ((error = softdep_flushworklist(oldmnt, &count, p)) != 0 ||
		    count == 0)
			break;
	}
a662 1
#define NODELAY         0x0002  /* cannot do background work */
d672 1
a672 1
STATIC struct sema pagedep_in_progress;
d680 1
a680 1
STATIC int
d699 2
a700 1
	LIST_FOREACH(pagedep, pagedephd, pd_hash)
d718 1
a718 1
		M_SOFTDEP_FLAGS);
d739 2
a740 2
STATIC u_long	inodedep_hash;	/* size of hash table - 1 */
STATIC long	num_inodedep;	/* number of inodedep allocated */
d743 1
a743 1
STATIC struct sema inodedep_in_progress;
d751 1
a751 1
STATIC int
d769 2
a770 1
	LIST_FOREACH(inodedep, inodedephd, id_hash)
d784 1
a784 1
	if (num_inodedep > max_softdeps && firsttry && (flags & NODELAY) == 0 &&
d795 1
a795 1
		M_INODEDEP, M_SOFTDEP_FLAGS);
d823 1
a823 1
STATIC struct sema newblk_in_progress;
d830 1
a830 1
STATIC int
d842 2
a843 1
	LIST_FOREACH(newblk, newblkhd, nb_hash)
d857 1
a857 1
		M_NEWBLK, M_SOFTDEP_FLAGS);
d877 1
a877 6
#ifdef KMEMSTATS
	max_softdeps = min (desiredvnodes * 8,
	    kmemstats[M_INODEDEP].ks_limit / (2 * sizeof(struct inodedep)));
#else
	max_softdeps = desiredvnodes * 4;
#endif
a885 1
	timeout_set(&proc_waiting_timeout, pause_timer, 0);
d904 1
d910 1
a910 1
	if ((fs->fs_flags & FS_UNCLEAN) == 0)
d986 1
a986 3
	if (inodedep_lookup(ip->i_fs, newinum, DEPALLOC | NODELAY, &inodedep)
	    != 0) {
		FREE_LOCK(&lk);
a987 1
	}
d1027 1
a1027 1
STATIC struct bmsafemap *
d1038 1
a1038 1
	LIST_FOREACH(wk, &bp->b_dep, wk_list)
d1043 1
a1043 1
		M_BMSAFEMAP, M_SOFTDEP_FLAGS);
d1103 1
a1103 1
		M_ALLOCDIRECT, M_SOFTDEP_FLAGS);
d1121 1
a1121 1
	inodedep_lookup(ip->i_fs, ip->i_number, DEPALLOC | NODELAY, &inodedep);
d1139 1
a1139 2
		if (oldblkno != 0) {
			FREE_LOCK(&lk);
a1140 1
		}
d1175 2
a1176 1
	TAILQ_FOREACH(oldadp, adphead, ad_next) {
d1180 1
a1180 2
	if (oldadp == NULL) {
		FREE_LOCK(&lk);
a1181 1
	}
d1193 1
a1193 1
STATIC void
d1207 1
a1207 2
	    newadp->ad_lbn >= NDADDR) {
		FREE_LOCK(&lk);
a1210 1
	}
d1240 1
a1240 1
STATIC struct freefrag *
d1255 1
a1255 1
		M_FREEFRAG, M_SOFTDEP_FLAGS);
d1259 1
a1259 1
	freefrag->ff_mnt = ITOV(ip)->v_mount;
d1270 1
a1270 1
STATIC void 
d1276 1
a1276 2
	tip.i_vnode = NULL;
	tip.i_fs = VFSTOUFS(freefrag->ff_mnt)->um_fs;
d1313 1
a1313 1
STATIC struct allocindir *
d1323 2
a1324 2
		M_ALLOCINDIR, M_SOFTDEP_FLAGS);
	bzero(aip,sizeof(struct allocindir));
d1391 1
a1391 1
STATIC void 
d1408 2
a1409 1
		LIST_FOREACH(wk, &bp->b_dep, wk_list) {
d1447 2
a1448 2

				LIST_FOREACH(oldaip, &indirdep->ir_deplisthd, ai_next)
a1450 1
			freefrag = NULL;
d1452 1
a1452 2
				if (oldaip->ai_newblkno != aip->ai_oldblkno) {
					FREE_LOCK(&lk);
a1453 1
				}
d1455 3
a1457 3
				freefrag = aip->ai_freefrag;
				aip->ai_freefrag = oldaip->ai_freefrag;
				oldaip->ai_freefrag = NULL;
a1463 2
			if (freefrag != NULL)
				handle_workitem_freefrag(freefrag);
d1473 1
a1473 1
			M_INDIRDEP, M_SOFTDEP_FLAGS);
d1478 1
a1478 4
		if (bp->b_blkno == bp->b_lblkno) {
			VOP_BMAP(bp->b_vp, bp->b_lblkno, NULL, &bp->b_blkno,
				NULL);
		}
d1481 2
a1482 4
#if 0
		BUF_KERNPROC(newindirdep->ir_savebp);
#endif
		bcopy(bp->b_data, newindirdep->ir_savebp->b_data, bp->b_bcount);
d1515 1
d1527 1
a1527 1
	int i, delay, error;
d1531 7
a1537 1
		panic("softdep_setup_freeblocks: non-zero length");
d1539 1
a1539 1
	    M_FREEBLKS, M_SOFTDEP_FLAGS);
d1545 1
a1545 1
	freeblks->fb_mnt = ITOV(ip)->v_mount;
d1575 1
a1575 2
	if ((inodedep->id_state & IOSTARTED) != 0) {
		FREE_LOCK(&lk);
a1576 1
	}
d1579 3
a1581 8
	 * must await the zero'ed inode being written to disk. If we
	 * still have a bitmap dependency (delay == 0), then the inode
	 * has never been written to disk, so we can process the
	 * freeblks below once we have deleted the dependencies.
	 */
	delay = (inodedep->id_state & DEPCOMPLETE);
	if (delay)
		WORKLIST_INSERT(&inodedep->id_bufwait, &freeblks->fb_list);
a1587 2
	 * If we still have a bitmap dependency, then the inode has never
	 * been written to disk, so we can free any fragments without delay.
d1591 1
a1591 1
		free_allocdirect(&inodedep->id_inoupdt, adp, delay);
d1607 1
a1607 1
		bp->b_flags |= B_INVAL | B_NOCACHE;
d1612 4
a1615 1
	if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) != 0)
a1617 7
	/*
	 * If the inode has never been written to disk (delay == 0),
	 * then we can process the freeblks now that we have deleted
	 * the dependencies.
	 */
	if (!delay)
		handle_workitem_freeblocks(freeblks);
d1627 1
a1627 1
STATIC void
d1659 1
a1659 2
			if (indirdep->ir_state & GOINGAWAY) {
				FREE_LOCK(&lk);
a1660 1
			}
d1665 1
a1665 2
			    bp->b_blkno != indirdep->ir_savebp->b_lblkno) {
				FREE_LOCK(&lk);
a1666 1
			}
d1691 2
a1692 1
			LIST_FOREACH(dirrem, &pagedep->pd_dirremhd, dm_next) {
d1695 1
a1695 3
				if (inodedep == NULL ||
				    (inodedep->id_state & ALLCOMPLETE) ==
				     ALLCOMPLETE)
a1711 1
			FREE_LOCK(&lk);
a1716 1
			FREE_LOCK(&lk);
d1728 1
a1728 1
STATIC void
d1758 1
d1760 6
a1765 4
softdep_freefile(pvp, ino, mode)
		struct vnode *pvp;
		ino_t ino;
		int mode;
d1767 1
a1767 1
	struct inode *ip = VTOI(pvp);
d1772 5
d1779 1
d1781 1
a1781 1
		M_FREEFILE, M_SOFTDEP_FLAGS);
d1784 2
a1785 2
	freefile->fx_mode = mode;
	freefile->fx_oldinum = ino;
d1787 1
a1787 1
	freefile->fx_mnt = ITOV(ip)->v_mount;
d1791 1
a1791 3
	 * been written to disk. If the allocated inode has never been
	 * written to disk, then the on-disk inode is zero'ed. In either
	 * case we can free the file immediately.
d1794 2
a1795 2
	if (inodedep_lookup(ip->i_fs, ino, 0, &inodedep) == 0 ||
	    check_inode_unwritten(inodedep)) {
a1796 1
		handle_workitem_freefile(freefile);
d1799 25
a1823 1
	WORKLIST_INSERT(&inodedep->id_inowait, &freefile->fx_list);
a1827 44
 * Check to see if an inode has never been written to disk. If
 * so free the inodedep and return success, otherwise return failure.
 * This routine must be called with splbio interrupts blocked.
 *
 * If we still have a bitmap dependency, then the inode has never
 * been written to disk. Drop the dependency as it is no longer
 * necessary since the inode is being deallocated. We set the
 * ALLCOMPLETE flags since the bitmap now properly shows that the
 * inode is not allocated. Even if the inode is actively being
 * written, it has been rolled back to its zero'ed state, so we
 * are ensured that a zero inode is what is on the disk. For short
 * lived files, this change will usually result in removing all the
 * dependencies from the inode so that it can be freed immediately.
 */
STATIC int
check_inode_unwritten(inodedep)
	struct inodedep *inodedep;
{

	if ((inodedep->id_state & DEPCOMPLETE) != 0 ||
	    LIST_FIRST(&inodedep->id_pendinghd) != NULL ||
	    LIST_FIRST(&inodedep->id_bufwait) != NULL ||
	    LIST_FIRST(&inodedep->id_inowait) != NULL ||
	    TAILQ_FIRST(&inodedep->id_inoupdt) != NULL ||
	    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL ||
	    inodedep->id_nlinkdelta != 0)
		return (0);
	inodedep->id_state |= ALLCOMPLETE;
	LIST_REMOVE(inodedep, id_deps);
	inodedep->id_buf = NULL;
	if (inodedep->id_state & ONWORKLIST)
		WORKLIST_REMOVE(&inodedep->id_list);
	if (inodedep->id_savedino != NULL) {
		FREE(inodedep->id_savedino, M_INODEDEP);
		inodedep->id_savedino = NULL;
	}
	if (free_inodedep(inodedep) == 0) {
		FREE_LOCK(&lk);
		panic("check_inode_unwritten: busy inode");
	}
	return (1);
}

/*
d1830 1
a1830 1
STATIC int
d1858 1
a1858 1
STATIC void
a1869 1
	tip.i_fs = fs = VFSTOUFS(freeblks->fb_mnt)->um_fs;
d1873 1
d1876 1
a1876 1
	tip.i_vnode = NULL;
d1910 1
a1910 1
		printf("handle_workitem_freeblocks: block count");
d1915 1
d1924 1
a1924 1
STATIC int
d1962 1
a1962 2
		    (indirdep->ir_state & GOINGAWAY) == 0) {
			FREE_LOCK(&lk);
a1963 1
		}
d1966 1
a1966 2
		if (LIST_FIRST(&bp->b_dep) != NULL) {
			FREE_LOCK(&lk);
a1967 1
		}
d1991 1
a1991 1
	bp->b_flags |= B_INVAL | B_NOCACHE;
d2000 1
a2000 1
STATIC void
d2077 2
a2078 3
	MALLOC(dap, struct diradd *, sizeof(struct diradd), M_DIRADD,
		M_SOFTDEP_FLAGS);
	bzero(dap,sizeof(struct diradd));
d2089 1
a2089 1
		    M_SOFTDEP_FLAGS);
d2094 1
a2094 1
		    M_SOFTDEP_FLAGS);
d2168 2
a2169 2

	LIST_FOREACH(dap, &pagedep->pd_diraddhd[DIRADDHASH(oldoffset)], da_pdlist) {
d2181 2
a2182 2

		LIST_FOREACH(dap, &pagedep->pd_pendinghd, da_pdlist) {
d2198 1
a2198 1
STATIC void
d2234 1
a2234 2
		if ((dap->da_state & (MKDIR_PARENT | MKDIR_BODY)) != 0) {
			FREE_LOCK(&lk);
a2235 1
		}
d2264 1
a2264 1
	struct dirrem *dirrem, *prevdirrem;
d2269 1
a2269 13
	dirrem = newdirrem(bp, dp, ip, isrmdir, &prevdirrem);

	/*
	 * If the COMPLETE flag is clear, then there were no active
	 * entries and we want to roll back to a zeroed entry until
	 * the new inode is committed to disk. If the COMPLETE flag is
	 * set then we have deleted an entry that never made it to
	 * disk. If the entry we deleted resulted from a name change,
	 * then the old name still resides on disk. We cannot delete
	 * its inode (returned to us in prevdirrem) until the zeroed
	 * directory entry gets to disk. The new inode has never been
	 * referenced on the disk, so can be deleted immediately.
	 */
a2272 1
		FREE_LOCK(&lk);
a2273 3
		if (prevdirrem != NULL)
			LIST_INSERT_HEAD(&dirrem->dm_pagedep->pd_dirremhd,
			    prevdirrem, dm_next);
d2275 1
a2275 2
		FREE_LOCK(&lk);
		handle_workitem_remove(dirrem);
d2277 1
d2284 2
a2285 3
STATIC long num_dirrem;		/* number of dirrem allocated */
STATIC struct dirrem *
newdirrem(bp, dp, ip, isrmdir, prevdirremp)
a2289 1
	struct dirrem **prevdirremp; /* previously referenced inode, if any */
a2301 8
	/*
	 * If we are over our limit, try to improve the situation.
	 * Limiting the number of dirrem structures will also limit
	 * the number of freefile and freeblks structures.
	 */
	if (num_dirrem > max_softdeps / 2)
		(void) request_cleanup(FLUSH_REMOVE, 0);
	num_dirrem += 1;
d2303 2
a2304 2
		M_DIRREM, M_SOFTDEP_FLAGS);
	bzero(dirrem,sizeof(struct dirrem));
a2308 1
	*prevdirremp = NULL;
d2322 2
a2323 2

	LIST_FOREACH(dap, &pagedep->pd_diraddhd[DIRADDHASH(offset)], da_pdlist)
d2327 2
a2328 2

		LIST_FOREACH(dap, &pagedep->pd_pendinghd, da_pdlist)
d2335 1
a2335 1
	 * Must be ATTACHED at this point.
d2337 1
a2337 2
	if ((dap->da_state & ATTACHED) == 0) {
		FREE_LOCK(&lk);
d2339 1
a2339 3
	}
	if (dap->da_newinum != ip->i_number) {
		FREE_LOCK(&lk);
d2342 1
a2342 15
	}
	/*
	 * If we are deleting a changed name that never made it to disk,
	 * then return the dirrem describing the previous inode (which
	 * represents the inode currently referenced from this entry on disk).
	 */
	if ((dap->da_state & DIRCHG) != 0) {
		*prevdirremp = dap->da_previous;
		dap->da_state &= ~DIRCHG;
		dap->da_pagedep = pagedep;
	}
	/*
	 * We are deleting an entry that never made it to disk.
	 * Mark it COMPLETE so we can delete its inode immediately.
	 */
a2343 1
	free_diradd(dap);
d2374 1
a2374 1
	struct dirrem *dirrem, *prevdirrem;
d2385 2
a2386 2
		    M_DIRADD, M_SOFTDEP_FLAGS);
		bzero(dap,sizeof(struct diradd));
d2396 1
a2396 1
	dirrem = newdirrem(bp, dp, ip, isrmdir, &prevdirrem);
a2429 25
	 * If the COMPLETE flag is clear, then there were no active
	 * entries and we want to roll back to the previous inode until
	 * the new inode is committed to disk. If the COMPLETE flag is
	 * set, then we have deleted an entry that never made it to disk.
	 * If the entry we deleted resulted from a name change, then the old
	 * inode reference still resides on disk. Any rollback that we do
	 * needs to be to that old inode (returned to us in prevdirrem). If
	 * the entry we deleted resulted from a create, then there is
	 * no entry on the disk, so we want to roll back to zero rather
	 * than the uncommitted inode. In either of the COMPLETE cases we
	 * want to immediately free the unwritten and unreferenced inode.
	 */
	if ((dirrem->dm_state & COMPLETE) == 0) {
		dap->da_previous = dirrem;
	} else {
		if (prevdirrem != NULL) {
			dap->da_previous = prevdirrem;
		} else {
			dap->da_state &= ~DIRCHG;
			dap->da_pagedep = pagedep;
		}
		dirrem->dm_dirinum = pagedep->pd_ino;
		add_to_worklist(&dirrem->dm_list);
	}
	/*
d2434 1
d2445 12
d2461 1
a2461 1
 * Called whenever the link count on an inode is changed.
d2467 1
a2467 1
softdep_change_linkcnt(ip)
a2473 5
	if (ip->i_ffs_nlink < ip->i_effnlink) {
		FREE_LOCK(&lk);
		panic("softdep_change_linkcnt: bad delta");
	}
	inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
d2481 1
a2481 1
STATIC void 
a2488 1
	ino_t oldinum;
a2495 6
	ACQUIRE_LOCK(&lk);
	if ((inodedep_lookup(ip->i_fs, dirrem->dm_oldinum, 0, &inodedep)) 
	    == 0) {
		FREE_LOCK(&lk);
		panic("handle_workitem_remove: lost inodedep");
	}
d2501 2
a2503 6
		if (ip->i_ffs_nlink < ip->i_effnlink) {
			FREE_LOCK(&lk);
			panic("handle_workitem_remove: bad file delta");
		}
		inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
		FREE_LOCK(&lk);
a2504 1
		num_dirrem -= 1;
a2515 1
	ip->i_flag |= IN_CHANGE;
d2518 1
a2518 2
	inodedep->id_nlinkdelta = ip->i_ffs_nlink - ip->i_effnlink;
	FREE_LOCK(&lk);
a2527 1
		num_dirrem -= 1;
a2530 6
	/*
	 * If the inodedep does not exist, then the zero'ed inode has
	 * been written to disk. If the allocated inode has never been
	 * written to disk, then the on-disk inode is zero'ed. In either
	 * case we can remove the file immediately.
	 */
d2532 2
a2534 1
	oldinum = dirrem->dm_oldinum;
a2535 7
	if (inodedep_lookup(ip->i_fs, oldinum, 0, &inodedep) == 0 ||
	    check_inode_unwritten(inodedep)) {
		FREE_LOCK(&lk);
		vput(vp);
		handle_workitem_remove(dirrem);
		return;
	}
d2555 1
a2555 1
STATIC void 
a2558 1
	struct fs *fs;
d2562 1
a2564 1
	fs = VFSTOUFS(freefile->fx_mnt)->um_fs;
d2567 2
a2568 1
	error = inodedep_lookup(fs, freefile->fx_oldinum, 0, &idp);
a2569 2
	if (error)
		panic("handle_workitem_freefile: inodedep survived");
d2573 1
a2573 2
	tip.i_fs = fs;
	tip.i_vnode = &vp;
d2575 5
a2579 10
	{
		struct vop_vfree_args  vargs;

		vargs.a_pvp = &vp;
		vargs.a_ino = freefile->fx_oldinum;
		vargs.a_mode = freefile->fx_mode;

		if ((error = ffs_freefile(&vargs)) != 0)
			softdep_error("handle_workitem_freefile", error);
	}
d2581 1
d2644 1
a2644 1
				indirdep->ir_savebp->b_flags |= B_INVAL | B_NOCACHE;
a2654 2
			MALLOC(indirdep->ir_saveddata, caddr_t, bp->b_bcount,
			    M_INDIRDEP, M_SOFTDEP_FLAGS);
d2658 1
a2658 3
			bcopy(bp->b_data, indirdep->ir_saveddata, bp->b_bcount);
			bcopy(indirdep->ir_savebp->b_data, bp->b_data,
			    bp->b_bcount);
d2682 1
a2682 1
STATIC void
d2703 2
a2704 1
		LIST_FOREACH(dap, &pagedep->pd_diraddhd[i], da_pdlist) {
d2707 1
a2707 2
			if (ep->d_ino != dap->da_newinum) {
				FREE_LOCK(&lk);
a2710 1
			}
d2728 1
a2728 1
STATIC void 
d2753 1
a2753 1
		    sizeof(struct dinode), M_INODEDEP, M_SOFTDEP_FLAGS);
d2771 1
a2771 2
		if (deplist != 0 && prevlbn >= adp->ad_lbn) {
			FREE_LOCK(&lk);
a2772 1
		}
d2775 1
a2775 2
		    dp->di_db[adp->ad_lbn] != adp->ad_newblkno) {
			FREE_LOCK(&lk);
a2778 1
		}
d2780 1
a2780 2
		    dp->di_ib[adp->ad_lbn - NDADDR] != adp->ad_newblkno) {
			FREE_LOCK(&lk);
a2783 1
		}
d2785 1
a2785 2
		if ((adp->ad_state & ATTACHED) == 0) {
			FREE_LOCK(&lk);
a2787 1
		}
d2809 1
a2809 2
			if (dp->di_db[i] != 0 && (deplist & (1 << i)) == 0) {
				FREE_LOCK(&lk);
a2810 1
			}
d2817 1
a2817 2
			    (deplist & ((1 << NDADDR) << i)) == 0) {
				FREE_LOCK(&lk);
a2818 1
			}
d2945 1
a2945 3
			bcopy(indirdep->ir_saveddata, bp->b_data, bp->b_bcount);
			FREE(indirdep->ir_saveddata, M_INDIRDEP);
			indirdep->ir_saveddata = 0;
d2956 1
a2956 1
			buf_dirty(bp);
d2984 1
a2984 1
STATIC void 
d2990 1
a2990 1
	long bsize, delay;
a2995 1

d3007 2
a3008 1
	TAILQ_FOREACH(listadp, &inodedep->id_inoupdt, ad_next) {
d3027 2
a3028 1
		TAILQ_FOREACH(listadp, &inodedep->id_newinoupdt, ad_next)
a3039 3
	 * If the inode still has a bitmap dependency, then it has
	 * never been written to disk, hence the on-disk inode cannot
	 * reference the old fragment so we can free it without delay.
a3040 1
	delay = (inodedep->id_state & DEPCOMPLETE);
d3045 1
a3045 1
		free_allocdirect(&inodedep->id_inoupdt, adp, delay);
d3054 1
a3054 1
STATIC void
d3084 1
a3084 1
STATIC int 
d3113 1
a3113 1
		buf_dirty(bp);
d3162 1
a3162 1
		buf_dirty(bp);
d3231 1
a3231 1
STATIC void
d3253 1
a3253 1
STATIC void
d3286 1
a3286 1
STATIC int 
d3347 1
a3347 1
		buf_dirty(bp);
d3399 6
a3404 1
	ip->i_effnlink -= inodedep->id_nlinkdelta;
d3436 4
a3439 1
	if (inodedep_lookup(ip->i_fs, ip->i_number, 0, &inodedep) == 0) {
a3440 2
		if (ip->i_effnlink != ip->i_ffs_nlink)
			panic("softdep_update_inodeblock: bad link count");
d3443 1
a3443 2
	if (inodedep->id_nlinkdelta != ip->i_ffs_nlink - ip->i_effnlink) {
		FREE_LOCK(&lk);
d3445 1
a3445 1
	}
d3485 1
a3485 2
	if (gotit &&
	    (error = bwrite(inodedep->id_buf)) != 0)
d3496 1
a3496 1
STATIC void
d3531 1
a3534 1
	struct diradd *dap;
d3541 2
a3542 1
	int error, flushparent;
a3544 1
	struct timespec ts;
d3548 9
a3556 13
	ACQUIRE_LOCK(&lk);
	if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) == 0) {
		FREE_LOCK(&lk);
		return (0);
	}
	if (LIST_FIRST(&inodedep->id_inowait) != NULL ||
	    LIST_FIRST(&inodedep->id_bufwait) != NULL ||
	    TAILQ_FIRST(&inodedep->id_inoupdt) != NULL ||
	    TAILQ_FIRST(&inodedep->id_newinoupdt) != NULL) {
		FREE_LOCK(&lk);
		panic("softdep_fsync: pending ops");
	}
	for (error = 0, flushparent = 0; ; ) {
d3559 1
a3559 2
		if (wk->wk_type != D_DIRADD) {
			FREE_LOCK(&lk);
a3561 1
		}
d3564 7
d3581 1
a3581 2
		if ((dap->da_state & (MKDIR_BODY | COMPLETE)) != COMPLETE) {
			FREE_LOCK(&lk);
a3582 1
		}
d3603 4
a3606 1
		error = VFS_VGET(mnt, parentino, &pvp);
a3607 2
		if (error != 0)
			return (error);
d3620 1
a3620 2
		if (error == 0)
			error = bwrite(bp);
d3624 2
a3625 3
		ACQUIRE_LOCK(&lk);
		if (inodedep_lookup(fs, ip->i_number, 0, &inodedep) == 0)
			break;
d3643 2
a3644 2
	if (!vn_isdisk(vp, NULL))
		panic("softdep_fsync_mountdev: vnode not a disk");
d3653 1
a3653 4
		bp->b_flags |= B_BUSY;

		if ((bp->b_flags & B_DELWRI) == 0) {
			FREE_LOCK(&lk);
a3654 1
		}
d3660 1
a3660 2
		    wk->wk_type != D_BMSAFEMAP) {
			bp->b_flags &= ~B_BUSY;
a3661 1
		}
d3663 1
d3704 1
a3704 1
	if (!vn_isdisk(vp, NULL)) {
d3748 2
a3749 1
	LIST_FOREACH(wk, &bp->b_dep, wk_list) {
d3788 2
a3789 2

			LIST_FOREACH(aip, &WK_INDIRDEP(wk)->ir_deplisthd, ai_next) {
a3878 1
			FREE_LOCK(&lk);
d3929 1
a3929 2
	if (vn_isdisk(vp, NULL) &&
	    vp->v_specmountpoint && !VOP_ISLOCKED(vp) &&
d3940 1
a3940 1
STATIC int
d3968 2
a3969 1
		TAILQ_FOREACH(adp, &inodedep->id_inoupdt, ad_next) {
d3990 2
a3991 1
		TAILQ_FOREACH(adp, &inodedep->id_newinoupdt, ad_next) {
d4031 1
a4031 1
STATIC int
d4064 2
a4065 4
			if (dap->da_state & MKDIR_PARENT) {
				FREE_LOCK(&lk);
				panic("flush_pagedep_deps: MKDIR_PARENT");
			}
d4068 8
a4075 10
		 * A newly allocated directory must have its "." and
		 * ".." entries written out before its name can be
		 * committed in its parent. We do not want or need
		 * the full semantics of a synchronous VOP_FSYNC as
		 * that may end up here again, once for each directory
		 * level in the filesystem. Instead, we push the blocks
		 * and wait for them to clear. We have to fsync twice
		 * because the first call may choose to defer blocks
		 * that still have dependencies, but deferral will
		 * happen at most once.
d4078 2
a4079 11
		if (dap->da_state & MKDIR_BODY) {
			FREE_LOCK(&lk);
			if ((error = VFS_VGET(mp, inum, &vp)) != 0)
				break;
			if ((error=VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p)) ||
			    (error=VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p))) {
				vput(vp);
				break;
			}
			drain_output(vp, 0);
			vput(vp);
d4081 3
d4085 2
a4086 1
			 * If that cleared dependencies, go on to next.
d4088 8
d4098 4
a4101 16
			if (dap->da_state & MKDIR_BODY) {
				FREE_LOCK(&lk);
				panic("flush_pagedep_deps: MKDIR_BODY");
			}
		}
		/*
		 * Flush the inode on which the directory entry depends.
		 * Having accounted for MKDIR_PARENT and MKDIR_BODY above,
		 * the only remaining dependency is that the updated inode
		 * count must get pushed to disk. The inode has already
		 * been pushed into its inode buffer (via VOP_UPDATE) at
		 * the time of the reference count change. So we need only
		 * locate that buffer, ensure that there will be no rollback
		 * caused by a bitmap dependency, then write the inode buffer.
		 */
		if (inodedep_lookup(ump->um_fs, inum, 0, &inodedep) == 0) {
d4103 10
a4112 1
			panic("flush_pagedep_deps: lost inode");
d4114 13
a4126 9
		/*
		 * If the inode still has bitmap dependencies,
		 * push them to disk.
		 */
		if ((inodedep->id_state & DEPCOMPLETE) == 0) {
			gotit = getdirtybuf(&inodedep->id_buf, MNT_WAIT);
			FREE_LOCK(&lk);
			if (gotit &&
			    (error = bwrite(inodedep->id_buf)) != 0)
d4128 2
a4129 3
			ACQUIRE_LOCK(&lk);
			if (dap != LIST_FIRST(diraddhdp))
				continue;
d4131 4
a4134 8
		/*
		 * If the inode is still sitting in a buffer waiting
		 * to be written, push it to disk.
		 */
		FREE_LOCK(&lk);
		if ((error = bread(ump->um_devvp,
		    fsbtodb(ump->um_fs, ino_to_fsba(ump->um_fs, inum)),
		    (int)ump->um_fs->fs_bsize, NOCRED, &bp)) != 0)
a4135 3
		if ((error = bwrite(bp)) != 0)
			break;
		ACQUIRE_LOCK(&lk);
d4140 3
a4142 4
		if (dap == LIST_FIRST(diraddhdp)) {
			FREE_LOCK(&lk);
			panic("flush_pagedep_deps: flush failed");
		}
d4151 3
a4153 22
 * memory load excessively high. First attempt to slow things down
 * using the techniques below. If that fails, this routine requests
 * the offending operations to fall back to running synchronously
 * until the memory load returns to a reasonable level.
 */
int
softdep_slowdown(vp)
	struct vnode *vp;
{
	int max_softdeps_hard;

	max_softdeps_hard = max_softdeps * 11 / 10;
	if (num_dirrem < max_softdeps_hard / 2 &&
	    num_inodedep < max_softdeps_hard)
		return (0);
	stat_sync_limit_hit += 1;
	return (1);
}

/*
 * If memory utilization has gotten too high, deliberately slow things
 * down and speed up the I/O processing.
d4155 1
a4155 1
STATIC int
a4167 23
	 * First check to see if the work list has gotten backlogged.
	 * If it has, co-opt this process to help clean up two entries.
	 * Because this process may hold inodes locked, we cannot
	 * handle any remove requests that might block on a locked
	 * inode as that could lead to deadlock.
	 */
	if (num_on_worklist > max_softdeps / 10) {
		if (islocked)
			FREE_LOCK(&lk);
		process_worklist_item(NULL, LK_NOWAIT);
		process_worklist_item(NULL, LK_NOWAIT);
		stat_worklist_push += 2;
		if (islocked)
			ACQUIRE_LOCK(&lk);
		return(1);
	}
	/*
	 * Next, we attempt to speed up the syncer process. If that
	 * is successful, then we allow the process to continue.
	 */
	if (speedup_syncer())
		return(0);
	/*
d4181 1
a4181 2
		req_clear_inodedeps += 1;
		stat_countp = &stat_ino_limit_hit;
d4186 1
a4186 2
		req_clear_remove += 1;
		stat_countp = &stat_blk_limit_hit;
a4189 2
		if (islocked)
			FREE_LOCK(&lk);
d4198 4
a4201 4
	proc_waiting += 1;
	if (!timeout_pending(&proc_waiting_timeout))
		timeout_add(&proc_waiting_timeout, tickdelay > 2 ? tickdelay : 2);

d4203 1
a4203 1
	(void) tsleep((caddr_t)&proc_waiting, PPAUSE, "softupdate", 0);
d4205 15
a4219 1
	proc_waiting -= 1;
d4234 2
a4235 4
	*stat_countp += 1;
	wakeup_one(&proc_waiting);
	if (proc_waiting > 0)
		timeout_add(&proc_waiting_timeout, tickdelay > 2 ? tickdelay : 2);
d4239 2
a4240 2
 * Flush out a directory with at least one removal dependency in an effort to
 * reduce the number of dirrem, freefile, and freeblks dependency structures.
d4242 1
a4242 1
STATIC void
d4259 2
a4260 1
		LIST_FOREACH(pagedep, pagedephd, pd_hash) {
a4265 4
#if 0
			if (vn_start_write(NULL, &mp, V_NOWAIT) != 0)
				continue;
#endif
a4267 3
#if 0
				vn_finished_write(mp);
#endif
a4273 3
#if 0
			vn_finished_write(mp);
#endif
d4284 1
a4284 1
STATIC void
a4309 4
	if (inodedep == NULL) {
		FREE_LOCK(&lk);
		return;
	}
d4314 2
a4315 1
	CIRCLEQ_FOREACH(mp, &mountlist, mnt_list)
a4333 4
#if 0
		if (vn_start_write(NULL, &mp, V_NOWAIT) != 0)
			continue;
#endif
a4335 3
#if 0
			vn_finished_write(mp);
#endif
a4346 3
#if 0
		vn_finished_write(mp);
#endif
a4352 86
 * Function to determine if the buffer has outstanding dependencies
 * that will cause a roll-back if the buffer is written. If wantcount
 * is set, return number of dependencies, otherwise just yes or no.
 */
int
softdep_count_dependencies(bp, wantcount, islocked)
	struct buf *bp;
	int wantcount;
	int islocked;
{
	struct worklist *wk;
	struct inodedep *inodedep;
	struct indirdep *indirdep;
	struct allocindir *aip;
	struct pagedep *pagedep;
	struct diradd *dap;
	int i, retval;

	retval = 0;
	if (!islocked)
		ACQUIRE_LOCK(&lk);
	LIST_FOREACH(wk, &bp->b_dep, wk_list) {
		switch (wk->wk_type) {

		case D_INODEDEP:
			inodedep = WK_INODEDEP(wk);
			if ((inodedep->id_state & DEPCOMPLETE) == 0) {
				/* bitmap allocation dependency */
				retval += 1;
				if (!wantcount)
					goto out;
			}
			if (TAILQ_FIRST(&inodedep->id_inoupdt)) {
				/* direct block pointer dependency */
				retval += 1;
				if (!wantcount)
					goto out;
			}
			continue;

		case D_INDIRDEP:
			indirdep = WK_INDIRDEP(wk);

			LIST_FOREACH(aip, &indirdep->ir_deplisthd, ai_next) {
				/* indirect block pointer dependency */
				retval += 1;
				if (!wantcount)
					goto out;
			}
			continue;

		case D_PAGEDEP:
			pagedep = WK_PAGEDEP(wk);
			for (i = 0; i < DAHASHSZ; i++) {

				LIST_FOREACH(dap, &pagedep->pd_diraddhd[i], da_pdlist) {
					/* directory entry dependency */
					retval += 1;
					if (!wantcount)
						goto out;
				}
			}
			continue;

		case D_BMSAFEMAP:
		case D_ALLOCDIRECT:
		case D_ALLOCINDIR:
		case D_MKDIR:
			/* never a dependency on these blocks */
			continue;

		default:
			if (!islocked)
				FREE_LOCK(&lk);
			panic("softdep_check_for_rollback: Unexpected type %s",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
	}
out:
	if (!islocked)
		FREE_LOCK(&lk);
	return retval;
}

/*
d4357 1
a4357 1
STATIC int
d4387 1
a4387 1
STATIC void
d4392 1
a4392 1
	
d4396 1
a4396 1
		vp->v_bioflag |= VBIOWAIT;
d4398 1
a4398 1
		tsleep((caddr_t)&vp->v_numoutput, PRIBIO + 1, "drain_output", 0);
@


1.8.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.8.2.2 2001/05/14 22:47:40 niklas Exp $	*/
d2835 1
a2835 1
	if ((error = UFS_TRUNCATE(ip, (off_t)0, 0, p->p_ucred)) != 0)
d2907 2
d2910 6
a2915 3
	if ((error = ffs_freefile(&tip, freefile->fx_oldinum, 
		 freefile->fx_mode)) != 0) {
		softdep_error("handle_workitem_freefile", error);
d3898 1
d3963 2
a3964 1
			if ((error = UFS_UPDATE(VTOI(pvp), MNT_WAIT))) {
d4401 1
d4414 1
d4416 1
a4416 1
			if ((error = UFS_UPDATE(VTOI(pvp), MNT_WAIT)))
@


1.8.2.4
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d465 1
a465 1
#include <uvm/uvm_extern.h>
@


1.8.2.5
log
@Merge in -current
@
text
@a58 1
#include <sys/pool.h>
a71 4
#include <uvm/uvm.h>
struct pool sdpcpool;
int softdep_lockedbufs;

a111 7
 * Definitions for page cache info hashtable.
 */
#define PCBPHASHSIZE 1024
LIST_HEAD(, buf) pcbphashhead[PCBPHASHSIZE];
#define PCBPHASH(vp, lbn) ((((vaddr_t)(vp) >> 8) ^ (lbn)) & (PCBPHASHSIZE - 1))

/*
a162 7
STATIC struct buf *softdep_setup_pagecache __P((struct inode *, ufs_lbn_t,
						long));
STATIC void softdep_collect_pagecache __P((struct inode *));
STATIC void softdep_free_pagecache __P((struct inode *));
STATIC struct vnode *softdep_lookupvp(struct fs *, ino_t);
STATIC struct buf *softdep_lookup_pcbp __P((struct vnode *, ufs_lbn_t));
void softdep_pageiodone __P((struct buf *));
a178 1
	softdep_pageiodone,			/* io_pagedone */
d202 2
a203 2
#define ACQUIRE_LOCK_INTERLOCKED(lk,s)	(lk)->lkt_spl = (s)
#define FREE_LOCK_INTERLOCKED(lk)	((lk)->lkt_spl)
d215 2
a216 2
STATIC	void acquire_lock_interlocked __P((struct lockit *, int, int));
STATIC	int free_lock_interlocked __P((struct lockit *, int));
d220 1
a220 1
#define ACQUIRE_LOCK_INTERLOCKED(lk,s)	acquire_lock_interlocked(lk, (s), __LINE__)
d259 1
a259 1
acquire_lock_interlocked(lk, s, line)
a260 1
	int s;
a276 1
	lk->lkt_spl = s;
d280 1
a280 1
STATIC int
a288 2

	return (lk->lkt_spl);
a324 1
	int s;
d328 1
a328 1
			s = FREE_LOCK_INTERLOCKED(interlock);
d331 1
a331 1
			ACQUIRE_LOCK_INTERLOCKED(interlock, s);
a1057 1
	int i;
a1075 5
	pool_init(&sdpcpool, sizeof(struct buf), 0, 0, 0, "sdpcpool",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_TEMP);
	for (i = 0; i < PCBPHASHSIZE; i++) {
		LIST_INIT(&pcbphashhead[i]);
	}
a1327 11
	/*
	 * If we were not passed a bp to attach the dep to,
	 * then this must be for a regular file.
	 * Allocate a buffer to represent the page cache pages
	 * that are the real dependency.  The pages themselves
	 * cannot refer to the dependency since we don't want to
	 * add a field to struct vm_page for this.
	 */
	if (bp == NULL) {
		bp = softdep_setup_pagecache(ip, lbn, newsize);
	}
a1558 3
	if (nbp == NULL) {
		nbp = softdep_setup_pagecache(ip, lbn, ip->i_fs->fs_bsize);
	}
a1735 1
	vp = ITOV(ip);
a1794 5
	 * We must remove any pagecache markers from the pagecache
	 * hashtable first because any I/Os in flight will want to see
	 * dependencies attached to their pagecache markers.  We cannot
	 * free the pagecache markers until after we've freed all the
	 * dependencies that reference them later.
a1797 1
	softdep_collect_pagecache(ip);
d1809 1
a1820 1
	softdep_free_pagecache(ip);
d2889 1
d2905 2
a2906 1
	tip.i_vnode = NULL;
a4303 9
	struct vnode *vp;
	struct uvm_object *uobj;

	vp = softdep_lookupvp(fs, ino);
#ifdef DIAGNOSTIC
	if (vp == NULL)
		panic("flush_inodedep_deps: null vp");
#endif
	uobj = &vp->v_uvm.u_obj;
a4322 20

		/*
		 * When file data was in the buffer cache,
		 * softdep_sync_metadata() would start i/o on
		 * file data buffers itself.  But now that
		 * we're using the page cache to hold file data,
		 * we need something else to trigger those flushes.
		 * let's just do it here.
		 */
		FREE_LOCK(&lk);
		simple_lock(&uobj->vmobjlock);
		(uobj->pgops->pgo_flush)(uobj, 0, 0,
		    PGO_ALLPAGES|PGO_CLEANIT|
		    (waitfor == MNT_NOWAIT ? 0: PGO_SYNCIO));
		simple_unlock(&uobj->vmobjlock);
		if (waitfor == MNT_WAIT) {
			drain_output(vp, 0);
		}
		ACQUIRE_LOCK(&lk);

a4539 1
	int s;
d4608 1
a4608 1
	s = FREE_LOCK_INTERLOCKED(&lk);
d4610 1
a4610 1
	ACQUIRE_LOCK_INTERLOCKED(&lk, s);
a4864 1
	int s;
d4874 1
a4874 1
		s = FREE_LOCK_INTERLOCKED(&lk);
d4876 1
a4876 1
		ACQUIRE_LOCK_INTERLOCKED(&lk, s);
a4893 1
	int s;
d4899 1
a4899 1
		s = FREE_LOCK_INTERLOCKED(&lk);
d4901 1
a4901 1
		ACQUIRE_LOCK_INTERLOCKED(&lk, s);
a4933 191
}

/*
 * Allocate a buffer on which to attach a dependency.
 */
STATIC struct buf *
softdep_setup_pagecache(ip, lbn, size)
	struct inode *ip;
	ufs_lbn_t lbn;
	long size;
{
	struct vnode *vp = ITOV(ip);
	struct buf *bp;
	int s;

	/*
	 * Enter pagecache dependency buf in hash.
	 * Always reset b_resid to be the full amount of data in the block
	 * since the caller has the corresponding pages locked and dirty.
	 */

	bp = softdep_lookup_pcbp(vp, lbn);
	if (bp == NULL) {
		s = splbio();
		bp = pool_get(&sdpcpool, PR_WAITOK);
		splx(s);

		bp->b_vp = vp;
		bp->b_lblkno = lbn;
		LIST_INIT(&bp->b_dep);
		LIST_INSERT_HEAD(&pcbphashhead[PCBPHASH(vp, lbn)], bp, b_hash);
		LIST_INSERT_HEAD(&ip->i_pcbufhd, bp, b_vnbufs);
	}
	bp->b_bcount = bp->b_resid = size;
	return bp;
}

/*
 * softdep_collect_pagecache() and softdep_free_pagecache()
 * are used to remove page cache dependency buffers when
 * a file is being truncated to 0.
 */

STATIC void
softdep_collect_pagecache(ip)
	struct inode *ip;
{
	struct buf *bp;

	LIST_FOREACH(bp, &ip->i_pcbufhd, b_vnbufs) {
		LIST_REMOVE(bp, b_hash);
	}
}

STATIC void
softdep_free_pagecache(ip)
	struct inode *ip;
{
	struct buf *bp, *nextbp;

	for (bp = LIST_FIRST(&ip->i_pcbufhd); bp != NULL; bp = nextbp) {
		nextbp = LIST_NEXT(bp, b_vnbufs);
		LIST_REMOVE(bp, b_vnbufs);
		KASSERT(LIST_FIRST(&bp->b_dep) == NULL);
		pool_put(&sdpcpool, bp);
	}
}

STATIC struct vnode *
softdep_lookupvp(fs, ino)
	struct fs *fs;
	ino_t ino;
{
	struct mount *mp;
	extern struct vfsops ffs_vfsops;

	CIRCLEQ_FOREACH(mp, &mountlist, mnt_list) {
		if (mp->mnt_op == &ffs_vfsops &&
		    VFSTOUFS(mp)->um_fs == fs) {
			break;
		}
	}
	if (mp == NULL) {
		return NULL;
	}
	return ufs_ihashlookup(VFSTOUFS(mp)->um_dev, ino);
}

STATIC struct buf *
softdep_lookup_pcbp(vp, lbn)
	struct vnode *vp;
	ufs_lbn_t lbn;
{
	struct buf *bp;

	LIST_FOREACH(bp, &pcbphashhead[PCBPHASH(vp, lbn)], b_hash) {
		if (bp->b_vp == vp && bp->b_lblkno == lbn) {
			break;
		}
	}
	return bp;	     
}

/*
 * Do softdep i/o completion processing for page cache writes.
 */
 
void
softdep_pageiodone(bp)
	struct buf *bp;
{
	int npages = bp->b_bufsize >> PAGE_SHIFT;
	struct vnode *vp = bp->b_vp;
	struct vm_page *pg;
	struct buf *pcbp = NULL;
	struct allocdirect *adp;
	struct allocindir *aip;
	struct worklist *wk;
	ufs_lbn_t lbn;
	voff_t off;
	long iosize = bp->b_bcount;
	int size, asize, bshift, bsize;
	int i;

	KASSERT(!(bp->b_flags & B_READ));
	bshift = vp->v_mount->mnt_fs_bshift;
	bsize = 1 << bshift;
	asize = min(PAGE_SIZE, bsize);
	ACQUIRE_LOCK(&lk);
	for (i = 0; i < npages; i++) {
		pg = uvm_pageratop((vaddr_t)bp->b_data + (i << PAGE_SHIFT));
		if (pg == NULL) {
			continue;
		}

		for (off = pg->offset;
		     off < pg->offset + PAGE_SIZE;
		     off += bsize) {
			size = min(asize, iosize);
			iosize -= size;
			lbn = off >> bshift;
			if (pcbp == NULL || pcbp->b_lblkno != lbn) {
				pcbp = softdep_lookup_pcbp(vp, lbn);
			}
			if (pcbp == NULL) {
				continue;
			}
			pcbp->b_resid -= size;
			if (pcbp->b_resid < 0) {
				panic("softdep_pageiodone: "
				      "resid < 0, vp %p lbn 0x%lx pcbp %p",
				      vp, lbn, pcbp);
			}
			if (pcbp->b_resid > 0) {
				continue;
			}

			/*
			 * We've completed all the i/o for this block.
			 * mark the dep complete.
			 */

			KASSERT(LIST_FIRST(&pcbp->b_dep) != NULL);
			while ((wk = LIST_FIRST(&pcbp->b_dep))) {
				WORKLIST_REMOVE(wk);
				switch (wk->wk_type) {
				case D_ALLOCDIRECT:
					adp = WK_ALLOCDIRECT(wk);
					adp->ad_state |= COMPLETE;
					handle_allocdirect_partdone(adp);
					break;

				case D_ALLOCINDIR:
					aip = WK_ALLOCINDIR(wk);
					aip->ai_state |= COMPLETE;
					handle_allocindir_partdone(aip);
					break;

				default:
					panic("softdep_pageiodone: "
					      "bad type %d, pcbp %p wk %p",
					      wk->wk_type, pcbp, wk);
				}
			}
			LIST_REMOVE(pcbp, b_hash);
			LIST_REMOVE(pcbp, b_vnbufs);
			pool_put(&sdpcpool, pcbp);
			pcbp = NULL;
		}
	}
	FREE_LOCK(&lk);
@


1.8.2.6
log
@Merge in trunk
@
text
@d44 1
a44 1
 * For now we want the safety net that the DIAGNOSTIC flag provide.
d49 3
a63 1
#include <ufs/ufs/extattr.h>
d73 4
d80 7
d89 19
a107 34
#define	D_PAGEDEP	1
#define	D_INODEDEP	2
#define	D_NEWBLK	3
#define	D_BMSAFEMAP	4
#define	D_ALLOCDIRECT	5
#define	D_INDIRDEP	6
#define	D_ALLOCINDIR	7
#define	D_FREEFRAG	8
#define	D_FREEBLKS	9
#define	D_FREEFILE	10
#define	D_DIRADD	11
#define	D_MKDIR		12
#define	D_DIRREM	13
#define	D_LAST		13
/*
 * Names of softdep types.
 */
const char *softdep_typenames[] = {
	"invalid",
	"pagedep",
	"inodedep",
	"newblk",
	"bmsafemap",
	"allocdirect",
	"indirdep",
	"allocindir",
	"freefrag",
	"freeblks",
	"diradd",
	"mkdir",
	"dirrem",
};
#define	TYPENAME(type) \
	((unsigned)(type) < D_LAST ? softdep_typenames[type] : "???")
d117 7
d175 7
d198 1
a372 1
#ifdef DEBUG
a374 1
#endif
a384 104
 * Memory management.
 */
STATIC struct pool pagedep_pool;
STATIC struct pool inodedep_pool;
STATIC struct pool newblk_pool;
STATIC struct pool bmsafemap_pool;
STATIC struct pool allocdirect_pool;
STATIC struct pool indirdep_pool;
STATIC struct pool allocindir_pool;
STATIC struct pool freefrag_pool;
STATIC struct pool freeblks_pool;
STATIC struct pool freefile_pool;
STATIC struct pool diradd_pool;
STATIC struct pool mkdir_pool;
STATIC struct pool dirrem_pool;

static __inline void
softdep_free(struct worklist *item, int type)
{

	switch (type) {
	case D_PAGEDEP:
		pool_put(&pagedep_pool, item);
		break;

	case D_INODEDEP:
		pool_put(&inodedep_pool, item);
		break;

	case D_BMSAFEMAP:
		pool_put(&bmsafemap_pool, item);
		break;

	case D_ALLOCDIRECT:
		pool_put(&allocdirect_pool, item);
		break;

	case D_INDIRDEP:
		pool_put(&indirdep_pool, item);
		break;

	case D_ALLOCINDIR:
		pool_put(&allocindir_pool, item);
		break;

	case D_FREEFRAG:
		pool_put(&freefrag_pool, item);
		break;

	case D_FREEBLKS:
		pool_put(&freeblks_pool, item);
		break;

	case D_FREEFILE:
		pool_put(&freefile_pool, item);
		break;

	case D_DIRADD:
		pool_put(&diradd_pool, item);
		break;

	case D_MKDIR:
		pool_put(&mkdir_pool, item);
		break;

	case D_DIRREM:
		pool_put(&dirrem_pool, item);
		break;

	default:
#ifdef DEBUG
		if (lk.lkt_held != -1)
			FREE_LOCK(&lk);
#endif
		panic("softdep_free: unknown type %d", type);
	}
}

struct workhead softdep_freequeue;

static __inline void
softdep_freequeue_add(struct worklist *item)
{
	int s;

	s = splbio();
	LIST_INSERT_HEAD(&softdep_freequeue, item, wk_list);
	splx(s);
}

static __inline void
softdep_freequeue_process(void)
{
	struct worklist *wk;

	while ((wk = LIST_FIRST(&softdep_freequeue)) != NULL) {
		LIST_REMOVE(wk, wk_list);
		FREE_LOCK(&lk);
		softdep_free(wk, wk->wk_type);
		ACQUIRE_LOCK(&lk);
	}
}

/*
d397 1
a397 1
#define WORKITEM_FREE(item, type) softdep_freequeue_add((struct worklist *)item)
d402 1
a402 1
STATIC	void workitem_free __P((struct worklist *));
d406 1
a406 1
#define WORKITEM_FREE(item, type) workitem_free((struct worklist *)item)
d440 1
a440 1
workitem_free(item)
d442 1
d450 6
a455 1
	softdep_freequeue_add(item);
d489 16
a519 1
#ifdef DEBUG
a521 1
#endif
a550 7
	 * First process any items on the delayed-free queue.
	 */
	ACQUIRE_LOCK(&lk);
	softdep_freequeue_process();
	FREE_LOCK(&lk);

	/*
a633 7

		/*
		 * Process any new items on the delayed-free queue.
		 */
		ACQUIRE_LOCK(&lk);
		softdep_freequeue_process();
		FREE_LOCK(&lk);
d928 2
a929 1
	pagedep = pool_get(&pagedep_pool, PR_WAITOK);
d1004 2
a1005 1
	inodedep = pool_get(&inodedep_pool, PR_WAITOK);
d1065 2
a1066 1
	newblk = pool_get(&newblk_pool, PR_WAITOK);
d1083 1
d1102 5
a1106 26
	pool_init(&pagedep_pool, sizeof(struct pagedep), 0, 0, 0,
	    "pagedeppl", &pool_allocator_nointr);
	pool_init(&inodedep_pool, sizeof(struct inodedep), 0, 0, 0,
	    "inodedeppl", &pool_allocator_nointr);
	pool_init(&newblk_pool, sizeof(struct newblk), 0, 0, 0,
	    "newblkpl", &pool_allocator_nointr);
	pool_init(&bmsafemap_pool, sizeof(struct bmsafemap), 0, 0, 0,
	    "bmsafemappl", &pool_allocator_nointr);
	pool_init(&allocdirect_pool, sizeof(struct allocdirect), 0, 0, 0,
	    "allocdirectpl", &pool_allocator_nointr);
	pool_init(&indirdep_pool, sizeof(struct indirdep), 0, 0, 0,
	    "indirdeppl", &pool_allocator_nointr);
	pool_init(&allocindir_pool, sizeof(struct allocindir), 0, 0, 0,
	    "allocindirpl", &pool_allocator_nointr);
	pool_init(&freefrag_pool, sizeof(struct freefrag), 0, 0, 0,
	    "freefragpl", &pool_allocator_nointr);
	pool_init(&freeblks_pool, sizeof(struct freeblks), 0, 0, 0,
	    "freeblkspl", &pool_allocator_nointr);
	pool_init(&freefile_pool, sizeof(struct freefile), 0, 0, 0,
	    "freefilepl", &pool_allocator_nointr);
	pool_init(&diradd_pool, sizeof(struct diradd), 0, 0, 0,
	    "diraddpl", &pool_allocator_nointr);
	pool_init(&mkdir_pool, sizeof(struct mkdir), 0, 0, 0,
	    "mkdirpl", &pool_allocator_nointr);
	pool_init(&dirrem_pool, sizeof(struct dirrem), 0, 0, 0,
	    "dirrempl", &pool_allocator_nointr);
d1265 2
a1266 1
	bmsafemap = pool_get(&bmsafemap_pool, PR_WAITOK);
d1325 2
a1326 1
	adp = pool_get(&allocdirect_pool, PR_WAITOK);
d1357 1
a1357 1
	pool_put(&newblk_pool, newblk);
d1359 8
d1368 1
a1368 4
		/*
		 * XXXUBC - Yes, I know how to fix this, but not right now.
		 */
		panic("softdep_setup_allocdirect: Bonk art in the head\n");
d1493 2
a1494 1
	freefrag = pool_get(&freefrag_pool, PR_WAITOK);
d1522 1
a1522 1
	pool_put(&freefrag_pool, freefrag);
d1562 2
a1563 1
	aip = pool_get(&allocindir_pool, PR_WAITOK);
d1602 1
a1602 4
		/*
		 * XXXUBC - Yes, I know how to fix this, but not right now.
		 */
		panic("softdep_setup_allocindir_page: Bonk art in the head\n");
d1679 1
a1679 1
			pool_put(&newblk_pool, newblk);
d1715 1
a1715 1
			WORKITEM_FREE(newindirdep, D_INDIRDEP);
d1719 2
a1720 1
		newindirdep = pool_get(&indirdep_pool, PR_WAITOK);
d1781 1
d1784 2
a1785 1
	freeblks = pool_get(&freeblks_pool, PR_WAITOK);
d1841 5
d1849 1
a1860 1
	vp = ITOV(ip);
d1872 1
d2043 2
a2044 1
	freefile = pool_get(&freefile_pool, PR_WAITOK);
d2366 2
a2367 1
	dap = pool_get(&diradd_pool, PR_WAITOK);
d2378 2
a2379 1
		mkdir1 = pool_get(&mkdir_pool, PR_WAITOK);
d2383 2
a2384 1
		mkdir2 = pool_get(&mkdir_pool, PR_WAITOK);
d2620 2
a2621 1
	dirrem = pool_get(&dirrem_pool, PR_WAITOK);
d2721 2
a2722 1
		dap = pool_get(&diradd_pool, PR_WAITOK);
a2940 1
	struct vnode vp;
a2941 1
#ifdef DEBUG
a2942 1
#endif
d2956 1
a2956 2
	tip.i_vnode = &vp;
	vp.v_data = &tip;
d3037 1
a3037 1
			    M_INDIRDEP, M_WAITOK);
d3139 1
a3139 1
		    sizeof(struct dinode), M_INODEDEP, M_WAITOK);
d4354 9
d4382 20
d5016 191
@


1.8.2.7
log
@Merge in -current from roughly a week ago
@
text
@d121 49
a169 49
STATIC	void softdep_error(char *, int);
STATIC	void drain_output(struct vnode *, int);
STATIC	int getdirtybuf(struct buf **, int);
STATIC	void clear_remove(struct proc *);
STATIC	void clear_inodedeps(struct proc *);
STATIC	int flush_pagedep_deps(struct vnode *, struct mount *,
	    struct diraddhd *);
STATIC	int flush_inodedep_deps(struct fs *, ino_t);
STATIC	int handle_written_filepage(struct pagedep *, struct buf *);
STATIC  void diradd_inode_written(struct diradd *, struct inodedep *);
STATIC	int handle_written_inodeblock(struct inodedep *, struct buf *);
STATIC	void handle_allocdirect_partdone(struct allocdirect *);
STATIC	void handle_allocindir_partdone(struct allocindir *);
STATIC	void initiate_write_filepage(struct pagedep *, struct buf *);
STATIC	void handle_written_mkdir(struct mkdir *, int);
STATIC	void initiate_write_inodeblock(struct inodedep *, struct buf *);
STATIC	void handle_workitem_freefile(struct freefile *);
STATIC	void handle_workitem_remove(struct dirrem *);
STATIC	struct dirrem *newdirrem(struct buf *, struct inode *,
	    struct inode *, int, struct dirrem **);
STATIC	void free_diradd(struct diradd *);
STATIC	void free_allocindir(struct allocindir *, struct inodedep *);
STATIC	int indir_trunc(struct inode *, ufs_daddr_t, int, ufs_lbn_t,
	    long *);
STATIC	void deallocate_dependencies(struct buf *, struct inodedep *);
STATIC	void free_allocdirect(struct allocdirectlst *,
	    struct allocdirect *, int);
STATIC	int check_inode_unwritten(struct inodedep *);
STATIC	int free_inodedep(struct inodedep *);
STATIC	void handle_workitem_freeblocks(struct freeblks *);
STATIC	void merge_inode_lists(struct inodedep *);
STATIC	void setup_allocindir_phase2(struct buf *, struct inode *,
	    struct allocindir *);
STATIC	struct allocindir *newallocindir(struct inode *, int, ufs_daddr_t,
	    ufs_daddr_t);
STATIC	void handle_workitem_freefrag(struct freefrag *);
STATIC	struct freefrag *newfreefrag(struct inode *, ufs_daddr_t, long);
STATIC	void allocdirect_merge(struct allocdirectlst *,
	    struct allocdirect *, struct allocdirect *);
STATIC	struct bmsafemap *bmsafemap_lookup(struct buf *);
STATIC	int newblk_lookup(struct fs *, ufs_daddr_t, int,
	    struct newblk **);
STATIC	int inodedep_lookup(struct fs *, ino_t, int, struct inodedep **);
STATIC	int pagedep_lookup(struct inode *, ufs_lbn_t, int,
	    struct pagedep **);
STATIC	void pause_timer(void *);
STATIC	int request_cleanup(int, int);
STATIC	int process_worklist_item(struct mount *, int);
STATIC	void add_to_worklist(struct worklist *);
d174 5
a178 5
void softdep_disk_io_initiation(struct buf *);
void softdep_disk_write_complete(struct buf *);
void softdep_deallocate_dependencies(struct buf *);
void softdep_move_dependencies(struct buf *, struct buf *);
int softdep_count_dependencies(struct buf *bp, int, int);
d220 4
a223 4
STATIC	void acquire_lock(struct lockit *, int);
STATIC	void free_lock(struct lockit *, int);
STATIC	void acquire_lock_interlocked(struct lockit *, int, int);
STATIC	int free_lock_interlocked(struct lockit *, int);
d313 3
a315 3
STATIC	void sema_init(struct sema *, char *, int, int);
STATIC	int sema_get(struct sema *, struct lockit *);
STATIC	void sema_release(struct sema *);
d493 3
a495 3
STATIC	void worklist_insert(struct workhead *, struct worklist *);
STATIC	void worklist_remove(struct worklist *);
STATIC	void workitem_free(struct worklist *);
@


1.8.2.8
log
@Sync the SMP branch with 3.3
@
text
@d43 7
a103 1
	"freefile",
d109 1
a109 1
	((unsigned)(type) <= D_LAST ? softdep_typenames[type] : "???")
d931 2
d1465 1
a1465 1
		panic("softdep_setup_allocdirect: Bonk art in the head");
d1700 1
a1700 1
		panic("softdep_setup_allocindir_page: Bonk art in the head");
d2266 1
a2266 1
		    baselbns[level], &blocksreleased)) != 0)
a3208 1
#ifdef DIAGNOSTIC
a3209 1
#endif
@


1.8.2.9
log
@merge the trunk so we will get the genfs and locking fixes
@
text
@d174 8
a1158 6

	bioops.io_start = softdep_disk_io_initiation;
	bioops.io_complete = softdep_disk_write_complete;
	bioops.io_deallocate = softdep_deallocate_dependencies;
	bioops.io_movedeps = softdep_move_dependencies;
	bioops.io_countdeps = softdep_count_dependencies;
@


1.8.2.10
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: ffs_softdep.c,v 1.8.2.9 2003/05/16 00:29:45 niklas Exp $	*/
d137 1
a137 1
STATIC	int indir_trunc(struct inode *, daddr_t, int, ufs_lbn_t,
d148 2
a149 2
STATIC	struct allocindir *newallocindir(struct inode *, int, daddr_t,
	    daddr_t);
d151 1
a151 1
STATIC	struct freefrag *newfreefrag(struct inode *, daddr_t, long);
d155 1
a155 1
STATIC	int newblk_lookup(struct fs *, daddr_t, int,
d1112 1
a1112 1
	daddr_t newblkno;
d1320 1
a1320 1
	daddr_t newblkno;	/* number of newly allocated block */
d1405 2
a1406 2
	daddr_t newblkno;	/* disk block number being added */
	daddr_t oldblkno;	/* previous block number, 0 unless frag */
d1569 1
a1569 1
	daddr_t blkno;
d1643 2
a1644 2
	daddr_t newblkno;	/* disk block number being added */
	daddr_t oldblkno;	/* previous block number, 0 if none */
d1669 2
a1670 2
	daddr_t newblkno;	/* disk block number being added */
	daddr_t oldblkno;	/* previous block number, 0 if none */
d1707 1
a1707 1
	daddr_t newblkno;	/* disk block number being added */
d1794 1
a1794 1
			((daddr_t *)indirdep->ir_savebp->b_data)
d2227 1
a2227 1
	daddr_t bn;
d2290 1
a2290 1
	daddr_t dbn;
d2296 2
a2297 2
	daddr_t *bap;
	daddr_t nb;
d2345 1
a2345 1
	bap = (daddr_t *)bp->b_data;
d3552 1
a3552 1
	((daddr_t *)indirdep->ir_savebp->b_data)[aip->ai_offset] =
@


1.8.2.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1603 1
a1603 1
	tip.i_ump = VFSTOUFS(freefrag->ff_mnt);
d1899 2
a1900 2
	*((struct ufs1_dinode *)bp->b_data + ino_to_fsbo(fs, ip->i_number)) =
	    ip->i_din1;
d2236 1
a2236 1
	tip.i_ump = VFSTOUFS(freeblks->fb_mnt);
d2274 1
a2274 1
		printf("handle_workitem_freeblocks: block count\n");
d3029 1
a3029 1
	tip.i_ump = VFSTOUFS(freefile->fx_mnt);
d3197 1
a3197 1
	struct ufs1_dinode *dp;
d3208 1
a3208 1
	dp = (struct ufs1_dinode *)bp->b_data +
d3217 2
a3218 2
		MALLOC(inodedep->id_savedino, struct ufs1_dinode *,
		    sizeof(struct ufs1_dinode), M_INODEDEP, M_WAITOK);
d3220 1
a3220 1
		bzero((caddr_t)dp, sizeof(struct ufs1_dinode));
d3573 1
a3573 1
	struct ufs1_dinode *dp;
d3580 1
a3580 1
	dp = (struct ufs1_dinode *)bp->b_data +
@


1.8.2.12
log
@Merge with the trunk
@
text
@d69 13
a81 14
#define	D_PAGEDEP	0
#define	D_INODEDEP	1
#define	D_NEWBLK	2
#define	D_BMSAFEMAP	3
#define	D_ALLOCDIRECT	4
#define	D_INDIRDEP	5
#define	D_ALLOCINDIR	6
#define	D_FREEFRAG	7
#define	D_FREEBLKS	8
#define	D_FREEFILE	9
#define	D_DIRADD	10
#define	D_MKDIR		11
#define	D_DIRREM	12
#define	D_NEWDIRBLK	13
d87 1
a100 1
	"newdirblk",
a136 1
STATIC	void free_newdirblk(struct newdirblk *);
a374 1
STATIC struct pool newdirblk_pool;
a428 4
	case D_NEWDIRBLK:
		pool_put(&newdirblk_pool, item);
		break;

d958 1
a958 2
 * Look up a pagedep. Return 1 if found, 0 if not found or found
 * when asked to allocate but not associated with any buffer.
a988 3
		if ((flags & DEPALLOC) != 0 &&
		    (pagedep->pd_state & ONWORKLIST) == 0)
			return (0);
a1200 2
	pool_init(&newdirblk_pool, sizeof(struct newdirblk), 0, 0, 0,
	    "newdirblkpl", &pool_allocator_nointr);
a1426 1
	LIST_INIT(&adp->ad_newdirblk);
a1522 1
	struct worklist *wk;
a1523 1
	struct newdirblk *newdirblk;
d1533 1
a1533 1
		panic("allocdirect_merge: old %d != new %d || lbn %ld >= %d",
a1559 11
	/*
	 * If we are tracking a new directory-block allocation,
	 * move it from the old allocdirect to the new allocdirect.
	 */
	if ((wk = LIST_FIRST(&oldadp->ad_newdirblk)) != NULL) {
		newdirblk = WK_NEWDIRBLK(wk);
		WORKLIST_REMOVE(&newdirblk->db_list);
		if (LIST_FIRST(&oldadp->ad_newdirblk) != NULL)
			panic("allocdirect_merge: extra newdirblk");
		WORKLIST_INSERT(&newadp->ad_newdirblk, &newdirblk->db_list);
	}
a2049 15
			if ((pagedep->pd_state & NEWBLOCK) != 0) {
				LIST_FOREACH(wk, &inodedep->id_bufwait, wk_list)
					if (wk->wk_type == D_NEWDIRBLK &&
					    WK_NEWDIRBLK(wk)->db_pagedep ==
					    pagedep)
						break;
				if (wk != NULL) {
					WORKLIST_REMOVE(wk);
					free_newdirblk(WK_NEWDIRBLK(wk));
				} else {
					FREE_LOCK(&lk);
					panic("deallocate_dependencies: "
					    "lost pagedep");
					}
			}
a2084 2
	struct newdirblk *newdirblk;
	struct worklist *wk;
a2101 11
	if ((wk = LIST_FIRST(&adp->ad_newdirblk)) != NULL) {
		newdirblk = WK_NEWDIRBLK(wk);
		WORKLIST_REMOVE(&newdirblk->db_list);
		if (LIST_FIRST(&adp->ad_newdirblk) != NULL)
			panic("free_allocdirect: extra newdirblk");
		if (delay)
			WORKLIST_INSERT(&adp->ad_inodedep->id_bufwait,
			    &newdirblk->db_list);
		else
			free_newdirblk(newdirblk);
	}
a2105 44
 * Free a newdirblk. Clear the NEWBLOCK flag on its associated pagedep.
 * This routine must be called with splbio interrupts blocked.
 */
void
free_newdirblk(newdirblk)
	struct newdirblk *newdirblk;
{
	struct pagedep *pagedep;
	struct diradd *dap;
	int i;

#ifdef DEBUG
	if (lk.lkt_held == -1)
		panic("free_newdirblk: lock not held");
#endif
	/*
	 * If the pagedep is still linked onto the directory buffer
	 * dependency chain, then some of the entries on the
	 * pd_pendinghd list may not be committed to disk yet. In
	 * this case, we will simply clear the NEWBLOCK flag and
	 * let the pd_pendinghd list be processed when the pagedep
	 * is next written. If the pagedep is no longer on the buffer
	 * dependency chain, then all the entries on the pd_pending
	 * list are committed to disk and we can free them here.
	 */
	pagedep = newdirblk->db_pagedep;
	pagedep->pd_state &= ~NEWBLOCK;
	if ((pagedep->pd_state & ONWORKLIST) == 0)
		while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
			free_diradd(dap);
	/*
	 * If no dependencies remain, the pagedep will be freed.
	 */
	for (i = 0; i < DAHASHSZ; i++)
		if (LIST_FIRST(&pagedep->pd_diraddhd[i]) != NULL)
			break;
	if (i == DAHASHSZ && (pagedep->pd_state & ONWORKLIST) == 0) {
		LIST_REMOVE(pagedep, pd_hash);
		WORKITEM_FREE(pagedep, D_PAGEDEP);
	}
	WORKITEM_FREE(newdirblk, D_NEWDIRBLK);
}

/*
d2416 2
a2417 2
int 
softdep_setup_directory_add(bp, dp, diroffset, newinum, newdirbp, isnewblk)
a2422 1
	int isnewblk;		/* entry is in a newly allocated block */
a2427 1
	struct allocdirect *adp;
a2429 1
	struct newdirblk *newdirblk = NULL;
a2430 1
	
d2438 1
a2438 1
		return (0);
a2449 5
	if (isnewblk && lbn < NDADDR && fragoff(fs, diroffset) == 0) {
		newdirblk = pool_get(&newdirblk_pool, PR_WAITOK);
		newdirblk->db_list.wk_type = D_NEWDIRBLK;
		newdirblk->db_state = 0;
	}
d2476 1
a2476 1
		if (inodedep_lookup(fs, dp->i_number, 0, &inodedep) == 0
a2502 49
	if (isnewblk) {
		/*
		 * Directories growing into indirect blocks are rare
		 * enough and the frequency of new block allocation
		 * in those cases even more rare, that we choose not
		 * to bother tracking them. Rather we simply force the
		 * new directory entry to disk.
		 */
		if (lbn >= NDADDR) {
			FREE_LOCK(&lk);
			/*
			 * We only have a new allocation when at the
			 * beginning of a new block, not when we are
			 * expanding into an existing block.
			 */
			if (blkoff(fs, diroffset) == 0)
				return (1);
			return (0);
		}
		/*
		 * We only have a new allocation when at the beginning
		 * of a new fragment, not when we are expanding into an
		 * existing fragment. Also, there is nothing to do if we
		 * are already tracking this block.
		 */
		if (fragoff(fs, diroffset) != 0) {
			FREE_LOCK(&lk);
			return (0);
		}
			
		if ((pagedep->pd_state & NEWBLOCK) != 0) {
			WORKITEM_FREE(newdirblk, D_NEWDIRBLK);
			FREE_LOCK(&lk);
			return (0);
		}
		/*
		 * Find our associated allocdirect and have it track us.
		 */
		if (inodedep_lookup(fs, dp->i_number, 0, &inodedep) == 0)
			panic("softdep_setup_directory_add: lost inodedep");
		adp = TAILQ_LAST(&inodedep->id_newinoupdt, allocdirectlst);
		if (adp == NULL || adp->ad_lbn != lbn) {
			FREE_LOCK(&lk);
			panic("softdep_setup_directory_add: lost entry");
		}
		pagedep->pd_state |= NEWBLOCK;
		newdirblk->db_pagedep = pagedep;
		WORKLIST_INSERT(&adp->ad_newdirblk, &newdirblk->db_list);
	}
a2503 1
	return (0);
d3113 1
a3113 1
			indirdep->ir_saveddata = malloc(bp->b_bcount,
d3423 1
a3423 1
			free(indirdep->ir_saveddata, M_INDIRDEP);
a3687 4
		case D_NEWDIRBLK:
			free_newdirblk(WK_NEWDIRBLK(wk));
			continue;

a3790 2
	 * If it is a newly allocated block, we have to wait until
	 * the on-disk directory inode claims the new block.
d3792 2
a3793 3
	if ((pagedep->pd_state & NEWBLOCK) == 0)
		while ((dap = LIST_FIRST(&pagedep->pd_pendinghd)) != NULL)
			free_diradd(dap);
a3829 1
		return (1);
d3832 13
a3844 8
	 * If we are not waiting for a new directory block to be
	 * claimed by its inode, then the pagedep will be freed.
	 * Otherwise it will remain to track any new entries on
	 * the page in case they are fsync'ed.
	 */
	if ((pagedep->pd_state & NEWBLOCK) == 0) {
		LIST_REMOVE(pagedep, pd_hash);
		WORKITEM_FREE(pagedep, D_PAGEDEP);
d3846 1
a3846 1
	return (0);
a4015 1
	struct inode *pip;
d4047 2
a4048 2
		 * Flush our parent if this directory entry has a MKDIR_PARENT
		 * dependency or is contained in a newly allocated block.
d4061 1
a4061 5
		if ((dap->da_state & MKDIR_PARENT) ||
		    (pagedep->pd_state & NEWBLOCK))
			flushparent = 1;
		else
			flushparent = 0;
a4084 9
		/*
		 * All MKDIR_PARENT dependencies and all the NEWBLOCK pagedeps
		 * that are contained in direct blocks will be resolved by 
		 * doing a UFS_UPDATE. Pagedeps contained in indirect blocks
		 * may require a complete sync'ing of the directory. So, we
		 * try the cheap and fast UFS_UPDATE first, and if that fails,
		 * then we do the slower VOP_FSYNC of the directory.
		 */
		pip = VTOI(pvp);
d4086 1
a4086 6
			if ((error = UFS_UPDATE(pip, 1)) != 0) {
				vput(pvp);
				return (error);
			}
			if ((pagedep->pd_state & NEWBLOCK) &&
			    (error = UFS_UPDATE(pip, MNT_WAIT))) {
d4094 1
a4094 1
		error = bread(pvp, lbn, blksize(fs, pip, lbn), p->p_ucred,
a4220 5
	/*
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
	 */
	drain_output(vp, 1);
d4377 7
d4385 1
a4385 1
	 * processing to be done. Then proceed with the second pass.
a4397 3
	 *
	 * We must wait for any I/O in progress to finish so that
	 * all potential buffers on the dirty list will be visible.
a4398 1
	drain_output(vp, 1);
a4893 1
		drain_output(vp, 1);
@


1.7
log
@zap the newhashinit hack.
Add an extra flag to hashinit telling if it should wait in malloc.
update all calls to hashinit.
@
text
@d55 1
a55 1
 *	@@(#)ffs_softdep.c	9.30 (McKusick) 10/3/98
a57 2
#ifdef FFS_SOFTUPDATES

d125 1
d127 2
d169 1
a169 1
static	int checklimit __P((long *, int));
d179 1
a179 1
	softdep_fsync,                          /* io_fsync */
a413 2
static int max_limit_hit;	/* number of times slowdown imposed */
static int rush_requests;	/* number of times I/O speeded up */
d415 16
a430 1
static pid_t filesys_syncer_pid;/* records pid of filesystem syncer process */
d434 10
a443 4
struct ctldebug debug8 = { "max_softdeps", &max_softdeps };
struct ctldebug debug9 = { "tickdelay", &tickdelay };
struct ctldebug debug10 = { "max_limit_hit", &max_limit_hit };
struct ctldebug debug11 = { "rush_requests", &rush_requests };
d488 2
a489 2
	 * Record the process identifier of our caller so that we can
	 * give this process preferential treatment in checklimit below.
d491 1
a491 1
	filesys_syncer_pid = p->p_pid;
d504 13
d558 13
a640 65
 * A large burst of file addition or deletion activity can drive the
 * memory load excessively high. Therefore we deliberately slow things
 * down and speed up the I/O processing if we find ourselves with too
 * many dependencies in progress.
 */
static int
checklimit(resource, islocked)
	long *resource;
	int islocked;
{
	struct proc *p = CURPROC;

	/*
	 * If we are under our limit, just proceed.
	 */
	if (*resource < max_softdeps)
		return (0);
	/*
	 * We never hold up the filesystem syncer process.
	 */
	if (p->p_pid == filesys_syncer_pid)
		return (0);
	/*
	 * Our first approach is to speed up the syncer process.
	 * We never push it to speed up more than half of its
	 * normal turn time, otherwise it could take over the cpu.
	 */
	if (rushjob < syncdelay / 2) {
		rushjob += 1;
		rush_requests += 1;
		return (0);
	}
	/*
	 * Every trick has failed, so we pause momentarily to let
	 * the filesystem syncer process catch up.
	 */
	if (islocked == 0)
		ACQUIRE_LOCK(&lk);
	if (proc_waiting == 0) {
		proc_waiting = 1;
		timeout(pause_timer, NULL, tickdelay > 2 ? tickdelay : 2);
	}
	FREE_LOCK_INTERLOCKED(&lk);
	(void) tsleep((caddr_t)&proc_waiting, PPAUSE | PCATCH, "softupdate", 0);
	ACQUIRE_LOCK_INTERLOCKED(&lk);
	if (islocked == 0)
		FREE_LOCK(&lk);
	max_limit_hit += 1;
	return (1);
}

/*
 * Awaken processes pausing in checklimit and clear proc_waiting
 * to indicate that there is no longer a timer running.
 */
void
pause_timer(arg)
	void *arg;
{

	proc_waiting = 0;
	wakeup(&proc_waiting);
}

/*
d781 5
a785 1
	if (firsttry && checklimit(&num_inodedep, 1) == 1) {
d928 2
a929 14
	if (bcmp(&cstotal, &fs->fs_cstotal, sizeof cstotal)) {
		printf("ffs_mountfs: superblock updated\n");
		printf ("%d %d %d %d\n", 
			cstotal.cs_nffree,
			cstotal.cs_nbfree,
			cstotal.cs_nifree,
			cstotal.cs_ndir);

		printf ("%d %d %d %d\n", 
			fs->fs_cstotal.cs_nffree,
			fs->fs_cstotal.cs_nbfree,
			fs->fs_cstotal.cs_nifree,
			fs->fs_cstotal.cs_ndir);
	}
d1208 1
a1208 1
		panic("allocdirect_check: old %d != new %d || lbn %d >= %d",
d1532 5
a1536 1
	(void) checklimit(&num_freeblks, 0);
d1592 1
d1601 2
a1602 6
	while (vp->v_numoutput) {
		vp->v_flag |= VBWAIT;
		FREE_LOCK_INTERLOCKED(&lk);
		sleep((caddr_t)&vp->v_numoutput, PRIBIO + 1);
		ACQUIRE_LOCK_INTERLOCKED(&lk);
	}
d1608 1
d1610 1
d1680 2
a1681 1
				while ((dap=LIST_FIRST(&pagedep->pd_diraddhd[i])))
d1772 5
a1778 1
	(void) checklimit(&num_freefile, 0);
a2097 1
		ACQUIRE_LOCK(&lk);
d2101 2
d2105 1
d2110 1
a2325 1

a2333 1

a2343 1

d2373 1
a2373 1
	struct diradd *dap;
d2400 2
a2401 2
	 *      0 - non-directory file rename
	 *      1 - directory rename within same directory
d2644 1
d2776 1
a2776 1
			panic("%s: direct pointer #%d mismatch %d != %d",
d2781 1
a2781 1
			panic("%s: indirect pointer #%d mismatch %d != %d",
d2902 2
a2903 1
			while ((adp = LIST_FIRST(&bmsafemap->sm_allocdirecthd))) {
d2909 2
a2910 1
			while ((aip = LIST_FIRST(&bmsafemap->sm_allocindirhd))) {
d2917 1
a2917 1
			       LIST_FIRST(&bmsafemap->sm_inodedephd)) != NULL) {
a2948 1
				LIST_REMOVE(aip, ai_next);
d2950 2
d2954 2
d3111 2
d3127 1
a3127 1
				panic("%s: %s #%d mismatch %d != %d",
d3134 1
a3134 1
				panic("%s: %s #%d allocated as %d",
d3144 2
d3344 3
a3346 1
	if (chgs)
d3348 1
d3422 1
a3422 1
	int waitfor;		/* 1 => update must be allowed */
d3632 46
d3826 3
a3828 2
				if ((error = flush_pagedep_deps(vp,
				   pagedep->pd_mnt, &pagedep->pd_diraddhd[i]))) {
d3836 42
d3899 1
a3899 6
	while (vp->v_numoutput) {
		vp->v_flag |= VBWAIT;
		FREE_LOCK_INTERLOCKED(&lk);
		sleep((caddr_t)&vp->v_numoutput, PRIBIO + 1);
		ACQUIRE_LOCK_INTERLOCKED(&lk);
	}
d4124 2
a4125 1
			if ((error = VOP_FSYNC(vp, p->p_ucred, MNT_NOWAIT, p))) {
d4129 1
a4129 8
			ACQUIRE_LOCK(&lk);
			while (vp->v_numoutput) {
				vp->v_flag |= VBWAIT;
				FREE_LOCK_INTERLOCKED(&lk);
				sleep((caddr_t)&vp->v_numoutput, PRIBIO + 1);
				ACQUIRE_LOCK_INTERLOCKED(&lk);
			}
			FREE_LOCK(&lk);
d4150 203
d4373 1
a4373 1
		sleep((caddr_t)bp, PRIBIO + 1);
d4384 22
a4413 1
	struct worklist *wk;
d4418 1
a4418 32
	ACQUIRE_LOCK(&lk);
	while ((wk = LIST_FIRST(&bp->b_dep)) != NULL) {
		WORKLIST_REMOVE(wk);
		FREE_LOCK(&lk);
		switch (wk->wk_type) {
		/*
		 * XXX - should really clean up, but for now we will
		 * just leak memory and not worry about it. Also should
		 * mark the filesystem permanently dirty so that it will
		 * force fsck to be run (though this would best be done
		 * in the mainline code).
		 */
		case D_PAGEDEP:
		case D_INODEDEP:
		case D_BMSAFEMAP:
		case D_ALLOCDIRECT:
		case D_INDIRDEP:
		case D_ALLOCINDIR:
		case D_MKDIR:
#ifdef DEBUG
			printf("Lost type %s\n", TYPENAME(wk->wk_type));
#endif
			break;
		default:
			panic("%s: Unexpected type %s",
			    "softdep_deallocate_dependencies",
			    TYPENAME(wk->wk_type));
			/* NOTREACHED */
		}
		ACQUIRE_LOCK(&lk);
	}
	FREE_LOCK(&lk);
d4431 1
a4431 2
	log(LOG_ERR, "%s: got error %d while accessing filesystem\n",
	    func, error);
a4432 2

#endif /* FFS_SOFTUPDATES */
@


1.6
log
@

Integrate latest soft updates patches for McKusick.

Integrate cleaner ffs mount code from FreeBSD. Most notably, this mount
code prevents you from mounting an unclean file system read-write.
@
text
@d893 1
a893 1
	pagedep_hashtbl = hashinit(desiredvnodes / 5, M_PAGEDEP,
d896 2
a897 1
	inodedep_hashtbl = hashinit(desiredvnodes, M_INODEDEP, &inodedep_hash);
d899 1
a899 1
	newblk_hashtbl = hashinit(64, M_NEWBLK, &newblk_hash);
@


1.5
log
@

Integrate changes from Kirk McKusick
@
text
@d55 1
a55 1
 *	@@(#)ffs_softdep.c	9.27 (McKusick) 6/12/98
d178 1
d942 1
a942 1
	if (!bcmp(&cstotal, &fs->fs_cstotal, sizeof cstotal))
d944 12
d2195 9
d2233 1
d2399 1
a2399 1
	 * Whiteouts do not need addition dependencies.
d2416 14
d2445 1
a2445 1
 	}
d2473 3
a2475 3
 		add_to_worklist(&dirrem->dm_list);
 	}
 	FREE_LOCK(&lk);
d2539 10
@


1.4
log
@Make machine word size a factor in max_softdeps.  Keeps alpha from
panic'ing.
@
text
@d55 1
a55 1
 *	@@(#)ffs_softdep.c	9.23 (McKusick) 2/20/98
d2308 6
d2315 11
a2325 19
	     dap; dap = LIST_NEXT(dap, da_pdlist)) {
		/*
		 * Check for a diradd dependency for the same directory entry.
		 * If present, then both dependencies become obsolete and can
		 * be de-allocated.
		 */
		if (dap->da_offset != offset)
			continue;
		/*
		 * Must be ATTACHED at this point, so just delete it.
		 */
		if ((dap->da_state & ATTACHED) == 0)
			panic("newdirrem: not ATTACHED");
		if (dap->da_newinum != ip->i_number)
			panic("newdirrem: inum %d should be %d",
			    ip->i_number, dap->da_newinum);
		free_diradd(dap);
		dirrem->dm_state |= COMPLETE;
		break;
d2327 12
d2370 1
d2376 1
a2376 1
	 * Whiteouts have no addition dependencies.
d2378 1
a2378 3
	if (newinum == WINO) {
		dap = NULL;
	} else {
d2389 1
a2389 1
	 * Allocate a new dirrem if appropriate and ACQUIRE_LOCK.
d2392 1
d2395 2
a2396 2
	 * If the inode has already been written, then no addition
	 * dependency needs to be created.
d2398 19
a2416 1
	if (inodedep_lookup(dp->i_fs, newinum, 0, &inodedep) == 0 ||
d2418 5
a2422 8
		WORKITEM_FREE(dap, D_DIRADD);
		dap = NULL;
	}

	if (dap) {
		dap->da_previous = dirrem;
		LIST_INSERT_HEAD(
		    &dirrem->dm_pagedep->pd_diraddhd[DIRADDHASH(offset)],
a2424 6
	} else if ((dirrem->dm_state & COMPLETE) == 0) {
		LIST_INSERT_HEAD(&dirrem->dm_pagedep->pd_dirremhd, dirrem,
		    dm_next);
	} else {
		dirrem->dm_dirinum = dirrem->dm_pagedep->pd_ino;
		add_to_worklist(&dirrem->dm_list);
d2426 13
a2438 1
	FREE_LOCK(&lk);
@


1.3
log
@Add Kirk McKusick's soft update code.  This is an implementation of:
    http://www.ece.cmu.edu/~ganger/papers/CSE-TR-254-95/
This code currently has a restrictive copyright (for the time being)
and so is not enabled by default; "option FFS_SOFTUPDATES" will enable it.
Note that you will need to re-run config(8) as the old softdep stubs
have moved to ffs_softdep_stub.c.
@
text
@d891 1
a891 1
	max_softdeps = desiredvnodes * 8;
@


1.2
log
@$OpenBSD$
@
text
@a0 2
/*	$OpenBSD$	*/

d2 6
a7 1
 * Copyright 1997 Marshall Kirk McKusick. All Rights Reserved.
d9 10
a18 2
 * This code is derived from work done by Greg Ganger and Yale Patt at the
 * University of Michigan.
d23 1
d29 2
a30 2
 * 3. None of the names of McKusick, Ganger, Patt, or the University of 
 *    Michigan may be used to endorse or promote products derived from 
d32 10
d43 5
a47 5
 * THIS SOFTWARE IS PROVIDED BY MARSHALL KIRK MCKUSICK ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL MARSHALL KIRK MCKUSICK BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
d55 7
a61 1
 *	@@(#)ffs_softdep.stub.c	9.1 (McKusick) 7/9/97
d63 6
d71 7
d79 2
a80 1
#include <sys/systm.h>
d83 3
d89 444
d539 114
d654 2
a655 1
	panic("softdep_flushfiles called");
d658 49
a706 3
int
softdep_mount(devvp, mp, fs, cred)
	struct vnode *devvp;
d708 138
d847 3
a849 1
	struct ucred *cred;
d851 2
d854 24
d881 4
d889 57
a945 1
	return;
d948 35
d985 3
a987 3
	struct buf *bp;
	struct inode *ip;
	ino_t newinum;
d989 2
d992 14
a1005 1
	panic("softdep_setup_inomapdep called");
d1008 4
d1014 28
a1042 2
	struct fs *fs;
	ufs_daddr_t newblkno;
d1044 2
d1047 20
a1066 1
	panic("softdep_setup_blkmapdep called");
d1069 29
d1100 155
d1256 76
a1331 6
	ufs_lbn_t lbn;
	ufs_daddr_t newblkno;
	ufs_daddr_t oldblkno;
	long newsize;
	long oldsize;
	struct buf *bp;
d1333 12
a1344 2
	
	panic("softdep_setup_allocdirect called");
d1347 4
d1353 7
a1359 7
	struct inode *ip;
	ufs_lbn_t lbn;
	struct buf *bp;
	int ptrno;
	ufs_daddr_t newblkno;
	ufs_daddr_t oldblkno;
	struct buf *nbp;
d1361 2
d1364 13
a1376 1
	panic("softdep_setup_allocindir_page called");
d1379 4
d1385 5
a1389 5
	struct buf *nbp;
	struct inode *ip;
	struct buf *bp;
	int ptrno;
	ufs_daddr_t newblkno;
d1391 1
d1393 5
a1397 1
	panic("softdep_setup_allocindir_meta called");
d1400 129
d1531 211
a1741 2
	struct inode *ip;
	off_t length;
d1743 18
a1760 2
	
	panic("softdep_setup_freeblocks called");
d1763 5
d1776 55
d1832 21
a1852 1
	panic("softdep_freefile called");
d1855 199
d2056 5
a2060 5
	struct buf *bp;
	struct inode *dp;
	off_t diroffset;
	long newinum;
	struct buf *newdirbp;
d2062 79
a2140 2

	panic("softdep_setup_directory_add called");
d2143 7
d2152 5
a2156 5
	struct inode *dp;
	caddr_t base;
	caddr_t oldloc;
	caddr_t newloc;
	int entrysize;
d2158 28
d2187 43
a2229 1
	panic("softdep_change_directoryentry_offset called");
d2232 17
d2251 31
a2281 4
	struct buf *bp;
	struct inode *dp;
	struct inode *ip;
	int isrmdir;
d2283 47
a2329 2
	
	panic("softdep_setup_remove called");
d2332 17
d2351 5
a2355 5
	struct buf *bp;
	struct inode *dp;
	struct inode *ip;
	long newinum;
	int isrmdir;
d2357 51
a2407 2

	panic("softdep_setup_directory_change called");
d2410 6
d2418 20
d2439 110
d2550 2
d2553 60
a2612 1
	panic("softdep_increase_linkcnt called");
d2615 696
d3313 1
a3313 1
	struct inode *ip;
d3315 1
d3317 16
a3332 1
	panic("softdep_load_inodeblock called");
d3335 10
d3347 79
a3425 3
	struct inode *ip;
	struct buf *bp;
	int waitfor;
d3427 1
d3429 19
a3447 1
	panic("softdep_update_inodeblock called");
d3450 4
d3456 1
a3456 1
	struct vnode *vp;
d3458 14
d3473 83
a3555 2
	panic("softdep_fsync called");
	return (EIO);
d3558 6
d3573 219
d3793 74
d3869 223
@


1.1
log
@FFS softdep stub
@
text
@d1 2
@

