head	1.17;
access;
symbols
	SMP_SYNC_A:1.17
	SMP_SYNC_B:1.17
	UBC_SYNC_A:1.17
	UBC_SYNC_B:1.17
	OPENBSD_2_9_BASE:1.12
	OPENBSD_2_9:1.12.0.2
	OPENBSD_2_8:1.11.0.4
	OPENBSD_2_8_BASE:1.11
	OPENBSD_2_7:1.11.0.2
	OPENBSD_2_7_BASE:1.11
	SMP:1.9.0.6
	SMP_BASE:1.9
	kame_19991208:1.9
	OPENBSD_2_6:1.9.0.4
	OPENBSD_2_6_BASE:1.9
	OPENBSD_2_5:1.9.0.2
	OPENBSD_2_5_BASE:1.9
	OPENBSD_2_4:1.8.0.4
	OPENBSD_2_4_BASE:1.8
	OPENBSD_2_3:1.8.0.2
	OPENBSD_2_3_BASE:1.8
	OPENBSD_2_2:1.5.0.2
	OPENBSD_2_2_BASE:1.5
	OPENBSD_2_1:1.3.0.4
	OPENBSD_2_1_BASE:1.3
	OPENBSD_2_0:1.3.0.2
	OPENBSD_2_0_BASE:1.3
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.17
date	2001.08.12.22.41.15;	author mickey;	state dead;
branches;
next	1.16;

1.16
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.06.27.04.52.39;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.05.10.14.51.21;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.05.10.07.59.06;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.03.09.14.20.52;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2000.03.16.22.11.05;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2000.03.13.14.29.04;	author art;	state Exp;
branches;
next	1.9;

1.9
date	99.02.26.01.48.51;	author art;	state Exp;
branches
	1.9.6.1;
next	1.8;

1.8
date	98.03.01.00.38.12;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	97.11.11.20.16.41;	author millert;	state Exp;
branches;
next	1.6;

1.6
date	97.11.06.05.59.34;	author csapuntz;	state Exp;
branches;
next	1.5;

1.5
date	97.10.06.20.21.21;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	97.10.06.15.28.54;	author csapuntz;	state Exp;
branches;
next	1.3;

1.3
date	96.08.02.00.06.01;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	96.07.23.23.54.24;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.53.37;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.53.37;	author deraadt;	state Exp;
branches;
next	;

1.9.6.1
date	2000.03.24.09.09.53;	author niklas;	state Exp;
branches;
next	1.9.6.2;

1.9.6.2
date	2001.05.14.22.47.50;	author niklas;	state Exp;
branches;
next	1.9.6.3;

1.9.6.3
date	2001.07.04.11.01.21;	author niklas;	state Exp;
branches;
next	1.9.6.4;

1.9.6.4
date	2001.10.31.03.32.14;	author nate;	state dead;
branches;
next	;


desc
@@


1.17
log
@merge vm_map.h into uvm_map.h, kinda matches netbsd's approach
@
text
@/*	$OpenBSD: vm_map.h,v 1.16 2001/08/06 14:03:05 art Exp $	*/
/*	$NetBSD: vm_map.h,v 1.35 2000/03/26 20:54:48 kleink Exp $	*/

/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_map.h	8.9 (Berkeley) 5/17/95
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Authors: Avadis Tevanian, Jr., Michael Wayne Young
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 *	Virtual memory map module definitions.
 */

#ifndef	_VM_MAP_
#define	_VM_MAP_

#include <uvm/uvm_anon.h>

/*
 *	Types defined:
 *
 *	vm_map_t		the high-level address map data structure.
 *	vm_map_entry_t		an entry in an address map.
 *	vm_map_version_t	a timestamp of a map, for use with vm_map_lookup
 */

/*
 *	Objects which live in maps may be either VM objects, or
 *	another map (called a "sharing map") which denotes read-write
 *	sharing with other maps.
 *
 * XXXCDC: private pager data goes here now
 */

union vm_map_object {
	struct uvm_object	*uvm_obj;	/* UVM OBJECT */
	struct vm_map		*sub_map;	/* belongs to another map */
};

/*
 *	Address map entries consist of start and end addresses,
 *	a VM object (or sharing map) and offset into that object,
 *	and user-exported inheritance and protection information.
 *	Also included is control information for virtual copy operations.
 */
struct vm_map_entry {
	struct vm_map_entry	*prev;		/* previous entry */
	struct vm_map_entry	*next;		/* next entry */
	vaddr_t			start;		/* start address */
	vaddr_t			end;		/* end address */
	union vm_map_object	object;		/* object I point to */
	voff_t			offset;		/* offset into object */
	/* etype is a bitmap that replaces the following 4 items */
	int			etype;		/* entry type */
		/* Only in task maps: */
	vm_prot_t		protection;	/* protection code */
	vm_prot_t		max_protection;	/* maximum protection */
	vm_inherit_t		inheritance;	/* inheritance */
	int			wired_count;	/* can be paged if == 0 */
	struct vm_aref		aref;		/* anonymous overlay */
	int			advice;		/* madvise advice */
#define uvm_map_entry_stop_copy flags
	u_int8_t		flags;		/* flags */

#define UVM_MAP_STATIC		0x01		/* static map entry */
};

#define		VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)

/*
 *	Maps are doubly-linked lists of map entries, kept sorted
 *	by address.  A single hint is provided to start
 *	searches again from the last successful search,
 *	insertion, or removal.
 *
 *	LOCKING PROTOCOL NOTES:
 *	-----------------------
 *
 *	VM map locking is a little complicated.  There are both shared
 *	and exclusive locks on maps.  However, it is sometimes required
 *	to downgrade an exclusive lock to a shared lock, and upgrade to
 *	an exclusive lock again (to perform error recovery).  However,
 *	another thread *must not* queue itself to receive an exclusive
 *	lock while before we upgrade back to exclusive, otherwise the
 *	error recovery becomes extremely difficult, if not impossible.
 *
 *	In order to prevent this scenario, we introduce the notion of
 *	a `busy' map.  A `busy' map is read-locked, but other threads
 *	attempting to write-lock wait for this flag to clear before
 *	entering the lock manager.  A map may only be marked busy
 *	when the map is write-locked (and then the map must be downgraded
 *	to read-locked), and may only be marked unbusy by the thread
 *	which marked it busy (holding *either* a read-lock or a
 *	write-lock, the latter being gained by an upgrade).
 *
 *	Access to the map `flags' member is controlled by the `flags_lock'
 *	simple lock.  Note that some flags are static (set once at map
 *	creation time, and never changed), and thus require no locking
 *	to check those flags.  All flags which are r/w must be set or
 *	cleared while the `flags_lock' is asserted.  Additional locking
 *	requirements are:
 *
 *		VM_MAP_PAGEABLE		r/o static flag; no locking required
 *
 *		VM_MAP_INTRSAFE		r/o static flag; no locking required
 *
 *		VM_MAP_WIREFUTURE	r/w; may only be set or cleared when
 *					map is write-locked.  may be tested
 *					without asserting `flags_lock'.
 *
 *		VM_MAP_BUSY		r/w; may only be set when map is
 *					write-locked, may only be cleared by
 *					thread which set it, map read-locked
 *					or write-locked.  must be tested
 *					while `flags_lock' is asserted.
 *
 *		VM_MAP_WANTLOCK		r/w; may only be set when the map
 *					is busy, and thread is attempting
 *					to write-lock.  must be tested
 *					while `flags_lock' is asserted.
 */
struct vm_map {
	struct pmap *		pmap;		/* Physical map */
	lock_data_t		lock;		/* Lock for map data */
	struct vm_map_entry	header;		/* List of entries */
	int			nentries;	/* Number of entries */
	vsize_t			size;		/* virtual size */
	int			ref_count;	/* Reference count */
	simple_lock_data_t	ref_lock;	/* Lock for ref_count field */
	vm_map_entry_t		hint;		/* hint for quick lookups */
	simple_lock_data_t	hint_lock;	/* lock for hint storage */
	vm_map_entry_t		first_free;	/* First free space hint */
	int			flags;		/* flags */
	simple_lock_data_t	flags_lock;	/* Lock for flags field */
	unsigned int		timestamp;	/* Version number */
#define	min_offset		header.start
#define max_offset		header.end
};

/* vm_map flags */
#define VM_MAP_PAGEABLE		0x01		/* ro: entries are pageable*/
#define VM_MAP_INTRSAFE		0x02		/* ro: interrupt safe map */
#define VM_MAP_WIREFUTURE	0x04		/* rw: wire future mappings */
#define	VM_MAP_BUSY		0x08		/* rw: map is busy */
#define	VM_MAP_WANTLOCK		0x10		/* rw: want to write-lock */

#ifdef _KERNEL
#define	vm_map_modflags(map, set, clear)				\
do {									\
	simple_lock(&(map)->flags_lock);				\
	(map)->flags = ((map)->flags | (set)) & ~(clear);		\
	simple_unlock(&(map)->flags_lock);				\
} while (0)
#endif /* _KERNEL */

/*
 *     Interrupt-safe maps must also be kept on a special list,
 *     to assist uvm_fault() in avoiding locking problems.
 */
struct vm_map_intrsafe {
	struct vm_map   vmi_map;
	LIST_ENTRY(vm_map_intrsafe) vmi_list;
};

LIST_HEAD(vmi_list, vm_map_intrsafe);
#ifdef _KERNEL
extern simple_lock_data_t vmi_list_slock;
extern struct vmi_list vmi_list;

static __inline int vmi_list_lock __P((void));
static __inline void vmi_list_unlock __P((int));

static __inline int
vmi_list_lock()
{
	int s;

	s = splhigh();
	simple_lock(&vmi_list_slock);
	return (s);
}

static __inline void
vmi_list_unlock(s)
	int s;
{

	simple_unlock(&vmi_list_slock);
	splx(s);
}
#endif /* _KERNEL */

/*
 * VM map locking operations:
 *
 *	These operations perform locking on the data portion of the
 *	map.
 *
 *	vm_map_lock_try: try to lock a map, failing if it is already locked.
 *
 *	vm_map_lock: acquire an exclusive (write) lock on a map.
 *
 *	vm_map_lock_read: acquire a shared (read) lock on a map.
 *
 *	vm_map_unlock: release an exclusive lock on a map.
 *
 *	vm_map_unlock_read: release a shared lock on a map.
 *
 *	vm_map_downgrade: downgrade an exclusive lock to a shared lock.
 *
 *	vm_map_upgrade: upgrade a shared lock to an exclusive lock.
 *
 *	vm_map_busy: mark a map as busy.
 *
 *	vm_map_unbusy: clear busy status on a map.
 *
 * Note that "intrsafe" maps use only exclusive, spin locks.  We simply
 * use the sleep lock's interlock for this.
 */

#ifdef _KERNEL
/* XXX: clean up later */
#include <sys/time.h>
#include <sys/proc.h>	/* for tsleep(), wakeup() */
#include <sys/systm.h>	/* for panic() */

static __inline boolean_t vm_map_lock_try __P((vm_map_t));
static __inline void vm_map_lock __P((vm_map_t));

static __inline boolean_t
vm_map_lock_try(map)
	vm_map_t map;
{
	boolean_t rv;

	if (map->flags & VM_MAP_INTRSAFE)
		rv = simple_lock_try(&map->lock.lk_interlock);
	else {
		simple_lock(&map->flags_lock);
		if (map->flags & VM_MAP_BUSY) {
			simple_unlock(&map->flags_lock);
			return (FALSE);
		}
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT|LK_INTERLOCK,
		    &map->flags_lock, curproc) == 0);
	}

	if (rv)
		map->timestamp++;

	return (rv);
}

static __inline void
vm_map_lock(map)
	vm_map_t map;
{
	int error;

	if (map->flags & VM_MAP_INTRSAFE) {
		simple_lock(&map->lock.lk_interlock);
		return;
	}

 try_again:
	simple_lock(&map->flags_lock);
	if (map->flags & VM_MAP_BUSY) {
		map->flags |= VM_MAP_WANTLOCK;
		simple_unlock(&map->flags_lock);
		(void) tsleep(&map->flags, PVM, "vmmapbsy", 0);
		goto try_again;
	}

	error = lockmgr(&map->lock, LK_EXCLUSIVE|LK_SLEEPFAIL|LK_INTERLOCK,
	    &map->flags_lock, curproc);

	if (error) {
#ifdef DIAGNOSTIC
		if (error != ENOLCK)
			panic("vm_map_lock: failed to get lock");
#endif
		goto try_again;
	}
 
	(map)->timestamp++;
}

#ifdef DIAGNOSTIC
#define	vm_map_lock_read(map)						\
do {									\
	if (map->flags & VM_MAP_INTRSAFE)				\
		panic("vm_map_lock_read: intrsafe map");		\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc);		\
} while (0)
#else
#define	vm_map_lock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc)
#endif

#define	vm_map_unlock(map)						\
do {									\
	if ((map)->flags & VM_MAP_INTRSAFE)				\
		simple_unlock(&(map)->lock.lk_interlock);		\
	else								\
		(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc);\
} while (0)

#define	vm_map_unlock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)

#define	vm_map_downgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL, curproc)

#ifdef DIAGNOSTIC
#define	vm_map_upgrade(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc) != 0)	\
		panic("vm_map_upgrade: failed to upgrade lock");	\
} while (0)
#else
#define	vm_map_upgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc)
#endif

#define	vm_map_busy(map)						\
do {									\
	simple_lock(&(map)->flags_lock);				\
	(map)->flags |= VM_MAP_BUSY;					\
	simple_unlock(&(map)->flags_lock);				\
} while (0)

#define	vm_map_unbusy(map)						\
do {									\
	int oflags;							\
									\
	simple_lock(&(map)->flags_lock);				\
	oflags = (map)->flags;						\
	(map)->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);			\
	simple_unlock(&(map)->flags_lock);				\
	if (oflags & VM_MAP_WANTLOCK)					\
		wakeup(&(map)->flags);					\
} while (0)
#endif /* _KERNEL */

/*
 *	Functions implemented as macros
 */
#define		vm_map_min(map)		((map)->min_offset)
#define		vm_map_max(map)		((map)->max_offset)
#define		vm_map_pmap(map)	((map)->pmap)

/* XXX: number of kernel maps and entries to statically allocate */
#ifndef	MAX_KMAP
#define	MAX_KMAP	20
#endif
#ifndef	MAX_KMAPENT
#if (50 + (2 * NPROC) > 1000)
#define MAX_KMAPENT (50 + (2 * NPROC))
#else
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
#endif
#endif

#endif /* _VM_MAP_ */
@


1.16
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.15 2001/06/27 04:52:39 art Exp $	*/
@


1.15
log
@Remove junk from headers, just leave enough for UVM.
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_map.h,v 1.14 2001/05/10 14:51:21 art Exp $	*/
/*	$NetBSD: vm_map.h,v 1.11 1995/03/26 20:39:10 jtc Exp $	*/
d110 1
a110 1
	vsize_t			offset;		/* offset into object */
@


1.14
log
@More sync to NetBSD.
The highlight is some more advices to madvise(2).
 o MADV_DONTNEED will deactive the pages in the given range giving a quicker
   reuse.
 o MADV_FREE will garbage-collect the pages and swap resources causing the
   next fault to either page in new pages from backing store (mapped vnode)
   or allocate new zero-fill pages (anonymous mapping).
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.13 2001/05/10 07:59:06 art Exp $	*/
a74 1
#ifdef UVM
a75 1
#endif
a93 1
#ifdef UVM
a95 5
#else
	struct vm_object	*vm_object;	/* object object */
	struct vm_map		*sub_map;	/* belongs to another map */
	struct vm_map		*share_map;	/* share map */
#endif /* UVM */
a110 1
#if defined(UVM)
a112 7
#else
	boolean_t		is_a_map;	/* Is "object" a map? */
	boolean_t		is_sub_map;	/* Is "object" a submap? */
		/* Only in sharing maps: */
	boolean_t		copy_on_write;	/* is data copy-on-write */
	boolean_t		needs_copy;	/* does object need to be copied */
#endif
a117 1
#ifdef UVM
a123 2

#endif /* UVM */
a132 1
#if defined(UVM)
a178 1
#endif
a185 3
#ifndef UVM
	boolean_t		is_main_map;	/* Am I a main map? */
#endif
a190 1
#ifdef UVM
a192 3
#else
	boolean_t		entries_pageable; /* map entries pageable?? */
#endif
a197 1
#ifdef UVM
a249 20
#endif /* UVM */

#ifndef UVM	/* version handled elsewhere in uvm */
/*
 *	Map versions are used to validate a previous lookup attempt.
 *
 *	Since lookup operations may involve both a main map and
 *	a sharing map, it is necessary to have a timestamp from each.
 *	[If the main map timestamp has changed, the share_map and
 *	associated timestamp are no longer valid; the map version
 *	does not include a reference for the imbedded share_map.]
 */
typedef struct {
	int		main_timestamp;
	vm_map_t	share_map;
	int		share_timestamp;
} vm_map_version_t;
#endif /* UVM */

#ifdef UVM
a401 44
#else /* UVM */
/*
 *	Macros:		vm_map_lock, etc.
 *	Function:
 *		Perform locking on the data portion of a map.
 */

#include <sys/proc.h>	/* XXX for curproc and p_pid */

#define	vm_map_lock_drain_interlock(map) { \
	lockmgr(&(map)->lock, LK_DRAIN|LK_INTERLOCK, \
		&(map)->ref_lock, curproc); \
	(map)->timestamp++; \
}
#ifdef DIAGNOSTIC
#define	vm_map_lock(map) { \
	if (lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc) != 0) { \
		panic("vm_map_lock: failed to get lock"); \
	} \
	(map)->timestamp++; \
}
#else
#define	vm_map_lock(map) { \
	lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc); \
	(map)->timestamp++; \
}
#endif /* DIAGNOSTIC */
#define	vm_map_unlock(map) \
		lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)
#define	vm_map_lock_read(map) \
		lockmgr(&(map)->lock, LK_SHARED, NULL, curproc)
#define	vm_map_unlock_read(map) \
		lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)
#define vm_map_set_recursive(map) { \
	simple_lock(&(map)->lk_interlock); \
	(map)->lk_flags |= LK_CANRECURSE; \
	simple_unlock(&(map)->lk_interlock); \
}
#define vm_map_clear_recursive(map) { \
	simple_lock(&(map)->lk_interlock); \
	(map)->lk_flags &= ~LK_CANRECURSE; \
	simple_unlock(&(map)->lk_interlock); \
}
#endif /* UVM */
a421 49
#if defined(_KERNEL) && !defined(UVM)
boolean_t	 vm_map_check_protection __P((vm_map_t,
		    vm_offset_t, vm_offset_t, vm_prot_t));
int		 vm_map_copy __P((vm_map_t, vm_map_t, vm_offset_t,
		    vm_size_t, vm_offset_t, boolean_t, boolean_t));
void		 vm_map_copy_entry __P((vm_map_t,
		    vm_map_t, vm_map_entry_t, vm_map_entry_t));
struct pmap;
vm_map_t	 vm_map_create __P((struct pmap *,
		    vm_offset_t, vm_offset_t, boolean_t));
void		 vm_map_deallocate __P((vm_map_t));
int		 vm_map_delete __P((vm_map_t, vm_offset_t, vm_offset_t));
vm_map_entry_t	 vm_map_entry_create __P((vm_map_t));
void		 vm_map_entry_delete __P((vm_map_t, vm_map_entry_t));
void		 vm_map_entry_dispose __P((vm_map_t, vm_map_entry_t));
void		 vm_map_entry_unwire __P((vm_map_t, vm_map_entry_t));
int		 vm_map_find __P((vm_map_t, vm_object_t,
		    vm_offset_t, vm_offset_t *, vm_size_t, boolean_t));
int		 vm_map_findspace __P((vm_map_t,
		    vm_offset_t, vm_size_t, vm_offset_t *));
int		 vm_map_inherit __P((vm_map_t,
		    vm_offset_t, vm_offset_t, vm_inherit_t));
void		 vm_map_init __P((struct vm_map *,
		    vm_offset_t, vm_offset_t, boolean_t));
int		 vm_map_insert __P((vm_map_t,
		    vm_object_t, vm_offset_t, vm_offset_t, vm_offset_t));
int		 vm_map_lookup __P((vm_map_t *, vm_offset_t, vm_prot_t,
		    vm_map_entry_t *, vm_object_t *, vm_offset_t *, vm_prot_t *,
		    boolean_t *, boolean_t *));
void		 vm_map_lookup_done __P((vm_map_t, vm_map_entry_t));
boolean_t	 vm_map_lookup_entry __P((vm_map_t,
		    vm_offset_t, vm_map_entry_t *));
int		 vm_map_pageable __P((vm_map_t,
		    vm_offset_t, vm_offset_t, boolean_t));
int		 vm_map_clean __P((vm_map_t,
		    vm_offset_t, vm_offset_t, boolean_t, boolean_t));
void		 vm_map_print __P((vm_map_t, boolean_t));
void		 _vm_map_print __P((vm_map_t, boolean_t,
		    int (*)(const char *, ...)));
int		 vm_map_protect __P((vm_map_t,
		    vm_offset_t, vm_offset_t, vm_prot_t, boolean_t));
void		 vm_map_reference __P((vm_map_t));
int		 vm_map_remove __P((vm_map_t, vm_offset_t, vm_offset_t));
void		 vm_map_simplify __P((vm_map_t, vm_offset_t));
void		 vm_map_simplify_entry __P((vm_map_t, vm_map_entry_t));
void		 vm_map_startup __P((void));
int		 vm_map_submap __P((vm_map_t,
		    vm_offset_t, vm_offset_t, vm_map_t));
#endif /* _KERNEL & !UVM */
@


1.13
log
@Some locking protocol fixes and better enforcement of wiring limits.

From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.12 2001/03/09 14:20:52 art Exp $	*/
d233 9
@


1.12
log
@More syncing to NetBSD.

Implements mincore(2), mlockall(2) and munlockall(2). mlockall and munlockall
are disabled for the moment.

The rest is mostly cosmetic.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.11 2000/03/16 22:11:05 art Exp $	*/
d152 48
a215 7
	/*
	 * Locking note: read-only flags need not be locked to read
	 * them; they are set once at map creation time, and never
	 * changed again.  Only read-write flags require that the
	 * appropriate map lock be acquired before reading or writing
	 * the flag.
	 */
d217 1
d231 3
d307 8
d322 2
a323 1
#include <sys/proc.h>	/* XXX for curproc and p_pid */
d326 1
d336 9
a344 2
	else
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT, NULL, curproc) == 0);
d352 24
d377 2
a378 8
#define	_vm_map_lock(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc) != 0)	\
		panic("vm_map_lock: failed to get lock");		\
} while (0)
#else
#define	_vm_map_lock(map)						\
	(void) lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc)
d380 5
a384 9

#define	vm_map_lock(map)						\
do {									\
	if ((map)->flags & VM_MAP_INTRSAFE)				\
		simple_lock(&(map)->lock.lk_interlock);			\
	else								\
		_vm_map_lock((map));					\
	(map)->timestamp++;						\
} while (0)
d408 33
@


1.11
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.10 2000/03/13 14:29:04 art Exp $	*/
d133 1
a133 1
	int			wired_count;	/* can be paged if = 0 */
d145 2
d168 8
a175 1
	int			flags;		/* flags (read-only) */
d186 3
a188 2
#define VM_MAP_PAGEABLE		0x01		/* entries are pageable*/
#define VM_MAP_INTRSAFE		0x02		/* interrupt safe map */
d349 1
a349 1
	if (lockmgr(&(map)->lock, LK_EXCLUSIVE, (void *)0, curproc) != 0) { \
d356 1
a356 1
	lockmgr(&(map)->lock, LK_EXCLUSIVE, (void *)0, curproc); \
d361 1
a361 1
		lockmgr(&(map)->lock, LK_RELEASE, (void *)0, curproc)
d363 1
a363 1
		lockmgr(&(map)->lock, LK_SHARED, (void *)0, curproc)
d365 1
a365 1
		lockmgr(&(map)->lock, LK_RELEASE, (void *)0, curproc)
@


1.10
log
@Some vm_offset_t -> vaddr_t (&co) and some ifndef uvm.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.9 1999/02/26 01:48:51 art Exp $	*/
d165 3
d169 1
d175 42
d234 90
d366 1
a366 15
#if defined(UVM) && defined(_KERNEL)
/* XXX: clean up later */
static boolean_t vm_map_lock_try __P((vm_map_t));

static __inline boolean_t vm_map_lock_try(map)

vm_map_t map;

{
  if (lockmgr(&(map)->lock, LK_EXCLUSIVE|LK_NOWAIT, (void *)0, curproc) != 0)
    return(FALSE);
  map->timestamp++;
  return(TRUE);
}
#endif
d380 5
a384 1
#define	MAX_KMAPENT	1000
@


1.9
log
@add some struct members that uvm uses
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.8 1998/03/01 00:38:12 niklas Exp $	*/
d115 2
a116 2
	vm_offset_t		start;		/* start address */
	vm_offset_t		end;		/* end address */
d118 1
a118 1
	vm_offset_t		offset;		/* offset into object */
d156 1
a156 1
	vm_size_t		size;		/* virtual size */
d261 1
a261 1
#ifdef _KERNEL
d309 1
a309 1
#endif
@


1.9.6.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d115 2
a116 2
	vaddr_t			start;		/* start address */
	vaddr_t			end;		/* end address */
d118 1
a118 1
	vsize_t			offset;		/* offset into object */
d156 1
a156 1
	vsize_t			size;		/* virtual size */
a164 3
#ifdef UVM
	int			flags;		/* flags (read-only) */
#else
a165 1
#endif
a170 42
#ifdef UVM
/* vm_map flags */
#define VM_MAP_PAGEABLE		0x01		/* entries are pageable*/
#define VM_MAP_INTRSAFE		0x02		/* interrupt safe map */
/*
 *     Interrupt-safe maps must also be kept on a special list,
 *     to assist uvm_fault() in avoiding locking problems.
 */
struct vm_map_intrsafe {
	struct vm_map   vmi_map;
	LIST_ENTRY(vm_map_intrsafe) vmi_list;
};

LIST_HEAD(vmi_list, vm_map_intrsafe);
#ifdef _KERNEL
extern simple_lock_data_t vmi_list_slock;
extern struct vmi_list vmi_list;

static __inline int vmi_list_lock __P((void));
static __inline void vmi_list_unlock __P((int));

static __inline int
vmi_list_lock()
{
	int s;

	s = splhigh();
	simple_lock(&vmi_list_slock);
	return (s);
}

static __inline void
vmi_list_unlock(s)
	int s;
{

	simple_unlock(&vmi_list_slock);
	splx(s);
}
#endif /* _KERNEL */
#endif /* UVM */

a187 90
#ifdef UVM

/*
 * VM map locking operations:
 *
 *	These operations perform locking on the data portion of the
 *	map.
 *
 *	vm_map_lock_try: try to lock a map, failing if it is already locked.
 *
 *	vm_map_lock: acquire an exclusive (write) lock on a map.
 *
 *	vm_map_lock_read: acquire a shared (read) lock on a map.
 *
 *	vm_map_unlock: release an exclusive lock on a map.
 *
 *	vm_map_unlock_read: release a shared lock on a map.
 *
 * Note that "intrsafe" maps use only exclusive, spin locks.  We simply
 * use the sleep lock's interlock for this.
 */

#ifdef _KERNEL
/* XXX: clean up later */
#include <sys/time.h>
#include <sys/proc.h>	/* XXX for curproc and p_pid */

static __inline boolean_t vm_map_lock_try __P((vm_map_t));

static __inline boolean_t
vm_map_lock_try(map)
	vm_map_t map;
{
	boolean_t rv;

	if (map->flags & VM_MAP_INTRSAFE)
		rv = simple_lock_try(&map->lock.lk_interlock);
	else
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT, NULL, curproc) == 0);

	if (rv)
		map->timestamp++;

	return (rv);
}

#ifdef DIAGNOSTIC
#define	_vm_map_lock(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc) != 0)	\
		panic("vm_map_lock: failed to get lock");		\
} while (0)
#else
#define	_vm_map_lock(map)						\
	(void) lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc)
#endif

#define	vm_map_lock(map)						\
do {									\
	if ((map)->flags & VM_MAP_INTRSAFE)				\
		simple_lock(&(map)->lock.lk_interlock);			\
	else								\
		_vm_map_lock((map));					\
	(map)->timestamp++;						\
} while (0)

#ifdef DIAGNOSTIC
#define	vm_map_lock_read(map)						\
do {									\
	if (map->flags & VM_MAP_INTRSAFE)				\
		panic("vm_map_lock_read: intrsafe map");		\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc);		\
} while (0)
#else
#define	vm_map_lock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc)
#endif

#define	vm_map_unlock(map)						\
do {									\
	if ((map)->flags & VM_MAP_INTRSAFE)				\
		simple_unlock(&(map)->lock.lk_interlock);		\
	else								\
		(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc);\
} while (0)

#define	vm_map_unlock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)
#endif /* _KERNEL */
#else /* UVM */
d230 15
a244 1
#endif /* UVM */
d258 1
a258 5
#if (50 + (2 * NPROC) > 1000)
#define MAX_KMAPENT (50 + (2 * NPROC))
#else
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
#endif
d261 1
a261 1
#if defined(_KERNEL) && !defined(UVM)
d309 1
a309 1
#endif /* _KERNEL & !UVM */
@


1.9.6.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.12 2001/03/09 14:20:52 art Exp $	*/
d133 1
a133 1
	int			wired_count;	/* can be paged if == 0 */
a144 2
#define		VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)

d166 1
a166 8
	/*
	 * Locking note: read-only flags need not be locked to read
	 * them; they are set once at map creation time, and never
	 * changed again.  Only read-write flags require that the
	 * appropriate map lock be acquired before reading or writing
	 * the flag.
	 */
	int			flags;		/* flags */
d177 2
a178 3
#define VM_MAP_PAGEABLE		0x01		/* ro: entries are pageable*/
#define VM_MAP_INTRSAFE		0x02		/* ro: interrupt safe map */
#define VM_MAP_WIREFUTURE	0x04		/* rw: wire future mappings */
d339 1
a339 1
	if (lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc) != 0) { \
d346 1
a346 1
	lockmgr(&(map)->lock, LK_EXCLUSIVE, NULL, curproc); \
d351 1
a351 1
		lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)
d353 1
a353 1
		lockmgr(&(map)->lock, LK_SHARED, NULL, curproc)
d355 1
a355 1
		lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)
@


1.9.6.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d75 1
d77 1
d96 1
d99 5
d119 1
d122 7
d134 1
d141 2
a151 46
 *
 *	LOCKING PROTOCOL NOTES:
 *	-----------------------
 *
 *	VM map locking is a little complicated.  There are both shared
 *	and exclusive locks on maps.  However, it is sometimes required
 *	to downgrade an exclusive lock to a shared lock, and upgrade to
 *	an exclusive lock again (to perform error recovery).  However,
 *	another thread *must not* queue itself to receive an exclusive
 *	lock while before we upgrade back to exclusive, otherwise the
 *	error recovery becomes extremely difficult, if not impossible.
 *
 *	In order to prevent this scenario, we introduce the notion of
 *	a `busy' map.  A `busy' map is read-locked, but other threads
 *	attempting to write-lock wait for this flag to clear before
 *	entering the lock manager.  A map may only be marked busy
 *	when the map is write-locked (and then the map must be downgraded
 *	to read-locked), and may only be marked unbusy by the thread
 *	which marked it busy (holding *either* a read-lock or a
 *	write-lock, the latter being gained by an upgrade).
 *
 *	Access to the map `flags' member is controlled by the `flags_lock'
 *	simple lock.  Note that some flags are static (set once at map
 *	creation time, and never changed), and thus require no locking
 *	to check those flags.  All flags which are r/w must be set or
 *	cleared while the `flags_lock' is asserted.  Additional locking
 *	requirements are:
 *
 *		VM_MAP_PAGEABLE		r/o static flag; no locking required
 *
 *		VM_MAP_INTRSAFE		r/o static flag; no locking required
 *
 *		VM_MAP_WIREFUTURE	r/w; may only be set or cleared when
 *					map is write-locked.  may be tested
 *					without asserting `flags_lock'.
 *
 *		VM_MAP_BUSY		r/w; may only be set when map is
 *					write-locked, may only be cleared by
 *					thread which set it, map read-locked
 *					or write-locked.  must be tested
 *					while `flags_lock' is asserted.
 *
 *		VM_MAP_WANTLOCK		r/w; may only be set when the map
 *					is busy, and thread is attempting
 *					to write-lock.  must be tested
 *					while `flags_lock' is asserted.
d159 3
d167 8
d176 3
a178 1
	simple_lock_data_t	flags_lock;	/* Lock for flags field */
d184 1
a188 12
#define	VM_MAP_BUSY		0x08		/* rw: map is busy */
#define	VM_MAP_WANTLOCK		0x10		/* rw: want to write-lock */

#ifdef _KERNEL
#define	vm_map_modflags(map, set, clear)				\
do {									\
	simple_lock(&(map)->flags_lock);				\
	(map)->flags = ((map)->flags | (set)) & ~(clear);		\
	simple_unlock(&(map)->flags_lock);				\
} while (0)
#endif /* _KERNEL */

d225 20
a261 8
 *	vm_map_downgrade: downgrade an exclusive lock to a shared lock.
 *
 *	vm_map_upgrade: upgrade a shared lock to an exclusive lock.
 *
 *	vm_map_busy: mark a map as busy.
 *
 *	vm_map_unbusy: clear busy status on a map.
 *
d269 1
a269 2
#include <sys/proc.h>	/* for tsleep(), wakeup() */
#include <sys/systm.h>	/* for panic() */
a271 1
static __inline void vm_map_lock __P((vm_map_t));
d281 2
a282 9
	else {
		simple_lock(&map->flags_lock);
		if (map->flags & VM_MAP_BUSY) {
			simple_unlock(&map->flags_lock);
			return (FALSE);
		}
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT|LK_INTERLOCK,
		    &map->flags_lock, curproc) == 0);
	}
a289 24
static __inline void
vm_map_lock(map)
	vm_map_t map;
{
	int error;

	if (map->flags & VM_MAP_INTRSAFE) {
		simple_lock(&map->lock.lk_interlock);
		return;
	}

 try_again:
	simple_lock(&map->flags_lock);
	if (map->flags & VM_MAP_BUSY) {
		map->flags |= VM_MAP_WANTLOCK;
		simple_unlock(&map->flags_lock);
		(void) tsleep(&map->flags, PVM, "vmmapbsy", 0);
		goto try_again;
	}

	error = lockmgr(&map->lock, LK_EXCLUSIVE|LK_SLEEPFAIL|LK_INTERLOCK,
	    &map->flags_lock, curproc);

	if (error) {
d291 8
a298 2
		if (error != ENOLCK)
			panic("vm_map_lock: failed to get lock");
d300 9
a308 5
		goto try_again;
	}
 
	(map)->timestamp++;
}
d332 7
d340 1
a340 2
#define	vm_map_downgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL, curproc)
d342 5
d348 6
a353 5
#define	vm_map_upgrade(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc) != 0)	\
		panic("vm_map_upgrade: failed to upgrade lock");	\
} while (0)
d355 22
a376 23
#define	vm_map_upgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc)
#endif

#define	vm_map_busy(map)						\
do {									\
	simple_lock(&(map)->flags_lock);				\
	(map)->flags |= VM_MAP_BUSY;					\
	simple_unlock(&(map)->flags_lock);				\
} while (0)

#define	vm_map_unbusy(map)						\
do {									\
	int oflags;							\
									\
	simple_lock(&(map)->flags_lock);				\
	oflags = (map)->flags;						\
	(map)->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);			\
	simple_unlock(&(map)->flags_lock);				\
	if (oflags & VM_MAP_WANTLOCK)					\
		wakeup(&(map)->flags);					\
} while (0)
#endif /* _KERNEL */
d397 49
@


1.9.6.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.9.6.3 2001/07/04 11:01:21 niklas Exp $	*/
@


1.8
log
@Merge of MACHINE_NEW_CONTIG (aka MNN) code from Chuck Cranor,
<chuck@@openbsd.org>. This code is as of yet disabled on all platforms,
actually not yet supported on more than mvme68k, although other
platforms are expected soon, as code is already available.
This code makes handling of multiple physical memory regions
consistent over all platforms, as well as keeping the performance of
maintaining a single continuous memory chunk.  It is also a
requirement for the upcoming UVM replacement VM system.

What I did in this merge: just declared the pmap_map function in a
MD include file per port that needs it.  It's not an exported pmap
interface, says Chuck.  It ended up in differnt include files on
differnet ports, as I tried to follow the current policy on a per-arch
basis.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.7 1997/11/11 20:16:41 millert Exp $	*/
d96 4
d101 1
a102 3
	struct vm_map		*sub_map;	/* belongs to another map */
#ifdef UVM
	struct uvm_object	*uvm_obj;	/* UVM OBJECT */
d157 1
d159 1
d171 1
d186 1
@


1.7
log
@double MAX_KMAPENT and MAX_KMAP
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.6 1997/11/06 05:59:34 csapuntz Exp $	*/
d75 4
d91 2
d99 3
d117 4
d126 1
d132 9
d224 16
@


1.6
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.5 1997/10/06 20:21:21 deraadt Exp $	*/
d209 6
a214 2
#define MAX_KMAP	10
#define	MAX_KMAPENT	500
@


1.5
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.3 1996/08/02 00:06:01 niklas Exp $	*/
d39 1
a39 1
 *	@@(#)vm_map.h	8.3 (Berkeley) 3/15/94
d165 8
d174 3
a176 1
	lock_write(&(map)->lock); \
d179 22
a200 4
#define	vm_map_unlock(map)	lock_write_done(&(map)->lock)
#define	vm_map_lock_read(map)	lock_read(&(map)->lock)
#define	vm_map_unlock_read(map)	lock_read_done(&(map)->lock)

@


1.4
log
@VFS Lite2 Changes
@
text
@d39 1
a39 1
 *	@@(#)vm_map.h	8.9 (Berkeley) 5/17/95
a164 8
#include <sys/proc.h>	/* XXX for curproc and p_pid */

#define	vm_map_lock_drain_interlock(map) { \
	lockmgr(&(map)->lock, LK_DRAIN|LK_INTERLOCK, \
		&(map)->ref_lock, curproc); \
	(map)->timestamp++; \
}
#ifdef DIAGNOSTIC
d166 1
a166 3
	if (lockmgr(&(map)->lock, LK_EXCLUSIVE, (void *)0, curproc) != 0) { \
		panic("vm_map_lock: failed to get lock"); \
	} \
d169 4
a172 22
#else
#define	vm_map_lock(map) { \
	lockmgr(&(map)->lock, LK_EXCLUSIVE, (void *)0, curproc); \
	(map)->timestamp++; \
}
#endif /* DIAGNOSTIC */
#define	vm_map_unlock(map) \
		lockmgr(&(map)->lock, LK_RELEASE, (void *)0, curproc)
#define	vm_map_lock_read(map) \
		lockmgr(&(map)->lock, LK_SHARED, (void *)0, curproc)
#define	vm_map_unlock_read(map) \
		lockmgr(&(map)->lock, LK_RELEASE, (void *)0, curproc)
#define vm_map_set_recursive(map) { \
	simple_lock(&(map)->lk_interlock); \
	(map)->lk_flags |= LK_CANRECURSE; \
	simple_unlock(&(map)->lk_interlock); \
}
#define vm_map_clear_recursive(map) { \
	simple_lock(&(map)->lk_interlock); \
	(map)->lk_flags &= ~LK_CANRECURSE; \
	simple_unlock(&(map)->lk_interlock); \
}
@


1.3
log
@Fix long-standing swap-leak. Add OpenBSD tags. Optimize thread_wakeup.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_map.h,v 1.11 1995/03/26 20:39:10 jtc Exp $	*/
d39 1
a39 1
 *	@@(#)vm_map.h	8.3 (Berkeley) 3/15/94
d165 8
d174 3
a176 1
	lock_write(&(map)->lock); \
d179 22
a200 4
#define	vm_map_unlock(map)	lock_write_done(&(map)->lock)
#define	vm_map_lock_read(map)	lock_read(&(map)->lock)
#define	vm_map_unlock_read(map)	lock_read_done(&(map)->lock)

@


1.2
log
@make printf/addlog return 0, for compat to userland
@
text
@d1 1
@


1.1
log
@Initial revision
@
text
@d221 1
a221 1
		    void (*)(const char *, ...)));
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

