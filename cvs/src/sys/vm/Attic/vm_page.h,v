head	1.20;
access;
symbols
	SMP_SYNC_A:1.20
	SMP_SYNC_B:1.20
	UBC_SYNC_A:1.20
	UBC_SYNC_B:1.20
	OPENBSD_3_0:1.18.0.2
	OPENBSD_3_0_BASE:1.18
	OPENBSD_2_9_BASE:1.12
	OPENBSD_2_9:1.12.0.2
	OPENBSD_2_8:1.10.0.6
	OPENBSD_2_8_BASE:1.10
	OPENBSD_2_7:1.10.0.4
	OPENBSD_2_7_BASE:1.10
	SMP:1.10.0.2
	SMP_BASE:1.10
	kame_19991208:1.9
	OPENBSD_2_6:1.9.0.2
	OPENBSD_2_6_BASE:1.9
	OPENBSD_2_5:1.8.0.2
	OPENBSD_2_5_BASE:1.8
	OPENBSD_2_4:1.6.0.4
	OPENBSD_2_4_BASE:1.6
	OPENBSD_2_3:1.6.0.2
	OPENBSD_2_3_BASE:1.6
	OPENBSD_2_2:1.4.0.2
	OPENBSD_2_2_BASE:1.4
	OPENBSD_2_1:1.3.0.4
	OPENBSD_2_1_BASE:1.3
	OPENBSD_2_0:1.3.0.2
	OPENBSD_2_0_BASE:1.3
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.20
date	2001.11.06.03.11.39;	author todd;	state dead;
branches;
next	1.19;

1.19
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.08.12.17.55.56;	author mickey;	state Exp;
branches;
next	1.17;

1.17
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.07.25.14.47.58;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.07.05.07.32.54;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.06.27.04.52.40;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.04.07.17.13.44;	author niklas;	state Exp;
branches;
next	1.11;

1.11
date	2000.11.10.15.33.11;	author provos;	state Exp;
branches;
next	1.10;

1.10
date	99.12.30.18.21.56;	author provos;	state Exp;
branches
	1.10.2.1;
next	1.9;

1.9
date	99.08.23.07.56.03;	author art;	state Exp;
branches;
next	1.8;

1.8
date	99.02.26.01.48.51;	author art;	state Exp;
branches;
next	1.7;

1.7
date	99.02.19.02.54.38;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	98.03.20.15.40.38;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	98.03.01.00.38.20;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	97.09.22.20.44.53;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	96.08.13.22.22.17;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	96.08.02.00.06.03;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.53.38;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.53.38;	author deraadt;	state Exp;
branches;
next	;

1.10.2.1
date	2001.05.14.22.47.50;	author niklas;	state Exp;
branches;
next	1.10.2.2;

1.10.2.2
date	2001.07.04.11.01.24;	author niklas;	state Exp;
branches;
next	1.10.2.3;

1.10.2.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.10.2.4;

1.10.2.4
date	2001.11.13.23.02.31;	author niklas;	state dead;
branches;
next	;


desc
@@


1.20
log
@remove per art@@
@
text
@#include <uvm/uvm_page.h>
@


1.19
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@@


1.18
log
@moce pglisth into uvm_pglist.h
@
text
@d1 1
a1 334
/*	$OpenBSD: vm_page.h,v 1.17 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: vm_page.h,v 1.35 2000/03/26 20:54:48 kleink Exp $	*/

/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_page.h	7.3 (Berkeley) 4/21/91
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Authors: Avadis Tevanian, Jr., Michael Wayne Young
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 *	Resident memory system definitions.
 */
#ifndef	_VM_PAGE_
#define	_VM_PAGE_

/*
 *	Management of resident (logical) pages.
 *
 *	A small structure is kept for each resident
 *	page, indexed by page number.  Each structure
 *	is an element of several lists:
 *
 *		A hash table bucket used to quickly
 *		perform object/offset lookups
 *
 *		A list of all pages for a given object,
 *		so they can be quickly deactivated at
 *		time of deallocation.
 *
 *		An ordered list of pages due for pageout.
 *
 *	In addition, the structure contains the object
 *	and offset to which this page belongs (for pageout),
 *	and sundry status bits.
 *
 *	Fields in this structure are locked either by the lock on the
 *	object that the page belongs to (O) or by the lock on the page
 *	queues (P) [or both].
 */

/*
 * locking note: the mach version of this data structure had bit
 * fields for the flags, and the bit fields were divided into two
 * items (depending on who locked what).  some time, in BSD, the bit
 * fields were dumped and all the flags were lumped into one short.
 * that is fine for a single threaded uniprocessor OS, but bad if you
 * want to actual make use of locking (simple_lock's).  so, we've
 * separated things back out again.
 *
 * note the page structure has no lock of its own.
 */

#include <uvm/uvm_extern.h>
#include <uvm/uvm_pglist.h>

struct vm_page {
  TAILQ_ENTRY(vm_page)	pageq;		/* queue info for FIFO
					 * queue or free list (P) */
  TAILQ_ENTRY(vm_page)	hashq;		/* hash table links (O)*/
  TAILQ_ENTRY(vm_page)	listq;		/* pages in same object (O)*/

  struct vm_anon	*uanon;		/* anon (O,P) */
  struct uvm_object	*uobject;	/* object (O,P) */
  voff_t		offset;		/* offset into object (O,P) */

  u_short		flags;		/* object flags [O] */
  u_short		version;	/* version count [O] */
  u_short		wire_count;	/* wired down map refs [P] */
  u_short 		pqflags;	/* page queue flags [P] */
  u_int			loan_count;	/* number of active loans
					 * to read: [O or P]
					 * to modify: [O _and_ P] */

  paddr_t		phys_addr;	/* physical address of page */
#if defined(UVM_PAGE_TRKOWN)
  /* debugging fields to track page ownership */
  pid_t			owner;		/* proc that set PG_BUSY */
  char			*owner_tag;	/* why it was set busy */
#endif
};

/*
 * These are the flags defined for vm_page.
 *
 * Note: PG_FILLED and PG_DIRTY are added for the filesystems.
 */

/*
 * locking rules:
 *   PG_ ==> locked by object lock
 *   PQ_ ==> lock by page queue lock 
 *   PQ_FREE is locked by free queue lock and is mutex with all other PQs
 *
 * PG_ZERO is used to indicate that a page has been pre-zero'd.  This flag
 * is only set when the page is on no queues, and is cleared when the page
 * is placed on the free list.
 *
 * possible deadwood: PG_FAULTING, PQ_LAUNDRY
 */
#define	PG_CLEAN	0x0008		/* page has not been modified */
#define	PG_BUSY		0x0010		/* page is in transit  */
#define	PG_WANTED	0x0020		/* someone is waiting for page */
#define	PG_TABLED	0x0040		/* page is in VP table  */
#define	PG_ZERO		0x0100		/* page is pre-zero'd */
#define	PG_FAKE		0x0200		/* page is placeholder for pagein */
#define	PG_FILLED	0x0400		/* client flag to set when filled */
#define	PG_DIRTY	0x0800		/* client flag to set when dirty */
#define PG_RELEASED	0x1000		/* page released while paging */
#define	PG_FAULTING	0x2000		/* page is being faulted in */
#define PG_CLEANCHK	0x4000		/* clean bit has been checked */

#define PQ_FREE		0x0001		/* page is on free list */
#define PQ_INACTIVE	0x0002		/* page is in inactive list */
#define PQ_ACTIVE	0x0004		/* page is in active list */
#define PQ_LAUNDRY	0x0008		/* page is being cleaned now */
#define PQ_ANON		0x0010		/* page is part of an anon, rather
					   than an uvm_object */
#define PQ_AOBJ		0x0020		/* page is part of an anonymous
					   uvm_object */
#define PQ_SWAPBACKED	(PQ_ANON|PQ_AOBJ)
#define PQ_ENCRYPT	0x0040		/* page needs {en,de}cryption */

/*
 * physical memory layout structure
 *
 * MD vmparam.h must #define:
 *   VM_PHYSEG_MAX = max number of physical memory segments we support
 *		   (if this is "1" then we revert to a "contig" case)
 *   VM_PHYSSEG_STRAT: memory sort/search options (for VM_PHYSEG_MAX > 1)
 * 	- VM_PSTRAT_RANDOM:   linear search (random order)
 *	- VM_PSTRAT_BSEARCH:  binary search (sorted by address)
 *	- VM_PSTRAT_BIGFIRST: linear search (sorted by largest segment first)
 *      - others?
 *   XXXCDC: eventually we should remove contig and old non-contig cases
 *   and purge all left-over global variables...
 */
#define VM_PSTRAT_RANDOM	1
#define VM_PSTRAT_BSEARCH	2
#define VM_PSTRAT_BIGFIRST	3

/*
 * vm_physmemseg: describes one segment of physical memory
 */
struct vm_physseg {
	paddr_t start;			/* PF# of first page in segment */
	paddr_t end;			/* (PF# of last page in segment) + 1 */
	paddr_t avail_start;		/* PF# of first free page in segment */
	paddr_t avail_end;		/* (PF# of last free page in segment) +1  */
	int	free_list;		/* which free list they belong on */
	struct	vm_page *pgs;		/* vm_page structures (from start) */
	struct	vm_page *lastpg;	/* vm_page structure for end */
	struct	pmap_physseg pmseg;	/* pmap specific (MD) data */
};

#if defined(_KERNEL)

/*
 * physical memory config is stored in vm_physmem.
 */

extern struct vm_physseg vm_physmem[VM_PHYSSEG_MAX];
extern int vm_nphysseg;

/*
 * prototypes
 */

static struct vm_page *PHYS_TO_VM_PAGE __P((paddr_t));
static int vm_physseg_find __P((paddr_t, int *));

/*
 * macros and inlines
 */
#define VM_PAGE_TO_PHYS(entry)	((entry)->phys_addr)

/*
 * when VM_PHYSSEG_MAX is 1, we can simplify these functions
 */

/*
 * vm_physseg_find: find vm_physseg structure that belongs to a PA
 */
static __inline int
vm_physseg_find(pframe, offp)
	paddr_t pframe;
	int	*offp;
{
#if VM_PHYSSEG_MAX == 1

	/* 'contig' case */
	if (pframe >= vm_physmem[0].start && pframe < vm_physmem[0].end) {
		if (offp)
			*offp = pframe - vm_physmem[0].start;
		return(0);
	}
	return(-1);

#elif (VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	/* binary search for it */
	int	start, len, try;

	/*
	 * if try is too large (thus target is less than than try) we reduce
	 * the length to trunc(len/2) [i.e. everything smaller than "try"]
	 *
	 * if the try is too small (thus target is greater than try) then
	 * we set the new start to be (try + 1).   this means we need to
	 * reduce the length to (round(len/2) - 1).
	 *
	 * note "adjust" below which takes advantage of the fact that
	 *  (round(len/2) - 1) == trunc((len - 1) / 2)
	 * for any value of len we may have
	 */

	for (start = 0, len = vm_nphysseg ; len != 0 ; len = len / 2) {
		try = start + (len / 2);	/* try in the middle */

		/* start past our try? */
		if (pframe >= vm_physmem[try].start) {
			/* was try correct? */
			if (pframe < vm_physmem[try].end) {
				if (offp)
					*offp = pframe - vm_physmem[try].start;
				return(try);            /* got it */
			}
			start = try + 1;	/* next time, start here */
			len--;			/* "adjust" */
		} else {
			/*
			 * pframe before try, just reduce length of
			 * region, done in "for" loop
			 */
		}
	}
	return(-1);

#else
	/* linear search for it */
	int	lcv;

	for (lcv = 0; lcv < vm_nphysseg; lcv++) {
		if (pframe >= vm_physmem[lcv].start &&
		    pframe < vm_physmem[lcv].end) {
			if (offp)
				*offp = pframe - vm_physmem[lcv].start;
			return(lcv);		   /* got it */
		}
	}
	return(-1);

#endif
}


/*
 * PHYS_TO_VM_PAGE: find vm_page for a PA.   used by MI code to get vm_pages
 * back from an I/O mapping (ugh!).   used in some MD code as well.
 */
static __inline struct vm_page *
PHYS_TO_VM_PAGE(pa)
	paddr_t pa;
{
	paddr_t pf = atop(pa);
	int	off;
	int	psi;

	psi = vm_physseg_find(pf, &off);
	if (psi != -1)
		return(&vm_physmem[psi].pgs[off]);
	return(NULL);
}

#define VM_PAGE_IS_FREE(entry)  ((entry)->pqflags & PQ_FREE)

#endif /* _KERNEL */
#endif /* !_VM_PAGE_ */
@


1.17
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.16 2001/08/06 14:03:05 art Exp $	*/
d112 1
a112 1
#include <vm/pglist.h>
@


1.16
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.15 2001/07/25 14:47:58 art Exp $	*/
d152 4
d162 1
@


1.15
log
@Some updates to UVM from NetBSD. Nothing really critical, just a sync.
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_page.h,v 1.14 2001/07/05 07:32:54 art Exp $	*/
/*	$NetBSD: vm_page.h,v 1.24 1998/02/10 14:09:03 mrg Exp $	*/
d120 3
a122 1
  vaddr_t		offset;		/* offset into object (O,P) */
a123 2
  struct uvm_object	*uobject;	/* object (O,P) */
  struct vm_anon	*uanon;		/* anon (O,P) */
@


1.14
log
@Remove some left-overs from the old vm system.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.13 2001/06/27 04:52:40 art Exp $	*/
d198 4
a201 4
	vaddr_t start;			/* PF# of first page in segment */
	vaddr_t end;			/* (PF# of last page in segment) + 1 */
	vaddr_t avail_start;		/* PF# of first free page in segment */
	vaddr_t avail_end;		/* (PF# of last free page in segment) +1  */
@


1.13
log
@Remove junk from headers, just leave enough for UVM.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.12 2001/04/07 17:13:44 niklas Exp $	*/
a210 23
 *	Each pageable resident page falls into one of three lists:
 *
 *	free	
 *		Available for allocation now.
 *	inactive
 *		Not referenced in any map, but still has an
 *		object/offset-page mapping, and may be dirty.
 *		This is the list of pages that should be
 *		paged out next.
 *	active
 *		A list of pages which have been placed in
 *		at least one physical map.  This list is
 *		ordered, in LRU-like fashion.
 */

extern
struct pglist	vm_page_queue_free;	/* memory free queue */
extern
struct pglist	vm_page_queue_active;	/* active memory queue */
extern
struct pglist	vm_page_queue_inactive;	/* inactive memory queue */

/*
a308 6
 * IS_VM_PHYSADDR: only used my mips/pmax/pica trap/pmap.
 */

#define IS_VM_PHYSADDR(PA) (vm_physseg_find(atop(PA), NULL) != -1)

/*
a326 62

extern
simple_lock_data_t	vm_page_queue_lock;	/* lock on active and inactive
						   page queues */
extern						/* lock on free page queue */
simple_lock_data_t	vm_page_queue_free_lock;

#define PAGE_ASSERT_WAIT(m, interruptible)	{ \
				(m)->flags |= PG_WANTED; \
				assert_wait((m), (interruptible)); \
			}

#define PAGE_WAKEUP(m)	{ \
				(m)->flags &= ~PG_BUSY; \
				if ((m)->flags & PG_WANTED) { \
					(m)->flags &= ~PG_WANTED; \
					thread_wakeup((m)); \
				} \
			}

#define	vm_page_lock_queues()	simple_lock(&vm_page_queue_lock)
#define	vm_page_unlock_queues()	simple_unlock(&vm_page_queue_lock)

#define vm_page_set_modified(m)	{ (m)->flags &= ~PG_CLEAN; }

/*
 * XXXCDC: different versions of this should die
 */
#define	VM_PAGE_INIT(mem, obj, offset) { \
	(mem)->flags = PG_BUSY | PG_CLEAN | PG_FAKE; \
	if (obj) \
		vm_page_insert((mem), (obj), (offset)); \
	else \
		(mem)->object = NULL; \
	(mem)->wire_count = 0; \
}

#if VM_PAGE_DEBUG

/*
 * VM_PAGE_CHECK: debugging check of a vm_page structure
 */
static __inline void
VM_PAGE_CHECK(mem)
	struct vm_page *mem;
{
	int lcv;

	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		if ((unsigned int) mem >= (unsigned int) vm_physmem[lcv].pgs &&
		    (unsigned int) mem <= (unsigned int) vm_physmem[lcv].lastpg)
			break;
	}
	if (lcv == vm_nphysseg ||
	    (mem->flags & (PG_ACTIVE|PG_INACTIVE)) == (PG_ACTIVE|PG_INACTIVE))
		panic("vm_page_check: not valid!"); 
	return;
}

#else /* VM_PAGE_DEBUG */
#define	VM_PAGE_CHECK(mem)
#endif /* VM_PAGE_DEBUG */
@


1.12
log
@Always get struct pglist from vm/pglist.h, otherwise old VM compilations
generate redefinition errors.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.11 2000/11/10 15:33:11 provos Exp $	*/
a98 1
#if defined(UVM)
a111 1
#endif /* UVM */
a119 3
#if !defined(UVM) /* uvm uses obju */
  vm_object_t		object;		/* which object am I in (O,P)*/
#endif
a121 1
#if defined(UVM)
a130 4
#else
  u_short		wire_count;	/* wired down maps refs (P) */
  u_short		flags;		/* see below */
#endif
d133 1
a133 1
#if defined(UVM) && defined(UVM_PAGE_TRKOWN)
a144 1
#if defined(UVM)
a175 29
#else
#define	PG_INACTIVE	0x0001		/* page is in inactive list (P) */
#define	PG_ACTIVE	0x0002		/* page is in active list (P) */
#define	PG_LAUNDRY	0x0004		/* page is being cleaned now (P) */
#define	PG_CLEAN	0x0008		/* page has not been modified
					   There exists a case where this bit
					   will be cleared, although the page
					   is not physically dirty, which is
					   when a collapse operation moves
					   pages between two different pagers.
					   The bit is then used as a marker
					   for the pageout daemon to know it
					   should be paged out into the target
					   pager. */
#define	PG_BUSY		0x0010		/* page is in transit (O) */
#define	PG_WANTED	0x0020		/* someone is waiting for page (O) */
#define	PG_TABLED	0x0040		/* page is in VP table (O) */
#define	PG_COPYONWRITE	0x0080		/* must copy page before changing (O) */
#define	PG_FICTITIOUS	0x0100		/* physical page doesn't exist (O) */
#define	PG_FAKE		0x0200		/* page is placeholder for pagein (O) */
#define	PG_FILLED	0x0400		/* client flag to set when filled */
#define	PG_DIRTY	0x0800		/* client flag to set when dirty */
#define	PG_FREE		0x1000		/* XXX page is on free list */
#define	PG_FAULTING	0x2000		/* page is being faulted in */
#define	PG_PAGEROWNED	0x4000		/* DEBUG: async paging op in progress */
#define	PG_PTPAGE	0x8000		/* DEBUG: is a user page table page */
#endif

#if defined(MACHINE_NEW_NONCONTIG)
a201 1
#if defined(UVM)
a202 1
#endif
a207 2
#endif /* MACHINE_NEW_NONCONTIG */

a232 3

#if defined(MACHINE_NEW_NONCONTIG)

a239 34
#else
#if defined(MACHINE_NONCONTIG)
/* OLD NONCONTIG CODE: NUKE NUKE NUKE ONCE CONVERTED */
extern
u_long	first_page;			/* first physical page number */
extern
int	vm_page_count;			/* How many pages do we manage? */
extern
vm_page_t	vm_page_array;		/* First resident page in table */

#define	VM_PAGE_INDEX(pa) \
		(pmap_page_index((pa)) - first_page)
#else 
/* OLD CONTIG CODE: NUKE NUKE NUKE ONCE CONVERTED */
extern
long	first_page;			/* first physical page number */
					/* ... represented in vm_page_array */
extern
long	last_page;			/* last physical page number */
					/* ... represented in vm_page_array */
					/* [INCLUSIVE] */
extern
vm_offset_t first_phys_addr;		/* physical address for first_page */
extern
vm_offset_t last_phys_addr;		/* physical address for last_page */
extern
vm_page_t	vm_page_array;		/* First resident page in table */

#define	VM_PAGE_INDEX(pa) \
	(atop((pa)) - first_page)

#endif	/* MACHINE_NONCONTIG */
#endif /* MACHINE_NEW_NONCONTIG */

a243 1
#if defined(MACHINE_NEW_NONCONTIG)
a245 34
#endif

#if !defined(UVM)
void		 vm_page_activate __P((vm_page_t));
vm_page_t	 vm_page_alloc __P((vm_object_t, vm_offset_t));
vm_offset_t	 vm_page_alloc_contig(vm_offset_t, vm_offset_t,
			vm_offset_t, vm_offset_t);
int		 vm_page_alloc_memory __P((vm_size_t size, vm_offset_t low,
			vm_offset_t high, vm_offset_t alignment, vm_offset_t boundary,
			struct pglist *rlist, int nsegs, int waitok));
void		 vm_page_free_memory __P((struct pglist *list));
#if defined(MACHINE_NONCONTIG) || defined(MACHINE_NEW_NONCONTIG)
void		 vm_page_bootstrap __P((vm_offset_t *, vm_offset_t *));
vm_offset_t	 vm_bootstrap_steal_memory __P((vm_size_t));
#endif
void		 vm_page_copy __P((vm_page_t, vm_page_t));
void		 vm_page_deactivate __P((vm_page_t));
void		 vm_page_free __P((vm_page_t));
void		 vm_page_insert __P((vm_page_t, vm_object_t, vm_offset_t));
vm_page_t	 vm_page_lookup __P((vm_object_t, vm_offset_t));
#if defined(MACHINE_NEW_NONCONTIG)
void		 vm_page_physload __P((vm_offset_t, vm_offset_t,
					vm_offset_t, vm_offset_t));
void		 vm_page_physrehash __P((void));
#endif
void		 vm_page_remove __P((vm_page_t));
void		 vm_page_rename __P((vm_page_t, vm_object_t, vm_offset_t));
#if !defined(MACHINE_NONCONTIG) && !defined(MACHINE_NEW_NONCONTIG)
void		 vm_page_startup __P((vm_offset_t *, vm_offset_t *));
#endif
void		 vm_page_unwire __P((vm_page_t));
void		 vm_page_wire __P((vm_page_t));
boolean_t	 vm_page_zero_fill __P((vm_page_t));
#endif
a251 2
#if defined(MACHINE_NEW_NONCONTIG)

a354 22
#elif defined(MACHINE_NONCONTIG)

/* OLD NONCONTIG CODE: NUKE NUKE NUKE ONCE CONVERTED */
#define IS_VM_PHYSADDR(pa) \
		(pmap_page_index(pa) >= 0)

#define PHYS_TO_VM_PAGE(pa) \
		(&vm_page_array[pmap_page_index(pa) - first_page])

#else

/* OLD CONTIG CODE: NUKE NUKE NUKE ONCE CONVERTED */
#define IS_VM_PHYSADDR(pa) \
		((pa) >= first_phys_addr && (pa) <= last_phys_addr)

#define PHYS_TO_VM_PAGE(pa) \
		(&vm_page_array[atop(pa) - first_page ])

#endif /* (OLD) MACHINE_NONCONTIG */

#if defined(UVM)

a356 6
#else /* UVM */

#define VM_PAGE_IS_FREE(entry)  ((entry)->flags & PG_FREE)

#endif /* UVM */

a383 7
#if !defined(MACHINE_NONCONTIG) && !defined(MACHINE_NEW_NONCONTIG)
#define	VM_PAGE_INIT(mem, obj, offset) { \
	(mem)->flags = PG_BUSY | PG_CLEAN | PG_FAKE; \
	vm_page_insert((mem), (obj), (offset)); \
	(mem)->wire_count = 0; \
}
#else	/* MACHINE_NONCONTIG */
a391 1
#endif	/* MACHINE_NONCONTIG */
a393 1
#if defined(MACHINE_NEW_NONCONTIG) 
a413 26

#elif defined(MACHINE_NONCONTIG)

/* OLD NONCONTIG CODE: NUKE NUKE NUKE ONCE CONVERTED */
#define	VM_PAGE_CHECK(mem) { \
	if ((((unsigned int) mem) < ((unsigned int) &vm_page_array[0])) || \
	    (((unsigned int) mem) > \
		((unsigned int) &vm_page_array[vm_page_count])) || \
	    ((mem->flags & (PG_ACTIVE | PG_INACTIVE)) == \
		(PG_ACTIVE | PG_INACTIVE))) \
		panic("vm_page_check: not valid!"); \
}

#else

/* OLD CONTIG CODE: NUKE NUKE NUKE ONCE CONVERTED */
#define	VM_PAGE_CHECK(mem) { \
	if ((((unsigned int) mem) < ((unsigned int) &vm_page_array[0])) || \
	    (((unsigned int) mem) > \
		((unsigned int) &vm_page_array[last_page-first_page])) || \
	    ((mem->flags & (PG_ACTIVE | PG_INACTIVE)) == \
		(PG_ACTIVE | PG_INACTIVE))) \
		panic("vm_page_check: not valid!"); \
}

#endif
@


1.11
log
@seperate -> separate, okay aaron@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.10 1999/12/30 18:21:56 provos Exp $	*/
d113 1
a114 3
#else
TAILQ_HEAD(pglist, vm_page);
#endif /* UVM */
@


1.10
log
@swap encryption for UVM, option UVM_SWAP_ENCRYPT.  needs to be enabled
via sysctl.
Pages are encrypted with the Blowfish encryption algorithm, the key
is initialized randomly on first swap out, ensuring that entropy has
accumulated in the kernel randomness pool.  Eventually, swap encryption
will be decided on a process by process basis, e.g. a process that reads from
a cryptographic filesystem will enable swap encrypt for its pages. okay
art@@ and deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.9 1999/08/23 07:56:03 art Exp $	*/
d107 1
a107 1
 * seperated things back out again.
@


1.10.2.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.12 2001/04/07 17:13:44 niklas Exp $	*/
d107 1
a107 1
 * separated things back out again.
d113 3
a116 1
#include <vm/pglist.h>
@


1.10.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.10.2.1 2001/05/14 22:47:50 niklas Exp $	*/
d99 1
d113 1
d122 3
d127 1
d137 4
d143 1
a143 1
#if defined(UVM_PAGE_TRKOWN)
d155 1
d187 29
d242 1
d244 1
d250 2
d277 3
d287 34
d325 1
d328 34
d368 2
d473 22
d497 6
d530 7
d545 1
d548 1
d569 26
@


1.10.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: vm_page.h,v 1.35 2000/03/26 20:54:48 kleink Exp $	*/
d112 1
a112 1
#include <uvm/uvm_pglist.h>
d120 3
a123 3
  struct uvm_object	*uobject;	/* object (O,P) */
  voff_t		offset;		/* offset into object (O,P) */

a151 4
 * PG_ZERO is used to indicate that a page has been pre-zero'd.  This flag
 * is only set when the page is on no queues, and is cleared when the page
 * is placed on the free list.
 *
a157 1
#define	PG_ZERO		0x0100		/* page is pre-zero'd */
d198 4
a201 4
	paddr_t start;			/* PF# of first page in segment */
	paddr_t end;			/* (PF# of last page in segment) + 1 */
	paddr_t avail_start;		/* PF# of first free page in segment */
	paddr_t avail_end;		/* (PF# of last free page in segment) +1  */
d211 23
d332 6
d356 62
@


1.10.2.4
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.10.2.3 2001/10/31 03:32:14 nate Exp $	*/
@


1.9
log
@vm_offset_t -> {v,p}addr_t + some other cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.8 1999/02/26 01:48:51 art Exp $	*/
d187 1
@


1.8
log
@add some struct members that uvm uses
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.7 1999/02/19 02:54:38 deraadt Exp $	*/
d127 1
a127 1
  vm_offset_t		offset;		/* offset into object (O,P) */
d144 1
a144 1
  vm_offset_t		phys_addr;	/* physical address of page */
d239 4
a242 4
	vm_offset_t start;		/* PF# of first page in segment */
	vm_offset_t end;		/* (PF# of last page in segment) + 1 */
	vm_offset_t avail_start;	/* PF# of first free page in segment */
	vm_offset_t avail_end;		/* (PF# of last free page in segment) +1  */
d292 1
a292 1
u_long	first_page;		/* first physical page number */
d294 1
a294 1
int	vm_page_count;		/* How many pages do we manage? */
d303 1
a303 1
long	first_page;		/* first physical page number */
d306 1
a306 1
long	last_page;		/* last physical page number */
d310 1
a310 1
vm_offset_t first_phys_addr;	/* physical address for first_page */
d327 2
a328 2
static struct vm_page *PHYS_TO_VM_PAGE __P((vm_offset_t));
static int vm_physseg_find __P((vm_offset_t, int *));
d331 1
d362 1
d380 1
a380 1
	vm_offset_t pframe;
d462 1
a462 1
	vm_offset_t pa;
d464 1
a464 1
	vm_offset_t pf = atop(pa);
@


1.7
log
@make vm_page_alloc_contig() a std function
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.6 1998/03/20 15:40:38 niklas Exp $	*/
d243 3
@


1.6
log
@Some cleanup of page steals
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.5 1998/03/01 00:38:20 niklas Exp $	*/
d330 2
@


1.5
log
@Merge of MACHINE_NEW_CONTIG (aka MNN) code from Chuck Cranor,
<chuck@@openbsd.org>. This code is as of yet disabled on all platforms,
actually not yet supported on more than mvme68k, although other
platforms are expected soon, as code is already available.
This code makes handling of multiple physical memory regions
consistent over all platforms, as well as keeping the performance of
maintaining a single continuous memory chunk.  It is also a
requirement for the upcoming UVM replacement VM system.

What I did in this merge: just declared the pmap_map function in a
MD include file per port that needs it.  It's not an exported pmap
interface, says Chuck.  It ended up in differnt include files on
differnet ports, as I tried to follow the current policy on a per-arch
basis.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.4 1997/09/22 20:44:53 niklas Exp $	*/
d336 1
@


1.4
log
@From NetBSD: vm_page_alloc_memory giving continuous memory
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_page.h,v 1.3 1996/08/13 22:22:17 niklas Exp $	*/
/*	$NetBSD: vm_page.h,v 1.20 1997/06/06 23:10:25 thorpej Exp $	*/
a70 1

d96 14
a109 1
 *	queues (P).
d112 3
d116 1
d119 4
a122 4
	TAILQ_ENTRY(vm_page)	pageq;		/* queue info for FIFO
						 * queue or free list (P) */
	TAILQ_ENTRY(vm_page)	hashq;		/* hash table links (O)*/
	TAILQ_ENTRY(vm_page)	listq;		/* pages in same object (O)*/
d124 4
a127 2
	vm_object_t		object;		/* which object am I in (O,P)*/
	vm_offset_t		offset;		/* offset into object (O,P) */
d129 14
a142 2
	u_short			wire_count;	/* wired down maps refs (P) */
	u_short			flags;		/* see below */
d144 6
a149 1
	vm_offset_t		phys_addr;	/* physical address of page */
d157 32
d214 1
d216 35
a250 23
#if	VM_PAGE_DEBUG
#ifndef	MACHINE_NONCONTIG
#define	VM_PAGE_CHECK(mem) { \
	if ((((unsigned int) mem) < ((unsigned int) &vm_page_array[0])) || \
	    (((unsigned int) mem) > \
		((unsigned int) &vm_page_array[last_page-first_page])) || \
	    ((mem->flags & (PG_ACTIVE | PG_INACTIVE)) == \
		(PG_ACTIVE | PG_INACTIVE))) \
		panic("vm_page_check: not valid!"); \
}
#else	/* MACHINE_NONCONTIG */
#define	VM_PAGE_CHECK(mem) { \
	if ((((unsigned int) mem) < ((unsigned int) &vm_page_array[0])) || \
	    (((unsigned int) mem) > \
		((unsigned int) &vm_page_array[vm_page_count])) || \
	    ((mem->flags & (PG_ACTIVE | PG_INACTIVE)) == \
		(PG_ACTIVE | PG_INACTIVE))) \
		panic("vm_page_check: not valid!"); \
}
#endif	/* MACHINE_NONCONTIG */
#else /* VM_PAGE_DEBUG */
#define	VM_PAGE_CHECK(mem)
#endif /* VM_PAGE_DEBUG */
a251 1
#ifdef _KERNEL
d275 17
d295 4
a298 1
#ifndef MACHINE_NONCONTIG
d300 1
a300 1
long		first_page;		/* first physical page number */
d303 1
a303 1
long		last_page;		/* last physical page number */
d307 1
a307 1
vm_offset_t	first_phys_addr;	/* physical address for first_page */
d309 1
a309 2
vm_offset_t	last_phys_addr;		/* physical address for last_page */
#else	/* MACHINE_NONCONTIG */
d311 5
a315 3
u_long		first_page;		/* first physical page number */
extern
int		vm_page_count;		/* How many pages do we manage? */
d317 38
d356 3
d361 117
a477 1
#ifndef MACHINE_NONCONTIG
d481 8
a488 8
#define	VM_PAGE_INDEX(pa) \
		(atop((pa)) - first_page)
#else
#define	IS_VM_PHYSADDR(pa) \
({ \
	int __pmapidx = pmap_page_index(pa); \
	(__pmapidx >= 0 && __pmapidx >= first_page); \
})
d490 1
a490 3
#define	VM_PAGE_INDEX(pa) \
		(pmap_page_index((pa)) - first_page)
#endif /* MACHINE_NONCONTIG */
d492 1
a492 2
#define	PHYS_TO_VM_PAGE(pa) \
		(&vm_page_array[VM_PAGE_INDEX((pa))])
d494 1
a494 1
#define	VM_PAGE_IS_FREE(entry)	((entry)->flags & PG_FREE)
a501 4
/*
 *	Functions implemented as macros
 */

d520 4
a523 1
#ifndef MACHINE_NONCONTIG
d540 46
a585 2
/* XXX what is this here for? */
void		 vm_set_page_size __P((void));
a586 4
/* XXX probably should be elsewhere. */
#ifdef MACHINE_NONCONTIG
vm_offset_t	 pmap_steal_memory __P((vm_size_t));
void		 pmap_startup __P((vm_offset_t *, vm_offset_t *));
d589 3
a591 22
void		 vm_page_activate __P((vm_page_t));
vm_page_t	 vm_page_alloc __P((vm_object_t, vm_offset_t));
int		 vm_page_alloc_memory __P((vm_size_t, vm_offset_t,
			vm_offset_t, vm_offset_t, vm_offset_t,
			struct pglist *, int, int));
void		 vm_page_free_memory __P((struct pglist *));
#ifdef MACHINE_NONCONTIG
void		 vm_page_bootstrap __P((vm_offset_t *, vm_offset_t *));
#endif
void		 vm_page_copy __P((vm_page_t, vm_page_t));
void		 vm_page_deactivate __P((vm_page_t));
void		 vm_page_free __P((vm_page_t));
void		 vm_page_insert __P((vm_page_t, vm_object_t, vm_offset_t));
vm_page_t	 vm_page_lookup __P((vm_object_t, vm_offset_t));
void		 vm_page_remove __P((vm_page_t));
void		 vm_page_rename __P((vm_page_t, vm_object_t, vm_offset_t));
#ifndef MACHINE_NONCONTIG
void		 vm_page_startup __P((vm_offset_t *, vm_offset_t *));
#endif
void		 vm_page_unwire __P((vm_page_t));
void		 vm_page_wire __P((vm_page_t));
boolean_t	 vm_page_zero_fill __P((vm_page_t));
@


1.3
log
@Document PG_CLEANs role in collapse
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_page.h,v 1.2 1996/08/02 00:06:03 niklas Exp $	*/
/*	$NetBSD: vm_page.h,v 1.18 1995/03/26 20:39:13 jtc Exp $	*/
d143 1
d224 2
a225 2
#define PHYS_TO_VM_PAGE(pa) \
		(&vm_page_array[atop(pa) - first_page ])
d227 5
a231 2
#define IS_VM_PHYSADDR(pa) \
		(pmap_page_index(pa) >= 0)
d233 2
a234 2
#define PHYS_TO_VM_PAGE(pa) \
		(&vm_page_array[pmap_page_index(pa) - first_page])
d237 5
d298 4
@


1.2
log
@Fix long-standing swap-leak. Add OpenBSD tags. Optimize thread_wakeup.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_page.h,v 1.18 1995/03/26 20:39:13 jtc Exp $	*/
d124 11
a134 2
#define	PG_LAUNDRY	0x0004		/* page is being cleaned now (P)*/
#define	PG_CLEAN	0x0008		/* page has not been modified */
@


1.1
log
@Initial revision
@
text
@d1 1
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

