head	1.18;
access;
symbols
	SMP_SYNC_A:1.18
	SMP_SYNC_B:1.18
	UBC_SYNC_A:1.18
	UBC_SYNC_B:1.18
	OPENBSD_2_9_BASE:1.16
	OPENBSD_2_9:1.16.0.2
	OPENBSD_2_8:1.15.0.12
	OPENBSD_2_8_BASE:1.15
	OPENBSD_2_7:1.15.0.10
	OPENBSD_2_7_BASE:1.15
	SMP:1.15.0.8
	SMP_BASE:1.15
	kame_19991208:1.15
	OPENBSD_2_6:1.15.0.6
	OPENBSD_2_6_BASE:1.15
	OPENBSD_2_5:1.15.0.4
	OPENBSD_2_5_BASE:1.15
	OPENBSD_2_4:1.15.0.2
	OPENBSD_2_4_BASE:1.15
	OPENBSD_2_3:1.13.0.2
	OPENBSD_2_3_BASE:1.13
	OPENBSD_2_2:1.8.0.2
	OPENBSD_2_2_BASE:1.8
	OPENBSD_2_1:1.7.0.2
	OPENBSD_2_1_BASE:1.7
	OPENBSD_2_0:1.6.0.2
	OPENBSD_2_0_BASE:1.6
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.18
date	2001.06.27.04.53.31;	author art;	state dead;
branches;
next	1.17;

1.17
date	2001.05.05.21.26.47;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.03.09.15.11.47;	author art;	state Exp;
branches;
next	1.15;

1.15
date	98.06.02.05.22.27;	author deraadt;	state Exp;
branches
	1.15.8.1;
next	1.14;

1.14
date	98.05.11.19.43.11;	author niklas;	state Exp;
branches;
next	1.13;

1.13
date	98.02.25.22.13.46;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	98.02.19.22.00.27;	author niklas;	state Exp;
branches;
next	1.11;

1.11
date	98.02.18.23.36.12;	author deraadt;	state Exp;
branches;
next	1.10;

1.10
date	97.11.14.20.56.08;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	97.11.13.18.35.38;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	97.07.25.06.03.08;	author mickey;	state Exp;
branches;
next	1.7;

1.7
date	97.01.07.05.37.37;	author tholo;	state Exp;
branches;
next	1.6;

1.6
date	96.04.21.22.33.13;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	96.04.19.16.10.48;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	96.03.03.17.45.32;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	96.01.07.17.20.43;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.12.14.05.16.08;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.53.37;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.53.37;	author deraadt;	state Exp;
branches;
next	;

1.15.8.1
date	2001.05.14.22.47.50;	author niklas;	state Exp;
branches;
next	1.15.8.2;

1.15.8.2
date	2001.07.04.11.01.22;	author niklas;	state dead;
branches;
next	;


desc
@@


1.18
log
@Die!
@
text
@/*	$OpenBSD: vm_mmap.c,v 1.17 2001/05/05 21:26:47 art Exp $	*/
/*	$NetBSD: vm_mmap.c,v 1.47 1996/03/16 23:15:23 christos Exp $	*/

/*
 * Copyright (c) 1988 University of Utah.
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * from: Utah $Hdr: vm_mmap.c 1.6 91/10/21$
 *
 *	@@(#)vm_mmap.c	8.5 (Berkeley) 5/19/94
 */

/*
 * Mapped file (mmap) interface to VM
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/filedesc.h>
#include <sys/resourcevar.h>
#include <sys/proc.h>
#include <sys/vnode.h>
#include <sys/file.h>
#include <sys/mman.h>
#include <sys/conf.h>
#include <sys/stat.h>

#include <sys/mount.h>
#include <sys/syscallargs.h>

#include <miscfs/specfs/specdev.h>

#include <vm/vm.h>
#include <vm/vm_pager.h>
#include <vm/vm_prot.h>

#ifdef DEBUG
int mmapdebug = 0;
#define MDB_FOLLOW	0x01
#define MDB_SYNC	0x02
#define MDB_MAPIT	0x04
#endif

/* ARGSUSED */
int
sys_sbrk(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_sbrk_args /* {
		syscallarg(int) incr;
	} */ *uap = v;
#endif

	/* Not yet implemented */
	return (EOPNOTSUPP);
}

/* ARGSUSED */
int
sys_sstk(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_sstk_args /* {
		syscallarg(int) incr;
	} */ *uap = v;
#endif

	/* Not yet implemented */
	return (EOPNOTSUPP);
}


/*
 * Memory Map (mmap) system call.  Note that the file offset
 * and address are allowed to be NOT page aligned, though if
 * the MAP_FIXED flag it set, both must have the same remainder
 * modulo the PAGE_SIZE (POSIX 1003.1b).  If the address is not
 * page-aligned, the actual mapping starts at trunc_page(addr)
 * and the return value is adjusted up by the page offset.
 */
int
sys_mmap(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	register struct sys_mmap_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ *uap = v;
	struct vattr va;
	register struct filedesc *fdp = p->p_fd;
	register struct file *fp;
	struct vnode *vp;
	vm_offset_t addr, pos;
	vm_size_t size, pageoff;
	vm_prot_t prot, maxprot;
	caddr_t handle;
	int fd, flags, error;
	vm_offset_t vm_min_address = VM_MIN_ADDRESS;

	addr = (vm_offset_t) SCARG(uap, addr);
	size = (vm_size_t) SCARG(uap, len);
	prot = SCARG(uap, prot) & VM_PROT_ALL;
	flags = SCARG(uap, flags);
	fd = SCARG(uap, fd);
	pos = (vm_offset_t) SCARG(uap, pos);

#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("mmap(%d): addr %lx len %lx pro %x flg %x fd %d pos %lx\n",
		       p->p_pid, addr, size, prot, flags, fd, pos);
#endif

	/*
	 * Align the file position to a page boundary,
	 * and save its page offset component.
	 */
	pageoff = (pos & PAGE_MASK);
	pos  -= pageoff;

	/* Adjust size for rounding (on both ends). */
	size += pageoff;	/* low end... */
	size = (vm_size_t) round_page(size); /* hi end */

	/* Do not allow mappings that cause address wrap... */
	if ((ssize_t)size < 0)
		return (EINVAL);

	/*
	 * Check for illegal addresses.  Watch out for address wrap...
	 * Note that VM_*_ADDRESS are not constants due to casts (argh).
	 */
	if (flags & MAP_FIXED) {
		/*
		 * The specified address must have the same remainder
		 * as the file offset taken modulo PAGE_SIZE, so it
		 * should be aligned after adjustment by pageoff.
		 */
		addr -= pageoff;
		if (addr & PAGE_MASK)
			return (EINVAL);
		/* Address range must be all in user VM space. */
		if (VM_MAXUSER_ADDRESS > 0 &&
		    addr + size > VM_MAXUSER_ADDRESS)
			return (EINVAL);
		if (vm_min_address > 0 && addr < vm_min_address)
			return (EINVAL);
		if (addr > addr + size)
			return (EINVAL);
	}
	/*
	 * XXX for non-fixed mappings where no hint is provided or
	 * the hint would fall in the potential heap space,
	 * place it after the end of the largest possible heap.
	 *
	 * There should really be a pmap call to determine a reasonable
	 * location.  (To avoid VA cache alias problems, for example!)
	 */
	else if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ))
		addr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);

	if ((flags & MAP_ANON) == 0) {
		/*
		 * Mapping file, get fp for validation.
		 * Obtain vnode and make sure it is of appropriate type.
		 */
		if (((unsigned)fd) >= fdp->fd_nfiles ||
		    (fp = fdp->fd_ofiles[fd]) == NULL)
			return (EBADF);
		if (fp->f_type != DTYPE_VNODE)
			return (EINVAL);
		vp = (struct vnode *)fp->f_data;

		/*
		 * XXX hack to handle use of /dev/zero to map anon
		 * memory (ala SunOS).
		 */
		if (vp->v_type == VCHR && iszerodev(vp->v_rdev)) {
			flags |= MAP_ANON;
			goto is_anon;
		}

		/*
		 * Only files and cdevs are mappable, and cdevs does not
		 * provide private mappings of any kind.
		 */
		if (vp->v_type != VREG &&
		    (vp->v_type != VCHR || (flags & (MAP_PRIVATE|MAP_COPY))))
			return (EINVAL);
		/*
		 * Ensure that file and memory protections are
		 * compatible.  Note that we only worry about
		 * writability if mapping is shared; in this case,
		 * current and max prot are dictated by the open file.
		 * XXX use the vnode instead?  Problem is: what
		 * credentials do we use for determination?
		 * What if proc does a setuid?
		 */
		maxprot = VM_PROT_EXECUTE;	/* ??? */
		if (fp->f_flag & FREAD)
			maxprot |= VM_PROT_READ;
		else if (prot & PROT_READ)
			return (EACCES);

		/*
		 * If we are sharing potential changes (either via MAP_SHARED
		 * or via the implicit sharing of character device mappings),
		 * there are security issues with giving out PROT_WRITE
		 */
		if ((flags & MAP_SHARED) || vp->v_type == VCHR) {

			/* In case we opened the thing readonly... */
			if (!(fp->f_flag & FWRITE)) {
				/*
				 * If we are trying to get write permission
				 * bail out, otherwise go ahead but don't
				 * raise maxprot to contain VM_PROT_WRITE, as
				 * we have not asked for write permission at
				 * all.
				 */
				if (prot & PROT_WRITE)
					return (EACCES);

			/*
			 * If the file is writable, only add PROT_WRITE to
			 * maxprot if the file is not immutable, append-only.
			 * If it is, and if we are going for PROT_WRITE right
			 * away, return EPERM.
			 */
			} else if ((error =
			    VOP_GETATTR(vp, &va, p->p_ucred, p)))
				return (error);
			else if (va.va_flags & (IMMUTABLE|APPEND)) {
				if (prot & PROT_WRITE)
					return (EPERM);
			} else
				maxprot |= VM_PROT_WRITE;
		} else
			maxprot |= VM_PROT_WRITE;
		handle = (caddr_t)vp;
	} else {
		/*
		 * (flags & MAP_ANON) == TRUE
		 * Mapping blank space is trivial.
		 */
		if (fd != -1)
			return (EINVAL);
	is_anon:
		handle = NULL;
		maxprot = VM_PROT_ALL;
		pos = 0;
	}
	error = vm_mmap(&p->p_vmspace->vm_map, &addr, size, prot, maxprot,
	    flags, handle, pos);
	if (error == 0)
		*retval = (register_t)(addr + pageoff);
	return (error);
}

int
sys_msync(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_msync_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) flags;
	} */ *uap = v;
	vm_offset_t addr;
	vm_size_t size, pageoff;
	vm_map_t map;
	int rv, flags;
	boolean_t syncio, invalidate;

	addr = (vm_offset_t)SCARG(uap, addr);
	size = (vm_size_t)SCARG(uap, len);
	flags = SCARG(uap, flags);
#ifdef DEBUG
	if (mmapdebug & (MDB_FOLLOW|MDB_SYNC))
		printf("msync(%d): addr 0x%lx len %lx\n", p->p_pid, addr, size);
#endif

	/* sanity check flags */
	if ((flags & ~(MS_ASYNC | MS_SYNC | MS_INVALIDATE)) != 0 ||
	    (flags & (MS_ASYNC | MS_SYNC | MS_INVALIDATE)) == 0 ||
	    (flags & (MS_ASYNC | MS_SYNC)) == (MS_ASYNC | MS_SYNC))
		return (EINVAL);
	if ((flags & (MS_ASYNC | MS_SYNC)) == 0)
		flags |= MS_SYNC;

	/*
	 * Align the address to a page boundary,
	 * and adjust the size accordingly.
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vm_size_t) round_page(size);

	/* Disallow wrap-around. */
	if (addr + size < addr)
		return (ENOMEM);

	map = &p->p_vmspace->vm_map;
	/*
	 * XXX Gak!  If size is zero we are supposed to sync "all modified
	 * pages with the region containing addr".  Unfortunately, we
	 * don't really keep track of individual mmaps so we approximate
	 * by flushing the range of the map entry containing addr.
	 * This can be incorrect if the region splits or is coalesced
	 * with a neighbor.
	 */
	if (size == 0) {
		vm_map_entry_t entry;

		vm_map_lock_read(map);
		rv = vm_map_lookup_entry(map, addr, &entry);
		vm_map_unlock_read(map);
		if (rv == FALSE)
			return (ENOMEM);
		addr = entry->start;
		size = entry->end - entry->start;
	}
#ifdef DEBUG
	if (mmapdebug & MDB_SYNC)
		printf("msync: cleaning/flushing address range [0x%lx-0x%lx)\n",
		       addr, addr+size);
#endif

#if 0
	/*
	 * XXX Asynchronous msync() causes:
	 *	. the process to hang on wchan "vospgw", and
	 *	. a "vm_object_page_clean: pager_put error" message to
	 *	  be printed by the kernel.
	 */
	syncio = (flags & MS_SYNC) ? TRUE : FALSE;
#else
	syncio = TRUE;
#endif
	invalidate = (flags & MS_INVALIDATE) ? TRUE : FALSE;

	/*
	 * XXX bummer, gotta flush all cached pages to ensure
	 * consistency with the file system cache.  Otherwise, we could
	 * pass this in to implement Sun's MS_INVALIDATE.
	 */
	invalidate = TRUE;
	/*
	 * Clean the pages and interpret the return value.
	 */
	rv = vm_map_clean(map, addr, addr+size, syncio, invalidate);
	switch (rv) {
	case KERN_SUCCESS:
		break;
	case KERN_INVALID_ADDRESS:
		return (ENOMEM);
	case KERN_FAILURE:
		return (EIO);
	case KERN_PAGES_LOCKED:
		return (EBUSY);
	default:
		return (EINVAL);
	}
	return (0);
}

int
sys_munmap(p, v, retval)
	register struct proc *p;
	void *v;
	register_t *retval;
{
	register struct sys_munmap_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
	} */ *uap = v;
	vm_offset_t addr;
	vm_size_t size, pageoff;
	vm_map_t map;
	vm_offset_t vm_min_address = VM_MIN_ADDRESS;
	

	addr = (vm_offset_t) SCARG(uap, addr);
	size = (vm_size_t) SCARG(uap, len);
#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("munmap(%d): addr 0%lx len %lx\n", p->p_pid, addr, size);
#endif

	/*
	 * Align the address to a page boundary,
	 * and adjust the size accordingly.
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vm_size_t) round_page(size);
	if ((int)size < 0)
		return(EINVAL);
	if (size == 0)
		return(0);
	/*
	 * Check for illegal addresses.  Watch out for address wrap...
	 * Note that VM_*_ADDRESS are not constants due to casts (argh).
	 */
	if (VM_MAXUSER_ADDRESS > 0 && addr + size > VM_MAXUSER_ADDRESS)
		return (EINVAL);
	if (vm_min_address > 0 && addr < vm_min_address)
		return (EINVAL);
	if (addr > addr + size)
		return (EINVAL);
	map = &p->p_vmspace->vm_map;
	/*
	 * Make sure entire range is allocated.
	 */
	if (!vm_map_check_protection(map, addr, addr + size, VM_PROT_NONE))
		return(EINVAL);
	/* returns nothing but KERN_SUCCESS anyway */
	(void) vm_map_remove(map, addr, addr+size);
	return(0);
}

void
munmapfd(p, fd)
	struct proc *p;
	int fd;
{
#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("munmapfd(%d): fd %d\n", p->p_pid, fd);
#endif

	/*
	 * XXX should vm_deallocate any regions mapped to this file
	 */
	p->p_fd->fd_ofileflags[fd] &= ~UF_MAPPED;
}

int
sys_mprotect(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_mprotect_args /* {
		syscallarg(void *) addr;
		syscallarg(int) len;
		syscallarg(int) prot;
	} */ *uap = v;
	vm_offset_t addr;
	vm_size_t size, pageoff;
	register vm_prot_t prot;

	addr = (vm_offset_t)SCARG(uap, addr);
	size = (vm_size_t)SCARG(uap, len);
	prot = SCARG(uap, prot) & VM_PROT_ALL;
#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("mprotect(%d): addr 0x%lx len %lx prot %d\n", p->p_pid,
		    addr, size, prot);
#endif
	/*
	 * Align the address to a page boundary,
	 * and adjust the size accordingly.
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vm_size_t) round_page(size);
	if ((int)size < 0)
		return(EINVAL);

	switch (vm_map_protect(&p->p_vmspace->vm_map, addr, addr+size, prot,
	    FALSE)) {
	case KERN_SUCCESS:
		return (0);
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
	}
	return (EINVAL);
}

int
sys_minherit(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_minherit_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(int) len;
		syscallarg(int) inherit;
	} */ *uap = v;
	vm_offset_t addr;
	vm_size_t size, pageoff;
	register vm_inherit_t inherit;

	addr = (vm_offset_t)SCARG(uap, addr);
	size = (vm_size_t)SCARG(uap, len);
	inherit = SCARG(uap, inherit);
#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("minherit(%d): addr 0x%lx len %lx inherit %d\n", p->p_pid,
		    addr, size, inherit);
#endif
	/*
	 * Align the address to a page boundary,
	 * and adjust the size accordingly.
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vm_size_t) round_page(size);
	if ((int)size < 0)
		return(EINVAL);

	switch (vm_map_inherit(&p->p_vmspace->vm_map, addr, addr+size,
	    inherit)) {
	case KERN_SUCCESS:
		return (0);
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
	}
	return (EINVAL);
}

/* ARGSUSED */
int
sys_madvise(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_madvise_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) behav;
	} */ *uap = v;
#endif

	/* Not yet implemented */
	return (EOPNOTSUPP);
}

/* ARGSUSED */
int
sys_mincore(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_mincore_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(char *) vec;
	} */ *uap = v;
#endif

	/* Not yet implemented */
	return (EOPNOTSUPP);
}

int
sys_mlock(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_mlock_args /* {
		syscallarg(const void *) addr;
		syscallarg(size_t) len;
	} */ *uap = v;
	vm_offset_t addr;
	vm_size_t size, pageoff;
	int error;
	extern int vm_page_max_wired;

	addr = (vm_offset_t)SCARG(uap, addr);
	size = (vm_size_t)SCARG(uap, len);
#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("mlock(%d): addr 0%lx len %lx\n", p->p_pid, addr, size);
#endif
	/*
	 * Align the address to a page boundary,
	 * and adjust the size accordingly.
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vm_size_t) round_page(size);

	/* Disallow wrap-around. */
	if (addr + (int)size < addr)
		return (EINVAL);

	if (atop(size) + cnt.v_wire_count > vm_page_max_wired)
		return (EAGAIN);
#ifdef pmap_wired_count
	if (size + ptoa(pmap_wired_count(vm_map_pmap(&p->p_vmspace->vm_map))) >
	    p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur)
		return (EAGAIN);
#else
	if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
		return (error);
#endif

	error = vslock((caddr_t)addr, size);
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
}

int
sys_munlock(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_munlock_args /* {
		syscallarg(const void *) addr;
		syscallarg(size_t) len;
	} */ *uap = v;
	vm_offset_t addr;
	vm_size_t size, pageoff;
	int error;

	addr = (vm_offset_t)SCARG(uap, addr);
	size = (vm_size_t)SCARG(uap, len);
#ifdef DEBUG
	if (mmapdebug & MDB_FOLLOW)
		printf("munlock(%d): addr 0x%lx len %lx\n", p->p_pid, addr, size);
#endif
	/*
	 * Align the address to a page boundary,
	 * and adjust the size accordingly.
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vm_size_t) round_page(size);

	/* Disallow wrap-around. */
	if (addr + (int)size < addr)
		return (EINVAL);

#ifndef pmap_wired_count
	if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
		return (error);
#endif

	error = vsunlock((caddr_t)addr, size);
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
}

/*
 * Internal version of mmap.
 * Currently used by mmap, exec, and sys5 shared memory.
 * Handle is either a vnode pointer or NULL for MAP_ANON.
 * This (internal) interface requires the file offset to be
 * page-aligned by the caller.  (Also addr, if MAP_FIXED).
 */
int
vm_mmap(map, addr, size, prot, maxprot, flags, handle, foff)
	register vm_map_t map;
	register vm_offset_t *addr;
	register vm_size_t size;
	vm_prot_t prot, maxprot;
	register int flags;
	caddr_t handle;		/* XXX should be vp */
	vm_offset_t foff;
{
	register vm_pager_t pager;
	boolean_t fitit;
	vm_object_t object;
	struct vnode *vp = NULL;
	int type;
	int rv = KERN_SUCCESS;

	if (size == 0)
		return (0);

	/* The file offset must be page aligned. */
	if (foff & PAGE_MASK)
		return (EINVAL);

	if ((flags & MAP_FIXED) == 0) {
		/* The address is just a hint */
		fitit = TRUE;
		*addr = round_page(*addr);
	} else {
		/*
		 * Use the specified address exactly
		 * (but check alignment first).
		 */
		fitit = FALSE;
		if (*addr & PAGE_MASK)
			return (EINVAL);
		(void)vm_deallocate(map, *addr, size);
	}

	/*
	 * Lookup/allocate pager.  All except an unnamed anonymous lookup
	 * gain a reference to ensure continued existance of the object.
	 * (XXX the exception is to appease the pageout daemon)
	 */
	if (flags & MAP_ANON) {
		type = PG_DFLT;
		foff = 0;
	} else {
		vp = (struct vnode *)handle;
		if (vp->v_type == VCHR) {
			type = PG_DEVICE;
			handle = (caddr_t)(long)vp->v_rdev;
		} else
			type = PG_VNODE;
	}
	pager = vm_pager_allocate(type, handle, size, prot, foff);
	if (pager == NULL)
		return (type == PG_DEVICE ? EINVAL : ENOMEM);
	/*
	 * Find object and release extra reference gained by lookup
	 */
	object = vm_object_lookup(pager);
	vm_object_deallocate(object);

	/*
	 * Anonymous memory.
	 */
	if (flags & MAP_ANON) {
		rv = vm_allocate_with_pager(map, addr, size, fitit,
					    pager, foff, TRUE);
		if (rv != KERN_SUCCESS) {
			if (handle == NULL)
				vm_pager_deallocate(pager);
			else
				vm_object_deallocate(object);
			goto out;
		}
		/*
		 * Don't cache anonymous objects.
		 * Loses the reference gained by vm_pager_allocate.
		 * Note that object will be NULL when handle == NULL,
		 * this is ok since vm_allocate_with_pager has made
		 * sure that these objects are uncached.
		 */
		(void) pager_cache(object, FALSE);
#ifdef DEBUG
		if (mmapdebug & MDB_MAPIT)
			printf("vm_mmap(%d): ANON *addr %lx size %lx pager %p\n",
			       curproc->p_pid, *addr, size, pager);
#endif
	}
	/*
	 * Must be a mapped file.
	 * Distinguish between character special and regular files.
	 */
	else if (vp->v_type == VCHR) {
		rv = vm_allocate_with_pager(map, addr, size, fitit,
					    pager, foff, FALSE);
		/*
		 * Uncache the object and lose the reference gained
		 * by vm_pager_allocate().  If the call to
		 * vm_allocate_with_pager() was sucessful, then we
		 * gained an additional reference ensuring the object
		 * will continue to exist.  If the call failed then
		 * the deallocate call below will terminate the
		 * object which is fine.
		 */
		(void) pager_cache(object, FALSE);
		if (rv != KERN_SUCCESS)
			goto out;
	}
	/*
	 * A regular file
	 */
	else {
#ifdef DEBUG
		if (object == NULL)
			printf("vm_mmap: no object: vp %p, pager %p\n",
			       vp, pager);
#endif
		/*
		 * Map it directly.
		 * Allows modifications to go out to the vnode.
		 */
		if (flags & MAP_SHARED) {
			rv = vm_allocate_with_pager(map, addr, size,
						    fitit, pager,
						    foff, FALSE);
			if (rv != KERN_SUCCESS) {
				vm_object_deallocate(object);
				goto out;
			}
			/*
			 * Don't cache the object.  This is the easiest way
			 * of ensuring that data gets back to the filesystem
			 * because vnode_pager_deallocate() will fsync the
			 * vnode.  pager_cache() will lose the extra ref.
			 */
			if (prot & VM_PROT_WRITE)
				pager_cache(object, FALSE);
			else
				vm_object_deallocate(object);
		}
		/*
		 * Copy-on-write of file.  Two flavors.
		 * MAP_COPY is true COW, you essentially get a snapshot of
		 * the region at the time of mapping.  MAP_PRIVATE means only
		 * that your changes are not reflected back to the object.
		 * Changes made by others will be seen.
		 */
		else {
			vm_map_t tmap;
			vm_offset_t off;

			/* locate and allocate the target address space */
			vm_map_lock(map);
			if (fitit) {
				/*
				 * Find space in the map at a location
				 * that is compatible with the object/offset
				 * we're going to attach there.
				 */
			again:
				if (vm_map_findspace(map, *addr, size,
						     addr) == 1) {
					rv = KERN_NO_SPACE;
				} else {
#ifdef	PMAP_PREFER
					PMAP_PREFER(foff, addr);
#endif
					rv = vm_map_insert(map, NULL,
							   (vm_offset_t)0,
							   *addr, *addr+size);
					/*
					 * vm_map_insert() may fail if
					 * PMAP_PREFER() has altered
					 * the initial address.
					 * If so, we start again.
					 */
					if (rv == KERN_NO_SPACE)
						goto again;
				}
			} else {
				rv = vm_map_insert(map, NULL, (vm_offset_t)0,
						   *addr, *addr + size);

#ifdef DEBUG
				/*
				 * Check against PMAP preferred address. If
				 * there's a mismatch, these pages should not
				 * be shared with others. <howto?>
				 */
				if (rv == KERN_SUCCESS &&
				    (mmapdebug & MDB_MAPIT)) {
					vm_offset_t	paddr = *addr;
#ifdef	PMAP_PREFER
					PMAP_PREFER(foff, &paddr);
#endif
					if (paddr != *addr)
					    printf(
					      "vm_mmap: pmap botch! "
					      "[foff %lx, addr %lx, paddr %lx]\n",
					      foff, *addr, paddr);
				}
#endif
			}
			vm_map_unlock(map);

			if (rv != KERN_SUCCESS) {
				vm_object_deallocate(object);
				goto out;
			}
			tmap = vm_map_create(pmap_create(size), VM_MIN_ADDRESS,
					     VM_MIN_ADDRESS+size, TRUE);
			off = VM_MIN_ADDRESS;
			rv = vm_allocate_with_pager(tmap, &off, size,
						    FALSE, pager,
						    foff, FALSE);
			if (rv != KERN_SUCCESS) {
				vm_object_deallocate(object);
				vm_map_deallocate(tmap);
				goto out;
			}
			/*
			 * (XXX)
			 * MAP_PRIVATE implies that we see changes made by
			 * others.  To ensure that we need to guarentee that
			 * no copy object is created (otherwise original
			 * pages would be pushed to the copy object and we
			 * would never see changes made by others).  We
			 * totally sleeze it right now by marking the object
			 * internal temporarily.
			 */
			if ((flags & MAP_COPY) == 0)
				object->flags |= OBJ_INTERNAL;
			rv = vm_map_copy(map, tmap, *addr, size, off,
					 FALSE, FALSE);
			object->flags &= ~OBJ_INTERNAL;
			/*
			 * (XXX)
			 * My oh my, this only gets worse...
			 * Force creation of a shadow object so that
			 * vm_map_fork will do the right thing.
			 */
			if ((flags & MAP_COPY) == 0) {
				vm_map_t tmap;
				vm_map_entry_t tentry;
				vm_object_t tobject;
				vm_offset_t toffset;
				vm_prot_t tprot;
				boolean_t twired, tsu;

				tmap = map;
				vm_map_lookup(&tmap, *addr, VM_PROT_WRITE,
					      &tentry, &tobject, &toffset,
					      &tprot, &twired, &tsu);
				vm_map_lookup_done(tmap, tentry);
			}
			/*
			 * (XXX)
			 * Map copy code cannot detect sharing unless a
			 * sharing map is involved.  So we cheat and write
			 * protect everything ourselves.
			 */
			vm_object_pmap_copy(object, foff, foff + size);
			vm_object_deallocate(object);
			vm_map_deallocate(tmap);
			if (rv != KERN_SUCCESS)
				goto out;
		}
#ifdef DEBUG
		if (mmapdebug & MDB_MAPIT)
			printf("vm_mmap(%d): FILE *addr %lx size %lx pager %p\n",
			       curproc->p_pid, *addr, size, pager);
#endif
	}
	/*
	 * Correct protection (default is VM_PROT_ALL).
	 * If maxprot is different than prot, we must set both explicitly.
	 */
	rv = KERN_SUCCESS;
	if (maxprot != VM_PROT_ALL)
		rv = vm_map_protect(map, *addr, *addr+size, maxprot, TRUE);
	if (rv == KERN_SUCCESS && prot != maxprot)
		rv = vm_map_protect(map, *addr, *addr+size, prot, FALSE);
	if (rv != KERN_SUCCESS) {
		(void) vm_deallocate(map, *addr, size);
		goto out;
	}
	/*
	 * Shared memory is also shared with children.
	 */
	if (flags & MAP_SHARED) {
		rv = vm_map_inherit(map, *addr, *addr+size, VM_INHERIT_SHARE);
		if (rv != KERN_SUCCESS) {
			(void) vm_deallocate(map, *addr, size);
			goto out;
		}
	}
out:
#ifdef DEBUG
	if (mmapdebug & MDB_MAPIT)
		printf("vm_mmap: rv %d\n", rv);
#endif
	switch (rv) {
	case KERN_SUCCESS:
		return (0);
	case KERN_INVALID_ADDRESS:
	case KERN_NO_SPACE:
		return (ENOMEM);
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
	default:
		return (EINVAL);
	}
}

int
sys_mlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_mlockall_args /* {
		syscallarg(int) flags;
	} */ *uap = v;
#endif

	return (EOPNOTSUPP);
}

int
sys_munlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{

	return (EOPNOTSUPP);
}


	@


1.17
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.16 2001/03/09 15:11:47 art Exp $	*/
@


1.16
log
@Add mlockall and munlockall (dummy for the old vm system).
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.15 1998/06/02 05:22:27 deraadt Exp $	*/
d204 2
a205 2
	else if (addr < round_page(p->p_vmspace->vm_daddr + MAXDSIZ))
		addr = round_page(p->p_vmspace->vm_daddr + MAXDSIZ);
@


1.15
log
@const mlock(2) and munlock(2)
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.14 1998/05/11 19:43:11 niklas Exp $	*/
d1027 28
@


1.15.8.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.16 2001/03/09 15:11:47 art Exp $	*/
a1026 28

int
sys_mlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_mlockall_args /* {
		syscallarg(int) flags;
	} */ *uap = v;
#endif

	return (EOPNOTSUPP);
}

int
sys_munlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{

	return (EOPNOTSUPP);
}


	@


1.15.8.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.15.8.1 2001/05/14 22:47:50 niklas Exp $	*/
@


1.14
log
@Do not allow mapping of immutable/append-only in a way that allows writing.
Information about the problem gathered from NetBSD, but solved in a different
albeit similar way.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.13 1998/02/25 22:13:46 deraadt Exp $	*/
d620 1
a620 1
		syscallarg(void *) addr;
d669 1
a669 1
		syscallarg(void *) addr;
@


1.13
log
@check for zerodev special case before kicking out all VCHR cases
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.12 1998/02/19 22:00:27 niklas Exp $	*/
d59 1
d135 1
d253 1
a253 2
		 * and we are trying to get write permission although we
		 * opened it without asking for it, bail out.
d255 29
a283 4
		if (((flags & MAP_SHARED) != 0 || vp->v_type == VCHR) &&
		    (fp->f_flag & FWRITE) == 0 && (prot & PROT_WRITE) != 0)
			return (EACCES);
		else
@


1.12
log
@Tighten up the mmapping of char devices even more.  Do not allow
MAP_PRIVATE or MAP_COPY of them as it was incorrectly handled anyhow.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.11 1998/02/18 23:36:12 deraadt Exp $	*/
a217 7
		 * Only files and cdevs are mappable, and cdevs does not
		 * provide private mappings of any kind.
		 */
		if (vp->v_type != VREG &&
		    (vp->v_type != VCHR || (flags & (MAP_PRIVATE|MAP_COPY))))
			return (EINVAL);
		/*
d225 8
@


1.11
log
@do not permit read+write mmap on a read-only device-based descriptor;
mostly from chuck@@maria.wustl.edu
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.10 1997/11/14 20:56:08 deraadt Exp $	*/
d216 7
a222 1
		if (vp->v_type != VREG && vp->v_type != VCHR)
d247 10
a256 11
		if ((flags & MAP_SHARED) != 0 ||
		    ((flags & (MAP_PRIVATE|MAP_SHARED|MAP_COPY)) == MAP_FILE &&
		    vp->v_type == VCHR)) {
			if (fp->f_flag & FWRITE)
				maxprot |= VM_PROT_WRITE;
		} else if (flags & MAP_SHARED) {
			if (fp->f_flag & FWRITE)
				maxprot |= VM_PROT_WRITE;
			else if (prot & PROT_WRITE)
				return (EACCES);
		} else
@


1.10
log
@convert to void *addr
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.9 1997/11/13 18:35:38 deraadt Exp $	*/
d240 7
a246 1
		if (flags & MAP_SHARED) {
@


1.9
log
@make msync() have 3 args
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.8 1997/07/25 06:03:08 mickey Exp $	*/
d126 1
a126 1
		syscallarg(caddr_t) addr;
d384 1
a384 1
		syscallarg(caddr_t) addr;
d456 1
a456 1
		syscallarg(caddr_t) addr;
d546 1
a546 1
		syscallarg(caddr_t) addr;
d565 1
a565 1
		syscallarg(caddr_t) addr;
d582 1
a582 1
		syscallarg(caddr_t) addr;
d631 1
a631 1
		syscallarg(caddr_t) addr;
@


1.8
log
@tabify
some /lx/p/ printf changes
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.7 1997/01/07 05:37:37 tholo Exp $	*/
d274 1
a274 1
		syscallarg(caddr_t) addr;
d276 1
d281 1
a281 1
	int rv;
d286 1
d292 8
d310 2
a311 2
	if (addr + (int)size < addr)
		return (EINVAL);
d329 1
a329 1
			return (EINVAL);
d338 2
d341 4
a344 2
	 * Could pass this in as a third flag argument to implement
	 * Sun's MS_ASYNC.
d346 2
d349 3
d366 1
a366 1
		return (EINVAL);	/* Sun returns ENOMEM? */
d369 2
@


1.7
log
@Fix for final ptdi panic on i386
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_mmap.c,v 1.5 1996/04/19 16:10:48 niklas Exp $	*/
d287 1
a287 2
		printf("msync(%d): addr %lx len %lx\n",
		       p->p_pid, addr, size);
d325 1
a325 1
		printf("msync: cleaning/flushing address range [%lx-%lx)\n",
d376 1
a376 2
		printf("munmap(%d): addr %lx len %lx\n",
		       p->p_pid, addr, size);
d448 1
a448 1
		printf("mprotect(%d): addr %lx len %lx prot %d\n", p->p_pid,
d492 1
a492 1
		printf("minherit(%d): addr %x len %x inherit %d\n", p->p_pid,
d573 1
a573 2
		printf("mlock(%d): addr %lx len %lx\n",
		       p->p_pid, addr, size);
d621 1
a621 2
		printf("munlock(%d): addr %lx len %lx\n",
		       p->p_pid, addr, size);
@


1.6
log
@partial sync with netbsd 960418, more to come
@
text
@d602 1
a602 1
	error = vm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, FALSE);
d645 1
a645 1
	error = vm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, TRUE);
@


1.5
log
@NetBSD 960317 merge
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_mmap.c,v 1.4 1996/03/03 17:45:32 niklas Exp $	*/
/*	$NetBSD: vm_mmap.c,v 1.46 1996/02/28 22:39:13 gwr Exp $	*/
d153 1
a153 1
		printf("mmap(%d): addr %x len %x pro %x flg %x fd %d pos %x\n",
d287 1
a287 1
		printf("msync(%d): addr %x len %x\n",
d326 1
a326 1
		printf("msync: cleaning/flushing address range [%x-%x)\n",
d377 1
a377 1
		printf("munmap(%d): addr %x len %x\n",
d450 1
a450 1
		printf("mprotect(%d): addr %x len %x prot %d\n", p->p_pid,
d575 1
a575 1
		printf("mlock(%d): addr %x len %x\n",
d624 1
a624 1
		printf("munlock(%d): addr %x len %x\n",
d743 1
a743 1
			printf("vm_mmap(%d): ANON *addr %x size %x pager %p\n",
d857 1
a857 1
					      "[foff %x, addr %x, paddr %x]\n",
d928 1
a928 1
			printf("vm_mmap(%d): FILE *addr %x size %x pager %p\n",
@


1.4
log
@From NetBSD: merge with 960217
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: vm_mmap.c,v 1.45 1996/02/10 00:08:10 christos Exp $	*/
d823 3
a825 1
					vm_object_prefer(object, foff, addr);
d831 1
a831 1
					 * vm_object_prefer() has altered
d851 3
a853 1
					vm_object_prefer(object, foff, &paddr);
@


1.3
log
@add minherit() system call
@
text
@d1 2
a2 1
/*	$NetBSD: vm_mmap.c,v 1.43 1995/12/05 22:54:42 pk Exp $	*/
d83 1
d87 1
d100 1
d104 1
a109 76
#if defined(COMPAT_43) || defined(COMPAT_SUNOS) || defined(COMPAT_OSF1) || \
    defined(COMPAT_FREEBSD)
/* ARGSUSED */
int
compat_43_sys_getpagesize(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{

	*retval = PAGE_SIZE;
	return (0);
}
#endif /* COMPAT_43 || COMPAT_SUNOS || COMPAT_OSF1 || COMPAT_FREEBSD */

#if defined(COMPAT_43) || defined(COMPAT_FREEBSD)
int
compat_43_sys_mmap(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	register struct compat_43_sys_mmap_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pos;
	} */ *uap = v;
	struct sys_mmap_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ nargs;
	static const char cvtbsdprot[8] = {
		0,
		PROT_EXEC,
		PROT_WRITE,
		PROT_EXEC|PROT_WRITE,
		PROT_READ,
		PROT_EXEC|PROT_READ,
		PROT_WRITE|PROT_READ,
		PROT_EXEC|PROT_WRITE|PROT_READ,
	};
#define	OMAP_ANON	0x0002
#define	OMAP_COPY	0x0020
#define	OMAP_SHARED	0x0010
#define	OMAP_FIXED	0x0100
#define	OMAP_INHERIT	0x0800

	SCARG(&nargs, addr) = SCARG(uap, addr);
	SCARG(&nargs, len) = SCARG(uap, len);
	SCARG(&nargs, prot) = cvtbsdprot[SCARG(uap, prot)&0x7];
	SCARG(&nargs, flags) = 0;
	if (SCARG(uap, flags) & OMAP_ANON)
		SCARG(&nargs, flags) |= MAP_ANON;
	if (SCARG(uap, flags) & OMAP_COPY)
		SCARG(&nargs, flags) |= MAP_COPY;
	if (SCARG(uap, flags) & OMAP_SHARED)
		SCARG(&nargs, flags) |= MAP_SHARED;
	else
		SCARG(&nargs, flags) |= MAP_PRIVATE;
	if (SCARG(uap, flags) & OMAP_FIXED)
		SCARG(&nargs, flags) |= MAP_FIXED;
	if (SCARG(uap, flags) & OMAP_INHERIT)
		SCARG(&nargs, flags) |= MAP_INHERIT;
	SCARG(&nargs, fd) = SCARG(uap, fd);
	SCARG(&nargs, pos) = SCARG(uap, pos);
	return (sys_mmap(p, &nargs, retval));
}
#endif
d142 1
d186 2
a187 1
		if (VM_MAXUSER_ADDRESS > 0 && addr + size > VM_MAXUSER_ADDRESS)
d189 1
a189 1
		if (VM_MIN_ADDRESS > 0 && addr < VM_MIN_ADDRESS)
d370 2
d399 1
a399 1
	if (VM_MIN_ADDRESS > 0 && addr < VM_MIN_ADDRESS)
d525 1
d531 1
d544 1
d550 1
d598 1
a598 1
	if (error = suser(p->p_ucred, &p->p_acflag))
d641 1
a641 1
	if (error = suser(p->p_ucred, &p->p_acflag))
d743 1
a743 1
			printf("vm_mmap(%d): ANON *addr %x size %x pager %x\n",
d773 1
a773 1
			printf("vm_mmap: no object: vp %x, pager %x\n",
d924 1
a924 1
			printf("vm_mmap(%d): FILE *addr %x size %x pager %x\n",
@


1.2
log
@from netbsd:
Extend use of vm_object_prefer() to vm_allocate_with_pager().
Make vm_object_prefer() call MD aligner for "pageless" objects too,
so we can have more control over the virtual address to be used.

Implementation could be simpler if we by-pass the object to mapped, but
we'd loose the ability to adapt alignment to objects that were previously
mmap'ed with MAP_FIXED on.

Only expect vm_fork() to return if __FORK_BRAINDAMAGE is defined.
Eliminate unused third arg to vm_fork().
@
text
@d541 44
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: vm_mmap.c,v 1.42 1995/10/10 01:27:11 mycroft Exp $	*/
d830 1
d833 3
a835 5
				 * We cannot call vm_map_find() because
				 * a proposed address may be vetoed by
				 * the pmap module.
				 * So we look for space ourselves, validate
				 * it and insert it into the map. 
a836 1
				vm_map_lock(map);
d844 8
a851 2
							(vm_offset_t)0,
							*addr, *addr+size);
a852 5
						/*
						 * Modified address didn't fit
						 * after all, the gap must
						 * have been to small.
						 */
a854 1
				vm_map_unlock(map);
d856 2
a857 2
				rv = vm_map_find(map, NULL, (vm_offset_t)0,
					 addr, size, 0);
d859 1
d865 2
a866 1
				if (rv == KERN_SUCCESS) {
d870 4
a873 1
						printf("vm_mmap: pmap botch!\n");
d875 1
d877 1
d887 1
a887 1
						    TRUE, pager,
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

