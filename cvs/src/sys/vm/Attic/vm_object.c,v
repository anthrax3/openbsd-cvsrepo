head	1.23;
access;
symbols
	SMP_SYNC_A:1.23
	SMP_SYNC_B:1.23
	UBC_SYNC_A:1.23
	UBC_SYNC_B:1.23
	OPENBSD_2_9_BASE:1.22
	OPENBSD_2_9:1.22.0.14
	OPENBSD_2_8:1.22.0.12
	OPENBSD_2_8_BASE:1.22
	OPENBSD_2_7:1.22.0.10
	OPENBSD_2_7_BASE:1.22
	SMP:1.22.0.8
	SMP_BASE:1.22
	kame_19991208:1.22
	OPENBSD_2_6:1.22.0.6
	OPENBSD_2_6_BASE:1.22
	OPENBSD_2_5:1.22.0.4
	OPENBSD_2_5_BASE:1.22
	OPENBSD_2_4:1.22.0.2
	OPENBSD_2_4_BASE:1.22
	OPENBSD_2_3:1.21.0.2
	OPENBSD_2_3_BASE:1.21
	OPENBSD_2_2:1.19.0.2
	OPENBSD_2_2_BASE:1.19
	OPENBSD_2_1:1.15.0.2
	OPENBSD_2_1_BASE:1.15
	OPENBSD_2_0:1.11.0.2
	OPENBSD_2_0_BASE:1.11
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.23
date	2001.06.27.04.53.31;	author art;	state dead;
branches;
next	1.22;

1.22
date	98.04.25.07.17.21;	author niklas;	state Exp;
branches
	1.22.8.1;
next	1.21;

1.21
date	98.03.01.00.38.15;	author niklas;	state Exp;
branches;
next	1.20;

1.20
date	97.11.06.05.59.35;	author csapuntz;	state Exp;
branches;
next	1.19;

1.19
date	97.10.06.20.21.23;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	97.10.06.15.28.55;	author csapuntz;	state Exp;
branches;
next	1.17;

1.17
date	97.07.25.06.03.09;	author mickey;	state Exp;
branches;
next	1.16;

1.16
date	97.05.28.22.40.04;	author mickey;	state Exp;
branches;
next	1.15;

1.15
date	97.04.17.01.25.20;	author niklas;	state Exp;
branches;
next	1.14;

1.14
date	97.03.26.18.45.31;	author niklas;	state Exp;
branches;
next	1.13;

1.13
date	96.12.24.20.14.28;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	96.11.06.23.24.40;	author niklas;	state Exp;
branches;
next	1.11;

1.11
date	96.08.19.10.38.01;	author niklas;	state Exp;
branches;
next	1.10;

1.10
date	96.08.18.18.44.46;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	96.08.14.23.16.38;	author niklas;	state Exp;
branches;
next	1.8;

1.8
date	96.08.13.22.26.18;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	96.08.12.12.33.34;	author niklas;	state Exp;
branches;
next	1.6;

1.6
date	96.08.02.00.06.02;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	96.07.23.23.54.25;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	96.04.19.16.10.51;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	96.03.03.17.45.34;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	95.12.14.05.16.09;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.53.38;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.53.38;	author deraadt;	state Exp;
branches;
next	;

1.22.8.1
date	2001.07.04.11.01.22;	author niklas;	state dead;
branches;
next	;


desc
@@


1.23
log
@Die!
@
text
@/*	$OpenBSD: vm_object.c,v 1.22 1998/04/25 07:17:21 niklas Exp $	*/
/*	$NetBSD: vm_object.c,v 1.46 1997/03/30 20:56:12 mycroft Exp $	*/

/*-
 * Copyright (c) 1997 Charles M. Hannum.  All rights reserved.
 * Copyright (c) 1997 Niklas Hallqvist.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles M. Hannum.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_object.c	8.7 (Berkeley) 5/11/95
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Authors: Avadis Tevanian, Jr., Michael Wayne Young
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 *	Virtual memory object module.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/proc.h>

#include <vm/vm.h>
#include <vm/vm_page.h>
#include <vm/vm_pageout.h>

/*
 * Virtual memory objects maintain the actual data
 * associated with allocated virtual memory.  A given
 * page of memory exists within exactly one object.
 *
 * An object is only deallocated when all "references"
 * are given up.  Only one "reference" to a given
 * region of an object should be writeable.
 *
 * Associated with each object is a list of all resident
 * memory pages belonging to that object; this list is
 * maintained by the "vm_page" module, and locked by the object's
 * lock.
 *
 * Each object also records a "pager" routine which is
 * used to retrieve (and store) pages to the proper backing
 * storage.  In addition, objects may be backed by other
 * objects from which they were virtual-copied.
 *
 * The only items within the object structure which are
 * modified after time of creation are:
 *	reference count		locked by object's lock
 *	pager routine		locked by object's lock
 *
 */

struct vm_object	kernel_object_store;
struct vm_object	kmem_object_store;

#define	VM_OBJECT_HASH_COUNT	157

extern int vm_cache_max;	/* now in param.c */
struct	vm_object_hash_head vm_object_hashtable[VM_OBJECT_HASH_COUNT];

long	object_collapses = 0;
long	object_bypasses  = 0;
boolean_t vm_object_collapse_allowed = TRUE;

#ifndef VMDEBUG
#define VMDEBUG 0
#endif

#ifdef DEBUG
#define	VMDEBUG_SHADOW		0x1
#define	VMDEBUG_SHADOW_VERBOSE	0x2
#define	VMDEBUG_COLLAPSE	0x4
#define	VMDEBUG_COLLAPSE_PAGEIN	0x8
int	vmdebug = VMDEBUG;
#endif

void		_vm_object_allocate __P((vm_size_t, vm_object_t));
int		vm_object_bypass __P((vm_object_t));
void		vm_object_collapse_internal __P((vm_object_t, vm_object_t *));
int		vm_object_overlay __P((vm_object_t));
int		vm_object_remove_from_pager
		    __P((vm_object_t, vm_offset_t, vm_offset_t));
void		vm_object_set_shadow __P((vm_object_t, vm_object_t));

/*
 * vm_object_init:
 *
 * Initialize the VM objects module.
 */
void
vm_object_init(size)
	vm_size_t	size;
{
	register int	i;

	TAILQ_INIT(&vm_object_cached_list);
	TAILQ_INIT(&vm_object_list);
	vm_object_count = 0;
	simple_lock_init(&vm_cache_lock);
	simple_lock_init(&vm_object_list_lock);

	for (i = 0; i < VM_OBJECT_HASH_COUNT; i++)
		TAILQ_INIT(&vm_object_hashtable[i]);

	kernel_object = &kernel_object_store;
	_vm_object_allocate(size, kernel_object);

	kmem_object = &kmem_object_store;
	_vm_object_allocate(VM_KMEM_SIZE + VM_MBUF_SIZE, kmem_object);
}

/*
 * vm_object_allocate:
 *
 * Returns a new object with the given size.
 */
vm_object_t
vm_object_allocate(size)
	vm_size_t	size;
{
	register vm_object_t	result;

	result = (vm_object_t)malloc((u_long)sizeof *result, M_VMOBJ,
	    M_WAITOK);

	_vm_object_allocate(size, result);

	return(result);
}

void
_vm_object_allocate(size, object)
	vm_size_t		size;
	register vm_object_t	object;
{
	TAILQ_INIT(&object->memq);
	vm_object_lock_init(object);
	object->ref_count = 1;
	object->resident_page_count = 0;
	object->size = size;
	object->flags = OBJ_INTERNAL;	/* vm_allocate_with_pager will reset */
	object->paging_in_progress = 0;
	object->copy = NULL;

	/*
	 *	Object starts out read-write, with no pager.
	 */

	object->pager = NULL;
	object->paging_offset = 0;
	object->shadow = NULL;
	object->shadow_offset = (vm_offset_t) 0;
	LIST_INIT(&object->shadowers);

	simple_lock(&vm_object_list_lock);
	TAILQ_INSERT_TAIL(&vm_object_list, object, object_list);
	vm_object_count++;
	cnt.v_nzfod += atop(size);
	simple_unlock(&vm_object_list_lock);
}

/*
 * vm_object_reference:
 *
 * Gets another reference to the given object.
 */
void
vm_object_reference(object)
	register vm_object_t	object;
{
	if (object == NULL)
		return;

	vm_object_lock(object);
	object->ref_count++;
	vm_object_unlock(object);
}

/*
 * vm_object_deallocate:
 *
 * Release a reference to the specified object,
 * gained either through a vm_object_allocate
 * or a vm_object_reference call.  When all references
 * are gone, storage associated with this object
 * may be relinquished.
 *
 * No object may be locked.
 */
void
vm_object_deallocate(object)
	vm_object_t	object;
{
	/*
	 * While "temp" is used for other things as well, we
	 * initialize it to NULL here for being able to check
	 * if we are in the first revolution of the loop.
	 */
	vm_object_t	temp = NULL;

	while (object != NULL) {

		/*
		 * The cache holds a reference (uncounted) to the object; we
		 * must lock it before removing the object.
		 */

		vm_object_cache_lock();

		/*
		 * Lose the reference
		 */
		vm_object_lock(object);
		if (--(object->ref_count) != 0) {
			vm_object_unlock(object);
			vm_object_cache_unlock();

			/*
			 * If this is a deallocation of a shadow reference
			 * (which it is unless it's the first time round) and
			 * this operation made us singly-shadowed, try to
			 * collapse us with our shadower.  Otherwise we're
			 * ready.
			 */
			if (temp != NULL &&
			    (temp = object->shadowers.lh_first) != NULL &&
			    temp->shadowers_list.le_next == NULL) {
				vm_object_lock(temp);

				/*
				 * This is a bit tricky: the temp object can
				 * go away while collapsing, check the
				 * vm_object_collapse_internal comments for
				 * details.  In this case we get an object
				 * back to deallocate (it's done like this
				 * to prevent potential recursion and hence
				 * kernel stack overflow).  In the normal case
				 * we won't get an object back, if so, we are
				 * ready and may return.
				 */
				vm_object_collapse_internal(temp, &object);
				if (object != NULL) {
					vm_object_lock(object);
					vm_object_cache_lock();
				} else {
					vm_object_unlock(temp);
					return;
				}
			} else
				return;
		}

		/*
		 * See if this object can persist.  If so, enter it in the
		 * cache, then deactivate all of its pages.
		 */
		if (object->flags & OBJ_CANPERSIST) {

			TAILQ_INSERT_TAIL(&vm_object_cached_list, object,
			    cached_list);
			vm_object_cached++;
			vm_object_cache_unlock();

			vm_object_deactivate_pages(object);
			vm_object_unlock(object);

			vm_object_cache_trim();
			return;
		}

		/*
		 * Make sure no one can look us up now.
		 */
		vm_object_remove(object->pager);
		vm_object_cache_unlock();

		/*
		 * Deallocate the object, and move on to the backing object.
		 */
		temp = object->shadow;
		vm_object_reference(temp);
		vm_object_terminate(object);
		object = temp;
	}
}


/*
 * vm_object_terminate actually destroys the specified object, freeing
 * up all previously used resources.
 *
 * The object must be locked.
 */
void
vm_object_terminate(object)
	register vm_object_t	object;
{
	register vm_page_t	p;
	vm_object_t		shadow_object;

	/*
	 * Protect against simultaneous collapses.
	 */
	object->flags |= OBJ_FADING;

	/*
	 * Wait until the pageout daemon is through with the object or a
	 * potential collapse operation is finished.
	 */
	vm_object_paging_wait(object,"vmterm");

	/*
	 * Detach the object from its shadow if we are the shadow's
	 * copy.
	 */
	if ((shadow_object = object->shadow) != NULL) {
		vm_object_lock(shadow_object);
		vm_object_set_shadow(object, NULL);
		if (shadow_object->copy == object)
			shadow_object->copy = NULL;
#if 0
		else if (shadow_object->copy != NULL)
			panic("vm_object_terminate: "
			    "copy/shadow inconsistency");
#endif
		vm_object_unlock(shadow_object);
	}

	/*
	 * If not an internal object clean all the pages, removing them
	 * from paging queues as we go.
	 *
	 * XXX need to do something in the event of a cleaning error.
	 */
	if ((object->flags & OBJ_INTERNAL) == 0)
		(void) vm_object_page_clean(object, 0, 0, TRUE, TRUE);

	/*
	 * Now free the pages.
	 * For internal objects, this also removes them from paging queues.
	 */
	while ((p = object->memq.tqh_first) != NULL) {
		VM_PAGE_CHECK(p);
		vm_page_lock_queues();
		vm_page_free(p);
		cnt.v_pfree++;
		vm_page_unlock_queues();
	}
	if ((object->flags & OBJ_INTERNAL) != 0)
		vm_object_unlock(object);

	/*
	 * Let the pager know object is dead.
	 */
	if (object->pager != NULL)
		vm_pager_deallocate(object->pager);

	simple_lock(&vm_object_list_lock);
	TAILQ_REMOVE(&vm_object_list, object, object_list);
	vm_object_count--;
	simple_unlock(&vm_object_list_lock);

	/*
	 * Free the space for the object.
	 */
	free((caddr_t)object, M_VMOBJ);
}

/*
 * vm_object_page_clean
 *
 * Clean all dirty pages in the specified range of object.
 * If syncio is TRUE, page cleaning is done synchronously.
 * If de_queue is TRUE, pages are removed from any paging queue
 * they were on, otherwise they are left on whatever queue they
 * were on before the cleaning operation began.
 *
 * Odd semantics: if start == end, we clean everything.
 *
 * The object must be locked.
 *
 * Returns TRUE if all was well, FALSE if there was a pager error
 * somewhere.  We attempt to clean (and dequeue) all pages regardless
 * of where an error occurs.
 */
boolean_t
vm_object_page_clean(object, start, end, syncio, de_queue)
	register vm_object_t	object;
	register vm_offset_t	start;
	register vm_offset_t	end;
	boolean_t		syncio;
	boolean_t		de_queue;
{
	register vm_page_t	p;
	int onqueue = 0;
	boolean_t noerror = TRUE;

	if (object == NULL)
		return (TRUE);

	/*
	 * If it is an internal object and there is no pager, attempt to
	 * allocate one.  Note that vm_object_collapse may relocate one
	 * from a collapsed object so we must recheck afterward.
	 */
	if ((object->flags & OBJ_INTERNAL) && object->pager == NULL) {
		vm_object_collapse(object);
		if (object->pager == NULL) {
			vm_pager_t pager;

			vm_object_unlock(object);
			pager = vm_pager_allocate(PG_DFLT, (caddr_t)0,
			    object->size, VM_PROT_ALL, (vm_offset_t)0);
			if (pager)
				vm_object_setpager(object, pager, 0, FALSE);
			vm_object_lock(object);
		}
	}
	if (object->pager == NULL)
		return (FALSE);

again:
	/*
	 * Wait until the pageout daemon is through with the object.
	 */
	vm_object_paging_wait(object,"vclean");

	/*
	 * Loop through the object page list cleaning as necessary.
	 */
	for (p = object->memq.tqh_first; p != NULL; p = p->listq.tqe_next) {
		if ((start == end || (p->offset >= start && p->offset < end)) &&
		    !(p->flags & PG_FICTITIOUS)) {
			if ((p->flags & PG_CLEAN) &&
			    pmap_is_modified(VM_PAGE_TO_PHYS(p)))
				p->flags &= ~PG_CLEAN;
			/*
			 * Remove the page from any paging queue.
			 * This needs to be done if either we have been
			 * explicitly asked to do so or it is about to
			 * be cleaned (see comment below).
			 */
			if (de_queue || !(p->flags & PG_CLEAN)) {
				vm_page_lock_queues();
				if (p->flags & PG_ACTIVE) {
					TAILQ_REMOVE(&vm_page_queue_active,
						     p, pageq);
					p->flags &= ~PG_ACTIVE;
					cnt.v_active_count--;
					onqueue = 1;
				} else if (p->flags & PG_INACTIVE) {
					TAILQ_REMOVE(&vm_page_queue_inactive,
						     p, pageq);
					p->flags &= ~PG_INACTIVE;
					cnt.v_inactive_count--;
					onqueue = -1;
				} else
					onqueue = 0;
				vm_page_unlock_queues();
			}
			/*
			 * To ensure the state of the page doesn't change
			 * during the clean operation we do two things.
			 * First we set the busy bit and write-protect all
			 * mappings to ensure that write accesses to the
			 * page block (in vm_fault).  Second, we remove
			 * the page from any paging queue to foil the
			 * pageout daemon (vm_pageout_scan).
			 */
			pmap_page_protect(VM_PAGE_TO_PHYS(p), VM_PROT_READ);
			if (!(p->flags & PG_CLEAN)) {
				p->flags |= PG_BUSY;
				vm_object_paging_begin(object);
				vm_object_unlock(object);
				/*
				 * XXX if put fails we mark the page as
				 * clean to avoid an infinite loop.
				 * Will loose changes to the page.
				 */
				if (vm_pager_put(object->pager, p, syncio)) {
					printf("%s: pager_put error\n",
					    "vm_object_page_clean");
					p->flags |= PG_CLEAN;
					noerror = FALSE;
				}
				vm_object_lock(object);
				vm_object_paging_end(object);
				if (!de_queue && onqueue) {
					vm_page_lock_queues();
					if (onqueue > 0)
						vm_page_activate(p);
					else
						vm_page_deactivate(p);
					vm_page_unlock_queues();
				}
				p->flags &= ~PG_BUSY;
				PAGE_WAKEUP(p);
				goto again;
			}
		}
	}
	return (noerror);
}

/*
 * vm_object_deactivate_pages
 *
 * Deactivate all pages in the specified object.  (Keep its pages
 * in memory even though it is no longer referenced.)
 *
 * The object must be locked.
 */
void
vm_object_deactivate_pages(object)
	register vm_object_t	object;
{
	register vm_page_t	p, next;

	for (p = object->memq.tqh_first; p != NULL; p = next) {
		next = p->listq.tqe_next;
		vm_page_lock_queues();
		if (p->flags & PG_ACTIVE)
			vm_page_deactivate(p);
		vm_page_unlock_queues();
	}
}

/*
 * Trim the object cache to size.
 */
void
vm_object_cache_trim()
{
	register vm_object_t	object;

	vm_object_cache_lock();
	while (vm_object_cached > vm_cache_max) {
		object = vm_object_cached_list.tqh_first;
		vm_object_cache_unlock();

		if (object != vm_object_lookup(object->pager))
			panic("vm_object_cache_trim: I'm sooo confused.");

		pager_cache(object, FALSE);

		vm_object_cache_lock();
	}
	vm_object_cache_unlock();
}

/*
 * vm_object_pmap_copy:
 *
 * Makes all physical pages in the specified
 * object range copy-on-write.  No writeable
 * references to these pages should remain.
 *
 * The object must *not* be locked.
 */
void
vm_object_pmap_copy(object, start, end)
	register vm_object_t	object;
	register vm_offset_t	start;
	register vm_offset_t	end;
{
	register vm_page_t	p;

	if (object == NULL)
		return;

	vm_object_lock(object);
	for (p = object->memq.tqh_first; p != NULL; p = p->listq.tqe_next) {
		if ((start <= p->offset) && (p->offset < end)) {
			pmap_page_protect(VM_PAGE_TO_PHYS(p), VM_PROT_READ);
			p->flags |= PG_COPYONWRITE;
		}
	}
	vm_object_unlock(object);
}

/*
 * vm_object_pmap_remove:
 *
 * Removes all physical pages in the specified
 * object range from all physical maps.
 *
 * The object must *not* be locked.
 */
void
vm_object_pmap_remove(object, start, end)
	register vm_object_t	object;
	register vm_offset_t	start;
	register vm_offset_t	end;
{
	register vm_page_t	p;

	if (object == NULL)
		return;

	vm_object_lock(object);
	for (p = object->memq.tqh_first; p != NULL; p = p->listq.tqe_next)
		if ((start <= p->offset) && (p->offset < end))
			pmap_page_protect(VM_PAGE_TO_PHYS(p), VM_PROT_NONE);
	vm_object_unlock(object);
}

/*
 * vm_object_copy:
 *
 * Create a new object which is a copy of an existing
 * object, and mark all of the pages in the existing
 * object 'copy-on-write'.  The new object has one reference.
 * Returns the new object.
 *
 * May defer the copy until later if the object is not backed
 * up by a non-default pager.
 */
void
vm_object_copy(src_object, src_offset, size,
		    dst_object, dst_offset, src_needs_copy)
	register vm_object_t	src_object;
	vm_offset_t		src_offset;
	vm_size_t		size;
	vm_object_t		*dst_object;	/* OUT */
	vm_offset_t		*dst_offset;	/* OUT */
	boolean_t		*src_needs_copy;	/* OUT */
{
	register vm_object_t	new_copy;
	register vm_object_t	old_copy;
	vm_offset_t		new_start, new_end;

	register vm_page_t	p;

	if (src_object == NULL) {
		/*
		 * Nothing to copy
		 */
		*dst_object = NULL;
		*dst_offset = 0;
		*src_needs_copy = FALSE;
		return;
	}

	/*
	 * If the object's pager is null_pager or the
	 * default pager, we don't have to make a copy
	 * of it.  Instead, we set the needs copy flag and
	 * make a shadow later.
	 */

	vm_object_lock(src_object);
	if (src_object->pager == NULL ||
	    (src_object->flags & OBJ_INTERNAL)) {

		/*
		 * Make another reference to the object.
		 */
		src_object->ref_count++;

		/*
		 * Mark all of the pages copy-on-write.
		 */
		for (p = src_object->memq.tqh_first; p; p = p->listq.tqe_next)
			if (src_offset <= p->offset &&
			    p->offset < src_offset + size)
				p->flags |= PG_COPYONWRITE;
		vm_object_unlock(src_object);

		*dst_object = src_object;
		*dst_offset = src_offset;
		
		/*
		 * Must make a shadow when write is desired
		 */
		*src_needs_copy = TRUE;
		return;
	}

	/*
	 * Try to collapse the object before copying it.
	 */
	vm_object_collapse(src_object);

	/*
	 * If the object has a pager, the pager wants to
	 * see all of the changes.  We need a copy-object
	 * for the changed pages.
	 *
	 * If there is a copy-object, and it is empty,
	 * no changes have been made to the object since the
	 * copy-object was made.  We can use the same copy-
	 * object.
	 */

Retry1:
	old_copy = src_object->copy;
	if (old_copy != NULL) {
		/*
		 * Try to get the locks (out of order)
		 */
		if (!vm_object_lock_try(old_copy)) {
			vm_object_unlock(src_object);

			/* XXX should spin a bit here... */
			vm_object_lock(src_object);
			goto Retry1;
		}

		if (old_copy->resident_page_count == 0 &&
		    old_copy->pager == NULL) {
			/*
			 * Return another reference to
			 * the existing copy-object.
			 */
			old_copy->ref_count++;
			vm_object_unlock(old_copy);
			vm_object_unlock(src_object);
			*dst_object = old_copy;
			*dst_offset = src_offset;
			*src_needs_copy = FALSE;
			return;
		}
		vm_object_unlock(old_copy);
	}
	vm_object_unlock(src_object);

	/*
	 * If the object has a pager, the pager wants
	 * to see all of the changes.  We must make
	 * a copy-object and put the changed pages there.
	 *
	 * The copy-object is always made large enough to
	 * completely shadow the original object, since
	 * it may have several users who want to shadow
	 * the original object at different points.
	 */

	new_copy = vm_object_allocate(src_object->size);

Retry2:
	vm_object_lock(src_object);
	/*
	 * Copy object may have changed while we were unlocked
	 */
	old_copy = src_object->copy;
	if (old_copy != NULL) {
		/*
		 * Try to get the locks (out of order)
		 */
		if (!vm_object_lock_try(old_copy)) {
			vm_object_unlock(src_object);
			goto Retry2;
		}

		/*
		 * Consistency check
		 */
		if (old_copy->shadow != src_object ||
		    old_copy->shadow_offset != (vm_offset_t) 0)
			panic("vm_object_copy: copy/shadow inconsistency");

		/*
		 * Make the old copy-object shadow the new one.
		 * It will receive no more pages from the original
		 * object.  Locking of new_copy not needed.  We
		 * have the only pointer.
		 */
		vm_object_set_shadow(old_copy, new_copy);
		vm_object_unlock(old_copy);
	}

	/* Always shadow original at 0 for the whole object */
	new_start = (vm_offset_t)0;
	new_end = (vm_offset_t)new_copy->size;

	/*
	 * Point the new copy at the existing object.
	 */

	vm_object_set_shadow(new_copy, src_object);
	new_copy->shadow_offset = new_start;
	src_object->copy = new_copy;

	/*
	 * Mark all the affected pages of the existing object
	 * copy-on-write.
	 */
	for (p = src_object->memq.tqh_first; p != NULL; p = p->listq.tqe_next)
		if ((new_start <= p->offset) && (p->offset < new_end))
			p->flags |= PG_COPYONWRITE;

	vm_object_unlock(src_object);

	*dst_object = new_copy;
	*dst_offset = src_offset - new_start;
	*src_needs_copy = FALSE;
}

/*
 * vm_object_shadow:
 *
 * Create a new object which is backed by the
 * specified existing object range.  The source
 * object reference is deallocated.
 *
 * The new object and offset into that object
 * are returned in the source parameters.
 *
 * The old object should not be locked.
 */
void
vm_object_shadow(object, offset, length)
	vm_object_t	*object;	/* IN/OUT */
	vm_offset_t	*offset;	/* IN/OUT */
	vm_size_t	length;
{
	register vm_object_t	source;
	register vm_object_t	result;

	source = *object;

#ifdef DIAGNOSTIC
	if (source == NULL)
		panic("vm_object_shadow: attempt to shadow null object");
#endif

	/*
	 * Allocate a new object with the given length
	 */
	if ((result = vm_object_allocate(length)) == NULL)
		panic("vm_object_shadow: no object for shadowing");

	/*
	 * The new object shadows the source object.  Our caller changes his
	 * reference to point to the new object, removing a reference to the
	 * source object.
	 */
	vm_object_lock(source);
	vm_object_set_shadow(result, source);
	source->ref_count--;
	vm_object_unlock(source);
	
	/*
	 * Store the offset into the source object,
	 * and fix up the offset into the new object.
	 */
	result->shadow_offset = *offset;

	/*
	 * Return the new things
	 */
	*offset = 0;
	*object = result;
}

/*
 * Set the specified object's pager to the specified pager.
 */
void
vm_object_setpager(object, pager, paging_offset, read_only)
	vm_object_t	object;
	vm_pager_t	pager;
	vm_offset_t	paging_offset;
	boolean_t	read_only;
{
#ifdef	lint
	read_only++;	/* No longer used */
#endif

	vm_object_lock(object);			/* XXX ? */
	object->pager = pager;
	object->paging_offset = paging_offset;
	vm_object_unlock(object);			/* XXX ? */
}

/*
 * vm_object_hash hashes the pager/id pair.
 */

#define vm_object_hash(pager) \
	(((unsigned long)pager)%VM_OBJECT_HASH_COUNT)

/*
 * vm_object_lookup looks in the object cache for an object with the
 * specified pager and paging id.
 */
vm_object_t
vm_object_lookup(pager)
	vm_pager_t	pager;
{
	register vm_object_hash_entry_t	entry;
	vm_object_t			object;

	vm_object_cache_lock();

	for (entry = vm_object_hashtable[vm_object_hash(pager)].tqh_first;
	     entry != NULL;
	     entry = entry->hash_links.tqe_next) {
		object = entry->object;
		if (object->pager == pager) {
			vm_object_lock(object);
			if (object->ref_count == 0) {
				TAILQ_REMOVE(&vm_object_cached_list, object,
					cached_list);
				vm_object_cached--;
			}
			object->ref_count++;
			vm_object_unlock(object);
			vm_object_cache_unlock();
			return(object);
		}
	}

	vm_object_cache_unlock();
	return(NULL);
}

/*
 * vm_object_enter enters the specified object/pager/id into
 * the hash table.
 */

void
vm_object_enter(object, pager)
	vm_object_t	object;
	vm_pager_t	pager;
{
	struct vm_object_hash_head	*bucket;
	register vm_object_hash_entry_t	entry;

	/*
	 * We don't cache null objects, and we can't cache
	 * objects with the null pager.
	 */

	if (object == NULL)
		return;
	if (pager == NULL)
		return;

	bucket = &vm_object_hashtable[vm_object_hash(pager)];
	entry = (vm_object_hash_entry_t)
		malloc((u_long)sizeof *entry, M_VMOBJHASH, M_WAITOK);
	entry->object = object;
	object->flags |= OBJ_CANPERSIST;

	vm_object_cache_lock();
	TAILQ_INSERT_TAIL(bucket, entry, hash_links);
	vm_object_cache_unlock();
}

/*
 * vm_object_remove:
 *
 * Remove the pager from the hash table.
 * Note:  This assumes that the object cache
 * is locked.  XXX this should be fixed
 * by reorganizing vm_object_deallocate.
 */
void
vm_object_remove(pager)
	register vm_pager_t	pager;
{
	struct vm_object_hash_head	*bucket;
	register vm_object_hash_entry_t	entry;
	register vm_object_t		object;

	bucket = &vm_object_hashtable[vm_object_hash(pager)];

	for (entry = bucket->tqh_first;
	     entry != NULL;
	     entry = entry->hash_links.tqe_next) {
		object = entry->object;
		if (object->pager == pager) {
			TAILQ_REMOVE(bucket, entry, hash_links);
			free((caddr_t)entry, M_VMOBJHASH);
			break;
		}
	}
}

/*
 * vm_object_cache_clear removes all objects from the cache.
 */
void
vm_object_cache_clear()
{
	register vm_object_t	object;

	/*
	 * Remove each object in the cache by scanning down the
	 * list of cached objects.
	 */
	vm_object_cache_lock();
	while ((object = vm_object_cached_list.tqh_first) != NULL) {
		vm_object_cache_unlock();

		/* 
		 * Note: it is important that we use vm_object_lookup
		 * to gain a reference, and not vm_object_reference, because
		 * the logic for removing an object from the cache lies in 
		 * lookup.
		 */
		if (object != vm_object_lookup(object->pager))
			panic("vm_object_cache_clear: I'm sooo confused.");
		pager_cache(object, FALSE);

		vm_object_cache_lock();
	}
	vm_object_cache_unlock();
}

/*
 * vm_object_remove_from_pager:
 *
 * Tell object's pager that it needn't back the page
 * anymore.  If the pager ends up empty, deallocate it.
 */
int
vm_object_remove_from_pager(object, from, to)
	vm_object_t	object;
	vm_offset_t	from, to;
{
	vm_pager_t	pager = object->pager;
	int		cnt = 0;

	if (pager == NULL)
		return 0;

	cnt = vm_pager_remove(pager, from, to);

	/* If pager became empty, remove it.  */
	if (cnt > 0 && vm_pager_count(pager) == 0) {
		vm_pager_deallocate(pager);
		object->pager = NULL;
	}
	return(cnt);
}

#define	FREE_PAGE(m)	do {					\
	PAGE_WAKEUP(m);						\
	vm_page_lock_queues();					\
	vm_page_free(m);					\
	vm_page_unlock_queues();				\
} while(0)

/*
 * vm_object_overlay:
 *
 * Internal function to vm_object_collapse called when
 * it has been shown that a collapse operation is likely
 * to succeed.  We know that the backing object is only
 * referenced by me and that paging is not in progress.
 */
int
vm_object_overlay(object)
	vm_object_t	object;
{
	vm_object_t	backing_object = object->shadow;
	vm_offset_t	backing_offset = object->shadow_offset;
	vm_size_t	size = object->size;
	vm_offset_t	offset, paged_offset;
	vm_page_t	backing_page, page = NULL;
	int		rv;

#ifdef DEBUG
	if (vmdebug & VMDEBUG_COLLAPSE)
		printf("vm_object_overlay(0x%p)\n", object);
#endif

	/*
	 * Protect against multiple collapses.
	 */
	backing_object->flags |= OBJ_FADING;

	/*
	 * The algorithm used is roughly like this:
	 * (1)	Trim a potential pager in the backing object so it'll only hold
	 *      pages in reach.
	 * (2)	Loop over all the resident pages in the shadow object and
	 *      either remove them if they are shadowed or move them into the
	 * 	shadowing object.
	 * (3)	Loop over the paged out pages in the shadow object.  Start
	 *      pageins on those that aren't shadowed, and just deallocate
	 * 	the others.  In each iteration check if other users of these
	 *      objects have caused pageins resulting in new resident pages.
	 * 	This can happen while we are waiting for a page or a pagein of
	 *      ours.  If such resident pages turn up, restart from (2).
	 */

	/*
	 * As a first measure we know we can discard everything that the
	 * shadowing object doesn't shadow.
	 */
	if (backing_object->pager != NULL) {
		if (backing_offset > 0)
			vm_object_remove_from_pager(backing_object, 0,
			    backing_offset);
		if (backing_offset + size < backing_object->size)
			vm_object_remove_from_pager(backing_object,
			    backing_offset + size, backing_object->size);
	}

	/*
	 * At this point, there may still be asynchronous paging in the parent
	 * object.  Any pages being paged in will be represented by fake pages.
	 * There are three cases:
	 * 1) The page is being paged in from the parent object's own pager.
	 *    In this case, we just delete our copy, since it's not needed.
	 * 2) The page is being paged in from the backing object.  We prevent
	 *    this case by waiting for paging to complete on the backing object
	 *    before continuing.
	 * 3) The page is being paged in from a backing object behind the one
	 *    we're deleting.  We'll never notice this case, because the
	 *    backing object we're deleting won't have the page.
	 */

	vm_object_unlock(object);
retry:
	vm_object_paging_wait(backing_object,"vpagew");

	/*
	 * While we were asleep, the parent object might have been deleted.  If
	 * so, the backing object will now have only one reference (the one we
	 * hold).  If this happened, just deallocate the backing object and
	 * return failure status so vm_object_collapse() will stop.  This will
	 * continue vm_object_deallocate() where it stopped due to our
	 * reference.
	 */
	if (backing_object->ref_count == 1)
		goto fail;
	vm_object_lock(object);

	/*
	 * Next, get rid of resident pages in the backing object.  We can
	 * guarantee to remove every page thus we can write the while-test like
	 * this.
	 */
	while ((backing_page = backing_object->memq.tqh_first) != NULL) {
		offset = backing_page->offset - backing_offset;

#ifdef DIAGNOSTIC
		if (backing_page->flags & (PG_BUSY | PG_FAKE))
			panic("vm_object_overlay: "
			    "busy or fake page in backing_object");
#endif

		/*
		 * If the page is outside the shadowing object's range or if
		 * the page is shadowed (either by a resident page or a paged
		 * out one) we can discard it right away.  Otherwise we need to
		 * move the page to the shadowing object.
		 */
		if (backing_page->offset < backing_offset || offset >= size ||
		    ((page = vm_page_lookup(object, offset)) != NULL) ||
		    (object->pager != NULL &&
		     vm_pager_has_page(object->pager, offset))) {
			/*
			 * Just discard the page, noone needs it.  This
			 * includes removing the possible backing store too.
			 */
			if (backing_object->pager != NULL)
				vm_object_remove_from_pager(backing_object,
				    backing_page->offset,
				    backing_page->offset + PAGE_SIZE);
			vm_page_lock_queues();
			vm_page_free(backing_page);
			vm_page_unlock_queues();
		} else {
			/*
			 * If the backing page was ever paged out, it was due
			 * to it being dirty at one point.  Unless we have no
			 * pager allocated to the front object (thus will move
			 * forward the shadow's one), mark it dirty again so it
			 * won't be thrown away without being paged out to the
			 * front pager.
			 *
			 * XXX
			 * Should be able to move a page from one pager to
			 * another.
			 */
			if (object->pager != NULL &&
			    vm_object_remove_from_pager(backing_object,
			    backing_page->offset,
			    backing_page->offset + PAGE_SIZE))
				backing_page->flags &= ~PG_CLEAN;

			/* Move the page up front.  */
			vm_page_rename(backing_page, object, offset);
		}
	}

	/*
	 * If the shadowing object doesn't have a pager the easiest
	 * thing to do now is to just move the backing pager up front
	 * and everything is done.  
	 */
	if (object->pager == NULL && backing_object->pager != NULL) {
		object->pager = backing_object->pager;
		object->paging_offset = backing_object->paging_offset +
		    backing_offset;
		backing_object->pager = NULL;
		goto done;
	}

	/*
	 * What's left to do is to find all paged out pages in the
	 * backing pager and either discard or move it to the front
	 * object.  We need to recheck the resident page set as a
	 * pagein might have given other threads the chance to, via
	 * readfaults, page in another page into the resident set.  In
	 * this case we need to retry getting rid of pages from core.
	 */
	paged_offset = 0;
	while (backing_object->pager != NULL &&
	    (paged_offset = vm_pager_next(backing_object->pager,
	    paged_offset)) < backing_object->size) {
		offset = paged_offset - backing_offset;

		/*
		 * If the parent object already has this page, delete it.
		 * Otherwise, start a pagein.
		 */
		if (((page = vm_page_lookup(object, offset)) == NULL) &&
		    (object->pager == NULL ||
		     !vm_pager_has_page(object->pager, offset))) {
			vm_object_unlock(object);

			/*
			 * First allocate a page and mark it busy so another
			 * thread won't try to start another pagein.
			 */
			backing_page = vm_page_alloc(backing_object,
			    paged_offset);
			if (backing_page == NULL) {
				vm_object_unlock(backing_object);
				vm_wait("fVmcollapse");
				vm_object_lock(backing_object);
				goto retry;
			}
			backing_page->flags |= PG_BUSY;

#ifdef DEBUG
			if (vmdebug & VMDEBUG_COLLAPSE_PAGEIN)
				printf("vm_object_overlay: pagein needed\n");
#endif

			/*
			 * Second, start paging it in.  If this fails,
			 * what can we do but punt?
			 */
			vm_object_paging_begin(backing_object);
			vm_object_unlock(backing_object);
			cnt.v_pageins++;
			rv = vm_pager_get_pages(backing_object->pager,
			    &backing_page, 1, TRUE);
			vm_object_lock(backing_object);
			vm_object_paging_end(backing_object);

			/*
			 * IO error or page outside the range of the pager:
			 * cleanup and return an error.
			 */
			if (rv == VM_PAGER_ERROR || rv == VM_PAGER_BAD) {
				FREE_PAGE(backing_page);
				goto fail;
			}

			/* Handle the remaining failures.  */
			if (rv != VM_PAGER_OK) {
#ifdef DIAGNOSTIC
				panic("vm_object_overlay: pager returned %d",
				    rv);
#else
				FREE_PAGE(backing_page);
				goto fail;
#endif
			}
			cnt.v_pgpgin++;

			/*
			 * Third, relookup in case pager changed page.  Pager
			 * is responsible for disposition of old page if moved.
			 */
			backing_page = vm_page_lookup(backing_object,
			    paged_offset);

			/*
			 * This page was once dirty, otherwise it
			 * hadn't been paged out in this shadow object.
			 * As we now remove the persistant store of the
			 * page, make sure it will be paged out in the
			 * front pager by dirtying it.
			 */
			backing_page->flags &= ~(PG_FAKE | PG_CLEAN);

			/*
			 * Fourth, restart the process as we have slept,
			 * thereby letting other threads change object's
			 * internal structure.  Don't be tempted to move it up
			 * front here, the parent may be gone already.
			 */
			PAGE_WAKEUP(backing_page);
			goto retry;
		}
		vm_object_remove_from_pager(backing_object, paged_offset, 
		    paged_offset + PAGE_SIZE);
		paged_offset += PAGE_SIZE;
	}

done:
	/*
	 * I've seen this condition once in an out of VM situation.  For the
	 * moment I don't know why it occurred, although I suspect
	 * vm_object_page_clean can create a pager even if it won't use it.
	 */
	if (backing_object->pager != NULL &&
	    vm_pager_count(backing_object->pager) == 0) {
		vm_pager_deallocate(backing_object->pager);
		backing_object->pager = NULL;
	}

#ifdef DIAGNOSTIC
	if (backing_object->pager)
		panic("vm_object_overlay: backing_object->pager remains");
#endif

	/*
	 * Object now shadows whatever backing_object did.
	 */
	if (backing_object->shadow)
		vm_object_lock(backing_object->shadow);
	vm_object_set_shadow(object, backing_object->shadow);
	if (backing_object->shadow)
		vm_object_unlock(backing_object->shadow);
	object->shadow_offset += backing_object->shadow_offset;
	if (object->shadow != NULL && object->shadow->copy != NULL)
		panic("vm_object_overlay: we collapsed a copy-object!");

#ifdef DIAGNOSTIC
	if (backing_object->ref_count != 1)
		panic("vm_object_overlay: backing_object still referenced");
#endif

	object_collapses++;
	return KERN_SUCCESS;

fail:
	backing_object->flags &= ~OBJ_FADING;
	return KERN_FAILURE;
}

/*
 * vm_object_bypass:
 *
 * Internal function to vm_object_collapse called when collapsing
 * the object with its backing one is not allowed but there may
 * be an opportunity to bypass the backing object and shadow the
 * next object in the chain instead.
 *
 * If all of the pages in the backing object are shadowed by the parent
 * object, the parent object no longer has to shadow the backing
 * object; it can shadow the next one in the chain.
 */
int
vm_object_bypass(object)
	vm_object_t	object;
{
	vm_object_t	backing_object = object->shadow;
	vm_offset_t	backing_offset = object->shadow_offset;
	vm_offset_t	offset, new_offset;
	vm_page_t	p, pp;

	/*
	 * XXX Punt if paging is going on.  The issues in this case need to be
	 * looked into more closely.  For now play it safe and return.  There's
	 * no need to wait for it to end, as the expense will be much higher
	 * than the gain.
	 */
	if (vm_object_paging(backing_object))
		return KERN_FAILURE;

	/*
	 * Should have a check for a 'small' number of pages here.
	 */
	for (p = backing_object->memq.tqh_first; p != NULL;
	    p = p->listq.tqe_next) {
		new_offset = p->offset - backing_offset;

		/*
		 * If the parent has a page here, or if this page falls outside
		 * the parent, keep going.
		 *
		 * Otherwise, the backing_object must be left in the chain.
		 */
		if (p->offset >= backing_offset && new_offset < object->size &&
		    ((pp = vm_page_lookup(object, new_offset)) == NULL ||
		    (pp->flags & PG_FAKE)) &&
		    (object->pager == NULL ||
		    !vm_pager_has_page(object->pager, new_offset)))
			/*
			 * Page still needed.  Can't go any further.
			 */
			return KERN_FAILURE;
	}

	if (backing_object->pager) {
		/*
		 * Should have a check for a 'small' number of pages here.
		 */
		for (offset = vm_pager_next(backing_object->pager, 0);
		    offset < backing_object->size;
		    offset = vm_pager_next(backing_object->pager,
		    offset + PAGE_SIZE)) {
			new_offset = offset - backing_offset;

			/*
			 * If the parent has a page here, or if this page falls
			 * outside the parent, keep going.
			 *
			 * Otherwise, the backing_object must be left in the
			 * chain.
			 */
			if (offset >= backing_offset &&
			    new_offset < object->size &&
			    ((pp = vm_page_lookup(object, new_offset)) ==
			    NULL || (pp->flags & PG_FAKE)) &&
			    (object->pager == NULL ||
			    !vm_pager_has_page(object->pager, new_offset)))
				/*
				 * Page still needed.  Can't go any further.
				 */
				return KERN_FAILURE;
		}
	}

	/*
	 * Object now shadows whatever backing_object did.
	 */
	if (backing_object->shadow)
		vm_object_lock(backing_object->shadow);
	vm_object_set_shadow(object, backing_object->shadow);
	if (backing_object->shadow)
		vm_object_unlock(backing_object->shadow);
	object->shadow_offset += backing_object->shadow_offset;

	/*
	 * Backing object might have had a copy pointer to us.  If it did,
	 * clear it. 
	 */
	if (backing_object->copy == object)
		backing_object->copy = NULL;

	object_bypasses++;
	return KERN_SUCCESS;
}

/*
 * vm_object_collapse:
 *
 * Collapse an object with the object backing it.  Pages in the backing object
 * are moved into the parent, and the backing object is deallocated.
 *
 * Requires that the object be locked and the page queues be unlocked.
 */
void
vm_object_collapse(object)
	vm_object_t object;

{
	vm_object_collapse_internal(object, NULL);
}

/*
 * An internal to vm_object.c entry point to the collapsing logic, used by
 * vm_object_deallocate to get rid of a potential recursion case.  In that case
 * an object to be deallocated is fed back via the retry_object pointer.
 * External users will have that parameter wired to NULL, and then we are
 * allowed to do vm_object_deallocate calls that may mutually recursive call us
 * again.  In that case it will only get one level deep and thus not be a real
 * recursion.
 */
void
vm_object_collapse_internal(object, retry_object)
	vm_object_t	object, *retry_object;
{
	register vm_object_t	backing_object;
	int			rv;

	/* We'd better initialize this one if the pointer is given.  */
	if (retry_object)
		*retry_object = NULL;

	if (!vm_object_collapse_allowed || object == NULL)
		return;

	do {
		/*
		 * Verify that the conditions are right for collapse:
		 *
		 * There is a backing object, and
		 */
		if ((backing_object = object->shadow) == NULL)
			return;

		vm_object_lock(backing_object);

		/*
		 * ... the backing object is not read_only, is internal and is
		 * not already being collapsed, ...
		 */
		if ((backing_object->flags & (OBJ_INTERNAL | OBJ_FADING)) !=
		    OBJ_INTERNAL) {
			vm_object_unlock(backing_object);
			return;
		}
	
		/*
		 * The backing object can't be a copy-object: the shadow_offset
		 * for the copy-object must stay as 0.  Furthermore (for the
		 * we have all the pages' case), if we bypass backing_object
		 * and just shadow the next object in the chain, old pages from
		 * that object would then have to be copied BOTH into the
		 *(former) backing_object and into the parent object.
		 */
		if (backing_object->shadow != NULL &&
		    backing_object->shadow->copy != NULL) {
			vm_object_unlock(backing_object);
			return;
		}

		/*
		 * Grab a reference to the backing object so that it
		 * can't be deallocated behind our back.
		 */
		backing_object->ref_count++;

#ifdef DIAGNOSTIC
		if (backing_object->ref_count == 1)
			panic("vm_object_collapse: "
			    "collapsing unreferenced object");
#endif

		/*
		 * If there is exactly one reference to the backing object, we
		 * can collapse it into the parent, otherwise we might be able
		 * to bypass it completely.
		 */
		rv = backing_object->ref_count == 2 ?
		    vm_object_overlay(object) : vm_object_bypass(object);

		/*
		 * Unlock and note we're ready with the backing object.  If
		 * we are now the last referrer this will also deallocate the
		 * object itself.  If the backing object has been orphaned
		 * and still have a shadow (it is possible in case of
		 * KERN_FAILURE from vm_object_overlay) this might lead to a
		 * recursion.  However, if we are called from
		 * vm_object_deallocate, retry_object is not NULL and we are
		 * allowed to feedback the current backing object via that
		 * pointer.  That way the recursion case turns into an
		 * iteration in vm_object_deallcate instead.
		 */
		if (retry_object != NULL && backing_object->ref_count == 1 &&
		    backing_object->shadow != NULL) {
			*retry_object = backing_object;
			vm_object_unlock(backing_object);
			return;
		}
		vm_object_unlock(backing_object);
		vm_object_deallocate(backing_object);

		/*
		 * Try again with this object's new backing object.
		 */
	} while (rv == KERN_SUCCESS);
}

/*
 * vm_object_page_remove: [internal]
 *
 * Removes all physical pages in the specified
 * object range from the object's list of pages.
 *
 * The object must be locked.
 */
void
vm_object_page_remove(object, start, end)
	register vm_object_t	object;
	register vm_offset_t	start;
	register vm_offset_t	end;
{
	register vm_page_t	p, next;

	if (object == NULL)
		return;

	for (p = object->memq.tqh_first; p != NULL; p = next) {
		next = p->listq.tqe_next;
		if ((start <= p->offset) && (p->offset < end)) {
			pmap_page_protect(VM_PAGE_TO_PHYS(p), VM_PROT_NONE);
			vm_page_lock_queues();
			vm_page_free(p);
			vm_page_unlock_queues();
		}
	}
}

/*
 * Routine:	vm_object_coalesce
 * Function:	Coalesces two objects backing up adjoining
 *		regions of memory into a single object.
 *
 * returns TRUE if objects were combined.
 *
 * NOTE: Only works at the moment if the second object is NULL -
 *	 if it's not, which object do we lock first?
 *
 * Parameters:
 *	prev_object	First object to coalesce
 *	prev_offset	Offset into prev_object
 *	next_object	Second object into coalesce
 *	next_offset	Offset into next_object
 *
 *	prev_size	Size of reference to prev_object
 *	next_size	Size of reference to next_object
 *
 * Conditions:
 * The object must *not* be locked.
 */
boolean_t
vm_object_coalesce(prev_object, next_object, prev_offset, next_offset,
		   prev_size, next_size)
	register vm_object_t	prev_object;
	vm_object_t	next_object;
	vm_offset_t	prev_offset, next_offset;
	vm_size_t	prev_size, next_size;
{
	vm_size_t	newsize;

#ifdef	lint
	next_offset++;
#endif

	if (next_object != NULL) {
		return(FALSE);
	}

	if (prev_object == NULL) {
		return(TRUE);
	}

	vm_object_lock(prev_object);

	/*
	 * Try to collapse the object first
	 */
	vm_object_collapse(prev_object);

	/*
	 * Can't coalesce if:
	 * . more than one reference
	 * . paged out
	 * . shadows another object
	 * . has a copy elsewhere
	 * (any of which mean that the pages not mapped to
	 * prev_entry may be in use anyway)
	 */

	if (prev_object->ref_count > 1 ||  prev_object->pager != NULL ||
	    prev_object->shadow != NULL || prev_object->copy != NULL) {
		vm_object_unlock(prev_object);
		return(FALSE);
	}

	/*
	 * Remove any pages that may still be in the object from
	 * a previous deallocation.
	 */
	vm_object_page_remove(prev_object, prev_offset + prev_size,
	    prev_offset + prev_size + next_size);

	/*
	 * Extend the object if necessary.
	 */
	newsize = prev_offset + prev_size + next_size;
	if (newsize > prev_object->size)
		prev_object->size = newsize;

	vm_object_unlock(prev_object);
	return(TRUE);
}

/*
 * vm_object_print:	[ debug ]
 */
void
vm_object_print(object, full)
	vm_object_t	object;
	boolean_t	full;
{
	_vm_object_print(object, full, printf);
}

void
_vm_object_print(object, full, pr)
	vm_object_t	object;
	boolean_t	full;
	int		(*pr) __P((const char *, ...));
{
	register vm_page_t	p;
	char			*delim;
	vm_object_t		o;
	register int		count;
	extern int		indent;

	if (object == NULL)
		return;

	iprintf(pr, "Object 0x%p: size=0x%lx, res=%d, ref=%d, ", object,
	    (long)object->size, object->resident_page_count,
	    object->ref_count);
	(*pr)("pager=%p+0x%lx, shadow=(%p)+0x%lx\n", object->pager,
	    (long)object->paging_offset, object->shadow,
	    (long)object->shadow_offset);
	(*pr)("shadowers=(");
	delim = "";
	for (o = object->shadowers.lh_first; o;
	    o = o->shadowers_list.le_next) {
		(*pr)("%s0x%p", delim, o);
		delim = ", ";
	};
	(*pr)(")\n");
	(*pr)("cache: next=0x%p, prev=0x%p\n", object->cached_list.tqe_next,
	    object->cached_list.tqe_prev);

	if (!full)
		return;

	indent += 2;
	count = 0;
	for (p = object->memq.tqh_first; p != NULL; p = p->listq.tqe_next) {
		if (count == 0)
			iprintf(pr, "memory:=");
		else if (count == 6) {
			(*pr)("\n");
			iprintf(pr, " ...");
			count = 0;
		} else
			(*pr)(",");
		count++;

		(*pr)("(off=0x%lx,page=0x%lx)", (long)p->offset,
		    (long)VM_PAGE_TO_PHYS(p));
	}
	if (count != 0)
		(*pr)("\n");
	indent -= 2;
}

/*
 * vm_object_set_shadow:
 *
 * Maintain the shadow graph so that back-link consistency is always kept.
 *
 * Assumes both objects as well as the old shadow to be locked (unless NULL
 * of course).
 */
void
vm_object_set_shadow(object, shadow)
	vm_object_t	object, shadow;
{
	vm_object_t	old_shadow = object->shadow;

#ifdef DEBUG
	if (vmdebug & VMDEBUG_SHADOW)
		printf("vm_object_set_shadow(object=0x%p, shadow=0x%p) "
		    "old_shadow=0x%p\n", object, shadow, old_shadow);
	if (vmdebug & VMDEBUG_SHADOW_VERBOSE) {
		vm_object_print(object, 0);
		vm_object_print(old_shadow, 0);
		vm_object_print(shadow, 0);
	}
#endif
	if (old_shadow == shadow)
		return;
	if (old_shadow) {
		old_shadow->ref_count--;
		LIST_REMOVE(object, shadowers_list);
	}
	if (shadow) {
		shadow->ref_count++;
		LIST_INSERT_HEAD(&shadow->shadowers, object, shadowers_list);
	}
	object->shadow = shadow;
#ifdef DEBUG
	if (vmdebug & VMDEBUG_SHADOW_VERBOSE) {
		vm_object_print(object, 0);
		vm_object_print(old_shadow, 0);
		vm_object_print(shadow, 0);
	}
#endif
}
@


1.22
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.21 1998/03/01 00:38:15 niklas Exp $	*/
@


1.22.8.1
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.22 1998/04/25 07:17:21 niklas Exp $	*/
@


1.21
log
@Merge of MACHINE_NEW_CONTIG (aka MNN) code from Chuck Cranor,
<chuck@@openbsd.org>. This code is as of yet disabled on all platforms,
actually not yet supported on more than mvme68k, although other
platforms are expected soon, as code is already available.
This code makes handling of multiple physical memory regions
consistent over all platforms, as well as keeping the performance of
maintaining a single continuous memory chunk.  It is also a
requirement for the upcoming UVM replacement VM system.

What I did in this merge: just declared the pmap_map function in a
MD include file per port that needs it.  It's not an exported pmap
interface, says Chuck.  It ended up in differnt include files on
differnet ports, as I tried to follow the current policy on a per-arch
basis.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.20 1997/11/06 05:59:35 csapuntz Exp $	*/
d626 1
a626 1
			panic("vm_object_deactivate: I'm sooo confused.");
@


1.20
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.19 1997/10/06 20:21:23 deraadt Exp $	*/
d394 1
a394 1
	vm_object_paging_wait(object);
d510 1
a510 1
	vm_object_paging_wait(object);
d1204 1
a1204 1
	vm_object_paging_wait(backing_object);
d1321 1
a1321 1
				VM_WAIT;
@


1.19
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.17 1997/07/25 06:03:09 mickey Exp $	*/
d69 1
a69 1
 *	@@(#)vm_object.c	8.5 (Berkeley) 3/22/94
d419 1
a419 1
	if ((object->flags & OBJ_INTERNAL) == 0) {
a420 2
		vm_object_unlock(object);
	}
@


1.18
log
@VFS Lite2 Changes
@
text
@d69 1
a69 1
 *	@@(#)vm_object.c	8.7 (Berkeley) 5/11/95
d419 1
a419 1
	if ((object->flags & OBJ_INTERNAL) == 0)
d421 2
@


1.17
log
@tabify
some /lx/p/ printf changes
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.16 1997/05/28 22:40:04 mickey Exp $	*/
d69 1
a69 1
 *	@@(#)vm_object.c	8.5 (Berkeley) 3/22/94
d419 1
a419 1
	if ((object->flags & OBJ_INTERNAL) == 0) {
a420 2
		vm_object_unlock(object);
	}
@


1.16
log
@move vm_cache_max into param.c
make it maxusers adaptive
note, that NTEXT approximation is probably not the best idea
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.15 1997/04/17 01:25:20 niklas Exp $	*/
d1719 1
a1719 1
    prev_size, next_size)
d1788 1
a1788 1
        _vm_object_print(object, full, printf);
d1809 1
a1809 1
	(*pr)("pager=0x%p+0x%lx, shadow=(0x%p)+0x%lx\n", object->pager,
@


1.15
log
@Removal of race conditions.  Inspired by Charles Hannum's
<mycroft@@netbsd.org> reorganization of the vm_collapse logic, although not
used verbatim.  We no longer collapse objects from the pagedaemon as that
is not necessary anymore with the more aggressive collapses that gets done.
This also increases performance of loaded systems.  Much KNF too.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.14 1997/03/26 18:45:31 niklas Exp $	*/
d142 1
a142 1
int	vm_cache_max = 100;	/* can patch if necessary */
@


1.14
log
@Just some initial KNF in preparation for some other changes coming soon
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_object.c,v 1.13 1996/12/24 20:14:28 niklas Exp $	*/
/*	$NetBSD: vm_object.c,v 1.34 1996/02/28 22:35:35 gwr Exp $	*/
d4 31
a34 1
/* 
a161 1
int		vm_object_collapse_aux __P((vm_object_t));
d163 2
a164 1
void		vm_object_set_shadow __P((vm_object_t, vm_object_t));
d167 1
d207 2
a208 2
	result = (vm_object_t)malloc((u_long)sizeof *result,
	    M_VMOBJ, M_WAITOK);
d276 1
a276 1
	register vm_object_t	object;
d288 2
a289 3
		 * The cache holds a reference (uncounted) to
		 * the object; we must lock it before removing
		 * the object.
d299 3
d303 5
a307 5
			 * If this is a deallocation of a shadow
			 * reference (which it is unless it's the
			 * first time round) and this operation made
			 * us singly-shadowed, try to collapse us
			 * with our shadower.
a308 1
			vm_object_unlock(object);
a312 3
				vm_object_collapse(temp);
				vm_object_unlock(temp);
			}
d314 21
a334 6
			/*
			 * If there are still references, then
			 * we are done.
			 */
			vm_object_cache_unlock();
			return;
d338 2
a339 3
		 * See if this object can persist.  If so, enter
		 * it in the cache, then deactivate all of its
		 * pages.
d361 3
d365 1
a366 1
			/* unlocks and deallocates object */
d386 1
a386 2
	 * Setters of paging_in_progress might be interested that this object
	 * is going away as soon as we get a grip on it.
d394 1
a394 4
	while (object->paging_in_progress) {
		vm_object_sleep(object, object, FALSE);
		vm_object_lock(object);
	}
d407 2
a408 1
			panic("vm_object_terminate: copy/shadow inconsistency");
d512 2
a513 4
	while (object->paging_in_progress) {
		vm_object_sleep(object, object, FALSE);
		vm_object_lock(object);
	}
d559 1
a559 6
#ifdef DIAGNOSTIC
				if (object->paging_in_progress == 0xdead)
					panic("vm_object_page_clean: "
					    "object deallocated");
#endif
				object->paging_in_progress++;
d568 1
a568 1
					       "vm_object_page_clean");
d573 1
a573 1
				object->paging_in_progress--;
d742 1
a742 1
		 * Make another reference to the object
d790 1
a790 1
			/* should spin a bit here... */
a853 1
		src_object->ref_count--;	/* remove ref. from old_copy */
d855 1
a855 3
		new_copy->ref_count++;		/* locking not needed - we
						   have the only pointer */
		vm_object_unlock(old_copy);	/* done with old_copy */
d858 3
a860 2
	new_start = (vm_offset_t)0;	/* always shadow original at 0 */
	new_end   = (vm_offset_t)new_copy->size; /* for the whole object */
a867 1
	src_object->ref_count++;
d908 5
d920 3
a922 5
	 * The new object shadows the source object, adding
	 * a reference to it.  Our caller changes his reference
	 * to point to the new object, removing a reference to
	 * the source object.  Net result: no change of reference
	 * count.
d926 1
a1069 1
 *
d1118 1
a1118 1
	/* If pager became empty, remove it.	*/
d1126 7
d1134 1
a1134 1
 * vm_object_collapse_aux:
d1142 1
a1142 1
vm_object_collapse_aux(object)
d1150 1
d1154 1
a1154 1
		printf("vm_object_collapse_aux(0x%x)\n", object);
d1158 5
d1164 4
a1167 5
	 * (1)	Trim a potential pager in the backing
	 * 	object so it'll only hold pages in reach.
	 * (2)	Loop over all the resident pages in the
	 * 	shadow object and either remove them if
	 * 	they are shadowed or move them into the
d1169 6
a1174 9
	 * (3)	Loop over the paged out pages in the
	 * 	shadow object.  Start pageins on those
	 * 	that aren't shadowed, and just deallocate
	 * 	the others.  In each iteration check if
	 * 	other users of these objects have caused
	 * 	pageins resulting in new resident pages.
	 * 	This can happen while we are waiting for
	 * 	a pagein of ours.  If such resident pages
	 * 	turn up, restart from (2).
d1178 2
a1179 3
	 * As a first measure we know we can discard
	 * everything that the shadowing object doesn't
	 * shadow.
d1191 11
a1201 2
	 * This is the outer loop, iterating until all resident and
	 * paged out pages in the shadow object are drained.
d1203 31
a1233 3
	paged_offset = 0;
	while (backing_object->memq.tqh_first != NULL ||
	    backing_object->pager != NULL) {
d1235 9
a1243 7
		 * First of all get rid of resident pages in the
		 * backing object.  We can guarantee to remove
		 * every page thus we can write the while-test
		 * like this.
		 */
		while ((backing_page = backing_object->memq.tqh_first) !=
		    NULL) {
d1245 2
a1246 7
			 * If the page is outside the shadowing object's
			 * range or if the page is shadowed (either by a
			 * resident "non-fake" page or a paged out one) we
			 * can discard it right away.  Otherwise we need
			 * to move the page to the shadowing object,
			 * perhaps waking up waiters for "fake" pages
			 * first.
d1248 2
a1249 39
			if (backing_page->offset < backing_offset ||
			    (offset = backing_page->offset - backing_offset) >=
			    size ||
			    ((page = vm_page_lookup(object, offset)) != NULL &&
			     !(page->flags & PG_FAKE)) ||
			    (object->pager != NULL &&
			    vm_pager_has_page(object->pager, offset))) {

				/*
				 * Just discard the page, noone needs it.
				 */
				vm_page_lock_queues();
				vm_page_free(backing_page);
				vm_page_unlock_queues();
			} else {
				/*
				 * If a "fake" page was found, someone may
				 * be waiting for it.  Wake her up and
				 * then remove the page.
				 */
				if (page) {
					PAGE_WAKEUP(page);
					vm_page_lock_queues();
					vm_page_free(page);
					vm_page_unlock_queues();
				}

				/*
				 * If the backing page was ever paged out,
				 * it was due to it being dirty at one
				 * point.  Unless we have no pager
				 * allocated to the front object (thus
				 * will move forward the shadow's one),
				 * mark it dirty again so it won't be
				 * thrown away without being paged out to
				 * the front pager.
				 */
				if (object->pager != NULL &&
				    vm_object_remove_from_pager(backing_object,
d1251 22
a1272 2
				    backing_page->offset + PAGE_SIZE))
					backing_page->flags &= ~PG_CLEAN;
d1274 2
a1275 3
				/* Move the page up front.	*/
				vm_page_rename(backing_page, object, offset);
			}
d1277 1
d1279 26
a1304 6
		/*
		 * If there isn't a pager in the shadow object, we're
		 * ready.  Take the easy way out.
		 */
		if (backing_object->pager == NULL)
			break;
d1307 7
a1313 11
		 * If the shadowing object doesn't have a pager
		 * the easiest thing to do now is to just move the
		 * backing pager up front and everything is done.  
		 */
		if (object->pager == NULL) {
			object->pager = backing_object->pager;
			object->paging_offset = backing_object->paging_offset +
			    backing_offset;
			backing_object->pager = NULL;
			break;
		}
a1314 16
		/*
		 * What's left to do is to find all paged out
		 * pages in the backing pager and either discard
		 * or move it to the front object.  We need to
		 * recheck the resident page set as a pagein might
		 * have given other threads the chance to, via
		 * readfaults, page in another page into the
		 * resident set.  In this case the outer loop must
		 * get reentered.  That is also the case if some other
		 * thread removes the front pager, a case that has
		 * been seen...
		 */
		while (backing_object->memq.tqh_first == NULL &&
		    backing_object->pager != NULL && object->pager != NULL &&
		    (paged_offset = vm_pager_next(backing_object->pager,
		     paged_offset)) < backing_object->size) {
d1316 2
a1317 7
			 * If the shadowing object has this page, get
			 * rid of it from the backing pager.  Trust
			 * the loop condition to get us out of here
			 * quickly if we remove the last paged out page.
			 *
			 * XXX Would clustering several pages at a time
			 * be a win in this situation?
d1319 10
a1328 20
			if (((page = vm_page_lookup(object,
			    paged_offset - backing_offset)) == NULL ||
			    (page->flags & PG_FAKE)) &&
			    !vm_pager_has_page(object->pager,
			    paged_offset - backing_offset)) {
				/*
				 * If a "fake" page was found, someone
				 * may be waiting for it.  Wake her up
				 * and then remove the page.
				 */
				if (page) {
					PAGE_WAKEUP(page);
					vm_page_lock_queues();
					vm_page_free(page);
					vm_page_unlock_queues();
				}
				/*
				 * Suck the page from the pager and give
				 * it to the shadowing object.
				 */
d1330 2
a1331 3
				if (vmdebug & VMDEBUG_COLLAPSE_PAGEIN)
					printf("vm_object_collapse_aux: "
					    "pagein needed\n");
d1334 20
a1353 14
				/*
				 * First allocate a page and mark it
				 * busy so another thread won't try
				 * to start another pagein.
				 */
				for (;;) {
					backing_page =
					    vm_page_alloc(backing_object,
					    paged_offset);
					if (backing_page)
						break;
					VM_WAIT;
				}
				backing_page->flags |= PG_BUSY;
d1355 2
a1356 12
				/*
				 * Second, start paging it in.  If this
				 * fails, what can we do but punt?
				 * Even though the shadowing object
				 * isn't exactly paging we say so in
				 * order to not get simultaneous
				 * cascaded collapses.
				 */
				object->paging_in_progress++;
				backing_object->paging_in_progress++;
				if (vm_pager_get_pages(backing_object->pager,
				    &backing_page, 1, TRUE) != VM_PAGER_OK) {
d1358 5
a1362 2
					panic("vm_object_collapse_aux: "
					    "could not get paged out page");
d1364 2
a1365 3
					return KERN_FAILURE;
				}
				cnt.v_pgpgin++;
d1367 6
a1372 23
				/*
				 * A fault might have issued other
				 * pagein operations.  We must wait for
				 * them to complete, then we get to
				 * wakeup potential other waiters as
				 * well.
				 */
				while (backing_object->paging_in_progress != 1
				    || object->paging_in_progress != 1) {
					if (object->paging_in_progress != 1) {
						vm_object_sleep(object, object,
						    FALSE);
						vm_object_lock(object);
						continue;
					}
					vm_object_sleep(backing_object,
					    backing_object, FALSE);
					vm_object_lock(backing_object);
				}
				backing_object->paging_in_progress--;
				object->paging_in_progress--;
				thread_wakeup(backing_object);
				thread_wakeup(object);
d1374 8
a1381 11
				/*
				 * During the pagein vm_object_terminate
				 * might have slept on our front object in
				 * order to remove it.  If this is the
				 * case, we might as well stop all the
				 * collapse work right here.
				 */
				if (object->flags & OBJ_FADING) {
					PAGE_WAKEUP(backing_page);
					return KERN_FAILURE;
				}
d1383 8
a1390 30
				/*
				 * Third, relookup in case pager changed
				 * page.  Pager is responsible for
				 * disposition of old page if moved.
				 */
				backing_page = vm_page_lookup(backing_object,
				    paged_offset);

				/*
				 * This page was once dirty, otherwise
				 * it hadn't been paged out in this
				 * shadow object.  As we now remove the
				 * persistant store of the page, make
				 * sure it will be paged out in the
				 * front pager by dirtying it.
				 */
				backing_page->flags &= ~(PG_FAKE|PG_CLEAN);

				/*
				 * Fourth, move it up front, and wake up
				 * potential waiters.
				 */
				vm_page_rename(backing_page, object,
				    paged_offset - backing_offset);
				PAGE_WAKEUP(backing_page);

			}
			vm_object_remove_from_pager(backing_object,
			    paged_offset, paged_offset + PAGE_SIZE);
			paged_offset += PAGE_SIZE;
d1392 3
d1397 1
d1399 3
a1401 4
	 * I've seen this condition once in an out of VM situation.
	 * For the moment I don't know why it occurred, although I suspect
	 * vm_object_page_clean can create a pager even if it won't use
	 * it.
d1411 1
a1411 1
		panic("vm_object_collapse_aux: backing_object->pager remains");
a1415 2
	 * Note that the reference to backing_object->shadow
	 * moves from within backing_object to within object.
d1417 1
a1417 1
	if(backing_object->shadow)
d1420 1
a1420 2
	if(backing_object->shadow) {
		vm_object_set_shadow(backing_object, NULL);
a1421 1
	}
d1424 1
a1424 1
		panic("vm_object_collapse_aux: we collapsed a copy-object!");
d1426 4
a1429 9
	/* Fast cleanup is the only thing left now.	*/
	vm_object_unlock(backing_object);

	simple_lock(&vm_object_list_lock);
	TAILQ_REMOVE(&vm_object_list, backing_object, object_list);
	vm_object_count--;
	simple_unlock(&vm_object_list_lock);

	free((caddr_t)backing_object, M_VMOBJ);
d1433 4
d1446 4
d1455 4
a1458 4
	register vm_object_t	backing_object = object->shadow;
	register vm_offset_t	backing_offset = object->shadow_offset;
	register vm_offset_t	new_offset;
	register vm_page_t	p, pp;
d1461 4
a1464 9
	 * If all of the pages in the backing object are
	 * shadowed by the parent object, the parent
	 * object no longer has to shadow the backing
	 * object; it can shadow the next one in the
	 * chain.
	 *
	 * The backing object must not be paged out - we'd
	 * have to check all of the paged-out pages, as
	 * well.
d1466 1
a1466 2

	if (backing_object->pager != NULL)
d1470 1
a1470 2
	 * Should have a check for a 'small' number
	 * of pages here.
a1471 1

d1477 2
a1478 3
		 * If the parent has a page here, or if
		 * this page falls outside the parent,
		 * keep going.
d1480 1
a1480 2
		 * Otherwise, the backing_object must be
		 * left in the chain.
a1481 1

d1484 3
a1486 1
		    (pp->flags & PG_FAKE))) {
d1491 29
d1524 1
a1524 4
	 * Make the parent shadow the next object
	 * in the chain.  Deallocating backing_object
	 * will not remove it, since its reference
	 * count is at least 2.
a1525 2

	vm_object_lock(object->shadow);
a1530 2
	vm_object_reference(object->shadow);
	vm_object_unlock(object->shadow);
d1534 2
a1535 2
	 * Backing object might have had a copy pointer
	 * to us.  If it did, clear it. 
a1536 1

a1539 7
	/* Drop the reference count on backing_object.
	 * Since its ref_count was at least 2, it
	 * will not vanish; so we don't need to call
	 * vm_object_deallocate.
	 */
	backing_object->ref_count--;
	vm_object_unlock(backing_object);
d1547 2
a1548 6
 * Collapse an object with the object backing it.
 * Pages in the backing object are moved into the
 * parent, and the backing object is deallocated.
 *
 * Requires that the object be locked and the page
 * queues be unlocked.
d1550 1
d1554 5
a1558 1
	register vm_object_t	object;
d1560 12
d1574 5
d1580 1
a1580 1
	if (!vm_object_collapse_allowed)
d1583 1
a1583 1
	while (TRUE) {
d1587 1
a1587 2
		 * The object exists and no pages in it are currently
		 * being paged out.
d1589 1
a1589 1
		if (object == NULL || object->paging_in_progress)
a1591 7
		/*
		 * 	There is a backing object, and
		 */
	
		if ((backing_object = object->shadow) == NULL)
			return;
	
d1593 1
d1595 2
a1596 5
		 * ...
		 * 	The backing object is not read_only,
		 * 	and no pages in the backing object are
		 * 	currently being paged out.
		 * 	The backing object is internal.
d1598 2
a1599 3
	
		if ((backing_object->flags & OBJ_INTERNAL) == 0 ||
		    backing_object->paging_in_progress != 0) {
d1605 6
a1610 8
		 * The backing object can't be a copy-object:
		 * the shadow_offset for the copy-object must stay
		 * as 0.  Furthermore (for the 'we have all the
		 * pages' case), if we bypass backing_object and
		 * just shadow the next object in the chain, old
		 * pages from that object would then have to be copied
		 * BOTH into the (former) backing_object and into the
		 * parent object.
d1619 2
a1620 3
		 * If there is exactly one reference to the backing
		 * object, we can collapse it into the parent,
		 * otherwise we might be able to bypass it completely.
d1622 36
a1657 11
	
		if (backing_object->ref_count == 1) {
			if (vm_object_collapse_aux(object) != KERN_SUCCESS) {
				vm_object_unlock(backing_object);
				return;
			}
		} else
			if (vm_object_bypass(object) != KERN_SUCCESS) {
				vm_object_unlock(backing_object);
				return;
			}
d1662 1
a1662 1
	}
d1698 1
a1698 1
 * 		regions of memory into a single object.
d1702 2
a1703 2
 * NOTE:	Only works at the moment if the second object is NULL -
 * 	if it's not, which object do we lock first?
d1706 4
a1709 4
 * 	prev_object	First object to coalesce
 * 	prev_offset	Offset into prev_object
 * 	next_object	Second object into coalesce
 * 	next_offset	Offset into next_object
d1711 2
a1712 2
 * 	prev_size	Size of reference to prev_object
 * 	next_size	Size of reference to next_object
d1718 2
a1719 4
vm_object_coalesce(prev_object, next_object,
			prev_offset, next_offset,
			prev_size, next_size)

d1756 1
a1756 1
	if (prev_object->ref_count > 1 || prev_object->pager != NULL ||
a1765 1

d1806 6
a1811 6
	iprintf(pr, "Object 0x%lx: size=0x%lx, res=%d, ref=%d, ",
		(long)object, (long)object->size,
		object->resident_page_count, object->ref_count);
	(*pr)("pager=0x%lx+0x%lx, shadow=(0x%lx)+0x%lx\n",
	       (long)object->pager, (long)object->paging_offset,
	       (long)object->shadow, (long)object->shadow_offset);
d1816 1
a1816 1
		(*pr)("%s0x%x", delim, o);
d1820 2
a1821 3
	(*pr)("cache: next=0x%lx, prev=0x%lx\n",
	       (long)object->cached_list.tqe_next,
	       (long)object->cached_list.tqe_prev);
d1839 2
a1840 1
		(*pr)("(off=0x%x,page=0x%x)", p->offset, VM_PAGE_TO_PHYS(p));
d1850 1
a1850 2
 * Maintain the shadow graph so that back-link consistency is
 * always kept.
d1852 2
a1853 2
 * Assumes both objects as well as the old shadow to be locked
 * (unless NULL of course).
d1863 2
a1864 2
		printf("vm_object_set_shadow(object=0x%x, shadow=0x%x) "
		    "old_shadow=0x%x\n", object, shadow, old_shadow);
d1874 1
d1878 1
@


1.13
log
@Make termination of objects wait for possible collapses before altering
the object's shadow linkage.  Also note that the object is going to die
so that a possible collapse can finish early.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.12 1996/11/06 23:24:40 niklas Exp $	*/
d82 22
a103 22
 *	Virtual memory objects maintain the actual data
 *	associated with allocated virtual memory.  A given
 *	page of memory exists within exactly one object.
 *
 *	An object is only deallocated when all "references"
 *	are given up.  Only one "reference" to a given
 *	region of an object should be writeable.
 *
 *	Associated with each object is a list of all resident
 *	memory pages belonging to that object; this list is
 *	maintained by the "vm_page" module, and locked by the object's
 *	lock.
 *
 *	Each object also records a "pager" routine which is
 *	used to retrieve (and store) pages to the proper backing
 *	storage.  In addition, objects may be backed by other
 *	objects from which they were virtual-copied.
 *
 *	The only items within the object structure which are
 *	modified after time of creation are:
 *		reference count		locked by object's lock
 *		pager routine		locked by object's lock
d131 1
a131 1
static void	_vm_object_allocate __P((vm_size_t, vm_object_t));
d139 1
a139 1
 *	vm_object_init:
d141 1
a141 1
 *	Initialize the VM objects module.
d166 1
a166 1
 *	vm_object_allocate:
d168 1
a168 1
 *	Returns a new object with the given size.
d176 2
a177 2
	result = (vm_object_t)
		malloc((u_long)sizeof *result, M_VMOBJ, M_WAITOK);
d184 1
a184 1
static void
d216 1
a216 1
 *	vm_object_reference:
d218 1
a218 1
 *	Gets another reference to the given object.
d233 1
a233 1
 *	vm_object_deallocate:
d235 5
a239 5
 *	Release a reference to the specified object,
 *	gained either through a vm_object_allocate
 *	or a vm_object_reference call.  When all references
 *	are gone, storage associated with this object
 *	may be relinquished.
d241 1
a241 1
 *	No object may be locked.
d248 3
a250 3
	 *	While "temp" is used for other things as well, we
	 *	initialize it to NULL here for being able to check
	 *	if we are in the first revolution of the loop.
d257 3
a259 3
		 *	The cache holds a reference (uncounted) to
		 *	the object; we must lock it before removing
		 *	the object.
d265 1
a265 1
		 *	Lose the reference
d270 5
a274 5
			 *	If this is a deallocation of a shadow
			 *	reference (which it is unless it's the
			 *	first time round) and this operation made
			 *	us singly-shadowed, try to collapse us
			 *	with our shadower.
d286 2
a287 2
			 *	If there are still references, then
			 *	we are done.
d294 3
a296 3
		 *	See if this object can persist.  If so, enter
		 *	it in the cache, then deactivate all of its
		 *	pages.
a297 1

d301 1
a301 1
				cached_list);
d313 1
a313 1
		 *	Make sure no one can look us up now.
d327 2
a328 2
 *	vm_object_terminate actually destroys the specified object, freeing
 *	up all previously used resources.
d330 1
a330 1
 *	The object must be locked.
d355 2
a356 2
	 *	Detach the object from its shadow if we are the shadow's
	 *	copy.
d413 1
a413 1
 *	vm_object_page_clean
d415 5
a419 5
 *	Clean all dirty pages in the specified range of object.
 *	If syncio is TRUE, page cleaning is done synchronously.
 *	If de_queue is TRUE, pages are removed from any paging queue
 *	they were on, otherwise they are left on whatever queue they
 *	were on before the cleaning operation began.
d421 1
a421 1
 *	Odd semantics: if start == end, we clean everything.
d423 1
a423 1
 *	The object must be locked.
d425 3
a427 3
 *	Returns TRUE if all was well, FALSE if there was a pager error
 *	somewhere.  We attempt to clean (and dequeue) all pages regardless
 *	of where an error occurs.
d456 1
a456 2
						  object->size, VM_PROT_ALL,
						  (vm_offset_t)0);
d556 1
a556 1
 *	vm_object_deactivate_pages
d558 2
a559 2
 *	Deactivate all pages in the specified object.  (Keep its pages
 *	in memory even though it is no longer referenced.)
d561 1
a561 1
 *	The object must be locked.
d579 1
a579 1
 *	Trim the object cache to size.
d602 1
a602 1
 *	vm_object_pmap_copy:
d604 3
a606 3
 *	Makes all physical pages in the specified
 *	object range copy-on-write.  No writeable
 *	references to these pages should remain.
d608 1
a608 1
 *	The object must *not* be locked.
d632 1
a632 1
 *	vm_object_pmap_remove:
d634 2
a635 2
 *	Removes all physical pages in the specified
 *	object range from all physical maps.
d637 1
a637 1
 *	The object must *not* be locked.
d658 1
a658 1
 *	vm_object_copy:
d660 4
a663 4
 *	Create a new object which is a copy of an existing
 *	object, and mark all of the pages in the existing
 *	object 'copy-on-write'.  The new object has one reference.
 *	Returns the new object.
d665 2
a666 2
 *	May defer the copy until later if the object is not backed
 *	up by a non-default pager.
d686 1
a686 1
		 *	Nothing to copy
d695 4
a698 4
	 *	If the object's pager is null_pager or the
	 *	default pager, we don't have to make a copy
	 *	of it.  Instead, we set the needs copy flag and
	 *	make a shadow later.
d706 1
a706 1
		 *	Make another reference to the object
d711 1
a711 1
		 *	Mark all of the pages copy-on-write.
d723 1
a723 1
		 *	Must make a shadow when write is desired
d730 1
a730 1
	 *	Try to collapse the object before copying it.
d735 3
a737 3
	 *	If the object has a pager, the pager wants to
	 *	see all of the changes.  We need a copy-object
	 *	for the changed pages.
d739 4
a742 4
	 *	If there is a copy-object, and it is empty,
	 *	no changes have been made to the object since the
	 *	copy-object was made.  We can use the same copy-
	 *	object.
d745 1
a745 1
    Retry1:
d749 1
a749 1
		 *	Try to get the locks (out of order)
d762 2
a763 2
			 *	Return another reference to
			 *	the existing copy-object.
d778 3
a780 3
	 *	If the object has a pager, the pager wants
	 *	to see all of the changes.  We must make
	 *	a copy-object and put the changed pages there.
d782 4
a785 4
	 *	The copy-object is always made large enough to
	 *	completely shadow the original object, since
	 *	it may have several users who want to shadow
	 *	the original object at different points.
d790 1
a790 1
    Retry2:
d793 1
a793 1
	 *	Copy object may have changed while we were unlocked
d798 1
a798 1
		 *	Try to get the locks (out of order)
d806 1
a806 1
		 *	Consistency check
d813 4
a816 4
		 *	Make the old copy-object shadow the new one.
		 *	It will receive no more pages from the original
		 *	object.  Locking of new_copy not needed.  We
		 *	have the only pointer.
d825 2
a826 2
	new_start = (vm_offset_t) 0;	/* always shadow original at 0 */
	new_end   = (vm_offset_t) new_copy->size; /* for the whole object */
d829 1
a829 1
	 *	Point the new copy at the existing object.
d838 2
a839 2
	 *	Mark all the affected pages of the existing object
	 *	copy-on-write.
d853 1
a853 1
 *	vm_object_shadow:
d855 3
a857 3
 *	Create a new object which is backed by the
 *	specified existing object range.  The source
 *	object reference is deallocated.
d859 2
a860 2
 *	The new object and offset into that object
 *	are returned in the source parameters.
d862 1
a862 1
 *	The old object should not be locked.
d876 1
a876 1
	 *	Allocate a new object with the given length
d882 5
a886 5
	 *	The new object shadows the source object, adding
	 *	a reference to it.  Our caller changes his reference
	 *	to point to the new object, removing a reference to
	 *	the source object.  Net result: no change of reference
	 *	count.
d893 2
a894 2
	 *	Store the offset into the source object,
	 *	and fix up the offset into the new object.
d899 1
a899 1
	 *	Return the new things
d906 1
a906 1
 *	Set the specified object's pager to the specified pager.
d926 1
a926 1
 *	vm_object_hash hashes the pager/id pair.
d933 2
a934 2
 *	vm_object_lookup looks in the object cache for an object with the
 *	specified pager and paging id.
d968 2
a969 2
 *	vm_object_enter enters the specified object/pager/id into
 *	the hash table.
d981 2
a982 2
	 *	We don't cache null objects, and we can't cache
	 *	objects with the null pager.
d1002 1
a1002 1
 *	vm_object_remove:
d1004 4
a1007 4
 *	Remove the pager from the hash table.
 *	Note:  This assumes that the object cache
 *	is locked.  XXX this should be fixed
 *	by reorganizing vm_object_deallocate.
d1032 1
a1032 1
 *	vm_object_cache_clear removes all objects from the cache.
d1041 2
a1042 2
	 *	Remove each object in the cache by scanning down the
	 *	list of cached objects.
d1064 1
a1064 1
 *	vm_object_remove_from_pager:
d1066 2
a1067 2
 *	Tell object's pager that it needn't back the page
 *	anymore.  If the pager ends up empty, deallocate it.
d1082 1
a1082 1
	/*	If pager became empty, remove it.	*/
d1091 1
a1091 1
 *	vm_object_collapse_aux:
d1093 4
a1096 4
 *	Internal function to vm_object_collapse called when
 *	it has been shown that a collapse operation is likely
 *	to succeed.  We know that the backing object is only
 *	referenced by me and that paging is not in progress.
d1114 16
a1129 16
	 *	The algorithm used is roughly like this:
	 *	(1)	Trim a potential pager in the backing
	 *		object so it'll only hold pages in reach.
	 *	(2)	Loop over all the resident pages in the
	 *		shadow object and either remove them if
	 *		they are shadowed or move them into the
	 *		shadowing object.
	 *	(3)	Loop over the paged out pages in the
	 *		shadow object.  Start pageins on those
	 *		that aren't shadowed, and just deallocate
	 *		the others.  In each iteration check if
	 *		other users of these objects have caused
	 *		pageins resulting in new resident pages.
	 *		This can happen while we are waiting for
	 *		a pagein of ours.  If such resident pages
	 *		turn up, restart from (2).
d1133 3
a1135 3
	 *	As a first measure we know we can discard
	 *	everything that the shadowing object doesn't
	 *	shadow.
d1147 2
a1148 2
	 *	This is the outer loop, iterating until all resident and
	 *	paged out pages in the shadow object are drained.
d1154 4
a1157 4
		 *	First of all get rid of resident pages in the
		 *	backing object.  We can guarantee to remove
		 *	every page thus we can write the while-test
		 *	like this.
d1162 7
a1168 7
			 *	If the page is outside the shadowing object's
			 *	range or if the page is shadowed (either by a
			 *	resident "non-fake" page or a paged out one) we
			 *	can discard it right away.  Otherwise we need
			 *	to move the page to the shadowing object,
			 *	perhaps waking up waiters for "fake" pages
			 *	first.
d1179 1
a1179 1
				 *	Just discard the page, noone needs it.
d1186 3
a1188 3
				 *	If a "fake" page was found, someone may
				 *	be waiting for it.  Wake her up and
				 *	then remove the page.
d1198 8
a1205 8
				 *	If the backing page was ever paged out,
				 *	it was due to it being dirty at one
				 *	point.  Unless we have no pager
				 *	allocated to the front object (thus
				 *	will move forward the shadow's one),
				 *	mark it dirty again so it won't be
				 *	thrown away without being paged out to
				 *	the front pager.
d1213 1
a1213 1
				/*	Move the page up front.	*/
d1219 2
a1220 2
		 *	If there isn't a pager in the shadow object, we're
		 *	ready.  Take the easy way out.
d1226 3
a1228 3
		 *	If the shadowing object doesn't have a pager
		 *	the easiest thing to do now is to just move the
		 *	backing pager up front and everything is done.  
d1239 10
a1248 10
		 *	What's left to do is to find all paged out
		 *	pages in the backing pager and either discard
		 *	or move it to the front object.  We need to
		 *	recheck the resident page set as a pagein might
		 *	have given other threads the chance to, via
		 *	readfaults, page in another page into the
		 *	resident set.  In this case the outer loop must
		 *	get reentered.  That is also the case if some other
		 *	thread removes the front pager, a case that has
		 *	been seen...
d1255 4
a1258 4
			 *	If the shadowing object has this page, get
			 *	rid of it from the backing pager.  Trust
			 *	the loop condition to get us out of here
			 *	quickly if we remove the last paged out page.
d1260 2
a1261 2
			 *	XXX Would clustering several pages at a time
			 *	be a win in this situation?
d1269 3
a1271 3
				 *	If a "fake" page was found, someone
				 *	may be waiting for it.  Wake her up
				 *	and then remove the page.
d1280 2
a1281 2
				 *	Suck the page from the pager and give
				 *	it to the shadowing object.
d1290 3
a1292 3
				 *	First allocate a page and mark it
				 *	busy so another thread won't try
				 *	to start another pagein.
d1305 6
a1310 6
				 *	Second, start paging it in.  If this
				 *	fails, what can we do but punt?
				 *	Even though the shadowing object
				 *	isn't exactly paging we say so in
				 *	order to not get simultaneous
				 *	cascaded collapses.
d1325 5
a1329 5
				 *	A fault might have issued other
				 *	pagein operations.  We must wait for
				 *	them to complete, then we get to
				 *	wakeup potential other waiters as
				 *	well.
d1349 5
a1353 5
				 *	During the pagein vm_object_terminate
				 *	might have slept on our front object in
				 *	order to remove it.  If this is the
				 *	case, we might as well stop all the
				 *	collapse work right here.
d1361 3
a1363 3
				 *	Third, relookup in case pager changed
				 *	page.  Pager is responsible for
				 *	disposition of old page if moved.
d1369 6
a1374 6
				 *	This page was once dirty, otherwise
				 *	it hadn't been paged out in this
				 *	shadow object.  As we now remove the
				 *	persistant store of the page, make
				 *	sure it will be paged out in the
				 *	front pager by dirtying it.
d1379 2
a1380 2
				 *	Fourth, move it up front, and wake up
				 *	potential waiters.
d1394 4
a1397 4
	 *	I've seen this condition once in an out of VM situation.
	 *	For the moment I don't know why it occurred, although I suspect
	 *	vm_object_page_clean can create a pager even if it won't use
	 *	it.
d1411 3
a1413 3
	 *	Object now shadows whatever backing_object did.
	 *	Note that the reference to backing_object->shadow
	 *	moves from within backing_object to within object.
d1426 1
a1426 1
	/*	Fast cleanup is the only thing left now.	*/
d1441 1
a1441 1
 *	vm_object_bypass:
d1443 4
a1446 4
 *	Internal function to vm_object_collapse called when collapsing
 *	the object with its backing one is not allowed but there may
 *	be an opportunity to bypass the backing object and shadow the
 *	next object in the chain instead.
d1458 5
a1462 5
	 *	If all of the pages in the backing object are
	 *	shadowed by the parent object, the parent
	 *	object no longer has to shadow the backing
	 *	object; it can shadow the next one in the
	 *	chain.
d1464 3
a1466 3
	 *	The backing object must not be paged out - we'd
	 *	have to check all of the paged-out pages, as
	 *	well.
d1473 2
a1474 2
	 *	Should have a check for a 'small' number
	 *	of pages here.
d1482 3
a1484 3
		 *	If the parent has a page here, or if
		 *	this page falls outside the parent,
		 *	keep going.
d1486 2
a1487 2
		 *	Otherwise, the backing_object must be
		 *	left in the chain.
d1494 1
a1494 1
			 *	Page still needed.  Can't go any further.
d1501 4
a1504 4
	 *	Make the parent shadow the next object
	 *	in the chain.  Deallocating backing_object
	 *	will not remove it, since its reference
	 *	count is at least 2.
d1518 2
a1519 2
	 *	Backing object might have had a copy pointer
	 *	to us.  If it did, clear it. 
d1525 4
a1528 4
	/*	Drop the reference count on backing_object.
	 *	Since its ref_count was at least 2, it
	 *	will not vanish; so we don't need to call
	 *	vm_object_deallocate.
d1537 1
a1537 1
 *	vm_object_collapse:
d1539 3
a1541 3
 *	Collapse an object with the object backing it.
 *	Pages in the backing object are moved into the
 *	parent, and the backing object is deallocated.
d1543 2
a1544 2
 *	Requires that the object be locked and the page
 *	queues be unlocked.
d1559 1
a1559 1
		 *	Verify that the conditions are right for collapse:
d1561 2
a1562 2
		 *	The object exists and no pages in it are currently
		 *	being paged out.
d1568 1
a1568 1
		 *		There is a backing object, and
d1576 5
a1580 5
		 *	...
		 *		The backing object is not read_only,
		 *		and no pages in the backing object are
		 *		currently being paged out.
		 *		The backing object is internal.
d1590 8
a1597 8
		 *	The backing object can't be a copy-object:
		 *	the shadow_offset for the copy-object must stay
		 *	as 0.  Furthermore (for the 'we have all the
		 *	pages' case), if we bypass backing_object and
		 *	just shadow the next object in the chain, old
		 *	pages from that object would then have to be copied
		 *	BOTH into the (former) backing_object and into the
		 *	parent object.
d1606 3
a1608 3
		 *	If there is exactly one reference to the backing
		 *	object, we can collapse it into the parent,
		 *	otherwise we might be able to bypass it completely.
d1623 1
a1623 1
		 *	Try again with this object's new backing object.
d1629 1
a1629 1
 *	vm_object_page_remove: [internal]
d1631 2
a1632 2
 *	Removes all physical pages in the specified
 *	object range from the object's list of pages.
d1634 1
a1634 1
 *	The object must be locked.
d1659 14
a1672 14
 *	Routine:	vm_object_coalesce
 *	Function:	Coalesces two objects backing up adjoining
 *			regions of memory into a single object.
 *
 *	returns TRUE if objects were combined.
 *
 *	NOTE:	Only works at the moment if the second object is NULL -
 *		if it's not, which object do we lock first?
 *
 *	Parameters:
 *		prev_object	First object to coalesce
 *		prev_offset	Offset into prev_object
 *		next_object	Second object into coalesce
 *		next_offset	Offset into next_object
d1674 2
a1675 2
 *		prev_size	Size of reference to prev_object
 *		next_size	Size of reference to next_object
d1677 2
a1678 2
 *	Conditions:
 *	The object must *not* be locked.
d1707 1
a1707 1
	 *	Try to collapse the object first
d1712 7
a1718 7
	 *	Can't coalesce if:
	 *	. more than one reference
	 *	. paged out
	 *	. shadows another object
	 *	. has a copy elsewhere
	 *	(any of which mean that the pages not mapped to
	 *	prev_entry may be in use anyway)
d1721 2
a1722 4
	if (prev_object->ref_count > 1 ||
		prev_object->pager != NULL ||
		prev_object->shadow != NULL ||
		prev_object->copy != NULL) {
d1728 2
a1729 2
	 *	Remove any pages that may still be in the object from
	 *	a previous deallocation.
d1732 2
a1733 3
	vm_object_page_remove(prev_object,
			prev_offset + prev_size,
			prev_offset + prev_size + next_size);
d1736 1
a1736 1
	 *	Extend the object if necessary.
d1747 1
a1747 1
 *	vm_object_print:	[ debug ]
d1773 1
a1773 1
		(long) object, (long) object->size,
d1776 2
a1777 2
	       (long) object->pager, (long) object->paging_offset,
	       (long) object->shadow, (long) object->shadow_offset);
d1780 2
a1781 1
	for (o = object->shadowers.lh_first; o; o = o->shadowers_list.le_next) {
d1814 1
a1814 1
 *	vm_object_set_shadow:
d1816 2
a1817 2
 *	Maintain the shadow graph so that back-link consistency is
 *	always kept.
d1819 2
a1820 2
 *	Assumes both objects as well as the old shadow to be locked
 *	(unless NULL of course).
@


1.12
log
@Be more restrictive with concurrent uses of the objects involved while
paging in.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.11 1996/08/19 10:38:01 niklas Exp $	*/
d341 15
a371 8
	 * Wait until the pageout daemon is through with the object.
	 */
	while (object->paging_in_progress) {
		vm_object_sleep(object, object, FALSE);
		vm_object_lock(object);
	}

	/*
d1349 12
@


1.11
log
@A front pager can be lost during pagein, check for this case
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.10 1996/08/18 18:44:46 niklas Exp $	*/
d1302 4
d1307 1
d1318 24
a1341 2
				if (--backing_object->paging_in_progress == 0)
					thread_wakeup(backing_object);
@


1.10
log
@Restructured vm_object_collapse_aux so sharing object's pageins can go on
simultaneously without interfering.  Also maintain PG_BUSY at pagein time.
Removed some statics so gdb will be happier.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.9 1996/08/14 23:16:38 niklas Exp $	*/
d1241 3
a1243 1
		 *	get reentered.
d1246 1
a1246 1
		    backing_object->pager != NULL &&
@


1.9
log
@IMPORTANT FIX: arbitrary page could be removed during collapse.
Sanitize "fake" page handling once more.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.8 1996/08/13 22:26:18 niklas Exp $	*/
d131 6
a136 6
static void _vm_object_allocate __P((vm_size_t, vm_object_t));
static int vm_object_collapse_aux __P((vm_object_t));
static int vm_object_bypass __P((vm_object_t));
static void vm_object_set_shadow __P((vm_object_t, vm_object_t));
static int vm_object_remove_from_pager
    __P((vm_object_t, vm_offset_t, vm_offset_t));
d1064 1
a1064 1
static int
d1093 1
a1093 1
static int
d1100 1
a1100 1
	vm_offset_t	offset;
d1109 17
a1125 20
	 *	First of all get rid of resident pages in the
	 *	backing object.  We can guarantee to remove
	 *	every page thus we can write the while-test
	 *	like this.
	 */
	while ((backing_page = backing_object->memq.tqh_first) != NULL) {
		/*
		 *	If the page is outside the shadowing object's
		 *	range or if the page is shadowed (either by a
		 *	resident "non-fake" page or a paged out one) we
		 *	can discard it right away.  Otherwise we need to
		 *	move the page to the shadowing object, perhaps
		 *	waking up waiters for "fake" pages first.
		 */
		if (backing_page->offset < backing_offset ||
		    (offset = backing_page->offset - backing_offset) >= size ||
		    ((page = vm_page_lookup(object, offset)) != NULL &&
		    !(page->flags & PG_FAKE)) ||
		    (object->pager &&
		    vm_pager_has_page(object->pager, offset))) {
d1127 29
a1155 5
			/*	Just discard the page, noone needs it.	*/
			vm_page_lock_queues();
			vm_page_free(backing_page);
			vm_page_unlock_queues();
		} else {
d1157 7
a1163 3
			 *	If a "fake" page was found, someone may
			 *	be waiting for it.  Wake her up and then
			 *	remove the page.
d1165 11
a1175 2
			if (page) {
				PAGE_WAKEUP(page);
d1177 1
a1177 1
				vm_page_free(page);
d1179 12
a1190 1
			}
d1192 15
a1206 13
			/*
			 *	If the backing page was ever paged out, it was
			 *	due to it being dirty at one point.  Unless we
			 *	have no pager allocated to the front object
			 *	(thus will move forward the shadow's one),
			 *	mark it dirty again so it won't be thrown away
			 *	without being paged out to the front pager.
			 */
			if (object->pager != NULL &&
			    vm_object_remove_from_pager(backing_object,
			    backing_page->offset,
			    backing_page->offset + PAGE_SIZE))
				backing_page->flags &= ~PG_CLEAN;
d1208 3
a1210 2
			/*	Move the page up front.	*/
			vm_page_rename(backing_page, object, offset);
a1211 1
	}
d1213 11
a1223 12
	/*
	 *	If not both object have pagers we are essentially
	 *	ready.  Just see to that any existing pager is
	 *	attached to the shadowing object and then get rid
	 *	of the backing object (which at that time should
	 *	have neither resident nor paged out pages left).
	 */
	if (object->pager == NULL || backing_object->pager == NULL) {
		/*
		 *	If the shadowing object don't have a pager the
		 *	easiest thing to do now is to just move the
		 *	potential backing pager up front.
d1230 1
a1231 15
	} else {
		/*
		 *	OK we know both objects have pagers so now we need to
		 *	check if the backing objects's paged out pages can be
		 *	discarded or needs to be moved.
		 *
		 *	As a first measure we know we can discard everything
		 *	that the shadowing object doesn't shadow.
		 */
		if (backing_offset > 0)
			vm_object_remove_from_pager(backing_object, 0,
			    backing_offset);
		if (backing_offset + size < backing_object->size)
			vm_object_remove_from_pager(backing_object,
			    backing_offset + size, backing_object->size);
d1234 13
a1246 9
		 *	What's left to do is to find all paged out pages
		 *	in the backing pager and either discard or move
		 *	it to the front object.
		 */
		offset = 0;
		while (backing_object->pager &&
		    (offset = vm_pager_next(backing_object->pager, offset)) <
		    backing_object->size) {

d1257 1
a1257 1
			    offset - backing_offset)) == NULL ||
d1260 1
a1260 1
			    offset - backing_offset)) {
a1271 1

d1282 5
a1286 1
				/*	First allocate a page.	*/
d1290 1
a1290 1
					    offset);
d1295 1
d1297 4
a1300 1
				/*	Second, start paging it in.	*/
d1304 1
d1307 1
a1307 1
#if 0
a1308 1
#endif
d1310 1
d1320 1
a1320 3
				    offset);

				cnt.v_pgpgin++;
d1337 1
a1337 1
				    offset - backing_offset);
d1339 1
d1341 3
a1343 3
			vm_object_remove_from_pager(backing_object, offset,
			    offset + PAGE_SIZE);
			offset += PAGE_SIZE;
d1353 1
a1353 1
	if (backing_object->pager &&
d1402 1
a1402 1
static int
d1778 1
a1778 1
static void
@


1.8
log
@Clean up vm_object_collapse_aux's dirty page game.  Pun intended :-)
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.7 1996/08/12 12:33:34 niklas Exp $	*/
a1121 10
		 *
		 *	XXX There is a condition I'm unsure about, both
		 *	if it exists and if I handle it right.  The
		 *	case that worries me is if an object can hold
		 *	"fake" pages at the same time a real one is
		 *	paged out.  To me it sounds as this condition
		 *	can't exist.  Does anyone know?  The way the
		 *	condition below is done, "fake" pages are
		 *	handled suboptimally if pagers are guaranteed
		 *	not to have such pages in store.
a1146 3
			/*	Move the page up front.	*/
			vm_page_rename(backing_page, object, offset);

d1157 2
a1158 1
			    backing_offset, backing_offset + PAGE_SIZE))
d1160 3
a1218 2
			 *
			 *	XXX Is the "fake" page handling correct???
a1230 3
#ifdef DIAGNOSTIC
					printf("fake page blown away\n");
#endif
d1288 1
a1288 1
				backing_page->flags &= ~PG_CLEAN;
@


1.7
log
@Fixed CRITICAL bug hitting in former swap leak scenarios
when collapsing two objects a formerly COWed page might lose it's backing store
causing stuff like lost ld.so fixups of shared lib images.  Also handle fake
pages in all cases.  In short: UPDATE!
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.6 1996/08/02 00:06:02 niklas Exp $	*/
d1170 1
a1170 1
			    backing_offset, backing_offset + PAGE_SIZE)) {
a1171 3
				if (backing_page->flags & PG_INACTIVE)
					backing_page->flags |= PG_LAUNDRY;
			}
a1300 3
				 *
				 *	XXX these flag adjustments may be
				 *	overkill, look into them some time.
d1302 1
a1302 5
				backing_page->flags &= ~(PG_FAKE|PG_CLEAN);
				if (backing_page->flags & PG_INACTIVE)
					backing_page->flags |= PG_LAUNDRY;
				pmap_clear_modify(VM_PAGE_TO_PHYS(
				    backing_page));
@


1.6
log
@Fix long-standing swap-leak. Add OpenBSD tags. Optimize thread_wakeup.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.5 1996/07/23 23:54:25 deraadt Exp $	*/
a1062 1
 *	Assume object->pager is non-NULL.
d1072 3
d1157 1
a1157 1
			/*	Just move the page up front.	*/
d1159 16
a1228 4
			 *	XXX Should pages found paged out in the backing
			 *	object be marked for pageout in the shadowing
			 *	object?
			 *
d1232 1
a1232 1
			 *	XXX "fake" page handling???
d1234 4
a1237 2
			if (vm_page_lookup(object, offset - backing_offset) ==
			    NULL && !vm_pager_has_page(object->pager,
d1240 17
a1256 2
				 *	Suck the page from the pager and give it
				 *	to the shadowing object.
d1296 15
a1310 2
				backing_page->flags &= ~PG_FAKE;
				backing_page->flags |= PG_CLEAN;
@


1.5
log
@make printf/addlog return 0, for compat to userland
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.4 1996/04/19 16:10:51 niklas Exp $	*/
d75 1
d79 1
d117 13
d132 5
d206 1
d247 6
a252 1
	vm_object_t	temp;
d269 15
a288 1
			vm_object_unlock(object);
d346 1
d513 5
d810 2
a811 1
		 *	object.
a812 1

d814 1
a814 1
		old_copy->shadow = new_copy;
d827 1
a827 1
	new_copy->shadow = src_object;
d856 2
a872 1

d883 3
a885 1
	result->shadow = source;
a890 1

a895 1

d904 1
a904 2
vm_object_setpager(object, pager, paging_offset,
			read_only)
d1058 369
a1426 1
boolean_t	vm_object_collapse_allowed = TRUE;
a1443 4
	register vm_offset_t	backing_offset;
	register vm_size_t	size;
	register vm_offset_t	new_offset;
	register vm_page_t	p, pp;
d1453 1
a1453 1
		 *	being paged out (or have ever been paged out).
d1455 1
a1455 3
		if (object == NULL ||
		    object->paging_in_progress != 0 ||
		    object->pager != NULL)
a1496 10
		 *	We know that we can either collapse the backing
		 *	object (if the parent is the only reference to
		 *	it) or (perhaps) remove the parent's reference
		 *	to it.
		 */

		backing_offset = object->shadow_offset;
		size = object->size;

		/*
d1498 2
a1499 1
		 *	object, we can collapse it into the parent.
d1503 3
a1505 44

			/*
			 *	We can collapse the backing object.
			 *
			 *	Move all in-memory pages from backing_object
			 *	to the parent.  Pages that have been paged out
			 *	will be overwritten by any of the parent's
			 *	pages that shadow them.
			 */

			while ((p = backing_object->memq.tqh_first) != NULL) {
				new_offset = (p->offset - backing_offset);

				/*
				 *	If the parent has a page here, or if
				 *	this page falls outside the parent,
				 *	dispose of it.
				 *
				 *	Otherwise, move it as planned.
				 */

				if (p->offset < backing_offset ||
				    new_offset >= size) {
					vm_page_lock_queues();
					vm_page_free(p);
					vm_page_unlock_queues();
				} else {
				    pp = vm_page_lookup(object, new_offset);
				    if (pp != NULL && !(pp->flags & PG_FAKE)) {
					vm_page_lock_queues();
					vm_page_free(p);
					vm_page_unlock_queues();
				    }
				    else {
					if (pp) {
					    /* may be someone waiting for it */
					    PAGE_WAKEUP(pp);
					    vm_page_lock_queues();
					    vm_page_free(pp);
					    vm_page_unlock_queues();
					}
					vm_page_rename(p, object, new_offset);
				    }
				}
d1507 2
a1508 62

			/*
			 *	Move the pager from backing_object to object.
			 *
			 *	XXX We're only using part of the paging space
			 *	for keeps now... we ought to discard the
			 *	unused portion.
			 */

			if (backing_object->pager) {
				object->pager = backing_object->pager;
				object->paging_offset = backing_offset +
					backing_object->paging_offset;
				backing_object->pager = NULL;
			}

			/*
			 *	Object now shadows whatever backing_object did.
			 *	Note that the reference to backing_object->shadow
			 *	moves from within backing_object to within object.
			 */

			object->shadow = backing_object->shadow;
			object->shadow_offset += backing_object->shadow_offset;
			if (object->shadow != NULL &&
			    object->shadow->copy != NULL) {
				panic("vm_object_collapse: we collapsed a copy-object!");
			}
			/*
			 *	Discard backing_object.
			 *
			 *	Since the backing object has no pages, no
			 *	pager left, and no object references within it,
			 *	all that is necessary is to dispose of it.
			 */

			vm_object_unlock(backing_object);

			simple_lock(&vm_object_list_lock);
			TAILQ_REMOVE(&vm_object_list, backing_object,
			    object_list);
			vm_object_count--;
			simple_unlock(&vm_object_list_lock);

			free((caddr_t)backing_object, M_VMOBJ);

			object_collapses++;
		}
		else {
			/*
			 *	If all of the pages in the backing object are
			 *	shadowed by the parent object, the parent
			 *	object no longer has to shadow the backing
			 *	object; it can shadow the next one in the
			 *	chain.
			 *
			 *	The backing object must not be paged out - we'd
			 *	have to check all of the paged-out pages, as
			 *	well.
			 */

			if (backing_object->pager != NULL) {
a1512 64
			/*
			 *	Should have a check for a 'small' number
			 *	of pages here.
			 */

			for (p = backing_object->memq.tqh_first;
			     p != NULL;
			     p = p->listq.tqe_next) {
				new_offset = (p->offset - backing_offset);

				/*
				 *	If the parent has a page here, or if
				 *	this page falls outside the parent,
				 *	keep going.
				 *
				 *	Otherwise, the backing_object must be
				 *	left in the chain.
				 */

				if (p->offset >= backing_offset &&
				    new_offset < size &&
				    ((pp = vm_page_lookup(object, new_offset))
				      == NULL ||
				     (pp->flags & PG_FAKE))) {
					/*
					 *	Page still needed.
					 *	Can't go any further.
					 */
					vm_object_unlock(backing_object);
					return;
				}
			}

			/*
			 *	Make the parent shadow the next object
			 *	in the chain.  Deallocating backing_object
			 *	will not remove it, since its reference
			 *	count is at least 2.
			 */

			object->shadow = backing_object->shadow;
			vm_object_reference(object->shadow);
			object->shadow_offset += backing_object->shadow_offset;

			/*
			 *	Backing object might have had a copy pointer
			 *	to us.  If it did, clear it. 
			 */
			if (backing_object->copy == object) {
				backing_object->copy = NULL;
			}
	
			/*	Drop the reference count on backing_object.
			 *	Since its ref_count was at least 2, it
			 *	will not vanish; so we don't need to call
			 *	vm_object_deallocate.
			 */
			backing_object->ref_count--;
			vm_object_unlock(backing_object);

			object_bypasses ++;

		}

d1658 4
a1661 3
	extern indent;

	register int count;
d1672 7
d1704 43
@


1.4
log
@NetBSD 960317 merge
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_object.c,v 1.3 1996/03/03 17:45:34 niklas Exp $	*/
d1421 1
a1421 1
	void		(*pr) __P((const char *, ...));
@


1.3
log
@From NetBSD: merge with 960217
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: vm_object.c,v 1.33 1996/02/10 00:08:11 christos Exp $	*/
a1405 50
/*
 *	vm_object_prefer:
 *
 *	Return optimal virtual address for new mapping of this object.
 *
 *	The object must *not* be locked.
 */
void
vm_object_prefer(object, offset, addr)
	register vm_object_t	object;
	register vm_offset_t	offset;
	register vm_offset_t	*addr;
{
	register vm_page_t	p;
	register vm_offset_t	paddr;

#ifdef PMAP_PREFER
	if (object == NULL)
		goto first_map;

	/*
	 * Look for the first page that the pmap layer has something
	 * to say about. Since an object maps a contiguous range of
	 * virutal addresses, this will determine the preferred origin
	 * of the proposed mapping.
	 */
	vm_object_lock(object);
	for (p = object->memq.tqh_first; p != NULL; p = p->listq.tqe_next) {
		if (p->flags & (PG_FAKE | PG_FICTITIOUS))
			continue;
		paddr = PMAP_PREFER(VM_PAGE_TO_PHYS(p), *addr+p->offset-offset);
		if (paddr == (vm_offset_t)-1)
			continue;
		*addr = paddr - (p->offset - offset);
		vm_object_unlock(object);
		return;
	}
	vm_object_unlock(object);

first_map:
	/*
	 * No physical page attached; ask for a preferred address based
	 * only on the given virtual address.
	 */
	paddr = PMAP_PREFER((vm_offset_t)-1, *addr);
	if (paddr != (vm_offset_t)-1)
		*addr = paddr;

#endif
}
@


1.2
log
@from netbsd:
Extend use of vm_object_prefer() to vm_allocate_with_pager().
Make vm_object_prefer() call MD aligner for "pageless" objects too,
so we can have more control over the virtual address to be used.

Implementation could be simpler if we by-pass the object to mapped, but
we'd loose the ability to adapt alignment to objects that were previously
mmap'ed with MAP_FIXED on.

Only expect vm_fork() to return if __FORK_BRAINDAMAGE is defined.
Eliminate unused third arg to vm_fork().
@
text
@d1 2
a2 1
/*	$NetBSD: vm_object.c,v 1.31 1995/12/06 00:38:11 pk Exp $	*/
d391 1
a391 1
	int onqueue;
d431 1
a431 1
		if ((start == end || p->offset >= start && p->offset < end) &&
a1463 2
        extern void _vm_object_print();
        
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: vm_object.c,v 1.29 1995/07/13 12:35:29 pk Exp $	*/
d1421 1
d1423 1
a1423 1
		return;
a1424 2
#ifdef PMAP_PREFER
	vm_object_lock(object);
d1431 1
d1439 2
a1440 1
		break;
d1443 10
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

