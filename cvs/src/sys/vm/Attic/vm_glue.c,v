head	1.39;
access;
symbols
	SMP_SYNC_A:1.39
	SMP_SYNC_B:1.39
	UBC_SYNC_A:1.39
	UBC_SYNC_B:1.39
	OPENBSD_2_9_BASE:1.36
	OPENBSD_2_9:1.36.0.2
	OPENBSD_2_8:1.35.0.8
	OPENBSD_2_8_BASE:1.35
	OPENBSD_2_7:1.35.0.6
	OPENBSD_2_7_BASE:1.35
	SMP:1.35.0.4
	SMP_BASE:1.35
	kame_19991208:1.35
	OPENBSD_2_6:1.35.0.2
	OPENBSD_2_6_BASE:1.35
	OPENBSD_2_5:1.31.0.2
	OPENBSD_2_5_BASE:1.31
	OPENBSD_2_4:1.29.0.4
	OPENBSD_2_4_BASE:1.29
	OPENBSD_2_3:1.29.0.2
	OPENBSD_2_3_BASE:1.29
	OPENBSD_2_2:1.26.0.2
	OPENBSD_2_2_BASE:1.26
	OPENBSD_2_1:1.23.0.2
	OPENBSD_2_1_BASE:1.23
	OPENBSD_2_0:1.20.0.2
	OPENBSD_2_0_BASE:1.20
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.39
date	2001.06.27.04.53.30;	author art;	state dead;
branches;
next	1.38;

1.38
date	2001.06.08.08.09.43;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2001.05.05.21.26.46;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2001.04.02.21.43.12;	author niklas;	state Exp;
branches;
next	1.35;

1.35
date	99.09.03.18.02.27;	author art;	state Exp;
branches
	1.35.4.1;
next	1.34;

1.34
date	99.08.17.18.48.59;	author niklas;	state Exp;
branches;
next	1.33;

1.33
date	99.08.17.10.32.19;	author niklas;	state Exp;
branches;
next	1.32;

1.32
date	99.06.23.09.44.29;	author art;	state Exp;
branches;
next	1.31;

1.31
date	99.02.19.19.21.40;	author art;	state Exp;
branches;
next	1.30;

1.30
date	99.02.01.16.29.48;	author pefo;	state Exp;
branches;
next	1.29;

1.29
date	98.03.01.00.38.05;	author niklas;	state Exp;
branches;
next	1.28;

1.28
date	98.02.23.20.15.54;	author niklas;	state Exp;
branches;
next	1.27;

1.27
date	97.11.06.05.59.32;	author csapuntz;	state Exp;
branches;
next	1.26;

1.26
date	97.10.06.20.21.18;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	97.10.06.15.28.52;	author csapuntz;	state Exp;
branches;
next	1.24;

1.24
date	97.07.25.06.03.07;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	97.04.17.01.25.18;	author niklas;	state Exp;
branches;
next	1.22;

1.22
date	97.01.07.05.37.36;	author tholo;	state Exp;
branches;
next	1.21;

1.21
date	96.11.23.23.19.54;	author kstailey;	state Exp;
branches;
next	1.20;

1.20
date	96.08.02.00.05.59;	author niklas;	state Exp;
branches;
next	1.19;

1.19
date	96.07.23.23.54.22;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	96.06.24.20.00.51;	author pefo;	state Exp;
branches;
next	1.17;

1.17
date	96.06.20.10.51.08;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	96.05.28.12.16.32;	author deraadt;	state Exp;
branches;
next	1.15;

1.15
date	96.05.22.12.03.13;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	96.04.21.22.33.11;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	96.04.19.16.10.47;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	96.04.19.06.45.31;	author niklas;	state Exp;
branches;
next	1.11;

1.11
date	96.03.24.17.05.35;	author tholo;	state Exp;
branches;
next	1.10;

1.10
date	96.03.19.21.10.55;	author mickey;	state Exp;
branches;
next	1.9;

1.9
date	96.03.03.17.45.29;	author niklas;	state Exp;
branches;
next	1.8;

1.8
date	96.02.19.23.22.41;	author tholo;	state Exp;
branches;
next	1.7;

1.7
date	96.02.18.11.58.18;	author tholo;	state Exp;
branches;
next	1.6;

1.6
date	95.12.21.14.45.29;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	95.12.14.08.14.17;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	95.12.14.05.16.07;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	95.10.23.06.11.30;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.10.18.11.05.42;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.53.37;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.53.37;	author deraadt;	state Exp;
branches;
next	;

1.35.4.1
date	2001.05.14.22.47.50;	author niklas;	state Exp;
branches;
next	1.35.4.2;

1.35.4.2
date	2001.07.04.11.01.18;	author niklas;	state dead;
branches;
next	;


desc
@@


1.39
log
@Die!
@
text
@/*	$OpenBSD: vm_glue.c,v 1.38 2001/06/08 08:09:43 art Exp $    */
/*	$NetBSD: vm_glue.c,v 1.55.4.1 1996/06/13 17:25:45 cgd Exp $	*/

/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_glue.c	8.9 (Berkeley) 3/4/95
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/resourcevar.h>
#include <sys/buf.h>
#include <sys/user.h>
#ifdef SYSVSHM
#include <sys/shm.h>
#endif

#include <vm/vm.h>
#include <vm/vm_extern.h>
#include <vm/vm_page.h>
#include <vm/vm_kern.h>

#include <machine/cpu.h>

int	avefree = 0;		/* XXX */
unsigned maxdmap = MAXDSIZ;	/* XXX */ 
unsigned maxsmap = MAXSSIZ;	/* XXX */ 
int	readbuffers = 0;	/* XXX allow kgdb to read kernel buffer pool */

int
kernacc(addr, len, rw)
	caddr_t addr;
	int len, rw;
{
	boolean_t rv;
	vm_offset_t saddr, eaddr;
	vm_prot_t prot = rw == B_READ ? VM_PROT_READ : VM_PROT_WRITE;

	saddr = trunc_page((vaddr_t)addr);
	eaddr = round_page((vaddr_t)addr+len);
	rv = vm_map_check_protection(kernel_map, saddr, eaddr, prot);
	/*
	 * XXX there are still some things (e.g. the buffer cache) that
	 * are managed behind the VM system's back so even though an
	 * address is accessible in the mind of the VM system, there may
	 * not be physical pages where the VM thinks there is.  This can
	 * lead to bogus allocation of pages in the kernel address space
	 * or worse, inconsistencies at the pmap level.  We only worry
	 * about the buffer cache for now.
	 */
	if (!readbuffers && rv && (eaddr > (vm_offset_t)buffers &&
	    saddr < (vm_offset_t)buffers + MAXBSIZE * nbuf))
		rv = FALSE;
	return (rv == TRUE);
}

int
useracc(addr, len, rw)
	caddr_t addr;
	int len, rw;
{
	boolean_t rv;
	vm_prot_t prot = rw == B_READ ? VM_PROT_READ : VM_PROT_WRITE;

#if defined(i386) || defined(pc532)
	/*
	 * XXX - specially disallow access to user page tables - they are
	 * in the map.  This is here until i386 & pc532 pmaps are fixed...
	 */
	if ((vm_offset_t) addr >= VM_MAXUSER_ADDRESS
	    || (vm_offset_t) addr + len > VM_MAXUSER_ADDRESS
	    || (vm_offset_t) addr + len <= (vm_offset_t) addr)
		return (FALSE);
#endif

	rv = vm_map_check_protection(&curproc->p_vmspace->vm_map,
	    trunc_page((vaddr_t)addr), round_page((vaddr_t)addr+len), prot);
	return (rv == TRUE);
}

#ifdef KGDB
/*
 * Change protections on kernel pages from addr to addr+len
 * (presumably so debugger can plant a breakpoint).
 *
 * We force the protection change at the pmap level.  If we were
 * to use vm_map_protect a change to allow writing would be lazily-
 * applied meaning we would still take a protection fault, something
 * we really don't want to do.  It would also fragment the kernel
 * map unnecessarily.  We cannot use pmap_protect since it also won't
 * enforce a write-enable request.  Using pmap_enter is the only way
 * we can ensure the change takes place properly.
 */
void
chgkprot(addr, len, rw)
	register caddr_t addr;
	int len, rw;
{
	vm_prot_t prot;
	vm_offset_t pa, sva, eva;

	prot = rw == B_READ ? VM_PROT_READ : VM_PROT_READ|VM_PROT_WRITE;
	eva = round_page((vaddr_t)addr + len);
	for (sva = trunc_page((vaddr_t)addr); sva < eva; sva += PAGE_SIZE) {
		/*
		 * Extract physical address for the page.
		 */
		if (pmap_extract(pmap_kernel(), sva, &pa) == FALSE)
			panic("chgkprot: invalid page");
		pmap_enter(pmap_kernel(), sva, pa, prot, TRUE, 0);
	}
}
#endif

int
vslock(addr, len)
	caddr_t	addr;
	u_int	len;
{
#ifdef __i386__
	pmap_prefault(&curproc->p_vmspace->vm_map, (vm_offset_t)addr, len);
#endif
	return (vm_map_pageable(&curproc->p_vmspace->vm_map,
	    trunc_page((vaddr_t)addr),
	    round_page((vaddr_t)addr+len), FALSE));
}

int
vsunlock(addr, len)
	caddr_t	addr;
	u_int	len;
{
	return (vm_map_pageable(&curproc->p_vmspace->vm_map,
	    trunc_page((vaddr_t)addr),
	    round_page((vaddr_t)addr+len), TRUE));
}

/*
 * Implement fork's actions on an address space.
 * Here we arrange for the address space to be copied or referenced,
 * allocate a user struct (pcb and kernel stack), then call the
 * machine-dependent layer to fill those in and make the new process
 * ready to run.
 * NOTE: the kernel stack may be at a different location in the child
 * process, and thus addresses of automatic variables may be invalid
 * after cpu_fork returns in the child process.  We do nothing here
 * after cpu_fork returns.
 */
#ifdef __FORK_BRAINDAMAGE
int
#else
void
#endif
vm_fork(p1, p2, stack, stacksize)
	register struct proc *p1, *p2;
	void *stack;
	size_t stacksize;
{
	register struct user *up = p2->p_addr;

#if defined(i386) || defined(pc532)
	/*
	 * avoid copying any of the parent's pagetables or other per-process
	 * objects that reside in the map by marking all of them non-inheritable
	 */
	(void)vm_map_inherit(&p1->p_vmspace->vm_map,
		VM_MAXUSER_ADDRESS, VM_MAX_ADDRESS, VM_INHERIT_NONE);
#endif
	p2->p_vmspace = vmspace_fork(p1->p_vmspace);

#ifdef SYSVSHM
	if (p1->p_vmspace->vm_shm)
		shmfork(p1->p_vmspace, p2->p_vmspace);
#endif

	vm_map_pageable(kernel_map, (vm_offset_t)up,
			(vm_offset_t)up + USPACE, FALSE);

	/*
	 * p_stats currently point at fields in the user struct.  Copy
	 * parts of p_stats, and zero out the rest.
	 */
	p2->p_stats = &up->u_stats;
	bzero(&up->u_stats.pstat_startzero,
	    (unsigned) ((caddr_t)&up->u_stats.pstat_endzero -
	    (caddr_t)&up->u_stats.pstat_startzero));
	bcopy(&p1->p_stats->pstat_startcopy, &up->u_stats.pstat_startcopy,
	    ((caddr_t)&up->u_stats.pstat_endcopy -
	     (caddr_t)&up->u_stats.pstat_startcopy));

#if defined(i386) || defined(pc532)
	{
		 vm_offset_t addr = VM_MAXUSER_ADDRESS; struct vm_map *vp;

		 /* ream out old pagetables and kernel stack */
		 vp = &p2->p_vmspace->vm_map;
		 (void)vm_deallocate(vp, addr, VM_MAX_ADDRESS - addr);
		 (void)vm_allocate(vp, &addr, VM_MAX_ADDRESS - addr, FALSE);
		 (void)vm_map_inherit(vp, addr, VM_MAX_ADDRESS,
		     VM_INHERIT_NONE);
	}
#endif

#ifdef __FORK_BRAINDAMAGE
	/*
	 * cpu_fork will copy and update the kernel stack and pcb,
	 * and make the child ready to run.  It marks the child
	 * so that it can return differently than the parent.
	 * It returns twice, once in the parent process and
	 * once in the child.
	 */
	return (cpu_fork(p1, p2, stack, stacksize));
#else
	/*
	 * cpu_fork will copy and update the kernel stack and pcb,
	 * and make the child ready to run.  The child will exit
	 * directly to user mode on its first time slice, and will
	 * not return here.
	 */
	cpu_fork(p1, p2, stack, stacksize);
#endif
}

/*
 * Set default limits for VM system.
 * Called for proc 0, and then inherited by all others.
 */
void
vm_init_limits(p)
	register struct proc *p;
{

	/*
	 * Set up the initial limits on process VM.
	 * Set the maximum resident set size to be all
	 * of (reasonably) available memory.  This causes
	 * any single, large process to start random page
	 * replacement once it fills memory.
	 */
	p->p_rlimit[RLIMIT_STACK].rlim_cur = DFLSSIZ;
	p->p_rlimit[RLIMIT_STACK].rlim_max = MAXSSIZ;
	p->p_rlimit[RLIMIT_DATA].rlim_cur = DFLDSIZ;
	p->p_rlimit[RLIMIT_DATA].rlim_max = MAXDSIZ;
	p->p_rlimit[RLIMIT_RSS].rlim_cur = ptoa(cnt.v_free_count);
}

#include <vm/vm_pageout.h>

#ifdef DEBUG
int	enableswap = 1;
int	swapdebug = 0;
#define	SDB_FOLLOW	1
#define SDB_SWAPIN	2
#define SDB_SWAPOUT	4
#endif

/*
 * Swap in a process's u-area.
 */
void
swapin(p)
	struct proc *p;
{
	vm_offset_t addr;
	int s;

	addr = (vm_offset_t)p->p_addr;
	vm_map_pageable(kernel_map, addr, addr + USPACE, FALSE);
	/*
	 * Some architectures need to be notified when the
	 * user area has moved to new physical page(s) (e.g.
	 * see pmax/pmax/vm_machdep.c).
	 */
	cpu_swapin(p);
	s = splstatclock();
	if (p->p_stat == SRUN)
		setrunqueue(p);
	p->p_flag |= P_INMEM;
	splx(s);
	p->p_swtime = 0;
	++cnt.v_swpin;
}

/*
 * Brutally simple:
 *	1. Attempt to swapin every swaped-out, runnable process in
 *	   order of priority.
 *	2. If not enough memory, wake the pageout daemon and let it
 *	   clear some space.
 */
void
scheduler()
{
	register struct proc *p;
	register int pri;
	struct proc *pp;
	int ppri;

loop:
#ifdef DEBUG
	while (!enableswap) {
		panic ("swap disabled??");
		tsleep((caddr_t)&proc0, PVM, "noswap", 0);
	}
#endif
	pp = NULL;
	ppri = INT_MIN;
	for (p = allproc.lh_first; p != 0; p = p->p_list.le_next) {
		if (p->p_stat == SRUN && (p->p_flag & P_INMEM) == 0) {
			pri = p->p_swtime + p->p_slptime - p->p_nice * 8;
			if (pri > ppri) {
				pp = p;
				ppri = pri;
			}
		}
	}
#ifdef DEBUG
	if (swapdebug & SDB_FOLLOW)
		printf("scheduler: running, procp %p pri %d\n", pp, ppri);
#endif
	/*
	 * Nothing to do, back to sleep
	 */
	if ((p = pp) == NULL) {
		tsleep((caddr_t)&proc0, PVM, "scheduler", 0);
		goto loop;
	}

	/*
	 * We would like to bring someone in.
	 * This part is really bogus cuz we could deadlock on memory
	 * despite our feeble check.
	 * XXX should require at least vm_swrss / 2
	 */
	if (cnt.v_free_count > atop(USPACE)) {
#ifdef DEBUG
		if (swapdebug & SDB_SWAPIN)
			printf("swapin: pid %d(%s)@@%p, pri %d free %d\n",
			    p->p_pid, p->p_comm, p->p_addr, ppri,
			    cnt.v_free_count);
#endif
#if defined(arc) || defined(pica)
			vm_map_pageable(kernel_map, (vm_offset_t)p->p_addr,
			    (vm_offset_t)p->p_addr + atop(USPACE), FALSE);
#endif
		swapin(p);
		goto loop;
	}
	/*
	 * Not enough memory, jab the pageout daemon and wait til the
	 * coast is clear.
	 */
#ifdef DEBUG
	if (swapdebug & SDB_FOLLOW)
		printf("scheduler: no room for pid %d(%s), free %d\n",
		    p->p_pid, p->p_comm, cnt.v_free_count);
#endif
	(void)splhigh();
	vm_wait("fLowmem");
	(void)spl0();
#ifdef DEBUG
	if (swapdebug & SDB_FOLLOW)
		printf("scheduler: room again, free %d\n", cnt.v_free_count);
#endif
	goto loop;
}

#define	swappable(p)							\
	(((p)->p_flag & (P_SYSTEM | P_INMEM | P_WEXIT)) == P_INMEM &&	\
	    (p)->p_holdcnt == 0)

/*
 * Swapout is driven by the pageout daemon.  Very simple, we find eligible
 * procs and unwire their u-areas.  We try to always "swap" at least one
 * process in case we need the room for a swapin.
 * If any procs have been sleeping/stopped for at least maxslp seconds,
 * they are swapped.  Else, we swap the longest-sleeping or stopped process,
 * if any, otherwise the longest-resident process.
 */
void
swapout_threads()
{
	register struct proc *p;
	struct proc *outp, *outp2;
	int outpri, outpri2;
	int didswap = 0;
	extern int maxslp;

#ifdef DEBUG
	if (!enableswap)
		return;
#endif
	outp = outp2 = NULL;
	outpri = outpri2 = 0;
	for (p = allproc.lh_first; p != 0; p = p->p_list.le_next) {
		if (!swappable(p))
			continue;
		switch (p->p_stat) {
		case SRUN:
			if (p->p_swtime > outpri2) {
				outp2 = p;
				outpri2 = p->p_swtime;
			}
			continue;
			
		case SSLEEP:
		case SSTOP:
			if (p->p_slptime >= maxslp) {
				swapout(p);
				didswap++;
			} else if (p->p_slptime > outpri) {
				outp = p;
				outpri = p->p_slptime;
			}
			continue;
		}
	}
	/*
	 * If we didn't get rid of any real duds, toss out the next most
	 * likely sleeping/stopped or running candidate.  We only do this
	 * if we are real low on memory since we don't gain much by doing
	 * it (USPACE bytes).
	 */
	if (didswap == 0 &&
	    cnt.v_free_count <= atop(round_page(USPACE))) {
		if ((p = outp) == 0)
			p = outp2;
#ifdef DEBUG
		if (swapdebug & SDB_SWAPOUT)
			printf("swapout_threads: no duds, try procp %p\n", p);
#endif
		if (p)
			swapout(p);
	}
}

void
swapout(p)
	register struct proc *p;
{
	vm_offset_t addr;
	int s;

#ifdef DEBUG
	if (swapdebug & SDB_SWAPOUT)
		printf("swapout: pid %d(%s)@@%p, stat %x pri %d free %d\n",
		    p->p_pid, p->p_comm, p->p_addr, p->p_stat, p->p_slptime,
		    cnt.v_free_count);
#endif

	/*
	 * Do any machine-specific actions necessary before swapout.
	 * This can include saving floating point state, etc.
	 */
	cpu_swapout(p);

	/*
	 * Unwire the to-be-swapped process's user struct and kernel stack.
	 */
	addr = (vm_offset_t)p->p_addr;
	vm_map_pageable(kernel_map, addr, addr + USPACE, TRUE);
	pmap_collect(vm_map_pmap(&p->p_vmspace->vm_map));

	/*
	 * Mark it as (potentially) swapped out.
	 */
	s = splstatclock();
	p->p_flag &= ~P_INMEM;
	if (p->p_stat == SRUN)
		remrunqueue(p);
	splx(s);
	p->p_swtime = 0;
	++cnt.v_swpout;
}
@


1.38
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.37 2001/05/05 21:26:46 art Exp $    */
@


1.37
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.36 2001/04/02 21:43:12 niklas Exp $    */
a164 3
		 * We use a cheezy hack to differentiate physical
		 * page 0 from an invalid mapping, not that it
		 * really matters...
d166 1
a166 2
		pa = pmap_extract(pmap_kernel(), sva|1);
		if (pa == 0)
d168 1
a168 1
		pmap_enter(pmap_kernel(), sva, pa&~1, prot, TRUE, 0);
@


1.36
log
@On popular demand, the Linux-compatibility clone(2) implementation based
on NetBSD's code, as well as some faked Posix RT extensions by me.  This makes
at least simple linuxthreads tests work.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.35 1999/09/03 18:02:27 art Exp $    */
d97 2
a98 2
	saddr = trunc_page(addr);
	eaddr = round_page(addr+len);
d135 1
a135 1
	    trunc_page(addr), round_page(addr+len), prot);
d161 2
a162 2
	eva = round_page(addr + len);
	for (sva = trunc_page(addr); sva < eva; sva += PAGE_SIZE) {
d185 3
a187 2
	return (vm_map_pageable(&curproc->p_vmspace->vm_map, trunc_page(addr),
	    round_page(addr+len), FALSE));
d195 3
a197 2
	return (vm_map_pageable(&curproc->p_vmspace->vm_map, trunc_page(addr),
	    round_page(addr+len), TRUE));
@


1.35
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.34 1999/08/17 18:48:59 niklas Exp $    */
d240 2
a241 4
	 * p_stats and p_sigacts currently point at fields
	 * in the user struct but not at &u, instead at p_addr.
	 * Copy p_sigacts and parts of p_stats; zero the rest
	 * of p_stats (statistics).
a243 2
	p2->p_sigacts = &up->u_sigacts;
	up->u_sigacts = *p1->p_sigacts;
@


1.35.4.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.36 2001/04/02 21:43:12 niklas Exp $    */
d240 4
a243 2
	 * p_stats currently point at fields in the user struct.  Copy
	 * parts of p_stats, and zero out the rest.
d246 2
@


1.35.4.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.35.4.1 2001/05/14 22:47:50 niklas Exp $    */
@


1.34
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.33 1999/08/17 10:32:19 niklas Exp $    */
d172 1
a172 1
		pmap_enter(pmap_kernel(), sva, pa&~1, prot, TRUE);
@


1.33
log
@New cpu_fork API to take a stack in which you point the child's stackpointer
to, at the bottom or the top, depending on your architecture's stack growth
direction.  This is in preparation for Linux' clone(2) emulation.
port maintainers, please check that I did the work right.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.32 1999/06/23 09:44:29 art Exp $    */
d284 1
a284 1
	cpu_fork(p1, p2, stack, stack_size);
@


1.32
log
@Improved sysv shared memory. Works with UVM.
Original work done in FreeBSD, but this code was ported from NetBSD by
Chuck Cranor.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.31 1999/02/19 19:21:40 art Exp $    */
d214 1
a214 1
vm_fork(p1, p2)
d216 2
d276 1
a276 1
	return (cpu_fork(p1, p2));
d284 1
a284 1
	cpu_fork(p1, p2);
@


1.31
log
@Allocate the u-area early in fork1 instead of in vm_fork.
Now we can return ENOMEM instead of doing a panic when we run out of memory.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.30 1999/02/01 16:29:48 pefo Exp $    */
d231 1
a231 1
		shmfork(p1, p2);
@


1.30
log
@More general conditional for mips
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.29 1998/03/01 00:38:05 niklas Exp $    */
d217 1
a217 2
	register struct user *up;
	vm_offset_t addr;
d234 2
a235 24
#if !defined(vax)
	/*
	 * Allocate a wired-down (for now) pcb and kernel stack for the process
	 */
#if defined(arc) || defined(mips_cachealias)
	addr = kmem_alloc_upage(kernel_map, USPACE);
#else
	addr = kmem_alloc_pageable(kernel_map, USPACE);
#endif
	if (addr == 0)
		panic("vm_fork: no more kernel virtual memory");
	vm_map_pageable(kernel_map, addr, addr + USPACE, FALSE);
#else
	/*
	 * XXX somehow, on 386, ocassionally pageout removes active, wired down
	 * kstack and pagetables, WITHOUT going thru vm_page_unwire! Why this
	 * appears to work is not yet clear, yet it does...
	 */
	addr = kmem_alloc(kernel_map, USPACE);
	if (addr == 0)
		panic("vm_fork: no more kernel virtual memory");
#endif
	up = (struct user *)addr;
	p2->p_addr = up;
@


1.29
log
@Merge of MACHINE_NEW_CONTIG (aka MNN) code from Chuck Cranor,
<chuck@@openbsd.org>. This code is as of yet disabled on all platforms,
actually not yet supported on more than mvme68k, although other
platforms are expected soon, as code is already available.
This code makes handling of multiple physical memory regions
consistent over all platforms, as well as keeping the performance of
maintaining a single continuous memory chunk.  It is also a
requirement for the upcoming UVM replacement VM system.

What I did in this merge: just declared the pmap_map function in a
MD include file per port that needs it.  It's not an exported pmap
interface, says Chuck.  It ended up in differnt include files on
differnet ports, as I tried to follow the current policy on a per-arch
basis.
@
text
@d239 1
a239 1
#if defined(arc) || defined(pica)
@


1.28
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.27 1997/11/06 05:59:32 csapuntz Exp $    */
d444 1
a444 1
	VM_WAIT;
a558 75
}

/*
 * The rest of these routines fake thread handling
 */

void
assert_wait(event, ruptible)
	void *event;
	boolean_t ruptible;
{
#ifdef lint
	ruptible++;
#endif
	curproc->p_thread = event;
}

void
thread_block()
{
	int s = splhigh();

	if (curproc->p_thread)
		tsleep(curproc->p_thread, PVM, "thrd_block", 0);
	splx(s);
}

void
thread_sleep_msg(event, lock, ruptible, msg)
	void *event;
	simple_lock_t lock;
	boolean_t ruptible;
	char *msg;
{
	int s = splhigh();

#ifdef lint
	ruptible++;
#endif
	curproc->p_thread = event;
	simple_unlock(lock);
	if (curproc->p_thread)
		tsleep(event, PVM, msg, 0);
	splx(s);
}

/*
 * DEBUG stuff
 */

int indent = 0;

#include <machine/stdarg.h>		/* see subr_prf.c */

/*ARGSUSED2*/
void
#if __STDC__
iprintf(int (*pr)(const char *, ...), const char *fmt, ...)
#else
iprintf(pr, fmt /* , va_alist */)
	void (*pr)();
	char *fmt;
	/* va_dcl */
#endif
{
	register int i;
	va_list ap;

	for (i = indent; i >= 8; i -= 8)
		(*pr)("\t");
	while (--i >= 0)
		(*pr)(" ");
	va_start(ap, fmt);
	(*pr)("%:", fmt, ap);
	va_end(ap);
@


1.27
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.26 1997/10/06 20:21:18 deraadt Exp $    */
d110 1
a110 1
		   saddr < (vm_offset_t)buffers + MAXBSIZE * nbuf))
d112 1
a112 1
	return(rv == TRUE);
d136 1
a136 1
	return(rv == TRUE);
d185 2
a186 2
	return vm_map_pageable(&curproc->p_vmspace->vm_map, trunc_page(addr),
			       round_page(addr+len), FALSE);
d194 2
a195 2
	return vm_map_pageable(&curproc->p_vmspace->vm_map, trunc_page(addr),
			       round_page(addr+len), TRUE);
d277 2
a278 1
	{ vm_offset_t addr = VM_MAXUSER_ADDRESS; struct vm_map *vp;
d280 6
a285 5
	/* ream out old pagetables and kernel stack */
	vp = &p2->p_vmspace->vm_map;
	(void)vm_deallocate(vp, addr, VM_MAX_ADDRESS - addr);
	(void)vm_allocate(vp, &addr, VM_MAX_ADDRESS - addr, FALSE);
	(void)vm_map_inherit(vp, addr, VM_MAX_ADDRESS, VM_INHERIT_NONE);
a394 1

d424 2
a425 2
			       p->p_pid, p->p_comm, p->p_addr,
			       ppri, cnt.v_free_count);
d441 1
a441 1
		       p->p_pid, p->p_comm, cnt.v_free_count);
d443 1
a443 1
	(void) splhigh();
d445 1
a445 1
	(void) spl0();
d455 1
a455 1
	 (p)->p_holdcnt == 0)
d532 2
a533 2
		       p->p_pid, p->p_comm, p->p_addr, p->p_stat,
		       p->p_slptime, cnt.v_free_count);
@


1.26
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.24 1997/07/25 06:03:07 mickey Exp $    */
d39 1
a39 1
 *	@@(#)vm_glue.c	8.6 (Berkeley) 1/5/94
d384 2
a385 1
	while (!enableswap)
d387 1
d393 1
d417 1
@


1.25
log
@VFS Lite2 Changes
@
text
@d39 1
a39 1
 *	@@(#)vm_glue.c	8.9 (Berkeley) 3/4/95
d384 1
a384 2
	while (!enableswap) {
		panic ("swap disabled??");
a385 1
	}
a390 1

a413 1
	 * XXX should require at least vm_swrss / 2
@


1.24
log
@tabify
some /lx/p/ printf changes
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.23 1997/04/17 01:25:18 niklas Exp $    */
d39 1
a39 1
 *	@@(#)vm_glue.c	8.6 (Berkeley) 1/5/94
d384 2
a385 1
	while (!enableswap)
d387 1
d393 1
d417 1
@


1.23
log
@Removal of race conditions.  Inspired by Charles Hannum's
<mycroft@@netbsd.org> reorganization of the vm_collapse logic, although not
used verbatim.  We no longer collapse objects from the pagedaemon as that
is not necessary anymore with the more aggressive collapses that gets done.
This also increases performance of loaded systems.  Much KNF too.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.22 1997/01/07 05:37:36 tholo Exp $    */
d323 4
a326 4
        p->p_rlimit[RLIMIT_STACK].rlim_cur = DFLSSIZ;
        p->p_rlimit[RLIMIT_STACK].rlim_max = MAXSSIZ;
        p->p_rlimit[RLIMIT_DATA].rlim_cur = DFLDSIZ;
        p->p_rlimit[RLIMIT_DATA].rlim_max = MAXDSIZ;
@


1.22
log
@Fix for final ptdi panic on i386
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.21 1996/11/23 23:19:54 kstailey Exp $    */
d77 1
d235 1
a235 1
#if !defined(pc532) && !defined(vax)
d582 1
a582 1
thread_sleep(event, lock, ruptible)
d586 1
d596 1
a596 1
		tsleep(event, PVM, "thrd_sleep", 0);
@


1.21
log
@remrq -> remrunqueue
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.20 1996/08/02 00:05:59 niklas Exp $    */
d176 1
a176 1
void
d181 5
a185 2
	vm_map_pageable(&curproc->p_vmspace->vm_map, trunc_page(addr),
			round_page(addr+len), FALSE);
d188 1
a188 1
void
d193 2
a194 2
	vm_map_pageable(&curproc->p_vmspace->vm_map, trunc_page(addr),
			round_page(addr+len), TRUE);
@


1.20
log
@Fix long-standing swap-leak. Add OpenBSD tags. Optimize thread_wakeup.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.19 1996/07/23 23:54:22 deraadt Exp $    */
d546 1
a546 1
		remrq(p);
@


1.19
log
@make printf/addlog return 0, for compat to userland
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.18 1996/06/24 20:00:51 pefo Exp $    */
a591 10
	splx(s);
}

void
thread_wakeup(event)
	void *event;
{
	int s = splhigh();

	wakeup(event);
@


1.18
log
@arc and pica needs special alignment
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.17 1996/06/20 10:51:08 deraadt Exp $    */
d616 1
a616 1
iprintf(void (*pr)(const char *, ...), const char *fmt, ...)
@


1.17
log
@tick v_swpin & v_swpout
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.16 1996/05/28 12:16:32 deraadt Exp $    */
d235 1
a235 1
#ifdef pica
d418 1
a418 1
#ifdef pica
@


1.16
log
@thread changes
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_glue.c,v 1.15 1996/05/22 12:03:13 deraadt Exp $    */
/*	$NetBSD: vm_glue.c,v 1.55 1996/05/19 10:00:38 ragge Exp $	*/
d360 1
d549 1
@


1.15
log
@ragge: The unexpected remove of active kstack does happen on vax also,
not only on pc532. I've verivied that this "fix" work.
@
text
@d1 1
a1 1
/*	$OpenBSD: vm_glue.c,v 1.13 1996/04/19 16:10:47 niklas Exp $    */
d548 53
@


1.14
log
@partial sync with netbsd 960418, more to come
@
text
@d2 1
a2 1
/*	$NetBSD: vm_glue.c,v 1.54 1996/03/30 21:50:45 christos Exp $	*/
d231 1
a231 1
#if !defined(pc532)
@


1.13
log
@NetBSD 960317 merge
@
text
@d1 2
a2 2
/*	$OpenBSD: vm_glue.c,v 1.11 1996/03/24 17:05:35 tholo Exp $    */
/*	$NetBSD: vm_glue.c,v 1.53 1996/02/18 22:53:43 mycroft Exp $	*/
d577 1
a577 1
	(*pr)("%r", fmt, ap);
@


1.12
log
@NetBSD 960317 merge
@
text
@@


1.11
log
@iprint is used without DEBUG too
@
text
@d1 2
a2 2
/*    $OpenBSD: vm_glue.c,v 1.10 1996/03/19 21:10:55 mickey Exp $    */
/*    $NetBSD: vm_glue.c,v 1.52 1996/02/12 21:51:59 christos Exp $    */
d231 1
a231 1
#if !defined(i386) && !defined(pc532)
@


1.10
log
@Merging w/ NetBSD 021796.
speaker upgraded to the current.
some changes to the VM stuff (ie kern_thread.c added and so).
@
text
@d1 1
a1 1
/*    $OpenBSD: vm_glue.c,v 1.9 1996/03/03 17:45:29 niklas Exp $    */
a552 1
#ifdef	DEBUG
a579 2
#endif

@


1.9
log
@From NetBSD: merge with 960217
@
text
@d1 1
a1 1
/*    $OpenBSD$    */
a550 53
 * The rest of these routines fake thread handling
 */

void
assert_wait(event, ruptible)
	void *event;
	boolean_t ruptible;
{
#ifdef lint
	ruptible++;
#endif
	curproc->p_thread = event;
}

void
thread_block()
{
	int s = splhigh();

	if (curproc->p_thread)
		tsleep(curproc->p_thread, PVM, "thrd_block", 0);
	splx(s);
}

void
thread_sleep(event, lock, ruptible)
	void *event;
	simple_lock_t lock;
	boolean_t ruptible;
{
	int s = splhigh();

#ifdef lint
	ruptible++;
#endif
	curproc->p_thread = event;
	simple_unlock(lock);
	if (curproc->p_thread)
		tsleep(event, PVM, "thrd_sleep", 0);
	splx(s);
}

void
thread_wakeup(event)
	void *event;
{
	int s = splhigh();

	wakeup(event);
	splx(s);
}

/*
d553 1
d581 2
@


1.8
log
@Make swap accounting actually work...
@
text
@d1 2
a2 1
/*	$NetBSD: vm_glue.c,v 1.48 1995/12/09 04:28:19 mycroft Exp $	*/
d72 3
d186 1
a186 1
vsunlock(addr, len, dirtied)
a188 1
	int dirtied;
a189 3
#ifdef	lint
	dirtied++;
#endif
d395 1
a395 1
		printf("scheduler: running, procp %x pri %d\n", pp, ppri);
d413 1
a413 1
			printf("swapin: pid %d(%s)@@%x, pri %d free %d\n",
d505 1
a505 1
			printf("swapout_threads: no duds, try procp %x\n", p);
d521 1
a521 1
		printf("swapout: pid %d(%s)@@%x, stat %x pri %d free %d\n",
@


1.7
log
@Count swapins and swapouts in the sum structure
@
text
@a359 2
	cnt.v_swpin++;
	cnt.v_pswpin += USPACE * CLSIZE / NBPG;
a547 2
	cnt.v_swpin++;
	cnt.v_pswpin += USPACE * CLSIZE / NBPG;
@


1.6
log
@from netbsd; in swapin() do process queue removal at splstatclock instead of splhigh
@
text
@d360 2
d550 2
@


1.5
log
@put back the pica stuff i killed
@
text
@d517 1
a517 1
	vm_size_t size;
d535 2
a536 3
	addr = (vm_offset_t) p->p_addr;
	size = round_page(USPACE);
	vm_map_pageable(kernel_map, addr, addr+size, TRUE);
d542 1
a542 1
	(void) splhigh();
d546 1
a546 1
	(void) spl0();
@


1.4
log
@from netbsd:
Extend use of vm_object_prefer() to vm_allocate_with_pager().
Make vm_object_prefer() call MD aligner for "pageless" objects too,
so we can have more control over the virtual address to be used.

Implementation could be simpler if we by-pass the object to mapped, but
we'd loose the ability to adapt alignment to objects that were previously
mmap'ed with MAP_FIXED on.

Only expect vm_fork() to return if __FORK_BRAINDAMAGE is defined.
Eliminate unused third arg to vm_fork().
@
text
@d235 3
d239 1
d416 4
@


1.3
log
@vm_map_pageable(..,FALSE) of upage is pica specific
@
text
@d1 1
a1 1
/*	$NetBSD: vm_glue.c,v 1.46 1995/05/05 03:35:39 cgd Exp $	*/
d205 1
d207 4
a210 1
vm_fork(p1, p2, isvfork)
a211 1
	int isvfork;
d228 1
a228 1
		shmfork(p1, p2, isvfork);
a234 3
#ifdef pica
	addr = kmem_alloc_upage(kernel_map, USPACE);
#else
a235 1
#endif
d278 2
d288 9
d352 1
a352 1
	        setrunqueue(p);
a411 4
#endif
#ifdef pica
		vm_map_pageable(kernel_map, (vm_offset_t)p->p_addr,
		    (vm_offset_t)p->p_addr + atop(USPACE), FALSE);
@


1.2
log
@pica required changes for upage; from pefo
@
text
@d403 4
a406 2
		vm_map_pageable(kernel_map, p->p_addr,
		    p->p-addr + size, FALSE);
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: vm_glue.c,v 1.47 1995/08/13 09:04:47 mycroft Exp $	*/
d232 3
d236 1
d342 1
a342 1
		setrunqueue(p);
d403 2
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

