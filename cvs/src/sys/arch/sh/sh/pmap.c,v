head	1.28;
access;
symbols
	OPENBSD_6_2:1.28.0.2
	OPENBSD_6_2_BASE:1.28
	OPENBSD_6_1:1.28.0.4
	OPENBSD_6_1_BASE:1.28
	OPENBSD_6_0:1.25.0.2
	OPENBSD_6_0_BASE:1.25
	OPENBSD_5_9:1.23.0.2
	OPENBSD_5_9_BASE:1.23
	OPENBSD_5_8:1.22.0.6
	OPENBSD_5_8_BASE:1.22
	OPENBSD_5_7:1.22.0.2
	OPENBSD_5_7_BASE:1.22
	OPENBSD_5_6:1.21.0.6
	OPENBSD_5_6_BASE:1.21
	OPENBSD_5_5:1.21.0.4
	OPENBSD_5_5_BASE:1.21
	OPENBSD_5_4:1.20.0.2
	OPENBSD_5_4_BASE:1.20
	OPENBSD_5_3:1.19.0.8
	OPENBSD_5_3_BASE:1.19
	OPENBSD_5_2:1.19.0.6
	OPENBSD_5_2_BASE:1.19
	OPENBSD_5_1_BASE:1.19
	OPENBSD_5_1:1.19.0.4
	OPENBSD_5_0:1.19.0.2
	OPENBSD_5_0_BASE:1.19
	OPENBSD_4_9:1.17.0.2
	OPENBSD_4_9_BASE:1.17
	OPENBSD_4_8:1.14.0.2
	OPENBSD_4_8_BASE:1.14
	OPENBSD_4_7:1.13.0.2
	OPENBSD_4_7_BASE:1.13
	OPENBSD_4_6:1.11.0.6
	OPENBSD_4_6_BASE:1.11
	OPENBSD_4_5:1.11.0.2
	OPENBSD_4_5_BASE:1.11
	OPENBSD_4_4:1.9.0.2
	OPENBSD_4_4_BASE:1.9
	OPENBSD_4_3:1.8.0.2
	OPENBSD_4_3_BASE:1.8
	OPENBSD_4_2:1.7.0.2
	OPENBSD_4_2_BASE:1.7
	OPENBSD_4_1:1.6.0.2
	OPENBSD_4_1_BASE:1.6
	SH_20061006:1.1.1.1
	miod:1.1.1;
locks; strict;
comment	@ * @;


1.28
date	2016.10.19.08.28.20;	author guenther;	state Exp;
branches;
next	1.27;
commitid	VoR9X3uHTxRSYC5r;

1.27
date	2016.10.19.01.34.47;	author guenther;	state Exp;
branches;
next	1.26;
commitid	5uAmQhp24WansmmM;

1.26
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.25;
commitid	RlO92XR575sygHqm;

1.25
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.24;
commitid	N0upL0onl7Raz5yi;

1.24
date	2016.03.05.17.16.33;	author tobiasu;	state Exp;
branches;
next	1.23;
commitid	tnf9BWAlxRCsKh8M;

1.23
date	2015.09.08.21.28.36;	author kettenis;	state Exp;
branches;
next	1.22;
commitid	WSD3bUAMn8qMj0PM;

1.22
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.21;
commitid	yv0ECmCdICvq576h;

1.21
date	2014.01.20.21.19.28;	author guenther;	state Exp;
branches;
next	1.20;

1.20
date	2013.03.02.22.44.47;	author guenther;	state Exp;
branches;
next	1.19;

1.19
date	2011.08.09.20.21.44;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2011.04.28.20.40.36;	author ariane;	state Exp;
branches;
next	1.17;

1.17
date	2010.12.14.20.24.25;	author jasper;	state Exp;
branches;
next	1.16;

1.16
date	2010.12.06.20.57.17;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2010.11.20.20.33.24;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2010.04.21.03.03.26;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	2010.01.01.13.17.52;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2009.07.23.19.20.56;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.10;

1.10
date	2008.09.12.12.27.27;	author blambert;	state Exp;
branches;
next	1.9;

1.9
date	2008.06.26.05.42.13;	author ray;	state Exp;
branches;
next	1.8;

1.8
date	2007.09.01.12.08.17;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2007.06.21.04.41.21;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2007.03.05.21.48.23;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2007.03.05.21.47.55;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2007.03.05.21.39.22;	author drahn;	state Exp;
branches;
next	1.3;

1.3
date	2006.11.03.03.38.08;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2006.10.06.21.16.57;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	2006.10.06.21.02.55;	author miod;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.10.06.21.02.55;	author miod;	state Exp;
branches;
next	;


desc
@@


1.28
log
@Change pmap_proc_iflush() to take a process instead of a proc
powerpc: rename second argument of pmap_proc_iflush() to match other archs

ok kettenis@@
@
text
@/*	$OpenBSD: pmap.c,v 1.27 2016/10/19 01:34:47 guenther Exp $	*/
/*	$NetBSD: pmap.c,v 1.55 2006/08/07 23:19:36 tsutsui Exp $	*/

/*-
 * Copyright (c) 2002 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by UCHIYAMA Yasushi.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/pool.h>
#include <sys/msgbuf.h>

#include <uvm/uvm.h>

#include <sh/mmu.h>
#include <sh/cache.h>

#ifdef DEBUG
#define	STATIC
#else
#define	STATIC	static
#endif

#define	__PMAP_PTP_SHIFT	22
#define	__PMAP_PTP_TRUNC(va)						\
	(((va) + (1 << __PMAP_PTP_SHIFT) - 1) & ~((1 << __PMAP_PTP_SHIFT) - 1))
#define	__PMAP_PTP_PG_N		(PAGE_SIZE / sizeof(pt_entry_t))
#define	__PMAP_PTP_INDEX(va)	(((va) >> __PMAP_PTP_SHIFT) & (__PMAP_PTP_N - 1))
#define	__PMAP_PTP_OFSET(va)	((va >> PGSHIFT) & (__PMAP_PTP_PG_N - 1))

struct pmap __pmap_kernel;
STATIC vaddr_t __pmap_kve;	/* VA of last kernel virtual */

/* For the fast tlb miss handler */
pt_entry_t **curptd;		/* p1 va of curlwp->...->pm_ptp */

/* pmap pool */
STATIC struct pool __pmap_pmap_pool;

/* pv_entry ops. */
struct pv_entry {
	struct pmap *pv_pmap;
	vaddr_t pv_va;
	vm_prot_t pv_prot;
	SLIST_ENTRY(pv_entry) pv_link;
};
#define	__pmap_pv_alloc()	pool_get(&__pmap_pv_pool, PR_NOWAIT)
#define	__pmap_pv_free(pv)	pool_put(&__pmap_pv_pool, (pv))
STATIC int __pmap_pv_enter(pmap_t, struct vm_page *, vaddr_t, vm_prot_t);
STATIC void __pmap_pv_remove(pmap_t, struct vm_page *, vaddr_t);
STATIC void *__pmap_pv_page_alloc(struct pool *, int, int *);
STATIC void __pmap_pv_page_free(struct pool *, void *);
STATIC struct pool __pmap_pv_pool;
STATIC struct pool_allocator pmap_pv_page_allocator = {
	__pmap_pv_page_alloc, __pmap_pv_page_free, 0,
};

/* ASID ops. */
STATIC int __pmap_asid_alloc(void);
STATIC void __pmap_asid_free(int);
STATIC struct {
	uint32_t map[8];
	int hint;	/* hint for next allocation */
} __pmap_asid;

/* page table entry ops. */
STATIC pt_entry_t *__pmap_pte_alloc(pmap_t, vaddr_t);

/* pmap_enter util */
STATIC boolean_t __pmap_map_change(pmap_t, vaddr_t, paddr_t, vm_prot_t,
    pt_entry_t);

void
pmap_bootstrap(void)
{
	/* Steal msgbuf area */
	initmsgbuf((caddr_t)uvm_pageboot_alloc(MSGBUFSIZE), MSGBUFSIZE);

	__pmap_kve = VM_MIN_KERNEL_ADDRESS;

	pmap_kernel()->pm_refcnt = 1;
	pmap_kernel()->pm_ptp = (pt_entry_t **)uvm_pageboot_alloc(PAGE_SIZE);
	memset(pmap_kernel()->pm_ptp, 0, PAGE_SIZE);

	/* Enable MMU */
	sh_mmu_start();
	/* Mask all interrupt */
	_cpu_intr_suspend();
	/* Enable exception for P3 access */
	_cpu_exception_resume(0);
}

vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstart, vaddr_t *vend)
{
	struct vm_physseg *bank;
	int i, j, npage;
	paddr_t pa;
	vaddr_t va;

	KDASSERT(!uvm.page_init_done);

	size = round_page(size);
	npage = atop(size);

	for (i = 0, bank = &vm_physmem[i]; i < vm_nphysseg; i++, bank++)
		if (npage <= bank->avail_end - bank->avail_start)
			break;
	KDASSERT(i != vm_nphysseg);

	/* Steal pages */
	bank->avail_end -= npage;
	bank->end -= npage;
	pa = ptoa(bank->avail_end);

	/* GC memory bank */
	if (bank->avail_start == bank->end) {
		/* Remove this segment from the list. */
		vm_nphysseg--;
		KDASSERT(vm_nphysseg > 0);
		for (j = i; i < vm_nphysseg; j++)
			vm_physmem[j] = vm_physmem[j + 1];
	}

	va = SH3_PHYS_TO_P1SEG(pa);
	memset((caddr_t)va, 0, size);

	if (vstart)
		*vstart = VM_MIN_KERNEL_ADDRESS;
	if (vend)
		*vend = VM_MAX_KERNEL_ADDRESS;

	return (va);
}

vaddr_t
pmap_growkernel(vaddr_t maxkvaddr)
{
	int i, n;

	if (maxkvaddr <= __pmap_kve)
		return (__pmap_kve);

	i = __PMAP_PTP_INDEX(__pmap_kve - VM_MIN_KERNEL_ADDRESS);
	__pmap_kve = __PMAP_PTP_TRUNC(maxkvaddr);
	n = __PMAP_PTP_INDEX(__pmap_kve - VM_MIN_KERNEL_ADDRESS);

	/* Allocate page table pages */
	for (;i < n; i++) {
		if (__pmap_kernel.pm_ptp[i] != NULL)
			continue;

		if (uvm.page_init_done) {
			struct vm_page *pg = uvm_pagealloc(NULL, 0, NULL,
			    UVM_PGA_USERESERVE | UVM_PGA_ZERO);
			if (pg == NULL)
				goto error;
			__pmap_kernel.pm_ptp[i] = (pt_entry_t *)
			    SH3_PHYS_TO_P1SEG(VM_PAGE_TO_PHYS(pg));
		} else {
			pt_entry_t *ptp = (pt_entry_t *)
			    uvm_pageboot_alloc(PAGE_SIZE);
			if (ptp == NULL)
				goto error;
			__pmap_kernel.pm_ptp[i] = ptp;
			memset(ptp, 0, PAGE_SIZE);
		}
	}

	return (__pmap_kve);
 error:
	panic("pmap_growkernel: out of memory.");
	/* NOTREACHED */
}

void
pmap_init(void)
{
	/* Initialize pmap module */
	pool_init(&__pmap_pmap_pool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmappl", &pool_allocator_single);
	pool_init(&__pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pvpl", &pmap_pv_page_allocator);
	pool_setlowat(&__pmap_pv_pool, 16);
}

pmap_t
pmap_create(void)
{
	pmap_t pmap;
	struct vm_page *pg;

	pmap = pool_get(&__pmap_pmap_pool, PR_WAITOK|PR_ZERO);
	pmap->pm_asid = -1;
	pmap->pm_refcnt = 1;
	/* Allocate page table page holder (512 slot) */
	while ((pg = uvm_pagealloc(NULL, 0, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO)) == NULL)
		uvm_wait("pmap_create");
			
	pmap->pm_ptp = (pt_entry_t **)SH3_PHYS_TO_P1SEG(VM_PAGE_TO_PHYS(pg));

	return (pmap);
}

void
pmap_destroy(pmap_t pmap)
{
	int i;

	if (--pmap->pm_refcnt > 0)
		return;

	/* Deallocate all page table page */
	for (i = 0; i < __PMAP_PTP_N; i++) {
		vaddr_t va = (vaddr_t)pmap->pm_ptp[i];
		if (va == 0)
			continue;
#ifdef DEBUG	/* Check no mapping exists. */
		{
			int j;
			pt_entry_t *pte = (pt_entry_t *)va;
			for (j = 0; j < __PMAP_PTP_PG_N; j++, pte++)
				KDASSERT(*pte == 0);
		}
#endif /* DEBUG */
		/* Purge cache entry for next use of this page. */
		if (SH_HAS_VIRTUAL_ALIAS)
			sh_dcache_inv_range(va, PAGE_SIZE);
		/* Free page table */
		uvm_pagefree(PHYS_TO_VM_PAGE(SH3_P1SEG_TO_PHYS(va)));
	}
	/* Deallocate page table page holder */
	if (SH_HAS_VIRTUAL_ALIAS)
		sh_dcache_inv_range((vaddr_t)pmap->pm_ptp, PAGE_SIZE);
	uvm_pagefree(PHYS_TO_VM_PAGE(SH3_P1SEG_TO_PHYS((vaddr_t)pmap->pm_ptp)));

	/* Free ASID */
	__pmap_asid_free(pmap->pm_asid);

	pool_put(&__pmap_pmap_pool, pmap);
}

void
pmap_reference(pmap_t pmap)
{
	pmap->pm_refcnt++;
}

void
pmap_activate(struct proc *p)
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

	if (pmap->pm_asid == -1)
		pmap->pm_asid = __pmap_asid_alloc();

	KDASSERT(pmap->pm_asid >=0 && pmap->pm_asid < 256);

	sh_tlb_set_asid(pmap->pm_asid);
	curptd = pmap->pm_ptp;
}

int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	struct vm_page *pg;
	struct vm_page_md *pvh;
	pt_entry_t entry, *pte;
	boolean_t kva = (pmap == pmap_kernel());

	/* "flags" never exceed "prot" */
	KDASSERT(prot != 0 && ((flags & PROT_MASK) & ~prot) == 0);

	pg = PHYS_TO_VM_PAGE(pa);
	entry = (pa & PG_PPN) | PG_4K;
	if (flags & PMAP_WIRED)
		entry |= _PG_WIRED;

	if (pg != NULL) {	/* memory-space */
		pvh = &pg->mdpage;
		entry |= PG_C;	/* always cached */

		/* Modified/reference tracking */
		if (flags & PROT_WRITE) {
			entry |= PG_V | PG_D;
			pvh->pvh_flags |= PVH_MODIFIED | PVH_REFERENCED;
		} else if (flags & PROT_MASK) {
			entry |= PG_V;
			pvh->pvh_flags |= PVH_REFERENCED;
		}

		/* Protection */
		if ((prot & PROT_WRITE) && (pvh->pvh_flags & PVH_MODIFIED)) {
			if (kva)
				entry |= PG_PR_KRW | PG_SH;
			else
				entry |= PG_PR_URW;
		} else {
			/* RO, COW page */
			if (kva)
				entry |= PG_PR_KRO | PG_SH;
			else
				entry |= PG_PR_URO;
		}

		/* Check for existing mapping */
		if (__pmap_map_change(pmap, va, pa, prot, entry))
			return (0);

		/* Add to physical-virtual map list of this page */
		if (__pmap_pv_enter(pmap, pg, va, prot) != 0) {
			if (flags & PMAP_CANFAIL)
				return (ENOMEM);
			panic("pmap_enter: cannot allocate pv entry");
		}
	} else {	/* bus-space (always uncached map) */
		if (kva) {
			entry |= PG_V | PG_SH |
			    ((prot & PROT_WRITE) ?
			    (PG_PR_KRW | PG_D) : PG_PR_KRO);
		} else {
			entry |= PG_V |
			    ((prot & PROT_WRITE) ?
			    (PG_PR_URW | PG_D) : PG_PR_URO);
		}
	}

	/* Register to page table */
	if (kva)
		pte = __pmap_kpte_lookup(va);
	else {
		pte = __pmap_pte_alloc(pmap, va);
		if (pte == NULL) {
			if (flags & PMAP_CANFAIL)
				return ENOMEM;
			panic("pmap_enter: cannot allocate pte");
		}
	}

	*pte = entry;

	if (pmap->pm_asid != -1)
		sh_tlb_update(pmap->pm_asid, va, entry);

	if (!SH_HAS_UNIFIED_CACHE &&
	    (prot == (PROT_READ | PROT_EXEC)))
		sh_icache_sync_range_index(va, PAGE_SIZE);

	if (entry & _PG_WIRED)
		pmap->pm_stats.wired_count++;
	pmap->pm_stats.resident_count++;

	return (0);
}

/*
 * boolean_t __pmap_map_change(pmap_t pmap, vaddr_t va, paddr_t pa,
 *     vm_prot_t prot, pt_entry_t entry):
 *	Handle the situation that pmap_enter() is called to enter a
 *	mapping at a virtual address for which a mapping already
 *	exists.
 */
boolean_t
__pmap_map_change(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot,
    pt_entry_t entry)
{
	pt_entry_t *pte, oentry;
	vaddr_t eva = va + PAGE_SIZE;

	if ((pte = __pmap_pte_lookup(pmap, va)) == NULL ||
	    ((oentry = *pte) == 0))
		return (FALSE);		/* no mapping exists. */

	if (pa != (oentry & PG_PPN)) {
		/* Enter a mapping at a mapping to another physical page. */
		pmap_remove(pmap, va, eva);
		return (FALSE);
	}

	/* Pre-existing mapping */

	/* Protection change. */
	if ((oentry & PG_PR_MASK) != (entry & PG_PR_MASK))
		pmap_protect(pmap, va, eva, prot);

	/* Wired change */
	if (oentry & _PG_WIRED) {
		if (!(entry & _PG_WIRED)) {
			/* wired -> unwired */
			*pte = entry;
			/* "wired" is software bits. no need to update TLB */
			pmap->pm_stats.wired_count--;
		}
	} else if (entry & _PG_WIRED) {
		/* unwired -> wired. make sure to reflect "flags" */
		pmap_remove(pmap, va, eva);
		return (FALSE);
	}

	return (TRUE);	/* mapping was changed. */
}

/*
 * int __pmap_pv_enter(pmap_t pmap, struct vm_page *pg, vaddr_t vaddr):
 *	Insert physical-virtual map to vm_page.
 *	Assume pre-existed mapping is already removed.
 */
int
__pmap_pv_enter(pmap_t pmap, struct vm_page *pg, vaddr_t va, vm_prot_t prot)
{
	struct vm_page_md *pvh;
	struct pv_entry *pv;
	int s;
	int have_writeable = 0;

	s = splvm();
	if (SH_HAS_VIRTUAL_ALIAS) {
		/* Remove all other mapping on this physical page */
		pvh = &pg->mdpage;
		if (prot & PROT_WRITE)
			have_writeable = 1;
		else {
			SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
				if (pv->pv_prot & PROT_WRITE) {
					have_writeable = 1;
					break;
				}
			}
		}
		if (have_writeable != 0) {
			while ((pv = SLIST_FIRST(&pvh->pvh_head)) != NULL)
				pmap_remove(pv->pv_pmap, pv->pv_va,
				    pv->pv_va + PAGE_SIZE);
		}
	}

	/* Register pv map */
	pvh = &pg->mdpage;
	pv = __pmap_pv_alloc();
	if (pv == NULL) {
		splx(s);
		return (ENOMEM);
	}

	pv->pv_pmap = pmap;
	pv->pv_va = va;
	pv->pv_prot = prot;

	SLIST_INSERT_HEAD(&pvh->pvh_head, pv, pv_link);
	splx(s);
	return (0);
}

void
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{
	struct vm_page *pg;
	pt_entry_t *pte, entry;
	vaddr_t va;

	KDASSERT((sva & PGOFSET) == 0);

	for (va = sva; va < eva; va += PAGE_SIZE) {
		if ((pte = __pmap_pte_lookup(pmap, va)) == NULL ||
		    (entry = *pte) == 0)
			continue;

		if ((pg = PHYS_TO_VM_PAGE(entry & PG_PPN)) != NULL)
			__pmap_pv_remove(pmap, pg, va);

		if (entry & _PG_WIRED)
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;
		*pte = 0;

		/*
		 * When pmap->pm_asid == -1 (invalid ASID), old entry attribute
		 * to this pmap is already removed by pmap_activate().
		 */
		if (pmap->pm_asid != -1)
			sh_tlb_invalidate_addr(pmap->pm_asid, va);
	}
}

/*
 * void __pmap_pv_remove(pmap_t pmap, struct vm_page *pg, vaddr_t vaddr):
 *	Remove physical-virtual map from vm_page.
 */
void
__pmap_pv_remove(pmap_t pmap, struct vm_page *pg, vaddr_t vaddr)
{
	struct vm_page_md *pvh;
	struct pv_entry *pv;
	int s;

	s = splvm();
	pvh = &pg->mdpage;
	SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
		if (pv->pv_pmap == pmap && pv->pv_va == vaddr) {
			if (SH_HAS_VIRTUAL_ALIAS ||
			    (SH_HAS_WRITEBACK_CACHE &&
				(pg->mdpage.pvh_flags & PVH_MODIFIED))) {
				/*
				 * Always use index ops. since I don't want to
				 * worry about address space.
				 */
				sh_dcache_wbinv_range_index
				    (pv->pv_va, PAGE_SIZE);
			}

			SLIST_REMOVE(&pvh->pvh_head, pv, pv_entry, pv_link);
			__pmap_pv_free(pv);
			break;
		}
	}
#ifdef DEBUG
	/* Check duplicated map. */
	SLIST_FOREACH(pv, &pvh->pvh_head, pv_link)
	    KDASSERT(!(pv->pv_pmap == pmap && pv->pv_va == vaddr));
#endif
	splx(s);
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *pte, entry;

	KDASSERT((va & PGOFSET) == 0);
	KDASSERT(va >= VM_MIN_KERNEL_ADDRESS && va < VM_MAX_KERNEL_ADDRESS);

	entry = (pa & PG_PPN) | PG_V | PG_SH | PG_4K;
	if (prot & PROT_WRITE)
		entry |= (PG_PR_KRW | PG_D);
	else
		entry |= PG_PR_KRO;

	if (PHYS_TO_VM_PAGE(pa))
		entry |= PG_C;

	pte = __pmap_kpte_lookup(va);

	KDASSERT(*pte == 0);
	*pte = entry;

	sh_tlb_update(0, va, entry);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	pt_entry_t *pte;
	vaddr_t eva = va + len;

	KDASSERT((va & PGOFSET) == 0);
	KDASSERT((len & PGOFSET) == 0);
	KDASSERT(va >= VM_MIN_KERNEL_ADDRESS && eva <= VM_MAX_KERNEL_ADDRESS);

	for (; va < eva; va += PAGE_SIZE) {
		pte = __pmap_kpte_lookup(va);
		KDASSERT(pte != NULL);
		if (*pte == 0)
			continue;

		if (SH_HAS_VIRTUAL_ALIAS && PHYS_TO_VM_PAGE(*pte & PG_PPN))
			sh_dcache_wbinv_range(va, PAGE_SIZE);
		*pte = 0;

		sh_tlb_invalidate_addr(0, va);
	}
}

boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *pte;

	/* handle P1 and P2 specially: va == pa */
	if (pmap == pmap_kernel() && (va >> 30) == 2) {
		if (pap != NULL)
			*pap = va & SH3_PHYS_MASK;
		return (TRUE);
	}

	pte = __pmap_pte_lookup(pmap, va);
	if (pte == NULL || *pte == 0)
		return (FALSE);

	if (pap != NULL)
		*pap = (*pte & PG_PPN) | (va & PGOFSET);

	return (TRUE);
}

void
pmap_protect(pmap_t pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	boolean_t kernel = pmap == pmap_kernel();
	pt_entry_t *pte, entry, protbits;
	vaddr_t va;
	paddr_t pa;
	struct vm_page *pg;
	struct vm_page_md *pvh;
	struct pv_entry *pv, *head;

	sva = trunc_page(sva);

	if ((prot & PROT_READ) == PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	switch (prot) {
	default:
		panic("pmap_protect: invalid protection mode %x", prot);
		/* NOTREACHED */
	case PROT_READ:
		/* FALLTHROUGH */
	case PROT_READ | PROT_EXEC:
		protbits = kernel ? PG_PR_KRO : PG_PR_URO;
		break;
	case PROT_READ | PROT_WRITE:
		/* FALLTHROUGH */
	case PROT_MASK:
		protbits = kernel ? PG_PR_KRW : PG_PR_URW;
		break;
	}

	for (va = sva; va < eva; va += PAGE_SIZE) {

		if (((pte = __pmap_pte_lookup(pmap, va)) == NULL) ||
		    (entry = *pte) == 0)
			continue;

		if (SH_HAS_VIRTUAL_ALIAS && (entry & PG_D)) {
			if (!SH_HAS_UNIFIED_CACHE && (prot & PROT_EXEC))
				sh_icache_sync_range_index(va, PAGE_SIZE);
			else
				sh_dcache_wbinv_range_index(va, PAGE_SIZE);
		}

		entry = (entry & ~PG_PR_MASK) | protbits;
		*pte = entry;

		if (pmap->pm_asid != -1)
			sh_tlb_update(pmap->pm_asid, va, entry);

		pa = entry & PG_PPN;
		pg = PHYS_TO_VM_PAGE(pa);
		if (pg == NULL)
			continue;
		pvh = &pg->mdpage;

		while ((pv = SLIST_FIRST(&pvh->pvh_head)) != NULL) {
			if (pv->pv_pmap == pmap && pv->pv_va == va) {
				break;
			}
			pmap_remove(pv->pv_pmap, pv->pv_va,
			    pv->pv_va + PAGE_SIZE);
		}
		/* the matching pv is first in the list */
		SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
			if (pv->pv_pmap == pmap && pv->pv_va == va) {
				pv->pv_prot = prot;
				break;
			}
		}
		/* remove the rest of the elements */
		head = SLIST_FIRST(&pvh->pvh_head);
		if (head != NULL)
			while((pv = SLIST_NEXT(head, pv_link))!= NULL)
				pmap_remove(pv->pv_pmap, pv->pv_va,
				    pv->pv_va + PAGE_SIZE);
	}
}

void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	struct vm_page_md *pvh = &pg->mdpage;
	struct pv_entry *pv;
	struct pmap *pmap;
	vaddr_t va;
	int s;

	switch (prot) {
	case PROT_READ | PROT_WRITE:
		/* FALLTHROUGH */
	case PROT_MASK:
		break;

	case PROT_READ:
		/* FALLTHROUGH */
	case PROT_READ | PROT_EXEC:
		s = splvm();
		SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
			pmap = pv->pv_pmap;
			va = pv->pv_va;

			KDASSERT(pmap);
			pmap_protect(pmap, va, va + PAGE_SIZE, prot);
		}
		splx(s);
		break;

	default:
		/* Remove all */
		s = splvm();
		while ((pv = SLIST_FIRST(&pvh->pvh_head)) != NULL) {
			va = pv->pv_va;
			pmap_remove(pv->pv_pmap, va, va + PAGE_SIZE);
		}
		splx(s);
	}
}

void
pmap_unwire(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *pte, entry;

	if ((pte = __pmap_pte_lookup(pmap, va)) == NULL ||
	    (entry = *pte) == 0 ||
	    (entry & _PG_WIRED) == 0)
		return;

	*pte = entry & ~_PG_WIRED;
	pmap->pm_stats.wired_count--;
}

void
pmap_proc_iflush(struct process *pr, vaddr_t va, vsize_t len)
{
	if (!SH_HAS_UNIFIED_CACHE)
		sh_icache_sync_range_index(va, len);
}

void
pmap_zero_page(vm_page_t pg)
{
	paddr_t phys = VM_PAGE_TO_PHYS(pg);

	if (SH_HAS_VIRTUAL_ALIAS) {	/* don't pollute cache */
		/* sync cache since we access via P2. */
		sh_dcache_wbinv_all();
		memset((void *)SH3_PHYS_TO_P2SEG(phys), 0, PAGE_SIZE);
	} else {
		memset((void *)SH3_PHYS_TO_P1SEG(phys), 0, PAGE_SIZE);
	}
}

void
pmap_copy_page(vm_page_t srcpg, vm_page_t dstpg)
{
	paddr_t src,dst;
       
	src = VM_PAGE_TO_PHYS(srcpg);
	dst = VM_PAGE_TO_PHYS(dstpg);

	if (SH_HAS_VIRTUAL_ALIAS) {	/* don't pollute cache */
		/* sync cache since we access via P2. */
		sh_dcache_wbinv_all();
		memcpy((void *)SH3_PHYS_TO_P2SEG(dst),
		    (void *)SH3_PHYS_TO_P2SEG(src), PAGE_SIZE);
	} else {
		memcpy((void *)SH3_PHYS_TO_P1SEG(dst),
		    (void *)SH3_PHYS_TO_P1SEG(src), PAGE_SIZE);
	}
}

boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	return ((pg->mdpage.pvh_flags & PVH_REFERENCED) ? TRUE : FALSE);
}

boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	struct vm_page_md *pvh = &pg->mdpage;
	struct pv_entry *pv;
	pt_entry_t *pte;
	pmap_t pmap;
	vaddr_t va;
	int s;

	if ((pg->mdpage.pvh_flags & PVH_REFERENCED) == 0)
		return (FALSE);

	pg->mdpage.pvh_flags &= ~PVH_REFERENCED;

	s = splvm();
	/* Restart reference bit emulation */
	SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
		pmap = pv->pv_pmap;
		va = pv->pv_va;

		if ((pte = __pmap_pte_lookup(pmap, va)) == NULL)
			continue;
		if ((*pte & PG_V) == 0)
			continue;
		*pte &= ~PG_V;

		if (pmap->pm_asid != -1)
			sh_tlb_invalidate_addr(pmap->pm_asid, va);
	}
	splx(s);

	return (TRUE);
}

boolean_t
pmap_is_modified(struct vm_page *pg)
{
	return ((pg->mdpage.pvh_flags & PVH_MODIFIED) ? TRUE : FALSE);
}

boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	struct vm_page_md *pvh = &pg->mdpage;
	struct pv_entry *pv;
	struct pmap *pmap;
	pt_entry_t *pte, entry;
	boolean_t modified;
	vaddr_t va;
	int s;

	modified = pvh->pvh_flags & PVH_MODIFIED;
	if (!modified)
		return (FALSE);

	pvh->pvh_flags &= ~PVH_MODIFIED;

	s = splvm();
	if (SLIST_EMPTY(&pvh->pvh_head)) {/* no map on this page */
		splx(s);
		return (TRUE);
	}

	/* Write-back and invalidate TLB entry */
	if (!SH_HAS_VIRTUAL_ALIAS && SH_HAS_WRITEBACK_CACHE)
		sh_dcache_wbinv_all();

	SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
		pmap = pv->pv_pmap;
		va = pv->pv_va;
		if ((pte = __pmap_pte_lookup(pmap, va)) == NULL)
			continue;
		entry = *pte;
		if ((entry & PG_D) == 0)
			continue;

		if (SH_HAS_VIRTUAL_ALIAS)
			sh_dcache_wbinv_range_index(va, PAGE_SIZE);

		*pte = entry & ~PG_D;
		if (pmap->pm_asid != -1)
			sh_tlb_invalidate_addr(pmap->pm_asid, va);
	}
	splx(s);

	return (TRUE);
}

#ifdef SH4
/*
 * pmap_prefer(vaddr_t foff, vaddr_t *vap)
 *
 * Find first virtual address >= *vap that doesn't cause
 * a virtual cache alias against vaddr_t foff.
 */
vaddr_t
pmap_prefer(vaddr_t foff, vaddr_t va)
{
	if (SH_HAS_VIRTUAL_ALIAS)
		va += ((foff - va) & sh_cache_prefer_mask);

	return va;
}

/*
 * pmap_prefer_align()
 *
 * Return virtual cache alignment.
 */
vaddr_t
pmap_prefer_align(void)
{
	return SH_HAS_VIRTUAL_ALIAS ? sh_cache_prefer_mask + 1 : 0;
}

/*
 * pmap_prefer_offset(vaddr_t of)
 *
 * Calculate offset in virtual cache.
 */
vaddr_t
pmap_prefer_offset(vaddr_t of)
{
	return of & (SH_HAS_VIRTUAL_ALIAS ? sh_cache_prefer_mask : 0);
}
#endif /* SH4 */

/*
 * pv_entry pool allocator:
 *	void *__pmap_pv_page_alloc(struct pool *pool, int flags, int *slowdown):
 *	void __pmap_pv_page_free(struct pool *pool, void *v):
 */
void *
__pmap_pv_page_alloc(struct pool *pool, int flags, int *slowdown)
{
	struct vm_page *pg;

	*slowdown = 0;
	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (pg == NULL)
		return (NULL);

	return ((void *)SH3_PHYS_TO_P1SEG(VM_PAGE_TO_PHYS(pg)));
}

void
__pmap_pv_page_free(struct pool *pool, void *v)
{
	vaddr_t va = (vaddr_t)v;

	/* Invalidate cache for next use of this page */
	if (SH_HAS_VIRTUAL_ALIAS)
		sh_dcache_inv_range(va, PAGE_SIZE);
	uvm_pagefree(PHYS_TO_VM_PAGE(SH3_P1SEG_TO_PHYS(va)));
}

/*
 * pt_entry_t __pmap_pte_alloc(pmap_t pmap, vaddr_t va):
 *	lookup page table entry. if found returns it, else allocate it.
 *	page table is accessed via P1.
 */
pt_entry_t *
__pmap_pte_alloc(pmap_t pmap, vaddr_t va)
{
	struct vm_page *pg;
	pt_entry_t *ptp, *pte;

	if ((pte = __pmap_pte_lookup(pmap, va)) != NULL)
		return (pte);

	/* Allocate page table (not managed page) */
	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE | UVM_PGA_ZERO);
	if (pg == NULL)
		return NULL;

	ptp = (pt_entry_t *)SH3_PHYS_TO_P1SEG(VM_PAGE_TO_PHYS(pg));
	pmap->pm_ptp[__PMAP_PTP_INDEX(va)] = ptp;

	return (ptp + __PMAP_PTP_OFSET(va));
}

/*
 * pt_entry_t *__pmap_pte_lookup(pmap_t pmap, vaddr_t va):
 *	lookup page table entry, if not allocated, returns NULL.
 */
pt_entry_t *
__pmap_pte_lookup(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *ptp;

	if (pmap == pmap_kernel())
		return (__pmap_kpte_lookup(va));

	/* Lookup page table page */
	ptp = pmap->pm_ptp[__PMAP_PTP_INDEX(va)];
	if (ptp == NULL)
		return (NULL);

	return (ptp + __PMAP_PTP_OFSET(va));
}

/*
 * pt_entry_t *__pmap_kpte_lookup(vaddr_t va):
 *	kernel virtual only version of __pmap_pte_lookup().
 */
pt_entry_t *
__pmap_kpte_lookup(vaddr_t va)
{
	pt_entry_t *ptp;

	ptp =
	    __pmap_kernel.pm_ptp[__PMAP_PTP_INDEX(va - VM_MIN_KERNEL_ADDRESS)];
	return (ptp ? ptp + __PMAP_PTP_OFSET(va) : NULL);
}

/*
 * boolean_t __pmap_pte_load(pmap_t pmap, vaddr_t va, int flags):
 *	lookup page table entry, if found it, load to TLB.
 *	flags specify do emulate reference and/or modified bit or not.
 */
boolean_t
__pmap_pte_load(pmap_t pmap, vaddr_t va, int flags)
{
	struct vm_page *pg;
	pt_entry_t *pte;
	pt_entry_t entry;

	KDASSERT((((int)va < 0) && (pmap == pmap_kernel())) ||
	    (((int)va >= 0) && (pmap != pmap_kernel())));

	/* Lookup page table entry */
	if (((pte = __pmap_pte_lookup(pmap, va)) == NULL) ||
	    ((entry = *pte) == 0))
		return (FALSE);

	KDASSERT(va != 0);

	/* Emulate reference/modified tracking for managed page. */
	if (flags != 0 && (pg = PHYS_TO_VM_PAGE(entry & PG_PPN)) != NULL) {
		if (flags & PVH_REFERENCED) {
			pg->mdpage.pvh_flags |= PVH_REFERENCED;
			entry |= PG_V;
		}
		if (flags & PVH_MODIFIED) {
			pg->mdpage.pvh_flags |= PVH_MODIFIED;
			entry |= PG_D;
		}
		*pte = entry;
	}

	/* When pmap has valid ASID, register to TLB */
	if (pmap->pm_asid != -1)
		sh_tlb_update(pmap->pm_asid, va, entry);

	return (TRUE);
}

/*
 * int __pmap_asid_alloc(void):
 *	Allocate new ASID. if all ASID are used, steal from other process.
 */
int
__pmap_asid_alloc(void)
{
	struct process *pr;
	int i, j, k, n, map, asid;

	/* Search free ASID */
	i = __pmap_asid.hint >> 5;
	n = i + 8;
	for (; i < n; i++) {
		k = i & 0x7;
		map = __pmap_asid.map[k];
		for (j = 0; j < 32; j++) {
			if ((map & (1 << j)) == 0 && (k + j) != 0) {
				__pmap_asid.map[k] |= (1 << j);
				__pmap_asid.hint = (k << 5) + j;
				return (__pmap_asid.hint);
			}
		}
	}

	/* Steal ASID */
	/*
	 * XXX this always steals the ASID of the *newest* proc with one,
	 * so it's far from LRU but rather almost pessimal once you have
	 * too many processes.
	 */
	LIST_FOREACH(pr, &allprocess, ps_list) {
		pmap_t pmap = pr->ps_vmspace->vm_map.pmap;

		if ((asid = pmap->pm_asid) > 0) {
			pmap->pm_asid = -1;
			__pmap_asid.hint = asid;
			/* Invalidate all old ASID entry */
			sh_tlb_invalidate_asid(asid);

			return (__pmap_asid.hint);
		}
	}

	panic("No ASID allocated.");
	/* NOTREACHED */
}

/*
 * void __pmap_asid_free(int asid):
 *	Return unused ASID to pool. and remove all TLB entry of ASID.
 */
void
__pmap_asid_free(int asid)
{
	int i;

	if (asid < 1)	/* Don't invalidate kernel ASID 0 */
		return;

	sh_tlb_invalidate_asid(asid);

	i = asid >> 5;
	__pmap_asid.map[i] &= ~(1 << (asid - (i << 5)));
}

/*
 * Routines used by PMAP_MAP_DIRECT() and PMAP_UNMAP_DIRECT() to provide
 * directly-translated pages.
 *
 * Because of cache virtual aliases, it is necessary to evict these pages
 * from the cache, when `unmapping' them (as they might be reused by a
 * different allocator). We also rely upon all users of pages to either
 * use them with pmap_enter()/pmap_remove(), to enforce proper cache handling,
 * or to invoke sh_dcache_inv_range() themselves, as done for page tables.
 */
vaddr_t
pmap_map_direct(vm_page_t pg)
{
	return SH3_PHYS_TO_P1SEG(VM_PAGE_TO_PHYS(pg));
}

vm_page_t
pmap_unmap_direct(vaddr_t va)
{
	paddr_t pa = SH3_P1SEG_TO_PHYS(va);
	vm_page_t pg = PHYS_TO_VM_PAGE(pa);

	if (SH_HAS_VIRTUAL_ALIAS)
		sh_dcache_inv_range(va, PAGE_SIZE);

	return pg;
}
@


1.27
log
@struct process has a pointer to the vmspace now, so simplify the ASID search

ok deraadt@@ jca@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2016/09/15 02:00:17 dlg Exp $	*/
d759 1
a759 1
pmap_proc_iflush(struct proc *p, vaddr_t va, size_t len)
@


1.26
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2016/06/07 06:23:19 dlg Exp $	*/
a1069 1
	struct proc *p;
d1094 3
a1096 8
		/* find a thread that still has the process vmspace attached */
		TAILQ_FOREACH(p, &pr->ps_threads, p_thr_link)
			if (p->p_vmspace != NULL)
				break;
		if (p == NULL)
			continue;
		if ((asid = p->p_vmspace->vm_map.pmap->pm_asid) > 0) {
			pmap_t pmap = p->p_vmspace->vm_map.pmap;
@


1.25
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2016/03/05 17:16:33 tobiasu Exp $	*/
d206 1
a206 1
	pool_init(&__pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,
d208 2
a209 4
	pool_setipl(&__pmap_pmap_pool, IPL_NONE);
	pool_init(&__pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
	    &pmap_pv_page_allocator);
	pool_setipl(&__pmap_pv_pool, IPL_VM);
@


1.24
log
@Sync no-argument function declaration and definition by adding (void).
Covers all of landisk/sh, per naddy's suggestion.

ok on previous diff deraadt@@ naddy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2015/09/08 21:28:36 kettenis Exp $	*/
d206 3
a208 2
	pool_init(&__pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_single);
d211 1
@


1.23
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2014/11/16 12:30:58 deraadt Exp $	*/
d100 1
a100 1
pmap_bootstrap()
d203 1
a203 1
pmap_init()
d214 1
a214 1
pmap_create()
d1067 1
a1067 1
__pmap_asid_alloc()
@


1.22
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2014/01/20 21:19:28 guenther Exp $	*/
d207 1
a207 1
	    &pool_allocator_nointr);
@


1.21
log
@Threads can't be zombies, only processes, so change zombproc to zombprocess,
make it a list of processes, and change P_NOZOMBIE and P_STOPPED from thread
flags to process flags.  Add allprocess list for the code that just wants
to see processes.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2013/03/02 22:44:47 guenther Exp $	*/
d299 1
a299 1
	KDASSERT(prot != 0 && ((flags & VM_PROT_ALL) & ~prot) == 0);
d311 1
a311 1
		if (flags & VM_PROT_WRITE) {
d314 1
a314 1
		} else if (flags & VM_PROT_ALL) {
d320 1
a320 1
		if ((prot & VM_PROT_WRITE) && (pvh->pvh_flags & PVH_MODIFIED)) {
d346 1
a346 1
			    ((prot & VM_PROT_WRITE) ?
d350 1
a350 1
			    ((prot & VM_PROT_WRITE) ?
d373 1
a373 1
	    (prot == (VM_PROT_READ | VM_PROT_EXECUTE)))
d447 1
a447 1
		if (prot & VM_PROT_WRITE)
d451 1
a451 1
				if (pv->pv_prot & VM_PROT_WRITE) {
d560 1
a560 1
	if (prot & VM_PROT_WRITE)
d635 1
a635 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
d644 1
a644 1
	case VM_PROT_READ:
d646 1
a646 1
	case VM_PROT_READ | VM_PROT_EXECUTE:
d649 1
a649 1
	case VM_PROT_READ | VM_PROT_WRITE:
d651 1
a651 1
	case VM_PROT_ALL:
d663 1
a663 1
			if (!SH_HAS_UNIFIED_CACHE && (prot & VM_PROT_EXECUTE))
d714 1
a714 1
	case VM_PROT_READ | VM_PROT_WRITE:
d716 1
a716 1
	case VM_PROT_ALL:
d719 1
a719 1
	case VM_PROT_READ:
d721 1
a721 1
	case VM_PROT_READ | VM_PROT_EXECUTE:
@


1.20
log
@When stealing an ASID, pass sh_tlb_invalid_asid() the involved ASID
instead of -1.

ok and prod from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2011/08/09 20:21:44 miod Exp $	*/
d1069 1
d1089 12
a1100 1
	LIST_FOREACH(p, &allproc, p_list) {
@


1.19
log
@In pmap_create(), check uvm_pagealloc() return value and uvm_wait() until it
is non-NULL. Prevents a panic caused by a NULL dereference when physical
memory is exhausted at pmap_create() time.
ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2011/04/28 20:40:36 ariane Exp $	*/
d1094 1
a1094 1
			sh_tlb_invalidate_asid(pmap->pm_asid);
@


1.18
log
@Expose pmap_prefer parameters.
This enables future uvm_map code to make intelligent decisions.

Code is not called at the moment.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2010/12/14 20:24:25 jasper Exp $	*/
d217 1
d223 5
a227 4
	pmap->pm_ptp = (pt_entry_t **)
	    SH3_PHYS_TO_P1SEG(VM_PAGE_TO_PHYS(
		    uvm_pagealloc(NULL, 0, NULL,
			UVM_PGA_USERESERVE | UVM_PGA_ZERO)));
@


1.17
log
@"Implement fast path TLB miss handling.  Walk the page table without
creating a trapframe, with exceptions disabled and using only BANK1
registers.  If a valid pte is found, load it and return.  Otherwise
create a trapframe and proceed to the full-blown C handler."

from uwe@@netbsd, ok miod@@

speed-ups measured by miod@@ and me were between 44% and 50%...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2010/12/06 20:57:17 miod Exp $	*/
d905 22
@


1.16
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2010/11/20 20:33:24 miod Exp $	*/
d60 3
d283 1
d285 1
@


1.15
log
@This is a first step towards getting rid of avail_start and avail_end in the
kernel, currently limited to low-hanging fruit: these variables were used
by bus_dma to specify the range in which to allocate memory, back when
uvm_pglistalloc() was stupid and would not walk the vm_physseg[].

Nowadays, except on some platforms for early initialization, these variables
are not used, or do not need to be global variables. Therefore:
- remove `extern' declarations of avail_start and avail_end (or close cousins,
  such as arm physical_start and physical_end) from files which no longer need
  to use them.
- make them local variables whenever possible.
- remove them when they are assigned to but no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2010/04/21 03:03:26 deraadt Exp $	*/
d893 2
a894 2
void
pmap_prefer(vaddr_t foff, vaddr_t *vap)
d896 2
a897 1
	vaddr_t va;
d899 1
a899 5
	if (SH_HAS_VIRTUAL_ALIAS) {
		va = *vap;

		*vap = va + ((foff - va) & sh_cache_prefer_mask);
	}
@


1.14
log
@more cleanup to cope with the change that tries to make proc.h not act
like it is everything.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2010/01/01 13:17:52 miod Exp $	*/
a58 2
paddr_t avail_start;		/* PA of first available physical page */
paddr_t avail_end;		/* PA of last available physical page */
a101 2
	avail_start = ptoa(vm_physmem[0].start);
	avail_end = ptoa(vm_physmem[vm_nphysseg - 1].end);
@


1.13
log
@Make sure page is cache invalidated in pmap_unmap_direct(), for the next use
of this page may use different cache indexes.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2009/07/23 19:20:56 miod Exp $	*/
d35 1
@


1.12
log
@Allow __pmap_pv_enter() to fail, instead of dereferencing NULL; and let the
caller deal with this; this really makes the PMAP_CANFAIL logic work.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2008/10/23 23:54:02 tedu Exp $	*/
d934 1
a934 1
		sh_icache_sync_range_index(va, PAGE_SIZE);
d1097 28
@


1.11
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 2008/09/12 12:27:27 blambert Exp $	*/
d73 1
a73 1
STATIC void __pmap_pv_enter(pmap_t, struct vm_page *, vaddr_t, vm_prot_t);
d334 5
a338 2
		__pmap_pv_enter(pmap, pg, va, prot);

d427 1
a427 1
 * void __pmap_pv_enter(pmap_t pmap, struct vm_page *pg, vaddr_t vaddr):
d431 1
a431 1
void
d463 5
d474 1
@


1.10
log
@Remove bzero/memset calls after pool_gets by passing the PR_ZERO
flag to the pool_get call.

ok art@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 2008/06/26 05:42:13 ray Exp $	*/
d75 1
a75 1
STATIC void *__pmap_pv_page_alloc(struct pool *, int);
d902 1
a902 1
 *	void *__pmap_pv_page_alloc(struct pool *pool, int flags):
d906 1
a906 1
__pmap_pv_page_alloc(struct pool *pool, int flags)
d910 1
@


1.9
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2007/09/01 12:08:17 miod Exp $	*/
d218 1
a218 2
	pmap = pool_get(&__pmap_pmap_pool, PR_WAITOK);
	memset(pmap, 0, sizeof(struct pmap));
@


1.8
log
@In pmap_protect(), compute the bitmask to set in the ptes only once, instead
of every iteration.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 2007/06/21 04:41:21 miod Exp $	*/
a18 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *        This product includes software developed by the NetBSD
 *        Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.7
log
@Simple optimizations:
- in __pmap_pv_enter, only walk the pv list to search for writeable mappings
  if we are ading a readonly mapping.
- im pmap_protect, do not invoke pmap_extract(), instead directly extract
  the paddr from the pte we have already computed a few lines above.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 2007/03/05 21:48:23 miod Exp $	*/
d621 1
a621 1
	pt_entry_t *pte, entry;
d635 16
d664 1
a664 16
		entry &= ~PG_PR_MASK;
		switch (prot) {
		default:
			panic("pmap_protect: invalid protection mode %x", prot);
			/* NOTREACHED */
		case VM_PROT_READ:
			/* FALLTHROUGH */
		case VM_PROT_READ | VM_PROT_EXECUTE:
			entry |= kernel ? PG_PR_KRO : PG_PR_URO;
			break;
		case VM_PROT_READ | VM_PROT_WRITE:
			/* FALLTHROUGH */
		case VM_PROT_ALL:
			entry |= kernel ? PG_PR_KRW : PG_PR_URW;
			break;
		}
d1039 1
a1039 1
 *	Allocate new ASID. if all ASID is used, steal from other process.
@


1.6
log
@Add pmap_prefer() for SH4, from NetBSD; ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 2007/03/05 21:47:55 miod Exp $	*/
a443 3
	if (prot & VM_PROT_WRITE)
		have_writeable = 1;

d448 8
a455 4
		SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
			if (pv->pv_prot & VM_PROT_WRITE) {
				have_writeable = 1;
				break;
d669 5
a673 5
		if (pmap_extract(pmap, va, &pa)) {
			pg = PHYS_TO_VM_PAGE(pa);
			if (pg == NULL)
				continue;
			pvh = &pg->mdpage;
d675 3
a677 6
			while ((pv = SLIST_FIRST(&pvh->pvh_head)) != NULL) {
				if (pv->pv_pmap == pmap && pv->pv_va == va) {
					break;
				}
				pmap_remove(pv->pv_pmap, pv->pv_va,
				    pv->pv_va + PAGE_SIZE);
d679 8
a686 6
			/* the matching pv is first in the list */
			SLIST_FOREACH(pv, &pvh->pvh_head, pv_link) {
				if (pv->pv_pmap == pmap && pv->pv_va == va) {
					pv->pv_prot = prot;
					break;
				}
a687 6
			/* remove the rest of the elements */
			head = SLIST_FIRST(&pvh->pvh_head);
			if (head != NULL)
				while((pv = SLIST_NEXT(head, pv_link))!= NULL)
					pmap_remove(pv->pv_pmap, pv->pv_va,
					    pv->pv_va + PAGE_SIZE);
d689 6
@


1.5
log
@Prevent a NULL pointer dereference in __pmap_kptp_lookup(); from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 2007/03/05 21:39:22 drahn Exp $	*/
d886 20
@


1.4
log
@Work around a virtual aliasing conflict issue. Basically allow multiple
readers but only one writer on a physical page. Not the most optimal, but
has been tested. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 2006/11/03 03:38:08 mickey Exp $	*/
d967 5
a971 3
	return (__pmap_kernel.pm_ptp
	    [__PMAP_PTP_INDEX(va - VM_MIN_KERNEL_ADDRESS)] +
	    __PMAP_PTP_OFSET(va));
@


1.3
log
@must steal pages from the end of the physseg
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 2006/10/06 21:16:57 mickey Exp $	*/
d75 1
d80 1
a80 1
STATIC void __pmap_pv_enter(pmap_t, struct vm_page *, vaddr_t);
d342 1
a342 1
		__pmap_pv_enter(pmap, pg, va);
d437 1
a437 1
__pmap_pv_enter(pmap_t pmap, struct vm_page *pg, vaddr_t va)
d442 4
d451 10
a460 3
		while ((pv = SLIST_FIRST(&pvh->pvh_head)) != NULL) {
			pmap_remove(pv->pv_pmap, pv->pv_va,
			    pv->pv_va + PAGE_SIZE);
d469 1
d622 4
d667 28
@


1.2
log
@a few fixes to get thru the autoconf
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d144 3
a146 3
	pa = ptoa(bank->avail_start);
	bank->avail_start += npage;
	bank->start += npage;
@


1.1
log
@Initial revision
@
text
@d160 5
a205 7
}

void
pmap_virtual_space(vaddr_t *start, vaddr_t *end)
{
	*start = VM_MIN_KERNEL_ADDRESS;
	*end = VM_MAX_KERNEL_ADDRESS;
@


1.1.1.1
log
@Preliminary bits for SuperH-based ports, based on NetBSD/sh3 codebase with
minor changes.
@
text
@@
