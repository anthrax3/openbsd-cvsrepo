head	1.7;
access;
symbols
	OPENBSD_6_1:1.7.0.4
	OPENBSD_6_1_BASE:1.7
	OPENBSD_6_0:1.5.0.2
	OPENBSD_6_0_BASE:1.5
	OPENBSD_5_9:1.3.0.8
	OPENBSD_5_9_BASE:1.3
	OPENBSD_5_8:1.3.0.10
	OPENBSD_5_8_BASE:1.3
	OPENBSD_5_7:1.3.0.2
	OPENBSD_5_7_BASE:1.3
	OPENBSD_5_6:1.3.0.6
	OPENBSD_5_6_BASE:1.3
	OPENBSD_5_5:1.3.0.4
	OPENBSD_5_5_BASE:1.3
	OPENBSD_5_4:1.2.0.8
	OPENBSD_5_4_BASE:1.2
	OPENBSD_5_3:1.2.0.6
	OPENBSD_5_3_BASE:1.2
	OPENBSD_5_2:1.2.0.4
	OPENBSD_5_2_BASE:1.2
	OPENBSD_5_1_BASE:1.2
	OPENBSD_5_1:1.2.0.2
	OPENBSD_5_0:1.1.0.34
	OPENBSD_5_0_BASE:1.1
	OPENBSD_4_9:1.1.0.32
	OPENBSD_4_9_BASE:1.1
	OPENBSD_4_8:1.1.0.30
	OPENBSD_4_8_BASE:1.1
	OPENBSD_4_7:1.1.0.26
	OPENBSD_4_7_BASE:1.1
	OPENBSD_4_6:1.1.0.28
	OPENBSD_4_6_BASE:1.1
	OPENBSD_4_5:1.1.0.24
	OPENBSD_4_5_BASE:1.1
	OPENBSD_4_4:1.1.0.22
	OPENBSD_4_4_BASE:1.1
	OPENBSD_4_3:1.1.0.20
	OPENBSD_4_3_BASE:1.1
	OPENBSD_4_2:1.1.0.18
	OPENBSD_4_2_BASE:1.1
	OPENBSD_4_1:1.1.0.16
	OPENBSD_4_1_BASE:1.1
	OPENBSD_4_0:1.1.0.14
	OPENBSD_4_0_BASE:1.1
	OPENBSD_3_9:1.1.0.12
	OPENBSD_3_9_BASE:1.1
	OPENBSD_3_8:1.1.0.10
	OPENBSD_3_8_BASE:1.1
	OPENBSD_3_7:1.1.0.8
	OPENBSD_3_7_BASE:1.1
	OPENBSD_3_6:1.1.0.6
	OPENBSD_3_6_BASE:1.1
	SMP_SYNC_A:1.1
	SMP_SYNC_B:1.1
	OPENBSD_3_5:1.1.0.4
	OPENBSD_3_5_BASE:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@# @;


1.7
date	2017.01.06.00.06.02;	author jsg;	state Exp;
branches;
next	1.6;
commitid	YCbIRye8xzoWl68V;

1.6
date	2016.09.21.11.33.05;	author kettenis;	state Exp;
branches;
next	1.5;
commitid	iKZYl5ksAPvBHoAc;

1.5
date	2016.04.25.04.46.56;	author jsg;	state Exp;
branches;
next	1.4;
commitid	XqdsV6PrfEUXwK4U;

1.4
date	2016.04.04.09.13.44;	author patrick;	state Exp;
branches;
next	1.3;
commitid	kyi1uK5Qbg4pXWh9;

1.3
date	2013.09.12.11.43.51;	author patrick;	state Exp;
branches;
next	1.2;

1.2
date	2011.09.20.22.02.10;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2004.02.01.05.09.48;	author drahn;	state Exp;
branches;
next	;


desc
@@


1.7
log
@unifdef CPU_ARMv7 and ARM_ARCH_7
ok kettenis@@ patrick@@
@
text
@/*	$OpenBSD: bcopyinout.S,v 1.6 2016/09/21 11:33:05 kettenis Exp $	*/
/*	$NetBSD: bcopyinout.S,v 1.13 2003/10/31 16:54:05 scw Exp $	*/

/*
 * Copyright (c) 2002 Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Allen Briggs for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed for the NetBSD Project by
 *      Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#include "assym.h"

#include <machine/asm.h>
#include <arm/sysreg.h>

#ifdef __XSCALE__
#include "bcopyinout_xscale.S"
#else

	.text
	.align	2

#ifdef MULTIPROCESSOR
.Lcpu_info:
	.word	_C_LABEL(cpu_info)
#else
.Lcpu_info_primary:
	.word _C_LABEL(cpu_info_primary)
#endif

#define SAVE_REGS	stmfd	sp!, {r4-r11}
#define RESTORE_REGS	ldmfd	sp!, {r4-r11}
		
#if defined(__XSCALE__)
#define HELLOCPP #
#define PREFETCH(rx,o)	pld	[ rx , HELLOCPP (o) ]
#else
#define PREFETCH(rx,o)
#endif

/*
 * r0 = user space address
 * r1 = kernel space address
 * r2 = length
 *
 * Copies bytes from user space to kernel space
 *
 * We save/restore r4-r11:
 * r4-r11 are scratch
 */
ENTRY(copyin)
	/* Quick exit if length is zero */	
	teq	r2, #0
	moveq	r0, #0
	moveq	pc, lr

	SAVE_REGS

	/* Get curcpu from TPIDRPRW. */
	mrc	CP15_TPIDRPRW(r4)
	ldr	r4, [r4, #CI_CURPCB]

	ldr	r5, [r4, #PCB_ONFAULT]
	adr	r3, .Lcopyfault
	str	r3, [r4, #PCB_ONFAULT]

	PREFETCH(r0, 0)
	PREFETCH(r1, 0)

	/*
	 * If not too many bytes, take the slow path.
	 */
	cmp	r2, #0x08
	blt	.Licleanup

	/*
	 * Align destination to word boundary.
	 */
	and	r6, r1, #0x3
	ldr	pc, [pc, r6, lsl #2]
	b	.Lialend
	.word	.Lialend
	.word	.Lial3
	.word	.Lial2
	.word	.Lial1
.Lial3:	ldrbt	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lial2:	ldrbt	r7, [r0], #1
	sub	r2, r2, #1
	strb	r7, [r1], #1
.Lial1:	ldrbt	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lialend:

	/*
	 * If few bytes left, finish slow.
	 */
	cmp	r2, #0x08
	blt	.Licleanup

	/*
	 * If source is not aligned, finish slow.
	 */
	ands	r3, r0, #0x03
	bne	.Licleanup

	cmp	r2, #0x60	/* Must be > 0x5f for unrolled cacheline */
	blt	.Licleanup8

	/*
	 * Align destination to cacheline boundary.
	 * If source and destination are nicely aligned, this can be a big
	 * win.  If not, it's still cheaper to copy in groups of 32 even if
	 * we don't get the nice cacheline alignment.
	 */
	and	r6, r1, #0x1f
	ldr	pc, [pc, r6]
	b	.Licaligned
	.word	.Licaligned
	.word	.Lical28
	.word	.Lical24
	.word	.Lical20
	.word	.Lical16
	.word	.Lical12
	.word	.Lical8
	.word	.Lical4
.Lical28:ldrt	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4
.Lical24:ldrt	r7, [r0], #4
	sub	r2, r2, #4
	str	r7, [r1], #4
.Lical20:ldrt	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4
.Lical16:ldrt	r7, [r0], #4
	sub	r2, r2, #4
	str	r7, [r1], #4
.Lical12:ldrt	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4
.Lical8:ldrt	r7, [r0], #4
	sub	r2, r2, #4
	str	r7, [r1], #4
.Lical4:ldrt	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4

	/*
	 * We start with > 0x40 bytes to copy (>= 0x60 got us into this
	 * part of the code, and we may have knocked that down by as much
	 * as 0x1c getting aligned).
	 *
	 * This loop basically works out to:
	 * do {
	 * 	prefetch-next-cacheline(s)
	 *	bytes -= 0x20;
	 *	copy cacheline
	 * } while (bytes >= 0x40);
	 * bytes -= 0x20;
	 * copy cacheline
	 */
.Licaligned:
	PREFETCH(r0, 32)
	PREFETCH(r1, 32)

	sub	r2, r2, #0x20

	/* Copy a cacheline */
	ldrt	r10, [r0], #4
	ldrt	r11, [r0], #4
	ldrt	r6, [r0], #4
	ldrt	r7, [r0], #4
	ldrt	r8, [r0], #4
	ldrt	r9, [r0], #4
	stmia	r1!, {r10-r11}
	ldrt	r10, [r0], #4
	ldrt	r11, [r0], #4
	stmia	r1!, {r6-r11}

	cmp	r2, #0x40
	bge	.Licaligned

	sub	r2, r2, #0x20

	/* Copy a cacheline */
	ldrt	r10, [r0], #4
	ldrt	r11, [r0], #4
	ldrt	r6, [r0], #4
	ldrt	r7, [r0], #4
	ldrt	r8, [r0], #4
	ldrt	r9, [r0], #4
	stmia	r1!, {r10-r11}
	ldrt	r10, [r0], #4
	ldrt	r11, [r0], #4
	stmia	r1!, {r6-r11}

	cmp	r2, #0x08
	blt	.Liprecleanup

.Licleanup8:
	ldrt	r8, [r0], #4
	ldrt	r9, [r0], #4
	sub	r2, r2, #8
	stmia	r1!, {r8, r9}
	cmp	r2, #8
	bge	.Licleanup8

.Liprecleanup:
	/*
	 * If we're done, bail.
	 */
	cmp	r2, #0
	beq	.Lout

.Licleanup:
	and	r6, r2, #0x3
	ldr	pc, [pc, r6, lsl #2]
	b	.Licend
	.word	.Lic4
	.word	.Lic1
	.word	.Lic2
	.word	.Lic3
.Lic4:	ldrbt	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lic3:	ldrbt	r7, [r0], #1
	sub	r2, r2, #1
	strb	r7, [r1], #1
.Lic2:	ldrbt	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lic1:	ldrbt	r7, [r0], #1
	subs	r2, r2, #1
	strb	r7, [r1], #1
.Licend:
	bne	.Licleanup

.Liout:
	mov	r0, #0

	str	r5, [r4, #PCB_ONFAULT]
	RESTORE_REGS

	mov	pc, lr

.Lcopyfault:
	str	r5, [r4, #PCB_ONFAULT]
	RESTORE_REGS

	mov	pc, lr

/*
 * r0 = kernel space address
 * r1 = user space address
 * r2 = length
 *
 * Copies bytes from kernel space to user space
 *
 * We save/restore r4-r11:
 * r4-r11 are scratch
 */

ENTRY(copyout)
	/* Quick exit if length is zero */	
	teq	r2, #0
	moveq	r0, #0
	moveq	pc, lr

	SAVE_REGS

	/* Get curcpu from TPIDRPRW. */
	mrc	CP15_TPIDRPRW(r4)
	ldr	r4, [r4, #CI_CURPCB]

	ldr	r5, [r4, #PCB_ONFAULT]
	adr	r3, .Lcopyfault
	str	r3, [r4, #PCB_ONFAULT]

	PREFETCH(r0, 0)
	PREFETCH(r1, 0)

	/*
	 * If not too many bytes, take the slow path.
	 */
	cmp	r2, #0x08
	blt	.Lcleanup

	/*
	 * Align destination to word boundary.
	 */
	and	r6, r1, #0x3
	ldr	pc, [pc, r6, lsl #2]
	b	.Lalend
	.word	.Lalend
	.word	.Lal3
	.word	.Lal2
	.word	.Lal1
.Lal3:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strbt	r6, [r1], #1
.Lal2:	ldrb	r7, [r0], #1
	sub	r2, r2, #1
	strbt	r7, [r1], #1
.Lal1:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strbt	r6, [r1], #1
.Lalend:

	/*
	 * If few bytes left, finish slow.
	 */
	cmp	r2, #0x08
	blt	.Lcleanup

	/*
	 * If source is not aligned, finish slow.
	 */
	ands	r3, r0, #0x03
	bne	.Lcleanup

	cmp	r2, #0x60	/* Must be > 0x5f for unrolled cacheline */
	blt	.Lcleanup8

	/*
	 * Align source & destination to cacheline boundary.
	 */
	and	r6, r1, #0x1f
	ldr	pc, [pc, r6]
	b	.Lcaligned
	.word	.Lcaligned
	.word	.Lcal28
	.word	.Lcal24
	.word	.Lcal20
	.word	.Lcal16
	.word	.Lcal12
	.word	.Lcal8
	.word	.Lcal4
.Lcal28:ldr	r6, [r0], #4
	sub	r2, r2, #4
	strt	r6, [r1], #4
.Lcal24:ldr	r7, [r0], #4
	sub	r2, r2, #4
	strt	r7, [r1], #4
.Lcal20:ldr	r6, [r0], #4
	sub	r2, r2, #4
	strt	r6, [r1], #4
.Lcal16:ldr	r7, [r0], #4
	sub	r2, r2, #4
	strt	r7, [r1], #4
.Lcal12:ldr	r6, [r0], #4
	sub	r2, r2, #4
	strt	r6, [r1], #4
.Lcal8:	ldr	r7, [r0], #4
	sub	r2, r2, #4
	strt	r7, [r1], #4
.Lcal4:	ldr	r6, [r0], #4
	sub	r2, r2, #4
	strt	r6, [r1], #4

	/*
	 * We start with > 0x40 bytes to copy (>= 0x60 got us into this
	 * part of the code, and we may have knocked that down by as much
	 * as 0x1c getting aligned).
	 *
	 * This loop basically works out to:
	 * do {
	 * 	prefetch-next-cacheline(s)
	 *	bytes -= 0x20;
	 *	copy cacheline
	 * } while (bytes >= 0x40);
	 * bytes -= 0x20;
	 * copy cacheline
	 */
.Lcaligned:
	PREFETCH(r0, 32)
	PREFETCH(r1, 32)

	sub	r2, r2, #0x20

	/* Copy a cacheline */
	ldmia	r0!, {r6-r11}
	strt	r6, [r1], #4
	strt	r7, [r1], #4
	ldmia	r0!, {r6-r7}
	strt	r8, [r1], #4
	strt	r9, [r1], #4
	strt	r10, [r1], #4
	strt	r11, [r1], #4
	strt	r6, [r1], #4
	strt	r7, [r1], #4

	cmp	r2, #0x40
	bge	.Lcaligned

	sub	r2, r2, #0x20

	/* Copy a cacheline */
	ldmia	r0!, {r6-r11}
	strt	r6, [r1], #4
	strt	r7, [r1], #4
	ldmia	r0!, {r6-r7}
	strt	r8, [r1], #4
	strt	r9, [r1], #4
	strt	r10, [r1], #4
	strt	r11, [r1], #4
	strt	r6, [r1], #4
	strt	r7, [r1], #4

	cmp	r2, #0x08
	blt	.Lprecleanup

.Lcleanup8:
	ldmia	r0!, {r8-r9}
	sub	r2, r2, #8
	strt	r8, [r1], #4
	strt	r9, [r1], #4
	cmp	r2, #8
	bge	.Lcleanup8

.Lprecleanup:
	/*
	 * If we're done, bail.
	 */
	cmp	r2, #0
	beq	.Lout

.Lcleanup:
	and	r6, r2, #0x3
	ldr	pc, [pc, r6, lsl #2]
	b	.Lcend
	.word	.Lc4
	.word	.Lc1
	.word	.Lc2
	.word	.Lc3
.Lc4:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strbt	r6, [r1], #1
.Lc3:	ldrb	r7, [r0], #1
	sub	r2, r2, #1
	strbt	r7, [r1], #1
.Lc2:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strbt	r6, [r1], #1
.Lc1:	ldrb	r7, [r0], #1
	subs	r2, r2, #1
	strbt	r7, [r1], #1
.Lcend:
	bne	.Lcleanup

.Lout:
	mov	r0, #0

	str	r5, [r4, #PCB_ONFAULT]
	RESTORE_REGS

	mov	pc, lr

/*
 * r0 = kernel space source address
 * r1 = kernel space destination address
 * r2 = length
 *
 * Copies bytes from kernel space to kernel space, aborting on page fault
 *
 * Copy of copyout, but without the ldrt/strt instructions.
 */

ENTRY(kcopy)
	/* Quick exit if length is zero */	
	teq	r2, #0
	moveq	r0, #0
	moveq	pc, lr

	SAVE_REGS

	/* Get curcpu from TPIDRPRW. */
	mrc	CP15_TPIDRPRW(r4)
	ldr	r4, [r4, #CI_CURPCB]

	ldr	r5, [r4, #PCB_ONFAULT]
	adr	r3, .Lcopyfault
	str	r3, [r4, #PCB_ONFAULT]

	PREFETCH(r0, 0)
	PREFETCH(r1, 0)

	/*
	 * If not too many bytes, take the slow path.
	 */
	cmp	r2, #0x08
	blt	.Lkcleanup

	/*
	 * Align destination to word boundary.
	 */
	and	r6, r1, #0x3
	ldr	pc, [pc, r6, lsl #2]
	b	.Lkalend
	.word	.Lkalend
	.word	.Lkal3
	.word	.Lkal2
	.word	.Lkal1
.Lkal3:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lkal2:	ldrb	r7, [r0], #1
	sub	r2, r2, #1
	strb	r7, [r1], #1
.Lkal1:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lkalend:

	/*
	 * If few bytes left, finish slow.
	 */
	cmp	r2, #0x08
	blt	.Lkcleanup

	/*
	 * If source is not aligned, finish slow.
	 */
	ands	r3, r0, #0x03
	bne	.Lkcleanup

	cmp	r2, #0x60	/* Must be > 0x5f for unrolled cacheline */
	blt	.Lkcleanup8

	/*
	 * Align source & destination to cacheline boundary.
	 */
	and	r6, r1, #0x1f
	ldr	pc, [pc, r6]
	b	.Lkcaligned
	.word	.Lkcaligned
	.word	.Lkcal28
	.word	.Lkcal24
	.word	.Lkcal20
	.word	.Lkcal16
	.word	.Lkcal12
	.word	.Lkcal8
	.word	.Lkcal4
.Lkcal28:ldr	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4
.Lkcal24:ldr	r7, [r0], #4
	sub	r2, r2, #4
	str	r7, [r1], #4
.Lkcal20:ldr	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4
.Lkcal16:ldr	r7, [r0], #4
	sub	r2, r2, #4
	str	r7, [r1], #4
.Lkcal12:ldr	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4
.Lkcal8:ldr	r7, [r0], #4
	sub	r2, r2, #4
	str	r7, [r1], #4
.Lkcal4:ldr	r6, [r0], #4
	sub	r2, r2, #4
	str	r6, [r1], #4

	/*
	 * We start with > 0x40 bytes to copy (>= 0x60 got us into this
	 * part of the code, and we may have knocked that down by as much
	 * as 0x1c getting aligned).
	 *
	 * This loop basically works out to:
	 * do {
	 * 	prefetch-next-cacheline(s)
	 *	bytes -= 0x20;
	 *	copy cacheline
	 * } while (bytes >= 0x40);
	 * bytes -= 0x20;
	 * copy cacheline
	 */
.Lkcaligned:
	PREFETCH(r0, 32)
	PREFETCH(r1, 32)

	sub	r2, r2, #0x20

	/* Copy a cacheline */
	ldmia	r0!, {r6-r11}
	stmia	r1!, {r6, r7}
	ldmia	r0!, {r6, r7}
	stmia	r1!, {r8-r11}
	stmia	r1!, {r6, r7}

	cmp	r2, #0x40
	bge	.Lkcaligned

	sub	r2, r2, #0x20

	/* Copy a cacheline */
	ldmia	r0!, {r6-r11}
	stmia	r1!, {r6-r7}
	ldmia	r0!, {r6-r7}
	stmia	r1!, {r8-r11}
	stmia	r1!, {r6-r7}

	cmp	r2, #0x08
	blt	.Lkprecleanup

.Lkcleanup8:
	ldmia	r0!, {r8-r9}
	sub	r2, r2, #8
	stmia	r1!, {r8-r9}
	cmp	r2, #8
	bge	.Lkcleanup8

.Lkprecleanup:
	/*
	 * If we're done, bail.
	 */
	cmp	r2, #0
	beq	.Lkout

.Lkcleanup:
	and	r6, r2, #0x3
	ldr	pc, [pc, r6, lsl #2]
	b	.Lkcend
	.word	.Lkc4
	.word	.Lkc1
	.word	.Lkc2
	.word	.Lkc3
.Lkc4:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lkc3:	ldrb	r7, [r0], #1
	sub	r2, r2, #1
	strb	r7, [r1], #1
.Lkc2:	ldrb	r6, [r0], #1
	sub	r2, r2, #1
	strb	r6, [r1], #1
.Lkc1:	ldrb	r7, [r0], #1
	subs	r2, r2, #1
	strb	r7, [r1], #1
.Lkcend:
	bne	.Lkcleanup

.Lkout:
	mov	r0, #0

	str	r5, [r4, #PCB_ONFAULT]
	RESTORE_REGS

	mov	pc, lr
#endif	/* !__XSCALE__ */

/*
 * int badaddr_read_1(const uint8_t *src, uint8_t *dest)
 *
 * Copies a single 8-bit value from src to dest, returning 0 on success,
 * else EFAULT if a page fault occurred.
 */
ENTRY(badaddr_read_1)
	/* Get curcpu from TPIDRPRW. */
	mrc	CP15_TPIDRPRW(r2)
	ldr	r2, [r2, #CI_CURPCB]
	ldr	ip, [r2, #PCB_ONFAULT]
	adr	r3, 1f
	str	r3, [r2, #PCB_ONFAULT]
	nop
	nop
	nop
	ldrb	r3, [r0]
	nop
	nop
	nop
	strb	r3, [r1]
	mov	r0, #0		/* No fault */
1:	str	ip, [r2, #PCB_ONFAULT]
	mov	pc, lr

/*
 * int badaddr_read_2(const uint16_t *src, uint16_t *dest)
 *
 * Copies a single 16-bit value from src to dest, returning 0 on success,
 * else EFAULT if a page fault occurred.
 */
ENTRY(badaddr_read_2)
	/* Get curcpu from TPIDRPRW. */
	mrc	CP15_TPIDRPRW(r2)
	ldr	r2, [r2, #CI_CURPCB]
	ldr	ip, [r2, #PCB_ONFAULT]
	adr	r3, 1f
	str	r3, [r2, #PCB_ONFAULT]
	nop
	nop
	nop
	ldrh	r3, [r0]
	nop
	nop
	nop
	strh	r3, [r1]
	mov	r0, #0		/* No fault */
1:	str	ip, [r2, #PCB_ONFAULT]
	mov	pc, lr

/*
 * int badaddr_read_4(const uint32_t *src, uint32_t *dest)
 *
 * Copies a single 32-bit value from src to dest, returning 0 on success,
 * else EFAULT if a page fault occurred.
 */
ENTRY(badaddr_read_4)
	/* Get curcpu from TPIDRPRW. */
	mrc	CP15_TPIDRPRW(r2)
	ldr	r2, [r2, #CI_CURPCB]
	ldr	ip, [r2, #PCB_ONFAULT]
	adr	r3, 1f
	str	r3, [r2, #PCB_ONFAULT]
	nop
	nop
	nop
	ldr	r3, [r0]
	nop
	nop
	nop
	str	r3, [r1]
	mov	r0, #0		/* No fault */
1:	str	ip, [r2, #PCB_ONFAULT]
	mov	pc, lr
@


1.6
log
@Modernize arm assembly in the kernel for clang.
Based on a similar diff in bitrig.
No binary change when compiled with gcc.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bcopyinout.S,v 1.5 2016/04/25 04:46:56 jsg Exp $	*/
a86 1
#ifdef CPU_ARMv7
a88 3
#else
	ldr	r4, .Lcpu_info_primary
#endif
a301 1
#ifdef CPU_ARMv7
a303 3
#else
	ldr	r4, .Lcpu_info_primary
#endif
a506 1
#ifdef CPU_ARMv7
a508 3
#else
	ldr	r4, .Lcpu_info_primary
#endif
a690 1
#ifdef CPU_ARMv7
a692 3
#else
	ldr	r2, .Lcpu_info_primary
#endif
a715 1
#ifdef CPU_ARMv7
a717 3
#else
	ldr	r2, .Lcpu_info_primary
#endif
a740 1
#ifdef CPU_ARMv7
a742 3
#else
	ldr	r2, .Lcpu_info_primary
#endif
@


1.5
log
@Switch most of the cp14/cp15 use in .S files over to using sysreg.h

Matched and changed by a script, verified to cause no binary change with
armv7, armish, and zaurus kernels.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bcopyinout.S,v 1.4 2016/04/04 09:13:44 patrick Exp $	*/
d49 1
a49 1
	.align	0
@


1.4
log
@Store curcpu pointer in TPIDRPRW.

This will especially be helpful in future multiprocessor efforts.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bcopyinout.S,v 1.3 2013/09/12 11:43:51 patrick Exp $	*/
d42 1
d89 1
a89 1
	mrc	p15, 0, r4, c13, c0, 4
d308 1
a308 1
	mrc	p15, 0, r4, c13, c0, 4
d517 1
a517 1
	mrc	p15, 0, r4, c13, c0, 4
d705 1
a705 1
	mrc	p15, 0, r2, c13, c0, 4
d734 1
a734 1
	mrc	p15, 0, r2, c13, c0, 4
d763 1
a763 1
	mrc	p15, 0, r2, c13, c0, 4
@


1.3
log
@Store curpcb in cpu_info instead of a global variable, for SMP.

ok rapha@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bcopyinout.S,v 1.2 2011/09/20 22:02:10 miod Exp $	*/
d85 4
a88 8
#ifdef MULTIPROCESSOR
	/* XXX Probably not appropriate for non-Hydra SMPs */
	stmfd	sp!, {r0-r2, r14}
	bl	_C_LABEL(cpu_number)
	ldr	r4, .Lcpu_info
	ldr	r4, [r4, r0, lsl #2]
	ldr	r4, [r4, #CI_CURPCB]
	ldmfd	sp!, {r0-r2, r14}
d91 1
a92 1
#endif
d304 4
a307 8
#ifdef MULTIPROCESSOR
	/* XXX Probably not appropriate for non-Hydra SMPs */
	stmfd	sp!, {r0-r2, r14}
	bl	_C_LABEL(cpu_number)
	ldr	r4, .Lcpu_info
	ldr	r4, [r4, r0, lsl #2]
	ldr	r4, [r4, #CI_CURPCB]
	ldmfd	sp!, {r0-r2, r14}
d310 1
a311 1
#endif
d513 4
a516 8
#ifdef MULTIPROCESSOR
	/* XXX Probably not appropriate for non-Hydra SMPs */
	stmfd	sp!, {r0-r2, r14}
	bl	_C_LABEL(cpu_number)
	ldr	r4, .Lcpu_info
	ldr	r4, [r4, r0, lsl #2]
	ldr	r4, [r4, #CI_CURPCB]
	ldmfd	sp!, {r0-r2, r14}
d519 1
a520 1
#endif
d702 3
a704 8
#ifdef MULTIPROCESSOR
	/* XXX Probably not appropriate for non-Hydra SMPs */
	stmfd	sp!, {r0-r1, r14}
	bl	_C_LABEL(cpu_number)
	ldr	r2, .Lcpu_info
	ldr	r2, [r2, r0, lsl #2]
	ldr	r2, [r2, #CI_CURPCB]
	ldmfd	sp!, {r0-r1, r14}
d707 1
a708 1
#endif
d731 3
a733 8
#ifdef MULTIPROCESSOR
	/* XXX Probably not appropriate for non-Hydra SMPs */
	stmfd	sp!, {r0-r1, r14}
	bl	_C_LABEL(cpu_number)
	ldr	r2, .Lcpu_info
	ldr	r2, [r2, r0, lsl #2]
	ldr	r2, [r2, #CI_CURPCB]
	ldmfd	sp!, {r0-r1, r14}
d736 1
a737 1
#endif
d760 3
a762 8
#ifdef MULTIPROCESSOR
	/* XXX Probably not appropriate for non-Hydra SMPs */
	stmfd	sp!, {r0-r1, r14}
	bl	_C_LABEL(cpu_number)
	ldr	r2, .Lcpu_info
	ldr	r2, [r2, r0, lsl #2]
	ldr	r2, [r2, #CI_CURPCB]
	ldmfd	sp!, {r0-r1, r14}
d765 1
a766 1
#endif
@


1.2
log
@Late spring cleaning of the arm code for old dusty bits we do not want to
keep:
- remove bootconfig parameter passing feature (unused).
- unifdef __PROG32 and remove all remains of arm26 code.
- remove ARMFPE support (unused).
- remove support for ARM2, ARM2AS, ARM3, ARM6, ARM7, ARM7TDMI and StrongARM
  processor families, and the related silicon bug workarounds (especially
  the SA-110 STM^ bug).
- remove cpu_functions no longer necessary after previous removals.
- remove ARM32_DISABLE_ALIGNMENT_FAULTS option (unused).
- make FIQ support conditional on option FIQ (unused, but may be eventually).

Discussed with drahn@@ and jasper@@ long ago, I was sitting on this commit for
no good reason.
@
text
@d1 1
a1 1
/*	$OpenBSD: bcopyinout.S,v 1.1 2004/02/01 05:09:48 drahn Exp $	*/
d54 2
a55 2
.Lcurpcb:
	.word _C_LABEL(curpcb)
d94 2
a95 2
	ldr	r4, .Lcurpcb
	ldr	r4, [r4]
d317 2
a318 2
	ldr	r4, .Lcurpcb
	ldr	r4, [r4]
d530 2
a531 2
	ldr	r4, .Lcurpcb
	ldr	r4, [r4]
d723 2
a724 2
	ldr	r2, .Lcurpcb
	ldr	r2, [r2]
d757 2
a758 2
	ldr	r2, .Lcurpcb
	ldr	r2, [r2]
d791 2
a792 2
	ldr	r2, .Lcurpcb
	ldr	r2, [r2]
@


1.1
log
@Arm port, NetBSD codebase stripped down, 32bit only support.
@
text
@d1 1
a1 1
/*	$OpenBSD: bcopyinout.S,v 1.1 2004/01/29 16:17:11 drahn Exp $	*/
a57 1
#ifdef __PROG32
a59 5
#else
/* Need to save R14_svc because it'll get trampled if we take a page fault. */
#define SAVE_REGS	stmfd	sp!, {r4-r11, r14}
#define RESTORE_REGS	ldmfd	sp!, {r4-r11, r14}
#endif
a706 1
#ifdef __PROG32
a807 1
#endif /* __PROG32 */
@

