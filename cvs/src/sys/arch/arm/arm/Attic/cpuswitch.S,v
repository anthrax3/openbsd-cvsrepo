head	1.19;
access;
symbols
	OPENBSD_6_0:1.18.0.2
	OPENBSD_6_0_BASE:1.18
	OPENBSD_5_9:1.15.0.2
	OPENBSD_5_9_BASE:1.15
	OPENBSD_5_8:1.14.0.10
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.14.0.2
	OPENBSD_5_7_BASE:1.14
	OPENBSD_5_6:1.14.0.6
	OPENBSD_5_6_BASE:1.14
	OPENBSD_5_5:1.14.0.4
	OPENBSD_5_5_BASE:1.14
	OPENBSD_5_4:1.12.0.8
	OPENBSD_5_4_BASE:1.12
	OPENBSD_5_3:1.12.0.6
	OPENBSD_5_3_BASE:1.12
	OPENBSD_5_2:1.12.0.4
	OPENBSD_5_2_BASE:1.12
	OPENBSD_5_1_BASE:1.12
	OPENBSD_5_1:1.12.0.2
	OPENBSD_5_0:1.11.0.4
	OPENBSD_5_0_BASE:1.11
	OPENBSD_4_9:1.11.0.2
	OPENBSD_4_9_BASE:1.11
	OPENBSD_4_8:1.10.0.10
	OPENBSD_4_8_BASE:1.10
	OPENBSD_4_7:1.10.0.6
	OPENBSD_4_7_BASE:1.10
	OPENBSD_4_6:1.10.0.8
	OPENBSD_4_6_BASE:1.10
	OPENBSD_4_5:1.10.0.4
	OPENBSD_4_5_BASE:1.10
	OPENBSD_4_4:1.10.0.2
	OPENBSD_4_4_BASE:1.10
	OPENBSD_4_3:1.9.0.2
	OPENBSD_4_3_BASE:1.9
	OPENBSD_4_2:1.7.0.2
	OPENBSD_4_2_BASE:1.7
	OPENBSD_4_1:1.6.0.6
	OPENBSD_4_1_BASE:1.6
	OPENBSD_4_0:1.6.0.4
	OPENBSD_4_0_BASE:1.6
	OPENBSD_3_9:1.6.0.2
	OPENBSD_3_9_BASE:1.6
	OPENBSD_3_8:1.5.0.4
	OPENBSD_3_8_BASE:1.5
	OPENBSD_3_7:1.5.0.2
	OPENBSD_3_7_BASE:1.5
	OPENBSD_3_6:1.4.0.2
	OPENBSD_3_6_BASE:1.4
	SMP_SYNC_A:1.2
	SMP_SYNC_B:1.2
	OPENBSD_3_5:1.2.0.4
	OPENBSD_3_5_BASE:1.2
	SMP:1.2.0.2;
locks; strict;
comment	@# @;


1.19
date	2017.01.02.00.51.18;	author jsg;	state dead;
branches;
next	1.18;
commitid	adprazRZa4KO5WRh;

1.18
date	2016.04.25.06.49.46;	author jsg;	state Exp;
branches;
next	1.17;
commitid	E0ph2Megee2Rv1Yh;

1.17
date	2016.04.25.04.46.56;	author jsg;	state Exp;
branches;
next	1.16;
commitid	XqdsV6PrfEUXwK4U;

1.16
date	2016.04.24.01.31.02;	author patrick;	state Exp;
branches;
next	1.15;
commitid	a1zPZPWcIN0F3Zcz;

1.15
date	2016.01.31.00.14.50;	author jsg;	state Exp;
branches;
next	1.14;
commitid	pbLjedMudUFrVMk6;

1.14
date	2013.09.12.11.43.51;	author patrick;	state Exp;
branches;
next	1.13;

1.13
date	2013.09.12.11.42.22;	author patrick;	state Exp;
branches;
next	1.12;

1.12
date	2011.09.20.22.02.10;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2010.11.28.20.45.46;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2008.07.28.19.08.46;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2007.10.13.12.46.17;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2007.10.10.15.53.51;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2007.05.14.07.07.09;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2005.09.22.04.14.44;	author drahn;	state Exp;
branches;
next	1.5;

1.5
date	2004.12.30.23.43.42;	author drahn;	state Exp;
branches;
next	1.4;

1.4
date	2004.08.04.20.20.18;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2004.06.13.21.49.13;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	2004.02.01.06.10.33;	author drahn;	state Exp;
branches
	1.2.2.1;
next	1.1;

1.1
date	2004.02.01.05.09.48;	author drahn;	state Exp;
branches;
next	;

1.2.2.1
date	2004.02.19.10.48.00;	author niklas;	state Exp;
branches;
next	1.2.2.2;

1.2.2.2
date	2004.06.08.18.30.03;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.19
log
@remove unused armv4 and xscale files
ok kettenis@@ patrick@@
@
text
@/*	$OpenBSD: cpuswitch.S,v 1.18 2016/04/25 06:49:46 jsg Exp $	*/
/*	$NetBSD: cpuswitch.S,v 1.41 2003/11/15 08:44:18 scw Exp $	*/

/*
 * Copyright 2003 Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Steve C. Woodford for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed for the NetBSD Project by
 *      Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */
/*
 * Copyright (c) 1994-1998 Mark Brinicombe.
 * Copyright (c) 1994 Brini.
 * All rights reserved.
 *
 * This code is derived from software written for Brini by Mark Brinicombe
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Brini.
 * 4. The name of the company nor the name of the author may be used to
 *    endorse or promote products derived from this software without specific
 *    prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY BRINI ``AS IS'' AND ANY EXPRESS OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL BRINI OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * RiscBSD kernel project
 *
 * cpuswitch.S
 *
 * cpu switching functions
 *
 * Created      : 15/10/94
 */

#include "assym.h"
#include <machine/cpu.h>
#include <machine/frame.h>
#include <machine/intr.h>
#include <machine/asm.h>
#include <arm/sysreg.h>

/* LINTSTUB: include <sys/param.h> */
	
/*
 * These are used for switching the translation table/DACR.
 * Since the vector page can be invalid for a short time, we must
 * disable both regular IRQs *and* FIQs.
 *
 * XXX: This is not necessary if the vector table is relocated.
 */
#define IRQdisableALL \
	mrs	r14, cpsr ; \
	orr	r14, r14, #(PSR_I | PSR_F) ; \
	msr	cpsr_c, r14

#define IRQenableALL \
	mrs	r14, cpsr ; \
	bic	r14, r14, #(PSR_I | PSR_F) ; \
	msr	cpsr_c, r14

	.text

.Lcpu_info_primary:
	.word	_C_LABEL(cpu_info_primary)
.Lcurproc:
	.word	_C_LABEL(cpu_info_primary) + CI_CURPROC
.Lcurpcb:
	.word	_C_LABEL(cpu_info_primary) + CI_CURPCB

.Lcpufuncs:
	.word	_C_LABEL(cpufuncs)

.Lcpu_do_powersave:
	.word	_C_LABEL(cpu_do_powersave)

.Lpmap_kernel_cstate:
	.word	(kernel_pmap_store + PMAP_CSTATE)

.Llast_cache_state_ptr:
	.word	_C_LABEL(pmap_cache_state)

/*
 * Idle loop, exercised while waiting for a process to wake up.
 */
ENTRY(cpu_idle_enter)
	mov	pc, lr

ENTRY(cpu_idle_cycle)
	stmfd	sp!, {r6, lr}

	ldr	r6, .Lcpu_do_powersave
	ldr	r6, [r6]		/* r6 = cpu_do_powersave */

	teq	r6, #0			/* cpu_do_powersave non zero? */
	ldrne	r6, .Lcpufuncs
	ldrne	r6, [r6, #(CF_SLEEP)]

	teq	r6, #0			/* Powersave idle? */
	beq	.Lidle_return		/* Nope. Just continue. */

	/*
	 * Before going into powersave idle mode, disable interrupts.
	 */
	IRQdisableALL
	mov	lr, pc
	mov	pc, r6			/* If so, do powersave idle */
	IRQenableALL

.Lidle_return:
	ldmfd	sp!, {r6, pc}

ENTRY(cpu_idle_leave)
	mov	pc, lr


/*
 * cpu_switchto(struct proc *oldproc, struct proc *newproc)
 *
 * Performs a process context switch from oldproc (which may be NULL)
 * to newproc.
 *
 * Arguments:
 *	r0	'struct proc *' of the context to switch from
 *	r1	'struct proc *' of the context to switch to
 */

ENTRY(cpu_switchto)
	stmfd	sp!, {r4-r7, lr}
	sub	sp, sp, #4

#ifdef MULTIPROCESSOR
	/* XXX use curcpu() */
	ldr	r2, .Lcpu_info_primary
	str	r2, [r1, #(P_CPU)]
#else
	/* p->p_cpu initialized in fork1() for single-processor */
#endif

	/* Process is now on a processor. */
	mov	r2, #SONPROC			/* p->p_stat = SONPROC */
	strb	r2, [r1, #(P_STAT)]

	/* We have a new curproc now so make a note it */
	ldr	r7, .Lcurproc
	str	r1, [r7]

	/* Hook in a new pcb */
	ldr	r7, .Lcurpcb
	ldr	r6, [r7]			/* Remember the old PCB */
	ldr	r2, [r1, #(P_ADDR)]
	str	r2, [r7]

	/*
	 * If the old proc on entry to cpu_switch was zero then the
	 * process that called it was exiting. This means that we do
	 * not need to save the current context (we nevertheless need
	 * to clear the cache and TLB).
	 */
	teq	r0, #0x00000000
	beq	.Lswitch_exited

	/* Stage two: Save old context */

	/* Save all the registers in the old proc's pcb */
#ifndef __XSCALE__
	add	r7, r6, #(PCB_R8)
	stmia	r7, {r8-r13}
#else
	strd	r8, [r6, #(PCB_R8)]
	strd	r10, [r6, #(PCB_R10)]
	strd	r12, [r6, #(PCB_R12)]
#endif

.Lswitch_exited:
	/*
	 * NOTE: We can now use r8-r13 until it is time to restore
	 * them for the new process.
	 */

	/* Remember the old PCB. */
	mov	r8, r6

	/* Save new proc in r6 now. */
	mov	r6, r1

	/* Get the user structure for the new process in r9 */
	ldr	r9, [r6, #(P_ADDR)]

	/*
	 * This can be optimised... We know we want to go from SVC32
	 * mode to UND32 mode
	 */
	mrs	r3, cpsr
	bic	r2, r3, #(PSR_MODE)
	orr	r2, r2, #(PSR_UND32_MODE | PSR_I)
	msr	cpsr_c, r2

#ifdef notworthit
	teq	r0, #0x00000000
	strne	sp, [r8, #(PCB_UND_SP)]
#else
	str	sp, [r8, #(PCB_UND_SP)]
#endif

	msr	cpsr_c, r3		/* Restore the old mode */

	/* rem: r0 = old proc */
	/* rem: r1 = r6 = new process */
	/* rem: r8 = old PCB */
	/* rem: r9 = new PCB */

	/* What else needs to be saved  Only FPA stuff when that is supported */

	/* Third phase: restore saved context */

	/*
	 * Get the new L1 table pointer into r11.  If we're switching to
	 * an LWP with the same address space as the outgoing one, we can
	 * skip the cache purge and the TTB load.
	 *
	 * To avoid data dep stalls that would happen anyway, we try
	 * and get some useful work done in the mean time.
	 */
	ldr	r10, [r8, #(PCB_PAGEDIR)]	/* r10 = old L1 */
	ldr	r11, [r9, #(PCB_PAGEDIR)]	/* r11 = new L1 */

	ldr	r0, [r8, #(PCB_DACR)]		/* r0 = old DACR */
	ldr	r1, [r9, #(PCB_DACR)]		/* r1 = new DACR */
	ldr	r8, [r9, #(PCB_CSTATE)]		/* r8 = &new_pmap->pm_cstate */
	ldr	r5, .Llast_cache_state_ptr	/* Previous proc's cstate */

	teq	r10, r11			/* Same L1? */
	ldr	r5, [r5]
	cmpeq	r0, r1				/* Same DACR? */
	beq	.Lcs_context_switched		/* yes! */

	mov	r12, #0
	cmp	r5, #0				/* No last vm? (switch_exit) */
	beq	.Lcs_cache_purge_skipped	/* No, we can skip cache flsh */

	mov	r2, #DOMAIN_CLIENT
	cmp	r1, r2, lsl #(PMAP_DOMAIN_KERNEL * 2) /* Sw to kernel thread? */
	beq	.Lcs_cache_purge_skipped	/* Yup. Don't flush cache */

	cmp	r5, r8				/* Same userland VM space? */
	ldrneb	r12, [r5, #(CS_CACHE_ID)]	/* Last VM space cache state */

	/*
	 * We're definately switching to a new userland VM space,
	 * and the previous userland VM space has yet to be flushed
	 * from the cache/tlb.
	 *
	 * r12 holds the previous VM space's cs_cache_id state
	 */
	tst	r12, #0xff			/* Test cs_cache_id */
	beq	.Lcs_cache_purge_skipped	/* VM space is not in cache */

	/*
	 * Definitely need to flush the cache.
	 * Mark the old VM space as NOT being resident in the cache.
	 */
	mov	r2, #0x00000000
	strb	r2, [r5, #(CS_CACHE_ID)]
	strb	r2, [r5, #(CS_CACHE_D)]

	stmfd	sp!, {r0-r3}
	ldr	r1, .Lcpufuncs
	mov	lr, pc
	ldr	pc, [r1, #CF_IDCACHE_WBINV_ALL]
	ldmfd	sp!, {r0-r3}

.Lcs_cache_purge_skipped:
	/* rem: r1 = new DACR */
	/* rem: r5 = &old_pmap->pm_cstate (or NULL) */
	/* rem: r6 = new proc */
	/* rem: r8 = &new_pmap->pm_cstate */
	/* rem: r9 = new PCB */
	/* rem: r10 = old L1 */
	/* rem: r11 = new L1 */

	ldr	r7, [r9, #(PCB_PL1VEC)]

	/*
	 * At this point we need to kill IRQ's again.
	 *
	 * XXXSCW: Don't need to block FIQs if vectors have been relocated
	 */
	IRQdisableALL

	/*
	 * Ensure the vector table is accessible by fixing up the L1
	 */
	cmp	r7, #0			/* No need to fixup vector table? */
	ldrne	r2, [r7]		/* But if yes, fetch current value */
	ldrne	r0, [r9, #(PCB_L1VEC)]	/* Fetch new vector_page value */
	mcr	CP15_DACR(r1)		/* Update DACR for new context */
	cmpne	r2, r0			/* Stuffing the same value? */
#ifndef PMAP_INCLUDE_PTE_SYNC
	strne	r0, [r7]		/* Nope, update it */
#else
	beq	.Lcs_same_vector
	str	r0, [r7]		/* Otherwise, update it */

	/*
	 * Need to sync the cache to make sure that last store is
	 * visible to the MMU.
	 */
	ldr	r2, .Lcpufuncs
	mov	r0, r7
	mov	r1, #4
	mov	lr, pc
	ldr	pc, [r2, #CF_DCACHE_WB_RANGE]

.Lcs_same_vector:
#endif /* PMAP_INCLUDE_PTE_SYNC */

	cmp	r10, r11		/* Switching to the same L1? */
	ldr	r10, .Lcpufuncs
	beq	.Lcs_same_l1		/* Yup. */

	/*
	 * Do a full context switch, including full TLB flush.
	 */
	mov	r0, r11
	mov	lr, pc
	ldr	pc, [r10, #CF_CONTEXT_SWITCH]

	/*
	 * Mark the old VM space as NOT being resident in the TLB
	 */
	mov	r2, #0x00000000
	cmp	r5, #0
	strneh	r2, [r5, #(CS_TLB_ID)]
	b	.Lcs_context_switched

	/*
	 * We're switching to a different process in the same L1.
	 * In this situation, we only need to flush the TLB for the
	 * vector_page mapping, and even then only if r7 is non-NULL.
	 */
.Lcs_same_l1:
	cmp	r7, #0
	movne	r0, #0			/* We *know* vector_page's VA is 0x0 */
	movne	lr, pc
	ldrne	pc, [r10, #CF_TLB_FLUSHID_SE]

.Lcs_context_switched:
	/* rem: r8 = &new_pmap->pm_cstate */

	/* XXXSCW: Safe to re-enable FIQs here */

	/*
	 * The new VM space is live in the cache and TLB.
	 * Update its cache/tlb state, and if it's not the kernel
	 * pmap, update the 'last cache state' pointer.
	 */
	mov	r2, #-1
	ldr	r5, .Lpmap_kernel_cstate
	ldr	r0, .Llast_cache_state_ptr
	str	r2, [r8, #(CS_ALL)]
	cmp	r5, r8
	strne	r8, [r0]

	/* rem: r6 = new proc */
	/* rem: r9 = new PCB */

	/*
	 * This can be optimised... We know we want to go from SVC32
	 * mode to UND32 mode
	 */
	mrs	r3, cpsr
	bic	r2, r3, #(PSR_MODE)
	orr	r2, r2, #(PSR_UND32_MODE)
	msr	cpsr_c, r2

	ldr	sp, [r9, #(PCB_UND_SP)]

	msr	cpsr_c, r3		/* Restore the old mode */

	/* Restore all the save registers */
#ifndef __XSCALE__
	add	r7, r9, #PCB_R8
	ldmia	r7, {r8-r13}

	sub	r7, r7, #PCB_R8		/* restore PCB pointer */
#else
	mov	r7, r9
	ldr	r8, [r7, #(PCB_R8)]
	ldr	r9, [r7, #(PCB_R9)]
	ldr	r10, [r7, #(PCB_R10)]
	ldr	r11, [r7, #(PCB_R11)]
	ldr	r12, [r7, #(PCB_R12)]
	ldr	r13, [r7, #(PCB_SP)]
#endif

	/* rem: r6 = new proc */
	/* rem: r7 = new pcb */

	/* We can enable interrupts again */
	IRQenableALL

	/* rem: r6 = new proc */
	/* rem: r7 = new PCB */

.Lswitch_return:
	/*
	 * Pull the registers that got pushed when either savectx() or
	 * cpu_switch() was called and return.
	 */
	add	sp, sp, #4
	ldmfd	sp!, {r4-r7, pc}

/* LINTSTUB: Func: void savectx(struct pcb *pcb) */
ENTRY(savectx)
	/*
	 * r0 = pcb
	 */

	/* Push registers.*/
	stmfd	sp!, {r4-r7, lr}
	sub	sp, sp, #4

	/* Store all the registers in the process's pcb */
#ifndef __XSCALE__
	add	r2, r0, #(PCB_R8)
	stmia	r2, {r8-r13}
#else
	strd	r8, [r0, #(PCB_R8)]
	strd	r10, [r0, #(PCB_R10)]
	strd	r12, [r0, #(PCB_R12)]
#endif

	/* Pull the regs of the stack */
	add	sp, sp, #4
	ldmfd	sp!, {r4-r7, pc}

ENTRY(proc_trampoline)
	mov	r0, #(IPL_NONE)
	bl	_C_LABEL(_spllower)

#ifdef MULTIPROCESSOR
	bl	_C_LABEL(proc_trampoline_mp)
#endif
	mov	r0, r5
	mov	r1, sp
	mov	lr, pc
	mov	pc, r4

	/* Kill irq's */
	mrs	r0, cpsr
	orr	r0, r0, #(PSR_I)
	msr	cpsr_c, r0

	PULLFRAME

	movs	pc, lr			/* Exit */
@


1.18
log
@fix whitespace
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.17 2016/04/25 04:46:56 jsg Exp $	*/
@


1.17
log
@Switch most of the cp14/cp15 use in .S files over to using sysreg.h

Matched and changed by a script, verified to cause no binary change with
armv7, armish, and zaurus kernels.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.16 2016/04/24 01:31:02 patrick Exp $	*/
d239 1
a239 1
        mrs	r3, cpsr
d242 1
a242 1
        msr	cpsr_c, r2
d251 1
a251 1
        msr	cpsr_c, r3		/* Restore the old mode */
d417 1
a417 1
        mrs	r3, cpsr
d420 1
a420 1
        msr	cpsr_c, r2
d424 1
a424 1
        msr	cpsr_c, r3		/* Restore the old mode */
d496 3
a498 3
        mrs     r0, cpsr
        orr     r0, r0, #(PSR_I)
        msr     cpsr_c, r0
@


1.16
log
@EABI's Procedure Call Standard (AAPCS) requires the stack pointer
to be 8-byte aligned. To guarantee this, align the stack pointers
passed to user processes and make sure the in-kernel stacks are
properly aligned, too.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.15 2016/01/31 00:14:50 jsg Exp $	*/
d86 1
d342 1
a342 1
	mcr	p15, 0, r1, c3, c0, 0	/* Update DACR for new context */
@


1.15
log
@Switch from PSR_X_bit and X32_bit PSR macro names to just PSR_X.
This matches FreeBSD and makes things a bit more consistent.
Discussed with Patrick.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.14 2013/09/12 11:43:51 patrick Exp $	*/
d174 1
d455 1
d466 1
d479 1
@


1.14
log
@Store curpcb in cpu_info instead of a global variable, for SMP.

ok rapha@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.13 2013/09/12 11:42:22 patrick Exp $	*/
d98 1
a98 1
	orr	r14, r14, #(I32_bit | F32_bit) ; \
d103 1
a103 1
	bic	r14, r14, #(I32_bit | F32_bit) ; \
d239 1
a239 1
	orr	r2, r2, #(PSR_UND32_MODE | I32_bit)
d492 1
a492 1
        orr     r0, r0, #(I32_bit)
@


1.13
log
@Rename cpu_info_store to cpu_info_primary.  Create an array of cpu_infos
for SMP, like on amd64.  Add some SMP defines.

ok rapha@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.12 2011/09/20 22:02:10 miod Exp $	*/
d111 3
a113 1
	.word	_C_LABEL(cpu_info_parimary) + CI_CURPROC
d115 1
a115 1
.Lcpufuncs:	
a116 13
	
#ifndef MULTIPROCESSOR
.Lcurpcb:
	.word   _C_LABEL(curpcb)
	.data
	.global	_C_LABEL(curpcb)
_C_LABEL(curpcb):
	.word	0x00000000
	.text
#else
.Lcurpcb:
	.word	_C_LABEL(cpu_info_primary) + CI_CURPCB
#endif
@


1.12
log
@Late spring cleaning of the arm code for old dusty bits we do not want to
keep:
- remove bootconfig parameter passing feature (unused).
- unifdef __PROG32 and remove all remains of arm26 code.
- remove ARMFPE support (unused).
- remove support for ARM2, ARM2AS, ARM3, ARM6, ARM7, ARM7TDMI and StrongARM
  processor families, and the related silicon bug workarounds (especially
  the SA-110 STM^ bug).
- remove cpu_functions no longer necessary after previous removals.
- remove ARM32_DISABLE_ALIGNMENT_FAULTS option (unused).
- make FIQ support conditional on option FIQ (unused, but may be eventually).

Discussed with drahn@@ and jasper@@ long ago, I was sitting on this commit for
no good reason.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.11 2010/11/28 20:45:46 miod Exp $	*/
d108 2
a109 2
.Lcpu_info_store:
	.word	_C_LABEL(cpu_info_store)
d111 1
a111 1
	.word	_C_LABEL(cpu_info_store) + CI_CURPROC
d126 1
a126 1
	.word	_C_LABEL(cpu_info_store) + CI_CURPCB
d188 1
a188 1
	ldr	r2, .Lcpu_info_store
@


1.11
log
@This file redefines the IRQdisable and IRQenable macros, but never uses them,
so why bother.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.10 2008/07/28 19:08:46 miod Exp $	*/
a452 6

#ifdef ARMFPE
	add	r0, r7, #(USER_SIZE) & 0x00ff
	add	r0, r0, #(USER_SIZE) & 0xff00 
	bl	_C_LABEL(arm_fpe_core_changecontext)
#endif
@


1.10
log
@No longer clear ci_want_resched within cpu_switchto(), now that it's done
in the MI code.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.9 2007/10/13 12:46:17 miod Exp $	*/
a88 18
#undef IRQdisable
#undef IRQenable

/*
 * New experimental definitions of IRQdisable and IRQenable
 * These keep FIQ's enabled since FIQ's are special.
 */

#define IRQdisable \
	mrs	r14, cpsr ; \
	orr	r14, r14, #(I32_bit) ; \
	msr	cpsr_c, r14 ; \

#define IRQenable \
	mrs	r14, cpsr ; \
	bic	r14, r14, #(I32_bit) ; \
	msr	cpsr_c, r14 ; \

@


1.9
log
@There is no need to fiddle with spl in cpu_idle_{enter,leave}, actually.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.8 2007/10/10 15:53:51 art Exp $	*/
a130 3
.Lwant_resched:
	.word	_C_LABEL(want_resched)

a202 5

	/* Clear the want_resched flag */
	ldr	r7, .Lwant_resched
	mov	r2, #0x00000000
	str	r2, [r7]
@


1.8
log
@Make context switching much more MI:
 - Move the functionality of choosing a process from cpu_switch into
   a much simpler function: cpu_switchto. Instead of having the locore
   code walk the run queues, let the MI code choose the process we
   want to run and only implement the context switching itself in MD
   code.
 - Let MD context switching run without worrying about spls or locks.
 - Instead of having the idle loop implemented with special contexts
   in MD code, implement one idle proc for each cpu. make the idle
   loop MI with MD hooks.
 - Change the proc lists from the old style vax queues to TAILQs.
 - Change the sleep queue from vax queues to TAILQs. This makes
   wakeup() go from O(n^2) to O(n)

there will be some MD fallout, but it will be fixed shortly.
There's also a few cleanups to be done after this.

deraadt@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.7 2007/05/14 07:07:09 art Exp $	*/
d163 1
a163 9
	stmfd	sp!, {lr}

	IRQenable			/* Enable interrupts */

	/* Drop to spl0 (returns the current spl level in r0). */
	mov	r0, #(IPL_NONE)
	bl	_C_LABEL(_spllower)

	ldmfd	sp!, {pc}
d176 1
a176 1
	beq	1f			/* Nope. Just continue. */
d186 2
a187 1
1:	ldmfd	sp!, {r6, pc}
d190 1
a190 6
	stmfd	sp!, {lr}

	mov	r0, #(IPL_SCHED)
	bl	_C_LABEL(_splraise)

	ldmfd	sp!, {pc}
d243 1
a243 1
	/* Stage two : Save old context */
d295 1
a295 1
	/* Third phase : restore saved context */
@


1.7
log
@Switch arm to __HAVE_CPUINFO. Least effort.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.6 2005/09/22 04:14:44 drahn Exp $	*/
a125 130
.Lwhichqs:
	.word	_C_LABEL(whichqs)

.Lqs:
	.word	_C_LABEL(qs)

/*
 * On entry
 *	r0 = process
 */

ENTRY(setrunqueue)
	/*
	 * Local register usage
	 * 	r0 = process
	 * 	r1 = queue
	 * 	r2 = &qs[queue] and temp
	 * 	r3 = temp
	 *	r12 = whichqs
	 */
#ifdef DIAGNOSTIC
	ldr	r1, [r0, #(P_BACK)]
	teq	r1, #0x00000000
	bne	Lsetrunqueue_erg

	ldr	r1, [r0, #(P_WCHAN)]
	teq	r1, #0x00000000
	bne	Lsetrunqueue_erg
#endif

	/* Get the priority of the queue */
	ldrb	r1, [r0, #(P_PRIORITY)]
	mov	r1, r1, lsr #2

	/* Indicate that there is a process on this queue */
	ldr	r12, .Lwhichqs
	ldr	r2, [r12]
	mov	r3, #0x00000001
	mov	r3, r3, lsl r1
	orr	r2, r2, r3
	str	r2, [r12]

	/* Get the address of the queue */
	ldr	r2, .Lqs
	add	r1, r2, r1, lsl # 3

	/* Hook the process in */
	str	r1, [r0, #(P_FORW)]
	ldr	r2, [r1, #(P_BACK)]

	str	r0, [r1, #(P_BACK)]
#ifdef DIAGNOSTIC
	teq	r2, #0x00000000
	beq	Lsetrunqueue_erg
#endif
	str	r0, [r2, #(P_FORW)]
	str	r2, [r0, #(P_BACK)]

	mov	pc, lr

#ifdef DIAGNOSTIC
Lsetrunqueue_erg:
	mov	r2, r1
	mov	r1, r0
	add	r0, pc, #Ltext1 - . - 8
	bl	_C_LABEL(printf)

	ldr	r2, .Lqs
	ldr	r1, [r2]
	add	r0, pc, #Ltext2 - . - 8
	b	_C_LABEL(panic)	

Ltext1:
	.asciz	"setrunqueue : %08x %08x\n"
Ltext2:
	.asciz	"setrunqueue : [qs]=%08x qs=%08x\n"
	.align	0
#endif

/*
 * On entry
 *	r0 = process
 */

ENTRY(remrunqueue)
	/*
	 * Local register usage
	 *	r0 = oldproc
	 * 	r1 = queue
	 * 	r2 = &qs[queue] and scratch
	 *	r3 = scratch
	 *	r12 = whichqs
	 */

	/* Get the priority of the queue */
	ldrb	r1, [r0, #(P_PRIORITY)]
	mov	r1, r1, lsr #2

	/* Unhook the process */
	ldr	r2, [r0, #(P_FORW)]
	ldr	r3, [r0, #(P_BACK)]

	str	r3, [r2, #(P_BACK)]
	str	r2, [r3, #(P_FORW)]

	/* If the queue is now empty clear the queue not empty flag */
	teq	r2, r3

	/* This could be reworked to avoid the use of r4 */
	ldreq	r12, .Lwhichqs
	ldreq	r2, [r12]
	moveq	r3, #0x00000001
	moveq	r3, r3, lsl r1
	biceq	r2, r2, r3
	streq	r2, [r12]

	/* Remove the back pointer for the process */
	mov	r1, #0x00000000
	str	r1, [r0, #(P_BACK)]

	mov	pc, lr


/*
 * cpuswitch()
 *
 * preforms a process context switch.
 * This function has several entry points
 */

a130 1

a160 4
 *
 * NOTE: When we jump back to .Lswitch_search, we must have a
 * pointer to whichqs in r7, which is what it is when we arrive
 * here.
d162 3
a164 3
/* LINTSTUB: Ignore */
ASENTRY_NP(idle)
	ldr	r6, .Lcpu_do_powersave
a165 5
	ldr	r6, [r6]		/* r6 = cpu_do_powersave */

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
	bl	_C_LABEL(sched_unlock_idle)
#endif
d171 8
a180 1
	mov	r4, r0			/* Old interrupt level to r4 */
a182 7
	/*
	 * Main idle loop.
	 * r6 points to power-save idle function if required, else NULL.
	 */
1:	ldr	r3, [r7]		/* r3 = sched_whichqs */
	teq	r3, #0
	bne	2f			/* We have work to do */
d184 1
a184 1
	beq	1b			/* Nope. Just sit-n-spin. */
d187 1
a187 2
	 * Before going into powersave idle mode, disable interrupts
	 * and check sched_whichqs one more time.
d190 2
a191 5
	ldr	r3, [r7]
	mov	r0, #0
	teq	r3, #0			/* sched_whichqs still zero? */
	moveq	lr, pc
	moveq	pc, r6			/* If so, do powersave idle */
a192 1
	b	1b			/* Back around */
d194 9
a202 14
	/*
	 * sched_whichqs indicates that at least one proc is ready to run.
	 * Restore the original interrupt priority level, grab the
	 * scheduler lock if necessary, and jump back into cpu_switch.
	 */
2:	mov	r0, r4
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
	bl	_C_LABEL(splx)
	adr	lr, .Lswitch_search
	b	_C_LABEL(sched_lock_idle)
#else
	adr	lr, .Lswitch_search
	b	_C_LABEL(splx)
#endif
d206 4
a209 2
 * Find a new lwp to run, save the current context and
 * load the new context
d212 2
a213 1
 *	r0	'struct proc *' of the current LWP
d216 1
a216 11
ENTRY(cpu_switch)
/*
 * Local register usage. Some of these registers are out of date.
 * r1 = oldproc
 * r2 = spl level
 * r3 = whichqs
 * r4 = queue
 * r5 = &qs[queue]
 * r6 = newlwp
 * r7 = scratch
 */
a218 116
	/*
	 * Indicate that there is no longer a valid process (curlwp = 0).
	 * Zero the current PCB pointer while we're at it.
	 */
	ldr	r7, .Lcurproc
	ldr	r6, .Lcurpcb
	mov	r2, #0x00000000
	str	r2, [r7]		/* curproc = NULL */
	str	r2, [r6]		/* curpcb = NULL */

	/* stash the old proc while we call functions */
	mov	r5, r0

	/* First phase : find a new proc */
	ldr	r7, .Lwhichqs

	/* rem: r5 = old proc */
	/* rem: r7 = &whichqs */

.Lswitch_search:
	IRQdisable

	/* Do we have any active queues  */
	ldr	r3, [r7]

	/* If not we must idle until we do. */
	teq	r3, #0x00000000
	beq	_ASM_LABEL(idle)

	/* put old proc back in r1 */
	mov	r1, r5

	/* rem: r1 = old proc */
	/* rem: r3 = whichqs */
	/* rem: interrupts are disabled */

	/* used further down, saves SA stall */
	ldr	r6, .Lqs

	/*
	 * We have found an active queue. Currently we do not know which queue
	 * is active just that one of them is.
	 */
	/* Non-Xscale version of the ffs algorithm devised by d.seal and
	 * posted to comp.sys.arm on 16 Feb 1994.
	 */
 	rsb	r5, r3, #0
 	ands	r0, r3, r5

#ifndef __XSCALE__
	adr	r5, .Lcpu_switch_ffs_table

				    /* X = R0 */
	orr	r4, r0, r0, lsl #4  /* r4 = X * 0x11 */ 
	orr	r4, r4, r4, lsl #6  /* r4 = X * 0x451 */
	rsb	r4, r4, r4, lsl #16 /* r4 = X * 0x0450fbaf */
	
	/* now lookup in table indexed on top 6 bits of a4 */
	ldrb	r4, [ r5, r4, lsr #26 ]

#else	/* __XSCALE__ */
	clz	r4, r0
	rsb	r4, r4, #31
#endif	/* __XSCALE__ */

	/* rem: r0 = bit mask of chosen queue (1 << r4) */
	/* rem: r1 = old proc */
	/* rem: r3 = whichqs */
	/* rem: r4 = queue number */
	/* rem: interrupts are disabled */

	/* Get the address of the queue (&qs[queue]) */
	add	r5, r6, r4, lsl #3

	/*
	 * Get the proc from the queue and place the next process in
	 * the queue at the head. This basically unlinks the lwp at
	 * the head of the queue.
	 */
	ldr	r6, [r5, #(P_FORW)]

#ifdef DIAGNOSTIC
	cmp	r6, r5
	beq	.Lswitch_bogons
#endif

	/* rem: r6 = new proc */
	ldr	r7, [r6, #(P_FORW)]
	str	r7, [r5, #(P_FORW)]	

	/*
	 * Test to see if the queue is now empty. If the head of the queue
	 * points to the queue itself then there are no more procs in
	 * the queue. We can therefore clear the queue not empty flag held
	 * in r3.
	 */

	teq	r5, r7
	biceq	r3, r3, r0

	/* rem: r0 = bit mask of chosen queue (1 << r4) - NOT NEEDED AN MORE */

	/* Fix the back pointer for the lwp now at the head of the queue. */
	ldr	r0, [r6, #(P_BACK)]
	str	r0, [r7, #(P_BACK)]

	/* Update the RAM copy of the queue not empty flags word. */
	ldreq	r7, .Lwhichqs
	streq	r3, [r7]

	/* rem: r1 = old proc */
	/* rem: r3 = whichqs - NOT NEEDED ANY MORE */
	/* rem: r4 = queue number - NOT NEEDED ANY MORE */
	/* rem: r6 = new proc */
	/* rem: interrupts are disabled */

d221 2
a222 24
	mov	r0, #0x00000000
	str	r0, [r7]

	/*
	 * Clear the back pointer of the proc we have removed from
	 * the head of the queue. The new proc is isolated now.
	 */
	str	r0, [r6, #(P_BACK)]

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
	/*
	 * unlock the sched_lock, but leave interrupts off, for now.
	 */
	mov	r7, r1
	bl	_C_LABEL(sched_unlock_idle)
	mov	r1, r7
#endif


.Lswitch_resume:
	/* rem: r1 = old proc */
	/* rem: r4 = return value [not used if came from cpu_switchto()] */
	/* rem: r6 = new process */
	/* rem: interrupts are disabled */
d226 2
a227 2
	ldr	r0, .Lcpu_info_store
	str	r0, [r6, #(P_CPU)]
d229 1
a229 1
	/* l->l_cpu initialized in fork1() for single-processor */
d233 2
a234 2
	mov	r0, #SONPROC			/* p->p_stat = SONPROC */
	strb	r0, [r6, #(P_STAT)]
d238 1
a238 1
	str	r6, [r7]
d242 3
a244 30
	ldr	r0, [r6, #(P_ADDR)]
	str	r0, [r7]

	/* At this point we can allow IRQ's again. */
	IRQenable

	/* rem: r1 = old proc */
	/* rem: r4 = return value */
	/* rem: r6 = new process */
	/* rem: interrupts are enabled */

	/*
	 * If the new process is the same as the process that called
	 * cpu_switch() then we do not need to save and restore any
	 * contexts. This means we can make a quick exit.
	 * The test is simple if curproc on entry (now in r1) is the
	 * same as the proc removed from the queue we can jump to the exit.
	 */
	teq	r1, r6
	moveq	r4, #0x00000000		/* default to "didn't switch" */
	beq	.Lswitch_return

	/*
	 * At this point, we are guaranteed to be switching to
	 * a new proc.
	 */
	mov	r4, #0x00000001

	/* Remember the old proc in r0 */
	mov	r0, r1
d249 2
a250 2
	 * not need to save the current context. Instead we can jump
	 * straight to restoring the context for the new process.
a254 5
	/* rem: r0 = old proc */
	/* rem: r4 = return value */
	/* rem: r6 = new process */
	/* rem: interrupts are enabled */

a256 3
	/* Get the user structure for the old proc. */
	ldr	r1, [r0, #(P_ADDR)]

d259 1
a259 1
	add	r7, r1, #(PCB_R8)
d262 3
a264 3
	strd	r8, [r1, #(PCB_R8)]
	strd	r10, [r1, #(PCB_R10)]
	strd	r12, [r1, #(PCB_R12)]
d267 1
d274 1
a274 1
	mov	r8, r1
d276 2
a277 1
	/* r1 now free! */
d291 4
d296 1
d301 1
a301 2
	/* rem: r4 = return value */
	/* rem: r6 = new process */
a303 1
	/* rem: interrupts are enabled */
a308 7
	/* rem: r0 = old proc */
	/* rem: r4 = return value */
	/* rem: r6 = new proc */
	/* rem: r8 = old PCB */
	/* rem: r9 = new PCB */
	/* rem: interrupts are enabled */

d323 1
a323 1
	ldr	r5, .Llast_cache_state_ptr	/* Previous thread's cstate */
d352 1
a352 1
	 * Definately need to flush the cache.
a366 1
	/* rem: r4 = return value */
a456 1
	/* rem: r4 = return value */
a488 8
#if 0
	ldr	r5, [r6, #(L_PROC)]	/* fetch the proc for below */
#else
	mov	r5, r6
#endif

	/* rem: r4 = return value */
	/* rem: r5 = new proc's proc */
a500 2
	/* rem: r4 = return value */
	/* rem: r5 = new proc's proc */
a503 11
#if 0
	/* 
	 * Check for restartable atomic sequences (RAS).
	 */

	ldr	r2, [r5, #(P_RASLIST)]
	ldr	r1, [r7, #(PCB_TF)]	/* r1 = trapframe (used below) */
	teq	r2, #0			/* p->p_nras == 0? */
	bne	.Lswitch_do_ras		/* no, check for one */
#endif

a504 3
	/* cpu_switch returns 1 == switched, 0 == didn't switch */
	mov	r0, r4

a510 202
#if 0
.Lswitch_do_ras:
	ldr	r1, [r1, #(TF_PC)]	/* second ras_lookup() arg */
	mov	r0, r5			/* first ras_lookup() arg */
	bl	_C_LABEL(ras_lookup)
	cmn	r0, #1			/* -1 means "not in a RAS" */
	ldrne	r1, [r7, #(PCB_TF)]
	strne	r0, [r1, #(TF_PC)]
	b	.Lswitch_return
#endif

.Lswitch_exited:
	/*
	 * We skip the cache purge because switch_exit() already did it.
	 * Load up registers the way .Lcs_cache_purge_skipped expects.
	 * Userpsace access already blocked by switch_exit().
	 */
	ldr	r9, [r6, #(P_ADDR)]		/* r9 = new PCB */
	mrc	p15, 0, r10, c2, c0, 0		/* r10 = old L1 */
	mov	r5, #0				/* No previous cache state */
	ldr	r1, [r9, #(PCB_DACR)]		/* r1 = new DACR */
	ldr	r8, [r9, #(PCB_CSTATE)]		/* r8 = new cache state */
	ldr	r11, [r9, #(PCB_PAGEDIR)]	/* r11 = new L1 */
	b	.Lcs_cache_purge_skipped


#ifdef DIAGNOSTIC
.Lswitch_bogons:
	adr	r0, .Lswitch_panic_str
	bl	_C_LABEL(panic)
1:	nop
	b	1b

.Lswitch_panic_str:
	.asciz	"cpu_switch: sched_qs empty with non-zero sched_whichqs!\n"
#endif

/*
 * cpu_switchto(struct proc *current, struct proc *next)
 * Switch to the specified next LWP
 * Arguments:
 *
 *	r0	'struct proc *' of the current LWP
 *	r1	'struct proc *' of the LWP to switch to
 */
ENTRY(cpu_switchto)
	stmfd	sp!, {r4-r7, lr}

	mov	r6, r1		/* save new proc */

#if defined(LOCKDEBUG)
	mov	r5, r0		/* save old proc */
	bl	_C_LABEL(sched_unlock_idle)
	mov	r1, r5
#else
	mov	r1, r0
#endif

	IRQdisable

	/*
	 * Okay, set up registers the way cpu_switch() wants them,
	 * and jump into the middle of it (where we bring up the
	 * new process).
	 *
	 * r1 = old proc (r6 = new proc)
	 */
	b	.Lswitch_resume

/*
 * void switch_exit(struct proc *l, struct proc *l0,
 *    void (*exit)(struct proc *));
 * Switch to proc0's saved context and deallocate the address space and kernel
 * stack for l.  Then jump into cpu_switch(), as if we were in proc0 all along.
 */

/* LINTSTUB: Func: void switch_exit(struct proc *l, struct proc *l0,
    void (*func)(struct proc *)) */
ENTRY(switch_exit)
	/*
	 * The process is going away, so we can use callee-saved
	 * registers here without having to save them.
	 */

	mov	r4, r0
	ldr	r0, .Lcurproc

	mov	r5, r1
	mov	r6, r2

	/*
	 * r4 = proc
	 * r5 = proc0
	 * r6 = exit func
	 */

	mov	r2, #0x00000000		/* curproc = NULL */
	str	r2, [r0]

	/*
	 * We're about to clear both the cache and the TLB.
	 * Make sure to zap the 'last cache state' pointer since the
	 * pmap might be about to go away. Also ensure the outgoing
	 * VM space's cache state is marked as NOT resident in the
	 * cache, and that proc0's cache state IS resident.
	 */
	ldr	r7, [r4, #(P_ADDR)]		/* r7 = old proc's PCB */
	ldr	r0, .Llast_cache_state_ptr	/* Last userland cache state */
	ldr	r9, [r7, #(PCB_CSTATE)]		/* Fetch cache state pointer */
	ldr	r3, [r5, #(P_ADDR)]		/* r3 = proc0's PCB */
	str	r2, [r0]			/* No previous cache state */
	str	r2, [r9, #(CS_ALL)]		/* Zap old proc's cache state */
	ldr	r3, [r3, #(PCB_CSTATE)]		/* proc0's cache state */
	mov	r2, #-1
	str	r2, [r3, #(CS_ALL)]		/* proc0 is in da cache! */

	/* Switch to proc0 context */

	ldr	r9, .Lcpufuncs
	mov	lr, pc
	ldr	pc, [r9, #CF_IDCACHE_WBINV_ALL]

	ldr	r0, [r7, #(PCB_PL1VEC)]
	ldr	r1, [r7, #(PCB_DACR)]

	/*
	 * r0 = Pointer to L1 slot for vector_page (or NULL)
	 * r1 = proc0's DACR
	 * r4 = proc we're switching from
	 * r5 = proc0
	 * r6 = exit func
	 * r7 = proc0's PCB
	 * r9 = cpufuncs
	 */

	IRQdisableALL

	/*
	 * Ensure the vector table is accessible by fixing up proc0's L1
	 */
	cmp	r0, #0			/* No need to fixup vector table? */
	ldrne	r3, [r0]		/* But if yes, fetch current value */
	ldrne	r2, [r7, #(PCB_L1VEC)]	/* Fetch new vector_page value */
	mcr	p15, 0, r1, c3, c0, 0	/* Update DACR for proc0's context */
	cmpne	r3, r2			/* Stuffing the same value? */
	strne	r2, [r0]		/* Store if not. */

#ifdef PMAP_INCLUDE_PTE_SYNC
	/*
	 * Need to sync the cache to make sure that last store is
	 * visible to the MMU.
	 */
	movne	r1, #4
	movne	lr, pc
	ldrne	pc, [r9, #CF_DCACHE_WB_RANGE]
#endif /* PMAP_INCLUDE_PTE_SYNC */

	/*
	 * Note: We don't do the same optimisation as cpu_switch() with
	 * respect to avoiding flushing the TLB if we're switching to
	 * the same L1 since this process' VM space may be about to go
	 * away, so we don't want *any* turds left in the TLB.
	 */

	/* Switch the memory to the new process */
	ldr	r0, [r7, #(PCB_PAGEDIR)]
	mov	lr, pc
	ldr	pc, [r9, #CF_CONTEXT_SWITCH]

	ldr	r0, .Lcurpcb
   
	/* Restore all the save registers */
#ifndef __XSCALE__
	add	r1, r7, #PCB_R8
	ldmia	r1, {r8-r13}
#else
	ldr	r8, [r7, #(PCB_R8)]
	ldr	r9, [r7, #(PCB_R9)]
	ldr	r10, [r7, #(PCB_R10)]
	ldr	r11, [r7, #(PCB_R11)]
	ldr	r12, [r7, #(PCB_R12)]
	ldr	r13, [r7, #(PCB_SP)]
#endif
	str	r7, [r0]	/* curpcb = proc0's PCB */

	IRQenableALL

	/*
	 * Schedule the vmspace and stack to be freed.
	 */
	mov	r0, r4			/* {proc_}exit2(l) */
	mov	lr, pc
	mov	pc, r6

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
	bl	_C_LABEL(sched_lock_idle)
#endif

	ldr	r7, .Lwhichqs		/* r7 = &whichqs */
	mov	r5, #0x00000000		/* r5 = old proc = NULL */
	b	.Lswitch_search

a552 15

#ifndef __XSCALE__
	.type .Lcpu_switch_ffs_table, _ASM_TYPE_OBJECT;
.Lcpu_switch_ffs_table:
/* same as ffs table but all nums are -1 from that */
/*               0   1   2   3   4   5   6   7           */
	.byte	 0,  0,  1, 12,  2,  6,  0, 13  /*  0- 7 */
	.byte	 3,  0,  7,  0,  0,  0,  0, 14  /*  8-15 */
	.byte	10,  4,  0,  0,  8,  0,  0, 25  /* 16-23 */
	.byte	 0,  0,  0,  0,  0, 21, 27, 15  /* 24-31 */
	.byte	31, 11,  5,  0,  0,  0,  0,  0	/* 32-39 */
	.byte	 9,  0,  0, 24,  0,  0, 20, 26  /* 40-47 */
	.byte	30,  0,  0,  0,  0, 23,  0, 19  /* 48-55 */
	.byte   29,  0, 22, 18, 28, 17, 16,  0  /* 56-63 */
#endif	/* !__XSCALE_ */
@


1.6
log
@NEWINTR is not an option on OpenBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.5 2004/12/30 23:43:42 drahn Exp $	*/
a255 1
#ifdef MULTIPROCESSOR
d259 1
a259 2
	/* FIXME: This is bogus in the general case. */
	.word	_C_LABEL(cpu_info_store) + CI_CURLWP
a260 9
.Lcurpcb:
	.word	_C_LABEL(cpu_info_store) + CI_CURPCB
#else
.Lcurproc:
	.word	_C_LABEL(curproc)

.Lcurpcb:
	.word	_C_LABEL(curpcb)
#endif
d269 2
d276 3
@


1.5
log
@better #include
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.4 2004/08/04 20:20:18 miod Exp $	*/
a313 1
#ifdef __NEWINTR
a315 4
#else /* ! __NEWINTR */
	mov	r0, #(_SPL_0)
	bl	_C_LABEL(splx)
#endif /* __NEWINTR */
a1099 1
#ifdef __NEWINTR
a1101 4
#else /* ! __NEWINTR */
	mov	r0, #(_SPL_0)
	bl	_C_LABEL(splx)
#endif /* __NEWINTR */
@


1.4
log
@Remove the old fetch(9)/store(9) functions, since they are not used in
OpenBSD. This gives us a simpler and faster cpu_switch() as a bonus.

ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.3 2004/06/13 21:49:13 niklas Exp $	*/
d84 1
@


1.3
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a285 3
.Lblock_userspace_access:
	.word	_C_LABEL(block_userspace_access)

a671 1
	ldr	r3, .Lblock_userspace_access
a700 6
	/*
	 * Don't allow user space access between the purge and the switch.
	 */
	mov	r2, #0x00000001
	str	r2, [r3]

a708 1
	/* rem: r3 = &block_userspace_access */
a716 1
	mov	r2, #0x00000000
a726 7
	 * Interrupts are disabled so we can allow user space accesses again
	 * as none will occur until interrupts are re-enabled after the
	 * switch.
	 */
	str	r2, [r3]

	/*
a896 1
	ldr	r3, .Lblock_userspace_access
a966 2
	ldr	r1, .Lblock_userspace_access

a993 6

	/*
	 * Don't allow user space access between the purge and the switch.
	 */
	mov	r2, #0x00000001
	str	r2, [r1]
@


1.2
log
@fix tags...
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.1 2004/02/01 05:09:48 drahn Exp $	*/
a539 1
#if 0
d541 2
a542 3
	mov	r0, #LSONPROC			/* l->l_stat = LSONPROC */
	str	r0, [r6, #(P_STAT)]
#endif
@


1.2.2.1
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.2.2.2
log
@SONPROC, ok drahn deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: cpuswitch.S,v 1.2.2.1 2004/02/19 10:48:00 niklas Exp $	*/
d540 1
d542 3
a544 2
	mov	r0, #SONPROC			/* p->p_stat = SONPROC */
	strb	r0, [r6, #(P_STAT)]
@


1.1
log
@Arm port, NetBSD codebase stripped down, 32bit only support.
@
text
@d1 2
a2 2
/*	$OpenBSD: cpuswitch.S,v 1.3 2004/01/29 16:17:16 drahn Exp $	*/
/*	$NetBSD: cpuswitch.S,v 1.41 2003/11/15 08:44:18 scw Exp $	*/$
@

