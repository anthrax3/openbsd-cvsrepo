head	1.55;
access;
symbols
	OPENBSD_6_1:1.55.0.4
	OPENBSD_6_1_BASE:1.55
	OPENBSD_6_0:1.27.0.2
	OPENBSD_6_0_BASE:1.27
	OPENBSD_5_9:1.23.0.2
	OPENBSD_5_9_BASE:1.23
	OPENBSD_5_8:1.20.0.6
	OPENBSD_5_8_BASE:1.20
	OPENBSD_5_7:1.20.0.2
	OPENBSD_5_7_BASE:1.20
	OPENBSD_5_6:1.15.0.4
	OPENBSD_5_6_BASE:1.15
	OPENBSD_5_5:1.12.0.4
	OPENBSD_5_5_BASE:1.12
	OPENBSD_5_4:1.8.0.2
	OPENBSD_5_4_BASE:1.8
	OPENBSD_5_3:1.1.0.2
	OPENBSD_5_3_BASE:1.1;
locks; strict;
comment	@ * @;


1.55
date	2016.10.22.17.48.41;	author patrick;	state Exp;
branches;
next	1.54;
commitid	1RcSnPLGnOkAWVLw;

1.54
date	2016.10.19.08.28.19;	author guenther;	state Exp;
branches;
next	1.53;
commitid	VoR9X3uHTxRSYC5r;

1.53
date	2016.09.24.13.03.47;	author kettenis;	state Exp;
branches;
next	1.52;
commitid	9PbSYmtcqgKwPRCy;

1.52
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.51;
commitid	RlO92XR575sygHqm;

1.51
date	2016.08.31.12.24.12;	author jsg;	state Exp;
branches;
next	1.50;
commitid	cHXd64L7On2l2FRU;

1.50
date	2016.08.27.14.22.35;	author kettenis;	state Exp;
branches;
next	1.49;
commitid	DQQMw9JK3zW7Auja;

1.49
date	2016.08.26.16.02.33;	author kettenis;	state Exp;
branches;
next	1.48;
commitid	xFDqBa4TWNgGnBRy;

1.48
date	2016.08.26.11.59.04;	author kettenis;	state Exp;
branches;
next	1.47;
commitid	eHAV7iG810DcGo8p;

1.47
date	2016.08.24.13.09.52;	author kettenis;	state Exp;
branches;
next	1.46;
commitid	Emb0lwjQLbMxmqQS;

1.46
date	2016.08.20.21.08.16;	author kettenis;	state Exp;
branches;
next	1.45;
commitid	1naEEdmUogaMV4qY;

1.45
date	2016.08.20.21.07.07;	author kettenis;	state Exp;
branches;
next	1.44;
commitid	hoiKhneCZRpyilIK;

1.44
date	2016.08.20.21.04.18;	author kettenis;	state Exp;
branches;
next	1.43;
commitid	bCnan4fgI7Hkimr6;

1.43
date	2016.08.20.12.36.59;	author kettenis;	state Exp;
branches;
next	1.42;
commitid	FMBskR8HAJwQySsA;

1.42
date	2016.08.19.17.31.04;	author kettenis;	state Exp;
branches;
next	1.41;
commitid	MoNCYZAeTFoT7uMc;

1.41
date	2016.08.19.15.47.27;	author kettenis;	state Exp;
branches;
next	1.40;
commitid	jwc8G7zTkdxMAnjb;

1.40
date	2016.08.19.13.56.08;	author kettenis;	state Exp;
branches;
next	1.39;
commitid	uXjojXCZvopcWMeE;

1.39
date	2016.08.18.09.28.22;	author kettenis;	state Exp;
branches;
next	1.38;
commitid	ryjYlYdRzLtqz6LU;

1.38
date	2016.08.16.10.16.33;	author kettenis;	state Exp;
branches;
next	1.37;
commitid	TdF7ApuLuHwWqb59;

1.37
date	2016.08.11.00.28.06;	author kettenis;	state Exp;
branches;
next	1.36;
commitid	l66O0iSrl3bYh97A;

1.36
date	2016.08.09.13.13.51;	author kettenis;	state Exp;
branches;
next	1.35;
commitid	HSIVU0JbL5iGzd25;

1.35
date	2016.08.08.14.47.52;	author kettenis;	state Exp;
branches;
next	1.34;
commitid	CQxVw8UNradhT604;

1.34
date	2016.08.08.09.06.47;	author kettenis;	state Exp;
branches;
next	1.33;
commitid	NuiBtTVAHS1hEcH6;

1.33
date	2016.08.06.16.46.25;	author kettenis;	state Exp;
branches;
next	1.32;
commitid	CGAfg7FH1Wq4zBj1;

1.32
date	2016.08.03.11.52.43;	author kettenis;	state Exp;
branches;
next	1.31;
commitid	GxiPhwFosFMD1L8J;

1.31
date	2016.07.31.22.27.07;	author kettenis;	state Exp;
branches;
next	1.30;
commitid	oYaUZJft9fnO70FN;

1.30
date	2016.07.31.22.04.44;	author kettenis;	state Exp;
branches;
next	1.29;
commitid	D6sDnAu87wLlTiA0;

1.29
date	2016.07.29.06.46.15;	author patrick;	state Exp;
branches;
next	1.28;
commitid	RHCaHDHTi1wSAbl7;

1.28
date	2016.07.27.21.12.49;	author patrick;	state Exp;
branches;
next	1.27;
commitid	p8kaRWIhlxg42WWX;

1.27
date	2016.07.19.02.26.15;	author tom;	state Exp;
branches;
next	1.26;
commitid	mByVa6k8dajQxIhE;

1.26
date	2016.07.18.13.38.11;	author tom;	state Exp;
branches;
next	1.25;
commitid	Ru5XYf5zOeMkjoyT;

1.25
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.24;
commitid	N0upL0onl7Raz5yi;

1.24
date	2016.03.03.04.28.21;	author jsg;	state Exp;
branches;
next	1.23;
commitid	JABzdrwH20mQR2UF;

1.23
date	2016.02.01.04.28.45;	author jsg;	state Exp;
branches;
next	1.22;
commitid	GuYAGioFXe7G3dLA;

1.22
date	2016.01.31.00.14.50;	author jsg;	state Exp;
branches;
next	1.21;
commitid	pbLjedMudUFrVMk6;

1.21
date	2015.09.08.21.28.36;	author kettenis;	state Exp;
branches;
next	1.20;
commitid	WSD3bUAMn8qMj0PM;

1.20
date	2015.02.02.09.29.53;	author mlarkin;	state Exp;
branches;
next	1.19;
commitid	Gl0HfhXiwDmJ4Tqg;

1.19
date	2015.01.29.20.10.50;	author deraadt;	state Exp;
branches;
next	1.18;
commitid	J9beJfMoTflGapt5;

1.18
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.17;
commitid	yv0ECmCdICvq576h;

1.17
date	2014.10.27.00.49.05;	author jsg;	state Exp;
branches;
next	1.16;
commitid	DwyamtgTThvYqHgZ;

1.16
date	2014.10.07.07.14.55;	author jsg;	state Exp;
branches;
next	1.15;
commitid	Lon7MMg3dHoKgehP;

1.15
date	2014.07.12.18.44.41;	author tedu;	state Exp;
branches;
next	1.14;
commitid	uKVPYMN2MLxdZxzH;

1.14
date	2014.05.08.21.17.00;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2014.03.29.18.09.28;	author guenther;	state Exp;
branches;
next	1.12;

1.12
date	2013.11.04.00.35.30;	author dlg;	state Exp;
branches;
next	1.11;

1.11
date	2013.10.29.12.10.02;	author patrick;	state Exp;
branches;
next	1.10;

1.10
date	2013.10.22.15.18.06;	author patrick;	state Exp;
branches;
next	1.9;

1.9
date	2013.09.03.16.48.26;	author patrick;	state Exp;
branches;
next	1.8;

1.8
date	2013.06.09.12.53.42;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2013.05.22.12.27.44;	author patrick;	state Exp;
branches;
next	1.6;

1.6
date	2013.05.21.18.25.40;	author patrick;	state Exp;
branches;
next	1.5;

1.5
date	2013.05.09.20.07.25;	author patrick;	state Exp;
branches;
next	1.4;

1.4
date	2013.04.28.14.39.15;	author patrick;	state Exp;
branches;
next	1.3;

1.3
date	2013.04.16.14.44.13;	author patrick;	state Exp;
branches;
next	1.2;

1.2
date	2013.03.27.08.52.49;	author patrick;	state Exp;
branches;
next	1.1;

1.1
date	2013.01.17.20.52.39;	author bmercer;	state Exp;
branches;
next	;


desc
@@


1.55
log
@If an Access Flag fault happens while we were running the kernel and
it happened on a kernel page, we need to consult the kernel pmap
instead of the current proc's pmap.  Fixes panic when using tmpfs.

ok kettenis@@
@
text
@/*	$OpenBSD: pmap7.c,v 1.54 2016/10/19 08:28:19 guenther Exp $	*/
/*	$NetBSD: pmap.c,v 1.147 2004/01/18 13:03:50 scw Exp $	*/

/*
 * Copyright 2003 Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Steve C. Woodford for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed for the NetBSD Project by
 *      Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright (c) 2002-2003 Wasabi Systems, Inc.
 * Copyright (c) 2001 Richard Earnshaw
 * Copyright (c) 2001-2002 Christopher Gilbert
 * All rights reserved.
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. The name of the company nor the name of the author may be used to
 *    endorse or promote products derived from this software without specific
 *    prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

/*-
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Charles M. Hannum.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright (c) 1994-1998 Mark Brinicombe.
 * Copyright (c) 1994 Brini.
 * All rights reserved.
 *
 * This code is derived from software written for Brini by Mark Brinicombe
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Mark Brinicombe.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 *
 * RiscBSD kernel project
 *
 * pmap.c
 *
 * Machine dependant vm stuff
 *
 * Created      : 20/09/94
 */

/*
 * Performance improvements, UVM changes, overhauls and part-rewrites
 * were contributed by Neil A. Carson <neil@@causality.com>.
 */

/*
 * Overhauled again to speedup the pmap, use MMU Domains so that L1 tables
 * can be shared, and re-work the KVM layout, by Steve Woodford of Wasabi
 * Systems, Inc.
 *
 * There are still a few things outstanding at this time:
 *
 *   - There are some unresolved issues for MP systems:
 *
 *     o The L1 metadata needs a lock, or more specifically, some places
 *       need to acquire an exclusive lock when modifying L1 translation
 *       table entries.
 *
 *     o When one cpu modifies an L1 entry, and that L1 table is also
 *       being used by another cpu, then the latter will need to be told
 *       that a tlb invalidation may be necessary. (But only if the old
 *       domain number in the L1 entry being over-written is currently
 *       the active domain on that cpu). I guess there are lots more tlb
 *       shootdown issues too...
 *
 *     o If the vector_page is at 0x00000000 instead of 0xffff0000, then
 *       MP systems will lose big-time because of the MMU domain hack.
 *       The only way this can be solved (apart from moving the vector
 *       page to 0xffff0000) is to reserve the first 1MB of user address
 *       space for kernel use only. This would require re-linking all
 *       applications so that the text section starts above this 1MB
 *       boundary.
 *
 *     o Tracking which VM space is resident in the cache/tlb has not yet
 *       been implemented for MP systems.
 *
 *     o Finally, there is a pathological condition where two cpus running
 *       two separate processes (not procs) which happen to share an L1
 *       can get into a fight over one or more L1 entries. This will result
 *       in a significant slow-down if both processes are in tight loops.
 */

#include <sys/types.h>
#include <sys/param.h>
#include <sys/kernel.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/user.h>
#include <sys/pool.h>
#include <sys/cdefs.h>
#include <sys/sched.h>
 
#include <uvm/uvm.h>

#include <machine/bus.h>
#include <machine/pmap.h>
#include <machine/pcb.h>
#include <machine/param.h>
#include <arm/cpufunc.h>

//#define PMAP_DEBUG
#ifdef PMAP_DEBUG

/*
 * for switching to potentially finer grained debugging
 */
#define	PDB_FOLLOW	0x0001
#define	PDB_INIT	0x0002
#define	PDB_ENTER	0x0004
#define	PDB_REMOVE	0x0008
#define	PDB_CREATE	0x0010
#define	PDB_PTPAGE	0x0020
#define	PDB_GROWKERN	0x0040
#define	PDB_BITS	0x0080
#define	PDB_COLLECT	0x0100
#define	PDB_PROTECT	0x0200
#define	PDB_MAP_L1	0x0400
#define	PDB_BOOTSTRAP	0x1000
#define	PDB_PARANOIA	0x2000
#define	PDB_WIRING	0x4000
#define	PDB_PVDUMP	0x8000
#define	PDB_KENTER	0x20000
#define	PDB_KREMOVE	0x40000

#define pmapdebug (cold ? 0 : 0xffffffff)
#define	NPDEBUG(_lev_,_stat_) \
	if (pmapdebug & (_lev_)) \
        	((_stat_))
    
#else	/* PMAP_DEBUG */
#define NPDEBUG(_lev_,_stat_) /* Nothing */
#endif	/* PMAP_DEBUG */

/*
 * pmap_kernel() points here
 */
struct pmap     kernel_pmap_store;

/*
 * Pool and cache that pmap structures are allocated from.
 * We use a cache to avoid clearing the pm_l2[] array (1KB)
 * in pmap_create().
 */
struct pool pmap_pmap_pool;

/*
 * Pool of PV structures
 */
struct pool pmap_pv_pool;
void *pmap_bootstrap_pv_page_alloc(struct pool *, int, int *);
void pmap_bootstrap_pv_page_free(struct pool *, void *);
struct pool_allocator pmap_bootstrap_pv_allocator = {
	pmap_bootstrap_pv_page_alloc, pmap_bootstrap_pv_page_free
};

/*
 * Pool and cache of l2_dtable structures.
 * We use a cache to avoid clearing the structures when they're
 * allocated. (196 bytes)
 */
struct pool pmap_l2dtable_pool;
vaddr_t pmap_kernel_l2dtable_kva;

/*
 * Pool and cache of L2 page descriptors.
 * We use a cache to avoid clearing the descriptor table
 * when they're allocated. (1KB)
 */
struct pool pmap_l2ptp_pool;
vaddr_t pmap_kernel_l2ptp_kva;
paddr_t pmap_kernel_l2ptp_phys;

/*
 * pmap copy/zero page, wb page, and mem(5) hook point
 */
pt_entry_t *csrc_pte, *cdst_pte, *cwb_pte;
vaddr_t csrcp, cdstp, cwbp;
char *memhook;
extern caddr_t msgbufaddr;

/*
 * Flag to indicate if pmap_init() has done its thing
 */
boolean_t pmap_initialized;

/*
 * Metadata for L1 translation tables.
 */
struct l1_ttable {
	/* Entry on the L1 Table list */
	TAILQ_ENTRY(l1_ttable) l1_link;

	/* Physical address of this L1 page table */
	paddr_t l1_physaddr;

	/* KVA of this L1 page table */
	pd_entry_t *l1_kva;
};

/*
 * Convert a virtual address into its L1 table index. That is, the
 * index used to locate the L2 descriptor table pointer in an L1 table.
 * This is basically used to index l1->l1_kva[].
 *
 * Each L2 descriptor table represents 1MB of VA space.
 */
#define	L1_IDX(va)		(((vaddr_t)(va)) >> L1_S_SHIFT)

/*
 * Set if the PXN bit is supported.
 */
pd_entry_t l1_c_pxn;

/*
 * A list of all L1 tables
 */
TAILQ_HEAD(, l1_ttable) l1_list;

/*
 * The l2_dtable tracks L2_BUCKET_SIZE worth of L1 slots.
 *
 * This is normally 16MB worth L2 page descriptors for any given pmap.
 * Reference counts are maintained for L2 descriptors so they can be
 * freed when empty.
 */
struct l2_dtable {
	/* The number of L2 page descriptors allocated to this l2_dtable */
	u_int l2_occupancy;

	/* List of L2 page descriptors */
	struct l2_bucket {
		pt_entry_t *l2b_kva;	/* KVA of L2 Descriptor Table */
		paddr_t l2b_phys;	/* Physical address of same */
		u_short l2b_l1idx;	/* This L2 table's L1 index */
		u_short l2b_occupancy;	/* How many active descriptors */
	} l2_bucket[L2_BUCKET_SIZE];
};

/*
 * Given an L1 table index, calculate the corresponding l2_dtable index
 * and bucket index within the l2_dtable.
 */
#define	L2_IDX(l1idx)		(((l1idx) >> L2_BUCKET_LOG2) & \
				 (L2_SIZE - 1))
#define	L2_BUCKET(l1idx)	((l1idx) & (L2_BUCKET_SIZE - 1))

/*
 * Given a virtual address, this macro returns the
 * virtual address required to drop into the next L2 bucket.
 */
#define	L2_NEXT_BUCKET(va)	(((va) & L1_S_FRAME) + L1_S_SIZE)

/*
 * L2 allocation.
 */
#define	pmap_alloc_l2_dtable()		\
	    pool_get(&pmap_l2dtable_pool, PR_NOWAIT|PR_ZERO)
#define	pmap_free_l2_dtable(l2)		\
	    pool_put(&pmap_l2dtable_pool, (l2))

/*
 * We try to map the page tables write-through, if possible.  However, not
 * all CPUs have a write-through cache mode, so on those we have to sync
 * the cache when we frob page tables.
 *
 * We try to evaluate this at compile time, if possible.  However, it's
 * not always possible to do that, hence this run-time var.
 */
int	pmap_needs_pte_sync;

/*
 * Real definition of pv_entry.
 */
struct pv_entry {
	struct pv_entry *pv_next;       /* next pv_entry */
	pmap_t		pv_pmap;        /* pmap where mapping lies */
	vaddr_t		pv_va;          /* virtual address for mapping */
	u_int		pv_flags;       /* flags */
};

/*
 * Macro to determine if a mapping might be resident in the
 * instruction cache and/or TLB
 */
#define	PV_BEEN_EXECD(f)  (((f) & PVF_EXEC) != 0)

/*
 * Local prototypes
 */
void		pmap_alloc_specials(vaddr_t *, int, vaddr_t *,
		    pt_entry_t **);
static boolean_t	pmap_is_current(pmap_t);
void		pmap_enter_pv(struct vm_page *, struct pv_entry *,
		    pmap_t, vaddr_t, u_int);
static struct pv_entry *pmap_find_pv(struct vm_page *, pmap_t, vaddr_t);
struct pv_entry *pmap_remove_pv(struct vm_page *, pmap_t, vaddr_t);
u_int		pmap_modify_pv(struct vm_page *, pmap_t, vaddr_t,
		    u_int, u_int);

void		pmap_alloc_l1(pmap_t);
void		pmap_free_l1(pmap_t);

struct l2_bucket *pmap_get_l2_bucket(pmap_t, vaddr_t);
struct l2_bucket *pmap_alloc_l2_bucket(pmap_t, vaddr_t);
void		pmap_free_l2_bucket(pmap_t, struct l2_bucket *, u_int);

void		pmap_clearbit(struct vm_page *, u_int);
void		pmap_clean_page(struct vm_page *);
void		pmap_page_remove(struct vm_page *);

void		pmap_init_l1(struct l1_ttable *, pd_entry_t *);
vaddr_t		kernel_pt_lookup(paddr_t);


/*
 * External function prototypes
 */
extern void bzero_page(vaddr_t);
extern void bcopy_page(vaddr_t, vaddr_t);

/*
 * Misc variables
 */
vaddr_t virtual_avail;
vaddr_t virtual_end;
vaddr_t pmap_curmaxkvaddr;

extern pv_addr_t systempage;

static __inline boolean_t
pmap_is_current(pmap_t pm)
{
	if (pm == pmap_kernel() ||
	    (curproc && curproc->p_vmspace->vm_map.pmap == pm))
		return (TRUE);

	return (FALSE);
}

/*
 * A bunch of routines to conditionally flush the caches/TLB depending
 * on whether the specified pmap actually needs to be flushed at any
 * given time.
 */
static __inline void
pmap_tlb_flushID_SE(pmap_t pm, vaddr_t va)
{
	if (pmap_is_current(pm))
		cpu_tlb_flushID_SE(va);
}

static __inline void
pmap_tlb_flushID(pmap_t pm)
{
	if (pmap_is_current(pm))
		cpu_tlb_flushID();
}

/*
 * Returns a pointer to the L2 bucket associated with the specified pmap
 * and VA, or NULL if no L2 bucket exists for the address.
 */
struct l2_bucket *
pmap_get_l2_bucket(pmap_t pm, vaddr_t va)
{
	struct l2_dtable *l2;
	struct l2_bucket *l2b;
	u_short l1idx;

	l1idx = L1_IDX(va);

	if ((l2 = pm->pm_l2[L2_IDX(l1idx)]) == NULL ||
	    (l2b = &l2->l2_bucket[L2_BUCKET(l1idx)])->l2b_kva == NULL)
		return (NULL);

	return (l2b);
}

/*
 * main pv_entry manipulation functions:
 *   pmap_enter_pv: enter a mapping onto a vm_page list
 *   pmap_remove_pv: remove a mapping from a vm_page list
 *
 * NOTE: pmap_enter_pv expects to lock the pvh itself
 *       pmap_remove_pv expects te caller to lock the pvh before calling
 */

/*
 * pmap_enter_pv: enter a mapping onto a vm_page lst
 *
 * => caller should have pmap locked
 * => we will gain the lock on the vm_page and allocate the new pv_entry
 * => caller should adjust ptp's wire_count before calling
 * => caller should not adjust pmap's wire_count
 */
void
pmap_enter_pv(struct vm_page *pg, struct pv_entry *pve, pmap_t pm,
    vaddr_t va, u_int flags)
{

	NPDEBUG(PDB_PVDUMP,
	    printf("pmap_enter_pv: pm %p, pg %p, flags 0x%x\n", pm, pg, flags));

	pve->pv_pmap = pm;
	pve->pv_va = va;
	pve->pv_flags = flags;

	pve->pv_next = pg->mdpage.pvh_list;	/* add to ... */
	pg->mdpage.pvh_list = pve;		/* ... locked list */
	pg->mdpage.pvh_attrs |= flags & (PVF_REF | PVF_MOD);

	if (pve->pv_flags & PVF_WIRED)
		++pm->pm_stats.wired_count;
}

/*
 *
 * pmap_find_pv: Find a pv entry
 *
 * => caller should hold lock on vm_page
 */
static __inline struct pv_entry *
pmap_find_pv(struct vm_page *pg, pmap_t pm, vaddr_t va)
{
	struct pv_entry *pv;

	for (pv = pg->mdpage.pvh_list; pv; pv = pv->pv_next) {
		if (pm == pv->pv_pmap && va == pv->pv_va)
			break;
	}

	return (pv);
}

/*
 * pmap_remove_pv: try to remove a mapping from a pv_list
 *
 * => pmap should be locked
 * => caller should hold lock on vm_page [so that attrs can be adjusted]
 * => caller should adjust ptp's wire_count and free PTP if needed
 * => caller should NOT adjust pmap's wire_count
 * => we return the removed pve
 */
struct pv_entry *
pmap_remove_pv(struct vm_page *pg, pmap_t pm, vaddr_t va)
{
	struct pv_entry *pve, **prevptr;

	NPDEBUG(PDB_PVDUMP,
	    printf("pmap_remove_pv: pm %p, pg %p, va 0x%08lx\n", pm, pg, va));

	prevptr = &pg->mdpage.pvh_list;		/* previous pv_entry pointer */
	pve = *prevptr;

	while (pve) {
		if (pve->pv_pmap == pm && pve->pv_va == va) {	/* match? */
			NPDEBUG(PDB_PVDUMP,
			    printf("pmap_remove_pv: pm %p, pg %p, flags 0x%x\n", pm, pg, pve->pv_flags));
			*prevptr = pve->pv_next;		/* remove it! */
			if (pve->pv_flags & PVF_WIRED)
			    --pm->pm_stats.wired_count;
			break;
		}
		prevptr = &pve->pv_next;		/* previous pointer */
		pve = pve->pv_next;			/* advance */
	}

	return(pve);				/* return removed pve */
}

/*
 *
 * pmap_modify_pv: Update pv flags
 *
 * => caller should hold lock on vm_page [so that attrs can be adjusted]
 * => caller should NOT adjust pmap's wire_count
 * => we return the old flags
 * 
 * Modify a physical-virtual mapping in the pv table
 */
u_int
pmap_modify_pv(struct vm_page *pg, pmap_t pm, vaddr_t va,
    u_int clr_mask, u_int set_mask)
{
	struct pv_entry *npv;
	u_int flags, oflags;

	if ((npv = pmap_find_pv(pg, pm, va)) == NULL)
		return (0);

	NPDEBUG(PDB_PVDUMP,
	    printf("pmap_modify_pv: pm %p, pg %p, clr 0x%x, set 0x%x, flags 0x%x\n", pm, pg, clr_mask, set_mask, npv->pv_flags));

	/*
	 * There is at least one VA mapping this page.
	 */

	if (clr_mask & (PVF_REF | PVF_MOD))
		pg->mdpage.pvh_attrs |= set_mask & (PVF_REF | PVF_MOD);

	oflags = npv->pv_flags;
	npv->pv_flags = flags = (oflags & ~clr_mask) | set_mask;

	if ((flags ^ oflags) & PVF_WIRED) {
		if (flags & PVF_WIRED)
			++pm->pm_stats.wired_count;
		else
			--pm->pm_stats.wired_count;
	}

	return (oflags);
}

uint nl1;
/*
 * Allocate an L1 translation table for the specified pmap.
 * This is called at pmap creation time.
 */
void
pmap_alloc_l1(pmap_t pm)
{
	struct l1_ttable *l1;
	struct pglist plist;
	struct vm_page *m;
	pd_entry_t *pl1pt;
	vaddr_t va, eva;
	int error;

#ifdef PMAP_DEBUG
printf("%s: %d\n", __func__, ++nl1);
#endif
	/* XXX use a pool? or move to inside struct pmap? */
	l1 = malloc(sizeof(*l1), M_VMPMAP, M_WAITOK);

	/* Allocate a L1 page table */
	for (;;) {
		va = uvm_km_valloc(kernel_map, L1_TABLE_SIZE);
		if (va != 0)
			break;
		uvm_wait("alloc_l1_va");
	}

	for (;;) {
		TAILQ_INIT(&plist);
		error = uvm_pglistalloc(L1_TABLE_SIZE, 0, (paddr_t)-1,
		    L1_TABLE_SIZE, 0, &plist, 1, UVM_PLA_WAITOK);
		if (error == 0)
			break;
		uvm_wait("alloc_l1_pg");
	}

	pl1pt = (pd_entry_t *)va;
	m = TAILQ_FIRST(&plist);
	for (eva = va + L1_TABLE_SIZE; va < eva; va += PAGE_SIZE) {
		paddr_t pa = VM_PAGE_TO_PHYS(m);

		pmap_kenter_pa(va, pa, PROT_READ | PROT_WRITE);
		m = TAILQ_NEXT(m, pageq);
	}

	pmap_init_l1(l1, pl1pt);

	pm->pm_l1 = l1;
}

/*
 * Free an L1 translation table.
 * This is called at pmap destruction time.
 */
void
pmap_free_l1(pmap_t pm)
{
	struct l1_ttable *l1 = pm->pm_l1;
	struct pglist mlist;
	struct vm_page *pg;
	struct l2_bucket *l2b;
	pt_entry_t *ptep;
	vaddr_t va;
	uint npg;

	pm->pm_l1 = NULL;
	TAILQ_REMOVE(&l1_list, l1, l1_link);

	/* free backing pages */
	TAILQ_INIT(&mlist);
	va = (vaddr_t)l1->l1_kva;
	for (npg = atop(L1_TABLE_SIZE); npg != 0; npg--) {
		l2b = pmap_get_l2_bucket(pmap_kernel(), va);
		ptep = &l2b->l2b_kva[l2pte_index(va)];
		pg = PHYS_TO_VM_PAGE(l2pte_pa(*ptep));
		TAILQ_INSERT_TAIL(&mlist, pg, pageq);
		va += PAGE_SIZE;
	}
	pmap_kremove((vaddr_t)l1->l1_kva, L1_TABLE_SIZE);
	uvm_pglistfree(&mlist);

	/* free backing va */
	uvm_km_free(kernel_map, (vaddr_t)l1->l1_kva, L1_TABLE_SIZE);

	free(l1, M_VMPMAP, 0);
}

/*
 * void pmap_free_l2_ptp(pt_entry_t *)
 *
 * Free an L2 descriptor table.
 */
static __inline void
pmap_free_l2_ptp(pt_entry_t *l2)
{
	pool_put(&pmap_l2ptp_pool, (void *)l2);
}

/*
 * Returns a pointer to the L2 bucket associated with the specified pmap
 * and VA.
 *
 * If no L2 bucket exists, perform the necessary allocations to put an L2
 * bucket/page table in place.
 *
 * Note that if a new L2 bucket/page was allocated, the caller *must*
 * increment the bucket occupancy counter appropriately *before* 
 * releasing the pmap's lock to ensure no other thread or cpu deallocates
 * the bucket/page in the meantime.
 */
struct l2_bucket *
pmap_alloc_l2_bucket(pmap_t pm, vaddr_t va)
{
	struct l2_dtable *l2;
	struct l2_bucket *l2b;
	u_short l1idx;

	l1idx = L1_IDX(va);

	if ((l2 = pm->pm_l2[L2_IDX(l1idx)]) == NULL) {
		/*
		 * No mapping at this address, as there is
		 * no entry in the L1 table.
		 * Need to allocate a new l2_dtable.
		 */
		if ((l2 = pmap_alloc_l2_dtable()) == NULL)
			return (NULL);

		/*
		 * Link it into the parent pmap
		 */
		pm->pm_l2[L2_IDX(l1idx)] = l2;
	}

	l2b = &l2->l2_bucket[L2_BUCKET(l1idx)];

	/*
	 * Fetch pointer to the L2 page table associated with the address.
	 */
	if (l2b->l2b_kva == NULL) {
		pt_entry_t *ptep;

		/*
		 * No L2 page table has been allocated. Chances are, this
		 * is because we just allocated the l2_dtable, above.
		 */
		ptep = pool_get(&pmap_l2ptp_pool, PR_NOWAIT|PR_ZERO);
		if (ptep == NULL) {
			/*
			 * Oops, no more L2 page tables available at this
			 * time. We may need to deallocate the l2_dtable
			 * if we allocated a new one above.
			 */
			if (l2->l2_occupancy == 0) {
				pm->pm_l2[L2_IDX(l1idx)] = NULL;
				pmap_free_l2_dtable(l2);
			}
			return (NULL);
		}
		PTE_SYNC_RANGE(ptep, L2_TABLE_SIZE_REAL / sizeof(pt_entry_t));
		pmap_extract(pmap_kernel(), (vaddr_t)ptep, &l2b->l2b_phys);

		l2->l2_occupancy++;
		l2b->l2b_kva = ptep;
		l2b->l2b_l1idx = l1idx;
	}

	return (l2b);
}

/*
 * One or more mappings in the specified L2 descriptor table have just been
 * invalidated.
 *
 * Garbage collect the metadata and descriptor table itself if necessary.
 *
 * The pmap lock must be acquired when this is called (not necessary
 * for the kernel pmap).
 */
void
pmap_free_l2_bucket(pmap_t pm, struct l2_bucket *l2b, u_int count)
{
	struct l2_dtable *l2;
	pd_entry_t *pl1pd;
	pt_entry_t *ptep;
	u_short l1idx;

	KDASSERT(count <= l2b->l2b_occupancy);

	/*
	 * Update the bucket's reference count according to how many
	 * PTEs the caller has just invalidated.
	 */
	l2b->l2b_occupancy -= count;

	/*
	 * Note:
	 *
	 * Level 2 page tables allocated to the kernel pmap are never freed
	 * as that would require checking all Level 1 page tables and
	 * removing any references to the Level 2 page table. See also the
	 * comment elsewhere about never freeing bootstrap L2 descriptors.
	 *
	 * We make do with just invalidating the mapping in the L2 table.
	 *
	 * This isn't really a big deal in practice and, in fact, leads
	 * to a performance win over time as we don't need to continually
	 * alloc/free.
	 */
	if (l2b->l2b_occupancy > 0 || pm == pmap_kernel())
		return;

	/*
	 * There are no more valid mappings in this level 2 page table.
	 * Go ahead and NULL-out the pointer in the bucket, then
	 * free the page table.
	 */
	l1idx = l2b->l2b_l1idx;
	ptep = l2b->l2b_kva;
	l2b->l2b_kva = NULL;

	pl1pd = &pm->pm_l1->l1_kva[l1idx];

	/*
	 * Invalidate the L1 slot.
	 */
	*pl1pd = L1_TYPE_INV;
	PTE_SYNC(pl1pd);
	pmap_tlb_flushID_SE(pm, l1idx << L1_S_SHIFT);

	/*
	 * Release the L2 descriptor table back to the pool cache.
	 */
	pmap_free_l2_ptp(ptep);

	/*
	 * Update the reference count in the associated l2_dtable
	 */
	l2 = pm->pm_l2[L2_IDX(l1idx)];
	if (--l2->l2_occupancy > 0)
		return;

	/*
	 * There are no more valid mappings in any of the Level 1
	 * slots managed by this l2_dtable. Go ahead and NULL-out
	 * the pointer in the parent pmap and free the l2_dtable.
	 */
	pm->pm_l2[L2_IDX(l1idx)] = NULL;
	pmap_free_l2_dtable(l2);
}

/*
 * Modify pte bits for all ptes corresponding to the given physical address.
 * We use `maskbits' rather than `clearbits' because we're always passing
 * constants and the latter would require an extra inversion at run-time.
 */
void
pmap_clearbit(struct vm_page *pg, u_int maskbits)
{
	struct l2_bucket *l2b;
	struct pv_entry *pv;
	pt_entry_t *ptep, npte, opte;
	pmap_t pm;
	vaddr_t va;
	u_int oflags;

	NPDEBUG(PDB_BITS,
	    printf("pmap_clearbit: pg %p (0x%08lx) mask 0x%x\n",
	    pg, pg->phys_addr, maskbits));

	/*
	 * Clear saved attributes (modify, reference)
	 */
	pg->mdpage.pvh_attrs &= ~(maskbits & (PVF_MOD | PVF_REF));

	if (pg->mdpage.pvh_list == NULL)
		return;

	/*
	 * Loop over all current mappings setting/clearing as appropriate
	 */
	for (pv = pg->mdpage.pvh_list; pv; pv = pv->pv_next) {
		va = pv->pv_va;
		pm = pv->pv_pmap;
		oflags = pv->pv_flags;
		pv->pv_flags &= ~maskbits;

		l2b = pmap_get_l2_bucket(pm, va);
		KDASSERT(l2b != NULL);

		ptep = &l2b->l2b_kva[l2pte_index(va)];
		npte = opte = *ptep;
		NPDEBUG(PDB_BITS,
		    printf(
		    "pmap_clearbit: pv %p, pm %p, va 0x%08lx, flag 0x%x\n",
		    pv, pv->pv_pmap, pv->pv_va, oflags));

		if (maskbits & (PVF_WRITE|PVF_MOD)) {
			/* Disable write access. */
			npte |= L2_V7_AP(0x4);
		}

		if (maskbits & PVF_REF) {
			/*
			 * Clear the Access Flag such that we will
			 * take a page fault the next time the mapping
			 * is referenced.
			 */
			npte &= ~L2_V7_AF;
		}

		if (npte != opte) {
			*ptep = npte;
			PTE_SYNC(ptep);
			/* Flush the TLB entry if a current pmap. */
			if (opte & L2_V7_AF)
				pmap_tlb_flushID_SE(pm, pv->pv_va);
		}

		NPDEBUG(PDB_BITS,
		    printf("pmap_clearbit: pm %p va 0x%lx opte 0x%08x npte 0x%08x\n",
		    pm, va, opte, npte));
	}
}

/*
 * pmap_clean_page()
 *
 * Invalidate all I$ aliases for a single page.
 */
void
pmap_clean_page(struct vm_page *pg)
{
	pmap_t pm;
	struct pv_entry *pv;

	if (curproc)
		pm = curproc->p_vmspace->vm_map.pmap;
	else
		pm = pmap_kernel();

	for (pv = pg->mdpage.pvh_list; pv; pv = pv->pv_next) {
		/* inline !pmap_is_current(pv->pv_pmap) */
		if (pv->pv_pmap != pmap_kernel() && pv->pv_pmap != pm)
			continue;

		/*
		 * The page is mapped non-cacheable in 
		 * this map.  No need to flush the cache.
		 */
		if (pv->pv_flags & PVF_NC) /* XXX ought to be pg attr */
			break;

		if (PV_BEEN_EXECD(pv->pv_flags))
			cpu_icache_sync_range(pv->pv_va, PAGE_SIZE);
	}
}

/*
 * Routine:	pmap_page_remove
 * Function:
 *		Removes this physical page from
 *		all physical maps in which it resides.
 *		Reflects back modify bits to the pager.
 */
void
pmap_page_remove(struct vm_page *pg)
{
	struct l2_bucket *l2b;
	struct pv_entry *pv, *npv;
	pmap_t pm, curpm;
	pt_entry_t *ptep, opte;
	boolean_t flush;

	NPDEBUG(PDB_FOLLOW,
	    printf("pmap_page_remove: pg %p (0x%08lx)\n", pg, pg->phys_addr));

	pv = pg->mdpage.pvh_list;
	if (pv == NULL)
		return;

	flush = FALSE;
	if (curproc)
		curpm = curproc->p_vmspace->vm_map.pmap;
	else
		curpm = pmap_kernel();

	while (pv) {
		pm = pv->pv_pmap;

		l2b = pmap_get_l2_bucket(pm, pv->pv_va);
		KDASSERT(l2b != NULL);

		ptep = &l2b->l2b_kva[l2pte_index(pv->pv_va)];
		opte = *ptep;
		if (opte != L2_TYPE_INV) {
			/* inline pmap_is_current(pm) */
			if ((opte & L2_V7_AF) &&
			    (pm == curpm || pm == pmap_kernel())) {
				if (PV_BEEN_EXECD(pv->pv_flags))
					cpu_icache_sync_range(pv->pv_va, PAGE_SIZE);
				flush = TRUE;
			}

			/*
			 * Update statistics
			 */
			--pm->pm_stats.resident_count;

			/* Wired bit */
			if (pv->pv_flags & PVF_WIRED)
				--pm->pm_stats.wired_count;

			/*
			 * Invalidate the PTEs.
			 */
			*ptep = L2_TYPE_INV;
			PTE_SYNC(ptep);
			if (flush)
				cpu_tlb_flushID_SE(pv->pv_va);

			pmap_free_l2_bucket(pm, l2b, 1);
		}

		npv = pv->pv_next;
		pool_put(&pmap_pv_pool, pv);
		pv = npv;
	}
	pg->mdpage.pvh_list = NULL;
}

/*
 * pmap_t pmap_create(void)
 *  
 *      Create a new pmap structure from scratch.
 */
pmap_t
pmap_create(void)
{
	pmap_t pm;

	pm = pool_get(&pmap_pmap_pool, PR_WAITOK|PR_ZERO);

	pm->pm_refs = 1;
	pm->pm_stats.wired_count = 0;
	pmap_alloc_l1(pm);

	return (pm);
}

/*
 * void pmap_enter(pmap_t pm, vaddr_t va, paddr_t pa, vm_prot_t prot,
 *     int flags)
 *  
 *      Insert the given physical page (p) at
 *      the specified virtual address (v) in the
 *      target physical map with the protection requested.
 *
 *      NB:  This is the only routine which MAY NOT lazy-evaluate
 *      or lose information.  That is, this routine must actually
 *      insert this page into the given map NOW.
 */
int
pmap_enter(pmap_t pm, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	struct l2_bucket *l2b;
	struct vm_page *pg, *opg;
	struct pv_entry *pve;
	pt_entry_t *ptep, npte, opte;
	u_int nflags;
	u_int oflags;
	int mapped = 1;

	NPDEBUG(PDB_ENTER, printf("pmap_enter: pm %p va 0x%lx pa 0x%lx prot %x flag %x\n", pm, va, pa, prot, flags));

	KDASSERT((flags & PMAP_WIRED) == 0 || (flags & PROT_MASK) != 0);
	KDASSERT(((va | pa) & PGOFSET) == 0);

	/*
	 * Get a pointer to the page.  Later on in this function, we
	 * test for a managed page by checking pg != NULL.
	 */
	pg = pmap_initialized ? PHYS_TO_VM_PAGE(pa) : NULL;

	nflags = 0;
	if (prot & PROT_WRITE)
		nflags |= PVF_WRITE;
	if (prot & PROT_EXEC)
		nflags |= PVF_EXEC;
	if (flags & PMAP_WIRED)
		nflags |= PVF_WIRED;

	/*
	 * Fetch the L2 bucket which maps this page, allocating one if
	 * necessary for user pmaps.
	 */
	if (pm == pmap_kernel())
		l2b = pmap_get_l2_bucket(pm, va);
	else
		l2b = pmap_alloc_l2_bucket(pm, va);
	if (l2b == NULL) {
		if (flags & PMAP_CANFAIL)
			return (ENOMEM);

		panic("pmap_enter: failed to allocate L2 bucket");
	}
	ptep = &l2b->l2b_kva[l2pte_index(va)];
	opte = *ptep;
	npte = L2_S_PROTO | pa;

	if (opte != L2_TYPE_INV) {
		/*
		 * There is already a mapping at this address.
		 * If the physical address is different, lookup the
		 * vm_page.
		 */
		if (l2pte_pa(opte) != pa)
			opg = PHYS_TO_VM_PAGE(l2pte_pa(opte));
		else
			opg = pg;
	} else
		opg = NULL;

	if (pg) {
		/*
		 * This has to be a managed mapping.
		 */
		if ((flags & PROT_MASK) ||
		    (pg->mdpage.pvh_attrs & PVF_REF)) {
			/*
			 * - The access type indicates that we don't need
			 *   to do referenced emulation.
			 * OR
			 * - The physical page has already been referenced
			 *   so no need to re-do referenced emulation here.
			 */
			nflags |= PVF_REF;
			npte |= L2_V7_AF;

			if ((flags & PROT_WRITE) ||
			    (pg->mdpage.pvh_attrs & PVF_MOD)) {
				/*
				 * This is a writable mapping, and the
				 * page's mod state indicates it has
				 * already been modified. Make it
				 * writable from the outset.
				 */
				nflags |= PVF_MOD;
			} else {
				prot &= ~PROT_WRITE;
			}
		} else {
			/*
			 * Need to do page referenced emulation.
			 */
			prot &= ~PROT_WRITE;
			mapped = 0;
		}

		npte |= pte_l2_s_cache_mode;

		if (pg == opg) {
			/*
			 * We're changing the attrs of an existing mapping.
			 */
			oflags = pmap_modify_pv(pg, pm, va,
			    PVF_WRITE | PVF_EXEC | PVF_WIRED |
			    PVF_MOD | PVF_REF, nflags);
		} else {
			/*
			 * New mapping, or changing the backing page
			 * of an existing mapping.
			 */
			if (opg) {
				/*
				 * Replacing an existing mapping with a new one.
				 * It is part of our managed memory so we
				 * must remove it from the PV list
				 */
				pve = pmap_remove_pv(opg, pm, va);
			} else
			if ((pve = pool_get(&pmap_pv_pool, PR_NOWAIT)) == NULL){
				if ((flags & PMAP_CANFAIL) == 0)
					panic("pmap_enter: no pv entries");

				if (pm != pmap_kernel())
					pmap_free_l2_bucket(pm, l2b, 0);

				NPDEBUG(PDB_ENTER,
				    printf("pmap_enter: ENOMEM\n"));
				return (ENOMEM);
			}

			pmap_enter_pv(pg, pve, pm, va, nflags);
		}
	} else {
		/*
		 * We're mapping an unmanaged page.
		 * These are always readable, and possibly writable, from
		 * the get go as we don't need to track ref/mod status.
		 */
		npte |= L2_V7_AF;

		if (opg) {
			/*
			 * Looks like there's an existing 'managed' mapping
			 * at this address.
			 */
			pve = pmap_remove_pv(opg, pm, va);
			pool_put(&pmap_pv_pool, pve);
		}
	}

	/*
	 * Make sure userland mappings get the right permissions
	 */
	npte |= L2_S_PROT(pm == pmap_kernel() ?  PTE_KERNEL : PTE_USER, prot);

	/*
	 * Keep the stats up to date
	 */
	if (opte == L2_TYPE_INV) {
		l2b->l2b_occupancy++;
		pm->pm_stats.resident_count++;
	} 

	NPDEBUG(PDB_ENTER,
	    printf("pmap_enter: opte 0x%08x npte 0x%08x\n", opte, npte));

	/*
	 * If this is just a wiring change, the two PTEs will be
	 * identical, so there's no need to update the page table.
	 */
	if (npte != opte) {
		*ptep = npte;
		/*
		 * We only need to frob the cache/tlb if this pmap
		 * is current
		 */
		PTE_SYNC(ptep);
		if (npte & L2_V7_AF) {
			/*
			 * This mapping is likely to be accessed as
			 * soon as we return to userland. Fix up the
			 * L1 entry to avoid taking another page fault.
			 */
			pd_entry_t *pl1pd, l1pd;

			pl1pd = &pm->pm_l1->l1_kva[L1_IDX(va)];
			l1pd = L1_C_PROTO | l2b->l2b_phys | l1_c_pxn;
			if (*pl1pd != l1pd) {
				*pl1pd = l1pd;
				PTE_SYNC(pl1pd);
			}
		}

		if (opte & L2_V7_AF)
			pmap_tlb_flushID_SE(pm, va);
	}

	/*
	 * Make sure executable pages do not have stale data in I$,
	 * which is VIPT.
	 */
	if (mapped && (prot & PROT_EXEC) != 0 && pmap_is_current(pm))
		cpu_icache_sync_range(va, PAGE_SIZE);

	return (0);
}

/*
 * pmap_remove()
 *
 * pmap_remove is responsible for nuking a number of mappings for a range
 * of virtual address space in the current pmap.
 */

void
pmap_remove(pmap_t pm, vaddr_t sva, vaddr_t eva)
{
	struct l2_bucket *l2b;
	vaddr_t next_bucket;
	pt_entry_t *ptep;
	u_int mappings;

	NPDEBUG(PDB_REMOVE, printf("pmap_remove: pmap=%p sva=%08lx eva=%08lx\n",
	    pm, sva, eva));

	while (sva < eva) {
		/*
		 * Do one L2 bucket's worth at a time.
		 */
		next_bucket = L2_NEXT_BUCKET(sva);
		if (next_bucket > eva)
			next_bucket = eva;

		l2b = pmap_get_l2_bucket(pm, sva);
		if (l2b == NULL) {
			sva = next_bucket;
			continue;
		}

		ptep = &l2b->l2b_kva[l2pte_index(sva)];
		mappings = 0;

		while (sva < next_bucket) {
			struct vm_page *pg;
			pt_entry_t pte;
			paddr_t pa;

			pte = *ptep;

			if (pte == L2_TYPE_INV) {
				/*
				 * Nothing here, move along
				 */
				sva += PAGE_SIZE;
				ptep++;
				continue;
			}

			pm->pm_stats.resident_count--;
			pa = l2pte_pa(pte);

			/*
			 * Update flags. In a number of circumstances,
			 * we could cluster a lot of these and do a
			 * number of sequential pages in one go.
			 */
			pg = PHYS_TO_VM_PAGE(pa);
			if (pg != NULL) {
				struct pv_entry *pve;
				pve = pmap_remove_pv(pg, pm, sva);
				if (pve != NULL)
					pool_put(&pmap_pv_pool, pve);
			}

			/*
			 * If the cache is physically indexed, we need
			 * to flush any changes to the page before it
			 * gets invalidated.	
			 */
			if (pg != NULL)
				pmap_clean_page(pg);

			*ptep = L2_TYPE_INV;
			PTE_SYNC(ptep);
			if (pte & L2_V7_AF)
				pmap_tlb_flushID_SE(pm, sva);

			sva += PAGE_SIZE;
			ptep++;
			mappings++;
		}

		/*
		 * Deal with any left overs
		 */
		if (!pmap_is_current(pm))
			cpu_idcache_wbinv_all();

		pmap_free_l2_bucket(pm, l2b, mappings);
	}
}

/*
 * pmap_kenter_pa: enter an unmanaged, wired kernel mapping
 *
 * We assume there is already sufficient KVM space available
 * to do this, as we can't allocate L2 descriptor tables/metadata
 * from here.
 */
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep, opte, npte;
	pt_entry_t cache_mode = pte_l2_s_cache_mode;

	NPDEBUG(PDB_KENTER,
	    printf("pmap_kenter_pa: va 0x%08lx, pa 0x%08lx, prot 0x%x\n",
	    va, pa, prot));

	l2b = pmap_get_l2_bucket(pmap_kernel(), va);
	KDASSERT(l2b != NULL);

	ptep = &l2b->l2b_kva[l2pte_index(va)];
	opte = *ptep;

	if (opte == L2_TYPE_INV)
		l2b->l2b_occupancy++;

	if (pa & PMAP_DEVICE)
		cache_mode = L2_B | L2_V7_S_XN;
	else if (pa & PMAP_NOCACHE)
		cache_mode = L2_V7_S_TEX(1);

	npte = L2_S_PROTO | (pa & PMAP_PA_MASK) | L2_V7_AF |
	    L2_S_PROT(PTE_KERNEL, prot) | cache_mode;
	*ptep = npte;
	PTE_SYNC(ptep);
	if (opte & L2_V7_AF)
		cpu_tlb_flushD_SE(va);

	if (pa & PMAP_NOCACHE) {
		cpu_dcache_wbinv_range(va, PAGE_SIZE);
		cpu_sdcache_wbinv_range(va, (pa & PMAP_PA_MASK), PAGE_SIZE);
	}
}

void
pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cacheable)
{
	if (cacheable == 0)
		pa |= PMAP_NOCACHE;
	pmap_kenter_pa(va, pa, prot);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep, *sptep, opte;
	vaddr_t next_bucket, eva;
	u_int mappings;

	NPDEBUG(PDB_KREMOVE, printf("pmap_kremove: va 0x%08lx, len 0x%08lx\n",
	    va, len));

	eva = va + len;

	while (va < eva) {
		next_bucket = L2_NEXT_BUCKET(va);
		if (next_bucket > eva)
			next_bucket = eva;

		l2b = pmap_get_l2_bucket(pmap_kernel(), va);
		KDASSERT(l2b != NULL);

		sptep = ptep = &l2b->l2b_kva[l2pte_index(va)];
		mappings = 0;

		while (va < next_bucket) {
			opte = *ptep;
			if (opte != L2_TYPE_INV) {
				*ptep = L2_TYPE_INV;
				PTE_SYNC(ptep);
				mappings++;
			}
			if (opte & L2_V7_AF)
				cpu_tlb_flushD_SE(va);
			va += PAGE_SIZE;
			ptep++;
		}
		KDASSERT(mappings <= l2b->l2b_occupancy);
		l2b->l2b_occupancy -= mappings;
	}
}

boolean_t
pmap_extract(pmap_t pm, vaddr_t va, paddr_t *pap)
{
	struct l2_dtable *l2;
	pd_entry_t *pl1pd, l1pd;
	pt_entry_t *ptep, pte;
	paddr_t pa;
	u_int l1idx;


	l1idx = L1_IDX(va);
	pl1pd = &pm->pm_l1->l1_kva[l1idx];
	l1pd = *pl1pd;

	if (l1pte_section_p(l1pd)) {
		/*
		 * These should only happen for pmap_kernel()
		 */
		KDASSERT(pm == pmap_kernel());
		pa = (l1pd & L1_S_FRAME) | (va & L1_S_OFFSET);
	} else {
		/*
		 * Note that we can't rely on the validity of the L1
		 * descriptor as an indication that a mapping exists.
		 * We have to look it up in the L2 dtable.
		 */
		l2 = pm->pm_l2[L2_IDX(l1idx)];

		if (l2 == NULL ||
		    (ptep = l2->l2_bucket[L2_BUCKET(l1idx)].l2b_kva) == NULL) {
			return (FALSE);
		}

		ptep = &ptep[l2pte_index(va)];
		pte = *ptep;

		if (pte == L2_TYPE_INV)
			return (FALSE);

		switch (pte & L2_TYPE_MASK) {
		case L2_TYPE_L:
			pa = (pte & L2_L_FRAME) | (va & L2_L_OFFSET);
			break;
		/*
		 * Can't check for L2_TYPE_S on V7 because of the XN
		 * bit being part of L2_TYPE_MASK for S mappings.
		 */
		default:
			pa = (pte & L2_S_FRAME) | (va & L2_S_OFFSET);
			break;
		}
	}

	if (pap != NULL)
		*pap = pa;

	return (TRUE);
}

void
pmap_protect(pmap_t pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep, opte, npte;
	vaddr_t next_bucket;
	int flush;

	NPDEBUG(PDB_PROTECT,
	    printf("pmap_protect: pm %p sva 0x%lx eva 0x%lx prot 0x%x",
	    pm, sva, eva, prot));

	if ((prot & (PROT_WRITE | PROT_EXEC)) == (PROT_WRITE | PROT_EXEC))
		return;

	if (prot == PROT_NONE) {
		pmap_remove(pm, sva, eva);
		return;
	}
		
	/* XXX is that threshold of 4 the best choice for v7? */
	if (pmap_is_current(pm))
		flush = ((eva - sva) > (PAGE_SIZE * 4)) ? -1 : 0;
	else
		flush = -1;

	while (sva < eva) {
		next_bucket = L2_NEXT_BUCKET(sva);
		if (next_bucket > eva)
			next_bucket = eva;

		l2b = pmap_get_l2_bucket(pm, sva);
		if (l2b == NULL) {
			sva = next_bucket;
			continue;
		}

		ptep = &l2b->l2b_kva[l2pte_index(sva)];

		while (sva < next_bucket) {
			npte = opte = *ptep;
			if (opte != L2_TYPE_INV) {
				struct vm_page *pg;

				if ((prot & PROT_WRITE) == 0)
					npte |= L2_V7_AP(0x4);
				if ((prot & PROT_EXEC) == 0)
					npte |= L2_V7_S_XN;
				*ptep = npte;
				PTE_SYNC(ptep);

				pg = PHYS_TO_VM_PAGE(l2pte_pa(opte));
				if (pg != NULL && (prot & PROT_WRITE) == 0)
					pmap_modify_pv(pg, pm, sva,
					    PVF_WRITE, 0);

				if (flush >= 0) {
					flush++;
					if (opte & L2_V7_AF)
						cpu_tlb_flushID_SE(sva);
				}
			}

			sva += PAGE_SIZE;
			ptep++;
		}
	}

	if (flush < 0)
		pmap_tlb_flushID(pm);

	NPDEBUG(PDB_PROTECT, printf("\n"));
}

void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{

	NPDEBUG(PDB_PROTECT,
	    printf("pmap_page_protect: pg %p (0x%08lx), prot 0x%x\n",
	    pg, pg->phys_addr, prot));

	switch(prot) {
	case PROT_READ | PROT_WRITE | PROT_EXEC:
	case PROT_READ | PROT_WRITE:
		return;

	case PROT_READ:
	case PROT_READ | PROT_EXEC:
		pmap_clearbit(pg, PVF_WRITE);
		break;

	default:
		pmap_page_remove(pg);
		break;
	}
}

/*
 * pmap_clear_modify:
 *
 *	Clear the "modified" attribute for a page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	boolean_t rv;

	if (pg->mdpage.pvh_attrs & PVF_MOD) {
		rv = TRUE;
		pmap_clearbit(pg, PVF_MOD);
	} else
		rv = FALSE;

	return (rv);
}

/*
 * pmap_clear_reference:
 *
 *	Clear the "referenced" attribute for a page.
 */
boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	boolean_t rv;

	if (pg->mdpage.pvh_attrs & PVF_REF) {
		rv = TRUE;
		pmap_clearbit(pg, PVF_REF);
	} else
		rv = FALSE;

	return (rv);
}

/*
 * pmap_is_modified:
 *
 *	Test if a page has the "modified" attribute.
 */
/* See <arm/pmap.h> */

/*
 * pmap_is_referenced:
 *
 *	Test if a page has the "referenced" attribute.
 */
/* See <arm/pmap.h> */

/*
 * dab_access() handles the following data aborts:
 *
 *  FAULT_ACCESS_2 - Access flag fault -- Level 2
 *
 * Set the Access Flag and mark the page as referenced.
 */
int
dab_access(trapframe_t *tf, u_int fsr, u_int far, struct proc *p)
{
	struct pmap *pm = p->p_vmspace->vm_map.pmap;
	vaddr_t va = trunc_page(far);
	struct l2_dtable *l2;
	struct l2_bucket *l2b;
	pt_entry_t *ptep, pte;
	struct pv_entry *pv;
	struct vm_page *pg;
	paddr_t pa;
	u_int l1idx;

	if (!TRAP_USERMODE(tf) && far >= VM_MIN_KERNEL_ADDRESS)
		pm = pmap_kernel();

	l1idx = L1_IDX(va);

	/*
	 * If there is no l2_dtable for this address, then the process
	 * has no business accessing it.
	 */
	l2 = pm->pm_l2[L2_IDX(l1idx)];
	KASSERT(l2 != NULL);

	/*
	 * Likewise if there is no L2 descriptor table
	 */
	l2b = &l2->l2_bucket[L2_BUCKET(l1idx)];
	KASSERT(l2b->l2b_kva != NULL);

	/*
	 * Check the PTE itself.
	 */
	ptep = &l2b->l2b_kva[l2pte_index(va)];
	pte = *ptep;
	KASSERT(pte != L2_TYPE_INV);

	pa = l2pte_pa(pte);

	/*
	 * Perform page referenced emulation.
	 */
	KASSERT((pte & L2_V7_AF) == 0);

	/* Extract the physical address of the page */
	pg = PHYS_TO_VM_PAGE(pa);
	KASSERT(pg != NULL);

	/* Get the current flags for this page. */
	pv = pmap_find_pv(pg, pm, va);
	KASSERT(pv != NULL);

	pg->mdpage.pvh_attrs |= PVF_REF;
	pv->pv_flags |= PVF_REF;
	pte |= L2_V7_AF;

	*ptep = pte;
	PTE_SYNC(ptep);
	return 0;
}

/*
 * pmap_collect: free resources held by a pmap
 *
 * => optional function.
 * => called when a process is swapped out to free memory.
 */
void
pmap_collect(pmap_t pm)
{
	/*
	 * Nothing to do.
	 * We don't even need to free-up the process' L1.
	 */
}

/*
 * Routine:	pmap_proc_iflush
 *
 * Function:
 *	Synchronize caches corresponding to [addr, addr+len) in p.
 *
 */
void
pmap_proc_iflush(struct process *pr, vaddr_t va, vsize_t len)
{
	/* We only need to do anything if it is the current process. */
	if (pr == curproc->p_p)
		cpu_icache_sync_range(va, len);
}

/*
 * Routine:	pmap_unwire
 * Function:	Clear the wired attribute for a map/virtual-address pair.
 *
 * In/out conditions:
 *		The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap_t pm, vaddr_t va)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep, pte;
	struct vm_page *pg;
	paddr_t pa;

	NPDEBUG(PDB_WIRING, printf("pmap_unwire: pm %p, va 0x%08lx\n", pm, va));

	l2b = pmap_get_l2_bucket(pm, va);
	KDASSERT(l2b != NULL);

	ptep = &l2b->l2b_kva[l2pte_index(va)];
	pte = *ptep;

	/* Extract the physical address of the page */
	pa = l2pte_pa(pte);

	if ((pg = PHYS_TO_VM_PAGE(pa)) != NULL) {
		/* Update the wired bit in the pv entry for this page. */
		(void) pmap_modify_pv(pg, pm, va, PVF_WIRED, 0);
	}
}

void
pmap_activate(struct proc *p)
{
	pmap_t pm;
	struct pcb *pcb;

	pm = p->p_vmspace->vm_map.pmap;
	pcb = &p->p_addr->u_pcb;

	pmap_set_pcb_pagedir(pm, pcb);

	if (p == curproc) {
		u_int cur_ttb;

		__asm volatile("mrc p15, 0, %0, c2, c0, 0" : "=r"(cur_ttb));

		cur_ttb &= ~(L1_TABLE_SIZE - 1);

		if (cur_ttb == (u_int)pcb->pcb_pagedir) {
			/*
			 * No need to switch address spaces.
			 */
			return;
		}

		__asm volatile("cpsid if");
		cpu_setttb(pcb->pcb_pagedir);
		__asm volatile("cpsie if");
	}
}

void
pmap_update(pmap_t pm)
{
	/*
	 * make sure TLB/cache operations have completed.
	 */
}

/*
 * Retire the given physical map from service.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_destroy(pmap_t pm)
{
	u_int count;

	/*
	 * Drop reference count
	 */
	count = --pm->pm_refs;
	if (count > 0)
		return;

	/*
	 * reference count is zero, free pmap resources and then free pmap.
	 */

	pmap_free_l1(pm);

	/* return the pmap to the pool */
	pool_put(&pmap_pmap_pool, pm);
}


/*
 * void pmap_reference(pmap_t pm)
 *
 * Add a reference to the specified pmap.
 */
void
pmap_reference(pmap_t pm)
{
	if (pm == NULL)
		return;

	pm->pm_refs++;
}

/*
 * pmap_zero_page()
 * 
 * Zero a given physical page by mapping it at a page hook point.
 * In doing the zero page op, the page we zero is mapped cachable, as with
 * StrongARM accesses to non-cached pages are non-burst making writing
 * _any_ bulk data very slow.
 */
void
pmap_zero_page_generic(struct vm_page *pg)
{
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
#ifdef DEBUG
	if (pg->mdpage.pvh_list != NULL)
		panic("pmap_zero_page: page has mappings");
#endif

	/*
	 * Hook in the page, zero it, and purge the cache for that
	 * zeroed page. Invalidate the TLB as needed.
	 */
	*cdst_pte = L2_S_PROTO | phys | L2_V7_AF |
	    L2_S_PROT(PTE_KERNEL, PROT_WRITE) | pte_l2_s_cache_mode;
	PTE_SYNC(cdst_pte);
	cpu_tlb_flushD_SE(cdstp);
	bzero_page(cdstp);
}

/*
 * pmap_copy_page()
 *
 * Copy one physical page into another, by mapping the pages into
 * hook points. The same comment regarding cachability as in
 * pmap_zero_page also applies here.
 */
void
pmap_copy_page_generic(struct vm_page *src_pg, struct vm_page *dst_pg)
{
	paddr_t src = VM_PAGE_TO_PHYS(src_pg);
	paddr_t dst = VM_PAGE_TO_PHYS(dst_pg);
#ifdef DEBUG
	if (dst_pg->mdpage.pvh_list != NULL)
		panic("pmap_copy_page: dst page has mappings");
#endif

	/*
	 * Map the pages into the page hook points, copy them, and purge
	 * the cache for the appropriate page. Invalidate the TLB
	 * as required.
	 */
	*csrc_pte = L2_S_PROTO | src | L2_V7_AF |
	    L2_S_PROT(PTE_KERNEL, PROT_READ) | pte_l2_s_cache_mode;
	PTE_SYNC(csrc_pte);
	*cdst_pte = L2_S_PROTO | dst | L2_V7_AF |
	    L2_S_PROT(PTE_KERNEL, PROT_WRITE) | pte_l2_s_cache_mode;
	PTE_SYNC(cdst_pte);
	cpu_tlb_flushD_SE(csrcp);
	cpu_tlb_flushD_SE(cdstp);
	bcopy_page(csrcp, cdstp);
}

/*
 * void pmap_virtual_space(vaddr_t *start, vaddr_t *end)
 *
 * Return the start and end addresses of the kernel's virtual space.
 * These values are setup in pmap_bootstrap and are updated as pages
 * are allocated.
 */
void
pmap_virtual_space(vaddr_t *start, vaddr_t *end)
{
	*start = virtual_avail;
	*end = virtual_end;
}

/*
 * Helper function for pmap_grow_l2_bucket()
 */
static __inline int
pmap_grow_map(vaddr_t va, pt_entry_t cache_mode, paddr_t *pap)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep;
	paddr_t pa;

	if (uvm.page_init_done == FALSE) {
		if (uvm_page_physget(&pa) == FALSE)
			return (1);
	} else {
		struct vm_page *pg;
		pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
		if (pg == NULL)
			return (1);
		pa = VM_PAGE_TO_PHYS(pg);
	}

	if (pap)
		*pap = pa;

	l2b = pmap_get_l2_bucket(pmap_kernel(), va);
	KDASSERT(l2b != NULL);

	ptep = &l2b->l2b_kva[l2pte_index(va)];
	*ptep = L2_S_PROTO | pa | L2_V7_AF | cache_mode |
	    L2_S_PROT(PTE_KERNEL, PROT_READ | PROT_WRITE);
	PTE_SYNC(ptep);
	cpu_tlb_flushD_SE(va);

	memset((void *)va, 0, PAGE_SIZE);
	return (0);
}

/*
 * This is the same as pmap_alloc_l2_bucket(), except that it is only
 * used by pmap_growkernel().
 */
static __inline struct l2_bucket *
pmap_grow_l2_bucket(pmap_t pm, vaddr_t va)
{
	struct l2_dtable *l2;
	struct l2_bucket *l2b;
	u_short l1idx;
	vaddr_t nva;

	l1idx = L1_IDX(va);

	if ((l2 = pm->pm_l2[L2_IDX(l1idx)]) == NULL) {
		/*
		 * No mapping at this address, as there is
		 * no entry in the L1 table.
		 * Need to allocate a new l2_dtable.
		 */
		nva = pmap_kernel_l2dtable_kva;
		if ((nva & PGOFSET) == 0) {
			/*
			 * Need to allocate a backing page
			 */
			if (pmap_grow_map(nva, pte_l2_s_cache_mode, NULL))
				return (NULL);
		}

		l2 = (struct l2_dtable *)nva;
		nva += sizeof(struct l2_dtable);

		if ((nva & PGOFSET) < (pmap_kernel_l2dtable_kva & PGOFSET)) {
			/*
			 * The new l2_dtable straddles a page boundary.
			 * Map in another page to cover it.
			 */
			if (pmap_grow_map(nva, pte_l2_s_cache_mode, NULL))
				return (NULL);
		}

		pmap_kernel_l2dtable_kva = nva;

		/*
		 * Link it into the parent pmap
		 */
		pm->pm_l2[L2_IDX(l1idx)] = l2;
	}

	l2b = &l2->l2_bucket[L2_BUCKET(l1idx)];

	/*
	 * Fetch pointer to the L2 page table associated with the address.
	 */
	if (l2b->l2b_kva == NULL) {
		pt_entry_t *ptep;

		/*
		 * No L2 page table has been allocated. Chances are, this
		 * is because we just allocated the l2_dtable, above.
		 */
		nva = pmap_kernel_l2ptp_kva;
		ptep = (pt_entry_t *)nva;
		if ((nva & PGOFSET) == 0) {
			/*
			 * Need to allocate a backing page
			 */
			if (pmap_grow_map(nva, pte_l2_s_cache_mode_pt,
			    &pmap_kernel_l2ptp_phys))
				return (NULL);
			PTE_SYNC_RANGE(ptep, PAGE_SIZE / sizeof(pt_entry_t));
		}

		l2->l2_occupancy++;
		l2b->l2b_kva = ptep;
		l2b->l2b_l1idx = l1idx;
		l2b->l2b_phys = pmap_kernel_l2ptp_phys;

		pmap_kernel_l2ptp_kva += L2_TABLE_SIZE_REAL;
		pmap_kernel_l2ptp_phys += L2_TABLE_SIZE_REAL;
	}

	return (l2b);
}

vaddr_t
pmap_growkernel(vaddr_t maxkvaddr)
{
	pmap_t kpm = pmap_kernel();
	struct l1_ttable *l1;
	struct l2_bucket *l2b;
	pd_entry_t *pl1pd;
	int s;

	if (maxkvaddr <= pmap_curmaxkvaddr)
		goto out;		/* we are OK */

	NPDEBUG(PDB_GROWKERN,
	    printf("pmap_growkernel: growing kernel from 0x%lx to 0x%lx\n",
	    pmap_curmaxkvaddr, maxkvaddr));

	KDASSERT(maxkvaddr <= virtual_end);

	/*
	 * whoops!   we need to add kernel PTPs
	 */

	s = splhigh();	/* to be safe */

	/* Map 1MB at a time */
	for (; pmap_curmaxkvaddr < maxkvaddr; pmap_curmaxkvaddr += L1_S_SIZE) {

		l2b = pmap_grow_l2_bucket(kpm, pmap_curmaxkvaddr);
		KDASSERT(l2b != NULL);

		/* Distribute new L1 entry to all other L1s */
		TAILQ_FOREACH(l1, &l1_list, l1_link) {
			pl1pd = &l1->l1_kva[L1_IDX(pmap_curmaxkvaddr)];
			*pl1pd = L1_C_PROTO | l2b->l2b_phys;
			PTE_SYNC(pl1pd);
		}
	}

	/*
	 * flush out the cache, expensive but growkernel will happen so
	 * rarely
	 */
	cpu_dcache_wbinv_all();
	cpu_sdcache_wbinv_all();
	cpu_tlb_flushD();

	splx(s);

out:
	return (pmap_curmaxkvaddr);
}

/************************ Utility routines ****************************/

/*
 * vector_page_setprot:
 *
 *	Manipulate the protection of the vector page.
 */
void
vector_page_setprot(int prot)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep;

	l2b = pmap_get_l2_bucket(pmap_kernel(), vector_page);
	KDASSERT(l2b != NULL);

	ptep = &l2b->l2b_kva[l2pte_index(vector_page)];

	*ptep = (*ptep & ~L2_S_PROT_MASK) | L2_S_PROT(PTE_KERNEL, prot);
	PTE_SYNC(ptep);
	cpu_tlb_flushD_SE(vector_page);
}

/*
 * This is used to stuff certain critical values into the PCB where they
 * can be accessed quickly from cpu_switch() et al.
 */
void
pmap_set_pcb_pagedir(pmap_t pm, struct pcb *pcb)
{
	KDASSERT(pm->pm_l1);
	pcb->pcb_pagedir = pm->pm_l1->l1_physaddr;
}

/*
 * Fetch pointers to the PDE/PTE for the given pmap/VA pair.
 * Returns TRUE if the mapping exists, else FALSE.
 *
 * NOTE: This function is only used by a couple of arm-specific modules.
 * It is not safe to take any pmap locks here, since we could be right
 * in the middle of debugging the pmap anyway...
 *
 * It is possible for this routine to return FALSE even though a valid
 * mapping does exist. This is because we don't lock, so the metadata
 * state may be inconsistent.
 *
 * NOTE: We can return a NULL *ptp in the case where the L1 pde is
 * a "section" mapping.
 */
boolean_t
pmap_get_pde_pte(pmap_t pm, vaddr_t va, pd_entry_t **pdp, pt_entry_t **ptp)
{
	struct l2_dtable *l2;
	pd_entry_t *pl1pd, l1pd;
	pt_entry_t *ptep;
	u_short l1idx;

	if (pm->pm_l1 == NULL)
		return (FALSE);

	l1idx = L1_IDX(va);
	*pdp = pl1pd = &pm->pm_l1->l1_kva[l1idx];
	l1pd = *pl1pd;

	if (l1pte_section_p(l1pd)) {
		*ptp = NULL;
		return (TRUE);
	}

	l2 = pm->pm_l2[L2_IDX(l1idx)];
	if (l2 == NULL ||
	    (ptep = l2->l2_bucket[L2_BUCKET(l1idx)].l2b_kva) == NULL) {
		return (FALSE);
	}

	*ptp = &ptep[l2pte_index(va)];
	return (TRUE);
}

/************************ Bootstrapping routines ****************************/

void
pmap_init_l1(struct l1_ttable *l1, pd_entry_t *l1pt)
{
	l1->l1_kva = l1pt;

	/*
	 * Copy the kernel's L1 entries to each new L1.
	 */
	if (pmap_initialized)
		memcpy(l1pt, pmap_kernel()->pm_l1->l1_kva, L1_TABLE_SIZE);

	if (pmap_extract(pmap_kernel(), (vaddr_t)l1pt,
	    &l1->l1_physaddr) == FALSE)
		panic("pmap_init_l1: can't get PA of L1 at %p", l1pt);

	TAILQ_INSERT_TAIL(&l1_list, l1, l1_link);
}

/*
 * pmap_bootstrap() is called from the board-specific initarm() routine
 * once the kernel L1/L2 descriptors tables have been set up.
 *
 * This is a somewhat convoluted process since pmap bootstrap is, effectively,
 * spread over a number of disparate files/functions.
 *
 * We are passed the following parameters
 *  - kernel_l1pt
 *    This is a pointer to the base of the kernel's L1 translation table.
 *  - vstart
 *    1MB-aligned start of managed kernel virtual memory.
 *  - vend
 *    1MB-aligned end of managed kernel virtual memory.
 *
 * We use the first parameter to build the metadata (struct l1_ttable and
 * struct l2_dtable) necessary to track kernel mappings.
 */
#define	PMAP_STATIC_L2_SIZE 16
void
pmap_bootstrap(pd_entry_t *kernel_l1pt, vaddr_t vstart, vaddr_t vend)
{
	static struct l1_ttable static_l1;
	static struct l2_dtable static_l2[PMAP_STATIC_L2_SIZE];
	struct l1_ttable *l1 = &static_l1;
	struct l2_dtable *l2;
	struct l2_bucket *l2b;
	pmap_t pm = pmap_kernel();
	pd_entry_t pde;
	pt_entry_t *ptep;
	paddr_t pa;
	vsize_t size;
	int l1idx, l2idx, l2next = 0;

	/*
	 * Initialise the kernel pmap object
	 */
	pm->pm_l1 = l1;
	pm->pm_refs = 1;

	/*
	 * Scan the L1 translation table created by initarm() and create
	 * the required metadata for all valid mappings found in it.
	 */
	for (l1idx = 0; l1idx < (L1_TABLE_SIZE / sizeof(pd_entry_t)); l1idx++) {
		pde = kernel_l1pt[l1idx];

		/*
		 * We're only interested in Coarse mappings.
		 * pmap_extract() can deal with section mappings without
		 * recourse to checking L2 metadata.
		 */
		if ((pde & L1_TYPE_MASK) != L1_TYPE_C)
			continue;

		/*
		 * Lookup the KVA of this L2 descriptor table
		 */
		pa = (paddr_t)(pde & L1_C_ADDR_MASK);
		ptep = (pt_entry_t *)kernel_pt_lookup(pa);
		if (ptep == NULL) {
			panic("pmap_bootstrap: No L2 for va 0x%x, pa 0x%lx",
			    (u_int)l1idx << L1_S_SHIFT, pa);
		}

		/*
		 * Fetch the associated L2 metadata structure.
		 * Allocate a new one if necessary.
		 */
		if ((l2 = pm->pm_l2[L2_IDX(l1idx)]) == NULL) {
			if (l2next == PMAP_STATIC_L2_SIZE)
				panic("pmap_bootstrap: out of static L2s");
			pm->pm_l2[L2_IDX(l1idx)] = l2 = &static_l2[l2next++];
		}

		/*
		 * One more L1 slot tracked...
		 */
		l2->l2_occupancy++;

		/*
		 * Fill in the details of the L2 descriptor in the
		 * appropriate bucket.
		 */
		l2b = &l2->l2_bucket[L2_BUCKET(l1idx)];
		l2b->l2b_kva = ptep;
		l2b->l2b_phys = pa;
		l2b->l2b_l1idx = l1idx;

		/*
		 * Establish an initial occupancy count for this descriptor
		 */
		for (l2idx = 0;
		    l2idx < (L2_TABLE_SIZE_REAL / sizeof(pt_entry_t));
		    l2idx++) {
			if (ptep[l2idx] != L2_TYPE_INV)
				l2b->l2b_occupancy++;
		}
	}

	cpu_idcache_wbinv_all();
	cpu_sdcache_wbinv_all();
	cpu_tlb_flushID();

	/*
	 * now we allocate the "special" VAs which are used for tmp mappings
	 * by the pmap (and other modules).  we allocate the VAs by advancing
	 * virtual_avail (note that there are no pages mapped at these VAs).
	 *
	 * Managed KVM space start from wherever initarm() tells us.
	 */
	virtual_avail = vstart;
	virtual_end = vend;

	pmap_alloc_specials(&virtual_avail, 1, &csrcp, &csrc_pte);
	pmap_alloc_specials(&virtual_avail, 1, &cdstp, &cdst_pte);
	pmap_alloc_specials(&virtual_avail, 1, &cwbp, &cwb_pte);
	pmap_alloc_specials(&virtual_avail, 1, (void *)&memhook, NULL);
	pmap_alloc_specials(&virtual_avail, round_page(MSGBUFSIZE) / PAGE_SIZE,
	    (void *)&msgbufaddr, NULL);

	/*
	 * Allocate a range of kernel virtual address space to be used
	 * for L2 descriptor tables and metadata allocation in
	 * pmap_growkernel().
	 */
	size = ((virtual_end - pmap_curmaxkvaddr) + L1_S_OFFSET) / L1_S_SIZE;
	pmap_alloc_specials(&virtual_avail,
	    round_page(size * L2_TABLE_SIZE_REAL) / PAGE_SIZE,
	    &pmap_kernel_l2ptp_kva, NULL);

	size = (size + (L2_BUCKET_SIZE - 1)) / L2_BUCKET_SIZE;
	pmap_alloc_specials(&virtual_avail,
	    round_page(size * sizeof(struct l2_dtable)) / PAGE_SIZE,
	    &pmap_kernel_l2dtable_kva, NULL);

	/*
	 * We can now initialise the first L1's metadata.
	 */
	TAILQ_INIT(&l1_list);
	pmap_init_l1(l1, kernel_l1pt);

	/*
	 * Initialize the pmap pool.
	 */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmappl", &pool_allocator_single);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pvepl", &pmap_bootstrap_pv_allocator);
	pool_init(&pmap_l2dtable_pool, sizeof(struct l2_dtable), 0, IPL_VM, 0,
	    "l2dtblpl", NULL);
	pool_init(&pmap_l2ptp_pool, L2_TABLE_SIZE_REAL, L2_TABLE_SIZE_REAL,
	    IPL_VM, 0, "l2ptppl", &pool_allocator_single);

	cpu_dcache_wbinv_all();
	cpu_sdcache_wbinv_all();
}

void
pmap_alloc_specials(vaddr_t *availp, int pages, vaddr_t *vap, pt_entry_t **ptep)
{
	vaddr_t va = *availp;
	struct l2_bucket *l2b;

	if (ptep) {
		l2b = pmap_get_l2_bucket(pmap_kernel(), va);
		if (l2b == NULL)
			panic("pmap_alloc_specials: no l2b for 0x%lx", va);

		if (ptep)
			*ptep = &l2b->l2b_kva[l2pte_index(va)];
	}

	*vap = va;
	*availp = va + (PAGE_SIZE * pages);
}

void
pmap_init(void)
{
	pool_setlowat(&pmap_pv_pool, (PAGE_SIZE / sizeof(struct pv_entry)) * 2);

	pmap_initialized = TRUE;
}

static vaddr_t last_bootstrap_page = 0;
static void *free_bootstrap_pages = NULL;

void *
pmap_bootstrap_pv_page_alloc(struct pool *pp, int flags, int *slowdown)
{
	extern void *pool_page_alloc(struct pool *, int, int *);
	vaddr_t new_page;
	void *rv;

	if (pmap_initialized)
		return (pool_page_alloc(pp, flags, slowdown));
	*slowdown = 0;

	if (free_bootstrap_pages) {
		rv = free_bootstrap_pages;
		free_bootstrap_pages = *((void **)rv);
		return (rv);
	}

	new_page = uvm_km_kmemalloc(kernel_map, NULL, PAGE_SIZE,
	    (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT);

	last_bootstrap_page = new_page;
	return ((void *)new_page);
}

void
pmap_bootstrap_pv_page_free(struct pool *pp, void *v)
{
	extern void pool_page_free(struct pool *, void *);

	if (pmap_initialized) {
		pool_page_free(pp, v);
		return;
	}

	if ((vaddr_t)v < last_bootstrap_page) {
		*((void **)v) = free_bootstrap_pages;
		free_bootstrap_pages = v;
		return;
	}
}

/*
 * pmap_postinit()
 *
 * This routine is called after the vm and kmem subsystems have been
 * initialised. This allows the pmap code to perform any initialisation
 * that can only be done once the memory allocation is in place.
 */
void
pmap_postinit(void)
{
	pool_setlowat(&pmap_l2ptp_pool,
	    (PAGE_SIZE / L2_TABLE_SIZE_REAL) * 4);
	pool_setlowat(&pmap_l2dtable_pool,
	    (PAGE_SIZE / sizeof(struct l2_dtable)) * 2);
}

/*
 * Note that the following routines are used by board-specific initialisation
 * code to configure the initial kernel page tables.
 *
 * If ARM32_NEW_VM_LAYOUT is *not* defined, they operate on the assumption that
 * L2 page-table pages are 4KB in size and use 4 L1 slots. This mimics the
 * behaviour of the old pmap, and provides an easy migration path for
 * initial bring-up of the new pmap on existing ports. Fortunately,
 * pmap_bootstrap() compensates for this hackery. This is only a stop-gap and
 * will be deprecated.
 *
 * If ARM32_NEW_VM_LAYOUT *is* defined, these functions deal with 1KB L2 page
 * tables.
 */

/*
 * This list exists for the benefit of pmap_map_chunk().  It keeps track
 * of the kernel L2 tables during bootstrap, so that pmap_map_chunk() can
 * find them as necessary.
 *
 * Note that the data on this list MUST remain valid after initarm() returns,
 * as pmap_bootstrap() uses it to contruct L2 table metadata.
 */
SLIST_HEAD(, pv_addr) kernel_pt_list = SLIST_HEAD_INITIALIZER(kernel_pt_list);

vaddr_t
kernel_pt_lookup(paddr_t pa)
{
	pv_addr_t *pv;

	SLIST_FOREACH(pv, &kernel_pt_list, pv_list) {
#ifndef ARM32_NEW_VM_LAYOUT
		if (pv->pv_pa == (pa & ~PGOFSET))
			return (pv->pv_va | (pa & PGOFSET));
#else
		if (pv->pv_pa == pa)
			return (pv->pv_va);
#endif
	}
	return (0);
}

/*
 * pmap_map_section:
 *
 *	Create a single section mapping.
 */
void
pmap_map_section(vaddr_t l1pt, vaddr_t va, paddr_t pa, int prot, int cache)
{
	pd_entry_t *pde = (pd_entry_t *) l1pt;
	pd_entry_t fl;

	switch (cache) {
	case PTE_NOCACHE:
	default:
		fl = 0;
		break;

	case PTE_CACHE:
		fl = pte_l1_s_cache_mode;
		break;

	case PTE_PAGETABLE:
		fl = pte_l1_s_cache_mode_pt;
		break;
	}

	pde[va >> L1_S_SHIFT] = L1_S_PROTO | pa | L1_S_V7_AF |
	    L1_S_PROT(PTE_KERNEL, prot) | fl;
	PTE_SYNC(&pde[va >> L1_S_SHIFT]);
}

/*
 * pmap_map_entry:
 *
 *	Create a single page mapping.
 */
void
pmap_map_entry(vaddr_t l1pt, vaddr_t va, paddr_t pa, int prot, int cache)
{
	pd_entry_t *pde = (pd_entry_t *) l1pt;
	pt_entry_t fl;
	pt_entry_t *pte;

	switch (cache) {
	case PTE_NOCACHE:
	default:
		fl = 0;
		break;

	case PTE_CACHE:
		fl = pte_l2_s_cache_mode;
		break;

	case PTE_PAGETABLE:
		fl = pte_l2_s_cache_mode_pt;
		break;
	}

	if ((pde[va >> L1_S_SHIFT] & L1_TYPE_MASK) != L1_TYPE_C)
		panic("pmap_map_entry: no L2 table for VA 0x%08lx", va);

#ifndef ARM32_NEW_VM_LAYOUT
	pte = (pt_entry_t *)
	    kernel_pt_lookup(pde[va >> L1_S_SHIFT] & L2_S_FRAME);
#else
	pte = (pt_entry_t *) kernel_pt_lookup(pde[L1_IDX(va)] & L1_C_ADDR_MASK);
#endif
	if (pte == NULL)
		panic("pmap_map_entry: can't find L2 table for VA 0x%08lx", va);

#ifndef ARM32_NEW_VM_LAYOUT
	pte[(va >> PGSHIFT) & 0x3ff] = L2_S_PROTO | pa | L2_V7_AF |
	    L2_S_PROT(PTE_KERNEL, prot) | fl;
	PTE_SYNC(&pte[(va >> PGSHIFT) & 0x3ff]);
#else
	pte[l2pte_index(va)] = L2_S_PROTO | pa | L2_V7_AF |
	    L2_S_PROT(PTE_KERNEL, prot) | fl;
	PTE_SYNC(&pte[l2pte_index(va)]);
#endif
}

/*
 * pmap_link_l2pt:
 *
 *	Link the L2 page table specified by "l2pv" into the L1
 *	page table at the slot for "va".
 */
void
pmap_link_l2pt(vaddr_t l1pt, vaddr_t va, pv_addr_t *l2pv)
{
	pd_entry_t *pde = (pd_entry_t *) l1pt;
	u_int slot = va >> L1_S_SHIFT;

	pde[slot + 0] = L1_C_PROTO | (l2pv->pv_pa + 0x000);
#ifdef ARM32_NEW_VM_LAYOUT
	PTE_SYNC(&pde[slot]);
#else
	pde[slot + 1] = L1_C_PROTO | (l2pv->pv_pa + 0x400);
	pde[slot + 2] = L1_C_PROTO | (l2pv->pv_pa + 0x800);
	pde[slot + 3] = L1_C_PROTO | (l2pv->pv_pa + 0xc00);
	PTE_SYNC_RANGE(&pde[slot + 0], 4);
#endif

	SLIST_INSERT_HEAD(&kernel_pt_list, l2pv, pv_list);
}

/*
 * pmap_map_chunk:
 *
 *	Map a chunk of memory using the most efficient mappings
 *	possible (section, large page, small page) into the
 *	provided L1 and L2 tables at the specified virtual address.
 */
vsize_t
pmap_map_chunk(vaddr_t l1pt, vaddr_t va, paddr_t pa, vsize_t size,
    int prot, int cache)
{
	pd_entry_t *pde = (pd_entry_t *) l1pt;
	pt_entry_t *pte, f1, f2s, f2l;
	vsize_t resid;  
	int i;

	resid = (size + (PAGE_SIZE - 1)) & ~(PAGE_SIZE - 1);

	if (l1pt == 0)
		panic("pmap_map_chunk: no L1 table provided");

#ifdef VERBOSE_INIT_ARM     
	printf("pmap_map_chunk: pa=0x%lx va=0x%lx size=0x%lx resid=0x%lx "
	    "prot=0x%x cache=%d\n", pa, va, size, resid, prot, cache);
#endif

	switch (cache) {
	case PTE_NOCACHE:
	default:
		f1 = 0;
		f2l = 0;
		f2s = 0;
		break;

	case PTE_CACHE:
		f1 = pte_l1_s_cache_mode;
		f2l = pte_l2_l_cache_mode;
		f2s = pte_l2_s_cache_mode;
		break;

	case PTE_PAGETABLE:
		f1 = pte_l1_s_cache_mode_pt;
		f2l = pte_l2_l_cache_mode_pt;
		f2s = pte_l2_s_cache_mode_pt;
		break;
	}

	size = resid;

	while (resid > 0) {
		/* See if we can use a section mapping. */
		if (L1_S_MAPPABLE_P(va, pa, resid)) {
#ifdef VERBOSE_INIT_ARM
			printf("S");
#endif
			pde[va >> L1_S_SHIFT] = L1_S_PROTO | pa |
			    L1_S_V7_AF | L1_S_PROT(PTE_KERNEL, prot) | f1;
			PTE_SYNC(&pde[va >> L1_S_SHIFT]);
			va += L1_S_SIZE;
			pa += L1_S_SIZE;
			resid -= L1_S_SIZE;
			continue;
		}

		/*
		 * Ok, we're going to use an L2 table.  Make sure
		 * one is actually in the corresponding L1 slot
		 * for the current VA.
		 */
		if ((pde[va >> L1_S_SHIFT] & L1_TYPE_MASK) != L1_TYPE_C)
			panic("pmap_map_chunk: no L2 table for VA 0x%08lx", va);

#ifndef ARM32_NEW_VM_LAYOUT
		pte = (pt_entry_t *)
		    kernel_pt_lookup(pde[va >> L1_S_SHIFT] & L2_S_FRAME);
#else
		pte = (pt_entry_t *) kernel_pt_lookup(
		    pde[L1_IDX(va)] & L1_C_ADDR_MASK);
#endif
		if (pte == NULL)
			panic("pmap_map_chunk: can't find L2 table for VA"
			    "0x%08lx", va);

		/* See if we can use a L2 large page mapping. */
		if (L2_L_MAPPABLE_P(va, pa, resid)) {
#ifdef VERBOSE_INIT_ARM
			printf("L");
#endif
			for (i = 0; i < 16; i++) {
#ifndef ARM32_NEW_VM_LAYOUT
				pte[((va >> PGSHIFT) & 0x3f0) + i] =
				    L2_L_PROTO | pa | L2_V7_AF |
				    L2_L_PROT(PTE_KERNEL, prot) | f2l;
				PTE_SYNC(&pte[((va >> PGSHIFT) & 0x3f0) + i]);
#else
				pte[l2pte_index(va) + i] =
				    L2_L_PROTO | pa | L2_V7_AF |
				    L2_L_PROT(PTE_KERNEL, prot) | f2l;
				PTE_SYNC(&pte[l2pte_index(va) + i]);
#endif
			}
			va += L2_L_SIZE;
			pa += L2_L_SIZE;
			resid -= L2_L_SIZE;
			continue;
		}

		/* Use a small page mapping. */
#ifdef VERBOSE_INIT_ARM
		printf("P");
#endif
#ifndef ARM32_NEW_VM_LAYOUT
		pte[(va >> PGSHIFT) & 0x3ff] = L2_S_PROTO | pa | L2_V7_AF |
		    L2_S_PROT(PTE_KERNEL, prot) | f2s;
		PTE_SYNC(&pte[(va >> PGSHIFT) & 0x3ff]);
#else
		pte[l2pte_index(va)] = L2_S_PROTO | pa | L2_V7_AF |
		    L2_S_PROT(PTE_KERNEL, prot) | f2s;
		PTE_SYNC(&pte[l2pte_index(va)]);
#endif
		va += PAGE_SIZE;
		pa += PAGE_SIZE;
		resid -= PAGE_SIZE;
	}
#ifdef VERBOSE_INIT_ARM
	printf("\n");
#endif
	return (size);
}

/********************** PTE initialization routines **************************/

/*
 * These routines are called when the CPU type is identified to set up
 * the PTE prototypes, cache modes, etc.
 *
 * The variables are always here, just in case LKMs need to reference
 * them (though, they shouldn't).
 */

pt_entry_t	pte_l1_s_cache_mode;
pt_entry_t	pte_l1_s_cache_mode_pt;
pt_entry_t	pte_l1_s_cache_mask;

pt_entry_t	pte_l2_l_cache_mode;
pt_entry_t	pte_l2_l_cache_mode_pt;
pt_entry_t	pte_l2_l_cache_mask;

pt_entry_t	pte_l2_s_cache_mode;
pt_entry_t	pte_l2_s_cache_mode_pt;
pt_entry_t	pte_l2_s_cache_mask;

pt_entry_t	pte_l1_s_coherent;
pt_entry_t	pte_l2_l_coherent;
pt_entry_t	pte_l2_s_coherent;

pt_entry_t	pte_l1_s_prot_ur;
pt_entry_t	pte_l1_s_prot_uw;
pt_entry_t	pte_l1_s_prot_kr;
pt_entry_t	pte_l1_s_prot_kw;
pt_entry_t	pte_l1_s_prot_mask;

pt_entry_t	pte_l2_l_prot_ur;
pt_entry_t	pte_l2_l_prot_uw;
pt_entry_t	pte_l2_l_prot_kr;
pt_entry_t	pte_l2_l_prot_kw;
pt_entry_t	pte_l2_l_prot_mask;

pt_entry_t	pte_l2_s_prot_ur;
pt_entry_t	pte_l2_s_prot_uw;
pt_entry_t	pte_l2_s_prot_kr;
pt_entry_t	pte_l2_s_prot_kw;
pt_entry_t	pte_l2_s_prot_mask;

pt_entry_t	pte_l1_s_proto;
pt_entry_t	pte_l1_c_proto;
pt_entry_t	pte_l2_s_proto;

void		(*pmap_copy_page_func)(struct vm_page *, struct vm_page *);
void		(*pmap_zero_page_func)(struct vm_page *);

void
pmap_pte_init_armv7(void)
{
	uint32_t id_mmfr0, id_mmfr3;

	/*
	 * XXX We want to use proper TEX settings eventually.
	 */

	/* write-allocate should be tested */
	pte_l1_s_cache_mode = L1_S_C|L1_S_B;
	pte_l2_l_cache_mode = L2_C|L2_B;
	pte_l2_s_cache_mode = L2_C|L2_B;

	pte_l1_s_cache_mode_pt = L1_S_B|L1_S_C;
	pte_l2_l_cache_mode_pt = L2_B|L2_C;
	pte_l2_s_cache_mode_pt = L2_B|L2_C;
	pmap_needs_pte_sync = 1;

	pte_l1_s_cache_mask = L1_S_CACHE_MASK_v7;
	pte_l2_l_cache_mask = L2_L_CACHE_MASK_v7;
	pte_l2_s_cache_mask = L2_S_CACHE_MASK_v7;

	pte_l1_s_coherent = L1_S_COHERENT_v7;
	pte_l2_l_coherent = L2_L_COHERENT_v7;
	pte_l2_s_coherent = L2_S_COHERENT_v7;

	pte_l1_s_prot_ur = L1_S_PROT_UR_v7;
	pte_l1_s_prot_uw = L1_S_PROT_UW_v7;
	pte_l1_s_prot_kr = L1_S_PROT_KR_v7;
	pte_l1_s_prot_kw = L1_S_PROT_KW_v7;
	pte_l1_s_prot_mask = L1_S_PROT_MASK_v7;

	pte_l2_l_prot_ur = L2_L_PROT_UR_v7;
	pte_l2_l_prot_uw = L2_L_PROT_UW_v7;
	pte_l2_l_prot_kr = L2_L_PROT_KR_v7;
	pte_l2_l_prot_kw = L2_L_PROT_KW_v7;
	pte_l2_l_prot_mask = L2_L_PROT_MASK_v7;

	pte_l2_s_prot_ur = L2_S_PROT_UR_v7;
	pte_l2_s_prot_uw = L2_S_PROT_UW_v7;
	pte_l2_s_prot_kr = L2_S_PROT_KR_v7;
	pte_l2_s_prot_kw = L2_S_PROT_KW_v7;
	pte_l2_s_prot_mask = L2_S_PROT_MASK_v7;

	pte_l1_s_proto = L1_S_PROTO_v7;
	pte_l1_c_proto = L1_C_PROTO_v7;
	pte_l2_s_proto = L2_S_PROTO_v7;

	pmap_copy_page_func = pmap_copy_page_generic;
	pmap_zero_page_func = pmap_zero_page_generic;

	/* Check if the PXN bit is supported. */
	__asm volatile("mrc p15, 0, %0, c0, c1, 4" : "=r"(id_mmfr0));
	if ((id_mmfr0 & ID_MMFR0_VMSA_MASK) >= VMSA_V7_PXN)
		l1_c_pxn = L1_C_V7_PXN;

	/* Check for coherent walk. */
	__asm volatile("mrc p15, 0, %0, c0, c1, 7" : "=r"(id_mmfr3));
	if ((id_mmfr3 & 0x00f00000) == 0x00100000)
		pmap_needs_pte_sync = 0;
}

uint32_t pmap_alias_dist;
uint32_t pmap_alias_bits;

vaddr_t
pmap_prefer(vaddr_t foff, vaddr_t va)
{
	long d, m;

	m = pmap_alias_dist;
	if (m == 0)             /* m=0 => no cache aliasing */
		return va;

	d = foff - va;
	d &= (m - 1);
	return va + d;
}
@


1.54
log
@Change pmap_proc_iflush() to take a process instead of a proc
powerpc: rename second argument of pmap_proc_iflush() to match other archs

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.53 2016/09/24 13:03:47 kettenis Exp $	*/
d1699 3
@


1.53
log
@Remove a couple of unsused static inline functions.  Also remove a comparis
of an array to a null pointer that is always false.  Found with clang.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.52 2016/09/15 02:00:17 dlg Exp $	*/
d1769 1
a1769 1
pmap_proc_iflush(struct proc *p, vaddr_t va, vsize_t len)
d1772 1
a1772 1
	if (p == curproc)
@


1.52
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.51 2016/08/31 12:24:12 jsg Exp $	*/
a448 7
pmap_tlb_flushD_SE(pmap_t pm, vaddr_t va)
{
	if (pmap_is_current(pm))
		cpu_tlb_flushD_SE(va);
}

static __inline void
a454 7
static __inline void
pmap_tlb_flushD(pmap_t pm)
{
	if (pmap_is_current(pm))
		cpu_tlb_flushD();
}

a2205 3
	if (pm->pm_l2 == NULL)
		return (FALSE);

a2206 1

@


1.51
log
@In pmap_activate instead of doing disable_interrupts/enable_interrupts
nested inside of splhigh/splx just disable and enable interrupts
once with inline cps instructions.

Remove uneeded pcb_pl1vec block as well, suggested by kettenis.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.50 2016/08/27 14:22:35 kettenis Exp $	*/
d2399 1
a2399 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,
d2401 3
a2403 13
	pool_setipl(&pmap_pmap_pool, IPL_NONE);

	/*
	 * Initialize the pv pool.
	 */
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvepl",
	    &pmap_bootstrap_pv_allocator);
	pool_setipl(&pmap_pv_pool, IPL_VM);

	/*
	 * Initialize the L2 dtable pool.
	 */
	pool_init(&pmap_l2dtable_pool, sizeof(struct l2_dtable), 0, 0, 0,
d2405 2
a2406 8
	pool_setipl(&pmap_l2dtable_pool, IPL_VM);

	/*
	 * Initialise the L2 descriptor table pool.
	 */
	pool_init(&pmap_l2ptp_pool, L2_TABLE_SIZE_REAL, L2_TABLE_SIZE_REAL, 0,
	    0, "l2ptppl", &pool_allocator_single);
	pool_setipl(&pmap_l2ptp_pool, IPL_VM);
@


1.50
log
@Add support for the PXN bit in level 1 translation table descriptors and
enable it on CPUs that support it.  When enabled, this prevents the kernel
from executing userland code.

ok jsg@@, tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.49 2016/08/26 16:02:33 kettenis Exp $	*/
a1826 1
	int s;
d1847 1
a1847 18
		s = splhigh();
		disable_interrupts(PSR_I | PSR_F);

		/*
		 * We MUST, I repeat, MUST fix up the L1 entry corresponding
		 * to 'vector_page' in the incoming L1 table before switching
		 * to it otherwise subsequent interrupts/exceptions (including
		 * domain faults!) will jump into hyperspace.
		 */
		if (pcb->pcb_pl1vec) {
			*pcb->pcb_pl1vec = pcb->pcb_l1vec;
			/*
			 * Don't need to PTE_SYNC() at this point since
			 * cpu_setttb() is about to flush both the cache
			 * and the TLB.
			 */
		}

d1849 1
a1849 4

		enable_interrupts(PSR_I | PSR_F);

		splx(s);
a2182 1
	pcb->pcb_pl1vec = NULL;
@


1.49
log
@Remove cpu_cpwait() calls; they are no-ops on armv7.

ok tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.48 2016/08/26 11:59:04 kettenis Exp $	*/
d306 5
d1275 1
a1275 1
			l1pd = L1_C_PROTO | l2b->l2b_phys;
d2869 1
a2869 1
	uint32_t id_mmfr3;
d2917 5
@


1.48
log
@Remove the code that switches around MMU domains on armv7.  MMU domains are
basically a relic from the past.  Using them doesn't make a lot of sense
the way our pmaps work.  Support for MMU domains isn't present in
long-descriptor translation table format, so it is clearly on its way out.

Based on a diff from Artituri Alm.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.47 2016/08/24 13:09:52 kettenis Exp $	*/
a1049 3

	if (flush)
		cpu_cpwait();
d1422 1
a1422 1
	if (opte & L2_V7_AF) {
a1423 2
		cpu_cpwait();
	}
a1477 1
	cpu_cpwait();
a1874 1
	cpu_cpwait();
a1942 1
	cpu_cpwait();
a1975 1
	cpu_cpwait();
a2159 1
	cpu_cpwait();
a2187 1
	cpu_cpwait();
a2373 1
	cpu_cpwait();
@


1.47
log
@Replace pmap_fault_fixup() with an access flag fault handler on armv7.

ok tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.46 2016/08/20 21:08:16 kettenis Exp $	*/
d391 1
a391 1
void		pmap_alloc_l1(pmap_t, int);
d625 1
a625 1
pmap_alloc_l1(pmap_t pm, int domain)
d635 1
a635 1
printf("%s: %d %d\n", __func__, domain, ++nl1);
a668 1
	pm->pm_domain = domain;
d804 1
a804 1
	pd_entry_t *pl1pd, l1pd;
d845 1
a845 2
	 * If the L1 slot matches the pmap's domain
	 * number, then invalidate it.
d847 3
a849 6
	l1pd = *pl1pd & (L1_TYPE_MASK | L1_C_DOM_MASK);
	if (l1pd == (L1_C_DOM(pm->pm_domain) | L1_TYPE_C)) {
		*pl1pd = L1_TYPE_INV;
		PTE_SYNC(pl1pd);
		pmap_tlb_flushID_SE(pm, l1idx << L1_S_SHIFT);
	}
d1069 1
a1069 1
	pmap_alloc_l1(pm, PMAP_DOMAIN_USER_V7);
d1268 1
a1268 2
			 * L1 entry to avoid taking another
			 * page/domain fault.
d1273 1
a1273 2
			l1pd = l2b->l2b_phys | L1_C_DOM(pm->pm_domain) |
			    L1_C_PROTO;
d1836 1
a1836 1
		u_int cur_dacr, cur_ttb;
a1838 1
		__asm volatile("mrc p15, 0, %0, c3, c0, 0" : "=r"(cur_dacr));
d1842 1
a1842 2
		if (cur_ttb == (u_int)pcb->pcb_pagedir &&
		    cur_dacr == pcb->pcb_dacr) {
a1866 1
		cpu_domains(pcb->pcb_dacr);
d2157 1
a2157 2
			*pl1pd = l2b->l2b_phys | L1_C_DOM(PMAP_DOMAIN_KERNEL) |
			    L1_C_PROTO;
a2209 3
	pcb->pcb_dacr = (DOMAIN_CLIENT << (PMAP_DOMAIN_KERNEL * 2)) |
	    (DOMAIN_CLIENT << (pm->pm_domain * 2));

a2319 1
	pm->pm_domain = PMAP_DOMAIN_KERNEL;
d2613 1
a2613 1
	    L1_S_PROT(PTE_KERNEL, prot) | fl | L1_S_DOM(PMAP_DOMAIN_KERNEL);
d2676 1
a2676 1
	pd_entry_t *pde = (pd_entry_t *) l1pt, proto;
d2679 1
a2679 3
	proto = L1_C_DOM(PMAP_DOMAIN_KERNEL) | L1_C_PROTO;

	pde[slot + 0] = proto | (l2pv->pv_pa + 0x000);
d2683 3
a2685 3
	pde[slot + 1] = proto | (l2pv->pv_pa + 0x400);
	pde[slot + 2] = proto | (l2pv->pv_pa + 0x800);
	pde[slot + 3] = proto | (l2pv->pv_pa + 0xc00);
d2748 1
a2748 2
			    L1_S_V7_AF | L1_S_PROT(PTE_KERNEL, prot) |
			    f1 | L1_S_DOM(PMAP_DOMAIN_KERNEL);
@


1.46
log
@Fix indentation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.45 2016/08/20 21:07:07 kettenis Exp $	*/
d1702 7
d1710 1
a1710 1
pmap_fault_fixup(pmap_t pm, vaddr_t va, vm_prot_t ftype, int user)
d1712 2
a1715 1
	pd_entry_t *pl1pd, l1pd;
d1717 2
a1720 1
	int rv = 0;
a1726 3
	 *
	 * Note: This will catch userland processes trying to access
	 * kernel addresses.
d1729 1
a1729 2
	if (l2 == NULL)
		goto out;
d1735 1
a1735 2
	if (l2b->l2b_kva == NULL)
		goto out;
d1742 1
a1742 2
	if (pte == L2_TYPE_INV)
		goto out;
d1744 1
a1744 2
	if ((ftype & PROT_EXEC) && (pte & L2_V7_S_XN))
		goto out;
a1745 1
	/* only if vectors are low ?? */
d1747 1
a1747 1
	 * Catch a userland access to the vector page mapped at 0x0
d1749 1
a1749 4
	if (user) {
		if ((pte & L2_V7_AP(0x2)) == 0)
			goto out;
	}
d1751 3
a1753 1
	pa = l2pte_pa(pte);
d1755 3
a1757 7
	if ((ftype & PROT_WRITE) && !l2pte_is_writeable(pte, pm)) {
		/*
		 * This looks like a good candidate for "page modified"
		 * emulation...
		 */
		struct pv_entry *pv;
		struct vm_page *pg;
d1759 3
a1761 8
		/* Extract the physical address of the page */
		if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL)
			goto out;

		/* Get the current flags for this page. */
		pv = pmap_find_pv(pg, pm, va);
		if (pv == NULL)
			goto out;
d1763 3
a1765 77
		/*
		 * Do the flags say this page is writable? If not then it
		 * is a genuine write fault. If yes then the write fault is
		 * our fault as we did not reflect the write access in the
		 * PTE. Now we know a write has occurred we can correct this
		 * and also set the modified bit
		 */
		if ((pv->pv_flags & PVF_WRITE) == 0)
			goto out;

		NPDEBUG(PDB_FOLLOW,
		    printf("pmap_fault_fixup: mod emul. pm %p, va 0x%08lx, pa 0x%08lx\n",
		    pm, va, pg->phys_addr));

		pg->mdpage.pvh_attrs |= PVF_REF | PVF_MOD;
		pv->pv_flags |= PVF_REF | PVF_MOD;

		/* 
		 * Re-enable write permissions for the page.
		 * We've already set the cacheable bits based on
		 * the assumption that we can write to this page.
		 */
		*ptep = (pte & ~L2_V7_AP(0x4));
		PTE_SYNC(ptep);
		rv = 1;
	} else if ((pte & L2_V7_AF) == 0) {
		/*
		 * This looks like a good candidate for "page referenced"
		 * emulation.
		 */
		struct pv_entry *pv;
		struct vm_page *pg;

		/* Extract the physical address of the page */
		if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL)
			goto out;

		/* Get the current flags for this page. */
		pv = pmap_find_pv(pg, pm, va);
		if (pv == NULL)
			goto out;

		pg->mdpage.pvh_attrs |= PVF_REF;
		pv->pv_flags |= PVF_REF;
		pte |= L2_V7_AF;

		NPDEBUG(PDB_FOLLOW,
		    printf("pmap_fault_fixup: ref emul. pm %p, va 0x%08lx, pa 0x%08lx\n",
		    pm, va, pg->phys_addr));

		*ptep = pte;
		PTE_SYNC(ptep);
		rv = 1;
	} else {
printf("%s: va %08lx ftype %x %c pte %08x\n", __func__, va, ftype, user ? 'u' : 's', pte);
		goto out;
	}

	/*
	 * We know there is a valid mapping here, so simply
	 * fix up the L1 if necessary.
	 */
	pl1pd = &pm->pm_l1->l1_kva[l1idx];
	l1pd = l2b->l2b_phys | L1_C_DOM(pm->pm_domain) | L1_C_PROTO;
	if (*pl1pd != l1pd) {
		*pl1pd = l1pd;
		PTE_SYNC(pl1pd);
		rv = 1;
	}

	if (rv) {
		cpu_tlb_flushID_SE(va);
		cpu_cpwait();
	}

out:
	return (rv);
@


1.45
log
@Argh, commit from the wrong tree.  Revert previous commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.44 2016/08/20 21:04:18 kettenis Exp $	*/
d1169 1
a1169 1
			     (pg->mdpage.pvh_attrs & PVF_MOD)) {
@


1.44
log
@Fix indentation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.43 2016/08/20 12:36:59 kettenis Exp $	*/
d1169 1
a1169 1
			    (pg->mdpage.pvh_attrs & PVF_MOD)) {
a1704 1
#if 0
d1797 28
a1825 1
extern int last_fault_code;
a1826 1
printf("fsr 0x%08x\n", last_fault_code);
a1848 86
#else
	return 0;
#endif
}

/*
 * dab_access() handles the following data aborts:
 *
 *  Access flag fault -- Level 1
 *  Access flag fault -- Level 2
 *
 * Set the Access Flag and mark the page as referenced.
 */
int
dab_access(trapframe_t *tf, u_int fsr, u_int far, struct proc *p)
{
	struct pmap *pm = p->p_vmspace->vm_map.pmap;
	vaddr_t va = trunc_page(far);
	struct l2_dtable *l2;
	struct l2_bucket *l2b;
	pt_entry_t *ptep, pte;
	struct pv_entry *pv;
	struct vm_page *pg;
	paddr_t pa;
	u_int l1idx;

	l1idx = L1_IDX(va);

	/*
	 * If there is no l2_dtable for this address, then the process
	 * has no business accessing it.
	 */
	l2 = pm->pm_l2[L2_IDX(l1idx)];
	if (l2 == NULL) {
		printf("l2\n");
		return 1;
	}

	/*
	 * Likewise if there is no L2 descriptor table
	 */
	l2b = &l2->l2_bucket[L2_BUCKET(l1idx)];
	if (l2b->l2b_kva == NULL) {
		printf("l2b\n");
		return 1;
	}

	/*
	 * Check the PTE itself.
	 */
	ptep = &l2b->l2b_kva[l2pte_index(va)];
	pte = *ptep;
	if (pte == L2_TYPE_INV) {
		printf("pte\n");
		return 1;
	}

	pa = l2pte_pa(pte);

	/*
	 * Perform page referenced emulation.
	 */
	KASSERT((pte & L2_V7_AF) == 0);

	/* Extract the physical address of the page */
	if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL) {
		printf("pg va 0x%08lx pa 0x%08lx pte 0x%08x\n", va, pa, pte);
		printf("fsr 0x%08x far 0x%08x\n", fsr, far);
		Debugger();
		return 1;
	}

	/* Get the current flags for this page. */
	pv = pmap_find_pv(pg, pm, va);
	if (pv == NULL) {
		printf("pv\n");
		return 1;
	}

	pg->mdpage.pvh_attrs |= PVF_REF;
	pv->pv_flags |= PVF_REF;
	pte |= L2_V7_AF;

	*ptep = pte;
	PTE_SYNC(ptep);
	return 0;
@


1.43
log
@Correctly enter a mapping as writable if no "page modified" emulation
is needed.

ok visa@@, patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.42 2016/08/19 17:31:04 kettenis Exp $	*/
d1169 1
a1169 1
			     (pg->mdpage.pvh_attrs & PVF_MOD)) {
d1705 1
a1797 28
	} else if ((pte & L2_V7_AF) == 0) {
		/*
		 * This looks like a good candidate for "page referenced"
		 * emulation.
		 */
		struct pv_entry *pv;
		struct vm_page *pg;

		/* Extract the physical address of the page */
		if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL)
			goto out;

		/* Get the current flags for this page. */
		pv = pmap_find_pv(pg, pm, va);
		if (pv == NULL)
			goto out;

		pg->mdpage.pvh_attrs |= PVF_REF;
		pv->pv_flags |= PVF_REF;
		pte |= L2_V7_AF;

		NPDEBUG(PDB_FOLLOW,
		    printf("pmap_fault_fixup: ref emul. pm %p, va 0x%08lx, pa 0x%08lx\n",
		    pm, va, pg->phys_addr));

		*ptep = pte;
		PTE_SYNC(ptep);
		rv = 1;
d1799 1
d1801 1
d1824 86
@


1.42
log
@Start using to XN flag to enforce that mappings without PROT_EXEC are
non-executable.

ok visa@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.41 2016/08/19 15:47:27 kettenis Exp $	*/
d1168 2
a1169 3
			if ((prot & PROT_WRITE) != 0 &&
			    ((flags & PROT_WRITE) != 0 ||
			     (pg->mdpage.pvh_attrs & PVF_MOD) != 0)) {
d1177 2
d1245 1
a1245 2
	npte |= L2_S_PROT(pm == pmap_kernel() ?  PTE_KERNEL : PTE_USER,
	    prot & ~PROT_WRITE);
@


1.41
log
@Mark device memory as execute-never to prevent a speculative instruction fetch
to access it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.40 2016/08/19 13:56:08 kettenis Exp $	*/
d1556 1
a1556 1
	pt_entry_t *ptep, opte;
a1557 1
	u_int flags;
d1564 1
a1564 3
	if ((prot & PROT_READ) == 0) {
NPDEBUG(PDB_PROTECT, printf("\n"));
		pmap_remove(pm, sva, eva);
a1565 1
	}
d1567 2
a1568 7
	if (prot & PROT_WRITE) {
		/*
		 * If this is a read->write transition, just ignore it and let
		 * uvm_fault() take care of it later.
		 */
NPDEBUG(PDB_PROTECT, printf("\n"));
/* XXX WHAT IF RWX -> RW ??? */
d1571 1
a1571 5

	/*
	 * OK, at this point, we know we're doing write-protect operation.
	 */

a1576 1
	flags = 0;
d1592 2
a1593 2
			opte = *ptep;
			if (opte != L2_TYPE_INV && l2pte_is_writeable(opte, pm)) {
a1594 1
				u_int f;
d1596 5
a1600 2
				pg = PHYS_TO_VM_PAGE(l2pte_pa(opte));
				*ptep = opte | L2_V7_AP(0x4);
d1603 3
a1605 2
				if (pg != NULL) {
					f = pmap_modify_pv(pg, pm, sva,
a1606 2
				} else
					f = PVF_EXEC;
d1612 1
a1612 2
				} else
					flags |= f;
d1623 1
a1623 1
NPDEBUG(PDB_PROTECT, printf("\n"));
d1741 3
a1754 9
	if ((ftype & PROT_EXEC) && (pte & L2_V7_S_XN)) {
printf("%s: va %08lx ftype %x %c pte %08x\n", __func__, va, ftype, user ? 'u' : 's', pte);
printf("fault on exec\n");
#ifdef DDB
Debugger();
#endif
		/* XXX FIX THIS */
		goto out;
	}
@


1.40
log
@Use Access Flag to do page reference emulation.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.39 2016/08/18 09:28:22 kettenis Exp $	*/
d1424 1
a1424 1
		cache_mode = L2_B;
@


1.39
log
@Separate out the Access Flag bit from the Access Permission bits in the
armv7 pmap.

ok tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.38 2016/08/16 10:16:33 kettenis Exp $	*/
d924 2
a925 4
			/* make the pte read only */
			npte = (npte & ~L2_S_PROT_MASK) |
			    L2_S_PROT(pm == pmap_kernel() ? PTE_KERNEL : PTE_USER,
			      npte & L2_V7_S_XN ? PROT_READ : PROT_READ | PROT_EXEC);
d930 3
a932 3
			 * Make the PTE invalid so that we will take a
			 * page fault the next time the mapping is
			 * referenced.
d934 1
a934 2
			npte = (npte & ~L2_TYPE_MASK) | L2_TYPE_INV |
			    (npte & L2_V7_S_XN);
d941 1
a941 1
			if (l2pte_valid(opte))
d997 1
a997 1
	pt_entry_t *ptep;
d1020 2
a1021 1
		if (*ptep != 0) {
d1023 1
a1023 1
			if (l2pte_valid(*ptep) &&
d1137 1
a1137 1
	npte = pa | L2_V7_AF;
d1139 1
a1139 1
	if (opte != 0) {	/* not l2pte_valid!!! MIOD */
a1164 2
			npte |= L2_S_PROTO;

d1166 1
a1182 2
			npte &= ~L2_TYPE_MASK;
			npte |= L2_TYPE_INV;
d1229 1
a1229 1
		npte |= L2_S_PROTO;
d1250 1
a1250 1
	if (opte == 0) {	/* !! not l2pte_valid MIOD */
d1269 1
a1269 1
		if (/* va != vector_page && */ l2pte_valid(npte)) {
d1287 1
a1287 1
		if (l2pte_valid(opte))
d1343 1
a1343 1
			if (pte == 0) {	/* !!! not l2pte_valid */
d1378 1
a1378 1
			if (l2pte_valid(pte))
d1420 1
a1420 1
	if (opte == 0)
d1432 1
a1432 1
	if (l2pte_valid(opte)) {
d1477 1
a1477 1
			if (opte != 0) {	/* !! not l2pte_valid */
d1482 1
a1482 1
			if (l2pte_valid(opte))
d1529 1
a1529 1
		if (pte == 0)	/* !!! not l2pte_valid */
d1556 1
a1556 1
	pt_entry_t *ptep, pte, opte;
d1607 1
a1607 3
			/* !!! not l2pte_valid */
/* XXX actually would only matter if really valid ??? */
			if (opte != 0 && l2pte_is_writeable(opte, pm)) {
d1612 1
a1612 4
				pte = (opte & ~L2_S_PROT_MASK) |
				    L2_S_PROT(pm == pmap_kernel() ? PTE_KERNEL : PTE_USER,
				      opte & L2_V7_S_XN ? PROT_READ : PROT_READ | PROT_EXEC);
				*ptep = pte;
d1623 1
a1623 1
					if (l2pte_valid(opte))
d1752 1
a1752 1
	if (pte == 0)
d1760 1
a1760 3
		/* XXX use of L2_V7_S_XN */
		if ((pte & L2_S_PROT_MASK & ~L2_V7_S_XN) != L2_S_PROT(PTE_USER, PROT_READ) &&
		    (pte & L2_S_PROT_MASK & ~L2_V7_S_XN) != L2_S_PROT(PTE_USER, PROT_WRITE))
d1814 1
a1814 3
		*ptep = (pte & ~(L2_TYPE_MASK|L2_S_PROT_MASK)) | L2_S_PROTO |
		    L2_S_PROT(pm == pmap_kernel() ? PTE_KERNEL : PTE_USER,
		      pte & L2_V7_S_XN ? PROT_WRITE : PROT_WRITE | PROT_EXEC);
d1817 1
a1817 3
	} else
	/* XXX use of L2_V7_S_XN */
	if ((pte & L2_TYPE_MASK & ~L2_V7_S_XN) == L2_TYPE_INV) {
d1836 1
d1842 1
a1842 2
		/* XXX use of L2_V7_S_XN */
		*ptep = (pte & ~(L2_TYPE_MASK & ~L2_V7_S_XN)) | L2_S_PROTO;
d2495 1
a2495 1
			if ((ptep[l2idx] & L2_TYPE_MASK) != L2_TYPE_INV)
@


1.38
log
@Fix typo/inconsistensy where L1_S_DOMAIN was used instead of L1_C_DOMAIN.
These are functionally equivolent so it didn't matter and the resulting
code doesn't change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.37 2016/08/11 00:28:06 kettenis Exp $	*/
d1139 1
a1139 1
	npte = pa;
d1433 1
a1433 1
	npte = L2_S_PROTO | (pa & PMAP_PA_MASK) |
d2075 1
a2075 1
	*cdst_pte = L2_S_PROTO | phys |
d2105 1
a2105 1
	*csrc_pte = L2_S_PROTO | src |
d2108 1
a2108 1
	*cdst_pte = L2_S_PROTO | dst |
d2159 1
a2159 1
	*ptep = L2_S_PROTO | pa | cache_mode |
d2746 1
a2746 1
	pde[va >> L1_S_SHIFT] = L1_S_PROTO | pa |
d2791 2
a2792 2
	pte[(va >> PGSHIFT) & 0x3ff] =
	    L2_S_PROTO | pa | L2_S_PROT(PTE_KERNEL, prot) | fl;
d2795 2
a2796 2
	pte[l2pte_index(va)] =
	    L2_S_PROTO | pa | L2_S_PROT(PTE_KERNEL, prot) | fl;
d2884 2
a2885 2
			    L1_S_PROT(PTE_KERNEL, prot) | f1 |
			    L1_S_DOM(PMAP_DOMAIN_KERNEL);
d2920 1
a2920 1
				    L2_L_PROTO | pa |
d2925 1
a2925 1
				    L2_L_PROTO | pa |
d2941 2
a2942 2
		pte[(va >> PGSHIFT) & 0x3ff] =
		    L2_S_PROTO | pa | L2_S_PROT(PTE_KERNEL, prot) | f2s;
d2945 2
a2946 2
		pte[l2pte_index(va)] =
		    L2_S_PROTO | pa | L2_S_PROT(PTE_KERNEL, prot) | f2s;
@


1.37
log
@The ARMv7 ARM says that the TLB may hold translation table entries at any
level of the translation table, including entries that point to further
levels of the tables.  This means that we have to do a TLB flush whenever
we invalidate an L1 slot too.  Doing so fixes the pmap_fault_fixup
issue on Cortex-A7 processors.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.36 2016/08/09 13:13:51 kettenis Exp $	*/
d2813 1
a2813 1
	proto = L1_S_DOM(PMAP_DOMAIN_KERNEL) | L1_C_PROTO;
@


1.36
log
@The page tables are cached now, and given the significant speedup, I
don't think we'll ever go back.  So let's ditch the code that tries to
check and patch up incorrect memory attributes.

Also realize that pmap_clean_page(pg, FALSE) doesn't do anything
anymore so remove those calls and drop the 2nd argument from
pmap_clean_page(pg, TRUE) calls.

Last but not least, get rid of pmap_pte_init_generic() here.  The only
useful thing it did was setting pmap_copy_page_func() and
pmap_zero_page_func().

This diff should not introduce any change in behaviour.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.35 2016/08/08 14:47:52 kettenis Exp $	*/
d853 1
@


1.35
log
@Mapping non-cachable memory as cachable and subsequently changing the mapping
to non-cachable is retarded.  Fix this by introducing PMAP_NOCACHE and
PMAP_DEVICE flags that can be or'ed into the physical address passed to
pmap_kenter(9), like we have on many of our other architectures.  This way we
can also properly distinguish between device memory and normal (non-cachable)
memory.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.34 2016/08/08 09:06:47 kettenis Exp $	*/
a380 1
int		pmap_set_pt_cache_mode(pd_entry_t *, vaddr_t);
a396 1
void		pmap_l2ptp_ctor(void *);
d399 1
a399 1
void		pmap_clean_page(struct vm_page *, int);
a626 1
	struct l2_bucket *l2b;
a630 1
	pt_entry_t *ptep, pte;
a662 16
		/*
		 * Make sure the L1 descriptor table is mapped
		 * with the cache-mode set to write-through, or
		 * correctly synced.
		 */
		l2b = pmap_get_l2_bucket(pmap_kernel(), va);
		ptep = &l2b->l2b_kva[l2pte_index(va)];
		pte = *ptep;

		if ((pte & L2_S_CACHE_MASK) != pte_l2_s_cache_mode_pt) {
			pte = (pte & ~L2_S_CACHE_MASK) | pte_l2_s_cache_mode_pt;
			*ptep = pte;
			PTE_SYNC(ptep);
			cpu_tlb_flushD_SE(va);
		}

d781 1
a781 1
		pmap_l2ptp_ctor(ptep);
a876 34
 * Cache constructors for L2 descriptor tables, metadata and pmap
 * structures.
 */
void
pmap_l2ptp_ctor(void *v)
{
	struct l2_bucket *l2b;
	pt_entry_t *ptep, pte;
	vaddr_t va = (vaddr_t)v & ~PGOFSET;

	/*
	 * The mappings for these page tables were initially made using
	 * pmap_kenter_pa() by the pool subsystem. Therefore, the cache-
	 * mode will not be right for page table mappings. To avoid
	 * polluting the pmap_kenter_pa() code with a special case for
	 * page tables, we simply fix up the cache-mode here if it's not
	 * correct.
	 */
	l2b = pmap_get_l2_bucket(pmap_kernel(), va);
	KDASSERT(l2b != NULL);
	ptep = &l2b->l2b_kva[l2pte_index(va)];
	pte = *ptep;

	if ((pte & L2_S_CACHE_MASK) != pte_l2_s_cache_mode_pt) {
		*ptep = (pte & ~L2_S_CACHE_MASK) | pte_l2_s_cache_mode_pt;
		PTE_SYNC(ptep);
		cpu_tlb_flushD_SE(va);
		cpu_cpwait();
	}

	PTE_SYNC_RANGE(v, L2_TABLE_SIZE_REAL / sizeof(pt_entry_t));
}

/*
a903 7
	 * If we are changing a writable or modified page to
	 * read-only (or worse), be sure to flush it first.
	 */
	if (maskbits & (PVF_WRITE|PVF_MOD))
		pmap_clean_page(pg, FALSE);

	/*
d956 1
a956 7
 * This is a local function used to work out the best strategy to writeback
 * a single page.
 *
 * Its policy is effectively:
 *  o If there are no mappings, we don't bother doing anything with the cache.
 *  o If there is a valid mapping, we use it to clean that page.
 *  o If there is no valid mapping, we create a temporary one and wb through it.
d959 1
a959 1
pmap_clean_page(struct vm_page *pg, int isync)
d964 4
a967 9
	/*
	 * To save time, we are only walking the pv list if an I$ invalidation
	 * is required.  Otherwise all we need is to map the page and writeback.
	 */
	if (isync) {
		if (curproc)
			pm = curproc->p_vmspace->vm_map.pmap;
		else
			pm = pmap_kernel();
d969 4
a972 4
		for (pv = pg->mdpage.pvh_list; pv; pv = pv->pv_next) {
			/* inline !pmap_is_current(pv->pv_pmap) */
			if (pv->pv_pmap != pmap_kernel() && pv->pv_pmap != pm)
				continue;
d974 6
a979 6
			/*
			 * The page is mapped non-cacheable in 
			 * this map.  No need to flush the cache.
			 */
			if (pv->pv_flags & PVF_NC) /* XXX ought to be pg attr */
				break;
d981 2
a982 3
			if (PV_BEEN_EXECD(pv->pv_flags))
				cpu_icache_sync_range(pv->pv_va, PAGE_SIZE);
		}
a1076 5
	/*
	 * Note: The pool ctor ensures that the pm_l2[] array is already
	 * initialised to zero.
	 */

d1378 1
a1378 1
				pmap_clean_page(pg, TRUE);
a1617 2
				if (pg != NULL)
					pmap_clean_page(pg, FALSE);
a2444 1
	vaddr_t va;
a2512 22

		/*
		 * Make sure the descriptor itself has the correct cache mode.
		 * If not, fix it, but whine about the problem. Port-meisters
		 * should consider this a clue to fix up their initarm()
		 * function. :)
		 */
		if (pmap_set_pt_cache_mode(kernel_l1pt, (vaddr_t)ptep)) {
			printf("pmap_bootstrap: WARNING! wrong cache mode for "
			    "L2 pte @@ %p\n", ptep);
		}
	}

	/*
	 * Ensure the primary (kernel) L1 has the correct cache mode for
	 * a page table. Bitch if it is not correctly set.
	 */
	for (va = (vaddr_t)kernel_l1pt;
	    va < ((vaddr_t)kernel_l1pt + L1_TABLE_SIZE); va += PAGE_SIZE) {
		if (pmap_set_pt_cache_mode(kernel_l1pt, va))
			printf("pmap_bootstrap: WARNING! wrong cache mode for "
			    "primary L1 @@ 0x%lx\n", va);
a2530 1
	pmap_set_pt_cache_mode(kernel_l1pt, (vaddr_t)csrc_pte);
a2531 1
	pmap_set_pt_cache_mode(kernel_l1pt, (vaddr_t)cdst_pte);
a2532 1
	pmap_set_pt_cache_mode(kernel_l1pt, (vaddr_t)cwb_pte);
a2589 42
int
pmap_set_pt_cache_mode(pd_entry_t *kl1, vaddr_t va)
{
	pd_entry_t *pdep, pde;
	pt_entry_t *ptep, pte;
	vaddr_t pa;
	int rv = 0;

	/*
	 * Make sure the descriptor itself has the correct cache mode
	 */
	pdep = &kl1[L1_IDX(va)];
	pde = *pdep;

	if (l1pte_section_p(pde)) {
		if ((pde & L1_S_CACHE_MASK) != pte_l1_s_cache_mode_pt) {
			*pdep = (pde & ~L1_S_CACHE_MASK) |
			    pte_l1_s_cache_mode_pt;
			PTE_SYNC(pdep);
			cpu_tlb_flushD_SE(va);
			rv = 1;
		}
	} else {
		pa = (paddr_t)(pde & L1_C_ADDR_MASK);
		ptep = (pt_entry_t *)kernel_pt_lookup(pa);
		if (ptep == NULL)
			panic("pmap_bootstrap: No L2 for L2 @@ va %p", ptep);

		ptep = &ptep[l2pte_index(va)];
		pte = *ptep;
		if ((pte & L2_S_CACHE_MASK) != pte_l2_s_cache_mode_pt) {
			*ptep = (pte & ~L2_S_CACHE_MASK) |
			    pte_l2_s_cache_mode_pt;
			PTE_SYNC(ptep);
			cpu_tlb_flushD_SE(va);
			rv = 1;
		}
	}

	return (rv);
}

a3009 58
pmap_pte_init_generic(void)
{

	pte_l1_s_cache_mode = L1_S_B|L1_S_C;
	pte_l1_s_cache_mask = L1_S_CACHE_MASK_generic;

	pte_l2_l_cache_mode = L2_B|L2_C;
	pte_l2_l_cache_mask = L2_L_CACHE_MASK_generic;

	pte_l2_s_cache_mode = L2_B|L2_C;
	pte_l2_s_cache_mask = L2_S_CACHE_MASK_generic;

	/*
	 * If we have a write-through cache, set B and C.  If
	 * we have a write-back cache, then we assume setting
	 * only C will make those pages write-through.
	 */
	if (cpufuncs.cf_dcache_wb_range == (void *) cpufunc_nullop) {
		pte_l1_s_cache_mode_pt = L1_S_B|L1_S_C;
		pte_l2_l_cache_mode_pt = L2_B|L2_C;
		pte_l2_s_cache_mode_pt = L2_B|L2_C;
	} else {
		pte_l1_s_cache_mode_pt = L1_S_C;
		pte_l2_l_cache_mode_pt = L2_C;
		pte_l2_s_cache_mode_pt = L2_C;
	}

	pte_l1_s_coherent = L1_S_COHERENT_generic;
	pte_l2_l_coherent = L2_L_COHERENT_generic;
	pte_l2_s_coherent = L2_S_COHERENT_generic;

	pte_l1_s_prot_ur = L1_S_PROT_UR_generic;
	pte_l1_s_prot_uw = L1_S_PROT_UW_generic;
	pte_l1_s_prot_kr = L1_S_PROT_KR_generic;
	pte_l1_s_prot_kw = L1_S_PROT_KW_generic;
	pte_l1_s_prot_mask = L1_S_PROT_MASK_generic;

	pte_l2_l_prot_ur = L2_L_PROT_UR_generic;
	pte_l2_l_prot_uw = L2_L_PROT_UW_generic;
	pte_l2_l_prot_kr = L2_L_PROT_KR_generic;
	pte_l2_l_prot_kw = L2_L_PROT_KW_generic;
	pte_l2_l_prot_mask = L2_L_PROT_MASK_generic;

	pte_l2_s_prot_ur = L2_S_PROT_UR_generic;
	pte_l2_s_prot_uw = L2_S_PROT_UW_generic;
	pte_l2_s_prot_kr = L2_S_PROT_KR_generic;
	pte_l2_s_prot_kw = L2_S_PROT_KW_generic;
	pte_l2_s_prot_mask = L2_S_PROT_MASK_generic;

	pte_l1_s_proto = L1_S_PROTO_generic;
	pte_l1_c_proto = L1_C_PROTO_generic;
	pte_l2_s_proto = L2_S_PROTO_generic;

	pmap_copy_page_func = pmap_copy_page_generic;
	pmap_zero_page_func = pmap_zero_page_generic;
}

void
d3015 1
a3015 3
	 * XXX 
	 * ARMv7 is compatible with generic, but we want to use proper TEX
	 * settings eventually
a3016 1
	pmap_pte_init_generic();
d3057 3
@


1.34
log
@ARMv7 data caches are "effectively" PIPT.  This means there is in general
no need to clean and/or invalidate cached pages.  So remove most of the
cache cleaning and invalidation from the pmap.  We still need to synchronize
the instruction cache with the data cache in various places though.  And we
also need to make sure that we clean and invalidate when we make a page
non-cachable.

Tested by Daniel Bolgheroni, mglocker@@ and jsg@@. on Cortex-A8 and myself on
Cortex-A9.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.33 2016/08/06 16:46:25 kettenis Exp $	*/
a930 27
 * Make a pmap_kernel() mapping uncached. Used by bus_dma for coherent pages.
 */
void
pmap_uncache_page(paddr_t va, vaddr_t pa)
{
	struct vm_page *pg;
	struct pv_entry *pv;
	pt_entry_t *pte;

	if ((pg = PHYS_TO_VM_PAGE(pa)) != NULL) {
		pv = pmap_find_pv(pg, pmap_kernel(), va);
		if (pv != NULL)
			pv->pv_flags |= PVF_NC;	/* XXX ought to be pg attr */
	}

	pte = vtopte(va);
	*pte &= ~L2_S_CACHE_MASK;
	*pte |= L2_B; /* device memory */
	PTE_SYNC(pte);
	cpu_tlb_flushD_SE(va);
	cpu_cpwait();

	cpu_dcache_wbinv_range(va, PAGE_SIZE);
	cpu_sdcache_wbinv_range(va, pa, PAGE_SIZE);
}

/*
d1490 1
d1505 7
a1511 2
	npte = L2_S_PROTO | pa | L2_S_PROT(PTE_KERNEL, prot) |
	    pte_l2_s_cache_mode;
d1518 5
d1528 2
a1530 2
	if (cacheable == 0)
		pmap_uncache_page(va, pa);
@


1.33
log
@Put page tables in normal cachable memory on armv7.  Check if the MMU walks
the page tables coherently and also skip flushing modified ptes out of the
cache in that case.  Speeds up building a kernel with a factor of two on
Cortex-A9 (tested by me) and Cortex-A8 (tested by mglocker@@).

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.32 2016/08/03 11:52:43 kettenis Exp $	*/
a919 3
	/* XXX redundant with PTE_SYNC_RANGE() ? */
	cpu_idcache_wbinv_range(va, PAGE_SIZE);
	cpu_sdcache_wbinv_range(va, pte & L2_S_FRAME, PAGE_SIZE);
d952 3
a1056 1
	boolean_t wb = FALSE;
a1081 11

			/*
			 * If we have not written back that page yet, do this
			 * now while we still have a valid mapping for it.
			 */
			if (!wb) {
				cpu_dcache_wb_range(pv->pv_va, PAGE_SIZE);
				cpu_sdcache_wb_range(pv->pv_va,
				    VM_PAGE_TO_PHYS(pg), PAGE_SIZE);
				wb = TRUE;
			}
a1083 15

	/*
	 * If there is no active mapping left, or we did not bother checking
	 * for one, this does not mean the page doesn't have stale data. Map
	 * it in a working page and writeback.
	 */
	if (!wb) {
		*cwb_pte = L2_S_PROTO | VM_PAGE_TO_PHYS(pg) |
		    L2_S_PROT(PTE_KERNEL, PROT_WRITE) | pte_l2_s_cache_mode;
		PTE_SYNC(cwb_pte);
		cpu_tlb_flushD_SE(cwbp);
		cpu_cpwait();
		cpu_dcache_wb_range(cwbp, PAGE_SIZE);
		cpu_sdcache_wb_range(cwbp, VM_PAGE_TO_PHYS(pg), PAGE_SIZE);
	}
a1127 9
				if (flush == FALSE) {
					paddr_t pa;
					cpu_dcache_wb_range(pv->pv_va,
					    PAGE_SIZE);
					if (pmap_extract(pm, (vaddr_t)pv->pv_va,
					    &pa))
						cpu_sdcache_wb_range(pv->pv_va,
						    pa, PAGE_SIZE);
				}
a1304 12

			/*
			 * We may need to flush the cache if we're
			 * doing rw-ro...
			 */
			if ((oflags & PVF_NC) == 0 &&
			    l2pte_is_writeable(opte, pm) &&
			    (prot & PROT_WRITE) == 0) {
				cpu_dcache_wb_range(va, PAGE_SIZE);
				cpu_sdcache_wb_range(va, opte & L2_S_FRAME,
				    PAGE_SIZE);
			}
a1527 4
	if (l2pte_valid(opte)) {
		cpu_dcache_wb_range(va, PAGE_SIZE);
		cpu_sdcache_wb_range(va, opte & L2_S_FRAME, PAGE_SIZE);
	} else
a1574 5
			if (l2pte_valid(opte)) {
				cpu_dcache_wb_range(va, PAGE_SIZE);
				cpu_sdcache_wb_range(va, opte & L2_S_FRAME,
				    PAGE_SIZE);
			}
d2500 1
a2500 1
	if (pmap_initialized) {
a2501 3
/* XXX overkill? */
		cpu_dcache_wb_range((vaddr_t)l1pt, L1_TABLE_SIZE);
	}
@


1.32
log
@Simplify the way we handle TLB flushes.  Since ARMv7 effectively has a
unified TLB there is not much point in optimizing TLB flushing for pages
that have never been executable.  The only difference is a flush of the
branch predictor and even that isn't necessary anymore on all but the oldest
Cortex cores.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.31 2016/07/31 22:27:07 kettenis Exp $	*/
d3294 1
a3294 1
	uint32_t cachereg;
d3308 4
a3311 3
	pte_l1_s_cache_mode_pt = L1_S_C;
	pte_l2_l_cache_mode_pt = L2_C;
	pte_l2_s_cache_mode_pt = L2_C;
d3343 4
a3346 20
	/* probe L1 dcache */
	__asm volatile("mcr p15, 2, %0, c0, c0, 0" :: "r" (0) );
	__asm volatile("mrc p15, 1, %0, c0, c0, 0" : "=r" (cachereg) );
	if ((cachereg & 0x80000000) == 0) {
#if 0
		/*
		 * pmap_pte_init_generic() has defaulted to write-through
		 * settings for pte pages, but the cache does not support
		 * write-through.
		 */
		pmap_needs_pte_sync = 1;
		pte_l1_s_cache_mode_pt = L1_S_B|L1_S_C;
		pte_l2_l_cache_mode_pt = L2_B|L2_C;
		pte_l2_s_cache_mode_pt = L2_B|L2_C;
#endif
		/* XXX: Don't cache PTEs, until write-back is fixed. */
		pte_l1_s_cache_mode_pt = L1_S_V7_TEX(1);
		pte_l2_l_cache_mode_pt = L2_V7_L_TEX(1);
		pte_l2_s_cache_mode_pt = L2_V7_S_TEX(1);
	}
@


1.31
log
@Remove devmap stuff which is unused on armv7.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.30 2016/07/31 22:04:44 kettenis Exp $	*/
d1031 2
a1032 6
			if (l2pte_valid(opte)) {
				if (PV_BEEN_EXECD(oflags))
					pmap_tlb_flushID_SE(pm, pv->pv_va);
				else
					pmap_tlb_flushD_SE(pm, pv->pv_va);
			}
a1279 1
	oflags = 0;
a1364 1
				oflags = pve->pv_flags;
a1393 2
			oflags = pve->pv_flags;

d1444 2
a1445 6
		if (l2pte_valid(opte)) {
			if (PV_BEEN_EXECD(oflags))
				pmap_tlb_flushID_SE(pm, va);
			else
				pmap_tlb_flushD_SE(pm, va);
		}
d1471 1
a1471 1
	u_int mappings, is_exec;
a1510 1
			is_exec = 0;
d1521 1
a1521 2
				if (pve != NULL) {
					is_exec = PV_BEEN_EXECD(pve->pv_flags);
a1522 1
				}
d1535 2
a1536 6
			if (l2pte_valid(pte)) {
				if (is_exec)
					pmap_tlb_flushID_SE(pm, sva);
				else
					pmap_tlb_flushD_SE(pm, sva);
			}
d1785 2
a1786 6
					if (l2pte_valid(opte)) {
						if (PV_BEEN_EXECD(f))
							cpu_tlb_flushID_SE(sva);
						else
							cpu_tlb_flushD_SE(sva);
					}
d1796 3
a1798 6
	if (flush < 0) {
		if (PV_BEEN_EXECD(flags))
			pmap_tlb_flushID(pm);
		else
			pmap_tlb_flushD(pm);
	}
@


1.30
log
@According to te armv7 ARM TLB entries that caused a Permission fault might
be held in the TLB.  On top of that valid page table entries might be
speculatively loaded into the TLB.  As a result we need to flush TLB entries
even when the page in question has not been referenced.

Fixes pmap_fault_fixup messages on Cortex-A53, and presumably also on
Cortex-A7.

ok patrick@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.29 2016/07/29 06:46:15 patrick Exp $	*/
a3205 80
}

/********************** Static device map routines ***************************/

const struct pmap_devmap *pmap_devmap_table;

/*
 * Register the devmap table.  This is provided in case early console
 * initialization needs to register mappings created by bootstrap code
 * before pmap_devmap_bootstrap() is called.
 */
void
pmap_devmap_register(const struct pmap_devmap *table)
{

	pmap_devmap_table = table;
}

/*
 * Map all of the static regions in the devmap table, and remember
 * the devmap table so other parts of the kernel can look up entries
 * later.
 */
void
pmap_devmap_bootstrap(vaddr_t l1pt, const struct pmap_devmap *table)
{
	int i;

	pmap_devmap_table = table;

	for (i = 0; pmap_devmap_table[i].pd_size != 0; i++) {
#ifdef VERBOSE_INIT_ARM
		printf("devmap: %08lx -> %08lx @@ %08lx\n",
		    pmap_devmap_table[i].pd_pa,
		    pmap_devmap_table[i].pd_pa +
			pmap_devmap_table[i].pd_size - 1,
		    pmap_devmap_table[i].pd_va);
#endif
		pmap_map_chunk(l1pt, pmap_devmap_table[i].pd_va,
		    pmap_devmap_table[i].pd_pa,
		    pmap_devmap_table[i].pd_size,
		    pmap_devmap_table[i].pd_prot,
		    pmap_devmap_table[i].pd_cache);
	}
}

const struct pmap_devmap *
pmap_devmap_find_pa(paddr_t pa, psize_t size)
{
	int i;

	if (pmap_devmap_table == NULL)
		return (NULL);

	for (i = 0; pmap_devmap_table[i].pd_size != 0; i++) {
		if (pa >= pmap_devmap_table[i].pd_pa &&
		    pa + size <= pmap_devmap_table[i].pd_pa +
				 pmap_devmap_table[i].pd_size)
			return (&pmap_devmap_table[i]);
	}

	return (NULL);
}

const struct pmap_devmap *
pmap_devmap_find_va(vaddr_t va, vsize_t size)
{
	int i;

	if (pmap_devmap_table == NULL)
		return (NULL);

	for (i = 0; pmap_devmap_table[i].pd_size != 0; i++) {
		if (va >= pmap_devmap_table[i].pd_va &&
		    va + size <= pmap_devmap_table[i].pd_va +
				 pmap_devmap_table[i].pd_size)
			return (&pmap_devmap_table[i]);
	}

	return (NULL);
@


1.29
log
@Only flush the virtual page if it was actually mapped.  Otherwise
we will run into translation faults.

ok tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.28 2016/07/27 21:12:49 patrick Exp $	*/
d376 1
a376 7
#define	PV_BEEN_EXECD(f)  (((f) & (PVF_REF | PVF_EXEC)) == (PVF_REF | PVF_EXEC))

/*
 * Macro to determine if a mapping might be resident in the
 * data cache and/or TLB
 */
#define	PV_BEEN_REFD(f)   (((f) & PVF_REF) != 0)
d1031 6
a1036 5
			if (PV_BEEN_EXECD(oflags))
				pmap_tlb_flushID_SE(pm, pv->pv_va);
			else
			if (PV_BEEN_REFD(oflags))
				pmap_tlb_flushD_SE(pm, pv->pv_va);
d1452 6
a1457 5
		if (PV_BEEN_EXECD(oflags))
			pmap_tlb_flushID_SE(pm, va);
		else
		if (PV_BEEN_REFD(oflags))
			pmap_tlb_flushD_SE(pm, va);
d1483 1
a1483 1
	u_int mappings, is_exec, is_refd;
a1523 1
			is_refd = l2pte_valid(pte);
a1535 1
					is_refd = PV_BEEN_REFD(pve->pv_flags);
d1550 6
a1555 5
			if (is_exec)
				pmap_tlb_flushID_SE(pm, sva);
			else
			if (is_refd)
				pmap_tlb_flushD_SE(pm, sva);
d1730 1
a1730 1
	pt_entry_t *ptep, pte;
d1780 1
a1780 1
			pte = *ptep;
d1783 1
a1783 1
			if (pte != 0 && l2pte_is_writeable(pte, pm)) {
d1787 1
a1787 1
				pg = PHYS_TO_VM_PAGE(l2pte_pa(pte));
d1790 1
a1790 1
				pte = (pte & ~L2_S_PROT_MASK) |
d1792 1
a1792 1
				      pte & L2_V7_S_XN ? PROT_READ : PROT_READ | PROT_EXEC);
d1800 1
a1800 1
					f = PVF_REF | PVF_EXEC;
d1804 6
a1809 5
					if (PV_BEEN_EXECD(f))
						cpu_tlb_flushID_SE(sva);
					else
					if (PV_BEEN_REFD(f))
						cpu_tlb_flushD_SE(sva);
a1822 1
		if (PV_BEEN_REFD(flags))
@


1.28
log
@When pmap_page_remove() is called by UVM, a physical page is to be
removed from pmaps it currently is in.  To check if a virtual address
pointing to that physical page has been mapped, the code uses
the l2pte_valid() function.  Unfortunately there is a difference
between being valid and the PTE being zero.  If a page is mapped
but has never been accessed, it will be non-zero but invalid.

In that case the PTE for that virtual address will not be zeroed
and the virtual address will be removed from the vm page struct.

The next time someone tries to map a page to that virtual address,
other pmap code will consider the virtual address to be already
mapped, even though that assumption is completely wrong.

To make sure this does not happen, check the PTE for zero.  This way
the PTE will be zeroed correctly.  The check for zero is how other
ARM pmap code also handles this issue.

ok kettenis@@ tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.27 2016/07/19 02:26:15 tom Exp $	*/
d1135 1
a1135 1
	pt_entry_t *ptep, pte;
a1158 2
			pte = *ptep;

d1160 2
a1161 1
			if (pm == curpm || pm == pmap_kernel()) {
@


1.27
log
@Remove what appears to be a copy-paste error setting cur_ttb
in pmap_free_l1().

from aalm@@ - thanks

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.26 2016/07/18 13:38:11 tom Exp $	*/
d1158 1
a1158 1
		if (l2pte_valid(*ptep)) {
@


1.26
log
@Don't need a separate flags variable in armv7 pmap_clean_page() -
just use the pv_flags.  ('Twas a copy-paste from arm's pmap_clean_page(),
which did need it.)

Also remove even less used flags variable from pmap_page_remove().

First part from a diff from aalm@@ - thanks

ok kettenis@@ "looks good" patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.25 2016/06/07 06:23:19 dlg Exp $	*/
a712 7
{
	u_int cur_ttb;

	__asm volatile("mrc p15, 0, %0, c2, c0, 0" : "=r"(cur_ttb));
	cur_ttb &= ~(L1_TABLE_SIZE - 1);

}
@


1.25
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.24 2016/03/03 04:28:21 jsg Exp $	*/
a1073 1
	uint flags = 0;
a1089 2
			flags |= pv->pv_flags;

d1097 1
a1097 1
			if (PV_BEEN_EXECD(flags))
a1143 1
	u_int flags;
a1152 1
	flags = 0;
a1191 2

			flags |= pv->pv_flags;
@


1.24
log
@When a physical address is needed to flush the secondary cache use
VM_PAGE_TO_PHYS() instead of unnecessarily calling pmap_extract().

From Patrick Wildt.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.23 2016/02/01 04:28:45 jsg Exp $	*/
d2789 3
a2791 2
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_single);
d2798 1
d2805 1
d2812 1
@


1.23
log
@Fix the encoding of AP bits for large page second-level
short-descriptors with arm v7 (same as small page encoding, except XN is
in a different bit for the mask).

Expanded version of a diff from Patrick Wildt who also tested and
reviewed this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.22 2016/01/31 00:14:50 jsg Exp $	*/
a1107 1
				paddr_t pa;
d1109 2
a1110 3
				if (pmap_extract(pm, (vaddr_t)pv->pv_va, &pa))
					cpu_sdcache_wb_range(pv->pv_va, pa,
					    PAGE_SIZE);
a1126 1
		paddr_t pa;
d1128 1
a1128 2
		if (pmap_extract(pmap_kernel(), (vaddr_t)cwbp, &pa))
			cpu_sdcache_wb_range(cwbp, pa, PAGE_SIZE);
@


1.22
log
@Switch from PSR_X_bit and X32_bit PSR macro names to just PSR_X.
This matches FreeBSD and makes things a bit more consistent.
Discussed with Patrick.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.21 2015/09/08 21:28:36 kettenis Exp $	*/
d3340 6
d3397 6
d3451 6
@


1.21
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.20 2015/02/02 09:29:53 mlarkin Exp $	*/
d2180 1
a2180 1
		disable_interrupts(I32_bit | F32_bit);
d2200 1
a2200 1
		enable_interrupts(I32_bit | F32_bit);
@


1.20
log
@
Remove some pmap locks that were #defined to be nothing (empty). Discussed
with many, ok kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.19 2015/01/29 20:10:50 deraadt Exp $	*/
d2794 1
a2794 1
	    &pool_allocator_nointr);
d2812 1
a2812 1
	    0, "l2ptppl", &pool_allocator_nointr);
@


1.19
log
@remove no-op simple locks
tested by jsg, ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.18 2014/11/16 12:30:56 deraadt Exp $	*/
a282 9
 * Misc. locking data structures
 */

#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */
#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

/*
a988 2
	PMAP_HEAD_TO_MAP_LOCK();

d994 1
a994 2
	if (pg->mdpage.pvh_list == NULL) {
		PMAP_HEAD_TO_MAP_UNLOCK();
a995 1
	}
a1054 2

	PMAP_HEAD_TO_MAP_UNLOCK();
a1155 2
	PMAP_HEAD_TO_MAP_LOCK();

d1157 1
a1157 2
	if (pv == NULL) {
		PMAP_HEAD_TO_MAP_UNLOCK();
a1158 1
	}
a1219 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a1290 2
	PMAP_MAP_TO_HEAD_LOCK();

d1300 1
a1300 2
		if (flags & PMAP_CANFAIL) {
			PMAP_MAP_TO_HEAD_UNLOCK();
d1302 1
a1302 1
		}
d1402 1
a1402 1
				PMAP_MAP_TO_HEAD_UNLOCK();
a1489 2
	PMAP_MAP_TO_HEAD_UNLOCK();

a1510 5
	/*
	 * we lock in the pmap => pv_head direction
	 */
	PMAP_MAP_TO_HEAD_LOCK();

a1593 2

	PMAP_MAP_TO_HEAD_UNLOCK();
a1778 2
	PMAP_MAP_TO_HEAD_LOCK();

a1841 2
	PMAP_MAP_TO_HEAD_UNLOCK();

a1938 2
	PMAP_MAP_TO_HEAD_LOCK();

a2085 2
	PMAP_MAP_TO_HEAD_UNLOCK();

a2135 2
	PMAP_MAP_TO_HEAD_LOCK();

a2148 2

	PMAP_MAP_TO_HEAD_UNLOCK();
@


1.18
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.17 2014/10/27 00:49:05 jsg Exp $	*/
a290 13
#define	pmap_acquire_pmap_lock(pm)			\
	do {						\
		if ((pm) != pmap_kernel())		\
			simple_lock(&(pm)->pm_lock);	\
	} while (/*CONSTCOND*/0)

#define	pmap_release_pmap_lock(pm)			\
	do {						\
		if ((pm) != pmap_kernel())		\
			simple_unlock(&(pm)->pm_lock);	\
	} while (/*CONSTCOND*/0)


a1025 2
		pmap_acquire_pmap_lock(pm);

a1063 2
		pmap_release_pmap_lock(pm);

a1187 1
		pmap_acquire_pmap_lock(pm);
a1236 1
		pmap_release_pmap_lock(pm);
a1256 1
	simple_lock_init(&pm->pm_lock);
a1311 1
	pmap_acquire_pmap_lock(pm);
a1322 1
			pmap_release_pmap_lock(pm);
a1424 1
				pmap_release_pmap_lock(pm);
a1512 1
	pmap_release_pmap_lock(pm);
a1539 1
	pmap_acquire_pmap_lock(pm);
a1624 1
	pmap_release_pmap_lock(pm);
a1731 1
	pmap_acquire_pmap_lock(pm);
a1741 1
		pmap_release_pmap_lock(pm);
a1752 1
			pmap_release_pmap_lock(pm);
a1757 1
		pmap_release_pmap_lock(pm);
a1811 1
	pmap_acquire_pmap_lock(pm);
a1875 1
	pmap_release_pmap_lock(pm);
a1975 1
	pmap_acquire_pmap_lock(pm);
a2123 1
	pmap_release_pmap_lock(pm);
a2176 1
	pmap_acquire_pmap_lock(pm);
a2191 1
	pmap_release_pmap_lock(pm);
a2223 1
		pmap_acquire_pmap_lock(pm);
a2245 1
		pmap_release_pmap_lock(pm);
a2270 1
	simple_lock(&pm->pm_lock);
a2271 1
	simple_unlock(&pm->pm_lock);
a2296 1
	simple_lock(&pm->pm_lock);
a2297 1
	simple_unlock(&pm->pm_lock);
a2521 1
	simple_lock(&kpm->pm_lock);
a2546 1
	simple_unlock(&kpm->pm_lock);
a2703 1
	simple_lock_init(&pm->pm_lock);
d2856 1
a2856 1
	    0, "l2ptppl", NULL);
@


1.17
log
@use #ifdef DDB for Debugger()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.16 2014/10/07 07:14:55 jsg Exp $	*/
d694 1
a694 1
		pmap_kenter_pa(va, pa, VM_PROT_READ | VM_PROT_WRITE);
d1055 1
a1055 1
			      npte & L2_V7_S_XN ? VM_PROT_READ : VM_PROT_READ | VM_PROT_EXECUTE);
d1157 1
a1157 1
		    L2_S_PROT(PTE_KERNEL, VM_PROT_WRITE) | pte_l2_s_cache_mode;
d1314 1
a1314 1
	KDASSERT((flags & PMAP_WIRED) == 0 || (flags & VM_PROT_ALL) != 0);
d1324 1
a1324 1
	if (prot & VM_PROT_WRITE)
d1326 1
a1326 1
	if (prot & VM_PROT_EXECUTE)
d1372 1
a1372 1
		if ((flags & VM_PROT_ALL) ||
d1385 2
a1386 2
			if ((prot & VM_PROT_WRITE) != 0 &&
			    ((flags & VM_PROT_WRITE) != 0 ||
d1402 1
a1402 1
			prot &= ~VM_PROT_WRITE;
d1422 1
a1422 1
			    (prot & VM_PROT_WRITE) == 0) {
d1480 1
a1480 1
	    prot & ~VM_PROT_WRITE);
d1533 1
a1533 1
	if (mapped && (prot & VM_PROT_EXECUTE) != 0 && pmap_is_current(pm))
d1825 1
a1825 1
	if ((prot & VM_PROT_READ) == 0) {
d1831 1
a1831 1
	if (prot & VM_PROT_WRITE) {
d1881 1
a1881 1
				      pte & L2_V7_S_XN ? VM_PROT_READ : VM_PROT_READ | VM_PROT_EXECUTE);
d1929 2
a1930 2
	case VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE:
	case VM_PROT_READ|VM_PROT_WRITE:
d1933 2
a1934 2
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
d2044 2
a2045 2
		if ((pte & L2_S_PROT_MASK & ~L2_V7_S_XN) != L2_S_PROT(PTE_USER, VM_PROT_READ) &&
		    (pte & L2_S_PROT_MASK & ~L2_V7_S_XN) != L2_S_PROT(PTE_USER, VM_PROT_WRITE))
d2051 1
a2051 1
	if ((ftype & VM_PROT_EXECUTE) && (pte & L2_V7_S_XN)) {
d2060 1
a2060 1
	if ((ftype & VM_PROT_WRITE) && !l2pte_is_writeable(pte, pm)) {
d2101 1
a2101 1
		      pte & L2_V7_S_XN ? VM_PROT_WRITE : VM_PROT_WRITE | VM_PROT_EXECUTE);
d2364 1
a2364 1
	    L2_S_PROT(PTE_KERNEL, VM_PROT_WRITE) | pte_l2_s_cache_mode;
d2394 1
a2394 1
	    L2_S_PROT(PTE_KERNEL, VM_PROT_READ) | pte_l2_s_cache_mode;
d2397 1
a2397 1
	    L2_S_PROT(PTE_KERNEL, VM_PROT_WRITE) | pte_l2_s_cache_mode;
d2448 1
a2448 1
	    L2_S_PROT(PTE_KERNEL, VM_PROT_READ | VM_PROT_WRITE);
@


1.16
log
@Correct the l1 pte permission bits for armv7.  Problem pointed out
by Patrick Wildt who made a similiar change in Bitrig.

ok miod@@ rapha@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.15 2014/07/12 18:44:41 tedu Exp $	*/
d2054 1
d2056 1
@


1.15
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.14 2014/05/08 21:17:00 miod Exp $	*/
d3421 6
d3472 6
d3520 6
@


1.14
log
@Format string fixes and removal of -Wno-format for arm kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.13 2014/03/29 18:09:28 guenther Exp $	*/
d761 1
a761 1
	free(l1, M_VMPMAP);
@


1.13
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.12 2013/11/04 00:35:30 dlg Exp $	*/
d2052 1
a2052 1
printf("%s: va %08x ftype %x %c pte %08x\n", __func__, va, ftype, user ? 'u' : 's', pte);
d2133 1
a2133 1
printf("%s: va %08x ftype %x %c pte %08x\n", __func__, va, ftype, user ? 'u' : 's', pte);
@


1.12
log
@move the arm pmaps away from pool ctors to just initting memory after its
been allocated. also gets rid of a potential bug where pool_get could
return NULL and pmap_alloc_l2_ptp tried to blindly init it.

tests, tweaks, and ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.11 2013/10/29 12:10:02 patrick Exp $	*/
d738 1
a738 1
	__asm __volatile("mrc p15, 0, %0, c2, c0, 0" : "=r"(cur_ttb));
d2244 2
a2245 2
		__asm __volatile("mrc p15, 0, %0, c2, c0, 0" : "=r"(cur_ttb));
		__asm __volatile("mrc p15, 0, %0, c3, c0, 0" : "=r"(cur_dacr));
d3520 2
a3521 2
	__asm __volatile("mcr p15, 2, %0, c0, c0, 0" :: "r" (0) );
	__asm __volatile("mrc p15, 1, %0, c0, c0, 0" : "=r" (cachereg) );
@


1.11
log
@The cache mask for ARMv7 is a little bit different from the one inited by
the generic function.  While there, also set the cache bits manually.

ok aalm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.10 2013/10/22 15:18:06 patrick Exp $	*/
d370 1
a370 1
	    pool_get(&pmap_l2dtable_pool, PR_NOWAIT)
a373 10
static __inline pt_entry_t *
pmap_alloc_l2_ptp(paddr_t *pap)		
{
	pt_entry_t *pted;

	pted = pool_get(&pmap_l2ptp_pool, PR_NOWAIT);
	(void)pmap_extract(pmap_kernel(), (vaddr_t)pted, pap);
	return pted;
}

a419 2
int		pmap_pmap_ctor(void *, void *, int);

d426 1
a426 2
int		pmap_l2ptp_ctor(void *, void *, int);
int		pmap_l2dtable_ctor(void *, void *, int);
d823 2
a824 1
		if ((ptep = pmap_alloc_l2_ptp(&l2b->l2b_phys)) == NULL) {
d836 2
d932 1
a932 1
 * Pool cache constructors for L2 descriptor tables, metadata and pmap
d935 2
a936 2
int
pmap_l2ptp_ctor(void *arg, void *v, int flags)
a964 1
	memset(v, 0, L2_TABLE_SIZE_REAL);
a965 17
	return (0);
}

int
pmap_l2dtable_ctor(void *arg, void *v, int flags)
{

	memset(v, 0, sizeof(struct l2_dtable));
	return (0);
}

int
pmap_pmap_ctor(void *arg, void *v, int flags)
{

	memset(v, 0, sizeof(struct pmap));
	return (0);
d1274 1
a1274 1
	pm = pool_get(&pmap_pmap_pool, PR_WAITOK);
a2881 1
	pool_set_ctordtor(&pmap_pmap_pool, pmap_pmap_ctor, NULL, NULL);
a2893 2
	 pool_set_ctordtor(&pmap_l2dtable_pool, pmap_l2dtable_ctor, NULL,
	     NULL);
a2899 1
	pool_set_ctordtor(&pmap_l2ptp_pool, pmap_l2ptp_ctor, NULL, NULL);
@


1.10
log
@Some current boards do not have write-through caching.  For those,
we enable write-back, but it seems this is not working and those
boards hang on bootup.  Until that is fixed, do not cache PTEs
on those boards.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.9 2013/09/03 16:48:26 patrick Exp $	*/
d3523 13
@


1.9
log
@Rewrite the ARMv7 cache discovery, as some assumptions in the previous one
were utterly wrong.  Fix Log2(), correct one taken from the scheduler code.

Tested by rapha@@ and Artturi Alm.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.8 2013/06/09 12:53:42 miod Exp $	*/
d3542 1
d3552 5
@


1.8
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.7 2013/05/22 12:27:44 patrick Exp $	*/
a280 8

/*
 * Flag to indicate whether the data cache is virtually indexed and
 * virtually tagged. A value of zero implies physically indexed and
 * physically tagged data cache; there is no support for VIPT data
 * cache yet.
 */
boolean_t pmap_cachevivt = FALSE;
@


1.7
log
@We're handling L2 there, so use the corresponding define, not the L1 one.

ok bmercer@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.6 2013/05/21 18:25:40 patrick Exp $	*/
d3063 1
a3063 1
 * that can only be done one the memory allocation is in place.
@


1.6
log
@When mapping a new entry, map it read-only, even though it should be
writable. This will cause a pmap fault on first write, so that we can
mark the page as modified. Also mask the bits used for the protection
settings, so that there aren't any leftovers.

ok bmercer@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.5 2013/05/09 20:07:25 patrick Exp $	*/
d1022 1
a1022 1
	*pte |= L1_S_B; /* device memory */
@


1.5
log
@On ARMv7 we can't use the cache mask to check for coherency.
Therefore we add new macros to be able to check for it properly.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.4 2013/04/28 14:39:15 patrick Exp $	*/
d1515 2
a1516 1
	npte |= L2_S_PROT(pm == pmap_kernel() ?  PTE_KERNEL : PTE_USER, prot);
d2133 1
a2133 1
		*ptep = (pte & ~L2_TYPE_MASK) | L2_S_PROTO |
@


1.4
log
@Revert the ARMv7 header split introduced in pmap7.

ok bmercer@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.3 2013/04/16 14:44:13 patrick Exp $	*/
d3456 4
d3501 4
d3530 4
@


1.3
log
@Add secondary cache flushes to armv7's pmap.

ok bmercer@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.2 2013/03/27 08:52:49 patrick Exp $	*/
d195 1
a195 3
#include <arm/cpufuncv7.h>
#include <arm/pmapv7.h>
#include <arm/ptev7.h>
@


1.2
log
@Further updates to pmap7. Fixes some problems and removes debug printfs.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.c,v 1.1 2013/01/17 20:52:39 bmercer Exp $	*/
d977 1
d1178 1
d1180 3
d1199 1
d1201 2
d1256 2
a1257 1
				if (flush == FALSE)
d1260 5
d1460 1
a1460 1
			    (prot & VM_PROT_WRITE) == 0)
d1462 3
d1714 1
a1714 1
	if (l2pte_valid(opte))
d1716 2
a1717 1
	else
d1765 1
a1765 1
			if (l2pte_valid(opte))
d1767 3
d2621 1
d2869 1
d2943 1
@


1.1
log
@New pmap for panda boards. Work from lots of folks.
OK miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2011/11/09 10:15:49 miod Exp $	*/
d975 2
d1023 1
d1241 2
a1242 1
		pte = *ptep;
d1244 9
a1252 8
		/* inline pmap_is_current(pm) */
		if (pm == curpm || pm == pmap_kernel()) {
			if (PV_BEEN_EXECD(pv->pv_flags))
				cpu_icache_sync_range(pv->pv_va, PAGE_SIZE);
			if (flush == FALSE)
				cpu_dcache_wb_range(pv->pv_va, PAGE_SIZE);
			flush = TRUE;
		}
d1254 4
a1257 4
		/*
		 * Update statistics
		 */
		--pm->pm_stats.resident_count;
d1259 3
a1261 3
		/* Wired bit */
		if (pv->pv_flags & PVF_WIRED)
			--pm->pm_stats.wired_count;
d1263 1
a1263 1
		flags |= pv->pv_flags;
d1265 7
a1271 7
		/*
		 * Invalidate the PTEs.
		 */
		*ptep = L2_TYPE_INV;
		PTE_SYNC(ptep);
		if (flush)
			cpu_tlb_flushID_SE(pv->pv_va);
d1273 2
a1274 1
		pmap_free_l2_bucket(pm, l2b, 1);
d1334 1
d1427 1
d1553 1
a1553 7
	/*
	 * Somehow this makes us _write_ to the va.
	 * If it's write-protected, this will break.
	 * Massively.
	 */
	/*
	if ((prot & VM_PROT_EXECUTE) != 0 && pmap_is_current(pm))
a1554 1
	*/
d1579 1
a1579 1
	
a1778 2
if (hostid != 0)
printf("pmap_extract: va %08x l1idx %08x\n", va, l1idx);
a1793 2
if (hostid != 0)
printf("pmap_extract: l2 %p l2idx %08x\n", l2, L2_IDX(l1idx));
a1796 2
if (hostid != 0)
printf("pmap_extract: l2 %p ptep %p bucket %08x\n", l2, ptep, L2_BUCKET(l1idx));
a1800 2
if (hostid != 0)
printf("pmap_extract: l2pte_index %08x ptep %p -> %p\n", l2pte_index(va), ptep, &ptep[l2pte_index(va)]);
a1802 2
if (hostid != 0)
printf("pmap_extract: pte %08x\n", pte);
a2022 3
#ifdef PMAP_DEBUG
printf("%s: va %08x ftype %x %c\n", __func__, va, ftype, user ? 'u' : 's');
#endif
a2332 31
	if (vector_page < KERNEL_BASE) {
		struct pcb *pcb = &proc0.p_addr->u_pcb;

		if (pmap_is_current(pm)) {
			/*
			 * Frob the L1 entry corresponding to the vector
			 * page so that it contains the kernel pmap's domain
			 * number. This will ensure pmap_remove() does not
			 * pull the current vector page out from under us.
			 */
			disable_interrupts(I32_bit | F32_bit);
			*pcb->pcb_pl1vec = pcb->pcb_l1vec;
			cpu_domains(pcb->pcb_dacr);
			cpu_setttb(pcb->pcb_pagedir);
			enable_interrupts(I32_bit | F32_bit);
		}

		/* Remove the vector page mapping */
		pmap_remove(pm, vector_page, vector_page + PAGE_SIZE);
		pmap_update(pm);

		/*
		 * Make sure cpu_switchto(), et al, DTRT. This is safe to do
		 * since this process has no remaining mappings of its own.
		 */
		curpcb->pcb_pl1vec = pcb->pcb_pl1vec;
		curpcb->pcb_l1vec = pcb->pcb_l1vec;
		curpcb->pcb_dacr = pcb->pcb_dacr;
		curpcb->pcb_pagedir = pcb->pcb_pagedir;
	}

a2716 3
#ifdef PMAP_DEBUG
printf("l1 va %08x pa %08x\n", (vaddr_t)l1pt, l1->l1_physaddr);
#endif
d2827 1
a2827 1
		 * function. 
@

