head	1.36;
access;
symbols
	OPENBSD_6_2:1.36.0.2
	OPENBSD_6_2_BASE:1.36
	OPENBSD_6_1:1.35.0.4
	OPENBSD_6_1_BASE:1.35
	OPENBSD_6_0:1.30.0.2
	OPENBSD_6_0_BASE:1.30
	OPENBSD_5_9:1.28.0.4
	OPENBSD_5_9_BASE:1.28
	OPENBSD_5_8:1.28.0.6
	OPENBSD_5_8_BASE:1.28
	OPENBSD_5_7:1.28.0.2
	OPENBSD_5_7_BASE:1.28
	OPENBSD_5_6:1.26.0.4
	OPENBSD_5_6_BASE:1.26
	OPENBSD_5_5:1.24.0.6
	OPENBSD_5_5_BASE:1.24
	OPENBSD_5_4:1.24.0.2
	OPENBSD_5_4_BASE:1.24
	OPENBSD_5_3:1.21.0.8
	OPENBSD_5_3_BASE:1.21
	OPENBSD_5_2:1.21.0.6
	OPENBSD_5_2_BASE:1.21
	OPENBSD_5_1_BASE:1.21
	OPENBSD_5_1:1.21.0.4
	OPENBSD_5_0:1.21.0.2
	OPENBSD_5_0_BASE:1.21
	OPENBSD_4_9:1.20.0.2
	OPENBSD_4_9_BASE:1.20
	OPENBSD_4_8:1.17.0.2
	OPENBSD_4_8_BASE:1.17
	OPENBSD_4_7:1.15.0.2
	OPENBSD_4_7_BASE:1.15
	OPENBSD_4_6:1.15.0.4
	OPENBSD_4_6_BASE:1.15
	OPENBSD_4_5:1.12.0.4
	OPENBSD_4_5_BASE:1.12
	OPENBSD_4_4:1.12.0.2
	OPENBSD_4_4_BASE:1.12
	OPENBSD_4_3:1.11.0.2
	OPENBSD_4_3_BASE:1.11
	OPENBSD_4_2:1.10.0.2
	OPENBSD_4_2_BASE:1.10
	OPENBSD_4_1:1.8.0.4
	OPENBSD_4_1_BASE:1.8
	OPENBSD_4_0:1.8.0.2
	OPENBSD_4_0_BASE:1.8
	OPENBSD_3_9:1.7.0.2
	OPENBSD_3_9_BASE:1.7
	OPENBSD_3_8:1.6.0.2
	OPENBSD_3_8_BASE:1.6
	OPENBSD_3_7:1.5.0.2
	OPENBSD_3_7_BASE:1.5
	OPENBSD_3_6:1.2.0.6
	OPENBSD_3_6_BASE:1.2
	SMP_SYNC_A:1.2
	SMP_SYNC_B:1.2
	OPENBSD_3_5:1.2.0.4
	OPENBSD_3_5_BASE:1.2
	SMP:1.2.0.2;
locks; strict;
comment	@ * @;


1.36
date	2017.05.05.12.54.47;	author kettenis;	state Exp;
branches;
next	1.35;
commitid	ql2lrQ0p54oE3orq;

1.35
date	2016.09.20.16.31.56;	author patrick;	state Exp;
branches;
next	1.34;
commitid	MTKTB6I02I5taymq;

1.34
date	2016.08.26.21.50.42;	author patrick;	state Exp;
branches;
next	1.33;
commitid	QU80OHr8Zs8OY7wY;

1.33
date	2016.08.26.21.14.58;	author patrick;	state Exp;
branches;
next	1.32;
commitid	qeasfHxq5qOVBlZt;

1.32
date	2016.08.22.01.41.59;	author jsg;	state Exp;
branches;
next	1.31;
commitid	UOfhQ4F8JDkBUCg3;

1.31
date	2016.08.14.10.32.17;	author kettenis;	state Exp;
branches;
next	1.30;
commitid	HlD3YITvnkUBY2e9;

1.30
date	2016.05.16.15.13.50;	author kettenis;	state Exp;
branches;
next	1.29;
commitid	zpNswTr0D1AyEC4j;

1.29
date	2016.03.10.10.22.43;	author tobiasu;	state Exp;
branches;
next	1.28;
commitid	eW1lof8rcgY3vzUt;

1.28
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.27;
commitid	yv0ECmCdICvq576h;

1.27
date	2014.09.13.16.06.36;	author doug;	state Exp;
branches;
next	1.26;
commitid	jdBY2kKXhfcoQitp;

1.26
date	2014.07.12.18.44.41;	author tedu;	state Exp;
branches;
next	1.25;
commitid	uKVPYMN2MLxdZxzH;

1.25
date	2014.07.11.09.36.25;	author mpi;	state Exp;
branches;
next	1.24;
commitid	vsYjSRfS3Y783BvW;

1.24
date	2013.05.10.20.25.28;	author patrick;	state Exp;
branches;
next	1.23;

1.23
date	2013.05.09.20.07.25;	author patrick;	state Exp;
branches;
next	1.22;

1.22
date	2013.05.08.21.10.33;	author patrick;	state Exp;
branches;
next	1.21;

1.21
date	2011.06.23.20.44.39;	author ariane;	state Exp;
branches;
next	1.20;

1.20
date	2011.01.04.21.12.55;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2010.11.20.20.33.23;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2010.03.31.19.46.26;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2010.03.29.19.21.58;	author oga;	state Exp;
branches;
next	1.15;

1.15
date	2009.04.20.00.42.05;	author oga;	state Exp;
branches;
next	1.14;

1.14
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.13;

1.13
date	2009.03.07.15.34.34;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2008.06.26.05.42.09;	author ray;	state Exp;
branches;
next	1.11;

1.11
date	2007.10.06.23.12.17;	author krw;	state Exp;
branches;
next	1.10;

1.10
date	2007.05.29.21.00.50;	author jason;	state Exp;
branches;
next	1.9;

1.9
date	2007.04.10.18.22.07;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2006.07.16.00.18.33;	author drahn;	state Exp;
branches;
next	1.7;

1.7
date	2005.11.09.18.08.37;	author martin;	state Exp;
branches;
next	1.6;

1.6
date	2005.05.27.20.15.50;	author uwe;	state Exp;
branches;
next	1.5;

1.5
date	2004.12.30.23.24.57;	author drahn;	state Exp;
branches;
next	1.4;

1.4
date	2004.12.25.23.02.23;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2004.11.09.19.17.01;	author claudio;	state Exp;
branches;
next	1.2;

1.2
date	2004.02.01.06.10.33;	author drahn;	state Exp;
branches;
next	1.1;

1.1
date	2004.02.01.05.09.48;	author drahn;	state Exp;
branches;
next	;


desc
@@


1.36
log
@Remove /* FALLTHROUGH */ that isn't (and shouldn't).
@
text
@/*	$OpenBSD: bus_dma.c,v 1.35 2016/09/20 16:31:56 patrick Exp $	*/
/*	$NetBSD: bus_dma.c,v 1.38 2003/10/30 08:44:13 scw Exp $	*/

/*-
 * Copyright (c) 1996, 1997, 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#define _ARM32_BUS_DMA_PRIVATE

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/buf.h>
#include <sys/reboot.h>
#include <sys/conf.h>
#include <sys/file.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/vnode.h>
#include <sys/device.h>

#include <uvm/uvm_extern.h>

#include <machine/bus.h>
#include <machine/cpu.h>

#include <arm/cpufunc.h>

int	_bus_dmamap_load_buffer(bus_dma_tag_t, bus_dmamap_t, void *,
	    bus_size_t, struct proc *, int, paddr_t *, int *, int);
struct arm32_dma_range *_bus_dma_inrange(struct arm32_dma_range *,
	    int, bus_addr_t);


/*
 * Check to see if the specified busaddr is in an allowed DMA range.
 */
static inline paddr_t
_bus_dma_busaddr_to_paddr(bus_dma_tag_t t, bus_addr_t curaddr)
{
	struct arm32_dma_range *dr;
	u_int i;

	if (t->_nranges == 0)
		return curaddr;

	for (i = 0, dr = t->_ranges; i < t->_nranges; i++, dr++) {
		if (dr->dr_busbase <= curaddr
		    && round_page(curaddr) <= dr->dr_busbase + dr->dr_len)
			return curaddr - dr->dr_busbase + dr->dr_sysbase;
	}
	panic("%s: curaddr %#lx not in range", __func__, curaddr);
}

/*
 * Check to see if the specified page is in an allowed DMA range.
 */
__inline struct arm32_dma_range *
_bus_dma_inrange(struct arm32_dma_range *ranges, int nranges,
    bus_addr_t curaddr)
{
	struct arm32_dma_range *dr;
	int i;

	for (i = 0, dr = ranges; i < nranges; i++, dr++) {
		if (curaddr >= dr->dr_sysbase &&
		    round_page(curaddr) <= (dr->dr_sysbase + dr->dr_len))
			return (dr);
	}

	return (NULL);
}

/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
int
_bus_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
    bus_size_t maxsegsz, bus_size_t boundary, int flags, bus_dmamap_t *dmamp)
{
	struct arm32_bus_dmamap *map;
	void *mapstore;
	size_t mapsize;

#ifdef DEBUG_DMA
	printf("dmamap_create: t=%p size=%lx nseg=%x msegsz=%lx boundary=%lx flags=%x\n",
	    t, size, nsegments, maxsegsz, boundary, flags);
#endif	/* DEBUG_DMA */

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
	mapsize = sizeof(struct arm32_bus_dmamap) +
	    (sizeof(bus_dma_segment_t) * (nsegments - 1));
	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
		return (ENOMEM);

	map = (struct arm32_bus_dmamap *)mapstore;
	map->_dm_size = size;
	map->_dm_segcnt = nsegments;
	map->_dm_maxsegsz = maxsegsz;
	map->_dm_boundary = boundary;
	map->_dm_flags = flags & ~(BUS_DMA_WAITOK|BUS_DMA_NOWAIT);
	map->_dm_origbuf = NULL;
	map->_dm_buftype = ARM32_BUFTYPE_INVALID;
	map->_dm_proc = NULL;
	map->dm_mapsize = 0;		/* no valid mappings */
	map->dm_nsegs = 0;

	*dmamp = map;
#ifdef DEBUG_DMA
	printf("dmamap_create:map=%p\n", map);
#endif	/* DEBUG_DMA */
	return (0);
}

/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
void
_bus_dmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{

#ifdef DEBUG_DMA
	printf("dmamap_destroy: t=%p map=%p\n", t, map);
#endif	/* DEBUG_DMA */

	free(map, M_DEVBUF, 0);
}

/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
int
_bus_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags)
{
	paddr_t lastaddr;
	int seg, error;

#ifdef DEBUG_DMA
	printf("dmamap_load: t=%p map=%p buf=%p len=%lx p=%p f=%d\n",
	    t, map, buf, buflen, p, flags);
#endif	/* DEBUG_DMA */

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	/* _bus_dmamap_load_buffer() clears this if we're not... */
	map->_dm_flags |= ARM32_DMAMAP_COHERENT;

	seg = 0;
	error = _bus_dmamap_load_buffer(t, map, buf, buflen, p, flags,
	    &lastaddr, &seg, 1);
	if (error == 0) {
		map->dm_mapsize = buflen;
		map->dm_nsegs = seg + 1;
		map->_dm_origbuf = buf;
		map->_dm_buftype = ARM32_BUFTYPE_LINEAR;
		map->_dm_proc = p;
	}
#ifdef DEBUG_DMA
	printf("dmamap_load: error=%d\n", error);
#endif	/* DEBUG_DMA */
	return (error);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
_bus_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map, struct mbuf *m0,
    int flags)
{
#if 0
	struct arm32_dma_range *dr;
#endif
	paddr_t lastaddr;
	int seg, error, first;
	struct mbuf *m;

#ifdef DEBUG_DMA
	printf("dmamap_load_mbuf: t=%p map=%p m0=%p f=%d\n",
	    t, map, m0, flags);
#endif	/* DEBUG_DMA */

	/*
	 * Make sure that on error condition we return "no valid mappings."
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic("_bus_dmamap_load_mbuf: no packet header");
#endif	/* DIAGNOSTIC */

	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	/*
	 * Mbuf chains should almost never have coherent (i.e.
	 * un-cached) mappings, so clear that flag now.
	 */
	map->_dm_flags &= ~ARM32_DMAMAP_COHERENT;

	first = 1;
	seg = 0;
	error = 0;
	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
 		error = _bus_dmamap_load_buffer(t, map, m->m_data, m->m_len,
 		    NULL, flags, &lastaddr, &seg, first);
		first = 0;
	}
	if (error == 0) {
		map->dm_mapsize = m0->m_pkthdr.len;
		map->dm_nsegs = seg + 1;
		map->_dm_origbuf = m0;
		map->_dm_buftype = ARM32_BUFTYPE_MBUF;
		map->_dm_proc = NULL;	/* always kernel */
	}
#ifdef DEBUG_DMA
	printf("dmamap_load_mbuf: error=%d\n", error);
#endif	/* DEBUG_DMA */
	return (error);
}

/*
 * Like _bus_dmamap_load(), but for uios.
 */
int
_bus_dmamap_load_uio(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio,
    int flags)
{
	paddr_t lastaddr;
	int seg, i, error, first;
	bus_size_t minlen, resid;
	struct proc *p = NULL;
	struct iovec *iov;
	caddr_t addr;

	/*
	 * Make sure that on error condition we return "no valid mappings."
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	resid = uio->uio_resid;
	iov = uio->uio_iov;

	if (uio->uio_segflg == UIO_USERSPACE) {
		p = uio->uio_procp;
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic("_bus_dmamap_load_uio: USERSPACE but no proc");
#endif
	}

	/* _bus_dmamap_load_buffer() clears this if we're not... */
	map->_dm_flags |= ARM32_DMAMAP_COHERENT;

	first = 1;
	seg = 0;
	error = 0;
	for (i = 0; i < uio->uio_iovcnt && resid != 0 && error == 0; i++) {
		/*
		 * Now at the first iovec to load.  Load each iovec
		 * until we have exhausted the residual count.
		 */
		minlen = resid < iov[i].iov_len ? resid : iov[i].iov_len;
		addr = (caddr_t)iov[i].iov_base;

		error = _bus_dmamap_load_buffer(t, map, addr, minlen,
		    p, flags, &lastaddr, &seg, first);
		first = 0;

		resid -= minlen;
	}
	if (error == 0) {
		map->dm_mapsize = uio->uio_resid;
		map->dm_nsegs = seg + 1;
		map->_dm_origbuf = uio;
		map->_dm_buftype = ARM32_BUFTYPE_UIO;
		map->_dm_proc = p;
	}
	return (error);
}

/*
 * Like _bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
_bus_dmamap_load_raw(bus_dma_tag_t t, bus_dmamap_t map,
    bus_dma_segment_t *segs, int nsegs, bus_size_t size, int flags)
{
	struct arm32_dma_range *dr;
	bus_addr_t paddr, baddr, bmask, lastaddr = 0;
	bus_size_t plen, sgsize, mapsize;
	vaddr_t vaddr;
	int first = 1;
	int i, seg = 0;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (nsegs > map->_dm_segcnt || size > map->_dm_size)
		return (EINVAL);

	mapsize = size;
	bmask = ~(map->_dm_boundary - 1);

	/*
	 * Assume to be coherent at first.  If one of the segments
	 * isn't, we clear the flag for the whole map.
	 */
	map->_dm_flags |= ARM32_DMAMAP_COHERENT;

	for (i = 0; i < nsegs && size > 0; i++) {
		paddr = segs[i].ds_addr;
		vaddr = segs[i]._ds_vaddr;
		plen = MIN(segs[i].ds_len, size);

		if (!segs[i]._ds_coherent)
			map->_dm_flags &= ~ARM32_DMAMAP_COHERENT;

		while (plen > 0) {
			/*
			 * Make sure we're in an allowed DMA range.
			 */
			if (t->_ranges != NULL) {
				/* XXX cache last result? */
				dr = _bus_dma_inrange(t->_ranges, t->_nranges,
				    paddr);
				if (dr == NULL)
					return (EINVAL);

				/*
				 * In a valid DMA range.  Translate the physical
				 * memory address to an address in the DMA window.
				 */
				paddr = (paddr - dr->dr_sysbase) + dr->dr_busbase;
			}

			/*
			 * Compute the segment size, and adjust counts.
			 */
			sgsize = PAGE_SIZE - ((u_long)paddr & PGOFSET);
			if (plen < sgsize)
				sgsize = plen;

			/*
			 * Make sure we don't cross any boundaries.
			 */
			if (map->_dm_boundary > 0) {
				baddr = (paddr + map->_dm_boundary) & bmask;
				if (sgsize > (baddr - paddr))
					sgsize = (baddr - paddr);
			}

			/*
			 * Insert chunk into a segment, coalescing with
			 * previous segment if possible.
			 */
			if (first) {
				map->dm_segs[seg].ds_addr = paddr;
				map->dm_segs[seg].ds_len = sgsize;
				map->dm_segs[seg]._ds_vaddr = vaddr;
				first = 0;
			} else {
				if (paddr == lastaddr &&
				    (map->dm_segs[seg].ds_len + sgsize) <=
				     map->_dm_maxsegsz &&
				    (map->_dm_boundary == 0 ||
				     (map->dm_segs[seg].ds_addr & bmask) ==
				     (paddr & bmask)))
					map->dm_segs[seg].ds_len += sgsize;
				else {
					if (++seg >= map->_dm_segcnt)
						return (EINVAL);
					map->dm_segs[seg].ds_addr = paddr;
					map->dm_segs[seg].ds_len = sgsize;
					map->dm_segs[seg]._ds_vaddr = vaddr;
				}
			}

			paddr += sgsize;
			vaddr += sgsize;
			plen -= sgsize;
			size -= sgsize;

			lastaddr = paddr;
		}
	}

	map->dm_mapsize = mapsize;
	map->dm_nsegs = seg + 1;
	map->_dm_buftype = ARM32_BUFTYPE_RAW;
	map->_dm_origbuf = NULL;
	map->_dm_proc = NULL;
	return (0);
}

/*
 * Common function for unloading a DMA map.  May be called by
 * bus-specific DMA map unload functions.
 */
void
_bus_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{

#ifdef DEBUG_DMA
	printf("dmamap_unload: t=%p map=%p\n", t, map);
#endif	/* DEBUG_DMA */

	/*
	 * No resources to free; just mark the mappings as
	 * invalid.
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;
	map->_dm_origbuf = NULL;
	map->_dm_buftype = ARM32_BUFTYPE_INVALID;
	map->_dm_proc = NULL;
}

static void
_bus_dmamap_sync_segment(vaddr_t va, paddr_t pa, vsize_t len, int ops)
{
	KASSERT((va & PAGE_MASK) == (pa & PAGE_MASK));

#ifdef DEBUG_DMA
	printf("sync_segment: va=%#lx pa=%#lx len=%#lx ops=%#x\n",
	    va, pa, len, ops);
#endif

	switch (ops) {
	case BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE:
		cpu_dcache_wbinv_range(va, len);
		cpu_sdcache_wbinv_range(va, pa, len);
		break;

	case BUS_DMASYNC_PREREAD: {
		const size_t line_size = arm_dcache_align;
		const size_t line_mask = arm_dcache_align_mask;
		vsize_t misalignment = va & line_mask;
		if (misalignment) {
			va -= misalignment;
			pa -= misalignment;
			len += misalignment;
			cpu_dcache_wbinv_range(va, line_size);
			cpu_sdcache_wbinv_range(va, pa, line_size);
			if (len <= line_size)
				break;
			va += line_size;
			pa += line_size;
			len -= line_size;
		}
		misalignment = len & line_mask;
		len -= misalignment;
		if (len > 0) {
			cpu_dcache_inv_range(va, len);
			cpu_sdcache_inv_range(va, pa, len);
		}
		if (misalignment) {
			va += len;
			pa += len;
			cpu_dcache_wbinv_range(va, line_size);
			cpu_sdcache_wbinv_range(va, pa, line_size);
		}
		break;
	}

	case BUS_DMASYNC_PREWRITE:
		cpu_dcache_wb_range(va, len);
		cpu_sdcache_wb_range(va, pa, len);
		break;

	/*
	 * Cortex CPUs can do speculative loads so we need to clean the cache
	 * after a DMA read to deal with any speculatively loaded cache lines.
	 * Since these can't be dirty, we can just invalidate them and don't
	 * have to worry about having to write back their contents.
	 */
	case BUS_DMASYNC_POSTREAD:
	case BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE:
		membar_sync();
		cpu_dcache_inv_range(va, len);
		cpu_sdcache_inv_range(va, pa, len);
		break;
	}
}

static __inline void
_bus_dmamap_sync_linear(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
{
	bus_dma_segment_t *ds = map->dm_segs;
	vaddr_t va = (vaddr_t) map->_dm_origbuf;

	while (len > 0) {
		while (offset >= ds->ds_len) {
			offset -= ds->ds_len;
			va += ds->ds_len;
			ds++;
		}

		paddr_t pa = _bus_dma_busaddr_to_paddr(t, ds->ds_addr + offset);
		size_t seglen = min(len, ds->ds_len - offset);

		_bus_dmamap_sync_segment(va + offset, pa, seglen, ops);

		offset += seglen;
		len -= seglen;
	}
}

static __inline void
_bus_dmamap_sync_mbuf(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
{
	bus_dma_segment_t *ds = map->dm_segs;
	struct mbuf *m = map->_dm_origbuf;
	bus_size_t voff = offset;
	bus_size_t ds_off = offset;

	while (len > 0) {
		/* Find the current dma segment */
		while (ds_off >= ds->ds_len) {
			ds_off -= ds->ds_len;
			ds++;
		}
		/* Find the current mbuf. */
		while (voff >= m->m_len) {
			voff -= m->m_len;
			m = m->m_next;
		}

		/*
		 * Now at the first mbuf to sync; nail each one until
		 * we have exhausted the length.
		 */
		vsize_t seglen = min(len, min(m->m_len - voff, ds->ds_len - ds_off));
		vaddr_t va = mtod(m, vaddr_t) + voff;
		paddr_t pa = _bus_dma_busaddr_to_paddr(t, ds->ds_addr + ds_off);

		/*
		 * We can save a lot of work here if we know the mapping
		 * is read-only at the MMU:
		 *
		 * If a mapping is read-only, no dirty cache blocks will
		 * exist for it.  If a writable mapping was made read-only,
		 * we know any dirty cache lines for the range will have
		 * been cleaned for us already.  Therefore, if the upper
		 * layer can tell us we have a read-only mapping, we can
		 * skip all cache cleaning.
		 *
		 * NOTE: This only works if we know the pmap cleans pages
		 * before making a read-write -> read-only transition.  If
		 * this ever becomes non-true (e.g. Physically Indexed
		 * cache), this will have to be revisited.
		 */
		_bus_dmamap_sync_segment(va, pa, seglen, ops);

		voff += seglen;
		ds_off += seglen;
		len -= seglen;
	}
}

static __inline void
_bus_dmamap_sync_uio(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
{
	bus_dma_segment_t *ds = map->dm_segs;
	struct uio *uio = map->_dm_origbuf;
	struct iovec *iov = uio->uio_iov;
	bus_size_t voff = offset;
	bus_size_t ds_off = offset;

	while (len > 0) {
		/* Find the current dma segment */
		while (ds_off >= ds->ds_len) {
			ds_off -= ds->ds_len;
			ds++;
		}

		/* Find the current iovec. */
		while (voff >= iov->iov_len) {
			voff -= iov->iov_len;
			iov++;
		}

		/*
		 * Now at the first iovec to sync; nail each one until
		 * we have exhausted the length.
		 */
		vsize_t seglen = min(len, min(iov->iov_len - voff, ds->ds_len - ds_off));
		vaddr_t va = (vaddr_t) iov->iov_base + voff;
		paddr_t pa = _bus_dma_busaddr_to_paddr(t, ds->ds_addr + ds_off);

		_bus_dmamap_sync_segment(va, pa, seglen, ops);

		voff += seglen;
		ds_off += seglen;
		len -= seglen;
	}
}

static __inline void
_bus_dmamap_sync_raw(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
{
	bus_dma_segment_t *ds = map->dm_segs;

	while (len > 0) {
		while (offset >= ds->ds_len) {
			offset -= ds->ds_len;
			ds++;
		}

		vaddr_t va = ds->_ds_vaddr + offset;
		paddr_t pa = _bus_dma_busaddr_to_paddr(t, ds->ds_addr + offset);
		size_t seglen = min(len, ds->ds_len - offset);

		_bus_dmamap_sync_segment(va, pa, seglen, ops);

		offset += seglen;
		len -= seglen;
	}
}

/*
 * Common function for DMA map synchronization.  May be called
 * by bus-specific DMA map synchronization functions.
 *
 * This version works for the Virtually Indexed Virtually Tagged
 * cache found on 32-bit ARM processors.
 *
 * XXX Should have separate versions for write-through vs.
 * XXX write-back caches.  We currently assume write-back
 * XXX here, which is not as efficient as it could be for
 * XXX the write-through case.
 */
void
_bus_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
{

#ifdef DEBUG_DMA
	printf("dmamap_sync: t=%p map=%p offset=%lx len=%lx ops=%x\n",
	    t, map, offset, len, ops);
#endif	/* DEBUG_DMA */

	/*
	 * Mixing of PRE and POST operations is not allowed.
	 */
	if ((ops & (BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE)) != 0 &&
	    (ops & (BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE)) != 0)
		panic("_bus_dmamap_sync: mix PRE and POST");

#ifdef DIAGNOSTIC
	if (offset >= map->dm_mapsize)
		panic("_bus_dmamap_sync: bad offset %lu (map size is %lu)",
		    offset, map->dm_mapsize);
	if ((offset + len) > map->dm_mapsize)
		panic("_bus_dmamap_sync: bad length");
#endif

	/*
	 * For a virtually-indexed write-back cache, we need
	 * to do the following things:
	 *
	 *	PREREAD -- Invalidate the D-cache.  We do this
	 *	here in case a write-back is required by the back-end.
	 *
	 *	PREWRITE -- Write-back the D-cache.  Note that if
	 *	we are doing a PREREAD|PREWRITE, we can collapse
	 *	the whole thing into a single Wb-Inv.
	 *
	 *	POSTREAD -- Invalidate the D-Cache. Contents of
	 *	the cache could be from before a device wrote
	 *	to the memory.
	 *
	 *	POSTWRITE -- Nothing.
	 */

	/* Skip cache frobbing if mapping was COHERENT. */
	if (map->_dm_flags & ARM32_DMAMAP_COHERENT) {
		/* Drain the write buffer. */
		cpu_drain_writebuf();
		cpu_sdcache_drain_writebuf();
		return;
	}

	/*
	 * If the mapping belongs to a non-kernel vmspace, and the
	 * vmspace has not been active since the last time a full
	 * cache flush was performed, we don't need to do anything.
	 */
	if (__predict_false(map->_dm_proc != NULL &&
	    map->_dm_proc->p_vmspace->vm_map.pmap->pm_cstate.cs_cache_d == 0))
		return;

	switch (map->_dm_buftype) {
	case ARM32_BUFTYPE_LINEAR:
		_bus_dmamap_sync_linear(t, map, offset, len, ops);
		break;

	case ARM32_BUFTYPE_MBUF:
		_bus_dmamap_sync_mbuf(t, map, offset, len, ops);
		break;

	case ARM32_BUFTYPE_UIO:
		_bus_dmamap_sync_uio(t, map, offset, len, ops);
		break;

	case ARM32_BUFTYPE_RAW:
		_bus_dmamap_sync_raw(t, map, offset, len, ops);
		break;

	case ARM32_BUFTYPE_INVALID:
		panic("_bus_dmamap_sync: ARM32_BUFTYPE_INVALID");
		break;

	default:
		printf("unknown buffer type %d\n", map->_dm_buftype);
		panic("_bus_dmamap_sync");
	}

	/* Drain the write buffer. */
	cpu_drain_writebuf();
}

/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */

int
_bus_dmamem_alloc(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags)
{
	struct arm32_dma_range *dr;
	int error, i;

#ifdef DEBUG_DMA
	printf("dmamem_alloc t=%p size=%lx align=%lx boundary=%lx "
	    "segs=%p nsegs=%x rsegs=%p flags=%x\n", t, size, alignment,
	    boundary, segs, nsegs, rsegs, flags);
#endif

	if ((dr = t->_ranges) != NULL) {
		error = ENOMEM;
		for (i = 0; i < t->_nranges; i++, dr++) {
			if (dr->dr_len == 0)
				continue;
			error = _bus_dmamem_alloc_range(t, size, alignment,
			    boundary, segs, nsegs, rsegs, flags,
			    trunc_page(dr->dr_sysbase),
			    trunc_page(dr->dr_sysbase + dr->dr_len) - 1);
			if (error == 0)
				break;
		}
	} else {
		error = _bus_dmamem_alloc_range(t, size, alignment, boundary,
		    segs, nsegs, rsegs, flags, 0, -1);
	}

#ifdef DEBUG_DMA
	printf("dmamem_alloc: =%d\n", error);
#endif

	return(error);
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
_bus_dmamem_free(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs)
{
	struct vm_page *m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

#ifdef DEBUG_DMA
	printf("dmamem_free: t=%p segs=%p nsegs=%x\n", t, segs, nsegs);
#endif	/* DEBUG_DMA */

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE(addr);
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}
	uvm_pglistfree(&mlist);
}

/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
int
_bus_dmamem_map(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
    size_t size, caddr_t *kvap, int flags)
{
	vaddr_t va, sva;
	size_t ssize;
	bus_addr_t addr;
	int curseg;
	const struct kmem_dyn_mode *kd;
#ifdef DEBUG_DMA
	pt_entry_t *ptep;
#endif

#ifdef DEBUG_DMA
	printf("dmamem_map: t=%p segs=%p nsegs=%x size=%lx flags=%x\n", t,
	    segs, nsegs, (unsigned long)size, flags);
#endif	/* DEBUG_DMA */

	size = round_page(size);
	kd = flags & BUS_DMA_NOWAIT ? &kd_trylock : &kd_waitok;
	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, kd);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	sva = va;
	ssize = size;
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE, va += PAGE_SIZE, size -= PAGE_SIZE) {
#ifdef DEBUG_DMA
			printf("wiring p%lx to v%lx", addr, va);
#endif	/* DEBUG_DMA */
			if (size == 0)
				panic("_bus_dmamem_map: size botch");
			segs[curseg]._ds_coherent =
			    !!(flags & BUS_DMA_COHERENT);
			segs[curseg]._ds_vaddr = va;
			pmap_kenter_cache(va, addr,
			    PROT_READ | PROT_WRITE,
			    !(flags & BUS_DMA_COHERENT));

#ifdef DEBUG_DMA
			ptep = vtopte(va);
			printf(" pte=v%p *pte=%x\n", ptep, *ptep);
#endif	/* DEBUG_DMA */
		}
	}
	pmap_update(pmap_kernel());
#ifdef DEBUG_DMA
	printf("dmamem_map: =%p\n", *kvap);
#endif	/* DEBUG_DMA */
	return (0);
}

/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
void
_bus_dmamem_unmap(bus_dma_tag_t t, caddr_t kva, size_t size)
{

#ifdef DEBUG_DMA
	printf("dmamem_unmap: t=%p kva=%p size=%lx\n", t, kva,
	    (unsigned long)size);
#endif	/* DEBUG_DMA */
#ifdef DIAGNOSTIC
	if ((u_long)kva & PGOFSET)
		panic("_bus_dmamem_unmap");
#endif	/* DIAGNOSTIC */

	km_free(kva, round_page(size), &kv_any, &kp_none);
}

/*
 * Common function for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
paddr_t
_bus_dmamem_mmap(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
    off_t off, int prot, int flags)
{
	int i;

	for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
		if (off & PGOFSET)
			panic("_bus_dmamem_mmap: offset unaligned");
		if (segs[i].ds_addr & PGOFSET)
			panic("_bus_dmamem_mmap: segment unaligned");
		if (segs[i].ds_len & PGOFSET)
			panic("_bus_dmamem_mmap: segment size not multiple"
			    " of page size");
#endif	/* DIAGNOSTIC */
		if (off >= segs[i].ds_len) {
			off -= segs[i].ds_len;
			continue;
		}

		return (segs[i].ds_addr + off);
	}

	/* Page not found. */
	return (-1);
}

/**********************************************************************
 * DMA utility functions
 **********************************************************************/

/*
 * Utility function to load a linear buffer.  lastaddrp holds state
 * between invocations (for multiple-buffer loads).  segp contains
 * the starting segment on entrance, and the ending segment on exit.
 * first indicates if this is the first invocation of this function.
 */
int
_bus_dmamap_load_buffer(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, paddr_t *lastaddrp,
    int *segp, int first)
{
	struct arm32_dma_range *dr;
	bus_size_t sgsize;
	bus_addr_t curaddr, lastaddr, baddr, bmask;
	vaddr_t vaddr = (vaddr_t)buf;
	pd_entry_t *pde;
	pt_entry_t pte;
	int seg;
	pmap_t pmap;
	pt_entry_t *ptep;

#ifdef DEBUG_DMA
	printf("_bus_dmamem_load_buffer(buf=%p, len=%lx, flags=%d, 1st=%d)\n",
	    buf, buflen, flags, first);
#endif	/* DEBUG_DMA */

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = *lastaddrp;
	bmask  = ~(map->_dm_boundary - 1);

	for (seg = *segp; buflen > 0; ) {
		/*
		 * Get the physical address for this segment.
		 *
		 * XXX Don't support checking for coherent mappings
		 * XXX in user address space.
		 */
		if (__predict_true(pmap == pmap_kernel())) {
			(void) pmap_get_pde_pte(pmap, vaddr, &pde, &ptep);
			if (__predict_false(pmap_pde_section(pde))) {
				curaddr = (*pde & L1_S_FRAME) |
				    (vaddr & L1_S_OFFSET);
				if (*pde & L1_S_COHERENT) {
					map->_dm_flags &=
					    ~ARM32_DMAMAP_COHERENT;
				}
			} else {
				pte = *ptep;
				KDASSERT((pte & L2_TYPE_MASK) != L2_TYPE_INV);
				if (__predict_false((pte & L2_TYPE_MASK)
						    == L2_TYPE_L)) {
					curaddr = (pte & L2_L_FRAME) |
					    (vaddr & L2_L_OFFSET);
					if (pte & L2_L_COHERENT) {
						map->_dm_flags &=
						    ~ARM32_DMAMAP_COHERENT;
					}
				} else {
					curaddr = (pte & L2_S_FRAME) |
					    (vaddr & L2_S_OFFSET);
					if (pte & L2_S_COHERENT) {
						map->_dm_flags &=
						    ~ARM32_DMAMAP_COHERENT;
					}
				}
			}
		} else {
			(void) pmap_extract(pmap, vaddr, &curaddr);
			map->_dm_flags &= ~ARM32_DMAMAP_COHERENT;
		}

		/*
		 * Make sure we're in an allowed DMA range.
		 */
		if (t->_ranges != NULL) {
			/* XXX cache last result? */
			dr = _bus_dma_inrange(t->_ranges, t->_nranges,
			    curaddr);
			if (dr == NULL)
				return (EINVAL);
			
			/*
			 * In a valid DMA range.  Translate the physical
			 * memory address to an address in the DMA window.
			 */
			curaddr = (curaddr - dr->dr_sysbase) + dr->dr_busbase;
		}

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = PAGE_SIZE - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = (curaddr + map->_dm_boundary) & bmask;
			if (sgsize > (baddr - curaddr))
				sgsize = (baddr - curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr = curaddr;
			map->dm_segs[seg].ds_len = sgsize;
			first = 0;
		} else {
			if (curaddr == lastaddr &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			    (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     (curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr = curaddr;
				map->dm_segs[seg].ds_len = sgsize;
			}
		}

		lastaddr = curaddr + sgsize;
		vaddr += sgsize;
		buflen -= sgsize;
	}

	*segp = seg;
	*lastaddrp = lastaddr;

	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */
	return (0);
}

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
int
_bus_dmamem_alloc_range(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags, paddr_t low, paddr_t high)
{
	paddr_t curaddr, lastaddr;
	struct vm_page *m;
	struct pglist mlist;
	int curseg, error, plaflag;

#ifdef DEBUG_DMA
	printf("alloc_range: t=%p size=%lx align=%lx boundary=%lx segs=%p nsegs=%x rsegs=%p flags=%x lo=%lx hi=%lx\n",
	    t, size, alignment, boundary, segs, nsegs, rsegs, flags, low, high);
#endif	/* DEBUG_DMA */

	/* Always round the size. */
	size = round_page(size);

	/*
	 * Allocate pages from the VM system.
	 */
	plaflag = flags & BUS_DMA_NOWAIT ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & BUS_DMA_ZERO)
		plaflag |= UVM_PLA_ZERO;

	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, low, high, alignment, boundary,
	    &mlist, nsegs, plaflag);
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = TAILQ_FIRST(&mlist);
	curseg = 0;
	lastaddr = segs[curseg].ds_addr = VM_PAGE_TO_PHYS(m);
	segs[curseg].ds_len = PAGE_SIZE;
#ifdef DEBUG_DMA
		printf("alloc: page %lx\n", lastaddr);
#endif	/* DEBUG_DMA */
	m = TAILQ_NEXT(m, pageq);

	for (; m != NULL; m = TAILQ_NEXT(m, pageq)) {
		curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
		if (curaddr < low || curaddr >= high) {
			printf("uvm_pglistalloc returned non-sensical"
			    " address 0x%lx\n", curaddr);
			panic("_bus_dmamem_alloc_range");
		}
#endif	/* DIAGNOSTIC */
#ifdef DEBUG_DMA
		printf("alloc: page %lx\n", curaddr);
#endif	/* DEBUG_DMA */
		if (curaddr == (lastaddr + PAGE_SIZE))
			segs[curseg].ds_len += PAGE_SIZE;
		else {
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	return (0);
}

/*
 * Check if a memory region intersects with a DMA range, and return the
 * page-rounded intersection if it does.
 */
int
arm32_dma_range_intersect(struct arm32_dma_range *ranges, int nranges,
    paddr_t pa, psize_t size, paddr_t *pap, psize_t *sizep)
{
	struct arm32_dma_range *dr;
	int i;

	if (ranges == NULL)
		return (0);

	for (i = 0, dr = ranges; i < nranges; i++, dr++) {
		if (dr->dr_sysbase <= pa &&
		    pa < (dr->dr_sysbase + dr->dr_len)) {
			/*
			 * Beginning of region intersects with this range.
			 */
			*pap = trunc_page(pa);
			*sizep = round_page(min(pa + size,
			    dr->dr_sysbase + dr->dr_len) - pa);
			return (1);
		}
		if (pa < dr->dr_sysbase && dr->dr_sysbase < (pa + size)) {
			/*
			 * End of region intersects with this range.
			 */
			*pap = trunc_page(dr->dr_sysbase);
			*sizep = round_page(min((pa + size) - dr->dr_sysbase,
			    dr->dr_len));
			return (1);
		}
	}

	/* No intersection found. */
	return (0);
}

/*
 * probably should be ppc_space_copy
 */

#define _CONCAT(A,B) A ## B
#define __C(A,B)	_CONCAT(A,B)

#define BUS_SPACE_READ_RAW_MULTI_N(BYTES,SHIFT,TYPE)			\
void									\
__C(bus_space_read_raw_multi_,BYTES)(bus_space_tag_t bst,		\
    bus_space_handle_t h, bus_addr_t o, u_int8_t *dst, bus_size_t size)	\
{									\
	TYPE *rdst = (TYPE *)dst;					\
	int i;								\
	int count = size >> SHIFT;					\
									\
	for (i = 0; i < count; i++) {					\
		rdst[i] = __bs_rs(BYTES, bst, h, o);			\
	}								\
}
BUS_SPACE_READ_RAW_MULTI_N(2,1,u_int16_t)
BUS_SPACE_READ_RAW_MULTI_N(4,2,u_int32_t)

#define BUS_SPACE_WRITE_RAW_MULTI_N(BYTES,SHIFT,TYPE)			\
void									\
__C(bus_space_write_raw_multi_,BYTES)( bus_space_tag_t bst,		\
    bus_space_handle_t h, bus_addr_t o, const u_int8_t *src,		\
    bus_size_t size)							\
{									\
	int i;								\
	TYPE *rsrc = (TYPE *)src;					\
	int count = size >> SHIFT;					\
									\
	for (i = 0; i < count; i++) {					\
		__bs_ws(BYTES, bst, h, o, rsrc[i]);			\
	}								\
}

BUS_SPACE_WRITE_RAW_MULTI_N(2,1,u_int16_t)
BUS_SPACE_WRITE_RAW_MULTI_N(4,2,u_int32_t)
@


1.35
log
@Complete bus_dmamap_load_raw(9) implementation for ARM.  My initial
commit did not copy the vaddr information to the map's segments.  This
means non-coherent bus dma raw mappings could not be synced.

As only agp(4) and radeondrm(4) seem to make use of non-coherent raw
mappings at the moment, this bug did not cause any visible effects.

From Marius Strobl.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.34 2016/08/26 21:50:42 patrick Exp $	*/
a492 1
		/* FALLTHROUGH */
@


1.34
log
@Implement bus dma support for loading raw mappings so that we can use
xhci(4) on ARM.  The only way the load raw operation can get to know
about the coherent flag is via the segments.  Store it there when the
memory is initially mapped.  Also store the virtual address which we
need to know when we have to flush the caches on a non-coherent mapping.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.33 2016/08/26 21:14:58 patrick Exp $	*/
d348 1
d372 1
d419 1
d434 1
d439 1
@


1.33
log
@Since the caller already checks and handles COHERENT we don't need
to explicitly check for the flag.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.32 2016/08/22 01:41:59 jsg Exp $	*/
d345 23
d369 79
a447 1
	panic("_bus_dmamap_load_raw: not implemented");
d657 23
d765 4
d897 3
@


1.32
log
@Before pmap7.c rev 1.35 and pmap.h rev 1.44 DMA'able memory with the
BUS_DMA_COHERENT flag was mapped as device memory which does not use the
store buffer.  It is now mapped as normal inner and outer non-cacheable
which does.

While we drain the cpu store buffer for this case, on cortex a9 systems we
also need to explicitly drain the PL310 L2's store buffer.  With PL310
revisions r3p2 and later this is done automatically after being present in
the store buffer for 256 cycles.  On i.MX6 PL310 is rev r3p1 which does
not have this behaviour.  This issue is i.MX6 errata ERR055199 and PL310
errata 769419.

This change restores io performance with a usb flash drive attached to
my cubox.  Raw reads go from 3 MB/s to 19 MB/s for example.

Based on code written by patrick@@ some time ago.
ok kettenis@@ patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.31 2016/08/14 10:32:17 kettenis Exp $	*/
d457 1
a457 2
		if ((map->_dm_flags & ARM32_DMAMAP_COHERENT) == 0)
			_bus_dmamap_sync_segment(va + offset, pa, seglen, ops);
d509 1
a510 2
		if ((map->_dm_flags & ARM32_DMAMAP_COHERENT) == 0)
			_bus_dmamap_sync_segment(va, pa, seglen, ops);
d548 1
a548 2
		if ((map->_dm_flags & ARM32_DMAMAP_COHERENT) == 0)
			_bus_dmamap_sync_segment(va, pa, seglen, ops);
@


1.31
log
@Allow a bus_dmamap_sync() of length zero.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.30 2016/05/16 15:13:50 kettenis Exp $	*/
d618 1
@


1.30
log
@POSTREAD needs to flush the D-cache since speculative loads might (and do)
bring back cache lines after a PREREAD.  Eliminates random data corruption
on my CuBox-i4Pro.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.29 2016/03/10 10:22:43 tobiasu Exp $	*/
d592 1
a592 1
	if (len == 0 || (offset + len) > map->dm_mapsize)
@


1.29
log
@Remove the explicit map invalidation, free() is going to overwrite
it with junk anyway.

ok patrick mpi dlg and tested by Daniel Bolghero
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.28 2014/11/16 12:30:56 deraadt Exp $	*/
d423 13
@


1.28
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.27 2014/09/13 16:06:36 doug Exp $	*/
a165 9

	/*
	 * Explicit unload.
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;
	map->_dm_origbuf = NULL;
	map->_dm_buftype = ARM32_BUFTYPE_INVALID;
	map->_dm_proc = NULL;
@


1.27
log
@Replace all queue *_END macro calls except CIRCLEQ_END with NULL.

CIRCLEQ_* is deprecated and not called in the tree.  The other queue types
have *_END macros which were added for symmetry with CIRCLEQ_END.  They are
defined as NULL.  There's no reason to keep the other *_END macro calls.

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.26 2014/07/12 18:44:41 tedu Exp $	*/
d768 1
a768 1
			    VM_PROT_READ | VM_PROT_WRITE,
@


1.26
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.25 2014/07/11 09:36:25 mpi Exp $	*/
d1036 1
a1036 1
	for (; m != TAILQ_END(&mlist); m = TAILQ_NEXT(m, pageq)) {
@


1.25
log
@Convert bus_dmamem_map(9) to km_alloc(9) in order to make it fail and
not sleep if the allocator cannot obtain a lock when BUS_DMA_NOWAIT is
specified.

idea and inputs from kettenis@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.24 2013/05/10 20:25:28 patrick Exp $	*/
d176 1
a176 1
	free(map, M_DEVBUF);
@


1.24
log
@Simplify mapping pages by just calling pmap_kenter_cache.
From oga at bitrig.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.23 2013/05/09 20:07:25 patrick Exp $	*/
d738 1
d749 2
a750 2
	va = uvm_km_valloc(kernel_map, size);

d801 1
a801 2
	size = round_page(size);
	uvm_km_free(kernel_map, (vaddr_t)kva, size);
@


1.23
log
@On ARMv7 we can't use the cache mask to check for coherency.
Therefore we add new macros to be able to check for it properly.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.22 2013/05/08 21:10:33 patrick Exp $	*/
d737 1
a737 1
	int curseg, error;
d766 4
a769 22
			error = pmap_enter(pmap_kernel(), va, addr,
			    VM_PROT_READ | VM_PROT_WRITE, VM_PROT_READ |
			    VM_PROT_WRITE | PMAP_WIRED | PMAP_CANFAIL);
			if (error) {
				pmap_update(pmap_kernel());
				uvm_km_free(kernel_map, sva, ssize);
				return (error);
			}
			/*
			 * If the memory must remain coherent with the
			 * cache then we must make the memory uncacheable
			 * in order to maintain virtual cache coherency.
			 * We must also guarantee the cache does not already
			 * contain the virtual addresses we are making
			 * uncacheable.
			 */
			if (flags & BUS_DMA_COHERENT) {
				cpu_dcache_wbinv_range(va, PAGE_SIZE);
				cpu_drain_writebuf();
				pmap_uncache_page(va, addr);
				tlb_flush();
			}
@


1.22
log
@Port over NetBSD's arm dma sync code. This makes it easier to flush
the secondary cache, as we always have the physical address.

tested on panda and zaurus
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.21 2011/06/23 20:44:39 ariane Exp $	*/
d904 1
a904 1
				if (*pde & L1_S_CACHE_MASK) {
d915 1
a915 1
					if (pte & L2_L_CACHE_MASK) {
d922 1
a922 1
					if (pte & L2_S_CACHE_MASK) {
@


1.21
log
@Fix the error path in bus_dmamem_map.
As discussed on icb: remove the comment,
remove pmap_remove (uvm_km_free does that for us).

ok oga@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.20 2011/01/04 21:12:55 miod Exp $	*/
d61 21
d381 2
a382 3
static __inline void
_bus_dmamap_sync_linear(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
d384 1
a384 1
	vaddr_t addr = (vaddr_t) map->_dm_origbuf;
d386 4
a389 1
	addr += offset;
d393 2
a394 1
		cpu_dcache_wbinv_range(addr, len);
d396 1
d398 28
a425 5
	case BUS_DMASYNC_PREREAD:
		if (((addr | len) & arm_dcache_align_mask) == 0)
			cpu_dcache_inv_range(addr, len);
		else
			cpu_dcache_wbinv_range(addr, len);
d427 1
d430 2
a431 1
		cpu_dcache_wb_range(addr, len);
d433 22
d456 2
a457 3
	case BUS_DMASYNC_POSTREAD:
		cpu_dcache_inv_range(addr, len);
		break;
d465 15
a479 10
	struct mbuf *m, *m0 = map->_dm_origbuf;
	bus_size_t minlen, moff;
	vaddr_t maddr;

	for (moff = offset, m = m0; m != NULL && len != 0;
	     m = m->m_next) {
		/* Find the beginning mbuf. */
		if (moff >= m->m_len) {
			moff -= m->m_len;
			continue;
d486 3
a488 6
		minlen = m->m_len - moff;
		if (len < minlen)
			minlen = len;

		maddr = mtod(m, vaddr_t);
		maddr += moff;
a505 7
		switch (ops) {
		case BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE:
			/* if (! M_ROMAP(m)) */{
				cpu_dcache_wbinv_range(maddr, minlen);
				break;
			}
			/* else FALLTHROUGH */
d507 5
a511 14
		case BUS_DMASYNC_PREREAD:
			if (((maddr | minlen) & arm_dcache_align_mask) == 0)
				cpu_dcache_inv_range(maddr, minlen);
			else
				cpu_dcache_wbinv_range(maddr, minlen);
			break;

		case BUS_DMASYNC_PREWRITE:
			/* if (! M_ROMAP(m)) */
				cpu_dcache_wb_range(maddr, minlen);
			break;
		}
		moff = 0;
		len -= minlen;
d519 1
d521 10
a530 3
	struct iovec *iov;
	bus_size_t minlen, ioff;
	vaddr_t addr;
d532 4
a535 5
	for (iov = uio->uio_iov, ioff = offset; len != 0; iov++) {
		/* Find the beginning iovec. */
		if (ioff >= iov->iov_len) {
			ioff -= iov->iov_len;
			continue;
d542 10
a551 25
		minlen = iov->iov_len - ioff;
		if (len < minlen)
			minlen = len;

		addr = (vaddr_t) iov->iov_base;
		addr += ioff;

		switch (ops) {
		case BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE:
			cpu_dcache_wbinv_range(addr, minlen);
			break;

		case BUS_DMASYNC_PREREAD:
			if (((addr | minlen) & arm_dcache_align_mask) == 0)
				cpu_dcache_inv_range(addr, minlen);
			else
				cpu_dcache_wbinv_range(addr, minlen);
			break;

		case BUS_DMASYNC_PREWRITE:
			cpu_dcache_wb_range(addr, minlen);
			break;
		}
		ioff = 0;
		len -= minlen;
@


1.20
log
@Use pmap_uncache_page() to alter cacheability of pages in bus_dmamem_map()
instead of playing with pte bits directly; this will cause cacheability to
be restored eventually; makes the zaurus textmode console memory cached again
after exiting from X.
ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.19 2010/12/26 15:40:59 miod Exp $	*/
a720 4
				/*
				 * Clean up after ourselves.
				 * XXX uvm_wait on WAITOK
				 */
d722 1
a722 1
				uvm_km_free(kernel_map, va, ssize);
@


1.19
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.18 2010/11/20 20:33:23 miod Exp $	*/
d689 3
a691 1
	pt_entry_t *ptep/*, pte*/;
d740 1
a740 3
				ptep = vtopte(va);
				*ptep &= ~L2_S_CACHE_MASK;
				PTE_SYNC(ptep);
@


1.18
log
@This is a first step towards getting rid of avail_start and avail_end in the
kernel, currently limited to low-hanging fruit: these variables were used
by bus_dma to specify the range in which to allocate memory, back when
uvm_pglistalloc() was stupid and would not walk the vm_physseg[].

Nowadays, except on some platforms for early initialization, these variables
are not used, or do not need to be global variables. Therefore:
- remove `extern' declarations of avail_start and avail_end (or close cousins,
  such as arm physical_start and physical_end) from files which no longer need
  to use them.
- make them local variables whenever possible.
- remove them when they are assigned to but no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.17 2010/03/31 19:46:26 miod Exp $	*/
d802 1
a802 1
		return (atop(segs[i].ds_addr + off));
@


1.17
log
@Make sure the boundaries of uvm_pglistalloc() calls are set up with low
being page-aligned, and high being end of page (i.e.
high & PAGE_MASK == PAGE_MASK) everywhere, for consistency. Future code
will depend on this.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.16 2010/03/29 19:21:58 oga Exp $	*/
a606 3

extern paddr_t physical_start;
extern paddr_t physical_end;
@


1.16
log
@PMAP_CANFAIL for bus_dmamem_map on all other architectures (and some
whitespace tweaks on i386 so that it matches).

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.15 2009/04/20 00:42:05 oga Exp $	*/
d633 1
a633 1
			    trunc_page(dr->dr_sysbase + dr->dr_len));
@


1.15
log
@Add a BUS_DMA_ZERO flag for bus_dmamem_alloc() to return zeroed memory.

Saves every damned driver calling bzero(), and continues the M_ZERO,
PR_ZERO symmetry.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.14 2009/04/14 16:01:04 oga Exp $	*/
d688 2
a689 1
	vaddr_t va;
d691 1
a691 1
	int curseg;
d707 2
d718 12
a729 3
			pmap_enter(pmap_kernel(), va, addr,
			    VM_PROT_READ | VM_PROT_WRITE,
			    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
@


1.14
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.13 2009/03/07 15:34:34 miod Exp $	*/
d978 2
@


1.13
log
@When allocating memory in bus_dmamem_alloc() with uvm_pglistalloc(), do not
try to be smart for the address range, uvm_pglistalloc() is smart enough
nowadays.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.12 2008/06/26 05:42:09 ray Exp $	*/
d964 1
a964 1
	int curseg, error;
a973 1
	TAILQ_INIT(&mlist);
d977 3
d981 1
a981 1
	    &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.12
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.11 2007/10/06 23:12:17 krw Exp $	*/
d639 1
a639 2
		    segs, nsegs, rsegs, flags, trunc_page(physical_start),
		    trunc_page(physical_end));
@


1.11
log
@Some archs used memset() rather than bzero(). So duplicate diff
previously applied to other archs deleting a memset() this time. e.g.

-	if ((mapstore = malloc(mapsize, M_DEVBUF,
-	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
+	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
+	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
 		return (ENOMEM);

-	memset(mapstore, 0, mapsize);
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.10 2007/05/29 21:00:50 jason Exp $	*/
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.10
log
@s/entrace/entrance (not obvious that the code was cut/paste =)
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.9 2007/04/10 18:22:07 miod Exp $	*/
d118 2
a119 2
	if ((mapstore = malloc(mapsize, M_DEVBUF,
	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
a121 1
	memset(mapstore, 0, mapsize);
@


1.9
log
@Remove ARM32_BUFTYPE_RAW, it's never used.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.8 2006/07/16 00:18:33 drahn Exp $	*/
d816 1
a816 1
 * the starting segment on entrace, and the ending segment on exit.
@


1.8
log
@Contrary to where this was stolen from, arm does indeed need
BUS_DMASYNC_POSTREAD implemented otherwise the old device state
may still be present in the cache. Allows em(4) to work _much_ better on
iodata.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.7 2005/11/09 18:08:37 martin Exp $	*/
a597 4
	case ARM32_BUFTYPE_RAW:
		panic("_bus_dmamap_sync: ARM32_BUFTYPE_RAW");
		break;

d732 1
a732 1
			 * contain the virtal addresses we are making
@


1.7
log
@use atop() and ptoa()

tested by jolan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.6 2005/05/27 20:15:50 uwe Exp $	*/
d391 4
d562 3
a564 1
	 *	POSTREAD -- Nothing.
a567 4

	ops &= (BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE);
	if (ops == 0)
		return;
@


1.6
log
@typo in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.5 2004/12/30 23:24:57 drahn Exp $	*/
d804 1
a804 1
		return (arm_btop((u_long)segs[i].ds_addr + off));
@


1.5
log
@Fix (and de-inline) bus_space_(read|write)_raw* functions, they did
not terminate correctly on odd length, they still truncate the transfer.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.4 2004/12/25 23:02:23 miod Exp $	*/
d780 1
a780 1
 * Common functin for mmap(2)'ing DMA-safe memory.  May be called by
@


1.4
log
@Use list and queue macros where applicable to make the code easier to read;
no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.3 2004/11/09 19:17:01 claudio Exp $	*/
d1073 41
@


1.3
log
@Do not map empty mbufs (m_len == 0) in bus_dmamap_load_mbuf() as these mappings
may disturb the dma as seen in ipw(4). Emtpy mbufs are at the beginning of the
mbuf chain and are as example a "side-effect" of a previous m_adj() call.
OK miod@@ mickey@@ jason@@ markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.2 2004/02/01 06:10:33 drahn Exp $	*/
d998 1
a998 1
	m = mlist.tqh_first;
d1005 1
a1005 1
	m = m->pageq.tqe_next;
d1007 1
a1007 1
	for (; m != NULL; m = m->pageq.tqe_next) {
@


1.2
log
@fix tags...
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.1 2004/02/01 05:09:48 drahn Exp $	*/
d253 2
@


1.1
log
@Arm port, NetBSD codebase stripped down, 32bit only support.
@
text
@d1 2
a2 2
/*	$OpenBSD: bus_dma.c,v 1.2 2004/01/29 16:17:16 drahn Exp $	*/
/*^I$NetBSD: bus_dma.c,v 1.38 2003/10/30 08:44:13 scw Exp $^I*/$
@

