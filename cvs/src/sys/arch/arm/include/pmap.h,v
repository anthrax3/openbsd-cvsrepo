head	1.48;
access;
symbols
	OPENBSD_6_1:1.48.0.2
	OPENBSD_6_1_BASE:1.48
	OPENBSD_6_0:1.40.0.2
	OPENBSD_6_0_BASE:1.40
	OPENBSD_5_9:1.35.0.2
	OPENBSD_5_9_BASE:1.35
	OPENBSD_5_8:1.33.0.4
	OPENBSD_5_8_BASE:1.33
	OPENBSD_5_7:1.32.0.2
	OPENBSD_5_7_BASE:1.32
	OPENBSD_5_6:1.25.0.6
	OPENBSD_5_6_BASE:1.25
	OPENBSD_5_5:1.25.0.4
	OPENBSD_5_5_BASE:1.25
	OPENBSD_5_4:1.24.0.2
	OPENBSD_5_4_BASE:1.24
	OPENBSD_5_3:1.21.0.6
	OPENBSD_5_3_BASE:1.21
	OPENBSD_5_2:1.21.0.4
	OPENBSD_5_2_BASE:1.21
	OPENBSD_5_1_BASE:1.21
	OPENBSD_5_1:1.21.0.2
	OPENBSD_5_0:1.18.0.2
	OPENBSD_5_0_BASE:1.18
	OPENBSD_4_9:1.16.0.2
	OPENBSD_4_9_BASE:1.16
	OPENBSD_4_8:1.12.0.2
	OPENBSD_4_8_BASE:1.12
	OPENBSD_4_7:1.11.0.2
	OPENBSD_4_7_BASE:1.11
	OPENBSD_4_6:1.11.0.4
	OPENBSD_4_6_BASE:1.11
	OPENBSD_4_5:1.9.0.6
	OPENBSD_4_5_BASE:1.9
	OPENBSD_4_4:1.9.0.4
	OPENBSD_4_4_BASE:1.9
	OPENBSD_4_3:1.9.0.2
	OPENBSD_4_3_BASE:1.9
	OPENBSD_4_2:1.7.0.2
	OPENBSD_4_2_BASE:1.7
	OPENBSD_4_1:1.6.0.4
	OPENBSD_4_1_BASE:1.6
	OPENBSD_4_0:1.6.0.2
	OPENBSD_4_0_BASE:1.6
	OPENBSD_3_9:1.4.0.2
	OPENBSD_3_9_BASE:1.4
	OPENBSD_3_8:1.3.0.6
	OPENBSD_3_8_BASE:1.3
	OPENBSD_3_7:1.3.0.4
	OPENBSD_3_7_BASE:1.3
	OPENBSD_3_6:1.3.0.2
	OPENBSD_3_6_BASE:1.3
	SMP_SYNC_A:1.2
	SMP_SYNC_B:1.2
	OPENBSD_3_5:1.1.0.4
	OPENBSD_3_5_BASE:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.48
date	2017.01.06.00.06.02;	author jsg;	state Exp;
branches;
next	1.47;
commitid	YCbIRye8xzoWl68V;

1.47
date	2017.01.04.00.40.49;	author jsg;	state Exp;
branches;
next	1.46;
commitid	wyJlaIFS0yAv07lJ;

1.46
date	2016.08.26.11.59.04;	author kettenis;	state Exp;
branches;
next	1.45;
commitid	eHAV7iG810DcGo8p;

1.45
date	2016.08.19.17.31.04;	author kettenis;	state Exp;
branches;
next	1.44;
commitid	MoNCYZAeTFoT7uMc;

1.44
date	2016.08.19.14.05.23;	author kettenis;	state Exp;
branches;
next	1.43;
commitid	8FytF3t8wQ3O7ruR;

1.43
date	2016.08.19.13.56.08;	author kettenis;	state Exp;
branches;
next	1.42;
commitid	uXjojXCZvopcWMeE;

1.42
date	2016.08.10.21.22.43;	author kettenis;	state Exp;
branches;
next	1.41;
commitid	TF23RAMj7RpPlALy;

1.41
date	2016.08.08.14.47.52;	author kettenis;	state Exp;
branches;
next	1.40;
commitid	CQxVw8UNradhT604;

1.40
date	2016.03.22.23.35.01;	author patrick;	state Exp;
branches;
next	1.39;
commitid	RkEnvixfXW2bEpue;

1.39
date	2016.03.22.23.28.02;	author patrick;	state Exp;
branches;
next	1.38;
commitid	bAxWaXK3mkjM56Iw;

1.38
date	2016.03.19.09.36.57;	author patrick;	state Exp;
branches;
next	1.37;
commitid	PJBEdVboqiwjXPXy;

1.37
date	2016.03.18.13.16.02;	author jsg;	state Exp;
branches;
next	1.36;
commitid	RKG2cJTBtpcdjKgj;

1.36
date	2016.03.18.06.54.21;	author jsg;	state Exp;
branches;
next	1.35;
commitid	82aYg3LAWI4D5T1j;

1.35
date	2016.02.01.04.28.45;	author jsg;	state Exp;
branches;
next	1.34;
commitid	GuYAGioFXe7G3dLA;

1.34
date	2015.08.15.22.20.20;	author miod;	state Exp;
branches;
next	1.33;
commitid	rjFF3mVWQmD2XvGW;

1.33
date	2015.06.24.21.26.04;	author miod;	state Exp;
branches;
next	1.32;
commitid	KB1S6AlciarxGFRW;

1.32
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.31;
commitid	eahBabNpxnDWKzqJ;

1.31
date	2015.02.07.01.46.27;	author kettenis;	state Exp;
branches;
next	1.30;
commitid	7b6QyxnUYRNhvNHU;

1.30
date	2015.01.29.20.10.52;	author deraadt;	state Exp;
branches;
next	1.29;
commitid	J9beJfMoTflGapt5;

1.29
date	2015.01.01.03.26.13;	author jsg;	state Exp;
branches;
next	1.28;
commitid	jQqKNwsJFHWhLmRI;

1.28
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.27;
commitid	yv0ECmCdICvq576h;

1.27
date	2014.10.07.10.10.58;	author jsg;	state Exp;
branches;
next	1.26;
commitid	AH6ZfpO27t1GxxND;

1.26
date	2014.10.07.07.14.55;	author jsg;	state Exp;
branches;
next	1.25;
commitid	Lon7MMg3dHoKgehP;

1.25
date	2014.01.30.18.16.41;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2013.05.18.18.06.05;	author patrick;	state Exp;
branches;
next	1.23;

1.23
date	2013.05.09.20.07.26;	author patrick;	state Exp;
branches;
next	1.22;

1.22
date	2013.04.26.11.51.22;	author patrick;	state Exp;
branches;
next	1.21;

1.21
date	2011.11.05.18.11.26;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2011.09.21.10.12.59;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2011.09.20.22.02.13;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2011.04.28.20.50.58;	author ariane;	state Exp;
branches;
next	1.17;

1.17
date	2011.03.23.16.54.34;	author pirofti;	state Exp;
branches;
next	1.16;

1.16
date	2011.01.04.21.11.41;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2010.12.06.20.57.15;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2010.11.27.20.45.27;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2010.07.01.22.40.10;	author drahn;	state Exp;
branches;
next	1.11;

1.11
date	2009.05.24.04.56.19;	author drahn;	state Exp;
branches;
next	1.10;

1.10
date	2009.05.08.02.57.32;	author drahn;	state Exp;
branches;
next	1.9;

1.9
date	2007.10.10.15.53.51;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2007.09.10.18.49.44;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2007.04.21.19.26.04;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2006.05.27.20.36.05;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2006.05.26.17.11.41;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2005.11.09.18.08.37;	author martin;	state Exp;
branches;
next	1.3;

1.3
date	2004.08.06.22.39.12;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2004.05.20.09.20.41;	author kettenis;	state Exp;
branches;
next	1.1;

1.1
date	2004.02.01.05.09.49;	author drahn;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2004.02.19.10.48.02;	author niklas;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2004.06.05.23.10.45;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.48
log
@unifdef CPU_ARMv7 and ARM_ARCH_7
ok kettenis@@ patrick@@
@
text
@/*	$OpenBSD: pmap.h,v 1.47 2017/01/04 00:40:49 jsg Exp $	*/
/*	$NetBSD: pmap.h,v 1.76 2003/09/06 09:10:46 rearnsha Exp $	*/

/*
 * Copyright (c) 2002, 2003 Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Jason R. Thorpe & Steve C. Woodford for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed for the NetBSD Project by
 *	Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright (c) 1994,1995 Mark Brinicombe.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Mark Brinicombe
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef	_ARM_PMAP_H_
#define	_ARM_PMAP_H_

#ifdef _KERNEL

#include <arm/cpuconf.h>
#include <arm/pte.h>
#ifndef _LOCORE
#include <arm/cpufunc.h>
#endif

/*
 * a pmap describes a processes' 4GB virtual address space.  this
 * virtual address space can be broken up into 4096 1MB regions which
 * are described by L1 PTEs in the L1 table.
 *
 * There is a line drawn at KERNEL_BASE.  Everything below that line
 * changes when the VM context is switched.  Everything above that line
 * is the same no matter which VM context is running.  This is achieved
 * by making the L1 PTEs for those slots above KERNEL_BASE reference
 * kernel L2 tables.
 *
 * The basic layout of the virtual address space thus looks like this:
 *
 *	0xffffffff
 *	.
 *	.
 *	.
 *	KERNEL_BASE
 *	--------------------
 *	.
 *	.
 *	.
 *	0x00000000
 */

/*
 * The number of L2 descriptor tables which can be tracked by an l2_dtable.
 * A bucket size of 16 provides for 16MB of contiguous virtual address
 * space per l2_dtable. Most processes will, therefore, require only two or
 * three of these to map their whole working set.
 */
#define	L2_BUCKET_LOG2	4
#define	L2_BUCKET_SIZE	(1 << L2_BUCKET_LOG2)

/*
 * Given the above "L2-descriptors-per-l2_dtable" constant, the number
 * of l2_dtable structures required to track all possible page descriptors
 * mappable by an L1 translation table is given by the following constants:
 */
#define	L2_LOG2		((32 - L1_S_SHIFT) - L2_BUCKET_LOG2)
#define	L2_SIZE		(1 << L2_LOG2)

#ifndef _LOCORE

struct l1_ttable;
struct l2_dtable;

/*
 * Track cache/tlb occupancy using the following structure
 */
union pmap_cache_state {
	struct {
		union {
			u_int8_t csu_cache_b[2];
			u_int16_t csu_cache;
		} cs_cache_u;

		union {
			u_int8_t csu_tlb_b[2];
			u_int16_t csu_tlb;
		} cs_tlb_u;
	} cs_s;
	u_int32_t cs_all;
};
#define	cs_cache_id	cs_s.cs_cache_u.csu_cache_b[0]
#define	cs_cache_d	cs_s.cs_cache_u.csu_cache_b[1]
#define	cs_cache	cs_s.cs_cache_u.csu_cache
#define	cs_tlb_id	cs_s.cs_tlb_u.csu_tlb_b[0]
#define	cs_tlb_d	cs_s.cs_tlb_u.csu_tlb_b[1]
#define	cs_tlb		cs_s.cs_tlb_u.csu_tlb

/*
 * Assigned to cs_all to force cacheops to work for a particular pmap
 */
#define	PMAP_CACHE_STATE_ALL	0xffffffffu

/*
 * This structure is used by machine-dependent code to describe
 * static mappings of devices, created at bootstrap time.
 */
struct pmap_devmap {
	vaddr_t		pd_va;		/* virtual address */
	paddr_t		pd_pa;		/* physical address */
	psize_t		pd_size;	/* size of region */
	vm_prot_t	pd_prot;	/* protection code */
	int		pd_cache;	/* cache attributes */
};

/*
 * The pmap structure itself
 */
struct pmap {
	u_int8_t		pm_domain;
	boolean_t		pm_remove_all;
	struct l1_ttable	*pm_l1;
	union pmap_cache_state	pm_cstate;
	u_int			pm_refs;
	struct l2_dtable	*pm_l2[L2_SIZE];
	struct pmap_statistics	pm_stats;
};

typedef struct pmap *pmap_t;

/*
 * MD flags that we use for pmap_enter (in the pa):
 */
#define PMAP_PA_MASK	~((paddr_t)PAGE_MASK) /* to remove the flags */
#define PMAP_NOCACHE	0x1 /* non-cacheable memory. */
#define PMAP_DEVICE	0x2 /* device memory. */

/*
 * Physical / virtual address structure. In a number of places (particularly
 * during bootstrapping) we need to keep track of the physical and virtual
 * addresses of various pages
 */
typedef struct pv_addr {
	SLIST_ENTRY(pv_addr) pv_list;
	paddr_t pv_pa;
	vaddr_t pv_va;
} pv_addr_t;

/*
 * Determine various modes for PTEs (user vs. kernel, cacheable
 * vs. non-cacheable).
 */
#define	PTE_KERNEL	0
#define	PTE_USER	1
#define	PTE_NOCACHE	0
#define	PTE_CACHE	1
#define	PTE_PAGETABLE	2

/*
 * Flags that indicate attributes of pages or mappings of pages.
 *
 * The PVF_MOD and PVF_REF flags are stored in the mdpage for each
 * page.  PVF_WIRED, PVF_WRITE, and PVF_NC are kept in individual
 * pv_entry's for each page.  They live in the same "namespace" so
 * that we can clear multiple attributes at a time.
 *
 * Note the "non-cacheable" flag generally means the page has
 * multiple mappings in a given address space.
 */
#define	PVF_MOD		0x01		/* page is modified */
#define	PVF_REF		0x02		/* page is referenced */
#define	PVF_WIRED	0x04		/* mapping is wired */
#define	PVF_WRITE	0x08		/* mapping is writable */
#define	PVF_EXEC	0x10		/* mapping is executable */
#define	PVF_UNC		0x20		/* mapping is 'user' non-cacheable */
#define	PVF_KNC		0x40		/* mapping is 'kernel' non-cacheable */
#define	PVF_NC		(PVF_UNC|PVF_KNC)

/*
 * Commonly referenced structures
 */
extern struct pmap	kernel_pmap_store;

/*
 * Macros that we need to export
 */
#define pmap_kernel()			(&kernel_pmap_store)
#define	pmap_resident_count(pmap)	((pmap)->pm_stats.resident_count)
#define	pmap_wired_count(pmap)		((pmap)->pm_stats.wired_count)

#define	pmap_is_modified(pg)	\
	(((pg)->mdpage.pvh_attrs & PVF_MOD) != 0)
#define	pmap_is_referenced(pg)	\
	(((pg)->mdpage.pvh_attrs & PVF_REF) != 0)

#define	pmap_deactivate(p)		do { /* nothing */ } while (0)
#define	pmap_copy(dp, sp, da, l, sa)	do { /* nothing */ } while (0)

#define pmap_unuse_final(p)		do { /* nothing */ } while (0)
#define	pmap_remove_holes(vm)		do { /* nothing */ } while (0)

/*
 * Functions that we need to export
 */
void	pmap_remove_all(pmap_t);
void	pmap_uncache_page(paddr_t, vaddr_t);

#define PMAP_GROWKERNEL		/* turn on pmap_growkernel interface */

/* Functions we use internally. */
void	pmap_bootstrap(pd_entry_t *, vaddr_t, vaddr_t);

int	pmap_fault_fixup(pmap_t, vaddr_t, vm_prot_t, int);
boolean_t pmap_get_pde_pte(pmap_t, vaddr_t, pd_entry_t **, pt_entry_t **);
boolean_t pmap_get_pde(pmap_t, vaddr_t, pd_entry_t **);
void	pmap_set_pcb_pagedir(pmap_t, struct pcb *);

void	pmap_postinit(void);

void	vector_page_setprot(int);

/* XXX */
void pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cacheable);

const struct pmap_devmap *pmap_devmap_find_pa(paddr_t, psize_t);
const struct pmap_devmap *pmap_devmap_find_va(vaddr_t, vsize_t);

/* Bootstrapping routines. */
void	pmap_map_section(vaddr_t, vaddr_t, paddr_t, int, int);
void	pmap_map_entry(vaddr_t, vaddr_t, paddr_t, int, int);
vsize_t	pmap_map_chunk(vaddr_t, vaddr_t, paddr_t, vsize_t, int, int);
void	pmap_link_l2pt(vaddr_t, vaddr_t, pv_addr_t *);
void	pmap_devmap_bootstrap(vaddr_t, const struct pmap_devmap *);
void	pmap_devmap_register(const struct pmap_devmap *);

/*
 * The current top of kernel VM
 */
extern vaddr_t	pmap_curmaxkvaddr;

/*
 * Useful macros and constants 
 */

/* Virtual address to page table entry */
static __inline pt_entry_t *
vtopte(vaddr_t va)
{
	pd_entry_t *pdep;
	pt_entry_t *ptep;

	if (pmap_get_pde_pte(pmap_kernel(), va, &pdep, &ptep) == FALSE)
		return (NULL);
	return (ptep);
}

/*
 * The new pmap ensures that page-tables are always mapping Write-Thru.
 * Thus, on some platforms we can run fast and loose and avoid syncing PTEs
 * on every change.
 *
 * Unfortunately, not all CPUs have a write-through cache mode.  So we
 * define PMAP_NEEDS_PTE_SYNC for C code to conditionally do PTE syncs,
 * and if there is the chance for PTE syncs to be needed, we define
 * PMAP_INCLUDE_PTE_SYNC so e.g. assembly code can include (and run)
 * the code.
 */
extern int pmap_needs_pte_sync;

#define	PMAP_NEEDS_PTE_SYNC	pmap_needs_pte_sync
#define	PMAP_INCLUDE_PTE_SYNC

#define	PTE_SYNC(pte)							\
do {									\
	cpu_drain_writebuf();						\
	if (PMAP_NEEDS_PTE_SYNC) {					\
		paddr_t pa;						\
		cpu_dcache_wb_range((vaddr_t)(pte), sizeof(pt_entry_t));\
		if (cpu_sdcache_enabled()) { 				\
		(void)pmap_extract(pmap_kernel(), (vaddr_t)(pte), &pa);	\
		cpu_sdcache_wb_range((vaddr_t)(pte), (paddr_t)(pa),	\
		    sizeof(pt_entry_t));				\
		};							\
		cpu_drain_writebuf();					\
	}								\
} while (/*CONSTCOND*/0)

#define	PTE_SYNC_RANGE(pte, cnt)					\
do {									\
	cpu_drain_writebuf();						\
	if (PMAP_NEEDS_PTE_SYNC) {					\
		paddr_t pa;						\
		cpu_dcache_wb_range((vaddr_t)(pte),			\
		    (cnt) << 2); /* * sizeof(pt_entry_t) */		\
		if (cpu_sdcache_enabled()) { 				\
		(void)pmap_extract(pmap_kernel(), (vaddr_t)(pte), &pa);\
		cpu_sdcache_wb_range((vaddr_t)(pte), (paddr_t)(pa),	\
		    (cnt) << 2); /* * sizeof(pt_entry_t) */		\
		};							\
		cpu_drain_writebuf();					\
	}								\
} while (/*CONSTCOND*/0)

#define	l1pte_valid(pde)	(((pde) & L1_TYPE_MASK) != L1_TYPE_INV)
#define	l1pte_section_p(pde)	(((pde) & L1_TYPE_MASK) == L1_TYPE_S)
#define	l1pte_page_p(pde)	(((pde) & L1_TYPE_MASK) == L1_TYPE_C)
#define	l1pte_fpage_p(pde)	(((pde) & L1_TYPE_MASK) == L1_TYPE_F)

#define l2pte_index(v)		(((v) & L2_ADDR_BITS) >> L2_S_SHIFT)
#define	l2pte_valid(pte)	(((pte) & L2_TYPE_MASK) != L2_TYPE_INV)
#define	l2pte_pa(pte)		((pte) & L2_S_FRAME)

/* L1 and L2 page table macros */
#define pmap_pde_v(pde)		l1pte_valid(*(pde))
#define pmap_pde_section(pde)	l1pte_section_p(*(pde))
#define pmap_pde_page(pde)	l1pte_page_p(*(pde))
#define pmap_pde_fpage(pde)	l1pte_fpage_p(*(pde))

/************************* ARM MMU configuration *****************************/

#if (ARM_MMU_V7) != 0
void	pmap_copy_page_generic(struct vm_page *, struct vm_page *);
void	pmap_zero_page_generic(struct vm_page *);

void	pmap_pte_init_generic(void);
void	pmap_pte_init_armv7(void);
#endif /* (ARM_MMU_V7) != 0 */

#if ARM_MMU_V7 == 1
void	pmap_pte_init_v7(void);
#endif /* ARM_MMU_V7 == 1 */

extern pt_entry_t		pte_l1_s_cache_mode;
extern pt_entry_t		pte_l1_s_cache_mask;

extern pt_entry_t		pte_l2_l_cache_mode;
extern pt_entry_t		pte_l2_l_cache_mask;

extern pt_entry_t		pte_l2_s_cache_mode;
extern pt_entry_t		pte_l2_s_cache_mask;

extern pt_entry_t		pte_l1_s_cache_mode_pt;
extern pt_entry_t		pte_l2_l_cache_mode_pt;
extern pt_entry_t		pte_l2_s_cache_mode_pt;

extern pt_entry_t		pte_l1_s_coherent;
extern pt_entry_t		pte_l2_l_coherent;
extern pt_entry_t		pte_l2_s_coherent;

extern pt_entry_t		pte_l1_s_prot_ur;
extern pt_entry_t		pte_l1_s_prot_uw;
extern pt_entry_t		pte_l1_s_prot_kr;
extern pt_entry_t		pte_l1_s_prot_kw;
extern pt_entry_t		pte_l1_s_prot_mask;

extern pt_entry_t		pte_l2_l_prot_ur;
extern pt_entry_t		pte_l2_l_prot_uw;
extern pt_entry_t		pte_l2_l_prot_kr;
extern pt_entry_t		pte_l2_l_prot_kw;
extern pt_entry_t		pte_l2_l_prot_mask;

extern pt_entry_t		pte_l2_s_prot_ur;
extern pt_entry_t		pte_l2_s_prot_uw;
extern pt_entry_t		pte_l2_s_prot_kr;
extern pt_entry_t		pte_l2_s_prot_kw;
extern pt_entry_t		pte_l2_s_prot_mask;
 
extern pt_entry_t		pte_l1_s_proto;
extern pt_entry_t		pte_l1_c_proto;
extern pt_entry_t		pte_l2_s_proto;

extern void (*pmap_copy_page_func)(struct vm_page *, struct vm_page *);
extern void (*pmap_zero_page_func)(struct vm_page *);

#endif /* !_LOCORE */

/*****************************************************************************/

/*
 * Definitions for MMU domains
 */
#define	PMAP_DOMAINS		15	/* 15 'user' domains (0-14) */
#define	PMAP_DOMAIN_KERNEL	15	/* The kernel uses domain #15 */

/*
 * These macros define the various bit masks in the PTE.
 *
 * We use these macros since we use different bits on different processor
 * models.
 */
#define	L1_S_PROT_UR_v7		(L1_S_V7_AP(AP_V7_KRUR))
#define	L1_S_PROT_UW_v7		(L1_S_V7_AP(AP_KRWURW))
#define	L1_S_PROT_KR_v7		(L1_S_V7_AP(AP_V7_KR))
#define	L1_S_PROT_KW_v7		(L1_S_V7_AP(AP_KRW))
#define	L1_S_PROT_MASK_v7	(L1_S_V7_AP(0x07))

#define	L1_S_CACHE_MASK_v7	(L1_S_B|L1_S_C|L1_S_V7_TEX_MASK)

#define	L1_S_COHERENT_v7	(L1_S_C)

#define	L2_L_PROT_UR_v7		(L2_V7_AP(AP_V7_KRUR))
#define	L2_L_PROT_UW_v7		(L2_V7_AP(AP_KRWURW))
#define	L2_L_PROT_KR_v7		(L2_V7_AP(AP_V7_KR))
#define	L2_L_PROT_KW_v7		(L2_V7_AP(AP_KRW))
#define	L2_L_PROT_MASK_v7	(L2_V7_AP(0x07) | L2_V7_L_XN)

#define	L2_L_CACHE_MASK_v7	(L2_B|L2_C|L2_V7_L_TEX_MASK)

#define	L2_L_COHERENT_v7	(L2_C)

#define	L2_S_PROT_UR_v7		(L2_V7_AP(AP_V7_KRUR))
#define	L2_S_PROT_UW_v7		(L2_V7_AP(AP_KRWURW))
#define	L2_S_PROT_KR_v7		(L2_V7_AP(AP_V7_KR))
#define	L2_S_PROT_KW_v7		(L2_V7_AP(AP_KRW))
#define	L2_S_PROT_MASK_v7	(L2_V7_AP(0x07) | L2_V7_S_XN)

#define	L2_S_CACHE_MASK_v7	(L2_B|L2_C|L2_V7_S_TEX_MASK)

#define	L2_S_COHERENT_v7	(L2_C)

#define	L1_S_PROTO_v7		(L1_TYPE_S)

#define	L1_C_PROTO_v7		(L1_TYPE_C)

#define	L2_L_PROTO		(L2_TYPE_L)

#define	L2_S_PROTO_v7		(L2_TYPE_S)

/*
 * User-visible names for the ones that vary with MMU class.
 */

#if ARM_NMMUS > 1
/* More than one MMU class configured; use variables. */
#define	L1_S_PROT_UR		pte_l1_s_prot_ur
#define	L1_S_PROT_UW		pte_l1_s_prot_uw
#define	L1_S_PROT_KR		pte_l1_s_prot_kr
#define	L1_S_PROT_KW		pte_l1_s_prot_kw
#define	L1_S_PROT_MASK		pte_l1_s_prot_mask

#define	L2_L_PROT_UR		pte_l2_l_prot_ur
#define	L2_L_PROT_UW		pte_l2_l_prot_uw
#define	L2_L_PROT_KR		pte_l2_l_prot_kr
#define	L2_L_PROT_KW		pte_l2_l_prot_kw
#define	L2_L_PROT_MASK		pte_l2_l_prot_mask

#define	L2_S_PROT_UR		pte_l2_s_prot_ur
#define	L2_S_PROT_UW		pte_l2_s_prot_uw
#define	L2_S_PROT_KR		pte_l2_s_prot_kr
#define	L2_S_PROT_KW		pte_l2_s_prot_kw
#define	L2_S_PROT_MASK		pte_l2_s_prot_mask

#define	L1_S_CACHE_MASK		pte_l1_s_cache_mask
#define	L2_L_CACHE_MASK		pte_l2_l_cache_mask
#define	L2_S_CACHE_MASK		pte_l2_s_cache_mask

#define	L1_S_COHERENT		pte_l1_s_coherent
#define	L2_L_COHERENT		pte_l2_l_coherent
#define	L2_S_COHERENT		pte_l2_s_coherent

#define	L1_S_PROTO		pte_l1_s_proto
#define	L1_C_PROTO		pte_l1_c_proto
#define	L2_S_PROTO		pte_l2_s_proto

#define	pmap_copy_page(s, d)	(*pmap_copy_page_func)((s), (d))
#define	pmap_zero_page(d)	(*pmap_zero_page_func)((d))
#elif ARM_MMU_V7 == 1
#define	L1_S_PROT_UR		L1_S_PROT_UR_v7
#define	L1_S_PROT_UW		L1_S_PROT_UW_v7
#define	L1_S_PROT_KR		L1_S_PROT_KR_v7
#define	L1_S_PROT_KW		L1_S_PROT_KW_v7
#define	L1_S_PROT_MASK		L1_S_PROT_MASK_v7

#define	L2_L_PROT_UR		L2_L_PROT_UR_v7
#define	L2_L_PROT_UW		L2_L_PROT_UW_v7
#define	L2_L_PROT_KR		L2_L_PROT_KR_v7
#define	L2_L_PROT_KW		L2_L_PROT_KW_v7
#define	L2_L_PROT_MASK		L2_L_PROT_MASK_v7

#define	L2_S_PROT_UR		L2_S_PROT_UR_v7
#define	L2_S_PROT_UW		L2_S_PROT_UW_v7
#define	L2_S_PROT_KR		L2_S_PROT_KR_v7
#define	L2_S_PROT_KW		L2_S_PROT_KW_v7
#define	L2_S_PROT_MASK		L2_S_PROT_MASK_v7

#define	L1_S_CACHE_MASK		L1_S_CACHE_MASK_v7
#define	L2_L_CACHE_MASK		L2_L_CACHE_MASK_v7
#define	L2_S_CACHE_MASK		L2_S_CACHE_MASK_v7

#define	L1_S_COHERENT		L1_S_COHERENT_v7
#define	L2_L_COHERENT		L2_L_COHERENT_v7
#define	L2_S_COHERENT		L2_S_COHERENT_v7

#define	L1_S_PROTO		L1_S_PROTO_v7
#define	L1_C_PROTO		L1_C_PROTO_v7
#define	L2_S_PROTO		L2_S_PROTO_v7

#define	pmap_copy_page(s, d)	pmap_copy_page_generic((s), (d))
#define	pmap_zero_page(d)	pmap_zero_page_generic((d))
#endif /* ARM_NMMUS > 1 */

/*
 * These macros return various bits based on kernel/user and protection.
 * Note that the compiler will usually fold these at compile time.
 */
#ifndef _LOCORE
static __inline pt_entry_t
L1_S_PROT(int ku, vm_prot_t pr)
{
	pt_entry_t pte;

	if (ku == PTE_USER)
		pte = (pr & PROT_WRITE) ? L1_S_PROT_UW : L1_S_PROT_UR;
	else
		pte = (pr & PROT_WRITE) ? L1_S_PROT_KW : L1_S_PROT_KR;

	if ((pr & PROT_EXEC) == 0)
		pte |= L1_S_V7_XN;

	return pte;
}
static __inline pt_entry_t
L2_L_PROT(int ku, vm_prot_t pr)
{
	pt_entry_t pte;

	if (ku == PTE_USER)
		pte = (pr & PROT_WRITE) ? L2_L_PROT_UW : L2_L_PROT_UR;
	else
		pte = (pr & PROT_WRITE) ? L2_L_PROT_KW : L2_L_PROT_KR;

	if ((pr & PROT_EXEC) == 0)
		pte |= L2_V7_L_XN;

	return pte;
}
static __inline pt_entry_t
L2_S_PROT(int ku, vm_prot_t pr)
{
	pt_entry_t pte;

	if (ku == PTE_USER)
		pte = (pr & PROT_WRITE) ? L2_S_PROT_UW : L2_S_PROT_UR;
	else
		pte = (pr & PROT_WRITE) ? L2_S_PROT_KW : L2_S_PROT_KR;

	if ((pr & PROT_EXEC) == 0)
		pte |= L2_V7_S_XN;

	return pte;
}

static __inline boolean_t
l2pte_is_writeable(pt_entry_t pte, struct pmap *pm)
{
	return (pte & L2_V7_AP(0x4)) == 0;
}
#endif

/*
 * Macros to test if a mapping is mappable with an L1 Section mapping
 * or an L2 Large Page mapping.
 */
#define	L1_S_MAPPABLE_P(va, pa, size)					\
	((((va) | (pa)) & L1_S_OFFSET) == 0 && (size) >= L1_S_SIZE)

#define	L2_L_MAPPABLE_P(va, pa, size)					\
	((((va) | (pa)) & L2_L_OFFSET) == 0 && (size) >= L2_L_SIZE)

#ifndef _LOCORE
/* pmap_prefer bits for VIPT ARMv7 */
#define PMAP_PREFER(fo, ap)	pmap_prefer((fo), (ap))
vaddr_t	pmap_prefer(vaddr_t, vaddr_t);

extern uint32_t pmap_alias_dist;
extern uint32_t pmap_alias_bits;

/* pmap prefer alias alignment. */
#define PMAP_PREFER_ALIGN()	(pmap_alias_dist)
/* pmap prefer offset withing alignment. */
#define PMAP_PREFER_OFFSET(of)						\
    (PMAP_PREFER_ALIGN() == 0 ? 0 : ((of) & (PMAP_PREFER_ALIGN() - 1)))


#endif /* _LOCORE */

#endif /* _KERNEL */

#ifndef _LOCORE
/*
 * pmap-specific data store in the vm_page structure.
 */
struct vm_page_md {
	struct pv_entry *pvh_list;		/* pv_entry list */
	int pvh_attrs;				/* page attributes */
	u_int uro_mappings;
	u_int urw_mappings;
	union {
		u_short s_mappings[2];	/* Assume kernel count <= 65535 */
		u_int i_mappings;
	} k_u;
#define	kro_mappings	k_u.s_mappings[0]
#define	krw_mappings	k_u.s_mappings[1]
#define	k_mappings	k_u.i_mappings
};

#define	VM_MDPAGE_INIT(pg)						\
do {									\
	(pg)->mdpage.pvh_list = NULL;					\
	(pg)->mdpage.pvh_attrs = 0;					\
	(pg)->mdpage.uro_mappings = 0;					\
	(pg)->mdpage.urw_mappings = 0;					\
	(pg)->mdpage.k_mappings = 0;					\
} while (/*CONSTCOND*/0)
#endif /* _LOCORE */

#endif	/* _ARM_PMAP_H_ */
@


1.47
log
@unifdef CPU_XSCALE_PXA2X0, ARM_MMU_XSCALE, ARM_MMU_GENERIC (armv3)
and remove some xscale definitions.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.46 2016/08/26 11:59:04 kettenis Exp $	*/
a377 1
#if defined(CPU_ARMv7)
a378 1
#endif /* CPU_ARMv7 */
a568 1
#ifdef CPU_ARMv7
a570 1
#endif
a583 1
#ifdef CPU_ARMv7
a585 1
#endif
a598 1
#ifdef CPU_ARMv7
a600 1
#endif
@


1.46
log
@Remove the code that switches around MMU domains on armv7.  MMU domains are
basically a relic from the past.  Using them doesn't make a lot of sense
the way our pmaps work.  Support for MMU domains isn't present in
long-descriptor translation table format, so it is clearly on its way out.

Based on a diff from Artituri Alm.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.45 2016/08/19 17:31:04 kettenis Exp $	*/
a363 3
#define l2pte_minidata(pte)	(((pte) & \
				 (L2_B | L2_C | L2_XSCALE_T_TEX(TEX_XSCALE_X)))\
				 == (L2_C | L2_XSCALE_T_TEX(TEX_XSCALE_X)))
d373 1
a373 1
#if (ARM_MMU_GENERIC + ARM_MMU_V7) != 0
d381 1
a381 1
#endif /* (ARM_MMU_GENERIC + ARM_MMU_V7) != 0 */
a386 12
#if ARM_MMU_XSCALE == 1
void	pmap_copy_page_xscale(struct vm_page *, struct vm_page *);
void	pmap_zero_page_xscale(struct vm_page *);

void	pmap_pte_init_xscale(void);

void	xscale_setup_minidata(vaddr_t, vaddr_t, paddr_t);

#define	PMAP_UAREA(va)		pmap_uarea(va)
void	pmap_uarea(vaddr_t);
#endif /* ARM_MMU_XSCALE == 1 */

a444 12
#define	L1_S_PROT_UR_generic	(L1_S_AP(AP_U))
#define	L1_S_PROT_UW_generic	(L1_S_AP(AP_U|AP_W))
#define	L1_S_PROT_KR_generic	(L1_S_AP(0))
#define	L1_S_PROT_KW_generic	(L1_S_AP(AP_W))
#define	L1_S_PROT_MASK_generic	(L1_S_AP(0x03))

#define	L1_S_PROT_UR_xscale	(L1_S_AP(AP_U))
#define	L1_S_PROT_UW_xscale	(L1_S_AP(AP_U|AP_W))
#define	L1_S_PROT_KR_xscale	(L1_S_AP(0))
#define	L1_S_PROT_KW_xscale	(L1_S_AP(AP_W))
#define	L1_S_PROT_MASK_xscale	(L1_S_AP(0x03))

a450 2
#define	L1_S_CACHE_MASK_generic	(L1_S_B|L1_S_C)
#define	L1_S_CACHE_MASK_xscale	(L1_S_B|L1_S_C|L1_S_XSCALE_TEX(TEX_XSCALE_X))
a452 2
#define	L1_S_COHERENT_generic	(L1_S_B|L1_S_C)
#define	L1_S_COHERENT_xscale	(L1_S_B|L1_S_C|L1_S_XSCALE_TEX(TEX_XSCALE_X))
a454 12
#define	L2_L_PROT_KR_generic	(L2_AP(0))
#define	L2_L_PROT_UR_generic	(L2_AP(AP_U))
#define	L2_L_PROT_KW_generic	(L2_AP(AP_W))
#define	L2_L_PROT_UW_generic	(L2_AP(AP_U|AP_W))
#define	L2_L_PROT_MASK_generic	(L2_AP(AP_U|AP_W))

#define	L2_L_PROT_KR_xscale	(L2_AP(0))
#define	L2_L_PROT_UR_xscale	(L2_AP(AP_U))
#define	L2_L_PROT_KW_xscale	(L2_AP(AP_W))
#define	L2_L_PROT_UW_xscale	(L2_AP(AP_U|AP_W))
#define	L2_L_PROT_MASK_xscale	(L2_AP(AP_U|AP_W))

a460 2
#define	L2_L_CACHE_MASK_generic	(L2_B|L2_C)
#define	L2_L_CACHE_MASK_xscale	(L2_B|L2_C|L2_XSCALE_L_TEX(TEX_XSCALE_X))
a462 2
#define	L2_L_COHERENT_generic	(L2_B|L2_C)
#define	L2_L_COHERENT_xscale	(L2_B|L2_C|L2_XSCALE_L_TEX(TEX_XSCALE_X))
a464 12
#define	L2_S_PROT_UR_generic	(L2_AP(AP_U))
#define	L2_S_PROT_UW_generic	(L2_AP(AP_U|AP_W))
#define	L2_S_PROT_KR_generic	(L2_AP(0))
#define	L2_S_PROT_KW_generic	(L2_AP(AP_W))
#define	L2_S_PROT_MASK_generic	(L2_AP(AP_U|AP_W))

#define	L2_S_PROT_UR_xscale	(L2_AP0(AP_U))
#define	L2_S_PROT_UW_xscale	(L2_AP0(AP_U|AP_W))
#define	L2_S_PROT_KR_xscale	(L2_AP0(0))
#define	L2_S_PROT_KW_xscale	(L2_AP0(AP_W))
#define	L2_S_PROT_MASK_xscale	(L2_AP0(AP_U|AP_W))

a470 2
#define	L2_S_CACHE_MASK_generic	(L2_B|L2_C)
#define	L2_S_CACHE_MASK_xscale	(L2_B|L2_C|L2_XSCALE_T_TEX(TEX_XSCALE_X))
a472 2
#define	L2_S_COHERENT_generic	(L2_B|L2_C)
#define	L2_S_COHERENT_xscale	(L2_B|L2_C|L2_XSCALE_T_TEX(TEX_XSCALE_X))
a474 2
#define	L1_S_PROTO_generic	(L1_TYPE_S | L1_S_IMP)
#define	L1_S_PROTO_xscale	(L1_TYPE_S)
a476 2
#define	L1_C_PROTO_generic	(L1_TYPE_C | L1_C_IMP2)
#define	L1_C_PROTO_xscale	(L1_TYPE_C)
a480 2
#define	L2_S_PROTO_generic	(L2_TYPE_S)
#define	L2_S_PROTO_xscale	(L2_TYPE_XSCALE_XS)
a520 66
#elif ARM_MMU_GENERIC == 1
#define	L1_S_PROT_UR		L1_S_PROT_UR_generic
#define	L1_S_PROT_UW		L1_S_PROT_UW_generic
#define	L1_S_PROT_KR		L1_S_PROT_KR_generic
#define	L1_S_PROT_KW		L1_S_PROT_KW_generic
#define	L1_S_PROT_MASK		L1_S_PROT_MASK_generic

#define	L2_L_PROT_UR		L2_L_PROT_UR_generic
#define	L2_L_PROT_UW		L2_L_PROT_UW_generic
#define	L2_L_PROT_KR		L2_L_PROT_KR_generic
#define	L2_L_PROT_KW		L2_L_PROT_KW_generic
#define	L2_L_PROT_MASK		L2_L_PROT_MASK_generic

#define	L2_S_PROT_UR		L2_S_PROT_UR_generic
#define	L2_S_PROT_UW		L2_S_PROT_UW_generic
#define	L2_S_PROT_KR		L2_S_PROT_KR_generic
#define	L2_S_PROT_KW		L2_S_PROT_KW_generic
#define	L2_S_PROT_MASK		L2_S_PROT_MASK_generic

#define	L1_S_CACHE_MASK		L1_S_CACHE_MASK_generic
#define	L2_L_CACHE_MASK		L2_L_CACHE_MASK_generic
#define	L2_S_CACHE_MASK		L2_S_CACHE_MASK_generic

#define	L1_S_COHERENT		L1_S_COHERENT_generic
#define	L2_L_COHERENT		L2_L_COHERENT_generic
#define	L2_S_COHERENT		L2_S_COHERENT_generic

#define	L1_S_PROTO		L1_S_PROTO_generic
#define	L1_C_PROTO		L1_C_PROTO_generic
#define	L2_S_PROTO		L2_S_PROTO_generic

#define	pmap_copy_page(s, d)	pmap_copy_page_generic((s), (d))
#define	pmap_zero_page(d)	pmap_zero_page_generic((d))
#elif ARM_MMU_XSCALE == 1
#define	L1_S_PROT_UR		L1_S_PROT_UR_xscale
#define	L1_S_PROT_UW		L1_S_PROT_UW_xscale
#define	L1_S_PROT_KR		L1_S_PROT_KR_xscale
#define	L1_S_PROT_KW		L1_S_PROT_KW_xscale
#define	L1_S_PROT_MASK		L1_S_PROT_MASK_xscale

#define	L2_L_PROT_UR		L2_L_PROT_UR_xscale
#define	L2_L_PROT_UW		L2_L_PROT_UW_xscale
#define	L2_L_PROT_KR		L2_L_PROT_KR_xscale
#define	L2_L_PROT_KW		L2_L_PROT_KW_xscale
#define	L2_L_PROT_MASK		L2_L_PROT_MASK_xscale

#define	L2_S_PROT_UR		L2_S_PROT_UR_xscale
#define	L2_S_PROT_UW		L2_S_PROT_UW_xscale
#define	L2_S_PROT_KR		L2_S_PROT_KR_xscale
#define	L2_S_PROT_KW		L2_S_PROT_KW_xscale
#define	L2_S_PROT_MASK		L2_S_PROT_MASK_xscale

#define	L1_S_CACHE_MASK		L1_S_CACHE_MASK_xscale
#define	L2_L_CACHE_MASK		L2_L_CACHE_MASK_xscale
#define	L2_S_CACHE_MASK		L2_S_CACHE_MASK_xscale

#define	L1_S_COHERENT		L1_S_COHERENT_xscale
#define	L2_L_COHERENT		L2_L_COHERENT_xscale
#define	L2_S_COHERENT		L2_S_COHERENT_xscale

#define	L1_S_PROTO		L1_S_PROTO_xscale
#define	L1_C_PROTO		L1_C_PROTO_xscale
#define	L2_S_PROTO		L2_S_PROTO_xscale

#define	pmap_copy_page(s, d)	pmap_copy_page_xscale((s), (d))
#define	pmap_zero_page(d)	pmap_zero_page_xscale((d))
d552 2
a553 2
#define	pmap_copy_page(s, d)	pmap_copy_page_v7((s), (d))
#define	pmap_zero_page(d)	pmap_zero_page_v7((d))
@


1.45
log
@Start using to XN flag to enforce that mappings without PROT_EXEC are
non-executable.

ok visa@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.44 2016/08/19 14:05:23 kettenis Exp $	*/
a452 1
#define	PMAP_DOMAIN_USER_V7	0	/* V7 Userland uses a single domain */
@


1.44
log
@Adjust the definitions of L1_S_COHERENT_v7, L2_L_COHERENT_v7 and
L2_S_COHERENT_v7 such that bus_dmamap_sync(9) avoids unnecessary cache
flushes again for DMA'able memory mapped with the BUS_DMA_COHERENT flag.
I broke this in pmap7.c rev 1.35.

ok tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.43 2016/08/19 13:56:08 kettenis Exp $	*/
d706 2
a707 5
	/*
	 * If we set the XN bit, the abort handlers or the vector page
	 * might be marked as such. Needs Debugging.
	 */
	/*
d710 1
a710 1
	*/
d723 2
a724 5
	/*
	 * If we set the XN bit, the abort handlers or the vector page
	 * might be marked as such. Needs Debugging.
	 */
	/*
d727 1
a727 1
	*/
d740 2
a741 5
	/*
	 * If we set the XN bit, the abort handlers or the vector page
	 * might be marked as such. Needs Debugging.
	 */
	/*
d744 1
a744 1
	*/
@


1.43
log
@Use Access Flag to do page reference emulation.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.42 2016/08/10 21:22:43 kettenis Exp $	*/
d485 1
a485 1
#define	L1_S_COHERENT_v7	(L1_S_C|L1_S_V7_TEX_MASK)
d511 1
a511 1
#define	L2_L_COHERENT_v7	(L2_C|L2_V7_L_TEX_MASK)
d537 1
a537 1
#define	L2_S_COHERENT_v7	(L2_C|L2_V7_S_TEX_MASK)
@


1.42
log
@Shuffle armv7 access permission bits around to something that is compatible
with setting the Access Flag Enable bit in the System Control Register.
The new settings mean that read-only userland pages are no longer writable
by the kernel, which is a good thing.  Set the Access Flag Enable bit.

ok patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.41 2016/08/08 14:47:52 kettenis Exp $	*/
d761 1
a761 4
	/* XXX use of L2_V7_S_XN */
	return (pte & L2_S_PROT_MASK & ~L2_V7_S_XN) ==
	    L2_S_PROT(pm == pmap_kernel() ? PTE_KERNEL : PTE_USER,
	              PROT_WRITE);
@


1.41
log
@Mapping non-cachable memory as cachable and subsequently changing the mapping
to non-cachable is retarded.  Fix this by introducing PMAP_NOCACHE and
PMAP_DEVICE flags that can be or'ed into the physical address passed to
pmap_kenter(9), like we have on many of our other architectures.  This way we
can also properly distinguish between device memory and normal (non-cachable)
memory.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.40 2016/03/22 23:35:01 patrick Exp $	*/
d473 1
a473 1
#define	L1_S_PROT_UR_v7		(L1_S_V7_AP(AP_KRWUR))
d499 1
a499 1
#define	L2_L_PROT_UR_v7		(L2_V7_AP(AP_KRWUR))
d525 1
a525 1
#define	L2_S_PROT_UR_v7		(L2_V7_AP(AP_KRWUR))
@


1.40
log
@Remove support for ARM11.  This was the last unused and unmaintained
processor in our code.  Now we're left with only armv7 and XScale for
armish and zaurus.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.39 2016/03/22 23:28:02 patrick Exp $	*/
d182 7
@


1.39
log
@Remove support for ARM10.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.38 2016/03/19 09:36:57 patrick Exp $	*/
a373 3
#if defined(CPU_ARM11)
void	pmap_pte_init_arm11(void);
#endif /* CPU_ARM11 */
@


1.38
log
@Remove support for StrongARM (SA1) and IXP12x0.  Both are ARMv4 and
are not used by any of the arm platforms.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.37 2016/03/18 13:16:02 jsg Exp $	*/
a373 3
#if defined(CPU_ARM10)
void	pmap_pte_init_arm10(void);
#endif /* CPU_ARM10 */
@


1.37
log
@Remove support for ARM9T (armv4t).  Not used by any of the arm platforms.
From Patrick Wildt.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.36 2016/03/18 06:54:21 jsg Exp $	*/
d369 1
a369 1
#if (ARM_MMU_GENERIC + ARM_MMU_SA1 + ARM_MMU_V7) != 0
d383 1
a383 5
#endif /* (ARM_MMU_GENERIC + ARM_MMU_SA1 + ARM_MMU_V7) != 0 */

#if ARM_MMU_SA1 == 1
void	pmap_pte_init_sa1(void);
#endif /* ARM_MMU_SA1 == 1 */
d590 1
a590 1
#elif (ARM_MMU_GENERIC + ARM_MMU_SA1) != 0
@


1.36
log
@Remove support for ARM8, an old armv4 processor without thumb that was
never supported by any arm port and wouldn't have built due to a missing
cpufunc_asm_arm8.S file.

From Patrick Wildt.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.35 2016/02/01 04:28:45 jsg Exp $	*/
a373 3
#if defined(CPU_ARM9)
void	pmap_pte_init_arm9(void);
#endif /* CPU_ARM9 */
@


1.35
log
@Fix the encoding of AP bits for large page second-level
short-descriptors with arm v7 (same as small page encoding, except XN is
in a different bit for the mask).

Expanded version of a diff from Patrick Wildt who also tested and
reviewed this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.34 2015/08/15 22:20:20 miod Exp $	*/
a373 3
#if defined(CPU_ARM8)
void	pmap_pte_init_arm8(void);
#endif
@


1.34
log
@Remove orphaned debug code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.33 2015/06/24 21:26:04 miod Exp $	*/
d434 6
d496 17
a512 5
#define	L2_L_PROT_KR		(L2_AP(0))
#define	L2_L_PROT_UR		(L2_AP(AP_U))
#define	L2_L_PROT_KW		(L2_AP(AP_W))
#define	L2_L_PROT_UW		(L2_AP(AP_U|AP_W))
#define	L2_L_PROT_MASK		(L2_AP(AP_U|AP_W))
d574 6
d607 6
d640 6
d672 6
@


1.33
log
@Remove unused KERNEL_PD_SIZE macro which performs unsafe signed shift of
KERNEL_BASE.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.32 2015/02/15 21:34:33 miod Exp $	*/
a227 1
extern int		pmap_debug_level; /* Only exists if PMAP_DEBUG */
a262 1
void	pmap_debug(int);
@


1.32
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.31 2015/02/07 01:46:27 kettenis Exp $	*/
a367 4

/* Size of the kernel part of the L1 page table */
#define KERNEL_PD_SIZE	\
	(L1_TABLE_SIZE - (KERNEL_BASE >> L1_S_SHIFT) * sizeof(pd_entry_t))
@


1.31
log
@Tedu the old idle page zeroing code.

ok tedu@@, guenther@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.30 2015/01/29 20:10:52 deraadt Exp $	*/
d246 1
a246 1
#define	pmap_remove_holes(map)		do { /* nothing */ } while (0)
@


1.30
log
@remove no-op simple locks
tested by jsg, ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.29 2015/01/01 03:26:13 jsg Exp $	*/
a281 6

/*
 * Special page zero routine for use by the idle loop (no cache cleans). 
 */
boolean_t	pmap_pageidlezero(struct vm_page *);
#define PMAP_PAGEIDLEZERO(pg)	pmap_pageidlezero((pg))
@


1.29
log
@Always drain the write buffer on pte sync.  Needed for Cortex A7/A15
which have integrated L2 so changes will get flushed to memory ASAP.
From Patrick Wildt in Bitrig via rapha@@
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.28 2014/11/16 12:30:56 deraadt Exp $	*/
a71 2
#include <sys/lock.h>		/* struct simplelock */ 

a176 1
	simple_lock_data_t	pm_lock;
a781 1
	struct simplelock pvh_slock;		/* lock on this head */
a796 1
	simple_lock_init(&(pg)->mdpage.pvh_slock);			\
@


1.28
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.27 2014/10/07 10:10:58 jsg Exp $	*/
d331 1
a333 1
		cpu_drain_writebuf();					\
d346 1
a348 1
		cpu_drain_writebuf();					\
@


1.27
log
@Use L1_S_V7_AP instead of L1_S_AP for the v7 case
otherwise the high bit of AP will end up in TEX.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.26 2014/10/07 07:14:55 jsg Exp $	*/
d685 1
a685 1
		pte = (pr & VM_PROT_WRITE) ? L1_S_PROT_UW : L1_S_PROT_UR;
d687 1
a687 1
		pte = (pr & VM_PROT_WRITE) ? L1_S_PROT_KW : L1_S_PROT_KR;
d693 1
a693 1
	if ((pr & VM_PROT_EXECUTE) == 0)
d705 1
a705 1
		pte = (pr & VM_PROT_WRITE) ? L2_L_PROT_UW : L2_L_PROT_UR;
d707 1
a707 1
		pte = (pr & VM_PROT_WRITE) ? L2_L_PROT_KW : L2_L_PROT_KR;
d713 1
a713 1
	if ((pr & VM_PROT_EXECUTE) == 0)
d725 1
a725 1
		pte = (pr & VM_PROT_WRITE) ? L2_S_PROT_UW : L2_S_PROT_UR;
d727 1
a727 1
		pte = (pr & VM_PROT_WRITE) ? L2_S_PROT_KW : L2_S_PROT_KR;
d733 1
a733 1
	if ((pr & VM_PROT_EXECUTE) == 0)
d746 1
a746 1
	              VM_PROT_WRITE);
@


1.26
log
@Correct the l1 pte permission bits for armv7.  Problem pointed out
by Patrick Wildt who made a similiar change in Bitrig.

ok miod@@ rapha@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.25 2014/01/30 18:16:41 miod Exp $	*/
d491 5
a495 5
#define	L1_S_PROT_UR_v7		(L1_S_AP(AP_KRWUR))
#define	L1_S_PROT_UW_v7		(L1_S_AP(AP_KRWURW))
#define	L1_S_PROT_KR_v7		(L1_S_AP(AP_V7_KR))
#define	L1_S_PROT_KW_v7		(L1_S_AP(AP_KRW))
#define	L1_S_PROT_MASK_v7	(L1_S_AP(0x07))
@


1.25
log
@Move declaration of struct vm_page_md from <machine/vmparam.h> to
<machine/pmap.h> where it belongs, and compensate in <uvm/uvm_extern.h>
by including <uvm/uvm_pmap.h> before <uvm/uvm_page.h>. Tested on all
MACHINE_ARCH but amd64 and i386 (and hppa64).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.24 2013/05/18 18:06:05 patrick Exp $	*/
d443 6
d479 17
a495 3
#define	L1_S_PROT_U		(L1_S_AP(AP_U))
#define	L1_S_PROT_W		(L1_S_AP(AP_W))
#define	L1_S_PROT_MASK		(L1_S_PROT_U|L1_S_PROT_W)
d565 6
d592 6
d619 6
d646 6
d678 18
a695 2
#define	L1_S_PROT(ku, pr)	((((ku) == PTE_USER) ? L1_S_PROT_U : 0) | \
				 (((pr) & VM_PROT_WRITE) ? L1_S_PROT_W : 0))
d697 2
a698 1
#ifndef _LOCORE
@


1.24
log
@Modify pmap to work with the pmap header used on armv7. Merge both
headers so that we only need one of them.

"Go for it." miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap7.h,v 1.2 2013/05/09 20:07:26 patrick Exp $	*/
d72 2
d717 30
@


1.23
log
@On ARMv7 we can't use the cache mask to check for coherency.
Therefore we add new macros to be able to check for it properly.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.22 2013/04/26 11:51:22 patrick Exp $	*/
d329 3
a331 1
	if (PMAP_NEEDS_PTE_SYNC)					\
d333 7
d345 2
d349 6
d358 1
a358 1
#define	l1pte_valid(pde)	((pde) != 0)
d364 1
a364 1
#define	l2pte_valid(pte)	((pte) != 0)
a375 3
#define	pmap_pte_v(pte)		l2pte_valid(*(pte))
#define	pmap_pte_pa(pte)	l2pte_pa(*(pte))

d382 1
a382 1
#if (ARM_MMU_GENERIC + ARM_MMU_SA1) != 0
d402 1
a402 1
#endif /* (ARM_MMU_GENERIC + ARM_MMU_SA1) != 0 */
a408 3
void	pmap_copy_page_v7(struct vm_page *, struct vm_page *);
void	pmap_zero_page_v7(struct vm_page *);

d441 4
a444 2
extern pt_entry_t		pte_l2_s_prot_u;
extern pt_entry_t		pte_l2_s_prot_w;
d477 1
a477 1
#define	L1_S_CACHE_MASK_v7	(L1_S_B|L1_S_C|L1_S_V7_TEX(TEX_V7_X))
d481 1
a481 1
#define	L1_S_COHERENT_v7	(L1_S_C|L1_S_V7_TEX(TEX_V7_X))
d483 5
a487 3
#define	L2_L_PROT_U		(L2_AP(AP_U))
#define	L2_L_PROT_W		(L2_AP(AP_W))
#define	L2_L_PROT_MASK		(L2_L_PROT_U|L2_L_PROT_W)
d491 1
a491 1
#define	L2_L_CACHE_MASK_v7	(L2_B|L2_C|L2_V7_L_TEX(TEX_V7_X))
d495 1
a495 1
#define	L2_L_COHERENT_v7	(L2_C|L2_V7_L_TEX(TEX_V7_X))
d497 17
a513 11
#define	L2_S_PROT_U_generic	(L2_AP(AP_U))
#define	L2_S_PROT_W_generic	(L2_AP(AP_W))
#define	L2_S_PROT_MASK_generic	(L2_S_PROT_U|L2_S_PROT_W)

#define	L2_S_PROT_U_xscale	(L2_AP0(AP_U))
#define	L2_S_PROT_W_xscale	(L2_AP0(AP_W))
#define	L2_S_PROT_MASK_xscale	(L2_S_PROT_U|L2_S_PROT_W)

#define	L2_S_PROT_U_v7		(L2_AP0(AP_U))
#define	L2_S_PROT_W_v7		(L2_AP0(AP_W))
#define	L2_S_PROT_MASK_v7	(L2_S_PROT_U|L2_S_PROT_W)
d517 1
a517 1
#define	L2_S_CACHE_MASK_v7	(L2_B|L2_C)
d521 1
a521 1
#define	L2_S_COHERENT_v7	(L2_C)
d543 4
a546 2
#define	L2_S_PROT_U		pte_l2_s_prot_u
#define	L2_S_PROT_W		pte_l2_s_prot_w
d564 4
a567 2
#define	L2_S_PROT_U		L2_S_PROT_U_generic
#define	L2_S_PROT_W		L2_S_PROT_W_generic
d585 4
a588 2
#define	L2_S_PROT_U		L2_S_PROT_U_xscale
#define	L2_S_PROT_W		L2_S_PROT_W_xscale
d606 4
a609 2
#define	L2_S_PROT_U		L2_S_PROT_U_v7
#define	L2_S_PROT_W		L2_S_PROT_W_v7
d635 5
a639 2
#define	L2_L_PROT(ku, pr)	((((ku) == PTE_USER) ? L2_L_PROT_U : 0) | \
				 (((pr) & VM_PROT_WRITE) ? L2_L_PROT_W : 0))
d641 45
a685 2
#define	L2_S_PROT(ku, pr)	((((ku) == PTE_USER) ? L2_S_PROT_U : 0) | \
				 (((pr) & VM_PROT_WRITE) ? L2_S_PROT_W : 0))
@


1.22
log
@ARMv7 userland uses a single domain. This define is already used in pmap7.

ok bmercer@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.21 2011/11/05 18:11:26 miod Exp $	*/
d426 4
d466 4
d478 4
d498 4
d530 4
d549 4
d568 4
d586 4
@


1.21
log
@Remove incorrect logic leading to skip page table flushes because
PMAP_NEEDS_PTE_SYNC would get defined to zero on kernels lacking StrongArm
support.

discussed with and ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.20 2011/09/21 10:12:59 miod Exp $	*/
d446 1
@


1.20
log
@Rename pmap_procwr() to pmap_proc_iflush() to get the intended behaviour.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.19 2011/09/20 22:02:13 miod Exp $	*/
a323 17
/*
 * StrongARM SA-1 caches do not have a write-through mode.  So, on these,
 * we need to do PTE syncs.  If only SA-1 is configured, then evaluate
 * this at compile time.
 */
#if (ARM_MMU_SA1 == 1) && (ARM_NMMUS == 1)
#define	PMAP_NEEDS_PTE_SYNC	1
#define	PMAP_INCLUDE_PTE_SYNC
#elif (ARM_MMU_SA1 == 0)
#define	PMAP_NEEDS_PTE_SYNC	0
#endif

/*
 * Provide a fallback in case we were not able to determine it at
 * compile-time.
 */
#ifndef PMAP_NEEDS_PTE_SYNC
a325 1
#endif
@


1.19
log
@Late spring cleaning of the arm code for old dusty bits we do not want to
keep:
- remove bootconfig parameter passing feature (unused).
- unifdef __PROG32 and remove all remains of arm26 code.
- remove ARMFPE support (unused).
- remove support for ARM2, ARM2AS, ARM3, ARM6, ARM7, ARM7TDMI and StrongARM
  processor families, and the related silicon bug workarounds (especially
  the SA-110 STM^ bug).
- remove cpu_functions no longer necessary after previous removals.
- remove ARM32_DISABLE_ALIGNMENT_FAULTS option (unused).
- make FIQ support conditional on option FIQ (unused, but may be eventually).

Discussed with drahn@@ and jasper@@ long ago, I was sitting on this commit for
no good reason.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.18 2011/04/28 20:50:58 ariane Exp $	*/
a245 1
#define pmap_proc_iflush(p, va, len)	do { /* nothing */ } while (0)
a251 1
void	pmap_procwr(struct proc *, vaddr_t, int);
a252 1
boolean_t pmap_extract(pmap_t, vaddr_t, paddr_t *);
a254 1
#define	PMAP_NEED_PROCWR
@


1.18
log
@Expose pmap_prefer parameters.
This enables future uvm_map allocator to behave intelligently.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.17 2011/03/23 16:54:34 pirofti Exp $	*/
a461 5

/*
 * tell MI code that the cache is virtually-indexed *and* virtually-tagged.
 */
#define PMAP_CACHE_VIVT
@


1.17
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.16 2011/01/04 21:11:41 miod Exp $	*/
d623 8
@


1.16
log
@Introduce pmap_uncache_page(), created from the guts of pmap_kenter_cache()
to make a page uncached and maintain this both at the pte and pv list level,
and make pmap_kenter_cache() use it.
ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.15 2010/12/26 15:40:59 miod Exp $	*/
d69 2
a70 2
#ifndef	_ARM32_PMAP_H_
#define	_ARM32_PMAP_H_
d627 1
a627 1
#endif	/* _ARM32_PMAP_H_ */
@


1.15
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.14 2010/12/06 20:57:15 miod Exp $	*/
d256 1
@


1.14
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.13 2010/11/27 20:45:27 miod Exp $	*/
a244 2

#define pmap_phys_address(ppn)		(ptoa(ppn))
@


1.13
log
@Get rid of the global pmap list and related debug code. While there, merge
pmap_pinit() into pmap_create(). Help and ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12 2010/07/01 22:40:10 drahn Exp $	*/
d620 1
a620 1
void	pmap_prefer(vaddr_t, vaddr_t *);
@


1.12
log
@Add more support bits for ARMv7, including frame for VIPT (pmap_prefer).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.11 2009/05/24 04:56:19 drahn Exp $	*/
a179 1
	LIST_ENTRY(pmap)	pm_list;
@


1.11
log
@Improve the ARMv7 support, still work in progress.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.10 2009/05/08 02:57:32 drahn Exp $	*/
d617 9
@


1.10
log
@Pieces of arm11 and armv7 support for newer cpus. This is work in progress
and not complete.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.9 2007/10/10 15:53:51 art Exp $	*/
d418 7
d488 1
d496 1
d506 4
d512 1
d516 1
d520 1
d526 1
d578 15
@


1.9
log
@Make context switching much more MI:
 - Move the functionality of choosing a process from cpu_switch into
   a much simpler function: cpu_switchto. Instead of having the locore
   code walk the run queues, let the MI code choose the process we
   want to run and only implement the context switching itself in MD
   code.
 - Let MD context switching run without worrying about spls or locks.
 - Instead of having the idle loop implemented with special contexts
   in MD code, implement one idle proc for each cpu. make the idle
   loop MI with MD hooks.
 - Change the proc lists from the old style vax queues to TAILQs.
 - Change the sleep queue from vax queues to TAILQs. This makes
   wakeup() go from O(n^2) to O(n)

there will be some MD fallout, but it will be fixed shortly.
There's also a few cleanups to be done after this.

deraadt@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.8 2007/09/10 18:49:44 miod Exp $	*/
d276 3
d406 6
@


1.8
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.7 2007/04/21 19:26:04 miod Exp $	*/
d244 2
a245 1
#define	pmap_copy(dp, sp, da, l, sa)	/* nothing */
d249 2
a250 2
#define pmap_proc_iflush(p, va, len)	/* nothing */
#define pmap_unuse_final(p)		/* nothing */
@


1.7
log
@No more vtophys() on arm.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.6 2006/05/27 20:36:05 miod Exp $	*/
d250 1
@


1.6
log
@Nuke defopt remains. Tested on cats and zaurus.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.5 2006/05/26 17:11:41 miod Exp $	*/
a312 14
 * Virtual address to physical address
 */
static __inline paddr_t
vtophys(vaddr_t va)
{
	paddr_t pa;

	if (pmap_extract(pmap_kernel(), va, &pa) == FALSE)
		return (0);	/* XXXSCW: Panic? */

	return (pa);
}

/*
a573 5

/*
 * Hooks for the pool allocator.
 */
#define	POOL_VTOPHYS(va)	vtophys((vaddr_t) (va))
@


1.5
log
@No need to have a struct uvm_object in each pmap when all you want is a
lock and a reference count. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.4 2005/11/09 18:08:37 martin Exp $	*/
d338 1
a338 1
#if defined(_KERNEL_OPT)
a349 1
#endif /* _KERNEL_OPT */
@


1.4
log
@use atop() and ptoa()

tested by jolan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.3 2004/08/06 22:39:12 deraadt Exp $	*/
a77 1
#include <uvm/uvm_object.h>
d176 2
a177 2
	struct uvm_object	pm_obj;
#define	pm_lock pm_obj.vmobjlock
@


1.3
log
@rename sparc kill_user_windows() to pmap_unuse_final().  provide empty stubs
on all other architectures.  remove last architecture dependent #ifdef from
uvm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.2 2004/05/20 09:20:41 kettenis Exp $	*/
d247 1
a247 1
#define pmap_phys_address(ppn)		(arm_ptob((ppn)))
@


1.2
log
@Properly flush instruction cache for ptrace(PT_WRTIE_{DI}, ...) on powerpc
and m68k.
ok drahn@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.1 2004/02/01 05:09:49 drahn Exp $	*/
d250 1
@


1.1
log
@Arm port, NetBSD codebase stripped down, 32bit only support.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.2 2004/01/29 16:17:16 drahn Exp $	*/
d248 2
@


1.1.2.1
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.1.2.2
log
@Merge with the trunk
@
text
@a248 2
#define pmap_proc_iflush(p, va, len)	/* nothing */

@


