head	1.85;
access;
symbols
	OPENBSD_6_1:1.85.0.4
	OPENBSD_6_1_BASE:1.85
	OPENBSD_6_0:1.83.0.2
	OPENBSD_6_0_BASE:1.83
	OPENBSD_5_9:1.82.0.2
	OPENBSD_5_9_BASE:1.82
	OPENBSD_5_8:1.81.0.4
	OPENBSD_5_8_BASE:1.81
	OPENBSD_5_7:1.79.0.2
	OPENBSD_5_7_BASE:1.79
	OPENBSD_5_6:1.78.0.4
	OPENBSD_5_6_BASE:1.78
	OPENBSD_5_5:1.75.0.4
	OPENBSD_5_5_BASE:1.75
	OPENBSD_5_4:1.71.0.2
	OPENBSD_5_4_BASE:1.71
	OPENBSD_5_3:1.70.0.2
	OPENBSD_5_3_BASE:1.70
	OPENBSD_5_2:1.68.0.4
	OPENBSD_5_2_BASE:1.68
	OPENBSD_5_1_BASE:1.68
	OPENBSD_5_1:1.68.0.2
	OPENBSD_5_0:1.63.0.4
	OPENBSD_5_0_BASE:1.63
	OPENBSD_4_9:1.63.0.2
	OPENBSD_4_9_BASE:1.63
	OPENBSD_4_8:1.53.0.2
	OPENBSD_4_8_BASE:1.53
	OPENBSD_4_7:1.49.0.2
	OPENBSD_4_7_BASE:1.49
	OPENBSD_4_6:1.48.0.4
	OPENBSD_4_6_BASE:1.48
	OPENBSD_4_5:1.45.0.6
	OPENBSD_4_5_BASE:1.45
	OPENBSD_4_4:1.45.0.2
	OPENBSD_4_4_BASE:1.45
	OPENBSD_4_3:1.43.0.2
	OPENBSD_4_3_BASE:1.43
	OPENBSD_4_2:1.31.0.2
	OPENBSD_4_2_BASE:1.31
	OPENBSD_4_1:1.29.0.2
	OPENBSD_4_1_BASE:1.29
	OPENBSD_4_0:1.28.0.2
	OPENBSD_4_0_BASE:1.28
	OPENBSD_3_9:1.21.0.2
	OPENBSD_3_9_BASE:1.21
	OPENBSD_3_8:1.9.0.2
	OPENBSD_3_8_BASE:1.9
	OPENBSD_3_7:1.6.0.2
	OPENBSD_3_7_BASE:1.6
	OPENBSD_3_6:1.4.0.2
	OPENBSD_3_6_BASE:1.4;
locks; strict;
comment	@ * @;


1.85
date	2016.10.19.08.28.20;	author guenther;	state Exp;
branches;
next	1.84;
commitid	VoR9X3uHTxRSYC5r;

1.84
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.83;
commitid	RlO92XR575sygHqm;

1.83
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.82;
commitid	N0upL0onl7Raz5yi;

1.82
date	2015.09.08.21.28.36;	author kettenis;	state Exp;
branches;
next	1.81;
commitid	WSD3bUAMn8qMj0PM;

1.81
date	2015.07.29.17.54.35;	author miod;	state Exp;
branches;
next	1.80;
commitid	60BlIjhXbNna4zvA;

1.80
date	2015.07.25.20.45.05;	author miod;	state Exp;
branches;
next	1.79;
commitid	qIXrw7Yr4QCeqfDT;

1.79
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.78;
commitid	yv0ECmCdICvq576h;

1.78
date	2014.06.09.14.33.20;	author miod;	state Exp;
branches;
next	1.77;
commitid	T9yxec6ApVOognSZ;

1.77
date	2014.05.17.12.22.09;	author miod;	state Exp;
branches;
next	1.76;

1.76
date	2014.05.08.22.17.33;	author miod;	state Exp;
branches;
next	1.75;

1.75
date	2013.11.16.18.45.20;	author miod;	state Exp;
branches;
next	1.74;

1.74
date	2013.11.03.09.42.55;	author miod;	state Exp;
branches;
next	1.73;

1.73
date	2013.11.02.23.10.30;	author miod;	state Exp;
branches;
next	1.72;

1.72
date	2013.08.26.20.29.34;	author miod;	state Exp;
branches;
next	1.71;

1.71
date	2013.05.17.22.33.25;	author miod;	state Exp;
branches;
next	1.70;

1.70
date	2013.02.19.21.02.06;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2013.02.14.05.50.49;	author miod;	state Exp;
branches;
next	1.68;

1.68
date	2011.10.25.18.38.06;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2011.10.09.17.08.22;	author miod;	state Exp;
branches;
next	1.66;

1.66
date	2011.10.09.17.07.37;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2011.10.09.17.04.07;	author miod;	state Exp;
branches;
next	1.64;

1.64
date	2011.10.09.17.01.34;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2011.01.05.22.20.22;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2011.01.05.22.18.46;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2011.01.05.22.16.16;	author miod;	state Exp;
branches;
next	1.60;

1.60
date	2011.01.05.22.14.29;	author miod;	state Exp;
branches;
next	1.59;

1.59
date	2011.01.02.13.40.07;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2010.12.31.21.38.08;	author miod;	state Exp;
branches;
next	1.57;

1.57
date	2010.12.31.21.22.33;	author miod;	state Exp;
branches;
next	1.56;

1.56
date	2010.12.31.20.54.21;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2010.12.26.22.18.20;	author miod;	state Exp;
branches;
next	1.54;

1.54
date	2010.12.23.19.53.29;	author miod;	state Exp;
branches;
next	1.53;

1.53
date	2010.06.27.12.41.23;	author miod;	state Exp;
branches;
next	1.52;

1.52
date	2010.06.26.23.24.43;	author guenther;	state Exp;
branches;
next	1.51;

1.51
date	2010.05.09.15.46.17;	author jasper;	state Exp;
branches;
next	1.50;

1.50
date	2010.05.02.22.01.47;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2009.09.27.19.16.10;	author miod;	state Exp;
branches;
next	1.48;

1.48
date	2009.05.02.14.32.29;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2009.04.19.17.56.13;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2009.03.04.19.39.05;	author miod;	state Exp;
branches;
next	1.45;

1.45
date	2008.06.14.10.55.20;	author mk;	state Exp;
branches;
next	1.44;

1.44
date	2008.06.10.21.12.14;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2007.12.31.09.23.53;	author martin;	state Exp;
branches;
next	1.42;

1.42
date	2007.12.20.21.15.51;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2007.12.02.21.22.19;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2007.11.24.11.13.56;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2007.11.22.05.47.46;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2007.11.22.05.46.07;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2007.11.21.19.41.43;	author miod;	state Exp;
branches;
next	1.36;

1.36
date	2007.11.20.21.54.58;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2007.11.20.21.53.25;	author miod;	state Exp;
branches;
next	1.34;

1.34
date	2007.11.20.21.48.17;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2007.11.14.23.12.46;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2007.11.06.21.45.46;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2007.05.27.20.59.25;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2007.03.21.19.32.29;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2007.02.11.12.49.37;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2006.06.01.06.28.11;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2006.06.01.06.25.51;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2006.05.25.21.37.45;	author miod;	state Exp;
branches;
next	1.25;

1.25
date	2006.05.20.22.33.17;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2006.05.20.15.41.09;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2006.05.06.16.59.28;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2006.04.26.20.38.37;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2005.12.11.21.45.30;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2005.12.03.19.06.11;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2005.12.03.18.54.34;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2005.12.03.14.30.06;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2005.12.01.22.24.52;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2005.11.25.22.13.50;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2005.11.03.21.27.33;	author martin;	state Exp;
branches;
next	1.14;

1.14
date	2005.10.13.19.48.33;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2005.10.12.19.05.44;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2005.09.25.20.55.14;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2005.09.15.21.07.05;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2005.09.06.19.21.57;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2005.05.22.19.40.51;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2005.04.04.11.45.33;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2005.04.04.11.44.50;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2004.10.01.18.58.09;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2004.09.30.21.48.56;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2004.08.04.15.54.38;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2004.08.02.08.34.59;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2004.07.26.11.08.20;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2004.07.25.11.06.42;	author miod;	state Exp;
branches;
next	;


desc
@@


1.85
log
@Change pmap_proc_iflush() to take a process instead of a proc
powerpc: rename second argument of pmap_proc_iflush() to match other archs

ok kettenis@@
@
text
@/*	$OpenBSD: pmap.c,v 1.84 2016/09/15 02:00:17 dlg Exp $	*/

/*
 * Copyright (c) 2001-2004, 2010, Miodrag Vallat.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * Copyright (c) 1998-2001 Steve Murphree, Jr.
 * Copyright (c) 1996 Nivas Madhur
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Nivas Madhur.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 */
/*
 * Mach Operating System
 * Copyright (c) 1991 Carnegie Mellon University
 * Copyright (c) 1991 OMRON Corporation
 * All Rights Reserved.
 *
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/pool.h>

#include <uvm/uvm.h>

#include <machine/asm_macro.h>
#include <machine/cmmu.h>
#include <machine/cpu.h>
#include <machine/pmap_table.h>
#ifdef M88100
#include <machine/m8820x.h>
#endif
#ifdef M88110
#include <machine/m88110.h>
#endif

/*
 * VM externals
 */
extern paddr_t last_addr;
vaddr_t avail_start;
vaddr_t avail_end;
vaddr_t virtual_avail = VM_MIN_KERNEL_ADDRESS;
vaddr_t virtual_end = VM_MAX_KERNEL_ADDRESS;


#ifdef	PMAPDEBUG
/*
 * conditional debugging
 */
#define CD_ACTIVATE	0x00000001	/* pmap_activate */
#define CD_KMAP		0x00000002	/* pmap_expand_kmap */
#define CD_MAP		0x00000004	/* pmap_map */
#define CD_CACHE	0x00000008	/* pmap_cache_ctrl */
#define CD_INIT		0x00000010	/* pmap_init */
#define CD_CREAT	0x00000020	/* pmap_create */
#define CD_DESTR	0x00000040	/* pmap_destroy */
#define CD_RM		0x00000080	/* pmap_remove / pmap_kremove */
#define CD_RMPG		0x00000100	/* pmap_remove_page */
#define CD_EXP		0x00000200	/* pmap_expand */
#define CD_ENT		0x00000400	/* pmap_enter / pmap_kenter_pa */
#define CD_COL		0x00000800	/* pmap_collect */
#define CD_CBIT		0x00001000	/* pmap_changebit */
#define CD_TBIT		0x00002000	/* pmap_testbit */
#define CD_USBIT	0x00004000	/* pmap_unsetbit */
#define	CD_COPY		0x00008000	/* pmap_copy_page */
#define	CD_ZERO		0x00010000	/* pmap_zero_page */
#define	CD_BOOT		0x00020000	/* pmap_bootstrap */
#define CD_ALL		0xffffffff

int pmap_debug = CD_BOOT | CD_KMAP | CD_MAP;

#define	DPRINTF(flg, stmt) \
do { \
	if (pmap_debug & (flg)) \
		printf stmt; \
} while (0)

#else

#define	DPRINTF(flg, stmt) do { } while (0)

#endif	/* PMAPDEBUG */

struct pool pmappool, pvpool;
struct pmap kernel_pmap_store;

/*
 * Cacheability settings for page tables and kernel data.
 */

apr_t	pte_cmode = CACHE_WT;
apr_t	kernel_apr = CACHE_GLOBAL | CACHE_DFL | APR_V;
apr_t	userland_apr = CACHE_GLOBAL | CACHE_DFL | APR_V;

#define	KERNEL_APR_CMODE	(kernel_apr & (CACHE_MASK & ~CACHE_GLOBAL))
#define	USERLAND_APR_CMODE	(userland_apr & (CACHE_MASK & ~CACHE_GLOBAL))

/*
 * Address and size of the temporary firmware mapping
 */
paddr_t	s_firmware;
psize_t	l_firmware;

/*
 * Current BATC values.
 */

batc_t global_dbatc[BATC_MAX];
batc_t global_ibatc[BATC_MAX];

/*
 * Internal routines
 */
void		 pmap_changebit(struct vm_page *, int, int);
void		 pmap_clean_page(paddr_t);
pt_entry_t	*pmap_expand(pmap_t, vaddr_t, int);
pt_entry_t	*pmap_expand_kmap(vaddr_t, int);
void		 pmap_map(paddr_t, psize_t, vm_prot_t, u_int, boolean_t);
pt_entry_t	*pmap_pte(pmap_t, vaddr_t);
void		 pmap_remove_page(struct vm_page *);
void		 pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *,
		    struct vm_page *, boolean_t);
void		 pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
boolean_t	 pmap_testbit(struct vm_page *, int);

static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pv_ent;
}

/*
 * PTE routines
 */

#define	m88k_protection(prot)	((prot) & PROT_WRITE ? PG_RW : PG_RO)
#define	pmap_pte_w(pte)		(*(pte) & PG_W)

#define SDTENT(pm, va)		((pm)->pm_stab + SDTIDX(va))

/*
 * [INTERNAL]
 * Return the address of the pte for `va' within the page table pointed
 * to by the segment table entry `sdt'. Assumes *sdt is a valid segment
 * table entry.
 */
static __inline__
pt_entry_t *
sdt_pte(sdt_entry_t *sdt, vaddr_t va)
{
	return (pt_entry_t *)(*sdt & PG_FRAME) + PDTIDX(va);
}

/*
 * [INTERNAL]
 * Return the address of the pte for `va' in `pmap'. NULL if there is no
 * page table for `va'.
 */
pt_entry_t *
pmap_pte(pmap_t pmap, vaddr_t va)
{
	sdt_entry_t *sdt;

	sdt = SDTENT(pmap, va);
	if (!SDT_VALID(sdt))
		return NULL;

	return sdt_pte(sdt, va);
}

/*
 * [MI]
 * Checks how virtual address `va' would translate with `pmap' as the active
 * pmap. Returns TRUE and matching physical address in `pap' (if not NULL) if
 * translation is possible, FAILS otherwise.
 */
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
{
	paddr_t pa;
	uint32_t ti;
	int rv;

	rv = pmap_translation_info(pmap, va, &pa, &ti);
	if (rv == PTI_INVALID)
		return FALSE;
	else {
		if (pap != NULL)
			*pap = pa;
		return TRUE;
	}
}

/*
 * [MD PUBLIC]
 * Checks how virtual address `va' would translate with `pmap' as the active
 * pmap. Returns a PTI_xxx constant indicating which translation hardware
 * would perform the translation; if not PTI_INVALID, the matching physical
 * address is returned into `pap', and cacheability of the mapping is
 * returned into `ti'.
 */
int
pmap_translation_info(pmap_t pmap, vaddr_t va, paddr_t *pap, uint32_t *ti)
{
	pt_entry_t *pte;
	vaddr_t var;
	uint batcno;
	int s;
	int rv;

	/*
	 * Check for a BATC translation first.
	 * We only use BATC for supervisor mappings (i.e. pmap_kernel()).
	 */

	if (pmap == pmap_kernel()) {
		/*
		 * 88100-based designs (with 8820x CMMUs) have two hardwired
		 * BATC entries which map the upper 1MB (so-called
		 * `utility space') 1:1 in supervisor space.
		 */
#ifdef M88100
		if (CPU_IS88100) {
			if (va >= BATC9_VA) {
				*pap = va;
				*ti = 0;
				if (BATC9 & BATC_INH)
					*ti |= CACHE_INH;
				if (BATC9 & BATC_GLOBAL)
					*ti |= CACHE_GLOBAL;
				if (BATC9 & BATC_WT)
					*ti |= CACHE_WT;
				return PTI_BATC;
			}
			if (va >= BATC8_VA) {
				*pap = va;
				*ti = 0;
				if (BATC8 & BATC_INH)
					*ti |= CACHE_INH;
				if (BATC8 & BATC_GLOBAL)
					*ti |= CACHE_GLOBAL;
				if (BATC8 & BATC_WT)
					*ti |= CACHE_WT;
				return PTI_BATC;
			}
		}
#endif

		/*
		 * Now try all DBATC entries.
		 * Note that pmap_translation_info() might be invoked (via
		 * pmap_extract() ) for instruction faults; we *rely* upon
		 * the fact that all executable mappings covered by IBATC
		 * will be:
		 * - read-only, with no RO->RW upgrade allowed
		 * - dual mapped by ptes, so that pmap_extract() can still
		 *   return a meaningful result.
		 * Should this ever change, some kernel interfaces will need
		 * to be made aware of (and carry on to callees) whether the
		 * address should be resolved as an instruction or data
		 * address.
		 */
		var = trunc_batc(va);
		for (batcno = 0; batcno < BATC_MAX; batcno++) {
			vaddr_t batcva;
			paddr_t batcpa;
			batc_t batc;

			batc = global_dbatc[batcno];
			if ((batc & BATC_V) == 0)
				continue;

			batcva = (batc << (BATC_BLKSHIFT - BATC_VSHIFT)) &
			    ~BATC_BLKMASK;
			if (batcva == var) {
				batcpa = (batc <<
				    (BATC_BLKSHIFT - BATC_PSHIFT)) &
				    ~BATC_BLKMASK;
				*pap = batcpa + (va - var);
				*ti = 0;
				if (batc & BATC_INH)
					*ti |= CACHE_INH;
				if (batc & BATC_GLOBAL)
					*ti |= CACHE_GLOBAL;
				if (batc & BATC_WT)
					*ti |= CACHE_WT;
				return PTI_BATC;
			}
		}
	}

	/*
	 * Check for a regular PTE translation.
	 */

	s = splvm();
	pte = pmap_pte(pmap, va);
	if (pte != NULL && PDT_VALID(pte)) {
		*pap = ptoa(PG_PFNUM(*pte)) | (va & PAGE_MASK);
		*ti = (*pte | pmap->pm_apr) & CACHE_MASK;
		rv = PTI_PTE;
	} else
		rv = PTI_INVALID;

	splx(s);

	return rv;
}

/*
 * TLB (ATC) routines
 */

void		 tlb_flush(pmap_t, vaddr_t, pt_entry_t);
void		 tlb_kflush(vaddr_t, pt_entry_t);

/*
 * [INTERNAL]
 * Update translation cache entry for `va' in `pmap' to `pte'. May flush
 * instead of updating.
 */
void
tlb_flush(pmap_t pmap, vaddr_t va, pt_entry_t pte)
{
	struct cpu_info *ci;
	boolean_t kernel = pmap == pmap_kernel();
#ifdef MULTIPROCESSOR
	CPU_INFO_ITERATOR cpu;
#endif

#ifdef MULTIPROCESSOR
	CPU_INFO_FOREACH(cpu, ci)
#else
	ci = curcpu();
#endif
	{
		if (kernel)
			cmmu_tlbis(ci->ci_cpuid, va, pte);
		else if (pmap == ci->ci_curpmap)
			cmmu_tlbiu(ci->ci_cpuid, va, pte);
	}
}

/*
 * [INTERNAL]
 * Update translation cache entry for `va' in pmap_kernel() to `pte'. May
 * flush insteai of updating.
 */
void
tlb_kflush(vaddr_t va, pt_entry_t pte)
{
	struct cpu_info *ci;
#ifdef MULTIPROCESSOR
	CPU_INFO_ITERATOR cpu;
#endif

#ifdef MULTIPROCESSOR		/* { */
	CPU_INFO_FOREACH(cpu, ci) {
		cmmu_tlbis(ci->ci_cpuid, va, pte);
	}
#else	/* MULTIPROCESSOR */	/* } { */
	ci = curcpu();
	cmmu_tlbis(ci->ci_cpuid, va, pte);
#endif	/* MULTIPROCESSOR */	/* } */
}

/*
 * [MI]
 * Activate the pmap of process `p'.
 */
void
pmap_activate(struct proc *p)
{
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	struct cpu_info *ci = curcpu();

	DPRINTF(CD_ACTIVATE, ("pmap_activate(%p) pmap %p\n", p, pmap));

	if (pmap == pmap_kernel()) {
		ci->ci_curpmap = NULL;
	} else {
		if (pmap != ci->ci_curpmap) {
			cmmu_set_uapr(pmap->pm_apr);
			cmmu_tlbia(ci->ci_cpuid);
			ci->ci_curpmap = pmap;
		}
	}
}

/*
 * [MI]
 * Deactivates the pmap of process `p'.
 */
void
pmap_deactivate(struct proc *p)
{
	struct cpu_info *ci = curcpu();

	ci->ci_curpmap = NULL;
}

/*
 * Segment and page table management routines
 */

/*
 * [INTERNAL]
 * Expand pmap_kernel() to be able to map a page at `va', by allocating
 * a page table. Returns a pointer to the pte of this page, or NULL
 * if allocation failed and `canfail' is nonzero. Panics if allocation
 * fails and `canfail' is zero.
 * Caller is supposed to only invoke this function if
 * pmap_pte(pmap_kernel(), va) returns NULL.
 */
pt_entry_t *
pmap_expand_kmap(vaddr_t va, int canfail)
{
	sdt_entry_t *sdt;
	struct vm_page *pg;
	paddr_t pa;

	DPRINTF(CD_KMAP, ("pmap_expand_kmap(%p, %d)\n", va, canfail));

	if (__predict_true(uvm.page_init_done)) {
		pg = uvm_pagealloc(NULL, 0, NULL,
		    (canfail ? 0 : UVM_PGA_USERESERVE) | UVM_PGA_ZERO);
		if (pg == NULL) {
			if (canfail)
				return NULL;
			panic("pmap_expand_kmap(%p): uvm_pagealloc() failed",
			    (void *)va);
		}
		pa = VM_PAGE_TO_PHYS(pg);
	} else {
		pa = (paddr_t)uvm_pageboot_alloc(PAGE_SIZE);
		if (pa == 0)
			panic("pmap_expand_kmap(%p): uvm_pageboot_alloc() failed",
			    (void *)va);
		bzero((void *)pa, PAGE_SIZE);
	}

	pmap_cache_ctrl(pa, pa + PAGE_SIZE, pte_cmode);
	sdt = SDTENT(pmap_kernel(), va);
	*sdt = pa | SG_SO | SG_RW | PG_M | SG_V;
	return sdt_pte(sdt, va);
}

/*
 * [INTERNAL]
 * Expand `pmap' to be able to map a page at `va', by allocating
 * a page table. Returns a pointer to the pte of this page, or NULL
 * if allocation failed and `canfail' is nonzero. Waits until memory is
 * available if allocation fails and `canfail' is zero.
 * Caller is supposed to only invoke this function if
 * pmap_pte(pmap, va) returns NULL.
 */
pt_entry_t *
pmap_expand(pmap_t pmap, vaddr_t va, int canfail)
{
	struct vm_page *pg;
	paddr_t pa;
	sdt_entry_t *sdt;

	DPRINTF(CD_EXP, ("pmap_expand(%p, %p, %d)\n", pmap, va, canfail));

	sdt = SDTENT(pmap, va);
	for (;;) {
		pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_ZERO);
		if (pg != NULL)
			break;
		if (canfail)
			return NULL;
		uvm_wait(__func__);
	}

	pa = VM_PAGE_TO_PHYS(pg);
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, pte_cmode);

	*sdt = pa | SG_RW | PG_M | SG_V;

	return sdt_pte(sdt, va);
}

/*
 * Bootstrap routines
 */

/*
 * [MI]
 * Early allocation, directly from the vm_physseg ranges of managed pages
 * passed to UVM. Pages ``stolen'' by this routine will never be seen as
 * managed pages and will not have vm_page structs created for them,
 */
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
{
	vaddr_t va;
	u_int npg;

	size = round_page(size);
	npg = atop(size);

	/* m88k systems only have one segment. */
#ifdef DIAGNOSTIC
	if (vm_physmem[0].avail_end - vm_physmem[0].avail_start < npg)
		panic("pmap_steal_memory(%lx): out of memory", size);
#endif

	va = ptoa(vm_physmem[0].avail_start);
	vm_physmem[0].avail_start += npg;
	vm_physmem[0].start += npg;

	if (vstartp != NULL)
		*vstartp = virtual_avail;
	if (vendp != NULL)
		*vendp = virtual_end;
	
	bzero((void *)va, size);
	return (va);
}

/*
 * [INTERNAL]
 * Setup a wired mapping in pmap_kernel(). Similar to pmap_kenter_pa(),
 * but allows explicit cacheability control.
 * This is only used at bootstrap time. Mappings may also be backed up
 * by a BATC entry if requested and possible; but note that the BATC
 * entries set up here may be overwritten by cmmu_batc_setup() later on
 * (which is harmless since we are creating proper ptes anyway).
 */
void
pmap_map(paddr_t pa, psize_t sz, vm_prot_t prot, u_int cmode,
    boolean_t may_use_batc)
{
	pt_entry_t *pte, npte;
	batc_t batc;
	uint npg, batcno;
	paddr_t curpa;

	DPRINTF(CD_MAP, ("pmap_map(%p, %p, %x, %x)\n",
	    pa, sz, prot, cmode));
#ifdef DIAGNOSTIC
	if (pa != 0 && pa < VM_MAX_KERNEL_ADDRESS)
		panic("pmap_map: virtual range %p-%p overlaps KVM",
		    (void *)pa, (void *)(pa + sz));
#endif

	sz = round_page(pa + sz) - trunc_page(pa);
	pa = trunc_page(pa);

	npte = m88k_protection(prot) | cmode | PG_W | PG_V;
#ifdef M88110
	if (CPU_IS88110 && m88k_protection(prot) != PG_RO)
		npte |= PG_M;
#endif

	npg = atop(sz);
	curpa = pa;
	while (npg-- != 0) {
		if ((pte = pmap_pte(pmap_kernel(), curpa)) == NULL)
			pte = pmap_expand_kmap(curpa, 0);

		*pte = npte | curpa;
		curpa += PAGE_SIZE;
		pmap_kernel()->pm_stats.resident_count++;
		pmap_kernel()->pm_stats.wired_count++;
	}

	if (may_use_batc) {
		sz = round_batc(pa + sz) - trunc_batc(pa);
		pa = trunc_batc(pa);

		batc = BATC_SO | BATC_V;
		if ((prot & PROT_WRITE) == 0)
			batc |= BATC_PROT;
		if (cmode & CACHE_INH)
			batc |= BATC_INH;
		if (cmode & CACHE_WT)
			batc |= BATC_WT;
		batc |= BATC_GLOBAL;	/* XXX 88110 SP */

		for (; sz != 0; sz -= BATC_BLKBYTES, pa += BATC_BLKBYTES) {
			/* check if an existing BATC covers this area */
			for (batcno = 0; batcno < BATC_MAX; batcno++) {
				if ((global_dbatc[batcno] & BATC_V) == 0)
					continue;
				curpa = (global_dbatc[batcno] <<
				    (BATC_BLKSHIFT - BATC_PSHIFT)) &
				    ~BATC_BLKMASK;
				if (curpa == pa)
					break;
			}

			/*
			 * If there is a BATC covering this range, reuse it.
			 * We assume all BATC-possible mappings will use the
			 * same protection and cacheability settings.
			 */
			if (batcno != BATC_MAX)
				continue;

			/* create a new DBATC if possible */
			for (batcno = BATC_MAX; batcno != 0; batcno--) {
				if (global_dbatc[batcno - 1] & BATC_V)
					continue;
				global_dbatc[batcno - 1] = batc |
				    ((pa >> BATC_BLKSHIFT) << BATC_PSHIFT) |
				    ((pa >> BATC_BLKSHIFT) << BATC_VSHIFT);
				break;
			}
		}
	}
}

/*
 * [MD]
 * Initialize kernel translation tables.
 */
void
pmap_bootstrap(paddr_t s_rom, paddr_t e_rom)
{
	paddr_t s_low, s_text, e_rodata;
	unsigned int npdtpg, nsdt, npdt;
	unsigned int i;
	sdt_entry_t *sdt;
	pt_entry_t *pte, template;
	paddr_t pa, sdtpa, ptepa;
	const struct pmap_table *ptable;
	extern void *kernelstart;
	extern void *erodata;

	virtual_avail = (vaddr_t)avail_end;

	s_text = trunc_page((vaddr_t)&kernelstart);
	e_rodata = round_page((vaddr_t)&erodata);

	/*
	 * Reserve space for 1:1 memory mapping in supervisor space.
	 * We need:
	 * - roundup(avail_end, SDT_SIZE) / SDT_SIZE segment tables;
	 *   these will fit in one page.
	 * - roundup(avail_end, PDT_SIZE) / PDT_SIZE page tables;
	 *   these will span several pages.
	 */

	nsdt = roundup(avail_end, (1 << SDT_SHIFT)) >> SDT_SHIFT;
	npdt = roundup(avail_end, (1 << PDT_SHIFT)) >> PDT_SHIFT;
	DPRINTF(CD_BOOT, ("avail_end %08x pages %08x nsdt %08x npdt %08x\n",
	    avail_end, atop(avail_end), nsdt, npdt));

	/*
	 * Since page tables may need specific cacheability settings,
	 * we need to make sure they will not end up in the BATC
	 * mapping the end of the kernel data.
	 *
	 * The CMMU initialization code will try, whenever possible, to
	 * setup 512KB BATC entries to map the kernel text and data,
	 * therefore platform-specific code is expected to register a
	 * non-overlapping range of pages (so that their cacheability
	 * can be controlled at the PTE level).
	 *
	 * If there is enough room between the firmware image and the
	 * beginning of the BATC-mapped region, we will setup the
	 * initial page tables there (and actually try to setup as many
	 * second level pages as possible, since this memory is not
	 * given to the VM system).
	 */

	npdtpg = atop(round_page(npdt * sizeof(pt_entry_t)));
	s_low = trunc_batc(s_text);

	if (e_rom == 0)
		s_rom = e_rom = PAGE_SIZE;
	DPRINTF(CD_BOOT, ("nsdt %d npdt %d npdtpg %d\n", nsdt, npdt, npdtpg));
	DPRINTF(CD_BOOT, ("area below the kernel %p-%p: %d pages, need %d\n",
	    e_rom, s_low, atop(s_low - e_rom), npdtpg + 1));
	if (e_rom < s_low && npdtpg + 1 <= atop(s_low - e_rom)) {
		sdtpa = e_rom;
		ptepa = sdtpa + PAGE_SIZE;
	} else {
		sdtpa = (paddr_t)uvm_pageboot_alloc(PAGE_SIZE);
		ptepa = (paddr_t)uvm_pageboot_alloc(ptoa(npdtpg));
	}

	sdt = (sdt_entry_t *)sdtpa;
	pte = (pt_entry_t *)ptepa;
	pmap_kernel()->pm_stab = sdt;

	DPRINTF(CD_BOOT, ("kernel sdt %p", sdt));
	pa = ptepa;
	for (i = nsdt; i != 0; i--) {
		*sdt++ = pa | SG_SO | SG_RW | PG_M | SG_V;
		pa += PAGE_SIZE;
	}
	DPRINTF(CD_BOOT, ("-%p\n", sdt));
	for (i = (PAGE_SIZE / sizeof(sdt_entry_t)) - nsdt; i != 0; i--)
		*sdt++ = SG_NV;
	KDASSERT((vaddr_t)sdt == ptepa);

	DPRINTF(CD_BOOT, ("kernel pte %p", pte));
	/* memory below the kernel image */
	for (i = atop(s_text); i != 0; i--)
		*pte++ = PG_NV;
	/* kernel text and rodata */
	pa = s_text;
	for (i = atop(e_rodata) - atop(pa); i != 0; i--) {
		*pte++ = pa | PG_SO | PG_RO | PG_W | PG_V;
		pa += PAGE_SIZE;
	}
	/* kernel data and symbols */
	for (i = atop(avail_start) - atop(pa); i != 0; i--) {
#ifdef MULTIPROCESSOR
		*pte++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V | CACHE_WT;
#else
		*pte++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V;
#endif
		pa += PAGE_SIZE;
	}
	/* regular memory */
	for (i = atop(avail_end) - atop(pa); i != 0; i--) {
		*pte++ = pa | PG_SO | PG_RW | PG_M_U | PG_V;
		pa += PAGE_SIZE;
	}
	DPRINTF(CD_BOOT, ("-%p, pa %08x\n", pte, pa));
	for (i = (pt_entry_t *)round_page((vaddr_t)pte) - pte; i != 0; i--)
		*pte++ = PG_NV;

	/* kernel page tables */
	pte_cmode = cmmu_pte_cmode();
	template = PG_SO | PG_RW | PG_M_U | PG_W | PG_V | pte_cmode;
	pa = sdtpa;
	pte = (pt_entry_t *)ptepa + atop(pa);
	for (i = 1 + npdtpg; i != 0; i--) {
		*pte++ = pa | template;
		pa += PAGE_SIZE;
	}

	/*
	 * Create all the machine-specific mappings.
	 * XXX This should eventually get done in machdep.c instead of here;
	 * XXX and on a driver basis on luna88k... If only to be able to grow
	 * XXX VM_MAX_KERNEL_ADDRESS.
	 */

	if (e_rom != s_rom) {
		s_firmware = s_rom;
		l_firmware = e_rom - s_rom;
		pmap_map(s_firmware, l_firmware, PROT_READ | PROT_WRITE,
		    CACHE_INH, FALSE);
	}

	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		if (ptable->size != 0)
			pmap_map(ptable->start, ptable->size,
			    ptable->prot, ptable->cacheability,
			    ptable->may_use_batc);

	/*
	 * Adjust cache settings according to the hardware we are running on.
	 */

	kernel_apr = (kernel_apr & ~(CACHE_MASK & ~CACHE_GLOBAL)) |
	    cmmu_apr_cmode();
#if defined(M88110) && !defined(MULTIPROCESSOR)
	if (CPU_IS88110)
		kernel_apr &= ~CACHE_GLOBAL;
#endif
	userland_apr = (userland_apr & ~CACHE_MASK) | (kernel_apr & CACHE_MASK);

	/*
	 * Switch to using new page tables
	 */

	pmap_kernel()->pm_count = 1;
	pmap_kernel()->pm_apr = sdtpa | kernel_apr;

	DPRINTF(CD_BOOT, ("default apr %08x kernel apr %08x\n",
	    kernel_apr, sdtpa));

	pmap_bootstrap_cpu(cpu_number());
}

/*
 * [MD]
 * Enable address translation on the current processor.
 */
void
pmap_bootstrap_cpu(cpuid_t cpu)
{
	/* Load supervisor pointer to segment table. */
	cmmu_set_sapr(pmap_kernel()->pm_apr);
#ifdef PMAPDEBUG
	printf("cpu%d: running virtual\n", cpu);
#endif

	cmmu_batc_setup(cpu, kernel_apr & CACHE_MASK);

	curcpu()->ci_curpmap = NULL;
}

/*
 * [MD]
 * Remove firmware mappings when they are no longer necessary.
 */
void
pmap_unmap_firmware()
{
	if (l_firmware != 0) {
		pmap_kremove(s_firmware, l_firmware);
		pmap_update(pmap_kernel());
	}
}

/*
 * [MI]
 * Complete the pmap layer initialization, to be able to manage userland
 * pmaps.
 */
void
pmap_init(void)
{
	DPRINTF(CD_INIT, ("pmap_init()\n"));
	pool_init(&pmappool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmappl", &pool_allocator_single);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, IPL_VM, 0, "pvpl", NULL);
}

/*
 * Pmap structure management
 */

/*
 * [MI]
 * Create a new pmap.
 */
pmap_t
pmap_create(void)
{
	pmap_t pmap;
	struct vm_page *pg;
	paddr_t pa;

	pmap = pool_get(&pmappool, PR_WAITOK | PR_ZERO);

	/* Allocate the segment table page immediately. */
	for (;;) {
		pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_ZERO);
		if (pg != NULL)
			break;
		uvm_wait(__func__);
	}

	pa = VM_PAGE_TO_PHYS(pg);
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, pte_cmode);

	pmap->pm_stab = (sdt_entry_t *)pa;
	pmap->pm_apr = pa | userland_apr;
	pmap->pm_count = 1;

	DPRINTF(CD_CREAT, ("pmap_create() -> pmap %p, pm_stab %p\n", pmap, pa));

	return pmap;
}

/*
 * [MI]
 * Decreased the pmap reference count, and destroy it when it reaches zero.
 */
void
pmap_destroy(pmap_t pmap)
{
	u_int u;
	sdt_entry_t *sdt;
	paddr_t pa;

	DPRINTF(CD_DESTR, ("pmap_destroy(%p)\n", pmap));
	if (--pmap->pm_count == 0) {
		for (u = SDT_ENTRIES, sdt = pmap->pm_stab; u != 0; sdt++, u--) {
			if (SDT_VALID(sdt)) {
				pa = *sdt & PG_FRAME;
				pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_DFL);
				uvm_pagefree(PHYS_TO_VM_PAGE(pa));
			}
		}
		pa = (paddr_t)pmap->pm_stab;
		pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_DFL);
		uvm_pagefree(PHYS_TO_VM_PAGE(pa));
		pool_put(&pmappool, pmap);
	}
}

/*
 * [MI]
 * Increase the pmap reference count.
 */
void
pmap_reference(pmap_t pmap)
{
	pmap->pm_count++;
}

/*
 * [MI]
 * Attempt to regain memory by freeing disposable page tables.
 */
void
pmap_collect(pmap_t pmap)
{
	u_int u, v;
	sdt_entry_t *sdt;
	pt_entry_t *pte;
	vaddr_t va;
	paddr_t pa;
	int s;

	DPRINTF(CD_COL, ("pmap_collect(%p)\n", pmap));

	s = splvm();
	for (sdt = pmap->pm_stab, va = 0, u = SDT_ENTRIES; u != 0;
	    sdt++, va += (1 << SDT_SHIFT), u--) {
		if (!SDT_VALID(sdt))
			continue;
		pte = sdt_pte(sdt, 0);
		for (v = PDT_ENTRIES; v != 0; pte++, v--)
			if (pmap_pte_w(pte)) /* wired mappings can't go */
				break;
		if (v != 0)
			continue;
		/* found a suitable pte page to reclaim */
		pmap_remove_range(pmap, va, va + (1 << SDT_SHIFT));

		pa = *sdt & PG_FRAME;
		*sdt = SG_NV;
		pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_DFL);
		uvm_pagefree(PHYS_TO_VM_PAGE(pa));
	}
	splx(s);

	DPRINTF(CD_COL, ("pmap_collect(%p) done\n", pmap));
}

/*
 * Virtual mapping/unmapping routines
 */

/*
 * [MI]
 * Establish a `va' to `pa' translation with protection `prot' in `pmap'.
 * The `flags' argument contains the expected usage protection of the
 * mapping (and may differ from the currently requested protection), as
 * well as a possible PMAP_WIRED flag.
 */
int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	int s;
	pt_entry_t *pte, npte;
	paddr_t old_pa;
	pv_entry_t pv_e, head;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	struct vm_page *pg;

	DPRINTF(CD_ENT, ("pmap_enter(%p, %p, %p, %x, %x)\n",
	    pmap, va, pa, prot, flags));

	npte = m88k_protection(prot);

	/*
	 * Expand pmap to include this pte.
	 */
	if ((pte = pmap_pte(pmap, va)) == NULL) {
		if (pmap == pmap_kernel())
			pte = pmap_expand_kmap(va, flags & PMAP_CANFAIL);
		else
			pte = pmap_expand(pmap, va, flags & PMAP_CANFAIL);

		/* will only return NULL if PMAP_CANFAIL is set */
		if (pte == NULL) {
			DPRINTF(CD_ENT, ("failed (ENOMEM)\n"));
			return (ENOMEM);
		}
	}

	/*
	 * Special case if the physical page is already mapped at this address.
	 */
	old_pa = ptoa(PG_PFNUM(*pte));
	DPRINTF(CD_ENT, ("pmap_enter: old_pa %p pte %p\n", old_pa, *pte));

	pg = PHYS_TO_VM_PAGE(pa);
	s = splvm();

	if (old_pa == pa) {
		/* May be changing its wired attributes or protection */
		if (wired && !(pmap_pte_w(pte)))
			pmap->pm_stats.wired_count++;
		else if (!wired && pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;
	} else {
		/* Remove old mapping from the PV list if necessary. */
		if (PDT_VALID(pte))
			pmap_remove_pte(pmap, va, pte, NULL, FALSE);

		if (pg != NULL) {
			/*
			 * Enter the mapping in the PV list for this
			 * managed page.
			 */
			head = pg_to_pvh(pg);
			if (head->pv_pmap == NULL) {
				/*
				 * No mappings yet.
				 */
				head->pv_va = va;
				head->pv_pmap = pmap;
				head->pv_next = NULL;
				pg->mdpage.pv_flags = 0;
			} else {
				/*
				 * Add new pv_entry after header.
				 */
				pv_e = pool_get(&pvpool, PR_NOWAIT);
				if (pv_e == NULL) {
					/* Invalidate the old pte anyway */
					tlb_flush(pmap, va, PG_NV);

					if (flags & PMAP_CANFAIL) {
						splx(s);
						return (ENOMEM);
					} else
						panic("pmap_enter: "
						    "pvpool exhausted");
				}
				pv_e->pv_va = va;
				pv_e->pv_pmap = pmap;
				pv_e->pv_next = head->pv_next;
				head->pv_next = pv_e;
			}
		}

		/*
		 * And count the mapping.
		 */
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
	} /* if (pa == old_pa) ... else */

	npte |= PG_V;
	if (wired)
		npte |= PG_W;

	if (prot & PROT_WRITE) {
		/*
		 * On 88110, do not mark writable mappings as dirty unless we
		 * know the page is dirty, or we are using the kernel pmap.
		 */
		if (CPU_IS88110 && pmap != pmap_kernel() &&
		    pg != NULL && (pg->mdpage.pv_flags & PG_M) == 0)
			npte |= PG_U;
		else
			npte |= PG_M_U;
	} else if (prot & PROT_MASK)
		npte |= PG_U;

	/*
	 * If outside physical memory, disable cache on this (device) page.
	 */
	if (pa >= last_addr)
		npte |= CACHE_INH;

	/*
	 * Invalidate pte temporarily to avoid being written
	 * back the modified bit and/or the reference bit by
	 * any other cpu.
	 */
	npte |= invalidate_pte(pte) & PG_M_U;
	npte |= pa;
	*pte = npte;
	tlb_flush(pmap, va, npte);
	DPRINTF(CD_ENT, ("pmap_enter: new pte %p\n", npte));

	/*
	 * Cache attribute flags
	 */
	if (pg != NULL) {
		if (flags & PROT_WRITE) {
			if (CPU_IS88110 && pmap != pmap_kernel())
				pg->mdpage.pv_flags |= PG_U;
			else
				pg->mdpage.pv_flags |= PG_M_U;
		} else if (flags & PROT_MASK)
			pg->mdpage.pv_flags |= PG_U;
	}

	splx(s);

	return 0;
}

/*
 * [MI]
 * Fast pmap_enter() version for pmap_kernel() and unmanaged pages.
 */
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *pte, npte;

	DPRINTF(CD_ENT, ("pmap_kenter_pa(%p, %p, %x)\n", va, pa, prot));

	npte = m88k_protection(prot) | PG_W | PG_V;
#ifdef M88110
	if (CPU_IS88110 && m88k_protection(prot) != PG_RO)
		npte |= PG_M;
#endif
	/*
	 * If outside physical memory, disable cache on this (device) page.
	 */
	if (pa >= last_addr)
		npte |= CACHE_INH;

	/*
	 * Expand pmap to include this pte.
	 */
	if ((pte = pmap_pte(pmap_kernel(), va)) == NULL)
		pte = pmap_expand_kmap(va, 0);

	/*
	 * And count the mapping.
	 */
	pmap_kernel()->pm_stats.resident_count++;
	pmap_kernel()->pm_stats.wired_count++;

	invalidate_pte(pte);
	npte |= pa;
	*pte = npte;
	tlb_kflush(va, npte);
}

/*
 * [INTERNAL]
 * Remove the page at `va' in `pmap', which pte is pointed to by `pte', and
 * update the status of the vm_page matching this translation (if this is
 * indeed a managed page). Flush the tlb entry if `flush' is nonzero.
 */
void
pmap_remove_pte(pmap_t pmap, vaddr_t va, pt_entry_t *pte, struct vm_page *pg,
   boolean_t flush)
{
	pt_entry_t opte;
	pv_entry_t prev, cur, head;
	paddr_t pa;

	splassert(IPL_VM);
	DPRINTF(CD_RM, ("pmap_remove_pte(%p, %p, %d)\n", pmap, va, flush));

	/*
	 * Update statistics.
	 */
	pmap->pm_stats.resident_count--;
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;

	pa = ptoa(PG_PFNUM(*pte));

	/*
	 * Invalidate the pte.
	 */

	opte = invalidate_pte(pte) & PG_M_U;
	if (flush)
		tlb_flush(pmap, va, PG_NV);

	if (pg == NULL) {
		pg = PHYS_TO_VM_PAGE(pa);
		/* If this isn't a managed page, just return. */
		if (pg == NULL)
			return;
	}

	/*
	 * Remove the mapping from the pvlist for
	 * this physical page.
	 */
	head = pg_to_pvh(pg);

#ifdef DIAGNOSTIC
	if (head->pv_pmap == NULL)
		panic("pmap_remove_pte(%p, %p, %p, %p/%p, %d): null pv_list",
		   pmap, (void *)va, pte, (void *)pa, pg, flush);
#endif

	prev = NULL;
	for (cur = head; cur != NULL; cur = cur->pv_next) {
		if (cur->pv_va == va && cur->pv_pmap == pmap)
			break;
		prev = cur;
	}
	if (cur == NULL) {
		panic("pmap_remove_pte(%p, %p, %p, %p, %d): mapping for va "
		    "(pa %p) not in pv list at %p",
		    pmap, (void *)va, pte, pg, flush, (void *)pa, head);
	}

	if (prev == NULL) {
		/*
		 * Handler is the pv_entry. Copy the next one
		 * to handler and free the next one (we can't
		 * free the handler)
		 */
		cur = cur->pv_next;
		if (cur != NULL) {
			*head = *cur;
			pool_put(&pvpool, cur);
		} else {
			head->pv_pmap = NULL;
			/*
			 * This page is no longer in use, and is likely
			 * to be reused soon; since it may still have
			 * dirty cache lines and may be used for I/O
			 * (and risk being invalidated by the bus_dma
			 * code without getting a chance of writeback),
			 * we make sure the page gets written back.
			 */
			pmap_clean_page(pa);
		}
	} else {
		prev->pv_next = cur->pv_next;
		pool_put(&pvpool, cur);
	}

	/* Update saved attributes for managed page */
	pg->mdpage.pv_flags |= opte;
}

/*
 * [INTERNAL]
 * Removes all mappings within the `sva'..`eva' range in `pmap'.
 */
void
pmap_remove_range(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{
	vaddr_t va, eseg;
	pt_entry_t *pte;

	DPRINTF(CD_RM, ("pmap_remove_range(%p, %p, %p)\n", pmap, sva, eva));

	/*
	 * Loop through the range in PAGE_SIZE increments.
	 */
	va = sva;
	while (va != eva) {
		sdt_entry_t *sdt;

		eseg = (va & SDT_MASK) + (1 << SDT_SHIFT);
		if (eseg > eva || eseg == 0)
			eseg = eva;

		sdt = SDTENT(pmap, va);
		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt))
			va = eseg;
		else {
			pte = sdt_pte(sdt, va);
			while (va != eseg) {
				if (PDT_VALID(pte))
					pmap_remove_pte(pmap, va, pte, NULL,
					    TRUE);
				va += PAGE_SIZE;
				pte++;
			}
		}
	}
}

/*
 * [MI]
 * Removes all mappings within the `sva'..`eva' range in `pmap'.
 */
void
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{
	int s;

	KERNEL_LOCK();
	s = splvm();
	pmap_remove_range(pmap, sva, eva);
	splx(s);
	KERNEL_UNLOCK();
}

/*
 * [MI]
 * Fast pmap_remove() version for pmap_kernel() and unmanaged pages.
 */
void
pmap_kremove(vaddr_t va, vsize_t len)
{
	vaddr_t e, eseg;

	DPRINTF(CD_RM, ("pmap_kremove(%p, %x)\n", va, len));

	e = va + len;
	while (va != e) {
		sdt_entry_t *sdt;
		pt_entry_t *pte, opte;

		eseg = (va & SDT_MASK) + (1 << SDT_SHIFT);
		if (eseg > e || eseg == 0)
			eseg = e;

		sdt = SDTENT(pmap_kernel(), va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt))
			va = eseg;
		else {
			pte = sdt_pte(sdt, va);
			while (va != eseg) {
				if (PDT_VALID(pte)) {
					/* Update the counts */
					pmap_kernel()->pm_stats.resident_count--;
					pmap_kernel()->pm_stats.wired_count--;

					opte = invalidate_pte(pte);
					tlb_kflush(va, PG_NV);

					/*
					 * Make sure the page is written back
					 * if it was cached.
					 */
					if ((opte & (CACHE_INH | CACHE_WT)) ==
					    0)
						pmap_clean_page(
						    ptoa(PG_PFNUM(opte)));
				}
				va += PAGE_SIZE;
				pte++;
			}
		}
	}
}

/*
 * [INTERNAL]
 * Removes all mappings of managed page `pg'.
 */
void
pmap_remove_page(struct vm_page *pg)
{
	pt_entry_t *pte;
	pv_entry_t head, pvep;
	vaddr_t va;
	pmap_t pmap;
	int s;

	DPRINTF(CD_RMPG, ("pmap_remove_page(%p)\n", pg));

	s = splvm();
	/*
	 * Walk down PV list, removing all mappings.
	 */
	pvep = head = pg_to_pvh(pg);
	while (pvep != NULL && (pmap = pvep->pv_pmap) != NULL) {
		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		if (pte == NULL || !PDT_VALID(pte)) {
			pvep = pvep->pv_next;
			continue;	/* no page mapping */
		}

		pmap_remove_pte(pmap, va, pte, pg, TRUE);
		pvep = head;
		/*
		 * Do not free any empty page tables,
		 * leave that for when VM calls pmap_collect().
		 */
	}
	splx(s);
}

/*
 * [MI]
 * Strengthens the protection of the `sva'..`eva' range within `pmap' to `prot'.
 */
void
pmap_protect(pmap_t pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	int s;
	pt_entry_t *pte, ap, opte, npte;
	vaddr_t va, eseg;

	if ((prot & PROT_READ) == 0) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	ap = m88k_protection(prot);

	s = splvm();
	/*
	 * Loop through the range in PAGE_SIZE increments.
	 */
	va = sva;
	while (va != eva) {
		sdt_entry_t *sdt;

		eseg = (va & SDT_MASK) + (1 << SDT_SHIFT);
		if (eseg > eva || eseg == 0)
			eseg = eva;

		sdt = SDTENT(pmap, va);
		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt))
			va = eseg;
		else {
			pte = sdt_pte(sdt, va);
			while (va != eseg) {
				if (PDT_VALID(pte)) {
					/*
					 * Invalidate pte temporarily to avoid
					 * the modified bit and/or the
					 * reference bit being written back by
					 * any other cpu.
					 */
					opte = invalidate_pte(pte);
					npte = ap | (opte & ~PG_PROT);
					*pte = npte;
					tlb_flush(pmap, va, npte);
				}
				va += PAGE_SIZE;
				pte++;
			}
		}
	}
	splx(s);
}

/*
 * [MI]
 * Removes the wired state of the page at `va' in `pmap'.
 */
void
pmap_unwire(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *pte;

	pte = pmap_pte(pmap, va);
	if (pmap_pte_w(pte)) {
		pmap->pm_stats.wired_count--;
		*pte &= ~PG_W;
	}
}

/*
 * vm_page management routines
 */

/*
 * [MI]
 * Copies vm_page `srcpg' to `dstpg'.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

	DPRINTF(CD_COPY, ("pmap_copy_page(%p,%p) pa %p %p\n",
	    srcpg, dstpg, src, dst));
	curcpu()->ci_copypage((vaddr_t)src, (vaddr_t)dst);

	if (KERNEL_APR_CMODE == CACHE_DFL)
		cmmu_dcache_wb(cpu_number(), dst, PAGE_SIZE);
}

/*
 * [MI]
 * Clears vm_page `pg'.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

	DPRINTF(CD_ZERO, ("pmap_zero_page(%p) pa %p\n", pg, pa));
	curcpu()->ci_zeropage((vaddr_t)pa);

	if (KERNEL_APR_CMODE == CACHE_DFL)
		cmmu_dcache_wb(cpu_number(), pa, PAGE_SIZE);
}

/*
 * [INTERNAL]
 * Alters bits in the pte of all mappings of `pg'. For each pte, bits in
 * `set' are set and bits not in `mask' are cleared. The flags summary
 * at the head of the pv list is modified in a similar way.
 */
void
pmap_changebit(struct vm_page *pg, int set, int mask)
{
	pv_entry_t head, pvep;
	pt_entry_t *pte, npte, opte;
	pmap_t pmap;
	int s;
	vaddr_t va;

	DPRINTF(CD_CBIT, ("pmap_changebit(%p, %x, %x)\n", pg, set, mask));

	s = splvm();

	/*
	 * Clear saved attributes (modify, reference)
	 */
	pg->mdpage.pv_flags &= mask;

	head = pg_to_pvh(pg);
	if (head->pv_pmap != NULL) {
		/* for each listed pmap, update the affected bits */
		for (pvep = head; pvep != NULL; pvep = pvep->pv_next) {
			pmap = pvep->pv_pmap;
			va = pvep->pv_va;
			pte = pmap_pte(pmap, va);

			/*
			 * Check for existing and valid pte
			 */
			if (pte == NULL || !PDT_VALID(pte))
				continue;	 /* no page mapping */
#ifdef PMAPDEBUG
			if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
				panic("pmap_changebit: pte %08x in pmap %p doesn't point to page %p@@%p",
				    *pte, pmap, pg, VM_PAGE_TO_PHYS(pg));
#endif

			/*
			 * Update bits
			 */
			opte = *pte;
			npte = (opte | set) & mask;

			/*
			 * Invalidate pte temporarily to avoid the modified bit
			 * and/or the reference being written back by any other
			 * cpu.
			 */
			if (npte != opte) {
				invalidate_pte(pte);
				*pte = npte;
				tlb_flush(pmap, va, npte);
			}
		}
	}

	splx(s);
}

/*
 * [INTERNAL]
 * Checks for `bit' being set in at least one pte of all mappings of `pg'.
 * The flags summary at the head of the pv list is checked first, and will
 * be set if it wasn't but the bit is found set in one pte.
 * Returns TRUE if the bit is found, FALSE if not.
 */
boolean_t
pmap_testbit(struct vm_page *pg, int bit)
{
	pv_entry_t head, pvep;
	pt_entry_t *pte;
	pmap_t pmap;
	int s;

	DPRINTF(CD_TBIT, ("pmap_testbit(%p, %x): ", pg, bit));

	s = splvm();

	if (pg->mdpage.pv_flags & bit) {
		/* we've already cached this flag for this page,
		   no use looking further... */
		DPRINTF(CD_TBIT, ("cached\n"));
		splx(s);
		return (TRUE);
	}

	head = pg_to_pvh(pg);
	if (head->pv_pmap != NULL) {
		/* for each listed pmap, check modified bit for given page */
		for (pvep = head; pvep != NULL; pvep = pvep->pv_next) {
			pmap = pvep->pv_pmap;

			pte = pmap_pte(pmap, pvep->pv_va);
			if (pte == NULL || !PDT_VALID(pte))
				continue;

#ifdef PMAPDEBUG
			if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
				panic("pmap_testbit: pte %08x in pmap %p doesn't point to page %p@@%p",
				    *pte, pmap, pg, VM_PAGE_TO_PHYS(pg));
#endif

			if ((*pte & bit) != 0) {
				pg->mdpage.pv_flags |= bit;
				DPRINTF(CD_TBIT, ("found\n"));
				splx(s);
				return (TRUE);
			}
		}
	}

	DPRINTF(CD_TBIT, ("not found\n"));
	splx(s);
	return (FALSE);
}

/*
 * [INTERNAL]
 * Clears `bit' in the pte of all mapping of `pg', as well as in the flags
 * summary at the head of the pv list.
 * Returns TRUE if the bit was found set in either a mapping or the summary,
 * FALSE if not.
 */
boolean_t
pmap_unsetbit(struct vm_page *pg, int bit)
{
	boolean_t rv = FALSE;
	pv_entry_t head, pvep;
	pt_entry_t *pte, opte, npte;
	pmap_t pmap;
	int s;
	vaddr_t va;

	DPRINTF(CD_USBIT, ("pmap_unsetbit(%p, %x): ", pg, bit));

	s = splvm();

	/*
	 * Clear saved attributes
	 */
	if (pg->mdpage.pv_flags & bit) {
		pg->mdpage.pv_flags ^= bit;
		rv = TRUE;
	}

	head = pg_to_pvh(pg);
	if (head->pv_pmap != NULL) {
		/* for each listed pmap, update the specified bit */
		for (pvep = head; pvep != NULL; pvep = pvep->pv_next) {
			pmap = pvep->pv_pmap;
			va = pvep->pv_va;
			pte = pmap_pte(pmap, va);

			/*
			 * Check for existing and valid pte
			 */
			if (pte == NULL || !PDT_VALID(pte))
				continue;	 /* no page mapping */
#ifdef PMAPDEBUG
			if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
				panic("pmap_unsetbit: pte %08x in pmap %p doesn't point to page %p@@%p",
				    *pte, pmap, pg, VM_PAGE_TO_PHYS(pg));
#endif

			/*
			 * Update bits
			 */
			opte = *pte;
			if (opte & bit) {
				/*
				 * Invalidate pte temporarily to avoid the
				 * specified bit being written back by any
				 * other cpu.
				 */
				invalidate_pte(pte);
				npte = opte ^ bit;
				*pte = npte;
				tlb_flush(pmap, va, npte);
				rv = TRUE;
			}
		}
	}
	splx(s);

	DPRINTF(CD_USBIT, (rv ? "TRUE\n" : "FALSE\n"));
	return (rv);
}

/*
 * [MI]
 * Checks whether `pg' is dirty.
 * Returns TRUE if there is at least one mapping of `pg' with the modified
 * bit set in its pte, FALSE if not.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
#ifdef M88110
	/*
	 * Since on 88110 PG_M bit tracking is done in software, we can
	 * trust the page flags without having to walk the individual
	 * ptes in case the page flags are behind actual usage.
	 */
	if (CPU_IS88110) {
		boolean_t rc = FALSE;

		if (pg->mdpage.pv_flags & PG_M)
			rc = TRUE;
		DPRINTF(CD_TBIT, ("pmap_is_modified(%p) -> %x\n", pg, rc));
		return (rc);
	}
#endif

	return pmap_testbit(pg, PG_M);
}

/*
 * [MI]
 * Checks whether `pg' is in use.
 * Returns TRUE if there is at least one mapping of `pg' with the used bit
 * set in its pte, FALSE if not.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	return pmap_testbit(pg, PG_U);
}

/*
 * [MI]
 * Strengthens protection of `pg' to `prot'.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	if ((prot & PROT_READ) == PROT_NONE)
		pmap_remove_page(pg);
	else if ((prot & PROT_WRITE) == PROT_NONE)
		pmap_changebit(pg, PG_RO, ~0);
}

/*
 * Miscellaneous routines
 */

/*
 * [INTERNAL]
 * Writeback the data cache for the given page, on all processors.
 */
void
pmap_clean_page(paddr_t pa)
{
	struct cpu_info *ci;
#ifdef MULTIPROCESSOR
	CPU_INFO_ITERATOR cpu;
#endif

	if (KERNEL_APR_CMODE != CACHE_DFL && USERLAND_APR_CMODE != CACHE_DFL)
		return;

#ifdef MULTIPROCESSOR
	CPU_INFO_FOREACH(cpu, ci)
#else
	ci = curcpu();
#endif
	/* CPU_INFO_FOREACH(cpu, ci) */
		cmmu_dcache_wb(ci->ci_cpuid, pa, PAGE_SIZE);
}

/*
 * [MI]
 * Flushes instruction cache for the range `va'..`va'+`len' in proc `p'.
 */
void
pmap_proc_iflush(struct process *pr, vaddr_t va, vsize_t len)
{
	pmap_t pmap = vm_map_pmap(&pr->ps_vmspace->vm_map);
	paddr_t pa;
	vsize_t count;
	struct cpu_info *ci;

	if (KERNEL_APR_CMODE != CACHE_DFL && USERLAND_APR_CMODE != CACHE_DFL)
		return;

	while (len != 0) {
		count = min(len, PAGE_SIZE - (va & PAGE_MASK));
		if (pmap_extract(pmap, va, &pa)) {
#ifdef MULTIPROCESSOR
			CPU_INFO_ITERATOR cpu;

			CPU_INFO_FOREACH(cpu, ci)
#else
			ci = curcpu();
#endif
			/* CPU_INFO_FOREACH(cpu, ci) */ {
				cmmu_dcache_wb(ci->ci_cpuid, pa, count);
				/* XXX this should not be necessary, */
				/* XXX I$ is configured to snoop D$ */
				cmmu_icache_inv(ci->ci_cpuid, pa, count);
			}
		}
		va += count;
		len -= count;
	}
}

#ifdef M88110
/*
 * [INTERNAL]
 * Updates the pte mapping `va' in `pmap' upon write fault, to set the
 * modified bit in the pte (the 88110 MMU doesn't do this and relies upon
 * the kernel to achieve this).
 * Returns TRUE if the page was indeed writeable but not marked as dirty,
 * FALSE if this is a genuine write fault.
 */
int
pmap_set_modify(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *pte;
	paddr_t pa;
	vm_page_t pg;

	pte = pmap_pte(pmap, va);
#ifdef DEBUG
	if (pte == NULL)
		panic("NULL pte on write fault??");
#endif

	/* Not a first write to a writable page */
	if ((*pte & (PG_M | PG_RO)) != 0)
		return (FALSE);

	/* Mark the page as dirty */
	*pte |= PG_M;
	pa = *pte & PG_FRAME;
	pg = PHYS_TO_VM_PAGE(pa);
#ifdef DIAGNOSTIC
	if (pg == NULL)
		panic("Write fault to unmanaged page %p", (void *)pa);
#endif

	pg->mdpage.pv_flags |= PG_M_U;

	if (pmap == pmap_kernel())
		set_dcmd(CMMU_DCMD_INV_SATC);
	else
		set_dcmd(CMMU_DCMD_INV_UATC);

	return (TRUE);
}
#endif

/*
 * [MD PUBLIC]
 * Change the cache control bits of the address range `sva'..`eva' in
 * pmap_kernel to `mode'.
 */
void
pmap_cache_ctrl(vaddr_t sva, vaddr_t eva, u_int mode)
{
	int s;
	pt_entry_t *pte, opte, npte;
	vaddr_t va;
	paddr_t pa;
	cpuid_t cpu;

	DPRINTF(CD_CACHE, ("pmap_cache_ctrl(%p, %p, %x)\n",
	    sva, eva, mode));

	s = splvm();
	for (va = sva; va != eva; va += PAGE_SIZE) {
		if ((pte = pmap_pte(pmap_kernel(), va)) == NULL)
			continue;
		DPRINTF(CD_CACHE, ("cache_ctrl: pte@@%p\n", pte));

		/*
		 * Data cache should be copied back and invalidated if
		 * the old mapping was cached and the new isn't, or if
		 * we are downgrading from writeback to writethrough.
		 */
		if (((*pte & CACHE_INH) == 0 && (mode & CACHE_INH) != 0) ||
		    ((*pte & CACHE_WT) == 0 && (mode & CACHE_WT) != 0)) {
			pa = ptoa(PG_PFNUM(*pte));
#ifdef MULTIPROCESSOR
			for (cpu = 0; cpu < MAX_CPUS; cpu++)
				if (ISSET(m88k_cpus[cpu].ci_flags, CIF_ALIVE)) {
#else
			cpu = cpu_number();
#endif
					if (mode & CACHE_INH)
						cmmu_cache_wbinv(cpu,
						    pa, PAGE_SIZE);
					else if (KERNEL_APR_CMODE == CACHE_DFL ||
					    USERLAND_APR_CMODE == CACHE_DFL)
						cmmu_dcache_wb(cpu,
						    pa, PAGE_SIZE);
#ifdef MULTIPROCESSOR
				}
#endif
		}

		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 */

		opte = invalidate_pte(pte);
		npte = (opte & ~CACHE_MASK) | mode;
		*pte = npte;
		tlb_kflush(va, npte);
	}
	splx(s);
}

/*
 * [MD PUBLIC]
 * Change the cache control bits of all mappings of the given physical page to
 * disable cached accesses.
 */
void
pmap_page_uncache(paddr_t pa)
{
	struct vm_page *pg = PHYS_TO_VM_PAGE(pa);
	struct pmap *pmap;
	pv_entry_t head, pvep;
	pt_entry_t *pte, opte, npte;
	vaddr_t va;
	int s;

	s = splvm();
	head = pg_to_pvh(pg);
	if (head->pv_pmap != NULL) {
		for (pvep = head; pvep != NULL; pvep = pvep->pv_next) {
			pmap = pvep->pv_pmap;
			va = pvep->pv_va;
			pte = pmap_pte(pmap, va);

			if (pte == NULL || !PDT_VALID(pte))
				continue;	 /* no page mapping */
			opte = *pte;
			if ((opte & CACHE_MASK) != CACHE_INH) {
				/*
				 * Skip the direct mapping; it will be changed
				 * by the pmap_cache_ctrl() call below.
				 */
				if (pmap == pmap_kernel() && va == pa)
					continue;
				/*
				 * Invalidate pte temporarily to avoid the
				 * specified bit being written back by any
				 * other cpu.
				 */
				invalidate_pte(pte);
				npte = (opte & ~CACHE_MASK) | CACHE_INH;
				*pte = npte;
				tlb_flush(pmap, va, npte);
			}
		}
	}
	splx(s);
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_INH);
}
@


1.84
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2016/06/07 06:23:19 dlg Exp $	*/
d1820 1
a1820 1
pmap_proc_iflush(struct proc *p, vaddr_t va, vsize_t len)
d1822 1
a1822 1
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
@


1.83
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2015/09/08 21:28:36 kettenis Exp $	*/
d868 3
a870 5
	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_single);
	pool_setipl(&pmappool, IPL_NONE);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", NULL);
	pool_setipl(&pvpool, IPL_VM);
@


1.82
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2015/07/29 17:54:35 miod Exp $	*/
d870 1
d872 1
@


1.81
log
@Acquire the kernel lock in pmap_remove(). The reasons for this can't be
stated here as I have been asked to be polite in this commit message.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2015/07/25 20:45:05 miod Exp $	*/
d869 1
a869 1
	    &pool_allocator_nointr);
@


1.80
log
@Only store mod/ref flags in the vm_page_md struct, not in every pv_entry.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2014/11/16 12:30:58 deraadt Exp $	*/
d1331 1
d1335 1
@


1.79
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2014/06/09 14:33:20 miod Exp $	*/
a70 1
#include <machine/mmu.h>
d174 1
a174 1
	return &pg->mdpage.pvent;
d1004 1
a1004 1
	pv_entry_t pv_e, pvl;
a1036 4
	if (pg != NULL)
		pvl = pg_to_pvh(pg);
	else
		pvl = NULL;
d1049 1
a1049 1
		if (pvl != NULL) {
d1054 2
a1055 1
			if (pvl->pv_pmap == NULL) {
d1059 4
a1062 4
				pvl->pv_va = va;
				pvl->pv_pmap = pmap;
				pvl->pv_next = NULL;
				pvl->pv_flags = 0;
d1081 2
a1082 3
				pv_e->pv_next = pvl->pv_next;
				pv_e->pv_flags = 0;
				pvl->pv_next = pv_e;
d1104 1
a1104 1
		    pg != NULL && (pvl->pv_flags & PG_M) == 0)
d1131 1
a1131 1
	if (pvl != NULL) {
d1134 1
a1134 1
				pvl->pv_flags |= PG_U;
d1136 1
a1136 1
				pvl->pv_flags |= PG_M_U;
d1138 1
a1138 1
			pvl->pv_flags |= PG_U;
d1190 1
a1190 1
 * indeed a managed page). Flushe the tlb entry if `flush' is nonzero.
d1197 1
a1197 1
	pv_entry_t prev, cur, pvl;
d1231 1
a1231 1
	pvl = pg_to_pvh(pg);
d1234 1
a1234 1
	if (pvl->pv_pmap == NULL)
d1240 1
a1240 1
	for (cur = pvl; cur != NULL; cur = cur->pv_next) {
d1248 1
a1248 1
		    pmap, (void *)va, pte, pg, flush, (void *)pa, pvl);
d1259 1
a1259 2
			cur->pv_flags = pvl->pv_flags;
			*pvl = *cur;
d1262 1
a1262 1
			pvl->pv_pmap = NULL;
d1279 1
a1279 1
	pvl->pv_flags |= opte;
d1396 1
a1396 1
	pv_entry_t pvl;
d1407 3
a1409 3
	pvl = pg_to_pvh(pg);
	while (pvl != NULL && (pmap = pvl->pv_pmap) != NULL) {
		va = pvl->pv_va;
d1413 1
a1413 1
			pvl = pvl->pv_next;
d1418 1
d1547 1
a1547 1
	pv_entry_t pvl, pvep;
a1556 1
	pvl = pg_to_pvh(pg);
d1560 1
a1560 1
	pvl->pv_flags &= mask;
d1562 2
a1563 1
	if (pvl->pv_pmap != NULL) {
d1565 1
a1565 1
		for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
d1613 1
a1613 1
	pv_entry_t pvl, pvep;
d1622 1
a1622 2
	pvl = pg_to_pvh(pg);
	if (pvl->pv_flags & bit) {
d1630 2
a1631 1
	if (pvl->pv_pmap != NULL) {
d1633 1
a1633 1
		for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
d1647 1
a1647 1
				pvl->pv_flags |= bit;
d1671 1
a1671 1
	pv_entry_t pvl, pvep;
a1680 2
	pvl = pg_to_pvh(pg);

d1684 2
a1685 2
	if (pvl->pv_flags & bit) {
		pvl->pv_flags ^= bit;
d1689 2
a1690 1
	if (pvl->pv_pmap != NULL) {
d1692 1
a1692 1
		for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
a1747 1
		pv_entry_t pvl;
d1750 1
a1750 2
		pvl = pg_to_pvh(pg);
		if (pvl->pv_flags & PG_M)
a1864 1
	pv_entry_t pvl;
d1885 1
a1885 2
	pvl = pg_to_pvh(pg);
	pvl->pv_flags |= PG_M_U;
d1968 1
a1968 1
	pv_entry_t pvl, pvep;
d1974 3
a1976 3
	pvl = pg_to_pvh(pg);
	if (pvl->pv_pmap != NULL) {
		for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
@


1.78
log
@More format string fixes (in 88110 code)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2014/05/17 12:22:09 miod Exp $	*/
d182 1
a182 1
#define	m88k_protection(prot)	((prot) & VM_PROT_WRITE ? PG_RW : PG_RO)
d620 1
a620 1
		if ((prot & VM_PROT_WRITE) == 0)
d794 2
a795 1
		pmap_map(s_firmware, l_firmware, UVM_PROT_RW, CACHE_INH, FALSE);
d1103 1
a1103 1
	if (prot & VM_PROT_WRITE) {
d1113 1
a1113 1
	} else if (prot & VM_PROT_ALL)
d1137 1
a1137 1
		if (flags & VM_PROT_WRITE) {
d1142 1
a1142 1
		} else if (flags & VM_PROT_ALL)
d1443 1
a1443 1
	if ((prot & VM_PROT_READ) == 0) {
d1787 1
a1787 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE)
d1789 1
a1789 1
	else if ((prot & VM_PROT_WRITE) == VM_PROT_NONE)
@


1.77
log
@In pmap_remove_page(), do not keep wired mappings; the callers want them gone.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2014/05/08 22:17:33 miod Exp $	*/
d1890 1
a1890 1
		panic("Write fault to unmanaged page %p", pa);
@


1.76
log
@Format string fixes for m88k; remove -Wno-format from the m88k kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2013/11/16 18:45:20 miod Exp $	*/
a1419 6
		}
		if (pmap_pte_w(pte)) {
			DPRINTF(CD_RMPG, ("pmap_remove_page(%p): wired mapping not removed\n",
			    pg));
			pvl = pvl->pv_next;
			continue;
@


1.75
log
@Allow initial device mappings (from pmap_table) to be backed up by BATC.
Use this on luna88k to map the bitmap planes of the frame buffer used by
the driver. 10% speedup under X.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2013/11/03 09:42:55 miod Exp $	*/
d477 1
a477 1
			    va);
d484 1
a484 1
			    va);
d552 1
a552 1
		panic("pmap_steal_memory(%x): out of memory", size);
d591 1
a591 1
		    pa, pa + sz);
d1240 1
a1240 1
		   pmap, va, pte, pa, pg, flush);
d1252 1
a1252 1
		    pmap, va, pte, pg, flush, pa, pvl);
@


1.74
log
@Even saner kernel layout, so that .rodata can be batc mapped by a read-only
dbatc. Add batc mapping support for 88110 systems as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2013/11/02 23:10:30 miod Exp $	*/
d151 7
d164 1
a164 1
void		 pmap_map(paddr_t, psize_t, vm_prot_t, u_int);
d252 2
d259 1
a259 3
	 * Even though we do not use BATC yet, 88100-based designs (with
	 * 8820x CMMUs) have two hardwired BATC entries which map the
	 * upper 1MB (so-called `utility space') 1:1 in supervisor space.
d261 7
d269 23
a291 5
	if (CPU_IS88100 && pmap == pmap_kernel()) {
		if (va >= BATC9_VA) {
			*pap = va;
			*ti = BATC9 & CACHE_MASK;
			return PTI_BATC;
d293 42
a334 4
		if (va >= BATC8_VA) {
			*pap = va;
			*ti = BATC8 & CACHE_MASK;
			return PTI_BATC;
a336 1
#endif
d572 4
d578 2
a579 1
pmap_map(paddr_t pa, psize_t sz, vm_prot_t prot, u_int cmode)
d582 3
d594 3
d603 5
a607 5
	sz = atop(round_page(pa + sz) - trunc_page(pa));
	pa = trunc_page(pa);
	while (sz-- != 0) {
		if ((pte = pmap_pte(pmap_kernel(), pa)) == NULL)
			pte = pmap_expand_kmap(pa, 0);
d609 2
a610 2
		*pte = npte | pa;
		pa += PAGE_SIZE;
d614 45
d794 1
a794 1
		pmap_map(s_firmware, l_firmware, UVM_PROT_RW, CACHE_INH);
d800 2
a801 1
			    ptable->prot, ptable->cacheability);
@


1.73
log
@Create the initial page tables in the area between the end of the firmware
data area and the kernel image, whenever possible.

On 88100/88200 systems, use BATC mappings to map the kernel text (and the
kernel data for non-MULTIPROCESSOR kernels). 88110 to follow soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2013/08/26 20:29:34 miod Exp $	*/
d543 1
a543 1
	paddr_t s_low, s_text, e_text;
d551 1
a551 1
	extern void *etext;
d556 1
a556 1
	e_text = round_page((vaddr_t)&etext);
d625 1
a625 1
	/* kernel text */
d627 1
a627 1
	for (i = atop(e_text) - atop(pa); i != 0; i--) {
@


1.72
log
@When a page needs to be flushed from all caches, make sure the writeback
happens on all D$ in MULTIPROCESSOR kernels, for the dirty lines may not all
be on the current cpu. Fixes spurious segmentation faults or ill behaviours
in MULTIPROCESSOR kernels, but there are still some left.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2013/05/17 22:33:25 miod Exp $	*/
d86 1
d145 6
d543 2
a544 2
	paddr_t s_text, e_text;
	unsigned int nsdt, npdt;
d547 2
a548 2
	pt_entry_t *pdt, template;
	paddr_t pa, sptpa, eptpa;
d572 38
a609 3
	sdt = (sdt_entry_t *)uvm_pageboot_alloc(PAGE_SIZE);
	pdt = (pt_entry_t *)
	    uvm_pageboot_alloc(round_page(npdt * sizeof(pt_entry_t)));
d611 1
a611 4
	sptpa = (paddr_t)sdt;
	pmap_kernel()->pm_stab = sdt;
	pa = (paddr_t)pdt;
	eptpa = pa + round_page(npdt * sizeof(pt_entry_t));
d619 1
a619 2
	KDASSERT((vaddr_t)sdt == (vaddr_t)pdt);
	DPRINTF(CD_BOOT, ("kernel pdt %p", pdt));
d621 1
d624 1
a624 1
		*pdt++ = PG_NV;
d628 1
a628 1
		*pdt++ = pa | PG_SO | PG_RO | PG_W | PG_V;
d632 1
a632 1
	for (i = atop(sptpa) - atop(pa); i != 0; i--) {
d634 1
a634 1
		*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V | CACHE_WT;
d636 1
a636 1
		*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V;
d640 9
d652 4
a655 7
	for (i = atop(eptpa) - atop(pa); i != 0; i--) {
		*pdt++ = pa | template;
		pa += PAGE_SIZE;
	}
	/* regular memory */
	for (i = atop(avail_end) - atop(pa); i != 0; i--) {
		*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_V;
a657 3
	DPRINTF(CD_BOOT, ("-%p, pa %08x\n", pdt, pa));
	for (i = (pt_entry_t *)round_page((vaddr_t)pdt) - pdt; i != 0; i--)
		*pdt++ = PG_NV;
d666 6
a671 2
	if (e_rom != s_rom)
		pmap_map(s_rom, e_rom - s_rom, UVM_PROT_RW, CACHE_INH);
d694 1
a694 1
	pmap_kernel()->pm_apr = sptpa | kernel_apr;
d697 1
a697 1
	    kernel_apr, sptpa));
d714 3
d718 13
@


1.71
log
@Extend cmmu routines to return the caching mode to use for page tables.

Alter the 88200-specific code to enforce cache-inhibited page tables for
extremely old 88200 versions, and to disable write-back caching on systems
where xmem instructions do not behave correctly when applied to write-back
cached addresses.

No change introduced on 88110 systems, as well as most 88100 systems; the
affected systems are 88100 systems with 88100 revision < 10 and/or 88200
revision < 7; that is, only early MVME181 and MVME188 (not 188A) systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2013/02/19 21:02:06 miod Exp $	*/
d147 1
d1088 1
a1088 3
			if (KERNEL_APR_CMODE == CACHE_DFL ||
			    USERLAND_APR_CMODE == CACHE_DFL)
				cmmu_dcache_wb(cpu_number(), pa, PAGE_SIZE);
d1193 4
a1196 7
					if (KERNEL_APR_CMODE == CACHE_DFL &&
					    (opte & (CACHE_INH | CACHE_WT)) ==
					    0) {
						cmmu_dcache_wb(cpu_number(),
						    ptoa(PG_PFNUM(opte)),
						    PAGE_SIZE);
					}
d1615 24
d1650 3
d1664 3
a1666 2
				if (KERNEL_APR_CMODE == CACHE_DFL)
					cmmu_dcache_wb(ci->ci_cpuid, pa, count);
d1763 2
a1764 1
					else if (KERNEL_APR_CMODE == CACHE_DFL)
@


1.70
log
@Introduce a new cmmu method to return the preferred cache mode bits for the
kernel APR. Return write-back for every design but those involving 88410,
where write through is returned.

Apparently the use of writeback on single-processor kernels using 88410 (197SP,
197DP) has only been working by fat chance, and the last two years of uvm
changes, as well as the switch to ELF (causing kernel rodata to move `up')
exposes silent memory corruption on (88410-size) aliased addresses.
(I am guilty of not using my 197DP board much after making 197LE write-back
capable, as 197LE turned out to be faster and more stable, for I would have
noticed this earlier).

Further thought needs to happen about this. It might be possible to switch to
writeback by default again as long as bus_dma maps things write-through on
88410 designs, and perhaps with a part of the kernel mapped with a write-through
BATC, since BATC have precedence upon page tables. Right now I'm trying to get
a stable release out of the door.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2013/02/14 05:50:49 miod Exp $	*/
d136 3
a138 2
/* #define	CACHE_PT	(CPU_IS88100 ? CACHE_INH : CACHE_WT) */
#define	CACHE_PT	CACHE_WT
d140 2
a141 8
#if defined(M88110) && defined(M88410)
apr_t	kernel_apr_cmode = CACHE_DFL;		/* might be downgraded to WT */
#define	KERNEL_APR_CMODE	kernel_apr_cmode
#else
#define	KERNEL_APR_CMODE	CACHE_DFL
#endif
#define	USERLAND_APR_CMODE	CACHE_DFL
apr_t	default_apr = CACHE_GLOBAL | APR_V;
d411 1
a411 2
	/* memory for page tables should not be writeback */
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_PT);
d446 1
a446 2
	/* memory for page tables should not be writeback */
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_PT);
d601 2
a602 1
	template = PG_SO | PG_RW | PG_M_U | PG_W | PG_V | CACHE_PT;
d631 12
a645 10
#if defined(M88110)
	if (CPU_IS88110) {
#if defined(M88410)
		kernel_apr_cmode = cmmu_kapr_cmode();
#endif
#ifndef MULTIPROCESSOR
		default_apr &= ~CACHE_GLOBAL;
#endif
	}
#endif
d647 1
a647 1
	pmap_kernel()->pm_apr = sptpa | default_apr | KERNEL_APR_CMODE;
d650 1
a650 1
	    default_apr, sptpa));
d710 1
a710 2
	/* memory for page tables should not be writeback */
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CACHE_PT);
d713 1
a713 1
	pmap->pm_apr = pa | default_apr | USERLAND_APR_CMODE;
@


1.69
log
@Be sure to writeback D$ in addition to invalidate I$ in pmap_proc_iflush(),
or I$ may reload with stale data. It used to work without this because the
kernel used to run in write-through mode.
Repairs gdb single-stepping.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2011/10/25 18:38:06 miod Exp $	*/
d139 1
a139 1
#if defined(M88110) && defined(MULTIPROCESSOR)
d642 4
a645 5
#ifdef MULTIPROCESSOR
		/* XXX until whatever causes the kernel to hang without
		   XXX is understood and fixed */
		kernel_apr_cmode = CACHE_WT;
#else
@


1.68
log
@Replace the naive 88110 tlb update code, which would always invalidate the
whole tlb (32 of 'em), with smarter `tlb probe and update with new pte if tlb
match found' code. This makes the 88110-specific pmap_update() unnecessary, as
updates are no longer aggregated to avoid the number of flushes. This also
makes tlb handling similar between 88100 and 88110, from the pmap's point of
view, so there is no need to use different routines.

No impact on 88100, no user-noticeable performance change on 88100 GENERIC,
slight improvement on 88110 GENERIC.MP.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2011/10/09 17:08:22 miod Exp $	*/
d1647 2
@


1.67
log
@Let BUS_DMA_COHERENT allocations return cache-inhibited pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2011/10/09 17:07:37 miod Exp $	*/
d288 2
a289 22
#ifdef M88100
void		 tlb_flush_88100(pmap_t, vaddr_t, pt_entry_t);
void		 tlb_kflush_88100(vaddr_t, pt_entry_t);
#endif
#ifdef M88110
void		 tlb_flush_88110(pmap_t, vaddr_t, pt_entry_t);
void		 tlb_kflush_88110(vaddr_t, pt_entry_t);
#endif
#if defined(M88100) && defined(M88110)
void		 (*tlb_flush_fnptr)(pmap_t, vaddr_t, pt_entry_t);
void		 (*tlb_kflush_fnptr)(vaddr_t, pt_entry_t);
#define		tlb_flush(pm,va,pte)	(*tlb_flush_fnptr)(pm,va,pte)
#define		tlb_kflush(va,pte)	(*tlb_kflush_fnptr)(va,pte)
#else
#ifdef M88100
#define		tlb_flush	tlb_flush_88100
#define		tlb_kflush	tlb_kflush_88100
#else
#define		tlb_flush	tlb_flush_88110
#define		tlb_kflush	tlb_kflush_88110
#endif
#endif
d293 2
a294 1
 * Flush translation cache entry for `va' in `pmap'. May act lazily.
a295 1
#ifdef M88100
d297 1
a297 1
tlb_flush_88100(pmap_t pmap, vaddr_t va, pt_entry_t pte)
a304 3
	/*
	 * On 88100, we take action immediately.
	 */
a310 21
		if (kernel || pmap == ci->ci_curpmap)
			cmmu_tlb_inv(ci->ci_cpuid, kernel, va);
	}
}
#endif
#ifdef M88110
void
tlb_flush_88110(pmap_t pmap, vaddr_t va, pt_entry_t pte)
{
	struct cpu_info *ci;
	boolean_t kernel = pmap == pmap_kernel();
#ifdef MULTIPROCESSOR
	CPU_INFO_ITERATOR cpu;
#endif

	/*
	 * On 88110, we only remember which tlb need to be invalidated,
	 * and wait for pmap_update() to do it.
	 */
#ifdef MULTIPROCESSOR		/* { */
	CPU_INFO_FOREACH(cpu, ci) {
d312 1
a312 1
			ci->ci_pmap_ipi |= CI_IPI_TLB_FLUSH_KERNEL;
d314 1
a314 1
			ci->ci_pmap_ipi |= CI_IPI_TLB_FLUSH_USER;
a315 7
#else	/* MULTIPROCESSOR */	/* } { */
	ci = curcpu();
	if (kernel)
		ci->ci_pmap_ipi |= CI_IPI_TLB_FLUSH_KERNEL;
	else if (pmap == ci->ci_curpmap)
		ci->ci_pmap_ipi |= CI_IPI_TLB_FLUSH_USER;
#endif	/* MULTIPROCESSOR */	/* } */
a316 1
#endif
d320 2
a321 1
 * Flush translation cache entry for `va' in pmap_kernel(). Acts immediately.
a322 20
#ifdef M88100
void
tlb_kflush_88100(vaddr_t va, pt_entry_t pte)
{
	struct cpu_info *ci;
#ifdef MULTIPROCESSOR
	CPU_INFO_ITERATOR cpu;
#endif

#ifdef MULTIPROCESSOR
	CPU_INFO_FOREACH(cpu, ci)
#else
	ci = curcpu();
#endif
	{
		cmmu_tlb_inv(ci->ci_cpuid, TRUE, va);
	}
}
#endif
#ifdef M88110
d324 1
a324 1
tlb_kflush_88110(vaddr_t va, pt_entry_t pte)
d333 1
a333 1
		cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0);
d337 1
a337 1
	cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0);
a339 39
#endif

#ifdef M88110
/*
 * [MI]
 * Perform pending lazy tlb invalidates.
 */
void
pmap_update(pmap_t pm)
{
	/*
	 * Time to perform all necessary TLB invalidations.
	 */
#ifdef M88100
	if (CPU_IS88110) {
#endif
		u_int ipi;
		struct cpu_info *ci;
#ifdef MULTIPROCESSOR
		CPU_INFO_ITERATOR cpu;
#endif

#ifdef MULTIPROCESSOR
		CPU_INFO_FOREACH(cpu, ci)
#else
		ci = curcpu();
#endif
		{
			ipi = atomic_clear_int(&ci->ci_pmap_ipi);
			if (ipi & CI_IPI_TLB_FLUSH_KERNEL)
				cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0);
			if (ipi & CI_IPI_TLB_FLUSH_USER)
				cmmu_tlb_inv(ci->ci_cpuid, FALSE, 0);
		}
#ifdef M88100
	}
#endif
}
#endif
d358 1
a358 1
			cmmu_tlb_inv_all(ci->ci_cpuid);
a550 13

	/*
	 * Initialize function pointers depending on the CPU type
	 */
#if defined(M88100) && defined(M88110)
	if (CPU_IS88100) {
		tlb_flush_fnptr = tlb_flush_88100;
		tlb_kflush_fnptr = tlb_kflush_88100;
	} else {
		tlb_flush_fnptr = tlb_flush_88110;
		tlb_kflush_fnptr = tlb_kflush_88110;
	}
#endif
@


1.66
log
@Finally fix the kernel mode apr to remove the forced write-through cache
control, and enjoy the joys of write back (16% performance improvement for
non-cpu bound workloads).

While there, there is no need to map the page tables cache inhibited on
88100 systems; they only need to be write-through, and snooping will do the
rest.

Kernel WB is still disabled on 88110 SMP kernels, until the last MP bootstrap
bug is understood and fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2011/10/09 17:04:07 miod Exp $	*/
d1885 49
@


1.65
log
@Split the tlb invalidate routines into 88100 and 88110 specific routines,	and use function pointers to invoke them (if they can't be choosen at
compile-time depending upon the kernel configuration).
The routines now get the new pte value as an extra argument; they don't use
it yet, but code I am working on will soon on 88110.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2011/10/09 17:01:34 miod Exp $	*/
d132 14
a145 2
apr_t	kernel_apr_cmode = CACHE_WT;	/* XXX CACHE_DFL does not work yet */
apr_t	userland_apr_cmode = CACHE_DFL;
d527 1
a527 1
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CPU_IS88100 ? CACHE_INH : CACHE_WT);
d563 1
a563 1
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CPU_IS88100 ? CACHE_INH : CACHE_WT);
d721 1
a721 1
	/* kernel data */
d723 3
d727 1
d731 1
a731 2
	template = PG_SO | PG_RW | PG_M_U | PG_W | PG_V |
	    (CPU_IS88100 ? CACHE_INH : CACHE_WT);
d763 7
a769 2
#if !defined(MULTIPROCESSOR) && defined(M88110)
	if (CPU_IS88110)
d772 2
d775 1
a775 1
	pmap_kernel()->pm_apr = sptpa | default_apr | kernel_apr_cmode;
d839 1
a839 1
	pmap_cache_ctrl(pa, pa + PAGE_SIZE, CPU_IS88100 ? CACHE_INH : CACHE_WT);
d842 1
a842 1
	pmap->pm_apr = pa | default_apr | userland_apr_cmode;
d1208 11
a1218 12
#ifdef M88100
			if (CPU_IS88100 &&
			    kernel_apr_cmode != userland_apr_cmode) {
				/* XXX Why isn't cmmu_dcache_wb() enough? */
				if (0)
					cmmu_dcache_wb(cpu_number(),
					    pa, PAGE_SIZE);
				else
					cmmu_cache_wbinv(cpu_number(),
					    pa, PAGE_SIZE);
			}
#endif
d1318 12
a1470 5
#ifdef M88100
	if (CPU_IS88100 &&
	    kernel_apr_cmode != userland_apr_cmode)
		cmmu_dcache_wb(cpu_number(), src, PAGE_SIZE);
#endif
d1472 3
d1488 3
d1866 1
a1866 1
					else
@


1.64
log
@Rework secondary processor initialization. cmmu initialization is now
performed much earlier in the processor startup.
No visible change, paves the way for the much important diff three commits
from here.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2011/01/05 22:20:22 miod Exp $	*/
a148 2
void		 tlb_flush(pmap_t, vaddr_t);
void		 tlb_kflush(vaddr_t);
a195 60
 * [MD PUBLIC]
 * Change the cache control bits of the address range `sva'..`eva' in
 * pmap_kernel to `mode'.
 */
void
pmap_cache_ctrl(vaddr_t sva, vaddr_t eva, u_int mode)
{
	int s;
	pt_entry_t opte, *pte;
	vaddr_t va;
	paddr_t pa;
	cpuid_t cpu;

	DPRINTF(CD_CACHE, ("pmap_cache_ctrl(%p, %p, %x)\n",
	    sva, eva, mode));

	s = splvm();
	for (va = sva; va != eva; va += PAGE_SIZE) {
		if ((pte = pmap_pte(pmap_kernel(), va)) == NULL)
			continue;
		DPRINTF(CD_CACHE, ("cache_ctrl: pte@@%p\n", pte));

		/*
		 * Data cache should be copied back and invalidated if
		 * the old mapping was cached and the new isn't, or if
		 * we are downgrading from writeback to writethrough.
		 */
		if (((*pte & CACHE_INH) == 0 && (mode & CACHE_INH) != 0) ||
		    ((*pte & CACHE_WT) == 0 && (mode & CACHE_WT) != 0)) {
			pa = ptoa(PG_PFNUM(*pte));
#ifdef MULTIPROCESSOR
			for (cpu = 0; cpu < MAX_CPUS; cpu++)
				if (ISSET(m88k_cpus[cpu].ci_flags, CIF_ALIVE)) {
#else
			cpu = cpu_number();
#endif
					if (mode & CACHE_INH)
						cmmu_cache_wbinv(cpu,
						    pa, PAGE_SIZE);
					else
						cmmu_dcache_wb(cpu,
						    pa, PAGE_SIZE);
#ifdef MULTIPROCESSOR
				}
#endif
		}

		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 */

		opte = invalidate_pte(pte);
		*pte = (opte & ~CACHE_MASK) | mode;
		tlb_kflush(va);
	}
	splx(s);
}

/*
d276 23
d303 1
d305 1
a305 1
tlb_flush(pmap_t pmap, vaddr_t va)
d309 1
a309 2

#ifdef MULTIPROCESSOR	/* { */
d311 1
d316 8
a323 5
	if (CPU_IS88100) {
		CPU_INFO_FOREACH(cpu, ci) {
			if (kernel || pmap == ci->ci_curpmap)
				cmmu_tlb_inv(ci->ci_cpuid, kernel, va);
		}
d325 11
d341 6
a346 7
	if (CPU_IS88110) {
		CPU_INFO_FOREACH(cpu, ci) {
			if (kernel)
				ci->ci_pmap_ipi |= CI_IPI_TLB_FLUSH_KERNEL;
			else if (pmap == ci->ci_curpmap)
				ci->ci_pmap_ipi |= CI_IPI_TLB_FLUSH_USER;
		}
d350 4
a353 8

	if (kernel || pmap == ci->ci_curpmap) {
		if (CPU_IS88100)
			cmmu_tlb_inv(ci->ci_cpuid, kernel, va);
		if (CPU_IS88110)
			ci->ci_pmap_ipi |= kernel ?
			    CI_IPI_TLB_FLUSH_KERNEL : CI_IPI_TLB_FLUSH_USER;
	}
d356 1
d362 1
d364 1
a364 1
tlb_kflush(vaddr_t va)
d367 3
d371 16
a386 1
#ifdef MULTIPROCESSOR	/* { */
d388 1
d390 4
a393 6
	if (CPU_IS88100)
		CPU_INFO_FOREACH(cpu, ci)
			cmmu_tlb_inv(ci->ci_cpuid, TRUE, va);
	if (CPU_IS88110)
		CPU_INFO_FOREACH(cpu, ci)
			cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0);
d396 1
a396 5

	if (CPU_IS88100)
		cmmu_tlb_inv(ci->ci_cpuid, TRUE, va);
	if (CPU_IS88110)
		cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0);
d399 1
d416 1
a417 1
		struct cpu_info *ci;
d419 1
d421 1
d424 1
a424 1
		struct cpu_info *ci = curcpu();
d426 1
a426 1
		/* CPU_INFO_FOREACH(cpu, ci) */ {
d604 1
a604 1
	pt_entry_t template, *pte;
d614 1
a614 1
	template = m88k_protection(prot) | cmode | PG_W | PG_V;
d617 1
a617 1
		template |= PG_M;
d626 1
a626 1
		*pte = template | pa;
d650 13
d920 1
a920 1
	pt_entry_t *pte, template;
d929 1
a929 1
	template = m88k_protection(prot);
d991 1
a991 1
					tlb_flush(pmap, va);
d1016 1
a1016 1
	template |= PG_V;
d1018 1
a1018 1
		template |= PG_W;
d1027 1
a1027 1
			template |= PG_U;
d1029 1
a1029 1
			template |= PG_M_U;
d1031 1
a1031 1
		template |= PG_U;
d1037 1
a1037 1
		template |= CACHE_INH;
d1044 5
a1048 4
	template |= invalidate_pte(pte) & PG_M_U;
	*pte = template | pa;
	tlb_flush(pmap, va);
	DPRINTF(CD_ENT, ("pmap_enter: new pte %p\n", *pte));
d1075 1
a1075 1
	pt_entry_t template, *pte;
d1079 1
a1079 1
	template = m88k_protection(prot) | PG_W | PG_V;
d1082 1
a1082 1
		template |= PG_M;
d1088 1
a1088 1
		template |= CACHE_INH;
d1103 3
a1105 2
	*pte = template | pa;
	tlb_kflush(va);
d1140 1
a1140 1
		tlb_flush(pmap, va);
d1276 1
a1276 1
		pt_entry_t *pte;
d1295 2
a1296 2
					invalidate_pte(pte);
					tlb_kflush(va);
d1357 1
a1357 1
	pt_entry_t *pte, ap;
d1393 4
a1396 3
					*pte = ap |
					    (invalidate_pte(pte) & ~PG_PROT);
					tlb_flush(pmap, va);
d1516 1
a1516 1
				tlb_flush(pmap, va);
d1593 1
a1593 1
	pt_entry_t *pte, opte;
d1641 3
a1643 2
				*pte = opte ^ bit;
				tlb_flush(pmap, va);
d1791 61
@


1.63
log
@Pass pmap_bootstrap() the memory range used by the PROM/BUG/firmware/whatever,
instead of assuming it is ``everything below the kernel image''.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2011/01/05 22:18:46 miod Exp $	*/
a765 6
	/* Invalidate entire kernel TLB and get ready for address translation */
#ifdef MULTIPROCESSOR
	if (cpu != master_cpu)
		cmmu_initialize_cpu(cpu);
#endif

@


1.62
log
@Minor optimizations:
- move pte validity checks from pmap_remove_pte() to its callers.
- when iterating over a segment, compute pte address once and increment it
  in the loop, instead of recomputing it on every iteration.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2011/01/05 22:16:16 miod Exp $	*/
d620 1
a620 1
	if (pa < VM_MAX_KERNEL_ADDRESS)
d649 1
a649 1
pmap_bootstrap()
d655 2
a656 2
	pt_entry_t *pdt;
	paddr_t pa, epdtpa;
d684 1
d687 1
a687 1
	epdtpa = pa + round_page(npdt * sizeof(pt_entry_t));
d697 26
a722 17
	for (pa = 0; pa != avail_end; pa += PAGE_SIZE) {
		if (s_text != 0 && pa < s_text)
			/* PROM */
			*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V | CACHE_INH;
		else if (pa < e_text)
			/* kernel text */
			*pdt++ = pa | PG_SO | PG_RO | PG_W | PG_V;
		else if (pa < (paddr_t)pmap_kernel()->pm_stab)
			/* kernel data */
			*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V;
		else if (pa < epdtpa) {
			/* kernel page tables */
			*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_W | PG_V |
			    (CPU_IS88100 ? CACHE_INH : CACHE_WT);
		} else
			/* regular memory */
			*pdt++ = pa | PG_SO | PG_RW | PG_M_U | PG_V;
d725 1
a725 1
	while (((vaddr_t)pdt & ~PG_FRAME) != 0)
d735 2
d739 1
a739 1
			(void)pmap_map(ptable->start, ptable->size,
d751 1
a751 2
	pmap_kernel()->pm_apr = ((vaddr_t)pmap_kernel()->pm_stab) |
	    default_apr | kernel_apr_cmode;
d754 1
a754 1
	    default_apr, pmap_kernel()->pm_apr));
@


1.61
log
@Make copypage() and zeropage() per-cpu function pointers, and use a
different version on 88110, which does load allocate of
to-be-completely-overwritten cache lines.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2011/01/05 22:14:29 miod Exp $	*/
d961 2
a962 1
		pmap_remove_pte(pmap, va, pte, NULL, FALSE);
a1115 3
	if (pte == NULL || !PDT_VALID(pte))
		return;	 	/* no page mapping, nothing to do! */

d1148 2
a1149 1
		panic("pmap_remove_pte: null pv_list");
d1159 3
a1161 3
		panic("pmap_remove_pte: mapping for va "
		    "0x%lx (pa 0x%lx) not in pv list at %p",
		    va, pa, pvl);
d1207 1
d1227 1
d1229 3
a1231 2
				pmap_remove_pte(pmap, va, sdt_pte(sdt, va),
				    NULL, TRUE);
d1233 1
d1279 1
d1281 1
a1281 2
				pte = sdt_pte(sdt, va);
				if (pte != NULL && PDT_VALID(pte)) {
d1290 1
d1375 1
d1377 1
a1377 2
				pte = sdt_pte(sdt, va);
				if (pte != NULL && PDT_VALID(pte)) {
d1389 1
@


1.60
log
@Now that pmap_copy_page() no longer needs to flush a couple contiguous tlb
entries, drop the count parameter to cmmu_tlb_inv(), and introduce
cmmu_tlb_inv_all() to drop all user tlb entries (to be used during context
switches).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2011/01/02 13:40:07 miod Exp $	*/
a1419 1
	extern void copypage(vaddr_t, vaddr_t);
d1428 1
a1428 1
	copypage((vaddr_t)src, (vaddr_t)dst);
a1438 1
	extern void zeropage(vaddr_t);
d1441 1
a1441 1
	zeropage((vaddr_t)pa);
@


1.59
log
@Kill pmap_table_t typedef.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2010/12/31 21:38:08 miod Exp $	*/
d357 1
a357 1
				cmmu_tlb_inv(ci->ci_cpuid, kernel, va, 1);
d378 1
a378 1
			cmmu_tlb_inv(ci->ci_cpuid, kernel, va, 1);
d400 1
a400 1
			cmmu_tlb_inv(ci->ci_cpuid, TRUE, va, 1);
d403 1
a403 1
			cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0 ,0);
d408 1
a408 1
		cmmu_tlb_inv(ci->ci_cpuid, TRUE, va, 1);
d410 1
a410 1
		cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0 ,0);
d440 1
a440 1
				cmmu_tlb_inv(ci->ci_cpuid, TRUE, 0 ,0);
d442 1
a442 1
				cmmu_tlb_inv(ci->ci_cpuid, FALSE, 0 ,0);
d467 1
a467 1
			cmmu_tlb_inv(ci->ci_cpuid, FALSE, 0, -1);
a758 1
	else
a759 1
		cmmu_tlb_inv(cpu, TRUE, 0, -1);
@


1.58
log
@Massive overhauling of the m88k pmap, though I can't pretend it's a new pmap
since a large part of the structures and logic remains.

Since m88k has separate supervisor/user spaces, we can map physical memory 1:1
in supervisor space, and have the kernel virtual address space start from the
end of physical memory.

This allows us to switch to __HAVE_PMAP_DIRECT. And to get rid of the double
mapped sdt, since now their virtual and physical addresses will always match.

The upper bound of the kernel virtual memory space is now platform dependent,
until the code which relies upon some hardware devices being mapped 1:1 in
supervisor mode is updated to no longer require this (this is mainly a PITA on
luna88k, where onboard devices start at 0x40000000, leaving only 1GB of KVA at
the moment - still much better than the previous 512MB).

Tested on mvme88k only (187, 188, 197LE, 197DP). Other platforms ought to
work, aviion will be checked shortly and fixed if necessary. No known
OpenBSD/luna88k system in working condition at the moment.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2010/12/31 21:22:33 miod Exp $	*/
d657 1
a657 1
	pmap_table_t ptable;
@


1.57
log
@Handle writethrough mappings (as opposed to writeback) better:
- a writeback->writethrough downgrade only requires a cache writeback, not a
  writeback and invalidate.
- apparently the 88200 does not behave correctly (i.e. according to its
  documentation) when a given page is accessed with both writethrough and
  writeback cacheability (even though the cache lines are physically indexed
  and physically tagged), so make sure to force writeback of writeback pages
  when they risk being reused as writethrough (hair_count -= MAXHAIR).

Better coherency in page table backing store cacheability: these need to be
cache invalidated on 88100+88200 (because of hardware update of modified and
used pte bits), and writethrough on 88110.
@
text
@d1 17
a17 1
/*	$OpenBSD: pmap.c,v 1.56 2010/12/31 20:54:21 miod Exp $	*/
a18 1
 * Copyright (c) 2001-2004, Miodrag Vallat
a65 1
#include <sys/malloc.h>
d67 2
a68 1
#include <sys/msgbuf.h>
a73 1
#include <machine/lock.h>
d78 3
a80 2

#include <uvm/uvm.h>
d85 5
a89 3
extern vaddr_t	avail_start;
extern vaddr_t	virtual_avail, virtual_end;
extern vaddr_t	last_addr;
a92 4
 * Static variables, functions and variables for debugging
 */

/*
d95 27
a121 1
#define CD_FULL		0x02
d123 1
a123 21
#define CD_ACTIVATE	0x0000004	/* pmap_activate */
#define CD_KMAP		0x0000008	/* pmap_expand_kmap */
#define CD_MAP		0x0000010	/* pmap_map */
#define CD_CACHE	0x0000020	/* pmap_cache_ctrl */
#define CD_INIT		0x0000080	/* pmap_init */
#define CD_CREAT	0x0000100	/* pmap_create */
#define CD_FREE		0x0000200	/* pmap_release */
#define CD_DESTR	0x0000400	/* pmap_destroy */
#define CD_RM		0x0000800	/* pmap_remove */
#define CD_RMPG		0x0001000	/* pmap_remove_page */
#define CD_PROT		0x0002000	/* pmap_protect */
#define CD_EXP		0x0004000	/* pmap_expand */
#define CD_ENT		0x0008000	/* pmap_enter */
#define CD_UPD		0x0010000	/* pmap_update */
#define CD_COL		0x0020000	/* pmap_collect */
#define CD_CBIT		0x0040000	/* pmap_changebit */
#define CD_TBIT		0x0080000	/* pmap_testbit */
#define CD_USBIT	0x0100000	/* pmap_unsetbit */
#define	CD_COPY		0x0200000	/* pmap_copy_page */
#define	CD_ZERO		0x0400000	/* pmap_zero_page */
#define CD_ALL		0x0fffffc
d125 1
a125 1
int pmap_debug = 0;
a129 4

caddr_t vmmap;
pt_entry_t *vmpte, *msgbufmap;

a130 1
pmap_t kernel_pmap = &kernel_pmap_store;
d136 15
a150 5
typedef struct kpdt_entry *kpdt_entry_t;
struct kpdt_entry {
	kpdt_entry_t	next;
	paddr_t		phys;
};
d152 5
a156 1
kpdt_entry_t	kpdt_free;
d159 1
a159 2
 * Two pages of scratch space per cpu.
 * Used in pmap_copy_page() and pmap_zero_page().
a160 1
vaddr_t phys_map_vaddr, phys_map_vaddr_end;
d162 17
a178 1
static pv_entry_t pg_to_pvh(struct vm_page *);
d180 7
a186 2
static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
d188 7
a194 1
	return &pg->mdpage.pvent;
d198 3
a200 1
 *	Locking primitives
d202 17
d220 11
a230 3
#if defined(MULTIPROCESSOR) && 0
#define	PMAP_LOCK(pmap)		__cpu_simple_lock(&(pmap)->pm_lock)
#define	PMAP_UNLOCK(pmap)	__cpu_simple_unlock(&(pmap)->pm_lock)
d232 1
a232 2
#define	PMAP_LOCK(pmap)		do { /* nothing */ } while (0)
#define	PMAP_UNLOCK(pmap)	do { /* nothing */ } while (0)
d234 15
d250 6
a255 1
vaddr_t kmapva = 0;
d258 4
a261 1
 * Internal routines
d263 16
a278 11
void	flush_atc_entry(pmap_t, vaddr_t);
pt_entry_t *pmap_expand_kmap(vaddr_t, vm_prot_t, int);
void	pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *, boolean_t);
void	pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
void	pmap_expand(pmap_t, vaddr_t);
void	pmap_release(pmap_t);
vaddr_t	pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, u_int);
pt_entry_t *pmap_pte(pmap_t, vaddr_t);
void	pmap_remove_page(struct vm_page *);
void	pmap_changebit(struct vm_page *, int, int);
boolean_t pmap_testbit(struct vm_page *, int);
d281 6
a286 1
 * quick PTE field checking macros
d288 40
a327 2
#define	pmap_pte_w(pte)		(*(pte) & PG_W)
#define	pmap_pte_prot(pte)	(*(pte) & PG_PROT)
d329 1
a329 2
#define	pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define	pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))
d331 2
a332 1
#define	m88k_protection(prot)	((prot) & VM_PROT_WRITE ? PG_RW : PG_RO)
d334 3
a336 1
#define SDTENT(map, va)		((sdt_entry_t *)((map)->pm_stab + SDTIDX(va)))
d339 2
a340 8
 * Routine:	FLUSH_ATC_ENTRY
 *
 * Function:
 *	Flush atc (TLB) which maps given pmap and virtual address.
 *
 * Parameters:
 *	pmap	affected pmap
 *	va	virtual address that should be flushed
d343 1
a343 1
flush_atc_entry(pmap_t pmap, vaddr_t va)
d346 1
a346 1
	boolean_t kernel = pmap == kernel_pmap;
d386 28
d415 4
d425 1
d427 1
d432 1
a432 1
		
d444 1
d446 1
d451 2
a452 14
 * Routine:	PMAP_PTE
 *
 * Function:
 *	Given a map and a virtual address, compute a (virtual) pointer
 *	to the page table entry (PTE) which maps the address .
 *	If the page table associated with the address does not
 *	exist, NULL is returned (and the map may need to grow).
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	virt	virtual address for which page table entry is desired
 *
 *    Otherwise the page table address is extracted from the segment table,
 *    the page table index is added, and the result is returned.
d454 18
d473 6
a478 3
static __inline__
pt_entry_t *
sdt_pte(sdt_entry_t *sdt, vaddr_t va)
d480 3
a482 2
	return ((pt_entry_t *)
	    (PG_PFNUM(*(sdt + SDT_ENTRIES)) << PDT_SHIFT) + PDTIDX(va));
d485 13
d499 1
a499 1
pmap_pte(pmap_t pmap, vaddr_t virt)
d502 2
d505 1
a505 6
	sdt = SDTENT(pmap, virt);
	/*
	 * Check whether page table exists.
	 */
	if (!SDT_VALID(sdt))
		return (NULL);
d507 23
a529 1
	return (sdt_pte(sdt, virt));
d533 7
a539 27
 * Routine:	PMAP_EXPAND_KMAP (internal)
 *
 * Function:
 *    Allocate a page descriptor table (pte_table) and validate associated
 * segment table entry, returning pointer to page table entry. This is
 * much like 'pmap_expand', except that table space is acquired
 * from an area set up by pmap_bootstrap, instead of through
 * uvm_km_zalloc. (Obviously, because uvm_km_zalloc uses the kernel map
 * for allocation - which we can't do when trying to expand the
 * kernel map!) Note that segment tables for the kernel map were
 * all allocated at pmap_bootstrap time, so we only need to worry
 * about the page table here.
 *
 * Parameters:
 *	virt	VA for which translation tables are needed
 *	prot	protection attributes for segment entries
 *
 * Extern/Global:
 *	kpdt_free	kernel page table free queue
 *
 * This routine simply dequeues a table from the kpdt_free list,
 * initializes all its entries (invalidates them), and sets the
 * corresponding segment table entry to point to it. If the kpdt_free
 * list is empty - we panic (no other places to get memory, sorry). (Such
 * a panic indicates that pmap_bootstrap is not allocating enough table
 * space for the kernel virtual address space).
 *
d542 1
a542 1
pmap_expand_kmap(vaddr_t virt, vm_prot_t prot, int canfail)
d544 3
a546 7
	sdt_entry_t template, *sdt;
	kpdt_entry_t kpdt_ent;

#ifdef PMAPDEBUG
	if ((pmap_debug & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
		printf("pmap_expand_kmap(%p, %x, %d)\n", virt, prot, canfail);
#endif
d548 1
a548 1
	template = m88k_protection(prot) | PG_M | SG_V;
d550 5
a554 9
	/* segment table entry derivate from map and virt. */
	sdt = SDTENT(kernel_pmap, virt);
#ifdef PMAPDEBUG
	if (SDT_VALID(sdt))
		panic("pmap_expand_kmap: segment table entry VALID");
#endif

	kpdt_ent = kpdt_free;
	if (kpdt_ent == NULL) {
d556 2
a557 3
			return (NULL);
		else
			panic("pmap_expand_kmap: Ran out of kernel pte tables");
d560 3
a562 5
	kpdt_free = kpdt_free->next;
	/* physical table */
	*sdt = kpdt_ent->phys | template;
	/* virtual table */
	*(sdt + SDT_ENTRIES) = (vaddr_t)kpdt_ent | template;
d564 1
a564 2
	/* Reinitialize this kpdt area to zero */
	bzero((void *)kpdt_ent, PDT_SIZE);
d566 1
a566 1
	return (pt_entry_t *)(kpdt_ent) + PDTIDX(virt);
d570 8
a577 36
 * Routine:	PMAP_MAP
 *
 * Function:
 *    Map memory at initialization. The physical addresses being
 * mapped are not managed and are never unmapped.
 *
 * Parameters:
 *	virt	virtual address of range to map
 *	start	physical address of range to map
 *	end	physical address of end of range
 *	prot	protection attributes
 *	cmode	cache control attributes
 *
 * Calls:
 *	pmap_pte
 *	pmap_expand_kmap
 *
 * Special Assumptions
 *	For now, VM is already on, only need to map the specified
 * memory. Used only by pmap_bootstrap() and vm_page_startup().
 *
 * For each page that needs mapping:
 *	pmap_pte is called to obtain the address of the page table
 *	table entry (PTE). If the page table does not exist,
 *	pmap_expand_kmap is called to allocate it. Finally, the page table
 *	entry is set to point to the physical page.
 *
 *	initialize template with paddr, prot, dt
 *	look for number of phys pages in range
 *	{
 *		pmap_pte(virt)	- expand if necessary
 *		stuff pte from template
 *		increment virt one page
 *		increment template paddr one page
 *	}
 *
d580 1
a580 1
pmap_map(vaddr_t virt, paddr_t start, paddr_t end, vm_prot_t prot, u_int cmode)
d582 5
a586 4
	u_int npages;
	u_int num_phys_pages;
	pt_entry_t template, *pte;
	paddr_t	 page;
d588 4
a591 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_MAP)
		printf("pmap_map(%p, %p, %p, %x, %x)\n",
		    virt, start, end, prot, cmode);
d594 29
a622 4
#ifdef PMAPDEBUG
	/* Check for zero if we map the very end of the address space... */
	if (start > end && end != 0)
		panic("pmap_map: start greater than end address");
d631 5
a635 6
	page = trunc_page(start);
	npages = atop(round_page(end) - page);
	for (num_phys_pages = npages; num_phys_pages != 0; num_phys_pages--) {
		if ((pte = pmap_pte(kernel_pmap, virt)) == NULL)
			pte = pmap_expand_kmap(virt,
			    VM_PROT_READ | VM_PROT_WRITE, 0);
d637 4
a640 11
#ifdef PMAPDEBUG
		if ((pmap_debug & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			if (PDT_VALID(pte))
				printf("pmap_map: pte @@%p already valid\n", pte);
#endif

		*pte = template | page;
		virt += PAGE_SIZE;
		page += PAGE_SIZE;
		kernel_pmap->pm_stats.resident_count++;
		kernel_pmap->pm_stats.wired_count++;
a641 1
	return virt;
d645 2
a646 22
 * Routine:	PMAP_CACHE_CONTROL
 *
 * Function:
 *	Set the cache-control bits in the page table entries(PTE) which maps
 *	the specified virtual address range.
 *
 * Parameters:
 *	pmap_t		pmap
 *	vaddr_t		s
 *	vaddr_t		e
 *	u_int		mode
 *
 * Calls:
 *	pmap_pte
 *	invalidate_pte
 *	flush_atc_entry
 *
 *  This routine sequences through the pages of the specified range.
 * For each, it calls pmap_pte to acquire a pointer to the page table
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
 * Otherwise, the cache-control bits in the PTE's are adjusted as specified.
 *
d649 1
a649 1
pmap_cache_ctrl(pmap_t pmap, vaddr_t s, vaddr_t e, u_int mode)
d651 11
a661 5
	int spl;
	pt_entry_t opte, *pte;
	vaddr_t va;
	paddr_t pa;
	cpuid_t cpu;
d663 2
a664 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_CACHE)
		printf("pmap_cache_ctrl(%p, %p, %p, %x)\n", pmap, s, e, mode);
#endif
d666 51
a716 2
	spl = splvm();
	PMAP_LOCK(pmap);
d718 6
a723 15
	for (va = s; va != e; va += PAGE_SIZE) {
		if ((pte = pmap_pte(pmap, va)) == NULL)
			continue;
#ifdef PMAPDEBUG
		if ((pmap_debug & (CD_CACHE | CD_FULL)) == (CD_CACHE | CD_FULL))
			printf("cache_ctrl: pte@@%p\n", pte);
#endif
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 * XXX
		 */
		opte = invalidate_pte(pte);
		*pte = (opte & ~CACHE_MASK) | mode;
		flush_atc_entry(pmap, va);
d725 3
a727 213
		/*
		 * Data cache should be copied back and invalidated if
		 * the old mapping was cached, or if we are downgrading
		 * from writeback to writethrough.
		 */
		if (((opte & CACHE_INH) == 0 && (mode & CACHE_INH) != 0) ||
		    ((opte & CACHE_WT) == 0 && (mode & CACHE_WT) != 0)) {
			pa = ptoa(PG_PFNUM(opte));
#ifdef MULTIPROCESSOR
			for (cpu = 0; cpu < MAX_CPUS; cpu++)
				if (ISSET(m88k_cpus[cpu].ci_flags, CIF_ALIVE)) {
#else
			cpu = cpu_number();
#endif
					if (mode & CACHE_INH)
						cmmu_cache_wbinv(cpu, pa, PAGE_SIZE);
					else
						cmmu_dcache_wb(cpu, pa, PAGE_SIZE);
#ifdef MULTIPROCESSOR
				}
#endif
		}
	}
	PMAP_UNLOCK(pmap);
	splx(spl);
}

/*
 * Routine:	PMAP_BOOTSTRAP
 *
 * Function:
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, allocate the kernel
 *	translation table space, and map control registers
 *	and other IO addresses.
 *
 * Parameters:
 *	load_start	PA where kernel was loaded
 *
 * Extern/Global:
 *
 *	PAGE_SIZE	VM (software) page size
 *	etext		end of kernel text
 *	phys_map_vaddr	VA of page mapped arbitrarily for debug/IO
 *
 * Calls:
 *	__cpu_simple_lock_init
 *	pmap_map
 *
 *    The physical address 'load_start' is mapped at
 * VM_MIN_KERNEL_ADDRESS, which maps the kernel code and data at the
 * virtual address for which it was (presumably) linked. Immediately
 * following the end of the kernel code/data, sufficient page of
 * physical memory are reserved to hold translation tables for the kernel
 * address space.
 *
 *    A pair of virtual pages per cpu are reserved for debugging and
 * IO purposes. They are arbitrarily mapped when needed. They are used,
 * for example, by pmap_copy_page and pmap_zero_page.
 *
 *    This implementation also assumes that the space below the kernel
 * is reserved (typically from PROM purposes). We should ideally map it
 * read only except when invoking its services...
 */

void
pmap_bootstrap(vaddr_t load_start)
{
	kpdt_entry_t kpdt_virt;
	sdt_entry_t *kmap;
	vaddr_t vaddr, virt;
	paddr_t s_text, e_text, kpdt_phys;
	unsigned int kernel_pmap_size, pdt_size;
	int i;
	pmap_table_t ptable;
	extern void *etext;

#ifdef MULTIPROCESSOR
	__cpu_simple_lock_init(&kernel_pmap->pm_lock);
#endif

	/*
	 * Allocate the kernel page table from the front of available
	 * physical memory, i.e. just after where the kernel image was loaded.
	 */
	/*
	 * The calling sequence is
	 *    ...
	 *  pmap_bootstrap(&kernelstart, ...);
	 * kernelstart being the first symbol in the load image.
	 */

	avail_start = round_page(avail_start);
	virtual_avail = avail_start;

	/*
	 * Initialize kernel_pmap structure
	 */
	kernel_pmap->pm_count = 1;
	kmap = (sdt_entry_t *)(avail_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)virtual_avail;
	kmapva = virtual_avail;

	/*
	 * Reserve space for segment table entries.
	 * One for the regular segment table and one for the shadow table
	 * The shadow table keeps track of the virtual address of page
	 * tables. This is used in virtual-to-physical address translation
	 * functions. Remember, MMU cares only for physical addresses of
	 * segment and page table addresses. For kernel page tables, we
	 * really don't need this virtual stuff (since the kernel will
	 * be mapped 1-to-1) but for user page tables, this is required.
	 * Just to be consistent, we will maintain the shadow table for
	 * kernel pmap also.
	 */
	kernel_pmap_size = 2 * SDT_SIZE;

#ifdef PMAPDEBUG
	printf("kernel segment table size = %p\n", kernel_pmap_size);
#endif
	/* init all segment descriptors to zero */
	bzero(kernel_pmap->pm_stab, kernel_pmap_size);

	avail_start += kernel_pmap_size;
	virtual_avail += kernel_pmap_size;

	/* make sure page tables are page aligned!! XXX smurph */
	avail_start = round_page(avail_start);
	virtual_avail = round_page(virtual_avail);

	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = avail_start;
	kpdt_virt = (kpdt_entry_t)virtual_avail;

	/* Compute how much space we need for the kernel page table */
	pdt_size = atop(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS)
	    * sizeof(pt_entry_t);
	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		pdt_size += atop(ptable->size) * sizeof(pt_entry_t);
	pdt_size = round_page(pdt_size);
	kernel_pmap_size += pdt_size;
	avail_start += pdt_size;
	virtual_avail += pdt_size;

	/* init all page descriptors to zero */
	bzero((void *)kpdt_phys, pdt_size);
#ifdef PMAPDEBUG
	printf("kernel page start = %p\n", kpdt_phys);
	printf("kernel page table size = %p\n", pdt_size);
	printf("kernel page end = %p\n", avail_start);
	printf("kpdt_virt = %p\n", kpdt_virt);
#endif
	/*
	 * init the kpdt queue
	 */
	kpdt_free = kpdt_virt;
	for (i = pdt_size / PDT_SIZE; i != 0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vaddr_t)kpdt_virt + PDT_SIZE);
		kpdt_virt->phys = kpdt_phys;
		kpdt_virt = kpdt_virt->next;
		kpdt_phys += PDT_SIZE;
	}
	kpdt_virt->next = NULL; /* terminate the list */

	/*
	 * Map the kernel image into virtual space
	 */

	s_text = trunc_page(load_start);	/* paddr of text */
	e_text = round_page((vaddr_t)&etext);	/* paddr of end of text */

	/* map the PROM area */
	pmap_map(0, 0, s_text, VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);

	/* map the kernel text read only */
	pmap_map(s_text, s_text, e_text, VM_PROT_READ, 0);
	vaddr = pmap_map(e_text, e_text, (paddr_t)kmap,
	    VM_PROT_WRITE | VM_PROT_READ, 0);

	/*
	 * Map system segment & page tables - should be cache inhibited on
	 * 88200 systems, because of the hardware update of PG_M and PG_U
	 * bits in ptes.
	 */
	if (kmapva != vaddr) {
		while (vaddr < (virtual_avail - kernel_pmap_size))
			vaddr = round_page(vaddr + 1);
	}
	vaddr = pmap_map(vaddr, (paddr_t)kmap, avail_start,
	    VM_PROT_WRITE | VM_PROT_READ, CPU_IS88100 ? CACHE_INH : CACHE_WT);

	vaddr = pmap_bootstrap_md(vaddr);

	virtual_avail = round_page(virtual_avail);
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	/*
	 * Map two pages per cpu for copying/zeroing.
	 */

	phys_map_vaddr = virtual_avail;
	phys_map_vaddr_end = virtual_avail + 2 * (ncpusfound << PAGE_SHIFT);
	avail_start += 2 * (ncpusfound << PAGE_SHIFT);
	virtual_avail += 2 * (ncpusfound << PAGE_SHIFT);

	/*
	 * Create all the machine-specific mappings.
	 */

	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		if (ptable->size != 0) {
			pmap_map(ptable->virt_start, ptable->phys_start,
			    ptable->phys_start + ptable->size,
a728 27
		}

	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physical pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -
	 * else we will be referencing the wrong page.
	 */

#define	SYSMAP(c, p, v, n)	\
({ \
	v = (c)virt; \
	if ((p = pmap_pte(kernel_pmap, virt)) == NULL) \
		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE, 0); \
	virt += ((n) * PAGE_SIZE); \
})

	virt = virtual_avail;

	SYSMAP(caddr_t, vmpte, vmmap, 1);
	invalidate_pte(vmpte);

	SYSMAP(struct msgbuf *, msgbufmap, msgbufp, atop(MSGBUFSIZE));

	virtual_avail = virt;
d738 2
a739 1
	kernel_pmap->pm_apr = (atop((paddr_t)kmap) << PG_SHIFT) |
d742 3
d748 4
d764 1
a764 1
	cmmu_set_sapr(kernel_pmap->pm_apr);
d772 3
a774 11
 * Routine:	PMAP_INIT
 *
 * Function:
 *	Initialize the pmap module. It is called by vm_init, to initialize
 *	any structures that the pmap system needs to map virtual memory.
 *
 * Calls:
 *	pool_init
 *
 *   This routine does not really have much to do. It initializes
 * pools for pmap structures and pv_entry structures.
d779 1
a779 5
#ifdef PMAPDEBUG
	if (pmap_debug & CD_INIT)
		printf("pmap_init()\n");
#endif

d783 1
a783 1
} /* pmap_init() */
d786 1
a786 17
 * Routine:	PMAP_ZERO_PAGE
 *
 * Function:
 *	Zeroes the specified page.
 *
 * Parameters:
 *	pg		page to zero
 *
 * Extern/Global:
 *	phys_map_vaddr
 *
 * Special Assumptions:
 *	no locking required
 *
 *	This routine maps the physical pages at the 'phys_map' virtual
 * address set up in pmap_bootstrap. It flushes the TLB to make the new
 * mappings effective, and zeros all the bits.
a787 31
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	vaddr_t va;
	int spl;
	int cpu = cpu_number();
	pt_entry_t *pte;
	extern void zeropage(vaddr_t);

#ifdef PMAPDEBUG
	if (pmap_debug & CD_ZERO)
		printf("pmap_zero_page(%p) pa %p\n", pg, pa);
#endif

	va = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	pte = pmap_pte(kernel_pmap, va);

	spl = splvm();

	*pte = m88k_protection(VM_PROT_READ | VM_PROT_WRITE) |
	    PG_M /* 88110 */ | PG_U | PG_V | pa;

	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_tlb_inv(cpu, TRUE, va, 1);
	zeropage(va);
	splx(spl);
}
d790 2
a791 9
 * Routine:	PMAP_CREATE
 *
 * Function:
 *	Create and return a physical map. If the size specified for the
 *	map is zero, the map is an actual physical map, and may be referenced
 *	by the hardware. If the size specified is non-zero, the map will be
 *	used in software only, and is bounded by that size.
 *
 *  This routines allocates a pmap structure.
d797 2
a798 3
	sdt_entry_t *segdt;
	paddr_t stpa;
	u_int s;
d802 7
a808 4
	/*
	 * Allocate memory for *actual* segment table and *shadow* table.
	 */
	s = round_page(2 * SDT_SIZE);
d810 3
a812 3
	segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, s);
	if (segdt == NULL)
		panic("pmap_create: uvm_km_zalloc failure");
d814 3
a816 19
	/*
	 * Initialize pointer to segment table both virtual and physical.
	 */
	pmap->pm_stab = segdt;
	if (pmap_extract(kernel_pmap, (vaddr_t)segdt,
	    (paddr_t *)&stpa) == FALSE)
		panic("pmap_create: pmap_extract failed!");
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) |
	    default_apr | userland_apr_cmode;

#ifdef PMAPDEBUG
	if (pmap_debug & CD_CREAT)
		printf("pmap_create() -> pmap %p, pm_stab %p (pa %p)\n",
		    pmap, pmap->pm_stab, stpa);
#endif

	/* memory for page tables should not be writeback or local */
	pmap_cache_ctrl(kernel_pmap,
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CPU_IS88100 ? CACHE_INH : CACHE_WT);
d818 1
a818 15
	/*
	 * Initialize SDT_ENTRIES.
	 */
	/*
	 * There is no need to clear segment table, since uvm_km_zalloc
	 * provides us clean pages.
	 */

	/*
	 * Initialize pmap structure.
	 */
	pmap->pm_count = 1;
#ifdef MULTIPROCESSOR
	__cpu_simple_lock_init(&pmap->pm_lock);
#endif
d824 2
a825 20
 * Routine:	PMAP_RELEASE
 *
 *	Internal procedure used by pmap_destroy() to actualy deallocate
 *	the tables.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_pte
 *	uvm_km_free
 *
 * Special Assumptions:
 *	No locking is needed, since this is only called which the
 * 	pm_count field of the pmap structure goes to zero.
 *
 * This routine sequences of through the user address space, releasing
 * all translation table space back to the system using uvm_km_free.
 * The loops are indexed by the virtual address space
 * ranges represented by the table group sizes (1 << SDT_SHIFT).
d828 1
a828 1
pmap_release(pmap_t pmap)
d830 3
a832 3
	u_int sdt;		/* outer loop index */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
d834 8
a841 15
#ifdef PMAPDEBUG
	if (pmap_debug & CD_FREE)
		printf("pmap_release(%p)\n", pmap);
#endif

	/* segment table loop */
	for (sdt = VM_MIN_ADDRESS >> SDT_SHIFT;
	    sdt <= VM_MAX_ADDRESS >> SDT_SHIFT; sdt++) {
		if ((gdttbl = pmap_pte(pmap, sdt << SDT_SHIFT)) != NULL) {
#ifdef PMAPDEBUG
			if ((pmap_debug & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
				printf("pmap_release(%p) free page table %p\n",
				    pmap, gdttbl);
#endif
			uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
d843 3
a845 46
	}

	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
	sdttbl = pmap->pm_stab;		/* addr of segment table */
#ifdef PMAPDEBUG
	if ((pmap_debug & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
		printf("(pmap_release(%p) free segment table %p\n",
		    pmap, sdttbl);
#endif
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2 * SDT_SIZE));
}

/*
 * Routine:	PMAP_DESTROY
 *
 * Function:
 *	Retire the given physical map from service. Should only be called
 *	if the map contains no valid mappings.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_release
 *	pool_put
 *
 * Special Assumptions:
 *	Map contains no valid mappings.
 *
 *  This routine decrements the reference count in the pmap
 * structure. If it goes to zero, pmap_release is called to release
 * the memory space to the system. Then, call pool_put to free the
 * pmap structure.
 */
void
pmap_destroy(pmap_t pmap)
{
	int count;

	PMAP_LOCK(pmap);
	count = --pmap->pm_count;
	PMAP_UNLOCK(pmap);
	if (count == 0) {
		pmap_release(pmap);
a849 1

d851 2
a852 10
 * Routine:	PMAP_REFERENCE
 *
 * Function:
 *	Add a reference to the specified pmap.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Under a pmap read lock, the pm_count field of the pmap structure
 * is incremented. The function then returns.
a856 1
	PMAP_LOCK(pmap);
a857 1
	PMAP_UNLOCK(pmap);
d861 2
a862 28
 * Routine:	PMAP_REMOVE_PTE (internal)
 *
 * Function:
 *	Invalidate a given page table entry associated with the
 *	given virtual address.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	va		virtual address of page to remove
 *	pte		existing pte
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pool_put
 *	invalidate_pte
 *	flush_atc_entry
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *  If the PTE is valid, the routine must invalidate the entry. The
 * 'modified' bit, if on, is referenced to the VM, and into the appropriate
 * entry in the PV list entry. Next, the function must find the PV
 * list entry associated with this pmap/va (if it doesn't exist - the function
 * panics). The PV list entry is unlinked from the list, and returned to
 * its zone.
d865 1
a865 231
pmap_remove_pte(pmap_t pmap, vaddr_t va, pt_entry_t *pte, boolean_t flush)
{
	pt_entry_t opte;
	pv_entry_t prev, cur, pvl;
	struct vm_page *pg;
	paddr_t pa;

#ifdef PMAPDEBUG
	if (pmap_debug & CD_RM)
		printf("pmap_remove_pte(%p, %p, %d)\n", pmap, va, flush);
#endif

	if (pte == NULL || !PDT_VALID(pte)) {
		return;	 	/* no page mapping, nothing to do! */
	}

	/*
	 * Update statistics.
	 */
	pmap->pm_stats.resident_count--;
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;

	pa = ptoa(PG_PFNUM(*pte));

	/*
	 * Invalidate the pte.
	 */

	opte = invalidate_pte(pte) & PG_M_U;
	if (flush)
		flush_atc_entry(pmap, va);

	pg = PHYS_TO_VM_PAGE(pa);

	/* If this isn't a managed page, just return. */
	if (pg == NULL)
		return;

	/*
	 * Remove the mapping from the pvlist for
	 * this physical page.
	 */
	pvl = pg_to_pvh(pg);

#ifdef DIAGNOSTIC
	if (pvl->pv_pmap == NULL)
		panic("pmap_remove_pte: null pv_list");
#endif

	prev = NULL;
	for (cur = pvl; cur != NULL; cur = cur->pv_next) {
		if (cur->pv_va == va && cur->pv_pmap == pmap)
			break;
		prev = cur;
	}
	if (cur == NULL) {
		panic("pmap_remove_pte: mapping for va "
		    "0x%lx (pa 0x%lx) not in pv list at %p",
		    va, pa, pvl);
	}

	if (prev == NULL) {
		/*
		 * Handler is the pv_entry. Copy the next one
		 * to handler and free the next one (we can't
		 * free the handler)
		 */
		cur = cur->pv_next;
		if (cur != NULL) {
			cur->pv_flags = pvl->pv_flags;
			*pvl = *cur;
			pool_put(&pvpool, cur);
		} else {
			pvl->pv_pmap = NULL;
			if (CPU_IS88100 &&
			    kernel_apr_cmode != userland_apr_cmode) {
				/* XXX Why isn't cmmu_dcache_wb() enough? */
				if (0)
					cmmu_dcache_wb(cpu_number(),
					    pa, PAGE_SIZE);
				else
					cmmu_cache_wbinv(cpu_number(),
					    pa, PAGE_SIZE);
			}
		}
	} else {
		prev->pv_next = cur->pv_next;
		pool_put(&pvpool, cur);
	}

	/* Update saved attributes for managed page */
	pvl->pv_flags |= opte;
}

/*
 * Routine:	PMAP_REMOVE_RANGE (internal)
 *
 * Function:
 *	Invalidate page table entries associated with the
 *	given virtual address range. The entries given are the first
 *	(inclusive) and last (exclusive) entries for the VM pages.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		virtual address of start of range to remove
 *	e		virtual address of end of range to remove
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pmap_remove_pte
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *   This routine sequences through the pages defined by the given
 * range. For each page, the associated page table entry (PTE) is
 * invalidated via pmap_remove_pte().
 *
 * Empty segments are skipped for performance.
 */
void
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	vaddr_t va, eseg;

#ifdef PMAPDEBUG
	if (pmap_debug & CD_RM)
		printf("pmap_remove_range(%p, %p, %p)\n", pmap, s, e);
#endif

	/*
	 * Loop through the range in PAGE_SIZE increments.
	 */
	va = s;
	while (va != e) {
		sdt_entry_t *sdt;

		eseg = (va & SDT_MASK) + (1 << SDT_SHIFT);
		if (eseg > e || eseg == 0)
			eseg = e;

		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt))
			va = eseg;
		else {
			while (va != eseg) {
				pmap_remove_pte(pmap, va, sdt_pte(sdt, va),
				    TRUE);
				va += PAGE_SIZE;
			}
		}
	}
}

/*
 * Routine:	PMAP_REMOVE
 *
 * Function:
 *	Remove the given range of addresses from the specified map.
 *	It is assumed that start and end are properly rounded to the VM page
 *	size.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s
 *	e
 *
 * Special Assumptions:
 *	Assumes not all entries must be valid in specified range.
 *
 * Calls:
 *	pmap_remove_range
 *
 *  After taking pmap read lock, pmap_remove_range is called to do the
 * real work.
 */
void
pmap_remove(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	int spl;

	if (pmap == NULL)
		return;

	spl = splvm();
	PMAP_LOCK(pmap);
	pmap_remove_range(pmap, s, e);
	PMAP_UNLOCK(pmap);
	splx(spl);
}

/*
 * Routine:	PMAP_REMOVE_ALL
 *
 * Function:
 *	Removes this physical page from all physical maps in which it
 *	resides. Reflects back modify bits to the pager.
 *
 * Parameters:
 *	pg		physical pages which is to
 *			be removed from all maps
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pool_put
 *
 *  If the page specified by the given address is not a managed page,
 * this routine simply returns. Otherwise, the PV list associated with
 * that page is traversed. For each pmap/va pair pmap_pte is called to
 * obtain a pointer to the page table entry (PTE) associated with the
 * va (the PTE must exist and be valid, otherwise the routine panics).
 * The hardware 'modified' bit in the PTE is examined. If it is on, the
 * corresponding bit in the PV list entry corresponding
 * to the physical page is set to 1.
 * Then, the PTE is invalidated, and the PV list entry is unlinked and
 * freed.
 *
 *  At the end of this function, the PV list for the specified page
 * will be null.
 */
void
pmap_remove_page(struct vm_page *pg)
d867 2
a869 1
	pv_entry_t pvl;
d871 2
a872 215
	pmap_t pmap;
	int spl;

	if (pg == NULL) {
		/* not a managed page. */
#ifdef PMAPDEBUG
		if (pmap_debug & CD_RMPG)
			printf("pmap_remove_page(%p): not a managed page\n", pg);
#endif
		return;
	}

#ifdef PMAPDEBUG
	if (pmap_debug & CD_RMPG)
		printf("pmap_remove_page(%p)\n", pg);
#endif

	spl = splvm();
	/*
	 * Walk down PV list, removing all mappings.
	 * We don't have to lock the pv list, since we have the entire pmap
	 * system.
	 */
#if defined(MULTIPROCESSOR) && 0
remove_all_Retry:
#endif

	pvl = pg_to_pvh(pg);

	/*
	 * Loop for each entry on the pv list
	 */
	while (pvl != NULL && (pmap = pvl->pv_pmap) != NULL) {
#if defined(MULTIPROCESSOR) && 0
		if (!__cpu_simple_lock_try(&pmap->pm_lock))
			goto remove_all_Retry;
#endif

		va = pvl->pv_va;
		pte = pmap_pte(pmap, va);

		if (pte == NULL || !PDT_VALID(pte)) {
			pvl = pvl->pv_next;
			goto next;	/* no page mapping */
		}
		if (pmap_pte_w(pte)) {
#ifdef PMAPDEBUG
			if (pmap_debug & CD_RMPG)
				printf("pmap_remove_page(%p): wired mapping not removed\n",
				    pg);
#endif
			pvl = pvl->pv_next;
			goto next;
		}

		pmap_remove_pte(pmap, va, pte, TRUE);

		/*
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
		 */
next:
		PMAP_UNLOCK(pmap);
	}
	splx(spl);
}

/*
 * Routine:	PMAP_PROTECT
 *
 * Function:
 *	Sets the physical protection on the specified range of this map
 *	as requested.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		start address of start of range
 *	e		end address of end of range
 *	prot		desired protection attributes
 *
 *	Calls:
 *		PMAP_LOCK, PMAP_UNLOCK
 *		pmap_pte
 *		PDT_VALID
 *
 *  This routine sequences through the pages of the specified range.
 * For each, it calls pmap_pte to acquire a pointer to the page table
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
 * Otherwise, the PTE's protection attributes are adjusted as specified.
 */
void
pmap_protect(pmap_t pmap, vaddr_t s, vaddr_t e, vm_prot_t prot)
{
	int spl;
	pt_entry_t *pte, ap;
	vaddr_t va, eseg;

	if ((prot & VM_PROT_READ) == 0) {
		pmap_remove(pmap, s, e);
		return;
	}

	ap = m88k_protection(prot);

	spl = splvm();
	PMAP_LOCK(pmap);

	/*
	 * Loop through the range in PAGE_SIZE increments.
	 */
	va = s;
	while (va != e) {
		sdt_entry_t *sdt;

		eseg = (va & SDT_MASK) + (1 << SDT_SHIFT);
		if (eseg > e || eseg == 0)
			eseg = e;

		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt))
			va = eseg;
		else {
			while (va != eseg) {
				pte = sdt_pte(sdt, va);
				if (pte != NULL && PDT_VALID(pte)) {
					/*
					 * Invalidate pte temporarily to avoid
					 * the modified bit and/or the
					 * reference bit being written back by
					 * any other cpu.
					 */
					*pte = ap |
					    (invalidate_pte(pte) & ~PG_PROT);
					flush_atc_entry(pmap, va);
				}
				va += PAGE_SIZE;
			}
		}
	}
	PMAP_UNLOCK(pmap);
	splx(spl);
}

/*
 * Routine:	PMAP_EXPAND
 *
 * Function:
 *	Expands a pmap to be able to map the specified virtual address.
 *	New kernel virtual memory is allocated for a page table.
 *
 *	Must be called with the pmap system and the pmap unlocked, since
 *	these must be unlocked to use vm_allocate or vm_deallocate (via
 *	uvm_km_zalloc). Thus it must be called in a unlock/lock loop
 *	that checks whether the map has been expanded enough. (We won't loop
 *	forever, since page table aren't shrunk.)
 *
 * Parameters:
 *	pmap	point to pmap structure
 *	v	VA indicating which tables are needed
 *
 * Extern/Global:
 *	user_pt_map
 *	kernel_pmap
 *
 * Calls:
 *	pmap_pte
 *	uvm_km_free
 *	uvm_km_zalloc
 *	pmap_extract
 *
 * Special Assumptions
 *	no pmap locks held
 *	pmap != kernel_pmap
 *
 * 1:	This routine immediately allocates space for a page table.
 *
 * 2:	The page table entries (PTEs) are initialized (set invalid), and
 *	the corresponding segment table entry is set to point to the new
 *	page table.
 */
void
pmap_expand(pmap_t pmap, vaddr_t v)
{
	int spl;
	vaddr_t pdt_vaddr;
	paddr_t pdt_paddr;
	sdt_entry_t *sdt;
	pt_entry_t *pte;

#ifdef PMAPDEBUG
	if (pmap_debug & CD_EXP)
		printf ("pmap_expand(%p, %p)\n", pmap, v);
#endif

	/* XXX */
	pdt_vaddr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
	if (pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr) == FALSE)
		panic("pmap_expand: pmap_extract failed");

	/* memory for page tables should not be writeback or local */
	pmap_cache_ctrl(kernel_pmap,
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CPU_IS88100 ? CACHE_INH : CACHE_WT);

	spl = splvm();
	PMAP_LOCK(pmap);

	if ((pte = pmap_pte(pmap, v)) != NULL) {
		/*
		 * Someone else caused us to expand
		 * during our vm_allocate.
		 */
		PMAP_UNLOCK(pmap);
		uvm_km_free(kernel_map, pdt_vaddr, PAGE_SIZE);
d874 1
a874 17
#ifdef PMAPDEBUG
		if (pmap_debug & CD_EXP)
			printf("pmap_expand(%p, %p): table has already been allocated\n",
			    pmap, v);
#endif
		splx(spl);
		return;
	}
	/*
	 * Apply a mask to V to obtain the vaddr of the beginning of
	 * its containing page 'table group', i.e. the group of
	 * page tables that fit eithin a single VM page.
	 * Using that, obtain the segment table pointer that references the
	 * first page table in the group, and initialize all the
	 * segment table descriptions for the page 'table group'.
	 */
	v &= ~((1 << (PDT_BITS + PG_BITS)) - 1);
d876 13
a888 1
	sdt = SDTENT(pmap, v);
d890 6
a895 6
	/*
	 * Init each of the segment entries to point the freshly allocated
	 * page tables.
	 */
	*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
	*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;
d897 1
a897 2
	PMAP_UNLOCK(pmap);
	splx(spl);
d901 9
a909 62
 * Routine:	PMAP_ENTER
 *
 * Function:
 *	Insert the given physical page (p) at the specified virtual
 *	address (v) in the target phisical map with the protecton requested.
 *	If specified, the page will be wired down, meaning that the
 *	related pte can not be reclaimed.
 *
 * N.B.: This is the only routine which MAY NOT lazy-evaluation or lose
 *	information. That is, this routine must actually insert this page
 *	into the given map NOW.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	va	VA of page to be mapped
 *	pa	PA of page to be mapped
 *	prot	protection attributes for page
 *	wired	wired attribute for page
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pmap_expand
 *	pmap_remove_pte
 *
 *	This routine starts off by calling pmap_pte to obtain a (virtual)
 * pointer to the page table entry corresponding to given virtual
 * address. If the page table itself does not exist, pmap_expand is
 * called to allocate it.
 *
 *	If the page table entry (PTE) already maps the given physical page,
 * all that is needed is to set the protection and wired attributes as
 * given. TLB entries are flushed and pmap_enter returns.
 *
 *	If the page table entry (PTE) maps a different physical page than
 * that given, the old mapping is removed by a call to map_remove_range.
 * And execution of pmap_enter continues.
 *
 *	To map the new physical page, the routine first inserts a new
 * entry in the PV list exhibiting the given pmap and virtual address.
 * It then inserts the physical page address, protection attributes, and
 * wired attributes into the page table entry (PTE).
 *
 *
 *	get machine-dependent prot code
 *	get the pte for this page
 *	if necessary pmap_expand(pmap, v)
 *	if (changing wired attribute or protection) {
 * 		flush entry from TLB
 *		update template
 *		for (ptes per vm page)
 *			stuff pte
 *	} else if (mapped at wrong addr)
 *		flush entry from TLB
 *		pmap_remove_pte
 *	} else {
 *		enter mapping in pv_list
 *		setup template and stuff ptes
 *	}
 *
d914 1
a914 1
	int spl;
d921 2
a922 5
#ifdef PMAPDEBUG
	if (pmap_debug & CD_ENT)
		printf("pmap_enter(%p, %p, %p, %x, %x)\n",
		    pmap, va, pa, prot, flags);
#endif
a925 3
	spl = splvm();
	PMAP_LOCK(pmap);

d929 10
a938 18
	while ((pte = pmap_pte(pmap, va)) == NULL) {
		if (pmap == kernel_pmap) {
			/* will only return NULL if PMAP_CANFAIL is set */
			if (pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE,
			    flags & PMAP_CANFAIL) == NULL) {
#ifdef PMAPDEBUG
				if (pmap_debug & CD_ENT)
					printf("failed (ENOMEM)\n");
#endif
				return (ENOMEM);
			}
		} else {
			/*
			 * Must unlock to expand the pmap.
			 */
			PMAP_UNLOCK(pmap);
			pmap_expand(pmap, va);
			PMAP_LOCK(pmap);
d941 1
d946 1
a946 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_ENT)
		printf("pmap_enter: old_pa %p pte %p\n", old_pa, *pte);
#endif
d949 1
d963 1
a963 1
		pmap_remove_pte(pmap, va, pte, FALSE);
d985 1
a985 1
					flush_atc_entry(pmap, va);
d988 1
a988 2
						PMAP_UNLOCK(pmap);
						splx(spl);
d1019 1
a1019 1
		if (CPU_IS88110 && pmap != kernel_pmap &&
d1028 1
a1028 1
	 * If outside physical memory, disable cache on this (I/O) page.
d1030 1
a1030 1
	if ((unsigned long)pa >= last_addr)
d1040 2
a1041 5
	flush_atc_entry(pmap, va);
#ifdef PMAPDEBUG
	if (pmap_debug & CD_ENT)
		printf("pmap_enter: new pte %p\n", *pte);
#endif
d1048 1
a1048 1
			if (CPU_IS88110 && pmap != kernel_pmap)
d1056 1
a1056 2
	PMAP_UNLOCK(pmap);
	splx(spl);
d1062 2
a1063 13
 * Routine:	pmap_unwire
 *
 * Function:	Change the wiring attributes for a map/virtual-address pair.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	v	virtual address of page to be unwired
 *
 * Calls:
 *	pmap_pte
 *
 * Special Assumptions:
 *	The mapping must already exist in the pmap.
d1066 1
a1066 1
pmap_unwire(pmap_t pmap, vaddr_t v)
d1068 3
a1070 2
	pt_entry_t *pte;
	int spl;
d1072 10
a1081 2
	spl = splvm();
	PMAP_LOCK(pmap);
d1083 5
a1087 2
	if ((pte = pmap_pte(pmap, v)) == NULL)
		panic("pmap_unwire: pte missing");
d1089 5
a1093 5
	if (pmap_pte_w(pte)) {
		/* unwired mapping */
		pmap->pm_stats.wired_count--;
		*pte &= ~PG_W;
	}
d1095 3
a1097 2
	PMAP_UNLOCK(pmap);
	splx(spl);
d1101 4
a1104 20
 * Routine:	PMAP_EXTRACT
 *
 * Function:
 *	Extract the physical page address associoated
 *	with the given map/virtual_address pair.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	va		virtual address
 *	pap		storage for result.
 *
 * Calls:
 *	PMAP_LOCK, PMAP_UNLOCK
 *	pmap_pte
 *
 * The routine calls pmap_pte to get a (virtual) pointer to
 * the page table entry (PTE) associated with the given virtual
 * address. If the page table does not exist, or if the PTE is not valid,
 * then 0 address is returned. Otherwise, the physical page address from
 * the PTE is returned.
d1106 3
a1108 2
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
d1110 2
a1111 1
	pt_entry_t *pte;
d1113 36
a1148 2
	int spl;
	boolean_t rv = FALSE;
d1151 2
a1152 2
	if (pmap == NULL)
		panic("pmap_extract: pmap is NULL");
d1155 25
d1181 32
d1214 1
a1214 2
	 * 88100-based designs have two hardwired BATC entries which map
	 * the upper 1MB 1:1 in supervisor space.
d1216 18
a1233 4
	if (CPU_IS88100) {
		if (va >= BATC8_VA && pmap == kernel_pmap) {
			*pap = va;
			return (TRUE);
d1236 37
a1272 1
#endif
d1274 10
a1283 2
	spl = splvm();
	PMAP_LOCK(pmap);
d1285 5
a1289 7
	pte = pmap_pte(pmap, va);
	if (pte != NULL && PDT_VALID(pte)) {
		rv = TRUE;
		if (pap != NULL) {
			pa = ptoa(PG_PFNUM(*pte));
			pa |= (va & PAGE_MASK); /* offset within page */
			*pap = pa;
a1291 4

	PMAP_UNLOCK(pmap);
	splx(spl);
	return rv;
d1295 2
a1296 30
 * Routine:	PMAP_COLLECT
 *
 * Runction:
 *	Garbage collects the physical map system for pages which are
 *	no longer used. there may well be pages which are not
 *	referenced, but others may be collected as well.
 *	Called by the pageout daemon when pages are scarce.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_pte
 *	pmap_remove_range
 *	uvm_km_free
 *
 *	The intent of this routine is to release memory pages being used
 * by translation tables. They can be release only if they contain no
 * valid mappings, and their parent table entry has been invalidated.
 *
 *	The routine sequences through the entries user address space,
 * inspecting page-sized groups of page tables for wired entries. If
 * a full page of tables has no wired enties, any otherwise valid
 * entries are invalidated (via pmap_remove_range). Then, the segment
 * table entries corresponding to this group of page tables are
 * invalidated. Finally, uvm_km_free is called to return the page to the
 * system.
 *
 *	If all entries in a segment table are invalidated, it too can
 * be returned to the system.
d1299 1
a1299 1
pmap_collect(pmap_t pmap)
d1301 5
a1305 10
	u_int sdt;		/* outer loop index */
	vaddr_t sdt_va;
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t *gdttblend;	/* ptr to byte after last entry in
				   table group */
	pt_entry_t *gdtp;	/* ptr to index into a page table */
	boolean_t found_gdt_wired; /* flag indicating a wired page exists
				   in a page table's address range */
	int spl;
d1307 1
a1307 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_COL)
		printf ("pmap_collect(%p)\n", pmap);
#endif
d1309 8
a1316 2
	spl = splvm();
	PMAP_LOCK(pmap);
d1318 9
a1326 19
	sdtp = pmap->pm_stab; /* addr of segment table */

	/* segment table loop */
	for (sdt = VM_MIN_ADDRESS >> SDT_SHIFT;
	    sdt <= VM_MAX_ADDRESS >> SDT_SHIFT; sdt++, sdtp++) {
		sdt_va = sdt << SDT_SHIFT;
		gdttbl = pmap_pte(pmap, sdt_va);
		if (gdttbl == NULL)
			continue; /* no maps in this range */

		gdttblend = gdttbl + PDT_ENTRIES;

		/* scan page maps for wired pages */
		found_gdt_wired = FALSE;
		for (gdtp = gdttbl; gdtp < gdttblend; gdtp++) {
			if (pmap_pte_w(gdtp)) {
				found_gdt_wired = TRUE;
				break;
			}
d1329 1
a1329 6
		if (found_gdt_wired)
			continue; /* can't free this range */

		/* invalidate all maps in this range */
		pmap_remove_range(pmap, sdt_va, sdt_va + (1 << SDT_SHIFT));

d1331 2
a1332 1
		 * we can safely deallocate the page map(s)
a1333 10
		*((sdt_entry_t *) sdtp) = 0;
		*((sdt_entry_t *)(sdtp + SDT_ENTRIES)) = 0;

		/*
		 * we have to unlock before freeing the table, since
		 * uvm_km_free will invoke another pmap routine
		 */
		PMAP_UNLOCK(pmap);
		uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
		PMAP_LOCK(pmap);
d1335 1
a1335 8

	PMAP_UNLOCK(pmap);
	splx(spl);

#ifdef PMAPDEBUG
	if (pmap_debug & CD_COL)
		printf("pmap_collect(%p) done\n", pmap);
#endif
d1339 2
a1340 15
 * Routine:	PMAP_ACTIVATE
 *
 * Function:
 * 	Binds the pmap associated to the process to the current processor.
 *
 * Parameters:
 * 	p	pointer to proc structure
 *
 * Notes:
 *	If the specified pmap is not kernel_pmap, this routine stores its
 *	apr template into UAPR (user area pointer register) in the
 *	CMMUs connected to the specified CPU.
 *
 *	Then it flushes the TLBs mapping user virtual space, in the CMMUs
 *	connected to the specified CPU.
d1343 1
a1343 1
pmap_activate(struct proc *p)
d1345 18
a1362 2
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	struct cpu_info *ci = curcpu();
d1364 3
a1366 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_ACTIVATE)
		printf("pmap_activate(%p) pmap %p\n", p, pmap);
#endif
d1368 20
a1387 7
	if (pmap == kernel_pmap) {
		ci->ci_curpmap = NULL;
	} else {
		if (pmap != ci->ci_curpmap) {
			cmmu_set_uapr(pmap->pm_apr);
			cmmu_tlb_inv(ci->ci_cpuid, FALSE, 0, -1);
			ci->ci_curpmap = pmap;
d1390 1
d1394 2
a1395 7
 * Routine:	PMAP_DEACTIVATE
 *
 * Function:
 *	Unbinds the pmap associated to the process from the current processor.
 *
 * Parameters:
 *	p		pointer to proc structure
d1398 1
a1398 1
pmap_deactivate(struct proc *p)
d1400 1
a1400 1
	struct cpu_info *ci = curcpu();
d1402 5
a1406 1
	ci->ci_curpmap = NULL;
d1410 6
a1415 18
 * Routine:	PMAP_COPY_PAGE
 *
 * Function:
 *	Copies the specified pages.
 *
 * Parameters:
 *	src	PA of source page
 *	dst	PA of destination page
 *
 * Extern/Global:
 *	phys_map_vaddr
 *
 * Special Assumptions:
 *	no locking required
 *
 * This routine maps the physical pages at the 'phys_map' virtual
 * addresses set up in pmap_bootstrap. It flushes the TLB to make the
 * new mappings effective, and performs the copy.
a1421 4
	vaddr_t dstva, srcva;
	int spl;
	pt_entry_t *dstpte, *srcpte;
	int cpu = cpu_number();
d1424 6
a1429 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_COPY)
		printf("pmap_copy_page(%p,%p) pa %p %p\n",
		    srcpg, dstpg, src, dst);
d1431 2
d1434 9
a1442 4
	dstva = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	srcva = dstva + PAGE_SIZE;
	dstpte = pmap_pte(kernel_pmap, dstva);
	srcpte = pmap_pte(kernel_pmap, srcva);
d1444 2
a1445 17
	spl = splvm();

	*dstpte = m88k_protection(VM_PROT_READ | VM_PROT_WRITE) |
	    PG_M /* 88110 */ | PG_U | PG_V | dst;
	*srcpte = m88k_protection(VM_PROT_READ) |
	    PG_U | PG_V | src;

	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_tlb_inv(cpu, TRUE, dstva, 2);
	if (CPU_IS88100 && kernel_apr_cmode != userland_apr_cmode)
		cmmu_dcache_wb(cpu, src, PAGE_SIZE);
	copypage(srcva, dstva);

	splx(spl);
d1449 4
a1452 21
 * Routine:	PMAP_CHANGEBIT
 *
 * Function:
 *	Update the pte bits on the specified physical page.
 *
 * Parameters:
 *	pg	physical page
 *	set	bits to set
 *	mask	bits to mask
 *
 * Extern/Global:
 *	pv_lists
 *
 * Calls:
 *	pmap_pte
 *
 * The pte bits corresponding to the page's frame index will be changed as
 * requested. The PV list will be traversed.
 * For each pmap/va the hardware the necessary bits in the page descriptor
 * table entry will be altered as well if necessary. If any bits were changed,
 * a TLB flush will be performed.
d1460 1
a1460 1
	int spl;
d1463 1
a1463 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_CBIT)
		printf("pmap_changebit(%p, %x, %x)\n", pg, set, mask);
#endif
d1465 1
a1465 1
	spl = splvm();
a1466 3
#if defined(MULTIPROCESSOR) && 0
changebit_Retry:
#endif
a1467 1

d1473 6
a1478 2
	if (pvl->pv_pmap == NULL)
		goto done;
d1480 5
a1484 17
	/* for each listed pmap, update the affected bits */
	for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
#if defined(MULTIPROCESSOR) && 0
		if (!__cpu_simple_lock_try(&pmap->pm_lock)) {
			goto changebit_Retry;
		}
#endif
		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		/*
		 * Check for existing and valid pte
		 */
		if (pte == NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
d1486 3
a1488 3
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_changebit: pte %x in pmap %p doesn't point to page %p %lx",
			    *pte, pmap, pg, VM_PAGE_TO_PHYS(pg));
d1491 5
a1495 5
		/*
		 * Update bits
		 */
		opte = *pte;
		npte = (opte | set) & mask;
d1497 10
a1506 10
		/*
		 * Flush TLB of which cpus using pmap.
		 *
		 * Invalidate pte temporarily to avoid the modified bit
		 * and/or the reference being written back by any other cpu.
		 */
		if (npte != opte) {
			invalidate_pte(pte);
			*pte = npte;
			flush_atc_entry(pmap, va);
a1507 2
next:
		PMAP_UNLOCK(pmap);
d1510 1
a1510 2
done:
	splx(spl);
d1514 5
a1518 23
 * Routine:	PMAP_TESTBIT
 *
 * Function:
 *	Test the modified/referenced bits of a physical page.
 *
 * Parameters:
 *	pg	physical page
 *	bit	bit to test
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *
 * If the attribute list for the given page has the bit, this routine
 * returns TRUE.
 *
 * Otherwise, this routine walks the PV list corresponding to the
 * given page. For each pmap/va pair, the page descriptor table entry is
 * examined. If the selected bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list), and updates the
 * attribute list.
d1526 1
a1526 1
	int spl;
d1528 1
a1528 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_CBIT)
		printf("pmap_testbit(%p, %x): ", pg, bit);
#endif
d1530 1
a1530 1
	spl = splvm();
a1531 3
#if defined(MULTIPROCESSOR) && 0
testbit_Retry:
#endif
a1532 1

d1536 2
a1537 5
#ifdef PMAPDEBUG
		if (pmap_debug & CD_TBIT)
			printf("cached\n");
#endif
		splx(spl);
d1541 21
a1561 35
	if (pvl->pv_pmap == NULL)
		goto done;

	/* for each listed pmap, check modified bit for given page */
	for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
#if defined(MULTIPROCESSOR) && 0
		if (!__cpu_simple_lock_try(&pmap->pm_lock)) {
			goto testbit_Retry;
		}
#endif

		pte = pmap_pte(pmap, pvep->pv_va);
		if (pte == NULL || !PDT_VALID(pte)) {
			goto next;
		}

#ifdef PMAPDEBUG
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_testbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, pmap == kernel_pmap ? 1 : 0, pg, VM_PAGE_TO_PHYS(pg));
#endif

		if ((*pte & bit) != 0) {
			PMAP_UNLOCK(pmap);
			pvl->pv_flags |= bit;
#ifdef PMAPDEBUG
			if ((pmap_debug & (CD_TBIT | CD_FULL)) == (CD_TBIT | CD_FULL))
				printf("found in pte %p @@%p\n",
				    *pte, pte);
			else if (pmap_debug & CD_TBIT)
				printf("found\n");
#endif
			splx(spl);
			return (TRUE);
a1562 2
next:
		PMAP_UNLOCK(pmap);
d1565 2
a1566 6
done:
#ifdef PMAPDEBUG
		if (pmap_debug & CD_TBIT)
			printf("not found\n");
#endif
	splx(spl);
d1571 5
a1575 9
 * Routine:	PMAP_UNSETBIT
 *
 * Function:
 *	Clears a pte bit and returns its previous state, for the
 *	specified physical page.
 *	This is an optimized version of:
 *		rv = pmap_testbit(pg, bit);
 *		pmap_changebit(pg, 0, ~bit);
 *		return rv;
d1584 1
a1584 1
	int spl;
d1587 1
a1587 4
#ifdef PMAPDEBUG
	if (pmap_debug & CD_USBIT)
		printf("pmap_unsetbit(%p, %x): ", pg, bit);
#endif
d1589 1
a1589 1
	spl = splvm();
a1590 3
#if defined(MULTIPROCESSOR) && 0
unsetbit_Retry:
#endif
d1601 6
a1606 2
	if (pvl->pv_pmap == NULL)
		goto done;
d1608 5
a1612 17
	/* for each listed pmap, update the specified bit */
	for (pvep = pvl; pvep != NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
#if defined(MULTIPROCESSOR) && 0
		if (!__cpu_simple_lock_try(&pmap->pm_lock)) {
			goto unsetbit_Retry;
		}
#endif
		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		/*
		 * Check for existing and valid pte
		 */
		if (pte == NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
d1614 3
a1616 3
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_unsetbit: pte %x in pmap %p doesn't point to page %p %lx",
			    *pte, pmap, pg, VM_PAGE_TO_PHYS(pg));
a1618 5
		/*
		 * Update bits
		 */
		opte = *pte;
		if (opte & bit) {
d1620 1
a1620 4
			 * Flush TLB of which cpus using pmap.
			 *
			 * Invalidate pte temporarily to avoid the specified
			 * bit being written back by any other cpu.
d1622 12
a1633 4
			invalidate_pte(pte);
			*pte = opte ^ bit;
			flush_atc_entry(pmap, va);
			rv = TRUE;
a1634 2
next:
		PMAP_UNLOCK(pmap);
d1636 1
d1638 1
a1638 6
done:
	splx(spl);
#ifdef PMAPDEBUG
	if (pmap_debug & CD_USBIT)
		printf(rv ? "TRUE\n" : "FALSE\n");
#endif
d1643 4
a1646 5
 * Routine:	PMAP_IS_MODIFIED
 *
 * Function:
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d1664 1
a1664 4
#ifdef PMAPDEBUG
		if (pmap_debug & CD_TBIT)
			printf("pmap_is_modified(%p) -> %x\n", pg, rc);
#endif
d1673 4
a1676 5
 * Routine:	PMAP_IS_REFERENCED
 *
 * Function:
 *	Return whether or not the specified physical page is referenced by
 *	any physical maps.
d1685 2
a1686 7
 * Routine:	PMAP_PAGE_PROTECT
 *
 * Calls:
 *	pmap_changebit
 *	pmap_remove_page
 *
 *	Lower the permission for all mappings to a given page.
d1697 3
a1699 101
void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = virtual_avail;
	*endp = virtual_end;
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	int spl;
	pt_entry_t template, *pte;

#ifdef PMAPDEBUG
	if (pmap_debug & CD_ENT)
		printf ("pmap_kenter_pa(%p, %p, %x)\n", va, pa, prot);
#endif

	spl = splvm();
	PMAP_LOCK(kernel_pmap);

	template = m88k_protection(prot);
#ifdef M88110
	if (CPU_IS88110 && m88k_protection(prot) != PG_RO)
		template |= PG_M;
#endif

	/*
	 * Expand pmap to include this pte.
	 */
	while ((pte = pmap_pte(kernel_pmap, va)) == NULL)
		pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE, 0);

	/*
	 * And count the mapping.
	 */
	kernel_pmap->pm_stats.resident_count++;
	kernel_pmap->pm_stats.wired_count++;

	invalidate_pte(pte);

	/*
	 * If outside physical memory, disable cache on this (I/O) page.
	 */
	if ((unsigned long)pa >= last_addr)
		template |= CACHE_INH | PG_V | PG_W;
	else
		template |= PG_V | PG_W;
	*pte = template | pa;
	flush_atc_entry(kernel_pmap, va);

	PMAP_UNLOCK(kernel_pmap);
	splx(spl);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	int spl;
	vaddr_t e, eseg;

#ifdef PMAPDEBUG
	if (pmap_debug & CD_RM)
		printf("pmap_kremove(%p, %x)\n", va, len);
#endif

	spl = splvm();
	PMAP_LOCK(kernel_pmap);

	e = va + len;
	while (va != e) {
		sdt_entry_t *sdt;
		pt_entry_t *pte;

		eseg = (va & SDT_MASK) + (1 << SDT_SHIFT);
		if (eseg > e || eseg == 0)
			eseg = e;

		sdt = SDTENT(kernel_pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt))
			va = eseg;
		else {
			while (va != eseg) {
				pte = sdt_pte(sdt, va);
				if (pte != NULL && PDT_VALID(pte)) {
					/* Update the counts */
					kernel_pmap->pm_stats.resident_count--;
					kernel_pmap->pm_stats.wired_count--;

					invalidate_pte(pte);
					flush_atc_entry(kernel_pmap, va);
				}
				va += PAGE_SIZE;
			}
		}
	}
	PMAP_UNLOCK(kernel_pmap);
	splx(spl);
}
d1701 4
d1733 8
a1740 1
#include <machine/m88110.h>
d1771 1
a1771 1
	if (pmap == kernel_pmap)
@


1.56
log
@Standardize cache handling functions and defines to use wb/wbinv/inv instead
of flush/sync/inval. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2010/12/26 22:18:20 miod Exp $	*/
d118 2
d522 2
a523 1
		 * the old mapping was cached.
d525 2
a526 1
		if ((opte & CACHE_INH) == 0) {
d530 1
a530 1
				if (ISSET(m88k_cpus[cpu].ci_flags, CIF_ALIVE))
d534 7
a540 1
					cmmu_cache_wbinv(cpu, pa, PAGE_SIZE);
d700 3
a702 4
	 * Map system segment & page tables - should be cache inhibited?
	 * 88200 manual says that CI bit is driven on the Mbus while accessing
	 * the translation tree. I don't think we need to map it CACHE_INH
	 * here...
d709 1
a709 1
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d770 2
a771 2
	kernel_pmap->pm_apr = (atop((paddr_t)kmap) << PG_SHIFT) | default_apr |
	    CACHE_WT;
d909 2
a910 5
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) | default_apr;
#if !defined(MULTIPROCESSOR) && defined(M88110)
	if (CPU_IS88110)
		pmap->pm_apr &= ~CACHE_GLOBAL;
#endif
d920 1
a920 1
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_WT);
d1163 10
d1526 1
a1526 1
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_WT);
d2122 2
a2123 1
	cmmu_cache_wbinv(cpu, src, PAGE_SIZE);
@


1.55
log
@Misplaced splx() in pmap_unsetbit().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2010/12/23 19:53:29 miod Exp $	*/
d209 1
a209 1
				cmmu_flush_tlb(ci->ci_cpuid, kernel, va, 1);
d230 1
a230 1
			cmmu_flush_tlb(ci->ci_cpuid, kernel, va, 1);
d258 1
a258 1
				cmmu_flush_tlb(ci->ci_cpuid, TRUE, 0 ,0);
d260 1
a260 1
				cmmu_flush_tlb(ci->ci_cpuid, FALSE, 0 ,0);
d530 1
a530 1
					cmmu_flush_cache(cpu, pa, PAGE_SIZE);
d776 1
a776 1
		cmmu_flush_tlb(cpu, TRUE, 0, -1);
d858 1
a858 1
	cmmu_flush_tlb(cpu, TRUE, va, 1);
d2029 1
a2029 1
			cmmu_flush_tlb(ci->ci_cpuid, FALSE, 0, -1);
d2105 2
a2106 2
	cmmu_flush_tlb(cpu, TRUE, dstva, 2);
	cmmu_flush_cache(cpu, src, PAGE_SIZE);
d2599 1
a2599 1
				cmmu_flush_inst_cache(ci->ci_cpuid, pa, count);
@


1.54
log
@Make the pte created by pmap_map() wired, and count them in pmap_kernel()'s
statistics. This allows these mappings to be removed with pmap_kremove()
without messing with accounting.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2010/06/27 12:41:23 miod Exp $	*/
a2402 1
	splx(spl);
d2405 1
@


1.53
log
@Bunch of include adjustements to restore compilability.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2010/06/26 23:24:43 guenther Exp $	*/
d433 1
a433 1
	template = m88k_protection(prot) | cmode | PG_V;
d455 2
d682 1
a682 1
	vaddr = pmap_map(0, 0, s_text, VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d685 2
a686 3
	vaddr = pmap_map(s_text, s_text, e_text, VM_PROT_READ, 0);

	vaddr = pmap_map(vaddr, e_text, (paddr_t)kmap,
@


1.52
log
@Don't #include <sys/user.h> into files that don't need the stuff
it defines.  In some cases, this means pulling in uvm.h or pcb.h
instead, but most of the inclusions were just noise.  Tested on
alpha, amd64, armish, hppa, i386, macpcc, sgi, sparc64, and vax,
mostly by krw and naddy.
ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2010/05/09 15:46:17 jasper Exp $	*/
d56 1
@


1.51
log
@hander -> handler

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2010/05/02 22:01:47 miod Exp $	*/
a53 1
#include <sys/user.h>
@


1.50
log
@Precompute the userland process apr cache bits into a global variable.
On AViiON systems with the 6:1 CMMU:CPU configuration, force cached
mappings to be writethrough - this probably hides a bug in the code, but
that's the only way so far to get such a system running stably.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2009/09/27 19:16:10 miod Exp $	*/
d1145 3
a1147 3
		 * Hander is the pv_entry. Copy the next one
		 * to hander and free the next one (we can't
		 * free the hander)
@


1.49
log
@In pmap_proc_iflush(), do not bother checking for pmap being curpmap on any
processor, since caches are physically addressed and we are working on physical
addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2009/05/02 14:32:29 miod Exp $	*/
d118 2
a755 2
	kernel_pmap->pm_apr = (atop((paddr_t)kmap) << PG_SHIFT) |
	    CACHE_GLOBAL | CACHE_WT | APR_V;
d758 1
a758 1
		kernel_pmap->pm_apr &= ~CACHE_GLOBAL;
d760 2
d899 1
a899 1
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) | CACHE_GLOBAL | APR_V;
@


1.48
log
@Drop the pm_cpus bitmask field from struct pmap, and instead remember the
currently active userland pmap in each processors struct cpu_info.

This thus skips the complete tlb flush if idle switches back to
the proc previously running on this processor.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2009/04/19 17:56:13 miod Exp $	*/
d2596 1
a2596 3
				if (ci->ci_curpmap == pmap)
					cmmu_flush_inst_cache(ci->ci_cpuid,
					    pa, count);
@


1.47
log
@Rename max_cpus to ncpusfound and compute it regardless of option
MULTIPROCESSOR.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2009/03/04 19:39:05 miod Exp $	*/
a73 6
/*
 * Macros to operate pm_cpus field
 */
#define SETBIT_CPUSET(cpu_number, cpuset) (*(cpuset)) |= (1 << (cpu_number));
#define CLRBIT_CPUSET(cpu_number, cpuset) (*(cpuset)) &= ~(1 << (cpu_number));

d195 2
a196 5
	u_int32_t users;
	boolean_t kernel;

	if ((users = pmap->pm_cpus) == 0)
		return;
d198 2
a199 1
	kernel = pmap == kernel_pmap;
a200 1
#ifdef MULTIPROCESSOR
d205 3
a207 5
		int cpu;

		while ((cpu = ff1(users)) != 32) {
			cmmu_flush_tlb(cpu, kernel, va, 1);
			users ^= 1 << cpu;
a215 3
		struct cpu_info *ci;
		CPU_INFO_ITERATOR cpu;

d217 15
a231 5
			if (ISSET(users, 1 << ci->ci_cpuid))
				ci->ci_pmap_ipi |= kernel ?
				    CI_IPI_TLB_FLUSH_KERNEL :
				    CI_IPI_TLB_FLUSH_USER;
		}
d233 1
a233 7
#else	/* MULTIPROCESSOR */
	if (CPU_IS88100)
		cmmu_flush_tlb(cpu_number(), kernel, va, 1);
	if (CPU_IS88110)
		curcpu()->ci_pmap_ipi |= kernel ?
		    CI_IPI_TLB_FLUSH_KERNEL : CI_IPI_TLB_FLUSH_USER;
#endif	/* MULTIPROCESSOR */
a604 1
	kernel_pmap->pm_cpus = 0;
d780 1
a780 1
	SETBIT_CPUSET(cpu, &kernel_pmap->pm_cpus);
a927 1
	pmap->pm_cpus = 0;
d2014 1
a2014 1
	int cpu = cpu_number();
d2021 8
a2028 15
	if (pmap != kernel_pmap) {
		/*
		 * Lock the pmap to put this cpu in its active set.
		 */
		PMAP_LOCK(pmap);

		cmmu_set_uapr(pmap->pm_apr);
		cmmu_flush_tlb(cpu, FALSE, 0, -1);

		/*
		 * Mark that this cpu is using the pmap.
		 */
		SETBIT_CPUSET(cpu, &(pmap->pm_cpus));

		PMAP_UNLOCK(pmap);
d2044 1
a2044 2
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();
d2046 1
a2046 8
	if (pmap != kernel_pmap) {
		/*
		 * We expect the spl to already have been raised to sched level.
		 */
		PMAP_LOCK(pmap);
		CLRBIT_CPUSET(cpu, &(pmap->pm_cpus));
		PMAP_UNLOCK(pmap);
	}
d2583 1
a2583 2
	u_int32_t users;
	int cpu;
d2588 11
a2598 4
			users = pmap->pm_cpus;
			while ((cpu = ff1(users)) != 32) {
				cmmu_flush_inst_cache(cpu, pa, count);
				users &= ~(1 << cpu);
@


1.46
log
@Since 88110 processors can not flush individual TLB entries, instead of
flushing the whole TLB block every time a pte is modified, store a bitmask
of pending flushes and do them at pmap_update() time. 88100 behaviour is
unchanged.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2008/06/14 10:55:20 mk Exp $	*/
d720 3
a722 3
	phys_map_vaddr_end = virtual_avail + 2 * (max_cpus << PAGE_SHIFT);
	avail_start += 2 * (max_cpus << PAGE_SHIFT);
	virtual_avail += 2 * (max_cpus << PAGE_SHIFT);
@


1.45
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2008/06/10 21:12:14 miod Exp $	*/
d163 1
a163 1
static void flush_atc_entry(pmap_t, vaddr_t);
a197 4
static
#ifndef MULTIPROCESSOR
__inline__
#endif
a200 1
#ifdef MULTIPROCESSOR
a201 1
	int cpu;
d208 28
a235 3
	while ((cpu = ff1(users)) != 32) {
		cmmu_flush_tlb(cpu, kernel, va, 1);
		users ^= 1 << cpu;
d238 5
a242 2
	if (pmap->pm_cpus != 0)
		cmmu_flush_tlb(cpu_number(), pmap == kernel_pmap, va, 1);
d245 28
@


1.44
log
@Rename pmap_remove_all() to pmap_remove_page().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2007/12/31 09:23:53 martin Exp $	*/
d840 1
a840 2
	pmap = pool_get(&pmappool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));
@


1.43
log
@replace ctob/btoc by ptoa/atop as done for other architectures
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2007/12/20 21:15:51 miod Exp $	*/
d99 1
a99 1
#define CD_RMAL		0x0001000	/* pmap_remove_all */
d171 1
a171 1
void	pmap_remove_all(struct vm_page *);
d1262 1
a1262 1
pmap_remove_all(struct vm_page *pg)
d1273 2
a1274 2
		if (pmap_debug & CD_RMAL)
			printf("pmap_remove_all(%p): not a managed page\n", pg);
d1280 2
a1281 2
	if (pmap_debug & CD_RMAL)
		printf("pmap_remove_all(%p)\n", pg);
d1314 2
a1315 2
			if (pmap_debug & CD_RMAL)
				printf("pmap_remove_all(%p): wired mapping not removed\n",
d2440 1
a2440 1
 *	pmap_remove_all
d2448 1
a2448 1
		pmap_remove_all(pg);
@


1.42
log
@Flush the source page before copying in pmap_copy_page(). This should not
be necessary, but not doing it appears to break 88204 (not 88200) and
split CMMUs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2007/12/02 21:22:19 miod Exp $	*/
d707 1
a707 1
	SYSMAP(struct msgbuf *, msgbufmap, msgbufp, btoc(MSGBUFSIZE));
@


1.41
log
@Provide faster bcopy() and bzero() routines for pmap_copy_page() and
pmap_zero_page().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2007/11/24 11:13:56 miod Exp $	*/
d2081 1
@


1.40
log
@Do not mark any page as global on monoprocessor kernels if running on a
88110, to avoid snooping notification bus cycles, as recommended by the
manual.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2007/11/22 05:47:46 miod Exp $	*/
d797 1
d817 1
a817 3

	bzero((void *)va, PAGE_SIZE);

d2056 1
d2081 1
a2081 1
	bcopy((const void *)srcva, (void *)dstva, PAGE_SIZE);
@


1.39
log
@Remove the cpu parameter from cmmu_set_sapr(), since it is only invoked
for the current processor. And remove now unused cmmu_flush_data_page().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2007/11/22 05:46:07 miod Exp $	*/
d717 4
d861 4
@


1.38
log
@Add an extra parameter to pmap_remove_pte() to control whether it will
flush tlbs or not. This is used by pmap_enter() to avoid flushing the same
tlb entry twice.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2007/11/21 19:41:43 miod Exp $	*/
d733 1
a733 1
	cmmu_set_sapr(cpu, kernel_pmap->pm_apr);
@


1.37
log
@Don't bother flushing caches in pmap_{copy,zero}_page(), since our caches
are physically addressed.

Might be revisited for 88110 SMP, but we're not there yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2007/11/20 21:54:58 miod Exp $	*/
d165 1
a165 1
void	pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *);
d1035 1
a1035 1
pmap_remove_pte(pmap_t pmap, vaddr_t va, pt_entry_t *pte)
d1044 1
a1044 1
		printf("pmap_remove_pte(%p, %p)\n", pmap, va);
d1065 2
a1066 1
	flush_atc_entry(pmap, va);
d1177 2
a1178 1
				pmap_remove_pte(pmap, va, sdt_pte(sdt, va));
d1315 1
a1315 1
		pmap_remove_pte(pmap, va, pte);
d1638 1
a1638 1
		pmap_remove_pte(pmap, va, pte);
d1647 1
a1647 1
				 *	No mappings yet
d1659 3
@


1.36
log
@On 88110, use a shorter path in pmap_is_modified(), since we perform
accurate mod tracking through the write fault traps; there is no need
to walk the pv list since, unlike on 88200, hardware does not set PG_M
or PG_U in page table entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2007/11/20 21:53:25 miod Exp $	*/
a172 1
boolean_t pmap_unsetbit(struct vm_page *, int);
a173 1
int	pmap_set_modify(pmap_t, vaddr_t);
d1685 11
a1695 3
	if (prot & VM_PROT_WRITE)
		template |= PG_M_U;
	else if (prot & VM_PROT_ALL)
a1704 8
	 * On 88110, do not mark writable mappings as dirty unless we
	 * know the page is dirty, or we are using the kernel pmap.
	 */
	if (CPU_IS88110 && pmap != kernel_pmap &&
	    pg != NULL && (pvl->pv_flags & PG_M) == 0)
		template &= ~PG_M;

	/*
a2067 8

	/*
	 * The source page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the source pa flushed before the copy is
	 * attempted, and the destination pa flushed afterwards.
	 */
	cmmu_flush_data_page(cpu, src);
a2068 1
	cmmu_flush_data_page(cpu, dst);
d2388 1
a2388 1
	 * ptes in case the page flags are behind with actual usage.
@


1.35
log
@Move 88110 trap-on-write processing from two duplicated sections in trap.c to
a single function in pmap.c, which will do the right thing and also perform
mod/ref vm_page flags accounting.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2007/11/20 21:48:17 miod Exp $	*/
d807 1
a807 1
	    PG_M /* 88110 */ | PG_V | pa;
a814 5
	/*
	 * The page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the pa flushed after the filling.
	 */
a815 1
	cmmu_flush_data_page(cpu, pa);
d1687 5
d1698 7
a1704 4
	if (flags & VM_PROT_WRITE)
		template |= PG_M_U;
	else if (flags & VM_PROT_ALL)
		template |= PG_U;
d1722 9
a1730 2
	if (pvl != NULL)
		pvl->pv_flags |= template & PG_M_U;
d2061 1
a2061 1
	    PG_M /* 88110 */ | PG_V | dst;
d2063 1
a2063 1
	    PG_V | src;
d2395 21
@


1.34
log
@Overhaul of the pmap debug messages. They now provide slightly better
information, and are easier to parse.

While there, a few never-happening checks move from option DIAGNOSTIC to
option PMAPDEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2007/11/14 23:12:46 miod Exp $	*/
d175 1
d2544 41
@


1.33
log
@Merge the ci_alive and ci_primary boolean values of struct cpu_info into
a single ci_flags bitfield.

Also, set_cpu_number() will no longer set CIF_PRIMARY on the primary processor,
it's up to the initialization code to do this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2007/11/06 21:45:46 miod Exp $	*/
d80 1
a80 1
#ifdef	DEBUG
d108 3
a110 1
#define CD_ALL		0x0FFFFFC
d112 1
a112 1
int pmap_con_dbg = 0;
d114 1
a114 1
#endif	/* DEBUG */
d300 3
a302 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
		printf("(pmap_expand_kmap: %x) v %x\n", curproc, virt);
d309 1
a309 1
#ifdef DEBUG
d380 4
a383 4
#ifdef DEBUG
	if (pmap_con_dbg & CD_MAP)
		printf ("(pmap_map: %x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
			curproc, start, end, virt, prot, cmode);
d386 1
a386 1
#ifdef DEBUG
d388 1
a388 1
	if (start > end && end != 0) {
a389 1
	}
d405 2
a406 2
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
d408 1
a408 1
				printf("(pmap_map: %x) pte @@ %p already valid\n", curproc, pte);
d451 4
a454 12
#ifdef DEBUG
	if ((mode & CACHE_MASK) != mode) {
		printf("(cache_ctrl) illegal mode %x\n", mode);
		return;
	}
	if (pmap_con_dbg & CD_CACHE) {
		printf("(pmap_cache_ctrl: %x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
	}

	if (pmap == NULL)
		panic("pmap_cache_ctrl: pmap is NULL");
#endif /* DEBUG */
d462 4
a465 5
#ifdef DEBUG
		if (pmap_con_dbg & CD_CACHE) {
			printf("(cache_ctrl) pte@@%p\n", pte);
		}
#endif /* DEBUG */
d585 2
a586 2
#ifdef DEBUG
	printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
d614 5
a618 7
#ifdef DEBUG
	printf("--------------------------------------\n");
	printf("        kernel page start = 0x%x\n", kpdt_phys);
	printf("   kernel page table size = 0x%x\n", pdt_size);
	printf("          kernel page end = 0x%x\n", avail_start);

	printf("kpdt_virt = 0x%x\n", kpdt_virt);
d735 1
a735 1
#ifdef DEBUG
d757 2
a758 2
#ifdef DEBUG
	if (pmap_con_dbg & CD_INIT)
d795 5
a850 6
#ifdef DEBUG
	if (pmap_con_dbg & CD_CREAT) {
		printf("(pmap_create: %x) need %d pages for sdt\n",
		    curproc, atop(s));
	}
#endif
d865 4
a868 9
#ifdef DEBUG
	if (stpa & PAGE_MASK)
		panic("pmap_create: sdt_table 0x%x not aligned on page boundary",
		    (int)stpa);

	if (pmap_con_dbg & CD_CREAT) {
		printf("(pmap_create: %x) pmap=%p, pm_stab=0x%x (pa 0x%x)\n",
		    curproc, pmap, pmap->pm_stab, stpa);
	}
d924 3
a926 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_FREE)
		printf("(pmap_release: %x) pmap %x\n", curproc, pmap);
d933 4
a936 4
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
				printf("(pmap_release: %x) free page table = 0x%x\n",
				    curproc, gdttbl);
d946 4
a949 4
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
		printf("(pmap_release: %x) free segment table = 0x%x\n",
		    curproc, sdttbl);
a951 5

#ifdef DEBUG
	if (pmap_con_dbg & CD_FREE)
		printf("(pmap_release: %x) pm_count = 0\n", curproc);
#endif
a980 5
#ifdef DEBUG
	if (pmap == kernel_pmap)
		panic("pmap_destroy: Attempt to destroy kernel pmap");
#endif

d1049 3
a1051 7
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove_pte: %x) pmap kernel va %x\n", curproc, va);
		else
			printf("(pmap_remove_pte: %x) pmap %x va %x\n", curproc, pmap, va);
	}
d1160 3
a1162 7
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove_range: %x) pmap kernel s %x e %x\n", curproc, s, e);
		else
			printf("(pmap_remove_range: %x) pmap %x s %x e %x\n", curproc, pmap, s, e);
	}
a1219 5
#ifdef DEBUG
	if (s >= e)
		panic("pmap_remove: start greater than end address");
#endif

d1270 3
a1272 3
#ifdef DEBUG
		if (pmap_con_dbg & CD_RMAL)
			printf("(pmap_remove_all: %x) vm page 0x%x not a managed page\n", curproc, pg);
d1277 3
a1279 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_RMAL)
		printf("(pmap_remove_all: %x) va %x\n", curproc, pg, pg_to_pvh(pg)->pv_va);
d1311 3
a1313 3
#ifdef DEBUG
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
d1456 3
a1458 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_EXP)
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, pmap, v);
d1481 4
a1484 3
#ifdef DEBUG
		if (pmap_con_dbg & CD_EXP)
			printf("(pmap_expand: %x) table has already been allocated\n", curproc);
d1586 4
a1589 7
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT) {
		if (pmap == kernel_pmap)
			printf("(pmap_enter: %x) pmap kernel va %x pa %x\n", curproc, va, pa);
		else
			printf("(pmap_enter: %x) pmap %x va %x pa %x\n", curproc, pmap, va, pa);
	}
d1604 5
a1608 1
			    flags & PMAP_CANFAIL) == NULL)
d1610 1
d1624 3
a1626 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, *pte);
a1657 1

d1711 3
a1713 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) set pte to %x\n", *pte);
d1877 3
a1879 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_COL)
		printf ("(pmap_collect: %x) pmap %x\n", curproc, pmap);
d1930 3
a1932 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_COL)
		printf("(pmap_collect: %x) done\n", curproc);
d1959 3
a1961 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_ACTIVATE)
		printf("(pmap_activate: %x) pmap %p\n", p, pmap);
d2037 6
d2106 5
d2123 2
a2124 9
	if (pvl->pv_pmap == NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_CBIT)
			printf("(pmap_changebit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		splx(spl);
		return;
	}
d2143 1
a2143 1
#ifdef DIAGNOSTIC
d2169 2
d2207 5
d2222 3
a2224 4
#ifdef DEBUG
		if (pmap_con_dbg & CD_TBIT)
			printf("(pmap_testbit: %x) already cached a %x flag for this page\n",
			    curproc, bit);
d2230 2
a2231 9
	if (pvl->pv_pmap == NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_TBIT)
			printf("(pmap_testbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		splx(spl);
		return (FALSE);
	}
d2247 1
a2247 1
#ifdef DIAGNOSTIC
d2256 6
a2261 3
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_TBIT | CD_FULL)) == (CD_TBIT | CD_FULL))
				printf("(pmap_testbit: %x) true on page pte@@%p\n", curproc, pte);
d2270 5
d2300 5
d2315 4
a2318 1
	pvl->pv_flags &= ~bit;
d2320 2
a2321 9
	if (pvl->pv_pmap == NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_USBIT)
			printf("(pmap_unsetbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		splx(spl);
		return (FALSE);
	}
d2340 1
a2340 1
#ifdef DIAGNOSTIC
a2359 1
		} else
d2361 1
d2367 5
d2432 3
a2434 4
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT) {
		printf ("(pmap_kenter_pa: %x) va %x pa %x\n", curproc, va, pa);
	}
d2480 3
a2482 3
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM)
		printf("(pmap_kremove: %x) va %x len %x\n", curproc, va, len);
@


1.32
log
@Comment out the pmap fine grained locking stuff, it is not necessary for now
because of the global lock. It will get enabled again when locking work
progresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2007/05/27 20:59:25 miod Exp $	*/
d214 1
a214 4
#ifdef DIAGNOSTIC
		if (m88k_cpus[cpu].ci_alive)
#endif
			cmmu_flush_tlb(cpu, kernel, va, 1);
d491 1
a491 1
				if (m88k_cpus[cpu].ci_alive != 0)
@


1.31
log
@pagemove() is no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2007/03/21 19:32:29 miod Exp $	*/
d148 1
a148 1
#ifdef	MULTIPROCESSOR
a551 3
#ifndef MULTIPROCESSOR
	cpuid_t cpu;
#endif
a731 2
	/* Invalidate entire kernel TLB and get ready for address translation */
#ifdef MULTIPROCESSOR
a732 10
#else
	cpu = cpu_number();
	cmmu_flush_tlb(cpu, TRUE, 0, -1);
	/* Load supervisor pointer to segment table. */
	cmmu_set_sapr(cpu, kernel_pmap->pm_apr);
#ifdef DEBUG
	printf("cpu%d: running virtual\n", cpu);
#endif
	SETBIT_CPUSET(cpu, &kernel_pmap->pm_cpus);
#endif	/* MULTIPROCESSOR */
a734 1
#ifdef MULTIPROCESSOR
d738 3
a740 1
	if (cpu != master_cpu) {
d742 2
a743 1
	} else {
d745 1
a745 1
	}
a752 1
#endif
d1330 1
a1330 1
#ifdef MULTIPROCESSOR
d1340 1
a1340 1
#ifdef MULTIPROCESSOR
d2142 1
a2142 1
#ifdef MULTIPROCESSOR
d2165 1
a2165 1
#ifdef MULTIPROCESSOR
d2243 1
a2243 1
#ifdef MULTIPROCESSOR
d2273 1
a2273 1
#ifdef MULTIPROCESSOR
d2331 1
a2331 1
#ifdef MULTIPROCESSOR
d2354 1
a2354 1
#ifdef MULTIPROCESSOR
@


1.30
log
@Map the kernel text read only. Because we can.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2007/02/11 12:49:37 miod Exp $	*/
a107 1
#define CD_PGMV		0x0200000	/* pagemove */
@


1.29
log
@Rework the cache handling routines again. We now try to operate on the exact
address range we've been given, rounded to cache line boundaries, instead
of being lazy and operating on pages as soon as the range was large enough.

Also, since the ranges we'll be invoked for are reasonably small, it does
not make sense to check for segment sizes - we're always smaller, really.

While there, hardcode the size in cmmu_flush_data_cache(), which becomes
cmmu_flush_data_page(), since it was always invoked for complete pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2006/06/01 06:28:11 miod Exp $	*/
d660 1
a660 3
	vaddr = pmap_map(s_text, s_text, e_text,
	    VM_PROT_WRITE | VM_PROT_READ,	/* shouldn't it be RO? XXX*/
	    0);
@


1.28
log
@CACHE_GLOBAL is enforced at the APR level, no need to propagate it into
the individual PTEs. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2006/06/01 06:25:51 miod Exp $	*/
d845 1
a845 1
	cmmu_flush_data_cache(cpu, pa, PAGE_SIZE);
d2118 1
a2118 1
	cmmu_flush_data_cache(cpu, src, PAGE_SIZE);
d2120 1
a2120 1
	cmmu_flush_data_cache(cpu, dst, PAGE_SIZE);
a2565 1
	vaddr_t eva;
d2567 1
d2571 2
a2572 4
	eva = round_page(va + len);
	va = trunc_page(va);

	while (va != eva) {
d2576 1
a2576 1
				cmmu_flush_inst_cache(cpu, pa, PAGE_SIZE);
d2580 2
a2581 1
		va += PAGE_SIZE;
@


1.27
log
@Better test for cacheability of the previous mapping in pmap_cache_ctrl()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2006/05/25 21:37:45 miod Exp $	*/
d662 1
a662 1
	    CACHE_GLOBAL);
d665 1
a665 1
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);
d831 1
a831 1
	    CACHE_GLOBAL | PG_M /* 88110 */ | PG_V | pa;
d909 1
a909 1
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_GLOBAL | CACHE_WT);
d1527 1
a1527 1
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_GLOBAL | CACHE_WT);
a1753 2
	else
		template |= CACHE_GLOBAL;
d2102 1
a2102 1
	    CACHE_GLOBAL | PG_M /* 88110 */ | PG_V | dst;
d2104 1
a2104 1
	    CACHE_GLOBAL | PG_V | src;
d2507 1
a2507 1
		template |= CACHE_GLOBAL | PG_V | PG_W;
@


1.26
log
@When looping over va ranges, do not compute the same sdt pointer more than
once.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2006/05/20 22:33:17 miod Exp $	*/
d491 1
a491 1
		if ((opte & CACHE_MASK) != CACHE_INH) {
@


1.25
log
@Make sure pmap copes with address wraparounds when operating on ranges.
This removes the dependency for VM_MAX_ADDRESS to be aligned on a segment
boundary.
While there, remove a few internal pmap macros from mmu.h.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2006/05/20 15:41:09 miod Exp $	*/
d1208 1
a1208 1
	vaddr_t va;
d1226 4
d1233 7
a1239 8
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += 1 << SDT_SHIFT;
			if (va > e || va == 0)
				va = e;
		} else {
			pmap_remove_pte(pmap, va, sdt_pte(sdt, va));
			va += PAGE_SIZE;
d1419 1
a1419 1
	vaddr_t va;
d1438 4
d1445 17
a1461 15
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += 1 << SDT_SHIFT;
			if (va > e || va == 0)
				va = e;
		} else {
			pte = sdt_pte(sdt, va);
			if (pte != NULL && PDT_VALID(pte)) {
				/*
				 * Invalidate pte temporarily to avoid the
				 * modified bit and/or the reference bit being
				 * written back by any other cpu.
				 */
				*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
				flush_atc_entry(pmap, va);
a1462 1
			va += PAGE_SIZE;
d2521 1
a2521 1
	vaddr_t e;
d2536 4
d2543 9
a2551 11
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += 1 << SDT_SHIFT;
			if (va > e || va == 0)
				va = e;
		} else {
			pte = sdt_pte(sdt, va);
			if (pte != NULL && PDT_VALID(pte)) {
				/* Update the counts */
				kernel_pmap->pm_stats.resident_count--;
				kernel_pmap->pm_stats.wired_count--;
d2553 4
a2556 2
				invalidate_pte(pte);
				flush_atc_entry(kernel_pmap, va);
a2557 1
			va += PAGE_SIZE;
@


1.24
log
@Directly pass a pmap to flush_atc_entry() instead of individual fields of it.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2006/05/06 16:59:28 miod Exp $	*/
d186 2
d243 9
d264 1
a264 2
	return (pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES)) << PDT_SHIFT) +
		PDTIDX(virt);
d470 1
a470 1
	for (va = s; va < e; va += PAGE_SIZE) {
d743 1
a743 2
	cmmu_flush_tlb(cpu, TRUE, VM_MIN_KERNEL_ADDRESS,
	    btoc(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS));
d760 1
a760 2
		cmmu_flush_tlb(cpu, TRUE, VM_MIN_KERNEL_ADDRESS,
		    btoc(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS));
d897 1
a897 1
	if (!PAGE_ALIGNED(stpa))
d951 1
a951 2
 * ranges represented by the table group sizes(PDT_VA_SPACE).
 *
d956 1
a956 1
	unsigned long sdt_va;	/* outer loop index */
d965 4
a968 4
	/* Segment table Loop */
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE) {
		if ((gdttbl = pmap_pte(pmap, (vaddr_t)sdt_va)) != NULL) {
d1222 2
a1223 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1231 6
a1236 2
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
a1237 2

		pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
a1417 5
#ifdef DEBUG
	if (s >= e)
		panic("pmap_protect: start grater than end address");
#endif

d1431 2
a1432 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1440 15
a1454 7
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
		}

		pte = pmap_pte(pmap, va);
		if (pte == NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
a1455 9

		/*
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
		 * written back by any other cpu.
		 */
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
		flush_atc_entry(pmap, va);
		pte++;
d1917 2
a1918 1
	vaddr_t sdt_va;		/* outer loop index */
d1938 4
a1941 3
	/* Segment table loop */
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE, sdtp++) {
d1961 1
a1961 1
		pmap_remove_range(pmap, sdt_va, sdt_va + PDT_VA_SPACE);
d2022 1
a2022 2
		cmmu_flush_tlb(cpu, FALSE, VM_MIN_ADDRESS,
		    btoc(VM_MAX_ADDRESS - VM_MIN_ADDRESS));
d2524 1
a2524 1
	for (; va < e; va += PAGE_SIZE) {
d2533 9
a2541 3
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
		}
d2543 4
a2546 3
		pte = pmap_pte(kernel_pmap, va);
		if (pte == NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
a2547 9

		/*
		 * Update the counts
		 */
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;

		invalidate_pte(pte);
		flush_atc_entry(kernel_pmap, va);
d2565 1
a2565 1
	while (va < eva) {
@


1.23
log
@On 88100-based systems, take the two hardwired BATC into account in
pmap_extract(), and do not bother creating regular page table mappings
for obio regions which are covered by these BATC entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2006/04/26 20:38:37 miod Exp $	*/
d162 1
a162 1
static void flush_atc_entry(long, vaddr_t, boolean_t);
d190 1
a190 3
 *	Flush atc(TLB) which maps given virtual address, in the CPUs which
 *	are specified by 'users', for the operating mode specified by
 *	'kernel'.
d193 1
a193 2
 *	users	bit patterns of the CPUs which may hold the TLB, and
 *		should be flushed
a194 1
 *	kernel	TRUE if supervisor mode, FALSE if user mode
d201 1
a201 1
flush_atc_entry(long users, vaddr_t va, boolean_t kernel)
d204 1
d206 1
d208 1
a208 1
	if (users == 0)
d211 1
d220 2
a221 2
	if (users != 0)
		cmmu_flush_tlb(cpu_number(), kernel, va, 1);
a441 1
	boolean_t kflush;
a442 1
	u_int users;
a459 3
	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

d475 1
a475 1
		flush_atc_entry(users, va, kflush);
a1086 2
	u_int users;
	boolean_t kflush;
a1100 3
	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

d1115 1
a1115 1
	flush_atc_entry(users, va, kflush);
a1406 2
	u_int users;
	boolean_t kflush;
a1422 3
	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

d1449 1
a1449 1
		flush_atc_entry(users, va, kflush);
a1627 2
	u_int users;
	boolean_t kflush;
a1643 2
	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;
d1756 1
a1756 1
	flush_atc_entry(users, va, kflush);
d1852 1
a1852 1
		if (va >= BATC8_VA && pmap == pmap_kernel()) {
a2143 2
	u_int users;
	boolean_t kflush;
a2174 3
		users = pmap->pm_cpus;
		kflush = pmap == kernel_pmap;

d2186 2
a2187 2
			panic("pmap_changebit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
d2205 1
a2205 1
			flush_atc_entry(users, va, kflush);
a2332 2
	u_int users;
	boolean_t kflush;
a2363 3
		users = pmap->pm_cpus;
		kflush = pmap == kernel_pmap;

d2375 2
a2376 2
			panic("pmap_unsetbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
d2392 1
a2392 1
			flush_atc_entry(users, va, kflush);
a2458 1
	u_int users;
a2467 1
	users = kernel_pmap->pm_cpus;
d2497 1
a2497 1
	flush_atc_entry(users, va, TRUE);
a2506 1
	u_int users;
a2515 1
	users = kernel_pmap->pm_cpus;
d2543 1
a2543 1
		flush_atc_entry(users, va, TRUE);
d2555 2
a2556 1
	u_int cpu, users;
@


1.22
log
@In pmap_cache_ctrl(), do not flush cache if the previous mapping was
cache inhibited; from the AV tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2005/12/11 21:45:30 miod Exp $	*/
d61 3
d1864 13
@


1.21
log
@Work in progress SMP code; mvme88k boards can spin up secondary CPUs,
kernel boots single user. Still a lot of polishing and bugfixing to do.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2005/12/03 19:06:11 miod Exp $	*/
d437 1
a437 1
	pt_entry_t *pte;
d476 2
a477 1
		*pte = (invalidate_pte(pte) & ~CACHE_MASK) | mode;
d481 2
a482 1
		 * Data cache should be copied back and invalidated.
d484 2
a485 1
		pa = ptoa(PG_PFNUM(*pte));
d487 2
a488 2
		for (cpu = 0; cpu < MAX_CPUS; cpu++)
			if (m88k_cpus[cpu].ci_alive != 0)
d490 1
a490 1
		cpu = cpu_number();
d492 2
a493 1
				cmmu_flush_cache(cpu, pa, PAGE_SIZE);
@


1.20
log
@Replace simplelocks with __cpu_simple_locks for cmmu and pmap locking,
for the MULTIPROCESSOR case.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2005/12/03 18:54:34 miod Exp $	*/
d58 1
d161 5
a165 5
void pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *);
void pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
void pmap_expand(pmap_t, vaddr_t);
void pmap_release(pmap_t);
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, u_int);
d167 2
a168 2
void pmap_remove_all(struct vm_page *);
void pmap_changebit(struct vm_page *, int, int);
d743 19
a1043 1

a1291 1
 *	__cpu_simple_lock
a2231 1
 *	simple_lock, simple_unlock
@


1.19
log
@Dissociate pmap locks and spl usage; code will now do explicit spl processing
in addition to pmap locking; no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2005/12/03 14:30:06 miod Exp $	*/
a49 1
#include <sys/simplelock.h>
d58 1
d146 2
a147 2
#define	PMAP_LOCK(pmap)		simple_lock(&(pmap)->pm_lock)
#define	PMAP_UNLOCK(pmap)	simple_unlock(&(pmap)->pm_lock)
d513 1
a513 1
 *	simple_lock_init
d547 3
a549 1
	simple_lock_init(&kernel_pmap->pm_lock);
d894 3
a896 1
	simple_lock_init(&pmap->pm_lock);
d1273 1
a1273 1
 *	simple_lock
d1320 1
d1322 1
d1330 2
a1331 1
		if (!simple_lock_try(&pmap->pm_lock))
d1333 1
d2131 1
d2133 1
d2154 2
a2155 1
		if (!simple_lock_try(&pmap->pm_lock)) {
d2158 1
d2236 1
d2238 1
d2266 2
a2267 1
		if (!simple_lock_try(&pmap->pm_lock)) {
d2270 1
d2326 1
d2328 1
d2349 2
a2350 1
		if (!simple_lock_try(&pmap->pm_lock)) {
d2353 1
@


1.18
log
@Switch m88k ports to __HAVE_CPUINFO. Current cpu pointer is held in SR0
on all running processors.
Tested aoyama@@ and I
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2005/12/01 22:24:52 miod Exp $	*/
a56 2
#include <uvm/uvm.h>

d61 2
d145 7
a151 17
/*
 *	We raise the interrupt level to splvm, to block interprocessor
 *	interrupts during pmap operations.
 */
#define	SPLVM(spl)	spl = splvm()
#define	SPLX(spl)	splx(spl)

#define PMAP_LOCK(pmap,spl) \
	do { \
		SPLVM(spl); \
		simple_lock(&(pmap)->pm_lock); \
	} while (0)
#define PMAP_UNLOCK(pmap, spl) \
	do { \
		simple_unlock(&(pmap)->pm_lock); \
		SPLX(spl); \
	} while (0)
d456 2
a457 1
	PMAP_LOCK(pmap, spl);
d490 2
a491 1
	PMAP_UNLOCK(pmap, spl);
d797 1
a797 1
	SPLVM(spl);
d816 1
a816 1
	SPLX(spl);
d995 1
a995 1
	simple_lock(&pmap->pm_lock);
d997 1
a997 1
	simple_unlock(&pmap->pm_lock);
d1021 1
a1021 1
	simple_lock(&pmap->pm_lock);
d1023 1
a1023 1
	simple_unlock(&pmap->pm_lock);
d1247 2
a1248 1
	PMAP_LOCK(pmap, spl);
d1250 2
a1251 1
	PMAP_UNLOCK(pmap, spl);
d1310 1
a1310 1
	SPLVM(spl);
d1351 1
a1351 1
		simple_unlock(&pmap->pm_lock);
d1353 1
a1353 1
	SPLX(spl);
d1400 2
a1401 1
	PMAP_LOCK(pmap, spl);
d1435 2
a1436 1
	PMAP_UNLOCK(pmap, spl);
d1499 2
a1500 1
	PMAP_LOCK(pmap, spl);
d1507 1
a1507 1
		simple_unlock(&pmap->pm_lock);
d1536 2
a1537 1
	PMAP_UNLOCK(pmap, spl);
d1627 2
a1628 1
	PMAP_LOCK(pmap, spl);
d1645 1
a1645 1
			simple_unlock(&pmap->pm_lock);
d1647 1
a1647 1
			simple_lock(&pmap->pm_lock);
d1696 2
a1697 1
						PMAP_UNLOCK(pmap, spl);
d1755 2
a1756 1
	PMAP_UNLOCK(pmap, spl);
d1782 2
a1783 1
	PMAP_LOCK(pmap, spl);
d1794 2
a1795 1
	PMAP_UNLOCK(pmap, spl);
d1833 2
a1834 1
	PMAP_LOCK(pmap, spl);
d1846 2
a1847 1
	PMAP_UNLOCK(pmap, spl);
d1901 2
a1902 1
	PMAP_LOCK(pmap, spl);
d1940 1
a1940 1
		simple_unlock(&pmap->pm_lock);
d1942 1
a1942 1
		simple_lock(&pmap->pm_lock);
d1945 2
a1946 1
	PMAP_UNLOCK(pmap, spl);
d1986 1
a1986 1
		simple_lock(&pmap->pm_lock);
d1996 2
a1997 1
		simple_unlock(&pmap->pm_lock);
d2018 1
a2018 1
		 * we expect the spl is already raised to sched level.
d2020 1
a2020 1
		simple_lock(&pmap->pm_lock);
d2022 1
a2022 1
		simple_unlock(&pmap->pm_lock);
d2061 1
a2061 1
	SPLVM(spl);
d2084 1
a2084 1
	SPLX(spl);
d2121 1
a2121 1
	SPLVM(spl);
d2137 1
a2137 1
		SPLX(spl);
d2183 1
a2183 1
		simple_unlock(&pmap->pm_lock);
d2185 1
a2185 1
	SPLX(spl);
d2219 1
d2222 1
a2222 1
	SPLVM(spl);
d2235 1
a2235 1
		SPLX(spl);
d2245 1
a2245 1
		SPLX(spl);
d2251 2
a2252 1
		if (!simple_lock_try(&pvep->pv_pmap->pm_lock)) {
d2256 1
a2256 1
		pte = pmap_pte(pvep->pv_pmap, pvep->pv_va);
d2264 1
a2264 1
			    *pte, pvep->pv_pmap, pvep->pv_pmap == kernel_pmap ? 1 : 0, pg, VM_PAGE_TO_PHYS(pg));
d2268 1
a2268 1
			simple_unlock(&pvep->pv_pmap->pm_lock);
d2274 1
a2274 1
			SPLX(spl);
d2278 1
a2278 1
		simple_unlock(&pvep->pv_pmap->pm_lock);
d2281 1
a2281 1
	SPLX(spl);
d2308 1
a2308 1
	SPLVM(spl);
d2324 1
a2324 1
		SPLX(spl);
d2369 1
a2369 1
		simple_unlock(&pmap->pm_lock);
d2371 1
a2371 1
	SPLX(spl);
d2440 2
a2441 1
	PMAP_LOCK(kernel_pmap, spl);
d2474 2
a2475 1
	PMAP_UNLOCK(kernel_pmap, spl);
d2490 2
a2491 1
	PMAP_LOCK(kernel_pmap, spl);
d2522 2
a2523 1
	PMAP_UNLOCK(map, spl);
@


1.17
log
@Get rid of PMAP_NULL and xx_ENTRY_NULL and simply use NULL when necessary.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2005/11/25 22:13:50 miod Exp $	*/
d206 5
a210 1
static __inline__ void
d213 1
a213 1
#if defined(MULTIPROCESSOR)
d219 3
a221 4
#ifdef DEBUG
	if (ff1(users) >= MAX_CPUS) {
		panic("flush_atc_entry: invalid ff1 users = %d", ff1(users));
	}
a222 3

	while ((cpu = ff1(users)) != 32) {
		if (cpu_sets[cpu]) { /* just checking to make sure */
d224 1
a224 2
		}
		users &= ~(1 << cpu);
d226 1
a226 1
#else
d229 1
a229 1
#endif
d450 1
a450 1
	int cpu;
d491 1
d493 4
a496 1
			if (cpu_sets[cpu])
d549 3
d733 8
a740 6
	for (i = 0; i < MAX_CPUS; i++)
		if (cpu_sets[i]) {
			cmmu_flush_tlb(i, TRUE, VM_MIN_KERNEL_ADDRESS,
			    btoc(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS));
			/* Load supervisor pointer to segment table. */
			cmmu_set_sapr(i, kernel_pmap->pm_apr);
d742 1
a742 1
			printf("cpu%d: running virtual\n", i);
d744 2
a745 2
			SETBIT_CPUSET(i, &kernel_pmap->pm_cpus);
		}
@


1.16
log
@Need an explicit cast for atop() now.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2005/11/03 21:27:33 martin Exp $	*/
a123 1
#define	KPDT_ENTRY_NULL		((kpdt_entry_t)0)
a132 2
#define PV_ENTRY_NULL	((pv_entry_t) 0)

d240 1
a240 1
 *	exist, PT_ENTRY_NULL is returned (and the map may need to grow).
d259 1
a259 1
		return (PT_ENTRY_NULL);
d315 1
a315 1
	if (kpdt_ent == KPDT_ENTRY_NULL) {
d402 1
a402 1
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
d463 1
a463 1
	if (pmap == PMAP_NULL)
d473 1
a473 1
		if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
d635 1
a635 1
	kpdt_virt->next = KPDT_ENTRY_NULL; /* terminate the list */
d705 1
a705 1
	if ((p = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL) \
d936 1
a936 1
		if ((gdttbl = pmap_pte(pmap, (vaddr_t)sdt_va)) != PT_ENTRY_NULL) {
d1075 1
a1075 1
	if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d1111 1
a1111 1
	if (pvl->pv_pmap == PMAP_NULL)
d1115 2
a1116 2
	prev = PV_ENTRY_NULL;
	for (cur = pvl; cur != PV_ENTRY_NULL; cur = cur->pv_next) {
d1121 1
a1121 1
	if (cur == PV_ENTRY_NULL) {
d1127 1
a1127 1
	if (prev == PV_ENTRY_NULL) {
d1134 1
a1134 1
		if (cur != PV_ENTRY_NULL) {
d1139 1
a1139 1
			pvl->pv_pmap = PMAP_NULL;
d1239 1
a1239 1
	if (pmap == PMAP_NULL)
d1321 1
a1321 1
	while (pvl != PV_ENTRY_NULL && (pmap = pvl->pv_pmap) != PMAP_NULL) {
d1328 1
a1328 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d1419 1
a1419 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d1497 1
a1497 1
	if ((pte = pmap_pte(pmap, v)) != PT_ENTRY_NULL) {
d1628 1
a1628 1
	while ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
d1673 1
a1673 1
			if (pvl->pv_pmap == PMAP_NULL) {
d1679 1
a1679 1
				pvl->pv_next = PV_ENTRY_NULL;
d1775 1
a1775 1
	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
d1818 1
a1818 1
	if (pmap == PMAP_NULL)
d1825 1
a1825 1
	if (pte != PT_ENTRY_NULL && PDT_VALID(pte)) {
d1896 1
a1896 1
		if (gdttbl == PT_ENTRY_NULL)
d2115 1
a2115 1
	if (pvl->pv_pmap == PMAP_NULL) {
d2126 1
a2126 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
d2140 1
a2140 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d2222 1
a2222 1
	if (pvl->pv_pmap == PMAP_NULL) {
d2233 1
a2233 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
d2239 1
a2239 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d2300 1
a2300 1
	if (pvl->pv_pmap == PMAP_NULL) {
d2311 1
a2311 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
d2325 1
a2325 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d2434 1
a2434 1
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL)
d2488 1
a2488 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
@


1.15
log
@fix comment

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2005/10/13 19:48:33 miod Exp $	*/
d726 1
a726 1
	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) |
@


1.14
log
@Merge <machine/cpu_number.h> into <machine/cpu.h>, preparing for intrusive
changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2005/10/12 19:05:44 miod Exp $	*/
d1197 1
a1197 1
	 * Loop through the range in vm_page_size increments.
d1407 1
a1407 1
	 * Loop through the range in vm_page_size increments.
@


1.13
log
@Stop mapping the u area at fixed UADDR in addition to its actual va.
While there, attempt to clean and comment stack usage in the kernel.
No functional change.

From the m88k SMP tree; help&test martin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2005/09/25 20:55:14 miod Exp $	*/
a60 1
#include <machine/cpu_number.h>
@


1.12
log
@Change the size parameter of cmmu_flush_tlb() from bytes to pages. This makes
things easier for the callers, and allows us to inline the "fewer than 4 pages"
situation for speed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2005/09/15 21:07:05 miod Exp $	*/
a547 1
	pt_entry_t *pte;
a721 13

	/*
	 * Set translation for UPAGES at UADDR. The idea is we want to
	 * have translations set up for UADDR. Later on, the ptes for
	 * for this address will be set so that kstack will refer
	 * to the u area. Make sure pmap knows about this virtual
	 * address by doing vm_findspace on kernel_map.
	 */

	for (i = 0, virt = UADDR; i < UPAGES; i++, virt += PAGE_SIZE) {
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE, 0);
	}
@


1.11
log
@Define PG_M_U as PG_M | PG_U, and use it where appropriate; no functional
change today, will become useful in the neat future.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 2005/09/06 19:21:57 miod Exp $	*/
d227 1
a227 1
			cmmu_flush_tlb(cpu, kernel, va, PAGE_SIZE);
d233 1
a233 1
		cmmu_flush_tlb(cpu_number(), kernel, va, PAGE_SIZE);
a257 7
#ifdef DEBUG
	/*XXX will this change if physical memory is not contiguous? */
	/* take a look at PDTIDX XXXnivas */
	if (pmap == PMAP_NULL)
		panic("pmap_pte: pmap is NULL");
#endif

d748 1
a748 1
			    VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS);
d824 1
a824 1
	cmmu_flush_tlb(cpu, TRUE, va, PAGE_SIZE);
d1993 1
a1993 1
		    VM_MAX_ADDRESS - VM_MIN_ADDRESS);
d2074 1
a2074 1
	cmmu_flush_tlb(cpu, TRUE, dstva, 2 * PAGE_SIZE);
@


1.10
log
@Remove misleading NCPUS (hardcoded) option. From the SMP tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 2005/05/22 19:40:51 art Exp $	*/
d1120 1
a1120 1
	opte = invalidate_pte(pte) & (PG_U | PG_M);
d1749 1
a1749 1
		template |= PG_U | PG_M;
d1758 1
a1758 1
	template |= invalidate_pte(pte) & (PG_U | PG_M);
d1770 1
a1770 1
		pvl->pv_flags |= (template & (PG_U | PG_M));
@


1.9
log
@remove "0x" in front of %p in printfs.

eyeballed by krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2005/04/04 11:45:33 miod Exp $	*/
d213 1
a213 1
#if NCPUS > 1
@


1.8
log
@Fix an uninitialized variable in pmap_enter(), affecting only unmanaged pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 2005/04/04 11:44:50 miod Exp $	*/
d420 1
a420 1
				printf("(pmap_map: %x) pte @@ 0x%p already valid\n", curproc, pte);
d488 1
a488 1
			printf("(cache_ctrl) pte@@0x%p\n", pte);
d896 1
a896 1
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x (pa 0x%x)\n",
d1148 1
a1148 1
		    "0x%lx (pa 0x%lx) not in pv list at 0x%p",
d1989 1
a1989 1
		printf("(pmap_activate: %x) pmap 0x%p\n", p, pmap);
d2279 1
a2279 1
				printf("(pmap_testbit: %x) true on page pte@@0x%p\n", curproc, pte);
@


1.7
log
@Allow pmap_expand_kmap() to fail in low memory conditions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 2004/10/01 18:58:09 miod Exp $	*/
a1641 6

	/* copying/zeroing pages are magic */
	if (pmap == kernel_pmap &&
	    va >= phys_map_vaddr && va < phys_map_vaddr_end) {
		return 0;
	}
d1676 7
d1689 1
a1689 3

		pvl = NULL;
	} else { /* if (pa == old_pa) */
d1693 1
a1693 2
		pg = PHYS_TO_VM_PAGE(pa);
		if (pg != NULL) {
d1695 2
a1696 2
			 *	Enter the mapping in the PV list for this
			 *	physical page.
a1697 2
			pvl = pg_to_pvh(pg);

a1707 9
#ifdef DEBUG
				/*
				 * Check that this mapping is not already there
				 */
				for (pv_e = pvl; pv_e; pv_e = pv_e->pv_next)
					if (pv_e->pv_pmap == pmap &&
					    pv_e->pv_va == va)
						panic("pmap_enter: already in pv_list");
#endif
d1740 3
d2469 4
@


1.6
log
@Get rid of CHECK_PAGE_ALIGN debug macros.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 2004/09/30 21:48:56 miod Exp $	*/
d173 1
a173 1
pt_entry_t *pmap_expand_kmap(vaddr_t, vm_prot_t);
d306 1
a306 1
pmap_expand_kmap(vaddr_t virt, vm_prot_t prot)
d326 6
a331 2
	if (kpdt_ent == KPDT_ENTRY_NULL)
		panic("pmap_expand_kmap: Ran out of kernel pte tables");
d415 1
a415 1
			    VM_PROT_READ | VM_PROT_WRITE);
d718 1
a718 1
		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
d741 1
a741 1
			pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE);
d1661 4
a1664 1
			pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
d2470 1
a2470 1
		pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
@


1.5
log
@Introduce sparc-like CPU_ISFOO macros, to short-circuit evaluations when
compiling for one flavour only.

This makes code slightly smaller on all m88k-based platforms (my roughly
1KB), and saves more than meets the eye on luna88k, which is m88100-based.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 2004/08/04 15:54:38 miod Exp $	*/
a109 12
/*
 * Alignment checks for pages (must lie on page boundaries).
 */
#define PAGE_ALIGNED(ad)	(((vaddr_t)(ad) & PAGE_MASK) == 0)
#define	CHECK_PAGE_ALIGN(ad, who) \
	if (!PAGE_ALIGNED(ad)) \
		printf("%s: addr %x not page aligned.\n", who, ad)

#else	/* DEBUG */

#define	CHECK_PAGE_ALIGN(ad, who)

a814 2
	CHECK_PAGE_ALIGN(pa, "pmap_zero_page");

a1389 1
 *		CHECK_PAGE_ALIGN
a1423 2
	CHECK_PAGE_ALIGN(s, "pmap_protect");

a1506 2
	CHECK_PAGE_ALIGN(v, "pmap_expand");

a1630 3
	CHECK_PAGE_ALIGN(va, "pmap_entry - va");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - pa");

a2067 3
	CHECK_PAGE_ALIGN(src, "pmap_copy_page - src");
	CHECK_PAGE_ALIGN(dst, "pmap_copy_page - dst");

a2443 3
	CHECK_PAGE_ALIGN(va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_kenter_pa - PA");

a2492 3

	CHECK_PAGE_ALIGN(va, "pmap_kremove addr");
	CHECK_PAGE_ALIGN(len, "pmap_kremove len");
@


1.4
log
@Completely remove BATC code. BATC on 88200 are way too small to be worth
using as part of the general pmap machinery (though they might come back
at some point to speed up I/O mappings), and we don't use the 88110 BATC
yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 2004/08/02 08:34:59 miod Exp $	*/
d414 1
a414 1
	if (cputyp == CPU_88110 && m88k_protection(prot) != PG_RO)
d2483 1
a2483 1
	if (cputyp == CPU_88110 && m88k_protection(prot) != PG_RO)
@


1.3
log
@More include files cleaning:
- move MAX_CPUS constant to <machine/cpu.h>
- do not include <machine/board.h> unless needed. In fact, remove this file
  entirely on mvme88k, and include <machine/mvme*.h> on a
  compiling-for-this-board basis
- keep MAX_CMMUS constant private to the m8820x code
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 2004/07/26 11:08:20 miod Exp $	*/
a178 14
#ifdef	PMAP_USE_BATC

/*
 * number of BATC entries used
 */
int batc_used;

/*
 * keep track BATC mapping
 */
batc_entry_t batc_entry[BATC_MAX];

#endif	/* PMAP_USE_BATC */

a397 4
#ifdef	PMAP_USE_BATC
	u_int32_t batctmp;
	int i;
#endif
a417 12
#ifdef	PMAP_USE_BATC
	batctmp = BATC_SO | BATC_V;
	if (template & CACHE_WT)
		batctmp |= BATC_WT;
	if (template & CACHE_GLOBAL)
		batctmp |= BATC_GLOBAL;
	if (template & CACHE_INH)
		batctmp |= BATC_INH;
	if (template & PG_PROT)
		batctmp |= BATC_PROT;
#endif

a420 44
#ifdef	PMAP_USE_BATC

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			printf("(pmap_map: %x) num_phys_pg=%x, virt=%x, "
			    "align V=%d, page=%x, align P=%d\n",
			    curproc, num_phys_pages, virt,
			    BATC_BLK_ALIGNED(virt), page,
			    BATC_BLK_ALIGNED(page));
#endif

		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) &&
		     num_phys_pages >= BATC_BLKBYTES/PAGE_SIZE &&
		     batc_used < BATC_MAX ) {
			/*
			 * map by BATC
			 */
			batctmp |= M88K_BTOBLK(virt) << BATC_VSHIFT;
			batctmp |= M88K_BTOBLK(page) << BATC_PSHIFT;

			for (i = 0; i < MAX_CPUS; i++)
				if (cpu_sets[i])
					cmmu_set_pair_batc_entry(i, batc_used,
					    batctmp);
			batc_entry[batc_used] = batctmp;
#ifdef DEBUG
			if (pmap_con_dbg & CD_MAP) {
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp);
				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE) {
					pte = pmap_pte(kernel_pmap, virt + i);
					if (PDT_VALID(pte))
						printf("(pmap_map: %x) va %x is already mapped: pte %x\n",
						    curproc, virt + i, *pte);
				}
			}
#endif
			batc_used++;
			virt += BATC_BLKBYTES;
			page += BATC_BLKBYTES;
			num_phys_pages -= BATC_BLKBYTES/PAGE_SIZE;
			continue;
		}
#endif	/* PMAP_USE_BATC */

a871 3
#ifdef	PMAP_USE_BATC
	int i;
#endif
a929 8
#ifdef	PMAP_USE_BATC
	/* initialize block address translation cache */
	for (i = 0; i < BATC_MAX; i++) {
		pmap->pm_ibatc[i].bits = 0;
		pmap->pm_dbatc[i].bits = 0;
	}
#endif

d1853 1
a1853 4
 * If BATC mapping is enabled and the specified pmap is kernel_pmap,
 * batc_entry is scanned to find out the mapping.
 *
 * Then the routine calls pmap_pte to get a (virtual) pointer to
a1866 4
#ifdef	PMAP_USE_BATC
	int i;
#endif

a1871 14
#ifdef	PMAP_USE_BATC
	/*
	 * check BATC first
	 */
	if (pmap == kernel_pmap && batc_used != 0)
		for (i = batc_used - 1; i != 0; i--)
			if (batc_entry[i].lba == M88K_BTOBLK(va)) {
				if (pap != NULL)
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) |
						(va & BATC_BLKMASK);
				return TRUE;
			}
#endif

a2010 3
#ifdef	PMAP_USE_BATC
	int n;
#endif
a2022 12
#ifdef	PMAP_USE_BATC
		/*
		 * cmmu_pmap_activate will set the uapr and the batc entries,
		 * then flush the *USER* TLB. IF THE KERNEL WILL EVER CARE
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE
		 * FLUSHED AS WELL.
		 */
		cmmu_pmap_activate(cpu, pmap->pm_apr,
		    pmap->pm_ibatc, pmap->pm_dbatc);
		for (n = 0; n < BATC_MAX; n++)
			*(register_t *)&batc_entry[n] = pmap->pm_ibatc[n].bits;
#else
a2025 1
#endif	/* PMAP_USE_BATC */
@


1.2
log
@Provide a real pmap_proc_iflush() routine.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.1 2004/07/25 11:06:42 miod Exp $	*/
a59 1
#include <machine/board.h>
@


1.1
log
@Merge luna88k and mvme88k pmap. The pmap will now assume that the memory
below the kernel text is reserved for the PROM, instead of using fixed
(but different) values between luna88k and mvme88k.

Tested on mvme88k by myself, on luna88k by aoyama@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d2680 23
@

