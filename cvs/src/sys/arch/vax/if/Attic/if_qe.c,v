head	1.42;
access;
symbols
	OPENBSD_5_9:1.41.0.2
	OPENBSD_5_9_BASE:1.41
	OPENBSD_5_8:1.35.0.4
	OPENBSD_5_8_BASE:1.35
	OPENBSD_5_7:1.30.0.2
	OPENBSD_5_7_BASE:1.30
	OPENBSD_5_6:1.28.0.4
	OPENBSD_5_6_BASE:1.28
	OPENBSD_5_5:1.26.0.4
	OPENBSD_5_5_BASE:1.26
	OPENBSD_5_4:1.25.0.12
	OPENBSD_5_4_BASE:1.25
	OPENBSD_5_3:1.25.0.10
	OPENBSD_5_3_BASE:1.25
	OPENBSD_5_2:1.25.0.8
	OPENBSD_5_2_BASE:1.25
	OPENBSD_5_1_BASE:1.25
	OPENBSD_5_1:1.25.0.6
	OPENBSD_5_0:1.25.0.4
	OPENBSD_5_0_BASE:1.25
	OPENBSD_4_9:1.25.0.2
	OPENBSD_4_9_BASE:1.25
	OPENBSD_4_8:1.24.0.8
	OPENBSD_4_8_BASE:1.24
	OPENBSD_4_7:1.24.0.4
	OPENBSD_4_7_BASE:1.24
	OPENBSD_4_6:1.24.0.6
	OPENBSD_4_6_BASE:1.24
	OPENBSD_4_5:1.24.0.2
	OPENBSD_4_5_BASE:1.24
	OPENBSD_4_4:1.21.0.4
	OPENBSD_4_4_BASE:1.21
	OPENBSD_4_3:1.21.0.2
	OPENBSD_4_3_BASE:1.21
	OPENBSD_4_2:1.20.0.6
	OPENBSD_4_2_BASE:1.20
	OPENBSD_4_1:1.20.0.4
	OPENBSD_4_1_BASE:1.20
	OPENBSD_4_0:1.20.0.2
	OPENBSD_4_0_BASE:1.20
	OPENBSD_3_9:1.18.0.8
	OPENBSD_3_9_BASE:1.18
	OPENBSD_3_8:1.18.0.6
	OPENBSD_3_8_BASE:1.18
	OPENBSD_3_7:1.18.0.4
	OPENBSD_3_7_BASE:1.18
	OPENBSD_3_6:1.18.0.2
	OPENBSD_3_6_BASE:1.18
	SMP_SYNC_A:1.17
	SMP_SYNC_B:1.17
	OPENBSD_3_5:1.17.0.4
	OPENBSD_3_5_BASE:1.17
	OPENBSD_3_4:1.17.0.2
	OPENBSD_3_4_BASE:1.17
	UBC_SYNC_A:1.17
	OPENBSD_3_3:1.16.0.2
	OPENBSD_3_3_BASE:1.16
	OPENBSD_3_2:1.13.0.4
	OPENBSD_3_2_BASE:1.13
	OPENBSD_3_1:1.13.0.2
	OPENBSD_3_1_BASE:1.13
	UBC_SYNC_B:1.14
	UBC:1.12.0.6
	UBC_BASE:1.12
	OPENBSD_3_0:1.12.0.4
	OPENBSD_3_0_BASE:1.12
	OPENBSD_2_9_BASE:1.12
	OPENBSD_2_9:1.12.0.2
	OPENBSD_2_8:1.11.0.4
	OPENBSD_2_8_BASE:1.11
	OPENBSD_2_7:1.11.0.2
	OPENBSD_2_7_BASE:1.11
	SMP:1.10.0.4
	SMP_BASE:1.10
	kame_19991208:1.10
	OPENBSD_2_6:1.10.0.2
	OPENBSD_2_6_BASE:1.10
	OPENBSD_2_5:1.9.0.8
	OPENBSD_2_5_BASE:1.9
	OPENBSD_2_4:1.9.0.6
	OPENBSD_2_4_BASE:1.9
	OPENBSD_2_3:1.9.0.4
	OPENBSD_2_3_BASE:1.9
	OPENBSD_2_2:1.9.0.2
	OPENBSD_2_2_BASE:1.9
	OPENBSD_2_1:1.7.0.2
	OPENBSD_2_1_BASE:1.7
	OPENBSD_2_0:1.6.0.2
	OPENBSD_2_0_BASE:1.6
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.42
date	2016.03.09.16.28.48;	author deraadt;	state dead;
branches;
next	1.41;
commitid	OSDG2O3Cgeifnf1W;

1.41
date	2016.01.25.00.18.55;	author dlg;	state Exp;
branches;
next	1.40;
commitid	EgODt4ORIyjjhVEB;

1.40
date	2015.11.25.03.09.58;	author dlg;	state Exp;
branches;
next	1.39;
commitid	B0kwmVGiD5DVx4kv;

1.39
date	2015.11.24.17.11.38;	author mpi;	state Exp;
branches;
next	1.38;
commitid	5gdEnqVoJuTuwdTu;

1.38
date	2015.11.20.03.35.22;	author dlg;	state Exp;
branches;
next	1.37;
commitid	eYnPulzvLjDImPCa;

1.37
date	2015.11.14.17.26.40;	author mpi;	state Exp;
branches;
next	1.36;
commitid	BU9j1tIQLqhcXEX9;

1.36
date	2015.10.27.15.20.13;	author mpi;	state Exp;
branches;
next	1.35;
commitid	iTpqmkP6HKjkg92B;

1.35
date	2015.07.04.10.12.52;	author dlg;	state Exp;
branches;
next	1.34;
commitid	sOVbwdjLkj31qVxi;

1.34
date	2015.06.27.15.40.53;	author miod;	state Exp;
branches;
next	1.33;
commitid	51r4VBMTkslvpS0i;

1.33
date	2015.06.27.15.39.03;	author miod;	state Exp;
branches;
next	1.32;
commitid	tIgTZKphbcMt15Ux;

1.32
date	2015.05.13.10.42.46;	author jsg;	state Exp;
branches;
next	1.31;
commitid	hN5bFCE56DrAjl99;

1.31
date	2015.04.07.14.02.51;	author mpi;	state Exp;
branches;
next	1.30;
commitid	j8DR4xyIQoJ2dupG;

1.30
date	2014.12.23.21.39.12;	author miod;	state Exp;
branches;
next	1.29;
commitid	foSDoFEw0OUhYWGt;

1.29
date	2014.12.22.02.26.54;	author tedu;	state Exp;
branches;
next	1.28;
commitid	2Ez9mHW0jDzojG4V;

1.28
date	2014.08.06.15.40.40;	author jsg;	state Exp;
branches;
next	1.27;
commitid	CyMTRXwZURUEjr2F;

1.27
date	2014.07.12.18.44.43;	author tedu;	state Exp;
branches;
next	1.26;
commitid	uKVPYMN2MLxdZxzH;

1.26
date	2013.11.27.08.56.31;	author mpi;	state Exp;
branches;
next	1.25;

1.25
date	2010.09.20.06.33.47;	author matthew;	state Exp;
branches;
next	1.24;

1.24
date	2008.11.28.02.44.17;	author brad;	state Exp;
branches;
next	1.23;

1.23
date	2008.10.08.23.53.08;	author brad;	state Exp;
branches;
next	1.22;

1.22
date	2008.10.02.20.21.13;	author brad;	state Exp;
branches;
next	1.21;

1.21
date	2007.09.17.01.33.33;	author krw;	state Exp;
branches;
next	1.20;

1.20
date	2006.04.16.00.46.32;	author pascoe;	state Exp;
branches;
next	1.19;

1.19
date	2006.03.25.22.41.42;	author djm;	state Exp;
branches;
next	1.18;

1.18
date	2004.07.07.23.10.45;	author deraadt;	state Exp;
branches;
next	1.17;

1.17
date	2003.05.11.19.41.12;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	2003.02.05.00.05.15;	author hugh;	state Exp;
branches;
next	1.15;

1.15
date	2003.02.04.02.03.51;	author hugh;	state Exp;
branches;
next	1.14;

1.14
date	2002.10.12.01.09.44;	author krw;	state Exp;
branches;
next	1.13;

1.13
date	2002.03.14.01.26.48;	author millert;	state Exp;
branches;
next	1.12;

1.12
date	2001.02.20.19.39.35;	author mickey;	state Exp;
branches
	1.12.6.1;
next	1.11;

1.11
date	2000.04.27.03.14.43;	author bjc;	state Exp;
branches;
next	1.10;

1.10
date	99.05.13.15.44.50;	author jason;	state Exp;
branches
	1.10.4.1;
next	1.9;

1.9
date	97.09.10.08.28.41;	author maja;	state Exp;
branches;
next	1.8;

1.8
date	97.05.29.00.04.32;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	97.01.15.23.24.33;	author maja;	state Exp;
branches;
next	1.6;

1.6
date	96.06.12.08.20.22;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	96.05.05.13.38.04;	author mickey;	state Exp;
branches;
next	1.4;

1.4
date	96.05.03.09.09.32;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	95.12.27.22.32.53;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.12.14.05.27.15;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.05;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.05;	author deraadt;	state Exp;
branches;
next	;

1.10.4.1
date	2001.05.14.21.37.54;	author niklas;	state Exp;
branches;
next	1.10.4.2;

1.10.4.2
date	2002.03.28.11.26.46;	author niklas;	state Exp;
branches;
next	1.10.4.3;

1.10.4.3
date	2003.03.27.23.52.19;	author niklas;	state Exp;
branches;
next	1.10.4.4;

1.10.4.4
date	2003.05.13.19.41.09;	author ho;	state Exp;
branches;
next	;

1.12.6.1
date	2002.06.11.03.39.19;	author art;	state Exp;
branches;
next	1.12.6.2;

1.12.6.2
date	2002.10.29.00.28.13;	author art;	state Exp;
branches;
next	1.12.6.3;

1.12.6.3
date	2003.05.19.21.46.09;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.42
log
@We are done providing support for the vax.
lots of agreement.
@
text
@/*	$OpenBSD: if_qe.c,v 1.41 2016/01/25 00:18:55 dlg Exp $	*/
/*      $NetBSD: if_qe.c,v 1.51 2002/06/08 12:28:37 ragge Exp $ */
/*
 * Copyright (c) 1999 Ludd, University of Lule}, Sweden. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed at Ludd, University of 
 *      Lule}, Sweden and its contributors.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Driver for DEQNA/DELQA ethernet cards.
 * Things that is still to do:
 *	Handle ubaresets. Does not work at all right now.
 *	Fix ALLMULTI reception. But someone must tell me how...
 *	Collect statistics.
 */

#include "bpfilter.h"

#include <sys/param.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/device.h>
#include <sys/systm.h>
#include <sys/sockio.h>

#include <net/if.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#include <machine/bus.h>

#include <arch/vax/qbus/ubavar.h>
#include <arch/vax/if/if_qereg.h>

#define RXDESCS	30	/* # of receive descriptors */
#define TXDESCS	60	/* # transmit descs */

/*
 * Structure containing the elements that must be in DMA-safe memory.
 */
struct qe_cdata {
	struct qe_ring	qc_recv[RXDESCS+1];	/* Receive descriptors */
	struct qe_ring	qc_xmit[TXDESCS+1];	/* Transmit descriptors */
	u_int8_t	qc_setup[128];		/* Setup packet layout */
};

struct	qe_softc {
	struct device	sc_dev;		/* Configuration common part	*/
	struct evcount	sc_intrcnt;	/* Interrupt counting		*/
	int		sc_cvec;
	struct arpcom	sc_ac;		/* Ethernet common part		*/
#define sc_if	sc_ac.ac_if		/* network-visible interface	*/
	bus_space_tag_t sc_iot;
	bus_addr_t	sc_ioh;
	bus_dma_tag_t	sc_dmat;
	struct qe_cdata *sc_qedata;	/* Descriptor struct		*/
	struct qe_cdata *sc_pqedata;	/* Unibus address of above	*/
	struct mbuf*	sc_txmbuf[TXDESCS];
	struct mbuf*	sc_rxmbuf[RXDESCS];
	bus_dmamap_t	sc_xmtmap[TXDESCS];
	bus_dmamap_t	sc_rcvmap[RXDESCS];
	struct ubinfo	sc_ui;
	int		sc_intvec;	/* Interrupt vector		*/
	int		sc_nexttx;
	int		sc_inq;
	int		sc_lastack;
	int		sc_nextrx;
	int		sc_setup;	/* Setup packet in queue	*/
};

static	int	qematch(struct device *, struct cfdata *, void *);
static	void	qeattach(struct device *, struct device *, void *);
static	void	qeinit(struct qe_softc *);
static	void	qestart(struct ifnet *);
static	void	qeintr(void *);
static	int	qeioctl(struct ifnet *, u_long, caddr_t);
static	int	qe_add_rxbuf(struct qe_softc *, int);
static	void	qe_setup(struct qe_softc *);
static	void	qetimeout(struct ifnet *);

struct	cfattach qe_ca = {
	sizeof(struct qe_softc), (cfmatch_t)qematch, qeattach
};

struct cfdriver qe_cd = {
	NULL, "qe", DV_IFNET
};

#define	QE_WCSR(iot, ioh, csr, val) \
	bus_space_write_2(iot, ioh, csr, val)
#define	QE_RCSR(iot, ioh, csr) \
	bus_space_read_2(iot, ioh, csr)

#define	LOWORD(x)	((int)(x) & 0xffff)
#define	HIWORD(x)	(((int)(x) >> 16) & 0x3f)

/*
 * Check for present DEQNA. Done by sending a fake setup packet
 * and wait for interrupt.
 */
int
qematch(struct device *parent, struct cfdata *cf, void *aux)
{
	struct	uba_attach_args *ua = aux;
	struct	uba_softc *ubasc = (struct uba_softc *)parent;
	struct ubinfo ui;
#define	PROBESIZE	4096
	struct qe_ring *ring;
	struct	qe_ring *rp;
	int error;

	ring = malloc(PROBESIZE, M_TEMP, M_WAITOK | M_ZERO);

	ubasc->uh_lastiv -= 4;
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_CSR, QE_RESET);
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_VECTOR, ubasc->uh_lastiv);

	/*
	 * Map the ring area. Actually this is done only to be able to 
	 * send and receive a internal packet; some junk is loopbacked
	 * so that the DEQNA has a reason to interrupt.
	 */
	ui.ui_size = PROBESIZE;
	ui.ui_vaddr = (caddr_t)&ring[0];
	if ((error = uballoc((void *)parent, &ui, UBA_CANTWAIT))) {
		free(ring, M_TEMP, PROBESIZE);
		return 0;
	}

	/*
	 * Init a simple "fake" receive and transmit descriptor that
	 * points to some unused area. Send a fake setup packet.
	 */
	rp = (void *)ui.ui_baddr;
	ring[0].qe_flag = ring[0].qe_status1 = QE_NOTYET;
	ring[0].qe_addr_lo = LOWORD(&rp[4]);
	ring[0].qe_addr_hi = HIWORD(&rp[4]) | QE_VALID | QE_EOMSG | QE_SETUP;
	ring[0].qe_buf_len = -64;

	ring[2].qe_flag = ring[2].qe_status1 = QE_NOTYET;
	ring[2].qe_addr_lo = LOWORD(&rp[4]);
	ring[2].qe_addr_hi = HIWORD(&rp[4]) | QE_VALID;
	ring[2].qe_buf_len = -(1500/2);

	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_CSR,
	    QE_RCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_CSR) & ~QE_RESET);
	DELAY(1000);

	/*
	 * Start the interface and wait for the packet.
	 */
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_CSR,
	    QE_INT_ENABLE|QE_XMIT_INT|QE_RCV_INT);
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_RCLL, LOWORD(&rp[2]));
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_RCLH, HIWORD(&rp[2]));
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_XMTL, LOWORD(rp));
	QE_WCSR(ua->ua_iot, ua->ua_ioh, QE_CSR_XMTH, HIWORD(rp));
	DELAY(10000);

	/*
	 * All done with the bus resources.
	 */
	ubfree((void *)parent, &ui);
	free(ring, M_TEMP, PROBESIZE);
	return 1;
}

/*
 * Interface exists: make available by filling in network interface
 * record.  System will initialize the interface when it is ready
 * to accept packets.
 */
void
qeattach(struct device *parent, struct device *self, void *aux)
{
	struct	uba_attach_args *ua = aux;
	struct	uba_softc *ubasc = (struct uba_softc *)parent;
	struct	qe_softc *sc = (struct qe_softc *)self;
	struct	ifnet *ifp = (struct ifnet *)&sc->sc_if;
	struct	qe_ring *rp;
	int i, error;

	sc->sc_iot = ua->ua_iot;
	sc->sc_ioh = ua->ua_ioh;
	sc->sc_dmat = ua->ua_dmat;

        /*
         * Allocate DMA safe memory for descriptors and setup memory.
         */

	sc->sc_ui.ui_size = sizeof(struct qe_cdata);
	if ((error = ubmemalloc((struct uba_softc *)parent, &sc->sc_ui, 0))) {
		printf(": unable to ubmemalloc(), error = %d\n", error);
		return;
	}
	sc->sc_pqedata = (struct qe_cdata *)sc->sc_ui.ui_baddr;
	sc->sc_qedata = (struct qe_cdata *)sc->sc_ui.ui_vaddr;

	/*
	 * Zero the newly allocated memory.
	 */
	bzero(sc->sc_qedata, sizeof(struct qe_cdata));
	/*
	 * Create the transmit descriptor DMA maps. We take advantage
	 * of the fact that the Qbus address space is big, and therefore 
	 * allocate map registers for all transmit descriptors also,
	 * so that we can avoid this each time we send a packet.
	 */
	for (i = 0; i < TXDESCS; i++) {
		if ((error = bus_dmamap_create(sc->sc_dmat, MCLBYTES,
		    1, MCLBYTES, 0, BUS_DMA_NOWAIT|BUS_DMA_ALLOCNOW,
		    &sc->sc_xmtmap[i]))) {
			printf(": unable to create tx DMA map %d, error = %d\n",
			    i, error);
			goto fail_4;
		}
	}

	/*
	 * Create receive buffer DMA maps.
	 */
	for (i = 0; i < RXDESCS; i++) {
		if ((error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1,
		    MCLBYTES, 0, BUS_DMA_NOWAIT,
		    &sc->sc_rcvmap[i]))) {
			printf(": unable to create rx DMA map %d, error = %d\n",
			    i, error);
			goto fail_5;
		}
	}
	/*
	 * Pre-allocate the receive buffers.
	 */
	for (i = 0; i < RXDESCS; i++) {
		if ((error = qe_add_rxbuf(sc, i)) != 0) {
			printf(": unable to allocate or map rx buffer %d\n,"
			    " error = %d\n", i, error);
			goto fail_6;
		}
	}

	/*
	 * Create ring loops of the buffer chains.
	 * This is only done once.
	 */

	rp = sc->sc_qedata->qc_recv;
	rp[RXDESCS].qe_addr_lo = LOWORD(&sc->sc_pqedata->qc_recv[0]);
	rp[RXDESCS].qe_addr_hi = HIWORD(&sc->sc_pqedata->qc_recv[0]) |
	    QE_VALID | QE_CHAIN;
	rp[RXDESCS].qe_flag = rp[RXDESCS].qe_status1 = QE_NOTYET;

	rp = sc->sc_qedata->qc_xmit;
	rp[TXDESCS].qe_addr_lo = LOWORD(&sc->sc_pqedata->qc_xmit[0]);
	rp[TXDESCS].qe_addr_hi = HIWORD(&sc->sc_pqedata->qc_xmit[0]) |
	    QE_VALID | QE_CHAIN;
	rp[TXDESCS].qe_flag = rp[TXDESCS].qe_status1 = QE_NOTYET;

	/*
	 * Get the vector that were set at match time, and remember it.
	 */
	sc->sc_intvec = ubasc->uh_lastiv;
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR, QE_RESET);
	DELAY(1000);
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR,
	    QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR) & ~QE_RESET);

	/*
	 * Read out ethernet address and tell which type this card is.
	 */
	for (i = 0; i < 6; i++)
		sc->sc_ac.ac_enaddr[i] =
		    QE_RCSR(sc->sc_iot, sc->sc_ioh, i * 2) & 0xff;

	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_VECTOR, sc->sc_intvec | 1);
	printf(": %s, address %s\n",
	    QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_VECTOR) & 1 ?
	      "delqa" : "deqna", ether_sprintf(sc->sc_ac.ac_enaddr));

	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_VECTOR,
	    QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_VECTOR) & ~1); /* ??? */

	uba_intr_establish(ua->ua_icookie, ua->ua_cvec, qeintr,
		sc, &sc->sc_intrcnt);
	sc->sc_cvec = ua->ua_cvec;
	evcount_attach(&sc->sc_intrcnt, sc->sc_dev.dv_xname, &sc->sc_cvec);

	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, sizeof ifp->if_xname);
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_start = qestart;
	ifp->if_ioctl = qeioctl;
	ifp->if_watchdog = qetimeout;
	IFQ_SET_READY(&ifp->if_snd);

	/*
	 * Attach the interface.
	 */
	if_attach(ifp);
	ether_ifattach(ifp);

	return;

	/*
	 * Free any resources we've allocated during the failed attach
	 * attempt.  Do this in reverse order and fall through.
	 */
 fail_6:
	for (i = 0; i < RXDESCS; i++) {
		if (sc->sc_rxmbuf[i] != NULL) {
			bus_dmamap_unload(sc->sc_dmat, sc->sc_xmtmap[i]);
			m_freem(sc->sc_rxmbuf[i]);
		}
	}
 fail_5:
	for (i = 0; i < RXDESCS; i++) {
		if (sc->sc_rcvmap[i] != NULL)
			bus_dmamap_destroy(sc->sc_dmat, sc->sc_rcvmap[i]);
	}
 fail_4:
	for (i = 0; i < TXDESCS; i++) {
		if (sc->sc_xmtmap[i] != NULL)
			bus_dmamap_destroy(sc->sc_dmat, sc->sc_xmtmap[i]);
	}
}

/*
 * Initialization of interface.
 */
void
qeinit(struct qe_softc *sc)
{
	struct ifnet *ifp = (struct ifnet *)&sc->sc_if;
	struct qe_cdata *qc = sc->sc_qedata;
	int i;


	/*
	 * Reset the interface.
	 */
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR, QE_RESET);
	DELAY(1000);
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR,
	    QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR) & ~QE_RESET);
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_VECTOR, sc->sc_intvec);

	sc->sc_nexttx = sc->sc_inq = sc->sc_lastack = 0;
	/*
	 * Release and init transmit descriptors.
	 */
	for (i = 0; i < TXDESCS; i++) {
		if (sc->sc_txmbuf[i]) {
			bus_dmamap_unload(sc->sc_dmat, sc->sc_xmtmap[i]);
			m_freem(sc->sc_txmbuf[i]);
			sc->sc_txmbuf[i] = 0;
		}
		qc->qc_xmit[i].qe_addr_hi = 0; /* Clear valid bit */
		qc->qc_xmit[i].qe_status1 = qc->qc_xmit[i].qe_flag = QE_NOTYET;
	}


	/*
	 * Init receive descriptors.
	 */
	for (i = 0; i < RXDESCS; i++)
		qc->qc_recv[i].qe_status1 = qc->qc_recv[i].qe_flag = QE_NOTYET;
	sc->sc_nextrx = 0;

	/*
	 * Write the descriptor addresses to the device.
	 * Receiving packets will be enabled in the interrupt routine.
	 */
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR,
	    QE_INT_ENABLE|QE_XMIT_INT|QE_RCV_INT);
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_RCLL,
	    LOWORD(sc->sc_pqedata->qc_recv));
	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_RCLH,
	    HIWORD(sc->sc_pqedata->qc_recv));

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	/*
	 * Send a setup frame.
	 * This will start the transmit machinery as well.
	 */
	qe_setup(sc);

}

/*
 * Start output on interface.
 */
void
qestart(struct ifnet *ifp)
{
	struct qe_softc *sc = ifp->if_softc;
	struct qe_cdata *qc = sc->sc_qedata;
	paddr_t	buffer;
	struct mbuf *m, *m0;
	int idx, len, s, i, totlen, error;
	short orword, csr;

	if ((QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR) & QE_RCV_ENABLE) == 0)
		return;

	s = splnet();
	while (sc->sc_inq < (TXDESCS - 1)) {

		if (sc->sc_setup) {
			qe_setup(sc);
			continue;
		}
		idx = sc->sc_nexttx;
		m = ifq_deq_begin(&ifp->if_snd);
		if (m == NULL)
			goto out;
		/*
		 * Count number of mbufs in chain.
		 * Always do DMA directly from mbufs, therefore the transmit
		 * ring is really big.
		 */
		for (m0 = m, i = 0; m0; m0 = m0->m_next)
			if (m0->m_len)
				i++;
		if (i >= TXDESCS)
			panic("qestart");

		if ((i + sc->sc_inq) >= (TXDESCS - 1)) {
			ifq_deq_rollback(&ifp->if_snd, m);
			ifq_set_oactive(&ifp->if_snd);
			goto out;
		}

		ifq_deq_commit(&ifp->if_snd, m);

#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
		ifp->if_opackets++;

		/*
		 * m now points to a mbuf chain that can be loaded.
		 * Loop around and set it.
		 */
		totlen = 0;
		for (m0 = m; m0; m0 = m0->m_next) {
			error = bus_dmamap_load(sc->sc_dmat, sc->sc_xmtmap[idx],
			    mtod(m0, void *), m0->m_len, 0, 0);
			buffer = sc->sc_xmtmap[idx]->dm_segs[0].ds_addr;
			len = m0->m_len;
			if (len == 0)
				continue;

			totlen += len;
			/* Word alignment calc */
			orword = 0;
			if (totlen == m->m_pkthdr.len) {
				if (totlen < ETHER_MIN_LEN)
					len += (ETHER_MIN_LEN - totlen);
				orword |= QE_EOMSG;
				sc->sc_txmbuf[idx] = m;
			}
			if ((buffer & 1) || (len & 1))
				len += 2;
			if (buffer & 1)
				orword |= QE_ODDBEGIN;
			if ((buffer + len) & 1)
				orword |= QE_ODDEND;
			qc->qc_xmit[idx].qe_buf_len = -(len/2);
			qc->qc_xmit[idx].qe_addr_lo = LOWORD(buffer);
			qc->qc_xmit[idx].qe_addr_hi = HIWORD(buffer);
			qc->qc_xmit[idx].qe_flag =
			    qc->qc_xmit[idx].qe_status1 = QE_NOTYET;
			qc->qc_xmit[idx].qe_addr_hi |= (QE_VALID | orword);
			if (++idx == TXDESCS)
				idx = 0;
			sc->sc_inq++;
		}
#ifdef DIAGNOSTIC
		if (totlen != m->m_pkthdr.len)
			panic("qestart: len fault");
#endif

		/*
		 * Kick off the transmit logic, if it is stopped.
		 */
		csr = QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR);
		if (csr & QE_XL_INVALID) {
			QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_XMTL,
			    LOWORD(&sc->sc_pqedata->qc_xmit[sc->sc_nexttx]));
			QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_XMTH,
			    HIWORD(&sc->sc_pqedata->qc_xmit[sc->sc_nexttx]));
		}
		sc->sc_nexttx = idx;
	}
	if (sc->sc_inq == (TXDESCS - 1))
		ifq_set_oactive(&ifp->if_snd);

out:	if (sc->sc_inq)
		ifp->if_timer = 5; /* If transmit logic dies */
	splx(s);
}

static void
qeintr(void *arg)
{
	struct qe_softc *sc = arg;
	struct qe_cdata *qc = sc->sc_qedata;
	struct ifnet *ifp = &sc->sc_if;
	struct ether_header *eh;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *m;
	int csr, status1, status2, len;

	csr = QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR);

	QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR,
	    QE_RCV_ENABLE | QE_INT_ENABLE | QE_XMIT_INT | QE_RCV_INT | QE_ILOOP);

	if (csr & QE_RCV_INT) {
		while (qc->qc_recv[sc->sc_nextrx].qe_status1 != QE_NOTYET) {
			status1 = qc->qc_recv[sc->sc_nextrx].qe_status1;
			status2 = qc->qc_recv[sc->sc_nextrx].qe_status2;

			m = sc->sc_rxmbuf[sc->sc_nextrx];
			len = ((status1 & QE_RBL_HI) |
			    (status2 & QE_RBL_LO)) + 60;
			qe_add_rxbuf(sc, sc->sc_nextrx);
			m->m_pkthdr.len = m->m_len = len;
			if (++sc->sc_nextrx == RXDESCS)
				sc->sc_nextrx = 0;
			eh = mtod(m, struct ether_header *);
			/*
			 * ALLMULTI means PROMISC in this driver.
			 */
			if ((ifp->if_flags & IFF_ALLMULTI) &&
			    ((eh->ether_dhost[0] & 1) == 0) &&
			    bcmp(sc->sc_ac.ac_enaddr, eh->ether_dhost,
			    ETHER_ADDR_LEN)) {
				m_freem(m);
				continue;
			}

			if ((status1 & QE_ESETUP) == 0)
				ml_enqueue(&ml, m);
			else
				m_freem(m);
		}
		if_input(ifp, &ml);
	}

	if (csr & (QE_XMIT_INT|QE_XL_INVALID)) {
		while (qc->qc_xmit[sc->sc_lastack].qe_status1 != QE_NOTYET) {
			int idx = sc->sc_lastack;

			sc->sc_inq--;
			if (++sc->sc_lastack == TXDESCS)
				sc->sc_lastack = 0;

			/* XXX collect statistics */
			qc->qc_xmit[idx].qe_addr_hi &= ~QE_VALID;
			qc->qc_xmit[idx].qe_status1 =
			    qc->qc_xmit[idx].qe_flag = QE_NOTYET;

			if (qc->qc_xmit[idx].qe_addr_hi & QE_SETUP)
				continue;
			bus_dmamap_unload(sc->sc_dmat, sc->sc_xmtmap[idx]);
			if (sc->sc_txmbuf[idx]) {
				m_freem(sc->sc_txmbuf[idx]);
				sc->sc_txmbuf[idx] = 0;
			}
		}
		ifp->if_timer = 0;
		ifq_clr_oactive(&ifp->if_snd);
		qestart(ifp); /* Put in more in queue */
	}
	/*
	 * How can the receive list get invalid???
	 * Verified that it happens anyway.
	 */
	if ((qc->qc_recv[sc->sc_nextrx].qe_status1 == QE_NOTYET) &&
	    (QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR) & QE_RL_INVALID)) {
		QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_RCLL,
		    LOWORD(&sc->sc_pqedata->qc_recv[sc->sc_nextrx]));
		QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_RCLH,
		    HIWORD(&sc->sc_pqedata->qc_recv[sc->sc_nextrx]));
	}
}

/*
 * Process an ioctl request.
 */
int
qeioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct qe_softc *sc = ifp->if_softc;
	int s, error = 0;

	s = splnet();

	switch (cmd) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		qeinit(sc);
		break;

	case SIOCSIFFLAGS:
		if ((ifp->if_flags & IFF_UP) == 0 &&
		    (ifp->if_flags & IFF_RUNNING) != 0) {
			/*
			 * If interface is marked down and it is running,
			 * stop it. (by disabling receive mechanism).
			 */
			QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR,
			    QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR) &
			      ~QE_RCV_ENABLE);
			ifp->if_flags &= ~IFF_RUNNING;
		} else if ((ifp->if_flags & IFF_UP) != 0 &&
			   (ifp->if_flags & IFF_RUNNING) == 0) {
			/*
			 * If interface it marked up and it is stopped, then
			 * start it.
			 */
			qeinit(sc);
		} else if ((ifp->if_flags & IFF_UP) != 0) {
			/*
			 * Send a new setup packet to match any new changes.
			 * (Like IFF_PROMISC etc)
			 */
			qe_setup(sc);
		}
		break;

	default:
		error = ether_ioctl(ifp, &sc->sc_ac, cmd, data);
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			qe_setup(sc);
		error = 0;
	}

	splx(s);
	return (error);
}

/*
 * Add a receive buffer to the indicated descriptor.
 */
int
qe_add_rxbuf(struct qe_softc *sc, int i) 
{
	struct mbuf *m;
	struct qe_ring *rp;
	vaddr_t addr;
	int error;

	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL)
		return (ENOBUFS);

	MCLGET(m, M_DONTWAIT);
	if ((m->m_flags & M_EXT) == 0) {
		m_freem(m);
		return (ENOBUFS);
	}

	if (sc->sc_rxmbuf[i] != NULL)
		bus_dmamap_unload(sc->sc_dmat, sc->sc_rcvmap[i]);

	error = bus_dmamap_load(sc->sc_dmat, sc->sc_rcvmap[i],
	    m->m_ext.ext_buf, m->m_ext.ext_size, NULL, BUS_DMA_NOWAIT);
	if (error)
		panic("%s: can't load rx DMA map %d, error = %d",
		    sc->sc_dev.dv_xname, i, error);
	sc->sc_rxmbuf[i] = m;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_rcvmap[i], 0,
	    sc->sc_rcvmap[i]->dm_mapsize, BUS_DMASYNC_PREREAD);

	/*
	 * We know that the mbuf cluster is page aligned. Also, be sure
	 * that the IP header will be longword aligned.
	 */
	m->m_data += 2;
	addr = sc->sc_rcvmap[i]->dm_segs[0].ds_addr + 2;
	rp = &sc->sc_qedata->qc_recv[i];
	rp->qe_flag = rp->qe_status1 = QE_NOTYET;
	rp->qe_addr_lo = LOWORD(addr);
	rp->qe_addr_hi = HIWORD(addr) | QE_VALID;
	rp->qe_buf_len = -(m->m_ext.ext_size - 2)/2;

	return (0);
}

/*
 * Create a setup packet and put in queue for sending.
 */
void
qe_setup(struct qe_softc *sc)
{
	struct ether_multi *enm;
	struct ether_multistep step;
	struct qe_cdata *qc = sc->sc_qedata;
	struct ifnet *ifp = &sc->sc_if;
	struct arpcom *ac = &sc->sc_ac;
	u_int8_t *enaddr = ac->ac_enaddr;
	int i, j, k, idx, s;

	s = splnet();
	if (sc->sc_inq == (TXDESCS - 1)) {
		sc->sc_setup = 1;
		splx(s);
		return;
	}
	sc->sc_setup = 0;
	/*
	 * Init the setup packet with valid info.
	 */
	memset(qc->qc_setup, 0xff, sizeof(qc->qc_setup)); /* Broadcast */
	for (i = 0; i < ETHER_ADDR_LEN; i++)
		qc->qc_setup[i * 8 + 1] = enaddr[i]; /* Own address */

	/*
	 * Multicast handling. The DEQNA can handle up to 12 direct 
	 * ethernet addresses.
	 */
	j = 3; k = 0;
	ifp->if_flags &= ~IFF_ALLMULTI;

	if (ac->ac_multirangecnt > 0) {
		ifp->if_flags |= IFF_ALLMULTI;
		goto setit;
	}

	ETHER_FIRST_MULTI(step, ac, enm);
	while (enm != NULL) {
		for (i = 0; i < ETHER_ADDR_LEN; i++)
			qc->qc_setup[i * 8 + j + k] = enm->enm_addrlo[i];
		j++;
		if (j == 8) {
			j = 1; k += 64;
		}
		if (k > 64) {
			ifp->if_flags |= IFF_ALLMULTI;
			break;
		}
		ETHER_NEXT_MULTI(step, enm);
	}

setit:
	idx = sc->sc_nexttx;
	qc->qc_xmit[idx].qe_buf_len = -64;

	/*
	 * How is the DEQNA turned in ALLMULTI mode???
	 * Until someone tells me, fall back to PROMISC when more than
	 * 12 ethernet addresses.
	 */
	if (ifp->if_flags & IFF_ALLMULTI)
		ifp->if_flags |= IFF_PROMISC;
	else if (ifp->if_pcount == 0)
		ifp->if_flags &= ~IFF_PROMISC;
	if (ifp->if_flags & IFF_PROMISC)
		qc->qc_xmit[idx].qe_buf_len = -65;

	qc->qc_xmit[idx].qe_addr_lo = LOWORD(sc->sc_pqedata->qc_setup);
	qc->qc_xmit[idx].qe_addr_hi =
	    HIWORD(sc->sc_pqedata->qc_setup) | QE_SETUP | QE_EOMSG;
	qc->qc_xmit[idx].qe_status1 = qc->qc_xmit[idx].qe_flag = QE_NOTYET;
	qc->qc_xmit[idx].qe_addr_hi |= QE_VALID;

	if (QE_RCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_CSR) & QE_XL_INVALID) {
		QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_XMTL,
		    LOWORD(&sc->sc_pqedata->qc_xmit[idx]));
		QE_WCSR(sc->sc_iot, sc->sc_ioh, QE_CSR_XMTH,
		    HIWORD(&sc->sc_pqedata->qc_xmit[idx]));
	}

	sc->sc_inq++;
	if (++sc->sc_nexttx == TXDESCS)
		sc->sc_nexttx = 0;
	splx(s);
}

/*
 * Check for dead transmit logic. Not uncommon.
 */
void
qetimeout(struct ifnet *ifp)
{
	struct qe_softc *sc = ifp->if_softc;

	if (sc->sc_inq == 0)
		return;

	printf("%s: xmit logic died, resetting...\n", sc->sc_dev.dv_xname);
	/*
	 * Do a reset of interface, to get it going again.
	 * Will it work by just restart the transmit logic?
	 */
	qeinit(sc);
}
@


1.41
log
@fix some fallout from the IFQ rototilling.

from the ghost of architectures past
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.40 2015/11/25 03:09:58 dlg Exp $	*/
@


1.40
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.39 2015/11/24 17:11:38 mpi Exp $	*/
d444 1
a444 1
		m = ifq_deq_begin(&ifp->if_snd, m);
@


1.39
log
@You only need <net/if_dl.h> if you're using LLADDR() or a sockaddr_dl.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.38 2015/11/20 03:35:22 dlg Exp $	*/
d410 1
a410 1
	ifp->if_flags &= ~IFF_OACTIVE;
d460 1
a460 1
			ifp->if_flags |= IFF_OACTIVE;
d528 1
a528 1
		ifp->if_flags |= IFF_OACTIVE;
d605 1
a605 1
		ifp->if_flags &= ~IFF_OACTIVE;
@


1.38
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.37 2015/11/14 17:26:40 mpi Exp $	*/
a50 1
#include <net/if_dl.h>
@


1.37
log
@No need to include <net/bpfdesc.h>

Now that "struct bpf_d" depends on <sys/srp.h> this is one of the offender
for removing the header from <sys/param.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.36 2015/10/27 15:20:13 mpi Exp $	*/
d445 1
a445 1
		IFQ_POLL(&ifp->if_snd, m);
d460 1
d465 1
a465 1
		IFQ_DEQUEUE(&ifp->if_snd, m);
@


1.36
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.35 2015/07/04 10:12:52 dlg Exp $	*/
a57 1
#include <net/bpfdesc.h>
@


1.35
log
@count outgoing packets like every other driver.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.34 2015/06/27 15:40:53 miod Exp $	*/
a628 1
	struct ifaddr *ifa = (struct ifaddr *)data;
d636 1
a636 6
		switch(ifa->ifa_addr->sa_family) {
		case AF_INET:
			qeinit(sc);
			arp_ifinit(&sc->sc_ac, ifa);
			break;
		}
@


1.34
log
@Pass bus_space tag and handles to the QE_{WR,RD}CR macros, instead of assuming
there's an `sc' local variable. This allows us to no longer have to fake a
softc at match time.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.32 2015/05/13 10:42:46 jsg Exp $	*/
d471 2
@


1.33
log
@Memory leak in attach if uballoc() fails; Brainy/Maxime Villard
@
text
@d120 4
a123 4
#define	QE_WCSR(csr, val) \
	bus_space_write_2(sc->sc_iot, sc->sc_ioh, csr, val)
#define	QE_RCSR(csr) \
	bus_space_read_2(sc->sc_iot, sc->sc_ioh, csr)
a134 2
	struct	qe_softc ssc;
	struct	qe_softc *sc = &ssc;
a137 1

a143 4
	bzero(sc, sizeof(struct qe_softc));
	sc->sc_iot = ua->ua_iot;
	sc->sc_ioh = ua->ua_ioh;
	sc->sc_dmat = ua->ua_dmat;
d146 2
a147 2
	QE_WCSR(QE_CSR_CSR, QE_RESET);
	QE_WCSR(QE_CSR_VECTOR, ubasc->uh_lastiv);
d176 2
a177 1
	QE_WCSR(QE_CSR_CSR, QE_RCSR(QE_CSR_CSR) & ~QE_RESET);
d183 6
a188 5
	QE_WCSR(QE_CSR_CSR, QE_INT_ENABLE|QE_XMIT_INT|QE_RCV_INT);
	QE_WCSR(QE_CSR_RCLL, LOWORD(&rp[2]));
	QE_WCSR(QE_CSR_RCLH, HIWORD(&rp[2]));
	QE_WCSR(QE_CSR_XMTL, LOWORD(rp));
	QE_WCSR(QE_CSR_XMTH, HIWORD(rp));
d294 1
a294 1
	QE_WCSR(QE_CSR_CSR, QE_RESET);
d296 2
a297 1
	QE_WCSR(QE_CSR_CSR, QE_RCSR(QE_CSR_CSR) & ~QE_RESET);
d303 2
a304 1
		sc->sc_ac.ac_enaddr[i] = QE_RCSR(i * 2) & 0xff;
d306 1
a306 1
	QE_WCSR(QE_CSR_VECTOR, sc->sc_intvec | 1);
d308 2
a309 2
		QE_RCSR(QE_CSR_VECTOR) & 1 ? "delqa" : "deqna",
		ether_sprintf(sc->sc_ac.ac_enaddr));
d311 2
a312 1
	QE_WCSR(QE_CSR_VECTOR, QE_RCSR(QE_CSR_VECTOR) & ~1); /* ??? */
d372 1
a372 1
	QE_WCSR(QE_CSR_CSR, QE_RESET);
d374 3
a376 2
	QE_WCSR(QE_CSR_CSR, QE_RCSR(QE_CSR_CSR) & ~QE_RESET);
	QE_WCSR(QE_CSR_VECTOR, sc->sc_intvec);
d404 6
a409 3
	QE_WCSR(QE_CSR_CSR, QE_INT_ENABLE|QE_XMIT_INT|QE_RCV_INT);
	QE_WCSR(QE_CSR_RCLL, LOWORD(sc->sc_pqedata->qc_recv));
	QE_WCSR(QE_CSR_RCLH, HIWORD(sc->sc_pqedata->qc_recv));
d435 1
a435 1
	if ((QE_RCSR(QE_CSR_CSR) & QE_RCV_ENABLE) == 0)
d517 1
a517 1
		csr = QE_RCSR(QE_CSR_CSR);
d519 1
a519 1
			QE_WCSR(QE_CSR_XMTL,
d521 1
a521 1
			QE_WCSR(QE_CSR_XMTH,
d545 1
a545 1
	csr = QE_RCSR(QE_CSR_CSR);
d547 2
a548 2
	QE_WCSR(QE_CSR_CSR, QE_RCV_ENABLE | QE_INT_ENABLE | QE_XMIT_INT |
	    QE_RCV_INT | QE_ILOOP);
d612 2
a613 2
	    (QE_RCSR(QE_CSR_CSR) & QE_RL_INVALID)) {
		QE_WCSR(QE_CSR_RCLL,
d615 1
a615 1
		QE_WCSR(QE_CSR_RCLH,
d650 3
a652 2
			QE_WCSR(QE_CSR_CSR,
			    QE_RCSR(QE_CSR_CSR) & ~QE_RCV_ENABLE);
d810 2
a811 2
	if (QE_RCSR(QE_CSR_CSR) & QE_XL_INVALID) {
		QE_WCSR(QE_CSR_XMTL,
d813 1
a813 1
		QE_WCSR(QE_CSR_XMTH,
@


1.32
log
@test mbuf pointers against NULL not 0
ok krw@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.31 2015/04/07 14:02:51 mpi Exp $	*/
d163 2
a164 1
	if ((error = uballoc((void *)parent, &ui, UBA_CANTWAIT)))
d166 1
@


1.31
log
@Convert to if_input().

Tested with simh using a floppy image built by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.30 2014/12/23 21:39:12 miod Exp $	*/
d443 1
a443 1
		if (m == 0)
@


1.30
log
@Pass real sizes to free()
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.29 2014/12/22 02:26:54 tedu Exp $	*/
d537 1
d546 1
a546 1
	if (csr & QE_RCV_INT)
a554 1
			m->m_pkthdr.rcvif = ifp;
a558 12
#if NBPFILTER > 0
			if (ifp->if_bpf) {
				bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
				if ((ifp->if_flags & IFF_PROMISC) != 0 &&
				    bcmp(sc->sc_ac.ac_enaddr, eh->ether_dhost,
				    ETHER_ADDR_LEN) != 0 &&
				    ((eh->ether_dhost[0] & 1) == 0)) {
					m_freem(m);
					continue;
				}
			}
#endif
d571 1
a571 1
				ether_input_mbuf(ifp, m);
d575 2
@


1.29
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.28 2014/08/06 15:40:40 jsg Exp $	*/
d198 1
a198 1
	free(ring, M_TEMP, 0);
@


1.28
log
@Correct some dma cleanup error paths.

While the index variables were correct the arrays of
dma handles they indexed were swapped for rx and tx.

As there are a mismatched number of rx and tx descriptors
we'd walk off the end of the rx handle array by 30 items.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.27 2014/07/12 18:44:43 tedu Exp $	*/
a641 1
#ifdef INET
a645 1
#endif
@


1.27
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.26 2013/11/27 08:56:31 mpi Exp $	*/
d348 2
a349 2
		if (sc->sc_xmtmap[i] != NULL)
			bus_dmamap_destroy(sc->sc_dmat, sc->sc_xmtmap[i]);
d353 2
a354 2
		if (sc->sc_rcvmap[i] != NULL)
			bus_dmamap_destroy(sc->sc_dmat, sc->sc_rcvmap[i]);
@


1.26
log
@Instead of comparing the lower and higher addresses of all the multicast
entries to decide if the IFF_ALLMULTI flag should be set, check if there
is at least one real range between them.

This should not change the behavior of any driver but if you encounter
any problem, feel free to revert the offending chunk and ping me about
it.

ok naddy@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.25 2010/09/20 06:33:47 matthew Exp $	*/
d198 1
a198 1
	free(ring, M_TEMP);
@


1.25
log
@Get rid of evcount's support for arranging counters in a tree
hierarchy.  Everything attached to a single root node anyway, so at
best we had a bush.

"i think it is good" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.24 2008/11/28 02:44:17 brad Exp $	*/
d750 2
a751 1
	u_int8_t *enaddr = sc->sc_ac.ac_enaddr;
d774 7
a780 1
	ETHER_FIRST_MULTI(step, &sc->sc_ac, enm);
a781 4
		if (memcmp(enm->enm_addrlo, enm->enm_addrhi, 6)) {
			ifp->if_flags |= IFF_ALLMULTI;
			break;
		}
d794 2
@


1.24
log
@Eliminate the redundant bits of code for MTU and multicast handling
from the individual drivers now that ether_ioctl() handles this.

Shrinks the i386 kernels by..
RAMDISK - 2176 bytes
RAMDISKB - 1504 bytes
RAMDISKC - 736 bytes

Tested by naddy@@/okan@@/sthen@@/brad@@/todd@@/jmc@@ and lots of users.
Build tested on almost all archs by todd@@/brad@@

ok naddy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.23 2008/10/08 23:53:08 brad Exp $	*/
d317 1
a317 2
	evcount_attach(&sc->sc_intrcnt, sc->sc_dev.dv_xname,
	    (void *)&sc->sc_cvec, &evcount_intr);
@


1.23
log
@cosmetic change for ioctl funtions.. move splnet out from variable declaration.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.22 2008/10/02 20:21:13 brad Exp $	*/
a633 1
	struct ifreq *ifr = (struct ifreq *)data;
d678 3
a680 8
	case SIOCADDMULTI:
	case SIOCDELMULTI:
		/*
		 * Update our multicast list.
		 */
		error = (cmd == SIOCADDMULTI) ?
			ether_addmulti(ifr, &sc->sc_ac):
			ether_delmulti(ifr, &sc->sc_ac);
d682 2
a683 5
		if (error == ENETRESET) {
			/*
			 * Multicast list has changed; set the hardware filter
			 * accordingly.
			 */
d685 1
a685 6
			error = 0;
		}
		break;

	default:
		error = ether_ioctl(ifp, &sc->sc_ac, cmd, data);
@


1.22
log
@First step towards cleaning up the Ethernet driver ioctl handling.
Move calling ether_ioctl() from the top of the ioctl function, which
at the moment does absolutely nothing, to the default switch case.
Thus allowing drivers to define their own ioctl handlers and then
falling back on ether_ioctl(). The only functional change this results
in at the moment is having all Ethernet drivers returning the proper
errno of ENOTTY instead of EINVAL/ENXIO when encountering unknown
ioctl's.

Shrinks the i386 kernels by..
RAMDISK - 1024 bytes
RAMDISKB -  1120 bytes
RAMDISKC - 832 bytes

Tested by martin@@/jsing@@/todd@@/brad@@
Build tested on almost all archs by todd@@/brad@@

ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.21 2007/09/17 01:33:33 krw Exp $	*/
d636 3
a638 1
	int s = splnet(), error = 0;
@


1.21
log
@Only the most obvious bzero() -> M_ZERO changes. No cast changes, no
MALLOC/FREE, etc. Just adding M_ZERO to malloc() and deleting an
immediately adjacent bzero().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.20 2006/04/16 00:46:32 pascoe Exp $	*/
a638 1

d697 2
a698 1
		error = EINVAL;
a699 1
	}
@


1.20
log
@Convert the last remaining net-driver users of ether_input to ether_input_mbuf.

sgec ok martin@@
if_ie ok miod@@
if_de, if_hp not in GENERIC
ok brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.19 2006/03/25 22:41:42 djm Exp $	*/
d146 1
a146 1
	ring = malloc(PROBESIZE, M_TEMP, M_WAITOK);
a147 1
	bzero(ring, PROBESIZE);
@


1.19
log
@allow bpf(4) to ignore packets based on their direction (inbound or
outbound), using a new BIOCSDIRFILT ioctl;
guidance, feedback and ok canacar@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.18 2004/07/07 23:10:45 deraadt Exp $	*/
d584 3
a586 5
			if ((status1 & QE_ESETUP) == 0) {
				/* m_adj() the ethernet header out of the way and pass up */
				m_adj(m, sizeof(struct ether_header));
				ether_input(ifp, eh, m);
			} else
@


1.18
log
@new-style interrupt counters.  based on initial work by hugh.  ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.17 2003/05/11 19:41:12 deraadt Exp $	*/
d467 1
a467 1
			bpf_mtap(ifp->if_bpf, m);
d563 1
a563 1
				bpf_mtap(ifp->if_bpf, m);
@


1.17
log
@string cleaning; krw ok
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.16 2003/02/05 00:05:15 hugh Exp $	*/
d80 2
a81 1
	struct evcnt	sc_intrcnt;	/* Interrupt counting		*/
d309 2
a310 2
	printf("\n%s: %s, hardware address %s\n", sc->sc_dev.dv_xname,
		QE_RCSR(QE_CSR_VECTOR) & 1 ? "delqa":"deqna",
d317 3
a319 1
	evcnt_attach(&sc->sc_dev, "intr", &sc->sc_intrcnt);
@


1.16
log
@LLADDR macro doesn't work here, so copy mac into the softc directly.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.15 2003/02/04 02:03:51 hugh Exp $	*/
d318 1
a318 1
	strcpy(ifp->if_xname, sc->sc_dev.dv_xname);
@


1.15
log
@Bring qe closer to NetBSD and make it compile. As yet untested.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a214 1
	u_int8_t enaddr[ETHER_ADDR_LEN];
d305 1
a305 1
		enaddr[i] = QE_RCSR(i * 2) & 0xff;
d310 1
a310 1
		ether_sprintf(enaddr));
d562 1
a562 1
				    bcmp(LLADDR(ifp->if_sadl), eh->ether_dhost,
d575 1
a575 1
			    bcmp(LLADDR(ifp->if_sadl), eh->ether_dhost,
d764 1
a764 1
	u_int8_t *enaddr = LLADDR(ifp->if_sadl);
@


1.14
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 2
a2 2
/*	$OpenBSD: if_qe.c,v 1.13 2002/03/14 01:26:48 millert Exp $ */
/*      $NetBSD: if_qe.c,v 1.39 2000/01/24 02:40:29 matt Exp $ */
a35 1
 *	Have a timeout check for hang transmit logic.
d52 1
d64 1
a64 1
#include <arch/vax/qbus/if_qereg.h>
a67 1
#define ETHER_MINLEN 64	/* min frame + crc */
d80 1
a87 1
	bus_dmamap_t	sc_cmap;	/* Map for control structures	*/
d92 1
d115 4
d132 1
a132 4
qematch(parent, cf, aux)
	struct	device *parent;
	struct	cfdata *cf;
	void	*aux;
a133 1
	bus_dmamap_t	cmap;
d138 1
d140 2
a141 2
#define	PROBESIZE	(sizeof(struct qe_ring) * 4 + 128)
	struct	qe_ring ring[15]; /* For diag purposes only */
d145 1
d161 3
a163 9
	if ((error = bus_dmamap_create(sc->sc_dmat, PROBESIZE, 1, PROBESIZE, 0,
	    BUS_DMA_NOWAIT, &cmap))) {
		printf("qematch: bus_dmamap_create failed = %d\n", error);
		return 0;
	}
	if ((error = bus_dmamap_load(sc->sc_dmat, cmap, ring, PROBESIZE, 0,
	    BUS_DMA_NOWAIT))) {
		printf("qematch: bus_dmamap_load failed = %d\n", error);
		bus_dmamap_destroy(sc->sc_dmat, cmap);
a164 1
	}
d170 1
a170 1
	rp = (void *)cmap->dm_segs[0].ds_addr;
d174 1
a174 1
	ring[0].qe_buf_len = 128;
d179 1
a179 1
	ring[2].qe_buf_len = 128;
d197 2
a198 2
	bus_dmamap_unload(sc->sc_dmat, cmap);
	bus_dmamap_destroy(sc->sc_dmat, cmap);
d208 1
a208 3
qeattach(parent, self, aux)
	struct	device *parent, *self;
	void	*aux;
d216 1
a216 2
	bus_dma_segment_t seg;
	int i, rseg, error;
d225 5
a229 30
	if ((error = bus_dmamem_alloc(sc->sc_dmat,
	    sizeof(struct qe_cdata), NBPG, 0, &seg, 1, &rseg,
	    BUS_DMA_NOWAIT)) != 0) {
		printf(": unable to allocate control data, error = %d\n",
		    error);
		goto fail_0;
	}

	if ((error = bus_dmamem_map(sc->sc_dmat, &seg, rseg,
	    sizeof(struct qe_cdata), (caddr_t *)&sc->sc_qedata,
	    BUS_DMA_NOWAIT|BUS_DMA_COHERENT)) != 0) {
		printf(": unable to map control data, error = %d\n", error);
		goto fail_1;
	}

	if ((error = bus_dmamap_create(sc->sc_dmat,
	    sizeof(struct qe_cdata), 1,
	    sizeof(struct qe_cdata), 0, BUS_DMA_NOWAIT,
	    &sc->sc_cmap)) != 0) {
		printf(": unable to create control data DMA map, error = %d\n",
		    error);
		goto fail_2;
	}

	if ((error = bus_dmamap_load(sc->sc_dmat, sc->sc_cmap,
	    sc->sc_qedata, sizeof(struct qe_cdata), NULL,
	    BUS_DMA_NOWAIT)) != 0) {
		printf(": unable to load control data DMA map, error = %d\n",
		    error);
		goto fail_3;
d231 2
a280 1
	sc->sc_pqedata = (struct qe_cdata *)sc->sc_cmap->dm_segs[0].ds_addr;
d315 3
a317 1
	uba_intr_establish(ua->ua_icookie, ua->ua_cvec, qeintr, sc);
d325 1
d332 1
a355 10
	bus_dmamap_unload(sc->sc_dmat, sc->sc_cmap);
 fail_3:
	bus_dmamap_destroy(sc->sc_dmat, sc->sc_cmap);
 fail_2:
	bus_dmamem_unmap(sc->sc_dmat, (caddr_t)sc->sc_qedata,
	    sizeof(struct qe_cdata));
 fail_1:
	bus_dmamem_free(sc->sc_dmat, &seg, rseg);
 fail_0:
	return;
d362 1
a362 2
qeinit(sc)
	struct qe_softc *sc;
d422 1
a422 2
qestart(ifp)
	struct ifnet *ifp;
d429 1
a429 1
	short orword;
d434 1
a434 1
	s = splimp();
d442 1
a442 1
		IF_DEQUEUE(&sc->sc_if.if_snd, m);
a456 1
			IF_PREPEND(&sc->sc_if.if_snd, m);
d460 3
a462 1
		
d484 2
a485 2
				if (totlen < ETHER_MINLEN)
					len += (ETHER_MINLEN - totlen);
d513 2
a514 1
		if (QE_RCSR(QE_CSR_CSR) & QE_XL_INVALID) {
d531 1
a531 2
qeintr(arg)
	void *arg;
d549 1
d563 1
a563 1
				    bcmp(sc->sc_ac.ac_enaddr, eh->ether_dhost,
d576 1
a576 1
			    bcmp(sc->sc_ac.ac_enaddr, eh->ether_dhost,
d581 7
a587 1
			ether_input(ifp, eh, m);
d590 1
a590 1
	if (csr & QE_XMIT_INT) {
d632 1
a632 4
qeioctl(ifp, cmd, data)
	register struct ifnet *ifp;
	u_long cmd;
	caddr_t data;
d710 1
a710 3
qe_add_rxbuf(sc, i) 
	struct qe_softc *sc;
	int i;
d759 1
a759 2
qe_setup(sc)
	struct qe_softc *sc;
d765 1
a765 1
	u_int8_t *enaddr = sc->sc_ac.ac_enaddr;
d768 1
a768 1
	s = splimp();
d790 1
a790 1
		if (bcmp(enm->enm_addrlo, enm->enm_addrhi, 6)) {
d814 5
a818 1
	if (ifp->if_flags & (IFF_PROMISC|IFF_ALLMULTI))
d844 1
a844 2
qetimeout(ifp)
	struct ifnet *ifp;
@


1.13
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.12 2001/02/20 19:39:35 mickey Exp $ */
d770 1
a770 1
		panic("%s: can't load rx DMA map %d, error = %d\n",
@


1.12
log
@for ethernet ifaces attach bpf from ether_ifattach; jason@@, aaron@@, itojun@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.11 2000/04/27 03:14:43 bjc Exp $ */
d101 9
a109 9
static	int	qematch __P((struct device *, struct cfdata *, void *));
static	void	qeattach __P((struct device *, struct device *, void *));
static	void	qeinit __P((struct qe_softc *));
static	void	qestart __P((struct ifnet *));
static	void	qeintr __P((void *));
static	int	qeioctl __P((struct ifnet *, u_long, caddr_t));
static	int	qe_add_rxbuf __P((struct qe_softc *, int));
static	void	qe_setup __P((struct qe_softc *));
static	void	qetimeout __P((struct ifnet *));
@


1.12.6.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.12 2001/02/20 19:39:35 mickey Exp $ */
d101 9
a109 9
static	int	qematch(struct device *, struct cfdata *, void *);
static	void	qeattach(struct device *, struct device *, void *);
static	void	qeinit(struct qe_softc *);
static	void	qestart(struct ifnet *);
static	void	qeintr(void *);
static	int	qeioctl(struct ifnet *, u_long, caddr_t);
static	int	qe_add_rxbuf(struct qe_softc *, int);
static	void	qe_setup(struct qe_softc *);
static	void	qetimeout(struct ifnet *);
@


1.12.6.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.12.6.1 2002/06/11 03:39:19 art Exp $ */
d770 1
a770 1
		panic("%s: can't load rx DMA map %d, error = %d",
@


1.12.6.3
log
@sync
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*      $NetBSD: if_qe.c,v 1.51 2002/06/08 12:28:37 ragge Exp $ */
d36 1
a52 1

d64 1
a64 1
#include <arch/vax/if/if_qereg.h>
d68 1
a80 1
	struct evcnt	sc_intrcnt;	/* Interrupt counting		*/
d88 1
a92 1
	struct ubinfo	sc_ui;
a114 4
struct cfdriver qe_cd = {
	NULL, "qe", DV_IFNET
};

d128 4
a131 1
qematch(struct device *parent, struct cfdata *cf, void *aux)
d133 1
a137 1
	struct ubinfo ui;
d139 2
a140 2
#define	PROBESIZE	4096
	struct qe_ring *ring;
a143 1
	ring = malloc(PROBESIZE, M_TEMP, M_WAITOK);
d159 9
a167 3
	ui.ui_size = PROBESIZE;
	ui.ui_vaddr = (caddr_t)&ring[0];
	if ((error = uballoc((void *)parent, &ui, UBA_CANTWAIT)))
d169 1
d175 1
a175 1
	rp = (void *)ui.ui_baddr;
d179 1
a179 1
	ring[0].qe_buf_len = -64;
d184 1
a184 1
	ring[2].qe_buf_len = -(1500/2);
d202 2
a203 2
	ubfree((void *)parent, &ui);
	free(ring, M_TEMP);
d213 3
a215 1
qeattach(struct device *parent, struct device *self, void *aux)
d222 3
a224 1
	int i, error;
d233 30
a262 5

	sc->sc_ui.ui_size = sizeof(struct qe_cdata);
	if ((error = ubmemalloc((struct uba_softc *)parent, &sc->sc_ui, 0))) {
		printf(": unable to ubmemalloc(), error = %d\n", error);
		return;
a263 2
	sc->sc_pqedata = (struct qe_cdata *)sc->sc_ui.ui_baddr;
	sc->sc_qedata = (struct qe_cdata *)sc->sc_ui.ui_vaddr;
d312 1
d338 1
a338 1
		sc->sc_ac.ac_enaddr[i] = QE_RCSR(i * 2) & 0xff;
d343 1
a343 1
		ether_sprintf(sc->sc_ac.ac_enaddr));
d347 1
a347 3
	uba_intr_establish(ua->ua_icookie, ua->ua_cvec, qeintr,
		sc, &sc->sc_intrcnt);
	evcnt_attach(&sc->sc_dev, "intr", &sc->sc_intrcnt);
d349 1
a349 1
	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, sizeof ifp->if_xname);
a354 1
	IFQ_SET_READY(&ifp->if_snd);
a360 1

d384 10
d400 2
a401 1
qeinit(struct qe_softc *sc)
d461 2
a462 1
qestart(struct ifnet *ifp)
d469 1
a469 1
	short orword, csr;
d474 1
a474 1
	s = splnet();
d482 1
a482 1
		IFQ_POLL(&ifp->if_snd, m);
d497 1
d501 1
a501 3

		IFQ_DEQUEUE(&ifp->if_snd, m);

d523 2
a524 2
				if (totlen < ETHER_MIN_LEN)
					len += (ETHER_MIN_LEN - totlen);
d552 1
a552 2
		csr = QE_RCSR(QE_CSR_CSR);
		if (csr & QE_XL_INVALID) {
d569 2
a570 1
qeintr(void *arg)
a587 1

d619 1
a619 7

			if ((status1 & QE_ESETUP) == 0) {
				/* m_adj() the ethernet header out of the way and pass up */
				m_adj(m, sizeof(struct ether_header));
				ether_input(ifp, eh, m);
			} else
				m_freem(m);
d622 1
a622 1
	if (csr & (QE_XMIT_INT|QE_XL_INVALID)) {
d664 4
a667 1
qeioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
d745 3
a747 1
qe_add_rxbuf(struct qe_softc *sc, int i) 
d796 2
a797 1
qe_setup(struct qe_softc *sc)
d806 1
a806 1
	s = splnet();
d828 1
a828 1
		if (memcmp(enm->enm_addrlo, enm->enm_addrhi, 6)) {
d852 1
a852 5
	if (ifp->if_flags & IFF_ALLMULTI)
		ifp->if_flags |= IFF_PROMISC;
	else if (ifp->if_pcount == 0)
		ifp->if_flags &= ~IFF_PROMISC;
	if (ifp->if_flags & IFF_PROMISC)
d878 2
a879 1
qetimeout(struct ifnet *ifp)
@


1.11
log
@sync w/netbsd
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.10 1999/05/13 15:44:50 jason Exp $ */
a360 4

#if NBPFILTER > 0
	bpfattach(&ifp->if_bpf, ifp, DLT_EN10MB, sizeof(struct ether_header));
#endif
@


1.10
log
@Compensate for the check for onwership of unicast packets in promiscuous
mode being moved to if_ether.c.  This is the last of the drivers hopefully.
@
text
@d1 2
a2 3
/*	$OpenBSD: if_qe.c,v 1.9 1997/09/10 08:28:41 maja Exp $ */
/*	$NetBSD: if_qe.c,v 1.22 1997/05/02 17:11:24 ragge Exp $ */

d4 1
a4 5
 * Copyright (c) 1988 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Digital Equipment Corp.
d16 15
a30 115
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)if_qe.c	7.20 (Berkeley) 3/28/91
 */

/* from	 @@(#)if_qe.c	1.15	(ULTRIX)	4/16/86 */

/****************************************************************
 *								*
 *	  Licensed from Digital Equipment Corporation		*
 *			 Copyright (c)				*
 *		 Digital Equipment Corporation			*
 *		     Maynard, Massachusetts			*
 *			   1985, 1986				*
 *		      All rights reserved.			*
 *								*
 *	  The Information in this software is subject to change *
 *   without notice and should not be construed as a commitment *
 *   by	 Digital  Equipment  Corporation.   Digital   makes  no *
 *   representations about the suitability of this software for *
 *   any purpose.  It is supplied "As Is" without expressed  or *
 *   implied  warranty.						*
 *								*
 *	  If the Regents of the University of California or its *
 *   licensees modify the software in a manner creating		*
 *   derivative copyright rights, appropriate copyright		*
 *   legends may be placed on the derivative work in addition	*
 *   to that set forth above.					*
 *								*
 ****************************************************************/
/* ---------------------------------------------------------------------
 * Modification History
 *
 * 15-Apr-86  -- afd
 *	Rename "unused_multi" to "qunused_multi" for extending Generic
 *	kernel to MicroVAXen.
 *
 * 18-mar-86  -- jaw	 br/cvec changed to NOT use registers.
 *
 * 12 March 86 -- Jeff Chase
 *	Modified to handle the new MCLGET macro
 *	Changed if_qe_data.c to use more receive buffers
 *	Added a flag to poke with adb to log qe_restarts on console
 *
 * 19 Oct 85 -- rjl
 *	Changed the watch dog timer from 30 seconds to 3.  VMS is using
 *	less than 1 second in their's. Also turned the printf into an
 *	mprintf.
 *
 *  09/16/85 -- Larry Cohen
 *		Add 43bsd alpha tape changes for subnet routing
 *
 *  1 Aug 85 -- rjl
 *	Panic on a non-existent memory interrupt and the case where a packet
 *	was chained.  The first should never happen because non-existant
 *	memory interrupts cause a bus reset. The second should never happen
 *	because we hang 2k input buffers on the device.
 *
 *  1 Aug 85 -- rich
 *	Fixed the broadcast loopback code to handle Clusters without
 *	wedging the system.
 *
 *  27 Feb. 85 -- ejf
 *	Return default hardware address on ioctl request.
 *
 *  12 Feb. 85 -- ejf
 *	Added internal extended loopback capability.
 *
 *  27 Dec. 84 -- rjl
 *	Fixed bug that caused every other transmit descriptor to be used
 *	instead of every descriptor.
 *
 *  21 Dec. 84 -- rjl
 *	Added watchdog timer to mask hardware bug that causes device lockup.
 *
 *  18 Dec. 84 -- rjl
 *	Reworked driver to use q-bus mapping routines.	MicroVAX-I now does
 *	copying instead of m-buf shuffleing.
 *	A number of deficencies in the hardware/firmware were compensated
 *	for. See comments in qestart and qerint.
 *
 *  14 Nov. 84 -- jf
 *	Added usage counts for multicast addresses.
 *	Updated general protocol support to allow access to the Ethernet
 *	header.
 *
 *  04 Oct. 84 -- jf
 *	Added support for new ioctls to add and delete multicast addresses
 *	and set the physical address.
 *	Add support for general protocols.
 *
 *  14 Aug. 84 -- rjl
 *	Integrated Shannon changes. (allow arp above 1024 and ? )
 *
 *  13 Feb. 84 -- rjl
 *
 *	Initial version of driver. derived from IL driver.
 *
 * ---------------------------------------------------------------------
d34 6
a39 2
 * Digital Q-BUS to NI Adapter
 * supports DEQNA and DELQA in DEQNA-mode.
a44 1
#include <sys/systm.h>
a45 2
#include <sys/buf.h>
#include <sys/protosw.h>
a46 3
#include <sys/ioctl.h>
#include <sys/errno.h>
#include <sys/syslog.h>
d48 2
a49 2
#include <sys/time.h>
#include <sys/kernel.h>
d52 1
a52 4
#include <net/netisr.h>
#include <net/route.h>

#ifdef INET
a53 3
#include <netinet/in_systm.h>
#include <netinet/in_var.h>
#include <netinet/ip.h>
a54 20
#endif

#ifdef NS
#include <netns/ns.h>
#include <netns/ns_if.h>
#endif

#ifdef ISO
#include <netiso/iso.h>
#include <netiso/iso_var.h>
extern char all_es_snpa[], all_is_snpa[], all_l1is_snpa[], all_l2is_snpa[];
#endif

#if defined(CCITT) && defined(LLC)
#include <sys/socketvar.h>
#include <netccitt/x25.h>
#include <netccitt/pk.h>
#include <netccitt/pk_var.h>
#include <netccitt/pk_extern.h>
#endif
a59 12
 
#include <machine/pte.h>
#include <machine/cpu.h>

#include <vax/if/if_qereg.h>
#include <vax/if/if_uba.h>
#include <vax/uba/ubareg.h>
#include <vax/uba/ubavar.h>

#define NRCV	15			/* Receive descriptors		*/
#define NXMT	5			/* Transmit descriptors		*/
#define NTOT	(NXMT + NRCV)
d61 4
a64 2
#define QETIMEOUT	2		/* transmit timeout, must be > 1 */
#define QESLOWTIMEOUT	40		/* timeout when no xmits in progress */
d66 3
a68 1
#define MINDATA 60
d71 1
a71 5
 * Ethernet software status per interface.
 *
 * Each interface is referenced by a network interface structure,
 * qe_if, which the routing code uses to locate the interface.
 * This structure contains the output queue for the interface, its address, ...
d73 6
d80 19
a98 28
	struct	device qe_dev;		/* Configuration common part	*/
	struct	arpcom qe_ac;		/* Ethernet common part 	*/
#define	qe_if	qe_ac.ac_if		/* network-visible interface 	*/
#define	qe_addr	qe_ac.ac_enaddr		/* hardware Ethernet address 	*/
	struct	ifubinfo qe_uba;	/* Q-bus resources 		*/
	struct	ifrw qe_ifr[NRCV];	/*	for receive buffers;	*/
	struct	ifxmt qe_ifw[NXMT];	/*	for xmit buffers;	*/
	struct	qedevice *qe_vaddr;
	int	qe_flags;		/* software state		*/
#define	QEF_RUNNING	0x01
#define	QEF_SETADDR	0x02
#define QEF_FASTTIMEO	0x04
	int	setupaddr;		/* mapping info for setup pkts  */
	int	ipl;			/* interrupt priority		*/
	struct	qe_ring *rringaddr;	/* mapping info for rings	*/
	struct	qe_ring *tringaddr;	/*       ""			*/
	struct	qe_ring rring[NRCV+1];	/* Receive ring descriptors	*/
	struct	qe_ring tring[NXMT+1];	/* Xmit ring descriptors	*/
	u_char	setup_pkt[16][8];	/* Setup packet			*/
	int	rindex;			/* Receive index		*/
	int	tindex;			/* Transmit index		*/
	int	otindex;		/* Old transmit index		*/
	int	qe_intvec;		/* Interrupt vector 		*/
	struct	qedevice *addr;		/* device addr			*/
	int 	setupqueued;		/* setup packet queued		*/
	int	setuplength;		/* length if setup packet	*/
	int	nxmit;			/* Transmits in progress	*/
	int	qe_restarts;		/* timeouts			*/
d101 9
a109 15
int	qematch __P((struct device *, void *, void *));
void	qeattach __P((struct device *, struct device *, void *));
void	qereset __P((int));
void	qeinit __P((struct qe_softc *));
void	qestart __P((struct ifnet *));
void	qeintr __P((int));
void	qetint __P((int));
void	qerint __P((int));
int	qeioctl __P((struct ifnet *, u_long, caddr_t));
void	qe_setaddr __P((u_char *, struct qe_softc *));
void	qeinitdesc __P((struct qe_ring *, caddr_t, int));
void	qesetup __P((struct qe_softc *));
void	qeread __P((struct qe_softc *, struct ifrw *, int));
void	qetimeout __P((struct ifnet *));
void	qerestart __P((struct qe_softc *));
d111 2
a112 2
struct	cfdriver qe_cd = {
	NULL, "qe", DV_IFNET
d115 4
a118 3
struct	cfattach qe_ca = {
	sizeof(struct qe_softc), qematch, qeattach
};
d120 2
a121 8
#define QEUNIT(x)	minor(x)
/*
 * The deqna shouldn't receive more than ETHERMTU + sizeof(struct ether_header)
 * but will actually take in up to 2048 bytes. To guard against the receiver
 * chaining buffers (which we aren't prepared to handle) we allocate 2kb
 * size buffers.
 */
#define MAXPACKETSIZE 2048		/* Should really be ETHERMTU	*/
d124 2
a125 1
 * Probe the QNA to see if it's there
d128 1
a128 1
qematch(parent, match, aux)
d130 2
a131 1
	void	*match, *aux;
d133 3
a135 1
	struct	qe_softc *sc = match;
d138 3
d142 1
a142 3
	struct	qe_ring *prp;	/* physical rp		*/
	volatile struct qedevice *addr = (struct qedevice *)ua->ua_addr;
	int i;
d144 5
a148 4
	/*
	 * The QNA interrupts on i/o operations. To do an I/O operation
	 * we have to setup the interface by transmitting a setup  packet.
	 */
d150 3
a152 3
	addr->qe_csr = QE_RESET;
	addr->qe_csr &= ~QE_RESET;
	addr->qe_vector = (ubasc->uh_lastiv -= 4);
d155 3
a157 1
	 * Map the communications area and the setup packet.
d159 11
a169 5
	sc->setupaddr =
	    uballoc(ubasc, (caddr_t)sc->setup_pkt, sizeof(sc->setup_pkt), 0);
	sc->rringaddr = (struct qe_ring *) uballoc(ubasc, (caddr_t)sc->rring,
	    sizeof(struct qe_ring) * (NTOT+2), 0);
	prp = (struct qe_ring *)UBAI_ADDR((int)sc->rringaddr);
d172 2
a173 4
	 * The QNA will loop the setup packet back to the receive ring
	 * for verification, therefore we initialize the first
	 * receive & transmit ring descriptors and link the setup packet
	 * to them.
d175 5
a179 10
	qeinitdesc(sc->tring, (caddr_t)UBAI_ADDR(sc->setupaddr),
	    sizeof(sc->setup_pkt));
	qeinitdesc(sc->rring, (caddr_t)UBAI_ADDR(sc->setupaddr),
	    sizeof(sc->setup_pkt));

	rp = (struct qe_ring *)sc->tring;
	rp->qe_setup = 1;
	rp->qe_eomsg = 1;
	rp->qe_flag = rp->qe_status1 = QE_NOTYET;
	rp->qe_valid = 1;
d181 4
a184 3
	rp = (struct qe_ring *)sc->rring;
	rp->qe_flag = rp->qe_status1 = QE_NOTYET;
	rp->qe_valid = 1;
d186 2
a187 7
	/*
	 * Get the addr off of the interface and place it into the setup
	 * packet. This code looks strange due to the fact that the address
	 * is placed in the setup packet in col. major order.
	 */
	for (i = 0; i < 6; i++)
		sc->setup_pkt[i][1] = addr->qe_sta_addr[i];
a188 1
	qesetup(sc);
d192 5
a196 6
	addr->qe_csr = QE_INT_ENABLE | QE_XMIT_INT | QE_RCV_INT;
	addr->qe_rcvlist_lo = (short)((int)prp);
	addr->qe_rcvlist_hi = (short)((int)prp >> 16);
	prp += NRCV+1;
	addr->qe_xmtlist_lo = (short)((int)prp);
	addr->qe_xmtlist_hi = (short)((int)prp >> 16);
d198 1
d202 2
a203 4
	ubarelse(ubasc, &sc->setupaddr);
	ubarelse(ubasc, (int *)&sc->rringaddr);
	sc->ipl = 0x15;
	ua->ua_ivec = qeintr;
d218 1
d220 87
a306 3
	struct	ifnet *ifp = (struct ifnet *)&sc->qe_if;
	struct qedevice *addr =(struct qedevice *)ua->ua_addr;
	int i;
a307 4
	printf("\n");
	sc->qe_vaddr = addr;
	bcopy(sc->qe_dev.dv_xname, ifp->if_xname, IFNAMSIZ);
	ifp->if_softc = sc;
d309 2
a310 2
	 * The Deqna is cable of transmitting broadcasts, but
	 * doesn't listen to its own.
d312 13
a324 2
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_NOTRAILERS |
	    IFF_MULTICAST;
d327 1
a327 1
	 * Read the address from the prom and save it.
d329 4
a332 8
	for (i = 0; i < 6; i++)
		sc->setup_pkt[i][1] = sc->qe_addr[i] =
		    addr->qe_sta_addr[i] & 0xff;
	addr->qe_vector |= 1;
	printf("qe%d: %s, hardware address %s\n", sc->qe_dev.dv_unit,
		addr->qe_vector&01 ? "delqa":"deqna",
		ether_sprintf(sc->qe_addr));
	addr->qe_vector &= ~1;
d335 1
a335 1
	 * Save the vector for initialization at reset time.
d337 2
a338 1
	sc->qe_intvec = addr->qe_vector;
d340 12
d355 4
a358 1
	sc->qe_uba.iff_flags = UBA_CANTWAIT;
d365 1
a365 1
}
d367 31
a397 12
/*
 * Reset of interface after UNIBUS reset.
 */
void
qereset(unit)
	int unit;
{
	struct	qe_softc *sc = qe_cd.cd_devs[unit];

	printf(" %s", sc->qe_dev.dv_xname);
	sc->qe_if.if_flags &= ~IFF_RUNNING;
	qeinit(sc);
d407 2
a408 3
	struct qedevice *addr = sc->qe_vaddr;
	struct uba_softc *ubasc = (void *)sc->qe_dev.dv_parent;
	struct ifnet *ifp = (struct ifnet *)&sc->qe_if;
a409 1
	int s;
a410 5
	/* address not known */
	if (ifp->if_addrlist.tqh_first == (struct ifaddr *)0)
			return;
	if (sc->qe_flags & QEF_RUNNING)
		return;
d412 17
a428 26
	if ((ifp->if_flags & IFF_RUNNING) == 0) {
		/*
		 * map the communications area onto the device
		 */
		i = uballoc(ubasc, (caddr_t)sc->rring,
		    sizeof(struct qe_ring) * (NTOT+2), 0);
		if (i == 0)
			goto fail;
		sc->rringaddr = (struct qe_ring *)UBAI_ADDR(i);
		sc->tringaddr = sc->rringaddr + NRCV + 1;
		i = uballoc(ubasc, (caddr_t)sc->setup_pkt,
		    sizeof(sc->setup_pkt), 0);
		if (i == 0)
			goto fail;
		sc->setupaddr = UBAI_ADDR(i);
		/*
		 * init buffers and maps
		 */
		if (if_ubaminit(&sc->qe_uba, (void *)sc->qe_dev.dv_parent,
		    sizeof (struct ether_header), (int)btoc(MAXPACKETSIZE),
		    sc->qe_ifr, NRCV, sc->qe_ifw, NXMT) == 0) {
	fail:
			printf("%s: can't allocate uba resources\n", 
			    sc->qe_dev.dv_xname);
			sc->qe_if.if_flags &= ~IFF_UP;
			return;
d430 2
d433 9
d443 2
a444 2
	 * Init the buffer descriptors and indexes for each of the lists and
	 * loop them back to form a ring.
d446 13
a458 43
	for (i = 0; i < NRCV; i++) {
		qeinitdesc( &sc->rring[i],
		    (caddr_t)UBAI_ADDR(sc->qe_ifr[i].ifrw_info), MAXPACKETSIZE);
		sc->rring[i].qe_flag = sc->rring[i].qe_status1 = QE_NOTYET;
		sc->rring[i].qe_valid = 1;
	}
	qeinitdesc(&sc->rring[i], (caddr_t)NULL, 0);

	sc->rring[i].qe_addr_lo = (short)((int)sc->rringaddr);
	sc->rring[i].qe_addr_hi = (short)((int)sc->rringaddr >> 16);
	sc->rring[i].qe_chain = 1;
	sc->rring[i].qe_flag = sc->rring[i].qe_status1 = QE_NOTYET;
	sc->rring[i].qe_valid = 1;

	for( i = 0 ; i <= NXMT ; i++ )
		qeinitdesc(&sc->tring[i], (caddr_t)NULL, 0);
	i--;

	sc->tring[i].qe_addr_lo = (short)((int)sc->tringaddr);
	sc->tring[i].qe_addr_hi = (short)((int)sc->tringaddr >> 16);
	sc->tring[i].qe_chain = 1;
	sc->tring[i].qe_flag = sc->tring[i].qe_status1 = QE_NOTYET;
	sc->tring[i].qe_valid = 1;

	sc->nxmit = sc->otindex = sc->tindex = sc->rindex = 0;

	/*
	 * Take the interface out of reset, program the vector,
	 * enable interrupts, and tell the world we are up.
	 */
	s = splnet();
	addr->qe_vector = sc->qe_intvec;
	sc->addr = addr;
	addr->qe_csr = QE_RCV_ENABLE | QE_INT_ENABLE | QE_XMIT_INT |
	    QE_RCV_INT | QE_ILOOP;
	addr->qe_rcvlist_lo = (short)((int)sc->rringaddr);
	addr->qe_rcvlist_hi = (short)((int)sc->rringaddr >> 16);
	ifp->if_flags |= IFF_UP | IFF_RUNNING;
	sc->qe_flags |= QEF_RUNNING;
	qesetup( sc );
	qestart( ifp );
	sc->qe_if.if_timer = QESLOWTIMEOUT;	/* Start watchdog */
	splx( s );
a462 1
 *
d468 9
a476 6
	register struct qe_softc *sc = ifp->if_softc;
	volatile struct qedevice *addr = sc->qe_vaddr;
	register struct qe_ring *rp;
	register index;
	struct mbuf *m;
	int buf_addr, len, s;
d478 2
d481 25
a505 27
	s = splnet();
	/*
	 * The deqna doesn't look at anything but the valid bit
	 * to determine if it should transmit this packet. If you have
	 * a ring and fill it the device will loop indefinately on the
	 * packet and continue to flood the net with packets until you
	 * break the ring. For this reason we never queue more than n-1
	 * packets in the transmit ring.
	 *
	 * The microcoders should have obeyed their own defination of the
	 * flag and status words, but instead we have to compensate.
	 */
	for( index = sc->tindex;
		sc->tring[index].qe_valid == 0 && sc->nxmit < (NXMT-1) ;
		sc->tindex = index = ++index % NXMT){
		rp = &sc->tring[index];
		if( sc->setupqueued ) {
			buf_addr = sc->setupaddr;
			len = sc->setuplength;
			rp->qe_setup = 1;
			sc->setupqueued = 0;
		} else {
			IF_DEQUEUE(&sc->qe_if.if_snd, m);
			if (m == 0) {
				splx(s);
				return;
			}
d507 2
a508 2
			if (ifp->if_bpf)
				bpf_mtap(ifp->if_bpf, m);
a509 5
			buf_addr = sc->qe_ifw[index].ifw_info;
			len = if_ubaput(&sc->qe_uba, &sc->qe_ifw[index], m);
		}
		if( len < MINDATA )
			len = MINDATA;
d511 2
a512 1
		 *  Does buffer end on odd byte ?
d514 38
a551 16
		if( len & 1 ) {
			len++;
			rp->qe_odd_end = 1;
		}
		rp->qe_buf_len = -(len/2);
		buf_addr = UBAI_ADDR(buf_addr);
		rp->qe_flag = rp->qe_status1 = QE_NOTYET;
		rp->qe_addr_lo = (short)buf_addr;
		rp->qe_addr_hi = (short)(buf_addr >> 16);
		rp->qe_eomsg = 1;
		rp->qe_flag = rp->qe_status1 = QE_NOTYET;
		rp->qe_valid = 1;
		if (sc->nxmit++ == 0) {
			sc->qe_flags |= QEF_FASTTIMEO;
			sc->qe_if.if_timer = QETIMEOUT;
		}
d554 7
a560 6
		 * See if the xmit list is invalid.
		 */
		if( addr->qe_csr & QE_XL_INVALID ) {
			buf_addr = (int)(sc->tringaddr+index);
			addr->qe_xmtlist_lo = (short)buf_addr;
			addr->qe_xmtlist_hi = (short)(buf_addr >> 16);
d562 1
d564 5
a569 1
	return;
d572 10
a581 33
/*
 * Ethernet interface interrupt processor
 */
void
qeintr(unit)
	int	unit;
{
	register struct qe_softc *sc;
	volatile struct qedevice *addr;
	int buf_addr, csr;

	sc = qe_cd.cd_devs[unit];
	addr = sc->qe_vaddr;
	splx(sc->ipl);
	if (!(sc->qe_flags & QEF_FASTTIMEO))
		sc->qe_if.if_timer = QESLOWTIMEOUT; /* Restart timer clock */
	csr = addr->qe_csr;
	addr->qe_csr = QE_RCV_ENABLE | QE_INT_ENABLE |
	    QE_XMIT_INT | QE_RCV_INT | QE_ILOOP;
	if (csr & QE_RCV_INT)
		qerint(unit);
	if (csr & QE_XMIT_INT)
		qetint(unit );
	if (csr & QE_NEX_MEM_INT)
		printf("qe%d: Nonexistent memory interrupt\n", unit);

	if (addr->qe_csr & QE_RL_INVALID && sc->rring[sc->rindex].qe_status1 ==
	    QE_NOTYET) {
		buf_addr = (int)&sc->rringaddr[sc->rindex];
		addr->qe_rcvlist_lo = (short)buf_addr;
		addr->qe_rcvlist_hi = (short)(buf_addr >> 16);
	}
}
d583 1
a583 12
/*
 * Ethernet interface transmit interrupt.
 */
void
qetint(unit)
	int unit;
{
	register struct qe_softc *sc = qe_cd.cd_devs[unit];
	register struct qe_ring *rp;
	register struct ifxmt *ifxp;
	int status1, setupflag;
	short len;
d585 2
d588 25
a612 21
	while (sc->otindex != sc->tindex && sc->tring[sc->otindex].qe_status1
	    != QE_NOTYET && sc->nxmit > 0) {
		/*
		 * Save the status words from the descriptor so that it can
		 * be released.
		 */
		rp = &sc->tring[sc->otindex];
		status1 = rp->qe_status1;
		setupflag = rp->qe_setup;
		len = (-rp->qe_buf_len) * 2;
		if( rp->qe_odd_end )
			len++;
		/*
		 * Init the buffer descriptor
		 */
		bzero((caddr_t)rp, sizeof(struct qe_ring));
		if( --sc->nxmit == 0 ) {
			sc->qe_flags &= ~QEF_FASTTIMEO;
			sc->qe_if.if_timer = QESLOWTIMEOUT;
		}
		if( !setupflag ) {
d614 1
a614 1
			 * Do some statistics.
d616 6
a621 8
			sc->qe_if.if_opackets++;
			sc->qe_if.if_collisions += ( status1 & QE_CCNT ) >> 4;
			if (status1 & QE_ERROR)
				sc->qe_if.if_oerrors++;
			ifxp = &sc->qe_ifw[sc->otindex];
			if (ifxp->ifw_xtofree) {
				m_freem(ifxp->ifw_xtofree);
				ifxp->ifw_xtofree = 0;
d623 1
a624 4
		sc->otindex = ++sc->otindex % NXMT;
	}
	qestart(&sc->qe_if);
}
d626 24
a649 44
/*
 * Ethernet interface receiver interrupt.
 * If can't determine length from type, then have to drop packet.
 * Othewise decapsulate packet based on type and pass to type specific
 * higher-level input routine.
 */
void
qerint(unit)
	int unit;
{
	register struct qe_softc *sc = qe_cd.cd_devs[unit];
	register struct qe_ring *rp;
	register int nrcv = 0;
	int len, status1, status2;
	int bufaddr;

	/*
	 * Traverse the receive ring looking for packets to pass back.
	 * The search is complete when we find a descriptor not in use.
	 *
	 * As in the transmit case the deqna doesn't honor it's own protocols
	 * so there exists the possibility that the device can beat us around
	 * the ring. The proper way to guard against this is to insure that
	 * there is always at least one invalid descriptor. We chose instead
	 * to make the ring large enough to minimize the problem. With a ring
	 * size of 4 we haven't been able to see the problem. To be safe we
	 * doubled that to 8.
	 *
	 */
	while (sc->rring[sc->rindex].qe_status1 == QE_NOTYET && nrcv < NRCV) {
		/*
		 * We got an interrupt but did not find an input packet
		 * where we expected one to be, probably because the ring
		 * was overrun.
		 * We search forward to find a valid packet and start
		 * processing from there.  If no valid packet is found it
		 * means we processed all the packets during a previous
		 * interrupt and that the QE_RCV_INT bit was set while
		 * we were processing one of these earlier packets.  In
		 * this case we can safely ignore the interrupt (by dropping
		 * through the code below).
		 */
		sc->rindex = (sc->rindex + 1) % NRCV;
		nrcv++;
d651 10
a660 35
	if (nrcv && nrcv < NRCV)
		log(LOG_ERR, "qe%d: ring overrun, resync'd by skipping %d\n",
		    unit, nrcv);

	for (; sc->rring[sc->rindex].qe_status1 != QE_NOTYET;
	    sc->rindex = ++sc->rindex % NRCV) {
		rp = &sc->rring[sc->rindex];
		status1 = rp->qe_status1;
		status2 = rp->qe_status2;
		bzero((caddr_t)rp, sizeof(struct qe_ring));
		if( (status1 & QE_MASK) == QE_MASK )
			panic("qe: chained packet");
		len = ((status1 & QE_RBL_HI) | (status2 & QE_RBL_LO)) + 60;
		sc->qe_if.if_ipackets++;

		if (status1 & QE_ERROR) {
			if ((status1 & QE_RUNT) == 0)
				sc->qe_if.if_ierrors++;
		} else {
			/*
			 * We don't process setup packets.
			 */
			if (!(status1 & QE_ESETUP))
				qeread(sc, &sc->qe_ifr[sc->rindex],
					len - sizeof(struct ether_header));
		}
		/*
		 * Return the buffer to the ring
		 */
		bufaddr = (int)UBAI_ADDR(sc->qe_ifr[sc->rindex].ifrw_info);
		rp->qe_buf_len = -((MAXPACKETSIZE)/2);
		rp->qe_addr_lo = (short)bufaddr;
		rp->qe_addr_hi = (short)((int)bufaddr >> 16);
		rp->qe_flag = rp->qe_status1 = QE_NOTYET;
		rp->qe_valid = 1;
d674 1
a675 1
	struct ifreq *ifr = (struct ifreq *)data;
a681 1
		qeinit(sc);
d685 2
a686 12
			arp_ifinit(&sc->qe_ac, ifa);
			break;
#endif
#ifdef NS
		case AF_NS:
		    {
			register struct ns_addr *ina = &(IA_SNS(ifa)->sns_addr);

			if (ns_nullhost(*ina))
				ina->x_host = *(union ns_host *)(sc->qe_addr);
			else
				qe_setaddr(ina->x_host.c_host, sc);
a687 1
		    }
d694 14
a707 7
		    sc->qe_flags & QEF_RUNNING) {
			sc->qe_vaddr->qe_csr = QE_RESET;
			sc->qe_flags &= ~QEF_RUNNING;
		} else if ((ifp->if_flags & (IFF_UP|IFF_RUNNING)) ==
		    IFF_RUNNING && (sc->qe_flags & QEF_RUNNING) == 0)
			qerestart(sc);
		else
d709 7
a715 1

d724 2
a725 2
			ether_addmulti(ifr, &sc->qe_ac):
			ether_delmulti(ifr, &sc->qe_ac);
d732 1
a732 1
			qeinit(sc);
d746 1
a746 1
 * set ethernet address for unit
d748 2
a749 3
void
qe_setaddr(physaddr, sc)
	u_char *physaddr;
d751 1
d753 17
a769 1
	register int i;
d771 6
a776 7
	for (i = 0; i < 6; i++)
		sc->setup_pkt[i][1] = sc->qe_addr[i] = physaddr[i];
	sc->qe_flags |= QEF_SETADDR;
	if (sc->qe_if.if_flags & IFF_RUNNING)
		qesetup(sc);
	qeinit(sc);
}
d778 2
a780 9
/*
 * Initialize a ring descriptor with mbuf allocation side effects
 */
void
qeinitdesc(rp, addr, len)
	register struct qe_ring *rp;
	caddr_t addr;			/* mapped address */
	int len;
{
d782 2
a783 1
	 * clear the entire descriptor
d785 7
a791 1
	bzero((caddr_t)rp, sizeof(struct qe_ring));
d793 1
a793 5
	if (len) {
		rp->qe_buf_len = -(len/2);
		rp->qe_addr_lo = (short)((int)addr);
		rp->qe_addr_hi = (short)((int)addr >> 16);
	}
d795 1
d797 1
a797 2
 * Build a setup packet - the physical address will already be present
 * in first column.
d800 1
a800 1
qesetup(sc)
d803 14
a816 2
	register i, j;

d818 1
a818 1
	 * Copy the target address to the rest of the entries in this row.
d820 4
a823 3
	 for (j = 0; j < 6; j++)
		for (i = 2; i < 8; i++)
			sc->setup_pkt[j][i] = sc->setup_pkt[j][1];
d825 2
a826 1
	 * Duplicate the first half.
d828 17
a844 53
	bcopy((caddr_t)sc->setup_pkt[0], (caddr_t)sc->setup_pkt[8], 64);
	/*
	 * Fill in the broadcast (and ISO multicast) address(es).
	 */
	for (i = 0; i < 6; i++) {
		sc->setup_pkt[i][2] = 0xff;
#ifdef ISO
		/*
		 * XXX layer violation, should use SIOCADDMULTI.
		 * Will definitely break with IPmulticast.
		 */

		sc->setup_pkt[i][3] = all_es_snpa[i];
		sc->setup_pkt[i][4] = all_is_snpa[i];
		sc->setup_pkt[i][5] = all_l1is_snpa[i];
		sc->setup_pkt[i][6] = all_l2is_snpa[i];
#endif
	}
	if (sc->qe_if.if_flags & IFF_PROMISC) {
		sc->setuplength = QE_PROMISC;
	/* XXX no IFF_ALLMULTI support in 4.4bsd */
	} else if (sc->qe_if.if_flags & IFF_ALLMULTI) {
		sc->setuplength = QE_ALLMULTI;
	} else {
		register k;
		struct ether_multi *enm;
		struct ether_multistep step;
		/*
		 * Step through our list of multicast addresses, putting them
		 * in the third through fourteenth address slots of the setup
		 * packet.  (See the DEQNA manual to understand the peculiar
		 * layout of the bytes within the setup packet.)  If we have
		 * too many multicast addresses, or if we have to listen to
		 * a range of multicast addresses, turn on reception of all
		 * multicasts.
		 */
		sc->setuplength = QE_SOMEMULTI;
		i = 2;
		k = 0;
		ETHER_FIRST_MULTI(step, &sc->qe_ac, enm);
		while (enm != NULL) {
			if ((++i > 7 && k != 0) ||
			    bcmp(enm->enm_addrlo, enm->enm_addrhi, 6) != 0) {
				sc->setuplength = QE_ALLMULTI;
				break;
			}
			if (i > 7) {
				i = 1;
				k = 8;
			}
			for (j = 0; j < 6; j++)
				sc->setup_pkt[j+k][i] = enm->enm_addrlo[j];
			ETHER_NEXT_MULTI(step, enm);
d846 1
d848 2
a849 16
	sc->setupqueued++;
}

/*
 * Pass a packet to the higher levels.
 * We deal with the trailer protocol here.
 */
void
qeread(sc, ifrw, len)
	register struct qe_softc *sc;
	struct ifrw *ifrw;
	int len;
{
 	struct	ifnet *ifp = (struct ifnet *)&sc->qe_if;
	struct ether_header *eh;
	struct mbuf *m;
d852 3
a854 3
	 * Deal with trailer protocol: if type is INET trailer
	 * get true type from first 16-bit word past data.
	 * Remember that type was trailer by setting off.
d856 2
d859 5
a863 19
	eh = (struct ether_header *)ifrw->ifrw_addr;
	if (len == 0)
		return;

	/*
	 * Pull packet off interface.  Off is nonzero if packet
	 * has trailing header; qeget will then force this header
	 * information to be at the front, but we still have to drop
	 * the type and length which are at the front of any trailer data.
	 */
	m = if_ubaget(&sc->qe_uba, ifrw, len, &sc->qe_if);
#ifdef notdef
if (m) {
*(((u_long *)m->m_data)+0),
*(((u_long *)m->m_data)+1),
*(((u_long *)m->m_data)+2),
*(((u_long *)m->m_data)+3)
; }
#endif
d865 5
a869 17
#if NBPFILTER > 0
	/*
	 * Check for a BPF filter; if so, hand it up.
	 * Note that we have to stick an extra mbuf up front, because
	 * bpf_mtap expects to have the ether header at the front.
	 * It doesn't matter that this results in an ill-formatted mbuf chain,
	 * since BPF just looks at the data.  (It doesn't try to free the mbuf,
	 * tho' it will make a copy for tcpdump.)
	 */
	if (sc->qe_if.if_bpf) {
		struct mbuf m0;
		m0.m_len = sizeof (struct ether_header);
		m0.m_data = (caddr_t)eh;
		m0.m_next = m;
 
		/* Pass it up */
		bpf_mtap(sc->qe_if.if_bpf, &m0);
a870 1
#endif /* NBPFILTER > 0 */
d872 4
a875 2
	if (m)
		ether_input((struct ifnet *)&sc->qe_if, eh, m);
d879 1
a879 3
 * Watchdog timeout routine. There is a condition in the hardware that
 * causes the board to lock up under heavy load. This routine detects
 * the hang up and restarts the device.
d885 1
a885 1
	register struct qe_softc *sc = ifp->if_softc;
d887 9
a895 32
#ifdef notdef
	log(LOG_ERR, "%s: transmit timeout, restarted %d\n",
	     sc->sc_dev.dv_xname, sc->qe_restarts++);
#endif
	qerestart(sc);
}
/*
 * Restart for board lockup problem.
 */
void
qerestart(sc)
	struct qe_softc *sc;
{
	register struct ifnet *ifp = (struct ifnet *)&sc->qe_if;
	register struct qedevice *addr = sc->addr;
	register struct qe_ring *rp;
	register i;

	addr->qe_csr = QE_RESET;
	addr->qe_csr &= ~QE_RESET;
	qesetup(sc);
	for (i = 0, rp = sc->tring; i < NXMT; rp++, i++) {
		rp->qe_flag = rp->qe_status1 = QE_NOTYET;
		rp->qe_valid = 0;
	}
	sc->nxmit = sc->otindex = sc->tindex = sc->rindex = 0;
	addr->qe_csr = QE_RCV_ENABLE | QE_INT_ENABLE | QE_XMIT_INT |
	    QE_RCV_INT | QE_ILOOP;
	addr->qe_rcvlist_lo = (short)((int)sc->rringaddr);
	addr->qe_rcvlist_hi = (short)((int)sc->rringaddr >> 16);
	sc->qe_flags |= QEF_RUNNING;
	qestart(ifp);
@


1.10.4.1
log
@Continue the aborted merge of current just before 2.9 was cut into the
SMP branch.  Note that this will not make any progress of SMP functionality,
it is just merging of new code from the trunk into the old branch.
Please do not ask me questions about SMP status because of this mail,
instead go read the archives of smp@@openbsd.org, where I mailed about
these commits some week ago.  Another note: I am doing this in chunks now,
so as to not lock too much of the tree for long times
@
text
@d1 3
a3 2
/*	$OpenBSD: if_qe.c,v 1.12 2001/02/20 19:39:35 mickey Exp $ */
/*      $NetBSD: if_qe.c,v 1.39 2000/01/24 02:40:29 matt Exp $ */
d5 5
a9 1
 * Copyright (c) 1999 Ludd, University of Lule}, Sweden. All rights reserved.
d21 115
a135 15
 *      This product includes software developed at Ludd, University of 
 *      Lule}, Sweden and its contributors.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
d139 2
a140 6
 * Driver for DEQNA/DELQA ethernet cards.
 * Things that is still to do:
 *	Have a timeout check for hang transmit logic.
 *	Handle ubaresets. Does not work at all right now.
 *	Fix ALLMULTI reception. But someone must tell me how...
 *	Collect statistics.
d146 1
d148 2
d151 3
d155 2
a156 2
#include <sys/systm.h>
#include <sys/sockio.h>
d159 4
a162 1
#include <net/if_dl.h>
d164 3
d168 20
d193 12
d206 2
a207 4
#include <machine/bus.h>

#include <arch/vax/qbus/ubavar.h>
#include <arch/vax/qbus/if_qereg.h>
d209 1
a209 3
#define RXDESCS	30	/* # of receive descriptors */
#define TXDESCS	60	/* # transmit descs */
#define ETHER_MINLEN 64	/* min frame + crc */
d212 5
a216 1
 * Structure containing the elements that must be in DMA-safe memory.
d218 29
a246 4
struct qe_cdata {
	struct qe_ring	qc_recv[RXDESCS+1];	/* Receive descriptors */
	struct qe_ring	qc_xmit[TXDESCS+1];	/* Transmit descriptors */
	u_int8_t	qc_setup[128];		/* Setup packet layout */
d249 18
a266 20
struct	qe_softc {
	struct device	sc_dev;		/* Configuration common part	*/
	struct arpcom	sc_ac;		/* Ethernet common part		*/
#define sc_if	sc_ac.ac_if		/* network-visible interface	*/
	bus_space_tag_t sc_iot;
	bus_addr_t	sc_ioh;
	bus_dma_tag_t	sc_dmat;
	struct qe_cdata *sc_qedata;	/* Descriptor struct		*/
	struct qe_cdata *sc_pqedata;	/* Unibus address of above	*/
	bus_dmamap_t	sc_cmap;	/* Map for control structures	*/
	struct mbuf*	sc_txmbuf[TXDESCS];
	struct mbuf*	sc_rxmbuf[RXDESCS];
	bus_dmamap_t	sc_xmtmap[TXDESCS];
	bus_dmamap_t	sc_rcvmap[RXDESCS];
	int		sc_intvec;	/* Interrupt vector		*/
	int		sc_nexttx;
	int		sc_inq;
	int		sc_lastack;
	int		sc_nextrx;
	int		sc_setup;	/* Setup packet in queue	*/
a268 10
static	int	qematch __P((struct device *, struct cfdata *, void *));
static	void	qeattach __P((struct device *, struct device *, void *));
static	void	qeinit __P((struct qe_softc *));
static	void	qestart __P((struct ifnet *));
static	void	qeintr __P((void *));
static	int	qeioctl __P((struct ifnet *, u_long, caddr_t));
static	int	qe_add_rxbuf __P((struct qe_softc *, int));
static	void	qe_setup __P((struct qe_softc *));
static	void	qetimeout __P((struct ifnet *));

d270 1
a270 1
	sizeof(struct qe_softc), (cfmatch_t)qematch, qeattach
d273 8
a280 7
#define	QE_WCSR(csr, val) \
	bus_space_write_2(sc->sc_iot, sc->sc_ioh, csr, val)
#define	QE_RCSR(csr) \
	bus_space_read_2(sc->sc_iot, sc->sc_ioh, csr)

#define	LOWORD(x)	((int)(x) & 0xffff)
#define	HIWORD(x)	(((int)(x) >> 16) & 0x3f)
d283 1
a283 2
 * Check for present DEQNA. Done by sending a fake setup packet
 * and wait for interrupt.
d286 1
a286 1
qematch(parent, cf, aux)
d288 1
a288 2
	struct	cfdata *cf;
	void	*aux;
d290 1
a290 3
	bus_dmamap_t	cmap;
	struct	qe_softc ssc;
	struct	qe_softc *sc = &ssc;
a292 3

#define	PROBESIZE	(sizeof(struct qe_ring) * 4 + 128)
	struct	qe_ring ring[15]; /* For diag purposes only */
d294 3
a296 1
	int error;
d298 4
a301 5
	bzero(sc, sizeof(struct qe_softc));
	bzero(ring, PROBESIZE);
	sc->sc_iot = ua->ua_iot;
	sc->sc_ioh = ua->ua_ioh;
	sc->sc_dmat = ua->ua_dmat;
d303 3
a305 3
	ubasc->uh_lastiv -= 4;
	QE_WCSR(QE_CSR_CSR, QE_RESET);
	QE_WCSR(QE_CSR_VECTOR, ubasc->uh_lastiv);
d308 1
a308 3
	 * Map the ring area. Actually this is done only to be able to 
	 * send and receive a internal packet; some junk is loopbacked
	 * so that the DEQNA has a reason to interrupt.
d310 5
a314 11
	if ((error = bus_dmamap_create(sc->sc_dmat, PROBESIZE, 1, PROBESIZE, 0,
	    BUS_DMA_NOWAIT, &cmap))) {
		printf("qematch: bus_dmamap_create failed = %d\n", error);
		return 0;
	}
	if ((error = bus_dmamap_load(sc->sc_dmat, cmap, ring, PROBESIZE, 0,
	    BUS_DMA_NOWAIT))) {
		printf("qematch: bus_dmamap_load failed = %d\n", error);
		bus_dmamap_destroy(sc->sc_dmat, cmap);
		return 0;
	}
d317 4
a320 2
	 * Init a simple "fake" receive and transmit descriptor that
	 * points to some unused area. Send a fake setup packet.
d322 4
a325 5
	rp = (void *)cmap->dm_segs[0].ds_addr;
	ring[0].qe_flag = ring[0].qe_status1 = QE_NOTYET;
	ring[0].qe_addr_lo = LOWORD(&rp[4]);
	ring[0].qe_addr_hi = HIWORD(&rp[4]) | QE_VALID | QE_EOMSG | QE_SETUP;
	ring[0].qe_buf_len = 128;
d327 5
a331 4
	ring[2].qe_flag = ring[2].qe_status1 = QE_NOTYET;
	ring[2].qe_addr_lo = LOWORD(&rp[4]);
	ring[2].qe_addr_hi = HIWORD(&rp[4]) | QE_VALID;
	ring[2].qe_buf_len = 128;
d333 3
a335 2
	QE_WCSR(QE_CSR_CSR, QE_RCSR(QE_CSR_CSR) & ~QE_RESET);
	DELAY(1000);
d338 9
d349 6
a354 5
	QE_WCSR(QE_CSR_CSR, QE_INT_ENABLE|QE_XMIT_INT|QE_RCV_INT);
	QE_WCSR(QE_CSR_RCLL, LOWORD(&rp[2]));
	QE_WCSR(QE_CSR_RCLH, HIWORD(&rp[2]));
	QE_WCSR(QE_CSR_XMTL, LOWORD(rp));
	QE_WCSR(QE_CSR_XMTH, HIWORD(rp));
a355 1

d359 4
a362 2
	bus_dmamap_unload(sc->sc_dmat, cmap);
	bus_dmamap_destroy(sc->sc_dmat, cmap);
a376 1
	struct	uba_softc *ubasc = (struct uba_softc *)parent;
d378 3
a380 44
	struct	ifnet *ifp = (struct ifnet *)&sc->sc_if;
	struct	qe_ring *rp;
	u_int8_t enaddr[ETHER_ADDR_LEN];
	bus_dma_segment_t seg;
	int i, rseg, error;

	sc->sc_iot = ua->ua_iot;
	sc->sc_ioh = ua->ua_ioh;
	sc->sc_dmat = ua->ua_dmat;

        /*
         * Allocate DMA safe memory for descriptors and setup memory.
         */
	if ((error = bus_dmamem_alloc(sc->sc_dmat,
	    sizeof(struct qe_cdata), NBPG, 0, &seg, 1, &rseg,
	    BUS_DMA_NOWAIT)) != 0) {
		printf(": unable to allocate control data, error = %d\n",
		    error);
		goto fail_0;
	}

	if ((error = bus_dmamem_map(sc->sc_dmat, &seg, rseg,
	    sizeof(struct qe_cdata), (caddr_t *)&sc->sc_qedata,
	    BUS_DMA_NOWAIT|BUS_DMA_COHERENT)) != 0) {
		printf(": unable to map control data, error = %d\n", error);
		goto fail_1;
	}

	if ((error = bus_dmamap_create(sc->sc_dmat,
	    sizeof(struct qe_cdata), 1,
	    sizeof(struct qe_cdata), 0, BUS_DMA_NOWAIT,
	    &sc->sc_cmap)) != 0) {
		printf(": unable to create control data DMA map, error = %d\n",
		    error);
		goto fail_2;
	}

	if ((error = bus_dmamap_load(sc->sc_dmat, sc->sc_cmap,
	    sc->sc_qedata, sizeof(struct qe_cdata), NULL,
	    BUS_DMA_NOWAIT)) != 0) {
		printf(": unable to load control data DMA map, error = %d\n",
		    error);
		goto fail_3;
	}
d382 4
d387 2
a388 1
	 * Zero the newly allocated memory.
d390 2
a391 16
	bzero(sc->sc_qedata, sizeof(struct qe_cdata));
	/*
	 * Create the transmit descriptor DMA maps. We take advantage
	 * of the fact that the Qbus address space is big, and therefore 
	 * allocate map registers for all transmit descriptors also,
	 * so that we can avoid this each time we send a packet.
	 */
	for (i = 0; i < TXDESCS; i++) {
		if ((error = bus_dmamap_create(sc->sc_dmat, MCLBYTES,
		    1, MCLBYTES, 0, BUS_DMA_NOWAIT|BUS_DMA_ALLOCNOW,
		    &sc->sc_xmtmap[i]))) {
			printf(": unable to create tx DMA map %d, error = %d\n",
			    i, error);
			goto fail_4;
		}
	}
d394 1
a394 1
	 * Create receive buffer DMA maps.
d396 8
a403 37
	for (i = 0; i < RXDESCS; i++) {
		if ((error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1,
		    MCLBYTES, 0, BUS_DMA_NOWAIT,
		    &sc->sc_rcvmap[i]))) {
			printf(": unable to create rx DMA map %d, error = %d\n",
			    i, error);
			goto fail_5;
		}
	}
	/*
	 * Pre-allocate the receive buffers.
	 */
	for (i = 0; i < RXDESCS; i++) {
		if ((error = qe_add_rxbuf(sc, i)) != 0) {
			printf(": unable to allocate or map rx buffer %d\n,"
			    " error = %d\n", i, error);
			goto fail_6;
		}
	}

	/*
	 * Create ring loops of the buffer chains.
	 * This is only done once.
	 */
	sc->sc_pqedata = (struct qe_cdata *)sc->sc_cmap->dm_segs[0].ds_addr;

	rp = sc->sc_qedata->qc_recv;
	rp[RXDESCS].qe_addr_lo = LOWORD(&sc->sc_pqedata->qc_recv[0]);
	rp[RXDESCS].qe_addr_hi = HIWORD(&sc->sc_pqedata->qc_recv[0]) |
	    QE_VALID | QE_CHAIN;
	rp[RXDESCS].qe_flag = rp[RXDESCS].qe_status1 = QE_NOTYET;

	rp = sc->sc_qedata->qc_xmit;
	rp[TXDESCS].qe_addr_lo = LOWORD(&sc->sc_pqedata->qc_xmit[0]);
	rp[TXDESCS].qe_addr_hi = HIWORD(&sc->sc_pqedata->qc_xmit[0]) |
	    QE_VALID | QE_CHAIN;
	rp[TXDESCS].qe_flag = rp[TXDESCS].qe_status1 = QE_NOTYET;
d406 1
a406 1
	 * Get the vector that were set at match time, and remember it.
d408 1
a408 17
	sc->sc_intvec = ubasc->uh_lastiv;
	QE_WCSR(QE_CSR_CSR, QE_RESET);
	DELAY(1000);
	QE_WCSR(QE_CSR_CSR, QE_RCSR(QE_CSR_CSR) & ~QE_RESET);

	/*
	 * Read out ethernet address and tell which type this card is.
	 */
	for (i = 0; i < 6; i++)
		enaddr[i] = QE_RCSR(i * 2) & 0xff;

	QE_WCSR(QE_CSR_VECTOR, sc->sc_intvec | 1);
	printf("\n%s: %s, hardware address %s\n", sc->sc_dev.dv_xname,
		QE_RCSR(QE_CSR_VECTOR) & 1 ? "delqa":"deqna",
		ether_sprintf(enaddr));

	QE_WCSR(QE_CSR_VECTOR, QE_RCSR(QE_CSR_VECTOR) & ~1); /* ??? */
a409 5
	uba_intr_establish(ua->ua_icookie, ua->ua_cvec, qeintr, sc);

	strcpy(ifp->if_xname, sc->sc_dev.dv_xname);
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
d413 1
a413 4

	/*
	 * Attach the interface.
	 */
a415 1
	return;
d417 17
a433 31
	/*
	 * Free any resources we've allocated during the failed attach
	 * attempt.  Do this in reverse order and fall through.
	 */
 fail_6:
	for (i = 0; i < RXDESCS; i++) {
		if (sc->sc_rxmbuf[i] != NULL) {
			bus_dmamap_unload(sc->sc_dmat, sc->sc_xmtmap[i]);
			m_freem(sc->sc_rxmbuf[i]);
		}
	}
 fail_5:
	for (i = 0; i < RXDESCS; i++) {
		if (sc->sc_xmtmap[i] != NULL)
			bus_dmamap_destroy(sc->sc_dmat, sc->sc_xmtmap[i]);
	}
 fail_4:
	for (i = 0; i < TXDESCS; i++) {
		if (sc->sc_rcvmap[i] != NULL)
			bus_dmamap_destroy(sc->sc_dmat, sc->sc_rcvmap[i]);
	}
	bus_dmamap_unload(sc->sc_dmat, sc->sc_cmap);
 fail_3:
	bus_dmamap_destroy(sc->sc_dmat, sc->sc_cmap);
 fail_2:
	bus_dmamem_unmap(sc->sc_dmat, (caddr_t)sc->sc_qedata,
	    sizeof(struct qe_cdata));
 fail_1:
	bus_dmamem_free(sc->sc_dmat, &seg, rseg);
 fail_0:
	return;
d443 3
a445 2
	struct ifnet *ifp = (struct ifnet *)&sc->sc_if;
	struct qe_cdata *qc = sc->sc_qedata;
d447 1
d449 5
d455 26
a480 17
	/*
	 * Reset the interface.
	 */
	QE_WCSR(QE_CSR_CSR, QE_RESET);
	DELAY(1000);
	QE_WCSR(QE_CSR_CSR, QE_RCSR(QE_CSR_CSR) & ~QE_RESET);
	QE_WCSR(QE_CSR_VECTOR, sc->sc_intvec);

	sc->sc_nexttx = sc->sc_inq = sc->sc_lastack = 0;
	/*
	 * Release and init transmit descriptors.
	 */
	for (i = 0; i < TXDESCS; i++) {
		if (sc->sc_txmbuf[i]) {
			bus_dmamap_unload(sc->sc_dmat, sc->sc_xmtmap[i]);
			m_freem(sc->sc_txmbuf[i]);
			sc->sc_txmbuf[i] = 0;
a481 2
		qc->qc_xmit[i].qe_addr_hi = 0; /* Clear valid bit */
		qc->qc_xmit[i].qe_status1 = qc->qc_xmit[i].qe_flag = QE_NOTYET;
a482 20


	/*
	 * Init receive descriptors.
	 */
	for (i = 0; i < RXDESCS; i++)
		qc->qc_recv[i].qe_status1 = qc->qc_recv[i].qe_flag = QE_NOTYET;
	sc->sc_nextrx = 0;

	/*
	 * Write the descriptor addresses to the device.
	 * Receiving packets will be enabled in the interrupt routine.
	 */
	QE_WCSR(QE_CSR_CSR, QE_INT_ENABLE|QE_XMIT_INT|QE_RCV_INT);
	QE_WCSR(QE_CSR_RCLL, LOWORD(sc->sc_pqedata->qc_recv));
	QE_WCSR(QE_CSR_RCLH, HIWORD(sc->sc_pqedata->qc_recv));

	ifp->if_flags |= IFF_RUNNING;
	ifp->if_flags &= ~IFF_OACTIVE;

d484 2
a485 2
	 * Send a setup frame.
	 * This will start the transmit machinery as well.
d487 43
a529 2
	qe_setup(sc);

d534 1
d540 6
a545 6
	struct qe_softc *sc = ifp->if_softc;
	struct qe_cdata *qc = sc->sc_qedata;
	paddr_t	buffer;
	struct mbuf *m, *m0;
	int idx, len, s, i, totlen, error;
	short orword;
a546 2
	if ((QE_RCSR(QE_CSR_CSR) & QE_RCV_ENABLE) == 0)
		return;
d548 27
a574 28
	s = splimp();
	while (sc->sc_inq < (TXDESCS - 1)) {

		if (sc->sc_setup) {
			qe_setup(sc);
			continue;
		}
		idx = sc->sc_nexttx;
		IF_DEQUEUE(&sc->sc_if.if_snd, m);
		if (m == 0)
			goto out;
		/*
		 * Count number of mbufs in chain.
		 * Always do DMA directly from mbufs, therefore the transmit
		 * ring is really big.
		 */
		for (m0 = m, i = 0; m0; m0 = m0->m_next)
			if (m0->m_len)
				i++;
		if (i >= TXDESCS)
			panic("qestart");

		if ((i + sc->sc_inq) >= (TXDESCS - 1)) {
			IF_PREPEND(&sc->sc_if.if_snd, m);
			ifp->if_flags |= IFF_OACTIVE;
			goto out;
		}
		
d576 2
a577 2
		if (ifp->if_bpf)
			bpf_mtap(ifp->if_bpf, m);
d579 5
d585 1
a585 2
		 * m now points to a mbuf chain that can be loaded.
		 * Loop around and set it.
d587 16
a602 38
		totlen = 0;
		for (m0 = m; m0; m0 = m0->m_next) {
			error = bus_dmamap_load(sc->sc_dmat, sc->sc_xmtmap[idx],
			    mtod(m0, void *), m0->m_len, 0, 0);
			buffer = sc->sc_xmtmap[idx]->dm_segs[0].ds_addr;
			len = m0->m_len;
			if (len == 0)
				continue;

			totlen += len;
			/* Word alignment calc */
			orword = 0;
			if (totlen == m->m_pkthdr.len) {
				if (totlen < ETHER_MINLEN)
					len += (ETHER_MINLEN - totlen);
				orword |= QE_EOMSG;
				sc->sc_txmbuf[idx] = m;
			}
			if ((buffer & 1) || (len & 1))
				len += 2;
			if (buffer & 1)
				orword |= QE_ODDBEGIN;
			if ((buffer + len) & 1)
				orword |= QE_ODDEND;
			qc->qc_xmit[idx].qe_buf_len = -(len/2);
			qc->qc_xmit[idx].qe_addr_lo = LOWORD(buffer);
			qc->qc_xmit[idx].qe_addr_hi = HIWORD(buffer);
			qc->qc_xmit[idx].qe_flag =
			    qc->qc_xmit[idx].qe_status1 = QE_NOTYET;
			qc->qc_xmit[idx].qe_addr_hi |= (QE_VALID | orword);
			if (++idx == TXDESCS)
				idx = 0;
			sc->sc_inq++;
		}
#ifdef DIAGNOSTIC
		if (totlen != m->m_pkthdr.len)
			panic("qestart: len fault");
#endif
d605 6
a610 7
		 * Kick off the transmit logic, if it is stopped.
		 */
		if (QE_RCSR(QE_CSR_CSR) & QE_XL_INVALID) {
			QE_WCSR(QE_CSR_XMTL,
			    LOWORD(&sc->sc_pqedata->qc_xmit[sc->sc_nexttx]));
			QE_WCSR(QE_CSR_XMTH,
			    HIWORD(&sc->sc_pqedata->qc_xmit[sc->sc_nexttx]));
a611 1
		sc->sc_nexttx = idx;
a612 5
	if (sc->sc_inq == (TXDESCS - 1))
		ifp->if_flags |= IFF_OACTIVE;

out:	if (sc->sc_inq)
		ifp->if_timer = 5; /* If transmit logic dies */
d614 1
d617 33
a649 10
static void
qeintr(arg)
	void *arg;
{
	struct qe_softc *sc = arg;
	struct qe_cdata *qc = sc->sc_qedata;
	struct ifnet *ifp = &sc->sc_if;
	struct ether_header *eh;
	struct mbuf *m;
	int csr, status1, status2, len;
d651 12
a662 1
	csr = QE_RCSR(QE_CSR_CSR);
a663 2
	QE_WCSR(QE_CSR_CSR, QE_RCV_ENABLE | QE_INT_ENABLE | QE_XMIT_INT |
	    QE_RCV_INT | QE_ILOOP);
d665 21
a685 25
	if (csr & QE_RCV_INT)
		while (qc->qc_recv[sc->sc_nextrx].qe_status1 != QE_NOTYET) {
			status1 = qc->qc_recv[sc->sc_nextrx].qe_status1;
			status2 = qc->qc_recv[sc->sc_nextrx].qe_status2;
			m = sc->sc_rxmbuf[sc->sc_nextrx];
			len = ((status1 & QE_RBL_HI) |
			    (status2 & QE_RBL_LO)) + 60;
			qe_add_rxbuf(sc, sc->sc_nextrx);
			m->m_pkthdr.rcvif = ifp;
			m->m_pkthdr.len = m->m_len = len;
			if (++sc->sc_nextrx == RXDESCS)
				sc->sc_nextrx = 0;
			eh = mtod(m, struct ether_header *);
#if NBPFILTER > 0
			if (ifp->if_bpf) {
				bpf_mtap(ifp->if_bpf, m);
				if ((ifp->if_flags & IFF_PROMISC) != 0 &&
				    bcmp(sc->sc_ac.ac_enaddr, eh->ether_dhost,
				    ETHER_ADDR_LEN) != 0 &&
				    ((eh->ether_dhost[0] & 1) == 0)) {
					m_freem(m);
					continue;
				}
			}
#endif
d687 1
a687 1
			 * ALLMULTI means PROMISC in this driver.
d689 8
a696 6
			if ((ifp->if_flags & IFF_ALLMULTI) &&
			    ((eh->ether_dhost[0] & 1) == 0) &&
			    bcmp(sc->sc_ac.ac_enaddr, eh->ether_dhost,
			    ETHER_ADDR_LEN)) {
				m_freem(m);
				continue;
a697 1
			ether_input(ifp, eh, m);
d699 4
d704 70
a773 20
	if (csr & QE_XMIT_INT) {
		while (qc->qc_xmit[sc->sc_lastack].qe_status1 != QE_NOTYET) {
			int idx = sc->sc_lastack;

			sc->sc_inq--;
			if (++sc->sc_lastack == TXDESCS)
				sc->sc_lastack = 0;

			/* XXX collect statistics */
			qc->qc_xmit[idx].qe_addr_hi &= ~QE_VALID;
			qc->qc_xmit[idx].qe_status1 =
			    qc->qc_xmit[idx].qe_flag = QE_NOTYET;

			if (qc->qc_xmit[idx].qe_addr_hi & QE_SETUP)
				continue;
			bus_dmamap_unload(sc->sc_dmat, sc->sc_xmtmap[idx]);
			if (sc->sc_txmbuf[idx]) {
				m_freem(sc->sc_txmbuf[idx]);
				sc->sc_txmbuf[idx] = 0;
			}
d775 9
a783 14
		ifp->if_timer = 0;
		ifp->if_flags &= ~IFF_OACTIVE;
		qestart(ifp); /* Put in more in queue */
	}
	/*
	 * How can the receive list get invalid???
	 * Verified that it happens anyway.
	 */
	if ((qc->qc_recv[sc->sc_nextrx].qe_status1 == QE_NOTYET) &&
	    (QE_RCSR(QE_CSR_CSR) & QE_RL_INVALID)) {
		QE_WCSR(QE_CSR_RCLL,
		    LOWORD(&sc->sc_pqedata->qc_recv[sc->sc_nextrx]));
		QE_WCSR(QE_CSR_RCLH,
		    HIWORD(&sc->sc_pqedata->qc_recv[sc->sc_nextrx]));
d797 1
a798 1
	struct ifaddr *ifa = (struct ifaddr *)data;
d805 1
d809 12
a820 2
			qeinit(sc);
			arp_ifinit(&sc->sc_ac, ifa);
d822 1
d829 7
a835 14
		    (ifp->if_flags & IFF_RUNNING) != 0) {
			/*
			 * If interface is marked down and it is running,
			 * stop it. (by disabling receive mechanism).
			 */
			QE_WCSR(QE_CSR_CSR,
			    QE_RCSR(QE_CSR_CSR) & ~QE_RCV_ENABLE);
			ifp->if_flags &= ~IFF_RUNNING;
		} else if ((ifp->if_flags & IFF_UP) != 0 &&
			   (ifp->if_flags & IFF_RUNNING) == 0) {
			/*
			 * If interface it marked up and it is stopped, then
			 * start it.
			 */
d837 1
a837 7
		} else if ((ifp->if_flags & IFF_UP) != 0) {
			/*
			 * Send a new setup packet to match any new changes.
			 * (Like IFF_PROMISC etc)
			 */
			qe_setup(sc);
		}
d846 2
a847 2
			ether_addmulti(ifr, &sc->sc_ac):
			ether_delmulti(ifr, &sc->sc_ac);
d854 1
a854 1
			qe_setup(sc);
d868 1
a868 1
 * Add a receive buffer to the indicated descriptor.
d870 3
a872 2
int
qe_add_rxbuf(sc, i) 
a873 1
	int i;
d875 1
a875 4
	struct mbuf *m;
	struct qe_ring *rp;
	vaddr_t addr;
	int error;
d877 7
a883 3
	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL)
		return (ENOBUFS);
a884 18
	MCLGET(m, M_DONTWAIT);
	if ((m->m_flags & M_EXT) == 0) {
		m_freem(m);
		return (ENOBUFS);
	}

	if (sc->sc_rxmbuf[i] != NULL)
		bus_dmamap_unload(sc->sc_dmat, sc->sc_rcvmap[i]);

	error = bus_dmamap_load(sc->sc_dmat, sc->sc_rcvmap[i],
	    m->m_ext.ext_buf, m->m_ext.ext_size, NULL, BUS_DMA_NOWAIT);
	if (error)
		panic("%s: can't load rx DMA map %d, error = %d\n",
		    sc->sc_dev.dv_xname, i, error);
	sc->sc_rxmbuf[i] = m;

	bus_dmamap_sync(sc->sc_dmat, sc->sc_rcvmap[i], 0,
	    sc->sc_rcvmap[i]->dm_mapsize, BUS_DMASYNC_PREREAD);
d886 9
d896 1
a896 2
	 * We know that the mbuf cluster is page aligned. Also, be sure
	 * that the IP header will be longword aligned.
d898 1
a898 7
	m->m_data += 2;
	addr = sc->sc_rcvmap[i]->dm_segs[0].ds_addr + 2;
	rp = &sc->sc_qedata->qc_recv[i];
	rp->qe_flag = rp->qe_status1 = QE_NOTYET;
	rp->qe_addr_lo = LOWORD(addr);
	rp->qe_addr_hi = HIWORD(addr) | QE_VALID;
	rp->qe_buf_len = -(m->m_ext.ext_size - 2)/2;
d900 5
a904 1
	return (0);
a905 1

d907 2
a908 1
 * Create a setup packet and put in queue for sending.
d911 1
a911 1
qe_setup(sc)
d914 8
a921 14
	struct ether_multi *enm;
	struct ether_multistep step;
	struct qe_cdata *qc = sc->sc_qedata;
	struct ifnet *ifp = &sc->sc_if;
	u_int8_t *enaddr = sc->sc_ac.ac_enaddr;
	int i, j, k, idx, s;

	s = splimp();
	if (sc->sc_inq == (TXDESCS - 1)) {
		sc->sc_setup = 1;
		splx(s);
		return;
	}
	sc->sc_setup = 0;
d923 1
a923 1
	 * Init the setup packet with valid info.
d925 1
a925 4
	memset(qc->qc_setup, 0xff, sizeof(qc->qc_setup)); /* Broadcast */
	for (i = 0; i < ETHER_ADDR_LEN; i++)
		qc->qc_setup[i * 8 + 1] = enaddr[i]; /* Own address */

d927 1
a927 2
	 * Multicast handling. The DEQNA can handle up to 12 direct 
	 * ethernet addresses.
d929 49
a977 17
	j = 3; k = 0;
	ifp->if_flags &= ~IFF_ALLMULTI;
	ETHER_FIRST_MULTI(step, &sc->sc_ac, enm);
	while (enm != NULL) {
		if (bcmp(enm->enm_addrlo, enm->enm_addrhi, 6)) {
			ifp->if_flags |= IFF_ALLMULTI;
			break;
		}
		for (i = 0; i < ETHER_ADDR_LEN; i++)
			qc->qc_setup[i * 8 + j + k] = enm->enm_addrlo[i];
		j++;
		if (j == 8) {
			j = 1; k += 64;
		}
		if (k > 64) {
			ifp->if_flags |= IFF_ALLMULTI;
			break;
a978 1
		ETHER_NEXT_MULTI(step, enm);
d980 16
a995 2
	idx = sc->sc_nexttx;
	qc->qc_xmit[idx].qe_buf_len = -64;
d998 3
a1000 3
	 * How is the DEQNA turned in ALLMULTI mode???
	 * Until someone tells me, fall back to PROMISC when more than
	 * 12 ethernet addresses.
a1001 2
	if (ifp->if_flags & (IFF_PROMISC|IFF_ALLMULTI))
		qc->qc_xmit[idx].qe_buf_len = -65;
d1003 19
a1021 5
	qc->qc_xmit[idx].qe_addr_lo = LOWORD(sc->sc_pqedata->qc_setup);
	qc->qc_xmit[idx].qe_addr_hi =
	    HIWORD(sc->sc_pqedata->qc_setup) | QE_SETUP | QE_EOMSG;
	qc->qc_xmit[idx].qe_status1 = qc->qc_xmit[idx].qe_flag = QE_NOTYET;
	qc->qc_xmit[idx].qe_addr_hi |= QE_VALID;
d1023 17
a1039 5
	if (QE_RCSR(QE_CSR_CSR) & QE_XL_INVALID) {
		QE_WCSR(QE_CSR_XMTL,
		    LOWORD(&sc->sc_pqedata->qc_xmit[idx]));
		QE_WCSR(QE_CSR_XMTH,
		    HIWORD(&sc->sc_pqedata->qc_xmit[idx]));
d1041 1
d1043 2
a1044 4
	sc->sc_inq++;
	if (++sc->sc_nexttx == TXDESCS)
		sc->sc_nexttx = 0;
	splx(s);
d1048 3
a1050 1
 * Check for dead transmit logic. Not uncommon.
d1056 1
a1056 1
	struct qe_softc *sc = ifp->if_softc;
d1058 32
a1089 9
	if (sc->sc_inq == 0)
		return;

	printf("%s: xmit logic died, resetting...\n", sc->sc_dev.dv_xname);
	/*
	 * Do a reset of interface, to get it going again.
	 * Will it work by just restart the transmit logic?
	 */
	qeinit(sc);
@


1.10.4.2
log
@Merge in -current from about a week ago
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d101 9
a109 9
static	int	qematch(struct device *, struct cfdata *, void *);
static	void	qeattach(struct device *, struct device *, void *);
static	void	qeinit(struct qe_softc *);
static	void	qestart(struct ifnet *);
static	void	qeintr(void *);
static	int	qeioctl(struct ifnet *, u_long, caddr_t);
static	int	qe_add_rxbuf(struct qe_softc *, int);
static	void	qe_setup(struct qe_softc *);
static	void	qetimeout(struct ifnet *);
@


1.10.4.3
log
@Sync the SMP branch with 3.3
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*      $NetBSD: if_qe.c,v 1.51 2002/06/08 12:28:37 ragge Exp $ */
d36 1
a52 1

d64 1
a64 1
#include <arch/vax/if/if_qereg.h>
d68 1
a80 1
	struct evcnt	sc_intrcnt;	/* Interrupt counting		*/
d88 1
a92 1
	struct ubinfo	sc_ui;
a114 4
struct cfdriver qe_cd = {
	NULL, "qe", DV_IFNET
};

d128 4
a131 1
qematch(struct device *parent, struct cfdata *cf, void *aux)
d133 1
a137 1
	struct ubinfo ui;
d139 2
a140 2
#define	PROBESIZE	4096
	struct qe_ring *ring;
a143 1
	ring = malloc(PROBESIZE, M_TEMP, M_WAITOK);
d159 9
a167 3
	ui.ui_size = PROBESIZE;
	ui.ui_vaddr = (caddr_t)&ring[0];
	if ((error = uballoc((void *)parent, &ui, UBA_CANTWAIT)))
d169 1
d175 1
a175 1
	rp = (void *)ui.ui_baddr;
d179 1
a179 1
	ring[0].qe_buf_len = -64;
d184 1
a184 1
	ring[2].qe_buf_len = -(1500/2);
d202 2
a203 2
	ubfree((void *)parent, &ui);
	free(ring, M_TEMP);
d213 3
a215 1
qeattach(struct device *parent, struct device *self, void *aux)
d222 3
a224 1
	int i, error;
d233 30
a262 5

	sc->sc_ui.ui_size = sizeof(struct qe_cdata);
	if ((error = ubmemalloc((struct uba_softc *)parent, &sc->sc_ui, 0))) {
		printf(": unable to ubmemalloc(), error = %d\n", error);
		return;
a263 2
	sc->sc_pqedata = (struct qe_cdata *)sc->sc_ui.ui_baddr;
	sc->sc_qedata = (struct qe_cdata *)sc->sc_ui.ui_vaddr;
d312 1
d338 1
a338 1
		sc->sc_ac.ac_enaddr[i] = QE_RCSR(i * 2) & 0xff;
d343 1
a343 1
		ether_sprintf(sc->sc_ac.ac_enaddr));
d347 1
a347 3
	uba_intr_establish(ua->ua_icookie, ua->ua_cvec, qeintr,
		sc, &sc->sc_intrcnt);
	evcnt_attach(&sc->sc_dev, "intr", &sc->sc_intrcnt);
a354 1
	IFQ_SET_READY(&ifp->if_snd);
a360 1

d384 10
d400 2
a401 1
qeinit(struct qe_softc *sc)
d461 2
a462 1
qestart(struct ifnet *ifp)
d469 1
a469 1
	short orword, csr;
d474 1
a474 1
	s = splnet();
d482 1
a482 1
		IFQ_POLL(&ifp->if_snd, m);
d497 1
d501 1
a501 3

		IFQ_DEQUEUE(&ifp->if_snd, m);

d523 2
a524 2
				if (totlen < ETHER_MIN_LEN)
					len += (ETHER_MIN_LEN - totlen);
d552 1
a552 2
		csr = QE_RCSR(QE_CSR_CSR);
		if (csr & QE_XL_INVALID) {
d569 2
a570 1
qeintr(void *arg)
a587 1

d619 1
a619 7

			if ((status1 & QE_ESETUP) == 0) {
				/* m_adj() the ethernet header out of the way and pass up */
				m_adj(m, sizeof(struct ether_header));
				ether_input(ifp, eh, m);
			} else
				m_freem(m);
d622 1
a622 1
	if (csr & (QE_XMIT_INT|QE_XL_INVALID)) {
d664 4
a667 1
qeioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
d745 3
a747 1
qe_add_rxbuf(struct qe_softc *sc, int i) 
d770 1
a770 1
		panic("%s: can't load rx DMA map %d, error = %d",
d796 2
a797 1
qe_setup(struct qe_softc *sc)
d806 1
a806 1
	s = splnet();
d828 1
a828 1
		if (memcmp(enm->enm_addrlo, enm->enm_addrhi, 6)) {
d852 1
a852 5
	if (ifp->if_flags & IFF_ALLMULTI)
		ifp->if_flags |= IFF_PROMISC;
	else if (ifp->if_pcount == 0)
		ifp->if_flags &= ~IFF_PROMISC;
	if (ifp->if_flags & IFF_PROMISC)
d878 2
a879 1
qetimeout(struct ifnet *ifp)
@


1.10.4.4
log
@Sync the SMP branch to -current.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.10.4.3 2003/03/27 23:52:19 niklas Exp $	*/
d318 1
a318 1
	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, sizeof ifp->if_xname);
@


1.9
log
@Sync with NetBSD 970516. -moj
@
text
@d1 1
a1 1
/*	$OpenBSD: if_qe.c,v 1.8 1997/05/29 00:04:32 niklas Exp $ */
a1039 13

		/*
		 * Note that the interface cannot be in promiscuous mode if
		 * there are no BPF listeners.	And if we are in promiscuous
		 * mode, we have to check if this packet is really ours.
		 */
		if ((ifp->if_flags & IFF_PROMISC) &&
		    (eh->ether_dhost[0] & 1) == 0 && /* !mcast and !bcast */
		    bcmp(eh->ether_dhost, sc->qe_addr,
			    sizeof(eh->ether_dhost)) != 0) {
			m_freem(m);
			return;
		}
@


1.8
log
@RCS tagging
@
text
@d1 2
a2 2
/*	$OpenBSD: if_qe.c,v 1.18 1996/10/13 03:34:55 christos Exp $ */
/*	$NetBSD: if_qe.c,v 1.18 1996/10/13 03:34:55 christos Exp $ */
d42 1
a42 1
/* from  @@(#)if_qe.c	1.15	(ULTRIX)	4/16/86 */
d46 6
a51 6
 *        Licensed from Digital Equipment Corporation 		*
 *                       Copyright (c) 				*
 *               Digital Equipment Corporation			*
 *                   Maynard, Massachusetts 			*
 *                         1985, 1986 				*
 *                    All rights reserved. 			*
d53 1
a53 1
 *        The Information in this software is subject to change *
d55 1
a55 1
 *   by  Digital  Equipment  Corporation.   Digital   makes  no *
d58 1
a58 1
 *   implied  warranty. 					*
d60 5
a64 5
 *        If the Regents of the University of California or its *
 *   licensees modify the software in a manner creating  	*
 *   derivative copyright rights, appropriate copyright  	*
 *   legends may be placed on the derivative work in addition   *
 *   to that set forth above. 					*
d74 1
a74 1
 * 18-mar-86  -- jaw     br/cvec changed to NOT use registers.
d83 1
a83 1
 * 	less than 1 second in their's. Also turned the printf into an
d87 1
a87 1
 * 		Add 43bsd alpha tape changes for subnet routing
d96 2
a97 2
 *      Fixed the broadcast loopback code to handle Clusters without
 *      wedging the system.
d113 1
a113 1
 *	Reworked driver to use q-bus mapping routines.  MicroVAX-I now does
d140 1
d142 3
d181 13
a195 1
#include <machine/mtpr.h>
d202 2
a203 2
#define NRCV	15	 		/* Receive descriptors		*/
#define NXMT	5	 		/* Transmit descriptors		*/
d206 1
a206 1
#define	QETIMEOUT	2		/* transmit timeout, must be > 1 */
d224 2
a225 2
	struct	ifrw qe_ifr[NRCV]; /*	for receive buffers;	*/
	struct	ifxmt qe_ifw[NXMT]; /*	for xmit buffers;	*/
d235 2
a236 2
	struct	qe_ring rring[NRCV+1]; /* Receive ring descriptors */
	struct	qe_ring tring[NXMT+1]; /* Xmit ring descriptors */
d242 1
a242 1
	struct	qedevice *addr; /* device addr		*/
d244 1
d273 1
a273 1
#define	QEUNIT(x)	minor(x)
d293 2
a294 2
	struct	qe_ring	*rp;
	struct	qe_ring *prp;   /* physical rp          */
d390 2
a391 1
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_NOTRAILERS;
d416 4
d469 1
a469 1
		sc->setupaddr =	UBAI_ADDR(i);
d566 1
a566 1
			len = 128;
d575 4
d798 1
d835 22
d892 1
a892 1
	caddr_t addr; 			/* mapped address */
d932 5
d943 37
d993 1
d995 1
a995 1
    	struct mbuf *m;
d1020 1
a1020 1
); }
d1022 33
@


1.7
log
@sync with NetBSD 970112 -moj
@
text
@d1 1
@


1.6
log
@sync to 0611
@
text
@d1 1
a1 1
/*	$NetBSD: if_qe.c,v 1.15 1996/05/19 16:43:07 ragge Exp $ */
d293 2
a294 2
	    uballoc(0, (caddr_t)sc->setup_pkt, sizeof(sc->setup_pkt), 0);
	sc->rringaddr = (struct qe_ring *) uballoc(0, (caddr_t)sc->rring,
d341 2
a342 2
	ubarelse(0, &sc->setupaddr);
	ubarelse(0, (int *)&sc->rringaddr);
d421 1
d436 1
a436 1
		i = uballoc(0, (caddr_t)sc->rring,
d442 1
a442 1
		i = uballoc(0, (caddr_t)sc->setup_pkt,
d450 1
a450 1
		if (if_ubaminit(&sc->qe_uba, sc->qe_dev.dv_parent->dv_unit,
@


1.5
log
@add general ether_ioctl call in net/if_ethersubr.c,
NS,IPX,X.25 special processing is now handled in there.
reflect this amazing addition in all the ether ifaces.
ppl, pls check the stuff.
@
text
@d1 1
a1 1
/*	$NetBSD: if_qe.c,v 1.13 1996/03/18 16:47:25 ragge Exp $ */
d165 5
a193 2
void qetimeout(int);

d234 1
a234 1
void	qeinit __P((int));
d240 1
a240 1
void	qe_setaddr __P((u_char *, int));
d244 1
a244 1
void	qetimeout __P((int));
d366 2
a367 2
	ifp->if_unit = sc->qe_dev.dv_unit;
	ifp->if_name = "qe";
d410 1
a410 1
	qeinit(unit);
d417 2
a418 2
qeinit(unit)
	int unit;
a419 1
	struct qe_softc *sc = (struct qe_softc *)qe_cd.cd_devs[unit];
d453 2
a454 1
			printf("qe%d: can't allocate uba resources\n", unit);
d516 1
a516 1
	register struct qe_softc *sc = qe_cd.cd_devs[ifp->if_unit];
d768 1
a768 1
	struct qe_softc *sc = qe_cd.cd_devs[ifp->if_unit];
a771 5
	if ((error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data)) > 0) {
		splx(s);
		return error;
	}

d776 1
a776 1
		qeinit(ifp->if_unit);
d783 12
d820 1
a820 1
qe_setaddr(physaddr, unit)
d822 1
a822 1
	int unit;
a823 1
	register struct qe_softc *sc = qe_cd.cd_devs[unit];
d831 1
a831 1
	qeinit(unit);
d939 2
a940 2
qetimeout(unit)
	int unit;
d942 1
a942 1
	register struct qe_softc *sc;
a943 1
	sc = qe_cd.cd_devs[unit];
d945 2
a946 2
	log(LOG_ERR, "qe%d: transmit timeout, restarted %d\n",
	     unit, sc->qe_restarts++);
@


1.4
log
@sync w/ 0430
@
text
@a164 5
#ifdef NS
#include <netns/ns.h>
#include <netns/ns_if.h>
#endif

d769 5
a783 12
#endif
#ifdef NS
		case AF_NS:
		    {
			register struct ns_addr *ina = &(IA_SNS(ifa)->sns_addr);

			if (ns_nullhost(*ina))
				ina->x_host = *(union ns_host *)(sc->qe_addr);
			else
				qe_setaddr(ina->x_host.c_host, ifp->if_unit);
			break;
		    }
@


1.3
log
@from netbsd:
Change splimp -> splnet in Ethernet, ARCnet, and FDDI drivers.
@
text
@d1 1
a1 1
/*	$NetBSD: if_qe.c,v 1.8 1995/12/24 02:30:55 mycroft Exp $ */
d140 16
a155 16
#include "sys/param.h"
#include "sys/systm.h"
#include "sys/mbuf.h"
#include "sys/buf.h"
#include "sys/protosw.h"
#include "sys/socket.h"
#include "sys/ioctl.h"
#include "sys/errno.h"
#include "sys/syslog.h"
#include "sys/device.h"
#include "sys/time.h"
#include "sys/kernel.h"

#include "net/if.h"
#include "net/netisr.h"
#include "net/route.h"
d158 5
a162 5
#include "netinet/in.h"
#include "netinet/in_systm.h"
#include "netinet/in_var.h"
#include "netinet/ip.h"
#include "netinet/if_ether.h"
d166 2
a167 2
#include "netns/ns.h"
#include "netns/ns_if.h"
d171 2
a172 2
#include "netiso/iso.h"
#include "netiso/iso_var.h"
d176 8
a183 7
#include "machine/pte.h"
#include "machine/cpu.h"
#include "machine/mtpr.h"
#include "if_qereg.h"
#include "if_uba.h"
#include "vax/uba/ubareg.h"
#include "vax/uba/ubavar.h"
d204 1
a204 1
	struct	device qe_device;	/* Configuration common part	*/
d235 1
a235 1
int	qereset __P((int));
d249 3
a251 2
struct	cfdriver qecd =
	{ 0, "qe", qematch, qeattach, DV_IFNET, sizeof(struct qe_softc) };
d253 3
a346 1
	ua->ua_iarg = sc->qe_device.dv_unit;
d368 1
a368 1
	ifp->if_unit = sc->qe_device.dv_unit;
d383 1
a383 1
	printf("qe%d: %s, hardware address %s\n", sc->qe_device.dv_unit,
a394 1
	ifp->if_reset = qereset;
a402 1
 * If interface is on specified uba, reset its state.
d404 1
d408 1
a408 1
	register struct uba_device *ui;
d410 2
a411 7
	panic("qereset");
#ifdef notyet
	if (unit >= NQE || (ui = qeinfo[unit]) == 0 || ui->ui_alive == 0 ||
		ui->ui_ubanum != uban)
		return;
	printf(" qe%d", unit);
	qe_softc[unit].qe_if.if_flags &= ~IFF_RUNNING;
a412 1
#endif
d422 1
a422 1
	struct qe_softc *sc = (struct qe_softc *)qecd.cd_devs[unit];
d452 1
a452 1
		if (if_ubaminit(&sc->qe_uba, sc->qe_device.dv_parent->dv_unit,
d518 1
a518 2
	int unit =  ifp->if_unit;
	register struct qe_softc *sc = qecd.cd_devs[ifp->if_unit];
d602 1
a602 1
	sc = qecd.cd_devs[unit];
d632 1
a632 1
	register struct qe_softc *sc = qecd.cd_devs[unit];
d688 1
a688 1
	register struct qe_softc *sc = qecd.cd_devs[unit];
d770 1
a770 1
	struct qe_softc *sc = qecd.cd_devs[ifp->if_unit];
d826 1
a826 1
	register struct qe_softc *sc = qecd.cd_devs[unit];
a904 2
	int s;
	struct ifqueue *inq;
d947 1
a947 1
	sc = qecd.cd_devs[unit];
@


1.2
log
@update from netbsd (verbatim)
@
text
@d1 1
a1 1
/*	$NetBSD: if_qe.c,v 1.7 1995/12/01 19:37:59 ragge Exp $ */
d498 1
a498 1
	s = splimp();
d530 1
a530 1
	s = splimp();
d776 1
a776 1
	int s = splimp(), error = 0;
@


1.1
log
@Initial revision
@
text
@d1 2
a2 1
/*	$NetBSD: if_qe.c,v 1.4.2.1 1995/10/15 13:56:26 ragge Exp $ */
a136 2
#include "qe.h"
#if	NQE > 0
d176 3
a178 3
#include "vax/include/pte.h"
#include "vax/include/cpu.h"
#include "vax/include/mtpr.h"
a183 1
#if NQE == 1 && !defined(QNIVERT)
a184 3
#else
#define NRCV	10	 		/* Receive descriptors		*/
#endif
d203 1
d208 3
a210 2
	volatile struct	ifrw qe_ifr[NRCV]; /*	for receive buffers;	*/
	volatile struct	ifxmt qe_ifw[NXMT]; /*	for xmit buffers;	*/
d219 2
a220 2
	volatile struct	qe_ring rring[NRCV+1]; /* Receive ring descriptors */
	volatile struct qe_ring tring[NXMT+1]; /* Xmit ring descriptors */
d226 1
a226 1
	volatile struct	qedevice *addr; /* device addr		*/
d230 1
a230 3
} qe_softc[NQE];

struct	uba_device *qeinfo[NQE];
d232 15
a246 1
extern struct timeval time;
d248 2
a249 3
int	qeprobe(), qeattach(), qeintr();
int	qeinit(), qeioctl(), qereset();
void	qestart();
a250 3
u_short qestd[] = { 0 };
struct	uba_driver qedriver =
	{ qeprobe, 0, qeattach, 0, qestd, "qe", qeinfo };
d260 1
d264 12
a275 15
qeprobe(reg, ui)
	caddr_t reg;
	struct uba_device *ui;
{
	/* register int br, cvec;		r11, r10 value-result */
	register volatile struct qedevice *addr = (struct qedevice *)reg;
	register volatile struct qe_ring *rp;
	register struct qe_ring *prp; 	/* physical rp 		*/
	register int i;
	register volatile struct qe_softc *sc = &qe_softc[ui->ui_unit];

#ifdef lint
	br = 0; cvec = br; br = cvec;
	qeintr(0);
#endif
d281 1
d284 1
a284 1
	addr->qe_vector = (uba_hd[numuba].uh_lastiv -= 4);
d290 1
a290 1
		uballoc(0, (caddr_t)sc->setup_pkt, sizeof(sc->setup_pkt), 0);
d292 1
a292 1
		sizeof(struct qe_ring) * (NTOT+2), 0);
d321 1
a321 1
	for( i = 0 ; i < 6 ; i++ )
d324 1
a324 1
	qesetup( sc );
d341 3
a343 1
	return( sizeof(struct qedevice) );
d351 9
a359 6
qeattach(ui)
	struct uba_device *ui;
{
	struct qe_softc *sc = &qe_softc[ui->ui_unit];
	struct ifnet *ifp = (struct ifnet *)&sc->qe_if;
	volatile struct qedevice *addr=(struct qedevice *)ui->ui_addr;
d362 3
a364 1
	ifp->if_unit = ui->ui_unit;
d375 3
a377 2
	for( i=0 ; i<6 ; i++ )
		sc->setup_pkt[i][1] = sc->qe_addr[i] = addr->qe_sta_addr[i] & 0xff;
d379 1
a379 1
	printf("qe%d: %s, hardware address %s\n", ui->ui_unit,
d402 2
a403 2
qereset(unit, uban)
	int unit, uban;
d407 2
d415 1
d421 1
d425 2
a426 3
	volatile struct qe_softc *sc = &qe_softc[unit];
	struct uba_device *ui = qeinfo[unit];
	volatile struct qedevice *addr=(struct qedevice *)ui->ui_addr;
d455 1
a455 1
		if (if_ubaminit(&sc->qe_uba, ui->ui_ubanum,
d522 3
a524 4
	struct uba_device *ui = qeinfo[unit];
	register volatile struct qe_softc *sc = &qe_softc[unit];
	register volatile struct qedevice *addr;
	register volatile struct qe_ring *rp;
a530 1
	addr = (struct qedevice *)ui->ui_addr;
d553 1
a553 1
			if( m == 0 ){
d598 3
a600 1
qeintr(uba, vector, level, unit)
d602 2
a603 3
	register volatile struct qe_softc *sc = &qe_softc[unit];
	volatile struct qedevice *addr =
		(struct qedevice *)qeinfo[unit]->ui_addr;
d606 2
d612 7
a618 6
	addr->qe_csr = QE_RCV_ENABLE | QE_INT_ENABLE | QE_XMIT_INT | QE_RCV_INT | QE_ILOOP;
	if( csr & QE_RCV_INT )
		qerint( unit );
	if( csr & QE_XMIT_INT )
		qetint( unit );
	if( csr & QE_NEX_MEM_INT )
d621 2
a622 1
	if( addr->qe_csr & QE_RL_INVALID && sc->rring[sc->rindex].qe_status1 == QE_NOTYET ) {
d632 1
a632 1

d636 3
a638 3
	register volatile struct qe_softc *sc = &qe_softc[unit];
	register volatile struct qe_ring *rp;
	register volatile struct ifxmt *ifxp;
d643 2
a644 1
	while( sc->otindex != sc->tindex && sc->tring[sc->otindex].qe_status1 != QE_NOTYET && sc->nxmit > 0 ) {
d679 1
a679 1
	qestart( &sc->qe_if );
d688 1
d692 2
a693 2
	register volatile struct qe_softc *sc = &qe_softc[unit];
	register volatile struct qe_ring *rp;
d731 2
a732 1
	for( ; sc->rring[sc->rindex].qe_status1 != QE_NOTYET ; sc->rindex = ++sc->rindex % NRCV ){
d749 1
a749 1
			if( !(status1 & QE_ESETUP) )
d768 1
d771 1
a771 1
	int cmd;
d774 1
a774 1
	struct qe_softc *sc = &qe_softc[ifp->if_unit];
d807 1
a807 2
			((volatile struct qedevice *)
			   (qeinfo[ifp->if_unit]->ui_addr))->qe_csr = QE_RESET;
d825 1
d830 1
a830 1
	register volatile struct qe_softc *sc = &qe_softc[unit];
d845 1
d847 1
a847 1
	register volatile struct qe_ring *rp;
d856 1
a856 1
	if( len ) {
d866 3
a868 2
qesetup( sc )
	volatile struct qe_softc *sc;
d875 2
a876 2
	 for ( j = 0; j < 6 ; j++ )
		for ( i = 2 ; i < 8 ; i++ )
d885 1
a885 1
	for ( i = 0; i < 6 ; i++ ) {
d901 1
d903 2
a904 2
	register volatile struct qe_softc *sc;
	volatile struct ifrw *ifrw;
d951 1
a951 1
	register volatile struct qe_softc *sc;
d953 1
a953 1
	sc = &qe_softc[unit];
d963 1
d965 1
a965 1
	register volatile struct qe_softc *sc;
d968 2
a969 2
	register volatile struct qedevice *addr = sc->addr;
	register volatile struct qe_ring *rp;
d974 1
a974 1
	qesetup( sc );
a986 15

qe_match(){
	printf("qe_match\n");
	return 0;
}

void
qe_attach(){
	printf("qe_attach\n");
}

struct  cfdriver qecd =
        { 0,"qe",qe_match, qe_attach, DV_IFNET, sizeof(struct uba_driver) };

#endif
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
