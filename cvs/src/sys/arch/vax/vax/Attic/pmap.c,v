head	1.78;
access;
symbols
	OPENBSD_5_9:1.77.0.2
	OPENBSD_5_9_BASE:1.77
	OPENBSD_5_8:1.77.0.4
	OPENBSD_5_8_BASE:1.77
	OPENBSD_5_7:1.75.0.2
	OPENBSD_5_7_BASE:1.75
	OPENBSD_5_6:1.69.0.4
	OPENBSD_5_6_BASE:1.69
	OPENBSD_5_5:1.63.0.4
	OPENBSD_5_5_BASE:1.63
	OPENBSD_5_4:1.59.0.2
	OPENBSD_5_4_BASE:1.59
	OPENBSD_5_3:1.55.0.2
	OPENBSD_5_3_BASE:1.55
	OPENBSD_5_2:1.54.0.2
	OPENBSD_5_2_BASE:1.54
	OPENBSD_5_1_BASE:1.53
	OPENBSD_5_1:1.53.0.4
	OPENBSD_5_0:1.53.0.2
	OPENBSD_5_0_BASE:1.53
	OPENBSD_4_9:1.50.0.10
	OPENBSD_4_9_BASE:1.50
	OPENBSD_4_8:1.50.0.8
	OPENBSD_4_8_BASE:1.50
	OPENBSD_4_7:1.50.0.4
	OPENBSD_4_7_BASE:1.50
	OPENBSD_4_6:1.50.0.6
	OPENBSD_4_6_BASE:1.50
	OPENBSD_4_5:1.50.0.2
	OPENBSD_4_5_BASE:1.50
	OPENBSD_4_4:1.44.0.2
	OPENBSD_4_4_BASE:1.44
	OPENBSD_4_3:1.41.0.2
	OPENBSD_4_3_BASE:1.41
	OPENBSD_4_2:1.40.0.2
	OPENBSD_4_2_BASE:1.40
	OPENBSD_4_1:1.38.0.4
	OPENBSD_4_1_BASE:1.38
	OPENBSD_4_0:1.38.0.2
	OPENBSD_4_0_BASE:1.38
	OPENBSD_3_9:1.37.0.2
	OPENBSD_3_9_BASE:1.37
	OPENBSD_3_8:1.36.0.2
	OPENBSD_3_8_BASE:1.36
	OPENBSD_3_7:1.33.0.6
	OPENBSD_3_7_BASE:1.33
	OPENBSD_3_6:1.33.0.4
	OPENBSD_3_6_BASE:1.33
	SMP_SYNC_A:1.33
	SMP_SYNC_B:1.33
	OPENBSD_3_5:1.33.0.2
	OPENBSD_3_5_BASE:1.33
	OPENBSD_3_4:1.32.0.6
	OPENBSD_3_4_BASE:1.32
	UBC_SYNC_A:1.32
	OPENBSD_3_3:1.32.0.4
	OPENBSD_3_3_BASE:1.32
	OPENBSD_3_2:1.32.0.2
	OPENBSD_3_2_BASE:1.32
	OPENBSD_3_1:1.30.0.2
	OPENBSD_3_1_BASE:1.30
	UBC_SYNC_B:1.32
	UBC:1.28.0.2
	UBC_BASE:1.28
	OPENBSD_3_0:1.22.0.2
	OPENBSD_3_0_BASE:1.22
	OPENBSD_2_9_BASE:1.13
	OPENBSD_2_9:1.13.0.2
	OPENBSD_2_8:1.12.0.2
	OPENBSD_2_8_BASE:1.12
	OPENBSD_2_7:1.11.0.2
	OPENBSD_2_7_BASE:1.11
	SMP:1.10.0.12
	SMP_BASE:1.10
	kame_19991208:1.10
	OPENBSD_2_6:1.10.0.10
	OPENBSD_2_6_BASE:1.10
	OPENBSD_2_5:1.10.0.8
	OPENBSD_2_5_BASE:1.10
	OPENBSD_2_4:1.10.0.6
	OPENBSD_2_4_BASE:1.10
	OPENBSD_2_3:1.10.0.4
	OPENBSD_2_3_BASE:1.10
	OPENBSD_2_2:1.10.0.2
	OPENBSD_2_2_BASE:1.10
	OPENBSD_2_1:1.7.0.2
	OPENBSD_2_1_BASE:1.7
	OPENBSD_2_0:1.6.0.2
	OPENBSD_2_0_BASE:1.6
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.78
date	2016.03.09.16.28.49;	author deraadt;	state dead;
branches;
next	1.77;
commitid	OSDG2O3Cgeifnf1W;

1.77
date	2015.06.17.17.15.07;	author miod;	state Exp;
branches;
next	1.76;
commitid	zQTsUt2zkKXXgil0;

1.76
date	2015.05.07.01.55.43;	author jsg;	state Exp;
branches;
next	1.75;
commitid	KhO2CJgSFKm4Q3Hj;

1.75
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.74;
commitid	eahBabNpxnDWKzqJ;

1.74
date	2014.12.17.06.58.10;	author guenther;	state Exp;
branches;
next	1.73;
commitid	DImukoCWyTxwdbuh;

1.73
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.72;
commitid	ZxaujiOM0aYQRjFY;

1.72
date	2014.11.25.20.31.41;	author miod;	state Exp;
branches;
next	1.71;
commitid	T1SUB2fO0TXcVxjP;

1.71
date	2014.11.17.21.39.19;	author deraadt;	state Exp;
branches;
next	1.70;
commitid	Z2pD8pwnvrAXjZK2;

1.70
date	2014.11.16.12.30.59;	author deraadt;	state Exp;
branches;
next	1.69;
commitid	yv0ECmCdICvq576h;

1.69
date	2014.05.24.20.13.52;	author guenther;	state Exp;
branches;
next	1.68;

1.68
date	2014.05.17.23.21.36;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2014.05.17.21.32.19;	author miod;	state Exp;
branches;
next	1.66;

1.66
date	2014.05.17.21.21.17;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2014.05.15.03.52.25;	author guenther;	state Exp;
branches;
next	1.64;

1.64
date	2014.05.08.19.06.07;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2014.02.09.20.58.49;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2013.11.24.22.08.25;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2013.11.20.23.57.07;	author miod;	state Exp;
branches;
next	1.60;

1.60
date	2013.09.21.10.04.42;	author miod;	state Exp;
branches;
next	1.59;

1.59
date	2013.07.07.18.59.36;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2013.07.03.19.21.55;	author miod;	state Exp;
branches;
next	1.57;

1.57
date	2013.06.29.13.00.35;	author miod;	state Exp;
branches;
next	1.56;

1.56
date	2013.06.09.15.47.33;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2012.11.25.22.13.46;	author jsg;	state Exp;
branches;
next	1.54;

1.54
date	2012.04.10.15.50.52;	author guenther;	state Exp;
branches;
next	1.53;

1.53
date	2011.07.06.18.33.00;	author miod;	state Exp;
branches;
next	1.52;

1.52
date	2011.05.30.22.25.23;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2011.04.26.23.50.21;	author ariane;	state Exp;
branches;
next	1.50;

1.50
date	2008.09.30.20.00.29;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2008.08.30.20.45.31;	author martin;	state Exp;
branches;
next	1.48;

1.48
date	2008.08.30.18.58.24;	author martin;	state Exp;
branches;
next	1.47;

1.47
date	2008.08.18.23.19.29;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2008.08.15.22.38.23;	author miod;	state Exp;
branches;
next	1.45;

1.45
date	2008.08.14.11.41.30;	author martin;	state Exp;
branches;
next	1.44;

1.44
date	2008.06.14.10.55.20;	author mk;	state Exp;
branches;
next	1.43;

1.43
date	2008.06.09.20.31.49;	author miod;	state Exp;
branches;
next	1.42;

1.42
date	2008.03.30.18.25.13;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2007.05.20.14.14.12;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2007.04.22.10.05.52;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2006.07.31.05.44.23;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2005.11.24.04.52.23;	author brad;	state Exp;
branches;
next	1.36;

1.36
date	2005.06.30.21.53.13;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	2005.06.29.06.07.07;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2005.04.04.23.40.05;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2003.11.10.21.05.06;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2002.09.17.13.36.23;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2002.06.11.09.36.24;	author hugh;	state Exp;
branches;
next	1.30;

1.30
date	2002.03.14.01.26.49;	author millert;	state Exp;
branches;
next	1.29;

1.29
date	2001.12.22.12.01.53;	author hugh;	state Exp;
branches;
next	1.28;

1.28
date	2001.12.08.02.24.07;	author art;	state Exp;
branches
	1.28.2.1;
next	1.27;

1.27
date	2001.11.28.14.20.16;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.28.14.13.07;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.17.05.07.55;	author hugh;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.06.02.49.23;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.09.11.20.05.25;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2001.08.21.14.06.25;	author hugh;	state Exp;
branches;
next	1.20;

1.20
date	2001.08.11.01.56.18;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.07.05.10.00.40;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.06.08.08.09.32;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.05.16.17.29.39;	author hugh;	state Exp;
branches;
next	1.15;

1.15
date	2001.05.09.15.31.28;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.05.05.21.26.41;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.04.10.06.59.13;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	2000.10.11.11.40.17;	author bjc;	state Exp;
branches;
next	1.11;

1.11
date	2000.04.27.01.10.13;	author bjc;	state Exp;
branches;
next	1.10;

1.10
date	97.10.02.19.51.50;	author niklas;	state Exp;
branches
	1.10.12.1;
next	1.9;

1.9
date	97.09.12.09.30.56;	author maja;	state Exp;
branches;
next	1.8;

1.8
date	97.09.10.12.04.51;	author maja;	state Exp;
branches;
next	1.7;

1.7
date	97.01.15.23.25.20;	author maja;	state Exp;
branches;
next	1.6;

1.6
date	96.07.31.08.56.31;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	96.06.12.08.20.39;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	96.05.03.09.10.22;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	95.12.14.14.00.12;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.12.14.05.27.45;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.10;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.10;	author deraadt;	state Exp;
branches;
next	;

1.10.12.1
date	2001.05.14.21.39.06;	author niklas;	state Exp;
branches;
next	1.10.12.2;

1.10.12.2
date	2001.07.04.10.24.41;	author niklas;	state Exp;
branches;
next	1.10.12.3;

1.10.12.3
date	2001.10.31.03.08.01;	author nate;	state Exp;
branches;
next	1.10.12.4;

1.10.12.4
date	2001.11.13.21.04.18;	author niklas;	state Exp;
branches;
next	1.10.12.5;

1.10.12.5
date	2002.03.06.02.04.48;	author niklas;	state Exp;
branches;
next	1.10.12.6;

1.10.12.6
date	2002.03.28.11.26.47;	author niklas;	state Exp;
branches;
next	1.10.12.7;

1.10.12.7
date	2002.03.30.08.27.12;	author niklas;	state Exp;
branches;
next	1.10.12.8;

1.10.12.8
date	2003.03.27.23.52.20;	author niklas;	state Exp;
branches;
next	1.10.12.9;

1.10.12.9
date	2004.02.19.10.50.03;	author niklas;	state Exp;
branches;
next	;

1.28.2.1
date	2002.01.31.22.55.27;	author niklas;	state Exp;
branches;
next	1.28.2.2;

1.28.2.2
date	2002.06.11.03.39.19;	author art;	state Exp;
branches;
next	1.28.2.3;

1.28.2.3
date	2002.10.29.00.28.14;	author art;	state Exp;
branches;
next	;


desc
@@


1.78
log
@We are done providing support for the vax.
lots of agreement.
@
text
@/*	$OpenBSD: pmap.c,v 1.77 2015/06/17 17:15:07 miod Exp $ */
/*	$NetBSD: pmap.c,v 1.74 1999/11/13 21:32:25 matt Exp $	   */
/*
 * Copyright (c) 1994, 1998, 1999, 2003 Ludd, University of Lule}, Sweden.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *     This product includes software developed at Ludd, University of Lule}.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#include <sys/types.h>
#include <sys/param.h>
#include <sys/queue.h>
#include <sys/malloc.h>
#include <sys/extent.h>
#include <sys/proc.h>
#include <sys/user.h>
#include <sys/systm.h>
#include <sys/device.h>
#include <sys/msgbuf.h>
#include <sys/pool.h>

#ifdef PMAPDEBUG
#include <dev/cons.h>
#endif

#include <uvm/uvm.h>

#include <machine/pte.h>
#include <machine/pcb.h>
#include <machine/mtpr.h>
#include <machine/macros.h>
#include <machine/sid.h>
#include <machine/cpu.h>
#include <machine/scb.h>
#include <machine/rpb.h>

#define ISTACK_SIZE (NBPG * 2)
vaddr_t	istack;

struct pmap kernel_pmap_store;

pt_entry_t *Sysmap;		/* System page table */
u_int	sysptsize;

/*
 * Scratch pages usage:
 * Page 1: initial frame pointer during autoconfig. Stack and pcb for
 *	   processes during exit on boot CPU only.
 * Page 2: unused
 * Page 3: unused
 * Page 4: unused
 */
vaddr_t scratch;
#define	SCRATCHPAGES	4

vaddr_t	iospace;

vaddr_t ptemapstart, ptemapend;
struct	extent *ptemap;
#define	PTMAPSZ	EXTENT_FIXED_STORAGE_SIZE(100)
char	ptmapstorage[PTMAPSZ];

struct pool pmap_pmap_pool;
struct pool pmap_ptp_pool;
struct pool pmap_pv_pool;

#define	NPTEPG		0x80	/* # of PTEs per page (logical or physical) */
#define	PPTESZ		sizeof(pt_entry_t)
#define	NPTEPERREG	0x200000

#define	SEGTYPE(x)	(((vaddr_t)(x)) >> 30)
#define	P0SEG		0
#define	P1SEG		1
#define	SYSSEG		2

#define USRPTSIZE	((MAXTSIZ + MAXDSIZ + BRKSIZ + MAXSSIZ) / VAX_NBPG)
#define	NPTEPGS		(USRPTSIZE / (NBPG / (sizeof(pt_entry_t) * LTOHPN)))

/* Mapping macros used when allocating SPT */
#define MAPVIRT(ptr, count)						\
do {									\
	ptr = virtual_avail;						\
	virtual_avail += (count) * VAX_NBPG;				\
} while (0)

#ifdef PMAPDEBUG
volatile int recurse;
#define RECURSESTART							\
do {									\
	if (recurse)							\
		printf("enter at %d, previous %d\n", __LINE__, recurse);\
	recurse = __LINE__;						\
} while (0)
#define RECURSEEND							\
do {									\
	recurse = 0;							\
} while (0)
int	startpmapdebug = 0;
#define PMDEBUG(x) if (startpmapdebug) printf x
#else
#define RECURSESTART
#define RECURSEEND
#define	PMDEBUG(x)
#endif

vsize_t	calc_kvmsize(vsize_t);
u_long	pmap_extwrap(vsize_t);
void	rmpage(struct pmap *, pt_entry_t *);
void	update_pcbs(struct pmap *);
void	rmspace(struct pmap *);
int	pmap_rmproc(struct pmap *);
vaddr_t	pmap_getusrptes(struct pmap *, vsize_t, int);
void	rmptep(pt_entry_t *);
boolean_t grow_p0(struct pmap *, u_long, int);
boolean_t grow_p1(struct pmap *, u_long, int);
pt_entry_t *vaddrtopte(const struct pv_entry *pv);
void	pmap_remove_pcb(struct pmap *, struct pcb *);

/*
 * Map in a virtual page.
 */
static inline void
mapin8(pt_entry_t *ptep, pt_entry_t pte)
{
	ptep[0] = pte;
	ptep[1] = pte + 1;
	ptep[2] = pte + 2;
	ptep[3] = pte + 3;
	ptep[4] = pte + 4;
	ptep[5] = pte + 5;
	ptep[6] = pte + 6;
	ptep[7] = pte + 7;
}

/*
 * Check if page table page is in use.
 */
static inline int
ptpinuse(pt_entry_t *pte)
{
	pt_entry_t *pve = (pt_entry_t *)vax_trunc_page(pte);
	uint i;

	for (i = 0; i < NPTEPG; i += LTOHPN)
		if (pve[i] != PG_NV)
			return 1;
	return 0;
}

vaddr_t   avail_start, avail_end;
vaddr_t   virtual_avail, virtual_end; /* Available virtual memory */

#define	get_pventry()    (struct pv_entry *)pool_get(&pmap_pv_pool, PR_NOWAIT)
#define	free_pventry(pv) pool_put(&pmap_pv_pool, (void *)pv)

static inline
paddr_t
get_ptp(boolean_t waitok)
{
	pt_entry_t *ptp;

	ptp = (pt_entry_t *)pool_get(&pmap_ptp_pool,
	    PR_ZERO | (waitok ? PR_WAITOK : PR_NOWAIT));
	if (ptp == NULL)
		return 0;
	return ((paddr_t)ptp) & ~KERNBASE;
}

#define	free_ptp(pa)	pool_put(&pmap_ptp_pool, (void *)(pa | KERNBASE))

/*
 * Calculation of the System Page Table is somewhat a pain, because it
 * must be in contiguous physical memory and all size calculations must
 * be done before memory management is turned on.
 * Arg is usrptsize in ptes.
 */
vsize_t
calc_kvmsize(vsize_t usrptsize)
{
	vsize_t kvmsize;

	/*
	 * Compute the number of pages kmem_map will have.
	 */
	kmeminit_nkmempages();

	/* All physical memory (reverse mapping struct) */
	kvmsize = avail_end;
	/* User Page table area. This may be large */
	kvmsize += usrptsize * sizeof(pt_entry_t);
	/* Kernel stacks per process */
	kvmsize += USPACE * maxthread;
	/* kernel malloc arena */
	kvmsize += nkmempages * PAGE_SIZE;
	/* IO device register space */
	kvmsize += IOSPSZ * VAX_NBPG;
	/* Pager allocations */
	kvmsize += PAGER_MAP_SIZE;
	/* kernel malloc arena */
	kvmsize += avail_end;

	/* Exec arg space */
	kvmsize += 16 * NCARGS;
#if VAX46 || VAX48 || VAX49 || VAX53 || VAX60
	/* Physmap */
	kvmsize += VM_PHYS_SIZE;
#endif

	return round_page(kvmsize);
}

/*
 * pmap_bootstrap().
 * Called as part of vm bootstrap, allocates internal pmap structures.
 * Assumes that nothing is mapped, and that kernel stack is located
 * immediately after end.
 */
void
pmap_bootstrap()
{
	unsigned int i;
	extern	unsigned int etext, proc0paddr;
	struct pcb *pcb = (struct pcb *)proc0paddr;
	struct pmap *pmap = pmap_kernel();
	vsize_t kvmsize, usrptsize, minusrptsize;

	/* Set logical page size */
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();

	/*
	 * Compute how much page table space a process reaching all its
	 * limits would need. Try to afford four times such this space,
	 * but try and limit ourselves to 5% of the free memory.
	 */
	minusrptsize = (MAXTSIZ + MAXDSIZ + BRKSIZ + MAXSSIZ) / VAX_NBPG;
	usrptsize = 4 * minusrptsize;
	if (vax_btop(usrptsize * PPTESZ) > avail_end / 20)
		usrptsize = (avail_end / (20 * PPTESZ)) * VAX_NBPG;
	if (usrptsize < minusrptsize)
		usrptsize = minusrptsize;

	kvmsize = calc_kvmsize(usrptsize);
	sysptsize = vax_btop(kvmsize);

	/*
	 * Virtual_* and avail_* is used for mapping of system page table.
	 * The need for kernel virtual memory is linear dependent of the
	 * amount of physical memory also, therefore sysptsize is 
	 * a variable here that is changed dependent of the physical
	 * memory size.
	 */
	virtual_avail = avail_end + KERNBASE;
	virtual_end = KERNBASE + sysptsize * VAX_NBPG;
	/* clear SPT before using it */
	memset(Sysmap, 0, sysptsize * sizeof(pt_entry_t));

	/*
	 * The first part of Kernel Virtual memory is the physical
	 * memory mapped in. This makes some mm routines both simpler
	 * and faster, but takes ~0.75% more memory.
	 */
	pmap_map(KERNBASE, 0, avail_end, PROT_READ | PROT_WRITE);

	/* make sure kernel text is read-only */
	for (i = 0; i < ((unsigned)&etext & ~KERNBASE) >> VAX_PGSHIFT; i++)
		Sysmap[i] = (Sysmap[i] & ~PG_PROT) | PG_KR;

	/* Map System Page Table and zero it,  Sysmap already set. */
	mtpr((vaddr_t)Sysmap - KERNBASE, PR_SBR);

	/* Map Interrupt stack and set red zone */
	istack = (vaddr_t)Sysmap + round_page(sysptsize * sizeof(pt_entry_t));
	mtpr(istack + ISTACK_SIZE, PR_ISP);
	*kvtopte(istack) &= ~PG_V;

	/* Some scratch pages */
	scratch = istack + ISTACK_SIZE;

	avail_start = (vaddr_t)(scratch + SCRATCHPAGES * VAX_NBPG) - KERNBASE;

	/* Kernel message buffer */
	avail_end -= round_page(MSGBUFSIZE);
	msgbufp = (void *)(avail_end + KERNBASE);
	msgbufp->msg_magic = MSG_MAGIC-1; 	/* ensure that it will be zeroed */

	/* zero all mapped physical memory from Sysmap to here */
	memset((void *)istack, 0, (avail_start + KERNBASE) - istack);

	/* User page table map. This is big. */
	MAPVIRT(ptemapstart, vax_atop(usrptsize * sizeof(pt_entry_t)));
	ptemapend = virtual_avail;

	MAPVIRT(iospace, IOSPSZ); /* Device iospace mapping area */

	/* Init SCB and set up stray vectors. */
	avail_start = scb_init(avail_start);
	bcopy((caddr_t)proc0paddr + REDZONEADDR, 0, sizeof(struct rpb));

	if (dep_call->cpu_init)
		(*dep_call->cpu_init)();

	avail_start = round_page(avail_start);
	virtual_avail = round_page(virtual_avail);
	virtual_end = trunc_page(virtual_end);


#if 0 /* Breaks cninit() on some machines */
	cninit();
	printf("Sysmap %p, istack %p, scratch %p\n", Sysmap, istack, scratch);
	printf("etext %p\n", &etext);
	printf("SYSPTSIZE %x usrptsize %lx\n",
	    sysptsize, usrptsize * sizeof(pt_entry_t));
	printf("ptemapstart %lx ptemapend %lx\n", ptemapstart, ptemapend);
	printf("avail_start %lx, avail_end %lx\n", avail_start, avail_end);
	printf("virtual_avail %lx,virtual_end %lx\n",
	    virtual_avail, virtual_end);
	printf("startpmapdebug %p\n",&startpmapdebug);
#endif

	/* Init kernel pmap */
	pmap->pm_p1br = (pt_entry_t *)KERNBASE;
	pmap->pm_p0br = (pt_entry_t *)KERNBASE;
	pmap->pm_p1lr = NPTEPERREG;
	pmap->pm_p0lr = 0;
	pmap->pm_stats.wired_count = pmap->pm_stats.resident_count = 0;
	    /* btop(virtual_avail - KERNBASE); */

	pmap->pm_count = 1;

	/* Activate the kernel pmap. */
	pcb->P1BR = pmap->pm_p1br;
	pcb->P0BR = pmap->pm_p0br;
	pcb->P1LR = pmap->pm_p1lr;
	pcb->P0LR = pmap->pm_p0lr | AST_PCB;
	pcb->pcb_pm = pmap;
	pcb->pcb_pmnext = pmap->pm_pcbs;
	pmap->pm_pcbs = pcb;
	mtpr((register_t)pcb->P1BR, PR_P1BR);
	mtpr((register_t)pcb->P0BR, PR_P0BR);
	mtpr(pcb->P1LR, PR_P1LR);
	mtpr(pcb->P0LR, PR_P0LR);

	/* Create the pmap, ptp and pv_entry pools. */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,
	    "pmap_pool", NULL);
	pool_init(&pmap_ptp_pool, VAX_NBPG, 0, 0, 0, "ptp_pool", NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0,
	    "pv_pool", NULL);

	/*
	 * Now everything should be complete, start virtual memory.
	 */
	uvm_page_physload(atop(avail_start), atop(avail_end),
	    atop(avail_start), atop(avail_end), 0);
	mtpr(sysptsize, PR_SLR);
	rpb.sbr = mfpr(PR_SBR);
	rpb.slr = mfpr(PR_SLR);
	mtpr(1, PR_MAPEN);
}

void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = virtual_avail;
	*vendp = virtual_end;
}

/*
 * Let the VM system do early memory allocation from the direct-mapped
 * physical memory instead.
 */
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
{
	vaddr_t v;
	int npgs;

	PMDEBUG(("pmap_steal_memory: size 0x%lx start %p end %p\n",
	    size, vstartp, vendp));

	size = round_page(size);
	npgs = atop(size);

#ifdef DIAGNOSTIC
	if (uvm.page_init_done == TRUE)
		panic("pmap_steal_memory: called _after_ bootstrap");
#endif

	/*
	 * A vax only has one segment of memory.
	 */

	v = (vm_physmem[0].avail_start << PAGE_SHIFT) | KERNBASE;
	vm_physmem[0].avail_start += npgs;
	vm_physmem[0].start += npgs;
	if (vstartp)
		*vstartp = virtual_avail;
	if (vendp)
		*vendp = virtual_end;
	bzero((caddr_t)v, size);
	return v;
}

/*
 * pmap_init() is called as part of vm init after memory management
 * is enabled. It is meant to do machine-specific allocations.
 * The extent for the user page tables is initialized here.
 */
void 
pmap_init() 
{
        /*
         * Create the extent map used to manage the page table space.
         */
        ptemap = extent_create("ptemap", ptemapstart, ptemapend,
            M_VMPMAP, ptmapstorage, PTMAPSZ, EX_NOCOALESCE);
        if (ptemap == NULL)
		panic("pmap_init");
}

u_long
pmap_extwrap(vsize_t nsize)
{
	int res;
	u_long rv;

	for (;;) {
		res = extent_alloc(ptemap, nsize, PAGE_SIZE, 0, 0,
		    EX_WAITOK | EX_MALLOCOK, &rv);
		if (res == 0)
			return rv;
		if (res == EAGAIN)
			return 0;
	}
}

/*
 * Do a page removal from the pv list. A page is identified by its
 * virtual address combined with its struct pmap in the page's pv list.
 */
void
rmpage(struct pmap *pm, pt_entry_t *br)
{
	struct pv_entry *pv, *pl, *pf;
	vaddr_t vaddr;
	struct vm_page *pg;
	int s, found = 0;

	/*
	 * Check that we are working on a managed page.
	 */
	pg = PHYS_TO_VM_PAGE((*br & PG_FRAME) << VAX_PGSHIFT);
	if (pg == NULL)
		return;

	if (pm == pmap_kernel()) {
#ifdef DIAGNOSTIC
		if (br - Sysmap >= sysptsize)
			panic("%s: bogus Sysmap pte pointer %p", __func__, br);
#endif
		vaddr = (br - Sysmap) * VAX_NBPG + 0x80000000;
	} else if (br >= pm->pm_p0br && br < pm->pm_p0br + pm->pm_p0lr)
		vaddr = (br - pm->pm_p0br) * VAX_NBPG;
	else {
		vaddr = (br - pm->pm_p1br) * VAX_NBPG + 0x40000000;
#ifdef DIAGNOSTIC
		if (vaddr < 0x40000000 || vaddr >= 0x80000000)
			panic("%s: bogus pmap %p P1 pte pointer %p", __func__, pm, br);
#endif
	}

	s = splvm();
	for (pl = NULL, pv = pg->mdpage.pv_head; pv != NULL; pl = pv, pv = pf) {
		pf = pv->pv_next;
		if (pv->pv_pmap == pm && pv->pv_va == vaddr) {
			if ((pg->mdpage.pv_attr & (PG_V|PG_M)) != (PG_V|PG_M)) {
				switch (br[0] & PG_PROT) {
				case PG_URKW:
				case PG_KW:
				case PG_RW:
					pg->mdpage.pv_attr |=
					    br[0] | br[1] | br[2] | br[3] |
					    br[4] | br[5] | br[6] | br[7];
					break;
				}
			}
			if (pf != NULL) {
				*pv = *pf;
				free_pventry(pf);
			} else {
				if (pl != NULL)
					pl->pv_next = pv->pv_next;
				else
					pg->mdpage.pv_head = NULL;
				free_pventry(pv);
			}
			found++;
			break;
		}
	}
	splx(s);
	if (found == 0)
		panic("rmpage: pg %p br %p", pg, br);
}

/*
 * Update the PCBs using this pmap after a change.
 */
void
update_pcbs(struct pmap *pm)
{
	struct pcb *pcb;

	PMDEBUG(("update_pcbs pm %p\n", pm));

	for (pcb = pm->pm_pcbs; pcb != NULL; pcb = pcb->pcb_pmnext) {
		KASSERT(pcb->pcb_pm == pm);
		pcb->P0BR = pm->pm_p0br;
		pcb->P0LR = pm->pm_p0lr | AST_PCB;
		pcb->P1BR = pm->pm_p1br;
		pcb->P1LR = pm->pm_p1lr;
	}

	/* If curproc uses this pmap update the regs too */ 
	if (pm == curproc->p_vmspace->vm_map.pmap) {
		PMDEBUG(("update_pcbs: %08x %08x %08x %08x\n",
		    pm->pm_p0br, pm->pm_p0lr, pm->pm_p1br, pm->pm_p1lr));
                mtpr((register_t)pm->pm_p0br, PR_P0BR);
                mtpr(pm->pm_p0lr | AST_PCB, PR_P0LR);
                mtpr((register_t)pm->pm_p1br, PR_P1BR);
                mtpr(pm->pm_p1lr, PR_P1LR);
	}
}

/*
 * Remove a full process space. Update all processes pcbs.
 */
void
rmspace(struct pmap *pm)
{
	u_long lr, i, j;
	pt_entry_t *ptpp, *br;
	int s;

	if (pm->pm_p0lr == 0 && pm->pm_p1lr == NPTEPERREG)
		return; /* Already free */

	lr = pm->pm_p0lr / NPTEPG;
	for (i = 0; i < lr; i++) {
		ptpp = kvtopte((vaddr_t)&pm->pm_p0br[i * NPTEPG]);
		if (*ptpp == PG_NV)
			continue;
		br = &pm->pm_p0br[i * NPTEPG];
		for (j = 0; j < NPTEPG; j += LTOHPN) {
			if (br[j] == 0)
				continue;
			rmpage(pm, &br[j]);
		}
		free_ptp((*ptpp & PG_FRAME) << VAX_PGSHIFT);
		*ptpp = PG_NV;
	}
	lr = pm->pm_p1lr / NPTEPG;
	for (i = lr; i < NPTEPERREG / NPTEPG; i++) {
		ptpp = kvtopte((vaddr_t)&pm->pm_p1br[i * NPTEPG]);
		if (*ptpp == PG_NV)
			continue;
		br = &pm->pm_p1br[i * NPTEPG];
		for (j = 0; j < NPTEPG; j += LTOHPN) {
			if (br[j] == 0)
				continue;
			rmpage(pm, &br[j]);
		}
		free_ptp((*ptpp & PG_FRAME) << VAX_PGSHIFT);
		*ptpp = PG_NV;
	}

	s = splsched();

	if (pm->pm_p0lr != 0)
		extent_free(ptemap, (u_long)pm->pm_p0br,
		    pm->pm_p0lr * PPTESZ, EX_WAITOK);
	if (pm->pm_p1lr != NPTEPERREG)
		extent_free(ptemap, (u_long)pm->pm_p1ap,
		    (NPTEPERREG - pm->pm_p1lr) * PPTESZ, EX_WAITOK);
	pm->pm_p0br = pm->pm_p1br = (pt_entry_t *)KERNBASE;
	pm->pm_p0lr = 0;
	pm->pm_p1lr = NPTEPERREG;
	pm->pm_p1ap = NULL;
	update_pcbs(pm);

	splx(s);
}

/*
 * Find a process to remove the process space for. *sigh*
 * Avoid to remove ourselves. Logic is designed after uvm_swapout_threads().
 */

int
pmap_rmproc(struct pmap *pm)
{
	struct process *pr, *outpr;
	struct pmap *ppm;
	struct proc *p, *slpp;
	int outpri;
	int didswap = 0;
	extern int maxslp;

	outpr = NULL;
	outpri = 0;
	LIST_FOREACH(pr, &allprocess, ps_list) {
		if (pr->ps_flags & (PS_SYSTEM | PS_EXITING))
			continue;
		ppm = pr->ps_vmspace->vm_map.pmap;
		if (ppm == pm)		/* Don't swap ourself */
			continue;
		if (ppm->pm_p0lr == 0 && ppm->pm_p1lr == NPTEPERREG)
			continue;	/* Already swapped */

		/*
		 * slpp: the sleeping or stopped thread in pr with
		 * the smallest p_slptime
		 */
		slpp = NULL;
		TAILQ_FOREACH(p, &pr->ps_threads, p_thr_link) {
			switch (p->p_stat) {
			case SRUN:
			case SONPROC:
				goto next_process;

			case SSLEEP:
			case SSTOP:
				if (slpp == NULL ||
				    slpp->p_slptime < p->p_slptime)
					slpp = p;
				continue;
			}
		}
		if (slpp != NULL) {
			if (slpp->p_slptime >= maxslp) {
				rmspace(ppm);
				didswap++;
			} else if (slpp->p_slptime > outpri) {
				outpr = pr;
				outpri = slpp->p_slptime;
			}
		}
		if (didswap)
			break;
next_process:	;
	}

	if (didswap == 0 && outpr != NULL) {
		rmspace(outpr->ps_vmspace->vm_map.pmap);
		didswap++;
	}
	return didswap;
}

/*
 * Allocate space for user page tables, from ptemap.
 * If the map is full then:
 * 1) Remove processes idle for more than 20 seconds or stopped.
 * 2) Remove processes idle for less than 20 seconds.
 * 
 * Argument is needed space, in bytes.
 * Returns a pointer to the newly allocated space, or zero if space could not
 * be allocated and failure is allowed. Panics otherwise.
 */
vaddr_t
pmap_getusrptes(struct pmap *pm, vsize_t nsize, int canfail)
{
	u_long rv;

#ifdef DEBUG
	if (nsize & PAGE_MASK)
		panic("pmap_getusrptes: bad size %lx", nsize);
#endif
	for (;;) {
		rv = pmap_extwrap(nsize);
		if (rv != 0)
			return rv;
		if (pmap_rmproc(pm) == 0) {
			if (canfail)
				return 0;
			else
				panic("out of space in usrptmap");
		}
	}
}

/*
 * Remove a pte page when all references are gone.
 */
void
rmptep(pt_entry_t *pte)
{
	pt_entry_t *ptpp = kvtopte((vaddr_t)pte);

	PMDEBUG(("rmptep: pte %p -> ptpp %p\n", pte, ptpp));

#ifdef DEBUG
	{
		int i;
		pt_entry_t *ptr = (pt_entry_t *)vax_trunc_page(pte);
		for (i = 0; i < NPTEPG; i++)
			if (ptr[i] != 0)
				panic("rmptep: ptr[%d] != 0", i);
	}
#endif

	free_ptp((*ptpp & PG_FRAME) << VAX_PGSHIFT);
	*ptpp = PG_NV;
}

boolean_t 
grow_p0(struct pmap *pm, u_long reqlen, int canfail)
{
	vaddr_t nptespc;
	pt_entry_t *from, *to;
	size_t srclen, dstlen;
	u_long p0br, p0lr, len;
	int inuse;
	int s;

	PMDEBUG(("grow_p0: pmap %p reqlen %x\n", pm, reqlen));

	/* Get new pte space */
	p0lr = pm->pm_p0lr;
	inuse = p0lr != 0;
	len = round_page((reqlen + 1) * PPTESZ);
	RECURSEEND;
	nptespc = pmap_getusrptes(pm, len, canfail);
	if (nptespc == 0)
		return FALSE;
	RECURSESTART;

	s = splsched();

	/*
	 * Copy the old ptes to the new space.
	 * Done by moving on system page table.
	 */
	srclen = vax_btop(p0lr * PPTESZ) * PPTESZ;
	dstlen = vax_atop(len) * PPTESZ;
	from = kvtopte((vaddr_t)pm->pm_p0br);
	to = kvtopte(nptespc);

	PMDEBUG(("grow_p0: from %p to %p src %x dst %x\n",
	    from, to, srclen, dstlen));

	if (inuse)
		memcpy(to, from, srclen);
	bzero((char *)to + srclen, dstlen - srclen);

	p0br = (u_long)pm->pm_p0br;
	pm->pm_p0br = (pt_entry_t *)nptespc;
	pm->pm_p0lr = len / PPTESZ;
	update_pcbs(pm);

	splx(s);

	if (inuse)
		extent_free(ptemap, p0br, p0lr * PPTESZ, EX_WAITOK);

	return TRUE;
}

boolean_t
grow_p1(struct pmap *pm, u_long len, int canfail)
{
	vaddr_t nptespc, optespc;
	pt_entry_t *from, *to;
	size_t nlen, olen;
	int s;

	PMDEBUG(("grow_p1: pm %p len %x\n", pm, len));

	/* Get new pte space */
	nlen = (NPTEPERREG * PPTESZ) - trunc_page(len * PPTESZ);
	RECURSEEND;
	nptespc = pmap_getusrptes(pm, nlen, canfail);
	if (nptespc == 0)
		return FALSE;
	RECURSESTART;

	s = splsched();

	olen = (NPTEPERREG - pm->pm_p1lr) * PPTESZ;
	optespc = (vaddr_t)pm->pm_p1ap;

	/*
	 * Copy the old ptes to the new space.
	 * Done by moving on system page table.
	 */
	from = kvtopte(optespc);
	to = kvtopte(nptespc);

	PMDEBUG(("grow_p1: from %p to %p src %x dst %x\n",
	    from, to, vax_btop(olen), vax_btop(nlen)));

	bzero(to, vax_btop(nlen - olen) * PPTESZ);
	if (optespc)
		memcpy(kvtopte(nptespc + nlen - olen), from,
		    vax_btop(olen) * PPTESZ);

	pm->pm_p1ap = (pt_entry_t *)nptespc;
	pm->pm_p1br = (pt_entry_t *)(nptespc + nlen - (NPTEPERREG * PPTESZ));
	pm->pm_p1lr = NPTEPERREG - nlen / PPTESZ;
	update_pcbs(pm);

	splx(s);

	if (optespc)
		extent_free(ptemap, optespc, olen, EX_WAITOK);

	return TRUE;
}

/*
 * pmap_create() creates a pmap for a new task.
 */
struct pmap * 
pmap_create()
{
	struct pmap *pmap;

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK | PR_ZERO);

	/*
	 * Do not allocate any pte's here, we don't know the size and
	 * we'll get a page fault anyway when some page is referenced,
	 * so defer until then.
	 */
	pmap->pm_p0br = pmap->pm_p1br = (pt_entry_t *)KERNBASE;
	pmap->pm_p0lr = 0;
	pmap->pm_p1lr = NPTEPERREG;
	pmap->pm_p1ap = NULL;

	PMDEBUG(("pmap_create: pmap %p p0br=%p p0lr=0x%lx p1br=%p p1lr=0x%lx\n",
    	    pmap, pmap->pm_p0br, pmap->pm_p0lr, pmap->pm_p1br, pmap->pm_p1lr));

	pmap->pm_count = 1;
	/* pmap->pm_stats.resident_count = pmap->pm_stats.wired_count = 0; */

	return pmap;
}

void
pmap_remove_holes(struct vmspace *vm)
{
	struct vm_map *map = &vm->vm_map;
	struct pmap *pmap = map->pmap;
	vaddr_t shole, ehole;

	if (pmap == pmap_kernel())	/* can of worms */
		return;

	shole = MAXTSIZ + MAXDSIZ + BRKSIZ;
	ehole = (vaddr_t)vm->vm_maxsaddr;
	shole = max(vm_map_min(map), shole);
	ehole = min(vm_map_max(map), ehole);

	if (ehole <= shole)
		return;

	(void)uvm_map(map, &shole, ehole - shole, NULL, UVM_UNKNOWN_OFFSET, 0,
	    UVM_MAPFLAG(PROT_NONE, PROT_NONE, MAP_INHERIT_SHARE, MADV_RANDOM,
	      UVM_FLAG_NOMERGE | UVM_FLAG_HOLE | UVM_FLAG_FIXED));
}

void
pmap_unwire(struct pmap *pmap, vaddr_t va)
{
	pt_entry_t *pte;
	uint i;

	RECURSESTART;
	if (va & KERNBASE) {
		pte = Sysmap;
		i = vax_btop(va - KERNBASE);
	} else { 
		if (va < 0x40000000)
			pte = pmap->pm_p0br;
		else
			pte = pmap->pm_p1br;
		i = PG_PFNUM(va);
	}

	pte[i] &= ~PG_W;
	RECURSEEND;
	pmap->pm_stats.wired_count--;
}

/*
 * pmap_destroy(pmap): Remove a reference from the pmap. 
 * If this was the last reference, release all its resources.
 */
void
pmap_destroy(struct pmap *pmap)
{
	int count;
#ifdef DEBUG
	vaddr_t saddr, eaddr;
#endif
  
	PMDEBUG(("pmap_destroy: pmap %p\n",pmap));

	count = --pmap->pm_count;
	if (count != 0)
		return;

#ifdef DIAGNOSTIC
	if (pmap->pm_pcbs)
		panic("pmap_destroy used pmap");
#endif

	if (pmap->pm_p0br != 0) {
#ifdef DEBUG
		saddr = (vaddr_t)pmap->pm_p0br;
		eaddr = saddr + pmap->pm_p0lr * PPTESZ;
		for (; saddr < eaddr; saddr += PAGE_SIZE)
			if ((*kvtopte(saddr) & PG_FRAME) != 0)
				panic("pmap_release: P0 page mapped");
		saddr = (vaddr_t)pmap->pm_p1br + pmap->pm_p1lr * PPTESZ;
		eaddr = VM_MAXUSER_ADDRESS;
		for (; saddr < eaddr; saddr += PAGE_SIZE)
			if ((*kvtopte(saddr) & PG_FRAME) != 0)
				panic("pmap_release: P1 page mapped");
#endif
	}

	if (pmap->pm_p0lr != 0)
		extent_free(ptemap, (u_long)pmap->pm_p0br,
		    pmap->pm_p0lr * PPTESZ, EX_WAITOK);
	if (pmap->pm_p1lr != NPTEPERREG)
		extent_free(ptemap, (u_long)pmap->pm_p1ap,
		    (NPTEPERREG - pmap->pm_p1lr) * PPTESZ, EX_WAITOK);

	pool_put(&pmap_pmap_pool, pmap);
}

pt_entry_t *
vaddrtopte(const struct pv_entry *pv)
{
	struct pmap *pm;

	if (pv->pv_va & KERNBASE)
		return &Sysmap[(pv->pv_va & ~KERNBASE) >> VAX_PGSHIFT];
	pm = pv->pv_pmap;
	if (pv->pv_va & 0x40000000)
		return &pm->pm_p1br[vax_btop(pv->pv_va & ~0x40000000)];
	else
		return &pm->pm_p0br[vax_btop(pv->pv_va)];
}

/*
 * New (real nice!) function that allocates memory in kernel space
 * without tracking it in the MD code.
 */
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *ptp, opte;

	ptp = kvtopte(va);

	PMDEBUG(("pmap_kenter_pa: va: %lx, pa %lx, prot %x ptp %p\n",
	    va, pa, prot, ptp));

	opte = ptp[0];
	if ((opte & PG_FRAME) == 0) {
		pmap_kernel()->pm_stats.resident_count++;
		pmap_kernel()->pm_stats.wired_count++;
	}
	mapin8(ptp, PG_V | ((prot & PROT_WRITE) ? PG_KW : PG_KR) |
	    PG_PFNUM(pa) | PG_W | PG_SREF);
	if (opte & PG_V) {
		mtpr(0, PR_TBIA);
	}
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	pt_entry_t *pte;
#ifdef PMAPDEBUG
	int i;
#endif

	PMDEBUG(("pmap_kremove: va: %lx, len %lx, ptp %p\n",
	    va, len, kvtopte(va)));

	pte = kvtopte(va);

#ifdef PMAPDEBUG
	/*
	 * Check if any pages are on the pv list.
	 * This shouldn't happen anymore.
	 */
	len >>= PAGE_SHIFT;
	for (i = 0; i < len; i++) {
		if ((*pte & PG_FRAME) == 0)
			continue;
		pmap_kernel()->pm_stats.resident_count--;
		pmap_kernel()->pm_stats.wired_count--;
		if ((*pte & PG_SREF) == 0)
			panic("pmap_kremove");
		bzero(pte, LTOHPN * sizeof(pt_entry_t));
		pte += LTOHPN;
	}
#else
	len >>= PAGE_SHIFT;
	pmap_kernel()->pm_stats.resident_count -= len;
	pmap_kernel()->pm_stats.wired_count -= len;
	bzero(pte, len * LTOHPN * sizeof(pt_entry_t));
#endif
	mtpr(0, PR_TBIA);
}

/*
 * pmap_enter() is the main routine that puts in mappings for pages, or
 * upgrades mappings to more "rights".
 */
int
pmap_enter(struct pmap *pmap, vaddr_t v, paddr_t p, vm_prot_t prot, int flags)
{
	struct pv_entry *pv;
	struct vm_page *pg;
	pt_entry_t newpte, oldpte;
	pt_entry_t *pteptr;	/* current pte to write mapping info to */
	pt_entry_t *ptpptr;	/* ptr to page table page */
	u_long pteidx;
	int s;

	PMDEBUG(("pmap_enter: pmap %p v %lx p %lx prot %x wired %d flags %x\n",
	    pmap, v, p, prot, (flags & PMAP_WIRED) != 0, flags));

	RECURSESTART;

	/* Find address of correct pte */
	switch (SEGTYPE(v)) {
	case SYSSEG:
		pteptr = Sysmap + vax_btop(v - KERNBASE);
		newpte = prot & PROT_WRITE ? PG_KW : PG_KR;
		break;
	case P0SEG:
		pteidx = vax_btop(v);
		if (pteidx >= pmap->pm_p0lr) {
			if (!grow_p0(pmap, pteidx, flags & PMAP_CANFAIL))
				return ENOMEM;
		}
		pteptr = pmap->pm_p0br + pteidx;
		newpte = prot & PROT_WRITE ? PG_RW : PG_RO;
		break;
	case P1SEG:
		pteidx = vax_btop(v - 0x40000000);
		if (pteidx < pmap->pm_p1lr) {
			if (!grow_p1(pmap, pteidx, flags & PMAP_CANFAIL))
				return ENOMEM;
		}
		pteptr = pmap->pm_p1br + pteidx;
		newpte = prot & PROT_WRITE ? PG_RW : PG_RO;
		break;
	default:
		panic("bad seg");
	}
	newpte |= vax_btop(p);

	if (SEGTYPE(v) != SYSSEG) {
		/*
		 * Check if a pte page must be mapped in.
		 */
		ptpptr = kvtopte((vaddr_t)pteptr);

		if (*ptpptr == PG_NV) {
			paddr_t pa;

			pa = get_ptp((flags & PMAP_CANFAIL) != 0);
			if (pa == 0) {
				RECURSEEND;
				return ENOMEM;
			}
			*ptpptr = PG_V | PG_KW | PG_PFNUM(pa);
		}
	}

	/*
	 * Do not keep track of anything if mapping IO space.
	 */
	pg = PHYS_TO_VM_PAGE(p);
	if (pg == NULL) {
		mapin8(pteptr, newpte);
		RECURSEEND;
		return 0;
	}

	if (flags & PMAP_WIRED)
		newpte |= PG_W;

	oldpte = *pteptr & ~(PG_V | PG_M);

	/* just a wiring change ? */
	if ((newpte ^ oldpte) == PG_W) {
		if (flags & PMAP_WIRED) {
			pmap->pm_stats.wired_count++;
			*pteptr |= PG_W;
		} else {
			pmap->pm_stats.wired_count--;
			*pteptr &= ~PG_W;
		}
		RECURSEEND;
		return 0;
	}

	/* mapping unchanged? just return. */
	if (newpte == oldpte) {
		RECURSEEND;
		return 0;
	}

	/* Changing mapping? */
	if ((newpte & PG_FRAME) == (oldpte & PG_FRAME)) {
		/* protection change. */
#if 0 /* done below */
		mtpr(0, PR_TBIA);
#endif
	} else {
		/*
		 * Mapped before? Remove it then.
		 */
		if (oldpte & PG_FRAME) {
			pmap->pm_stats.resident_count--;
			if (oldpte & PG_W)
				pmap->pm_stats.wired_count--;
			RECURSEEND;
			if ((oldpte & PG_SREF) == 0)
				rmpage(pmap, pteptr);
			else
				panic("pmap_enter on PG_SREF page");
			RECURSESTART;
		}

		s = splvm();
		pv = get_pventry();
		if (pv == NULL) {
			if (flags & PMAP_CANFAIL) {
				splx(s);
				RECURSEEND;
				return ENOMEM;
			}
			panic("pmap_enter: could not allocate pv_entry");
		}
		pv->pv_va = v;
		pv->pv_pmap = pmap;
		pv->pv_next = pg->mdpage.pv_head;
		pg->mdpage.pv_head = pv;
		splx(s);
		pmap->pm_stats.resident_count++;
		if (newpte & PG_W)
			pmap->pm_stats.wired_count++;
	}

	if (flags & PROT_READ) {
		pg->mdpage.pv_attr |= PG_V;
		newpte |= PG_V;
	}
	if (flags & PROT_WRITE)
		pg->mdpage.pv_attr |= PG_M;

	if (flags & PMAP_WIRED)
		newpte |= PG_V; /* Not allowed to be invalid */

	mapin8(pteptr, newpte);
	RECURSEEND;

	mtpr(0, PR_TBIA); /* Always; safety belt */
	return 0;
}

vaddr_t
pmap_map(vaddr_t va, paddr_t pstart, paddr_t pend, int prot)
{
	vaddr_t count;
	pt_entry_t *pentry;

	PMDEBUG(("pmap_map: virt %lx, pstart %lx, pend %lx, Sysmap %p\n",
	    va, pstart, pend, Sysmap));

	pstart &= 0x7fffffffUL;
	pend &= 0x7fffffffUL;
	va &= 0x7fffffffUL;
	pentry = Sysmap + vax_btop(va);
	for (count = pstart; count < pend; count += VAX_NBPG) {
		*pentry++ = vax_btop(count) | PG_V |
		    (prot & PROT_WRITE ? PG_KW : PG_KR);
	}
	return va + (count - pstart) + KERNBASE;
}

boolean_t
pmap_extract(struct pmap *pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *pte;
	ulong sva;

	PMDEBUG(("pmap_extract: pmap %p, va %lx",pmap, va));

	sva = PG_PFNUM(va);
	if (va & KERNBASE) {
		if (sva >= sysptsize || (Sysmap[sva] & PG_V) == 0)
			goto fail;
		*pap = ((Sysmap[sva] & PG_FRAME) << VAX_PGSHIFT) |
		    (va & VAX_PGOFSET);
		PMDEBUG((" -> pa %lx\n", *pap));
		return TRUE;
	}

	if (va < 0x40000000) {
		if (sva >= pmap->pm_p0lr)
			goto fail;
		pte = pmap->pm_p0br;
	} else {
		if (sva < pmap->pm_p1lr)
			goto fail;
		pte = pmap->pm_p1br;
	}
	/*
	 * Since the PTE tables are sparsely allocated, make sure the page
	 * table page actually exists before dereferencing the pte itself.
	 */
	if ((*kvtopte((vaddr_t)&pte[sva]) & PG_V) && (pte[sva] & PG_V)) {
		*pap = ((pte[sva] & PG_FRAME) << VAX_PGSHIFT) |
		    (va & VAX_PGOFSET);
		PMDEBUG((" -> pa %lx\n", *pap));
		return TRUE;
	}
	
fail:
	PMDEBUG((" -> no mapping\n"));
	return FALSE;
}

/*
 * Sets protection for a given region to prot. If prot == none then
 * unmap region. pmap_remove is implemented as pmap_protect with
 * protection none.
 */
void
pmap_protect(struct pmap *pmap, vaddr_t start, vaddr_t end, vm_prot_t prot)
{
	pt_entry_t *pt, *pts, *ptd;
	pt_entry_t pr, lr;

	PMDEBUG(("pmap_protect: pmap %p, start %lx, end %lx, prot %x\n",
	    pmap, start, end,prot));

	RECURSESTART;

	switch (SEGTYPE(start)) {
	case SYSSEG:
		pt = Sysmap;
#ifdef DIAGNOSTIC
		if (PG_PFNUM(end) > mfpr(PR_SLR))
			panic("pmap_protect: outside SLR: %lx", end);
#endif
		start &= ~KERNBASE;
		end &= ~KERNBASE;
		pr = (prot & PROT_WRITE ? PG_KW : PG_KR);
		break;

	case P1SEG:
		if (vax_btop(end - 0x40000000) <= pmap->pm_p1lr) {
			RECURSEEND;
			return;
		}
		if (vax_btop(start - 0x40000000) < pmap->pm_p1lr)
			start = pmap->pm_p1lr * VAX_NBPG;
		pt = pmap->pm_p1br;
		start &= 0x3fffffff;
		end = (end == KERNBASE ? 0x40000000 : end & 0x3fffffff);
		pr = (prot & PROT_WRITE ? PG_RW : PG_RO);
		break;

	case P0SEG:
		lr = pmap->pm_p0lr;

		/* Anything to care about at all? */
		if (vax_btop(start) > lr) {
			RECURSEEND;
			return;
		}
		if (vax_btop(end) > lr)
			end = lr * VAX_NBPG;
		pt = pmap->pm_p0br;
		pr = (prot & PROT_WRITE ? PG_RW : PG_RO);
		break;
	default:
		panic("unsupported segtype: %d", (int)SEGTYPE(start));
	}

	pts = &pt[start >> VAX_PGSHIFT];
	ptd = &pt[end >> VAX_PGSHIFT];
#ifdef DEBUG
	if (((int)pts - (int)pt) & 7)
		panic("pmap_remove: pts not even");
	if (((int)ptd - (int)pt) & 7)
		panic("pmap_remove: ptd not even");
#endif

	while (pts < ptd) {
		if ((*kvtopte((vaddr_t)pts) & PG_FRAME) != 0 && *pts != PG_NV) {
			if (prot == PROT_NONE) {
				pmap->pm_stats.resident_count--;
				if ((*pts & PG_W))
					pmap->pm_stats.wired_count--;
				RECURSEEND;
				if ((*pts & PG_SREF) == 0)
					rmpage(pmap, pts);
				RECURSESTART;
				bzero(pts, sizeof(pt_entry_t) * LTOHPN);
				if (pt != Sysmap) {
					if (ptpinuse(pts) == 0)
						rmptep(pts);
				}
			} else {
				pts[0] = (pts[0] & ~PG_PROT) | pr;
				pts[1] = (pts[1] & ~PG_PROT) | pr;
				pts[2] = (pts[2] & ~PG_PROT) | pr;
				pts[3] = (pts[3] & ~PG_PROT) | pr;
				pts[4] = (pts[4] & ~PG_PROT) | pr;
				pts[5] = (pts[5] & ~PG_PROT) | pr;
				pts[6] = (pts[6] & ~PG_PROT) | pr;
				pts[7] = (pts[7] & ~PG_PROT) | pr;
			}
		}
		pts += LTOHPN;
	}
	RECURSEEND;
	mtpr(0,PR_TBIA);
}

/*
 * Called from interrupt vector routines if we get a page invalid fault.
 * Returns 0 if normal call, 1 if CVAX bug detected.
 */
int pmap_simulref(int, vaddr_t);
int
pmap_simulref(int bits, vaddr_t va)
{
	pt_entry_t *pte;
	struct vm_page *pg;
	paddr_t	pa;

	PMDEBUG(("pmap_simulref: bits %x addr %x\n", bits, va));
#ifdef DEBUG
	if (bits & 1)
		panic("pte trans len");
#endif
	/* Set address to logical page boundary */
	va &= ~PGOFSET;

	if (va & KERNBASE) {
		pte = kvtopte(va);
		pa = (paddr_t)pte & ~KERNBASE;
	} else {
		if (va < 0x40000000)
			pte = (pt_entry_t *)mfpr(PR_P0BR);
		else
			pte = (pt_entry_t *)mfpr(PR_P1BR);
		pte += PG_PFNUM(va);
		if (bits & 2) { /* PTE reference */
			pte = kvtopte(vax_trunc_page(pte));
			if (pte[0] == 0) /* Check for CVAX bug */
				return 1;	
			pa = (paddr_t)pte & ~KERNBASE;
		} else
			pa = (Sysmap[PG_PFNUM(pte)] & PG_FRAME) << VAX_PGSHIFT;
	}

	pte[0] |= PG_V;
	pte[1] |= PG_V;
	pte[2] |= PG_V;
	pte[3] |= PG_V;
	pte[4] |= PG_V;
	pte[5] |= PG_V;
	pte[6] |= PG_V;
	pte[7] |= PG_V;

	pa = trunc_page(pa);
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg != NULL) {
		pg->mdpage.pv_attr |= PG_V; /* Referenced */
		if (bits & 4)	/* (will be) modified. XXX page tables  */
			pg->mdpage.pv_attr |= PG_M;
	}
	return 0;
}

/*
 * Checks if page is referenced; returns true or false depending on result.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	PMDEBUG(("pmap_is_referenced: pg %p pv_attr %x\n",
	    pg, pg->mdpage.pv_attr));

	if (pg->mdpage.pv_attr & PG_V)
		return 1;

	return 0;
}

/*
 * Clears valid bit in all ptes referenced to this physical page.
 */
boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	struct pv_entry *pv;
	pt_entry_t *pte;
	boolean_t ref = FALSE;
	int s;

	PMDEBUG(("pmap_clear_reference: pg %p\n", pg));

	if (pg->mdpage.pv_attr & PG_V)
		ref = TRUE;

	pg->mdpage.pv_attr &= ~PG_V;

	RECURSESTART;
	s = splvm();
	for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next) {
		pte = vaddrtopte(pv);
		pte[0] &= ~PG_V;
		pte[1] &= ~PG_V;
		pte[2] &= ~PG_V;
		pte[3] &= ~PG_V;
		pte[4] &= ~PG_V;
		pte[5] &= ~PG_V;
		pte[6] &= ~PG_V;
		pte[7] &= ~PG_V;
	}
	splx(s);

	RECURSEEND;
	mtpr(0, PR_TBIA);
	return ref;
}

/*
 * Checks if page is modified; returns true or false depending on result.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
	struct pv_entry *pv;
	pt_entry_t *pte;
	boolean_t rv = FALSE;
	int s;

	PMDEBUG(("pmap_is_modified: pg %p pv_attr %x\n",
	    pg, pg->mdpage.pv_attr));

	if (pg->mdpage.pv_attr & PG_M)
		return TRUE;

	s = splvm();
	for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next) {
		pte = vaddrtopte(pv);
		if ((pte[0] | pte[1] | pte[2] | pte[3] | pte[4] | pte[5] |
		     pte[6] | pte[7]) & PG_M) {
			rv = TRUE;
			break;
		}
	}
	splx(s);

	return rv;
}

/*
 * Clears modify bit in all ptes referenced to this physical page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	struct pv_entry *pv;
	pt_entry_t *pte;
	boolean_t rv = FALSE;
	int s;

	PMDEBUG(("pmap_clear_modify: pg %p\n", pg));

	if (pg->mdpage.pv_attr & PG_M)
		rv = TRUE;
	pg->mdpage.pv_attr &= ~PG_M;

	s = splvm();
	for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next) {
		pte = vaddrtopte(pv);
		if ((pte[0] | pte[1] | pte[2] | pte[3] | pte[4] | pte[5] |
		     pte[6] | pte[7]) & PG_M) {
			rv = TRUE;

			pte[0] &= ~PG_M;
			pte[1] &= ~PG_M;
			pte[2] &= ~PG_M;
			pte[3] &= ~PG_M;
			pte[4] &= ~PG_M;
			pte[5] &= ~PG_M;
			pte[6] &= ~PG_M;
			pte[7] &= ~PG_M;
		}
	}
	splx(s);

	return rv;
}

/*
 * Lower the permission for all mappings to a given page.
 * Lower permission can only mean setting protection to either read-only
 * or none; where none is unmapping of the page.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	pt_entry_t *pte;
	struct	pv_entry *pv, *npv;
	int	s;

	PMDEBUG(("pmap_page_protect: pg %p, prot %x\n", pg, prot));

	if (pg->mdpage.pv_head == NULL)
		return;

	if (prot == PROT_MASK) /* 'cannot happen' */
		return;

	RECURSESTART;
	s = splvm();
	if (prot == PROT_NONE) {
		npv = pg->mdpage.pv_head;
		pg->mdpage.pv_head = NULL;
		while ((pv = npv) != NULL) {
			npv = pv->pv_next;
			pte = vaddrtopte(pv);
			pv->pv_pmap->pm_stats.resident_count--;
			if (pte[0] & PG_W)
				pv->pv_pmap->pm_stats.wired_count--;
			if ((pg->mdpage.pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pg->mdpage.pv_attr |= 
				    pte[0] | pte[1] | pte[2] | pte[3] |
				    pte[4] | pte[5] | pte[6] | pte[7];
			bzero(pte, sizeof(pt_entry_t) * LTOHPN);
			if (pv->pv_pmap != pmap_kernel()) {
				if (ptpinuse(pte) == 0)
					rmptep(pte);
			}
			free_pventry(pv);
		}
	} else { /* read-only */
		for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next) {
			pt_entry_t pr;

			pte = vaddrtopte(pv);
			pr = (vaddr_t)pte < ptemapstart ? 
			    PG_KR : PG_RO;

			pte[0] = (pte[0] & ~PG_PROT) | pr;
			pte[1] = (pte[1] & ~PG_PROT) | pr;
			pte[2] = (pte[2] & ~PG_PROT) | pr;
			pte[3] = (pte[3] & ~PG_PROT) | pr;
			pte[4] = (pte[4] & ~PG_PROT) | pr;
			pte[5] = (pte[5] & ~PG_PROT) | pr;
			pte[6] = (pte[6] & ~PG_PROT) | pr;
			pte[7] = (pte[7] & ~PG_PROT) | pr;
		}
	}
	splx(s);
	RECURSEEND;
	mtpr(0, PR_TBIA);
}

void
pmap_remove_pcb(struct pmap *pm, struct pcb *thispcb)
{
	struct pcb *pcb, **pcbp;

	PMDEBUG(("pmap_remove_pcb pm %p pcb %p\n", pm, thispcb));

	for (pcbp = &pm->pm_pcbs; (pcb = *pcbp) != NULL;
	    pcbp = &pcb->pcb_pmnext) {
#ifdef DIAGNOSTIC
		if (pcb->pcb_pm != pm)
			panic("%s: pcb %p (pm %p) not owned by pmap %p",
			    __func__, pcb, pcb->pcb_pm, pm);
#endif
		if (pcb == thispcb) {
			*pcbp = pcb->pcb_pmnext;
			thispcb->pcb_pm = NULL;
			return;
		}
	}
#ifdef DIAGNOSTIC
	panic("%s: pmap %p: pcb %p not in list", __func__, pm, thispcb);
#endif
}

/*
 * Activate the address space for the specified process.
 * Note that if the process to activate is the current process, then
 * the processor internal registers must also be loaded; otherwise
 * the current process will have wrong pagetables.
 */
void
pmap_activate(struct proc *p)
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	int s;

	PMDEBUG(("pmap_activate: p %p pcb %p pm %p (%08x %08x %08x %08x)\n",
	    p, pcb, pmap, pmap->pm_p0br, pmap->pm_p0lr, pmap->pm_p1br,
	    pmap->pm_p1lr));

	pcb->P0BR = pmap->pm_p0br;
	pcb->P0LR = pmap->pm_p0lr | AST_PCB;
	pcb->P1BR = pmap->pm_p1br;
	pcb->P1LR = pmap->pm_p1lr;

	if (pcb->pcb_pm != pmap) {
		s = splsched();
		if (pcb->pcb_pm != NULL)
			pmap_remove_pcb(pcb->pcb_pm, pcb);
		pcb->pcb_pmnext = pmap->pm_pcbs;
		pmap->pm_pcbs = pcb;
		pcb->pcb_pm = pmap;
		splx(s);
	}

	if (p == curproc) {
		mtpr((register_t)pmap->pm_p0br, PR_P0BR);
		mtpr(pmap->pm_p0lr | AST_PCB, PR_P0LR);
		mtpr((register_t)pmap->pm_p1br, PR_P1BR);
		mtpr(pmap->pm_p1lr, PR_P1LR);
		mtpr(0, PR_TBIA);
	}
}

void
pmap_deactivate(struct proc *p)
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	int s;

	PMDEBUG(("pmap_deactivate: p %p pcb %p\n", p, pcb));

	if (pcb->pcb_pm != NULL) {
		s = splsched();
#ifdef DIAGNOSTIC
		if (pcb->pcb_pm != pmap)
			panic("%s: proc %p pcb %p not owned by pmap %p",
			    __func__, p, pcb, pmap);
#endif
		pmap_remove_pcb(pmap, pcb);
		splx(s);
	}
}
@


1.77
log
@Make kernel text read-only and unreadable from userland, and remove the bogus
comment about the emulation code requiring kernel text to be readable from
userland.

Add a few DIAGNOSTIC checks for rogue ptes passed to rmpage().

Make sure the pte extent operations and update_pcbs() run at >= IPL_SCHED.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2015/05/07 01:55:43 jsg Exp $ */
@


1.76
log
@fix indentation
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2015/02/15 21:34:33 miod Exp $ */
d287 2
a288 7
	/*
	 * Kernel code is always readable for user, it must be because
	 * of the emulation code that is somewhere in there.
	 * And it doesn't hurt, the kernel file is also public readable.
	 * There are also a couple of other things that must be in
	 * physical memory and that isn't managed by the vm system.
	 */
d290 1
a290 1
		Sysmap[i] = (Sysmap[i] & ~PG_PROT) | PG_URKW;
d480 5
a484 1
	if (pm == pmap_kernel())
d486 1
a486 1
	else if (br >= pm->pm_p0br && br < pm->pm_p0br + pm->pm_p0lr)
d488 1
a488 1
	else
d490 5
d567 1
d601 2
d614 2
d748 1
d762 2
d785 2
d799 1
d810 3
d836 2
a1368 1
 * Note: the save mask must be or'ed with 0x3f for this function.
@


1.75
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2014/12/17 06:58:10 guenther Exp $ */
d650 1
a650 1
				slpp = p;
@


1.74
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2014/12/15 02:24:23 guenther Exp $ */
d854 1
a854 1
pmap_remove_holes(struct vm_map *map)
d856 1
d864 1
a864 1
	ehole = VM_MAXUSER_ADDRESS - MAXSSIZ;
@


1.73
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2014/11/25 20:31:41 miod Exp $ */
d871 1
a871 2
	    UVM_MAPFLAG(PROT_NONE, PROT_NONE, MAP_INHERIT_SHARE,
	      POSIX_MADV_RANDOM,
@


1.72
log
@A few reliability fixes:
- protect pv lists with splvm.
- try to return as soon as possible from pmap_rmproc().
- correctly maintain wired pages statistics.
- invoke pmap_remove_pcb() at splsched to avoid racing the scheduler.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2014/11/17 21:39:19 deraadt Exp $ */
d871 1
a871 1
	    UVM_MAPFLAG(PROT_NONE, PROT_NONE, UVM_INH_SHARE,
@


1.71
log
@Two additional POSIX_MADV_RANDOM conversions
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2014/11/16 12:30:59 deraadt Exp $ */
d476 1
a476 1
	int found = 0;
d492 1
a492 2
	pv = pg->mdpage.pv_head;

d496 11
a506 5
			if (((br[0] & PG_PROT) == PG_RW) && 
			    (pg->mdpage.pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pg->mdpage.pv_attr |=
				    br[0] | br[1] | br[2] | br[3] |
				    br[4] | br[5] | br[6] | br[7];
d521 1
d663 2
d1108 8
a1115 3
	if (newpte == (oldpte | PG_W)) {
		*pteptr |= PG_W; /* Just wiring change */
		pmap->pm_stats.wired_count++;
d1429 1
d1439 1
d1442 8
a1449 10
		if ((pte[0] & PG_W) == 0) {
			pte[0] &= ~PG_V;
			pte[1] &= ~PG_V;
			pte[2] &= ~PG_V;
			pte[3] &= ~PG_V;
			pte[4] &= ~PG_V;
			pte[5] &= ~PG_V;
			pte[6] &= ~PG_V;
			pte[7] &= ~PG_V;
		}
d1451 1
d1466 2
d1475 1
d1479 4
a1482 2
		     pte[6] | pte[7]) & PG_M)
			return TRUE;
d1484 1
d1486 1
a1486 1
	return FALSE;
d1498 1
d1506 1
d1523 1
d1549 1
a1550 1
		s = splvm();
a1569 1
		splx(s);
d1588 1
d1629 1
d1641 1
d1647 1
d1664 1
d1668 2
a1669 2
	if (pcb->pcb_pm == NULL)
		return;
d1671 3
a1673 3
	if (pcb->pcb_pm != pmap)
		panic("%s: proc %p pcb %p not owned by pmap %p",
		    __func__, p, pcb, pmap);
d1675 3
a1677 1
	pmap_remove_pcb(pmap, pcb);
@


1.70
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2014/05/24 20:13:52 guenther Exp $ */
d864 1
a864 1
	      UVM_ADV_RANDOM,
@


1.69
log
@Why didn't I use ps_vmspace when I scribbled all over pmap_rmproc()?

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2014/05/17 23:21:36 miod Exp $ */
d286 1
a286 1
	pmap_map(KERNBASE, 0, avail_end, VM_PROT_READ|VM_PROT_WRITE);
d863 1
a863 1
	    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_SHARE,
d972 1
a972 1
	mapin8(ptp, PG_V | ((prot & VM_PROT_WRITE) ? PG_KW : PG_KR) |
d1041 1
a1041 1
		newpte = prot & VM_PROT_WRITE ? PG_KW : PG_KR;
d1050 1
a1050 1
		newpte = prot & VM_PROT_WRITE ? PG_RW : PG_RO;
d1059 1
a1059 1
		newpte = prot & VM_PROT_WRITE ? PG_RW : PG_RO;
d1155 1
a1155 1
	if (flags & VM_PROT_READ) {
d1159 1
a1159 1
	if (flags & VM_PROT_WRITE)
d1187 1
a1187 1
		    (prot & VM_PROT_WRITE ? PG_KW : PG_KR);
d1260 1
a1260 1
		pr = (prot & VM_PROT_WRITE ? PG_KW : PG_KR);
d1273 1
a1273 1
		pr = (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
d1287 1
a1287 1
		pr = (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
d1304 1
a1304 1
			if (prot == VM_PROT_NONE) {
d1522 1
a1522 1
	if (prot == VM_PROT_ALL) /* 'cannot happen' */
d1526 1
a1526 1
	if (prot == VM_PROT_NONE) {
@


1.68
log
@One more mistake introduced in 1.65
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2014/05/17 21:32:19 miod Exp $ */
d623 1
a623 4
		p = TAILQ_FIRST(&pr->ps_threads);
		if (p == NULL)
			continue;
		ppm = p->p_vmspace->vm_map.pmap;
@


1.67
log
@...and it needs a guard against NULL as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2014/05/17 21:21:17 miod Exp $ */
d657 1
a657 1
				outpri = p->p_slptime;
@


1.66
log
@Fix previous commit by making sure we are not dereferencing uninitialized
pointers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2014/05/15 03:52:25 guenther Exp $ */
d624 2
@


1.65
log
@Move from struct proc to process the reference-count-holding pointers
to the process's vmspace and filedescs.  struct proc continues to
keep copies of the pointers, copying them on fork, clearing them
on exit, and (for vmspace) refreshing on exec.
Also, make uvm_swapout_threads() thread aware, eliminating p_swtime
in kernel.

particular testing by ajacoutot@@ and sebastia@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2014/05/08 19:06:07 miod Exp $ */
d623 1
@


1.64
log
@Format string fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2014/02/09 20:58:49 miod Exp $ */
a607 17
static inline boolean_t
pmap_vax_swappable(struct proc *p, struct pmap *pm)
{
	if (p->p_flag & (P_SYSTEM | P_WEXIT))	/* !swappable(p) */
		return FALSE;
	if (p->p_vmspace->vm_map.pmap == pm)
		return FALSE;
	switch (p->p_stat) {
	case SRUN:
	case SSLEEP:
	case SSTOP:
		return TRUE;
	default:
		return FALSE;
	}
}

d611 1
d613 2
a614 3
	struct proc *p;
	struct proc *outp, *outp2;
	int outpri, outpri2;
d618 4
a621 4
	outp = outp2 = NULL;
	outpri = outpri2 = 0;
	LIST_FOREACH(p, &allproc, p_list) {
		if (!pmap_vax_swappable(p, pm))
d624 2
d627 19
a645 9
			continue; /* Already swapped */
		switch (p->p_stat) {
		case SRUN:
#if 0 /* won't pass pmap_vax_swappable() */
		case SONPROC:
#endif
			if (p->p_swtime > outpri2) {
				outp2 = p;
				outpri2 = p->p_swtime;
d647 3
a649 4
			continue;
		case SSLEEP:
		case SSTOP:
			if (p->p_slptime >= maxslp) {
d652 2
a653 2
			} else if (p->p_slptime > outpri) {
				outp = p;
a655 1
			continue;
d657 1
d660 3
a662 7
	if (didswap == 0) {
		if ((p = outp) == NULL)
			p = outp2;
		if (p) {
			rmspace(p->p_vmspace->vm_map.pmap);
			didswap++;
		}
@


1.63
log
@Fix copying of the previous pte entries in grow_p1(). Found the hard way by
sebastia@@, this would only affect processes using more than 512KB of stack.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2013/11/24 22:08:25 miod Exp $ */
d1300 1
a1300 1
		panic("unsupported segtype: %d", SEGTYPE(start));
@


1.62
log
@Rework pmap to use dynamic P0 and P1 region allocation, instead of allocating
the largest possible page table for every pmap; from NetBSD. This allows the
kernel to use much less memory for page tables.

Significant differences against the NetBSD code are:
- allocation of page table pages is done with a pool instead of allocating
  whole logical pages from uvm and managing the freelist within pmap, never
  releasing allocated pages.
- try to use pt_entry_t * rather than int * whenever possible.
- growth of P0 and P1 regions is allowed to fail, if invoked from
  pmap_enter with the PMAP_CANFAIL flag. This will stall processes until
  memory for the page tables can be obtained, rather than panicing, in
  most cases.
- keep management of mappings for managed pages using pv lists tied to the
  vm_page (using __HAVE_VM_PAGE_MD), rather than a global pv_list head.
- bound check against Sysmap[] in pmap_extract() when asked for a kernel
  address.

As a result of this, bsd.rd can now install a working system on a 12MB machine
without needing to enable swap.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2013/11/20 23:57:07 miod Exp $ */
d167 2
a168 2
	for (i = 0; i < NPTEPG; i += 8)
		if (pve[i] != 0)
d422 1
a422 1
	v = (vm_physmem[0].avail_start << PGSHIFT) | KERNBASE;
d564 1
a564 1
		if (*ptpp == 0)
d573 1
a573 1
		*ptpp = 0;
d578 1
a578 1
		if (*ptpp == 0)
d587 1
a587 1
		*ptpp = 0;
d730 1
a730 1
	*ptpp = 0;
d812 2
a813 1
		memcpy((char *)to + nlen - olen, from, vax_btop(olen) * PPTESZ);
d1007 1
a1007 1
	len >>= PGSHIFT;
d1019 1
a1019 1
	len >>= VAX_PGSHIFT;
d1022 1
a1022 1
	bzero(pte, len * sizeof(pt_entry_t));
d1082 1
a1082 1
		if (*ptpptr == 0) {
d1282 1
a1282 1
		end = (end == KERNBASE ? end >> 1 : end & 0x3fffffff);
d1357 1
a1357 1
	PMDEBUG(("pmap_simulref: bits %x addr %x\n", bits, addr));
d1527 1
a1527 1
	PMDEBUG(("pmap_page_protect: pg %p, prot %x, ", pg, prot));
@


1.61
log
@Update comments mentioning `resource maps' to mention `extents' instead.
Resource maps have been removed more than 10 years ago, it's about time to
update comments to better match reality.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2013/09/21 10:04:42 miod Exp $ */
d4 1
a4 1
 * Copyright (c) 1994, 1998, 1999 Ludd, University of Lule}, Sweden.
d66 10
a75 1
unsigned int sysptsize;
d77 2
d87 1
d90 19
d111 2
a112 1
#define RECURSESTART {							\
d116 7
a122 2
}
#define RECURSEEND {recurse = 0; }
d126 1
d129 28
a156 3
#ifdef PMAPDEBUG
int	startpmapdebug = 0;
#endif
d158 8
a165 4
#ifndef DEBUG
static inline
#endif
void pmap_decpteref(struct pmap *, pt_entry_t *);
d167 5
a171 1
void rensa(pt_entry_t, pt_entry_t *);
d174 1
a174 1
vaddr_t   virtual_avail, virtual_end; /* Available virtual memory	*/
d179 56
d247 6
a252 1
	pmap_t pmap = pmap_kernel();
d255 3
a257 4
	 * Calculation of the System Page Table is somewhat a pain,
	 * because it must be in contiguous physical memory and all
	 * size calculations must be done now.
	 * Remember: sysptsize is in PTEs and nothing else!
d259 6
d266 2
a267 10
	/* Kernel alloc area */
	sysptsize = (((0x100000 * maxprocess) >> VAX_PGSHIFT) / 4);
	/* reverse mapping struct */
	sysptsize += (avail_end >> VAX_PGSHIFT) * 2;
	/* User Page table area. This may grow big */
	sysptsize += ((USRPTSIZE * 4) / VAX_NBPG) * maxprocess;
	/* Kernel stacks per process */
	sysptsize += UPAGES * maxthread;
	/* IO device register space */
	sysptsize += IOSPSZ;
d278 2
a279 1
	memset(Sysmap, 0, sysptsize * 4); /* clear SPT before using it */
d294 1
a294 1
	for (i = 0; i < ((unsigned)&etext - KERNBASE) >> VAX_PGSHIFT; i++)
d298 1
a298 1
	mtpr((unsigned)Sysmap - KERNBASE, PR_SBR);
d301 1
a301 1
	istack = (vaddr_t)Sysmap + round_page(sysptsize * 4);
d307 2
a308 1
	avail_start = scratch + 4 * VAX_NBPG - KERNBASE;
a317 4
	/* Set logical page size */
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();

d319 1
a319 1
	MAPVIRT(ptemapstart, USRPTSIZE);
d338 1
a338 1
	printf("Sysmap %p, istack %p, scratch %p\n",Sysmap,istack,scratch);
d340 2
a341 1
	printf("SYSPTSIZE %x\n",sysptsize);
d343 1
a343 1
	printf("avail_start %lx, avail_end %lx\n",avail_start,avail_end);
a348 1

d350 4
a353 4
	pmap->pm_p1br = (void *)KERNBASE;
	pmap->pm_p0br = (void *)KERNBASE;
	pmap->pm_p1lr = 0x200000;
	pmap->pm_p0lr = AST_PCB;
d357 1
a357 1
	pmap->ref_count = 1;
d360 11
a370 4
	mtpr((register_t)(pcb->P1BR = pmap->pm_p1br), PR_P1BR);
	mtpr((register_t)(pcb->P0BR = pmap->pm_p0br), PR_P0BR);
	mtpr(pcb->P1LR = pmap->pm_p1lr, PR_P1LR);
	mtpr(pcb->P0LR = pmap->pm_p0lr, PR_P0LR);
d372 1
a372 1
	/* Create the pmap and pv_entry pools. */
d375 1
d402 1
a402 3
pmap_steal_memory(size, vstartp, vendp)
	vsize_t size;
	vaddr_t *vstartp, *vendp;
d407 3
a409 5
#ifdef PMAPDEBUG
	if (startpmapdebug) 
		printf("pmap_steal_memory: size 0x%lx start %p end %p\n",
		    size, vstartp, vendp);
#endif
d450 16
d467 2
a468 2
 * Decrement a reference to a pte page. If all references are gone,
 * free the page.
d471 1
a471 3
pmap_decpteref(pmap, pte)
	struct pmap *pmap;
	pt_entry_t *pte;
d473 4
a476 2
	paddr_t paddr;
	int index;
d478 5
a482 1
	if (pmap == pmap_kernel())
a483 1
	index = ((vaddr_t)pte - (vaddr_t)pmap->pm_p0br) >> PGSHIFT;
d485 206
a690 6
	pte = (pt_entry_t *)trunc_page((vaddr_t)pte);
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_decpteref: pmap %p pte %p index %d refcnt %d\n",
		    pmap, pte, index, pmap->pm_refcnt[index]);
#endif
d693 2
a694 2
	if ((index < 0) || (index >= NPTEPGS))
		panic("pmap_decpteref: bad index %d", index);
d696 23
a718 1
	pmap->pm_refcnt[index]--;
d720 7
a726 2
	if (pmap->pm_refcnt[index] >= VAX_NBPG/sizeof(pt_entry_t))
		panic("pmap_decpteref");
d728 95
a822 5
	if (pmap->pm_refcnt[index] == 0) {
		paddr = (*kvtopte(pte) & PG_FRAME) << VAX_PGSHIFT;
		bzero(kvtopte(pte), sizeof(pt_entry_t) * LTOHPN);
		uvm_pagefree(PHYS_TO_VM_PAGE(paddr));
	}
a826 1
 * If not already allocated, malloc space for one.
a831 1
	int bytesiz, res;
d836 3
a838 2
	 * Allocate PTEs and stash them away in the pmap.
	 * XXX Ok to use kmem_alloc_wait() here?
d840 4
a843 10
	bytesiz = USRPTSIZE * sizeof(pt_entry_t);
	res = extent_alloc(ptemap, bytesiz, 4, 0, 0, EX_WAITSPACE|EX_WAITOK,
	    (u_long *)&pmap->pm_p0br);
	if (res)
		panic("pmap_create");
	pmap->pm_p0lr = vax_atop(MAXTSIZ + MAXDSIZ + BRKSIZ) | AST_PCB;
	pmap->pm_p1br = pmap->pm_p0br +
	    (bytesiz - 0x800000) / sizeof(pt_entry_t);
	pmap->pm_p1lr = vax_atop(0x40000000 - MAXSSIZ);
	pmap->pm_stack = USRSTACK;
d845 2
a846 7
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_create: pmap %p, "
		    "p0br=%p p0lr=0x%lx p1br=%p p1lr=0x%lx\n",
	    	    pmap, pmap->pm_p0br, pmap->pm_p0lr,
		    pmap->pm_p1br, pmap->pm_p1lr);
#endif
d848 2
a849 1
	pmap->ref_count = 1;
d851 1
a851 1
	return(pmap);
d863 2
a864 3
	shole = ((vaddr_t)(pmap->pm_p0lr & 0x3fffff)) << VAX_PGSHIFT;
	ehole = 0x40000000 +
	    (((vaddr_t)(pmap->pm_p1lr & 0x3fffff)) << VAX_PGSHIFT);
d878 1
a878 3
pmap_unwire(pmap, va)
	pmap_t pmap;
	vaddr_t va;
d880 2
a881 1
	int *p, *pte, i;
d883 1
d885 2
a886 2
		p = (int *)Sysmap;
		i = (va - KERNBASE) >> VAX_PGSHIFT;
d888 5
a892 7
		if(va < 0x40000000) {
			p = (int *)pmap->pm_p0br;
			i = va >> VAX_PGSHIFT;
		} else {
			p = (int *)pmap->pm_p1br;
			i = (va - 0x40000000) >> VAX_PGSHIFT;
		}
a893 1
	pte = &p[i];
d895 3
a897 1
	*pte &= ~PG_W;
d905 1
a905 2
pmap_destroy(pmap)
	pmap_t pmap;
a909 1
	int i;
d912 1
a912 4
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_destroy: pmap %p\n",pmap);
#endif
d914 1
a914 4
	simple_lock(&pmap->pm_lock);
	count = --pmap->ref_count;
	simple_unlock(&pmap->pm_lock);
  
d918 5
a924 5
		for (i = 0; i < NPTEPGS; i++)
			if (pmap->pm_refcnt[i])
				panic("pmap_release: refcnt %d index %d", 
				    pmap->pm_refcnt[i], i);

d926 7
a932 2
		eaddr = saddr + USRPTSIZE * sizeof(pt_entry_t);
		for (; saddr < eaddr; saddr += NBPG)
d934 1
a934 1
				panic("pmap_release: page mapped");
d936 3
d940 4
a943 2
		    USRPTSIZE * sizeof(pt_entry_t), EX_WAITOK);
	}
d948 2
a949 8
/*
 * Rensa is a help routine to remove a pv_entry from the pv list.
 * Arguments are physical clustering page and page table entry pointer.
 */
void
rensa(pte, ptp)
	pt_entry_t pte;
	pt_entry_t *ptp;
d951 1
a951 40
	struct vm_page *pg;
	struct pv_entry *pv, *npv, *ppv;
	paddr_t pa;
	int s, *g;

	/*
	 * Check that we are working on a managed page.
	 */
	pa = (pte & PG_FRAME) << VAX_PGSHIFT;
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
		return;

#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("rensa: pg %p ptp %p\n", pg, ptp);
#endif
	s = splvm();
	RECURSESTART;
	for (ppv = NULL, pv = pg->mdpage.pv_head; pv != NULL;
	    ppv = pv, pv = npv) {
		npv = pv->pv_next;
		if (pv->pv_pte == ptp) {
			g = (int *)pv->pv_pte;
			if ((pg->mdpage.pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pg->mdpage.pv_attr |=
				    g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
			if (npv != NULL) {
				*pv = *npv;
				free_pventry(npv);
			} else {
				if (ppv != NULL)
					ppv->pv_next = pv->pv_next;
				else
					pg->mdpage.pv_head = NULL;
				free_pventry(pv);
			}
			goto leave;
		}
	}
d953 7
a959 7
#ifdef DIAGNOSTIC
	panic("rensa(0x%x, %p) page %p: mapping not found", pte, ptp, pg);
#endif

leave:
	splx(s);
	RECURSEEND;
d967 1
a967 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t	pa;
	vm_prot_t prot;
d969 1
a969 1
	pt_entry_t *ptp;
d972 6
a977 5
#ifdef PMAPDEBUG
if(startpmapdebug)
	printf("pmap_kenter_pa: va: %lx, pa %lx, prot %x ptp %p\n", va, pa, prot, ptp);
#endif
	if ((*ptp & PG_FRAME) == 0)
d979 7
a985 9
	ptp[0] = PG_V | ((prot & VM_PROT_WRITE)? PG_KW : PG_KR) |
	    PG_PFNUM(pa) | PG_SREF;
	ptp[1] = ptp[0] + 1;
	ptp[2] = ptp[0] + 2;
	ptp[3] = ptp[0] + 3;
	ptp[4] = ptp[0] + 4;
	ptp[5] = ptp[0] + 5;
	ptp[6] = ptp[0] + 6;
	ptp[7] = ptp[0] + 7;
d989 1
a989 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d992 1
d994 6
a1001 4
if(startpmapdebug)
	printf("pmap_kremove: va: %lx, len %lx, ptp %p\n", va, len, kvtopte(va));
#endif

d1003 2
a1004 1
	 * Unfortunately we must check if any page may be on the pv list. 
a1005 1
	pte = kvtopte(va);
a1006 1

d1011 3
a1013 8
#ifdef DEBUG
		if ((*pte & PG_SREF) == 0) {
			printf("pmap_kremove(%p, %x): "
			    "pte %x@@%p does not have SREF set!\n", 
			    va, len << PGSHIFT, *pte, pte);
			rensa(*pte, pte);
		}
#endif
d1017 6
d1031 9
a1039 13
pmap_enter(pmap, v, p, prot, flags)
	pmap_t	pmap;
	vaddr_t	v;
	paddr_t	p;
	vm_prot_t prot;
	int flags;
{
	struct	vm_page *pg;
	struct	pv_entry *pv;
	int	i, s, newpte, oldpte, *patch, index = 0; /* XXX gcc */
#ifdef PMAPDEBUG
	boolean_t wired = (flags & PMAP_WIRED) != 0;
#endif
d1041 2
a1042 5
#ifdef PMAPDEBUG
if (startpmapdebug)
	printf("pmap_enter: pmap %p v %lx p %lx prot %x wired %d flags %x\n",
		    pmap, v, p, prot, wired, flags);
#endif
d1045 1
d1047 19
a1065 32
	if (v & KERNBASE) {
		patch = (int *)Sysmap;
		i = (v - KERNBASE) >> VAX_PGSHIFT;
		newpte = (p>>VAX_PGSHIFT)|(prot&VM_PROT_WRITE?PG_KW:PG_KR);
	} else {
		if (v < 0x40000000) {
			patch = (int *)pmap->pm_p0br;
			i = (v >> VAX_PGSHIFT);
			if (i >= (pmap->pm_p0lr & ~AST_MASK)) {
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (EFAULT);
				}
				panic("P0 too small in pmap_enter");
			}
			patch = (int *)pmap->pm_p0br;
			newpte = (p >> VAX_PGSHIFT) |
			    (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
		} else {
			patch = (int *)pmap->pm_p1br;
			i = (v - 0x40000000) >> VAX_PGSHIFT;
			if (i < pmap->pm_p1lr) {
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (EFAULT);
				}
				panic("pmap_enter: must expand P1");
			}
			if (v < pmap->pm_stack)
				pmap->pm_stack = v;
			newpte = (p >> VAX_PGSHIFT) |
			    (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
d1067 7
d1075 1
d1079 4
a1082 25
		index = ((u_int)&patch[i] - (u_int)pmap->pm_p0br) >> PGSHIFT;
#ifdef DIAGNOSTIC
		if ((index < 0) || (index >= NPTEPGS))
			panic("pmap_enter: bad index %d", index);
#endif
		if (pmap->pm_refcnt[index] == 0) {
			vaddr_t ptaddr = trunc_page((vaddr_t)&patch[i]);
			paddr_t phys;
			struct vm_page *pg;
#ifdef DEBUG
			if ((*kvtopte(&patch[i]) & PG_FRAME) != 0)
				panic("pmap_enter: refcnt == 0");
#endif
			/*
			 * It seems to be legal to sleep here to wait for
			 * pages; at least some other ports do so.
			 */
			for (;;) {
				pg = uvm_pagealloc(NULL, 0, NULL, 0);
				if (pg != NULL)
					break;
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (ENOMEM);
				}
d1084 4
a1087 1
				panic("pmap_enter: no free pages");
d1089 1
a1089 6

			phys = VM_PAGE_TO_PHYS(pg);
			bzero((caddr_t)(phys|KERNBASE), NBPG);
			pmap_kenter_pa(ptaddr, phys,
			    VM_PROT_READ|VM_PROT_WRITE);
			pmap_update(pmap_kernel());
d1098 1
a1098 10
		patch[i] = newpte;
		patch[i+1] = newpte+1;
		patch[i+2] = newpte+2;
		patch[i+3] = newpte+3;
		patch[i+4] = newpte+4;
		patch[i+5] = newpte+5;
		patch[i+6] = newpte+6;
		patch[i+7] = newpte+7;
		if (pmap != pmap_kernel())
			pmap->pm_refcnt[index]++; /* New mapping */
d1100 1
a1100 1
		return (0);
d1102 1
d1106 1
a1106 1
	oldpte = patch[i] & ~(PG_V|PG_M);
d1108 1
a1108 1
	/* wiring change? */
d1110 2
a1111 1
		patch[i] |= PG_W; /* Just wiring change */
d1113 1
a1113 1
		return (0);
d1119 1
a1119 1
		return (0);
d1123 6
a1128 1
	if ((newpte & PG_FRAME) != (oldpte & PG_FRAME)) {
d1134 2
d1138 3
a1140 1
				rensa(oldpte, (pt_entry_t *)&patch[i]);
d1142 1
a1142 2
		} else if (pmap != pmap_kernel())
			pmap->pm_refcnt[index]++; /* New mapping */
d1148 1
d1150 1
a1150 1
				return (ENOMEM);
d1154 1
a1154 1
		pv->pv_pte = (pt_entry_t *)&patch[i];
d1160 2
a1161 3
	} else {
		/* No mapping change, just flush the TLB; necessary? */
		mtpr(0, PR_TBIA);
d1174 1
a1174 8
	patch[i] = newpte;
	patch[i+1] = newpte+1;
	patch[i+2] = newpte+2;
	patch[i+3] = newpte+3;
	patch[i+4] = newpte+4;
	patch[i+5] = newpte+5;
	patch[i+6] = newpte+6;
	patch[i+7] = newpte+7;
a1175 5
#ifdef DEBUG
	if (pmap != pmap_kernel())
		if (pmap->pm_refcnt[index] > VAX_NBPG/sizeof(pt_entry_t))
			panic("pmap_enter: refcnt %d", pmap->pm_refcnt[index]);
#endif
d1178 1
a1178 1
	return (0);
d1182 1
a1182 4
pmap_map(virtuell, pstart, pend, prot)
	vaddr_t virtuell;
	paddr_t	pstart, pend;
	int prot;
d1185 1
a1185 1
	int *pentry;
d1187 2
a1188 5
#ifdef PMAPDEBUG
if(startpmapdebug)
	printf("pmap_map: virt %lx, pstart %lx, pend %lx, Sysmap %p\n",
	    virtuell, pstart, pend, Sysmap);
#endif
d1190 6
a1195 6
	pstart=(uint)pstart &0x7fffffff;
	pend=(uint)pend &0x7fffffff;
	virtuell=(uint)virtuell &0x7fffffff;
	pentry = (int *)((((uint)(virtuell)>>VAX_PGSHIFT)*4)+(uint)Sysmap);
	for(count=pstart;count<pend;count+=VAX_NBPG){
		*pentry++ = (count>>VAX_PGSHIFT)|PG_V|
d1198 1
a1198 1
	return(virtuell+(count-pstart)+KERNBASE);
d1202 1
a1202 4
pmap_extract(pmap, va, pap)
	pmap_t pmap;
	vaddr_t va;
	paddr_t *pap;
d1204 2
a1205 1
	int	*pte, sva;
d1207 1
a1207 3
#ifdef PMAPDEBUG
if(startpmapdebug)printf("pmap_extract: pmap %p, va %lx\n",pmap, va);
#endif
d1210 8
d1219 4
a1222 8
	if (va & KERNBASE) {
		if (sva >= sysptsize)
			return (FALSE);
		pte = Sysmap;
	} else if (va < 0x40000000) {
		if (sva > (pmap->pm_p0lr & ~AST_MASK))
			return (FALSE);
		pte = (int *)pmap->pm_p0br;
d1225 2
a1226 2
			return (FALSE);
		pte = (int *)pmap->pm_p1br;
d1228 5
a1232 2

	if ((*kvtopte(&pte[sva]) & PG_V) && (pte[sva] & PG_V)) {
d1235 2
a1236 1
		return (TRUE);
d1239 3
a1241 1
	return (FALSE);
d1250 1
a1250 4
pmap_protect(pmap, start, end, prot)
	pmap_t	pmap;
	vaddr_t	start, end;
	vm_prot_t prot;
d1253 1
a1253 1
	pt_entry_t pr;
d1255 2
a1256 4
#ifdef PMAPDEBUG
if(startpmapdebug) printf("pmap_protect: pmap %p, start %lx, end %lx, prot %x\n",
	pmap, start, end,prot);
#endif
d1258 1
a1258 2
	if (pmap == 0)
		return;
d1260 2
a1261 2
	RECURSESTART;
	if (start & KERNBASE) { /* System space */
d1264 1
a1264 1
		if (((end & 0x3fffffff) >> VAX_PGSHIFT) > mfpr(PR_SLR))
d1270 22
a1291 31
	} else {
		if (start & 0x40000000) { /* P1 space */
			if (end <= pmap->pm_stack) {
				RECURSEEND;
				return;
			}
			if (start < pmap->pm_stack)
				start = pmap->pm_stack;
			pt = pmap->pm_p1br;
			if (((start & 0x3fffffff) >> VAX_PGSHIFT) <
			    pmap->pm_p1lr) {
#ifdef PMAPDEBUG
				panic("pmap_protect: outside P1LR");
#else
				RECURSEEND;
				return;
#endif
			}
			start &= 0x3fffffff;
			end = (end == KERNBASE ? end >> 1 : end & 0x3fffffff);
		} else { /* P0 space */
			pt = pmap->pm_p0br;
			if ((end >> VAX_PGSHIFT) >
			    (pmap->pm_p0lr & ~AST_MASK)) {
#ifdef PMAPDEBUG
				panic("pmap_protect: outside P0LR");
#else
				RECURSEEND;
				return;
#endif
			}
d1293 3
d1297 3
d1301 1
d1312 1
a1312 1
		if ((*kvtopte(pts) & PG_FRAME) != 0 && *(int *)pts) {
d1314 3
d1318 2
a1319 2
				if ((*(int *)pts & PG_SREF) == 0)
					rensa(*pts, pts);
d1322 4
a1325 2
				pmap->pm_stats.resident_count--;
				pmap_decpteref(pmap, pts);
a1342 1
int pmap_simulref(int bits, int addr);
d1348 1
d1350 1
a1350 1
pmap_simulref(int bits, int addr)
d1356 1
a1356 4
#ifdef PMAPDEBUG
if (startpmapdebug) 
	printf("pmap_simulref: bits %x addr %x\n", bits, addr);
#endif
d1361 10
a1370 5
	/* Set address on logical page boundary */
	addr &= ~PGOFSET;
	/* First decode userspace addr */
	if (addr >= 0) {
		if ((addr << 1) < 0)
d1372 1
a1372 3
		else
			pte = (pt_entry_t *)mfpr(PR_P0BR);
		pte += PG_PFNUM(addr);
d1374 1
a1374 2
			pte = (pt_entry_t *)trunc_page((vaddr_t)pte);
			pte = kvtopte(pte);
a1379 3
	} else {
		pte = kvtopte(addr);
		pa = (paddr_t)pte & ~KERNBASE;
d1381 1
d1405 1
a1405 2
pmap_is_referenced(pg)
	struct vm_page *pg;
d1407 2
a1408 5
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_is_referenced: pg %p pv_attr %x\n",
		    pg, pg->mdpage.pv_attr);
#endif
d1420 1
a1420 2
pmap_clear_reference(pg)
	struct vm_page *pg;
d1423 1
d1426 1
a1426 4
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_clear_reference: pg %p\n", pg);
#endif
d1434 11
a1444 10
	for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next)
		if ((pv->pv_pte[0] & PG_W) == 0) {
			pv->pv_pte[0] &= ~PG_V;
			pv->pv_pte[1] &= ~PG_V;
			pv->pv_pte[2] &= ~PG_V;
			pv->pv_pte[3] &= ~PG_V;
			pv->pv_pte[4] &= ~PG_V;
			pv->pv_pte[5] &= ~PG_V;
			pv->pv_pte[6] &= ~PG_V;
			pv->pv_pte[7] &= ~PG_V;
d1446 1
d1449 1
d1457 1
a1457 2
pmap_is_modified(pg)
	struct vm_page *pg;
d1460 1
d1462 2
a1463 5
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_is_modified: pg %p pv_attr %x\n",
		    pg, pg->mdpage.pv_attr);
#endif
d1468 4
a1471 4
	for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next)
		if ((pv->pv_pte[0] | pv->pv_pte[1] | pv->pv_pte[2] |
		     pv->pv_pte[3] | pv->pv_pte[4] | pv->pv_pte[5] |
		     pv->pv_pte[6] | pv->pv_pte[7]) & PG_M)
d1473 1
d1482 1
a1482 2
pmap_clear_modify(pg)
	struct vm_page *pg;
d1485 1
d1488 2
a1489 4
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_clear_modify: pg %p\n", pg);
#endif
d1494 4
a1497 4
	for (pv = pg->mdpage.pv_head; pv != NULL; pv = pv->pv_next)
		if ((pv->pv_pte[0] | pv->pv_pte[1] | pv->pv_pte[2] |
		     pv->pv_pte[3] | pv->pv_pte[4] | pv->pv_pte[5] |
		     pv->pv_pte[6] | pv->pv_pte[7]) & PG_M) {
d1500 8
a1507 8
			pv->pv_pte[0] &= ~PG_M;
			pv->pv_pte[1] &= ~PG_M;
			pv->pv_pte[2] &= ~PG_M;
			pv->pv_pte[3] &= ~PG_M;
			pv->pv_pte[4] &= ~PG_M;
			pv->pv_pte[5] &= ~PG_M;
			pv->pv_pte[6] &= ~PG_M;
			pv->pv_pte[7] &= ~PG_M;
d1509 1
d1520 1
a1520 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t       prot;
d1522 1
a1522 1
	pt_entry_t *pt;
d1524 1
a1524 1
	int	s, *g;
d1526 1
a1526 4
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_page_protect: pg %p, prot %x, ", pg, prot);
#endif
d1541 4
a1544 1
			g = (int *)pv->pv_pte;
d1547 7
a1553 4
				    g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
			bzero(g, sizeof(pt_entry_t) * LTOHPN);
			pv->pv_pmap->pm_stats.resident_count--;
			pmap_decpteref(pv->pv_pmap, pv->pv_pte);
d1561 2
a1562 2
			pt = pv->pv_pte;
			pr = (vaddr_t)pv->pv_pte < ptemapstart ? 
d1565 8
a1572 8
			pt[0] = (pt[0] & ~PG_PROT) | pr;
			pt[1] = (pt[1] & ~PG_PROT) | pr;
			pt[2] = (pt[2] & ~PG_PROT) | pr;
			pt[3] = (pt[3] & ~PG_PROT) | pr;
			pt[4] = (pt[4] & ~PG_PROT) | pr;
			pt[5] = (pt[5] & ~PG_PROT) | pr;
			pt[6] = (pt[6] & ~PG_PROT) | pr;
			pt[7] = (pt[7] & ~PG_PROT) | pr;
d1579 25
d1611 1
a1611 2
pmap_activate(p)
	struct proc *p;
d1613 2
a1614 6
	pmap_t pmap;
	struct pcb *pcb;

#ifdef PMAPDEBUG
if(startpmapdebug) printf("pmap_activate: p %p\n", p);
#endif
d1616 3
a1618 2
	pmap = p->p_vmspace->vm_map.pmap;
	pcb = &p->p_addr->u_pcb;
d1621 1
a1621 1
	pcb->P0LR = pmap->pm_p0lr;
d1625 8
d1635 1
a1635 1
		mtpr(pmap->pm_p0lr, PR_P0LR);
d1638 1
d1640 18
a1657 1
	mtpr(0, PR_TBIA);
@


1.60
log
@Create process map holes with UVM_INH_SHARE so that they don't get lost in
fork-without-exec situation (such as privsep'd binaries).
Fixes occasional SIGSEGV in syslogd and pflogd on sun4/4c/4e.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2013/07/07 18:59:36 miod Exp $ */
d305 1
a305 1
 * Here is the resource map for the user page tables inited.
@


1.59
log
@Fix pmap_extract() to actually only return TRUE for mappings which have the
valid bit set; can't believe this went unnoticed for so long.
This fixes uvm_km_pgremove_intrsafe panics when pool_put()'ing items in pools
for larger-than-a-page-items at shutdown time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2013/07/03 19:21:55 miod Exp $ */
d417 1
a417 1
	    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
@


1.58
log
@Do not scribble to a page after uvm_pagefree()'ing it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2013/06/29 13:00:35 miod Exp $ */
d872 1
a872 1
	if ((*kvtopte(&pte[sva]) & PG_FRAME) != 0) {
@


1.57
log
@Minor fixes to let this build with gcc 3.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2013/06/09 15:47:33 miod Exp $ */
d353 1
a354 1
		bzero(kvtopte(pte), sizeof(pt_entry_t) * LTOHPN);
@


1.56
log
@Fix resident_count accounting. Resident count would be incremented when
mappings are created, but only decremented if the pages had actually been
faulted in.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2012/11/25 22:13:46 jsg Exp $ */
d233 2
a234 2
	mtpr(pcb->P1BR = pmap->pm_p1br, PR_P1BR);
	mtpr(pcb->P0BR = pmap->pm_p0br, PR_P0BR);
d1254 1
a1254 1
		mtpr(pmap->pm_p0br, PR_P0BR);
d1256 1
a1256 1
		mtpr(pmap->pm_p1br, PR_P1BR);
@


1.55
log
@remove the use of cast as lvalue which is verboten with newer gcc
tweaks from/tested by/ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2012/04/10 15:50:52 guenther Exp $ */
a99 3
#ifndef PMAPDEBUG
static inline
#endif
a527 1
			pv->pv_pmap->pm_stats.resident_count--;
d568 2
d603 2
a604 1
#ifdef DIAGNOSTIC /* DEBUG */
d761 1
d767 1
a767 1
				pmap->pm_refcnt[index]++; /* New mapping */
d964 1
@


1.54
log
@Make the KERN_NPROCS and KERN_MAXPROC sysctl()s and the RLIMIT_NPROC rlimit
count processes instead of threads.  New sysctl()s KERN_NTHREADS and
KERN_MAXTHREAD count and limit threads.  The nprocs and maxproc kernel
variables are replaced by nprocess, maxprocess, nthreads, and maxthread.

ok tedu@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2011/07/06 18:33:00 miod Exp $ */
d383 2
a384 1
	(vaddr_t)pmap->pm_p1br = (vaddr_t)pmap->pm_p0br + bytesiz - 0x800000;
@


1.53
log
@Remove support for non-microVAX, non-VAXstation, non-VXT hardware in the
VAX port. This means, 11/7xx, VAX6000 and VAX8x00 systems, as well as SBI,
CMI, BI, XMI and MASSBUS drivers.
Most of these files were not being compiled or kept in compilable state over
the years anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2011/05/30 22:25:23 oga Exp $ */
d133 1
a133 1
	sysptsize = (((0x100000 * maxproc) >> VAX_PGSHIFT) / 4);
d137 1
a137 1
	sysptsize += ((USRPTSIZE * 4) / VAX_NBPG) * maxproc;
d139 1
a139 1
	sysptsize += UPAGES * maxproc;
@


1.52
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2011/04/26 23:50:21 ariane Exp $ */
a59 4
/* QDSS console mapping hack */
#include "qd.h"
void	qdearly(void);

a192 5

        /* QDSS console mapping hack */
#if NQD > 0
	qdearly();
#endif
@


1.51
log
@MMU address space holes are at a fixed position (ofcourse).
Therefore set UVM_FLAG_FIXED and enforce this.

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2008/09/30 20:00:29 miod Exp $ */
d260 1
a260 1
	    atop(avail_start), atop(avail_end), VM_FREELIST_DEFAULT);
@


1.50
log
@Fix various pmap_extract() buglets:
- for kernel space addresses, check the page number fits in Sysmap before
  accessing the array.
- for user space addresses, return the right (in-page) address bits.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2008/08/30 20:45:31 martin Exp $ */
d429 2
a430 1
	      UVM_ADV_RANDOM, UVM_FLAG_NOMERGE | UVM_FLAG_HOLE));
@


1.49
log
@replace TRUNC_PAGE by trunc_page and ROUND_PAGE by round_page
plus cast to vaddr_t where necassary

from Miod's todo list

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2008/08/30 18:58:24 martin Exp $ */
d70 1
d124 1
a124 1
	unsigned int sysptsize, i;
a856 1
	paddr_t pa = 0;
d863 2
d866 4
a869 7
		pa = kvtophys(va); /* Is 0 if not mapped */
		*pap = pa;
		return (TRUE);
	}

	sva = PG_PFNUM(va);
	if (va < 0x40000000) {
d878 1
d880 2
a881 1
		*pap = ((pte[sva] & PG_FRAME) << VAX_PGSHIFT);
@


1.48
log
@no need to include uvm/uvm.h twice
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2008/08/18 23:19:29 miod Exp $ */
d177 1
a177 1
	istack = (vaddr_t)Sysmap + ROUND_PAGE(sysptsize * 4);
d215 3
a217 3
	avail_start = ROUND_PAGE(avail_start);
	virtual_avail = ROUND_PAGE(virtual_avail);
	virtual_end = TRUNC_PAGE(virtual_end);
d1019 1
a1019 1
			pte = (pt_entry_t *)TRUNC_PAGE(pte);
@


1.47
log
@Add support for the ``Firefox'' VAXstation 3520/3540/3820/3840 workstations,
currently limited to serial console and a single processor working.

All ``on-board'' devices, including the Q-bus adapter, but except for
the frame buffer, are supported. The machine will boot over the network
or from SCSI devices.

Lots of thanks to Al Kossow for www.bitsavers.org, on which I found the
technical documentation allowing me to complete this port (which was
lacking at the time I got that machine...).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2008/08/15 22:38:23 miod Exp $ */
a58 2

#include <uvm/uvm.h>
@


1.46
log
@Rename the cpu_dep hook ``steal_pages'' to ``init'', as it serves as an
early initialization routine (to enable caches, etc) while still running
physical, and does not allocate memory anymore.

(The irony in this is that forthcoming KA60 support actually steals pages
 in its init function...)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2008/08/14 11:41:30 martin Exp $ */
d188 1
a188 1
	avail_end -= MSGBUFSIZE;
d260 2
a261 3
	uvm_page_physload(avail_start >> PGSHIFT, avail_end >> PGSHIFT,
	    avail_start >> PGSHIFT, avail_end >> PGSHIFT,
	    VM_FREELIST_DEFAULT);
d301 1
a301 1
	 * A vax only have one segment of memory.
@


1.45
log
@convert the last traces of btoc/ctob macros

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2008/06/14 10:55:20 mk Exp $ */
d214 2
a215 2
	if (dep_call->cpu_steal_pages)
		(*dep_call->cpu_steal_pages)();
@


1.44
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2008/06/09 20:31:49 miod Exp $ */
d294 1
a294 1
	npgs = btoc(size);
d393 1
a393 1
	pmap->pm_p0lr = vax_btoc(MAXTSIZ + MAXDSIZ + BRKSIZ) | AST_PCB;
d395 1
a395 1
	pmap->pm_p1lr = vax_btoc(0x40000000 - MAXSSIZ);
@


1.43
log
@Create a real holp by using uvm_map() with UVM_FLAG_HOLE in pmap_remove_holes().
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2008/03/30 18:25:13 miod Exp $ */
d382 1
a382 2
	pmap =  pool_get(&pmap_pmap_pool, PR_WAITOK);
	bzero(pmap, sizeof(struct pmap));
@


1.42
log
@Use vaddr_t instead of void * for some initial virtual memory layout arithmetic.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2007/09/10 18:49:45 miod Exp $ */
d430 3
a432 1
	uvm_map_reserve(map, ehole - shole, UVM_UNKNOWN_OFFSET, 0, &shole);
@


1.41
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2007/05/20 14:14:12 miod Exp $ */
d72 1
a72 1
void	*scratch;
d179 1
a179 1
	istack = (unsigned)Sysmap + ROUND_PAGE(sysptsize * 4);
d184 2
a185 2
	scratch = (void *)((u_int)istack + ISTACK_SIZE);
	avail_start = (u_int)scratch + 4 * VAX_NBPG - KERNBASE;
d224 1
a224 1
	printf("Sysmap %p, istack %lx, scratch %p\n",Sysmap,istack,scratch);
@


1.40
log
@addess -> address
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2007/04/22 10:05:52 miod Exp $ */
d394 1
a394 1
	pmap->pm_p0lr = vax_btoc(MAXTSIZ + 40*1024*1024) | AST_PCB;
d396 1
a396 1
	pmap->pm_p1lr = (0x200000 - vax_btoc(MAXSSIZ));
d410 21
@


1.39
log
@Switch the vax pmap to __HAVE_VM_PAGE_MD.

pv_entry head of lists are no longer preallocated at boot, and will only be
allocated for managed pages (instead of all physical memory pages, including
those containing the kernel).

pmap and pv_entry will now be allocated from a pool, instead of malloc for the
former and a homegrown allocator which never relinquishes unused elements to
the VM system for the latter.

The net result is a slight decrease in memory usage, and better behaviour in
low-memory conditions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2006/07/31 05:44:23 miod Exp $ */
d632 1
a632 1
	/* Find addess of correct pte */
d990 1
a990 1
	/* Set addess on logical page boundary */
@


1.38
log
@Do not mess with pv_tables for I/O mappings, as these are never managed pages;
from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2005/11/24 04:52:23 brad Exp $ */
d43 1
d60 1
a60 1
#include <uvm/uvm_extern.h>
a71 2
struct	pv_entry *pv_table;	/* array of entries, one per LOGICAL page */
int	pventries;
d80 2
a81 1
#define	IOSPACE(p)	(((u_long)(p)) & 0xe0000000)
d108 1
a108 1
void rensa(int, pt_entry_t *);
d113 2
a114 5
void pmap_pinit(pmap_t);
void pmap_release(pmap_t);
struct pv_entry *get_pventry(void);
void free_pventry(struct pv_entry *);
void more_pventries(void);
d185 1
a185 6

	/* Physical-to-virtual translation table */
	(unsigned)pv_table = (u_int)scratch + 4 * VAX_NBPG;

	avail_start = (unsigned)pv_table + (ROUND_PAGE(avail_end >> PGSHIFT)) *
	    sizeof(struct pv_entry) - KERNBASE;
d227 1
a227 2
	printf("pv_table %p, ptemapstart %lx ptemapend %lx\n",
	    pv_table, ptemapstart, ptemapend);
d251 6
d380 1
d382 1
a382 1
	MALLOC(pmap, struct pmap *, sizeof(*pmap), M_VMPMAP, M_WAITOK);
a383 12
	pmap_pinit(pmap);
	return(pmap);
}

/*
 * Initialize a preallocated an zeroed pmap structure,
 */
void
pmap_pinit(pmap)
	pmap_t pmap;
{
	int bytesiz, res;
d393 1
a393 1
		panic("pmap_pinit");
d400 5
a404 3
if (startpmapdebug)
	printf("pmap_pinit(%p): p0br=%p p0lr=0x%lx p1br=%p p1lr=0x%lx\n",
	pmap, pmap->pm_p0br, pmap->pm_p0lr, pmap->pm_p1br, pmap->pm_p1lr);
a407 2
	pmap->pm_stats.resident_count = pmap->pm_stats.wired_count = 0;
}
d409 1
a409 35
/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_release(pmap)
	struct pmap *pmap;
{
#ifdef DEBUG
	vaddr_t saddr, eaddr;
	int i;
#endif

#ifdef PMAPDEBUG
if(startpmapdebug)printf("pmap_release: pmap %p\n",pmap);
#endif

	if (pmap->pm_p0br == 0)
		return;

#ifdef DEBUG
	for (i = 0; i < NPTEPGS; i++)
		if (pmap->pm_refcnt[i])
			panic("pmap_release: refcnt %d index %d", 
			    pmap->pm_refcnt[i], i);

	saddr = (vaddr_t)pmap->pm_p0br;
	eaddr = saddr + USRPTSIZE * sizeof(pt_entry_t);
	for (; saddr < eaddr; saddr += NBPG)
		if ((*kvtopte(saddr) & PG_FRAME) != 0)
			panic("pmap_release: page mapped");
#endif
	extent_free(ptemap, (u_long)pmap->pm_p0br,
	    USRPTSIZE * sizeof(pt_entry_t), EX_WAITOK);
d438 1
a438 3
 * If the pmap is NULL then just return else decrese pm_count.
 * If this was the last reference we call's pmap_relaese to release this pmap.
 * OBS! remember to set pm_lock
a439 1

d445 4
d451 2
a452 1
if(startpmapdebug)printf("pmap_destroy: pmap %p\n",pmap);
a453 2
	if (pmap == NULL)
		return;
d459 18
a476 3
	if (count == 0) {
		pmap_release(pmap);
		FREE((caddr_t)pmap, M_VMPMAP);
d478 2
d487 2
a488 2
rensa(clp, ptp)
	int clp;
d491 12
a502 2
	struct	pv_entry *pf, *pl, *pv = pv_table + clp;
	int	s, *g;
d505 2
a506 2
if (startpmapdebug)
	printf("rensa: pv %p clp 0x%x ptp %p\n", pv, clp, ptp);
a507 2
	if (IOSPACE((*ptp & PG_FRAME) << VAX_PGSHIFT))
		return;	/* Nothing in pv_table */
d510 7
a516 18
	if (pv->pv_pte == ptp) {
		g = (int *)pv->pv_pte;
		if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
			pv->pv_attr |= g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
		pv->pv_pte = 0;
		pv->pv_pmap->pm_stats.resident_count--;
		pv->pv_pmap = 0;
		splx(s);
		RECURSEEND;
		return;
	}
	for (pl = pv; pl->pv_next; pl = pl->pv_next) {
		if (pl->pv_next->pv_pte == ptp) {
			pf = pl->pv_next;
			pl->pv_next = pl->pv_next->pv_next;
			g = (int *)pf->pv_pte;
			if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pv->pv_attr |=
d518 12
a529 5
			pf->pv_pmap->pm_stats.resident_count--;
			free_pventry(pf);
			splx(s);
			RECURSEEND;
			return;
d532 8
a539 1
	panic("rensa");
d592 8
a599 2
		if ((*pte & PG_SREF) == 0)
			rensa((*pte & PG_FRAME) >> LTOHPS, pte);
d618 2
a619 1
	struct	pv_entry *pv, *tmp;
d706 1
d710 2
a711 1
	if (IOSPACE(p)) {
a729 2
	pv = pv_table + (p >> PGSHIFT);

a744 1

d751 1
a751 2
				rensa((oldpte & PG_FRAME) >> LTOHPS,
				    (pt_entry_t *)&patch[i]);
d757 7
a763 9
		if (pv->pv_pte == 0) {
			pv->pv_pte = (pt_entry_t *) & patch[i];
			pv->pv_pmap = pmap;
		} else {
			tmp = get_pventry();
			tmp->pv_pte = (pt_entry_t *)&patch[i];
			tmp->pv_pmap = pmap;
			tmp->pv_next = pv->pv_next;
			pv->pv_next = tmp;
d765 4
d777 1
a777 1
		pv->pv_attr |= PG_V;
d781 1
a781 1
		pv->pv_attr |= PG_M;
a799 2
	if (pventries < 10)
		more_pventries();
d948 1
a948 1
					rensa((*pts & PG_FRAME) >> LTOHPS, pts);
d979 1
a979 1
	struct  pv_entry *pv;
d1019 5
a1023 3
	if (IOSPACE(pa) == 0) {	/* No pv_table fiddling in iospace */
		pv = pv_table + (pa >> PGSHIFT);
		pv->pv_attr |= PG_V; /* Referenced */
d1025 1
a1025 1
			pv->pv_attr |= PG_M;
a1036 8
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;

#ifdef DEBUG
	if (IOSPACE(pa))
		panic("pmap_is_referenced: called for iospace");
#endif
	pv = pv_table + (pa >> PGSHIFT);
d1039 2
a1040 1
		printf("pmap_is_referenced: pa %lx pv_entry %p ", pa, pv);
d1043 1
a1043 1
	if (pv->pv_attr & PG_V)
d1056 2
a1057 3
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;
	int ref = 0;
a1058 5
#ifdef DEBUG
	if (IOSPACE(pa))
		panic("pmap_clear_reference: called for iospace");
#endif
	pv = pv_table + (pa >> PGSHIFT);
d1061 1
a1061 1
		printf("pmap_clear_reference: pa %lx pv_entry %p\n", pa, pv);
d1064 2
a1065 2
	if (pv->pv_attr & PG_V)
		ref++;
d1067 1
a1067 1
	pv->pv_attr &= ~PG_V;
d1070 1
a1070 12
	if (pv->pv_pte && (pv->pv_pte[0] & PG_W) == 0) {
		pv->pv_pte[0] &= ~PG_V;
		pv->pv_pte[1] &= ~PG_V;
		pv->pv_pte[2] &= ~PG_V;
		pv->pv_pte[3] &= ~PG_V;
		pv->pv_pte[4] &= ~PG_V;
		pv->pv_pte[5] &= ~PG_V;
		pv->pv_pte[6] &= ~PG_V;
		pv->pv_pte[7] &= ~PG_V;
	}

	while ((pv = pv->pv_next))
d1081 1
d1093 1
a1093 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;
a1094 5
#ifdef DEBUG
	if (IOSPACE(pa))
		panic("pmap_is_modified: called for iospace");
#endif
	pv = pv_table + (pa >> PGSHIFT);
d1097 2
a1098 1
		printf("pmap_is_modified: pa %lx pv_entry %p ", pa, pv);
d1101 2
a1102 7
	if (pv->pv_attr & PG_M) {
#ifdef PMAPDEBUG
		if (startpmapdebug)
			printf("Yes: (0)\n");
#endif
		return 1;
	}
d1104 5
a1108 14
	if (pv->pv_pte)
		if ((pv->pv_pte[0] & PG_M) != 0 ||
		    (pv->pv_pte[1] & PG_M) != 0 ||
		    (pv->pv_pte[2] & PG_M) != 0 ||
		    (pv->pv_pte[3] & PG_M) != 0 ||
		    (pv->pv_pte[4] & PG_M) != 0 ||
		    (pv->pv_pte[5] & PG_M) != 0 ||
		    (pv->pv_pte[6] & PG_M) != 0 ||
		    (pv->pv_pte[7] & PG_M) != 0) {
#ifdef PMAPDEBUG
			if (startpmapdebug) printf("Yes: (1)\n");
#endif
			return 1;
		}
d1110 1
a1110 19
	while ((pv = pv->pv_next)) {
		if ((pv->pv_pte[0] & PG_M) != 0 ||
		    (pv->pv_pte[1] & PG_M) != 0 ||
		    (pv->pv_pte[2] & PG_M) != 0 ||
		    (pv->pv_pte[3] & PG_M) != 0 ||
		    (pv->pv_pte[4] & PG_M) != 0 ||
		    (pv->pv_pte[5] & PG_M) != 0 ||
		    (pv->pv_pte[6] & PG_M) != 0 ||
		    (pv->pv_pte[7] & PG_M) != 0) {
#ifdef PMAPDEBUG
			if (startpmapdebug) printf("Yes: (2)\n");
#endif
			return 1;
		}
	}
#ifdef PMAPDEBUG
	if (startpmapdebug) printf("No\n");
#endif
	return 0;
d1120 1
a1120 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;
a1122 6
#ifdef DEBUG
	if (IOSPACE(pa))
		panic("pmap_clear_modify: called for iospace");
#endif
	pv = pv_table + (pa >> PGSHIFT);

d1125 1
a1125 1
		printf("pmap_clear_modify: pa %lx pv_entry %p\n", pa, pv);
d1127 1
a1127 1
	if (pv->pv_attr & PG_M)
d1129 1
a1129 1
	pv->pv_attr &= ~PG_M;
d1131 1
a1131 1
	if (pv->pv_pte) {
d1133 2
a1134 2
		    pv->pv_pte[3] | pv->pv_pte[4] | pv->pv_pte[5] |
		    pv->pv_pte[6] | pv->pv_pte[7]) & PG_M)
d1137 9
a1145 15
		pv->pv_pte[0] &= ~PG_M;
		pv->pv_pte[1] &= ~PG_M;
		pv->pv_pte[2] &= ~PG_M;
		pv->pv_pte[3] &= ~PG_M;
		pv->pv_pte[4] &= ~PG_M;
		pv->pv_pte[5] &= ~PG_M;
		pv->pv_pte[6] &= ~PG_M;
		pv->pv_pte[7] &= ~PG_M;
	}

	while ((pv = pv->pv_next)) {
		if ((pv->pv_pte[0] | pv->pv_pte[1] | pv->pv_pte[2] |
		    pv->pv_pte[3] | pv->pv_pte[4] | pv->pv_pte[5] |
		    pv->pv_pte[6] | pv->pv_pte[7]) & PG_M)
			rv = TRUE;
a1146 9
		pv->pv_pte[0] &= ~PG_M;
		pv->pv_pte[1] &= ~PG_M;
		pv->pv_pte[2] &= ~PG_M;
		pv->pv_pte[3] &= ~PG_M;
		pv->pv_pte[4] &= ~PG_M;
		pv->pv_pte[5] &= ~PG_M;
		pv->pv_pte[6] &= ~PG_M;
		pv->pv_pte[7] &= ~PG_M;
	}
d1161 1
a1161 1
	struct	pv_entry *pv, *opv, *pl;
d1163 1
a1163 1
	paddr_t	pa;
d1165 2
a1166 5
if(startpmapdebug) printf("pmap_page_protect: pg %p, prot %x, ",pg, prot);
#endif
	pa = VM_PAGE_TO_PHYS(pg);
#ifdef PMAPDEBUG
if(startpmapdebug) printf("pa %lx\n",pa);
d1169 1
a1169 6
#ifdef DEBUG
	if (IOSPACE(pa))
		panic("pmap_page_protect: called for iospace");
#endif
	pv = pv_table + (pa >> PGSHIFT);
	if (pv->pv_pte == 0 && pv->pv_next == 0)
d1178 7
a1184 4
		g = (int *)pv->pv_pte;
		if (g) {
			if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pv->pv_attr |= 
d1189 1
a1189 1
			pv->pv_pte = 0;
a1190 15
		pl = pv->pv_next;
		pv->pv_pmap = 0;
		pv->pv_next = 0;
		while (pl) {
			g = (int *)pl->pv_pte;
			if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pv->pv_attr |=
				    g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
			bzero(g, sizeof(pt_entry_t) * LTOHPN);
			pl->pv_pmap->pm_stats.resident_count--;
			pmap_decpteref(pl->pv_pmap, pl->pv_pte);
			opv = pl;
			pl = pl->pv_next;
			free_pventry(opv);
		} 
d1193 1
a1193 1
		do {
a1196 3
			if (pt == 0)
				continue;

d1208 1
a1208 1
		} while ((pv = pv->pv_next));
a1245 56
}

struct pv_entry *pv_list;

struct pv_entry *
get_pventry()
{
	struct pv_entry *tmp;
	int s = splvm();

	if (pventries == 0)
		panic("get_pventry");

	tmp = pv_list;
	pv_list = tmp->pv_next;
	pventries--;
	splx(s);
	return tmp;
}

void
free_pventry(pv)
	struct pv_entry *pv;
{
	int s = splvm();

	pv->pv_next = pv_list;
	pv_list = pv;
	pventries++;
	splx(s);
}

void
more_pventries()
{
	struct vm_page *pg;
	struct pv_entry *pv;
	vaddr_t v;
	int s, i, count;

	pg = uvm_pagealloc(NULL, 0, NULL, 0);
	if (pg == 0)
		return;

	v = VM_PAGE_TO_PHYS(pg) | KERNBASE;
	pv = (struct pv_entry *)v;
	count = NBPG/sizeof(struct pv_entry);

	for (i = 0; i < count; i++)
		pv[i].pv_next = &pv[i + 1];

	s = splvm();
	pv[count - 1].pv_next = pv_list;
	pv_list = pv;
	pventries += count;
	splx(s);
@


1.37
log
@splimp -> splvm

ok martin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2005/06/30 21:53:13 deraadt Exp $ */
d65 1
a65 1
#define ISTACK_SIZE NBPG
d81 2
d529 2
d622 1
a622 1
 * upgrades mappings to more "rights". Note that:
d719 17
d757 1
a757 2
	oldpte &= PG_FRAME;
	if ((newpte & PG_FRAME) != oldpte) {
d762 1
a762 1
		if (oldpte) {
d764 3
a766 1
			rensa(oldpte >> LTOHPS, (pt_entry_t *)&patch[i]);
d838 1
a838 1
	(uint)pentry= (((uint)(virtuell)>>VAX_PGSHIFT)*4)+(uint)Sysmap;
d1034 6
a1039 4
	pv = pv_table + (pa >> PGSHIFT);
	pv->pv_attr |= PG_V; /* Referenced */
	if (bits & 4)
		pv->pv_attr |= PG_M; /* (will be) modified. XXX page tables  */
d1053 4
d1080 4
d1132 4
d1195 1
d1197 4
d1207 2
d1212 5
d1228 5
d1242 1
a1242 1
	return TRUE; /* XXX */
d1267 4
@


1.36
log
@oops, used USRPTSIZE incorrectly
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2005/06/29 06:07:07 deraadt Exp $ */
d527 1
a527 1
	s = splimp();
d749 1
a749 1
		s = splimp();
d1223 1
a1223 1
		s = splimp();
d1315 1
a1315 1
	int s = splimp();
d1331 1
a1331 1
	int s = splimp();
d1358 1
a1358 1
	s = splimp();
@


1.35
log
@40MB virtual address space in p0; start mmap's 8MB into the data segment
ok tdeval
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2005/04/04 23:40:05 miod Exp $ */
d406 1
a406 1
	pmap->pm_p0lr = vax_btoc(MAXTSIZ + USRPTSIZE) | AST_PCB;
@


1.34
log
@Nuke pmap_bootstrap_alloc(), not used anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2003/11/10 21:05:06 miod Exp $ */
a137 1
#define USRPTSIZE ((MAXTSIZ + MAXDSIZ + MAXSSIZ + MMAPSPACE) / VAX_NBPG)
d406 1
a406 1
	pmap->pm_p0lr = vax_btoc(MAXTSIZ + MAXDSIZ + MMAPSPACE) | AST_PCB;
@


1.33
log
@Get rid of the "struct pte" bitfield, and use straight integers.

Makes the code slightly more readble, removes casts, and makes some
specific constants defined for the bitfields disappear in pmap.c...

No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2002/09/17 13:36:23 miod Exp $ */
a796 17
}

void *
pmap_bootstrap_alloc(size)
	int size;
{
	void *mem;

#ifdef PMAPDEBUG
if(startpmapdebug)
	printf("pmap_bootstrap_alloc: size 0x %x\n",size);
#endif
	size = round_page(size);
	mem = (caddr_t)avail_start + KERNBASE;
	avail_start += size;
	memset(mem, 0, size);
	return (mem);
@


1.32
log
@Work around a vax-specific problem where mmap()ing large amount of memory
would cause a panic.

To do so, instead of panic'ing in pmap_protect() if the pages which are
supposed to be used for the mapping are out of the affordable pmap limits,
just return, and handle this in pmap_enter, by either returning EFAULT if
pmap_enter has been invoked with PMAP_CANFAIL, or panicing there if there
is no escape.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2002/06/11 09:36:24 hugh Exp $ */
a66 9
/* 
 * This code uses bitfield operators for most page table entries.  
 */
#define PROTSHIFT	27
#define PROT_KW		(PG_KW >> PROTSHIFT)
#define PROT_KR		(PG_KR >> PROTSHIFT) 
#define PROT_RW		(PG_RW >> PROTSHIFT)
#define PROT_RO		(PG_RO >> PROTSHIFT)
#define PROT_URKW	(PG_URKW >> PROTSHIFT)
d70 1
a70 1
struct	pte *Sysmap;		/* System page table */
d101 1
a101 1
void pmap_decpteref(struct pmap *, struct pte *);
d106 1
a106 1
void rensa(int, struct pte *);
d170 1
a170 1
	 * And it doesn't hurt, /netbsd is also public readable.
d175 1
a175 1
		Sysmap[i].pg_prot = PROT_URKW;
d183 1
a183 1
	kvtopte(istack)->pg_v = 0;
d342 1
a342 1
	struct pte *pte;
d351 1
a351 1
	pte = (struct pte *)trunc_page((vaddr_t)pte);
d364 1
a364 1
	if (pmap->pm_refcnt[index] >= VAX_NBPG/sizeof(struct pte))
d368 1
a368 1
		paddr = kvtopte(pte)->pg_pfn << VAX_PGSHIFT;
d370 1
a370 1
		bzero(kvtopte(pte), sizeof(struct pte) * LTOHPN);
d402 1
a402 1
	bytesiz = USRPTSIZE * sizeof(struct pte);
d450 1
a450 1
	eaddr = saddr + USRPTSIZE * sizeof(struct pte);
d452 1
a452 1
		if (kvtopte(saddr)->pg_pfn)
d456 1
a456 1
	    USRPTSIZE * sizeof(struct pte), EX_WAITOK);
d519 1
a519 1
	struct pte *ptp;
d569 1
a569 1
	int *ptp;
d571 1
a571 1
	ptp = (int *)kvtopte(va);
d592 1
a592 1
	struct pte *pte;
d607 1
a607 1
		if (pte->pg_pfn == 0)
d609 3
a611 3
		if (pte->pg_sref == 0)
			rensa(pte->pg_pfn >> LTOHPS, pte);
		bzero(pte, LTOHPN * sizeof(struct pte));
d690 1
a690 1
			if (kvtopte(&patch[i])->pg_pfn)
d745 1
a745 1
			rensa(oldpte >> LTOHPS, (struct pte *)&patch[i]);
d752 1
a752 1
			pv->pv_pte = (struct pte *) & patch[i];
d756 1
a756 1
			tmp->pv_pte = (struct pte *)&patch[i];
d789 1
a789 1
		if (pmap->pm_refcnt[index] > VAX_NBPG/sizeof(struct pte))
d871 1
a871 1
	if (kvtopte(&pte[sva])->pg_pfn) {
d890 2
a891 2
	struct	pte *pt, *pts, *ptd;
	int	pr;
d910 1
a910 1
		pr = (prot & VM_PROT_WRITE ? PROT_KW : PROT_KR);
d943 1
a943 1
		pr = (prot & VM_PROT_WRITE ? PROT_RW : PROT_RO);
d955 1
a955 1
		if (kvtopte(pts)->pg_pfn && *(int *)pts) {
d959 1
a959 1
					rensa(pts->pg_pfn >> LTOHPS, pts);
d961 1
a961 1
				bzero(pts, sizeof(struct pte) * LTOHPN);
d964 8
a971 8
				pts[0].pg_prot = pr;
				pts[1].pg_prot = pr;
				pts[2].pg_prot = pr;
				pts[3].pg_prot = pr;
				pts[4].pg_prot = pr;
				pts[5].pg_prot = pr;
				pts[6].pg_prot = pr;
				pts[7].pg_prot = pr;
d989 1
a989 1
	u_int	*pte;
d1006 1
a1006 1
			pte = (u_int *)mfpr(PR_P1BR);
d1008 1
a1008 1
			pte = (u_int *)mfpr(PR_P0BR);
d1011 2
a1012 2
			pte = (u_int *)TRUNC_PAGE(pte);
			pte = (u_int *)kvtopte(pte);
d1015 1
a1015 1
			pa = (u_int)pte & ~KERNBASE;
d1017 1
a1017 1
			pa = Sysmap[PG_PFNUM(pte)].pg_pfn << VAX_PGSHIFT;
d1019 2
a1020 2
		pte = (u_int *)kvtopte(addr);
		pa = (u_int)pte & ~KERNBASE;
d1082 10
a1091 5
	if (pv->pv_pte && (pv->pv_pte[0].pg_w == 0))
		pv->pv_pte[0].pg_v = pv->pv_pte[1].pg_v = 
		    pv->pv_pte[2].pg_v = pv->pv_pte[3].pg_v = 
		    pv->pv_pte[4].pg_v = pv->pv_pte[5].pg_v = 
		    pv->pv_pte[6].pg_v = pv->pv_pte[7].pg_v = 0;
d1094 10
a1103 5
		if (pv->pv_pte[0].pg_w == 0)
			pv->pv_pte[0].pg_v = pv->pv_pte[1].pg_v =
			    pv->pv_pte[2].pg_v = pv->pv_pte[3].pg_v = 
			    pv->pv_pte[4].pg_v = pv->pv_pte[5].pg_v = 
			    pv->pv_pte[6].pg_v = pv->pv_pte[7].pg_v = 0;
d1133 8
a1140 4
		if ((pv->pv_pte[0].pg_m | pv->pv_pte[1].pg_m
		    | pv->pv_pte[2].pg_m | pv->pv_pte[3].pg_m
		    | pv->pv_pte[4].pg_m | pv->pv_pte[5].pg_m
		    | pv->pv_pte[6].pg_m | pv->pv_pte[7].pg_m)) {
d1148 8
a1155 4
		if ((pv->pv_pte[0].pg_m | pv->pv_pte[1].pg_m
		    | pv->pv_pte[2].pg_m | pv->pv_pte[3].pg_m
		    | pv->pv_pte[4].pg_m | pv->pv_pte[5].pg_m
		    | pv->pv_pte[6].pg_m | pv->pv_pte[7].pg_m)) {
d1186 10
a1195 5
	if (pv->pv_pte)
		pv->pv_pte[0].pg_m = pv->pv_pte[1].pg_m =
		    pv->pv_pte[2].pg_m = pv->pv_pte[3].pg_m = 
		    pv->pv_pte[4].pg_m = pv->pv_pte[5].pg_m = 
		    pv->pv_pte[6].pg_m = pv->pv_pte[7].pg_m = 0;
d1197 10
a1206 5
	while ((pv = pv->pv_next))
		pv->pv_pte[0].pg_m = pv->pv_pte[1].pg_m =
		    pv->pv_pte[2].pg_m = pv->pv_pte[3].pg_m = 
		    pv->pv_pte[4].pg_m = pv->pv_pte[5].pg_m = 
		    pv->pv_pte[6].pg_m = pv->pv_pte[7].pg_m = 0;
d1220 1
a1220 1
	struct	pte *pt;
d1247 1
a1247 1
			bzero(g, sizeof(struct pte) * LTOHPN);
d1260 1
a1260 1
			bzero(g, sizeof(struct pte) * LTOHPN);
d1270 2
d1275 12
a1286 6
			pt[0].pg_prot = pt[1].pg_prot = 
			    pt[2].pg_prot = pt[3].pg_prot = 
			    pt[4].pg_prot = pt[5].pg_prot = 
			    pt[6].pg_prot = pt[7].pg_prot = 
			    ((vaddr_t)pv->pv_pte < ptemapstart ? 
			    PROT_KR : PROT_RO);
@


1.31
log
@New boot code, mostly from ragge's work in NetBSD.
Some header syncing and a couple network drivers came along for the ride.
Assembly files have been renamed from .s to .S to facilitate diffs.
Kernel is backwards compat - with manual interaction.
OpenBSD features have been preserved.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2002/03/14 01:26:49 millert Exp $ */
d660 5
a664 1
			if (i >= (pmap->pm_p0lr & ~AST_MASK))
d666 1
d673 5
a677 1
			if (i < pmap->pm_p1lr)
d679 1
d929 3
a931 2
#ifdef DIAGNOSTIC
			if (((start & 0x3fffffff) >> VAX_PGSHIFT) < pmap->pm_p1lr)
d933 3
d937 1
d942 3
a944 2
#ifdef DIAGNOSTIC
			if ((end >> VAX_PGSHIFT) > (pmap->pm_p0lr & ~AST_MASK))
d946 3
d950 1
@


1.30
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/12/22 12:01:53 hugh Exp $ */
d57 1
d228 1
a228 1
	bzero(0, VAX_NBPG >> 1);
d275 2
@


1.29
log
@Correct res count. Based on fix in NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2001/12/08 02:24:07 art Exp $ */
d109 1
a109 1
void pmap_decpteref __P((struct pmap *, struct pte *));
d114 1
a114 1
void rensa __P((int, struct pte *));
d119 5
a123 5
void pmap_pinit __P((pmap_t));
void pmap_release __P((pmap_t));
struct pv_entry *get_pventry __P((void));
void free_pventry __P((struct pv_entry *));
void more_pventries __P((void));
@


1.28
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/11/28 14:20:16 art Exp $ */
d758 1
a762 1
	pmap->pm_stats.resident_count++;
@


1.28.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/12/22 12:01:53 hugh Exp $ */
a757 1
		pmap->pm_stats.resident_count++;
d762 1
@


1.28.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28.2.1 2002/01/31 22:55:27 niklas Exp $ */
d109 1
a109 1
void pmap_decpteref(struct pmap *, struct pte *);
d114 1
a114 1
void rensa(int, struct pte *);
d119 5
a123 5
void pmap_pinit(pmap_t);
void pmap_release(pmap_t);
struct pv_entry *get_pventry(void);
void free_pventry(struct pv_entry *);
void more_pventries(void);
@


1.28.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28.2.2 2002/06/11 03:39:19 art Exp $ */
a56 1
#include <machine/rpb.h>
d227 1
a227 1
	bcopy((caddr_t)proc0paddr + REDZONEADDR, 0, sizeof(struct rpb));
a273 2
	rpb.sbr = mfpr(PR_SBR);
	rpb.slr = mfpr(PR_SLR);
d657 1
a657 5
			if (i >= (pmap->pm_p0lr & ~AST_MASK)) {
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (EFAULT);
				}
a658 1
			}
d665 1
a665 5
			if (i < pmap->pm_p1lr) {
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (EFAULT);
				}
a666 1
			}
d916 2
a917 3
			if (((start & 0x3fffffff) >> VAX_PGSHIFT) <
			    pmap->pm_p1lr) {
#ifdef PMAPDEBUG
a918 3
#else
				RECURSEEND;
				return;
a919 1
			}
d924 2
a925 3
			if ((end >> VAX_PGSHIFT) >
			    (pmap->pm_p0lr & ~AST_MASK)) {
#ifdef PMAPDEBUG
a926 3
#else
				RECURSEEND;
				return;
a927 1
			}
@


1.27
log
@make pmap_virtual_space madatory in all pmaps.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2001/11/28 14:13:07 art Exp $ */
d709 1
@


1.26
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2001/11/28 13:47:39 art Exp $ */
d275 7
@


1.25
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2001/11/17 05:07:55 hugh Exp $ */
a613 33
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;
	int *ptp;

#ifdef PMAPDEBUG
if(startpmapdebug)
	printf("pmap_kenter_pgs: va: %lx, pgs %p, npgs %x\n", va, pgs, npgs);
#endif

	/*
	 * May this routine affect page tables? 
	 * We assume that, and uses TBIA.
	 */
	ptp = (int *)kvtopte(va);
	for (i = 0 ; i < npgs ; i++) {
		ptp[0] = PG_V | PG_KW |
		    PG_PFNUM(VM_PAGE_TO_PHYS(pgs[i])) | PG_SREF;
		ptp[1] = ptp[0] + 1;
		ptp[2] = ptp[0] + 2;
		ptp[3] = ptp[0] + 3;
		ptp[4] = ptp[0] + 4;
		ptp[5] = ptp[0] + 5;
		ptp[6] = ptp[0] + 6;
		ptp[7] = ptp[0] + 7;
		ptp += LTOHPN;
	}
@


1.24
log
@Pick up changes made in NetBSD to work with ubc.
Unbreaks art's mergings on vax.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2001/11/06 02:49:23 art Exp $ */
d725 1
a725 1
					return (KERN_RESOURCE_SHORTAGE);
d748 1
a748 1
		return (KERN_SUCCESS);
d754 1
a754 1
		return (KERN_SUCCESS);
d817 1
a817 1
	return (KERN_SUCCESS);
@


1.23
log
@remove the last uses of vm/vm_page.h
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2001/09/11 20:05:25 miod Exp $ */
d663 1
d665 1
a698 3
		if (wired)
			 newpte |= PG_W;

d737 2
d796 3
d1079 1
d1087 3
d1093 1
a1093 1
	if (pv->pv_pte)
d1100 5
a1104 4
		pv->pv_pte[0].pg_v = pv->pv_pte[1].pg_v =
		    pv->pv_pte[2].pg_v = pv->pv_pte[3].pg_v = 
		    pv->pv_pte[4].pg_v = pv->pv_pte[5].pg_v = 
		    pv->pv_pte[6].pg_v = pv->pv_pte[7].pg_v = 0;
d1106 1
a1106 1
	return TRUE; /* XXX */
@


1.22
log
@Don't include <vm/vm_kern.h> if you don't need foo_map.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2001/08/21 14:06:25 hugh Exp $ */
d58 1
a58 3
#include <vm/vm.h>
#include <vm/vm_page.h>

@


1.21
log
@Art says it's ok for pmap_enter to effect no mapping change,
so just return success if this happens.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2001/08/11 01:56:18 art Exp $ */
a59 1
#include <vm/vm_kern.h>
@


1.20
log
@Use more sensible malloc type.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2001/07/25 13:25:33 art Exp $ */
a743 4
	/* No mapping change. Not allowed to happen. */
	if (newpte == oldpte)
		panic("pmap_enter onto myself");

d749 6
@


1.19
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2001/07/05 10:00:40 art Exp $ */
a329 1
	 * XXX - M_HTABLE is bogus.
d332 1
a332 1
            M_HTABLE, ptmapstorage, PTMAPSZ, EX_NOCOALESCE);
@


1.18
log
@Get rid of the wrapper macros around extent_alloc*1
Pass the right amount of arguments and rename them back to their right names.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2001/06/08 08:09:32 art Exp $ */
d657 2
a658 2
void
pmap_enter(pmap, v, p, prot, wired, access_type)
d663 1
a663 2
	boolean_t wired;
	vm_prot_t access_type;
d667 1
d671 2
a672 2
	printf("pmap_enter: pmap %p v %lx p %lx prot %x wired %d access %x\n",
		    pmap, v, p, prot, wired, access_type & VM_PROT_ALL);
a673 3
	/* Can this happen with UVM??? */
	if (pmap == 0)
		return;
d728 4
d733 1
a733 4
				if (pmap == pmap_kernel())
					panic("pmap_enter: no free pages");
				else
					uvm_wait("pmap_enter");
d755 1
a755 1
		return;
d790 1
a790 1
	if (access_type & VM_PROT_READ) {
d794 1
a794 1
	if (access_type & VM_PROT_WRITE)
d815 1
a815 1
	return;
@


1.17
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2001/05/16 17:29:39 hugh Exp $ */
d406 1
a406 1
	res = extent_alloc(ptemap, bytesiz, 4, 0, EX_WAITSPACE|EX_WAITOK,
@


1.16
log
@Fix vax to work with recent pmap_change_wiring() -> pmap_unwire()
changes. Mostly from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2001/05/09 15:31:28 art Exp $ */
d863 2
a864 2
paddr_t
pmap_extract(pmap, va)
d867 1
d878 2
a879 1
		return(pa);
d885 1
a885 1
			return NULL;
d889 1
a889 1
			return NULL;
d892 4
a895 2
	if (kvtopte(&pte[sva])->pg_pfn) 
		return ((pte[sva] & PG_FRAME) << VAX_PGSHIFT);
d897 1
a897 1
	return (NULL);
@


1.15
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2001/05/05 21:26:41 art Exp $ */
d354 1
a354 1
	pte = (struct pte *)trunc_page(pte);
a459 1
	mtpr(0, PR_TBIA);
a587 1
	mtpr(0, PR_TBIA);
a650 1
	mtpr(0, PR_TBIA);
d704 3
d747 9
a755 2
	/* No mapping change. Can this happen??? */
	if (newpte == oldpte) {
a756 1
		mtpr(0, PR_TBIA); /* Always; safety belt */
a759 2
	pv = pv_table + (p >> PGSHIFT);

d787 1
a787 1
		/* No mapping change, just flush the TLB */
a791 3
	if(wired) 
		newpte |= PG_W;

a859 1
	mtpr(0,PR_TBIA);
@


1.14
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2001/04/10 06:59:13 niklas Exp $ */
d464 2
a465 2
pmap_change_wiring(pmap, va, wired) 
	register pmap_t pmap;
a466 1
	boolean_t wired;
d484 1
a484 4
	if(wired) 
		*pte |= PG_W;
	else
		*pte &= ~PG_W;
@


1.13
log
@Fix for machines which need to enlarge the kernel address space, at least
1GB i386 machines needs this.  The fix is heavily based on Jason Thorpe's
found in NetBSD.  Here is his original commit message:

Instead of checking vm_physmem[<physseg>].pgs to determine if
uvm_page_init() has completed, add a boolean uvm.page_init_done,
and test against that.  Use this same boolean (rather than
pmap_initialized) in pmap_growkernel() to determine if we are
being called via uvm_page_init() to grow the kernel address space.

This fixes a problem on some i386 configurations where pmap_init()
itself was needing to have the kernel page table grown, and since
pmap_initialized was not yet set to TRUE, pmap_growkernel() was
choosing the wrong code path.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2000/10/11 11:40:17 bjc Exp $ */
d720 1
a720 1
			vaddr_t ptaddr = trunc_page(&patch[i]);
@


1.12
log
@make sure to set PG_W on the pte!
also invalidate tb entries at certain points
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2000/04/27 01:10:13 bjc Exp $ */
d300 5
a307 4
#ifdef DIAGNOSTIC
	if (vm_physmem[0].pgs)
		panic("pmap_steal_memory: called _after_ bootstrap");
#endif
@


1.11
log
@sync with netbsd of early april; some archs still untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1997/10/02 19:51:50 niklas Exp $ */
d753 1
d791 3
d818 1
@


1.10
log
@Accomodate for the buffer cache, mbuf clusters and physio space when
caclulating sysptsize
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.9 1997/09/12 09:30:56 maja Exp $ */
/*	$NetBSD: pmap.c,v 1.37 1997/07/25 21:54:48 ragge Exp $	   */
d4 1
a4 1
 * Copyright (c) 1994 Ludd, University of Lule}, Sweden.
d37 1
a39 1
#include <sys/msgbuf.h>
d42 5
d48 1
a48 3
#include <vm/vm.h>
#include <vm/vm_page.h>
#include <vm/vm_kern.h>
d58 3
a60 1
extern int bufcalc __P((void));
a61 4
static	pt_entry_t *pmap_virt2pte __P((pmap_t, u_int));
static	pv_entry_t alloc_pv_entry __P((void));
static	void	free_pv_entry __P((pv_entry_t));
static	int	remove_pmap_from_mapping __P((pv_entry_t, pmap_t));
d63 15
a77 2
#define ISTACK_SIZE (4 * NBPG)
#define PTE_TO_PV(pte)	(PHYS_TO_PV((pte&PG_FRAME)<<PGSHIFT))
d81 3
a83 11
static int prot_array[]={ PG_NONE, PG_RO,   PG_RW,   PG_RW,
			  PG_RO,   PG_RO,   PG_RW,   PG_RW };
    
static int kernel_prot[]={ PG_NONE, PG_KR, PG_KW, PG_KW,
				PG_RO,PG_KR,PG_KW,PG_URKW};

static pv_entry_t   pv_head = NULL;
static unsigned int pv_count = 0;
pv_entry_t	pv_table;		/* array of entries,
					   one per LOGICAL page */
unsigned *pte_cmap;
d85 19
a106 1
/* extern	int startsysc, faultdebug; */
d109 4
a112 2
unsigned int vmmap;
vm_map_t pte_map;
d114 4
a117 2
vm_offset_t   avail_start, avail_end;
vm_offset_t   virtual_avail, virtual_end; /* Available virtual memory	*/
d119 8
a126 29
/*
 * THIS INFORMATION IS OUTDATED. It's left here just inform curious people.
 *
 * badaddr() doesn't work on some VAXstations 
 * (I've checked KA410 and KA43, don't know about others yet).
 *
 * Checking all pages of physical memory starting from address 0x0 and
 * waiting for being trapped by badaddr() is not enough on these machines:
 *
 * on VS2000/KA410 physical memory appears more than once.
 * eg. on a machine with 10MB memory (2MB base + 8MB extension)
 * the extension memory is mapped to 0x200000, 0xA00000, and so on.
 *
 * On VS3100/KA43 writing to addresses above the available memory
 * is implemented as a nop.
 *
 * On both of these machines the old check/count routine resulted in an
 * endless loop. Thus while checking/counting the memory, we write a 
 * pattern to all the pages we are visiting. (leaving a hole for kernel).
 * If we access a page which already holds a valid pattern, then we've 
 * seen this page already and thus reached the highest memory-address.
 * If the page doesn't hold the pattern directly after having written
 * it, then the page is bad or not available and we've reached the end.
 * 
 * VAXen can't have more than 512(?) MB of physical memory, so we also
 * have an upper limit for how much pages to check. If we're not trapped
 * within this address-range, something went wrong and we're assuming
 * some save amount of physical memory. This might be paranoid, but...
 */
d137 4
a140 19
	unsigned int junk, sysptsize, istack;
	extern	unsigned int proc0paddr, etext;
	extern	struct vmspace vmspace0;
	struct	pmap *p0pmap;

	p0pmap = &vmspace0.vm_pmap;

	/*
	 * Machines older than MicroVAX II have their boot blocks
	 * loaded directly or the boot program loaded from console
	 * media, so we need to figure out their memory size.
	 * This is not easily done on MicroVAXen, so we get it from
	 * VMB instead.
	 */
	if (avail_end == 0)
		while (badaddr((caddr_t)avail_end, 4) == 0)
			avail_end += NBPG * 128;

	avail_end = TRUNC_PAGE(avail_end); /* be sure */
d149 1
d151 1
a151 1
	sysptsize = (((0x100000 * maxproc) >> PGSHIFT) / 4);
d153 1
a153 1
	sysptsize += (avail_end >> PGSHIFT);
d155 1
a155 2
#define	USRPTSIZE ((MAXTSIZ + MAXDSIZ + MAXSSIZ + MMAPSPACE) / NBPG)
	sysptsize += ((USRPTSIZE * 4) / NBPG) * maxproc;
d158 2
a159 6
	/* Buffer cache */
	sysptsize += bufcalc() * CLSIZE;
	/* mbufs */
	sysptsize += VM_MBUF_SIZE / NBPG;
	/* physio space */
	sysptsize += VM_PHYS_SIZE / NBPG;
a162 1
	 * First set them to their max values and then decrement them.
d168 10
a177 4
	virtual_avail = KERNBASE;
	virtual_end = KERNBASE + sysptsize * NBPG;
	avail_start = 0;
	blkclr(Sysmap, sysptsize * 4); /* clear SPT before using it */
d179 3
a181 3
	 * Map kernel. Kernel code is always readable for user,
	 * it must be because of the emulation code that is somewhere
	 * in there. And it doesn't hurt, kernel is also public readable.
d185 2
a186 8
#ifdef DDB
	MAPPHYS(junk, btoc(ROUND_PAGE(&etext)  - KERNBASE),
	    VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE);
#else
	MAPPHYS(junk, btoc(ROUND_PAGE(&etext) - KERNBASE), VM_PROT_EXECUTE);
#endif
	MAPPHYS(junk, btoc((u_int)Sysmap - ROUND_PAGE(&etext)),
	    VM_PROT_READ|VM_PROT_WRITE);
d189 1
a189 3
	mtpr(avail_start, PR_SBR);
	MAPPHYS(junk, btoc(ROUND_PAGE(sysptsize * 4)),
	    VM_PROT_READ|VM_PROT_WRITE);
d192 1
a192 1
	MAPPHYS(istack, btoc(ISTACK_SIZE), VM_PROT_READ|VM_PROT_WRITE);
d196 8
a203 2
	/* Take four pages for scratch use */
	MAPPHYS(scratch, 4, VM_PROT_READ|VM_PROT_WRITE);
d206 6
a211 2
	MAPPHYS(msgbufp, btoc(ROUND_PAGE(sizeof(struct msgbuf))),
	    VM_PROT_READ|VM_PROT_WRITE);
d213 14
a226 3
	/* Physical-to-virtual translation table */
	MAPPHYS(pv_table, btoc((avail_end / PAGE_SIZE ) *
	    sizeof(struct pv_entry)), VM_PROT_READ|VM_PROT_WRITE);
d228 3
a230 2
	/* zero all mapped physical memory from Sysmap to here */
	blkclr((void *)istack, (avail_start | 0x80000000) - istack);
d232 2
a233 3
	/* Now map up what is only needed in virtual memory. */
	MAPVIRT(vmmap, 2);
	(pt_entry_t *)pte_cmap = kvtopte(vmmap);
a234 13
	/*
	 * We move SCB here from physical address 0 to an address
	 * somewhere else, so that we can dynamically allocate
	 * space for interrupt vectors and other machine-specific
	 * things. We move it here, but the rest of the allocation
	 * is done in a cpu-specific routine.
	 * avail_start is modified in the cpu-specific routine.
	 */
	scb = (struct scb *)virtual_avail;
	bcopy(0, (void *)avail_start, NBPG >> 1);
	mtpr(avail_start, PR_SCBB);
	bzero(0, NBPG >> 1);
	(*dep_call->cpu_steal_pages)();
d237 2
d240 4
a243 3
#ifdef PMAPDEBUG
	printf("Sysmap %x, istack %x, scratch %x\n",Sysmap,istack,scratch);
	printf("etext %x\n", &etext);
d245 6
a250 7
	printf("pv_table %x, vmmap %x, pte_cmap %x\n",
		pv_table,vmmap,pte_cmap);
	printf("avail_start %x, avail_end %x\n",avail_start,avail_end);
	printf("virtual_avail %x,virtual_end %x\n",virtual_avail,virtual_end);
	printf("clearomr: %x \n",(uint)vmmap-(uint)Sysmap);
/*	  printf("faultdebug %x, startsysc %x\n",&faultdebug, &startsysc);*/
	printf("startpmapdebug %x\n",&startpmapdebug);
d255 15
a269 12
	pmap_kernel()->ref_count = 1;
	simple_lock_init(&pmap_kernel()->pm_lock);
	p0pmap->pm_pcb = (struct pcb *)proc0paddr;

	p0pmap->pm_pcb->P1BR = (void *)0x80000000;
	p0pmap->pm_pcb->P0BR = (void *)0x80000000;
	p0pmap->pm_pcb->P1LR = 0x200000;
	p0pmap->pm_pcb->P0LR = AST_PCB;
	mtpr(0x80000000, PR_P1BR);
	mtpr(0x80000000, PR_P0BR);
	mtpr(0x200000, PR_P1LR);
	mtpr(AST_PCB, PR_P0LR);
d273 3
d280 38
d322 1
a322 1
 * Here we allocate virtual memory for user page tables.
d325 20
a344 2
pmap_init(start, end) 
	vm_offset_t start, end;
d346 2
a347 1
	vm_offset_t ptemapstart,ptemapend;
d349 25
a373 3
	/* reserve place on SPT for UPT */
	pte_map = kmem_suballoc(kernel_map, &ptemapstart, &ptemapend, 
	    USRPTSIZE * 4 * maxproc, TRUE);
a375 1

d380 17
a396 3
pmap_t 
pmap_create(phys_size)
	vm_size_t phys_size;
d398 15
a412 1
	pmap_t	 pmap;
d415 3
a417 1
if(startpmapdebug)printf("pmap_create: phys_size %x\n",phys_size);
a418 5
	if (phys_size)
		return NULL;

	pmap = (pmap_t) malloc(sizeof(struct pmap), M_VMPMAP, M_WAITOK);
	pmap_pinit(pmap); 
d420 2
a421 1
	return (pmap);
a423 1

d433 5
d439 1
a439 1
if(startpmapdebug)printf("pmap_release: pmap %x\n",pmap);
d442 2
a443 3
	if (pmap->pm_pcb->P0BR)
		kmem_free_wakeup(pte_map, (vm_offset_t)pmap->pm_pcb->P0BR, 
		    (pmap->pm_pcb->P0LR & ~AST_MASK) * 4);
d445 16
a460 3
	if (pmap->pm_pcb->P1BR)
		kmem_free_wakeup(pte_map, (vm_offset_t)pmap->pm_stack,
		    (0x200000 - pmap->pm_pcb->P1LR) * 4);
d462 26
a487 1
	bzero(pmap, sizeof(struct pmap));
a489 1

d504 1
a504 1
if(startpmapdebug)printf("pmap_destroy: pmap %x\n",pmap);
d513 1
a513 1
	if (!count) {
d515 1
a515 1
		free((caddr_t)pmap, M_VMPMAP);
d519 8
a526 7
void 
pmap_enter(pmap, v, p, prot, wired)
	register pmap_t pmap;
	vm_offset_t	v;
	vm_offset_t	p;
	vm_prot_t	prot;
	boolean_t	wired;
d528 2
a529 2
	u_int	i, pte, s, *patch;
	pv_entry_t pv, tmp;
d531 4
a534 2
	if (v > 0x7fffffff) pte = kernel_prot[prot] | PG_PFNUM(p) | PG_V;
	else pte = prot_array[prot] | PG_PFNUM(p) | PG_V;
d536 97
a632 1
	pv = PHYS_TO_PV(p);
d636 1
a636 2
printf("pmap_enter: pmap: %x,virt %x, phys %x,pv %x prot %x\n",
	pmap,v,p,pv,prot);
a637 2
	if (!pmap) return;
	if (wired) pte |= PG_W;
d639 19
d659 31
a689 11
	if (v < 0x40000000) {
		patch = (int *)pmap->pm_pcb->P0BR;
		i = (v >> PGSHIFT);
		if (i >= (pmap->pm_pcb->P0LR&~AST_MASK))
			pmap_expandp0(pmap, i);
		patch = (int *)pmap->pm_pcb->P0BR;
	} else if (v < (u_int)0x80000000) {
		patch = (int *)pmap->pm_pcb->P1BR;
		i = (v - 0x40000000) >> PGSHIFT;
		if (i < pmap->pm_pcb->P1LR)
			panic("pmap_enter: must expand P1");
d691 55
a745 2
		patch = (int *)Sysmap;
		i = (v - (u_int)0x80000000) >> PGSHIFT;
d748 5
a752 23
	if ((patch[i] & PG_FRAME) == (pte & PG_FRAME)) { /* no map change */
		if ((patch[i] & PG_W) != (pte & PG_W)) { /* wiring change */
			pmap_change_wiring(pmap, v, wired);
		} else if ((patch[i] & PG_PROT) != (pte & PG_PROT)) {
			patch[i] &= ~PG_PROT;
			patch[i++] |= prot_array[prot];
			patch[i] &= ~PG_PROT;
			patch[i] |= prot_array[prot];
			mtpr(v, PR_TBIS);
			mtpr(v + NBPG, PR_TBIS);
		} else if ((patch[i] & PG_V) == 0) {
			if (patch[i] & PG_SREF) {
				patch[i] &= ~PG_SREF;
				patch[i] |= PG_V | PG_REF;
			} else patch[i] |= PG_V;
			if (patch[++i] & PG_SREF) {
				patch[i] &= ~PG_SREF;
				patch[i] |= PG_V | PG_REF;
			} else patch[i] |= PG_V;
			mtpr(v, PR_TBIS);
			mtpr(v + NBPG, PR_TBIS);
		} /* else nothing to do */
		splx(s);
d756 28
a783 4
	if (!pv->pv_pmap) {
		pv->pv_pmap = pmap;
		pv->pv_next = NULL;
		pv->pv_va = v;
d785 30
a814 11
		tmp = alloc_pv_entry();
		tmp->pv_pmap = pmap;
		tmp->pv_next = pv->pv_next;
		tmp->pv_va = v;
		pv->pv_next = tmp;
	}
	patch[i++] = pte++;
	patch[i] = pte;
	mtpr(v, PR_TBIS);
	mtpr(v + NBPG, PR_TBIS);
	splx(s);
d825 1
a825 1
printf("pmap_bootstrap_alloc: size 0x %x\n",size);
d828 1
a828 3
	mem = (void *)virtual_avail;
	virtual_avail = pmap_map(virtual_avail, avail_start,
	    avail_start + size, VM_PROT_READ|VM_PROT_WRITE);
d830 1
a830 1
	blkclr(mem, size);
d834 1
a834 1
vm_offset_t
d836 2
a837 1
	vm_offset_t virtuell, pstart, pend;
d840 1
a840 1
	vm_offset_t count;
d845 1
a845 1
	printf("pmap_map: virt %x, pstart %x, pend %x, Sysmap %x\n",
d852 4
a855 3
	(uint)pentry= (((uint)(virtuell)>>PGSHIFT)*4)+(uint)Sysmap;
	for(count=pstart;count<pend;count+=NBPG){
		*pentry++ = (count>>PGSHIFT)|kernel_prot[prot]|PG_V;
d858 1
a858 1
	return(virtuell+(count-pstart)+0x80000000);
d861 1
a861 1
vm_offset_t 
d864 1
a864 1
	vm_offset_t va;
d866 2
a868 1
	int	*pte;
d870 1
a870 1
if(startpmapdebug)printf("pmap_extract: pmap %x, va %x\n",pmap, va);
d873 4
a876 4
	pte=(int *)pmap_virt2pte(pmap,va);
	if(pte) return(((*pte&PG_FRAME)<<PGSHIFT)+((u_int)va&PGOFSET));
	else return 0;
}
d878 9
a886 40
/*
 * pmap_protect( pmap, vstart, vend, protect)
 */
void
pmap_protect(pmap, start, end, prot)
	pmap_t pmap;
	vm_offset_t start;
	vm_offset_t	end;
	vm_prot_t	prot;
{
	int pte, *patch, s;

#ifdef PMAPDEBUG
if(startpmapdebug) printf("pmap_protect: pmap %x, start %x, end %x, prot %x\n",
	pmap, start, end,prot);
#endif
	if(pmap==NULL) return;
	s=splimp();
	if(start>0x7fffffff) pte=kernel_prot[prot];
	else pte=prot_array[prot];

	if(end<0x40000000){
		while((end>>PGSHIFT)>(pmap->pm_pcb->P0LR&~AST_MASK))
			pmap_expandp0(pmap,(end>>PGSHIFT));
	} else if(end<(u_int)0x80000000){
		u_int i;
		i=(start&0x3fffffff)>>PGSHIFT;
		if(i<pmap->pm_pcb->P1LR)
			start=((pmap->pm_pcb->P1LR)<<PGSHIFT)+0x40000000;
		i=(end&0x3fffffff)>>PGSHIFT;
		if(i<pmap->pm_pcb->P1LR) return;
	}
	while (start < end) {
		patch = (int *)pmap_virt2pte(pmap,start);
		if(patch){
			*patch&=(~PG_PROT);
			*patch|=pte;
			mtpr(start,PR_TBIS);
		}
		start += NBPG;
d888 4
a891 2
	mtpr(0,PR_TBIA);
	splx(s);
d895 3
a897 3
 * pmap_remove(pmap, start, slut) removes all valid mappings between
 * the two virtual adresses start and slut from pmap pmap.
 * NOTE: all adresses between start and slut may not be mapped.
a898 1

d900 1
a900 1
pmap_remove(pmap, start, slut)
d902 2
a903 1
	vm_offset_t	start, slut;
d905 2
a906 3
	u_int		*ptestart, *pteslut, s, *temp;
	pv_entry_t	pv;
	vm_offset_t	countup;
d909 2
a910 2
if(startpmapdebug) printf("pmap_remove: pmap=0x %x, start=0x %x, slut=0x %x\n",
	   pmap, start, slut);
d913 1
a913 1
	if (!pmap)
a914 20
	if(!pmap->pm_pcb&&start<0x80000000) return; /* No page registers */
/* First, get pte first address */
	if(start<0x40000000){ /* P0 */
		if (!(temp = (unsigned *)pmap->pm_pcb->P0BR))
			return; /* No page table */
		ptestart=&temp[start>>PGSHIFT];
		pteslut=&temp[slut>>PGSHIFT];
		if(pteslut>&temp[(pmap->pm_pcb->P0LR&~AST_MASK)])
			pteslut=&temp[(pmap->pm_pcb->P0LR&~AST_MASK)];
	} else if(start>0x7fffffff){ /* System region */
		ptestart=(u_int *)&Sysmap[(start&0x3fffffff)>>PGSHIFT];
		pteslut=(u_int *)&Sysmap[(slut&0x3fffffff)>>PGSHIFT];
	} else { /* P1 (stack) region */
		if (!(temp = (unsigned *)pmap->pm_pcb->P1BR))
			return; /* No page table */
		pteslut=&temp[(slut&0x3fffffff)>>PGSHIFT];
		ptestart=&temp[(start&0x3fffffff)>>PGSHIFT];
		if(ptestart<&temp[pmap->pm_pcb->P1LR])
			ptestart=&temp[pmap->pm_pcb->P1LR];
	}
d916 30
a945 3
#ifdef PMAPDEBUG
if(startpmapdebug)
printf("pmap_remove: ptestart %x, pteslut %x, pv %x\n",ptestart, pteslut,pv);
a946 10

	s=splimp();
	for(countup=start;ptestart<pteslut;ptestart+=2, countup+=PAGE_SIZE){

		if(!(*ptestart&PG_FRAME))
			continue; /* not valid phys addr,no mapping */

		pv=PTE_TO_PV(*ptestart);
		if(!remove_pmap_from_mapping(pv,pmap)){
			panic("pmap_remove: pmap not in pv_table");
d948 32
a979 2
		*ptestart=0;
		*(ptestart+1)=0;
d981 1
a982 1
	splx(s);
d985 6
d992 1
a992 3
remove_pmap_from_mapping(pv, pmap)
	pv_entry_t pv;
	pmap_t	pmap;
d994 3
a996 1
	pv_entry_t temp_pv,temp2;
d998 25
a1022 13
	if(!pv->pv_pmap&&pv->pv_next)
		panic("remove_pmap_from_mapping: j{ttefel");

	if(pv->pv_pmap==pmap){
		if(pv->pv_next){
			temp_pv=pv->pv_next;
			pv->pv_pmap=temp_pv->pv_pmap;
			pv->pv_va=temp_pv->pv_va;
			pv->pv_next=temp_pv->pv_next;
			free_pv_entry(temp_pv);
		} else {
			bzero(pv,sizeof(struct pv_entry));
		}
d1024 2
a1025 11
		temp_pv=pv;
		while(temp_pv->pv_next){
			if(temp_pv->pv_next->pv_pmap==pmap){
				temp2=temp_pv->pv_next;
				temp_pv->pv_next=temp2->pv_next;
				free_pv_entry(temp2);
				return 1;
			}
			temp_pv=temp_pv->pv_next;
		}
		return 0;
d1027 13
a1039 1
	return 1;
d1042 6
a1047 59
#ifndef notyet
void 
pmap_copy_page(src, dst)
	vm_offset_t   src;
	vm_offset_t   dst;
{
	int s;
	extern uint vmmap;

#ifdef PMAPDEBUG
if(startpmapdebug)printf("pmap_copy_page: src %x, dst %x\n",src, dst);
#endif
	s=splimp();
	pte_cmap[0]=(src>>PGSHIFT)|PG_V|PG_RO;
	pte_cmap[1]=(dst>>PGSHIFT)|PG_V|PG_KW;
	mtpr(vmmap,PR_TBIS);
	mtpr(vmmap+NBPG,PR_TBIS);
	bcopy((void *)vmmap, (void *)vmmap+NBPG, NBPG);
	pte_cmap[0]=((src+NBPG)>>PGSHIFT)|PG_V|PG_RO;
	pte_cmap[1]=((dst+NBPG)>>PGSHIFT)|PG_V|PG_RW;
	mtpr(vmmap,PR_TBIS);
	mtpr(vmmap+NBPG,PR_TBIS);
	bcopy((void *)vmmap, (void *)vmmap+NBPG, NBPG);
	splx(s);
}
#else
	asm("

_pmap_copy_page:.globl _pmap_copy_page

	.word	64
	mfpr	$0x12, r6
	mtpr	$0x16, $0x12	# splimp();
	movl	_vmmap, r0
	movl	_pte_cmap, r1

	ashl	$-9, 4(ap), r2	# pte_cmap[0]=(src>>PGSHIFT)|PG_V|PG_RO;
	bisl3	$0xf8000000, r2, (r1)
	addl2	$4, r1
	addl3	$1, r2, (r1)+

	ashl	$-9, 8(ap), r2	# pte_cmap[1]=(dst>>PGSHIFT)|PG_V|PG_KW;
	bisl3	$0xa0000000, r2, (r1)
	addl2	$4, r1
	addl3	$1, r2, (r1)

	mtpr	$0, $57		# mtpr(0, PR_TBIA);

	addl3	$1024, r0, r1	# bcopy(vmmap, vmmap + 2 * NBPG, 2 * NBPG);
	movc3	$1024, (r0), (r1)

	mtpr	r6, $0x12
	ret

	");
#endif

pv_entry_t 
alloc_pv_entry()
d1049 2
a1050 1
	pv_entry_t temporary;
d1052 1
a1052 7
	if(!pv_head) {
		temporary=(pv_entry_t)malloc(sizeof(struct pv_entry),
			M_VMPVENT, M_NOWAIT);
#ifdef DIAGNOSTIC
	if (temporary == 0)
		panic("alloc_pv_entry");
#endif
d1054 2
a1055 1
if(startpmapdebug) printf("alloc_pv_entry: %x\n",temporary);
a1056 8
	} else {
		temporary=pv_head;
		pv_head=temporary->pv_next;
		pv_count--;
	}
	bzero(temporary, sizeof(struct pv_entry));
	return temporary;
}
d1058 4
a1061 11
void
free_pv_entry(entry)
	pv_entry_t entry;
{
	if(pv_count>=100) {	 /* Should be a define? */
		free(entry, M_VMPVENT);
	} else {
		entry->pv_next=pv_head;
		pv_head=entry;
		pv_count++;
	}
d1064 3
d1068 2
a1069 2
pmap_is_referenced(pa)
	vm_offset_t	pa;
d1071 2
a1072 2
	struct pv_entry *pv;
	u_int *pte,spte=0;
d1074 5
a1078 1
	pv=PHYS_TO_PV(pa);
d1080 1
a1080 1
	if(!pv->pv_pmap) return 0;
d1082 14
a1095 6
	do {
		pte=(u_int *)pmap_virt2pte(pv->pv_pmap,pv->pv_va);
		spte|=*pte++;
		spte|=*pte;
	} while((pv=pv->pv_next));
	return((spte&PG_REF)?1:0);
d1098 3
d1102 2
a1103 2
pmap_is_modified(pa)
     vm_offset_t     pa;
d1105 2
a1106 2
	struct pv_entry *pv;
	u_int *pte, spte=0;
d1108 5
a1112 9
	pv=PHYS_TO_PV(pa);
	if(!pv->pv_pmap) return 0;
	do {
		pte=(u_int *)pmap_virt2pte(pv->pv_pmap,pv->pv_va);
		spte|=*pte++;
		spte|=*pte;
	} while((pv=pv->pv_next));
	return((spte&PG_M)?1:0);
}
d1114 7
a1120 4
/*
 * Reference bits are simulated and connected to logical pages,
 * not physical. This makes reference simulation much easier.
 */
d1122 5
a1126 10
void 
pmap_clear_reference(pa)
	vm_offset_t	pa;
{
	struct pv_entry *pv;
	int *pte;
/*
 * Simulate page reference bit
 */
	pv=PHYS_TO_PV(pa);
d1128 1
a1128 1
if(startpmapdebug) printf("pmap_clear_reference: pa %x, pv %x\n",pa,pv);
d1130 2
d1133 15
a1147 11
	pv->pv_flags&=~PV_REF;
	if(!pv->pv_pmap) return;

	do {
		pte=(int *)pmap_virt2pte(pv->pv_pmap,pv->pv_va);
		*pte&= ~(PG_REF|PG_V);
		*pte++|=PG_SREF;
		*pte&= ~(PG_REF|PG_V);
		*pte|=PG_SREF;
	} while((pv=pv->pv_next));
	mtpr(0,PR_TBIA);
d1150 6
a1155 3
void 
pmap_clear_modify(pa)
	vm_offset_t	pa;
d1157 2
a1158 2
	struct pv_entry *pv;
	u_int *pte;
d1160 1
a1160 8
	pv=PHYS_TO_PV(pa);
	if(!pv->pv_pmap) return;
	do {
		pte=(u_int *)pmap_virt2pte(pv->pv_pmap,pv->pv_va);
		*pte++&= ~PG_M;
		*pte&= ~PG_M;
	} while((pv=pv->pv_next));
}
a1161 7
void 
pmap_change_wiring(pmap, va, wired)
	register pmap_t pmap;
	vm_offset_t	va;
	boolean_t	wired;
{
	int *pte;
d1163 2
a1164 2
if(startpmapdebug) printf("pmap_change_wiring: pmap %x, va %x, wired %x\n",
	pmap, va, wired);
d1166 1
d1168 12
a1179 4
	pte=(int *)pmap_virt2pte(pmap,va);
	if(!pte) return; /* no pte allocated */
	if(wired) *pte|=PG_W;
	else *pte&=~PG_W;
d1183 3
a1185 3
 *	pmap_page_protect:
 *
 *	Lower the permission for all mappings to a given page.
d1188 12
a1199 7
pmap_page_protect(pa, prot)
	vm_offset_t	pa;
	vm_prot_t	prot;
{
	pv_entry_t pv,opv;
	u_int s,*pte,*pte1,nyprot,kprot;
  
d1201 1
a1201 1
if(startpmapdebug) printf("pmap_page_protect: pa %x, prot %x\n",pa, prot);
a1202 27
	pv = PHYS_TO_PV(pa);
	if(!pv->pv_pmap) return;
	nyprot=prot_array[prot];
	kprot=kernel_prot[prot];

	switch (prot) {

	case VM_PROT_ALL:
		break;
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		do {
			pte=pte1=(int *)pmap_virt2pte(pv->pv_pmap, pv->pv_va);
			s=splimp();
			*pte1++&=~PG_PROT;
			*pte1&=~PG_PROT;
			if(pv->pv_va>0x7fffffff){
				*pte|=kprot;
				*pte1|=kprot;
			} else{
				*pte|=nyprot;
				*pte1|=nyprot;
			}
			splx(s);
		} while((pv=pv->pv_next));
		mtpr(0,PR_TBIA);
		break;
d1204 6
a1209 1
	default:
d1211 2
a1212 1
		pte=(int *)pmap_virt2pte(pv->pv_pmap, pv->pv_va);
d1214 9
a1222 12
		*pte++=0;
		*pte=0;
		opv=pv;
		pv=pv->pv_next;
		bzero(opv,sizeof(struct pv_entry));
		while(pv){
			pte=(int *)pmap_virt2pte(pv->pv_pmap, pv->pv_va);
			*pte++=0;
			*pte=0;
			opv=pv;
			pv=pv->pv_next;
			free_pv_entry(opv);
d1224 15
a1238 2

		mtpr(0,PR_TBIA);
d1240 12
a1251 1
		break;
d1253 2
d1258 4
a1261 4
 *	pmap_zero_page zeros the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bzero to clear its contents, one machine dependent page
 *	at a time.
d1264 2
a1265 2
pmap_zero_page(phys)
	vm_offset_t    phys;
d1267 2
a1268 1
	int	s;
d1271 1
a1271 2
if(startpmapdebug)printf("pmap_zero_page(phys %x, vmmap %x, pte_cmap %x\n",
	phys,vmmap,pte_cmap);
d1273 16
a1288 10
	s = splimp();
	pte_cmap[0] = (phys >> PGSHIFT) | PG_V|PG_KW;
	pte_cmap[1] = pte_cmap[0] + 1;
	mtpr(vmmap, PR_TBIS);
	mtpr(vmmap+ NBPG, PR_TBIS);
	bzero((void *)vmmap, NBPG * 2);
	pte_cmap[0] = pte_cmap[1] = 0;
	mtpr(vmmap, PR_TBIS);
	mtpr(vmmap + NBPG, PR_TBIS);
	splx(s);
d1291 4
a1294 4
pt_entry_t *
pmap_virt2pte(pmap, vaddr)
	pmap_t	pmap;
	u_int	vaddr;
d1296 2
a1297 1
	u_int *pte;
d1299 2
a1300 10
	if (vaddr < 0x40000000) {
		pte = (unsigned *)pmap->pm_pcb->P0BR;
		if ((vaddr >> PGSHIFT) > (pmap->pm_pcb->P0LR & ~AST_MASK))
			return 0;
	} else if (vaddr < (u_int)0x80000000) {
		pte = (unsigned *)pmap->pm_pcb->P1BR;
		if (((vaddr & 0x3fffffff) >> PGSHIFT) < pmap->pm_pcb->P1LR)
			return 0;
	} else
		pte = (u_int *)Sysmap;
d1302 5
a1306 3
	vaddr &= (u_int)0x3fffffff;

	return ((pt_entry_t *)&pte[vaddr >> PGSHIFT]);
d1310 2
a1311 2
pmap_expandp0(pmap, ny_storlek)
	struct	pmap *pmap;
d1313 1
a1313 8
	u_int	tmp, s, size, osize, oaddr, astlvl;

	astlvl = pmap->pm_pcb->P0LR & AST_MASK;
	osize = (pmap->pm_pcb->P0LR & ~AST_MASK) * 4;
	size = ny_storlek * 4;
	tmp = kmem_alloc_wait(pte_map, size);
	if (osize)
		blkcpy(pmap->pm_pcb->P0BR, (void*)tmp, osize);
d1315 3
a1317 7
	s = splimp();
	oaddr = (u_int)pmap->pm_pcb->P0BR;
	mtpr(tmp, PR_P0BR);
	mtpr(((size >> 2) | astlvl), PR_P0LR);
	mtpr(0, PR_TBIA);
	pmap->pm_pcb->P0BR = (void*)tmp;
	pmap->pm_pcb->P0LR = ((size >> 2) | astlvl);
a1318 3

	if(osize)
		kmem_free_wakeup(pte_map, (vm_offset_t)oaddr, osize);
d1322 1
a1322 2
pmap_expandp1(pmap)
	struct	pmap *pmap;
d1324 8
a1331 1
	u_int	tmp, s, size, osize, oaddr;
d1333 3
a1335 3
	osize = 0x800000 - (pmap->pm_pcb->P1LR * 4);
	size = osize + PAGE_SIZE;
	tmp = kmem_alloc_wait(pte_map, size);
d1337 2
a1338 2
	if (osize)
		blkcpy((void*)pmap->pm_stack, (void*)tmp + PAGE_SIZE, osize);
d1341 3
a1343 7
	oaddr = pmap->pm_stack;
	pmap->pm_pcb->P1BR = (void*)(tmp + size - 0x800000);
	pmap->pm_pcb->P1LR = (0x800000 - size) >> 2;
	pmap->pm_stack = tmp;
	mtpr(pmap->pm_pcb->P1BR, PR_P1BR);
	mtpr(pmap->pm_pcb->P1LR, PR_P1LR);
	mtpr(0, PR_TBIA);
a1344 3

	if (osize)
		kmem_free_wakeup(pte_map, (vm_offset_t)oaddr, osize);
@


1.10.12.1
log
@Continue the aborted merge of current just before 2.9 was cut into the
SMP branch.  Note that this will not make any progress of SMP functionality,
it is just merging of new code from the trunk into the old branch.
Please do not ask me questions about SMP status because of this mail,
instead go read the archives of smp@@openbsd.org, where I mailed about
these commits some week ago.  Another note: I am doing this in chunks now,
so as to not lock too much of the tree for long times
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.13 2001/04/10 06:59:13 niklas Exp $ */
/*	$NetBSD: pmap.c,v 1.74 1999/11/13 21:32:25 matt Exp $	   */
d4 1
a4 1
 * Copyright (c) 1994, 1998, 1999 Ludd, University of Lule}, Sweden.
a36 1
#include <sys/extent.h>
d39 1
a41 1
#include <sys/msgbuf.h>
d43 3
a45 5
#ifdef PMAPDEBUG
#include <dev/cons.h>
#endif

#include <uvm/uvm.h>
d55 1
a55 3
#include <vm/vm.h>
#include <vm/vm_page.h>
#include <vm/vm_kern.h>
d57 4
d62 2
a63 15
/* QDSS console mapping hack */
#include "qd.h"
void	qdearly(void);

#define ISTACK_SIZE NBPG
vaddr_t	istack;
/* 
 * This code uses bitfield operators for most page table entries.  
 */
#define PROTSHIFT	27
#define PROT_KW		(PG_KW >> PROTSHIFT)
#define PROT_KR		(PG_KR >> PROTSHIFT) 
#define PROT_RW		(PG_RW >> PROTSHIFT)
#define PROT_RO		(PG_RO >> PROTSHIFT)
#define PROT_URKW	(PG_URKW >> PROTSHIFT)
d67 11
a77 3
struct	pte *Sysmap;		/* System page table */
struct	pv_entry *pv_table;	/* array of entries, one per LOGICAL page */
int	pventries;
a78 19
vaddr_t	iospace;

vaddr_t ptemapstart, ptemapend;
struct	extent *ptemap;
#define	PTMAPSZ	EXTENT_FIXED_STORAGE_SIZE(100)
char	ptmapstorage[PTMAPSZ];

#ifdef PMAPDEBUG
volatile int recurse;
#define RECURSESTART {							\
	if (recurse)							\
		printf("enter at %d, previous %d\n", __LINE__, recurse);\
	recurse = __LINE__;						\
}
#define RECURSEEND {recurse = 0; }
#else
#define RECURSESTART
#define RECURSEEND
#endif
d82 1
d85 2
a86 4
#ifndef DEBUG
static inline
#endif
void pmap_decpteref __P((struct pmap *, struct pte *));
d88 2
a89 4
#ifndef PMAPDEBUG
static inline
#endif
void rensa __P((int, struct pte *));
d91 29
a119 8
vaddr_t   avail_start, avail_end;
vaddr_t   virtual_avail, virtual_end; /* Available virtual memory	*/

void pmap_pinit __P((pmap_t));
void pmap_release __P((pmap_t));
struct pv_entry *get_pventry __P((void));
void free_pventry __P((struct pv_entry *));
void more_pventries __P((void));
d130 19
a148 4
	unsigned int sysptsize, i;
	extern	unsigned int etext, proc0paddr;
	struct pcb *pcb = (struct pcb *)proc0paddr;
	pmap_t pmap = pmap_kernel();
a156 1
#define USRPTSIZE ((MAXTSIZ + MAXDSIZ + MAXSSIZ + MMAPSPACE) / VAX_NBPG)
d158 1
a158 1
	sysptsize = (((0x100000 * maxproc) >> VAX_PGSHIFT) / 4);
d160 1
a160 1
	sysptsize += (avail_end >> VAX_PGSHIFT) * 2;
d162 2
a163 1
	sysptsize += ((USRPTSIZE * 4) / VAX_NBPG) * maxproc;
d166 6
a171 2
	/* IO device register space */
	sysptsize += IOSPSZ;
d175 1
d181 4
a184 10
	virtual_avail = avail_end + KERNBASE;
	virtual_end = KERNBASE + sysptsize * VAX_NBPG;
	memset(Sysmap, 0, sysptsize * 4); /* clear SPT before using it */

	/*
	 * The first part of Kernel Virtual memory is the physical
	 * memory mapped in. This makes some mm routines both simpler
	 * and faster, but takes ~0.75% more memory.
	 */
	pmap_map(KERNBASE, 0, avail_end, VM_PROT_READ|VM_PROT_WRITE);
d186 3
a188 3
	 * Kernel code is always readable for user, it must be because
	 * of the emulation code that is somewhere in there.
	 * And it doesn't hurt, /netbsd is also public readable.
d192 8
a199 2
	for (i = 0; i < ((unsigned)&etext - KERNBASE) >> VAX_PGSHIFT; i++)
		Sysmap[i].pg_prot = PROT_URKW;
d202 3
a204 1
	mtpr((unsigned)Sysmap - KERNBASE, PR_SBR);
d207 1
a207 1
	istack = (unsigned)Sysmap + ROUND_PAGE(sysptsize * 4);
d211 6
a216 2
	/* Some scratch pages */
	scratch = (void *)((u_int)istack + ISTACK_SIZE);
d219 2
a220 9
	(unsigned)pv_table = (u_int)scratch + 4 * VAX_NBPG;

	avail_start = (unsigned)pv_table + (ROUND_PAGE(avail_end >> PGSHIFT)) *
	    sizeof(struct pv_entry) - KERNBASE;

	/* Kernel message buffer */
	avail_end -= MSGBUFSIZE;
	msgbufp = (void *)(avail_end + KERNBASE);
	msgbufp->msg_magic = MSG_MAGIC-1; 	/* ensure that it will be zeroed */
d223 1
a223 1
	memset((void *)istack, 0, (avail_start + KERNBASE) - istack);
d225 3
a227 21
	/* Set logical page size */
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();

        /* QDSS console mapping hack */
#if NQD > 0
	qdearly();
#endif

	/* User page table map. This is big. */
	MAPVIRT(ptemapstart, USRPTSIZE);
	ptemapend = virtual_avail;

	MAPVIRT(iospace, IOSPSZ); /* Device iospace mapping area */

	/* Init SCB and set up stray vectors. */
	avail_start = scb_init(avail_start);
	bzero(0, VAX_NBPG >> 1);

	if (dep_call->cpu_steal_pages)
		(*dep_call->cpu_steal_pages)();
d229 13
a243 1
	virtual_end = TRUNC_PAGE(virtual_end);
d245 3
a247 5

#if 0 /* Breaks cninit() on some machines */
	cninit();
	printf("Sysmap %p, istack %lx, scratch %p\n",Sysmap,istack,scratch);
	printf("etext %p\n", &etext);
d249 7
a255 6
	printf("pv_table %p, ptemapstart %lx ptemapend %lx\n",
	    pv_table, ptemapstart, ptemapend);
	printf("avail_start %lx, avail_end %lx\n",avail_start,avail_end);
	printf("virtual_avail %lx,virtual_end %lx\n",
	    virtual_avail, virtual_end);
	printf("startpmapdebug %p\n",&startpmapdebug);
d260 12
a271 15
	pmap->pm_p1br = (void *)KERNBASE;
	pmap->pm_p0br = (void *)KERNBASE;
	pmap->pm_p1lr = 0x200000;
	pmap->pm_p0lr = AST_PCB;
	pmap->pm_stats.wired_count = pmap->pm_stats.resident_count = 0;
	    /* btop(virtual_avail - KERNBASE); */

	pmap->ref_count = 1;

	/* Activate the kernel pmap. */
	mtpr(pcb->P1BR = pmap->pm_p1br, PR_P1BR);
	mtpr(pcb->P0BR = pmap->pm_p0br, PR_P0BR);
	mtpr(pcb->P1LR = pmap->pm_p1lr, PR_P1LR);
	mtpr(pcb->P0LR = pmap->pm_p0lr, PR_P0LR);

a274 3
	uvm_page_physload(avail_start >> PGSHIFT, avail_end >> PGSHIFT,
	    avail_start >> PGSHIFT, avail_end >> PGSHIFT,
	    VM_FREELIST_DEFAULT);
a278 39
/*
 * Let the VM system do early memory allocation from the direct-mapped
 * physical memory instead.
 */
vaddr_t
pmap_steal_memory(size, vstartp, vendp)
	vsize_t size;
	vaddr_t *vstartp, *vendp;
{
	vaddr_t v;
	int npgs;

#ifdef PMAPDEBUG
	if (startpmapdebug) 
		printf("pmap_steal_memory: size 0x%lx start %p end %p\n",
		    size, vstartp, vendp);
#endif
	size = round_page(size);
	npgs = btoc(size);

#ifdef DIAGNOSTIC
	if (uvm.page_init_done == TRUE)
		panic("pmap_steal_memory: called _after_ bootstrap");
#endif

	/*
	 * A vax only have one segment of memory.
	 */

	v = (vm_physmem[0].avail_start << PGSHIFT) | KERNBASE;
	vm_physmem[0].avail_start += npgs;
	vm_physmem[0].start += npgs;
	if (vstartp)
		*vstartp = virtual_avail;
	if (vendp)
		*vendp = virtual_end;
	bzero((caddr_t)v, size);
	return v;
}
d283 1
a283 1
 * Here is the resource map for the user page tables inited.
d286 2
a287 1
pmap_init() 
d289 5
a293 8
        /*
         * Create the extent map used to manage the page table space.
	 * XXX - M_HTABLE is bogus.
         */
        ptemap = extent_create("ptemap", ptemapstart, ptemapend,
            M_HTABLE, ptmapstorage, PTMAPSZ, EX_NOCOALESCE);
        if (ptemap == NULL)
		panic("pmap_init");
a295 38
/*
 * Decrement a reference to a pte page. If all references are gone,
 * free the page.
 */
void
pmap_decpteref(pmap, pte)
	struct pmap *pmap;
	struct pte *pte;
{
	paddr_t paddr;
	int index;

	if (pmap == pmap_kernel())
		return;
	index = ((vaddr_t)pte - (vaddr_t)pmap->pm_p0br) >> PGSHIFT;

	pte = (struct pte *)trunc_page(pte);
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_decpteref: pmap %p pte %p index %d refcnt %d\n",
		    pmap, pte, index, pmap->pm_refcnt[index]);
#endif

#ifdef DEBUG
	if ((index < 0) || (index >= NPTEPGS))
		panic("pmap_decpteref: bad index %d", index);
#endif
	pmap->pm_refcnt[index]--;
#ifdef DEBUG
	if (pmap->pm_refcnt[index] >= VAX_NBPG/sizeof(struct pte))
		panic("pmap_decpteref");
#endif
	if (pmap->pm_refcnt[index] == 0) {
		paddr = kvtopte(pte)->pg_pfn << VAX_PGSHIFT;
		uvm_pagefree(PHYS_TO_VM_PAGE(paddr));
		bzero(kvtopte(pte), sizeof(struct pte) * LTOHPN);
	}
}
d301 3
a303 2
struct pmap * 
pmap_create()
d305 1
a305 30
	struct pmap *pmap;

	MALLOC(pmap, struct pmap *, sizeof(*pmap), M_VMPMAP, M_WAITOK);
	bzero(pmap, sizeof(struct pmap));
	pmap_pinit(pmap);
	return(pmap);
}

/*
 * Initialize a preallocated an zeroed pmap structure,
 */
void
pmap_pinit(pmap)
	pmap_t pmap;
{
	int bytesiz, res;

	/*
	 * Allocate PTEs and stash them away in the pmap.
	 * XXX Ok to use kmem_alloc_wait() here?
	 */
	bytesiz = USRPTSIZE * sizeof(struct pte);
	res = extent_alloc(ptemap, bytesiz, 4, 0, EX_WAITSPACE|EX_WAITOK,
	    (u_long *)&pmap->pm_p0br);
	if (res)
		panic("pmap_pinit");
	pmap->pm_p0lr = vax_btoc(MAXTSIZ + MAXDSIZ + MMAPSPACE) | AST_PCB;
	(vaddr_t)pmap->pm_p1br = (vaddr_t)pmap->pm_p0br + bytesiz - 0x800000;
	pmap->pm_p1lr = (0x200000 - vax_btoc(MAXSSIZ));
	pmap->pm_stack = USRSTACK;
d308 1
a308 3
if (startpmapdebug)
	printf("pmap_pinit(%p): p0br=%p p0lr=0x%lx p1br=%p p1lr=0x%lx\n",
	pmap, pmap->pm_p0br, pmap->pm_p0lr, pmap->pm_p1br, pmap->pm_p1lr);
d310 5
d316 1
a316 2
	pmap->ref_count = 1;
	pmap->pm_stats.resident_count = pmap->pm_stats.wired_count = 0;
d319 1
d329 2
a330 3
#ifdef DEBUG
	vaddr_t saddr, eaddr;
	int i;
d333 3
a335 3
#ifdef PMAPDEBUG
if(startpmapdebug)printf("pmap_release: pmap %p\n",pmap);
#endif
d337 3
a339 2
	if (pmap->pm_p0br == 0)
		return;
d341 1
a341 15
#ifdef DEBUG
	for (i = 0; i < NPTEPGS; i++)
		if (pmap->pm_refcnt[i])
			panic("pmap_release: refcnt %d index %d", 
			    pmap->pm_refcnt[i], i);

	saddr = (vaddr_t)pmap->pm_p0br;
	eaddr = saddr + USRPTSIZE * sizeof(struct pte);
	for (; saddr < eaddr; saddr += NBPG)
		if (kvtopte(saddr)->pg_pfn)
			panic("pmap_release: page mapped");
#endif
	extent_free(ptemap, (u_long)pmap->pm_p0br,
	    USRPTSIZE * sizeof(struct pte), EX_WAITOK);
	mtpr(0, PR_TBIA);
a343 27
void
pmap_change_wiring(pmap, va, wired) 
	register pmap_t pmap;
	vaddr_t va;
	boolean_t wired;
{
	int *p, *pte, i;

	if (va & KERNBASE) {
		p = (int *)Sysmap;
		i = (va - KERNBASE) >> VAX_PGSHIFT;
	} else { 
		if(va < 0x40000000) {
			p = (int *)pmap->pm_p0br;
			i = va >> VAX_PGSHIFT;
		} else {
			p = (int *)pmap->pm_p1br;
			i = (va - 0x40000000) >> VAX_PGSHIFT;
		}
	}
	pte = &p[i];

	if(wired) 
		*pte |= PG_W;
	else
		*pte &= ~PG_W;
}
d359 1
a359 1
if(startpmapdebug)printf("pmap_destroy: pmap %p\n",pmap);
d368 1
a368 1
	if (count == 0) {
d370 1
a370 1
		FREE((caddr_t)pmap, M_VMPMAP);
d374 7
a380 8
/*
 * Rensa is a help routine to remove a pv_entry from the pv list.
 * Arguments are physical clustering page and page table entry pointer.
 */
void
rensa(clp, ptp)
	int clp;
	struct pte *ptp;
d382 2
a383 2
	struct	pv_entry *pf, *pl, *pv = pv_table + clp;
	int	s, *g;
d385 2
a386 4
#ifdef PMAPDEBUG
if (startpmapdebug)
	printf("rensa: pv %p clp 0x%x ptp %p\n", pv, clp, ptp);
#endif
d388 1
a388 29
	RECURSESTART;
	if (pv->pv_pte == ptp) {
		g = (int *)pv->pv_pte;
		if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
			pv->pv_attr |= g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
		pv->pv_pte = 0;
		pv->pv_pmap->pm_stats.resident_count--;
		pv->pv_pmap = 0;
		splx(s);
		RECURSEEND;
		return;
	}
	for (pl = pv; pl->pv_next; pl = pl->pv_next) {
		if (pl->pv_next->pv_pte == ptp) {
			pf = pl->pv_next;
			pl->pv_next = pl->pv_next->pv_next;
			g = (int *)pf->pv_pte;
			if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pv->pv_attr |=
				    g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
			pf->pv_pmap->pm_stats.resident_count--;
			free_pventry(pf);
			splx(s);
			RECURSEEND;
			return;
		}
	}
	panic("rensa");
}
a389 13
/*
 * New (real nice!) function that allocates memory in kernel space
 * without tracking it in the MD code.
 */
void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t	pa;
	vm_prot_t prot;
{
	int *ptp;

	ptp = (int *)kvtopte(va);
d392 2
a393 1
	printf("pmap_kenter_pa: va: %lx, pa %lx, prot %x ptp %p\n", va, pa, prot, ptp);
d395 2
a396 11
	ptp[0] = PG_V | ((prot & VM_PROT_WRITE)? PG_KW : PG_KR) |
	    PG_PFNUM(pa) | PG_SREF;
	ptp[1] = ptp[0] + 1;
	ptp[2] = ptp[0] + 2;
	ptp[3] = ptp[0] + 3;
	ptp[4] = ptp[0] + 4;
	ptp[5] = ptp[0] + 5;
	ptp[6] = ptp[0] + 6;
	ptp[7] = ptp[0] + 7;
	mtpr(0, PR_TBIA);
}
a397 7
void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	struct pte *pte;
	int i;
d399 12
a410 84
#ifdef PMAPDEBUG
if(startpmapdebug)
	printf("pmap_kremove: va: %lx, len %lx, ptp %p\n", va, len, kvtopte(va));
#endif

	/*
	 * Unfortunately we must check if any page may be on the pv list. 
	 */
	pte = kvtopte(va);
	len >>= PGSHIFT;

	for (i = 0; i < len; i++) {
		if (pte->pg_pfn == 0)
			continue;
		if (pte->pg_sref == 0)
			rensa(pte->pg_pfn >> LTOHPS, pte);
		bzero(pte, LTOHPN * sizeof(struct pte));
		pte += LTOHPN;
	}
	mtpr(0, PR_TBIA);
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;
	int *ptp;

#ifdef PMAPDEBUG
if(startpmapdebug)
	printf("pmap_kenter_pgs: va: %lx, pgs %p, npgs %x\n", va, pgs, npgs);
#endif

	/*
	 * May this routine affect page tables? 
	 * We assume that, and uses TBIA.
	 */
	ptp = (int *)kvtopte(va);
	for (i = 0 ; i < npgs ; i++) {
		ptp[0] = PG_V | PG_KW |
		    PG_PFNUM(VM_PAGE_TO_PHYS(pgs[i])) | PG_SREF;
		ptp[1] = ptp[0] + 1;
		ptp[2] = ptp[0] + 2;
		ptp[3] = ptp[0] + 3;
		ptp[4] = ptp[0] + 4;
		ptp[5] = ptp[0] + 5;
		ptp[6] = ptp[0] + 6;
		ptp[7] = ptp[0] + 7;
		ptp += LTOHPN;
	}
	mtpr(0, PR_TBIA);
}

/*
 * pmap_enter() is the main routine that puts in mappings for pages, or
 * upgrades mappings to more "rights". Note that:
 */
void
pmap_enter(pmap, v, p, prot, wired, access_type)
	pmap_t	pmap;
	vaddr_t	v;
	paddr_t	p;
	vm_prot_t prot;
	boolean_t wired;
	vm_prot_t access_type;
{
	struct	pv_entry *pv, *tmp;
	int	i, s, newpte, oldpte, *patch, index = 0; /* XXX gcc */

#ifdef PMAPDEBUG
if (startpmapdebug)
	printf("pmap_enter: pmap %p v %lx p %lx prot %x wired %d access %x\n",
		    pmap, v, p, prot, wired, access_type & VM_PROT_ALL);
#endif
	/* Can this happen with UVM??? */
	if (pmap == 0)
		return;

	RECURSESTART;
	/* Find addess of correct pte */
	if (v & KERNBASE) {
d412 1
a412 58
		i = (v - KERNBASE) >> VAX_PGSHIFT;
		newpte = (p>>VAX_PGSHIFT)|(prot&VM_PROT_WRITE?PG_KW:PG_KR);
	} else {
		if (v < 0x40000000) {
			patch = (int *)pmap->pm_p0br;
			i = (v >> VAX_PGSHIFT);
			if (i >= (pmap->pm_p0lr & ~AST_MASK))
				panic("P0 too small in pmap_enter");
			patch = (int *)pmap->pm_p0br;
			newpte = (p >> VAX_PGSHIFT) |
			    (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
		} else {
			patch = (int *)pmap->pm_p1br;
			i = (v - 0x40000000) >> VAX_PGSHIFT;
			if (i < pmap->pm_p1lr)
				panic("pmap_enter: must expand P1");
			if (v < pmap->pm_stack)
				pmap->pm_stack = v;
			newpte = (p >> VAX_PGSHIFT) |
			    (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
		}

		/*
		 * Check if a pte page must be mapped in.
		 */
		index = ((u_int)&patch[i] - (u_int)pmap->pm_p0br) >> PGSHIFT;
#ifdef DIAGNOSTIC
		if ((index < 0) || (index >= NPTEPGS))
			panic("pmap_enter: bad index %d", index);
#endif
		if (pmap->pm_refcnt[index] == 0) {
			vaddr_t ptaddr = trunc_page(&patch[i]);
			paddr_t phys;
			struct vm_page *pg;
#ifdef DEBUG
			if (kvtopte(&patch[i])->pg_pfn)
				panic("pmap_enter: refcnt == 0");
#endif
			/*
			 * It seems to be legal to sleep here to wait for
			 * pages; at least some other ports do so.
			 */
			for (;;) {
				pg = uvm_pagealloc(NULL, 0, NULL, 0);
				if (pg != NULL)
					break;

				if (pmap == pmap_kernel())
					panic("pmap_enter: no free pages");
				else
					uvm_wait("pmap_enter");
			}

			phys = VM_PAGE_TO_PHYS(pg);
			bzero((caddr_t)(phys|KERNBASE), NBPG);
			pmap_kenter_pa(ptaddr, phys,
			    VM_PROT_READ|VM_PROT_WRITE);
		}
d415 23
a437 6
	oldpte = patch[i] & ~(PG_V|PG_M);

	/* No mapping change. Can this happen??? */
	if (newpte == oldpte) {
		RECURSEEND;
		mtpr(0, PR_TBIA); /* Always; safety belt */
d441 4
a444 28
	pv = pv_table + (p >> PGSHIFT);

	/* Changing mapping? */
	oldpte &= PG_FRAME;
	if ((newpte & PG_FRAME) != oldpte) {

		/*
		 * Mapped before? Remove it then.
		 */
		if (oldpte) {
			RECURSEEND;
			rensa(oldpte >> LTOHPS, (struct pte *)&patch[i]);
			RECURSESTART;
		} else if (pmap != pmap_kernel())
				pmap->pm_refcnt[index]++; /* New mapping */

		s = splimp();
		if (pv->pv_pte == 0) {
			pv->pv_pte = (struct pte *) & patch[i];
			pv->pv_pmap = pmap;
		} else {
			tmp = get_pventry();
			tmp->pv_pte = (struct pte *)&patch[i];
			tmp->pv_pmap = pmap;
			tmp->pv_next = pv->pv_next;
			pv->pv_next = tmp;
		}
		splx(s);
d446 11
a456 34
		/* No mapping change, just flush the TLB */
		mtpr(0, PR_TBIA);
	}
	pmap->pm_stats.resident_count++;

	if(wired) 
		newpte |= PG_W;

	if (access_type & VM_PROT_READ) {
		pv->pv_attr |= PG_V;
		newpte |= PG_V;
	}
	if (access_type & VM_PROT_WRITE)
		pv->pv_attr |= PG_M;

	patch[i] = newpte;
	patch[i+1] = newpte+1;
	patch[i+2] = newpte+2;
	patch[i+3] = newpte+3;
	patch[i+4] = newpte+4;
	patch[i+5] = newpte+5;
	patch[i+6] = newpte+6;
	patch[i+7] = newpte+7;
	RECURSEEND;
#ifdef DEBUG
	if (pmap != pmap_kernel())
		if (pmap->pm_refcnt[index] > VAX_NBPG/sizeof(struct pte))
			panic("pmap_enter: refcnt %d", pmap->pm_refcnt[index]);
#endif
	if (pventries < 10)
		more_pventries();

	mtpr(0, PR_TBIA); /* Always; safety belt */
	return;
d467 1
a467 1
	printf("pmap_bootstrap_alloc: size 0x %x\n",size);
d470 3
a472 1
	mem = (caddr_t)avail_start + KERNBASE;
d474 1
a474 1
	memset(mem, 0, size);
d478 1
a478 1
vaddr_t
d480 1
a480 2
	vaddr_t virtuell;
	paddr_t	pstart, pend;
d483 1
a483 1
	vaddr_t count;
d488 1
a488 1
	printf("pmap_map: virt %lx, pstart %lx, pend %lx, Sysmap %p\n",
d495 3
a497 4
	(uint)pentry= (((uint)(virtuell)>>VAX_PGSHIFT)*4)+(uint)Sysmap;
	for(count=pstart;count<pend;count+=VAX_NBPG){
		*pentry++ = (count>>VAX_PGSHIFT)|PG_V|
		    (prot & VM_PROT_WRITE ? PG_KW : PG_KR);
d500 1
a500 1
	return(virtuell+(count-pstart)+KERNBASE);
d503 1
a503 1
paddr_t
d506 1
a506 1
	vaddr_t va;
a507 2
	paddr_t pa = 0;
	int	*pte, sva;
d509 1
d511 1
a511 1
if(startpmapdebug)printf("pmap_extract: pmap %p, va %lx\n",pmap, va);
d514 16
a529 4
	if (va & KERNBASE) {
		pa = kvtophys(va); /* Is 0 if not mapped */
		return(pa);
	}
d531 28
a558 9
	sva = PG_PFNUM(va);
	if (va < 0x40000000) {
		if (sva > (pmap->pm_p0lr & ~AST_MASK))
			return NULL;
		pte = (int *)pmap->pm_p0br;
	} else {
		if (sva < pmap->pm_p1lr)
			return NULL;
		pte = (int *)pmap->pm_p1br;
d560 2
a561 4
	if (kvtopte(&pte[sva])->pg_pfn) 
		return ((pte[sva] & PG_FRAME) << VAX_PGSHIFT);
	
	return (NULL);
d565 3
a567 3
 * Sets protection for a given region to prot. If prot == none then
 * unmap region. pmap_remove is implemented as pmap_protect with
 * protection none.
d569 1
d571 1
a571 1
pmap_protect(pmap, start, end, prot)
d573 1
a573 2
	vaddr_t	start, end;
	vm_prot_t prot;
d575 3
a577 2
	struct	pte *pt, *pts, *ptd;
	int	pr;
d580 2
a581 2
if(startpmapdebug) printf("pmap_protect: pmap %p, start %lx, end %lx, prot %x\n",
	pmap, start, end,prot);
d584 1
a584 1
	if (pmap == 0)
d586 20
d607 3
a609 30
	RECURSESTART;
	if (start & KERNBASE) { /* System space */
		pt = Sysmap;
#ifdef DIAGNOSTIC
		if (((end & 0x3fffffff) >> VAX_PGSHIFT) > mfpr(PR_SLR))
			panic("pmap_protect: outside SLR: %lx", end);
#endif
		start &= ~KERNBASE;
		end &= ~KERNBASE;
		pr = (prot & VM_PROT_WRITE ? PROT_KW : PROT_KR);
	} else {
		if (start & 0x40000000) { /* P1 space */
			if (end <= pmap->pm_stack) {
				RECURSEEND;
				return;
			}
			if (start < pmap->pm_stack)
				start = pmap->pm_stack;
			pt = pmap->pm_p1br;
#ifdef DIAGNOSTIC
			if (((start & 0x3fffffff) >> VAX_PGSHIFT) < pmap->pm_p1lr)
				panic("pmap_protect: outside P1LR");
#endif
			start &= 0x3fffffff;
			end = (end == KERNBASE ? end >> 1 : end & 0x3fffffff);
		} else { /* P0 space */
			pt = pmap->pm_p0br;
#ifdef DIAGNOSTIC
			if ((end >> VAX_PGSHIFT) > (pmap->pm_p0lr & ~AST_MASK))
				panic("pmap_protect: outside P0LR");
d611 10
d622 2
a623 1
		pr = (prot & VM_PROT_WRITE ? PROT_RW : PROT_RO);
d625 32
a656 27
	pts = &pt[start >> VAX_PGSHIFT];
	ptd = &pt[end >> VAX_PGSHIFT];
#ifdef DEBUG
	if (((int)pts - (int)pt) & 7)
		panic("pmap_remove: pts not even");
	if (((int)ptd - (int)pt) & 7)
		panic("pmap_remove: ptd not even");
#endif

	while (pts < ptd) {
		if (kvtopte(pts)->pg_pfn && *(int *)pts) {
			if (prot == VM_PROT_NONE) {
				RECURSEEND;
				if ((*(int *)pts & PG_SREF) == 0)
					rensa(pts->pg_pfn >> LTOHPS, pts);
				RECURSESTART;
				bzero(pts, sizeof(struct pte) * LTOHPN);
				pmap_decpteref(pmap, pts);
			} else {
				pts[0].pg_prot = pr;
				pts[1].pg_prot = pr;
				pts[2].pg_prot = pr;
				pts[3].pg_prot = pr;
				pts[4].pg_prot = pr;
				pts[5].pg_prot = pr;
				pts[6].pg_prot = pr;
				pts[7].pg_prot = pr;
d658 1
d660 1
a660 1
		pts += LTOHPN;
d662 27
a688 2
	RECURSEEND;
	mtpr(0,PR_TBIA);
d690 10
d701 23
a723 8
int pmap_simulref(int bits, int addr);
/*
 * Called from interrupt vector routines if we get a page invalid fault.
 * Note: the save mask must be or'ed with 0x3f for this function.
 * Returns 0 if normal call, 1 if CVAX bug detected.
 */
int
pmap_simulref(int bits, int addr)
d725 1
a725 3
	u_int	*pte;
	struct  pv_entry *pv;
	paddr_t	pa;
d727 7
d735 1
a735 2
if (startpmapdebug) 
	printf("pmap_simulref: bits %x addr %x\n", bits, addr);
a736 21
#ifdef DEBUG
	if (bits & 1)
		panic("pte trans len");
#endif
	/* Set addess on logical page boundary */
	addr &= ~PGOFSET;
	/* First decode userspace addr */
	if (addr >= 0) {
		if ((addr << 1) < 0)
			pte = (u_int *)mfpr(PR_P1BR);
		else
			pte = (u_int *)mfpr(PR_P0BR);
		pte += PG_PFNUM(addr);
		if (bits & 2) { /* PTE reference */
			pte = (u_int *)TRUNC_PAGE(pte);
			pte = (u_int *)kvtopte(pte);
			if (pte[0] == 0) /* Check for CVAX bug */
				return 1;	
			pa = (u_int)pte & ~KERNBASE;
		} else
			pa = Sysmap[PG_PFNUM(pte)].pg_pfn << VAX_PGSHIFT;
d738 18
a755 2
		pte = (u_int *)kvtopte(addr);
		pa = (u_int)pte & ~KERNBASE;
a756 13
	pte[0] |= PG_V;
	pte[1] |= PG_V;
	pte[2] |= PG_V;
	pte[3] |= PG_V;
	pte[4] |= PG_V;
	pte[5] |= PG_V;
	pte[6] |= PG_V;
	pte[7] |= PG_V;
	pv = pv_table + (pa >> PGSHIFT);
	pv->pv_attr |= PG_V; /* Referenced */
	if (bits & 4)
		pv->pv_attr |= PG_M; /* (will be) modified. XXX page tables  */
	return 0;
a758 3
/*
 * Checks if page is referenced; returns true or false depending on result.
 */
d760 2
a761 2
pmap_is_referenced(pg)
	struct vm_page *pg;
d763 2
a764 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;
d766 1
a766 5
	pv = pv_table + (pa >> PGSHIFT);
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_is_referenced: pa %lx pv_entry %p ", pa, pv);
#endif
d768 1
a768 2
	if (pv->pv_attr & PG_V)
		return 1;
d770 6
a775 1
	return 0;
a777 3
/*
 * Clears valid bit in all ptes referenced to this physical page.
 */
d779 2
a780 2
pmap_clear_reference(pg)
	struct vm_page *pg;
d782 2
a783 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;
d785 8
a792 22
	pv = pv_table + (pa >> PGSHIFT);
#ifdef PMAPDEBUG
	if (startpmapdebug)
		printf("pmap_clear_reference: pa %lx pv_entry %p\n", pa, pv);
#endif

	pv->pv_attr &= ~PG_V;

	RECURSESTART;
	if (pv->pv_pte)
		pv->pv_pte[0].pg_v = pv->pv_pte[1].pg_v = 
		    pv->pv_pte[2].pg_v = pv->pv_pte[3].pg_v = 
		    pv->pv_pte[4].pg_v = pv->pv_pte[5].pg_v = 
		    pv->pv_pte[6].pg_v = pv->pv_pte[7].pg_v = 0;

	while ((pv = pv->pv_next))
		pv->pv_pte[0].pg_v = pv->pv_pte[1].pg_v =
		    pv->pv_pte[2].pg_v = pv->pv_pte[3].pg_v = 
		    pv->pv_pte[4].pg_v = pv->pv_pte[5].pg_v = 
		    pv->pv_pte[6].pg_v = pv->pv_pte[7].pg_v = 0;
	RECURSEEND;
	return TRUE; /* XXX */
d796 2
a797 1
 * Checks if page is modified; returns true or false depending on result.
d799 4
a802 3
boolean_t
pmap_is_modified(pg)
	struct vm_page *pg;
d804 6
a809 4
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;

	pv = pv_table + (pa >> PGSHIFT);
d811 1
a811 2
	if (startpmapdebug)
		printf("pmap_is_modified: pa %lx pv_entry %p ", pa, pv);
d814 2
a815 7
	if (pv->pv_attr & PG_M) {
#ifdef PMAPDEBUG
		if (startpmapdebug)
			printf("Yes: (0)\n");
#endif
		return 1;
	}
d817 8
a824 26
	if (pv->pv_pte)
		if ((pv->pv_pte[0].pg_m | pv->pv_pte[1].pg_m
		    | pv->pv_pte[2].pg_m | pv->pv_pte[3].pg_m
		    | pv->pv_pte[4].pg_m | pv->pv_pte[5].pg_m
		    | pv->pv_pte[6].pg_m | pv->pv_pte[7].pg_m)) {
#ifdef PMAPDEBUG
			if (startpmapdebug) printf("Yes: (1)\n");
#endif
			return 1;
		}

	while ((pv = pv->pv_next)) {
		if ((pv->pv_pte[0].pg_m | pv->pv_pte[1].pg_m
		    | pv->pv_pte[2].pg_m | pv->pv_pte[3].pg_m
		    | pv->pv_pte[4].pg_m | pv->pv_pte[5].pg_m
		    | pv->pv_pte[6].pg_m | pv->pv_pte[7].pg_m)) {
#ifdef PMAPDEBUG
			if (startpmapdebug) printf("Yes: (2)\n");
#endif
			return 1;
		}
	}
#ifdef PMAPDEBUG
	if (startpmapdebug) printf("No\n");
#endif
	return 0;
d827 3
a829 6
/*
 * Clears modify bit in all ptes referenced to this physical page.
 */
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
d831 2
a832 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct	pv_entry *pv;
d834 8
a841 1
	pv = pv_table + (pa >> PGSHIFT);
d843 7
d851 2
a852 2
	if (startpmapdebug)
		printf("pmap_clear_modify: pa %lx pv_entry %p\n", pa, pv);
a853 1
	pv->pv_attr &= ~PG_M;
d855 4
a858 12
	if (pv->pv_pte)
		pv->pv_pte[0].pg_m = pv->pv_pte[1].pg_m =
		    pv->pv_pte[2].pg_m = pv->pv_pte[3].pg_m = 
		    pv->pv_pte[4].pg_m = pv->pv_pte[5].pg_m = 
		    pv->pv_pte[6].pg_m = pv->pv_pte[7].pg_m = 0;

	while ((pv = pv->pv_next))
		pv->pv_pte[0].pg_m = pv->pv_pte[1].pg_m =
		    pv->pv_pte[2].pg_m = pv->pv_pte[3].pg_m = 
		    pv->pv_pte[4].pg_m = pv->pv_pte[5].pg_m = 
		    pv->pv_pte[6].pg_m = pv->pv_pte[7].pg_m = 0;
	return TRUE; /* XXX */
d862 3
a864 3
 * Lower the permission for all mappings to a given page.
 * Lower permission can only mean setting protection to either read-only
 * or none; where none is unmapping of the page.
d867 7
a873 8
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t       prot;
{
	struct	pte *pt;
	struct	pv_entry *pv, *opv, *pl;
	int	s, *g;
	paddr_t	pa;
d875 1
a875 1
if(startpmapdebug) printf("pmap_page_protect: pg %p, prot %x, ",pg, prot);
d877 27
a903 8
	pa = VM_PAGE_TO_PHYS(pg);
#ifdef PMAPDEBUG
if(startpmapdebug) printf("pa %lx\n",pa);
#endif

	pv = pv_table + (pa >> PGSHIFT);
	if (pv->pv_pte == 0 && pv->pv_next == 0)
		return;
d905 1
a905 2
	if (prot == VM_PROT_ALL) /* 'cannot happen' */
		return;
d907 1
a907 2
	RECURSESTART;
	if (prot == VM_PROT_NONE) {
d909 12
a920 9
		g = (int *)pv->pv_pte;
		if (g) {
			if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pv->pv_attr |= 
				    g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
			bzero(g, sizeof(struct pte) * LTOHPN);
			pv->pv_pmap->pm_stats.resident_count--;
			pmap_decpteref(pv->pv_pmap, pv->pv_pte);
			pv->pv_pte = 0;
d922 2
a923 15
		pl = pv->pv_next;
		pv->pv_pmap = 0;
		pv->pv_next = 0;
		while (pl) {
			g = (int *)pl->pv_pte;
			if ((pv->pv_attr & (PG_V|PG_M)) != (PG_V|PG_M))
				pv->pv_attr |=
				    g[0]|g[1]|g[2]|g[3]|g[4]|g[5]|g[6]|g[7];
			bzero(g, sizeof(struct pte) * LTOHPN);
			pl->pv_pmap->pm_stats.resident_count--;
			pmap_decpteref(pl->pv_pmap, pl->pv_pte);
			opv = pl;
			pl = pl->pv_next;
			free_pventry(opv);
		} 
d925 1
a925 12
	} else { /* read-only */
		do {
			pt = pv->pv_pte;
			if (pt == 0)
				continue;
			pt[0].pg_prot = pt[1].pg_prot = 
			    pt[2].pg_prot = pt[3].pg_prot = 
			    pt[4].pg_prot = pt[5].pg_prot = 
			    pt[6].pg_prot = pt[7].pg_prot = 
			    ((vaddr_t)pv->pv_pte < ptemapstart ? 
			    PROT_KR : PROT_RO);
		} while ((pv = pv->pv_next));
a926 2
	RECURSEEND;
	mtpr(0, PR_TBIA);
d930 4
a933 4
 * Activate the address space for the specified process.
 * Note that if the process to activate is the current process, then
 * the processor internal registers must also be loaded; otherwise
 * the current process will have wrong pagetables.
d936 2
a937 2
pmap_activate(p)
	struct proc *p;
d939 1
a939 2
	pmap_t pmap;
	struct pcb *pcb;
d942 2
a943 1
if(startpmapdebug) printf("pmap_activate: p %p\n", p);
d945 10
a954 16

	pmap = p->p_vmspace->vm_map.pmap;
	pcb = &p->p_addr->u_pcb;

	pcb->P0BR = pmap->pm_p0br;
	pcb->P0LR = pmap->pm_p0lr;
	pcb->P1BR = pmap->pm_p1br;
	pcb->P1LR = pmap->pm_p1lr;

	if (p == curproc) {
		mtpr(pmap->pm_p0br, PR_P0BR);
		mtpr(pmap->pm_p0lr, PR_P0LR);
		mtpr(pmap->pm_p1br, PR_P1BR);
		mtpr(pmap->pm_p1lr, PR_P1LR);
	}
	mtpr(0, PR_TBIA);
d957 6
a962 1
struct pv_entry *pv_list;
d964 10
a973 5
struct pv_entry *
get_pventry()
{
	struct pv_entry *tmp;
	int s = splimp();
d975 1
a975 2
	if (pventries == 0)
		panic("get_pventry");
d977 1
a977 5
	tmp = pv_list;
	pv_list = tmp->pv_next;
	pventries--;
	splx(s);
	return tmp;
d981 2
a982 2
free_pventry(pv)
	struct pv_entry *pv;
d984 8
a991 1
	int s = splimp();
d993 7
a999 3
	pv->pv_next = pv_list;
	pv_list = pv;
	pventries++;
d1001 3
d1007 2
a1008 1
more_pventries()
d1010 1
a1010 8
	struct vm_page *pg;
	struct pv_entry *pv;
	vaddr_t v;
	int s, i, count;

	pg = uvm_pagealloc(NULL, 0, NULL, 0);
	if (pg == 0)
		return;
d1012 3
a1014 3
	v = VM_PAGE_TO_PHYS(pg) | KERNBASE;
	pv = (struct pv_entry *)v;
	count = NBPG/sizeof(struct pv_entry);
d1016 2
a1017 2
	for (i = 0; i < count; i++)
		pv[i].pv_next = &pv[i + 1];
d1020 7
a1026 3
	pv[count - 1].pv_next = pv_list;
	pv_list = pv;
	pventries += count;
d1028 3
@


1.10.12.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10.12.1 2001/05/14 21:39:06 niklas Exp $ */
d354 1
a354 1
	pte = (struct pte *)trunc_page((vaddr_t)pte);
d460 1
d464 2
a465 2
pmap_unwire(pmap, va)
	pmap_t pmap;
d467 1
d485 4
a488 1
	*pte &= ~PG_W;
d593 1
d657 1
a710 3
		if (wired)
			 newpte |= PG_W;

d720 1
a720 1
			vaddr_t ptaddr = trunc_page((vaddr_t)&patch[i]);
d751 2
a752 9
	/* No mapping change. Not allowed to happen. */
	if (newpte == oldpte)
		panic("pmap_enter onto myself");

	pv = pv_table + (p >> PGSHIFT);

	/* wiring change? */
	if (newpte == (oldpte | PG_W)) {
		patch[i] |= PG_W; /* Just wiring change */
d754 1
d758 2
d787 1
a787 1
		/* No mapping change, just flush the TLB; necessary? */
d792 3
d863 1
d867 2
a868 2
boolean_t
pmap_extract(pmap, va, pap)
a870 1
	paddr_t *pap;
d881 1
a881 2
		*pap = pa;
		return (TRUE);
d887 1
a887 1
			return (FALSE);
d891 1
a891 1
			return (FALSE);
d894 2
a895 4
	if (kvtopte(&pte[sva])->pg_pfn) {
		*pap = ((pte[sva] & PG_FRAME) << VAX_PGSHIFT);
		return (TRUE);
	}
d897 1
a897 1
	return (FALSE);
@


1.10.12.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10.12.2 2001/07/04 10:24:41 niklas Exp $ */
d60 1
d330 1
d333 1
a333 1
            M_VMPMAP, ptmapstorage, PTMAPSZ, EX_NOCOALESCE);
d406 1
a406 1
	res = extent_alloc(ptemap, bytesiz, 4, 0, 0, EX_WAITSPACE|EX_WAITOK,
d657 2
a658 2
int
pmap_enter(pmap, v, p, prot, flags)
d663 2
a664 1
	int flags;
a667 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d671 2
a672 2
	printf("pmap_enter: pmap %p v %lx p %lx prot %x wired %d flags %x\n",
		    pmap, v, p, prot, wired, flags);
d674 3
a730 4
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (KERN_RESOURCE_SHORTAGE);
				}
d732 4
a735 1
				panic("pmap_enter: no free pages");
d747 4
d757 1
a757 7
		return (KERN_SUCCESS);
	}

	/* mapping unchanged? just return. */
	if (newpte == oldpte) {
		RECURSEEND;
		return (KERN_SUCCESS);
d792 1
a792 1
	if (flags & VM_PROT_READ) {
d796 1
a796 1
	if (flags & VM_PROT_WRITE)
d817 1
a817 1
	return (KERN_SUCCESS);
@


1.10.12.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d58 3
a60 1
#include <uvm/uvm_extern.h>
@


1.10.12.5
log
@Merge in trunk
@
text
@a735 1
			pmap_update(pmap_kernel());
a781 1
		pmap->pm_stats.resident_count++;
d786 1
@


1.10.12.6
log
@Merge in -current from about a week ago
@
text
@d109 1
a109 1
void pmap_decpteref(struct pmap *, struct pte *);
d114 1
a114 1
void rensa(int, struct pte *);
d119 5
a123 5
void pmap_pinit(pmap_t);
void pmap_release(pmap_t);
struct pv_entry *get_pventry(void);
void free_pventry(struct pv_entry *);
void more_pventries(void);
@


1.10.12.7
log
@manually merge stuff cvs missed long ago
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2002/03/14 01:26:49 millert Exp $ */
a276 7
void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = virtual_avail;
	*vendp = virtual_end;
}

d616 33
a662 1
#ifdef PMAPDEBUG
a663 1
#endif
d697 3
d726 1
a726 1
					return (ENOMEM);
a738 2
	if (flags & PMAP_WIRED)
		newpte |= PG_W;
d748 1
a748 1
		return (0);
d754 1
a754 1
		return (0);
a795 3
	if (flags & PMAP_WIRED)
		newpte |= PG_V; /* Not allowed to be invalid */

d814 1
a814 1
	return (0);
a1075 1
	int ref = 0;
a1082 3
	if (pv->pv_attr & PG_V)
		ref++;

d1086 1
a1086 1
	if (pv->pv_pte && (pv->pv_pte[0].pg_w == 0))
d1093 4
a1096 5
		if (pv->pv_pte[0].pg_w == 0)
			pv->pv_pte[0].pg_v = pv->pv_pte[1].pg_v =
			    pv->pv_pte[2].pg_v = pv->pv_pte[3].pg_v = 
			    pv->pv_pte[4].pg_v = pv->pv_pte[5].pg_v = 
			    pv->pv_pte[6].pg_v = pv->pv_pte[7].pg_v = 0;
d1098 1
a1098 1
	return ref;
@


1.10.12.8
log
@Sync the SMP branch with 3.3
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
a56 1
#include <machine/rpb.h>
d227 1
a227 1
	bcopy((caddr_t)proc0paddr + REDZONEADDR, 0, sizeof(struct rpb));
a273 2
	rpb.sbr = mfpr(PR_SBR);
	rpb.slr = mfpr(PR_SLR);
d657 1
a657 5
			if (i >= (pmap->pm_p0lr & ~AST_MASK)) {
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (EFAULT);
				}
a658 1
			}
d665 1
a665 5
			if (i < pmap->pm_p1lr) {
				if (flags & PMAP_CANFAIL) {
					RECURSEEND;
					return (EFAULT);
				}
a666 1
			}
d916 2
a917 3
			if (((start & 0x3fffffff) >> VAX_PGSHIFT) <
			    pmap->pm_p1lr) {
#ifdef PMAPDEBUG
a918 3
#else
				RECURSEEND;
				return;
a919 1
			}
d924 2
a925 3
			if ((end >> VAX_PGSHIFT) >
			    (pmap->pm_p0lr & ~AST_MASK)) {
#ifdef PMAPDEBUG
a926 3
#else
				RECURSEEND;
				return;
a927 1
			}
@


1.10.12.9
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d67 9
d79 1
a79 1
pt_entry_t *Sysmap;		/* System page table */
d110 1
a110 1
void pmap_decpteref(struct pmap *, pt_entry_t *);
d115 1
a115 1
void rensa(int, pt_entry_t *);
d179 1
a179 1
	 * And it doesn't hurt, the kernel file is also public readable.
d184 1
a184 1
		Sysmap[i] = (Sysmap[i] & ~PG_PROT) | PG_URKW;
d192 1
a192 1
	*kvtopte(istack) &= ~PG_V;
d351 1
a351 1
	pt_entry_t *pte;
d360 1
a360 1
	pte = (pt_entry_t *)trunc_page((vaddr_t)pte);
d373 1
a373 1
	if (pmap->pm_refcnt[index] >= VAX_NBPG/sizeof(pt_entry_t))
d377 1
a377 1
		paddr = (*kvtopte(pte) & PG_FRAME) << VAX_PGSHIFT;
d379 1
a379 1
		bzero(kvtopte(pte), sizeof(pt_entry_t) * LTOHPN);
d411 1
a411 1
	bytesiz = USRPTSIZE * sizeof(pt_entry_t);
d459 1
a459 1
	eaddr = saddr + USRPTSIZE * sizeof(pt_entry_t);
d461 1
a461 1
		if ((*kvtopte(saddr) & PG_FRAME) != 0)
d465 1
a465 1
	    USRPTSIZE * sizeof(pt_entry_t), EX_WAITOK);
d528 1
a528 1
	pt_entry_t *ptp;
d578 1
a578 1
	pt_entry_t *ptp;
d580 1
a580 1
	ptp = kvtopte(va);
d601 1
a601 1
	pt_entry_t *pte;
d616 1
a616 1
		if ((*pte & PG_FRAME) == 0)
d618 3
a620 3
		if ((*pte & PG_SREF) == 0)
			rensa((*pte & PG_FRAME) >> LTOHPS, pte);
		bzero(pte, LTOHPN * sizeof(pt_entry_t));
d699 1
a699 1
			if ((*kvtopte(&patch[i]) & PG_FRAME) != 0)
d754 1
a754 1
			rensa(oldpte >> LTOHPS, (pt_entry_t *)&patch[i]);
d761 1
a761 1
			pv->pv_pte = (pt_entry_t *) & patch[i];
d765 1
a765 1
			tmp->pv_pte = (pt_entry_t *)&patch[i];
d798 1
a798 1
		if (pmap->pm_refcnt[index] > VAX_NBPG/sizeof(pt_entry_t))
d880 1
a880 1
	if ((*kvtopte(&pte[sva]) & PG_FRAME) != 0) {
d899 2
a900 2
	pt_entry_t *pt, *pts, *ptd;
	pt_entry_t pr;
d919 1
a919 1
		pr = (prot & VM_PROT_WRITE ? PG_KW : PG_KR);
d952 1
a952 1
		pr = (prot & VM_PROT_WRITE ? PG_RW : PG_RO);
d964 1
a964 1
		if ((*kvtopte(pts) & PG_FRAME) != 0 && *(int *)pts) {
d968 1
a968 1
					rensa((*pts & PG_FRAME) >> LTOHPS, pts);
d970 1
a970 1
				bzero(pts, sizeof(pt_entry_t) * LTOHPN);
d973 8
a980 8
				pts[0] = (pts[0] & ~PG_PROT) | pr;
				pts[1] = (pts[1] & ~PG_PROT) | pr;
				pts[2] = (pts[2] & ~PG_PROT) | pr;
				pts[3] = (pts[3] & ~PG_PROT) | pr;
				pts[4] = (pts[4] & ~PG_PROT) | pr;
				pts[5] = (pts[5] & ~PG_PROT) | pr;
				pts[6] = (pts[6] & ~PG_PROT) | pr;
				pts[7] = (pts[7] & ~PG_PROT) | pr;
d998 1
a998 1
	pt_entry_t *pte;
d1015 1
a1015 1
			pte = (pt_entry_t *)mfpr(PR_P1BR);
d1017 1
a1017 1
			pte = (pt_entry_t *)mfpr(PR_P0BR);
d1020 2
a1021 2
			pte = (pt_entry_t *)TRUNC_PAGE(pte);
			pte = kvtopte(pte);
d1024 1
a1024 1
			pa = (paddr_t)pte & ~KERNBASE;
d1026 1
a1026 1
			pa = (Sysmap[PG_PFNUM(pte)] & PG_FRAME) << VAX_PGSHIFT;
d1028 2
a1029 2
		pte = kvtopte(addr);
		pa = (paddr_t)pte & ~KERNBASE;
d1091 5
a1095 10
	if (pv->pv_pte && (pv->pv_pte[0] & PG_W) == 0) {
		pv->pv_pte[0] &= ~PG_V;
		pv->pv_pte[1] &= ~PG_V;
		pv->pv_pte[2] &= ~PG_V;
		pv->pv_pte[3] &= ~PG_V;
		pv->pv_pte[4] &= ~PG_V;
		pv->pv_pte[5] &= ~PG_V;
		pv->pv_pte[6] &= ~PG_V;
		pv->pv_pte[7] &= ~PG_V;
	}
d1098 5
a1102 10
		if ((pv->pv_pte[0] & PG_W) == 0) {
			pv->pv_pte[0] &= ~PG_V;
			pv->pv_pte[1] &= ~PG_V;
			pv->pv_pte[2] &= ~PG_V;
			pv->pv_pte[3] &= ~PG_V;
			pv->pv_pte[4] &= ~PG_V;
			pv->pv_pte[5] &= ~PG_V;
			pv->pv_pte[6] &= ~PG_V;
			pv->pv_pte[7] &= ~PG_V;
		}
d1132 4
a1135 8
		if ((pv->pv_pte[0] & PG_M) != 0 ||
		    (pv->pv_pte[1] & PG_M) != 0 ||
		    (pv->pv_pte[2] & PG_M) != 0 ||
		    (pv->pv_pte[3] & PG_M) != 0 ||
		    (pv->pv_pte[4] & PG_M) != 0 ||
		    (pv->pv_pte[5] & PG_M) != 0 ||
		    (pv->pv_pte[6] & PG_M) != 0 ||
		    (pv->pv_pte[7] & PG_M) != 0) {
d1143 4
a1146 8
		if ((pv->pv_pte[0] & PG_M) != 0 ||
		    (pv->pv_pte[1] & PG_M) != 0 ||
		    (pv->pv_pte[2] & PG_M) != 0 ||
		    (pv->pv_pte[3] & PG_M) != 0 ||
		    (pv->pv_pte[4] & PG_M) != 0 ||
		    (pv->pv_pte[5] & PG_M) != 0 ||
		    (pv->pv_pte[6] & PG_M) != 0 ||
		    (pv->pv_pte[7] & PG_M) != 0) {
d1177 5
a1181 10
	if (pv->pv_pte) {
		pv->pv_pte[0] &= ~PG_M;
		pv->pv_pte[1] &= ~PG_M;
		pv->pv_pte[2] &= ~PG_M;
		pv->pv_pte[3] &= ~PG_M;
		pv->pv_pte[4] &= ~PG_M;
		pv->pv_pte[5] &= ~PG_M;
		pv->pv_pte[6] &= ~PG_M;
		pv->pv_pte[7] &= ~PG_M;
	}
d1183 5
a1187 10
	while ((pv = pv->pv_next)) {
		pv->pv_pte[0] &= ~PG_M;
		pv->pv_pte[1] &= ~PG_M;
		pv->pv_pte[2] &= ~PG_M;
		pv->pv_pte[3] &= ~PG_M;
		pv->pv_pte[4] &= ~PG_M;
		pv->pv_pte[5] &= ~PG_M;
		pv->pv_pte[6] &= ~PG_M;
		pv->pv_pte[7] &= ~PG_M;
	}
d1201 1
a1201 1
	pt_entry_t *pt;
d1228 1
a1228 1
			bzero(g, sizeof(pt_entry_t) * LTOHPN);
d1241 1
a1241 1
			bzero(g, sizeof(pt_entry_t) * LTOHPN);
a1250 2
			pt_entry_t pr;

d1254 6
a1259 12

			pr = (vaddr_t)pv->pv_pte < ptemapstart ? 
			    PG_KR : PG_RO;

			pt[0] = (pt[0] & ~PG_PROT) | pr;
			pt[1] = (pt[1] & ~PG_PROT) | pr;
			pt[2] = (pt[2] & ~PG_PROT) | pr;
			pt[3] = (pt[3] & ~PG_PROT) | pr;
			pt[4] = (pt[4] & ~PG_PROT) | pr;
			pt[5] = (pt[5] & ~PG_PROT) | pr;
			pt[6] = (pt[6] & ~PG_PROT) | pr;
			pt[7] = (pt[7] & ~PG_PROT) | pr;
@


1.9
log
@Sync with NetBSD 970827. -moj
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1997/09/10 12:04:51 maja Exp $ */
d55 1
a61 1

d166 6
@


1.8
log
@Sync with NetBSD 970516. -moj
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.7 1997/01/15 23:25:20 maja Exp $ */
/*	$NetBSD: pmap.c,v 1.35 1997/03/22 12:50:56 ragge Exp $	   */
d187 1
a187 1
	MAPPHYS(junk, ((ROUND_PAGE(&etext)  - KERNBASE) >> PGSHIFT),
d190 1
a190 2
	MAPPHYS(junk, ((ROUND_PAGE(&etext) - KERNBASE) >> PGSHIFT),
	    VM_PROT_EXECUTE);
d192 1
a192 1
	MAPPHYS(junk, (((u_int)Sysmap - ROUND_PAGE(&etext)) >> PGSHIFT),
d197 1
a197 1
	MAPPHYS(junk, (ROUND_PAGE(sysptsize * 4) >> PGSHIFT),
d201 1
a201 1
	MAPPHYS(istack, (ISTACK_SIZE >> PGSHIFT), VM_PROT_READ|VM_PROT_WRITE);
d209 1
a209 1
	MAPPHYS(msgbufp, ((u_int)ROUND_PAGE(sizeof(struct msgbuf)) >> PGSHIFT),
d213 2
a214 2
	MAPPHYS(pv_table, ((avail_end / PAGE_SIZE ) * sizeof(struct pv_entry))
	    >> PGSHIFT, VM_PROT_READ|VM_PROT_WRITE);
@


1.7
log
@sync with NetBSD 970112 -moj
@
text
@d1 2
a2 2
/*	$OpenBSD$ */
/*	$NetBSD: pmap.c,v 1.30 1996/10/13 03:35:57 christos Exp $	   */
d92 2
a119 6
#ifndef MAX_PHYSMEM_AVAIL
#define MAX_PHYSMEM_AVAIL     512*1024*1024
#endif
#ifndef MIN_PHYSMEM_AVAIL
#define MIN_PHYSMEM_AVAIL	8*1024*1024
#endif
a136 2
	sysptsize = SYSPTSIZE;

d138 5
a142 6
	 * Because of the badaddr() problem with some VAXstations we
	 * compare the first page of memory (the SCB) with the new
	 * counted up pages for equality. It's very unlikely that
	 * another page will hold the same info as the SCB.
	 * This is neccessary only if badaddr() doesn't work, but on other
	 * machines checking the pattern doesn't hurt anyway...
d144 3
d148 1
a148 2
	/* Kickoff for memory checking */
	avail_end = 0x200000;	/* 2 MB */
d150 6
a155 7
	while (badaddr((caddr_t)avail_end, 4) == 0) {
#if VAX410 || VAX420 || VAX43 || VAX46 || VAX49 || VAX50
		if (bcmp(0, (caddr_t)avail_end, NBPG) == 0)
			break;
#endif
		avail_end += NBPG * 128;/* Memory is checked in 64K hunks */
	}
d157 9
a165 3
#if VAX410 || VAX420 || VAX43 || VAX46 || VAX49 || VAX50
	sysptsize += (16 * 1024) >> PGSHIFT;  /* guc->uc_sysptSpace ?? */
#endif
a174 1
	sysptsize += avail_end >> PGSHIFT;
d236 1
a236 1
	(cpu_calls[vax_cputype].cpu_steal_pages)();
d243 1
a243 1
	printf("SYSPTSIZE %x, USRPTSIZE %x\n",sysptsize,USRPTSIZE);
d288 1
a288 1
	    USRPTSIZE * 4, TRUE);
@


1.6
log
@kill /netbsd
@
text
@d1 2
a2 1
/*      $NetBSD: pmap.c,v 1.27 1996/05/19 16:44:20 ragge Exp $     */
d62 2
a63 2
#define	ISTACK_SIZE (4 * NBPG)
#define	PTE_TO_PV(pte)	(PHYS_TO_PV((pte&PG_FRAME)<<PGSHIFT))
d75 2
a76 2
pv_entry_t      pv_table;               /* array of entries,
                                           one per LOGICAL page */
d78 1
a78 1
void 	*scratch;
d82 1
a82 1
extern	int startsysc, faultdebug;
d89 35
a123 1
vm_offset_t   virtual_avail, virtual_end; /* Available virtual memory   */
d142 25
a174 2
	while (!badaddr((caddr_t)avail_end, 4)) /* Memory is in 64K hunks */
		avail_end += NBPG * 128;
d198 1
a198 1
        mtpr(avail_start, PR_SBR);
d237 15
a251 13
	(cpu_calls[cpunumber].cpu_steal_pages)();

#ifdef PMAPDEBUG
        printf("Sysmap %x, istack %x, scratch %x\n",Sysmap,istack,scratch);
        printf("etext %x\n", &etext);
        printf("SYSPTSIZE %x, USRPTSIZE %x\n",sysptsize,USRPTSIZE);
        printf("pv_table %x, vmmap %x, pte_cmap %x\n",
                pv_table,vmmap,pte_cmap);
        printf("avail_start %x, avail_end %x\n",avail_start,avail_end);
        printf("virtual_avail %x,virtual_end %x\n",virtual_avail,virtual_end);
        printf("clearomr: %x \n",(uint)vmmap-(uint)Sysmap);
        printf("faultdebug %x, startsysc %x\n",&faultdebug, &startsysc);
        printf("startpmapdebug %x\n",&startpmapdebug);
d255 13
a267 13
        /* Init kernel pmap */
        pmap_kernel()->ref_count = 1;
        simple_lock_init(&pmap_kernel()->pm_lock);
        p0pmap->pm_pcb = (struct pcb *)proc0paddr;

        p0pmap->pm_pcb->P1BR = (void *)0x80000000;
        p0pmap->pm_pcb->P0BR = (void *)0x80000000;
        p0pmap->pm_pcb->P1LR = 0x200000;
        p0pmap->pm_pcb->P0LR = AST_PCB;
        mtpr(0x80000000, PR_P1BR);
        mtpr(0x80000000, PR_P0BR);
        mtpr(0x200000, PR_P1LR);
        mtpr(AST_PCB, PR_P0LR);
d271 2
a272 2
        mtpr(sysptsize, PR_SLR);
        mtpr(1, PR_MAPEN);
d301 1
a301 1
	pmap_t   pmap;
d373 4
a376 4
	vm_offset_t     v;
	vm_offset_t     p;
	vm_prot_t       prot;
	boolean_t       wired;
d461 4
d522 2
a523 2
	vm_offset_t     end;
	vm_prot_t       prot;
d757 1
a757 1
	vm_offset_t     pa;
d784 3
a786 3
                pte=(u_int *)pmap_virt2pte(pv->pv_pmap,pv->pv_va);
                spte|=*pte++;
                spte|=*pte;
d798 1
a798 1
	vm_offset_t     pa;
d825 1
a825 1
	vm_offset_t     pa;
d842 2
a843 2
	vm_offset_t     va;
	boolean_t       wired;
d858 1
a858 1
 *      pmap_page_protect:
d860 1
a860 1
 *      Lower the permission for all mappings to a given page.
d864 2
a865 2
	vm_offset_t     pa;
	vm_prot_t       prot;
d926 4
a929 4
 *      pmap_zero_page zeros the specified (machine independent)
 *      page by mapping the page into virtual memory and using
 *      bzero to clear its contents, one machine dependent page
 *      at a time.
@


1.5
log
@sync to 0611
@
text
@d125 1
a125 1
	 * in there. And it doesn't hurt, /netbsd is also public readable.
@


1.4
log
@sync w/ 0430
@
text
@d1 1
a1 1
/*      $NetBSD: pmap.c,v 1.26 1996/04/08 18:32:53 ragge Exp $     */
d40 1
a50 1
#include <machine/uvaxII.h>
@


1.3
log
@update from netbsd
@
text
@d1 1
a1 2
/*      $NetBSD: pmap.c,v 1.19 1995/12/13 18:50:20 ragge Exp $     */
#define DEBUG
d32 27
a58 25
#include "sys/types.h"
#include "sys/param.h"
#include "sys/queue.h"
#include "sys/malloc.h"
#include "sys/proc.h"
#include "sys/user.h"
#include "sys/msgbuf.h"

#include "vm/vm.h"
#include "vm/vm_page.h"
#include "vm/vm_kern.h"

#include "machine/pte.h"
#include "machine/pcb.h"
#include "machine/mtpr.h"
#include "machine/macros.h"
#include "machine/sid.h"
#include "machine/uvaxII.h"
#include "machine/cpu.h"
#include "machine/scb.h"


pt_entry_t *pmap_virt2pte(pmap_t, u_int);
static	pv_entry_t alloc_pv_entry();
static	void	free_pv_entry();
a63 2


d74 2
a75 1

d79 1
a79 1
#ifdef DEBUG
d100 1
a100 1
	extern	unsigned int proc0paddr, sigcode, esigcode, etext;
d115 1
a115 1
	while (!badaddr(avail_end, 4)) /* Memory is in 64K hunks */
d181 1
a181 1
#ifdef DEBUG
d201 1
a201 1
        p0pmap->pm_pcb->P0BR = 0;
d205 1
a205 1
        mtpr(0, PR_P0BR);
d208 3
a210 3
/*
 * Now everything should be complete, start virtual memory.
 */
d243 1
a243 1
#ifdef DEBUG
d265 1
a265 1
#ifdef DEBUG
d294 1
a294 1
#ifdef DEBUG
d318 1
a318 1
	u_int j, i, pte, s, *patch;
d326 1
a326 1
#ifdef DEBUG
d418 1
a418 1
#ifdef DEBUG
d441 2
a442 2
	int	*pte, nypte;
#ifdef DEBUG
d463 1
a463 1
#ifdef DEBUG
d507 1
a507 1
	u_int *ptestart, *pteslut,i,s,*temp;
d511 1
a511 1
#ifdef DEBUG
d516 2
a517 1
	if(!pmap) return;
d521 2
a522 1
		if(!(temp=pmap->pm_pcb->P0BR)) return; /* No page table */
d531 2
a532 1
		if(!(temp=pmap->pm_pcb->P1BR)) return; /* No page table */
d539 1
a539 1
#ifdef DEBUG
d561 1
a561 1

d606 1
a606 1
#ifdef DEBUG
d666 1
a666 1
#ifdef DEBUG
d706 1
a706 1
	} while(pv=pv->pv_next);
d723 1
a723 1
	} while(pv=pv->pv_next);
d737 1
a737 1
	int *pte,s,i;
d742 1
a742 1
#ifdef DEBUG
d755 1
a755 1
	} while(pv=pv->pv_next);
d764 1
a764 1
	u_int *pte,spte=0,s;
d772 1
a772 1
	} while(pv=pv->pv_next);
d782 1
a782 1
#ifdef DEBUG
d806 1
a806 1
#ifdef DEBUG
d833 1
a833 1
		} while(pv=pv->pv_next);
d871 1
a871 1
	int s;
d873 1
a873 1
#ifdef DEBUG
d877 9
a885 9
	s=splimp();
	pte_cmap[0]=(phys>>PGSHIFT)|PG_V|PG_KW;
	pte_cmap[1]=pte_cmap[0]+1;
	mtpr(vmmap,PR_TBIS);
	mtpr(vmmap+NBPG,PR_TBIS);
	bzero((void *)vmmap,NBPG*2);
	pte_cmap[0]=pte_cmap[1]=0;
	mtpr(vmmap,PR_TBIS);
	mtpr(vmmap+NBPG,PR_TBIS);
d890 1
a890 1
pmap_virt2pte(pmap,vaddr)
d894 1
a894 1
	u_int *pte,scr;
d896 10
a905 9
	if(vaddr<0x40000000){
		pte=pmap->pm_pcb->P0BR;
		if((vaddr>>PGSHIFT)>(pmap->pm_pcb->P0LR&~AST_MASK)) return 0;
	} else if(vaddr<(u_int)0x80000000){
		pte=pmap->pm_pcb->P1BR;
		if(((vaddr&0x3fffffff)>>PGSHIFT)<pmap->pm_pcb->P1LR) return 0;
	} else {
		pte=(u_int *)Sysmap;
	}
d907 1
a907 1
	vaddr&=(u_int)0x3fffffff;
d909 1
a909 1
	return((pt_entry_t *)&pte[vaddr>>PGSHIFT]);
d912 3
a914 2
pmap_expandp0(pmap,ny_storlek)
	struct pmap *pmap;
d916 8
a923 1
	u_int tmp,s,size,osize,oaddr,astlvl,*i,j;
d925 7
a931 12
	astlvl=pmap->pm_pcb->P0LR&AST_MASK;
	osize=(pmap->pm_pcb->P0LR&~AST_MASK)*4;
	size=ny_storlek*4;
	tmp=kmem_alloc_wait(pte_map, size);
	s=splhigh();
	if(osize) blkcpy(pmap->pm_pcb->P0BR, (void*)tmp,osize);
	oaddr=(u_int)pmap->pm_pcb->P0BR;
	mtpr(tmp,PR_P0BR);
	mtpr(((size>>2)|astlvl),PR_P0LR);
	mtpr(0,PR_TBIA);
	pmap->pm_pcb->P0BR=(void*)tmp;
	pmap->pm_pcb->P0LR=((size>>2)|astlvl);
d933 1
d938 1
d940 1
a940 1
	struct pmap *pmap;
d942 5
a946 1
	u_int tmp,s,size,osize,oaddr,*i,j;
d948 11
a958 13
	osize=0x800000-(pmap->pm_pcb->P1LR*4);
	size=osize+PAGE_SIZE;
	tmp=kmem_alloc_wait(pte_map, size);
	s=splhigh();

	if(osize) blkcpy((void*)pmap->pm_stack, (void*)tmp+PAGE_SIZE,osize);
	oaddr=pmap->pm_stack;
	pmap->pm_pcb->P1BR=(void*)(tmp+size-0x800000);
	pmap->pm_pcb->P1LR=(0x800000-size)>>2;
	pmap->pm_stack=tmp;
	mtpr(pmap->pm_pcb->P1BR,PR_P1BR);
	mtpr(pmap->pm_pcb->P1LR,PR_P1LR);
	mtpr(0,PR_TBIA);
d960 1
@


1.2
log
@update from netbsd (verbatim)
@
text
@d1 1
a1 1
/*      $NetBSD: pmap.c,v 1.18 1995/11/10 18:52:54 ragge Exp $     */
d100 3
a102 3
	extern unsigned int proc0paddr, sigcode, esigcode, etext;
	extern struct vmspace vmspace0;
	struct pmap *p0pmap;
d110 4
d115 3
a120 3
	while (!badaddr(avail_end, 4)) /* Memory is in 64K hunks */
		avail_end += NBPG * 128;

a166 1
#ifdef VAX750
d180 1
a180 6
#else
#if VAX630
        if (cpu_type == VAX_630)
                avail_end -= 8 * NBPG;       /* Avoid console scratchpad */
#endif
#endif
d211 1
a211 1
        mtpr(SYSPTSIZE, PR_SLR);
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*      $NetBSD: pmap.c,v 1.17 1995/08/22 04:34:17 ragge Exp $     */
a32 2
 /* All bugs are subject to removal without further notice */
		
d40 1
a43 6
#include "vax/include/pte.h"
#include "vax/include/pcb.h"
#include "vax/include/mtpr.h"
#include "vax/include/loconf.h"
#include "vax/include/macros.h"
#include "vax/include/sid.h"
d45 9
a53 1
#include "uba.h"
d56 6
a62 1
#define	PTE_TO_PV(pte)	(PHYS_TO_PV((pte&PG_FRAME)<<PG_SHIFT))
a65 4
unsigned int gurkskit[50],istack;

static pv_entry_t alloc_pv_entry();
static void	free_pv_entry();
d73 5
a77 20
static pv_entry_t   pv_head =NULL;
static unsigned int pv_count=0;
vm_offset_t ptemapstart,ptemapend;

extern uint etext;
extern u_int *pte_cmap;
extern int  maxproc;
extern struct vmspace vmspace0;
extern int edata, end;
uint*  UMEMmap;
void*  Numem;
void  *scratch;
uint   sigsida;
#ifdef DEBUG
int startpmapdebug=0;
extern int startsysc, faultdebug;
#endif
unsigned int *valueptr=gurkskit, vmmap;
pt_entry_t *Sysmap;
vm_map_t	pte_map;
d79 9
a87 1
vm_offset_t     avail_start, avail_end;
d96 1
a96 2

void 
d99 35
a133 18
	uint	i;
	extern	u_int sigcode, esigcode, proc0paddr;
	extern char *esym;
	struct pmap *p0pmap=&vmspace0.vm_pmap;
	vm_offset_t	pend=0;
#define	ROUND_PAGE(x)	(((uint)(x) + PAGE_SIZE-1)& ~(PAGE_SIZE - 1))

 /* These are in phys memory */
	istack = ROUND_PAGE((uint)Sysmap + SYSPTSIZE * 4);
	(u_int)scratch = istack + ISTACK_SIZE;
	mtpr(scratch, PR_ISP); /* set interrupt stack pointer */
	msgbufp = (void *)(scratch + NBPG * 4);
	(u_int)pv_table = (int)ROUND_PAGE(sizeof(struct msgbuf)) +
	    (u_int)msgbufp;

/* Count up phys memory */
	while (!badaddr(pend, 4))
		pend += NBPG * 128;
d135 43
d179 3
a181 2
	if (cpu_type == VAX_630)
		pend -= 8 * NBPG;       /* Avoid console scratchpad */
d183 11
a193 27
#if VAX650
	if (cpu_type == VAX_650)
		pend -= 64 * NBPG;
#endif
/* These are virt only */
	vmmap = ROUND_PAGE(pv_table + (pend / PAGE_SIZE));
	(u_int)Numem = vmmap + NBPG * 2;

	(pt_entry_t *)UMEMmap=kvtopte(Numem);
	(pt_entry_t *)pte_cmap=kvtopte(vmmap);

	avail_start=ROUND_PAGE(vmmap)&0x7fffffff;
	avail_end=pend-ROUND_PAGE(sizeof(struct msgbuf));
	virtual_avail=ROUND_PAGE((uint)Numem+NUBA*NBPG*NBPG);
	virtual_end=SYSPTSIZE*NBPG+KERNBASE;
#ifdef DEBUG
	printf("Sysmap %x, istack %x, scratch %x\n",Sysmap,istack,scratch);
	printf("etext %x, edata %x, end %x, esym %x\n",
	    &etext,&edata, &end, esym);
	printf("SYSPTSIZE %x, USRPTSIZE %x\n",SYSPTSIZE,USRPTSIZE);
	printf("pv_table %x, vmmap %x, Numem %x, pte_cmap %x\n",
		pv_table,vmmap,Numem,pte_cmap);
	printf("avail_start %x, avail_end %x\n",avail_start,avail_end);
	printf("virtual_avail %x,virtual_end %x\n",virtual_avail,virtual_end);
	printf("clearomr: %x \n",(uint)vmmap-(uint)Sysmap);
	printf("faultdebug %x, startsysc %x\n",&faultdebug, &startsysc);
	printf("startpmapdebug %x\n",&startpmapdebug);
d196 14
a209 39
	blkclr(Sysmap,(uint)vmmap-(uint)Sysmap);
	pmap_map(0x80000000,0,2*NBPG,VM_PROT_READ|VM_PROT_WRITE);
#ifdef DDB
	pmap_map(0x80000400,2*NBPG,(vm_offset_t)(&etext),
	    VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE);
#else
	pmap_map(0x80000400,2*NBPG,(vm_offset_t)(&etext),VM_PROT_EXECUTE);
#endif
	pmap_map((vm_offset_t)(&etext),(vm_offset_t)&etext,
		(vm_offset_t)Sysmap,VM_PROT_READ|VM_PROT_WRITE);
	pmap_map((vm_offset_t)Sysmap,(vm_offset_t)Sysmap,istack,
		VM_PROT_READ|VM_PROT_WRITE);
	pmap_map(istack,istack,istack+NBPG,VM_PROT_NONE);/* Red zone */
	pmap_map(istack+NBPG,istack+NBPG,(vm_offset_t)scratch,
		VM_PROT_READ|VM_PROT_WRITE);
	pmap_map((vm_offset_t)scratch,(vm_offset_t)scratch,
		(vm_offset_t)msgbufp, VM_PROT_READ|VM_PROT_WRITE);
        pmap_map((vm_offset_t)msgbufp, (vm_offset_t)msgbufp,
	    (vm_offset_t)pv_table, VM_PROT_ALL);
	pmap_map((vm_offset_t)pv_table,(vm_offset_t)pv_table,vmmap,
		VM_PROT_READ|VM_PROT_WRITE);

	/* Init kernel pmap */
	pmap_kernel()->ref_count = 1;
	simple_lock_init(&pmap_kernel()->pm_lock);
	p0pmap->pm_pcb=(struct pcb *)proc0paddr;

		 /* used for signal trampoline code */
	sigsida=(u_int)(scratch+NBPG)&0x7fffffff;
	bcopy(&sigcode, (void *)sigsida, (u_int)&esigcode-(u_int)&sigcode);

	p0pmap->pm_pcb->P1BR = (void *)0x80000000;
	p0pmap->pm_pcb->P0BR = 0;
	p0pmap->pm_pcb->P1LR = 0x200000;
	p0pmap->pm_pcb->P0LR = AST_PCB;
	mtpr(0x80000000, PR_P1BR);
	mtpr(0, PR_P0BR);
	mtpr(0x200000, PR_P1LR);
	mtpr(AST_PCB, PR_P0LR);
d213 2
a214 4
	mtpr((uint)Sysmap&0x7fffffff,PR_SBR); /* Where is SPT? */
	mtpr(SYSPTSIZE,PR_SLR);
	mtpr(1,PR_MAPEN);
	bzero(valueptr, 200);
d217 5
a221 7
/****************************************************************************** *
 * pmap_init()
 *
 ******************************************************************************
 *
 * Called as part of vm init.
 *
a222 1

d224 2
a225 2
pmap_init(s, e) 
	vm_offset_t s,e;
d227 1
d234 4
a237 14
/******************************************************************************
 *
 * pmap_create()
 *
 ******************************************************************************
 *
 * pmap_t pmap_create(phys_size)
 *
 * Create a pmap for a new task.
 * 
 * Allocate a pmap form kernel memory with malloc.
 * Clear the pmap.
 * Allocate a ptab for the pmap.
 * 
d248 2
a249 3
	if(phys_size) return NULL;

/* Malloc place for pmap struct */
d339 1
a339 1
		i = (v >> PG_SHIFT);
d345 1
a345 1
		i = (v - 0x40000000) >> PG_SHIFT;
d350 1
a350 1
		i = (v - (u_int)0x80000000) >> PG_SHIFT;
d421 3
a423 1
if(startpmapdebug)printf("pmap_map: virt %x, pstart %x, pend %x\n",virtuell, pstart, pend);
d449 1
a449 1
	if(pte) return(((*pte&PG_FRAME)<<PG_SHIFT)+((u_int)va&PGOFSET));
d475 2
a476 2
		while((end>>PG_SHIFT)>(pmap->pm_pcb->P0LR&~AST_MASK))
			pmap_expandp0(pmap,(end>>PG_SHIFT));
d479 1
a479 1
		i=(start&0x3fffffff)>>PG_SHIFT;
d481 2
a482 2
			start=((pmap->pm_pcb->P1LR)<<PG_SHIFT)+0x40000000;
		i=(end&0x3fffffff)>>PG_SHIFT;
d523 2
a524 2
		ptestart=&temp[start>>PG_SHIFT];
		pteslut=&temp[slut>>PG_SHIFT];
d528 2
a529 2
		ptestart=(u_int *)&Sysmap[(start&0x3fffffff)>>PG_SHIFT];
		pteslut=(u_int *)&Sysmap[(slut&0x3fffffff)>>PG_SHIFT];
d532 2
a533 2
		pteslut=&temp[(slut&0x3fffffff)>>PG_SHIFT];
		ptestart=&temp[(start&0x3fffffff)>>PG_SHIFT];
d596 1
d621 31
d661 4
d877 1
a877 1
	pte_cmap[0]=(phys>>PG_SHIFT)|PG_V|PG_KW;
d897 1
a897 1
		if((vaddr>>PG_SHIFT)>(pmap->pm_pcb->P0LR&~AST_MASK)) return 0;
d900 1
a900 1
		if(((vaddr&0x3fffffff)>>PG_SHIFT)<pmap->pm_pcb->P1LR) return 0;
d907 1
a907 1
	return((pt_entry_t *)&pte[vaddr>>PG_SHIFT]);
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
