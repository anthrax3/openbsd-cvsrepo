head	1.13;
access;
symbols
	OPENBSD_6_0:1.13.0.8
	OPENBSD_6_0_BASE:1.13
	OPENBSD_5_9:1.13.0.4
	OPENBSD_5_9_BASE:1.13
	OPENBSD_5_8:1.13.0.6
	OPENBSD_5_8_BASE:1.13
	OPENBSD_5_7:1.13.0.2
	OPENBSD_5_7_BASE:1.13
	OPENBSD_5_6:1.11.0.4
	OPENBSD_5_6_BASE:1.11
	OPENBSD_5_5:1.9.0.18
	OPENBSD_5_5_BASE:1.9
	OPENBSD_5_4:1.9.0.14
	OPENBSD_5_4_BASE:1.9
	OPENBSD_5_3:1.9.0.12
	OPENBSD_5_3_BASE:1.9
	OPENBSD_5_2:1.9.0.10
	OPENBSD_5_2_BASE:1.9
	OPENBSD_5_1_BASE:1.9
	OPENBSD_5_1:1.9.0.8
	OPENBSD_5_0:1.9.0.6
	OPENBSD_5_0_BASE:1.9
	OPENBSD_4_9:1.9.0.4
	OPENBSD_4_9_BASE:1.9
	OPENBSD_4_8:1.9.0.2
	OPENBSD_4_8_BASE:1.9
	OPENBSD_4_7:1.8.0.2
	OPENBSD_4_7_BASE:1.8
	OPENBSD_4_6:1.8.0.4
	OPENBSD_4_6_BASE:1.8
	OPENBSD_4_5:1.4.0.6
	OPENBSD_4_5_BASE:1.4
	OPENBSD_4_4:1.4.0.4
	OPENBSD_4_4_BASE:1.4
	OPENBSD_4_3:1.4.0.2
	OPENBSD_4_3_BASE:1.4
	OPENBSD_4_2:1.3.0.2
	OPENBSD_4_2_BASE:1.3
	OPENBSD_4_1:1.2.0.2
	OPENBSD_4_1_BASE:1.2
	LANDISK_20061006:1.1.1.1
	miod:1.1.1;
locks; strict;
comment	@ * @;


1.13
date	2015.01.25.11.36.41;	author dlg;	state Exp;
branches;
next	1.12;
commitid	hvsstAcOkDtNNkwA;

1.12
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.11;
commitid	yv0ECmCdICvq576h;

1.11
date	2014.07.12.18.44.42;	author tedu;	state Exp;
branches;
next	1.10;
commitid	uKVPYMN2MLxdZxzH;

1.10
date	2014.07.11.09.36.26;	author mpi;	state Exp;
branches;
next	1.9;
commitid	vsYjSRfS3Y783BvW;

1.9
date	2010.04.21.03.03.26;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	2009.04.20.00.42.06;	author oga;	state Exp;
branches;
next	1.7;

1.7
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.6;

1.6
date	2009.03.15.09.35.50;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2009.03.07.15.34.34;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2007.10.06.23.12.17;	author krw;	state Exp;
branches;
next	1.3;

1.3
date	2007.03.19.20.13.19;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2006.10.19.21.16.54;	author kettenis;	state Exp;
branches;
next	1.1;

1.1
date	2006.10.06.21.16.15;	author miod;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.10.06.21.16.15;	author miod;	state Exp;
branches;
next	;


desc
@@


1.13
log
@refactor loading of dmamaps.

bus_dmama_load and bus_dmamap_load mbuf figure out the physical
addresses of the memory theyre given and then hand it to
_bus_dmamap_load_paddr to store in the dmamaps sg lists.

unfortunately bus_dmamap_load_mbuf assumes it is only given memory
from the kernels direct mapped region, and blindly translates
anything its given into phys addresses to hand to _load_paddr.

i recently committed change to pool asking them to allocate large
pages, which meant uvm allocated mbufs outside the direct map, which
meant bus_dmamap_load_mbuf was handing out bogus physical addresses.
the pool change got backed out until i could debug this.

now _load and _load_mbuf now call _bus_dmamap_load_vaddr for every
buffer theyve been given, which properly determines if the addresses
are in the direct map or via the tlb. _load_vaddr then feeds the
physical addresses into _bus_dmamap_load_paddr to store them in the
dmamap.

tldr; _load_mbuf doesnt make naive assumptions about its addresses
now.

ok miod@@ kettenis@@
@
text
@/*	$OpenBSD: bus_dma.c,v 1.12 2014/11/16 12:30:57 deraadt Exp $	*/
/*	$NetBSD: bus_dma.c,v 1.1 2006/09/01 21:26:18 uwe Exp $	*/

/*
 * Copyright (c) 2005 NONAKA Kimihiro
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>

#include <uvm/uvm_extern.h>

#include <sh/cache.h>

#include <machine/autoconf.h>
#define	_LANDISK_BUS_DMA_PRIVATE
#include <machine/bus.h>

#if defined(DEBUG) && defined(BUSDMA_DEBUG)
#define	DPRINTF(a)	printf a
#else
#define	DPRINTF(a)
#endif

struct _bus_dma_tag landisk_bus_dma = {
	._cookie = NULL,

	._dmamap_create = _bus_dmamap_create,
	._dmamap_destroy = _bus_dmamap_destroy,
	._dmamap_load = _bus_dmamap_load,
	._dmamap_load_mbuf = _bus_dmamap_load_mbuf,
	._dmamap_load_uio = _bus_dmamap_load_uio,
	._dmamap_load_raw = _bus_dmamap_load_raw,
	._dmamap_unload = _bus_dmamap_unload,
	._dmamap_sync = _bus_dmamap_sync,

	._dmamem_alloc = _bus_dmamem_alloc,
	._dmamem_free = _bus_dmamem_free,
	._dmamem_map = _bus_dmamem_map,
	._dmamem_unmap = _bus_dmamem_unmap,
	._dmamem_mmap = _bus_dmamem_mmap,
};

#define DMAMAP_RESET(_m) do { \
	(_m)->dm_mapsize = 0; \
	(_m)->dm_nsegs = 0; \
} while (0)

int	_bus_dmamap_load_vaddr(bus_dma_tag_t, bus_dmamap_t,
	    void *, bus_size_t, pmap_t);
int	_bus_dmamap_load_paddr(bus_dma_tag_t, bus_dmamap_t,
	    paddr_t, vaddr_t, bus_size_t);

/*
 * Create a DMA map.
 */
int
_bus_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
    bus_size_t maxsegsz, bus_size_t boundary, int flags, bus_dmamap_t *dmamp)
{
	bus_dmamap_t map;
	void *mapstore;
	size_t mapsize;
	int error;

	DPRINTF(("bus_dmamap_create: t = %p, size = %ld, nsegments = %d, maxsegsz = %ld, boundary = %ld, flags = %x\n", t, size, nsegments, maxsegsz, boundary, flags));

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
	error = 0;
	mapsize = sizeof(struct _bus_dmamap) +
	    (sizeof(bus_dma_segment_t) * (nsegments - 1));
	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
		return (ENOMEM);

	DPRINTF(("bus_dmamap_create: dmamp = %p\n", mapstore));

	map = (bus_dmamap_t)mapstore;
	map->_dm_size = size;
	map->_dm_segcnt = nsegments;
	map->_dm_maxsegsz = maxsegsz;
	map->_dm_boundary = boundary;
	map->_dm_flags = flags & ~(BUS_DMA_WAITOK|BUS_DMA_NOWAIT);

	DMAMAP_RESET(map); /* no valid mappings */

	*dmamp = map;

	return (0);
}

/*
 * Destroy a DMA map.
 */
void
_bus_dmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{

	DPRINTF(("bus_dmamap_destroy: t = %p, map = %p\n", t, map));

	free(map, M_DEVBUF, 0);
}

int
_bus_dmamap_load_paddr(bus_dma_tag_t t, bus_dmamap_t map,
    paddr_t paddr, vaddr_t vaddr, bus_size_t size)
{
	bus_dma_segment_t * const segs = map->dm_segs;
	bus_addr_t bmask = ~(map->_dm_boundary - 1);

	int first = map->dm_mapsize == 0;
	int nseg = map->dm_nsegs;
	paddr_t lastaddr = SH3_P2SEG_TO_PHYS(segs[nseg].ds_addr);

	map->dm_mapsize += size;

	do {
		bus_size_t sgsize = size;

		/* Make sure we don't cross any boundaries. */
		if (map->_dm_boundary > 0) {
			bus_addr_t baddr; /* next boundary address */

			baddr = (paddr + map->_dm_boundary) & bmask;
			if (sgsize > (baddr - paddr))
				sgsize = (baddr - paddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			/* first segment */
			segs[nseg].ds_addr = SH3_PHYS_TO_P2SEG(paddr);
			segs[nseg].ds_len = sgsize;
			segs[nseg]._ds_vaddr = vaddr;
			first = 0;
		} else if ((paddr == lastaddr)
		 && (segs[nseg].ds_len + sgsize <= map->_dm_maxsegsz)
		 && (map->_dm_boundary == 0 ||
		     (segs[nseg].ds_addr & bmask) == (paddr & bmask))) {
			/* coalesce */
			segs[nseg].ds_len += sgsize;
		} else {
			if (++nseg >= map->_dm_segcnt)
				return (EFBIG);

			/* new segment */
			segs[nseg].ds_addr = SH3_PHYS_TO_P2SEG(paddr);
			segs[nseg].ds_len = sgsize;
			segs[nseg]._ds_vaddr = vaddr;
		}

		lastaddr = paddr + sgsize;
		paddr += sgsize;
		vaddr += sgsize;
		size -= sgsize;
	} while (size > 0);

	map->dm_nsegs = nseg;

	return (0);
}

int
_bus_dmamap_load_vaddr(bus_dma_tag_t t, bus_dmamap_t map,
    void *buf, bus_size_t size, pmap_t pmap)
{
	vaddr_t vaddr;
	paddr_t paddr;
	vaddr_t next, end;
	int error;

	vaddr = (vaddr_t)buf;
	end = vaddr + size;

	if (pmap == pmap_kernel() &&
	    vaddr >= SH3_P1SEG_BASE && end <= SH3_P2SEG_END)
		paddr = SH3_P1SEG_TO_PHYS(vaddr);
	else {
		for (next = (vaddr + PAGE_SIZE) & ~PAGE_MASK;
		    next < end; next += PAGE_SIZE) {
			pmap_extract(pmap, vaddr, &paddr);
			error = _bus_dmamap_load_paddr(t, map,
			    paddr, vaddr, next - vaddr);
			if (error != 0)
				return (error);

			vaddr = next;
		}

		pmap_extract(pmap, vaddr, &paddr);
		size = end - vaddr;
	}

	return (_bus_dmamap_load_paddr(t, map, paddr, vaddr, size));
}

/*
 * Load a DMA map with a linear buffer.
 */
int
_bus_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags)
{
	int error;

	DPRINTF(("bus_dmamap_load: t = %p, map = %p, buf = %p, buflen = %ld, p = %p, flags = %x\n", t, map, buf, buflen, p, flags));

	DMAMAP_RESET(map);

	if (buflen > map->_dm_size)
		return (EINVAL);

	error = _bus_dmamap_load_vaddr(t, map, buf, buflen,
	    p == NULL ? pmap_kernel() : p->p_vmspace->vm_map.pmap);
	if (error != 0) {
		DMAMAP_RESET(map); /* no valid mappings */
		return (error);
	}

	map->dm_nsegs++;

	return (0);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
_bus_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map, struct mbuf *m0,
    int flags)
{
	struct mbuf *m;
	int error;

	DMAMAP_RESET(map);

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic("_bus_dmamap_load_mbuf: no packet header");
#endif

	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	for (m = m0; m != NULL; m = m->m_next) {
		if (m->m_len == 0)
			continue;

		error = _bus_dmamap_load_vaddr(t, map, m->m_data, m->m_len,
		    pmap_kernel());
		if (error != 0) {
			DMAMAP_RESET(map);
			return (error);
		}
	}

	map->dm_nsegs++;

	return (0);
}

/*
 * Like _bus_dmamap_load(), but for uios.
 */
int
_bus_dmamap_load_uio(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio,
    int flags)
{

	panic("_bus_dmamap_load_uio: not implemented");
}

/*
 * Like _bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
_bus_dmamap_load_raw(bus_dma_tag_t t, bus_dmamap_t map,
    bus_dma_segment_t *segs, int nsegs, bus_size_t size, int flags)
{

	panic("_bus_dmamap_load_raw: not implemented");
}

/*
 * Unload a DMA map.
 */
void
_bus_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{

	DPRINTF(("bus_dmamap_unload: t = %p, map = %p\n", t, map));

	map->dm_nsegs = 0;
	map->dm_mapsize = 0;
}

/*
 * Synchronize a DMA map.
 */
void
_bus_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t offset,
    bus_size_t len, int ops)
{
	bus_size_t minlen;
	bus_addr_t addr, naddr;
	int i;

	DPRINTF(("bus_dmamap_sync: t = %p, map = %p, offset = %ld, len = %ld, ops = %x\n", t, map, offset, len, ops));

#ifdef DIAGNOSTIC
	/*
	 * Mixing PRE and POST operations is not allowed.
	 */
	if ((ops & (BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE)) != 0 &&
	    (ops & (BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE)) != 0)
		panic("_bus_dmamap_sync: mix PRE and POST");

	if (offset >= map->dm_mapsize)
		panic("_bus_dmamap_sync: bad offset");
	if ((offset + len) > map->dm_mapsize)
		panic("_bus_dmamap_sync: bad length");
#endif

	if (!sh_cache_enable_dcache) {
		/* Nothing to do */
		DPRINTF(("bus_dmamap_sync: disabled D-Cache\n"));
		return;
	}

	for (i = 0; i < map->dm_nsegs && len != 0; i++) {
		/* Find the beginning segment. */
		if (offset >= map->dm_segs[i].ds_len) {
			offset -= map->dm_segs[i].ds_len;
			continue;
		}

		/*
		 * Now at the first segment to sync; nail
		 * each segment until we have exhausted the
		 * length.
		 */
		minlen = len < map->dm_segs[i].ds_len - offset ?
		    len : map->dm_segs[i].ds_len - offset;

		addr = map->dm_segs[i]._ds_vaddr;
		naddr = addr + offset;

		if ((naddr >= SH3_P2SEG_BASE)
		 && (naddr + minlen <= SH3_P2SEG_END)) {
			DPRINTF(("bus_dmamap_sync: P2SEG (0x%08lx)\n", naddr));
			offset = 0;
			len -= minlen;
			continue;
		}

		DPRINTF(("bus_dmamap_sync: flushing segment %d "
		    "(0x%lx+%lx, 0x%lx+0x%lx) (remain = %ld)\n",
		    i, addr, offset, addr, offset + minlen - 1, len));

		switch (ops) {
		case BUS_DMASYNC_PREREAD|BUS_DMASYNC_PREWRITE:
			if (SH_HAS_WRITEBACK_CACHE)
				sh_dcache_wbinv_range(naddr, minlen);
			else
				sh_dcache_inv_range(naddr, minlen);
			break;

		case BUS_DMASYNC_PREREAD:
			if (SH_HAS_WRITEBACK_CACHE &&
			    ((naddr | minlen) & (sh_cache_line_size - 1)) != 0)
				sh_dcache_wbinv_range(naddr, minlen);
			else
				sh_dcache_inv_range(naddr, minlen);
			break;

		case BUS_DMASYNC_PREWRITE:
			if (SH_HAS_WRITEBACK_CACHE)
				sh_dcache_wb_range(naddr, minlen);
			break;

		case BUS_DMASYNC_POSTREAD:
		case BUS_DMASYNC_POSTREAD|BUS_DMASYNC_POSTWRITE:
			sh_dcache_inv_range(naddr, minlen);
			break;
		}
		offset = 0;
		len -= minlen;
	}
}

/*
 * Allocate memory safe for DMA.
 */
int
_bus_dmamem_alloc(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags)
{
	struct pglist mlist;
	paddr_t curaddr, lastaddr;
	struct vm_page *m;
	int curseg, error, plaflag;

	DPRINTF(("bus_dmamem_alloc: t = %p, size = %ld, alignment = %ld, boundary = %ld, segs = %p, nsegs = %d, rsegs = %p, flags = %x\n", t, size, alignment, boundary, segs, nsegs, rsegs, flags));

	/* Always round the size. */
	size = round_page(size);

	/*
	 * Allocate the pages from the VM system.
	 */
	plaflag = flags & BUS_DMA_NOWAIT ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & BUS_DMA_ZERO)
		plaflag |= UVM_PLA_ZERO;

	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, 0, -1, alignment, boundary,
	    &mlist, nsegs, plaflag);
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = mlist.tqh_first;
	curseg = 0;
	lastaddr = segs[curseg].ds_addr = VM_PAGE_TO_PHYS(m);
	segs[curseg].ds_len = PAGE_SIZE;

	DPRINTF(("bus_dmamem_alloc: m = %p, lastaddr = 0x%08lx\n",m,lastaddr));

	while ((m = TAILQ_NEXT(m, pageq)) != NULL) {
		curaddr = VM_PAGE_TO_PHYS(m);
		DPRINTF(("bus_dmamem_alloc: m = %p, curaddr = 0x%08lx, lastaddr = 0x%08lx\n", m, curaddr, lastaddr));
		if (curaddr == (lastaddr + PAGE_SIZE)) {
			segs[curseg].ds_len += PAGE_SIZE;
		} else {
			DPRINTF(("bus_dmamem_alloc: new segment\n"));
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	DPRINTF(("bus_dmamem_alloc: curseg = %d, *rsegs = %d\n",curseg,*rsegs));

	return (0);
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
_bus_dmamem_free(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs)
{
	struct vm_page *m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

	DPRINTF(("bus_dmamem_free: t = %p, segs = %p, nsegs = %d\n", t, segs, nsegs));

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		DPRINTF(("bus_dmamem_free: segs[%d]: ds_addr = 0x%08lx, ds_len = %ld\n", curseg, segs[curseg].ds_addr, segs[curseg].ds_len));
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE(addr);
			DPRINTF(("bus_dmamem_free: m = %p\n", m));
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}

	uvm_pglistfree(&mlist);
}


int
_bus_dmamem_map(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
    size_t size, caddr_t *kvap, int flags)
{
	vaddr_t va;
	bus_addr_t addr;
	int curseg;
	const struct kmem_dyn_mode *kd;

	DPRINTF(("bus_dmamem_map: t = %p, segs = %p, nsegs = %d, size = %d, kvap = %p, flags = %x\n", t, segs, nsegs, size, kvap, flags));

        /*
	 * If we're only mapping 1 segment, use P2SEG, to avoid
	 * TLB thrashing.
	 */
	if (nsegs == 1) {
		if (flags & BUS_DMA_COHERENT) {
			*kvap = (caddr_t)SH3_PHYS_TO_P2SEG(segs[0].ds_addr);
		} else {
			*kvap = (caddr_t)SH3_PHYS_TO_P1SEG(segs[0].ds_addr);
		}
		DPRINTF(("bus_dmamem_map: addr = 0x%08lx, kva = %p\n", segs[0].ds_addr, *kvap));
		return 0;
	}

	/* Always round the size. */
	size = round_page(size);
	kd = flags & BUS_DMA_NOWAIT ? &kd_trylock : &kd_waitok;
	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, kd);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;
	for (curseg = 0; curseg < nsegs; curseg++) {
		DPRINTF(("bus_dmamem_map: segs[%d]: ds_addr = 0x%08lx, ds_len = %ld\n", curseg, segs[curseg].ds_addr, segs[curseg].ds_len));
		for (addr = segs[curseg].ds_addr;
		     addr < segs[curseg].ds_addr + segs[curseg].ds_len;
		     addr += PAGE_SIZE, va += PAGE_SIZE, size -= PAGE_SIZE) {
			if (size == 0)
				panic("_bus_dmamem_map: size botch");
			pmap_kenter_pa(va, addr,
			    PROT_READ | PROT_WRITE);
		}
	}
	pmap_update(pmap_kernel());

	return (0);
}

void
_bus_dmamem_unmap(bus_dma_tag_t t, caddr_t kva, size_t size)
{

	DPRINTF(("bus_dmamem_unmap: t = %p, kva = %p, size = %d\n", t, kva, size));

#ifdef DIAGNOSTIC
	if ((u_long)kva & PAGE_MASK)
		panic("_bus_dmamem_unmap");
#endif

        /*
	 * Nothing to do if we mapped it with P[12]SEG.
	 */
	if ((kva >= (caddr_t)SH3_P1SEG_BASE)
	 && (kva <= (caddr_t)SH3_P2SEG_END)) {
		return;
	}

	size = round_page(size);
	pmap_kremove((vaddr_t)kva, size);
	pmap_update(pmap_kernel());
	km_free(kva, size, &kv_any, &kp_none);
}

paddr_t
_bus_dmamem_mmap(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
    off_t off, int prot, int flags)
{

	/* Not implemented. */
	return (paddr_t)(-1);
}
@


1.12
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.11 2014/07/12 18:44:42 tedu Exp $	*/
d70 10
d122 1
a122 2
	map->dm_mapsize = 0;		/* no valid mappings */
	map->dm_nsegs = 0;
d141 1
a141 1
static inline int
d143 1
a143 2
    paddr_t paddr, vaddr_t vaddr, int size, int *segp, paddr_t *lastaddrp,
    int first)
a146 3
	bus_addr_t lastaddr;
	int nseg;
	int sgsize;
d148 3
a150 2
	nseg = *segp;
	lastaddr = *lastaddrp;
d152 1
a152 2
	DPRINTF(("_bus_dmamap_load_paddr: t = %p, map = %p, paddr = 0x%08lx, vaddr = 0x%08lx, size = %d\n", t, map, paddr, vaddr, size));
        DPRINTF(("_bus_dmamap_load_paddr: nseg = %d, bmask = 0x%08lx, lastaddr = 0x%08lx\n", nseg, bmask, lastaddr));
d155 1
a155 1
		sgsize = size;
d157 1
a157 3
		/*
		 * Make sure we don't cross any boundaries.
		 */
a165 2
		DPRINTF(("_bus_dmamap_load_paddr: sgsize = %d\n", sgsize));

a171 1
			DPRINTF(("_bus_dmamap_load_paddr: first\n"));
d176 6
d183 7
a189 17
			if ((paddr == lastaddr)
			 && (segs[nseg].ds_len + sgsize <= map->_dm_maxsegsz)
			 && (map->_dm_boundary == 0 ||
			     (segs[nseg].ds_addr & bmask) == (paddr & bmask))) {
				/* coalesce */
				DPRINTF(("_bus_dmamap_load_paddr: coalesce\n"));
				segs[nseg].ds_len += sgsize;
			} else {
				if (++nseg >= map->_dm_segcnt) {
					break;
				}
				/* new segment */
				DPRINTF(("_bus_dmamap_load_paddr: new\n"));
				segs[nseg].ds_addr = SH3_PHYS_TO_P2SEG(paddr);
				segs[nseg].ds_len = sgsize;
				segs[nseg]._ds_vaddr = vaddr;
			}
a195 1
		DPRINTF(("_bus_dmamap_load_paddr: lastaddr = 0x%08lx, paddr = 0x%08lx, vaddr = 0x%08lx, size = %d\n", lastaddr, paddr, vaddr, size));
d198 1
a198 15
	DPRINTF(("_bus_dmamap_load_paddr: nseg = %d\n", nseg));

	*segp = nseg;
	*lastaddrp = lastaddr;

	/*
	 * Did we fit?
	 */
	if (size != 0) {
		/*
		 * If there is a chained window, we will automatically
		 * fall back to it.
		 */
		return (EFBIG);		/* XXX better return value here? */
	}
d203 3
a205 3
static inline int
_bus_bus_dmamap_load_buffer(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, int *segp)
d207 3
a209 7
	bus_size_t sgsize;
	bus_addr_t curaddr;
	bus_size_t len;
	paddr_t lastaddr;
	vaddr_t vaddr = (vaddr_t)buf;
	pmap_t pmap;
	int first;
d212 2
a213 1
	DPRINTF(("_bus_dmamap_load_buffer: t = %p, map = %p, buf = %p, buflen = %ld, p = %p, flags = %x\n", t, map, buf, buflen, p, flags));
d215 11
a225 14
	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	*segp = 0;
	first = 1;
	len = buflen;
	lastaddr = 0;		/* XXX: uwe: gag gcc4, but this is WRONG!!! */
	while (len > 0) {
		/*
		 * Get the physical address for this segment.
		 */
		(void)pmap_extract(pmap, vaddr, &curaddr);
d227 2
a228 11
		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = PAGE_SIZE - ((u_long)vaddr & PGOFSET);
		if (len < sgsize)
			sgsize = len;

		error = _bus_dmamap_load_paddr(t, map, curaddr, vaddr, sgsize,
		    segp, &lastaddr, first);
		if (error)
			return (error);
d230 2
a231 3
		vaddr += sgsize;
		len -= sgsize;
		first = 0;
d234 1
a234 1
	return (0);
a243 4
	bus_addr_t addr = (bus_addr_t)buf;
	paddr_t lastaddr;
	int seg;
	int first;
d248 1
a248 5
	/*
	 * Make sure on error condition we return "no valid mappings."
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;
d253 5
a257 31
	lastaddr = 0;		/* XXX: uwe: gag gcc4, but this is WRONG!!! */

	if ((addr >= SH3_P1SEG_BASE) && (addr + buflen <= SH3_P2SEG_END)) {
		bus_addr_t curaddr;
		bus_size_t sgsize;
		bus_size_t len = buflen;

		DPRINTF(("bus_dmamap_load: P[12]SEG (0x%08lx)\n", addr));

		seg = 0;
		first = 1;
		while (len > 0) {
			curaddr = SH3_P1SEG_TO_PHYS(addr);

			sgsize = PAGE_SIZE - ((u_long)addr & PGOFSET);
			if (len < sgsize)
				sgsize = len;

			error = _bus_dmamap_load_paddr(t, map, curaddr, addr,
			    sgsize, &seg, &lastaddr, first);
			if (error)
				return (error);

			addr += sgsize;
			len -= sgsize;
			first = 0;
		}

		map->dm_nsegs = seg + 1;
		map->dm_mapsize = buflen;
		return (0);
d260 1
a260 7
	error = _bus_bus_dmamap_load_buffer(t, map, buf, buflen, p, flags,
	 &seg);
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = buflen;
		return (0);
	}
d262 1
a262 1
	return (error);
a272 3
	paddr_t lastaddr;
	int seg;
	int first;
d275 1
a275 7
	DPRINTF(("bus_dmamap_load_mbuf: t = %p, map = %p, m0 = %p, flags = %x\n", t, map, m0, flags));

	/*
	 * Make sure on error condition we return "no valid mappings."
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;
d285 1
a285 9
	seg = 0;
	first = 1;
	error = 0;
	lastaddr = 0;		/* XXX: uwe: gag gcc4, but this is WRONG!!! */
	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		paddr_t paddr;
		vaddr_t vaddr;
		int size;

d289 6
a294 6
		vaddr = (vaddr_t)(m->m_data);
		paddr = (paddr_t)(SH3_P1SEG_TO_PHYS(vaddr));
		size = m->m_len;
		error = _bus_dmamap_load_paddr(t, map, paddr, vaddr, size,
		    &seg, &lastaddr, first);
		first = 0;
d297 1
a297 5
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = m0->m_pkthdr.len;
		return (0);
	}
d299 1
a299 1
	return (error);
@


1.11
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.10 2014/07/11 09:36:26 mpi Exp $	*/
d665 1
a665 1
			    VM_PROT_READ | VM_PROT_WRITE);
@


1.10
log
@Convert bus_dmamem_map(9) to km_alloc(9) in order to make it fail and
not sleep if the allocator cannot obtain a lock when BUS_DMA_NOWAIT is
specified.

idea and inputs from kettenis@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.9 2010/04/21 03:03:26 deraadt Exp $	*/
d129 1
a129 1
	free(map, M_DEVBUF);
@


1.9
log
@more cleanup to cope with the change that tries to make proc.h not act
like it is everything.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.8 2009/04/20 00:42:06 oga Exp $	*/
d631 1
d651 2
a652 2

	va = uvm_km_valloc(kernel_map, size);
d695 1
a695 1
	uvm_km_free(kernel_map, (vaddr_t)kva, size);
@


1.8
log
@Add a BUS_DMA_ZERO flag for bus_dmamem_alloc() to return zeroed memory.

Saves every damned driver calling bzero(), and continues the M_ZERO,
PR_ZERO symmetry.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.7 2009/04/14 16:01:04 oga Exp $	*/
d31 1
@


1.7
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.6 2009/03/15 09:35:50 miod Exp $	*/
d549 2
@


1.6
log
@In _bus_dmamem_alloc(), do not bother printing avail_start and avail_end
in the DEBUG code, since these values aren't used there anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.5 2009/03/07 15:34:34 miod Exp $	*/
d538 1
a538 1
	int curseg, error;
a544 1
	TAILQ_INIT(&mlist);
d548 5
a552 2
	error = uvm_pglistalloc(size, 0, -1,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.5
log
@When allocating memory in bus_dmamem_alloc() with uvm_pglistalloc(), do not
try to be smart for the address range, uvm_pglistalloc() is smart enough
nowadays.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.4 2007/10/06 23:12:17 krw Exp $	*/
a534 1
	extern paddr_t avail_start, avail_end;	/* from pmap.c */
a540 1
        DPRINTF(("bus_dmamem_alloc: avail_start = 0x%08lx, avail_end = 0x%08lx\n", avail_start, avail_end));
@


1.4
log
@Some archs used memset() rather than bzero(). So duplicate diff
previously applied to other archs deleting a memset() this time. e.g.

-	if ((mapstore = malloc(mapsize, M_DEVBUF,
-	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
+	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
+	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
 		return (ENOMEM);

-	memset(mapstore, 0, mapsize);
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.3 2007/03/19 20:13:19 miod Exp $	*/
d551 1
a551 1
	error = uvm_pglistalloc(size, avail_start, avail_end - PAGE_SIZE,
@


1.3
log
@bus_dmamap_sync fixes to cope with real life.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.2 2006/10/19 21:16:54 kettenis Exp $	*/
d98 2
a99 2
	if ((mapstore = malloc(mapsize, M_DEVBUF,
	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
a103 1
	memset(mapstore, 0, mapsize);
@


1.2
log
@Add missing TAILQ_INIT.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.1.1.1 2006/10/06 21:16:15 miod Exp $	*/
d447 1
a454 1
#ifdef DIAGNOSTIC
d499 4
a502 1
			sh_dcache_wbinv_range(naddr, minlen);
d506 4
a509 1
			if (((naddr | minlen) & (~(sh_cache_line_size - 1))) == 0) {
a510 3
			} else {
				sh_dcache_wbinv_range(naddr, minlen);
			}
d514 1
a514 1
			if (SH_HAS_WRITEBACK_CACHE) {
d516 5
a520 1
			}
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d541 1
@


1.1.1.1
log
@Incomplete bits for an OpenBSD/landisk port to the I/O DATA USL-5P appliances,
mickey@@ has the other part.
@
text
@@
