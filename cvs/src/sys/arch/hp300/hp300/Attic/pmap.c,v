head	1.39;
access;
symbols
	SMP_SYNC_A:1.39
	SMP_SYNC_B:1.39
	UBC_SYNC_A:1.39
	UBC_SYNC_B:1.39
	OPENBSD_3_0:1.28.0.2
	OPENBSD_3_0_BASE:1.28
	OPENBSD_2_9:1.15.0.2
	OPENBSD_2_9_BASE:1.15
	OPENBSD_2_8:1.14.0.8
	OPENBSD_2_8_BASE:1.14
	OPENBSD_2_7:1.14.0.6
	OPENBSD_2_7_BASE:1.14
	SMP:1.14.0.4
	SMP_BASE:1.14
	kame_19991208:1.14
	OPENBSD_2_6:1.14.0.2
	OPENBSD_2_6_BASE:1.14
	OPENBSD_2_5:1.10.0.2
	OPENBSD_2_5_BASE:1.10
	OPENBSD_2_4:1.9.0.4
	OPENBSD_2_4_BASE:1.9
	OPENBSD_2_3:1.9.0.2
	OPENBSD_2_3_BASE:1.9
	OPENBSD_2_2:1.8.0.2
	OPENBSD_2_2_BASE:1.8
	OPENBSD_2_1:1.7.0.2
	OPENBSD_2_1_BASE:1.7
	OPENBSD_2_0:1.2.0.2
	OPENBSD_2_0_BASE:1.2
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.39
date	2002.01.10.21.10.45;	author miod;	state dead;
branches;
next	1.38;

1.38
date	2001.12.20.19.02.27;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2001.11.30.21.01.58;	author miod;	state dead;
branches;
next	1.36;

1.36
date	2001.11.28.16.24.26;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.11.28.16.13.28;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.11.28.13.47.38;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.11.27.22.05.29;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2001.11.07.01.18.00;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.06.01.53.42;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.04.02.58.54;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2001.09.19.20.50.56;	author mickey;	state Exp;
branches;
next	1.27;

1.27
date	2001.07.25.13.25.31;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.07.18.10.47.04;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.06.27.04.05.45;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.06.08.13.32.54;	author millert;	state Exp;
branches;
next	1.23;

1.23
date	2001.06.08.08.08.43;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.06.05.16.13.15;	author millert;	state Exp;
branches;
next	1.21;

1.21
date	2001.05.16.17.40.02;	author millert;	state Exp;
branches;
next	1.20;

1.20
date	2001.05.09.15.31.24;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.05.06.21.38.59;	author millert;	state Exp;
branches;
next	1.18;

1.18
date	2001.05.05.21.26.35;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.05.05.02.41.24;	author millert;	state Exp;
branches;
next	1.16;

1.16
date	2001.05.04.22.48.59;	author aaron;	state Exp;
branches;
next	1.15;

1.15
date	2001.04.06.23.54.47;	author millert;	state Exp;
branches;
next	1.14;

1.14
date	99.09.03.18.00.42;	author art;	state Exp;
branches
	1.14.4.1;
next	1.13;

1.13
date	99.07.18.18.00.04;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	99.07.18.16.45.47;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	99.07.18.16.23.46;	author deraadt;	state Exp;
branches;
next	1.10;

1.10
date	99.01.11.05.11.20;	author millert;	state Exp;
branches;
next	1.9;

1.9
date	97.11.06.19.42.38;	author millert;	state Exp;
branches;
next	1.8;

1.8
date	97.07.06.08.02.07;	author downsj;	state Exp;
branches;
next	1.7;

1.7
date	97.04.16.11.56.30;	author downsj;	state Exp;
branches;
next	1.6;

1.6
date	97.03.26.08.32.44;	author downsj;	state Exp;
branches;
next	1.5;

1.5
date	97.02.24.01.16.09;	author downsj;	state Exp;
branches;
next	1.4;

1.4
date	97.02.10.11.13.32;	author downsj;	state Exp;
branches;
next	1.3;

1.3
date	97.01.12.15.13.25;	author downsj;	state Exp;
branches;
next	1.2;

1.2
date	95.12.14.05.29.03;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.50.23;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.50.23;	author deraadt;	state Exp;
branches;
next	;

1.14.4.1
date	2001.04.18.16.05.40;	author niklas;	state Exp;
branches;
next	1.14.4.2;

1.14.4.2
date	2001.07.04.10.15.47;	author niklas;	state Exp;
branches;
next	1.14.4.3;

1.14.4.3
date	2001.10.31.02.52.46;	author nate;	state Exp;
branches;
next	1.14.4.4;

1.14.4.4
date	2001.11.13.21.00.51;	author niklas;	state Exp;
branches;
next	1.14.4.5;

1.14.4.5
date	2001.12.05.00.39.09;	author niklas;	state dead;
branches;
next	;


desc
@@


1.39
log
@Switch m68k arches that do not exist in 68060 models back to pmap_motorola.
Others will be switched after more 060 tests.

This time: hp300
@
text
@/*	$OpenBSD: pmap.c,v 1.38 2001/12/20 19:02:27 miod Exp $	*/
/*	$NetBSD: pmap.c,v 1.80 1999/09/16 14:52:06 chs Exp $	*/

/*-
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.6 (Berkeley) 5/27/94
 */

/*
 * HP9000/300 series physical map management code.
 *
 * Supports:
 *	68020 with HP MMU	models 320, 350
 *	68020 with 68851 MMU	models 318, 319, 330
 *	68030 with on-chip MMU	models 340, 360, 370, 345, 375, 400
 *	68040 with on-chip MMU	models 380, 385, 425, 433
 *
 * Notes:
 *	Don't even pay lip service to multiprocessor support.
 *
 *	We assume TLB entries don't have process tags (except for the
 *	supervisor/user distinction) so we only invalidate TLB entries
 *	when changing mappings for the current (or kernel) pmap.  This is
 *	technically not true for the 68851 but we flush the TLB on every
 *	context switch, so it effectively winds up that way.
 *
 *	Bitwise and/or operations are significantly faster than bitfield
 *	references so we use them when accessing STE/PTEs in the pmap_pte_*
 *	macros.  Note also that the two are not always equivalent; e.g.:
 *		(*pte & PG_PROT) [4] != pte->pg_prot [1]
 *	and a couple of routines that deal with protection and wiring take
 *	some shortcuts that assume the and/or definitions.
 *
 *	This implementation will only work for PAGE_SIZE == NBPG
 *	(i.e. 4096 bytes).
 */

/*
 *	Manages physical address maps.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/user.h>
#include <sys/pool.h>

#include <machine/pte.h>

#include <uvm/uvm.h>

#include <machine/cpu.h>

#ifdef DEBUG
#define PDB_FOLLOW	0x0001
#define PDB_INIT	0x0002
#define PDB_ENTER	0x0004
#define PDB_REMOVE	0x0008
#define PDB_CREATE	0x0010
#define PDB_PTPAGE	0x0020
#define PDB_CACHE	0x0040
#define PDB_BITS	0x0080
#define PDB_COLLECT	0x0100
#define PDB_PROTECT	0x0200
#define PDB_SEGTAB	0x0400
#define PDB_MULTIMAP	0x0800
#define PDB_PARANOIA	0x2000
#define PDB_WIRING	0x4000
#define PDB_PVDUMP	0x8000

int debugmap = 0;
int pmapdebug = PDB_PARANOIA;

#define	PMAP_DPRINTF(l, x)	if (pmapdebug & (l)) printf x

#if defined(M68040)
int dowriteback = 1;	/* 68040: enable writeback caching */
int dokwriteback = 1;	/* 68040: enable writeback caching of kernel AS */
#endif
#else /* ! DEBUG */
#define	PMAP_DPRINTF(l, x)	/* nothing */
#endif /* DEBUG */

/*
 * Get STEs and PTEs for user/kernel address space
 */
#if defined(M68040)
#define	pmap_ste1(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
/* XXX assumes physically contiguous ST pages (if more than one) */
#define pmap_ste2(m, v) \
	(&((m)->pm_stab[(st_entry_t *)(*(u_int *)pmap_ste1(m, v) & SG4_ADDR1) \
			- (m)->pm_stpa + (((v) & SG4_MASK2) >> SG4_SHIFT2)]))
#define	pmap_ste(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) \
			>> (mmutype == MMU_68040 ? SG4_SHIFT1 : SG_ISHIFT)]))
#define pmap_ste_v(m, v) \
	(mmutype == MMU_68040 \
	 ? ((*pmap_ste1(m, v) & SG_V) && \
	    (*pmap_ste2(m, v) & SG_V)) \
	 : (*pmap_ste(m, v) & SG_V))
#else
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
#define pmap_ste_v(m, v) (*pmap_ste(m, v) & SG_V)
#endif

#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))
#define pmap_pte_pa(pte)	(*(pte) & PG_FRAME)
#define pmap_pte_w(pte)		(*(pte) & PG_W)
#define pmap_pte_ci(pte)	(*(pte) & PG_CI)
#define pmap_pte_m(pte)		(*(pte) & PG_M)
#define pmap_pte_u(pte)		(*(pte) & PG_U)
#define pmap_pte_prot(pte)	(*(pte) & PG_PROT)
#define pmap_pte_v(pte)		(*(pte) & PG_V)

#define pmap_pte_set_w(pte, v) \
	if (v) *(pte) |= PG_W; else *(pte) &= ~PG_W
#define pmap_pte_set_prot(pte, v) \
	if (v) *(pte) |= PG_PROT; else *(pte) &= ~PG_PROT
#define pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

/*
 * Given a map and a machine independent protection code,
 * convert to an hp300 protection code.
 */
#define pte_prot(m, p)	(protection_codes[p])
int	protection_codes[8];

/*
 * Kernel page table page management.
 */
struct kpt_page {
	struct kpt_page *kpt_next;	/* link on either used or free list */
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
};
struct kpt_page *kpt_free_list, *kpt_used_list;
struct kpt_page *kpt_pages;

/*
 * Kernel segment/page table and page table map.
 * The page table map gives us a level of indirection we need to dynamically
 * expand the page table.  It is essentially a copy of the segment table
 * with PTEs instead of STEs.  All are initialized in locore at boot time.
 * Sysmap will initially contain VM_KERNEL_PT_PAGES pages of PTEs.
 * Segtabzero is an empty segment table which all processes share til they
 * reference something.
 */
st_entry_t	*Sysseg;
pt_entry_t	*Sysmap, *Sysptmap;
st_entry_t	*Segtabzero, *Segtabzeropa;
vsize_t		Sysptsize = VM_KERNEL_PT_PAGES;

struct pmap	kernel_pmap_store;
struct vm_map	*st_map, *pt_map;
struct vm_map	st_map_store, pt_map_store;

paddr_t		avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vsize_t		mem_size;	/* memory size in bytes */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
int		page_cnt;	/* number of pages managed by VM system */

boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
struct pv_entry	*pv_table;
char		*pmap_attributes;	/* reference and modify bits */
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int		pv_nfree;

#ifdef M68K_MMU_HP
int		pmap_aliasmask;	/* seperation at which VA aliasing ok */
#endif
#if defined(M68040)
int		protostfree;	/* prototype (default) free ST map */
#endif

extern caddr_t	CADDR1, CADDR2;

pt_entry_t	*caddr1_pte;	/* PTE for CADDR1 */
pt_entry_t	*caddr2_pte;	/* PTE for CADDR2 */

struct pool	pmap_pmap_pool;	/* memory pool for pmap structures */

struct pv_entry *pmap_alloc_pv __P((void));
void	pmap_free_pv __P((struct pv_entry *));
void	pmap_collect_pv __P((void));
#ifdef COMPAT_HPUX
int	pmap_mapmulti __P((pmap_t, vaddr_t));
#endif /* COMPAT_HPUX */

#define	PAGE_IS_MANAGED(pa)	(pmap_initialized &&			\
				 vm_physseg_find(atop((pa)), NULL) != -1)

#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})

#define	pa_to_attribute(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
})

/*
 * Internal routines
 */
void	pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));
boolean_t pmap_testbit	__P((paddr_t, int));
void	pmap_changebit	__P((paddr_t, int, int));
void	pmap_enter_ptpage	__P((pmap_t, vaddr_t));
void	pmap_ptpage_addref __P((vaddr_t));
int	pmap_ptpage_delref __P((vaddr_t));
void	pmap_collect1	__P((pmap_t, paddr_t, paddr_t));
void	pmap_pinit __P((pmap_t));
void	pmap_release __P((pmap_t));

#ifdef DEBUG
void pmap_pvdump	__P((paddr_t));
void pmap_check_wiring	__P((char *, vaddr_t));
#endif

/* pmap_remove_mapping flags */
#define	PRM_TFLUSH	0x01
#define	PRM_CFLUSH	0x02
#define	PRM_KEEPPTPAGE	0x04

/*
 * pmap_virtual_space:		[ INTERFACE ]
 *
 *	Report the range of available kernel virtual address
 *	space to the VM system during bootstrap.
 *
 *	This is only an interface function if we do not use
 *	pmap_steal_memory()!
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_virtual_space(vstartp, vendp)
	vaddr_t	*vstartp, *vendp;
{

	*vstartp = virtual_avail;
	*vendp = virtual_end;
}

/*
 * pmap_init:			[ INTERFACE ]
 *
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_init()
{
	vaddr_t		addr, addr2;
	vsize_t		s;
	struct pv_entry	*pv;
	char		*attr;
	int		rv;
	int		npages;
	int		bank;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));

	/*
	 * Before we do anything else, initialize the PTE pointers
	 * used by pmap_zero_page() and pmap_copy_page().
	 */
	caddr1_pte = pmap_pte(pmap_kernel(), CADDR1);
	caddr2_pte = pmap_pte(pmap_kernel(), CADDR2);

	/*
	 * Now that kernel map has been allocated, we can mark as
	 * unavailable regions which we have mapped in pmap_bootstrap().
	 */
	addr = (vaddr_t) intiobase;
	if (uvm_map(kernel_map, &addr,
		    m68k_ptob(IIOMAPSIZE+EIOMAPSIZE),
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)))
		goto bogons;
	addr = (vaddr_t) Sysmap;
	if (uvm_map(kernel_map, &addr, HP_MAX_PTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED))) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
 bogons:
		panic("pmap_init: bogons in the VM system!\n");
	}

	PMAP_DPRINTF(PDB_INIT,
	    ("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
	    Sysseg, Sysmap, Sysptmap));
	PMAP_DPRINTF(PDB_INIT,
	    ("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
	    avail_start, avail_end, virtual_avail, virtual_end));

	/*
	 * Allocate memory for random pmap data structures.  Includes the
	 * initial segment table, pv_head_table and pmap_attributes.
	 */
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++)
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
	s = HP_STSIZE;					/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */
	s = round_page(s);
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: can't allocate data structures");

	Segtabzero = (st_entry_t *) addr;
	pmap_extract(pmap_kernel(), addr, (paddr_t *)Segtabzeropa);
	addr += HP_STSIZE;

	pv_table = (struct pv_entry *) addr;
	addr += page_cnt * sizeof(struct pv_entry);

	pmap_attributes = (char *) addr;

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
	    "tbl %p atr %p\n",
	    s, page_cnt, Segtabzero, Segtabzeropa,
	    pv_table, pmap_attributes));

	/*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}

	/*
	 * Allocate physical memory for kernel PT pages and their management.
	 * We need 1 PT page per possible task plus some slop.
	 */
	npages = min(atop(HP_MAX_KPTSIZE), maxproc+16);
	s = ptoa(npages) + round_page(npages * sizeof(struct kpt_page));

	/*
	 * Verify that space will be allocated in region for which
	 * we already have kernel PT pages.
	 */
	addr = 0;
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv || (addr + s) >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	uvm_unmap(kernel_map, addr, addr + s);

	/*
	 * Now allocate the space and link the pages together to
	 * form the KPT free list.
	 */
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
	s = ptoa(npages);
	addr2 = addr + s;
	kpt_pages = &((struct kpt_page *)addr2)[npages];
	kpt_free_list = NULL;
	do {
		addr2 -= NBPG;
		(--kpt_pages)->kpt_next = kpt_free_list;
		kpt_free_list = kpt_pages;
		kpt_pages->kpt_va = addr2;
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
	} while (addr != addr2);

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: KPT: %ld pages from %lx to %lx\n",
	    atop(s), addr, addr + s));

	/*
	 * Allocate the segment table map and the page table map.
	 */
	s = maxproc * HP_STSIZE;
	st_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, 0, FALSE,
	    &st_map_store);

	addr = HP_PTBASE;
	if ((HP_PTMAXSIZE / HP_MAX_PTSIZE) < maxproc) {
		s = HP_PTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(3).
		 */
		maxproc = (HP_PTMAXSIZE / HP_MAX_PTSIZE);
	} else
		s = (maxproc * HP_MAX_PTSIZE);
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, VM_MAP_PAGEABLE,
	    TRUE, &pt_map_store);

#if defined(M68040)
	if (mmutype == MMU_68040) {
		protostfree = ~l2tobm(0);
		for (rv = MAXUL2SIZE; rv < sizeof(protostfree)*NBBY; rv++)
			protostfree &= ~l2tobm(rv);
	}
#endif

	/*
	 * Initialize the pmap pools.
	 */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

	/*
	 * Now it is safe to enable pv_table recording.
	 */
	pmap_initialized = TRUE;
}

/*
 * pmap_alloc_pv:
 *
 *	Allocate a pv_entry.
 */
struct pv_entry *
pmap_alloc_pv()
{
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;

	if (pv_nfree == 0) {
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		}
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
	return pv;
}

/*
 * pmap_free_pv:
 *
 *	Free a pv_entry.
 */
void
pmap_free_pv(pv)
	struct pv_entry *pv;
{
	struct pv_page *pvp;

	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
		break;
	}
}

/*
 * pmap_collect_pv:
 *
 *	Perform compaction on the PV list, called via pmap_collect().
 */
void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;

	TAILQ_INIT(&pv_page_collectlist);

	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp,
			    pvp_pgi.pgi_list);
			pv_nfree -= NPVPPG;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}

	if (pv_page_collectlist.tqh_first == 0)
		return;

	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
		if (ph->pv_pmap == 0)
			continue;
		s = splvm();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp,
					    pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: pgi_nfree inconsistent");
#endif
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}

	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
	}
}

/*
 * pmap_map:
 *
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
 */
vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));

	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
		va += NBPG;
		spa += NBPG;
	}
	pmap_update(pmap_kernel());
	return (va);
}

/*
 * pmap_create:			[ INTERFACE ]
 *
 *	Create and return a physical map.
 *
 *	Note: no locking is necessary in this function.
 */
pmap_t
pmap_create()
{
	pmap_t pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create\n"));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return (pmap);
}

/*
 * pmap_pinit:
 *
 *	Initialize a preallocated and zeroed pmap structure.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_create()!
 */
void
pmap_pinit(pmap)
	struct pmap *pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_pinit(%p)\n", pmap));

	/*
	 * No need to allocate page table space yet but we do need a
	 * valid segment table.  Initially, we point everyone at the
	 * "null" segment table.  On the first pmap_enter, a real
	 * segment table will be allocated.
	 */
	pmap->pm_stab = Segtabzero;
	pmap->pm_stpa = Segtabzeropa;
#if defined(M68040)
	if (mmutype == MMU_68040)
		pmap->pm_stfree = protostfree;
#endif
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
}

/*
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
 */
void
pmap_destroy(pmap)
	pmap_t pmap;
{
	int count;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));
	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		pool_put(&pmap_pmap_pool, pmap);
	}
}

/*
 * pmap_release:
 *
 *	Release the resources held by a pmap.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_destroy().
 */
void
pmap_release(pmap)
	struct pmap *pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_release(%p)\n", pmap));

#ifdef notdef /* DIAGNOSTIC */
	/* count would be 0 from pmap_destroy... */
	simple_lock(&pmap->pm_lock);
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif

	if (pmap->pm_ptab) {
		pmap_remove(pmap_kernel(), (vaddr_t)pmap->pm_ptab,
		    (vaddr_t)pmap->pm_ptab + HP_MAX_PTSIZE);
		uvm_km_pgremove(uvm.kernel_object, (vaddr_t)pmap->pm_ptab,
		    (vaddr_t)pmap->pm_ptab + HP_MAX_PTSIZE);
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
				   HP_MAX_PTSIZE);
	}
	KASSERT(pmap->pm_stab == Segtabzero);
}

/*
 * pmap_reference:		[ INTERFACE ]
 *
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap)
	pmap_t	pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_reference(%p)\n", pmap));
	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}

/*
 * pmap_activate:		[ INTERFACE ]
 *
 *	Activate the pmap used by the specified process.  This includes
 *	reloading the MMU context of the current process, and marking
 *	the pmap in use by the processor.
 *
 *	Note: we may only use spin locks here, since we are called
 *	by a critical section in cpu_switch()!
 */
void
pmap_activate(p)
	struct proc *p;
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_SEGTAB,
	    ("pmap_activate(%p)\n", p));

	PMAP_ACTIVATE(pmap, p == curproc);
}

/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.
 */
void
pmap_deactivate(p)
	struct proc *p;
{

	/* No action necessary in this pmap implementation. */
}

/*
 * pmap_remove:			[ INTERFACE ]
 *
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_remove(pmap, sva, eva)
	pmap_t pmap;
	vaddr_t sva, eva;
{
	vaddr_t nssva;
	pt_entry_t *pte;
	boolean_t firstpage, needcflush;
	int flags;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva));

	firstpage = TRUE;
	needcflush = FALSE;
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
	while (sva < eva) {
		nssva = hp300_trunc_seg(sva) + HP_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;

		/*
		 * Invalidate every valid mapping within this segment.
		 */

		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {

			/*
			 * If this segment is unallocated,
			 * skip to the next segment boundary.
			 */

			if (!pmap_ste_v(pmap, sva)) {
				sva = nssva;
				break;
			}
			if (pmap_pte_v(pte)) {
#ifdef M68K_MMU_HP
				if (pmap_aliasmask) {
					/*
					 * Purge kernel side of VAC to ensure
					 * we get the correct state of any
					 * hardware maintained bits.
					 */
					if (firstpage) {
						DCIS();
					}
					/*
					 * Remember if we may need to
					 * flush the VAC due to a non-CI
					 * mapping.
					 */
					if (!needcflush && !pmap_pte_ci(pte))
						needcflush = TRUE;

				}
#endif
				pmap_remove_mapping(pmap, sva, pte, flags);
				firstpage = FALSE;
			}
			pte++;
			sva += NBPG;
		}
	}
	/*
	 * Didn't do anything, no need for cache flushes
	 */
	if (firstpage)
		return;
#ifdef M68K_MMU_HP
	/*
	 * In a couple of cases, we don't need to worry about flushing
	 * the VAC:
	 * 	1. if this is a kernel mapping,
	 *	   we have already done it
	 *	2. if it is a user mapping not for the current process,
	 *	   it won't be there
	 */
	if (pmap_aliasmask && !active_user_pmap(pmap))
		needcflush = FALSE;
	if (needcflush) {
		if (pmap == pmap_kernel()) {
			DCIS();
		} else {
			DCIU();
		}
	}
#endif
}

/*
 * pmap_page_protect:		[ INTERFACE ]
 *
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
 */
void
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t	prot;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	struct pv_entry *pv;
	int s;

#ifdef DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    (prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE)))
		printf("pmap_page_protect(%lx, %x)\n", pa, prot);
#endif

	switch (prot) {
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
		return;
	/* copy_on_write */
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_changebit(pa, PG_RO, ~0);
		return;
	/* remove_all */
	default:
		break;
	}
	pv = pa_to_pvh(pa);
	s = splvm();
	while (pv->pv_pmap != NULL) {
		pt_entry_t *pte;

		pte = pmap_pte(pv->pv_pmap, pv->pv_va);
#ifdef DEBUG
		if (!pmap_ste_v(pv->pv_pmap, pv->pv_va) ||
		    pmap_pte_pa(pte) != pa)
			panic("pmap_page_protect: bad mapping");
#endif
		if (!pmap_pte_w(pte))
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
					    pte, PRM_TFLUSH|PRM_CFLUSH);
		else {
			pv = pv->pv_next;
#ifdef DEBUG
			if (pmapdebug & PDB_PARANOIA)
				printf("%s wired mapping for %lx not removed\n",
				       "pmap_page_protect:", pa);
#endif
			if (pv == NULL)
				break;
		}
	}
	splx(s);
}

/*
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protection on the specified range of this map
 *	as requested.
 */
void
pmap_protect(pmap, sva, eva, prot)
	pmap_t		pmap;
	vaddr_t		sva, eva;
	vm_prot_t	prot;
{
	vaddr_t nssva;
	pt_entry_t *pte;
	boolean_t firstpage, needtflush;
	int isro;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_protect(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, prot));

	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	isro = pte_prot(pmap, prot);
	needtflush = active_pmap(pmap);
	firstpage = TRUE;
	while (sva < eva) {
		nssva = hp300_trunc_seg(sva) + HP_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}
		/*
		 * Change protection on mapping if it is valid and doesn't
		 * already have the correct protection.
		 */
		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte) && pmap_pte_prot_chg(pte, isro)) {
#ifdef M68K_MMU_HP
				/*
				 * Purge kernel side of VAC to ensure we
				 * get the correct state of any hardware
				 * maintained bits.
				 *
				 * XXX do we need to clear the VAC in
				 * general to reflect the new protection?
				 */
				if (firstpage && pmap_aliasmask)
					DCIS();
#endif
#if defined(M68040)
				/*
				 * Clear caches if making RO (see section
				 * "7.3 Cache Coherency" in the manual).
				 */
				if (isro && mmutype == MMU_68040) {
					paddr_t pa = pmap_pte_pa(pte);

					DCFP(pa);
					ICPP(pa);
				}
#endif
				pmap_pte_set_prot(pte, isro);
				if (needtflush)
					TBIS(sva);
				firstpage = FALSE;
			}
			pte++;
			sva += NBPG;
		}
	}
}

/*
 * pmap_enter:			[ INTERFACE ]
 *
 *	Insert the given physical page (pa) at
 *	the specified virtual address (va) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte cannot be reclaimed.
 *
 *	Note: This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
int
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	pt_entry_t *pte;
	int npte;
	paddr_t opa;
	boolean_t cacheable = TRUE;
	boolean_t checkpv = TRUE;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, %lx, %lx, %x, %x)\n",
	    pmap, va, pa, prot, wired));

#ifdef DIAGNOSTIC
	/*
	 * pmap_enter() should never be used for CADDR1 and CADDR2.
	 */
	if (pmap == pmap_kernel() &&
	    (va == (vaddr_t)CADDR1 || va == (vaddr_t)CADDR2))
		panic("pmap_enter: used for CADDR1 or CADDR2");
#endif

	/*
	 * For user mapping, allocate kernel VM resources if necessary.
	 */
	if (pmap->pm_ptab == NULL)
		pmap->pm_ptab = (pt_entry_t *)
			uvm_km_valloc_wait(pt_map, HP_MAX_PTSIZE);

	/*
	 * Segment table entry not valid, we need a new PT page
	 */
	if (!pmap_ste_v(pmap, va))
		pmap_enter_ptpage(pmap, va);

	pa = trunc_page(pa);
	pte = pmap_pte(pmap, va);
	opa = pmap_pte_pa(pte);

	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));

	/*
	 * Mapping has not changed, must be protection or wiring change.
	 */
	if (opa == pa) {
		/*
		 * Wiring change, just update stats.
		 * We don't worry about wiring PT pages as they remain
		 * resident as long as there are valid mappings in them.
		 * Hence, if a user page is wired, the PT page will be also.
		 */
		if (pmap_pte_w_chg(pte, wired ? PG_W : 0)) {
			PMAP_DPRINTF(PDB_ENTER,
			    ("enter: wiring change -> %x\n", wired));
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
		}
		/*
		 * Retain cache inhibition status
		 */
		checkpv = FALSE;
		if (pmap_pte_ci(pte))
			cacheable = FALSE;
		goto validate;
	}

	/*
	 * Mapping has changed, invalidate old range and fall through to
	 * handle validating new mapping.
	 */
	if (opa) {
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: removing old mapping %lx\n", va));
		pmap_remove_mapping(pmap, va, pte,
		    PRM_TFLUSH|PRM_CFLUSH|PRM_KEEPPTPAGE);
	}

	/*
	 * If this is a new user mapping, increment the wiring count
	 * on this PT page.  PT pages are wired down as long as there
	 * is a valid mapping in the page.
	 */
	if (pmap != pmap_kernel()) {
		pmap_ptpage_addref(trunc_page((vaddr_t)pte));
	}

	/*
	 * Enter on the PV list if part of our managed memory
	 * Note that we raise IPL while manipulating pv_table
	 * since pmap_enter can be called at interrupt time.
	 */
	if (PAGE_IS_MANAGED(pa)) {
		struct pv_entry *pv, *npv;
		int s;

		pv = pa_to_pvh(pa);
		s = splvm();
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: pv at %p: %lx/%p/%p\n",
		    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
		/*
		 * No entries yet, use header as the first entry
		 */
		if (pv->pv_pmap == NULL) {
			pv->pv_va = va;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
			pv->pv_ptste = NULL;
			pv->pv_ptpmap = NULL;
			pv->pv_flags = 0;
		}
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
		else {
#ifdef DEBUG
			for (npv = pv; npv; npv = npv->pv_next)
				if (pmap == npv->pv_pmap && va == npv->pv_va)
					panic("pmap_enter: already in pv_tab");
#endif
			npv = pmap_alloc_pv();
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_ptste = NULL;
			npv->pv_ptpmap = NULL;
			npv->pv_flags = 0;
			pv->pv_next = npv;
#ifdef M68K_MMU_HP
			/*
			 * Since there is another logical mapping for the
			 * same page we may need to cache-inhibit the
			 * descriptors on those CPUs with external VACs.
			 * We don't need to CI if:
			 *
			 * - No two mappings belong to the same user pmaps.
			 *   Since the cache is flushed on context switches
			 *   there is no problem between user processes.
			 *
			 * - Mappings within a single pmap are a certain
			 *   magic distance apart.  VAs at these appropriate
			 *   boundaries map to the same cache entries or
			 *   otherwise don't conflict.
			 *
			 * To keep it simple, we only check for these special
			 * cases if there are only two mappings, otherwise we
			 * punt and always CI.
			 *
			 * Note that there are no aliasing problems with the
			 * on-chip data-cache when the WA bit is set.
			 */
			if (pmap_aliasmask) {
				if (pv->pv_flags & PV_CI) {
					PMAP_DPRINTF(PDB_CACHE,
					    ("enter: pa %lx already CI'ed\n",
					    pa));
					checkpv = cacheable = FALSE;
				} else if (npv->pv_next ||
					   ((pmap == pv->pv_pmap ||
					     pmap == pmap_kernel() ||
					     pv->pv_pmap == pmap_kernel()) &&
					    ((pv->pv_va & pmap_aliasmask) !=
					     (va & pmap_aliasmask)))) {
					PMAP_DPRINTF(PDB_CACHE,
					    ("enter: pa %lx CI'ing all\n",
					    pa));
					cacheable = FALSE;
					pv->pv_flags |= PV_CI;
				}
			}
#endif
		}

		/*
		 * Speed pmap_is_referenced() or pmap_is_modified() based
		 * on the hint provided in access_type.
		 */
#ifdef DIAGNOSTIC
		if ((flags & VM_PROT_ALL) & ~prot)
			panic("pmap_enter: access type exceeds prot");
#endif
		if (flags & VM_PROT_WRITE)
			*pa_to_attribute(pa) |= (PG_U|PG_M);
		else if (flags & VM_PROT_ALL)
			*pa_to_attribute(pa) |= PG_U;

		splx(s);
	}
	/*
	 * Assumption: if it is not part of our managed memory
	 * then it must be device memory which may be volitile.
	 */
	else if (pmap_initialized) {
		checkpv = cacheable = FALSE;
	}

	/*
	 * Increment counters
	 */
	pmap->pm_stats.resident_count++;
	if (wired)
		pmap->pm_stats.wired_count++;

validate:
#ifdef M68K_MMU_HP
	/*
	 * Purge kernel side of VAC to ensure we get correct state
	 * of HW bits so we don't clobber them.
	 */
	if (pmap_aliasmask)
		DCIS();
#endif
	/*
	 * Build the new PTE.
	 */
	npte = pa | pte_prot(pmap, prot) | (*pte & (PG_M|PG_U)) | PG_V;
	if (wired)
		npte |= PG_W;

#if defined(M68040)
	/* Don't cache if process can't take it, like SunOS ones.  */
	if (mmutype == MMU_68040 && pmap != pmap_kernel() &&
	    (curproc->p_md.md_flags & MDP_UNCACHE_WX) &&
	    (prot & VM_PROT_EXECUTE) && (prot & VM_PROT_WRITE))
		checkpv = cacheable = FALSE;
#endif

	if (!checkpv && !cacheable)
		npte |= PG_CI;
#if defined(M68040)
	if (mmutype == MMU_68040 && (npte & (PG_PROT|PG_CI)) == PG_RW)
#ifdef DEBUG
		if (dowriteback && (dokwriteback || pmap != pmap_kernel()))
#endif
		npte |= PG_CCB;
#endif

	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));

	/*
	 * Remember if this was a wiring-only change.
	 * If so, we need not flush the TLB and caches.
	 */
	wired = ((*pte ^ npte) == PG_W);
#if defined(M68040)
	if (mmutype == MMU_68040 && !wired) {
		DCFP(pa);
		ICPP(pa);
	}
#endif
	*pte = npte;
	if (!wired && active_pmap(pmap))
		TBIS(va);
#ifdef M68K_MMU_HP
	/*
	 * The following is executed if we are entering a second
	 * (or greater) mapping for a physical page and the mappings
	 * may create an aliasing problem.  In this case we must
	 * cache inhibit the descriptors involved and flush any
	 * external VAC.
	 */
	if (checkpv && !cacheable) {
		pmap_changebit(pa, PG_CI, ~0);
		DCIA();
#ifdef DEBUG
		if ((pmapdebug & (PDB_CACHE|PDB_PVDUMP)) ==
		    (PDB_CACHE|PDB_PVDUMP))
			pmap_pvdump(pa);
#endif
	}
#endif
#ifdef DEBUG
	if ((pmapdebug & PDB_WIRING) && pmap != pmap_kernel())
		pmap_check_wiring("enter", trunc_page((vaddr_t)pte));
#endif

	return (0);
}

void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	struct pmap *pmap = pmap_kernel();
	pt_entry_t *pte;
	int s, npte;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%lx, %lx, %x)\n", va, pa, prot));

	/*
	 * Segment table entry not valid, we need a new PT page
	 */

	if (!pmap_ste_v(pmap, va)) { 
		s = splvm();
		pmap_enter_ptpage(pmap, va);
		splx(s);
	}

	pa = m68k_trunc_page(pa);
	pte = pmap_pte(pmap, va);

	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));
	KASSERT(!pmap_pte_v(pte));

	/*
	 * Increment counters
	 */

	pmap->pm_stats.resident_count++;
	pmap->pm_stats.wired_count++;

	/*
	 * Build the new PTE.
	 */

	npte = pa | pte_prot(pmap, prot) | PG_V | PG_W;
#if defined(M68040)
	if (mmutype == MMU_68040 && (npte & (PG_PROT)) == PG_RW)
		npte |= PG_CCB;
#endif

	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));
#if defined(M68040)
	if (mmutype == MMU_68040) {
		DCFP(pa);
		ICPP(pa);
	}
#endif
	*pte = npte;
}

void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	struct pmap *pmap = pmap_kernel();
	vaddr_t sva, eva, nssva;
	pt_entry_t *pte;
	boolean_t firstpage, needcflush;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_kremove(%lx, %lx)\n", va, len));

	sva = va;
	eva = va + len;
	firstpage = TRUE;
	needcflush = FALSE;
	while (sva < eva) {
		nssva = hp300_trunc_seg(sva) + HP_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;

		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */

		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}

		/*
		 * Invalidate every valid mapping within this segment.
		 */

		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte)) {
#ifdef DEBUG
				struct pv_entry *pv;
				int s;

				pv = pa_to_pvh(pmap_pte_pa(pte));
				s = splvm();
				while (pv->pv_pmap != NULL) {
					KASSERT(pv->pv_pmap != pmap_kernel() ||
					    pv->pv_va != sva);
					pv = pv->pv_next;
					if (pv == NULL) {
						break;
					}
				}
				splx(s);
#endif
#ifdef M68K_MMU_HP
				if (pmap_aliasmask) {

					/*
					 * Purge kernel side of VAC to ensure
					 * we get the correct state of any
					 * hardware maintained bits.
					 */

					if (firstpage) {
						DCIS();
					}

					/*
					 * Remember if we may need to
					 * flush the VAC.
					 */

					needcflush = TRUE;
				}
#endif
				/*
				 * Update statistics
				 */

				pmap->pm_stats.wired_count--;
				pmap->pm_stats.resident_count--;

				/*
				 * Invalidate the PTE.
				 */

				*pte = PG_NV;
				TBIS(sva);
				firstpage = FALSE;
			}
			pte++;
			sva += NBPG;
		}
	}

	/*
	 * Didn't do anything, no need for cache flushes
	 */

	if (firstpage)
		return;
#ifdef M68K_MMU_HP

	/*
	 * In a couple of cases, we don't need to worry about flushing
	 * the VAC:
	 *      1. if this is a kernel mapping,
	 *         we have already done it
	 *      2. if it is a user mapping not for the current process,
	 *         it won't be there
	 */

	if (pmap_aliasmask && !active_user_pmap(pmap))
		needcflush = FALSE;
	if (needcflush) {
		if (pmap == pmap_kernel()) {
			DCIS();
		} else {
			DCIU();
		}
	}
#endif
}

/*
 * pmap_unwire:			[ INTERFACE]
 *
 *	Clear the wired attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap, va)
	pmap_t		pmap;
	vaddr_t		va;
{
	pt_entry_t *pte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %lx)\n", pmap, va));

	pte = pmap_pte(pmap, va);
#ifdef DEBUG
	/*
	 * Page table page is not allocated.
	 * Should this ever happen?  Ignore it for now,
	 * we don't want to force allocation of unnecessary PTE pages.
	 */
	if (!pmap_ste_v(pmap, va)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid STE for %lx\n", va);
		return;
	}
	/*
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
	 */
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid PTE for %lx\n", va);
	}
#endif
	/*
	 * If wiring actually changed (always?) set the wire bit and
	 * update the wire count.  Note that wiring is not a hardware
	 * characteristic so there is no need to invalidate the TLB.
	 */
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, 0);
		pmap->pm_stats.wired_count--;
	}
}

/*
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
 */
boolean_t
pmap_extract(pmap, va, pap)
	pmap_t	pmap;
	vaddr_t va;
	paddr_t *pap;
{
	boolean_t rv = FALSE;
	paddr_t pa;
	u_int pte;

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_extract(%p, %lx) -> ", pmap, va));

	if (pmap_ste_v(pmap, va)) {
		pte = *(u_int *)pmap_pte(pmap, va);
		if (pte) {
			pa = (pte & PG_FRAME) | (va & ~PG_FRAME);
			if (pap != NULL)
				*pap = pa;
			rv = TRUE;
		}
	}
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		if (rv)
			printf("%lx\n", pa);
		else
			printf("failed\n");
	}
#endif
	return (rv);
}

/*
 * pmap_copy:		[ INTERFACE ]
 *
 *	Copy the mapping range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t		dst_pmap;
	pmap_t		src_pmap;
	vaddr_t		dst_addr;
	vsize_t		len;
	vaddr_t		src_addr;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
	    dst_pmap, src_pmap, dst_addr, len, src_addr));
}

/*
 * pmap_collect:		[ INTERFACE ]
 *
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap)
	pmap_t		pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_collect(%p)\n", pmap));

	if (pmap == pmap_kernel()) {
		int bank, s;

		/*
		 * XXX This is very bogus.  We should handle kernel PT
		 * XXX pages much differently.
		 */

		s = splvm();
		for (bank = 0; bank < vm_nphysseg; bank++)
			pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			    ptoa(vm_physmem[bank].end));
		splx(s);
	} else {
		/*
		 * This process is about to be swapped out; free all of
		 * the PT pages by removing the physical mappings for its
		 * entire address space.  Note: pmap_remove() performs
		 * all necessary locking.
		 */
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
		pmap_update(pmap);
	}

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
#endif
}

/*
 * pmap_collect1:
 *
 *	Garbage-collect KPT pages.  Helper for the above (bogus)
 *	pmap_collect().
 *
 *	Note: THIS SHOULD GO AWAY, AND BE REPLACED WITH A BETTER
 *	WAY OF HANDLING PT PAGES!
 */
void
pmap_collect1(pmap, startpa, endpa)
	pmap_t		pmap;
	paddr_t		startpa, endpa;
{
	paddr_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	paddr_t kpa;
#ifdef DEBUG
	st_entry_t *ste;
	int opmapdebug = 0 /* XXX initialize to quiet gcc -Wall */;
#endif

	for (pa = startpa; pa < endpa; pa += NBPG) {
		struct kpt_page *kpt, **pkpt;

		/*
		 * Locate physical pages which are being used as kernel
		 * page table pages.
		 */
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptste && pv->pv_ptpmap == pmap_kernel())
				break;
		} while ((pv = pv->pv_next));
		if (pv == NULL)
			continue;
#ifdef DEBUG
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + HP_MAX_PTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (pt_entry_t *)(pv->pv_va + NBPG);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf("collect: freeing KPT page at %lx (ste %x@@%p)\n",
			       pv->pv_va, *pv->pv_ptste, pv->pv_ptste);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
		}

		ste = pv->pv_ptste;
#endif
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove_entry to take care of invalidating
		 * ST and Sysptmap entries.
		 */
		pmap_extract(pmap, pv->pv_va, &kpa);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				    PRM_TFLUSH|PRM_CFLUSH);
		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != NULL;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef DEBUG
		if (kpt == NULL)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			       kpt->kpt_va, kpa);
#endif
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste != SG_NV)
			printf("collect: kernel STE at %p still valid (%x)\n",
			       ste, *ste);
		ste = &Sysptmap[ste - pmap_ste(pmap_kernel(), 0)];
		if (*ste != SG_NV)
			printf("collect: kernel PTmap at %p still valid (%x)\n",
			       ste, *ste);
#endif
	}
}

/*
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and using bzero to clear its contents, one
 *	machine dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 */
void
pmap_zero_page(phys)
	paddr_t phys;
{
	int npte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%lx)\n", phys));

	npte = phys | PG_V;
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Cache-inhibit the mapping on VAC machines, as we would
		 * be wasting the cache load.
		 */
		npte |= PG_CI;
	}
#endif

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		/*
		 * Set copyback caching on the page; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte |= PG_CCB;
	}
#endif

	*caddr1_pte = npte;
	TBIS((vaddr_t)CADDR1);

	zeropage(CADDR1);

#ifdef DEBUG
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);
#endif
}

/*
 * pmap_copy_page:		[ INTERFACE ]
 *
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using bcopy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 */
void
pmap_copy_page(src, dst)
	paddr_t src, dst;
{
	int npte1, npte2;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_copy_page(%lx, %lx)\n", src, dst));

	npte1 = src | PG_RO | PG_V;
	npte2 = dst | PG_V;
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Cache-inhibit the mapping on VAC machines, as we would
		 * be wasting the cache load.
		 */
		npte1 |= PG_CI;
		npte2 |= PG_CI;
	}
#endif

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		/*
		 * Set copyback caching on the pages; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte1 |= PG_CCB;
		npte2 |= PG_CCB;
	}
#endif

	*caddr1_pte = npte1;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = npte2;
	TBIS((vaddr_t)CADDR2);

	copypage(CADDR1, CADDR2);

#ifdef DEBUG
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = PG_NV;
	TBIS((vaddr_t)CADDR2);
#endif
}

/*
 * pmap_clear_modify:		[ INTERFACE ]
 *
 *	Clear the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));

	rv = pmap_testbit(pa, PG_M);
	pmap_changebit(pa, 0, ~PG_M);
	return rv;
}

/*
 * pmap_clear_reference:	[ INTERFACE ]
 *
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));

	rv = pmap_testbit(pa, PG_U);
	pmap_changebit(pa, 0, ~PG_U);
	return rv;
}

/*
 * pmap_is_referenced:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
 */
boolean_t
pmap_is_referenced(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pa, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pa, "FT"[rv]);
		return(rv);
	}
#endif
	return(pmap_testbit(pa, PG_U));
}

/*
 * pmap_is_modified:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pa, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pa, "FT"[rv]);
		return(rv);
	}
#endif
	return(pmap_testbit(pa, PG_M));
}

/*
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
pmap_phys_address(ppn)
	int ppn;
{
	return(m68k_ptob(ppn));
}

#ifdef M68K_MMU_HP
/*
 * pmap_prefer:			[ INTERFACE ]
 *
 *	Find the first virtual address >= *vap that does not
 *	cause a virtually-tagged cache alias problem.
 */
void
pmap_prefer(foff, vap)
	vaddr_t foff, *vap;
{
	vaddr_t va;
	vsize_t d;

#ifdef M68K_MMU_MOTOROLA
	if (pmap_aliasmask)
#endif
	{
		va = *vap;
		d = foff - va;
		d &= pmap_aliasmask;
		*vap = va + d;
	}
}
#endif /* M68K_MMU_HP */

#ifdef COMPAT_HPUX
/*
 * pmap_mapmulti:
 *
 *	'PUX hack for dealing with the so called multi-mapped address space.
 *	The first 256mb is mapped in at every 256mb region from 0x10000000
 *	up to 0xF0000000.  This allows for 15 bits of tag information.
 *
 *	We implement this at the segment table level, the machine independent
 *	VM knows nothing about it.
 */
int
pmap_mapmulti(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	st_entry_t *ste, *bste;

#ifdef DEBUG
	if (pmapdebug & PDB_MULTIMAP) {
		ste = pmap_ste(pmap, HPMMBASEADDR(va));
		printf("pmap_mapmulti(%p, %lx): bste %p(%x)",
		       pmap, va, ste, *ste);
		ste = pmap_ste(pmap, va);
		printf(" ste %p(%x)\n", ste, *ste);
	}
#endif
	bste = pmap_ste(pmap, HPMMBASEADDR(va));
	ste = pmap_ste(pmap, va);
	if (*ste == SG_NV && (*bste & SG_V)) {
		*ste = *bste;
		TBIAU();
		return (0);
	}
	return (EFAULT);
}
#endif /* COMPAT_HPUX */

/*
 * Miscellaneous support routines follow
 */

/*
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
 *
 *	If (flags & PRM_KEEPPTPAGE), we don't free the page table page
 *	if the reference drops to zero.
 */
/* static */
void
pmap_remove_mapping(pmap, va, pte, flags)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
	int flags;
{
	paddr_t pa;
	struct pv_entry *pv, *npv;
	pmap_t ptpmap;
	st_entry_t *ste;
	int s, bits;
#ifdef DEBUG
	pt_entry_t opte;
#endif

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    pmap, va, pte, flags));

	/*
	 * PTE not provided, compute it from pmap and va.
	 */

	if (pte == PT_ENTRY_NULL) {
		pte = pmap_pte(pmap, va);
		if (*pte == PG_NV)
			return;
	}
#ifdef M68K_MMU_HP
	if (pmap_aliasmask && (flags & PRM_CFLUSH)) {

		/*
		 * Purge kernel side of VAC to ensure we get the correct
		 * state of any hardware maintained bits.
		 */

		DCIS();

		/*
		 * If this is a non-CI user mapping for the current process,
		 * flush the VAC.  Note that the kernel side was flushed
		 * above so we don't worry about non-CI kernel mappings.
		 */

		if (active_user_pmap(pmap) && !pmap_pte_ci(pte)) {
			DCIU();
		}
	}
#endif
	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	opte = *pte;
#endif

	/*
	 * Update statistics
	 */

	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */

	PMAP_DPRINTF(PDB_REMOVE, ("remove: invalidating pte at %p\n", pte));
	bits = *pte & (PG_U|PG_M);
	*pte = PG_NV;
	if ((flags & PRM_TFLUSH) && active_pmap(pmap))
		TBIS(va);

	/*
	 * For user mappings decrement the wiring count on
	 * the PT page.
	 */

	if (pmap != pmap_kernel()) {
		vaddr_t ptpva = trunc_page((vaddr_t)pte);
		int refs = pmap_ptpage_delref(ptpva);
#ifdef DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", ptpva);
#endif

		/*
		 * If reference count drops to 1, and we're not instructed
		 * to keep it around, free the PT page.
		 */

		if (refs == 1 && (flags & PRM_KEEPPTPAGE) == 0) {
#ifdef DIAGNOSTIC
			struct pv_entry *pv;
#endif
			paddr_t pa;

			pa = pmap_pte_pa(pmap_pte(pmap_kernel(), ptpva));
#ifdef DIAGNOSTIC
			if (PAGE_IS_MANAGED(pa) == 0)
				panic("pmap_remove_mapping: unmanaged PT page");
			pv = pa_to_pvh(pa);
			if (pv->pv_ptste == NULL)
				panic("pmap_remove_mapping: ptste == NULL");
			if (pv->pv_pmap != pmap_kernel() ||
			    pv->pv_va != ptpva ||
			    pv->pv_next != NULL)
				panic("pmap_remove_mapping: "
				    "bad PT page pmap %p, va 0x%lx, next %p",
				    pv->pv_pmap, pv->pv_va, pv->pv_next);
#endif
			pmap_remove_mapping(pmap_kernel(), ptpva,
			    NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(PHYS_TO_VM_PAGE(pa));
			PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
			    ("remove: PT page 0x%lx (0x%lx) freed\n",
			    ptpva, pa));
		}
	}

	/*
	 * If this isn't a managed page, we are all done.
	 */

	if (PAGE_IS_MANAGED(pa) == 0)
		return;

	/*
	 * Otherwise remove it from the PV table
	 * (raise IPL since we may be called at interrupt time).
	 */

	pv = pa_to_pvh(pa);
	ste = ST_ENTRY_NULL;
	s = splvm();

	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		ste = pv->pv_ptste;
		ptpmap = pv->pv_ptpmap;
		npv = pv->pv_next;
		if (npv) {
			npv->pv_flags = pv->pv_flags;
			*pv = *npv;
			pmap_free_pv(npv);
		} else
			pv->pv_pmap = NULL;
	} else {
		for (npv = pv->pv_next; npv; npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
			pv = npv;
		}
#ifdef DEBUG
		if (npv == NULL)
			panic("pmap_remove: PA not in pv_tab");
#endif
		ste = npv->pv_ptste;
		ptpmap = npv->pv_ptpmap;
		pv->pv_next = npv->pv_next;
		pmap_free_pv(npv);
		pv = pa_to_pvh(pa);
	}
#ifdef M68K_MMU_HP

	/*
	 * If only one mapping left we no longer need to cache inhibit
	 */

	if (pmap_aliasmask &&
	    pv->pv_pmap && pv->pv_next == NULL && (pv->pv_flags & PV_CI)) {
		PMAP_DPRINTF(PDB_CACHE,
		    ("remove: clearing CI for pa %lx\n", pa));
		pv->pv_flags &= ~PV_CI;
		pmap_changebit(pa, 0, ~PG_CI);
#ifdef DEBUG
		if ((pmapdebug & (PDB_CACHE|PDB_PVDUMP)) ==
		    (PDB_CACHE|PDB_PVDUMP))
			pmap_pvdump(pa);
#endif
	}
#endif

	/*
	 * If this was a PT page we must also remove the
	 * mapping from the associated segment table.
	 */

	if (ste) {
		PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
		    ("remove: ste was %x@@%p pte was %x@@%p\n",
		    *ste, ste, opte, pmap_pte(pmap, va)));
#if defined(M68040)
		if (mmutype == MMU_68040) {
			st_entry_t *este = &ste[NPTEPG/SG4_LEV3SIZE];

			while (ste < este)
				*ste++ = SG_NV;
#ifdef DEBUG
			ste -= NPTEPG/SG4_LEV3SIZE;
#endif
		} else
#endif
		*ste = SG_NV;

		/*
		 * If it was a user PT page, we decrement the
		 * reference count on the segment table as well,
		 * freeing it if it is now empty.
		 */

		if (ptpmap != pmap_kernel()) {
			PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
			    ("remove: stab %p, refcnt %d\n",
			    ptpmap->pm_stab, ptpmap->pm_sref - 1));
#ifdef DEBUG
			if ((pmapdebug & PDB_PARANOIA) &&
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
				panic("remove: bogus ste");
#endif
			if (--(ptpmap->pm_sref) == 0) {
				PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
				    ("remove: free stab %p\n",
				    ptpmap->pm_stab));
				pmap_remove(pmap_kernel(),
				    (vaddr_t)ptpmap->pm_stab,
				    (vaddr_t)ptpmap->pm_stab + HP_STSIZE);
				uvm_pagefree(PHYS_TO_VM_PAGE((paddr_t)
				    ptpmap->pm_stpa));
				uvm_km_free_wakeup(st_map,
						(vaddr_t)ptpmap->pm_stab,
						HP_STSIZE);
				ptpmap->pm_stab = Segtabzero;
				ptpmap->pm_stpa = Segtabzeropa;
#if defined(M68040)
				if (mmutype == MMU_68040)
					ptpmap->pm_stfree = protostfree;
#endif

				/*
				 * XXX may have changed segment table
				 * pointer for current process so
				 * update now to reload hardware.
				 */

				if (active_user_pmap(ptpmap))
					PMAP_ACTIVATE(ptpmap, 1);
			}
#ifdef DEBUG
			else if (ptpmap->pm_sref < 0)
				panic("remove: sref < 0");
#endif
		}
#if 0
		/*
		 * XXX this should be unnecessary as we have been
		 * flushing individual mappings as we go.
		 */
		if (ptpmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
#endif
		pv->pv_flags &= ~PV_PTPAGE;
		ptpmap->pm_ptpages--;
	}

	/*
	 * Update saved attributes for managed page
	 */

	*pa_to_attribute(pa) |= bits;
	splx(s);
}

/*
 * pmap_testbit:
 *
 *	Test the modified/referenced bits of a physical page.
 */
/* static */
boolean_t
pmap_testbit(pa, bit)
	paddr_t pa;
	int bit;
{
	struct pv_entry *pv;
	pt_entry_t *pte;
	int s;

	pv = pa_to_pvh(pa);
	s = splvm();

	/*
	 * Check saved info first
	 */

	if (*pa_to_attribute(pa) & bit) {
		splx(s);
		return(TRUE);
	}
#ifdef M68K_MMU_HP
	/*
	 * Flush VAC to get correct state of any hardware maintained bits.
	 */
	if (pmap_aliasmask && (bit & (PG_U|PG_M)))
		DCIS();
#endif
	/*
	 * Not found.  Check current mappings, returning immediately if
	 * found.  Cache a hit to speed future lookups.
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
			if (*pte & bit) {
				*pa_to_attribute(pa) |= bit;
				splx(s);
				return(TRUE);
			}
		}
	}
	splx(s);
	return(FALSE);
}

/*
 * pmap_changebit:
 *
 *	Change the modified/referenced bits, or other PTE bits,
 *	for a physical page.
 */
/* static */
void
pmap_changebit(pa, set, mask)
	paddr_t pa;
	int set, mask;
{
	struct pv_entry *pv;
	pt_entry_t *pte, npte;
	vaddr_t va;
	int s;
#if defined(M68K_MMU_HP) || defined(M68040)
	boolean_t firstpage = TRUE;
#endif

	PMAP_DPRINTF(PDB_BITS,
	    ("pmap_changebit(%lx, %x, %x)\n", pa, set, mask));

	pv = pa_to_pvh(pa);
	s = splvm();

	/*
	 * Clear saved attributes (modify, reference)
	 */

	*pa_to_attribute(pa) &= mask;

	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */

	if (pv->pv_pmap != NULL) {
#ifdef DEBUG
		int toflush = 0;
#endif
		for (; pv; pv = pv->pv_next) {
#ifdef DEBUG
			toflush |= (pv->pv_pmap == pmap_kernel()) ? 2 : 1;
#endif
			va = pv->pv_va;
			pte = pmap_pte(pv->pv_pmap, va);
#ifdef M68K_MMU_HP
			/*
			 * Flush VAC to ensure we get correct state of HW bits
			 * so we don't clobber them.
			 */
			if (firstpage && pmap_aliasmask) {
				firstpage = FALSE;
				DCIS();
			}
#endif
			npte = (*pte | set) & mask;
			if (*pte != npte) {
#if defined(M68040)
				/*
				 * If we are changing caching status or
				 * protection make sure the caches are
				 * flushed (but only once).
				 */
				if (firstpage && (mmutype == MMU_68040) &&
				    ((set == PG_RO) ||
				     (set & PG_CMASK) ||
				     (mask & PG_CMASK) == 0)) {
					firstpage = FALSE;
					DCFP(pa);
					ICPP(pa);
				}
#endif
				*pte = npte;
				if (active_pmap(pv->pv_pmap))
					TBIS(va);
			}
		}
	}
	splx(s);
}

/*
 * pmap_enter_ptpage:
 *
 *	Allocate and map a PT page for the specified pmap/va pair.
 */
/* static */
void
pmap_enter_ptpage(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	paddr_t ptpa;
	struct vm_page *pg;
	struct pv_entry *pv;
	st_entry_t *ste;
	int s;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE,
	    ("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va));

	/*
	 * Allocate a segment table if necessary.  Note that it is allocated
	 * from a private map and not pt_map.  This keeps user page tables
	 * aligned on segment boundaries in the kernel address space.
	 * The segment table is wired down.  It will be freed whenever the
	 * reference count drops to zero.
	 */
	if (pmap->pm_stab == Segtabzero) {
		pmap->pm_stab = (st_entry_t *)
			uvm_km_zalloc(st_map, HP_STSIZE);
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab, 
			(paddr_t *)&pmap->pm_stpa);
#if defined(M68040)
		if (mmutype == MMU_68040) {
#ifdef DEBUG
			if (dowriteback && dokwriteback)
#endif
			pmap_changebit((paddr_t)pmap->pm_stpa, 0, ~PG_CCB);
			pmap->pm_stfree = protostfree;
		}
#endif
		/*
		 * XXX may have changed segment table pointer for current
		 * process so update now to reload hardware.
		 */
		if (active_user_pmap(pmap))
			PMAP_ACTIVATE(pmap, 1);

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: pmap %p stab %p(%p)\n",
		    pmap, pmap->pm_stab, pmap->pm_stpa));
	}

	ste = pmap_ste(pmap, va);
#if defined(M68040)
	/*
	 * Allocate level 2 descriptor block if necessary
	 */
	if (mmutype == MMU_68040) {
		if (*ste == SG_NV) {
			int ix;
			caddr_t addr;

			ix = bmtol2(pmap->pm_stfree);
			if (ix == -1)
				panic("enter: out of address space"); /* XXX */
			pmap->pm_stfree &= ~l2tobm(ix);
			addr = (caddr_t)&pmap->pm_stab[ix*SG4_LEV2SIZE];
			bzero(addr, SG4_LEV2SIZE*sizeof(st_entry_t));
			addr = (caddr_t)&pmap->pm_stpa[ix*SG4_LEV2SIZE];
			*ste = (u_int)addr | SG_RW | SG_U | SG_V;

			PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
			    ("enter: alloc ste2 %d(%p)\n", ix, addr));
		}
		ste = pmap_ste2(pmap, va);
		/*
		 * Since a level 2 descriptor maps a block of SG4_LEV3SIZE
		 * level 3 descriptors, we need a chunk of NPTEPG/SG4_LEV3SIZE
		 * (16) such descriptors (NBPG/SG4_LEV3SIZE bytes) to map a
		 * PT page--the unit of allocation.  We set `ste' to point
		 * to the first entry of that chunk which is validated in its
		 * entirety below.
		 */
		ste = (st_entry_t *)((int)ste & ~(NBPG/SG4_LEV3SIZE-1));

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: ste2 %p (%p)\n", pmap_ste2(pmap, va), ste));
	}
#endif
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));

	/*
	 * In the kernel we allocate a page from the kernel PT page
	 * free list and map it into the kernel page table map (via
	 * pmap_enter).
	 */
	if (pmap == pmap_kernel()) {
		struct kpt_page *kpt;

		s = splvm();
		if ((kpt = kpt_free_list) == NULL) {
			/*
			 * No PT pages available.
			 * Try once to free up unused ones.
			 */
			PMAP_DPRINTF(PDB_COLLECT,
			    ("enter: no KPT pages, collecting...\n"));
			pmap_collect(pmap_kernel());
			if ((kpt = kpt_free_list) == (struct kpt_page *)0)
				panic("pmap_enter_ptpage: can't get KPT page");
		}
		kpt_free_list = kpt->kpt_next;
		kpt->kpt_next = kpt_used_list;
		kpt_used_list = kpt;
		ptpa = kpt->kpt_pa;
		bzero((caddr_t)kpt->kpt_va, NBPG);
		pmap_enter(pmap, va, ptpa, VM_PROT_READ | VM_PROT_WRITE,
		    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
		pmap_update(pmap);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE)) {
			int ix = pmap_ste(pmap, va) - pmap_ste(pmap, 0);

			printf("enter: add &Sysptmap[%d]: %x (KPT page %lx)\n",
			       ix, Sysptmap[ix], kpt->kpt_va);
		}
#endif
		splx(s);
	} else {

		/*
		 * For user processes we just allocate a page from the
		 * VM system.  Note that we set the page "wired" count to 1,
		 * which is what we use to check if the page can be freed.
		 * See pmap_remove_mapping().
		 *
		 * Count the segment table reference first so that we won't
		 * lose the segment table when low on memory.
		 */

		pmap->pm_sref++;
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
		    ("enter: about to alloc UPT pg at %lx\n", va));
		while ((pg = uvm_pagealloc(uvm.kernel_object, va, NULL,
		    UVM_PGA_ZERO)) == NULL) {
			uvm_wait("ptpage");
		}
		pg->wire_count = 1;
		pg->flags &= ~(PG_BUSY|PG_FAKE);
		UVM_PAGE_OWN(pg, NULL);
		ptpa = VM_PAGE_TO_PHYS(pg);
		pmap_enter(pmap_kernel(), va, ptpa,
		    VM_PROT_READ | VM_PROT_WRITE,
		    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
		pmap_update(pmap);
	}
#if defined(M68040)
	/*
	 * Turn off copyback caching of page table pages,
	 * could get ugly otherwise.
	 */
#ifdef DEBUG
	if (dowriteback && dokwriteback)
#endif
	if (mmutype == MMU_68040) {
#ifdef DEBUG
		pt_entry_t *pte = pmap_pte(pmap_kernel(), va);
		if ((pmapdebug & PDB_PARANOIA) && (*pte & PG_CCB) == 0)
			printf("%s PT no CCB: kva=%lx ptpa=%lx pte@@%p=%x\n",
			       pmap == pmap_kernel() ? "Kernel" : "User",
			       va, ptpa, pte, *pte);
#endif
		pmap_changebit(ptpa, 0, ~PG_CCB);
	}
#endif
	/*
	 * Locate the PV entry in the kernel for this PT page and
	 * record the STE address.  This is so that we can invalidate
	 * the STE when we remove the mapping for the page.
	 */
	pv = pa_to_pvh(ptpa);
	s = splvm();
	if (pv) {
		pv->pv_flags |= PV_PTPAGE;
		do {
			if (pv->pv_pmap == pmap_kernel() && pv->pv_va == va)
				break;
		} while ((pv = pv->pv_next));
	}
#ifdef DEBUG
	if (pv == NULL)
		panic("pmap_enter_ptpage: PT page not entered");
#endif
	pv->pv_ptste = ste;
	pv->pv_ptpmap = pmap;

	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
	    ("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste));

	/*
	 * Map the new PT page into the segment table.
	 * Also increment the reference count on the segment table if this
	 * was a user page table page.  Note that we don't use vm_map_pageable
	 * to keep the count like we do for PT pages, this is mostly because
	 * it would be difficult to identify ST pages in pmap_pageable to
	 * release them.  We also avoid the overhead of vm_map_pageable.
	 */
#if defined(M68040)
	if (mmutype == MMU_68040) {
		st_entry_t *este;

		for (este = &ste[NPTEPG/SG4_LEV3SIZE]; ste < este; ste++) {
			*ste = ptpa | SG_U | SG_RW | SG_V;
			ptpa += SG4_LEV3SIZE * sizeof(st_entry_t);
		}
	} else
#endif
	*ste = (ptpa & SG_FRAME) | SG_RW | SG_V;
	if (pmap != pmap_kernel()) {
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: stab %p refcnt %d\n",
		    pmap->pm_stab, pmap->pm_sref));
	}
#if 0
	/*
	 * Flush stale TLB info.
	 */
	if (pmap == pmap_kernel())
		TBIAS();
	else
		TBIAU();
#endif
	pmap->pm_ptpages++;
	splx(s);
}

/*
 * pmap_ptpage_addref:
 *
 *	Add a reference to the specified PT page.
 */
void
pmap_ptpage_addref(ptpva)
	vaddr_t ptpva;
{
	struct vm_page *pg;

	simple_lock(&uvm.kernel_object->vmobjlock);
	pg = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	pg->wire_count++;
	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
	    ("ptpage addref: pg %p now %d\n", pg, pg->wire_count));
	simple_unlock(&uvm.kernel_object->vmobjlock);
}

/*
 * pmap_ptpage_delref:
 *
 *	Delete a reference to the specified PT page.
 */
int
pmap_ptpage_delref(ptpva)
	vaddr_t ptpva;
{
	struct vm_page *pg;
	int rv;

	simple_lock(&uvm.kernel_object->vmobjlock);
	pg = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	rv = --pg->wire_count;
	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
	    ("ptpage delref: pg %p now %d\n", pg, pg->wire_count));
	simple_unlock(&uvm.kernel_object->vmobjlock);
	return (rv);
}

#ifdef DEBUG
/*
 * pmap_pvdump:
 *
 *	Dump the contents of the PV list for the specified physical page.
 */
/* static */
void
pmap_pvdump(pa)
	paddr_t pa;
{
	struct pv_entry *pv;

	printf("pa %lx", pa);
	for (pv = pa_to_pvh(pa); pv; pv = pv->pv_next)
		printf(" -> pmap %p, va %lx, ptste %p, ptpmap %p, flags %x",
		       pv->pv_pmap, pv->pv_va, pv->pv_ptste, pv->pv_ptpmap,
		       pv->pv_flags);
	printf("\n");
}

/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
/* static */
void
pmap_check_wiring(str, va)
	char *str;
	vaddr_t va;
{
	pt_entry_t *pte;
	paddr_t pa;
	struct vm_page *pg;
	int count;

	if (!pmap_ste_v(pmap_kernel(), va) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;

	pa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg->wire_count < 1) {
		printf("*%s*: 0x%lx: wire count %d\n", str, va, pg->wire_count);
		return;
	}

	count = 0;
	for (pte = (pt_entry_t *)va; pte < (pt_entry_t *)(va + NBPG); pte++)
		if (*pte)
			count++;
	if ((pg->wire_count - 1) != count)
		printf("*%s*: 0x%lx: w%d/a%d\n",
		       str, va, (pg->wire_count - 1), count);
}
#endif /* DEBUG */
@


1.38
log
@Temporarily revert the pmap_motorola changes, as they may account for
some problems as well.
Requested by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2001/11/28 16:24:26 art Exp $	*/
@


1.37
log
@Not needed anymore.
@
text
@d692 1
a692 1
	pmap_update();
d1732 1
a1732 1
		pmap_update();
d2671 1
a2671 1
		pmap_update();
d2707 1
a2707 1
		pmap_update();
@


1.36
log
@more typedef zapping vm_page_t -> struct vm_page *
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2001/11/28 16:13:28 art Exp $	*/
@


1.35
log
@zap some typedefs.
vm_map_t -> struct vm_map *
vm_map_entry_t -> struct vm_map_entry *
simple_lock_data_t -> struct simplelock

(uvm not done yet, coming in the next commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2001/11/28 14:13:06 art Exp $	*/
d2564 1
a2564 1
	vm_page_t pg;
d2798 1
a2798 1
	vm_page_t pg;
d2817 1
a2817 1
	vm_page_t pg;
d2865 1
a2865 1
	vm_page_t pg;
@


1.34
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2001/11/28 13:47:38 art Exp $	*/
d249 1
a249 1
vm_map_t	st_map, pt_map;
@


1.33
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2001/11/27 22:05:29 miod Exp $	*/
a1456 13
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE)
		pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pgs[i]),
		    VM_PROT_READ|VM_PROT_WRITE);
@


1.32
log
@Various pmap_k* optimizations, as well as uvm interface updates,
from NetBSD.
Soon to be found in other m68k pmap, this one is just a teaser to please art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2001/11/07 01:18:00 art Exp $	*/
d386 1
a386 1
				UVM_FLAG_FIXED)) != KERN_SUCCESS)
d393 1
a393 1
				UVM_FLAG_FIXED)) != KERN_SUCCESS) {
d467 1
a467 1
	if (rv != KERN_SUCCESS || (addr + s) >= (vaddr_t)Sysmap)
d469 1
a469 3
	rv = uvm_unmap(kernel_map, addr, addr + s);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: uvm_unmap failed");
d1400 1
a1400 1
	return (KERN_SUCCESS);
d2125 1
a2125 1
		return (KERN_SUCCESS);
d2127 1
a2127 1
	return (KERN_INVALID_ADDRESS);
@


1.31
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2001/11/06 01:53:42 art Exp $	*/
d84 1
a84 1
 *	68020 with 68551 MMU	models 318, 319, 330
d94 1
a94 1
 *	technically not true for the 68551 but we flush the TLB on every
d483 1
a483 1
	kpt_free_list = (struct kpt_page *) 0;
d509 1
a509 1
		 * this value via sysctl(2).
d638 1
a638 1
		s = splimp();
d694 1
a761 3
	if (pmap == NULL)
		return;

a762 1

d793 5
a797 1
	if (pmap->pm_ptab)
d800 2
a801 3
	if (pmap->pm_stab != Segtabzero)
		uvm_km_free_wakeup(st_map, (vaddr_t)pmap->pm_stab,
				   HP_STSIZE);
a813 3
	if (pmap == NULL)
		return;

a814 1

a879 3
	if (pmap == NULL)
		return;

d887 1
a887 8
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}
d891 1
d894 10
a977 2
	if (PAGE_IS_MANAGED(pa) == 0)
		return;
d993 1
a993 1
	s = splimp();
a1040 3
	if (pmap == NULL)
		return;

a1044 2
	if (prot & VM_PROT_WRITE)
		return;
d1221 1
a1221 1
		s = splimp();
d1411 48
a1458 1
	pmap_enter(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d1469 3
a1471 4
	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
				VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED);
	}
d1479 116
a1594 2
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
d1596 1
a1614 3
	if (pmap == NULL)
		return;

d1659 1
d1661 1
d1666 18
a1683 12
	if (pmap && pmap_ste_v(pmap, va))
		pa = *pmap_pte(pmap, va);
	else
		return (FALSE);
	if (pa)
		pa = (pa & PG_FRAME) | (va & ~PG_FRAME);

	PMAP_DPRINTF(PDB_FOLLOW, ("%lx\n", pa));

	if (pap)
		*pap = pa;
	return (TRUE);
a1709 16
 * pmap_update:
 *
 *	Require that all active physical maps contain no
 *	incorrect entries NOW, by processing any deferred
 *	map operations.
 */
void
pmap_update()
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_update()\n"));

	TBIA();		/* XXX should not be here. */
}

/*
d1734 1
a1734 1
		s = splimp();
d1747 1
d1835 1
a1835 1
		     kpt != (struct kpt_page *)0;
d1840 1
a1840 1
		if (kpt == (struct kpt_page *)0)
a1871 2
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
d1877 1
a1877 1
	int s, npte;
a1902 2
	s = splimp();

a1911 2

	splx(s);
a1921 2
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
d1927 1
a1927 1
	int s, npte1, npte2;
a1955 2
	s = splimp();

a1970 2

	splx(s);
d2176 1
d2184 1
d2189 1
d2191 1
d2197 1
d2207 1
d2211 1
d2219 1
d2225 1
d2230 1
d2238 1
a2241 5
		 *
		 * Note: refcnt == 1 comes from the fact that we allocate
		 * the page with uvm_fault_wire(), which initially wires
		 * the page.  The first reference we actually add causes
		 * the refcnt to be 2.
d2243 1
d2245 1
d2247 1
a2253 1
#endif
a2254 1
#ifdef DIAGNOSTIC
d2264 1
a2264 1
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
d2272 1
d2276 1
d2279 1
d2284 1
d2287 2
a2288 1
	s = splimp();
d2322 1
d2326 1
d2340 1
d2345 1
d2362 1
d2368 1
d2382 5
d2396 1
d2402 1
d2424 1
d2428 1
d2448 2
a2449 2
	if (PAGE_IS_MANAGED(pa) == 0)
		return(FALSE);
a2450 2
	pv = pa_to_pvh(pa);
	s = splimp();
d2454 1
a2506 3
	if (PAGE_IS_MANAGED(pa) == 0)
		return;

d2508 1
a2508 1
	s = splimp();
d2513 1
d2520 1
a2529 9

			/*
			 * XXX don't write protect pager mappings
			 */
			if (set == PG_RO) {
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
					continue;
			}

d2579 1
d2629 1
a2629 1
			
d2667 2
a2668 2
		s = splimp();
		if ((kpt = kpt_free_list) == (struct kpt_page *)0) {
d2684 3
a2686 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT,
		    VM_PROT_DEFAULT|PMAP_WIRED);
d2696 2
a2697 11
	}
	/*
	 * For user processes we just simulate a fault on that location
	 * letting the VM system allocate a zero-filled page.
	 *
	 * Note we use a wire-fault to keep the page off the paging
	 * queues.  This sets our PT page's reference (wire) count to
	 * 1, which is what we use to check if the page can be freed.
	 * See pmap_remove_mapping().
	 */
	else {
d2699 6
a2704 1
		 * Count the segment table reference now so that we won't
d2707 1
d2710 13
a2722 9
		    ("enter: about to fault UPT pg at %lx\n", va));
		s = uvm_fault_wire(pt_map, va, va + PAGE_SIZE,
		    VM_PROT_READ|VM_PROT_WRITE);
		if (s != KERN_SUCCESS) {
			printf("uvm_fault_wire(pt_map, 0x%lx, 0x%lx, RW) "
			    "-> %d\n", va, va + PAGE_SIZE, s);
			panic("pmap_enter: uvm_fault_wire failed");
		}
		ptpa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
d2749 1
a2749 1
	s = splimp();
d2813 1
a2813 1
	vm_page_t m;
d2816 4
a2819 2
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	m->wire_count++;
d2832 1
a2832 1
	vm_page_t m;
d2836 4
a2839 2
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	rv = --m->wire_count;
d2880 1
a2880 1
	vm_page_t m;
d2888 3
a2890 3
	m = PHYS_TO_VM_PAGE(pa);
	if (m->wire_count < 1) {
		printf("*%s*: 0x%lx: wire count %d\n", str, va, m->wire_count);
d2898 1
a2898 1
	if ((m->wire_count - 1) != count)
d2900 1
a2900 1
		       str, va, (m->wire_count - 1), count);
@


1.30
log
@Redundant includes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/11/04 02:58:54 miod Exp $	*/
d383 1
a383 1
		    NULL, UVM_UNKNOWN_OFFSET,
d390 1
a390 1
		    NULL, UVM_UNKNOWN_OFFSET,
d464 1
a464 1
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET,
@


1.29
log
@hp300spu.h information not needed anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2001/09/19 20:50:56 mickey Exp $	*/
a142 3
#include <vm/vm.h>
#include <vm/vm_page.h>

a143 1
#include <uvm/uvm_extern.h>
@


1.28
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/07/25 13:25:31 art Exp $	*/
a132 2

#include <machine/hp300spu.h>	/* XXX param.h includes cpu.h */
@


1.27
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2001/07/18 10:47:04 art Exp $	*/
a145 1
#include <vm/vm_kern.h>
d149 1
@


1.26
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2001/06/27 04:05:45 art Exp $	*/
d696 1
a696 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE, 0);
d1133 2
a1134 2
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
d1139 1
a1139 2
	boolean_t wired;
	vm_prot_t access_type;
d1146 1
a1151 3
	if (pmap == NULL)
		return;

d1320 2
a1321 2
		if (access_type & ~prot)
			panic("pmap_enter: access_type exceeds prot");
d1323 1
a1323 1
		if (access_type & VM_PROT_WRITE)
d1325 1
a1325 1
		else if (access_type & VM_PROT_ALL)
d1417 2
d1427 1
a1427 1
	pmap_enter(pmap_kernel(), va, pa, prot, TRUE, 0);
d1440 1
a1440 1
				VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
d2544 2
a2545 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE,
		    VM_PROT_DEFAULT);
@


1.25
log
@no more old VM
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2001/06/08 13:32:54 millert Exp $	*/
a709 1
#ifdef PMAP_NEW
a722 23
#else
pmap_t
pmap_create(size)
	vsize_t	size;
{
	pmap_t pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create(%lx)\n", size));

	/*
	 * Software use map does not need a pmap
	 */
	if (size)
		return (NULL);

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);

	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return (pmap);
}
#endif
a973 1
#ifdef PMAP_NEW
a978 6
#else
pmap_page_protect(pa, prot)
	paddr_t		pa;
	vm_prot_t	prot;
{
#endif
a1421 1
#ifdef PMAP_NEW
a1453 1
#endif
a1855 1
#ifdef PMAP_NEW
a1868 11
#else
void
pmap_clear_modify(pa)
	paddr_t	pa;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_M);
}
#endif
a1874 1
#ifdef PMAP_NEW
a1887 11
#else
void
pmap_clear_reference(pa)
	paddr_t	pa;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_U);
}
#endif
a1895 1
#ifdef PMAP_NEW
a1899 5
#else
pmap_is_referenced(pa)
	paddr_t	pa;
{
#endif
a1916 1
#ifdef PMAP_NEW
a1920 5
#else
pmap_is_modified(pa)
	paddr_t	pa;
{
#endif
@


1.24
log
@Fix the XXX #debug bits in vm_machdep().
Makes the pmap_extract() accept a NULL argument for pap; from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2001/06/08 08:08:43 art Exp $	*/
a148 1
#if defined(UVM)
a149 1
#endif
a255 1
#if defined(UVM)
a256 1
#endif
a385 1
#if defined(UVM)
a407 19
#else
	addr = (vaddr_t) intiobase;
	(void) vm_map_find(kernel_map, NULL, (vaddr_t) 0,
			   &addr, m68k_ptob(IIOMAPSIZE+EIOMAPSIZE), FALSE);
	if (addr != (vaddr_t)intiobase)
		goto bogons;
	addr = (vaddr_t) Sysmap;
	vm_object_reference(kernel_object);
	(void) vm_map_find(kernel_map, kernel_object, addr,
			   &addr, HP_MAX_PTSIZE, FALSE);
	/*
	 * If this fails it is probably because the static portion of
	 * the kernel page table isn't big enough and we overran the
	 * page table map.
	 */
	if (addr != (vaddr_t)Sysmap)
bogons:
		panic("pmap_init: bogons in the VM system!");
#endif /* UVM */
a425 1
#if defined(UVM)
a428 3
#else
	addr = kmem_alloc(kernel_map, s);
#endif
a468 1
#if defined(UVM)
a477 7
#else
	addr = 0;
	rv = vm_map_find(kernel_map, NULL, 0, &addr, s, TRUE);
	if (rv != KERN_SUCCESS || addr + s >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	vm_map_remove(kernel_map, addr, addr + s);
#endif
a482 1
#if defined(UVM)
a485 3
#else
	addr = kmem_alloc(kernel_map, s);
#endif
a500 1
#if defined(UVM)
a521 38
#else
	/*
	 * Allocate the segment table map
	 */
	s = maxproc * HP_STSIZE;
	st_map = kmem_suballoc(kernel_map, &addr, &addr2, s, TRUE);

	/*
	 * Slightly modified version of kmem_suballoc() to get page table
	 * map where we want it.
	 */
	addr = HP_PTBASE;
	if ((HP_PTMAXSIZE / HP_MAX_PTSIZE) < maxproc) {
		s = HP_PTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(2).
		 */
		maxproc = (HP_PTMAXSIZE / HP_MAX_PTSIZE);
	} else
		s = (maxproc * HP_MAX_PTSIZE);
	addr2 = addr + s;
	rv = vm_map_find(kernel_map, NULL, 0, &addr, s, TRUE);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: cannot allocate space for PT map");
	pmap_reference(vm_map_pmap(kernel_map));
	pt_map = vm_map_create(vm_map_pmap(kernel_map), addr, addr2, TRUE);
	if (pt_map == NULL)
		panic("pmap_init: cannot create pt_map");
	rv = vm_map_submap(kernel_map, addr, addr2, pt_map);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: cannot map range to pt_map");

	PMAP_DPRINTF(PDB_INIT,
	/* ( */ ("pmap_init: pt_map [%lx - %lx)\n", addr, addr2));
#endif /* UVM */
a555 1
#if defined(UVM)
a558 5
#else
		pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: kmem_alloc() failed");
#endif
a604 1
#if defined(UVM)
a605 3
#else
		kmem_free(kernel_map, (vaddr_t)pvp, NBPG);
#endif
a669 1
#if defined(UVM)
a670 3
#else
		kmem_free(kernel_map, (vaddr_t)pvp, NBPG);
#endif
a826 1
#if defined(UVM)
a828 4
#else
		kmem_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
				 HP_MAX_PTSIZE);
#endif
a829 1
#if defined(UVM)
a831 4
#else
		kmem_free_wakeup(st_map, (vaddr_t)pmap->pm_stab,
				 HP_STSIZE);
#endif
a1198 1
#if defined(UVM)
a1200 4
#else
		pmap->pm_ptab = (pt_entry_t *)
			kmem_alloc_wait(pt_map, HP_MAX_PTSIZE);
#endif
a1257 1
#ifdef UVM
a1258 4
#else
		(void) vm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)pte + 1), FALSE);
#endif
a2167 1
#ifdef UVM
a2211 12
#else
	if (pmap != pmap_kernel()) {
		vaddr_t ptpva = trunc_page((vaddr_t)pte);

		(void) vm_map_pageable(pt_map, ptpva,
		    round_page((vaddr_t)pte + 1), TRUE);
#ifdef DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", ptpva);
#endif
	}
#endif
a2310 1
#if defined(UVM)
a2313 5
#else
				kmem_free_wakeup(st_map,
						 (vaddr_t)ptpmap->pm_stab,
						 HP_STSIZE);
#endif
a2456 1
#if defined(UVM)
a2458 6
#else
				extern vaddr_t pager_sva, pager_eva;

				if (va >= pager_sva && va < pager_eva)
					continue;
#endif
a2524 1
#if defined(UVM)
a2526 4
#else
		pmap->pm_stab = (st_entry_t *)
			kmem_alloc(st_map, HP_STSIZE);
#endif
a2642 1
#if defined(UVM)
a2649 7
#else
		s = vm_fault(pt_map, va, VM_PROT_READ|VM_PROT_WRITE, FALSE);
		if (s != KERN_SUCCESS) {
			printf("vm_fault(pt_map, %lx, RW, 0) -> %d\n", va, s);
			panic("pmap_enter: vm_fault failed");
		}
#endif
a2650 5
#if !defined(UVM)
#ifdef DEBUG
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_PTPAGE;
#endif
#endif
a2731 1
#ifdef UVM
a2766 1
#endif
@


1.23
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2001/06/05 16:13:15 millert Exp $	*/
d1678 2
a1679 1
	*pap = pa;
@


1.22
log
@Use mi round_page() and trunc_page() macros
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2001/05/16 17:40:02 millert Exp $	*/
d459 1
a459 1
	Segtabzeropa = (st_entry_t *) pmap_extract(pmap_kernel(), addr);
d535 1
a535 1
		kpt_pages->kpt_pa = pmap_extract(pmap_kernel(), addr2);
d1658 2
a1659 2
paddr_t
pmap_extract(pmap, va)
d1662 1
a1668 1
	pa = 0;
d1671 2
d1678 2
a1679 1
	return (pa);
d1837 1
a1837 1
		kpa = pmap_extract(pmap, pv->pv_va);
d2670 2
a2671 2
		pmap->pm_stpa = (st_entry_t *)
			pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab);
@


1.21
log
@PMAP_NEW support; thorpej@@netbsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2001/05/09 15:31:24 art Exp $	*/
d1316 1
a1316 1
	pa = m68k_trunc_page(pa);
@


1.20
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.19 2001/05/06 21:38:59 millert Exp $	*/
/*	$NetBSD: pmap.c,v 1.75 1999/06/15 22:18:07 thorpej Exp $	*/
d803 15
d839 1
d901 1
a901 1
 *	Release the sources held by a pmap.
d1101 7
d1112 1
d1566 35
d1602 1
a1602 1
 * pmap_unwire:		[ INTERFACE]
d1604 1
a1604 1
 *	Change the wiring attribute for a map/virtual-address pair.
d1998 15
d2022 1
d2029 15
d2053 1
d2062 6
d2071 1
d2089 6
d2098 1
@


1.19
log
@Add casts to vaddr_t to round_page() and trunc_page() calls since
those macros no longer have an explicit cast inline.  Needed because
you can't do bitwise ops on a void *.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2001/05/05 21:26:35 art Exp $	*/
d1543 1
a1543 1
 * pmap_change_wiring:		[ INTERFACE]
d1550 1
a1550 1
pmap_change_wiring(pmap, va, wired)
a1552 1
	boolean_t	wired;
d1556 1
a1556 2
	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_change_wiring(%p, %lx, %x)\n", pmap, va, wired));
d1570 1
a1570 1
			printf("pmap_change_wiring: invalid STE for %lx\n", va);
d1579 1
a1579 1
			printf("pmap_change_wiring: invalid PTE for %lx\n", va);
d1587 3
a1589 6
	if (pmap_pte_w_chg(pte, wired ? PG_W : 0)) {
		pmap_pte_set_w(pte, wired);
		if (wired)
			pmap->pm_stats.wired_count++;
		else
			pmap->pm_stats.wired_count--;
a1931 24
}

/*
 * pmap_pageable:		[ INTERFACE ]
 *
 *	Make the specified pages (by pmap, offset) pageable (or not) as
 *	requested.
 *
 *	A page which is not pageable may not take a fault; therefore,
 *	its page table entry must remain valid for the duration.
 *
 *	This routine is merely advisory; pmap_enter() will specify that
 *	these pages are to be wired down (or not) as appropriate.
 */
void
pmap_pageable(pmap, sva, eva, pageable)
	pmap_t		pmap;
	vaddr_t		sva, eva;
	boolean_t	pageable;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_pageable(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, pageable));
@


1.18
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2001/05/05 02:41:24 millert Exp $	*/
d1343 1
a1343 1
		pmap_ptpage_addref(trunc_page(pte));
d1345 2
a1346 2
		(void) vm_map_pageable(pt_map, trunc_page(pte),
			round_page(pte+1), FALSE);
d2248 1
a2248 1
		vaddr_t ptpva = trunc_page(pte);
d2250 2
a2251 1
		(void) vm_map_pageable(pt_map, ptpva, round_page(pte+1), TRUE);
@


1.17
log
@Repair non-UVM kernels
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2001/05/04 22:48:59 aaron Exp $	*/
d678 1
a678 1
	pvp = (struct pv_page *) trunc_page(pv);
d735 1
a735 1
			pvp = (struct pv_page *) trunc_page(pv);
d1341 1
a1341 1
	if (pmap != pmap_kernel())
d1346 1
a1346 1
		    round_page(pte+1), FALSE);
d1348 1
d1538 1
a1538 1
		pmap_check_wiring("enter", trunc_page(pte));
d2203 1
a2203 1
		vaddr_t ptpva = trunc_page(pte);
d2349 1
a2349 1
			    ptpmap->pm_stab != (st_entry_t *)trunc_page(ste))
@


1.16
log
@Substantial update from NetBSD, most notably gives us UVM support; millert@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2001/04/06 23:54:47 millert Exp $	*/
d1342 1
d1344 4
d2200 1
d2245 11
d2807 1
d2843 1
@


1.15
log
@Use pool to allocate pmap instead of malloc.  OK'd by art@@
@
text
@d1 38
a38 2
/*	$OpenBSD: pmap.c,v 1.14 1999/09/03 18:00:42 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.36 1997/06/10 18:52:23 veego Exp $	*/
d140 1
a141 1
#include <sys/user.h>
d149 4
a154 49
#ifdef PMAPSTATS
struct {
	int collectscans;
	int collectpages;
	int kpttotal;
	int kptinuse;
	int kptmaxuse;
} kpt_stats;
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int nochange;	/* no change at all */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int pchange;	/* no mapping change, just protection */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
} enter_stats;
struct {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
} remove_stats;
struct {
	int calls;
	int changed;
	int alreadyro;
	int alreadyrw;
} protect_stats;
struct chgstats {
	int setcalls;
	int sethits;
	int setmiss;
	int clrcalls;
	int clrhits;
	int clrmiss;
} changebit_stats[16];
#endif

a155 2
int debugmap = 0;
int pmapdebug = 0x2000;
d172 4
a175 7
#ifdef M68K_MMU_HP
int pmapvacflush = 0;
#define	PVF_ENTER	0x01
#define	PVF_REMOVE	0x02
#define	PVF_PROTECT	0x04
#define	PVF_TOTAL	0x80
#endif
d181 3
a183 3

extern vm_offset_t pager_sva, pager_eva;
#endif
d190 1
a190 1
	(&((m)->pm_stab[(vm_offset_t)(v) >> SG4_SHIFT1]))
d196 1
a196 1
	(&((m)->pm_stab[(vm_offset_t)(v) \
d204 1
a204 1
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vm_offset_t)(v) >> SG_ISHIFT]))
d208 1
a208 1
#define pmap_pte(m, v)	(&((m)->pm_ptab[(vm_offset_t)(v) >> PG_SHIFT]))
d236 2
a237 2
	vm_offset_t	kpt_va;		/* always valid kernel VA */
	vm_offset_t	kpt_pa;		/* PA of this page (for speed) */
d254 1
a254 1
vm_size_t	Sysptsize = VM_KERNEL_PT_PAGES;
d258 3
d262 6
a267 8
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
vm_size_t	mem_size;	/* memory size in bytes */
vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
vm_offset_t	vm_first_phys;	/* PA of first managed page */
vm_offset_t	vm_last_phys;	/* PA just past last managed page */
int		npages;
a274 2
struct pool	pmap_pmap_pool;	/* pool that pmap structs are allocated from */

d282 7
a291 2
void	pmap_activate __P((pmap_t, struct pcb *));
void	pmap_deactivate __P((pmap_t, struct pcb *));
d293 1
a293 1
int	pmap_mapmulti __P((pmap_t, vm_offset_t));
d296 19
d318 9
a326 4
void	pmap_remove_mapping __P((pmap_t, vm_offset_t, pt_entry_t *, int));
boolean_t pmap_testbit	__P((vm_offset_t, int));
void	pmap_changebit	__P((vm_offset_t, int, boolean_t));
void	pmap_enter_ptpage	__P((pmap_t, vm_offset_t));
d329 2
a330 2
void pmap_pvdump	__P((vm_offset_t));
void pmap_check_wiring	__P((char *, vm_offset_t));
d334 3
a336 2
#define	PRM_TFLUSH	1
#define	PRM_CFLUSH	2
d339 14
a352 25
 * Bootstrap memory allocator. This function allows for early dynamic
 * memory allocation until the virtual memory system has been bootstrapped.
 * After that point, either kmem_alloc or malloc should be used. This
 * function works by stealing pages from the (to be) managed page pool,
 * stealing virtual address space, then mapping the pages and zeroing them.
 *
 * It should be used from pmap_bootstrap till vm_page_startup, afterwards
 * it cannot be used, and will generate a panic if tried. Note that this
 * memory will never be freed, and in essence it is wired down.
 */
void *
pmap_bootstrap_alloc(size)
	int size;
{
	extern boolean_t vm_page_startup_initialized;
	vm_offset_t val;
	
	if (vm_page_startup_initialized)
		panic("pmap_bootstrap_alloc: called after startup initialized");
	size = round_page(size);
	val = virtual_avail;

	virtual_avail = pmap_map(virtual_avail, avail_start,
		avail_start + size, VM_PROT_READ|VM_PROT_WRITE);
	avail_start += size;
d354 2
a355 6
	bzero ((caddr_t) val, size);

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
		0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

	return ((void *) val);
d359 6
a364 3
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
d367 1
a367 2
pmap_init(phys_start, phys_end)
	vm_offset_t	phys_start, phys_end;
d369 4
a372 2
	vm_offset_t	addr, addr2;
	vm_size_t	s;
d374 11
a385 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_init(%lx, %lx)\n", phys_start, phys_end);
#endif
d388 1
a388 1
	 * unavailable regions which we have mapped in locore.
d390 26
a415 2
	addr = (vm_offset_t) intiobase;
	(void) vm_map_find(kernel_map, NULL, (vm_offset_t) 0,
d417 1
a417 1
	if (addr != (vm_offset_t)intiobase)
d419 1
a419 1
	addr = (vm_offset_t) Sysmap;
d426 1
a426 1
	 * page table map.   Need to adjust pmap_size() in hp300_init.c.
d428 1
a428 1
	if (addr != (vm_offset_t)Sysmap)
d431 1
d433 6
a438 8
#ifdef DEBUG
	if (pmapdebug & PDB_INIT) {
		printf("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
		       Sysseg, Sysmap, Sysptmap);
		printf("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
		       avail_start, avail_end, virtual_avail, virtual_end);
	}
#endif
d444 5
a448 2
	npages = atop(phys_end - phys_start);
	s = (vm_size_t) (HP_STSIZE + sizeof(struct pv_entry) * npages + npages);
d450 8
a457 1
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
d461 1
d463 2
a464 1
	addr += sizeof(struct pv_entry) * npages;
d466 19
a484 6
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %lx bytes: npages %x s0 %p(%p) tbl %p atr %p\n",
		       s, npages, Segtabzero, Segtabzeropa,
		       pv_table, pmap_attributes);
#endif
d497 11
d510 1
a510 1
	if (rv != KERN_SUCCESS || addr + s >= (vm_offset_t)Sysmap)
d513 1
d519 7
a525 1
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
a536 8
#ifdef PMAPSTATS
	kpt_stats.kpttotal = atop(s);
#endif
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: KPT: %ld pages from %lx to %lx\n",
		       atop(s), addr, addr + s);
#endif
d538 26
d597 4
a600 4
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: pt_map [%lx - %lx)\n", addr, addr2);
#endif
d611 6
a618 2
	vm_first_phys = phys_start;
	vm_last_phys = phys_end;
d622 5
d635 5
d643 1
d667 5
d690 5
a694 1
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
d699 5
d720 3
a722 2
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp, pvp_pgi.pgi_list);
			pv_nfree -= pvp->pvp_pgi.pgi_nfree;
d730 1
a730 1
	for (ph = &pv_table[npages - 1]; ph >= &pv_table[0]; ph--) {
d739 2
a740 1
					TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
d759 5
a763 1
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
d768 2
d775 2
d778 1
a778 1
vm_offset_t
d780 2
a781 1
	vm_offset_t va, spa, epa;
d785 2
a786 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot);
#endif
d797 2
d801 1
a801 8
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
 *
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
d805 1
a805 1
	vm_size_t	size;
d809 2
a810 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create(%lx)\n", size);
#endif
d819 1
d826 5
a830 2
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
d837 2
a838 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_pinit(%p)\n", pmap);
#endif
a851 1
	pmap->pm_stchanged = TRUE;
d857 4
a860 3
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
d871 1
a871 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%p)\n", pmap);
#endif
d883 5
a887 3
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
d894 1
a894 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_release(%p)\n", pmap);
#endif
d904 5
a908 1
		kmem_free_wakeup(pt_map, (vm_offset_t)pmap->pm_ptab,
d910 1
d912 5
a916 1
		kmem_free_wakeup(st_map, (vm_offset_t)pmap->pm_stab,
d918 1
d922 2
d934 1
a934 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%p)\n", pmap);
#endif
d941 10
d952 2
a953 3
pmap_activate(pmap, pcb)
	pmap_t pmap;
	struct pcb *pcb;
d955 1
d957 2
a958 7
	if (pmap == NULL)
		return;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_SEGTAB))
		printf("pmap_activate(%p, %p)\n", pmap, pcb);
#endif
d960 1
a960 1
	PMAP_ACTIVATE(pmap, pcb, pmap == curproc->p_vmspace->vm_map.pmap);
d963 9
d973 2
a974 3
pmap_deactivate(pmap, pcb)
	pmap_t pmap;
	struct pcb *pcb;
d976 2
d981 2
d991 1
a991 1
	vm_offset_t sva, eva;
d993 1
a993 1
	vm_offset_t nssva;
d998 2
a999 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
#endif
a1003 3
#ifdef PMAPSTATS
	remove_stats.calls++;
#endif
a1033 3
#ifdef PMAPSTATS
						remove_stats.sflushes++;
#endif
a1067 10
#ifdef DEBUG
	if (pmap_aliasmask && (pmapvacflush & PVF_REMOVE)) {
		if (pmapvacflush & PVF_TOTAL)
			DCIA();
		else if (pmap == pmap_kernel())
			DCIS();
		else
			DCIU();
	} else
#endif
a1070 3
#ifdef PMAPSTATS
			remove_stats.sflushes++;
#endif
a1072 3
#ifdef PMAPSTATS
			remove_stats.uflushes++;
#endif
d1079 1
a1079 1
 *	pmap_page_protect:
d1081 2
a1082 1
 *	Lower the permission for all mappings to a given page.
d1086 1
a1086 1
	vm_offset_t	pa;
d1097 1
a1097 1
	if (pa < vm_first_phys || pa >= vm_last_phys)
d1107 1
a1107 1
		pmap_changebit(pa, PG_RO, TRUE);
d1134 2
d1142 4
a1145 2
 *	Set the physical protection on the
 *	specified range of this map as requested.
d1149 3
a1151 3
	pmap_t	pmap;
	vm_offset_t sva, eva;
	vm_prot_t prot;
d1153 1
a1153 1
	vm_offset_t nssva;
d1158 3
a1160 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%p, %lx, %lx, %x)\n", pmap, sva, eva, prot);
#endif
a1164 3
#ifdef PMAPSTATS
	protect_stats.calls++;
#endif
d1212 1
a1212 1
					vm_offset_t pa = pmap_pte_pa(pte);
a1220 3
#ifdef PMAPSTATS
				protect_stats.changed++;
#endif
a1222 8
#ifdef PMAPSTATS
			else if (pmap_pte_v(pte)) {
				if (isro)
					protect_stats.alreadyro++;
				else
					protect_stats.alreadyrw++;
			}
#endif
a1226 10
#if defined(M68K_MMU_HP) && defined(DEBUG)
	if (pmap_aliasmask && (pmapvacflush & PVF_PROTECT)) {
		if (pmapvacflush & PVF_TOTAL)
			DCIA();
		else if (pmap == pmap_kernel())
			DCIS();
		else
			DCIU();
	}
#endif
d1230 4
a1233 2
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
d1237 1
a1237 1
 *	that the related pte can not be reclaimed.
d1239 1
a1239 1
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
d1246 2
a1247 2
	vm_offset_t va;
	vm_offset_t pa;
d1254 1
a1254 1
	vm_offset_t opa;
d1258 4
a1261 5
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
		       pmap, va, pa, prot, wired);
#endif
d1265 7
a1271 5
#ifdef PMAPSTATS
	if (pmap == pmap_kernel())
		enter_stats.kernel++;
	else
		enter_stats.user++;
d1273 1
d1278 4
d1284 1
d1295 2
a1296 4
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: pte %p, *pte %x\n", pte, *pte);
#endif
a1301 3
#ifdef PMAPSTATS
		enter_stats.pwchange++;
#endif
d1309 2
a1310 4
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("enter: wiring change -> %x\n", wired);
#endif
a1314 4
#ifdef PMAPSTATS
			if (pmap_pte_prot(pte) == pte_prot(pmap, prot))
				enter_stats.wchange++;
#endif
a1315 6
#ifdef PMAPSTATS
		else if (pmap_pte_prot(pte) != pte_prot(pmap, prot))
			enter_stats.pchange++;
		else
			enter_stats.nochange++;
#endif
d1330 4
a1333 8
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: removing old mapping %lx\n", va);
#endif
		pmap_remove_mapping(pmap, va, pte, PRM_TFLUSH|PRM_CFLUSH);
#ifdef PMAPSTATS
		enter_stats.mchange++;
#endif
d1342 1
a1342 2
		(void) vm_map_pageable(pt_map, trunc_page(pte),
				       round_page(pte+1), FALSE);
d1349 1
a1349 1
	if (pa >= vm_first_phys && pa < vm_last_phys) {
a1352 3
#ifdef PMAPSTATS
		enter_stats.managed++;
#endif
d1355 3
a1357 5
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: pv at %p: %lx/%p/%p\n",
			       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
a1361 3
#ifdef PMAPSTATS
			enter_stats.firstpv++;
#endif
a1386 4
#ifdef PMAPSTATS
			if (!npv->pv_next)
				enter_stats.secondpv++;
#endif
d1412 3
a1414 5
#ifdef DEBUG
					if (pmapdebug & PDB_CACHE)
					printf("enter: pa %lx already CI'ed\n",
					       pa);
#endif
d1422 3
a1424 5
#ifdef DEBUG
					if (pmapdebug & PDB_CACHE)
					printf("enter: pa %lx CI'ing all\n",
					       pa);
#endif
a1426 3
#ifdef PMAPSTATS
					enter_stats.ci++;
#endif
d1431 14
a1452 3
#ifdef PMAPSTATS
		enter_stats.unmanaged++;
#endif
d1495 3
a1497 4
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: new pte value %x\n", npte);
#endif
d1521 1
a1521 1
		pmap_changebit(pa, PG_CI, TRUE);
a1522 3
#ifdef PMAPSTATS
		enter_stats.flushes++;
#endif
a1528 10
#ifdef DEBUG
	else if (pmapvacflush & PVF_ENTER) {
		if (pmapvacflush & PVF_TOTAL)
			DCIA();
		else if (pmap == pmap_kernel())
			DCIS();
		else
			DCIU();
	}
#endif
d1532 1
a1532 1
		pmap_check_wiring("enter", trunc_page(pmap_pte(pmap, va)));
d1537 5
a1541 5
 *	Routine:	pmap_change_wiring
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
d1545 2
a1546 2
	pmap_t	pmap;
	vm_offset_t	va;
d1551 3
a1553 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_change_wiring(%p, %lx, %x)\n", pmap, va, wired);
#endif
d1593 4
a1596 4
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
d1598 1
a1598 2

vm_offset_t
d1601 1
a1601 1
	vm_offset_t va;
d1603 4
a1606 1
	vm_offset_t pa;
a1607 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%p, %lx) -> ", pmap, va);
#endif
d1613 4
a1616 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("%lx\n", pa);
#endif
	return(pa);
d1620 3
a1622 1
 *	Copy the range specified by src_addr/len
d1628 2
a1629 1
void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
d1632 3
a1634 3
	vm_offset_t	dst_addr;
	vm_size_t	len;
	vm_offset_t	src_addr;
d1636 4
a1639 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
#endif
d1643 2
d1646 14
a1659 2
 *	incorrect entries NOW.  [This update includes
 *	forcing updates of any address map caching.]
d1661 6
a1666 2
 *	Generally used to insure that a thread about
 *	to run will see a semantically correct world.
d1668 3
a1670 1
void pmap_update()
d1672 29
a1700 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_update()\n");
a1701 1
	TBIA();
d1705 7
a1711 9
 *	Routine:	pmap_collect
 *	Function:
 *		Garbage collects the physical map system for
 *		pages which are no longer used.
 *		Success need not be guaranteed -- that is, there
 *		may well be pages which are not referenced, but
 *		others may be collected.
 *	Usage:
 *		Called by the pageout daemon when pages are scarce.
d1714 1
a1714 1
pmap_collect(pmap)
d1716 1
d1718 1
a1718 1
	vm_offset_t pa;
d1721 1
a1721 3
	vm_offset_t kpa;
	int s;

a1725 2
	if (pmap != pmap_kernel())
		return;
d1727 1
a1727 9
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%p)\n", pmap);
#endif
#ifdef PMAPSTATS
	kpt_stats.collectscans++;
#endif
	s = splimp();
	for (pa = vm_first_phys; pa < vm_last_phys; pa += NBPG) {
d1744 2
a1745 2
		if (pv->pv_va < (vm_offset_t)Sysmap ||
		    pv->pv_va >= (vm_offset_t)Sysmap + HP_MAX_PTSIZE)
a1796 4
#ifdef PMAPSTATS
		kpt_stats.kptinuse--;
		kpt_stats.collectpages++;
#endif
a1809 1
	splx(s);
d1813 9
a1821 10
 *	pmap_zero_page zeros the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bzero to clear its contents, one machine dependent page
 *	at a time.
 *
 *	XXX this is a bad implementation for virtual cache machines
 *	(320/350) because pmap_enter doesn't cache-inhibit the temporary
 *	kernel mapping and we wind up with data cached for that KVA.
 *	It is probably a win for physical cache machines (370/380)
 *	as the cache loading is not wasted.
d1825 1
a1825 1
	vm_offset_t phys;
d1827 32
a1858 2
	vm_offset_t kva;
	extern caddr_t CADDR1;
d1861 2
a1862 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%lx)\n", phys);
d1864 2
a1865 6
	kva = (vm_offset_t) CADDR1;
	pmap_enter(pmap_kernel(), kva, phys, VM_PROT_READ|VM_PROT_WRITE, TRUE,
		   VM_PROT_READ|VM_PROT_WRITE);
	zeropage((caddr_t)kva);
	pmap_remove_mapping(pmap_kernel(), kva, PT_ENTRY_NULL,
			    PRM_TFLUSH|PRM_CFLUSH);
d1869 1
a1869 5
 *	pmap_copy_page copies the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bcopy to copy the page, one machine dependent page at a
 *	time.
 *
d1871 7
a1877 5
 *	XXX this is a bad implementation for virtual cache machines
 *	(320/350) because pmap_enter doesn't cache-inhibit the temporary
 *	kernel mapping and we wind up with data cached for that KVA.
 *	It is probably a win for physical cache machines (370/380)
 *	as the cache loading is not wasted.
d1881 1
a1881 1
	vm_offset_t src, dst;
d1883 38
a1920 2
	vm_offset_t skva, dkva;
	extern caddr_t CADDR1, CADDR2;
d1923 5
a1927 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%lx, %lx)\n", src, dst);
d1929 2
a1930 8
	skva = (vm_offset_t) CADDR1;
	dkva = (vm_offset_t) CADDR2;
	pmap_enter(pmap_kernel(), skva, src, VM_PROT_READ, TRUE, VM_PROT_READ);
	pmap_enter(pmap_kernel(), dkva, dst, VM_PROT_READ|VM_PROT_WRITE, TRUE,
		   VM_PROT_READ|VM_PROT_WRITE);
	copypage((caddr_t)skva, (caddr_t)dkva);
	/* CADDR1 and CADDR2 are virtually contiguous */
	pmap_remove(pmap_kernel(), skva, skva + (2 * NBPG));
d1934 1
a1934 4
 *	Routine:	pmap_pageable
 *	Function:
 *		Make the specified pages (by pmap, offset)
 *		pageable (or not) as requested.
d1936 5
a1940 3
 *		A page which is not pageable may not take
 *		a fault; therefore, its page table entry
 *		must remain valid for the duration.
d1942 2
a1943 3
 *		This routine is merely advisory; pmap_enter
 *		will specify that these pages are to be wired
 *		down (or not) as appropriate.
d1948 1
a1948 1
	vm_offset_t	sva, eva;
a1950 16
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pageable(%p, %lx, %lx, %x)\n",
		       pmap, sva, eva, pageable);
#endif
	/*
	 * If we are making a PT page pageable then all valid
	 * mappings must be gone from that page.  Hence it should
	 * be all zeros and there is no need to clean it.
	 * Assumptions:
	 *	- we are called with only one page at a time
	 *	- PT pages have only one pv_table entry
	 */
	if (pmap == pmap_kernel() && pageable && sva + NBPG == eva) {
		struct pv_entry *pv;
		vm_offset_t pa;
d1952 3
a1954 37
#ifdef DEBUG
		if ((pmapdebug & (PDB_FOLLOW|PDB_PTPAGE)) == PDB_PTPAGE)
			printf("pmap_pageable(%p, %lx, %lx, %x)\n",
			       pmap, sva, eva, pageable);
#endif
		if (!pmap_ste_v(pmap, sva))
			return;
		pa = pmap_pte_pa(pmap_pte(pmap, sva));
		if (pa < vm_first_phys || pa >= vm_last_phys)
			return;
		pv = pa_to_pvh(pa);
		if (pv->pv_ptste == NULL)
			return;
#ifdef DEBUG
		if (pv->pv_va != sva || pv->pv_next) {
			printf("pmap_pageable: bad PT page va %lx next %p\n",
			       pv->pv_va, pv->pv_next);
			return;
		}
#endif
		/*
		 * Mark it unmodified to avoid pageout
		 */
		pmap_changebit(pa, PG_M, FALSE);
#ifdef DEBUG
		if ((PHYS_TO_VM_PAGE(pa)->flags & PG_CLEAN) == 0) {
			printf("pa %lx: flags=%x: not clean\n",
			       pa, PHYS_TO_VM_PAGE(pa)->flags);
			PHYS_TO_VM_PAGE(pa)->flags |= PG_CLEAN;
		}
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_pageable: PT page %lx(%x) unmodified\n",
			       sva, *pmap_pte(pmap, sva));
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("pageable", sva);
#endif
	}
d1958 2
a1961 1

d1964 1
a1964 1
	vm_offset_t	pa;
d1966 4
a1969 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%lx)\n", pa);
#endif
	pmap_changebit(pa, PG_M, FALSE);
d1973 1
a1973 1
 *	pmap_clear_reference:
d1977 6
d1984 1
a1984 8
void pmap_clear_reference(pa)
	vm_offset_t	pa;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%lx)\n", pa);
#endif
	pmap_changebit(pa, PG_U, FALSE);
d1988 1
a1988 1
 *	pmap_is_referenced:
a1992 1

d1995 1
a1995 1
	vm_offset_t	pa;
d2008 1
a2008 1
 *	pmap_is_modified:
a2012 1

d2015 1
a2015 1
	vm_offset_t	pa;
d2027 10
a2036 1
vm_offset_t
d2043 26
d2071 1
a2071 3
 * 'PUX hack for dealing with the so called multi-mapped address space.
 * The first 256mb is mapped in at every 256mb region from 0x10000000
 * up to 0xF0000000.  This allows for 15 bits of tag information.
d2073 6
a2078 2
 * We implement this at the segment table level, the machine independent
 * VM knows nothing about it.
d2083 1
a2083 1
	vm_offset_t va;
d2105 1
a2105 1
#endif
d2112 13
a2124 4
 * Invalidate a single page denoted by pmap/va.
 * If (pte != NULL), it is the already computed PTE for the page.
 * If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 * If (flags & PRM_CFLUSH), we must flush/invalidate any cache information.
d2130 1
a2130 1
	vm_offset_t va;
d2134 1
a2134 1
	vm_offset_t pa;
d2141 1
d2143 3
a2145 4
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_mapping(%p, %lx, %p, %x)\n",
		       pmap, va, pte, flags);
#endif
a2161 3
#ifdef PMAPSTATS
		remove_stats.sflushes++;
#endif
a2168 3
#ifdef PMAPSTATS
			remove_stats.uflushes++;
#endif
a2175 3
#ifdef PMAPSTATS
	remove_stats.removes++;
#endif
d2186 1
a2186 4
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("remove: invalidating pte at %p\n", pte);
#endif
d2193 1
a2193 4
	 * the PT page.  We do this after the PTE has been
	 * invalidated because vm_map_pageable winds up in
	 * pmap_pageable which clears the modify bit for the
	 * PT page.
d2196 2
a2197 2
		(void) vm_map_pageable(pt_map, trunc_page(pte),
				       round_page(pte+1), TRUE);
d2200 1
a2200 1
			pmap_check_wiring("remove", trunc_page(pte));
d2202 36
d2242 1
a2242 1
	if (pa < vm_first_phys || pa >= vm_last_phys)
a2266 3
#ifdef PMAPSTATS
		remove_stats.pvfirst++;
#endif
a2268 3
#ifdef PMAPSTATS
			remove_stats.pvsearch++;
#endif
d2289 2
a2290 4
#ifdef DEBUG
		if (pmapdebug & PDB_CACHE)
			printf("remove: clearing CI for pa %lx\n", pa);
#endif
d2292 1
a2292 1
		pmap_changebit(pa, PG_CI, FALSE);
d2305 3
a2307 8
#ifdef PMAPSTATS
		remove_stats.ptinvalid++;
#endif
#ifdef DEBUG
		if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
			printf("remove: ste was %x@@%p pte was %x@@%p\n",
			       *ste, ste, opte, pmap_pte(pmap, va));
#endif
d2326 3
a2329 3
			if (pmapdebug & (PDB_REMOVE|PDB_SEGTAB))
				printf("remove: stab %p, refcnt %d\n",
				       ptpmap->pm_stab, ptpmap->pm_sref - 1);
d2335 8
a2342 5
#ifdef DEBUG
				if (pmapdebug&(PDB_REMOVE|PDB_SEGTAB))
					printf("remove: free stab %p\n",
					       ptpmap->pm_stab);
#endif
d2344 1
a2344 1
						 (vm_offset_t)ptpmap->pm_stab,
d2346 1
a2352 1
				ptpmap->pm_stchanged = TRUE;
d2359 1
a2359 2
					PMAP_ACTIVATE(ptpmap,
					    &curproc->p_addr->u_pcb, 1);
d2382 1
a2382 1
	pmap_attributes[pmap_page_index(pa)] |= bits;
d2386 5
d2394 1
a2394 1
	vm_offset_t pa;
d2401 1
a2401 1
	if (pa < vm_first_phys || pa >= vm_last_phys)
d2409 1
a2409 1
	if (pmap_attributes[pmap_page_index(pa)] & bit) {
d2421 2
a2422 2
	 * Not found, check current mappings returning
	 * immediately if found.
d2428 1
d2438 6
d2446 3
a2448 4
pmap_changebit(pa, bit, setem)
	vm_offset_t pa;
	int bit;
	boolean_t setem;
d2452 1
a2452 1
	vm_offset_t va;
a2456 3
#ifdef PMAPSTATS
	struct chgstats *chgp;
#endif
d2458 4
a2461 6
#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(%lx, %x, %s)\n",
		       pa, bit, setem ? "set" : "clear");
#endif
	if (pa < vm_first_phys || pa >= vm_last_phys)
a2463 7
#ifdef PMAPSTATS
	chgp = &changebit_stats[(bit>>2)-1];
	if (setem)
		chgp->setcalls++;
	else
		chgp->clrcalls++;
#endif
d2466 1
d2470 2
a2471 2
	if (!setem)
		pmap_attributes[pmap_page_index(pa)] &= ~bit;
d2489 6
a2494 2
			if (bit == PG_RO) {
				extern vm_offset_t pager_sva, pager_eva;
d2498 1
d2512 1
a2512 4
			if (setem)
				npte = *pte | bit;
			else
				npte = *pte & ~bit;
d2520 4
a2523 3
				if (firstpage && mmutype == MMU_68040 &&
				    ((bit == PG_RO && setem) ||
				    (bit & PG_CMASK))) {
a2531 6
#ifdef PMAPSTATS
				if (setem)
					chgp->sethits++;
				else
					chgp->clrhits++;
#endif
a2532 8
#ifdef PMAPSTATS
			else {
				if (setem)
					chgp->setmiss++;
				else
					chgp->clrmiss++;
			}
#endif
a2533 10
#if defined(M68K_MMU_HP) && defined(DEBUG)
		if (setem && bit == PG_RO && (pmapvacflush & PVF_PROTECT)) {
			if ((pmapvacflush & PVF_TOTAL) || toflush == 3)
				DCIA();
			else if (toflush == 2)
				DCIS();
			else
				DCIU();
		}
#endif
d2538 5
d2547 1
a2547 1
	vm_offset_t va;
d2549 1
a2549 1
	vm_offset_t ptpa;
d2554 3
a2556 7
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE))
		printf("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va);
#endif
#ifdef PMAPSTATS
	enter_stats.ptpneeded++;
#endif
d2565 4
d2571 1
d2573 1
a2573 1
			pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_stab);
d2579 1
a2579 1
			pmap_changebit((vm_offset_t)pmap->pm_stpa, PG_CCB, 0);
a2582 1
		pmap->pm_stchanged = TRUE;
d2588 5
a2592 6
			PMAP_ACTIVATE(pmap, &curproc->p_addr->u_pcb, 1);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: pmap %p stab %p(%p)\n",
			       pmap, pmap->pm_stab, pmap->pm_stpa);
#endif
d2613 3
a2615 4
#ifdef DEBUG
			if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
				printf("enter: alloc ste2 %d(%p)\n", ix, addr);
#endif
d2627 3
a2629 5
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: ste2 %p (%p)\n",
			       pmap_ste2(pmap, va), ste);
#endif
d2632 1
a2632 1
	va = trunc_page((vm_offset_t)pmap_pte(pmap, va));
d2648 2
a2649 4
#ifdef DEBUG
			if (pmapdebug & PDB_COLLECT)
				printf("enter: no KPT pages, collecting...\n");
#endif
a2653 4
#ifdef PMAPSTATS
		if (++kpt_stats.kptinuse > kpt_stats.kptmaxuse)
			kpt_stats.kptmaxuse = kpt_stats.kptinuse;
#endif
d2660 1
a2660 1
			   VM_PROT_DEFAULT);
d2674 5
d2686 11
a2696 4
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
			printf("enter: about to fault UPT pg at %lx\n", va);
#endif
d2702 3
a2704 7
		ptpa = pmap_extract(pmap_kernel(), va);
		/*
		 * Mark the page clean now to avoid its pageout (and
		 * hence creation of a pager) between now and when it
		 * is wired; i.e. while it is on a paging queue.
		 */
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_CLEAN;
d2708 1
d2726 1
a2726 1
		pmap_changebit(ptpa, PG_CCB, 0);
d2749 3
a2751 4
#ifdef DEBUG
	if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
		printf("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste);
#endif
d2773 3
a2775 5
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: stab %p refcnt %d\n",
			       pmap->pm_stab, pmap->pm_sref);
#endif
d2790 36
d2827 5
d2835 1
a2835 1
	vm_offset_t pa;
d2847 7
d2858 1
a2858 1
	vm_offset_t va;
d2860 3
a2862 1
	vm_map_entry_t entry;
a2863 1
	pt_entry_t *pte;
a2864 1
	va = trunc_page(va);
d2869 4
a2872 2
	if (!vm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
d2875 1
d2880 3
a2882 3
	if (entry->wired_count != count)
		printf("*%s*: %lx: w%d/a%d\n",
		       str, va, entry->wired_count, count);
d2884 1
a2884 1
#endif
@


1.14
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1999/07/18 18:00:04 deraadt Exp $	*/
d104 1
d288 2
d351 4
d692 1
a692 6
	/* XXX: is it ok to wait here? */
	pmap = (pmap_t) malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
#ifdef notifwewait
	if (pmap == NULL)
		panic("pmap_create: cannot allocate a pmap");
#endif
d753 1
a753 1
		free((caddr_t)pmap, M_VMPMAP);
@


1.14.4.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2001/04/06 23:54:47 millert Exp $	*/
a103 1
#include <sys/pool.h>
a286 2
struct pool	pmap_pmap_pool;	/* pool that pmap structs are allocated from */

a347 4

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
		0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

d685 6
a690 1
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
d751 1
a751 1
		pool_put(&pmap_pmap_pool, pmap);
@


1.14.4.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 38
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.c,v 1.80 1999/09/16 14:52:06 chs Exp $	*/

/*-
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */
d104 1
a105 1
#include <sys/pool.h>
d113 1
a113 1
#include <uvm/uvm.h>
d115 48
a162 1
#include <machine/cpu.h>
d165 2
d183 7
a189 4
int debugmap = 0;
int pmapdebug = PDB_PARANOIA;

#define	PMAP_DPRINTF(l, x)	if (pmapdebug & (l)) printf x
d195 3
a197 3
#else /* ! DEBUG */
#define	PMAP_DPRINTF(l, x)	/* nothing */
#endif /* DEBUG */
d204 1
a204 1
	(&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
d210 1
a210 1
	(&((m)->pm_stab[(vaddr_t)(v) \
d218 1
a218 1
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
d222 1
a222 1
#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))
d250 2
a251 2
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
d268 1
a268 1
vsize_t		Sysptsize = VM_KERNEL_PT_PAGES;
a271 1
struct vm_map	st_map_store, pt_map_store;
d273 8
a280 6
paddr_t		avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vsize_t		mem_size;	/* memory size in bytes */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
int		page_cnt;	/* number of pages managed by VM system */
d288 2
a296 7
extern caddr_t	CADDR1, CADDR2;

pt_entry_t	*caddr1_pte;	/* PTE for CADDR1 */
pt_entry_t	*caddr2_pte;	/* PTE for CADDR2 */

struct pool	pmap_pmap_pool;	/* memory pool for pmap structures */

d300 2
d303 1
a303 1
int	pmap_mapmulti __P((pmap_t, vaddr_t));
a305 19
#define	PAGE_IS_MANAGED(pa)	(pmap_initialized &&			\
				 vm_physseg_find(atop((pa)), NULL) != -1)

#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})

#define	pa_to_attribute(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
})

d309 4
a312 9
void	pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));
boolean_t pmap_testbit	__P((paddr_t, int));
void	pmap_changebit	__P((paddr_t, int, int));
void	pmap_enter_ptpage	__P((pmap_t, vaddr_t));
void	pmap_ptpage_addref __P((vaddr_t));
int	pmap_ptpage_delref __P((vaddr_t));
void	pmap_collect1	__P((pmap_t, paddr_t, paddr_t));
void	pmap_pinit __P((pmap_t));
void	pmap_release __P((pmap_t));
d315 2
a316 2
void pmap_pvdump	__P((paddr_t));
void pmap_check_wiring	__P((char *, vaddr_t));
d320 2
a321 3
#define	PRM_TFLUSH	0x01
#define	PRM_CFLUSH	0x02
#define	PRM_KEEPPTPAGE	0x04
d324 30
a353 14
 * pmap_virtual_space:		[ INTERFACE ]
 *
 *	Report the range of available kernel virtual address
 *	space to the VM system during bootstrap.
 *
 *	This is only an interface function if we do not use
 *	pmap_steal_memory()!
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_virtual_space(vstartp, vendp)
	vaddr_t	*vstartp, *vendp;
{
d355 1
a355 2
	*vstartp = virtual_avail;
	*vendp = virtual_end;
d359 3
a361 6
 * pmap_init:			[ INTERFACE ]
 *
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
 *
 *	Note: no locking is necessary in this function.
d364 2
a365 1
pmap_init()
d367 2
a368 4
	vaddr_t		addr, addr2;
	vsize_t		s;
	struct pv_entry	*pv;
	char		*attr;
a369 11
	int		npages;
	int		bank;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));

	/*
	 * Before we do anything else, initialize the PTE pointers
	 * used by pmap_zero_page() and pmap_copy_page().
	 */
	caddr1_pte = pmap_pte(pmap_kernel(), CADDR1);
	caddr2_pte = pmap_pte(pmap_kernel(), CADDR2);
d371 4
d377 1
a377 1
	 * unavailable regions which we have mapped in pmap_bootstrap().
d379 4
a382 7
	addr = (vaddr_t) intiobase;
	if (uvm_map(kernel_map, &addr,
		    m68k_ptob(IIOMAPSIZE+EIOMAPSIZE),
		    NULL, UVM_UNKNOWN_OFFSET,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)) != KERN_SUCCESS)
d384 21
a404 21
	addr = (vaddr_t) Sysmap;
	if (uvm_map(kernel_map, &addr, HP_MAX_PTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)) != KERN_SUCCESS) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
 bogons:
		panic("pmap_init: bogons in the VM system!\n");
	}

	PMAP_DPRINTF(PDB_INIT,
	    ("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
	    Sysseg, Sysmap, Sysptmap));
	PMAP_DPRINTF(PDB_INIT,
	    ("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
	    avail_start, avail_end, virtual_avail, virtual_end));
d410 2
a411 5
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++)
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
	s = HP_STSIZE;					/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */
d413 1
a413 4
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: can't allocate data structures");

d415 1
a415 1
	pmap_extract(pmap_kernel(), addr, (paddr_t *)Segtabzeropa);
a416 1

d418 1
a418 2
	addr += page_cnt * sizeof(struct pv_entry);

d420 6
a425 19

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
	    "tbl %p atr %p\n",
	    s, page_cnt, Segtabzero, Segtabzeropa,
	    pv_table, pmap_attributes));

	/*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}
d439 2
a440 4
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv != KERN_SUCCESS || (addr + s) >= (vaddr_t)Sysmap)
d442 1
a442 3
	rv = uvm_unmap(kernel_map, addr, addr + s);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: uvm_unmap failed");
d448 1
a448 3
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
d458 1
a458 1
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
d460 8
a467 3

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: KPT: %ld pages from %lx to %lx\n",
	    atop(s), addr, addr + s));
d470 1
a470 1
	 * Allocate the segment table map and the page table map.
d473 1
a473 2
	st_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, 0, FALSE,
	    &st_map_store);
d475 4
d491 15
a505 2
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, VM_MAP_PAGEABLE,
	    TRUE, &pt_map_store);
a515 6
	 * Initialize the pmap pools.
	 */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

	/*
d518 2
a522 5
/*
 * pmap_alloc_pv:
 *
 *	Allocate a pv_entry.
 */
d531 1
a531 1
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG);
d533 1
a533 1
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
a556 5
/*
 * pmap_free_pv:
 *
 *	Free a pv_entry.
 */
d563 1
a563 1
	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
d575 1
a575 1
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
a579 5
/*
 * pmap_collect_pv:
 *
 *	Perform compaction on the PV list, called via pmap_collect().
 */
d596 2
a597 3
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp,
			    pvp_pgi.pgi_list);
			pv_nfree -= NPVPPG;
d605 1
a605 1
	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
d610 1
a610 1
			pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
d614 1
a614 2
					TAILQ_REMOVE(&pv_page_freelist, pvp,
					    pvp_pgi.pgi_list);
d633 1
a633 1
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
a637 2
 * pmap_map:
 *
a642 2
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
d644 1
a644 1
vaddr_t
d646 1
a646 2
	vaddr_t va;
	paddr_t spa, epa;
d650 4
a653 2
	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));
a663 2
 * pmap_create:			[ INTERFACE ]
 *
d666 8
a673 1
 *	Note: no locking is necessary in this function.
a674 15
#ifdef PMAP_NEW
pmap_t
pmap_create()
{
	pmap_t pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create\n"));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return (pmap);
}
#else
d677 1
a677 1
	vsize_t	size;
d681 4
a684 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create(%lx)\n", size));
a692 1

a696 1
#endif
d699 2
a700 5
 * pmap_pinit:
 *
 *	Initialize a preallocated and zeroed pmap structure.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_create()!
d707 4
a710 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_pinit(%p)\n", pmap));
d724 1
d730 3
a732 4
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
d743 4
a746 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));
d758 3
a760 5
 * pmap_release:
 *
 *	Release the resources held by a pmap.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_destroy().
d767 4
a770 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_release(%p)\n", pmap));
d780 2
a781 2
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
				   HP_MAX_PTSIZE);
d783 2
a784 2
		uvm_km_free_wakeup(st_map, (vaddr_t)pmap->pm_stab,
				   HP_STSIZE);
a787 2
 * pmap_reference:		[ INTERFACE ]
 *
d798 4
a801 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_reference(%p)\n", pmap));
a807 10
/*
 * pmap_activate:		[ INTERFACE ]
 *
 *	Activate the pmap used by the specified process.  This includes
 *	reloading the MMU context of the current process, and marking
 *	the pmap in use by the processor.
 *
 *	Note: we may only use spin locks here, since we are called
 *	by a critical section in cpu_switch()!
 */
d809 3
a811 2
pmap_activate(p)
	struct proc *p;
a812 1
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d814 7
a820 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_SEGTAB,
	    ("pmap_activate(%p)\n", p));
d822 1
a822 1
	PMAP_ACTIVATE(pmap, p == curproc);
a824 9
/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.
 */
d826 3
a828 2
pmap_deactivate(p)
	struct proc *p;
a829 2

	/* No action necessary in this pmap implementation. */
a832 2
 * pmap_remove:			[ INTERFACE ]
 *
d841 1
a841 1
	vaddr_t sva, eva;
d843 1
a843 1
	vaddr_t nssva;
d848 4
a851 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva));
d856 3
d889 3
d926 10
d939 3
d944 3
d953 1
a953 1
 * pmap_page_protect:		[ INTERFACE ]
d955 1
a955 2
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
a957 7
#ifdef PMAP_NEW
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t	prot;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#else
d959 1
a959 1
	paddr_t		pa;
a961 1
#endif
d970 1
a970 1
	if (PAGE_IS_MANAGED(pa) == 0)
d980 1
a980 1
		pmap_changebit(pa, PG_RO, ~0);
a1006 2
			if (pv == NULL)
				break;
d1013 2
a1014 4
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protection on the specified range of this map
 *	as requested.
d1018 3
a1020 3
	pmap_t		pmap;
	vaddr_t		sva, eva;
	vm_prot_t	prot;
d1022 1
a1022 1
	vaddr_t nssva;
d1027 4
a1030 3
	PMAP_DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_protect(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, prot));
d1035 3
d1085 1
a1085 1
					paddr_t pa = pmap_pte_pa(pte);
d1094 3
d1099 8
d1111 10
d1124 2
a1125 4
 * pmap_enter:			[ INTERFACE ]
 *
 *	Insert the given physical page (pa) at
 *	the specified virtual address (va) in the
d1129 1
a1129 1
 *	that the related pte cannot be reclaimed.
d1131 1
a1131 1
 *	Note: This is the only routine which MAY NOT lazy-evaluate
d1138 2
a1139 2
	vaddr_t va;
	paddr_t pa;
d1146 1
a1146 1
	paddr_t opa;
d1150 5
a1154 4
	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, %lx, %lx, %x, %x)\n",
	    pmap, va, pa, prot, wired));

d1158 5
a1162 7
#ifdef DIAGNOSTIC
	/*
	 * pmap_enter() should never be used for CADDR1 and CADDR2.
	 */
	if (pmap == pmap_kernel() &&
	    (va == (vaddr_t)CADDR1 || va == (vaddr_t)CADDR2))
		panic("pmap_enter: used for CADDR1 or CADDR2");
a1163 1

d1169 1
a1169 1
			uvm_km_valloc_wait(pt_map, HP_MAX_PTSIZE);
d1177 1
a1177 1
	pa = trunc_page(pa);
d1180 4
a1183 2

	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));
d1189 3
d1199 4
a1202 2
			PMAP_DPRINTF(PDB_ENTER,
			    ("enter: wiring change -> %x\n", wired));
d1207 4
d1212 6
d1232 8
a1239 4
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: removing old mapping %lx\n", va));
		pmap_remove_mapping(pmap, va, pte,
		    PRM_TFLUSH|PRM_CFLUSH|PRM_KEEPPTPAGE);
d1247 3
a1249 3
	if (pmap != pmap_kernel()) {
		pmap_ptpage_addref(trunc_page((vaddr_t)pte));
	}
d1256 1
a1256 1
	if (PAGE_IS_MANAGED(pa)) {
d1260 3
d1265 5
a1269 3
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: pv at %p: %lx/%p/%p\n",
		    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
d1274 3
d1302 4
d1331 5
a1335 3
					PMAP_DPRINTF(PDB_CACHE,
					    ("enter: pa %lx already CI'ed\n",
					    pa));
d1343 5
a1347 3
					PMAP_DPRINTF(PDB_CACHE,
					    ("enter: pa %lx CI'ing all\n",
					    pa));
d1350 3
a1356 14

		/*
		 * Speed pmap_is_referenced() or pmap_is_modified() based
		 * on the hint provided in access_type.
		 */
#ifdef DIAGNOSTIC
		if (access_type & ~prot)
			panic("pmap_enter: access_type exceeds prot");
#endif
		if (access_type & VM_PROT_WRITE)
			*pa_to_attribute(pa) |= (PG_U|PG_M);
		else if (access_type & VM_PROT_ALL)
			*pa_to_attribute(pa) |= PG_U;

d1365 3
d1410 4
a1413 3

	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));

d1437 1
a1437 1
		pmap_changebit(pa, PG_CI, ~0);
d1439 3
d1448 10
d1461 1
a1461 1
		pmap_check_wiring("enter", trunc_page((vaddr_t)pte));
a1464 35
#ifdef PMAP_NEW
void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	pmap_enter(pmap_kernel(), va, pa, prot, TRUE, 0);
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
				VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
	}
}

void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
#endif

d1466 5
a1470 5
 * pmap_unwire:			[ INTERFACE]
 *
 *	Clear the wired attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
d1473 4
a1476 3
pmap_unwire(pmap, va)
	pmap_t		pmap;
	vaddr_t		va;
d1480 4
a1483 2
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %lx)\n", pmap, va));

d1496 1
a1496 1
			printf("pmap_unwire: invalid STE for %lx\n", va);
d1505 1
a1505 1
			printf("pmap_unwire: invalid PTE for %lx\n", va);
d1513 6
a1518 3
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, 0);
		pmap->pm_stats.wired_count--;
d1523 4
a1526 4
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
d1528 3
a1530 2
boolean_t
pmap_extract(pmap, va, pap)
d1532 1
a1532 2
	vaddr_t va;
	paddr_t *pap;
d1534 1
a1534 4
	paddr_t pa;

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_extract(%p, %lx) -> ", pmap, va));
d1536 5
a1542 2
	else
		return (FALSE);
d1545 6
a1550 7

	PMAP_DPRINTF(PDB_FOLLOW, ("%lx\n", pa));

	if (pap)
		*pap = pa;
	return (TRUE);
}
d1553 1
a1553 3
 * pmap_copy:		[ INTERFACE ]
 *
 *	Copy the mapping range specified by src_addr/len
d1559 1
a1559 2
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
d1562 3
a1564 3
	vaddr_t		dst_addr;
	vsize_t		len;
	vaddr_t		src_addr;
d1566 5
a1570 4

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
	    dst_pmap, src_pmap, dst_addr, len, src_addr));
a1573 2
 * pmap_update:
 *
d1575 2
a1576 14
 *	incorrect entries NOW, by processing any deferred
 *	map operations.
 */
void
pmap_update()
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_update()\n"));

	TBIA();		/* XXX should not be here. */
}

/*
 * pmap_collect:		[ INTERFACE ]
d1578 2
a1579 6
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
d1581 1
a1581 3
void
pmap_collect(pmap)
	pmap_t		pmap;
d1583 3
a1585 29

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_collect(%p)\n", pmap));

	if (pmap == pmap_kernel()) {
		int bank, s;

		/*
		 * XXX This is very bogus.  We should handle kernel PT
		 * XXX pages much differently.
		 */

		s = splimp();
		for (bank = 0; bank < vm_nphysseg; bank++)
			pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			    ptoa(vm_physmem[bank].end));
		splx(s);
	} else {
		/*
		 * This process is about to be swapped out; free all of
		 * the PT pages by removing the physical mappings for its
		 * entire address space.  Note: pmap_remove() performs
		 * all necessary locking.
		 */
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
	}

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
d1587 1
d1591 9
a1599 7
 * pmap_collect1:
 *
 *	Garbage-collect KPT pages.  Helper for the above (bogus)
 *	pmap_collect().
 *
 *	Note: THIS SHOULD GO AWAY, AND BE REPLACED WITH A BETTER
 *	WAY OF HANDLING PT PAGES!
d1602 1
a1602 1
pmap_collect1(pmap, startpa, endpa)
a1603 1
	paddr_t		startpa, endpa;
d1605 1
a1605 1
	paddr_t pa;
d1608 3
a1610 1
	paddr_t kpa;
d1615 2
d1618 9
a1626 1
	for (pa = startpa; pa < endpa; pa += NBPG) {
d1643 2
a1644 2
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + HP_MAX_PTSIZE)
d1673 1
a1673 1
		pmap_extract(pmap, pv->pv_va, &kpa);
d1696 4
d1713 1
d1717 10
a1726 9
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and using bzero to clear its contents, one
 *	machine dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
d1730 1
a1730 1
	paddr_t phys;
d1732 2
a1733 1
	int s, npte;
d1735 3
a1737 11
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%lx)\n", phys));

	npte = phys | PG_V;
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Cache-inhibit the mapping on VAC machines, as we would
		 * be wasting the cache load.
		 */
		npte |= PG_CI;
	}
d1739 7
d1747 19
a1765 17
#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		/*
		 * Set copyback caching on the page; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte |= PG_CCB;
	}
#endif

	s = splimp();

	*caddr1_pte = npte;
	TBIS((vaddr_t)CADDR1);

	zeropage(CADDR1);
d1768 2
a1769 2
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);
d1771 8
a1778 2

	splx(s);
d1782 8
a1789 1
 * pmap_copy_page:		[ INTERFACE ]
d1791 3
a1793 7
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using bcopy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
d1796 4
a1799 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d1801 16
a1816 1
	int s, npte1, npte2;
d1818 19
a1836 13
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_copy_page(%lx, %lx)\n", src, dst));

	npte1 = src | PG_RO | PG_V;
	npte2 = dst | PG_V;
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Cache-inhibit the mapping on VAC machines, as we would
		 * be wasting the cache load.
		 */
		npte1 |= PG_CI;
		npte2 |= PG_CI;
	}
a1837 3

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
d1839 1
a1839 3
		 * Set copyback caching on the pages; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
d1841 1
a1841 15
		npte1 |= PG_CCB;
		npte2 |= PG_CCB;
	}
#endif

	s = splimp();

	*caddr1_pte = npte1;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = npte2;
	TBIS((vaddr_t)CADDR2);

	copypage(CADDR1, CADDR2);

d1843 10
a1852 5
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = PG_NV;
	TBIS((vaddr_t)CADDR2);
d1854 1
a1854 2

	splx(s);
a1857 2
 * pmap_clear_modify:		[ INTERFACE ]
 *
a1859 7
#ifdef PMAP_NEW
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;
a1860 7
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));

	rv = pmap_testbit(pa, PG_M);
	pmap_changebit(pa, 0, ~PG_M);
	return rv;
}
#else
d1863 1
a1863 1
	paddr_t	pa;
d1865 5
a1869 4

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_M);
a1870 1
#endif
d1873 1
a1873 1
 * pmap_clear_reference:	[ INTERFACE ]
a1876 7
#ifdef PMAP_NEW
boolean_t
pmap_clear_reference(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;
d1878 2
a1879 10
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));

	rv = pmap_testbit(pa, PG_U);
	pmap_changebit(pa, 0, ~PG_U);
	return rv;
}
#else
void
pmap_clear_reference(pa)
	paddr_t	pa;
d1881 5
a1885 4

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_U);
a1886 1
#endif
d1889 1
a1889 1
 * pmap_is_referenced:		[ INTERFACE ]
d1894 1
a1895 6
#ifdef PMAP_NEW
pmap_is_referenced(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#else
d1897 1
a1897 1
	paddr_t	pa;
a1898 1
#endif
d1910 1
a1910 1
 * pmap_is_modified:		[ INTERFACE ]
d1915 1
a1916 6
#ifdef PMAP_NEW
pmap_is_modified(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#else
d1918 1
a1918 1
	paddr_t	pa;
a1919 1
#endif
d1930 1
a1930 10
/*
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
a1936 26
#ifdef M68K_MMU_HP
/*
 * pmap_prefer:			[ INTERFACE ]
 *
 *	Find the first virtual address >= *vap that does not
 *	cause a virtually-tagged cache alias problem.
 */
void
pmap_prefer(foff, vap)
	vaddr_t foff, *vap;
{
	vaddr_t va;
	vsize_t d;

#ifdef M68K_MMU_MOTOROLA
	if (pmap_aliasmask)
#endif
	{
		va = *vap;
		d = foff - va;
		d &= pmap_aliasmask;
		*vap = va + d;
	}
}
#endif /* M68K_MMU_HP */

d1939 3
a1941 5
 * pmap_mapmulti:
 *
 *	'PUX hack for dealing with the so called multi-mapped address space.
 *	The first 256mb is mapped in at every 256mb region from 0x10000000
 *	up to 0xF0000000.  This allows for 15 bits of tag information.
d1943 2
a1944 2
 *	We implement this at the segment table level, the machine independent
 *	VM knows nothing about it.
d1949 1
a1949 1
	vaddr_t va;
d1971 1
a1971 1
#endif /* COMPAT_HPUX */
d1978 4
a1981 13
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
 *
 *	If (flags & PRM_KEEPPTPAGE), we don't free the page table page
 *	if the reference drops to zero.
d1987 1
a1987 1
	vaddr_t va;
d1991 1
a1991 1
	paddr_t pa;
d1998 4
a2003 4
	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    pmap, va, pte, flags));

d2019 3
d2029 3
d2039 3
d2052 4
a2055 1
	PMAP_DPRINTF(PDB_REMOVE, ("remove: invalidating pte at %p\n", pte));
d2062 4
a2065 1
	 * the PT page.
d2068 2
a2069 2
		vaddr_t ptpva = trunc_page((vaddr_t)pte);
		int refs = pmap_ptpage_delref(ptpva);
d2072 1
a2072 1
			pmap_check_wiring("remove", ptpva);
a2073 36
		/*
		 * If reference count drops to 1, and we're not instructed
		 * to keep it around, free the PT page.
		 *
		 * Note: refcnt == 1 comes from the fact that we allocate
		 * the page with uvm_fault_wire(), which initially wires
		 * the page.  The first reference we actually add causes
		 * the refcnt to be 2.
		 */
		if (refs == 1 && (flags & PRM_KEEPPTPAGE) == 0) {
			struct pv_entry *pv;
			paddr_t pa;

			pa = pmap_pte_pa(pmap_pte(pmap_kernel(), ptpva));
#ifdef DIAGNOSTIC
			if (PAGE_IS_MANAGED(pa) == 0)
				panic("pmap_remove_mapping: unmanaged PT page");
#endif
			pv = pa_to_pvh(pa);
#ifdef DIAGNOSTIC
			if (pv->pv_ptste == NULL)
				panic("pmap_remove_mapping: ptste == NULL");
			if (pv->pv_pmap != pmap_kernel() ||
			    pv->pv_va != ptpva ||
			    pv->pv_next != NULL)
				panic("pmap_remove_mapping: "
				    "bad PT page pmap %p, va 0x%lx, next %p",
				    pv->pv_pmap, pv->pv_va, pv->pv_next);
#endif
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
			    NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(PHYS_TO_VM_PAGE(pa));
			PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
			    ("remove: PT page 0x%lx (0x%lx) freed\n",
			    ptpva, pa));
		}
d2078 1
a2078 1
	if (PAGE_IS_MANAGED(pa) == 0)
d2103 3
d2108 3
d2131 4
a2134 2
		PMAP_DPRINTF(PDB_CACHE,
		    ("remove: clearing CI for pa %lx\n", pa));
d2136 1
a2136 1
		pmap_changebit(pa, 0, ~PG_CI);
d2149 8
a2156 3
		PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
		    ("remove: ste was %x@@%p pte was %x@@%p\n",
		    *ste, ste, opte, pmap_pte(pmap, va)));
a2174 3
			PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
			    ("remove: stab %p, refcnt %d\n",
			    ptpmap->pm_stab, ptpmap->pm_sref - 1));
d2176 3
d2180 1
a2180 1
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
d2184 8
a2191 6
				PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
				    ("remove: free stab %p\n",
				    ptpmap->pm_stab));
				uvm_km_free_wakeup(st_map,
						(vaddr_t)ptpmap->pm_stab,
						HP_STSIZE);
d2198 1
d2205 2
a2206 1
					PMAP_ACTIVATE(ptpmap, 1);
d2229 1
a2229 1
	*pa_to_attribute(pa) |= bits;
a2232 5
/*
 * pmap_testbit:
 *
 *	Test the modified/referenced bits of a physical page.
 */
d2236 1
a2236 1
	paddr_t pa;
d2243 1
a2243 1
	if (PAGE_IS_MANAGED(pa) == 0)
d2251 1
a2251 1
	if (*pa_to_attribute(pa) & bit) {
d2263 2
a2264 2
	 * Not found.  Check current mappings, returning immediately if
	 * found.  Cache a hit to speed future lookups.
a2269 1
				*pa_to_attribute(pa) |= bit;
a2278 6
/*
 * pmap_changebit:
 *
 *	Change the modified/referenced bits, or other PTE bits,
 *	for a physical page.
 */
d2281 4
a2284 3
pmap_changebit(pa, set, mask)
	paddr_t pa;
	int set, mask;
d2288 1
a2288 1
	vaddr_t va;
d2293 3
d2297 6
a2302 4
	PMAP_DPRINTF(PDB_BITS,
	    ("pmap_changebit(%lx, %x, %x)\n", pa, set, mask));

	if (PAGE_IS_MANAGED(pa) == 0)
d2305 7
a2313 1

d2317 2
a2318 2
	*pa_to_attribute(pa) &= mask;

d2336 4
a2339 2
			if (set == PG_RO) {
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
d2354 4
a2357 1
			npte = (*pte | set) & mask;
d2365 3
a2367 4
				if (firstpage && (mmutype == MMU_68040) &&
				    ((set == PG_RO) ||
				     (set & PG_CMASK) ||
				     (mask & PG_CMASK) == 0)) {
d2376 6
d2383 8
d2392 10
a2405 5
/*
 * pmap_enter_ptpage:
 *
 *	Allocate and map a PT page for the specified pmap/va pair.
 */
d2410 1
a2410 1
	vaddr_t va;
d2412 1
a2412 1
	paddr_t ptpa;
d2417 7
a2423 3
	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE,
	    ("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va));

d2433 3
a2435 3
			uvm_km_zalloc(st_map, HP_STSIZE);
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab, 
			(paddr_t *)&pmap->pm_stpa);
d2441 1
a2441 1
			pmap_changebit((paddr_t)pmap->pm_stpa, 0, ~PG_CCB);
d2445 1
d2451 6
a2456 5
			PMAP_ACTIVATE(pmap, 1);

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: pmap %p stab %p(%p)\n",
		    pmap, pmap->pm_stab, pmap->pm_stpa));
d2477 4
a2480 3

			PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
			    ("enter: alloc ste2 %d(%p)\n", ix, addr));
d2492 5
a2496 3

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: ste2 %p (%p)\n", pmap_ste2(pmap, va), ste));
d2499 1
a2499 1
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));
d2515 4
a2518 2
			PMAP_DPRINTF(PDB_COLLECT,
			    ("enter: no KPT pages, collecting...\n"));
d2523 4
d2533 1
a2533 1
		    VM_PROT_DEFAULT);
a2546 5
	 *
	 * Note we use a wire-fault to keep the page off the paging
	 * queues.  This sets our PT page's reference (wire) count to
	 * 1, which is what we use to check if the page can be freed.
	 * See pmap_remove_mapping().
d2554 5
a2558 4
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
		    ("enter: about to fault UPT pg at %lx\n", va));
		s = uvm_fault_wire(pt_map, va, va + PAGE_SIZE,
		    VM_PROT_READ|VM_PROT_WRITE);
d2560 2
a2561 3
			printf("uvm_fault_wire(pt_map, 0x%lx, 0x%lx, RW) "
			    "-> %d\n", va, va + PAGE_SIZE, s);
			panic("pmap_enter: uvm_fault_wire failed");
d2563 10
a2572 1
		ptpa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
d2590 1
a2590 1
		pmap_changebit(ptpa, 0, ~PG_CCB);
d2613 4
a2616 3

	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
	    ("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste));
d2638 5
a2642 3
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: stab %p refcnt %d\n",
		    pmap->pm_stab, pmap->pm_sref));
a2656 36
/*
 * pmap_ptpage_addref:
 *
 *	Add a reference to the specified PT page.
 */
void
pmap_ptpage_addref(ptpva)
	vaddr_t ptpva;
{
	vm_page_t m;

	simple_lock(&uvm.kernel_object->vmobjlock);
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	m->wire_count++;
	simple_unlock(&uvm.kernel_object->vmobjlock);
}

/*
 * pmap_ptpage_delref:
 *
 *	Delete a reference to the specified PT page.
 */
int
pmap_ptpage_delref(ptpva)
	vaddr_t ptpva;
{
	vm_page_t m;
	int rv;

	simple_lock(&uvm.kernel_object->vmobjlock);
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	rv = --m->wire_count;
	simple_unlock(&uvm.kernel_object->vmobjlock);
	return (rv);
}

a2657 5
/*
 * pmap_pvdump:
 *
 *	Dump the contents of the PV list for the specified physical page.
 */
d2661 1
a2661 1
	paddr_t pa;
a2672 7
/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
d2677 1
a2677 1
	vaddr_t va;
d2679 2
a2681 3
	paddr_t pa;
	vm_page_t m;
	int count;
d2683 1
d2688 2
a2689 4
	pa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
	m = PHYS_TO_VM_PAGE(pa);
	if (m->wire_count < 1) {
		printf("*%s*: 0x%lx: wire count %d\n", str, va, m->wire_count);
a2691 1

d2696 3
a2698 3
	if ((m->wire_count - 1) != count)
		printf("*%s*: 0x%lx: w%d/a%d\n",
		       str, va, (m->wire_count - 1), count);
d2700 1
a2700 1
#endif /* DEBUG */
@


1.14.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14.4.2 2001/07/04 10:15:47 niklas Exp $	*/
d146 1
a149 1
#include <uvm/uvm_extern.h>
d696 1
a696 1
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
d710 1
d724 23
d998 1
d1004 6
d1164 2
a1165 2
int
pmap_enter(pmap, va, pa, prot, flags)
d1170 2
a1171 1
	int flags;
a1177 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d1183 3
d1354 2
a1355 2
		if ((flags & VM_PROT_ALL) & ~prot)
			panic("pmap_enter: access type exceeds prot");
d1357 1
a1357 1
		if (flags & VM_PROT_WRITE)
d1359 1
a1359 1
		else if (flags & VM_PROT_ALL)
a1450 2

	return (KERN_SUCCESS);
d1453 1
d1460 1
a1460 1
	pmap_enter(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d1473 1
a1473 1
				VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED);
d1486 1
d1889 1
d1903 11
d1920 1
d1934 11
d1953 1
d1958 5
d1980 1
d1985 5
d2614 2
a2615 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT,
		    VM_PROT_DEFAULT|PMAP_WIRED);
@


1.14.4.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d134 2
d145 3
d149 1
d389 1
a389 1
		    NULL, UVM_UNKNOWN_OFFSET, 0,
d396 1
a396 1
		    NULL, UVM_UNKNOWN_OFFSET, 0,
d470 1
a470 1
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
@


1.14.4.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14.4.4 2001/11/13 21:00:51 niklas Exp $	*/
@


1.13
log
@pmap_activate() and pmap_deactivate() are MD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1999/01/11 05:11:20 millert Exp $	*/
d649 1
a649 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE);
d1134 1
a1134 1
pmap_enter(pmap, va, pa, prot, wired)
d1140 1
d1738 2
a1739 1
	pmap_enter(pmap_kernel(), kva, phys, VM_PROT_READ|VM_PROT_WRITE, TRUE);
d1771 3
a1773 2
	pmap_enter(pmap_kernel(), skva, src, VM_PROT_READ, TRUE);
	pmap_enter(pmap_kernel(), dkva, dst, VM_PROT_READ|VM_PROT_WRITE, TRUE);
d2530 2
a2531 1
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE);
@


1.12
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1999/07/18 16:23:46 deraadt Exp $	*/
d297 2
d807 3
a809 2
pmap_activate(p)
	struct proc *p;
a810 2
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d824 3
a826 2
pmap_deactivate(p)
	struct proc *p;
@


1.11
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1999/01/11 05:11:20 millert Exp $	*/
a296 2
void	pmap_activate __P((pmap_t));
void	pmap_deactivate __P((pmap_t));
d805 2
a806 2
pmap_activate(pmap)
	pmap_t pmap;
d809 1
d823 2
a824 2
pmap_deactivate(pmap)
	pmap_t pmap;
@


1.10
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1997/11/06 19:42:38 millert Exp $	*/
d297 2
a298 2
void	pmap_activate __P((pmap_t, struct pcb *));
void	pmap_deactivate __P((pmap_t, struct pcb *));
d807 1
a807 1
pmap_activate(pmap, pcb)
a808 1
	struct pcb *pcb;
d810 1
d824 1
a824 1
pmap_deactivate(pmap, pcb)
a825 1
	struct pcb *pcb;
@


1.9
log
@Idnetify/support hp385 (33Mhz 68040)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1997/07/06 08:02:07 downsj Exp $	*/
d388 1
a388 1
		panic("pmap_init: bogons in the VM system!\n");
@


1.8
log
@Sync with changes from NetBSD, up to 970705.

This includes all of the new frodo drivers, from Michael Smith, hacked up
by Jason R. Thorpe; other changes, mostly configuration and/or m68k
abstraction related, from Thorpe.  A few others of my own.

This compiles and runs, have not yet tested Domain kbd or apci ports.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 1997/04/16 11:56:30 downsj Exp $	*/
d50 1
a50 1
 *	68040 with on-chip MMU	models 380, 425, 433
@


1.7
log
@Merge portions of NetBSD up to 970415.

This includes:
	* All of the prototyping and KNF changes.
	* The new interrupt system.
	* Portions of the Domain Keyboard support from Mike Smith.

This does not include:
	* m68k changes, namely generic dumps.

Various sundry changes and fixes by me.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.6 1997/03/26 08:32:44 downsj Exp $	*/
/*	$NetBSD: pmap.c,v 1.32 1997/04/02 22:41:39 scottr Exp $	*/
d98 2
d374 1
a374 1
			   &addr, hp300_ptob(IIOMAPSIZE+EIOMAPSIZE), FALSE);
d1174 1
a1174 1
	pa = hp300_trunc_page(pa);
d1929 1
a1929 1
	return(hp300_ptob(ppn));
d2285 1
d2287 1
@


1.6
log
@Merge changes from NetBSD, up to 3/19/97; undoes some local changes.
Changed to match new arch/m68k code.
genassym.cf is currently just a place holder.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.5 1997/02/24 01:16:09 downsj Exp $	*/
/*	$NetBSD: pmap.c,v 1.30 1997/03/18 14:13:55 mycroft Exp $	*/
d292 9
d304 1
a304 1
void pmap_remove_mapping __P((pmap_t, vm_offset_t, pt_entry_t *, int));
d306 3
a308 2
void pmap_changebit	__P((vm_offset_t, int, boolean_t));
void pmap_enter_ptpage	__P((pmap_t, vm_offset_t));
d364 1
a364 1
		printf("pmap_init(%x, %x)\n", phys_start, phys_end);
d390 1
a390 1
		printf("pmap_init: Sysseg %x, Sysmap %x, Sysptmap %x\n",
d392 1
a392 1
		printf("  pstart %x, pend %x, vstart %x, vend %x\n",
d413 1
a413 1
		printf("pmap_init: %x bytes: npages %x s0 %x(%x) tbl %x atr %x\n",
d456 1
a456 1
		printf("pmap_init: KPT: %d pages from %x to %x\n",
d495 1
a495 1
		printf("pmap_init: pt_map [%x - %x)\n", addr, addr2);
d552 1
a552 2
	register struct pv_page *pvp;
	register int i;
d643 1
a643 1
		printf("pmap_map(%x, %x, %x, %x)\n", va, spa, epa, prot);
d670 1
a670 1
	register pmap_t pmap;
d674 1
a674 1
		printf("pmap_create(%x)\n", size);
d700 1
a700 1
	register struct pmap *pmap;
d705 1
a705 1
		printf("pmap_pinit(%x)\n", pmap);
d732 1
a732 1
	register pmap_t pmap;
d741 1
a741 1
		printf("pmap_destroy(%x)\n", pmap);
d760 1
a760 1
	register struct pmap *pmap;
d765 1
a765 1
		printf("pmap_release(%x)\n", pmap);
d796 1
a796 1
		printf("pmap_reference(%x)\n", pmap);
d806 1
a806 1
	register pmap_t pmap;
d815 1
a815 1
		printf("pmap_activate(%x, %x)\n", pmap, pcb);
d823 1
a823 1
	register pmap_t pmap;
d836 2
a837 2
	register pmap_t pmap;
	register vm_offset_t sva, eva;
d839 2
a840 2
	register vm_offset_t nssva;
	register pt_entry_t *pte;
d846 1
a846 1
		printf("pmap_remove(%x, %x, %x)\n", pmap, sva, eva);
d958 1
a958 1
	register struct pv_entry *pv;
d963 2
a964 2
	    prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE))
		printf("pmap_page_protect(%x, %x)\n", pa, prot);
d985 1
a985 1
		register pt_entry_t *pte;
d1000 1
a1000 1
				printf("%s wired mapping for %x not removed\n",
d1014 2
a1015 2
	register pmap_t	pmap;
	register vm_offset_t sva, eva;
d1018 2
a1019 2
	register vm_offset_t nssva;
	register pt_entry_t *pte;
d1025 1
a1025 1
		printf("pmap_protect(%x, %x, %x, %x)\n", pmap, sva, eva, prot);
d1133 1
a1133 1
	register pmap_t pmap;
d1135 1
a1135 1
	register vm_offset_t pa;
d1139 2
a1140 2
	register pt_entry_t *pte;
	register int npte;
d1147 1
a1147 1
		printf("pmap_enter(%x, %x, %x, %x, %x)\n",
d1177 1
a1177 1
		printf("enter: pte %x, *pte %x\n", pte, *pte);
d1229 1
a1229 1
			printf("enter: removing old mapping %x\n", va);
d1252 1
a1252 1
		register struct pv_entry *pv, *npv;
d1262 1
a1262 1
			printf("enter: pv at %x: %x/%x/%x\n",
d1328 1
a1328 1
					printf("enter: pa %x already CI'ed\n",
d1340 1
a1340 1
					printf("enter: pa %x CI'ing all\n",
d1469 1
a1469 1
	register pmap_t	pmap;
d1473 1
a1473 1
	register pt_entry_t *pte;
d1477 1
a1477 1
		printf("pmap_change_wiring(%x, %x, %x)\n", pmap, va, wired);
d1491 1
a1491 1
			printf("pmap_change_wiring: invalid STE for %x\n", va);
d1500 1
a1500 1
			printf("pmap_change_wiring: invalid PTE for %x\n", va);
d1526 1
a1526 1
	register pmap_t	pmap;
d1529 1
a1529 1
	register vm_offset_t pa;
d1533 1
a1533 1
		printf("pmap_extract(%x, %x) -> ", pmap, va);
d1542 1
a1542 1
		printf("%x\n", pa);
d1563 1
a1563 1
		printf("pmap_copy(%x, %x, %x, %x, %x)\n",
d1600 3
a1602 3
	register vm_offset_t pa;
	register struct pv_entry *pv;
	register pt_entry_t *pte;
d1608 1
a1608 1
	int opmapdebug;
d1615 1
a1615 1
		printf("pmap_collect(%x)\n", pmap);
d1622 1
a1622 1
		register struct kpt_page *kpt, **pkpt;
d1634 1
a1634 1
		} while (pv = pv->pv_next);
d1655 1
a1655 1
			printf("collect: freeing KPT page at %x (ste %x@@%x)\n",
d1685 1
a1685 1
			printf("collect: %x (%x) to free list\n",
d1700 1
a1700 1
			printf("collect: kernel STE at %x still valid (%x)\n",
d1704 1
a1704 1
			printf("collect: kernel PTmap at %x still valid (%x)\n",
d1727 1
a1727 1
	register vm_offset_t kva;
d1732 1
a1732 1
		printf("pmap_zero_page(%x)\n", phys);
d1758 1
a1758 1
	register vm_offset_t skva, dkva;
d1763 1
a1763 1
		printf("pmap_copy_page(%x, %x)\n", src, dst);
d1796 1
a1796 1
		printf("pmap_pageable(%x, %x, %x, %x)\n",
d1808 2
a1809 2
		register struct pv_entry *pv;
		register vm_offset_t pa;
d1813 1
a1813 1
			printf("pmap_pageable(%x, %x, %x, %x)\n",
d1826 1
a1826 1
			printf("pmap_pageable: bad PT page va %x next %x\n",
d1837 1
a1837 1
			printf("pa %x: flags=%x: not clean\n",
d1842 1
a1842 1
			printf("pmap_pageable: PT page %x(%x) unmodified\n",
d1860 1
a1860 1
		printf("pmap_clear_modify(%x)\n", pa);
d1876 1
a1876 1
		printf("pmap_clear_reference(%x)\n", pa);
d1895 1
a1895 1
		printf("pmap_is_referenced(%x) -> %c\n", pa, "FT"[rv]);
d1916 1
a1916 1
		printf("pmap_is_modified(%x) -> %c\n", pa, "FT"[rv]);
d1939 1
d1949 1
a1949 1
		printf("pmap_mapmulti(%x, %x): bste %x(%x)",
d1952 1
a1952 1
		printf(" ste %x(%x)\n", ste, *ste);
d1979 3
a1981 3
	register pmap_t pmap;
	register vm_offset_t va;
	register pt_entry_t *pte;
d1984 2
a1985 2
	register vm_offset_t pa;
	register struct pv_entry *pv, *npv;
d1993 1
a1993 1
		printf("pmap_remove_mapping(%x, %x, %x, %x)\n",
d2047 1
a2047 1
		printf("remove: invalidating pte at %x\n", pte);
d2126 1
a2126 1
			printf("remove: clearing CI for pa %x\n", pa);
d2147 1
a2147 1
			printf("remove: ste was %x@@%x pte was %x@@%x\n",
d2170 1
a2170 1
				printf("remove: stab %x, refcnt %d\n",
d2179 1
a2179 1
					printf("remove: free stab %x\n",
d2229 1
a2229 1
	register vm_offset_t pa;
d2232 2
a2233 2
	register struct pv_entry *pv;
	register pt_entry_t *pte;
d2275 1
a2275 1
	register vm_offset_t pa;
d2279 2
a2280 2
	register struct pv_entry *pv;
	register pt_entry_t *pte, npte;
d2290 1
a2290 1
		printf("pmap_changebit(%x, %x, %s)\n",
d2357 2
a2358 2
				    (bit == PG_RO && setem ||
				     (bit & PG_CMASK))) {
d2400 2
a2401 2
	register pmap_t pmap;
	register vm_offset_t va;
d2403 2
a2404 2
	register vm_offset_t ptpa;
	register struct pv_entry *pv;
d2410 1
a2410 1
		printf("pmap_enter_ptpage: pmap %x, va %x\n", pmap, va);
d2445 1
a2445 1
			printf("enter: pmap %x stab %x(%x)\n",
d2470 1
a2470 1
				printf("enter: alloc ste2 %d(%x)\n", ix, addr);
d2485 1
a2485 1
			printf("enter: ste2 %x (%x)\n",
d2498 1
a2498 1
		register struct kpt_page *kpt;
d2528 1
a2528 1
			printf("enter: add &Sysptmap[%d]: %x (KPT page %x)\n",
d2546 1
a2546 1
			printf("enter: about to fault UPT pg at %x\n", va);
d2550 1
a2550 1
			printf("vm_fault(pt_map, %x, RW, 0) -> %d\n", va, s);
d2573 1
a2574 1
#ifdef DEBUG
d2576 1
a2576 1
			printf("%s PT no CCB: kva=%x ptpa=%x pte@@%x=%x\n",
d2595 1
a2595 1
		} while (pv = pv->pv_next);
d2605 1
a2605 1
		printf("enter: new PT page at PA %x, ste at %x\n", ptpa, ste);
d2630 1
a2630 1
			printf("enter: stab %x refcnt %d\n",
d2653 1
a2653 1
	register struct pv_entry *pv;
d2655 1
a2655 1
	printf("pa %x", pa);
d2657 1
a2657 1
		printf(" -> pmap %x, va %x, ptste %x, ptpmap %x, flags %x",
d2670 2
a2671 2
	register int count;
	register pt_entry_t *pte;
d2679 1
a2679 1
		printf("wired_check: entry for %x not found\n", va);
d2687 1
a2687 1
		printf("*%s*: %x: w%d/a%d\n",
@


1.5
log
@Add support for COMPAT_SUNOS and enable it.

hp300 now runs sun3 SunOS executables!
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.4 1997/02/10 11:13:32 downsj Exp $	*/
/*	$NetBSD: pmap.c,v 1.28 1997/02/02 08:01:32 thorpej Exp $	*/
d911 1
a911 2
	if (pmap_aliasmask &&
	    (pmap == pmap_kernel() || pmap != curproc->p_vmspace->vm_map.pmap))
d2010 1
a2010 2
		if (pmap == curproc->p_vmspace->vm_map.pmap &&
		    !pmap_pte_ci(pte)) {
d2187 1
a2187 2
				if (curproc != NULL &&
				    ptpmap == curproc->p_vmspace->vm_map.pmap)
d2431 1
a2431 1
		if (pmap == curproc->p_vmspace->vm_map.pmap)
@


1.4
log
@hp300 portion of the copypage/zeropage/mappedcopy changes from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 1997/01/12 15:13:25 downsj Exp $	*/
d1379 9
@


1.3
log
@Mostly sync to NetBSD-current of 011196.  Compiles, links, boots to single
user.

KNOWN NOT WORKING:
	* DIO-II devices
	* Dynamic executables

NOT EVEN TRIED:
	* New config support
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.c,v 1.27 1996/10/13 03:14:32 christos Exp $	*/
d1719 1
a1719 1
	bzero((caddr_t)kva, NBPG);
d1754 1
a1754 1
	pmap_remove(pmap_kernel(), skva, skva+2*NBPG);
@


1.2
log
@update from netbsd (verbatim)
@
text
@d1 2
a2 1
/*	$NetBSD: pmap.c,v 1.24 1995/12/11 17:09:14 thorpej Exp $	*/
d48 1
a48 1
 *	68020 with 68551 MMU	models 318, 319, 330 (all untested)
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.22 1995/10/08 19:33:36 thorpej Exp $	*/
d179 1
a179 1
#ifdef HAVEVAC
d187 1
a187 1
#if defined(HP380)
d198 1
a198 1
#if defined(HP380)
d284 1
a284 1
#ifdef HAVEVAC
d287 1
a287 1
#if defined(HP380)
d460 11
a470 1
	s = min(HP_PTMAXSIZE, maxproc*HP_MAX_PTSIZE);
d487 1
a487 1
#if defined(HP380)
d706 1
a706 1
#if defined(HP380)
d866 1
a866 1
#ifdef HAVEVAC
d901 1
a901 1
#ifdef HAVEVAC
d1054 1
a1054 1
#ifdef HAVEVAC
d1066 1
a1066 1
#if defined(HP380)
d1098 1
a1098 1
#if defined(HAVEVAC) && defined(DEBUG)
d1292 1
a1292 1
#ifdef HAVEVAC
d1364 1
a1364 1
#ifdef HAVEVAC
d1380 1
a1380 1
#if defined(HP380)
d1396 1
a1396 1
#if defined(HP380)
d1405 1
a1405 1
#ifdef HAVEVAC
d1986 1
a1986 1
#ifdef HAVEVAC
d2100 1
a2100 1
#ifdef HAVEVAC
d2132 1
a2132 1
#if defined(HP380)
d2169 1
a2169 1
#if defined(HP380)
d2231 1
a2231 1
#ifdef HAVEVAC
d2318 1
a2318 1
#ifdef HAVEVAC
d2333 1
a2333 1
#if defined(HP380)
d2366 1
a2366 1
#if defined(HAVEVAC) && defined(DEBUG)
d2410 1
a2410 1
#if defined(HP380)
d2434 1
a2434 1
#if defined(HP380)
d2547 1
a2547 1
#if defined(HP380)
d2599 1
a2599 1
#if defined(HP380)
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
