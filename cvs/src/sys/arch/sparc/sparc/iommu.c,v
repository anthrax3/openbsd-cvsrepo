head	1.29;
access;
symbols
	OPENBSD_6_0:1.29.0.6
	OPENBSD_6_0_BASE:1.29
	OPENBSD_5_9:1.29.0.2
	OPENBSD_5_9_BASE:1.29
	OPENBSD_5_8:1.29.0.4
	OPENBSD_5_8_BASE:1.29
	OPENBSD_5_7:1.28.0.2
	OPENBSD_5_7_BASE:1.28
	OPENBSD_5_6:1.27.0.4
	OPENBSD_5_6_BASE:1.27
	OPENBSD_5_5:1.25.0.18
	OPENBSD_5_5_BASE:1.25
	OPENBSD_5_4:1.25.0.14
	OPENBSD_5_4_BASE:1.25
	OPENBSD_5_3:1.25.0.12
	OPENBSD_5_3_BASE:1.25
	OPENBSD_5_2:1.25.0.10
	OPENBSD_5_2_BASE:1.25
	OPENBSD_5_1_BASE:1.25
	OPENBSD_5_1:1.25.0.8
	OPENBSD_5_0:1.25.0.6
	OPENBSD_5_0_BASE:1.25
	OPENBSD_4_9:1.25.0.4
	OPENBSD_4_9_BASE:1.25
	OPENBSD_4_8:1.25.0.2
	OPENBSD_4_8_BASE:1.25
	OPENBSD_4_7:1.22.0.2
	OPENBSD_4_7_BASE:1.22
	OPENBSD_4_6:1.21.0.4
	OPENBSD_4_6_BASE:1.21
	OPENBSD_4_5:1.20.0.8
	OPENBSD_4_5_BASE:1.20
	OPENBSD_4_4:1.20.0.6
	OPENBSD_4_4_BASE:1.20
	OPENBSD_4_3:1.20.0.4
	OPENBSD_4_3_BASE:1.20
	OPENBSD_4_2:1.20.0.2
	OPENBSD_4_2_BASE:1.20
	OPENBSD_4_1:1.19.0.8
	OPENBSD_4_1_BASE:1.19
	OPENBSD_4_0:1.19.0.6
	OPENBSD_4_0_BASE:1.19
	OPENBSD_3_9:1.19.0.4
	OPENBSD_3_9_BASE:1.19
	OPENBSD_3_8:1.19.0.2
	OPENBSD_3_8_BASE:1.19
	OPENBSD_3_7:1.18.0.10
	OPENBSD_3_7_BASE:1.18
	OPENBSD_3_6:1.18.0.8
	OPENBSD_3_6_BASE:1.18
	SMP_SYNC_A:1.18
	SMP_SYNC_B:1.18
	OPENBSD_3_5:1.18.0.6
	OPENBSD_3_5_BASE:1.18
	OPENBSD_3_4:1.18.0.4
	OPENBSD_3_4_BASE:1.18
	UBC_SYNC_A:1.18
	OPENBSD_3_3:1.18.0.2
	OPENBSD_3_3_BASE:1.18
	OPENBSD_3_2:1.16.0.4
	OPENBSD_3_2_BASE:1.16
	OPENBSD_3_1:1.16.0.2
	OPENBSD_3_1_BASE:1.16
	UBC_SYNC_B:1.18
	UBC:1.13.0.2
	UBC_BASE:1.13
	OPENBSD_3_0:1.10.0.2
	OPENBSD_3_0_BASE:1.10
	OPENBSD_2_9_BASE:1.6
	OPENBSD_2_9:1.6.0.8
	OPENBSD_2_8:1.6.0.6
	OPENBSD_2_8_BASE:1.6
	OPENBSD_2_7:1.6.0.4
	OPENBSD_2_7_BASE:1.6
	SMP:1.6.0.2
	SMP_BASE:1.6
	kame_19991208:1.5
	OPENBSD_2_6:1.4.0.4
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.4.0.2
	OPENBSD_2_5_BASE:1.4
	OPENBSD_2_4:1.3.0.6
	OPENBSD_2_4_BASE:1.3
	OPENBSD_2_3:1.3.0.4
	OPENBSD_2_3_BASE:1.3
	OPENBSD_2_2:1.3.0.2
	OPENBSD_2_2_BASE:1.3
	OPENBSD_2_1:1.2.0.2
	OPENBSD_2_1_BASE:1.2
	OPENBSD_2_0:1.1.0.2
	OPENBSD_2_0_BASE:1.1;
locks; strict;
comment	@ * @;


1.29
date	2015.03.30.20.30.22;	author miod;	state Exp;
branches;
next	1.28;
commitid	f66FukLLgPJs9j5H;

1.28
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.27;
commitid	yv0ECmCdICvq576h;

1.27
date	2014.07.11.09.36.26;	author mpi;	state Exp;
branches;
next	1.26;
commitid	vsYjSRfS3Y783BvW;

1.26
date	2014.04.08.13.23.51;	author mpi;	state Exp;
branches;
next	1.25;

1.25
date	2010.07.10.19.32.25;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2010.06.17.16.11.19;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2010.04.21.03.03.26;	author deraadt;	state Exp;
branches;
next	1.22;

1.22
date	2009.07.13.19.50.00;	author kettenis;	state Exp;
branches;
next	1.21;

1.21
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.20;

1.20
date	2007.05.29.09.53.59;	author sobrado;	state Exp;
branches;
next	1.19;

1.19
date	2005.03.29.11.33.18;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2002.10.07.18.35.56;	author mickey;	state Exp;
branches;
next	1.17;

1.17
date	2002.10.06.22.06.15;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2002.03.14.20.30.00;	author jason;	state Exp;
branches;
next	1.15;

1.15
date	2002.03.14.01.26.44;	author millert;	state Exp;
branches;
next	1.14;

1.14
date	2002.02.21.19.55.13;	author jason;	state Exp;
branches;
next	1.13;

1.13
date	2001.12.08.02.24.07;	author art;	state Exp;
branches
	1.13.2.1;
next	1.12;

1.12
date	2001.11.22.09.47.37;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.06.19.53.16;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2001.09.19.20.50.57;	author mickey;	state Exp;
branches;
next	1.9;

1.9
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.06.08.08.09.27;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.05.10.10.34.46;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2000.01.01.19.41.00;	author deraadt;	state Exp;
branches
	1.6.2.1;
next	1.5;

1.5
date	99.11.05.18.07.11;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.01.11.05.11.58;	author millert;	state Exp;
branches;
next	1.3;

1.3
date	97.08.08.08.27.20;	author downsj;	state Exp;
branches;
next	1.2;

1.2
date	96.11.23.21.46.21;	author kstailey;	state Exp;
branches;
next	1.1;

1.1
date	96.08.11.05.35.13;	author deraadt;	state Exp;
branches;
next	;

1.6.2.1
date	2001.07.04.10.23.42;	author niklas;	state Exp;
branches;
next	1.6.2.2;

1.6.2.2
date	2001.10.31.03.07.57;	author nate;	state Exp;
branches;
next	1.6.2.3;

1.6.2.3
date	2001.11.13.21.04.17;	author niklas;	state Exp;
branches;
next	1.6.2.4;

1.6.2.4
date	2001.12.05.00.39.13;	author niklas;	state Exp;
branches;
next	1.6.2.5;

1.6.2.5
date	2002.03.06.02.04.46;	author niklas;	state Exp;
branches;
next	1.6.2.6;

1.6.2.6
date	2002.03.28.10.57.10;	author niklas;	state Exp;
branches;
next	;

1.13.2.1
date	2002.06.11.03.38.16;	author art;	state Exp;
branches;
next	1.13.2.2;

1.13.2.2
date	2002.10.29.00.28.10;	author art;	state Exp;
branches;
next	;


desc
@@


1.29
log
@Add a bus_dma_tag_t for DVMA usage, suitable for use for devices not sitting
behind a sun4m iommu.

Move the existing dvma routines from vm_machdep.c to this new dvma.c; this
allows for a few declarations to be removed from public headers.

Extend the device attachment arguments (struct confargs) to pass a
bus_dma_tag_t. mainbus receives the dvma bus_dma_tag_t, and devices pass the
tag unchanged to their children, except for iommu(4) which replaces it with
its own.

Change the few sun4m-only drivers to pick the bus_dma_tag_t from confargs
rather than assume iommu; this allows qlw(4) to attach and work on sun4c.

ok kettenis@@
@
text
@/*	$OpenBSD: iommu.c,v 1.28 2014/11/16 12:30:58 deraadt Exp $	*/
/*	$NetBSD: iommu.c,v 1.13 1997/07/29 09:42:04 fair Exp $ */

/*
 * Copyright (c) 1996
 * 	The President and Fellows of Harvard College. All rights reserved.
 * Copyright (c) 1995 	Paul Kranenburg
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Aaron Brown and
 *	Harvard University.
 *	This product includes software developed by Paul Kranenburg.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/device.h>
#include <sys/extent.h>
#include <sys/mbuf.h>

#include <uvm/uvm_extern.h>

#include <machine/autoconf.h>
#include <machine/bus.h>
#include <machine/ctlreg.h>
#include <sparc/sparc/asm.h>
#include <sparc/sparc/vaddrs.h>
#include <sparc/sparc/cpuvar.h>
#include <sparc/sparc/iommureg.h>

struct iommu_softc {
	struct device	sc_dev;		/* base device */
	struct iommureg	*sc_reg;
	u_int		sc_pagesize;
	u_int		sc_range;
	u_int		sc_dvmabase;
	iopte_t		*sc_ptes;
	int		sc_hasiocache;
#define sc_cachecoherent sc_hasiocache

/*
 * Note: operations on the extent map are being protected with
 * splhigh(), since we cannot predict at which interrupt priority
 * our clients will run.
 */
	struct sparc_bus_dma_tag sc_dmatag;
	struct extent *sc_dvmamap;
};

struct	iommu_softc *iommu_sc;/*XXX*/
struct sparc_bus_dma_tag *iommu_dmatag;/*XXX*/
int	has_iocache;

/* autoconfiguration driver */
int	iommu_print(void *, const char *);
void	iommu_attach(struct device *, struct device *, void *);
int	iommu_match(struct device *, void *, void *);

struct cfattach iommu_ca = {
	sizeof(struct iommu_softc), iommu_match, iommu_attach
};

struct cfdriver iommu_cd = {
	NULL, "iommu", DV_DULL
};

/* IOMMU DMA map functions */
int	iommu_dmamap_create(bus_dma_tag_t, bus_size_t, int, bus_size_t,
			bus_size_t, int, bus_dmamap_t *);
int	iommu_dmamap_load(bus_dma_tag_t, bus_dmamap_t, void *,
			bus_size_t, struct proc *, int);
int	iommu_dmamap_load_mbuf(bus_dma_tag_t, bus_dmamap_t,
			struct mbuf *, int);
int	iommu_dmamap_load_uio(bus_dma_tag_t, bus_dmamap_t,
			struct uio *, int);
int	iommu_dmamap_load_raw(bus_dma_tag_t, bus_dmamap_t,
			bus_dma_segment_t *, int, bus_size_t, int);
void	iommu_dmamap_unload(bus_dma_tag_t, bus_dmamap_t);
void	iommu_dmamap_sync(bus_dma_tag_t, bus_dmamap_t, bus_addr_t,
			bus_size_t, int);

int	iommu_dmamem_map(bus_dma_tag_t, bus_dma_segment_t *,
			int, size_t, caddr_t *, int);
void	iommu_dmamem_unmap(bus_dma_tag_t, void *, size_t);
paddr_t	iommu_dmamem_mmap(bus_dma_tag_t, bus_dma_segment_t *,
			int, off_t, int, int);
int	iommu_dvma_alloc(struct iommu_softc *, bus_dmamap_t, vaddr_t,
			 bus_size_t, int, bus_addr_t *, bus_size_t *);

int	iommu_dmamap_load_buffer(bus_dma_tag_t, bus_dmamap_t, void *,
				 bus_size_t, struct proc *, int);

/*
 * Print the location of some iommu-attached device (called just
 * before attaching that device).  If `iommu' is not NULL, the
 * device was found but not configured; print the iommu as well.
 * Return UNCONF (config_find ignores this if the device was configured).
 */
int
iommu_print(args, iommu)
	void *args;
	const char *iommu;
{
	register struct confargs *ca = args;

	if (iommu)
		printf("%s at %s", ca->ca_ra.ra_name, iommu);
	return (UNCONF);
}

int
iommu_match(parent, vcf, aux)
	struct device *parent;
	void *vcf, *aux;
{
	register struct cfdata *cf = vcf;
	register struct confargs *ca = aux;
	register struct romaux *ra = &ca->ca_ra;

	if (CPU_ISSUN4OR4COR4E)
		return (0);
	return (strcmp(cf->cf_driver->cd_name, ra->ra_name) == 0);
}

/*
 * Attach the iommu.
 */
void
iommu_attach(parent, self, aux)
	struct device *parent;
	struct device *self;
	void *aux;
{
#if defined(SUN4M)
	struct iommu_softc *sc = (struct iommu_softc *)self;
	struct confargs oca, *ca = aux;
	struct sparc_bus_dma_tag *dmat = &sc->sc_dmatag;
	register struct romaux *ra = &ca->ca_ra;
	register int node;
	register char *name;
	register u_int pbase, pa;
	register int i, mmupcrsave, s;
	register iopte_t *tpte_p;
	struct pglist mlist;
	struct vm_page *m;
	vaddr_t va;
	paddr_t iopte_pa;

	iommu_sc = sc;
	iommu_dmatag = dmat;
	/*
	 * XXX there is only one iommu, for now -- do not know how to
	 * address children on others
	 */
	if (sc->sc_dev.dv_unit > 0) {
		printf(" unsupported\n");
		return;
	}

	dmat->_cookie = sc;
	dmat->_dmamap_create = iommu_dmamap_create;
	dmat->_dmamap_destroy = _bus_dmamap_destroy;
	dmat->_dmamap_load = iommu_dmamap_load;
	dmat->_dmamap_load_mbuf = iommu_dmamap_load_mbuf;
	dmat->_dmamap_load_uio = iommu_dmamap_load_uio;
	dmat->_dmamap_load_raw = iommu_dmamap_load_raw;
	dmat->_dmamap_unload = iommu_dmamap_unload;
	dmat->_dmamap_sync = iommu_dmamap_sync;

	dmat->_dmamem_alloc = _bus_dmamem_alloc;
	dmat->_dmamem_free = _bus_dmamem_free;
	dmat->_dmamem_map = iommu_dmamem_map;
	dmat->_dmamem_unmap = _bus_dmamem_unmap;
	dmat->_dmamem_mmap = iommu_dmamem_mmap;

	node = ra->ra_node;

#if 0
	if (ra->ra_vaddr)
		sc->sc_reg = (struct iommureg *)ca->ca_ra.ra_vaddr;
#else
	/*
	 * Map registers into our space. The PROM may have done this
	 * already, but I feel better if we have our own copy. Plus, the
	 * prom doesn't map the entire register set
	 *
	 * XXX struct iommureg is bigger than ra->ra_len; what are the
	 *     other fields for?
	 */
	sc->sc_reg = (struct iommureg *)
		mapiodev(ra->ra_reg, 0, ra->ra_len);
#endif

	sc->sc_hasiocache = node_has_property(node, "cache-coherence?");
	if (CACHEINFO.c_enabled == 0) /* XXX - is this correct? */
		sc->sc_hasiocache = 0;
	has_iocache = sc->sc_hasiocache; /* Set global flag */

	sc->sc_pagesize = getpropint(node, "page-size", NBPG),
	sc->sc_range = (1 << 24) <<
	    ((sc->sc_reg->io_cr & IOMMU_CTL_RANGE) >> IOMMU_CTL_RANGESHFT);
#if 0
	sc->sc_dvmabase = (0 - sc->sc_range);
#endif
	pbase = (sc->sc_reg->io_bar & IOMMU_BAR_IBA) <<
			(14 - IOMMU_BAR_IBASHFT);

	/*
	 * Allocate memory for I/O pagetables. This takes 64k of memory
	 * since we want to have 64M of dvma space (this actually depends
	 * on the definition of DVMA4M_BASE...we may drop it back to 32M).
	 * The table must be aligned on a (-DVMA4M_BASE/NBPG) boundary
	 * (i.e. 64K for 64M of dvma space).
	 */
	TAILQ_INIT(&mlist);
#define DVMA_PTESIZE ((0 - DVMA4M_BASE) / 1024)
	if (uvm_pglistalloc(DVMA_PTESIZE, 0, 0xffffffff, DVMA_PTESIZE,
			    0, &mlist, 1, UVM_PLA_NOWAIT) ||
	    (va = uvm_km_valloc(kernel_map, DVMA_PTESIZE)) == 0)
		panic("iommu_attach: can't allocate memory for pagetables");
#undef DVMA_PTESIZE
	m = TAILQ_FIRST(&mlist);
	iopte_pa = VM_PAGE_TO_PHYS(m);
	sc->sc_ptes = (iopte_t *) va;

	while (m) {
		paddr_t pa = VM_PAGE_TO_PHYS(m);
		pmap_kenter_pa(va, pa | PMAP_NC, PROT_READ | PROT_WRITE);
		va += PAGE_SIZE;
		m = TAILQ_NEXT(m, pageq);
	}
	pmap_update(pmap_kernel());

	/*
	 * Now we build our own copy of the IOMMU page tables. We need to
	 * do this since we're going to change the range to give us 64M of
	 * mappings, and thus we can move DVMA space down to 0xfd000000 to
	 * give us lots of space and to avoid bumping into the PROM, etc.
	 *
	 * XXX Note that this is rather messy.
	 */

	/*
	 * Ok. We've got to read in the original table using MMU bypass,
	 * and copy all of its entries to the appropriate place in our
	 * new table, even if the sizes are different.
	 * This is pretty easy since we know DVMA ends at 0xffffffff.
	 *
	 * XXX: PGOFSET, NBPG assume same page size as SRMMU
	 */
	if (cpuinfo.cpu_vers == 4 && cpuinfo.mxcc) {
		/* set MMU AC bit */
		sta(SRMMU_PCR, ASI_SRMMU,
		    ((mmupcrsave = lda(SRMMU_PCR, ASI_SRMMU)) | VIKING_PCR_AC));
	}

	for (tpte_p = &sc->sc_ptes[((0 - DVMA4M_BASE)/NBPG) - 1],
	     pa = (u_int)pbase - sizeof(iopte_t) +
		   ((u_int)sc->sc_range/NBPG)*sizeof(iopte_t);
	     tpte_p >= &sc->sc_ptes[0] && pa >= (u_int)pbase;
	     tpte_p--, pa -= sizeof(iopte_t)) {

		IOMMU_FLUSHPAGE(sc,
			        (tpte_p - &sc->sc_ptes[0])*NBPG + DVMA4M_BASE);
		*tpte_p = lda(pa, ASI_BYPASS);
	}
	if (cpuinfo.cpu_vers == 4 && cpuinfo.mxcc) {
		/* restore mmu after bug-avoidance */
		sta(SRMMU_PCR, ASI_SRMMU, mmupcrsave);
	}

	/*
	 * Now we can install our new pagetable into the IOMMU
	 */
	sc->sc_range = 0 - DVMA4M_BASE;
	sc->sc_dvmabase = DVMA4M_BASE;

	/* calculate log2(sc->sc_range/16MB) */
	i = ffs(sc->sc_range/(1 << 24)) - 1;
	if ((1 << i) != (sc->sc_range/(1 << 24)))
		panic("bad iommu range: %d",i);

	s = splhigh();
	IOMMU_FLUSHALL(sc);

	sc->sc_reg->io_cr = (sc->sc_reg->io_cr & ~IOMMU_CTL_RANGE) |
			  (i << IOMMU_CTL_RANGESHFT) | IOMMU_CTL_ME;
	sc->sc_reg->io_bar = (iopte_pa >> 4) & IOMMU_BAR_IBA;

	IOMMU_FLUSHALL(sc);
	splx(s);

	printf(": version 0x%x/0x%x, page-size %d, range %dMB\n",
		(sc->sc_reg->io_cr & IOMMU_CTL_VER) >> 24,
		(sc->sc_reg->io_cr & IOMMU_CTL_IMPL) >> 28,
		sc->sc_pagesize,
		sc->sc_range >> 20);

	sc->sc_dvmamap = dvmamap_extent; /* XXX */

	/* Propagate bootpath */
	if (ra->ra_bp != NULL && strcmp(ra->ra_bp->name, "iommu") == 0)
		oca.ca_ra.ra_bp = ra->ra_bp + 1;
	else
		oca.ca_ra.ra_bp = NULL;

	/*
	 * Loop through ROM children (expect SBus among them).
	 */
	oca.ca_dmat = dmat;
	for (node = firstchild(node); node; node = nextsibling(node)) {
		name = getpropstring(node, "name");
		if (!romprop(&oca.ca_ra, name, node))
			continue;
		oca.ca_bustype = BUS_MAIN; /* ??? */
		oca.ca_dmat = dmat;
		config_found(&sc->sc_dev, (void *)&oca, iommu_print);
	}
#endif
}

void
iommu_enter(va, pa)
	u_int va, pa;
{
	struct iommu_softc *sc = iommu_sc;
	int pte;

#ifdef DEBUG
	if (va < sc->sc_dvmabase)
		panic("iommu_enter: va 0x%x not in DVMA space",va);
#endif

	pte = atop(pa) << IOPTE_PPNSHFT;
	pte &= IOPTE_PPN;
	pte |= IOPTE_V | IOPTE_W | (has_iocache ? IOPTE_C : 0);
	sc->sc_ptes[atop(va - sc->sc_dvmabase)] = pte;
	IOMMU_FLUSHPAGE(sc, va);
}

/*
 * iommu_remove: clears mappings created by iommu_enter
 */
void
iommu_remove(va, len)
	register u_int va, len;
{
	register struct iommu_softc *sc = iommu_sc;

#ifdef DEBUG
	if (va < sc->sc_dvmabase)
		panic("iommu_enter: va 0x%x not in DVMA space", va);
#endif

	while (len > 0) {
#ifdef notyet
#ifdef DEBUG
		if ((sc->sc_ptes[atop(va - sc->sc_dvmabase)] & IOPTE_V) == 0)
			panic("iommu_remove: clearing invalid pte at va 0x%x",
				va);
#endif
#endif
		sc->sc_ptes[atop(va - sc->sc_dvmabase)] = 0;
		IOMMU_FLUSHPAGE(sc, va);
		len -= sc->sc_pagesize;
		va += sc->sc_pagesize;
	}
}

extern u_long dvma_cachealign;

/*
 * IOMMU DMA map functions.
 */
int
iommu_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
		    bus_size_t maxsegsz, bus_size_t boundary, int flags,
		    bus_dmamap_t *dmamp)
{
	struct iommu_softc *sc = t->_cookie;
	bus_dmamap_t map;
	int error;

	if ((error = _bus_dmamap_create(t, size, nsegments, maxsegsz,
					boundary, flags, &map)) != 0)
		return (error);

	if ((flags & BUS_DMA_24BIT) != 0) {
		/* Limit this map to the range usable by `24-bit' devices */
		map->_dm_ex_start = DVMA_D24_BASE;
		map->_dm_ex_end = DVMA_D24_END;
	} else {
		/* Enable allocations from the entire map */
		map->_dm_ex_start = sc->sc_dvmamap->ex_start;
		map->_dm_ex_end = sc->sc_dvmamap->ex_end;
	}

	*dmamp = map;
	return (0);
}

/*
 * Internal routine to allocate space in the IOMMU map.
 */
int
iommu_dvma_alloc(struct iommu_softc *sc, bus_dmamap_t map,
		 vaddr_t va, bus_size_t len, int flags,
		 bus_addr_t *dvap, bus_size_t *sgsizep)
{
	bus_size_t sgsize;
	u_long align, voff, dvaddr;
	int s, error;
	int pagesz = PAGE_SIZE;

	/*
	 * Remember page offset, then truncate the buffer address to
	 * a page boundary.
	 */
	voff = va & (pagesz - 1);
	va &= -pagesz;

	if (len > map->_dm_size)
		return (EINVAL);

	sgsize = (len + voff + pagesz - 1) & -pagesz;
	align = dvma_cachealign ? dvma_cachealign : map->_dm_align;

	s = splhigh();
	error = extent_alloc_subregion(sc->sc_dvmamap, map->_dm_ex_start,
	    map->_dm_ex_end, sgsize, align, va & (align-1), map->_dm_boundary,
	    (flags & BUS_DMA_NOWAIT) == 0 ? EX_WAITOK : EX_NOWAIT, &dvaddr);
	splx(s);
	*dvap = (bus_addr_t)dvaddr;
	*sgsizep = sgsize;
	return (error);
}

int
iommu_dmamap_load_buffer(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags)
{
	struct iommu_softc *sc = t->_cookie;
	bus_size_t sgsize;
	bus_addr_t dva;
	vaddr_t va = (vaddr_t)buf;
	int pagesz = PAGE_SIZE;
	pmap_t pmap;
	int error;

	if (map->dm_nsegs >= map->_dm_segcnt)
		return (EFBIG);

	/* Allocate IOMMU resources */
	if ((error = iommu_dvma_alloc(sc, map, va, buflen, flags,
					&dva, &sgsize)) != 0)
		return (error);

	if ((sc->sc_cachecoherent == 0) || 
	    (CACHEINFO.ec_totalsize == 0))
		cpuinfo.cache_flush(buf, buflen); /* XXX - move to bus_dma_sync? */

	/*
	 * We always use just one segment.
	 */
	map->dm_segs[map->dm_nsegs].ds_addr = dva + (va & (pagesz - 1));
	map->dm_segs[map->dm_nsegs].ds_len = buflen;
	map->dm_segs[map->dm_nsegs]._ds_sgsize = sgsize;
	map->dm_nsegs++;

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	for (; sgsize != 0; ) {
		paddr_t pa;
		/*
		 * Get the physical address for this page.
		 */
		if (!pmap_extract(pmap, va, &pa))
			return (EFAULT);

		iommu_enter(dva, pa);

		dva += pagesz;
		va += pagesz;
		sgsize -= pagesz;
	}

	return (0);
}

/*
 * Prepare buffer for DMA transfer.
 */
int
iommu_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map,
		  void *buf, bus_size_t buflen,
		  struct proc *p, int flags)
{
	int error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = buflen;
	map->dm_nsegs = 0;

	error = iommu_dmamap_load_buffer(t, map, buf, buflen, p, flags);
	if (error)
		iommu_dmamap_unload(t, map);

	return (error);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
iommu_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map,
		       struct mbuf *m0, int flags)
{
	struct mbuf *m;
	int error = 0;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = m0->m_pkthdr.len;
	map->dm_nsegs = 0;

	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		error = iommu_dmamap_load_buffer(t, map, m->m_data, m->m_len,
		    NULL, flags);
	}

	if (error)
		iommu_dmamap_unload(t, map);

	return (error);
}

/*
 * Like _bus_dmamap_load(), but for uios.
 */
int
iommu_dmamap_load_uio(bus_dma_tag_t t, bus_dmamap_t map,
		      struct uio *uio, int flags)
{

	panic("_bus_dmamap_load_uio: not implemented");
}

/*
 * Like _bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
iommu_dmamap_load_raw(bus_dma_tag_t t, bus_dmamap_t map,
		      bus_dma_segment_t *segs, int nsegs, bus_size_t size,
		      int flags)
{
	struct iommu_softc *sc = t->_cookie;
	struct vm_page *m;
	paddr_t pa;
	bus_addr_t dva;
	bus_size_t sgsize;
	struct pglist *mlist;
	int pagesz = PAGE_SIZE;
	int error;

	map->dm_nsegs = 0;

	/* Allocate IOMMU resources */
	if ((error = iommu_dvma_alloc(sc, map, segs[0]._ds_va, size,
				      flags, &dva, &sgsize)) != 0)
		return (error);

	/*
	 * Note DVMA address in case bus_dmamem_map() is called later.
	 * It can then insure cache coherency by choosing a KVA that
	 * is aligned to `ds_addr'.
	 */
	segs[0].ds_addr = dva;
	segs[0].ds_len = size;

	map->dm_segs[0].ds_addr = dva;
	map->dm_segs[0].ds_len = size;
	map->dm_segs[0]._ds_sgsize = sgsize;

	/* Map physical pages into IOMMU */
	mlist = segs[0]._ds_mlist;
	for (m = TAILQ_FIRST(mlist); m != NULL; m = TAILQ_NEXT(m,pageq)) {
		if (sgsize == 0)
			panic("iommu_dmamap_load_raw: size botch");
		pa = VM_PAGE_TO_PHYS(m);
		iommu_enter(dva, pa);
		dva += pagesz;
		sgsize -= pagesz;
	}

	map->dm_nsegs = 1;
	map->dm_mapsize = size;

	return (0);
}

/*
 * Unload an IOMMU DMA map.
 */
void
iommu_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{
	struct iommu_softc *sc = t->_cookie;
	bus_dma_segment_t *segs = map->dm_segs;
	int nsegs = map->dm_nsegs;
	bus_addr_t dva;
	bus_size_t len;
	int i, s, error;

	for (i = 0; i < nsegs; i++) {
		dva = segs[i].ds_addr & -PAGE_SIZE;
		len = segs[i]._ds_sgsize;

		iommu_remove(dva, len);
		s = splhigh();
		error = extent_free(sc->sc_dvmamap, dva, len, EX_NOWAIT);
		splx(s);
		if (error != 0)
			printf("warning: %ld of DVMA space lost\n", (long)len);
	}

	/* Mark the mappings as invalid. */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;
}

/*
 * DMA map synchronization.
 */
void
iommu_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map,
		  bus_addr_t offset, bus_size_t len, int ops)
{

	/*
	 * XXX Should flush CPU write buffers.
	 */
}

/*
 * Map DMA-safe memory.
 */
int
iommu_dmamem_map(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
		 size_t size, caddr_t *kvap, int flags)
{
	struct iommu_softc *sc = t->_cookie;
	struct vm_page *m;
	vaddr_t va;
	bus_addr_t addr;
	struct pglist *mlist;
	int cbit;
	u_long align;
	int pagesz = PAGE_SIZE;
	const struct kmem_dyn_mode *kd;

	if (nsegs != 1)
		panic("iommu_dmamem_map: nsegs = %d", nsegs);

	cbit = sc->sc_cachecoherent ? 0 : PMAP_NC;
	align = dvma_cachealign ? dvma_cachealign : pagesz;

	size = round_page(size);

#if 0
	/*
	 * In case the segment has already been loaded by
	 * iommu_dmamap_load_raw(), find a region of kernel virtual
	 * addresses that can accommodate our alignment requirements.
	 */
	va = _bus_dma_valloc_skewed(size, 0, align,
				    segs[0].ds_addr & (align - 1));
#else
	kd = flags & BUS_DMA_NOWAIT ? &kd_trylock : &kd_waitok;
	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, kd);
#endif
	if (va == 0)
		return (ENOMEM);

	segs[0]._ds_va = va;
	*kvap = (void *)va;

	/*
	 * Map the pages allocated in _bus_dmamem_alloc() to the
	 * kernel virtual address space.
	 */
	mlist = segs[0]._ds_mlist;
	for (m = TAILQ_FIRST(mlist); m != NULL; m = TAILQ_NEXT(m,pageq)) {

		if (size == 0)
			panic("iommu_dmamem_map: size botch");

		addr = VM_PAGE_TO_PHYS(m);
		pmap_kenter_pa(va, addr | cbit, PROT_READ | PROT_WRITE);
#if 0
			if (flags & BUS_DMA_COHERENT)
				/* XXX */;
#endif
		va += pagesz;
		size -= pagesz;
	}
	pmap_update(pmap_kernel());

	return (0);
}

void
iommu_dmamem_unmap(bus_dma_tag_t t, void *kva, size_t size)
{

#ifdef DIAGNOSTIC
	if ((u_long)kva & PAGE_MASK)
		panic("iommu_dmamem_unmap");
#endif

	km_free(kva, round_page(size), &kv_any, &kp_none);
}


/*
 * mmap(2)'ing DMA-safe memory.
 */
paddr_t
iommu_dmamem_mmap(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
		  off_t off, int prot, int flags)
{
	panic("_bus_dmamem_mmap: not implemented");
}
@


1.28
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.27 2014/07/11 09:36:26 mpi Exp $	*/
a48 2
#include <machine/pmap.h>

d336 1
d342 2
a343 1
		(void) config_found(&sc->sc_dev, (void *)&oca, iommu_print);
@


1.27
log
@Convert bus_dmamem_map(9) to km_alloc(9) in order to make it fail and
not sleep if the allocator cannot obtain a lock when BUS_DMA_NOWAIT is
specified.

idea and inputs from kettenis@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.26 2014/04/08 13:23:51 mpi Exp $	*/
d256 1
a256 1
		pmap_kenter_pa(va, pa | PMAP_NC, VM_PROT_READ|VM_PROT_WRITE);
d733 1
a733 1
		pmap_kenter_pa(va, addr | cbit, VM_PROT_READ | VM_PROT_WRITE);
@


1.26
log
@Fewer <uvm/uvm.h>!
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.25 2010/07/10 19:32:25 miod Exp $	*/
d694 1
d713 2
a714 1
	va = uvm_km_valloc(kernel_map, size);
d755 1
a755 4
	size = round_page(size);
	pmap_kremove((vaddr_t)kva, size);
	pmap_update(pmap_kernel());
	uvm_unmap(kernel_map, (vaddr_t)kva, (vaddr_t)kva + size);
@


1.25
log
@sun4e (i.e. SPARCengine 1e) support. This platform is a mix between sun4 and
sun4c, as it has a sun4c OpenPROM but a sun4 8KB pagesize. VME devices are
not supported yet.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.24 2010/06/17 16:11:19 miod Exp $	*/
d47 1
a47 1
#include <uvm/uvm.h>
@


1.24
log
@aligment -> alignment
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.23 2010/04/21 03:03:26 deraadt Exp $	*/
d148 1
a148 1
	if (CPU_ISSUN4OR4C)
@


1.23
log
@more cleanup to cope with the change that tries to make proc.h not act
like it is everything.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.22 2009/07/13 19:50:00 kettenis Exp $	*/
d707 1
a707 1
	 * addresses that can accommodate our aligment requirements.
@


1.22
log
@Add a somewhat minimal bus_dma(9) implementation.  Only fully implemented for
the sun4m iommu.  Bits and pieces from NetBSD, with some additional tweaks
and a bus_dmamap_load_mbuf() implementation from myself.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.21 2009/04/14 16:01:04 oga Exp $	*/
d42 1
@


1.21
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.20 2007/05/29 09:53:59 sobrado Exp $	*/
d43 2
d51 1
d66 9
d76 1
d78 1
a80 1

d94 26
d162 1
a162 1
	register struct iommu_softc *sc = (struct iommu_softc *)self;
d164 1
d177 1
d186 17
d326 2
d393 374
@


1.20
log
@use the right capitalization for `SBus'

ok jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.19 2005/03/29 11:33:18 miod Exp $	*/
d187 1
a187 1
			    0, &mlist, 1, 0) ||
@


1.19
log
@Remove workarounds for old compilers.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.18 2002/10/07 18:35:56 mickey Exp $	*/
d275 1
a275 1
	 * Loop through ROM children (expect Sbus among them).
@


1.18
log
@this removes the functionality of adding allocated
pages into the queue already containing allocated pages.
breaks i386:setup_buffers() because of this.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.17 2002/10/06 22:06:15 art Exp $	*/
a135 1
/*XXX-GCC!*/mmupcrsave=0;
@


1.17
log
@No more need to initialize the result list before uvm_pglistalloc.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.16 2002/03/14 20:30:00 jason Exp $	*/
d185 1
@


1.16
log
@Remove a bunch of #if 0 code
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.15 2002/03/14 01:26:44 millert Exp $	*/
a184 1
	TAILQ_INIT(&mlist);
@


1.15
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.14 2002/02/21 19:55:13 jason Exp $	*/
a334 45

#if 0	/* These registers aren't there??? */
void
iommu_error()
{
	struct iommu_softc *sc = X;
	struct iommureg *iop = sc->sc_reg;

	printf("iommu: afsr 0x%x, afar 0x%x\n", iop->io_afsr, iop->io_afar);
	printf("iommu: mfsr 0x%x, mfar 0x%x\n", iop->io_mfsr, iop->io_mfar);
}
int
iommu_alloc(va, len)
	u_int va, len;
{
	struct iommu_softc *sc = X;
	int off, tva, pa, iovaddr, pte;

	off = (int)va & PGOFSET;
	len = round_page(len + off);
	va -= off;

if ((int)sc->sc_dvmacur + len > 0)
	sc->sc_dvmacur = sc->sc_dvmabase;

	iovaddr = tva = sc->sc_dvmacur;
	sc->sc_dvmacur += len;
	while (len) {
		pmap_extract(pmap_kernel(), va, &pa);

#define IOMMU_PPNSHIFT	8
#define IOMMU_V		0x00000002
#define IOMMU_W		0x00000004

		pte = atop(pa) << IOMMU_PPNSHIFT;
		pte |= IOMMU_V | IOMMU_W;
		sta(sc->sc_ptes + atop(tva - sc->sc_dvmabase), ASI_BYPASS, pte);
		sc->sc_reg->io_flushpage = tva;
		len -= NBPG;
		va += NBPG;
		tva += NBPG;
	}
	return iovaddr + off;
}
#endif
@


1.14
log
@%s/iommu_clear/iommu_remove/g
(iommu_clear was renamed awhile ago, but comments and such were not updated)
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.13 2001/12/08 02:24:07 art Exp $	*/
d69 3
a71 3
int	iommu_print __P((void *, const char *));
void	iommu_attach __P((struct device *, struct device *, void *));
int	iommu_match __P((struct device *, void *, void *));
@


1.13
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.12 2001/11/22 09:47:37 art Exp $	*/
d308 1
a308 1
 * iommu_clear: clears mappings created by iommu_enter
d325 1
a325 1
			panic("iommu_clear: clearing invalid pte at va 0x%x",
@


1.13.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.13 2001/12/08 02:24:07 art Exp $	*/
d69 3
a71 3
int	iommu_print(void *, const char *);
void	iommu_attach(struct device *, struct device *, void *);
int	iommu_match(struct device *, void *, void *);
d308 1
a308 1
 * iommu_remove: clears mappings created by iommu_enter
d325 1
a325 1
			panic("iommu_remove: clearing invalid pte at va 0x%x",
d335 45
@


1.13.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.13.2.1 2002/06/11 03:38:16 art Exp $	*/
@


1.12
log
@simplify the iommu page table mapping. use pmap_kenter.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.11 2001/11/06 19:53:16 miod Exp $	*/
d202 1
@


1.11
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.10 2001/09/19 20:50:57 mickey Exp $	*/
d133 1
a133 1
	vaddr_t iopte_va;
d189 1
a189 1
	    (iopte_va = uvm_km_valloc(kernel_map, DVMA_PTESIZE)) == 0)
d194 1
a194 1
	sc->sc_ptes = (iopte_t *) iopte_va;
d197 3
a199 6
		/* XXX - art, pagewire breaks the tailq */
		uvm_pagewire(m);
		pmap_enter(pmap_kernel(), iopte_va, VM_PAGE_TO_PHYS(m),
			   VM_PROT_READ|VM_PROT_WRITE,
			   VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
		iopte_va += NBPG;
a210 8


	/*
	 * Now discache the page tables so that the IOMMU sees our
	 * changes.
	 */
	kvm_uncache((caddr_t)sc->sc_ptes,
		(((0 - DVMA4M_BASE)/sc->sc_pagesize) * sizeof(iopte_t)) / NBPG);
@


1.10
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.9 2001/07/25 13:25:33 art Exp $	*/
a43 1
#include <vm/vm.h>
@


1.9
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.8 2001/06/08 08:09:27 art Exp $	*/
d43 1
a44 1
#include <vm/vm_kern.h>
@


1.8
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.7 2001/05/10 10:34:46 art Exp $	*/
d201 2
a202 2
			   VM_PROT_READ|VM_PROT_WRITE, 1,
			   VM_PROT_READ|VM_PROT_WRITE);
@


1.7
log
@UVM is no longer optional on sparc.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.6 2000/01/01 19:41:00 deraadt Exp $	*/
d374 1
a374 1
		pa = pmap_extract(pmap_kernel(), va);
@


1.6
log
@make the kernel compile
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.5 1999/11/05 18:07:11 art Exp $	*/
a44 1
#if defined(UVM)
a45 1
#endif
a187 1
#if defined(UVM)
a191 6
#else
	if (vm_page_alloc_memory(DVMA_PTESIZE, 0, 0xffffffff, DVMA_PTESIZE,
				 0, &mlist, 1, 0) ||
	    (iopte_va = kmem_alloc_pageable(kernel_map, DVMA_PTESIZE)) == 0)
		panic("iommu_attach: can't allocate memory for pagetables");
#endif
d198 1
a198 1
#if defined(UVM)
a199 3
#else
		vm_page_wire(m);
#endif
@


1.6.2.1
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.6 2000/01/01 19:41:00 deraadt Exp $	*/
d45 1
d47 1
d190 1
d195 6
d207 1
a207 1
		/* XXX - art, pagewire breaks the tailq */
d209 3
d386 1
a386 1
		pmap_extract(pmap_kernel(), va, &pa);
@


1.6.2.2
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.6.2.1 2001/07/04 10:23:42 niklas Exp $	*/
a42 1

d44 1
d201 2
a202 2
			   VM_PROT_READ|VM_PROT_WRITE,
			   VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
@


1.6.2.3
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d44 1
@


1.6.2.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.6.2.3 2001/11/13 21:04:17 niklas Exp $	*/
d133 1
a133 1
	vaddr_t va;
d189 1
a189 1
	    (va = uvm_km_valloc(kernel_map, DVMA_PTESIZE)) == 0)
d194 1
a194 1
	sc->sc_ptes = (iopte_t *) va;
d197 6
a202 3
		paddr_t pa = VM_PAGE_TO_PHYS(m);
		pmap_kenter_pa(va, pa | PMAP_NC, VM_PROT_READ|VM_PROT_WRITE);
		va += PAGE_SIZE;
d214 8
@


1.6.2.5
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a201 1
	pmap_update(pmap_kernel());
d307 1
a307 1
 * iommu_remove: clears mappings created by iommu_enter
d324 1
a324 1
			panic("iommu_remove: clearing invalid pte at va 0x%x",
@


1.6.2.6
log
@Merge in -current from about a week ago
@
text
@d69 3
a71 3
int	iommu_print(void *, const char *);
void	iommu_attach(struct device *, struct device *, void *);
int	iommu_match(struct device *, void *, void *);
d335 45
@


1.5
log
@Move the allocation of iomme ptes to iommuattach (where it belongs).
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.4 1999/01/11 05:11:58 millert Exp $	*/
d48 2
@


1.4
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.3 1997/08/08 08:27:20 downsj Exp $	*/
d44 4
d132 4
a135 2
	extern u_int *kernel_iopte_table;
	extern u_int kernel_iopte_table_pa;
d180 38
d225 1
a225 1
	sc->sc_ptes = (iopte_t *) kernel_iopte_table;
d279 1
a279 1
	sc->sc_reg->io_bar = (kernel_iopte_table_pa >> 4) & IOMMU_BAR_IBA;
@


1.3
log
@Mostly sync to NetBSD-current 970804.

GENERIC currently compiles and runs; some devices (isp) are not complete and
not yet enabled.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d228 1
a228 1
		panic("bad iommu range: %d\n",i);
@


1.2
log
@added const to second parameter of cfprint_t routines
@
text
@d1 2
a2 1
/*	$NetBSD: iommu.c,v 1.4 1996/05/21 07:25:07 pk Exp $ */
d49 1
d101 1
a101 1
	struct cfdata *cf = vcf;
d126 1
a126 1
	register int i, mmupcrsav, s, wierdviking = 0;
d131 1
a131 1
/*XXX-GCC!*/mmupcrsav=0;
d156 1
a156 1
		mapdev(ra->ra_reg, 0, 0, ra->ra_len, ra->ra_iospace);
d160 2
d198 4
a201 4
	if ((getpsr() & 0x40000000) && (!(lda(SRMMU_PCR,ASI_SRMMU) & 0x800))) {
		wierdviking = 1;
		sta(SRMMU_PCR, ASI_SRMMU, 	/* set MMU AC bit */
		    ((mmupcrsav = lda(SRMMU_PCR,ASI_SRMMU)) | SRMMU_PCR_AC));
d214 3
a216 2
	if (wierdviking) {	/* restore mmu after bug-avoidance */
		sta(SRMMU_PCR, ASI_SRMMU, mmupcrsav);
d240 1
a240 1
	printf(": version %x/%x, page-size %d, range %dMB\n",
a306 1
		sta(sc->sc_ptes + atop(va - sc->sc_dvmabase), ASI_BYPASS, 0);
d320 2
a321 2
	printf("iommu: afsr %x, afar %x\n", iop->io_afsr, iop->io_afar);
	printf("iommu: mfsr %x, mfar %x\n", iop->io_mfsr, iop->io_mfar);
@


1.1
log
@netbsd port, now we merge our changes back in
@
text
@d64 1
a64 1
int	iommu_print __P((void *, char *));
d85 1
a85 1
	char *iommu;
@
