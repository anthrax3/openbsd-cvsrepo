head	1.21;
access;
symbols
	OPENBSD_6_0:1.21.0.4
	OPENBSD_6_0_BASE:1.21
	OPENBSD_5_9:1.21.0.2
	OPENBSD_5_9_BASE:1.21
	OPENBSD_5_8:1.20.0.24
	OPENBSD_5_8_BASE:1.20
	OPENBSD_5_7:1.20.0.16
	OPENBSD_5_7_BASE:1.20
	OPENBSD_5_6:1.20.0.20
	OPENBSD_5_6_BASE:1.20
	OPENBSD_5_5:1.20.0.18
	OPENBSD_5_5_BASE:1.20
	OPENBSD_5_4:1.20.0.14
	OPENBSD_5_4_BASE:1.20
	OPENBSD_5_3:1.20.0.12
	OPENBSD_5_3_BASE:1.20
	OPENBSD_5_2:1.20.0.10
	OPENBSD_5_2_BASE:1.20
	OPENBSD_5_1_BASE:1.20
	OPENBSD_5_1:1.20.0.8
	OPENBSD_5_0:1.20.0.6
	OPENBSD_5_0_BASE:1.20
	OPENBSD_4_9:1.20.0.4
	OPENBSD_4_9_BASE:1.20
	OPENBSD_4_8:1.20.0.2
	OPENBSD_4_8_BASE:1.20
	OPENBSD_4_7:1.18.0.12
	OPENBSD_4_7_BASE:1.18
	OPENBSD_4_6:1.18.0.14
	OPENBSD_4_6_BASE:1.18
	OPENBSD_4_5:1.18.0.10
	OPENBSD_4_5_BASE:1.18
	OPENBSD_4_4:1.18.0.8
	OPENBSD_4_4_BASE:1.18
	OPENBSD_4_3:1.18.0.6
	OPENBSD_4_3_BASE:1.18
	OPENBSD_4_2:1.18.0.4
	OPENBSD_4_2_BASE:1.18
	OPENBSD_4_1:1.18.0.2
	OPENBSD_4_1_BASE:1.18
	OPENBSD_4_0:1.17.0.6
	OPENBSD_4_0_BASE:1.17
	OPENBSD_3_9:1.17.0.4
	OPENBSD_3_9_BASE:1.17
	OPENBSD_3_8:1.17.0.2
	OPENBSD_3_8_BASE:1.17
	OPENBSD_3_7:1.16.0.14
	OPENBSD_3_7_BASE:1.16
	OPENBSD_3_6:1.16.0.12
	OPENBSD_3_6_BASE:1.16
	SMP_SYNC_A:1.16
	SMP_SYNC_B:1.16
	OPENBSD_3_5:1.16.0.10
	OPENBSD_3_5_BASE:1.16
	OPENBSD_3_4:1.16.0.8
	OPENBSD_3_4_BASE:1.16
	UBC_SYNC_A:1.16
	OPENBSD_3_3:1.16.0.6
	OPENBSD_3_3_BASE:1.16
	OPENBSD_3_2:1.16.0.4
	OPENBSD_3_2_BASE:1.16
	OPENBSD_3_1:1.16.0.2
	OPENBSD_3_1_BASE:1.16
	UBC_SYNC_B:1.16
	UBC:1.14.0.2
	UBC_BASE:1.14
	OPENBSD_3_0:1.13.0.8
	OPENBSD_3_0_BASE:1.13
	OPENBSD_2_9_BASE:1.13
	OPENBSD_2_9:1.13.0.6
	OPENBSD_2_8:1.13.0.4
	OPENBSD_2_8_BASE:1.13
	OPENBSD_2_7:1.13.0.2
	OPENBSD_2_7_BASE:1.13
	SMP:1.9.0.4
	SMP_BASE:1.9
	kame_19991208:1.9
	OPENBSD_2_6:1.9.0.2
	OPENBSD_2_6_BASE:1.9
	OPENBSD_2_5:1.7.0.2
	OPENBSD_2_5_BASE:1.7
	OPENBSD_2_4:1.4.0.6
	OPENBSD_2_4_BASE:1.4
	OPENBSD_2_3:1.4.0.4
	OPENBSD_2_3_BASE:1.4
	OPENBSD_2_2:1.4.0.2
	OPENBSD_2_2_BASE:1.4
	OPENBSD_2_1:1.3.0.4
	OPENBSD_2_1_BASE:1.3
	OPENBSD_2_0:1.3.0.2
	OPENBSD_2_0_BASE:1.3
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.21
date	2015.12.10.19.48.04;	author mmcc;	state Exp;
branches;
next	1.20;
commitid	IegoPc6ss7aut6L1;

1.20
date	2010.07.10.19.32.24;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2010.06.29.21.24.41;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2007.01.22.19.39.33;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2005.04.19.21.30.20;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2002.03.13.00.24.21;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2001.12.19.08.58.05;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.12.05.14.40.48;	author art;	state Exp;
branches
	1.14.2.1;
next	1.13;

1.13
date	2000.03.17.21.54.07;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	2000.02.21.14.51.20;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2000.02.21.14.43.35;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2000.02.19.21.45.55;	author art;	state Exp;
branches;
next	1.9;

1.9
date	99.07.20.11.07.09;	author art;	state Exp;
branches
	1.9.4.1;
next	1.8;

1.8
date	99.07.05.16.09.05;	author art;	state Exp;
branches;
next	1.7;

1.7
date	99.03.22.07.17.13;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	99.03.15.17.55.31;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	99.03.03.22.02.21;	author jason;	state Exp;
branches;
next	1.4;

1.4
date	97.08.08.08.27.03;	author downsj;	state Exp;
branches;
next	1.3;

1.3
date	96.08.12.01.43.53;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	96.08.11.05.35.04;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.51.45;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.51.45;	author deraadt;	state Exp;
branches;
next	;

1.9.4.1
date	2000.02.20.11.56.51;	author niklas;	state Exp;
branches;
next	1.9.4.2;

1.9.4.2
date	2000.02.21.22.29.02;	author niklas;	state Exp;
branches;
next	1.9.4.3;

1.9.4.3
date	2000.03.24.09.08.49;	author niklas;	state Exp;
branches;
next	1.9.4.4;

1.9.4.4
date	2002.03.28.10.57.10;	author niklas;	state Exp;
branches;
next	;

1.14.2.1
date	2002.06.11.03.38.16;	author art;	state Exp;
branches;
next	;


desc
@@


1.21
log
@Remove remaining Solbourne code.

ok deraadt@@
@
text
@/*	$OpenBSD: cache.c,v 1.20 2010/07/10 19:32:24 miod Exp $	*/
/*	$NetBSD: cache.c,v 1.34 1997/09/26 22:17:23 pk Exp $	*/

/*
 * Copyright (c) 1996
 *	The President and Fellows of Harvard College. All rights reserved.
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This software was developed by the Computer Systems Engineering group
 * at Lawrence Berkeley Laboratory under DARPA contract BG 91-66 and
 * contributed to Berkeley.
 *
 * All advertising materials mentioning features or use of this software
 * must display the following acknowledgement:
 *	This product includes software developed by Harvard University.
 *	This product includes software developed by the University of
 *	California, Lawrence Berkeley Laboratory.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Aaron Brown and
 *	Harvard University.
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)cache.c	8.2 (Berkeley) 10/30/93
 *
 */

/*
 * Cache routines.
 *
 * TODO:
 *	- rework range flush
 */

#include <sys/param.h>
#include <sys/systm.h>

#include <uvm/uvm_extern.h>

#include <machine/ctlreg.h>

#include <sparc/sparc/asm.h>
#include <sparc/sparc/cache.h>
#include <sparc/sparc/cpuvar.h>

struct cachestats cachestats;

int cache_alias_dist;		/* Cache anti-aliasing constants */
int cache_alias_bits;

/*
 * Enable the cache.
 * We need to clear out the valid bits first.
 */
void
sun4_cache_enable()
{
	u_int i, lim, ls, ts;

	cache_alias_bits = CPU_ISSUN4OR4E
				? CACHE_ALIAS_BITS_SUN4
				: CACHE_ALIAS_BITS_SUN4C;
	cache_alias_dist = CPU_ISSUN4OR4E
				? CACHE_ALIAS_DIST_SUN4
				: CACHE_ALIAS_DIST_SUN4C;

	ls = CACHEINFO.c_linesize;
	ts = CACHEINFO.c_totalsize;

	for (i = AC_CACHETAGS, lim = i + ts; i < lim; i += ls)
		sta(i, ASI_CONTROL, 0);

	stba(AC_SYSENABLE, ASI_CONTROL,
	     lduba(AC_SYSENABLE, ASI_CONTROL) | SYSEN_CACHE);
	CACHEINFO.c_enabled = 1;

	printf("cache enabled\n");

#ifdef notyet
	if (cpuinfo.flags & SUN4_IOCACHE) {
		stba(AC_SYSENABLE, ASI_CONTROL,
		     lduba(AC_SYSENABLE, ASI_CONTROL) | SYSEN_IOCACHE);
		printf("iocache enabled\n");
	}
#endif
}

#if defined(SUN4M)
void
ms1_cache_enable()
{
	u_int pcr;

	cache_alias_dist = max(
	    CACHEINFO.ic_totalsize / CACHEINFO.ic_associativity,
	    CACHEINFO.dc_totalsize / CACHEINFO.dc_associativity);
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;

	pcr = lda(SRMMU_PCR, ASI_SRMMU);

	/* We "flash-clear" the I/D caches. */
	if ((pcr & MS1_PCR_ICE) == 0)
		sta(0, ASI_ICACHECLR, 0);
	if ((pcr & MS1_PCR_DCE) == 0)
		sta(0, ASI_DCACHECLR, 0);

	/* Turn on caches */
	sta(SRMMU_PCR, ASI_SRMMU, pcr | MS1_PCR_DCE | MS1_PCR_ICE);

	CACHEINFO.c_enabled = CACHEINFO.dc_enabled = 1;

	/*
	 * When zeroing or copying pages, there might still be entries in
	 * the cache, since we don't flush pages from the cache when
	 * unmapping them (`vactype' is VAC_NONE).  Fortunately, the
	 * MS1 cache is write-through and not write-allocate, so we can
	 * use cacheable accesses while not displacing cache lines.
	 */
	cpuinfo.flags |= CPUFLG_CACHE_MANDATORY;

	printf("cache enabled\n");
}

void
viking_cache_enable()
{
	u_int pcr;

	cache_alias_dist = max(
		CACHEINFO.ic_totalsize / CACHEINFO.ic_associativity,
		CACHEINFO.dc_totalsize / CACHEINFO.dc_associativity);
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;

	pcr = lda(SRMMU_PCR, ASI_SRMMU);

	if ((pcr & VIKING_PCR_ICE) == 0) {
		/* I-cache not on; "flash-clear" it now. */
		sta(0x80000000, ASI_ICACHECLR, 0);	/* Unlock */
		sta(0, ASI_ICACHECLR, 0);		/* clear */
	}
	if ((pcr & VIKING_PCR_DCE) == 0) {
		/* D-cache not on: "flash-clear" it. */
		sta(0x80000000, ASI_DCACHECLR, 0);
		sta(0, ASI_DCACHECLR, 0);
	}

	/* Turn on caches via MMU */
	sta(SRMMU_PCR, ASI_SRMMU, pcr | VIKING_PCR_DCE | VIKING_PCR_ICE);

	CACHEINFO.c_enabled = CACHEINFO.dc_enabled = 1;

	/* Now turn on MultiCache if it exists */
	if (cpuinfo.mxcc && CACHEINFO.ec_totalsize > 0) {
		/* Multicache controller */
		stda(MXCC_ENABLE_ADDR, ASI_CONTROL,
		     ldda(MXCC_ENABLE_ADDR, ASI_CONTROL) |
		     (u_int64_t)MXCC_ENABLE_BIT);
		cpuinfo.flags |= CPUFLG_CACHEPAGETABLES; /* Ok to cache PTEs */
		CACHEINFO.ec_enabled = 1;
	}
	printf("cache enabled\n");
}

void
hypersparc_cache_enable()
{
	int i, ls, ts;
	u_int pcr, v;
	extern u_long dvma_cachealign;

	ls = CACHEINFO.c_linesize;
	ts = CACHEINFO.c_totalsize;

	pcr = lda(SRMMU_PCR, ASI_SRMMU);

	/*
	 * Setup the anti-aliasing constants and DVMA alignment constraint.
	 */
	cache_alias_dist = CACHEINFO.c_totalsize;
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;
	dvma_cachealign = cache_alias_dist;

	/* Now reset cache tag memory if cache not yet enabled */
	if ((pcr & HYPERSPARC_PCR_CE) == 0)
		for (i = 0; i < ts; i += ls) {
			sta(i, ASI_DCACHETAG, 0);
			while (lda(i, ASI_DCACHETAG))
				sta(i, ASI_DCACHETAG, 0);
		}

	pcr &= ~(HYPERSPARC_PCR_CE | HYPERSPARC_PCR_CM);

	hypersparc_cache_flush_all();

	pcr |= HYPERSPARC_PCR_CE;
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pcr |= HYPERSPARC_PCR_CM;

	sta(SRMMU_PCR, ASI_SRMMU, pcr);
	CACHEINFO.c_enabled = 1;

	/* XXX: should add support */
	if (CACHEINFO.c_hwflush)
		panic("cache_enable: can't handle 4M with hw-flush cache");

#ifdef notyet
	/*
	 * Enable instruction cache and, on single-processor machines,
	 * disable `Unimplemented Flush Traps'.
	 */
	v = HYPERSPARC_ICCR_ICE | (ncpu == 1 ? HYPERSPARC_ICCR_FTD : 0);
#else
	v = HYPERSPARC_ICCR_FTD | HYPERSPARC_ICCR_ICE;
#endif
	wrasr(v, HYPERSPARC_ASRNUM_ICCR);

	printf("cache enabled\n");
}

void
swift_cache_enable()
{
	int i, ls, ts;
	u_int pcr;

	cache_alias_dist = max(
		CACHEINFO.ic_totalsize / CACHEINFO.ic_associativity,
		CACHEINFO.dc_totalsize / CACHEINFO.dc_associativity);
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;

	pcr = lda(SRMMU_PCR, ASI_SRMMU);
	pcr |= (SWIFT_PCR_ICE | SWIFT_PCR_DCE);
	sta(SRMMU_PCR, ASI_SRMMU, pcr);

	/* Now reset cache tag memory if cache not yet enabled */
	ls = CACHEINFO.ic_linesize;
	ts = CACHEINFO.ic_totalsize;
	if ((pcr & SWIFT_PCR_ICE) == 0)
		for (i = 0; i < ts; i += ls)
			sta(i, ASI_ICACHETAG, 0);

	ls = CACHEINFO.dc_linesize;
	ts = CACHEINFO.dc_totalsize;
	if ((pcr & SWIFT_PCR_DCE) == 0)
		for (i = 0; i < ts; i += ls) {
			sta(i, ASI_DCACHETAG, 0);
			while (lda(i, ASI_DCACHETAG))
				sta(i, ASI_DCACHETAG, 0);
		}

	CACHEINFO.c_enabled = 1;
	printf("cache enabled\n");
}

void
cypress_cache_enable()
{
	int i, ls, ts;
	u_int pcr;
	extern u_long dvma_cachealign;

	cache_alias_dist = CACHEINFO.c_totalsize;
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;
	dvma_cachealign = cache_alias_dist;

	pcr = lda(SRMMU_PCR, ASI_SRMMU);
	pcr &= ~CYPRESS_PCR_CM;

	/* Now reset cache tag memory if cache not yet enabled */
	if ((pcr & CYPRESS_PCR_CE) == 0) {
		ls = CACHEINFO.c_linesize;
		ts = CACHEINFO.c_totalsize;
		for (i = 0; i < ts; i += ls)
			sta(i, ASI_DCACHETAG, 0);
		pcr |= CYPRESS_PCR_CE;
	}

	/* If put in write-back mode, turn it on */
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pcr |= CYPRESS_PCR_CM;

	sta(SRMMU_PCR, ASI_SRMMU, pcr);
	CACHEINFO.c_enabled = 1;
	printf("cache enabled\n");
}

void
turbosparc_cache_enable()
{
	int i, ls, ts;
	u_int pcr, pcf;

	cache_alias_dist = max(
		CACHEINFO.ic_totalsize / CACHEINFO.ic_associativity,
		CACHEINFO.dc_totalsize / CACHEINFO.dc_associativity);
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;

	pcr = lda(SRMMU_PCR, ASI_SRMMU);

	/* Now reset cache tag memory if cache not yet enabled */
	ls = CACHEINFO.ic_linesize;
	ts = CACHEINFO.ic_totalsize;
	if ((pcr & TURBOSPARC_PCR_ICE) == 0)
		for (i = 0; i < ts; i += ls)
			sta(i, ASI_ICACHETAG, 0);

	ls = CACHEINFO.dc_linesize;
	ts = CACHEINFO.dc_totalsize;
	if ((pcr & TURBOSPARC_PCR_DCE) == 0)
		for (i = 0; i < ts; i += ls) {
			sta(i, ASI_DCACHETAG, 0);
			while (lda(i, ASI_DCACHETAG))
				sta(i, ASI_DCACHETAG, 0);
		}

	pcr |= (TURBOSPARC_PCR_ICE | TURBOSPARC_PCR_DCE);
	sta(SRMMU_PCR, ASI_SRMMU, pcr);

	pcf = lda(SRMMU_PCFG, ASI_SRMMU);
	if (pcf & TURBOSPARC_PCFG_SNP)
		printf("DVMA coherent ");

	CACHEINFO.c_enabled = 1;
	printf("cache enabled\n");
}
#endif /* defined(SUN4M) */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
/*
 * Flush the current context from the cache.
 *
 * This is done by writing to each cache line in the `flush context'
 * address space (or, for hardware flush, once to each page in the
 * hardware flush space, for all cache pages).
 */
void
sun4_vcache_flush_context()
{
	char *p;
	int i, ls;

	cachestats.cs_ncxflush++;
	p = (char *)0;	/* addresses 0..cacheinfo.c_totalsize will do fine */
	if (CACHEINFO.c_hwflush) {
		ls = NBPG;
		i = CACHEINFO.c_totalsize >> PGSHIFT;
		for (; --i >= 0; p += ls)
			sta(p, ASI_HWFLUSHCTX, 0);
	} else {
		ls = CACHEINFO.c_linesize;
		i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
		for (; --i >= 0; p += ls)
			sta(p, ASI_FLUSHCTX, 0);
	}
}

/*
 * Flush the given virtual region from the cache.
 *
 * This is also done by writing to each cache line, except that
 * now the addresses must include the virtual region number, and
 * we use the `flush region' space.
 *
 * This function is only called on sun4's with 3-level MMUs; there's
 * no hw-flush space.
 */
void
sun4_vcache_flush_region(vreg)
	int vreg;
{
	int i, ls;
	char *p;

	cachestats.cs_nrgflush++;
	p = (char *)VRTOVA(vreg);	/* reg..reg+sz rather than 0..sz */
	ls = CACHEINFO.c_linesize;
	i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		sta(p, ASI_FLUSHREG, 0);
}

/*
 * Flush the given virtual segment from the cache.
 *
 * This is also done by writing to each cache line, except that
 * now the addresses must include the virtual segment number, and
 * we use the `flush segment' space.
 *
 * Again, for hardware, we just write each page (in hw-flush space).
 */
void
sun4_vcache_flush_segment(vreg, vseg)
	int vreg, vseg;
{
	int i, ls;
	char *p;

	cachestats.cs_nsgflush++;
	p = (char *)VSTOVA(vreg, vseg);	/* seg..seg+sz rather than 0..sz */
	if (CACHEINFO.c_hwflush) {
		ls = NBPG;
		i = CACHEINFO.c_totalsize >> PGSHIFT;
		for (; --i >= 0; p += ls)
			sta(p, ASI_HWFLUSHSEG, 0);
	} else {
		ls = CACHEINFO.c_linesize;
		i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
		for (; --i >= 0; p += ls)
			sta(p, ASI_FLUSHSEG, 0);
	}
}

/*
 * Flush the given virtual page from the cache.
 * (va is the actual address, and must be aligned on a page boundary.)
 * Again we write to each cache line.
 */
void
sun4_vcache_flush_page(va)
	int va;
{
	int i, ls;
	char *p;

#ifdef DEBUG
	if (va & PGOFSET)
		panic("cache_flush_page: asked to flush misaligned va 0x%x",va);
#endif

	cachestats.cs_npgflush++;
	p = (char *)va;
	if (CACHEINFO.c_hwflush)
		sta(p, ASI_HWFLUSHPG, 0);
	else {
		ls = CACHEINFO.c_linesize;
		i = NBPG >> CACHEINFO.c_l2linesize;
		for (; --i >= 0; p += ls)
			sta(p, ASI_FLUSHPG, 0);
	}
}

/*
 * Flush a range of virtual addresses (in the current context).
 * The first byte is at (base&~PGOFSET) and the last one is just
 * before byte (base+len).
 *
 * We choose the best of (context,segment,page) here.
 */

#define CACHE_FLUSH_MAGIC	(CACHEINFO.c_totalsize / NBPG)

void
sun4_cache_flush(base, len)
	caddr_t base;
	u_int len;
{
	int i, ls, baseoff;
	char *p;

	if (CACHEINFO.c_vactype == VAC_NONE)
		return;

	/*
	 * Figure out how much must be flushed.
	 *
	 * If we need to do CACHE_FLUSH_MAGIC pages,  we can do a segment
	 * in the same number of loop iterations.  We can also do the whole
	 * region. If we need to do between 2 and NSEGRG, do the region.
	 * If we need to do two or more regions, just go ahead and do the
	 * whole context. This might not be ideal (e.g., fsck likes to do
	 * 65536-byte reads, which might not necessarily be aligned).
	 *
	 * We could try to be sneaky here and use the direct mapping
	 * to avoid flushing things `below' the start and `above' the
	 * ending address (rather than rounding to whole pages and
	 * segments), but I did not want to debug that now and it is
	 * not clear it would help much.
	 *
	 * (XXX the magic number 16 is now wrong, must review policy)
	 */
	baseoff = (int)base & PGOFSET;
	i = (baseoff + len + PGOFSET) >> PGSHIFT;

	cachestats.cs_nraflush++;
#ifdef notyet
	cachestats.cs_ra[min(i, MAXCACHERANGE)]++;
#endif

	if (i < CACHE_FLUSH_MAGIC) {
		/* cache_flush_page, for i pages */
		p = (char *)((int)base & ~baseoff);
		if (CACHEINFO.c_hwflush) {
			for (; --i >= 0; p += NBPG)
				sta(p, ASI_HWFLUSHPG, 0);
		} else {
			ls = CACHEINFO.c_linesize;
			i <<= PGSHIFT - CACHEINFO.c_l2linesize;
			for (; --i >= 0; p += ls)
				sta(p, ASI_FLUSHPG, 0);
		}
		return;
	}
	baseoff = (u_int)base & SGOFSET;
	i = (baseoff + len + SGOFSET) >> SGSHIFT;
	if (i == 1)
		sun4_vcache_flush_segment(VA_VREG(base), VA_VSEG(base));
	else {
		if (HASSUN4_MMU3L) {
			baseoff = (u_int)base & RGOFSET;
			i = (baseoff + len + RGOFSET) >> RGSHIFT;
			if (i == 1)
				sun4_vcache_flush_region(VA_VREG(base));
			else
				sun4_vcache_flush_context();
		} else
			sun4_vcache_flush_context();
	}
}
#endif /* SUN4 || SUN4C || SUN4E */

#if defined(SUN4M)
/*
 * Flush the current context from the cache.
 *
 * This is done by writing to each cache line in the `flush context'
 * address space (or, for hardware flush, once to each page in the
 * hardware flush space, for all cache pages).
 */
void
srmmu_vcache_flush_context()
{
	char *p;
	int i, ls;

	cachestats.cs_ncxflush++;
	p = (char *)0;	/* addresses 0..cacheinfo.c_totalsize will do fine */
	ls = CACHEINFO.c_linesize;
	i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		sta(p, ASI_IDCACHELFC, 0);
}

/*
 * Flush the given virtual region from the cache.
 *
 * This is also done by writing to each cache line, except that
 * now the addresses must include the virtual region number, and
 * we use the `flush region' space.
 */
void
srmmu_vcache_flush_region(vreg)
	int vreg;
{
	int i, ls;
	char *p;

	cachestats.cs_nrgflush++;
	p = (char *)VRTOVA(vreg);	/* reg..reg+sz rather than 0..sz */
	ls = CACHEINFO.c_linesize;
	i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		sta(p, ASI_IDCACHELFR, 0);
}

/*
 * Flush the given virtual segment from the cache.
 *
 * This is also done by writing to each cache line, except that
 * now the addresses must include the virtual segment number, and
 * we use the `flush segment' space.
 *
 * Again, for hardware, we just write each page (in hw-flush space).
 */
void
srmmu_vcache_flush_segment(vreg, vseg)
	int vreg, vseg;
{
	int i, ls;
	char *p;

	cachestats.cs_nsgflush++;
	p = (char *)VSTOVA(vreg, vseg);	/* seg..seg+sz rather than 0..sz */
	ls = CACHEINFO.c_linesize;
	i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		sta(p, ASI_IDCACHELFS, 0);
}

/*
 * Flush the given virtual page from the cache.
 * (va is the actual address, and must be aligned on a page boundary.)
 * Again we write to each cache line.
 */
void
srmmu_vcache_flush_page(va)
	int va;
{
	int i, ls;
	char *p;

#ifdef DEBUG
	if (va & PGOFSET)
		panic("cache_flush_page: asked to flush misaligned va 0x%x",va);
#endif

	cachestats.cs_npgflush++;
	p = (char *)va;
	ls = CACHEINFO.c_linesize;
	i = NBPG >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		sta(p, ASI_IDCACHELFP, 0);
}

void
srmmu_cache_flush_all()
{
	srmmu_vcache_flush_context();
}

/*
 * Flush a range of virtual addresses (in the current context).
 * The first byte is at (base&~PGOFSET) and the last one is just
 * before byte (base+len).
 *
 * We choose the best of (context,segment,page) here.
 */

#define CACHE_FLUSH_MAGIC	(CACHEINFO.c_totalsize / NBPG)

void
srmmu_cache_flush(base, len)
	caddr_t base;
	u_int len;
{
	int i, ls, baseoff;
	char *p;

	/*
	 * Figure out how much must be flushed.
	 *
	 * If we need to do CACHE_FLUSH_MAGIC pages,  we can do a segment
	 * in the same number of loop iterations.  We can also do the whole
	 * region. If we need to do between 2 and NSEGRG, do the region.
	 * If we need to do two or more regions, just go ahead and do the
	 * whole context. This might not be ideal (e.g., fsck likes to do
	 * 65536-byte reads, which might not necessarily be aligned).
	 *
	 * We could try to be sneaky here and use the direct mapping
	 * to avoid flushing things `below' the start and `above' the
	 * ending address (rather than rounding to whole pages and
	 * segments), but I did not want to debug that now and it is
	 * not clear it would help much.
	 *
	 * (XXX the magic number 16 is now wrong, must review policy)
	 */
	baseoff = (int)base & PGOFSET;
	i = (baseoff + len + PGOFSET) >> PGSHIFT;

	cachestats.cs_nraflush++;
#ifdef notyet
	cachestats.cs_ra[min(i, MAXCACHERANGE)]++;
#endif

	if (i < CACHE_FLUSH_MAGIC) {
		/* cache_flush_page, for i pages */
		p = (char *)((int)base & ~baseoff);
		ls = CACHEINFO.c_linesize;
		i <<= PGSHIFT - CACHEINFO.c_l2linesize;
		for (; --i >= 0; p += ls)
			sta(p, ASI_IDCACHELFP, 0);
		return;
	}
	baseoff = (u_int)base & SGOFSET;
	i = (baseoff + len + SGOFSET) >> SGSHIFT;
	if (i == 1)
		srmmu_vcache_flush_segment(VA_VREG(base), VA_VSEG(base));
	else {
		baseoff = (u_int)base & RGOFSET;
		i = (baseoff + len + RGOFSET) >> RGSHIFT;
		if (i == 1)
			srmmu_vcache_flush_region(VA_VREG(base));
		else
			srmmu_vcache_flush_context();
	}
}

#ifndef	MS1_CACHEFLUSH_MAGIC
#define	MS1_CACHEFLUSH_MAGIC	0 /* 48 */
#endif
void
ms1_cache_flush(base, len)
	caddr_t base;
	u_int len;
{
	/*
	 * Although physically tagged, we still need to flush the
	 * data cache after (if we have a write-through cache) or before
	 * (in case of write-back caches) DMA operations.
	 */
#if MS1_CACHEFLUSH_MAGIC != 0
	if (len <= MS1_CACHEFLUSH_MAGIC) {
		/*
		 * If the range to be flushed is sufficiently small
		 * invalidate the covered cache lines by hand.
		 *
		 * The MicroSPARC I has a direct-mapped virtually addressed
		 * physically tagged data cache which is organised as
		 * 128 lines of 16 bytes. Virtual address bits [4-10]
		 * select the cache line. The cache tags are accessed
		 * through the standard DCACHE control space using the
		 * same address bits as those used to select the cache
		 * line in the virtual address.
		 *
		 * Note: we don't bother to compare the actual tags
		 * since that would require looking up physical addresses.
		 *
		 * The format of the tags we read from ASI_DCACHE control
		 * space is:
		 *
		 * 31     27 26            11 10         1 0
		 * +--------+----------------+------------+-+
		 * |  xxx   |    PA[26-11]   |    xxx     |V|
		 * +--------+----------------+------------+-+
		 *
		 * PA: bits 11-26 of the physical address
		 * V:  line valid bit
		 */
		int tagaddr = ((u_int)base & 0x7f0);

		len = roundup(len, 16);
		while (len != 0) {
			int tag = lda(tagaddr, ASI_DCACHETAG);
			if ((tag & 1) == 1) {
				/* Mark this cache line invalid */
				sta(tagaddr, ASI_DCACHETAG, 0);
			}
			len -= 16;
			tagaddr = (tagaddr + 16) & 0x7f0;
		}
	} else
#endif
		/* Flush entire data cache */
		sta(0, ASI_DCACHECLR, 0);
}

/*
 * Flush entire cache.
 */
void
ms1_cache_flush_all()
{

	/* Flash-clear both caches */
	sta(0, ASI_ICACHECLR, 0);
	sta(0, ASI_DCACHECLR, 0);
}

void
hypersparc_cache_flush_all()
{

	srmmu_vcache_flush_context();
	/* Flush instruction cache */
	hypersparc_pure_vcache_flush();
}

void
cypress_cache_flush_all()
{
	extern char kernel_text[];
	char *p;
	int i, ls;

	/* Fill the cache with known read-only content */
	p = (char *)kernel_text;
	ls = CACHEINFO.c_linesize;
	i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		(*(volatile char *)p);
}

void
viking_cache_flush(base, len)
	caddr_t base;
	u_int len;
{
	/*
	 * Although physically tagged, we still need to flush the
	 * data cache after (if we have a write-through cache) or before
	 * (in case of write-back caches) DMA operations.
	 */

}

void
viking_pcache_flush_line(va, pa)
	int va;
	int pa;
{
	/*
	 * Flush cache line corresponding to virtual address `va'
	 * which is mapped at physical address `pa'.
	 */
	extern char etext[];
	static char *base;
	int i;
	char *v;

	/*
	 * Construct a virtual address that hits the same cache line
	 * as PA, then read from 2*ASSOCIATIVITY-1 different physical
	 * locations (all different from PA).
	 */

#if 0
	if (base == 0) {
		cshift = CACHEINFO.ic_l2linesize;
		csize = CACHEINFO.ic_nlines << cshift;
		cmask = csize - 1;
		base = (char *)roundup((int)etext, csize);
	}

	v = base + (((va & cmask) >> cshift) << cshift);
	i = CACHEINFO.dc_associativity * 2 - 1;

	while (i--) {
		(*(volatile int *)v);
		v += csize;
	}
#else
#define cshift	5			/* CACHEINFO.ic_l2linesize */
#define csize	(128 << cshift)		/* CACHEINFO.ic_nlines << cshift */
#define cmask	(csize - 1)
#define cass	4			/* CACHEINFO.dc_associativity */

	if (base == 0)
		base = (char *)roundup((unsigned int)etext, csize);

	v = base + (((pa & cmask) >> cshift) << cshift);
	i = 2 * cass - 1;

	while (i--) {
		(*(volatile int *)v);
		v += csize;
	}
#undef cass
#undef cmask
#undef csize
#undef cshift
#endif
}

void
srmmu_pcache_flush_line(va, pa)
	int va;
	int pa;
{
	/*
	 * Flush cache line corresponding to virtual address `va'
	 * which is mapped at physical address `pa'.
	 */
	sta(va, ASI_IDCACHELFP, 0);
}
#endif /* SUN4M */
@


1.20
log
@sun4e (i.e. SPARCengine 1e) support. This platform is a mix between sun4 and
sun4c, as it has a sun4c OpenPROM but a sun4 8KB pagesize. VME devices are
not supported yet.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.19 2010/06/29 21:24:41 miod Exp $	*/
a72 5
#if defined(solbourne)
#include <machine/idt.h>
#include <machine/kap.h>
#endif

a891 56

#if defined(solbourne)
void
kap_cache_enable()
{
	kap_cache_flush(NULL, 0);
	sta(0, ASI_ICACHE_INVAL, 0);

	sta(ICU_CONF, ASI_PHYS_IO,
	    lda(ICU_CONF, ASI_PHYS_IO) & ~CONF_ICACHE_DISABLE);
	CACHEINFO.c_enabled = 1;

	printf("cache enabled\n");
}

void
kap_vcache_flush_context()
{
	kap_cache_flush(0, 0);
	sta(0, ASI_DCACHE_INVAL, 0);
	sta(0, ASI_ICACHE_INVAL, 0);
}

void
kap_vcache_flush_page(va)
	int va;
{
	kap_cache_flush((caddr_t)va, PAGE_SIZE);
}

void
kap_cache_flush(base, len)
	caddr_t base;
	u_int len;
{
	u_int line;
	u_int32_t mmcr;

	/*
	 * Due to the small size of the data cache and the fact that we
	 * would be flushing 4 bytes by 4 bytes, it is faster to flush
	 * the whole cache instead.
	 */

	mmcr = lda(0, ASI_MMCR) & ~(MMCR_DSET0 | MMCR_DSET1);
	/* flush bank 0 */
	sta(0, ASI_MMCR, mmcr | MMCR_DSET0);
	for (line = 0; line < DCACHE_LINE(DCACHE_LINES); line += DCACHE_INCR)
		(void)lda(line, ASI_DCACHE_FLUSH);
	/* flush bank 1 */
	sta(0, ASI_MMCR, mmcr | MMCR_DSET1);
	for (line = 0; line < DCACHE_LINE(DCACHE_LINES); line += DCACHE_INCR)
		(void)lda(line, ASI_DCACHE_FLUSH);
}

#endif	/* solbourne */
@


1.19
log
@Rework cypress cpu cache setup, and enable writeback mode. Adapted from
NetBSD; tested on SM100.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.18 2007/01/22 19:39:33 miod Exp $	*/
d92 1
a92 1
	cache_alias_bits = CPU_ISSUN4
d95 1
a95 1
	cache_alias_dist = CPU_ISSUN4
d360 1
a360 1
#if defined(SUN4) || defined(SUN4C)
d551 1
a551 1
#endif /* defined(SUN4) || defined(SUN4C) */
@


1.18
log
@Allow pmap_zeropage and pmap_copypage to work with cache enabled on
MicroSPARC-1, since cache is not write allocate; also provide a smarter routine
to flush a small section of the data cache, but do not enable it since we
almost never flush less than 2KB (which is the whole MS1 D cache size);
adapted from NetBSD, tested on SPARCclassic.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.17 2005/04/19 21:30:20 miod Exp $	*/
d292 1
d296 1
d299 1
a299 1
	pcr &= ~(CYPRESS_PCR_CE | CYPRESS_PCR_CM);
d302 4
a305 4
	ls = CACHEINFO.c_linesize;
	ts = CACHEINFO.c_totalsize;
	if ((pcr & CYPRESS_PCR_CE) == 0)
		for (i = 0; i < ts; i += ls) {
d307 2
a308 3
			while (lda(i, ASI_DCACHETAG))
				sta(i, ASI_DCACHETAG, 0);
		}
a309 5
	pcr |= CYPRESS_PCR_CE;

#if 1
	pcr &= ~CYPRESS_PCR_CM;		/* XXX Disable write-back mode */
#else
a312 1
#endif
@


1.17
log
@As a late birthday present, a preliminary port to the Solbourne IDT systems
(S3000, S4000 and S4000DX).

Currently limited to diskless and serial console, and userland has issues.
Things will get better in the near future.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.16 2002/03/13 00:24:21 miod Exp $	*/
d90 1
a90 1
	register u_int i, lim, ls, ts;
d126 4
a129 2
	cache_alias_bits = GUESS_CACHE_ALIAS_BITS;
	cache_alias_dist = GUESS_CACHE_ALIAS_DIST;
d144 9
d376 2
a377 2
	register char *p;
	register int i, ls;
d406 1
a406 1
	register int vreg;
d408 2
a409 2
	register int i, ls;
	register char *p;
d430 1
a430 1
	register int vreg, vseg;
d432 2
a433 2
	register int i, ls;
	register char *p;
d459 2
a460 2
	register int i, ls;
	register char *p;
d492 1
a492 1
	register u_int len;
d494 2
a495 2
	register int i, ls, baseoff;
	register char *p;
d569 2
a570 2
	register char *p;
	register int i, ls;
d589 1
a589 1
	register int vreg;
d591 2
a592 2
	register int i, ls;
	register char *p;
d613 1
a613 1
	register int vreg, vseg;
d615 2
a616 2
	register int i, ls;
	register char *p;
d635 2
a636 2
	register int i, ls;
	register char *p;
d670 1
a670 1
	register u_int len;
d672 2
a673 2
	register int i, ls, baseoff;
	register char *p;
d724 3
d730 1
a730 1
	register u_int len;
d737 44
a780 3

	/* XXX investigate other methods instead of blowing the entire cache */
	sta(0, ASI_DCACHECLR, 0);
d822 1
a822 1
	register u_int len;
d935 1
a935 1
	register u_int len;
@


1.16
log
@On sparc, PAGE_SIZE and friends might not be a compile-time constant.
Instead of using a homegrown set of variables in this case, rely on uvmexp
fields once uvm has been initialized.

This requires a few #include <uvm/uvm_extern.h> here and there in the kernel
as well.

Idea from art@@, changes by me.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.15 2001/12/19 08:58:05 art Exp $	*/
d73 5
d354 1
d545 1
a545 1

d847 56
@


1.15
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.13 2000/03/17 21:54:07 deraadt Exp $	*/
d65 2
a67 1
#include <machine/pte.h>
@


1.14
log
@Change pcache_flush_line to pcache_flush_page since this is the only way
we use the function.
At the same time fix the pte pool backend functions to allocate and map
the memory themselves.
@
text
@d771 3
a773 3
viking_pcache_flush_page(pa, invalidate_only)
	paddr_t pa;
	int invalidate_only;
d775 8
a782 1
	int set, i;
d785 3
a787 4
	 * The viking's on-chip data cache is 4-way set associative,
	 * consisting of 128 sets, each holding 4 lines of 32 bytes.
	 * Note that one 4096 byte page exactly covers all 128 sets
	 * in the cache.
d789 31
a819 84
	if (invalidate_only) {
		u_int pa_tag = (pa >> 12);
		u_int tagaddr;
		u_int64_t tag;

		/*
		 * Loop over all sets and invalidate all entries tagged
		 * with the given physical address by resetting the cache
		 * tag in ASI_DCACHETAG control space.
		 *
		 * The address format for accessing a tag is:
		 *
		 * 31   30      27   26                  11      5 4  3 2    0
		 * +------+-----+------+-------//--------+--------+----+-----+
		 * | type | xxx | line |       xxx       |  set   | xx | 0   |
		 * +------+-----+------+-------//--------+--------+----+-----+
		 *
		 * set:  the cache set tag to be read (0-127)
		 * line: the line within the set (0-3)
		 * type: 1: read set tag; 2: read physical tag
		 *
		 * The (type 2) tag read from this address is a 64-bit word
		 * formatted as follows:
		 *
		 *          5         4         4
		 * 63       6         8         0            23               0
		 * +-------+-+-------+-+-------+-+-----------+----------------+
		 * |  xxx  |V|  xxx  |D|  xxx  |S|    xxx    |    PA[35-12]   |
		 * +-------+-+-------+-+-------+-+-----------+----------------+
		 *
		 * PA: bits 12-35 of the physical address
		 * S:  line shared bit
		 * D:  line dirty bit
		 * V:  line valid bit
		 */

#define VIKING_DCACHETAG_S	0x0000010000000000UL	/* line valid bit */
#define VIKING_DCACHETAG_D	0x0001000000000000UL	/* line dirty bit */
#define VIKING_DCACHETAG_V	0x0100000000000000UL	/* line shared bit */
#define VIKING_DCACHETAG_PAMASK	0x0000000000ffffffUL	/* PA tag field */

		for (set = 0; set < 128; set++) {
			/* Set set number and access type */
			tagaddr = (set << 5) | (2 << 30);

			/* Examine the tag for each line in the set */
			for (i = 0 ; i < 4; i++) {
				tag = ldda(tagaddr | (i << 26), ASI_DCACHETAG);
				/*
				 * If this is a valid tag and the PA field
				 * matches clear the tag.
				 */
				if ((tag & VIKING_DCACHETAG_PAMASK) == pa_tag &&
				    (tag & VIKING_DCACHETAG_V) != 0)
					stda(tagaddr | (i << 26),
					     ASI_DCACHETAG, 0);
			}
		}

	} else {
		extern char kernel_text[];

		/*
		 * Force the cache to validate its backing memory
		 * by displacing all cache lines with known read-only
		 * content from the start of kernel text.
		 *
		 * Note that this thrashes the entire cache. However,
		 * we currently only need to call upon this code
		 * once at boot time.
		 */
		for (set = 0; set < 128; set++) {
			int *v = (int *)(kernel_text + (set << 5));

			/*
			 * We need to read (2*associativity-1) different
			 * locations to be sure to displace the entire set.
			 */
			i = 2 * 4 - 1;
			while (i--) {
				(*(volatile int *)v);
				v += 4096;
			}
		}
d821 17
@


1.14.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.14 2001/12/05 14:40:48 art Exp $	*/
a64 2
#include <uvm/uvm_extern.h>

d66 1
@


1.13
log
@move more sun4m stuff behind an #ifdef
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.12 2000/02/21 14:51:20 art Exp $	*/
d771 3
a773 3
viking_pcache_flush_line(va, pa)
	int va;
	int pa;
d775 1
a775 8
	/*
	 * Flush cache line corresponding to virtual address `va'
	 * which is mapped at physical address `pa'.
	 */
	extern char etext[];
	static char *base;
	int i;
	char *v;
d778 4
a781 3
	 * Construct a virtual address that hits the same cache line
	 * as PA, then read from 2*ASSOCIATIVITY-1 different physical
	 * locations (all different from PA).
d783 58
d842 2
a843 7
#if 0
	if (base == 0) {
		cshift = CACHEINFO.ic_l2linesize;
		csize = CACHEINFO.ic_nlines << cshift;
		cmask = csize - 1;
		base = (char *)roundup((int)etext, csize);
	}
d845 22
a866 6
	v = base + (((va & cmask) >> cshift) << cshift);
	i = CACHEINFO.dc_associativity * 2 - 1;

	while (i--) {
		(*(volatile int *)v);
		v += csize;
a867 33
#else
#define cshift	5			/* CACHEINFO.ic_l2linesize */
#define csize	(128 << cshift)		/* CACHEINFO.ic_nlines << cshift */
#define cmask	(csize - 1)
#define cass	4			/* CACHEINFO.dc_associativity */

	if (base == 0)
		base = (char *)roundup((unsigned int)etext, csize);

	v = base + (((pa & cmask) >> cshift) << cshift);
	i = 2 * cass - 1;

	while (i--) {
		(*(volatile int *)v);
		v += csize;
	}
#undef cass
#undef cmask
#undef csize
#undef cshift
#endif
}

void
srmmu_pcache_flush_line(va, pa)
	int va;
	int pa;
{
	/*
	 * Flush cache line corresponding to virtual address `va'
	 * which is mapped at physical address `pa'.
	 */
	sta(va, ASI_IDCACHELFP, 0);
@


1.12
log
@When mapping something into iommu space hypersparc requires us to align it
so that cache_alias_bits match in the kernel mapping and the iommu mapping.
(see code for better explaination).
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.11 2000/02/21 14:43:35 art Exp $	*/
d114 1
d346 1
@


1.11
log
@Take the hypersparc cache size from prom, enable instruction cache and
disable "Unimplemented Flush Traps".
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.10 2000/02/19 21:45:55 art Exp $	*/
a182 1
#ifdef notyet
a183 1
#endif
a194 1
#ifdef notyet
a195 1
#endif
@


1.10
log
@Add two new cache operations.
cache_flush_all - (pretty obvious).
pure_vcache_flush - flush all VIVT caches (needed on context switch).
(From NetBSD).
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.9 1999/07/20 11:07:09 art Exp $	*/
d182 4
a185 1
	u_int pcr;
d193 1
a193 2
	 * First we determine what type of cache we have, and
	 * setup the anti-aliasing constants appropriately.
d195 5
a199 7
	if (pcr & HYPERSPARC_PCR_CS) {
		cache_alias_bits = CACHE_ALIAS_BITS_HS256k;
		cache_alias_dist = CACHE_ALIAS_DIST_HS256k;
	} else {
		cache_alias_bits = CACHE_ALIAS_BITS_HS128k;
		cache_alias_dist = CACHE_ALIAS_DIST_HS128k;
	}
d209 8
a216 2
	/* Enable write-back cache */
	pcr |= (HYPERSPARC_PCR_CE | HYPERSPARC_PCR_CM);
d223 11
@


1.9
log
@don't cache pagetables on microsparc 1 either. From NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.8 1999/07/05 16:09:05 art Exp $	*/
d618 6
d704 36
@


1.9.4.1
log
@Merge in recent code from the trunk
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.10 2000/02/19 21:45:55 art Exp $	*/
a617 6
void
srmmu_cache_flush_all()
{
	srmmu_vcache_flush_context();
}

a697 36
}

/*
 * Flush entire cache.
 */
void
ms1_cache_flush_all()
{

	/* Flash-clear both caches */
	sta(0, ASI_ICACHECLR, 0);
	sta(0, ASI_DCACHECLR, 0);
}

void
hypersparc_cache_flush_all()
{

	srmmu_vcache_flush_context();
	/* Flush instruction cache */
	hypersparc_pure_vcache_flush();
}

void
cypress_cache_flush_all()
{
	extern char kernel_text[];
	char *p;
	int i, ls;

	/* Fill the cache with known read-only content */
	p = (char *)kernel_text;
	ls = CACHEINFO.c_linesize;
	i = CACHEINFO.c_totalsize >> CACHEINFO.c_l2linesize;
	for (; --i >= 0; p += ls)
		(*(volatile char *)p);
@


1.9.4.2
log
@sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.12 2000/02/21 14:51:20 art Exp $	*/
d182 1
a182 2
	u_int pcr, v;
	extern u_long dvma_cachealign;
d190 2
a191 1
	 * Setup the anti-aliasing constants and DVMA alignment constraint.
d193 7
a199 3
	cache_alias_dist = CACHEINFO.c_totalsize;
	cache_alias_bits = (cache_alias_dist - 1) & ~PGOFSET;
	dvma_cachealign = cache_alias_dist;
d209 2
a210 8
	pcr &= ~(HYPERSPARC_PCR_CE | HYPERSPARC_PCR_CM);

	hypersparc_cache_flush_all();

	pcr |= HYPERSPARC_PCR_CE;
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pcr |= HYPERSPARC_PCR_CM;

a216 11

#ifdef notyet
	/*
	 * Enable instruction cache and, on single-processor machines,
	 * disable `Unimplemented Flush Traps'.
	 */
	v = HYPERSPARC_ICCR_ICE | (ncpu == 1 ? HYPERSPARC_ICCR_FTD : 0);
#else
	v = HYPERSPARC_ICCR_FTD | HYPERSPARC_ICCR_ICE;
#endif
	wrasr(v, HYPERSPARC_ASRNUM_ICCR);
@


1.9.4.3
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a113 1
#if defined(SUN4M)
a344 1
#endif /* defined(SUN4M) */
@


1.9.4.4
log
@Merge in -current from about a week ago
@
text
@a64 2
#include <uvm/uvm_extern.h>

d66 1
@


1.8
log
@don't cache pagetables on a microsparc 2, From NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD: cache.c,v 1.7 1999/03/22 07:17:13 deraadt Exp $	*/
/*	$NetBSD: cache.c,v 1.33 1997/07/29 09:41:56 fair Exp $ */
a131 2

	cpuinfo.flags |= CPUFLG_CACHEPAGETABLES;
@


1.7
log
@workaround for STP1020A errata #148494
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.6 1999/03/15 17:55:31 deraadt Exp $	*/
a253 3
	/* XXX - assume that an MS2 with ecache is really a turbo in disguise */
	if (CACHEINFO.ec_totalsize == 0)
		cpuinfo.flags |= CPUFLG_CACHEPAGETABLES; /* Ok to cache PTEs */
@


1.6
log
@on cypress, disable write caching for now
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.5 1999/03/03 22:02:21 jason Exp $	*/
d205 1
a205 1
		for (i = 0; i < ts; i += ls)
d207 3
d248 1
a248 1
		for (i = 0; i < ts; i += ls)
d250 3
d277 1
a277 1
		for (i = 0; i < ts; i += ls)
d279 3
d321 1
a321 1
		for (i = 0; i < ts; i += ls)
d323 3
@


1.5
log
@unsigned vs. signed nit
@
text
@d1 1
a1 1
/*	$OpenBSD: cache.c,v 1.4 1997/08/08 08:27:03 downsj Exp $	*/
d275 4
d282 2
@


1.4
log
@Mostly sync to NetBSD-current 970804.

GENERIC currently compiles and runs; some devices (isp) are not complete and
not yet enabled.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d742 1
a742 1
		base = (char *)roundup((int)etext, csize);
@


1.3
log
@MMU_3L always on for sun4, what the heck
@
text
@d1 2
a2 1
/*	$NetBSD: cache.c,v 1.8.4.1 1996/06/12 20:40:35 pk Exp $ */
d70 1
a71 1
enum vactype vactype;
a73 1
#if defined(SUN4M)
a75 36
#endif

/*
 * A few quick macros to allow for different ASI's between 4M/4/4C machines
 */
#define flushls_ctx(p)	do {			\
	CPU_ISSUN4M				\
		? sta(p, ASI_IDCACHELFC, 0)	\
		: sta(p, ASI_FLUSHCTX, 0);	\
} while (0)
#define flushls_reg(p)	do {			\
	CPU_ISSUN4M				\
		? sta(p, ASI_IDCACHELFR, 0)	\
		: sta(p, ASI_FLUSHREG, 0);	\
} while (0)
#define flushls_seg(p)	do {			\
	CPU_ISSUN4M				\
		? sta(p, ASI_IDCACHELFS, 0)	\
		: sta(p, ASI_FLUSHSEG, 0);	\
} while (0)
#define flushls_pag(p)	do {			\
	CPU_ISSUN4M				\
		? sta(p, ASI_IDCACHELFP, 0)	\
		: sta(p, ASI_FLUSHPG, 0);	\
} while (0)
#define flushls_usr(p)	do {			\
	if (CPU_ISSUN4M)			\
		sta(p, ASI_IDCACHELFU, 0);	\
while(0)

/* XXX: (ABB) need more generic Sun4m cache support */
#if !defined(SUN4) && !defined(SUN4C)
#define	ASI_HWFLUSHSEG	0x05	/* hardware assisted version of FLUSHSEG */
#define	ASI_HWFLUSHPG	0x06	/* hardware assisted version of FLUSHPG */
#define	ASI_HWFLUSHCTX	0x07	/* hardware assisted version of FLUSHCTX */
#endif
d82 1
a82 1
cache_enable()
d86 79
a164 2
	ls = cacheinfo.c_linesize;
	ts = cacheinfo.c_totalsize;
d166 24
a189 87
	if (CPU_ISSUN4M) {
		i = lda(SRMMU_PCR, ASI_SRMMU);
		switch (mmumod) {
		case SUN4M_MMU_HS:	/* HyperSPARC */
			/*
			 * First we determine what type of cache we have, and
			 * setup the anti-aliasing constants appropriately.
			 */
			if (i & SRMMU_PCR_CS) {
				cache_alias_bits = CACHE_ALIAS_BITS_HS256k;
				cache_alias_dist = CACHE_ALIAS_DIST_HS256k;
			} else {
				cache_alias_bits = CACHE_ALIAS_BITS_HS128k;
				cache_alias_dist = CACHE_ALIAS_DIST_HS128k;
			}
			/* Now reset cache tag memory */
			for (i = 0, lim = ts; i < lim; i += ls)
				sta(i, ASI_DCACHETAG, 0);

			sta(SRMMU_PCR, ASI_SRMMU, /* Enable write-back cache */
			    lda(SRMMU_PCR, ASI_SRMMU) | SRMMU_PCR_CE |
				SRMMU_PCR_CM);
			cacheinfo.c_enabled = 1;
			vactype = VAC_NONE;
			/* HyperSPARC uses phys. tagged cache */

			/* XXX: should add support */
			if (cacheinfo.c_hwflush)
				panic("cache_enable: can't handle 4M with hw-flush cache");

			printf("cache enabled\n");
			break;

		case SUN4M_MMU_SS:	/* SuperSPARC */
			if ((cpumod & 0xf0) != (SUN4M_SS & 0xf0)) {
				printf(
			"cache NOT enabled for %x/%x cpu/mmu combination\n",
					cpumod, mmumod);
				break;	/* ugh, SS and MS have same MMU # */
			}
			cache_alias_bits = CACHE_ALIAS_BITS_SS;
			cache_alias_dist = CACHE_ALIAS_DIST_SS;

			/* We "flash-clear" the I/D caches. */
			sta(0x80000000, ASI_ICACHECLR, 0); /* Unlock */
			sta(0, ASI_ICACHECLR, 0);	/* clear */
			sta(0x80000000, ASI_DCACHECLR, 0);
			sta(0, ASI_DCACHECLR, 0);

			/* Turn on caches via MMU */
			sta(SRMMU_PCR, ASI_SRMMU,
			    lda(SRMMU_PCR,ASI_SRMMU) | SRMMU_PCR_DCE |
				SRMMU_PCR_ICE);
			cacheinfo.c_enabled = cacheinfo.dc_enabled = 1;

			/* Now try to turn on MultiCache if it exists */
			/* XXX (ABB) THIS IS BROKEN MUST FIX */
			if (0&&(lda(SRMMU_PCR, ASI_SRMMU) & SRMMU_PCR_MB) == 0
				&& cacheinfo.ec_totalsize > 0) {
				/* Multicache controller */
				sta(MXCC_ENABLE_ADDR, ASI_CONTROL,
				    lda(MXCC_ENABLE_ADDR, ASI_CONTROL) |
					MXCC_ENABLE_BIT);
				cacheinfo.ec_enabled = 1;
			}
			printf("cache enabled\n");
			break;
		case SUN4M_MMU_MS1: /* MicroSPARC */
			/* We "flash-clear" the I/D caches. */
			sta(0, ASI_ICACHECLR, 0);	/* clear */
			sta(0, ASI_DCACHECLR, 0);

			/* Turn on caches via MMU */
			sta(SRMMU_PCR, ASI_SRMMU,
			    lda(SRMMU_PCR,ASI_SRMMU) | SRMMU_PCR_DCE |
				SRMMU_PCR_ICE);
			cacheinfo.c_enabled = cacheinfo.dc_enabled = 1;

			printf("cache enabled\n");
			break;

		default:
			printf("Unknown MMU architecture...cache disabled.\n");
			/* XXX: not HyperSPARC -- guess for now */
			cache_alias_bits = GUESS_CACHE_ALIAS_BITS;
			cache_alias_dist = GUESS_CACHE_ALIAS_DIST;
		}
d191 7
d199 17
d217 8
a224 2
		for (i = AC_CACHETAGS, lim = i + ts; i < lim; i += ls)
			sta(i, ASI_CONTROL, 0);
d226 62
a287 3
		stba(AC_SYSENABLE, ASI_CONTROL,
		     lduba(AC_SYSENABLE, ASI_CONTROL) | SYSEN_CACHE);
		cacheinfo.c_enabled = 1;
d289 26
a314 1
		printf("cache enabled\n");
d316 2
a317 8
#ifdef notyet
		if (cpumod == SUN4_400) {
			stba(AC_SYSENABLE, ASI_CONTROL,
			     lduba(AC_SYSENABLE, ASI_CONTROL) | SYSEN_IOCACHE);
			printf("iocache enabled\n");
		}
#endif
	}
d328 1
a328 1
cache_flush_context()
d335 1
a335 1
	if (cacheinfo.c_hwflush) {
d337 1
a337 1
		i = cacheinfo.c_totalsize >> PGSHIFT;
d341 2
a342 2
		ls = cacheinfo.c_linesize;
		i = cacheinfo.c_totalsize >> cacheinfo.c_l2linesize;
d344 1
a344 1
			flushls_ctx(p);
a347 1
#if defined(SUN4) || defined(SUN4M)
d355 1
a355 1
 * This function is only called on sun4m's or sun4's with 3-level MMUs; there's
d359 1
a359 1
cache_flush_region(vreg)
d367 2
a368 2
	ls = cacheinfo.c_linesize;
	i = cacheinfo.c_totalsize >> cacheinfo.c_l2linesize;
d370 1
a370 1
		flushls_reg(p);
a371 1
#endif
d383 1
a383 1
cache_flush_segment(vreg, vseg)
d391 1
a391 1
	if (cacheinfo.c_hwflush) {
d393 1
a393 1
		i = cacheinfo.c_totalsize >> PGSHIFT;
d397 2
a398 2
		ls = cacheinfo.c_linesize;
		i = cacheinfo.c_totalsize >> cacheinfo.c_l2linesize;
d400 1
a400 1
			flushls_seg(p);
d410 1
a410 1
cache_flush_page(va)
d418 1
a418 1
		panic("cache_flush_page: asked to flush misaligned va %x",va);
d423 1
a423 1
	if (cacheinfo.c_hwflush)
d426 2
a427 2
		ls = cacheinfo.c_linesize;
		i = NBPG >> cacheinfo.c_l2linesize;
d429 1
a429 1
			flushls_pag(p);
d441 1
a441 1
#define CACHE_FLUSH_MAGIC	(cacheinfo.c_totalsize / NBPG)
d444 1
a444 1
cache_flush(base, len)
a449 3
#if defined(SUN4)
	extern int mmu_3l;
#endif
d451 3
a453 1
#if defined(SUN4M)
d455 14
a468 3
	 * Although physically tagged, we still need to flush the
	 * data cache after (if we have a write-through cache) or before
	 * (in case of write-back caches) DMA operations.
d470 1
a470 1
	 * SS10s and 20s (supersparcs) use cached DVMA, no need to flush.
d472 20
a491 4
	if (CPU_ISSUN4M && mmumod != SUN4M_MMU_SS) {
		/* XXX (ABB) - Need more generic cache interface */
		/* XXX above test conflicts on MicroSPARC (SUN4M_MMU_MS=_SS) */
		sta(0, ASI_DCACHECLR, 0);
d494 101
d597 25
a621 2
	if (vactype == VAC_NONE)
		return;
d652 4
a655 9
		if (cacheinfo.c_hwflush) {
			for (; --i >= 0; p += NBPG)
				sta(p, ASI_HWFLUSHPG, 0);
		} else {
			ls = cacheinfo.c_linesize;
			i <<= PGSHIFT - cacheinfo.c_l2linesize;
			for (; --i >= 0; p += ls)
				flushls_pag(p);
		}
d661 1
a661 1
		cache_flush_segment(VA_VREG(base), VA_VSEG(base));
a662 1
#if defined(SUN4) || defined(SUN4M)
d665 70
a734 5
		if (i == 1
#if !defined(SUN4)
			&& CPU_ISSUN4M
#elif !defined(SUN4M)
			&& mmu_3l
d736 19
a754 1
			&& (CPU_ISSUN4M || mmu_3l)
a755 6
				)
			cache_flush_region(VA_VREG(base));
		else
#endif
			cache_flush_context();
	}
d757 13
@


1.2
log
@netbsd port, now we merge our changes back in
@
text
@d262 1
a262 1
#if defined(MMU_3L) || defined(SUN4M)
d366 1
a366 1
#if defined(MMU_3L)
d434 1
a434 1
#if defined(MMU_3L) || defined(SUN4M)
d438 1
a438 1
#if !defined(MMU_3L)
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: cache.c,v 1.5 1995/04/13 14:32:44 pk Exp $ */
d4 2
d15 1
d22 1
d30 2
d51 1
d62 1
d73 40
a123 2
	for (i = AC_CACHETAGS, lim = i + ts; i < lim; i += ls)
		sta(i, ASI_CONTROL, 0);
d125 96
a220 3
	stba(AC_SYSENABLE, ASI_CONTROL,
	    lduba(AC_SYSENABLE, ASI_CONTROL) | SYSEN_CACHE);
	cacheinfo.c_enabled = 1;
d222 1
a222 1
	printf("cache enabled\n");
d225 6
a230 4
	if (cpumod == SUN4_400) {
		stba(AC_SYSENABLE, ASI_CONTROL,
		    lduba(AC_SYSENABLE, ASI_CONTROL) | SYSEN_IOCACHE);
		printf("iocache enabled\n");
a231 1
#endif
a233 1

d258 1
a258 1
			sta(p, ASI_FLUSHCTX, 0);
d262 1
a262 1
#ifdef MMU_3L
d270 1
a270 1
 * This function is only called on sun4's with 3-level MMUs; there's
d285 1
a285 1
		sta(p, ASI_FLUSHREG, 0);
d316 1
a316 1
			sta(p, ASI_FLUSHSEG, 0);
d332 5
d345 1
a345 1
			sta(p, ASI_FLUSHPG, 0);
d356 3
d366 19
d392 6
a397 5
	 * If we need to do 16 pages, we can do a segment in the same
	 * number of loop iterations.  We can also do the context.  If
	 * we would need to do two segments, do the whole context.
	 * This might not be ideal (e.g., fsck likes to do 65536-byte
	 * reads, which might not necessarily be aligned).
d415 1
a415 1
	if (i <= 15) {
d425 1
a425 1
				sta(p, ASI_FLUSHPG, 0);
d433 18
a450 2
	else
		cache_flush_context();
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
