head	1.14;
access;
symbols
	OPENBSD_6_0:1.14.0.10
	OPENBSD_6_0_BASE:1.14
	OPENBSD_5_9:1.14.0.6
	OPENBSD_5_9_BASE:1.14
	OPENBSD_5_8:1.14.0.8
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.14.0.2
	OPENBSD_5_7_BASE:1.14
	OPENBSD_5_6:1.14.0.4
	OPENBSD_5_6_BASE:1.14
	OPENBSD_5_5:1.12.0.28
	OPENBSD_5_5_BASE:1.12
	OPENBSD_5_4:1.12.0.24
	OPENBSD_5_4_BASE:1.12
	OPENBSD_5_3:1.12.0.22
	OPENBSD_5_3_BASE:1.12
	OPENBSD_5_2:1.12.0.20
	OPENBSD_5_2_BASE:1.12
	OPENBSD_5_1_BASE:1.12
	OPENBSD_5_1:1.12.0.18
	OPENBSD_5_0:1.12.0.16
	OPENBSD_5_0_BASE:1.12
	OPENBSD_4_9:1.12.0.14
	OPENBSD_4_9_BASE:1.12
	OPENBSD_4_8:1.12.0.12
	OPENBSD_4_8_BASE:1.12
	OPENBSD_4_7:1.12.0.8
	OPENBSD_4_7_BASE:1.12
	OPENBSD_4_6:1.12.0.10
	OPENBSD_4_6_BASE:1.12
	OPENBSD_4_5:1.12.0.6
	OPENBSD_4_5_BASE:1.12
	OPENBSD_4_4:1.12.0.4
	OPENBSD_4_4_BASE:1.12
	OPENBSD_4_3:1.12.0.2
	OPENBSD_4_3_BASE:1.12
	OPENBSD_4_2:1.11.0.10
	OPENBSD_4_2_BASE:1.11
	OPENBSD_4_1:1.11.0.8
	OPENBSD_4_1_BASE:1.11
	OPENBSD_4_0:1.11.0.6
	OPENBSD_4_0_BASE:1.11
	OPENBSD_3_9:1.11.0.4
	OPENBSD_3_9_BASE:1.11
	OPENBSD_3_8:1.11.0.2
	OPENBSD_3_8_BASE:1.11
	OPENBSD_3_7:1.10.0.8
	OPENBSD_3_7_BASE:1.10
	OPENBSD_3_6:1.10.0.6
	OPENBSD_3_6_BASE:1.10
	SMP_SYNC_A:1.10
	SMP_SYNC_B:1.10
	OPENBSD_3_5:1.10.0.4
	OPENBSD_3_5_BASE:1.10
	OPENBSD_3_4:1.10.0.2
	OPENBSD_3_4_BASE:1.10
	UBC_SYNC_A:1.8
	OPENBSD_3_3:1.8.0.28
	OPENBSD_3_3_BASE:1.8
	OPENBSD_3_2:1.8.0.26
	OPENBSD_3_2_BASE:1.8
	OPENBSD_3_1:1.8.0.24
	OPENBSD_3_1_BASE:1.8
	UBC_SYNC_B:1.8
	UBC:1.8.0.22
	UBC_BASE:1.8
	OPENBSD_3_0:1.8.0.20
	OPENBSD_3_0_BASE:1.8
	OPENBSD_2_9_BASE:1.8
	OPENBSD_2_9:1.8.0.18
	OPENBSD_2_8:1.8.0.16
	OPENBSD_2_8_BASE:1.8
	OPENBSD_2_7:1.8.0.14
	OPENBSD_2_7_BASE:1.8
	SMP:1.8.0.12
	SMP_BASE:1.8
	kame_19991208:1.8
	OPENBSD_2_6:1.8.0.10
	OPENBSD_2_6_BASE:1.8
	OPENBSD_2_5:1.8.0.8
	OPENBSD_2_5_BASE:1.8
	OPENBSD_2_4:1.8.0.6
	OPENBSD_2_4_BASE:1.8
	OPENBSD_2_3:1.8.0.4
	OPENBSD_2_3_BASE:1.8
	OPENBSD_2_2:1.8.0.2
	OPENBSD_2_2_BASE:1.8
	OPENBSD_2_1:1.6.0.4
	OPENBSD_2_1_BASE:1.6
	OPENBSD_2_0:1.6.0.2
	OPENBSD_2_0_BASE:1.6
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.14
date	2014.07.22.10.35.35;	author mpi;	state Exp;
branches;
next	1.13;
commitid	EXpEDqO4x724IPl8;

1.13
date	2014.03.29.18.09.30;	author guenther;	state Exp;
branches;
next	1.12;

1.12
date	2008.02.16.23.02.41;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2005.05.03.00.39.39;	author brad;	state Exp;
branches;
next	1.10;

1.10
date	2003.06.04.22.08.17;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	2003.06.02.23.27.55;	author millert;	state Exp;
branches;
next	1.8;

1.8
date	97.08.08.08.27.15;	author downsj;	state Exp;
branches
	1.8.12.1;
next	1.7;

1.7
date	97.06.25.13.16.04;	author downsj;	state Exp;
branches;
next	1.6;

1.6
date	96.09.16.06.05.00;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	96.08.11.05.35.10;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	95.11.21.13.00.25;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	95.11.07.20.22.45;	author chuck;	state Exp;
branches;
next	1.2;

1.2
date	95.10.18.13.54.03;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.51.46;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.51.46;	author deraadt;	state Exp;
branches;
next	;

1.8.12.1
date	2003.06.07.11.14.44;	author ho;	state Exp;
branches;
next	;


desc
@@


1.14
log
@<netinet/in_systm.h> is no longer needed.
@
text
@/*	$OpenBSD: in_cksum.c,v 1.13 2014/03/29 18:09:30 guenther Exp $	*/
/*	$NetBSD: in_cksum.c,v 1.7 1996/10/05 23:44:34 mrg Exp $ */

/*
 * Copyright (c) 1995 Zubin Dittia.
 * Copyright (c) 1995 Matthew R. Green.
 * Copyright (c) 1994 Charles Hannum.
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)in_cksum.c	8.1 (Berkeley) 6/11/93
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/mbuf.h>
#include <sys/socketvar.h>

#include <netinet/in.h>
#include <netinet/ip.h>
#include <netinet/ip_var.h>

/*
 * Checksum routine for Internet Protocol family headers.
 *
 * This routine is very heavily used in the network
 * code and should be modified for each CPU to be as fast as possible.
 *
 * SPARC version.
 */

/*
 * The checksum computation code here is significantly faster than its
 * vanilla C counterpart (by significantly, I mean 2-3 times faster if
 * the data is in cache, and 1.5-2 times faster if the data is not in
 * cache).
 * We optimize on three fronts:
 *	1. By using the add-with-carry (addxcc) instruction, we can use
 *	   32-bit operations instead of 16-bit operations.
 *	2. By unrolling the main loop to reduce branch overheads.
 *	3. By doing a sequence of load,load,add,add,load,load,add,add,
 *	   we can avoid the extra stall cycle which is incurred if the
 *	   instruction immediately following a load tries to use the
 *	   target register of the load.
 * Another possible optimization is to replace a pair of 32-bit loads
 * with a single 64-bit load (ldd) instruction, but I found that although
 * this improves performance somewhat on Sun4c machines, it actually
 * reduces performance considerably on Sun4m machines (I don't know why).
 * So I chose to leave it out.
 *
 * Zubin Dittia (zubin@@dworkin.wustl.edu)
 */

#define Asm	__asm volatile
#define ADD64		Asm("	ld [%4+ 0],%1;   ld [%4+ 4],%2;		\
				addcc  %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+ 8],%1;   ld [%4+12],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+16],%1;   ld [%4+20],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+24],%1;   ld [%4+28],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+32],%1;   ld [%4+36],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+40],%1;   ld [%4+44],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+48],%1;   ld [%4+52],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+56],%1;   ld [%4+60],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				addxcc %0,0,%0"				\
				: "=r" (sum), "=&r" (tmp1), "=&r" (tmp2)\
				: "0" (sum), "r" (w))
#define ADD32		Asm("	ld [%4+ 0],%1;   ld [%4+ 4],%2;		\
				addcc  %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+ 8],%1;   ld [%4+12],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+16],%1;   ld [%4+20],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+24],%1;   ld [%4+28],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				addxcc %0,0,%0"				\
				: "=r" (sum), "=&r" (tmp1), "=&r" (tmp2)\
				: "0" (sum), "r" (w))
#define ADD16		Asm("	ld [%4+ 0],%1;   ld [%4+ 4],%2;		\
				addcc  %0,%1,%0; addxcc %0,%2,%0;	\
				ld [%4+ 8],%1;   ld [%4+12],%2;		\
				addxcc %0,%1,%0; addxcc %0,%2,%0;	\
				addxcc %0,0,%0"				\
				: "=r" (sum), "=&r" (tmp1), "=&r" (tmp2)\
				: "0" (sum), "r" (w))
#define ADD8		Asm("	ld [%4+ 0],%1;   ld [%4+ 4],%2;		\
				addcc  %0,%1,%0; addxcc %0,%2,%0;	\
				addxcc %0,0,%0"				\
				: "=r" (sum), "=&r" (tmp1), "=&r" (tmp2)\
				: "0" (sum), "r" (w))
#define ADD4		Asm("	ld [%3+ 0],%1; 				\
				addcc  %0,%1,%0;			\
				addxcc %0,0,%0"				\
				: "=r" (sum), "=&r" (tmp1)		\
				: "0" (sum), "r" (w))

#define REDUCE		{sum = (sum & 0xffff) + (sum >> 16);}
#define ROL		{sum = sum << 8;}	/* depends on recent REDUCE */
#define ADDBYTE		{ROL; sum += *w; byte_swapped ^= 1;}
#define ADDSHORT	{sum += *(u_short *)w;}
#define ADVANCE(n)	{w += n; mlen -= n;}

static __inline__ int
in_cksum_internal(struct mbuf *m, int off, int len, u_int sum)
{
	u_char *w;
	int mlen = 0;
	int byte_swapped = 0;

	/*
	 * Declare two temporary registers for use by the asm code.  We
	 * allow the compiler to pick which specific machine registers to
	 * use, instead of hard-coding this in the asm code above.
	 */
	u_int tmp1, tmp2;

	for (; m && len; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		w = mtod(m, u_char *) + off;
		mlen = m->m_len - off;
		off = 0;
		if (len < mlen)
			mlen = len;
		len -= mlen;

		/*
		 * Ensure that we're aligned on a word boundary here so
		 * that we can do 32 bit operations below.
		 */
		if ((3 & (long)w) != 0) {
			REDUCE;
			if ((1 & (long)w) != 0 && mlen >= 1) {
				ADDBYTE;
				ADVANCE(1);
			}
			if ((2 & (long)w) != 0 && mlen >= 2) {
				ADDSHORT;
				ADVANCE(2);
			}
		}

		/*
		 * Do as many 32 bit operations as possible using the
		 * 64/32/16/8/4 macro's above, using as many as possible of
		 * these.
		 */
		while (mlen >= 64) {
			ADD64;
			ADVANCE(64);
		}
		if (mlen >= 32) {
			ADD32;
			ADVANCE(32);
		}
		if (mlen >= 16) {
			ADD16;
			ADVANCE(16);
		}
		if (mlen >= 8) {
			ADD8;
			ADVANCE(8);
		}
		if (mlen >= 4) {
			ADD4;
			ADVANCE(4)
		}
		if (mlen == 0)
			continue;

		REDUCE;
		if (mlen >= 2) {
			ADDSHORT;
			ADVANCE(2);
		}
		if (mlen == 1) {
			ADDBYTE;
		}
	}
	if (byte_swapped) {
		REDUCE;
		ROL;
	}
	/* Two REDUCEs is faster than REDUCE1; if (sum > 65535) sum -= 65535; */
	REDUCE;
	REDUCE;

	return (0xffff ^ sum);
}

int
in_cksum(struct mbuf *m, int len)
{

	return (in_cksum_internal(m, 0, len, 0));
}

int
in4_cksum(struct mbuf *m, u_int8_t nxt, int off, int len)
{
	u_char *w;
	u_int sum = 0;
	struct ipovly ipov;

	/*
	 * Declare two temporary registers for use by the asm code.  We
	 * allow the compiler to pick which specific machine registers to
	 * use, instead of hard-coding this in the asm code above.
	 */
	u_int tmp1, tmp2;

	if (nxt != 0) {
		/* pseudo header */
		memset(&ipov, 0, sizeof(ipov));
		ipov.ih_len = htons(len);
		ipov.ih_pr = nxt;
		ipov.ih_src = mtod(m, struct ip *)->ip_src;
		ipov.ih_dst = mtod(m, struct ip *)->ip_dst;
		w = (u_char *)&ipov;
		/* assumes sizeof(ipov) == 20 */
		ADD16;
		w += 16;
		ADD4;
	}

	/* skip unnecessary part */
	while (m && off > 0) {
		if (m->m_len > off)
			break;
		off -= m->m_len;
		m = m->m_next;
	}

	return (in_cksum_internal(m, off, len, sum));
}
@


1.13
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: in_cksum.c,v 1.12 2008/02/16 23:02:41 miod Exp $	*/
d42 1
a43 1
#include <netinet/in_systm.h>
@


1.12
log
@On these platforms, REDUCEing unconditionnaly a second time is faster and
shorter code than a conditional ADDCARRY, so use it; inspired by hppa.
@
text
@d1 1
a1 1
/*	$OpenBSD: in_cksum.c,v 1.11 2005/05/03 00:39:39 brad Exp $	*/
d78 1
a78 1
#define Asm	__asm __volatile
@


1.11
log
@- Fix bogus asm statements; tmp1 and tmp2 need to be outputs rather than inputs.
- Merge in4_cksum().

From NetBSD

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: in_cksum.c,v 1.10 2003/06/04 22:08:17 deraadt Exp $	*/
a127 1
#define ADDCARRY	{if (sum > 0xffff) sum -= 0xffff;}
d214 2
a216 1
	ADDCARRY;
@


1.10
log
@Delete UCB term 3.  When there are other copyright owners, we affirm
that we can see no legal situation under which they could require that
term to remain (ie. This is equivelant to us taking the old UCB file,
removing term 3 as specified by UCB, then re-applying each diff
afterwards from the various authors)
@
text
@d1 1
a1 1
/*	$OpenBSD: in_cksum.c,v 1.9 2003/06/02 23:27:55 millert Exp $	*/
d39 1
d41 1
d43 3
d79 16
a94 16
#define ADD64		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+8],%3; ld [%2+12],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+16],%3; ld [%2+20],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+24],%3; ld [%2+28],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+32],%3; ld [%2+36],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+40],%3; ld [%2+44],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+48],%3; ld [%2+52],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+56],%3; ld [%2+60],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
d96 10
a105 10
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD32		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+8],%3; ld [%2+12],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+16],%3; ld [%2+20],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+24],%3; ld [%2+28],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
d107 6
a112 6
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD16		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+8],%3; ld [%2+12],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
d114 4
a117 4
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD8		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
d119 4
a122 3
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD4		Asm("	ld [%2],%3; addcc  %0,%3,%0;		\
d124 2
a125 2
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1))
d134 2
a135 4
int
in_cksum(m, len)
	register struct mbuf *m;
	register int len;
d137 2
a138 3
	register u_char *w;
	register u_int sum = 0;
	register int mlen = 0;
d146 1
a146 2
	/* XXX - initialized because of gcc's `-Wuninitialized' ! */
	register u_int tmp1 = 0, tmp2 = 0;
d151 3
a153 2
		w = mtod(m, u_char *);
		mlen = m->m_len;
d175 1
a175 1
		 * Do as many 32 bit operattions as possible using the
d219 46
@


1.9
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: in_cksum.c,v 1.8 1997/08/08 08:27:15 downsj Exp $	*/
a9 5
 *
 * All advertising materials mentioning features or use of this software
 * must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, and it's contributors.
@


1.8
log
@Mostly sync to NetBSD-current 970804.

GENERIC currently compiles and runs; some devices (isp) are not complete and
not yet enabled.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d24 1
a24 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.8.12.1
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: in_cksum.c,v 1.8 1997/08/08 08:27:15 downsj Exp $	*/
d11 5
d24 5
a28 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.7
log
@Add netinet/in.h for in_cksum() proto.
@
text
@d1 2
a2 1
/*	$NetBSD: in_cksum.c,v 1.3 1995/04/26 13:30:03 pk Exp $ */
d6 1
a6 2
 * Copyright (c) 1995 Theo de Raadt.
 * Copyright (c) 1995 Matthew Green.
a48 1

d76 2
a77 2
 * reduces performance considerably on Sun4m machines (because of their
 * superscaler architecture).  So I chose to leave it out.
d152 2
a153 1
	register u_int tmp1, tmp2;
d163 1
a163 1
									 
@


1.6
log
@revert to in_cksum by zubin and I
@
text
@d50 2
@


1.5
log
@netbsd port, now we merge our changes back in
@
text
@d1 1
a1 1
/*	$NetBSD: in_cksum.c,v 1.5 1996/03/31 23:45:24 pk Exp $ */
d4 2
a48 1
#include <netinet/in.h>
d60 17
a76 3
 * This idea here is that we do as many 32 bit operations as possible
 * for maximum efficiency.  We also unroll all loops in to assembly.
 * This gains about 20% extra efficiency over the non-pipelined method.
d78 1
a78 2
 * XXX - this code really needs further performance analysis.  At the
 * moment it has only been run on a SPARC ELC.
d81 17
a97 9
#define Asm		__asm __volatile
#define ADD32		Asm("	ld [%2+28],%%i0; ld [%2+24],%%i1; 	\
				ld [%2+20],%%i2; ld [%2+16],%%i3; 	\
				ld [%2+12],%%i4; ld [%2+8],%%i5;	\
				ld [%2+4],%%g3; ld [%2],%%g4;		\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
				addxcc %0,%%i2,%0; addxcc %0,%%i3,%0;	\
				addxcc %0,%%i4,%0; addxcc %0,%%i5,%0;	\
				addxcc %0,%%g3,%0; addxcc %0,%%g4,%0;	\
d99 10
a108 7
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0", "%i1", "%i2", "%i3",		\
				  "%i4", "%i5", "%g3", "%g4")
#define ADD16		Asm("	ld [%2+12],%%i0; ld [%2+8],%%i1;	\
				ld [%2+4],%%i2; ld [%2],%%i3;		\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
				addxcc %0,%%i2,%0; addxcc %0,%%i3,%0;	\
d110 6
a115 4
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0", "%i1", "%i2", "%i3")
#define ADD8		Asm("	ld [%2+4],%%i0; ld [%2],%%i1;		\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
d117 4
a120 3
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0", "%i1")
#define ADD4		Asm("	ld [%2],%%i0; addcc  %0,%%i0,%0;	\
d122 6
a127 2
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0")
d132 3
a134 3
#define ADDB		{ROL; sum += *w; byte_swapped ^= 1;}
#define ADDS		{sum += *(u_short *)w;}
#define SHIFT(n)	{w += n; mlen -= n;}
d146 7
d161 1
a161 1

d169 2
a170 2
				ADDB;
				SHIFT(1);
d173 2
a174 2
				ADDS;
				SHIFT(2);
d177 1
d180 1
a180 1
		 * 32/16/8/4 macro's above, using as many as possible of
d183 5
a187 1
		while (mlen >= 32) {
d189 1
a189 1
			SHIFT(32);
d193 1
a193 1
			SHIFT(16);
d197 1
a197 1
			SHIFT(8);
d201 1
a201 1
			SHIFT(4)
d208 2
a209 2
			ADDS;
			SHIFT(2);
d212 1
a212 1
			ADDB;
d221 1
@


1.4
log
@silly theo forgot to assert copyright on his optimized version
@
text
@d1 1
a1 1
/*	$NetBSD: in_cksum.c,v 1.3 1995/04/26 13:30:03 pk Exp $ */
a3 2
 * Copyright (c) 1995 Zubin Dittia.
 * Copyright (c) 1995 Theo de Raadt.
d47 1
d59 3
a61 17
 * The checksum computation code here is significantly faster than its
 * vanilla C counterpart (by significantly, I mean 2-3 times faster if
 * the data is in cache, and 1.5-2 times faster if the data is not in
 * cache).
 * We optimize on three fronts:
 *	1. By using the add-with-carry (addxcc) instruction, we can use
 *	   32-bit operations instead of 16-bit operations.
 *	2. By unrolling the main loop to reduce branch overheads.
 *	3. By doing a sequence of load,load,add,add,load,load,add,add,
 *	   we can avoid the extra stall cycle which is incurred if the
 *	   instruction immediately following a load tries to use the
 *	   target register of the load.
 * Another possible optimization is to replace a pair of 32-bit loads
 * with a single 64-bit load (ldd) instruction, but I found that although
 * this improves performance somewhat on Sun4c machines, it actually
 * reduces performance considerably on Sun4m machines (because of their
 * superscaler architecture).  So I chose to leave it out.
d63 2
a64 1
 * Zubin Dittia (zubin@@dworkin.wustl.edu)
d67 9
a75 17
#define Asm	__asm __volatile
#define ADD64		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+8],%3; ld [%2+12],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+16],%3; ld [%2+20],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+24],%3; ld [%2+28],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+32],%3; ld [%2+36],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+40],%3; ld [%2+44],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+48],%3; ld [%2+52],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+56],%3; ld [%2+60],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
d77 7
a83 10
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD32		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+8],%3; ld [%2+12],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+16],%3; ld [%2+20],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+24],%3; ld [%2+28],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
d85 4
a88 6
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD16		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
				ld [%2+8],%3; ld [%2+12],%4;		\
				addxcc %0,%3,%0; addxcc %0,%4,%0;	\
d90 3
a92 4
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD8		Asm("	ld [%2],%3; ld [%2+4],%4;		\
				addcc %0,%3,%0; addxcc %0,%4,%0;	\
d94 2
a95 6
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1), "r" (tmp2))
#define ADD4		Asm("	ld [%2],%3; addcc  %0,%3,%0;		\
				addxcc %0,0,%0"				\
				: "=r" (sum)				\
				: "0" (sum), "r" (w), "r" (tmp1))
d100 3
a102 3
#define ADDBYTE		{ROL; sum += *w; byte_swapped ^= 1;}
#define ADDSHORT	{sum += *(u_short *)w;}
#define ADVANCE(n)	{w += n; mlen -= n;}
a113 7
	/*
	 * Declare two temporary registers for use by the asm code.  We
	 * allow the compiler to pick which specific machine registers to
	 * use, instead of hard-coding this in the asm code above.
	 */
	register u_int tmp1, tmp2;

d122 1
a122 1
									 
d130 2
a131 2
				ADDBYTE;
				ADVANCE(1);
d134 2
a135 2
				ADDSHORT;
				ADVANCE(2);
a137 1

d140 1
a140 1
		 * 64/32/16/8/4 macro's above, using as many as possible of
d143 1
a143 5
		while (mlen >= 64) {
			ADD64;
			ADVANCE(64);
		}
		if (mlen >= 32) {
d145 1
a145 1
			ADVANCE(32);
d149 1
a149 1
			ADVANCE(16);
d153 1
a153 1
			ADVANCE(8);
d157 1
a157 1
			ADVANCE(4)
d164 2
a165 2
			ADDSHORT;
			ADVANCE(2);
d168 1
a168 1
			ADDBYTE;
a176 1

@


1.3
log
@optimized in_cksum from Zubin Dittia <zubin@@dworkin.wustl.edu>
Zubin says:
 The checksum computation code here is significantly faster than its
 vanilla C counterpart (by significantly, I mean 2-3 times faster if
 the data is in cache, and 1.5-2 times faster if the data is not in
 cache).
 We optimize on three fronts:
      1. By using the add-with-carry (addxcc) instruction, we can use
         32-bit operations instead of 16-bit operations.
      2. By unrolling the main loop to reduce branch overheads.
      3. By doing a sequence of load,load,add,add,load,load,add,add,
         we can avoid the extra stall cycle which is incurred if the
         instruction immediately following a load tries to use the
         target register of the load.
 Another possible optimization is to replace a pair of 32-bit loads
 with a single 64-bit load (ldd) instruction, but I found that although
 this improves performance somewhat on Sun4c machines, it actually
 reduces performance considerably on Sun4m machines (because of their
 superscaler architecture).  So I chose to leave it out.
@
text
@d5 1
@


1.2
log
@interleave ld and operations for better performance
@
text
@d4 1
d59 17
a75 3
 * This idea here is that we do as many 32 bit operations as possible
 * for maximum efficiency.  We also unroll all loops in to assembly.
 * This gains about 20% extra efficiency over the non-pipelined method.
d77 1
a77 2
 * XXX - this code really needs further performance analysis.  At the
 * moment it has only been run on a SPARC ELC.
d80 17
a96 9
#define Asm		__asm __volatile
#define ADD32		Asm("	ld [%2+28],%%i0; ld [%2+24],%%i1; 	\
				ld [%2+20],%%i2; ld [%2+16],%%i3; 	\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
				ld [%2+12],%%i4; ld [%2+8],%%i5;	\
				addxcc %0,%%i2,%0; addxcc %0,%%i3,%0;	\
				ld [%2+4],%%i0; ld [%2],%%i1;		\
				addxcc %0,%%i4,%0; addxcc %0,%%i5,%0;	\
				addxcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
d98 10
a107 6
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0", "%i1", "%i2", "%i3", "%i4", "%i5")
#define ADD16		Asm("	ld [%2+12],%%i0; ld [%2+8],%%i1;	\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
				ld [%2+4],%%i2; ld [%2],%%i3;		\
				addxcc %0,%%i2,%0; addxcc %0,%%i3,%0;	\
d109 6
a114 4
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0", "%i1", "%i2", "%i3")
#define ADD8		Asm("	ld [%2+4],%%i0; ld [%2],%%i1;		\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
d116 4
a119 3
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0", "%i1")
#define ADD4		Asm("	ld [%2],%%i0; addcc  %0,%%i0,%0;	\
d121 6
a126 2
				: "=r" (sum) : "0" (sum), "r" (w)	\
				: "%i0")
d131 3
a133 3
#define ADDB		{ROL; sum += *w; byte_swapped ^= 1;}
#define ADDS		{sum += *(u_short *)w;}
#define SHIFT(n)	{w += n; mlen -= n;}
d145 7
d168 2
a169 2
				ADDB;
				SHIFT(1);
d172 2
a173 2
				ADDS;
				SHIFT(2);
d176 1
d179 1
a179 1
		 * 32/16/8/4 macro's above, using as many as possible of
d182 5
a186 1
		while (mlen >= 32) {
d188 1
a188 1
			SHIFT(32);
d192 1
a192 1
			SHIFT(16);
d196 1
a196 1
			SHIFT(8);
d200 1
a200 1
			SHIFT(4)
d207 2
a208 2
			ADDS;
			SHIFT(2);
d211 1
a211 1
			ADDB;
d220 1
@


1.1
log
@Initial revision
@
text
@d69 1
a70 2
				ld [%2+4],%%g3; ld [%2],%%g4;		\
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
d72 1
d74 1
a74 1
				addxcc %0,%%g3,%0; addxcc %0,%%g4,%0;	\
d77 1
a77 2
				: "%i0", "%i1", "%i2", "%i3",		\
				  "%i4", "%i5", "%g3", "%g4")
d79 1
a80 1
				addcc %0,%%i0,%0; addxcc %0,%%i1,%0;	\
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
