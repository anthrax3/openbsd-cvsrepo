head	1.181;
access;
symbols
	OPENBSD_6_0:1.180.0.2
	OPENBSD_6_0_BASE:1.180
	OPENBSD_5_9:1.179.0.2
	OPENBSD_5_9_BASE:1.179
	OPENBSD_5_8:1.177.0.4
	OPENBSD_5_8_BASE:1.177
	OPENBSD_5_7:1.174.0.2
	OPENBSD_5_7_BASE:1.174
	OPENBSD_5_6:1.165.0.4
	OPENBSD_5_6_BASE:1.165
	OPENBSD_5_5:1.163.0.4
	OPENBSD_5_5_BASE:1.163
	OPENBSD_5_4:1.162.0.2
	OPENBSD_5_4_BASE:1.162
	OPENBSD_5_3:1.161.0.6
	OPENBSD_5_3_BASE:1.161
	OPENBSD_5_2:1.161.0.4
	OPENBSD_5_2_BASE:1.161
	OPENBSD_5_1_BASE:1.161
	OPENBSD_5_1:1.161.0.2
	OPENBSD_5_0:1.160.0.2
	OPENBSD_5_0_BASE:1.160
	OPENBSD_4_9:1.158.0.2
	OPENBSD_4_9_BASE:1.158
	OPENBSD_4_8:1.157.0.2
	OPENBSD_4_8_BASE:1.157
	OPENBSD_4_7:1.152.0.4
	OPENBSD_4_7_BASE:1.152
	OPENBSD_4_6:1.152.0.6
	OPENBSD_4_6_BASE:1.152
	OPENBSD_4_5:1.152.0.2
	OPENBSD_4_5_BASE:1.152
	OPENBSD_4_4:1.149.0.2
	OPENBSD_4_4_BASE:1.149
	OPENBSD_4_3:1.147.0.2
	OPENBSD_4_3_BASE:1.147
	OPENBSD_4_2:1.145.0.2
	OPENBSD_4_2_BASE:1.145
	OPENBSD_4_1:1.144.0.4
	OPENBSD_4_1_BASE:1.144
	OPENBSD_4_0:1.144.0.2
	OPENBSD_4_0_BASE:1.144
	OPENBSD_3_9:1.143.0.4
	OPENBSD_3_9_BASE:1.143
	OPENBSD_3_8:1.143.0.2
	OPENBSD_3_8_BASE:1.143
	OPENBSD_3_7:1.140.0.2
	OPENBSD_3_7_BASE:1.140
	OPENBSD_3_6:1.137.0.2
	OPENBSD_3_6_BASE:1.137
	SMP_SYNC_A:1.137
	SMP_SYNC_B:1.137
	OPENBSD_3_5:1.136.0.2
	OPENBSD_3_5_BASE:1.136
	OPENBSD_3_4:1.134.0.2
	OPENBSD_3_4_BASE:1.134
	UBC_SYNC_A:1.134
	OPENBSD_3_3:1.133.0.2
	OPENBSD_3_3_BASE:1.133
	OPENBSD_3_2:1.129.0.2
	OPENBSD_3_2_BASE:1.129
	OPENBSD_3_1:1.123.0.2
	OPENBSD_3_1_BASE:1.123
	UBC_SYNC_B:1.130
	UBC:1.116.0.2
	UBC_BASE:1.116
	OPENBSD_3_0:1.91.0.2
	OPENBSD_3_0_BASE:1.91
	OPENBSD_2_9_BASE:1.80
	OPENBSD_2_9:1.80.0.2
	OPENBSD_2_8:1.78.0.4
	OPENBSD_2_8_BASE:1.78
	OPENBSD_2_7:1.78.0.2
	OPENBSD_2_7_BASE:1.78
	SMP:1.75.0.2
	SMP_BASE:1.75
	kame_19991208:1.46
	OPENBSD_2_6:1.36.0.2
	OPENBSD_2_6_BASE:1.36
	OPENBSD_2_5:1.23.0.2
	OPENBSD_2_5_BASE:1.23
	OPENBSD_2_4:1.22.0.2
	OPENBSD_2_4_BASE:1.22
	OPENBSD_2_3:1.20.0.2
	OPENBSD_2_3_BASE:1.20
	OPENBSD_2_2:1.18.0.2
	OPENBSD_2_2_BASE:1.18
	OPENBSD_2_1:1.11.0.4
	OPENBSD_2_1_BASE:1.11
	OPENBSD_2_0:1.11.0.2
	OPENBSD_2_0_BASE:1.11
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.181
date	2016.09.01.09.23.43;	author tedu;	state dead;
branches;
next	1.180;
commitid	Q2PxaFNhqAe0Wmla;

1.180
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.179;
commitid	N0upL0onl7Raz5yi;

1.179
date	2015.12.29.04.46.28;	author mmcc;	state Exp;
branches;
next	1.178;
commitid	QKSTqnEw1KIrhSLM;

1.178
date	2015.09.18.12.50.28;	author miod;	state Exp;
branches;
next	1.177;
commitid	aBaVLnMs8wxyaAGJ;

1.177
date	2015.03.27.20.25.39;	author miod;	state Exp;
branches;
next	1.176;
commitid	DNjbblqLgBZmk6yi;

1.176
date	2015.03.18.20.56.40;	author miod;	state Exp;
branches;
next	1.175;
commitid	1t8JWNihm6Vc4kyS;

1.175
date	2015.03.18.20.49.40;	author miod;	state Exp;
branches;
next	1.174;
commitid	L6hoLZci9pFouDHc;

1.174
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.173;
commitid	eahBabNpxnDWKzqJ;

1.173
date	2015.02.11.07.05.39;	author dlg;	state Exp;
branches;
next	1.172;
commitid	JTpbkhDknrIuy9pn;

1.172
date	2014.12.17.06.58.10;	author guenther;	state Exp;
branches;
next	1.171;
commitid	DImukoCWyTxwdbuh;

1.171
date	2014.12.17.06.05.52;	author deraadt;	state Exp;
branches;
next	1.170;
commitid	1Ms05ApnZbB2QS7L;

1.170
date	2014.12.15.02.24.22;	author guenther;	state Exp;
branches;
next	1.169;
commitid	ZxaujiOM0aYQRjFY;

1.169
date	2014.11.22.22.51.29;	author deraadt;	state Exp;
branches;
next	1.168;
commitid	gFyuTGLAMgerJnks;

1.168
date	2014.11.22.07.30.17;	author deraadt;	state Exp;
branches;
next	1.167;
commitid	jPhSvLERKDSWdmF8;

1.167
date	2014.11.17.21.39.19;	author deraadt;	state Exp;
branches;
next	1.166;
commitid	Z2pD8pwnvrAXjZK2;

1.166
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.165;
commitid	yv0ECmCdICvq576h;

1.165
date	2014.07.12.18.44.43;	author tedu;	state Exp;
branches;
next	1.164;
commitid	uKVPYMN2MLxdZxzH;

1.164
date	2014.04.08.13.23.51;	author mpi;	state Exp;
branches;
next	1.163;

1.163
date	2013.09.21.10.04.42;	author miod;	state Exp;
branches;
next	1.162;

1.162
date	2013.06.11.16.42.11;	author deraadt;	state Exp;
branches;
next	1.161;

1.161
date	2011.09.22.17.41.00;	author jasper;	state Exp;
branches;
next	1.160;

1.160
date	2011.05.30.22.25.22;	author oga;	state Exp;
branches;
next	1.159;

1.159
date	2011.04.26.23.50.21;	author ariane;	state Exp;
branches;
next	1.158;

1.158
date	2010.12.06.20.57.18;	author miod;	state Exp;
branches;
next	1.157;

1.157
date	2010.07.10.19.32.25;	author miod;	state Exp;
branches;
next	1.156;

1.156
date	2010.06.30.20.35.03;	author miod;	state Exp;
branches;
next	1.155;

1.155
date	2010.06.29.21.28.11;	author miod;	state Exp;
branches;
next	1.154;

1.154
date	2010.06.17.16.11.19;	author miod;	state Exp;
branches;
next	1.153;

1.153
date	2010.03.27.15.14.34;	author oga;	state Exp;
branches;
next	1.152;

1.152
date	2009.02.12.18.52.17;	author miod;	state Exp;
branches;
next	1.151;

1.151
date	2009.01.27.22.14.13;	author miod;	state Exp;
branches;
next	1.150;

1.150
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.149;

1.149
date	2008.06.09.20.31.47;	author miod;	state Exp;
branches;
next	1.148;

1.148
date	2008.06.08.20.57.19;	author miod;	state Exp;
branches;
next	1.147;

1.147
date	2007.11.28.16.33.20;	author martin;	state Exp;
branches;
next	1.146;

1.146
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.145;

1.145
date	2007.06.06.17.15.12;	author deraadt;	state Exp;
branches;
next	1.144;

1.144
date	2006.06.26.22.23.03;	author miod;	state Exp;
branches;
next	1.143;

1.143
date	2005.04.17.18.47.50;	author miod;	state Exp;
branches;
next	1.142;

1.142
date	2005.04.03.10.36.12;	author miod;	state Exp;
branches;
next	1.141;

1.141
date	2005.03.29.11.33.18;	author miod;	state Exp;
branches;
next	1.140;

1.140
date	2005.03.07.16.43.54;	author miod;	state Exp;
branches;
next	1.139;

1.139
date	2005.03.03.19.30.36;	author miod;	state Exp;
branches;
next	1.138;

1.138
date	2004.12.25.23.02.25;	author miod;	state Exp;
branches;
next	1.137;

1.137
date	2004.05.08.21.45.26;	author miod;	state Exp;
branches;
next	1.136;

1.136
date	2003.12.20.00.49.46;	author miod;	state Exp;
branches;
next	1.135;

1.135
date	2003.11.03.07.01.33;	author david;	state Exp;
branches;
next	1.134;

1.134
date	2003.04.06.22.50.37;	author miod;	state Exp;
branches;
next	1.133;

1.133
date	2003.03.13.22.09.20;	author miod;	state Exp;
branches;
next	1.132;

1.132
date	2002.12.14.21.40.52;	author miod;	state Exp;
branches;
next	1.131;

1.131
date	2002.11.11.05.04.46;	author miod;	state Exp;
branches;
next	1.130;

1.130
date	2002.10.28.19.30.21;	author art;	state Exp;
branches;
next	1.129;

1.129
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.128;

1.128
date	2002.09.06.22.46.48;	author art;	state Exp;
branches;
next	1.127;

1.127
date	2002.08.08.17.40.27;	author jason;	state Exp;
branches;
next	1.126;

1.126
date	2002.07.24.00.55.52;	author art;	state Exp;
branches;
next	1.125;

1.125
date	2002.07.09.18.08.18;	author art;	state Exp;
branches;
next	1.124;

1.124
date	2002.07.09.18.06.08;	author art;	state Exp;
branches;
next	1.123;

1.123
date	2002.04.18.05.44.35;	author deraadt;	state Exp;
branches;
next	1.122;

1.122
date	2002.03.31.21.38.10;	author miod;	state Exp;
branches;
next	1.121;

1.121
date	2002.03.14.01.26.44;	author millert;	state Exp;
branches;
next	1.120;

1.120
date	2002.03.13.00.24.21;	author miod;	state Exp;
branches;
next	1.119;

1.119
date	2002.01.24.10.15.07;	author art;	state Exp;
branches;
next	1.118;

1.118
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.117;

1.117
date	2001.12.19.08.58.05;	author art;	state Exp;
branches;
next	1.116;

1.116
date	2001.12.09.04.51.35;	author art;	state Exp;
branches
	1.116.2.1;
next	1.115;

1.115
date	2001.12.09.04.44.20;	author art;	state Exp;
branches;
next	1.114;

1.114
date	2001.12.09.04.29.51;	author art;	state Exp;
branches;
next	1.113;

1.113
date	2001.12.09.04.20.42;	author art;	state Exp;
branches;
next	1.112;

1.112
date	2001.12.09.04.14.14;	author art;	state Exp;
branches;
next	1.111;

1.111
date	2001.12.09.03.46.54;	author art;	state Exp;
branches;
next	1.110;

1.110
date	2001.12.08.02.24.07;	author art;	state Exp;
branches;
next	1.109;

1.109
date	2001.12.07.18.06.49;	author mickey;	state Exp;
branches;
next	1.108;

1.108
date	2001.12.07.15.11.49;	author art;	state Exp;
branches;
next	1.107;

1.107
date	2001.12.07.13.43.30;	author art;	state Exp;
branches;
next	1.106;

1.106
date	2001.12.07.10.57.58;	author art;	state Exp;
branches;
next	1.105;

1.105
date	2001.12.07.10.52.25;	author art;	state Exp;
branches;
next	1.104;

1.104
date	2001.12.07.10.44.52;	author art;	state Exp;
branches;
next	1.103;

1.103
date	2001.12.07.10.39.46;	author art;	state Exp;
branches;
next	1.102;

1.102
date	2001.12.07.10.38.11;	author art;	state Exp;
branches;
next	1.101;

1.101
date	2001.12.05.14.40.48;	author art;	state Exp;
branches;
next	1.100;

1.100
date	2001.11.28.15.34.16;	author art;	state Exp;
branches;
next	1.99;

1.99
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.98;

1.98
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.97;

1.97
date	2001.11.22.09.54.04;	author art;	state Exp;
branches;
next	1.96;

1.96
date	2001.11.22.09.08.32;	author art;	state Exp;
branches;
next	1.95;

1.95
date	2001.11.22.08.24.08;	author art;	state Exp;
branches;
next	1.94;

1.94
date	2001.11.22.08.11.23;	author art;	state Exp;
branches;
next	1.93;

1.93
date	2001.11.22.07.50.10;	author art;	state Exp;
branches;
next	1.92;

1.92
date	2001.11.06.19.53.16;	author miod;	state Exp;
branches;
next	1.91;

1.91
date	2001.09.19.20.50.57;	author mickey;	state Exp;
branches;
next	1.90;

1.90
date	2001.08.12.15.19.58;	author mickey;	state Exp;
branches;
next	1.89;

1.89
date	2001.08.11.23.01.11;	author art;	state Exp;
branches;
next	1.88;

1.88
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.87;

1.87
date	2001.06.27.18.30.30;	author art;	state Exp;
branches;
next	1.86;

1.86
date	2001.06.10.01.45.04;	author deraadt;	state Exp;
branches;
next	1.85;

1.85
date	2001.05.10.12.52.35;	author art;	state Exp;
branches;
next	1.84;

1.84
date	2001.05.10.10.39.11;	author art;	state Exp;
branches;
next	1.83;

1.83
date	2001.05.10.10.34.48;	author art;	state Exp;
branches;
next	1.82;

1.82
date	2001.05.10.08.43.56;	author art;	state Exp;
branches;
next	1.81;

1.81
date	2001.05.09.15.31.27;	author art;	state Exp;
branches;
next	1.80;

1.80
date	2000.11.22.11.57.04;	author art;	state Exp;
branches;
next	1.79;

1.79
date	2000.11.07.11.42.10;	author art;	state Exp;
branches;
next	1.78;

1.78
date	2000.02.21.17.08.37;	author art;	state Exp;
branches;
next	1.77;

1.77
date	2000.02.19.22.08.51;	author art;	state Exp;
branches;
next	1.76;

1.76
date	2000.02.18.17.05.33;	author art;	state Exp;
branches;
next	1.75;

1.75
date	2000.02.04.19.16.13;	author art;	state Exp;
branches
	1.75.2.1;
next	1.74;

1.74
date	2000.02.04.15.46.09;	author art;	state Exp;
branches;
next	1.73;

1.73
date	2000.02.01.16.16.36;	author art;	state Exp;
branches;
next	1.72;

1.72
date	2000.02.01.10.34.32;	author art;	state Exp;
branches;
next	1.71;

1.71
date	2000.02.01.10.17.08;	author art;	state Exp;
branches;
next	1.70;

1.70
date	2000.01.27.21.11.09;	author art;	state Exp;
branches;
next	1.69;

1.69
date	2000.01.27.17.37.15;	author art;	state Exp;
branches;
next	1.68;

1.68
date	2000.01.27.17.00.02;	author art;	state Exp;
branches;
next	1.67;

1.67
date	2000.01.27.15.48.19;	author art;	state Exp;
branches;
next	1.66;

1.66
date	2000.01.27.02.06.24;	author art;	state Exp;
branches;
next	1.65;

1.65
date	2000.01.27.00.18.43;	author art;	state Exp;
branches;
next	1.64;

1.64
date	2000.01.27.00.03.09;	author art;	state Exp;
branches;
next	1.63;

1.63
date	2000.01.27.00.01.06;	author art;	state Exp;
branches;
next	1.62;

1.62
date	2000.01.26.23.30.04;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2000.01.26.15.54.30;	author art;	state Exp;
branches;
next	1.60;

1.60
date	2000.01.17.21.06.44;	author art;	state Exp;
branches;
next	1.59;

1.59
date	2000.01.14.22.18.28;	author art;	state Exp;
branches;
next	1.58;

1.58
date	2000.01.14.21.24.26;	author art;	state Exp;
branches;
next	1.57;

1.57
date	2000.01.12.11.11.26;	author d;	state Exp;
branches;
next	1.56;

1.56
date	99.12.09.21.46.51;	author art;	state Exp;
branches;
next	1.55;

1.55
date	99.12.09.21.35.28;	author art;	state Exp;
branches;
next	1.54;

1.54
date	99.12.09.16.11.48;	author art;	state Exp;
branches;
next	1.53;

1.53
date	99.12.09.14.26.04;	author art;	state Exp;
branches;
next	1.52;

1.52
date	99.12.09.13.22.59;	author art;	state Exp;
branches;
next	1.51;

1.51
date	99.12.08.15.16.12;	author art;	state Exp;
branches;
next	1.50;

1.50
date	99.12.08.12.27.17;	author deraadt;	state Exp;
branches;
next	1.49;

1.49
date	99.12.08.12.15.03;	author deraadt;	state Exp;
branches;
next	1.48;

1.48
date	99.12.08.11.38.38;	author art;	state Exp;
branches;
next	1.47;

1.47
date	99.12.08.10.44.49;	author art;	state Exp;
branches;
next	1.46;

1.46
date	99.12.07.15.32.30;	author art;	state Exp;
branches;
next	1.45;

1.45
date	99.11.24.16.07.15;	author art;	state Exp;
branches;
next	1.44;

1.44
date	99.11.16.12.21.41;	author art;	state Exp;
branches;
next	1.43;

1.43
date	99.11.16.10.49.56;	author art;	state Exp;
branches;
next	1.42;

1.42
date	99.11.16.10.11.35;	author art;	state Exp;
branches;
next	1.41;

1.41
date	99.11.12.21.02.50;	author art;	state Exp;
branches;
next	1.40;

1.40
date	99.11.11.12.30.36;	author art;	state Exp;
branches;
next	1.39;

1.39
date	99.11.05.19.21.02;	author art;	state Exp;
branches;
next	1.38;

1.38
date	99.11.05.18.07.10;	author art;	state Exp;
branches;
next	1.37;

1.37
date	99.11.05.16.22.08;	author art;	state Exp;
branches;
next	1.36;

1.36
date	99.09.06.07.13.39;	author art;	state Exp;
branches;
next	1.35;

1.35
date	99.09.03.18.38.04;	author art;	state Exp;
branches;
next	1.34;

1.34
date	99.09.03.18.33.42;	author art;	state Exp;
branches;
next	1.33;

1.33
date	99.09.03.18.11.29;	author art;	state Exp;
branches;
next	1.32;

1.32
date	99.09.03.18.01.59;	author art;	state Exp;
branches;
next	1.31;

1.31
date	99.08.20.09.30.55;	author art;	state Exp;
branches;
next	1.30;

1.30
date	99.08.20.09.15.05;	author art;	state Exp;
branches;
next	1.29;

1.29
date	99.07.09.21.30.02;	author art;	state Exp;
branches;
next	1.28;

1.28
date	99.06.04.23.03.00;	author deraadt;	state Exp;
branches;
next	1.27;

1.27
date	99.04.27.17.58.26;	author art;	state Exp;
branches;
next	1.26;

1.26
date	99.04.23.17.38.48;	author art;	state Exp;
branches;
next	1.25;

1.25
date	99.04.22.20.36.22;	author art;	state Exp;
branches;
next	1.24;

1.24
date	99.04.22.17.07.30;	author art;	state Exp;
branches;
next	1.23;

1.23
date	99.01.11.05.11.59;	author millert;	state Exp;
branches;
next	1.22;

1.22
date	98.05.29.16.21.35;	author jason;	state Exp;
branches;
next	1.21;

1.21
date	98.05.10.18.30.40;	author deraadt;	state Exp;
branches;
next	1.20;

1.20
date	98.03.04.10.58.11;	author niklas;	state Exp;
branches;
next	1.19;

1.19
date	97.11.07.08.11.44;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	97.09.17.06.47.21;	author downsj;	state Exp;
branches;
next	1.17;

1.17
date	97.08.08.08.27.36;	author downsj;	state Exp;
branches;
next	1.16;

1.16
date	97.07.07.07.45.32;	author grr;	state Exp;
branches;
next	1.15;

1.15
date	97.06.26.01.00.59;	author downsj;	state Exp;
branches;
next	1.14;

1.14
date	97.06.25.14.30.23;	author downsj;	state Exp;
branches;
next	1.13;

1.13
date	97.06.12.21.44.02;	author grr;	state Exp;
branches;
next	1.12;

1.12
date	97.06.11.10.32.12;	author grr;	state Exp;
branches;
next	1.11;

1.11
date	96.10.04.03.41.38;	author deraadt;	state Exp;
branches;
next	1.10;

1.10
date	96.08.12.08.38.20;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	96.08.12.01.43.54;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	96.08.12.00.55.17;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	96.08.11.05.35.22;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	95.12.15.13.50.37;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	95.10.23.08.56.04;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	95.10.23.06.10.30;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	95.10.19.02.54.46;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.10.18.18.20.58;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.51.47;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.51.47;	author deraadt;	state Exp;
branches;
next	;

1.75.2.1
date	2000.02.20.11.56.54;	author niklas;	state Exp;
branches;
next	1.75.2.2;

1.75.2.2
date	2000.02.21.22.29.04;	author niklas;	state Exp;
branches;
next	1.75.2.3;

1.75.2.3
date	2001.05.14.21.37.17;	author niklas;	state Exp;
branches;
next	1.75.2.4;

1.75.2.4
date	2001.07.04.10.23.46;	author niklas;	state Exp;
branches;
next	1.75.2.5;

1.75.2.5
date	2001.10.31.03.07.57;	author nate;	state Exp;
branches;
next	1.75.2.6;

1.75.2.6
date	2001.11.13.21.04.17;	author niklas;	state Exp;
branches;
next	1.75.2.7;

1.75.2.7
date	2001.12.05.00.39.13;	author niklas;	state Exp;
branches;
next	1.75.2.8;

1.75.2.8
date	2002.03.06.02.04.46;	author niklas;	state Exp;
branches;
next	1.75.2.9;

1.75.2.9
date	2002.03.28.10.57.11;	author niklas;	state Exp;
branches;
next	1.75.2.10;

1.75.2.10
date	2003.03.27.23.49.26;	author niklas;	state Exp;
branches;
next	1.75.2.11;

1.75.2.11
date	2003.05.13.19.41.08;	author ho;	state Exp;
branches;
next	1.75.2.12;

1.75.2.12
date	2004.02.19.10.49.58;	author niklas;	state Exp;
branches;
next	1.75.2.13;

1.75.2.13
date	2004.06.05.23.10.58;	author niklas;	state Exp;
branches;
next	;

1.116.2.1
date	2002.01.31.22.55.23;	author niklas;	state Exp;
branches;
next	1.116.2.2;

1.116.2.2
date	2002.02.02.03.28.25;	author art;	state Exp;
branches;
next	1.116.2.3;

1.116.2.3
date	2002.06.11.03.38.17;	author art;	state Exp;
branches;
next	1.116.2.4;

1.116.2.4
date	2002.10.29.00.28.10;	author art;	state Exp;
branches;
next	1.116.2.5;

1.116.2.5
date	2002.10.31.21.13.26;	author art;	state Exp;
branches;
next	1.116.2.6;

1.116.2.6
date	2002.11.04.16.38.31;	author art;	state Exp;
branches;
next	1.116.2.7;

1.116.2.7
date	2003.05.19.21.46.33;	author tedu;	state Exp;
branches;
next	1.116.2.8;

1.116.2.8
date	2003.05.29.19.06.45;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.181
log
@Celebrate OpenBSD 6.0 release by retiring the sparc port.
You've served us well, good friend, but now it's time to rest.
ok deraadt
@
text
@/*	$OpenBSD: pmap.c,v 1.180 2016/06/07 06:23:19 dlg Exp $	*/
/*	$NetBSD: pmap.c,v 1.118 1998/05/19 19:00:18 thorpej Exp $ */

/*
 * Copyright (c) 1996
 * 	The President and Fellows of Harvard College. All rights reserved.
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This software was developed by the Computer Systems Engineering group
 * at Lawrence Berkeley Laboratory under DARPA contract BG 91-66 and
 * contributed to Berkeley.
 *
 * All advertising materials mentioning features or use of this software
 * must display the following acknowledgement:
 *	This product includes software developed by Harvard University.
 *	This product includes software developed by the University of
 *	California, Lawrence Berkeley Laboratory.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Aaron Brown and
 *	Harvard University.
 *      This product includes software developed by the University of
 *      California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.4 (Berkeley) 2/5/94
 *
 */

/*
 * SPARC physical map management code.
 * Does not function on multiprocessors (yet).
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/device.h>
#include <sys/proc.h>
#include <sys/queue.h>
#include <sys/malloc.h>
#include <sys/exec.h>
#include <sys/core.h>
#include <sys/kcore.h>
#include <sys/pool.h>

#include <uvm/uvm_extern.h>

#include <machine/autoconf.h>
#include <machine/bsd_openprom.h>
#include <machine/oldmon.h>
#include <machine/cpu.h>
#include <machine/ctlreg.h>
#include <machine/kcore.h>

#include <sparc/sparc/asm.h>
#include <sparc/sparc/cache.h>
#include <sparc/sparc/vaddrs.h>
#include <sparc/sparc/cpuvar.h>

#ifdef DEBUG
#define PTE_BITS "\20\40V\37W\36S\35NC\33IO\32U\31M"
#define PTE_BITS4M "\20\10C\7M\6R\5ACC3\4ACC2\3ACC1\2TYP2\1TYP1"
#endif

/*
 * The SPARCstation offers us the following challenges:
 *
 *   1. A virtual address cache.  This is, strictly speaking, not
 *	part of the architecture, but the code below assumes one.
 *	This is a write-through cache on the 4c and a write-back cache
 *	on others.
 *
 *   2. (4/4c only) An MMU that acts like a cache.  There is not enough
 *	space in the MMU to map everything all the time.  Instead, we need
 *	to load MMU with the `working set' of translations for each
 *	process. The sun4m does not act like a cache; tables are maintained
 *	in physical memory.
 *
 *   3.	Segmented virtual and physical spaces.  The upper 12 bits of
 *	a virtual address (the virtual segment) index a segment table,
 *	giving a physical segment.  The physical segment selects a
 *	`Page Map Entry Group' (PMEG) and the virtual page number---the
 *	next 5 or 6 bits of the virtual address---select the particular
 *	`Page Map Entry' for the page.  We call the latter a PTE and
 *	call each Page Map Entry Group a pmeg (for want of a better name).
 *	Note that the sun4m has an unsegmented 36-bit physical space.
 *
 *	Since there are no valid bits in the segment table, the only way
 *	to have an invalid segment is to make one full pmeg of invalid PTEs.
 *	We use the last one (since the ROM does as well) (sun4/4c only)
 *
 *   4. Discontiguous physical pages.  The Mach VM expects physical pages
 *	to be in one sequential lump.
 *
 *   5. The MMU is always on: it is not possible to disable it.  This is
 *	mainly a startup hassle.
 */

struct pmap_stats {
	int	ps_alias_uncache;	/* # of uncaches due to bad aliases */
	int	ps_alias_recache;	/* # of recaches due to bad aliases */
	int	ps_unlink_pvfirst;	/* # of pv_unlinks on head */
	int	ps_unlink_pvsearch;	/* # of pv_unlink searches */
	int	ps_changeprots;		/* # of calls to changeprot */
	int	ps_useless_changeprots;	/* # of changeprots for wiring */
	int	ps_enter_firstpv;	/* pv heads entered */
	int	ps_enter_secondpv;	/* pv nonheads entered */
	int	ps_useless_changewire;	/* useless wiring changes */
	int	ps_npg_prot_all;	/* # of active pages protected */
	int	ps_npg_prot_actual;	/* # pages actually affected */
	int	ps_npmeg_free;		/* # of free pmegs */
	int	ps_npmeg_locked;	/* # of pmegs on locked list */
	int	ps_npmeg_lru;		/* # of pmegs on lru list */
} pmap_stats;

#ifdef DEBUG
#define	PDB_CREATE	0x0001
#define	PDB_DESTROY	0x0002
#define	PDB_REMOVE	0x0004
#define	PDB_CHANGEPROT	0x0008
#define	PDB_ENTER	0x0010
#define	PDB_FOLLOW	0x0020

#define	PDB_MMU_ALLOC	0x0100
#define	PDB_MMU_STEAL	0x0200
#define	PDB_CTX_ALLOC	0x0400
#define	PDB_CTX_STEAL	0x0800
#define	PDB_MMUREG_ALLOC	0x1000
#define	PDB_MMUREG_STEAL	0x2000
#define	PDB_CACHESTUFF	0x4000
#define	PDB_SWITCHMAP	0x8000
#define	PDB_SANITYCHK	0x10000
int	pmapdebug = 0;
#endif

/*
 * Internal helpers.
 */
static __inline struct pvlist *pvhead(int);

#if defined(SUN4M)
u_int	VA2PA(caddr_t);
#endif

/*
 * Given a page number, return the head of its pvlist.
 */
static __inline struct pvlist *
pvhead(int pnum)
{
	int bank, off;

	bank = vm_physseg_find(pnum, &off);
	if (bank == -1)
		return NULL;

	return &vm_physmem[bank].pgs[off].mdpage.pv_head;
}

struct pool pvpool;

unsigned int nureg, nkreg;
#if (defined(SUN4) || defined(SUN4C) || defined(SUN4E)) && \
    !(defined(SUN4D) || defined(SUN4M))
#define	NUREG	NUREG_4C
#define	NKREG	NKREG_4C
#elif (defined(SUN4D) || defined(SUN4M)) && \
      !(defined(SUN4) || defined(SUN4C) || defined(SUN4E))
#define	NUREG	NUREG_4M
#define	NKREG	NKREG_4M
#else
#define	NUREG	nureg
#define	NKREG	nkreg
#endif

#if defined(SUN4M)
/*
 * Memory pools and back-end supplier for SRMMU page tables.
 * Share a pool between the level 2 and level 3 page tables,
 * since these are equal in size.
 */
static struct pool L1_pool;
static struct pool L23_pool;
void	*pgt_page_alloc(struct pool *, int, int *);
void	 pgt_page_free(struct pool *, void *);

struct pool_allocator pgt_allocator = {
	pgt_page_alloc, pgt_page_free, 0,
};

void    pcache_flush(caddr_t, caddr_t, int);
void
pcache_flush(va, pa, n)
        caddr_t va, pa;
        int     n;
{
        void (*f)(int,int) = cpuinfo.pcache_flush_line;

        while ((n -= 4) >= 0)
                (*f)((u_int)va+n, (u_int)pa+n);
}

/*
 * Page table pool back-end.
 */
void *
pgt_page_alloc(struct pool *pp, int flags, int *slowdown)
{
	extern void	*pool_page_alloc(struct pool *, int, int *);
	void		*pga;

	if ((pga = pool_page_alloc(pp, flags, slowdown)) != NULL &&
	     (cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0) {
		pcache_flush((caddr_t)pga, (caddr_t)VA2PA(pga), PAGE_SIZE);
		kvm_uncache((caddr_t)pga, 1);
	}

	return (pga);
}       

void
pgt_page_free(struct pool *pp, void *pga)
{
	extern void	*pool_page_free(struct pool *, void *);
	/*
	 * if we marked the page uncached, we must recache it to go back to
	 * the uvm_km_thread, so other pools don't get uncached pages from us.
	 */
	if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
		kvm_recache((caddr_t)pga, 1);
	pool_page_free(pp, pga);
	
}
#endif /* SUN4M */

/*
 * Each virtual segment within each pmap is either valid or invalid.
 * It is valid if pm_npte[VA_VSEG(va)] is not 0.  This does not mean
 * it is in the MMU, however; that is true iff pm_segmap[VA_VSEG(va)]
 * does not point to the invalid PMEG.
 *
 * In the older SPARC architectures (pre-4m), page tables are cached in the
 * MMU. The following discussion applies to these architectures:
 *
 * If a virtual segment is valid and loaded, the correct PTEs appear
 * in the MMU only.  If it is valid and unloaded, the correct PTEs appear
 * in the pm_pte[VA_VSEG(va)] only.  However, some effort is made to keep
 * the software copies consistent enough with the MMU so that libkvm can
 * do user address translations.  In particular, pv_changepte() and
 * pmap_enu() maintain consistency, while less critical changes are
 * not maintained.  pm_pte[VA_VSEG(va)] always points to space for those
 * PTEs, unless this is the kernel pmap, in which case pm_pte[x] is not
 * used (sigh).
 *
 * Each PMEG in the MMU is either free or contains PTEs corresponding to
 * some pmap and virtual segment.  If it contains some PTEs, it also contains
 * reference and modify bits that belong in the pv_table.  If we need
 * to steal a PMEG from some process (if we need one and none are free)
 * we must copy the ref and mod bits, and update pm_segmap in the other
 * pmap to show that its virtual segment is no longer in the MMU.
 *
 * There are 128 PMEGs in a small Sun-4, of which only a few dozen are
 * tied down permanently, leaving `about' 100 to be spread among
 * running processes.  These are managed as an LRU cache.  Before
 * calling the VM paging code for a user page fault, the fault handler
 * calls mmu_load(pmap, va) to try to get a set of PTEs put into the
 * MMU.  mmu_load will check the validity of the segment and tell whether
 * it did something.
 *
 * Since I hate the name PMEG I call this data structure an `mmu entry'.
 * Each mmuentry is on exactly one of three `usage' lists: free, LRU,
 * or locked.  The LRU list is for user processes; the locked list is
 * for kernel entries; both are doubly linked queues headed by `mmuhd's.
 * The free list is a simple list, headed by a free list pointer.
 *
 * In the sun4m architecture using the SPARC Reference MMU (SRMMU), three
 * levels of page tables are maintained in physical memory. We use the same
 * structures as with the 3-level old-style MMU (pm_regmap, pm_segmap,
 * rg_segmap, sg_pte, etc) to maintain kernel-edible page tables; we also
 * build a parallel set of physical tables that can be used by the MMU.
 * (XXX: This seems redundant, but is it necessary for the unified kernel?)
 *
 * If a virtual segment is valid, its entries will be in both parallel lists.
 * If it is not valid, then its entry in the kernel tables will be zero, and
 * its entry in the MMU tables will either be nonexistent or zero as well.
 *
 * The Reference MMU generally uses a Translation Look-aside Buffer (TLB)
 * to cache the result of recently executed page table walks. When
 * manipulating page tables, we need to ensure consistency of the
 * in-memory and TLB copies of the page table entries. This is handled
 * by flushing (and invalidating) a TLB entry when appropriate before
 * altering an in-memory page table entry.
 */
struct mmuentry {
	TAILQ_ENTRY(mmuentry)	me_list;	/* usage list link */
	TAILQ_ENTRY(mmuentry)	me_pmchain;	/* pmap owner link */
	struct	pmap *me_pmap;		/* pmap, if in use */
	u_short	me_vreg;		/* associated virtual region/segment */
	u_short	me_vseg;		/* associated virtual region/segment */
	u_short	me_cookie;		/* hardware SMEG/PMEG number */
};
struct mmuentry *mmusegments;	/* allocated in pmap_bootstrap */
struct mmuentry *mmuregions;	/* allocated in pmap_bootstrap */

struct mmuhd segm_freelist, segm_lru, segm_locked;
struct mmuhd region_freelist, region_lru, region_locked;

int	seginval;		/* [4/4c] the invalid segment number */
int	reginval;		/* [4/3mmu] the invalid region number */

/*
 * (sun4/4c)
 * A context is simply a small number that dictates which set of 4096
 * segment map entries the MMU uses.  The Sun 4c has eight such sets.
 * These are alloted in an `almost MRU' fashion.
 * (sun4m)
 * A context is simply a small number that indexes the context table, the
 * root-level page table mapping 4G areas. Each entry in this table points
 * to a 1st-level region table. A SPARC reference MMU will usually use 16
 * such contexts, but some offer as many as 64k contexts; the theoretical
 * maximum is 2^32 - 1, but this would create overlarge context tables.
 *
 * Each context is either free or attached to a pmap.
 *
 * Since the virtual address cache is tagged by context, when we steal
 * a context we have to flush (that part of) the cache.
 */
union ctxinfo {
	union	ctxinfo *c_nextfree;	/* free list (if free) */
	struct	pmap *c_pmap;		/* pmap (if busy) */
};

#define ncontext	(cpuinfo.mmu_ncontext)
#define ctx_kick	(cpuinfo.ctx_kick)
#define ctx_kickdir	(cpuinfo.ctx_kickdir)
#define ctx_freelist	(cpuinfo.ctx_freelist)

#if 0
union ctxinfo *ctxinfo;		/* allocated at in pmap_bootstrap */

union	ctxinfo *ctx_freelist;	/* context free list */
int	ctx_kick;		/* allocation rover when none free */
int	ctx_kickdir;		/* ctx_kick roves both directions */

char	*ctxbusyvector;		/* [4m] tells what contexts are busy (XXX)*/
#endif

caddr_t	vpage[2];		/* two reserved MD virtual pages */

smeg_t		tregion;	/* [4/3mmu] Region for temporary mappings */

struct pmap	kernel_pmap_store;		/* the kernel's pmap */
struct regmap	kernel_regmap_store[NKREG_MAX];	/* the kernel's regmap */
struct segmap	kernel_segmap_store[NKREG_MAX*NSEGRG];/* the kernel's segmaps */

#if defined(SUN4M)
u_int 	*kernel_regtable_store;		/* 8k of storage to map the kernel */
u_int	*kernel_segtable_store;		/* 16k of storage to map the kernel */
u_int	*kernel_pagtable_store;		/* 1M of storage to map the kernel */
#endif

struct	memarr *pmemarr;		/* physical memory regions */
int	npmemarr;			/* number of entries in pmemarr */

vaddr_t avail_start;			/* first available physical page */
vaddr_t	virtual_avail;			/* first free virtual page number */
vaddr_t	virtual_end;			/* last free virtual page number */
paddr_t phys_avail;			/* first free physical page
					   XXX - pmap_pa_exists needs this */
vaddr_t pagetables_start, pagetables_end;

static void pmap_page_upload(void);
void pmap_release(pmap_t);

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
int mmu_has_hole;
#endif

vaddr_t prom_vstart;	/* For /dev/kmem */
vaddr_t prom_vend;

#if defined(SUN4)
/*
 * [sun4]: segfixmask: on some systems (4/110) "getsegmap()" returns a
 * partly invalid value. getsegmap returns a 16 bit value on the sun4,
 * but only the first 8 or so bits are valid (the rest are *supposed* to
 * be zero. On the 4/110 the bits that are supposed to be zero are
 * all one instead. e.g. KERNBASE is usually mapped by pmeg number zero.
 * On a 4/300 getsegmap(KERNBASE) == 0x0000, but
 * on a 4/100 getsegmap(KERNBASE) == 0xff00
 *
 * This confuses mmu_reservemon() and causes it to not reserve the PROM's
 * pmegs. Then the PROM's pmegs get used during autoconfig and everything
 * falls apart!  (not very fun to debug, BTW.)
 *
 * solution: mask the invalid bits in the getsetmap macro.
 */

static u_long segfixmask = 0xffffffff; /* all bits valid to start */
#else
#define segfixmask 0xffffffff	/* It's in getsegmap's scope */
#endif

/*
 * pseudo-functions for mnemonic value
 */
#define getcontext4()		lduba(AC_CONTEXT, ASI_CONTROL)
#define getcontext4m()		lda(SRMMU_CXR, ASI_SRMMU)
#define getcontext()		(CPU_ISSUN4M \
					? getcontext4m() \
					: getcontext4()  )

#define setcontext4(c)		stba(AC_CONTEXT, ASI_CONTROL, c)
#define setcontext4m(c)		sta(SRMMU_CXR, ASI_SRMMU, c)
#define setcontext(c)		(CPU_ISSUN4M \
					? setcontext4m(c) \
					: setcontext4(c)  )

#define	getsegmap(va)		(CPU_ISSUN4 \
					? (lduha(va, ASI_SEGMAP) & segfixmask) \
					: lduba(va, ASI_SEGMAP))
#define	setsegmap(va, pmeg)	(CPU_ISSUN4 \
					? stha(va, ASI_SEGMAP, pmeg) \
					: stba(va, ASI_SEGMAP, pmeg))

/* 3-level sun4 MMU only: */
#define	getregmap(va)		((unsigned)lduha((va)+2, ASI_REGMAP) >> 8)
#define	setregmap(va, smeg)	stha((va)+2, ASI_REGMAP, (smeg << 8))

#if defined(SUN4M)
#define getpte4m(va)		lda((va & 0xFFFFF000) | ASI_SRMMUFP_L3, \
				    ASI_SRMMUFP)
u_int	*getptep4m(struct pmap *, vaddr_t);
static __inline void	setpgt4m(int *, int);
void	setpte4m(vaddr_t va, int pte);
#endif

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
#define	getpte4(va)		lda(va, ASI_PTE)
#define	setpte4(va, pte)	sta(va, ASI_PTE, pte)
#endif

/* Function pointer messiness for supporting multiple sparc architectures
 * within a single kernel: notice that there are two versions of many of the
 * functions within this file/module, one for the sun4/sun4c and the other
 * for the sun4m. For performance reasons (since things like pte bits don't
 * map nicely between the two architectures), there are separate functions
 * rather than unified functions which test the cputyp variable. If only
 * one architecture is being used, then the non-suffixed function calls
 * are macro-translated into the appropriate xxx4_4c or xxx4m call. If
 * multiple architectures are defined, the calls translate to (*xxx_p),
 * i.e. they indirect through function pointers initialized as appropriate
 * to the run-time architecture in pmap_bootstrap. See also pmap.h.
 */

#if defined(SUN4M)
void mmu_setup4m_L1(int, struct pmap *);
void mmu_setup4m_L2(int, struct regmap *);
void  mmu_setup4m_L3(int, struct segmap *);
void	mmu_reservemon4m(struct pmap *);

void	pmap_rmk4m(struct pmap *, vaddr_t, vaddr_t, int, int);
void	pmap_rmu4m(struct pmap *, vaddr_t, vaddr_t, int, int);
int	pmap_enk4m(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
int	pmap_enu4m(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
void	pv_changepte4m(struct pvlist *, int, int);
int	pv_syncflags4m(struct pvlist *);
int	pv_link4m(struct pvlist *, struct pmap *, vaddr_t, int);
void	pv_unlink4m(struct pvlist *, struct pmap *, vaddr_t);
#endif

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
void	mmu_reservemon4_4c(int *, int *);
void	pmap_rmk4_4c(struct pmap *, vaddr_t, vaddr_t, int, int);
void	pmap_rmu4_4c(struct pmap *, vaddr_t, vaddr_t, int, int);
int	pmap_enk4_4c(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
int	pmap_enu4_4c(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
void	pv_changepte4_4c(struct pvlist *, int, int);
int	pv_syncflags4_4c(struct pvlist *);
int	pv_link4_4c(struct pvlist *, struct pmap *, vaddr_t, int);
void	pv_unlink4_4c(struct pvlist *, struct pmap *, vaddr_t);
#endif

#if !(defined(SUN4D) || defined(SUN4M)) && (defined(SUN4) || defined(SUN4C) || defined(SUN4E))
#define		pmap_rmk	pmap_rmk4_4c
#define		pmap_rmu	pmap_rmu4_4c

#elif (defined(SUN4D) || defined(SUN4M)) && !(defined(SUN4) || defined(SUN4C) || defined(SUN4E))
#define		pmap_rmk	pmap_rmk4m
#define		pmap_rmu	pmap_rmu4m

#else  /* must use function pointers */

/* function pointer declarations */
/* from pmap.h: */
boolean_t	(*pmap_clear_modify_p)(struct vm_page *);
boolean_t	(*pmap_clear_reference_p)(struct vm_page *);
int		(*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t	(*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *);
boolean_t	(*pmap_is_modified_p)(struct vm_page *);
boolean_t	(*pmap_is_referenced_p)(struct vm_page *);
void		(*pmap_kenter_pa_p)(vaddr_t, paddr_t, vm_prot_t);
void		(*pmap_page_protect_p)(struct vm_page *, vm_prot_t);
void		(*pmap_protect_p)(pmap_t, vaddr_t, vaddr_t, vm_prot_t);
void		(*pmap_copy_page_p)(struct vm_page *, struct vm_page *);
void            (*pmap_zero_page_p)(struct vm_page *);
void	       	(*pmap_changeprot_p)(pmap_t, vaddr_t, vm_prot_t, int);
/* local: */
void 		(*pmap_rmk_p)(struct pmap *, vaddr_t, vaddr_t, int, int);
void 		(*pmap_rmu_p)(struct pmap *, vaddr_t, vaddr_t, int, int);

#define		pmap_rmk	(*pmap_rmk_p)
#define		pmap_rmu	(*pmap_rmu_p)

#endif

/* --------------------------------------------------------------*/

/*
 * Next we have some Sun4m-specific routines which have no 4/4c
 * counterparts, or which are 4/4c macros.
 */

#if defined(SUN4M)

/*
 * Macros which implement SRMMU TLB flushing/invalidation
 */

#define tlb_flush_page(va)	\
	sta(((vaddr_t)(va) & ~0xfff) | ASI_SRMMUFP_L3, ASI_SRMMUFP,0)
#define tlb_flush_segment(vr, vs)	\
	sta(((vr)<<RGSHIFT) | ((vs)<<SGSHIFT) | ASI_SRMMUFP_L2, ASI_SRMMUFP,0)
#define tlb_flush_context()   sta(ASI_SRMMUFP_L1, ASI_SRMMUFP, 0)
#define tlb_flush_all()	      sta(ASI_SRMMUFP_LN, ASI_SRMMUFP, 0)

/*
 * VA2PA(addr) -- converts a virtual address to a physical address using
 * the MMU's currently-installed page tables. As a side effect, the address
 * translation used may cause the associated pte to be encached. The correct
 * context for VA must be set before this is called.
 *
 * This routine should work with any level of mapping, as it is used
 * during bootup to interact with the ROM's initial L1 mapping of the kernel.
 */
u_int
VA2PA(addr)
	caddr_t addr;
{
	u_int pte;

	/* we'll use that handy SRMMU flush/probe! %%%: make consts below! */
	/* Try each level in turn until we find a valid pte. Otherwise panic */

	pte = lda(((u_int)addr & ~0xfff) | ASI_SRMMUFP_L3, ASI_SRMMUFP);
	/* Unlock fault status; required on Hypersparc modules */
	(void)lda(SRMMU_SFSR, ASI_SRMMU);
	if ((pte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((pte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0xfff));

	/* A `TLB Flush Entire' is required before any L0, L1 or L2 probe */
	tlb_flush_all();

	pte = lda(((u_int)addr & ~0xfff) | ASI_SRMMUFP_L2, ASI_SRMMUFP);
	if ((pte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((pte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0x3ffff));
	pte = lda(((u_int)addr & ~0xfff) | ASI_SRMMUFP_L1, ASI_SRMMUFP);
	if ((pte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((pte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0xffffff));
	pte = lda(((u_int)addr & ~0xfff) | ASI_SRMMUFP_L0, ASI_SRMMUFP);
	if ((pte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((pte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0xffffffff));

	panic("VA2PA: Asked to translate unmapped VA %p", addr);
}

/*
 * Get the pointer to the pte for the given (pmap, va).
 *
 * Assumes level 3 mapping (for now).
 */
u_int *
getptep4m(pm, va)
        struct pmap *pm;
        vaddr_t va;
{
        struct regmap *rm;
        struct segmap *sm;
        int vr, vs;
        vr = VA_VREG(va);
        vs = VA_VSEG(va);

        rm = &pm->pm_regmap[vr];
#ifdef notyet
        if ((rm->rg_seg_ptps[vs] & SRMMU_TETYPE) == SRMMU_TEPTE)
                return &rm->rg_seg_ptps[vs];
#endif
	if (rm->rg_segmap == NULL)
		return NULL;

        sm = &rm->rg_segmap[vs];

	if (sm->sg_pte == NULL)
		return NULL;

        return &sm->sg_pte[VA_SUN4M_VPG(va)];
}

/*
 * Set the pte at "ptep" to "pte".
 */
static __inline void
setpgt4m(ptep, pte)
	int *ptep;
	int pte;
{
	swap(ptep, pte);
}

/*
 * Set the page table entry for va to pte. Only legal for kernel mappings.
 */
void
setpte4m(va, pte)
	vaddr_t va;
	int pte;
{
	int *ptep;

	ptep = getptep4m(pmap_kernel(), va);
	tlb_flush_page(va);
	setpgt4m(ptep, pte);
}

/*
 * Translation table for kernel vs. PTE protection bits.
 */
u_int protection_codes[2][8];
#define pte_prot4m(pm, prot) (protection_codes[pm == pmap_kernel()?0:1][prot])

void
sparc_protection_init4m(void)
{
	u_int prot, *kp, *up;

	kp = protection_codes[0];
	up = protection_codes[1];

	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case PROT_READ | PROT_WRITE | PROT_EXEC:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RWX_RWX;
			break;
		case PROT_READ | PROT_WRITE | PROT_NONE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RW_RW;
			break;
		case PROT_READ | PROT_NONE  | PROT_EXEC:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_RX_RX;
			break;
		case PROT_READ | PROT_NONE  | PROT_NONE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_R_R;
			break;
		case PROT_NONE | PROT_WRITE | PROT_EXEC:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RWX_RWX;
			break;
		case PROT_NONE | PROT_WRITE | PROT_NONE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RW_RW;
			break;
		case PROT_NONE | PROT_NONE  | PROT_EXEC:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_X_X;
			break;
		case PROT_NONE | PROT_NONE  | PROT_NONE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_R_R;
			break;
		}
	}
}

#endif /* 4m only */

/*----------------------------------------------------------------*/

/*
 * The following three macros are to be used in sun4/sun4c code only.
 */
#if defined(SUN4_MMU3L)
#define CTX_USABLE(pm,rp) (					\
		((pm)->pm_ctx != NULL &&			\
		 (!HASSUN4_MMU3L || (rp)->rg_smeg != reginval))	\
)
#else
#define CTX_USABLE(pm,rp)	((pm)->pm_ctx != NULL )
#endif

#define GAP_WIDEN(pm,vr) do if (CPU_ISSUN4OR4COR4E) {	\
	if (vr + 1 == pm->pm_gap_start)			\
		pm->pm_gap_start = vr;			\
	if (vr == pm->pm_gap_end)			\
		pm->pm_gap_end = vr + 1;		\
} while (0)

#define GAP_SHRINK(pm,vr) do if (CPU_ISSUN4OR4COR4E) {			\
	int x;							\
	x = pm->pm_gap_start + (pm->pm_gap_end - pm->pm_gap_start) / 2;	\
	if (vr > x) {							\
		if (vr < pm->pm_gap_end)				\
			pm->pm_gap_end = vr;				\
	} else {							\
		if (vr >= pm->pm_gap_start && x != pm->pm_gap_start)	\
			pm->pm_gap_start = vr + 1;			\
	}								\
} while (0)


void get_phys_mem(void **);
void	ctx_alloc(struct pmap *);
void	ctx_free(struct pmap *);
void	pg_flushcache(struct vm_page *);
#ifdef DEBUG
void	pm_check(char *, struct pmap *);
void	pm_check_k(char *, struct pmap *);
void	pm_check_u(char *, struct pmap *);
#endif

/*
 * During the PMAP bootstrap, we can use a simple translation to map a
 * kernel virtual address to a physical memory address (this is arranged
 * in locore).  Usually, KERNBASE maps to physical address 0. This is always
 * the case on sun4 and sun4c machines (unless the kernel is too large to fit
 * under the second stage bootloader in memory). On sun4m machines, if no
 * memory is installed in the bank corresponding to physical address 0, or
 * again if the kernel is large, the boot blocks may elect to load us at
 * some other address, presumably at the start of the first memory bank that
 * is large enough to hold the kernel image. We set the up the variable
 * `va2pa_offset' to hold the physical address corresponding to KERNBASE.
 */

static u_long va2pa_offset;
#define PMAP_BOOTSTRAP_VA2PA(v) ((paddr_t)((u_long)(v) - va2pa_offset))
#define PMAP_BOOTSTRAP_PA2VA(p) ((vaddr_t)((u_long)(p) + va2pa_offset))

/*
 * Grab physical memory list.
 * While here, compute `physmem'.
 */
void
get_phys_mem(void **top)
{
	struct memarr *mp;
	char *p;
	int i;

	/* Load the memory descriptor array at the current kernel top */
	p = (void *)ALIGN(*top);
	pmemarr = (struct memarr *)p;
	npmemarr = makememarr(pmemarr, 1000, MEMARR_AVAILPHYS);

	/* Update kernel top */
	p += npmemarr * sizeof(struct memarr);
	*top = p;

	for (physmem = 0, mp = pmemarr, i = npmemarr; --i >= 0; mp++) {
#ifdef SUN4D
		if (CPU_ISSUN4D) {
			/*
			 * XXX Limit ourselves to 2GB of physical memory
			 * XXX for now.
			 */
			uint32_t addr, len;
			int skip = 0;

			addr = mp->addr_lo;
			len = mp->len;
			if (mp->addr_hi != 0 || addr >= 0x80000000)
				skip = 1;
			else {
				if (len >= 0x80000000)
					len = 0x80000000;
				if (addr + len > 0x80000000)
					len = 0x80000000 - addr;
			}
			if (skip)
				len = 0;	/* disable this entry */
			mp->len = len;
		}
#endif
		physmem += atop(mp->len);
	}
}

/*
 * Support functions for vm_page_bootstrap();
 */

/*
 * How much virtual space does this kernel have?
 * (After mapping kernel text, data, etc.)
 */
void
pmap_virtual_space(v_start, v_end)
        vaddr_t *v_start;
        vaddr_t *v_end;
{
        *v_start = virtual_avail;
        *v_end   = virtual_end;
}

/*
 * Helper routine that hands off available physical pages to the VM system.
 */
void
pmap_page_upload(void)
{
	int	n;
	paddr_t	start, end;

	for (n = 0; n < npmemarr; n++) {
		start = pmemarr[n].addr_lo;
		end = start + pmemarr[n].len;

		/*
		 * Exclude any memory allocated for the kernel as computed
		 * by pmap_bootstrap(), i.e. the range
		 *	[KERNBASE_PA, avail_start>.
		 */
		if (start < PMAP_BOOTSTRAP_VA2PA(KERNBASE)) {
			/*
			 * This segment starts below the kernel load address.
			 * Chop it off at the start of the kernel.
			 */
			paddr_t	chop = PMAP_BOOTSTRAP_VA2PA(KERNBASE);

			if (end < chop)
				chop = end;
#ifdef DEBUG
			printf("bootstrap gap: start %lx, chop %lx, end %lx\n",
				start, chop, end);
#endif
			uvm_page_physload(atop(start), atop(chop),
				atop(start), atop(chop), 0);

			/*
			 * Adjust the start address to reflect the
			 * uploaded portion of this segment.
			 */
			start = chop;
		}

		/* Skip the current kernel address range */
		if (start <= avail_start && avail_start < end)
			start = avail_start;

		if (start == end)
			continue;

		/* Upload (the rest of) this segment */
		uvm_page_physload(atop(start), atop(end),
			atop(start), atop(end), 0);
	}
}

/*
 * This routine is used by mmrw() to validate access to `/dev/mem'.
 */
int
pmap_pa_exists(paddr_t pa)
{
	int nmem;
	struct memarr *mp;

	for (mp = pmemarr, nmem = npmemarr; --nmem >= 0; mp++) {
#ifdef SUN4D
		if (mp->len == 0)
			continue;
#endif
		if (pa >= mp->addr_lo && pa < mp->addr_lo + mp->len)
			return 1;
	}

	return 0;
}

/* update pv_flags given a valid pte */
#define	MR4_4C(pte) (((pte) >> PG_M_SHIFT) & (PV_MOD | PV_REF))
#define MR4M(pte) (((pte) >> PG_M_SHIFT4M) & (PV_MOD4M | PV_REF4M))

/*----------------------------------------------------------------*/

/*
 * Agree with the monitor ROM as to how many MMU entries are
 * to be reserved, and map all of its segments into all contexts.
 *
 * Unfortunately, while the Version 0 PROM had a nice linked list of
 * taken virtual memory, the Version 2 PROM provides instead a convoluted
 * description of *free* virtual memory.  Rather than invert this, we
 * resort to two magic constants from the PROM vector description file.
 */
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
void
mmu_reservemon4_4c(nrp, nsp)
	int *nrp, *nsp;
{
	u_int va = 0, eva = 0;
	int mmuseg, i, nr, ns, vr, lastvr;
#if defined(SUN4_MMU3L)
	int mmureg;
#endif
	struct regmap *rp;

#if defined(SUN4)
	if (CPU_ISSUN4) {
		prom_vstart = va = OLDMON_STARTVADDR;
		prom_vend = eva = OLDMON_ENDVADDR;
	}
#endif
#if defined(SUN4C) || defined(SUN4E)
	if (CPU_ISSUN4C || CPU_ISSUN4E) {
		prom_vstart = va = OPENPROM_STARTVADDR;
		prom_vend = eva = OPENPROM_ENDVADDR;
	}
#endif
	ns = *nsp;
	nr = *nrp;
	lastvr = 0;
	while (va < eva) {
		vr = VA_VREG(va);
		rp = &pmap_kernel()->pm_regmap[vr];

#if defined(SUN4_MMU3L)
		if (HASSUN4_MMU3L && vr != lastvr) {
			lastvr = vr;
			mmureg = getregmap(va);
			if (mmureg < nr)
				rp->rg_smeg = nr = mmureg;
			/*
			 * On 3-level MMU machines, we distribute regions,
			 * rather than segments, amongst the contexts.
			 */
			for (i = ncontext; --i > 0;)
				(*promvec->pv_setctxt)(i, (caddr_t)va, mmureg);
		}
#endif
		mmuseg = getsegmap(va);
		if (mmuseg < ns)
			ns = mmuseg;

		if (!HASSUN4_MMU3L)
			for (i = ncontext; --i > 0;)
				(*promvec->pv_setctxt)(i, (caddr_t)va, mmuseg);

		if (mmuseg == seginval) {
			va += NBPSG;
			continue;
		}
		/*
		 * Another PROM segment. Enter into region map.
		 * Assume the entire segment is valid.
		 */
		rp->rg_nsegmap += 1;
		rp->rg_segmap[VA_VSEG(va)].sg_pmeg = mmuseg;
		rp->rg_segmap[VA_VSEG(va)].sg_npte = NPTESG;

		/* PROM maps its memory user-accessible: fix it. */
		for (i = NPTESG; --i >= 0; va += NBPG)
			setpte4(va, getpte4(va) | PG_S);
	}
	*nsp = ns;
	*nrp = nr;
	return;
}
#endif

#if defined(SUN4M) /* Sun4M versions of above */

/*
 * Take the monitor's initial page table layout, convert it to 3rd-level pte's
 * (it starts out as a L1 mapping), and install it along with a set of kernel
 * mapping tables as the kernel's initial page table setup. Also create and
 * enable a context table. I suppose we also want to block user-mode access
 * to the new kernel/ROM mappings.
 */

/*
 * mmu_reservemon4m(): Copies the existing (ROM) page tables to kernel space,
 * converting any L1/L2 PTEs to L3 PTEs. Does *not* copy the L1 entry mapping
 * the kernel at KERNBASE since we don't want to map 16M of physical
 * memory for the kernel. Thus the kernel must be installed later!
 * Also installs ROM mappings into the kernel pmap.
 * NOTE: This also revokes all user-mode access to the mapped regions.
 */
void
mmu_reservemon4m(kpmap)
	struct pmap *kpmap;
{
	unsigned int rom_ctxtbl;
	int te;
	unsigned int mmupcrsave;

	/*
	 * XXX: although the Sun4M can handle 36 bits of physical
	 * address space, we assume that all these page tables, etc
	 * are in the lower 4G (32-bits) of address space, i.e. out of I/O
	 * space. Eventually this should be changed to support the 36 bit
	 * physical addressing, in case some crazed ROM designer decides to
	 * stick the pagetables up there. In that case, we should use MMU
	 * transparent mode, (i.e. ASI 0x20 to 0x2f) to access
	 * physical memory.
	 */

	rom_ctxtbl = (lda(SRMMU_CXTPTR,ASI_SRMMU) << SRMMU_PPNPASHIFT);

	/* We're going to have to use MMU passthrough. If we're on a
	 * Viking MicroSparc without an mbus, we need to turn off traps
	 * and set the AC bit at 0x8000 in the MMU's control register. Ugh.
	 * XXX: Once we've done this, can we still access kernel vm?
	 */
	if (cpuinfo.cpu_vers == 4 && cpuinfo.mxcc) {
		sta(SRMMU_PCR, ASI_SRMMU, 	/* set MMU AC bit */
		    ((mmupcrsave = lda(SRMMU_PCR,ASI_SRMMU)) | VIKING_PCR_AC));
	}

	te = lda(rom_ctxtbl, ASI_BYPASS);	/* i.e. context 0 */
	switch (te & SRMMU_TETYPE) {
	case SRMMU_TEINVALID:
		cpuinfo.ctx_tbl[0] = SRMMU_TEINVALID;
		panic("mmu_reservemon4m: no existing L0 mapping! "
		      "(How are we running?");
		break;
	case SRMMU_TEPTE:
#ifdef DEBUG
		printf("mmu_reservemon4m: trying to remap 4G segment!\n");
#endif
		panic("mmu_reservemon4m: can't handle ROM 4G page size");
		/* XXX: Should make this work, however stupid it is */
		break;
	case SRMMU_TEPTD:
		mmu_setup4m_L1(te, kpmap);
		break;
	default:
		panic("mmu_reservemon4m: unknown pagetable entry type");
	}

	if (cpuinfo.cpu_vers == 4 && cpuinfo.mxcc) {
		sta(SRMMU_PCR, ASI_SRMMU, mmupcrsave);
	}
}

void
mmu_setup4m_L1(regtblptd, kpmap)
	int regtblptd;		/* PTD for region table to be remapped */
	struct pmap *kpmap;
{
	unsigned int regtblrover;
	int i;
	unsigned int te;
	struct regmap *rp;
	int j, k;

	/*
	 * Here we scan the region table to copy any entries which appear.
	 * We are only concerned with regions in kernel space and above
	 * (i.e. regions VA_VREG(VM_MIN_KERNEL_ADDRESS_SRMMU) == NUREG to 0xff).
	 */
	regtblrover = ((regtblptd & ~SRMMU_TETYPE) << SRMMU_PPNPASHIFT) +
	    NUREG_4M * sizeof(long); /* kernel only */

	for (i = NUREG_4M; i < SRMMU_L1SIZE; i++, regtblrover += sizeof(long)) {
		/*
		 * Ignore the region spanning the area where the kernel has
		 * been loaded, since this is the 16MB L1 mapping that the ROM
		 * used to map the kernel in initially.
		 * Later, we will rebuild a new L3 mapping for the kernel
		 * and install it before switching to the new pagetables.
		 */
		if (i == VA_VREG(KERNBASE))
			continue;

		/* The region we're dealing with */
		rp = &kpmap->pm_regmap[i];

		te = lda(regtblrover, ASI_BYPASS);
		switch(te & SRMMU_TETYPE) {
		case SRMMU_TEINVALID:
			break;

		case SRMMU_TEPTE:
#ifdef DEBUG
			printf("mmu_setup4m_L1: "
			       "converting region 0x%x from L1->L3\n", i);
#endif
			/*
			 * This region entry covers 64MB of memory -- or
			 * (NSEGRG * NPTESG) pages -- which we must convert
			 * into a 3-level description.
			 */

			for (j = 0; j < SRMMU_L2SIZE; j++) {
				struct segmap *sp = &rp->rg_segmap[j];

				for (k = 0; k < SRMMU_L3SIZE; k++) {
					sp->sg_npte++;
					setpgt4m(&sp->sg_pte[k],
					    (te & SRMMU_L1PPNMASK) |
					    (j << SRMMU_L2PPNSHFT) |
					    (k << SRMMU_L3PPNSHFT) |
					    (te & SRMMU_PGBITSMSK) |
					    ((te & SRMMU_PROT_MASK) |
					     PPROT_U2S_OMASK) |
					    SRMMU_TEPTE);
				}
			}
			break;

		case SRMMU_TEPTD:
			mmu_setup4m_L2(te, rp);
			break;

		default:
			panic("mmu_setup4m_L1: unknown pagetable entry type");
		}
	}
}

void
mmu_setup4m_L2(segtblptd, rp)
	int segtblptd;
	struct regmap *rp;
{
	unsigned int segtblrover;
	int i, k;
	unsigned int te;
	struct segmap *sp;

	segtblrover = (segtblptd & ~SRMMU_TETYPE) << SRMMU_PPNPASHIFT;
	for (i = 0; i < SRMMU_L2SIZE; i++, segtblrover += sizeof(long)) {

		sp = &rp->rg_segmap[i];

		te = lda(segtblrover, ASI_BYPASS);
		switch(te & SRMMU_TETYPE) {
		case SRMMU_TEINVALID:
			break;

		case SRMMU_TEPTE:
#ifdef DEBUG
			printf("mmu_setup4m_L2: converting L2 entry at segment 0x%x to L3\n",i);
#endif
			/*
			 * This segment entry covers 256KB of memory -- or
			 * (NPTESG) pages -- which we must convert
			 * into a 3-level description.
			 */
			for (k = 0; k < SRMMU_L3SIZE; k++) {
				sp->sg_npte++;
				setpgt4m(&sp->sg_pte[k],
				    (te & SRMMU_L1PPNMASK) |
				    (te & SRMMU_L2PPNMASK) |
				    (k << SRMMU_L3PPNSHFT) |
				    (te & SRMMU_PGBITSMSK) |
				    ((te & SRMMU_PROT_MASK) |
				     PPROT_U2S_OMASK) |
				    SRMMU_TEPTE);
			}
			break;

		case SRMMU_TEPTD:
			mmu_setup4m_L3(te, sp);
			break;

		default:
			panic("mmu_setup4m_L2: unknown pagetable entry type");
		}
	}
}

void
mmu_setup4m_L3(pagtblptd, sp)
	int pagtblptd;
	struct segmap *sp;
{
	unsigned int pagtblrover;
	int i;
	unsigned int te;

	pagtblrover = (pagtblptd & ~SRMMU_TETYPE) << SRMMU_PPNPASHIFT;
	for (i = 0; i < SRMMU_L3SIZE; i++, pagtblrover += sizeof(long)) {
		te = lda(pagtblrover, ASI_BYPASS);
		switch(te & SRMMU_TETYPE) {
		case SRMMU_TEINVALID:
			break;
		case SRMMU_TEPTE:
			sp->sg_npte++;
			setpgt4m(&sp->sg_pte[i], te | PPROT_U2S_OMASK);
			pmap_kernel()->pm_stats.resident_count++;
			break;
		case SRMMU_TEPTD:
			panic("mmu_setup4m_L3: PTD found in L3 page table");
		default:
			panic("mmu_setup4m_L3: unknown pagetable entry type");
		}
	}
}
#endif /* defined SUN4M */

/*----------------------------------------------------------------*/

/*
 * MMU management.
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E) /* This is old sun MMU stuff */

/*
 * Change contexts.  We need the old context number as well as the new
 * one.  If the context is changing, we must write all user windows
 * first, lest an interrupt cause them to be written to the (other)
 * user whose context we set here.
 */
#define	CHANGE_CONTEXTS(old, new) \
	if ((old) != (new)) { \
		write_user_windows(); \
		setcontext4(new); \
	}

struct mmuentry *me_alloc(struct mmuhd *, struct pmap *, int, int);
void		me_free(struct pmap *, u_int);
struct mmuentry	*region_alloc(struct mmuhd *, struct pmap *, int);
void		region_free(struct pmap *, u_int);

/*
 * Allocate an MMU entry (i.e., a PMEG).
 * If necessary, steal one from someone else.
 * Put it on the tail of the given queue
 * (which is either the LRU list or the locked list).
 * The locked list is not actually ordered, but this is easiest.
 * Also put it on the given (new) pmap's chain,
 * enter its pmeg number into that pmap's segmap,
 * and store the pmeg's new virtual segment number (me->me_vseg).
 *
 * This routine is large and complicated, but it must be fast
 * since it implements the dynamic allocation of MMU entries.
 */
struct mmuentry *
me_alloc(mh, newpm, newvreg, newvseg)
	struct mmuhd *mh;
	struct pmap *newpm;
	int newvreg, newvseg;
{
	struct mmuentry *me;
	struct pmap *pm;
	int i, va, *pte, tpte;
	int ctx;
	struct regmap *rp;
	struct segmap *sp;

	/* try free list first */
	if (!TAILQ_EMPTY(&segm_freelist)) {
		me = TAILQ_FIRST(&segm_freelist);
		TAILQ_REMOVE(&segm_freelist, me, me_list);
#ifdef DEBUG
		if (me->me_pmap != NULL)
			panic("me_alloc: freelist entry has pmap");
		if (pmapdebug & PDB_MMU_ALLOC)
			printf("me_alloc: got pmeg %d\n", me->me_cookie);
#endif
		TAILQ_INSERT_TAIL(mh, me, me_list);

		/* onto on pmap chain; pmap is already locked, if needed */
		TAILQ_INSERT_TAIL(&newpm->pm_seglist, me, me_pmchain);
#ifdef DIAGNOSTIC
		pmap_stats.ps_npmeg_free--;
		if (mh == &segm_locked)
			pmap_stats.ps_npmeg_locked++;
		else
			pmap_stats.ps_npmeg_lru++;
#endif

		/* into pmap segment table, with backpointers */
		newpm->pm_regmap[newvreg].rg_segmap[newvseg].sg_pmeg = me->me_cookie;
		me->me_pmap = newpm;
		me->me_vseg = newvseg;
		me->me_vreg = newvreg;

		return (me);
	}

	/* no luck, take head of LRU list */
	if ((me = TAILQ_FIRST(&segm_lru)) == NULL)
		panic("me_alloc: all pmegs gone");

	pm = me->me_pmap;
	if (pm == NULL)
		panic("me_alloc: LRU entry has no pmap");
	if (pm == pmap_kernel())
		panic("me_alloc: stealing from kernel");
#ifdef DEBUG
	if (pmapdebug & (PDB_MMU_ALLOC | PDB_MMU_STEAL))
		printf("me_alloc: stealing pmeg 0x%x from pmap %p\n",
		    me->me_cookie, pm);
#endif
	/*
	 * Remove from LRU list, and insert at end of new list
	 * (probably the LRU list again, but so what?).
	 */
	TAILQ_REMOVE(&segm_lru, me, me_list);
	TAILQ_INSERT_TAIL(mh, me, me_list);

#ifdef DIAGNOSTIC
	if (mh == &segm_locked) {
		pmap_stats.ps_npmeg_lru--;
		pmap_stats.ps_npmeg_locked++;
	}
#endif

	rp = &pm->pm_regmap[me->me_vreg];
	if (rp->rg_segmap == NULL)
		panic("me_alloc: LRU entry's pmap has no segments");
	sp = &rp->rg_segmap[me->me_vseg];
	pte = sp->sg_pte;
	if (pte == NULL)
		panic("me_alloc: LRU entry's pmap has no ptes");

	/*
	 * The PMEG must be mapped into some context so that we can
	 * read its PTEs.  Use its current context if it has one;
	 * if not, and since context 0 is reserved for the kernel,
	 * the simplest method is to switch to 0 and map the PMEG
	 * to virtual address 0---which, being a user space address,
	 * is by definition not in use.
	 *
	 * XXX for ncpus>1 must use per-cpu VA?
	 * XXX do not have to flush cache immediately
	 */
	ctx = getcontext4();
	if (CTX_USABLE(pm,rp)) {
		CHANGE_CONTEXTS(ctx, pm->pm_ctxnum);
		cache_flush_segment(me->me_vreg, me->me_vseg);
		va = VSTOVA(me->me_vreg,me->me_vseg);
	} else {
		CHANGE_CONTEXTS(ctx, 0);
		if (HASSUN4_MMU3L)
			setregmap(0, tregion);
		setsegmap(0, me->me_cookie);
		/*
		 * No cache flush needed: it happened earlier when
		 * the old context was taken.
		 */
		va = 0;
	}

	/*
	 * Record reference and modify bits for each page,
	 * and copy PTEs into kernel memory so that they can
	 * be reloaded later.
	 */
	i = NPTESG;
	do {
		tpte = getpte4(va);
		if ((tpte & (PG_V | PG_TYPE)) == (PG_V | PG_OBMEM)) {
			struct pvlist *pv;

			pv = pvhead(tpte & PG_PFNUM);
			if (pv)
				pv->pv_flags |= MR4_4C(tpte);
		}
		*pte++ = tpte & ~(PG_U|PG_M);
		va += NBPG;
	} while (--i > 0);

	/* update segment tables */
	if (CTX_USABLE(pm,rp))
		setsegmap(VSTOVA(me->me_vreg,me->me_vseg), seginval);
	sp->sg_pmeg = seginval;

	/* off old pmap chain */
	TAILQ_REMOVE(&pm->pm_seglist, me, me_pmchain);
	setcontext4(ctx);	/* done with old context */

	/* onto new pmap chain; new pmap is already locked, if needed */
	TAILQ_INSERT_TAIL(&newpm->pm_seglist, me, me_pmchain);

	/* into new segment table, with backpointers */
	newpm->pm_regmap[newvreg].rg_segmap[newvseg].sg_pmeg = me->me_cookie;
	me->me_pmap = newpm;
	me->me_vseg = newvseg;
	me->me_vreg = newvreg;

	return (me);
}

/*
 * Free an MMU entry.
 *
 * Assumes the corresponding pmap is already locked.
 * Does NOT flush cache, but does record ref and mod bits.
 * The rest of each PTE is discarded.
 * CALLER MUST SET CONTEXT to pm->pm_ctxnum (if pmap has
 * a context) or to 0 (if not).  Caller must also update
 * pm->pm_segmap and (possibly) the hardware.
 */
void
me_free(pm, pmeg)
	struct pmap *pm;
	u_int pmeg;
{
	struct mmuentry *me = &mmusegments[pmeg];
	int i, va, tpte;
	int vr;
	struct regmap *rp;

	vr = me->me_vreg;

#ifdef DEBUG
	if (pmapdebug & PDB_MMU_ALLOC)
		printf("me_free: freeing pmeg %d from pmap %p\n",
		    me->me_cookie, pm);
	if (me->me_cookie != pmeg)
		panic("me_free: wrong mmuentry");
	if (pm != me->me_pmap)
		panic("me_free: pm != me_pmap");
#endif

	rp = &pm->pm_regmap[vr];

	/* just like me_alloc, but no cache flush, and context already set */
	if (CTX_USABLE(pm,rp)) {
		va = VSTOVA(vr,me->me_vseg);
	} else {
#ifdef DEBUG
if (getcontext4() != 0) panic("me_free: ctx != 0");
#endif
		if (HASSUN4_MMU3L)
			setregmap(0, tregion);
		setsegmap(0, me->me_cookie);
		va = 0;
	}
	i = NPTESG;
	do {
		tpte = getpte4(va);
		if ((tpte & (PG_V | PG_TYPE)) == (PG_V | PG_OBMEM)) {
			struct pvlist *pv;

			pv = pvhead(tpte & PG_PFNUM);
			if (pv)
				pv->pv_flags |= MR4_4C(tpte);
		}
		va += NBPG;
	} while (--i > 0);

	/* take mmu entry off pmap chain */
	TAILQ_REMOVE(&pm->pm_seglist, me, me_pmchain);
	/* ... and remove from segment map */
	if (rp->rg_segmap == NULL)
		panic("me_free: no segments in pmap");
	rp->rg_segmap[me->me_vseg].sg_pmeg = seginval;

	/* off LRU or lock chain */
	if (pm == pmap_kernel()) {
		TAILQ_REMOVE(&segm_locked, me, me_list);
#ifdef DIAGNOSTIC
		pmap_stats.ps_npmeg_locked--;
#endif
	} else {
		TAILQ_REMOVE(&segm_lru, me, me_list);
#ifdef DIAGNOSTIC
		pmap_stats.ps_npmeg_lru--;
#endif
	}

	/* no associated pmap; on free list */
	me->me_pmap = NULL;
	TAILQ_INSERT_TAIL(&segm_freelist, me, me_list);
#ifdef DIAGNOSTIC
	pmap_stats.ps_npmeg_free++;
#endif
}

#if defined(SUN4_MMU3L)

/* XXX - Merge with segm_alloc/segm_free ? */

struct mmuentry *
region_alloc(mh, newpm, newvr)
	struct mmuhd *mh;
	struct pmap *newpm;
	int newvr;
{
	struct mmuentry *me;
	struct pmap *pm;
	int ctx;
	struct regmap *rp;

	/* try free list first */
	if (!TAILQ_EMPTY(&region_freelist)) {
		me = TAILQ_FIRST(&region_freelist);
		TAILQ_REMOVE(&region_freelist, me, me_list);
#ifdef DEBUG
		if (me->me_pmap != NULL)
			panic("region_alloc: freelist entry has pmap");
		if (pmapdebug & PDB_MMUREG_ALLOC)
			printf("region_alloc: got smeg 0x%x\n", me->me_cookie);
#endif
		TAILQ_INSERT_TAIL(mh, me, me_list);

		/* onto on pmap chain; pmap is already locked, if needed */
		TAILQ_INSERT_TAIL(&newpm->pm_reglist, me, me_pmchain);

		/* into pmap segment table, with backpointers */
		newpm->pm_regmap[newvr].rg_smeg = me->me_cookie;
		me->me_pmap = newpm;
		me->me_vreg = newvr;

		return (me);
	}

	/* no luck, take head of LRU list */
	if ((me = TAILQ_FIRST(&region_lru)) == NULL)
		panic("region_alloc: all smegs gone");

	pm = me->me_pmap;
	if (pm == NULL)
		panic("region_alloc: LRU entry has no pmap");
	if (pm == pmap_kernel())
		panic("region_alloc: stealing from kernel");
#ifdef DEBUG
	if (pmapdebug & (PDB_MMUREG_ALLOC | PDB_MMUREG_STEAL))
		printf("region_alloc: stealing smeg 0x%x from pmap %p\n",
		    me->me_cookie, pm);
#endif
	/*
	 * Remove from LRU list, and insert at end of new list
	 * (probably the LRU list again, but so what?).
	 */
	TAILQ_REMOVE(&region_lru, me, me_list);
	TAILQ_INSERT_TAIL(mh, me, me_list);

	rp = &pm->pm_regmap[me->me_vreg];
	ctx = getcontext4();
	if (pm->pm_ctx) {
		CHANGE_CONTEXTS(ctx, pm->pm_ctxnum);
		cache_flush_region(me->me_vreg);
	}

	/* update region tables */
	if (pm->pm_ctx)
		setregmap(VRTOVA(me->me_vreg), reginval);
	rp->rg_smeg = reginval;

	/* off old pmap chain */
	TAILQ_REMOVE(&pm->pm_reglist, me, me_pmchain);
	setcontext4(ctx);	/* done with old context */

	/* onto new pmap chain; new pmap is already locked, if needed */
	TAILQ_INSERT_TAIL(&newpm->pm_reglist, me, me_pmchain);

	/* into new segment table, with backpointers */
	newpm->pm_regmap[newvr].rg_smeg = me->me_cookie;
	me->me_pmap = newpm;
	me->me_vreg = newvr;

	return (me);
}

/*
 * Free an MMU entry.
 *
 * Assumes the corresponding pmap is already locked.
 * Does NOT flush cache. ???
 * CALLER MUST SET CONTEXT to pm->pm_ctxnum (if pmap has
 * a context) or to 0 (if not).  Caller must also update
 * pm->pm_regmap and (possibly) the hardware.
 */
void
region_free(pm, smeg)
	struct pmap *pm;
	u_int smeg;
{
	struct mmuentry *me = &mmuregions[smeg];

#ifdef DEBUG
	if (pmapdebug & PDB_MMUREG_ALLOC)
		printf("region_free: freeing smeg 0x%x from pmap %p\n",
		    me->me_cookie, pm);
	if (me->me_cookie != smeg)
		panic("region_free: wrong mmuentry");
	if (pm != me->me_pmap)
		panic("region_free: pm != me_pmap");
#endif

	if (pm->pm_ctx)
		cache_flush_region(me->me_vreg);

	/* take mmu entry off pmap chain */
	TAILQ_REMOVE(&pm->pm_reglist, me, me_pmchain);
	/* ... and remove from segment map */
	pm->pm_regmap[smeg].rg_smeg = reginval;

	/* off LRU or lock chain */
	if (pm == pmap_kernel()) {
		TAILQ_REMOVE(&region_locked, me, me_list);
	} else {
		TAILQ_REMOVE(&region_lru, me, me_list);
	}

	/* no associated pmap; on free list */
	me->me_pmap = NULL;
	TAILQ_INSERT_TAIL(&region_freelist, me, me_list);
}
#endif

/*
 * `Page in' (load or inspect) an MMU entry; called on page faults.
 * Returns 1 if we reloaded the segment, -1 if the segment was
 * already loaded and the page was marked valid (in which case the
 * fault must be a bus error or something), or 0 (segment loaded but
 * PTE not valid, or segment not loaded at all).
 */
int
mmu_pagein(pm, va, prot)
	struct pmap *pm;
	vaddr_t va;
	int prot;
{
	int *pte;
	int vr, vs, pmeg, i, s, bits;
	struct regmap *rp;
	struct segmap *sp;

	if (prot != PROT_NONE)
		bits = PG_V | ((prot & PROT_WRITE) ? PG_W : 0);
	else
		bits = 0;

	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
#ifdef DEBUG
if (pm == pmap_kernel())
printf("mmu_pagein: kernel wants map at va 0x%lx, vr %d, vs %d\n", va, vr, vs);
#endif

	/* return 0 if we have no PMEGs to load */
	if (rp->rg_segmap == NULL)
		return (0);

#if defined(SUN4_MMU3L)
	if (HASSUN4_MMU3L && rp->rg_smeg == reginval) {
		smeg_t smeg;
		unsigned int tva = VA_ROUNDDOWNTOREG(va);
		struct segmap *sp = rp->rg_segmap;

		s = splvm();		/* paranoid */
		smeg = region_alloc(&region_lru, pm, vr)->me_cookie;
		setregmap(tva, smeg);
		i = NSEGRG;
		do {
			setsegmap(tva, sp++->sg_pmeg);
			tva += NBPSG;
		} while (--i > 0);
		splx(s);
	}
#endif
	sp = &rp->rg_segmap[vs];

	/* return 0 if we have no PTEs to load */
	if ((pte = sp->sg_pte) == NULL)
		return (0);

	/* return -1 if the fault is `hard', 0 if not */
	if (sp->sg_pmeg != seginval)
		return (bits && (getpte4(va) & bits) == bits ? -1 : 0);

	/* reload segment: write PTEs into a new LRU entry */
	va = VA_ROUNDDOWNTOSEG(va);
	s = splvm();		/* paranoid */
	pmeg = me_alloc(&segm_lru, pm, vr, vs)->me_cookie;
	setsegmap(va, pmeg);
	i = NPTESG;
	do {
		setpte4(va, *pte++);
		va += NBPG;
	} while (--i > 0);
	splx(s);
	return (1);
}
#endif /* SUN4 || SUN4C || SUN4E */

/*
 * Allocate a context.  If necessary, steal one from someone else.
 * Changes hardware context number and loads segment map.
 *
 * This routine is only ever called from locore.s just after it has
 * saved away the previous process, so there are no active user windows.
 */
void
ctx_alloc(pm)
	struct pmap *pm;
{
	union ctxinfo *c;
	int s, cnum, i, doflush;
	struct regmap *rp;
	int gap_start, gap_end;
	unsigned long va;

#ifdef DEBUG
	if (pm->pm_ctx)
		panic("ctx_alloc pm_ctx");
	if (pmapdebug & PDB_CTX_ALLOC)
		printf("ctx_alloc(%p)\n", pm);
#endif
	if (CPU_ISSUN4OR4COR4E) {
		gap_start = pm->pm_gap_start;
		gap_end = pm->pm_gap_end;
	}

	s = splvm();
	if ((c = ctx_freelist) != NULL) {
		ctx_freelist = c->c_nextfree;
		cnum = c - cpuinfo.ctxinfo;
		doflush = 0;
	} else {
		if ((ctx_kick += ctx_kickdir) >= ncontext) {
			ctx_kick = ncontext - 1;
			ctx_kickdir = -1;
		} else if (ctx_kick < 1) {
			ctx_kick = 1;
			ctx_kickdir = 1;
		}
		c = &cpuinfo.ctxinfo[cnum = ctx_kick];
#ifdef DEBUG
		if (c->c_pmap == NULL)
			panic("ctx_alloc cu_pmap");
		if (pmapdebug & (PDB_CTX_ALLOC | PDB_CTX_STEAL))
			printf("ctx_alloc: steal context %d from %p\n",
			    cnum, c->c_pmap);
#endif
		c->c_pmap->pm_ctx = NULL;
		doflush = (CACHEINFO.c_vactype != VAC_NONE);
		if (CPU_ISSUN4OR4COR4E) {
			if (gap_start < c->c_pmap->pm_gap_start)
				gap_start = c->c_pmap->pm_gap_start;
			if (gap_end > c->c_pmap->pm_gap_end)
				gap_end = c->c_pmap->pm_gap_end;
		}
	}

	c->c_pmap = pm;
	pm->pm_ctx = c;
	pm->pm_ctxnum = cnum;

	if (CPU_ISSUN4OR4COR4E) {
		/*
		 * Write pmap's region (3-level MMU) or segment table into
		 * the MMU.
		 *
		 * Only write those entries that actually map something in
		 * this context by maintaining a pair of region numbers in
		 * between which the pmap has no valid mappings.
		 *
		 * If a context was just allocated from the free list, trust
		 * that all its pmeg numbers are `seginval'. We make sure this
		 * is the case initially in pmap_bootstrap(). Otherwise, the
		 * context was freed by calling ctx_free() in pmap_release(),
		 * which in turn is supposedly called only when all mappings
		 * have been removed.
		 *
		 * On the other hand, if the context had to be stolen from
		 * another pmap, we possibly shrink the gap to be the
		 * disjuction of the new and the previous map.
		 */

		setcontext4(cnum);
		if (doflush)
			cache_flush_context();

		rp = pm->pm_regmap;
		for (va = 0, i = NUREG_4C; --i >= 0; ) {
			if (VA_VREG(va) >= gap_start) {
				va = VRTOVA(gap_end);
				i -= gap_end - gap_start;
				rp += gap_end - gap_start;
				if (i < 0)
					break;
				/* mustn't re-enter this branch */
				gap_start = NUREG_4C;
			}
			if (HASSUN4_MMU3L) {
				setregmap(va, rp++->rg_smeg);
				va += NBPRG;
			} else {
				int j;
				struct segmap *sp = rp->rg_segmap;
				for (j = NSEGRG; --j >= 0; va += NBPSG)
					setsegmap(va,
						  sp?sp++->sg_pmeg:seginval);
				rp++;
			}
		}
		splx(s);

	} else if (CPU_ISSUN4M) {

#if defined(SUN4M)
		/*
		 * Reload page and context tables to activate the page tables
		 * for this context.
		 *
		 * The gap stuff isn't really needed in the Sun4m architecture,
		 * since we don't have to worry about excessive mappings (all
		 * mappings exist since the page tables must be complete for
		 * the mmu to be happy).
		 *
		 * If a context was just allocated from the free list, trust
		 * that all of its mmu-edible page tables are zeroed out
		 * (except for those associated with the kernel). We make
		 * sure this is the case initially in pmap_bootstrap() and
		 * pmap_init() (?).
		 * Otherwise, the context was freed by calling ctx_free() in
		 * pmap_release(), which in turn is supposedly called only
		 * when all mappings have been removed.
		 *
		 * XXX: Do we have to flush cache after reloading ctx tbl?
		 */

		/* Do any cache flush needed on context switch */
		(*cpuinfo.pure_vcache_flush)();
#ifdef DEBUG
#if 0
		ctxbusyvector[cnum] = 1; /* mark context as busy */
#endif
		if (pm->pm_reg_ptps_pa == 0)
			panic("ctx_alloc: no region table in current pmap");
#endif
		/*setcontext(0); * paranoia? can we modify curr. ctx? */
		setpgt4m(&cpuinfo.ctx_tbl[cnum],
			(pm->pm_reg_ptps_pa >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD);

		setcontext4m(cnum);
		if (doflush)
			cache_flush_context();
		tlb_flush_context(); /* remove any remnant garbage from tlb */
#endif
		splx(s);
	}
}

/*
 * Give away a context.  Flushes cache and sets current context to 0.
 */
void
ctx_free(pm)
	struct pmap *pm;
{
	union ctxinfo *c;
	int newc, oldc;

	if ((c = pm->pm_ctx) == NULL)
		panic("ctx_free");
	pm->pm_ctx = NULL;

	if (CPU_ISSUN4M) {
#if defined(SUN4M)
		oldc = getcontext4m();
		/* Do any cache flush needed on context switch */
		(*cpuinfo.pure_vcache_flush)();
		newc = pm->pm_ctxnum;
		if (oldc != newc) {
			write_user_windows();
			setcontext4m(newc);
		}
		cache_flush_context();
		tlb_flush_context();
		setcontext4m(0);
#endif
	} else {
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
		oldc = getcontext4();
		if (CACHEINFO.c_vactype != VAC_NONE) {
			newc = pm->pm_ctxnum;
			CHANGE_CONTEXTS(oldc, newc);
			cache_flush_context();
			setcontext4(0);
		} else {
			CHANGE_CONTEXTS(oldc, 0);
		}
#endif
	}

	c->c_nextfree = ctx_freelist;
	ctx_freelist = c;

#if 0
#if defined(SUN4M)
	if (CPU_ISSUN4M) {
		/* Map kernel back into unused context */
		newc = pm->pm_ctxnum;
		cpuinfo.ctx_tbl[newc] = cpuinfo.ctx_tbl[0];
		if (newc)
			ctxbusyvector[newc] = 0; /* mark as free */
	}
#endif
#endif
}


/*----------------------------------------------------------------*/

/*
 * pvlist functions.
 */

/*
 * Walk the given pv list, and for each PTE, set or clear some bits
 * (e.g., PG_W or PG_NC).
 *
 * As a special case, this never clears PG_W on `pager' pages.
 * These, being kernel addresses, are always in hardware and have
 * a context.
 *
 * This routine flushes the cache for any page whose PTE changes,
 * as long as the process has a context; this is overly conservative.
 * It also copies ref and mod bits to the pvlist, on the theory that
 * this might save work later.  (XXX should test this theory)
 *
 * In addition, if the cacheable bit (PG_NC) is updated in the PTE
 * the corresponding PV_NC flag is also updated in each pv entry. This
 * is done so kvm_uncache() can use this routine and have the uncached
 * status stick.
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)

void
pv_changepte4_4c(pv0, bis, bic)
	struct pvlist *pv0;
	int bis, bic;
{
	int *pte;
	struct pvlist *pv;
	struct pmap *pm;
	int va, vr, vs;
	int ctx, s;
	struct regmap *rp;
	struct segmap *sp;

	write_user_windows();		/* paranoid? */

	s = splvm();			/* paranoid? */
	if (pv0->pv_pmap == NULL) {
		splx(s);
		return;
	}
	ctx = getcontext4();
	for (pv = pv0; pv != NULL; pv = pv->pv_next) {
		pm = pv->pv_pmap;
#ifdef DIAGNOSTIC
		if(pm == NULL)
			panic("pv_changepte: pm == NULL");
#endif
		va = pv->pv_va;
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
		if (rp->rg_segmap == NULL)
			panic("pv_changepte: no segments");

		sp = &rp->rg_segmap[vs];
		pte = sp->sg_pte;

		if (sp->sg_pmeg == seginval) {
			/* not in hardware: just fix software copy */
			if (pte == NULL)
				panic("pv_changepte: pte == NULL");
			pte += VA_VPG(va);
			*pte = (*pte | bis) & ~bic;
		} else {
			int tpte;

			/* in hardware: fix hardware copy */
			if (CTX_USABLE(pm,rp)) {
				/*
				 * Bizarreness: we never clear PG_NC on
				 * DVMA pages.
				 * XXX should we ever get invoked on such
				 * XXX pages?
				 */
				if (bic == PG_NC &&
				    va >= DVMA_BASE && va < DVMA_END)
					continue;
				setcontext4(pm->pm_ctxnum);
				/* XXX should flush only when necessary */
				tpte = getpte4(va);
				/*
				 * XXX: always flush cache; conservative, but
				 * needed to invalidate cache tag protection
				 * bits and when disabling caching.
				 */
				cache_flush_page(va);
			} else {
				/* XXX per-cpu va? */
				setcontext4(0);
				if (HASSUN4_MMU3L)
					setregmap(0, tregion);
				setsegmap(0, sp->sg_pmeg);
				va = VA_VPG(va) << PGSHIFT;
				tpte = getpte4(va);
			}
			if (tpte & PG_V)
				pv0->pv_flags |= MR4_4C(tpte);
			tpte = (tpte | bis) & ~bic;
			setpte4(va, tpte);
			if (pte != NULL)	/* update software copy */
				pte[VA_VPG(va)] = tpte;

			/* Update PV_NC flag if required */
			if (bis & PG_NC)
				pv->pv_flags |= PV_NC;
			if (bic & PG_NC)
				pv->pv_flags &= ~PV_NC;
		}
	}
	setcontext4(ctx);
	splx(s);
}

/*
 * Sync ref and mod bits in pvlist (turns off same in hardware PTEs).
 * Returns the new flags.
 *
 * This is just like pv_changepte, but we never add or remove bits,
 * hence never need to adjust software copies.
 */
int
pv_syncflags4_4c(pv0)
	struct pvlist *pv0;
{
	struct pvlist *pv;
	struct pmap *pm;
	int tpte, va, vr, vs, pmeg, flags;
	int ctx, s;
	struct regmap *rp;
	struct segmap *sp;

	write_user_windows();		/* paranoid? */

	s = splvm();			/* paranoid? */
	if (pv0->pv_pmap == NULL) {	/* paranoid */
		splx(s);
		return (0);
	}
	ctx = getcontext4();
	flags = pv0->pv_flags;
	for (pv = pv0; pv != NULL; pv = pv->pv_next) {
		pm = pv->pv_pmap;
		va = pv->pv_va;
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
		if (rp->rg_segmap == NULL)
			panic("pv_syncflags: no segments");
		sp = &rp->rg_segmap[vs];

		if ((pmeg = sp->sg_pmeg) == seginval)
			continue;

		if (CTX_USABLE(pm,rp)) {
			setcontext4(pm->pm_ctxnum);
			/* XXX should flush only when necessary */
			tpte = getpte4(va);
			if (tpte & PG_M)
				cache_flush_page(va);
		} else {
			/* XXX per-cpu va? */
			setcontext4(0);
			if (HASSUN4_MMU3L)
				setregmap(0, tregion);
			setsegmap(0, pmeg);
			va = VA_VPG(va) << PGSHIFT;
			tpte = getpte4(va);
		}
		if (tpte & (PG_M|PG_U) && tpte & PG_V) {
			flags |= MR4_4C(tpte);
			tpte &= ~(PG_M|PG_U);
			setpte4(va, tpte);
		}
	}
	pv0->pv_flags = flags;
	setcontext4(ctx);
	splx(s);
	return (flags);
}

/*
 * pv_unlink is a helper function for pmap_remove.
 * It takes a pointer to the pv_table head for some physical address
 * and removes the appropriate (pmap, va) entry.
 *
 * Once the entry is removed, if the pv_table head has the cache
 * inhibit bit set, see if we can turn that off; if so, walk the
 * pvlist and turn off PG_NC in each PTE.  (The pvlist is by
 * definition nonempty, since it must have at least two elements
 * in it to have PV_NC set, and we only remove one here.)
 */
void
pv_unlink4_4c(pv, pm, va)
	struct pvlist *pv;
	struct pmap *pm;
	vaddr_t va;
{
	struct pvlist *npv;

#ifdef DIAGNOSTIC
	if (pv->pv_pmap == NULL)
		panic("pv_unlink0");
#endif
	/*
	 * First entry is special (sigh).
	 */
	npv = pv->pv_next;
	if (pv->pv_pmap == pm && pv->pv_va == va) {
		pmap_stats.ps_unlink_pvfirst++;
		if (npv != NULL) {
			/*
			 * Shift next entry into the head.
			 * Make sure to retain the REF, MOD and ANC flags.
			 */
			pv->pv_next = npv->pv_next;
			pv->pv_pmap = npv->pv_pmap;
			pv->pv_va = npv->pv_va;
			pv->pv_flags &= ~PV_NC;
			pv->pv_flags |= npv->pv_flags & PV_NC;
			pool_put(&pvpool, npv);
		} else {
			/*
			 * No mappings left; we still need to maintain
			 * the REF and MOD flags. since pmap_is_modified()
			 * can still be called for this page.
			 */
			if (pv->pv_flags & PV_ANC)
				pmap_stats.ps_alias_recache++;
			pv->pv_pmap = NULL;
			pv->pv_flags &= ~(PV_NC|PV_ANC);
			return;
		}
	} else {
		struct pvlist *prev;

		for (prev = pv;; prev = npv, npv = npv->pv_next) {
			pmap_stats.ps_unlink_pvsearch++;
			if (npv == NULL)
				panic("pv_unlink");
			if (npv->pv_pmap == pm && npv->pv_va == va)
				break;
		}
		prev->pv_next = npv->pv_next;
		pool_put(&pvpool, npv);
	}
	if (pv->pv_flags & PV_ANC && (pv->pv_flags & PV_NC) == 0) {
		/*
		 * Not cached: check to see if we can fix that now.
		 */
		va = pv->pv_va;
		for (npv = pv->pv_next; npv != NULL; npv = npv->pv_next)
			if (BADALIAS(va, npv->pv_va) || (npv->pv_flags & PV_NC))
				return;
		pmap_stats.ps_alias_recache++;
		pv->pv_flags &= ~PV_ANC;
		pv_changepte4_4c(pv, 0, PG_NC);
	}
}

/*
 * pv_link is the inverse of pv_unlink, and is used in pmap_enter.
 * It returns PG_NC if the (new) pvlist says that the address cannot
 * be cached.
 */
int
pv_link4_4c(pv, pm, va, nc)
	struct pvlist *pv;
	struct pmap *pm;
	vaddr_t va;
	int nc;
{
	struct pvlist *npv;
	int ret;

	ret = nc ? PG_NC : 0;

	if (pv->pv_pmap == NULL) {
		/* no pvlist entries yet */
		pmap_stats.ps_enter_firstpv++;
		pv->pv_next = NULL;
		pv->pv_pmap = pm;
		pv->pv_va = va;
		pv->pv_flags |= nc ? PV_NC : 0;
		return (ret);
	}

	/*
	 * Before entering the new mapping, see if
	 * it will cause old mappings to become aliased
	 * and thus need to be `discached'.
	 */
	pmap_stats.ps_enter_secondpv++;
	if (pv->pv_flags & (PV_NC|PV_ANC)) {
		/* already uncached, just stay that way */
		ret = PG_NC;
	} else {
		for (npv = pv; npv != NULL; npv = npv->pv_next) {
			if (npv->pv_flags & PV_NC) {
				ret = PG_NC;
				break;
			}
			if (BADALIAS(va, npv->pv_va)) {
#ifdef DEBUG
				if (pmapdebug & PDB_CACHESTUFF)
					printf(
			"pv_link: badalias: pid %d, 0x%lx<=>0x%lx, pa 0x%lx\n",
					curproc ? curproc->p_pid : -1,
					va, npv->pv_va, (vaddr_t)-1); /* XXX -1 */
#endif
				/* Mark list head `uncached due to aliases' */
				pmap_stats.ps_alias_uncache++;
				pv->pv_flags |= PV_ANC;
				pv_changepte4_4c(pv, ret = PG_NC, 0);
				break;
			}
		}
	}

	npv = pool_get(&pvpool, PR_NOWAIT);
	if (npv == NULL)
		panic("pv_link_4_4c: allocation failed");
	npv->pv_next = pv->pv_next;
	npv->pv_pmap = pm;
	npv->pv_va = va;
	npv->pv_flags = nc ? PV_NC : 0;
	pv->pv_next = npv;
	return (ret);
}

#endif /* sun4, sun4c code */

#if defined(SUN4M)		/* Sun4M versions of above */
/*
 * Walk the given pv list, and for each PTE, set or clear some bits
 * (e.g., PG_W or PG_NC).
 *
 * As a special case, this never clears PG_W on `pager' pages.
 * These, being kernel addresses, are always in hardware and have
 * a context.
 *
 * This routine flushes the cache for any page whose PTE changes,
 * as long as the process has a context; this is overly conservative.
 * It also copies ref and mod bits to the pvlist, on the theory that
 * this might save work later.  (XXX should test this theory)
 *
 * In addition, if the cacheable bit (SRMMU_PG_C) is updated in the PTE
 * the corresponding PV_C4M flag is also updated in each pv entry. This
 * is done so kvm_uncache() can use this routine and have the uncached
 * status stick.
 */
void
pv_changepte4m(pv0, bis, bic)
	struct pvlist *pv0;
	int bis, bic;
{
	struct pvlist *pv;
	struct pmap *pm;
	int ctx, s;
	vaddr_t va;

	write_user_windows();		/* paranoid? */

	s = splvm();			/* paranoid? */
	if (pv0->pv_pmap == NULL) {
		splx(s);
		return;
	}
	ctx = getcontext4m();
	for (pv = pv0; pv != NULL; pv = pv->pv_next) {
		int tpte;
		int *ptep;

		pm = pv->pv_pmap;
		va = pv->pv_va;
#ifdef DIAGNOSTIC
		if (pm == NULL)
			panic("pv_changepte4m: pmap == NULL");
#endif

		ptep = getptep4m(pm, va);

		if (pm->pm_ctx) {
			setcontext4m(pm->pm_ctxnum);

			/*
			 * XXX: always flush cache; conservative, but
			 * needed to invalidate cache tag protection
			 * bits and when disabling caching.
			 */
			cache_flush_page(va);

			tlb_flush_page(va);

		}

		tpte = *ptep;
#ifdef DIAGNOSTIC
		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE)
			panic("pv_changepte: invalid PTE for 0x%lx", va);
#endif

		pv0->pv_flags |= MR4M(tpte);
		tpte = (tpte | bis) & ~bic;
		setpgt4m(ptep, tpte);

		/* Update PV_C4M flag if required */
		/*
		 * XXX - this is incorrect. The PV_C4M means that _this_
		 *       mapping should be kept uncached. This way we
		 *       effectively uncache this pa until all mappings
		 *       to it are gone (see also the XXX in pv_link4m and
		 *       pv_unlink4m).
		 */
		if (bis & SRMMU_PG_C)
			pv->pv_flags |= PV_C4M;
		if (bic & SRMMU_PG_C)
			pv->pv_flags &= ~PV_C4M;
	}
	setcontext4m(ctx);
	splx(s);
}

/*
 * Sync ref and mod bits in pvlist. If page has been ref'd or modified,
 * update ref/mod bits in pvlist, and clear the hardware bits.
 *
 * Return the new flags.
 */
int
pv_syncflags4m(pv0)
	struct pvlist *pv0;
{
	struct pvlist *pv;
	struct pmap *pm;
	int tpte, va, flags;
	int ctx, s;

	write_user_windows();		/* paranoid? */

	s = splvm();			/* paranoid? */
	if (pv0->pv_pmap == NULL) {	/* paranoid */
		splx(s);
		return (0);
	}
	ctx = getcontext4m();
	flags = pv0->pv_flags;
	for (pv = pv0; pv != NULL; pv = pv->pv_next) {
		int *ptep;

		pm = pv->pv_pmap;
		va = pv->pv_va;

		ptep = getptep4m(pm, va);

		/*
		 * XXX - This can't happen?!?
		 */
		if (ptep == NULL) {	/* invalid */
			printf("pv_syncflags4m: no pte pmap: %p, va: 0x%x\n",
			    pm, va);
			continue;
		}

		/*
		 * We need the PTE from memory as the TLB version will
		 * always have the SRMMU_PG_R bit on.
		 */
		if (pm->pm_ctx) {
			setcontext4m(pm->pm_ctxnum);
			tlb_flush_page(va);
		}
			
		tpte = *ptep;

		if ((tpte & SRMMU_TETYPE) == SRMMU_TEPTE && /* if valid pte */
		    (tpte & (SRMMU_PG_M|SRMMU_PG_R))) {	  /* and mod/refd */

			flags |= MR4M(tpte);

			if (pm->pm_ctx && (tpte & SRMMU_PG_M)) {
				cache_flush_page(va); /* XXX:do we need this?*/
				tlb_flush_page(va);
			}

			/* Clear mod/ref bits from PTE and write it back */
			tpte &= ~(SRMMU_PG_M | SRMMU_PG_R);
			setpgt4m(ptep, tpte);
		}
	}
	pv0->pv_flags = flags;
	setcontext4m(ctx);
	splx(s);
	return (flags);
}

void
pv_unlink4m(pv, pm, va)
	struct pvlist *pv;
	struct pmap *pm;
	vaddr_t va;
{
	struct pvlist *npv;

#ifdef DIAGNOSTIC
	if (pv->pv_pmap == NULL)
		panic("pv_unlink0");
#endif
	/*
	 * First entry is special (sigh).
	 */
	npv = pv->pv_next;
	if (pv->pv_pmap == pm && pv->pv_va == va) {
		pmap_stats.ps_unlink_pvfirst++;
		if (npv != NULL) {
			/*
			 * Shift next entry into the head.
			 * Make sure to retain the REF, MOD and ANC flags.
			 */
			pv->pv_next = npv->pv_next;
			pv->pv_pmap = npv->pv_pmap;
			pv->pv_va = npv->pv_va;
			pv->pv_flags &= ~PV_C4M;
			pv->pv_flags |= (npv->pv_flags & PV_C4M);
			pool_put(&pvpool, npv);
		} else {
			/*
			 * No mappings left; we still need to maintain
			 * the REF and MOD flags. since pmap_is_modified()
			 * can still be called for this page.
			 */
			if (pv->pv_flags & PV_ANC)
				pmap_stats.ps_alias_recache++;
			pv->pv_pmap = NULL;
			pv->pv_flags &= ~(PV_C4M|PV_ANC);
			return;
		}
	} else {
		struct pvlist *prev;

		for (prev = pv;; prev = npv, npv = npv->pv_next) {
			pmap_stats.ps_unlink_pvsearch++;
			if (npv == NULL)
				panic("pv_unlink");
			if (npv->pv_pmap == pm && npv->pv_va == va)
				break;
		}
		prev->pv_next = npv->pv_next;
		pool_put(&pvpool, npv);
	}
	if ((pv->pv_flags & (PV_C4M|PV_ANC)) == (PV_C4M|PV_ANC)) {
		/*
		 * Not cached: check to see if we can fix that now.
		 */
		/*
		 * XXX - This code is incorrect. Even if the bad alias
		 *       has disappeared we keep the PV_ANC flag because
		 *       one of the mappings is not PV_C4M.
		 */
		va = pv->pv_va;
		for (npv = pv->pv_next; npv != NULL; npv = npv->pv_next)
			if (BADALIAS(va, npv->pv_va) ||
			    (npv->pv_flags & PV_C4M) == 0)
				return;
		pmap_stats.ps_alias_recache++;
		pv->pv_flags &= ~PV_ANC;
		pv_changepte4m(pv, SRMMU_PG_C, 0);
	}
}

/*
 * pv_link is the inverse of pv_unlink, and is used in pmap_enter.
 * It returns SRMMU_PG_C if the (new) pvlist says that the address cannot
 * be cached (i.e. its results must be (& ~)'d in.
 */
int
pv_link4m(pv, pm, va, nc)
	struct pvlist *pv;
	struct pmap *pm;
	vaddr_t va;
	int nc;
{
	struct pvlist *npv, *mpv;
	int ret;

	ret = nc ? SRMMU_PG_C : 0;

	if (pv->pv_pmap == NULL) {
		/* no pvlist entries yet */
		pmap_stats.ps_enter_firstpv++;
		pv->pv_next = NULL;
		pv->pv_pmap = pm;
		pv->pv_va = va;
		/*
		 * XXX - should we really keep the MOD/REF flags?
		 */
		pv->pv_flags |= nc ? 0 : PV_C4M;
		return (ret);
	}

	/*
	 * We do the malloc early so that we catch all changes that happen
	 * during the (possible) sleep.
	 */
	mpv = pool_get(&pvpool, PR_NOWAIT);
	if (mpv == NULL)
		panic("pv_link4m: allocation failed");

	/*
	 * Before entering the new mapping, see if
	 * it will cause old mappings to become aliased
	 * and thus need to be `discached'.
	 */
	pmap_stats.ps_enter_secondpv++;
	if ((pv->pv_flags & PV_ANC) != 0 || (pv->pv_flags & PV_C4M) == 0) {
		/* already uncached, just stay that way */
		ret = SRMMU_PG_C;
	} else {
		for (npv = pv; npv != NULL; npv = npv->pv_next) {
			/*
			 * XXX - This code is incorrect. Even when we have
			 *       a bad alias we can fail to set PV_ANC because
			 *       one of the mappings doesn't have PV_C4M set.
			 */
			if ((npv->pv_flags & PV_C4M) == 0) {
				ret = SRMMU_PG_C;
				break;
			}
			if (BADALIAS(va, npv->pv_va)) {
#ifdef DEBUG
				if (pmapdebug & PDB_CACHESTUFF)
					printf(
			"pv_link: badalias: pid %d, 0x%lx<=>0x%lx, pa 0x%lx\n",
					curproc ? curproc->p_pid : -1,
					va, npv->pv_va, (vaddr_t)-1); /* XXX -1 */
#endif
				/* Mark list head `uncached due to aliases' */
				pmap_stats.ps_alias_uncache++;
				pv->pv_flags |= PV_ANC;
				pv_changepte4m(pv, 0, ret = SRMMU_PG_C);
				/* cache_flush_page(va); XXX: needed? */
				break;
			}
		}
	}

	mpv->pv_next = pv->pv_next;
	mpv->pv_pmap = pm;
	mpv->pv_va = va;
	mpv->pv_flags = nc ? 0 : PV_C4M;
	pv->pv_next = mpv;
	return (ret);
}
#endif

/*
 * Walk the given list and flush the cache for each (MI) page that is
 * potentially in the cache. Called only if vactype != VAC_NONE.
 */
void
pg_flushcache(struct vm_page *pg)
{
	struct pvlist *pv = &pg->mdpage.pv_head;
	struct pmap *pm;
	int s, ctx;

	write_user_windows();	/* paranoia? */

	s = splvm();		/* XXX extreme paranoia */
	if ((pm = pv->pv_pmap) != NULL) {
		ctx = getcontext();
		for (;;) {
			if (pm->pm_ctx) {
				setcontext(pm->pm_ctxnum);
				cache_flush_page(pv->pv_va);
			}
			pv = pv->pv_next;
			if (pv == NULL)
				break;
			pm = pv->pv_pmap;
		}
		setcontext(ctx);
	}
	splx(s);
}

/*----------------------------------------------------------------*/

/*
 * At last, pmap code.
 */

#if (defined(SUN4) || defined(SUN4E)) && defined(SUN4C)
int nptesg;
#endif

#if defined(SUN4M)
void pmap_bootstrap4m(void *);
#endif
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
void pmap_bootstrap4_4c(void *, int, int, int);
#endif

/*
 * Bootstrap the system enough to run with VM enabled.
 *
 * nsegment is the number of mmu segment entries (``PMEGs'');
 * nregion is the number of mmu region entries (``SMEGs'');
 * nctx is the number of contexts.
 */
void
pmap_bootstrap(int nctx, int nregion, int nsegment)
{
	void *p;
	extern char end[];
#ifdef DDB
	extern char *esym;
#endif
	extern int nbpg;	/* locore.s */

	uvmexp.pagesize = nbpg;
	uvm_setpagesize();

#if (defined(SUN4) || defined(SUN4E)) && (defined(SUN4C) || defined(SUN4D) || defined(SUN4M))
	/* In this case NPTESG is not a #define */
	nptesg = (NBPSG >> uvmexp.pageshift);
#endif

	/*
	 * Grab physical memory list.
	 */
	p = end;
#ifdef DDB
	if (esym != 0)
		p = esym;
#endif
	get_phys_mem(&p);

	if (CPU_ISSUN4M) {
#if defined(SUN4M)
		pmap_bootstrap4m(p);
#endif
	} else if (CPU_ISSUN4OR4COR4E) {
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
		pmap_bootstrap4_4c(p, nctx, nregion, nsegment);
#endif
	}

	pmap_page_upload();
}

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
void
pmap_bootstrap4_4c(void *top, int nctx, int nregion, int nsegment)
{
	union ctxinfo *ci;
	struct mmuentry *mmuseg;
#if defined(SUN4_MMU3L)
	struct mmuentry *mmureg;
#endif
	struct regmap *rp;
	int i, j;
	int npte, zseg, vr, vs;
	int startscookie, scookie;
#if defined(SUN4_MMU3L)
	int startrcookie, rcookie;
#endif
	caddr_t p;
	vaddr_t va;
	void (*rom_setmap)(int ctx, caddr_t va, int pmeg);
	int lastpage;
	extern char kernel_text[];

	nureg = NUREG_4C;
	nkreg = NKREG_4C;

	/*
	 * Compute `va2pa_offset'.
	 * Use `kernel_text' to probe the MMU translation since
	 * the pages at KERNBASE might not be mapped.
	 */
	va2pa_offset = (vaddr_t)kernel_text -
	    ((getpte4(kernel_text) & PG_PFNUM) << PGSHIFT);

	switch (cputyp) {
	default:
	case CPU_SUN4C:
	case CPU_SUN4E:
		mmu_has_hole = 1;
		break;
	case CPU_SUN4:
		if (cpuinfo.cpu_type != CPUTYP_4_400) {
			mmu_has_hole = 1;
			break;
		}
	}

#if defined(SUN4)
	/*
	 * set up the segfixmask to mask off invalid bits
	 */
	segfixmask =  nsegment - 1; /* assume nsegment is a power of 2 */
#ifdef DIAGNOSTIC
	if (((nsegment & segfixmask) | (nsegment & ~segfixmask)) != nsegment) {
		printf("pmap_bootstrap: unsuitable number of segments (%d)\n",
			nsegment);
		callrom();
	}
#endif
#endif

#if defined(SUN4D) || defined(SUN4M) /* We're in a dual-arch kernel. Setup 4/4c fn. ptrs */
	pmap_clear_modify_p 	=	pmap_clear_modify4_4c;
	pmap_clear_reference_p 	= 	pmap_clear_reference4_4c;
	pmap_copy_page_p 	=	pmap_copy_page4_4c;
	pmap_enter_p 		=	pmap_enter4_4c;
	pmap_extract_p 		=	pmap_extract4_4c;
	pmap_is_modified_p 	=	pmap_is_modified4_4c;
	pmap_is_referenced_p	=	pmap_is_referenced4_4c;
	pmap_kenter_pa_p	=	pmap_kenter_pa4_4c;
	pmap_page_protect_p	=	pmap_page_protect4_4c;
	pmap_protect_p		=	pmap_protect4_4c;
	pmap_zero_page_p	=	pmap_zero_page4_4c;
	pmap_changeprot_p	=	pmap_changeprot4_4c;
	pmap_rmk_p		=	pmap_rmk4_4c;
	pmap_rmu_p		=	pmap_rmu4_4c;
#endif /* defined SUN4M */

	/*
	 * Last segment is the `invalid' one (one PMEG of pte's with !pg_v).
	 * It will never be used for anything else.
	 */
	seginval = --nsegment;

#if defined(SUN4_MMU3L)
	if (HASSUN4_MMU3L)
		reginval = --nregion;
#endif

	/*
	 * Initialize the kernel pmap.
	 */
	/* kernel_pmap_store.pm_ctxnum = 0; */
	kernel_pmap_store.pm_refcount = 1;
#if defined(SUN4_MMU3L)
	TAILQ_INIT(&kernel_pmap_store.pm_reglist);
#endif
	TAILQ_INIT(&kernel_pmap_store.pm_seglist);

	kernel_pmap_store.pm_regmap = &kernel_regmap_store[-NUREG_4C];
	for (i = NKREG_4C; --i >= 0;) {
#if defined(SUN4_MMU3L)
		kernel_regmap_store[i].rg_smeg = reginval;
#endif
		kernel_regmap_store[i].rg_segmap =
			&kernel_segmap_store[i * NSEGRG];
		for (j = NSEGRG; --j >= 0;)
			kernel_segmap_store[i * NSEGRG + j].sg_pmeg = seginval;
	}

	/*
	 * Preserve the monitor ROM's reserved VM region, so that
	 * we can use L1-A or the monitor's debugger.  As a side
	 * effect we map the ROM's reserved VM into all contexts
	 * (otherwise L1-A crashes the machine!).
	 */

	mmu_reservemon4_4c(&nregion, &nsegment);

#if defined(SUN4_MMU3L)
	/* Reserve one region for temporary mappings */
	tregion = --nregion;
#endif

	/*
	 * Allocate and clear mmu entries and context structures.
	 */
	p = top;
#if defined(SUN4_MMU3L)
	mmuregions = (struct mmuentry *)p;
	p += nregion * sizeof(struct mmuentry);
	bzero(mmuregions, nregion * sizeof(struct mmuentry));
#endif
	mmusegments = (struct mmuentry *)p;
	p += nsegment * sizeof(struct mmuentry);
	bzero(mmusegments, nsegment * sizeof(struct mmuentry));

	pmap_kernel()->pm_ctx = cpuinfo.ctxinfo = ci = (union ctxinfo *)p;
	p += nctx * sizeof *ci;

	/* Initialize MMU resource queues */
#if defined(SUN4_MMU3L)
	TAILQ_INIT(&region_freelist);
	TAILQ_INIT(&region_lru);
	TAILQ_INIT(&region_locked);
#endif
	TAILQ_INIT(&segm_freelist);
	TAILQ_INIT(&segm_lru);
	TAILQ_INIT(&segm_locked);

	/*
	 * Set up the `constants' for the call to uvm_init()
	 * in main().  All pages beginning at p (rounded up to
	 * the next whole page) and continuing through the number
	 * of available pages are free, but they start at a higher
	 * virtual address.  This gives us two mappable MD pages
	 * for pmap_zero_page and pmap_copy_page, and some pages
	 * for dumpsys(), all with no associated physical memory.
	 */
	p = (caddr_t)round_page((vaddr_t)p);
	avail_start = PMAP_BOOTSTRAP_VA2PA(p);

	i = (int)p;
	vpage[0] = p, p += NBPG;
	vpage[1] = p, p += NBPG;
	p = reserve_dumppages(p);

	virtual_avail = (vaddr_t)p;
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	p = (caddr_t)i;			/* retract to first free phys */

	/*
	 * All contexts are free except the kernel's.
	 *
	 * XXX sun4c could use context 0 for users?
	 */
	ci->c_pmap = pmap_kernel();
	ctx_freelist = ci + 1;
	for (i = 1; i < ncontext; i++) {
		ci++;
		ci->c_nextfree = ci + 1;
	}
	ci->c_nextfree = NULL;
	ctx_kick = 0;
	ctx_kickdir = -1;

	/*
	 * Init mmu entries that map the kernel physical addresses.
	 *
	 * All the other MMU entries are free.
	 *
	 * THIS ASSUMES THE KERNEL IS MAPPED BY A CONTIGUOUS RANGE OF
	 * MMU SEGMENTS/REGIONS DURING THE BOOT PROCESS
	 */

	rom_setmap = promvec->pv_setctxt;
	zseg = ((((u_int)p + NBPSG - 1) & ~SGOFSET) - KERNBASE) >> SGSHIFT;
	lastpage = VA_VPG(p);
	if (lastpage == 0)
		/*
		 * If the page bits in p are 0, we filled the last segment
		 * exactly (now how did that happen?); if not, it is
		 * the last page filled in the last segment.
		 */
		lastpage = NPTESG;

	p = (caddr_t)KERNBASE;	/* first kernel va */
	vs = VA_VSEG((vaddr_t)p);/* first virtual segment */
	vr = VA_VREG((vaddr_t)p);/* first virtual region */
	rp = &pmap_kernel()->pm_regmap[vr];

	/* Get region/segment where kernel addresses start */
#if defined(SUN4_MMU3L)
	if (HASSUN4_MMU3L)
		startrcookie = rcookie = getregmap(p);
	mmureg = &mmuregions[rcookie];
#endif
	startscookie = scookie = getsegmap(p);
	mmuseg = &mmusegments[scookie];
	zseg += scookie;	/* First free segment */

	for (;;) {

		/*
		 * Distribute each kernel region/segment into all contexts.
		 * This is done through the monitor ROM, rather than
		 * directly here: if we do a setcontext we will fault,
		 * as we are not (yet) mapped in any other context.
		 */

		if ((vs % NSEGRG) == 0) {
			/* Entering a new region */
			if (VA_VREG(p) > vr) {
#ifdef DEBUG
				printf("note: giant kernel!\n");
#endif
				vr++, rp++;
			}
#if defined(SUN4_MMU3L)
			if (HASSUN4_MMU3L) {
				for (i = 1; i < nctx; i++)
					rom_setmap(i, p, rcookie);

				TAILQ_INSERT_TAIL(&region_locked,
						  mmureg, me_list);
				TAILQ_INSERT_TAIL(&pmap_kernel()->pm_reglist,
						  mmureg, me_pmchain);
				mmureg->me_cookie = rcookie;
				mmureg->me_pmap = pmap_kernel();
				mmureg->me_vreg = vr;
				rp->rg_smeg = rcookie;
				mmureg++;
				rcookie++;
			}
#endif
		}

#if defined(SUN4_MMU3L)
		if (!HASSUN4_MMU3L)
#endif
			for (i = 1; i < nctx; i++)
				rom_setmap(i, p, scookie);

		/* set up the mmu entry */
		TAILQ_INSERT_TAIL(&segm_locked, mmuseg, me_list);
		TAILQ_INSERT_TAIL(&pmap_kernel()->pm_seglist, mmuseg, me_pmchain);
		pmap_stats.ps_npmeg_locked++;
		mmuseg->me_cookie = scookie;
		mmuseg->me_pmap = pmap_kernel();
		mmuseg->me_vreg = vr;
		mmuseg->me_vseg = vs % NSEGRG;
		rp->rg_segmap[vs % NSEGRG].sg_pmeg = scookie;
		npte = ++scookie < zseg ? NPTESG : lastpage;
		rp->rg_segmap[vs % NSEGRG].sg_npte = npte;
		pmap_kernel()->pm_stats.resident_count += npte;
		rp->rg_nsegmap += 1;
		mmuseg++;
		vs++;
		if (scookie < zseg) {
			p += NBPSG;
			continue;
		}

		/*
		 * Unmap the pages, if any, that are not part of
		 * the final segment.
		 */
		for (p += npte << PGSHIFT; npte < NPTESG; npte++, p += NBPG)
			setpte4(p, 0);

#if defined(SUN4_MMU3L)
		if (HASSUN4_MMU3L) {
			/*
			 * Unmap the segments, if any, that are not part of
			 * the final region.
			 */
			for (i = rp->rg_nsegmap; i < NSEGRG; i++, p += NBPSG)
				setsegmap(p, seginval);

			/*
			 * Unmap any kernel regions that we aren't using.
			 */
			for (i = 0; i < nctx; i++) {
				setcontext4(i);
				for (va = (vaddr_t)p;
				    va < (OPENPROM_STARTVADDR & ~(NBPRG - 1));
				    va += NBPRG)
					setregmap(va, reginval);
			}
		} else
#endif
		{
			/*
			 * Unmap any kernel regions that we aren't using.
			 */
			for (i = 0; i < nctx; i++) {
				setcontext4(i);
				for (va = (vaddr_t)p;
				    va < (OPENPROM_STARTVADDR & ~(NBPSG - 1));
				    va += NBPSG)
					setsegmap(va, seginval);
			}
		}
		break;
	}

#if defined(SUN4_MMU3L)
	if (HASSUN4_MMU3L)
		for (rcookie = 0; rcookie < nregion; rcookie++) {
			if (rcookie == startrcookie)
				/* Kernel must fit in one region! */
				rcookie++;
			mmureg = &mmuregions[rcookie];
			mmureg->me_cookie = rcookie;
			TAILQ_INSERT_TAIL(&region_freelist, mmureg, me_list);
		}
#endif

	for (scookie = 0; scookie < nsegment; scookie++) {
		if (scookie == startscookie)
			scookie = zseg;
		mmuseg = &mmusegments[scookie];
		mmuseg->me_cookie = scookie;
		TAILQ_INSERT_TAIL(&segm_freelist, mmuseg, me_list);
		pmap_stats.ps_npmeg_free++;
	}

	/* Erase all spurious user-space segmaps */
	for (i = 1; i < ncontext; i++) {
		setcontext4(i);
		if (HASSUN4_MMU3L)
			for (p = 0, j = NUREG_4C; --j >= 0; p += NBPRG)
				setregmap(p, reginval);
		else
			for (p = 0, vr = 0; vr < NUREG_4C; vr++) {
				if (VA_INHOLE(p)) {
					p = (caddr_t)MMU_HOLE_END;
					vr = VA_VREG(p);
				}
				for (j = NSEGRG; --j >= 0; p += NBPSG)
					setsegmap(p, seginval);
			}
	}
	setcontext4(0);

	/*
	 * write protect & encache kernel text;
	 * set red zone at kernel base; enable cache on message buffer.
	 */
	{
		extern char __data_start[];
		caddr_t sdata = (caddr_t)trunc_page((vaddr_t)__data_start);

		/* Enable cache on message buffer */
		for (p = (caddr_t)KERNBASE; p < (caddr_t)trapbase; p += NBPG)
			setpte4(p, getpte4(p) & ~PG_NC);

		/* Enable cache and write protext kernel text and rodata */
		for (; p < sdata; p += NBPG)
			setpte4(p, getpte4(p) & ~(PG_W | PG_NC));

		/* Enable cache on data & bss */
		for (; p < (caddr_t)virtual_avail; p += NBPG)
			setpte4(p, getpte4(p) & ~PG_NC);
	}
}
#endif

#if defined(SUN4M)		/* Sun4M version of pmap_bootstrap */
/*
 * Bootstrap the system enough to run with VM enabled on a Sun4M machine.
 *
 * Switches from ROM to kernel page tables, and sets up initial mappings.
 */
void
pmap_bootstrap4m(void *top)
{
	int i, j;
	caddr_t p;
	caddr_t q;
	union ctxinfo *ci;
	int reg, seg;
	unsigned int ctxtblsize;
	paddr_t pagetables_start_pa;
	extern char kernel_text[];
	extern caddr_t reserve_dumppages(caddr_t);

	nureg = NUREG_4M;
	nkreg = NKREG_4M;

	/*
	 * Compute `va2pa_offset'.
	 * Use `kernel_text' to probe the MMU translation since
	 * the pages at KERNBASE might not be mapped.
	 */
	va2pa_offset = (vaddr_t)kernel_text - VA2PA(kernel_text);

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E) /* setup 4M fn. ptrs for dual-arch kernel */
	pmap_clear_modify_p 	=	pmap_clear_modify4m;
	pmap_clear_reference_p 	= 	pmap_clear_reference4m;
	pmap_copy_page_p 	=	pmap_copy_page4m;
	pmap_enter_p 		=	pmap_enter4m;
	pmap_extract_p 		=	pmap_extract4m;
	pmap_is_modified_p 	=	pmap_is_modified4m;
	pmap_is_referenced_p	=	pmap_is_referenced4m;
	pmap_kenter_pa_p	=	pmap_kenter_pa4m;
	pmap_page_protect_p	=	pmap_page_protect4m;
	pmap_protect_p		=	pmap_protect4m;
	pmap_zero_page_p	=	pmap_zero_page4m;
	pmap_changeprot_p	=	pmap_changeprot4m;
	pmap_rmk_p		=	pmap_rmk4m;
	pmap_rmu_p		=	pmap_rmu4m;
#endif /* SUN4 || SUN4C || SUN4E */

	/*
	 * Initialize the kernel pmap.
	 */
	/* kernel_pmap_store.pm_ctxnum = 0; */
	kernel_pmap_store.pm_refcount = 1;

	/*
	 * Set up pm_regmap for kernel to point NUREG_4M *below* the beginning
	 * of kernel regmap storage. Since the kernel only uses regions
	 * above NUREG_4M, we save storage space and can index kernel and
	 * user regions in the same way
	 */
	kernel_pmap_store.pm_regmap = &kernel_regmap_store[-NUREG_4M];
	kernel_pmap_store.pm_reg_ptps = NULL;
	kernel_pmap_store.pm_reg_ptps_pa = 0;
	bzero(kernel_regmap_store, NKREG_4M * sizeof(struct regmap));
	bzero(kernel_segmap_store, NKREG_4M * NSEGRG * sizeof(struct segmap));
	for (i = NKREG_4M; --i >= 0;) {
		kernel_regmap_store[i].rg_segmap =
			&kernel_segmap_store[i * NSEGRG];
		kernel_regmap_store[i].rg_seg_ptps = NULL;
		for (j = NSEGRG; --j >= 0;)
			kernel_segmap_store[i * NSEGRG + j].sg_pte = NULL;
	}

	p = top;		/* p points to top of kernel mem */
	p = (caddr_t)round_page((vaddr_t)p);

	/* Allocate context administration */
	pmap_kernel()->pm_ctx = cpuinfo.ctxinfo = ci = (union ctxinfo *)p;
	p += ncontext * sizeof *ci;
	bzero((caddr_t)ci, (u_int)p - (u_int)ci);
#if 0
	ctxbusyvector = p;
	p += ncontext;
	bzero(ctxbusyvector, ncontext);
	ctxbusyvector[0] = 1;	/* context 0 is always in use */
#endif

	/*
	 * Set up the `constants' for the call to uvm_init()
	 * in main().  All pages beginning at p (rounded up to
	 * the next whole page) and continuing through the number
	 * of available pages are free.
	 */
	p = (caddr_t)round_page((vaddr_t)p);

	/*
	 * Reserve memory for MMU pagetables. Some of these have severe
	 * alignment restrictions.
	 */
	pagetables_start = (vaddr_t)p;
	pagetables_start_pa = PMAP_BOOTSTRAP_VA2PA(p);

	/*
	 * Allocate context table.
	 * To keep supersparc happy, minimum alignment is on a 4K boundary.
	 */
	ctxtblsize = max(ncontext, 1024) * sizeof(int);
	cpuinfo.ctx_tbl = (int *)roundup((u_int)p, ctxtblsize);
	p = (caddr_t)((u_int)cpuinfo.ctx_tbl + ctxtblsize);
	qzero(cpuinfo.ctx_tbl, ctxtblsize);

	/*
	 * Reserve memory for segment and page tables needed to map the entire
	 * kernel. This takes (2k + NKREG_4M * 16k) of space, but
	 * unfortunately is necessary since pmap_enk *must* be able to enter
	 * a kernel mapping without resorting to malloc, or else the
	 * possibility of deadlock arises (pmap_enk4m is called to enter a
	 * mapping; it needs to malloc a page table; malloc then calls
	 * pmap_enk4m to enter the new malloc'd page; pmap_enk4m needs to
	 * malloc a page table to enter _that_ mapping; malloc deadlocks since
	 * it is already allocating that object).
	 */
	p = (caddr_t) roundup((u_int)p, SRMMU_L1SIZE * sizeof(long));
	kernel_regtable_store = (u_int *)p;
	p += SRMMU_L1SIZE * sizeof(long);
	bzero(kernel_regtable_store, p - (caddr_t)kernel_regtable_store);

	p = (caddr_t) roundup((u_int)p, SRMMU_L2SIZE * sizeof(long));
	kernel_segtable_store = (u_int *)p;
	p += (SRMMU_L2SIZE * sizeof(long)) * NKREG_4M;
	bzero(kernel_segtable_store, p - (caddr_t)kernel_segtable_store);

	p = (caddr_t) roundup((u_int)p, SRMMU_L3SIZE * sizeof(long));
	kernel_pagtable_store = (u_int *)p;
	p += ((SRMMU_L3SIZE * sizeof(long)) * NKREG_4M) * NSEGRG;
	bzero(kernel_pagtable_store, p - (caddr_t)kernel_pagtable_store);

	/* Round to next page and mark end of stolen pages */
	p = (caddr_t)round_page((vaddr_t)p);
	pagetables_end = (vaddr_t)p;

	avail_start = PMAP_BOOTSTRAP_VA2PA(p);

	/*
	 * Since we've statically allocated space to map the entire kernel,
	 * we might as well pre-wire the mappings to save time in pmap_enter.
	 * This also gets around nasty problems with caching of L1/L2 ptp's.
	 *
	 * XXX WHY DO WE HAVE THIS CACHING PROBLEM WITH L1/L2 PTPS????? %%%
	 */

	pmap_kernel()->pm_reg_ptps = (int *)kernel_regtable_store;
	pmap_kernel()->pm_reg_ptps_pa =
	    PMAP_BOOTSTRAP_VA2PA(kernel_regtable_store);

	/* Install L1 table in context 0 */
	setpgt4m(&cpuinfo.ctx_tbl[0],
	    (pmap_kernel()->pm_reg_ptps_pa >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD);

	/* XXX:rethink - Store pointer to region table address */
	cpuinfo.L1_ptps = pmap_kernel()->pm_reg_ptps;

	for (reg = 0; reg < NKREG_4M; reg++) {
		struct regmap *rp;
		caddr_t kphyssegtbl;

		/*
		 * Entering new region; install & build segtbl
		 */

		rp = &pmap_kernel()->pm_regmap[reg + NUREG];

		kphyssegtbl = (caddr_t)
		    &kernel_segtable_store[reg * SRMMU_L2SIZE];

		setpgt4m(&pmap_kernel()->pm_reg_ptps[reg + NUREG],
		    (PMAP_BOOTSTRAP_VA2PA(kphyssegtbl) >> SRMMU_PPNPASHIFT) |
		    SRMMU_TEPTD);

		rp->rg_seg_ptps = (int *)kphyssegtbl;

		for (seg = 0; seg < NSEGRG; seg++) {
			struct segmap *sp;
			caddr_t kphyspagtbl;

			rp->rg_nsegmap++;

			sp = &rp->rg_segmap[seg];
			kphyspagtbl = (caddr_t)
			    &kernel_pagtable_store
				[((reg * NSEGRG) + seg) * SRMMU_L3SIZE];

			setpgt4m(&rp->rg_seg_ptps[seg],
				 (PMAP_BOOTSTRAP_VA2PA(kphyspagtbl) >> SRMMU_PPNPASHIFT) |
				 SRMMU_TEPTD);
			sp->sg_pte = (int *) kphyspagtbl;
		}
	}

	/*
	 * Preserve the monitor ROM's reserved VM region, so that
	 * we can use L1-A or the monitor's debugger.
	 */
	mmu_reservemon4m(&kernel_pmap_store);

	/*
	 * Reserve virtual address space for two mappable MD pages
	 * for pmap_zero_page and pmap_copy_page, and some more for
	 * dumpsys().
	 */
	q = p;
	vpage[0] = p, p += NBPG;
	vpage[1] = p, p += NBPG;
	p = reserve_dumppages(p);

	virtual_avail = (vaddr_t)p;
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	p = q;			/* retract to first free phys */

	/*
	 * Set up the ctxinfo structures (freelist of contexts)
	 */
	ci->c_pmap = pmap_kernel();
	ctx_freelist = ci + 1;
	for (i = 1; i < ncontext; i++) {
		ci++;
		ci->c_nextfree = ci + 1;
	}
	ci->c_nextfree = NULL;
	ctx_kick = 0;
	ctx_kickdir = -1;

	/*
	 * Now map the kernel into our new set of page tables, then
	 * (finally) switch over to our running page tables.
	 * We map from VM_MIN_KERNEL_ADDRESS to p into context 0's
	 * page tables (and the kernel pmap).
	 */
#ifdef DEBUG			/* Sanity checks */
	if ((u_int)p % NBPG != 0)
		panic("pmap_bootstrap4m: p misaligned?!?");
	if (VM_MIN_KERNEL_ADDRESS_SRMMU % NBPRG != 0)
		panic("pmap_bootstrap4m: VM_MIN_KERNEL_ADDRESS not region-aligned");
#endif

	for (q = (caddr_t) KERNBASE; q < p; q += NBPG) {
		struct regmap *rp;
		struct segmap *sp;
		int pte, *ptep;
		extern char __data_start[];
		caddr_t sdata = (caddr_t)trunc_page((vaddr_t)__data_start);

		/*
		 * Now install entry for current page.
		 */
		rp = &pmap_kernel()->pm_regmap[VA_VREG(q)];
		sp = &rp->rg_segmap[VA_VSEG(q)];
		ptep = &sp->sg_pte[VA_VPG(q)];

		pte = PMAP_BOOTSTRAP_VA2PA(q) >> SRMMU_PPNPASHIFT;
		pte |= PPROT_N_RX | SRMMU_TEPTE;

		/* Deal with the cacheable bit for pagetable memory */
		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) != 0 ||
		    q < (caddr_t)pagetables_start ||
		    q >= (caddr_t)pagetables_end)
			pte |= SRMMU_PG_C;

		/* write-protect kernel text */
		if (q < (caddr_t)trapbase || q >= sdata)
			pte |= PPROT_WRITE;

		setpgt4m(ptep, pte);
		sp->sg_npte++;
		pmap_kernel()->pm_stats.resident_count++;
	}

#if 0
	/*
	 * We also install the kernel mapping into all other contexts by
	 * copying the context 0 L1 PTP from cpuinfo.ctx_tbl[0] into the
	 * remainder of the context table (i.e. we share the kernel page-
	 * tables). Each user pmap automatically gets the kernel mapped
	 * into it when it is created, but we do this extra step early on
	 * in case some twit decides to switch to a context with no user
	 * pmap associated with it.
	 */
	for (i = 1; i < ncontext; i++)
		cpuinfo.ctx_tbl[i] = cpuinfo.ctx_tbl[0];
#endif

	if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
		/* Flush page tables from cache */
		pcache_flush((caddr_t)pagetables_start,
			     (caddr_t)pagetables_start_pa,
			     pagetables_end - pagetables_start);

	/*
	 * Now switch to kernel pagetables (finally!)
	 */
	mmu_install_tables(&cpuinfo);

	sparc_protection_init4m();
}

void
mmu_install_tables(sc)
	struct cpu_softc *sc;
{

#ifdef DEBUG
	printf("pmap_bootstrap: installing kernel page tables...");
#endif
	setcontext4m(0);	/* paranoia? %%%: Make 0x3 a define! below */

	/* Enable MMU tablewalk caching, flush TLB */
	if (sc->mmu_enable != 0)
		sc->mmu_enable();

	tlb_flush_all();

	sta(SRMMU_CXTPTR, ASI_SRMMU,
	    (VA2PA((caddr_t)sc->ctx_tbl) >> SRMMU_PPNPASHIFT) & ~0x3);

	tlb_flush_all();

#ifdef DEBUG
	printf("done.\n");
#endif
}

#ifdef MULTIPROCESSOR
/*
 * Allocate per-CPU page tables.
 * Note: this routine is called in the context of the boot CPU
 * during autoconfig.
 */
void
pmap_alloc_cpu(sc)
	struct cpu_softc *sc;
{
	caddr_t cpustore;
	int *ctxtable;
	int *regtable;
	int *segtable;
	int *pagtable;
	int vr, vs, vpg;
	struct regmap *rp;
	struct segmap *sp;

	/* Allocate properly aligned and physically contiguous memory here */
	cpustore = 0;
	ctxtable = 0;
	regtable = 0;
	segtable = 0;
	pagtable = 0;

	vr = VA_VREG(CPUINFO_VA);
	vs = VA_VSEG(CPUINFO_VA);
	vpg = VA_VPG(CPUINFO_VA);
	rp = &pmap_kernel()->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];

	/*
	 * Copy page tables, then modify entry for CPUINFO_VA so that
	 * it points at the per-CPU pages.
	 */
	bcopy(cpuinfo.L1_ptps, regtable, SRMMU_L1SIZE * sizeof(int));
	regtable[vr] =
		(VA2PA((caddr_t)segtable) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;

	bcopy(rp->rg_seg_ptps, segtable, SRMMU_L2SIZE * sizeof(int));
	segtable[vs] =
		(VA2PA((caddr_t)pagtable) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;

	bcopy(sp->sg_pte, pagtable, SRMMU_L3SIZE * sizeof(int));
	pagtable[vpg] =
		(VA2PA((caddr_t)cpustore) >> SRMMU_PPNPASHIFT) |
		(SRMMU_TEPTE | PPROT_RW_RW | SRMMU_PG_C);

	/* Install L1 table in context 0 */
	ctxtable[0] = ((u_int)regtable >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;

	sc->ctx_tbl = ctxtable;
	sc->L1_ptps = regtable;

#if 0
	if ((sc->flags & CPUFLG_CACHEPAGETABLES) == 0) {
		kvm_uncache((caddr_t)0, 1);
	}
#endif
}
#endif /* MULTIPROCESSOR */

#endif /* defined sun4m */


void
pmap_init()
{
	pool_init(&pvpool, sizeof(struct pvlist), 0, 0, 0, "pvpl", NULL);
	pool_setipl(&pvpool, IPL_VM);

#if defined(SUN4M)
        if (CPU_ISSUN4M) {
                /*
                 * The SRMMU only ever needs chunks in one of two sizes:
                 * 1024 (for region level tables) and 256 (for segment
                 * and page level tables).
                 */
                int n;

                n = SRMMU_L1SIZE * sizeof(int);
                pool_init(&L1_pool, n, n, 0, 0, "L1 pagetable",
		    &pgt_allocator);
		pool_setipl(&L1_pool, IPL_VM);

                n = SRMMU_L2SIZE * sizeof(int);
                pool_init(&L23_pool, n, n, 0, 0, "L2/L3 pagetable",
                    &pgt_allocator);
		pool_setipl(&L23_pool, IPL_VM);
        }
#endif
}

/*
 * Called just after enabling cache (so that CPUFLG_CACHEPAGETABLES is
 * set correctly).
 */
void
pmap_cache_enable()
{
#ifdef SUN4M
	if (CPU_ISSUN4M) {
		int pte;

		/*
		 * Deal with changed CPUFLG_CACHEPAGETABLES.
		 *
		 * If the tables were uncached during the initial mapping
		 * and cache_enable set the flag we recache the tables.
		 */

		pte = getpte4m(pagetables_start);

		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) != 0 &&
		    (pte & SRMMU_PG_C) == 0)
			kvm_recache((caddr_t)pagetables_start,
				    atop(pagetables_end - pagetables_start));
	}
#endif
}


/*
 * Map physical addresses into kernel VM.
 */
vaddr_t
pmap_map(va, pa, endpa, prot)
	vaddr_t va;
	paddr_t pa, endpa;
	int prot;
{
	int pgsize = PAGE_SIZE;

	while (pa < endpa) {
		pmap_kenter_pa(va, pa, prot);
		va += pgsize;
		pa += pgsize;
	}
	return (va);
}

/*
 * Create and return a physical map.
 *
 * If size is nonzero, the map is useless. (ick)
 */
struct pmap *
pmap_create()
{
	struct pmap *pm;
	int size;
	void *urp;

	pm = (struct pmap *)malloc(sizeof *pm, M_VMPMAP, M_WAITOK);
#ifdef DEBUG
	if (pmapdebug & PDB_CREATE)
		printf("pmap_create: created %p\n", pm);
#endif
	bzero((caddr_t)pm, sizeof *pm);

	size = NUREG * sizeof(struct regmap);

	pm->pm_regstore = urp = malloc(size, M_VMPMAP, M_WAITOK);
	qzero((caddr_t)urp, size);
	/* pm->pm_ctx = NULL; */
	pm->pm_refcount = 1;
	pm->pm_regmap = urp;

	if (CPU_ISSUN4OR4COR4E) {
		TAILQ_INIT(&pm->pm_seglist);
#if defined(SUN4_MMU3L)
		TAILQ_INIT(&pm->pm_reglist);
		if (HASSUN4_MMU3L) {
			int i;
			for (i = NUREG_4C; --i >= 0;)
				pm->pm_regmap[i].rg_smeg = reginval;
		}
#endif
	}
#if defined(SUN4M)
	else {
		int i;

		/*
		 * We must allocate and initialize hardware-readable (MMU)
		 * pagetables. We must also map the kernel regions into this
		 * pmap's pagetables, so that we can access the kernel from
		 * this user context.
		 *
		 * Note: pm->pm_regmap's have been zeroed already, so we don't
		 * need to explicitly mark them as invalid (a null
		 * rg_seg_ptps pointer indicates invalid for the 4m)
		 */
		urp = pool_get(&L1_pool, PR_WAITOK);
		pm->pm_reg_ptps = urp;
		pm->pm_reg_ptps_pa = VA2PA(urp);

		/* Invalidate user mappings */
		for (i = 0; i < NUREG_4M; i++)
			setpgt4m(&pm->pm_reg_ptps[i], SRMMU_TEINVALID);

		/* Copy kernel regions */
		for (i = 0; i < NKREG_4M; i++) {
			setpgt4m(&pm->pm_reg_ptps[NUREG_4M + i],
				 cpuinfo.L1_ptps[NUREG_4M + i]);
		}
	}
#endif

	pm->pm_gap_end = VA_VREG(VM_MAXUSER_ADDRESS);

	return (pm);
}

/*
 * Retire the given pmap from service.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_destroy(pm)
	struct pmap *pm;
{
	int count;

	if (pm == NULL)
		return;
#ifdef DEBUG
	if (pmapdebug & PDB_DESTROY)
		printf("pmap_destroy(%p)\n", pm);
#endif
	count = --pm->pm_refcount;
	if (count == 0) {
		pmap_release(pm);
		free(pm, M_VMPMAP, 0);
	}
}

/*
 * Release any resources held by the given physical map.
 */
void
pmap_release(pm)
	struct pmap *pm;
{
	union ctxinfo *c;
	int s = splvm();	/* paranoia */

#ifdef DEBUG
	if (pmapdebug & PDB_DESTROY)
		printf("pmap_release(%p)\n", pm);
#endif

	if (CPU_ISSUN4OR4COR4E) {
#if defined(SUN4_MMU3L)
		if (!TAILQ_EMPTY(&pm->pm_reglist))
			panic("pmap_release: region list not empty");
#endif
		if (!TAILQ_EMPTY(&pm->pm_seglist))
			panic("pmap_release: segment list not empty");

		if ((c = pm->pm_ctx) != NULL) {
			if (pm->pm_ctxnum == 0)
				panic("pmap_release: releasing kernel");
			ctx_free(pm);
		}
	}
	splx(s);

#ifdef DEBUG
if (pmapdebug) {
	int vs, vr;
	for (vr = 0; vr < NUREG; vr++) {
		struct regmap *rp = &pm->pm_regmap[vr];
		if (rp->rg_nsegmap != 0)
			printf("pmap_release: %d segments remain in "
				"region %d\n", rp->rg_nsegmap, vr);
		if (rp->rg_segmap != NULL) {
			printf("pmap_release: segments still "
				"allocated in region %d\n", vr);
			for (vs = 0; vs < NSEGRG; vs++) {
				struct segmap *sp = &rp->rg_segmap[vs];
				if (sp->sg_npte != 0)
					printf("pmap_release: %d ptes "
					     "remain in segment %d\n",
						sp->sg_npte, vs);
				if (sp->sg_pte != NULL) {
					printf("pmap_release: ptes still "
					     "allocated in segment %d\n", vs);
				}
			}
		}
	}
}
#endif
	free(pm->pm_regstore, M_VMPMAP, 0);

#if defined(SUN4M)
	if (CPU_ISSUN4M) {
		if ((c = pm->pm_ctx) != NULL) {
			if (pm->pm_ctxnum == 0)
				panic("pmap_release: releasing kernel");
			ctx_free(pm);
		}
		pool_put(&L1_pool, pm->pm_reg_ptps);
		pm->pm_reg_ptps = NULL;
		pm->pm_reg_ptps_pa = 0;
	}
#endif
}

/*
 * Add a reference to the given pmap.
 */
void
pmap_reference(pm)
	struct pmap *pm;
{
	if (pm != NULL)
		pm->pm_refcount++;
}

/*
 * Remove the given range of mapping entries.
 * The starting and ending addresses are already rounded to pages.
 * Sheer lunacy: pmap_remove is often asked to remove nonexistent
 * mappings.
 */
void
pmap_remove(pm, va, endva)
	struct pmap *pm;
	vaddr_t va, endva;
{
	vaddr_t nva;
	int vr, vs, s, ctx;
	void (*rm)(struct pmap *, vaddr_t, vaddr_t, int, int);

	if (pm == NULL)
		return;

#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("pmap_remove(%p, 0x%lx, 0x%lx)\n", pm, va, endva);
#endif

	if (pm == pmap_kernel()) {
		/*
		 * Removing from kernel address space.
		 */
		rm = pmap_rmk;
	} else {
		/*
		 * Removing from user address space.
		 */
		write_user_windows();
		rm = pmap_rmu;
	}

	ctx = getcontext();
	s = splvm();		/* XXX conservative */
	for (; va < endva; va = nva) {
		/* do one virtual segment at a time */
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		nva = VSTOVA(vr, vs + 1);
		if (nva == 0 || nva > endva)
			nva = endva;
		if (pm->pm_regmap[vr].rg_nsegmap != 0)
			(*rm)(pm, va, nva, vr, vs);
	}
	splx(s);
	setcontext(ctx);
}

void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	struct pmap *pm = pmap_kernel();
	vaddr_t nva, endva = va + len;
	int vr, vs, s, ctx;

#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("pmap_kremove(0x%lx, 0x%lx)\n", va, len);
#endif

	ctx = getcontext();
	s = splvm();		/* XXX conservative */

	for (; va < endva; va = nva) {
		/* do one virtual segment at a time */
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		nva = VSTOVA(vr, vs + 1);
		if (nva == 0 || nva > endva)
			nva = endva;
		if (pm->pm_regmap[vr].rg_nsegmap != 0)
			pmap_rmk(pm, va, nva, vr, vs);
	}

	splx(s);
	setcontext(ctx);
}

/*
 * The following magic number was chosen because:
 *	1. It is the same amount of work to cache_flush_page 4 pages
 *	   as to cache_flush_segment 1 segment (so at 4 the cost of
 *	   flush is the same).
 *	2. Flushing extra pages is bad (causes cache not to work).
 *	3. The current code, which malloc()s 5 pages for each process
 *	   for a user vmspace/pmap, almost never touches all 5 of those
 *	   pages.
 */
#if 0
#define	PMAP_RMK_MAGIC	(cacheinfo.c_hwflush?5:64)	/* if > magic, use cache_flush_segment */
#else
#define	PMAP_RMK_MAGIC	5	/* if > magic, use cache_flush_segment */
#endif

/*
 * Remove a range contained within a single segment.
 * These are egregiously complicated routines.
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)

/* remove from kernel */
void
pmap_rmk4_4c(pm, va, endva, vr, vs)
	struct pmap *pm;
	vaddr_t va, endva;
	int vr, vs;
{
	int i, tpte, perpage, npg;
	struct pvlist *pv;
	int nleft, pmeg;
	struct regmap *rp;
	struct segmap *sp;
	int s;

	rp = &pm->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];

	if (rp->rg_nsegmap == 0)
		return;

#ifdef DEBUG
	if (rp->rg_segmap == NULL)
		panic("pmap_rmk: no segments");
#endif

	if ((nleft = sp->sg_npte) == 0)
		return;

	pmeg = sp->sg_pmeg;

#ifdef DEBUG
	if (pmeg == seginval)
		panic("pmap_rmk: not loaded");
	if (pm->pm_ctx == NULL)
		panic("pmap_rmk: lost context");
#endif

	setcontext4(0);
	/* decide how to flush cache */
	npg = (endva - va) >> PGSHIFT;
	if (npg > PMAP_RMK_MAGIC) {
		/* flush the whole segment */
		perpage = 0;
		cache_flush_segment(vr, vs);
	} else {
		/* flush each page individually; some never need flushing */
		perpage = (CACHEINFO.c_vactype != VAC_NONE);
	}
	while (va < endva) {
		tpte = getpte4(va);
		if ((tpte & PG_V) == 0) {
			va += NBPG;
			continue;
		}
		if ((tpte & PG_TYPE) == PG_OBMEM) {
			/* if cacheable, flush page as needed */
			if (perpage && (tpte & PG_NC) == 0)
				cache_flush_page(va);
			pv = pvhead(tpte & PG_PFNUM);
			if (pv) {
				pv->pv_flags |= MR4_4C(tpte);
				s = splvm();
				pv_unlink4_4c(pv, pm, va);
				splx(s);
			}
		}
		nleft--;
		setpte4(va, 0);
		pm->pm_stats.resident_count--;
		va += NBPG;
	}

	/*
	 * If the segment is all gone, remove it from everyone and
	 * free the MMU entry.
	 */
	if ((sp->sg_npte = nleft) == 0) {
		va = VSTOVA(vr,vs);		/* retract */
#if defined(SUN4_MMU3L)
		if (HASSUN4_MMU3L)
			setsegmap(va, seginval);
		else
#endif
			for (i = ncontext; --i >= 0;) {
				setcontext4(i);
				setsegmap(va, seginval);
			}
		me_free(pm, pmeg);
		if (--rp->rg_nsegmap == 0) {
#if defined(SUN4_MMU3L)
			if (HASSUN4_MMU3L) {
				for (i = ncontext; --i >= 0;) {
					setcontext4(i);
					setregmap(va, reginval);
				}
				/* note: context is 0 */
				region_free(pm, rp->rg_smeg);
			}
#endif
		}
	}
}

#endif /* sun4, sun4c */

#if defined(SUN4M)		/* 4M version of pmap_rmk */
/* remove from kernel (4m)*/
void
pmap_rmk4m(pm, va, endva, vr, vs)
	struct pmap *pm;
	vaddr_t va, endva;
	int vr, vs;
{
	int tpte, perpage, npg;
	struct pvlist *pv;
	int nleft;
	struct regmap *rp;
	struct segmap *sp;

	rp = &pm->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];

	if (rp->rg_nsegmap == 0)
		return;

#ifdef DEBUG
	if (rp->rg_segmap == NULL)
		panic("pmap_rmk: no segments");
#endif

	if ((nleft = sp->sg_npte) == 0)
		return;

#ifdef DEBUG
	if (sp->sg_pte == NULL || rp->rg_seg_ptps == NULL)
		panic("pmap_rmk: segment/region does not exist");
	if (pm->pm_ctx == NULL)
		panic("pmap_rmk: lost context");
#endif

	setcontext4m(0);
	/* decide how to flush cache */
	npg = (endva - va) >> PGSHIFT;
	if (npg > PMAP_RMK_MAGIC) {
		/* flush the whole segment */
		perpage = 0;
		if (CACHEINFO.c_vactype != VAC_NONE)
			cache_flush_segment(vr, vs);
	} else {
		/* flush each page individually; some never need flushing */
		perpage = (CACHEINFO.c_vactype != VAC_NONE);
	}
	while (va < endva) {
		tpte = sp->sg_pte[VA_SUN4M_VPG(va)];
		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE) {
#ifdef DEBUG
			if ((pmapdebug & PDB_SANITYCHK) &&
			    (getpte4m(va) & SRMMU_TETYPE) == SRMMU_TEPTE)
				panic("pmap_rmk: Spurious kTLB entry for 0x%lx",
				      va);
#endif
			va += NBPG;
			continue;
		}
		if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
			/* if cacheable, flush page as needed */
			if (perpage && (tpte & SRMMU_PG_C))
				cache_flush_page(va);
			pv = pvhead((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (pv) {
				pv->pv_flags |= MR4M(tpte);
				pv_unlink4m(pv, pm, va);
			}
		}
		nleft--;
		tlb_flush_page(va);
		setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], SRMMU_TEINVALID);
		pm->pm_stats.resident_count--;
		va += NBPG;
	}

	sp->sg_npte = nleft;
}
#endif /* sun4m */

/*
 * Just like pmap_rmk_magic, but we have a different threshold.
 * Note that this may well deserve further tuning work.
 */
#if 0
#define	PMAP_RMU_MAGIC	(cacheinfo.c_hwflush?4:64)	/* if > magic, use cache_flush_segment */
#else
#define	PMAP_RMU_MAGIC	4	/* if > magic, use cache_flush_segment */
#endif

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)

/* remove from user */
void
pmap_rmu4_4c(pm, va, endva, vr, vs)
	struct pmap *pm;
	vaddr_t va, endva;
	int vr, vs;
{
	int *pte0, pteva, tpte, perpage, npg;
	struct pvlist *pv;
	int nleft, pmeg;
	struct regmap *rp;
	struct segmap *sp;
	int s;

	rp = &pm->pm_regmap[vr];
	if (rp->rg_nsegmap == 0)
		return;
	if (rp->rg_segmap == NULL)
		panic("pmap_rmu: no segments");

	sp = &rp->rg_segmap[vs];
	if ((nleft = sp->sg_npte) == 0)
		return;

	if (sp->sg_pte == NULL)
		panic("pmap_rmu: no pages");

	pmeg = sp->sg_pmeg;
	pte0 = sp->sg_pte;

	if (pmeg == seginval) {
		int *pte = pte0 + VA_VPG(va);

		/*
		 * PTEs are not in MMU.  Just invalidate software copies.
		 */
		for (; va < endva; pte++, va += NBPG) {
			tpte = *pte;
			if ((tpte & PG_V) == 0) {
				/* nothing to remove (braindead VM layer) */
				continue;
			}
			if ((tpte & PG_TYPE) == PG_OBMEM) {
				struct pvlist *pv;

				pv = pvhead(tpte & PG_PFNUM);
				if (pv) {
					s = splvm();
					pv_unlink4_4c(pv, pm, va);
					splx(s);
				}
			}
			nleft--;
			*pte = 0;
			pm->pm_stats.resident_count--;
		}
		if ((sp->sg_npte = nleft) == 0) {
			free(pte0, M_VMPMAP, 0);
			sp->sg_pte = NULL;
			if (--rp->rg_nsegmap == 0) {
				free(rp->rg_segmap, M_VMPMAP, 0);
				rp->rg_segmap = NULL;
#if defined(SUN4_MMU3L)
				if (HASSUN4_MMU3L && rp->rg_smeg != reginval) {
					if (pm->pm_ctx) {
						setcontext4(pm->pm_ctxnum);
						setregmap(va, reginval);
					} else
						setcontext4(0);
					region_free(pm, rp->rg_smeg);
				}
#endif
			}
		}
		return;
	}

	/*
	 * PTEs are in MMU.  Invalidate in hardware, update ref &
	 * mod bits, and flush cache if required.
	 */
	if (CTX_USABLE(pm,rp)) {
		/* process has a context, must flush cache */
		setcontext4(pm->pm_ctxnum);
		npg = (endva - va) >> PGSHIFT;
		if (npg > PMAP_RMU_MAGIC) {
			perpage = 0; /* flush the whole segment */
			cache_flush_segment(vr, vs);
		} else
			perpage = (CACHEINFO.c_vactype != VAC_NONE);
		pteva = va;
	} else {
		/* no context, use context 0; cache flush unnecessary */
		setcontext4(0);
		if (HASSUN4_MMU3L)
			setregmap(0, tregion);
		/* XXX use per-cpu pteva? */
		setsegmap(0, pmeg);
		pteva = VA_VPG(va) << PGSHIFT;
		perpage = 0;
	}
	for (; va < endva; pteva += NBPG, va += NBPG) {
		tpte = getpte4(pteva);
		if ((tpte & PG_V) == 0)
			continue;
		if ((tpte & PG_TYPE) == PG_OBMEM) {
			/* if cacheable, flush page as needed */
			if (perpage && (tpte & PG_NC) == 0)
				cache_flush_page(va);
			pv = pvhead(tpte & PG_PFNUM);
			if (pv) {
				pv->pv_flags |= MR4_4C(tpte);
				s = splvm();
				pv_unlink4_4c(pv, pm, va);
				splx(s);
			}
		}
		nleft--;
		setpte4(pteva, 0);
		pte0[VA_VPG(pteva)] = 0;
		pm->pm_stats.resident_count--;
	}

	/*
	 * If the segment is all gone, and the context is loaded, give
	 * the segment back.
	 */
	if ((sp->sg_npte = nleft) == 0 /* ??? && pm->pm_ctx != NULL*/) {
#ifdef DEBUG
if (pm->pm_ctx == NULL) {
	printf("pmap_rmu: no context here...");
}
#endif
		va = VSTOVA(vr,vs);		/* retract */
		if (CTX_USABLE(pm,rp))
			setsegmap(va, seginval);
		else if (HASSUN4_MMU3L && rp->rg_smeg != reginval) {
			/* note: context already set earlier */
			setregmap(0, rp->rg_smeg);
			setsegmap(vs << SGSHIFT, seginval);
		}
		free(pte0, M_VMPMAP, 0);
		sp->sg_pte = NULL;
		me_free(pm, pmeg);

		if (--rp->rg_nsegmap == 0) {
			free(rp->rg_segmap, M_VMPMAP, 0);
			rp->rg_segmap = NULL;
			GAP_WIDEN(pm,vr);

#if defined(SUN4_MMU3L)
			if (HASSUN4_MMU3L && rp->rg_smeg != reginval) {
				/* note: context already set */
				if (pm->pm_ctx)
					setregmap(va, reginval);
				region_free(pm, rp->rg_smeg);
			}
#endif
		}

	}
}

#endif /* sun4,4c */

#if defined(SUN4M)		/* 4M version of pmap_rmu */
/* remove from user */
void
pmap_rmu4m(pm, va, endva, vr, vs)
	struct pmap *pm;
	vaddr_t va, endva;
	int vr, vs;
{
	int *pte0, perpage, npg;
	struct pvlist *pv;
	int nleft;
	struct regmap *rp;
	struct segmap *sp;

	rp = &pm->pm_regmap[vr];
	if (rp->rg_nsegmap == 0)
		return;
	if (rp->rg_segmap == NULL)
		panic("pmap_rmu: no segments");

	sp = &rp->rg_segmap[vs];
	if ((nleft = sp->sg_npte) == 0)
		return;

	if (sp->sg_pte == NULL)
		panic("pmap_rmu: no pages");

	pte0 = sp->sg_pte;

	/*
	 * Invalidate PTE in MMU pagetables. Flush cache if necessary.
	 */
	if (pm->pm_ctx) {
		/* process has a context, must flush cache */
		setcontext4m(pm->pm_ctxnum);
		if (CACHEINFO.c_vactype != VAC_NONE) {
			npg = (endva - va) >> PGSHIFT;
			if (npg > PMAP_RMU_MAGIC) {
				perpage = 0; /* flush the whole segment */
				cache_flush_segment(vr, vs);
			} else
				perpage = 1;
		} else
			perpage = 0;
	} else {
		/* no context; cache flush unnecessary */
		perpage = 0;
	}
	for (; va < endva; va += NBPG) {

		int tpte = pte0[VA_SUN4M_VPG(va)];

		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE) {
#ifdef DEBUG
			if ((pmapdebug & PDB_SANITYCHK) &&
			    pm->pm_ctx &&
			    (getpte4m(va) & SRMMU_TEPTE) == SRMMU_TEPTE)
				panic("pmap_rmu: Spurious uTLB entry for 0x%lx",
				      va);
#endif
			continue;
		}

		if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
			/* if cacheable, flush page as needed */
			if (perpage && (tpte & SRMMU_PG_C))
				cache_flush_page(va);
			pv = pvhead((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (pv) {
				pv->pv_flags |= MR4M(tpte);
				pv_unlink4m(pv, pm, va);
			}
		}
		nleft--;
		if (pm->pm_ctx)
			tlb_flush_page(va);
		setpgt4m(&pte0[VA_SUN4M_VPG(va)], SRMMU_TEINVALID);
		pm->pm_stats.resident_count--;
	}

	/*
	 * If the segment is all gone, and the context is loaded, give
	 * the segment back.
	 */
	if ((sp->sg_npte = nleft) == 0) {
#ifdef DEBUG
		if (pm->pm_ctx == NULL) {
			printf("pmap_rmu: no context here...");
		}
#endif
		va = VSTOVA(vr,vs);		/* retract */

		if (pm->pm_ctx)
			tlb_flush_segment(vr, vs); 	/* Paranoia? */
		setpgt4m(&rp->rg_seg_ptps[vs], SRMMU_TEINVALID);
		pool_put(&L23_pool, pte0);
		sp->sg_pte = NULL;

		if (--rp->rg_nsegmap == 0) {
			if (pm->pm_ctx)
				tlb_flush_context(); 	/* Paranoia? */
			setpgt4m(&pm->pm_reg_ptps[vr], SRMMU_TEINVALID);
			free(rp->rg_segmap, M_VMPMAP, 0);
			rp->rg_segmap = NULL;
			pool_put(&L23_pool, rp->rg_seg_ptps);
		}
	}
}
#endif /* sun4m */

/*
 * Lower (make more strict) the protection on the specified
 * physical page.
 *
 * There are only two cases: either the protection is going to 0
 * (in which case we do the dirty work here), or it is going from
 * to read-only (in which case pv_changepte does the trick).
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
void
pmap_page_protect4_4c(struct vm_page *pg, vm_prot_t prot)
{
	struct pvlist *pv, *pv0, *npv;
	struct pmap *pm;
	int va, vr, vs, pteva, tpte;
	int flags, nleft, i, s, ctx;
	struct regmap *rp;
	struct segmap *sp;

#ifdef DEBUG
	if ((pmapdebug & PDB_CHANGEPROT) ||
	    (pmapdebug & PDB_REMOVE && prot == PROT_NONE))
		printf("pmap_page_protect(%p, 0x%x)\n", pg, prot);
#endif
	pv = &pg->mdpage.pv_head;
	/*
	 * Skip operations that do not take away write permission.
	 */
	if (prot & PROT_WRITE)
		return;
	write_user_windows();	/* paranoia */
	if (prot & PROT_READ) {
		pv_changepte4_4c(pv, 0, PG_W);
		return;
	}

	/*
	 * Remove all access to all people talking to this page.
	 * Walk down PV list, removing all mappings.
	 * The logic is much like that for pmap_remove,
	 * but we know we are removing exactly one page.
	 */
	s = splvm();
	if ((pm = pv->pv_pmap) == NULL) {
		splx(s);
		return;
	}
	ctx = getcontext4();
	pv0 = pv;
	flags = pv->pv_flags & ~PV_NC;
	while (pv != NULL) {
		pm = pv->pv_pmap;
		va = pv->pv_va;
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
#ifdef DIAGNOSTIC
		if (rp->rg_nsegmap == 0)
			panic("pmap_remove_all: empty vreg");
#endif
		sp = &rp->rg_segmap[vs];
#ifdef DIAGNOSTIC
		if (sp->sg_npte == 0)
			panic("pmap_remove_all: empty vseg");
#endif
		nleft = --sp->sg_npte;

		if (sp->sg_pmeg == seginval) {
			/* Definitely not a kernel map */
			if (nleft) {
				sp->sg_pte[VA_VPG(va)] = 0;
			} else {
				free(sp->sg_pte, M_VMPMAP, 0);
				sp->sg_pte = NULL;
				if (--rp->rg_nsegmap == 0) {
					free(rp->rg_segmap, M_VMPMAP, 0);
					rp->rg_segmap = NULL;
					GAP_WIDEN(pm,vr);
#if defined(SUN4_MMU3L)
					if (HASSUN4_MMU3L && rp->rg_smeg != reginval) {
						if (pm->pm_ctx) {
							setcontext4(pm->pm_ctxnum);
							setregmap(va, reginval);
						} else
							setcontext4(0);
						region_free(pm, rp->rg_smeg);
					}
#endif
				}
			}
			goto nextpv;
		}

		if (CTX_USABLE(pm,rp)) {
			setcontext4(pm->pm_ctxnum);
			pteva = va;
			cache_flush_page(va);
		} else {
			setcontext4(0);
			/* XXX use per-cpu pteva? */
			if (HASSUN4_MMU3L)
				setregmap(0, tregion);
			setsegmap(0, sp->sg_pmeg);
			pteva = VA_VPG(va) << PGSHIFT;
		}

		tpte = getpte4(pteva);
#ifdef DIAGNOSTIC
		if ((tpte & PG_V) == 0)
			panic("pmap_page_protect !PG_V: ctx %d, va 0x%x, pte 0x%x",
			      pm->pm_ctxnum, va, tpte);
#endif
		flags |= MR4_4C(tpte);

		if (nleft) {
			setpte4(pteva, 0);
			if (sp->sg_pte != NULL)
				sp->sg_pte[VA_VPG(pteva)] = 0;
			goto nextpv;
		}

		/* Entire segment is gone */
		if (pm == pmap_kernel()) {
#if defined(SUN4_MMU3L)
			if (!HASSUN4_MMU3L)
#endif
				for (i = ncontext; --i >= 0;) {
					setcontext4(i);
					setsegmap(va, seginval);
				}
			me_free(pm, sp->sg_pmeg);
			if (--rp->rg_nsegmap == 0) {
#if defined(SUN4_MMU3L)
				if (HASSUN4_MMU3L) {
					for (i = ncontext; --i >= 0;) {
						setcontext4(i);
						setregmap(va, reginval);
					}
					region_free(pm, rp->rg_smeg);
				}
#endif
			}
		} else {
			if (CTX_USABLE(pm,rp))
				/* `pteva'; we might be using tregion */
				setsegmap(pteva, seginval);
#if defined(SUN4_MMU3L)
			else if (HASSUN4_MMU3L &&
				 rp->rg_smeg != reginval) {
				/* note: context already set earlier */
				setregmap(0, rp->rg_smeg);
				setsegmap(vs << SGSHIFT, seginval);
			}
#endif
			free(sp->sg_pte, M_VMPMAP, 0);
			sp->sg_pte = NULL;
			me_free(pm, sp->sg_pmeg);

			if (--rp->rg_nsegmap == 0) {
#if defined(SUN4_MMU3L)
				if (HASSUN4_MMU3L &&
				    rp->rg_smeg != reginval) {
					if (pm->pm_ctx)
						setregmap(va, reginval);
					region_free(pm, rp->rg_smeg);
				}
#endif
				free(rp->rg_segmap, M_VMPMAP, 0);
				rp->rg_segmap = NULL;
				GAP_WIDEN(pm,vr);
			}
		}

	nextpv:
		pm->pm_stats.resident_count--;
		npv = pv->pv_next;
		if (pv != pv0)
			pool_put(&pvpool, pv);
		pv = npv;
	}
	pv0->pv_pmap = NULL;
	pv0->pv_next = NULL;
	pv0->pv_flags = flags;
	setcontext4(ctx);
	splx(s);
}

/*
 * Lower (make more strict) the protection on the specified
 * range of this pmap.
 *
 * There are only two cases: either the protection is going to 0
 * (in which case we call pmap_remove to do the dirty work), or
 * it is going from read/write to read-only.  The latter is
 * fairly easy.
 */
void
pmap_protect4_4c(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	int va, nva, vr, vs;
	int s, ctx;
	struct regmap *rp;
	struct segmap *sp;

	if (pm == NULL || prot & PROT_WRITE)
		return;

	if ((prot & PROT_READ) == 0) {
		pmap_remove(pm, sva, eva);
		return;
	}

	write_user_windows();
	ctx = getcontext4();
	s = splvm();

	for (va = sva; va < eva;) {
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
		nva = VSTOVA(vr,vs + 1);
if (nva == 0) panic("pmap_protect: last segment");	/* cannot happen */
		if (nva > eva)
			nva = eva;
		if (rp->rg_nsegmap == 0) {
			va = nva;
			continue;
		}
#ifdef DEBUG
		if (rp->rg_segmap == NULL)
			panic("pmap_protect: no segments");
#endif
		sp = &rp->rg_segmap[vs];
		if (sp->sg_npte == 0) {
			va = nva;
			continue;
		}
#ifdef DEBUG
		if (sp->sg_pte == NULL)
			panic("pmap_protect: no pages");
#endif
		if (sp->sg_pmeg == seginval) {
			int *pte = &sp->sg_pte[VA_VPG(va)];

			/* not in MMU; just clear PG_W from core copies */
			for (; va < nva; va += NBPG)
				*pte++ &= ~PG_W;
		} else {
			/* in MMU: take away write bits from MMU PTEs */
			if (CTX_USABLE(pm,rp)) {
				int tpte;

				/*
				 * Flush cache so that any existing cache
				 * tags are updated.  This is really only
				 * needed for PTEs that lose PG_W.
				 */
				setcontext4(pm->pm_ctxnum);
				for (; va < nva; va += NBPG) {
					tpte = getpte4(va);
					pmap_stats.ps_npg_prot_all++;
					if ((tpte & (PG_W|PG_TYPE)) ==
					    (PG_W|PG_OBMEM)) {
						pmap_stats.ps_npg_prot_actual++;
						cache_flush_page(va);
						setpte4(va, tpte & ~PG_W);
					}
				}
			} else {
				int pteva;

				/*
				 * No context, hence not cached;
				 * just update PTEs.
				 */
				setcontext4(0);
				/* XXX use per-cpu pteva? */
				if (HASSUN4_MMU3L)
					setregmap(0, tregion);
				setsegmap(0, sp->sg_pmeg);
				pteva = VA_VPG(va) << PGSHIFT;
				for (; va < nva; pteva += NBPG, va += NBPG)
					setpte4(pteva, getpte4(pteva) & ~PG_W);
			}
		}
	}
	splx(s);
	setcontext4(ctx);
}

/*
 * Change the protection and/or wired status of the given (MI) virtual page.
 * XXX: should have separate function (or flag) telling whether only wiring
 * is changing.
 */
void
pmap_changeprot4_4c(pm, va, prot, wired)
	struct pmap *pm;
	vaddr_t va;
	vm_prot_t prot;
	int wired;
{
	int vr, vs, tpte, newprot, ctx, s;
	struct regmap *rp;
	struct segmap *sp;

#ifdef DEBUG
	if (pmapdebug & PDB_CHANGEPROT)
		printf("pmap_changeprot(%p, 0x%lx, 0x%x, 0x%x)\n",
		    pm, va, prot, wired);
#endif

	write_user_windows();	/* paranoia */

	va = trunc_page(va);
	if (pm == pmap_kernel())
		newprot = prot & PROT_WRITE ? PG_S|PG_W : PG_S;
	else
		newprot = prot & PROT_WRITE ? PG_W : 0;
	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	s = splvm();		/* conservative */
	rp = &pm->pm_regmap[vr];
	if (rp->rg_nsegmap == 0) {
		printf("pmap_changeprot: no segments in %d\n", vr);
		splx(s);
		return;
	}
	if (rp->rg_segmap == NULL) {
		printf("pmap_changeprot: no segments in %d!\n", vr);
		splx(s);
		return;
	}
	sp = &rp->rg_segmap[vs];

	pmap_stats.ps_changeprots++;

#ifdef DEBUG
	if (pm != pmap_kernel() && sp->sg_pte == NULL)
		panic("pmap_changeprot: no pages");
#endif

	/* update PTEs in software or hardware */
	if (sp->sg_pmeg == seginval) {
		int *pte = &sp->sg_pte[VA_VPG(va)];

		/* update in software */
		if ((*pte & PG_PROT) == newprot)
			goto useless;
		*pte = (*pte & ~PG_PROT) | newprot;
	} else {
		/* update in hardware */
		ctx = getcontext4();
		if (CTX_USABLE(pm,rp)) {
			/*
			 * Use current context.
			 * Flush cache if page has been referenced to
			 * avoid stale protection bits in the cache tags.
			 */
			setcontext4(pm->pm_ctxnum);
			tpte = getpte4(va);
			if ((tpte & PG_PROT) == newprot) {
				setcontext4(ctx);
				goto useless;
			}
			if ((tpte & (PG_U|PG_NC|PG_TYPE)) == (PG_U|PG_OBMEM))
				cache_flush_page((int)va);
		} else {
			setcontext4(0);
			/* XXX use per-cpu va? */
			if (HASSUN4_MMU3L)
				setregmap(0, tregion);
			setsegmap(0, sp->sg_pmeg);
			va = VA_VPG(va) << PGSHIFT;
			tpte = getpte4(va);
			if ((tpte & PG_PROT) == newprot) {
				setcontext4(ctx);
				goto useless;
			}
		}
		tpte = (tpte & ~PG_PROT) | newprot;
		setpte4(va, tpte);
		setcontext4(ctx);
	}
	splx(s);
	return;

useless:
	/* only wiring changed, and we ignore wiring */
	pmap_stats.ps_useless_changeprots++;
	splx(s);
}

#endif /* sun4, 4c */

#if defined(SUN4M)		/* 4M version of protection routines above */
/*
 * Lower (make more strict) the protection on the specified
 * physical page.
 *
 * There are only two cases: either the protection is going to 0
 * (in which case we do the dirty work here), or it is going
 * to read-only (in which case pv_changepte does the trick).
 */
void
pmap_page_protect4m(struct vm_page *pg, vm_prot_t prot)
{
	struct pvlist *pv, *pv0, *npv;
	struct pmap *pm;
	int va, vr, vs, tpte;
	int flags, s, ctx;
	struct regmap *rp;
	struct segmap *sp;

#ifdef DEBUG
	if ((pmapdebug & PDB_CHANGEPROT) ||
	    (pmapdebug & PDB_REMOVE && prot == PROT_NONE))
		printf("pmap_page_protect(%p, 0x%x)\n", pg, prot);
#endif
	pv = &pg->mdpage.pv_head;
	/*
	 * Skip operations that do not take away write permission.
	 */
	if (prot & PROT_WRITE)
		return;
	write_user_windows();	/* paranoia */
	if (prot & PROT_READ) {
		pv_changepte4m(pv, 0, PPROT_WRITE);
		return;
	}

	/*
	 * Remove all access to all people talking to this page.
	 * Walk down PV list, removing all mappings.
	 * The logic is much like that for pmap_remove,
	 * but we know we are removing exactly one page.
	 */
	s = splvm();
	if ((pm = pv->pv_pmap) == NULL) {
		splx(s);
		return;
	}
	ctx = getcontext4m();
	pv0 = pv;
	flags = pv->pv_flags;
	while (pv != NULL) {
		pm = pv->pv_pmap;
		va = pv->pv_va;
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
#ifdef DIAGNOSTIC
		if (rp->rg_nsegmap == 0)
			panic("pmap_page_protect4m: empty vreg");
#endif
		sp = &rp->rg_segmap[vs];
#ifdef DIAGNOSTIC
		if (sp->sg_npte == 0)
			panic("pmap_page_protect4m: empty vseg");
#endif
		sp->sg_npte--;

		/* Invalidate PTE in pagetables. Flush cache if necessary */
		if (pm->pm_ctx) {
			setcontext4m(pm->pm_ctxnum);
			cache_flush_page(va);
			tlb_flush_page(va);
		}

		tpte = sp->sg_pte[VA_SUN4M_VPG(va)];

#ifdef DIAGNOSTIC
		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE)
			panic("pmap_page_protect4m: !TEPTE");
#endif

		flags |= MR4M(tpte);

		setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], SRMMU_TEINVALID);
		pm->pm_stats.resident_count--;

		/* Entire segment is gone */
		if (sp->sg_npte == 0 && pm != pmap_kernel()) {
			if (pm->pm_ctx)
				tlb_flush_segment(vr, vs);
			setpgt4m(&rp->rg_seg_ptps[vs], SRMMU_TEINVALID);
			pool_put(&L23_pool, sp->sg_pte);
			sp->sg_pte = NULL;

			if (--rp->rg_nsegmap == 0) {
				if (pm->pm_ctx)
					tlb_flush_context();
				setpgt4m(&pm->pm_reg_ptps[vr], SRMMU_TEINVALID);
				free(rp->rg_segmap, M_VMPMAP, 0);
				rp->rg_segmap = NULL;
				pool_put(&L23_pool, rp->rg_seg_ptps);
			}
		}

		npv = pv->pv_next;
		if (pv != pv0)
			pool_put(&pvpool, pv);
		pv = npv;
	}
	pv0->pv_pmap = NULL;
	pv0->pv_next = NULL;
	pv0->pv_flags = (flags | PV_C4M) & ~PV_ANC;
	setcontext4m(ctx);
	splx(s);
}

/*
 * Lower (make more strict) the protection on the specified
 * range of this pmap.
 *
 * There are only two cases: either the protection is going to 0
 * (in which case we call pmap_remove to do the dirty work), or
 * it is going from read/write to read-only.  The latter is
 * fairly easy.
 */
void
pmap_protect4m(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	int va, nva, vr, vs;
	int s, ctx;
	struct regmap *rp;
	struct segmap *sp;
	int newprot;

	if ((prot & PROT_READ) == 0) {
		pmap_remove(pm, sva, eva);
		return;
	}

	/*
	 * Since the caller might request either a removal of PROT_EXEC
	 * or PROT_WRITE, we don't attempt to guess what to do, just lower
	 * to read-only and let the real protection be faulted in.
	 */
	newprot = pte_prot4m(pm, PROT_READ);

	write_user_windows();
	ctx = getcontext4m();
	s = splvm();

	for (va = sva; va < eva;) {
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
		nva = VSTOVA(vr,vs + 1);
		if (nva == 0)	/* XXX */
			panic("pmap_protect: last segment"); /* cannot happen(why?)*/
		if (nva > eva)
			nva = eva;
		if (rp->rg_nsegmap == 0) {
			va = nva;
			continue;
		}
#ifdef DEBUG
		if (rp->rg_segmap == NULL)
			panic("pmap_protect: no segments");
#endif
		sp = &rp->rg_segmap[vs];
		if (sp->sg_npte == 0) {
			va = nva;
			continue;
		}
#ifdef DEBUG
		if (sp->sg_pte == NULL)
			panic("pmap_protect: no pages");
#endif
		/* pages loaded: take away write bits from MMU PTEs */
		if (pm->pm_ctx)
			setcontext4m(pm->pm_ctxnum);

		pmap_stats.ps_npg_prot_all += (nva - va) >> PGSHIFT;
		for (; va < nva; va += NBPG) {
			int tpte, npte;

			tpte = sp->sg_pte[VA_SUN4M_VPG(va)];
			npte = (tpte & ~SRMMU_PROT_MASK) | newprot;

			/* Only do work when needed. */
			if (npte == tpte)
				continue;

			pmap_stats.ps_npg_prot_actual++;
			/*
			 * Flush cache so that any existing cache
			 * tags are updated.
			 */
			if (pm->pm_ctx) {
				if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
					cache_flush_page(va);
				}
				tlb_flush_page(va);
			}
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], npte);
		}
	}
	splx(s);
	setcontext4m(ctx);
}

/*
 * Change the protection and/or wired status of the given (MI) virtual page.
 * XXX: should have separate function (or flag) telling whether only wiring
 * is changing.
 */
void
pmap_changeprot4m(pm, va, prot, wired)
	struct pmap *pm;
	vaddr_t va;
	vm_prot_t prot;
	int wired;
{
	int tpte, newprot, ctx, s;
	int *ptep;

#ifdef DEBUG
	if (pmapdebug & PDB_CHANGEPROT)
		printf("pmap_changeprot(%p, 0x%lx, 0x%x, 0x%x)\n",
		    pm, va, prot, wired);
#endif

	write_user_windows();	/* paranoia */

	va = trunc_page(va);
	newprot = pte_prot4m(pm, prot);

	pmap_stats.ps_changeprots++;

	s = splvm();		/* conservative */
	ptep = getptep4m(pm, va);
	if (pm->pm_ctx) {
		ctx = getcontext4m();
		setcontext4m(pm->pm_ctxnum);
		/*
		 * Use current context.
		 * Flush cache if page has been referenced to
		 * avoid stale protection bits in the cache tags.
		 */
		tpte = *ptep;
		if ((tpte & (SRMMU_PG_C|SRMMU_PGTYPE)) ==
		    (SRMMU_PG_C|PG_SUN4M_OBMEM))
			cache_flush_page(va);
		tlb_flush_page(va);
		setcontext4m(ctx);
	} else {
		tpte = *ptep;
	}
	if ((tpte & SRMMU_PROT_MASK) == newprot) {
		/* only wiring changed, and we ignore wiring */
		pmap_stats.ps_useless_changeprots++;
		goto out;
	}
	setpgt4m(ptep, (tpte & ~SRMMU_PROT_MASK) | newprot);

out:
	splx(s);
}
#endif /* 4m */

/*
 * Insert (MI) physical page pa at virtual address va in the given pmap.
 * NB: the pa parameter includes type bits PMAP_OBIO, PMAP_NC as necessary.
 *
 * If pa is not in the `managed' range it will not be `bank mapped'.
 * This works during bootstrap only because the first 4MB happens to
 * map one-to-one.
 *
 * There may already be something else there, or we might just be
 * changing protections and/or wiring on an existing mapping.
 *	XXX	should have different entry points for changing!
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)

int
pmap_enter4_4c(pm, va, pa, prot, flags)
	struct pmap *pm;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	struct pvlist *pv;
	int pteproto, ctx;
	int ret;

	if (VA_INHOLE(va)) {
#ifdef DEBUG
		printf("pmap_enter: pm %p, va 0x%lx, pa 0x%lx: in MMU hole\n",
			pm, va, pa);
#endif
		return (EINVAL);
	}

#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter(%p, 0x%lx, 0x%lx, 0x%x, 0x%x)\n",
		    pm, va, pa, prot, flags);
#endif

	pteproto = PG_V | PMAP_T2PTE_4(pa);
	pa &= ~PMAP_TNC_4;
	/*
	 * Set up prototype for new PTE.  Cannot set PG_NC from PV_NC yet
	 * since the pvlist no-cache bit might change as a result of the
	 * new mapping.
	 */
	if ((pteproto & PG_TYPE) == PG_OBMEM)
		pv = pvhead(atop(pa));
	else
		pv = NULL;

	pteproto |= atop(pa) & PG_PFNUM;
	if (prot & PROT_WRITE)
		pteproto |= PG_W;

	ctx = getcontext4();
	if (pm == pmap_kernel())
		ret = pmap_enk4_4c(pm, va, prot, flags, pv, pteproto | PG_S);
	else
		ret = pmap_enu4_4c(pm, va, prot, flags, pv, pteproto);
#ifdef DIAGNOSTIC
	if ((flags & PMAP_CANFAIL) == 0 && ret != 0)
		panic("pmap_enter4_4c: can't fail, but did");
#endif
	setcontext4(ctx);

	return (ret);
}

/* enter new (or change existing) kernel mapping */
int
pmap_enk4_4c(pm, va, prot, flags, pv, pteproto)
	struct pmap *pm;
	vaddr_t va;
	vm_prot_t prot;
	int flags;
	struct pvlist *pv;
	int pteproto;
{
	int vr, vs, tpte, i, s;
	struct regmap *rp;
	struct segmap *sp;
	int wired = (flags & PMAP_WIRED) != 0;

	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];
	s = splvm();		/* XXX way too conservative */

#if defined(SUN4_MMU3L)
	if (HASSUN4_MMU3L && rp->rg_smeg == reginval) {
		vaddr_t tva;
		rp->rg_smeg = region_alloc(&region_locked, pm, vr)->me_cookie;
		i = ncontext - 1;
		do {
			setcontext4(i);
			setregmap(va, rp->rg_smeg);
		} while (--i >= 0);

		/* set all PTEs to invalid, then overwrite one PTE below */
		tva = VA_ROUNDDOWNTOREG(va);
		for (i = 0; i < NSEGRG; i++) {
			setsegmap(tva, rp->rg_segmap[i].sg_pmeg);
			tva += NBPSG;
		};
	}
#endif
	if (sp->sg_pmeg != seginval && (tpte = getpte4(va)) & PG_V) {
		/* old mapping exists, and is of the same pa type */
		if ((tpte & (PG_PFNUM|PG_TYPE)) ==
		    (pteproto & (PG_PFNUM|PG_TYPE))) {
			/* just changing protection and/or wiring */
			splx(s);
			pmap_changeprot4_4c(pm, va, prot, wired);
			return (0);
		}

		if ((tpte & PG_TYPE) == PG_OBMEM) {
			struct pvlist *pv1;

			/*
			 * Switcheroo: changing pa for this va.
			 * If old pa was managed, remove from pvlist.
			 * If old page was cached, flush cache.
			 */
			pv1 = pvhead(tpte & PG_PFNUM);
			if (pv1)
				pv_unlink4_4c(pv1, pm, va);
			if ((tpte & PG_NC) == 0) {
				setcontext4(0);	/* ??? */
				cache_flush_page((int)va);
			}
		}
		pm->pm_stats.resident_count--;
	} else {
		/* adding new entry */
		sp->sg_npte++;
	}

	/*
	 * If the new mapping is for a managed PA, enter into pvlist.
	 * Note that the mapping for a malloc page will always be
	 * unique (hence will never cause a second call to malloc).
	 */
	if (pv != NULL)
		pteproto |= pv_link4_4c(pv, pm, va, pteproto & PG_NC);

	if (sp->sg_pmeg == seginval) {
		int tva;

		/*
		 * Allocate an MMU entry now (on locked list),
		 * and map it into every context.  Set all its
		 * PTEs invalid (we will then overwrite one, but
		 * this is more efficient than looping twice).
		 */
#ifdef DEBUG
		if (pm->pm_ctx == NULL || pm->pm_ctxnum != 0)
			panic("pmap_enk: kern seg but no kern ctx");
#endif
		sp->sg_pmeg = me_alloc(&segm_locked, pm, vr, vs)->me_cookie;
		rp->rg_nsegmap++;

#if defined(SUN4_MMU3L)
		if (HASSUN4_MMU3L)
			setsegmap(va, sp->sg_pmeg);
		else
#endif
		{
			i = ncontext - 1;
			do {
				setcontext4(i);
				setsegmap(va, sp->sg_pmeg);
			} while (--i >= 0);
		}

		/* set all PTEs to invalid, then overwrite one PTE below */
		tva = VA_ROUNDDOWNTOSEG(va);
		i = NPTESG;
		do {
			setpte4(tva, 0);
			tva += NBPG;
		} while (--i > 0);
	}

	/* ptes kept in hardware only */
	setpte4(va, pteproto);
	pm->pm_stats.resident_count++;
	splx(s);

	return (0);
}

/* enter new (or change existing) user mapping */
int
pmap_enu4_4c(pm, va, prot, flags, pv, pteproto)
	struct pmap *pm;
	vaddr_t va;
	vm_prot_t prot;
	int flags;
	struct pvlist *pv;
	int pteproto;
{
	int vr, vs, *pte, tpte, pmeg, s, doflush;
	struct regmap *rp;
	struct segmap *sp;
	int wired = (flags & PMAP_WIRED) != 0;

	write_user_windows();		/* XXX conservative */
	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	s = splvm();			/* XXX conservative */

	/*
	 * If there is no space in which the PTEs can be written
	 * while they are not in the hardware, this must be a new
	 * virtual segment.  Get PTE space and count the segment.
	 *
	 * TO SPEED UP CTX ALLOC, PUT SEGMENT BOUNDS STUFF HERE
	 * AND IN pmap_rmu()
	 */

	GAP_SHRINK(pm,vr);

#ifdef DEBUG
	if (pm->pm_gap_end < pm->pm_gap_start) {
		printf("pmap_enu: gap_start 0x%x, gap_end 0x%x",
			pm->pm_gap_start, pm->pm_gap_end);
		panic("pmap_enu: gap botch");
	}
#endif

	if (rp->rg_segmap == NULL) {
		/* definitely a new mapping */
		int i;
		int size = NSEGRG * sizeof (struct segmap);

		sp = malloc((u_long)size, M_VMPMAP, M_NOWAIT);
		if (sp == NULL) {
			splx(s);
			return (ENOMEM);
		}
		qzero((caddr_t)sp, size);
		rp->rg_segmap = sp;
		rp->rg_nsegmap = 0;
		for (i = NSEGRG; --i >= 0;)
			sp++->sg_pmeg = seginval;
	}

	sp = &rp->rg_segmap[vs];
	if ((pte = sp->sg_pte) == NULL) {
		/* definitely a new mapping */
		int size = NPTESG * sizeof *pte;

		pte = malloc((u_long)size, M_VMPMAP, M_NOWAIT);
		if (pte == NULL) {
			splx(s);
			return (ENOMEM);
		}
#ifdef DEBUG
		if (sp->sg_pmeg != seginval)
			panic("pmap_enter: new ptes, but not seginval");
#endif
		qzero((caddr_t)pte, size);
		sp->sg_pte = pte;
		sp->sg_npte = 1;
		rp->rg_nsegmap++;
	} else {
		/* might be a change: fetch old pte */
		doflush = 0;
		if ((pmeg = sp->sg_pmeg) == seginval) {
			/* software pte */
			tpte = pte[VA_VPG(va)];
		} else {
			/* hardware pte */
			if (CTX_USABLE(pm,rp)) {
				setcontext4(pm->pm_ctxnum);
				tpte = getpte4(va);
				doflush = CACHEINFO.c_vactype != VAC_NONE;
			} else {
				setcontext4(0);
				/* XXX use per-cpu pteva? */
				if (HASSUN4_MMU3L)
					setregmap(0, tregion);
				setsegmap(0, pmeg);
				tpte = getpte4(VA_VPG(va) << PGSHIFT);
			}
		}
		if (tpte & PG_V) {
			/* old mapping exists, and is of the same pa type */
			if ((tpte & (PG_PFNUM|PG_TYPE)) ==
			    (pteproto & (PG_PFNUM|PG_TYPE))) {
				/* just changing prot and/or wiring */
				splx(s);
				/* caller should call this directly: */
				pmap_changeprot4_4c(pm, va, prot, wired);
				if (wired)
					pm->pm_stats.wired_count++;
				else
					pm->pm_stats.wired_count--;
				return (0);
			}
			/*
			 * Switcheroo: changing pa for this va.
			 * If old pa was managed, remove from pvlist.
			 * If old page was cached, flush cache.
			 */
			if ((tpte & PG_TYPE) == PG_OBMEM) {
				struct pvlist *pv1;

				pv1 = pvhead(tpte & PG_PFNUM);
				if (pv1)
					pv_unlink4_4c(pv1, pm, va);
				if (doflush && (tpte & PG_NC) == 0)
					cache_flush_page((int)va);
			}
			pm->pm_stats.resident_count--;
		} else {
			/* adding new entry */
			sp->sg_npte++;

			/*
			 * Increment counters
			 */
			if (wired)
				pm->pm_stats.wired_count++;
		}
	}

	if (pv != NULL)
		pteproto |= pv_link4_4c(pv, pm, va, pteproto & PG_NC);

	/*
	 * Update hardware & software PTEs.
	 */
	if ((pmeg = sp->sg_pmeg) != seginval) {
		/* ptes are in hardware */
		if (CTX_USABLE(pm,rp))
			setcontext4(pm->pm_ctxnum);
		else {
			setcontext4(0);
			/* XXX use per-cpu pteva? */
			if (HASSUN4_MMU3L)
				setregmap(0, tregion);
			setsegmap(0, pmeg);
			va = VA_VPG(va) << PGSHIFT;
		}
		setpte4(va, pteproto);
	}
	/* update software copy */
	pte += VA_VPG(va);
	*pte = pteproto;
	pm->pm_stats.resident_count++;

	splx(s);

	return (0);
}

void
pmap_kenter_pa4_4c(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	struct pvlist *pv;
	int pteproto, ctx;

	pteproto = PG_S | PG_V | PMAP_T2PTE_4(pa);
	if (prot & PROT_WRITE)
		pteproto |= PG_W;

	pa &= ~PMAP_TNC_4;

	if ((pteproto & PG_TYPE) == PG_OBMEM)
		pv = pvhead(atop(pa));
	else
		pv = NULL;

	pteproto |= atop(pa) & PG_PFNUM;

	ctx = getcontext4();
	pmap_enk4_4c(pmap_kernel(), va, prot, PMAP_WIRED, pv, pteproto);
	setcontext4(ctx);
}

#endif /*sun4,4c*/

#if defined(SUN4M)		/* Sun4M versions of enter routines */
/*
 * Insert (MI) physical page pa at virtual address va in the given pmap.
 * NB: the pa parameter includes type bits PMAP_OBIO, PMAP_NC as necessary.
 *
 * If pa is not in the `managed' range it will not be `bank mapped'.
 * This works during bootstrap only because the first 4MB happens to
 * map one-to-one.
 *
 * There may already be something else there, or we might just be
 * changing protections and/or wiring on an existing mapping.
 */

int
pmap_enter4m(pm, va, pa, prot, flags)
	struct pmap *pm;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	struct pvlist *pv;
	int pteproto, ctx;
	int ret;

#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter(%p, 0x%lx, 0x%lx, 0x%x, 0x%x)\n",
		    pm, va, pa, prot, flags);
#endif

	/* Initialise pteproto with cache bit */
	pteproto = (pa & PMAP_NC) == 0 ? SRMMU_PG_C : 0;

#ifdef DEBUG
	if (pa & PMAP_TYPE_SRMMU) {	/* this page goes in an iospace */
		if (cpuinfo.cpu_type == CPUTYP_MS1)
			panic("pmap_enter4m: attempt to use 36-bit iospace on"
			      " MicroSPARC");
	}
#endif
	pteproto |= PMAP_T2PTE_SRMMU(pa);

	pteproto |= SRMMU_TEPTE;

	pa &= ~PMAP_TNC_SRMMU;
	/*
	 * Set up prototype for new PTE.  Cannot set PG_NC from PV_NC yet
	 * since the pvlist no-cache bit might change as a result of the
	 * new mapping.
	 */
	if ((pteproto & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
		pv = pvhead(atop(pa));
	else
		pv = NULL;

	pteproto |= (atop(pa) << SRMMU_PPNSHIFT);

	/* correct protections */
	pteproto |= pte_prot4m(pm, prot);

	ctx = getcontext4m();
	if (pm == pmap_kernel())
		ret = pmap_enk4m(pm, va, prot, flags, pv, pteproto);
	else
		ret = pmap_enu4m(pm, va, prot, flags, pv, pteproto);
#ifdef DIAGNOSTIC
	if ((flags & PMAP_CANFAIL) == 0 && ret != 0)
		panic("pmap_enter4m: can't fail, but did");
#endif
	if (pv) {
		if (flags & PROT_WRITE)
			pv->pv_flags |= PV_MOD4M;
		if (flags & PROT_READ)
			pv->pv_flags |= PV_REF4M;
	}
	setcontext4m(ctx);

	return (ret);
}

/* enter new (or change existing) kernel mapping */
int
pmap_enk4m(pm, va, prot, flags, pv, pteproto)
	struct pmap *pm;
	vaddr_t va;
	vm_prot_t prot;
	int flags;
	struct pvlist *pv;
	int pteproto;
{
	int tpte, s;
	struct regmap *rp;
	struct segmap *sp;
	int wired = (flags & PMAP_WIRED) != 0;

#ifdef DIAGNOSTIC
	if (VA_VREG(va) < NUREG_4M)
		panic("pmap_enk4m: can't enter va 0x%lx below VM_MIN_KERNEL_ADDRESS", va);
#endif
	rp = &pm->pm_regmap[VA_VREG(va)];
	sp = &rp->rg_segmap[VA_VSEG(va)];

	s = splvm();		/* XXX way too conservative */

#ifdef DEBUG
	if (rp->rg_seg_ptps == NULL) /* enter new region */
		panic("pmap_enk4m: missing region table for va 0x%lx", va);
	if (sp->sg_pte == NULL) /* If no existing pagetable */
		panic("pmap_enk4m: missing segment table for va 0x%lx", va);
#endif

	tpte = sp->sg_pte[VA_SUN4M_VPG(va)];
	if ((tpte & SRMMU_TETYPE) == SRMMU_TEPTE) {
		/* old mapping exists, and is of the same pa type */

		if ((tpte & SRMMU_PPNMASK) == (pteproto & SRMMU_PPNMASK)) {
			/* just changing protection and/or wiring */
			splx(s);
			pmap_changeprot4m(pm, va, prot, wired);
			return (0);
		}

		if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
			struct pvlist *pv1;

			/*
			 * Switcheroo: changing pa for this va.
			 * If old pa was managed, remove from pvlist.
			 * If old page was cached, flush cache.
			 */
			pv1 = pvhead((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (pv1)
				pv_unlink4m(pv1, pm, va);
			if (tpte & SRMMU_PG_C) {
				setcontext4m(0);	/* ??? */
				cache_flush_page((int)va);
			}
		}
		pm->pm_stats.resident_count--;
	} else {
		/* adding new entry */
		sp->sg_npte++;
	}

	/*
	 * If the new mapping is for a managed PA, enter into pvlist.
	 * Note that the mapping for a malloc page will always be
	 * unique (hence will never cause a second call to malloc).
	 */
	if (pv != NULL)
	        pteproto &= ~(pv_link4m(pv, pm, va, (pteproto & SRMMU_PG_C) == 0));

	tlb_flush_page(va);
	setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], pteproto);
	pm->pm_stats.resident_count++;

	splx(s);

	return (0);
}

/* enter new (or change existing) user mapping */
int
pmap_enu4m(pm, va, prot, flags, pv, pteproto)
	struct pmap *pm;
	vaddr_t va;
	vm_prot_t prot;
	int flags;
	struct pvlist *pv;
	int pteproto;
{
	int vr, vs, *pte, tpte, s;
	struct regmap *rp;
	struct segmap *sp;
	int wired = (flags & PMAP_WIRED) != 0;

#ifdef DEBUG
	if (VA_VREG(va) >= NUREG_4M)
		panic("pmap_enu4m: can't enter va 0x%lx above VM_MIN_KERNEL_ADDRESS", va);
#endif

	write_user_windows();		/* XXX conservative */
	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	s = splvm();			/* XXX conservative */
	if (rp->rg_segmap == NULL) {
		/* definitely a new mapping */
		int size = NSEGRG * sizeof (struct segmap);

		sp = malloc((u_long)size, M_VMPMAP, M_NOWAIT);
		if (sp == NULL) {
			splx(s);
			return (ENOMEM);
		}
		qzero((caddr_t)sp, size);
		rp->rg_segmap = sp;
		rp->rg_nsegmap = 0;
		rp->rg_seg_ptps = NULL;
	}

	if (rp->rg_seg_ptps == NULL) {
		/* Need a segment table */
		int i, *ptd;

		ptd = pool_get(&L23_pool, PR_NOWAIT);
		if (ptd == NULL) {
			splx(s);
			return (ENOMEM);
		}

		rp->rg_seg_ptps = ptd;
		for (i = 0; i < SRMMU_L2SIZE; i++)
			setpgt4m(&ptd[i], SRMMU_TEINVALID);
		setpgt4m(&pm->pm_reg_ptps[vr],
			 (VA2PA((caddr_t)ptd) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD);
	}

	sp = &rp->rg_segmap[vs];
	if ((pte = sp->sg_pte) == NULL) {
		/* definitely a new mapping */
		int i;

		pte = pool_get(&L23_pool, PR_NOWAIT);
		if (pte == NULL) {
			splx(s);
			return (ENOMEM);
		}

		sp->sg_pte = pte;
		sp->sg_npte = 1;
		rp->rg_nsegmap++;
		for (i = 0; i < SRMMU_L3SIZE; i++)
			setpgt4m(&pte[i], SRMMU_TEINVALID);
		setpgt4m(&rp->rg_seg_ptps[vs],
			(VA2PA((caddr_t)pte) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD);
	} else {
		/*
		 * Might be a change: fetch old pte
		 * Note we're only interested in the PTE's page frame
		 * number and type bits, so the memory copy will do.
		 */
		tpte = pte[VA_SUN4M_VPG(va)];

		if ((tpte & SRMMU_TETYPE) == SRMMU_TEPTE) {
			/* old mapping exists, and is of the same pa type */
			if ((tpte & SRMMU_PPNMASK) ==
			    (pteproto & SRMMU_PPNMASK)) {
				/* just changing prot and/or wiring */
				splx(s);
				/* caller should call this directly: */
				pmap_changeprot4m(pm, va, prot, wired);
				if (wired)
					pm->pm_stats.wired_count++;
				else
					pm->pm_stats.wired_count--;
				return (0);
			}
			/*
			 * Switcheroo: changing pa for this va.
			 * If old pa was managed, remove from pvlist.
			 * If old page was cached, flush cache.
			 */
			if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
				struct pvlist *pv1;

				pv1 = pvhead((tpte & SRMMU_PPNMASK) >>
					     SRMMU_PPNSHIFT);
				if (pv1)
					pv_unlink4m(pv1, pm, va);
				if (pm->pm_ctx && (tpte & SRMMU_PG_C))
					cache_flush_page((int)va);
			}
			pm->pm_stats.resident_count--;
		} else {
			/* adding new entry */
			sp->sg_npte++;

			/*
			 * Increment counters
			 */
			if (wired)
				pm->pm_stats.wired_count++;
		}
	}
	if (pv != NULL)
	        pteproto &= ~(pv_link4m(pv, pm, va, (pteproto & SRMMU_PG_C) == 0));

	/*
	 * Update PTEs, flush TLB as necessary.
	 */
	if (pm->pm_ctx) {
		setcontext4m(pm->pm_ctxnum);
		tlb_flush_page(va);
	}
	setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], pteproto);
	pm->pm_stats.resident_count++;

	splx(s);

	return (0);
}

void
pmap_kenter_pa4m(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	struct pvlist *pv;
	int pteproto, ctx;

	pteproto = ((pa & PMAP_NC) == 0 ? SRMMU_PG_C : 0) |
		PMAP_T2PTE_SRMMU(pa) | SRMMU_TEPTE |
		((prot & PROT_WRITE) ? PPROT_N_RWX : PPROT_N_RX);

	pa &= ~PMAP_TNC_SRMMU;

	pteproto |= atop(pa) << SRMMU_PPNSHIFT;

	pv = pvhead(atop(pa));

	ctx = getcontext4m();
	pmap_enk4m(pmap_kernel(), va, prot, PMAP_WIRED, pv, pteproto);
	setcontext4m(ctx);
}

#endif /* sun4m */

/*
 * Change the wiring attribute for a map/virtual-address pair.
 */
/* ARGSUSED */
void
pmap_unwire(pm, va)
	struct pmap *pm;
	vaddr_t va;
{

	pmap_stats.ps_useless_changewire++;
}

/*
 * Extract the physical page address associated
 * with the given map/virtual_address pair.
 * GRR, the vm code knows; we should not have to do this!
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
boolean_t
pmap_extract4_4c(pm, va, pa)
	struct pmap *pm;
	vaddr_t va;
	paddr_t *pa;
{
	int tpte;
	int vr, vs;
	struct regmap *rp;
	struct segmap *sp;

	if (pm == NULL) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: null pmap\n");
#endif
		return (FALSE);
	}
	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	if (rp->rg_segmap == NULL) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: invalid segment (%d)\n", vr);
#endif
		return (FALSE);
	}
	sp = &rp->rg_segmap[vs];

	if (sp->sg_pmeg != seginval) {
		int ctx = getcontext4();

		if (CTX_USABLE(pm,rp)) {
			CHANGE_CONTEXTS(ctx, pm->pm_ctxnum);
			tpte = getpte4(va);
		} else {
			CHANGE_CONTEXTS(ctx, 0);
			if (HASSUN4_MMU3L)
				setregmap(0, tregion);
			setsegmap(0, sp->sg_pmeg);
			tpte = getpte4(VA_VPG(va) << PGSHIFT);
		}
		setcontext4(ctx);
	} else {
		int *pte = sp->sg_pte;

		if (pte == NULL) {
#ifdef DEBUG
			if (pmapdebug & PDB_FOLLOW)
				printf("pmap_extract: invalid segment\n");
#endif
			return (FALSE);
		}
		tpte = pte[VA_VPG(va)];
	}
	if ((tpte & PG_V) == 0) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: invalid pte\n");
#endif
		return (FALSE);
	}
	tpte &= PG_PFNUM;
	tpte = tpte;
	*pa = ((tpte << PGSHIFT) | (va & PGOFSET));
	return (TRUE);
}
#endif /*4,4c*/

#if defined(SUN4M)		/* 4m version of pmap_extract */
/*
 * Extract the physical page address associated
 * with the given map/virtual_address pair.
 * GRR, the vm code knows; we should not have to do this!
 */
boolean_t
pmap_extract4m(pm, va, pa)
	struct pmap *pm;
	vaddr_t va;
	paddr_t *pa;
{
	struct regmap *rm;
	struct segmap *sm;
	int pte;

	if (pm == NULL) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: null pmap\n");
#endif
		return (FALSE);
	}

	if ((rm = pm->pm_regmap) == NULL) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: no regmap entry");
#endif
		return (FALSE);
	}

	rm += VA_VREG(va);
	if ((sm = rm->rg_segmap) == NULL) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: no segmap");
#endif
		return (FALSE);
	}

	sm += VA_VSEG(va);
	if (sm->sg_pte == NULL) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			panic("pmap_extract: no ptes");
#endif
		return FALSE;
	}

	pte = sm->sg_pte[VA_SUN4M_VPG(va)];
	if ((pte & SRMMU_TETYPE) != SRMMU_TEPTE) {
#ifdef DEBUG
		if (pmapdebug & PDB_FOLLOW)
			printf("pmap_extract: invalid pte of type %d\n",
			       pte & SRMMU_TETYPE);
#endif
		return (FALSE);
	}

	*pa = (ptoa((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT) | VA_OFF(va));
	return (TRUE);
}
#endif /* sun4m */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)

/*
 * Clear the modify bit for the given physical page.
 */
boolean_t
pmap_clear_modify4_4c(struct vm_page *pg)
{
	struct pvlist *pv;
	boolean_t ret;

	pv = &pg->mdpage.pv_head;	

	(void) pv_syncflags4_4c(pv);
	ret = pv->pv_flags & PV_MOD;
	pv->pv_flags &= ~PV_MOD;

	return ret;
}

/*
 * Tell whether the given physical page has been modified.
 */
boolean_t
pmap_is_modified4_4c(struct vm_page *pg)
{
	struct pvlist *pv;

	pv = &pg->mdpage.pv_head;

	return (pv->pv_flags & PV_MOD || pv_syncflags4_4c(pv) & PV_MOD);
}

/*
 * Clear the reference bit for the given physical page.
 */
boolean_t
pmap_clear_reference4_4c(struct vm_page *pg)
{
	struct pvlist *pv;
	boolean_t ret;

	pv = &pg->mdpage.pv_head;

	(void) pv_syncflags4_4c(pv);
	ret = pv->pv_flags & PV_REF;
	pv->pv_flags &= ~PV_REF;

	return ret;
}

/*
 * Tell whether the given physical page has been referenced.
 */
boolean_t
pmap_is_referenced4_4c(struct vm_page *pg)
{
	struct pvlist *pv;

	pv = &pg->mdpage.pv_head;

	return (pv->pv_flags & PV_REF || pv_syncflags4_4c(pv) & PV_REF);
}
#endif /*4,4c*/

#if defined(SUN4M)

/*
 * 4m versions of bit test/set routines
 *
 * Note that the 4m-specific routines should eventually service these
 * requests from their page tables, and the whole pvlist bit mess should
 * be dropped for the 4m (unless this causes a performance hit from
 * tracing down pagetables/regmap/segmaps).
 */

/*
 * Clear the modify bit for the given physical page.
 */
boolean_t
pmap_clear_modify4m(struct vm_page *pg)
{
	struct pvlist *pv;
	boolean_t ret;

	pv = &pg->mdpage.pv_head;

	(void) pv_syncflags4m(pv);
	ret = pv->pv_flags & PV_MOD4M;
	pv->pv_flags &= ~PV_MOD4M;

	return ret;
}

/*
 * Tell whether the given physical page has been modified.
 */
boolean_t
pmap_is_modified4m(struct vm_page *pg)
{
	struct pvlist *pv;

	pv = &pg->mdpage.pv_head;

	return (pv->pv_flags & PV_MOD4M || pv_syncflags4m(pv) & PV_MOD4M);
}

/*
 * Clear the reference bit for the given physical page.
 */
boolean_t
pmap_clear_reference4m(struct vm_page *pg)
{
	struct pvlist *pv;
	boolean_t ret;

	pv = &pg->mdpage.pv_head;

	(void) pv_syncflags4m(pv);
	ret = pv->pv_flags & PV_REF4M;
	pv->pv_flags &= ~PV_REF4M;

	return ret;
}

/*
 * Tell whether the given physical page has been referenced.
 */
boolean_t
pmap_is_referenced4m(struct vm_page *pg)
{
	struct pvlist *pv;

	pv = &pg->mdpage.pv_head;

	return (pv->pv_flags & PV_REF4M || pv_syncflags4m(pv) & PV_REF4M);
}
#endif /* 4m */

/*
 * Fill the given MI physical page with zero bytes.
 *
 * We avoid stomping on the cache.
 * XXX	might be faster to use destination's context and allow cache to fill?
 */

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)

void
pmap_zero_page4_4c(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	caddr_t va;
	int pte;

	/*
	 * The following might not be necessary since the page
	 * is being cleared because it is about to be allocated,
	 * i.e., is in use by no one.
	 */
	pg_flushcache(pg);

	pte = PG_V | PG_S | PG_W | PG_NC | (atop(pa) & PG_PFNUM);

	va = vpage[0];
	setpte4(va, pte);
	qzero(va, NBPG);
	setpte4(va, 0);
}

/*
 * Copy the given MI physical source page to its destination.
 *
 * We avoid stomping on the cache as above (with same `XXX' note).
 * We must first flush any write-back cache for the source page.
 * We go ahead and stomp on the kernel's virtual cache for the
 * source page, since the cache can read memory MUCH faster than
 * the processor.
 */
void
pmap_copy_page4_4c(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
	caddr_t sva, dva;
	int spte, dpte;

	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pg_flushcache(srcpg);

	spte = PG_V | PG_S | (atop(src) & PG_PFNUM);

	if (CACHEINFO.c_vactype != VAC_NONE)
		pg_flushcache(dstpg);

	dpte = PG_V | PG_S | PG_W | PG_NC | (atop(dst) & PG_PFNUM);

	sva = vpage[0];
	dva = vpage[1];
	setpte4(sva, spte);
	setpte4(dva, dpte);
	qcopy(sva, dva, NBPG);	/* loads cache, so we must ... */
	cache_flush_page((int)sva);
	setpte4(sva, 0);
	setpte4(dva, 0);
}
#endif /* 4, 4c */

#if defined(SUN4M)		/* Sun4M version of copy/zero routines */
/*
 * Fill the given MI physical page with zero bytes.
 *
 * We avoid stomping on the cache.
 * XXX	might be faster to use destination's context and allow cache to fill?
 */
void
pmap_zero_page4m(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	static vaddr_t va;
	static int *ptep;
	int pte;

	if (ptep == NULL)
		ptep = getptep4m(pmap_kernel(), (va = (vaddr_t)vpage[0]));

	if (CACHEINFO.c_vactype != VAC_NONE) {
		/*
		 * The following might not be necessary since the page
		 * is being cleared because it is about to be allocated,
		 * i.e., is in use by no one.
		 */
		pg_flushcache(pg);
	}

	pte = (SRMMU_TEPTE | (atop(pa) << SRMMU_PPNSHIFT) | PPROT_N_RWX);

	if (cpuinfo.flags & CPUFLG_CACHE_MANDATORY)
		pte |= SRMMU_PG_C;
	else
		pte &= ~SRMMU_PG_C;

	tlb_flush_page(va);
	setpgt4m(ptep, pte);
	qzero((caddr_t)va, PAGE_SIZE);
	tlb_flush_page(va);
	setpgt4m(ptep, SRMMU_TEINVALID);
}

/*
 * Copy the given MI physical source page to its destination.
 *
 * We avoid stomping on the cache as above (with same `XXX' note).
 * We must first flush any write-back cache for the source page.
 * We go ahead and stomp on the kernel's virtual cache for the
 * source page, since the cache can read memory MUCH faster than
 * the processor.
 */
void
pmap_copy_page4m(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
	static int *sptep, *dptep;
	static vaddr_t sva, dva;
	int spte, dpte;

	if (sptep == NULL) {
		sptep = getptep4m(pmap_kernel(), (sva = (vaddr_t)vpage[0]));
		dptep = getptep4m(pmap_kernel(), (dva = (vaddr_t)vpage[1]));
	}

	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pg_flushcache(srcpg);

	spte = SRMMU_TEPTE | SRMMU_PG_C | (atop(src) << SRMMU_PPNSHIFT) |
	    PPROT_N_RX;

	if (CACHEINFO.c_vactype != VAC_NONE)
		pg_flushcache(dstpg);

	dpte = (SRMMU_TEPTE | (atop(dst) << SRMMU_PPNSHIFT) | PPROT_N_RWX);
	if (cpuinfo.flags & CPUFLG_CACHE_MANDATORY)
		dpte |= SRMMU_PG_C;
	else
		dpte &= ~SRMMU_PG_C;

	tlb_flush_page(sva);
	setpgt4m(sptep, spte);
	tlb_flush_page(dva);
	setpgt4m(dptep, dpte);
	qcopy((caddr_t)sva, (caddr_t)dva, PAGE_SIZE);
	cache_flush_page((int)sva);
	tlb_flush_page(sva);
	setpgt4m(sptep, SRMMU_TEINVALID);
	tlb_flush_page(dva);
	setpgt4m(dptep, SRMMU_TEINVALID);
}
#endif /* Sun4M */

/*
 * Turn on/off cache for a given (va, number of pages).
 *
 * We just assert PG_NC for each PTE; the addresses must reside
 * in locked kernel space.  A cache flush is also done.
 */
void
kvm_setcache(va, npages, cached)
	caddr_t va;
	int npages;
	int cached;
{
	int pte, ctx;
	struct pvlist *pv;

	if (CPU_ISSUN4M) {
#if defined(SUN4M)
		ctx = getcontext4m();
		setcontext4m(0);
		for (; --npages >= 0; va += NBPG) {
			int *ptep;

			ptep = getptep4m(pmap_kernel(), (vaddr_t)va);
			pte = *ptep;
#ifdef DIAGNOSTIC
			if ((pte & SRMMU_TETYPE) != SRMMU_TEPTE)
				panic("kvm_uncache: table entry not pte");
#endif
			pv = pvhead((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (pv) {
				if (cached)
					pv_changepte4m(pv, SRMMU_PG_C, 0);
				else
					pv_changepte4m(pv, 0, SRMMU_PG_C);
			}
			if (cached)
				pte |= SRMMU_PG_C;
			else
				pte &= ~SRMMU_PG_C;
			tlb_flush_page((vaddr_t)va);
			setpgt4m(ptep, pte);

			if ((pte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
				cache_flush_page((int)va);

		}
		setcontext4m(ctx);

#endif
	} else {
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
		ctx = getcontext4();
		setcontext4(0);
		for (; --npages >= 0; va += NBPG) {
			pte = getpte4(va);
			if ((pte & PG_V) == 0)
				panic("kvm_uncache !pg_v");

			pv = pvhead(pte & PG_PFNUM);
			/* XXX - we probably don't need to check for OBMEM */
			if ((pte & PG_TYPE) == PG_OBMEM && pv) {
				if (cached)
					pv_changepte4_4c(pv, 0, PG_NC);
				else
					pv_changepte4_4c(pv, PG_NC, 0);
			}
			if (cached)
				pte &= ~PG_NC;
			else
				pte |= PG_NC;
			setpte4(va, pte);

			if ((pte & PG_TYPE) == PG_OBMEM)
				cache_flush_page((int)va);
		}
		setcontext4(ctx);
#endif
	}
}

/*
 * Find first virtual address >= *va that is
 * least likely to cause cache aliases.
 * (This will just seg-align mappings.)
 */
vaddr_t
pmap_prefer(vaddr_t foff, vaddr_t va)
{
	vaddr_t d, m;

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
	if (VA_INHOLE(va))
		va = MMU_HOLE_END;
#endif

	m = CACHE_ALIAS_DIST;
	if (m != 0) {		/* m=0 => no cache aliasing */
		d = foff - va;
		d &= (m - 1);
		va += d;
	}

	return va;
}

void
pmap_remove_holes(struct vmspace *vm)
{
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
	if (mmu_has_hole) {
		struct vm_map *map = &vm->vm_map;
		vaddr_t shole, ehole;

		shole = max(vm_map_min(map), (vaddr_t)MMU_HOLE_START);
		ehole = min(vm_map_max(map), (vaddr_t)MMU_HOLE_END);

		if (ehole <= shole)
			return;

		(void)uvm_map(map, &shole, ehole - shole, NULL,
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(PROT_NONE, PROT_NONE, MAP_INHERIT_SHARE,
		      MADV_RANDOM,
		      UVM_FLAG_NOMERGE | UVM_FLAG_HOLE | UVM_FLAG_FIXED));
	}
#endif
}

void
pmap_redzone()
{
#if defined(SUN4M)
	if (CPU_ISSUN4M) {
		setpte4m(KERNBASE, 0);
		return;
	}
#endif
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
	if (CPU_ISSUN4OR4COR4E) {
		setpte4(KERNBASE, 0);
		return;
	}
#endif
}

/*
 * Activate the address space for the specified process.  If the
 * process is the current process, load the new MMU context.
 */
void
pmap_activate(p)
	struct proc *p;
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	int s;

	/*
	 * This is essentially the same thing that happens in cpu_switch()
	 * when the newly selected process is about to run, except that we
	 * have to make sure to clean the windows before we set
	 * the new context.
	 */

	s = splvm();
	if (p == curproc) {
		write_user_windows();
		if (pmap->pm_ctx == NULL) {
			ctx_alloc(pmap);	/* performs setcontext() */
		} else {
			/* Do any cache flush needed on context switch */
			(*cpuinfo.pure_vcache_flush)();
			setcontext(pmap->pm_ctxnum);
		}
	}
	splx(s);
}

#ifdef DEBUG
/*
 * Check consistency of a pmap (time consuming!).
 */
void
pm_check(s, pm)
	char *s;
	struct pmap *pm;
{
	if (pm == pmap_kernel())
		pm_check_k(s, pm);
	else
		pm_check_u(s, pm);
}

void
pm_check_u(s, pm)
	char *s;
	struct pmap *pm;
{
	struct regmap *rp;
	struct segmap *sp;
	int n, vs, vr, j, m, *pte;

	if (pm->pm_regmap == NULL)
		panic("%s: CHK(pmap %p): no region mapping", s, pm);

#if defined(SUN4M)
	if (CPU_ISSUN4M &&
	    (pm->pm_reg_ptps == NULL ||
	     pm->pm_reg_ptps_pa != VA2PA((caddr_t)pm->pm_reg_ptps)))
		panic("%s: CHK(pmap %p): no SRMMU region table or bad pa: "
		      "tblva=%p, tblpa=0x%x",
			s, pm, pm->pm_reg_ptps, pm->pm_reg_ptps_pa);

	if (CPU_ISSUN4M && pm->pm_ctx != NULL &&
	    (cpuinfo.ctx_tbl[pm->pm_ctxnum] != ((VA2PA((caddr_t)pm->pm_reg_ptps)
					      >> SRMMU_PPNPASHIFT) |
					     SRMMU_TEPTD)))
	    panic("%s: CHK(pmap %p): SRMMU region table at 0x%x not installed "
		  "for context %d", s, pm, pm->pm_reg_ptps_pa, pm->pm_ctxnum);
#endif

	for (vr = 0; vr < NUREG; vr++) {
		rp = &pm->pm_regmap[vr];
		if (rp->rg_nsegmap == 0)
			continue;
		if (rp->rg_segmap == NULL)
			panic("%s: CHK(vr %d): nsegmap = %d; sp==NULL",
				s, vr, rp->rg_nsegmap);
#if defined(SUN4M)
		if (CPU_ISSUN4M && rp->rg_seg_ptps == NULL)
		    panic("%s: CHK(vr %d): nsegmap=%d; no SRMMU segment table",
			  s, vr, rp->rg_nsegmap);
		if (CPU_ISSUN4M &&
		    pm->pm_reg_ptps[vr] != ((VA2PA((caddr_t)rp->rg_seg_ptps) >>
					    SRMMU_PPNPASHIFT) | SRMMU_TEPTD))
		    panic("%s: CHK(vr %d): SRMMU segtbl not installed",s,vr);
#endif
		if ((unsigned int)rp < vm_min_kernel_address)
			panic("%s: rp=%p", s, rp);
		n = 0;
		for (vs = 0; vs < NSEGRG; vs++) {
			sp = &rp->rg_segmap[vs];
			if ((unsigned int)sp < vm_min_kernel_address)
				panic("%s: sp=%p", s, sp);
			if (sp->sg_npte != 0) {
				n++;
				if (sp->sg_pte == NULL)
					panic("%s: CHK(vr %d, vs %d): npte=%d, "
					   "pte=NULL", s, vr, vs, sp->sg_npte);
#if defined(SUN4M)
				if (CPU_ISSUN4M &&
				    rp->rg_seg_ptps[vs] !=
				     ((VA2PA((caddr_t)sp->sg_pte)
					>> SRMMU_PPNPASHIFT) |
				       SRMMU_TEPTD))
				    panic("%s: CHK(vr %d, vs %d): SRMMU page "
					  "table not installed correctly",s,vr,
					  vs);
#endif
				pte=sp->sg_pte;
				m = 0;
				for (j=0; j<NPTESG; j++,pte++)
				    if ((CPU_ISSUN4M
					 ?((*pte & SRMMU_TETYPE) == SRMMU_TEPTE)
					 :(*pte & PG_V)))
					m++;
				if (m != sp->sg_npte)
				    /*if (pmapdebug & 0x10000)*/
					printf("%s: user CHK(vr %d, vs %d): "
					    "npte(%d) != # valid(%d)\n",
						s, vr, vs, sp->sg_npte, m);
			}
		}
		if (n != rp->rg_nsegmap)
			panic("%s: CHK(vr %d): inconsistent "
				"# of pte's: %d, should be %d",
				s, vr, rp->rg_nsegmap, n);
	}
	return;
}

void
pm_check_k(s, pm)		/* Note: not as extensive as pm_check_u. */
	char *s;
	struct pmap *pm;
{
	struct regmap *rp;
	int vr, vs, n;

	if (pm->pm_regmap == NULL)
	    panic("%s: CHK(pmap %p): no region mapping", s, pm);

#if defined(SUN4M)
	if (CPU_ISSUN4M &&
	    (pm->pm_reg_ptps == NULL ||
	     pm->pm_reg_ptps_pa != VA2PA((caddr_t)pm->pm_reg_ptps)))
	    panic("%s: CHK(pmap %p): no SRMMU region table or bad pa: tblva=%p, tblpa=0x%x",
		  s, pm, pm->pm_reg_ptps, pm->pm_reg_ptps_pa);

	if (CPU_ISSUN4M &&
	    (cpuinfo.ctx_tbl[0] != ((VA2PA((caddr_t)pm->pm_reg_ptps) >>
					     SRMMU_PPNPASHIFT) | SRMMU_TEPTD)))
	    panic("%s: CHK(pmap %p): SRMMU region table at 0x%x not installed "
		  "for context %d", s, pm, pm->pm_reg_ptps_pa, 0);
#endif
	for (vr = NUREG; vr < NUREG+NKREG; vr++) {
		rp = &pm->pm_regmap[vr];
		if (rp->rg_segmap == NULL)
			panic("%s: CHK(vr %d): nsegmap = %d; sp==NULL",
				s, vr, rp->rg_nsegmap);
		if (rp->rg_nsegmap == 0)
			continue;
#if defined(SUN4M)
		if (CPU_ISSUN4M && rp->rg_seg_ptps == NULL)
		    panic("%s: CHK(vr %d): nsegmap=%d; no SRMMU segment table",
			  s, vr, rp->rg_nsegmap);
		if (CPU_ISSUN4M &&
		    pm->pm_reg_ptps[vr] != ((VA2PA((caddr_t)rp->rg_seg_ptps) >>
					    SRMMU_PPNPASHIFT) | SRMMU_TEPTD))
		    panic("%s: CHK(vr %d): SRMMU segtbl not installed",s,vr);
#endif
		if (CPU_ISSUN4M) {
			n = NSEGRG;
		} else {
			for (n = 0, vs = 0; vs < NSEGRG; vs++) {
				if (rp->rg_segmap[vs].sg_npte)
					n++;
			}
		}
		if (n != rp->rg_nsegmap)
			printf("%s: kernel CHK(vr %d): inconsistent "
				"# of pte's: %d, should be %d\n",
				s, vr, rp->rg_nsegmap, n);
	}
	return;
}
#endif

/*
 * Return the number bytes that pmap_dumpmmu() will dump.
 * For each pmeg in the MMU, we'll write NPTESG PTEs.
 * The last page or two contains stuff so libkvm can bootstrap.
 */
int
pmap_dumpsize()
{
	long	sz;

	sz = ALIGN(sizeof(kcore_seg_t)) + ALIGN(sizeof(cpu_kcore_hdr_t));
	sz += npmemarr * sizeof(phys_ram_seg_t);

	if (CPU_ISSUN4OR4COR4E)
		sz += (seginval + 1) * NPTESG * sizeof(int);

	return (atop(sz));
}

/*
 * Write the mmu contents to the dump device.
 * This gets appended to the end of a crash dump since
 * there is no in-core copy of kernel memory mappings on a 4/4c machine.
 */
int
pmap_dumpmmu(dump, blkno)
	daddr_t blkno;
	int (*dump)(dev_t, daddr_t, caddr_t, size_t);
{
	kcore_seg_t	*ksegp;
	cpu_kcore_hdr_t	*kcpup;
	phys_ram_seg_t	memseg;
	int	error = 0;
	int	i, memsegoffset, pmegoffset;
	int		buffer[dbtob(1) / sizeof(int)];
	int		*bp, *ep;
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
	int	pmeg;
#endif

#define EXPEDITE(p,n) do {						\
	int *sp = (int *)(p);						\
	int sz = (n);							\
	while (sz > 0) {						\
		*bp++ = *sp++;						\
		if (bp >= ep) {						\
			error = (*dump)(dumpdev, blkno,			\
					(caddr_t)buffer, dbtob(1));	\
			if (error != 0)					\
				return (error);				\
			++blkno;					\
			bp = buffer;					\
		}							\
		sz -= 4;						\
	}								\
} while (0)

	setcontext(0);

	/* Setup bookkeeping pointers */
	bp = buffer;
	ep = &buffer[sizeof(buffer) / sizeof(buffer[0])];

	/* Fill in MI segment header */
	ksegp = (kcore_seg_t *)bp;
	CORE_SETMAGIC(*ksegp, KCORE_MAGIC, MID_MACHINE, CORE_CPU);
	ksegp->c_size = ptoa(pmap_dumpsize()) - ALIGN(sizeof(kcore_seg_t));

	/* Fill in MD segment header (interpreted by MD part of libkvm) */
	kcpup = (cpu_kcore_hdr_t *)((int)bp + ALIGN(sizeof(kcore_seg_t)));
	kcpup->cputype = cputyp;
	kcpup->nmemseg = npmemarr;
	kcpup->memsegoffset = memsegoffset = ALIGN(sizeof(cpu_kcore_hdr_t));
	kcpup->npmeg = (CPU_ISSUN4OR4COR4E) ? seginval + 1 : 0; 
	kcpup->pmegoffset = pmegoffset =
		memsegoffset + npmemarr * sizeof(phys_ram_seg_t);

	/* Note: we have assumed everything fits in buffer[] so far... */
	bp = (int *)&kcpup->segmap_store;
	EXPEDITE(&kernel_segmap_store, sizeof(kernel_segmap_store));

	/* Align storage for upcoming quad-aligned segment array */
	while (bp != (int *)ALIGN(bp)) {
		int dummy = 0;
		EXPEDITE(&dummy, 4);
	}
	for (i = 0; i < npmemarr; i++) {
		memseg.start = pmemarr[i].addr_lo;
		memseg.size = pmemarr[i].len;
		EXPEDITE(&memseg, sizeof(phys_ram_seg_t));
	}

	if (CPU_ISSUN4M)
		goto out;

#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
	/*
	 * dump page table entries
	 *
	 * We dump each pmeg in order (by segment number).  Since the MMU
	 * automatically maps the given virtual segment to a pmeg we must
	 * iterate over the segments by incrementing an unused segment slot
	 * in the MMU.  This fixed segment number is used in the virtual
	 * address argument to getpte().
	 */

	/*
	 * Go through the pmegs and dump each one.
	 */
	for (pmeg = 0; pmeg <= seginval; ++pmeg) {
		int va = 0;

		setsegmap(va, pmeg);
		i = NPTESG;
		do {
			int pte = getpte4(va);
			EXPEDITE(&pte, sizeof(pte));
			va += NBPG;
		} while (--i > 0);
	}
	setsegmap(0, seginval);
#endif

out:
	if (bp != buffer)
		error = (*dump)(dumpdev, blkno++, (caddr_t)buffer, dbtob(1));

	return (error);
}

/*
 * Helper function for debuggers.
 */
void
pmap_writetext(dst, ch)
	unsigned char *dst;
	int ch;
{
	int s, pte0, pte, ctx;
	vaddr_t va;

	s = splvm();
	va = (unsigned long)dst & (~PGOFSET);
	cpuinfo.cache_flush(dst, 1);

	ctx = getcontext();
	setcontext(0);

#if defined(SUN4M)
	if (CPU_ISSUN4M) {
		int *ptep;

		ptep = getptep4m(pmap_kernel(), va);
		pte0 = *ptep;
		if ((pte0 & SRMMU_TETYPE) != SRMMU_TEPTE) {
			splx(s);
			return;
		}
		pte = pte0 | PPROT_WRITE;
		tlb_flush_page((vaddr_t)va);
		setpgt4m(ptep, pte);
		*dst = (unsigned char)ch;
		tlb_flush_page((vaddr_t)va);
		setpgt4m(ptep, pte0);
	}
#endif
#if defined(SUN4) || defined(SUN4C) || defined(SUN4E)
	if (CPU_ISSUN4OR4COR4E) {
		pte0 = getpte4(va);
		if ((pte0 & PG_V) == 0) {
			splx(s);
			return;
		}
		pte = pte0 | PG_W;
		setpte4(va, pte);
		*dst = (unsigned char)ch;
		setpte4(va, pte0);
	}
#endif
	cpuinfo.cache_flush(dst, 1);
	setcontext(ctx);
	splx(s);
}

#ifdef EXTREME_DEBUG

static void test_region(int, int, int);

void
debug_pagetables()
{
	int i;
	int *regtbl;
	int te;

	printf("\nncontext=%d. ",ncontext);
	printf("Context table is at va 0x%x. Level 0 PTP: 0x%x\n",
	       cpuinfo.ctx_tbl, cpuinfo.ctx_tbl[0]);
	printf("Context 0 region table is at va 0x%x, pa 0x%x. Contents:\n",
	       pmap_kernel()->pm_reg_ptps, pmap_kernel()->pm_reg_ptps_pa);

	regtbl = pmap_kernel()->pm_reg_ptps;

	printf("PROM vector is at 0x%x\n",promvec);
	printf("PROM reboot routine is at 0x%x\n",promvec->pv_reboot);
	printf("PROM abort routine is at 0x%x\n",promvec->pv_abort);
	printf("PROM halt routine is at 0x%x\n",promvec->pv_halt);

	printf("Testing region 0xfe: ");
	test_region(0xfe,0,16*1024*1024);
	printf("Testing region 0xff: ");
	test_region(0xff,0,16*1024*1024);
#if 0	/* XXX avail_start */
	printf("Testing kernel region 0x%x: ", VA_VREG(vm_min_kernel_address));
	test_region(VA_VREG(vm_min_kernel_address), 4096, avail_start);
#endif
	cnpollc(1);
	cngetc();
	cnpollc(0);

	for (i = 0; i < SRMMU_L1SIZE; i++) {
		te = regtbl[i];
		if ((te & SRMMU_TETYPE) == SRMMU_TEINVALID)
		    continue;
		printf("Region 0x%x: PTE=0x%x <%s> L2PA=0x%x kernL2VA=0x%x\n",
		       i, te, ((te & SRMMU_TETYPE) == SRMMU_TEPTE ? "pte" :
			       ((te & SRMMU_TETYPE) == SRMMU_TEPTD ? "ptd" :
				((te & SRMMU_TETYPE) == SRMMU_TEINVALID ?
				 "invalid" : "reserved"))),
		       (te & ~0x3) << SRMMU_PPNPASHIFT,
		       pmap_kernel()->pm_regmap[i].rg_seg_ptps);
	}
	printf("Press q to halt...\n");
	cnpollc(1);
	if (cngetc()=='q')
	    callrom();
	cnpollc(0);
}

static u_int
VA2PAsw(ctx, addr, pte)
	int ctx;
	caddr_t addr;
	int *pte;
{
	int *curtbl;
	int curpte;

#ifdef EXTREME_EXTREME_DEBUG
	printf("Looking up addr 0x%x in context 0x%x\n",addr,ctx);
#endif
	/* L0 */
	*pte = curpte = cpuinfo.ctx_tbl[ctx];
#ifdef EXTREME_EXTREME_DEBUG
	printf("Got L0 pte 0x%x\n",pte);
#endif
	if ((curpte & SRMMU_TETYPE) == SRMMU_TEPTE) {
		return (((curpte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
			((u_int)addr & 0xffffffff));
	}
	if ((curpte & SRMMU_TETYPE) != SRMMU_TEPTD) {
		printf("Bad context table entry 0x%x for context 0x%x\n",
		       curpte, ctx);
		return 0;
	}
	/* L1 */
	curtbl = ((curpte & ~0x3) << 4) | vm_min_kernel_address; /* correct for krn*/
	*pte = curpte = curtbl[VA_VREG(addr)];
#ifdef EXTREME_EXTREME_DEBUG
	printf("L1 table at 0x%x.\nGot L1 pte 0x%x\n",curtbl,curpte);
#endif
	if ((curpte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((curpte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0xffffff));
	if ((curpte & SRMMU_TETYPE) != SRMMU_TEPTD) {
		printf("Bad region table entry 0x%x for region 0x%x\n",
		       curpte, VA_VREG(addr));
		return 0;
	}
	/* L2 */
	curtbl = ((curpte & ~0x3) << 4) | vm_min_kernel_address; /* correct for krn*/
	*pte = curpte = curtbl[VA_VSEG(addr)];
#ifdef EXTREME_EXTREME_DEBUG
	printf("L2 table at 0x%x.\nGot L2 pte 0x%x\n",curtbl,curpte);
#endif
	if ((curpte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((curpte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0x3ffff));
	if ((curpte & SRMMU_TETYPE) != SRMMU_TEPTD) {
		printf("Bad segment table entry 0x%x for reg 0x%x, seg 0x%x\n",
		       curpte, VA_VREG(addr), VA_VSEG(addr));
		return 0;
	}
	/* L3 */
	curtbl = ((curpte & ~0x3) << 4) | vm_min_kernel_address; /* correct for krn*/
	*pte = curpte = curtbl[VA_VPG(addr)];
#ifdef EXTREME_EXTREME_DEBUG
	printf("L3 table at 0x%x.\nGot L3 pte 0x%x\n",curtbl,curpte);
#endif
	if ((curpte & SRMMU_TETYPE) == SRMMU_TEPTE)
	    return (((curpte & SRMMU_PPNMASK) << SRMMU_PPNPASHIFT) |
		    ((u_int)addr & 0xfff));
	else {
		printf("Bad L3 pte 0x%x for reg 0x%x, seg 0x%x, pg 0x%x\n",
		       curpte, VA_VREG(addr), VA_VSEG(addr), VA_VPG(addr));
		return 0;
	}
	printf("Bizarreness with address 0x%x!\n",addr);
}

void
test_region(reg, start, stop)
	int reg;
	int start, stop;
{
	int i;
	int addr;
	int pte;
	int ptesw;
/*	int cnt=0;
*/

	cnpollc(1);
	for (i = start; i < stop; i+= NBPG) {
		addr = (reg << RGSHIFT) | i;
		pte=lda(((u_int)(addr)) | ASI_SRMMUFP_LN, ASI_SRMMUFP);
		if (pte) {
/*			printf("Valid address 0x%x\n",addr);
			if (++cnt == 20) {
				cngetc();
				cnt=0;
			}
*/
			if (VA2PA(addr) != VA2PAsw(0,addr,&ptesw)) {
				printf("Mismatch at address 0x%x.\n",addr);
				if (cngetc()=='q') break;
			}
			if (reg == VA_VREG(vm_min_kernel_address))
				/* kernel permissions are different */
				continue;
			if ((pte&SRMMU_PROT_MASK)!=(ptesw&SRMMU_PROT_MASK)) {
				printf("Mismatched protections at address "
				       "0x%x; pte=0x%x, ptesw=0x%x\n",
				       addr,pte,ptesw);
				if (cngetc()=='q') break;
			}
		}
	}
	cnpollc(0);
	printf("done.\n");
}


void print_fe_map(void)
{
	u_int i, pte;

	printf("map of region 0xfe:\n");
	for (i = 0xfe000000; i < 0xff000000; i+=4096) {
		if (((pte = getpte4m(i)) & SRMMU_TETYPE) != SRMMU_TEPTE)
		    continue;
		printf("0x%x -> 0x%x%x (pte 0x%x)\n", i, pte >> 28,
		       (pte & ~0xff) << 4, pte);
	}
	printf("done\n");
}

#endif
@


1.180
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.179 2015/12/29 04:46:28 mmcc Exp $	*/
@


1.179
log
@Remove NULL-checks before free().

ok tb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.178 2015/09/18 12:50:28 miod Exp $	*/
d3492 1
d3506 1
d3511 1
@


1.178
log
@Drop the workarounds allowing SMALL_KERNEL (i.e. installation media) to be
booted from old (< 2.10) bootblocks.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.177 2015/03/27 20:25:39 miod Exp $	*/
d3714 1
a3714 2
	if (pm->pm_regstore)
		free(pm->pm_regstore, M_VMPMAP, 0);
@


1.177
log
@Lower VM_MIN_KERNEL_ADDRESS by 128MB on non-SRMMU systems (sun4/4c/4e) as well,
in order to give these systems a more reasonable amount of kva, yet still
providing .75GB to userland processes.

Although there is no dependency upon a recent boot loader on non-SRMMU systems,
SMALL_KERNEL will nevertheless stick to the legacy kvm layout, for the time
being.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.176 2015/03/18 20:56:40 miod Exp $	*/
a83 20

#ifdef SMALL_KERNEL
/*
 * Force the SRMMU code to be limited to the Sun-4 compatible VM layout.
 * (this is done here to allow installation kernels to be loaded by older
 *  boot blocks which do not map enough data after the kernel image to
 *  cover pmap_bootstrap() needs.)
 */
#define	NKREG_OLD \
	((unsigned int)(-VM_MIN_KERNEL_ADDRESS_OLD / NBPRG))	/* 8 */
#define	NUREG_OLD	(256 - NKREG_OLD)			/* 248 */
#undef	NKREG_4C
#undef	NUREG_4C
#undef	NKREG_4M
#undef	NUREG_4M
#define	NKREG_4C	NKREG_OLD
#define	NUREG_4C	NUREG_OLD
#define	NKREG_4M	NKREG_OLD
#define	NUREG_4M	NUREG_OLD
#endif
@


1.176
log
@Rework the virtual memory layout on SRMMU systems (sun4d/sun4m) to use a much
lower VM_MIN_KERNEL_ADDRESS, since these systems are not crippled by the
Sun-4 MMU hole and have the real 4GB of address space.

Kernels running on Sun-4 MMU are not affected and will still be restricted
to the existing 128MB of kernel space, with 1GB - 128MB of user space.

Kernels running on SRMMU will now provide the low 3GB of address space to
userland, and use the top 1GB for the kernel, except when compiled with
option SMALL_KERNEL, in which case they will keep Sun-4 style the layout
(this is temporary to allow for people to boot bsd.rd to upgrade even when
not running 2.10 boot blocks, and will be removed eventually)

A consequence of this is that the top of the userland stack is no longer at
0xf0000000. But since nothing in userland uses USRSTACK anymore, this should
not be an issue.

Tested on sun4c and various sun4m, with physical memory sizes ranging from 32
to 448MB.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.175 2015/03/18 20:49:40 miod Exp $	*/
d92 6
d99 4
a102 3
#undef	NKREG_4M
#define	NUREG_4M	NUREG_4C
#define	NKREG_4M	NKREG_4C
d187 1
a187 1
static u_int	VA2PA(caddr_t);
d502 3
a504 3
static void mmu_setup4m_L1(int, struct pmap *);
static void mmu_setup4m_L2(int, struct regmap *);
static void  mmu_setup4m_L3(int, struct segmap *);
d595 1
a595 1
static u_int
d694 1
a694 1
static void
d776 1
a776 1
static void get_phys_mem(void **);
d872 1
a872 1
static void
d2670 1
a2670 1
static void pmap_bootstrap4m(void *);
d2673 1
a2673 1
static void pmap_bootstrap4_4c(void *, int, int, int);
d2758 1
d2930 3
a2932 3
	p = (caddr_t)VM_MIN_KERNEL_ADDRESS_OLD;	/* first va */
	vs = VA_VSEG(VM_MIN_KERNEL_ADDRESS_OLD);/* first virtual segment */
	vr = VA_VREG(VM_MIN_KERNEL_ADDRESS_OLD);/* first virtual region */
d3118 1
a3118 1
static void
@


1.175
log
@Allow for VM_MIN_KERNEL_ADDRESS to not be a constant.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.174 2015/02/15 21:34:33 miod Exp $	*/
d85 13
d200 14
d391 2
a392 2
struct regmap	kernel_regmap_store[NKREG];	/* the kernel's regmap */
struct segmap	kernel_segmap_store[NKREG*NSEGRG];/* the kernel's segmaps */
d395 3
a397 3
u_int 	*kernel_regtable_store;		/* 1k of storage to map the kernel */
u_int	*kernel_segtable_store;		/* 2k of storage to map the kernel */
u_int	*kernel_pagtable_store;		/* 128k of storage to map the kernel */
d890 1
a890 1
			prom_printf("bootstrap gap: start %lx, chop %lx, end %lx\n",
d1116 4
a1119 9
	 * (i.e. regions VA_VREG(VM_MIN_KERNEL_ADDRESS)+1 to 0xff). We ignore
	 * the first region (at VA_VREG(VM_MIN_KERNEL_ADDRESS)), since that
	 * is the 16MB L1 mapping that the ROM used to map the kernel in
	 * initially. Later, we will rebuild a new L3 mapping for the kernel
	 * and install it before switching to the new pagetables.
	 */
	regtblrover =
		((regtblptd & ~SRMMU_TETYPE) << SRMMU_PPNPASHIFT) +
		(VA_VREG(VM_MIN_KERNEL_ADDRESS)+1) * sizeof(long);	/* kernel only */
d1121 10
a1130 2
	for (i = VA_VREG(VM_MIN_KERNEL_ADDRESS) + 1; i < SRMMU_L1SIZE;
	     i++, regtblrover += sizeof(long)) {
d1693 1
a1693 1
printf("mmu_pagein: kernel wants map at va 0x%x, vr %d, vs %d\n", va, vr, vs);
d1831 1
a1831 1
		for (va = 0, i = NUREG; --i >= 0; ) {
d1839 1
a1839 1
				gap_start = NUREG;
d2274 1
a2274 1
					va, npv->pv_va, -1); /* XXX -1 */
d2600 1
a2600 1
					va, npv->pv_va, -1); /* XXX -1 */
d2739 3
d2776 1
a2776 1
#if defined(SUN4M) /* We're in a dual-arch kernel. Setup 4/4c fn. ptrs */
d2814 2
a2815 2
	kernel_pmap_store.pm_regmap = &kernel_regmap_store[-NUREG];
	for (i = NKREG; --i >= 0;) {
d2922 3
a2924 3
	p = (caddr_t)VM_MIN_KERNEL_ADDRESS;	/* first va */
	vs = VA_VSEG(VM_MIN_KERNEL_ADDRESS);	/* first virtual segment */
	vr = VA_VREG(VM_MIN_KERNEL_ADDRESS);	/* first virtual region */
d3067 1
a3067 1
			for (p = 0, j = NUREG; --j >= 0; p += NBPRG)
d3070 1
a3070 1
			for (p = 0, vr = 0; vr < NUREG; vr++) {
d3123 3
d3157 1
a3157 1
	 * Set up pm_regmap for kernel to point NUREG *below* the beginning
d3159 1
a3159 1
	 * above NUREG, we save storage space and can index kernel and
d3162 1
a3162 1
	kernel_pmap_store.pm_regmap = &kernel_regmap_store[-NUREG];
d3165 3
a3167 3
	bzero(kernel_regmap_store, NKREG * sizeof(struct regmap));
	bzero(kernel_segmap_store, NKREG * NSEGRG * sizeof(struct segmap));
	for (i = NKREG; --i >= 0;) {
d3208 1
a3208 1
	ctxtblsize = max(ncontext,1024) * sizeof(int);
d3215 1
a3215 1
	 * kernel. This takes (2k + NKREG * 16k) of space, but
d3231 1
a3231 1
	p += (SRMMU_L2SIZE * sizeof(long)) * NKREG;
d3236 1
a3236 1
	p += ((SRMMU_L3SIZE * sizeof(long)) * NKREG) * NSEGRG;
d3264 1
a3264 1
	for (reg = 0; reg < NKREG; reg++) {
d3272 1
a3272 1
		rp = &pmap_kernel()->pm_regmap[reg + VA_VREG(VM_MIN_KERNEL_ADDRESS)];
d3277 1
a3277 1
		setpgt4m(&pmap_kernel()->pm_reg_ptps[reg + VA_VREG(VM_MIN_KERNEL_ADDRESS)],
d3344 1
a3344 1
	if (VM_MIN_KERNEL_ADDRESS % NBPRG != 0)
d3348 1
a3348 1
	for (q = (caddr_t) VM_MIN_KERNEL_ADDRESS; q < p; q += NBPG) {
d3606 1
a3606 1
			for (i = NUREG; --i >= 0;)
d3630 1
a3630 1
		for (i = 0; i < NUREG; i++)
d3634 3
a3636 3
		for (i = 0; i < NKREG; i++) {
			setpgt4m(&pm->pm_reg_ptps[VA_VREG(VM_MIN_KERNEL_ADDRESS) + i],
				 cpuinfo.L1_ptps[VA_VREG(VM_MIN_KERNEL_ADDRESS) + i]);
d4356 1
a4356 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pg, prot);
d4751 1
a4751 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pg, prot);
d5486 1
a5486 1
	if (va < VM_MIN_KERNEL_ADDRESS)
d5567 1
a5567 1
	if (VM_MIN_KERNEL_ADDRESS < va)
@


1.174
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.173 2015/02/11 07:05:39 dlg Exp $	*/
d6374 1
a6374 1
		if ((unsigned int)rp < VM_MIN_KERNEL_ADDRESS)
d6379 1
a6379 1
			if ((unsigned int)sp < VM_MIN_KERNEL_ADDRESS)
d6688 2
a6689 2
	printf("Testing kernel region 0x%x: ", VA_VREG(VM_MIN_KERNEL_ADDRESS));
	test_region(VA_VREG(VM_MIN_KERNEL_ADDRESS), 4096, avail_start);
d6741 1
a6741 1
	curtbl = ((curpte & ~0x3) << 4) | VM_MIN_KERNEL_ADDRESS; /* correct for krn*/
d6755 1
a6755 1
	curtbl = ((curpte & ~0x3) << 4) | VM_MIN_KERNEL_ADDRESS; /* correct for krn*/
d6769 1
a6769 1
	curtbl = ((curpte & ~0x3) << 4) | VM_MIN_KERNEL_ADDRESS; /* correct for krn*/
d6785 2
a6786 1
void test_region(reg, start, stop)
d6812 1
a6812 1
			if (reg == VA_VREG(VM_MIN_KERNEL_ADDRESS))
@


1.173
log
@no md code wants lockmgr locks, so no md code needs to include sys/lock.h

with and ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.172 2014/12/17 06:58:10 guenther Exp $	*/
d6244 1
a6244 1
pmap_remove_holes(struct vm_map *map)
d6248 1
@


1.172
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.171 2014/12/17 06:05:52 deraadt Exp $	*/
a68 1
#include <sys/lock.h>
@


1.171
log
@delete archaic simplelock use.
ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.170 2014/12/15 02:24:22 guenther Exp $	*/
d6260 1
a6260 1
		      POSIX_MADV_RANDOM,
@


1.170
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.169 2014/11/22 22:51:29 deraadt Exp $	*/
a1395 1
	simple_lock(&pm->pm_lock); /* what if other cpu takes mmuentry ?? */
a1401 1
	simple_unlock(&pm->pm_lock);
a1569 1
	simple_lock(&pm->pm_lock); /* what if other cpu takes mmuentry ?? */
a1575 1
	simple_unlock(&pm->pm_lock);
a2775 1
	simple_lock_init(&kernel_pmap_store.pm_lock);
a3118 1
	simple_lock_init(&kernel_pmap_store.pm_lock);
a3561 1
	simple_lock_init(&pm->pm_lock);
a3626 1
	simple_lock(&pm->pm_lock);
a3627 1
	simple_unlock(&pm->pm_lock);
d3715 1
a3715 3

	if (pm != NULL) {
		simple_lock(&pm->pm_lock);
a3716 2
		simple_unlock(&pm->pm_lock);
	}
a3756 1
	simple_lock(&pm->pm_lock);
a3766 1
	simple_unlock(&pm->pm_lock);
a3786 1
	simple_lock(pm->pm_lock);
a3798 1
	simple_unlock(pm->pm_lock);
a4513 1
	simple_lock(&pm->pm_lock);
a4584 1
	simple_unlock(&pm->pm_lock);
a4842 1
	simple_lock(&pm->pm_lock);
a4898 1
	simple_unlock(&pm->pm_lock);
@


1.169
log
@the per-cpu storage page does not need to be executable in this
fashion.  Anyways, we don't do MP here yet, so #ifdef it all away.
with miod
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.168 2014/11/22 07:30:17 deraadt Exp $	*/
d6280 1
a6280 1
		    UVM_MAPFLAG(PROT_NONE, PROT_NONE, UVM_INH_SHARE,
@


1.168
log
@spell PROT_EXEC correctly
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.167 2014/11/17 21:39:19 deraadt Exp $	*/
d3405 1
d3452 1
a3452 1
		(SRMMU_TEPTE | PPROT_RWX_RWX | SRMMU_PG_C);
d3466 2
@


1.167
log
@Two additional POSIX_MADV_RANDOM conversions
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.166 2014/11/16 12:30:58 deraadt Exp $	*/
d4850 1
a4850 1
	 * Since the caller might request either a removal of PROT_EXECUTE
@


1.166
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.165 2014/07/12 18:44:43 tedu Exp $	*/
d6278 1
a6278 1
		      UVM_ADV_RANDOM,
@


1.165
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.164 2014/04/08 13:23:51 mpi Exp $	*/
d671 1
a671 1
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
d675 1
a675 1
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
d679 1
a679 1
		case VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE:
d683 1
a683 1
		case VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE:
d687 1
a687 1
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
d691 1
a691 1
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
d695 1
a695 1
		case VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE:
d699 1
a699 1
		case VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE:
d1658 2
a1659 2
	if (prot != VM_PROT_NONE)
		bits = PG_V | ((prot & VM_PROT_WRITE) ? PG_W : 0);
d4334 1
a4334 1
	    (pmapdebug & PDB_REMOVE && prot == VM_PROT_NONE))
d4341 1
a4341 1
	if (prot & VM_PROT_WRITE)
d4344 1
a4344 1
	if (prot & VM_PROT_READ) {
d4517 1
a4517 1
	if (pm == NULL || prot & VM_PROT_WRITE)
d4520 1
a4520 1
	if ((prot & VM_PROT_READ) == 0) {
d4631 1
a4631 1
		newprot = prot & VM_PROT_WRITE ? PG_S|PG_W : PG_S;
d4633 1
a4633 1
		newprot = prot & VM_PROT_WRITE ? PG_W : 0;
d4731 1
a4731 1
	    (pmapdebug & PDB_REMOVE && prot == VM_PROT_NONE))
d4738 1
a4738 1
	if (prot & VM_PROT_WRITE)
d4741 1
a4741 1
	if (prot & VM_PROT_READ) {
d4844 1
a4844 1
	if ((prot & VM_PROT_READ) == 0) {
d4854 1
a4854 1
	newprot = pte_prot4m(pm, VM_PROT_READ);
d5034 1
a5034 1
	if (prot & VM_PROT_WRITE)
d5353 1
a5353 1
	if (prot & VM_PROT_WRITE)
d5443 1
a5443 1
		if (flags & VM_PROT_WRITE)
d5445 1
a5445 1
		if (flags & VM_PROT_READ)
d5687 1
a5687 1
		((prot & VM_PROT_WRITE) ? PPROT_N_RWX : PPROT_N_RX);
d6277 1
a6277 1
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_SHARE,
@


1.164
log
@Fewer <uvm/uvm.h>!
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.163 2013/09/21 10:04:42 miod Exp $	*/
d3636 1
a3636 1
		free(pm, M_VMPMAP);
d3698 1
a3698 1
		free(pm->pm_regstore, M_VMPMAP);
d4094 1
a4094 1
			free(pte0, M_VMPMAP);
d4097 1
a4097 1
				free(rp->rg_segmap, M_VMPMAP);
d4178 1
a4178 1
		free(pte0, M_VMPMAP);
d4183 1
a4183 1
			free(rp->rg_segmap, M_VMPMAP);
d4304 1
a4304 1
			free(rp->rg_segmap, M_VMPMAP);
d4385 1
a4385 1
				free(sp->sg_pte, M_VMPMAP);
d4388 1
a4388 1
					free(rp->rg_segmap, M_VMPMAP);
d4467 1
a4467 1
			free(sp->sg_pte, M_VMPMAP);
d4480 1
a4480 1
				free(rp->rg_segmap, M_VMPMAP);
d4808 1
a4808 1
				free(rp->rg_segmap, M_VMPMAP);
@


1.163
log
@Create process map holes with UVM_INH_SHARE so that they don't get lost in
fork-without-exec situation (such as privsep'd binaries).
Fixes occasional SIGSEGV in syslogd and pflogd on sun4/4c/4e.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.162 2013/06/11 16:42:11 deraadt Exp $	*/
d70 1
d72 1
a72 2
#include <uvm/uvm.h>
#include <sys/pool.h>
@


1.162
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.161 2011/09/22 17:41:00 jasper Exp $	*/
d6277 1
a6277 1
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
@


1.161
log
@nowadays uvm_init() calls pmap_init(), not vm_init(); so update the comments.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.160 2011/05/30 22:25:22 oga Exp $	*/
d6519 2
a6520 2
	daddr64_t blkno;
	int (*dump)(dev_t, daddr64_t, caddr_t, size_t);
@


1.160
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.159 2011/04/26 23:50:21 ariane Exp $	*/
d2839 1
a2839 1
	 * Set up the `constants' for the call to vm_init()
d3161 1
a3161 1
	 * Set up the `constants' for the call to vm_init()
@


1.159
log
@MMU address space holes are at a fixed position (ofcourse).
Therefore set UVM_FLAG_FIXED and enforce this.

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.158 2010/12/06 20:57:18 miod Exp $	*/
d868 1
a868 1
				atop(start), atop(chop), VM_FREELIST_DEFAULT);
d886 1
a886 1
			atop(start), atop(end), VM_FREELIST_DEFAULT);
@


1.158
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.157 2010/07/10 19:32:25 miod Exp $	*/
d6278 2
a6279 1
		      UVM_ADV_RANDOM, UVM_FLAG_NOMERGE | UVM_FLAG_HOLE));
@


1.157
log
@sun4e (i.e. SPARCengine 1e) support. This platform is a mix between sun4 and
sun4c, as it has a sun4c OpenPROM but a sun4 8KB pagesize. VME devices are
not supported yet.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.156 2010/06/30 20:35:03 miod Exp $	*/
d6242 2
a6243 4
void
pmap_prefer(foff, vap)
	vaddr_t foff;
	vaddr_t *vap;
d6245 1
a6245 2
	vaddr_t va = *vap;
	long d, m;
d6253 5
a6257 2
	if (m == 0)		/* m=0 => no cache aliasing */
		return;
d6259 1
a6259 3
	d = foff - va;
	d &= (m - 1);
	*vap = va + d;
@


1.156
log
@Remove the ``never cache virtual addresses in the DVMA range'' in
pv_changepte4m() since DVMA addresses are never loaded in the SRMMU; this
chunk was forgotten in the previous `don't steal DVMA out of kernel_map on 4m'
commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.155 2010/06/29 21:28:11 miod Exp $	*/
d387 1
a387 1
#if defined(SUN4) || defined(SUN4C)
d431 6
a436 6
#define	getsegmap(va)		(CPU_ISSUN4C \
					? lduba(va, ASI_SEGMAP) \
					: (lduha(va, ASI_SEGMAP) & segfixmask))
#define	setsegmap(va, pmeg)	(CPU_ISSUN4C \
					? stba(va, ASI_SEGMAP, pmeg) \
					: stha(va, ASI_SEGMAP, pmeg))
d450 1
a450 1
#if defined(SUN4) || defined(SUN4C)
d486 1
a486 1
#if defined(SUN4) || defined(SUN4C)
d500 1
a500 1
#if !defined(SUN4M) && (defined(SUN4) || defined(SUN4C))
d504 1
a504 1
#elif defined(SUN4M) && !(defined(SUN4) || defined(SUN4C))
d723 1
a723 1
#define GAP_WIDEN(pm,vr) do if (CPU_ISSUN4OR4C) {	\
d730 1
a730 1
#define GAP_SHRINK(pm,vr) do if (CPU_ISSUN4OR4C) {			\
d926 1
a926 1
#if defined(SUN4) || defined(SUN4C)
d944 2
a945 2
#if defined(SUN4C)
	if (CPU_ISSUN4C) {
d1235 2
a1236 4
struct mmuentry *me_alloc(struct mmuhd *, struct pmap *, int, int);
void		me_free(struct pmap *, u_int);
struct mmuentry	*region_alloc(struct mmuhd *, struct pmap *, int);
void		region_free(struct pmap *, u_int);
d1247 1
a1247 1
		setcontext(new); \
d1250 5
a1254 1
#if defined(SUN4) || defined(SUN4C) /* This is old sun MMU stuff */
d1674 1
d1715 1
a1715 1
#endif /* defined SUN4 or SUN4C */
d1740 1
a1740 1
	if (CPU_ISSUN4OR4C) {
d1768 1
a1768 1
		if (CPU_ISSUN4OR4C) {
d1780 1
a1780 1
	if (CPU_ISSUN4OR4C) {
d1905 1
d1915 1
d1960 1
a1960 1
#if defined(SUN4) || defined(SUN4C)
d2633 1
a2633 1
#if defined(SUN4) && defined(SUN4C)
d2640 1
a2640 1
#if defined(SUN4) || defined(SUN4C)
d2664 1
a2664 1
#if defined(SUN4) && (defined(SUN4C) || defined(SUN4M))
d2683 2
a2684 2
	} else if (CPU_ISSUN4OR4C) {
#if defined(SUN4) || defined(SUN4C)
d2692 1
a2692 1
#if defined(SUN4) || defined(SUN4C)
d2724 1
d3103 1
a3103 1
#if defined(SUN4) || defined(SUN4C) /* setup 4M fn. ptrs for dual-arch kernel */
d3118 1
a3118 1
#endif /* defined Sun4/Sun4c */
d3569 1
a3569 1
	if (CPU_ISSUN4OR4C) {
d3655 1
a3655 1
	if (CPU_ISSUN4OR4C) {
d3838 1
a3838 1
#if defined(SUN4) || defined(SUN4C)
d4035 1
a4035 1
#if defined(SUN4) || defined(SUN4C)
d4321 1
a4321 1
#if defined(SUN4) || defined(SUN4C)
d4993 1
a4993 1
#if defined(SUN4) || defined(SUN4C)
d5721 1
a5721 1
#if defined(SUN4) || defined(SUN4C)
d5857 1
a5857 1
#if defined(SUN4) || defined(SUN4C)
d6003 1
a6003 1
#if defined(SUN4) || defined(SUN4C)
d6207 1
a6207 1
#if defined(SUN4) || defined(SUN4C)
d6250 1
a6250 1
#if defined(SUN4) || defined(SUN4C)
d6267 1
a6267 1
#if defined(SUN4) || defined(SUN4C)
d6294 2
a6295 2
#if defined(SUN4) || defined(SUN4C)
	if (CPU_ISSUN4OR4C) {
d6507 1
a6507 1
	if (CPU_ISSUN4OR4C)
d6530 1
a6530 1
#if defined(SUN4C) || defined(SUN4)
d6567 1
a6567 1
	kcpup->npmeg = (CPU_ISSUN4OR4C) ? seginval + 1 : 0; 
d6589 1
a6589 1
#if defined(SUN4C) || defined(SUN4)
d6660 2
a6661 2
#if defined(SUN4) || defined(SUN4C)
	if (CPU_ISSUN4C || CPU_ISSUN4) {
@


1.155
log
@During kernel bootstrap, stop assuming the kernel image has been loaded in
low physical memory, but instead figure out where it has been loaded from
the current MMU setup.

From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.154 2010/06/17 16:11:19 miod Exp $	*/
d2008 2
a2318 7
			/*
			 * Bizarreness:  we never set PG_C on DVMA pages.
			 */
			if ((bis & SRMMU_PG_C) &&
			    va >= DVMA_BASE && va < DVMA_END)
				continue;

@


1.154
log
@aligment -> alignment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.153 2010/03/27 15:14:34 oga Exp $	*/
d374 4
d384 1
a384 9
/*
 * XXX - these have to be global for dumpsys()
 */
#define	MA_SIZE	32		/* size of memory descriptor arrays */
struct	memarr pmemarr[MA_SIZE];/* physical memory regions */
int	npmemarr;		/* number of entries in pmemarr */

static void pmap_page_upload(paddr_t);
void pmap_pinit(pmap_t);
d743 1
a743 1
static void sortm(struct memarr *, int);
d754 19
a772 1
 * Sort a memory array by address.
d774 3
a776 2
static void
sortm(mp, n)
d778 21
a798 6
	int n;
{
	struct memarr *mpj;
	int i, j;
	paddr_t addr;
	psize_t len;
d800 16
a815 13
	/* Insertion sort.  This is O(n^2), but so what? */
	for (i = 1; i < n; i++) {
		/* save i'th entry */
		addr = mp[i].addr;
		len = mp[i].len;
		/* find j such that i'th entry goes before j'th */
		for (j = 0, mpj = mp; j < i; j++, mpj++)
			if (addr < mpj->addr)
				break;
		/* slide up any additional entries */
		ovbcopy(mpj, mpj + 1, (i - j) * sizeof(*mp));
		mpj->addr = addr;
		mpj->len = len;
d820 1
a820 5
 * For our convenience, vm_page.c implements:
 *       vm_bootstrap_steal_memory()
 * using the functions:
 *       pmap_virtual_space(), pmap_free_pages(), pmap_next_page(),
 * which are much simpler to implement.
d840 1
a840 2
pmap_page_upload(first_pa)
	paddr_t first_pa;
d842 6
a847 2
	int	n = 0;
	paddr_t start, end;
d849 20
a868 4
	phys_avail = first_pa;
	
	npmemarr = makememarr(pmemarr, MA_SIZE, MEMARR_AVAILPHYS);
	sortm(pmemarr, npmemarr);
d870 6
a875 2
	if (pmemarr[0].addr != 0)
		panic("pmap_page_upload: no memory?");
d877 3
a879 6
	/*
	 * Compute physmem
	 */
	physmem = 0;
	for (n = 0; n < npmemarr; n++)
		physmem += atop(pmemarr[n].len);
d881 1
a881 5
	for (n = 0; n < npmemarr; n++) {
		start = (first_pa > pmemarr[n].addr) ? first_pa :
						       pmemarr[n].addr;
		end = pmemarr[n].addr + pmemarr[n].len;
		if (start >= end)
d884 1
d886 1
a886 1
				  atop(start), atop(end), VM_FREELIST_DEFAULT);
d890 3
d894 1
a894 2
pmap_pa_exists(pa)
	paddr_t pa;
d896 13
a908 1
	return (pa < phys_avail || (pvhead(atop(pa)) != NULL));
d2638 1
a2638 1
static void pmap_bootstrap4m(void);
d2641 1
a2641 1
static void pmap_bootstrap4_4c(int, int, int);
d2652 1
a2652 2
pmap_bootstrap(nctx, nregion, nsegment)
	int nsegment, nctx, nregion;
d2654 5
d2669 7
a2675 2
#if 0
	ncontext = nctx;
d2677 1
d2679 1
d2681 1
a2681 4
	if (CPU_ISSUN4M) {
		pmap_bootstrap4m();
		return;
	}
d2683 1
d2685 2
a2686 3
	if (CPU_ISSUN4OR4C) {
		pmap_bootstrap4_4c(nctx, nregion, nsegment);
		return;
d2688 2
a2689 1
#endif
d2694 1
a2694 2
pmap_bootstrap4_4c(nctx, nregion, nsegment)
	int nsegment, nctx, nregion;
d2701 1
a2701 1
	struct   regmap *rp;
d2704 4
a2707 1
	int rcookie, scookie;
d2709 1
d2712 9
a2720 5
	paddr_t avail_start;
	extern char end[];
#ifdef DDB
	extern char *esym;
#endif
d2814 1
a2814 5
	p = end;
#ifdef DDB
	if (esym != 0)
		p = esym;
#endif
d2816 1
a2816 1
	mmuregions = mmureg = (struct mmuentry *)p;
d2820 1
a2820 1
	mmusegments = mmuseg = (struct mmuentry *)p;
d2847 1
a2847 1
	avail_start = (paddr_t)p - KERNBASE;
d2879 2
a2880 2
	 * THIS ASSUMES SEGMENT i IS MAPPED BY MMU ENTRY i DURING THE
	 * BOOT PROCESS
d2899 11
a2909 1
	for (rcookie = 0, scookie = 0;;) {
d2986 24
a3010 1
#endif
d3016 5
a3020 1
		for (; rcookie < nregion; rcookie++, mmureg++) {
d3026 4
a3029 1
	for (; scookie < nsegment; scookie++, mmuseg++) {
d3058 2
a3059 6
		extern char etext[];
#ifdef KGDB
		int mask = ~PG_NC;	/* XXX chgkprot is busted */
#else
		int mask = ~(PG_W | PG_NC);
#endif
d3061 11
a3071 2
		for (p = (caddr_t)trapbase; p < etext; p += NBPG)
			setpte4(p, getpte4(p) & mask);
a3072 2

	pmap_page_upload(avail_start);
d3083 1
a3083 1
pmap_bootstrap4m(void)
d3091 2
a3092 3
	paddr_t avail_start;
	extern char end[];
	extern char etext[];
d3094 7
a3100 3
#ifdef DDB
	extern char *esym;
#endif
d3145 2
a3146 5
	p = end;		/* p points to top of kernel mem */
#ifdef DDB
	if (esym != 0)
		p = esym;
#endif
d3172 2
d3197 1
a3197 2
	bzero(kernel_regtable_store,
	      p - (caddr_t) kernel_regtable_store);
d3202 1
a3202 2
	bzero(kernel_segtable_store,
	      p - (caddr_t) kernel_segtable_store);
d3207 1
a3207 2
	bzero(kernel_pagtable_store,
	      p - (caddr_t) kernel_pagtable_store);
d3213 1
a3213 1
	avail_start = (paddr_t)p - KERNBASE;
d3223 1
a3223 1
	pmap_kernel()->pm_reg_ptps = (int *) kernel_regtable_store;
d3225 1
a3225 1
		VA2PA((caddr_t)pmap_kernel()->pm_reg_ptps);
d3248 2
a3249 1
		    (VA2PA(kphyssegtbl) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD);
a3252 5
		if (rp->rg_segmap == NULL) {
			printf("rp->rg_segmap == NULL!\n");
			rp->rg_segmap = &kernel_segmap_store[reg * NSEGRG];
		}

d3265 1
a3265 1
				 (VA2PA(kphyspagtbl) >> SRMMU_PPNPASHIFT) |
d3321 3
a3323 1
		int pte;
d3330 1
a3330 1
		sp->sg_npte++;
d3332 1
a3332 1
		pte = ((int)q - VM_MIN_KERNEL_ADDRESS) >> SRMMU_PPNPASHIFT;
d3335 1
d3342 1
a3342 1
		if (q < (caddr_t) trapbase || q >= etext)
d3345 2
a3346 1
		setpgt4m(&sp->sg_pte[VA_VPG(q)], pte);
d3367 1
a3367 1
			     (caddr_t)VA2PA((caddr_t)pagetables_start),
a3374 1
	pmap_page_upload(avail_start);
a3640 1
 * Called when a pmap initialized by pmap_pinit is being released.
d6580 1
a6580 1
		memseg.start = pmemarr[i].addr;
@


1.153
log
@Fix the pageqlock recusion on sun4m.

In uvm_pageout() we called pmap_page_protect(VM_PROT_NONE) with the
pageqlock held, when pageqlock was a mutex this caused recursion panics.
The problem was that freeing the pv_entries called uvm_km_free, which
needed the pageqlock itself. Fix this by changing the homegrown pool
allocator to just use the normal pool functions, then change the
cacheability as necessary.

That I know of, this leaves m68k still to be fixed, maybe arm bootstrap
(but I do not think so).

Tested by NicM@@ on a 4m machine. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.152 2009/02/12 18:52:17 miod Exp $	*/
d3059 1
a3059 1
	 * To keep supersparc happy, minimum aligment is on a 4K boundary.
@


1.152
log
@Keep track of resident pages in pm_stats, and use this to implement a real
pmap_resident_count(). From NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.151 2009/01/27 22:14:13 miod Exp $	*/
d196 2
a197 2
void *pgt_page_alloc(struct pool *, int, int *);
void  pgt_page_free(struct pool *, void *);
d221 2
a222 1
        caddr_t p;
d224 7
a230 8
	*slowdown = 0;
        p = (caddr_t)uvm_km_kmemalloc(kernel_map, uvm.kernel_object,
                                      PAGE_SIZE, UVM_KMF_NOWAIT);
        if (p != NULL && ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)) {
                pcache_flush(p, (caddr_t)VA2PA(p), PAGE_SIZE);
                kvm_uncache(p, 1);
        }
        return (p);
d232 1
a232 1
   
d234 1
a234 1
pgt_page_free(struct pool *pp, void *v)
d236 9
a244 1
        uvm_km_free(kernel_map, (vaddr_t)v, PAGE_SIZE);
@


1.151
log
@Get rid of the last traces of uvm.pager_[se]va
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.150 2008/10/23 23:54:02 tedu Exp $	*/
d1151 1
d2872 1
d3225 1
d3787 1
d3896 1
d3970 1
d4036 1
d4158 1
d4366 1
d4673 1
d4995 1
d5049 1
d5179 1
d5215 1
d5390 1
d5406 1
d5525 1
d5548 1
a6113 22
}

int
pmap_count_ptes(pm)
	struct pmap *pm;
{
	int idx, total;
	struct regmap *rp;
	struct segmap *sp;

	if (pm == pmap_kernel()) {
		rp = &pm->pm_regmap[NUREG];
		idx = NKREG;
	} else {
		rp = pm->pm_regmap;
		idx = NUREG;
	}
	for (total = 0; idx;)
		if ((sp = rp[--idx].rg_segmap) != NULL)
			total += sp->sg_npte;
	pm->pm_stats.resident_count = total;
	return (total);
@


1.150
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.149 2008/06/09 20:31:47 miod Exp $	*/
d1937 2
a1938 2
				 * Bizarreness:  we never clear PG_W on
				 * pager pages, nor PG_NC on DVMA pages.
a1939 3
				if (bic == PG_W &&
				    va >= uvm.pager_sva && va < uvm.pager_eva)
					continue;
d2249 1
a2249 2
			 * Bizarreness:  we never clear PG_W on
			 * pager pages, nor set PG_C on DVMA pages.
a2250 3
			if ((bic & PPROT_WRITE) &&
			    va >= uvm.pager_sva && va < uvm.pager_eva)
				continue;
@


1.149
log
@Create a real holp by using uvm_map() with UVM_FLAG_HOLE in pmap_remove_holes().
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.148 2008/06/08 20:57:19 miod Exp $	*/
d196 1
a196 1
void *pgt_page_alloc(struct pool *, int);
d219 1
a219 1
pgt_page_alloc(struct pool *pp, int flags)
d223 1
@


1.148
log
@Make sure to cnpollc(1)/cnpollc(0) around cngetc() or getsn() calls.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.147 2007/11/28 16:33:20 martin Exp $	*/
d6166 4
a6169 2
		uvm_map_reserve(map, ehole - shole, UVM_UNKNOWN_OFFSET,
		    0, &shole);
@


1.147
log
@ctob/btoc -> ptoa/atop

from Rodolfo Gouveia
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.146 2007/09/10 18:49:45 miod Exp $	*/
d6597 1
d6599 1
d6614 1
d6617 1
d6702 1
d6728 1
@


1.146
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.145 2007/06/06 17:15:12 deraadt Exp $	*/
d821 1
a821 1
		physmem += btoc(pmemarr[n].len);
d6397 1
a6397 1
	return (btoc(sz));
d6447 1
a6447 1
	ksegp->c_size = ctob(pmap_dumpsize()) - ALIGN(sizeof(kcore_seg_t));
@


1.145
log
@now that all partition size/offsets are potentially 64-bit, change the
type of all variables to daddr64_t.  this includes the APIs for XXsize()
and XXdump(), all range checks inside bio drivers, internal variables
for disklabel handling, and even uvm's swap offsets.  re-read numerous
times by otto, miod, krw, thib to look for errors
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.144 2006/06/26 22:23:03 miod Exp $	*/
d6151 19
@


1.144
log
@atop(PAGE_SIZE) -> 1, 'tis true but it gets computed faster when PAGE_SIZE
is not a constant.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.143 2005/04/17 18:47:50 miod Exp $	*/
d6388 2
a6389 2
	daddr_t blkno;
	int (*dump)(dev_t, daddr_t, caddr_t, size_t);
@


1.143
log
@Do not use KERNBASE when VM_MIN_KERNEL_ADDRESS or VM_MAXUSER_ADDRESS are
implied; this currently does not change anything (yet).

Also, define the I/O space range in <machine/vmparam.h> rather than in
<sparc/sparc/vaddrs.h>.

ok deraadt@@ mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.142 2005/04/03 10:36:12 miod Exp $	*/
d227 1
a227 1
                kvm_uncache(p, atop(PAGE_SIZE));
@


1.142
log
@Simple performance improvements:
- inline empty pmap_deactivate() and pmap_collect().
- inline pmap_phys_address().
- provide a real pmap_kremove() implementation, rather than invoking
  pmap_remove() on behalf of pmap_kernel().
- do not check for the MMU hole in pmap_prefer() for SUN4M-only kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.141 2005/03/29 11:33:18 miod Exp $	*/
d1021 5
a1025 5
	 * (i.e. regions VA_VREG(KERNBASE)+1 to 0xff). We ignore the first
	 * region (at VA_VREG(KERNBASE)), since that is the 16MB L1 mapping
	 * that the ROM used to map the kernel in initially. Later, we will
	 * rebuild a new L3 mapping for the kernel and install it before
	 * switching to the new pagetables.
d1029 1
a1029 1
		(VA_VREG(KERNBASE)+1) * sizeof(long);	/* kernel only */
d1031 1
a1031 1
	for (i = VA_VREG(KERNBASE) + 1; i < SRMMU_L1SIZE;
d2819 3
a2821 3
	p = (caddr_t)KERNBASE;		/* first va */
	vs = VA_VSEG(KERNBASE);		/* first virtual segment */
	vr = VA_VREG(KERNBASE);		/* first virtual region */
d3124 1
a3124 1
		rp = &pmap_kernel()->pm_regmap[reg + VA_VREG(KERNBASE)];
d3129 1
a3129 1
		setpgt4m(&pmap_kernel()->pm_reg_ptps[reg + VA_VREG(KERNBASE)],
d3194 2
a3195 2
	 * We map from KERNBASE to p into context 0's page tables (and
	 * the kernel pmap).
d3200 2
a3201 2
	if (KERNBASE % NBPRG != 0)
		panic("pmap_bootstrap4m: KERNBASE not region-aligned");
d3204 1
a3204 1
	for (q = (caddr_t) KERNBASE; q < p; q += NBPG) {
d3216 1
a3216 1
		pte = ((int)q - KERNBASE) >> SRMMU_PPNPASHIFT;
d3485 2
a3486 2
			setpgt4m(&pm->pm_reg_ptps[VA_VREG(KERNBASE) + i],
				 cpuinfo.L1_ptps[VA_VREG(KERNBASE) + i]);
d5340 2
a5341 2
	if (va < KERNBASE)
		panic("pmap_enk4m: can't enter va 0x%lx below KERNBASE", va);
d5419 2
a5420 2
	if (KERNBASE < va)
		panic("pmap_enu4m: can't enter va 0x%lx above KERNBASE", va);
d6261 1
a6261 1
		if ((unsigned int)rp < KERNBASE)
d6266 1
a6266 1
			if ((unsigned int)sp < KERNBASE)
d6575 2
a6576 2
	printf("Testing kernel region 0x%x: ", VA_VREG(KERNBASE));
	test_region(VA_VREG(KERNBASE), 4096, avail_start);
d6624 1
a6624 1
	curtbl = ((curpte & ~0x3) << 4) | KERNBASE; /* correct for krn*/
d6638 1
a6638 1
	curtbl = ((curpte & ~0x3) << 4) | KERNBASE; /* correct for krn*/
d6652 1
a6652 1
	curtbl = ((curpte & ~0x3) << 4) | KERNBASE; /* correct for krn*/
d6693 1
a6693 1
			if (reg == VA_VREG(KERNBASE))
@


1.141
log
@Remove workarounds for old compilers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.140 2005/03/07 16:43:54 miod Exp $	*/
d382 1
d384 1
a513 1
void		(*pmap_kremove_p)(vaddr_t, vsize_t);
a868 7
#if defined(SUN4M)
	if (CPU_ISSUN4M) {
		panic("mmu_reservemon4_4c called on Sun4M machine");
		return;
	}
#endif

a2676 1
	pmap_kremove_p		=	pmap_kremove4_4c;
a2988 1
	pmap_kremove_p		=	pmap_kremove4m;
d3431 2
a3439 19
	pmap_pinit(pm);
	return (pm);
}

/*
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
 */
void
pmap_pinit(pm)
	struct pmap *pm;
{
	int size;
	void *urp;

#ifdef DEBUG
	if (pmapdebug & PDB_CREATE)
		printf("pmap_pinit(%p)\n", pm);
#endif
d3493 1
a3493 1
	return;
d3665 34
a5240 8
void
pmap_kremove4_4c(va, len)
	vaddr_t va;
	vsize_t len;
{
	pmap_remove(pmap_kernel(), va, va + len);
}

a5566 8
void
pmap_kremove4m(va, len)
	vaddr_t va;
	vsize_t len;
{
	pmap_remove(pmap_kernel(), va, va + len);
}

a5723 15
/*
 * Garbage collects the physical map system for
 * pages which are no longer used.
 * Success need not be guaranteed -- that is, there
 * may well be pages which are not referenced, but
 * others may be collected.
 * Called by the pageout daemon when pages are scarce.
 */
/* ARGSUSED */
void
pmap_collect(pm)
	struct pmap *pm;
{
}

a6024 13
 * Turn a cdevsw d_mmap value into a byte address for pmap_enter.
 * XXX	this should almost certainly be done differently, and
 *	elsewhere, or even not at all
 */
paddr_t
pmap_phys_address(x)
	int x;
{

	return (x);
}

/*
d6139 1
d6142 1
a6199 9
}

/*
 * Deactivate the address space of the specified process.
 */
void
pmap_deactivate(p)
	struct proc *p;
{
@


1.140
log
@Always force a null context in kvm_setcache(); no functional change for sun4m.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.139 2005/03/03 19:30:36 miod Exp $	*/
a963 2
/*XXX-GCC!*/mmupcrsave = 0;

a1666 1
/*XXX-GCC!*/gap_start=gap_end=0;
@


1.139
log
@Unbreak sun4/4c; my bad, spotted by nick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.138 2004/12/25 23:02:25 miod Exp $	*/
d6074 1
a6074 1
	int pte;
d6079 1
a6079 2
		int ctx = getcontext4m();

d6113 2
d6133 1
d6137 1
@


1.138
log
@Use list and queue macros where applicable to make the code easier to read;
no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.137 2004/05/08 21:45:26 miod Exp $	*/
d3567 1
a3567 1
		if (!TAILQ_EMPTY(&pm->pm_reglist));
d3570 1
a3570 1
		if (!TAILQ_EMPTY(&pm->pm_seglist));
@


1.137
log
@In pmap_enu*(), be sure to splx() on low memory conditions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.136 2003/12/20 00:49:46 miod Exp $	*/
d1218 2
a1219 1
	if ((me = segm_freelist.tqh_first) != NULL) {
d1249 1
a1249 1
	if ((me = segm_lru.tqh_first) == NULL)
d1457 2
a1458 1
	if ((me = region_freelist.tqh_first) != NULL) {
d1480 1
a1480 1
	if ((me = region_lru.tqh_first) == NULL)
d3567 1
a3567 1
		if (pm->pm_reglist.tqh_first)
d3570 1
a3570 1
		if (pm->pm_seglist.tqh_first)
@


1.136
log
@Pass -Wformat.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.135 2003/11/03 07:01:33 david Exp $	*/
d5087 2
a5088 1
		if (sp == NULL)
d5090 1
d5104 2
a5105 1
		if (pte == NULL)
d5107 1
d5433 2
a5434 1
		if (sp == NULL)
d5436 1
d5448 2
a5449 1
		if (ptd == NULL)
d5451 1
d5466 2
a5467 1
		if (pte == NULL)
d5469 1
@


1.135
log
@spelling fixes (in the comments)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.134 2003/04/06 22:50:37 miod Exp $	*/
d2284 1
a2284 1
			panic("pv_changepte: invalid PTE for 0x%x", va);
d2344 2
a2345 2
			printf("pv_syncflags4m: no pte pmap: 0x%x, va: 0x%x\n",
			       pm, va);
@


1.134
log
@kvm_iocache() is not used anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.133 2003/03/13 22:09:20 miod Exp $	*/
d2705 1
a2705 1
	 * Intialize the kernel pmap.
d3007 1
a3007 1
	 * Intialize the kernel pmap.
@


1.133
log
@Always protect pv_unlink4_4c() with splvm(). This suddenly makes sun4/4c
feel much much happier.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.132 2002/12/14 21:40:52 miod Exp $	*/
a740 1
void	kvm_iocache(caddr_t, int);
a6124 27
}

/*
 * Turn on IO cache for a given (va, number of pages).
 *
 * We just assert PG_NC for each PTE; the addresses must reside
 * in locked kernel space.  A cache flush is also done.
 */
void
kvm_iocache(va, npages)
	caddr_t va;
	int npages;
{

#ifdef SUN4M
	if (CPU_ISSUN4M) /* %%%: Implement! */
		panic("kvm_iocache: 4m iocache not implemented");
#endif
#if defined(SUN4) || defined(SUN4C)
	for (; --npages >= 0; va += NBPG) {
		int pte = getpte4(va);
		if ((pte & PG_V) == 0)
			panic("kvm_iocache !pg_v");
		pte |= PG_IOC;
		setpte4(va, pte);
	}
#endif
@


1.132
log
@Let this compile with option DEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.131 2002/11/11 05:04:46 miod Exp $	*/
d3727 1
d3776 1
d3778 1
d3922 1
d3956 2
a3957 1
				if (pv)
d3959 2
d4021 1
d4023 1
@


1.131
log
@Various little pmap changes:
- get rid of splpmap, use splvm everywhere
- harmonize some 4_4c routines to wake them easier to diff against 4m
- fix an spl botch in can't happen situations on 4_4c
- fix pmap_stas.ps_npg_prot_all on 4m in case someone cares
- slightly better pmap_kenter_pa and pmap_kremove on 4_4c

art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.130 2002/10/28 19:30:21 art Exp $	*/
a4193 2
	if (!pmap_pa_exists(pa))
		panic("pmap_page_protect: no such address: 0x%lx", pa);
d4196 1
a4196 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pa, prot);
d4592 1
a4592 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pa, prot);
@


1.130
log
@Convert sparc pmap from physseg to VM_PAGE_MD.
This allows us to remove some ambiguities on how some functions are called,
remove some diagnostic checks for conditions that can never happen and
remove the ugly hack with "pmap_initialized". It also removes some unnecessary
overhead in time-critical functions like pmap_{zero,copy}_page and
pmap_{is,clear}_{mod*,ref*}.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.129 2002/09/10 18:29:43 art Exp $	*/
d1615 1
a1615 1
		s = splpmap();		/* paranoid */
d1638 1
a1638 1
	s = splpmap();		/* paranoid */
d1680 1
a1680 1
	s = splpmap();
d1910 1
a1910 1
	s = splpmap();			/* paranoid? */
d2009 1
a2009 1
	s = splpmap();			/* paranoid? */
d2161 1
d2193 1
d2238 1
a2238 1
	s = splpmap();			/* paranoid? */
d2326 1
a2326 1
	s = splpmap();			/* paranoid? */
d2554 1
a2554 1
	s = splpmap();		/* XXX extreme paranoia */
d3557 1
a3557 1
	int s = splpmap();	/* paranoia */
d3675 1
a3675 1
	s = splpmap();		/* XXX conservative */
d3929 1
a3932 1

d3985 1
a3986 1
		setcontext4(pm->pm_ctxnum);
d4200 1
a4203 1
	pv = &pg->mdpage.pv_head;
d4218 1
a4218 1
	s = splpmap();
d4226 2
a4227 1
	for (;; pm = pv->pv_pmap) {
d4232 1
d4235 1
d4237 2
a4238 1
		if ((nleft = sp->sg_npte) == 0)
d4240 2
a4241 2
		nleft--;
		sp->sg_npte = nleft;
d4283 1
d4287 1
d4353 1
a4353 2
		if ((pv = npv) == NULL)
			break;
d4356 1
a4356 1
	pv0->pv_next = NULL; /* ? */
d4372 1
a4372 4
pmap_protect4_4c(pm, sva, eva, prot)
	struct pmap *pm;
	vaddr_t sva, eva;
	vm_prot_t prot;
d4389 1
a4389 1
	s = splpmap();
d4414 1
a4414 1
		if (pm != pmap_kernel() && sp->sg_pte == NULL)
d4498 1
a4498 1
	s = splpmap();		/* conservative */
d4502 1
d4507 1
d4614 1
a4614 1
	s = splpmap();
d4719 1
a4719 1
	s = splpmap();
d4752 1
a4752 1
		pmap_stats.ps_npg_prot_all = (nva - va) >> PGSHIFT;
d4810 1
a4810 1
	s = splpmap();		/* conservative */
d4904 1
a4904 1
	if ((flags & PMAP_CANFAIL) == 0 && ret)
d4931 1
a4931 1
	s = splpmap();		/* XXX way too conservative */
d5054 1
a5054 1
	s = splpmap();			/* XXX conservative */
d5202 19
a5220 1
	pmap_enter4_4c(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d5228 1
a5228 3
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
d5336 1
a5336 1
	s = splpmap();		/* XXX way too conservative */
d5547 1
a5547 1
	pmap_enk4m(pmap_kernel(), va, prot, TRUE, pv, pteproto);
d6230 1
a6230 1
	s = splpmap();
d6554 1
a6554 1
	s = splpmap();
@


1.129
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.128 2002/09/06 22:46:48 art Exp $	*/
d175 1
a175 2
pvhead(pnum)
	int pnum;
d183 1
a183 1
	return &vm_physmem[bank].pmseg.pv_head[off];
a239 6
 * XXX - horrible kludge to let us use "managed" pages to map the pv lists.
 *       We check this in pmap_enter to see if it's safe to use pvhead.
 */
static int pmap_initialized = 0;

/*
d740 1
a740 1
void	pv_flushcache(struct pvlist *);
d2544 1
a2544 2
pv_flushcache(pv)
	struct pvlist *pv;
d2546 1
a3359 20
	int n, npages;
	vsize_t size;
	vaddr_t addr;

	npages = 0;
	for (n = 0; n < vm_nphysseg; n++)
		npages += vm_physmem[n].end - vm_physmem[n].start;

	size = npages * sizeof(struct pvlist);
	size = round_page(size);
	addr = uvm_km_zalloc(kernel_map, size);
	if (addr == 0)
		panic("pmap_init: no memory for pv_list");

	for (n = 0; n < vm_nphysseg; n++) {
		vm_physmem[n].pmseg.pv_head = (struct pvlist *)addr;
		addr += (vm_physmem[n].end - vm_physmem[n].start) *
			sizeof(struct pvlist);
	}

a3361 6
	/*
	 * We can set it here since it's only used in pmap_enter to see
	 * if pv lists have been mapped.
	 */
	pmap_initialized = 1;

d4182 1
a4182 3
pmap_page_protect4_4c(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
a4189 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d4199 1
a4199 2
	 * Skip unmanaged pages, or operations that do not take
	 * away write permission.
d4201 2
a4202 2
	pv = pvhead(atop(pa));
	if (!pv || prot & VM_PROT_WRITE)
d4576 1
a4576 3
pmap_page_protect4m(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
a4583 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a4585 2
	if (!pmap_pa_exists(pa))
		panic("pmap_page_protect: no such address: 0x%lx", pa);
d4590 1
a4590 1
	pv = pvhead(atop(pa));
d4592 1
a4592 2
	 * Skip unmanaged pages, or operations that do not take
	 * away write permission.
d4594 1
a4594 1
	if (!pv || prot & VM_PROT_WRITE)
d4883 1
a4883 1
	if (pmap_initialized && (pteproto & PG_TYPE) == PG_OBMEM)
d5262 1
a5262 1
	if ((pteproto & SRMMU_PGTYPE) == PG_SUN4M_OBMEM && pmap_initialized)
d5715 1
a5715 2
pmap_clear_modify4_4c(pg)
	struct vm_page *pg;
a5717 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5720 1
a5720 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5733 1
a5733 2
pmap_is_modified4_4c(pg)
	struct vm_page *pg;
a5735 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5737 1
a5737 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5746 1
a5746 2
pmap_clear_reference4_4c(pg)
	struct vm_page *pg;
a5748 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5751 1
a5751 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5764 1
a5764 2
pmap_is_referenced4_4c(pg)
	struct vm_page *pg;
a5766 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5768 1
a5768 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5789 1
a5789 2
pmap_clear_modify4m(pg)
	struct vm_page *pg;
a5791 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5794 1
a5794 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5807 1
a5807 2
pmap_is_modified4m(pg)
	struct vm_page *pg;
a5809 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5811 2
a5812 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5820 1
a5820 2
pmap_clear_reference4m(pg)
	struct vm_page *pg;
a5822 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5825 1
a5825 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
d5838 1
a5838 2
pmap_is_referenced4m(pg)
	struct vm_page *pg;
a5840 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5842 2
a5843 2
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
a5862 1
	struct pvlist *pv;
d5864 7
a5870 8
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL) {
		/*
		 * The following might not be necessary since the page
		 * is being cleared because it is about to be allocated,
		 * i.e., is in use by no one.
		 */
		pv_flushcache(pv);
	}
a5894 1
	struct pvlist *pv;
d5896 2
a5897 3
	pv = pvhead(atop(src));
	if (pv && CACHEINFO.c_vactype == VAC_WRITEBACK)
		pv_flushcache(pv);
d5901 2
a5902 3
	pv = pvhead(atop(dst));
	if (pv && CACHEINFO.c_vactype != VAC_NONE)
		pv_flushcache(pv);
d5928 2
a5930 3
	struct pvlist *pv;
	static int *ptep;
	static vaddr_t va;
d5935 1
a5935 2
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL &&
	    CACHEINFO.c_vactype != VAC_NONE) {
d5941 1
a5941 1
		pv_flushcache(pv);
a5971 2
	int spte, dpte;
	struct pvlist *pv;
d5974 1
d5981 2
a5982 3
	pv = pvhead(atop(src));
	if (pv && CACHEINFO.c_vactype == VAC_WRITEBACK)
		pv_flushcache(pv);
d5987 2
a5988 3
	pv = pvhead(atop(dst));
	if (pv && CACHEINFO.c_vactype != VAC_NONE)
		pv_flushcache(pv);
@


1.128
log
@Fix pmap_protect for VM_PROT_EXECUTE
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.127 2002/08/08 17:40:27 jason Exp $	*/
a513 1
void		(*pmap_copy_page_p)(paddr_t, paddr_t);
d522 2
a523 1
void            (*pmap_zero_page_p)(paddr_t);
d5923 1
a5923 2
pmap_zero_page4_4c(pa)
	paddr_t pa;
d5925 1
d5956 1
a5956 2
pmap_copy_page4_4c(src, dst)
	paddr_t src, dst;
d5958 2
d5995 1
a5995 2
pmap_zero_page4m(pa)
	paddr_t pa;
d5997 1
d6040 1
a6040 2
pmap_copy_page4m(src, dst)
	paddr_t src, dst;
d6042 2
@


1.127
log
@call setcontext{4,4m} directly when we know the CPU type already
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.126 2002/07/24 00:55:52 art Exp $	*/
d4734 1
a4734 4
pmap_protect4m(pm, sva, eva, prot)
	struct pmap *pm;
	vaddr_t sva, eva;
	vm_prot_t prot;
d4740 1
a4740 3

	if (pm == NULL || prot & VM_PROT_WRITE)
		return;
d4747 7
d4791 2
a4792 1
			int tpte;
d4794 7
d4803 1
a4803 2
			 * tags are updated.  This is really only
			 * needed for PTEs that lose PG_W.
d4805 2
a4806 4
			if ((tpte & (PPROT_WRITE|SRMMU_PGTYPE)) ==
			    (PPROT_WRITE|PG_SUN4M_OBMEM)) {
				pmap_stats.ps_npg_prot_actual++;
				if (pm->pm_ctx) {
a4807 1
					tlb_flush_page(va);
d4809 1
a4809 2
				setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)],
					 tpte & ~PPROT_WRITE);
d4811 1
@


1.126
log
@Support for non-exec mappings on sun4m.

 - support exec traps and deal with them correctly.
 - Instead of pretending that the pte permissions are a bit-mask, just
   make two stupid 8-entry tables (one for kernel, one for userland) that
   provides translation between VM_PROT* masks and pte permissions.

This gives sun4m a non-exec stack.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.125 2002/07/09 18:08:18 art Exp $	*/
d1852 1
a1852 1
			setcontext(0);
d5563 1
a5563 1
	setcontext(ctx);
@


1.125
log
@typo in panic message.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.124 2002/07/09 18:06:08 art Exp $	*/
d656 52
d3272 1
d4837 1
a4837 4
	if (pm == pmap_kernel())
		newprot = prot & VM_PROT_WRITE ? PPROT_N_RWX : PPROT_N_RX;
	else
		newprot = prot & VM_PROT_WRITE ? PPROT_RWX_RWX : PPROT_RX_RX;
d5291 1
a5291 2
	/* Make sure we get a pte with appropriate perms! */
	pteproto |= SRMMU_TEPTE | PPROT_RX_RX;
d5306 2
a5307 2
	if (prot & VM_PROT_WRITE)
		pteproto |= PPROT_WRITE;
d5311 1
a5311 1
		ret = pmap_enk4m(pm, va, prot, flags, pv, pteproto | PPROT_S);
@


1.124
log
@When building ptes manually for pmap_zero_page4m and pmap_copy_page4m
use explicit PPROT_N_RX and PPROT_N_RWX, don't treat the pte permissions
as bitmasks (they aren't).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.123 2002/04/18 05:44:35 deraadt Exp $	*/
d5267 1
a5267 1
		panic("pmap_enter4_4c: can't fail, but did");
@


1.123
log
@call pool_get() with PR_NOWAIT.  allocation can fail with low memory, but
will work in contexts with curproc is NULL.  from art.  going into 3.1
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.122 2002/03/31 21:38:10 miod Exp $	*/
d5960 2
a5961 2
	pte = (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
	       (atop(pa) << SRMMU_PPNSHIFT));
d6001 2
a6002 2
	spte = SRMMU_TEPTE | SRMMU_PG_C | PPROT_S |
		(atop(src) << SRMMU_PPNSHIFT);
d6008 1
a6008 2
	dpte = (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
	       (atop(dst) << SRMMU_PPNSHIFT));
@


1.122
log
@Fix a spl botch for sun4/sun4c, which should fix reported stability issues
on some machines.
Tested on ss2 only.
ok art@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.121 2002/03/14 01:26:44 millert Exp $	*/
a165 2
static __inline struct pvlist *pvalloc(void);
static __inline void pvfree(struct pvlist *);
a188 18
/*
 * Wrappers around some memory allocation.
 * XXX - the plan is to make them non-sleeping.
 */

static __inline struct pvlist *
pvalloc()
{
	return pool_get(&pvpool, PR_WAITOK);
}

static __inline void
pvfree(pv)
	struct pvlist *pv;
{
	pool_put(&pvpool, pv);
}

d2050 1
a2050 1
			pvfree(npv);
d2074 1
a2074 1
		pvfree(npv);
d2147 3
a2149 1
	npv = pvalloc();
d2363 1
a2363 1
			pvfree(npv);
d2387 1
a2387 1
		pvfree(npv);
a2425 1
retry:
d2443 3
a2445 10
	mpv = pvalloc();
	if (pv->pv_pmap == NULL) {
		/*
		 * XXX - remove this printf some day when we know that
		 * can/can't happen.
		 */
		printf("pv_link4m: pv changed during sleep!\n");
		pvfree(mpv);
		goto retry;
	}
d4328 1
a4328 1
			pvfree(pv);
d4661 1
a4661 1
			pvfree(pv);
d5174 1
a5174 1
	return (KERN_SUCCESS);
d5490 1
a5490 1
	return (KERN_SUCCESS);
@


1.121
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.120 2002/03/13 00:24:21 miod Exp $	*/
a1711 1
		splx(s);
d1738 1
@


1.120
log
@On sparc, PAGE_SIZE and friends might not be a compile-time constant.
Instead of using a homegrown set of variables in this case, rely on uvmexp
fields once uvm has been initialized.

This requires a few #include <uvm/uvm_extern.h> here and there in the kernel
as well.

Idea from art@@, changes by me.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.119 2002/01/24 10:15:07 art Exp $	*/
d165 3
a167 3
static __inline struct pvlist *pvhead __P((int));
static __inline struct pvlist *pvalloc __P((void));
static __inline void pvfree __P((struct pvlist *));
d170 1
a170 1
static u_int	VA2PA __P((caddr_t));
d224 1
a224 1
void    pcache_flush __P((caddr_t, caddr_t, int));
d230 1
a230 1
        void (*f)__P((int,int)) = cpuinfo.pcache_flush_line;
d405 3
a407 3
static void pmap_page_upload __P((paddr_t));
void pmap_pinit __P((pmap_t));
void pmap_release __P((pmap_t));
d465 3
a467 3
u_int	*getptep4m __P((struct pmap *, vaddr_t));
static __inline void	setpgt4m __P((int *, int));
void	setpte4m __P((vaddr_t va, int pte));
d489 15
a503 15
static void mmu_setup4m_L1 __P((int, struct pmap *));
static void mmu_setup4m_L2 __P((int, struct regmap *));
static void  mmu_setup4m_L3 __P((int, struct segmap *));
void	mmu_reservemon4m __P((struct pmap *));

void	pmap_rmk4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_rmu4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
int	pmap_enk4m __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
int	pmap_enu4m __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
void	pv_changepte4m __P((struct pvlist *, int, int));
int	pv_syncflags4m __P((struct pvlist *));
int	pv_link4m __P((struct pvlist *, struct pmap *, vaddr_t, int));
void	pv_unlink4m __P((struct pvlist *, struct pmap *, vaddr_t));
d507 11
a517 11
void	mmu_reservemon4_4c __P((int *, int *));
void	pmap_rmk4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_rmu4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
int	pmap_enk4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
int	pmap_enu4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
void	pv_changepte4_4c __P((struct pvlist *, int, int));
int	pv_syncflags4_4c __P((struct pvlist *));
int	pv_link4_4c __P((struct pvlist *, struct pmap *, vaddr_t, int));
void	pv_unlink4_4c __P((struct pvlist *, struct pmap *, vaddr_t));
d532 13
a544 13
boolean_t	(*pmap_clear_modify_p) __P((struct vm_page *));
boolean_t	(*pmap_clear_reference_p) __P((struct vm_page *));
void		(*pmap_copy_page_p) __P((paddr_t, paddr_t));
int		(*pmap_enter_p) __P((pmap_t, vaddr_t, paddr_t, vm_prot_t, int));
boolean_t	(*pmap_extract_p) __P((pmap_t, vaddr_t, paddr_t *));
boolean_t	(*pmap_is_modified_p) __P((struct vm_page *));
boolean_t	(*pmap_is_referenced_p) __P((struct vm_page *));
void		(*pmap_kenter_pa_p) __P((vaddr_t, paddr_t, vm_prot_t));
void		(*pmap_kremove_p) __P((vaddr_t, vsize_t));
void		(*pmap_page_protect_p) __P((struct vm_page *, vm_prot_t));
void		(*pmap_protect_p) __P((pmap_t, vaddr_t, vaddr_t, vm_prot_t));
void            (*pmap_zero_page_p) __P((paddr_t));
void	       	(*pmap_changeprot_p) __P((pmap_t, vaddr_t, vm_prot_t, int));
d546 2
a547 2
void 		(*pmap_rmk_p) __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void 		(*pmap_rmu_p) __P((struct pmap *, vaddr_t, vaddr_t, int, int));
d712 9
a720 9
static void sortm __P((struct memarr *, int));
void	ctx_alloc __P((struct pmap *));
void	ctx_free __P((struct pmap *));
void	pv_flushcache __P((struct pvlist *));
void	kvm_iocache __P((caddr_t, int));
#ifdef DEBUG
void	pm_check __P((char *, struct pmap *));
void	pm_check_k __P((char *, struct pmap *));
void	pm_check_u __P((char *, struct pmap *));
d1149 4
a1152 4
struct mmuentry *me_alloc __P((struct mmuhd *, struct pmap *, int, int));
void		me_free __P((struct pmap *, u_int));
struct mmuentry	*region_alloc __P((struct mmuhd *, struct pmap *, int));
void		region_free __P((struct pmap *, u_int));
d2562 1
a2562 1
static void pmap_bootstrap4m __P((void));
d2565 1
a2565 1
static void pmap_bootstrap4_4c __P((int, int, int));
d6463 1
a6463 1
	int (*dump)	__P((dev_t, daddr_t, caddr_t, size_t));
d6622 1
a6622 1
static void test_region __P((int, int, int));
@


1.119
log
@Fix a few typos
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.118 2002/01/23 00:39:47 art Exp $	*/
d2579 1
d2581 1
a2581 1
	uvmexp.pagesize = NBPG;
d2586 1
a2586 1
	nptesg = (NBPSG >> pgshift);
@


1.118
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.117 2001/12/19 08:58:05 art Exp $	*/
d254 1
a254 1
pgt_page_free(struct pool *pp, void *v);
d3378 1
a3378 1
		    &pgt_page_allocator);
d3382 1
a3382 1
                    &pgt_page_allocator);
@


1.117
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2001/11/22 09:54:04 art Exp $	*/
d217 6
a222 2
void *pgt_page_alloc __P((unsigned long, int, int));
void  pgt_page_free __P((void *, unsigned long, int));
d240 1
a240 4
pgt_page_alloc(sz, flags, mtype)
        unsigned long sz;
        int flags;
        int mtype;
d245 1
a245 2
                                      (vsize_t)sz, UVM_KMF_NOWAIT);

d247 2
a248 2
                pcache_flush(p, (caddr_t)VA2PA(p), sz);
                kvm_uncache(p, atop(sz));
d254 1
a254 4
pgt_page_free(v, sz, mtype)
        void *v;
        unsigned long sz;
        int mtype;
d256 1
a256 1
        uvm_km_free(kernel_map, (vaddr_t)v, sz);
d3359 1
a3359 2
	pool_init(&pvpool, sizeof(struct pvlist), 0, 0, 0, "pvpl", 0,
	    NULL, NULL, 0);
d3377 2
a3378 2
                pool_init(&L1_pool, n, n, 0, 0, "L1 pagetable", 0,
                          pgt_page_alloc, pgt_page_free, 0);
d3381 2
a3382 2
                pool_init(&L23_pool, n, n, 0, 0, "L2/L3 pagetable", 0,
                          pgt_page_alloc, pgt_page_free, 0);
@


1.116
log
@Even more ansification.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.115 2001/12/09 04:44:20 art Exp $	*/
d165 7
a171 1
static __inline struct pvlist *pvhead(int);
d177 2
a178 1
pvhead(int pnum)
d191 18
d217 14
a230 2
void *pgt_page_alloc(unsigned long, int, int);
void  pgt_page_free(void *, unsigned long, int);
d236 15
a250 23
pgt_page_alloc(unsigned long sz, int flags, int mtype)
{
	struct vm_page *pg;
	int nocache = (cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0;
	vaddr_t va;
	paddr_t pa;

	if ((pg = uvm_pagealloc(NULL, 0, NULL, 0)) == NULL)
		return (NULL);

	if ((va = uvm_km_valloc(kernel_map, PAGE_SIZE)) == 0) {
		uvm_pagefree(pg);
		return (NULL);
	}

	pa = VM_PAGE_TO_PHYS(pg);
	if (nocache)
		pcache_flush_page(pa, 1);

	pmap_kenter_pa(va, pa | (nocache ? PMAP_NC : 0),
	    VM_PROT_READ|VM_PROT_WRITE);
	pmap_update(pmap_kernel());
        return ((void *)va);
d254 4
a257 1
pgt_page_free(void *v, unsigned long sz, int mtype)
d259 1
a259 8
	vaddr_t va = (vaddr_t)v;
	paddr_t pa;

	if (pmap_extract(pmap_kernel(), va, &pa) == FALSE)
		panic("pgt_page_free");
	uvm_pagefree(PHYS_TO_VM_PAGE(pa));
	pmap_kremove(va, sz);
	uvm_km_free(kernel_map, (vaddr_t)v, sz);
d408 3
a410 3
static void pmap_page_upload(paddr_t);
void pmap_pinit(pmap_t);
void pmap_release(pmap_t);
d468 3
a470 3
u_int	*getptep4m(struct pmap *, vaddr_t);
static __inline void setpgt4m(int *, int);
void	setpte4m(vaddr_t va, int pte);
d492 15
a506 15
static void mmu_setup4m_L1(int, struct pmap *);
static void mmu_setup4m_L2(int, struct regmap *);
static void  mmu_setup4m_L3(int, struct segmap *);
void	mmu_reservemon4m(struct pmap *);

void	pmap_rmk4m(struct pmap *, vaddr_t, vaddr_t, int, int);
void	pmap_rmu4m(struct pmap *, vaddr_t, vaddr_t, int, int);
int	pmap_enk4m(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
int	pmap_enu4m(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
void	pv_changepte4m(struct pvlist *, int, int);
int	pv_syncflags4m(struct pvlist *);
int	pv_link4m(struct pvlist *, struct pmap *, vaddr_t, int);
void	pv_unlink4m(struct pvlist *, struct pmap *, vaddr_t);
d510 11
a520 11
void	mmu_reservemon4_4c(int *, int *);
void	pmap_rmk4_4c(struct pmap *, vaddr_t, vaddr_t, int, int);
void	pmap_rmu4_4c(struct pmap *, vaddr_t, vaddr_t, int, int);
int	pmap_enk4_4c(struct pmap *, vaddr_t, vm_prot_t, int, struct pvlist *,
		int);
int	pmap_enu4_4c(struct pmap *, vaddr_t, vm_prot_t, int, struct pvlist *,
		int);
void	pv_changepte4_4c(struct pvlist *, int, int);
int	pv_syncflags4_4c(struct pvlist *);
int	pv_link4_4c(struct pvlist *, struct pmap *, vaddr_t, int);
void	pv_unlink4_4c(struct pvlist *, struct pmap *, vaddr_t);
d535 13
a547 13
boolean_t	(*pmap_clear_modify_p)(struct vm_page *);
boolean_t	(*pmap_clear_reference_p)(struct vm_page *);
void		(*pmap_copy_page_p)(paddr_t, paddr_t);
int		(*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t	(*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *);
boolean_t	(*pmap_is_modified_p)(struct vm_page *);
boolean_t	(*pmap_is_referenced_p)(struct vm_page *);
void		(*pmap_kenter_pa_p)(vaddr_t, paddr_t, vm_prot_t);
void		(*pmap_kremove_p)(vaddr_t, vsize_t);
void		(*pmap_page_protect_p)(struct vm_page *, vm_prot_t);
void		(*pmap_protect_p)(pmap_t, vaddr_t, vaddr_t, vm_prot_t);
void            (*pmap_zero_page_p)(paddr_t);
void	       	(*pmap_changeprot_p)(pmap_t, vaddr_t, vm_prot_t, int);
d549 2
a550 2
void 		(*pmap_rmk_p)(struct pmap *, vaddr_t, vaddr_t, int, int);
void 		(*pmap_rmu_p)(struct pmap *, vaddr_t, vaddr_t, int, int);
a564 1
static u_int	VA2PA(caddr_t);
d587 2
a588 1
VA2PA(caddr_t addr)
d627 3
a629 1
getptep4m(struct pmap *pm, vaddr_t va)
d657 3
a659 1
setpgt4m(int *ptep, int pte)
d668 3
a670 1
setpte4m(vaddr_t va, int pte)
d715 9
a723 8
static void sortm(struct memarr *, int);
void	ctx_alloc(struct pmap *);
void	ctx_free(struct pmap *);
void	pv_flushcache(struct pvlist *);
#ifdef DEBUG
void	pm_check(char *, struct pmap *);
void	pm_check_k(char *, struct pmap *);
void	pm_check_u(char *, struct pmap *);
d730 3
a732 1
sortm(struct memarr *mp, int n)
d756 8
d768 3
a770 1
pmap_virtual_space(vaddr_t *v_start, vaddr_t *v_end)
d780 2
a781 1
pmap_page_upload(paddr_t first_pa)
d814 2
a815 1
pmap_pa_exists(paddr_t pa)
d837 2
a838 1
mmu_reservemon4_4c(int *nrp, int *nsp)
d936 2
a937 1
mmu_reservemon4m(struct pmap *kpmap)
d943 2
d995 3
a997 1
mmu_setup4m_L1(int regtblptd, struct pmap *kpmap)
d1068 3
a1070 1
mmu_setup4m_L2(int segtblptd, struct regmap *rp)
d1593 1
a1593 1
		s = splvm();		/* paranoid */
d1616 1
a1616 1
	s = splvm();		/* paranoid */
d1658 1
a1658 1
	s = splvm();
d1888 1
a1888 1
	s = splvm();			/* paranoid? */
d1987 1
a1987 1
	s = splvm();			/* paranoid? */
d2073 1
a2073 1
			pool_put(&pvpool, npv);
d2097 1
a2097 1
		pool_put(&pvpool, npv);
d2170 1
a2170 3
	npv = pool_get(&pvpool, PR_NOWAIT);
	if (npv == NULL)
		panic("pvpool exhausted");
d2194 5
d2201 3
a2203 1
pv_changepte4m(struct pvlist *pv0, int bis, int bic)
d2212 1
a2212 1
	s = splvm();			/* paranoid? */
d2265 13
d2300 1
a2300 1
	s = splvm();			/* paranoid? */
d2384 1
a2384 1
			pool_put(&pvpool, npv);
d2408 1
a2408 1
		pool_put(&pvpool, npv);
d2442 1
a2442 1
	struct pvlist *npv;
d2447 1
d2462 15
d2514 5
a2518 8
	npv = pool_get(&pvpool, PR_NOWAIT);
	if (npv == NULL)
		panic("pvpool exhausted");
	npv->pv_next = pv->pv_next;
	npv->pv_pmap = pm;
	npv->pv_va = va;
	npv->pv_flags = nc ? 0 : PV_C4M;
	pv->pv_next = npv;
d2536 1
a2536 1
	s = splvm();		/* XXX extreme paranoia */
d3236 5
a3240 26
	if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0) {
		/*
		 * The page tables have been setup. Since we're still
		 * running on the PROM's memory map, the memory we
		 * allocated for our page tables might still be cached.
		 * Flush it now, and don't touch it again until we
		 * switch to our own tables (will be done immediately below).
		 */
		int size = pagetables_end - pagetables_start;

		if (CACHEINFO.c_vactype != VAC_NONE) {
			int va = (vaddr_t)pagetables_start;
			while (size != 0) {
				cache_flush_page(va);
				va += NBPG;
				size -= NBPG;
			}
		} else if (cpuinfo.pcache_flush_page != NULL) {
			int pa = pagetables_start;
			while (size != 0) {
				pcache_flush_page(pa, 0);
				pa += NBPG;
				size -= NBPG;
			}
		}
	}
d3276 60
d3392 29
a3435 1
	pmap_update(pmap_kernel());
d3564 1
a3564 1
	int s = splvm();	/* paranoia */
d3682 1
a3682 1
	s = splvm();		/* XXX conservative */
d4229 1
a4229 1
	s = splvm();
d4357 1
a4357 1
			pool_put(&pvpool, pv);
d4398 1
a4398 1
	s = splvm();
d4507 1
a4507 1
	s = splvm();		/* conservative */
d4627 1
a4627 1
	s = splvm();
d4690 1
a4690 1
			pool_put(&pvpool, pv);
d4730 1
a4730 1
	s = splvm();
d4820 1
a4820 1
	s = splvm();		/* conservative */
d4941 1
a4941 1
	s = splvm();		/* XXX way too conservative */
d4958 1
a4958 1
		}
d5064 1
a5064 1
	s = splvm();			/* XXX conservative */
d5203 1
a5203 1
	return (0);
d5207 4
a5210 1
pmap_kenter_pa4_4c(vaddr_t va, paddr_t pa, vm_prot_t prot)
d5212 1
a5212 78
	struct pmap *pm = pmap_kernel();
	struct regmap *rp;
	struct segmap *sp;
	int vr, vs, i, s;
	int pteproto, ctx;

	pteproto = PG_V | PMAP_T2PTE_4(pa);
	pa &= ~PMAP_TNC_4;
	pteproto |= atop(pa) & PG_PFNUM;
	if (prot & VM_PROT_WRITE)
		pteproto |= PG_W;

	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];

	ctx = getcontext4();
	s = splvm();
#if defined(SUN4_MMU3L)
	if (HASSUN4_MMU3L && rp->rg_smeg == reginval) {
		vaddr_t tva;
		rp->rg_smeg = region_alloc(&region_locked, pm, vr)->me_cookie;
		i = ncontext - 1;
		do {
			setcontext4(i);
			setregmap(va, rp->rg_smeg);
		} while (--i >= 0);

		/* set all PTEs to invalid, then overwrite one PTE below */
		tva = VA_ROUNDDOWNTOREG(va);
		for (i = 0; i < NSEGRG; i++) {
			setsegmap(tva, rp->rg_segmap[i].sg_pmeg);
			tva += NBPSG;
		}
	}
#endif
	KASSERT(sp->sg_pmeg == seginval || (getpte4(va) & PG_V) == 0);
	if (sp->sg_pmeg == seginval) {
		int tva;

		/*
		 * Allocate an MMU entry now (on locked list),
		 * and map it into every context.  Set all its
		 * PTEs invalid (we will then overwrite one, but
		 * this is more efficient than looping twice).
		 */

		sp->sg_pmeg = me_alloc(&segm_locked, pm, vr, vs)->me_cookie;
		rp->rg_nsegmap++;

#if defined(SUN4_MMU3L)
		if (HASSUN4_MMU3L)
			setsegmap(va, sp->sg_pmeg);
		else
#endif
		{
			i = ncontext - 1;
			do {
				setcontext4(i);
				setsegmap(va, sp->sg_pmeg);
			} while (--i >= 0);
		}

		/* set all PTEs to invalid, then overwrite one PTE below */
		tva = VA_ROUNDDOWNTOSEG(va);
		i = NPTESG;
		do {
			setpte4(tva, 0);
			tva += NBPG;
		} while (--i > 0);
	}

	/* ptes kept in hardware only */
	setpte4(va, pteproto);
	sp->sg_npte++;
	splx(s);
	setcontext4(ctx);
d5216 3
a5218 1
pmap_kremove4_4c(vaddr_t va, vsize_t len)
d5220 2
a5221 101
	struct pmap *pm = pmap_kernel();
	struct regmap *rp;
	struct segmap *sp;
	vaddr_t nva, endva;
	int i, tpte, perpage, npg;
	int nleft, pmeg;
	int vr, vs, s, ctx;

	endva = va + len;
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("pmap_kremove(0x%lx, 0x%lx)\n", va, endva);
#endif

	s = splvm();
	ctx = getcontext();
	simple_lock(&pm->pm_lock);
	for (; va < endva; va = nva) {
		/* do one virtual segment at a time */
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		nva = VSTOVA(vr, vs + 1);
		if (nva == 0 || nva > endva)
			nva = endva;

		rp = &pm->pm_regmap[vr];
		sp = &rp->rg_segmap[vs];

		if (rp->rg_nsegmap == 0)
			continue;
		nleft = sp->sg_npte;
		if (nleft == 0)
			continue;
		pmeg = sp->sg_pmeg;
		KASSERT(pmeg != seginval);
		setcontext4(0);
		/* decide how to flush cache */
		npg = (endva - va) >> PGSHIFT;
		if (npg > PMAP_RMK_MAGIC) {
			/* flush the whole segment */
			perpage = 0;
			cache_flush_segment(vr, vs);
		} else {
			/*
			 * flush each page individually;
			 * some never need flushing
			 */
			perpage = (CACHEINFO.c_vactype != VAC_NONE);
		}
		while (va < nva) {
			tpte = getpte4(va);
			if ((tpte & PG_V) == 0) {
				va += NBPG;
				continue;
			}
			if ((tpte & PG_TYPE) == PG_OBMEM) {
				/* if cacheable, flush page as needed */
				if (perpage && (tpte & PG_NC) == 0)
					cache_flush_page(va);
			}
			nleft--;
#ifdef DIAGNOSTIC
			if (nleft < 0)
				panic("pmap_kremove: too many PTEs in segment; "
				      "va 0x%lx; endva 0x%lx", va, endva);
#endif
			setpte4(va, 0);
			va += NBPG;
		}

		/*
		 * If the segment is all gone, remove it from everyone and
		 * free the MMU entry.
		 */

		sp->sg_npte = nleft;
		if (nleft == 0) {
			va = VSTOVA(vr, vs);
#if defined(SUN4_MMU3L)
			if (HASSUN4_MMU3L)
				setsegmap(va, seginval);
			else
#endif
				for (i = ncontext; --i >= 0;) {
					setcontext4(i);
					setsegmap(va, seginval);
				}
			me_free(pm, pmeg);
			if (--rp->rg_nsegmap == 0) {
#if defined(SUN4_MMU3L)
				if (HASSUN4_MMU3L) {
					for (i = ncontext; --i >= 0;) {
						setcontext4(i);
						setregmap(va, reginval);
					}
					/* note: context is 0 */
					region_free(pm, rp->rg_smeg);
				}
#endif
			}
		}
a5222 3
	simple_unlock(&pm->pm_lock);
	setcontext(ctx);
	splx(s);
d5331 1
a5331 1
	s = splvm();		/* XXX way too conservative */
d5519 1
a5519 1
	return (0);
d5523 4
a5526 1
pmap_kenter_pa4m(vaddr_t va, paddr_t pa, vm_prot_t prot)
d5528 2
a5529 4
	int pteproto;
	struct regmap *rp;
	struct segmap *sp;
	int *ptep;
d5532 2
a5533 13
	    PMAP_T2PTE_SRMMU(pa) | SRMMU_TEPTE |
	    ((prot & VM_PROT_WRITE) ? PPROT_N_RWX : PPROT_N_RX) |
	    (atop((pa & ~PMAP_TNC_SRMMU)) << SRMMU_PPNSHIFT);
	rp = &pmap_kernel()->pm_regmap[VA_VREG(va)];
	sp = &rp->rg_segmap[VA_VSEG(va)];
	ptep = &sp->sg_pte[VA_SUN4M_VPG(va)];
#ifdef DIAGNOSTIC
	if ((*ptep & SRMMU_TETYPE) == SRMMU_TEPTE)
		panic("pmap_kenter_pa4m: mapping exists");
#endif
	sp->sg_npte++;
	setpgt4m(ptep, pteproto);
}
d5535 1
a5535 10
void
pmap_kremove4m(vaddr_t va, vsize_t len)
{
	struct pmap *pm = pmap_kernel();
	struct regmap *rp;
	struct segmap *sp;
	vaddr_t endva, nva;
	int vr, vs, ctx;
	int tpte, perpage, npg;
	int nleft;
d5537 1
a5537 10
	endva = va + len;
	ctx = getcontext();
	for (; va < endva; va = nva) {
		/* do one virtual segment at a time */
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		nva = VSTOVA(vr, vs + 1);
		if (nva == 0 || nva > endva) {
			nva = endva;
		}
d5539 1
a5539 4
		rp = &pm->pm_regmap[vr];
		if (rp->rg_nsegmap == 0) {
			continue;
		}
d5541 4
a5544 5
		sp = &rp->rg_segmap[vs];
		nleft = sp->sg_npte;
		if (nleft == 0) {
			continue;
		}
d5546 6
a5551 35
		setcontext4m(0);
		/* decide how to flush cache */
		npg = (nva - va) >> PGSHIFT;
		if (npg > PMAP_RMK_MAGIC) {
			/* flush the whole segment */
			perpage = 0;
			if (CACHEINFO.c_vactype != VAC_NONE) {
				cache_flush_segment(vr, vs);
			}
		} else {

			/*
			 * flush each page individually;
			 * some never need flushing
			 */

			perpage = (CACHEINFO.c_vactype != VAC_NONE);
		}
		for (; va < nva; va += NBPG) {
			tpte = sp->sg_pte[VA_SUN4M_VPG(va)];
			if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE) {
				continue;
			}
			if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
				/* if cacheable, flush page as needed */
				if (perpage && (tpte & SRMMU_PG_C))
					cache_flush_page(va);
			}
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)],
			    SRMMU_TEINVALID);
			nleft--;
		}
		sp->sg_npte = nleft;
	}
	setcontext(ctx);
a5711 18
 * Copy the range specified by src_addr/len
 * from the source map to the range dst_addr/len
 * in the destination map.
 *
 * This routine is only advisory and need not do anything.
 */
/* ARGSUSED */
int pmap_copy_disabled=0;
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	struct pmap *dst_pmap, *src_pmap;
	vaddr_t dst_addr;
	vsize_t len;
	vaddr_t src_addr;
{
}

/*
d5897 2
a5898 1
pmap_zero_page4_4c(paddr_t pa)
a5899 1
	struct pvlist *pv;
a5900 1
	u_int pfn;
d5902 1
d5904 1
a5904 2
	pfn = atop(pa);
	if (pmap_initialized && (pv = pvhead(pfn)) != NULL) {
d5912 1
a5912 1
	pte = PG_V | PG_S | PG_W | PG_NC | (pfn & PG_PFNUM);
d5930 2
a5931 1
pmap_copy_page4_4c(paddr_t src, paddr_t dst)
a5932 1
	struct pvlist *pv;
d5935 1
a5935 1
	u_int pfn;
d5937 2
a5938 2
	pfn = atop(src);
	if ((pv = pvhead(pfn)) != NULL && CACHEINFO.c_vactype == VAC_WRITEBACK)
a5939 1
	spte = PG_V | PG_S | (pfn & PG_PFNUM);
d5941 4
a5944 2
	pfn = atop(dst);
	if ((pv = pvhead(pfn)) != NULL && CACHEINFO.c_vactype != VAC_NONE)
d5946 2
a5947 1
	dpte = PG_V | PG_S | PG_W | PG_NC | (pfn & PG_PFNUM);
d5968 2
a5969 1
pmap_zero_page4m(paddr_t pa)
d5971 1
a5974 2
	u_int pfn;
	int pte;
d5979 8
a5986 6
	pfn = atop(pa);
	if (pmap_initialized && (pv = pvhead(pfn)) != NULL) {
		if (CACHEINFO.c_vactype != VAC_NONE)
			pv_flushcache(pv);
		else
			pcache_flush_page(pa, 1);
d5989 2
a5990 1
	pte = (SRMMU_TEPTE | PPROT_S | PPROT_WRITE | (pfn << SRMMU_PPNSHIFT));
d6013 2
a6014 1
pmap_copy_page4m(paddr_t src, paddr_t dst)
a6019 1
	u_int pfn;
d6026 2
a6027 2
	pfn = atop(src);
	if ((pv = pvhead(pfn)) != NULL && CACHEINFO.c_vactype == VAC_WRITEBACK)
a6028 1
	spte = SRMMU_TEPTE | SRMMU_PG_C | PPROT_S | (pfn << SRMMU_PPNSHIFT);
d6030 5
a6034 2
	pfn = atop(dst);
	if ((pv = pvhead(pfn)) != NULL && CACHEINFO.c_vactype != VAC_NONE)
a6035 1
	dpte = (SRMMU_TEPTE | PPROT_S | PPROT_WRITE | (pfn << SRMMU_PPNSHIFT));
d6037 2
d6063 2
a6064 1
pmap_phys_address(int x)
d6066 1
a6074 2
 * Please do not use this function in new code.
 * Doesn't work on sun4m, nor for pages with multiple mappings.
d6077 4
a6080 1
kvm_uncache(caddr_t va, int npages)
a6081 1
#if defined(SUN4) || defined(SUN4C)
d6084 14
d6099 19
d6119 1
a6119 2
	if (CPU_ISSUN4M) {
		panic("kvm_uncache on 4m");
a6123 1
#ifdef DIAGNOSTIC
d6126 8
a6133 3
#endif
			if ((pv = pvhead(pte & PG_PFNUM)) != NULL) {
				pv_changepte4_4c(pv, PG_NC, 0);
d6135 4
a6138 1
			pte |= PG_NC;
d6147 27
d6175 2
a6176 1
pmap_count_ptes(struct pmap *pm)
d6202 3
a6204 1
pmap_prefer(vaddr_t foff, vaddr_t *vap)
d6222 1
a6222 1
pmap_redzone(void)
d6243 2
a6244 1
pmap_activate(struct proc *p)
d6256 1
a6256 1
	s = splvm();
d6274 2
a6275 1
pmap_deactivate(struct proc *p)
d6284 3
a6286 1
pm_check(char *s, struct pmap *pm)
d6295 3
a6297 1
pm_check_u(char *s, struct pmap *pm)
d6383 3
a6385 1
pm_check_k(char *s, struct pmap *pm) /* Note: not as extensive as pm_check_u. */
d6445 1
a6445 1
pmap_dumpsize(void)
d6464 3
a6466 1
pmap_dumpmmu(int (*dump)(dev_t, daddr_t, caddr_t, size_t), daddr_t blkno)	
d6473 2
a6474 2
	int	buffer[dbtob(1) / sizeof(int)];
	int	*bp, *ep;
d6573 3
a6575 1
pmap_writetext(unsigned char *dst, int ch)
d6580 1
a6580 1
	s = splvm();
d6625 1
a6625 1
static void test_region(int, int, int);
d6628 1
a6628 1
debug_pagetables(void)
d6675 4
a6678 1
VA2PAsw(int ctx, caddr_t addr, int *pte)
d6745 3
a6747 2
void
test_region(int reg, int start, int stop)
d6785 1
a6785 2
void
print_fe_map(void)
@


1.116.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.119 2002/01/24 10:15:07 art Exp $	*/
d192 2
a193 6
void *pgt_page_alloc(struct pool *, int);
void  pgt_page_free(struct pool *, void *);

struct pool_allocator pgt_allocator = {
	pgt_page_alloc, pgt_page_free, 0,
};
d199 1
a199 1
pgt_page_alloc(struct pool *pp, int flags)
d222 1
a222 1
}
d225 1
a225 1
pgt_page_free(struct pool *pp, void *v)
d3238 2
a3239 1
	pool_init(&pvpool, sizeof(struct pvlist), 0, 0, 0, "pvpl", NULL);
d3257 2
a3258 2
                pool_init(&L1_pool, n, n, 0, 0, "L1 pagetable",
		    &pgt_allocator);
d3261 2
a3262 2
                pool_init(&L23_pool, n, n, 0, 0, "L2/L3 pagetable",
                    &pgt_allocator);
@


1.116.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116.2.1 2002/01/31 22:55:23 niklas Exp $	*/
d1872 2
a1873 2
				 * Bizarreness:  we never clear PG_NC on
				 * DVMA pages.
d1875 3
d2178 2
a2179 1
			 * Bizarreness:  we never set PG_C on DVMA pages.
d2181 3
@


1.116.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116.2.2 2002/02/02 03:28:25 art Exp $	*/
d1102 4
a1105 4
struct mmuentry *me_alloc(struct mmuhd *, struct pmap *, int, int);
void		me_free(struct pmap *, u_int);
struct mmuentry	*region_alloc(struct mmuhd *, struct pmap *, int);
void		region_free(struct pmap *, u_int);
d1665 1
a1691 1
		splx(s);
d2477 1
a2477 1
static void pmap_bootstrap4m(void);
d2480 1
a2480 1
static void pmap_bootstrap4_4c(int, int, int);
a2493 1
	extern int nbpg;	/* locore.s */
d2495 1
a2495 1
	uvmexp.pagesize = nbpg;
d2500 1
a2500 1
	nptesg = (NBPSG >> uvmexp.pageshift);
@


1.116.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116.2.3 2002/06/11 03:38:17 art Exp $	*/
d179 1
a179 1
	return &vm_physmem[bank].pgs[off].mdpage.pv_head;
d243 6
d516 1
d525 1
a525 2
void		(*pmap_copy_page_p)(struct vm_page *, struct vm_page *);
void            (*pmap_zero_page_p)(struct vm_page *);
a651 52
/*
 * Translation table for kernel vs. PTE protection bits.
 */
u_int protection_codes[2][8];
#define pte_prot4m(pm, prot) (protection_codes[pm == pmap_kernel()?0:1][prot])

static void
sparc_protection_init4m(void)
{
	u_int prot, *kp, *up;

	kp = protection_codes[0];
	up = protection_codes[1];

	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RWX_RWX;
			break;
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RW_RW;
			break;
		case VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_RX_RX;
			break;
		case VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_R_R;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RWX_RWX;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RW_RW;
			break;
		case VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_X_X;
			break;
		case VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_R_R;
			break;
		}
	}
}

d691 1
a691 1
void	pg_flushcache(struct vm_page *);
d1773 1
a1773 1
			setcontext4(0);
d2440 2
a2441 1
pg_flushcache(struct vm_page *pg)
a2442 1
	struct pvlist *pv = &pg->mdpage.pv_head;
a3181 1
	sparc_protection_init4m();
d3216 20
d3238 6
d4036 3
a4038 1
pmap_page_protect4_4c(struct vm_page *pg, vm_prot_t prot)
d4046 1
d4056 2
a4057 1
	 * Skip operations that do not take away write permission.
d4059 2
a4060 2
	pv = &pg->mdpage.pv_head;
	if (prot & VM_PROT_WRITE)
d4434 3
a4436 1
pmap_page_protect4m(struct vm_page *pg, vm_prot_t prot)
d4444 1
d4447 2
d4453 1
a4453 1
	pv = &pg->mdpage.pv_head;
d4455 2
a4456 1
	 * Skip operations that do not take away write permission.
d4458 1
a4458 1
	if (prot & VM_PROT_WRITE)
d4555 4
a4558 1
pmap_protect4m(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d4564 3
a4566 1
	int newprot;
a4572 7
	/*
	 * Since the caller might request either a removal of PROT_EXECUTE
	 * or PROT_WRITE, we don't attempt to guess what to do, just lower
	 * to read-only and let the real protection be faulted in.
	 */
	newprot = pte_prot4m(pm, VM_PROT_READ);

d4610 1
a4610 2
			int tpte, npte;

a4611 7
			npte = (tpte & ~SRMMU_PROT_MASK) | newprot;

			/* Only do work when needed. */
			if (npte == tpte)
				continue;

			pmap_stats.ps_npg_prot_actual++;
d4614 2
a4615 1
			 * tags are updated.
d4617 4
a4620 2
			if (pm->pm_ctx) {
				if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
d4622 1
d4624 2
a4625 1
				tlb_flush_page(va);
a4626 1
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], npte);
d4658 4
a4661 1
	newprot = pte_prot4m(pm, prot);
d4744 1
a4744 1
	if ((pteproto & PG_TYPE) == PG_OBMEM)
d5289 2
a5290 1
	pteproto |= SRMMU_TEPTE;
d5298 1
a5298 1
	if ((pteproto & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
d5305 2
a5306 2
	/* correct protections */
	pteproto |= pte_prot4m(pm, prot);
d5310 1
a5310 1
		ret = pmap_enk4m(pm, va, prot, flags, pv, pteproto);
d5315 1
a5315 1
		panic("pmap_enter4m: can't fail, but did");
d5612 1
d5830 2
a5831 1
pmap_clear_modify4_4c(struct vm_page *pg)
d5834 1
d5837 2
a5838 1
	pv = &pg->mdpage.pv_head;	
d5851 2
a5852 1
pmap_is_modified4_4c(struct vm_page *pg)
d5855 1
d5857 2
a5858 1
	pv = &pg->mdpage.pv_head;
d5867 2
a5868 1
pmap_clear_reference4_4c(struct vm_page *pg)
d5871 1
d5874 2
a5875 1
	pv = &pg->mdpage.pv_head;
d5888 2
a5889 1
pmap_is_referenced4_4c(struct vm_page *pg)
d5892 1
d5894 2
a5895 1
	pv = &pg->mdpage.pv_head;
d5916 2
a5917 1
pmap_clear_modify4m(struct vm_page *pg)
d5920 1
d5923 2
a5924 1
	pv = &pg->mdpage.pv_head;
d5937 2
a5938 1
pmap_is_modified4m(struct vm_page *pg)
d5941 1
d5943 2
a5944 2
	pv = &pg->mdpage.pv_head;

d5952 2
a5953 1
pmap_clear_reference4m(struct vm_page *pg)
d5956 1
d5959 2
a5960 1
	pv = &pg->mdpage.pv_head;
d5973 2
a5974 1
pmap_is_referenced4m(struct vm_page *pg)
d5977 1
d5979 2
a5980 2
	pv = &pg->mdpage.pv_head;

d5995 1
a5995 1
pmap_zero_page4_4c(struct vm_page *pg)
d5997 1
a5997 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d5999 1
d6002 10
a6011 8
	/*
	 * The following might not be necessary since the page
	 * is being cleared because it is about to be allocated,
	 * i.e., is in use by no one.
	 */
	pg_flushcache(pg);

	pte = PG_V | PG_S | PG_W | PG_NC | (atop(pa) & PG_PFNUM);
d6029 1
a6029 1
pmap_copy_page4_4c(struct vm_page *srcpg, struct vm_page *dstpg)
d6031 1
a6031 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
d6034 1
d6036 9
a6044 9
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pg_flushcache(srcpg);

	spte = PG_V | PG_S | (atop(src) & PG_PFNUM);

	if (CACHEINFO.c_vactype != VAC_NONE)
		pg_flushcache(dstpg);

	dpte = PG_V | PG_S | PG_W | PG_NC | (atop(dst) & PG_PFNUM);
d6065 1
a6065 1
pmap_zero_page4m(struct vm_page *pg)
d6067 2
a6068 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d6070 1
a6070 1
	static int *ptep;
d6076 6
a6081 7
	if (CACHEINFO.c_vactype != VAC_NONE) {
		/*
		 * The following might not be necessary since the page
		 * is being cleared because it is about to be allocated,
		 * i.e., is in use by no one.
		 */
		pg_flushcache(pg);
d6084 1
a6084 2
	pte = (SRMMU_TEPTE | (atop(pa) << SRMMU_PPNSHIFT) | PPROT_N_RWX);

d6107 1
a6107 1
pmap_copy_page4m(struct vm_page *srcpg, struct vm_page *dstpg)
d6109 2
a6110 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
d6113 1
a6113 1
	int spte, dpte;
d6120 9
a6128 8
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pg_flushcache(srcpg);

	spte = SRMMU_TEPTE | SRMMU_PG_C | (atop(src) << SRMMU_PPNSHIFT) |
	    PPROT_N_RX;

	if (CACHEINFO.c_vactype != VAC_NONE)
		pg_flushcache(dstpg);
a6129 1
	dpte = (SRMMU_TEPTE | (atop(dst) << SRMMU_PPNSHIFT) | PPROT_N_RWX);
@


1.116.2.5
log
@Fix some merge problems.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116.2.4 2002/10/29 00:28:10 art Exp $	*/
d237 2
a238 2
	pmap_kremove(va, PAGE_SIZE);
	uvm_km_free(kernel_map, (vaddr_t)v, PAGE_SIZE);
@


1.116.2.6
log
@Add two missing tlb flushes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116.2.5 2002/10/31 21:13:26 art Exp $	*/
a5573 2

	tlb_flush_page(va);	/* XXX - necessary? there is no mapping */
d5589 1
a5589 1
	ctx = getcontext4m();
a5636 1
			tlb_flush_page(va);
d5643 1
a5643 1
	setcontext4m(ctx);
@


1.116.2.7
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2131 1

a2162 1

a3599 1
	int s;
a3647 1
				s = splvm();
a3648 1
				splx(s);
a3791 1
	int s;
a3801 1

d3805 1
d3825 1
a3825 2
				if (pv) {
					s = splvm();
a3826 2
					splx(s);
				}
d3858 1
a3859 1
		npg = (endva - va) >> PGSHIFT;
a3886 1
				s = splvm();
a3887 1
				splx(s);
d4067 2
d4071 1
a4071 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pg, prot);
a4072 1
	pv = &pg->mdpage.pv_head;
d4076 1
d4099 1
a4099 2
	while (pv != NULL) {
		pm = pv->pv_pmap;
a4103 1
#ifdef DIAGNOSTIC
a4105 1
#endif
d4107 1
a4107 2
#ifdef DIAGNOSTIC
		if (sp->sg_npte == 0)
d4109 2
a4110 2
#endif
		nleft = --sp->sg_npte;
a4151 1
#ifdef DIAGNOSTIC
a4154 1
#endif
d4220 2
a4221 1
		pv = npv;
d4224 1
a4224 1
	pv0->pv_next = NULL;
d4240 4
a4243 1
pmap_protect4_4c(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d4285 1
a4285 1
		if (sp->sg_pte == NULL)
a4372 1
		splx(s);
a4376 1
		splx(s);
d4463 1
a4463 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pg, prot);
d4621 1
a4621 1
		pmap_stats.ps_npg_prot_all += (nva - va) >> PGSHIFT;
d4773 1
a4773 1
	if ((flags & PMAP_CANFAIL) == 0 && ret != 0)
a5067 1
#ifdef USE_NETBSD_CODE
a5145 21
#else /* code from main branch */
	struct pvlist *pv;
	int pteproto, ctx;

	pteproto = PG_S | PG_V | PMAP_T2PTE_4(pa);
	if (prot & VM_PROT_WRITE)
		pteproto |= PG_W;

	pa &= ~PMAP_TNC_4;

	if ((pteproto & PG_TYPE) == PG_OBMEM)
		pv = pvhead(atop(pa));
	else
		pv = NULL;

	pteproto |= atop(pa) & PG_PFNUM;

	ctx = getcontext4();
	pmap_enk4_4c(pmap_kernel(), va, prot, PMAP_WIRED, pv, pteproto);
	setcontext4(ctx);
#endif
a5150 1
#ifdef USE_NETBSD_CODE
a5255 3
#else /* code from main branch */
	pmap_remove(pmap_kernel(), va, va + len);
#endif
a5622 1
<<<<<<< pmap.c
a5645 4
=======
	ctx = getcontext4m();
	pmap_enk4m(pmap_kernel(), va, prot, PMAP_WIRED, pv, pteproto);
>>>>>>> 1.134
@


1.116.2.8
log
@fix a missed conflict spotted by todd
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116.2.7 2003/05/19 21:46:33 tedu Exp $	*/
d5662 1
d5686 4
@


1.115
log
@Random ansification.
(Does it show that I'm doing a binary search for a bug in a bigger diff?)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.114 2001/12/09 04:29:51 art Exp $	*/
d791 1
a791 2
mmu_reservemon4_4c(nrp, nsp)
	int *nrp, *nsp;
d889 1
a889 2
mmu_reservemon4m(kpmap)
	struct pmap *kpmap;
a894 2
/*XXX-GCC!*/mmupcrsave = 0;

d945 1
a945 3
mmu_setup4m_L1(regtblptd, kpmap)
	int regtblptd;		/* PTD for region table to be remapped */
	struct pmap *kpmap;
d1016 1
a1016 3
mmu_setup4m_L2(segtblptd, rp)
	int segtblptd;
	struct regmap *rp;
@


1.114
log
@un-__P
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.113 2001/12/09 04:20:42 art Exp $	*/
d602 1
a602 3
getptep4m(pm, va)
        struct pmap *pm;
        vaddr_t va;
d630 1
a630 3
setpgt4m(ptep, pte)
	int *ptep;
	int pte;
d639 1
a639 3
setpte4m(va, pte)
	vaddr_t va;
	int pte;
d684 4
a687 4
static void sortm __P((struct memarr *, int));
void	ctx_alloc __P((struct pmap *));
void	ctx_free __P((struct pmap *));
void	pv_flushcache __P((struct pvlist *));
d689 3
a691 3
void	pm_check __P((char *, struct pmap *));
void	pm_check_k __P((char *, struct pmap *));
void	pm_check_u __P((char *, struct pmap *));
d698 1
a698 3
sortm(mp, n)
	struct memarr *mp;
	int n;
a721 8
 * For our convenience, vm_page.c implements:
 *       vm_bootstrap_steal_memory()
 * using the functions:
 *       pmap_virtual_space(), pmap_free_pages(), pmap_next_page(),
 * which are much simpler to implement.
 */

/*
d726 1
a726 3
pmap_virtual_space(v_start, v_end)
        vaddr_t *v_start;
        vaddr_t *v_end;
d736 1
a736 2
pmap_page_upload(first_pa)
	paddr_t first_pa;
d769 1
a769 2
pmap_pa_exists(pa)
	paddr_t pa;
@


1.113
log
@Explicitly unmap and free the pages in pgt_page_free.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.112 2001/12/09 04:14:14 art Exp $	*/
d383 3
a385 3
static void pmap_page_upload __P((paddr_t));
void pmap_pinit __P((pmap_t));
void pmap_release __P((pmap_t));
d443 3
a445 3
u_int	*getptep4m __P((struct pmap *, vaddr_t));
static __inline void	setpgt4m __P((int *, int));
void	setpte4m __P((vaddr_t va, int pte));
d467 15
a481 15
static void mmu_setup4m_L1 __P((int, struct pmap *));
static void mmu_setup4m_L2 __P((int, struct regmap *));
static void  mmu_setup4m_L3 __P((int, struct segmap *));
void	mmu_reservemon4m __P((struct pmap *));

void	pmap_rmk4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_rmu4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
int	pmap_enk4m __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
int	pmap_enu4m __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
void	pv_changepte4m __P((struct pvlist *, int, int));
int	pv_syncflags4m __P((struct pvlist *));
int	pv_link4m __P((struct pvlist *, struct pmap *, vaddr_t, int));
void	pv_unlink4m __P((struct pvlist *, struct pmap *, vaddr_t));
d485 11
a495 11
void	mmu_reservemon4_4c __P((int *, int *));
void	pmap_rmk4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_rmu4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
int	pmap_enk4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
int	pmap_enu4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int));
void	pv_changepte4_4c __P((struct pvlist *, int, int));
int	pv_syncflags4_4c __P((struct pvlist *));
int	pv_link4_4c __P((struct pvlist *, struct pmap *, vaddr_t, int));
void	pv_unlink4_4c __P((struct pvlist *, struct pmap *, vaddr_t));
d510 13
a522 13
boolean_t	(*pmap_clear_modify_p) __P((struct vm_page *));
boolean_t	(*pmap_clear_reference_p) __P((struct vm_page *));
void		(*pmap_copy_page_p) __P((paddr_t, paddr_t));
int		(*pmap_enter_p) __P((pmap_t, vaddr_t, paddr_t, vm_prot_t, int));
boolean_t	(*pmap_extract_p) __P((pmap_t, vaddr_t, paddr_t *));
boolean_t	(*pmap_is_modified_p) __P((struct vm_page *));
boolean_t	(*pmap_is_referenced_p) __P((struct vm_page *));
void		(*pmap_kenter_pa_p) __P((vaddr_t, paddr_t, vm_prot_t));
void		(*pmap_kremove_p) __P((vaddr_t, vsize_t));
void		(*pmap_page_protect_p) __P((struct vm_page *, vm_prot_t));
void		(*pmap_protect_p) __P((pmap_t, vaddr_t, vaddr_t, vm_prot_t));
void            (*pmap_zero_page_p) __P((paddr_t));
void	       	(*pmap_changeprot_p) __P((pmap_t, vaddr_t, vm_prot_t, int));
d524 2
a525 2
void 		(*pmap_rmk_p) __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void 		(*pmap_rmu_p) __P((struct pmap *, vaddr_t, vaddr_t, int, int));
@


1.112
log
@remove pvalloc and pvfree, use the raw pool_get and pool_put.
Make pv allocation non-waiting. sneak in some ansification.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.111 2001/12/09 03:46:54 art Exp $	*/
d192 2
a193 2
void *pgt_page_alloc __P((unsigned long, int, int));
void  pgt_page_free __P((void *, unsigned long, int));
d199 1
a199 4
pgt_page_alloc(sz, flags, mtype)
        unsigned long sz;
        int flags;
        int mtype;
d225 1
a225 4
pgt_page_free(v, sz, mtype)
        void *v;
        unsigned long sz;
        int mtype;
d227 8
a234 1
        uvm_km_free(kernel_map, (vaddr_t)v, sz);
@


1.111
log
@Real pmap_k*. The sun4/sun4c functions are from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.110 2001/12/08 02:24:07 art Exp $	*/
d165 1
a165 7
static __inline struct pvlist *pvhead __P((int));
static __inline struct pvlist *pvalloc __P((void));
static __inline void pvfree __P((struct pvlist *));

#if defined(SUN4M)
static u_int	VA2PA __P((caddr_t));
#endif
d171 1
a171 2
pvhead(pnum)
	int pnum;
a183 18
/*
 * Wrappers around some memory allocation.
 * XXX - the plan is to make them non-sleeping.
 */

static __inline struct pvlist *
pvalloc()
{
	return pool_get(&pvpool, PR_WAITOK);
}

static __inline void
pvfree(pv)
	struct pvlist *pv;
{
	pool_put(&pvpool, pv);
}

d539 1
d562 1
a562 2
VA2PA(addr)
	caddr_t addr;
d2046 1
a2046 1
			pvfree(npv);
d2070 1
a2070 1
		pvfree(npv);
d2143 3
a2145 1
	npv = pvalloc();
d2339 1
a2339 1
			pvfree(npv);
d2363 1
a2363 1
		pvfree(npv);
d2397 1
a2397 1
	struct pvlist *npv, *mpv;
a2401 1
retry:
a2415 15
	 * We do the malloc early so that we catch all changes that happen
	 * during the (possible) sleep.
	 */
	mpv = pvalloc();
	if (pv->pv_pmap == NULL) {
		/*
		 * XXX - remove this printf some day when we know that
		 * can/can't happen.
		 */
		printf("pv_link4m: pv changed during sleep!\n");
		pvfree(mpv);
		goto retry;
	}

	/*
d2453 8
a2460 5
	mpv->pv_next = pv->pv_next;
	mpv->pv_pmap = pm;
	mpv->pv_va = va;
	mpv->pv_flags = nc ? 0 : PV_C4M;
	pv->pv_next = mpv;
d4232 1
a4232 1
			pvfree(pv);
d4565 1
a4565 1
			pvfree(pv);
d4833 1
a4833 1
		};
@


1.110
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.109 2001/12/07 18:06:49 mickey Exp $	*/
d5118 1
a5118 4
pmap_kenter_pa4_4c(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d5120 78
a5197 1
	pmap_enter4_4c(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d5201 1
a5201 3
pmap_kremove4_4c(va, len)
	vaddr_t va;
	vsize_t len;
d5203 101
a5303 2
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
d5305 3
d5608 1
a5608 4
pmap_kenter_pa4m(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d5610 4
a5613 2
	struct pvlist *pv;
	int pteproto, ctx;
d5616 35
a5650 2
		PMAP_T2PTE_SRMMU(pa) | SRMMU_TEPTE |
		((prot & VM_PROT_WRITE) ? PPROT_N_RWX : PPROT_N_RX);
d5652 4
a5655 1
	pa &= ~PMAP_TNC_SRMMU;
d5657 5
a5661 1
	pteproto |= atop(pa) << SRMMU_PPNSHIFT;
d5663 15
a5677 1
	pv = pvhead(atop(pa));
d5679 18
a5696 2
	ctx = getcontext4m();
	pmap_enk4m(pmap_kernel(), va, prot, TRUE, pv, pteproto);
a5699 8
void
pmap_kremove4m(va, len)
	vaddr_t va;
	vsize_t len;
{
	pmap_remove(pmap_kernel(), va, va + len);
}

d6108 1
a6108 2
	pv = pvhead(pfn);
	if (pv && CACHEINFO.c_vactype != VAC_NONE)
@


1.109
log
@make it compile on SUN4M-only
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.108 2001/12/07 15:11:49 art Exp $	*/
d248 1
d3346 1
@


1.108
log
@Some ansification and cleanups in pmap_{zero,copy}_page*
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.107 2001/12/07 13:43:30 art Exp $	*/
d6000 1
d6003 1
@


1.107
log
@kill kvm_iocache, not used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.106 2001/12/07 10:57:58 art Exp $	*/
d5824 1
a5824 2
pmap_zero_page4_4c(pa)
	paddr_t pa;
d5826 1
d5828 1
a5829 1
	struct pvlist *pv;
d5831 2
a5832 1
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL) {
d5840 1
a5840 1
	pte = PG_V | PG_S | PG_W | PG_NC | (atop(pa) & PG_PFNUM);
d5858 1
a5858 2
pmap_copy_page4_4c(src, dst)
	paddr_t src, dst;
d5860 1
d5863 1
a5863 1
	struct pvlist *pv;
d5865 2
a5866 2
	pv = pvhead(atop(src));
	if (pv && CACHEINFO.c_vactype == VAC_WRITEBACK)
d5868 1
d5870 2
a5871 3
	spte = PG_V | PG_S | (atop(src) & PG_PFNUM);

	pv = pvhead(atop(dst));
d5874 1
a5874 2

	dpte = PG_V | PG_S | PG_W | PG_NC | (atop(dst) & PG_PFNUM);
d5895 1
a5895 2
pmap_zero_page4m(pa)
	paddr_t pa;
a5896 1
	int pte;
d5900 2
d5906 2
a5907 1
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL) {
d5914 1
a5914 2
	pte = (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
	       (atop(pa) << SRMMU_PPNSHIFT));
d5937 1
a5937 2
pmap_copy_page4m(src, dst)
	paddr_t src, dst;
d5943 1
d5950 2
a5951 2
	pv = pvhead(atop(src));
	if (pv && CACHEINFO.c_vactype == VAC_WRITEBACK)
d5953 1
d5955 2
a5956 5
	spte = SRMMU_TEPTE | SRMMU_PG_C | PPROT_S |
		(atop(src) << SRMMU_PPNSHIFT);

	pv = pvhead(atop(dst));
	if (pv && CACHEINFO.c_vactype != VAC_NONE)
d5958 1
a5959 2
	dpte = (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
	       (atop(dst) << SRMMU_PPNSHIFT));
d5984 1
a5984 2
pmap_phys_address(x)
	int x;
a5985 1

d6070 1
a6070 1
pmap_redzone()
d6091 1
a6091 2
pmap_activate(p)
	struct proc *p;
d6121 1
a6121 2
pmap_deactivate(p)
	struct proc *p;
d6130 1
a6130 3
pm_check(s, pm)
	char *s;
	struct pmap *pm;
d6139 1
a6139 3
pm_check_u(s, pm)
	char *s;
	struct pmap *pm;
d6225 1
a6225 3
pm_check_k(s, pm)		/* Note: not as extensive as pm_check_u. */
	char *s;
	struct pmap *pm;
d6285 1
a6285 1
pmap_dumpsize()
d6304 1
a6304 3
pmap_dumpmmu(dump, blkno)
	daddr_t blkno;
	int (*dump)	__P((dev_t, daddr_t, caddr_t, size_t));
d6311 2
a6312 2
	int		buffer[dbtob(1) / sizeof(int)];
	int		*bp, *ep;
d6411 1
a6411 3
pmap_writetext(dst, ch)
	unsigned char *dst;
	int ch;
d6461 1
a6461 1
static void test_region __P((int, int, int));
d6464 1
a6464 1
debug_pagetables()
d6511 1
a6511 4
VA2PAsw(ctx, addr, pte)
	int ctx;
	caddr_t addr;
	int *pte;
d6578 2
a6579 3
void test_region(reg, start, stop)
	int reg;
	int start, stop;
d6617 2
a6618 1
void print_fe_map(void)
@


1.106
log
@Remove 4m bits and put some other nasty restrictions on kvm_uncache so
that noone ever has a thought of using it again.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.105 2001/12/07 10:52:25 art Exp $	*/
a716 1
void	kvm_iocache __P((caddr_t, int));
a6029 27
/*
 * Turn on IO cache for a given (va, number of pages).
 *
 * We just assert PG_NC for each PTE; the addresses must reside
 * in locked kernel space.  A cache flush is also done.
 */
void
kvm_iocache(va, npages)
	caddr_t va;
	int npages;
{

#ifdef SUN4M
	if (CPU_ISSUN4M) /* %%%: Implement! */
		panic("kvm_iocache: 4m iocache not implemented");
#endif
#if defined(SUN4) || defined(SUN4C)
	for (; --npages >= 0; va += NBPG) {
		int pte = getpte4(va);
		if ((pte & PG_V) == 0)
			panic("kvm_iocache !pg_v");
		pte |= PG_IOC;
		setpte4(va, pte);
	}
#endif
}

d6031 1
a6031 2
pmap_count_ptes(pm)
	struct pmap *pm;
d6057 1
a6057 3
pmap_prefer(foff, vap)
	vaddr_t foff;
	vaddr_t *vap;
@


1.105
log
@kvm_recache is now unnecessary, simplify.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.104 2001/12/07 10:44:52 art Exp $	*/
a2191 5
 *
 * In addition, if the cacheable bit (SRMMU_PG_C) is updated in the PTE
 * the corresponding PV_C4M flag is also updated in each pv entry. This
 * is done so kvm_uncache() can use this routine and have the uncached
 * status stick.
d2194 1
a2194 3
pv_changepte4m(pv0, bis, bic)
	struct pvlist *pv0;
	int bis, bic;
a2255 13

		/* Update PV_C4M flag if required */
		/*
		 * XXX - this is incorrect. The PV_C4M means that _this_
		 *       mapping should be kept uncached. This way we
		 *       effectively uncache this pa until all mappings
		 *       to it are gone (see also the XXX in pv_link4m and
		 *       pv_unlink4m).
		 */
		if (bis & SRMMU_PG_C)
			pv->pv_flags |= PV_C4M;
		if (bic & SRMMU_PG_C)
			pv->pv_flags &= ~PV_C4M;
d6000 2
d6010 1
a6010 28
#if defined(SUN4M)
		int ctx = getcontext4m();

		setcontext4m(0);
		for (; --npages >= 0; va += NBPG) {
			int *ptep;

			ptep = getptep4m(pmap_kernel(), (vaddr_t)va);
			pte = *ptep;
#ifdef DIAGNOSTIC
			if ((pte & SRMMU_TETYPE) != SRMMU_TEPTE)
				panic("kvm_uncache: table entry not pte");
#endif
			pv = pvhead((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (pv) {
				pv_changepte4m(pv, 0, SRMMU_PG_C);
			}
			pte &= ~SRMMU_PG_C;
			tlb_flush_page((vaddr_t)va);
			setpgt4m(ptep, pte);

			if ((pte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
				cache_flush_page((int)va);

		}
		setcontext4m(ctx);

#endif
d6015 1
d6018 2
a6019 4

			pv = pvhead(pte & PG_PFNUM);
			/* XXX - we probably don't need to check for OBMEM */
			if ((pte & PG_TYPE) == PG_OBMEM && pv) {
@


1.104
log
@Remove pmap_cache_enable. it was a mistake.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.103 2001/12/07 10:39:46 art Exp $	*/
d6022 1
a6022 4
kvm_setcache(va, npages, cached)
	caddr_t va;
	int npages;
	int cached;
d6043 1
a6043 4
				if (cached)
					pv_changepte4m(pv, SRMMU_PG_C, 0);
				else
					pv_changepte4m(pv, 0, SRMMU_PG_C);
d6045 1
a6045 4
			if (cached)
				pte |= SRMMU_PG_C;
			else
				pte &= ~SRMMU_PG_C;
d6066 1
a6066 4
				if (cached)
					pv_changepte4_4c(pv, 0, PG_NC);
				else
					pv_changepte4_4c(pv, PG_NC, 0);
d6068 1
a6068 4
			if (cached)
				pte &= ~PG_NC;
			else
				pte |= PG_NC;
@


1.103
log
@Zap pmap_alloc_cpu. this is not how we want to do this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.102 2001/12/07 10:38:11 art Exp $	*/
a3348 29

/*
 * Called just after enabling cache (so that CPUFLG_CACHEPAGETABLES is
 * set correctly).
 */
void
pmap_cache_enable()
{
#ifdef SUN4M
	if (CPU_ISSUN4M) {
		int pte;

		/*
		 * Deal with changed CPUFLG_CACHEPAGETABLES.
		 *
		 * If the tables were uncached during the initial mapping
		 * and cache_enable set the flag we recache the tables.
		 */

		pte = getpte4m(pagetables_start);

		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) != 0 &&
		    (pte & SRMMU_PG_C) == 0)
			kvm_recache((caddr_t)pagetables_start,
				    atop(pagetables_end - pagetables_start));
	}
#endif
}

@


1.102
log
@splpmap -> splvm
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.101 2001/12/05 14:40:48 art Exp $	*/
a3294 60
/*
 * Allocate per-CPU page tables.
 * Note: this routine is called in the context of the boot CPU
 * during autoconfig.
 */
void
pmap_alloc_cpu(sc)
	struct cpu_softc *sc;
{
	caddr_t cpustore;
	int *ctxtable;
	int *regtable;
	int *segtable;
	int *pagtable;
	int vr, vs, vpg;
	struct regmap *rp;
	struct segmap *sp;

	/* Allocate properly aligned and physically contiguous memory here */
	cpustore = 0;
	ctxtable = 0;
	regtable = 0;
	segtable = 0;
	pagtable = 0;

	vr = VA_VREG(CPUINFO_VA);
	vs = VA_VSEG(CPUINFO_VA);
	vpg = VA_VPG(CPUINFO_VA);
	rp = &pmap_kernel()->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];

	/*
	 * Copy page tables, then modify entry for CPUINFO_VA so that
	 * it points at the per-CPU pages.
	 */
	bcopy(cpuinfo.L1_ptps, regtable, SRMMU_L1SIZE * sizeof(int));
	regtable[vr] =
		(VA2PA((caddr_t)segtable) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;

	bcopy(rp->rg_seg_ptps, segtable, SRMMU_L2SIZE * sizeof(int));
	segtable[vs] =
		(VA2PA((caddr_t)pagtable) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;

	bcopy(sp->sg_pte, pagtable, SRMMU_L3SIZE * sizeof(int));
	pagtable[vpg] =
		(VA2PA((caddr_t)cpustore) >> SRMMU_PPNPASHIFT) |
		(SRMMU_TEPTE | PPROT_RWX_RWX | SRMMU_PG_C);

	/* Install L1 table in context 0 */
	ctxtable[0] = ((u_int)regtable >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;

	sc->ctx_tbl = ctxtable;
	sc->L1_ptps = regtable;

#if 0
	if ((sc->flags & CPUFLG_CACHEPAGETABLES) == 0) {
		kvm_uncache((caddr_t)0, 1);
	}
#endif
}
@


1.101
log
@Change pcache_flush_line to pcache_flush_page since this is the only way
we use the function.
At the same time fix the pte pool backend functions to allocate and map
the memory themselves.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2001/11/28 15:34:16 art Exp $	*/
d1591 1
a1591 1
		s = splpmap();		/* paranoid */
d1614 1
a1614 1
	s = splpmap();		/* paranoid */
d1656 1
a1656 1
	s = splpmap();
d1886 1
a1886 1
	s = splpmap();			/* paranoid? */
d1985 1
a1985 1
	s = splpmap();			/* paranoid? */
d2210 1
a2210 1
	s = splpmap();			/* paranoid? */
d2298 1
a2298 1
	s = splpmap();			/* paranoid? */
d2534 1
a2534 1
	s = splpmap();		/* XXX extreme paranoia */
d3583 1
a3583 1
	int s = splpmap();	/* paranoia */
d3701 1
a3701 1
	s = splpmap();		/* XXX conservative */
d4248 1
a4248 1
	s = splpmap();
d4417 1
a4417 1
	s = splpmap();
d4526 1
a4526 1
	s = splpmap();		/* conservative */
d4646 1
a4646 1
	s = splpmap();
d4749 1
a4749 1
	s = splpmap();
d4839 1
a4839 1
	s = splpmap();		/* conservative */
d4960 1
a4960 1
	s = splpmap();		/* XXX way too conservative */
d5083 1
a5083 1
	s = splpmap();			/* XXX conservative */
d5350 1
a5350 1
	s = splpmap();		/* XXX way too conservative */
d5431 1
a5431 1
	s = splpmap();			/* XXX conservative */
d6290 1
a6290 1
	s = splpmap();
d6614 1
a6614 1
	s = splpmap();
@


1.100
log
@Make pmap_update functions into nops so that we can have a consistent
pmap_update API (right now it's nop).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2001/11/28 14:13:06 art Exp $	*/
a219 12
void    pcache_flush __P((caddr_t, caddr_t, int));
void
pcache_flush(va, pa, n)
        caddr_t va, pa;
        int     n;
{
        void (*f)__P((int,int)) = cpuinfo.pcache_flush_line;

        while ((n -= 4) >= 0)
                (*f)((u_int)va+n, (u_int)pa+n);
}

d229 4
a232 1
        caddr_t p;
d234 2
a235 2
        p = (caddr_t)uvm_km_kmemalloc(kernel_map, uvm.kernel_object,
                                      (vsize_t)sz, UVM_KMF_NOWAIT);
d237 12
a248 5
        if (p != NULL && ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)) {
                pcache_flush(p, (caddr_t)VA2PA(p), sz);
                kvm_uncache(p, atop(sz));
        }
        return (p);
d3234 26
a3259 5
	if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
		/* Flush page tables from cache */
		pcache_flush((caddr_t)pagetables_start,
			     (caddr_t)VA2PA((caddr_t)pagetables_start),
			     pagetables_end - pagetables_start);
d6016 5
a6020 8
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL &&
	    CACHEINFO.c_vactype != VAC_NONE) {
		/*
		 * The following might not be necessary since the page
		 * is being cleared because it is about to be allocated,
		 * i.e., is in use by no one.
		 */
		pv_flushcache(pv);
@


1.99
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2001/11/28 13:47:39 art Exp $	*/
a5726 14
}

/*
 * Require that all active physical maps contain no
 * incorrect entries NOW.  [This update includes
 * forcing updates of any address map caching.]
 */
void
pmap_update()
{
#if defined(SUN4M)
	if (CPU_ISSUN4M)
		tlb_flush_all();	/* %%%: Extreme Paranoia?  */
#endif
@


1.98
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2001/11/22 09:54:04 art Exp $	*/
a542 1
void		(*pmap_kenter_pgs_p) __P((vaddr_t, struct vm_page **, int));
a2665 1
	pmap_kenter_pgs_p	=	pmap_kenter_pgs4_4c;
a2978 1
	pmap_kenter_pgs_p	=	pmap_kenter_pgs4m;
a5215 14
pmap_kenter_pgs4_4c(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter4_4c(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
                               VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED);
	}
}

void
a5542 44
	setcontext(ctx);
}

void
pmap_kenter_pgs4m(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i, pteproto, pte, ctx;
	struct pvlist *pv;

	/*
	 * The pages will always be "normal" so they can always be
	 * cached.
	 */
	pteproto = SRMMU_PG_C |	SRMMU_TEPTE | PPROT_N_RX;
#if 0
	/*
	 * XXX - make the pages read-only until we know what protection they
	 *       should have.
	 */
	| ((prot & VM_PROT_WRITE) ? PPROT_WRITE : 0);
#endif

	/*
	 * We can do even nastier stuff here. We happen to know that the
	 * pte's are contig in the kernel (don't have to worry about
	 * segment/region boundaries).
	 */
	ctx = getcontext4m();
	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		int pnum;
		paddr_t pa;

		pa = VM_PAGE_TO_PHYS(pgs[i]);
		pnum = atop(pa);

		pte = pteproto | (pnum << SRMMU_PPNSHIFT) |
			PMAP_T2PTE_SRMMU(pa);
		pv = pvhead(pnum);
		pmap_enk4m(pmap_kernel(), va, VM_PROT_READ /* XXX */,
			   TRUE, pv, pte);
	}
@


1.97
log
@Use pmap_kenter_pa in pmap_map..
Remove the (commented out) body of pmap_copy, it's very unlikely that it
will be ever used, and right now it's just confusing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2001/11/22 09:08:32 art Exp $	*/
d4886 1
a4886 1
		return (KERN_FAILURE);
d4917 1
a4917 1
	if ((flags & PMAP_CANFAIL) == 0 && ret != KERN_SUCCESS)
d4971 1
a4971 1
			return (KERN_SUCCESS);
d5045 1
a5045 1
	return (KERN_SUCCESS);
d5095 1
a5095 1
			return (KERN_RESOURCE_SHORTAGE);
d5110 1
a5110 1
			return (KERN_RESOURCE_SHORTAGE);
d5152 1
a5152 1
				return (KERN_SUCCESS);
d5206 1
a5206 1
	return (KERN_SUCCESS);
d5312 1
a5312 1
	if ((flags & PMAP_CANFAIL) == 0 && ret != KERN_SUCCESS)
d5365 1
a5365 1
			return (KERN_SUCCESS);
d5402 1
a5402 1
	return (KERN_SUCCESS);
d5436 1
a5436 1
			return (KERN_RESOURCE_SHORTAGE);
d5449 1
a5449 1
			return (KERN_RESOURCE_SHORTAGE);
d5465 1
a5465 1
			return (KERN_RESOURCE_SHORTAGE);
d5494 1
a5494 1
				return (KERN_SUCCESS);
d5536 1
a5536 1
	return (KERN_SUCCESS);
@


1.96
log
@Now that pmap_enter is allowed to fail in some cases, allow
pmap_enk* and pmap_enu* to fail and return failure to pmap_enter.
pmap_enter will panic when PMAP_CANFAIL is not set, but the alternative
is worse. Besides, it shouldn't happen.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2001/11/22 08:24:08 art Exp $	*/
d3435 1
a3435 1
		pmap_enter(pmap_kernel(), va, pa, prot, PMAP_WIRED);
a5787 49
#if notyet
	struct regmap *rm;
	struct segmap *sm;

	if (pmap_copy_disabled)
		return;
#ifdef DIAGNOSTIC
	if (VA_OFF(src_addr) != 0)
		printf("pmap_copy: addr not page aligned: 0x%lx\n", src_addr);
	if ((len & (NBPG-1)) != 0)
		printf("pmap_copy: length not page aligned: 0x%lx\n", len);
#endif

	if (src_pmap == NULL)
		return;

	if (CPU_ISSUN4M) {
		int i, npg, pte;
		paddr_t pa;

		npg = len >> PGSHIFT;
		for (i = 0; i < npg; i++) {
			tlb_flush_page(src_addr);
			if ((rm = src_pmap->pm_regmap) == NULL)
				continue;
			rm += VA_VREG(src_addr);

			if ((sm = rm->rg_segmap) == NULL)
				continue;
			sm += VA_VSEG(src_addr);
			if (sm->sg_npte == 0)
				continue;

			pte = sm->sg_pte[VA_SUN4M_VPG(src_addr)];
			if ((pte & SRMMU_TETYPE) != SRMMU_TEPTE)
				continue;

			pa = ptoa((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			pmap_enter(dst_pmap, dst_addr,
				   pa,
				   (pte & PPROT_WRITE)
					? (VM_PROT_WRITE | VM_PROT_READ)
					: VM_PROT_READ,
				   0);
			src_addr += NBPG;
			dst_addr += NBPG;
		}
	}
#endif
@


1.95
log
@Use pool to allocate pv list entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2001/11/22 08:11:23 art Exp $	*/
d499 1
a499 1
void	pmap_enk4m __P((struct pmap *, vaddr_t, vm_prot_t,
d501 1
a501 1
void	pmap_enu4m __P((struct pmap *, vaddr_t, vm_prot_t,
d513 1
a513 1
void	pmap_enk4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d515 1
a515 1
void	pmap_enu4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
a2171 1

d4879 1
a4879 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d4892 1
a4892 1
		    pm, va, pa, prot, wired);
d4913 1
a4913 1
		pmap_enk4_4c(pm, va, prot, wired, pv, pteproto | PG_S);
d4915 5
a4919 1
		pmap_enu4_4c(pm, va, prot, wired, pv, pteproto);
d4922 1
a4922 1
	return (KERN_SUCCESS);
d4926 2
a4927 2
void
pmap_enk4_4c(pm, va, prot, wired, pv, pteproto)
d4931 1
a4931 1
	int wired;
d4938 1
d4971 1
a4971 1
			return;
d4976 1
a4976 4
#ifdef DEBUG
printf("pmap_enk: changing existing va=>pa entry: va 0x%lx, pteproto 0x%x\n",
	va, pteproto);
#endif
d5044 2
d5049 2
a5050 2
void
pmap_enu4_4c(pm, va, prot, wired, pv, pteproto)
d5054 1
a5054 1
	int wired;
d5061 1
a5087 1
rretry:
d5093 3
a5095 6
		sp = (struct segmap *)malloc((u_long)size, M_VMPMAP, M_WAITOK);
		if (rp->rg_segmap != NULL) {
printf("pmap_enter: segment filled during sleep\n");	/* can this happen? */
			free(sp, M_VMPMAP);
			goto rretry;
		}
a5103 2

sretry:
d5108 3
a5110 6
		pte = (int *)malloc((u_long)size, M_VMPMAP, M_WAITOK);
		if (sp->sg_pte != NULL) {
printf("pmap_enter: pte filled during sleep\n");	/* can this happen? */
			free(pte, M_VMPMAP);
			goto sretry;
		}
d5152 1
a5152 1
				return;
a5158 4
#if 0
printf("%s[%d]: pmap_enu: changing existing va(0x%x)=>pa entry\n",
	curproc->p_comm, curproc->p_pid, va);
#endif
d5205 2
d5267 1
a5267 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d5272 1
a5272 1
		    pm, va, pa, prot, wired);
a5306 1

d5308 1
a5308 1
		pmap_enk4m(pm, va, prot, wired, pv, pteproto | PPROT_S);
d5310 5
a5314 2
		pmap_enu4m(pm, va, prot, wired, pv, pteproto);

a5320 1

d5323 1
a5323 1
	return (KERN_SUCCESS);
d5327 2
a5328 2
void
pmap_enk4m(pm, va, prot, wired, pv, pteproto)
d5332 1
a5332 1
	int wired;
d5339 1
d5350 1
a5350 1
#ifdef DIAGNOSTIC
d5365 1
a5365 1
			return;
d5370 1
a5370 4
#ifdef DEBUG
printf("pmap_enk4m: changing existing va=>pa entry: va 0x%lx, pteproto 0x%x, "
       "oldpte 0x%x\n", va, pteproto, tpte);
#endif
d5401 2
d5406 2
a5407 2
void
pmap_enu4m(pm, va, prot, wired, pv, pteproto)
d5411 1
a5411 1
	int wired;
d5418 1
a5429 2

rretry:
d5434 3
a5436 6
		sp = (struct segmap *)malloc((u_long)size, M_VMPMAP, M_WAITOK);
		if (rp->rg_segmap != NULL) {
printf("pmap_enu4m: segment filled during sleep\n");	/* can this happen? */
			free(sp, M_VMPMAP);
			goto rretry;
		}
d5442 1
a5442 1
rgretry:
d5447 3
a5449 8
		ptd = pool_get(&L23_pool, PR_WAITOK);
		if (rp->rg_seg_ptps != NULL) {
#ifdef DEBUG
printf("pmap_enu4m: bizarre segment table fill during sleep\n");
#endif
			pool_put(&L23_pool, ptd);
			goto rgretry;
		}
a5458 2

sretry:
d5463 3
a5465 6
		pte = pool_get(&L23_pool, PR_WAITOK);
		if (sp->sg_pte != NULL) {
printf("pmap_enter: pte filled during sleep\n");	/* can this happen? */
			pool_put(&L23_pool, pte);
			goto sretry;
		}
d5494 1
a5494 1
				return;
a5500 5
#ifdef DEBUG
if (pmapdebug & PDB_SWITCHMAP)
printf("%s[%d]: pmap_enu: changing existing va(0x%x)=>pa(pte=0x%x) entry\n",
	curproc->p_comm, curproc->p_pid, (int)va, (int)pte);
#endif
d5535 2
@


1.94
log
@Don't check for PMAP_NC in pmap_zero_page.
Also make the code in there slightly more efficient.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2001/11/22 07:50:10 art Exp $	*/
d189 2
d199 1
a199 1
	return malloc(sizeof(struct pvlist), M_VMPVENT, M_WAITOK);
d206 1
a206 1
	free(pv, M_VMPVENT);
d3365 3
@


1.93
log
@Simplify pmap_is_* and pmap_clear_*.
This should also allow sun4/sun4c to work a bit better.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2001/11/06 19:53:16 miod Exp $	*/
a5256 1
 *	XXX	should have different entry points for changing!
d6064 1
a6064 3
	pv = pvhead(atop(pa));
	if (((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0) && pv &&
	    pmap_initialized) {
d6139 2
a6140 4
	pv = pvhead(atop(pa));
	if (((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0) && pv &&
	    CACHEINFO.c_vactype != VAC_NONE &&
	    pmap_initialized)
d6147 1
@


1.92
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2001/09/19 20:50:57 mickey Exp $	*/
d4214 1
a4214 2
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) ||
	     !pv || prot & VM_PROT_WRITE)
d5897 5
a5901 2
	paddr_t pa;
	boolean_t ret = 0;
d5903 3
a5905 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && pv) {
		(void) pv_syncflags4_4c(pv);
		ret = pv->pv_flags & PV_MOD;
		pv->pv_flags &= ~PV_MOD;
	}
d5918 4
a5921 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d5923 1
a5923 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && pv) {
		if (pv->pv_flags & PV_MOD || pv_syncflags4_4c(pv) & PV_MOD)
			return (1);
	}
	return (0);
d5934 5
a5938 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret = 0;
d5940 3
a5942 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && pv) {
		(void) pv_syncflags4_4c(pv);
		ret = pv->pv_flags & PV_REF;
		pv->pv_flags &= ~PV_REF;
	}
d5955 4
a5958 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d5960 1
a5960 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && pv) {
		if (pv->pv_flags & PV_REF || pv_syncflags4_4c(pv) & PV_REF)
			return (1);
	}
	return (0);
d5983 5
a5987 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret = 0;
d5989 3
a5991 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && pv) {
		(void) pv_syncflags4m(pv);
		ret = pv->pv_flags & PV_MOD4M;
		pv->pv_flags &= ~PV_MOD4M;
	}
d6004 1
a6004 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d6006 3
a6008 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && pv) {
		if (pv->pv_flags & PV_MOD4M || pv_syncflags4m(pv) & PV_MOD4M)
		        return(1);
	}
	return (0);
d6019 5
a6023 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret = 0;
d6025 3
a6027 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && pv) {
		(void) pv_syncflags4m(pv);
		ret = pv->pv_flags & PV_REF4M;
		pv->pv_flags &= ~PV_REF4M;
	}
d6040 1
a6040 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d6042 3
a6044 6
	pv = pvhead(atop(pa));
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && pv) {
		if (pv->pv_flags & PV_REF4M || pv_syncflags4m(pv) & PV_REF4M)
		        return(1);
	}
	return (0);
@


1.91
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2001/08/12 15:19:58 mickey Exp $	*/
a69 2

#include <vm/vm.h>
@


1.90
log
@make it compile
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2001/08/11 23:01:11 art Exp $	*/
a71 1
#include <vm/vm_kern.h>
@


1.89
log
@unnecessary includes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2001/07/25 13:25:33 art Exp $	*/
d72 1
@


1.88
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2001/06/27 18:30:30 art Exp $	*/
a71 3
#include <vm/vm_kern.h>
#include <vm/vm_prot.h>
#include <vm/vm_page.h>
@


1.87
log
@PMAP_NEW is no longer an option on sparc.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2001/06/10 01:45:04 deraadt Exp $	*/
d541 1
a541 2
void		(*pmap_enter_p) __P((pmap_t, vaddr_t, paddr_t, vm_prot_t,
				     boolean_t, vm_prot_t));
d3436 1
a3436 1
		pmap_enter(pmap_kernel(), va, pa, prot, 1, 0);
d4871 2
a4872 2
void
pmap_enter4_4c(pm, va, pa, prot, wired, access_type)
d4877 1
a4877 2
	int wired;
	vm_prot_t access_type;
d4881 1
a4881 3

	if (pm == NULL)
		return;
d4888 1
a4888 1
		return;
d4919 2
d5223 1
a5223 1
	pmap_enter4_4c(pmap_kernel(), va, pa, prot, TRUE, 0);
d5236 1
a5236 1
                               VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
d5266 2
a5267 2
void
pmap_enter4m(pm, va, pa, prot, wired, access_type)
d5272 1
a5272 2
	int wired;
	vm_prot_t access_type;
d5276 1
a5276 3

	if (pm == NULL)
		return;
d5323 1
a5323 1
		if (access_type & VM_PROT_WRITE)
d5325 1
a5325 1
		if (access_type & VM_PROT_READ)
d5330 2
d5856 1
a5856 1
				   0, 0);
@


1.86
log
@Art error #2
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2001/05/10 12:52:35 art Exp $	*/
a537 1
#ifdef PMAP_NEW
a539 4
#else
void		(*pmap_clear_modify_p) __P((paddr_t pa));
void		(*pmap_clear_reference_p) __P((paddr_t pa));
#endif
a543 1
#ifdef PMAP_NEW
a549 5
#else
boolean_t	(*pmap_is_modified_p) __P((paddr_t pa));
boolean_t	(*pmap_is_referenced_p) __P((paddr_t pa));
void		(*pmap_page_protect_p) __P((paddr_t, vm_prot_t));
#endif
a2670 1
#ifdef PMAP_NEW
a2673 1
#endif
a2984 1
#ifdef PMAP_NEW
a2987 1
#endif
a3449 1
#ifdef PMAP_NEW
a3450 4
#else
pmap_create(size)
	vsize_t size;
#endif
a3453 4
#ifndef PMAP_NEW
	if (size)
		return (NULL);
#endif
a4195 1
#ifdef PMAP_NEW
a4197 4
#else
pmap_page_protect4_4c(pa, prot)
	paddr_t pa;
#endif
a4205 1
#ifdef PMAP_NEW
a4206 1
#endif
a4594 1
#ifdef PMAP_NEW
a4596 4
#else
pmap_page_protect4m(pa, prot)
	paddr_t pa;
#endif
a4604 1
#ifdef PMAP_NEW
a4605 1
#endif
a5218 1
#ifdef PMAP_NEW
a5250 1
#endif
a5566 1
#ifdef PMAP_NEW
a5641 1
#endif /* PMAP_NEW */
a5900 1
#ifdef PMAP_NEW
a5903 5
#else
void
pmap_clear_modify4_4c(pa)
	paddr_t pa;
#endif
a5905 1
#ifdef PMAP_NEW
a5907 1
#endif
a5911 1
#ifdef PMAP_NEW
a5912 1
#endif
d5915 1
a5915 1
#ifdef PMAP_NEW
a5916 1
#endif
a5921 1
#ifdef PMAP_NEW
a5924 5
#else
int
pmap_is_modified4_4c(pa)
	paddr_t pa;
#endif
a5926 1
#ifdef PMAP_NEW
a5927 1
#endif
a5939 1
#ifdef PMAP_NEW
a5942 5
#else
void
pmap_clear_reference4_4c(pa)
	paddr_t pa;
#endif
a5944 1
#ifdef PMAP_NEW
a5946 1
#endif
a5950 1
#ifdef PMAP_NEW
a5951 1
#endif
d5954 1
a5954 1
#ifdef PMAP_NEW
a5955 1
#endif
a5960 1
#ifdef PMAP_NEW
a5963 5
#else
int
pmap_is_referenced4_4c(pa)
	paddr_t pa;
#endif
a5965 1
#ifdef PMAP_NEW
a5966 1
#endif
a5990 1
#ifdef PMAP_NEW
a5993 5
#else
void
pmap_clear_modify4m(pa)	   /* XXX %%%: Should service from swpagetbl for 4m */
	paddr_t pa;
#endif
a5995 1
#ifdef PMAP_NEW
a5997 1
#endif
a6001 1
#ifdef PMAP_NEW
a6002 1
#endif
d6005 1
a6005 1
#ifdef PMAP_NEW
a6006 1
#endif
a6011 1
#ifdef PMAP_NEW
a6014 5
#else
int
pmap_is_modified4m(pa) /* Test performance with SUN4M && SUN4/4C. XXX */
	paddr_t pa;
#endif
a6016 1
#ifdef PMAP_NEW
a6017 1
#endif
a6029 1
#ifdef PMAP_NEW
a6032 5
#else
void
pmap_clear_reference4m(pa)
	paddr_t pa;
#endif
a6034 1
#ifdef PMAP_NEW
a6036 1
#endif
a6040 1
#ifdef PMAP_NEW
a6041 1
#endif
d6044 1
a6044 1
#ifdef PMAP_NEW
a6045 1
#endif
a6050 1
#ifdef PMAP_NEW
a6053 5
#else
int
pmap_is_referenced4m(pa)
	paddr_t pa;
#endif
a6055 1
#ifdef PMAP_NEW
a6056 1
#endif
@


1.85
log
@remove the pt{1,23}_{alloc,free} functions now that pool works without
UVM and UVM is required anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2001/05/10 10:39:11 art Exp $	*/
d548 1
a548 1
paddr_t		(*pmap_extract_p) __P((pmap_t, vaddr_t));
d5707 2
a5708 2
paddr_t
pmap_extract4_4c(pm, va)
d5711 1
d5723 1
a5723 1
		return (0);
d5733 1
a5733 1
		return (0);
d5759 1
a5759 1
			return (0);
d5768 1
a5768 1
		return (0);
d5772 2
a5773 1
	return ((tpte << PGSHIFT) | (va & PGOFSET));
d5783 2
a5784 2
paddr_t
pmap_extract4m(pm, va)
d5787 1
d5798 1
a5798 1
		return (0);
d5806 1
a5806 1
		return (0);
d5815 1
a5815 1
		return (0);
d5824 1
a5824 1
		return 0;
d5834 1
a5834 1
		return (0);
d5837 2
a5838 1
	return (ptoa((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT) | VA_OFF(va));
@


1.84
log
@Don't crash in pgt_page_alloc if uvm_km_kmemalloc fails.
(we'll crash somewhere else anyway, but that's a later problem).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2001/05/10 10:34:48 art Exp $	*/
a174 5
static __inline int *pt1_alloc __P((void));
static __inline int *pt23_alloc __P((void));
static __inline void pt1_free __P((int *));
static __inline void pt23_free __P((int *));

a263 26

static __inline int *
pt1_alloc()
{
	return pool_get(&L1_pool, PR_WAITOK);
}

static __inline void
pt1_free(pt)
	int *pt;
{
	pool_put(&L1_pool, pt);
}

static __inline int *
pt23_alloc()
{
	return pool_get(&L23_pool, PR_WAITOK);
}

static __inline void
pt23_free(pt)
	int *pt;
{
	pool_put(&L23_pool, pt);
}
d3538 1
a3538 1
		urp = pt1_alloc();
d3652 1
a3652 1
		pt1_free(pm->pm_reg_ptps);
d4194 1
a4194 1
		pt23_free(pte0);
d4203 1
a4203 1
			pt23_free(rp->rg_seg_ptps);
d4719 1
a4719 1
			pt23_free(sp->sg_pte);
d4728 1
a4728 1
				pt23_free(rp->rg_seg_ptps);
d5502 1
a5502 1
		ptd = pt23_alloc();
d5507 1
a5507 1
			pt23_free(ptd);
d5525 1
a5525 1
		pte = pt23_alloc();
d5528 1
a5528 1
			pt23_free(pte);
@


1.83
log
@UVM is no longer optional on sparc.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2001/05/10 08:43:56 art Exp $	*/
d254 1
a254 1
        if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0) {
@


1.82
log
@oops.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2001/05/09 15:31:27 art Exp $	*/
a75 1
#if defined(UVM)
a77 1
#endif
a217 1
#if defined(UVM)	/* We can only use pool with uvm */
a294 30

#else

static __inline int *
pt1_alloc()
{
	return malloc(SRMMU_L1SIZE * sizeof(int), M_VMPMAP, M_WAITOK);
}

static __inline void
pt1_free(pt)
	int *pt;
{
	free(pt, M_VMPMAP);
}

static __inline int *
pt23_alloc()
{
	return malloc(SRMMU_L2SIZE * sizeof(int), M_VMPMAP, M_WAITOK);
}

static __inline void
pt23_free(pt)
	int *pt;
{
	free(pt, M_VMPMAP);
}

#endif /* !UVM */
a854 1
#if defined(UVM)
a856 4
#else
		vm_page_physload(atop(start), atop(end),
				 atop(start), atop(end));
#endif
a1971 1
#if defined(UVM)
a1974 7
#else
				extern vaddr_t pager_sva, pager_eva;

				if (bic == PG_W &&
				    va >= pager_sva && va < pager_eva)
					continue;
#endif
a2283 1
#if defined(UVM)
a2286 7
#else
			extern vaddr_t pager_sva, pager_eva;

			if ((bic & PPROT_WRITE) &&
			    va >= pager_sva && va < pager_eva)
				continue;
#endif
a2630 1
#if defined(UVM)
a2632 4
#else
	cnt.v_page_size = NBPG;
	vm_set_page_size();
#endif
a3405 1
#if defined(UVM)
a3406 3
#else
	addr = kmem_alloc(kernel_map, size);
#endif
d3422 1
a3422 1
#if defined(SUN4M) && defined(UVM)
@


1.81
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2000/11/22 11:57:04 art Exp $	*/
d5786 1
a5786 1
pmap_unwire(pm, va, wired)
a5788 1
	int wired;
@


1.80
log
@cosmetic.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2000/11/07 11:42:10 art Exp $	*/
d5786 1
a5786 1
pmap_change_wiring(pm, va, wired)
a6274 19

/*
 * Make the specified pages (by pmap, offset) pageable (or not) as requested.
 *
 * A page which is not pageable may not take a fault; therefore, its page
 * table entry must remain valid for the duration (or at least, the trap
 * handler must not call vm_fault).
 *
 * This routine is merely advisory; pmap_enter will specify that these pages
 * are to be wired down (or not) as appropriate.
 */
/* ARGSUSED */
void
pmap_pageable(pm, start, end, pageable)
	struct pmap *pm;
	vaddr_t start, end;
	int pageable;
{
}
@


1.79
log
@Cleanup ctx_free wrt. 4m vs. 4c.
plus some other minor cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2000/02/21 17:08:37 art Exp $	*/
d562 1
a562 1
/*static*/ void	mmu_reservemon4m __P((struct pmap *));
d564 3
a566 3
/*static*/ void pmap_rmk4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
/*static*/ void pmap_rmu4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
/*static*/ void pmap_enk4m __P((struct pmap *, vaddr_t, vm_prot_t,
d568 1
a568 1
/*static*/ void pmap_enu4m __P((struct pmap *, vaddr_t, vm_prot_t,
d570 4
a573 4
/*static*/ void pv_changepte4m __P((struct pvlist *, int, int));
/*static*/ int  pv_syncflags4m __P((struct pvlist *));
/*static*/ int  pv_link4m __P((struct pvlist *, struct pmap *, vaddr_t, int));
/*static*/ void pv_unlink4m __P((struct pvlist *, struct pmap *, vaddr_t));
d577 4
a580 4
/*static*/ void	mmu_reservemon4_4c __P((int *, int *));
/*static*/ void pmap_rmk4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
/*static*/ void pmap_rmu4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
/*static*/ void pmap_enk4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d582 1
a582 1
/*static*/ void pmap_enu4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d584 4
a587 4
/*static*/ void pv_changepte4_4c __P((struct pvlist *, int, int));
/*static*/ int  pv_syncflags4_4c __P((struct pvlist *));
/*static*/ int  pv_link4_4c __P((struct pvlist *, struct pmap *, vaddr_t, int));
/*static*/ void pv_unlink4_4c __P((struct pvlist *, struct pmap *, vaddr_t));
d2138 1
a2138 1
/*static*/ void
d2211 1
a2211 1
/*static*/ int
d2537 1
a2537 1
/*static*/ int
d3846 1
a3846 1
/*static*/ void
d3948 1
a3948 1
/*static*/ void
d4038 1
a4038 1
/*static*/ void
d4196 1
a4196 1
/*static*/ void
@


1.78
log
@The last pieces of hypersparc support.
 - Split get_faultstatus into get_syncflt and get_asyncflt.
 - Get the syncflt status before setting up the trap frame and put the
   results in cpuinfo.syncfltdump (related to the next change).
 - unlock the sfsr/sfva after reading from ASI_SRMMUFP (it gets locked if
   the translation fails).
 - + other interface changes and cleanups.
 (most work from NetBSD).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2000/02/19 22:08:51 art Exp $	*/
a1886 1
	oldc = getcontext();
d1888 3
a1890 1
	if (CACHEINFO.c_vactype != VAC_NONE) {
a1892 1

d1894 4
a1897 1
		CHANGE_CONTEXTS(oldc, newc);
d1899 2
a1900 3
#if defined(SUN4M)
		if (CPU_ISSUN4M)
			tlb_flush_context();
a1901 1
		setcontext(0);
d1903 2
a1904 4
#if defined(SUN4M)
		if (CPU_ISSUN4M) {
			/* Do any cache flush needed on context switch */
			(*cpuinfo.pure_vcache_flush)();
d1907 4
a1910 1
			tlb_flush_context();
a1911 2
#endif
		CHANGE_CONTEXTS(oldc, 0);
d1913 1
d2416 7
a2422 1
		if (ptep == NULL)	/* invalid */
d2424 1
d2434 1
d2443 2
a2444 2
				cache_flush_page(va); /* XXX: do we need this?*/
				tlb_flush_page(va); /* paranoid? */
@


1.77
log
@flush the caches that need flushing on context switch.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2000/02/18 17:05:33 art Exp $	*/
d676 2
@


1.76
log
@setpte4m is now only legal for kernel mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2000/02/04 19:16:13 art Exp $	*/
d1850 2
d1888 3
d1902 2
d6662 2
@


1.75
log
@In cases where we flush a page from the cache and tlb we want to
flush it from the tlb _after_ the cache because some cache flushes can reload
the tlb. (fixes random coredumps on some cpus).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2000/02/04 15:46:09 art Exp $	*/
d537 1
a537 1
__inline void	setpte4m __P((vaddr_t va, int pte));
d742 4
a745 2
/* Set the page table entry for va to pte. */
__inline void
a749 1
	struct pmap *pm;
d752 1
a752 3
	pm = cpuinfo.ctxinfo[getcontext4m()].c_pmap;

	ptep = getptep4m(pm, va);
@


1.75.2.1
log
@Merge in recent code from the trunk
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2000/02/19 22:08:51 art Exp $	*/
d537 1
a537 1
void	setpte4m __P((vaddr_t va, int pte));
d742 2
a743 4
/*
 * Set the page table entry for va to pte. Only legal for kernel mappings.
 */
void
d748 1
d751 3
a753 1
	ptep = getptep4m(pmap_kernel(), va);
a1850 2
		/* Do any cache flush needed on context switch */
		(*cpuinfo.pure_vcache_flush)();
a1886 3
		/* Do any cache flush needed on context switch */
		(*cpuinfo.pure_vcache_flush)();

a1897 2
			/* Do any cache flush needed on context switch */
			(*cpuinfo.pure_vcache_flush)();
a6655 2
			/* Do any cache flush needed on context switch */
			(*cpuinfo.pure_vcache_flush)();
@


1.75.2.2
log
@sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2000/02/21 17:08:37 art Exp $	*/
a675 2
	/* Unlock fault status; required on Hypersparc modules */
	(void)lda(SRMMU_SFSR, ASI_SRMMU);
@


1.75.2.3
log
@Continue the aborted merge of current just before 2.9 was cut into the
SMP branch.  Note that this will not make any progress of SMP functionality,
it is just merging of new code from the trunk into the old branch.
Please do not ask me questions about SMP status because of this mail,
instead go read the archives of smp@@openbsd.org, where I mailed about
these commits some week ago.  Another note: I am doing this in chunks now,
so as to not lock too much of the tree for long times
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2000/11/22 11:57:04 art Exp $	*/
d562 1
a562 1
void	mmu_reservemon4m __P((struct pmap *));
d564 3
a566 3
void	pmap_rmk4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_rmu4m __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_enk4m __P((struct pmap *, vaddr_t, vm_prot_t,
d568 1
a568 1
void	pmap_enu4m __P((struct pmap *, vaddr_t, vm_prot_t,
d570 4
a573 4
void	pv_changepte4m __P((struct pvlist *, int, int));
int	pv_syncflags4m __P((struct pvlist *));
int	pv_link4m __P((struct pvlist *, struct pmap *, vaddr_t, int));
void	pv_unlink4m __P((struct pvlist *, struct pmap *, vaddr_t));
d577 4
a580 4
void	mmu_reservemon4_4c __P((int *, int *));
void	pmap_rmk4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_rmu4_4c __P((struct pmap *, vaddr_t, vaddr_t, int, int));
void	pmap_enk4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d582 1
a582 1
void	pmap_enu4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d584 4
a587 4
void	pv_changepte4_4c __P((struct pvlist *, int, int));
int	pv_syncflags4_4c __P((struct pvlist *));
int	pv_link4_4c __P((struct pvlist *, struct pmap *, vaddr_t, int));
void	pv_unlink4_4c __P((struct pvlist *, struct pmap *, vaddr_t));
d1887 1
d1889 1
a1889 3
	if (CPU_ISSUN4M) {
#if defined(SUN4M)
		oldc = getcontext4m();
d1892 1
d1894 1
a1894 4
		if (oldc != newc) {
			write_user_windows();
			setcontext4m(newc);
		}
d1896 3
a1898 2
		tlb_flush_context();
		setcontext4m(0);
d1900 1
d1902 4
a1905 2
		oldc = getcontext4();
		if (CACHEINFO.c_vactype != VAC_NONE) {
d1908 1
a1908 4
			cache_flush_context();
			setcontext(0);
		} else {
			CHANGE_CONTEXTS(oldc, 0);
d1910 2
a1912 1

d2137 1
a2137 1
void
d2210 1
a2210 1
int
d2415 1
a2415 7

		/*
		 * XXX - This can't happen?!?
		 */
		if (ptep == NULL) {	/* invalid */
			printf("pv_syncflags4m: no pte pmap: 0x%x, va: 0x%x\n",
			       pm, va);
a2416 1
		}
a2425 1
			
d2434 2
a2435 2
				cache_flush_page(va); /* XXX:do we need this?*/
				tlb_flush_page(va);
d2528 1
a2528 1
int
d3837 1
a3837 1
void
d3939 1
a3939 1
void
d4029 1
a4029 1
void
d4187 1
a4187 1
void
@


1.75.2.4
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75.2.3 2001/05/14 21:37:17 niklas Exp $	*/
d76 1
d79 1
d177 5
d220 1
d257 1
a257 1
        if (p != NULL && ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)) {
d272 56
d602 1
d605 4
d612 2
a613 1
boolean_t	(*pmap_extract_p) __P((pmap_t, vaddr_t, paddr_t *));
d620 5
d888 1
d891 4
d2010 1
d2014 7
d2330 1
d2334 7
d2685 1
d2688 4
d2772 1
d2776 1
d3088 1
d3092 1
d3465 1
d3467 3
d3485 1
a3485 1
#if defined(SUN4M)
d3559 1
d3561 4
d3568 4
d3632 1
a3632 1
		urp = pool_get(&L1_pool, PR_WAITOK);
d3746 1
a3746 1
		pool_put(&L1_pool, pm->pm_reg_ptps);
d4288 1
a4288 1
		pool_put(&L23_pool, pte0);
d4297 1
a4297 1
			pool_put(&L23_pool, rp->rg_seg_ptps);
d4314 1
d4317 4
d4329 1
d4331 1
d4720 1
d4723 4
d4735 1
d4737 1
d4813 1
a4813 1
			pool_put(&L23_pool, sp->sg_pte);
d4822 1
a4822 1
				pool_put(&L23_pool, rp->rg_seg_ptps);
d5351 1
d5384 1
d5596 1
a5596 1
		ptd = pool_get(&L23_pool, PR_WAITOK);
d5601 1
a5601 1
			pool_put(&L23_pool, ptd);
d5619 1
a5619 1
		pte = pool_get(&L23_pool, PR_WAITOK);
d5622 1
a5622 1
			pool_put(&L23_pool, pte);
d5701 1
d5777 1
d5786 1
a5786 1
pmap_unwire(pm, va)
d5789 1
d5802 2
a5803 2
boolean_t
pmap_extract4_4c(pm, va, pa)
a5805 1
	paddr_t *pa;
d5817 1
a5817 1
		return (FALSE);
d5827 1
a5827 1
		return (FALSE);
d5853 1
a5853 1
			return (FALSE);
d5862 1
a5862 1
		return (FALSE);
d5866 1
a5866 2
	*pa = ((tpte << PGSHIFT) | (va & PGOFSET));
	return (TRUE);
d5876 2
a5877 2
boolean_t
pmap_extract4m(pm, va, pa)
a5879 1
	paddr_t *pa;
d5890 1
a5890 1
		return (FALSE);
d5898 1
a5898 1
		return (FALSE);
d5907 1
a5907 1
		return (FALSE);
d5916 1
a5916 1
		return FALSE;
d5926 1
a5926 1
		return (FALSE);
d5929 1
a5929 2
	*pa = (ptoa((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT) | VA_OFF(va));
	return (TRUE);
d6034 1
d6038 5
d6045 1
d6048 1
d6053 1
d6055 1
d6058 1
a6058 1

d6060 1
d6066 1
d6070 5
d6077 1
d6079 1
d6092 1
d6096 5
d6103 1
d6106 1
d6111 1
d6113 1
d6116 1
a6116 1

d6118 1
d6124 1
d6128 5
d6135 1
d6137 1
d6162 1
d6166 5
d6173 1
d6176 1
d6181 1
d6183 1
d6186 1
a6186 1

d6188 1
d6194 1
d6198 5
d6205 1
d6207 1
d6220 1
d6224 5
d6231 1
d6234 1
d6239 1
d6241 1
d6244 1
a6244 1

d6246 1
d6252 1
d6256 5
d6263 1
d6265 1
d6275 19
@


1.75.2.5
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75.2.4 2001/07/04 10:23:46 niklas Exp $	*/
d72 3
d541 2
a542 1
int		(*pmap_enter_p) __P((pmap_t, vaddr_t, paddr_t, vm_prot_t, int));
d3437 1
a3437 1
		pmap_enter(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d4872 2
a4873 2
int
pmap_enter4_4c(pm, va, pa, prot, flags)
d4878 2
a4879 1
	int flags;
d4883 3
a4885 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d4892 1
a4892 1
		return (KERN_FAILURE);
a4922 2

	return (KERN_SUCCESS);
d5225 1
a5225 1
	pmap_enter4_4c(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d5238 1
a5238 1
                               VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED);
d5268 2
a5269 2
int
pmap_enter4m(pm, va, pa, prot, flags)
d5274 2
a5275 1
	int flags;
d5279 3
a5281 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d5328 1
a5328 1
		if (flags & VM_PROT_WRITE)
d5330 1
a5330 1
		if (flags & VM_PROT_READ)
a5334 2

	return (KERN_SUCCESS);
d5859 1
a5859 1
				   0);
@


1.75.2.6
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d70 2
@


1.75.2.7
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75.2.6 2001/11/13 21:04:17 niklas Exp $	*/
a188 2
struct pool pvpool;

d197 1
a197 1
	return pool_get(&pvpool, PR_WAITOK);
d204 1
a204 1
	pool_put(&pvpool, pv);
d497 1
a497 1
int	pmap_enk4m __P((struct pmap *, vaddr_t, vm_prot_t,
d499 1
a499 1
int	pmap_enu4m __P((struct pmap *, vaddr_t, vm_prot_t,
d511 1
a511 1
int	pmap_enk4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d513 1
a513 1
int	pmap_enu4_4c __P((struct pmap *, vaddr_t, vm_prot_t,
d541 1
d2170 1
d2666 1
d2980 1
a3363 3
	pool_init(&pvpool, sizeof(struct pvlist), 0, 0, 0, "pvpl", 0,
	    NULL, NULL, 0);

d3431 1
a3431 1
		pmap_kenter_pa(va, pa, prot);
d4214 2
a4215 1
	if (!pv || prot & VM_PROT_WRITE)
d4876 1
a4876 1
	int ret;
d4883 1
a4883 1
		return (EINVAL);
d4889 1
a4889 1
		    pm, va, pa, prot, flags);
d4910 1
a4910 1
		ret = pmap_enk4_4c(pm, va, prot, flags, pv, pteproto | PG_S);
d4912 1
a4912 5
		ret = pmap_enu4_4c(pm, va, prot, flags, pv, pteproto);
#ifdef DIAGNOSTIC
	if ((flags & PMAP_CANFAIL) == 0 && ret)
		panic("pmap_enter4_4c: can't fail, but did");
#endif
d4915 1
a4915 1
	return (ret);
d4919 2
a4920 2
int
pmap_enk4_4c(pm, va, prot, flags, pv, pteproto)
d4924 1
a4924 1
	int flags;
a4930 1
	int wired = (flags & PMAP_WIRED) != 0;
d4963 1
a4963 1
			return (0);
d4968 4
a4971 1

a5038 2

	return (0);
d5042 2
a5043 2
int
pmap_enu4_4c(pm, va, prot, flags, pv, pteproto)
d5047 1
a5047 1
	int flags;
a5053 1
	int wired = (flags & PMAP_WIRED) != 0;
d5080 1
d5086 6
a5091 3
		sp = malloc((u_long)size, M_VMPMAP, M_NOWAIT);
		if (sp == NULL)
			return (ENOMEM);
d5100 2
d5106 6
a5111 3
		pte = malloc((u_long)size, M_VMPMAP, M_NOWAIT);
		if (pte == NULL)
			return (ENOMEM);
d5153 1
a5153 1
				return (0);
d5160 4
a5209 2

	return (0);
d5222 14
d5258 1
d5271 1
a5271 1
	int ret;
d5276 1
a5276 1
		    pm, va, pa, prot, flags);
d5311 1
d5313 1
a5313 1
		ret = pmap_enk4m(pm, va, prot, flags, pv, pteproto | PPROT_S);
d5315 2
a5316 5
		ret = pmap_enu4m(pm, va, prot, flags, pv, pteproto);
#ifdef DIAGNOSTIC
	if ((flags & PMAP_CANFAIL) == 0 && ret != 0)
		panic("pmap_enter4_4c: can't fail, but did");
#endif
d5323 1
d5326 1
a5326 1
	return (ret);
d5330 2
a5331 2
int
pmap_enk4m(pm, va, prot, flags, pv, pteproto)
d5335 1
a5335 1
	int flags;
a5341 1
	int wired = (flags & PMAP_WIRED) != 0;
d5352 1
a5352 1
#ifdef DEBUG
d5367 1
a5367 1
			return (0);
d5372 4
a5375 1

a5405 2

	return (0);
d5409 2
a5410 2
int
pmap_enu4m(pm, va, prot, flags, pv, pteproto)
d5414 1
a5414 1
	int flags;
a5420 1
	int wired = (flags & PMAP_WIRED) != 0;
d5432 2
d5438 6
a5443 3
		sp = malloc((u_long)size, M_VMPMAP, M_NOWAIT);
		if (sp == NULL)
			return (ENOMEM);
d5449 1
a5449 1

d5454 8
a5461 3
		ptd = pool_get(&L23_pool, PR_NOWAIT);
		if (ptd == NULL)
			return (ENOMEM);
d5471 2
d5477 6
a5482 3
		pte = pool_get(&L23_pool, PR_NOWAIT);
		if (pte == NULL)
			return (ENOMEM);
d5511 1
a5511 1
				return (0);
d5518 5
a5556 2

	return (0);
d5584 44
d5808 63
d5898 2
a5899 2
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
	boolean_t ret;
d5901 6
a5906 6
	if ((pv = pvhead(pfn)) == NULL)
		return (0);

	(void) pv_syncflags4_4c(pv);
	ret = pv->pv_flags & PV_MOD;
	pv->pv_flags &= ~PV_MOD;
d5919 1
a5919 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5921 6
a5926 4
	if ((pv = pvhead(pfn)) == NULL)
		return (0);

	return (pv->pv_flags & PV_MOD || pv_syncflags4_4c(pv) & PV_MOD);
d5937 2
a5938 2
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
	boolean_t ret;
d5940 6
a5945 6
	if ((pv = pvhead(pfn)) == NULL)
		return (0);

	(void) pv_syncflags4_4c(pv);
	ret = pv->pv_flags & PV_REF;
	pv->pv_flags &= ~PV_REF;
d5958 1
a5958 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d5960 6
a5965 4
	if ((pv = pvhead(pfn)) == NULL)
		return (0);

	return (pv->pv_flags & PV_REF || pv_syncflags4_4c(pv) & PV_REF);
d5988 2
a5989 2
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
	boolean_t ret;
d5991 6
a5996 6
	if ((pv = pvhead(pfn)) == NULL)
		return (0);

	(void) pv_syncflags4m(pv);
	ret = pv->pv_flags & PV_MOD4M;
	pv->pv_flags &= ~PV_MOD4M;
d6009 1
a6009 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d6011 6
a6016 3
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
	return (pv->pv_flags & PV_MOD4M || pv_syncflags4m(pv) & PV_MOD4M);
d6027 2
a6028 2
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
	boolean_t ret;
d6030 6
a6035 6
	if ((pv = pvhead(pfn)) == NULL)
		return (0);

	(void) pv_syncflags4m(pv);
	ret = pv->pv_flags & PV_REF4M;
	pv->pv_flags &= ~PV_REF4M;
d6048 1
a6048 1
	u_int pfn = atop(VM_PAGE_TO_PHYS(pg));
d6050 6
a6055 3
	if ((pv = pvhead(pfn)) == NULL)
		return (0);
	return (pv->pv_flags & PV_REF4M || pv_syncflags4m(pv) & PV_REF4M);
d6076 3
a6078 1
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL) {
d6153 4
a6156 2
	if (pmap_initialized && (pv = pvhead(atop(pa))) != NULL &&
	    CACHEINFO.c_vactype != VAC_NONE) {
a6162 1
	}
@


1.75.2.8
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d217 2
a218 6
void *pgt_page_alloc(struct pool *, int);
void  pgt_page_free(struct pool *, void *);

struct pool_allocator pgt_allocator = {
	pgt_page_alloc, pgt_page_free, 0,
};
d236 4
a239 1
pgt_page_alloc(struct pool *pp, int flags)
d244 2
a245 1
                                      PAGE_SIZE, UVM_KMF_NOWAIT);
d247 2
a248 2
                pcache_flush(p, (caddr_t)VA2PA(p), PAGE_SIZE);
                kvm_uncache(p, atop(PAGE_SIZE));
d254 4
a257 1
pgt_page_free(struct pool *pp, void *v)
d259 1
a259 1
        uvm_km_free(kernel_map, (vaddr_t)v, PAGE_SIZE);
d3362 2
a3363 1
	pool_init(&pvpool, sizeof(struct pvlist), 0, 0, 0, "pvpl", NULL);
d3381 2
a3382 2
                pool_init(&L1_pool, n, n, 0, 0, "L1 pagetable",
		    &pgt_allocator);
d3385 2
a3386 2
                pool_init(&L23_pool, n, n, 0, 0, "L2/L3 pagetable",
                    &pgt_allocator);
d5203 1
a5203 1
	return (KERN_SUCCESS);
d5412 1
a5412 1
	s = splvm();			/* XXX conservative */
d5519 1
a5519 1
	return (KERN_SUCCESS);
d5710 18
@


1.75.2.9
log
@Merge in -current from about a week ago
@
text
@d165 3
a167 3
static __inline struct pvlist *pvhead(int);
static __inline struct pvlist *pvalloc(void);
static __inline void pvfree(struct pvlist *);
d170 1
a170 1
static u_int	VA2PA(caddr_t);
d224 1
a224 1
void    pcache_flush(caddr_t, caddr_t, int);
d230 1
a230 1
        void (*f)(int,int) = cpuinfo.pcache_flush_line;
d405 3
a407 3
static void pmap_page_upload(paddr_t);
void pmap_pinit(pmap_t);
void pmap_release(pmap_t);
d465 3
a467 3
u_int	*getptep4m(struct pmap *, vaddr_t);
static __inline void	setpgt4m(int *, int);
void	setpte4m(vaddr_t va, int pte);
d489 15
a503 15
static void mmu_setup4m_L1(int, struct pmap *);
static void mmu_setup4m_L2(int, struct regmap *);
static void  mmu_setup4m_L3(int, struct segmap *);
void	mmu_reservemon4m(struct pmap *);

void	pmap_rmk4m(struct pmap *, vaddr_t, vaddr_t, int, int);
void	pmap_rmu4m(struct pmap *, vaddr_t, vaddr_t, int, int);
int	pmap_enk4m(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
int	pmap_enu4m(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
void	pv_changepte4m(struct pvlist *, int, int);
int	pv_syncflags4m(struct pvlist *);
int	pv_link4m(struct pvlist *, struct pmap *, vaddr_t, int);
void	pv_unlink4m(struct pvlist *, struct pmap *, vaddr_t);
d507 11
a517 11
void	mmu_reservemon4_4c(int *, int *);
void	pmap_rmk4_4c(struct pmap *, vaddr_t, vaddr_t, int, int);
void	pmap_rmu4_4c(struct pmap *, vaddr_t, vaddr_t, int, int);
int	pmap_enk4_4c(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
int	pmap_enu4_4c(struct pmap *, vaddr_t, vm_prot_t,
			  int, struct pvlist *, int);
void	pv_changepte4_4c(struct pvlist *, int, int);
int	pv_syncflags4_4c(struct pvlist *);
int	pv_link4_4c(struct pvlist *, struct pmap *, vaddr_t, int);
void	pv_unlink4_4c(struct pvlist *, struct pmap *, vaddr_t);
d532 13
a544 13
boolean_t	(*pmap_clear_modify_p)(struct vm_page *);
boolean_t	(*pmap_clear_reference_p)(struct vm_page *);
void		(*pmap_copy_page_p)(paddr_t, paddr_t);
int		(*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t	(*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *);
boolean_t	(*pmap_is_modified_p)(struct vm_page *);
boolean_t	(*pmap_is_referenced_p)(struct vm_page *);
void		(*pmap_kenter_pa_p)(vaddr_t, paddr_t, vm_prot_t);
void		(*pmap_kremove_p)(vaddr_t, vsize_t);
void		(*pmap_page_protect_p)(struct vm_page *, vm_prot_t);
void		(*pmap_protect_p)(pmap_t, vaddr_t, vaddr_t, vm_prot_t);
void            (*pmap_zero_page_p)(paddr_t);
void	       	(*pmap_changeprot_p)(pmap_t, vaddr_t, vm_prot_t, int);
d546 2
a547 2
void 		(*pmap_rmk_p)(struct pmap *, vaddr_t, vaddr_t, int, int);
void 		(*pmap_rmu_p)(struct pmap *, vaddr_t, vaddr_t, int, int);
d712 9
a720 9
static void sortm(struct memarr *, int);
void	ctx_alloc(struct pmap *);
void	ctx_free(struct pmap *);
void	pv_flushcache(struct pvlist *);
void	kvm_iocache(caddr_t, int);
#ifdef DEBUG
void	pm_check(char *, struct pmap *);
void	pm_check_k(char *, struct pmap *);
void	pm_check_u(char *, struct pmap *);
d1149 4
a1152 4
struct mmuentry *me_alloc(struct mmuhd *, struct pmap *, int, int);
void		me_free(struct pmap *, u_int);
struct mmuentry	*region_alloc(struct mmuhd *, struct pmap *, int);
void		region_free(struct pmap *, u_int);
d2562 1
a2562 1
static void pmap_bootstrap4m(void);
d2565 1
a2565 1
static void pmap_bootstrap4_4c(int, int, int);
a2578 1
	extern int nbpg;	/* locore.s */
d2580 1
a2580 1
	uvmexp.pagesize = nbpg;
d2585 1
a2585 1
	nptesg = (NBPSG >> uvmexp.pageshift);
d6462 1
a6462 1
	int (*dump)(dev_t, daddr_t, caddr_t, size_t);
d6621 1
a6621 1
static void test_region(int, int, int);
@


1.75.2.10
log
@Sync the SMP branch with 3.3
@
text
@d166 2
d177 2
a178 1
pvhead(int pnum)
d186 1
a186 1
	return &vm_physmem[bank].pgs[off].mdpage.pv_head;
d191 18
d261 6
d534 1
d543 1
a543 2
void		(*pmap_copy_page_p)(struct vm_page *, struct vm_page *);
void            (*pmap_zero_page_p)(struct vm_page *);
a675 52
/*
 * Translation table for kernel vs. PTE protection bits.
 */
u_int protection_codes[2][8];
#define pte_prot4m(pm, prot) (protection_codes[pm == pmap_kernel()?0:1][prot])

static void
sparc_protection_init4m(void)
{
	u_int prot, *kp, *up;

	kp = protection_codes[0];
	up = protection_codes[1];

	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RWX_RWX;
			break;
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RW_RW;
			break;
		case VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_RX_RX;
			break;
		case VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_R_R;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RWX_RWX;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] = PPROT_N_RWX;
			up[prot] = PPROT_RW_RW;
			break;
		case VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_X_X;
			break;
		case VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE:
			kp[prot] = PPROT_N_RX;
			up[prot] = PPROT_R_R;
			break;
		}
	}
}

d715 1
a715 1
void	pg_flushcache(struct vm_page *);
d1590 1
a1590 1
		s = splvm();		/* paranoid */
d1613 1
a1613 1
	s = splvm();		/* paranoid */
d1655 1
a1655 1
	s = splvm();
d1712 1
a1738 1
		splx(s);
d1820 1
a1820 1
			setcontext4(0);
d1885 1
a1885 1
	s = splvm();			/* paranoid? */
d1984 1
a1984 1
	s = splvm();			/* paranoid? */
d2070 1
a2070 1
			pool_put(&pvpool, npv);
d2094 1
a2094 1
		pool_put(&pvpool, npv);
a2135 1

d2167 1
a2167 4

	npv = pool_get(&pvpool, PR_NOWAIT);
	if (npv == NULL)
		panic("pv_link_4_4c: allocation failed");
d2209 1
a2209 1
	s = splvm();			/* paranoid? */
d2297 1
a2297 1
	s = splvm();			/* paranoid? */
d2381 1
a2381 1
			pool_put(&pvpool, npv);
d2405 1
a2405 1
		pool_put(&pvpool, npv);
d2444 1
d2462 10
a2471 3
	mpv = pool_get(&pvpool, PR_NOWAIT);
	if (mpv == NULL)
		panic("pv_link4m: allocation failed");
d2525 2
a2526 1
pg_flushcache(struct vm_page *pg)
a2527 1
	struct pvlist *pv = &pg->mdpage.pv_head;
d2533 1
a2533 1
	s = splvm();		/* XXX extreme paranoia */
a3245 1
	sparc_protection_init4m();
d3340 20
d3362 6
d3561 1
a3561 1
	int s = splvm();	/* paranoia */
d3679 1
a3679 1
	s = splvm();		/* XXX conservative */
a3730 1
	int s;
a3778 1
				s = splvm();
a3779 1
				splx(s);
a3922 1
	int s;
a3932 1

d3936 1
d3956 1
a3956 2
				if (pv) {
					s = splvm();
a3957 2
					splx(s);
				}
d3989 1
a3990 1
		npg = (endva - va) >> PGSHIFT;
a4017 1
				s = splvm();
a4018 1
				splx(s);
d4188 3
a4190 1
pmap_page_protect4_4c(struct vm_page *pg, vm_prot_t prot)
d4198 1
d4201 2
d4205 1
a4205 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pg, prot);
a4206 1
	pv = &pg->mdpage.pv_head;
d4208 2
a4209 1
	 * Skip operations that do not take away write permission.
d4211 2
a4212 1
	if (prot & VM_PROT_WRITE)
d4226 1
a4226 1
	s = splvm();
d4234 1
a4234 2
	while (pv != NULL) {
		pm = pv->pv_pmap;
a4238 1
#ifdef DIAGNOSTIC
a4240 1
#endif
d4242 1
a4242 2
#ifdef DIAGNOSTIC
		if (sp->sg_npte == 0)
d4244 2
a4245 2
#endif
		nleft = --sp->sg_npte;
a4286 1
#ifdef DIAGNOSTIC
a4289 1
#endif
d4354 3
a4356 2
			pool_put(&pvpool, pv);
		pv = npv;
d4359 1
a4359 1
	pv0->pv_next = NULL;
d4375 4
a4378 1
pmap_protect4_4c(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d4395 1
a4395 1
	s = splvm();
d4420 1
a4420 1
		if (sp->sg_pte == NULL)
d4504 1
a4504 1
	s = splvm();		/* conservative */
a4507 1
		splx(s);
a4511 1
		splx(s);
d4586 3
a4588 1
pmap_page_protect4m(struct vm_page *pg, vm_prot_t prot)
d4596 1
d4599 2
d4603 1
a4603 1
		printf("pmap_page_protect(0x%lx, 0x%x)\n", pg, prot);
d4605 1
a4605 1
	pv = &pg->mdpage.pv_head;
d4607 2
a4608 1
	 * Skip operations that do not take away write permission.
d4610 1
a4610 1
	if (prot & VM_PROT_WRITE)
d4624 1
a4624 1
	s = splvm();
d4687 1
a4687 1
			pool_put(&pvpool, pv);
d4707 4
a4710 1
pmap_protect4m(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d4716 3
a4718 1
	int newprot;
a4724 7
	/*
	 * Since the caller might request either a removal of PROT_EXECUTE
	 * or PROT_WRITE, we don't attempt to guess what to do, just lower
	 * to read-only and let the real protection be faulted in.
	 */
	newprot = pte_prot4m(pm, VM_PROT_READ);

d4727 1
a4727 1
	s = splvm();
d4760 1
a4760 1
		pmap_stats.ps_npg_prot_all += (nva - va) >> PGSHIFT;
d4762 1
a4762 2
			int tpte, npte;

a4763 7
			npte = (tpte & ~SRMMU_PROT_MASK) | newprot;

			/* Only do work when needed. */
			if (npte == tpte)
				continue;

			pmap_stats.ps_npg_prot_actual++;
d4766 2
a4767 1
			 * tags are updated.
d4769 4
a4772 2
			if (pm->pm_ctx) {
				if ((tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM) {
d4774 1
d4776 2
a4777 1
				tlb_flush_page(va);
a4778 1
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], npte);
d4810 4
a4813 1
	newprot = pte_prot4m(pm, prot);
d4817 1
a4817 1
	s = splvm();		/* conservative */
d4896 1
a4896 1
	if ((pteproto & PG_TYPE) == PG_OBMEM)
d4911 1
a4911 1
	if ((flags & PMAP_CANFAIL) == 0 && ret != 0)
d4938 1
a4938 1
	s = splvm();		/* XXX way too conservative */
d5061 1
a5061 1
	s = splvm();			/* XXX conservative */
d5200 1
a5200 1
	return (0);
d5209 1
a5209 19
	struct pvlist *pv;
	int pteproto, ctx;

	pteproto = PG_S | PG_V | PMAP_T2PTE_4(pa);
	if (prot & VM_PROT_WRITE)
		pteproto |= PG_W;

	pa &= ~PMAP_TNC_4;

	if ((pteproto & PG_TYPE) == PG_OBMEM)
		pv = pvhead(atop(pa));
	else
		pv = NULL;

	pteproto |= atop(pa) & PG_PFNUM;

	ctx = getcontext4();
	pmap_enk4_4c(pmap_kernel(), va, prot, PMAP_WIRED, pv, pteproto);
	setcontext4(ctx);
d5217 3
a5219 1
	pmap_remove(pmap_kernel(), va, va + len);
d5267 2
a5268 1
	pteproto |= SRMMU_TEPTE;
d5276 1
a5276 1
	if ((pteproto & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
d5283 2
a5284 2
	/* correct protections */
	pteproto |= pte_prot4m(pm, prot);
d5288 1
a5288 1
		ret = pmap_enk4m(pm, va, prot, flags, pv, pteproto);
d5293 1
a5293 1
		panic("pmap_enter4m: can't fail, but did");
d5328 1
a5328 1
	s = splvm();		/* XXX way too conservative */
d5516 1
a5516 1
	return (0);
d5539 2
a5540 2
	pmap_enk4m(pmap_kernel(), va, prot, PMAP_WIRED, pv, pteproto);
	setcontext4m(ctx);
d5729 2
a5730 1
pmap_clear_modify4_4c(struct vm_page *pg)
d5733 1
d5736 2
a5737 1
	pv = &pg->mdpage.pv_head;	
d5750 2
a5751 1
pmap_is_modified4_4c(struct vm_page *pg)
d5754 1
d5756 2
a5757 1
	pv = &pg->mdpage.pv_head;
d5766 2
a5767 1
pmap_clear_reference4_4c(struct vm_page *pg)
d5770 1
d5773 2
a5774 1
	pv = &pg->mdpage.pv_head;
d5787 2
a5788 1
pmap_is_referenced4_4c(struct vm_page *pg)
d5791 1
d5793 2
a5794 1
	pv = &pg->mdpage.pv_head;
d5815 2
a5816 1
pmap_clear_modify4m(struct vm_page *pg)
d5819 1
d5822 2
a5823 1
	pv = &pg->mdpage.pv_head;
d5836 2
a5837 1
pmap_is_modified4m(struct vm_page *pg)
d5840 1
d5842 2
a5843 2
	pv = &pg->mdpage.pv_head;

d5851 2
a5852 1
pmap_clear_reference4m(struct vm_page *pg)
d5855 1
d5858 2
a5859 1
	pv = &pg->mdpage.pv_head;
d5872 2
a5873 1
pmap_is_referenced4m(struct vm_page *pg)
d5876 1
d5878 2
a5879 2
	pv = &pg->mdpage.pv_head;

d5894 2
a5895 1
pmap_zero_page4_4c(struct vm_page *pg)
a5896 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d5899 1
d5901 8
a5908 7
	/*
	 * The following might not be necessary since the page
	 * is being cleared because it is about to be allocated,
	 * i.e., is in use by no one.
	 */
	pg_flushcache(pg);

d5927 2
a5928 1
pmap_copy_page4_4c(struct vm_page *srcpg, struct vm_page *dstpg)
a5929 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
d5932 1
d5934 3
a5936 2
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pg_flushcache(srcpg);
d5940 3
a5942 2
	if (CACHEINFO.c_vactype != VAC_NONE)
		pg_flushcache(dstpg);
d5965 2
a5966 1
pmap_zero_page4m(struct vm_page *pg)
d5968 3
a5970 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a5971 2
	static int *ptep;
	int pte;
d5976 2
a5977 1
	if (CACHEINFO.c_vactype != VAC_NONE) {
d5983 1
a5983 1
		pg_flushcache(pg);
d5986 2
a5987 2
	pte = (SRMMU_TEPTE | (atop(pa) << SRMMU_PPNSHIFT) | PPROT_N_RWX);

d6010 2
a6011 1
pmap_copy_page4m(struct vm_page *srcpg, struct vm_page *dstpg)
d6013 2
a6014 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
a6016 1
	int spte, dpte;
d6023 10
a6032 8
	if (CACHEINFO.c_vactype == VAC_WRITEBACK)
		pg_flushcache(srcpg);

	spte = SRMMU_TEPTE | SRMMU_PG_C | (atop(src) << SRMMU_PPNSHIFT) |
	    PPROT_N_RX;

	if (CACHEINFO.c_vactype != VAC_NONE)
		pg_flushcache(dstpg);
d6034 2
a6035 1
	dpte = (SRMMU_TEPTE | (atop(dst) << SRMMU_PPNSHIFT) | PPROT_N_RWX);
d6253 1
a6253 1
	s = splvm();
d6577 1
a6577 1
	s = splvm();
@


1.75.2.11
log
@Sync the SMP branch to -current.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75.2.10 2003/03/27 23:49:26 niklas Exp $	*/
d741 1
d6126 27
@


1.75.2.12
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d2284 1
a2284 1
			panic("pv_changepte: invalid PTE for 0x%lx", va);
d2344 2
a2345 2
			printf("pv_syncflags4m: no pte pmap: %p, va: 0x%x\n",
			    pm, va);
d2705 1
a2705 1
	 * Initialize the kernel pmap.
d3007 1
a3007 1
	 * Initialize the kernel pmap.
@


1.75.2.13
log
@Merge with the trunk
@
text
@d5087 1
a5087 2
		if (sp == NULL) {
			splx(s);
a5088 1
		}
d5102 1
a5102 2
		if (pte == NULL) {
			splx(s);
a5103 1
		}
d5429 1
a5429 2
		if (sp == NULL) {
			splx(s);
a5430 1
		}
d5442 1
a5442 2
		if (ptd == NULL) {
			splx(s);
a5443 1
		}
d5458 1
a5458 2
		if (pte == NULL) {
			splx(s);
a5459 1
		}
@


1.74
log
@Some cpus might have the cache running even before we enable it.
Uncache the pagetables in pmap_bootstrap4m and recache them again
in pmap_cache_enable if we can. This fixes the problems seen on SM30.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2000/02/01 16:16:36 art Exp $	*/
a4945 1
		tlb_flush_page(va);
d4955 1
a6487 1
			tlb_flush_page((vaddr_t)va);
d6504 1
@


1.73
log
@Add a bunch of XXX on incorrect handling of pv_flags.
None of the noted problems are critical (the code still works), but they
can have a severe impact on the performance and they are all really hard
to fix in an elegant way.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2000/02/01 10:34:32 art Exp $	*/
d3304 7
a3310 1
		pte |= PPROT_N_RX | SRMMU_PG_C | SRMMU_TEPTE;
d3332 6
d3489 1
a3489 1
 * Called just after enabling cache (so that we CPUFLG_CACHEPAGETABLES is
d3497 2
d3500 1
a3500 1
		 * Mark all pagetables uncacheable, if required
d3502 2
a3503 4
		 * We assume that we have not allocated any tables except
		 * the kernel pagetables. It's safe to assume because
		 * only userland can alloc extra pagetables and we're
		 * still in autoconfiguration.
d3505 6
a3510 2
		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
			kvm_uncache((caddr_t)pagetables_start,
@


1.72
log
@Opps. Missed two cases of ps_alias_recache.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2000/02/01 10:17:08 art Exp $	*/
d2360 7
d2499 5
d2539 3
d2572 5
@


1.71
log
@Keep stats of pages we uncache because of bad cache aliases.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2000/01/27 21:11:09 art Exp $	*/
d2164 2
d2469 2
@


1.70
log
@remove bogus comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2000/01/27 17:37:15 art Exp $	*/
d133 2
d2189 1
d2245 1
d2493 1
d2566 1
@


1.69
log
@use {round,trunc}_page instead of manually doing the same thing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2000/01/27 17:00:02 art Exp $	*/
a2338 1
			/* Flush TLB so memory copy is up-to-date */
@


1.68
log
@pmap_writetext - use getptep4m/setpgt and avoid one unnecessary table walk
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2000/01/27 15:48:19 art Exp $	*/
d257 1
a257 1
                kvm_uncache(p, sz/NBPG);
d2825 1
a2825 1
	p = (caddr_t)(((u_int)p + NBPG - 1) & ~PGOFSET);
d3104 1
a3104 1
	p = (caddr_t)(((u_int)p + NBPG - 1) & ~PGOFSET);
d3150 1
a3150 1
	p = (caddr_t)(((u_int)p + NBPG - 1) & ~PGOFSET);
a3400 5
#ifdef DEBUG
	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE!=1");
#endif

d4569 1
a4569 1
	va &= ~(NBPG-1);
d4888 1
a4888 1
	va &= ~(NBPG-1);
@


1.67
log
@It's amazing what you can find with some code reading.
In pv_unlink4m we check if the page that was uncached due to bad aliases
can be cached again. The check was correct but instead of clearing this
flag we clear all other flags except PV_ANC.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2000/01/27 02:06:24 art Exp $	*/
d6941 4
a6944 1
		pte0 = getpte4m(va);
d6950 2
a6951 1
		setpte4m(va, pte);
d6953 2
a6954 2
		setpte4m(va, pte0);

@


1.66
log
@zap setptesw4m, not used anymore
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2000/01/27 00:18:43 art Exp $	*/
d2490 1
a2490 1
		pv->pv_flags &= PV_ANC;
@


1.65
log
@zap getptesw4m, not used anymore (and the comment was bogus)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2000/01/27 00:03:09 art Exp $	*/
a534 1
static __inline void	setptesw4m __P((struct pmap *pm, vaddr_t va, int pte));
a737 15
}

/*
 * Set the page table entry for va to pte. Only affects software MMU page-
 * tables (the in-core pagetables read by the MMU). Ignores TLB, and
 * thus should _not_ be called if the pte translation could be in the TLB.
 * In this case, use setpte4m().
 */
static __inline void
setptesw4m(pm, va, pte)
	struct pmap *pm;
	vaddr_t va;
	int pte;
{
	setpgt4m(getptep4m(pm, va), pte);
@


1.64
log
@update comment to reflect reality (remove it).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2000/01/27 00:01:06 art Exp $	*/
a535 1
static __inline u_int	getptesw4m __P((struct pmap *pm, vaddr_t va));
a738 20
}

/*
 * Get the page table entry (PTE) for va by looking it up in the software
 * page tables. These are the same tables that are used by the MMU; this
 * routine allows easy access to the page tables even if the context
 * corresponding to the table is not loaded or selected.
 * This routine should NOT be used if there is any chance that the desired
 * pte is in the TLB cache, since it will return stale data in that case.
 * For that case, and for general use, use getpte4m, which is much faster
 * and avoids walking in-memory page tables if the page is in the cache.
 * Note also that this routine only works if a kernel mapping has been
 * installed for the given page!
 */
static __inline u_int
getptesw4m(pm, va)
	struct pmap *pm;
	vaddr_t va;
{
	return *getptep4m(pm, va);
@


1.63
log
@pmap_rmk4m: There is no need to flush the segment from the tlb or
to zero the table when we have 0 mappings in a segment.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2000/01/26 23:30:04 art Exp $	*/
a4000 4
	/*
	 * If the segment is all gone, remove it from everyone and
	 * flush the TLB.
	 */
@


1.62
log
@cleanup and simplify pmap_page_protect4m:
 - Use a normal while-loop when walking the pv lists instead of an ugly hack.
 - always clean the pte even if we know that it will be freed.
 - No need to flush the segment from the tlb in kernel.
 - add some ifdef DIAGNOSTIC.
 - clear the PV_ANC flag on the pv.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2000/01/26 15:54:30 art Exp $	*/
d4005 1
a4005 20
	if ((sp->sg_npte = nleft) == 0) {
		va = VSTOVA(vr,vs);		/* retract */

		tlb_flush_segment(vr, vs); 	/* Paranoia? */

		/*
		 * We need to free the segment table. The problem is that
		 * we can't free the initial (bootstrap) mapping, so
		 * we have to explicitly check for this case (ugh).
		 */
		if (va < virtual_avail) {
#ifdef DEBUG
			printf("pmap_rmk4m: attempt to free base kernel alloc\n");
#endif
			return;
		}
		/* no need to free the table; it is statically allocated */
		qzero(sp->sg_pte, SRMMU_L3SIZE * sizeof(long));
	}
	/* if we're done with a region, leave it wired */
d4008 1
@


1.61
log
@Replace more table-walks and getpte4m/setpte4m with getptep4m/setpgt4m.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2000/01/17 21:06:44 art Exp $	*/
d4734 1
a4734 1
	int flags, nleft, s, ctx;
d4774 3
a4776 2
	flags = pv->pv_flags /*| PV_C4M*/;	/* %%%: ???? */
	for (;; pm = pv->pv_pmap) {
d4781 1
d4783 2
a4784 1
			panic("pmap_remove_all: empty vreg");
d4786 5
a4790 4
		if ((nleft = sp->sg_npte) == 0)
			panic("pmap_remove_all: empty vseg");
		nleft--;
		sp->sg_npte = nleft;
d4792 1
a4792 1
		/* Invalidate PTE in MMU pagetables. Flush cache if necessary */
d4801 1
d4803 2
a4804 1
			panic("pmap_page_protect !PG_V");
d4808 1
a4808 4
		if (nleft) {
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], SRMMU_TEINVALID);
			goto nextpv;
		}
d4811 1
a4811 19
		if (pm == pmap_kernel()) {
			tlb_flush_segment(vr, vs); /* Paranoid? */
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], SRMMU_TEINVALID);
			if (va < virtual_avail) {
#ifdef DEBUG
				printf(
				 "pmap_page_protect: attempt to free"
				 " base kernel allocation\n");
#endif
				goto nextpv;
			}
#if 0 /* no need for this */
			/* no need to free the table; it is static */
			qzero(sp->sg_pte, SRMMU_L3SIZE * sizeof(int));
#endif

			/* if we're done with a region, leave it */

		} else { 	/* User mode mapping */
a4827 1
	nextpv:
d4831 1
a4831 2
		if ((pv = npv) == NULL)
			break;
d4834 2
a4835 2
	pv0->pv_next = NULL; /* ? */
	pv0->pv_flags = flags;
@


1.60
log
@pmap_{zero,copy}_page4m: use getptep4m to get the ptes for vpages.
This way we only walk the tables once every boot and we don't have to
switch to context 0 on every call.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2000/01/14 22:18:28 art Exp $	*/
d720 3
d724 4
a2322 1
	int va, vr;
d2324 1
a2324 2
	struct regmap *rp;
	struct segmap *sp;
d2336 2
d2339 1
d2342 1
a2342 1
			panic("pv_changepte: pm == NULL");
a2343 5
		va = pv->pv_va;
		vr = VA_VREG(va);
		rp = &pm->pm_regmap[vr];
		if (rp->rg_segmap == NULL)
			panic("pv_changepte: no segments");
d2345 1
a2345 1
		sp = &rp->rg_segmap[VA_VSEG(va)];
d2381 5
a2385 5
		tpte = sp->sg_pte[VA_SUN4M_VPG(va)];
		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE) {
			printf("pv_changepte: invalid PTE for 0x%x\n", va);
			continue;
		}
d2389 1
a2389 1
		setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], tpte);
a2395 2


d2413 1
a2413 1
	int tpte, va, vr, vs, flags;
a2414 2
	struct regmap *rp;
	struct segmap *sp;
d2426 2
a2429 6
		vr = VA_VREG(va);
		vs = VA_VSEG(va);
		rp = &pm->pm_regmap[vr];
		if (rp->rg_segmap == NULL)
			panic("pv_syncflags: no segments");
		sp = &rp->rg_segmap[vs];
d2431 2
a2432 1
		if (sp->sg_pte == NULL)	/* invalid */
d2443 1
a2443 1
		tpte = sp->sg_pte[VA_SUN4M_VPG(va)];
d2457 1
a2457 1
			setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], tpte);
d4959 1
d4978 1
a4978 1
	ctx = getcontext4m();
d4980 3
d4988 1
a4988 2
		setcontext4m(pm->pm_ctxnum);
		tpte = getpte4m(va);
d4992 1
d4994 1
a4994 1
		tpte = getptesw4m(pm, va);
d5001 1
a5001 4
	if (pm->pm_ctx)
		setpte4m(va, (tpte & ~SRMMU_PROT_MASK) | newprot);
	else
		setptesw4m(pm, va, (tpte & ~SRMMU_PROT_MASK) | newprot);
a5003 1
	setcontext4m(ctx);
d6521 5
a6525 1
			pte = getpte4m((vaddr_t) va);
d6531 1
a6531 2
			/* XXX - we probably don't need check for OBMEM */
			if ((pte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM && pv) {
d6541 1
a6541 1
			setpte4m((vaddr_t) va, pte);
@


1.59
log
@Cleanup the handling of ptes on 4m.
 Implement a generic getptep4m that returns the pointer to the pte
 for (pmap, va). reimplement (and make static __inline) {set,get}ptesw4m
 and setpte4m using getptep4m. (more on this topic coming soon)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2000/01/14 21:24:26 art Exp $	*/
a6403 1
	caddr_t va;
a6404 1
	int ctx;
d6406 5
d6430 5
a6434 8
	/* XXX - must use context 0 or else setpte4m() will fail */
	ctx = getcontext4m();
	setcontext4m(0);
	va = vpage[0];
	setpte4m((vaddr_t) va, pte);
	qzero(va, NBPG);
	setpte4m((vaddr_t) va, SRMMU_TEINVALID);
	setcontext4m(ctx);
a6449 1
	caddr_t sva, dva;
a6450 1
	int ctx;
d6452 7
d6478 5
a6482 8
	/* XXX - must use context 0 or else setpte4m() will fail */
	ctx = getcontext4m();
	setcontext4m(0);
	sva = vpage[0];
	dva = vpage[1];
	setpte4m((vaddr_t) sva, spte);
	setpte4m((vaddr_t) dva, dpte);
	qcopy(sva, dva, NBPG);	/* loads cache, so we must ... */
d6484 4
a6487 3
	setpte4m((vaddr_t) sva, SRMMU_TEINVALID);
	setpte4m((vaddr_t) dva, SRMMU_TEINVALID);
	setcontext4m(ctx);
@


1.58
log
@make setpgt4m static __inline, remove dead code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2000/01/12 11:11:26 d Exp $	*/
d533 5
a537 4
static __inline void setpgt4m __P((int *, int));
void	setpte4m __P((vaddr_t va, int pte));
void	setptesw4m __P((struct pmap *pm, vaddr_t va, int pte));
u_int	getptesw4m __P((struct pmap *pm, vaddr_t va));
d700 36
d747 2
a748 2
__inline u_int
getptesw4m(pm, va)		/* Assumes L3 mapping! */
d752 1
a752 23
	struct regmap *rm;
	struct segmap *sm;

	rm = &pm->pm_regmap[VA_VREG(va)];
#ifdef DEBUG
	if (rm == NULL)
		panic("getptesw4m: no regmap entry");
#endif
	sm = &rm->rg_segmap[VA_VSEG(va)];
#ifdef DEBUG
	if (sm == NULL)
		panic("getptesw4m: no segmap");
#endif
	return (sm->sg_pte[VA_SUN4M_VPG(va)]); 	/* return pte */
}


static __inline void
setpgt4m(ptep, pte)
	int *ptep;
	int pte;
{
	swap(ptep, pte);
d761 1
a761 1
__inline void
d767 1
a767 16
	struct regmap *rm;
	struct segmap *sm;

	rm = &pm->pm_regmap[VA_VREG(va)];

#ifdef DEBUG
	if (pm->pm_regmap == NULL || rm == NULL)
		panic("setptesw4m: no regmap entry");
#endif
	sm = &rm->rg_segmap[VA_VSEG(va)];

#ifdef DEBUG
	if (rm->rg_segmap == NULL || sm == NULL || sm->sg_pte == NULL)
		panic("setptesw4m: no segmap for va %p", (caddr_t)va);
#endif
	setpgt4m(sm->sg_pte + VA_SUN4M_VPG(va), pte);
d777 1
a777 3
	struct regmap *rm;
	struct segmap *sm;
	union ctxinfo *c;
d779 1
a779 2
	c = &cpuinfo.ctxinfo[getcontext4m()];
	pm = c->c_pmap;
d781 1
a781 17
	/* Note: inline version of setptesw4m() */
#ifdef DEBUG
	if (pm->pm_regmap == NULL)
		panic("setpte4m: no regmap entry");
#endif
	rm = &pm->pm_regmap[VA_VREG(va)];
	sm = &rm->rg_segmap[VA_VSEG(va)];

#ifdef DEBUG
	if (rm == NULL || rm->rg_segmap == NULL)
		panic("setpte4m: no segmap for va %p (rp=%p)",
			(caddr_t)va, (caddr_t)rm);   

	if (sm == NULL || sm->sg_pte == NULL)
		panic("setpte4m: no pte for va %p (rp=%p, sp=%p)",
			(caddr_t)va, rm, sm);
#endif
d783 1
a783 1
	setpgt4m(sm->sg_pte + VA_SUN4M_VPG(va), pte);
@


1.57
log
@ifdef SUN4M, ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 1999/12/09 21:46:51 art Exp $	*/
d533 1
a533 1
void	setpgt4m __P((int *ptep, int pte));
d731 2
a732 1
__inline void
a737 4
#if 0
	if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
		cpuinfo.pcache_flush_line((int)ptep, VA2PA((caddr_t)ptep));
#endif
@


1.56
log
@Opps, missed to change one FREE(pv, ...) to pvfree(pv).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 1999/12/09 21:35:28 art Exp $	*/
d174 1
a179 1
#if defined(SUN4M)
d3745 1
d3756 1
@


1.55
log
@Fix a bug we've had for ages.
On some sun4m the pagetables must be uncached. This is indicated by the
CPUFLG_CACHEPAGETABLES in cpuinfo.flags. This was done in pmap_bootstrap4m.
The problem is that the CPUFLG_CACHEPAGETABLES is not set until after
pmap_bootstrap4m, so even the machines that could cache the pagetables
had them uncached, reducing performance.
Fix this by creating pmap_cache_enable that is called just after the
cache has been switched on (XXX - actually, we should call it before, but
CPUFLG_CACHEPAGETABLES can be set in the code that enables the cache).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 1999/12/09 16:11:48 art Exp $	*/
d2597 1
a2597 1
		FREE(mpv, M_VMPVENT);
@


1.54
log
@Change the kvm_uncache interface to a kvm_setcache that can uncache
a memory and allow the range to be cached again.
Make kvm_uncache and kvm_recache to macros that call kvm_setcache.
(also in the commit: Fix protection for pmap_kenter* and remove a redundant
 call to uvm_setpagesize).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 1999/12/09 14:26:04 art Exp $	*/
d464 1
a3088 1
	caddr_t pagetables_start, pagetables_end;
d3173 1
a3173 1
	pagetables_start = p;
d3214 1
a3214 1
	pagetables_end = p;
a3364 5
	/* Mark all MMU tables uncacheable, if required */
	if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
		kvm_uncache(pagetables_start,
			    (pagetables_end - pagetables_start) >> PGSHIFT);

d3515 24
d5801 1
a5801 1
		pa = atop(VM_PAGE_TO_PHYS(pgs[i]));
@


1.53
log
@Make PMAP_NEW compile again.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 1999/12/09 13:22:59 art Exp $	*/
a2765 8
#if defined(UVM)
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();
#else
	cnt.v_page_size = NBPG;
	vm_set_page_size();
#endif

d5736 2
a5737 2
		PMAP_T2PTE_SRMMU(pa) | SRMMU_TEPTE | PPROT_RX_RX | PPROT_S |
		((prot & VM_PROT_WRITE) ? PPROT_WRITE : 0);
d5763 1
a5763 1
	pteproto = SRMMU_PG_C |	SRMMU_TEPTE | PPROT_RX_RX | PPROT_S;
d6503 1
a6503 1
 * Turn off cache for a given (va, number of pages).
d6509 1
a6509 1
kvm_uncache(va, npages)
d6512 1
d6531 4
a6534 1
				pv_changepte4m(pv, 0, SRMMU_PG_C);
d6536 4
a6539 1
			pte &= ~SRMMU_PG_C;
d6559 4
a6562 1
				pv_changepte4_4c(pv, PG_NC, 0);
d6564 4
a6567 1
			pte |= PG_NC;
@


1.52
log
@Fix pmap_kenter_pa4m. First shot at a better pmap_kenter_pgs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 1999/12/08 15:16:12 art Exp $	*/
d5790 1
a5790 1
		pa = atop(VM_PAGE_TO_PHYS(pgs[i]);
@


1.51
log
@Readd the PMAP_NEW code, this time with the missing ifdef.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 1999/12/08 11:38:38 art Exp $	*/
a5532 1

a5744 1
		(atop(pa) << SRMMU_PPNSHIFT) |
d5746 5
d5755 1
d5764 28
a5791 1
	int i;
d5793 7
a5799 3
	for (i = 0; i < npgs; i++, va += PAGE_SIZE)
		pmap_kenter_pa4m(va, VM_PAGE_TO_PHYS(pgs[i]),
				 VM_PROT_READ|VM_PROT_WRITE);
d5807 1
a5807 3
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
@


1.50
log
@undo even more damage
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 1999/12/07 15:32:30 art Exp $	*/
d600 4
d606 1
d611 8
d622 1
d2796 5
d3113 5
d3554 3
d3559 1
d3563 1
d3566 1
d4329 4
d4335 1
d4344 3
d4735 4
d4741 1
d4750 3
d5383 35
d5733 45
d6033 5
d6041 1
d6044 4
d6052 3
d6057 3
d6065 5
d6073 1
d6076 3
d6091 5
d6099 1
d6102 4
d6110 3
d6115 3
d6123 5
d6131 1
d6134 3
d6161 5
d6169 1
d6172 4
d6180 3
d6185 3
d6193 5
d6201 1
d6204 3
d6219 5
d6227 1
d6230 4
d6238 3
d6243 3
d6251 5
d6259 1
d6262 3
@


1.49
log
@undo total garbage untested sloppy commit from art
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 1999/12/08 10:44:49 art Exp $	*/
a599 4
#ifdef PMAP_NEW
boolean_t	(*pmap_clear_modify_p) __P((struct vm_page *));
boolean_t	(*pmap_clear_reference_p) __P((struct vm_page *));
#else
a601 1
#endif
a605 8
#ifdef PMAP_NEW
boolean_t	(*pmap_is_modified_p) __P((struct vm_page *));
boolean_t	(*pmap_is_referenced_p) __P((struct vm_page *));
void		(*pmap_kenter_pa_p) __P((vaddr_t, paddr_t, vm_prot_t));
void		(*pmap_kenter_pgs_p) __P((vaddr_t, struct vm_page **, int));
void		(*pmap_kremove_p) __P((vaddr_t, vsize_t));
void		(*pmap_page_protect_p) __P((struct vm_page *, vm_prot_t));
#else
a608 1
#endif
a2781 5
#ifdef PMAP_NEW
	pmap_kenter_pa_p	=	pmap_kenter_pa4_4c;
	pmap_kenter_pgs_p	=	pmap_kenter_pgs4_4c;
	pmap_kremove_p		=	pmap_kremove4_4c;
#endif
a3093 5
#ifdef PMAP_NEW
	pmap_kenter_pa_p	=	pmap_kenter_pa4m;
	pmap_kenter_pgs_p	=	pmap_kenter_pgs4m;
	pmap_kremove_p		=	pmap_kremove4m;
#endif
a3529 3
#ifdef PMAP_NEW
pmap_create()
#else
a3531 1
#endif
a3534 1
#ifndef PMAP_NEW
a3536 1
#endif
a4298 4
#ifdef PMAP_NEW
pmap_page_protect4_4c(pg, prot)
	struct vm_page *pg;
#else
a4300 1
#endif
a4308 3
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
a4696 4
#ifdef PMAP_NEW
pmap_page_protect4m(pg, prot)
	struct vm_page *pg;
#else
a4698 1
#endif
a4706 3
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
a5336 35
#ifdef PMAP_NEW
void
pmap_kenter_pa4_4c(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	pmap_enter4_4c(pmap_kernel(), va, pa, prot, TRUE, 0);
}

void
pmap_kenter_pgs4_4c(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter4_4c(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
                               VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
	}
}

void
pmap_kremove4_4c(va, len)
	vaddr_t va;
	vsize_t len;
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
#endif

a5651 36

#ifdef PMAP_NEW
void
pmap_kenter_pa4m(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	pmap_enter4m(pmap_kernel(), va, pa, prot, TRUE, 0);
}

void
pmap_kenter_pgs4m(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter4m(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			     VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
	}
}

void
pmap_kremove4m(va, len)
	vaddr_t va;
	vsize_t len;
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
#endif /* PMAP_NEW */

a5906 5
#ifdef PMAP_NEW
boolean_t
pmap_clear_modify4_4c(pg)
	struct vm_page *pg;
#else
a5909 1
#endif
a5911 4
#ifdef PMAP_NEW
	paddr_t pa;
	boolean_t ret = 0;
#endif
a5915 3
#ifdef PMAP_NEW
		ret = pv->pv_flags & PV_MOD;
#endif
a5917 3
#ifdef PMAP_NEW
	return ret;
#endif
a5922 5
#ifdef PMAP_NEW
boolean_t
pmap_is_modified4_4c(pg)
	struct vm_page *pg;
#else
a5925 1
#endif
a5927 3
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
a5939 5
#ifdef PMAP_NEW
boolean_t
pmap_clear_reference4_4c(pg)
	struct vm_page *pg;
#else
a5942 1
#endif
a5944 4
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret = 0;
#endif
a5948 3
#ifdef PMAP_NEW
		ret = pv->pv_flags & PV_REF;
#endif
a5950 3
#ifdef PMAP_NEW
	return ret;
#endif
a5955 5
#ifdef PMAP_NEW
boolean_t
pmap_is_referenced4_4c(pg)
	struct vm_page *pg;
#else
a5958 1
#endif
a5960 3
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
a5984 5
#ifdef PMAP_NEW
boolean_t
pmap_clear_modify4m(pg)
	struct vm_page *pg;
#else
a5987 1
#endif
a5989 4
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret = 0;
#endif
a5993 3
#ifdef PMAP_NEW
		ret = pv->pv_flags & PV_MOD4M;
#endif
a5995 3
#ifdef PMAP_NEW
	return ret;
#endif
a6000 5
#ifdef PMAP_NEW
boolean_t
pmap_is_modified4m(pg)
	struct vm_page *pg;
#else
a6003 1
#endif
a6005 3
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
a6017 5
#ifdef PMAP_NEW
boolean_t
pmap_clear_reference4m(pg)
	struct vm_page *pg;
#else
a6020 1
#endif
a6022 4
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret = 0;
#endif
a6026 3
#ifdef PMAP_NEW
		ret = pv->pv_flags & PV_REF4M;
#endif
a6028 1
	return ret;
a6033 5
#ifdef PMAP_NEW
boolean_t
pmap_is_referenced4m(pg)
	struct vm_page *pg;
#else
a6036 1
#endif
a6038 3
#ifdef PMAP_NEW
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
@


1.48
log
@Better pmap_kenter_pa4m.
Use pmap_kenter_pa4m in pmap_kenter_pgs4m.
@
text
@a5740 13
#if 1
	struct pvlist *pv;
	int pteproto, ctx;

	pteproto = ((pa & PMAP_NC) == 0 ? SRMMU_PG_C : 0) |
		PMAP_T2PTE_SRMMU(pa) | SRMMU_TEPTE | PPROT_RX_RX | PPROT_S |
		(atop(pa) << SRMMU_PPNSHIFT) |
		((prot & VM_PROT_WRITE) ? PPROT_WRITE : 0);
	pv = pvhead(atop(pa));

	ctx = getcontext4m();
	pmap_enk4m(pmap_kernel(), va, prot, TRUE, pv, pteproto);
#else
a5741 1
#endif
d5752 4
a5755 3
	for (i = 0; i < npgs; i++, va += PAGE_SIZE)
		pmap_kenter_pa4m(va, VM_PAGE_TO_PHYS(pgs[i]),
				 VM_PROT_READ|VM_PROT_WRITE);
@


1.47
log
@Dumb implementation of PMAP_NEW, it works but without any gains yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 1999/12/07 15:32:30 art Exp $	*/
d5741 13
d5755 1
d5766 3
a5768 4
	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter4m(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			     VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
	}
@


1.46
log
@Use pool to allocate page tables. This has the nice effect that we can uncache
them on allocation instead of flushing cache every time we fiddle with them.
Some parts from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 1999/11/16 12:21:41 art Exp $	*/
d600 4
d606 1
d611 8
d622 1
d2796 5
d3113 5
d3554 3
d3559 1
d3563 1
d3566 1
d4329 4
d4335 1
d4344 3
d4735 4
d4741 1
d4750 3
d5383 35
d5733 36
d6024 5
d6032 1
d6035 4
d6043 3
d6048 3
d6056 5
d6064 1
d6067 3
d6082 5
d6090 1
d6093 4
d6101 3
d6106 3
d6114 5
d6122 1
d6125 3
d6152 5
d6160 1
d6163 4
d6171 3
d6176 3
d6184 5
d6192 1
d6195 3
d6210 5
d6218 1
d6221 4
d6229 3
d6234 1
d6240 5
d6248 1
d6251 3
@


1.45
log
@No need to set the page size twice
@
text
@d78 1
d179 4
d217 82
d325 3
a446 1
caddr_t	vdumppages;		/* 32KB worth of reserved dump pages */
a640 2
static u_int	VA2PA __P((caddr_t));

d650 1
a650 1
static __inline u_int
d722 1
a722 1
#if 1
d2752 8
d3478 4
d3483 19
d6086 2
a6087 1
	if (((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0) && pv) {
d6161 2
a6162 1
		CACHEINFO.c_vactype != VAC_NONE)
@


1.44
log
@Don't allocate vmmap in pmap_bootstrap, allocate it in mmrw when needed.
At the same time, give it a 'better' name.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 1999/11/16 10:49:56 art Exp $	*/
a2663 8

#if defined(UVM)
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();
#else
	cnt.v_page_size = NBPG;
	vm_set_page_size();
#endif
@


1.43
log
@Remove duplicate variable definitions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 1999/11/16 10:11:35 art Exp $	*/
a356 1
caddr_t	vmmap;			/* one reserved MI vpage for /dev/mem */
d2786 2
a2787 2
	 * for pmap_zero_page and pmap_copy_page, and one MI page
	 * for /dev/mem, all with no associated physical memory.
a2794 1
	vmmap = p, p += NBPG;
d3183 2
a3184 2
	 * for pmap_zero_page and pmap_copy_page, one MI page
	 * for /dev/mem, and some more for dumpsys().
a3188 1
	vmmap = p, p += NBPG;
@


1.42
log
@No need to do "sp->sg_npte = 0" in an if (sp->sg_npte == 0)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 1999/11/12 21:02:50 art Exp $	*/
a382 3

/*static*/ vaddr_t	virtual_avail;	/* first free virtual page number */
/*static*/ vaddr_t	virtual_end;	/* last free virtual page number */
@


1.41
log
@Break out the allocation of pv entries and 4m pagetables into functions.
This is a preparation to make them non-sleeping.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 1999/11/11 12:30:36 art Exp $	*/
a3897 2
			/* sp->sg_pte = NULL; */
			sp->sg_npte = 0;
@


1.40
log
@General cleanup of pmap.

- MACHINE_NONCONTIG will no longer work and all code that is not
  MACHINE_NEW_NONCONTIG is removed.
- Kill the pv_table hack which adds complexity to the code and wastes some
  memory, let the vm "handle" the pv lists through the vm_physmem array,
  this makes allocation a lot easier.
- kill the managed() macro, since it was only used to see if pvhead would
  end up in a "hole" in pv_table.
- pvhead() is now a function that returns NULL if the page is unmanaged. It
  also takes a page number as an argument instead of the physical address,
  since in most cases pvhead was called as pvhead(ptoa(pa)) anyway and it
  did an atop internally.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 1999/11/05 19:21:02 art Exp $	*/
d167 12
a180 1
static struct pvlist *pvhead __P((int));
d195 44
d2109 1
a2109 1
			FREE(npv, M_VMPVENT);
d2131 1
a2131 1
		FREE(npv, M_VMPVENT);
d2202 2
a2203 1
	MALLOC(npv, struct pvlist *, sizeof *npv, M_VMPVENT, M_WAITOK);
d2422 1
a2422 1
			FREE(npv, M_VMPVENT);
d2444 1
a2444 1
		FREE(npv, M_VMPVENT);
d2492 1
a2492 1
	MALLOC(mpv, struct pvlist *, sizeof *npv, M_VMPVENT, M_WAITOK);
d3493 1
a3493 5
		urp = malloc(SRMMU_L1SIZE * sizeof(int), M_VMPMAP, M_WAITOK);
#ifdef DEBUG
		if ((u_int) urp % (SRMMU_L1SIZE * sizeof(int)))
			panic("pmap_pinit: malloc() not giving aligned memory");
#endif
d3496 2
d3606 1
a3606 1
		free(pm->pm_reg_ptps, M_VMPMAP);
d4171 1
a4171 1
		free(pte0, M_VMPMAP);
d4180 1
a4180 1
			free(rp->rg_seg_ptps, M_VMPMAP);
d4363 1
a4363 1
			FREE(pv, M_VMPVENT);
d4695 1
a4695 1
			free(sp->sg_pte, M_VMPMAP);
d4704 1
a4704 1
				free(rp->rg_seg_ptps, M_VMPMAP);
d4711 1
a4711 1
			FREE(pv, M_VMPVENT);
d5129 1
a5129 1
		pte = malloc(size, M_VMPMAP, M_WAITOK);
d5444 1
a5444 1
		int size, i, *ptd;
d5446 1
a5446 2
		size = SRMMU_L2SIZE * sizeof(long);
		ptd = malloc(size, M_VMPMAP, M_WAITOK);
d5451 1
a5451 1
			free(ptd, M_VMPMAP);
d5467 1
a5467 1
		int i, size = SRMMU_L3SIZE * sizeof(*pte);
d5469 1
a5469 1
		pte = malloc(size, M_VMPMAP, M_WAITOK);
d5472 1
a5472 1
			free(pte, M_VMPMAP);
@


1.39
log
@Since we no longer allocate the iommu page tables in pmap_bootstrap, we can
remove unavail_{start,end} and simplify the code in pmap_page_upload.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 1999/11/05 18:07:10 art Exp $	*/
a165 4
#if 0
#define	splpmap() splimp()
#endif

d167 1
a167 1
 * First and last managed physical addresses.
d169 13
a181 1
paddr_t	vm_first_phys, vm_num_phys;
d184 2
a185 12
 * For each managed physical page, there is a list of all currently
 * valid virtual mappings of that page.  Since there is usually one
 * (or zero) mapping per page, the table begins with an initial entry,
 * rather than a pointer; this head entry is empty iff its pv_pmap
 * field is NULL.
 *
 * Note that these are per machine independent page (so there may be
 * only one for every two hardware pages, e.g.).  Since the virtual
 * address is aligned on a page boundary, the low order bits are free
 * for storing flags.  Only the head of each list has flags.
 *
 * THIS SHOULD BE PART OF THE CORE MAP
d187 1
a187 26
struct pvlist {
	struct		pvlist *pv_next;	/* next pvlist, if any */
	struct		pmap *pv_pmap;		/* pmap of this va */
	vaddr_t		pv_va;			/* virtual address */
	int		pv_flags;		/* flags (below) */
};

/*
 * Flags in pv_flags.  Note that PV_MOD must be 1 and PV_REF must be 2
 * since they must line up with the bits in the hardware PTEs (see pte.h).
 * SUN4M bits are at a slightly different location in the PTE.
 * Note: the REF, MOD and ANC flag bits occur only in the head of a pvlist.
 * The cacheable bit (either PV_NC or PV_C4M) is meaningful in each
 * individual pv entry.
 */
#define PV_MOD		1	/* page modified */
#define PV_REF		2	/* page referenced */
#define PV_NC		4	/* page cannot be cached */
#define PV_REF4M	1	/* page referenced (SRMMU) */
#define PV_MOD4M	2	/* page modified (SRMMU) */
#define PV_C4M		4	/* page _can_ be cached (SRMMU) */
#define PV_ANC		0x10	/* page has incongruent aliases */

struct pvlist *pv_table;	/* array of entries, one per physical page */

#define pvhead(pa)	(&pv_table[((pa) - vm_first_phys) >> PGSHIFT])
d317 8
d328 1
a328 7
int	cpmemarr;		/* pmap_next_page() state */
/*static*/ paddr_t	avail_start;	/* first free physical page */
/*static*/ paddr_t	avail_end;	/* last free physical page */
#ifdef MACHINE_NONCONTIG
/*static*/ paddr_t	avail_next;	/* pmap_next_page() state:
					   next free physical page */
#endif
d332 1
a332 3
#ifdef MACHINE_NEW_NONCONTIG
static void pmap_page_upload __P((void));
#endif
d584 1
d587 1
a690 1
static void get_phys_mem __P((void));
a701 26

/*
 * Grab physical memory list and use it to compute `physmem' and
 * `avail_end'. The latter is used in conjuction with
 * `avail_start' and `avail_next' to dispatch left-over
 * physical pages to the VM system.
 */
static void
get_phys_mem()
{
	struct memarr *mp;
	int i;

	npmemarr = makememarr(pmemarr, MA_SIZE, MEMARR_AVAILPHYS);
	sortm(pmemarr, npmemarr);
	if (pmemarr[0].addr != 0)
		panic("pmap_bootstrap: no kernel memory?!");

	avail_end = pmemarr[npmemarr-1].addr + pmemarr[npmemarr-1].len;
#ifdef MACHINE_NONCONTIG
	avail_next = avail_start;
#endif
	for (physmem = 0, mp = pmemarr, i = npmemarr; --i >= 0; mp++)
		physmem += btoc(mp->len);
}

a751 1
#ifdef MACHINE_NEW_NONCONTIG
d756 2
a757 1
pmap_page_upload()
d762 15
d778 2
a779 5
		/*
		 * Assume `avail_start' is always in the first segment; we
		 * already made that assumption in pmap_bootstrap()..
		 */
		start = (n == 0) ? avail_start : pmemarr[n].addr;
d781 1
a781 1
		if (start == end)
d785 2
a786 5
		uvm_page_physload(
			atop(start),
			atop(end),
			atop(start),
			atop(end), VM_FREELIST_DEFAULT);
d788 2
a789 9
		vm_page_physload(
			atop(start),
			atop(end),
			atop(start),
			atop(end));
#endif
	}

}
a790 86

#ifdef MACHINE_NONCONTIG
/*
 * Return the number of page indices in the range of
 * possible return values for pmap_page_index() for
 * all addresses provided by pmap_next_page().  This
 * return value is used to allocate per-page data.
 *
 */
u_int
pmap_free_pages()
{
	int long bytes;
	int nmem;
	struct memarr *mp;

	bytes = -avail_start;
	for (mp = pmemarr, nmem = npmemarr; --nmem >= 0; mp++)
		bytes += mp->len;

        return atop(bytes);
}

/*
 * If there are still physical pages available, put the address of
 * the next available one at paddr and return TRUE.  Otherwise,
 * return FALSE to indicate that there are no more free pages.
 * Note that avail_next is set to avail_start in pmap_bootstrap().
 *
 * Imporant:  The page indices of the pages returned here must be
 * in ascending order.
 */
int
pmap_next_page(paddr)
        paddr_t *paddr;
{

        /* Is it time to skip over a hole? */
	if (avail_next == pmemarr[cpmemarr].addr + pmemarr[cpmemarr].len) {
		if (++cpmemarr == npmemarr)
			return FALSE;
		avail_next = pmemarr[cpmemarr].addr;
	}

#ifdef DIAGNOSTIC
        /* Any available memory remaining? */
        if (avail_next >= avail_end) {
		panic("pmap_next_page: too much memory?!");
	}
#endif

        /* Have memory, will travel... */
        *paddr = avail_next;
        avail_next += NBPG;
        return TRUE;
}
#endif

/*
 * pmap_page_index()
 *
 * Given a physical address, return a page index.
 *
 * There can be some values that we never return (i.e. a hole)
 * as long as the range of indices returned by this function
 * is smaller than the value returned by pmap_free_pages().
 * The returned index does NOT need to start at zero.
 *
 */
int
pmap_page_index(pa)
	paddr_t pa;
{
	paddr_t idx;
	int nmem;
	struct memarr *mp;

#ifdef  DIAGNOSTIC
	if (pa < avail_start || pa >= avail_end)
		panic("pmap_page_index: pa=0x%lx", pa);
#endif

	for (idx = 0, mp = pmemarr, nmem = npmemarr; --nmem >= 0; mp++) {
		if (pa >= mp->addr && pa < mp->addr + mp->len)
			break;
		idx += atop(mp->len);
a791 2

	return (int)(idx + atop(pa - mp->addr));
d798 1
a798 9
	int nmem;
	struct memarr *mp;

	for (mp = pmemarr, nmem = npmemarr; --nmem >= 0; mp++) {
		if (pa >= mp->addr && pa < mp->addr + mp->len)
			return 1;
	}

	return 0;
d1172 1
a1172 1
	int i, va, pa, *pte, tpte;
d1280 5
a1284 3
			pa = ptoa(tpte & PG_PFNUM);
			if (managed(pa))
				pvhead(pa)->pv_flags |= MR4_4C(tpte);
d1329 1
a1329 1
	int i, va, pa, tpte;
d1363 5
a1367 3
			pa = ptoa(tpte & PG_PFNUM);
			if (managed(pa))
				pvhead(pa)->pv_flags |= MR4_4C(tpte);
d2138 1
a2138 2
					va, npv->pv_va,
					vm_first_phys + (pv-pv_table)*NBPG);
d2468 1
a2468 2
					va, npv->pv_va,
					vm_first_phys + (pv-pv_table)*NBPG);
d2596 1
a2739 2
	get_phys_mem();

a2745 7
	/*
	 * Allocate virtual memory for pv_table[], which will be mapped
	 * sparsely in pmap_init().
	 */
	pv_table = (struct pvlist *)p;
	p += round_page(sizeof(struct pvlist) * atop(avail_end - avail_start));

d2919 2
a2920 3
#if defined(MACHINE_NEW_NONCONTIG)
	pmap_page_upload();
#endif
d2940 1
d3017 1
a3017 2
	 * alignment restrictions. We allocate in a sequence that
	 * minimizes alignment gaps.
a3063 2
	get_phys_mem();

a3140 7
	/*
	 * Allocate virtual memory for pv_table[], which will be mapped
	 * sparsely in pmap_init().
	 */
	pv_table = (struct pvlist *)p;
	p += round_page(sizeof(struct pvlist) * atop(avail_end - avail_start));

d3217 1
a3217 3
#if defined(MACHINE_NEW_NONCONTIG)
	pmap_page_upload();
#endif
d3312 3
a3314 5
	vsize_t s;
	int pass1, nmem;
	struct memarr *mp;
	vaddr_t sva, va, eva;
	paddr_t pa = 0;
d3316 1
d3319 1
d3321 3
a3323 55
	/*
	 * Map pv_table[] as a `sparse' array. This requires two passes
	 * over the `pmemarr': (1) to determine the number of physical
	 * pages needed, and (2), to map the correct pieces of virtual
	 * memory allocated to pv_table[].
	 */

	s = 0;
	pass1 = 1;

pass2:
	sva = eva = 0;
	for (mp = pmemarr, nmem = npmemarr; --nmem >= 0; mp++) {
		int len;
		paddr_t addr;

		len = mp->len;
		if ((addr = mp->addr) < avail_start) {
			/*
			 * pv_table[] covers everything above `avail_start'.
			 */
			addr = avail_start;
			len -= avail_start;
		}
		len = sizeof(struct pvlist) * atop(len);

		if (addr < avail_start || addr >= avail_end)
			panic("pmap_init: unmanaged address: 0x%lx", addr);

		va = (vaddr_t)&pv_table[atop(addr - avail_start)];
		sva = trunc_page(va);

		if (sva < eva) {
			/* This chunk overlaps the previous in pv_table[] */
			sva += PAGE_SIZE;
			if (sva < eva)
				panic("pmap_init: sva(0x%lx) < eva(0x%lx)",
				      sva, eva);
		}
		eva = round_page(va + len);
		if (pass1) {
			/* Just counting */
			s += eva - sva;
			continue;
		}

		/* Map this piece of pv_table[] */
		for (va = sva; va < eva; va += PAGE_SIZE) {
			pmap_enter(pmap_kernel(), va, pa,
				   VM_PROT_READ|VM_PROT_WRITE, 1,
				   VM_PROT_READ|VM_PROT_WRITE);
			pa += PAGE_SIZE;
		}
		bzero((caddr_t)sva, eva - sva);
	}
d3325 2
a3326 1
	if (pass1) {
d3328 1
a3328 4
		vaddr_t va = uvm_km_alloc(kernel_map, s);
		if (!va)
			panic("pmap_init: Out of mem in kernel_map");
		pa = pmap_extract(pmap_kernel(), va);
d3330 1
a3330 1
		pa = pmap_extract(pmap_kernel(), kmem_alloc(kernel_map, s));
d3332 7
a3338 2
		pass1 = 0;
		goto pass2;
d3341 1
a3341 2
	vm_first_phys = avail_start;
	vm_num_phys = avail_end - avail_start;
d3707 2
a3708 3
			i = ptoa(tpte & PG_PFNUM);
			if (managed(i)) {
				pv = pvhead(i);
d3759 1
a3759 1
	int i, tpte, perpage, npg;
d3814 2
a3815 3
			i = ptoa((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (managed(i)) {
				pv = pvhead(i);
d3873 1
a3873 1
	int *pte0, i, pteva, tpte, perpage, npg;
d3908 5
a3912 3
				i = ptoa(tpte & PG_PFNUM);
				if (managed(i))
					pv_unlink4_4c(pvhead(i), pm, va);
d3970 2
a3971 3
			i = ptoa(tpte & PG_PFNUM);
			if (managed(i)) {
				pv = pvhead(i);
d4031 1
a4031 1
	int *pte0, i, perpage, npg;
d4090 2
a4091 3
			i = ptoa((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (managed(i)) {
				pv = pvhead(i);
d4165 1
d4167 1
a4167 1
	     !managed(pa) || prot & VM_PROT_WRITE)
d4171 1
a4171 1
		pv_changepte4_4c(pvhead(pa), 0, PG_W);
a4180 1
	pv = pvhead(pa);
d4559 1
d4564 1
a4564 1
	if (!managed(pa) || prot & VM_PROT_WRITE)
d4568 1
a4568 1
		pv_changepte4m(pvhead(pa), 0, PPROT_WRITE);
a4577 1
	pv = pvhead(pa);
d4870 3
a4872 7
	if ((pteproto & PG_TYPE) == PG_OBMEM && managed(pa)) {
#ifdef DIAGNOSTIC
		if (!pmap_pa_exists(pa))
			panic("pmap_enter: no such address: 0x%lx", pa);
#endif
		pv = pvhead(pa);
	} else {
d4874 1
a4874 1
	}
a4925 2
		int addr;

d4936 1
d4946 3
a4948 3
			addr = ptoa(tpte & PG_PFNUM);
			if (managed(addr))
				pv_unlink4_4c(pvhead(addr), pm, va);
d5075 1
a5075 1
		pte = (int *)malloc((u_long)size, M_VMPMAP, M_WAITOK);
a5110 2
			int addr;

d5134 5
a5138 3
				addr = ptoa(tpte & PG_PFNUM);
				if (managed(addr))
					pv_unlink4_4c(pvhead(addr), pm, va);
d5239 3
a5241 7
	if ((pteproto & SRMMU_PGTYPE) == PG_SUN4M_OBMEM && managed(pa)) {
#ifdef DIAGNOSTIC
		if (!pmap_pa_exists(pa))
			panic("pmap_enter: no such address: 0x%lx", pa);
#endif
		pv = pvhead(pa);
	} else {
d5243 1
a5243 1
	}
a5298 2
		int addr;

d5309 1
d5319 3
a5321 3
			addr = ptoa((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if (managed(addr))
				pv_unlink4m(pvhead(addr), pm, va);
a5377 1
#ifdef DEBUG
a5378 1
#endif
d5393 1
a5393 1
		ptd = (int *)malloc(size, M_VMPMAP, M_WAITOK);
d5416 1
a5416 1
		pte = (int *)malloc((u_long)size, M_VMPMAP, M_WAITOK);
a5438 2
			int addr;

d5463 3
a5465 1
				addr = ptoa( (tpte & SRMMU_PPNMASK) >>
d5467 2
a5468 2
				if (managed(addr))
					pv_unlink4m(pvhead(addr), pm, va);
d5758 2
a5759 2
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5774 2
a5775 2
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5791 2
a5792 2
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5807 2
a5808 2
	if ((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5836 2
a5837 2
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5852 2
a5853 2
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5869 2
a5870 2
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5885 2
a5886 2
	if ((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0 && managed(pa)) {
		pv = pvhead(pa);
d5928 1
d5930 2
a5931 1
	if (((pa & (PMAP_TNC_4 & ~PMAP_NC)) == 0) && managed(pa)) {
d5937 1
a5937 1
		pv_flushcache(pvhead(pa));
d5962 5
a5967 4
	if (managed(src)) {
		if (CACHEINFO.c_vactype == VAC_WRITEBACK)
			pv_flushcache(pvhead(src));
	}
d5970 4
a5973 5
	if (managed(dst)) {
		/* similar `might not be necessary' comment applies */
		if (CACHEINFO.c_vactype != VAC_NONE)
			pv_flushcache(pvhead(dst));
	}
d6001 1
d6003 3
a6005 1
	if (((pa & (PMAP_TNC_SRMMU & ~PMAP_NC)) == 0) && managed(pa)) {
d6011 2
a6012 3
		if (CACHEINFO.c_vactype != VAC_NONE)
			pv_flushcache(pvhead(pa));
	}
d6046 5
a6051 4
	if (managed(src)) {
		if (CACHEINFO.c_vactype == VAC_WRITEBACK)
			pv_flushcache(pvhead(src));
	}
d6055 4
a6058 5
	if (managed(dst)) {
		/* similar `might not be necessary' comment applies */
		if (CACHEINFO.c_vactype != VAC_NONE)
			pv_flushcache(pvhead(dst));
	}
d6106 1
a6106 1
	paddr_t pa;
a6111 1

d6115 1
d6118 5
a6122 5

			pa = ptoa((pte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT);
			if ((pte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM &&
			    managed(pa)) {
				pv_changepte4m(pvhead(pa), 0, SRMMU_PG_C);
d6126 1
d6141 5
a6145 5
			pa = ptoa(pte & PG_PFNUM);
			if ((pte & PG_TYPE) == PG_OBMEM &&
			    managed(pa)) {
				pv_changepte4_4c(pvhead(pa), PG_NC, 0);
			} 
d6653 1
d6656 1
@


1.38
log
@Move the allocation of iomme ptes to iommuattach (where it belongs).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 1999/11/05 16:22:08 art Exp $	*/
a353 2
/*static*/ paddr_t	unavail_start;	/* first stolen free physical page */
/*static*/ paddr_t	unavail_end;	/* last stolen free physical page */
d812 1
a812 23
	paddr_t start, end, avail_next;

	avail_next = avail_start;
	if (unavail_start != 0) {
		/* First, the gap we created in pmap_bootstrap() */
		if (avail_next != unavail_start)
			/* Avoid empty ranges */
#if defined(UVM)
			uvm_page_physload(
				atop(avail_next),
				atop(unavail_start),
				atop(avail_next),
				atop(unavail_start),
				VM_FREELIST_DEFAULT);
#else
			vm_page_physload(
				atop(avail_next),
				atop(unavail_start),
				atop(avail_next),
				atop(unavail_start));
#endif
		avail_next = unavail_end;
	}
d816 1
a816 1
		 * Assume `avail_next' is always in the first segment; we
d819 1
a819 1
		start = (n == 0) ? avail_next : pmemarr[n].addr;
d883 1
a883 2
	} else if (avail_next == unavail_start)
		avail_next = unavail_end;
d2881 1
a2881 1
	avail_start = (int)p - KERNBASE;
a3147 1

a3158 1

a3165 3
	avail_start = (int)p - KERNBASE;

	get_phys_mem();
a3170 2
	 * The amount of physical memory that becomes unavailable for
	 * general VM use is marked by [unavail_start, unavail_end>.
a3171 1
	unavail_start = (paddr_t)p - KERNBASE;
d3214 4
a3217 1
	unavail_end = (paddr_t)p - KERNBASE;
a3337 5

		if ((int)q >= KERNBASE + avail_start &&
		    (int)q < KERNBASE + unavail_start)
			/* This gap is part of VM-managed pages */
			continue;
@


1.37
log
@remove some #if 0 code that will never be enabled anyway
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 1999/09/06 07:13:39 art Exp $	*/
a341 3

u_int	*kernel_iopte_table;		/* 64k of storage for iommu */
u_int 	kernel_iopte_table_pa;
a3203 14

	/*
	 * Reserve memory for I/O pagetables. This takes 64k of memory
	 * since we want to have 64M of dvma space (this actually depends
	 * on the definition of DVMA4M_BASE...we may drop it back to 32M).
	 * The table must be aligned on a (-DVMA4M_BASE/NBPG) boundary
	 * (i.e. 64K for 64M of dvma space).
	 */
#ifdef DEBUG
	if ((0 - DVMA4M_BASE) % (16*1024*1024))
	    panic("pmap_bootstrap4m: invalid DVMA4M_BASE of 0x%x", DVMA4M_BASE);
#endif

	p = (caddr_t) roundup((u_int)p, (0 - DVMA4M_BASE) / 1024);
a3204 5

	kernel_iopte_table = (u_int *)p;
	kernel_iopte_table_pa = VA2PA((caddr_t)kernel_iopte_table);
	p += (0 - DVMA4M_BASE) / 1024;

@


1.36
log
@unbreak pmap_enter
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 1999/09/03 18:38:04 art Exp $	*/
a3708 6
#if 0
		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
			kvm_uncache(urp,
				    ((SRMMU_L1SIZE*sizeof(int))+NBPG-1)/NBPG);
#endif

a5685 4
#if 0
		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
			kvm_uncache((char *)ptd, (size+NBPG-1)/NBPG);
#endif
a5706 4
#if 0
		if ((cpuinfo.flags & CPUFLG_CACHEPAGETABLES) == 0)
			kvm_uncache((caddr_t)pte, (size+NBPG-1)/NBPG);
#endif
@


1.35
log
@in pmap_enter4m use access_type to set the initial mod/ref flags on the pv
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 1999/09/03 18:33:42 art Exp $	*/
d5114 1
a5114 1
pmap_enter4_4c(pm, va, pa, prot, wired, access_prot)
d5120 1
a5120 1
	vm_prot_t access_prot;
d5482 1
a5482 1
pmap_enter4m(pm, va, pa, prot, wired, access_prot)
d5488 1
a5488 1
	vm_prot_t access_prot;
@


1.34
log
@use setpgt4m for the initial mappings too, not that it really matters since
those pages are already uncached when needed, but it makes it more obvious
what happens here.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 1999/09/03 18:11:29 art Exp $	*/
d5543 7
@


1.33
log
@We have to set the pagetable pointers with the atomic swap instruction.
From NetBSD. (actually from a paper I read recently, but since NetBSD
already did that it was easier to grab the code from them)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 1999/09/03 18:01:59 art Exp $	*/
d1200 1
a1200 1
					(sp->sg_pte)[k] =
d1207 1
a1207 1
					    SRMMU_TEPTE;
d1253 1
a1253 1
				(sp->sg_pte)[k] =
d1260 1
a1260 1
				    SRMMU_TEPTE;
d1291 1
a1291 1
			sp->sg_pte[i] = te | PPROT_U2S_OMASK;
@


1.32
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 1999/08/20 09:30:55 art Exp $	*/
d615 1
a615 1
	*ptep = pte;
@


1.31
log
@cleanup pmap_enk4m
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 1999/08/20 09:15:05 art Exp $	*/
d496 1
a496 1
				     boolean_t));
d3591 2
a3592 1
				   VM_PROT_READ|VM_PROT_WRITE, 1);
d3628 1
a3628 1
		pmap_enter(pmap_kernel(), va, pa, prot, 1);
d5114 1
a5114 1
pmap_enter4_4c(pm, va, pa, prot, wired)
d5120 1
d5482 1
a5482 1
pmap_enter4m(pm, va, pa, prot, wired)
d5488 1
d6002 1
a6002 1
				   0);
@


1.30
log
@in pv_link4m malloc can sleep. Make the malloc earlier and check if
reality has changed while we were (possibly) sleeping.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 1999/07/09 21:30:02 art Exp $	*/
d5554 1
a5554 1
	int vr, vs, tpte, s;
d5558 1
a5558 1
#ifdef DEBUG
d5562 2
a5563 4
	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];
d5567 1
d5569 5
a5573 1
		panic("pmap_enk4m: missing kernel region table for va 0x%lx",va);
a5617 5

#ifdef DEBUG
	if (sp->sg_pte == NULL) /* If no existing pagetable */
		panic("pmap_enk4m: missing segment table for va 0x%lx",va);
#endif
@


1.29
log
@vm_offset_t -> {v,p}addr_t and vm_size_t -> {v,p}size_t
remove "register" keywords
Various cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 1999/06/04 23:03:00 deraadt Exp $	*/
d2587 1
a2587 1
	struct pvlist *npv;
d2592 1
d2602 16
d2650 6
a2655 6
	MALLOC(npv, struct pvlist *, sizeof *npv, M_VMPVENT, M_WAITOK);
	npv->pv_next = pv->pv_next;
	npv->pv_pmap = pm;
	npv->pv_va = va;
	npv->pv_flags = nc ? 0 : PV_C4M;
	pv->pv_next = npv;
@


1.28
log
@Think-o in pmap_extract4m(), detected by UVM; pk
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 1999/04/27 17:58:26 art Exp $	*/
d173 1
a173 1
vm_offset_t	vm_first_phys, vm_num_phys;
d192 1
a192 1
	vm_offset_t	pv_va;			/* virtual address */
d351 2
a352 2
/*static*/ vm_offset_t	avail_start;	/* first free physical page */
/*static*/ vm_offset_t	avail_end;	/* last free physical page */
d354 1
a354 1
/*static*/ vm_offset_t	avail_next;	/* pmap_next_page() state:
d357 4
a360 4
/*static*/ vm_offset_t	unavail_start;	/* first stolen free physical page */
/*static*/ vm_offset_t	unavail_end;	/* last stolen free physical page */
/*static*/ vm_offset_t	virtual_avail;	/* first free virtual page number */
/*static*/ vm_offset_t	virtual_end;	/* last free virtual page number */
d370 2
a371 2
vm_offset_t prom_vstart;	/* For /dev/kmem */
vm_offset_t prom_vend;
d425 3
a427 3
void	setpte4m __P((vm_offset_t va, int pte));
void	setptesw4m __P((struct pmap *pm, vm_offset_t va, int pte));
u_int	getptesw4m __P((struct pmap *pm, vm_offset_t va));
d454 3
a456 5
/*static*/ void pmap_rmk4m __P((struct pmap *, vm_offset_t, vm_offset_t,
                          int, int));
/*static*/ void pmap_rmu4m __P((struct pmap *, vm_offset_t, vm_offset_t,
                          int, int));
/*static*/ void pmap_enk4m __P((struct pmap *, vm_offset_t, vm_prot_t,
d458 1
a458 1
/*static*/ void pmap_enu4m __P((struct pmap *, vm_offset_t, vm_prot_t,
d462 2
a463 2
/*static*/ int  pv_link4m __P((struct pvlist *, struct pmap *, vm_offset_t, int));
/*static*/ void pv_unlink4m __P((struct pvlist *, struct pmap *, vm_offset_t));
d468 3
a470 5
/*static*/ void pmap_rmk4_4c __P((struct pmap *, vm_offset_t, vm_offset_t,
                          int, int));
/*static*/ void pmap_rmu4_4c __P((struct pmap *, vm_offset_t, vm_offset_t,
                          int, int));
/*static*/ void pmap_enk4_4c __P((struct pmap *, vm_offset_t, vm_prot_t,
d472 1
a472 1
/*static*/ void pmap_enu4_4c __P((struct pmap *, vm_offset_t, vm_prot_t,
d476 2
a477 2
/*static*/ int  pv_link4_4c __P((struct pvlist *, struct pmap *, vm_offset_t, int));
/*static*/ void pv_unlink4_4c __P((struct pvlist *, struct pmap *, vm_offset_t));
d492 12
a503 14
void      	(*pmap_clear_modify_p) __P((vm_offset_t pa));
void            (*pmap_clear_reference_p) __P((vm_offset_t pa));
void            (*pmap_copy_page_p) __P((vm_offset_t, vm_offset_t));
void            (*pmap_enter_p) __P((pmap_t,
		     vm_offset_t, vm_offset_t, vm_prot_t, boolean_t));
vm_offset_t     (*pmap_extract_p) __P((pmap_t, vm_offset_t));
boolean_t       (*pmap_is_modified_p) __P((vm_offset_t pa));
boolean_t       (*pmap_is_referenced_p) __P((vm_offset_t pa));
void            (*pmap_page_protect_p) __P((vm_offset_t, vm_prot_t));
void            (*pmap_protect_p) __P((pmap_t,
		     vm_offset_t, vm_offset_t, vm_prot_t));
void            (*pmap_zero_page_p) __P((vm_offset_t));
void	       	(*pmap_changeprot_p) __P((pmap_t, vm_offset_t,
		     vm_prot_t, int));
d505 2
a506 4
void 		(*pmap_rmk_p) __P((struct pmap *, vm_offset_t, vm_offset_t,
                          int, int));
void 		(*pmap_rmu_p) __P((struct pmap *, vm_offset_t, vm_offset_t,
                          int, int));
d522 3
a524 1
/* Macros which implement SRMMU TLB flushing/invalidation */
d526 4
a529 3
#define tlb_flush_page(va)    sta((va & ~0xfff) | ASI_SRMMUFP_L3, ASI_SRMMUFP,0)
#define tlb_flush_segment(vreg, vseg) sta((vreg << RGSHIFT) | (vseg << SGSHIFT)\
					  | ASI_SRMMUFP_L2, ASI_SRMMUFP,0)
d546 1
a546 1
	register caddr_t addr;
d548 1
a548 1
	register u_int pte;
d591 2
a592 2
	register struct pmap *pm;
	register vm_offset_t va;
d594 2
a595 2
	register struct regmap *rm;
	register struct segmap *sm;
d628 3
a630 3
	register struct pmap *pm;
	register vm_offset_t va;
	register int pte;
d632 2
a633 2
	register struct regmap *rm;
	register struct segmap *sm;
d653 1
a653 1
	vm_offset_t va;
d709 1
a709 1
	register int x;							\
d744 1
a744 1
	int j;
d748 3
a750 4
	if (pmemarr[0].addr != 0) {
		printf("pmap_bootstrap: no kernel memory?!\n");
		callrom();
	}
d755 1
a755 1
	for (physmem = 0, mp = pmemarr, j = npmemarr; --j >= 0; mp++)
d769 2
a770 1
	u_int addr, len;
d802 2
a803 2
        vm_offset_t *v_start;
        vm_offset_t *v_end;
d817 1
a817 1
	vm_offset_t start, end, avail_next;
d902 1
a902 1
        vm_offset_t *paddr;
d940 1
a940 1
	vm_offset_t pa;
d942 1
a942 1
	int idx;
d944 1
a944 1
	register struct memarr *mp;
d957 1
a957 1
	return (idx + atop(pa - mp->addr));
d962 1
a962 1
	vm_offset_t pa;
d964 2
a965 2
	register int nmem;
	register struct memarr *mp;
d993 1
a993 1
	register int *nrp, *nsp;
d995 2
a996 2
	register u_int va = 0, eva = 0;
	register int mmuseg, i, nr, ns, vr, lastvr;
d998 1
a998 1
	register int mmureg;
d1000 1
a1000 1
	register struct regmap *rp;
d1095 1
a1095 1
	register int te;
d1154 2
a1155 2
	register unsigned int regtblrover;
	register int i;
d1227 2
a1228 2
	register unsigned int segtblrover;
	register int i, k;
d1276 1
a1276 1
	register int pagtblptd;
d1279 3
a1281 3
	register unsigned int pagtblrover;
	register int i;
	register unsigned int te;
d1340 7
a1346 7
	register struct mmuhd *mh;
	register struct pmap *newpm;
	register int newvreg, newvseg;
{
	register struct mmuentry *me;
	register struct pmap *pm;
	register int i, va, pa, *pte, tpte;
d1497 2
a1498 2
	register struct pmap *pm;
	register u_int pmeg;
d1500 4
a1503 4
	register struct mmuentry *me = &mmusegments[pmeg];
	register int i, va, pa, tpte;
	register int vr;
	register struct regmap *rp;
d1576 3
a1578 3
	register struct mmuhd *mh;
	register struct pmap *newpm;
	register int newvr;
d1580 2
a1581 2
	register struct mmuentry *me;
	register struct pmap *pm;
d1668 2
a1669 2
	register struct pmap *pm;
	register u_int smeg;
d1671 1
a1671 1
	register struct mmuentry *me = &mmuregions[smeg];
d1713 3
a1715 2
	register struct pmap *pm;
	register int va, prot;
d1717 2
a1718 2
	register int *pte;
	register int vr, vs, pmeg, i, s, bits;
d1789 1
a1789 1
	register struct pmap *pm;
d1791 5
a1795 5
	register union ctxinfo *c;
	register int s, cnum, i, doflush;
	register struct regmap *rp;
	register int gap_start, gap_end;
	register unsigned long va;
d1885 2
a1886 2
				register int j;
				register struct segmap *sp = rp->rg_segmap;
d1945 2
a1946 2
	register union ctxinfo *c;
	register int newc, oldc;
d2073 1
a2073 1
				extern vm_offset_t pager_sva, pager_eva;
d2127 1
a2127 1
	register struct pvlist *pv0;
d2129 3
a2131 3
	register struct pvlist *pv;
	register struct pmap *pm;
	register int tpte, va, vr, vs, pmeg, flags;
d2198 3
a2200 3
	register struct pvlist *pv;
	register struct pmap *pm;
	register vm_offset_t va;
d2202 1
a2202 1
	register struct pvlist *npv;
d2236 1
a2236 1
		register struct pvlist *prev;
d2270 1
a2270 1
	vm_offset_t va;
d2393 1
a2393 1
			extern vm_offset_t pager_sva, pager_eva;
d2447 1
a2447 1
	register struct pvlist *pv0;
d2449 3
a2451 3
	register struct pvlist *pv;
	register struct pmap *pm;
	register int tpte, va, vr, vs, flags;
d2511 3
a2513 3
	register struct pvlist *pv;
	register struct pmap *pm;
	register vm_offset_t va;
d2515 1
a2515 1
	register struct pvlist *npv;
d2549 1
a2549 1
		register struct pvlist *prev;
d2584 1
a2584 1
	vm_offset_t va;
d2649 1
a2649 1
	register struct pvlist *pv;
d2651 2
a2652 2
	register struct pmap *pm;
	register int s, ctx;
d2739 2
a2740 2
	register union ctxinfo *ci;
	register struct mmuentry *mmuseg;
d2742 1
a2742 1
	register struct mmuentry *mmureg;
d2745 5
a2749 5
	register int i, j;
	register int npte, zseg, vr, vs;
	register int rcookie, scookie;
	register caddr_t p;
	register void (*rom_setmap)(int ctx, caddr_t va, int pmeg);
d2909 1
a2909 1
	virtual_avail = (vm_offset_t)p;
d3097 1
a3097 1
	register int i, j;
d3099 3
a3101 3
	register caddr_t q;
	register union ctxinfo *ci;
	register int reg, seg;
d3204 1
a3204 1
	unavail_start = (vm_offset_t)p - KERNBASE;
d3252 1
a3252 1
	unavail_end = (vm_offset_t)p - KERNBASE;
d3338 1
a3338 1
	virtual_avail = (vm_offset_t)p;
d3516 1
a3516 1
	register vm_size_t s;
d3518 3
a3520 3
	register struct memarr *mp;
	vm_offset_t sva, va, eva;
	vm_offset_t pa = 0;
d3539 1
a3539 1
		vm_offset_t addr;
d3554 1
a3554 1
		va = (vm_offset_t)&pv_table[atop(addr - avail_start)];
d3582 1
a3582 2
		vm_offset_t va = uvm_km_alloc(kernel_map, s);

d3601 1
a3601 1
vm_offset_t
d3603 3
a3605 2
	register vm_offset_t va, pa, endpa;
	register int prot;
d3607 1
a3607 1
	register int pgsize = PAGE_SIZE;
d3624 1
a3624 1
	vm_size_t size;
d3626 1
a3626 1
	register struct pmap *pm;
d3646 1
a3646 1
	register struct pmap *pm;
d3648 1
a3648 1
	register int size;
d3725 1
a3725 1
	register struct pmap *pm;
d3750 1
a3750 1
	register struct pmap *pm;
d3752 2
a3753 2
	register union ctxinfo *c;
	register int s = splpmap();	/* paranoia */
d3840 2
a3841 2
	register struct pmap *pm;
	register vm_offset_t va, endva;
d3843 3
a3845 3
	register vm_offset_t nva;
	register int vr, vs, s, ctx;
	register void (*rm)(struct pmap *, vm_offset_t, vm_offset_t, int, int);
d3912 7
a3918 7
	register struct pmap *pm;
	register vm_offset_t va, endva;
	register int vr, vs;
{
	register int i, tpte, perpage, npg;
	register struct pvlist *pv;
	register int nleft, pmeg;
d4015 7
a4021 7
	register struct pmap *pm;
	register vm_offset_t va, endva;
	register int vr, vs;
{
	register int i, tpte, perpage, npg;
	register struct pvlist *pv;
	register int nleft;
d4130 7
a4136 7
	register struct pmap *pm;
	register vm_offset_t va, endva;
	register int vr, vs;
{
	register int *pte0, i, pteva, tpte, perpage, npg;
	register struct pvlist *pv;
	register int nleft, pmeg;
d4157 1
a4157 1
		register int *pte = pte0 + VA_VPG(va);
d4287 7
a4293 7
	register struct pmap *pm;
	register vm_offset_t va, endva;
	register int vr, vs;
{
	register int *pte0, i, perpage, npg;
	register struct pvlist *pv;
	register int nleft;
d4405 1
a4405 1
	vm_offset_t pa;
d4408 4
a4411 4
	register struct pvlist *pv, *pv0, *npv;
	register struct pmap *pm;
	register int va, vr, vs, pteva, tpte;
	register int flags, nleft, i, s, ctx;
d4592 2
a4593 2
	register struct pmap *pm;
	vm_offset_t sva, eva;
d4596 2
a4597 2
	register int va, nva, vr, vs;
	register int s, ctx;
d4640 1
a4640 1
			register int *pte = &sp->sg_pte[VA_VPG(va)];
d4648 1
a4648 1
				register int tpte;
d4667 1
a4667 1
				register int pteva;
d4696 2
a4697 2
	register struct pmap *pm;
	register vm_offset_t va;
d4701 1
a4701 1
	register int vr, vs, tpte, newprot, ctx, s;
d4741 1
a4741 1
		register int *pte = &sp->sg_pte[VA_VPG(va)];
d4803 1
a4803 1
	vm_offset_t pa;
d4806 4
a4809 4
	register struct pvlist *pv, *pv0, *npv;
	register struct pmap *pm;
	register int va, vr, vs, tpte;
	register int flags, nleft, s, ctx;
d4940 2
a4941 2
	register struct pmap *pm;
	vm_offset_t sva, eva;
d4944 2
a4945 2
	register int va, nva, vr, vs;
	register int s, ctx;
d5025 2
a5026 2
	register struct pmap *pm;
	register vm_offset_t va;
d5030 1
a5030 1
	register int tpte, newprot, ctx, s;
d5097 3
a5099 2
	register struct pmap *pm;
	vm_offset_t va, pa;
d5103 2
a5104 2
	register struct pvlist *pv;
	register int pteproto, ctx;
d5154 2
a5155 2
	register struct pmap *pm;
	vm_offset_t va;
d5158 2
a5159 2
	register struct pvlist *pv;
	register int pteproto;
d5161 1
a5161 1
	register int vr, vs, tpte, i, s;
d5173 1
a5173 1
		vm_offset_t tva;
d5190 1
a5190 1
		register int addr;
d5233 1
a5233 1
		register int tva;
d5278 2
a5279 2
	register struct pmap *pm;
	vm_offset_t va;
d5282 2
a5283 2
	register struct pvlist *pv;
	register int pteproto;
d5285 1
a5285 1
	register int vr, vs, *pte, tpte, pmeg, s, doflush;
d5317 2
a5318 2
		register int i;
		register int size = NSEGRG * sizeof (struct segmap);
d5338 1
a5338 1
		register int size = NPTESG * sizeof *pte;
d5376 1
a5376 1
			register int addr;
d5464 3
a5466 2
	register struct pmap *pm;
	vm_offset_t va, pa;
d5470 2
a5471 2
	register struct pvlist *pv;
	register int pteproto, ctx;
d5530 2
a5531 2
	register struct pmap *pm;
	vm_offset_t va;
d5534 2
a5535 2
	register struct pvlist *pv;
	register int pteproto;
d5537 1
a5537 1
	register int vr, vs, tpte, s;
d5557 1
a5557 1
		register int addr;
d5613 2
a5614 2
	register struct pmap *pm;
	vm_offset_t va;
d5617 2
a5618 2
	register struct pvlist *pv;
	register int pteproto;
d5620 1
a5620 1
	register int vr, vs, *pte, tpte, s;
d5638 1
a5638 1
		register int size = NSEGRG * sizeof (struct segmap);
d5713 1
a5713 1
			register int addr;
d5780 1
a5780 1
	vm_offset_t va;
d5794 1
a5794 1
vm_offset_t
d5796 2
a5797 2
	register struct pmap *pm;
	vm_offset_t va;
d5799 2
a5800 2
	register int tpte;
	register int vr, vs;
d5824 1
a5824 1
		register int ctx = getcontext4();
d5838 1
a5838 1
		register int *pte = sp->sg_pte;
d5868 1
a5868 1
vm_offset_t
d5871 1
a5871 1
	vm_offset_t va;
d5937 3
a5939 3
	vm_offset_t dst_addr;
	vm_size_t len;
	vm_offset_t src_addr;
d5959 1
a5959 1
		vm_offset_t pa;
d6028 1
a6028 1
	register vm_offset_t pa;
d6030 1
a6030 1
	register struct pvlist *pv;
d6044 1
a6044 1
	register vm_offset_t pa;
d6046 1
a6046 1
	register struct pvlist *pv;
d6061 1
a6061 1
	vm_offset_t pa;
d6063 1
a6063 1
	register struct pvlist *pv;
d6077 1
a6077 1
	vm_offset_t pa;
d6079 1
a6079 1
	register struct pvlist *pv;
d6106 1
a6106 1
	register vm_offset_t pa;
d6108 1
a6108 1
	register struct pvlist *pv;
d6122 1
a6122 1
	register vm_offset_t pa;
d6124 1
a6124 1
	register struct pvlist *pv;
d6139 1
a6139 1
	vm_offset_t pa;
d6141 1
a6141 1
	register struct pvlist *pv;
d6155 1
a6155 1
	vm_offset_t pa;
d6157 1
a6157 1
	register struct pvlist *pv;
d6182 1
a6182 1
	vm_offset_t start, end;
d6198 1
a6198 1
	register vm_offset_t pa;
d6200 2
a6201 2
	register caddr_t va;
	register int pte;
d6230 1
a6230 1
	vm_offset_t src, dst;
d6232 2
a6233 2
	register caddr_t sva, dva;
	register int spte, dpte;
d6268 1
a6268 1
	register vm_offset_t pa;
d6270 2
a6271 2
	register caddr_t va;
	register int pte;
d6294 1
a6294 1
	setpte4m((vm_offset_t) va, pte);
d6296 1
a6296 1
	setpte4m((vm_offset_t) va, SRMMU_TEINVALID);
d6311 1
a6311 1
	vm_offset_t src, dst;
d6313 2
a6314 2
	register caddr_t sva, dva;
	register int spte, dpte;
d6341 2
a6342 2
	setpte4m((vm_offset_t) sva, spte);
	setpte4m((vm_offset_t) dva, dpte);
d6345 2
a6346 2
	setpte4m((vm_offset_t) sva, SRMMU_TEINVALID);
	setpte4m((vm_offset_t) dva, SRMMU_TEINVALID);
d6356 1
a6356 1
vm_offset_t
d6376 1
a6376 1
	vm_offset_t pa;
d6385 1
a6385 1
			pte = getpte4m((vm_offset_t) va);
d6395 1
a6395 1
			setpte4m((vm_offset_t) va, pte);
d6432 2
a6433 2
	register caddr_t va;
	register int npages;
d6442 1
a6442 1
		register int pte = getpte4(va);
d6453 1
a6453 1
	register struct pmap *pm;
d6455 3
a6457 3
	register int idx, total;
	register struct regmap *rp;
	register struct segmap *sp;
d6480 2
a6481 2
	register vm_offset_t foff;
	register vm_offset_t *vap;
d6483 2
a6484 2
	register vm_offset_t va = *vap;
	register long d, m;
d6529 1
a6529 1
	 * have to make sure to clean the register windows before we set
d6740 2
a6741 2
	register daddr_t blkno;
	register int (*dump)	__P((dev_t, daddr_t, caddr_t, size_t));
d6746 2
a6747 2
	register int	error = 0;
	register int	i, memsegoffset, pmegoffset;
d6751 1
a6751 1
	register int	pmeg;
d6824 1
a6824 1
		register int va = 0;
d6853 1
a6853 1
	vm_offset_t va;
d6901 3
a6903 3
	register int i;
	register int *regtbl;
	register int te;
d6945 2
a6946 2
	register int ctx;
	register caddr_t addr;
d6949 2
a6950 2
	register int *curtbl;
	register int curpte;
d7015 2
a7016 2
	register int reg;
	register int start, stop;
d7018 3
a7020 3
	register int i;
	register int addr;
	register int pte;
@


1.27
log
@make this compile for !MNN case. (which is the default). DUH.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 1999/04/23 17:38:48 art Exp $	*/
d5872 1
a5872 1
	register struct pmap *pm;
d5887 1
a5887 2
	rm = &pm->pm_regmap[VA_VREG(va)];
	if (rm == NULL) {
d5894 3
a5896 2
	sm = &rm->rg_segmap[VA_VSEG(va)];
	if (sm == NULL) {
d5899 1
a5899 1
			panic("pmap_extract: no segmap");
d5903 10
a5913 1

@


1.26
log
@last part of UVM stuff for pmap (aborted yesterday by a routing loop).
Just changed names of variables and functions
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 1999/04/22 20:36:22 art Exp $	*/
d362 1
d364 1
d814 1
d872 1
a2752 1
	register struct memarr *mp;
a3104 1
	register struct memarr *mp;
@


1.25
log
@MACHINE_NEW_NONCONTIG code. From NetBSD. Needed by UVM
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 1999/04/22 17:07:30 art Exp $	*/
d76 4
d214 1
a214 1
#define pvhead(pa)	(&pv_table[atop((pa) - vm_first_phys)])
d2060 1
a2060 1
			register int tpte;
a2063 2
				extern vm_offset_t pager_sva, pager_eva;

d2068 7
d2078 1
a2383 2
			extern vm_offset_t pager_sva, pager_eva;

d2388 7
d2398 1
d2703 4
d2709 1
d2768 4
d2774 1
d3583 7
d3591 1
@


1.24
log
@implement pmap_{,de}activate
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 1999/01/11 05:11:59 millert Exp $	*/
d349 1
d352 1
d358 1
d720 1
d734 26
d764 2
a765 2
	register struct memarr *mp;
	register int n;
d767 3
a769 3
	register struct memarr *mpj;
	register int i, j;
	register u_int addr, len;
d809 59
d879 1
a879 1
	register struct memarr *mp;
d922 1
d2869 1
a2869 13
	/*
	 * Grab physical memory list, so pmap_next_page() can do its bit.
	 */
	npmemarr = makememarr(pmemarr, MA_SIZE, MEMARR_AVAILPHYS);
	sortm(pmemarr, npmemarr);
	if (pmemarr[0].addr != 0) {
		printf("pmap_bootstrap: no kernel memory?!\n");
		callrom();
	}
	avail_end = pmemarr[npmemarr-1].addr + pmemarr[npmemarr-1].len;
	avail_next = avail_start;
	for (physmem = 0, mp = pmemarr, j = npmemarr; --j >= 0; mp++)
		physmem += btoc(mp->len);
d3049 1
a3049 1
		register int mask = ~PG_NC;	/* XXX chgkprot is busted */
d3051 1
a3051 1
		register int mask = ~(PG_W | PG_NC);
d3057 3
d3156 2
a3157 16
	/*
	 * Grab physical memory list use it to compute `physmem' and
	 * `avail_end'. The latter is used in conjuction with
	 * `avail_start' and `avail_next' to dispatch left-over
	 * physical pages to the VM system.
	 */
	npmemarr = makememarr(pmemarr, MA_SIZE, MEMARR_AVAILPHYS);
	sortm(pmemarr, npmemarr);
	if (pmemarr[0].addr != 0) {
		printf("pmap_bootstrap: no kernel memory?!\n");
		callrom();
	}
	avail_end = pmemarr[npmemarr-1].addr + pmemarr[npmemarr-1].len;
	avail_next = avail_start;
	for (physmem = 0, mp = pmemarr, j = npmemarr; --j >= 0; mp++)
		physmem += btoc(mp->len);
d3180 1
a3180 1
	unavail_start = (int)p - KERNBASE;
d3228 1
a3228 1
	unavail_end = (int)p - KERNBASE;
d3395 3
@


1.23
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 1998/05/29 16:21:35 jason Exp $	*/
d6400 39
@


1.22
log
@Sync with NetBSD:
In kvm_uncache(), turn off the PTE cache bit even after calling
pv_changepte(). Reason: the managed() macro does not take into
account the gap in the managed pages range that may have been introduced
by the page table allocation in bootstrap().

Don't bzero() the IO page table; it is fully initialized in iommu_attach().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 1998/05/10 18:30:40 deraadt Exp $	*/
d824 1
a824 1
		panic("pmap_next_page: too much memory?!\n");
@


1.21
log
@merge in newer netbsd changes
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.20 1998/03/04 10:58:11 niklas Exp $	*/
/*	$NetBSD: pmap.c,v 1.115 1998/05/06 14:17:53 pk Exp $ */
d356 3
a3117 1
	bzero(kernel_iopte_table, p - (caddr_t) kernel_iopte_table);
d6280 5
a6284 1
			} else {
a6285 5
				pte &= ~SRMMU_PG_C;
				setpte4m((vm_offset_t) va, pte);
				if ((pte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
					cache_flush_page((int)va);
			}
d6301 5
a6305 7
			} else {

				pte |= PG_NC;
				setpte4(va, pte);
				if ((pte & PG_TYPE) == PG_OBMEM)
					cache_flush_page((int)va);
			}
@


1.20
log
@Adapt comments to reality
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.19 1997/11/07 08:11:44 deraadt Exp $	*/
/*	$NetBSD: pmap.c,v 1.96 1997/09/14 19:20:48 pk Exp $ */
d195 12
a206 10
 * Sun4M bits are different (reversed), and at a different location in the
 * pte. Now why did they do that?
 */
#define PV_MOD	1		/* page modified */
#define PV_REF	2		/* page referenced */
#define PV_NC	4		/* page cannot be cached */
#define PV_REF4M	1	/* page referenced on sun4m */
#define PV_MOD4M	2	/* page modified on 4m (why reversed?!?) */
#define PV_C4M		4	/* page _can_ be cached on 4m */
/*efine	PV_ALLF	7		** all of the above */
d452 1
a452 1
/*static*/ int  pv_link4m __P((struct pvlist *, struct pmap *, vm_offset_t));
d468 1
a468 1
/*static*/ int  pv_link4_4c __P((struct pvlist *, struct pmap *, vm_offset_t));
d646 2
a647 2
	register vm_offset_t va;
	register int pte;
d649 4
a652 4
	register struct pmap *pm;
	register struct regmap *rm;
	register struct segmap *sm;
	register union ctxinfo *c;
a653 11
	/*
	 * Walk tables to find pte. We use ctxinfo to locate the pmap
	 * from the current context.
	 */
#if 0
#ifdef DEBUG
	if (ctxbusyvector[getcontext4m()] == 0)
		panic("setpte4m: no pmap for current context (%d)",
		      getcontext4m());
#endif
#endif
d666 7
a672 2
	if (rm->rg_segmap == NULL || sm == NULL || sm->sg_pte == NULL)
		panic("setpte4m: no segmap for va %p", (caddr_t)va);
d1910 5
d1921 2
a1922 2
	register struct pvlist *pv0;
	register int bis, bic;
d1924 4
a1927 4
	register int *pte;
	register struct pvlist *pv;
	register struct pmap *pm;
	register int va, vr, vs, flags;
a1939 1
	flags = pv0->pv_flags;
d1998 1
a1998 1
				flags |= MR4_4C(tpte);
d2003 6
a2010 1
	pv0->pv_flags = flags;
d2112 4
d2119 2
d2123 5
d2129 1
a2129 1
			pv->pv_flags &= ~PV_NC;
d2145 1
a2145 1
	if (pv->pv_flags & PV_NC) {
d2151 1
a2151 1
			if (BADALIAS(va, npv->pv_va))
d2153 1
a2153 1
		pv->pv_flags &= ~PV_NC;
d2164 5
a2168 4
pv_link4_4c(pv, pm, va)
	register struct pvlist *pv;
	register struct pmap *pm;
	register vm_offset_t va;
d2170 4
a2173 2
	register struct pvlist *npv;
	register int ret;
d2181 2
a2182 1
		return (0);
a2188 1
	ret = 0;
d2190 1
a2190 1
	if (pv->pv_flags & PV_NC) {
a2193 1
		/* MAY NEED TO DISCACHE ANYWAY IF va IS IN DVMA SPACE? */
d2195 4
d2208 2
a2209 1
				pv->pv_flags |= PV_NC;
d2219 1
d2239 5
d2247 2
a2248 2
	register struct pvlist *pv0;
	register int bis, bic;
d2250 3
a2252 3
	register struct pvlist *pv;
	register struct pmap *pm;
	register int va, vr, flags;
a2264 1
	flags = pv0->pv_flags;
d2266 1
a2266 1
		register int tpte;
d2314 1
a2314 1
		flags |= (tpte >> PG_M_SHIFT4M) & (PV_MOD4M|PV_REF4M|PV_C4M);
d2316 8
a2324 1
		setpgt4m(&sp->sg_pte[VA_SUN4M_VPG(va)], tpte);
a2325 1
	pv0->pv_flags = flags;
d2382 1
a2382 2
			flags |= (tpte >> PG_M_SHIFT4M) &
				 (PV_MOD4M|PV_REF4M|PV_C4M);
d2419 4
d2426 2
d2430 5
d2436 1
a2436 1
			pv->pv_flags |= PV_C4M;
d2452 1
a2452 1
	if (!(pv->pv_flags & PV_C4M)) {
d2458 2
a2459 1
			if (BADALIAS(va, npv->pv_va))
d2461 1
a2461 1
		pv->pv_flags |= PV_C4M;
d2472 5
a2476 4
pv_link4m(pv, pm, va)
	register struct pvlist *pv;
	register struct pmap *pm;
	register vm_offset_t va;
d2478 4
a2481 2
	register struct pvlist *npv;
	register int ret;
d2489 2
a2490 2
		pv->pv_flags |= PV_C4M;
		return (0);
a2496 1
	ret = 0;
d2498 1
a2498 1
	if (!(pv->pv_flags & PV_C4M)) {
d2503 4
d2516 2
a2517 1
				pv->pv_flags &= ~PV_C4M;
d2528 1
a2528 1
	npv->pv_flags |= (ret == SRMMU_PG_C ? 0 : PV_C4M);
d5125 1
a5125 1
		pteproto |= pv_link4_4c(pv, pm, va);
d5315 1
a5315 1
		pteproto |= pv_link4_4c(pv, pm, va);
d5491 1
a5491 1
	        pteproto &= ~(pv_link4m(pv, pm, va));
d5652 1
a5652 1
		pteproto &= ~(pv_link4m(pv, pm, va));
d5849 5
a5853 2
			rm = &src_pmap->pm_regmap[VA_VREG(src_addr)];
			if (rm == NULL)
d5855 2
a5856 2
			sm = &rm->rg_segmap[VA_VSEG(src_addr)];
			if (sm == NULL || sm->sg_npte == 0)
d5858 1
d6257 2
a6258 2
	register caddr_t va;
	register int npages;
d6260 2
a6261 1
	register int pte;
d6265 4
d6273 12
a6284 4
			pte &= ~SRMMU_PG_C;
			setpte4m((vm_offset_t) va, pte);
			if ((pte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
				cache_flush_page((int)va);
d6286 2
d6295 12
a6306 4
			pte |= PG_NC;
			setpte4(va, pte);
			if ((pte & PG_TYPE) == PG_OBMEM)
				cache_flush_page((int)va);
@


1.19
log
@simple_lock api changed slightly
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 1997/09/17 06:47:21 downsj Exp $	*/
d760 1
a760 1
 *       pmap_startup(), pmap_steal_memory()
@


1.18
log
@NETBSD_CURRENT_970916.  Lot's just ID changes, since changes don't apply to
us.  Includes some pmap changes, for which I don't have the original commit
message(s) handy.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 1997/08/08 08:27:36 downsj Exp $	*/
d69 1
a74 1
#include <vm/lock.h>
d2650 1
a2650 1
	simple_lock_init(kernel_pmap_store.pm_lock);
@


1.17
log
@Mostly sync to NetBSD-current 970804.

GENERIC currently compiles and runs; some devices (isp) are not complete and
not yet enabled.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.c,v 1.91 1997/07/29 09:42:11 fair Exp $ */
d641 1
a641 1
/* Set the page table entry for va to pte. Flushes cache. */
a651 2
	cache_flush_page(va);

d653 2
a654 2
	 * Now walk tables to find pte. We use ctxinfo to locate the pmap
	 * from the current context
d993 2
a994 2
 * the kernel at KERNBASE (0xf8000000) since we don't want to map 16M of
 * physical memory for the kernel. Thus the kernel must be installed later!
d1071 5
a1075 4
	 * (i.e. regions 0xf8 to 0xff). We also ignore region 0xf8, since
	 * that is the 16MB L1 mapping that the ROM used to map the kernel
	 * in initially. Later, we will rebuild a new L3 mapping for the
	 * kernel and install it before switching to the new pagetables.
d3077 1
a3077 1
	 * kernel (from regions 0xf8 -> 0xff). This takes 130k of space, but
d3128 1
a3128 1
	for (reg = VA_VREG(KERNBASE); reg < NKREG+VA_VREG(KERNBASE); reg++) {
a3134 1
		int kregnum = reg - VA_VREG(KERNBASE);
d3136 1
a3136 1
		rp = &pmap_kernel()->pm_regmap[reg];
d3139 1
a3139 1
		    &kernel_segtable_store[kregnum * SRMMU_L2SIZE];
d3141 1
a3141 1
		setpgt4m(&pmap_kernel()->pm_reg_ptps[reg],
d3148 1
a3148 1
			rp->rg_segmap = &kernel_segmap_store[kregnum * NSEGRG];
d3160 1
a3160 1
				[((kregnum * NSEGRG) + seg) * SRMMU_L3SIZE];
d5731 1
a5731 1
			printf("getptesw4m: no regmap entry");
d5739 1
a5739 1
			panic("getptesw4m: no segmap");
d5766 1
d5774 16
a5789 1
#if 0
d5791 17
a5807 3
		register int i, pte;
		for (i = 0; i < len/NBPG; i++) {
			pte = getptesw4m(src_pmap, src_addr);
d5809 1
a5809 3
				   ptoa((pte & SRMMU_PPNMASK) >>
					SRMMU_PPNSHIFT) |
				    VA_OFF(src_addr),
d5811 1
a5811 1
					? VM_PROT_WRITE| VM_PROT_READ
d6614 50
d6692 2
a6693 2
	printf("Testing kernel region 0xf8: ");
	test_region(0xf8, 4096, avail_start);
d6740 1
a6740 1
	curtbl = ((curpte & ~0x3) << 4) | (0xf8 << RGSHIFT); /* correct for krn*/
d6754 1
a6754 1
	curtbl = ((curpte & ~0x3) << 4) | (0xf8 << RGSHIFT); /* correct for krn*/
d6768 1
a6768 1
	curtbl = ((curpte & ~0x3) << 4) | (0xf8 << RGSHIFT); /* correct for krn*/
d6809 3
a6811 2
			if (reg == 0xf8) /* kernel permissions are different */
			    continue;
@


1.16
log
@Fix a major sun4m stability problem showed up as random segv's and other
failures in innocent programs.  Typically something like a make build
of libc or something executing a large number of shell commands would
fail, dumping core in sh or random faults in as or cc*.

According to Chris Torek, the problem is that he make the destination
pages for zero and copy be non-cached to prevent wiping otherwise
relevant stuff from the cache.

On systems with a L2 cache like the SS10 modules with cache, setting
the pages as non-cacheable inhibits updating the chache, and also
disable PA chache snooping, doesn't do anything about possible dirty
lines in the L2 cache, which may eventually get flushed and corrupt the
nice new page.

It's not clear that he's got the failure mode 100% correct, but making
the pages cacheable does seem to avoid the symptoms and testing the
e-cache flush hypothesis is difficult.

Aaron Brown proposed a fix that was conditional on mmutype, and netbsd-current
has it conditional on a cpu.quirk, at the moment this is unconditional, the
performance hit is arguable...
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.15 1997/06/26 01:00:59 downsj Exp $	*/
/*	$NetBSD: pmap.c,v 1.60.4.1 1996/06/12 20:36:30 pk Exp $ */
d66 2
a68 2
#include <sys/core.h>
#include <sys/exec_aout.h>
d74 1
d86 1
d137 3
d148 1
d157 2
d186 4
a189 4
	struct	pvlist *pv_next;	/* next pvlist, if any */
	struct	pmap *pv_pmap;		/* pmap of this va */
	vm_offset_t	pv_va;		/* virtual address */
	int	pv_flags;		/* flags (below) */
d260 7
d282 2
a283 4
int	seginval;		/* the invalid segment number */
#ifdef MMU_3L
int	reginval;		/* the invalid region number */
#endif
d306 7
a313 1
int	ncontext;
d319 2
a320 2
char	*ctxbusyvector;		/* [4m] tells what contexts are busy */
int	*ctx_phys_tbl;		/* [4m] SRMMU-edible context table */
d326 2
a327 3
#ifdef MMU_3L
smeg_t		tregion;
#endif
d332 1
a332 1
#ifdef SUN4M
a338 2

static 	int cant_cache_pagetables = 0;	/* 1 if page tables are uncachable */
d349 2
a352 10
#ifdef SUN4M
#if 0 /* not needed anymore */
static void * pmap_bootstrap_malloc __P((unsigned long, int, int));

caddr_t		minipool_start;
caddr_t		minipool_current;
#define MINIPOOL_SIZE	8192  /* number of bytes to reserve for early malloc */
                              /* (must be multiple of pagesize) */
#endif
#endif
d384 2
d387 5
a391 2
					? lda(SRMMU_CXR, ASI_SRMMU) \
					: lduba(AC_CONTEXT, ASI_CONTROL))
d393 2
a394 2
					? sta(SRMMU_CXR, ASI_SRMMU, c) \
					: stba(AC_CONTEXT, ASI_CONTROL, c))
d410 1
d435 4
a438 5
static void mmu_setup4m_L1 __P((int, caddr_t, struct pmap *, caddr_t *));
static void mmu_setup4m_L2 __P((int, caddr_t, struct regmap *, caddr_t *,
				struct segmap *));
static int  mmu_setup4m_L3 __P((int, caddr_t));
/*static*/ void	mmu_reservemon4m __P((struct pmap *, caddr_t *));
d601 10
d630 1
a630 1
		panic("setpte4m: no regmap entry");
d636 1
a636 1
		panic("setpte4m: no segmap for va %p", (caddr_t)va);
d638 1
a638 1
	sm->sg_pte[VA_SUN4M_VPG(va)] = pte; /* set new pte */
d652 1
a652 2
	if (vactype != VAC_NONE)
		cache_flush_page(va);
d658 1
d660 1
a660 1
	if (ctxbusyvector[getcontext()] == 0)
d662 2
a663 1
		      getcontext());
d665 1
a665 1
	c = &ctxinfo[getcontext()];
d681 1
a681 1
	sm->sg_pte[VA_SUN4M_VPG(va)] = pte; /* set new pte */
d683 1
d688 8
a695 5
#if defined(MMU_3L)
#define CTX_USABLE(pm,rp)	(CPU_ISSUN4M \
				    ? ((pm)->pm_ctx != NULL ) \
				    : ((pm)->pm_ctx != NULL && \
					(!mmu_3l || (rp)->rg_smeg != reginval)))
d719 1
d821 2
a822 1
	}
d905 1
a905 1
	register vm_offset_t va = 0, eva = 0;
d907 1
a907 1
#ifdef MMU_3L
d914 1
a914 1
		panic("mmu_reservemon called on Sun4M machine");
d938 2
a939 2
#ifdef MMU_3L
		if (mmu_3l && vr != lastvr) {
d955 2
a956 3
#ifdef MMU_3L
		if (!mmu_3l)
#endif
d995 1
a995 1
 * the kernel at 0xf8000000 (KERNBASE) since we don't want to map 16M of
a998 1
 * NOTE ALSO: Must set up kernel pmap as all invalid before this is called.
d1001 1
a1001 1
mmu_reservemon4m(kpmap, kmemtop)
a1002 1
	register caddr_t *kmemtop; /* Note: this is a *virtual* address! */
d1004 1
a1004 2
	unsigned int ctxtblptr;
	int wierdviking = 0;
d1006 1
a1006 2
	register caddr_t tableptr;
	unsigned int mmupcrsav;
d1008 1
a1008 1
/*XXX-GCC!*/mmupcrsav = 0;
d1021 1
a1021 1
	ctxtblptr = (lda(SRMMU_CXTPTR,ASI_SRMMU) << SRMMU_PPNPASHIFT);
d1028 1
a1028 2
	if ((getpsr() & 0x40000000) && (!(lda(SRMMU_PCR,ASI_SRMMU) & 0x800))) {
		wierdviking = 1;
d1030 1
a1030 13
		    ((mmupcrsav = lda(SRMMU_PCR,ASI_SRMMU)) | SRMMU_PCR_AC));
	}
	if (ncontext < 1024) {
		/* to keep supersparc happy, still align on 4K boundary */
		ctx_phys_tbl = (int *)roundup((u_int) *kmemtop, 4096);
		*kmemtop = (caddr_t)((u_int)ctx_phys_tbl + 4096);
		qzero(ctx_phys_tbl, 4096);
	} else {
		ctx_phys_tbl = (int *)roundup((u_int) *kmemtop,
					      ncontext * sizeof(int));
		*kmemtop = (caddr_t)((u_int)ctx_phys_tbl +
				     (ncontext * sizeof(int)));
		qzero(ctx_phys_tbl, ncontext * sizeof(int));
d1033 1
a1033 1
	te = lda(ctxtblptr, ASI_BYPASS);
d1036 3
a1038 2
		ctx_phys_tbl[0] = SRMMU_TEINVALID;
		panic("mmu_reservemon4m: no existing L0 mapping! (How are we running?");
d1048 1
a1048 9
		tableptr = (caddr_t) roundup((u_int) *kmemtop,
					     SRMMU_L1SIZE * sizeof(long));
		*kmemtop = tableptr + (SRMMU_L1SIZE * sizeof(long));
		bzero(tableptr, *kmemtop - tableptr);
		ctx_phys_tbl[0] = (VA2PA(tableptr) >> SRMMU_PPNPASHIFT) |
		    SRMMU_TEPTD;
		kpmap->pm_reg_ptps = (int *)tableptr;
		kpmap->pm_reg_ptps_pa = VA2PA(tableptr);
		mmu_setup4m_L1(te, tableptr, kpmap, kmemtop);
d1054 2
a1055 2
	if (wierdviking == 1) {
		sta(SRMMU_PCR, ASI_SRMMU, mmupcrsav);
d1060 1
a1060 1
mmu_setup4m_L1(regtblptd, newtableptr, kpmap, kmemtop)
a1061 1
	caddr_t newtableptr;	/* Virtual ptr to new region table */
a1062 1
	register caddr_t *kmemtop;
d1067 1
a1067 4
	caddr_t segtblptr;
	register caddr_t pagtblptr;
	struct regmap *thisregmap = NULL;
	struct segmap *segmaps;
d1070 2
a1071 1
	/* Here we scan the region table to copy any entries which appear.
d1078 3
a1080 2
	regtblrover = ((regtblptd & ~SRMMU_TETYPE) << SRMMU_PPNPASHIFT) +
	    (VA_VREG(KERNBASE)+1) * sizeof(long);      /* kernel only */
d1084 4
a1090 1
			((int *)newtableptr)[i] = SRMMU_TEINVALID;
d1092 1
d1095 2
a1096 1
			printf("mmu_reservemon4m: converting region 0x%x from L1->L3\n",i);
d1098 4
a1101 2
			/* We must build segment and page tables, then
			 *  fill them in
d1103 1
a1103 13
			thisregmap = &kpmap->pm_regmap[i];
			bzero(thisregmap, sizeof(struct regmap));
			segmaps = &kernel_segmap_store[(i-VA_VREG(KERNBASE))*NSEGRG];
			segtblptr = (caddr_t) roundup((u_int) *kmemtop,
					    SRMMU_L2SIZE * sizeof(long));
			*kmemtop = segtblptr + (SRMMU_L2SIZE * sizeof(long));
			bzero(segtblptr, *kmemtop - segtblptr);
			bzero(segmaps, NSEGRG * sizeof(struct segmap));
			thisregmap->rg_segmap = segmaps;
			thisregmap->rg_seg_ptps = (int *)segtblptr;
			((int *)newtableptr)[i] = (VA2PA(segtblptr) >>
						   SRMMU_PPNPASHIFT) |
						       SRMMU_TEPTD;
d1105 1
a1105 10
				thisregmap->rg_nsegmap++;
				pagtblptr = (caddr_t) roundup((u_int) *kmemtop,
						    SRMMU_L3SIZE * sizeof(int));
				*kmemtop = segtblptr + (SRMMU_L3SIZE *
							sizeof(long));
				bzero(pagtblptr, *kmemtop - pagtblptr);
				((int *)segtblptr)[j] = (VA2PA(pagtblptr) >>
							 SRMMU_PPNPASHIFT) |
							     SRMMU_TEPTD;
				segmaps[j].sg_pte = (int *)pagtblptr;
d1108 2
a1109 2
					segmaps[j].sg_npte++;
					((int *)pagtblptr)[k] =
d1120 1
d1122 1
a1122 14
			thisregmap = &kpmap->pm_regmap[i];
			bzero(thisregmap, sizeof(struct regmap));
			segtblptr = (caddr_t) roundup((u_int)*kmemtop,
					    SRMMU_L2SIZE * sizeof(long));
			*kmemtop = segtblptr + (SRMMU_L2SIZE * sizeof(long));
			segmaps = &kernel_segmap_store[(i-(KERNBASE >> RGSHIFT))*NSEGRG];
			bzero(segtblptr, *kmemtop - segtblptr);
			bzero(segmaps, NSEGRG * sizeof(struct segmap));
			thisregmap->rg_segmap = segmaps;
			thisregmap->rg_seg_ptps = (int *)segtblptr;
			((int *)newtableptr)[i] = (VA2PA(segtblptr) >>
						   SRMMU_PPNPASHIFT) |
						       SRMMU_TEPTD;
			mmu_setup4m_L2(te, segtblptr, thisregmap, kmemtop, segmaps);
d1124 1
d1132 1
a1132 1
mmu_setup4m_L2(segtblptd, newtableptr, pregmap, kmemtop, segmaps)
d1134 1
a1134 4
	caddr_t newtableptr;
	struct regmap *pregmap;
	register caddr_t *kmemtop;
	struct segmap *segmaps;
d1139 1
a1139 2
	register caddr_t pagtblptr;
	struct segmap *thissegmap = NULL;
d1143 3
a1148 1
			((int *)newtableptr)[i] = SRMMU_TEINVALID;
d1150 1
d1153 1
a1153 1
			printf("mmu_reservemon4m: converting L2 entry at segment 0x%x to L3\n",i);
d1155 5
a1159 12
			pregmap->rg_nsegmap++;
			/* We must build page tables and fill them in */
			pagtblptr = (caddr_t) roundup((u_int) *kmemtop,
						      SRMMU_L3SIZE*sizeof(int));
			*kmemtop = pagtblptr + (SRMMU_L3SIZE *
						sizeof(int));
			bzero(pagtblptr, *kmemtop - pagtblptr);
			((int *)newtableptr)[i] = (VA2PA(pagtblptr) >>
						 SRMMU_PPNPASHIFT) |
						     SRMMU_TEPTD;
			segmaps[i].sg_pte = (int *) pagtblptr;

d1161 2
a1162 2
				segmaps[i].sg_npte++;
				((int *)pagtblptr)[k] =
d1172 1
d1174 1
a1174 11
			pregmap->rg_nsegmap++;
			thissegmap = &segmaps[i];
			pagtblptr = (caddr_t) roundup((u_int) *kmemtop,
						      SRMMU_L3SIZE*sizeof(int));
			*kmemtop = pagtblptr + (SRMMU_L3SIZE * sizeof(int));
			bzero(pagtblptr, *kmemtop - pagtblptr);
			((int *)newtableptr)[i] = (VA2PA(pagtblptr) >>
						   SRMMU_PPNPASHIFT) |
						       SRMMU_TEPTD;
			thissegmap->sg_pte = (int *) pagtblptr;
			thissegmap->sg_npte += mmu_setup4m_L3(te, pagtblptr);
d1176 1
d1183 2
a1184 2
int
mmu_setup4m_L3(pagtblptd, newtableptr)
d1186 1
a1186 1
	register caddr_t newtableptr;
d1189 1
a1189 1
	register int i, n = 0;
a1196 1
			((int *)newtableptr)[i] = SRMMU_TEINVALID;
d1199 2
a1200 2
			((int *)newtableptr)[i] = te | PPROT_U2S_OMASK;
			n++;
a1207 1
	return n;
a1210 5
/*
 * TODO: agree with the ROM on physical pages by taking them away
 * from the page list, rather than having a dinky BTSIZE above.
 */

d1273 7
d1301 1
a1301 1
		printf("me_alloc: stealing pmeg %x from pmap %p\n",
d1311 7
d1337 1
a1337 1
	ctx = getcontext();
d1340 1
a1340 2
		if (vactype != VAC_NONE)
			cache_flush_segment(me->me_vreg, me->me_vseg);
d1344 1
a1344 2
#ifdef MMU_3L
		if (mmu_3l)
a1345 1
#endif
d1380 1
a1380 1
	setcontext(ctx);	/* done with old context */
d1433 1
a1433 1
if (getcontext() != 0) panic("me_free: ctx != 0");
d1435 1
a1435 2
#ifdef MMU_3L
		if (mmu_3l)
a1436 1
#endif
d1461 3
d1466 3
d1474 3
d1479 1
a1479 1
#ifdef MMU_3L
d1501 1
a1501 1
			printf("region_alloc: got smeg %x\n", me->me_cookie);
d1527 1
a1527 1
		printf("region_alloc: stealing smeg %x from pmap %p\n",
d1538 1
a1538 1
	ctx = getcontext();
d1541 1
a1541 2
		if (vactype != VAC_NONE)
			cache_flush_region(me->me_vreg);
d1553 1
a1553 1
	setcontext(ctx);	/* done with old context */
d1584 1
a1584 1
		printf("region_free: freeing smeg %x from pmap %p\n",
d1593 1
a1593 2
		if (vactype != VAC_NONE)
			cache_flush_region(me->me_vreg);
d1623 1
a1623 2
	register vm_offset_t va;
	vm_prot_t prot;
d1640 1
a1640 1
printf("mmu_pagein: kernel wants map at va %x, vr %d, vs %d\n", va, vr, vs);
d1646 2
a1647 2
#ifdef MMU_3L
	if (mmu_3l && rp->rg_smeg == reginval) {
a1687 50
#if defined(DEBUG) && defined(SUN4M)
/*
 * `Page in' (load or inspect) an MMU entry; called on page faults.
 * Returns -1 if the desired page was marked valid (in which case the
 * fault must be a bus error or something), or 0 (segment loaded but
 * PTE not valid, or segment not loaded at all).
 *
 * The SRMMU does not have the concept of `loading PMEGs into the MMU'.
 * For now, we use it to debug certain sporadic and strange memory
 * fault traps.
 */
int
mmu_pagein4m(pm, va, prot)
	register struct pmap *pm;
	register vm_offset_t va;
	register int prot;
{
	register int pte;
	register int bits;

	pte = getpte4m(va);
	if ((pte & SRMMU_TETYPE) != SRMMU_TEPTE)
		/* pte not found == not valid */
		return 0;

	/*
	 * If pte has "correct" protection (according to bits), then
	 * something bizarre has happened, so we return -1 to generate
	 * a fault. Otherwise, we return 0 and let the VM code reload
	 * the page.
	 * XXX: Does this actually work, or will it cause a loop?
	 */

	if (prot != VM_PROT_NONE)
		bits = ((prot & VM_PROT_WRITE) ? PPROT_RWX_RWX : PPROT_RX_RX);
	else
		bits = 0;
	/* If we're in kernelland, mask out user RWX */
/*	if (getcontext() == 0)
		bits |= PPROT_S;
*/
	if (bits && (pte & bits) == bits) {
		printf("pagein4m(%s[%d]): OOPS: prot=%x, va=%x, pte=%x, bits=%x\n",
			curproc->p_comm, curproc->p_pid, prot, va, pte, bits);
		return -1;
	}
	return 0;
}
#endif

d1720 1
a1720 1
		cnum = c - ctxinfo;
d1730 1
a1730 1
		c = &ctxinfo[cnum = ctx_kick];
d1739 1
a1739 1
		doflush = (vactype != VAC_NONE);
d1773 1
a1773 1
		setcontext(cnum);
d1789 1
a1789 2
#ifdef MMU_3L
			if (mmu_3l) {
d1792 1
a1792 3
			} else
#endif
			{
d1804 1
d1826 2
a1827 19
		/*
		 * We install kernel mappings into the pmap here, since when
		 * the kernel expands it only propagates the expansion to pmaps
		 * corresponding to valid contexts. Thus it is possible (and
		 * it has happened!) that a pmap is created just before
		 * the kernel expands, but the pmap gets a context *after*
		 * the kernel expands, thus missing getting mappings.
		 */
		qcopy(&pmap_kernel()->pm_reg_ptps[VA_VREG(KERNBASE)],
		      &pm->pm_reg_ptps[VA_VREG(KERNBASE)],
		      NKREG * sizeof(int));
		/*
		 * We must also install the regmap/segmap/etc stuff for
		 * kernel maps.
		 */
		qcopy(&pmap_kernel()->pm_regmap[VA_VREG(KERNBASE)],
		       &pm->pm_regmap[VA_VREG(KERNBASE)],
		       NKREG * sizeof(struct regmap));

d1829 1
a1829 1
#ifdef DEBUG
d1834 2
a1835 2
		ctx_phys_tbl[cnum] =
			(pm->pm_reg_ptps_pa >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;
d1837 1
a1837 1
		setcontext(cnum);
a1839 1
#if defined(SUN4M)
d1861 1
a1861 1
	if (vactype != VAC_NONE) {
d1872 3
a1874 1
		if (CPU_ISSUN4M)
d1876 1
d1883 1
d1888 1
a1888 1
		ctx_phys_tbl[newc] = ctx_phys_tbl[0];
d1893 1
d1927 1
a1927 2
	register vm_offset_t va;
	register int vr, vs, flags;
d1939 1
a1939 1
	ctx = getcontext();
d1943 4
a1946 1
if(pm==NULL)panic("pv_changepte 1");
d1960 1
a1960 1
				panic("pv_changepte 2");
d1980 1
a1980 1
				setcontext(pm->pm_ctxnum);
d1983 6
a1988 2
				if (vactype != VAC_NONE && (tpte & PG_M))
					cache_flush_page(va);
d1991 2
a1992 3
				setcontext(0);
#ifdef MMU_3L
				if (mmu_3l)
a1993 1
#endif
d1999 1
a1999 1
				flags |= (tpte >> PG_M_SHIFT) & (PV_MOD|PV_REF);
d2007 1
a2007 1
	setcontext(ctx);
d2036 1
a2036 1
	ctx = getcontext();
d2052 1
a2052 1
			setcontext(pm->pm_ctxnum);
d2055 1
a2055 1
			if (vactype != VAC_NONE && (tpte & PG_M))
d2059 2
a2060 3
			setcontext(0);
#ifdef MMU_3L
			if (mmu_3l)
a2061 1
#endif
d2067 1
a2067 2
			flags |= (tpte >> PG_M_SHIFT) &
			    (PV_MOD|PV_REF);
d2073 1
a2073 1
	setcontext(ctx);
a2097 2
	if (pv == NULL)
		panic("pv_unlink: pv==NULL");
d2099 1
a2099 1
		panic("pv_unlink: pv->pv_pmap==NULL");
d2111 1
a2111 1
			free(npv, M_VMPVENT);
d2114 1
d2123 1
a2123 1
				panic("pv_unlink: npv==NULL");
d2128 1
a2128 1
		free(npv, M_VMPVENT);
d2180 6
a2185 4
				if (pmapdebug) printf(
				"pv_link: badalias: pid %d, %lx<=>%x, pa %lx\n",
				curproc?curproc->p_pid:-1, va, npv->pv_va,
				vm_first_phys + (pv-pv_table)*NBPG);
d2193 1
a2193 1
	npv = (struct pvlist *)malloc(sizeof *npv, M_VMPVENT, M_WAITOK);
d2224 1
a2224 2
	register vm_offset_t va;
	register int vr, flags;
d2227 1
d2236 1
a2236 1
	ctx = getcontext();
d2241 1
d2243 2
a2244 1
			panic("pv_changepte 1");
d2251 3
a2253 1
		if (CTX_USABLE(pm,rp)) {
d2266 19
a2284 8
			setcontext(pm->pm_ctxnum);
			/* %%%: Do we need to always flush? */
			tpte = getpte4m(va);
			if (vactype != VAC_NONE && (tpte & SRMMU_PG_M))
				cache_flush_page(va);
		} else {
			/* PTE that we want has no context. Use sw getpte*/
			tpte = getptesw4m(pm, va);
a2285 2
		if ((tpte & SRMMU_TETYPE) == SRMMU_TEPTE) /* i.e. if valid pte */
			flags |= (tpte >> PG_M_SHIFT4M) & (PV_MOD4M|PV_REF4M|PV_C4M);
d2287 1
d2290 1
a2290 4
		if (CTX_USABLE(pm,rp))
			setpte4m(va, tpte);
		else
			setptesw4m(pm, va, tpte);
d2293 1
a2293 1
	setcontext(ctx);
d2321 1
a2321 1
	ctx = getcontext();
d2340 2
a2341 2
		if (CTX_USABLE(pm,rp)) {
			setcontext(pm->pm_ctxnum);
d2344 1
a2344 1
		tpte = getptesw4m(pm, va);
d2348 1
d2351 7
d2359 1
a2359 5
			/* TLB has been invalidated, so just update memory */
			setptesw4m(pm, va, tpte);
			if (vactype != VAC_NONE &&
			    CTX_USABLE(pm,rp) && (tpte & SRMMU_PG_M))
				cache_flush_page(va); /* XXX: do we need this?*/
d2363 1
a2363 1
	setcontext(ctx);
a2376 2
	if (pv == NULL)
		panic("pv_unlink: pv==NULL");
d2378 1
a2378 1
		panic("pv_unlink: pv->pv_pmap==NULL");
d2390 1
a2390 1
			free(npv, M_VMPVENT);
d2393 1
d2402 1
a2402 1
				panic("pv_unlink: npv==NULL");
d2407 1
a2407 1
		free(npv, M_VMPVENT);
d2459 6
a2464 4
				if (pmapdebug & PDB_CACHESTUFF) printf(
				"pv_link: badalias: pid %d, %lx<=>%x, pa %lx\n",
				curproc?curproc->p_pid:-1, va, npv->pv_va,
				vm_first_phys + (pv-pv_table)*NBPG);
d2473 1
a2473 1
	npv = (struct pvlist *)malloc(sizeof *npv, M_VMPVENT, M_WAITOK);
d2551 1
d2553 1
d2576 1
a2576 1
#ifdef MMU_3L
a2589 1
	char *theend = end;
d2597 1
a2597 1
		if (cpumod != SUN4_400) {
d2642 2
a2643 2
#ifdef MMU_3L
	if (mmu_3l)
d2653 1
a2653 1
#ifdef MMU_3L
d2660 1
a2660 1
#ifdef MMU_3L
d2678 1
a2678 1
#ifdef MMU_3L
d2689 1
a2689 1
		theend = p = esym;
d2691 1
a2691 1
#ifdef MMU_3L
d2694 1
d2698 3
a2700 1
	pmap_kernel()->pm_ctx = ctxinfo = ci = (union ctxinfo *)p;
a2701 5
#ifdef DDB
	bzero(theend, p - theend);
#else
	bzero(end, p - end);
#endif
d2704 1
a2704 1
#ifdef MMU_3L
d2814 2
a2815 2
#ifdef MMU_3L
			if (mmu_3l) {
d2833 2
a2834 2
#ifdef MMU_3L
		if (!mmu_3l)
d2842 1
d2865 2
a2866 2
#ifdef MMU_3L
		if (mmu_3l) {
d2878 2
a2879 2
#ifdef MMU_3L
	if (mmu_3l)
d2889 1
d2894 2
a2895 3
		setcontext(i);
#ifdef MMU_3L
		if (mmu_3l)
a2898 1
#endif
d2908 1
a2908 1
	setcontext(0);
d2938 1
a2938 1
	caddr_t p, p2;
a2941 2
	struct   regmap *rmapp = NULL;
	struct	 segmap *smapp = NULL;
d2943 2
a2944 5
#if 0
	int nkreg, nkseg, nkpag, kernsize, newpgs;
#endif
	caddr_t kphyssegtbl, kphyspagtbl = NULL;
	int deadfill, deadspace;
d2947 1
a2949 1
	char *theend = end;
a2950 1
	extern caddr_t reserve_dumppages(caddr_t);
d2972 1
a2972 1
	simple_lock_init(kernel_pmap_store.pm_lock);
d2974 3
a2976 1
	/* Set up pm_regmap for kernel to point NUREG *below* the beginning
d2997 1
a2997 1
		theend = p = esym;
a2999 6
	/*
	 * Preserve the monitor ROM's reserved VM region, so that
	 * we can use L1-A or the monitor's debugger.
	 */

	mmu_reservemon4m(&kernel_pmap_store, &p);
d3001 2
a3002 1
	pmap_kernel()->pm_ctx = ctxinfo = ci = (union ctxinfo *)p;
d3004 2
a3005 1
	bzero((caddr_t)ctxinfo, (u_int)p - (u_int)ctxinfo);
d3010 27
d3039 9
a3047 1
	 * reserve memory for I/O pagetables. This takes 64k of memory
d3049 3
a3051 6
	 * on the definition of DVMA4M_BASE...we may drop it back to 32M)
	 * but since the table must be aligned, we might end up using
	 * as much as 128K. (note 1024 = NBPG / sizeof(iopte_t))
	 *
	 * We optimize with some space saving song and dance to
	 * squeeze other pagetables in the dead space.
d3055 1
a3055 1
	    panic("pmap_bootstrap4m: invalid DVMA4M_BASE of 0x%x",DVMA4M_BASE);
d3058 2
a3059 37
	deadfill = 0;
	p = (caddr_t) roundup((u_int) p, max(SRMMU_L1SIZE * sizeof(long),
				   max(SRMMU_L2SIZE * sizeof(long),
				       SRMMU_L3SIZE * sizeof(long))));

	deadspace = (int) (((caddr_t)roundup((u_int)p,
					     (0 - DVMA4M_BASE) / 1024)) - p);

	if (deadspace >= SRMMU_L3SIZE * sizeof(long) * NKREG * NSEGRG) {
		p = (caddr_t) roundup((u_int)p, SRMMU_L3SIZE * sizeof(long));
		kernel_pagtable_store = (u_int *)p;
		p += ((SRMMU_L3SIZE * sizeof(long)) * NKREG) * NSEGRG;
		bzero(kernel_pagtable_store,
		      p - (caddr_t) kernel_pagtable_store);
		deadfill |= 4;
		deadspace -= (int)(p - (caddr_t) kernel_pagtable_store);
	}
	if (deadspace >= SRMMU_L2SIZE * sizeof(long) * NKREG) {
		p = (caddr_t) roundup((u_int)p, SRMMU_L2SIZE * sizeof(long));
		kernel_segtable_store = (u_int *)p;
		p += (SRMMU_L2SIZE * sizeof(long)) * NKREG;
		bzero(kernel_segtable_store,
		      p - (caddr_t) kernel_segtable_store);
		deadfill |= 2;
		deadspace -= (int)(p - (caddr_t) kernel_segtable_store);
	}
	if (deadspace >= SRMMU_L1SIZE * sizeof(long)) {
		p = (caddr_t) roundup((u_int)p, SRMMU_L1SIZE * sizeof(long));
		kernel_regtable_store = (u_int *)p;
		p += SRMMU_L1SIZE * sizeof(long);
		bzero(kernel_regtable_store,
		      p - (caddr_t) kernel_regtable_store);
		deadfill |= 1;
		deadspace -= (int)(p - (caddr_t) kernel_regtable_store);
	}
	if (deadspace < 0)
	    printf("pmap_bootstrap4m: botch in memory-saver\n");
a3060 1
	p = (caddr_t) roundup((u_int)p, (0 - DVMA4M_BASE) / 1024);
d3066 10
d3077 1
a3077 1
	 * reserve memory for segment and page tables needed to map the entire
d3086 28
d3115 1
a3115 1
	 * We only do this if it wasn't done above...
d3117 52
a3168 20
	if (!(deadfill & 2)) {
		p = (caddr_t) roundup((u_int)p, SRMMU_L2SIZE * sizeof(long));
		kernel_segtable_store = (u_int *)p;
		p += (SRMMU_L2SIZE * sizeof(long)) * NKREG;
		bzero(kernel_segtable_store,
		      p - (caddr_t) kernel_segtable_store);
	}
	if (!(deadfill & 4)) {
		p = (caddr_t) roundup((u_int)p, SRMMU_L3SIZE * sizeof(long));
		kernel_pagtable_store = (u_int *)p;
		p += ((SRMMU_L3SIZE * sizeof(long)) * NKREG) * NSEGRG;
		bzero(kernel_pagtable_store,
		      p - (caddr_t) kernel_pagtable_store);
	}
	if (!(deadfill & 1)) {
		p = (caddr_t) roundup((u_int)p, SRMMU_L1SIZE * sizeof(long));
		kernel_regtable_store = (u_int *)p;
		p += SRMMU_L1SIZE * sizeof(long);
		bzero(kernel_regtable_store,
		      p - (caddr_t) kernel_regtable_store);
d3171 3
a3173 14
#if 0
	/* At this point, "p" is the highest location of kernel memory that
	 * we need to lock in/map initially. We now need to calculate the
	 * size of this kernel image and allocate memory for its page tables,
	 * which we won't set up until the end of this function. After this
	 * point, it is NOT POSSIBLE to allocate kernel physical memory
	 * by bumping up p!!!
	 */
	p = (caddr_t) roundup((u_int)p,SRMMU_L1SIZE * sizeof(long));
	kernsize = (u_int)p - KERNBASE;

	/* We keep adding pages until the number of added pages is sufficient
	 * to hold the map for both the kernel and the new pages. This
	 * converges since it takes at most about 1152 bytes to map one page.
d3175 1
a3175 14
	newpgs = 0;
	do {
		newpgs++;
		nkreg = (kernsize + (newpgs * NBPG) + NBPRG - 1) / NBPRG;
		nkseg = (kernsize + (newpgs * NBPG) + NBPSG - 1) / NBPSG;
                nkpag = (kernsize + NBPG - 1) / NBPG + newpgs;
	} while (((SRMMU_L1SIZE + (nkreg * SRMMU_L2SIZE) +
		   (nkseg * SRMMU_L3SIZE) + (nkpag)) * sizeof(long)) >
		 newpgs * NBPG);

	kernsize += newpgs * NBPG;
	p2 = p;			/* Page tables go from p2 to p. */
	p += newpgs * NBPG;	/* p is now the _real_ top of kernel mem. */
#endif
d3178 3
a3180 7
	 * Set up the `constants' for the call to vm_init()
	 * in main().  All pages beginning at p (rounded up to
	 * the next whole page) and continuing through the number
	 * of available pages are free, but they start at a higher
	 * virtual address.  This gives us two mappable MD pages
	 * for pmap_zero_page and pmap_copy_page, and one MI page
	 * for /dev/mem, all with no associated physical memory.
d3182 1
a3182 17
	p = (caddr_t)(((u_int)p + NBPG - 1) & ~PGOFSET);
	avail_start = (int)p - KERNBASE;
	/*
	 * Grab physical memory list, so pmap_next_page() can do its bit.
	 */
	npmemarr = makememarr(pmemarr, MA_SIZE, MEMARR_AVAILPHYS);
	sortm(pmemarr, npmemarr);
	if (pmemarr[0].addr != 0) {
		printf("pmap_bootstrap: no kernel memory?!\n");
		callrom();
	}
	avail_end = pmemarr[npmemarr-1].addr + pmemarr[npmemarr-1].len;
	avail_next = avail_start;
	for (physmem = 0, mp = pmemarr, j = npmemarr; --j >= 0; mp++)
		physmem += btoc(mp->len);

	i = (int)p;
d3198 1
a3198 1
	p = (caddr_t)i;			/* retract to first free phys */
d3200 2
a3201 1
	/* Set up the ctxinfo structures (freelist of contexts)
d3213 2
a3214 1
	/* Now map the kernel into our new set of page tables, then
d3221 1
a3221 1
	    panic("pmap_bootstrap4m: p misaligned?!?");
d3223 1
a3223 1
	    panic("pmap_bootstrap4m: KERNBASE not region-aligned");
d3225 1
a3225 13
	if (pmap_kernel()->pm_reg_ptps == NULL) {
#ifdef DEBUG
		printf("pmap_bootstrap4m: no kernel regiontable; creating.\n");
#endif
		pmap_kernel()->pm_reg_ptps = (int *) kernel_regtable_store;
		pmap_kernel()->pm_reg_ptps_pa =
			VA2PA((caddr_t)pmap_kernel()->pm_reg_ptps);
		p2 = (caddr_t) pmap_kernel()->pm_reg_ptps +
		    (SRMMU_L1SIZE * sizeof(int));
		bzero(pmap_kernel()->pm_reg_ptps, SRMMU_L1SIZE * sizeof(long));
	}
	reg = -1;
	seg = -1;
d3227 7
a3233 45
		if ((VA_VSEG(q) % NSEGRG == 0) && VA_VREG(q) != reg) {
			/* entering new region; install & build segtbl
			 * XXX: WE TRASH ANY EXISTING MAPPINGS IN THE KERNEL
			 *      REGION. SHOULD BE FIXED!
			 */
			reg = VA_VREG(q);
			rmapp = &(pmap_kernel()->pm_regmap[reg]);
			kphyssegtbl = (caddr_t)
			    &kernel_segtable_store[(reg - VA_VREG(KERNBASE)) *
						   SRMMU_L2SIZE];
			bzero(kphyssegtbl, SRMMU_L2SIZE * sizeof(long));

			((int *)pmap_kernel()->pm_reg_ptps)[VA_VREG(q)] =
			    (VA2PA(kphyssegtbl) >> SRMMU_PPNPASHIFT) |
				SRMMU_TEPTD;

			rmapp->rg_seg_ptps = (int *)kphyssegtbl;

			if (rmapp->rg_segmap == NULL)
			    rmapp->rg_segmap = &kernel_segmap_store[(reg -
			        VA_VREG(KERNBASE)) * NSEGRG];
		}
		if (((VA_VPG(q) % NPTESG) == 0) && VA_VSEG(q) != seg) {
			rmapp->rg_nsegmap++;
			/* Entering new segment. XXX: trashes existing maps */
			seg = VA_VSEG(q);
			smapp = &(rmapp->rg_segmap[seg]);
			kphyspagtbl = (caddr_t)
			    &kernel_pagtable_store[(((reg-VA_VREG(KERNBASE))*
						     NSEGRG) + seg)*SRMMU_L3SIZE];
			bzero(kphyspagtbl, SRMMU_L3SIZE * sizeof(long));

			rmapp->rg_seg_ptps[VA_VSEG(q)] =
			    (VA2PA(kphyspagtbl) >> SRMMU_PPNPASHIFT) |
				SRMMU_TEPTD;
			smapp->sg_pte = (int *) kphyspagtbl;
		}
		/* Now install entry for current page. Cache and write-protect
		 * kernel text.
		 */
		smapp->sg_npte++;
		if (q == (caddr_t) KERNBASE) {
			/* Must map in message buffer in low page. */
			((int *)kphyspagtbl)[VA_VPG(q)] = PPROT_N_RWX |
			    SRMMU_PG_C | SRMMU_TEPTE;
a3234 15
		}
		else if (q < (caddr_t) (KERNBASE+0x4000)) {
			/* Special case: retain interrupt register mapping */
			((int *)kphyspagtbl)[VA_VPG(q)] =
			    getpte4m((vm_offset_t)q);
		}
		if (q >= (caddr_t) trapbase && q < etext)
			((int *)kphyspagtbl)[VA_VPG(q)] =
				(VA2PA(q) >> SRMMU_PPNPASHIFT) |
				PPROT_N_RX | SRMMU_PG_C | SRMMU_TEPTE;
		else
			((int *)kphyspagtbl)[VA_VPG(q)] =
				(VA2PA(q) >> SRMMU_PPNPASHIFT) |
				PPROT_N_RWX | SRMMU_PG_C | SRMMU_TEPTE;
	}
d3236 6
a3241 15
	/*
	 * We must also create a segment table for region 0xfe and any
	 * needed page tables necessary to map the special (important)
	 * devices that get mapped at the beginning of this I/O segment.
	 * This is necessary since we have to map the interrupt registers
	 * at the start of region 0xfe in bootstrap().
	 *
	 * Since we've statically allocated space to map the entire kernel,
	 * we might as well pre-wire the mappings to save time in pmap_enter.
	 * This also gets around nasty problems with caching of L1/L2 ptp's.
	 *
	 * XXX WHY DO WE HAVE THIS CACHING PROBLEM WITH L1/L2 PTPS????? %%%
	 */
	for (reg = VA_VREG(KERNBASE); reg < NKREG+VA_VREG(KERNBASE); reg++) {
		rmapp = &(pmap_kernel()->pm_regmap[reg]);
d3243 5
a3247 18
		if (((int *)pmap_kernel()->pm_reg_ptps)[reg] == NULL) {
			kphyssegtbl = (caddr_t)
			    &kernel_segtable_store[(reg - VA_VREG(KERNBASE)) *
						   SRMMU_L2SIZE];
			bzero(kphyssegtbl, SRMMU_L2SIZE * sizeof(long));
			((int *)pmap_kernel()->pm_reg_ptps)[reg] =
			    (VA2PA(kphyssegtbl) >> SRMMU_PPNPASHIFT) |
				SRMMU_TEPTD;

			rmapp->rg_seg_ptps = (int *)kphyssegtbl;

			if (rmapp->rg_segmap == NULL)
				rmapp->rg_segmap = &kernel_segmap_store
					[(reg - VA_VREG(KERNBASE)) * NSEGRG];
		}
		for (seg = 0; seg < NSEGRG; seg++) {
			if (rmapp->rg_seg_ptps[seg] == NULL) {
				rmapp->rg_nsegmap++;
d3249 1
a3249 15
				smapp = &(rmapp->rg_segmap[seg]);
				kphyspagtbl = (caddr_t)
				    &kernel_pagtable_store[(((reg-
							    VA_VREG(KERNBASE))*
							     NSEGRG) + seg)*
							   SRMMU_L3SIZE];
				bzero(kphyspagtbl,
				      SRMMU_L3SIZE * sizeof(long));

				rmapp->rg_seg_ptps[seg] =
				    (VA2PA(kphyspagtbl) >> SRMMU_PPNPASHIFT) |
					SRMMU_TEPTD;
				smapp->sg_pte = (int *) kphyspagtbl;
			}
		}
d3252 1
d3255 1
a3255 1
	 * copying the context 0 L1 PTP from ctx_phys_tbl[0] into the
d3263 2
a3264 1
	    ctx_phys_tbl[i] = ctx_phys_tbl[0];
d3269 14
d3286 1
a3286 1
	setcontext(0);		/* paranoia? %%%: Make 0x3 a define! below */
d3289 2
a3290 22

	i = lda(SRMMU_PCR, ASI_SRMMU);
	switch(mmumod) {
/*	case SUN4M_MMU_MS: */  /* These have the same model # as SS */
	case SUN4M_MMU_SS:
		if ((cpumod & 0xf0) == (SUN4M_SS & 0xf0))
			sta(SRMMU_PCR, ASI_SRMMU, (i | SRMMU_PCR_TC));
		else	/* microsparc */
			printf("(if this doesn't work, "
			       "fix pmap_bootstrap4m in pmap.c)");
		break;
	case SUN4M_MMU_HS:
		printf("(if this doesn't work, fix pmap_bootstrap4m in pmap.c)");
		sta(SRMMU_PCR, ASI_SRMMU, (i | SRMMU_PCR_C) & ~SRMMU_PCR_CE);
		/* above: CHECKFIX %%% below: add microsparc*/
		break;
	case SUN4M_MMU_MS1:
		printf("(if this doesn't work, fix pmap_bootstrap4m in pmap.c)");
		break;
	default:
		panic("Unimplemented MMU architecture %d",mmumod);
	}
d3295 1
a3295 1
	    (VA2PA((caddr_t)ctx_phys_tbl) >> SRMMU_PPNPASHIFT) & ~0x3);
d3302 32
d3336 2
a3337 3
	 * On SuperSPARC machines without a MXCC, we *cannot* cache the
	 * page tables.
	 * XXX: this needs to be cleaned up with the cpu_softc stuff...
d3339 12
a3350 2
	if (mmumod == SUN4M_MMU_SS &&
	    (lda(SRMMU_PCR, ASI_SRMMU) & SRMMU_PCR_MB)) {
d3352 2
a3353 2
		int bytes, numpages;
		cant_cache_pagetables = 1;
d3355 2
a3356 3
#define DO_THE_MATH(math)						\
	bytes = (math);							\
	numpages = (bytes >> PGSHIFT) + (bytes % NBPG ? 1 : 0);
d3358 3
a3360 22
		DO_THE_MATH(SRMMU_L3SIZE * sizeof(long) * NKREG * NSEGRG);
#ifdef DEBUG
		printf("pmap_bootstrap4m: uncaching %d PT pages at 0x%lx\n",
		    numpages, (long)kernel_pagtable_store);
#endif
		kvm_uncache((caddr_t)kernel_pagtable_store, numpages);

		DO_THE_MATH(SRMMU_L2SIZE * sizeof(long) * NKREG);
#ifdef DEBUG
		printf("pmap_bootstrap4m: uncaching %d ST pages at 0x%lx\n",
		    numpages, (long)kernel_segtable_store);
#endif
		kvm_uncache((caddr_t)kernel_segtable_store, numpages);

		DO_THE_MATH(SRMMU_L1SIZE * sizeof(long));
#ifdef DEBUG
		printf("pmap_bootstrap4m: uncaching %d RT pages at 0x%lx\n",
		    numpages, (long)kernel_regtable_store);
#endif
		kvm_uncache((caddr_t)kernel_regtable_store, numpages);

#undef DO_THE_MATH
a3361 3

#ifdef DEBUG
	printf("\n");	/* Might as well make it pretty... */
a3362 2

	/* All done! */
d3366 1
d3412 1
a3412 7
#if defined(DEBUG) && !defined(SUN4M)
			/*
			 * crowded chunks are normal on SS20s; don't clutter
			 * screen with messages
			 */
			printf("note: crowded chunk at 0x%x\n", mp->addr);
#endif
d3415 1
a3415 1
				panic("pmap_init: sva(%lx) < eva(%lx)",
d3513 1
a3513 1
#ifdef MMU_3L
d3515 5
a3519 3
		if (mmu_3l)
		    for (i = NUREG; --i >= 0;)
			pm->pm_regmap[i].rg_smeg = reginval;
d3524 2
d3530 1
a3530 1
		 * user mode!
d3537 2
a3538 1
		if (cant_cache_pagetables)
d3541 1
d3549 2
a3550 1
		qzero(urp, SRMMU_L1SIZE * sizeof(int));
d3552 5
d3607 1
a3607 1
#ifdef MMU_3L
d3698 1
a3698 1
		printf("pmap_remove(%p, %lx, %lx)\n", pm, va, endva);
d3724 2
a3725 1
		(*rm)(pm, va, nva, vr, vs);
d3791 1
a3791 1
	setcontext(0);
d3797 1
a3797 2
		if (vactype != VAC_NONE)
			cache_flush_segment(vr, vs);
d3800 1
a3800 1
		perpage = (vactype != VAC_NONE);
d3830 2
a3831 2
#ifdef MMU_3L
		if (mmu_3l)
d3836 1
a3836 1
				setcontext(i);
d3841 2
a3842 2
#ifdef MMU_3L
			if (mmu_3l) {
d3844 1
a3844 1
					setcontext(i);
d3892 1
a3892 1
	setcontext(0);
d3898 1
a3898 1
		if (vactype != VAC_NONE)
d3902 1
a3902 1
		perpage = (vactype != VAC_NONE);
d3905 1
a3905 1
		tpte = getpte4m(va);
d3907 6
d3919 1
a3919 1
			    cache_flush_page(va);
d3928 2
a3929 1
		setpte4m(va, SRMMU_TEINVALID);
d4028 2
a4029 2
#ifdef MMU_3L
				if (mmu_3l && rp->rg_smeg != reginval) {
d4031 1
a4031 1
						setcontext(pm->pm_ctxnum);
d4034 1
a4034 1
						setcontext(0);
d4050 1
a4050 1
		setcontext(pm->pm_ctxnum);
d4053 1
a4053 2
			if (vactype != VAC_NONE)
				cache_flush_segment(vr, vs);
d4055 1
a4055 1
			perpage = (vactype != VAC_NONE);
d4059 2
a4060 3
		setcontext(0);
#ifdef MMU_3L
		if (mmu_3l)
a4061 1
#endif
a4083 2
#define PMAP_PTESYNC
#ifdef PMAP_PTESYNC
a4084 1
#endif
d4100 1
a4100 2
#ifdef MMU_3L
		else if (mmu_3l && rp->rg_smeg != reginval) {
a4104 1
#endif
d4114 2
a4115 2
#ifdef MMU_3L
			if (mmu_3l && rp->rg_smeg != reginval) {
d4137 1
a4137 1
	register int *pte0, i, tpte, perpage, npg;
d4152 1
d4161 1
a4161 1
	if (CTX_USABLE(pm,rp)) {
d4163 2
a4164 2
		setcontext(pm->pm_ctxnum);
		if (vactype != VAC_NONE) {
d4178 11
a4188 9
		/* Note: we use sw pagetables here since pages have been
		 * flushed already. This avoids over-zealous cache flushing.
		 */
		if (CTX_USABLE(pm,rp)) {    /* %%% XXX: Performance hit? */
			tpte = getpte4m(va); /* should we just flush seg? */
			tlb_flush_page(va);
		} else
			tpte = getptesw4m(pm, va);
		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE)
d4190 2
d4204 3
a4206 1
		setptesw4m(pm, va, SRMMU_TEINVALID);	/* Update pagetables */
d4213 1
a4213 1
	if ((sp->sg_npte = nleft) == 0 /* ??? && pm->pm_ctx != NULL*/) {
d4221 3
a4223 2
		tlb_flush_segment(vr, vs); 	/* Paranoia? */
		rp->rg_seg_ptps[vs] = SRMMU_TEINVALID;
d4228 3
a4233 2
			pm->pm_reg_ptps[vr] = SRMMU_TEINVALID;
			GAP_WIDEN(pm,vr);
d4256 1
a4256 2
	register vm_offset_t va;
	register int vr, vs, pteva, tpte;
d4263 1
a4263 1
		panic("pmap_page_protect: no such address: %lx", pa);
d4266 1
a4266 1
		printf("pmap_page_protect(%lx, %x)\n", pa, prot);
d4272 1
a4272 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) ||
d4293 1
a4293 1
	ctx = getcontext();
d4320 2
a4321 2
#ifdef MMU_3L
					if (mmu_3l && rp->rg_smeg != reginval) {
d4323 1
a4323 1
							setcontext(pm->pm_ctxnum);
d4326 1
a4326 1
							setcontext(0);
d4334 1
d4336 1
a4336 1
			setcontext(pm->pm_ctxnum);
d4338 1
a4338 2
			if (vactype != VAC_NONE)
				cache_flush_page(va);
d4340 1
a4340 1
			setcontext(0);
d4342 1
a4342 2
#ifdef MMU_3L
			if (mmu_3l)
a4343 1
#endif
d4350 2
a4351 1
			panic("pmap_page_protect !PG_V");
a4355 1
#ifdef PMAP_PTESYNC
d4358 7
d4366 8
a4373 5
		} else {
			if (pm == pmap_kernel()) {
#ifdef MMU_3L
				if (!mmu_3l)
#endif
d4375 2
a4376 12
						setcontext(i);
						setsegmap(va, seginval);
					}
				me_free(pm, sp->sg_pmeg);
				if (--rp->rg_nsegmap == 0) {
#ifdef MMU_3L
					if (mmu_3l) {
						for (i = ncontext; --i >= 0;) {
							setcontext(i);
							setregmap(va, reginval);
						}
						region_free(pm, rp->rg_smeg);
d4378 2
d4381 12
a4392 11
				}
			} else {
				if (CTX_USABLE(pm,rp))
					/* `pteva'; we might be using tregion */
					setsegmap(pteva, seginval);
#ifdef MMU_3L
				else if (mmu_3l && rp->rg_smeg != reginval) {
					/* note: context already set earlier */
					setregmap(0, rp->rg_smeg);
					setsegmap(vs << SGSHIFT, seginval);
				}
d4394 3
a4396 3
				free(sp->sg_pte, M_VMPMAP);
				sp->sg_pte = NULL;
				me_free(pm, sp->sg_pmeg);
d4398 8
a4405 7
				if (--rp->rg_nsegmap == 0) {
#ifdef MMU_3L
					if (mmu_3l && rp->rg_smeg != reginval) {
						if (pm->pm_ctx)
							setregmap(va, reginval);
						region_free(pm, rp->rg_smeg);
					}
d4407 3
a4409 4
					free(rp->rg_segmap, M_VMPMAP);
					rp->rg_segmap = NULL;
					GAP_WIDEN(pm,vr);
				}
d4412 1
d4416 1
a4416 1
			free(pv, M_VMPVENT);
d4423 1
a4423 1
	setcontext(ctx);
d4442 1
a4442 2
	register vm_offset_t va, nva;
	register int vr, vs;
d4456 1
a4456 1
	ctx = getcontext();
d4501 1
a4501 1
				setcontext(pm->pm_ctxnum);
d4508 1
a4508 2
						if (vactype != VAC_NONE)
							cache_flush_page(va);
d4519 1
a4519 1
				setcontext(0);
d4521 1
a4521 2
#ifdef MMU_3L
				if (mmu_3l)
a4522 1
#endif
d4532 1
a4532 1
	setcontext(ctx);
d4553 1
a4553 1
		printf("pmap_changeprot(%p, %lx, %x, %x)\n",
d4595 1
a4595 1
		ctx = getcontext();
d4597 6
a4602 2
			/* use current context; flush writeback cache */
			setcontext(pm->pm_ctxnum);
d4605 1
a4605 1
				setcontext(ctx);
d4608 1
a4608 8

			/*
			 * the latter check deals with a writethrough cache
			 * problem on the 4/300
			 */
			if ((vactype==VAC_WRITEBACK ||
			    (vactype==VAC_WRITETHROUGH && cputyp==CPU_SUN4)) &&
			    (tpte & (PG_U|PG_NC|PG_TYPE)) == (PG_U|PG_OBMEM))
d4611 1
a4611 1
			setcontext(0);
d4613 1
a4613 2
#ifdef MMU_3L
			if (mmu_3l)
a4614 1
#endif
d4619 1
a4619 1
				setcontext(ctx);
d4625 1
a4625 1
		setcontext(ctx);
d4654 1
a4654 2
	register vm_offset_t va;
	register int vr, vs, tpte;
d4664 1
a4664 1
		printf("pmap_page_protect(%lx, %x)\n", pa, prot);
d4690 1
a4690 1
	ctx = getcontext();
d4707 3
a4709 5
		if (CTX_USABLE(pm,rp)) { 	/* Must flush */
			setcontext(pm->pm_ctxnum);
			tpte = getpte4m(va);
			if (vactype != VAC_NONE)
				cache_flush_page(va);
d4711 3
a4713 2
		} else
			tpte = getptesw4m(pm, va);
d4717 1
d4720 4
a4723 14
		if (nleft)
			setptesw4m(pm, va, SRMMU_TEINVALID);
		else {
			if (pm == pmap_kernel()) {
				tlb_flush_segment(vr, vs); /* Paranoid? */
				if (va < virtual_avail) {
#ifdef DEBUG
					printf("pmap_rmk4m: attempt to free "
					       "base kernel allocation\n");
#endif
					goto nextpv;
				}
				/* no need to free the table; it is static */
				qzero(sp->sg_pte, SRMMU_L3SIZE * sizeof(int));
d4725 25
a4749 1
				/* if we're done with a region, leave it */
d4751 7
a4757 14
			} else { 	/* User mode mapping */
				if (CTX_USABLE(pm,rp))
					tlb_flush_segment(vr, vs);
				rp->rg_seg_ptps[vs] = SRMMU_TEINVALID;
				free(sp->sg_pte, M_VMPMAP);
				sp->sg_pte = NULL;

				if (--rp->rg_nsegmap == 0) {
					free(rp->rg_segmap, M_VMPMAP);
					rp->rg_segmap = NULL;
					free(rp->rg_seg_ptps, M_VMPMAP);
					pm->pm_reg_ptps[vr] = SRMMU_TEINVALID;
					GAP_WIDEN(pm,vr);
				}
d4760 1
d4764 1
a4764 1
			free(pv, M_VMPVENT);
d4771 1
a4771 1
	setcontext(ctx);
d4790 1
a4790 2
	register vm_offset_t va, nva;
	register int vr, vs;
d4804 1
a4804 1
	ctx = getcontext();
d4834 8
a4841 2
		/* in MMU: take away write bits from MMU PTEs */
		if (CTX_USABLE(pm,rp)) {
d4847 6
a4852 10
			setcontext(pm->pm_ctxnum);
			for (; va < nva; va += NBPG) {
				register int tpte = getpte4m(va);
				pmap_stats.ps_npg_prot_all++;
				if ((tpte & (PPROT_WRITE|SRMMU_PGTYPE)) ==
				    (PPROT_WRITE|PG_SUN4M_OBMEM)) {
					pmap_stats.ps_npg_prot_actual++;
					if (vactype != VAC_NONE)
						cache_flush_page(va);
					setpte4m(va, tpte & ~PPROT_WRITE);
d4854 2
a4855 13
			}
		} else {
			/*
			 * No context, hence not cached;
			 * just update PTEs.
			 */
			setcontext(0);
			for (; va < nva; va += NBPG) {
				register int tpte = getptesw4m(pm, va);
				pmap_stats.ps_npg_prot_all++;
				if ((tpte & (PPROT_WRITE)))
					pmap_stats.ps_npg_prot_actual++;
				setptesw4m(pm, va, tpte & ~PPROT_WRITE);
d4861 1
a4861 1
	setcontext(ctx);
d4880 1
a4880 1
		printf("pmap_changeprot(%p, %lx, %x, %x)\n",
d4895 1
a4895 1
	ctx = getcontext();
d4897 6
a4902 1
		setcontext(pm->pm_ctxnum);
d4904 3
a4906 3
		if (vactype == VAC_WRITEBACK &&
		    (tpte & SRMMU_PGTYPE) == PG_SUN4M_OBMEM)
			cache_flush_page(va); /* XXX: paranoia? */
d4911 3
a4913 2
		setcontext(ctx);
		goto useless;
a4918 3
	setcontext(ctx);
	splx(s);
	return;
d4920 2
a4921 3
useless:
	/* only wiring changed, and we ignore wiring */
	pmap_stats.ps_useless_changeprots++;
d4956 1
a4956 1
		printf("pmap_enter: pm %p, va %lx, pa %lx: in MMU hole\n",
d4964 1
a4964 1
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
d4968 2
a4969 2
	pteproto = PG_V | ((pa & PMAP_TNC) << PG_TNC_SHIFT);
	pa &= ~PMAP_TNC;
d4978 1
a4978 1
			panic("pmap_enter: no such address: %lx", pa);
d4988 1
a4988 1
	ctx = getcontext();
d4993 1
a4993 1
	setcontext(ctx);
d5016 2
a5017 2
#ifdef MMU_3L
	if (mmu_3l && rp->rg_smeg == reginval) {
d5022 1
a5022 1
			setcontext(i);
d5042 1
a5042 1
			pmap_changeprot(pm, va, prot, wired);
d5048 1
a5048 1
printf("pmap_enk: changing existing va=>pa entry: va %lx, pteproto %x\n",
d5060 2
a5061 3
				setcontext(0);	/* ??? */
				if (vactype != VAC_NONE)
					cache_flush_page((int)va);
d5093 2
a5094 2
#ifdef MMU_3L
		if (mmu_3l)
d5101 1
a5101 1
				setcontext(i);
d5153 1
a5153 1
		printf("pmap_enu: gap_start %x, gap_end %x",
d5208 1
a5208 1
				setcontext(pm->pm_ctxnum);
d5210 1
a5210 1
				doflush = 1;
d5212 1
a5212 1
				setcontext(0);
d5214 1
a5214 2
#ifdef MMU_3L
				if (mmu_3l)
a5215 1
#endif
d5241 4
a5244 2
/*printf("%s[%d]: pmap_enu: changing existing va(%x)=>pa entry\n",
curproc->p_comm, curproc->p_pid, va);*/
d5249 1
a5249 2
				if (vactype != VAC_NONE &&
				    doflush && (tpte & PG_NC) == 0)
d5271 1
a5271 1
		/* ptes are in hardare */
d5273 1
a5273 1
			setcontext(pm->pm_ctxnum);
d5275 1
a5275 1
			setcontext(0);
d5277 1
a5277 2
#ifdef MMU_3L
			if (mmu_3l)
a5278 1
#endif
d5322 1
a5322 1
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
d5329 3
a5331 2
	if (pa & PMAP_TYPE4M) {		/* this page goes in an iospace */
		if (cpumod == SUN4M_MS)
a5333 1
		pteproto |= (pa & PMAP_TYPE4M) << PMAP_PTESHFT4M;
d5335 2
d5341 1
a5341 1
	pa &= ~PMAP_TNC;
d5350 1
a5350 1
			panic("pmap_enter: no such address: %lx", pa);
d5361 1
a5361 1
	ctx = getcontext();
d5368 1
a5368 1
	setcontext(ctx);
d5387 1
a5387 1
	    panic("pmap_enk4m: can't enter va 0x%lx below KERNBASE",va);
d5397 1
a5397 1
		panic("pmap_enk4m: missing kernel region table for va %lx",va);
d5399 2
a5400 1
	if (((tpte = getpte4m(va)) & SRMMU_TETYPE) == SRMMU_TEPTE) {
d5408 1
a5408 1
			pmap_changeprot(pm, va, prot, wired);
d5414 2
a5415 2
printf("pmap_enk4m: changing existing va=>pa entry: va %lx, pteproto %x, "
       "oldpte %x\n", va, pteproto, tpte);
d5426 2
a5427 3
				setcontext(0);	/* ??? */
				if (vactype != VAC_NONE)
					cache_flush_page((int)va);
d5443 1
d5446 1
d5448 2
a5449 2
	/* ptes kept in hardware only */
	setpte4m(va, pteproto);
d5464 1
a5464 1
	register int vr, vs, *pte, tpte, s, doflush;
d5468 5
a5478 8
#ifdef DEBUG
	if (pm->pm_gap_end < pm->pm_gap_start) {
		printf("pmap_enu: gap_start %x, gap_end %x",
			pm->pm_gap_start, pm->pm_gap_end);
		panic("pmap_enu: gap botch");
	}
#endif

d5500 2
a5501 2
		register int size;
		register caddr_t tblp;
d5503 1
a5503 1
		tblp = malloc(size, M_VMPMAP, M_WAITOK);
d5508 1
a5508 1
			free(tblp,M_VMPMAP);
d5511 4
a5514 2
		if (cant_cache_pagetables)
			kvm_uncache(tblp, (size+NBPG-1)/NBPG);
d5516 5
a5520 4
		rp->rg_seg_ptps = (int *)tblp;
		qzero(tblp, size);
		pm->pm_reg_ptps[vr] =
			(VA2PA(tblp) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;
d5528 1
a5528 1
		register int size = SRMMU_L3SIZE * sizeof(*pte);
d5536 2
a5537 1
		if (cant_cache_pagetables)
d5539 1
a5540 1
		qzero((caddr_t)pte, size);
d5544 4
a5547 2
		rp->rg_seg_ptps[vs] =
			(VA2PA((caddr_t)pte) >> SRMMU_PPNPASHIFT) | SRMMU_TEPTD;
d5549 6
a5554 2
		/* might be a change: fetch old pte */
		doflush = 0;
a5555 7
		if (CTX_USABLE(pm,rp)) {
			setcontext(pm->pm_ctxnum);
			tpte = getpte4m(va);
			doflush = 1;
		} else {
			tpte = getptesw4m(pm, va);
		}
d5578 3
a5580 3
if (pmapdebug & PDB_ENTER)
printf("%s[%d]: pmap_enu: changing existing va(%x)=>pa(pte=%x) entry\n",
curproc->p_comm, curproc->p_pid, (int)va, (int)pte);
d5587 1
a5587 2
				if (vactype != VAC_NONE &&
				    doflush && (tpte & SRMMU_PG_C))
d5605 1
a5605 1
	 * Update hardware & software PTEs.
d5607 5
a5611 6
	if (CTX_USABLE(pm,rp)) {
		setcontext(pm->pm_ctxnum);
		setpte4m(va, pteproto);
	} else
		setptesw4m(pm, va, pteproto);
	/* XXX: restore previous context here? */
d5649 4
a5652 1
		printf("pmap_extract: null pmap\n");
d5659 4
a5662 1
		printf("pmap_extract: invalid segment (%d)\n", vr);
d5668 1
a5668 1
		register int ctx = getcontext();
d5675 1
a5675 2
#ifdef MMU_3L
			if (mmu_3l)
a5676 1
#endif
d5680 1
a5680 1
		setcontext(ctx);
d5685 4
a5688 1
			printf("pmap_extract: invalid segment\n");
d5694 4
a5697 1
		printf("pmap_extract: invalid pte\n");
d5717 3
a5719 1
	register int tpte, ctx;
d5722 21
a5742 1
		printf("pmap_extract: null pmap\n");
d5745 1
d5747 1
a5747 4
	if (pm->pm_ctx) {
		ctx = getcontext();
		CHANGE_CONTEXTS(ctx, pm->pm_ctxnum);
		tpte = getpte4m(va);
d5749 1
a5749 1
		if ((tpte & SRMMU_TETYPE) != SRMMU_TEPTE) {
d5751 1
a5751 3
			       tpte & SRMMU_TETYPE);
			return (0);
		}
d5753 2
a5754 4
		setcontext(ctx);
	} else
		tpte = getptesw4m(pm, va);

d5756 1
a5756 1
	return (ptoa((tpte & SRMMU_PPNMASK) >> SRMMU_PPNSHIFT) | VA_OFF(va));
d5835 1
a5835 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5851 1
a5851 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5868 1
a5868 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5884 1
a5884 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5913 1
a5913 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5929 1
a5929 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5946 1
a5946 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d5962 1
a5962 1
	if ((pa & (PMAP_TNC & ~PMAP_NC)) == 0 && managed(pa)) {
d6006 1
a6006 1
	if (((pa & (PMAP_TNC & ~PMAP_NC)) == 0) && managed(pa)) {
d6012 1
a6012 2
		if (vactype != VAC_NONE)
			pv_flushcache(pvhead(pa));
d6039 1
a6039 1
		if (vactype == VAC_WRITEBACK)
d6046 1
a6046 1
		if (vactype != VAC_NONE)
d6056 1
a6056 2
	if (vactype != VAC_NONE)
		cache_flush_page((int)sva);
d6075 1
d6077 1
a6077 1
	if (((pa & (PMAP_TNC & ~PMAP_NC)) == 0) && managed(pa)) {
d6083 1
a6083 1
		if (vactype != VAC_NONE)
d6086 10
a6095 8
	/*
	 * there really needs to be a conditional ecache flush here, but
	 * making the page cacheable avoids problems with potentially
	 * leaving dirty cache lines pending in the e-cache while the page
	 * is marked non-cachable.  Per Chris Torek and Aaron Brown...
	 */
	pte = SRMMU_PG_C | (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
	      (atop(pa) << SRMMU_PPNSHIFT));
d6100 1
d6118 1
d6121 1
a6121 1
		if (vactype == VAC_WRITEBACK)
d6129 1
a6129 1
		if (vactype != VAC_NONE)
d6132 6
a6137 8
	/*
	 * there really needs to be a conditional ecache flush here, but
	 * making the page cacheable avoids problems with potentially
	 * leaving dirty cache lines pending in the e-cache while the page
	 * is marked non-cachable.  Per Chris Torek and Aaron Brown...
	 */
	dpte = SRMMU_PG_C | (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
		(atop(dst) << SRMMU_PPNSHIFT));
d6139 3
d6147 1
a6147 2
	if (vactype != VAC_NONE)
		cache_flush_page((int)sva);
d6150 1
d6179 1
d6188 1
a6188 1
			if (vactype != VAC_NONE && (pte & PG_TYPE) == PG_OBMEM)
d6200 1
a6200 1
			if (vactype != VAC_NONE && (pte & PG_TYPE) == PG_OBMEM)
d6323 1
a6323 1
	    panic("%s: CHK(pmap %p): no region mapping", s, pm);
d6329 3
a6331 2
	    panic("%s: CHK(pmap %p): no SRMMU region table or bad pa: tblva=%p, tblpa=0x%x",
		  s, pm, pm->pm_reg_ptps, pm->pm_reg_ptps_pa);
d6334 1
a6334 1
	    (ctx_phys_tbl[pm->pm_ctxnum] != ((VA2PA((caddr_t)pm->pm_reg_ptps)
d6337 1
a6337 1
	    panic("%s: CHK(pmap %p): SRMMU region table at %x not installed "
d6416 1
a6416 1
	    panic("%s: CHK(pmap %p): no SRMMU region table or bad pa: tblva=%p, tblpa=%x",
d6420 1
a6420 1
	    (ctx_phys_tbl[0] != ((VA2PA((caddr_t)pm->pm_reg_ptps) >>
d6422 1
a6422 1
	    panic("%s: CHK(pmap %p): SRMMU region table at %x not installed "
d6441 7
a6447 4
		n = 0;
		for (vs = 0; vs < NSEGRG; vs++) {
			if (rp->rg_segmap[vs].sg_npte)
				n++;
d6478 3
a6480 4
 * Write the dump header and memory mapping information at the front
 * of the dumpfile to make life easy for savecore and libkvm.  Also
 * dump the MMU contents for Sun 4/4c systems since they don't have
 * a separate incore copy of the kernel memory mappings.
d6490 1
a6490 1
	register int error = 0;
d6492 1
a6492 1
	int buffer[dbtob(1) / sizeof(int)];
d6511 1
a6511 1
		sz -= sizeof *sp;					\
d6542 1
a6542 1
		EXPEDITE(&dummy, sizeof dummy);
d6568 1
a6568 1
		register vm_offset_t va = 0;
d6601 1
a6601 1
	       ctx_phys_tbl, ctx_phys_tbl[0]);
d6650 1
a6650 1
	*pte = curpte = ctx_phys_tbl[ctx];
d6755 1
a6755 1
		printf("0x%x -> 0x%x%x (pte %x)\n", i, pte >> 28,
@


1.15
log
@Obviously, those assignments weren't wanted at all...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 1997/06/25 14:30:23 downsj Exp $	*/
d6209 7
a6215 1
	pte = ~SRMMU_PG_C & (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
d6251 7
a6257 1
	dpte = ~SRMMU_PG_C & (SRMMU_TEPTE | PPROT_S | PPROT_WRITE |
@


1.14
log
@== is not an assignment.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2210 1
			pv->pv_va = NULL;
a2473 1
			pv->pv_va = NULL;
@


1.13
log
@A fix for the pv_unlink0 problem, based on the patch included in Erik Fair's
netbsd PR.  Also change the panic text to something more descriptive so that
pv_unlink0 is no more and make use of typedef vm_offset_type is consistently
used for "va" and friends thoughout pmap.c instead of mixed int, u_int and
vm_offset_type...
@
text
@d1 1
d2211 1
a2211 1
			pv->pv_va == NULL;
d2475 1
a2475 1
			pv->pv_va == NULL;
@


1.12
log
@import some cleanup for libkvm/kvm.c from netbsd, udpate libkvm so that sparc can use libkvm vs. libkvm.old and update sparc pmap.c and machdep.c to make dumps compatible with the new libkvm's notions.
@
text
@d179 1
a179 1
	int	pv_va;			/* virtual address */
d874 1
a874 1
	register u_int va = 0, eva = 0;
d1652 1
a1652 1
	register int va;
d1732 2
a1733 1
	register int va, prot;
d2022 2
a2023 1
	register int va, vr, vs, flags;
d2192 2
d2195 1
a2195 1
		panic("pv_unlink0");
d2208 1
a2208 1
		} else
d2210 3
d2219 1
a2219 1
				panic("pv_unlink");
d2318 2
a2319 1
	register int va, vr, flags;
d2456 2
d2459 1
a2459 1
		panic("pv_unlink0");
d2472 1
a2472 1
		} else
d2474 3
d2483 1
a2483 1
				panic("pv_unlink");
d4394 2
a4395 1
	register int va, vr, vs, pteva, tpte;
d4579 2
a4580 1
	register int va, nva, vr, vs;
d4800 2
a4801 1
	register int va, vr, vs, tpte;
d4929 2
a4930 1
	register int va, nva, vr, vs;
d6671 1
a6671 1
		register int va = 0;
@


1.11
log
@reverse
@
text
@d65 3
d79 1
d6551 4
a6554 2
	if (CPU_ISSUN4M) /* No need to dump on 4m; its in-core. */
		return (0);
d6557 3
a6559 5
		return btoc(((seginval + 1) * NPTESG * sizeof(int)) +
			    sizeof(seginval) +
			    sizeof(pmemarr) +
			    sizeof(kernel_segmap_store));
	return 0;
d6563 4
a6566 3
 * Write the mmu contents to the dump device.
 * This gets appended to the end of a crash dump since
 * there is no in-core copy of kernel memory mappings on a 4/4c machine.
d6573 3
a6575 4
#if defined(SUN4C) || defined(SUN4)
	register int pmeg;
	register int i;
	register int *pte, *ptend;
d6577 1
a6577 1
	register int *kp;
d6579 3
d6584 54
a6637 2
	if (CPU_ISSUN4M) /* No need to dump on 4m; its in-core. */
		return (0);
a6649 2
	setcontext(0);

a6652 2
	pte = buffer;
	ptend = &buffer[sizeof(buffer) / sizeof(buffer[0])];
d6659 2
a6660 16
			*pte++ = getpte4(va);
			if (pte >= ptend) {
				/*
				 * Note that we'll dump the last block
				 * the last time through the loops because
				 * all the PMEGs occupy 32KB which is
				 * a multiple of the block size.
				 */
				error = (*dump)(dumpdev, blkno,
						(caddr_t)buffer,
						dbtob(1));
				if (error != 0)
					return (error);
				++blkno;
				pte = buffer;
			}
d6665 1
d6667 2
a6668 23
	/*
	 * Next, dump # of pmegs, the physical memory table and the
	 * kernel's segment map.
	 */
	pte = buffer;
	*pte++ = seginval;
	*pte++ = npmemarr;
	bcopy((char *)pmemarr, (char *)pte, sizeof(pmemarr));
	pte = (int *)((int)pte + sizeof(pmemarr));
	kp = (int *)kernel_segmap_store;
	i = sizeof(kernel_segmap_store) / sizeof(int);
	do {
		*pte++ = *kp++;
		if (pte >= ptend) {
			error = (*dump)(dumpdev, blkno, (caddr_t)buffer,
					dbtob(1));
			if (error != 0)
				return (error);
			++blkno;
			pte = buffer;
		}
	} while (--i > 0);
	if (pte != buffer)
a6671 1
#endif
@


1.10
log
@mmu_3l changes
@
text
@d263 1
a263 1
#ifdef SUN4
d302 1
a302 1
#ifdef SUN4
d658 1
a658 5
#ifdef SUN4
extern int mmu_3l;
#endif

#ifdef SUN4
d872 1
a872 1
#ifdef SUN4
d903 1
a903 1
#ifdef SUN4
d920 1
a920 1
#ifdef SUN4
d1372 1
a1372 1
#ifdef SUN4
d1465 1
a1465 1
#ifdef SUN4
d1502 1
a1502 1
#ifdef SUN4
d1672 1
a1672 1
#ifdef SUN4
d1864 1
a1864 1
#ifdef SUN4
d2075 1
a2075 1
#ifdef SUN4
d2145 1
a2145 1
#ifdef SUN4
d2636 1
a2636 1
#ifdef SUN4
d2703 1
a2703 1
#ifdef SUN4
d2714 1
a2714 1
#ifdef SUN4
d2721 1
a2721 1
#ifdef SUN4
d2739 1
a2739 1
#ifdef SUN4
d2752 1
a2752 1
#ifdef SUN4
d2767 1
a2767 1
#ifdef SUN4
d2877 1
a2877 1
#ifdef SUN4
d2896 1
a2896 1
#ifdef SUN4
d2927 1
a2927 1
#ifdef SUN4
d2940 1
a2940 1
#ifdef SUN4
d2956 1
a2956 1
#ifdef SUN4
d3654 1
a3654 1
#ifdef SUN4
d3656 3
a3658 6
		if (mmu_3l) {
			int i;

			for (i = NUREG; --i >= 0;)
				pm->pm_regmap[i].rg_smeg = reginval;
		}
d3736 1
a3736 1
#ifdef SUN4
d3959 1
a3959 1
#ifdef SUN4
d3970 1
a3970 1
#ifdef SUN4
d4150 1
a4150 1
#ifdef SUN4
d4183 1
a4183 1
#ifdef SUN4
d4228 1
a4228 1
#ifdef SUN4
d4244 1
a4244 1
#ifdef SUN4
d4441 1
a4441 1
#ifdef SUN4
d4463 1
a4463 1
#ifdef SUN4
d4484 1
a4484 1
#ifdef SUN4
d4493 1
a4493 1
#ifdef SUN4
d4507 1
a4507 1
#ifdef SUN4
d4519 1
a4519 1
#ifdef SUN4
d4641 1
a4641 1
#ifdef SUN4
d4738 1
a4738 1
#ifdef SUN4
d5142 1
a5142 1
#ifdef SUN4
d5220 1
a5220 1
#ifdef SUN4
d5341 1
a5341 1
#ifdef SUN4
d5405 1
a5405 1
#ifdef SUN4
d5797 1
a5797 1
#ifdef SUN4
@


1.9
log
@MMU_3L always on for sun4, what the heck
@
text
@d659 4
d3660 6
a3665 3
		if (mmu_3l)
		    for (i = NUREG; --i >= 0;)
			pm->pm_regmap[i].rg_smeg = reginval;
@


1.8
log
@merge 1.2 mmu_pagein decl; 1.3-5 4/300 writethrough vac bug
@
text
@d263 1
a263 1
#ifdef MMU_3L
d302 1
a302 1
#ifdef MMU_3L
d658 1
a658 1
#if defined(MMU_3L)
d872 1
a872 1
#ifdef MMU_3L
d903 1
a903 1
#ifdef MMU_3L
d920 1
a920 1
#ifdef MMU_3L
d1372 1
a1372 1
#ifdef MMU_3L
d1465 1
a1465 1
#ifdef MMU_3L
d1502 1
a1502 1
#ifdef MMU_3L
d1672 1
a1672 1
#ifdef MMU_3L
d1864 1
a1864 1
#ifdef MMU_3L
d2075 1
a2075 1
#ifdef MMU_3L
d2145 1
a2145 1
#ifdef MMU_3L
d2636 1
a2636 1
#ifdef MMU_3L
d2703 1
a2703 1
#ifdef MMU_3L
d2714 1
a2714 1
#ifdef MMU_3L
d2721 1
a2721 1
#ifdef MMU_3L
d2739 1
a2739 1
#ifdef MMU_3L
d2752 1
a2752 1
#ifdef MMU_3L
d2767 1
a2767 1
#ifdef MMU_3L
d2877 1
a2877 1
#ifdef MMU_3L
d2896 1
a2896 1
#ifdef MMU_3L
d2927 1
a2927 1
#ifdef MMU_3L
d2940 1
a2940 1
#ifdef MMU_3L
d2956 1
a2956 1
#ifdef MMU_3L
d3654 1
a3654 1
#ifdef MMU_3L
d3736 1
a3736 1
#ifdef MMU_3L
d3959 1
a3959 1
#ifdef MMU_3L
d3970 1
a3970 1
#ifdef MMU_3L
d4150 1
a4150 1
#ifdef MMU_3L
d4183 1
a4183 1
#ifdef MMU_3L
d4228 1
a4228 1
#ifdef MMU_3L
d4244 1
a4244 1
#ifdef MMU_3L
d4441 1
a4441 1
#ifdef MMU_3L
d4463 1
a4463 1
#ifdef MMU_3L
d4484 1
a4484 1
#ifdef MMU_3L
d4493 1
a4493 1
#ifdef MMU_3L
d4507 1
a4507 1
#ifdef MMU_3L
d4519 1
a4519 1
#ifdef MMU_3L
d4641 1
a4641 1
#ifdef MMU_3L
d4738 1
a4738 1
#ifdef MMU_3L
d5142 1
a5142 1
#ifdef MMU_3L
d5220 1
a5220 1
#ifdef MMU_3L
d5341 1
a5341 1
#ifdef MMU_3L
d5405 1
a5405 1
#ifdef MMU_3L
d5797 1
a5797 1
#ifdef MMU_3L
@


1.7
log
@netbsd port, now we merge our changes back in
@
text
@d1648 2
a1649 1
	register int va, prot;
d4726 7
a4732 1
			if (vactype == VAC_WRITEBACK &&
@


1.6
log
@from netbsd:
Prevent possible race condition in ctx_alloc().
Remove some bogus casts
Make pmap_prefer() also return a preferred virtual address when there's no
associated physical page
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.47 1995/07/05 18:52:32 pk Exp $ */
d4 2
d15 1
d22 1
d30 4
a33 2
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
d51 1
d83 1
a85 2
extern struct promvec *promvec;

d94 2
a95 2
 *   2. An MMU that acts like a cache.  There is not enough space
 *	in the MMU to map everything all the time.  Instead, we need
d97 2
a98 1
 *	process.
d107 1
d111 1
a111 1
 *	We use the last one (since the ROM does as well).
d145 2
a146 1
int	pmapdebug = 0x0;
d149 1
d151 1
d182 2
d188 3
d203 3
d236 11
d268 1
d272 6
d295 3
d300 1
a300 1
caddr_t vdumppages;		/* 32KB worth of reserved dump pages */
d309 11
d330 10
d346 1
a346 1
#ifdef SUN4
d348 6
a353 6
 * segfixmask: on some systems (4/110) "getsegmap()" returns a partly
 * invalid value.   getsegmap returns a 16 bit value on the sun4, but
 * only the first 8 or so bits are valid (the rest are *supposed* to
 * be zero.   on the 4/110 the bits that are supposed to be zero are
 * all one instead.   e.g. KERNBASE is usually mapped by pmeg number zero.
 * on a 4/300 getsegmap(KERNBASE) == 0x0000, but
d356 2
a357 2
 * this confuses mmu_reservemon() and causes it to not reserve the PROM's
 * pmegs.   then the PROM's pmegs get used during autoconfig and everything
d364 2
a369 2
 * NB: setsegmap should be stba for 4c, but stha works and makes the
 * code right for the Sun-4 as well.
d371 264
a634 9
#define	getcontext()		lduba(AC_CONTEXT, ASI_CONTROL)
#define	setcontext(c)		stba(AC_CONTEXT, ASI_CONTROL, c)
#if defined(SUN4) && !defined(SUN4C)
#define	getsegmap(va)		(lduha(va, ASI_SEGMAP) & segfixmask)
#define	setsegmap(va, pmeg)	stha(va, ASI_SEGMAP, pmeg)
#endif
#if !defined(SUN4) && defined(SUN4C)
#define	getsegmap(va)		lduba(va, ASI_SEGMAP)
#define	setsegmap(va, pmeg)	stba(va, ASI_SEGMAP, pmeg)
d636 7
a642 9
#if defined(SUN4) && defined(SUN4C)
#define	getsegmap(va)		(cputyp==CPU_SUN4C ? lduba(va, ASI_SEGMAP) \
				    : (lduha(va, ASI_SEGMAP) & segfixmask))
#define	setsegmap(va, pmeg)	(cputyp==CPU_SUN4C ? stba(va, ASI_SEGMAP, pmeg) \
				    : stha(va, ASI_SEGMAP, pmeg))
#endif
#if defined(SUN4) && defined(MMU_3L)
#define	getregmap(va)		((unsigned)lduha(va+2, ASI_REGMAP) >> 8)
#define	setregmap(va, smeg)	stha(va+2, ASI_REGMAP, (smeg << 8))
d644 2
d647 8
a654 2
#define	getpte(va)		lda(va, ASI_PTE)
#define	setpte(va, pte)		sta(va, ASI_PTE, pte)
d658 5
a662 6
#define	HWTOSW(pg) (pg)
#define	SWTOHW(pg) (pg)

#ifdef MMU_3L
#define CTX_USABLE(pm,rp)	((pm)->pm_ctx && \
				 (!mmu_3l || (rp)->rg_smeg != reginval))
d664 1
a664 1
#define CTX_USABLE(pm,rp)	((pm)->pm_ctx)
d667 5
a671 5
#define GAP_WIDEN(pm,vr) do {		\
	if (vr + 1 == pm->pm_gap_start)	\
		pm->pm_gap_start = vr;	\
	if (vr == pm->pm_gap_end)	\
		pm->pm_gap_end = vr + 1;\
d674 1
a674 1
#define GAP_SHRINK(pm,vr) do {						\
d686 12
d813 1
a813 1
u_long
d823 1
a823 1
		panic("pmap_page_index: pa=0x%x", pa);
d851 2
a852 1
#define	MR(pte) (((pte) >> PG_M_SHIFT) & (PV_MOD | PV_REF))
d865 1
d867 1
a867 1
mmu_reservemon(nrp, nsp)
d870 5
a874 2
	register u_int va, eva;
	register int mmureg, mmuseg, i, nr, ns, vr, lastvr;
d877 7
d885 1
a885 1
	if (cputyp == CPU_SUN4) {
d891 1
a891 1
	if (cputyp == CPU_SUN4C) {
d940 1
a940 1
			setpte(va, getpte(va) | PG_S);
d946 300
d1274 1
d1335 1
a1335 1
		printf("me_alloc: stealing pmeg %x from pmap %x\n",
d1391 1
a1391 1
		tpte = getpte(va);
d1393 1
a1393 1
			pa = ptoa(HWTOSW(tpte & PG_PFNUM));
d1395 1
a1395 1
				pvhead(pa)->pv_flags |= MR(tpte);
d1448 1
a1448 1
		printf("me_free: freeing pmeg %d from pmap %x\n",
d1474 1
a1474 1
		tpte = getpte(va);
d1476 1
a1476 1
			pa = ptoa(HWTOSW(tpte & PG_PFNUM));
d1478 1
a1478 1
				pvhead(pa)->pv_flags |= MR(tpte);
a1513 1
	register int i, va, pa;
a1515 1
	struct segmap *sp;
d1550 1
a1550 1
		printf("region_alloc: stealing smeg %x from pmap %x\n",
d1608 1
a1608 1
		printf("region_free: freeing smeg %x from pmap %x\n",
d1648 1
a1648 2
	register int va;
	vm_prot_t prot;
a1650 1
	register struct mmuentry *me;
d1696 1
a1696 1
		return (bits && (getpte(va) & bits) == bits ? -1 : 0);
d1705 1
a1705 1
		setpte(va, *pte++);
d1711 50
d1779 1
d1784 1
a1784 1
		printf("ctx_alloc(%x)\n", pm);
d1786 4
a1789 2
	gap_start = pm->pm_gap_start;
	gap_end = pm->pm_gap_end;
d1809 1
a1809 1
			printf("ctx_alloc: steal context %x from %x\n",
d1814 6
a1819 4
		if (gap_start < c->c_pmap->pm_gap_start)
			gap_start = c->c_pmap->pm_gap_start;
		if (gap_end > c->c_pmap->pm_gap_end)
			gap_end = c->c_pmap->pm_gap_end;
a1821 1
	setcontext(cnum);
a1824 3
	splx(s);
	if (doflush)
		cache_flush_context();
d1826 20
a1845 17
	/*
	 * Write pmap's region (3-level MMU) or segment table into the MMU.
	 *
	 * Only write those entries that actually map something in this
	 * context by maintaining a pair of region numbers in between
	 * which the pmap has no valid mappings.
	 *
	 * If a context was just allocated from the free list, trust that
	 * all its pmeg numbers are `seginval'. We make sure this is the
	 * case initially in pmap_bootstrap(). Otherwise, the context was
	 * freed by calling ctx_free() in pmap_release(), which in turn is
	 * supposedly called only when all mappings have been removed.
	 *
	 * On the other hand, if the context had to be stolen from another
	 * pmap, we possibly shrink the gap to be the disjuction of the new
	 * and the previous map.
	 */
d1847 4
d1852 25
a1876 9
	rp = pm->pm_regmap;
	for (va = 0, i = NUREG; --i >= 0; ) {
		if (VA_VREG(va) >= gap_start) {
			va = VRTOVA(gap_end);
			i -= gap_end - gap_start;
			rp += gap_end - gap_start;
			if (i < 0)
				break;
			gap_start = NUREG; /* mustn't re-enter this branch */
d1878 57
a1934 5
#ifdef MMU_3L
		if (mmu_3l) {
			setregmap(va, rp++->rg_smeg);
			va += NBPRG;
		} else
d1936 1
a1936 7
		{
			register int j;
			register struct segmap *sp = rp->rg_segmap;
			for (j = NSEGRG; --j >= 0; va += NBPSG)
				setsegmap(va, sp?sp++->sg_pmeg:seginval);
			rp++;
		}
d1954 1
d1959 4
d1965 4
d1973 10
d2005 3
d2009 1
a2009 1
pv_changepte(pv0, bis, bic)
d2016 1
a2016 1
	register int va, vr, vs, i, flags;
d2068 1
a2068 1
				tpte = getpte(va);
d2080 1
a2080 1
				tpte = getpte(va);
d2083 1
a2083 2
				flags |= (tpte >> PG_M_SHIFT) &
				    (PV_MOD|PV_REF);
d2085 1
a2085 1
			setpte(va, tpte);
d2103 1
a2103 1
pv_syncflags(pv0)
d2108 1
a2108 1
	register int tpte, va, vr, vs, pmeg, i, flags;
d2138 1
a2138 1
			tpte = getpte(va);
d2150 1
a2150 1
			tpte = getpte(va);
d2156 1
a2156 1
			setpte(va, tpte);
d2177 1
a2177 1
pv_unlink(pv, pm, va)
d2223 1
a2223 1
		pv_changepte(pv, 0, PG_NC);
d2233 1
a2233 1
pv_link(pv, pm, va)
d2265 1
a2265 1
				"pv_link: badalias: pid %d, %x<=>%x, pa %x\n",
d2270 1
a2270 1
				pv_changepte(pv, ret = PG_NC, 0);
d2283 3
d2287 11
a2297 2
 * Walk the given list and flush the cache for each (MI) page that is
 * potentially in the cache. Called only if vactype != VAC_NONE.
d2299 5
a2303 1
pv_flushcache(pv)
a2304 1
{
d2306 3
a2308 1
	register int i, s, ctx;
d2310 1
a2310 1
	write_user_windows();	/* paranoia? */
d2312 259
a2570 14
	s = splpmap();		/* XXX extreme paranoia */
	if ((pm = pv->pv_pmap) != NULL) {
		ctx = getcontext();
		for (;;) {
			if (pm->pm_ctx) {
				setcontext(pm->pm_ctxnum);
				cache_flush_page(pv->pv_va);
			}
			pv = pv->pv_next;
			if (pv == NULL)
				break;
			pm = pv->pv_pmap;
		}
		setcontext(ctx);
d2585 7
d2603 30
d2634 4
a2637 1
	register struct mmuentry *mmuseg, *mmureg;
a2650 1
	extern caddr_t reserve_dumppages(caddr_t);
a2665 5
#if defined(SUN4) && defined(SUN4C)
	/* In this case NPTESG is not a #define */
	nptesg = (NBPSG >> pgshift);
#endif

d2680 15
a2694 1
	ncontext = nctx;
d2736 1
a2736 1
	mmu_reservemon(&nregion, &nsegment);
d2924 1
a2924 1
			setpte(p, 0);
d2985 1
a2985 1
			setpte(p, getpte(p) & mask);
d2988 1
d2990 8
a2997 2
void
pmap_init()
d2999 4
a3002 2
	register vm_size_t s;
	int pass1, nmem;
d3004 15
a3018 2
	vm_offset_t sva, va, eva;
	vm_offset_t pa;
d3020 15
a3034 2
	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE!=1");
d3037 481
a3517 4
	 * Map pv_table[] as a `sparse' array. This requires two passes
	 * over the `pmemarr': (1) to determine the number of physical
	 * pages needed, and (2), to map the correct pieces of virtual
	 * memory allocated to pv_table[].
d3540 1
a3540 1
			panic("pmap_init: unmanaged address: 0x%x", addr);
d3544 1
d3546 5
a3550 1
#ifdef DEBUG
d3555 2
a3556 1
				panic("pmap_init: sva(%x) < eva(%x)", sva, eva);
d3619 1
a3619 1
		printf("pmap_create: created %x\n", pm);
d3634 1
a3634 1
	register int i, size;
d3639 1
a3639 1
		printf("pmap_pinit(%x)\n", pm);
d3643 1
d3645 1
a3645 1
	bzero((caddr_t)urp, size);
a3648 4
#ifdef MMU_3L
	TAILQ_INIT(&pm->pm_reglist);
#endif
	TAILQ_INIT(&pm->pm_seglist);
d3650 3
d3654 3
a3656 2
	if (mmu_3l)
		for (i = NUREG; --i >= 0;)
d3659 29
d3707 1
a3707 1
		printf("pmap_destroy(%x)\n", pm);
d3731 1
a3731 1
		printf("pmap_release(%x)\n", pm);
d3733 2
d3736 2
a3737 2
	if (pm->pm_reglist.tqh_first)
		panic("pmap_release: region list not empty");
d3739 8
a3746 6
	if (pm->pm_seglist.tqh_first)
		panic("pmap_release: segment list not empty");
	if ((c = pm->pm_ctx) != NULL) {
		if (pm->pm_ctxnum == 0)
			panic("pmap_release: releasing kernel");
		ctx_free(pm);
d3749 1
d3751 1
a3751 1
{
d3778 11
a3805 5
static void pmap_rmk __P((struct pmap *, vm_offset_t, vm_offset_t,
			  int, int));
static void pmap_rmu __P((struct pmap *, vm_offset_t, vm_offset_t,
			  int, int));

d3826 1
a3826 1
		printf("pmap_remove(%x, %x, %x)\n", pm, va, endva);
d3880 2
d3883 2
a3884 2
static void
pmap_rmk(pm, va, endva, vr, vs)
d3931 1
a3931 1
		tpte = getpte(va);
d3933 1
a3933 1
			va += PAGE_SIZE;
d3940 1
a3940 1
			i = ptoa(HWTOSW(tpte & PG_PFNUM));
d3943 2
a3944 2
				pv->pv_flags |= MR(tpte);
				pv_unlink(pv, pm, va);
d3948 1
a3948 1
		setpte(va, 0);
d3983 1
a3983 9
/*
 * Just like pmap_rmk_magic, but we have a different threshold.
 * Note that this may well deserve further tuning work.
 */
#if 0
#define	PMAP_RMU_MAGIC	(cacheinfo.c_hwflush?4:64)	/* if > magic, use cache_flush_segment */
#else
#define	PMAP_RMU_MAGIC	4	/* if > magic, use cache_flush_segment */
#endif
d3985 4
a3988 3
/* remove from user */
static void
pmap_rmu(pm, va, endva, vr, vs)
d3993 1
a3993 1
	register int *pte0, i, pteva, tpte, perpage, npg;
d3995 1
a3995 1
	register int nleft, pmeg;
d4000 2
d4004 2
d4007 2
a4008 1
		panic("pmap_rmu: no segments");
a4009 1
	sp = &rp->rg_segmap[vs];
d4012 106
a4117 2
	if (sp->sg_pte == NULL)
		panic("pmap_rmu: no pages");
d4129 1
a4129 1
		for (; va < endva; pte++, va += PAGE_SIZE) {
d4136 1
a4136 1
				i = ptoa(HWTOSW(tpte & PG_PFNUM));
d4138 1
a4138 1
					pv_unlink(pvhead(i), pm, va);
d4191 2
a4192 2
	for (; va < endva; pteva += PAGE_SIZE, va += PAGE_SIZE) {
		tpte = getpte(pteva);
d4199 1
a4199 1
			i = ptoa(HWTOSW(tpte & PG_PFNUM));
d4202 2
a4203 2
				pv->pv_flags |= MR(tpte);
				pv_unlink(pv, pm, va);
d4207 1
a4207 1
		setpte(pteva, 0);
d4256 103
d4367 2
d4370 1
a4370 1
pmap_page_protect(pa, prot)
d4377 1
a4377 1
	register int flags, nleft, i, s, ctx, doflush;
d4383 1
a4383 1
		panic("pmap_page_protect: no such address: %x", pa);
d4386 1
a4386 1
		printf("pmap_page_protect(%x, %x)\n", pa, prot);
d4397 1
a4397 1
		pv_changepte(pvhead(pa), 0, PG_W);
d4470 1
a4470 1
		tpte = getpte(pteva);
d4473 1
a4473 1
		flags |= MR(tpte);
d4476 1
a4476 1
			setpte(pteva, 0);
d4555 1
a4555 1
pmap_protect(pm, sva, eva, prot)
d4560 1
a4560 1
	register int va, nva, vr, vs, pteva;
d4621 1
a4621 1
					tpte = getpte(va);
d4628 1
a4628 1
						setpte(va, tpte & ~PG_W);
d4647 1
a4647 1
					setpte(pteva, getpte(pteva) & ~PG_W);
d4662 1
a4662 1
pmap_changeprot(pm, va, prot, wired)
d4668 1
a4668 1
	register int vr, vs, tpte, newprot, ctx, i, s;
d4674 1
a4674 1
		printf("pmap_changeprot(%x, %x, %x, %x)\n",
d4680 1
d4720 1
a4720 1
			tpte = getpte(va);
d4725 1
a4725 7

			/*
			 * the latter check deals with a writethrough cache
			 * problem on the 4/300
			 */
			if ((vactype==VAC_WRITEBACK ||
			    (vactype==VAC_WRITETHROUGH && cputyp==CPU_SUN4)) &&
d4737 1
a4737 1
			tpte = getpte(va);
d4744 1
a4744 1
		setpte(va, tpte);
d4756 3
d4760 2
a4761 2
 * Insert (MI) physical page pa at virtual address va in the given pmap.
 * NB: the pa parameter includes type bits PMAP_OBIO, PMAP_NC as necessary.
d4763 3
a4765 7
 * If pa is not in the `managed' range it will not be `bank mapped'.
 * This works during bootstrap only because the first 4MB happens to
 * map one-to-one.
 *
 * There may already be something else there, or we might just be
 * changing protections and/or wiring on an existing mapping.
 *	XXX	should have different entry points for changing!
d4768 2
a4769 3
pmap_enter(pm, va, pa, prot, wired)
	register struct pmap *pm;
	vm_offset_t va, pa;
a4770 1
	int wired;
d4772 6
a4777 5
	register struct pvlist *pv;
	register int pteproto, ctx;

	if (pm == NULL)
		return;
a4778 1
	if (VA_INHOLE(va)) {
d4780 5
a4784 2
		printf("pmap_enter: pm %x, va %x, pa %x: in MMU hole\n",
			pm, va, pa);
d4786 9
a4797 8
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter(%x, %x, %x, %x, %x)\n",
		    pm, va, pa, prot, wired);
#endif

	pteproto = PG_V | ((pa & PMAP_TNC) << PG_TNC_SHIFT);
	pa &= ~PMAP_TNC;
d4799 4
a4802 3
	 * Set up prototype for new PTE.  Cannot set PG_NC from PV_NC yet
	 * since the pvlist no-cache bit might change as a result of the
	 * new mapping.
d4804 5
a4808 10
	if ((pteproto & PG_TYPE) == PG_OBMEM && managed(pa)) {
#ifdef DIAGNOSTIC
		if (!pmap_pa_exists(pa))
			panic("pmap_enter: no such address: %x", pa);
#endif
		pteproto |= SWTOHW(atop(pa));
		pv = pvhead(pa);
	} else {
		pteproto |= atop(pa) & PG_PFNUM;
		pv = NULL;
a4809 3
	if (prot & VM_PROT_WRITE)
		pteproto |= PG_W;

d4811 14
a4824 6
	if (pm == pmap_kernel())
		pmap_enk(pm, va, prot, wired, pv, pteproto | PG_S);
	else
		pmap_enu(pm, va, prot, wired, pv, pteproto);
	setcontext(ctx);
}
d4826 9
a4834 12
/* enter new (or change existing) kernel mapping */
pmap_enk(pm, va, prot, wired, pv, pteproto)
	register struct pmap *pm;
	vm_offset_t va;
	vm_prot_t prot;
	int wired;
	register struct pvlist *pv;
	register int pteproto;
{
	register int vr, vs, tpte, i, s;
	struct regmap *rp;
	struct segmap *sp;
d4836 3
a4838 5
	vr = VA_VREG(va);
	vs = VA_VSEG(va);
	rp = &pm->pm_regmap[vr];
	sp = &rp->rg_segmap[vs];
	s = splpmap();		/* XXX way too conservative */
d4840 14
a4853 9
#ifdef MMU_3L
	if (mmu_3l && rp->rg_smeg == reginval) {
		vm_offset_t tva;
		rp->rg_smeg = region_alloc(&region_locked, pm, vr)->me_cookie;
		i = ncontext - 1;
		do {
			setcontext(i);
			setregmap(va, rp->rg_smeg);
		} while (--i >= 0);
d4855 300
a5154 10
		/* set all PTEs to invalid, then overwrite one PTE below */
		tva = VA_ROUNDDOWNTOREG(va);
		for (i = 0; i < NSEGRG; i++) {
			setsegmap(tva, rp->rg_segmap[i].sg_pmeg);
			tva += NBPSG;
		};
	}
#endif
	if (sp->sg_pmeg != seginval && (tpte = getpte(va)) & PG_V) {
		register int addr;
d5167 1
a5167 1
printf("pmap_enk: changing existing va=>pa entry: va %x, pteproto %x\n",
d5175 1
a5175 1
			addr = ptoa(HWTOSW(tpte & PG_PFNUM));
d5177 1
a5177 1
				pv_unlink(pvhead(addr), pm, va);
d5195 1
a5195 1
		pteproto |= pv_link(pv, pm, va);
d5230 1
a5230 1
			setpte(tva, 0);
d5236 1
a5236 1
	setpte(va, pteproto);
d5241 2
a5242 1
pmap_enu(pm, va, prot, wired, pv, pteproto)
d5291 1
a5291 1
		bzero((caddr_t)sp, size);
d5315 1
a5315 1
		bzero((caddr_t)pte, size);
d5329 1
a5329 1
				tpte = getpte(va);
d5339 1
a5339 1
				tpte = getpte(VA_VPG(va) << PGSHIFT);
d5351 1
a5351 1
				pmap_changeprot(pm, va, prot, wired);
d5366 1
a5366 1
				addr = ptoa(HWTOSW(tpte & PG_PFNUM));
d5368 1
a5368 1
					pv_unlink(pvhead(addr), pm, va);
d5386 1
a5386 1
		pteproto |= pv_link(pv, pm, va);
d5404 329
a5732 6
		}
		setpte(va, pteproto);
	}
	/* update software copy */
	pte += VA_VPG(va);
	*pte = pteproto;
d5736 1
d5757 2
d5760 1
a5760 1
pmap_extract(pm, va)
d5786 2
a5787 2
			setcontext(pm->pm_ctxnum);
			tpte = getpte(va);
d5789 1
a5789 1
			setcontext(0);
d5795 1
a5795 1
			tpte = getpte(VA_VPG(va) << PGSHIFT);
d5812 1
a5812 1
	tpte = HWTOSW(tpte);
d5815 39
d5870 18
d5898 4
d5919 2
d5925 1
a5925 1
pmap_clear_modify(pa)
d5932 1
a5932 1
		(void) pv_syncflags(pv);
d5941 1
a5941 1
pmap_is_modified(pa)
d5948 1
a5948 1
		if (pv->pv_flags & PV_MOD || pv_syncflags(pv) & PV_MOD)
d5958 1
a5958 1
pmap_clear_reference(pa)
d5965 1
a5965 1
		(void) pv_syncflags(pv);
d5974 1
a5974 1
pmap_is_referenced(pa)
d5981 1
a5981 1
		if (pv->pv_flags & PV_REF || pv_syncflags(pv) & PV_REF)
d5986 79
d6091 3
d6095 1
a6095 1
pmap_zero_page(pa)
d6109 63
a6171 3
		pte = PG_V | PG_S | PG_W | PG_NC | SWTOHW(atop(pa));
	} else
		pte = PG_V | PG_S | PG_W | PG_NC | (atop(pa) & PG_PFNUM);
d6173 11
d6185 1
a6185 1
	setpte(va, pte);
d6187 1
a6187 1
	setpte(va, 0);
d6200 1
a6200 1
pmap_copy_page(src, dst)
d6209 3
a6211 3
		spte = PG_V | PG_S | SWTOHW(atop(src));
	} else
		spte = PG_V | PG_S | (atop(src) & PG_PFNUM);
d6217 3
a6219 3
		dpte = PG_V | PG_S | PG_W | PG_NC | SWTOHW(atop(dst));
	} else
		dpte = PG_V | PG_S | PG_W | PG_NC | (atop(dst) & PG_PFNUM);
d6223 2
a6224 2
	setpte(sva, spte);
	setpte(dva, dpte);
d6228 2
a6229 2
	setpte(sva, 0);
	setpte(dva, 0);
d6231 1
d6252 1
d6258 24
a6281 9

	for (; --npages >= 0; va += NBPG) {
		pte = getpte(va);
		if ((pte & PG_V) == 0)
			panic("kvm_uncache !pg_v");
		pte |= PG_NC;
		setpte(va, pte);
		if (vactype != VAC_NONE && (pte & PG_TYPE) == PG_OBMEM)
			cache_flush_page((int)va);
d6291 1
a6295 1
	register int pte;
d6297 5
d6303 1
a6303 1
		pte = getpte(va);
d6307 1
a6307 1
		setpte(va, pte);
d6309 1
d6335 3
a6337 2
 * Find first virtual address >= va that doesn't cause
 * a cache alias on physical address pa.
d6339 4
a6342 4
vm_offset_t
pmap_prefer(pa, va)
	register vm_offset_t pa;
	register vm_offset_t va;
d6344 5
a6348 2
	register struct pvlist	*pv;
	register long		m, d;
d6352 1
a6352 18
		return (va);

	if (pa == (vm_offset_t)-1) {
		/*
		 * Do not consider physical address. Just return
		 * a cache aligned address.
		 */     
		if (VA_INHOLE(va))
			va = MMU_HOLE_END;

		/* XXX - knowledge about `exec' formats; can we get by without? */
		va -= USRTEXT;
		va = (va + m - 1) & ~(m - 1);
		return (va + USRTEXT);
	}

	if ((pa & (PMAP_TNC & ~PMAP_NC)) || !managed(pa))
		return va;
d6354 3
a6356 12
	pv = pvhead(pa);
	if (pv->pv_pmap == NULL) {
		/* Unusable, tell caller to try another one */
		return (vm_offset_t)-1;
	}

	d = (long)(pv->pv_va & (m - 1)) - (long)(va & (m - 1));
	if (d < 0)
		va += m;
	va += d;

	return va;
d6359 1
d6362 12
a6373 1
	setpte(KERNBASE, 0);
d6380 1
a6380 1
int
d6391 1
a6391 1
int
d6400 18
d6425 9
d6435 1
a6435 1
			panic("%s: rp=%x", s, rp);
d6440 1
a6440 1
				panic("%s: sp=%x", s, sp);
d6446 10
a6455 1

d6459 4
a6462 2
					if (*pte & PG_V)
						m++;
d6475 1
a6475 1
	return 0;
d6478 2
a6479 2
int
pm_check_k(s, pm)
a6483 1
	struct segmap *sp;
d6486 16
d6509 9
d6528 1
a6528 1
	return 0;
d6540 9
a6548 4
	return btoc(((seginval + 1) * NPTESG * sizeof(int)) +
		    sizeof(seginval) +
		    sizeof(pmemarr) +
		    sizeof(kernel_segmap_store));
d6554 1
a6554 1
 * there is no in-core copy of kernel memory mappings.
d6561 1
a6562 1
	register int addr;	/* unused kernel virtual address */
d6565 1
a6565 1
	register int error;
d6568 1
d6570 4
d6583 1
d6597 1
a6597 1
			*pte++ = getpte(va);
d6602 1
a6602 1
				 * all the PMEGs occupy 32KB which is 
d6644 159
d6804 17
@


1.5
log
@4/300 cache really is write through but has a slight bug.
@
text
@d1082 1
a1082 1
	register int cnum, i;
d1096 1
d1100 1
a1100 1
		setcontext(cnum);
d1118 1
a1118 3
		setcontext(cnum);
		if (vactype != VAC_NONE)
			cache_flush_context();
d1124 2
d1129 3
d1415 1
a1415 1
			free((caddr_t)npv, M_VMPVENT);
d1429 1
a1429 1
		free((caddr_t)npv, M_VMPVENT);
d2073 1
a2073 1
		free((caddr_t)pm, M_VMPMAP);
d2131 1
a2131 1
		free((caddr_t)pm->pm_regstore, M_VMPMAP);
d2389 1
a2389 1
			free((caddr_t)pte0, M_VMPMAP);
d2392 1
a2392 1
				free((caddr_t)rp->rg_segmap, M_VMPMAP);
d2479 1
a2479 1
		free((caddr_t)pte0, M_VMPMAP);
d2484 1
a2484 1
			free((caddr_t)rp->rg_segmap, M_VMPMAP);
d2574 1
a2574 1
				free((caddr_t)sp->sg_pte, M_VMPMAP);
d2577 1
a2577 1
					free((caddr_t)rp->rg_segmap, M_VMPMAP);
d2653 1
a2653 1
				free((caddr_t)sp->sg_pte, M_VMPMAP);
d2665 1
a2665 1
					free((caddr_t)rp->rg_segmap, M_VMPMAP);
d2674 1
a2674 1
			free((caddr_t)pv, M_VMPVENT);
d3140 1
a3140 1
			free((caddr_t)sp, M_VMPMAP);
d3160 1
a3160 1
			free((caddr_t)pte, M_VMPMAP);
d3633 3
a3635 3
	if (cputyp == CPU_SUN4M)
		/* does the sun4m have the cache alias problem? */
		return va;
d3637 13
a3649 1
	m = CACHE_ALIAS_DIST;
a3655 3
#if 0
		return ((va + m - 1) & ~(m - 1));
#else
a3657 1
#endif
@


1.4
log
@XXX is 4/300 really write-through?
@
text
@d2865 2
a2866 1
			if (vactype == VAC_WRITEBACK &&
@


1.3
log
@4/300 cache is writethrough; but... it fails to update pte's in the cache
problem reported by chuck cranor
@
text
@d2865 1
a2865 2
			if ((vactype==VAC_WRITEBACK ||
			    (vactype==VAC_WRITETHROUGH && cputyp==CPU_SUN4)) &&
@


1.2
log
@correct mmu_pagein decl
@
text
@d2860 7
a2866 1
			if (vactype == VAC_WRITEBACK &&
@


1.1
log
@Initial revision
@
text
@d1004 2
a1005 1
	register int va, prot;
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

