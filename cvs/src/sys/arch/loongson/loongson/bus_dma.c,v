head	1.19;
access;
symbols
	OPENBSD_6_1:1.19.0.6
	OPENBSD_6_1_BASE:1.19
	OPENBSD_6_0:1.19.0.4
	OPENBSD_6_0_BASE:1.19
	OPENBSD_5_9:1.19.0.2
	OPENBSD_5_9_BASE:1.19
	OPENBSD_5_8:1.18.0.6
	OPENBSD_5_8_BASE:1.18
	OPENBSD_5_7:1.18.0.2
	OPENBSD_5_7_BASE:1.18
	OPENBSD_5_6:1.15.0.4
	OPENBSD_5_6_BASE:1.15
	OPENBSD_5_5:1.11.0.8
	OPENBSD_5_5_BASE:1.11
	OPENBSD_5_4:1.11.0.4
	OPENBSD_5_4_BASE:1.11
	OPENBSD_5_3:1.11.0.2
	OPENBSD_5_3_BASE:1.11
	OPENBSD_5_2:1.10.0.2
	OPENBSD_5_2_BASE:1.10
	OPENBSD_5_1_BASE:1.7
	OPENBSD_5_1:1.7.0.4
	OPENBSD_5_0:1.7.0.2
	OPENBSD_5_0_BASE:1.7
	OPENBSD_4_9:1.6.0.2
	OPENBSD_4_9_BASE:1.6
	OPENBSD_4_8:1.5.0.2
	OPENBSD_4_8_BASE:1.5
	OPENBSD_4_7:1.3.0.2
	OPENBSD_4_7_BASE:1.3
	xmas_lemote:1.1.1.1
	miod:1.1.1;
locks; strict;
comment	@ * @;


1.19
date	2015.08.15.22.31.38;	author miod;	state Exp;
branches;
next	1.18;
commitid	yF3Npu6AQ8EgYJml;

1.18
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.17;
commitid	yv0ECmCdICvq576h;

1.17
date	2014.09.26.14.32.07;	author jsing;	state Exp;
branches;
next	1.16;
commitid	OvRx4Hf7atBtxEXK;

1.16
date	2014.09.13.16.06.36;	author doug;	state Exp;
branches;
next	1.15;
commitid	jdBY2kKXhfcoQitp;

1.15
date	2014.07.12.18.44.42;	author tedu;	state Exp;
branches;
next	1.14;
commitid	uKVPYMN2MLxdZxzH;

1.14
date	2014.07.11.09.36.26;	author mpi;	state Exp;
branches;
next	1.13;
commitid	vsYjSRfS3Y783BvW;

1.13
date	2014.03.21.21.49.45;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2014.03.10.21.32.15;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2012.10.03.21.44.51;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2012.04.21.12.20.30;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2012.03.25.13.52.52;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2012.03.15.18.57.20;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2011.06.23.20.44.39;	author ariane;	state Exp;
branches;
next	1.6;

1.6
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2010.06.26.23.24.43;	author guenther;	state Exp;
branches;
next	1.4;

1.4
date	2010.03.29.19.21.58;	author oga;	state Exp;
branches;
next	1.3;

1.3
date	2010.02.09.19.23.19;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2010.01.09.23.34.29;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2009.12.11.17.23.29;	author miod;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2009.12.11.17.23.29;	author miod;	state Exp;
branches;
next	;


desc
@@


1.19
log
@Some bits for Loongson 3A support.
@
text
@/*	$OpenBSD: bus_dma.c,v 1.18 2014/11/16 12:30:57 deraadt Exp $ */

/*
 * Copyright (c) 2003-2004 Opsycon AB  (www.opsycon.se / www.opsycon.com)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */
/*-
 * Copyright (c) 1996, 1997, 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>

#include <uvm/uvm_extern.h>

#include <mips64/cache.h>
#include <machine/cpu.h>
#include <machine/autoconf.h>

#include <machine/bus.h>

/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
int
_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
    bus_size_t maxsegsz, bus_size_t boundary, int flags, bus_dmamap_t *dmamp)
{
	struct machine_bus_dmamap *map;
	void *mapstore;
	size_t mapsize;

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
	mapsize = sizeof(struct machine_bus_dmamap) +
	    (sizeof(bus_dma_segment_t) * (nsegments - 1));
	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
		return (ENOMEM);

	map = (struct machine_bus_dmamap *)mapstore;
	map->_dm_size = size;
	map->_dm_segcnt = nsegments;
	map->_dm_maxsegsz = maxsegsz;
	map->_dm_boundary = boundary;
	map->_dm_flags = flags & ~(BUS_DMA_WAITOK|BUS_DMA_NOWAIT);

	*dmamp = map;
	return (0);
}

/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
void
_dmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{
	free(map, M_DEVBUF, 0);
}

/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
int
_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map, void *buf, bus_size_t buflen,
    struct proc *p, int flags)
{
	paddr_t lastaddr;
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	seg = 0;
	error = (*t->_dmamap_load_buffer)(t, map, buf, buflen, p, flags,
	    &lastaddr, &seg, 1);
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = buflen;
	}

	return (error);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map, struct mbuf *m0, int flags)
{
	paddr_t lastaddr;
	int seg, error, first;
	struct mbuf *m;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic("_dmamap_load_mbuf: no packet header");
#endif

	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	first = 1;
	seg = 0;
	error = 0;
	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		error = (*t->_dmamap_load_buffer)(t, map, m->m_data, m->m_len,
		    NULL, flags, &lastaddr, &seg, first);
		first = 0;
	}
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = m0->m_pkthdr.len;
	}

	return (error);
}

/*
 * Like _dmamap_load(), but for uios.
 */
int
_dmamap_load_uio(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio, int flags)
{
	paddr_t lastaddr;
	int seg, i, error, first;
	bus_size_t minlen, resid;
	struct proc *p = NULL;
	struct iovec *iov;
	void *addr;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;

	resid = uio->uio_resid;
	iov = uio->uio_iov;

	if (uio->uio_segflg == UIO_USERSPACE) {
		p = uio->uio_procp;
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic("_dmamap_load_uio: USERSPACE but no proc");
#endif
	}

	first = 1;
	seg = 0;
	error = 0;
	for (i = 0; i < uio->uio_iovcnt && resid != 0 && error == 0; i++) {
		/*
		 * Now at the first iovec to load.  Load each iovec
		 * until we have exhausted the residual count.
		 */
		minlen = resid < iov[i].iov_len ? resid : iov[i].iov_len;
		addr = (void *)iov[i].iov_base;

		error = (*t->_dmamap_load_buffer)(t, map, addr, minlen,
		    p, flags, &lastaddr, &seg, first);
		first = 0;

		resid -= minlen;
	}
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = uio->uio_resid;
	}

	return (error);
}

/*
 * Like _dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
_dmamap_load_raw(bus_dma_tag_t t, bus_dmamap_t map, bus_dma_segment_t *segs,
    int nsegs, bus_size_t size, int flags)
{
	if (nsegs > map->_dm_segcnt || size > map->_dm_size)
		return (EINVAL);

	/*
	 * Make sure we don't cross any boundaries.
	 */
	if (map->_dm_boundary) {
		bus_addr_t bmask = ~(map->_dm_boundary - 1);
		int i;

		if (t->_dma_mask != 0)
			bmask &= t->_dma_mask;
		for (i = 0; i < nsegs; i++) {
			if (segs[i].ds_len > map->_dm_maxsegsz)
				return (EINVAL);
			if ((segs[i].ds_addr & bmask) !=
			    ((segs[i].ds_addr + segs[i].ds_len - 1) & bmask))
				return (EINVAL);
		}
	}

	bcopy(segs, map->dm_segs, nsegs * sizeof(*segs));
	map->dm_nsegs = nsegs;
	map->dm_mapsize = size;
	return (0);
}

/*
 * Common function for unloading a DMA map.  May be called by
 * bus-specific DMA map unload functions.
 */
void
_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{
	/*
	 * No resources to free; just mark the mappings as
	 * invalid.
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;
}

/*
 * Common function for DMA map synchronization.  May be called
 * by bus-specific DMA map synchronization functions.
 */
void
_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t addr,
    bus_size_t size, int op)
{
	int nsegs;
	int curseg;
	int cacheop;
	struct cpu_info *ci = curcpu();

	nsegs = map->dm_nsegs;
	curseg = 0;

	while (size && nsegs) {
		vaddr_t vaddr;
		paddr_t paddr;
		bus_size_t ssize;

		ssize = map->dm_segs[curseg].ds_len;
		paddr = map->dm_segs[curseg]._ds_paddr;
		vaddr = map->dm_segs[curseg]._ds_vaddr;

		if (addr != 0) {
			if (addr >= ssize) {
				addr -= ssize;
				ssize = 0;
			} else {
				vaddr += addr;
				paddr += addr;
				ssize -= addr;
				addr = 0;
			}
		}
		if (ssize > size)
			ssize = size;

		if (IS_XKPHYS(vaddr) && XKPHYS_TO_CCA(vaddr) == CCA_NC) {
			size -= ssize;
			ssize = 0;
		}

		if (ssize != 0) {
			/*
			 * If only PREWRITE is requested, writeback.
			 * PREWRITE with PREREAD writebacks
			 * and invalidates (if noncoherent) *all* cache levels.
			 * Otherwise, just invalidate (if noncoherent).
			 */
			if (op & BUS_DMASYNC_PREWRITE) {
				if (op & BUS_DMASYNC_PREREAD)
					cacheop = CACHE_SYNC_X;
				else
					cacheop = CACHE_SYNC_W;
			} else {
				if (op & BUS_DMASYNC_PREREAD)
					cacheop = CACHE_SYNC_R;
				else if (op & BUS_DMASYNC_POSTREAD)
					cacheop = CACHE_SYNC_R;
				else
					cacheop = -1;
			}

			if (cacheop >= 0)
				Mips_IOSyncDCache(ci, vaddr, ssize, cacheop);
			size -= ssize;
		}
		curseg++;
		nsegs--;
	}

#ifdef DIAGNOSTIC
	if (size != 0) {
		panic("_dmamap_sync: ran off map!");
	}
#endif
}

/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */
int
_dmamem_alloc(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags)
{
	return _dmamem_alloc_range(t, size, alignment, boundary,
	    segs, nsegs, rsegs, flags, (paddr_t)0, (paddr_t)-1);
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
_dmamem_free(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs)
{
	vm_page_t m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE((*t->_device_to_pa)(addr));
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}

	uvm_pglistfree(&mlist);
}

/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
int
_dmamem_map(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs, size_t size,
    caddr_t *kvap, int flags)
{
	vaddr_t va, sva;
	size_t ssize;
	paddr_t pa;
	bus_addr_t addr;
	int curseg, error, pmap_flags;
	const struct kmem_dyn_mode *kd;

#ifdef CPU_LOONGSON3
	/*
	 * Loongson 3 caches are coherent.
	 */
	if (loongson_ver >= 0x3a)
		flags &= ~BUS_DMA_COHERENT;
#endif

	if (nsegs == 1) {
		pa = (*t->_device_to_pa)(segs[0].ds_addr);
		if (flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE))
			*kvap = (caddr_t)PHYS_TO_XKPHYS(pa, CCA_NC);
		else
			*kvap = (caddr_t)PHYS_TO_XKPHYS(pa, CCA_CACHED);
		return (0);
	}

	size = round_page(size);
	kd = flags & BUS_DMA_NOWAIT ? &kd_trylock : &kd_waitok;
	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, kd);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	sva = va;
	ssize = size;
	pmap_flags = PMAP_WIRED | PMAP_CANFAIL;
	if (flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE))
		pmap_flags |= PMAP_NOCACHE;
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += NBPG, va += NBPG, size -= NBPG) {
			if (size == 0)
				panic("_dmamem_map: size botch");
			pa = (*t->_device_to_pa)(addr);
			error = pmap_enter(pmap_kernel(), va, pa,
			    PROT_READ | PROT_WRITE,
			    PROT_READ | PROT_WRITE | pmap_flags);
			if (error) {
				pmap_update(pmap_kernel());
				km_free((void *)sva, ssize, &kv_any, &kp_none);
				return (error);
			}

			/*
			 * This is redundant with what pmap_enter() did 
			 * above, but will take care of forcing other
			 * mappings of the same page (if any) to be
			 * uncached. 
			 * If there are no multiple mappings of that 
			 * page, this amounts to a noop.
			 */
			if (flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE)) 
				pmap_page_cache(PHYS_TO_VM_PAGE(pa),
				    PGF_UNCACHED);
		}
		pmap_update(pmap_kernel());
	}

	return (0);
}

/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
void
_dmamem_unmap(bus_dma_tag_t t, caddr_t kva, size_t size)
{
	if (IS_XKPHYS((vaddr_t)kva))
		return;

	km_free(kva, round_page(size), &kv_any, &kp_none);
}

/*
 * Common function for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
paddr_t
_dmamem_mmap(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs, off_t off,
    int prot, int flags)
{
	int i;

	for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
		if (off & PGOFSET)
			panic("_dmamem_mmap: offset unaligned");
		if (segs[i].ds_addr & PGOFSET)
			panic("_dmamem_mmap: segment unaligned");
		if (segs[i].ds_len & PGOFSET)
			panic("_dmamem_mmap: segment size not multiple"
			    " of page size");
#endif
		if (off >= segs[i].ds_len) {
			off -= segs[i].ds_len;
			continue;
		}

		return ((*t->_device_to_pa)(segs[i].ds_addr) + off);
	}

	/* Page not found. */
	return (-1);
}

/**********************************************************************
 * DMA utility functions
 **********************************************************************/

/*
 * Utility function to load a linear buffer.  lastaddrp holds state
 * between invocations (for multiple-buffer loads).  segp contains
 * the starting segment on entrance, and the ending segment on exit.
 * first indicates if this is the first invocation of this function.
 */
int
_dmamap_load_buffer(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, paddr_t *lastaddrp,
    int *segp, int first)
{
	bus_size_t sgsize;
	bus_addr_t lastaddr, baddr, bmask;
	paddr_t curaddr;
	vaddr_t vaddr = (vaddr_t)buf;
	int seg;
	pmap_t pmap;

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = *lastaddrp;
	bmask  = ~(map->_dm_boundary - 1);
	if (t->_dma_mask != 0)
		bmask &= t->_dma_mask;

	for (seg = *segp; buflen > 0; ) {
		/*
		 * Get the physical address for this segment.
		 */
		if (pmap_extract(pmap, vaddr, &curaddr) == FALSE)
			panic("_dmapmap_load_buffer: pmap_extract(%p, %lx) "
			    "failed!", pmap, vaddr);

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = NBPG - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = ((bus_addr_t)curaddr + map->_dm_boundary) &
			    bmask;
			if (sgsize > (baddr - (bus_addr_t)curaddr))
				sgsize = (baddr - (bus_addr_t)curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr =
			    (*t->_pa_to_device)(curaddr);
			map->dm_segs[seg].ds_len = sgsize;
			map->dm_segs[seg]._ds_paddr = curaddr;
			map->dm_segs[seg]._ds_vaddr = vaddr;
			first = 0;
		} else {
			if ((bus_addr_t)curaddr == lastaddr &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			     (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     ((bus_addr_t)curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr =
				    (*t->_pa_to_device)(curaddr);
				map->dm_segs[seg].ds_len = sgsize;
				map->dm_segs[seg]._ds_paddr = curaddr;
				map->dm_segs[seg]._ds_vaddr = vaddr;
			}
		}

		lastaddr = (bus_addr_t)curaddr + sgsize;
		vaddr += sgsize;
		buflen -= sgsize;
	}

	*segp = seg;
	*lastaddrp = lastaddr;

	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */

	return (0);
}

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
int
_dmamem_alloc_range(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags, paddr_t low, paddr_t high)
{
	paddr_t curaddr, lastaddr;
	vm_page_t m;
	struct pglist mlist;
	int curseg, error, plaflag;

	/* Always round the size. */
	size = round_page(size);

	/*
	 * Allocate pages from the VM system.
	 */
	plaflag = flags & BUS_DMA_NOWAIT ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & BUS_DMA_ZERO)
		plaflag |= UVM_PLA_ZERO;

	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, low, high, alignment, boundary,
	    &mlist, nsegs, plaflag);
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = TAILQ_FIRST(&mlist);
	curseg = 0;
	lastaddr = segs[curseg].ds_addr =
	    (*t->_pa_to_device)(VM_PAGE_TO_PHYS(m));
	segs[curseg].ds_len = PAGE_SIZE;
	m = TAILQ_NEXT(m, pageq);

	for (; m != NULL; m = TAILQ_NEXT(m, pageq)) {
		curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
		if (curaddr < low || curaddr >= high) {
			printf("vm_page_alloc_memory returned non-sensical"
			    " address 0x%lx\n", curaddr);
			panic("_dmamem_alloc_range");
		}
#endif
		curaddr = (*t->_pa_to_device)(curaddr);
		if (curaddr == (lastaddr + PAGE_SIZE))
			segs[curseg].ds_len += PAGE_SIZE;
		else {
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	return (0);
}
@


1.18
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.17 2014/09/26 14:32:07 jsing Exp $ */
d436 8
@


1.17
log
@Use correct format specifiers in various loongson machine dependent code.

Makes a loongson kernel buildable without -Wno-format.

ok miod@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.16 2014/09/13 16:06:36 doug Exp $ */
d467 2
a468 2
			    VM_PROT_READ | VM_PROT_WRITE, VM_PROT_READ |
			    VM_PROT_WRITE | pmap_flags);
@


1.16
log
@Replace all queue *_END macro calls except CIRCLEQ_END with NULL.

CIRCLEQ_* is deprecated and not called in the tree.  The other queue types
have *_END macros which were added for symmetry with CIRCLEQ_END.  They are
defined as NULL.  There's no reason to keep the other *_END macro calls.

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.15 2014/07/12 18:44:42 tedu Exp $ */
d575 2
a576 2
			panic("_dmapmap_load_buffer: pmap_extract(%x, %x) failed!",
			    pmap, vaddr);
@


1.15
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.14 2014/07/11 09:36:26 mpi Exp $ */
d683 1
a683 1
	for (; m != TAILQ_END(&mlist); m = TAILQ_NEXT(m, pageq)) {
@


1.14
log
@Convert bus_dmamem_map(9) to km_alloc(9) in order to make it fail and
not sleep if the allocator cannot obtain a lock when BUS_DMA_NOWAIT is
specified.

idea and inputs from kettenis@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.13 2014/03/21 21:49:45 miod Exp $ */
d120 1
a120 1
	free(map, M_DEVBUF);
@


1.13
log
@Rename the symbolic constants for the pmap-specific vm_pag pg_flags from
PV_xxx to PGF_xxx for consistency (these are not stored in pvlist entries
anymore since years). The PG_ prefix can't be used here because of name
conflicts with <machine/pte.h> names, and I'd rather not rename the pte
constants.

No functional change. But it makes my life easier.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.12 2014/03/10 21:32:15 miod Exp $ */
d435 1
d447 2
a448 1
	va = uvm_km_valloc(kernel_map, size);
d471 1
a471 1
				uvm_km_free(kernel_map, sva, ssize);
d503 1
a503 2
	size = round_page(size);
	uvm_km_free(kernel_map, (vaddr_t)kva, size);
@


1.12
log
@Support BUS_DMA_NOCACHE in bus_dma(9). Memory allocations done with
BUS_DMA_NOCACHE (or BUS_DMA_COHERENT if the platform does not have coherent
caches) will use PMAP_NOCACHE when invoking pmap_enter(), to avoid creating
cached mappings, and then evicting them from the cache.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.11 2012/10/03 21:44:51 miod Exp $ */
d483 1
a483 1
				    PV_UNCACHED);
@


1.11
log
@Do not use <mips64/archtype.h> for loongson model numbers, but rather put the
list in loongson's <machine/autoconf.h> directly. <mips64/archtype.h> is
intended to be only used on ARCBios-like platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.10 2012/04/21 12:20:30 miod Exp $ */
d391 1
a391 1
	    segs, nsegs, rsegs, flags, (vaddr_t)0, (vaddr_t)-1);
d434 1
a434 1
	int curseg, error;
d438 1
a438 1
		if (flags & BUS_DMA_COHERENT)
d454 3
d466 1
a466 1
			    VM_PROT_WRITE | PMAP_WIRED | PMAP_CANFAIL);
d473 9
a481 1
			if (flags & BUS_DMA_COHERENT)
d648 1
a648 1
    int flags, vaddr_t low, vaddr_t high)
d650 1
a650 1
	vaddr_t curaddr, lastaddr;
@


1.10
log
@Rework the signature of the cache handling routines again. It makes more sense
to pass both the virtual and physical addresses of the page to clean to
SyncDCachePage, which is the only routine using `Index' operations on the data
cache, which might be virtually indexed at some levels but physically indexed
at others. On the other hand, it does not make any sense to pass a physical
address to routines using `Hit' operations (and they were discarding them
anyway).

In addition to making things cleaner, this fixes sporadic userland misbehaviour
(read: SIGSGEV) on RM7000 O2 systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.9 2012/03/25 13:52:52 miod Exp $ */
a65 1
#include <mips64/archtype.h>
@


1.9
log
@Move cache handling routines related definitions to a dedicated header file,
rather than abusing <machine/cpu.h>.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.8 2012/03/15 18:57:20 miod Exp $ */
d368 1
a368 2
				Mips_IOSyncDCache(ci, vaddr, paddr,
				    ssize, cacheop);
@


1.8
log
@uncached_base was introduced early in IP27 support, since these designs use
subspaces in the CCA_NC uncached memory space. However, being coherent,
there was never a need for bus_dma to use uncached addresses.

This means that, on the only systems where uncached_base was not set to
PHYS_TO_XKPHYS(0, CCA_NC), it was never used.

Remove the variable, and replace PHYS_TO_UNCACHED() with
PHYS_TO_XKPHYS(, CCA_NC). No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.7 2011/06/23 20:44:39 ariane Exp $ */
d67 1
a309 3
#define SYNC_R 0	/* WB invalidate, WT invalidate */
#define SYNC_W 1	/* WB writeback, WT unaffected */
#define SYNC_X 2	/* WB writeback + invalidate, WT invalidate */
d355 1
a355 1
					cacheop = SYNC_X;
d357 1
a357 1
					cacheop = SYNC_W;
d360 1
a360 1
					cacheop = SYNC_R;
d362 1
a362 1
					cacheop = SYNC_R;
@


1.7
log
@Fix the error path in bus_dmamem_map.
As discussed on icb: remove the comment,
remove pmap_remove (uvm_km_free does that for us).

ok oga@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.6 2010/12/26 15:40:59 miod Exp $ */
d443 1
a443 1
			*kvap = (caddr_t)PHYS_TO_UNCACHED(pa);
@


1.6
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.5 2010/06/26 23:24:43 guenther Exp $ */
a468 4
				/*
				 * Clean up after ourselves.
				 * XXX uvm_wait on WAITOK
				 */
d470 1
a470 1
				uvm_km_free(kernel_map, va, ssize);
@


1.5
log
@Don't #include <sys/user.h> into files that don't need the stuff
it defines.  In some cases, this means pulling in uvm.h or pcb.h
instead, but most of the inclusions were just noise.  Tested on
alpha, amd64, armish, hppa, i386, macpcc, sgi, sparc64, and vax,
mostly by krw and naddy.
ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.4 2010/03/29 19:21:58 oga Exp $ */
d527 1
a527 1
		return (atop((*t->_device_to_pa)(segs[i].ds_addr) + off));
@


1.4
log
@PMAP_CANFAIL for bus_dmamem_map on all other architectures (and some
whitespace tweaks on i386 so that it matches).

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.3 2010/02/09 19:23:19 miod Exp $ */
a62 1
#include <sys/user.h>
@


1.3
log
@Less aggressive cache ops on BUS_DMASYNC_PREREAD alone (leftover from older
code before I got DMA address computation reliable).
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.2 2010/01/09 23:34:29 miod Exp $ */
d435 2
a436 1
	vaddr_t va;
d439 1
a439 1
	int curseg;
d457 2
d466 12
a477 3
			pmap_enter(pmap_kernel(), va, pa,
			    VM_PROT_READ | VM_PROT_WRITE,
			    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
a499 2
	pmap_remove(pmap_kernel(), (vaddr_t)kva, (vaddr_t)kva + size);
	pmap_update(pmap_kernel());
@


1.2
log
@Move cache information from global variables to per-cpu_info fields; this
allows processors with different cache sizes to be used.

Cache management routines now take a struct cpu_info * as first parameter.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.1.1.1 2009/12/11 17:23:29 miod Exp $ */
d363 1
a363 1
					cacheop = SYNC_X;
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.15 2009/10/14 21:26:54 miod Exp $ */
d316 1
d361 7
a367 14
			} else
#if 0
			if (op & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTREAD))
				cacheop = SYNC_R;
#else
			if (op & BUS_DMASYNC_PREREAD)
				cacheop = SYNC_X;
			else if (op & BUS_DMASYNC_POSTREAD)
				cacheop = SYNC_R;
#endif
			else
				cacheop = -1;
			if (cacheop >= 0) {
				Mips_IOSyncDCache(vaddr, paddr, ssize, cacheop);
d369 4
@


1.1.1.1
log
@By popular demand and peer pressure, check-in work in progress work to support
the Yeelong Lemote mips-based netbook. Kernel bits only for now, needs
polishing; most of this work done during h2k9 last month, although the
porting effort started earlier this year.
@
text
@@
