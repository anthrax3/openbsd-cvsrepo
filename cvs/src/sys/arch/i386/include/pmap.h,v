head	1.83;
access;
symbols
	OPENBSD_6_2:1.83.0.6
	OPENBSD_6_2_BASE:1.83
	OPENBSD_6_1:1.83.0.4
	OPENBSD_6_1_BASE:1.83
	OPENBSD_6_0:1.82.0.2
	OPENBSD_6_0_BASE:1.82
	OPENBSD_5_9:1.79.0.2
	OPENBSD_5_9_BASE:1.79
	OPENBSD_5_8:1.76.0.4
	OPENBSD_5_8_BASE:1.76
	OPENBSD_5_7:1.71.0.2
	OPENBSD_5_7_BASE:1.71
	OPENBSD_5_6:1.65.0.4
	OPENBSD_5_6_BASE:1.65
	OPENBSD_5_5:1.64.0.4
	OPENBSD_5_5_BASE:1.64
	OPENBSD_5_4:1.62.0.2
	OPENBSD_5_4_BASE:1.62
	OPENBSD_5_3:1.59.0.8
	OPENBSD_5_3_BASE:1.59
	OPENBSD_5_2:1.59.0.6
	OPENBSD_5_2_BASE:1.59
	OPENBSD_5_1_BASE:1.59
	OPENBSD_5_1:1.59.0.4
	OPENBSD_5_0:1.59.0.2
	OPENBSD_5_0_BASE:1.59
	OPENBSD_4_9:1.56.0.2
	OPENBSD_4_9_BASE:1.56
	OPENBSD_4_8:1.55.0.2
	OPENBSD_4_8_BASE:1.55
	OPENBSD_4_7:1.54.0.2
	OPENBSD_4_7_BASE:1.54
	OPENBSD_4_6:1.52.0.4
	OPENBSD_4_6_BASE:1.52
	OPENBSD_4_5:1.51.0.2
	OPENBSD_4_5_BASE:1.51
	OPENBSD_4_4:1.48.0.4
	OPENBSD_4_4_BASE:1.48
	OPENBSD_4_3:1.48.0.2
	OPENBSD_4_3_BASE:1.48
	OPENBSD_4_2:1.47.0.2
	OPENBSD_4_2_BASE:1.47
	OPENBSD_4_1:1.43.0.2
	OPENBSD_4_1_BASE:1.43
	OPENBSD_4_0:1.42.0.2
	OPENBSD_4_0_BASE:1.42
	OPENBSD_3_9:1.41.0.2
	OPENBSD_3_9_BASE:1.41
	OPENBSD_3_8:1.37.0.4
	OPENBSD_3_8_BASE:1.37
	OPENBSD_3_7:1.37.0.2
	OPENBSD_3_7_BASE:1.37
	OPENBSD_3_6:1.36.0.2
	OPENBSD_3_6_BASE:1.36
	SMP_SYNC_A:1.34
	SMP_SYNC_B:1.34
	OPENBSD_3_5:1.33.0.4
	OPENBSD_3_5_BASE:1.33
	OPENBSD_3_4:1.33.0.2
	OPENBSD_3_4_BASE:1.33
	UBC_SYNC_A:1.32
	OPENBSD_3_3:1.29.0.2
	OPENBSD_3_3_BASE:1.29
	OPENBSD_3_2:1.28.0.2
	OPENBSD_3_2_BASE:1.28
	OPENBSD_3_1:1.26.0.2
	OPENBSD_3_1_BASE:1.26
	UBC_SYNC_B:1.28
	UBC:1.24.0.2
	UBC_BASE:1.24
	OPENBSD_3_0:1.17.0.2
	OPENBSD_3_0_BASE:1.17
	OPENBSD_2_9:1.13.0.2
	OPENBSD_2_9_BASE:1.13
	OPENBSD_2_8:1.12.0.8
	OPENBSD_2_8_BASE:1.12
	OPENBSD_2_7:1.12.0.6
	OPENBSD_2_7_BASE:1.12
	SMP:1.12.0.4
	SMP_BASE:1.12
	kame_19991208:1.12
	OPENBSD_2_6:1.12.0.2
	OPENBSD_2_6_BASE:1.12
	OPENBSD_2_5:1.10.0.2
	OPENBSD_2_5_BASE:1.10
	OPENBSD_2_4:1.9.0.2
	OPENBSD_2_4_BASE:1.9
	OPENBSD_2_3:1.8.0.6
	OPENBSD_2_3_BASE:1.8
	OPENBSD_2_2:1.8.0.4
	OPENBSD_2_2_BASE:1.8
	OPENBSD_2_1:1.8.0.2
	OPENBSD_2_1_BASE:1.8
	OPENBSD_2_0:1.6.0.2
	OPENBSD_2_0_BASE:1.6
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.83
date	2016.10.21.06.20.59;	author mlarkin;	state Exp;
branches;
next	1.82;
commitid	szRuKZ9HgqvwYLcM;

1.82
date	2016.03.15.03.17.51;	author guenther;	state Exp;
branches;
next	1.81;
commitid	hTA8iQcFPhTNwQXL;

1.81
date	2016.03.07.05.32.47;	author naddy;	state Exp;
branches;
next	1.80;
commitid	Ht3NH0pdlkYC6Nxx;

1.80
date	2016.03.03.12.41.30;	author naddy;	state Exp;
branches;
next	1.79;
commitid	Ykztt9UU7jxBEqeD;

1.79
date	2015.10.23.09.36.09;	author kettenis;	state Exp;
branches;
next	1.78;
commitid	AJyCutJ1mVHbxB6K;

1.78
date	2015.08.22.07.16.10;	author mlarkin;	state Exp;
branches;
next	1.77;
commitid	kNcQSOWLJMgUor2a;

1.77
date	2015.08.20.03.43.29;	author mlarkin;	state Exp;
branches;
next	1.76;
commitid	uhvW5z9wGAxHDgXd;

1.76
date	2015.07.02.16.14.43;	author kettenis;	state Exp;
branches;
next	1.75;
commitid	Pj63rbiQMvtKOpRg;

1.75
date	2015.04.21.00.07.51;	author mlarkin;	state Exp;
branches;
next	1.74;
commitid	wSzAe38Tgttxx9P3;

1.74
date	2015.04.12.21.37.33;	author mlarkin;	state Exp;
branches;
next	1.73;
commitid	9CHPP1SHAUmxpVnl;

1.73
date	2015.04.12.18.37.54;	author mlarkin;	state Exp;
branches;
next	1.72;
commitid	5ST94uMTezmXYdhY;

1.72
date	2015.03.13.23.23.13;	author mlarkin;	state Exp;
branches;
next	1.71;
commitid	OgmZIpBZPHlMNamP;

1.71
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.70;
commitid	eahBabNpxnDWKzqJ;

1.70
date	2015.01.09.03.43.52;	author mlarkin;	state Exp;
branches;
next	1.69;
commitid	TzpVlzYKb3Vx3GSZ;

1.69
date	2014.12.23.01.24.51;	author deraadt;	state Exp;
branches;
next	1.68;
commitid	u5ByZ7JDYGBMbD06;

1.68
date	2014.12.22.23.59.43;	author mlarkin;	state Exp;
branches;
next	1.67;
commitid	VnzXkNOPyYOCYtPk;

1.67
date	2014.12.02.18.13.10;	author tedu;	state Exp;
branches;
next	1.66;
commitid	ZYUxNRICiD9sC1vn;

1.66
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.65;
commitid	yv0ECmCdICvq576h;

1.65
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.64;
commitid	7NtJNW9udCOFtDNM;

1.64
date	2014.01.30.18.16.41;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2014.01.06.14.29.25;	author sf;	state Exp;
branches;
next	1.62;

1.62
date	2013.03.31.17.07.03;	author deraadt;	state Exp;
branches;
next	1.61;

1.61
date	2013.03.25.19.32.52;	author deraadt;	state Exp;
branches;
next	1.60;

1.60
date	2013.03.23.16.12.23;	author deraadt;	state Exp;
branches;
next	1.59;

1.59
date	2011.06.25.19.20.41;	author jsg;	state Exp;
branches;
next	1.58;

1.58
date	2011.05.07.15.27.01;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2011.03.23.16.54.35;	author pirofti;	state Exp;
branches;
next	1.56;

1.56
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2010.05.08.16.54.08;	author oga;	state Exp;
branches;
next	1.54;

1.54
date	2009.12.09.14.31.57;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2009.08.11.17.15.54;	author oga;	state Exp;
branches;
next	1.52;

1.52
date	2009.06.03.00.49.12;	author art;	state Exp;
branches;
next	1.51;

1.51
date	2009.02.05.01.13.21;	author oga;	state Exp;
branches;
next	1.50;

1.50
date	2008.12.18.14.17.28;	author kurt;	state Exp;
branches;
next	1.49;

1.49
date	2008.11.22.18.13.03;	author mikeb;	state Exp;
branches;
next	1.48;

1.48
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2007.05.29.18.18.20;	author tom;	state Exp;
branches;
next	1.46;

1.46
date	2007.05.25.15.55.27;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2007.04.26.11.31.52;	author art;	state Exp;
branches;
next	1.44;

1.44
date	2007.04.12.19.25.15;	author art;	state Exp;
branches;
next	1.43;

1.43
date	2007.02.20.21.15.01;	author tom;	state Exp;
branches;
next	1.42;

1.42
date	2006.04.27.15.37.53;	author mickey;	state Exp;
branches;
next	1.41;

1.41
date	2006.01.12.22.39.21;	author weingart;	state Exp;
branches;
next	1.40;

1.40
date	2005.11.23.16.51.28;	author mickey;	state Exp;
branches;
next	1.39;

1.39
date	2005.11.22.12.52.12;	author mickey;	state Exp;
branches;
next	1.38;

1.38
date	2005.11.14.23.50.26;	author martin;	state Exp;
branches;
next	1.37;

1.37
date	2004.12.14.16.57.22;	author hshoexer;	state Exp;
branches
	1.37.2.1
	1.37.4.1;
next	1.36;

1.36
date	2004.08.06.22.39.13;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	2004.06.13.21.49.16;	author niklas;	state Exp;
branches;
next	1.34;

1.34
date	2004.05.20.09.20.42;	author kettenis;	state Exp;
branches;
next	1.33;

1.33
date	2003.05.26.16.25.32;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2003.05.13.03.49.04;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2003.04.09.07.53.57;	author niklas;	state Exp;
branches;
next	1.30;

1.30
date	2003.04.07.06.14.30;	author niklas;	state Exp;
branches;
next	1.29;

1.29
date	2002.11.24.19.54.54;	author pb;	state Exp;
branches;
next	1.28;

1.28
date	2002.09.12.12.56.16;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2002.07.31.02.30.29;	author mickey;	state Exp;
branches;
next	1.26;

1.26
date	2002.03.14.01.26.33;	author millert;	state Exp;
branches;
next	1.25;

1.25
date	2001.12.19.08.58.05;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.12.11.17.24.34;	author art;	state Exp;
branches
	1.24.2.1;
next	1.23;

1.23
date	2001.12.11.16.35.18;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.12.08.02.24.06;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.12.04.23.22.42;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.28.16.13.28;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.11.28.15.02.58;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.08.18.20.50.18;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.08.12.17.55.56;	author mickey;	state Exp;
branches;
next	1.15;

1.15
date	2001.08.11.11.45.27;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.05.05.23.25.47;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.03.22.23.36.52;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	99.09.20.02.47.43;	author deraadt;	state Exp;
branches
	1.12.4.1;
next	1.11;

1.11
date	99.09.17.16.52.05;	author deraadt;	state Exp;
branches;
next	1.10;

1.10
date	99.02.26.10.26.58;	author art;	state Exp;
branches;
next	1.9;

1.9
date	98.04.25.20.31.35;	author mickey;	state Exp;
branches;
next	1.8;

1.8
date	97.01.07.05.37.34;	author tholo;	state Exp;
branches;
next	1.7;

1.7
date	96.10.25.11.14.16;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	96.05.26.00.06.07;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	96.05.09.10.16.47;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	96.05.07.07.22.09;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	96.03.19.21.09.27;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	96.02.28.15.03.41;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.50.35;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.50.35;	author deraadt;	state Exp;
branches;
next	;

1.12.4.1
date	2001.04.18.16.07.38;	author niklas;	state Exp;
branches;
next	1.12.4.2;

1.12.4.2
date	2001.07.04.10.16.49;	author niklas;	state Exp;
branches;
next	1.12.4.3;

1.12.4.3
date	2001.10.31.03.01.12;	author nate;	state Exp;
branches;
next	1.12.4.4;

1.12.4.4
date	2001.11.13.21.00.52;	author niklas;	state Exp;
branches;
next	1.12.4.5;

1.12.4.5
date	2001.12.05.00.39.10;	author niklas;	state Exp;
branches;
next	1.12.4.6;

1.12.4.6
date	2002.03.06.01.01.00;	author niklas;	state Exp;
branches;
next	1.12.4.7;

1.12.4.7
date	2002.03.28.10.31.04;	author niklas;	state Exp;
branches;
next	1.12.4.8;

1.12.4.8
date	2003.03.27.23.26.55;	author niklas;	state Exp;
branches;
next	1.12.4.9;

1.12.4.9
date	2003.04.05.20.43.39;	author niklas;	state Exp;
branches;
next	1.12.4.10;

1.12.4.10
date	2003.04.06.16.19.07;	author niklas;	state Exp;
branches;
next	1.12.4.11;

1.12.4.11
date	2003.04.15.03.53.47;	author niklas;	state Exp;
branches;
next	1.12.4.12;

1.12.4.12
date	2003.05.13.19.42.08;	author ho;	state Exp;
branches;
next	1.12.4.13;

1.12.4.13
date	2003.05.16.00.29.39;	author niklas;	state Exp;
branches;
next	1.12.4.14;

1.12.4.14
date	2003.06.07.11.11.37;	author ho;	state Exp;
branches;
next	1.12.4.15;

1.12.4.15
date	2004.02.20.22.19.55;	author niklas;	state Exp;
branches;
next	1.12.4.16;

1.12.4.16
date	2004.06.05.23.09.00;	author niklas;	state Exp;
branches;
next	;

1.24.2.1
date	2002.06.11.03.35.54;	author art;	state Exp;
branches;
next	1.24.2.2;

1.24.2.2
date	2002.10.29.00.28.04;	author art;	state Exp;
branches;
next	1.24.2.3;

1.24.2.3
date	2003.05.19.21.45.11;	author tedu;	state Exp;
branches;
next	;

1.37.2.1
date	2006.01.13.00.49.21;	author brad;	state Exp;
branches;
next	;

1.37.4.1
date	2006.01.13.01.56.55;	author brad;	state Exp;
branches;
next	;


desc
@@


1.83
log
@
vmm(4) for i386. Userland changes forthcoming. Note that for the time being,
i386 hosts are limited to running only i386 guests, even if the underlying
hardware supports amd64. This is a restriction I hope to lift moving forward,
but for now please don't report problems running amd64 guests on i386 hosts.

This was a straightforward port of the in-tree amd64 code plus the old rotted
tree I had from last year for i386 support. Changes included converting 64-bit
VMREAD/VMWRITE ops to 2x32-bit ops, and fixing treatment of the TSS, which
differs on i386.

ok deraadt@@
@
text
@/*	$OpenBSD: pmap.h,v 1.82 2016/03/15 03:17:51 guenther Exp $	*/
/*	$NetBSD: pmap.h,v 1.44 2000/04/24 17:18:18 thorpej Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * pmap.h: see pmap.c for the history of this pmap module.
 */

#ifndef	_MACHINE_PMAP_H_
#define	_MACHINE_PMAP_H_

#ifdef _KERNEL
#include <machine/cpufunc.h>
#include <machine/segments.h>
#endif
#include <sys/mutex.h>
#include <uvm/uvm_object.h>
#include <machine/pte.h>

#define	PDSLOT_PTE	((KERNBASE/NBPD)-2) /* 830: for recursive PDP map */
#define	PDSLOT_KERN	(KERNBASE/NBPD) /* 832: start of kernel space */
#define	PDSLOT_APTE	((unsigned)1022) /* 1022: alternative recursive slot */

/*
 * The following define determines how many PTPs should be set up for the
 * kernel by locore.s at boot time.  This should be large enough to
 * get the VM system running.  Once the VM system is running, the
 * pmap module can add more PTPs to the kernel area on demand.
 */

#ifndef	NKPTP
#define	NKPTP		8	/* 16/32MB to start */
#endif
#define	NKPTP_MIN	4	/* smallest value we allow */

/*
 * PG_AVAIL usage: we make use of the ignored bits of the PTE
 */

#define PG_W		PG_AVAIL1	/* "wired" mapping */
#define PG_PVLIST	PG_AVAIL2	/* mapping has entry on pvlist */
#define	PG_X		PG_AVAIL3	/* executable mapping */

#define PTP0_PA             (PAGE_SIZE * 3)

#ifdef _KERNEL
/*
 * pmap data structures: see pmap.c for details of locking.
 */

struct pmap;
typedef struct pmap *pmap_t;

/*
 * We maintain a list of all non-kernel pmaps.
 */

LIST_HEAD(pmap_head, pmap); /* struct pmap_head: head of a pmap list */

/*
 * The pmap structure
 *
 * Note that the pm_obj contains the reference count,
 * page list, and number of PTPs within the pmap.
 */

#define PMAP_TYPE_NORMAL	1
#define PMAP_TYPE_EPT		2
#define PMAP_TYPE_RVI		3
#define pmap_nested(pm) ((pm)->pm_type != PMAP_TYPE_NORMAL)

struct pmap {
	uint64_t pm_pdidx[4];		/* PDIEs for PAE mode */

	struct mutex pm_mtx;
	struct mutex pm_apte_mtx;

	paddr_t pm_pdirpa;		/* PA of PD (read-only after create) */
	vaddr_t pm_pdir;		/* VA of PD (lck by object lock) */
	int	pm_pdirsize;		/* PD size (4k vs 16k on PAE) */
	struct uvm_object pm_obj;	/* object (lck by object lock) */
	LIST_ENTRY(pmap) pm_list;	/* list (lck by pm_list lock) */
	struct vm_page *pm_ptphint;	/* pointer to a PTP in our pmap */
	struct pmap_statistics pm_stats;  /* pmap stats (lck by object lock) */

	vaddr_t pm_hiexec;		/* highest executable mapping */
	int pm_flags;			/* see below */

	struct segment_descriptor pm_codeseg;	/* cs descriptor for process */
	int pm_type;			/* Type of pmap this is (PMAP_TYPE_x) */
	vaddr_t pm_npt_pml4;		/* Nested paging PML4 VA */
	paddr_t pm_npt_pa;		/* Nested paging PML4 PA */
	vaddr_t pm_npt_pdpt;		/* Nested paging PDPT */
};

/*
 * For each managed physical page we maintain a list of <PMAP,VA>s
 * which it is mapped at.  The list is headed by a pv_head structure.
 * there is one pv_head per managed phys page (allocated at boot time).
 * The pv_head structure points to a list of pv_entry structures (each
 * describes one mapping).
 */

struct pv_entry {			/* locked by its list's pvh_lock */
	struct pv_entry *pv_next;	/* next entry */
	struct pmap *pv_pmap;		/* the pmap */
	vaddr_t pv_va;			/* the virtual address */
	struct vm_page *pv_ptp;		/* the vm_page of the PTP */
};
/*
 * MD flags to pmap_enter:
 */

/* to get just the pa from params to pmap_enter */
#define PMAP_PA_MASK	~((paddr_t)PAGE_MASK)
#define	PMAP_NOCACHE	0x1		/* map uncached */
#define	PMAP_WC		0x2		/* map write combining. */

/*
 * We keep mod/ref flags in struct vm_page->pg_flags.
 */
#define	PG_PMAP_MOD	PG_PMAP0
#define	PG_PMAP_REF	PG_PMAP1
#define	PG_PMAP_WC	PG_PMAP2

/*
 * pv_entrys are dynamically allocated in chunks from a single page.
 * we keep track of how many pv_entrys are in use for each page and
 * we can free pv_entry pages if needed.  There is one lock for the
 * entire allocation system.
 */

struct pv_page_info {
	TAILQ_ENTRY(pv_page) pvpi_list;
	struct pv_entry *pvpi_pvfree;
	int pvpi_nfree;
};

/*
 * number of pv_entries in a pv_page
 */

#define PVE_PER_PVPAGE ((PAGE_SIZE - sizeof(struct pv_page_info)) / \
			sizeof(struct pv_entry))

/*
 * a pv_page: where pv_entrys are allocated from
 */

struct pv_page {
	struct pv_page_info pvinfo;
	struct pv_entry pvents[PVE_PER_PVPAGE];
};

/*
 * pv_entrys are dynamically allocated in chunks from a single page.
 * we keep track of how many pv_entrys are in use for each page and
 * we can free pv_entry pages if needed.  There is one lock for the
 * entire allocation system.
 */

extern char PTD[];
extern struct pmap kernel_pmap_store; /* kernel pmap */
extern int nkptp_max;

#define PMAP_REMOVE_ALL 0
#define PMAP_REMOVE_SKIPWIRED 1

extern struct pool pmap_pv_pool;

/*
 * Macros
 */

#define	pmap_kernel()			(&kernel_pmap_store)
#define	pmap_wired_count(pmap)		((pmap)->pm_stats.wired_count)
#define	pmap_resident_count(pmap)	((pmap)->pm_stats.resident_count)
#define	pmap_update(pm)			/* nada */

#define pmap_clear_modify(pg)		pmap_clear_attrs(pg, PG_M)
#define pmap_clear_reference(pg)	pmap_clear_attrs(pg, PG_U)
#define pmap_copy(DP,SP,D,L,S)
#define pmap_is_modified(pg)		pmap_test_attrs(pg, PG_M)
#define pmap_is_referenced(pg)		pmap_test_attrs(pg, PG_U)
#define pmap_valid_entry(E) 		((E) & PG_V) /* is PDE or PTE valid? */

#define pmap_proc_iflush(p,va,len)	/* nothing */
#define pmap_unuse_final(p)		/* nothing */
#define	pmap_remove_holes(vm)		do { /* nothing */ } while (0)

/*
 * Prototypes
 */

vaddr_t pmap_tmpmap_pa(paddr_t);
void pmap_tmpunmap_pa(void);

void pmap_bootstrap(vaddr_t);
void pmap_bootstrap_pae(void);
void pmap_virtual_space(vaddr_t *, vaddr_t *);
void pmap_init(void);
struct pmap *pmap_create(void);
void pmap_destroy(struct pmap *);
void pmap_reference(struct pmap *);
void pmap_remove(struct pmap *, vaddr_t, vaddr_t);
void pmap_collect(struct pmap *);
void pmap_activate(struct proc *);
void pmap_deactivate(struct proc *);
void pmap_kenter_pa(vaddr_t, paddr_t, vm_prot_t);
void pmap_kremove(vaddr_t, vsize_t);
void pmap_zero_page(struct vm_page *);
void pmap_copy_page(struct vm_page *, struct vm_page *);
void pmap_enter_pv(struct vm_page *, struct pv_entry *,
    struct pmap *, vaddr_t, struct vm_page *);
boolean_t pmap_clear_attrs(struct vm_page *, int);
static void pmap_page_protect(struct vm_page *, vm_prot_t);
void pmap_page_remove(struct vm_page *);
static void pmap_protect(struct pmap *, vaddr_t,
    vaddr_t, vm_prot_t);
void pmap_remove(struct pmap *, vaddr_t, vaddr_t);
boolean_t pmap_test_attrs(struct vm_page *, int);
void pmap_write_protect(struct pmap *, vaddr_t,
    vaddr_t, vm_prot_t);
int pmap_exec_fixup(struct vm_map *, struct trapframe *,
    struct pcb *);
void pmap_exec_account(struct pmap *, vaddr_t, u_int32_t,
    u_int32_t);
struct pv_entry *pmap_remove_pv(struct vm_page *, struct pmap *, vaddr_t);
void pmap_apte_flush(void);
void pmap_switch(struct proc *, struct proc *);
vaddr_t reserve_dumppages(vaddr_t); /* XXX: not a pmap fn */
paddr_t vtophys(vaddr_t va);
paddr_t vtophys_pae(vaddr_t va);
int pmap_convert(struct pmap *, int);

extern u_int32_t (*pmap_pte_set_p)(vaddr_t, paddr_t, u_int32_t);
extern u_int32_t (*pmap_pte_setbits_p)(vaddr_t, u_int32_t, u_int32_t);
extern u_int32_t (*pmap_pte_bits_p)(vaddr_t);
extern paddr_t (*pmap_pte_paddr_p)(vaddr_t);
extern boolean_t (*pmap_clear_attrs_p)(struct vm_page *, int);
extern int (*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
extern boolean_t (*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *);
extern vaddr_t (*pmap_growkernel_p)(vaddr_t);
extern void (*pmap_page_remove_p)(struct vm_page *);
extern void (*pmap_do_remove_p)(struct pmap *, vaddr_t, vaddr_t, int);
extern boolean_t (*pmap_test_attrs_p)(struct vm_page *, int);
extern void (*pmap_unwire_p)(struct pmap *, vaddr_t);
extern void (*pmap_write_protect_p)(struct pmap*, vaddr_t, vaddr_t, vm_prot_t);
extern void (*pmap_pinit_pd_p)(pmap_t);
extern void (*pmap_zero_phys_p)(paddr_t);
extern boolean_t (*pmap_zero_page_uncached_p)(paddr_t);
extern void (*pmap_copy_page_p)(struct vm_page *, struct vm_page *);

u_int32_t pmap_pte_set_pae(vaddr_t, paddr_t, u_int32_t);
u_int32_t pmap_pte_setbits_pae(vaddr_t, u_int32_t, u_int32_t);
u_int32_t pmap_pte_bits_pae(vaddr_t);
paddr_t pmap_pte_paddr_pae(vaddr_t);
boolean_t pmap_clear_attrs_pae(struct vm_page *, int);
int pmap_enter_pae(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t pmap_extract_pae(pmap_t, vaddr_t, paddr_t *);
vaddr_t pmap_growkernel_pae(vaddr_t);
void pmap_page_remove_pae(struct vm_page *);
void pmap_do_remove_pae(struct pmap *, vaddr_t, vaddr_t, int);
boolean_t pmap_test_attrs_pae(struct vm_page *, int);
void pmap_unwire_pae(struct pmap *, vaddr_t);
void pmap_write_protect_pae(struct pmap *, vaddr_t, vaddr_t, vm_prot_t);
void pmap_pinit_pd_pae(pmap_t);
void pmap_zero_phys_pae(paddr_t);
boolean_t pmap_zero_page_uncached_pae(paddr_t);
void pmap_copy_page_pae(struct vm_page *, struct vm_page *);
void pae_copy_phys(paddr_t, paddr_t, int, int);

#define	pmap_pte_set		(*pmap_pte_set_p)
#define	pmap_pte_setbits	(*pmap_pte_setbits_p)
#define	pmap_pte_bits		(*pmap_pte_bits_p)
#define	pmap_pte_paddr		(*pmap_pte_paddr_p)
#define	pmap_clear_attrs	(*pmap_clear_attrs_p)
#define	pmap_page_remove	(*pmap_page_remove_p)
#define	pmap_do_remove		(*pmap_do_remove_p)
#define	pmap_test_attrs		(*pmap_test_attrs_p)
#define	pmap_unwire		(*pmap_unwire_p)
#define	pmap_write_protect	(*pmap_write_protect_p)
#define	pmap_pinit_pd		(*pmap_pinit_pd_p)
#define	pmap_zero_phys		(*pmap_zero_phys_p)
#define	pmap_zero_page_uncached	(*pmap_zero_page_uncached_p)
#define	pmap_copy_page		(*pmap_copy_page_p)

u_int32_t pmap_pte_set_86(vaddr_t, paddr_t, u_int32_t);
u_int32_t pmap_pte_setbits_86(vaddr_t, u_int32_t, u_int32_t);
u_int32_t pmap_pte_bits_86(vaddr_t);
paddr_t pmap_pte_paddr_86(vaddr_t);
boolean_t pmap_clear_attrs_86(struct vm_page *, int);
int pmap_enter_86(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t pmap_extract_86(pmap_t, vaddr_t, paddr_t *);
vaddr_t pmap_growkernel_86(vaddr_t);
void pmap_page_remove_86(struct vm_page *);
void pmap_do_remove_86(struct pmap *, vaddr_t, vaddr_t, int);
boolean_t pmap_test_attrs_86(struct vm_page *, int);
void pmap_unwire_86(struct pmap *, vaddr_t);
void pmap_write_protect_86(struct pmap *, vaddr_t, vaddr_t, vm_prot_t);
void pmap_pinit_pd_86(pmap_t);
void pmap_zero_phys_86(paddr_t);
boolean_t pmap_zero_page_uncached_86(paddr_t);
void pmap_copy_page_86(struct vm_page *, struct vm_page *);
void pmap_tlb_shootpage(struct pmap *, vaddr_t);
void pmap_tlb_shootrange(struct pmap *, vaddr_t, vaddr_t);
void pmap_tlb_shoottlb(void);
#ifdef MULTIPROCESSOR
void pmap_tlb_droppmap(struct pmap *);
void pmap_tlb_shootwait(void);
#else
#define pmap_tlb_shootwait()
#endif

void pmap_prealloc_lowmem_ptp(void);
void pmap_prealloc_lowmem_ptp_pae(void);
vaddr_t pmap_tmpmap_pa(paddr_t);
void pmap_tmpunmap_pa(void);
vaddr_t pmap_tmpmap_pa_pae(paddr_t);
void pmap_tmpunmap_pa_pae(void);


/* 
 * functions for flushing the cache for vaddrs and pages.
 * these functions are not part of the MI pmap interface and thus
 * should not be used as such.
 */
void pmap_flush_cache(vaddr_t, vsize_t);
void pmap_flush_page(paddr_t);
void pmap_flush_page_pae(paddr_t);

#define PMAP_GROWKERNEL		/* turn on pmap_growkernel interface */

/*
 * Do idle page zero'ing uncached to avoid polluting the cache.
 */
#define	PMAP_PAGEIDLEZERO(pg)	pmap_zero_page_uncached(VM_PAGE_TO_PHYS(pg))

/*
 * Inline functions
 */

/*
 * pmap_update_pg: flush one page from the TLB (or flush the whole thing
 *	if hardware doesn't support one-page flushing)
 */

#define pmap_update_pg(va)	invlpg((u_int)(va))

/*
 * pmap_update_2pg: flush two pages from the TLB
 */

#define pmap_update_2pg(va, vb) { invlpg((u_int)(va)); invlpg((u_int)(vb)); }

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => This function is a front end for pmap_page_remove/pmap_clear_attrs
 * => We only have to worry about making the page more protected.
 *	Unprotecting a page is done on-demand at fault time.
 */

__inline static void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	if ((prot & PROT_WRITE) == 0) {
		if (prot & (PROT_READ | PROT_EXEC)) {
			(void) pmap_clear_attrs(pg, PG_RW);
		} else {
			pmap_page_remove(pg);
		}
	}
}

/*
 * pmap_protect: change the protection of pages in a pmap
 *
 * => This function is a front end for pmap_remove/pmap_write_protect.
 * => We only have to worry about making the page more protected.
 *	Unprotecting a page is done on-demand at fault time.
 */

__inline static void
pmap_protect(struct pmap *pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	if ((prot & PROT_WRITE) == 0) {
		if (prot & (PROT_READ | PROT_EXEC)) {
			pmap_write_protect(pmap, sva, eva, prot);
		} else {
			pmap_remove(pmap, sva, eva);
		}
	}
}

/*
 * pmap_growkernel, pmap_enter, and pmap_extract get picked up in variuos
 * modules from both uvm_pmap.h and pmap.h. Since uvm_pmap.h defines these
 * as functions, inline them here to suppress linker warnings.
 */
__inline static vaddr_t
pmap_growkernel(vaddr_t maxkvaddr)
{
	return (*pmap_growkernel_p)(maxkvaddr);
}

__inline static int
pmap_enter(struct pmap *pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	return (*pmap_enter_p)(pmap, va, pa, prot, flags);
}

__inline static boolean_t
pmap_extract(struct pmap *pmap, vaddr_t va, paddr_t *pa)
{
	return (*pmap_extract_p)(pmap, va, pa);
}

/*
 * p m a p   i n l i n e   h e l p e r   f u n c t i o n s
 */

/*
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
 */

static __inline boolean_t
pmap_is_active(struct pmap *pmap, struct cpu_info *ci)
{
	return (pmap == pmap_kernel() || ci->ci_curpmap == pmap);
}

static __inline boolean_t
pmap_is_curpmap(struct pmap *pmap)
{
	return (pmap_is_active(pmap, curcpu()));
}

#endif /* _KERNEL */

struct pv_entry;
struct vm_page_md {
	struct mutex pv_mtx;
	struct pv_entry *pv_list;
};

#define VM_MDPAGE_INIT(pg) do {			\
	mtx_init(&(pg)->mdpage.pv_mtx, IPL_VM); \
	(pg)->mdpage.pv_list = NULL;	\
} while (0)

#endif	/* _MACHINE_PMAP_H_ */
@


1.82
log
@Burn more LDT deadwood: stop allocating one for each idle thread,
load the ldt register with the null selector (disabling use of it),
stop reloading it on every context switch, and blow away the table
itself, as well as the pcb and pmap bits that were used to track
it (making sure to keep pcb_savefpu correctly aligned).

testing naddy@@
ok kettenis@@ mpi@@ mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.81 2016/03/07 05:32:47 naddy Exp $	*/
d91 5
d114 4
d258 1
@


1.81
log
@Sync no-argument function declaration and definition by adding (void).
ok mlarkin@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.80 2016/03/03 12:41:30 naddy Exp $	*/
a108 3
	union descriptor *pm_ldt;	/* user-set LDT */
	int pm_ldt_len;			/* number of LDT entries */
	int pm_ldt_sel;			/* LDT selector */
@


1.80
log
@Remove option USER_LDT and everything depending on it.
Remove machdep.userldt sysctl.
Remove i386_[gs]et_ldt syscall stub from libi386.
Remove i386_[gs]et_ldt regression test.

ok mlarkin@@ millert@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.79 2015/10/23 09:36:09 kettenis Exp $	*/
d332 2
a333 2
void pmap_prealloc_lowmem_ptp();
void pmap_prealloc_lowmem_ptp_pae();
@


1.79
log
@Zap pv allocation abstraction layer.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.78 2015/08/22 07:16:10 mlarkin Exp $	*/
a113 3
/* pm_flags */
#define	PMF_USER_LDT	0x01	/* pmap has user-set LDT */

a222 1
void pmap_fork(struct pmap *, struct pmap *);
a455 5

#if defined(USER_LDT)
void	pmap_ldt_cleanup(struct proc *);
#define	PMAP_FORK
#endif /* USER_LDT */
@


1.78
log
@
delete some wrong comments
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.77 2015/08/20 03:43:29 mlarkin Exp $	*/
d190 1
a190 3
#define ALLOCPV_NEED	0	/* need PV now */
#define ALLOCPV_TRY	1	/* just try to allocate */
#define ALLOCPV_NONEED	2	/* don't need PV, just growing cache */
a234 1
struct pv_entry *pmap_alloc_pv(struct pmap *, int);
a236 2
void pmap_free_pv(struct pmap *, struct pv_entry *);
void pmap_free_pvs(struct pmap *, struct pv_entry *);
@


1.77
log
@
Remove an unused #include file from i386 and amd64 pmap.h

ok miod@@, millert@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.76 2015/07/02 16:14:43 kettenis Exp $	*/
a42 8

/*
 * The following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
 */
@


1.76
log
@Make the i386 pmap (almost) mpsafe by protecting the pmap itself, the pv
lists and the apte with a mutex.  Rearrange some code to avoid
sleeping/spinning with one of these locks held.  This should make
pmap_enter(9), pmap_remove(9) and pmap_page_protect(9) safe to use without
holding the kernel lock.  Unfortunately there still seems to be an issue
that causes deadlocks under pressure.  That shouldn't be an issue as
long as uvm still calls the pmap functions with the kernel lock held.

Hopefully committed this will help finding the last bugs.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.75 2015/04/21 00:07:51 mlarkin Exp $	*/
a36 1
#include <sys/mman.h>
@


1.75
log
@
Reduce differences between i386 pmap modes.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.74 2015/04/12 21:37:33 mlarkin Exp $	*/
d41 2
a43 1
#include <uvm/uvm_object.h>
d102 4
a109 1
#define	pm_lock	pm_obj.vmobjlock
d484 1
d489 1
@


1.74
log
@
Fix some KNF, spacing, and typo issues. Moving the deck chairs around to
reduce differences between PAE and no-PAE i386 pmaps.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.73 2015/04/12 18:37:54 mlarkin Exp $	*/
d113 1
a113 1
	struct	segment_descriptor pm_codeseg;	/* cs descriptor for process */
a180 1

d188 1
a188 1
extern char	PTD[];
@


1.73
log
@
Bring PAE code back to life, in a different form. This diff (via bluhm then
to deraadt, then myself) brings the PAE pmap on i386 (not touched in any
significant way for years) closer to the current non-PAE pmap and allows
us to take a big next step toward better i386 W^X in the kernel (similar to
what we did a few months ago on amd64). Unlike the original PAE pmap, this
diff will not be supporting > 4GB physical memory on i386 - this effort is
specifically geared toward providing W^X (via NX) only.

There still seems to be a bug removing certain pmap entries when PAE is
enabled, so I'm leaving PAE mode disabled for the moment until we can
figure out what is going on, but with this diff in the tree hopefully
others can help.

The pmap functions now operate through function pointers, due to the need
to support both non-PAE and PAE forms. My unscientific testing showed
less than 0.3% (a third of a percent) slowdown with this approach during
a base build.

Discussed for months with guenther, kettenis, and deraadt.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.72 2015/03/13 23:23:13 mlarkin Exp $	*/
d243 1
d246 1
@


1.72
log
@
move some deck chairs around in preparation for i386 PAE. no functional
change, just moving a few hundred lines of comments from one place to
another. Note that some of these comments are giant lies that will get
rewritten later.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.71 2015/02/15 21:34:33 miod Exp $	*/
a44 8
 * The following defines identify the slots used as described above.
 */

#define PDSLOT_PTE	((KERNBASE/NBPD)-1) /* 831: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD)	    /* 832: start of kernel space */
#define PDSLOT_APTE	((unsigned)1023) /* 1023: alternative recursive slot */

/*
d52 3
a54 6
#define PTE_BASE	((pt_entry_t *)  (PDSLOT_PTE * NBPD) )
#define APTE_BASE	((pt_entry_t *)  (PDSLOT_APTE * NBPD) )
#define PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * PAGE_SIZE)))
#define APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * PAGE_SIZE)))
#define PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define APDP_PDE	(PDP_BASE + PDSLOT_APTE)
d63 2
a64 2
#ifndef NKPTP
#define NKPTP		4	/* 16MB to start */
d66 1
a66 39
#define NKPTP_MIN	4	/* smallest value we allow */
#define NKPTP_MAX	(1024 - (KERNBASE/NBPD) - 1)
				/* largest value (-1 for APTP space) */

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 *  kvtopte: same as above (takes a KVA, but doesn't matter with this pmap)
 *  ptetov: given a pointer to a PTE, return the VA that it maps
 *  vtophys: translate a VA to the PA mapped to it
 *
 * plus alternative versions of the above
 */

#define vtopte(VA)	(PTE_BASE + atop(VA))
#define kvtopte(VA)	vtopte(VA)
#define ptetov(PT)	(ptoa(PT - PTE_BASE))
#define	vtophys(VA)	((*vtopte(VA) & PG_FRAME) | \
			 ((unsigned)(VA) & ~PG_FRAME))
#define	avtopte(VA)	(APTE_BASE + atop(VA))
#define	ptetoav(PT)	(ptoa(PT - APTE_BASE))
#define	avtophys(VA)	((*avtopte(VA) & PG_FRAME) | \
			 ((unsigned)(VA) & ~PG_FRAME))

/*
 * PTP macros:
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
 *
 * Note that PAGE_SIZE == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
 */

#define ptp_i2o(I)	((I) * PAGE_SIZE)	/* index => offset */
#define ptp_o2i(O)	((O) / PAGE_SIZE)	/* offset => index */
#define ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */
d76 2
d100 4
a106 2
	pd_entry_t *pm_pdir;		/* VA of PD (lck by object lock) */
	paddr_t pm_pdirpa;		/* PA of PD (read-only after create) */
d181 1
d183 4
a186 1
 * global kernel variables
d189 10
a198 8
extern pd_entry_t	PTD[];

/* PTDpaddr: is the physical address of the kernel's PDP */
extern u_int32_t PTDpaddr;

extern struct pmap kernel_pmap_store;	/* kernel pmap */
extern int nkpde;			/* current # of PDEs for kernel */
extern int pmap_pg_g;			/* do we support PG_G? */
a219 1

d224 2
a225 13
void		pmap_bootstrap(vaddr_t);
boolean_t	pmap_clear_attrs(struct vm_page *, int);
static void	pmap_page_protect(struct vm_page *, vm_prot_t);
void		pmap_page_remove(struct vm_page *);
static void	pmap_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
void		pmap_remove(struct pmap *, vaddr_t, vaddr_t);
boolean_t	pmap_test_attrs(struct vm_page *, int);
void		pmap_write_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
int		pmap_exec_fixup(struct vm_map *, struct trapframe *,
		    struct pcb *);
void		pmap_switch(struct proc *, struct proc *);
d227 35
d263 2
d266 72
a337 3
void	pmap_tlb_shootpage(struct pmap *, vaddr_t);
void	pmap_tlb_shootrange(struct pmap *, vaddr_t, vaddr_t);
void	pmap_tlb_shoottlb(void);
d339 2
a340 2
void	pmap_tlb_droppmap(struct pmap *);
void	pmap_tlb_shootwait(void);
d345 7
a351 1
void	pmap_prealloc_lowmem_ptp(paddr_t);
d358 3
a360 2
void	pmap_flush_cache(vaddr_t, vsize_t);
void	pmap_flush_page(paddr_t);
a366 1
boolean_t	pmap_zero_page_uncached(paddr_t);
d425 43
@


1.71
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.70 2015/01/09 03:43:52 mlarkin Exp $	*/
a42 96

/*
 * See pte.h for a description of i386 MMU terminology and hardware
 * interface.
 *
 * A pmap describes a process' 4GB virtual address space.  This
 * virtual address space can be broken up into 1024 4MB regions which
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
 *
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
 *
 * PDE#s	VA range		Usage
 * 0->831	0x0 -> 0xcfc00000	user address space, note that the
 *					max user address is 0xcfbfe000
 *					the final two pages in the last 4MB
 *					used to be reserved for the UAREA
 *					but now are no longer used.
 * 831		0xcfc00000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 832->1023	0xd0000000->		kernel address space (constant
 *			0xffc00000	across all pmaps/processes).
 * 1023		0xffc00000->		"alternate" recursive PDP mapping
 *			<end>		(for other pmaps).
 *
 *
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
 * words, the PTE for page 0 is the first int mapped into the 4MB recursive
 * area.  The PTE for page 1 is the second int.  The very last int in the
 * 4MB range is the PTE that maps VA 0xffffe000 (the last page in a 4GB
 * address).
 *
 * All pmaps' PDs must have the same values in slots 832->1023 so that
 * the kernel is always mapped in every process.  These values are loaded
 * into the PD at pmap creation time.
 *
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
 * will have all its PTEs mapped into memory at the recursive mapping
 * point (slot #831 as show above).  When the pmap code wants to find the
 * PTE for a virtual address, all it has to do is the following:
 *
 * Address of PTE = (831 * 4MB) + (VA / PAGE_SIZE) * sizeof(pt_entry_t)
 *                = 0xcfc00000 + (VA / 4096) * 4
 *
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
 * case we take the PA of the PDP of non-active pmap and put it in
 * slot 1023 of the active pmap.  This causes the non-active pmap's
 * PTEs to get mapped in the final 4MB of the 4GB address space
 * (e.g. starting at 0xffc00000).
 *
 * The following figure shows the effects of the recursive PDP mapping:
 *
 *   PDP (%cr3)
 *   +----+
 *   |   0| -> PTP#0 that maps VA 0x0 -> 0x400000
 *   |    |
 *   |    |
 *   | 831| -> points back to PDP (%cr3) mapping VA 0xcfc00000 -> 0xd0000000
 *   | 832| -> first kernel PTP (maps 0xd0000000 -> 0xe0400000)
 *   |    |
 *   |1023| -> points to alternate pmap's PDP (maps 0xffc00000 -> end)
 *   +----+
 *
 * Note that the PDE#831 VA (0xcfc00000) is defined as "PTE_BASE".
 * Note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE".
 *
 * Starting at VA 0xcfc00000 the current active PDP (%cr3) acts as a
 * PTP:
 *
 * PTP#831 == PDP(%cr3) => maps VA 0xcfc00000 -> 0xd0000000
 *   +----+
 *   |   0| -> maps the contents of PTP#0 at VA 0xcfc00000->0xcfc01000
 *   |    |
 *   |    |
 *   | 831| -> maps the contents of PTP#831 (the PDP) at VA 0xcff3f000
 *   | 832| -> maps the contents of first kernel PTP
 *   |    |
 *   |1023|
 *   +----+
 *
 * Note that mapping of the PDP at PTP#831's VA (0xcff3f000) is
 * defined as "PDP_BASE".... within that mapping there are two
 * defines:
 *   "PDP_PDE" (0xcff3fcfc) is the VA of the PDE in the PDP
 *      which points back to itself.
 *   "APDP_PDE" (0xcff3fffc) is the VA of the PDE in the PDP which
 *      establishes the recursive mapping of the alternate pmap.
 *      To set the alternate PDP, one just has to put the correct
 *	PA info in *APDP_PDE.
 *
 * Note that in the APTE_BASE space, the APDP appears at VA
 * "APDP_BASE" (0xfffff000).
 */
@


1.70
log
@
Cleanup some macros and #defines in i386 pmap. Previously committed and
backed out because of libkvm breakage, recommitting now with libkvm fix.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.69 2014/12/23 01:24:51 deraadt Exp $	*/
d353 1
a353 1
#define	pmap_remove_holes(map)		do { /* nothing */ } while (0)
@


1.69
log
@backout previous, because libkvm needs two pieces.  will let mike
find a different way.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.67 2014/12/02 18:13:10 tedu Exp $	*/
a196 6

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define	pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define	ptei(VA)	(((VA) & PT_MASK) >> PAGE_SHIFT)
@


1.68
log
@Move PD_MASK, PT_MASK and a couple macros into pmap.c. The only other
user of these was hibernate, which now gets its own PD_MASK (since
the resume time PD_MASK is essentially disjoint from the runtime
PD_MASK). No functional change, just moving the deck chairs around in
preparation for an upcoming change.

ok deraadt
@
text
@d199 6
@


1.67
log
@delete all the simplelocks. ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.66 2014/11/16 12:30:57 deraadt Exp $	*/
a196 6

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define	pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define	ptei(VA)	(((VA) & PT_MASK) >> PAGE_SHIFT)
@


1.66
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.65 2014/07/11 16:35:40 jsg Exp $	*/
d244 1
a244 1
 * Note that the pm_obj contains the simple_lock, the reference count,
@


1.65
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.64 2014/01/30 18:16:41 miod Exp $	*/
d37 1
d439 2
a440 2
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
d459 2
a460 2
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
@


1.64
log
@Move declaration of struct vm_page_md from <machine/vmparam.h> to
<machine/pmap.h> where it belongs, and compensate in <uvm/uvm_extern.h>
by including <uvm/uvm_pmap.h> before <uvm/uvm_page.h>. Tested on all
MACHINE_ARCH but amd64 and i386 (and hppa64).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.63 2014/01/06 14:29:25 sf Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgment:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.63
log
@Increase NPTECL, as cache-lines are 64-bytes nowadays.
Also move it from pmap.h to pmap.c because it is an internal detail.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.62 2013/03/31 17:07:03 deraadt Exp $	*/
d480 10
@


1.62
log
@try to avoid pulling in pte.h and other more crazy things.  Checked against
the things that libkvm needs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.61 2013/03/25 19:32:52 deraadt Exp $	*/
a231 6

/*
 * Number of PTE's per cache line.  4 byte pte, 32-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL			8
@


1.61
log
@PGSHIFT -> PAGE_SHIFT
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.60 2013/03/23 16:12:23 deraadt Exp $	*/
d43 1
d45 2
a47 1
#include <machine/segments.h>
@


1.60
log
@refactor sys/param.h and machine/param.h.  A lot of #ifdef _KERNEL is added
to keep definitions our of user space.  The MD files now follow a consistant
order -- all namespace intrusion is at the tail can be cleaned up
independently.  locore, bootblocks, and libkvm still see enough visibility to
build.  Checked on 90% of platforms...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.59 2011/06/25 19:20:41 jsg Exp $	*/
d206 1
a206 1
#define	ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)
@


1.59
log
@ansi, no binary change
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.58 2011/05/07 15:27:01 oga Exp $	*/
d90 1
a90 1
 * Address of PTE = (831 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
d162 2
a163 2
#define PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
d214 1
a214 1
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
d218 2
a219 2
#define ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define ptp_o2i(O)	((O) / NBPG)	/* offset => index */
a323 1
 * (note: won't work on systems where NPBG isn't a constant)
d326 1
a326 1
#define PVE_PER_PVPAGE ((NBPG - sizeof(struct pv_page_info)) / \
@


1.58
log
@So long, uvm_pglist.h

This header defined three thing. two of which are unused throughout the tree,
the final one was the definition of the pagq head type, move that to uvm_page.h
and nuke the header

ok thib@@. Thanks to krw@@ for testing the hppa build for me.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.57 2011/03/23 16:54:35 pirofti Exp $	*/
d448 1
a448 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
d468 1
a468 4
pmap_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
@


1.57
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.56 2010/12/26 15:40:59 miod Exp $	*/
a45 1
#include <uvm/uvm_pglist.h>
@


1.56
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.55 2010/05/08 16:54:08 oga Exp $	*/
d40 2
a41 2
#ifndef	_I386_PMAP_H_
#define	_I386_PMAP_H_
d491 1
a491 1
#endif	/* _I386_PMAP_H_ */
@


1.55
log
@Page Attribute Tables (PAT) support for x86.

PAT allows setting per-mapping cachability bits. Our main interest in it
for write combining mappings so we do not have to rely so heaviliy on
mtrrs (which are stupidly set up on more and more machines). MD flags to
pmap allow setting these bits (which bus_space now uses for PREFETCHABLE
maps), if a vm page has a bit set, then we will use WC for all mappings
of a page (used for userland mappings). We also check for known errata
and fall back to UC- mappings in that case.

comments from kettenis@@, tedu@@ and william@@. kettenis@@, tedu@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.54 2009/12/09 14:31:57 oga Exp $	*/
a366 1
#define pmap_phys_address(ppn)		ptoa(ppn)
@


1.54
log
@add two new MD only pmap apis to amd64 and i386 (not to be used in MI
code):

pmap_flush_cache(vaddr_t, vsize_t) and pmap_flush_page(paddr_t) to flush
the cache for virtual addresses and physical pages respectively using
the clflush instruction. These apis will shortly be used by the agp
bus_dma functions to avoid doing a wbinvd on each dmamap_sync.

ok kettenis@@, some comments from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.53 2009/08/11 17:15:54 oga Exp $	*/
d301 1
d306 1
a306 1
#define PG_PMAP_MOD	PG_PMAP0
d308 1
@


1.53
log
@fix some stupidity in x86 bus_space_map.

right now, we do a pmap_kenter_pa(), we then get the pte (behind pmap's
back) and check for the cache inhibit bit (if needed). If it isn't what
we want (this is the normal case) then we change it ourselves, and do a
manual tlb shootdown (i386 was a bit more stupid about it than amd64,
too).

Instead, make it so that like on some other archs (sparc64 comes to
mind) you can pass in flags in the low bits of the physical address,
pmap then does everything correctly for you.

Discovered this when I had some code doing a lot of bus_space_maps(), it
was incredibly slow, and profilling was dominated by
pmap_tlb_shootwait();

discussed with kettenis@@, miod@@, toby@@ and art@@.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.52 2009/06/03 00:49:12 art Exp $	*/
d404 8
@


1.52
log
@Just like on amd64. Instead of keeping a bitmap of which cpus a pmap
is active on, save a curpmap pointer in cpu_info. This lets us simplify
a few things and do lazy context switching from a user process to a
kernel thread. There's a new IPI introduced for forcing a cr3 reload
when we're tearing down a dead pmap.

kettenis@@ ok (after I polished a few minor things)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.51 2009/02/05 01:13:21 oga Exp $	*/
d297 4
a300 1
#define	PMAP_NOCACHE	PMAP_MD0
@


1.51
log
@add MD PMAP_NOCACHE flag to i386 and use it to implement the
BUS_DMA_NOCACHE flag with guarantees that the dma memory will be mapped
uncached. Some broken/odd hardware needs this.

discussion with miod, toby, art and kettenis. ok miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.50 2008/12/18 14:17:28 kurt Exp $	*/
a274 1
	uint32_t pm_cpus;		/* mask of CPUs using map */
d386 1
d394 1
@


1.50
log
@Don't set the global bit PG_G for kernel pmap low memory mappings. Use a
new function pmap_prealloc_lowmem_ptp() to setup kernel pmap ptp 0 without
the PG_G bit set. This fixes the remaining reaper -> pmap_page_remove
panics. With much diagnostic help from Art and Theo.

ok  deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.49 2008/11/22 18:13:03 mikeb Exp $	*/
d295 4
@


1.49
log
@Allow wired memory to be accounted on i386.  This automatically changes
the policy for the mlock(2) on this architecture: all users are allowed
to call mlock, while the limit is imposed by the current RLIMIT_MEMLOCK
value.

"makes sense" art, ok hshoexer (who was running with the same change for
about 10 months).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.48 2007/09/10 18:49:45 miod Exp $	*/
d394 2
@


1.48
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.47 2007/05/29 18:18:20 tom Exp $	*/
d350 1
@


1.47
log
@Remove support for 80386 processors.  Apologies if you have one of
the rare 80386-bases system with enough memory, a 387 FPU, a useable
disk subsystem, and the patience to wait for it to unpack the
distribution .tgz files.

approval from art@@ and many others (esp. nick@@); ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.46 2007/05/25 15:55:27 art Exp $	*/
d363 1
@


1.46
log
@Replace the overdesigned and overcomplicated tlb shootdown code with
very simple and dumb fast tlb IPI handlers that have in the order of
the same amount of instructions as the old code had function calls.

All TLB shootdowns are reorganized so that we always shoot the,
without looking at PG_U and when we're shooting a range (primarily in
pmap_remove), we shoot the range when there are 32 or less pages in
it, otherwise we just nuke the whole TLB (this might need tweaking if
someone is interested in micro-optimization). The IPIs are not handled
through the normal interrupt vectoring code, they are not blockable
and they only shoot one page or a range of pages or the whole tlb.

This gives a 15% reduction in system time on my dual-core laptop
during a kernel compile and an 18% reduction in real time on a quad
machine doing bulk ports build.

Tested by many, in snaps for a week, no slowdowns reported (although not
everyone is seeing such huge wins).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.45 2007/04/26 11:31:52 art Exp $	*/
a376 2
static void	pmap_update_pg(vaddr_t);
static void	pmap_update_2pg(vaddr_t,vaddr_t);
d410 1
a410 11
__inline static void
pmap_update_pg(va)
	vaddr_t va;
{
#if defined(I386_CPU)
	if (cpu_class == CPUCLASS_386)
		tlbflush();
	else
#endif
		invlpg((u_int) va);
}
d416 1
a416 14
__inline static void
pmap_update_2pg(va, vb)
	vaddr_t va, vb;
{
#if defined(I386_CPU)
	if (cpu_class == CPUCLASS_386)
		tlbflush();
	else
#endif
	{
		invlpg((u_int) va);
		invlpg((u_int) vb);
	}
}
@


1.45
log
@Switch i386 pmap to VM_PAGE_MD. We store the MOD/REF flags in
pg_flags, so we actually shave quite a few bytes from the memory
we eat at boot. (a machine with 1GB memory saves 256k).

deraadt@@, pedro@@, krw@@ ok. Lots of testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.44 2007/04/12 19:25:15 art Exp $	*/
d386 8
a393 3
void	pmap_tlb_shootdown(pmap_t, vaddr_t, pt_entry_t, int32_t *);
void	pmap_tlb_shootnow(int32_t);
void	pmap_do_tlb_shootdown(struct cpu_info *);
@


1.44
log
@untypo
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.43 2007/02/20 21:15:01 tom Exp $	*/
a288 7
struct pv_entry;

struct pv_head {
	struct simplelock pvh_lock;	/* locks every pv on this list */
	struct pv_entry *pvh_list;	/* head of list (locked by pvh_lock) */
};

d297 6
d353 2
a354 2
#define pmap_clear_modify(pg)		pmap_change_attrs(pg, 0, PG_M)
#define pmap_clear_reference(pg)	pmap_change_attrs(pg, 0, PG_U)
d370 1
a370 1
boolean_t	pmap_change_attrs(struct vm_page *, int, int);
d442 1
a442 1
 * => This function is a front end for pmap_page_remove/pmap_change_attrs
d454 1
a454 1
			(void) pmap_change_attrs(pg, PG_RO, PG_RW);
@


1.43
log
@Revert PAE pmap for now, until the strange bug is found.  This stops
the freezes many of us are seeing (especially on amd64 machines running
OpenBSD/i386).

Much testing by nick@@ (as always - thanks!), hugh@@, ian@@, kettenis@@
and Sam Smith (s (at) msmith (dot) net).

Requested by, input from, and ok deraadt@@  ok art@@, kettenis@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.41 2006/01/12 22:39:21 weingart Exp $	*/
d275 1
a275 1
	uint32_t pm_cpus;		/* mask oc CPUs using map */
@


1.42
log
@implement separate PAE pmap that allows access to 64g of physmem
if supported by the cpu(s). currently not enabled by default and
not compiled into ramdisks. this grows paddr_t to 64bit but yet
leaves bus_addr_t at 32bits. measures are taken to favour dmaable
memory allocation from below 4g line such that buffer cache is
already allocated form below, pool backend allocator prefers lower
memory and then finally bounce buffers are used as last resort.
PAE is engaged only if global variable cpu_pae is manually set
to non-zero and there is physical memory present above 4g.
simplify pcibios address math to use u_long as we always will
be in the 32bit space.
@
text
@d50 93
a142 1
 * The following defines identify the slots used as described in pmap.c .
d144 23
a166 3
#define PDSLOT_PTE	((KERNBASE/NBPD)-2) /* 830: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD) /* 832: start of kernel space */
#define PDSLOT_APTE	((unsigned)1022) /* 1022: alternative recursive slot */
d174 1
d176 1
a176 1
#define NKPTP		8	/* 16/32MB to start */
d179 44
d232 6
a259 4
	paddr_t pm_pdidx[4];		/* PDIEs for PAE mode */
	paddr_t pm_pdirpa;		/* PA of PD (read-only after create) */
	vaddr_t pm_pdir;		/* VA of PD (lck by object lock) */
	int	pm_pdirsize;		/* PD size (4k vs 16k on pae */
d263 2
d266 1
a266 1
	struct pmap_statistics pm_stats;/* pmap stats (lck by object lock) */
d336 6
a341 1
extern char	PTD[];
d343 2
a344 1
extern int nkptp_max;
d347 1
a347 2
 * Our dual-pmap design requires to play a pointer-and-seek.
 * Although being nice folks we are handle single-pmap kernels special.
a348 1
#define	PMAP_EXCLUDE_DECLS	/* tells uvm_pmap.h *not* to include decls */  
a349 3
/*
 * Dumb macros
 */
d354 10
a363 7
#define	pmap_clear_modify(pg)		pmap_change_attrs(pg, 0, PG_M)
#define	pmap_clear_reference(pg)	pmap_change_attrs(pg, 0, PG_U)
#define	pmap_copy(DP,SP,D,L,S)		/* nicht */
#define	pmap_is_modified(pg)		pmap_test_attrs(pg, PG_M)
#define	pmap_is_referenced(pg)		pmap_test_attrs(pg, PG_U)
#define	pmap_phys_address(ppn)		ptoa(ppn)
#define	pmap_valid_entry(E)		((E) & PG_V) /* is PDE or PTE valid? */
a364 2
#define	pmap_proc_iflush(p,va,len)	/* nothing */
#define	pmap_unuse_final(p)		/* 4anaEB u nycToTa */
d369 1
d371 1
a371 22
void		pmap_bootstrap_pae(void);
void		pmap_virtual_space(vaddr_t *, vaddr_t *);
void		pmap_init(void);
struct pmap *	pmap_create(void);
void		pmap_destroy(struct pmap *);
void		pmap_reference(struct pmap *);
void		pmap_fork(struct pmap *, struct pmap *);
void		pmap_collect(struct pmap *);
void		pmap_activate(struct proc *);
void		pmap_deactivate(struct proc *);
void		pmap_kenter_pa(vaddr_t, paddr_t, vm_prot_t);
void		pmap_kremove(vaddr_t, vsize_t);
void		pmap_zero_page(struct vm_page *);
void		pmap_copy_page(struct vm_page *, struct vm_page *);

struct pv_entry*pmap_alloc_pv(struct pmap *, int);
void		pmap_enter_pv(struct pv_head *, struct pv_entry *,
		    struct pmap *, vaddr_t, struct vm_page *);
void		pmap_free_pv(struct pmap *, struct pv_entry *);
void		pmap_free_pvs(struct pmap *, struct pv_entry *);
void		pmap_free_pv_doit(struct pv_entry *);
void		pmap_free_pvpage(void);
d373 5
a377 1
static void	pmap_protect(struct pmap *, vaddr_t, vaddr_t, vm_prot_t);
d379 3
a381 1
static void	pmap_update_2pg(vaddr_t, vaddr_t);
a383 2
void		pmap_exec_account(struct pmap *, vaddr_t, u_int32_t,
		    u_int32_t);
a385 1
paddr_t	vtophys(vaddr_t va);
d387 1
a387 1
void	pmap_tlb_shootdown(pmap_t, vaddr_t, u_int32_t, int32_t *);
a389 104
boolean_t pmap_is_curpmap(struct pmap *);
boolean_t pmap_is_active(struct pmap *, int);
void	pmap_apte_flush(struct pmap *);
struct pv_entry *pmap_remove_pv(struct pv_head *, struct pmap *, vaddr_t);

#ifdef SMALL_KERNEL
#define	pmap_pte_set_86		pmap_pte_set
#define	pmap_pte_setbits_86	pmap_pte_setbits
#define	pmap_pte_bits_86	pmap_pte_bits
#define	pmap_pte_paddr_86	pmap_pte_paddr
#define	pmap_change_attrs_86	pmap_change_attrs
#define	pmap_enter_86		pmap_enter
#define	pmap_extract_86		pmap_extract
#define	pmap_growkernel_86	pmap_growkernel
#define	pmap_page_remove_86	pmap_page_remove
#define	pmap_remove_86		pmap_remove
#define	pmap_test_attrs_86	pmap_test_attrs
#define	pmap_unwire_86		pmap_unwire
#define	pmap_write_protect_86	pmap_write_protect
#define	pmap_pinit_pd_86	pmap_pinit_pd
#define	pmap_zero_phys_86	pmap_zero_phys
#define	pmap_zero_page_uncached_86	pmap_zero_page_uncached
#define	pmap_copy_page_86	pmap_copy_page
#define	pmap_try_steal_pv_86	pmap_try_steal_pv
#else
extern u_int32_t (*pmap_pte_set_p)(vaddr_t, paddr_t, u_int32_t);
extern u_int32_t (*pmap_pte_setbits_p)(vaddr_t, u_int32_t, u_int32_t);
extern u_int32_t (*pmap_pte_bits_p)(vaddr_t);
extern paddr_t	(*pmap_pte_paddr_p)(vaddr_t);
extern boolean_t (*pmap_change_attrs_p)(struct vm_page *, int, int);
extern int	(*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
extern boolean_t (*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *);
extern vaddr_t	(*pmap_growkernel_p)(vaddr_t);
extern void	(*pmap_page_remove_p)(struct vm_page *);
extern void	(*pmap_remove_p)(struct pmap *, vaddr_t, vaddr_t);
extern boolean_t (*pmap_test_attrs_p)(struct vm_page *, int);
extern void	(*pmap_unwire_p)(struct pmap *, vaddr_t);
extern void (*pmap_write_protect_p)(struct pmap*, vaddr_t, vaddr_t, vm_prot_t);
extern void	(*pmap_pinit_pd_p)(pmap_t);
extern void	(*pmap_zero_phys_p)(paddr_t);
extern boolean_t (*pmap_zero_page_uncached_p)(paddr_t);
extern void	(*pmap_copy_page_p)(struct vm_page *, struct vm_page *);
extern boolean_t (*pmap_try_steal_pv_p)(struct pv_head *pvh,
		     struct pv_entry *cpv, struct pv_entry *prevpv);

u_int32_t pmap_pte_set_pae(vaddr_t, paddr_t, u_int32_t);
u_int32_t pmap_pte_setbits_pae(vaddr_t, u_int32_t, u_int32_t);
u_int32_t pmap_pte_bits_pae(vaddr_t);
paddr_t	pmap_pte_paddr_pae(vaddr_t);
boolean_t pmap_try_steal_pv_pae(struct pv_head *pvh, struct pv_entry *cpv,
	    struct pv_entry *prevpv);
boolean_t pmap_change_attrs_pae(struct vm_page *, int, int);
int	pmap_enter_pae(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t pmap_extract_pae(pmap_t, vaddr_t, paddr_t *);
vaddr_t	pmap_growkernel_pae(vaddr_t);
void	pmap_page_remove_pae(struct vm_page *);
void	pmap_remove_pae(struct pmap *, vaddr_t, vaddr_t);
boolean_t pmap_test_attrs_pae(struct vm_page *, int);
void	pmap_unwire_pae(struct pmap *, vaddr_t);
void	pmap_write_protect_pae(struct pmap *, vaddr_t, vaddr_t, vm_prot_t);
void	pmap_pinit_pd_pae(pmap_t);
void	pmap_zero_phys_pae(paddr_t);
boolean_t pmap_zero_page_uncached_pae(paddr_t);
void	pmap_copy_page_pae(struct vm_page *, struct vm_page *);

#define	pmap_pte_set		(*pmap_pte_set_p)
#define	pmap_pte_setbits	(*pmap_pte_setbits_p)
#define	pmap_pte_bits		(*pmap_pte_bits_p)
#define	pmap_pte_paddr		(*pmap_pte_paddr_p)
#define	pmap_change_attrs	(*pmap_change_attrs_p)
#define	pmap_enter		(*pmap_enter_p)
#define	pmap_extract		(*pmap_extract_p)
#define	pmap_growkernel		(*pmap_growkernel_p)
#define	pmap_page_remove	(*pmap_page_remove_p)
#define	pmap_remove		(*pmap_remove_p)
#define	pmap_test_attrs		(*pmap_test_attrs_p)
#define	pmap_unwire		(*pmap_unwire_p)
#define	pmap_write_protect	(*pmap_write_protect_p)
#define	pmap_pinit_pd		(*pmap_pinit_pd_p)
#define	pmap_zero_phys		(*pmap_zero_phys_p)
#define	pmap_zero_page_uncached	(*pmap_zero_page_uncached_p)
#define	pmap_copy_page		(*pmap_copy_page_p)
#define	pmap_try_steal_pv	(*pmap_try_steal_pv_p)
#endif

u_int32_t pmap_pte_set_86(vaddr_t, paddr_t, u_int32_t);
u_int32_t pmap_pte_setbits_86(vaddr_t, u_int32_t, u_int32_t);
u_int32_t pmap_pte_bits_86(vaddr_t);
paddr_t	pmap_pte_paddr_86(vaddr_t);
boolean_t pmap_try_steal_pv_86(struct pv_head *pvh, struct pv_entry *cpv,
	    struct pv_entry *prevpv);
boolean_t pmap_change_attrs_86(struct vm_page *, int, int);
int	pmap_enter_86(pmap_t, vaddr_t, paddr_t, vm_prot_t, int);
boolean_t pmap_extract_86(pmap_t, vaddr_t, paddr_t *);
vaddr_t	pmap_growkernel_86(vaddr_t);
void	pmap_page_remove_86(struct vm_page *);
void	pmap_remove_86(struct pmap *, vaddr_t, vaddr_t);
boolean_t pmap_test_attrs_86(struct vm_page *, int);
void	pmap_unwire_86(struct pmap *, vaddr_t);
void	pmap_write_protect_86(struct pmap *, vaddr_t, vaddr_t, vm_prot_t);
void	pmap_pinit_pd_86(pmap_t);
void	pmap_zero_phys_86(paddr_t);
boolean_t pmap_zero_page_uncached_86(paddr_t);
void	pmap_copy_page_86(struct vm_page *, struct vm_page *);
d396 1
@


1.41
log
@Move to using gdt only (no more ldt in general case) but with a variable
limit selector, so that the w^x line can float much more dynamically.
Much work done by tom.  Tested by various people.  Addresses concerns of
(Julien Tinnes) <julien ATHOST cr0.org>
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.40 2005/11/23 16:51:28 mickey Exp $	*/
d50 1
a50 93
 * See pte.h for a description of i386 MMU terminology and hardware
 * interface.
 *
 * A pmap describes a process' 4GB virtual address space.  This
 * virtual address space can be broken up into 1024 4MB regions which
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
 *
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
 *
 * PDE#s	VA range		Usage
 * 0->831	0x0 -> 0xcfc00000	user address space, note that the
 *					max user address is 0xcfbfe000
 *					the final two pages in the last 4MB
 *					used to be reserved for the UAREA
 *					but now are no longer used.
 * 831		0xcfc00000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 832->1023	0xd0000000->		kernel address space (constant
 *			0xffc00000	across all pmaps/processes).
 * 1023		0xffc00000->		"alternate" recursive PDP mapping
 *			<end>		(for other pmaps).
 *
 *
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
 * words, the PTE for page 0 is the first int mapped into the 4MB recursive
 * area.  The PTE for page 1 is the second int.  The very last int in the
 * 4MB range is the PTE that maps VA 0xffffe000 (the last page in a 4GB
 * address).
 *
 * All pmaps' PDs must have the same values in slots 832->1023 so that
 * the kernel is always mapped in every process.  These values are loaded
 * into the PD at pmap creation time.
 *
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
 * will have all its PTEs mapped into memory at the recursive mapping
 * point (slot #831 as show above).  When the pmap code wants to find the
 * PTE for a virtual address, all it has to do is the following:
 *
 * Address of PTE = (831 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xcfc00000 + (VA / 4096) * 4
 *
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
 * case we take the PA of the PDP of non-active pmap and put it in
 * slot 1023 of the active pmap.  This causes the non-active pmap's
 * PTEs to get mapped in the final 4MB of the 4GB address space
 * (e.g. starting at 0xffc00000).
 *
 * The following figure shows the effects of the recursive PDP mapping:
 *
 *   PDP (%cr3)
 *   +----+
 *   |   0| -> PTP#0 that maps VA 0x0 -> 0x400000
 *   |    |
 *   |    |
 *   | 831| -> points back to PDP (%cr3) mapping VA 0xcfc00000 -> 0xd0000000
 *   | 832| -> first kernel PTP (maps 0xd0000000 -> 0xe0400000)
 *   |    |
 *   |1023| -> points to alternate pmap's PDP (maps 0xffc00000 -> end)
 *   +----+
 *
 * Note that the PDE#831 VA (0xcfc00000) is defined as "PTE_BASE".
 * Note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE".
 *
 * Starting at VA 0xcfc00000 the current active PDP (%cr3) acts as a
 * PTP:
 *
 * PTP#831 == PDP(%cr3) => maps VA 0xcfc00000 -> 0xd0000000
 *   +----+
 *   |   0| -> maps the contents of PTP#0 at VA 0xcfc00000->0xcfc01000
 *   |    |
 *   |    |
 *   | 831| -> maps the contents of PTP#831 (the PDP) at VA 0xcff3f000
 *   | 832| -> maps the contents of first kernel PTP
 *   |    |
 *   |1023|
 *   +----+
 *
 * Note that mapping of the PDP at PTP#831's VA (0xcff3f000) is
 * defined as "PDP_BASE".... within that mapping there are two
 * defines:
 *   "PDP_PDE" (0xcff3fcfc) is the VA of the PDE in the PDP
 *      which points back to itself.
 *   "APDP_PDE" (0xcff3fffc) is the VA of the PDE in the PDP which
 *      establishes the recursive mapping of the alternate pmap.
 *      To set the alternate PDP, one just has to put the correct
 *	PA info in *APDP_PDE.
 *
 * Note that in the APTE_BASE space, the APDP appears at VA
 * "APDP_BASE" (0xfffff000).
d52 3
a54 23

/*
 * The following defines identify the slots used as described above.
 */

#define PDSLOT_PTE	((KERNBASE/NBPD)-1) /* 831: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD)	    /* 832: start of kernel space */
#define PDSLOT_APTE	((unsigned)1023) /* 1023: alternative recursive slot */

/*
 * The following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
 */

#define PTE_BASE	((pt_entry_t *)  (PDSLOT_PTE * NBPD) )
#define APTE_BASE	((pt_entry_t *)  (PDSLOT_APTE * NBPD) )
#define PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
#define PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define APDP_PDE	(PDP_BASE + PDSLOT_APTE)
a61 1

d63 1
a63 1
#define NKPTP		4	/* 16MB to start */
a65 44
#define NKPTP_MAX	(1024 - (KERNBASE/NBPD) - 1)
				/* largest value (-1 for APTP space) */

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 *  kvtopte: same as above (takes a KVA, but doesn't matter with this pmap)
 *  ptetov: given a pointer to a PTE, return the VA that it maps
 *  vtophys: translate a VA to the PA mapped to it
 *
 * plus alternative versions of the above
 */

#define vtopte(VA)	(PTE_BASE + atop(VA))
#define kvtopte(VA)	vtopte(VA)
#define ptetov(PT)	(ptoa(PT - PTE_BASE))
#define	vtophys(VA)	((*vtopte(VA) & PG_FRAME) | \
			 ((unsigned)(VA) & ~PG_FRAME))
#define	avtopte(VA)	(APTE_BASE + atop(VA))
#define	ptetoav(PT)	(ptoa(PT - APTE_BASE))
#define	avtophys(VA)	((*avtopte(VA) & PG_FRAME) | \
			 ((unsigned)(VA) & ~PG_FRAME))

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define	pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define	ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)

/*
 * PTP macros:
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
 *
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
 */

#define ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */
a74 6
/*
 * Number of PTE's per cache line.  4 byte pte, 32-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL			8

d97 4
a103 2
	pd_entry_t *pm_pdir;		/* VA of PD (lck by object lock) */
	paddr_t pm_pdirpa;		/* PA of PD (read-only after create) */
d105 1
a105 1
	struct pmap_statistics pm_stats;  /* pmap stats (lck by object lock) */
d175 1
a175 6

extern pd_entry_t	PTD[];

/* PTDpaddr: is the physical address of the kernel's PDP */
extern u_int32_t PTDpaddr;

d177 1
a177 2
extern int nkpde;			/* current # of PDEs for kernel */
extern int pmap_pg_g;			/* do we support PG_G? */
d180 2
a181 1
 * Macros
d183 1
d185 3
d192 7
a198 10
#define pmap_clear_modify(pg)		pmap_change_attrs(pg, 0, PG_M)
#define pmap_clear_reference(pg)	pmap_change_attrs(pg, 0, PG_U)
#define pmap_copy(DP,SP,D,L,S)
#define pmap_is_modified(pg)		pmap_test_attrs(pg, PG_M)
#define pmap_is_referenced(pg)		pmap_test_attrs(pg, PG_U)
#define pmap_phys_address(ppn)		ptoa(ppn)
#define pmap_valid_entry(E) 		((E) & PG_V) /* is PDE or PTE valid? */

#define pmap_proc_iflush(p,va,len)	/* nothing */
#define pmap_unuse_final(p)		/* nothing */
d200 2
a205 1

d207 22
a228 1
boolean_t	pmap_change_attrs(struct vm_page *, int, int);
d230 1
a230 5
void		pmap_page_remove(struct vm_page *);
static void	pmap_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
void		pmap_remove(struct pmap *, vaddr_t, vaddr_t);
boolean_t	pmap_test_attrs(struct vm_page *, int);
d232 1
a232 3
static void	pmap_update_2pg(vaddr_t,vaddr_t);
void		pmap_write_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
d235 2
d239 1
d241 1
a241 1
void	pmap_tlb_shootdown(pmap_t, vaddr_t, pt_entry_t, int32_t *);
d244 104
a353 1
boolean_t	pmap_zero_page_uncached(paddr_t);
@


1.40
log
@finnish the PTDPTDI and APTDPTDI conversion to PDSLOT_PTE and PDSLOT_APTE thus reducing confusion; remove compatibility defines and comments
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.39 2005/11/22 12:52:12 mickey Exp $	*/
d271 1
@


1.39
log
@pm_pdirpa is apaddr_t and PTDpaddr (on the contrary) is u_int32_t
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.38 2005/11/14 23:50:26 martin Exp $	*/
a166 7

/*
 * XXXCDC: tmp xlate from old names:
 * PTDPTDI -> PDSLOT_PTE
 * KPTDI -> PDSLOT_KERN
 * APTDPTDI -> PDSLOT_APTE
 */
@


1.38
log
@convert and remove the last traces of i386_round_page(),
i386_trunc_page(), i386_btop() and i386_ptob()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.37 2004/12/14 16:57:22 hshoexer Exp $	*/
d271 1
a271 1
	u_int32_t pm_pdirpa;		/* PA of PD (read-only after create) */
d346 1
a346 1
extern u_long PTDpaddr;
@


1.37
log
@fix a comment.

ok deraatd tdeval
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.36 2004/08/06 22:39:13 deraadt Exp $	*/
d200 1
a200 1
#define vtopte(VA)	(PTE_BASE + i386_btop(VA))
d202 1
a202 1
#define ptetov(PT)	(i386_ptob(PT - PTE_BASE))
d205 2
a206 2
#define	avtopte(VA)	(APTE_BASE + i386_btop(VA))
#define	ptetoav(PT)	(i386_ptob(PT - APTE_BASE))
d365 1
a365 1
#define pmap_phys_address(ppn)		i386_ptob(ppn)
@


1.37.4.1
log
@MFC:
Fix by weingart@@

Move to using gdt only (no more ldt in general case) but with a variable
limit selector, so that the w^x line can float much more dynamically.
Much work done by tom.  Tested by various people.  Addresses concerns of
(Julien Tinnes) <julien ATHOST cr0.org>

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.37 2004/12/14 16:57:22 hshoexer Exp $	*/
a277 1
	struct	segment_descriptor pm_codeseg;	/* cs descriptor for process */
@


1.37.2.1
log
@MFC:
Fix by weingart@@

Move to using gdt only (no more ldt in general case) but with a variable
limit selector, so that the w^x line can float much more dynamically.
Much work done by tom.  Tested by various people.  Addresses concerns of
(Julien Tinnes) <julien ATHOST cr0.org>

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.37 2004/12/14 16:57:22 hshoexer Exp $	*/
a277 1
	struct	segment_descriptor pm_codeseg;	/* cs descriptor for process */
@


1.36
log
@rename sparc kill_user_windows() to pmap_unuse_final().  provide empty stubs
on all other architectures.  remove last architecture dependent #ifdef from
uvm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.35 2004/06/13 21:49:16 niklas Exp $	*/
d117 1
a117 1
 * Starting at VA 0xdfc00000 the current active PDP (%cr3) acts as a
@


1.35
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d369 1
@


1.34
log
@Properly flush instruction cache for ptrace(PT_WRTIE_{DI}, ...) on powerpc
and m68k.
ok drahn@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.33 2003/05/26 16:25:32 art Exp $	*/
d239 6
d281 1
d340 1
a340 13
 * pmap_remove_record: a record of VAs that have been unmapped, used to
 * flush TLB.  If we have more than PMAP_RR_MAX then we stop recording.
 */

#define PMAP_RR_MAX	16	/* max of 16 pages (64K) */

struct pmap_remove_record {
	int prr_npages;
	vaddr_t prr_vas[PMAP_RR_MAX];
};

/*
 * Global kernel variables
d391 4
@


1.33
log
@pmap_protect strikes again.

Not only do we have to think about PROT_EXEC while dealing with it, but we
also have to think about wiring. sigh.

We'll have to fix this with an API change.

fixes the mlockall problem.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.32 2003/05/13 03:49:04 art Exp $	*/
d372 2
@


1.32
log
@The current solution to handle the protection fault trap is not
correct.  It breaks down if we're trying to jump through a function
pointer. The protection fault trap on i386 must be one of the most
braindead traps ever invented in the history of humankind. It doesn't
give you any information about what went wrong except the instruction
that faulted. Since the problem we're trying to deal with is a
segmentation problem, we don't get the desitination that we want to
jump to, we just get the instruction and we won't add a disassembler
to trap handling just to try to figure out what went wrong.

What we want to do is to handle this as a normal fault to let noexec
accounting in pmap_enter deal with the changes to the code
segment. Unfortunately that's impossible. We don't know the faulting
address, so we need to change how the exec accounting works. Basically
the code segment must already cover the address we want to execute
before we can fault it in.

New scheme:

 o Start with conservative code segment.

 o If we get a protection fault, go through all mappings in the process
  and find the highest executable mapping, fix up the code segment and
  record that address. If the code segment didn't change, the protection
  fault wasn't fixable - just die.

 o If the highest executable mapping is removed, just reset the code
  segment to something conservative and let the next protection fault
  deal with it.  We can't read all the vm mappings of the process from
  the pmap because of locking hell.

This should allow floating code segment whenever someone implements that.

Also, fix the pmap_protect function to behave more like the other
pmaps we have and be slightly more agressive to force more proper
protection changes.

ok:ed by various people.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.31 2003/04/09 07:53:57 niklas Exp $	*/
d480 6
a485 9
	if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) ==
	    (VM_PROT_WRITE|VM_PROT_EXECUTE))
		return;

	if ((prot & (VM_PROT_READ|VM_PROT_EXECUTE)) ==
	    (VM_PROT_READ|VM_PROT_EXECUTE)) {
		pmap_write_protect(pmap, sva, eva, prot);
	} else {
		pmap_remove(pmap, sva, eva);
@


1.31
log
@Correct commentary describing page table layouts
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.30 2003/04/07 06:14:30 niklas Exp $	*/
d269 1
a269 1
	int pm_nxpages;			/* # of executable pages on stack */
d390 2
d480 9
a488 6
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
			pmap_write_protect(pmap, sva, eva, prot);
		} else {
			pmap_remove(pmap, sva, eva);
		}
@


1.30
log
@Spring cleaning: remove unused code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.29 2002/11/24 19:54:54 pb Exp $	*/
d50 1
a50 1
 * see pte.h for a description of i386 MMU terminology and hardware
d53 1
a53 1
 * a pmap describes a processes' 4GB virtual address space.  this
d55 1
a55 1
 * are described by PDEs in the PDP.  the PDEs are defined as follows:
d57 2
a58 2
 * (ranges are inclusive -> exclusive, just like vm_map_entry start/end)
 * (the following assumes that KERNBASE is 0xc0000000)
d60 3
a62 3
 * PDE#s	VA range		usage
 * 0->767	0x0 -> 0xbfc00000	user address space, note that the
 *					max user address is 0xbfbfe000
d65 5
a69 5
 *					but now are no longer used
 * 768		0xbfc00000->		recursive mapping of PDP (used for
 *			0xc0000000	linear mapping of PTPs)
 * 768->1023	0xc0000000->		kernel address space (constant
 *			0xffc00000	across all pmap's/processes)
d71 1
a71 1
 *			<end>		(for other pmaps)
d74 2
a75 2
 * note: a recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  in other
d77 1
a77 1
 * area.  the PTE for page 1 is the second int.  the very last int in the
d81 2
a82 2
 * all pmap's PD's must have the same values in slots 768->1023 so that
 * the kernel is always mapped in every process.  these values are loaded
d85 2
a86 2
 * at any one time only one pmap can be active on a processor.  this is
 * the pmap whose PDP is pointed to by processor register %cr3.  this pmap
d88 1
a88 1
 * point (slot #767 as show above).  when the pmap code wants to find the
d91 2
a92 2
 * address of PTE = (767 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xbfc00000 + (VA / 4096) * 4
d94 2
a95 2
 * what happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  in that
d97 1
a97 1
 * slot 1023 of the active pmap.  this causes the non-active pmap's
d101 1
a101 1
 * the following figure shows the effects of the recursive PDP mapping:
d108 2
a109 2
 *   | 767| -> points back to PDP (%cr3) mapping VA 0xbfc00000 -> 0xc0000000
 *   | 768| -> first kernel PTP (maps 0xc0000000 -> 0xf0400000)
d114 2
a115 2
 * note that the PDE#767 VA (0xbfc00000) is defined as "PTE_BASE"
 * note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE"
d117 1
a117 1
 * starting at VA 0xbfc00000 the current active PDP (%cr3) acts as a
d120 1
a120 1
 * PTP#767 == PDP(%cr3) => maps VA 0xbfc00000 -> 0xc0000000
d122 1
a122 1
 *   |   0| -> maps the contents of PTP#0 at VA 0xbfc00000->0xbfc01000
d125 2
a126 2
 *   | 767| -> maps contents of PTP#767 (the PDP) at VA 0xbffbf000
 *   | 768| -> maps contents of first kernel PTP
d131 1
a131 1
 * note that mapping of the PDP at PTP#959's VA (0xeffbf000) is
d134 1
a134 1
 *   "PDP_PDE" (0xeffbfefc) is the VA of the PDE in the PDP
d136 1
a136 1
 *   "APDP_PDE" (0xeffbfffc) is the VA of the PDE in the PDP which
d138 1
a138 1
 *      to set the alternate PDP, one just has to put the correct
d141 1
a141 1
 * note that in the APTE_BASE space, the APDP appears at VA
d146 1
a146 1
 * the following defines identify the slots used as described above.
d149 2
a150 2
#define PDSLOT_PTE	((KERNBASE/NBPD)-1) /* 767: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD)	    /* 768: start of kernel space */
d154 1
a154 1
 * the following defines give the virtual addresses of various MMU
d176 3
a178 3
 * the follow define determines how many PTPs should be set up for the
 * kernel by locore.s at boot time.  this should be large enough to
 * get the VM system running.  once the VM system is running, the
d218 3
a220 3
 *   a PTP's index is the PD index of the PDE that points to it
 *   a PTP's offset is the byte-offset in the PTE space that this PTP is at
 *   a PTP's VA is the first VA mapped by that PTP
d222 1
a222 1
 * note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
d248 1
a248 1
 * we maintain a list of all non-kernel pmaps
d254 1
a254 1
 * the pmap structure
d256 1
a256 1
 * note that the pm_obj contains the simple_lock, the reference count,
d281 2
a282 2
 * for each managed physical page we maintain a list of <PMAP,VA>'s
 * which it is mapped at.  the list is headed by a pv_head structure.
d284 1
a284 1
 * the pv_head structure points to a list of pv_entry structures (each
d305 1
a305 1
 * we can free pv_entry pages if needed.  there is one lock for the
d316 1
a316 1
 * number of pv_entry's in a pv_page
d334 1
a334 1
 * flush TLB.  if we have more than PMAP_RR_MAX then we stop recording.
d345 1
a345 1
 * global kernel variables
d358 1
a358 1
 * macros
d375 1
a375 1
 * prototypes
d402 1
a402 1
 * inline functions
d445 3
a447 3
 * => this function is a frontend for pmap_page_remove/pmap_change_attrs
 * => we only have to worry about making the page more protected.
 *	unprotecting a page is done on-demand at fault time.
d467 3
a469 3
 * => this function is a frontend for pmap_remove/pmap_write_protect
 * => we only have to worry about making the page more protected.
 *	unprotecting a page is done on-demand at fault time.
@


1.29
log
@also no opt_user_ldt.h

miod@@, millert@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.28 2002/09/12 12:56:16 art Exp $	*/
a344 12
 * pmap_transfer_location: used to pass the current location in the
 * pmap between pmap_transfer and pmap_transfer_ptes [e.g. during
 * a pmap_copy].
 */

struct pmap_transfer_location {
	vaddr_t addr;			/* the address (page-aligned) */
	pt_entry_t *pte;		/* the PTE that maps address */
	struct vm_page *ptp;		/* the PTP that the PTE lives in */
};

/*
a385 2
void		pmap_transfer(struct pmap *, struct pmap *, vaddr_t,
				   vsize_t, vaddr_t, boolean_t);
@


1.28
log
@Change the PMAP_PAGEIDLEZERO api to take the struct vm_page instead of the pa.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.27 2002/07/31 02:30:29 mickey Exp $	*/
a41 4

#if defined(_KERNEL) && !defined(_LKM) && defined(__NetBSD__)
#include "opt_user_ldt.h"
#endif
@


1.27
log
@support for changing stack execution protection through mprotect()
by emulating the page execution protection bit and accounting
for pages mapped executable on the stack and swapping the
global user code descriptors for the process accordingly.
this is tested w/ the regress test and art@@ looked over it.

there is still a mistery how executable mappings on fault
works on i386 since no prot_exec faults ever happen.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.26 2002/03/14 01:26:33 millert Exp $	*/
d417 1
a417 1
#define	PMAP_PAGEIDLEZERO(pa)	pmap_zero_page_uncached((pa))
@


1.26
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.25 2001/12/19 08:58:05 art Exp $	*/
d241 1
a241 1
/* PG_AVAIL3 not used */
d273 1
@


1.25
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.18 2001/11/07 02:55:50 art Exp $	*/
d393 14
a406 14
void		pmap_bootstrap __P((vaddr_t));
boolean_t	pmap_change_attrs __P((struct vm_page *, int, int));
static void	pmap_page_protect __P((struct vm_page *, vm_prot_t));
void		pmap_page_remove  __P((struct vm_page *));
static void	pmap_protect __P((struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t));
void		pmap_remove __P((struct pmap *, vaddr_t, vaddr_t));
boolean_t	pmap_test_attrs __P((struct vm_page *, int));
void		pmap_transfer __P((struct pmap *, struct pmap *, vaddr_t,
				   vsize_t, vaddr_t, boolean_t));
static void	pmap_update_pg __P((vaddr_t));
static void	pmap_update_2pg __P((vaddr_t,vaddr_t));
void		pmap_write_protect __P((struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t));
d408 1
a408 1
vaddr_t reserve_dumppages __P((vaddr_t)); /* XXX: not a pmap fn */
d415 1
a415 1
boolean_t	pmap_zero_page_uncached __P((paddr_t));
d505 1
a505 1
void	pmap_ldt_cleanup __P((struct proc *));
@


1.24
log
@More sync to NetBSD.
 - don't remove wired mappings in pmap_collect.
 - some support for large pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.23 2001/12/11 16:35:18 art Exp $	*/
a206 3
#ifdef LARGEPAGES
paddr_t vtophys(vaddr_t);
#else
a208 1
#endif
@


1.24.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.24 2001/12/11 17:24:34 art Exp $	*/
d397 14
a410 14
void		pmap_bootstrap(vaddr_t);
boolean_t	pmap_change_attrs(struct vm_page *, int, int);
static void	pmap_page_protect(struct vm_page *, vm_prot_t);
void		pmap_page_remove(struct vm_page *);
static void	pmap_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
void		pmap_remove(struct pmap *, vaddr_t, vaddr_t);
boolean_t	pmap_test_attrs(struct vm_page *, int);
void		pmap_transfer(struct pmap *, struct pmap *, vaddr_t,
				   vsize_t, vaddr_t, boolean_t);
static void	pmap_update_pg(vaddr_t);
static void	pmap_update_2pg(vaddr_t,vaddr_t);
void		pmap_write_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
d412 1
a412 1
vaddr_t reserve_dumppages(vaddr_t); /* XXX: not a pmap fn */
d419 1
a419 1
boolean_t	pmap_zero_page_uncached(paddr_t);
d509 1
a509 1
void	pmap_ldt_cleanup(struct proc *);
@


1.24.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.24.2.1 2002/06/11 03:35:54 art Exp $	*/
d245 1
a245 1
#define	PG_X		PG_AVAIL3	/* executable mapping */
a276 1
	int pm_nxpages;			/* # of executable pages on stack */
d420 1
a420 1
#define	PMAP_PAGEIDLEZERO(pg)	pmap_zero_page_uncached(VM_PAGE_TO_PHYS(pg))
@


1.24.2.3
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d43 4
d54 1
a54 1
 * See pte.h for a description of i386 MMU terminology and hardware
d57 1
a57 1
 * A pmap describes a process' 4GB virtual address space.  This
d59 1
a59 1
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
d61 2
a62 2
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
d64 3
a66 3
 * PDE#s	VA range		Usage
 * 0->831	0x0 -> 0xcfc00000	user address space, note that the
 *					max user address is 0xcfbfe000
d69 5
a73 5
 *					but now are no longer used.
 * 831		0xcfc00000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 832->1023	0xd0000000->		kernel address space (constant
 *			0xffc00000	across all pmaps/processes).
d75 1
a75 1
 *			<end>		(for other pmaps).
d78 2
a79 2
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
d81 1
a81 1
 * area.  The PTE for page 1 is the second int.  The very last int in the
d85 2
a86 2
 * All pmaps' PDs must have the same values in slots 832->1023 so that
 * the kernel is always mapped in every process.  These values are loaded
d89 2
a90 2
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
d92 1
a92 1
 * point (slot #831 as show above).  When the pmap code wants to find the
d95 2
a96 2
 * Address of PTE = (831 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xcfc00000 + (VA / 4096) * 4
d98 2
a99 2
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
d101 1
a101 1
 * slot 1023 of the active pmap.  This causes the non-active pmap's
d105 1
a105 1
 * The following figure shows the effects of the recursive PDP mapping:
d112 2
a113 2
 *   | 831| -> points back to PDP (%cr3) mapping VA 0xcfc00000 -> 0xd0000000
 *   | 832| -> first kernel PTP (maps 0xd0000000 -> 0xe0400000)
d118 2
a119 2
 * Note that the PDE#831 VA (0xcfc00000) is defined as "PTE_BASE".
 * Note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE".
d121 1
a121 1
 * Starting at VA 0xdfc00000 the current active PDP (%cr3) acts as a
d124 1
a124 1
 * PTP#831 == PDP(%cr3) => maps VA 0xcfc00000 -> 0xd0000000
d126 1
a126 1
 *   |   0| -> maps the contents of PTP#0 at VA 0xcfc00000->0xcfc01000
d129 2
a130 2
 *   | 831| -> maps the contents of PTP#831 (the PDP) at VA 0xcff3f000
 *   | 832| -> maps the contents of first kernel PTP
d135 1
a135 1
 * Note that mapping of the PDP at PTP#831's VA (0xcff3f000) is
d138 1
a138 1
 *   "PDP_PDE" (0xcff3fcfc) is the VA of the PDE in the PDP
d140 1
a140 1
 *   "APDP_PDE" (0xcff3fffc) is the VA of the PDE in the PDP which
d142 1
a142 1
 *      To set the alternate PDP, one just has to put the correct
d145 1
a145 1
 * Note that in the APTE_BASE space, the APDP appears at VA
d150 1
a150 1
 * The following defines identify the slots used as described above.
d153 2
a154 2
#define PDSLOT_PTE	((KERNBASE/NBPD)-1) /* 831: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD)	    /* 832: start of kernel space */
d158 1
a158 1
 * The following defines give the virtual addresses of various MMU
d180 3
a182 3
 * The following define determines how many PTPs should be set up for the
 * kernel by locore.s at boot time.  This should be large enough to
 * get the VM system running.  Once the VM system is running, the
d226 3
a228 3
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
d230 1
a230 1
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
d256 1
a256 1
 * We maintain a list of all non-kernel pmaps.
d262 1
a262 1
 * The pmap structure
d264 1
a264 1
 * Note that the pm_obj contains the simple_lock, the reference count,
d277 1
a277 1
	vaddr_t pm_hiexec;		/* highest executable mapping */
d289 2
a290 2
 * For each managed physical page we maintain a list of <PMAP,VA>s
 * which it is mapped at.  The list is headed by a pv_head structure.
d292 1
a292 1
 * The pv_head structure points to a list of pv_entry structures (each
d313 1
a313 1
 * we can free pv_entry pages if needed.  There is one lock for the
d324 1
a324 1
 * number of pv_entries in a pv_page
d342 1
a342 1
 * flush TLB.  If we have more than PMAP_RR_MAX then we stop recording.
d353 13
a365 1
 * Global kernel variables
d378 1
a378 1
 * Macros
d395 1
a395 1
 * Prototypes
d406 2
a411 2
int		pmap_exec_fixup(struct vm_map *, struct trapframe *,
		    struct pcb *);
d424 1
a424 1
 * Inline functions
d467 3
a469 3
 * => This function is a front end for pmap_page_remove/pmap_change_attrs
 * => We only have to worry about making the page more protected.
 *	Unprotecting a page is done on-demand at fault time.
d489 3
a491 3
 * => This function is a front end for pmap_remove/pmap_write_protect.
 * => We only have to worry about making the page more protected.
 *	Unprotecting a page is done on-demand at fault time.
d500 6
a505 9
	if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) ==
	    (VM_PROT_WRITE|VM_PROT_EXECUTE))
		return;

	if ((prot & (VM_PROT_READ|VM_PROT_EXECUTE)) ==
	    (VM_PROT_READ|VM_PROT_EXECUTE)) {
		pmap_write_protect(pmap, sva, eva, prot);
	} else {
		pmap_remove(pmap, sva, eva);
@


1.23
log
@Merge in some pmap improvements from NetBSD.
 - disable pmap_copy (and comment out the code), it makes things slower.
 - don't return TRUE from pmap_extract if the pte is not valid.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.22 2001/12/08 02:24:06 art Exp $	*/
d207 3
d212 1
@


1.22
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.21 2001/12/04 23:22:42 art Exp $	*/
d382 1
a382 1
#define pmap_copy(DP,SP,D,L,S)		pmap_transfer(DP,SP,D,L,S, FALSE)
a384 1
#define pmap_move(DP,SP,D,L,S)		pmap_transfer(DP,SP,D,L,S, TRUE)
@


1.21
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.20 2001/11/28 16:13:28 art Exp $	*/
a503 2

vaddr_t	pmap_map __P((vaddr_t, paddr_t, paddr_t, vm_prot_t));
@


1.20
log
@zap some typedefs.
vm_map_t -> struct vm_map *
vm_map_entry_t -> struct vm_map_entry *
simple_lock_data_t -> struct simplelock

(uvm not done yet, coming in the next commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.19 2001/11/28 15:02:58 art Exp $	*/
d378 1
a378 1
#define	pmap_update()			/* nada */
@


1.19
log
@Don't use pmap_update when we mean tlbflush. make pmap_update into a noop.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.18 2001/11/07 02:55:50 art Exp $	*/
d294 1
a294 1
	simple_lock_data_t pvh_lock;	/* locks every pv on this list */
@


1.18
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.17 2001/08/18 20:50:18 art Exp $	*/
d378 1
a378 1
#define	pmap_update()			tlbflush()
d434 1
a434 1
		pmap_update();
d450 1
a450 1
		pmap_update();
@


1.17
log
@Move pmap_{de,}activate to vm/pmap.h, it's same on all archs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.16 2001/08/12 17:55:56 mickey Exp $	*/
d416 1
a416 1
void		pmap_zero_page_uncached __P((paddr_t));
@


1.16
log
@moce pglisth into uvm_pglist.h
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.15 2001/08/11 11:45:27 art Exp $	*/
a393 1
void		pmap_activate __P((struct proc *));
a395 1
void		pmap_deactivate __P((struct proc *));
@


1.15
log
@Some fixes from NetBSD.
Parts of the glue needed for page zeroing in the idle loop.
More careful handling of ptp pages (should solve some problems, most notably
with Intel 440GX+ motherboards).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.14 2001/05/05 23:25:47 art Exp $	*/
d50 1
a50 1
#include <vm/pglist.h>
@


1.14
log
@PMAP_NEW and UVM are no longer optional on i386.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.new.h,v 1.1 2001/03/22 23:36:52 niklas Exp $	*/
/*	$NetBSD: pmap.h,v 1.43 2000/02/11 07:00:13 thorpej Exp $	*/
d414 6
@


1.13
log
@Merge in NetBSD's PMAP_NEW, still disabled
@
text
@d1 44
a44 4
#ifdef PMAP_NEW
#include <machine/pmap.new.h>
#else
#include <machine/pmap.old.h>
d46 464
@


1.12
log
@permit config file overriding of NKPDE
@
text
@d1 4
a4 74
/*	$OpenBSD: pmap.h,v 1.11 1999/09/17 16:52:05 deraadt Exp $	*/
/*	$NetBSD: pmap.h,v 1.23 1996/05/03 19:26:30 christos Exp $	*/

/* 
 * Copyright (c) 1995 Charles M. Hannum.  All rights reserved.
 * Copyright (c) 1991 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department and William Jolitz of UUNET Technologies Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.h	7.4 (Berkeley) 5/12/91
 */

/*
 * Derived from hp300 version by Mike Hibler, this version by William
 * Jolitz uses a recursive map [a pde points to the page directory] to
 * map the page tables using the pagetables themselves. This is done to
 * reduce the impact on kernel virtual memory for lots of sparse address
 * space, and to reduce the cost of memory to each process.
 *
 * from hp300:	@@(#)pmap.h	7.2 (Berkeley) 12/16/90
 */

#ifndef	_I386_PMAP_H_
#define	_I386_PMAP_H_

#include <machine/cpufunc.h>
#include <machine/pte.h>

/*
 * 386 page table entry and page table directory
 * W.Jolitz, 8/89
 */

/*
 * One page directory, shared between
 * kernel and user modes.
 */
#define	KPTDI		(KERNBASE>>22)	/* start of kernel virtual pde's */
#define	PTDPTDI		(KPTDI-1)	/* ptd entry that points to ptd! */
#define	APTDPTDI	0x3ff		/* start of alternate page directory */
#define	MAXKPDE		(APTDPTDI-KPTDI)
#ifndef NKPDE		/* permit config file override */
#define	NKPDE		127		/* # to static alloc */
a5 135

/*
 * Address of current and alternate address space page table maps
 * and directories.
 */
#ifdef _KERNEL
extern pt_entry_t	PTmap[], APTmap[], Upte;
extern pd_entry_t	PTD[], APTD[], PTDpde, APTDpde, Upde;
extern pt_entry_t	*Sysmap;

extern int	PTDpaddr;	/* physical address of kernel PTD */

void pmap_bootstrap __P((vm_offset_t start));
boolean_t pmap_testbit __P((vm_offset_t, int));
void pmap_changebit __P((vm_offset_t, int, int));
void pmap_prefault __P((vm_map_t, vm_offset_t, vm_size_t));
#endif

/*
 * virtual address to page table entry and
 * to physical address. Likewise for alternate address space.
 * Note: these work recursively, thus vtopte of a pte will give
 * the corresponding pde that in turn maps it.
 */
#define	vtopte(va)	(PTmap + i386_btop(va))
#define	kvtopte(va)	vtopte(va)
#define	ptetov(pt)	(i386_ptob(pt - PTmap)) 
#define	vtophys(va) \
	((*vtopte(va) & PG_FRAME) | ((unsigned)(va) & ~PG_FRAME))

#define	avtopte(va)	(APTmap + i386_btop(va))
#define	ptetoav(pt)	(i386_ptob(pt - APTmap)) 
#define	avtophys(va) \
	((*avtopte(va) & PG_FRAME) | ((unsigned)(va) & ~PG_FRAME))

/*
 * macros to generate page directory/table indicies
 */
#define	pdei(va)	(((va) & PD_MASK) >> PDSHIFT)
#define	ptei(va)	(((va) & PT_MASK) >> PGSHIFT)

/*
 * Pmap stuff
 */
typedef struct pmap {
	pd_entry_t		*pm_pdir;	/* KVA of page directory */
	boolean_t		pm_pdchanged;	/* pdir changed */
	short			pm_dref;	/* page directory ref count */
	short			pm_count;	/* pmap reference count */
	simple_lock_data_t	pm_lock;	/* lock on pmap */
	struct pmap_statistics	pm_stats;	/* pmap statistics */
	long			pm_ptpages;	/* more stats: PT pages */
} *pmap_t;

/*
 * For each vm_page_t, there is a list of all currently valid virtual
 * mappings of that page.  An entry is a pv_entry, the list is pv_table.
 */
struct pv_entry {
	struct pv_entry	*pv_next;	/* next pv_entry */
	pmap_t		pv_pmap;	/* pmap where mapping lies */
	vm_offset_t	pv_va;		/* virtual address for mapping */
};

struct pv_page;

struct pv_page_info {
	TAILQ_ENTRY(pv_page) pgi_list;
	struct pv_entry *pgi_freelist;
	int pgi_nfree;
};

/*
 * This is basically:
 * ((NBPG - sizeof(struct pv_page_info)) / sizeof(struct pv_entry))
 */
#define	NPVPPG	340

struct pv_page {
	struct pv_page_info pvp_pgi;
	struct pv_entry pvp_pv[NPVPPG];
};

#ifdef	_KERNEL
extern struct pmap	kernel_pmap_store;

#define	pmap_kernel()			(&kernel_pmap_store)
#define	pmap_resident_count(pmap)	((pmap)->pm_stats.resident_count)
#define	pmap_update()			tlbflush()

vm_offset_t reserve_dumppages __P((vm_offset_t));

static __inline void
pmap_clear_modify(vm_offset_t pa)
{
	pmap_changebit(pa, 0, ~PG_M);
}

static __inline void
pmap_clear_reference(vm_offset_t pa)
{
	pmap_changebit(pa, 0, ~PG_U);
}

static __inline void
pmap_copy_on_write(vm_offset_t pa)
{
	pmap_changebit(pa, PG_RO, ~PG_RW);
}

static __inline boolean_t
pmap_is_modified(vm_offset_t pa)
{
	return pmap_testbit(pa, PG_M);
}

static __inline boolean_t
pmap_is_referenced(vm_offset_t pa)
{
	return pmap_testbit(pa, PG_U);
}

static __inline vm_offset_t
pmap_phys_address(int ppn)
{
	return i386_ptob(ppn);
}

void pmap_activate __P((struct proc *));
void pmap_deactivate __P((struct proc *));
vm_offset_t pmap_map __P((vm_offset_t, vm_offset_t, vm_offset_t, int));

#endif	/* _KERNEL */

#endif /* _I386_PMAP_H_ */
@


1.12.4.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 74
a74 4
#ifdef PMAP_NEW
#include <machine/pmap.new.h>
#else
#include <machine/pmap.old.h>
d76 135
@


1.12.4.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 4
a4 44
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.h,v 1.43 2000/02/11 07:00:13 thorpej Exp $	*/

/*
 *
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgment:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * pmap.h: see pmap.c for the history of this pmap module.
 */

#ifndef	_I386_PMAP_H_
#define	_I386_PMAP_H_

#if defined(_KERNEL) && !defined(_LKM) && defined(__NetBSD__)
#include "opt_user_ldt.h"
a5 464

#include <machine/cpufunc.h>
#include <machine/pte.h>
#include <machine/segments.h>
#include <vm/pglist.h>
#include <uvm/uvm_object.h>

/*
 * see pte.h for a description of i386 MMU terminology and hardware
 * interface.
 *
 * a pmap describes a processes' 4GB virtual address space.  this
 * virtual address space can be broken up into 1024 4MB regions which
 * are described by PDEs in the PDP.  the PDEs are defined as follows:
 *
 * (ranges are inclusive -> exclusive, just like vm_map_entry start/end)
 * (the following assumes that KERNBASE is 0xc0000000)
 *
 * PDE#s	VA range		usage
 * 0->767	0x0 -> 0xbfc00000	user address space, note that the
 *					max user address is 0xbfbfe000
 *					the final two pages in the last 4MB
 *					used to be reserved for the UAREA
 *					but now are no longer used
 * 768		0xbfc00000->		recursive mapping of PDP (used for
 *			0xc0000000	linear mapping of PTPs)
 * 768->1023	0xc0000000->		kernel address space (constant
 *			0xffc00000	across all pmap's/processes)
 * 1023		0xffc00000->		"alternate" recursive PDP mapping
 *			<end>		(for other pmaps)
 *
 *
 * note: a recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  in other
 * words, the PTE for page 0 is the first int mapped into the 4MB recursive
 * area.  the PTE for page 1 is the second int.  the very last int in the
 * 4MB range is the PTE that maps VA 0xffffe000 (the last page in a 4GB
 * address).
 *
 * all pmap's PD's must have the same values in slots 768->1023 so that
 * the kernel is always mapped in every process.  these values are loaded
 * into the PD at pmap creation time.
 *
 * at any one time only one pmap can be active on a processor.  this is
 * the pmap whose PDP is pointed to by processor register %cr3.  this pmap
 * will have all its PTEs mapped into memory at the recursive mapping
 * point (slot #767 as show above).  when the pmap code wants to find the
 * PTE for a virtual address, all it has to do is the following:
 *
 * address of PTE = (767 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xbfc00000 + (VA / 4096) * 4
 *
 * what happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  in that
 * case we take the PA of the PDP of non-active pmap and put it in
 * slot 1023 of the active pmap.  this causes the non-active pmap's
 * PTEs to get mapped in the final 4MB of the 4GB address space
 * (e.g. starting at 0xffc00000).
 *
 * the following figure shows the effects of the recursive PDP mapping:
 *
 *   PDP (%cr3)
 *   +----+
 *   |   0| -> PTP#0 that maps VA 0x0 -> 0x400000
 *   |    |
 *   |    |
 *   | 767| -> points back to PDP (%cr3) mapping VA 0xbfc00000 -> 0xc0000000
 *   | 768| -> first kernel PTP (maps 0xc0000000 -> 0xf0400000)
 *   |    |
 *   |1023| -> points to alternate pmap's PDP (maps 0xffc00000 -> end)
 *   +----+
 *
 * note that the PDE#767 VA (0xbfc00000) is defined as "PTE_BASE"
 * note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE"
 *
 * starting at VA 0xbfc00000 the current active PDP (%cr3) acts as a
 * PTP:
 *
 * PTP#767 == PDP(%cr3) => maps VA 0xbfc00000 -> 0xc0000000
 *   +----+
 *   |   0| -> maps the contents of PTP#0 at VA 0xbfc00000->0xbfc01000
 *   |    |
 *   |    |
 *   | 767| -> maps contents of PTP#767 (the PDP) at VA 0xbffbf000
 *   | 768| -> maps contents of first kernel PTP
 *   |    |
 *   |1023|
 *   +----+
 *
 * note that mapping of the PDP at PTP#959's VA (0xeffbf000) is
 * defined as "PDP_BASE".... within that mapping there are two
 * defines:
 *   "PDP_PDE" (0xeffbfefc) is the VA of the PDE in the PDP
 *      which points back to itself.
 *   "APDP_PDE" (0xeffbfffc) is the VA of the PDE in the PDP which
 *      establishes the recursive mapping of the alternate pmap.
 *      to set the alternate PDP, one just has to put the correct
 *	PA info in *APDP_PDE.
 *
 * note that in the APTE_BASE space, the APDP appears at VA
 * "APDP_BASE" (0xfffff000).
 */

/*
 * the following defines identify the slots used as described above.
 */

#define PDSLOT_PTE	((KERNBASE/NBPD)-1) /* 767: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD)	    /* 768: start of kernel space */
#define PDSLOT_APTE	((unsigned)1023) /* 1023: alternative recursive slot */

/*
 * the following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
 */

#define PTE_BASE	((pt_entry_t *)  (PDSLOT_PTE * NBPD) )
#define APTE_BASE	((pt_entry_t *)  (PDSLOT_APTE * NBPD) )
#define PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
#define PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define APDP_PDE	(PDP_BASE + PDSLOT_APTE)

/*
 * XXXCDC: tmp xlate from old names:
 * PTDPTDI -> PDSLOT_PTE
 * KPTDI -> PDSLOT_KERN
 * APTDPTDI -> PDSLOT_APTE
 */

/*
 * the follow define determines how many PTPs should be set up for the
 * kernel by locore.s at boot time.  this should be large enough to
 * get the VM system running.  once the VM system is running, the
 * pmap module can add more PTPs to the kernel area on demand.
 */

#ifndef NKPTP
#define NKPTP		4	/* 16MB to start */
#endif
#define NKPTP_MIN	4	/* smallest value we allow */
#define NKPTP_MAX	(1024 - (KERNBASE/NBPD) - 1)
				/* largest value (-1 for APTP space) */

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 *  kvtopte: same as above (takes a KVA, but doesn't matter with this pmap)
 *  ptetov: given a pointer to a PTE, return the VA that it maps
 *  vtophys: translate a VA to the PA mapped to it
 *
 * plus alternative versions of the above
 */

#define vtopte(VA)	(PTE_BASE + i386_btop(VA))
#define kvtopte(VA)	vtopte(VA)
#define ptetov(PT)	(i386_ptob(PT - PTE_BASE))
#define	vtophys(VA)	((*vtopte(VA) & PG_FRAME) | \
			 ((unsigned)(VA) & ~PG_FRAME))
#define	avtopte(VA)	(APTE_BASE + i386_btop(VA))
#define	ptetoav(PT)	(i386_ptob(PT - APTE_BASE))
#define	avtophys(VA)	((*avtopte(VA) & PG_FRAME) | \
			 ((unsigned)(VA) & ~PG_FRAME))

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define	pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define	ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)

/*
 * PTP macros:
 *   a PTP's index is the PD index of the PDE that points to it
 *   a PTP's offset is the byte-offset in the PTE space that this PTP is at
 *   a PTP's VA is the first VA mapped by that PTP
 *
 * note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
 */

#define ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */

/*
 * PG_AVAIL usage: we make use of the ignored bits of the PTE
 */

#define PG_W		PG_AVAIL1	/* "wired" mapping */
#define PG_PVLIST	PG_AVAIL2	/* mapping has entry on pvlist */
/* PG_AVAIL3 not used */

#ifdef _KERNEL
/*
 * pmap data structures: see pmap.c for details of locking.
 */

struct pmap;
typedef struct pmap *pmap_t;

/*
 * we maintain a list of all non-kernel pmaps
 */

LIST_HEAD(pmap_head, pmap); /* struct pmap_head: head of a pmap list */

/*
 * the pmap structure
 *
 * note that the pm_obj contains the simple_lock, the reference count,
 * page list, and number of PTPs within the pmap.
 */

struct pmap {
	struct uvm_object pm_obj;	/* object (lck by object lock) */
#define	pm_lock	pm_obj.vmobjlock
	LIST_ENTRY(pmap) pm_list;	/* list (lck by pm_list lock) */
	pd_entry_t *pm_pdir;		/* VA of PD (lck by object lock) */
	u_int32_t pm_pdirpa;		/* PA of PD (read-only after create) */
	struct vm_page *pm_ptphint;	/* pointer to a PTP in our pmap */
	struct pmap_statistics pm_stats;  /* pmap stats (lck by object lock) */

	int pm_flags;			/* see below */

	union descriptor *pm_ldt;	/* user-set LDT */
	int pm_ldt_len;			/* number of LDT entries */
	int pm_ldt_sel;			/* LDT selector */
};

/* pm_flags */
#define	PMF_USER_LDT	0x01	/* pmap has user-set LDT */

/*
 * for each managed physical page we maintain a list of <PMAP,VA>'s
 * which it is mapped at.  the list is headed by a pv_head structure.
 * there is one pv_head per managed phys page (allocated at boot time).
 * the pv_head structure points to a list of pv_entry structures (each
 * describes one mapping).
 */

struct pv_entry;

struct pv_head {
	simple_lock_data_t pvh_lock;	/* locks every pv on this list */
	struct pv_entry *pvh_list;	/* head of list (locked by pvh_lock) */
};

struct pv_entry {			/* locked by its list's pvh_lock */
	struct pv_entry *pv_next;	/* next entry */
	struct pmap *pv_pmap;		/* the pmap */
	vaddr_t pv_va;			/* the virtual address */
	struct vm_page *pv_ptp;		/* the vm_page of the PTP */
};

/*
 * pv_entrys are dynamically allocated in chunks from a single page.
 * we keep track of how many pv_entrys are in use for each page and
 * we can free pv_entry pages if needed.  there is one lock for the
 * entire allocation system.
 */

struct pv_page_info {
	TAILQ_ENTRY(pv_page) pvpi_list;
	struct pv_entry *pvpi_pvfree;
	int pvpi_nfree;
};

/*
 * number of pv_entry's in a pv_page
 * (note: won't work on systems where NPBG isn't a constant)
 */

#define PVE_PER_PVPAGE ((NBPG - sizeof(struct pv_page_info)) / \
			sizeof(struct pv_entry))

/*
 * a pv_page: where pv_entrys are allocated from
 */

struct pv_page {
	struct pv_page_info pvinfo;
	struct pv_entry pvents[PVE_PER_PVPAGE];
};

/*
 * pmap_remove_record: a record of VAs that have been unmapped, used to
 * flush TLB.  if we have more than PMAP_RR_MAX then we stop recording.
 */

#define PMAP_RR_MAX	16	/* max of 16 pages (64K) */

struct pmap_remove_record {
	int prr_npages;
	vaddr_t prr_vas[PMAP_RR_MAX];
};

/*
 * pmap_transfer_location: used to pass the current location in the
 * pmap between pmap_transfer and pmap_transfer_ptes [e.g. during
 * a pmap_copy].
 */

struct pmap_transfer_location {
	vaddr_t addr;			/* the address (page-aligned) */
	pt_entry_t *pte;		/* the PTE that maps address */
	struct vm_page *ptp;		/* the PTP that the PTE lives in */
};

/*
 * global kernel variables
 */

extern pd_entry_t	PTD[];

/* PTDpaddr: is the physical address of the kernel's PDP */
extern u_long PTDpaddr;

extern struct pmap kernel_pmap_store;	/* kernel pmap */
extern int nkpde;			/* current # of PDEs for kernel */
extern int pmap_pg_g;			/* do we support PG_G? */

/*
 * macros
 */

#define	pmap_kernel()			(&kernel_pmap_store)
#define	pmap_resident_count(pmap)	((pmap)->pm_stats.resident_count)
#define	pmap_update()			tlbflush()

#define pmap_clear_modify(pg)		pmap_change_attrs(pg, 0, PG_M)
#define pmap_clear_reference(pg)	pmap_change_attrs(pg, 0, PG_U)
#define pmap_copy(DP,SP,D,L,S)		pmap_transfer(DP,SP,D,L,S, FALSE)
#define pmap_is_modified(pg)		pmap_test_attrs(pg, PG_M)
#define pmap_is_referenced(pg)		pmap_test_attrs(pg, PG_U)
#define pmap_move(DP,SP,D,L,S)		pmap_transfer(DP,SP,D,L,S, TRUE)
#define pmap_phys_address(ppn)		i386_ptob(ppn)
#define pmap_valid_entry(E) 		((E) & PG_V) /* is PDE or PTE valid? */


/*
 * prototypes
 */

void		pmap_activate __P((struct proc *));
void		pmap_bootstrap __P((vaddr_t));
boolean_t	pmap_change_attrs __P((struct vm_page *, int, int));
void		pmap_deactivate __P((struct proc *));
static void	pmap_page_protect __P((struct vm_page *, vm_prot_t));
void		pmap_page_remove  __P((struct vm_page *));
static void	pmap_protect __P((struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t));
void		pmap_remove __P((struct pmap *, vaddr_t, vaddr_t));
boolean_t	pmap_test_attrs __P((struct vm_page *, int));
void		pmap_transfer __P((struct pmap *, struct pmap *, vaddr_t,
				   vsize_t, vaddr_t, boolean_t));
static void	pmap_update_pg __P((vaddr_t));
static void	pmap_update_2pg __P((vaddr_t,vaddr_t));
void		pmap_write_protect __P((struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t));

vaddr_t reserve_dumppages __P((vaddr_t)); /* XXX: not a pmap fn */

#define PMAP_GROWKERNEL		/* turn on pmap_growkernel interface */

/*
 * inline functions
 */

/*
 * pmap_update_pg: flush one page from the TLB (or flush the whole thing
 *	if hardware doesn't support one-page flushing)
 */

__inline static void
pmap_update_pg(va)
	vaddr_t va;
{
#if defined(I386_CPU)
	if (cpu_class == CPUCLASS_386)
		pmap_update();
	else
#endif
		invlpg((u_int) va);
}

/*
 * pmap_update_2pg: flush two pages from the TLB
 */

__inline static void
pmap_update_2pg(va, vb)
	vaddr_t va, vb;
{
#if defined(I386_CPU)
	if (cpu_class == CPUCLASS_386)
		pmap_update();
	else
#endif
	{
		invlpg((u_int) va);
		invlpg((u_int) vb);
	}
}

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => this function is a frontend for pmap_page_remove/pmap_change_attrs
 * => we only have to worry about making the page more protected.
 *	unprotecting a page is done on-demand at fault time.
 */

__inline static void
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
{
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
			(void) pmap_change_attrs(pg, PG_RO, PG_RW);
		} else {
			pmap_page_remove(pg);
		}
	}
}

/*
 * pmap_protect: change the protection of pages in a pmap
 *
 * => this function is a frontend for pmap_remove/pmap_write_protect
 * => we only have to worry about making the page more protected.
 *	unprotecting a page is done on-demand at fault time.
 */

__inline static void
pmap_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
{
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
			pmap_write_protect(pmap, sva, eva, prot);
		} else {
			pmap_remove(pmap, sva, eva);
		}
	}
}

vaddr_t	pmap_map __P((vaddr_t, paddr_t, paddr_t, vm_prot_t));

#if defined(USER_LDT)
void	pmap_ldt_cleanup __P((struct proc *));
#define	PMAP_FORK
#endif /* USER_LDT */

#endif /* _KERNEL */
#endif	/* _I386_PMAP_H_ */
@


1.12.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.h,v 1.12.4.2 2001/07/04 10:16:49 niklas Exp $	*/
/*	$NetBSD: pmap.h,v 1.44 2000/04/24 17:18:18 thorpej Exp $	*/
d50 1
a50 1
#include <uvm/uvm_pglist.h>
d394 1
d397 1
a413 6

/*
 * Do idle page zero'ing uncached to avoid polluting the cache.
 */
void		pmap_zero_page_uncached __P((paddr_t));
#define	PMAP_PAGEIDLEZERO(pa)	pmap_zero_page_uncached((pa))
@


1.12.4.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d416 1
a416 1
boolean_t	pmap_zero_page_uncached __P((paddr_t));
@


1.12.4.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12.4.4 2001/11/13 21:00:52 niklas Exp $	*/
d294 1
a294 1
	struct simplelock pvh_lock;	/* locks every pv on this list */
d378 1
a378 1
#define	pmap_update()			/* nada */
d434 1
a434 1
		tlbflush();
d450 1
a450 1
		tlbflush();
@


1.12.4.6
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d378 1
a378 1
#define	pmap_update(pm)			/* nada */
d382 1
a382 1
#define pmap_copy(DP,SP,D,L,S)
d385 1
d504 2
@


1.12.4.7
log
@Merge in -current from about a week ago
@
text
@d393 14
a406 14
void		pmap_bootstrap(vaddr_t);
boolean_t	pmap_change_attrs(struct vm_page *, int, int);
static void	pmap_page_protect(struct vm_page *, vm_prot_t);
void		pmap_page_remove(struct vm_page *);
static void	pmap_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
void		pmap_remove(struct pmap *, vaddr_t, vaddr_t);
boolean_t	pmap_test_attrs(struct vm_page *, int);
void		pmap_transfer(struct pmap *, struct pmap *, vaddr_t,
				   vsize_t, vaddr_t, boolean_t);
static void	pmap_update_pg(vaddr_t);
static void	pmap_update_2pg(vaddr_t,vaddr_t);
void		pmap_write_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
d408 1
a408 1
vaddr_t reserve_dumppages(vaddr_t); /* XXX: not a pmap fn */
d415 1
a415 1
boolean_t	pmap_zero_page_uncached(paddr_t);
d505 1
a505 1
void	pmap_ldt_cleanup(struct proc *);
@


1.12.4.8
log
@Sync the SMP branch with 3.3
@
text
@d43 4
d241 1
a241 1
#define	PG_X		PG_AVAIL3	/* executable mapping */
a272 1
	int pm_nxpages;			/* # of executable pages on stack */
d416 1
a416 1
#define	PMAP_PAGEIDLEZERO(pg)	pmap_zero_page_uncached(VM_PAGE_TO_PHYS(pg))
@


1.12.4.9
log
@pmap_deactivate for MP.  Per-CPU ptes for zeroing, copying and tmp ptp.
static cleanup.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12.4.8 2003/03/27 23:26:55 niklas Exp $	*/
a238 6
/*
 * Number of PTE's per cache line.  4 byte pte, 32-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL			8

a274 1
	uint32_t pm_cpus;		/* mask oc CPUs using map */
@


1.12.4.10
log
@Remove unused code.  Create TLB shootdown stubs, but only implement normal
uniprocessor functionality, call the stubs everywhere except in pmap_steal_ptp
which is tough, and rather should be removed.  TLB shootdown API from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12.4.9 2003/04/05 20:43:39 niklas Exp $	*/
d352 12
d405 2
a412 3

void	pmap_tlb_shootdown(pmap_t, vaddr_t, pt_entry_t, int32_t *);
void	pmap_tlb_shootnow(int32_t);
@


1.12.4.11
log
@Add TLB shootdown logic, mostly from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12.4.11 2003/04/14 14:02:52 niklas Exp $	*/
d340 12
a401 5
#ifdef notyet
void	pmap_do_tlb_shootdown(struct cpu_info *);
#else
void	pmap_do_tlb_shootdown(void);
#endif
@


1.12.4.12
log
@Sync the SMP branch to -current, plus some ELF-related fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12.4.11 2003/04/15 03:53:47 niklas Exp $	*/
d50 1
a50 1
 * See pte.h for a description of i386 MMU terminology and hardware
d53 1
a53 1
 * A pmap describes a process' 4GB virtual address space.  This
d55 1
a55 1
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
d57 2
a58 2
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
d60 3
a62 3
 * PDE#s	VA range		Usage
 * 0->831	0x0 -> 0xcfc00000	user address space, note that the
 *					max user address is 0xcfbfe000
d65 5
a69 5
 *					but now are no longer used.
 * 831		0xcfc00000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 832->1023	0xd0000000->		kernel address space (constant
 *			0xffc00000	across all pmaps/processes).
d71 1
a71 1
 *			<end>		(for other pmaps).
d74 2
a75 2
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
d77 1
a77 1
 * area.  The PTE for page 1 is the second int.  The very last int in the
d81 2
a82 2
 * All pmaps' PDs must have the same values in slots 832->1023 so that
 * the kernel is always mapped in every process.  These values are loaded
d85 2
a86 2
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
d88 1
a88 1
 * point (slot #831 as show above).  When the pmap code wants to find the
d91 2
a92 2
 * Address of PTE = (831 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xcfc00000 + (VA / 4096) * 4
d94 2
a95 2
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
d97 1
a97 1
 * slot 1023 of the active pmap.  This causes the non-active pmap's
d101 1
a101 1
 * The following figure shows the effects of the recursive PDP mapping:
d108 2
a109 2
 *   | 831| -> points back to PDP (%cr3) mapping VA 0xcfc00000 -> 0xd0000000
 *   | 832| -> first kernel PTP (maps 0xd0000000 -> 0xe0400000)
d114 2
a115 2
 * Note that the PDE#831 VA (0xcfc00000) is defined as "PTE_BASE".
 * Note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE".
d117 1
a117 1
 * Starting at VA 0xdfc00000 the current active PDP (%cr3) acts as a
d120 1
a120 1
 * PTP#831 == PDP(%cr3) => maps VA 0xcfc00000 -> 0xd0000000
d122 1
a122 1
 *   |   0| -> maps the contents of PTP#0 at VA 0xcfc00000->0xcfc01000
d125 2
a126 2
 *   | 831| -> maps the contents of PTP#831 (the PDP) at VA 0xcff3f000
 *   | 832| -> maps the contents of first kernel PTP
d131 1
a131 1
 * Note that mapping of the PDP at PTP#831's VA (0xcff3f000) is
d134 1
a134 1
 *   "PDP_PDE" (0xcff3fcfc) is the VA of the PDE in the PDP
d136 1
a136 1
 *   "APDP_PDE" (0xcff3fffc) is the VA of the PDE in the PDP which
d138 1
a138 1
 *      To set the alternate PDP, one just has to put the correct
d141 1
a141 1
 * Note that in the APTE_BASE space, the APDP appears at VA
d146 1
a146 1
 * The following defines identify the slots used as described above.
d149 2
a150 2
#define PDSLOT_PTE	((KERNBASE/NBPD)-1) /* 831: for recursive PDP map */
#define PDSLOT_KERN	(KERNBASE/NBPD)	    /* 832: start of kernel space */
d154 1
a154 1
 * The following defines give the virtual addresses of various MMU
d176 3
a178 3
 * The following define determines how many PTPs should be set up for the
 * kernel by locore.s at boot time.  This should be large enough to
 * get the VM system running.  Once the VM system is running, the
d218 3
a220 3
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
d222 1
a222 1
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
d254 1
a254 1
 * We maintain a list of all non-kernel pmaps.
d260 1
a260 1
 * The pmap structure
d262 1
a262 1
 * Note that the pm_obj contains the simple_lock, the reference count,
d288 2
a289 2
 * For each managed physical page we maintain a list of <PMAP,VA>s
 * which it is mapped at.  The list is headed by a pv_head structure.
d291 1
a291 1
 * The pv_head structure points to a list of pv_entry structures (each
d312 1
a312 1
 * we can free pv_entry pages if needed.  There is one lock for the
d323 1
a323 1
 * number of pv_entries in a pv_page
d353 1
a353 1
 * Macros
d370 1
a370 1
 * Prototypes
d405 1
a405 1
 * Inline functions
d448 3
a450 3
 * => This function is a front end for pmap_page_remove/pmap_change_attrs
 * => We only have to worry about making the page more protected.
 *	Unprotecting a page is done on-demand at fault time.
d470 3
a472 3
 * => This function is a front end for pmap_remove/pmap_write_protect.
 * => We only have to worry about making the page more protected.
 *	Unprotecting a page is done on-demand at fault time.
@


1.12.4.13
log
@merge the trunk so we will get the genfs and locking fixes
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d275 1
a275 1
	vaddr_t pm_hiexec;		/* highest executable mapping */
a384 2
int		pmap_exec_fixup(struct vm_map *, struct trapframe *,
		    struct pcb *);
d481 6
a486 9
	if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) ==
	    (VM_PROT_WRITE|VM_PROT_EXECUTE))
		return;

	if ((prot & (VM_PROT_READ|VM_PROT_EXECUTE)) ==
	    (VM_PROT_READ|VM_PROT_EXECUTE)) {
		pmap_write_protect(pmap, sva, eva, prot);
	} else {
		pmap_remove(pmap, sva, eva);
@


1.12.4.14
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12.4.13 2003/05/16 00:29:39 niklas Exp $	*/
d483 9
a491 6
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
			pmap_write_protect(pmap, sva, eva, prot);
		} else {
			pmap_remove(pmap, sva, eva);
		}
@


1.12.4.15
log
@Import NetBSD updates to NPX logic, and IPI API
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d392 1
d394 3
@


1.12.4.16
log
@Merge with the trunk
@
text
@a367 2
#define pmap_proc_iflush(p,va,len)	/* nothing */

@


1.11
log
@grown i386 kvm to 512MB
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.10 1999/02/26 10:26:58 art Exp $	*/
d73 1
d75 1
@


1.10
log
@change pmap_{de,}activate to take a struct proc *.
XXX - This should be done to other archs, but since nothing (except uvm)
      uses it right now, the interface will be changed there when
      support for uvm is added.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.9 1998/04/25 20:31:35 mickey Exp $	*/
d69 3
a71 3
#define	PTDPTDI		0x3bf		/* ptd entry that points to ptd! */
#define	KPTDI		0x3c0		/* start of kernel virtual pde's */
#define	NKPDE		63		/* # to static alloc */
d73 1
a73 1
#define	APTDPTDI	0x3ff		/* start of alternate page directory */
@


1.9
log
@convert i386 to MNN
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.8 1997/01/07 05:37:34 tholo Exp $	*/
d202 2
a203 1
void pmap_activate __P((pmap_t, struct pcb *));
@


1.8
log
@Fix for final ptdi panic on i386
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.7 1996/10/25 11:14:16 deraadt Exp $	*/
a158 1
struct pv_entry		*pv_table;	/* array of entries, one per page */
d203 1
@


1.7
log
@grow kvm space; fix an over-agressive pmap optimization
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.6 1996/05/26 00:06:07 deraadt Exp $	*/
d89 1
@


1.6
log
@force NKPDE to 31 on all machines until a better fix is available
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.5 1996/05/09 10:16:47 deraadt Exp $	*/
d69 4
a72 3
#define	PTDPTDI		0x3df		/* ptd entry that points to ptd! */
#define	KPTDI		0x3e0		/* start of kernel virtual pde's */
#define	NKPDE		31
@


1.5
log
@allow config file to override NKPDE (yuck)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.4 1996/05/07 07:22:09 deraadt Exp $	*/
d71 1
a71 3
#ifndef NKPDE
#define	NKPDE		12
#endif
@


1.4
log
@sync with 0504; prototype changes
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.3 1996/03/19 21:09:27 mickey Exp $	*/
d71 1
a71 3
#ifdef BABY
#define	NKPDE		31
#else
@


1.3
log
@Merging w/ NetBSD 021796.
speaker upgraded to the current.
some changes to the VM stuff (ie kern_thread.c added and so).
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.h,v 1.2 1996/02/28 15:03:41 mickey Exp $	*/
/*	$NetBSD: pmap.h,v 1.22 1996/02/12 21:12:29 christos Exp $	*/
d71 3
d75 1
d167 2
d204 2
@


1.2
log
@From NetBSD:
pmap_page_index return type must be int, not u_int.
@
text
@d1 2
a2 1
/*	$OpenBSD: pmap.h,v 1.21 1995/10/11 04:20:20 mycroft Exp $	*/
a87 1
__pure int pmap_page_index __P((vm_offset_t));
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.h,v 1.21 1995/10/11 04:20:20 mycroft Exp $	*/
d87 1
a87 1
__pure u_int pmap_page_index __P((vm_offset_t));
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
