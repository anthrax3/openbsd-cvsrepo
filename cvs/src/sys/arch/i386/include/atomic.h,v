head	1.18;
access;
symbols
	OPENBSD_6_1:1.15.0.10
	OPENBSD_6_1_BASE:1.15
	OPENBSD_6_0:1.15.0.6
	OPENBSD_6_0_BASE:1.15
	OPENBSD_5_9:1.15.0.2
	OPENBSD_5_9_BASE:1.15
	OPENBSD_5_8:1.15.0.4
	OPENBSD_5_8_BASE:1.15
	OPENBSD_5_7:1.14.0.2
	OPENBSD_5_7_BASE:1.14
	OPENBSD_5_6:1.12.0.4
	OPENBSD_5_6_BASE:1.12
	OPENBSD_5_5:1.11.0.8
	OPENBSD_5_5_BASE:1.11
	OPENBSD_5_4:1.11.0.4
	OPENBSD_5_4_BASE:1.11
	OPENBSD_5_3:1.11.0.2
	OPENBSD_5_3_BASE:1.11
	OPENBSD_5_2:1.9.0.6
	OPENBSD_5_2_BASE:1.9
	OPENBSD_5_1_BASE:1.9
	OPENBSD_5_1:1.9.0.4
	OPENBSD_5_0:1.9.0.2
	OPENBSD_5_0_BASE:1.9
	OPENBSD_4_9:1.7.0.12
	OPENBSD_4_9_BASE:1.7
	OPENBSD_4_8:1.7.0.10
	OPENBSD_4_8_BASE:1.7
	OPENBSD_4_7:1.7.0.6
	OPENBSD_4_7_BASE:1.7
	OPENBSD_4_6:1.7.0.8
	OPENBSD_4_6_BASE:1.7
	OPENBSD_4_5:1.7.0.4
	OPENBSD_4_5_BASE:1.7
	OPENBSD_4_4:1.7.0.2
	OPENBSD_4_4_BASE:1.7
	OPENBSD_4_3:1.6.0.4
	OPENBSD_4_3_BASE:1.6
	OPENBSD_4_2:1.6.0.2
	OPENBSD_4_2_BASE:1.6
	OPENBSD_4_1:1.5.0.2
	OPENBSD_4_1_BASE:1.5
	OPENBSD_4_0:1.3.0.2
	OPENBSD_4_0_BASE:1.3
	OPENBSD_3_9:1.2.0.8
	OPENBSD_3_9_BASE:1.2
	OPENBSD_3_8:1.2.0.6
	OPENBSD_3_8_BASE:1.2
	OPENBSD_3_7:1.2.0.4
	OPENBSD_3_7_BASE:1.2
	OPENBSD_3_6:1.2.0.2
	OPENBSD_3_6_BASE:1.2
	SMP_SYNC_A:1.1
	SMP_SYNC_B:1.1
	UBC_SYNC_A:1.1
	UBC_SYNC_B:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.18
date	2017.05.28.01.33.26;	author jsg;	state Exp;
branches;
next	1.17;
commitid	NrszIq0xKmuBN4JS;

1.17
date	2017.05.27.20.12.12;	author kettenis;	state Exp;
branches;
next	1.16;
commitid	wnLzXin7FL6GumIs;

1.16
date	2017.05.12.08.46.28;	author mpi;	state Exp;
branches;
next	1.15;
commitid	PA52H6oQQYmCFJyn;

1.15
date	2015.05.30.08.41.30;	author kettenis;	state Exp;
branches;
next	1.14;
commitid	WTgCsX48WglrjG8M;

1.14
date	2014.10.08.19.40.28;	author sf;	state Exp;
branches;
next	1.13;
commitid	tjRVuxiDN7VnYLrQ;

1.13
date	2014.09.11.20.32.16;	author kettenis;	state Exp;
branches;
next	1.12;
commitid	Uz26KLKg68Z6vU63;

1.12
date	2014.03.29.18.09.29;	author guenther;	state Exp;
branches
	1.12.4.1;
next	1.11;

1.11
date	2012.11.19.15.18.06;	author pirofti;	state Exp;
branches
	1.11.8.1;
next	1.10;

1.10
date	2012.11.19.15.03.55;	author pirofti;	state Exp;
branches;
next	1.9;

1.9
date	2011.07.09.01.49.16;	author pirofti;	state Exp;
branches;
next	1.8;

1.8
date	2011.03.23.16.54.35;	author pirofti;	state Exp;
branches;
next	1.7;

1.7
date	2008.06.26.05.42.10;	author ray;	state Exp;
branches;
next	1.6;

1.6
date	2007.05.25.15.55.27;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2007.02.19.17.18.42;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2007.02.06.17.13.33;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2006.04.27.15.37.53;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2004.06.13.21.49.16;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	2001.07.14.10.02.35;	author ho;	state dead;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2001.07.14.10.02.35;	author ho;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2001.07.15.15.13.28;	author ho;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2003.04.05.20.40.16;	author niklas;	state Exp;
branches;
next	;

1.11.8.1
date	2014.12.09.13.03.17;	author sf;	state Exp;
branches;
next	;
commitid	ux5h60hA6HQhMlAQ;

1.12.4.1
date	2014.12.09.12.53.03;	author sf;	state Exp;
branches;
next	;
commitid	j4gSsn64Ru5hB56g;


desc
@@


1.18
log
@remove bogus atomic_swap_64 code from i386

xchg can't handle 64 bit values on i386.  gcc errors if the code
is called, clang errors if it is included.

ok mlarkin@@ kettenis@@
@
text
@/*	$OpenBSD: atomic.h,v 1.17 2017/05/27 20:12:12 kettenis Exp $	*/
/* $NetBSD: atomic.h,v 1.1.2.2 2000/02/21 18:54:07 sommerfeld Exp $ */

/*-
 * Copyright (c) 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by RedBack Networks Inc.
 *
 * Author: Bill Sommerfeld
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _MACHINE_ATOMIC_H_
#define _MACHINE_ATOMIC_H_

/*
 * Perform atomic operations on memory. Should be atomic with respect
 * to interrupts and multiple processors.
 *
 * void atomic_setbits_int(volatile u_int *a, u_int mask) { *a |= mask; }
 * void atomic_clearbits_int(volatile u_int *a, u_int mas) { *a &= ~mask; }
 */
#if !defined(_LOCORE)

#if defined(MULTIPROCESSOR) || !defined(_KERNEL)
#define _LOCK "lock"
#else
#define _LOCK
#endif

static inline unsigned int
_atomic_cas_uint(volatile unsigned int *p, unsigned int e, unsigned int n)
{
	__asm volatile(_LOCK " cmpxchgl %2, %1"
	    : "=a" (n), "=m" (*p)
	    : "r" (n), "a" (e), "m" (*p));

	return (n);
}
#define atomic_cas_uint(_p, _e, _n) _atomic_cas_uint((_p), (_e), (_n))

static inline unsigned long
_atomic_cas_ulong(volatile unsigned long *p, unsigned long e, unsigned long n)
{
	__asm volatile(_LOCK " cmpxchgl %2, %1"
	    : "=a" (n), "=m" (*p)
	    : "r" (n), "a" (e), "m" (*p));

	return (n);
}
#define atomic_cas_ulong(_p, _e, _n) _atomic_cas_ulong((_p), (_e), (_n))

static inline void *
_atomic_cas_ptr(volatile void *p, void *e, void *n)
{
	__asm volatile(_LOCK " cmpxchgl %2, %1"
	    : "=a" (n), "=m" (*(unsigned long *)p)
	    : "r" (n), "a" (e), "m" (*(unsigned long *)p));

	return (n);
}
#define atomic_cas_ptr(_p, _e, _n) _atomic_cas_ptr((_p), (_e), (_n))

static inline unsigned int
_atomic_swap_uint(volatile unsigned int *p, unsigned int n)
{
	__asm volatile("xchgl %0, %1"
	    : "=a" (n), "=m" (*p)
	    : "0" (n), "m" (*p));

	return (n);
}
#define atomic_swap_uint(_p, _n) _atomic_swap_uint((_p), (_n))
#define atomic_swap_32(_p, _n) _atomic_swap_uint((_p), (_n))

static inline unsigned long
_atomic_swap_ulong(volatile unsigned long *p, unsigned long n)
{
	__asm volatile("xchgl %0, %1"
	    : "=a" (n), "=m" (*p)
	    : "0" (n), "m" (*p));

	return (n);
}
#define atomic_swap_ulong(_p, _n) _atomic_swap_ulong((_p), (_n))

static inline void *
_atomic_swap_ptr(volatile void *p, void *n)
{
	__asm volatile("xchgl %0, %1"
	    : "=a" (n), "=m" (*(unsigned long *)p)
	    : "0" (n), "m" (*(unsigned long *)p));

	return (n);
}
#define atomic_swap_ptr(_p, _n) _atomic_swap_ptr((_p), (_n))

static inline void
_atomic_inc_int(volatile unsigned int *p)
{
	__asm volatile(_LOCK " incl %0"
	    : "+m" (*p));
}
#define atomic_inc_int(_p) _atomic_inc_int(_p)

static inline void
_atomic_inc_long(volatile unsigned long *p)
{
	__asm volatile(_LOCK " incl %0"
	    : "+m" (*p));
}
#define atomic_inc_long(_p) _atomic_inc_long(_p)

static inline void
_atomic_dec_int(volatile unsigned int *p)
{
	__asm volatile(_LOCK " decl %0"
	    : "+m" (*p));
}
#define atomic_dec_int(_p) _atomic_dec_int(_p)

static inline void
_atomic_dec_long(volatile unsigned long *p)
{
	__asm volatile(_LOCK " decl %0"
	    : "+m" (*p));
}
#define atomic_dec_long(_p) _atomic_dec_long(_p)

static inline void
_atomic_add_int(volatile unsigned int *p, unsigned int v)
{
	__asm volatile(_LOCK " addl %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_add_int(_p, _v) _atomic_add_int(_p, _v)

static inline void
_atomic_add_long(volatile unsigned long *p, unsigned long v)
{
	__asm volatile(_LOCK " addl %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_add_long(_p, _v) _atomic_add_long(_p, _v)

static inline void
_atomic_sub_int(volatile unsigned int *p, unsigned int v)
{
	__asm volatile(_LOCK " subl %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_sub_int(_p, _v) _atomic_sub_int(_p, _v)

static inline void
_atomic_sub_long(volatile unsigned long *p, unsigned long v)
{
	__asm volatile(_LOCK " subl %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_sub_long(_p, _v) _atomic_sub_long(_p, _v)


static inline unsigned long
_atomic_add_int_nv(volatile unsigned int *p, unsigned int v)
{
	unsigned int rv = v;

	__asm volatile(_LOCK " xaddl %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv + v);
}
#define atomic_add_int_nv(_p, _v) _atomic_add_int_nv(_p, _v)

static inline unsigned long
_atomic_add_long_nv(volatile unsigned long *p, unsigned long v)
{
	unsigned long rv = v;

	__asm volatile(_LOCK " xaddl %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv + v);
}
#define atomic_add_long_nv(_p, _v) _atomic_add_long_nv(_p, _v)

static inline unsigned long
_atomic_sub_int_nv(volatile unsigned int *p, unsigned int v)
{
	unsigned int rv = 0 - v;

	__asm volatile(_LOCK " xaddl %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv - v);
}
#define atomic_sub_int_nv(_p, _v) _atomic_sub_int_nv(_p, _v)

static inline unsigned long
_atomic_sub_long_nv(volatile unsigned long *p, unsigned long v)
{
	unsigned long rv = 0 - v;

	__asm volatile(_LOCK " xaddl %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv - v);
}
#define atomic_sub_long_nv(_p, _v) _atomic_sub_long_nv(_p, _v)

/*
 * The IA-32 architecture is rather strongly ordered.  When accessing
 * normal write-back cachable memory, only reads may be reordered with
 * older writes to different locations.  There are a few instructions
 * (clfush, non-temporal move instructions) that obey weaker ordering
 * rules, but those instructions will only be used in (inline)
 * assembly code where we can add the necessary fence instructions
 * ourselves.
 */

#define __membar(_f) do { __asm __volatile(_f ::: "memory"); } while (0)

#if defined(MULTIPROCESSOR) || !defined(_KERNEL)
#define membar_enter()		__membar("lock; addl $0,0(%%esp)")
#define membar_exit()		__membar("")
#define membar_producer()	__membar("")
#define membar_consumer()	__membar("")
#define membar_sync()		__membar("lock; addl $0,0(%%esp)")
#else
#define membar_enter()		__membar("")
#define membar_exit()		__membar("")
#define membar_producer()	__membar("")
#define membar_consumer()	__membar("")
#define membar_sync()		__membar("")
#endif

#define membar_enter_after_atomic()	__membar("")
#define membar_exit_before_atomic()	__membar("")

#ifdef _KERNEL

/* virtio needs MP membars even on SP kernels */
#define virtio_membar_producer()	__membar("")
#define virtio_membar_consumer()	__membar("")
#define virtio_membar_sync()		__membar("lock; addl $0,0(%%esp)")

static __inline u_int64_t
i386_atomic_testset_uq(volatile u_int64_t *ptr, u_int64_t val)
{
	__asm__ volatile ("\n1:\t" _LOCK " cmpxchg8b (%1); jnz 1b" : "+A" (val) :
	    "r" (ptr), "b" ((u_int32_t)val), "c" ((u_int32_t)(val >> 32)));
	return val;
}

static __inline u_int32_t
i386_atomic_testset_ul(volatile u_int32_t *ptr, unsigned long val)
{
	__asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
}

static __inline int
i386_atomic_testset_i(volatile int *ptr, unsigned long val)
{
	__asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
}

static __inline void
i386_atomic_setbits_l(volatile u_int32_t *ptr, unsigned long bits)
{
	__asm volatile(_LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
}

static __inline void
i386_atomic_clearbits_l(volatile u_int32_t *ptr, unsigned long bits)
{
	bits = ~bits;
	__asm volatile(_LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (bits));
}

#define atomic_setbits_int i386_atomic_setbits_l
#define atomic_clearbits_int i386_atomic_clearbits_l

#endif /* _KERNEL */

#undef _LOCK

#endif /* !defined(_LOCORE) */
#endif /* _MACHINE_ATOMIC_H_ */
@


1.17
log
@On i386 and amd64, atomic instructions include an implicit memory barrier.

ok mikeb@@, visa@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.16 2017/05/12 08:46:28 mpi Exp $	*/
a107 11

static inline uint64_t
_atomic_swap_64(volatile uint64_t *p, uint64_t n)
{
	__asm volatile("xchgl %0, %1"
	    : "=a" (n), "=m" (*p)
	    : "0" (n), "m" (*p));

	return (n);
}
#define atomic_swap_64(_p, _n) _atomic_swap_64((_p), (_n))
@


1.16
log
@Make atomic.h ready to be included in userland.

- prefix the LOCK macro with an underscore
- keep setbits/clearbits and virtio barriers inside _KERNEL
- Get rid of unused futex_atomic_ucas_int32().

ok dlg@@, kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.15 2015/05/30 08:41:30 kettenis Exp $	*/
d273 3
@


1.15
log
@Native atomic operations for i386.

ok deraadt@@, guenther@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.14 2014/10/08 19:40:28 sf Exp $	*/
d45 1
a45 1
#if defined(_KERNEL) && !defined(_LOCORE)
d47 2
a48 2
#ifdef MULTIPROCESSOR
#define LOCK "lock"
d50 1
a50 1
#define LOCK
d56 1
a56 1
	__asm volatile(LOCK " cmpxchgl %2, %1"
d67 1
a67 1
	__asm volatile(LOCK " cmpxchgl %2, %1"
d78 1
a78 1
	__asm volatile(LOCK " cmpxchgl %2, %1"
d134 1
a134 1
	__asm volatile(LOCK " incl %0"
d142 1
a142 1
	__asm volatile(LOCK " incl %0"
d150 1
a150 1
	__asm volatile(LOCK " decl %0"
d158 1
a158 1
	__asm volatile(LOCK " decl %0"
d166 1
a166 1
	__asm volatile(LOCK " addl %1,%0"
d175 1
a175 1
	__asm volatile(LOCK " addl %1,%0"
d184 1
a184 1
	__asm volatile(LOCK " subl %1,%0"
d193 1
a193 1
	__asm volatile(LOCK " subl %1,%0"
d205 1
a205 1
	__asm volatile(LOCK " xaddl %0,%1"
d217 1
a217 1
	__asm volatile(LOCK " xaddl %0,%1"
d229 1
a229 1
	__asm volatile(LOCK " xaddl %0,%1"
d241 1
a241 1
	__asm volatile(LOCK " xaddl %0,%1"
d260 1
a260 1
#ifdef MULTIPROCESSOR
d274 2
d284 1
a284 1
	__asm__ volatile ("\n1:\t" LOCK " cmpxchg8b (%1); jnz 1b" : "+A" (val) :
d306 1
a306 1
	__asm volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d313 1
a313 1
	__asm volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (bits));
a315 3
int ucas_32(volatile int32_t *, int32_t, int32_t);
#define futex_atomic_ucas_int32 ucas_32

d319 3
a321 1
#undef LOCK
d323 1
a323 1
#endif /* defined(_KERNEL) && !defined(_LOCORE) */
@


1.14
log
@Add virtio_membar_* macros

Virtio needs the MP memory barriers even on SP kernels.

OK kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.13 2014/09/11 20:32:16 kettenis Exp $	*/
d53 121
a173 2
static __inline u_int64_t
i386_atomic_testset_uq(volatile u_int64_t *ptr, u_int64_t val)
d175 3
a177 3
	__asm__ volatile ("\n1:\t" LOCK " cmpxchg8b (%1); jnz 1b" : "+A" (val) :
	    "r" (ptr), "b" ((u_int32_t)val), "c" ((u_int32_t)(val >> 32)));
	return val;
d179 1
d181 2
a182 2
static __inline u_int32_t
i386_atomic_testset_ul(volatile u_int32_t *ptr, unsigned long val)
d184 3
a186 2
	__asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
d188 1
d190 2
a191 2
static __inline int
i386_atomic_testset_i(volatile int *ptr, unsigned long val)
d193 3
a195 2
	__asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
d197 1
d199 3
a201 2
static __inline void
i386_atomic_setbits_l(volatile u_int32_t *ptr, unsigned long bits)
d203 6
a208 1
	__asm volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d210 1
d212 2
a213 2
static __inline void
i386_atomic_clearbits_l(volatile u_int32_t *ptr, unsigned long bits)
d215 6
a220 2
	bits = ~bits;
	__asm volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d222 1
d224 2
a225 5
/*
 * cas = compare and set
 */
static __inline int
i486_atomic_cas_int(volatile u_int *ptr, u_int expect, u_int set)
d227 1
a227 1
	int res;
d229 2
a230 2
	__asm volatile(LOCK " cmpxchgl %2, %1" : "=a" (res), "=m" (*ptr)
	     : "r" (set), "a" (expect), "m" (*ptr) : "memory");
d232 1
a232 1
	return (res);
d234 1
d236 2
a237 2
static __inline int
i386_atomic_cas_int32(volatile int32_t *ptr, int32_t expect, int32_t set)
d239 1
a239 1
	int res;
d241 2
a242 2
	__asm volatile(LOCK " cmpxchgl %2, %1" : "=a" (res), "=m" (*ptr)
	    : "r" (set), "a" (expect), "m" (*ptr) : "memory");
d244 1
a244 1
	return (res);
d246 1
d278 35
@


1.13
log
@mplement membar(9) API for i386.

ok matthew@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.12 2014/03/29 18:09:29 guenther Exp $	*/
d138 5
@


1.12
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.11 2012/11/19 15:18:06 pirofti Exp $	*/
d112 26
@


1.12.4.1
log
@Backport arch/amd64/include/atomic.h 1.16
         arch/i386/include/atomic.h  1.14

Add virtio_membar_* macros

Virtio needs the MP memory barriers even on SP kernels.

no objections kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.12 2014/03/29 18:09:29 guenther Exp $	*/
a51 7

#define __membar(_f) do { __asm __volatile(_f ::: "memory"); } while (0)

/* virtio needs MP membars even on SP kernels */
#define virtio_membar_producer()	__membar("")
#define virtio_membar_consumer()	__membar("")
#define virtio_membar_sync()		__membar("lock; addl $0,0(%%esp)")
@


1.11
log
@Add atomic 32-bit cas operations.

This is needed for future acpi global locking routines.

Okay kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.10 2012/11/19 15:03:55 pirofti Exp $	*/
d78 1
a78 1
	__asm __volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d85 1
a85 1
	__asm __volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (bits));
@


1.11.8.1
log
@Backport arch/amd64/include/atomic.h 1.16
         arch/i386/include/atomic.h  1.14

Add virtio_membar_* macros

Virtio needs the MP memory barriers even on SP kernels.

no objections kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.11 2012/11/19 15:18:06 pirofti Exp $	*/
a51 7

#define __membar(_f) do { __asm __volatile(_f ::: "memory"); } while (0)

/* virtio needs MP membars even on SP kernels */
#define virtio_membar_producer()	__membar("")
#define virtio_membar_consumer()	__membar("")
#define virtio_membar_sync()		__membar("lock; addl $0,0(%%esp)")
@


1.10
log
@Use a more descriptive name for the userland cas operation.

Substitute atomic_ucas_32 with futex_atomic_ucas_int32 to make it
obvious who's using this api.

Suggested by and okay kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.9 2011/07/09 01:49:16 pirofti Exp $	*/
d98 11
@


1.9
log
@Add an atomic compare and exchange operation dealing with addresses
from userland for i386.

Okay art@@, assembly okay mlarkin@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.8 2011/03/23 16:54:35 pirofti Exp $	*/
d103 1
a103 1
#define atomic_ucas_32 ucas_32
@


1.8
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.7 2008/06/26 05:42:10 ray Exp $	*/
d101 3
@


1.7
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.6 2007/05/25 15:55:27 art Exp $	*/
d35 2
a36 2
#ifndef _I386_ATOMIC_H_
#define _I386_ATOMIC_H_
d108 1
a108 1
#endif /* _I386_ATOMIC_H_ */
@


1.6
log
@Replace the overdesigned and overcomplicated tlb shootdown code with
very simple and dumb fast tlb IPI handlers that have in the order of
the same amount of instructions as the old code had function calls.

All TLB shootdowns are reorganized so that we always shoot the,
without looking at PG_U and when we're shooting a range (primarily in
pmap_remove), we shoot the range when there are 32 or less pages in
it, otherwise we just nuke the whole TLB (this might need tweaking if
someone is interested in micro-optimization). The IPIs are not handled
through the normal interrupt vectoring code, they are not blockable
and they only shoot one page or a range of pages or the whole tlb.

This gives a 15% reduction in system time on my dual-core laptop
during a kernel compile and an 18% reduction in real time on a quad
machine doing bulk ports build.

Tested by many, in snaps for a week, no slowdowns reported (although not
everyone is seeing such huge wins).
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.5 2007/02/19 17:18:42 deraadt Exp $	*/
a20 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *        This product includes software developed by the NetBSD
 *        Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.5
log
@only make this interface available to the kernel for now, discussed witha
rt and such; tested and ok miod drahn
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.4 2007/02/06 17:13:33 art Exp $	*/
d93 14
@


1.4
log
@Add machine/atomic.h to all architectures and define two operations
right now that are supposed to be atomic with respect to interrupts and
SMP: atomic_setbits_int and atomic_clearbits_int.

All architectures other than i386 and amd64 get dummy implementations
since at first we'll be replacing operations that are done with
"a |= bit" and "a &= ~bit" today. More proper implementations will follow

kettenis@@, miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.3 2006/04/27 15:37:53 mickey Exp $	*/
d42 2
a43 2
#ifndef _ATOMIC_H_
#define _ATOMIC_H_
d52 1
a52 1
#ifndef _LOCORE
d61 5
a65 4
i386_atomic_testset_uq (volatile u_int64_t *ptr, u_int64_t val) {
    __asm__ volatile ("\n1:\t" LOCK " cmpxchg8b (%1); jnz 1b" : "+A" (val) :
	"r" (ptr), "b" ((u_int32_t)val), "c" ((u_int32_t)(val >> 32)));
    return val;
d69 4
a72 3
i386_atomic_testset_ul (volatile u_int32_t *ptr, unsigned long val) {
    __asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
d76 4
a79 3
i386_atomic_testset_i (volatile int *ptr, unsigned long val) {
    __asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
d82 4
a85 3
static __inline void 
i386_atomic_setbits_l (volatile u_int32_t *ptr, unsigned long bits) {
    __asm __volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d88 5
a92 4
static __inline void 
i386_atomic_clearbits_l (volatile u_int32_t *ptr, unsigned long bits) {
    bits = ~bits;
    __asm __volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d100 2
a101 3
#endif
#endif

@


1.3
log
@implement separate PAE pmap that allows access to 64g of physmem
if supported by the cpu(s). currently not enabled by default and
not compiled into ramdisks. this grows paddr_t to 64bit but yet
leaves bus_addr_t at 32bits. measures are taken to favour dmaable
memory allocation from below 4g line such that buffer cache is
already allocated form below, pool backend allocator prefers lower
memory and then finally bounce buffers are used as last resort.
PAE is engaged only if global variable cpu_pae is manually set
to non-zero and there is physical memory present above 4g.
simplify pcibios address math to use u_long as we always will
be in the 32bit space.
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.2 2004/06/13 21:49:16 niklas Exp $	*/
d45 7
d54 6
d62 1
a62 1
    __asm__ volatile ("\n1:\tlock; cmpxchg8b (%1); jnz 1b" : "+A" (val) :
d81 1
a81 1
    __asm __volatile("lock ; orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d87 1
a87 1
    __asm __volatile("lock ; and %1,%0" :  "=m" (*ptr) : "ir" (bits));
d89 5
@


1.2
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d46 7
@


1.1
log
@file atomic.h was initially added on branch SMP.
@
text
@d1 72
@


1.1.2.1
log
@Initial import of some SMP code from NetBSD.
Not really working here yet, but there is some work in progress.
@
text
@a0 71
/* $NetBSD: atomic.h,v 1.1.2.2 2000/02/21 18:54:07 sommerfeld Exp $ */

/*-
 * Copyright (c) 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by RedBack Networks Inc.
 *
 * Author: Bill Sommerfeld
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *        This product includes software developed by the NetBSD
 *        Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _ATOMIC_H_
#define _ATOMIC_H_

#ifndef _LOCORE

static __inline u_int32_t
i386_atomic_testset_ul (volatile u_int32_t *ptr, unsigned long val) {
    __asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
}

static __inline int
i386_atomic_testset_i (volatile int *ptr, unsigned long val) {
    __asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
}

static __inline void 
i386_atomic_setbits_l (u_int32_t *ptr, unsigned long bits) {
    __asm __volatile("lock ; orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
}

static __inline void 
i386_atomic_clearbits_l (u_int32_t *ptr, unsigned long bits) {
    bits = ~bits;
    __asm __volatile("lock ; and %1,%0" :  "=m" (*ptr) : "ir" (bits));
}

#endif
#endif

@


1.1.2.2
log
@Add $OpenBSD$.
@
text
@a0 1
/*	$OpenBSD$	*/
@


1.1.2.3
log
@the atomic utils may take volatile args
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.1.2.2 2001/07/15 15:13:28 ho Exp $	*/
d60 1
a60 1
i386_atomic_setbits_l (volatile u_int32_t *ptr, unsigned long bits) {
d65 1
a65 1
i386_atomic_clearbits_l (volatile u_int32_t *ptr, unsigned long bits) {
@


