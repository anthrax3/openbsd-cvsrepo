head	1.196;
access;
symbols
	OPENBSD_6_1:1.195.0.4
	OPENBSD_6_1_BASE:1.195
	OPENBSD_6_0:1.190.0.2
	OPENBSD_6_0_BASE:1.190
	OPENBSD_5_9:1.186.0.2
	OPENBSD_5_9_BASE:1.186
	OPENBSD_5_8:1.181.0.4
	OPENBSD_5_8_BASE:1.181
	OPENBSD_5_7:1.169.0.2
	OPENBSD_5_7_BASE:1.169
	OPENBSD_5_6:1.160.0.4
	OPENBSD_5_6_BASE:1.160
	OPENBSD_5_5:1.159.0.4
	OPENBSD_5_5_BASE:1.159
	OPENBSD_5_4:1.158.0.4
	OPENBSD_5_4_BASE:1.158
	OPENBSD_5_3:1.158.0.2
	OPENBSD_5_3_BASE:1.158
	OPENBSD_5_2:1.157.0.2
	OPENBSD_5_2_BASE:1.157
	OPENBSD_5_1_BASE:1.155
	OPENBSD_5_1:1.155.0.4
	OPENBSD_5_0:1.155.0.2
	OPENBSD_5_0_BASE:1.155
	OPENBSD_4_9:1.151.0.2
	OPENBSD_4_9_BASE:1.151
	OPENBSD_4_8:1.148.0.2
	OPENBSD_4_8_BASE:1.148
	OPENBSD_4_7:1.146.0.2
	OPENBSD_4_7_BASE:1.146
	OPENBSD_4_6:1.142.0.4
	OPENBSD_4_6_BASE:1.142
	OPENBSD_4_5:1.136.0.2
	OPENBSD_4_5_BASE:1.136
	OPENBSD_4_4:1.123.0.2
	OPENBSD_4_4_BASE:1.123
	OPENBSD_4_3:1.122.0.2
	OPENBSD_4_3_BASE:1.122
	OPENBSD_4_2:1.119.0.2
	OPENBSD_4_2_BASE:1.119
	OPENBSD_4_1:1.97.0.2
	OPENBSD_4_1_BASE:1.97
	OPENBSD_4_0:1.94.0.2
	OPENBSD_4_0_BASE:1.94
	OPENBSD_3_9:1.89.0.2
	OPENBSD_3_9_BASE:1.89
	OPENBSD_3_8:1.82.0.2
	OPENBSD_3_8_BASE:1.82
	OPENBSD_3_7:1.80.0.2
	OPENBSD_3_7_BASE:1.80
	OPENBSD_3_6:1.79.0.2
	OPENBSD_3_6_BASE:1.79
	SMP_SYNC_A:1.75
	SMP_SYNC_B:1.75
	OPENBSD_3_5:1.75.0.2
	OPENBSD_3_5_BASE:1.75
	OPENBSD_3_4:1.73.0.2
	OPENBSD_3_4_BASE:1.73
	UBC_SYNC_A:1.72
	OPENBSD_3_3:1.65.0.2
	OPENBSD_3_3_BASE:1.65
	OPENBSD_3_2:1.63.0.2
	OPENBSD_3_2_BASE:1.63
	OPENBSD_3_1:1.59.0.2
	OPENBSD_3_1_BASE:1.59
	UBC_SYNC_B:1.65
	UBC:1.56.0.2
	UBC_BASE:1.56
	OPENBSD_3_0:1.45.0.2
	OPENBSD_3_0_BASE:1.45
	OPENBSD_2_9:1.39.0.2
	OPENBSD_2_9_BASE:1.39
	OPENBSD_2_8:1.36.0.4
	OPENBSD_2_8_BASE:1.36
	OPENBSD_2_7:1.36.0.2
	OPENBSD_2_7_BASE:1.36
	SMP:1.34.0.2
	SMP_BASE:1.34
	kame_19991208:1.33
	OPENBSD_2_6:1.32.0.2
	OPENBSD_2_6_BASE:1.32
	OPENBSD_2_5:1.27.0.2
	OPENBSD_2_5_BASE:1.27
	OPENBSD_2_4:1.25.0.2
	OPENBSD_2_4_BASE:1.25
	OPENBSD_2_3:1.24.0.2
	OPENBSD_2_3_BASE:1.24
	OPENBSD_2_2:1.22.0.2
	OPENBSD_2_2_BASE:1.22
	OPENBSD_2_1:1.16.0.2
	OPENBSD_2_1_BASE:1.16
	OPENBSD_2_0:1.13.0.2
	OPENBSD_2_0_BASE:1.13
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.196
date	2017.05.29.14.19.49;	author mpi;	state Exp;
branches;
next	1.195;
commitid	4u6PWvBw90PH7UDq;

1.195
date	2016.10.21.06.20.58;	author mlarkin;	state Exp;
branches;
next	1.194;
commitid	szRuKZ9HgqvwYLcM;

1.194
date	2016.09.17.07.37.57;	author mlarkin;	state Exp;
branches;
next	1.193;
commitid	aHOfHpl2qCSTgi8H;

1.193
date	2016.09.16.02.35.41;	author dlg;	state Exp;
branches;
next	1.192;
commitid	Fei4687v68qad1tP;

1.192
date	2016.09.16.01.09.54;	author dlg;	state Exp;
branches;
next	1.191;
commitid	S1LT7BcQMYzBQOe8;

1.191
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.190;
commitid	RlO92XR575sygHqm;

1.190
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.189;
commitid	N0upL0onl7Raz5yi;

1.189
date	2016.03.15.03.17.51;	author guenther;	state Exp;
branches;
next	1.188;
commitid	hTA8iQcFPhTNwQXL;

1.188
date	2016.03.07.05.32.47;	author naddy;	state Exp;
branches;
next	1.187;
commitid	Ht3NH0pdlkYC6Nxx;

1.187
date	2016.03.03.12.41.30;	author naddy;	state Exp;
branches;
next	1.186;
commitid	Ykztt9UU7jxBEqeD;

1.186
date	2015.10.23.09.36.09;	author kettenis;	state Exp;
branches;
next	1.185;
commitid	AJyCutJ1mVHbxB6K;

1.185
date	2015.09.03.18.49.19;	author kettenis;	state Exp;
branches;
next	1.184;
commitid	FcVXc8R49wlBBFLt;

1.184
date	2015.08.31.20.42.41;	author kettenis;	state Exp;
branches;
next	1.183;
commitid	paPhFmg6gRFtRtM3;

1.183
date	2015.08.25.04.57.31;	author mlarkin;	state Exp;
branches;
next	1.182;
commitid	9OQndlj4AhqxzfSp;

1.182
date	2015.08.22.07.16.10;	author mlarkin;	state Exp;
branches;
next	1.181;
commitid	kNcQSOWLJMgUor2a;

1.181
date	2015.07.10.10.07.31;	author kettenis;	state Exp;
branches;
next	1.180;
commitid	KP6IvZ8VHEvkXDUk;

1.180
date	2015.07.02.16.14.43;	author kettenis;	state Exp;
branches;
next	1.179;
commitid	Pj63rbiQMvtKOpRg;

1.179
date	2015.05.30.08.41.30;	author kettenis;	state Exp;
branches;
next	1.178;
commitid	WTgCsX48WglrjG8M;

1.178
date	2015.04.22.06.26.23;	author mlarkin;	state Exp;
branches;
next	1.177;
commitid	KKR3WvcGhVQ59IC1;

1.177
date	2015.04.21.18.47.57;	author mlarkin;	state Exp;
branches;
next	1.176;
commitid	NENEjzstDjAGVdN4;

1.176
date	2015.04.21.04.40.40;	author mlarkin;	state Exp;
branches;
next	1.175;
commitid	2gj43Y0hXaEuMoUO;

1.175
date	2015.04.21.00.07.51;	author mlarkin;	state Exp;
branches;
next	1.174;
commitid	wSzAe38Tgttxx9P3;

1.174
date	2015.04.12.21.37.33;	author mlarkin;	state Exp;
branches;
next	1.173;
commitid	9CHPP1SHAUmxpVnl;

1.173
date	2015.04.12.19.21.32;	author mlarkin;	state Exp;
branches;
next	1.172;
commitid	zqO23f3TXoDqmWUN;

1.172
date	2015.04.12.18.37.53;	author mlarkin;	state Exp;
branches;
next	1.171;
commitid	5ST94uMTezmXYdhY;

1.171
date	2015.03.13.23.23.13;	author mlarkin;	state Exp;
branches;
next	1.170;
commitid	OgmZIpBZPHlMNamP;

1.170
date	2015.03.09.07.46.03;	author kettenis;	state Exp;
branches;
next	1.169;
commitid	YC483Mpe3SbQTU4t;

1.169
date	2015.02.11.05.54.48;	author dlg;	state Exp;
branches;
next	1.168;
commitid	fAl1KR17j4jH74Xf;

1.168
date	2015.02.02.09.29.53;	author mlarkin;	state Exp;
branches;
next	1.167;
commitid	Gl0HfhXiwDmJ4Tqg;

1.167
date	2015.01.27.02.09.07;	author mlarkin;	state Exp;
branches;
next	1.166;
commitid	Cz4ltRdn4cSstrbs;

1.166
date	2015.01.09.03.43.52;	author mlarkin;	state Exp;
branches;
next	1.165;
commitid	TzpVlzYKb3Vx3GSZ;

1.165
date	2014.12.23.01.24.50;	author deraadt;	state Exp;
branches;
next	1.164;
commitid	u5ByZ7JDYGBMbD06;

1.164
date	2014.12.22.23.59.43;	author mlarkin;	state Exp;
branches;
next	1.163;
commitid	VnzXkNOPyYOCYtPk;

1.163
date	2014.12.02.18.13.10;	author tedu;	state Exp;
branches;
next	1.162;
commitid	ZYUxNRICiD9sC1vn;

1.162
date	2014.11.19.20.09.01;	author mlarkin;	state Exp;
branches;
next	1.161;
commitid	A1R8j3OYk6zYWabw;

1.161
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.160;
commitid	yv0ECmCdICvq576h;

1.160
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.159;
commitid	7NtJNW9udCOFtDNM;

1.159
date	2014.01.06.14.29.25;	author sf;	state Exp;
branches;
next	1.158;

1.158
date	2013.02.13.20.45.41;	author kurt;	state Exp;
branches;
next	1.157;

1.157
date	2012.03.09.13.01.28;	author ariane;	state Exp;
branches;
next	1.156;

1.156
date	2012.02.19.17.14.28;	author kettenis;	state Exp;
branches;
next	1.155;

1.155
date	2011.07.08.03.35.39;	author kettenis;	state Exp;
branches;
next	1.154;

1.154
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.153;

1.153
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.152;

1.152
date	2011.03.12.03.52.26;	author guenther;	state Exp;
branches;
next	1.151;

1.151
date	2010.11.30.19.28.59;	author kettenis;	state Exp;
branches;
next	1.150;

1.150
date	2010.11.20.20.33.24;	author miod;	state Exp;
branches;
next	1.149;

1.149
date	2010.11.20.20.21.13;	author miod;	state Exp;
branches;
next	1.148;

1.148
date	2010.05.08.16.54.08;	author oga;	state Exp;
branches;
next	1.147;

1.147
date	2010.04.30.21.56.39;	author oga;	state Exp;
branches;
next	1.146;

1.146
date	2009.12.09.14.31.57;	author oga;	state Exp;
branches;
next	1.145;

1.145
date	2009.08.11.17.15.54;	author oga;	state Exp;
branches;
next	1.144;

1.144
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.143;

1.143
date	2009.07.25.12.55.39;	author miod;	state Exp;
branches;
next	1.142;

1.142
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.141;

1.141
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.140;

1.140
date	2009.06.03.02.31.48;	author art;	state Exp;
branches;
next	1.139;

1.139
date	2009.06.03.00.49.12;	author art;	state Exp;
branches;
next	1.138;

1.138
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.137;

1.137
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.136;

1.136
date	2009.02.05.01.13.21;	author oga;	state Exp;
branches;
next	1.135;

1.135
date	2009.01.27.22.14.13;	author miod;	state Exp;
branches;
next	1.134;

1.134
date	2008.12.18.14.17.28;	author kurt;	state Exp;
branches;
next	1.133;

1.133
date	2008.12.18.13.43.24;	author kurt;	state Exp;
branches;
next	1.132;

1.132
date	2008.11.24.16.32.45;	author art;	state Exp;
branches;
next	1.131;

1.131
date	2008.11.24.15.50.47;	author art;	state Exp;
branches;
next	1.130;

1.130
date	2008.11.24.03.19.42;	author kurt;	state Exp;
branches;
next	1.129;

1.129
date	2008.11.24.03.13.22;	author kurt;	state Exp;
branches;
next	1.128;

1.128
date	2008.11.14.20.43.54;	author weingart;	state Exp;
branches;
next	1.127;

1.127
date	2008.11.14.15.10.31;	author kurt;	state Exp;
branches;
next	1.126;

1.126
date	2008.11.06.19.14.26;	author deraadt;	state Exp;
branches;
next	1.125;

1.125
date	2008.11.06.19.13.31;	author deraadt;	state Exp;
branches;
next	1.124;

1.124
date	2008.10.24.06.34.55;	author deraadt;	state Exp;
branches;
next	1.123;

1.123
date	2008.06.14.18.09.47;	author hshoexer;	state Exp;
branches;
next	1.122;

1.122
date	2008.01.13.20.47.00;	author kettenis;	state Exp;
branches;
next	1.121;

1.121
date	2007.11.28.17.05.09;	author tedu;	state Exp;
branches;
next	1.120;

1.120
date	2007.11.16.16.16.06;	author deraadt;	state Exp;
branches;
next	1.119;

1.119
date	2007.06.27.16.16.53;	author art;	state Exp;
branches;
next	1.118;

1.118
date	2007.06.19.09.41.39;	author art;	state Exp;
branches;
next	1.117;

1.117
date	2007.06.07.15.31.09;	author art;	state Exp;
branches;
next	1.116;

1.116
date	2007.05.31.22.30.25;	author art;	state Exp;
branches;
next	1.115;

1.115
date	2007.05.29.18.18.20;	author tom;	state Exp;
branches;
next	1.114;

1.114
date	2007.05.28.18.31.11;	author krw;	state Exp;
branches;
next	1.113;

1.113
date	2007.05.27.21.33.25;	author tom;	state Exp;
branches;
next	1.112;

1.112
date	2007.05.25.15.55.26;	author art;	state Exp;
branches;
next	1.111;

1.111
date	2007.05.20.14.14.09;	author miod;	state Exp;
branches;
next	1.110;

1.110
date	2007.05.17.15.17.02;	author art;	state Exp;
branches;
next	1.109;

1.109
date	2007.05.04.05.52.28;	author art;	state Exp;
branches;
next	1.108;

1.108
date	2007.05.03.05.32.05;	author art;	state Exp;
branches;
next	1.107;

1.107
date	2007.04.28.17.34.33;	author art;	state Exp;
branches;
next	1.106;

1.106
date	2007.04.26.11.31.51;	author art;	state Exp;
branches;
next	1.105;

1.105
date	2007.04.19.18.34.23;	author art;	state Exp;
branches;
next	1.104;

1.104
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.103;

1.103
date	2007.04.13.13.27.46;	author art;	state Exp;
branches;
next	1.102;

1.102
date	2007.04.13.10.36.03;	author miod;	state Exp;
branches;
next	1.101;

1.101
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.100;

1.100
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.99;

1.99
date	2007.03.18.14.23.57;	author mickey;	state Exp;
branches;
next	1.98;

1.98
date	2007.03.13.15.11.41;	author art;	state Exp;
branches;
next	1.97;

1.97
date	2007.02.20.21.15.01;	author tom;	state Exp;
branches
	1.97.2.1;
next	1.96;

1.96
date	2007.02.03.16.48.23;	author miod;	state Exp;
branches;
next	1.95;

1.95
date	2006.09.19.11.06.33;	author jsg;	state Exp;
branches;
next	1.94;

1.94
date	2006.08.28.12.18.10;	author mickey;	state Exp;
branches
	1.94.2.1;
next	1.93;

1.93
date	2006.05.19.20.53.31;	author brad;	state Exp;
branches;
next	1.92;

1.92
date	2006.04.27.15.37.51;	author mickey;	state Exp;
branches;
next	1.91;

1.91
date	2006.03.13.18.42.16;	author mickey;	state Exp;
branches;
next	1.90;

1.90
date	2006.03.12.21.28.54;	author tom;	state Exp;
branches;
next	1.89;

1.89
date	2006.02.16.16.08.53;	author mickey;	state Exp;
branches
	1.89.2.1;
next	1.88;

1.88
date	2006.01.12.22.39.20;	author weingart;	state Exp;
branches;
next	1.87;

1.87
date	2005.12.25.21.39.06;	author miod;	state Exp;
branches;
next	1.86;

1.86
date	2005.12.10.11.45.41;	author miod;	state Exp;
branches;
next	1.85;

1.85
date	2005.11.18.17.05.04;	author brad;	state Exp;
branches;
next	1.84;

1.84
date	2005.11.13.14.23.26;	author martin;	state Exp;
branches;
next	1.83;

1.83
date	2005.09.25.20.48.21;	author miod;	state Exp;
branches;
next	1.82;

1.82
date	2005.07.12.21.12.03;	author hshoexer;	state Exp;
branches
	1.82.2.1;
next	1.81;

1.81
date	2005.05.24.21.11.46;	author tedu;	state Exp;
branches;
next	1.80;

1.80
date	2004.12.25.23.02.24;	author miod;	state Exp;
branches
	1.80.2.1;
next	1.79;

1.79
date	2004.07.20.20.18.13;	author art;	state Exp;
branches;
next	1.78;

1.78
date	2004.07.02.16.29.55;	author niklas;	state Exp;
branches;
next	1.77;

1.77
date	2004.06.23.17.42.46;	author niklas;	state Exp;
branches;
next	1.76;

1.76
date	2004.06.13.21.49.15;	author niklas;	state Exp;
branches;
next	1.75;

1.75
date	2004.02.01.12.26.45;	author grange;	state Exp;
branches;
next	1.74;

1.74
date	2004.01.29.19.01.54;	author tedu;	state Exp;
branches;
next	1.73;

1.73
date	2003.05.23.16.33.35;	author mpech;	state Exp;
branches;
next	1.72;

1.72
date	2003.05.13.03.49.04;	author art;	state Exp;
branches;
next	1.71;

1.71
date	2003.05.09.23.51.23;	author art;	state Exp;
branches;
next	1.70;

1.70
date	2003.05.05.17.54.59;	author drahn;	state Exp;
branches;
next	1.69;

1.69
date	2003.05.02.21.07.47;	author mickey;	state Exp;
branches;
next	1.68;

1.68
date	2003.04.17.03.56.20;	author drahn;	state Exp;
branches;
next	1.67;

1.67
date	2003.04.09.12.11.15;	author niklas;	state Exp;
branches;
next	1.66;

1.66
date	2003.04.07.06.14.30;	author niklas;	state Exp;
branches;
next	1.65;

1.65
date	2002.10.13.18.26.12;	author krw;	state Exp;
branches;
next	1.64;

1.64
date	2002.10.12.01.09.43;	author krw;	state Exp;
branches;
next	1.63;

1.63
date	2002.09.11.22.39.00;	author art;	state Exp;
branches;
next	1.62;

1.62
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2002.07.31.07.21.04;	author aaron;	state Exp;
branches;
next	1.60;

1.60
date	2002.07.31.02.30.29;	author mickey;	state Exp;
branches;
next	1.59;

1.59
date	2002.03.14.01.26.33;	author millert;	state Exp;
branches;
next	1.58;

1.58
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.57;

1.57
date	2001.12.19.08.58.05;	author art;	state Exp;
branches;
next	1.56;

1.56
date	2001.12.11.18.49.25;	author art;	state Exp;
branches
	1.56.2.1;
next	1.55;

1.55
date	2001.12.11.17.24.34;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2001.12.11.16.35.18;	author art;	state Exp;
branches;
next	1.53;

1.53
date	2001.12.10.17.27.01;	author art;	state Exp;
branches;
next	1.52;

1.52
date	2001.12.08.02.24.06;	author art;	state Exp;
branches;
next	1.51;

1.51
date	2001.11.28.16.13.28;	author art;	state Exp;
branches;
next	1.50;

1.50
date	2001.11.28.15.02.58;	author art;	state Exp;
branches;
next	1.49;

1.49
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2001.11.28.13.47.38;	author art;	state Exp;
branches;
next	1.47;

1.47
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.46;

1.46
date	2001.11.06.01.43.48;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2001.09.19.20.50.56;	author mickey;	state Exp;
branches;
next	1.44;

1.44
date	2001.08.11.11.45.27;	author art;	state Exp;
branches;
next	1.43;

1.43
date	2001.07.25.13.25.32;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2001.07.15.12.23.55;	author assar;	state Exp;
branches;
next	1.41;

1.41
date	2001.06.08.08.08.52;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2001.05.09.15.31.25;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2001.04.10.06.59.13;	author niklas;	state Exp;
branches;
next	1.38;

1.38
date	2001.03.22.23.36.51;	author niklas;	state Exp;
branches;
next	1.37;

1.37
date	2001.03.22.20.44.59;	author niklas;	state Exp;
branches;
next	1.36;

1.36
date	2000.05.04.18.55.06;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	2000.02.22.19.27.48;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2000.01.29.21.41.51;	author mickey;	state Exp;
branches
	1.34.2.1;
next	1.33;

1.33
date	99.11.30.06.44.51;	author art;	state Exp;
branches;
next	1.32;

1.32
date	99.09.03.18.00.51;	author art;	state Exp;
branches;
next	1.31;

1.31
date	99.08.25.09.13.53;	author ho;	state Exp;
branches;
next	1.30;

1.30
date	99.07.02.12.25.50;	author niklas;	state Exp;
branches;
next	1.29;

1.29
date	99.06.04.16.37.48;	author mickey;	state Exp;
branches;
next	1.28;

1.28
date	99.06.01.06.37.22;	author art;	state Exp;
branches;
next	1.27;

1.27
date	99.02.26.10.37.51;	author art;	state Exp;
branches;
next	1.26;

1.26
date	99.02.26.10.26.57;	author art;	state Exp;
branches;
next	1.25;

1.25
date	98.04.25.20.31.30;	author mickey;	state Exp;
branches;
next	1.24;

1.24
date	98.03.20.15.40.32;	author niklas;	state Exp;
branches;
next	1.23;

1.23
date	98.01.20.18.40.15;	author niklas;	state Exp;
branches;
next	1.22;

1.22
date	97.10.25.06.57.59;	author niklas;	state Exp;
branches;
next	1.21;

1.21
date	97.10.24.22.15.07;	author mickey;	state Exp;
branches;
next	1.20;

1.20
date	97.09.24.22.28.15;	author niklas;	state Exp;
branches;
next	1.19;

1.19
date	97.09.22.21.03.54;	author niklas;	state Exp;
branches;
next	1.18;

1.18
date	97.08.17.17.31.37;	author grr;	state Exp;
branches;
next	1.17;

1.17
date	97.07.28.23.46.10;	author mickey;	state Exp;
branches;
next	1.16;

1.16
date	97.01.07.05.37.32;	author tholo;	state Exp;
branches;
next	1.15;

1.15
date	96.10.25.11.14.13;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	96.10.18.15.33.08;	author mickey;	state Exp;
branches;
next	1.13;

1.13
date	96.09.26.14.04.27;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	96.06.16.10.24.19;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	96.06.02.10.44.21;	author mickey;	state Exp;
branches;
next	1.10;

1.10
date	96.05.30.09.30.08;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	96.05.07.07.21.51;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	96.04.21.22.16.33;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	96.04.18.04.04.54;	author mickey;	state Exp;
branches;
next	1.6;

1.6
date	96.04.17.05.18.55;	author mickey;	state Exp;
branches;
next	1.5;

1.5
date	96.03.19.21.09.22;	author mickey;	state Exp;
branches;
next	1.4;

1.4
date	96.02.17.22.32.31;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	95.11.28.16.43.47;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.11.13.04.53.26;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.50.34;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.50.34;	author deraadt;	state Exp;
branches;
next	;

1.34.2.1
date	2000.03.02.07.04.28;	author niklas;	state Exp;
branches;
next	1.34.2.2;

1.34.2.2
date	2001.04.18.16.07.21;	author niklas;	state Exp;
branches;
next	1.34.2.3;

1.34.2.3
date	2001.07.04.10.16.39;	author niklas;	state Exp;
branches;
next	1.34.2.4;

1.34.2.4
date	2001.07.14.10.02.30;	author ho;	state Exp;
branches;
next	1.34.2.5;

1.34.2.5
date	2001.07.15.13.38.16;	author ho;	state Exp;
branches;
next	1.34.2.6;

1.34.2.6
date	2001.10.31.03.01.12;	author nate;	state Exp;
branches;
next	1.34.2.7;

1.34.2.7
date	2001.11.13.21.00.52;	author niklas;	state Exp;
branches;
next	1.34.2.8;

1.34.2.8
date	2001.12.05.00.39.10;	author niklas;	state Exp;
branches;
next	1.34.2.9;

1.34.2.9
date	2002.03.06.01.01.00;	author niklas;	state Exp;
branches;
next	1.34.2.10;

1.34.2.10
date	2002.03.28.10.31.04;	author niklas;	state Exp;
branches;
next	1.34.2.11;

1.34.2.11
date	2003.03.27.23.26.55;	author niklas;	state Exp;
branches;
next	1.34.2.12;

1.34.2.12
date	2003.04.05.20.43.38;	author niklas;	state Exp;
branches;
next	1.34.2.13;

1.34.2.13
date	2003.04.06.16.19.07;	author niklas;	state Exp;
branches;
next	1.34.2.14;

1.34.2.14
date	2003.04.15.03.53.47;	author niklas;	state Exp;
branches;
next	1.34.2.15;

1.34.2.15
date	2003.05.13.19.42.08;	author ho;	state Exp;
branches;
next	1.34.2.16;

1.34.2.16
date	2003.05.16.00.29.39;	author niklas;	state Exp;
branches;
next	1.34.2.17;

1.34.2.17
date	2003.05.16.01.03.19;	author niklas;	state Exp;
branches;
next	1.34.2.18;

1.34.2.18
date	2003.05.18.17.41.16;	author niklas;	state Exp;
branches;
next	1.34.2.19;

1.34.2.19
date	2003.06.07.11.11.37;	author ho;	state Exp;
branches;
next	1.34.2.20;

1.34.2.20
date	2004.02.19.10.48.42;	author niklas;	state Exp;
branches;
next	1.34.2.21;

1.34.2.21
date	2004.02.20.22.19.55;	author niklas;	state Exp;
branches;
next	1.34.2.22;

1.34.2.22
date	2004.03.30.09.08.38;	author niklas;	state Exp;
branches;
next	1.34.2.23;

1.34.2.23
date	2004.04.05.10.15.56;	author niklas;	state Exp;
branches;
next	1.34.2.24;

1.34.2.24
date	2004.04.06.13.36.43;	author niklas;	state Exp;
branches;
next	1.34.2.25;

1.34.2.25
date	2004.06.06.18.21.15;	author grange;	state Exp;
branches;
next	1.34.2.26;

1.34.2.26
date	2004.06.08.21.43.12;	author grange;	state Exp;
branches;
next	;

1.56.2.1
date	2002.01.31.22.55.11;	author niklas;	state Exp;
branches;
next	1.56.2.2;

1.56.2.2
date	2002.02.02.03.28.25;	author art;	state Exp;
branches;
next	1.56.2.3;

1.56.2.3
date	2002.06.11.03.35.53;	author art;	state Exp;
branches;
next	1.56.2.4;

1.56.2.4
date	2002.10.29.00.28.04;	author art;	state Exp;
branches;
next	1.56.2.5;

1.56.2.5
date	2003.05.19.21.45.11;	author tedu;	state Exp;
branches;
next	;

1.80.2.1
date	2006.01.13.00.49.21;	author brad;	state Exp;
branches;
next	1.80.2.2;

1.80.2.2
date	2006.05.02.02.42.51;	author brad;	state Exp;
branches;
next	;

1.82.2.1
date	2006.01.13.01.56.55;	author brad;	state Exp;
branches;
next	1.82.2.2;

1.82.2.2
date	2006.05.02.02.32.19;	author brad;	state Exp;
branches;
next	;

1.89.2.1
date	2006.05.02.02.40.02;	author brad;	state Exp;
branches;
next	;

1.94.2.1
date	2007.07.05.11.09.39;	author reyk;	state Exp;
branches;
next	;

1.97.2.1
date	2007.07.05.11.07.57;	author reyk;	state Exp;
branches;
next	;


desc
@@


1.196
log
@Kill SPINLOCK_SPIN_HOOK, use CPU_BUSY_CYCLE() instead.

ok visa@@, kettenis@@
@
text
@/*	$OpenBSD: pmap.c,v 1.195 2016/10/21 06:20:58 mlarkin Exp $	*/
/*	$NetBSD: pmap.c,v 1.91 2000/06/02 17:46:37 thorpej Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * pmap.c: i386 pmap module rewrite
 * Chuck Cranor <chuck@@ccrc.wustl.edu>
 * 11-Aug-97
 *
 * history of this pmap module: in addition to my own input, i used
 *    the following references for this rewrite of the i386 pmap:
 *
 * [1] the NetBSD i386 pmap.   this pmap appears to be based on the
 *     BSD hp300 pmap done by Mike Hibler at University of Utah.
 *     it was then ported to the i386 by William Jolitz of UUNET
 *     Technologies, Inc.   Then Charles M. Hannum of the NetBSD
 *     project fixed some bugs and provided some speed ups.
 *
 * [2] the FreeBSD i386 pmap.   this pmap seems to be the
 *     Hibler/Jolitz pmap, as modified for FreeBSD by John S. Dyson
 *     and David Greenman.
 *
 * [3] the Mach pmap.   this pmap, from CMU, seems to have migrated
 *     between several processors.   the VAX version was done by
 *     Avadis Tevanian, Jr., and Michael Wayne Young.    the i386
 *     version was done by Lance Berc, Mike Kupfer, Bob Baron,
 *     David Golub, and Richard Draves.    the alpha version was
 *     done by Alessandro Forin (CMU/Mach) and Chris Demetriou
 *     (NetBSD/alpha).
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/atomic.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/user.h>
#include <sys/kernel.h>
#include <sys/mutex.h>

#include <uvm/uvm.h>

#include <machine/cpu.h>
#include <machine/specialreg.h>
#include <machine/gdt.h>

#include <dev/isa/isareg.h>
#include <sys/msgbuf.h>
#include <stand/boot/bootarg.h>

#include "vmm.h"

/*
 * this file contains the code for the "pmap module."   the module's
 * job is to manage the hardware's virtual to physical address mappings.
 * note that there are two levels of mapping in the VM system:
 *
 *  [1] the upper layer of the VM system uses vm_map's and vm_map_entry's
 *      to map ranges of virtual address space to objects/files.  for
 *      example, the vm_map may say: "map VA 0x1000 to 0x22000 read-only
 *      to the file /bin/ls starting at offset zero."   note that
 *      the upper layer mapping is not concerned with how individual
 *      vm_pages are mapped.
 *
 *  [2] the lower layer of the VM system (the pmap) maintains the mappings
 *      from virtual addresses.   it is concerned with which vm_page is
 *      mapped where.   for example, when you run /bin/ls and start
 *      at page 0x1000 the fault routine may lookup the correct page
 *      of the /bin/ls file and then ask the pmap layer to establish
 *      a mapping for it.
 *
 * note that information in the lower layer of the VM system can be
 * thrown away since it can easily be reconstructed from the info
 * in the upper layer.
 *
 * data structures we use include:
 *
 *  - struct pmap: describes the address space of one thread
 *  - struct pv_entry: describes one <PMAP,VA> mapping of a PA
 *  - struct pv_head: there is one pv_head per managed page of
 *	physical memory.   the pv_head points to a list of pv_entry
 *	structures which describe all the <PMAP,VA> pairs that this
 *      page is mapped in.    this is critical for page based operations
 *      such as pmap_page_protect() [change protection on _all_ mappings
 *      of a page]
 */
/*
 * i386 MMU hardware structure:
 *
 * the i386 MMU is a two-level MMU which maps 4GB of virtual memory.
 * the pagesize is 4K (4096 [0x1000] bytes), although newer pentium
 * processors can support a 4MB pagesize as well.
 *
 * the first level table (segment table?) is called a "page directory"
 * and it contains 1024 page directory entries (PDEs).   each PDE is
 * 4 bytes (an int), so a PD fits in a single 4K page.   this page is
 * the page directory page (PDP).  each PDE in a PDP maps 4MB of space
 * (1024 * 4MB = 4GB).   a PDE contains the physical address of the
 * second level table: the page table.   or, if 4MB pages are being used,
 * then the PDE contains the PA of the 4MB page being mapped.
 *
 * a page table consists of 1024 page table entries (PTEs).  each PTE is
 * 4 bytes (an int), so a page table also fits in a single 4K page.  a
 * 4K page being used as a page table is called a page table page (PTP).
 * each PTE in a PTP maps one 4K page (1024 * 4K = 4MB).   a PTE contains
 * the physical address of the page it maps and some flag bits (described
 * below).
 *
 * the processor has a special register, "cr3", which points to the
 * the PDP which is currently controlling the mappings of the virtual
 * address space.
 *
 * the following picture shows the translation process for a 4K page:
 *
 * %cr3 register [PA of PDP]
 *      |
 *      |
 *      |   bits <31-22> of VA         bits <21-12> of VA   bits <11-0>
 *      |   index the PDP (0 - 1023)   index the PTP        are the page offset
 *      |         |                           |                  |
 *      |         v                           |                  |
 *      +--->+----------+                     |                  |
 *           | PD Page  |   PA of             v                  |
 *           |          |---PTP-------->+------------+           |
 *           | 1024 PDE |               | page table |--PTE--+   |
 *           | entries  |               | (aka PTP)  |       |   |
 *           +----------+               | 1024 PTE   |       |   |
 *                                      | entries    |       |   |
 *                                      +------------+       |   |
 *                                                           |   |
 *                                                bits <31-12>   bits <11-0>
 *                                                p h y s i c a l  a d d r
 *
 * the i386 caches PTEs in a TLB.   it is important to flush out old
 * TLB mappings when making a change to a mapping.   writing to the
 * %cr3 will flush the entire TLB.    newer processors also have an
 * instruction that will invalidate the mapping of a single page (which
 * is useful if you are changing a single mapping because it preserves
 * all the cached TLB entries).
 *
 * as shows, bits 31-12 of the PTE contain PA of the page being mapped.
 * the rest of the PTE is defined as follows:
 *   bit#	name	use
 *   11		n/a	available for OS use, hardware ignores it
 *   10		n/a	available for OS use, hardware ignores it
 *   9		n/a	available for OS use, hardware ignores it
 *   8		G	global bit (see discussion below)
 *   7		PS	page size [for PDEs] (0=4k, 1=4M <if supported>)
 *   6		D	dirty (modified) page
 *   5		A	accessed (referenced) page
 *   4		PCD	cache disable
 *   3		PWT	prevent write through (cache)
 *   2		U/S	user/supervisor bit (0=supervisor only, 1=both u&s)
 *   1		R/W	read/write bit (0=read only, 1=read-write)
 *   0		P	present (valid)
 *
 * notes:
 *  - on the i386 the R/W bit is ignored if processor is in supervisor
 *    state (bug!)
 *  - PS is only supported on newer processors
 *  - PTEs with the G bit are global in the sense that they are not
 *    flushed from the TLB when %cr3 is written (to flush, use the
 *    "flush single page" instruction).   this is only supported on
 *    newer processors.    this bit can be used to keep the kernel's
 *    TLB entries around while context switching.   since the kernel
 *    is mapped into all processes at the same place it does not make
 *    sense to flush these entries when switching from one process'
 *    pmap to another.
 */
/*
 * A pmap describes a process' 4GB virtual address space.  This
 * virtual address space can be broken up into 1024 4MB regions which
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
 *
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
 *
 * PDE#s	VA range		Usage
 * 0->831	0x0 -> 0xcfc00000	user address space, note that the
 *					max user address is 0xcfbfe000
 *					the final two pages in the last 4MB
 *					used to be reserved for the UAREA
 *					but now are no longer used.
 * 831		0xcfc00000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 832->1023	0xd0000000->		kernel address space (constant
 *			0xffc00000	across all pmaps/processes).
 * 1023		0xffc00000->		"alternate" recursive PDP mapping
 *			<end>		(for other pmaps).
 *
 *
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
 * words, the PTE for page 0 is the first int mapped into the 4MB recursive
 * area.  The PTE for page 1 is the second int.  The very last int in the
 * 4MB range is the PTE that maps VA 0xffffe000 (the last page in a 4GB
 * address).
 *
 * All pmaps' PDs must have the same values in slots 832->1023 so that
 * the kernel is always mapped in every process.  These values are loaded
 * into the PD at pmap creation time.
 *
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
 * will have all its PTEs mapped into memory at the recursive mapping
 * point (slot #831 as show above).  When the pmap code wants to find the
 * PTE for a virtual address, all it has to do is the following:
 *
 * Address of PTE = (831 * 4MB) + (VA / PAGE_SIZE) * sizeof(pt_entry_t)
 *                = 0xcfc00000 + (VA / 4096) * 4
 *
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
 * case we take the PA of the PDP of the non-active pmap and put it in
 * slot 1023 of the active pmap.  This causes the non-active pmap's
 * PTEs to get mapped in the final 4MB of the 4GB address space
 * (e.g. starting at 0xffc00000).
 *
 * The following figure shows the effects of the recursive PDP mapping:
 *
 *   PDP (%cr3)
 *   +----+
 *   |   0| -> PTP#0 that maps VA 0x0 -> 0x400000
 *   |    |
 *   |    |
 *   | 831| -> points back to PDP (%cr3) mapping VA 0xcfc00000 -> 0xd0000000
 *   | 832| -> first kernel PTP (maps 0xd0000000 -> 0xe0400000)
 *   |    |
 *   |1023| -> points to alternate pmap's PDP (maps 0xffc00000 -> end)
 *   +----+
 *
 * Note that the PDE#831 VA (0xcfc00000) is defined as "PTE_BASE".
 * Note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE".
 *
 * Starting at VA 0xcfc00000 the current active PDP (%cr3) acts as a
 * PTP:
 *
 * PTP#831 == PDP(%cr3) => maps VA 0xcfc00000 -> 0xd0000000
 *   +----+
 *   |   0| -> maps the contents of PTP#0 at VA 0xcfc00000->0xcfc01000
 *   |    |
 *   |    |
 *   | 831| -> maps the contents of PTP#831 (the PDP) at VA 0xcff3f000
 *   | 832| -> maps the contents of first kernel PTP
 *   |    |
 *   |1023|
 *   +----+
 *
 * Note that mapping of the PDP at PTP#831's VA (0xcff3f000) is
 * defined as "PDP_BASE".... within that mapping there are two
 * defines:
 *   "PDP_PDE" (0xcff3fcfc) is the VA of the PDE in the PDP
 *      which points back to itself.
 *   "APDP_PDE" (0xcff3fffc) is the VA of the PDE in the PDP which
 *      establishes the recursive mapping of the alternate pmap.
 *      To set the alternate PDP, one just has to put the correct
 *	PA info in *APDP_PDE.
 *
 * Note that in the APTE_BASE space, the APDP appears at VA
 * "APDP_BASE" (0xfffff000).
 */
#define PG_FRAME	0xfffff000	/* page frame mask */
#define PG_LGFRAME	0xffc00000	/* large (4M) page frame mask */

/*
 * The following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
 */
#define PTE_BASE	((pt_entry_t *) (PDSLOT_PTE * NBPD))
#define APTE_BASE	((pt_entry_t *) (PDSLOT_APTE * NBPD))
#define PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
#define PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define APDP_PDE	(PDP_BASE + PDSLOT_APTE)

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define PD_MASK		0xffc00000	/* page directory address bits */
#define PT_MASK		0x003ff000	/* page table address bits */
#define pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)

/*
 * Mach derived conversion macros
 */
#define i386_round_pdr(x)	((((unsigned)(x)) + ~PD_MASK) & PD_MASK)

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 */
#define vtopte(VA)	(PTE_BASE + atop((vaddr_t)VA))

/*
 * PTP macros:
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
 *
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
 */

#define ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */

/*
 * Access PD and PT
 */
#define PDE(pm,i)	(((pd_entry_t *)(pm)->pm_pdir)[(i)])

/*
 * here we define the data types for PDEs and PTEs
 */
typedef u_int32_t pd_entry_t;		/* PDE */
typedef u_int32_t pt_entry_t;		/* PTE */

/*
 * Number of PTEs per cache line. 4 byte pte, 64-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL			16

/*
 * global data structures
 */

/* The kernel's pmap (proc0), 32 byte aligned in case we are using PAE */
struct pmap __attribute__ ((aligned (32))) kernel_pmap_store;

/*
 * nkpde is the number of kernel PTPs allocated for the kernel at
 * boot time (NKPTP is a compile time override).   this number can
 * grow dynamically as needed (but once allocated, we never free
 * kernel PTPs).
 */

int nkpde = NKPTP;
int nkptp_max = 1024 - (KERNBASE / NBPD) - 1;

extern int cpu_pae;

/*
 * pmap_pg_g: if our processor supports PG_G in the PTE then we
 * set pmap_pg_g to PG_G (otherwise it is zero).
 */

int pmap_pg_g = 0;

/*
 * pmap_pg_wc: if our processor supports PAT then we set this
 * to be the pte bits for Write Combining. Else we fall back to
 * UC- so mtrrs can override the cacheability
 */
int pmap_pg_wc = PG_UCMINUS;

/*
 * other data structures
 */

uint32_t protection_codes[8];		/* maps MI prot to i386 prot code */
boolean_t pmap_initialized = FALSE;	/* pmap_init done yet? */

/*
 * MULTIPROCESSOR: special VA's/ PTE's are actually allocated inside a
 * MAXCPUS*NPTECL array of PTE's, to avoid cache line thrashing
 * due to false sharing.
 */

#ifdef MULTIPROCESSOR
#define PTESLEW(pte, id) ((pte)+(id)*NPTECL)
#define VASLEW(va,id) ((va)+(id)*NPTECL*NBPG)
#else
#define PTESLEW(pte, id) (pte)
#define VASLEW(va,id) (va)
#endif

/*
 * pv management structures.
 */
struct pool pmap_pv_pool;

#define PVE_LOWAT (PVE_PER_PVPAGE / 2)	/* free pv_entry low water mark */
#define PVE_HIWAT (PVE_LOWAT + (PVE_PER_PVPAGE * 2))
					/* high water mark */

/*
 * the following two vaddr_t's are used during system startup
 * to keep track of how much of the kernel's VM space we have used.
 * once the system is started, the management of the remaining kernel
 * VM space is turned over to the kernel_map vm_map.
 */

static vaddr_t virtual_avail;	/* VA of first free KVA */
static vaddr_t virtual_end;	/* VA of last free KVA */

/*
 * linked list of all non-kernel pmaps
 */

struct pmap_head pmaps;

/*
 * pool that pmap structures are allocated from
 */

struct pool pmap_pmap_pool;

/*
 * special VAs and the PTEs that map them
 */

pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte, *flsh_pte;
caddr_t pmap_csrcp, pmap_cdstp, pmap_zerop, pmap_ptpp, pmap_flshp;
caddr_t vmmap; /* XXX: used by mem.c... it should really uvm_map_reserve it */

/*
 * local prototypes
 */
struct vm_page	*pmap_alloc_ptp_86(struct pmap *, int, pt_entry_t);
struct vm_page	*pmap_get_ptp_86(struct pmap *, int);
pt_entry_t	*pmap_map_ptes_86(struct pmap *);
void		 pmap_unmap_ptes_86(struct pmap *);
void		 pmap_do_remove_86(struct pmap *, vaddr_t, vaddr_t, int);
void		 pmap_remove_ptes_86(struct pmap *, struct vm_page *, vaddr_t,
		    vaddr_t, vaddr_t, int, struct pv_entry **);
void		*pmap_pv_page_alloc(struct pool *, int, int *);
void		pmap_pv_page_free(struct pool *, void *);

struct pool_allocator pmap_pv_page_allocator = {
	pmap_pv_page_alloc, pmap_pv_page_free,
};

void		 pmap_sync_flags_pte_86(struct vm_page *, pt_entry_t);

void		 pmap_drop_ptp_86(struct pmap *, vaddr_t, struct vm_page *,
    pt_entry_t *);

void		 setcslimit(struct pmap *, struct trapframe *, struct pcb *,
		     vaddr_t);
void		 pmap_pinit_pd_86(struct pmap *);

static __inline u_int
pmap_pte2flags(pt_entry_t pte)
{
	return (((pte & PG_U) ? PG_PMAP_REF : 0) |
	    ((pte & PG_M) ? PG_PMAP_MOD : 0));
}

void
pmap_sync_flags_pte_86(struct vm_page *pg, pt_entry_t pte)
{
	if (pte & (PG_U|PG_M)) {
		atomic_setbits_int(&pg->pg_flags, pmap_pte2flags(pte));
	}
}

void
pmap_apte_flush(void)
{
	pmap_tlb_shoottlb();
	pmap_tlb_shootwait();
}

/*
 * pmap_map_ptes: map a pmap's PTEs into KVM and lock them in
 *
 * => we lock enough pmaps to keep things locked in
 * => must be undone with pmap_unmap_ptes before returning
 */

pt_entry_t *
pmap_map_ptes_86(struct pmap *pmap)
{
	pd_entry_t opde;

	/* the kernel's pmap is always accessible */
	if (pmap == pmap_kernel()) {
		return(PTE_BASE);
	}

	mtx_enter(&pmap->pm_mtx);

	/* if curpmap then we are always mapped */
	if (pmap_is_curpmap(pmap)) {
		return(PTE_BASE);
	}

	mtx_enter(&curcpu()->ci_curpmap->pm_apte_mtx);

	/* need to load a new alternate pt space into curpmap? */
	opde = *APDP_PDE;
#if defined(MULTIPROCESSOR) && defined(DIAGNOSTIC)
	if (pmap_valid_entry(opde))
		panic("pmap_map_ptes_86: APTE valid");
#endif
	if (!pmap_valid_entry(opde) || (opde & PG_FRAME) != pmap->pm_pdirpa) {
		*APDP_PDE = (pd_entry_t) (pmap->pm_pdirpa | PG_RW | PG_V |
		    PG_U | PG_M);
		if (pmap_valid_entry(opde))
			pmap_apte_flush();
	}
	return(APTE_BASE);
}

/*
 * pmap_unmap_ptes: unlock the PTE mapping of "pmap"
 */

void
pmap_unmap_ptes_86(struct pmap *pmap)
{
	if (pmap == pmap_kernel())
		return;

	if (!pmap_is_curpmap(pmap)) {
#if defined(MULTIPROCESSOR)
		*APDP_PDE = 0;
		pmap_apte_flush();
#endif
		mtx_leave(&curcpu()->ci_curpmap->pm_apte_mtx);
	}

	mtx_leave(&pmap->pm_mtx);
}

void
pmap_exec_account(struct pmap *pm, vaddr_t va,
    uint32_t opte, uint32_t npte)
{
	if (pm == pmap_kernel())
		return;

	if (curproc->p_vmspace == NULL ||
	    pm != vm_map_pmap(&curproc->p_vmspace->vm_map))
		return;

	if ((opte ^ npte) & PG_X)
		pmap_tlb_shootpage(pm, va);
			
	/*
	 * Executability was removed on the last executable change.
	 * Reset the code segment to something conservative and
	 * let the trap handler deal with setting the right limit.
	 * We can't do that because of locking constraints on the vm map.
	 *
	 * XXX - floating cs - set this _really_ low.
	 */
	if ((opte & PG_X) && (npte & PG_X) == 0 && va == pm->pm_hiexec) {
		struct trapframe *tf = curproc->p_md.md_regs;
		struct pcb *pcb = &curproc->p_addr->u_pcb;

		pm->pm_hiexec = I386_MAX_EXE_ADDR;
		setcslimit(pm, tf, pcb, I386_MAX_EXE_ADDR);
	}
}

#define SEGDESC_LIMIT(sd) (ptoa(((sd).sd_hilimit << 16) | (sd).sd_lolimit))

/*
 * Fixup the code segment to cover all potential executable mappings.
 * Called by kernel SEGV trap handler.
 * returns 0 if no changes to the code segment were made.
 */
int
pmap_exec_fixup(struct vm_map *map, struct trapframe *tf, struct pcb *pcb)
{
	struct vm_map_entry *ent;
	struct pmap *pm = vm_map_pmap(map);
	vaddr_t va = 0;
	vaddr_t pm_cs, gdt_cs;

	vm_map_lock(map);
	RBT_FOREACH_REVERSE(ent, uvm_map_addr, &map->addr) {
		if (ent->protection & PROT_EXEC)
			break;
	}
	/*
	 * This entry has greater va than the entries before.
	 * We need to make it point to the last page, not past it.
	 */
	if (ent)
		va = trunc_page(ent->end - 1);
	vm_map_unlock(map);

	pm_cs = SEGDESC_LIMIT(pm->pm_codeseg);
	gdt_cs = SEGDESC_LIMIT(curcpu()->ci_gdt[GUCODE_SEL].sd);

	/*
	 * Another thread running on another cpu can change
	 * pm_hiexec and pm_codeseg. If this has happened
	 * during our timeslice, our gdt code segment will
	 * be stale. So only allow the fault through if the
	 * faulting address is less then pm_hiexec and our
	 * gdt code segment is not stale.
	 */
	if (va <= pm->pm_hiexec && pm_cs == pm->pm_hiexec &&
	    gdt_cs == pm->pm_hiexec) {
		return (0);
	}

	pm->pm_hiexec = va;

	/*
	 * We have a new 'highest executable' va, so we need to update
	 * the value for the code segment limit, which is stored in the
	 * PCB.
	 */
	setcslimit(pm, tf, pcb, va);

	return (1);
}

u_int32_t
pmap_pte_set_86(vaddr_t va, paddr_t pa, u_int32_t bits)
{
	pt_entry_t pte, *ptep = vtopte(va);

	pa &= PMAP_PA_MASK;

	pte = i386_atomic_testset_ul(ptep, pa | bits);  /* zap! */
	return (pte & ~PG_FRAME);
}

u_int32_t
pmap_pte_setbits_86(vaddr_t va, u_int32_t set, u_int32_t clr)
{
	pt_entry_t *ptep = vtopte(va);
	pt_entry_t pte = *ptep;

	*ptep = (pte | set) & ~clr;
	return (pte & ~PG_FRAME);
}

u_int32_t
pmap_pte_bits_86(vaddr_t va)
{
	pt_entry_t *ptep = vtopte(va);

	return (*ptep & ~PG_FRAME);
}

paddr_t
pmap_pte_paddr_86(vaddr_t va)
{
	pt_entry_t *ptep = vtopte(va);

	return (*ptep & PG_FRAME);
}

/*
 * pmap_tmpmap_pa: map a page in for tmp usage
 */

vaddr_t
pmap_tmpmap_pa(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte;
	caddr_t ptpva;

	if (cpu_pae)
		return pmap_tmpmap_pa_pae(pa);

	ptpte = PTESLEW(ptp_pte, id);
	ptpva = VASLEW(pmap_ptpp, id);

#if defined(DIAGNOSTIC)
	if (*ptpte)
		panic("pmap_tmpmap_pa: ptp_pte in use?");
#endif
	*ptpte = PG_V | PG_RW | pa;	/* always a new mapping */
	return((vaddr_t)ptpva);
}

/*
 * pmap_tmpunmap_pa: unmap a tmp use page (undoes pmap_tmpmap_pa)
 */

void
pmap_tmpunmap_pa(void)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte;
	caddr_t ptpva;

	if (cpu_pae) {
		pmap_tmpunmap_pa_pae();
		return;
	}

	ptpte = PTESLEW(ptp_pte, id);
	ptpva = VASLEW(pmap_ptpp, id);

#if defined(DIAGNOSTIC)
	if (!pmap_valid_entry(*ptpte))
		panic("pmap_tmpunmap_pa: our pte invalid?");
#endif

	*ptpte = 0;
	pmap_update_pg((vaddr_t)ptpva);
#ifdef MULTIPROCESSOR
	/*
	 * No need for tlb shootdown here, since ptp_pte is per-CPU.
	 */
#endif
}

paddr_t
vtophys(vaddr_t va)
{
	if (cpu_pae)
		return vtophys_pae(va);
	else
		return ((*vtopte(va) & PG_FRAME) | (va & ~PG_FRAME));
}

void
setcslimit(struct pmap *pm, struct trapframe *tf, struct pcb *pcb,
    vaddr_t limit)
{
	/*
	 * Called when we have a new 'highest executable' va, so we need
	 * to update the value for the code segment limit, which is stored
	 * in the PCB.
	 *
	 * There are no caching issues to be concerned with: the
	 * processor reads the whole descriptor from the GDT when the
	 * appropriate selector is loaded into a segment register, and
	 * this only happens on the return to userland.
	 *
	 * This also works in the MP case, since whichever CPU gets to
	 * run the process will pick up the right descriptor value from
	 * the PCB.
	 */
	limit = min(limit, VM_MAXUSER_ADDRESS - 1);

	setsegment(&pm->pm_codeseg, 0, atop(limit),
	    SDT_MEMERA, SEL_UPL, 1, 1);

	/* And update the GDT since we may be called by the
	 * trap handler (cpu_switch won't get a chance).
	 */
	curcpu()->ci_gdt[GUCODE_SEL].sd = pm->pm_codeseg;

	pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
}

/*
 * p m a p   k e n t e r   f u n c t i o n s
 *
 * functions to quickly enter/remove pages from the kernel address
 * space.   pmap_kremove is exported to MI kernel.  we make use of
 * the recursive PTE mappings.
 */

/*
 * pmap_kenter_pa: enter a kernel mapping without R/M (pv_entry) tracking
 *
 * => no need to lock anything, assume va is already allocated
 * => should be faster than normal pmap enter function
 */

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	uint32_t bits;
	uint32_t global = 0;

	/* special 1:1 mappings in the first large page must not be global */
	if (!cpu_pae) {
		if (va >= (vaddr_t)NBPD)	/* 4MB pages on non-PAE */
			global = pmap_pg_g;
	} else {
		if (va >= (vaddr_t)NBPD / 2)	/* 2MB pages on PAE */
			global = pmap_pg_g;
	}

	bits = pmap_pte_set(va, pa, ((prot & PROT_WRITE) ? PG_RW : PG_RO) |
		PG_V | global | PG_U | PG_M |
		((prot & PROT_EXEC) ? PG_X : 0) |
		((pa & PMAP_NOCACHE) ? PG_N : 0) |
		((pa & PMAP_WC) ? pmap_pg_wc : 0));
	if (pmap_valid_entry(bits)) {
		if (pa & PMAP_NOCACHE && (bits & PG_N) == 0)
			wbinvd();
		/* NB. - this should not happen. */
		pmap_tlb_shootpage(pmap_kernel(), va);
		pmap_tlb_shootwait();
	}
}

/*
 * pmap_kremove: remove a kernel mapping(s) without R/M (pv_entry) tracking
 *
 * => no need to lock anything
 * => caller must dispose of any vm_page mapped in the va range
 * => note: not an inline function
 * => we assume the va is page aligned and the len is a multiple of PAGE_SIZE
 */

void
pmap_kremove(vaddr_t sva, vsize_t len)
{
	uint32_t bits;
	vaddr_t va, eva;

	eva = sva + len;

	for (va = sva; va != eva; va += PAGE_SIZE) {
		bits = pmap_pte_set(va, 0, 0);
#ifdef DIAGNOSTIC
		if (bits & PG_PVLIST)
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx", va);
#endif
	}
	pmap_tlb_shootrange(pmap_kernel(), sva, eva);
	pmap_tlb_shootwait();
}

/*
 * p m a p   i n i t   f u n c t i o n s
 *
 * pmap_bootstrap and pmap_init are called during system startup
 * to init the pmap module.   pmap_bootstrap() does a low level
 * init just to get things rolling.   pmap_init() finishes the job.
 */

/*
 * pmap_bootstrap: get the system in a state where it can run with VM
 *	properly enabled (called before main()).   the VM system is
 *      fully init'd later...
 *
 * => on i386, locore.s has already enabled the MMU by allocating
 *	a PDP for the kernel, and nkpde PTPs for the kernel.
 * => kva_start is the first free virtual address in kernel space
 */

void
pmap_bootstrap(vaddr_t kva_start)
{
	struct pmap *kpm;
	vaddr_t kva;
	pt_entry_t *pte;

	/*
	 * set the page size (default value is 4K which is ok)
	 */

	uvm_setpagesize();

	/*
	 * a quick sanity check
	 */

	if (PAGE_SIZE != NBPG)
		panic("pmap_bootstrap: PAGE_SIZE != NBPG");

	/*
	 * set up our local static global vars that keep track of the
	 * usage of KVM before kernel_map is set up
	 */

	virtual_avail = kva_start;		/* first free KVA */
	virtual_end = VM_MAX_KERNEL_ADDRESS;	/* last KVA */

	/*
	 * set up protection_codes: we need to be able to convert from
	 * a MI protection code (some combo of VM_PROT...) to something
	 * we can jam into a i386 PTE.
	 */

	protection_codes[PROT_NONE] = 0;  			/* --- */
	protection_codes[PROT_EXEC] = PG_X;			/* --x */
	protection_codes[PROT_READ] = PG_RO;			/* -r- */
	protection_codes[PROT_READ | PROT_EXEC] = PG_X;		/* -rx */
	protection_codes[PROT_WRITE] = PG_RW;			/* w-- */
	protection_codes[PROT_WRITE | PROT_EXEC] = PG_RW|PG_X;	/* w-x */
	protection_codes[PROT_READ | PROT_WRITE] = PG_RW;	/* wr- */
	protection_codes[PROT_READ | PROT_WRITE | PROT_EXEC] = PG_RW|PG_X; /* wrx */

	/*
	 * now we init the kernel's pmap
	 *
	 * the kernel pmap's pm_obj is not used for much.   however, in
	 * user pmaps the pm_obj contains the list of active PTPs.
	 * the pm_obj currently does not have a pager.   it might be possible
	 * to add a pager that would allow a process to read-only mmap its
	 * own page tables (fast user level vtophys?).   this may or may not
	 * be useful.
	 */

	kpm = pmap_kernel();
	uvm_objinit(&kpm->pm_obj, NULL, 1);
	bzero(&kpm->pm_list, sizeof(kpm->pm_list));  /* pm_list not used */
	kpm->pm_pdir = (vaddr_t)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
	kpm->pm_pdirpa = proc0.p_addr->u_pcb.pcb_cr3;
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count =
		atop(kva_start - VM_MIN_KERNEL_ADDRESS);
	kpm->pm_type = PMAP_TYPE_NORMAL;
#if NVMM > 0
	kpm->pm_npt_pml4 = 0;
	kpm->pm_npt_pdpt = 0;
#endif /* NVMM > 0 */

	/*
	 * the above is just a rough estimate and not critical to the proper
	 * operation of the system.
	 */

	/*
	 * enable global TLB entries if they are supported
	 */

	if (cpu_feature & CPUID_PGE) {
		lcr4(rcr4() | CR4_PGE);	/* enable hardware (via %cr4) */
		pmap_pg_g = PG_G;		/* enable software */

		/* add PG_G attribute to already mapped kernel pages */
		for (kva = VM_MIN_KERNEL_ADDRESS; kva < virtual_avail;
		     kva += PAGE_SIZE)
			if (pmap_valid_entry(PTE_BASE[atop(kva)]))
				PTE_BASE[atop(kva)] |= PG_G;
	}

	/*
	 * now we allocate the "special" VAs which are used for tmp mappings
	 * by the pmap (and other modules).    we allocate the VAs by advancing
	 * virtual_avail (note that there are no pages mapped at these VAs).
	 * we find the PTE that maps the allocated VA via the linear PTE
	 * mapping.
	 */

	pte = PTE_BASE + atop(virtual_avail);

#ifdef MULTIPROCESSOR
	/*
	 * Waste some VA space to avoid false sharing of cache lines
	 * for page table pages: Give each possible CPU a cache line
	 * of PTE's (16) to play with, though we only need 4.  We could
	 * recycle some of this waste by putting the idle stacks here
	 * as well; we could waste less space if we knew the largest
	 * CPU ID beforehand.
	 */
	pmap_csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;

	pmap_cdstp = (caddr_t) virtual_avail+PAGE_SIZE;  cdst_pte = pte+1;

	pmap_zerop = (caddr_t) virtual_avail+PAGE_SIZE*2;  zero_pte = pte+2;

	pmap_ptpp = (caddr_t) virtual_avail+PAGE_SIZE*3;  ptp_pte = pte+3;

	pmap_flshp = (caddr_t) virtual_avail+PAGE_SIZE*4;  flsh_pte = pte+4;

	virtual_avail += PAGE_SIZE * MAXCPUS * NPTECL;
	pte += MAXCPUS * NPTECL;
#else
	pmap_csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;	/* allocate */
	virtual_avail += PAGE_SIZE; pte++;			/* advance */

	pmap_cdstp = (caddr_t) virtual_avail;  cdst_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;

	pmap_zerop = (caddr_t) virtual_avail;  zero_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;

	pmap_ptpp = (caddr_t) virtual_avail;  ptp_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;

	pmap_flshp = (caddr_t) virtual_avail;  flsh_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;
#endif

	/* XXX: vmmap used by mem.c... should be uvm_map_reserve */
	vmmap = (char *)virtual_avail;			/* don't need pte */
	virtual_avail += PAGE_SIZE;

	msgbufp = (struct msgbuf *)virtual_avail;	/* don't need pte */
	virtual_avail += round_page(MSGBUFSIZE); pte++;

	bootargp = (bootarg_t *)virtual_avail;
	virtual_avail += round_page(bootargc); pte++;

	/*
	 * now we reserve some VM for mapping pages when doing a crash dump
	 */

	virtual_avail = reserve_dumppages(virtual_avail);

	/*
	 * init the static-global locks and global lists.
	 */

	LIST_INIT(&pmaps);

	/*
	 * initialize the pmap pool.
	 */

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 32, IPL_NONE, 0,
	    "pmappl", NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pvpl", &pmap_pv_page_allocator);

	/*
	 * ensure the TLB is sync'd with reality by flushing it...
	 */

	tlbflush();
}

/*
 * Pre-allocate PTP 0 for low memory, so that 1:1 mappings for various
 * trampoline code can be entered.
 */
void
pmap_prealloc_lowmem_ptp(void)
{
	pt_entry_t *pte, npte;
	vaddr_t ptpva = (vaddr_t)vtopte(0);

	/* If PAE, use the PAE-specific preallocator */
	if (cpu_pae) {
		pmap_prealloc_lowmem_ptp_pae();
		return;
	}

	/* enter pa for pte 0 into recursive map */
	pte = vtopte(ptpva);
	npte = PTP0_PA | PG_RW | PG_V | PG_U | PG_M;

	i386_atomic_testset_ul(pte, npte);

	/* make sure it is clean before using */
	memset((void *)ptpva, 0, NBPG);
}

/*
 * pmap_init: called from uvm_init, our job is to get the pmap
 * system ready to manage mappings... this mainly means initing
 * the pv_entry stuff.
 */

void
pmap_init(void)
{
	/*
	 * prime the pool with pv_entry structures to allow us to get
	 * the kmem_map allocated and inited (done after this function
	 * is finished).  we do this by setting a low water mark such
	 * that we are more likely to have these around in extreme
	 * memory starvation.
	 */

	pool_setlowat(&pmap_pv_pool, PVE_LOWAT);
	pool_sethiwat(&pmap_pv_pool, PVE_HIWAT);

	/*
	 * done: pmap module is up (and ready for business)
	 */

	pmap_initialized = TRUE;
}

/*
 * p v _ e n t r y   f u n c t i o n s
 */

void *
pmap_pv_page_alloc(struct pool *pp, int flags, int *slowdown)
{
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;

	kd.kd_waitok = ISSET(flags, PR_WAITOK);
	kd.kd_slowdown = slowdown;

	return (km_alloc(pp->pr_pgsize,
	    pmap_initialized ? &kv_page : &kv_any, pp->pr_crange, &kd));
}

void
pmap_pv_page_free(struct pool *pp, void *v)
{
	km_free(v, pp->pr_pgsize, &kv_page, pp->pr_crange);
}

/*
 * main pv_entry manipulation functions:
 *   pmap_enter_pv: enter a mapping onto a pv list
 *   pmap_remove_pv: remove a mappiing from a pv list
 */

/*
 * pmap_enter_pv: enter a mapping onto a pv list
 *
 * => caller should have pmap locked
 * => we will gain the lock on the pv and allocate the new pv_entry
 * => caller should adjust ptp's wire_count before calling
 *
 * pve: preallocated pve for us to use
 * ptp: PTP in pmap that maps this VA
 */

void
pmap_enter_pv(struct vm_page *pg, struct pv_entry *pve, struct pmap *pmap,
    vaddr_t va, struct vm_page *ptp)
{
	pve->pv_pmap = pmap;
	pve->pv_va = va;
	pve->pv_ptp = ptp;			/* NULL for kernel pmap */
	mtx_enter(&pg->mdpage.pv_mtx);
	pve->pv_next = pg->mdpage.pv_list;	/* add to ... */
	pg->mdpage.pv_list = pve;		/* ... locked list */
	mtx_leave(&pg->mdpage.pv_mtx);
}

/*
 * pmap_remove_pv: try to remove a mapping from a pv_list
 *
 * => pmap should be locked
 * => caller should hold lock on pv [so that attrs can be adjusted]
 * => caller should adjust ptp's wire_count and free PTP if needed
 * => we return the removed pve
 */

struct pv_entry *
pmap_remove_pv(struct vm_page *pg, struct pmap *pmap, vaddr_t va)
{
	struct pv_entry *pve, **prevptr;

	mtx_enter(&pg->mdpage.pv_mtx);
	prevptr = &pg->mdpage.pv_list;		/* previous pv_entry pointer */
	while ((pve = *prevptr) != NULL) {
		if (pve->pv_pmap == pmap && pve->pv_va == va) {	/* match? */
			*prevptr = pve->pv_next;		/* remove it! */
			break;
		}
		prevptr = &pve->pv_next;		/* previous pointer */
	}
	mtx_leave(&pg->mdpage.pv_mtx);
	return(pve);				/* return removed pve */
}

/*
 * p t p   f u n c t i o n s
 */

/*
 * pmap_alloc_ptp: allocate a PTP for a PMAP
 *
 * => pmap should already be locked by caller
 * => we use the ptp's wire_count to count the number of active mappings
 *	in the PTP (we start it at one to prevent any chance this PTP
 *	will ever leak onto the active/inactive queues)
 */

struct vm_page *
pmap_alloc_ptp_86(struct pmap *pmap, int pde_index, pt_entry_t pde_flags)
{
	struct vm_page *ptp;

	ptp = uvm_pagealloc(&pmap->pm_obj, ptp_i2o(pde_index), NULL,
			    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (ptp == NULL)
		return (NULL);

	/* got one! */
	atomic_clearbits_int(&ptp->pg_flags, PG_BUSY);
	ptp->wire_count = 1;	/* no mappings yet */
	PDE(pmap, pde_index) = (pd_entry_t)(VM_PAGE_TO_PHYS(ptp) |
	    PG_RW | PG_V | PG_M | PG_U | pde_flags);
	pmap->pm_stats.resident_count++;	/* count PTP as resident */
	pmap->pm_ptphint = ptp;
	return(ptp);
}

/*
 * pmap_get_ptp: get a PTP (if there isn't one, allocate a new one)
 *
 * => pmap should NOT be pmap_kernel()
 * => pmap should be locked
 */

struct vm_page *
pmap_get_ptp_86(struct pmap *pmap, int pde_index)
{
	struct vm_page *ptp;

	if (pmap_valid_entry(PDE(pmap, pde_index))) {
		/* valid... check hint (saves us a PA->PG lookup) */
		if (pmap->pm_ptphint &&
		    (PDE(pmap, pde_index) & PG_FRAME) ==
		    VM_PAGE_TO_PHYS(pmap->pm_ptphint))
			return(pmap->pm_ptphint);

		ptp = uvm_pagelookup(&pmap->pm_obj, ptp_i2o(pde_index));
#ifdef DIAGNOSTIC
		if (ptp == NULL)
			panic("pmap_get_ptp_86: unmanaged user PTP");
#endif
		pmap->pm_ptphint = ptp;
		return(ptp);
	}

	/* allocate a new PTP (updates ptphint) */
	return (pmap_alloc_ptp_86(pmap, pde_index, PG_u));
}

void
pmap_drop_ptp_86(struct pmap *pm, vaddr_t va, struct vm_page *ptp,
    pt_entry_t *ptes)
{
	i386_atomic_testset_ul(&PDE(pm, pdei(va)), 0);
	pmap_tlb_shootpage(curcpu()->ci_curpmap, ((vaddr_t)ptes) + ptp->offset);
#ifdef MULTIPROCESSOR
	/*
	 * Always shoot down the other pmap's
	 * self-mapping of the PTP.
	 */
	pmap_tlb_shootpage(pm, ((vaddr_t)PTE_BASE) + ptp->offset);
#endif
	pm->pm_stats.resident_count--;
	/* update hint */
	if (pm->pm_ptphint == ptp)
		pm->pm_ptphint = RBT_ROOT(uvm_objtree, &pm->pm_obj.memt);
	ptp->wire_count = 0;
	/* Postpone free to after shootdown. */
	uvm_pagerealloc(ptp, NULL, 0);
}

/*
 * p m a p  l i f e c y c l e   f u n c t i o n s
 */

/*
 * pmap_create: create a pmap
 *
 * => note: old pmap interface took a "size" args which allowed for
 *	the creation of "software only" pmaps (not in bsd).
 */

struct pmap *
pmap_create(void)
{
	struct pmap *pmap;

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);

	mtx_init(&pmap->pm_mtx, IPL_VM);
	mtx_init(&pmap->pm_apte_mtx, IPL_VM);

	/* init uvm_object */
	uvm_objinit(&pmap->pm_obj, NULL, 1);
	pmap->pm_stats.wired_count = 0;
	pmap->pm_stats.resident_count = 1;	/* count the PDP allocd below */
	pmap->pm_ptphint = NULL;
	pmap->pm_hiexec = 0;
	pmap->pm_flags = 0;

	setsegment(&pmap->pm_codeseg, 0, atop(I386_MAX_EXE_ADDR) - 1,
	    SDT_MEMERA, SEL_UPL, 1, 1);

	pmap->pm_type = PMAP_TYPE_NORMAL;
#if NVMM > 0
	pmap->pm_npt_pml4 = 0;
	pmap->pm_npt_pdpt = 0;
#endif /* NVMM > 0 */

	pmap_pinit_pd(pmap);
	return (pmap);
}

void
pmap_pinit_pd_86(struct pmap *pmap)
{
	/* allocate PDP */
	pmap->pm_pdir = uvm_km_alloc(kernel_map, NBPG);
	if (pmap->pm_pdir == 0)
		panic("pmap_pinit_pd_86: kernel_map out of virtual space!");
	pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
			    &pmap->pm_pdirpa);
	pmap->pm_pdirsize = NBPG;

	/* init PDP */
	/* zero init area */
	bzero((void *)pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
	/* put in recursive PDE to map the PTEs */
	PDE(pmap, PDSLOT_PTE) = pmap->pm_pdirpa | PG_V | PG_KW | PG_U | PG_M;
	PDE(pmap, PDSLOT_PTE + 1) = 0;

	/*
	 * we need to lock pmaps_lock to prevent nkpde from changing on
	 * us.   note that there is no need to splvm to protect us from
	 * malloc since malloc allocates out of a submap and we should have
	 * already allocated kernel PTPs to cover the range...
	 */
	/* put in kernel VM PDEs */
	bcopy(&PDP_BASE[PDSLOT_KERN], &PDE(pmap, PDSLOT_KERN),
	       nkpde * sizeof(pd_entry_t));
	/* zero the rest */
	bzero(&PDE(pmap, PDSLOT_KERN + nkpde),
	       NBPG - ((PDSLOT_KERN + nkpde) * sizeof(pd_entry_t)));
	LIST_INSERT_HEAD(&pmaps, pmap, pm_list);
}

/*
 * pmap_destroy: drop reference count on pmap.   free pmap if
 *	reference count goes to zero.
 */

void
pmap_destroy(struct pmap *pmap)
{
	struct vm_page *pg;
	int refs;

	refs = atomic_dec_int_nv(&pmap->pm_obj.uo_refs);
	if (refs > 0)
		return;

#ifdef MULTIPROCESSOR
	pmap_tlb_droppmap(pmap);	
#endif

	LIST_REMOVE(pmap, pm_list);

	/* Free any remaining PTPs. */
	while ((pg = RBT_ROOT(uvm_objtree, &pmap->pm_obj.memt)) != NULL) {
		pg->wire_count = 0;
		uvm_pagefree(pg);
	}

	uvm_km_free(kernel_map, pmap->pm_pdir, pmap->pm_pdirsize);
	pmap->pm_pdir = 0;

#if NVMM > 0
	if (pmap->pm_npt_pml4)
		km_free((void *)pmap->pm_npt_pml4, PAGE_SIZE, &kv_any,
		    &kp_zero);
	if (pmap->pm_npt_pdpt)
		km_free((void *)pmap->pm_npt_pdpt, PAGE_SIZE, &kv_any,
		    &kp_zero);
#endif /* NVMM > 0 */

	pool_put(&pmap_pmap_pool, pmap);
}


/*
 *	Add a reference to the specified pmap.
 */

void
pmap_reference(struct pmap *pmap)
{
	atomic_inc_int(&pmap->pm_obj.uo_refs);
}

void
pmap_activate(struct proc *p)
{
	KASSERT(curproc == p);
	KASSERT(&p->p_addr->u_pcb == curpcb);
	pmap_switch(NULL, p);
}

int nlazy_cr3_hit;
int nlazy_cr3;

void
pmap_switch(struct proc *o, struct proc *p)
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap, *opmap;
	struct cpu_info *self = curcpu();

	pmap = p->p_vmspace->vm_map.pmap;
	opmap = self->ci_curpmap;

	pcb->pcb_pmap = pmap;
	pcb->pcb_cr3 = pmap->pm_pdirpa;

	if (opmap == pmap) {
		if (pmap != pmap_kernel())
			nlazy_cr3_hit++;
	} else if (o != NULL && pmap == pmap_kernel()) {
		nlazy_cr3++;
	} else {
		curcpu()->ci_curpmap = pmap;
		lcr3(pmap->pm_pdirpa);
	}

	/*
	 * Set the correct descriptor value (i.e. with the
	 * correct code segment X limit) in the GDT.
	 */
	self->ci_gdt[GUCODE_SEL].sd = pmap->pm_codeseg;
	self->ci_gdt[GUFS_SEL].sd = pcb->pcb_threadsegs[TSEG_FS];
	self->ci_gdt[GUGS_SEL].sd = pcb->pcb_threadsegs[TSEG_GS];
}

void
pmap_deactivate(struct proc *p)
{
}

/*
 * pmap_extract: extract a PA for the given VA
 */

boolean_t
pmap_extract_86(struct pmap *pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *ptes, pte;

	if (pmap_valid_entry(PDE(pmap, pdei(va)))) {
		ptes = pmap_map_ptes_86(pmap);
		pte = ptes[atop(va)];
		pmap_unmap_ptes_86(pmap);
		if (!pmap_valid_entry(pte))
			return (FALSE);
		if (pap != NULL)
			*pap = (pte & PG_FRAME) | (va & ~PG_FRAME);
		return (TRUE);
	}
	return (FALSE);
}

/*
 * pmap_virtual_space: used during bootup [pmap_steal_memory] to
 *	determine the bounds of the kernel virtual address space.
 */

void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = virtual_avail;
	*endp = virtual_end;
}

/*
 * pmap_zero_page: zero a page
 */
void (*pagezero)(void *, size_t) = bzero;

void
pmap_zero_page(struct vm_page *pg)
{
	pmap_zero_phys(VM_PAGE_TO_PHYS(pg));
}

/*
 * pmap_zero_phys: same as pmap_zero_page, but for use before vm_pages are
 * initialized.
 */
void
pmap_zero_phys_86(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(pmap_zerop, id);

#ifdef DIAGNOSTIC
	if (*zpte)
		panic("pmap_zero_phys_86: lock botch");
#endif

	*zpte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	pmap_update_pg((vaddr_t)zerova);	/* flush TLB */
	pagezero(zerova, PAGE_SIZE);		/* zero */
	*zpte = 0;
}

/*
 * pmap_zero_page_uncached: the same, except uncached.
 */

boolean_t
pmap_zero_page_uncached_86(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(pmap_zerop, id);

#ifdef DIAGNOSTIC
	if (*zpte)
		panic("pmap_zero_page_uncached_86: lock botch");
#endif

	*zpte = (pa & PG_FRAME) | PG_V | PG_RW | PG_N;	/* map in */
	pmap_update_pg((vaddr_t)zerova);		/* flush TLB */
	pagezero(zerova, PAGE_SIZE);		/* zero */
	*zpte = 0;

	return (TRUE);
}

/*
 * pmap_flush_cache: flush the cache for a virtual address.
 */
void
pmap_flush_cache(vaddr_t addr, vsize_t len)
{
	vaddr_t i;

	if (curcpu()->ci_cflushsz == 0) {
		wbinvd();
		return;
	}
	
	mfence();
	for (i = addr; i < addr + len; i += curcpu()->ci_cflushsz)
		clflush(i);
	mfence();
}

void
pmap_flush_page(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *pte;
	caddr_t va;

	KDASSERT(PHYS_TO_VM_PAGE(pa) != NULL);

	if (cpu_pae) {
		pmap_flush_page_pae(pa);
		return;
	}	

	pte = PTESLEW(flsh_pte, id);
	va = VASLEW(pmap_flshp, id);

#ifdef DIAGNOSTIC
	if (*pte)
		panic("pmap_flush_page: lock botch");
#endif

	*pte = (pa & PG_FRAME) | PG_V | PG_RW;
	pmap_update_pg(va);
	pmap_flush_cache((vaddr_t)va, PAGE_SIZE);
	*pte = 0;
	pmap_update_pg(va);
}

/*
 * pmap_copy_page: copy a page
 */

void
pmap_copy_page_86(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *spte = PTESLEW(csrc_pte, id);
	pt_entry_t *dpte = PTESLEW(cdst_pte, id);
	caddr_t csrcva = VASLEW(pmap_csrcp, id);
	caddr_t cdstva = VASLEW(pmap_cdstp, id);

#ifdef DIAGNOSTIC
	if (*spte || *dpte)
		panic("pmap_copy_page_86: lock botch");
#endif

	*spte = (srcpa & PG_FRAME) | PG_V | PG_RW;
	*dpte = (dstpa & PG_FRAME) | PG_V | PG_RW;
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
	bcopy(csrcva, cdstva, PAGE_SIZE);
	*spte = *dpte = 0;
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
}

/*
 * p m a p   r e m o v e   f u n c t i o n s
 *
 * functions that remove mappings
 */

/*
 * pmap_remove_ptes: remove PTEs from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => caller must hold pmap's lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 */

void
pmap_remove_ptes_86(struct pmap *pmap, struct vm_page *ptp, vaddr_t ptpva,
    vaddr_t startva, vaddr_t endva, int flags, struct pv_entry **free_pvs)
{
	struct pv_entry *pve;
	pt_entry_t *pte = (pt_entry_t *) ptpva;
	struct vm_page *pg;
	pt_entry_t opte;

	/*
	 * note that ptpva points to the PTE that maps startva.   this may
	 * or may not be the first PTE in the PTP.
	 *
	 * we loop through the PTP while there are still PTEs to look at
	 * and the wire_count is greater than 1 (because we use the wire_count
	 * to keep track of the number of real PTEs in the PTP).
	 */

	for (/*null*/; startva < endva && (ptp == NULL || ptp->wire_count > 1)
			     ; pte++, startva += NBPG) {
		if (!pmap_valid_entry(*pte))
			continue;			/* VA not mapped */

		if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W))
			continue;

		/* atomically save the old PTE and zero it */
		opte = i386_atomic_testset_ul(pte, 0);

		if (opte & PG_W)
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		if (ptp)
			ptp->wire_count--;		/* dropping a PTE */

		/*
		 * Unnecessary work if not PG_PVLIST.
		 */
		pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);

		/*
		 * if we are not on a pv list we are done.
		 */
		if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
			if (pg != NULL)
				panic("pmap_remove_ptes_86: managed page "
				     "without PG_PVLIST for 0x%lx", startva);
#endif
			continue;
		}

#ifdef DIAGNOSTIC
		if (pg == NULL)
			panic("pmap_remove_ptes_86: unmanaged page marked "
			      "PG_PVLIST, va = 0x%lx, pa = 0x%lx",
			      startva, (u_long)(opte & PG_FRAME));
#endif

		/* sync R/M bits */
		pmap_sync_flags_pte_86(pg, opte);
		pve = pmap_remove_pv(pg, pmap, startva);
		if (pve) {
			pve->pv_next = *free_pvs;
			*free_pvs = pve;
		}

		/* end of "for" loop: time for next pte */
	}
}

/*
 * pmap_remove: top level mapping removal function
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_remove(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	pmap_do_remove(pmap, sva, eva, PMAP_REMOVE_ALL);
}

void
pmap_do_remove_86(struct pmap *pmap, vaddr_t sva, vaddr_t eva, int flags)
{
	pt_entry_t *ptes;
	paddr_t ptppa;
	vaddr_t blkendva;
	struct vm_page *ptp;
	struct pv_entry *pve;
	struct pv_entry *free_pvs = NULL;
	TAILQ_HEAD(, vm_page) empty_ptps;
	int shootall;
	vaddr_t va;

	TAILQ_INIT(&empty_ptps);

	ptes = pmap_map_ptes_86(pmap);	/* locks pmap */

	/*
	 * Decide if we want to shoot the whole tlb or just the range.
	 * Right now, we simply shoot everything when we remove more
	 * than 32 pages, but never in the kernel pmap. XXX - tune.
	 */
	if ((eva - sva > 32 * PAGE_SIZE) && pmap != pmap_kernel())
		shootall = 1;
	else
		shootall = 0;

	for (va = sva ; va < eva ; va = blkendva) {
		/* determine range of block */
		blkendva = i386_round_pdr(va + 1);
		if (blkendva > eva)
			blkendva = eva;

		/*
		 * XXXCDC: our PTE mappings should never be removed
		 * with pmap_remove!  if we allow this (and why would
		 * we?) then we end up freeing the pmap's page
		 * directory page (PDP) before we are finished using
		 * it when we hit in in the recursive mapping.  this
		 * is BAD.
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		if (pdei(va) == PDSLOT_PTE)
			/* XXXCDC: ugly hack to avoid freeing PDP here */
			continue;

		if (!pmap_valid_entry(PDE(pmap, pdei(va))))
			/* valid block? */
			continue;

		/* PA of the PTP */
		ptppa = PDE(pmap, pdei(va)) & PG_FRAME;

		/* get PTP if non-kernel mapping */
		if (pmap == pmap_kernel()) {
			/* we never free kernel PTPs */
			ptp = NULL;
		} else {
			if (pmap->pm_ptphint &&
			    VM_PAGE_TO_PHYS(pmap->pm_ptphint) == ptppa) {
				ptp = pmap->pm_ptphint;
			} else {
				ptp = PHYS_TO_VM_PAGE(ptppa);
#ifdef DIAGNOSTIC
				if (ptp == NULL)
					panic("pmap_do_remove_86: unmanaged "
					      "PTP detected");
#endif
			}
		}
		pmap_remove_ptes_86(pmap, ptp, (vaddr_t)&ptes[atop(va)],
		    va, blkendva, flags, &free_pvs);

		/* If PTP is no longer being used, free it. */
		if (ptp && ptp->wire_count <= 1) {
			pmap_drop_ptp_86(pmap, va, ptp, ptes);
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, pageq);
		}

		if (!shootall)
			pmap_tlb_shootrange(pmap, va, blkendva);
	}

	if (shootall)
		pmap_tlb_shoottlb();

	pmap_unmap_ptes_86(pmap);
	pmap_tlb_shootwait();

	while ((pve = free_pvs) != NULL) {
		free_pvs = pve->pv_next;
		pool_put(&pmap_pv_pool, pve);
	}

	while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
		TAILQ_REMOVE(&empty_ptps, ptp, pageq);
		uvm_pagefree(ptp);
	}
}

/*
 * pmap_page_remove: remove a managed vm_page from all pmaps that map it
 *
 * => R/M bits are sync'd back to attrs
 */

void
pmap_page_remove_86(struct vm_page *pg)
{
	struct pv_entry *pve;
	struct pmap *pm;
	pt_entry_t *ptes, opte;
	TAILQ_HEAD(, vm_page) empty_ptps;
	struct vm_page *ptp;

	if (pg->mdpage.pv_list == NULL)
		return;

	TAILQ_INIT(&empty_ptps);

	mtx_enter(&pg->mdpage.pv_mtx);
	while ((pve = pg->mdpage.pv_list) != NULL) {
		pmap_reference(pve->pv_pmap);
		pm = pve->pv_pmap;
		mtx_leave(&pg->mdpage.pv_mtx);

		ptes = pmap_map_ptes_86(pm);		/* locks pmap */

		/*
		 * We dropped the pvlist lock before grabbing the pmap
		 * lock to avoid lock ordering problems.  This means
		 * we have to check the pvlist again since somebody
		 * else might have modified it.  All we care about is
		 * that the pvlist entry matches the pmap we just
		 * locked.  If it doesn't, unlock the pmap and try
		 * again.
		 */
		mtx_enter(&pg->mdpage.pv_mtx);
		if ((pve = pg->mdpage.pv_list) == NULL ||
		    pve->pv_pmap != pm) {
			mtx_leave(&pg->mdpage.pv_mtx);
			pmap_unmap_ptes_86(pm);		/* unlocks pmap */
			pmap_destroy(pm);
			mtx_enter(&pg->mdpage.pv_mtx);
			continue;
		}

		pg->mdpage.pv_list = pve->pv_next;
		mtx_leave(&pg->mdpage.pv_mtx);

#ifdef DIAGNOSTIC
		if (pve->pv_ptp && (PDE(pve->pv_pmap, pdei(pve->pv_va)) &
				    PG_FRAME)
		    != VM_PAGE_TO_PHYS(pve->pv_ptp)) {
			printf("pmap_page_remove_86: pg=%p: va=%lx, "
				"pv_ptp=%p\n",
				pg, pve->pv_va, pve->pv_ptp);
			printf("pmap_page_remove_86: PTP's phys addr: "
				"actual=%x, recorded=%lx\n",
				(PDE(pve->pv_pmap, pdei(pve->pv_va)) &
				PG_FRAME), VM_PAGE_TO_PHYS(pve->pv_ptp));
			panic("pmap_page_remove_86: mapped managed page has "
				"invalid pv_ptp field");
}
#endif
		opte = i386_atomic_testset_ul(&ptes[atop(pve->pv_va)], 0);

		if (opte & PG_W)
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;

		/* sync R/M bits */
		pmap_sync_flags_pte_86(pg, opte);

		/* update the PTP reference count.  free if last reference. */
		if (pve->pv_ptp && --pve->pv_ptp->wire_count <= 1) {
			pmap_drop_ptp_86(pve->pv_pmap, pve->pv_va,
			    pve->pv_ptp, ptes);
			TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp, pageq);
		}

		pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va);

		pmap_unmap_ptes_86(pve->pv_pmap);	/* unlocks pmap */
		pmap_destroy(pve->pv_pmap);
		pool_put(&pmap_pv_pool, pve);
		mtx_enter(&pg->mdpage.pv_mtx);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	pmap_tlb_shootwait();

	while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
		TAILQ_REMOVE(&empty_ptps, ptp, pageq);
		uvm_pagefree(ptp);
	}
}

/*
 * p m a p   a t t r i b u t e  f u n c t i o n s
 * functions that test/change managed page's attributes
 * since a page can be mapped multiple times we must check each PTE that
 * maps it by going down the pv lists.
 */

/*
 * pmap_test_attrs: test a page's attributes
 */

boolean_t
pmap_test_attrs_86(struct vm_page *pg, int testbits)
{
	struct pv_entry *pve;
	pt_entry_t *ptes, pte;
	u_long mybits, testflags;
	paddr_t ptppa;

	testflags = pmap_pte2flags(testbits);

	if (pg->pg_flags & testflags)
		return (TRUE);

	mybits = 0;
	mtx_enter(&pg->mdpage.pv_mtx);
	for (pve = pg->mdpage.pv_list; pve != NULL && mybits == 0;
	    pve = pve->pv_next) {
		ptppa = PDE(pve->pv_pmap, pdei(pve->pv_va)) & PG_FRAME;
		ptes = (pt_entry_t *)pmap_tmpmap_pa(ptppa);
		pte = ptes[ptei(pve->pv_va)];
		pmap_tmpunmap_pa();
		mybits |= (pte & testbits);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	if (mybits == 0)
		return (FALSE);

	atomic_setbits_int(&pg->pg_flags, pmap_pte2flags(mybits));

	return (TRUE);
}

/*
 * pmap_clear_attrs: change a page's attributes
 *
 * => we return TRUE if we cleared one of the bits we were asked to
 */

boolean_t
pmap_clear_attrs_86(struct vm_page *pg, int clearbits)
{
	struct pv_entry *pve;
	pt_entry_t *ptes, opte;
	u_long clearflags;
	paddr_t ptppa;
	int result;

	clearflags = pmap_pte2flags(clearbits);

	result = pg->pg_flags & clearflags;
	if (result)
		atomic_clearbits_int(&pg->pg_flags, clearflags);

	mtx_enter(&pg->mdpage.pv_mtx);
	for (pve = pg->mdpage.pv_list; pve != NULL; pve = pve->pv_next) {
		ptppa = PDE(pve->pv_pmap, pdei(pve->pv_va)) & PG_FRAME;
		ptes = (pt_entry_t *)pmap_tmpmap_pa(ptppa);
#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(PDE(pve->pv_pmap, pdei(pve->pv_va))))
			panic("pmap_clear_attrs_86: mapping without PTP "
			      "detected");
#endif

		opte = ptes[ptei(pve->pv_va)];
		if (opte & clearbits) {
			result = TRUE;
			i386_atomic_clearbits_l(&ptes[ptei(pve->pv_va)],
			    (opte & clearbits));
			pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va);
		}
		pmap_tmpunmap_pa();
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	pmap_tlb_shootwait();

	return (result != 0);
}

/*
 * p m a p   p r o t e c t i o n   f u n c t i o n s
 */

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_protect: set the protection in of the pages in a pmap
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_write_protect: write-protect pages in a pmap
 */

void
pmap_write_protect_86(struct pmap *pmap, vaddr_t sva, vaddr_t eva,
    vm_prot_t prot)
{
	pt_entry_t *ptes, *spte, *epte, npte, opte;
	vaddr_t blockend;
	u_int32_t md_prot;
	vaddr_t va;
	int shootall = 0;

	ptes = pmap_map_ptes_86(pmap);		/* locks pmap */

	/* should be ok, but just in case ... */
	sva &= PG_FRAME;
	eva &= PG_FRAME;

	if ((eva - sva > 32 * PAGE_SIZE) && pmap != pmap_kernel())
		shootall = 1;

	for (va = sva; va < eva; va = blockend) {
		blockend = (va & PD_MASK) + NBPD;
		if (blockend > eva)
			blockend = eva;

		/*
		 * XXXCDC: our PTE mappings should never be write-protected!
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		/* XXXCDC: ugly hack to avoid freeing PDP here */
		if (pdei(va) == PDSLOT_PTE)
			continue;

		/* empty block? */
		if (!pmap_valid_entry(PDE(pmap, pdei(va))))
			continue;

		md_prot = protection_codes[prot];
		if (va < VM_MAXUSER_ADDRESS)
			md_prot |= PG_u;
		else if (va < VM_MAX_ADDRESS)
			/* XXX: write-prot our PTES? never! */
			md_prot |= PG_RW;

		spte = &ptes[atop(va)];
		epte = &ptes[atop(blockend)];

		for (/*null */; spte < epte ; spte++, va += PAGE_SIZE) {

			if (!pmap_valid_entry(*spte))	/* no mapping? */
				continue;

			opte = *spte;
			npte = (opte & ~PG_PROT) | md_prot;

			if (npte != opte) {
				pmap_exec_account(pmap, va, *spte, npte);
				i386_atomic_clearbits_l(spte,
				    (~md_prot & opte) & PG_PROT);
				i386_atomic_setbits_l(spte, md_prot);
			}
		}
	}
	if (shootall)
		pmap_tlb_shoottlb();
	else
		pmap_tlb_shootrange(pmap, sva, eva);

	pmap_unmap_ptes_86(pmap);		/* unlocks pmap */
	pmap_tlb_shootwait();
}

/*
 * end of protection functions
 */

/*
 * pmap_unwire: clear the wired bit in the PTE
 *
 * => mapping should already be in map
 */

void
pmap_unwire_86(struct pmap *pmap, vaddr_t va)
{
	pt_entry_t *ptes;

	if (pmap_valid_entry(PDE(pmap, pdei(va)))) {
		ptes = pmap_map_ptes_86(pmap);		/* locks pmap */

#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(ptes[atop(va)]))
			panic("pmap_unwire_86: invalid (unmapped) va "
			      "0x%lx", va);
#endif

		if ((ptes[atop(va)] & PG_W) != 0) {
			i386_atomic_clearbits_l(&ptes[atop(va)], PG_W);
			pmap->pm_stats.wired_count--;
		}
#ifdef DIAGNOSTIC
		else {
			printf("pmap_unwire_86: wiring for pmap %p va 0x%lx "
			       "didn't change!\n", pmap, va);
		}
#endif
		pmap_unmap_ptes_86(pmap);		/* unlocks map */
	}
#ifdef DIAGNOSTIC
	else {
		panic("pmap_unwire_86: invalid PDE");
	}
#endif
}

/*
 * pmap_collect: free resources held by a pmap
 *
 * => optional function.
 * => called when a process is swapped out to free memory.
 */

void
pmap_collect(struct pmap *pmap)
{
	/*
	 * free all of the pt pages by removing the physical mappings
	 * for its entire address space.
	 */

	pmap_do_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS,
	    PMAP_REMOVE_SKIPWIRED);
}

/*
 * pmap_copy: copy mappings from one pmap to another
 *
 * => optional function
 * void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
 */

/*
 * defined as macro in pmap.h
 */

/*
 * pmap_enter: enter a mapping into a pmap
 *
 * => must be done "now" ... no lazy-evaluation
 */

int
pmap_enter_86(struct pmap *pmap, vaddr_t va, paddr_t pa,
    vm_prot_t prot, int flags)
{
	pt_entry_t *ptes, opte, npte;
	struct vm_page *ptp;
	struct pv_entry *pve, *opve = NULL;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	boolean_t nocache = (pa & PMAP_NOCACHE) != 0;
	boolean_t wc = (pa & PMAP_WC) != 0;
	struct vm_page *pg = NULL;
	int error, wired_count, resident_count, ptp_count;

	KASSERT(!(wc && nocache));
	pa &= PMAP_PA_MASK;	/* nuke flags from pa */

#ifdef DIAGNOSTIC
	/* sanity check: totally out of range? */
	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter_86: too big");

	if (va == (vaddr_t) PDP_BASE || va == (vaddr_t) APDP_BASE)
		panic("pmap_enter_86: trying to map over PDP/APDP!");

	/* sanity check: kernel PTPs should already have been pre-allocated */
	if (va >= VM_MIN_KERNEL_ADDRESS &&
	    !pmap_valid_entry(PDE(pmap, pdei(va))))
		panic("pmap_enter: missing kernel PTP!");
#endif
	if (pmap_initialized)
		pve = pool_get(&pmap_pv_pool, PR_NOWAIT);
	else
		pve = NULL;
	wired_count = resident_count = ptp_count = 0;

	/*
	 * map in ptes and get a pointer to our PTP (unless we are the kernel)
	 */

	ptes = pmap_map_ptes_86(pmap);		/* locks pmap */
	if (pmap == pmap_kernel()) {
		ptp = NULL;
	} else {
		ptp = pmap_get_ptp_86(pmap, pdei(va));
		if (ptp == NULL) {
			if (flags & PMAP_CANFAIL) {
				pmap_unmap_ptes_86(pmap);
				error = ENOMEM;
				goto out;
			}
			panic("pmap_enter_86: get ptp failed");
		}
	}
	/*
	 * not allowed to sleep after here!
	 */
	opte = ptes[atop(va)];			/* old PTE */

	/*
	 * is there currently a valid mapping at our VA?
	 */

	if (pmap_valid_entry(opte)) {

		/*
		 * first, calculate pm_stats updates.  resident count will not
		 * change since we are replacing/changing a valid
		 * mapping.  wired count might change...
		 */

		if (wired && (opte & PG_W) == 0)
			wired_count++;
		else if (!wired && (opte & PG_W) != 0)
			wired_count--;

		/*
		 * is the currently mapped PA the same as the one we
		 * want to map?
		 */

		if ((opte & PG_FRAME) == pa) {

			/* if this is on the PVLIST, sync R/M bit */
			if (opte & PG_PVLIST) {
				pg = PHYS_TO_VM_PAGE(pa);
#ifdef DIAGNOSTIC
				if (pg == NULL)
					panic("pmap_enter_86: same pa "
					     "PG_PVLIST mapping with "
					     "unmanaged page "
					     "pa = 0x%lx (0x%lx)", pa,
					     atop(pa));
#endif
				pmap_sync_flags_pte_86(pg, opte);
			}
			goto enter_now;
		}

		/*
		 * changing PAs: we must remove the old one first
		 */

		/*
		 * if current mapping is on a pvlist,
		 * remove it (sync R/M bits)
		 */

		if (opte & PG_PVLIST) {
			pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);
#ifdef DIAGNOSTIC
			if (pg == NULL)
				panic("pmap_enter_86: PG_PVLIST mapping with "
				      "unmanaged page "
				      "pa = 0x%lx (0x%lx)", pa, atop(pa));
#endif
			pmap_sync_flags_pte_86(pg, opte);
			opve = pmap_remove_pv(pg, pmap, va);
			pg = NULL; /* This is not the page we are looking for */
		}
	} else {	/* opte not valid */
		resident_count++;
		if (wired)
			wired_count++;
		if (ptp)
			ptp_count++;	/* count # of valid entries */
	}

	/*
	 * pve is either NULL or points to a now-free pv_entry structure
	 * (the latter case is if we called pmap_remove_pv above).
	 *
	 * if this entry is to be on a pvlist, enter it now.
	 */

	if (pmap_initialized && pg == NULL)
		pg = PHYS_TO_VM_PAGE(pa);

	if (pg != NULL) {
		if (pve == NULL) {
			pve = opve;
			opve = NULL;
		}
		if (pve == NULL) {
			if (flags & PMAP_CANFAIL) {
				pmap_unmap_ptes_86(pmap);
				error = ENOMEM;
				goto out;
			}
			panic("pmap_enter_86: no pv entries available");
		}
		/* lock pg when adding */
		pmap_enter_pv(pg, pve, pmap, va, ptp);
		pve = NULL;
	}

enter_now:
	/*
	 * at this point pg is !NULL if we want the PG_PVLIST bit set
	 */

	npte = pa | protection_codes[prot] | PG_V;
	pmap_exec_account(pmap, va, opte, npte);
	if (wired)
		npte |= PG_W;
	if (nocache)
		npte |= PG_N;
	if (va < VM_MAXUSER_ADDRESS)
		npte |= PG_u;
	else if (va < VM_MAX_ADDRESS)
		npte |= PG_RW;	/* XXXCDC: no longer needed? */
	if (pmap == pmap_kernel())
		npte |= pmap_pg_g;
	if (flags & PROT_READ)
		npte |= PG_U;
	if (flags & PROT_WRITE)
		npte |= PG_M;
	if (pg) {
		npte |= PG_PVLIST;
		if (pg->pg_flags & PG_PMAP_WC) {
			KASSERT(nocache == 0);
			wc = TRUE;
		}
		pmap_sync_flags_pte_86(pg, npte);
	}
	if (wc)
		npte |= pmap_pg_wc;

	opte = i386_atomic_testset_ul(&ptes[atop(va)], npte);
	if (ptp)
		ptp->wire_count += ptp_count;
	pmap->pm_stats.resident_count += resident_count;
	pmap->pm_stats.wired_count += wired_count;

	if (pmap_valid_entry(opte)) {
		if (nocache && (opte & PG_N) == 0)
			wbinvd(); /* XXX clflush before we enter? */
		pmap_tlb_shootpage(pmap, va);
	}

	pmap_unmap_ptes_86(pmap);
	pmap_tlb_shootwait();

	error = 0;

out:
	if (pve)
		pool_put(&pmap_pv_pool, pve);
	if (opve)
		pool_put(&pmap_pv_pool, opve);

	return error;
}

/*
 * pmap_growkernel: increase usage of KVM space
 *
 * => we allocate new PTPs for the kernel and install them in all
 *	the pmaps on the system.
 */

vaddr_t
pmap_growkernel_86(vaddr_t maxkvaddr)
{
	struct pmap *kpm = pmap_kernel(), *pm;
	int needed_kpde;   /* needed number of kernel PTPs */
	int s;
	paddr_t ptaddr;

	needed_kpde = (int)(maxkvaddr - VM_MIN_KERNEL_ADDRESS + (NBPD-1))
		/ NBPD;
	if (needed_kpde <= nkpde)
		goto out;		/* we are OK */

	/*
	 * whoops!   we need to add kernel PTPs
	 */

	s = splhigh();	/* to be safe */

	for (/*null*/ ; nkpde < needed_kpde ; nkpde++) {

		if (uvm.page_init_done == FALSE) {

			/*
			 * we're growing the kernel pmap early (from
			 * uvm_pageboot_alloc()).  this case must be
			 * handled a little differently.
			 */

			if (uvm_page_physget(&ptaddr) == FALSE)
				panic("pmap_growkernel: out of memory");
			pmap_zero_phys_86(ptaddr);

			PDE(kpm, PDSLOT_KERN + nkpde) =
				ptaddr | PG_RW | PG_V | PG_U | PG_M;

			/* count PTP as resident */
			kpm->pm_stats.resident_count++;
			continue;
		}

		/*
		 * THIS *MUST* BE CODED SO AS TO WORK IN THE
		 * pmap_initialized == FALSE CASE!  WE MAY BE
		 * INVOKED WHILE pmap_init() IS RUNNING!
		 */

		while (!pmap_alloc_ptp_86(kpm, PDSLOT_KERN + nkpde, 0))
			uvm_wait("pmap_growkernel");

		/* distribute new kernel PTP to all active pmaps */
		LIST_FOREACH(pm, &pmaps, pm_list) {
			PDE(pm, PDSLOT_KERN + nkpde) =
				PDE(kpm, PDSLOT_KERN + nkpde);
		}
	}

	splx(s);

out:
	return (VM_MIN_KERNEL_ADDRESS + (nkpde * NBPD));
}

#ifdef MULTIPROCESSOR
/*
 * Locking for tlb shootdown.
 *
 * We lock by setting tlb_shoot_wait to the number of cpus that will
 * receive our tlb shootdown. After sending the IPIs, we don't need to
 * worry about locking order or interrupts spinning for the lock because
 * the call that grabs the "lock" isn't the one that releases it. And
 * there is nothing that can block the IPI that releases the lock.
 *
 * The functions are organized so that we first count the number of
 * cpus we need to send the IPI to, then we grab the counter, then
 * we send the IPIs, then we finally do our own shootdown.
 *
 * Our shootdown is last to make it parallel with the other cpus
 * to shorten the spin time.
 *
 * Notice that we depend on failures to send IPIs only being able to
 * happen during boot. If they happen later, the above assumption
 * doesn't hold since we can end up in situations where noone will
 * release the lock if we get an interrupt in a bad moment.
 */

volatile int tlb_shoot_wait;

volatile vaddr_t tlb_shoot_addr1;
volatile vaddr_t tlb_shoot_addr2;

void
pmap_tlb_shootpage(struct pmap *pm, vaddr_t va)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	int wait = 0;
	u_int64_t mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !pmap_is_active(pm, ci) ||
		    !(ci->ci_flags & CPUF_RUNNING))
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait > 0) {
		int s = splvm();

		while (atomic_cas_uint(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}
		tlb_shoot_addr1 = va;
		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (i386_fast_ipi(ci, LAPIC_IPI_INVLPG) != 0)
				panic("pmap_tlb_shootpage: ipi failed");
		}
		splx(s);
	}

	if (pmap_is_curpmap(pm))
		pmap_update_pg(va);
}

void
pmap_tlb_shootrange(struct pmap *pm, vaddr_t sva, vaddr_t eva)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	int wait = 0;
	u_int64_t mask = 0;
	vaddr_t va;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !pmap_is_active(pm, ci) ||
		    !(ci->ci_flags & CPUF_RUNNING))
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait > 0) {
		int s = splvm();

		while (atomic_cas_uint(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}
		tlb_shoot_addr1 = sva;
		tlb_shoot_addr2 = eva;
		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (i386_fast_ipi(ci, LAPIC_IPI_INVLRANGE) != 0)
				panic("pmap_tlb_shootrange: ipi failed");
		}
		splx(s);
	}

	if (pmap_is_curpmap(pm))
		for (va = sva; va < eva; va += PAGE_SIZE)
			pmap_update_pg(va);
}

void
pmap_tlb_shoottlb(void)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	int wait = 0;
	u_int64_t mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !(ci->ci_flags & CPUF_RUNNING))
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait) {
		int s = splvm();

		while (atomic_cas_uint(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}

		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (i386_fast_ipi(ci, LAPIC_IPI_INVLTLB) != 0)
				panic("pmap_tlb_shoottlb: ipi failed");
		}
		splx(s);
	}

	tlbflush();
}

void
pmap_tlb_droppmap(struct pmap *pm)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	int wait = 0;
	u_int64_t mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !(ci->ci_flags & CPUF_RUNNING) ||
		    ci->ci_curpmap != pm)
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait) {
		int s = splvm();

		while (atomic_cas_uint(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}

		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (i386_fast_ipi(ci, LAPIC_IPI_RELOADCR3) != 0)
				panic("pmap_tlb_droppmap: ipi failed");
		}
		splx(s);
	}

	if (self->ci_curpmap == pm)
		pmap_activate(curproc);

	pmap_tlb_shootwait();
}

void
pmap_tlb_shootwait(void)
{
	while (tlb_shoot_wait != 0)
		CPU_BUSY_CYCLE();
}

#else

void
pmap_tlb_shootpage(struct pmap *pm, vaddr_t va)
{
	if (pmap_is_curpmap(pm))
		pmap_update_pg(va);

}

void
pmap_tlb_shootrange(struct pmap *pm, vaddr_t sva, vaddr_t eva)
{
	vaddr_t va;

	for (va = sva; va < eva; va += PAGE_SIZE)
		pmap_update_pg(va);	
}

void
pmap_tlb_shoottlb(void)
{
	tlbflush();
}
#endif /* MULTIPROCESSOR */

u_int32_t	(*pmap_pte_set_p)(vaddr_t, paddr_t, u_int32_t) =
    pmap_pte_set_86;
u_int32_t	(*pmap_pte_setbits_p)(vaddr_t, u_int32_t, u_int32_t) =
    pmap_pte_setbits_86;
u_int32_t	(*pmap_pte_bits_p)(vaddr_t) = pmap_pte_bits_86;
paddr_t		(*pmap_pte_paddr_p)(vaddr_t) = pmap_pte_paddr_86;
boolean_t	(*pmap_clear_attrs_p)(struct vm_page *, int) =
    pmap_clear_attrs_86;
int		(*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int) =
    pmap_enter_86;
boolean_t	(*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *) =
    pmap_extract_86;
vaddr_t		(*pmap_growkernel_p)(vaddr_t) = pmap_growkernel_86;
void		(*pmap_page_remove_p)(struct vm_page *) = pmap_page_remove_86;
void		(*pmap_do_remove_p)(struct pmap *, vaddr_t, vaddr_t, int) =
    pmap_do_remove_86;
boolean_t	 (*pmap_test_attrs_p)(struct vm_page *, int) =
    pmap_test_attrs_86;
void		(*pmap_unwire_p)(struct pmap *, vaddr_t) = pmap_unwire_86;
void		(*pmap_write_protect_p)(struct pmap *, vaddr_t, vaddr_t,
    vm_prot_t) = pmap_write_protect_86;
void		(*pmap_pinit_pd_p)(pmap_t) = pmap_pinit_pd_86;
void		(*pmap_zero_phys_p)(paddr_t) = pmap_zero_phys_86;
boolean_t	(*pmap_zero_page_uncached_p)(paddr_t) =
    pmap_zero_page_uncached_86;
void		(*pmap_copy_page_p)(struct vm_page *, struct vm_page *) =
    pmap_copy_page_86;
@


1.195
log
@
vmm(4) for i386. Userland changes forthcoming. Note that for the time being,
i386 hosts are limited to running only i386 guests, even if the underlying
hardware supports amd64. This is a restriction I hope to lift moving forward,
but for now please don't report problems running amd64 guests on i386 hosts.

This was a straightforward port of the in-tree amd64 code plus the old rotted
tree I had from last year for i386 support. Changes included converting 64-bit
VMREAD/VMWRITE ops to 2x32-bit ops, and fixing treatment of the TSS, which
differs on i386.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.194 2016/09/17 07:37:57 mlarkin Exp $	*/
a67 1
#include <machine/lock.h>
d2522 1
a2522 1
				SPINLOCK_SPIN_HOOK;
d2560 1
a2560 1
				SPINLOCK_SPIN_HOOK;
d2598 1
a2598 1
				SPINLOCK_SPIN_HOOK;
d2634 1
a2634 1
				SPINLOCK_SPIN_HOOK;
d2656 1
a2656 1
		SPINLOCK_SPIN_HOOK;
@


1.194
log
@
remove unused pmap_dump functions

ok kettenis, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.193 2016/09/16 02:35:41 dlg Exp $	*/
d77 2
d936 5
d1299 6
d1371 9
@


1.193
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.192 2016/09/16 01:09:54 dlg Exp $	*/
a2450 52

#ifdef DEBUG
void		 pmap_dump_86(struct pmap *, vaddr_t, vaddr_t);

/*
 * pmap_dump: dump all the mappings from a pmap
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_dump_86(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	pt_entry_t *ptes, *pte;
	vaddr_t blkendva;

	/*
	 * if end is out of range truncate.
	 * if (end == start) update to max.
	 */

	if (eva > VM_MAXUSER_ADDRESS || eva <= sva)
		eva = VM_MAXUSER_ADDRESS;

	ptes = pmap_map_ptes_86(pmap);	/* locks pmap */

	/*
	 * dumping a range of pages: we dump in PTP sized blocks (4MB)
	 */

	for (/* null */ ; sva < eva ; sva = blkendva) {

		/* determine range of block */
		blkendva = i386_round_pdr(sva+1);
		if (blkendva > eva)
			blkendva = eva;

		/* valid block? */
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
			continue;

		pte = &ptes[atop(sva)];
		for (/* null */; sva < blkendva ; sva += NBPG, pte++) {
			if (!pmap_valid_entry(*pte))
				continue;
			printf("va %#lx -> pa %#x (pte=%#x)\n",
			       sva, *pte, *pte & PG_FRAME);
		}
	}
	pmap_unmap_ptes_86(pmap);
}
#endif
@


1.192
log
@move the uvm_map_addr RB tree from RB macros to the RBT functions

this tree is interesting because it uses all the red black tree
features, specifically the augment callback thats called on tree
topology changes, and it poisons and checks entries as theyre removed
from and inserted back into the tree respectively.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.191 2016/09/15 02:00:17 dlg Exp $	*/
d1254 1
a1254 1
		pm->pm_ptphint = RB_ROOT(&pm->pm_obj.memt);
d1351 1
a1351 1
	while ((pg = RB_ROOT(&pmap->pm_obj.memt)) != NULL) {
@


1.191
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.190 2016/06/07 06:23:19 dlg Exp $	*/
d604 1
a604 1
	RB_FOREACH_REVERSE(ent, uvm_map_addr, &map->addr) {
@


1.190
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.189 2016/03/15 03:17:51 guenther Exp $	*/
d1029 1
a1029 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 32, 0, 0,
d1031 2
a1032 4
	pool_setipl(&pmap_pmap_pool, IPL_NONE);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
	    &pmap_pv_page_allocator);
	pool_setipl(&pmap_pv_pool, IPL_VM);
@


1.189
log
@Burn more LDT deadwood: stop allocating one for each idle thread,
load the ldt register with the null selector (disabling use of it),
stop reloading it on every context switch, and blow away the table
itself, as well as the pcb and pmap bits that were used to track
it (making sure to keep pcb_savefpu correctly aligned).

testing naddy@@
ok kettenis@@ mpi@@ mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.188 2016/03/07 05:32:47 naddy Exp $	*/
d1029 1
a1029 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 32, 0, PR_WAITOK,
d1031 1
@


1.188
log
@Sync no-argument function declaration and definition by adding (void).
ok mlarkin@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.187 2016/03/03 12:41:30 naddy Exp $	*/
a1289 5
	/* init the LDT */
	pmap->pm_ldt = NULL;
	pmap->pm_ldt_len = 0;
	pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);

a1395 7
	/* Get the LDT that this process will actually use */
#ifdef MULTIPROCESSOR
	pcb->pcb_ldt = pmap->pm_ldt == NULL ? self->ci_ldt : pmap->pm_ldt;
#else
	pcb->pcb_ldt = pmap->pm_ldt == NULL ? ldt : pmap->pm_ldt;
#endif
	pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
a1414 2

	lldt(pcb->pcb_ldt_sel);
@


1.187
log
@Remove option USER_LDT and everything depending on it.
Remove machdep.userldt sysctl.
Remove i386_[gs]et_ldt syscall stub from libi386.
Remove i386_[gs]et_ldt regression test.

ok mlarkin@@ millert@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.186 2015/10/23 09:36:09 kettenis Exp $	*/
d713 1
a713 1
pmap_tmpunmap_pa()
@


1.186
log
@Zap pv allocation abstraction layer.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.185 2015/09/03 18:49:19 kettenis Exp $	*/
a1364 14
#ifdef USER_LDT
	if (pmap->pm_flags & PMF_USER_LDT) {
		/*
		 * no need to switch the LDT; this address space is gone,
		 * nothing is using it.
		 *
		 * No need to lock the pmap for ldt_free (or anything else),
		 * we're the last one to use it.
		 */
		ldt_free(pmap);
		uvm_km_free(kernel_map, (vaddr_t)pmap->pm_ldt,
			    pmap->pm_ldt_len * sizeof(union descriptor));
	}
#endif
a1377 69

#if defined(PMAP_FORK)
/*
 * pmap_fork: perform any necessary data structure manipulation when
 * a VM space is forked.
 */

void
pmap_fork(struct pmap *pmap1, struct pmap *pmap2)
{
#ifdef USER_LDT
	/* Copy the LDT, if necessary. */
	if (pmap1->pm_flags & PMF_USER_LDT) {
		union descriptor *new_ldt;
		size_t len;

		len = pmap1->pm_ldt_len * sizeof(union descriptor);
		new_ldt = (union descriptor *)uvm_km_alloc(kernel_map, len);
		if (new_ldt == NULL) {
			/* XXX needs to be able to fail properly */
			panic("pmap_fork: out of kva");
		}
		bcopy(pmap1->pm_ldt, new_ldt, len);
		pmap2->pm_ldt = new_ldt;
		pmap2->pm_ldt_len = pmap1->pm_ldt_len;
		pmap2->pm_flags |= PMF_USER_LDT;
		ldt_alloc(pmap2, new_ldt, len);
	}
#endif /* USER_LDT */
}
#endif /* PMAP_FORK */

#ifdef USER_LDT
/*
 * pmap_ldt_cleanup: if the pmap has a local LDT, deallocate it, and
 * restore the default.
 */

void
pmap_ldt_cleanup(struct proc *p)
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	union descriptor *old_ldt = NULL;
	size_t len = 0;

	if (pmap->pm_flags & PMF_USER_LDT) {
		ldt_free(pmap);
		pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);
		pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
		/* Reset the cached address of the LDT that this process uses */
#ifdef MULTIPROCESSOR
		pcb->pcb_ldt = curcpu()->ci_ldt;
#else
		pcb->pcb_ldt = ldt;
#endif
		if (pcb == curpcb)
			lldt(pcb->pcb_ldt_sel);
		old_ldt = pmap->pm_ldt;
		len = pmap->pm_ldt_len * sizeof(union descriptor);
		pmap->pm_ldt = NULL;
		pmap->pm_ldt_len = 0;
		pmap->pm_flags &= ~PMF_USER_LDT;
	}

	if (old_ldt != NULL)
		uvm_km_free(kernel_map, (vaddr_t)old_ldt, len);
}
#endif /* USER_LDT */
@


1.185
log
@Fix a race in pmap_page_remove_86() and pmap_page_remove_pae().

ok millert@@, tedu@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.184 2015/08/31 20:42:41 kettenis Exp $	*/
a1098 36
/*
 * pv_entry allocation functions:
 *   the main pv_entry allocation functions are:
 *     pmap_alloc_pv: allocate a pv_entry structure
 *     pmap_free_pv: free one pv_entry
 *
 * the rest are helper functions
 */

/*
 * pmap_alloc_pv: inline function to allocate a pv_entry structure
 * => 3 modes:
 *    ALLOCPV_NEED   = we really need a pv_entry
 *    ALLOCPV_TRY    = we want a pv_entry
 *    ALLOCPV_NONEED = we are trying to grow our free list, don't really need
 *			one now
 *
 * "try" is for optional functions like pmap_copy().
 */

struct pv_entry *
pmap_alloc_pv(struct pmap *pmap, int mode)
{
	return pool_get(&pmap_pv_pool, PR_NOWAIT);
}

/*
 * pmap_free_pv: free a single pv_entry
 */

void
pmap_free_pv(struct pmap *pmap, struct pv_entry *pv)
{
	pool_put(&pmap_pv_pool, pv);
}

d1889 1
a1889 1
		pmap_free_pv(pmap, pve);
d1983 1
a1983 1
		pmap_free_pv(NULL, pve);
d2297 1
a2297 1
		pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d2473 1
a2473 1
		pmap_free_pv(pmap, pve);
d2475 1
a2475 1
		pmap_free_pv(pmap, opve);
@


1.184
log
@The pmap_pmap_pool pool will never be used in interrupt context, so pass the
PR_WAITOK flag to pool_init and pass NULL as the pool allocator.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.183 2015/08/25 04:57:31 mlarkin Exp $	*/
d1944 1
a1955 1
		pg->mdpage.pv_list = pve->pv_next;
d1957 1
d1960 23
a1982 1
		ptes = pmap_map_ptes_86(pve->pv_pmap);	/* locks pmap */
@


1.183
log
@
Enforce kernel w^x policy by properly setting NX (as needed) for
kernel text, PTEs, .rodata, data, bss and the symbol regions. This has
been in snaps for a while with no reported fallout.

The APTE space and MP/ACPI trampolines will be fixed next.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.182 2015/08/22 07:16:10 mlarkin Exp $	*/
d1029 2
a1030 2
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 32, 0, 0, "pmappl",
	    &pool_allocator_nointr);
@


1.182
log
@
delete some wrong comments
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.181 2015/07/10 10:07:31 kettenis Exp $	*/
d815 1
@


1.181
log
@Don't call pool_put(9) while holding a mutex.  Instead collect pv entries in
a list and put them back into the pool after releasing the mutex.  This
prevents a lock ordering problem between the per-pmap mutexes and the kernel
lock that arises because pool_put(9) may grab the kernel lock when it decides
to free a pool page.

This seems to make the i386 pmap mpsafe enough to run the reaper without
holding the kernel lock.

ok sthen@@ (who helped me a lot debugging this)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.180 2015/07/02 16:14:43 kettenis Exp $	*/
a292 1
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
@


1.180
log
@Make the i386 pmap (almost) mpsafe by protecting the pmap itself, the pv
lists and the apte with a mutex.  Rearrange some code to avoid
sleeping/spinning with one of these locks held.  This should make
pmap_enter(9), pmap_remove(9) and pmap_page_protect(9) safe to use without
holding the kernel lock.  Unfortunately there still seems to be an issue
that causes deadlocks under pressure.  That shouldn't be an issue as
long as uvm still calls the pmap functions with the kernel lock held.

Hopefully committed this will help finding the last bugs.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.179 2015/05/30 08:41:30 kettenis Exp $	*/
d457 1
a457 1
		    vaddr_t, vaddr_t, int);
d1747 1
a1747 1
    vaddr_t startva, vaddr_t endva, int flags)
d1808 4
a1811 2
		if (pve)
			pmap_free_pv(NULL, pve);
d1836 2
d1905 1
a1905 1
		    va, blkendva, flags);
d1922 5
@


1.179
log
@Native atomic operations for i386.

ok deraadt@@, guenther@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.178 2015/04/22 06:26:23 mlarkin Exp $	*/
d58 1
a67 1
#include <machine/atomic.h>
d513 2
d520 2
d552 1
d554 2
d1033 1
a1103 1
 *     pmap_free_pvs: free a list of pv_entrys
a1134 15
/*
 * pmap_free_pvs: free a list of pv_entrys
 */

void
pmap_free_pvs(struct pmap *pmap, struct pv_entry *pvs)
{
	struct pv_entry *nextpv;

	for ( /* null */ ; pvs != NULL ; pvs = nextpv) {
		nextpv = pvs->pv_next;
		pool_put(&pmap_pv_pool, pvs);
	}
}

d1177 1
d1180 1
d1197 1
d1206 1
d1315 3
d1382 1
a1382 1
	refs = --pmap->pm_obj.uo_refs;
d1426 1
a1426 1
	pmap->pm_obj.uo_refs++;
a1748 1
	struct pv_entry *pv_tofree = NULL;	/* list of pv_entrys to free */
d1808 2
a1809 4
		if (pve) {
			pve->pv_next = pv_tofree;
			pv_tofree = pve;
		}
a1812 2
	if (pv_tofree)
		pmap_free_pvs(pmap, pv_tofree);
d1916 1
d1918 1
a1918 1
	pmap_unmap_ptes_86(pmap);
d1944 6
a1949 1
	for (pve = pg->mdpage.pv_list ; pve != NULL ; pve = pve->pv_next) {
d1951 1
d1986 3
d1990 2
a1991 2
	pmap_free_pvs(NULL, pg->mdpage.pv_list);
	pg->mdpage.pv_list = NULL;
d2017 1
d2025 1
d2028 4
a2031 3
		ptes = pmap_map_ptes_86(pve->pv_pmap);
		pte = ptes[atop(pve->pv_va)];
		pmap_unmap_ptes_86(pve->pv_pmap);
d2034 1
d2056 1
d2065 1
d2067 2
a2068 1
		ptes = pmap_map_ptes_86(pve->pv_pmap);	/* locks pmap */
d2075 1
a2075 1
		opte = ptes[atop(pve->pv_va)];
d2078 1
a2078 1
			i386_atomic_clearbits_l(&ptes[atop(pve->pv_va)],
d2082 1
a2082 1
		pmap_unmap_ptes_86(pve->pv_pmap);	/* unlocks pmap */
d2084 1
d2188 1
a2189 1
	pmap_unmap_ptes_86(pmap);		/* unlocks pmap */
d2277 1
a2277 1
	struct pv_entry *pve = NULL, *freepve;
d2301 1
a2301 1
		freepve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d2303 1
a2303 1
		freepve = NULL;
d2317 1
d2387 1
a2387 1
			pve = pmap_remove_pv(pg, pmap, va);
d2410 8
a2417 7
			pve = freepve;
			if (pve == NULL) {
				if (flags & PMAP_CANFAIL) {
					error = ENOMEM;
					goto out;
				}
				panic("pmap_enter_86: no pv entries available");
d2419 1
a2419 1
			freepve = NULL;
d2423 1
a2423 4
	} else {
		/* new mapping is not PG_PVLIST.   free pve if we've got one */
		if (pve)
			pmap_free_pv(pmap, pve);
a2467 1
		pmap_tlb_shootwait();
d2470 3
d2476 4
a2479 3
	pmap_unmap_ptes_86(pmap);
	if (freepve)
		pmap_free_pv(pmap, freepve);
@


1.178
log
@
Reduce differences between pae and no-pae modes.

discussed with deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.177 2015/04/21 18:47:57 mlarkin Exp $	*/
d2639 1
a2639 1
		while (i486_atomic_cas_int(&tlb_shoot_wait, 0, wait) != 0) {
d2677 1
a2677 1
		while (i486_atomic_cas_int(&tlb_shoot_wait, 0, wait) != 0) {
d2715 1
a2715 1
		while (i486_atomic_cas_int(&tlb_shoot_wait, 0, wait) != 0) {
d2751 1
a2751 1
		while (i486_atomic_cas_int(&tlb_shoot_wait, 0, wait) != 0) {
@


1.177
log
@
Remove an extra lcr3 that snuck into pmap_switch during yesterday's
cleanup, responsible for various reaper panics pointed out on bugs@@ this
morning.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.176 2015/04/21 04:40:40 mlarkin Exp $	*/
d350 1
a350 1
 * Number of PTEs per cache line.  4 byte pte, 64-byte cache line
d522 1
a522 1
		panic("pmap_map_ptes: APTE valid");
d1616 1
a1616 1
		panic("pmap_zero_phys: lock botch");
d1640 1
a1640 1
		panic("pmap_zero_page_uncached: lock botch");
d1720 1
a1720 1
		panic("pmap_copy_page: lock botch");
d2050 1
a2050 1
	clearflags = pmap_pte2flags(clearbits);	
d2541 1
a2541 1
void pmap_dump(struct pmap *, vaddr_t, vaddr_t);
@


1.176
log
@
Remove a duplicate variable and #define that snuck in, in a previous
commit.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.175 2015/04/21 00:07:51 mlarkin Exp $	*/
a1538 1
	lcr3(pmap->pm_pdirpa);
@


1.175
log
@
Reduce differences between i386 pmap modes.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.174 2015/04/12 21:37:33 mlarkin Exp $	*/
a407 9

/*
 * pv management structures.
 */
struct pool pmap_pv_pool;

#define PVE_LOWAT (PVE_PER_PVPAGE / 2) /* free pv_entry low water mark */
#define PVE_HIWAT (PVE_LOWAT + (PVE_PER_PVPAGE * 2))
					/* high water mark */
@


1.174
log
@
Fix some KNF, spacing, and typo issues. Moving the deck chairs around to
reduce differences between PAE and no-PAE i386 pmaps.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.173 2015/04/12 19:21:32 mlarkin Exp $	*/
d476 1
a476 1
void		 pmap_drop_ptp(struct pmap *, vaddr_t, struct vm_page *,
d1195 1
a1195 1
	pg->mdpage.pv_list = pve;			/* ... locked list */
d1278 1
a1278 1
			panic("pmap_get_ptp: unmanaged user PTP");
d1289 1
a1289 1
pmap_drop_ptp(struct pmap *pm, vaddr_t va, struct vm_page *ptp,
d1354 1
a1354 1
		panic("pmap_pinit: kernel_map out of virtual space!");
d1548 1
d1794 1
a1794 1
		 * Unnecessary work if not PG_VLIST.
d1804 2
a1805 2
				panic("pmap_remove_ptes: managed page without "
				      "PG_PVLIST for 0x%lx", startva);
d1812 1
a1812 1
			panic("pmap_remove_ptes: unmanaged page marked "
d1911 2
a1912 2
					panic("pmap_remove: unmanaged PTP "
					      "detected");
d1921 1
a1921 1
			pmap_drop_ptp(pmap, va, ptp, ptes);
d1965 2
a1966 1
			printf("pmap_page_remove: pg=%p: va=%lx, pv_ptp=%p\n",
d1968 1
a1968 1
			printf("pmap_page_remove: PTP's phys addr: "
d1972 1
a1972 1
			panic("pmap_page_remove: mapped managed page has "
d1987 1
a1987 1
			pmap_drop_ptp(pve->pv_pmap, pve->pv_va,
d2288 1
a2288 1
		panic("pmap_enter: too big");
d2291 1
a2291 1
		panic("pmap_enter: trying to map over PDP/APDP!");
d2318 1
a2318 1
			panic("pmap_enter: get ptp failed");
d2385 1
a2385 1
			pg = NULL; /* This is not page we are looking for */
d2392 1
a2392 1
			ptp_count++;      /* count # of valid entries */
d2413 1
a2413 1
				panic("pmap_enter: no pv entries available");
@


1.173
log
@
Fix some typos in comments, and remove an outdated comment about how
certain pmap structures are allocated.

No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.172 2015/04/12 18:37:53 mlarkin Exp $	*/
d297 1
a297 1
#define APTE_BASE       ((pt_entry_t *) (PDSLOT_APTE * NBPD))
d321 1
a321 2
#define	vtopte(VA)	(PTE_BASE + atop((vaddr_t)VA))

d333 4
a336 4
#define	ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define	ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define	ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define	ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */
d338 4
a341 1
#define	PDE(pm,i)	(((pd_entry_t *)(pm)->pm_pdir)[(i)])
d346 8
a353 2
typedef	u_int32_t pd_entry_t;		/* PDE */
typedef	u_int32_t pt_entry_t;		/* PTE */
d359 2
a360 2
/* the kernel's pmap (proc0) */
struct	pmap __attribute__ ((aligned (32))) kernel_pmap_store;
a370 3
#ifdef NKPDE
#error "obsolete NKPDE: use NKPTP"
#endif
d384 1
a384 1
 * UC- so mtrrs can override the cacheability;
d396 14
a449 20
 * Number of PTE's per cache line.  4 byte pte, 64-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL			16

/*
 * MULTIPROCESSOR: special VA's/ PTE's are actually allocated inside a
 * MAXCPUS*NPTECL array of PTE's, to avoid cache line thrashing
 * due to false sharing.
 */

#ifdef MULTIPROCESSOR
#define PTESLEW(pte, id) ((pte)+(id)*NPTECL)
#define VASLEW(va,id) ((va)+(id)*NPTECL*NBPG)
#else
#define PTESLEW(pte, id) (pte)
#define VASLEW(va,id) (va)
#endif

/*
a459 7

struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes in pmap.h */
void		 pmap_enter_pv(struct vm_page *, struct pv_entry *,
    struct pmap *, vaddr_t, struct vm_page *);
void		 pmap_free_pv(struct pmap *, struct pv_entry *);
void		 pmap_free_pvs(struct pmap *, struct pv_entry *);

a477 4

void		 pmap_apte_flush(void);
void		 pmap_exec_account(struct pmap *, vaddr_t, pt_entry_t,
		    pt_entry_t);
@


1.172
log
@
Bring PAE code back to life, in a different form. This diff (via bluhm then
to deraadt, then myself) brings the PAE pmap on i386 (not touched in any
significant way for years) closer to the current non-PAE pmap and allows
us to take a big next step toward better i386 W^X in the kernel (similar to
what we did a few months ago on amd64). Unlike the original PAE pmap, this
diff will not be supporting > 4GB physical memory on i386 - this effort is
specifically geared toward providing W^X (via NX) only.

There still seems to be a bug removing certain pmap entries when PAE is
enabled, so I'm leaving PAE mode disabled for the moment until we can
figure out what is going on, but with this diff in the tree hopefully
others can help.

The pmap functions now operate through function pointers, due to the need
to support both non-PAE and PAE forms. My unscientific testing showed
less than 0.3% (a third of a percent) slowdown with this approach during
a base build.

Discussed for months with guenther, kettenis, and deraadt.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.171 2015/03/13 23:23:13 mlarkin Exp $	*/
d159 1
a159 1
 * TLB mappings when making a change to a mappings.   writing to the
d162 1
a162 1
 * is useful if you are changing a single mappings because it preserves
d238 1
a238 1
 * case we take the PA of the PDP of non-active pmap and put it in
a285 38

/*
 * memory allocation
 *
 *  - there are three data structures that we must dynamically allocate:
 *
 * [A] new process' page directory page (PDP)
 *	- plan 1: done at pmap_create() we use
 *	  uvm_km_alloc(kernel_map, PAGE_SIZE)  [fka kmem_alloc] to do this
 *	  allocation.
 *
 * if we are low in free physical memory then we sleep in
 * uvm_km_alloc -- in this case this is ok since we are creating
 * a new pmap and should not be holding any locks.
 *
 * if the kernel is totally out of virtual space
 * (i.e. uvm_km_alloc returns NULL), then we panic.
 *
 * XXX: the fork code currently has no way to return an "out of
 * memory, try again" error code since uvm_fork [fka vm_fork]
 * is a void function.
 *
 * [B] new page tables pages (PTP)
 * 	call uvm_pagealloc()
 * 		=> success: zero page, add to pm_pdir
 * 		=> failure: we are out of free vm_pages, let pmap_enter()
 *		   tell UVM about it.
 *
 * note: for kernel PTPs, we start with NKPTP of them.   as we map
 * kernel memory (at uvm_map time) we check to see if we've grown
 * the kernel pmap.   if so, we call the optional function
 * pmap_growkernel() to grow the kernel PTPs in advance.
 *
 * [C] pv_entry structures
 *	call pool_get()
 *	If we fail, we simply let pmap_enter() tell UVM about it.
 */

@


1.171
log
@
move some deck chairs around in preparation for i386 PAE. no functional
change, just moving a few hundred lines of comments from one place to
another. Note that some of these comments are giant lies that will get
rewritten later.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.170 2015/03/09 07:46:03 kettenis Exp $	*/
d324 16
a339 2
#define PD_MASK         0xffc00000      /* page directory address bits */
#define PT_MASK         0x003ff000      /* page table address bits */
d344 40
a383 2
#define pdei(VA)        (((VA) & PD_MASK) >> PDSHIFT)
#define ptei(VA)        (((VA) & PT_MASK) >> PAGE_SHIFT)
d389 2
a390 1
struct pmap kernel_pmap_store;	/* the kernel's pmap (proc0) */
d400 1
d405 2
d425 11
a435 2
static pt_entry_t protection_codes[8];     /* maps MI prot to i386 prot code */
static boolean_t pmap_initialized = FALSE; /* pmap_init done yet? */
d492 2
a493 2
static pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte, *flsh_pte;
static caddr_t csrcp, cdstp, zerop, ptpp, flshp;
d500 1
a500 5
struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t, pt_entry_t);
struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
#define ALLOCPV_NEED	0	/* need PV now */
#define ALLOCPV_TRY	1	/* just try to allocate */
#define ALLOCPV_NONEED	2	/* don't need PV, just growing cache */
a504 9
struct vm_page	*pmap_get_ptp(struct pmap *, int, boolean_t);
void		 pmap_drop_ptp(struct pmap *, vaddr_t, struct vm_page *,
    pt_entry_t *);
void		 pmap_sync_flags_pte(struct vm_page *, u_long);
pt_entry_t	*pmap_map_ptes(struct pmap *);
struct pv_entry	*pmap_remove_pv(struct vm_page *, struct pmap *, vaddr_t);
void		 pmap_do_remove(struct pmap *, vaddr_t, vaddr_t, int);
void		 pmap_remove_ptes(struct pmap *, struct vm_page *, vaddr_t,
    vaddr_t, vaddr_t, int);
d506 7
d514 1
a514 1
void		 pmap_pv_page_free(struct pool *, void *);
d520 4
a523 2
#define PMAP_REMOVE_ALL		0
#define PMAP_REMOVE_SKIPWIRED	1
a524 2
vaddr_t		 pmap_tmpmap_pa(paddr_t);
void		 pmap_tmpunmap_pa(void);
d526 1
a526 2
void		pmap_unmap_ptes(struct pmap *);
void		pmap_exec_account(struct pmap *, vaddr_t, pt_entry_t,
d529 3
a531 26
void			pmap_pinit(pmap_t);

void			pmap_zero_phys(paddr_t);

void	setcslimit(struct pmap *, struct trapframe *, struct pcb *, vaddr_t);

/*
 * p m a p   i n l i n e   h e l p e r   f u n c t i o n s
 */

/*
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
 */

static __inline boolean_t
pmap_is_active(struct pmap *pmap, struct cpu_info *ci)
{

	return (pmap == pmap_kernel() || ci->ci_curpmap == pmap);
}

static __inline boolean_t
pmap_is_curpmap(struct pmap *pmap)
{
	return (pmap_is_active(pmap, curcpu()));
}
d534 1
a534 1
pmap_pte2flags(u_long pte)
a539 7
static __inline u_int
pmap_flags2pte(u_long pte)
{
	return (((pte & PG_PMAP_REF) ? PG_U : 0) |
	    ((pte & PG_PMAP_MOD) ? PG_M : 0));
}

d541 1
a541 1
pmap_sync_flags_pte(struct vm_page *pg, u_long pte)
a547 45
/*
 * pmap_tmpmap_pa: map a page in for tmp usage
 */

vaddr_t
pmap_tmpmap_pa(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(ptpp, id);
#if defined(DIAGNOSTIC)
	if (*ptpte)
		panic("pmap_tmpmap_pa: ptp_pte in use?");
#endif
	*ptpte = PG_V | PG_RW | pa;		/* always a new mapping */
	return((vaddr_t)ptpva);
}

/*
 * pmap_tmpunmap_pa: unmap a tmp use page (undoes pmap_tmpmap_pa)
 */

void
pmap_tmpunmap_pa()
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(ptpp, id);
#if defined(DIAGNOSTIC)
	if (!pmap_valid_entry(*ptpte))
		panic("pmap_tmpunmap_pa: our pte invalid?");
#endif
	*ptpte = 0;
	pmap_update_pg((vaddr_t)ptpva);
#ifdef MULTIPROCESSOR
	/*
	 * No need for tlb shootdown here, since ptp_pte is per-CPU.
	 */
#endif
}

d563 1
a563 1
pmap_map_ptes(struct pmap *pmap)
d597 1
a597 1
pmap_unmap_ptes(struct pmap *pmap)
d612 1
a612 1
    pt_entry_t opte, pt_entry_t npte)
d697 108
d854 2
a855 1
	pt_entry_t *pte, opte, npte;
d857 8
a864 8
	pte = vtopte(va);
	npte = (pa & PMAP_PA_MASK) | ((prot & PROT_WRITE)? PG_RW : PG_RO) |
	    PG_V | PG_U | PG_M | ((pa & PMAP_NOCACHE) ? PG_N : 0) |
	    ((pa & PMAP_WC) ? pmap_pg_wc : 0);

	/* special 1:1 mappings in the first 4MB must not be global */
	if (va >= (vaddr_t)NBPD)
		npte |= pmap_pg_g;
d866 6
a871 3
	opte = i386_atomic_testset_ul(pte, npte);
	if (pmap_valid_entry(opte)) {
		if (pa & PMAP_NOCACHE && (opte & PG_N) == 0)
d891 1
a891 1
	pt_entry_t *pte, opte;
d897 1
a897 2
		pte = kvtopte(va);
		opte = i386_atomic_testset_ul(pte, 0);
d899 1
a899 1
		if (opte & PG_PVLIST)
d982 2
a983 2
	kpm->pm_pdir = (pd_entry_t *)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
	kpm->pm_pdirpa = (u_int32_t) proc0.p_addr->u_pcb.pcb_cr3;
d1026 1
a1026 1
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;
d1028 1
a1028 1
	cdstp = (caddr_t) virtual_avail+PAGE_SIZE;  cdst_pte = pte+1;
d1030 1
a1030 1
	zerop = (caddr_t) virtual_avail+PAGE_SIZE*2;  zero_pte = pte+2;
d1032 1
a1032 1
	ptpp = (caddr_t) virtual_avail+PAGE_SIZE*3;  ptp_pte = pte+3;
d1034 1
a1034 1
	flshp = (caddr_t) virtual_avail+PAGE_SIZE*4;  flsh_pte = pte+4;
d1039 1
a1039 1
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;	/* allocate */
d1042 1
a1042 1
	cdstp = (caddr_t) virtual_avail;  cdst_pte = pte;
d1045 1
a1045 1
	zerop = (caddr_t) virtual_avail;  zero_pte = pte;
d1048 1
a1048 1
	ptpp = (caddr_t) virtual_avail;  ptp_pte = pte;
d1051 1
a1051 1
	flshp = (caddr_t) virtual_avail;  flsh_pte = pte;
d1081 1
a1081 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
d1098 1
a1098 1
pmap_prealloc_lowmem_ptp(paddr_t ptppa)
d1103 6
d1111 1
a1111 1
	npte = ptppa | PG_RW | PG_V | PG_U | PG_M;
a1283 3
 * => we may need to lock pv lists if we have to steal a PTP
 * => just_try: true if we want a PTP, but not enough to steal one
 * 	from another pmap (e.g. during optional functions like pmap_copy)
d1287 1
a1287 2
pmap_alloc_ptp(struct pmap *pmap, int pde_index, boolean_t just_try,
    pt_entry_t pde_flags)
d1299 1
a1299 1
	pmap->pm_pdir[pde_index] = (pd_entry_t)(VM_PAGE_TO_PHYS(ptp) |
d1314 1
a1314 1
pmap_get_ptp(struct pmap *pmap, int pde_index, boolean_t just_try)
d1318 1
a1318 2
	if (pmap_valid_entry(pmap->pm_pdir[pde_index])) {

d1321 1
a1321 1
		    (pmap->pm_pdir[pde_index] & PG_FRAME) ==
d1335 1
a1335 1
	return (pmap_alloc_ptp(pmap, pde_index, just_try, PG_u));
d1342 1
a1342 1
	i386_atomic_testset_ul(&pm->pm_pdir[pdei(va)], 0);
a1376 3
	pmap_pinit(pmap);
	return(pmap);
}
a1377 7
/*
 * pmap_pinit: given a zero'd pmap structure, init it.
 */

void
pmap_pinit(struct pmap *pmap)
{
d1386 5
d1394 7
d1402 2
a1403 2
	pmap->pm_pdir = (pd_entry_t *) uvm_km_alloc(kernel_map, NBPG);
	if (pmap->pm_pdir == NULL)
d1405 3
a1407 2
	(void) pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
			    (paddr_t *)&pmap->pm_pdirpa);
d1411 1
a1411 1
	bzero(pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
d1413 2
a1414 7
	pmap->pm_pdir[PDSLOT_PTE] = pmap->pm_pdirpa | PG_V | PG_KW | PG_U |
	    PG_M;

	/* init the LDT */
	pmap->pm_ldt = NULL;
	pmap->pm_ldt_len = 0;
	pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);
d1423 1
a1423 1
	bcopy(&PDP_BASE[PDSLOT_KERN], &pmap->pm_pdir[PDSLOT_KERN],
d1426 1
a1426 1
	bzero(&pmap->pm_pdir[PDSLOT_KERN + nkpde],
d1458 2
a1459 2
	uvm_km_free(kernel_map, (vaddr_t)pmap->pm_pdir, NBPG);
	pmap->pm_pdir = NULL;
d1620 1
a1620 1
pmap_extract(struct pmap *pmap, vaddr_t va, paddr_t *pap)
d1624 2
a1625 2
	if (pmap_valid_entry(pmap->pm_pdir[pdei(va)])) {
		ptes = pmap_map_ptes(pmap);
d1627 1
a1627 1
		pmap_unmap_ptes(pmap);
d1665 1
a1665 1
pmap_zero_phys(paddr_t pa)
d1671 1
a1671 1
	caddr_t zerova = VASLEW(zerop, id);
d1689 1
a1689 1
pmap_zero_page_uncached(paddr_t pa)
d1695 1
a1695 1
	caddr_t zerova = VASLEW(zerop, id);
d1704 1
a1704 1
	pagezero(zerova, PAGE_SIZE);				/* zero */
d1735 2
a1736 2
	pt_entry_t *pte = PTESLEW(flsh_pte, id);
	caddr_t va = VASLEW(flshp, id);
d1739 9
d1765 1
a1765 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
d1774 2
a1775 2
	caddr_t csrcva = VASLEW(csrcp, id);
	caddr_t cdstva = VASLEW(cdstp, id);
d1806 1
a1806 1
pmap_remove_ptes(struct pmap *pmap, struct vm_page *ptp, vaddr_t ptpva,
d1867 1
a1867 1
		pmap_sync_flags_pte(pg, opte);
d1893 1
a1893 1
pmap_do_remove(struct pmap *pmap, vaddr_t sva, vaddr_t eva, int flags)
d1905 1
a1905 1
	ptes = pmap_map_ptes(pmap);	/* locks pmap */
d1941 1
a1941 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(va)]))
d1946 1
a1946 1
		ptppa = (pmap->pm_pdir[pdei(va)] & PG_FRAME);
d1965 1
a1965 1
		pmap_remove_ptes(pmap, ptp, (vaddr_t)&ptes[atop(va)],
d1982 1
a1982 1
	pmap_unmap_ptes(pmap);
d1996 1
a1996 1
pmap_page_remove(struct vm_page *pg)
d2009 1
a2009 2
		ptes = pmap_map_ptes(pve->pv_pmap);	/* locks pmap */

d2011 1
a2011 1
		if (pve->pv_ptp && (pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] &
d2015 1
a2015 1
			       pg, pve->pv_va, pve->pv_ptp);
d2017 2
a2018 2
			       "actual=%x, recorded=%lx\n",
			       (pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] &
d2021 2
a2022 2
			      "invalid pv_ptp field");
		}
a2023 1

d2031 1
a2031 1
		pmap_sync_flags_pte(pg, opte);
d2042 1
a2042 1
		pmap_unmap_ptes(pve->pv_pmap);	/* unlocks pmap */
d2066 1
a2066 1
pmap_test_attrs(struct vm_page *pg, int testbits)
d2080 1
a2080 1
		ptes = pmap_map_ptes(pve->pv_pmap);
d2082 1
a2082 1
		pmap_unmap_ptes(pve->pv_pmap);
d2101 1
a2101 1
pmap_clear_attrs(struct vm_page *pg, int clearbits)
d2115 1
a2115 1
		ptes = pmap_map_ptes(pve->pv_pmap);	/* locks pmap */
d2117 2
a2118 2
		if (!pmap_valid_entry(pve->pv_pmap->pm_pdir[pdei(pve->pv_va)]))
			panic("pmap_change_attrs: mapping without PTP "
d2129 1
a2129 1
		pmap_unmap_ptes(pve->pv_pmap);	/* unlocks pmap */
d2163 1
a2163 1
pmap_write_protect(struct pmap *pmap, vaddr_t sva, vaddr_t eva,
d2172 1
a2172 1
	ptes = pmap_map_ptes(pmap);		/* locks pmap */
d2200 1
a2200 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(va)]))
d2235 1
a2235 1
	pmap_unmap_ptes(pmap);		/* unlocks pmap */
d2249 1
a2249 1
pmap_unwire(struct pmap *pmap, vaddr_t va)
d2253 2
a2254 2
	if (pmap_valid_entry(pmap->pm_pdir[pdei(va)])) {
		ptes = pmap_map_ptes(pmap);		/* locks pmap */
d2258 2
a2259 1
			panic("pmap_unwire: invalid (unmapped) va 0x%lx", va);
d2261 1
d2268 1
a2268 1
			printf("pmap_unwire: wiring for pmap %p va 0x%lx "
d2272 1
a2272 1
		pmap_unmap_ptes(pmap);		/* unlocks map */
d2276 1
a2276 1
		panic("pmap_unwire: invalid PDE");
d2318 1
a2318 1
pmap_enter(struct pmap *pmap, vaddr_t va, paddr_t pa,
d2343 1
a2343 1
	    !pmap_valid_entry(pmap->pm_pdir[pdei(va)]))
d2356 1
a2356 1
	ptes = pmap_map_ptes(pmap);		/* locks pmap */
d2360 1
a2360 1
		ptp = pmap_get_ptp(pmap, pdei(va), FALSE);
d2403 5
a2407 4
					panic("pmap_enter: same pa PG_PVLIST "
					      "mapping with unmanaged page "
					      "pa = 0x%lx (0x%lx)", pa,
					      atop(pa));
d2409 1
a2409 1
				pmap_sync_flags_pte(pg, opte);
d2427 1
a2427 1
				panic("pmap_enter: PG_PVLIST mapping with "
d2431 1
a2431 1
			pmap_sync_flags_pte(pg, opte);
d2500 1
a2500 1
		pmap_sync_flags_pte(pg, npte);
d2521 1
a2521 1
	pmap_unmap_ptes(pmap);
d2536 1
a2536 1
pmap_growkernel(vaddr_t maxkvaddr)
d2566 1
a2566 1
			pmap_zero_phys(ptaddr);
d2568 1
a2568 1
			kpm->pm_pdir[PDSLOT_KERN + nkpde] =
d2582 1
a2582 1
		while (!pmap_alloc_ptp(kpm, PDSLOT_KERN + nkpde, FALSE, 0))
d2587 2
a2588 2
			pm->pm_pdir[PDSLOT_KERN + nkpde] =
				kpm->pm_pdir[PDSLOT_KERN + nkpde];
d2608 1
a2608 1
pmap_dump(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
d2621 1
a2621 1
	ptes = pmap_map_ptes(pmap);	/* locks pmap */
d2635 1
a2635 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
d2646 1
a2646 1
	pmap_unmap_ptes(pmap);
d2861 28
@


1.170
log
@Switch pv entries over to a pool in preparation for making the i386 (more)
mpsafe.  Most (all?) other architectures now use pools for this, including
non-direct pmap architetcures like sparc and sparc64.  Use a special back-end
allocator for pool pages to solve bootstrapping problems.  This back-end
allocator allocates the initial pages from kernel_map, switching to the
uvm_km_page allocator once the pmap has been fully initialized.  The old
pv entry allocator allocated pages from kmem_map.  Using the uvm_km_page
allocator avoids certain locking issues, but might change behaviour under
kva pressure.  Time will tell if that's a good or a bad thing.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.169 2015/02/11 05:54:48 dlg Exp $	*/
a77 9
 * general info:
 *
 *  - for an explanation of how the i386 MMU hardware works see
 *    the comments in <machine/pte.h>.
 *
 *  - for an explanation of the general memory structure used by
 *    this pmap (including the recursive mapping), see the comments
 *    in <machine/pmap.h>.
 *
d112 176
a321 18
 */
/*
 * locking
 *
 * we have the following locks that we must contend with:
 *
 * "simple" locks:
 *
 * - pmap lock (per pmap, part of uvm_object)
 *   this lock protects the fields in the pmap structure including
 *   the non-kernel PDEs in the PDP, and the PTEs.  it also locks
 *   in the alternate PTE space (since that is determined by the
 *   entry in the PDP).
 *
 * - pmaps_lock
 *   this lock protects the list of active pmaps (headed by "pmaps").
 *   we lock it when adding or removing pmaps from this list.
 *
@


1.169
log
@deprecate use of sys/lock.h and replace it with sys/atomic.h or
machine/lock.h as appropriate.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.168 2015/02/02 09:29:53 mlarkin Exp $	*/
a118 3
 *  - pv_page/pv_page_info: pv_entry's are allocated out of pv_page's.
 *      if we run out of pv_entry's we allocate a new pv_page and free
 *      its pv_entrys.
d153 1
a153 15
 *	- plan 1: try to allocate one off the free list
 *		=> success: done!
 *		=> failure: no more free pv_entrys on the list
 *	- plan 2: try to allocate a new pv_page to add a chunk of
 *	pv_entrys to the free list
 *		[a] obtain a free, unmapped, VA in kmem_map.  either
 *		we have one saved from a previous call, or we allocate
 *		one now using a "vm_map_lock_try" in uvm_map
 *		=> success: we have an unmapped VA, continue to [b]
 *		=> failure: unable to lock kmem_map or out of VA in it.
 *			move on to plan 3.
 *		[b] allocate a page for the VA
 *		=> success: map it in, free the pv_entry's, DONE!
 *		=> failure: no free vm_pages, etc.
 *			save VA for later call to [a], go to plan 3.
a168 4
 * - pvalloc_lock
 *   this lock protects the data structures which are used to manage
 *   the free list of pv_entry structures.
 *
d224 9
a242 15
 * pv_page management structures: locked by pvalloc_lock
 */

TAILQ_HEAD(pv_pagelist, pv_page);
static struct pv_pagelist pv_freepages;	/* list of pv_pages with free entries */
static struct pv_pagelist pv_unusedpgs; /* list of unused pv_pages */
static int pv_nfpvents;			/* # of free pv entries */
static struct pv_page *pv_initpage;	/* bootstrap page from kernel_map */
static vaddr_t pv_cachedva;		/* cached VA for later use */

#define PVE_LOWAT (PVE_PER_PVPAGE / 2)	/* free pv_entry low water mark */
#define PVE_HIWAT (PVE_LOWAT + (PVE_PER_PVPAGE * 2))
					/* high water mark */

/*
a285 1
struct pv_entry	*pmap_add_pvpage(struct pv_page *, boolean_t);
a290 1
struct pv_entry	*pmap_alloc_pvpage(struct pmap *, int);
a294 2
void		 pmap_free_pv_doit(struct pv_entry *);
void		 pmap_free_pvpage(void);
d305 7
a832 2
	TAILQ_INIT(&pv_freepages);
	TAILQ_INIT(&pv_unusedpgs);
d840 2
d880 5
a884 5
	 * now we need to free enough pv_entry structures to allow us to get
	 * the kmem_map allocated and inited (done after this function is
	 * finished).  to do this we allocate one bootstrap page out of
	 * kernel_map and use it to provide an initial pool of pv_entry
	 * structures.   we never free this page.
d887 2
a888 6
	pv_initpage = (struct pv_page *) uvm_km_alloc(kernel_map, PAGE_SIZE);
	if (pv_initpage == NULL)
		panic("pmap_init: pv_initpage");
	pv_cachedva = 0;   /* a VA we have allocated but not used yet */
	pv_nfpvents = 0;
	(void) pmap_add_pvpage(pv_initpage, FALSE);
a912 2
 * => we lock pvalloc_lock
 * => if we fail, we call out to pmap_alloc_pvpage
d925 1
a925 177
	struct pv_page *pvpage;
	struct pv_entry *pv;

	if (!TAILQ_EMPTY(&pv_freepages)) {
		pvpage = TAILQ_FIRST(&pv_freepages);
		pvpage->pvinfo.pvpi_nfree--;
		if (pvpage->pvinfo.pvpi_nfree == 0) {
			/* nothing left in this one? */
			TAILQ_REMOVE(&pv_freepages, pvpage, pvinfo.pvpi_list);
		}
		pv = pvpage->pvinfo.pvpi_pvfree;
#ifdef DIAGNOSTIC
		if (pv == NULL)
			panic("pmap_alloc_pv: pvpi_nfree off");
#endif
		pvpage->pvinfo.pvpi_pvfree = pv->pv_next;
		pv_nfpvents--;  /* took one from pool */
	} else {
		pv = NULL;		/* need more of them */
	}

	/*
	 * if below low water mark or we didn't get a pv_entry we try and
	 * create more pv_entrys ...
	 */

	if (pv_nfpvents < PVE_LOWAT || pv == NULL) {
		if (pv == NULL)
			pv = pmap_alloc_pvpage(pmap, (mode == ALLOCPV_TRY) ?
					       mode : ALLOCPV_NEED);
		else
			(void) pmap_alloc_pvpage(pmap, ALLOCPV_NONEED);
	}

	return(pv);
}

/*
 * pmap_alloc_pvpage: maybe allocate a new pvpage
 *
 * if need_entry is false: try and allocate a new pv_page
 * if need_entry is true: try and allocate a new pv_page and return a
 *	new pv_entry from it.
 *
 * => we assume that the caller holds pvalloc_lock
 */

struct pv_entry *
pmap_alloc_pvpage(struct pmap *pmap, int mode)
{
	struct vm_page *pg;
	struct pv_page *pvpage;
	struct pv_entry *pv;
	int s;

	/*
	 * if we need_entry and we've got unused pv_pages, allocate from there
	 */

	if (mode != ALLOCPV_NONEED && !TAILQ_EMPTY(&pv_unusedpgs)) {

		/* move it to pv_freepages list */
		pvpage = TAILQ_FIRST(&pv_unusedpgs);
		TAILQ_REMOVE(&pv_unusedpgs, pvpage, pvinfo.pvpi_list);
		TAILQ_INSERT_HEAD(&pv_freepages, pvpage, pvinfo.pvpi_list);

		/* allocate a pv_entry */
		pvpage->pvinfo.pvpi_nfree--;	/* can't go to zero */
		pv = pvpage->pvinfo.pvpi_pvfree;
#ifdef DIAGNOSTIC
		if (pv == NULL)
			panic("pmap_alloc_pvpage: pvpi_nfree off");
#endif
		pvpage->pvinfo.pvpi_pvfree = pv->pv_next;

		pv_nfpvents--;  /* took one from pool */
		return(pv);
	}

	/*
	 *  see if we've got a cached unmapped VA that we can map a page in.
	 * if not, try to allocate one.
	 */

	s = splvm();   /* must protect kmem_map with splvm! */
	if (pv_cachedva == 0) {
		pv_cachedva = uvm_km_kmemalloc(kmem_map, NULL,
		    NBPG, UVM_KMF_TRYLOCK|UVM_KMF_VALLOC);
	}
	splx(s);
	if (pv_cachedva == 0)
		return (NULL);

	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (pg == NULL)
		return (NULL);

	atomic_clearbits_int(&pg->pg_flags, PG_BUSY);

	/*
	 * add a mapping for our new pv_page and free its entries (save one!)
	 *
	 * NOTE: If we are allocating a PV page for the kernel pmap, the
	 * pmap is already locked!  (...but entering the mapping is safe...)
	 */

	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg),
	    PROT_READ | PROT_WRITE);
	pvpage = (struct pv_page *) pv_cachedva;
	pv_cachedva = 0;
	return (pmap_add_pvpage(pvpage, mode != ALLOCPV_NONEED));
}

/*
 * pmap_add_pvpage: add a pv_page's pv_entrys to the free list
 *
 * => caller must hold pvalloc_lock
 * => if need_entry is true, we allocate and return one pv_entry
 */

struct pv_entry *
pmap_add_pvpage(struct pv_page *pvp, boolean_t need_entry)
{
	int tofree, lcv;

	/* do we need to return one? */
	tofree = (need_entry) ? PVE_PER_PVPAGE - 1 : PVE_PER_PVPAGE;

	pvp->pvinfo.pvpi_pvfree = NULL;
	pvp->pvinfo.pvpi_nfree = tofree;
	for (lcv = 0 ; lcv < tofree ; lcv++) {
		pvp->pvents[lcv].pv_next = pvp->pvinfo.pvpi_pvfree;
		pvp->pvinfo.pvpi_pvfree = &pvp->pvents[lcv];
	}
	if (need_entry)
		TAILQ_INSERT_TAIL(&pv_freepages, pvp, pvinfo.pvpi_list);
	else
		TAILQ_INSERT_TAIL(&pv_unusedpgs, pvp, pvinfo.pvpi_list);
	pv_nfpvents += tofree;
	return((need_entry) ? &pvp->pvents[lcv] : NULL);
}

/*
 * pmap_free_pv_doit: actually free a pv_entry
 *
 * => do not call this directly!  instead use either
 *    1. pmap_free_pv ==> free a single pv_entry
 *    2. pmap_free_pvs => free a list of pv_entrys
 * => we must be holding pvalloc_lock
 */

void
pmap_free_pv_doit(struct pv_entry *pv)
{
	struct pv_page *pvp;

	pvp = (struct pv_page*)trunc_page((vaddr_t)pv);
	pv_nfpvents++;
	pvp->pvinfo.pvpi_nfree++;

	/* nfree == 1 => fully allocated page just became partly allocated */
	if (pvp->pvinfo.pvpi_nfree == 1) {
		TAILQ_INSERT_HEAD(&pv_freepages, pvp, pvinfo.pvpi_list);
	}

	/* free it */
	pv->pv_next = pvp->pvinfo.pvpi_pvfree;
	pvp->pvinfo.pvpi_pvfree = pv;

	/*
	 * are all pv_page's pv_entry's free?  move it to unused queue.
	 */

	if (pvp->pvinfo.pvpi_nfree == PVE_PER_PVPAGE) {
		TAILQ_REMOVE(&pv_freepages, pvp, pvinfo.pvpi_list);
		TAILQ_INSERT_HEAD(&pv_unusedpgs, pvp, pvinfo.pvpi_list);
	}
a929 2
 *
 * => we gain the pvalloc_lock
d935 1
a935 9
	pmap_free_pv_doit(pv);

	/*
	 * Can't free the PV page if the PV entries were associated with
	 * the kernel pmap; the pmap is already locked.
	 */
	if (pv_nfpvents > PVE_HIWAT && TAILQ_FIRST(&pv_unusedpgs) != NULL &&
	    pmap != pmap_kernel())
		pmap_free_pvpage();
a939 2
 *
 * => we gain the pvalloc_lock
d949 1
a949 1
		pmap_free_pv_doit(pvs);
d951 1
d953 4
a956 8
	/*
	 * Can't free the PV page if the PV entries were associated with
	 * the kernel pmap; the pmap is already locked.
	 */
	if (pv_nfpvents > PVE_HIWAT && TAILQ_FIRST(&pv_unusedpgs) != NULL &&
	    pmap != pmap_kernel())
		pmap_free_pvpage();
}
d958 2
d961 3
a963 7
/*
 * pmap_free_pvpage: try and free an unused pv_page structure
 *
 * => assume caller is holding the pvalloc_lock and that
 *	there is a page on the pv_unusedpgs list
 * => if we can't get a lock on the kmem_map we try again later
 */
d966 1
a966 1
pmap_free_pvpage(void)
d968 1
a968 38
	int s;
	struct vm_map *map;
	struct uvm_map_deadq dead_entries;
	struct pv_page *pvp;

	s = splvm(); /* protect kmem_map */
	pvp = TAILQ_FIRST(&pv_unusedpgs);

	/*
	 * note: watch out for pv_initpage which is allocated out of
	 * kernel_map rather than kmem_map.
	 */

	if (pvp == pv_initpage)
		map = kernel_map;
	else
		map = kmem_map;
	if (vm_map_lock_try(map)) {

		/* remove pvp from pv_unusedpgs */
		TAILQ_REMOVE(&pv_unusedpgs, pvp, pvinfo.pvpi_list);

		/* unmap the page */
		TAILQ_INIT(&dead_entries);
		uvm_unmap_remove(map, (vaddr_t)pvp, ((vaddr_t)pvp) + PAGE_SIZE,
		    &dead_entries, FALSE, TRUE);
		vm_map_unlock(map);

		uvm_unmap_detach(&dead_entries, 0);

		pv_nfpvents -= PVE_PER_PVPAGE;  /* update free count */
	}

	if (pvp == pv_initpage)
		/* no more initpage, we've freed it */
		pv_initpage = NULL;

	splx(s);
@


1.168
log
@
Remove some pmap locks that were #defined to be nothing (empty). Discussed
with many, ok kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.167 2015/01/27 02:09:07 mlarkin Exp $	*/
d68 1
@


1.167
log
@
Remove an unused variable whose functionality was moved to locore long ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.166 2015/01/09 03:43:52 mlarkin Exp $	*/
a194 10
/*
 * locking data structures
 */

#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */

#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

a1912 1
 	PMAP_MAP_TO_HEAD_LOCK();
a1990 1
	PMAP_MAP_TO_HEAD_UNLOCK();
a2015 2
	PMAP_HEAD_TO_MAP_LOCK();

a2055 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a2086 1
	PMAP_HEAD_TO_MAP_LOCK();
a2094 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a2119 2
	PMAP_HEAD_TO_MAP_LOCK();

a2141 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a2359 3
	/* get lock */
	PMAP_MAP_TO_HEAD_LOCK();

a2528 1
	PMAP_MAP_TO_HEAD_UNLOCK();
a2627 1
	PMAP_MAP_TO_HEAD_LOCK();
a2653 1
	PMAP_MAP_TO_HEAD_UNLOCK();
@


1.166
log
@
Cleanup some macros and #defines in i386 pmap. Previously committed and
backed out because of libkvm breakage, recommitting now with libkvm fix.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.165 2014/12/23 01:24:50 deraadt Exp $	*/
a316 3

/* stuff to fix the pentium f00f bug */
extern vaddr_t pentium_idt_vaddr;
@


1.165
log
@backout previous, because libkvm needs two pieces.  will let mike
find a different way.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.163 2014/12/02 18:13:10 tedu Exp $	*/
d204 9
@


1.164
log
@Move PD_MASK, PT_MASK and a couple macros into pmap.c. The only other
user of these was hibernate, which now gets its own PD_MASK (since
the resume time PD_MASK is essentially disjoint from the runtime
PD_MASK). No functional change, just moving the deck chairs around in
preparation for an upcoming change.

ok deraadt
@
text
@a204 9
#define PD_MASK         0xffc00000      /* page directory address bits */
#define PT_MASK         0x003ff000      /* page table address bits */

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define pdei(VA)        (((VA) & PD_MASK) >> PDSHIFT)
#define ptei(VA)        (((VA) & PT_MASK) >> PAGE_SHIFT)

@


1.163
log
@delete all the simplelocks. ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.162 2014/11/19 20:09:01 mlarkin Exp $	*/
d204 9
@


1.162
log
@
Remove some unused i386 pmap functions. Also fix two typos in comments
while I had the hood open.

ok deraadt@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.161 2014/11/16 12:30:57 deraadt Exp $	*/
a198 3
struct simplelock pvalloc_lock;
struct simplelock pmaps_lock;

a468 1
		simple_lock(&pmap->pm_obj.vmobjlock);
a471 9
	/* need to lock both curpmap and pmap: use ordered locking */
	if ((unsigned) pmap < (unsigned) curpcb->pcb_pmap) {
		simple_lock(&pmap->pm_obj.vmobjlock);
		simple_lock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
	} else {
		simple_lock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
		simple_lock(&pmap->pm_obj.vmobjlock);
	}

d497 1
a497 3
	if (pmap_is_curpmap(pmap)) {
		simple_unlock(&pmap->pm_obj.vmobjlock);
	} else {
a501 2
		simple_unlock(&pmap->pm_obj.vmobjlock);
		simple_unlock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
a858 2
	simple_lock_init(&pvalloc_lock);
	simple_lock_init(&pmaps_lock);
a960 2
	simple_lock(&pvalloc_lock);

a991 1
	simple_unlock(&pvalloc_lock);
a1145 1
	simple_lock(&pvalloc_lock);
a1154 2

	simple_unlock(&pvalloc_lock);
a1167 2
	simple_lock(&pvalloc_lock);

a1179 2

	simple_unlock(&pvalloc_lock);
a1442 1
	simple_lock(&pmaps_lock);
a1449 1
	simple_unlock(&pmaps_lock);
a1470 1
	simple_lock(&pmaps_lock);
a1471 1
	simple_unlock(&pmaps_lock);
a1506 1
	simple_lock(&pmap->pm_obj.vmobjlock);
a1507 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
a1518 3
	simple_lock(&pmap1->pm_obj.vmobjlock);
	simple_lock(&pmap2->pm_obj.vmobjlock);

a1537 3

	simple_unlock(&pmap2->pm_obj.vmobjlock);
	simple_unlock(&pmap1->pm_obj.vmobjlock);
a1554 2
	simple_lock(&pmap->pm_obj.vmobjlock);

a1573 2
	simple_unlock(&pmap->pm_obj.vmobjlock);

a2577 1
	simple_lock(&kpm->pm_obj.vmobjlock);
a2610 1
		simple_lock(&pmaps_lock);
a2614 1
		simple_unlock(&pmaps_lock);
a2616 1
	simple_unlock(&kpm->pm_obj.vmobjlock);
@


1.161
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.160 2014/07/11 16:35:40 jsg Exp $	*/
a345 1
pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
a346 1
void		 pmap_tmpunmap_pvepte(struct pv_entry *);
a445 36
/*
 * pmap_tmpmap_pvepte: get a quick mapping of a PTE for a pv_entry
 *
 * => do NOT use this on kernel mappings [why?  because pv_ptp may be NULL]
 */

pt_entry_t *
pmap_tmpmap_pvepte(struct pv_entry *pve)
{
#ifdef DIAGNOSTIC
	if (pve->pv_pmap == pmap_kernel())
		panic("pmap_tmpmap_pvepte: attempt to map kernel");
#endif

	/* is it current pmap?  use direct mapping... */
	if (pmap_is_curpmap(pve->pv_pmap))
		return(vtopte(pve->pv_va));

	return(((pt_entry_t *)pmap_tmpmap_pa(VM_PAGE_TO_PHYS(pve->pv_ptp)))
	       + ptei((unsigned)pve->pv_va));
}

/*
 * pmap_tmpunmap_pvepte: release a mapping obtained with pmap_tmpmap_pvepte
 */

void
pmap_tmpunmap_pvepte(struct pv_entry *pve)
{
	/* was it current pmap?   if so, return */
	if (pmap_is_curpmap(pve->pv_pmap))
		return;

	pmap_tmpunmap_pa();
}

d722 1
a722 1
 *	a PDP for the kernel, and nkpde PTP's for the kernel.
d2740 1
a2740 1
 * Our shootdown is last to make it parallell with the other cpus
@


1.160
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.159 2014/01/06 14:29:25 sf Exp $	*/
d608 1
a608 1
		if (ent->protection & VM_PROT_EXECUTE)
d699 1
a699 1
	npte = (pa & PMAP_PA_MASK) | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) |
d798 8
a805 8
	protection_codes[UVM_PROT_NONE] = 0;  			/* --- */
	protection_codes[UVM_PROT_EXEC] = PG_X;			/* --x */
	protection_codes[UVM_PROT_READ] = PG_RO;		/* -r- */
	protection_codes[UVM_PROT_RX] = PG_X;			/* -rx */
	protection_codes[UVM_PROT_WRITE] = PG_RW;		/* w-- */
	protection_codes[UVM_PROT_WX] = PG_RW|PG_X;		/* w-x */
	protection_codes[UVM_PROT_RW] = PG_RW;			/* wr- */
	protection_codes[UVM_PROT_RWX] = PG_RW|PG_X;		/* wrx */
d1125 1
a1125 1
	    VM_PROT_READ|VM_PROT_WRITE);
d2597 1
a2597 1
	if (flags & VM_PROT_READ)
d2599 1
a2599 1
	if (flags & VM_PROT_WRITE)
@


1.159
log
@Increase NPTECL, as cache-lines are 64-bytes nowadays.
Also move it from pmap.h to pmap.c because it is an internal detail.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.158 2013/02/13 20:45:41 kurt Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.158
log
@- Detect and correct stale GDT user code segment limit caused
by another thread in the same process running on another cpu
raising pm_highexec and pm_codeseg. w/help from kettenis@@
okay beck@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.157 2012/03/09 13:01:28 ariane Exp $	*/
d292 6
d867 1
a867 1
	 * of PTE's (8) to play with, though we only need 4.  We could
@


1.157
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.156 2012/02/19 17:14:28 kettenis Exp $	*/
d592 2
d605 1
d620 13
a632 1
	if (va <= pm->pm_hiexec) {
@


1.156
log
@small KNF nit
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.155 2011/07/08 03:35:39 kettenis Exp $	*/
d605 1
a605 5
	for (ent = (&map->header)->next; ent != &map->header; ent = ent->next) {
		/*
		 * This entry has greater va than the entries before.
		 * We need to make it point to the last page, not past it.
		 */
d607 1
a607 1
			va = trunc_page(ent->end - 1);
d609 6
d1249 1
a1249 1
	struct vm_map_entry *dead_entries;
d1270 1
a1270 1
		dead_entries = NULL;
d1272 1
a1272 1
		    &dead_entries, NULL, FALSE);
d1275 1
a1275 2
		if (dead_entries != NULL)
			uvm_unmap_detach(dead_entries, 0);
@


1.155
log
@Remove spurious prototype.

ok guenther@@, deraadt@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.154 2011/06/06 17:10:23 ariane Exp $	*/
d824 1
a824 1
		for (kva = VM_MIN_KERNEL_ADDRESS ; kva < virtual_avail ;
@


1.154
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.153 2011/05/24 15:27:36 ariane Exp $	*/
a339 2
boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *, pt_entry_t *,
    vaddr_t, int);
@


1.153
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.152 2011/03/12 03:52:26 guenther Exp $	*/
d607 5
a611 1
	RB_FOREACH_REVERSE(ent, uvm_map_addr, &map->addr) {
d613 1
a613 1
			break;
a614 6
	/*
	 * This entry has greater va than the entries before.
	 * We need to make it point to the last page, not past it.
	 */
	if (ent)
		va = trunc_page(ent->end - 1);
d1249 1
a1249 1
	struct uvm_map_deadq dead_entries;
d1270 1
a1270 1
		TAILQ_INIT(&dead_entries);
d1272 1
a1272 1
		    &dead_entries, FALSE, TRUE);
d1275 2
a1276 1
		uvm_unmap_detach(&dead_entries, 0);
@


1.152
log
@Provide distinct segments for the %fs and %gs selectors to use by
default, with per-rthread base offsets and with sysarch() functions,
I386_{GET,SET}_{FS,GS}BASE, for fetching and setting those base
offsets.  This is necessary for both rthread and Linux compat support.

suggestions from kettenis@@, prodding from pirofti@@ and deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.151 2010/11/30 19:28:59 kettenis Exp $	*/
d607 1
a607 5
	for (ent = (&map->header)->next; ent != &map->header; ent = ent->next) {
		/*
		 * This entry has greater va than the entries before.
		 * We need to make it point to the last page, not past it.
		 */
d609 1
a609 1
			va = trunc_page(ent->end - 1);
d611 6
d1251 1
a1251 1
	struct vm_map_entry *dead_entries;
d1272 1
a1272 1
		dead_entries = NULL;
d1274 1
a1274 1
		    &dead_entries, NULL, FALSE);
d1277 1
a1277 2
		if (dead_entries != NULL)
			uvm_unmap_detach(dead_entries, 0);
@


1.151
log
@Extend bitmasks to 64-bit such that we can support up to 64 CPU cores.

ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.150 2010/11/20 20:33:24 miod Exp $	*/
d1695 2
d2302 1
a2302 1
			md_prot |= (PG_u | PG_RW);
d2581 1
a2581 1
		npte |= (PG_u | PG_RW);	/* XXXCDC: no longer needed? */
@


1.150
log
@This is a first step towards getting rid of avail_start and avail_end in the
kernel, currently limited to low-hanging fruit: these variables were used
by bus_dma to specify the range in which to allocate memory, back when
uvm_pglistalloc() was stupid and would not walk the vm_physseg[].

Nowadays, except on some platforms for early initialization, these variables
are not used, or do not need to be global variables. Therefore:
- remove `extern' declarations of avail_start and avail_end (or close cousins,
  such as arm physical_start and physical_end) from files which no longer need
  to use them.
- make them local variables whenever possible.
- remove them when they are assigned to but no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.149 2010/11/20 20:21:13 miod Exp $	*/
d2783 1
a2783 1
	int mask = 0;
d2789 1
a2789 1
		mask |= 1 << ci->ci_cpuid;
d2802 1
a2802 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
d2820 1
a2820 1
	int mask = 0;
d2827 1
a2827 1
		mask |= 1 << ci->ci_cpuid;
d2841 1
a2841 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
d2860 1
a2860 1
	int mask = 0;
d2865 1
a2865 1
		mask |= 1 << ci->ci_cpuid;
d2878 1
a2878 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
d2895 1
a2895 1
	int mask = 0;
d2901 1
a2901 1
		mask |= 1 << ci->ci_cpuid;
d2914 1
a2914 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
@


1.149
log
@Check uvm_km_alloc() return values; pmap_fork() will currently panic,
while i386_set_ldt() can fail gracefully. To be improved eventually.
From mpech@@ sometime ago. ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.148 2010/05/08 16:54:08 oga Exp $	*/
a245 10

/*
 * i386 physical memory comes in a big contig chunk with a small
 * hole toward the front of it...  the following 4 paddr_t's
 * (shared with machdep.c) describe the physical address space
 * of this machine.
 */
paddr_t avail_start;	/* PA of first available physical page */
paddr_t hole_start;	/* PA of start of "hole" */
paddr_t hole_end;	/* PA of end of "hole" */
@


1.148
log
@Page Attribute Tables (PAT) support for x86.

PAT allows setting per-mapping cachability bits. Our main interest in it
for write combining mappings so we do not have to rely so heaviliy on
mtrrs (which are stupidly set up on more and more machines). MD flags to
pmap allow setting these bits (which bus_space now uses for PREFETCHABLE
maps), if a vm page has a bit set, then we will use WC for all mappings
of a page (used for userland mappings). We also check for known errata
and fall back to UC- mappings in that case.

comments from kettenis@@, tedu@@ and william@@. kettenis@@, tedu@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.147 2010/04/30 21:56:39 oga Exp $	*/
d1600 4
@


1.147
log
@Right now, if anything internal changes with a uvm object, diverse
places in the tree need to be touched to update the object
initialisation with respect to that.

So, make a function (uvm_initobj) that takes the refcount, object and
pager ops and does this initialisation for us. This should save on
maintainance in the future.

looked good to fgs@@. Tedu complained about the British spelling but OKed
it anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.146 2009/12/09 14:31:57 oga Exp $	*/
d241 7
a325 1

d696 2
a697 1
	    PG_V | PG_U | PG_M | ((pa & PMAP_NOCACHE) ? PG_N : 0);
d2422 1
d2426 1
d2594 4
d2600 2
@


1.146
log
@add two new MD only pmap apis to amd64 and i386 (not to be used in MI
code):

pmap_flush_cache(vaddr_t, vsize_t) and pmap_flush_page(paddr_t) to flush
the cache for virtual addresses and physical pages respectively using
the clflush instruction. These apis will shortly be used by the agp
bus_dma functions to avoid doing a wbinvd on each dmamap_sync.

ok kettenis@@, some comments from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.145 2009/08/11 17:15:54 oga Exp $	*/
d808 1
a808 5
	simple_lock_init(&kpm->pm_obj.vmobjlock);
	kpm->pm_obj.pgops = NULL;
	RB_INIT(&kpm->pm_obj.memt);
	kpm->pm_obj.uo_npages = 0;
	kpm->pm_obj.uo_refs = 1;
d1465 1
a1465 5
	simple_lock_init(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.pgops = NULL;	/* currently not a mappable object */
	RB_INIT(&pmap->pm_obj.memt);
	pmap->pm_obj.uo_npages = 0;
	pmap->pm_obj.uo_refs = 1;
@


1.145
log
@fix some stupidity in x86 bus_space_map.

right now, we do a pmap_kenter_pa(), we then get the pte (behind pmap's
back) and check for the cache inhibit bit (if needed). If it isn't what
we want (this is the normal case) then we change it ourselves, and do a
manual tlb shootdown (i386 was a bit more stupid about it than amd64,
too).

Instead, make it so that like on some other archs (sparc64 comes to
mind) you can pass in flags in the low bits of the physical address,
pmap then does everything correctly for you.

Discovered this when I had some code doing a lot of bus_space_maps(), it
was incredibly slow, and profilling was dominated by
pmap_tlb_shootwait();

discussed with kettenis@@, miod@@, toby@@ and art@@.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.144 2009/08/06 15:28:14 oga Exp $	*/
d312 2
a313 2
static pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte;
static caddr_t csrcp, cdstp, zerop, ptpp;
d866 2
d882 3
d1804 41
@


1.144
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.143 2009/07/25 12:55:39 miod Exp $	*/
d689 2
a690 2
	npte = pa | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) | PG_V |
	    PG_U | PG_M;
d698 2
d2376 1
d2380 2
d2533 1
a2533 1
	if (flags & PMAP_NOCACHE)
d2556 3
a2558 1
	if (opte & PG_V) {
@


1.143
log
@Add an extra argument to uvm_unmap_remove(), for the caller to tell it
whether removing holes or parts of them is allowed or not.
Only allow hole removal in uvmspace_free(), when tearing the vmspace down.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.142 2009/06/16 16:42:41 ariane Exp $	*/
d808 1
a808 1
	TAILQ_INIT(&kpm->pm_obj.memq);
d1427 1
a1427 1
		pm->pm_ptphint = TAILQ_FIRST(&pm->pm_obj.memq);
d1464 1
a1464 1
	TAILQ_INIT(&pmap->pm_obj.memq);
d1536 1
a1536 2
	while (!TAILQ_EMPTY(&pmap->pm_obj.memq)) {
		pg = TAILQ_FIRST(&pmap->pm_obj.memq);
d2011 1
a2011 1
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
d2025 1
a2025 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
d2082 1
a2082 1
			TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp, listq);
d2095 1
a2095 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
@


1.142
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.136 2009/02/05 01:13:21 oga Exp $	*/
d1272 1
a1272 1
		    &dead_entries, NULL);
@


1.141
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.137 2009/06/01 17:42:33 ariane Exp $	*/
d2012 1
a2012 1
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, fq.queues.listq);
d2026 1
a2026 1
		TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
d2083 1
a2083 2
			TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp,
			    fq.queues.listq);
d2096 1
a2096 1
		TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
@


1.140
log
@droppmap is only necessary (and declared) on MULTIPROCESSOR
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.139 2009/06/03 00:49:12 art Exp $	*/
d808 1
a808 1
	RB_INIT(&kpm->pm_obj.memt);
d1427 1
a1427 1
		pm->pm_ptphint = RB_ROOT(&pm->pm_obj.memt);
d1464 1
a1464 1
	RB_INIT(&pmap->pm_obj.memt);
d1536 2
a1537 1
	while ((pg = RB_ROOT(&pmap->pm_obj.memt)) != NULL) {
@


1.139
log
@Just like on amd64. Instead of keeping a bitmap of which cpus a pmap
is active on, save a curpmap pointer in cpu_info. This lets us simplify
a few things and do lazy context switching from a user process to a
kernel thread. There's a new IPI introduced for forcing a cr3 reload
when we're tearing down a dead pmap.

kettenis@@ ok (after I polished a few minor things)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.138 2009/06/02 23:00:19 oga Exp $	*/
d1527 1
d1529 1
@


1.138
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.137 2009/06/01 17:42:33 ariane Exp $	*/
a339 2
boolean_t	 pmap_is_curpmap(struct pmap *);
boolean_t	 pmap_is_active(struct pmap *, int);
d356 1
a356 1
void		 pmap_apte_flush(struct pmap *);
a361 1
void			pmap_release(pmap_t);
d372 1
a372 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d375 2
a376 3
boolean_t
pmap_is_curpmap(pmap)
	struct pmap *pmap;
d378 2
a379 2
	return((pmap == pmap_kernel()) ||
	       (pmap->pm_pdirpa == (paddr_t) rcr3()));
d382 2
a383 8
/*
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
 */

boolean_t
pmap_is_active(pmap, cpu_id)
	struct pmap *pmap;
	int cpu_id;
d385 1
a385 3

	return (pmap == pmap_kernel() ||
	    (pmap->pm_cpus & (1U << cpu_id)) != 0);
d446 1
a446 1
	*ptpte = 0;		/* zap! */
d492 1
a492 1
pmap_apte_flush(struct pmap *pmap)
d540 1
a540 1
			pmap_apte_flush(curpcb->pcb_pmap);
d560 1
a560 1
		pmap_apte_flush(curpcb->pcb_pmap);
d696 1
a696 1
	opte = i386_atomic_testset_ul(pte, npte); /* zap! */
d934 1
a934 1
	i386_atomic_testset_ul(pte, npte); /* zap! */
d1416 1
a1416 1
	pmap_tlb_shootpage(curpcb->pcb_pmap, ((vaddr_t)ptes) + ptp->offset);
a1471 1
	pmap->pm_cpus = 0;
d1520 1
a1522 5
	/*
	 * drop reference count
	 */

	simple_lock(&pmap->pm_obj.vmobjlock);
a1523 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1527 1
a1527 25
	/*
	 * reference count is zero, free pmap resources and then free pmap.
	 */

	pmap_release(pmap);
	pool_put(&pmap_pmap_pool, pmap);
}

/*
 * pmap_release: release all resources held by a pmap
 *
 * => if pmap is still referenced it should be locked
 * => XXX: we currently don't expect any busy PTPs because we don't
 *    allow anything to map them (except for the kernel's private
 *    recursive mapping) or make them busy.
 */

void
pmap_release(struct pmap *pmap)
{
	struct vm_page *pg;

	/*
	 * remove it from global list of pmaps
	 */
d1533 1
a1533 9
	/*
	 * Before we free the pmap just make sure it's not cached anywhere.
	 */
	tlbflushg();

	/*
	 * free any remaining PTPs
	 */

a1534 6
#ifdef DIAGNOSTIC
		if (pg->pg_flags & PG_BUSY)
			panic("pmap_release: busy page table page");
#endif
		/* pmap_page_protect?  currently no need for it. */

a1538 4
	/*
	 * MULTIPROCESSOR -- no need to flush out of other processors'
	 * APTE space because we do that in pmap_unmap_ptes().
	 */
d1556 1
d1559 1
d1647 10
a1656 6
/*
 * pmap_activate: activate a process' pmap (fill in %cr3 and LDT info)
 *
 * => called from cpu_switchto()
 * => if proc is the curproc, then load it into the MMU
 */
d1659 1
a1659 1
pmap_activate(struct proc *p)
d1662 1
a1662 1
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
d1665 3
a1676 6
	if (p == curproc) {
		/*
		 * Set the correct descriptor value (i.e. with the
		 * correct code segment X limit) in the GDT.
		 */
		self->ci_gdt[GUCODE_SEL].sd = pmap->pm_codeseg;
d1678 15
a1692 2
		lcr3(pcb->pcb_cr3);
		lldt(pcb->pcb_ldt_sel);
d1694 1
a1694 5
		/*
		 * mark the pmap in use by this processor.
		 */
		i386_atomic_setbits_l(&pmap->pm_cpus, (1U << cpu_number()));
	}
a1696 4
/*
 * pmap_deactivate: deactivate a process' pmap
 */

a1699 6
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	/*
	 * mark the pmap no longer in use by this processor.
	 */
	i386_atomic_clearbits_l(&pmap->pm_cpus, (1U << cpu_number()));
a1702 8
 * end of lifecycle functions
 */

/*
 * some misc. functions
 */

/*
d1768 1
a1768 1
	*zpte = 0;				/* zap! */
d1792 1
a1792 1
	*zpte = 0;					/* zap! */
d1823 1
a1823 1
	*spte = *dpte = 0;			/* zap! */
d1869 1
a1869 1
		/* atomically save the old PTE and zap! it */
d2731 1
a2731 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2769 1
a2769 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2832 39
@


1.137
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.136 2009/02/05 01:13:21 oga Exp $	*/
d821 1
a821 1
	TAILQ_INIT(&kpm->pm_obj.memq);
d1440 1
a1440 1
		pm->pm_ptphint = TAILQ_FIRST(&pm->pm_obj.memq);
d1477 1
a1477 1
	TAILQ_INIT(&pmap->pm_obj.memq);
d1585 1
a1585 2
	while (!TAILQ_EMPTY(&pmap->pm_obj.memq)) {
		pg = TAILQ_FIRST(&pmap->pm_obj.memq);
@


1.136
log
@add MD PMAP_NOCACHE flag to i386 and use it to implement the
BUS_DMA_NOCACHE flag with guarantees that the dma memory will be mapped
uncached. Some broken/odd hardware needs this.

discussion with miod, toby, art and kettenis. ok miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.135 2009/01/27 22:14:13 miod Exp $	*/
d2077 1
a2077 1
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
d2091 1
a2091 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
d2148 2
a2149 1
			TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp, listq);
d2162 1
a2162 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
@


1.135
log
@Get rid of the last traces of uvm.pager_[se]va
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.134 2008/12/18 14:17:28 kurt Exp $	*/
d2594 2
@


1.134
log
@Don't set the global bit PG_G for kernel pmap low memory mappings. Use a
new function pmap_prealloc_lowmem_ptp() to setup kernel pmap ptp 0 without
the PG_G bit set. This fixes the remaining reaper -> pmap_page_remove
panics. With much diagnostic help from Art and Theo.

ok  deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.133 2008/12/18 13:43:24 kurt Exp $	*/
a2120 2
		if (pve->pv_va >= uvm.pager_sva && pve->pv_va < uvm.pager_eva)
			printf("pmap_page_remove: found pager VA on pv_list\n");
@


1.133
log
@use atomic operations to update ptes in pmap_unwire(). okay weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.132 2008/11/24 16:32:45 art Exp $	*/
d703 6
a708 1
	    pmap_pg_g | PG_U | PG_M;
d931 20
@


1.132
log
@oops.
something snuck in here that wasn't supposed to be.
noticed by yuo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.131 2008/11/24 15:50:47 art Exp $	*/
d2355 1
a2355 1
			ptes[atop(va)] &= ~PG_W;
@


1.131
log
@ - Remove the special case for one page pmap_remove, it's useless.
 - Move the code that drops a ptp into a function and use that
   in both pmap_do_remove and pmap_page_remove.
 - In pmap_do_remove, instead of flushing the whole range from the tlb
   at the same time, shoot one PDE at a time. It will allow us to skip
   empty PDEs and reduces the amount of work one IPI has to do (although
   we'll get more IPIs).
ok toby@@ "I won't protest" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.130 2008/11/24 03:19:42 kurt Exp $	*/
a1474 1
#if 0
a1477 1
#endif
@


1.130
log
@In pmap_write_protect() clear and set the protection bits atomically and
leave the rest alone. Also don't read *spte twice and compare results.
feedback drahn@@ okay art@@ weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.129 2008/11/24 03:13:22 kurt Exp $	*/
d338 2
d545 4
d1399 22
d1475 1
d1479 1
a1961 65

/*
 * pmap_remove_pte: remove a single PTE from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => caller must hold pmap's lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 * => returns true if we removed a mapping
 */

boolean_t
pmap_remove_pte(struct pmap *pmap, struct vm_page *ptp, pt_entry_t *pte,
    vaddr_t va, int flags)
{
	struct pv_entry *pve;
	struct vm_page *pg;
	pt_entry_t opte;

	if (!pmap_valid_entry(*pte))
		return (FALSE);		/* VA not mapped */

	if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W))
		return (FALSE);

	/* atomically save the old PTE and zap! it */
	opte = i386_atomic_testset_ul(pte, 0);

	pmap_exec_account(pmap, va, opte, 0);

	if (opte & PG_W)
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	if (ptp)
		ptp->wire_count--;		/* dropping a PTE */

	pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);

	/*
	 * if we are not on a pv list we are done.
	 */
	if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
		if (pg != NULL)
			panic("pmap_remove_pte: managed page without "
			      "PG_PVLIST for 0x%lx", va);
#endif
		return(TRUE);
	}

#ifdef DIAGNOSTIC
	if (pg == NULL)
		panic("pmap_remove_pte: unmanaged page marked "
		    "PG_PVLIST, va = 0x%lx, pa = 0x%lx", va,
		    (u_long)(opte & PG_FRAME));
#endif

	pmap_sync_flags_pte(pg, opte);
	pve = pmap_remove_pv(pg, pmap, va);
	if (pve)
		pmap_free_pv(pmap, pve);
	return(TRUE);
}

d1977 1
a1977 2
	pt_entry_t *ptes, opte;
	boolean_t result;
a1990 86
	 * removing one page?  take shortcut function.
	 */

	if (sva + PAGE_SIZE == eva) {

		if (pmap_valid_entry(pmap->pm_pdir[pdei(sva)])) {

			/* PA of the PTP */
			ptppa = pmap->pm_pdir[pdei(sva)] & PG_FRAME;

			/* get PTP if non-kernel mapping */

			if (pmap == pmap_kernel()) {
				/* we never free kernel PTPs */
				ptp = NULL;
			} else {
				if (pmap->pm_ptphint &&
				    VM_PAGE_TO_PHYS(pmap->pm_ptphint) ==
				    ptppa) {
					ptp = pmap->pm_ptphint;
				} else {
					ptp = PHYS_TO_VM_PAGE(ptppa);
#ifdef DIAGNOSTIC
					if (ptp == NULL)
						panic("pmap_remove: unmanaged "
						      "PTP detected");
#endif
				}
			}

			/* do it! */
			result = pmap_remove_pte(pmap, ptp, &ptes[atop(sva)],
			    sva, flags);

			/*
			 * if mapping removed and the PTP is no longer
			 * being used, free it!
			 */

			if (result && ptp && ptp->wire_count <= 1) {
				opte = i386_atomic_testset_ul(
				    &pmap->pm_pdir[pdei(sva)], 0);
#ifdef MULTIPROCESSOR
				/*
				 * XXXthorpej Redundant shootdown can happen
				 * here if we're using APTE space.
				 */
#endif
				pmap_tlb_shootpage(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + ptp->offset);
#ifdef MULTIPROCESSOR
				/*
				 * Always shoot down the pmap's self-mapping
				 * of the PTP.
				 * XXXthorpej Redundant shootdown can happen
				 * here if pmap == curpcb->pcb_pmap (not APTE
				 * space).
				 */
				pmap_tlb_shootpage(pmap,
				    ((vaddr_t)PTE_BASE) + ptp->offset);
#endif
				pmap->pm_stats.resident_count--;
				if (pmap->pm_ptphint == ptp)
					pmap->pm_ptphint =
					    TAILQ_FIRST(&pmap->pm_obj.memq);
				ptp->wire_count = 0;
				/* Postpone free to after shootdown. */
				uvm_pagerealloc(ptp, NULL, 0);
				TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
			}
			/*
			 * Shoot the tlb after any updates to the PDE.
			 */
			pmap_tlb_shootpage(pmap, sva);
		}
		pmap_tlb_shootwait();
		pmap_unmap_ptes(pmap);		/* unlock pmap */
		PMAP_MAP_TO_HEAD_UNLOCK();
		while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
			TAILQ_REMOVE(&empty_ptps, ptp, listq);
			uvm_pagefree(ptp);
		}
		return;
	}

	/*
d2051 1
a2051 1
		/* if PTP is no longer being used, free it! */
d2053 1
a2053 27
			opte = i386_atomic_testset_ul(
			    &pmap->pm_pdir[pdei(va)], 0);
#if defined(MULTIPROCESSOR)
			/*
			 * XXXthorpej Redundant shootdown can happen here
			 * if we're using APTE space.
			 */
#endif
			pmap_tlb_shootpage(curpcb->pcb_pmap,
			    ((vaddr_t)ptes) + ptp->offset);
#if defined(MULTIPROCESSOR)
			/*
			 * Always shoot down the pmap's self-mapping
			 * of the PTP.
			 * XXXthorpej Redundant shootdown can happen here
			 * if pmap == curpcb->pcb_pmap (not APTE space).
			 */
			pmap_tlb_shootpage(pmap,
			    ((vaddr_t)PTE_BASE) + ptp->offset);
#endif
			pmap->pm_stats.resident_count--;
			if (pmap->pm_ptphint == ptp)	/* update hint? */
				pmap->pm_ptphint =
				    TAILQ_FIRST(&pmap->pm_obj.memq);
			ptp->wire_count = 0;
			/* Postpone free to after shootdown. */
			uvm_pagerealloc(ptp, NULL, 0);
d2056 3
d2060 2
a2061 3
	if (!shootall)
		pmap_tlb_shootrange(pmap, sva, eva);
	else
d2124 4
a2127 27
		if (pve->pv_ptp) {
			pve->pv_ptp->wire_count--;
			if (pve->pv_ptp->wire_count <= 1) {
				opte = i386_atomic_testset_ul(
				    &pve->pv_pmap->pm_pdir[pdei(pve->pv_va)],
				    0);
				pmap_tlb_shootpage(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + pve->pv_ptp->offset);
#if defined(MULTIPROCESSOR)
				/*
				 * Always shoot down the other pmap's
				 * self-mapping of the PTP.
				 */
				pmap_tlb_shootpage(pve->pv_pmap,
				    ((vaddr_t)PTE_BASE) + pve->pv_ptp->offset);
#endif
				pve->pv_pmap->pm_stats.resident_count--;
				/* update hint? */
				if (pve->pv_pmap->pm_ptphint == pve->pv_ptp)
					pve->pv_pmap->pm_ptphint =
					    TAILQ_FIRST(&pve->pv_pmap->pm_obj.memq);
				pve->pv_ptp->wire_count = 0;
				/* Postpone free to after shootdown. */
				uvm_pagerealloc(pve->pv_ptp, NULL, 0);
				TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp,
				    listq);
			}
@


1.129
log
@In pmap_clear_attrs() use i386_atomic_clearbits_l() to only clear the
bits necessary. Also move up the pmap_map_ptes() call to before the
diagnostic check. okay art@@, weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.128 2008/11/14 20:43:54 weingart Exp $	*/
d2431 1
a2431 1
	pt_entry_t *ptes, *spte, *epte, npte;
d2483 2
a2484 1
			npte = (*spte & ~PG_PROT) | md_prot;
d2486 1
a2486 1
			if (npte != *spte) {
d2488 3
a2490 1
				i386_atomic_testset_ul(spte, npte);
@


1.128
log
@Garbage collect the LDT segments, and simply use the GDT segments.
The beginning of i386 segment review/cleanup.

Tested by various people.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.127 2008/11/14 15:10:31 kurt Exp $	*/
d2366 1
a2366 1
	pt_entry_t *ptes, npte, opte;
d2379 1
d2386 2
a2387 3
		ptes = pmap_map_ptes(pve->pv_pmap);	/* locks pmap */
		npte = ptes[atop(pve->pv_va)];
		if (npte & clearbits) {
d2389 2
a2390 3
			npte &= ~clearbits;
			opte = i386_atomic_testset_ul(
			    &ptes[atop(pve->pv_va)], npte);
@


1.127
log
@Atomically update the PTE.

okay weingart@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.126 2008/11/06 19:14:26 deraadt Exp $	*/
d667 1
a667 1
	/* And update the GDT and LDT since we may be called by the
d670 1
a670 2
	curcpu()->ci_gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
	    pm->pm_codeseg;
d1680 1
a1680 1
		 * correct code segment X limit) in the GDT and the LDT.
d1682 1
a1682 2
		self->ci_gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
		    pmap->pm_codeseg;
@


1.126
log
@correct comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.125 2008/11/06 19:13:31 deraadt Exp $	*/
d1959 2
a1960 2
	opte = *pte;			/* save the old PTE */
	*pte = 0;			/* zap! */
@


1.125
log
@delete checks for impossible conditions
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.124 2008/10/24 06:34:55 deraadt Exp $	*/
d1658 1
a1658 1
 * => called from cpu_switch()
@


1.124
log
@clear pointer after freeing it
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.123 2008/06/14 18:09:47 hshoexer Exp $	*/
d581 1
a581 1
	if (curproc == NULL || curproc->p_vmspace == NULL ||
@


1.123
log
@Correctly calculate wired_count of pages on i386.  Now, pmap->pm_stats is only
updated when we actually can commit the pte.

With mickey.

ok art@@ weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.122 2008/01/13 20:47:00 kettenis Exp $	*/
d1550 1
@


1.122
log
@Don't reserve space for msgbuf and bootargs in pmap_bootstrap(), since we
do those reservations again (and now for real) in init386().

ok weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.121 2007/11/28 17:05:09 tedu Exp $	*/
d1285 1
a1285 1
 * ptp: PTP in pmap that maps this VA 
d1418 1
a1418 1
 
d2585 1
a2585 1
	struct pv_entry *pve = NULL;
d2588 1
a2588 1
	int error;
d2603 5
d2629 3
d2641 1
a2641 1
		 * first, update pm_stats.  resident count will not
d2647 1
a2647 1
			pmap->pm_stats.wired_count++;
d2649 1
a2649 1
			pmap->pm_stats.wired_count--;
d2695 1
a2695 1
		pmap->pm_stats.resident_count++;
d2697 1
a2697 1
			pmap->pm_stats.wired_count++;
d2699 1
a2699 1
			ptp->wire_count++;      /* count # of valid entries */
d2703 2
a2704 3
	 * at this point pm_stats has been updated.   pve is either NULL
	 * or points to a now-free pv_entry structure (the latter case is
	 * if we called pmap_remove_pv above).
d2714 1
a2714 1
			pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
a2716 3
					/*
					 * XXX - Back out stats changes!
					 */
d2722 1
d2724 1
a2724 1
		/* lock pvh when adding */
a2726 1

d2734 1
a2734 1
	 * at this point pvh is !NULL if we want the PG_PVLIST bit set
d2757 4
d2772 2
@


1.121
log
@quite a bit of simplification by removing cpu classes.
also assume that 386 cpus are really unknown, and promote them to 486
instead of panic.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.120 2007/11/16 16:16:06 deraadt Exp $	*/
a756 1
	extern paddr_t avail_end;
a772 10

	/*
	 * use the very last page of physical memory for the message buffer
	 */

	avail_end -= round_page(MSGBUFSIZE);
	/*
	 * The arguments passed in from /boot needs space too.
	 */
	avail_end -= round_page(bootargc);
@


1.120
log
@fix the bus_space #define nightmare, so that amd64 and i386 are much more
uniform. as a result shared code like acpi needs less #ifdef's
ok marco kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.119 2007/06/27 16:16:53 art Exp $	*/
a315 1
#if defined(I586_CPU)
a317 1
#endif
@


1.119
log
@According to Intel errata:
"AI91 - Update of attribute bits on page directories without immediate
 tlb shootdown may cause unexpected processor behavior.".

When we're allocating kernel page tables, we use the generic page table
allocation function that sets PG_u then we immediately remove the PG_u.
This might not be enough, because the PDE can get preloaded into the
translation cache before we clear the PG_u. So even without the errata,
this could cause us horrible trouble.

Instead of first entering the PDE with PG_u and then removing it for
kernel page tables, just enter it with the right bits every time.

tom@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.118 2007/06/19 09:41:39 art Exp $	*/
d296 1
a296 1
 * I386_MAXPROCS*NPTECL array of PTE's, to avoid cache line thrashing
d880 2
a881 2
	virtual_avail += PAGE_SIZE * I386_MAXPROCS * NPTECL;
	pte += I386_MAXPROCS * NPTECL;
@


1.118
log
@Mark PDE entries with PG_U and PG_M. We never need that information from the
mmu, it slightly speeds up tlb misses and according to an errata from AMD
it can actually work around a bug in the mmu.

toby@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.117 2007/06/07 15:31:09 art Exp $	*/
d327 1
a327 1
struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t);
d1354 2
a1355 1
pmap_alloc_ptp(struct pmap *pmap, int pde_index, boolean_t just_try)
d1367 2
a1368 2
	pmap->pm_pdir[pde_index] = (pd_entry_t)(VM_PAGE_TO_PHYS(ptp) | PG_u |
	    PG_RW | PG_V | PG_M | PG_U);
d1404 1
a1404 1
	return (pmap_alloc_ptp(pmap, pde_index, just_try));
d2836 1
a2836 1
		while (!pmap_alloc_ptp(kpm, PDSLOT_KERN + nkpde, FALSE))
a2837 3

		/* PG_u not for kernel */
		kpm->pm_pdir[PDSLOT_KERN + nkpde] &= ~PG_u;
@


1.117
log
@roller-coaster. The bug that corrupts the page tables is back.
So put back the flushg in pmap_release since at least it hides it
and I can now reproduce.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.115 2007/05/29 18:18:20 tom Exp $	*/
d546 2
a547 1
		*APDP_PDE = (pd_entry_t) (pmap->pm_pdirpa | PG_RW | PG_V);
d1461 2
a1462 1
	pmap->pm_pdir[PDSLOT_PTE] = pmap->pm_pdirpa | PG_V | PG_KW;
d2822 1
a2822 1
				ptaddr | PG_RW | PG_V;
@


1.116
log
@Well, whatever the bug was, it does not seem to appear anymore.
Remove the workaround.
(krw tried to reproduce many, many times)
@
text
@d1535 5
@


1.115
log
@Remove support for 80386 processors.  Apologies if you have one of
the rare 80386-bases system with enough memory, a 387 FPU, a useable
disk subsystem, and the patience to wait for it to unpack the
distribution .tgz files.

approval from art@@ and many others (esp. nick@@); ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.114 2007/05/28 18:31:11 krw Exp $	*/
a1532 5

	/*
	 * Before we free the pmap just make sure it's not cached anywhere.
	 */
	tlbflushg();
@


1.114
log
@Flush pmap from tlb before freeing it. Makes Core2Duo boxes more
stable, but is not a fully-understood or final fix.

From and ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.113 2007/05/27 21:33:25 tom Exp $	*/
d1815 1
a1815 2
	*zpte = (pa & PG_FRAME) | PG_V | PG_RW |	/* map in */
	    ((cpu_class != CPUCLASS_386) ? PG_N : 0);
a2944 5
	if (cpu_class == CPUCLASS_386) {
		tlbflush();
		return;
	}

a2982 5
	if (cpu_class == CPUCLASS_386) {
		tlbflush();
		return;
	}

a3021 5
	if (cpu_class == CPUCLASS_386) {
		tlbflush();
		return;
	}

a3060 5
	if (cpu_class == CPUCLASS_386) {
		tlbflush();
		return;
	}

a3070 5
	if (cpu_class == CPUCLASS_386) {
		tlbflush();
		return;
	}

a3072 1

@


1.113
log
@We don't need to special-case access to the GDT for UP or MP, since
ci_gdt is set correctly even when ! MULTIPROCESSOR.

ok art@@ toby@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.112 2007/05/25 15:55:26 art Exp $	*/
d1533 5
@


1.112
log
@Replace the overdesigned and overcomplicated tlb shootdown code with
very simple and dumb fast tlb IPI handlers that have in the order of
the same amount of instructions as the old code had function calls.

All TLB shootdowns are reorganized so that we always shoot the,
without looking at PG_U and when we're shooting a range (primarily in
pmap_remove), we shoot the range when there are 32 or less pages in
it, otherwise we just nuke the whole TLB (this might need tweaking if
someone is interested in micro-optimization). The IPIs are not handled
through the normal interrupt vectoring code, they are not blockable
and they only shoot one page or a range of pages or the whole tlb.

This gives a 15% reduction in system time on my dual-core laptop
during a kernel compile and an 18% reduction in real time on a quad
machine doing bulk ports build.

Tested by many, in snaps for a week, no slowdowns reported (although not
everyone is seeing such huge wins).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.111 2007/05/20 14:14:09 miod Exp $	*/
a670 1
#ifdef MULTIPROCESSOR
a672 3
#else
	gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd = pm->pm_codeseg;
#endif
a1670 1
#ifdef MULTIPROCESSOR
a1671 1
#endif
a1686 1
#ifdef MULTIPROCESSOR
a1688 4
#else
		gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
		    pmap->pm_codeseg;
#endif
@


1.111
log
@addess -> address
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.110 2007/05/17 15:17:02 art Exp $	*/
a215 43
 * TLB Shootdown:
 *
 * When a mapping is changed in a pmap, the TLB entry corresponding to
 * the virtual address must be invalidated on all processors.  In order
 * to accomplish this on systems with multiple processors, messages are
 * sent from the processor which performs the mapping change to all
 * processors on which the pmap is active.  For other processors, the
 * ASN generation numbers for that processor is invalidated, so that
 * the next time the pmap is activated on that processor, a new ASN
 * will be allocated (which implicitly invalidates all TLB entries).
 *
 * Shootdown job queue entries are allocated using a simple special-
 * purpose allocator for speed.
 */
struct pmap_tlb_shootdown_job {
	TAILQ_ENTRY(pmap_tlb_shootdown_job) pj_list;
	vaddr_t pj_va;			/* virtual address */
	pmap_t pj_pmap;			/* the pmap which maps the address */
	pt_entry_t pj_pte;		/* the PTE bits */
	struct pmap_tlb_shootdown_job *pj_nextfree;
};

struct pmap_tlb_shootdown_q {
	TAILQ_HEAD(, pmap_tlb_shootdown_job) pq_head;
	int pq_pte;			/* aggregate PTE bits */
	int pq_count;			/* number of pending requests */
	struct mutex pq_mutex;		/* mutex on queue */
	int pq_flushg;			/* pending flush global */
	int pq_flushu;			/* pending flush user */
} pmap_tlb_shootdown_q[I386_MAXPROCS];

#define	PMAP_TLB_MAXJOBS	16

void	pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *);
struct pmap_tlb_shootdown_job *pmap_tlb_shootdown_job_get(
    struct pmap_tlb_shootdown_q *);
void	pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *,
    struct pmap_tlb_shootdown_job *);

struct mutex pmap_tlb_shootdown_job_mutex;
struct pmap_tlb_shootdown_job *pj_page, *pj_free;

/*
d347 1
a347 1
    vaddr_t, int32_t *, int);
d349 1
a349 1
    vaddr_t, vaddr_t, int32_t *, int);
d507 2
a508 27
#if defined(MULTIPROCESSOR)
	struct pmap_tlb_shootdown_q *pq;
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
#endif

	tlbflush();		/* flush TLB on current processor */
#if defined(MULTIPROCESSOR)
	/*
	 * Flush the APTE mapping from all other CPUs that
	 * are using the pmap we are using (who's APTE space
	 * is the one we've just modified).
	 *
	 * XXXthorpej -- find a way to defer the IPI.
	 */
	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self)
			continue;
		if (pmap_is_active(pmap, ci->ci_cpuid)) {
			pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];
			mtx_enter(&pq->pq_mutex);
			pq->pq_flushu++;
			mtx_leave(&pq->pq_mutex);
			i386_send_ipi(ci, I386_IPI_TLB);
		}
	}
#endif
d586 2
a587 11
	if ((opte ^ npte) & PG_X) {
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pm, va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		pmap_update_pg(va);
#endif
	}
d702 2
a703 1
	npte = pa | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) | PG_V | pmap_pg_g;
d706 3
a708 9
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		pmap_update_pg(va);
#endif
d722 1
a722 1
pmap_kremove(vaddr_t va, vsize_t len)
d725 3
a727 3
#ifdef MULTIPROCESSOR
	int32_t cpumask = 0;
#endif
d729 3
a731 7
	len >>= PAGE_SHIFT;
	for ( /* null */ ; len ; len--, va += PAGE_SIZE) {
		if (va < VM_MIN_KERNEL_ADDRESS)
			pte = vtopte(va);
		else
			pte = kvtopte(va);
		opte = i386_atomic_testset_ul(pte, 0); /* zap! */
a735 6
		if ((opte & (PG_V | PG_U)) == (PG_V | PG_U))
#ifdef MULTIPROCESSOR
			pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
#else
			pmap_update_pg(va);
#endif
d737 2
a738 3
#ifdef MULTIPROCESSOR
	pmap_tlb_shootnow(cpumask);
#endif
a765 1
	int i;
a932 11
	 * Initialize the TLB shootdown queues.
	 */

	mtx_init(&pmap_tlb_shootdown_job_mutex, IPL_NONE);

	for (i = 0; i < I386_MAXPROCS; i++) {
		TAILQ_INIT(&pmap_tlb_shootdown_q[i].pq_head);
		mtx_init(&pmap_tlb_shootdown_q[i].pq_mutex, IPL_IPI);
	}

	/*
a947 2
	int i;

a962 9
	pj_page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
	if (pj_page == NULL)
		panic("pmap_init: pj_page");

	for (i = 0; i < PAGE_SIZE / sizeof *pj_page - 1; i++)
		pj_page[i].pj_nextfree = &pj_page[i + 1];
	pj_page[i].pj_nextfree = NULL;
	pj_free = &pj_page[0];

d1369 2
a1370 2
	pmap->pm_pdir[pde_index] =
		(pd_entry_t) (VM_PAGE_TO_PHYS(ptp) | PG_u | PG_RW | PG_V);
d1842 2
a1843 2
	pt_entry_t *spte = PTESLEW(csrc_pte,id);
	pt_entry_t *dpte = PTESLEW(cdst_pte,id);
a1857 3
#ifdef MULTIPROCESSOR
	/* Using per-cpu VA; no shootdown required here. */
#endif
d1877 1
a1877 1
    vaddr_t startva, vaddr_t endva, int32_t *cpumaskp, int flags)
d1909 1
a1909 4
		if (opte & PG_U)
			pmap_tlb_shootdown(pmap, startva, opte, cpumaskp);

		if (ptp) {
a1910 5
			/* Make sure that the PDE is flushed */
			if ((ptp->wire_count <= 1) && !(opte & PG_U))
				pmap_tlb_shootdown(pmap, startva, opte,
				    cpumaskp);
		}
d1963 1
a1963 1
    vaddr_t va, int32_t *cpumaskp, int flags)
d1984 1
a1984 4
	if (opte & PG_U)
		pmap_tlb_shootdown(pmap, va, opte, cpumaskp);

	if (ptp) {
a1985 5
		/* Make sure that the PDE is flushed */
		if ((ptp->wire_count <= 1) && !(opte & PG_U))
			pmap_tlb_shootdown(pmap, va, opte, cpumaskp);

	}
a2034 1
	int32_t cpumask = 0;
d2036 2
d2076 2
a2077 2
			result = pmap_remove_pte(pmap, ptp,
			    &ptes[atop(sva)], sva, &cpumask, flags);
a2084 1
				/* zap! */
d2093 2
a2094 3
				pmap_tlb_shootdown(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + ptp->offset, opte,
				    &cpumask);
d2103 2
a2104 3
				pmap_tlb_shootdown(pmap,
				    ((vaddr_t)PTE_BASE) + ptp->offset, opte,
				    &cpumask);
d2115 4
d2120 1
a2120 1
		pmap_tlb_shootnow(cpumask);
d2130 9
a2138 1
	for (/* null */ ; sva < eva ; sva = blkendva) {
d2140 1
d2142 1
a2142 1
		blkendva = i386_round_pdr(sva+1);
d2160 1
a2160 1
		if (pdei(sva) == PDSLOT_PTE)
d2164 1
a2164 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
d2169 1
a2169 1
		ptppa = (pmap->pm_pdir[pdei(sva)] & PG_FRAME);
d2188 2
a2189 2
		pmap_remove_ptes(pmap, ptp, (vaddr_t)&ptes[atop(sva)],
		    sva, blkendva, &cpumask, flags);
a2192 1
			/* zap! */
d2194 1
a2194 1
			    &pmap->pm_pdir[pdei(sva)], 0);
d2201 2
a2202 2
			pmap_tlb_shootdown(curpcb->pcb_pmap,
			    ((vaddr_t)ptes) + ptp->offset, opte, &cpumask);
d2210 2
a2211 2
			pmap_tlb_shootdown(pmap,
			    ((vaddr_t)PTE_BASE) + ptp->offset, opte, &cpumask);
d2223 4
d2228 1
a2228 1
	pmap_tlb_shootnow(cpumask);
a2247 1
	int32_t cpumask = 0;
d2278 1
a2278 2
		opte = ptes[atop(pve->pv_va)];
		ptes[atop(pve->pv_va)] = 0;			/* zap! */
a2283 5
		/* Shootdown only if referenced */
		if (opte & PG_U)
			pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va, opte,
			    &cpumask);

a2290 9
				/*
				 * Do we have to shootdown the page just to
				 * get the pte out of the TLB ?
				 */
				if(!(opte & PG_U))
					pmap_tlb_shootdown(pve->pv_pmap,
					    pve->pv_va, opte, &cpumask);

				/* zap! */
d2294 2
a2295 3
				pmap_tlb_shootdown(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + pve->pv_ptp->offset,
				    opte, &cpumask);
d2301 2
a2302 3
				pmap_tlb_shootdown(pve->pv_pmap,
				    ((vaddr_t)PTE_BASE) + pve->pv_ptp->offset,
				    opte, &cpumask);
d2316 3
d2324 2
a2325 1
	pmap_tlb_shootnow(cpumask);
a2384 1
	int32_t cpumask = 0;
d2410 1
a2410 2
			pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va,
			    opte, &cpumask);
d2416 1
a2416 1
	pmap_tlb_shootnow(cpumask);
d2453 2
a2454 1
	int32_t cpumask = 0;
d2462 2
a2463 1
	for (/* null */ ; sva < eva ; sva = blockend) {
d2465 2
a2466 1
		blockend = (sva & PD_MASK) + NBPD;
d2480 1
a2480 1
		if (pdei(sva) == PDSLOT_PTE)
d2484 1
a2484 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
d2488 1
a2488 1
		if (sva < VM_MAXUSER_ADDRESS)
d2490 1
a2490 1
		else if (sva < VM_MAX_ADDRESS)
d2494 1
a2494 1
		spte = &ptes[atop(sva)];
d2497 1
a2497 1
		for (/*null */; spte < epte ; spte++, sva += PAGE_SIZE) {
d2505 2
a2506 3
				pmap_exec_account(pmap, sva, *spte, npte);
				i386_atomic_testset_ul(spte, npte); /* zap! */
				pmap_tlb_shootdown(pmap, sva, *spte, &cpumask);
d2510 4
d2515 1
a2515 1
	pmap_tlb_shootnow(cpumask);
a2751 2
	if (pg != NULL)
		npte |= PG_PVLIST;
d2760 8
d2769 1
a2769 5
	ptes[atop(va)] = npte;			/* zap! */

	if ((opte & ~(PG_M|PG_U)) != npte) {
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;
d2771 3
a2773 7
		pmap_tlb_shootdown(pmap, va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		if (pmap_is_curpmap(pmap))
			pmap_update_pg(va);
#endif
d2916 24
d2941 2
a2942 1
/******************** TLB shootdown code ********************/
d2945 1
a2945 1
pmap_tlb_shootnow(int32_t cpumask)
d2947 1
a2947 2
#ifdef MULTIPROCESSOR
	struct cpu_info *ci, *self;
d2949 2
a2950 5
	int s;
#ifdef DIAGNOSTIC
	int count = 0;
#endif
#endif
d2952 2
a2953 1
	if (cpumask == 0)
d2955 1
a2956 17
#ifdef MULTIPROCESSOR
	self = curcpu();
	s = splipi();
	self->ci_tlb_ipi_mask = cpumask;
#endif

	pmap_do_tlb_shootdown(0);	/* do *our* work. */

#ifdef MULTIPROCESSOR
	splx(s);

	if (cold)
		return;

	/*
	 * Send the TLB IPI to other CPUs pending shootdowns.
	 */
d2958 2
a2959 1
		if (ci == self)
d2961 2
a2962 4
		if (cpumask & (1U << ci->ci_cpuid))
			if (i386_send_ipi(ci, I386_IPI_TLB) != 0)
				i386_atomic_clearbits_l(&self->ci_tlb_ipi_mask,
				    (1U << ci->ci_cpuid));
d2965 15
a2979 7
	while (self->ci_tlb_ipi_mask != 0) {
		SPINLOCK_SPIN_HOOK;
#ifdef DIAGNOSTIC
		if (count++ > 100000000)
			panic("%s: TLB IPI rendezvous failed (mask 0x%x)",
			    self->ci_dev.dv_xname, self->ci_tlb_ipi_mask);
#endif
d2981 3
a2983 1
#endif
a2985 5
/*
 * pmap_tlb_shootdown:
 *
 *	Cause the TLB entry for pmap/va to be shot down.
 */
d2987 1
a2987 1
pmap_tlb_shootdown(pmap_t pmap, vaddr_t va, pt_entry_t pte, int32_t *cpumaskp)
d2989 1
a2989 3
	struct cpu_info *ci, *self;
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
d2991 3
a2993 1
	int s;
d2995 2
a2996 2
	if (pmap_initialized == FALSE) {
		pmap_update_pg(va);
a2999 7
	self = curcpu();

	s = splipi();
#if 0
	printf("dshootdown %lx\n", va);
#endif

d3001 2
a3002 2
		/* Note: we queue shootdown events for ourselves here! */
		if (pmap_is_active(pmap, ci->ci_cpuid) == 0)
d3004 3
a3006 4
		if (ci != self && !(ci->ci_flags & CPUF_RUNNING))
			continue;
		pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];
		mtx_enter(&pq->pq_mutex);
d3008 2
a3009 10
		/*
		 * If there's a global flush already queued, or a
		 * non-global flush, and this pte doesn't have the G
		 * bit set, don't bother.
		 */
		if (pq->pq_flushg > 0 ||
		    (pq->pq_flushu > 0 && (pte & pmap_pg_g) == 0)) {
			mtx_leave(&pq->pq_mutex);
			continue;
		}
d3011 3
a3013 15
#ifdef I386_CPU
		/*
		 * i386 CPUs can't invalidate a single VA, only
		 * flush the entire TLB, so don't bother allocating
		 * jobs for them -- just queue a `flushu'.
		 *
		 * XXX note that this can be executed for non-i386
		 * when called early (before identifycpu() has set
		 * cpu_class)
		 */
		if (cpu_class == CPUCLASS_386) {
			pq->pq_flushu++;
			*cpumaskp |= 1U << ci->ci_cpuid;
			mtx_leave(&pq->pq_mutex);
			continue;
d3015 4
a3018 14
#endif

		pj = pmap_tlb_shootdown_job_get(pq);
		pq->pq_pte |= pte;
		if (pj == NULL) {
			/*
			 * Couldn't allocate a job entry.
			 * Kill it now for this cpu, unless the failure
			 * was due to too many pending flushes; otherwise,
			 * tell other cpus to kill everything..
			 */
			if (ci == self && pq->pq_count < PMAP_TLB_MAXJOBS) {
				pmap_update_pg(va);
				mtx_leave(&pq->pq_mutex);
d3020 2
a3021 19
			} else {
				if (pq->pq_pte & pmap_pg_g)
					pq->pq_flushg++;
				else
					pq->pq_flushu++;
				/*
				 * Since we've nailed the whole thing,
				 * drain the job entries pending for that
				 * processor.
				 */
				pmap_tlb_shootdown_q_drain(pq);
				*cpumaskp |= 1U << ci->ci_cpuid;
			}
		} else {
			pj->pj_pmap = pmap;
			pj->pj_va = va;
			pj->pj_pte = pte;
			TAILQ_INSERT_TAIL(&pq->pq_head, pj, pj_list);
			*cpumaskp |= 1U << ci->ci_cpuid;
d3023 1
a3023 1
		mtx_leave(&pq->pq_mutex);
d3025 4
a3028 1
	splx(s);
a3030 5
/*
 * pmap_do_tlb_shootdown:
 *
 *	Process pending TLB shootdown operations for this processor.
 */
d3032 1
a3032 1
pmap_do_tlb_shootdown(struct cpu_info *self)
d3034 1
a3034 5
	u_long cpu_id = cpu_number();
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj;
#ifdef MULTIPROCESSOR
	struct cpu_info *ci;
d3036 2
a3037 1
#endif
d3039 4
a3042 1
	mtx_enter(&pq->pq_mutex);
d3044 6
a3049 15
	if (pq->pq_flushg) {
		tlbflushg();
		pq->pq_flushg = 0;
		pq->pq_flushu = 0;
		pmap_tlb_shootdown_q_drain(pq);
	} else {
		/*
		 * TLB flushes for PTEs with PG_G set may be in the queue
		 * after a flushu, they need to be dealt with.
		 */
		if (pq->pq_flushu) {
			tlbflush();
		}
		while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
			TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
d3051 2
a3052 3
			if ((!pq->pq_flushu && pmap_is_curpmap(pj->pj_pmap)) ||
			    (pj->pj_pte & pmap_pg_g))
				pmap_update_pg(pj->pj_va);
d3054 3
a3056 1
			pmap_tlb_shootdown_job_put(pq, pj);
d3059 7
a3065 1
		pq->pq_flushu = pq->pq_pte = 0;
d3068 1
a3068 6
#ifdef MULTIPROCESSOR
	CPU_INFO_FOREACH(cii, ci)
		i386_atomic_clearbits_l(&ci->ci_tlb_ipi_mask,
		    (1U << cpu_id));
#endif
	mtx_leave(&pq->pq_mutex);
a3070 9
/*
 * pmap_tlb_shootdown_q_drain:
 *
 *	Drain a processor's TLB shootdown queue.  We do not perform
 *	the shootdown operations.  This is merely a convenience
 *	function.
 *
 *	Note: We expect the queue to be locked.
 */
d3072 1
a3072 1
pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *pq)
d3074 3
a3076 1
	struct pmap_tlb_shootdown_job *pj;
d3078 8
a3085 3
	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		pmap_tlb_shootdown_job_put(pq, pj);
d3087 4
a3090 1
	pq->pq_pte = 0;
d3093 2
a3094 10
/*
 * pmap_tlb_shootdown_job_get:
 *
 *	Get a TLB shootdown job queue entry.  This places a limit on
 *	the number of outstanding jobs a processor may have.
 *
 *	Note: We expect the queue to be locked.
 */
struct pmap_tlb_shootdown_job *
pmap_tlb_shootdown_job_get(struct pmap_tlb_shootdown_q *pq)
d3096 1
a3096 1
	struct pmap_tlb_shootdown_job *pj;
d3098 4
a3101 2
	if (pq->pq_count >= PMAP_TLB_MAXJOBS)
		return (NULL);
d3103 2
a3104 8
	mtx_enter(&pmap_tlb_shootdown_job_mutex);
	if (pj_free == NULL) {
		mtx_leave(&pmap_tlb_shootdown_job_mutex);
		return NULL;
	}
	pj = pj_free;
	pj_free = pj_free->pj_nextfree;
	mtx_leave(&pmap_tlb_shootdown_job_mutex);
a3105 2
	pq->pq_count++;
	return (pj);
a3107 7
/*
 * pmap_tlb_shootdown_job_put:
 *
 *	Put a TLB shootdown job queue entry onto the free list.
 *
 *	Note: We expect the queue to be locked.
 */
d3109 1
a3109 2
pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *pq,
    struct pmap_tlb_shootdown_job *pj)
d3111 1
a3111 10
#ifdef DIAGNOSTIC
	if (pq->pq_count == 0)
		panic("pmap_tlb_shootdown_job_put: queue length inconsistency");
#endif
	mtx_enter(&pmap_tlb_shootdown_job_mutex);
	pj->pj_nextfree = pj_free;
	pj_free = pj;
	mtx_leave(&pmap_tlb_shootdown_job_mutex);

	pq->pq_count--;
d3113 1
@


1.110
log
@Since we're initializing everything in pmap_pinit, it's probably a good
idea to initialize pm_cpus as well. Otherwise we'll get stray tlb
shootdowns and pm_cpus is not clear on exit sometimes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.109 2007/05/04 05:52:28 art Exp $	*/
d1874 1
a1874 1
 *	determine the bounds of the kernel virtual addess space.
@


1.109
log
@Skip the kernel pmap when accounting for executability range changes.
This could lead to some heavy problems if called from kernel threads
(which is the only way to get past the next test with the kernel
pmap).

From mickey. art@@ toby@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.108 2007/05/03 05:32:05 art Exp $	*/
d1564 1
@


1.108
log
@Remove the pv stealing code. Other than being dangerous, it's also
slightly incorrect and didn't actually do much until a week ago
or so when I fixed it when doing VM_PAGE_MD.
deraadt@@, miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.107 2007/04/28 17:34:33 art Exp $	*/
d647 3
@


1.107
log
@Fix pmap_extract to not return TRUE just because we have a PTP. Make sure
that the PTP and the PTE are valid before returning success.

deraadt@@ ok, tested by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.106 2007/04/26 11:31:51 art Exp $	*/
d373 1
a373 1
#define ALLOCPV_TRY	1	/* just try to allocate, don't steal */
a401 1
boolean_t	 pmap_try_steal_pv(struct pv_entry *, struct pv_entry **);
d1102 2
a1103 2
 *    ALLOCPV_NEED   = we really need a pv_entry, even if we have to steal it
 *    ALLOCPV_TRY    = we want a pv_entry, but not enough to steal
d1158 1
a1158 3
 *	new pv_entry from it.   if we are unable to allocate a pv_page
 *	we make a last ditch effort to steal a pv_page from some other
 *	mapping.    if that fails, we panic...
a1167 1
	int lcv, idx, npg, s;
d1169 1
d1207 1
a1207 1
		goto steal_one;
d1211 1
a1211 1
		goto steal_one;
d1226 1
a1226 107
	return(pmap_add_pvpage(pvpage, mode != ALLOCPV_NONEED));

steal_one:
	/*
	 * if we don't really need a pv_entry right now, we can just return.
	 */

	if (mode != ALLOCPV_NEED)
		return(NULL);

	/*
	 * last ditch effort!   we couldn't allocate a free page to make
	 * more pv_entrys so we try and steal one from someone else.
	 */

	pv = NULL;
	for (lcv = 0 ; pv == NULL && lcv < vm_nphysseg ; lcv++) {
		npg = vm_physmem[lcv].end - vm_physmem[lcv].start;
		for (idx = 0 ; idx < npg ; idx++) {
			struct vm_page *pg = &vm_physmem[lcv].pgs[idx];
			struct pv_entry *cpv, **ppv;

			if (pg->mdpage.pv_list == NULL)
				continue;

			ppv = &pg->mdpage.pv_list;
			while ((cpv = *ppv) != NULL) {
				if (pmap_try_steal_pv(cpv, ppv))
					break;
				ppv = &cpv->pv_next;
			}
			/* got one?  break out of the loop! */
			if (cpv) {
				pv = cpv;
				break;
			}
		}
	}

	return(pv);
}

/*
 * pmap_try_steal_pv: try and steal a pv_entry from a pmap
 *
 * => return true if we did it!
 */

boolean_t
pmap_try_steal_pv(struct pv_entry *cpv, struct pv_entry **ppv)
{
	pt_entry_t *ptep, opte;
#ifdef MULTIPROCESSOR
	int32_t cpumask = 0;
#endif

	/*
	 * we never steal kernel mappings or mappings from pmaps we can't lock
	 */

	if (cpv->pv_pmap == pmap_kernel() ||
	    !simple_lock_try(&cpv->pv_pmap->pm_obj.vmobjlock))
		return(FALSE);

	/*
	 * yes, we can try and steal it.   first we need to remove the
	 * mapping from the pmap.
	 */

	ptep = pmap_tmpmap_pvepte(cpv);
	if (*ptep & PG_W) {
		ptep = NULL;	/* wired page, avoid stealing this one */
	} else {
		opte = i386_atomic_testset_ul(ptep, 0); /* zap! */
#ifdef MULTIPROCESSOR
		pmap_tlb_shootdown(cpv->pv_pmap, cpv->pv_va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		if (pmap_is_curpmap(cpv->pv_pmap))
			pmap_update_pg(cpv->pv_va);
#endif
		pmap_tmpunmap_pvepte(cpv);
	}
	if (ptep == NULL) {
		simple_unlock(&cpv->pv_pmap->pm_obj.vmobjlock);
		return(FALSE);	/* wired page, abort! */
	}
	cpv->pv_pmap->pm_stats.resident_count--;
	if (cpv->pv_ptp && cpv->pv_ptp->wire_count)
		/* drop PTP's wired count */
		cpv->pv_ptp->wire_count--;

	/*
	 * XXX: if wire_count goes to one the PTP could be freed, however,
	 * we'd have to lock the page queues (etc.) to do that and it could
	 * cause deadlock headaches.   besides, the pmap we just stole from
	 * may want the mapping back anyway, so leave the PTP around.
	 */

	/*
	 * now we need to remove the entry from the pvlist
	 */

	*ppv = cpv->pv_next;

	return(TRUE);
@


1.106
log
@Switch i386 pmap to VM_PAGE_MD. We store the MOD/REF flags in
pg_flags, so we actually shave quite a few bytes from the memory
we eat at boot. (a machine with 1GB memory saves 256k).

deraadt@@, pedro@@, krw@@ ok. Lots of testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.105 2007/04/19 18:34:23 art Exp $	*/
d1962 1
a1962 2
	paddr_t retval;
	pt_entry_t *ptes;
d1964 1
a1964 1
	if (pmap->pm_pdir[pdei(va)]) {
d1966 1
a1966 1
		retval = (paddr_t)(ptes[atop(va)] & PG_FRAME);
d1968 2
d1971 1
a1971 1
			*pap = retval | (va & ~PG_FRAME);
@


1.105
log
@When doing pmap_collect, skip wired mappings when removing.

From NetBSD, miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.104 2007/04/13 18:57:49 art Exp $	*/
a191 6
 * - pvh_lock (per pv_head)
 *   this lock protects the pv_entry list which is chained off the
 *   pv_head structure for a specific managed PA.   it is locked
 *   when traversing the list (e.g. adding/removing mappings,
 *   syncing R/M bits, etc.)
 *
d376 2
a377 3
void		 pmap_enter_pv(struct pv_head *,
					    struct pv_entry *, struct pmap *,
					    vaddr_t, struct vm_page *);
d385 1
d387 1
a387 2
struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
					     vaddr_t);
d402 1
a402 2
boolean_t	 pmap_try_steal_pv(struct pv_head *, struct pv_entry *,
    struct pv_entry *);
d445 22
d1049 1
a1049 1
pmap_init()
d1051 1
a1051 41
	int npages, lcv, i;
	vaddr_t addr;
	vsize_t s;

	/*
	 * compute the number of pages we have and then allocate RAM
	 * for each pages' pv_head and saved attributes.
	 */

	npages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		npages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
	s = (vsize_t) (sizeof(struct pv_head) * npages +
		       sizeof(char) * npages);
	s = round_page(s); /* round up */
	addr = (vaddr_t) uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: unable to allocate pv_heads");

	/*
	 * init all pv_head's and attrs in one bzero
	 */

	/* allocate pv_head stuff first */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.pvhead = (struct pv_head *) addr;
		addr = (vaddr_t)(vm_physmem[lcv].pmseg.pvhead +
				 (vm_physmem[lcv].end - vm_physmem[lcv].start));
		for (i = 0;
		     i < (vm_physmem[lcv].end - vm_physmem[lcv].start); i++) {
			simple_lock_init(
			    &vm_physmem[lcv].pmseg.pvhead[i].pvh_lock);
		}
	}

	/* now allocate attrs */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.attrs = (char *) addr;
		addr = (vaddr_t)(vm_physmem[lcv].pmseg.attrs +
				 (vm_physmem[lcv].end - vm_physmem[lcv].start));
	}
d1172 1
a1172 1
	struct pv_entry *pv, *cpv, *prevpv;
d1248 2
a1249 1
			struct pv_head *pvhead = vm_physmem[lcv].pmseg.pvhead;
d1251 1
a1251 3
			if (pvhead->pvh_list == NULL)
				continue;	/* spot check */
			if (!simple_lock_try(&pvhead->pvh_lock))
d1253 4
a1256 3
			cpv = prevpv = pvhead->pvh_list;
			while (cpv) {
				if (pmap_try_steal_pv(pvhead, cpv, prevpv))
d1258 1
a1258 2
				prevpv = cpv;
				cpv = cpv->pv_next;
a1259 1
			simple_unlock(&pvhead->pvh_lock);
d1278 1
a1278 2
pmap_try_steal_pv(struct pv_head *pvh, struct pv_entry *cpv,
    struct pv_entry *prevpv)
d1333 2
a1334 4
	if (cpv == pvh->pvh_list)
		pvh->pvh_list = cpv->pv_next;
	else
		prevpv->pv_next = cpv->pv_next;
d1511 2
a1512 5
 *   pmap_enter_pv: enter a mapping onto a pv_head list
 *   pmap_remove_pv: remove a mapping from a pv_head list
 *
 * NOTE: pmap_enter_pv expects to lock the pvh itself
 *       pmap_remove_pv expects te caller to lock the pvh before calling
d1516 1
a1516 1
 * pmap_enter_pv: enter a mapping onto a pv_head lst
d1519 1
a1519 1
 * => we will gain the lock on the pv_head and allocate the new pv_entry
d1527 1
a1527 1
pmap_enter_pv(struct pv_head *pvh, struct pv_entry *pve, struct pmap *pmap,
d1533 2
a1534 4
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	pve->pv_next = pvh->pvh_list;		/* add to ... */
	pvh->pvh_list = pve;			/* ... locked list */
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
d1541 1
a1541 1
 * => caller should hold lock on pv_head [so that attrs can be adjusted]
d1547 1
a1547 1
pmap_remove_pv(struct pv_head *pvh, struct pmap *pmap, vaddr_t va)
d1551 2
a1552 3
	prevptr = &pvh->pvh_list;		/* previous pv_entry pointer */
	pve = *prevptr;
	while (pve) {
a1557 1
		pve = pve->pv_next;			/* advance */
d1573 1
a1573 1
 * => we may need to lock pv_head's if we have to steal a PTP
d2105 1
a2106 1
	int bank, off;
d2144 1
a2144 1
		 * if we are not on a pv_head list we are done.
d2146 1
d2148 3
d2153 1
a2153 2
			if (vm_physseg_find(atop(opte & PG_FRAME), &off)
			    != -1)
a2159 1
		bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
d2161 1
a2161 1
		if (bank == -1)
d2168 2
a2169 6
		simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
		pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap,
				     startva);
		simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);

d2196 2
a2198 2
	int bank, off;
	struct pv_entry *pve;
d2226 2
d2229 1
a2229 1
	 * if we are not on a pv_head list we are done.
a2230 1

d2233 1
a2233 1
		if (vm_physseg_find(atop(opte & PG_FRAME), &off) != -1)
a2239 1
	bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
d2241 1
a2241 1
	if (bank == -1)
d2247 2
a2248 6
	/* sync R/M bits */
	simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
	vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
	pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap, va);
	simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);

a2276 4
	/*
	 * we lock in the pmap => pv_head direction
	 */

a2464 1
 * => we set pv_head => pmap locking
a2470 2
	int bank, off;
	struct pv_head *pvh;
d2477 1
a2477 4
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
a2478 6
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return;
	}
a2481 1
	/* set pv_head => pmap locking */
d2484 1
a2484 4
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	for (pve = pvh->pvh_list ; pve != NULL ; pve = pve->pv_next) {
d2517 1
a2517 1
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
d2561 2
a2562 3
	pmap_free_pvs(NULL, pvh->pvh_list);
	pvh->pvh_list = NULL;
	simple_unlock(&pvh->pvh_lock);
a2579 2
 *
 * => we set pv_head => pmap locking
a2584 3
	int bank, off;
	char *myattrs;
	struct pv_head *pvh;
d2587 1
d2589 1
a2589 6
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_test_attrs: unmanaged page?\n");
		return(FALSE);
	}
d2591 2
a2592 8
	/*
	 * before locking: see if attributes are already set and if so,
	 * return!
	 */

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	if (*myattrs & testbits)
		return(TRUE);
a2593 7
	/* test to see if there is a list before bothering to lock */
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return(FALSE);
	}

	/* nope, gonna have to do it the hard way */
d2595 3
a2597 5
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	for (pve = pvh->pvh_list; pve != NULL && (*myattrs & testbits) == 0;
	     pve = pve->pv_next) {
d2601 1
a2601 1
		*myattrs |= pte;
d2603 4
d2608 1
a2608 4
	/*
	 * note that we will exit the for loop with a non-null pve if
	 * we have found the bits we are testing for.
	 */
d2610 1
a2610 3
	simple_unlock(&pvh->pvh_lock);
	PMAP_HEAD_TO_MAP_UNLOCK();
	return((*myattrs & testbits) != 0);
d2614 1
a2614 1
 * pmap_change_attrs: change a page's attributes
a2615 1
 * => we set pv_head => pmap locking
d2620 1
a2620 1
pmap_change_attrs(struct vm_page *pg, int setbits, int clearbits)
a2621 3
	u_int32_t result;
	int bank, off;
	struct pv_head *pvh;
a2623 1
	char *myattrs;
d2625 2
d2628 1
a2628 6
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_change_attrs: unmanaged page?\n");
		return(FALSE);
	}
a2630 7
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	result = *myattrs & clearbits;
	*myattrs = (*myattrs | setbits) & ~clearbits;
d2632 5
a2636 1
	for (pve = pvh->pvh_list; pve != NULL; pve = pve->pv_next) {
d2645 3
a2647 3
		result |= (npte & clearbits);
		npte = (npte | setbits) & ~clearbits;
		if (ptes[atop(pve->pv_va)] != npte) {
a2655 1
	simple_unlock(&pvh->pvh_lock);
d2659 1
a2659 1
	return(result != 0);
a2828 1
 * => we set pmap => pv_head locking
d2837 1
a2837 3
	struct pv_head *pvh;
	struct pv_entry *pve;
	int bank, off, error;
d2839 2
d2904 1
a2904 1
				bank = vm_physseg_find(atop(pa), &off);
d2906 1
a2906 1
				if (bank == -1)
d2912 1
a2912 6
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				simple_lock(&pvh->pvh_lock);
				vm_physmem[bank].pmseg.attrs[off] |= opte;
				simple_unlock(&pvh->pvh_lock);
			} else {
				pvh = NULL;	/* ensure !PG_PVLIST */
d2927 1
a2927 1
			bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
d2929 1
a2929 1
			if (bank == -1)
d2934 3
a2936 7
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pve = pmap_remove_pv(pvh, pmap, va);
			vm_physmem[bank].pmseg.attrs[off] |= opte;
			simple_unlock(&pvh->pvh_lock);
		} else {
			pve = NULL;
a2938 1
		pve = NULL;
d2954 4
a2957 3
	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
d2962 3
d2972 1
a2972 1
		pmap_enter_pv(pvh, pve, pmap, va, ptp);
a2975 1
		pvh = NULL;		/* ensure !PG_PVLIST */
d2987 1
a2987 1
	if (pvh)
a3120 4

	/*
	 * we lock in the pmap => pv_head direction
	 */
@


1.104
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.103 2007/04/13 13:27:46 art Exp $	*/
d395 1
d397 1
a397 1
    vaddr_t, int32_t *);
d399 5
a403 1
    vaddr_t, vaddr_t, int32_t *);
d409 2
a410 3
boolean_t	 pmap_try_steal_pv(struct pv_head *,
						struct pv_entry *,
						struct pv_entry *);
d2138 1
a2138 1
    vaddr_t startva, vaddr_t endva, int32_t *cpumaskp)
d2160 3
d2234 1
a2234 1
    vaddr_t va, int32_t *cpumaskp)
d2241 4
a2244 1
		return(FALSE);		/* VA not mapped */
d2307 6
d2363 1
a2363 1
			    &ptes[atop(sva)], sva, &cpumask);
d2465 1
a2465 1
		    sva, blkendva, &cpumask);
d2912 2
a2913 1
	pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
@


1.103
log
@When freeing PTP pages, we need to wait until TLB shootdown has been
done until we put them on the freelist and risk them being reused.

If the page is reused before being shot, there's a risk that it's still
in the PDE TLB and speculative execution might use data from it to
load TLBs. If the page isn't zeroed anymore we risk loading bogus
tlbs from it.

Inspired by a similar change in NetBSD.

toby@@ ok, tested by many at the hackathon.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.102 2007/04/13 10:36:03 miod Exp $	*/
d1235 1
a1235 3
	if (pg)
		pg->pg_flags &= ~PG_BUSY;	/* never busy */
	else
d1238 2
d1623 1
a1623 1
	ptp->pg_flags &= ~PG_BUSY;	/* never busy */
@


1.102
log
@map peeing -> mapping
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.101 2007/04/11 12:10:42 art Exp $	*/
d2303 1
d2309 2
d2384 3
a2386 1
				uvm_pagefree(ptp);
d2392 4
d2479 3
a2481 1
			uvm_pagefree(ptp);
d2488 4
d2509 2
d2524 2
d2601 4
a2604 1
				uvm_pagefree(pve->pv_ptp);
d2614 4
@


1.101
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2007/04/04 17:44:45 art Exp $	*/
d1539 1
a1539 1
 *   pmap_remove_pv: remove a mappiing from a pv_head list
@


1.100
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2007/03/18 14:23:57 mickey Exp $	*/
d173 1
a173 1
 *		[b] allocate a page in kmem_object for the VA
d175 1
a175 1
 *		=> failure: kmem_object locked, no free vm_pages, etc.
d1077 3
a1079 3
	 * the kmem_map/kmem_object allocated and inited (done after this
	 * function is finished).  to do this we allocate one bootstrap page out
	 * of kernel_map and use it to provide an initial pool of pv_entry
d1225 1
a1225 1
	s = splvm();   /* must protect kmem_map/kmem_object with splvm! */
d1227 1
a1227 1
		pv_cachedva = uvm_km_kmemalloc(kmem_map, uvmexp.kmem_object,
a1228 4
		if (pv_cachedva == 0) {
			splx(s);
			goto steal_one;
		}
d1230 2
a1231 8

	/*
	 * we have a VA, now let's try and allocate a page in the object
	 * note: we are still holding splvm to protect kmem_object
	 */

	if (!simple_lock_try(&uvmexp.kmem_object->vmobjlock)) {
		splx(s);
a1232 1
	}
d1234 1
a1234 3
	pg = uvm_pagealloc(uvmexp.kmem_object, pv_cachedva -
			   vm_map_min(kernel_map),
			   NULL, UVM_PGA_USERESERVE);
d1237 1
a1237 6

	simple_unlock(&uvmexp.kmem_object->vmobjlock);
	splx(s);
	/* splvm now dropped */

	if (pg == NULL)
a1489 3
 * => note: analysis of MI kmem_map usage [i.e. malloc/free] shows
 *	that if we can lock the kmem_map then we are not already
 *	holding kmem_object's lock.
@


1.99
log
@do not steal page table pages on page allocation failure -- it can be dealt w/ in a more simple way as other archs do; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2007/03/13 15:11:41 art Exp $	*/
d1249 1
a1249 1
		pg->flags &= ~PG_BUSY;	/* never busy */
d1644 1
a1644 1
	ptp->flags &= ~PG_BUSY;	/* never busy */
d1821 1
a1821 1
		if (pg->flags & PG_BUSY)
@


1.98
log
@tlb shootdown in pmap_change_attrs was shooting the wrong va.
"yes man" mickey@@ (I think that meant 'ok').
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2007/02/20 21:15:01 tom Exp $	*/
d335 1
a335 2
static struct pmap_head pmaps;
static struct pmap *pmaps_hand = NULL;	/* used by pmap_steal_ptp */
a398 1
struct vm_page	*pmap_steal_ptp(struct uvm_object *, vaddr_t);
a1627 2
 * => we should not be holding any pv_head locks (in case we are forced
 *	to call pmap_steal_ptp())
d1640 2
a1641 10
	if (ptp == NULL) {
		if (just_try)
			return(NULL);
		ptp = pmap_steal_ptp(&pmap->pm_obj, ptp_i2o(pde_index));
		if (ptp == NULL) {
			return (NULL);
		}
		/* stole one; zero it. */
		pmap_zero_page(ptp);
	}
a1653 106
 * pmap_steal_ptp: steal a PTP from any pmap that we can access
 *
 * => obj is locked by caller.
 * => we can throw away mappings at this level (except in the kernel's pmap)
 * => stolen PTP is placed in <obj,offset> pmap
 * => we lock pv_head's
 * => hopefully, this function will be seldom used [much better to have
 *	enough free pages around for us to allocate off the free page list]
 */

struct vm_page *
pmap_steal_ptp(struct uvm_object *obj, vaddr_t offset)
{
	struct vm_page *ptp = NULL;
	struct pmap *firstpmap;
	struct uvm_object *curobj;
	pt_entry_t *ptes;
	int idx, lcv;
	boolean_t caller_locked, we_locked;
	int32_t cpumask = 0;

	simple_lock(&pmaps_lock);
	if (pmaps_hand == NULL)
		pmaps_hand = LIST_FIRST(&pmaps);
	firstpmap = pmaps_hand;

	do { /* while we haven't looped back around to firstpmap */

		curobj = &pmaps_hand->pm_obj;
		we_locked = FALSE;
		caller_locked = (curobj == obj);
		if (!caller_locked) {
			we_locked = simple_lock_try(&curobj->vmobjlock);
		}
		if (caller_locked || we_locked) {
			TAILQ_FOREACH(ptp, &curobj->memq, listq) {

				/*
				 * might have found a PTP we can steal
				 * (unless it has wired pages).
				 */

				idx = ptp_o2i(ptp->offset);
#ifdef DIAGNOSTIC
				if (VM_PAGE_TO_PHYS(ptp) !=
				    (pmaps_hand->pm_pdir[idx] & PG_FRAME))
					panic("pmap_steal_ptp: PTP mismatch!");
#endif

				ptes = (pt_entry_t *)
				    pmap_tmpmap_pa(VM_PAGE_TO_PHYS(ptp));
				for (lcv = 0 ; lcv < PTES_PER_PTP ; lcv++)
					if ((ptes[lcv] & (PG_V|PG_W)) ==
					    (PG_V|PG_W))
						break;
				if (lcv == PTES_PER_PTP)
					pmap_remove_ptes(pmaps_hand, ptp,
					    (vaddr_t)ptes, ptp_i2v(idx),
					    ptp_i2v(idx+1), &cpumask);
				pmap_tmpunmap_pa();

				if (lcv != PTES_PER_PTP)
					/* wired, try next PTP */
					continue;

				/*
				 * got it!!!
				 */

				pmaps_hand->pm_pdir[idx] = 0;	/* zap! */
				pmaps_hand->pm_stats.resident_count--;
#ifdef MULTIPROCESSOR
				pmap_apte_flush(pmaps_hand);
#else
				if (pmap_is_curpmap(pmaps_hand))
					pmap_apte_flush(pmaps_hand);
				else if (pmap_valid_entry(*APDP_PDE) &&
				    (*APDP_PDE & PG_FRAME) ==
				    pmaps_hand->pm_pdirpa)
					pmap_update_pg(((vaddr_t)APTE_BASE) +
						       ptp->offset);
#endif

				/* put it in our pmap! */
				uvm_pagerealloc(ptp, obj, offset);
				break;	/* break out of "for" loop */
			}
			if (we_locked) {
				simple_unlock(&curobj->vmobjlock);
			}
		}

		/* advance the pmaps_hand */
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
		if (pmaps_hand == NULL) {
			pmaps_hand = LIST_FIRST(&pmaps);
		}

	} while (ptp == NULL && pmaps_hand != firstpmap);

	simple_unlock(&pmaps_lock);
	pmap_tlb_shootnow(cpumask);
	return(ptp);
}

/*
d1683 1
a1683 1
	return(pmap_alloc_ptp(pmap, pde_index, just_try));
a1810 2
	if (pmap == pmaps_hand)
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
d3165 2
a3166 3
		if (pmap_alloc_ptp(kpm, PDSLOT_KERN+nkpde, FALSE) == NULL) {
			panic("pmap_growkernel: alloc ptp failed");
		}
@


1.97
log
@Revert PAE pmap for now, until the strange bug is found.  This stops
the freezes many of us are seeing (especially on amd64 machines running
OpenBSD/i386).

Much testing by nick@@ (as always - thanks!), hugh@@, ian@@, kettenis@@
and Sam Smith (s (at) msmith (dot) net).

Requested by, input from, and ok deraadt@@  ok art@@, kettenis@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2007/02/03 16:48:23 miod Exp $	*/
d2850 2
a2851 2
			pmap_tlb_shootdown(pve->pv_pmap,
			    atop(pve->pv_va), opte, &cpumask);
@


1.97.2.1
log
@Before we free the pmap just make sure it's not cached anywhere.  This
fixes possible panics on amd64 machines running i386 (like intel
Core2Duos, AMD Opterons).

from thib@@
ok deraadt@@ art@@

backport from 1.114 in -current:
---snip---
Changes by: krw@@cvs.openbsd.org 2007/05/28 12:31:11

Log message:
Flush pmap from tlb before freeing it. Makes Core2Duo boxes more
stable, but is not a fully-understood or final fix.

From and ok art
---snap---
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2007/02/20 21:15:01 tom Exp $	*/
a1932 5

	/*
	 * Before we free the pmap just make sure it's not cached anywhere.
	 */
	tlbflushg();
@


1.96
log
@Remove unused functionality from lockmgr():
- LK_EXCLUPGRADE is never used.
- LK_REENABLE is never used.
- LK_SETRECURSE is never used. Because of this, the lk_recurselevel
  field is always zero, so it can be removed to.
- the spinlock version (and LK_SPIN) is never used, since it was decided
  to use different locking structure for MP-safe protection.

Tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2006/09/19 11:06:33 jsg Exp $	*/
d84 9
a129 175
 * i386 MMU hardware structure:
 *
 * the i386 MMU is a two-level MMU which maps 4GB of virtual memory.
 * the pagesize is 4K (4096 [0x1000] bytes), although newer pentium
 * processors can support a 4MB pagesize as well.
 *
 * the first level table (segment table?) is called a "page directory"
 * and it contains 1024 page directory entries (PDEs).   each PDE is
 * 4 bytes (an int), so a PD fits in a single 4K page.   this page is
 * the page directory page (PDP).  each PDE in a PDP maps 4MB of space
 * (1024 * 4MB = 4GB).   a PDE contains the physical address of the
 * second level table: the page table.   or, if 4MB pages are being used,
 * then the PDE contains the PA of the 4MB page being mapped.
 *
 * a page table consists of 1024 page table entries (PTEs).  each PTE is
 * 4 bytes (an int), so a page table also fits in a single 4K page.  a
 * 4K page being used as a page table is called a page table page (PTP).
 * each PTE in a PTP maps one 4K page (1024 * 4K = 4MB).   a PTE contains
 * the physical address of the page it maps and some flag bits (described
 * below).
 *
 * the processor has a special register, "cr3", which points to the
 * the PDP which is currently controlling the mappings of the virtual
 * address space.
 *
 * the following picture shows the translation process for a 4K page:
 *
 * %cr3 register [PA of PDP]
 *      |
 *      |
 *      |   bits <31-22> of VA         bits <21-12> of VA   bits <11-0>
 *      |   index the PDP (0 - 1023)   index the PTP        are the page offset
 *      |         |                           |                  |
 *      |         v                           |                  |
 *      +--->+----------+                     |                  |
 *           | PD Page  |   PA of             v                  |
 *           |          |---PTP-------->+------------+           |
 *           | 1024 PDE |               | page table |--PTE--+   |
 *           | entries  |               | (aka PTP)  |       |   |
 *           +----------+               | 1024 PTE   |       |   |
 *                                      | entries    |       |   |
 *                                      +------------+       |   |
 *                                                           |   |
 *                                                bits <31-12>   bits <11-0>
 *                                                p h y s i c a l  a d d r
 *
 * the i386 caches PTEs in a TLB.   it is important to flush out old
 * TLB mappings when making a change to a mappings.   writing to the
 * %cr3 will flush the entire TLB.    newer processors also have an
 * instruction that will invalidate the mapping of a single page (which
 * is useful if you are changing a single mappings because it preserves
 * all the cached TLB entries).
 *
 * as shows, bits 31-12 of the PTE contain PA of the page being mapped.
 * the rest of the PTE is defined as follows:
 *   bit#	name	use
 *   11		n/a	available for OS use, hardware ignores it
 *   10		n/a	available for OS use, hardware ignores it
 *   9		n/a	available for OS use, hardware ignores it
 *   8		G	global bit (see discussion below)
 *   7		PS	page size [for PDEs] (0=4k, 1=4M <if supported>)
 *   6		D	dirty (modified) page
 *   5		A	accessed (referenced) page
 *   4		PCD	cache disable
 *   3		PWT	prevent write through (cache)
 *   2		U/S	user/supervisor bit (0=supervisor only, 1=both u&s)
 *   1		R/W	read/write bit (0=read only, 1=read-write)
 *   0		P	present (valid)
 *
 * notes:
 *  - on the i386 the R/W bit is ignored if processor is in supervisor
 *    state (bug!)
 *  - PS is only supported on newer processors
 *  - PTEs with the G bit are global in the sense that they are not
 *    flushed from the TLB when %cr3 is written (to flush, use the
 *    "flush single page" instruction).   this is only supported on
 *    newer processors.    this bit can be used to keep the kernel's
 *    TLB entries around while context switching.   since the kernel
 *    is mapped into all processes at the same place it does not make
 *    sense to flush these entries when switching from one process'
 *    pmap to another.
 */
/*
 * A pmap describes a process' 4GB virtual address space.  This
 * virtual address space can be broken up into 1024 4MB regions which
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
 *
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
 *
 * PDE#s	VA range		Usage
 * 0->831	0x0 -> 0xcfc00000	user address space, note that the
 *					max user address is 0xcfbfe000
 *					the final two pages in the last 4MB
 *					used to be reserved for the UAREA
 *					but now are no longer used.
 * 831		0xcfc00000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 832->1023	0xd0000000->		kernel address space (constant
 *			0xffc00000	across all pmaps/processes).
 * 1023		0xffc00000->		"alternate" recursive PDP mapping
 *			<end>		(for other pmaps).
 *
 *
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
 * words, the PTE for page 0 is the first int mapped into the 4MB recursive
 * area.  The PTE for page 1 is the second int.  The very last int in the
 * 4MB range is the PTE that maps VA 0xffffe000 (the last page in a 4GB
 * address).
 *
 * All pmaps' PDs must have the same values in slots 832->1023 so that
 * the kernel is always mapped in every process.  These values are loaded
 * into the PD at pmap creation time.
 *
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
 * will have all its PTEs mapped into memory at the recursive mapping
 * point (slot #831 as show above).  When the pmap code wants to find the
 * PTE for a virtual address, all it has to do is the following:
 *
 * Address of PTE = (831 * 4MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xcfc00000 + (VA / 4096) * 4
 *
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
 * case we take the PA of the PDP of non-active pmap and put it in
 * slot 1023 of the active pmap.  This causes the non-active pmap's
 * PTEs to get mapped in the final 4MB of the 4GB address space
 * (e.g. starting at 0xffc00000).
 *
 * The following figure shows the effects of the recursive PDP mapping:
 *
 *   PDP (%cr3)
 *   +----+
 *   |   0| -> PTP#0 that maps VA 0x0 -> 0x400000
 *   |    |
 *   |    |
 *   | 831| -> points back to PDP (%cr3) mapping VA 0xcfc00000 -> 0xd0000000
 *   | 832| -> first kernel PTP (maps 0xd0000000 -> 0xe0400000)
 *   |    |
 *   |1023| -> points to alternate pmap's PDP (maps 0xffc00000 -> end)
 *   +----+
 *
 * Note that the PDE#831 VA (0xcfc00000) is defined as "PTE_BASE".
 * Note that the PDE#1023 VA (0xffc00000) is defined as "APTE_BASE".
 *
 * Starting at VA 0xcfc00000 the current active PDP (%cr3) acts as a
 * PTP:
 *
 * PTP#831 == PDP(%cr3) => maps VA 0xcfc00000 -> 0xd0000000
 *   +----+
 *   |   0| -> maps the contents of PTP#0 at VA 0xcfc00000->0xcfc01000
 *   |    |
 *   |    |
 *   | 831| -> maps the contents of PTP#831 (the PDP) at VA 0xcff3f000
 *   | 832| -> maps the contents of first kernel PTP
 *   |    |
 *   |1023|
 *   +----+
 *
 * Note that mapping of the PDP at PTP#831's VA (0xcff3f000) is
 * defined as "PDP_BASE".... within that mapping there are two
 * defines:
 *   "PDP_PDE" (0xcff3fcfc) is the VA of the PDE in the PDP
 *      which points back to itself.
 *   "APDP_PDE" (0xcff3fffc) is the VA of the PDE in the PDP which
 *      establishes the recursive mapping of the alternate pmap.
 *      To set the alternate PDP, one just has to put the correct
 *	PA info in *APDP_PDE.
 *
 * Note that in the APTE_BASE space, the APDP appears at VA
 * "APDP_BASE" (0xfffff000).
 */
/*
a220 73
#define	PG_FRAME	0xfffff000	/* page frame mask */
#define	PG_LGFRAME	0xffc00000	/* large (4M) page frame mask */

/*
 * The following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
 */
#define	PTE_BASE	((pt_entry_t *)  (PDSLOT_PTE * NBPD) )
#define	APTE_BASE	((pt_entry_t *)  (PDSLOT_APTE * NBPD) )
#define	PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define	APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
#define	PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define	APDP_PDE	(PDP_BASE + PDSLOT_APTE)

#define	PDOFSET		(NBPD-1)	/* mask for non-PD part of VA */
#define	PTES_PER_PTP	(NBPD / NBPG)	/* # of PTEs in a PTP */

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 *
 */
#define	vtopte(VA)	(PTE_BASE + atop((vaddr_t)VA))

/*
 * Mach derived conversion macros
 */
#define	i386_round_pdr(x)	((((unsigned)(x)) + PDOFSET) & ~PDOFSET)

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define	PD_MASK		0xffc00000	/* page directory address bits */
#define	PT_MASK		0x003ff000	/* page table address bits */
#define	pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define	ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)

/*
 * PTP macros:
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
 *
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
 */

#define	ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define	ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define	ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define	ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */

/*
 * Access PD and PT
 */
#define	PDE(pm,i)	(((pd_entry_t *)(pm)->pm_pdir)[(i)])

/*
 * here we define the data types for PDEs and PTEs
 */
typedef u_int32_t pd_entry_t;           /* PDE */
typedef u_int32_t pt_entry_t;           /* PTE */

/*
 * Number of PTE's per cache line. 4 byte pte, 32-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define	NPTECL		8

d240 1
a240 1
	u_int32_t pj_pte;		/* the PTE bits */
d246 1
a246 1
	u_int32_t pq_pte;		/* aggregate PTE bits */
d268 1
a268 2
struct pmap kernel_pmap_store	/* the kernel's pmap (proc0) */
	__attribute__((aligned(32)));
a277 1
int nkptp_max = 1024 - (KERNBASE/NBPD) - 1; /* largest value (-1 for APTP space) */
d303 2
a304 2
u_int32_t protection_codes[8];     /* maps MI prot to i386 prot code */
boolean_t pmap_initialized = FALSE; /* pmap_init done yet? */
d313 2
a314 4
vaddr_t virtual_avail;	/* VA of first free KVA */
vaddr_t virtual_end;	/* VA of last free KVA */

vaddr_t vm_max_address = (PDSLOT_PTE << PDSHIFT) + (PDSLOT_PTE << PGSHIFT);
d335 2
a336 2
struct pmap_head pmaps;
struct pmap *pmaps_hand = NULL;	/* used by pmap_steal_ptp */
d363 1
a363 1
caddr_t pmap_csrcp, pmap_cdstp, pmap_zerop, pmap_ptpp;
d377 2
d383 29
a411 15
struct vm_page	*pmap_alloc_ptp_86(struct pmap *, int, boolean_t);
struct vm_page	*pmap_get_ptp_86(struct pmap *, int, boolean_t);
struct vm_page	*pmap_steal_ptp_86(struct uvm_object *, vaddr_t);
pt_entry_t	*pmap_map_ptes_86(struct pmap *);
void		 pmap_unmap_ptes_86(struct pmap *);
boolean_t	 pmap_remove_pte_86(struct pmap *, struct vm_page *,
		     pt_entry_t *, vaddr_t, int32_t *);
void		 pmap_remove_ptes_86(struct pmap *, struct vm_page *, vaddr_t,
		     vaddr_t, vaddr_t, int32_t *);
vaddr_t		 pmap_tmpmap_pa_86(paddr_t);
pt_entry_t	*pmap_tmpmap_pvepte_86(struct pv_entry *);
void		 pmap_tmpunmap_pa_86(void);
void		 pmap_tmpunmap_pvepte_86(struct pv_entry *);
boolean_t	 pmap_try_steal_pv_86(struct pv_head *,
		     struct pv_entry *, struct pv_entry *);
d413 4
a416 1
void		pmap_release(pmap_t);
d456 1
a456 1
pmap_tmpmap_pa_86(paddr_t pa)
d462 1
a462 1
	caddr_t ptpva = VASLEW(pmap_ptpp, id);
d476 1
a476 1
pmap_tmpunmap_pa_86()
d482 1
a482 1
	caddr_t ptpva = VASLEW(pmap_ptpp, id);
d503 1
a503 1
pmap_tmpmap_pvepte_86(struct pv_entry *pve)
d514 1
a514 1
	return(((pt_entry_t *)pmap_tmpmap_pa_86(VM_PAGE_TO_PHYS(pve->pv_ptp)))
d523 1
a523 1
pmap_tmpunmap_pvepte_86(struct pv_entry *pve)
d529 1
a529 1
	pmap_tmpunmap_pa_86();
d572 1
a572 1
pmap_map_ptes_86(struct pmap *pmap)
d611 1
a611 1
pmap_unmap_ptes_86(struct pmap *pmap)
d630 1
a630 1
    u_int32_t opte, u_int32_t npte)
a703 42
u_int32_t
pmap_pte_set_86(vaddr_t va, paddr_t pa, u_int32_t bits)
{
	pt_entry_t pte, *ptep = vtopte(va);

	pte = i386_atomic_testset_ul(ptep, pa | bits);	/* zap! */
	return (pte & ~PG_FRAME);
}

u_int32_t
pmap_pte_setbits_86(vaddr_t va, u_int32_t set, u_int32_t clr)
{
	pt_entry_t *ptep = vtopte(va);
	pt_entry_t pte = *ptep;

	*ptep = (pte | set) & ~clr;
	return (pte & ~PG_FRAME);

}

u_int32_t
pmap_pte_bits_86(vaddr_t va)
{
	pt_entry_t *ptep = vtopte(va);

	return (*ptep & ~PG_FRAME);
}

paddr_t
pmap_pte_paddr_86(vaddr_t va)
{
	pt_entry_t *ptep = vtopte(va);

	return (*ptep & PG_FRAME);
}

paddr_t
vtophys(vaddr_t va)
{
	return ((*vtopte(va) & PG_FRAME) | (va & ~PG_FRAME));
}

d758 1
a758 1
	u_int32_t bits;
d760 4
a763 3
	bits = pmap_pte_set(va, pa, ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) |
	    PG_V | pmap_pg_g);
	if (pmap_valid_entry(bits)) {
d767 1
a767 1
		pmap_tlb_shootdown(pmap_kernel(), va, bits, &cpumask);
d788 1
a788 1
	u_int32_t bits;
d795 5
a799 1
		bits = pmap_pte_set(va, 0, 0);
d801 1
a801 1
		if (bits & PG_PVLIST)
d804 1
a804 1
		if ((bits & (PG_V | PG_U)) == (PG_V | PG_U))
d806 1
a806 1
			pmap_tlb_shootdown(pmap_kernel(), va, bits, &cpumask);
d907 2
a908 2
	kpm->pm_pdir = (vaddr_t)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
	kpm->pm_pdirpa = proc0.p_addr->u_pcb.pcb_cr3;
d951 1
a951 1
	pmap_csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;
d953 1
a953 1
	pmap_cdstp = (caddr_t) virtual_avail+PAGE_SIZE;  cdst_pte = pte+1;
d955 1
a955 1
	pmap_zerop = (caddr_t) virtual_avail+PAGE_SIZE*2;  zero_pte = pte+2;
d957 1
a957 1
	pmap_ptpp = (caddr_t) virtual_avail+PAGE_SIZE*3;  ptp_pte = pte+3;
d962 1
a962 1
	pmap_csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;	/* allocate */
d965 1
a965 1
	pmap_cdstp = (caddr_t) virtual_avail;  cdst_pte = pte;
d968 1
a968 1
	pmap_zerop = (caddr_t) virtual_avail;  zero_pte = pte;
d971 1
a971 1
	pmap_ptpp = (caddr_t) virtual_avail;  ptp_pte = pte;
d1005 1
a1005 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 32, 0, 0, "pmappl",
a1018 8
#if defined(MULTIPROCESSOR)
	/* install the page after boot args as PT page for first 4M */
	pmap_enter(pmap_kernel(), (u_long)vtopte(0),
	    round_page((vaddr_t)(bootargv + bootargc)),
	    VM_PROT_READ|VM_PROT_WRITE, VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	memset(vtopte(0), 0, NBPG);  /* make sure it is clean before using */
#endif

d1322 1
a1322 1
pmap_try_steal_pv_86(struct pv_head *pvh, struct pv_entry *cpv,
d1343 1
a1343 1
	ptep = pmap_tmpmap_pvepte_86(cpv);
d1356 1
a1356 1
		pmap_tmpunmap_pvepte_86(cpv);
d1638 1
a1638 1
pmap_alloc_ptp_86(struct pmap *pmap, int pde_index, boolean_t just_try)
d1647 1
a1647 1
		ptp = pmap_steal_ptp_86(&pmap->pm_obj, ptp_i2o(pde_index));
d1658 2
a1659 2
	PDE(pmap, pde_index) =
	    (pd_entry_t) (VM_PAGE_TO_PHYS(ptp) | PG_u | PG_RW | PG_V);
d1677 1
a1677 1
pmap_steal_ptp_86(struct uvm_object *obj, vaddr_t offset)
d1711 1
a1711 1
				    (PDE(pmaps_hand, idx) & PG_FRAME))
d1716 1
a1716 1
				    pmap_tmpmap_pa_86(VM_PAGE_TO_PHYS(ptp));
d1722 1
a1722 1
					pmap_remove_ptes_86(pmaps_hand, ptp,
d1725 1
a1725 1
				pmap_tmpunmap_pa_86();
d1735 1
a1735 1
				PDE(pmaps_hand, idx) = 0; /* zap! */
d1779 1
a1779 1
pmap_get_ptp_86(struct pmap *pmap, int pde_index, boolean_t just_try)
d1783 1
a1783 1
	if (pmap_valid_entry(PDE(pmap, pde_index))) {
d1787 1
a1787 1
		    (PDE(pmap, pde_index) & PG_FRAME) ==
d1801 1
a1801 1
	return(pmap_alloc_ptp_86(pmap, pde_index, just_try));
d1821 3
d1825 7
a1843 5
	/* init the LDT */
	pmap->pm_ldt = NULL;
	pmap->pm_ldt_len = 0;
	pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);

a1846 11
	pmap_pinit_pd(pmap);
	return(pmap);
}

/*
 * pmap_pinit: given a zero'd pmap structure, init it.
 */

void
pmap_pinit_pd_86(struct pmap *pmap)
{
d1848 1
a1848 1
	pmap->pm_pdir = uvm_km_alloc(kernel_map, NBPG);
d1851 2
a1852 2
	pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir, &pmap->pm_pdirpa);
	pmap->pm_pdirsize = NBPG;
d1856 1
a1856 1
	bzero((void *)pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
d1858 6
a1863 1
	PDE(pmap, PDSLOT_PTE) = pmap->pm_pdirpa | PG_V | PG_KW;
d1873 1
a1873 1
	bcopy(&PDP_BASE[PDSLOT_KERN], &PDE(pmap, PDSLOT_KERN),
d1876 1
a1876 1
	bzero(&PDE(pmap, PDSLOT_KERN + nkpde),
d1954 1
a1954 1
	uvm_km_free(kernel_map, pmap->pm_pdir, pmap->pm_pdirsize);
d2135 1
a2135 1
pmap_extract_86(struct pmap *pmap, vaddr_t va, paddr_t *pap)
d2140 2
a2141 2
	if (PDE(pmap, pdei(va))) {
		ptes = pmap_map_ptes_86(pmap);
d2143 1
a2143 1
		pmap_unmap_ptes_86(pmap);
d2179 1
a2179 1
pmap_zero_phys_86(paddr_t pa)
d2185 1
a2185 1
	caddr_t zerova = VASLEW(pmap_zerop, id);
d2203 1
a2203 1
pmap_zero_page_uncached_86(paddr_t pa)
d2209 1
a2209 1
	caddr_t zerova = VASLEW(pmap_zerop, id);
d2230 1
a2230 1
pmap_copy_page_86(struct vm_page *srcpg, struct vm_page *dstpg)
d2239 2
a2240 2
	caddr_t csrcva = VASLEW(pmap_csrcp, id);
	caddr_t cdstva = VASLEW(pmap_cdstp, id);
d2274 1
a2274 1
pmap_remove_ptes_86(struct pmap *pmap, struct vm_page *ptp, vaddr_t ptpva,
d2333 1
a2333 1
			      "PG_PVLIST, va = 0x%lx, pa = 0x%llx",
d2367 1
a2367 1
pmap_remove_pte_86(struct pmap *pmap, struct vm_page *ptp, pt_entry_t *pte,
d2414 1
a2414 1
		    "PG_PVLIST, va = 0x%lx, pa = 0x%llx", va,
d2436 1
a2436 1
pmap_remove_86(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
d2450 1
a2450 1
	ptes = pmap_map_ptes_86(pmap);	/* locks pmap */
d2458 1
a2458 1
		if (pmap_valid_entry(PDE(pmap, pdei(sva)))) {
d2461 1
a2461 1
			ptppa = PDE(pmap, pdei(sva)) & PG_FRAME;
d2484 1
a2484 1
			result = pmap_remove_pte_86(pmap, ptp,
d2495 1
a2495 1
				    &PDE(pmap, pdei(sva)), 0);
d2526 1
a2526 1
		pmap_unmap_ptes_86(pmap);		/* unlock pmap */
d2556 1
a2556 1
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
d2561 1
a2561 1
		ptppa = PDE(pmap, pdei(sva)) & PG_FRAME;
d2580 1
a2580 1
		pmap_remove_ptes_86(pmap, ptp, (vaddr_t)&ptes[atop(sva)],
d2587 1
a2587 1
			    &PDE(pmap, pdei(sva)), 0);
d2616 1
a2616 1
	pmap_unmap_ptes_86(pmap);
d2628 1
a2628 1
pmap_page_remove_86(struct vm_page *pg)
d2655 1
a2655 1
		ptes = pmap_map_ptes_86(pve->pv_pmap);	/* locks pmap */
d2660 3
a2662 3
		if (pve->pv_ptp && (PDE(pve->pv_pmap,
		    pdei(pve->pv_va)) & PG_FRAME) !=
		    VM_PAGE_TO_PHYS(pve->pv_ptp)) {
d2667 1
a2667 1
			       (PDE(pve->pv_pmap, pdei(pve->pv_va)) &
d2703 2
a2704 1
				    &PDE(pve->pv_pmap, pdei(pve->pv_va)), 0);
d2726 1
a2726 1
		pmap_unmap_ptes_86(pve->pv_pmap);	/* unlocks pmap */
d2749 1
a2749 1
pmap_test_attrs_86(struct vm_page *pg, int testbits)
d2786 1
a2786 1
		ptes = pmap_map_ptes_86(pve->pv_pmap);
d2788 1
a2788 1
		pmap_unmap_ptes_86(pve->pv_pmap);
d2810 1
a2810 1
pmap_change_attrs_86(struct vm_page *pg, int setbits, int clearbits)
d2838 1
a2838 1
		if (!pmap_valid_entry(PDE(pve->pv_pmap, pdei(pve->pv_va))))
d2843 1
a2843 1
		ptes = pmap_map_ptes_86(pve->pv_pmap);	/* locks pmap */
d2853 1
a2853 1
		pmap_unmap_ptes_86(pve->pv_pmap);	/* unlocks pmap */
d2889 1
a2889 1
pmap_write_protect_86(struct pmap *pmap, vaddr_t sva, vaddr_t eva,
d2897 1
a2897 1
	ptes = pmap_map_ptes_86(pmap);		/* locks pmap */
d2923 1
a2923 1
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
d2952 1
a2952 1
	pmap_unmap_ptes_86(pmap);		/* unlocks pmap */
d2966 1
a2966 1
pmap_unwire_86(struct pmap *pmap, vaddr_t va)
d2970 2
a2971 2
	if (pmap_valid_entry(PDE(pmap, pdei(va)))) {
		ptes = pmap_map_ptes_86(pmap);		/* locks pmap */
d2987 1
a2987 1
		pmap_unmap_ptes_86(pmap);		/* unlocks map */
d3033 1
a3033 1
pmap_enter_86(struct pmap *pmap, vaddr_t va, paddr_t pa,
d3053 1
a3053 1
	    !pmap_valid_entry(PDE(pmap, pdei(va))))
d3064 1
a3064 1
	ptes = pmap_map_ptes_86(pmap);		/* locks pmap */
d3068 1
a3068 1
		ptp = pmap_get_ptp_86(pmap, pdei(va), FALSE);
d3110 1
a3110 1
					      "pa = 0x%llx (0x%lx)", pa,
d3138 1
a3138 1
				      "pa = 0x%llx (0x%lx)", pa, atop(pa));
d3224 1
a3224 1
	pmap_unmap_ptes_86(pmap);
d3238 1
a3238 1
pmap_growkernel_86(vaddr_t maxkvaddr)
d3269 1
a3269 1
			pmap_zero_phys_86(ptaddr);
d3271 2
a3272 1
			PDE(kpm, PDSLOT_KERN + nkpde) = ptaddr | PG_RW | PG_V;
d3285 1
a3285 1
		if (pmap_alloc_ptp_86(kpm, PDSLOT_KERN+nkpde, FALSE) == NULL) {
d3290 1
a3290 1
		PDE(kpm, PDSLOT_KERN + nkpde) &= ~PG_u;
d3295 2
a3296 2
			PDE(pm, PDSLOT_KERN + nkpde) =
				PDE(kpm, PDSLOT_KERN + nkpde);
d3309 1
a3309 1
void pmap_dump_86(struct pmap *, vaddr_t, vaddr_t);
d3318 1
a3318 1
pmap_dump_86(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
d3336 1
a3336 1
	ptes = pmap_map_ptes_86(pmap);	/* locks pmap */
d3350 1
a3350 1
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
d3361 1
a3361 1
	pmap_unmap_ptes_86(pmap);
d3427 1
a3427 1
pmap_tlb_shootdown(pmap_t pmap, vaddr_t va, u_int32_t pte, int32_t *cpumaskp)
a3646 28

#ifndef SMALL_KERNEL
u_int32_t (*pmap_pte_set_p)(vaddr_t, paddr_t, u_int32_t) =
	  pmap_pte_set_86;
u_int32_t (*pmap_pte_setbits_p)(vaddr_t, u_int32_t, u_int32_t) =
	    pmap_pte_setbits_86;
u_int32_t (*pmap_pte_bits_p)(vaddr_t) = pmap_pte_bits_86;
paddr_t	(*pmap_pte_paddr_p)(vaddr_t) = pmap_pte_paddr_86;
boolean_t (*pmap_change_attrs_p)(struct vm_page *, int, int) =
	    pmap_change_attrs_86;
int	(*pmap_enter_p)(pmap_t, vaddr_t, paddr_t, vm_prot_t, int) =
	  pmap_enter_86;
boolean_t (*pmap_extract_p)(pmap_t, vaddr_t, paddr_t *) = pmap_extract_86;
vaddr_t	(*pmap_growkernel_p)(vaddr_t) = pmap_growkernel_86;
void	(*pmap_page_remove_p)(struct vm_page *) = pmap_page_remove_86;
void	(*pmap_remove_p)(struct pmap *, vaddr_t, vaddr_t) = pmap_remove_86;
boolean_t (*pmap_test_attrs_p)(struct vm_page *, int) = pmap_test_attrs_86;
void	(*pmap_unwire_p)(struct pmap *, vaddr_t) = pmap_unwire_86;
void	(*pmap_write_protect_p)(struct pmap *, vaddr_t, vaddr_t, vm_prot_t) =
	  pmap_write_protect_86;
void	(*pmap_pinit_pd_p)(pmap_t) = pmap_pinit_pd_86;
void	(*pmap_zero_phys_p)(paddr_t) = pmap_zero_phys_86;
boolean_t (*pmap_zero_page_uncached_p)(paddr_t) = pmap_zero_page_uncached_86;
void	(*pmap_copy_page_p)(struct vm_page *, struct vm_page *) =
	  pmap_copy_page_86;
boolean_t (*pmap_try_steal_pv_p)(struct pv_head *, struct pv_entry *,
	    struct pv_entry *) = pmap_try_steal_pv_86;
#endif /* !SMALL_KERNEL */
@


1.95
log
@ansi/deregister
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2006/08/28 12:18:10 mickey Exp $	*/
a349 15
 * "normal" locks:
 *
 *  - pmap_main_lock
 *    this lock is used to prevent deadlock and/or provide mutex
 *    access to the pmap system.   most operations lock the pmap
 *    structure first, then they lock the pv_lists (if needed).
 *    however, some operations such as pmap_page_protect lock
 *    the pv_lists and then lock pmaps.   in order to prevent a
 *    cycle, we require a mutex lock when locking the pv_lists
 *    first.   thus, the "pmap = >pv_list" lockers must gain a
 *    read-lock on pmap_main_lock before locking the pmap.   and
 *    the "pv_list => pmap" lockers must gain a write-lock on
 *    pmap_main_lock before locking.    since only one thread
 *    can write-lock a lock at a time, this provides mutex.
 *
a380 16
#if defined(MULTIPROCESSOR) && 0

struct lock pmap_main_lock;

#define PMAP_MAP_TO_HEAD_LOCK() \
     spinlockmgr(&pmap_main_lock, LK_SHARED, (void *) 0)
#define PMAP_MAP_TO_HEAD_UNLOCK() \
     spinlockmgr(&pmap_main_lock, LK_RELEASE, (void *) 0)

#define PMAP_HEAD_TO_MAP_LOCK() \
     spinlockmgr(&pmap_main_lock, LK_EXCLUSIVE, (void *) 0)
#define PMAP_HEAD_TO_MAP_UNLOCK() \
     spinlockmgr(&pmap_main_lock, LK_RELEASE, (void *) 0)

#else

a386 2
#endif

a1255 3
#if defined(MULTIPROCESSOR) && 0
	spinlockinit(&pmap_main_lock, "pmaplk", 0);
#endif
a1839 1
 * => caller should hold the proper lock on pmap_main_lock
a1863 1
 * => caller should hold proper lock on pmap_main_lock
@


1.94
log
@avoid avail_end common; no binary change; found by grunk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2006/05/19 20:53:31 brad Exp $	*/
d1055 1
a1055 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d1084 1
a1084 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d1129 1
a1129 2
pmap_bootstrap(kva_start)
	vaddr_t kva_start;
d1441 1
a1441 3
pmap_alloc_pv(pmap, mode)
	struct pmap *pmap;
	int mode;
d1496 1
a1496 3
pmap_alloc_pvpage(pmap, mode)
	struct pmap *pmap;
	int mode;
d1698 1
a1698 3
pmap_add_pvpage(pvp, need_entry)
	struct pv_page *pvp;
	boolean_t need_entry;
d1729 1
a1729 2
pmap_free_pv_doit(pv)
	struct pv_entry *pv;
d1763 1
a1763 3
pmap_free_pv(pmap, pv)
	struct pmap *pmap;
	struct pv_entry *pv;
d1786 1
a1786 3
pmap_free_pvs(pmap, pvs)
	struct pmap *pmap;
	struct pv_entry *pvs;
d1821 1
a1821 1
pmap_free_pvpage()
d1880 3
d1886 2
a1887 6
pmap_enter_pv(pvh, pve, pmap, va, ptp)
	struct pv_head *pvh;
	struct pv_entry *pve;	/* preallocated pve for us to use */
	struct pmap *pmap;
	vaddr_t va;
	struct vm_page *ptp;	/* PTP in pmap that maps this VA */
d1909 1
a1909 4
pmap_remove_pv(pvh, pmap, va)
	struct pv_head *pvh;
	struct pmap *pmap;
	vaddr_t va;
d1945 1
a1945 4
pmap_alloc_ptp_86(pmap, pde_index, just_try)
	struct pmap *pmap;
	int pde_index;
	boolean_t just_try;
d2123 1
a2123 1
pmap_create()
d2196 1
a2196 2
pmap_destroy(pmap)
	struct pmap *pmap;
d2228 1
a2228 2
pmap_release(pmap)
	struct pmap *pmap;
d2285 1
a2285 2
pmap_reference(pmap)
	struct pmap *pmap;
d2299 1
a2299 2
pmap_fork(pmap1, pmap2)
	struct pmap *pmap1, *pmap2;
d2332 1
a2332 2
pmap_ldt_cleanup(p)
	struct proc *p;
d2375 1
a2375 2
pmap_activate(p)
	struct proc *p;
d2420 1
a2420 2
pmap_deactivate(p)
	struct proc *p;
d2465 1
a2465 3
pmap_virtual_space(startp, endp)
	vaddr_t *startp;
	vaddr_t *endp;
d2675 2
a2676 6
pmap_remove_pte_86(pmap, ptp, pte, va, cpumaskp)
	struct pmap *pmap;
	struct vm_page *ptp;
	pt_entry_t *pte;
	vaddr_t va;
	int32_t *cpumaskp;
d3311 1
a3311 2
pmap_collect(pmap)
	struct pmap *pmap;
d3624 1
a3624 3
pmap_dump_86(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva, eva;
d3733 1
a3733 5
pmap_tlb_shootdown(pmap, va, pte, cpumaskp)
	pmap_t pmap;
	vaddr_t va;
	u_int32_t pte;
	int32_t *cpumaskp;
d3891 1
a3891 2
pmap_tlb_shootdown_q_drain(pq)
	struct pmap_tlb_shootdown_q *pq;
d3911 1
a3911 2
pmap_tlb_shootdown_job_get(pq)
	struct pmap_tlb_shootdown_q *pq;
d3939 2
a3940 3
pmap_tlb_shootdown_job_put(pq, pj)
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
@


1.94.2.1
log
@Before we free the pmap just make sure it's not cached anywhere.  This
fixes possible panics on amd64 machines running i386 (like intel
Core2Duos, AMD Opterons).

from thib@@
ok deraadt@@ art@@

backport from 1.114 in -current:
---snip---
Changes by: krw@@cvs.openbsd.org 2007/05/28 12:31:11

Log message:
Flush pmap from tlb before freeing it. Makes Core2Duo boxes more
stable, but is not a fully-understood or final fix.

From and ok art
---snap---
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2006/08/28 12:18:10 mickey Exp $	*/
a2266 5

	/*
	 * Before we free the pmap just make sure it's not cached anywhere.
	 */
	tlbflushg();
@


1.93
log
@clean out some NetBSD cruft from the pmap code.

ok mickey@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2006/04/27 15:37:51 mickey Exp $	*/
a569 1
paddr_t avail_end;	/* PA of last available physical page */
d1137 1
@


1.92
log
@implement separate PAE pmap that allows access to 64g of physmem
if supported by the cpu(s). currently not enabled by default and
not compiled into ramdisks. this grows paddr_t to 64bit but yet
leaves bus_addr_t at 32bits. measures are taken to favour dmaable
memory allocation from below 4g line such that buffer cache is
already allocated form below, pool backend allocator prefers lower
memory and then finally bounce buffers are used as last resort.
PAE is engaged only if global variable cpu_pae is manually set
to non-zero and there is physical memory present above 4g.
simplify pcibios address math to use u_long as we always will
be in the 32bit space.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2006/03/13 18:42:16 mickey Exp $	*/
a79 4
#ifdef __NetBSD__
#include <machine/isa_machdep.h>
#endif
#ifdef __OpenBSD__
a81 1
#endif
a642 8
#ifdef __NetBSD__
extern vaddr_t msgbuf_vaddr;
extern paddr_t msgbuf_paddr;

extern vaddr_t idt_vaddr;			/* we allocate IDT early */
extern paddr_t idt_paddr;
#endif

a1278 4
#ifdef __NetBSD
	msgbuf_vaddr = virtual_avail;			/* don't need pte */
#endif
#ifdef __OpenBSD__
a1279 1
#endif
a1281 14
#ifdef __NetBSD__
	idt_vaddr = virtual_avail;			/* don't need pte */
	virtual_avail += PAGE_SIZE; pte++;
	idt_paddr = avail_start;			/* steal a page */
	avail_start += PAGE_SIZE;

#if defined(I586_CPU)
	/* pentium f00f bug stuff */
	pentium_idt_vaddr = virtual_avail;		/* don't need pte */
	virtual_avail += PAGE_SIZE; pte++;
#endif
#endif

#ifdef __OpenBSD__
a1283 1
#endif
a1320 36

#ifdef __NetBSD__
	/*
	 * we must call uvm_page_physload() after we are done playing with
	 * virtual_avail but before we call pmap_steal_memory.  [i.e. here]
	 * this call tells the VM system how much physical memory it
	 * controls.  If we have 16M of RAM or less, just put it all on
	 * the default free list.  Otherwise, put the first 16M of RAM
	 * on a lower priority free list (so that all of the ISA DMA'able
	 * memory won't be eaten up first-off).
	 */

	if (avail_end <= (16 * 1024 * 1024))
		first16q = VM_FREELIST_DEFAULT;
	else
		first16q = VM_FREELIST_FIRST16;

	if (avail_start < hole_start)   /* any free memory before the hole? */
		uvm_page_physload(atop(avail_start), atop(hole_start),
				  atop(avail_start), atop(hole_start),
				  first16q);

	if (first16q != VM_FREELIST_DEFAULT &&
	    hole_end < 16 * 1024 * 1024) {
		uvm_page_physload(atop(hole_end), atop(16 * 1024 * 1024),
				  atop(hole_end), atop(16 * 1024 * 1024),
				  first16q);
		uvm_page_physload(atop(16 * 1024 * 1024), atop(avail_end),
				  atop(16 * 1024 * 1024), atop(avail_end),
				  VM_FREELIST_DEFAULT);
	} else {
		uvm_page_physload(atop(hole_end), atop(avail_end),
				  atop(hole_end), atop(avail_end),
				  VM_FREELIST_DEFAULT);
	}
#endif
@


1.91
log
@time-bind and lower power in lock spinning and a couple of other ipi loops by using pause insn; brad@@ ok tedu@@ ok and feedback krw@@ testing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2006/03/12 21:28:54 tom Exp $	*/
a88 9
 * general info:
 *
 *  - for an explanation of how the i386 MMU hardware works see
 *    the comments in <machine/pte.h>.
 *
 *  - for an explanation of the general memory structure used by
 *    this pmap (including the recursive mapping), see the comments
 *    in <machine/pmap.h>.
 *
d125 175
a299 1

a349 1

d425 73
d517 1
a517 1
	pt_entry_t pj_pte;		/* the PTE bits */
d523 1
a523 1
	int pq_pte;			/* aggregate PTE bits */
d545 2
a546 1
struct pmap kernel_pmap_store;	/* the kernel's pmap (proc0) */
d556 1
d583 2
a584 2
static pt_entry_t protection_codes[8];     /* maps MI prot to i386 prot code */
static boolean_t pmap_initialized = FALSE; /* pmap_init done yet? */
d593 2
a594 2
static vaddr_t virtual_avail;	/* VA of first free KVA */
static vaddr_t virtual_end;	/* VA of last free KVA */
d596 1
d617 2
a618 2
static struct pmap_head pmaps;
static struct pmap *pmaps_hand = NULL;	/* used by pmap_steal_ptp */
d645 1
a645 1
static caddr_t csrcp, cdstp, zerop, ptpp;
a666 2
struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t);
struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
d671 15
a685 29
void		 pmap_enter_pv(struct pv_head *,
					    struct pv_entry *, struct pmap *,
					    vaddr_t, struct vm_page *);
void		 pmap_free_pv(struct pmap *, struct pv_entry *);
void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
void		 pmap_free_pv_doit(struct pv_entry *);
void		 pmap_free_pvpage(void);
struct vm_page	*pmap_get_ptp(struct pmap *, int, boolean_t);
boolean_t	 pmap_is_curpmap(struct pmap *);
boolean_t	 pmap_is_active(struct pmap *, int);
pt_entry_t	*pmap_map_ptes(struct pmap *);
struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
					     vaddr_t);
boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *, pt_entry_t *,
    vaddr_t, int32_t *);
void		 pmap_remove_ptes(struct pmap *, struct vm_page *, vaddr_t,
    vaddr_t, vaddr_t, int32_t *);
struct vm_page	*pmap_steal_ptp(struct uvm_object *, vaddr_t);
vaddr_t		 pmap_tmpmap_pa(paddr_t);
pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
void		 pmap_tmpunmap_pa(void);
void		 pmap_tmpunmap_pvepte(struct pv_entry *);
void		 pmap_apte_flush(struct pmap *);
boolean_t	 pmap_try_steal_pv(struct pv_head *,
						struct pv_entry *,
						struct pv_entry *);
void		pmap_unmap_ptes(struct pmap *);
void		pmap_exec_account(struct pmap *, vaddr_t, pt_entry_t,
		    pt_entry_t);
d687 1
a687 4
void			pmap_pinit(pmap_t);
void			pmap_release(pmap_t);

void			pmap_zero_phys(paddr_t);
d727 1
a727 2
pmap_tmpmap_pa(pa)
	paddr_t pa;
d733 1
a733 1
	caddr_t ptpva = VASLEW(ptpp, id);
d747 1
a747 1
pmap_tmpunmap_pa()
d753 1
a753 1
	caddr_t ptpva = VASLEW(ptpp, id);
d774 1
a774 2
pmap_tmpmap_pvepte(pve)
	struct pv_entry *pve;
d785 1
a785 1
	return(((pt_entry_t *)pmap_tmpmap_pa(VM_PAGE_TO_PHYS(pve->pv_ptp)))
d794 1
a794 2
pmap_tmpunmap_pvepte(pve)
	struct pv_entry *pve;
d800 1
a800 1
	pmap_tmpunmap_pa();
d843 1
a843 2
pmap_map_ptes(pmap)
	struct pmap *pmap;
d882 1
a882 2
pmap_unmap_ptes(pmap)
	struct pmap *pmap;
d884 1
a884 1
	if (pmap == pmap_kernel()) {
d886 1
a886 1
	}
d901 1
a901 1
    pt_entry_t opte, pt_entry_t npte)
d975 42
d1074 1
a1074 1
	pt_entry_t *pte, opte, npte;
d1076 3
a1078 4
	pte = vtopte(va);
	npte = pa | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) | PG_V | pmap_pg_g;
	opte = i386_atomic_testset_ul(pte, npte); /* zap! */
	if (pmap_valid_entry(opte)) {
d1082 1
a1082 1
		pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
d1105 1
a1105 1
	pt_entry_t *pte, opte;
d1112 1
a1112 5
		if (va < VM_MIN_KERNEL_ADDRESS)
			pte = vtopte(va);
		else
			pte = kvtopte(va);
		opte = i386_atomic_testset_ul(pte, 0); /* zap! */
d1114 1
a1114 1
		if (opte & PG_PVLIST)
d1117 1
a1117 1
		if ((opte & (PG_V | PG_U)) == (PG_V | PG_U))
d1119 1
a1119 1
			pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
d1220 2
a1221 2
	kpm->pm_pdir = (pd_entry_t *)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
	kpm->pm_pdirpa = (u_int32_t) proc0.p_addr->u_pcb.pcb_cr3;
d1264 1
a1264 1
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;
d1266 1
a1266 1
	cdstp = (caddr_t) virtual_avail+PAGE_SIZE;  cdst_pte = pte+1;
d1268 1
a1268 1
	zerop = (caddr_t) virtual_avail+PAGE_SIZE*2;  zero_pte = pte+2;
d1270 1
a1270 1
	ptpp = (caddr_t) virtual_avail+PAGE_SIZE*3;  ptp_pte = pte+3;
d1275 1
a1275 1
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;	/* allocate */
d1278 1
a1278 1
	cdstp = (caddr_t) virtual_avail;  cdst_pte = pte;
d1281 1
a1281 1
	zerop = (caddr_t) virtual_avail;  zero_pte = pte;
d1284 1
a1284 1
	ptpp = (caddr_t) virtual_avail;  ptp_pte = pte;
d1341 1
a1341 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
d1391 8
d1706 2
a1707 3
pmap_try_steal_pv(pvh, cpv, prevpv)
	struct pv_head *pvh;
	struct pv_entry *cpv, *prevpv;
d1727 1
a1727 1
	ptep = pmap_tmpmap_pvepte(cpv);
d1740 1
a1740 1
		pmap_tmpunmap_pvepte(cpv);
d2035 1
a2035 1
pmap_alloc_ptp(pmap, pde_index, just_try)
d2047 1
a2047 1
		ptp = pmap_steal_ptp(&pmap->pm_obj, ptp_i2o(pde_index));
d2058 2
a2059 2
	pmap->pm_pdir[pde_index] =
		(pd_entry_t) (VM_PAGE_TO_PHYS(ptp) | PG_u | PG_RW | PG_V);
d2077 1
a2077 3
pmap_steal_ptp(obj, offset)
	struct uvm_object *obj;
	vaddr_t offset;
d2111 1
a2111 1
				    (pmaps_hand->pm_pdir[idx] & PG_FRAME))
d2116 1
a2116 1
					pmap_tmpmap_pa(VM_PAGE_TO_PHYS(ptp));
d2122 1
a2122 1
					pmap_remove_ptes(pmaps_hand, ptp,
d2125 1
a2125 1
				pmap_tmpunmap_pa();
d2135 1
a2135 1
				pmaps_hand->pm_pdir[idx] = 0;	/* zap! */
d2179 1
a2179 4
pmap_get_ptp(pmap, pde_index, just_try)
	struct pmap *pmap;
	int pde_index;
	boolean_t just_try;
d2183 1
a2183 1
	if (pmap_valid_entry(pmap->pm_pdir[pde_index])) {
d2187 1
a2187 1
		    (pmap->pm_pdir[pde_index] & PG_FRAME) ==
d2201 1
a2201 1
	return(pmap_alloc_ptp(pmap, pde_index, just_try));
a2220 7
	pmap_pinit(pmap);
	return(pmap);
}

/*
 * pmap_pinit: given a zero'd pmap structure, init it.
 */
a2221 4
void
pmap_pinit(pmap)
	struct pmap *pmap;
{
d2234 5
d2242 11
d2254 1
a2254 1
	pmap->pm_pdir = (pd_entry_t *) uvm_km_alloc(kernel_map, NBPG);
d2257 2
a2258 2
	(void) pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
			    (paddr_t *)&pmap->pm_pdirpa);
d2262 1
a2262 1
	bzero(pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
d2264 1
a2264 6
	pmap->pm_pdir[PDSLOT_PTE] = pmap->pm_pdirpa | PG_V | PG_KW;

	/* init the LDT */
	pmap->pm_ldt = NULL;
	pmap->pm_ldt_len = 0;
	pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);
d2274 1
a2274 1
	bcopy(&PDP_BASE[PDSLOT_KERN], &pmap->pm_pdir[PDSLOT_KERN],
d2277 1
a2277 1
	bzero(&pmap->pm_pdir[PDSLOT_KERN + nkpde],
d2301 1
a2301 1
	if (refs > 0) {
a2302 1
	}
d2357 1
a2357 1
	uvm_km_free(kernel_map, (vaddr_t)pmap->pm_pdir, NBPG);
d2543 1
a2543 4
pmap_extract(pmap, va, pap)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t *pap;
d2548 2
a2549 2
	if (pmap->pm_pdir[pdei(va)]) {
		ptes = pmap_map_ptes(pmap);
d2551 1
a2551 1
		pmap_unmap_ptes(pmap);
d2589 1
a2589 1
pmap_zero_phys(paddr_t pa)
d2595 1
a2595 1
	caddr_t zerova = VASLEW(zerop, id);
d2613 1
a2613 2
pmap_zero_page_uncached(pa)
	paddr_t pa;
d2619 1
a2619 1
	caddr_t zerova = VASLEW(zerop, id);
d2640 1
a2640 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
d2649 2
a2650 2
	caddr_t csrcva = VASLEW(csrcp, id);
	caddr_t cdstva = VASLEW(cdstp, id);
d2684 2
a2685 6
pmap_remove_ptes(pmap, ptp, ptpva, startva, endva, cpumaskp)
	struct pmap *pmap;
	struct vm_page *ptp;
	vaddr_t ptpva;
	vaddr_t startva, endva;
	int32_t *cpumaskp;
d2743 1
a2743 1
			      "PG_PVLIST, va = 0x%lx, pa = 0x%lx",
d2777 1
a2777 1
pmap_remove_pte(pmap, ptp, pte, va, cpumaskp)
d2828 1
a2828 1
		    "PG_PVLIST, va = 0x%lx, pa = 0x%lx", va,
d2850 1
a2850 3
pmap_remove(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva, eva;
d2864 1
a2864 1
	ptes = pmap_map_ptes(pmap);	/* locks pmap */
d2872 1
a2872 1
		if (pmap_valid_entry(pmap->pm_pdir[pdei(sva)])) {
d2875 1
a2875 1
			ptppa = pmap->pm_pdir[pdei(sva)] & PG_FRAME;
d2898 1
a2898 1
			result = pmap_remove_pte(pmap, ptp,
d2909 1
a2909 1
				    &pmap->pm_pdir[pdei(sva)], 0);
d2940 1
a2940 1
		pmap_unmap_ptes(pmap);		/* unlock pmap */
d2970 1
a2970 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
d2975 1
a2975 1
		ptppa = (pmap->pm_pdir[pdei(sva)] & PG_FRAME);
d2994 1
a2994 1
		pmap_remove_ptes(pmap, ptp, (vaddr_t)&ptes[atop(sva)],
d3001 1
a3001 1
			    &pmap->pm_pdir[pdei(sva)], 0);
d3030 1
a3030 1
	pmap_unmap_ptes(pmap);
d3042 1
a3042 2
pmap_page_remove(pg)
	struct vm_page *pg;
d3069 1
a3069 1
		ptes = pmap_map_ptes(pve->pv_pmap);		/* locks pmap */
d3072 1
a3072 1
		if (pve->pv_va >= uvm.pager_sva && pve->pv_va < uvm.pager_eva) {
d3074 3
a3076 4
		}
		if (pve->pv_ptp && (pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] &
				    PG_FRAME)
		    != VM_PAGE_TO_PHYS(pve->pv_ptp)) {
d3081 1
a3081 1
			       (pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] &
d3117 1
a3117 2
				    &pve->pv_pmap->pm_pdir[pdei(pve->pv_va)],
				    0);
d3139 1
a3139 1
		pmap_unmap_ptes(pve->pv_pmap);		/* unlocks pmap */
d3162 1
a3162 3
pmap_test_attrs(pg, testbits)
	struct vm_page *pg;
	int testbits;
d3199 1
a3199 1
		ptes = pmap_map_ptes(pve->pv_pmap);
d3201 1
a3201 1
		pmap_unmap_ptes(pve->pv_pmap);
d3223 1
a3223 3
pmap_change_attrs(pg, setbits, clearbits)
	struct vm_page *pg;
	int setbits, clearbits;
d3251 1
a3251 1
		if (!pmap_valid_entry(pve->pv_pmap->pm_pdir[pdei(pve->pv_va)]))
d3256 1
a3256 1
		ptes = pmap_map_ptes(pve->pv_pmap);		/* locks pmap */
d3266 1
a3266 1
		pmap_unmap_ptes(pve->pv_pmap);		/* unlocks pmap */
d3302 2
a3303 4
pmap_write_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
d3310 1
a3310 1
	ptes = pmap_map_ptes(pmap);		/* locks pmap */
d3336 1
a3336 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
d3365 1
a3365 1
	pmap_unmap_ptes(pmap);		/* unlocks pmap */
d3379 1
a3379 3
pmap_unwire(pmap, va)
	struct pmap *pmap;
	vaddr_t va;
d3383 2
a3384 2
	if (pmap_valid_entry(pmap->pm_pdir[pdei(va)])) {
		ptes = pmap_map_ptes(pmap);		/* locks pmap */
d3400 1
a3400 1
		pmap_unmap_ptes(pmap);		/* unlocks map */
d3447 2
a3448 6
pmap_enter(pmap, va, pa, prot, flags)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d3467 1
a3467 1
	    !pmap_valid_entry(pmap->pm_pdir[pdei(va)]))
d3478 1
a3478 1
	ptes = pmap_map_ptes(pmap);		/* locks pmap */
d3482 1
a3482 1
		ptp = pmap_get_ptp(pmap, pdei(va), FALSE);
d3524 1
a3524 1
					      "pa = 0x%lx (0x%lx)", pa,
d3552 1
a3552 1
				      "pa = 0x%lx (0x%lx)", pa, atop(pa));
d3638 1
a3638 1
	pmap_unmap_ptes(pmap);
d3652 1
a3652 2
pmap_growkernel(maxkvaddr)
	vaddr_t maxkvaddr;
d3683 1
a3683 1
			pmap_zero_phys(ptaddr);
d3685 1
a3685 2
			kpm->pm_pdir[PDSLOT_KERN + nkpde] =
				ptaddr | PG_RW | PG_V;
d3698 1
a3698 1
		if (pmap_alloc_ptp(kpm, PDSLOT_KERN + nkpde, FALSE) == NULL) {
d3703 1
a3703 1
		kpm->pm_pdir[PDSLOT_KERN + nkpde] &= ~PG_u;
d3708 2
a3709 2
			pm->pm_pdir[PDSLOT_KERN + nkpde] =
				kpm->pm_pdir[PDSLOT_KERN + nkpde];
d3722 1
a3722 1
void pmap_dump(struct pmap *, vaddr_t, vaddr_t);
d3731 1
a3731 1
pmap_dump(pmap, sva, eva)
d3751 1
a3751 1
	ptes = pmap_map_ptes(pmap);	/* locks pmap */
d3765 1
a3765 1
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
d3776 1
a3776 1
	pmap_unmap_ptes(pmap);
d3845 1
a3845 1
	pt_entry_t pte;
d4069 28
@


1.90
log
@Don't leave a dangling pointer to the process-specific LDT after
we free it.  Instead set it to the default value for a new process.

Bug found by hugh@@, fix tested by several - thanks.

ok weingart@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2006/02/16 16:08:53 mickey Exp $	*/
d3598 2
a3599 1
	while (self->ci_tlb_ipi_mask != 0)
a3603 3
#else
		/* XXX insert pause instruction */
		;
d3605 1
@


1.89
log
@make panic msg more useful
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2006/01/12 22:39:20 weingart Exp $	*/
d2189 6
@


1.89.2.1
log
@MFC:
Fix by tom@@

Don't leave a dangling pointer to the process-specific LDT after
we free it.  Instead set it to the default value for a new process.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2006/02/16 16:08:53 mickey Exp $	*/
a2188 6
		/* Reset the cached address of the LDT that this process uses */
#ifdef MULTIPROCESSOR
		pcb->pcb_ldt = curcpu()->ci_ldt;
#else
		pcb->pcb_ldt = ldt;
#endif
@


1.88
log
@Move to using gdt only (no more ldt in general case) but with a variable
limit selector, so that the w^x line can float much more dynamically.
Much work done by tom.  Tested by various people.  Addresses concerns of
(Julien Tinnes) <julien ATHOST cr0.org>
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2005/12/25 21:39:06 miod Exp $	*/
d3595 2
a3596 2
			panic("TLB IPI rendezvous failed (mask %x)",
			    self->ci_tlb_ipi_mask);
@


1.87
log
@KERN_RESOURCE_SHORTAGE -> ENOMEM
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2005/12/10 11:45:41 miod Exp $	*/
d468 2
a714 1
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
d716 1
d722 1
d739 1
a739 1
			va = trunc_page(ent->end) - PAGE_SIZE;
d743 1
a743 1
	if (va == pm->pm_hiexec)
d745 1
d749 7
a755 6
	if (pm->pm_hiexec > (vaddr_t)I386_MAX_EXE_ADDR) {
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE1_SEL, SEL_UPL);
	} else {
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
	}
	
d759 36
d1990 3
d2218 3
d2223 6
d2232 12
@


1.86
log
@{en,re}trys -> {en,re}tries; eyeballed by jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2005/11/18 17:05:04 brad Exp $	*/
d3185 1
a3185 1
				error = KERN_RESOURCE_SHORTAGE;
d3286 1
a3286 1
					error = KERN_RESOURCE_SHORTAGE;
@


1.85
log
@splimp -> splvm. no binary diff.

ok mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2005/11/13 14:23:26 martin Exp $	*/
d363 1
a363 1
static struct pv_pagelist pv_freepages;	/* list of pv_pages with free entrys */
d1343 1
a1343 1
	 * add a mapping for our new pv_page and free its entrys (save one!)
d3268 1
a3268 1
			ptp->wire_count++;      /* count # of valid entrys */
@


1.84
log
@convert more MD macros to their MI counterparts, this time
i386_round_page(), i386_trunc_page(), i386_btop() and i386_ptob()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2005/09/25 20:48:21 miod Exp $	*/
d1309 1
a1309 1
	s = splimp();   /* must protect kmem_map/kmem_object with splimp! */
d1321 1
a1321 1
	 * note: we are still holding splimp to protect kmem_object
d1337 1
a1337 1
	/* splimp now dropped */
d1969 1
a1969 1
	 * us.   note that there is no need to splimp to protect us from
@


1.83
log
@Turn CPU_INFO_FOREACH into a real construct, like all queue(3) iterators,
instead of the contents of a for() loop. No functional change.
From the m88k SMP tree; ok art@@ deraadt@@

[complete diff this time]
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2005/07/12 21:12:03 hshoexer Exp $	*/
d879 1
a879 1
	avail_end -= i386_round_page(MSGBUFSIZE);
d883 1
a883 1
	avail_end -= i386_round_page(bootargc);
d929 1
a929 1
		i386_btop(kva_start - VM_MIN_KERNEL_ADDRESS);
d947 2
a948 2
			if (pmap_valid_entry(PTE_BASE[i386_btop(kva)]))
				PTE_BASE[i386_btop(kva)] |= PG_G;
d959 1
a959 1
	pte = PTE_BASE + i386_btop(virtual_avail);
d1514 1
a1514 1
	pvp = (struct pv_page *) i386_trunc_page(pv);
d2228 1
a2228 1
		retval = (paddr_t)(ptes[i386_btop(va)] & PG_FRAME);
d2414 1
a2414 1
			if (vm_physseg_find(i386_btop(opte & PG_FRAME), &off)
d2422 1
a2422 1
		bank = vm_physseg_find(i386_btop(opte & PG_FRAME), &off);
d2500 1
a2500 1
		if (vm_physseg_find(i386_btop(opte & PG_FRAME), &off) != -1)
d2507 1
a2507 1
	bank = vm_physseg_find(i386_btop(opte & PG_FRAME), &off);
d2584 1
a2584 1
			    &ptes[i386_btop(sva)], sva, &cpumask);
d2679 1
a2679 1
		pmap_remove_ptes(pmap, ptp, (vaddr_t)&ptes[i386_btop(sva)],
d2775 2
a2776 2
		opte = ptes[i386_btop(pve->pv_va)];
		ptes[i386_btop(pve->pv_va)] = 0;		/* zap! */
d2890 1
a2890 1
		pte = ptes[i386_btop(pve->pv_va)];
d2949 1
a2949 1
		npte = ptes[i386_btop(pve->pv_va)];
d2952 1
a2952 1
		if (ptes[i386_btop(pve->pv_va)] != npte) {
d2954 1
a2954 1
			    &ptes[i386_btop(pve->pv_va)], npte);
d2956 1
a2956 1
			    i386_btop(pve->pv_va), opte, &cpumask);
d3040 2
a3041 2
		spte = &ptes[i386_btop(sva)];
		epte = &ptes[i386_btop(blockend)];
d3083 1
a3083 1
		if (!pmap_valid_entry(ptes[i386_btop(va)]))
d3086 2
a3087 2
		if ((ptes[i386_btop(va)] & PG_W) != 0) {
			ptes[i386_btop(va)] &= ~PG_W;
d3191 1
a3191 1
	opte = ptes[i386_btop(va)];		/* old PTE */
d3320 1
a3320 1
	ptes[i386_btop(va)] = npte;		/* zap! */
d3470 1
a3470 1
		pte = &ptes[i386_btop(sva)];
@


1.82
log
@call pmap_apte_flush() with the current pmap, not the one to be mapped.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2005/05/24 21:11:46 tedu Exp $	*/
d601 1
a601 1
	for (CPU_INFO_FOREACH(cii, ci)) {
d3518 1
a3518 1
	for (CPU_INFO_FOREACH(cii, ci)) {
d3569 1
a3569 1
	for (CPU_INFO_FOREACH(cii, ci)) {
d3690 1
a3690 1
	for (CPU_INFO_FOREACH(cii, ci))
@


1.82.2.1
log
@MFC:
Fix by weingart@@

Move to using gdt only (no more ldt in general case) but with a variable
limit selector, so that the w^x line can float much more dynamically.
Much work done by tom.  Tested by various people.  Addresses concerns of
(Julien Tinnes) <julien ATHOST cr0.org>

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2005/07/12 21:12:03 hshoexer Exp $	*/
a467 2
void	setcslimit(struct pmap *, struct trapframe *, struct pcb *, vaddr_t);

d713 1
a714 1
		setcslimit(pm, tf, pcb, I386_MAX_EXE_ADDR);
a719 1
 * Called by kernel SEGV trap handler.
d736 1
a736 1
			va = trunc_page(ent->end - 1);
d740 1
a740 1
	if (va <= pm->pm_hiexec) {
a741 1
	}
d745 6
a750 7
	/*
	 * We have a new 'highest executable' va, so we need to update
	 * the value for the code segment limit, which is stored in the
	 * PCB.
	 */
	setcslimit(pm, tf, pcb, va);

a753 36
void
setcslimit(struct pmap *pm, struct trapframe *tf, struct pcb *pcb,
    vaddr_t limit)
{
	/*
	 * Called when we have a new 'highest executable' va, so we need
	 * to update the value for the code segment limit, which is stored
	 * in the PCB.
	 *
	 * There are no caching issues to be concerned with: the
	 * processor reads the whole descriptor from the GDT when the
	 * appropriate selector is loaded into a segment register, and
	 * this only happens on the return to userland.
	 *
	 * This also works in the MP case, since whichever CPU gets to
	 * run the process will pick up the right descriptor value from
	 * the PCB.
	 */
	limit = min(limit, VM_MAXUSER_ADDRESS - 1);

	setsegment(&pm->pm_codeseg, 0, atop(limit),
	    SDT_MEMERA, SEL_UPL, 1, 1);

	/* And update the GDT and LDT since we may be called by the
	 * trap handler (cpu_switch won't get a chance).
	 */
#ifdef MULTIPROCESSOR
	curcpu()->ci_gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
	    pm->pm_codeseg;
#else
	gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd = pm->pm_codeseg;
#endif

	pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
}

a1948 3
	setsegment(&pmap->pm_codeseg, 0, atop(I386_MAX_EXE_ADDR) - 1,
	    SDT_MEMERA, SEL_UPL, 1, 1);

a2173 3
#ifdef MULTIPROCESSOR
	struct cpu_info *self = curcpu();
#endif
a2175 6
	/* Get the LDT that this process will actually use */
#ifdef MULTIPROCESSOR
	pcb->pcb_ldt = pmap->pm_ldt == NULL ? self->ci_ldt : pmap->pm_ldt;
#else
	pcb->pcb_ldt = pmap->pm_ldt == NULL ? ldt : pmap->pm_ldt;
#endif
a2178 12
		/*
		 * Set the correct descriptor value (i.e. with the
		 * correct code segment X limit) in the GDT and the LDT.
		 */
#ifdef MULTIPROCESSOR
		self->ci_gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
		    pmap->pm_codeseg;
#else
		gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
		    pmap->pm_codeseg;
#endif

@


1.82.2.2
log
@MFC:
Fix by tom@@

Don't leave a dangling pointer to the process-specific LDT after
we free it.  Instead set it to the default value for a new process.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82.2.1 2006/01/13 01:56:55 brad Exp $	*/
a2188 6
		/* Reset the cached address of the LDT that this process uses */
#ifdef MULTIPROCESSOR
		pcb->pcb_ldt = curcpu()->ci_ldt;
#else
		pcb->pcb_ldt = ldt;
#endif
@


1.81
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2004/12/25 23:02:24 miod Exp $	*/
d653 1
a653 1
			pmap_apte_flush(pmap);
@


1.80
log
@Use list and queue macros where applicable to make the code easier to read;
no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2004/07/20 20:18:13 art Exp $	*/
d1633 1
a1633 1
		    &dead_entries);
@


1.80.2.1
log
@MFC:
Fix by weingart@@

Move to using gdt only (no more ldt in general case) but with a variable
limit selector, so that the w^x line can float much more dynamically.
Much work done by tom.  Tested by various people.  Addresses concerns of
(Julien Tinnes) <julien ATHOST cr0.org>

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2004/12/25 23:02:24 miod Exp $	*/
a467 2
void	setcslimit(struct pmap *, struct trapframe *, struct pcb *, vaddr_t);

d713 1
a714 1
		setcslimit(pm, tf, pcb, I386_MAX_EXE_ADDR);
a719 1
 * Called by kernel SEGV trap handler.
d736 1
a736 1
			va = trunc_page(ent->end - 1);
d740 1
a740 1
	if (va <= pm->pm_hiexec) {
a741 1
	}
d745 6
a750 7
	/*
	 * We have a new 'highest executable' va, so we need to update
	 * the value for the code segment limit, which is stored in the
	 * PCB.
	 */
	setcslimit(pm, tf, pcb, va);

a753 36
void
setcslimit(struct pmap *pm, struct trapframe *tf, struct pcb *pcb,
    vaddr_t limit)
{
	/*
	 * Called when we have a new 'highest executable' va, so we need
	 * to update the value for the code segment limit, which is stored
	 * in the PCB.
	 *
	 * There are no caching issues to be concerned with: the
	 * processor reads the whole descriptor from the GDT when the
	 * appropriate selector is loaded into a segment register, and
	 * this only happens on the return to userland.
	 *
	 * This also works in the MP case, since whichever CPU gets to
	 * run the process will pick up the right descriptor value from
	 * the PCB.
	 */
	limit = min(limit, VM_MAXUSER_ADDRESS - 1);

	setsegment(&pm->pm_codeseg, 0, atop(limit),
	    SDT_MEMERA, SEL_UPL, 1, 1);

	/* And update the GDT and LDT since we may be called by the
	 * trap handler (cpu_switch won't get a chance).
	 */
#ifdef MULTIPROCESSOR
	curcpu()->ci_gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
	    pm->pm_codeseg;
#else
	gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd = pm->pm_codeseg;
#endif

	pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
}

a1948 3
	setsegment(&pmap->pm_codeseg, 0, atop(I386_MAX_EXE_ADDR) - 1,
	    SDT_MEMERA, SEL_UPL, 1, 1);

a2173 3
#ifdef MULTIPROCESSOR
	struct cpu_info *self = curcpu();
#endif
a2175 6
	/* Get the LDT that this process will actually use */
#ifdef MULTIPROCESSOR
	pcb->pcb_ldt = pmap->pm_ldt == NULL ? self->ci_ldt : pmap->pm_ldt;
#else
	pcb->pcb_ldt = pmap->pm_ldt == NULL ? ldt : pmap->pm_ldt;
#endif
a2178 12
		/*
		 * Set the correct descriptor value (i.e. with the
		 * correct code segment X limit) in the GDT and the LDT.
		 */
#ifdef MULTIPROCESSOR
		self->ci_gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
		    pmap->pm_codeseg;
#else
		gdt[GUCODE_SEL].sd = pcb->pcb_ldt[LUCODE_SEL].sd =
		    pmap->pm_codeseg;
#endif

@


1.80.2.2
log
@MFC:
Fix by tom@@

Don't leave a dangling pointer to the process-specific LDT after
we free it.  Instead set it to the default value for a new process.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80.2.1 2006/01/13 00:49:21 brad Exp $	*/
a2188 6
		/* Reset the cached address of the LDT that this process uses */
#ifdef MULTIPROCESSOR
		pcb->pcb_ldt = curcpu()->ci_ldt;
#else
		pcb->pcb_ldt = ldt;
#endif
@


1.79
log
@Use mutex where we used to protect pmap internals with SIMPLELOCK.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2004/07/02 16:29:55 niklas Exp $	*/
d1223 2
a1224 2
	if (pv_freepages.tqh_first != NULL) {
		pvpage = pv_freepages.tqh_first;
d1284 1
a1284 1
	if (mode != ALLOCPV_NONEED && pv_unusedpgs.tqh_first != NULL) {
d1287 1
a1287 1
		pvpage = pv_unusedpgs.tqh_first;
d1802 1
a1802 2
			ptp = curobj->memq.tqh_first;
			for (/*null*/; ptp != NULL; ptp = ptp->listq.tqe_next) {
d2043 2
a2044 2
	while (pmap->pm_obj.memq.tqh_first != NULL) {
		pg = pmap->pm_obj.memq.tqh_first;
d3409 1
a3409 2
		for (pm = pmaps.lh_first; pm != NULL;
		     pm = pm->pm_list.le_next) {
@


1.78
log
@Maintain %f and %gs over traps.  Mostly from NetBSD.  Preparation for SMP
speedups.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2004/06/23 17:42:46 niklas Exp $	*/
d70 1
d288 3
a290 3
	struct SIMPLELOCK pq_slock;	/* spin lock on queue */
	int pq_flushg;		/* pending flush global */
	int pq_flushu;		/* pending flush user */
d301 1
a301 1
struct SIMPLELOCK pmap_tlb_shootdown_job_lock;
a589 1
	int s;
d606 1
a606 2
			s = splipi();
			SIMPLE_LOCK(&pq->pq_slock);
d608 1
a608 2
			SIMPLE_UNLOCK(&pq->pq_slock);
			splx(s);
d1054 1
a1054 1
	SIMPLE_LOCK_INIT(&pmap_tlb_shootdown_job_lock);
d1058 1
a1058 1
		SIMPLE_LOCK_INIT(&pmap_tlb_shootdown_q[i].pq_slock);
d3578 1
a3578 1
		SIMPLE_LOCK(&pq->pq_slock);
d3587 1
a3587 1
			SIMPLE_UNLOCK(&pq->pq_slock);
d3604 1
a3604 1
			SIMPLE_UNLOCK(&pq->pq_slock);
d3620 1
a3620 1
				SIMPLE_UNLOCK(&pq->pq_slock);
d3642 1
a3642 1
		SIMPLE_UNLOCK(&pq->pq_slock);
a3657 1
	int s;
d3663 1
a3663 3
	s = splipi();

	SIMPLE_LOCK(&pq->pq_slock);
d3696 1
a3696 3
	SIMPLE_UNLOCK(&pq->pq_slock);

	splx(s);
d3738 1
a3738 1
	SIMPLE_LOCK(&pmap_tlb_shootdown_job_lock);
d3740 1
a3740 1
		SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
d3745 1
a3745 1
	SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
d3767 1
a3767 1
	SIMPLE_LOCK(&pmap_tlb_shootdown_job_lock);
d3770 1
a3770 1
	SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
@


1.77
log
@Do not send IPIs while cold.  Should probably fix the pmap_kremove panics some have seen.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2004/06/13 21:49:15 niklas Exp $	*/
a936 2

	curpcb->pcb_pmap = kpm;	/* proc0's pcb */
@


1.76
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d3517 3
@


1.75
log
@Sync user ldt code with NetBSD:
- finally remove it from pcb, it's a pmap thing only
- more sanity checks
- better lockin
- may be something else

Fixes panics when using apps requiring it (mplayer-win32 e.g.).
Problem found and test espie@@.
OKs from miod@@ (sshhh, don't tell anyone) and art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2004/01/29 19:01:54 tedu Exp $	*/
d73 1
a131 2
 * - pmap_remove_record: a list of virtual addresses whose mappings
 *	have been changed.   used for TLB flushing.
a226 11
 * - pmap_copy_page_lock
 *   locks the tmp kernel PTE mappings we used to copy data
 *
 * - pmap_zero_page_lock
 *   locks the tmp kernel PTE mapping we use to zero a page
 *
 * - pmap_tmpptp_lock
 *   locks the tmp kernel PTE mapping we use to look at a PTP
 *   in another process
 *
 * XXX: would be nice to have per-CPU VAs for the above 4
d233 4
a236 5
#ifdef __OpenBSD__
/* XXX */
#define spinlockinit(lock, name, flags) /* nada */
#define spinlockmgr(lock, flags, slock) /* nada */
#endif
a238 5
struct simplelock pvalloc_lock;
struct simplelock pmaps_lock;
struct simplelock pmap_copy_page_lock;
struct simplelock pmap_zero_page_lock;
struct simplelock pmap_tmpptp_lock;
d250 53
d386 14
d425 3
a427 3
static struct pv_entry	*pmap_add_pvpage(struct pv_page *, boolean_t);
static struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t);
static struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
d431 2
a432 2
static struct pv_entry	*pmap_alloc_pvpage(struct pmap *, int);
static void		 pmap_enter_pv(struct pv_head *,
d435 9
a443 16
static void		 pmap_free_pv(struct pmap *, struct pv_entry *);
static void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
static void		 pmap_free_pv_doit(struct pv_entry *);
static void		 pmap_free_pvpage(void);
static struct vm_page	*pmap_get_ptp(struct pmap *, int, boolean_t);
static boolean_t	 pmap_is_curpmap(struct pmap *);
static pt_entry_t	*pmap_map_ptes(struct pmap *);
static struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
					     vaddr_t);
static boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *,
					      pt_entry_t *, vaddr_t);
static void		 pmap_remove_ptes(struct pmap *,
					       struct pmap_remove_record *,
					       struct vm_page *, vaddr_t,
					       vaddr_t, vaddr_t);
static struct vm_page	*pmap_steal_ptp(struct uvm_object *,
d445 11
a455 5
static vaddr_t		 pmap_tmpmap_pa(paddr_t);
static pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
static void		 pmap_tmpunmap_pa(void);
static void		 pmap_tmpunmap_pvepte(struct pv_entry *);
static boolean_t	 pmap_try_steal_pv(struct pv_head *,
d458 3
a460 1
static void		pmap_unmap_ptes(struct pmap *);
d476 1
a476 1
__inline static boolean_t
d485 14
a499 2
 *
 * => returns with pmap_tmpptp_lock held
d502 1
a502 1
__inline static vaddr_t
d506 5
a510 1
	simple_lock(&pmap_tmpptp_lock);
d512 1
a512 1
	if (*ptp_pte)
d515 2
a516 2
	*ptp_pte = PG_V | PG_RW | pa;		/* always a new mapping */
	return((vaddr_t)ptpp);
a520 2
 *
 * => we release pmap_tmpptp_lock
d523 1
a523 1
__inline static void
d526 5
d532 1
a532 1
	if (!pmap_valid_entry(*ptp_pte))
d535 7
a541 3
	*ptp_pte = 0;		/* zap! */
	pmap_update_pg((vaddr_t)ptpp);
	simple_unlock(&pmap_tmpptp_lock);
a547 1
 * => we may grab pmap_tmpptp_lock and return with it held
d550 1
a550 1
__inline static pt_entry_t *
a568 2
 *
 * => we will release pmap_tmpptp_lock if we hold it
d571 1
a571 1
__inline static void
d582 35
d624 1
a624 1
__inline static pt_entry_t *
d655 1
a655 1
			tlbflush();
d664 1
a664 1
__inline static void
d674 4
d683 1
a683 1
__inline static void
d691 8
a698 1
	if ((opte ^ npte) & PG_X)
d700 3
a702 1
		
d777 1
a777 1
	pt_entry_t *pte, opte;
d780 10
a789 4
	opte = *pte;
	*pte = pa | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) |
		PG_V | pmap_pg_g;	/* zap! */
	if (pmap_valid_entry(opte))
d791 2
a801 2
 * => we assume kernel only unmaps valid addresses and thus don't bother
 *    checking the valid bit before doing TLB flushing
d809 4
a812 1
	pt_entry_t *pte;
d815 6
a820 2
	for ( /* null */ ; len ; len--, va += NBPG) {
		pte = vtopte(va);
d822 2
a823 7
		if (*pte & PG_PVLIST)
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx",
			      va);
#endif
		*pte = 0;		/* zap! */
#if defined(I386_CPU)
		if (cpu_class != CPUCLASS_386)
d825 4
d830 1
d832 2
a833 3
#if defined(I386_CPU)
	if (cpu_class == CPUCLASS_386)
		tlbflush();
d862 1
d965 20
d996 1
d1000 1
a1000 1
	virtual_avail += PAGE_SIZE; pte++;
d1038 1
d1040 1
a1042 3
	simple_lock_init(&pmap_copy_page_lock);
	simple_lock_init(&pmap_zero_page_lock);
	simple_lock_init(&pmap_tmpptp_lock);
d1054 11
d1174 9
d1217 1
a1217 1
__inline static struct pv_entry *
d1274 1
a1274 1
static struct pv_entry *
d1407 1
a1407 1
static boolean_t
d1412 4
a1415 1
	pt_entry_t *ptep;	/* pointer to a PTE */
d1434 6
a1439 1
		*ptep = 0;		/* zap! */
d1442 1
d1479 1
a1479 1
static struct pv_entry *
d1512 1
a1512 1
__inline static void
d1547 1
a1547 1
__inline static void
d1572 1
a1572 1
__inline static void
d1609 1
a1609 1
static void
d1671 1
a1671 1
__inline static void
d1698 1
a1698 1
__inline static struct pv_entry *
d1737 1
a1737 1
__inline static struct vm_page *
d1779 1
a1779 1
static struct vm_page *
d1790 1
d1828 3
a1830 4
					pmap_remove_ptes(pmaps_hand, NULL, ptp,
							 (vaddr_t)ptes,
							 ptp_i2v(idx),
							 ptp_i2v(idx+1));
d1843 3
d1847 1
a1847 1
					tlbflush();
d1849 2
a1850 2
					 (*APDP_PDE & PG_FRAME) ==
					 pmaps_hand->pm_pdirpa) {
d1853 1
a1853 1
				}
d1873 1
d1884 1
a1884 1
static struct vm_page *
d2060 4
a2063 1
	/* XXX: need to flush it out of other processor's APTE space? */
d2183 1
a2183 1
	if (p == curproc)
a2184 1
	if (pcb == curpcb)
d2186 6
a2195 2
 *
 * => XXX: what should this do, if anything?
d2202 6
d2264 1
a2264 13
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

	simple_lock(&pmap_zero_page_lock);
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page: lock botch");
#endif

	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	pagezero(zerop, PAGE_SIZE);				/* zero */
	*zero_pte = 0;				/* zap! */
	pmap_update_pg((vaddr_t)zerop);		/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
d2274 6
a2279 1
	simple_lock(&pmap_zero_page_lock);
d2281 2
a2282 2
	if (*zero_pte)
		panic("pmap_zero_page: lock botch");
d2285 4
a2288 5
	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	pagezero(zerop, PAGE_SIZE);				/* zero */
	*zero_pte = 0;				/* zap! */
	pmap_update_pg((vaddr_t)zerop);		/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
d2299 6
a2304 1
	simple_lock(&pmap_zero_page_lock);
d2306 1
a2306 1
	if (*zero_pte)
d2310 1
a2310 1
	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW |	/* map in */
d2312 3
a2314 4
	pagezero(zerop, PAGE_SIZE);				/* zero */
	*zero_pte = 0;					/* zap! */
	pmap_update_pg((vaddr_t)zerop);			/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
d2328 7
a2335 1
	simple_lock(&pmap_copy_page_lock);
d2337 1
a2337 1
	if (*csrc_pte || *cdst_pte)
d2341 9
a2349 6
	*csrc_pte = (srcpa & PG_FRAME) | PG_V | PG_RW;
	*cdst_pte = (dstpa & PG_FRAME) | PG_V | PG_RW;
	bcopy(csrcp, cdstp, PAGE_SIZE);
	*csrc_pte = *cdst_pte = 0;			/* zap! */
	pmap_update_2pg((vaddr_t)csrcp, (vaddr_t)cdstp);
	simple_unlock(&pmap_copy_page_lock);
d2367 2
a2368 2
static void
pmap_remove_ptes(pmap, pmap_rr, ptp, ptpva, startva, endva)
a2369 1
	struct pmap_remove_record *pmap_rr;
d2373 1
d2395 3
a2397 2
		opte = *pte;		/* save the old PTE */
		*pte = 0;			/* zap! */
d2402 9
a2410 14
		if (pmap_rr) {		/* worried about tlb flushing? */
			if (opte & PG_G) {
				/* PG_G requires this */
				pmap_update_pg(startva);
			} else {
				if (pmap_rr->prr_npages < PMAP_RR_MAX) {
					pmap_rr->prr_vas[pmap_rr->prr_npages++]
						= startva;
				} else {
					if (pmap_rr->prr_npages == PMAP_RR_MAX)
						/* signal an overflow */
						pmap_rr->prr_npages++;
				}
			}
a2411 2
		if (ptp)
			ptp->wire_count--;		/* dropping a PTE */
d2464 2
a2465 2
static boolean_t
pmap_remove_pte(pmap, ptp, pte, va)
d2470 1
d2488 4
a2491 1
	if (ptp)
d2493 3
d2497 1
a2497 2
	if (pmap_is_curpmap(pmap))
		pmap_update_pg(va);		/* flush TLB */
d2542 1
a2542 1
	pt_entry_t *ptes;
d2547 1
a2547 1
	struct pmap_remove_record pmap_rr, *prr;
d2553 1
a2553 1
	PMAP_MAP_TO_HEAD_LOCK();
d2589 1
a2589 1
						 &ptes[i386_btop(sva)], sva);
d2597 23
a2619 4
				pmap->pm_pdir[pdei(sva)] = 0;	/* zap! */
#if defined(I386_CPU)
				/* already dumped whole TLB on i386 */
				if (cpu_class != CPUCLASS_386)
a2620 4
				{
					pmap_update_pg(((vaddr_t) ptes) +
						       ptp->offset);
				}
d2629 1
a2629 1

a2634 14
	/*
	 * removing a range of pages: we unmap in PTP sized blocks (4MB)
	 *
	 * if we are the currently loaded pmap, we use prr to keep track
	 * of the VAs we unload so that we can flush them out of the tlb.
	 */

	if (pmap_is_curpmap(pmap)) {
		prr = &pmap_rr;
		prr->prr_npages = 0;
	} else {
		prr = NULL;
	}

d2684 2
a2685 2
		pmap_remove_ptes(pmap, prr, ptp,
				 (vaddr_t)&ptes[i386_btop(sva)], sva, blkendva);
d2689 20
a2708 6
			pmap->pm_pdir[pdei(sva)] = 0;	/* zap! */
			pmap_update_pg( ((vaddr_t) ptes) + ptp->offset);
#if defined(I386_CPU)
			/* cancel possible pending pmap update on i386 */
			if (cpu_class == CPUCLASS_386 && prr)
				prr->prr_npages = 0;
d2719 1
a2719 21
	/*
	 * if we kept a removal record and removed some pages update the TLB
	 */

	if (prr && prr->prr_npages) {
#if defined(I386_CPU)
		if (cpu_class == CPUCLASS_386) {
			tlbflush();
		} else
#endif
		{ /* not I386 */
			if (prr->prr_npages > PMAP_RR_MAX) {
				tlbflush();
			} else {
				while (prr->prr_npages) {
					pmap_update_pg(
					    prr->prr_vas[--prr->prr_npages]);
				}
			}
		} /* not I386 */
	}
d2739 1
a2739 3
#if defined(I386_CPU)
	boolean_t needs_update = FALSE;
#endif
d2787 4
a2790 8
		if (pmap_is_curpmap(pve->pv_pmap)) {
#if defined(I386_CPU)
			if (cpu_class == CPUCLASS_386)
				needs_update = TRUE;
			else
#endif
				pmap_update_pg(pve->pv_va);
		}
d2799 8
d2808 14
a2821 5
				pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] = 0;
				pmap_update_pg(((vaddr_t)ptes) +
					       pve->pv_ptp->offset);
#if defined(I386_CPU)
				needs_update = FALSE;
d2838 1
a2838 4
#if defined(I386_CPU)
	if (needs_update)
		tlbflush();
#endif
d2926 1
a2926 1
	pt_entry_t *ptes, npte;
d2928 1
a2928 3
#if defined(I386_CPU)
	boolean_t needs_update = FALSE;
#endif
d2958 4
a2961 10
			ptes[i386_btop(pve->pv_va)] = npte;	/* zap! */

			if (pmap_is_curpmap(pve->pv_pmap)) {
#if defined(I386_CPU)
				if (cpu_class == CPUCLASS_386)
					needs_update = TRUE;
				else
#endif
					pmap_update_pg(pve->pv_va);
			}
d2968 1
a2969 4
#if defined(I386_CPU)
	if (needs_update)
		tlbflush();
#endif
d3005 1
a3005 2
	struct pmap_remove_record pmap_rr, *prr;
	vaddr_t blockend, va;
d3007 1
a3010 8
	/* need to worry about TLB? [TLB stores protection bits] */
	if (pmap_is_curpmap(pmap)) {
		prr = &pmap_rr;
		prr->prr_npages = 0;
	} else {
		prr = NULL;
	}

d3057 4
a3060 24

				*spte = npte;		/* zap! */

				if (prr) {    /* worried about tlb flushing? */
					va = i386_ptob(spte - ptes);
					if (npte & PG_G) {
						/* PG_G requires this */
						pmap_update_pg(va);
					} else {
						if (prr->prr_npages <
						    PMAP_RR_MAX) {
							prr->prr_vas[
							    prr->prr_npages++] =
								va;
						} else {
						    if (prr->prr_npages ==
							PMAP_RR_MAX)
							/* signal an overflow */
							    prr->prr_npages++;
						}
					}
				}	/* if (prr) */
			}	/* npte != *spte */
		}	/* for loop */
d3063 1
a3063 21
	/*
	 * if we kept a removal record and removed some pages update the TLB
	 */

	if (prr && prr->prr_npages) {
#if defined(I386_CPU)
		if (cpu_class == CPUCLASS_386) {
			tlbflush();
		} else
#endif
		{ /* not I386 */
			if (prr->prr_npages > PMAP_RR_MAX) {
				tlbflush();
			} else {
				while (prr->prr_npages) {
					pmap_update_pg(prr->prr_vas[
						       --prr->prr_npages]);
				}
			}
		} /* not I386 */
	}
d3327 12
a3338 2
	if ((opte & ~(PG_M|PG_U)) != npte && pmap_is_curpmap(pmap))
		pmap_update_pg(va);
d3488 292
@


1.74
log
@as seen in freebsd:  asm pagezero implementations, but use a fn pointer.
the sse2 version cuts ZOD fault time in half.
suggestions mickey deraadt.  many many testers. ok deraadt.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2003/05/23 16:33:35 mpech Exp $	*/
d1884 3
@


1.73
log
@Do all unlocks before exit from pmap_enter().

deraadt@@, drahn@@ OK
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2003/05/13 03:49:04 art Exp $	*/
d2060 1
d2074 1
a2074 1
	bzero(zerop, NBPG);				/* zero */
d2094 1
a2094 1
	bzero(zerop, NBPG);				/* zero */
d2116 1
a2116 1
	memset(zerop, 0, NBPG);				/* zero */
@


1.72
log
@The current solution to handle the protection fault trap is not
correct.  It breaks down if we're trying to jump through a function
pointer. The protection fault trap on i386 must be one of the most
braindead traps ever invented in the history of humankind. It doesn't
give you any information about what went wrong except the instruction
that faulted. Since the problem we're trying to deal with is a
segmentation problem, we don't get the desitination that we want to
jump to, we just get the instruction and we won't add a disassembler
to trap handling just to try to figure out what went wrong.

What we want to do is to handle this as a normal fault to let noexec
accounting in pmap_enter deal with the changes to the code
segment. Unfortunately that's impossible. We don't know the faulting
address, so we need to change how the exec accounting works. Basically
the code segment must already cover the address we want to execute
before we can fault it in.

New scheme:

 o Start with conservative code segment.

 o If we get a protection fault, go through all mappings in the process
  and find the highest executable mapping, fix up the code segment and
  record that address. If the code segment didn't change, the protection
  fault wasn't fixable - just die.

 o If the highest executable mapping is removed, just reset the code
  segment to something conservative and let the next protection fault
  deal with it.  We can't read all the vm mappings of the process from
  the pmap because of locking hell.

This should allow floating code segment whenever someone implements that.

Also, fix the pmap_protect function to behave more like the other
pmaps we have and be slightly more agressive to force more proper
protection changes.

ok:ed by various people.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2003/05/09 23:51:23 art Exp $	*/
d3041 2
a3042 1
				return (KERN_RESOURCE_SHORTAGE);
@


1.71
log
@Map page tables non-exec.
deraadt@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2003/05/05 17:54:59 drahn Exp $	*/
d575 1
a575 1
pmap_nxstack_account(struct pmap *pmap, vaddr_t va,
d578 16
a593 2
	if (((opte ^ npte) & PG_X) &&
	    va < VM_MAXUSER_ADDRESS && va >= I386_MAX_EXE_ADDR) {
a594 1
		struct vm_map *map = &curproc->p_vmspace->vm_map;
d597 36
a632 15
		if (npte & PG_X && !(opte & PG_X)) {
			if (++pmap->pm_nxpages == 1 &&
			    pmap == vm_map_pmap(map)) {
				pcb->pcb_cs = tf->tf_cs =
				    GSEL(GUCODE1_SEL, SEL_UPL);
				pmap_update_pg(va);
			}
		} else {
			if (!--pmap->pm_nxpages &&
			    pmap == vm_map_pmap(map)) {
				pcb->pcb_cs = tf->tf_cs =
				    GSEL(GUCODE_SEL, SEL_UPL);
				pmap_update_pg(va);
			}
		}
d634 2
d1767 1
a1767 1
	pmap->pm_nxpages = 0;
d2282 1
a2282 1
	pmap_nxstack_account(pmap, va, opte, 0);
d2867 1
a2867 2
				/* account for executable pages on the stack */
				pmap_nxstack_account(pmap, sva, *spte, npte);
d3163 1
a3163 1
	pmap_nxstack_account(pmap, va, opte, npte);
@


1.70
log
@Move exec base to 0x1c000000, exe/data gap to 512MB. Allows better
interleave of exe/shared libs. Raise MAXDSIZ back to 1G.
This change REQUIRES a binary update on i386.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2003/05/02 21:07:47 mickey Exp $	*/
d1146 2
a1147 1
	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg), VM_PROT_ALL);
@


1.69
log
@when flipping the code descriptors also update cs in the
tss and not only in the frame since we might be returning that way too.
add a heuristic for detecting an exec protection fault:
iff we get a read protection fault (which we normally never
get due to our segments being always readable) we assume that
it was an exec protection indeed and go to page fault
routine which will decide the rest for us (including sending
a signal should that be needed).
problem found by drahn@@ and testing by many ppl.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2003/04/17 03:56:20 drahn Exp $	*/
d579 1
a579 1
	    va < VM_MAXUSER_ADDRESS && va >= 0x40000000) {
@


1.68
log
@A SEVERE hack given to me by mickey to draw the line in the sand at 1G.
memory by default will be executable, above, non executable. If memory
is requested to be exectable above 1G, this limit is relaxed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2003/04/09 12:11:15 niklas Exp $	*/
d582 1
d587 2
a588 1
				tf->tf_cs = GSEL(GUCODE1_SEL, SEL_UPL);
d594 2
a595 1
				tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
@


1.67
log
@Remove one of two identical comment blocks
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2003/04/07 06:14:30 niklas Exp $	*/
d579 1
a579 1
	    va < VM_MAXUSER_ADDRESS && va >= VM_MAXUSER_ADDRESS - MAXSSIZ) {
@


1.66
log
@Spring cleaning: remove unused code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2002/10/13 18:26:12 krw Exp $	*/
a3198 6

		/*
		 * THIS *MUST* BE CODED SO AS TO WORK IN THE
		 * pmap_initialized == FALSE CASE!  WE MAY BE
		 * INVOKED WHILE pmap_init() IS RUNNING!
		 */
@


1.65
log
@Remove more '\n's from panic() statements.  From Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2002/10/12 01:09:43 krw Exp $	*/
a406 5
static boolean_t	 pmap_transfer_ptes(struct pmap *,
					 struct pmap_transfer_location *,
					 struct pmap *,
					 struct pmap_transfer_location *,
					 int, boolean_t);
a2941 443
 * pmap_transfer: transfer (move or copy) mapping from one pmap
 * 	to another.
 *
 * => this function is optional, it doesn't have to do anything
 * => we assume that the mapping in the src pmap is valid (i.e. that
 *    it doesn't run off the end of the map's virtual space).
 * => we assume saddr, daddr, and len are page aligned/lengthed
 */

void
pmap_transfer(dstpmap, srcpmap, daddr, len, saddr, move)
	struct pmap *dstpmap, *srcpmap;
	vaddr_t daddr, saddr;
	vsize_t len;
	boolean_t move;
{
	/* base address of PTEs, dst could be NULL */
	pt_entry_t *srcptes, *dstptes;

	struct pmap_transfer_location srcl, dstl;
	int dstvalid;		  /* # of PTEs left in dst's current PTP */
	struct pmap *mapped_pmap; /* the pmap we passed to pmap_map_ptes */
	vsize_t blklen;
	int blkpgs, toxfer;
	boolean_t ok;

#ifdef DIAGNOSTIC
	/*
	 * sanity check: let's make sure our len doesn't overflow our dst
	 * space.
	 */

	if (daddr < VM_MAXUSER_ADDRESS) {
		if (VM_MAXUSER_ADDRESS - daddr < len) {
			printf("pmap_transfer: no room in user pmap "
			       "(addr=0x%lx, len=0x%lx)\n", daddr, len);
			return;
		}
	} else if (daddr < VM_MIN_KERNEL_ADDRESS ||
		   daddr >= VM_MAX_KERNEL_ADDRESS) {
		printf("pmap_transfer: invalid transfer address 0x%lx\n",
		       daddr);
	} else {
		if (VM_MAX_KERNEL_ADDRESS - daddr < len) {
			printf("pmap_transfer: no room in kernel pmap "
			       "(addr=0x%lx, len=0x%lx)\n", daddr, len);
			return;
		}
	}
#endif

	/*
	 * ideally we would like to have either src or dst pmap's be the
	 * current pmap so that we can map the other one in APTE space
	 * (if needed... one of the maps could be the kernel's pmap).
	 *
	 * however, if we can't get this, then we have to use the tmpmap
	 * (alternately we could punt).
	 */

	if (!pmap_is_curpmap(dstpmap) && !pmap_is_curpmap(srcpmap)) {
		dstptes = NULL;			/* dstptes NOT mapped */
		srcptes = pmap_map_ptes(srcpmap);   /* let's map the source */
		mapped_pmap = srcpmap;
	} else {
		if (!pmap_is_curpmap(srcpmap)) {
			srcptes = pmap_map_ptes(srcpmap);   /* possible APTE */
			dstptes = PTE_BASE;
			mapped_pmap = srcpmap;
		} else {
			dstptes = pmap_map_ptes(dstpmap);   /* possible APTE */
			srcptes = PTE_BASE;
			mapped_pmap = dstpmap;
		}
	}

	/*
	 * at this point we know that the srcptes are mapped.   the dstptes
	 * are mapped if (dstptes != NULL).    if (dstptes == NULL) then we
	 * will have to map the dst PTPs page at a time using the tmpmap.
	 * [XXX: is it worth the effort, or should we just punt?]
	 */

	srcl.addr = saddr;
	srcl.pte = &srcptes[i386_btop(srcl.addr)];
	srcl.ptp = NULL;
	dstl.addr = daddr;
	if (dstptes)
		dstl.pte = &dstptes[i386_btop(dstl.addr)];
	else
		dstl.pte  = NULL;		/* we map page at a time */
	dstl.ptp = NULL;
	dstvalid = 0;		/* force us to load a new dst PTP to start */

	while (len) {

		/*
		 * compute the size of this block.
		 */

		/* length in bytes */
		blklen = i386_round_pdr(srcl.addr+1) - srcl.addr;
		if (blklen > len)
			blklen = len;
		blkpgs = i386_btop(blklen);

		/*
		 * if the block is not valid in the src pmap,
		 * then we can skip it!
		 */

		if (!pmap_valid_entry(srcpmap->pm_pdir[pdei(srcl.addr)])) {
			len = len - blklen;
			srcl.pte  = srcl.pte + blkpgs;
			srcl.addr += blklen;
			dstl.addr += blklen;
			if (blkpgs > dstvalid) {
				dstvalid = 0;
				dstl.ptp = NULL;
			} else {
				dstvalid = dstvalid - blkpgs;
			}
			if (dstptes == NULL && (len == 0 || dstvalid == 0)) {
				if (dstl.pte) {
					pmap_tmpunmap_pa();
					dstl.pte = NULL;
				}
			} else {
				dstl.pte += blkpgs;
			}
			continue;
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs to transfer.
		 * if we don't have any dst PTEs ready, then get some.
		 */

		if (dstvalid == 0) {
			if (!pmap_valid_entry(dstpmap->
					      pm_pdir[pdei(dstl.addr)])) {
#ifdef DIAGNOSTIC
				if (dstl.addr >= VM_MIN_KERNEL_ADDRESS)
					panic("pmap_transfer: missing kernel "
					      "PTP at 0x%lx", dstl.addr);
#endif
				dstl.ptp = pmap_get_ptp(dstpmap,
							pdei(dstl.addr), TRUE);
				if (dstl.ptp == NULL)	/* out of RAM?  punt. */
					break;
			} else {
				dstl.ptp = NULL;
			}
			dstvalid = i386_btop(i386_round_pdr(dstl.addr+1) -
					     dstl.addr);
			if (dstptes == NULL) {
				dstl.pte = (pt_entry_t *)
					pmap_tmpmap_pa(dstpmap->
						       pm_pdir[pdei(dstl.addr)]
						       & PG_FRAME);
				dstl.pte = dstl.pte + (PTES_PER_PTP - dstvalid);
			}
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs to transfer.
		 * we have a valid dst block of "dstvalid" PTEs ready.
		 * thus we can transfer min(blkpgs, dstvalid) PTEs now.
		 */

		srcl.ptp = NULL;	/* don't know source PTP yet */
		if (dstvalid < blkpgs)
			toxfer = dstvalid;
		else
			toxfer = blkpgs;

		if (toxfer > 0) {
			ok = pmap_transfer_ptes(srcpmap, &srcl, dstpmap, &dstl,
						toxfer, move);

			if (!ok)		/* memory shortage?  punt. */
				break;

			dstvalid -= toxfer;
			blkpgs -= toxfer;
			len -= i386_ptob(toxfer);
			if (blkpgs == 0)	/* out of src PTEs?  restart */
				continue;
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs left
		 * to transfer.  we have just used up our "dstvalid"
		 * PTEs, and thus must obtain more dst PTEs to finish
		 * off the src block.  since we are now going to
		 * obtain a brand new dst PTP, we know we can finish
		 * the src block in one more transfer.
		 */

#ifdef DIAGNOSTIC
		if (dstvalid)
			panic("pmap_transfer: dstvalid non-zero after drain");
		if ((dstl.addr & (NBPD-1)) != 0)
			panic("pmap_transfer: dstaddr not on PD boundary "
			      "(0x%lx)", dstl.addr);
#endif

		if (dstptes == NULL && dstl.pte != NULL) {
			/* dispose of old PT mapping */
			pmap_tmpunmap_pa();
			dstl.pte = NULL;
		}

		/*
		 * get new dst PTP
		 */
		if (!pmap_valid_entry(dstpmap->pm_pdir[pdei(dstl.addr)])) {
#ifdef DIAGNOSTIC
			if (dstl.addr >= VM_MIN_KERNEL_ADDRESS)
				panic("pmap_transfer: missing kernel PTP at "
				      "0x%lx", dstl.addr);
#endif
			dstl.ptp = pmap_get_ptp(dstpmap, pdei(dstl.addr), TRUE);
			if (dstl.ptp == NULL)	/* out of free RAM?  punt. */
				break;
		} else {
			dstl.ptp = NULL;
		}

		dstvalid = PTES_PER_PTP;	/* new PTP */

		/*
		 * if the dstptes are un-mapped, then we need to tmpmap in the
		 * dstl.ptp.
		 */

		if (dstptes == NULL) {
			dstl.pte = (pt_entry_t *)
				pmap_tmpmap_pa(dstpmap->pm_pdir[pdei(dstl.addr)]
					       & PG_FRAME);
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs left
		 * to transfer.  we just got a brand new dst PTP to
		 * receive these PTEs.
		 */

#ifdef DIAGNOSTIC
		if (dstvalid < blkpgs)
			panic("pmap_transfer: too many blkpgs?");
#endif
		toxfer = blkpgs;
		ok = pmap_transfer_ptes(srcpmap, &srcl, dstpmap, &dstl, toxfer,
					move);

		if (!ok)		/* memory shortage?   punt. */
			break;

		dstvalid -= toxfer;
		blkpgs -= toxfer;
		len -= i386_ptob(toxfer);

		/*
		 * done src pte block
		 */
	}
	if (dstptes == NULL && dstl.pte != NULL)
		pmap_tmpunmap_pa();		/* dst PTP still mapped? */
	pmap_unmap_ptes(mapped_pmap);
}

/*
 * pmap_transfer_ptes: transfer PTEs from one pmap to another
 *
 * => we assume that the needed PTPs are mapped and that we will
 *	not cross a block boundary.
 * => we return TRUE if we transfered all PTEs, FALSE if we were
 *	unable to allocate a pv_entry
 */

static boolean_t
pmap_transfer_ptes(srcpmap, srcl, dstpmap, dstl, toxfer, move)
	struct pmap *srcpmap, *dstpmap;
	struct pmap_transfer_location *srcl, *dstl;
	int toxfer;
	boolean_t move;
{
	pt_entry_t dstproto, opte;
	int bank, off;
	struct pv_head *pvh;
	struct pv_entry *pve, *lpve;

	/*
	 * generate "prototype" dst PTE
	 */

	if (dstl->addr < VM_MAX_ADDRESS)
		dstproto = PG_u;		/* "user" page */
	else
		dstproto = pmap_pg_g;	/* kernel page */

	/*
	 * ensure we have dst PTP for user addresses.
	 */

	if (dstl->ptp == NULL && dstl->addr < VM_MAXUSER_ADDRESS)
		dstl->ptp = PHYS_TO_VM_PAGE(dstpmap->pm_pdir[pdei(dstl->addr)] &
					    PG_FRAME);

	/*
	 * main loop over range
	 */

	for (/*null*/; toxfer > 0 ; toxfer--,
			     srcl->addr += NBPG, dstl->addr += NBPG,
			     srcl->pte++, dstl->pte++) {

		if (!pmap_valid_entry(*srcl->pte))  /* skip invalid entrys */
			continue;

#ifdef DIAGNOSTIC
		if (pmap_valid_entry(*dstl->pte))
			panic("pmap_transfer_ptes: attempt to overwrite "
			      "active entry");
#endif

		/*
		 * let's not worry about non-pvlist mappings (typically device
		 * pager mappings).
		 */

		opte = *srcl->pte;

		if ((opte & PG_PVLIST) == 0)
			continue;

		/*
		 * if we are moving the mapping, then we can just adjust the
		 * current pv_entry.    if we are copying the mapping, then we
		 * need to allocate a new pv_entry to account for it.
		 */

		if (move == FALSE) {
			pve = pmap_alloc_pv(dstpmap, ALLOCPV_TRY);
			if (pve == NULL)
				return(FALSE); 		/* punt! */
		} else {
			pve = NULL;  /* XXX: quiet gcc warning */
		}

		/*
		 * find the pv_head for this mapping.  since our mapping is
		 * on the pvlist (PG_PVLIST), there must be a pv_head.
		 */

		bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
		if (bank == -1)
			panic("pmap_transfer_ptes: PG_PVLIST PTE and "
			      "no pv_head!");
#endif
		pvh = &vm_physmem[bank].pmseg.pvhead[off];

		/*
		 * now lock down the pvhead and find the current entry (there
		 * must be one).
		 */

		simple_lock(&pvh->pvh_lock);
		for (lpve = pvh->pvh_list ; lpve ; lpve = lpve->pv_next)
			if (lpve->pv_pmap == srcpmap &&
			    lpve->pv_va == srcl->addr)
				break;
#ifdef DIAGNOSTIC
		if (lpve == NULL)
			panic("pmap_transfer_ptes: PG_PVLIST PTE, but "
			      "entry not found");
#endif

		/*
		 * update src ptp.   if the ptp is null in the pventry, then
		 * we are not counting valid entrys for this ptp (this is only
		 * true for kernel PTPs).
		 */

		if (srcl->ptp == NULL)
			srcl->ptp = lpve->pv_ptp;
#ifdef DIAGNOSTIC
		if (srcl->ptp &&
		    (srcpmap->pm_pdir[pdei(srcl->addr)] & PG_FRAME) !=
		    VM_PAGE_TO_PHYS(srcl->ptp))
			panic("pmap_transfer_ptes: pm_pdir - pv_ptp mismatch!");
#endif

		/*
		 * for move, update the pve we just found (lpve) to
		 * point to its new mapping.  for copy, init the new
		 * pve and put it in the list.
		 */

		if (move == TRUE) {
			pve = lpve;
		}
		pve->pv_pmap = dstpmap;
		pve->pv_va = dstl->addr;
		pve->pv_ptp = dstl->ptp;
		if (move == FALSE) {		/* link in copy */
			pve->pv_next = lpve->pv_next;
			lpve->pv_next = pve;
		}

		/*
		 * sync the R/M bits while we are here.
		 */

		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));

		/*
		 * now actually update the ptes and unlock the pvlist.
		 */

		if (move) {
			*srcl->pte = 0;		/* zap! */
			if (pmap_is_curpmap(srcpmap))
				pmap_update_pg(srcl->addr);
			if (srcl->ptp)
				/* don't bother trying to free PTP */
				srcl->ptp->wire_count--;
			srcpmap->pm_stats.resident_count--;
			if (opte & PG_W)
				srcpmap->pm_stats.wired_count--;
		}
		*dstl->pte = (opte & ~(PG_u|PG_U|PG_M|PG_G|PG_W)) | dstproto;
		dstpmap->pm_stats.resident_count++;
		if (dstl->ptp)
			dstl->ptp->wire_count++;
		simple_unlock(&pvh->pvh_lock);
	}
	return(TRUE);
}

/*
d2949 1
a2949 1
 * defined as macro call in pmap.h
@


1.64
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2002/09/11 22:39:00 art Exp $	*/
d3151 1
a3151 1
			      "(0x%lx)\n", dstl.addr);
@


1.63
log
@Some early callers of pmap_growkernel call it before vm_page structures
are initialized. So we can't to PHYS_TO_VM_PAGE becuase there are no
vm_pages. Reintroduce the old pmap_zero_page renamed to pmap_zero_phys
that can zero pages without struct vm_page.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2002/09/10 18:29:43 art Exp $	*/
d658 1
a658 1
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx\n",
@


1.62
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2002/07/31 07:21:04 aaron Exp $	*/
d420 2
d2045 20
d3638 1
a3638 1
			pmap_zero_page(PHYS_TO_VM_PAGE(ptaddr));
@


1.61
log
@Remove mickey's debugging printf goop.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2002/07/31 02:30:29 mickey Exp $	*/
d1538 1
a1538 1
		pmap_zero_page(VM_PAGE_TO_PHYS(ptp));
d2025 1
a2025 2
pmap_zero_page(pa)
	paddr_t pa;
d2027 2
d2071 1
a2071 2
pmap_copy_page(srcpa, dstpa)
	paddr_t srcpa, dstpa;
d2073 3
d3616 1
a3616 1
			pmap_zero_page(ptaddr);
@


1.60
log
@support for changing stack execution protection through mprotect()
by emulating the page execution protection bit and accounting
for pages mapped executable on the stack and swapping the
global user code descriptors for the process accordingly.
this is tested w/ the regress test and art@@ looked over it.

there is still a mistery how executable mappings on fault
works on i386 since no prot_exec faults ever happen.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2002/03/14 01:26:33 millert Exp $	*/
d585 1
a585 1
if (pmap != pmap_kernel()) printf("nxpages=%d npte=%x opte=%x", pmap->pm_nxpages, npte & 0xfff, opte & 0xfff);
a598 1
if (pmap != pmap_kernel()) printf("nxpages1=%d ", pmap->pm_nxpages);
@


1.59
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2002/01/23 00:39:47 art Exp $	*/
d577 26
d735 8
a742 8
	protection_codes[VM_PROT_NONE] = 0;  			/* --- */
	protection_codes[VM_PROT_EXECUTE] = PG_RO;		/* --x */
	protection_codes[VM_PROT_READ] = PG_RO;			/* -r- */
	protection_codes[VM_PROT_READ|VM_PROT_EXECUTE] = PG_RO;	/* -rx */
	protection_codes[VM_PROT_WRITE] = PG_RW;		/* w-- */
	protection_codes[VM_PROT_WRITE|VM_PROT_EXECUTE] = PG_RW;/* w-x */
	protection_codes[VM_PROT_WRITE|VM_PROT_READ] = PG_RW;	/* wr- */
	protection_codes[VM_PROT_ALL] = PG_RW;			/* wrx */
d1731 1
d2223 2
d2800 1
a2800 1
		for (/*null */; spte < epte ; spte++) {
d2808 3
d3548 1
@


1.58
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2001/12/19 08:58:05 art Exp $	*/
d376 3
a378 3
static struct pv_entry	*pmap_add_pvpage __P((struct pv_page *, boolean_t));
static struct vm_page	*pmap_alloc_ptp __P((struct pmap *, int, boolean_t));
static struct pv_entry	*pmap_alloc_pv __P((struct pmap *, int)); /* see codes below */
d382 2
a383 2
static struct pv_entry	*pmap_alloc_pvpage __P((struct pmap *, int));
static void		 pmap_enter_pv __P((struct pv_head *,
d385 13
a397 13
					    vaddr_t, struct vm_page *));
static void		 pmap_free_pv __P((struct pmap *, struct pv_entry *));
static void		 pmap_free_pvs __P((struct pmap *, struct pv_entry *));
static void		 pmap_free_pv_doit __P((struct pv_entry *));
static void		 pmap_free_pvpage __P((void));
static struct vm_page	*pmap_get_ptp __P((struct pmap *, int, boolean_t));
static boolean_t	 pmap_is_curpmap __P((struct pmap *));
static pt_entry_t	*pmap_map_ptes __P((struct pmap *));
static struct pv_entry	*pmap_remove_pv __P((struct pv_head *, struct pmap *,
					     vaddr_t));
static boolean_t	 pmap_remove_pte __P((struct pmap *, struct vm_page *,
					      pt_entry_t *, vaddr_t));
static void		 pmap_remove_ptes __P((struct pmap *,
d400 8
a407 8
					       vaddr_t, vaddr_t));
static struct vm_page	*pmap_steal_ptp __P((struct uvm_object *,
					     vaddr_t));
static vaddr_t		 pmap_tmpmap_pa __P((paddr_t));
static pt_entry_t	*pmap_tmpmap_pvepte __P((struct pv_entry *));
static void		 pmap_tmpunmap_pa __P((void));
static void		 pmap_tmpunmap_pvepte __P((struct pv_entry *));
static boolean_t	 pmap_transfer_ptes __P((struct pmap *,
d411 2
a412 2
					 int, boolean_t));
static boolean_t	 pmap_try_steal_pv __P((struct pv_head *,
d414 2
a415 2
						struct pv_entry *));
static void		pmap_unmap_ptes __P((struct pmap *));
d417 2
a418 2
void			pmap_pinit __P((pmap_t));
void			pmap_release __P((pmap_t));
d3628 1
a3628 1
void pmap_dump __P((struct pmap *, vaddr_t, vaddr_t));
@


1.57
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2001/11/07 02:55:50 art Exp $	*/
d840 1
a840 1
		  0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
@


1.56
log
@Yet another sync to NetBSD. Kills lot of unnecessary code and cleans up a bit.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.55 2001/12/11 17:24:34 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.120 2001/04/22 23:42:14 thorpej Exp $	*/
a292 9
#ifdef LARGEPAGES
/*
 * pmap_largepages: if our processor supports PG_PS and we are
 * using it, this is set to TRUE.
 */

int pmap_largepages;
#endif

d301 2
d342 1
a350 9
 * pool and cache that PDPs are allocated from
 */

struct pool pmap_pdp_pool;
struct pool_cache pmap_pdp_cache;

int	pmap_pdp_ctor(void *, void *, int);

/*
d377 1
a377 1
static struct vm_page	*pmap_alloc_ptp __P((struct pmap *, int));
d390 1
a390 1
static struct vm_page	*pmap_get_ptp __P((struct pmap *, int));
a394 2
static void		 pmap_do_remove __P((struct pmap *, vaddr_t,
						vaddr_t, int));
d396 1
a396 1
					      pt_entry_t *, vaddr_t, int));
d400 3
a402 3
					       vaddr_t, vaddr_t, int));
#define PMAP_REMOVE_ALL		0	/* remove all mappings */
#define PMAP_REMOVE_SKIPWIRED	1	/* skip wired mappings */
d407 8
d417 3
d600 1
a600 4
	if (va < VM_MIN_KERNEL_ADDRESS)
		pte = vtopte(va);
	else
		pte = kvtopte(va);
a601 5
#ifdef LARGEPAGES
	/* XXX For now... */
	if (opte & PG_PS)
		panic("pmap_kenter_pa: PG_PS");
#endif
d627 2
a628 10
	for ( /* null */ ; len ; len--, va += PAGE_SIZE) {
		if (va < VM_MIN_KERNEL_ADDRESS)
			pte = vtopte(va);
		else
			pte = kvtopte(va);
#ifdef LARGEPAGES
		/* XXX For now... */
		if (*pte & PG_PS)
			panic("pmap_kremove: PG_PS");
#endif
a762 38
#ifdef LARGEPAGES
	/*
	 * enable large pages of they are supported.
	 */

	if (cpu_feature & CPUID_PSE) {
		paddr_t pa;
		vaddr_t kva_end;
		pd_entry_t *pde;
		extern char _etext;

		lcr4(rcr4() | CR4_PSE);	/* enable hardware (via %cr4) */
		pmap_largepages = 1;	/* enable software */

		/*
		 * the TLB must be flushed after enabling large pages
		 * on Pentium CPUs, according to section 3.6.2.2 of
		 * "Intel Architecture Software Developer's Manual,
		 * Volume 3: System Programming".
		 */
		tlbflush();

		/*
		 * now, remap the kernel text using large pages.  we
		 * assume that the linker has properly aligned the
		 * .data segment to a 4MB boundary.
		 */
		kva_end = roundup((vaddr_t)&_etext, NBPD);
		for (pa = 0, kva = KERNBASE; kva < kva_end;
		     kva += NBPD, pa += NBPD) {
			pde = &kpm->pm_pdir[pdei(kva)];
			*pde = pa | pmap_pg_g | PG_PS |
			    PG_KR | PG_V;	/* zap! */
			tlbflush();
		}
	}
#endif /* LARGEPAGES */

d842 9
a850 2
  	/*
	 * initialize the PDE pool and cache.
d853 24
a876 4
	pool_init(&pmap_pdp_pool, PAGE_SIZE, 0, 0, 0, "pdppl",
		  0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
	pool_cache_init(&pmap_pdp_cache, &pmap_pdp_pool,
			pmap_pdp_ctor, NULL, NULL);
d995 2
a996 2
	pvpage = TAILQ_FIRST(&pv_freepages);
	if (pvpage != NULL) {
d1049 2
a1050 2
	struct pv_entry *pv;
	int s;
d1056 1
a1056 2
	pvpage = TAILQ_FIRST(&pv_unusedpgs);
	if (mode != ALLOCPV_NONEED && pvpage != NULL) {
d1059 1
d1081 1
a1081 1
	s = splvm();   /* must protect kmem_map/kmem_object with splvm! */
d1084 1
a1084 1
		    PAGE_SIZE, UVM_KMF_TRYLOCK|UVM_KMF_VALLOC);
d1087 1
a1087 1
			return (NULL);
d1093 1
a1093 1
	 * note: we are still holding splvm to protect kmem_object
d1098 1
a1098 1
		return (NULL);
d1109 1
a1109 1
	/* splvm now dropped */
d1112 1
a1112 1
		return (NULL);
d1121 2
a1122 4
	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg),
	    VM_PROT_READ|VM_PROT_WRITE);
	pmap_update(pmap_kernel());
	pvpage = (struct pv_page *)pv_cachedva;
d1124 104
a1227 1
	return (pmap_add_pvpage(pvpage, mode != ALLOCPV_NONEED));
d1488 5
d1496 1
a1496 1
pmap_alloc_ptp(pmap, pde_index)
d1499 1
d1505 10
a1514 2
	if (ptp == NULL)
		return(NULL);
d1527 105
d1639 1
a1639 1
pmap_get_ptp(pmap, pde_index)
d1642 1
d1664 1
a1664 1
	return(pmap_alloc_ptp(pmap, pde_index));
a1671 35
 * pmap_pdp_ctor: constructor for the PDP cache.
 */

int
pmap_pdp_ctor(void *arg, void *object, int flags)
{
	pd_entry_t *pdir = object;
	paddr_t pdirpa;

	/*
	 * NOTE: The `pmap_lock' is held when the PDP is allocated.
	 * WE MUST NOT BLOCK!
	 */

	/* fetch the physical address of the page directory. */
	(void) pmap_extract(pmap_kernel(), (vaddr_t) pdir, &pdirpa);

	/* zero init area */
	memset(pdir, 0, PDSLOT_PTE * sizeof(pd_entry_t));

	/* put in recursibve PDE to map the PTEs */
	pdir[PDSLOT_PTE] = pdirpa | PG_V | PG_KW;

	/* put in kernel VM PDEs */
	memcpy(&pdir[PDSLOT_KERN], &PDP_BASE[PDSLOT_KERN],
	    nkpde * sizeof(pd_entry_t));

	/* zero the rest */
	memset(&pdir[PDSLOT_KERN + nkpde], 0,
	    PAGE_SIZE - ((PDSLOT_KERN + nkpde) * sizeof(pd_entry_t)));

	return (0);
}

/*
d1684 3
d1688 8
d1707 13
a1724 2
	/* allocate PDP */

d1727 3
a1729 5
	 * us.  note that there is no need to splvm to protect us from
	 * malloc since malloc allocates out of a submap and we should
	 * have already allocated kernel PTPs to cover the range...
	 *
	 * NOTE: WE MUST NOT BLOCK WHILE HOLDING THE `pmap_lock'!
d1732 6
a1737 8

	/* XXX Need a generic "I want memory" wchan */
	while ((pmap->pm_pdir =
	    pool_cache_get(&pmap_pdp_cache, PR_NOWAIT)) == NULL)
		(void) ltsleep(&lbolt, PVM, "pmapcr", hz >> 3, &pmaps_lock);

	pmap->pm_pdirpa = pmap->pm_pdir[PDSLOT_PTE] & PG_FRAME;

a1738 1

a1739 2

	return (pmap);
a1750 1
	struct vm_page *pg;
d1768 19
d1792 2
d1801 2
a1802 1
	while ((pg = TAILQ_FIRST(&pmap->pm_obj.memq)) != NULL) {
d1814 1
a1814 1
	pool_cache_put(&pmap_pdp_cache, pmap->pm_pdir);
a1826 2

	pool_put(&pmap_pmap_pool, pmap);
d1966 2
a1967 11
	pt_entry_t *ptes, pte;
	pd_entry_t pde;

	if (__predict_true((pde = pmap->pm_pdir[pdei(va)]) != 0)) {
#ifdef LARGEPAGES
		if (pde & PG_PS) {
			if (pap != NULL)
				*pap = (pde & PG_LGFRAME) | (va & ~PG_LGFRAME);
			return (TRUE);
		}
#endif
d1969 1
d1971 1
a1971 1
		pte = ptes[i386_btop(va)];
d1973 3
a1975 6

		if (__predict_true((pte & PG_V) != 0)) {
			if (pap != NULL)
				*pap = (pte & PG_FRAME) | (va & ~PG_FRAME);
			return (TRUE);
		}
a1979 18
#ifdef LARGEPAGES
/*
 * vtophys: virtual address to physical address.  For use by
 * machine-dependent code only.
 */

paddr_t
vtophys(va)
	vaddr_t va;
{
	paddr_t pa;

	if (pmap_extract(pmap_kernel(), va, &pa) == TRUE)
		return (pa);
	return (0);
}
#endif

d2002 5
a2007 1
	simple_lock(&pmap_zero_page_lock);
d2009 2
a2010 1
	bzero(zerop, PAGE_SIZE);				/* zero */
a2022 3
	int i, *ptr;
	boolean_t rv = TRUE;

d2024 4
d2031 3
a2033 15
	pmap_update_pg((vaddr_t)zerop);
	for (i = 0, ptr = (int *) zerop; i < PAGE_SIZE / sizeof(int); i++) {
		if (whichqs != 0) {
			/*
			 * A process has become ready.  Abort now,
			 * so we don't keep it waiting while we
			 * do slow memory access to finish this
			 * page.
			 */
			rv = FALSE;
			break;
		}
		*ptr++ = 0;
	}

d2036 1
a2036 1
	return (rv);
d2077 1
a2077 1
pmap_remove_ptes(pmap, pmap_rr, ptp, ptpva, startva, endva, flags)
a2082 1
	int flags;
d2100 1
a2100 1
			     ; pte++, startva += PAGE_SIZE) {
a2102 3
		if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W)) {
			continue;
		}
d2180 1
a2180 1
pmap_remove_pte(pmap, ptp, pte, va, flags)
a2184 1
	int flags;
a2191 3
	if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W)) {
		return(FALSE);
	}
a2248 15
	pmap_do_remove(pmap, sva, eva, PMAP_REMOVE_ALL);
}

/*
 * pmap_do_remove: mapping removal guts
 *
 * => caller should not be holding any pmap locks
 */

static void
pmap_do_remove(pmap, sva, eva, flags)
	struct pmap *pmap;
	vaddr_t sva, eva;
	int flags;
{
d2296 1
a2296 1
			    &ptes[i386_btop(sva)], sva, flags);
d2391 1
a2391 1
		    (vaddr_t)&ptes[i386_btop(sva)], sva, blkendva, flags);
d2888 443
a3330 1
	pmap_update(pmap);
d3391 1
a3391 1
		ptp = pmap_get_ptp(pmap, pdei(va));
d3394 1
a3394 1
				return ENOMEM;
d3494 1
a3494 1
					error = ENOMEM;
d3603 1
a3603 1
		if (pmap_alloc_ptp(kpm, PDSLOT_KERN + nkpde) == NULL) {
d3612 2
a3613 1
		LIST_FOREACH(pm, &pmaps, pm_list) {
a3616 4

		/* Invalidate the PDP cache. */
		pool_cache_invalidate(&pmap_pdp_cache);

d3675 1
a3675 1
		for (/* null */; sva < blkendva ; sva += PAGE_SIZE, pte++) {
@


1.56.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2002/01/23 00:39:47 art Exp $	*/
d900 10
a909 1
	    &pool_allocator_nointr);
@


1.56.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56.2.1 2002/01/31 22:55:11 niklas Exp $	*/
d1105 1
d1107 2
a1108 4
		s = splvm();   /* must protect kmem_map with splvm! */
		pv_cachedva = uvm_km_kmemalloc(kmem_map, NULL,
		    NBPG, UVM_KMF_TRYLOCK|UVM_KMF_VALLOC);
		splx(s);
d1110 1
d1115 20
a1134 2
	pg = uvm_pagealloc(NULL, pv_cachedva - vm_map_min(kernel_map), NULL,
	    UVM_PGA_USERESERVE);
d1285 3
d1326 1
d1495 1
a1495 1
	/* put in recursive PDE to map the PTEs */
d2339 3
@


1.56.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56.2.2 2002/02/02 03:28:25 art Exp $	*/
d391 3
a393 3
static struct pv_entry	*pmap_add_pvpage(struct pv_page *, boolean_t);
static struct vm_page	*pmap_alloc_ptp(struct pmap *, int);
static struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
d397 2
a398 2
static struct pv_entry	*pmap_alloc_pvpage(struct pmap *, int);
static void		 pmap_enter_pv(struct pv_head *,
d400 18
a417 17
					    vaddr_t, struct vm_page *);
static void		 pmap_free_pv(struct pmap *, struct pv_entry *);
static void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
static void		 pmap_free_pv_doit(struct pv_entry *);
static void		 pmap_free_pvpage(void);
static struct vm_page	*pmap_get_ptp(struct pmap *, int);
static boolean_t	 pmap_is_curpmap(struct pmap *);
static pt_entry_t	*pmap_map_ptes(struct pmap *);
static struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
					    vaddr_t);
static void		 pmap_do_remove(struct pmap *, vaddr_t, vaddr_t, int);
static boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *,
					    pt_entry_t *, vaddr_t, int);
static void		 pmap_remove_ptes(struct pmap *,
					    struct pmap_remove_record *,
					    struct vm_page *, vaddr_t,
					    vaddr_t, vaddr_t, int);
d420 5
a424 5
static vaddr_t		 pmap_tmpmap_pa(paddr_t);
static pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
static void		 pmap_tmpunmap_pa(void);
static void		 pmap_tmpunmap_pvepte(struct pv_entry *);
static void		pmap_unmap_ptes(struct pmap *);
a902 9
	 * initialize the PDE pool and cache.
	 */

	pool_init(&pmap_pdp_pool, PAGE_SIZE, 0, 0, 0, "pdppl",
	    &pool_allocator_nointr);
	pool_cache_init(&pmap_pdp_cache, &pmap_pdp_pool,
	    pmap_pdp_ctor, NULL, NULL);

	/*
a1118 1
	pg->flags &= ~PG_BUSY;
d3026 1
a3026 1
void pmap_dump(struct pmap *, vaddr_t, vaddr_t);
@


1.56.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56.2.3 2002/06/11 03:35:53 art Exp $	*/
a424 2
void			pmap_zero_phys(paddr_t);

a581 25
__inline static void
pmap_nxstack_account(struct pmap *pmap, vaddr_t va,
    pt_entry_t opte, pt_entry_t npte)
{
	if (((opte ^ npte) & PG_X) &&
	    va < VM_MAXUSER_ADDRESS && va >= VM_MAXUSER_ADDRESS - MAXSSIZ) {
		struct trapframe *tf = curproc->p_md.md_regs;
		struct vm_map *map = &curproc->p_vmspace->vm_map;

		if (npte & PG_X && !(opte & PG_X)) {
			if (++pmap->pm_nxpages == 1 &&
			    pmap == vm_map_pmap(map)) {
				tf->tf_cs = GSEL(GUCODE1_SEL, SEL_UPL);
				pmap_update_pg(va);
			}
		} else {
			if (!--pmap->pm_nxpages &&
			    pmap == vm_map_pmap(map)) {
				tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
				pmap_update_pg(va);
			}
		}
	}
}

d652 1
a652 1
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx",
d730 8
a737 8
	protection_codes[UVM_PROT_NONE] = 0;  			/* --- */
	protection_codes[UVM_PROT_EXEC] = PG_X;			/* --x */
	protection_codes[UVM_PROT_READ] = PG_RO;		/* -r- */
	protection_codes[UVM_PROT_RX] = PG_X;			/* -rx */
	protection_codes[UVM_PROT_WRITE] = PG_RW;		/* w-- */
	protection_codes[UVM_PROT_WX] = PG_RW|PG_X;		/* w-x */
	protection_codes[UVM_PROT_RW] = PG_RW;			/* wr- */
	protection_codes[UVM_PROT_RWX] = PG_RW|PG_X;		/* wrx */
a1518 1
	pmap->pm_nxpages = 0;
d1819 2
a1820 23
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

	simple_lock(&pmap_zero_page_lock);
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page: lock botch");
#endif

	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	bzero(zerop, NBPG);				/* zero */
	*zero_pte = 0;				/* zap! */
	pmap_update_pg((vaddr_t)zerop);		/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
}

/*
 * pmap_zero_phys: same as pmap_zero_page, but for use before vm_pages are
 * initialized.
 */
void
pmap_zero_phys(paddr_t pa)
d1870 2
a1871 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a1872 3
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);

a2029 2
	pmap_nxstack_account(pmap, va, opte, 0);

d2617 1
a2617 1
		for (/*null */; spte < epte ; spte++, sva += PAGE_SIZE) {
a2624 3
				/* account for executable pages on the stack */
				pmap_nxstack_account(pmap, sva, *spte, npte);

a2919 1
	pmap_nxstack_account(pmap, va, opte, npte);
d2985 1
a2985 1
			pmap_zero_phys(ptaddr);
@


1.56.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d585 1
a585 1
pmap_exec_account(struct pmap *pm, vaddr_t va,
d588 2
a589 16
	if (curproc == NULL || curproc->p_vmspace == NULL ||
	    pm != vm_map_pmap(&curproc->p_vmspace->vm_map))
		return;

	if ((opte ^ npte) & PG_X)
		pmap_update_pg(va);
		
	/*
	 * Executability was removed on the last executable change.
	 * Reset the code segment to something conservative and
	 * let the trap handler deal with setting the right limit.
	 * We can't do that because of locking constraints on the vm map.
	 *
	 * XXX - floating cs - set this _really_ low.
	 */
	if ((opte & PG_X) && (npte & PG_X) == 0 && va == pm->pm_hiexec) {
d591 1
a591 1
		struct pcb *pcb = &curproc->p_addr->u_pcb;
d593 13
a605 36
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
		pm->pm_hiexec = I386_MAX_EXE_ADDR;
	}
}

/*
 * Fixup the code segment to cover all potential executable mappings.
 * returns 0 if no changes to the code segment were made.
 */
int
pmap_exec_fixup(struct vm_map *map, struct trapframe *tf, struct pcb *pcb)
{
	struct vm_map_entry *ent;
	struct pmap *pm = vm_map_pmap(map);
	vaddr_t va = 0;

	vm_map_lock(map);
	for (ent = (&map->header)->next; ent != &map->header; ent = ent->next) {
		/*
		 * This entry has greater va than the entries before.
		 * We need to make it point to the last page, not past it.
		 */
		if (ent->protection & VM_PROT_EXECUTE)
			va = trunc_page(ent->end) - PAGE_SIZE;
	}
	vm_map_unlock(map);

	if (va == pm->pm_hiexec)
		return (0);

	pm->pm_hiexec = va;

	if (pm->pm_hiexec > (vaddr_t)I386_MAX_EXE_ADDR) {
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE1_SEL, SEL_UPL);
	} else {
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
a606 2
	
	return (1);
d1546 1
a1546 1
	pmap->pm_hiexec = 0;
d2081 1
a2081 1
	pmap_exec_account(pmap, va, opte, 0);
d2678 2
a2679 1
				pmap_exec_account(pmap, sva, *spte, npte);
d2801 1
a2801 1
 * defined as macro in pmap.h
d2976 1
a2976 1
	pmap_exec_account(pmap, va, opte, npte);
d3051 6
@


1.55
log
@More sync to NetBSD.
 - don't remove wired mappings in pmap_collect.
 - some support for large pages.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.54 2001/12/11 16:35:18 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.107 2000/09/21 21:43:24 thorpej Exp $	*/
a62 7
#ifdef __NetBSD__
#include "opt_cputype.h"
#include "opt_user_ldt.h"
#include "opt_lockdebug.h"
#include "opt_multiprocessor.h"
#endif

d69 1
d141 2
a142 2
 *	- plan 1: done at pmap_pinit() we use
 *	  uvm_km_alloc(kernel_map, NBPG)  [fka kmem_alloc] to do this
d157 1
a157 1
 * 	- plan 1: call uvm_pagealloc()
d159 2
a160 8
 * 		=> failure: we are out of free vm_pages
 * 	- plan 2: using a linked LIST of active pmaps we attempt
 * 	to "steal" a PTP from another process.   we lock
 * 	the target pmap with simple_lock_try so that if it is
 * 	busy we do not block.
 * 		=> success: remove old mappings, zero, add to pm_pdir
 * 		=> failure: highly unlikely
 * 	- plan 3: panic
d183 1
a183 8
 *	- plan 3: using the pv_entry/pv_head lists find a pv_entry
 *		structure that is part of a non-kernel lockable pmap
 *		and "steal" that pv_entry by removing the mapping
 *		and reusing that pv_entry.
 *		=> success: done
 *		=> failure: highly unlikely: unable to lock and steal
 *			pv_entry
 *	- plan 4: we panic.
a244 1
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
d246 3
a248 2
#define spinlockinit(lock, name, flags)  lockinit(lock, 0, name, 0, flags)
#define spinlockmgr(lock, flags, slock) lockmgr(lock, flags, slock, curproc)
a267 10
#else

#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */

#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

#endif

a309 2
paddr_t hole_start;	/* PA of start of "hole" */
paddr_t hole_end;	/* PA of end of "hole" */
a348 1
static struct pmap *pmaps_hand = NULL;	/* used by pmap_steal_ptp */
d357 9
d392 1
a392 1
static struct vm_page	*pmap_alloc_ptp __P((struct pmap *, int, boolean_t));
d405 1
a405 1
static struct vm_page	*pmap_get_ptp __P((struct pmap *, int, boolean_t));
a419 2
static struct vm_page	*pmap_steal_ptp __P((struct uvm_object *,
					     vaddr_t));
a423 10
#if 0
static boolean_t	 pmap_transfer_ptes __P((struct pmap *,
					 struct pmap_transfer_location *,
					 struct pmap *,
					 struct pmap_transfer_location *,
					 int, boolean_t));
#endif
static boolean_t	 pmap_try_steal_pv __P((struct pv_head *,
						struct pv_entry *,
						struct pv_entry *));
a425 3
void			pmap_pinit __P((pmap_t));
void			pmap_release __P((pmap_t));

d587 2
a588 2
 * space.   pmap_kremove are exported to MI kernel.
 * we make use of the recursive PTE mappings.
d628 1
a628 1
 * => we assume the va is page aligned and the len is a multiple of NBPG
d641 1
a641 1
	for ( /* null */ ; len ; len--, va += NBPG) {
a683 2
 * => we make use of the global vars from machdep.c:
 *	avail_start, avail_end, hole_start, hole_end
a692 3
#ifdef __NetBSD__
	int first16q;
#endif
a711 5
#ifdef __NetBSD__
	msgbuf_paddr = avail_end;
#endif

#ifdef __OpenBSD__
a715 1
#endif
d780 1
a780 1
		     kva += NBPG)
d833 2
a834 2
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;  /* allocate */
	virtual_avail += NBPG; pte++;			     /* advance */
d837 1
a837 1
	virtual_avail += NBPG; pte++;
d840 1
a840 1
	virtual_avail += NBPG; pte++;
d843 1
a843 1
	virtual_avail += NBPG; pte++;
d847 1
a847 1
	virtual_avail += NBPG; pte++;
d859 3
a861 3
	virtual_avail += NBPG; pte++;
	avail_end -= NBPG;
	idt_paddr = avail_end;
d866 1
a866 1
	virtual_avail += NBPG; pte++;
a884 1
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
a890 1
#endif
d902 2
a903 9
#ifdef __NetBSD__
	/*
	 * we must call uvm_page_physload() after we are done playing with
	 * virtual_avail but before we call pmap_steal_memory.  [i.e. here]
	 * this call tells the VM system how much physical memory it
	 * controls.  If we have 16M of RAM or less, just put it all on
	 * the default free list.  Otherwise, put the first 16M of RAM
	 * on a lower priority free list (so that all of the ISA DMA'able
	 * memory won't be eaten up first-off).
d906 4
a909 24
	if (avail_end <= (16 * 1024 * 1024))
		first16q = VM_FREELIST_DEFAULT;
	else
		first16q = VM_FREELIST_FIRST16;

	if (avail_start < hole_start)   /* any free memory before the hole? */
		uvm_page_physload(atop(avail_start), atop(hole_start),
				  atop(avail_start), atop(hole_start),
				  first16q);

	if (first16q != VM_FREELIST_DEFAULT &&
	    hole_end < 16 * 1024 * 1024) {
		uvm_page_physload(atop(hole_end), atop(16 * 1024 * 1024),
				  atop(hole_end), atop(16 * 1024 * 1024),
				  first16q);
		uvm_page_physload(atop(16 * 1024 * 1024), atop(avail_end),
				  atop(16 * 1024 * 1024), atop(avail_end),
				  VM_FREELIST_DEFAULT);
	} else {
		uvm_page_physload(atop(hole_end), atop(avail_end),
				  atop(hole_end), atop(avail_end),
				  VM_FREELIST_DEFAULT);
	}
#endif
d977 1
a977 1
	pv_initpage = (struct pv_page *) uvm_km_alloc(kernel_map, NBPG);
d1082 2
a1083 2
	int lcv, idx, npg, s;
	struct pv_entry *pv, *cpv, *prevpv;
d1114 1
a1114 1
	s = splimp();   /* must protect kmem_map/kmem_object with splimp! */
d1117 1
a1117 1
		    NBPG, UVM_KMF_TRYLOCK|UVM_KMF_VALLOC);
d1120 1
a1120 1
			goto steal_one;
d1126 1
a1126 1
	 * note: we are still holding splimp to protect kmem_object
d1131 1
a1131 1
		goto steal_one;
d1142 1
a1142 1
	/* splimp now dropped */
d1145 1
a1145 1
		goto steal_one;
d1159 1
a1159 104
	return(pmap_add_pvpage(pvpage, mode != ALLOCPV_NONEED));

steal_one:
	/*
	 * if we don't really need a pv_entry right now, we can just return.
	 */

	if (mode != ALLOCPV_NEED)
		return(NULL);

	/*
	 * last ditch effort!   we couldn't allocate a free page to make
	 * more pv_entrys so we try and steal one from someone else.
	 */

	pv = NULL;
	for (lcv = 0 ; pv == NULL && lcv < vm_nphysseg ; lcv++) {
		npg = vm_physmem[lcv].end - vm_physmem[lcv].start;
		for (idx = 0 ; idx < npg ; idx++) {
			struct pv_head *pvhead = vm_physmem[lcv].pmseg.pvhead;

			if (pvhead->pvh_list == NULL)
				continue;	/* spot check */
			if (!simple_lock_try(&pvhead->pvh_lock))
				continue;
			cpv = prevpv = pvhead->pvh_list;
			while (cpv) {
				if (pmap_try_steal_pv(pvhead, cpv, prevpv))
					break;
				prevpv = cpv;
				cpv = cpv->pv_next;
			}
			simple_unlock(&pvhead->pvh_lock);
			/* got one?  break out of the loop! */
			if (cpv) {
				pv = cpv;
				break;
			}
		}
	}

	return(pv);
}

/*
 * pmap_try_steal_pv: try and steal a pv_entry from a pmap
 *
 * => return true if we did it!
 */

static boolean_t
pmap_try_steal_pv(pvh, cpv, prevpv)
	struct pv_head *pvh;
	struct pv_entry *cpv, *prevpv;
{
	pt_entry_t *ptep;	/* pointer to a PTE */

	/*
	 * we never steal kernel mappings or mappings from pmaps we can't lock
	 */

	if (cpv->pv_pmap == pmap_kernel() ||
	    !simple_lock_try(&cpv->pv_pmap->pm_obj.vmobjlock))
		return(FALSE);

	/*
	 * yes, we can try and steal it.   first we need to remove the
	 * mapping from the pmap.
	 */

	ptep = pmap_tmpmap_pvepte(cpv);
	if (*ptep & PG_W) {
		ptep = NULL;	/* wired page, avoid stealing this one */
	} else {
		*ptep = 0;		/* zap! */
		if (pmap_is_curpmap(cpv->pv_pmap))
			pmap_update_pg(cpv->pv_va);
		pmap_tmpunmap_pvepte(cpv);
	}
	if (ptep == NULL) {
		simple_unlock(&cpv->pv_pmap->pm_obj.vmobjlock);
		return(FALSE);	/* wired page, abort! */
	}
	cpv->pv_pmap->pm_stats.resident_count--;
	if (cpv->pv_ptp && cpv->pv_ptp->wire_count)
		/* drop PTP's wired count */
		cpv->pv_ptp->wire_count--;

	/*
	 * XXX: if wire_count goes to one the PTP could be freed, however,
	 * we'd have to lock the page queues (etc.) to do that and it could
	 * cause deadlock headaches.   besides, the pmap we just stole from
	 * may want the mapping back anyway, so leave the PTP around.
	 */

	/*
	 * now we need to remove the entry from the pvlist
	 */

	if (cpv == pvh->pvh_list)
		pvh->pvh_list = cpv->pv_next;
	else
		prevpv->pv_next = cpv->pv_next;
	return(TRUE);
d1307 1
a1307 1
	s = splimp(); /* protect kmem_map */
d1326 2
a1327 2
		(void)uvm_unmap_remove(map, (vaddr_t) pvp,
				       ((vaddr_t) pvp) + NBPG, &dead_entries);
a1419 5
 * => we should not be holding any pv_head locks (in case we are forced
 *	to call pmap_steal_ptp())
 * => we may need to lock pv_head's if we have to steal a PTP
 * => just_try: true if we want a PTP, but not enough to steal one
 * 	from another pmap (e.g. during optional functions like pmap_copy)
d1423 1
a1423 1
pmap_alloc_ptp(pmap, pde_index, just_try)
a1425 1
	boolean_t just_try;
d1431 2
a1432 10
	if (ptp == NULL) {
		if (just_try)
			return(NULL);
		ptp = pmap_steal_ptp(&pmap->pm_obj, ptp_i2o(pde_index));
		if (ptp == NULL) {
			return (NULL);
		}
		/* stole one; zero it. */
		pmap_zero_page(VM_PAGE_TO_PHYS(ptp));
	}
a1444 106
 * pmap_steal_ptp: steal a PTP from any pmap that we can access
 *
 * => obj is locked by caller.
 * => we can throw away mappings at this level (except in the kernel's pmap)
 * => stolen PTP is placed in <obj,offset> pmap
 * => we lock pv_head's
 * => hopefully, this function will be seldom used [much better to have
 *	enough free pages around for us to allocate off the free page list]
 */

static struct vm_page *
pmap_steal_ptp(obj, offset)
	struct uvm_object *obj;
	vaddr_t offset;
{
	struct vm_page *ptp = NULL;
	struct pmap *firstpmap;
	struct uvm_object *curobj;
	pt_entry_t *ptes;
	int idx, lcv;
	boolean_t caller_locked, we_locked;

	simple_lock(&pmaps_lock);
	if (pmaps_hand == NULL)
		pmaps_hand = LIST_FIRST(&pmaps);
	firstpmap = pmaps_hand;

	do { /* while we haven't looped back around to firstpmap */

		curobj = &pmaps_hand->pm_obj;
		we_locked = FALSE;
		caller_locked = (curobj == obj);
		if (!caller_locked) {
			we_locked = simple_lock_try(&curobj->vmobjlock);
		}
		if (caller_locked || we_locked) {
			ptp = curobj->memq.tqh_first;
			for (/*null*/; ptp != NULL; ptp = ptp->listq.tqe_next) {

				/*
				 * might have found a PTP we can steal
				 * (unless it has wired pages).
				 */

				idx = ptp_o2i(ptp->offset);
#ifdef DIAGNOSTIC
				if (VM_PAGE_TO_PHYS(ptp) !=
				    (pmaps_hand->pm_pdir[idx] & PG_FRAME))
					panic("pmap_steal_ptp: PTP mismatch!");
#endif

				ptes = (pt_entry_t *)
					pmap_tmpmap_pa(VM_PAGE_TO_PHYS(ptp));
				for (lcv = 0 ; lcv < PTES_PER_PTP ; lcv++)
					if ((ptes[lcv] & (PG_V|PG_W)) ==
					    (PG_V|PG_W))
						break;
				if (lcv == PTES_PER_PTP)
					pmap_remove_ptes(pmaps_hand, NULL, ptp,
							 (vaddr_t)ptes,
							 ptp_i2v(idx),
							 ptp_i2v(idx+1),
							 PMAP_REMOVE_ALL);
				pmap_tmpunmap_pa();

				if (lcv != PTES_PER_PTP)
					/* wired, try next PTP */
					continue;

				/*
				 * got it!!!
				 */

				pmaps_hand->pm_pdir[idx] = 0;	/* zap! */
				pmaps_hand->pm_stats.resident_count--;
				if (pmap_is_curpmap(pmaps_hand))
					tlbflush();
				else if (pmap_valid_entry(*APDP_PDE) &&
					 (*APDP_PDE & PG_FRAME) ==
					 pmaps_hand->pm_pdirpa) {
					pmap_update_pg(((vaddr_t)APTE_BASE) +
						       ptp->offset);
				}

				/* put it in our pmap! */
				uvm_pagerealloc(ptp, obj, offset);
				break;	/* break out of "for" loop */
			}
			if (we_locked) {
				simple_unlock(&curobj->vmobjlock);
			}
		}

		/* advance the pmaps_hand */
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
		if (pmaps_hand == NULL) {
			pmaps_hand = LIST_FIRST(&pmaps);
		}

	} while (ptp == NULL && pmaps_hand != firstpmap);

	simple_unlock(&pmaps_lock);
	return(ptp);
}

/*
d1452 1
a1452 1
pmap_get_ptp(pmap, pde_index, just_try)
a1454 1
	boolean_t just_try;
d1476 1
a1476 1
	return(pmap_alloc_ptp(pmap, pde_index, just_try));
d1484 35
a1530 3
	pmap_pinit(pmap);
	return(pmap);
}
a1531 8
/*
 * pmap_pinit: given a zero'd pmap structure, init it.
 */

void
pmap_pinit(pmap)
	struct pmap *pmap;
{
a1542 13
	/* allocate PDP */
	pmap->pm_pdir = (pd_entry_t *) uvm_km_alloc(kernel_map, NBPG);
	if (pmap->pm_pdir == NULL)
		panic("pmap_pinit: kernel_map out of virtual space!");
	(void) pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
			    (paddr_t *)&pmap->pm_pdirpa);

	/* init PDP */
	/* zero init area */
	bzero(pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
	/* put in recursive PDE to map the PTEs */
	pmap->pm_pdir[PDSLOT_PTE] = pmap->pm_pdirpa | PG_V | PG_KW;

d1548 2
d1552 5
a1556 3
	 * us.   note that there is no need to splimp to protect us from
	 * malloc since malloc allocates out of a submap and we should have
	 * already allocated kernel PTPs to cover the range...
d1559 8
a1566 6
	/* put in kernel VM PDEs */
	bcopy(&PDP_BASE[PDSLOT_KERN], &pmap->pm_pdir[PDSLOT_KERN],
	       nkpde * sizeof(pd_entry_t));
	/* zero the rest */
	bzero(&pmap->pm_pdir[PDSLOT_KERN + nkpde],
	       NBPG - ((PDSLOT_KERN + nkpde) * sizeof(pd_entry_t)));
d1568 1
d1570 2
d1583 1
a1600 19
	pmap_release(pmap);
	pool_put(&pmap_pmap_pool, pmap);
}

/*
 * pmap_release: release all resources held by a pmap
 *
 * => if pmap is still referenced it should be locked
 * => XXX: we currently don't expect any busy PTPs because we don't
 *    allow anything to map them (except for the kernel's private
 *    recursive mapping) or make them busy.
 */

void
pmap_release(pmap)
	struct pmap *pmap;
{
	struct vm_page *pg;

a1605 2
	if (pmap == pmaps_hand)
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
d1625 1
a1625 1
	uvm_km_free(kernel_map, (vaddr_t)pmap->pm_pdir, NBPG);
d1638 2
d1844 1
a1845 5
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page: lock botch");
#endif

d1847 1
a1847 2
	bzero(zerop, NBPG);				/* zero */
	*zero_pte = 0;				/* zap! */
a1863 4
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page_uncached: lock botch");
#endif
d1867 2
a1868 1
	for (i = 0, ptr = (int *) zerop; i < NBPG / sizeof(int); i++) {
d1881 1
a1881 2
	*zero_pte = 0;					/* zap! */
	pmap_update_pg((vaddr_t)zerop);			/* flush TLB */
d1903 1
a1903 1
	bcopy(csrcp, cdstp, NBPG);
d1949 1
a1949 1
			     ; pte++, startva += NBPG) {
d2138 1
a2138 1
	if (sva + NBPG == eva) {
d2348 3
d2721 1
a2721 1
			panic("pmap_unwire: invalid (unmapped) va");
a2761 444
#if 0
/*
 * pmap_transfer: transfer (move or copy) mapping from one pmap
 * 	to another.
 *
 * => this function is optional, it doesn't have to do anything
 * => we assume that the mapping in the src pmap is valid (i.e. that
 *    it doesn't run off the end of the map's virtual space).
 * => we assume saddr, daddr, and len are page aligned/lengthed
 */

void
pmap_transfer(dstpmap, srcpmap, daddr, len, saddr, move)
	struct pmap *dstpmap, *srcpmap;
	vaddr_t daddr, saddr;
	vsize_t len;
	boolean_t move;
{
	/* base address of PTEs, dst could be NULL */
	pt_entry_t *srcptes, *dstptes;

	struct pmap_transfer_location srcl, dstl;
	int dstvalid;		  /* # of PTEs left in dst's current PTP */
	struct pmap *mapped_pmap; /* the pmap we passed to pmap_map_ptes */
	vsize_t blklen;
	int blkpgs, toxfer;
	boolean_t ok;

#ifdef DIAGNOSTIC
	/*
	 * sanity check: let's make sure our len doesn't overflow our dst
	 * space.
	 */

	if (daddr < VM_MAXUSER_ADDRESS) {
		if (VM_MAXUSER_ADDRESS - daddr < len) {
			printf("pmap_transfer: no room in user pmap "
			       "(addr=0x%lx, len=0x%lx)\n", daddr, len);
			return;
		}
	} else if (daddr < VM_MIN_KERNEL_ADDRESS ||
		   daddr >= VM_MAX_KERNEL_ADDRESS) {
		printf("pmap_transfer: invalid transfer address 0x%lx\n",
		       daddr);
	} else {
		if (VM_MAX_KERNEL_ADDRESS - daddr < len) {
			printf("pmap_transfer: no room in kernel pmap "
			       "(addr=0x%lx, len=0x%lx)\n", daddr, len);
			return;
		}
	}
#endif

	/*
	 * ideally we would like to have either src or dst pmap's be the
	 * current pmap so that we can map the other one in APTE space
	 * (if needed... one of the maps could be the kernel's pmap).
	 *
	 * however, if we can't get this, then we have to use the tmpmap
	 * (alternately we could punt).
	 */

	if (!pmap_is_curpmap(dstpmap) && !pmap_is_curpmap(srcpmap)) {
		dstptes = NULL;			/* dstptes NOT mapped */
		srcptes = pmap_map_ptes(srcpmap);   /* let's map the source */
		mapped_pmap = srcpmap;
	} else {
		if (!pmap_is_curpmap(srcpmap)) {
			srcptes = pmap_map_ptes(srcpmap);   /* possible APTE */
			dstptes = PTE_BASE;
			mapped_pmap = srcpmap;
		} else {
			dstptes = pmap_map_ptes(dstpmap);   /* possible APTE */
			srcptes = PTE_BASE;
			mapped_pmap = dstpmap;
		}
	}

	/*
	 * at this point we know that the srcptes are mapped.   the dstptes
	 * are mapped if (dstptes != NULL).    if (dstptes == NULL) then we
	 * will have to map the dst PTPs page at a time using the tmpmap.
	 * [XXX: is it worth the effort, or should we just punt?]
	 */

	srcl.addr = saddr;
	srcl.pte = &srcptes[i386_btop(srcl.addr)];
	srcl.ptp = NULL;
	dstl.addr = daddr;
	if (dstptes)
		dstl.pte = &dstptes[i386_btop(dstl.addr)];
	else
		dstl.pte  = NULL;		/* we map page at a time */
	dstl.ptp = NULL;
	dstvalid = 0;		/* force us to load a new dst PTP to start */

	while (len) {

		/*
		 * compute the size of this block.
		 */

		/* length in bytes */
		blklen = i386_round_pdr(srcl.addr+1) - srcl.addr;
		if (blklen > len)
			blklen = len;
		blkpgs = i386_btop(blklen);

		/*
		 * if the block is not valid in the src pmap,
		 * then we can skip it!
		 */

		if (!pmap_valid_entry(srcpmap->pm_pdir[pdei(srcl.addr)])) {
			len = len - blklen;
			srcl.pte  = srcl.pte + blkpgs;
			srcl.addr += blklen;
			dstl.addr += blklen;
			if (blkpgs > dstvalid) {
				dstvalid = 0;
				dstl.ptp = NULL;
			} else {
				dstvalid = dstvalid - blkpgs;
			}
			if (dstptes == NULL && (len == 0 || dstvalid == 0)) {
				if (dstl.pte) {
					pmap_tmpunmap_pa();
					dstl.pte = NULL;
				}
			} else {
				dstl.pte += blkpgs;
			}
			continue;
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs to transfer.
		 * if we don't have any dst PTEs ready, then get some.
		 */

		if (dstvalid == 0) {
			if (!pmap_valid_entry(dstpmap->
					      pm_pdir[pdei(dstl.addr)])) {
#ifdef DIAGNOSTIC
				if (dstl.addr >= VM_MIN_KERNEL_ADDRESS)
					panic("pmap_transfer: missing kernel "
					      "PTP at 0x%lx", dstl.addr);
#endif
				dstl.ptp = pmap_get_ptp(dstpmap,
							pdei(dstl.addr), TRUE);
				if (dstl.ptp == NULL)	/* out of RAM?  punt. */
					break;
			} else {
				dstl.ptp = NULL;
			}
			dstvalid = i386_btop(i386_round_pdr(dstl.addr+1) -
					     dstl.addr);
			if (dstptes == NULL) {
				dstl.pte = (pt_entry_t *)
					pmap_tmpmap_pa(dstpmap->
						       pm_pdir[pdei(dstl.addr)]
						       & PG_FRAME);
				dstl.pte = dstl.pte + (PTES_PER_PTP - dstvalid);
			}
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs to transfer.
		 * we have a valid dst block of "dstvalid" PTEs ready.
		 * thus we can transfer min(blkpgs, dstvalid) PTEs now.
		 */

		srcl.ptp = NULL;	/* don't know source PTP yet */
		if (dstvalid < blkpgs)
			toxfer = dstvalid;
		else
			toxfer = blkpgs;

		if (toxfer > 0) {
			ok = pmap_transfer_ptes(srcpmap, &srcl, dstpmap, &dstl,
						toxfer, move);

			if (!ok)		/* memory shortage?  punt. */
				break;

			dstvalid -= toxfer;
			blkpgs -= toxfer;
			len -= i386_ptob(toxfer);
			if (blkpgs == 0)	/* out of src PTEs?  restart */
				continue;
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs left
		 * to transfer.  we have just used up our "dstvalid"
		 * PTEs, and thus must obtain more dst PTEs to finish
		 * off the src block.  since we are now going to
		 * obtain a brand new dst PTP, we know we can finish
		 * the src block in one more transfer.
		 */

#ifdef DIAGNOSTIC
		if (dstvalid)
			panic("pmap_transfer: dstvalid non-zero after drain");
		if ((dstl.addr & (NBPD-1)) != 0)
			panic("pmap_transfer: dstaddr not on PD boundary "
			      "(0x%lx)\n", dstl.addr);
#endif

		if (dstptes == NULL && dstl.pte != NULL) {
			/* dispose of old PT mapping */
			pmap_tmpunmap_pa();
			dstl.pte = NULL;
		}

		/*
		 * get new dst PTP
		 */
		if (!pmap_valid_entry(dstpmap->pm_pdir[pdei(dstl.addr)])) {
#ifdef DIAGNOSTIC
			if (dstl.addr >= VM_MIN_KERNEL_ADDRESS)
				panic("pmap_transfer: missing kernel PTP at "
				      "0x%lx", dstl.addr);
#endif
			dstl.ptp = pmap_get_ptp(dstpmap, pdei(dstl.addr), TRUE);
			if (dstl.ptp == NULL)	/* out of free RAM?  punt. */
				break;
		} else {
			dstl.ptp = NULL;
		}

		dstvalid = PTES_PER_PTP;	/* new PTP */

		/*
		 * if the dstptes are un-mapped, then we need to tmpmap in the
		 * dstl.ptp.
		 */

		if (dstptes == NULL) {
			dstl.pte = (pt_entry_t *)
				pmap_tmpmap_pa(dstpmap->pm_pdir[pdei(dstl.addr)]
					       & PG_FRAME);
		}

		/*
		 * we have a valid source block of "blkpgs" PTEs left
		 * to transfer.  we just got a brand new dst PTP to
		 * receive these PTEs.
		 */

#ifdef DIAGNOSTIC
		if (dstvalid < blkpgs)
			panic("pmap_transfer: too many blkpgs?");
#endif
		toxfer = blkpgs;
		ok = pmap_transfer_ptes(srcpmap, &srcl, dstpmap, &dstl, toxfer,
					move);

		if (!ok)		/* memory shortage?   punt. */
			break;

		dstvalid -= toxfer;
		blkpgs -= toxfer;
		len -= i386_ptob(toxfer);

		/*
		 * done src pte block
		 */
	}
	if (dstptes == NULL && dstl.pte != NULL)
		pmap_tmpunmap_pa();		/* dst PTP still mapped? */
	pmap_unmap_ptes(mapped_pmap);
}

/*
 * pmap_transfer_ptes: transfer PTEs from one pmap to another
 *
 * => we assume that the needed PTPs are mapped and that we will
 *	not cross a block boundary.
 * => we return TRUE if we transfered all PTEs, FALSE if we were
 *	unable to allocate a pv_entry
 */

static boolean_t
pmap_transfer_ptes(srcpmap, srcl, dstpmap, dstl, toxfer, move)
	struct pmap *srcpmap, *dstpmap;
	struct pmap_transfer_location *srcl, *dstl;
	int toxfer;
	boolean_t move;
{
	pt_entry_t dstproto, opte;
	int bank, off;
	struct pv_head *pvh;
	struct pv_entry *pve, *lpve;

	/*
	 * generate "prototype" dst PTE
	 */

	if (dstl->addr < VM_MAX_ADDRESS)
		dstproto = PG_u;		/* "user" page */
	else
		dstproto = pmap_pg_g;	/* kernel page */

	/*
	 * ensure we have dst PTP for user addresses.
	 */

	if (dstl->ptp == NULL && dstl->addr < VM_MAXUSER_ADDRESS)
		dstl->ptp = PHYS_TO_VM_PAGE(dstpmap->pm_pdir[pdei(dstl->addr)] &
					    PG_FRAME);

	/*
	 * main loop over range
	 */

	for (/*null*/; toxfer > 0 ; toxfer--,
			     srcl->addr += NBPG, dstl->addr += NBPG,
			     srcl->pte++, dstl->pte++) {

		if (!pmap_valid_entry(*srcl->pte))  /* skip invalid entrys */
			continue;

#ifdef DIAGNOSTIC
		if (pmap_valid_entry(*dstl->pte))
			panic("pmap_transfer_ptes: attempt to overwrite "
			      "active entry");
#endif

		/*
		 * let's not worry about non-pvlist mappings (typically device
		 * pager mappings).
		 */

		opte = *srcl->pte;

		if ((opte & PG_PVLIST) == 0)
			continue;

		/*
		 * if we are moving the mapping, then we can just adjust the
		 * current pv_entry.    if we are copying the mapping, then we
		 * need to allocate a new pv_entry to account for it.
		 */

		if (move == FALSE) {
			pve = pmap_alloc_pv(dstpmap, ALLOCPV_TRY);
			if (pve == NULL)
				return(FALSE); 		/* punt! */
		} else {
			pve = NULL;  /* XXX: quiet gcc warning */
		}

		/*
		 * find the pv_head for this mapping.  since our mapping is
		 * on the pvlist (PG_PVLIST), there must be a pv_head.
		 */

		bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
		if (bank == -1)
			panic("pmap_transfer_ptes: PG_PVLIST PTE and "
			      "no pv_head!");
#endif
		pvh = &vm_physmem[bank].pmseg.pvhead[off];

		/*
		 * now lock down the pvhead and find the current entry (there
		 * must be one).
		 */

		simple_lock(&pvh->pvh_lock);
		for (lpve = pvh->pvh_list ; lpve ; lpve = lpve->pv_next)
			if (lpve->pv_pmap == srcpmap &&
			    lpve->pv_va == srcl->addr)
				break;
#ifdef DIAGNOSTIC
		if (lpve == NULL)
			panic("pmap_transfer_ptes: PG_PVLIST PTE, but "
			      "entry not found");
#endif

		/*
		 * update src ptp.   if the ptp is null in the pventry, then
		 * we are not counting valid entrys for this ptp (this is only
		 * true for kernel PTPs).
		 */

		if (srcl->ptp == NULL)
			srcl->ptp = lpve->pv_ptp;
#ifdef DIAGNOSTIC
		if (srcl->ptp &&
		    (srcpmap->pm_pdir[pdei(srcl->addr)] & PG_FRAME) !=
		    VM_PAGE_TO_PHYS(srcl->ptp))
			panic("pmap_transfer_ptes: pm_pdir - pv_ptp mismatch!");
#endif

		/*
		 * for move, update the pve we just found (lpve) to
		 * point to its new mapping.  for copy, init the new
		 * pve and put it in the list.
		 */

		if (move == TRUE) {
			pve = lpve;
		}
		pve->pv_pmap = dstpmap;
		pve->pv_va = dstl->addr;
		pve->pv_ptp = dstl->ptp;
		if (move == FALSE) {		/* link in copy */
			pve->pv_next = lpve->pv_next;
			lpve->pv_next = pve;
		}

		/*
		 * sync the R/M bits while we are here.
		 */

		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));

		/*
		 * now actually update the ptes and unlock the pvlist.
		 */

		if (move) {
			*srcl->pte = 0;		/* zap! */
			if (pmap_is_curpmap(srcpmap))
				pmap_update_pg(srcl->addr);
			if (srcl->ptp)
				/* don't bother trying to free PTP */
				srcl->ptp->wire_count--;
			srcpmap->pm_stats.resident_count--;
			if (opte & PG_W)
				srcpmap->pm_stats.wired_count--;
		}
		*dstl->pte = (opte & ~(PG_u|PG_U|PG_M|PG_G|PG_W)) | dstproto;
		dstpmap->pm_stats.resident_count++;
		if (dstl->ptp)
			dstl->ptp->wire_count++;
		simple_unlock(&pvh->pvh_lock);
	}
	return(TRUE);
}

d2770 1
a2770 1
 * defined as macro call to pmap_transfer in pmap.h
a2773 12
 * pmap_move: move mappings from one pmap to another
 *
 * => optional function
 * void pmap_move(dst_pmap, src_pmap, dst_addr, len, src_addr)
 */

/*
 * defined as macro call to pmap_transfer in pmap.h
 */
#endif

/*
d2820 1
a2820 1
		ptp = pmap_get_ptp(pmap, pdei(va), FALSE);
d2823 1
a2823 1
				return (ENOMEM);
d3032 1
a3032 1
		if (pmap_alloc_ptp(kpm, PDSLOT_KERN + nkpde, FALSE) == NULL) {
d3045 4
d3107 1
a3107 1
		for (/* null */; sva < blkendva ; sva += NBPG, pte++) {
@


1.54
log
@Merge in some pmap improvements from NetBSD.
 - disable pmap_copy (and comment out the code), it makes things slower.
 - don't return TRUE from pmap_extract if the pte is not valid.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.53 2001/12/10 17:27:01 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.99 2000/09/05 21:56:41 thorpej Exp $	*/
d322 9
d433 2
d436 1
a436 1
					      pt_entry_t *, vaddr_t));
d440 3
a442 1
					       vaddr_t, vaddr_t));
d644 4
a647 1
	pte = vtopte(va);
d649 5
d680 9
a688 1
		pte = vtopte(va);
d834 38
d1701 2
a1702 1
							 ptp_i2v(idx+1));
d2083 1
d2089 1
d2104 18
d2165 3
d2176 13
a2188 1
	memset(zerop, 0, NBPG);				/* zero */
d2193 1
a2193 1
	return (TRUE);
d2234 1
a2234 1
pmap_remove_ptes(pmap, pmap_rr, ptp, ptpva, startva, endva)
d2240 1
d2261 3
d2341 1
a2341 1
pmap_remove_pte(pmap, ptp, pte, va)
d2346 1
d2354 3
d2414 15
d2476 1
a2476 1
						 &ptes[i386_btop(sva)], sva);
d2571 1
a2571 1
				 (vaddr_t)&ptes[i386_btop(sva)], sva, blkendva);
@


1.53
log
@minor cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.52 2001/12/08 02:24:06 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.91 2000/06/02 17:46:37 thorpej Exp $	*/
d436 1
d442 1
d938 1
a938 1
	int npages, lcv;
d966 5
d1959 1
a1959 1
 * pmap_activate: activate a process' pmap (fill in %cr3 info)
d1961 1
a1961 1
 * => called from cpu_fork()
d2011 9
a2019 2
	paddr_t retval;
	pt_entry_t *ptes;
a2020 1
	if (pmap->pm_pdir[pdei(va)]) {
d2022 1
a2022 1
		retval = (paddr_t)(ptes[i386_btop(va)] & PG_FRAME);
d2024 6
a2029 3
		if (pap != NULL)
			*pap = retval | (va & ~PG_FRAME);
		return (TRUE);
d2942 1
d3407 1
@


1.52
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2001/11/28 16:13:28 art Exp $	*/
d1032 2
a1033 2
	if (pv_freepages.tqh_first != NULL) {
		pvpage = pv_freepages.tqh_first;
d1093 2
a1094 1
	if (mode != ALLOCPV_NONEED && pv_unusedpgs.tqh_first != NULL) {
a1096 1
		pvpage = pv_unusedpgs.tqh_first;
d1158 2
a1159 1
	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg), VM_PROT_ALL);
d1161 1
a1161 1
	pvpage = (struct pv_page *) pv_cachedva;
d1356 1
a1356 1
	if (pv_nfpvents > PVE_HIWAT && pv_unusedpgs.tqh_first != NULL &&
d1387 1
a1387 1
	if (pv_nfpvents > PVE_HIWAT && pv_unusedpgs.tqh_first != NULL &&
d1415 1
a1415 2

	pvp = pv_unusedpgs.tqh_first;
d1421 1
a1425 1

d1840 1
a1840 2
	while (pmap->pm_obj.memq.tqh_first != NULL) {
		pg = pmap->pm_obj.memq.tqh_first;
d2354 1
a2354 1
						pmap->pm_obj.memq.tqh_first;
d2442 2
a2443 1
				pmap->pm_ptphint = pmap->pm_obj.memq.tqh_first;
d2563 1
a2563 1
					    pve->pv_pmap->pm_obj.memq.tqh_first;
d3659 1
a3659 2
		for (pm = pmaps.lh_first; pm != NULL;
		     pm = pm->pm_list.le_next) {
@


1.51
log
@zap some typedefs.
vm_map_t -> struct vm_map *
vm_map_entry_t -> struct vm_map_entry *
simple_lock_data_t -> struct simplelock

(uvm not done yet, coming in the next commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2001/11/28 15:02:58 art Exp $	*/
d1159 1
a2033 21
 * pmap_map: map a range of PAs into kvm
 *
 * => used during crash dump
 * => XXX: pmap_map() should be phased out?
 */

vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	vm_prot_t prot;
{
	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
		va += NBPG;
		spa += NBPG;
	}
	return va;
}

/*
d2923 1
@


1.50
log
@Don't use pmap_update when we mean tlbflush. make pmap_update into a noop.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2001/11/28 14:13:06 art Exp $	*/
d271 5
a275 5
simple_lock_data_t pvalloc_lock;
simple_lock_data_t pmaps_lock;
simple_lock_data_t pmap_copy_page_lock;
simple_lock_data_t pmap_zero_page_lock;
simple_lock_data_t pmap_tmpptp_lock;
d1409 1
a1409 1
	vm_map_entry_t dead_entries;
@


1.49
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2001/11/28 13:47:38 art Exp $	*/
d582 1
a582 1
			pmap_update();
d671 1
a671 1
		pmap_update();
d924 1
a924 1
	pmap_update();
d1640 1
a1640 1
					pmap_update();
d2476 1
a2476 1
			pmap_update();
d2481 1
a2481 1
				pmap_update();
d2596 1
a2596 1
		pmap_update();
d2738 1
a2738 1
		pmap_update();
d2866 1
a2866 1
			pmap_update();
d2871 1
a2871 1
				pmap_update();
@


1.48
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2001/11/07 02:55:50 art Exp $	*/
d610 1
a610 1
 * space.   pmap_kremove/pmap_kenter_pgs are exported to MI kernel.
a670 38
		pmap_update();
#endif
}

/*
 * pmap_kenter_pgs: enter in a number of vm_pages
 */

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	pt_entry_t *pte, opte;
	int lcv;
	vaddr_t tva;
#if defined(I386_CPU)
	boolean_t need_update = FALSE;
#endif

	for (lcv = 0 ; lcv < npgs ; lcv++) {
		tva = va + lcv * NBPG;
		pte = vtopte(tva);
		opte = *pte;
		*pte = VM_PAGE_TO_PHYS(pgs[lcv]) | PG_RW | PG_V | pmap_pg_g;
#if defined(I386_CPU)
		if (cpu_class == CPUCLASS_386) {
			if (pmap_valid_entry(opte))
				need_update = TRUE;
			continue;
		}
#endif
		if (pmap_valid_entry(opte))
			pmap_update_pg(tva);
	}
#if defined(I386_CPU)
	if (need_update && cpu_class == CPUCLASS_386)
@


1.47
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2001/11/06 01:43:48 art Exp $	*/
a2746 3
		if (pve->pv_va >= uvm.pager_sva && pve->pv_va < uvm.pager_eva) {
			printf("pmap_change_attrs: found pager VA on pv_list\n");
		}
d3498 1
a3498 1
				return (KERN_RESOURCE_SHORTAGE);
d3598 1
a3598 1
					error = KERN_RESOURCE_SHORTAGE;
d3636 1
a3636 1
	error = KERN_SUCCESS;
@


1.46
log
@Zap some redundant includes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2001/09/19 20:50:56 mickey Exp $	*/
d2116 1
a2116 1
void
d2132 2
@


1.45
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2001/08/11 11:45:27 art Exp $	*/
a75 3

#include <vm/vm.h>
#include <vm/vm_page.h>
@


1.44
log
@Some fixes from NetBSD.
Parts of the glue needed for page zeroing in the idle loop.
More careful handling of ptp pages (should solve some problems, most notably
with Intel 440GX+ motherboards).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2001/07/25 13:25:32 art Exp $	*/
a77 1
#include <vm/vm_kern.h>
@


1.43
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.42 2001/07/15 12:23:55 assar Exp $	*/
/*	$NetBSD: pmap.c,v 1.84 2000/02/21 02:01:24 chs Exp $	*/
d994 1
a994 1
	if (addr == NULL)
d1026 1
a1026 1
	pv_cachedva = NULL;   /* a VA we have allocated but not used yet */
d1161 1
a1161 1
	if (pv_cachedva == NULL) {
d1164 1
a1164 1
		if (pv_cachedva == NULL) {
d1202 1
a1202 1
	pv_cachedva = NULL;
d1584 1
a1584 1
			    UVM_PGA_USERESERVE);
d1592 2
a1598 1
	pmap_zero_page(VM_PAGE_TO_PHYS(ptp));
d2117 22
d2245 2
a2246 1
			      "PG_PVLIST");
d2312 1
a2312 1
			panic("pmap_remove_ptes: managed page without "
d2321 3
a2323 1
		panic("pmap_remove_pte: unmanaged page marked PG_PVLIST");
d2750 1
a2750 1
			printf("pmap_change_attrs: found pager VA on pv_list");
d2954 1
a2954 2
#if 0
#ifdef DIAGNOSITC
a2959 1
#endif
d3539 4
a3542 2
					panic("pmap_enter: PG_PVLIST mapping "
					    "with unmanaged page");
d3568 2
a3569 1
				    "unmanaged page");
d3690 1
d3699 6
@


1.42
log
@add macros for spinlockinit, spinlockmgr to build for MULTIPROCESSOR
or LOCKDEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2001/06/08 08:08:52 art Exp $	*/
d2087 1
a2087 1
		_pmap_enter(pmap_kernel(), va, spa, prot, 0);
d3437 1
a3437 1
_pmap_enter(pmap, va, pa, prot, flags)
@


1.41
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2001/05/09 15:31:25 art Exp $	*/
d269 5
@


1.40
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2001/04/10 06:59:13 niklas Exp $	*/
d1785 1
a1785 1
	(void) _pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
d2035 1
a2035 1
_pmap_extract(pmap, va, pap)
a2051 12
}

paddr_t
pmap_extract(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	paddr_t pa;

	if (_pmap_extract(pmap, va, &pa))
		return (pa);
	return (NULL);
@


1.39
log
@Fix for machines which need to enlarge the kernel address space, at least
1GB i386 machines needs this.  The fix is heavily based on Jason Thorpe's
found in NetBSD.  Here is his original commit message:

Instead of checking vm_physmem[<physseg>].pgs to determine if
uvm_page_init() has completed, add a boolean uvm.page_init_done,
and test against that.  Use this same boolean (rather than
pmap_initialized) in pmap_growkernel() to determine if we are
being called via uvm_page_init() to grow the kernel address space.

This fixes a problem on some i386 configurations where pmap_init()
itself was needing to have the kernel page table grown, and since
pmap_initialized was not yet set to TRUE, pmap_growkernel() was
choosing the wrong code path.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2001/03/22 23:36:51 niklas Exp $	*/
d2918 1
a2918 1
pmap_change_wiring(pmap, va, wired)
a2920 1
	boolean_t wired;
@


1.38
log
@Merge in NetBSD's PMAP_NEW, still disabled
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2000/02/21 02:01:24 chs Exp $	*/
d3661 1
a3661 1
		if (pmap_initialized == FALSE) {
d3679 6
@


1.37
log
@KNF a stmt
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.36 2000/05/04 18:55:06 deraadt Exp $	*/
/*	$NetBSD: pmap.c,v 1.36 1996/05/03 19:42:22 christos Exp $	*/
d5 2
a6 2
 * Copyright (c) 1993, 1994, 1995 Charles M. Hannum.  All rights reserved.
 * Copyright (c) 1991 Regents of the University of California.
a8 4
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department and William Jolitz of UUNET Technologies Inc.
 *
d19 15
a33 19
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	7.7 (Berkeley)	5/12/91
d37 24
a60 9
 * Derived originally from an old hp300 version by Mike Hibler.  The version
 * by William Jolitz has been heavily modified to allow non-contiguous
 * mapping of physical memory by Wolfgang Solfrank, and to fix several bugs
 * and greatly speedup it up by Charles Hannum.
 * 
 * A recursive map [a pde which points to the page directory] is used to map
 * the page tables using the pagetables themselves. This is done to reduce
 * the impact on kernel virtual memory for lots of sparse address space, and
 * to reduce the cost of memory to each process.
d63 6
a68 25
/*
 *	Manages physical address maps.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */
d74 1
a80 1
#if defined(UVM)
a81 1
#endif
d84 2
d88 5
d94 1
a94 1
#include <i386/isa/isa_machdep.h>
d96 48
a143 2
#include "isa.h"
#include "isadma.h"
d146 61
a206 2
 * Allocate various and sundry SYSMAPs used in the days of old VM
 * and not yet converted.  XXX.
a207 92
#define	BSDVM_COMPAT	1

#ifdef DEBUG
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
} enter_stats;
struct {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
} remove_stats;

int pmapdebug = 0 /* 0xffff */;
#define	PDB_FOLLOW	0x0001
#define	PDB_INIT	0x0002
#define	PDB_ENTER	0x0004
#define	PDB_REMOVE	0x0008
#define	PDB_CREATE	0x0010
#define	PDB_PTPAGE	0x0020
#define	PDB_CACHE	0x0040
#define	PDB_BITS	0x0080
#define	PDB_COLLECT	0x0100
#define	PDB_PROTECT	0x0200
#define	PDB_PDRTAB	0x0400
#define	PDB_PARANOIA	0x2000
#define	PDB_WIRING	0x4000
#define	PDB_PVDUMP	0x8000
#endif

/*
 * Get PDEs and PTEs for user/kernel address space
 */
#define	pmap_pde(m, v)	(&((m)->pm_pdir[((vm_offset_t)(v) >> PDSHIFT)&1023]))

/*
 * Empty PTEs and PDEs are always 0, but checking only the valid bit allows
 * the compiler to generate `testb' rather than `testl'.
 */
#define	pmap_pde_v(pde)			(*(pde) & PG_V)
#define	pmap_pte_pa(pte)		(*(pte) & PG_FRAME)
#define	pmap_pte_w(pte)			(*(pte) & PG_W)
#define	pmap_pte_m(pte)			(*(pte) & PG_M)
#define	pmap_pte_u(pte)			(*(pte) & PG_U)
#define	pmap_pte_v(pte)			(*(pte) & PG_V)
#define	pmap_pte_set_w(pte, v)		((v) ? (*(pte) |= PG_W) : (*(pte) &= ~PG_W))
#define	pmap_pte_set_prot(pte, v)	((*(pte) &= ~PG_PROT), (*(pte) |= (v)))

/*
 * Given a map and a machine independent protection code,
 * convert to a vax protection code.
 */
pt_entry_t	protection_codes[8];

struct pmap	kernel_pmap_store;

vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
int		npages;

boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int		pv_nfree;

pt_entry_t *pmap_pte __P((pmap_t, vm_offset_t));
struct pv_entry * pmap_alloc_pv __P((void));
void pmap_free_pv __P((struct pv_entry *));
void i386_protection_init __P((void));
void pmap_collect_pv __P((void));
__inline void pmap_remove_pv __P((pmap_t, vm_offset_t, struct pv_entry *));
__inline void pmap_enter_pv __P((pmap_t, vm_offset_t, struct pv_entry *));
void pmap_remove_all __P((vm_offset_t));
void pads __P((pmap_t pm));
void pmap_dump_pvlist __P((vm_offset_t phys, char *m));
void pmap_pvdump __P((vm_offset_t pa));

#if BSDVM_COMPAT
#include <sys/msgbuf.h>
d210 52
a261 1
 * All those kernel PT submaps that BSD is so fond of
a262 4
pt_entry_t	*CMAP1, *CMAP2, *XXX_mmap;
caddr_t		CADDR1, CADDR2, vmmap;
pt_entry_t	*msgbufmap, *bootargmap;
#endif	/* BSDVM_COMPAT */
d265 1
a265 9
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, and allocate the system page table.
 *
 *	On the I386 this is called after mapping has already been enabled
 *	and just syncs the pmap module with what has already been done.
 *	[We can't call it easily with mapping off since the kernel is not
 *	mapped with PA == VA, hence we would have to relocate every address
 *	from the linked base (virtual) address to the actual (physical)
 *	address starting relative to 0]
d268 17
a284 8
void
pmap_bootstrap(virtual_start)
	vm_offset_t virtual_start;
{
#if BSDVM_COMPAT
	vm_offset_t va;
	pt_entry_t *pte;
#endif
a285 3
	/* Register the page size with the vm system */
#if defined(UVM)
	uvm_setpagesize();
a286 5
	vm_set_page_size();
#endif

	virtual_avail = virtual_start;
	virtual_end = VM_MAX_KERNEL_ADDRESS;
d288 2
a289 4
	/*
	 * Initialize protection array.
	 */
	i386_protection_init();
d291 2
a292 8
#ifdef notdef
	/*
	 * Create Kernel page directory table and page maps.
	 * [ currently done in locore. i have wild and crazy ideas -wfj ]
	 */
	bzero(firstaddr, (1+NKPDE)*NBPG);
	pmap_kernel()->pm_pdir = firstaddr + VM_MIN_KERNEL_ADDRESS;
	pmap_kernel()->pm_ptab = firstaddr + VM_MIN_KERNEL_ADDRESS + NBPG;
a293 10
	firstaddr += NBPG;
	for (x = i386_btod(VM_MIN_KERNEL_ADDRESS);
	     x < i386_btod(VM_MIN_KERNEL_ADDRESS) + NKPDE; x++) {
		pd_entry_t *pde;
		pde = pmap_kernel()->pm_pdir + x;
		*pde = (firstaddr + x*NBPG) | PG_V | PG_KW;
	}
#else
	pmap_kernel()->pm_pdir =
	    (pd_entry_t *)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
d296 3
a298 2
	simple_lock_init(&pmap_kernel()->pm_lock);
	pmap_kernel()->pm_count = 1;
d300 1
a300 6
#if BSDVM_COMPAT
	/*
	 * Allocate all the submaps we need
	 */
#define	SYSMAP(c, p, v, n)	\
	v = (c)va; va += ((n)*NBPG); p = pte; pte += (n);
d302 6
a307 2
	va = virtual_avail;
	pte = pmap_pte(pmap_kernel(), va);
d309 3
a311 6
	SYSMAP(caddr_t		,CMAP1		,CADDR1	   ,1		)
	SYSMAP(caddr_t		,CMAP2		,CADDR2	   ,1		)
	SYSMAP(caddr_t		,XXX_mmap	,vmmap	   ,1		)
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,btoc(MSGBUFSIZE))
	SYSMAP(bootarg_t *	,bootargmap	,bootargp  ,btoc(bootargc))
	virtual_avail = va;
d314 4
a317 4
	/*
	 * Reserve pmap space for mapping physical pages during dump.
	 */
	virtual_avail = reserve_dumppages(virtual_avail);
d319 1
a319 14
	/* flawed, no mappings?? */
	if (ctob(physmem) > 31*1024*1024 && MAXKPDE != NKPDE) {
		vm_offset_t p;
		int i;

		p = virtual_avail;
		virtual_avail += (MAXKPDE-NKPDE+1) * NBPG;
		bzero((void *)p, (MAXKPDE-NKPDE+1) * NBPG);
		p = round_page(p);
		for (i = NKPDE; i < MAXKPDE; i++, p += NBPG)
			PTD[KPTDI+i] = (pd_entry_t)p |
			    PG_V | PG_KW;
	}
}
d321 10
a330 8
void
pmap_virtual_space(startp, endp)
	vm_offset_t *startp;
	vm_offset_t *endp;
{
	*startp = virtual_avail;
	*endp = virtual_end;
}
d333 1
a333 3
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
a334 6
void
pmap_init()
{
	vm_offset_t addr;
	vm_size_t s;
	int lcv;
d336 2
a337 2
	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE != 1");
d339 6
a344 12
	npages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) 
		npages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
	s = (vm_size_t) (sizeof(struct pv_entry) * npages + npages);
	s = round_page(s);
#if defined(UVM)
	addr = (vm_offset_t) uvm_km_zalloc(kernel_map, s);
	if (addr == NULL)
		panic("pmap_init");
#else
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
#endif
d346 2
a347 13
	/* allocate pv_entry stuff first */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.pvent = (struct pv_entry *) addr;
		addr = (vm_offset_t)(vm_physmem[lcv].pmseg.pvent +
			(vm_physmem[lcv].end - vm_physmem[lcv].start));
	}
	/* allocate attrs next */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.attrs = (char *) addr;
		addr = (vm_offset_t)(vm_physmem[lcv].pmseg.attrs +
			(vm_physmem[lcv].end - vm_physmem[lcv].start));
	}
	TAILQ_INIT(&pv_page_freelist);
a348 5
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %lx bytes (%x pgs)\n",
		       s, npages);
#endif
d350 3
a352 5
	/*
	 * Now it is safe to enable pv_entry recording.
	 */
	pmap_initialized = TRUE;
}
d354 6
a359 6
struct pv_entry *
pmap_alloc_pv()
{
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;
d361 3
a363 31
	if (pv_nfree == 0) {
#if defined(UVM)
		/* NOTE: can't lock kernel_map here */
		MALLOC(pvp, struct pv_page *, NBPG, M_VMPVENT, M_WAITOK);
#else
		pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG);
#endif
		if (pvp == 0)
			panic("pmap_alloc_pv: kmem_alloc() failed");
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		}
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
	return pv;
}
d365 3
a367 5
void
pmap_free_pv(pv)
	struct pv_entry *pv;
{
	register struct pv_page *pvp;
d369 2
a370 20
	pvp = (struct pv_page *) trunc_page(pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
#if defined(UVM)
		FREE((vaddr_t) pvp, M_VMPVENT);
#else
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
#endif
		break;
	}
}
d372 3
a374 8
void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;
	int bank, off;
d376 1
a376 1
	TAILQ_INIT(&pv_page_collectlist);
d378 3
a380 11
	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp, pvp_pgi.pgi_list);
			pv_nfree -= pvp->pvp_pgi.pgi_nfree;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}
d382 3
a384 2
	if (pv_page_collectlist.tqh_first == 0)
		return;
d386 3
a388 4
	if ((bank = vm_physseg_find(atop(0), &off)) == -1) { 
		printf("INVALID PA!");
		return;
	}
d390 2
a391 15
	for (ph = &vm_physmem[bank].pmseg.pvent[off]; ph; ph = ph->pv_next) {
		if (ph->pv_pmap == 0)
			continue;
		s = splimp();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *) trunc_page(pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: pgi_nfree inconsistent");
a392 9
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}
d394 3
a396 6
	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
#if defined(UVM)
		FREE((vaddr_t) pvp, M_VMPVENT);
#else
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
a397 2
	}
}
a398 8
__inline void
pmap_enter_pv(pmap, va, pv)
	register pmap_t pmap;
	vm_offset_t va;
	struct pv_entry *pv;
{	
	register struct pv_entry *npv;
	int s;
d400 3
a402 2
	if (!pmap_initialized)
		return;
d404 40
a443 6
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter_pv: pv %x: %x/%x/%x\n",
		       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
	s = splimp();
d445 2
a446 32
	if (pv->pv_pmap == NULL) {
		/*
		 * No entries yet, use header as the first entry
		 */
#ifdef DEBUG
		enter_stats.firstpv++;
#endif
		pv->pv_va = va;
		pv->pv_pmap = pmap;
		pv->pv_next = NULL;
	} else {
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
#ifdef DEBUG
		for (npv = pv; npv; npv = npv->pv_next)
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				panic("pmap_enter_pv: already in pv_tab");
#endif
		npv = pmap_alloc_pv();
		npv->pv_va = va;
		npv->pv_pmap = pmap;
		npv->pv_next = pv->pv_next;
		pv->pv_next = npv;
#ifdef DEBUG
		if (!npv->pv_next)
			enter_stats.secondpv++;
#endif
	}
	splx(s);
}
d448 3
a450 8
__inline void
pmap_remove_pv(pmap, va, pv)
	register pmap_t pmap;
	vm_offset_t va;
	struct pv_entry *pv;
{	
	register struct pv_entry *npv;
	int s;
d452 4
a455 5
	/*
	 * Remove from the PV table (raise IPL since we
	 * may be called at interrupt time).
	 */
	s = splimp();
d457 6
a462 24
	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		npv = pv->pv_next;
		if (npv) {
			*pv = *npv;
			pmap_free_pv(npv);
		} else
			pv->pv_pmap = NULL;
	} else {
		for (npv = pv->pv_next; npv; pv = npv, npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
		}
		if (npv) {
			pv->pv_next = npv->pv_next;
			pmap_free_pv(npv);
		}
	}
	splx(s);
d466 1
a466 2
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
d468 1
a468 2
 *	For now, VM is already on, we only need to map the
 *	specified memory.
d470 4
a473 4
vm_offset_t
pmap_map(va, spa, epa, prot)
	vm_offset_t va, spa, epa;
	int prot;
d475 4
a478 4

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%x, %x, %x, %x)\n", va, spa, epa, prot);
d480 2
a481 7

	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE, 0);
		va += NBPG;
		spa += NBPG;
	}
	return va;
d485 1
a485 6
 *	Create and return a physical map.
 *
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
d487 1
a487 6
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
 *
 * [ just allocate a ptd and mark it uninitialize -- should we track
 *   with a table which process has which ptd? -wfj ]
d489 3
a491 3
pmap_t
pmap_create(size)
	vm_size_t size;
d493 3
a495 5
	register pmap_t pmap;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create(%x)\n", size);
d497 3
a499 11

	/*
	 * Software use map does not need a pmap
	 */
	if (size)
		return NULL;

	pmap = (pmap_t) malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return pmap;
d503 4
a506 2
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
d508 4
a511 3
void
pmap_pinit(pmap)
	register struct pmap *pmap;
a512 16

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_pinit(%x)\n", pmap);
#endif

	/*
	 * No need to allocate page table space yet but we do need a
	 * valid page directory table.
	 */
#if defined(UVM)
	pmap->pm_pdir = (pd_entry_t *) uvm_km_zalloc(kernel_map, NBPG);
#else
	pmap->pm_pdir = (pd_entry_t *) kmem_alloc(kernel_map, NBPG);
#endif

d514 2
a515 2
	if (pmap->pm_pdir == NULL)
		panic("pmap_pinit: alloc failed");
a516 3
	/* wire in kernel global address entries */
	bcopy(&PTD[KPTDI], &pmap->pm_pdir[KPTDI], MAXKPDE *
	    sizeof(pd_entry_t));
d518 3
a520 3
	/* install self-referential address mapping entry */
	pmap->pm_pdir[PTDPTDI] = pmap_extract(pmap_kernel(),
	    (vm_offset_t)pmap->pm_pdir) | PG_V | PG_KW;
d522 2
a523 2
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
d527 3
a529 3
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
d531 4
a534 3
void
pmap_destroy(pmap)
	register pmap_t pmap;
d536 2
a537 3
	int count;

	if (pmap == NULL)
d540 1
a540 12
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%x)\n", pmap);
#endif

	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		free((caddr_t)pmap, M_VMPMAP);
	}
d544 4
a547 3
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
d549 4
a552 3
void
pmap_release(pmap)
	register struct pmap *pmap;
d554 6
d561 5
a565 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_release(%x)\n", pmap);
#endif
d567 8
a574 5
#ifdef DIAGNOSTICx
	/* sometimes 1, sometimes 0; could rearrange pmap_destroy */
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif
d576 8
a583 5
#if defined(UVM)
	uvm_km_free(kernel_map, (vaddr_t)pmap->pm_pdir, NBPG);
#else
	kmem_free(kernel_map, (vm_offset_t)pmap->pm_pdir, NBPG);
#endif
d587 1
a587 1
 *	Add a reference to the specified pmap.
d589 1735
d2325 3
a2327 2
pmap_reference(pmap)
	pmap_t pmap;
d2329 72
d2402 2
a2403 1
	if (pmap == NULL)
d2405 15
d2421 46
a2466 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%x)", pmap);
d2468 25
d2494 19
a2512 3
	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
d2515 7
d2523 178
a2700 2
pmap_activate(p)
	struct proc *p;
d2702 77
a2778 2
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d2780 5
a2784 4
	pcb->pcb_cr3 = pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_pdir);
	if (p == curproc)
		lcr3(pcb->pcb_cr3);
}
d2786 1
a2786 5
void
pmap_deactivate(p)
	struct proc *p;
{
}
d2789 1
a2789 4
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
d2791 1
d2793 1
a2793 1
pmap_remove(pmap, sva, eva)
d2795 2
a2796 1
	register vm_offset_t sva, eva;
d2798 14
a2811 4
	register pt_entry_t *pte;
	vm_offset_t pa;
	int bank, off;
	int flush = 0;
d2813 1
d2817 5
a2821 10
	/*
	 * We need to acquire a pointer to a page table page before entering
	 * the following loop.
	 */
	while (sva < eva) {
		pte = pmap_pte(pmap, sva);
		if (pte)
			break;
		sva = (sva & PD_MASK) + NBPD;
	}
d2823 8
a2830 10
	while (sva < eva) {
		/* only check once in a while */
		if ((sva & PT_MASK) == 0) {
			if (!pmap_pde_v(pmap_pde(pmap, sva))) {
				/* We can race ahead here, to the next pde. */
				sva += NBPD;
				pte += i386_btop(NBPD);
				continue;
			}
		}
d2832 2
a2833 4
		pte = pmap_pte(pmap, sva);
		if (pte == NULL) {
			/* We can race ahead here, to the next pde. */
			sva = (sva & PD_MASK) + NBPD;
a2834 1
		}
d2836 3
a2838 9
		if (!pmap_pte_v(pte)) {
#ifdef __GNUC__
			/*
			 * Scan ahead in a tight loop for the next used PTE in
			 * this page.  We don't scan the whole region here
			 * because we don't want to zero-fill unused page table
			 * pages.
			 */
			int n, m;
d2840 6
a2845 14
			n = min(eva - sva, NBPD - (sva & PT_MASK)) >> PGSHIFT;
			__asm __volatile(
			    "cld\n\trepe\n\tscasl\n\tje 1f\n\tincl %1\n\t1:"
			    : "=D" (pte), "=c" (m)
			    : "0" (pte), "1" (n), "a" (0));
			sva += (n - m) << PGSHIFT;
			if (!m)
				continue;
			/* Overshot. */
			--pte;
#else
			goto next;
#endif
		}
d2847 2
a2848 1
		flush = 1;
d2850 1
a2850 6
		/*
		 * Update statistics
		 */
		if (pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;
d2852 2
a2853 1
		pa = pmap_pte_pa(pte);
d2855 1
a2855 9
		/*
		 * Invalidate the PTEs.
		 * XXX: should cluster them up and invalidate as many
		 * as possible at once.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_REMOVE)
			printf("remove: inv pte at %x(%x) ", pte, *pte);
#endif
d2857 2
a2858 3
#ifdef needednotdone
reduce wiring count on page table pages as references drop
#endif
d2860 22
a2881 6
		if ((bank = vm_physseg_find(atop(pa), &off)) != -1) {
			vm_physmem[bank].pmseg.attrs[off] |=
				*pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, sva,
				&vm_physmem[bank].pmseg.pvent[off]);
		}
d2883 3
a2885 1
		*pte = 0;
d2887 5
a2891 2
#ifndef __GNUC__
	next:
d2893 10
a2902 2
		sva += NBPG;
		pte++;
d2904 1
a2904 3

	if (flush)
		pmap_update();
d2908 1
a2908 5
 *	Routine:	pmap_remove_all
 *	Function:
 *		Removes this physical page from
 *		all physical maps in which it resides.
 *		Reflects back modify bits to the pager.
a2909 9
void
pmap_remove_all(pa)
	vm_offset_t pa;
{
	struct pv_entry *ph, *pv, *npv;
	register pmap_t pmap;
	register pt_entry_t *pte;
	int bank, off;
	int s;
d2911 5
a2915 5
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_all(%x)", pa);
	/*pmap_pvdump(pa);*/
#endif
d2917 7
a2923 3
	bank = vm_physseg_find(atop(pa), &off);
	if (bank == -1)
		return;
d2925 2
a2926 2
	pv = ph = &vm_physmem[bank].pmseg.pvent[off];
	s = splimp();
d2928 3
a2930 12
	if (ph->pv_pmap == NULL) {
		splx(s);
		return;
	}

	while (pv) {
		pmap = pv->pv_pmap;
		pte = pmap_pte(pmap, pv->pv_va);

#ifdef DEBUG
		if (!pte || !pmap_pte_v(pte) || pmap_pte_pa(pte) != pa)
			panic("pmap_remove_all: bad mapping");
d2932 2
a2933 5

		/*
		 * Update statistics
		 */
		if (pmap_pte_w(pte))
d2935 7
a2941 10
		pmap->pm_stats.resident_count--;

		/*
		 * Invalidate the PTEs.
		 * XXX: should cluster them up and invalidate as many
		 * as possible at once.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_REMOVE)
			printf("remove: inv pte at %x(%x) ", pte, *pte);
a2942 3

#ifdef needednotdone
reduce wiring count on page table pages as references drop
d2944 5
a2948 13

		/*
		 * Update saved attributes for managed page
		 */
		vm_physmem[bank].pmseg.attrs[off] |= *pte & (PG_M | PG_U);
		*pte = 0;

		npv = pv->pv_next;
		if (pv == ph)
			ph->pv_pmap = NULL;
		else
			pmap_free_pv(pv);
		pv = npv;
d2950 1
a2950 3
	splx(s);

	pmap_update();
d2954 4
a2957 2
 *	Set the physical protection on the
 *	specified range of this map as requested.
d2959 1
d2961 2
a2962 4
pmap_protect(pmap, sva, eva, prot)
	register pmap_t pmap;
	vm_offset_t sva, eva;
	vm_prot_t prot;
d2964 4
a2967 3
	register pt_entry_t *pte;
	register int i386prot;
	int flush = 0;
d2969 2
a2970 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%x, %x, %x, %x)", pmap, sva, eva, prot);
#endif
d2972 9
a2980 4
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}
d2982 16
a2997 5
	if (prot & VM_PROT_WRITE)
		return;

	sva &= PG_FRAME;
	eva &= PG_FRAME;
d2999 1
d3001 2
a3002 2
	 * We need to acquire a pointer to a page table page before entering
	 * the following loop.
a3003 6
	while (sva < eva) {
		pte = pmap_pte(pmap, sva);
		if (pte)
			break;
		sva = (sva & PD_MASK) + NBPD;
	}
d3005 15
a3019 9
	while (sva < eva) {
		/* only check once in a while */
		if ((sva & PT_MASK) == 0) {
			if (!pmap_pde_v(pmap_pde(pmap, sva))) {
				/* We can race ahead here, to the next pde. */
				sva += NBPD;
				pte += i386_btop(NBPD);
				continue;
			}
d3021 2
d3024 8
a3031 9
		if (!pmap_pte_v(pte)) {
#ifdef __GNUC__
			/*
			 * Scan ahead in a tight loop for the next used PTE in
			 * this page.  We don't scan the whole region here
			 * because we don't want to zero-fill unused page table
			 * pages.
			 */
			int n, m;
d3033 13
a3045 13
			n = min(eva - sva, NBPD - (sva & PT_MASK)) >> PGSHIFT;
			__asm __volatile(
			    "cld\n\trepe\n\tscasl\n\tje 1f\n\tincl %1\n\t1:"
			    : "=D" (pte), "=c" (m)
			    : "0" (pte), "1" (n), "a" (0));
			sva += (n - m) << PGSHIFT;
			if (!m)
				continue;
			/* Overshot. */
			--pte;
#else
			goto next;
#endif
d3047 1
d3049 6
a3054 1
		flush = 1;
d3056 10
a3065 6
		i386prot = protection_codes[prot];
		if (sva < VM_MAXUSER_ADDRESS)	/* see also pmap_enter() */
			i386prot |= PG_u;
		else if (sva < VM_MAX_ADDRESS)
			i386prot |= PG_u | PG_RW;
		pmap_pte_set_prot(pte, i386prot);
d3067 1
a3067 6
#ifndef __GNUC__
	next:
#endif
		sva += NBPG;
		pte++;
	}
d3069 3
a3071 3
	if (flush)
		pmap_update();
}
d3073 5
a3077 26
/*
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte can not be reclaimed.
 *
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
	register pmap_t pmap;
	vm_offset_t va;
	register vm_offset_t pa;
	vm_prot_t prot;
	boolean_t wired;
	vm_prot_t access_type;
{
	register pt_entry_t *pte;
	register pt_entry_t npte;
	int bank, off;
	int flush = 0;
	boolean_t cacheable;
d3079 4
a3082 5
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%x, %x, %x, %x, %x)", pmap, va, pa, prot,
		    wired);
#endif
d3084 21
a3104 2
	if (pmap == NULL)
		return;
d3106 4
a3109 3
	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter: too big");
	/* also, should not muck with PTD va! */
d3111 25
a3135 6
#ifdef DEBUG
	if (pmap == pmap_kernel())
		enter_stats.kernel++;
	else
		enter_stats.user++;
#endif
a3136 2
	pte = pmap_pte(pmap, va);
	if (!pte) {
d3138 14
a3151 31
		 * Page Directory table entry not valid, we need a new PT page
		 *
		 * we want to vm_fault in a new zero-filled PT page for our
		 * use.   in order to do this, we want to call vm_fault()
		 * with the VA of where we want to put the PTE.   but in
		 * order to call vm_fault() we need to know which vm_map
		 * we are faulting in.    in the m68k pmap's this is easy
		 * since all PT pages live in one global vm_map ("pt_map")
		 * and we have a lot of virtual space we can use for the
		 * pt_map (since the kernel doesn't have to share its 4GB
		 * address space with processes).    but in the i386 port
		 * the kernel must live in the top part of the virtual 
		 * address space and PT pages live in their process' vm_map
		 * rather than a global one.    the problem is that we have
		 * no way of knowing which vm_map is the correct one to 
		 * fault on.
		 * 
		 * XXX: see NetBSD PR#1834 and Mycroft's posting to 
		 *	tech-kern on 7 Jan 1996.
		 *
		 * rather than always calling panic, we try and make an 
		 * educated guess as to which vm_map to use by using curproc.
		 * this is a workaround and may not fully solve the problem?
	 	 */
		struct vm_map *vmap;
		int rv;
		vm_offset_t v;

		if (curproc == NULL || curproc->p_vmspace == NULL ||
		    pmap != curproc->p_vmspace->vm_map.pmap)
			panic("ptdi %x", pmap->pm_pdir[PTDPTDI]);
d3153 2
a3154 1
		/* our guess about the vm_map was good!  fault it in.  */
d3156 5
a3160 29
		vmap = &curproc->p_vmspace->vm_map;
		v = trunc_page(vtopte(va));
#ifdef DEBUG
		printf("faulting in a pt page map %x va %x\n", vmap, v);
#endif
#if defined(UVM)
		rv = uvm_fault(vmap, v, 0, VM_PROT_READ|VM_PROT_WRITE);
#else
		rv = vm_fault(vmap, v, VM_PROT_READ|VM_PROT_WRITE, FALSE);
#endif
		if (rv != KERN_SUCCESS)
			panic("ptdi2 %x", pmap->pm_pdir[PTDPTDI]);
#if defined(UVM)
		/*
		 * XXX It is possible to get here from uvm_fault with vmap
		 * locked.  uvm_map_pageable requires it to be unlocked, so
		 * try to record the state of the lock, unlock it, and then
		 * after the call, reacquire the original lock.
		 * THIS IS A GROSS HACK!
		 */
		{
			int ls = lockstatus(&vmap->lock);

			if (ls)
				lockmgr(&vmap->lock, LK_RELEASE, (void *)0,
				    curproc);
			uvm_map_pageable(vmap, v, round_page(v+1), FALSE);
			if (ls)
				lockmgr(&vmap->lock, ls, (void *)0, curproc);
a3161 14
#else
		vm_map_pageable(vmap, v, round_page(v+1), FALSE);
#endif
		pte = pmap_pte(pmap, va);
		if (!pte) 
			panic("ptdi3 %x", pmap->pm_pdir[PTDPTDI]);
	}
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: pte %x, *pte %x ", pte, *pte);
#endif

	if (pmap_pte_v(pte)) {
		register vm_offset_t opa;
d3164 6
a3169 1
		 * Check for wiring change and adjust statistics.
d3171 7
a3177 17
		if ((wired && !pmap_pte_w(pte)) ||
		    (!wired && pmap_pte_w(pte))) {
			/*
			 * We don't worry about wiring PT pages as they remain
			 * resident as long as there are valid mappings in them.
			 * Hence, if a user page is wired, the PT page will be also.
			 */
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("enter: wiring change -> %x ", wired);
#endif
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
#ifdef DEBUG
			enter_stats.wchange++;
d3179 5
a3185 3
		flush = 1;
		opa = pmap_pte_pa(pte);

d3187 1
a3187 1
		 * Mapping has not changed, must be protection or wiring change.
d3189 5
a3193 3
		if (opa == pa) {
#ifdef DEBUG
			enter_stats.pwchange++;
d3195 5
a3199 1
			goto validate;
d3201 3
a3203 1
		
d3205 2
a3206 2
		 * Mapping has changed, invalidate old range and fall through to
		 * handle validating new mapping.
d3208 5
a3212 9
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: removing old mapping %x pa %x ", va, opa);
#endif
		if ((bank = vm_physseg_find(atop(opa), &off)) != -1) {
			vm_physmem[bank].pmseg.attrs[off] |=
				*pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, va,
				&vm_physmem[bank].pmseg.pvent[off]);
d3214 1
a3214 4
#ifdef DEBUG
		enter_stats.mchange++;
#endif
	} else {
d3216 3
a3218 1
		 * Increment counters
a3219 4
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
	}
d3221 3
a3223 12
	/*
	 * Enter on the PV list if part of our managed memory
	 */
	if ((bank = vm_physseg_find(atop(pa), &off)) != -1) {
#ifdef DEBUG     
		enter_stats.managed++;
#endif
		pmap_enter_pv(pmap, va, &vm_physmem[bank].pmseg.pvent[off]);
		cacheable = TRUE;
	} else if (pmap_initialized) {
#ifdef DEBUG
		enter_stats.unmanaged++;
d3225 6
a3230 6
		/*
		 * Assumption: if it is not part of our managed memory
		 * then it must be device memory which may be volatile.
		 */
		cacheable = FALSE;
	}
d3232 3
a3234 9
validate:
	/*
	 * Now validate mapping with desired protection/wiring.
	 * Assume uniform modified and referenced status for all
	 * I386 pages in a MACH page.
	 */
	npte = (pa & PG_FRAME) | protection_codes[prot] | PG_V;
	if (wired)
		npte |= PG_W;
a3235 3
	if (va < VM_MAXUSER_ADDRESS)	/* i.e. below USRSTACK */
		npte |= PG_u;
	else if (va < VM_MAX_ADDRESS)
d3237 1
a3237 4
		 * Page tables need to be user RW, for some reason, and the
		 * user area must be writable too.  Anything above
		 * VM_MAXUSER_ADDRESS is protected from user access by
		 * the user data and code segment descriptors, so this is OK.
d3239 4
a3242 10
		npte |= PG_u | PG_RW;

#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: new pte value %x ", npte);
#endif

	*pte = npte;
	if (flush)
		pmap_update();
d3246 1
a3246 1
 *      pmap_page_protect:
d3248 4
a3251 1
 *      Lower the permission for all mappings to a given page.
d3253 7
a3259 4
void
pmap_page_protect(phys, prot)
	vm_offset_t     phys;
	vm_prot_t       prot;
d3261 4
d3266 3
a3268 12
	switch (prot) {
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_copy_on_write(phys);
		break;
	case VM_PROT_ALL:
		break;
	default:
		pmap_remove_all(phys);
		break;
	}
}
d3270 4
a3273 14
/*
 *	Routine:	pmap_change_wiring
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 */
void
pmap_change_wiring(pmap, va, wired)
	register pmap_t pmap;
	vm_offset_t va;
	boolean_t wired;
{
	register pt_entry_t *pte;
d3275 3
a3277 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_change_wiring(%x, %x, %x)", pmap, va, wired);
#endif
d3279 3
a3281 3
	pte = pmap_pte(pmap, va);
	if (!pte)
		return;
a3282 1
#ifdef DEBUG
d3284 1
a3284 2
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
d3286 12
a3297 4
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_change_wiring: invalid PTE for %x ", va);
	}
d3300 4
a3303 8
	if ((wired && !pmap_pte_w(pte)) || (!wired && pmap_pte_w(pte))) {
		if (wired)
			pmap->pm_stats.wired_count++;
		else
			pmap->pm_stats.wired_count--;
		pmap_pte_set_w(pte, wired);
	}
}
d3305 1
a3305 12
/*
 *	Routine:	pmap_pte
 *	Function:
 *		Extract the page table entry associated
 *		with the given map/virtual_address pair.
 */
pt_entry_t *
pmap_pte(pmap, va)
	register pmap_t pmap;
	vm_offset_t va;
{
	pt_entry_t *ptp;
d3307 2
a3308 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pte(%x, %x) ->\n", pmap, va);
#endif
d3310 5
a3314 2
	if (!pmap || !pmap_pde_v(pmap_pde(pmap, va)))
		return NULL;
d3316 6
a3321 9
	if ((pmap->pm_pdir[PTDPTDI] & PG_FRAME) == (PTDpde & PG_FRAME) ||
	    pmap == pmap_kernel())
		/* current address space or kernel */
		ptp = PTmap;
	else {
		/* alternate address space */
		if ((pmap->pm_pdir[PTDPTDI] & PG_FRAME) != (APTDpde & PG_FRAME)) {
			APTDpde = pmap->pm_pdir[PTDPTDI];
			pmap_update();
a3322 2
		ptp = APTmap;
	}
d3324 4
a3327 2
	return ptp + i386_btop(va);
}
d3329 5
a3333 17
/*
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
 */
vm_offset_t
pmap_extract(pmap, va)
	register pmap_t pmap;
	vm_offset_t va;
{
	register pt_entry_t *pte;
	register vm_offset_t pa;

#ifdef DEBUGx
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%x, %x) -> ", pmap, va);
d3335 1
d3337 4
a3340 5
	pte = pmap_pte(pmap, va);
	if (!pte)
		return NULL;
	if (!pmap_pte_v(pte))
		return NULL;
d3342 9
a3350 4
	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("%x\n", pa);
a3351 2
	return pa | (va & ~PG_FRAME);
}
d3353 5
a3357 13
/*
 *	Copy the range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t dst_pmap, src_pmap;
	vm_offset_t dst_addr, src_addr;
	vm_size_t len;
{
d3359 7
a3365 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%x, %x, %x, %x, %x)",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
a3366 1
}
d3368 5
a3372 20
/*
 *	Routine:	pmap_collect
 *	Function:
 *		Garbage collects the physical map system for
 *		pages which are no longer used.
 *		Success need not be guaranteed -- that is, there
 *		may well be pages which are not referenced, but
 *		others may be collected.
 *	Usage:
 *		Called by the pageout daemon when pages are scarce.
 * [ needs to be written -wfj ]  XXXX
 */
void
pmap_collect(pmap)
	pmap_t pmap;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%x) ", pmap);
#endif
d3374 10
a3383 2
	if (pmap != pmap_kernel())
		return;
d3385 3
a3387 1
}
d3389 1
a3389 8
#if DEBUG
void
pmap_dump_pvlist(phys, m)
	vm_offset_t phys;
	char *m;
{
	register struct pv_entry *pv;
	int bank, off;
d3391 3
a3393 2
	if (!(pmapdebug & PDB_PARANOIA))
		return;
d3395 16
a3410 8
	if (!pmap_initialized)
		return;
	printf("%s %08x:", m, phys);
	bank = vm_physseg_find(atop(phys), &off);
	pv = &vm_physmem[bank].pmseg.pvent[off];
	if (pv->pv_pmap == NULL) {
		printf(" no mappings\n");
		return;
d3412 1
a3412 3
	for (; pv; pv = pv->pv_next)
		printf(" pmap %08x va %08x", pv->pv_pmap, pv->pv_va);
	printf("\n");
a3413 3
#else
#define	pmap_dump_pvlist(a,b)
#endif
d3416 4
a3419 2
 *	pmap_zero_page zeros the specified by mapping it into
 *	virtual memory and using bzero to clear its contents.
a3420 4
void
pmap_zero_page(phys)
	register vm_offset_t phys;
{
d3422 3
a3424 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%x)", phys);
#endif
d3426 6
a3431 5
	pmap_dump_pvlist(phys, "pmap_zero_page: phys");
	*CMAP2 = (phys & PG_FRAME) | PG_V | PG_KW /*| PG_N*/;
	pmap_update();
	bzero(CADDR2, NBPG);
}
d3434 1
a3434 3
 *	pmap_copy_page copies the specified page by mapping
 *	it into virtual memory and using bcopy to copy its
 *	contents.
a3435 17
void
pmap_copy_page(src, dst)
	register vm_offset_t src, dst;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%x, %x)", src, dst);
#endif

	pmap_dump_pvlist(src, "pmap_copy_page: src");
	pmap_dump_pvlist(dst, "pmap_copy_page: dst");
	*CMAP1 = (src & PG_FRAME) | PG_V | PG_KR;
	*CMAP2 = (dst & PG_FRAME) | PG_V | PG_KW /*| PG_N*/;
	pmap_update();
	bcopy(CADDR1, CADDR2, NBPG);
}
d3438 1
a3438 4
 *	Routine:	pmap_pageable
 *	Function:
 *		Make the specified pages (by pmap, offset)
 *		pageable (or not) as requested.
d3440 2
a3441 7
 *		A page which is not pageable may not take
 *		a fault; therefore, its page table entry
 *		must remain valid for the duration.
 *
 *		This routine is merely advisory; pmap_enter
 *		will specify that these pages are to be wired
 *		down (or not) as appropriate.
d3444 7
a3450 5
void
pmap_pageable(pmap, sva, eva, pageable)
	pmap_t pmap;
	vm_offset_t sva, eva;
	boolean_t pageable;
d3452 6
d3459 4
a3462 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pageable(%x, %x, %x, %x)",
		       pmap, sva, eva, pageable);
#endif
d3464 2
a3465 21
	/*
	 * If we are making a PT page pageable then all valid
	 * mappings must be gone from that page.  Hence it should
	 * be all zeros and there is no need to clean it.
	 * Assumption:
	 *	- PT pages have only one pv_table entry
	 *	- PT pages are the only single-page allocations
	 *	  between the user stack and kernel va's 
	 * See also pmap_enter & pmap_protect for rehashes of this...
	 */

	if (pageable &&
	    pmap == pmap_kernel() &&
	    sva >= VM_MAXUSER_ADDRESS && eva <= VM_MAX_ADDRESS &&
	    eva - sva == NBPG) {
		register vm_offset_t pa;
		register pt_entry_t *pte;
#ifdef DIAGNOSTIC
		int bank, off;
		register struct pv_entry *pv;
#endif
d3467 4
a3470 4
#ifdef DEBUG
		if ((pmapdebug & (PDB_FOLLOW|PDB_PTPAGE)) == PDB_PTPAGE)
			printf("pmap_pageable(%x, %x, %x, %x)",
			       pmap, sva, eva, pageable);
d3473 2
a3474 5
		pte = pmap_pte(pmap, sva);
		if (!pte)
			return;
		if (!pmap_pte_v(pte))
			return;
d3476 3
a3478 1
		pa = pmap_pte_pa(pte);
d3480 10
a3489 11
#ifdef DIAGNOSTIC
		if ((*pte & (PG_u | PG_RW)) != (PG_u | PG_RW))
			printf("pmap_pageable: unexpected pte=%x va %x\n",
				*pte, sva);
		if ((bank = vm_physseg_find(atop(pa), &off)) == -1)
			return;
		pv = &vm_physmem[bank].pmseg.pvent[off];
		if (pv->pv_va != sva || pv->pv_next) {
			printf("pmap_pageable: bad PT page va %x next %x\n",
			       pv->pv_va, pv->pv_next);
			return;
d3491 8
a3498 1
#endif
d3501 3
a3503 1
		 * Mark it unmodified to avoid pageout
a3504 1
		pmap_clear_modify(pa);
d3506 9
a3514 9
#ifdef needsomethinglikethis
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_pageable: PT page %x(%x) unmodified\n",
			       sva, *pmap_pte(pmap, sva));
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("pageable", sva);
#endif
	}
}
d3516 1
a3516 6
/*
 * Miscellaneous support routines follow
 */
void
i386_protection_init()
{
d3518 17
a3534 9
	protection_codes[VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE] = 0;
	protection_codes[VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE] =
	protection_codes[VM_PROT_NONE | VM_PROT_READ | VM_PROT_NONE] =
	protection_codes[VM_PROT_NONE | VM_PROT_READ | VM_PROT_EXECUTE] = PG_RO;
	protection_codes[VM_PROT_WRITE | VM_PROT_NONE | VM_PROT_NONE] =
	protection_codes[VM_PROT_WRITE | VM_PROT_NONE | VM_PROT_EXECUTE] =
	protection_codes[VM_PROT_WRITE | VM_PROT_READ | VM_PROT_NONE] =
	protection_codes[VM_PROT_WRITE | VM_PROT_READ | VM_PROT_EXECUTE] = PG_RW;
}
d3536 3
a3538 9
boolean_t
pmap_testbit(pa, setbits)
	register vm_offset_t pa;
	int setbits;
{
	register struct pv_entry *pv;
	register pt_entry_t *pte;
	int s;
	int bank, off;
d3540 4
a3543 4
	if ((bank = vm_physseg_find(atop(pa), &off)) == -1)
		return FALSE;
	pv = &vm_physmem[bank].pmseg.pvent[off];
	s = splimp();
d3545 22
a3566 6
	/*
	 * Check saved info first
	 */
	if (vm_physmem[bank].pmseg.attrs[off] & setbits) {
		splx(s);
		return TRUE;
d3570 5
a3574 2
	 * Not found, check current mappings returning
	 * immediately if found.
d3576 12
a3587 6
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
			if (*pte & setbits) {
				splx(s);
				return TRUE;
d3590 8
d3599 30
a3628 2
	splx(s);
	return FALSE;
d3632 4
a3635 3
 * Modify pte bits for all ptes corresponding to the given physical address.
 * We use `maskbits' rather than `clearbits' because we're always passing
 * constants and the latter would require an extra inversion at run-time.
d3637 4
a3640 4
void
pmap_changebit(pa, setbits, maskbits)
	register vm_offset_t pa;
	int setbits, maskbits;
d3642 2
a3643 3
	register struct pv_entry *pv;
	register pt_entry_t *pte;
	vm_offset_t va;
d3645 1
a3645 7
	int bank, off;

#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(%x, %x, %x)",
		       pa, setbits, ~maskbits);
#endif
d3647 4
a3650 4
	if ((bank = vm_physseg_find(atop(pa), &off)) == -1)
		return;
	pv = &vm_physmem[bank].pmseg.pvent[off];
	s = splimp();
d3653 1
a3653 1
	 * Clear saved attributes (modify, reference)
a3654 2
	if (~maskbits)
		vm_physmem[bank].pmseg.attrs[off] &= maskbits;
d3656 6
a3661 7
	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			va = pv->pv_va;
d3664 3
a3666 1
			 * XXX don't write protect pager mappings
a3667 7
			if ((PG_RO && setbits == PG_RO) ||
			    (PG_RW && maskbits == ~PG_RW)) {
#if defined(UVM)
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
					continue;
#else
				extern vm_offset_t pager_sva, pager_eva;
d3669 17
a3685 4
				if (va >= pager_sva && va < pager_eva)
					continue;
#endif
			}
d3687 6
a3692 2
			pte = pmap_pte(pv->pv_pmap, va);
			*pte = (*pte & maskbits) | setbits;
d3694 1
a3694 1
		pmap_update();
d3696 2
a3698 1
}
d3700 2
a3701 19
void
pmap_prefault(map, v, l)
	vm_map_t map;
	vm_offset_t v;
	vm_size_t l;
{
	vm_offset_t pv, pv2;

	for (pv = v; pv < v + l ; pv += ~PD_MASK + 1) {
		if (!pmap_pde_v(pmap_pde(map->pmap, pv))) {
			pv2 = trunc_page(vtopte(pv));
#if defined(UVM)
			uvm_fault(map, pv2, 0, VM_PROT_READ);
#else
			vm_fault(map, pv2, VM_PROT_READ, FALSE);
#endif
		}
		pv &= PD_MASK;
	}
d3705 8
d3714 3
a3716 2
pmap_pvdump(pa)
	vm_offset_t pa;
d3718 14
a3731 2
	register struct pv_entry *pv;
	int bank, off;
d3733 2
a3734 12
	printf("pa %x", pa);
	if ((bank = vm_physseg_find(atop(pa), &off)) == -1) {
		printf("INVALID PA!");
	} else {
		for (pv = &vm_physmem[bank].pmseg.pvent[off] ; pv ;
		     pv = pv->pv_next) {
			printf(" -> pmap %p, va %lx", pv->pv_pmap, pv->pv_va);
			       pads(pv->pv_pmap);
		}
	}
	printf(" ");
}
d3736 3
a3738 8
#ifdef notyet
void
pmap_check_wiring(str, va)
	char *str;
	vm_offset_t va;
{
	vm_map_entry_t entry;
	register int count, *pte;
d3740 1
a3740 4
	va = trunc_page(va);
	if (!pmap_pde_v(pmap_pde(pmap_kernel(), va)) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;
d3742 4
a3745 13
	if (!vm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %x not found\n", va);
		return;
	}
	count = 0;
	for (pte = (int *)va; pte < (int *)(va + NBPG); pte++)
		if (*pte)
			count++;
	if (entry->wired_count != count)
		printf("*%s*: %x: w%d/a%d\n",
		       str, va, entry->wired_count, count);
}
#endif
d3747 3
a3749 7
/* print address space of pmap*/
void
pads(pm)
	pmap_t pm;
{
	unsigned va, i, j;
	register pt_entry_t *pte;
d3751 10
a3760 16
	if (pm == pmap_kernel())
		return;
	for (i = 0; i < 1024; i++) 
		if (pmap_pde_v(&pm->pm_pdir[i]))
			for (j = 0; j < 1024 ; j++) {
				va = (i << PDSHIFT) | (j << PGSHIFT);
				if (pm == pmap_kernel() &&
				    va < VM_MIN_KERNEL_ADDRESS)
					continue;
				if (pm != pmap_kernel() &&
				    va > VM_MAX_ADDRESS)
					continue;
				pte = pmap_pte(pm, va);
				if (pmap_pte_v(pte)) 
					printf("%x:%x ", va, *pte); 
			}
@


1.36
log
@revision 1.31 patch was incorrect
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2000/02/22 19:27:48 deraadt Exp $	*/
d1123 2
a1124 2
		printf("pmap_enter(%x, %x, %x, %x, %x)",
		       pmap, va, pa, prot, wired);
@


1.35
log
@enlarge msgbuf, somewhat line netbsd did
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2000/01/29 21:41:51 mickey Exp $	*/
a292 1
#if !defined(UVM)
a305 1
#endif
@


1.34
log
@get usage of memory maps supplied from /boot.
gives two immidiate advances: memory holes support (two
best known are 640k-1M and 15M-16M), and bizaare apm segments placements.
/boot must be at least from 2.5 (well, some earlier might work too ;)
also, allows usage of new libkvm.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 1999/11/30 06:44:51 art Exp $	*/
d283 1
a283 2
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,
	    btoc(sizeof(struct msgbuf))		)
@


1.34.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d283 2
a284 1
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,btoc(MSGBUFSIZE))
@


1.34.2.2
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.39 2001/04/10 06:59:13 niklas Exp $	*/
/*	$NetBSD: pmap.c,v 1.84 2000/02/21 02:01:24 chs Exp $	*/
d5 3
d9 3
a11 2
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
d23 19
a41 15
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
d45 9
a53 24
 * pmap.c: i386 pmap module rewrite
 * Chuck Cranor <chuck@@ccrc.wustl.edu>
 * 11-Aug-97
 *
 * history of this pmap module: in addition to my own input, i used
 *    the following references for this rewrite of the i386 pmap:
 *
 * [1] the NetBSD i386 pmap.   this pmap appears to be based on the
 *     BSD hp300 pmap done by Mike Hibler at University of Utah.
 *     it was then ported to the i386 by William Jolitz of UUNET
 *     Technologies, Inc.   Then Charles M. Hannum of the NetBSD
 *     project fixed some bugs and provided some speed ups.
 *
 * [2] the FreeBSD i386 pmap.   this pmap seems to be the
 *     Hibler/Jolitz pmap, as modified for FreeBSD by John S. Dyson
 *     and David Greenman.
 *
 * [3] the Mach pmap.   this pmap, from CMU, seems to have migrated
 *     between several processors.   the VAX version was done by
 *     Avadis Tevanian, Jr., and Michael Wayne Young.    the i386
 *     version was done by Lance Berc, Mike Kupfer, Bob Baron,
 *     David Golub, and Richard Draves.    the alpha version was
 *     done by Alessandro Forin (CMU/Mach) and Chris Demetriou
 *     (NetBSD/alpha).
d56 25
a80 6
#ifdef __NetBSD__
#include "opt_cputype.h"
#include "opt_user_ldt.h"
#include "opt_lockdebug.h"
#include "opt_multiprocessor.h"
#endif
a85 1
#include <sys/pool.h>
d92 1
d94 1
a96 2
#include <machine/specialreg.h>
#include <machine/gdt.h>
a98 5
#ifdef __NetBSD__
#include <machine/isa_machdep.h>
#endif
#ifdef __OpenBSD__
#include <sys/msgbuf.h>
d100 4
a103 1
#endif
d106 2
a107 46
 * general info:
 *
 *  - for an explanation of how the i386 MMU hardware works see
 *    the comments in <machine/pte.h>.
 *
 *  - for an explanation of the general memory structure used by
 *    this pmap (including the recursive mapping), see the comments
 *    in <machine/pmap.h>.
 *
 * this file contains the code for the "pmap module."   the module's
 * job is to manage the hardware's virtual to physical address mappings.
 * note that there are two levels of mapping in the VM system:
 *
 *  [1] the upper layer of the VM system uses vm_map's and vm_map_entry's
 *      to map ranges of virtual address space to objects/files.  for
 *      example, the vm_map may say: "map VA 0x1000 to 0x22000 read-only
 *      to the file /bin/ls starting at offset zero."   note that
 *      the upper layer mapping is not concerned with how individual
 *      vm_pages are mapped.
 *
 *  [2] the lower layer of the VM system (the pmap) maintains the mappings
 *      from virtual addresses.   it is concerned with which vm_page is
 *      mapped where.   for example, when you run /bin/ls and start
 *      at page 0x1000 the fault routine may lookup the correct page
 *      of the /bin/ls file and then ask the pmap layer to establish
 *      a mapping for it.
 *
 * note that information in the lower layer of the VM system can be
 * thrown away since it can easily be reconstructed from the info
 * in the upper layer.
 *
 * data structures we use include:
 *
 *  - struct pmap: describes the address space of one thread
 *  - struct pv_entry: describes one <PMAP,VA> mapping of a PA
 *  - struct pv_head: there is one pv_head per managed page of
 *	physical memory.   the pv_head points to a list of pv_entry
 *	structures which describe all the <PMAP,VA> pairs that this
 *      page is mapped in.    this is critical for page based operations
 *      such as pmap_page_protect() [change protection on _all_ mappings
 *      of a page]
 *  - pv_page/pv_page_info: pv_entry's are allocated out of pv_page's.
 *      if we run out of pv_entry's we allocate a new pv_page and free
 *      its pv_entrys.
 * - pmap_remove_record: a list of virtual addresses whose mappings
 *	have been changed.   used for TLB flushing.
d109 92
d203 1
a203 61
 * memory allocation
 *
 *  - there are three data structures that we must dynamically allocate:
 *
 * [A] new process' page directory page (PDP)
 *	- plan 1: done at pmap_pinit() we use
 *	  uvm_km_alloc(kernel_map, NBPG)  [fka kmem_alloc] to do this
 *	  allocation.
 *
 * if we are low in free physical memory then we sleep in
 * uvm_km_alloc -- in this case this is ok since we are creating
 * a new pmap and should not be holding any locks.
 *
 * if the kernel is totally out of virtual space
 * (i.e. uvm_km_alloc returns NULL), then we panic.
 *
 * XXX: the fork code currently has no way to return an "out of
 * memory, try again" error code since uvm_fork [fka vm_fork]
 * is a void function.
 *
 * [B] new page tables pages (PTP)
 * 	- plan 1: call uvm_pagealloc()
 * 		=> success: zero page, add to pm_pdir
 * 		=> failure: we are out of free vm_pages
 * 	- plan 2: using a linked LIST of active pmaps we attempt
 * 	to "steal" a PTP from another process.   we lock
 * 	the target pmap with simple_lock_try so that if it is
 * 	busy we do not block.
 * 		=> success: remove old mappings, zero, add to pm_pdir
 * 		=> failure: highly unlikely
 * 	- plan 3: panic
 *
 * note: for kernel PTPs, we start with NKPTP of them.   as we map
 * kernel memory (at uvm_map time) we check to see if we've grown
 * the kernel pmap.   if so, we call the optional function
 * pmap_growkernel() to grow the kernel PTPs in advance.
 *
 * [C] pv_entry structures
 *	- plan 1: try to allocate one off the free list
 *		=> success: done!
 *		=> failure: no more free pv_entrys on the list
 *	- plan 2: try to allocate a new pv_page to add a chunk of
 *	pv_entrys to the free list
 *		[a] obtain a free, unmapped, VA in kmem_map.  either
 *		we have one saved from a previous call, or we allocate
 *		one now using a "vm_map_lock_try" in uvm_map
 *		=> success: we have an unmapped VA, continue to [b]
 *		=> failure: unable to lock kmem_map or out of VA in it.
 *			move on to plan 3.
 *		[b] allocate a page in kmem_object for the VA
 *		=> success: map it in, free the pv_entry's, DONE!
 *		=> failure: kmem_object locked, no free vm_pages, etc.
 *			save VA for later call to [a], go to plan 3.
 *	- plan 3: using the pv_entry/pv_head lists find a pv_entry
 *		structure that is part of a non-kernel lockable pmap
 *		and "steal" that pv_entry by removing the mapping
 *		and reusing that pv_entry.
 *		=> success: done
 *		=> failure: highly unlikely: unable to lock and steal
 *			pv_entry
 *	- plan 4: we panic.
d205 4
d211 2
a212 26
 * locking
 *
 * we have the following locks that we must contend with:
 *
 * "normal" locks:
 *
 *  - pmap_main_lock
 *    this lock is used to prevent deadlock and/or provide mutex
 *    access to the pmap system.   most operations lock the pmap
 *    structure first, then they lock the pv_lists (if needed).
 *    however, some operations such as pmap_page_protect lock
 *    the pv_lists and then lock pmaps.   in order to prevent a
 *    cycle, we require a mutex lock when locking the pv_lists
 *    first.   thus, the "pmap = >pv_list" lockers must gain a
 *    read-lock on pmap_main_lock before locking the pmap.   and
 *    the "pv_list => pmap" lockers must gain a write-lock on
 *    pmap_main_lock before locking.    since only one thread
 *    can write-lock a lock at a time, this provides mutex.
 *
 * "simple" locks:
 *
 * - pmap lock (per pmap, part of uvm_object)
 *   this lock protects the fields in the pmap structure including
 *   the non-kernel PDEs in the PDP, and the PTEs.  it also locks
 *   in the alternate PTE space (since that is determined by the
 *   entry in the PDP).
d214 6
a219 25
 * - pvh_lock (per pv_head)
 *   this lock protects the pv_entry list which is chained off the
 *   pv_head structure for a specific managed PA.   it is locked
 *   when traversing the list (e.g. adding/removing mappings,
 *   syncing R/M bits, etc.)
 *
 * - pvalloc_lock
 *   this lock protects the data structures which are used to manage
 *   the free list of pv_entry structures.
 *
 * - pmaps_lock
 *   this lock protects the list of active pmaps (headed by "pmaps").
 *   we lock it when adding or removing pmaps from this list.
 *
 * - pmap_copy_page_lock
 *   locks the tmp kernel PTE mappings we used to copy data
 *
 * - pmap_zero_page_lock
 *   locks the tmp kernel PTE mapping we use to zero a page
 *
 * - pmap_tmpptp_lock
 *   locks the tmp kernel PTE mapping we use to look at a PTP
 *   in another process
 *
 * XXX: would be nice to have per-CPU VAs for the above 4
d222 15
a236 3
/*
 * locking data structures
 */
d238 2
a239 7
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
struct lock pmap_main_lock;
simple_lock_data_t pvalloc_lock;
simple_lock_data_t pmaps_lock;
simple_lock_data_t pmap_copy_page_lock;
simple_lock_data_t pmap_zero_page_lock;
simple_lock_data_t pmap_tmpptp_lock;
d241 4
a244 4
#define PMAP_MAP_TO_HEAD_LOCK() \
     spinlockmgr(&pmap_main_lock, LK_SHARED, (void *) 0)
#define PMAP_MAP_TO_HEAD_UNLOCK() \
     spinlockmgr(&pmap_main_lock, LK_RELEASE, (void *) 0)
d246 8
a253 4
#define PMAP_HEAD_TO_MAP_LOCK() \
     spinlockmgr(&pmap_main_lock, LK_EXCLUSIVE, (void *) 0)
#define PMAP_HEAD_TO_MAP_UNLOCK() \
     spinlockmgr(&pmap_main_lock, LK_RELEASE, (void *) 0)
d255 7
d263 2
a264 7

#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */

#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

d267 2
a268 3
/*
 * global data structures
 */
d270 6
a275 1
struct pmap kernel_pmap_store;	/* the kernel's pmap (proc0) */
d277 2
a278 6
/*
 * nkpde is the number of kernel PTPs allocated for the kernel at
 * boot time (NKPTP is a compile time override).   this number can
 * grow dynamically as needed (but once allocated, we never free
 * kernel PTPs).
 */
d280 6
a285 3
int nkpde = NKPTP;
#ifdef NKPDE
#error "obsolete NKPDE: use NKPTP"
d288 4
a291 4
/*
 * pmap_pg_g: if our processor supports PG_G in the PTE then we
 * set pmap_pg_g to PG_G (otherwise it is zero).
 */
d293 16
a308 1
int pmap_pg_g = 0;
d310 8
a317 10
/*
 * i386 physical memory comes in a big contig chunk with a small
 * hole toward the front of it...  the following 4 paddr_t's
 * (shared with machdep.c) describe the physical address space
 * of this machine.
 */
paddr_t avail_start;	/* PA of first available physical page */
paddr_t avail_end;	/* PA of last available physical page */
paddr_t hole_start;	/* PA of start of "hole" */
paddr_t hole_end;	/* PA of end of "hole" */
d320 3
a322 1
 * other data structures
d324 6
d331 2
a332 2
static pt_entry_t protection_codes[8];     /* maps MI prot to i386 prot code */
static boolean_t pmap_initialized = FALSE; /* pmap_init done yet? */
d334 12
a345 6
/*
 * the following two vaddr_t's are used during system startup
 * to keep track of how much of the kernel's VM space we have used.
 * once the system is started, the management of the remaining kernel
 * VM space is turned over to the kernel_map vm_map.
 */
d347 13
a359 2
static vaddr_t virtual_avail;	/* VA of first free KVA */
static vaddr_t virtual_end;	/* VA of last free KVA */
d361 5
d367 5
a371 3
/*
 * pv_page management structures: locked by pvalloc_lock
 */
d373 6
a378 6
TAILQ_HEAD(pv_pagelist, pv_page);
static struct pv_pagelist pv_freepages;	/* list of pv_pages with free entrys */
static struct pv_pagelist pv_unusedpgs; /* list of unused pv_pages */
static int pv_nfpvents;			/* # of free pv entries */
static struct pv_page *pv_initpage;	/* bootstrap page from kernel_map */
static vaddr_t pv_cachedva;		/* cached VA for later use */
d380 31
a410 3
#define PVE_LOWAT (PVE_PER_PVPAGE / 2)	/* free pv_entry low water mark */
#define PVE_HIWAT (PVE_LOWAT + (PVE_PER_PVPAGE * 2))
					/* high water mark */
d412 5
a416 3
/*
 * linked list of all non-kernel pmaps
 */
d418 20
a437 2
static struct pmap_head pmaps;
static struct pmap *pmaps_hand = NULL;	/* used by pmap_steal_ptp */
d439 8
a446 3
/*
 * pool that pmap structures are allocated from
 */
d448 1
a448 1
struct pool pmap_pmap_pool;
d450 11
a460 3
/*
 * special VAs and the PTEs that map them
 */
d462 2
a463 3
static pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte;
static caddr_t csrcp, cdstp, zerop, ptpp;
caddr_t vmmap; /* XXX: used by mem.c... it should really uvm_map_reserve it */
d465 4
a468 3
#ifdef __NetBSD__
extern vaddr_t msgbuf_vaddr;
extern paddr_t msgbuf_paddr;
d470 15
a484 2
extern vaddr_t idt_vaddr;			/* we allocate IDT early */
extern paddr_t idt_paddr;
d486 9
d496 6
a501 3
#if defined(I586_CPU)
/* stuff to fix the pentium f00f bug */
extern vaddr_t pentium_idt_vaddr;
d503 2
d506 8
d515 2
a516 3
/*
 * local prototypes
 */
d518 6
a523 40
static struct pv_entry	*pmap_add_pvpage __P((struct pv_page *, boolean_t));
static struct vm_page	*pmap_alloc_ptp __P((struct pmap *, int, boolean_t));
static struct pv_entry	*pmap_alloc_pv __P((struct pmap *, int)); /* see codes below */
#define ALLOCPV_NEED	0	/* need PV now */
#define ALLOCPV_TRY	1	/* just try to allocate, don't steal */
#define ALLOCPV_NONEED	2	/* don't need PV, just growing cache */
static struct pv_entry	*pmap_alloc_pvpage __P((struct pmap *, int));
static void		 pmap_enter_pv __P((struct pv_head *,
					    struct pv_entry *, struct pmap *,
					    vaddr_t, struct vm_page *));
static void		 pmap_free_pv __P((struct pmap *, struct pv_entry *));
static void		 pmap_free_pvs __P((struct pmap *, struct pv_entry *));
static void		 pmap_free_pv_doit __P((struct pv_entry *));
static void		 pmap_free_pvpage __P((void));
static struct vm_page	*pmap_get_ptp __P((struct pmap *, int, boolean_t));
static boolean_t	 pmap_is_curpmap __P((struct pmap *));
static pt_entry_t	*pmap_map_ptes __P((struct pmap *));
static struct pv_entry	*pmap_remove_pv __P((struct pv_head *, struct pmap *,
					     vaddr_t));
static boolean_t	 pmap_remove_pte __P((struct pmap *, struct vm_page *,
					      pt_entry_t *, vaddr_t));
static void		 pmap_remove_ptes __P((struct pmap *,
					       struct pmap_remove_record *,
					       struct vm_page *, vaddr_t,
					       vaddr_t, vaddr_t));
static struct vm_page	*pmap_steal_ptp __P((struct uvm_object *,
					     vaddr_t));
static vaddr_t		 pmap_tmpmap_pa __P((paddr_t));
static pt_entry_t	*pmap_tmpmap_pvepte __P((struct pv_entry *));
static void		 pmap_tmpunmap_pa __P((void));
static void		 pmap_tmpunmap_pvepte __P((struct pv_entry *));
static boolean_t	 pmap_transfer_ptes __P((struct pmap *,
					 struct pmap_transfer_location *,
					 struct pmap *,
					 struct pmap_transfer_location *,
					 int, boolean_t));
static boolean_t	 pmap_try_steal_pv __P((struct pv_head *,
						struct pv_entry *,
						struct pv_entry *));
static void		pmap_unmap_ptes __P((struct pmap *));
d525 32
a556 2
void			pmap_pinit __P((pmap_t));
void			pmap_release __P((pmap_t));
d558 8
a565 3
/*
 * p m a p   i n l i n e   h e l p e r   f u n c t i o n s
 */
d567 5
a571 4
/*
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
 */
d573 24
a596 6
__inline static boolean_t
pmap_is_curpmap(pmap)
	struct pmap *pmap;
{
	return((pmap == pmap_kernel()) ||
	       (pmap->pm_pdirpa == (paddr_t) rcr3()));
d600 2
a601 1
 * pmap_tmpmap_pa: map a page in for tmp usage
d603 2
a604 1
 * => returns with pmap_tmpptp_lock held
d606 5
d612 3
a614 8
__inline static vaddr_t
pmap_tmpmap_pa(pa)
	paddr_t pa;
{
	simple_lock(&pmap_tmpptp_lock);
#if defined(DIAGNOSTIC)
	if (*ptp_pte)
		panic("pmap_tmpmap_pa: ptp_pte in use?");
d616 7
a622 2
	*ptp_pte = PG_V | PG_RW | pa;		/* always a new mapping */
	return((vaddr_t)ptpp);
d626 10
a635 1
 * pmap_tmpunmap_pa: unmap a tmp use page (undoes pmap_tmpmap_pa)
d637 2
a638 1
 * => we release pmap_tmpptp_lock
d640 5
d646 3
a648 6
__inline static void
pmap_tmpunmap_pa()
{
#if defined(DIAGNOSTIC)
	if (!pmap_valid_entry(*ptp_pte))
		panic("pmap_tmpunmap_pa: our pte invalid?");
d650 11
a660 3
	*ptp_pte = 0;		/* zap! */
	pmap_update_pg((vaddr_t)ptpp);
	simple_unlock(&pmap_tmpptp_lock);
d664 2
a665 4
 * pmap_tmpmap_pvepte: get a quick mapping of a PTE for a pv_entry
 *
 * => do NOT use this on kernel mappings [why?  because pv_ptp may be NULL]
 * => we may grab pmap_tmpptp_lock and return with it held
d667 19
a686 4
__inline static pt_entry_t *
pmap_tmpmap_pvepte(pve)
	struct pv_entry *pve;
{
d688 2
a689 2
	if (pve->pv_pmap == pmap_kernel())
		panic("pmap_tmpmap_pvepte: attempt to map kernel");
d691 3
d695 3
a697 3
	/* is it current pmap?  use direct mapping... */
	if (pmap_is_curpmap(pve->pv_pmap))
		return(vtopte(pve->pv_va));
d699 2
a700 2
	return(((pt_entry_t *)pmap_tmpmap_pa(VM_PAGE_TO_PHYS(pve->pv_ptp)))
	       + ptei((unsigned)pve->pv_va));
d704 3
a706 3
 * pmap_tmpunmap_pvepte: release a mapping obtained with pmap_tmpmap_pvepte
 *
 * => we will release pmap_tmpptp_lock if we hold it
d708 5
d714 1
a714 6
__inline static void
pmap_tmpunmap_pvepte(pve)
	struct pv_entry *pve;
{
	/* was it current pmap?   if so, return */
	if (pmap_is_curpmap(pve->pv_pmap))
d717 12
a728 1
	pmap_tmpunmap_pa();
d732 3
a734 4
 * pmap_map_ptes: map a pmap's PTEs into KVM and lock them in
 *
 * => we lock enough pmaps to keep things locked in
 * => must be undone with pmap_unmap_ptes before returning
d736 3
a738 4

__inline static pt_entry_t *
pmap_map_ptes(pmap)
	struct pmap *pmap;
a739 1
	pd_entry_t opde;
d741 4
a744 4
	/* the kernel's pmap is always accessible */
	if (pmap == pmap_kernel()) {
		return(PTE_BASE);
	}
d746 5
a750 5
	/* if curpmap then we are always mapped */
	if (pmap_is_curpmap(pmap)) {
		simple_lock(&pmap->pm_obj.vmobjlock);
		return(PTE_BASE);
	}
d752 5
a756 17
	/* need to lock both curpmap and pmap: use ordered locking */
	if ((unsigned) pmap < (unsigned) curpcb->pcb_pmap) {
		simple_lock(&pmap->pm_obj.vmobjlock);
		simple_lock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
	} else {
		simple_lock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
		simple_lock(&pmap->pm_obj.vmobjlock);
	}

	/* need to load a new alternate pt space into curpmap? */
	opde = *APDP_PDE;
	if (!pmap_valid_entry(opde) || (opde & PG_FRAME) != pmap->pm_pdirpa) {
		*APDP_PDE = (pd_entry_t) (pmap->pm_pdirpa | PG_RW | PG_V);
		if (pmap_valid_entry(opde))
			pmap_update();
	}
	return(APTE_BASE);
d760 1
a760 1
 * pmap_unmap_ptes: unlock the PTE mapping of "pmap"
d762 2
a763 1467

__inline static void
pmap_unmap_ptes(pmap)
	struct pmap *pmap;
{
	if (pmap == pmap_kernel()) {
		return;
	}
	if (pmap_is_curpmap(pmap)) {
		simple_unlock(&pmap->pm_obj.vmobjlock);
	} else {
		simple_unlock(&pmap->pm_obj.vmobjlock);
		simple_unlock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
	}
}

/*
 * p m a p   k e n t e r   f u n c t i o n s
 *
 * functions to quickly enter/remove pages from the kernel address
 * space.   pmap_kremove/pmap_kenter_pgs are exported to MI kernel.
 * we make use of the recursive PTE mappings.
 */

/*
 * pmap_kenter_pa: enter a kernel mapping without R/M (pv_entry) tracking
 *
 * => no need to lock anything, assume va is already allocated
 * => should be faster than normal pmap enter function
 */

void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	pt_entry_t *pte, opte;

	pte = vtopte(va);
	opte = *pte;
	*pte = pa | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) |
		PG_V | pmap_pg_g;	/* zap! */
	if (pmap_valid_entry(opte))
		pmap_update_pg(va);
}

/*
 * pmap_kremove: remove a kernel mapping(s) without R/M (pv_entry) tracking
 *
 * => no need to lock anything
 * => caller must dispose of any vm_page mapped in the va range
 * => note: not an inline function
 * => we assume the va is page aligned and the len is a multiple of NBPG
 * => we assume kernel only unmaps valid addresses and thus don't bother
 *    checking the valid bit before doing TLB flushing
 */

void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	pt_entry_t *pte;

	len >>= PAGE_SHIFT;
	for ( /* null */ ; len ; len--, va += NBPG) {
		pte = vtopte(va);
#ifdef DIAGNOSTIC
		if (*pte & PG_PVLIST)
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx\n",
			      va);
#endif
		*pte = 0;		/* zap! */
#if defined(I386_CPU)
		if (cpu_class != CPUCLASS_386)
#endif
			pmap_update_pg(va);
	}
#if defined(I386_CPU)
	if (cpu_class == CPUCLASS_386)
		pmap_update();
#endif
}

/*
 * pmap_kenter_pgs: enter in a number of vm_pages
 */

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	pt_entry_t *pte, opte;
	int lcv;
	vaddr_t tva;
#if defined(I386_CPU)
	boolean_t need_update = FALSE;
#endif

	for (lcv = 0 ; lcv < npgs ; lcv++) {
		tva = va + lcv * NBPG;
		pte = vtopte(tva);
		opte = *pte;
		*pte = VM_PAGE_TO_PHYS(pgs[lcv]) | PG_RW | PG_V | pmap_pg_g;
#if defined(I386_CPU)
		if (cpu_class == CPUCLASS_386) {
			if (pmap_valid_entry(opte))
				need_update = TRUE;
			continue;
		}
#endif
		if (pmap_valid_entry(opte))
			pmap_update_pg(tva);
	}
#if defined(I386_CPU)
	if (need_update && cpu_class == CPUCLASS_386)
		pmap_update();
#endif
}

/*
 * p m a p   i n i t   f u n c t i o n s
 *
 * pmap_bootstrap and pmap_init are called during system startup
 * to init the pmap module.   pmap_bootstrap() does a low level
 * init just to get things rolling.   pmap_init() finishes the job.
 */

/*
 * pmap_bootstrap: get the system in a state where it can run with VM
 *	properly enabled (called before main()).   the VM system is
 *      fully init'd later...
 *
 * => on i386, locore.s has already enabled the MMU by allocating
 *	a PDP for the kernel, and nkpde PTP's for the kernel.
 * => kva_start is the first free virtual address in kernel space
 * => we make use of the global vars from machdep.c:
 *	avail_start, avail_end, hole_start, hole_end
 */

void
pmap_bootstrap(kva_start)
	vaddr_t kva_start;
{
	struct pmap *kpm;
	vaddr_t kva;
	pt_entry_t *pte;
#ifdef __NetBSD__
	int first16q;
#endif

	/*
	 * set the page size (default value is 4K which is ok)
	 */

	uvm_setpagesize();

	/*
	 * a quick sanity check
	 */

	if (PAGE_SIZE != NBPG)
		panic("pmap_bootstrap: PAGE_SIZE != NBPG");

	/*
	 * use the very last page of physical memory for the message buffer
	 */

	avail_end -= i386_round_page(MSGBUFSIZE);
#ifdef __NetBSD__
	msgbuf_paddr = avail_end;
#endif

#ifdef __OpenBSD__
	/*
	 * The arguments passed in from /boot needs space too.
	 */
	avail_end -= i386_round_page(bootargc);
#endif

	/*
	 * set up our local static global vars that keep track of the
	 * usage of KVM before kernel_map is set up
	 */

	virtual_avail = kva_start;		/* first free KVA */
	virtual_end = VM_MAX_KERNEL_ADDRESS;	/* last KVA */

	/*
	 * set up protection_codes: we need to be able to convert from
	 * a MI protection code (some combo of VM_PROT...) to something
	 * we can jam into a i386 PTE.
	 */

	protection_codes[VM_PROT_NONE] = 0;  			/* --- */
	protection_codes[VM_PROT_EXECUTE] = PG_RO;		/* --x */
	protection_codes[VM_PROT_READ] = PG_RO;			/* -r- */
	protection_codes[VM_PROT_READ|VM_PROT_EXECUTE] = PG_RO;	/* -rx */
	protection_codes[VM_PROT_WRITE] = PG_RW;		/* w-- */
	protection_codes[VM_PROT_WRITE|VM_PROT_EXECUTE] = PG_RW;/* w-x */
	protection_codes[VM_PROT_WRITE|VM_PROT_READ] = PG_RW;	/* wr- */
	protection_codes[VM_PROT_ALL] = PG_RW;			/* wrx */

	/*
	 * now we init the kernel's pmap
	 *
	 * the kernel pmap's pm_obj is not used for much.   however, in
	 * user pmaps the pm_obj contains the list of active PTPs.
	 * the pm_obj currently does not have a pager.   it might be possible
	 * to add a pager that would allow a process to read-only mmap its
	 * own page tables (fast user level vtophys?).   this may or may not
	 * be useful.
	 */

	kpm = pmap_kernel();
	simple_lock_init(&kpm->pm_obj.vmobjlock);
	kpm->pm_obj.pgops = NULL;
	TAILQ_INIT(&kpm->pm_obj.memq);
	kpm->pm_obj.uo_npages = 0;
	kpm->pm_obj.uo_refs = 1;
	bzero(&kpm->pm_list, sizeof(kpm->pm_list));  /* pm_list not used */
	kpm->pm_pdir = (pd_entry_t *)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
	kpm->pm_pdirpa = (u_int32_t) proc0.p_addr->u_pcb.pcb_cr3;
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count =
		i386_btop(kva_start - VM_MIN_KERNEL_ADDRESS);

	/*
	 * the above is just a rough estimate and not critical to the proper
	 * operation of the system.
	 */

	curpcb->pcb_pmap = kpm;	/* proc0's pcb */

	/*
	 * enable global TLB entries if they are supported
	 */

	if (cpu_feature & CPUID_PGE) {
		lcr4(rcr4() | CR4_PGE);	/* enable hardware (via %cr4) */
		pmap_pg_g = PG_G;		/* enable software */

		/* add PG_G attribute to already mapped kernel pages */
		for (kva = VM_MIN_KERNEL_ADDRESS ; kva < virtual_avail ;
		     kva += NBPG)
			if (pmap_valid_entry(PTE_BASE[i386_btop(kva)]))
				PTE_BASE[i386_btop(kva)] |= PG_G;
	}

	/*
	 * now we allocate the "special" VAs which are used for tmp mappings
	 * by the pmap (and other modules).    we allocate the VAs by advancing
	 * virtual_avail (note that there are no pages mapped at these VAs).
	 * we find the PTE that maps the allocated VA via the linear PTE
	 * mapping.
	 */

	pte = PTE_BASE + i386_btop(virtual_avail);

	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;  /* allocate */
	virtual_avail += NBPG; pte++;			     /* advance */

	cdstp = (caddr_t) virtual_avail;  cdst_pte = pte;
	virtual_avail += NBPG; pte++;

	zerop = (caddr_t) virtual_avail;  zero_pte = pte;
	virtual_avail += NBPG; pte++;

	ptpp = (caddr_t) virtual_avail;  ptp_pte = pte;
	virtual_avail += NBPG; pte++;

	/* XXX: vmmap used by mem.c... should be uvm_map_reserve */
	vmmap = (char *)virtual_avail;			/* don't need pte */
	virtual_avail += NBPG; pte++;

#ifdef __NetBSD
	msgbuf_vaddr = virtual_avail;			/* don't need pte */
#endif
#ifdef __OpenBSD__
	msgbufp = (struct msgbuf *)virtual_avail;	/* don't need pte */
#endif
	virtual_avail += round_page(MSGBUFSIZE); pte++;

#ifdef __NetBSD__
	idt_vaddr = virtual_avail;			/* don't need pte */
	virtual_avail += NBPG; pte++;
	avail_end -= NBPG;
	idt_paddr = avail_end;

#if defined(I586_CPU)
	/* pentium f00f bug stuff */
	pentium_idt_vaddr = virtual_avail;		/* don't need pte */
	virtual_avail += NBPG; pte++;
#endif
#endif

#ifdef __OpenBSD__
	bootargp = (bootarg_t *)virtual_avail;
	virtual_avail += round_page(bootargc); pte++;
#endif

	/*
	 * now we reserve some VM for mapping pages when doing a crash dump
	 */

	virtual_avail = reserve_dumppages(virtual_avail);

	/*
	 * init the static-global locks and global lists.
	 */

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
	spinlockinit(&pmap_main_lock, "pmaplk", 0);
	simple_lock_init(&pvalloc_lock);
	simple_lock_init(&pmaps_lock);
	simple_lock_init(&pmap_copy_page_lock);
	simple_lock_init(&pmap_zero_page_lock);
	simple_lock_init(&pmap_tmpptp_lock);
#endif
	LIST_INIT(&pmaps);
	TAILQ_INIT(&pv_freepages);
	TAILQ_INIT(&pv_unusedpgs);

	/*
	 * initialize the pmap pool.
	 */

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
		  0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

#ifdef __NetBSD__
	/*
	 * we must call uvm_page_physload() after we are done playing with
	 * virtual_avail but before we call pmap_steal_memory.  [i.e. here]
	 * this call tells the VM system how much physical memory it
	 * controls.  If we have 16M of RAM or less, just put it all on
	 * the default free list.  Otherwise, put the first 16M of RAM
	 * on a lower priority free list (so that all of the ISA DMA'able
	 * memory won't be eaten up first-off).
	 */

	if (avail_end <= (16 * 1024 * 1024))
		first16q = VM_FREELIST_DEFAULT;
	else
		first16q = VM_FREELIST_FIRST16;

	if (avail_start < hole_start)   /* any free memory before the hole? */
		uvm_page_physload(atop(avail_start), atop(hole_start),
				  atop(avail_start), atop(hole_start),
				  first16q);

	if (first16q != VM_FREELIST_DEFAULT &&
	    hole_end < 16 * 1024 * 1024) {
		uvm_page_physload(atop(hole_end), atop(16 * 1024 * 1024),
				  atop(hole_end), atop(16 * 1024 * 1024),
				  first16q);
		uvm_page_physload(atop(16 * 1024 * 1024), atop(avail_end),
				  atop(16 * 1024 * 1024), atop(avail_end),
				  VM_FREELIST_DEFAULT);
	} else {
		uvm_page_physload(atop(hole_end), atop(avail_end),
				  atop(hole_end), atop(avail_end),
				  VM_FREELIST_DEFAULT);
	}
#endif

	/*
	 * ensure the TLB is sync'd with reality by flushing it...
	 */

	pmap_update();
}

/*
 * pmap_init: called from uvm_init, our job is to get the pmap
 * system ready to manage mappings... this mainly means initing
 * the pv_entry stuff.
 */

void
pmap_init()
{
	int npages, lcv;
	vaddr_t addr;
	vsize_t s;

	/*
	 * compute the number of pages we have and then allocate RAM
	 * for each pages' pv_head and saved attributes.
	 */

	npages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		npages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
	s = (vsize_t) (sizeof(struct pv_head) * npages +
		       sizeof(char) * npages);
	s = round_page(s); /* round up */
	addr = (vaddr_t) uvm_km_zalloc(kernel_map, s);
	if (addr == NULL)
		panic("pmap_init: unable to allocate pv_heads");

	/*
	 * init all pv_head's and attrs in one bzero
	 */

	/* allocate pv_head stuff first */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.pvhead = (struct pv_head *) addr;
		addr = (vaddr_t)(vm_physmem[lcv].pmseg.pvhead +
				 (vm_physmem[lcv].end - vm_physmem[lcv].start));
	}

	/* now allocate attrs */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.attrs = (char *) addr;
		addr = (vaddr_t)(vm_physmem[lcv].pmseg.attrs +
				 (vm_physmem[lcv].end - vm_physmem[lcv].start));
	}

	/*
	 * now we need to free enough pv_entry structures to allow us to get
	 * the kmem_map/kmem_object allocated and inited (done after this
	 * function is finished).  to do this we allocate one bootstrap page out
	 * of kernel_map and use it to provide an initial pool of pv_entry
	 * structures.   we never free this page.
	 */

	pv_initpage = (struct pv_page *) uvm_km_alloc(kernel_map, NBPG);
	if (pv_initpage == NULL)
		panic("pmap_init: pv_initpage");
	pv_cachedva = NULL;   /* a VA we have allocated but not used yet */
	pv_nfpvents = 0;
	(void) pmap_add_pvpage(pv_initpage, FALSE);

	/*
	 * done: pmap module is up (and ready for business)
	 */

	pmap_initialized = TRUE;
}

/*
 * p v _ e n t r y   f u n c t i o n s
 */

/*
 * pv_entry allocation functions:
 *   the main pv_entry allocation functions are:
 *     pmap_alloc_pv: allocate a pv_entry structure
 *     pmap_free_pv: free one pv_entry
 *     pmap_free_pvs: free a list of pv_entrys
 *
 * the rest are helper functions
 */

/*
 * pmap_alloc_pv: inline function to allocate a pv_entry structure
 * => we lock pvalloc_lock
 * => if we fail, we call out to pmap_alloc_pvpage
 * => 3 modes:
 *    ALLOCPV_NEED   = we really need a pv_entry, even if we have to steal it
 *    ALLOCPV_TRY    = we want a pv_entry, but not enough to steal
 *    ALLOCPV_NONEED = we are trying to grow our free list, don't really need
 *			one now
 *
 * "try" is for optional functions like pmap_copy().
 */

__inline static struct pv_entry *
pmap_alloc_pv(pmap, mode)
	struct pmap *pmap;
	int mode;
{
	struct pv_page *pvpage;
	struct pv_entry *pv;

	simple_lock(&pvalloc_lock);

	if (pv_freepages.tqh_first != NULL) {
		pvpage = pv_freepages.tqh_first;
		pvpage->pvinfo.pvpi_nfree--;
		if (pvpage->pvinfo.pvpi_nfree == 0) {
			/* nothing left in this one? */
			TAILQ_REMOVE(&pv_freepages, pvpage, pvinfo.pvpi_list);
		}
		pv = pvpage->pvinfo.pvpi_pvfree;
#ifdef DIAGNOSTIC
		if (pv == NULL)
			panic("pmap_alloc_pv: pvpi_nfree off");
#endif
		pvpage->pvinfo.pvpi_pvfree = pv->pv_next;
		pv_nfpvents--;  /* took one from pool */
	} else {
		pv = NULL;		/* need more of them */
	}

	/*
	 * if below low water mark or we didn't get a pv_entry we try and
	 * create more pv_entrys ...
	 */

	if (pv_nfpvents < PVE_LOWAT || pv == NULL) {
		if (pv == NULL)
			pv = pmap_alloc_pvpage(pmap, (mode == ALLOCPV_TRY) ?
					       mode : ALLOCPV_NEED);
		else
			(void) pmap_alloc_pvpage(pmap, ALLOCPV_NONEED);
	}

	simple_unlock(&pvalloc_lock);
	return(pv);
}

/*
 * pmap_alloc_pvpage: maybe allocate a new pvpage
 *
 * if need_entry is false: try and allocate a new pv_page
 * if need_entry is true: try and allocate a new pv_page and return a
 *	new pv_entry from it.   if we are unable to allocate a pv_page
 *	we make a last ditch effort to steal a pv_page from some other
 *	mapping.    if that fails, we panic...
 *
 * => we assume that the caller holds pvalloc_lock
 */

static struct pv_entry *
pmap_alloc_pvpage(pmap, mode)
	struct pmap *pmap;
	int mode;
{
	struct vm_page *pg;
	struct pv_page *pvpage;
	int lcv, idx, npg, s;
	struct pv_entry *pv, *cpv, *prevpv;

	/*
	 * if we need_entry and we've got unused pv_pages, allocate from there
	 */

	if (mode != ALLOCPV_NONEED && pv_unusedpgs.tqh_first != NULL) {

		/* move it to pv_freepages list */
		pvpage = pv_unusedpgs.tqh_first;
		TAILQ_REMOVE(&pv_unusedpgs, pvpage, pvinfo.pvpi_list);
		TAILQ_INSERT_HEAD(&pv_freepages, pvpage, pvinfo.pvpi_list);

		/* allocate a pv_entry */
		pvpage->pvinfo.pvpi_nfree--;	/* can't go to zero */
		pv = pvpage->pvinfo.pvpi_pvfree;
#ifdef DIAGNOSTIC
		if (pv == NULL)
			panic("pmap_alloc_pvpage: pvpi_nfree off");
#endif
		pvpage->pvinfo.pvpi_pvfree = pv->pv_next;

		pv_nfpvents--;  /* took one from pool */
		return(pv);
	}

	/*
	 *  see if we've got a cached unmapped VA that we can map a page in.
	 * if not, try to allocate one.
	 */

	s = splimp();   /* must protect kmem_map/kmem_object with splimp! */
	if (pv_cachedva == NULL) {
		pv_cachedva = uvm_km_kmemalloc(kmem_map, uvmexp.kmem_object,
		    NBPG, UVM_KMF_TRYLOCK|UVM_KMF_VALLOC);
		if (pv_cachedva == NULL) {
			splx(s);
			goto steal_one;
		}
	}

	/*
	 * we have a VA, now let's try and allocate a page in the object
	 * note: we are still holding splimp to protect kmem_object
	 */

	if (!simple_lock_try(&uvmexp.kmem_object->vmobjlock)) {
		splx(s);
		goto steal_one;
	}

	pg = uvm_pagealloc(uvmexp.kmem_object, pv_cachedva -
			   vm_map_min(kernel_map),
			   NULL, UVM_PGA_USERESERVE);
	if (pg)
		pg->flags &= ~PG_BUSY;	/* never busy */

	simple_unlock(&uvmexp.kmem_object->vmobjlock);
	splx(s);
	/* splimp now dropped */

	if (pg == NULL)
		goto steal_one;

	/*
	 * add a mapping for our new pv_page and free its entrys (save one!)
	 *
	 * NOTE: If we are allocating a PV page for the kernel pmap, the
	 * pmap is already locked!  (...but entering the mapping is safe...)
	 */

	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg), VM_PROT_ALL);
	pvpage = (struct pv_page *) pv_cachedva;
	pv_cachedva = NULL;
	return(pmap_add_pvpage(pvpage, mode != ALLOCPV_NONEED));

steal_one:
	/*
	 * if we don't really need a pv_entry right now, we can just return.
	 */

	if (mode != ALLOCPV_NEED)
		return(NULL);

	/*
	 * last ditch effort!   we couldn't allocate a free page to make
	 * more pv_entrys so we try and steal one from someone else.
	 */

	pv = NULL;
	for (lcv = 0 ; pv == NULL && lcv < vm_nphysseg ; lcv++) {
		npg = vm_physmem[lcv].end - vm_physmem[lcv].start;
		for (idx = 0 ; idx < npg ; idx++) {
			struct pv_head *pvhead = vm_physmem[lcv].pmseg.pvhead;

			if (pvhead->pvh_list == NULL)
				continue;	/* spot check */
			if (!simple_lock_try(&pvhead->pvh_lock))
				continue;
			cpv = prevpv = pvhead->pvh_list;
			while (cpv) {
				if (pmap_try_steal_pv(pvhead, cpv, prevpv))
					break;
				prevpv = cpv;
				cpv = cpv->pv_next;
			}
			simple_unlock(&pvhead->pvh_lock);
			/* got one?  break out of the loop! */
			if (cpv) {
				pv = cpv;
				break;
			}
		}
	}

	return(pv);
}

/*
 * pmap_try_steal_pv: try and steal a pv_entry from a pmap
 *
 * => return true if we did it!
 */

static boolean_t
pmap_try_steal_pv(pvh, cpv, prevpv)
	struct pv_head *pvh;
	struct pv_entry *cpv, *prevpv;
{
	pt_entry_t *ptep;	/* pointer to a PTE */

	/*
	 * we never steal kernel mappings or mappings from pmaps we can't lock
	 */

	if (cpv->pv_pmap == pmap_kernel() ||
	    !simple_lock_try(&cpv->pv_pmap->pm_obj.vmobjlock))
		return(FALSE);

	/*
	 * yes, we can try and steal it.   first we need to remove the
	 * mapping from the pmap.
	 */

	ptep = pmap_tmpmap_pvepte(cpv);
	if (*ptep & PG_W) {
		ptep = NULL;	/* wired page, avoid stealing this one */
	} else {
		*ptep = 0;		/* zap! */
		if (pmap_is_curpmap(cpv->pv_pmap))
			pmap_update_pg(cpv->pv_va);
		pmap_tmpunmap_pvepte(cpv);
	}
	if (ptep == NULL) {
		simple_unlock(&cpv->pv_pmap->pm_obj.vmobjlock);
		return(FALSE);	/* wired page, abort! */
	}
	cpv->pv_pmap->pm_stats.resident_count--;
	if (cpv->pv_ptp && cpv->pv_ptp->wire_count)
		/* drop PTP's wired count */
		cpv->pv_ptp->wire_count--;

	/*
	 * XXX: if wire_count goes to one the PTP could be freed, however,
	 * we'd have to lock the page queues (etc.) to do that and it could
	 * cause deadlock headaches.   besides, the pmap we just stole from
	 * may want the mapping back anyway, so leave the PTP around.
	 */

	/*
	 * now we need to remove the entry from the pvlist
	 */

	if (cpv == pvh->pvh_list)
		pvh->pvh_list = cpv->pv_next;
	else
		prevpv->pv_next = cpv->pv_next;
	return(TRUE);
}

/*
 * pmap_add_pvpage: add a pv_page's pv_entrys to the free list
 *
 * => caller must hold pvalloc_lock
 * => if need_entry is true, we allocate and return one pv_entry
 */

static struct pv_entry *
pmap_add_pvpage(pvp, need_entry)
	struct pv_page *pvp;
	boolean_t need_entry;
{
	int tofree, lcv;

	/* do we need to return one? */
	tofree = (need_entry) ? PVE_PER_PVPAGE - 1 : PVE_PER_PVPAGE;

	pvp->pvinfo.pvpi_pvfree = NULL;
	pvp->pvinfo.pvpi_nfree = tofree;
	for (lcv = 0 ; lcv < tofree ; lcv++) {
		pvp->pvents[lcv].pv_next = pvp->pvinfo.pvpi_pvfree;
		pvp->pvinfo.pvpi_pvfree = &pvp->pvents[lcv];
	}
	if (need_entry)
		TAILQ_INSERT_TAIL(&pv_freepages, pvp, pvinfo.pvpi_list);
	else
		TAILQ_INSERT_TAIL(&pv_unusedpgs, pvp, pvinfo.pvpi_list);
	pv_nfpvents += tofree;
	return((need_entry) ? &pvp->pvents[lcv] : NULL);
}

/*
 * pmap_free_pv_doit: actually free a pv_entry
 *
 * => do not call this directly!  instead use either
 *    1. pmap_free_pv ==> free a single pv_entry
 *    2. pmap_free_pvs => free a list of pv_entrys
 * => we must be holding pvalloc_lock
 */

__inline static void
pmap_free_pv_doit(pv)
	struct pv_entry *pv;
{
	struct pv_page *pvp;

	pvp = (struct pv_page *) i386_trunc_page(pv);
	pv_nfpvents++;
	pvp->pvinfo.pvpi_nfree++;

	/* nfree == 1 => fully allocated page just became partly allocated */
	if (pvp->pvinfo.pvpi_nfree == 1) {
		TAILQ_INSERT_HEAD(&pv_freepages, pvp, pvinfo.pvpi_list);
	}

	/* free it */
	pv->pv_next = pvp->pvinfo.pvpi_pvfree;
	pvp->pvinfo.pvpi_pvfree = pv;

	/*
	 * are all pv_page's pv_entry's free?  move it to unused queue.
	 */

	if (pvp->pvinfo.pvpi_nfree == PVE_PER_PVPAGE) {
		TAILQ_REMOVE(&pv_freepages, pvp, pvinfo.pvpi_list);
		TAILQ_INSERT_HEAD(&pv_unusedpgs, pvp, pvinfo.pvpi_list);
	}
}

/*
 * pmap_free_pv: free a single pv_entry
 *
 * => we gain the pvalloc_lock
 */

__inline static void
pmap_free_pv(pmap, pv)
	struct pmap *pmap;
	struct pv_entry *pv;
{
	simple_lock(&pvalloc_lock);
	pmap_free_pv_doit(pv);

	/*
	 * Can't free the PV page if the PV entries were associated with
	 * the kernel pmap; the pmap is already locked.
	 */
	if (pv_nfpvents > PVE_HIWAT && pv_unusedpgs.tqh_first != NULL &&
	    pmap != pmap_kernel())
		pmap_free_pvpage();

	simple_unlock(&pvalloc_lock);
}

/*
 * pmap_free_pvs: free a list of pv_entrys
 *
 * => we gain the pvalloc_lock
 */

__inline static void
pmap_free_pvs(pmap, pvs)
	struct pmap *pmap;
	struct pv_entry *pvs;
{
	struct pv_entry *nextpv;

	simple_lock(&pvalloc_lock);

	for ( /* null */ ; pvs != NULL ; pvs = nextpv) {
		nextpv = pvs->pv_next;
		pmap_free_pv_doit(pvs);
	}

	/*
	 * Can't free the PV page if the PV entries were associated with
	 * the kernel pmap; the pmap is already locked.
	 */
	if (pv_nfpvents > PVE_HIWAT && pv_unusedpgs.tqh_first != NULL &&
	    pmap != pmap_kernel())
		pmap_free_pvpage();

	simple_unlock(&pvalloc_lock);
}


/*
 * pmap_free_pvpage: try and free an unused pv_page structure
 *
 * => assume caller is holding the pvalloc_lock and that
 *	there is a page on the pv_unusedpgs list
 * => if we can't get a lock on the kmem_map we try again later
 * => note: analysis of MI kmem_map usage [i.e. malloc/free] shows
 *	that if we can lock the kmem_map then we are not already
 *	holding kmem_object's lock.
 */

static void
pmap_free_pvpage()
{
	int s;
	struct vm_map *map;
	vm_map_entry_t dead_entries;
	struct pv_page *pvp;

	s = splimp(); /* protect kmem_map */

	pvp = pv_unusedpgs.tqh_first;

	/*
	 * note: watch out for pv_initpage which is allocated out of
	 * kernel_map rather than kmem_map.
	 */
	if (pvp == pv_initpage)
		map = kernel_map;
	else
		map = kmem_map;

	if (vm_map_lock_try(map)) {

		/* remove pvp from pv_unusedpgs */
		TAILQ_REMOVE(&pv_unusedpgs, pvp, pvinfo.pvpi_list);

		/* unmap the page */
		dead_entries = NULL;
		(void)uvm_unmap_remove(map, (vaddr_t) pvp,
				       ((vaddr_t) pvp) + NBPG, &dead_entries);
		vm_map_unlock(map);

		if (dead_entries != NULL)
			uvm_unmap_detach(dead_entries, 0);

		pv_nfpvents -= PVE_PER_PVPAGE;  /* update free count */
	}

	if (pvp == pv_initpage)
		/* no more initpage, we've freed it */
		pv_initpage = NULL;

	splx(s);
}

/*
 * main pv_entry manipulation functions:
 *   pmap_enter_pv: enter a mapping onto a pv_head list
 *   pmap_remove_pv: remove a mappiing from a pv_head list
 *
 * NOTE: pmap_enter_pv expects to lock the pvh itself
 *       pmap_remove_pv expects te caller to lock the pvh before calling
 */

/*
 * pmap_enter_pv: enter a mapping onto a pv_head lst
 *
 * => caller should hold the proper lock on pmap_main_lock
 * => caller should have pmap locked
 * => we will gain the lock on the pv_head and allocate the new pv_entry
 * => caller should adjust ptp's wire_count before calling
 */

__inline static void
pmap_enter_pv(pvh, pve, pmap, va, ptp)
	struct pv_head *pvh;
	struct pv_entry *pve;	/* preallocated pve for us to use */
	struct pmap *pmap;
	vaddr_t va;
	struct vm_page *ptp;	/* PTP in pmap that maps this VA */
{
	pve->pv_pmap = pmap;
	pve->pv_va = va;
	pve->pv_ptp = ptp;			/* NULL for kernel pmap */
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	pve->pv_next = pvh->pvh_list;		/* add to ... */
	pvh->pvh_list = pve;			/* ... locked list */
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
}

/*
 * pmap_remove_pv: try to remove a mapping from a pv_list
 *
 * => caller should hold proper lock on pmap_main_lock
 * => pmap should be locked
 * => caller should hold lock on pv_head [so that attrs can be adjusted]
 * => caller should adjust ptp's wire_count and free PTP if needed
 * => we return the removed pve
 */

__inline static struct pv_entry *
pmap_remove_pv(pvh, pmap, va)
	struct pv_head *pvh;
	struct pmap *pmap;
	vaddr_t va;
{
	struct pv_entry *pve, **prevptr;

	prevptr = &pvh->pvh_list;		/* previous pv_entry pointer */
	pve = *prevptr;
	while (pve) {
		if (pve->pv_pmap == pmap && pve->pv_va == va) {	/* match? */
			*prevptr = pve->pv_next;		/* remove it! */
			break;
		}
		prevptr = &pve->pv_next;		/* previous pointer */
		pve = pve->pv_next;			/* advance */
	}
	return(pve);				/* return removed pve */
}

/*
 * p t p   f u n c t i o n s
 */

/*
 * pmap_alloc_ptp: allocate a PTP for a PMAP
 *
 * => pmap should already be locked by caller
 * => we use the ptp's wire_count to count the number of active mappings
 *	in the PTP (we start it at one to prevent any chance this PTP
 *	will ever leak onto the active/inactive queues)
 * => we should not be holding any pv_head locks (in case we are forced
 *	to call pmap_steal_ptp())
 * => we may need to lock pv_head's if we have to steal a PTP
 * => just_try: true if we want a PTP, but not enough to steal one
 * 	from another pmap (e.g. during optional functions like pmap_copy)
 */

__inline static struct vm_page *
pmap_alloc_ptp(pmap, pde_index, just_try)
	struct pmap *pmap;
	int pde_index;
	boolean_t just_try;
{
	struct vm_page *ptp;

	ptp = uvm_pagealloc(&pmap->pm_obj, ptp_i2o(pde_index), NULL,
			    UVM_PGA_USERESERVE);
	if (ptp == NULL) {
		if (just_try)
			return(NULL);
		ptp = pmap_steal_ptp(&pmap->pm_obj, ptp_i2o(pde_index));
		if (ptp == NULL) {
			return (NULL);
		}
	}

	/* got one! */
	ptp->flags &= ~PG_BUSY;	/* never busy */
	ptp->wire_count = 1;	/* no mappings yet */
	pmap_zero_page(VM_PAGE_TO_PHYS(ptp));
	pmap->pm_pdir[pde_index] =
		(pd_entry_t) (VM_PAGE_TO_PHYS(ptp) | PG_u | PG_RW | PG_V);
	pmap->pm_stats.resident_count++;	/* count PTP as resident */
	pmap->pm_ptphint = ptp;
	return(ptp);
}

/*
 * pmap_steal_ptp: steal a PTP from any pmap that we can access
 *
 * => obj is locked by caller.
 * => we can throw away mappings at this level (except in the kernel's pmap)
 * => stolen PTP is placed in <obj,offset> pmap
 * => we lock pv_head's
 * => hopefully, this function will be seldom used [much better to have
 *	enough free pages around for us to allocate off the free page list]
 */

static struct vm_page *
pmap_steal_ptp(obj, offset)
	struct uvm_object *obj;
	vaddr_t offset;
{
	struct vm_page *ptp = NULL;
	struct pmap *firstpmap;
	struct uvm_object *curobj;
	pt_entry_t *ptes;
	int idx, lcv;
	boolean_t caller_locked, we_locked;

	simple_lock(&pmaps_lock);
	if (pmaps_hand == NULL)
		pmaps_hand = LIST_FIRST(&pmaps);
	firstpmap = pmaps_hand;

	do { /* while we haven't looped back around to firstpmap */

		curobj = &pmaps_hand->pm_obj;
		we_locked = FALSE;
		caller_locked = (curobj == obj);
		if (!caller_locked) {
			we_locked = simple_lock_try(&curobj->vmobjlock);
		}
		if (caller_locked || we_locked) {
			ptp = curobj->memq.tqh_first;
			for (/*null*/; ptp != NULL; ptp = ptp->listq.tqe_next) {

				/*
				 * might have found a PTP we can steal
				 * (unless it has wired pages).
				 */

				idx = ptp_o2i(ptp->offset);
#ifdef DIAGNOSTIC
				if (VM_PAGE_TO_PHYS(ptp) !=
				    (pmaps_hand->pm_pdir[idx] & PG_FRAME))
					panic("pmap_steal_ptp: PTP mismatch!");
#endif

				ptes = (pt_entry_t *)
					pmap_tmpmap_pa(VM_PAGE_TO_PHYS(ptp));
				for (lcv = 0 ; lcv < PTES_PER_PTP ; lcv++)
					if ((ptes[lcv] & (PG_V|PG_W)) ==
					    (PG_V|PG_W))
						break;
				if (lcv == PTES_PER_PTP)
					pmap_remove_ptes(pmaps_hand, NULL, ptp,
							 (vaddr_t)ptes,
							 ptp_i2v(idx),
							 ptp_i2v(idx+1));
				pmap_tmpunmap_pa();

				if (lcv != PTES_PER_PTP)
					/* wired, try next PTP */
					continue;

				/*
				 * got it!!!
				 */

				pmaps_hand->pm_pdir[idx] = 0;	/* zap! */
				pmaps_hand->pm_stats.resident_count--;
				if (pmap_is_curpmap(pmaps_hand))
					pmap_update();
				else if (pmap_valid_entry(*APDP_PDE) &&
					 (*APDP_PDE & PG_FRAME) ==
					 pmaps_hand->pm_pdirpa) {
					pmap_update_pg(((vaddr_t)APTE_BASE) +
						       ptp->offset);
				}

				/* put it in our pmap! */
				uvm_pagerealloc(ptp, obj, offset);
				break;	/* break out of "for" loop */
			}
			if (we_locked) {
				simple_unlock(&curobj->vmobjlock);
			}
		}

		/* advance the pmaps_hand */
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
		if (pmaps_hand == NULL) {
			pmaps_hand = LIST_FIRST(&pmaps);
		}

	} while (ptp == NULL && pmaps_hand != firstpmap);

	simple_unlock(&pmaps_lock);
	return(ptp);
}

/*
 * pmap_get_ptp: get a PTP (if there isn't one, allocate a new one)
 *
 * => pmap should NOT be pmap_kernel()
 * => pmap should be locked
 */

static struct vm_page *
pmap_get_ptp(pmap, pde_index, just_try)
	struct pmap *pmap;
	int pde_index;
	boolean_t just_try;
{
	struct vm_page *ptp;

	if (pmap_valid_entry(pmap->pm_pdir[pde_index])) {

		/* valid... check hint (saves us a PA->PG lookup) */
		if (pmap->pm_ptphint &&
		    (pmap->pm_pdir[pde_index] & PG_FRAME) ==
		    VM_PAGE_TO_PHYS(pmap->pm_ptphint))
			return(pmap->pm_ptphint);

		ptp = uvm_pagelookup(&pmap->pm_obj, ptp_i2o(pde_index));
#ifdef DIAGNOSTIC
		if (ptp == NULL)
			panic("pmap_get_ptp: unmanaged user PTP");
#endif
		pmap->pm_ptphint = ptp;
		return(ptp);
	}

	/* allocate a new PTP (updates ptphint) */
	return(pmap_alloc_ptp(pmap, pde_index, just_try));
}

/*
 * p m a p  l i f e c y c l e   f u n c t i o n s
 */

/*
 * pmap_create: create a pmap
 *
 * => note: old pmap interface took a "size" args which allowed for
 *	the creation of "software only" pmaps (not in bsd).
 */

struct pmap *
pmap_create()
{
	struct pmap *pmap;

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	pmap_pinit(pmap);
	return(pmap);
}

/*
 * pmap_pinit: given a zero'd pmap structure, init it.
 */

void
pmap_pinit(pmap)
	struct pmap *pmap;
{
	/* init uvm_object */
	simple_lock_init(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.pgops = NULL;	/* currently not a mappable object */
	TAILQ_INIT(&pmap->pm_obj.memq);
	pmap->pm_obj.uo_npages = 0;
	pmap->pm_obj.uo_refs = 1;
	pmap->pm_stats.wired_count = 0;
	pmap->pm_stats.resident_count = 1;	/* count the PDP allocd below */
	pmap->pm_ptphint = NULL;
	pmap->pm_flags = 0;

	/* allocate PDP */
	pmap->pm_pdir = (pd_entry_t *) uvm_km_alloc(kernel_map, NBPG);
	if (pmap->pm_pdir == NULL)
		panic("pmap_pinit: kernel_map out of virtual space!");
	(void) _pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
			    (paddr_t *)&pmap->pm_pdirpa);

	/* init PDP */
	/* zero init area */
	bzero(pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
	/* put in recursive PDE to map the PTEs */
	pmap->pm_pdir[PDSLOT_PTE] = pmap->pm_pdirpa | PG_V | PG_KW;

	/* init the LDT */
	pmap->pm_ldt = NULL;
	pmap->pm_ldt_len = 0;
	pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);

	/*
	 * we need to lock pmaps_lock to prevent nkpde from changing on
	 * us.   note that there is no need to splimp to protect us from
	 * malloc since malloc allocates out of a submap and we should have
	 * already allocated kernel PTPs to cover the range...
	 */
	simple_lock(&pmaps_lock);
	/* put in kernel VM PDEs */
	bcopy(&PDP_BASE[PDSLOT_KERN], &pmap->pm_pdir[PDSLOT_KERN],
	       nkpde * sizeof(pd_entry_t));
	/* zero the rest */
	bzero(&pmap->pm_pdir[PDSLOT_KERN + nkpde],
	       NBPG - ((PDSLOT_KERN + nkpde) * sizeof(pd_entry_t)));
	LIST_INSERT_HEAD(&pmaps, pmap, pm_list);
	simple_unlock(&pmaps_lock);
}

/*
 * pmap_destroy: drop reference count on pmap.   free pmap if
 *	reference count goes to zero.
 */

void
pmap_destroy(pmap)
	struct pmap *pmap;
{
	int refs;

	/*
	 * drop reference count
	 */

	simple_lock(&pmap->pm_obj.vmobjlock);
	refs = --pmap->pm_obj.uo_refs;
	simple_unlock(&pmap->pm_obj.vmobjlock);
	if (refs > 0) {
		return;
	}

	/*
	 * reference count is zero, free pmap resources and then free pmap.
	 */

	pmap_release(pmap);
	pool_put(&pmap_pmap_pool, pmap);
}

/*
 * pmap_release: release all resources held by a pmap
 *
 * => if pmap is still referenced it should be locked
 * => XXX: we currently don't expect any busy PTPs because we don't
 *    allow anything to map them (except for the kernel's private
 *    recursive mapping) or make them busy.
 */

void
pmap_release(pmap)
	struct pmap *pmap;
{
	struct vm_page *pg;

	/*
	 * remove it from global list of pmaps
	 */

	simple_lock(&pmaps_lock);
	if (pmap == pmaps_hand)
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
	LIST_REMOVE(pmap, pm_list);
	simple_unlock(&pmaps_lock);

	/*
	 * free any remaining PTPs
	 */

	while (pmap->pm_obj.memq.tqh_first != NULL) {
		pg = pmap->pm_obj.memq.tqh_first;
#ifdef DIAGNOSTIC
		if (pg->flags & PG_BUSY)
			panic("pmap_release: busy page table page");
#endif
		/* pmap_page_protect?  currently no need for it. */

		pg->wire_count = 0;
		uvm_pagefree(pg);
	}

	/* XXX: need to flush it out of other processor's APTE space? */
	uvm_km_free(kernel_map, (vaddr_t)pmap->pm_pdir, NBPG);

#ifdef USER_LDT
	if (pmap->pm_flags & PMF_USER_LDT) {
		/*
		 * no need to switch the LDT; this address space is gone,
		 * nothing is using it.
		 */
		ldt_free(pmap);
		uvm_km_free(kernel_map, (vaddr_t)pmap->pm_ldt,
			    pmap->pm_ldt_len * sizeof(union descriptor));
	}
#endif
}

/*
 *	Add a reference to the specified pmap.
 */

void
pmap_reference(pmap)
	struct pmap *pmap;
{
	simple_lock(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.uo_refs++;
	simple_unlock(&pmap->pm_obj.vmobjlock);
}

#if defined(PMAP_FORK)
/*
 * pmap_fork: perform any necessary data structure manipulation when
 * a VM space is forked.
 */

void
pmap_fork(pmap1, pmap2)
	struct pmap *pmap1, *pmap2;
{
	simple_lock(&pmap1->pm_obj.vmobjlock);
	simple_lock(&pmap2->pm_obj.vmobjlock);

#ifdef USER_LDT
	/* Copy the LDT, if necessary. */
	if (pmap1->pm_flags & PMF_USER_LDT) {
		union descriptor *new_ldt;
		size_t len;

		len = pmap1->pm_ldt_len * sizeof(union descriptor);
		new_ldt = (union descriptor *)uvm_km_alloc(kernel_map, len);
		bcopy(pmap1->pm_ldt, new_ldt, len);
		pmap2->pm_ldt = new_ldt;
		pmap2->pm_ldt_len = pmap1->pm_ldt_len;
		pmap2->pm_flags |= PMF_USER_LDT;
		ldt_alloc(pmap2, new_ldt, len);
	}
#endif /* USER_LDT */

	simple_unlock(&pmap2->pm_obj.vmobjlock);
	simple_unlock(&pmap1->pm_obj.vmobjlock);
}
#endif /* PMAP_FORK */

#ifdef USER_LDT
/*
 * pmap_ldt_cleanup: if the pmap has a local LDT, deallocate it, and
 * restore the default.
 */

void
pmap_ldt_cleanup(p)
	struct proc *p;
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	union descriptor *old_ldt = NULL;
	size_t len = 0;

	simple_lock(&pmap->pm_obj.vmobjlock);

	if (pmap->pm_flags & PMF_USER_LDT) {
		ldt_free(pmap);
		pmap->pm_ldt_sel = GSEL(GLDT_SEL, SEL_KPL);
		pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
		if (pcb == curpcb)
			lldt(pcb->pcb_ldt_sel);
		old_ldt = pmap->pm_ldt;
		len = pmap->pm_ldt_len * sizeof(union descriptor);
		pmap->pm_ldt = NULL;
		pmap->pm_ldt_len = 0;
		pmap->pm_flags &= ~PMF_USER_LDT;
	}

	simple_unlock(&pmap->pm_obj.vmobjlock);

	if (old_ldt != NULL)
		uvm_km_free(kernel_map, (vaddr_t)old_ldt, len);
}
#endif /* USER_LDT */

/*
 * pmap_activate: activate a process' pmap (fill in %cr3 info)
 *
 * => called from cpu_fork()
 * => if proc is the curproc, then load it into the MMU
 */

void
pmap_activate(p)
	struct proc *p;
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	pcb->pcb_pmap = pmap;
	pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
	pcb->pcb_cr3 = pmap->pm_pdirpa;
	if (p == curproc)
		lcr3(pcb->pcb_cr3);
	if (pcb == curpcb)
		lldt(pcb->pcb_ldt_sel);
}

/*
 * pmap_deactivate: deactivate a process' pmap
 *
 * => XXX: what should this do, if anything?
 */

void
pmap_deactivate(p)
	struct proc *p;
{
}

/*
 * end of lifecycle functions
 */

/*
 * some misc. functions
 */

/*
 * pmap_extract: extract a PA for the given VA
 */

boolean_t
_pmap_extract(pmap, va, pap)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t *pap;
{
	paddr_t retval;
	pt_entry_t *ptes;

	if (pmap->pm_pdir[pdei(va)]) {
		ptes = pmap_map_ptes(pmap);
		retval = (paddr_t)(ptes[i386_btop(va)] & PG_FRAME);
		pmap_unmap_ptes(pmap);
		if (pap != NULL)
			*pap = retval | (va & ~PG_FRAME);
		return (TRUE);
	}
	return (FALSE);
}

paddr_t
pmap_extract(pmap, va)
a764 271
	vaddr_t va;
{
	paddr_t pa;

	if (_pmap_extract(pmap, va, &pa))
		return (pa);
	return (NULL);
}

/*
 * pmap_virtual_space: used during bootup [pmap_steal_memory] to
 *	determine the bounds of the kernel virtual addess space.
 */

void
pmap_virtual_space(startp, endp)
	vaddr_t *startp;
	vaddr_t *endp;
{
	*startp = virtual_avail;
	*endp = virtual_end;
}

/*
 * pmap_map: map a range of PAs into kvm
 *
 * => used during crash dump
 * => XXX: pmap_map() should be phased out?
 */

vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	vm_prot_t prot;
{
	while (spa < epa) {
		_pmap_enter(pmap_kernel(), va, spa, prot, 0);
		va += NBPG;
		spa += NBPG;
	}
	return va;
}

/*
 * pmap_zero_page: zero a page
 */

void
pmap_zero_page(pa)
	paddr_t pa;
{
	simple_lock(&pmap_zero_page_lock);
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page: lock botch");
#endif

	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	bzero(zerop, NBPG);				/* zero */
	*zero_pte = 0;				/* zap! */
	pmap_update_pg((vaddr_t)zerop);		/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
}

/*
 * pmap_copy_page: copy a page
 */

void
pmap_copy_page(srcpa, dstpa)
	paddr_t srcpa, dstpa;
{
	simple_lock(&pmap_copy_page_lock);
#ifdef DIAGNOSTIC
	if (*csrc_pte || *cdst_pte)
		panic("pmap_copy_page: lock botch");
#endif

	*csrc_pte = (srcpa & PG_FRAME) | PG_V | PG_RW;
	*cdst_pte = (dstpa & PG_FRAME) | PG_V | PG_RW;
	bcopy(csrcp, cdstp, NBPG);
	*csrc_pte = *cdst_pte = 0;			/* zap! */
	pmap_update_2pg((vaddr_t)csrcp, (vaddr_t)cdstp);
	simple_unlock(&pmap_copy_page_lock);
}

/*
 * p m a p   r e m o v e   f u n c t i o n s
 *
 * functions that remove mappings
 */

/*
 * pmap_remove_ptes: remove PTEs from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => caller must hold pmap's lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 */

static void
pmap_remove_ptes(pmap, pmap_rr, ptp, ptpva, startva, endva)
	struct pmap *pmap;
	struct pmap_remove_record *pmap_rr;
	struct vm_page *ptp;
	vaddr_t ptpva;
	vaddr_t startva, endva;
{
	struct pv_entry *pv_tofree = NULL;	/* list of pv_entrys to free */
	struct pv_entry *pve;
	pt_entry_t *pte = (pt_entry_t *) ptpva;
	pt_entry_t opte;
	int bank, off;

	/*
	 * note that ptpva points to the PTE that maps startva.   this may
	 * or may not be the first PTE in the PTP.
	 *
	 * we loop through the PTP while there are still PTEs to look at
	 * and the wire_count is greater than 1 (because we use the wire_count
	 * to keep track of the number of real PTEs in the PTP).
	 */

	for (/*null*/; startva < endva && (ptp == NULL || ptp->wire_count > 1)
			     ; pte++, startva += NBPG) {
		if (!pmap_valid_entry(*pte))
			continue;			/* VA not mapped */

		opte = *pte;		/* save the old PTE */
		*pte = 0;			/* zap! */
		if (opte & PG_W)
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		if (pmap_rr) {		/* worried about tlb flushing? */
			if (opte & PG_G) {
				/* PG_G requires this */
				pmap_update_pg(startva);
			} else {
				if (pmap_rr->prr_npages < PMAP_RR_MAX) {
					pmap_rr->prr_vas[pmap_rr->prr_npages++]
						= startva;
				} else {
					if (pmap_rr->prr_npages == PMAP_RR_MAX)
						/* signal an overflow */
						pmap_rr->prr_npages++;
				}
			}
		}
		if (ptp)
			ptp->wire_count--;		/* dropping a PTE */

		/*
		 * if we are not on a pv_head list we are done.
		 */

		if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
			if (vm_physseg_find(i386_btop(opte & PG_FRAME), &off)
			    != -1)
				panic("pmap_remove_ptes: managed page without "
				      "PG_PVLIST for 0x%lx", startva);
#endif
			continue;
		}

		bank = vm_physseg_find(i386_btop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
		if (bank == -1)
			panic("pmap_remove_ptes: unmanaged page marked "
			      "PG_PVLIST");
#endif

		/* sync R/M bits */
		simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
		pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap,
				     startva);
		simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);

		if (pve) {
			pve->pv_next = pv_tofree;
			pv_tofree = pve;
		}

		/* end of "for" loop: time for next pte */
	}
	if (pv_tofree)
		pmap_free_pvs(pmap, pv_tofree);
}


/*
 * pmap_remove_pte: remove a single PTE from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => caller must hold pmap's lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 * => returns true if we removed a mapping
 */

static boolean_t
pmap_remove_pte(pmap, ptp, pte, va)
	struct pmap *pmap;
	struct vm_page *ptp;
	pt_entry_t *pte;
	vaddr_t va;
{
	pt_entry_t opte;
	int bank, off;
	struct pv_entry *pve;

	if (!pmap_valid_entry(*pte))
		return(FALSE);		/* VA not mapped */

	opte = *pte;			/* save the old PTE */
	*pte = 0;			/* zap! */

	if (opte & PG_W)
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	if (ptp)
		ptp->wire_count--;		/* dropping a PTE */

	if (pmap_is_curpmap(pmap))
		pmap_update_pg(va);		/* flush TLB */

	/*
	 * if we are not on a pv_head list we are done.
	 */

	if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
		if (vm_physseg_find(i386_btop(opte & PG_FRAME), &off) != -1)
			panic("pmap_remove_ptes: managed page without "
			      "PG_PVLIST for 0x%lx", va);
#endif
		return(TRUE);
	}

	bank = vm_physseg_find(i386_btop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
	if (bank == -1)
		panic("pmap_remove_pte: unmanaged page marked PG_PVLIST");
#endif

	/* sync R/M bits */
	simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
	vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
	pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap, va);
	simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);

	if (pve)
		pmap_free_pv(pmap, pve);
	return(TRUE);
}

/*
 * pmap_remove: top level mapping removal function
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_remove(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva, eva;
a765 72
	pt_entry_t *ptes;
	boolean_t result;
	paddr_t ptppa;
	vaddr_t blkendva;
	struct vm_page *ptp;
	struct pmap_remove_record pmap_rr, *prr;

	/*
	 * we lock in the pmap => pv_head direction
	 */

	PMAP_MAP_TO_HEAD_LOCK();
	ptes = pmap_map_ptes(pmap);	/* locks pmap */

	/*
	 * removing one page?  take shortcut function.
	 */

	if (sva + NBPG == eva) {

		if (pmap_valid_entry(pmap->pm_pdir[pdei(sva)])) {

			/* PA of the PTP */
			ptppa = pmap->pm_pdir[pdei(sva)] & PG_FRAME;

			/* get PTP if non-kernel mapping */

			if (pmap == pmap_kernel()) {
				/* we never free kernel PTPs */
				ptp = NULL;
			} else {
				if (pmap->pm_ptphint &&
				    VM_PAGE_TO_PHYS(pmap->pm_ptphint) ==
				    ptppa) {
					ptp = pmap->pm_ptphint;
				} else {
					ptp = PHYS_TO_VM_PAGE(ptppa);
#ifdef DIAGNOSTIC
					if (ptp == NULL)
						panic("pmap_remove: unmanaged "
						      "PTP detected");
#endif
				}
			}

			/* do it! */
			result = pmap_remove_pte(pmap, ptp,
						 &ptes[i386_btop(sva)], sva);

			/*
			 * if mapping removed and the PTP is no longer
			 * being used, free it!
			 */

			if (result && ptp && ptp->wire_count <= 1) {
				pmap->pm_pdir[pdei(sva)] = 0;	/* zap! */
#if defined(I386_CPU)
				/* already dumped whole TLB on i386 */
				if (cpu_class != CPUCLASS_386)
#endif
				{
					pmap_update_pg(((vaddr_t) ptes) +
						       ptp->offset);
				}
				pmap->pm_stats.resident_count--;
				if (pmap->pm_ptphint == ptp)
					pmap->pm_ptphint =
						pmap->pm_obj.memq.tqh_first;
				ptp->wire_count = 0;
				uvm_pagefree(ptp);
			}
		}
d767 1
a767 2
		pmap_unmap_ptes(pmap);		/* unlock pmap */
		PMAP_MAP_TO_HEAD_UNLOCK();
a768 1
	}
d770 3
a772 60
	/*
	 * removing a range of pages: we unmap in PTP sized blocks (4MB)
	 *
	 * if we are the currently loaded pmap, we use prr to keep track
	 * of the VAs we unload so that we can flush them out of the tlb.
	 */

	if (pmap_is_curpmap(pmap)) {
		prr = &pmap_rr;
		prr->prr_npages = 0;
	} else {
		prr = NULL;
	}

	for (/* null */ ; sva < eva ; sva = blkendva) {

		/* determine range of block */
		blkendva = i386_round_pdr(sva+1);
		if (blkendva > eva)
			blkendva = eva;

		/*
		 * XXXCDC: our PTE mappings should never be removed
		 * with pmap_remove!  if we allow this (and why would
		 * we?) then we end up freeing the pmap's page
		 * directory page (PDP) before we are finished using
		 * it when we hit in in the recursive mapping.  this
		 * is BAD.
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		if (pdei(sva) == PDSLOT_PTE)
			/* XXXCDC: ugly hack to avoid freeing PDP here */
			continue;

		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
			/* valid block? */
			continue;

		/* PA of the PTP */
		ptppa = (pmap->pm_pdir[pdei(sva)] & PG_FRAME);

		/* get PTP if non-kernel mapping */
		if (pmap == pmap_kernel()) {
			/* we never free kernel PTPs */
			ptp = NULL;
		} else {
			if (pmap->pm_ptphint &&
			    VM_PAGE_TO_PHYS(pmap->pm_ptphint) == ptppa) {
				ptp = pmap->pm_ptphint;
			} else {
				ptp = PHYS_TO_VM_PAGE(ptppa);
#ifdef DIAGNOSTIC
				if (ptp == NULL)
					panic("pmap_remove: unmanaged PTP "
					      "detected");
a773 4
			}
		}
		pmap_remove_ptes(pmap, prr, ptp,
				 (vaddr_t)&ptes[i386_btop(sva)], sva, blkendva);
d775 3
a777 40
		/* if PTP is no longer being used, free it! */
		if (ptp && ptp->wire_count <= 1) {
			pmap->pm_pdir[pdei(sva)] = 0;	/* zap! */
			pmap_update_pg( ((vaddr_t) ptes) + ptp->offset);
#if defined(I386_CPU)
			/* cancel possible pending pmap update on i386 */
			if (cpu_class == CPUCLASS_386 && prr)
				prr->prr_npages = 0;
#endif
			pmap->pm_stats.resident_count--;
			if (pmap->pm_ptphint == ptp)	/* update hint? */
				pmap->pm_ptphint = pmap->pm_obj.memq.tqh_first;
			ptp->wire_count = 0;
			uvm_pagefree(ptp);
		}
	}

	/*
	 * if we kept a removal record and removed some pages update the TLB
	 */

	if (prr && prr->prr_npages) {
#if defined(I386_CPU)
		if (cpu_class == CPUCLASS_386) {
			pmap_update();
		} else
#endif
		{ /* not I386 */
			if (prr->prr_npages > PMAP_RR_MAX) {
				pmap_update();
			} else {
				while (prr->prr_npages) {
					pmap_update_pg(
					    prr->prr_vas[--prr->prr_npages]);
				}
			}
		} /* not I386 */
	}
	pmap_unmap_ptes(pmap);
	PMAP_MAP_TO_HEAD_UNLOCK();
a779 7
/*
 * pmap_page_remove: remove a managed vm_page from all pmaps that map it
 *
 * => we set pv_head => pmap locking
 * => R/M bits are sync'd back to attrs
 */

d781 2
a782 2
pmap_page_remove(pg)
	struct vm_page *pg;
d784 2
a785 7
	int bank, off;
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *ptes, opte;
#if defined(I386_CPU)
	boolean_t needs_update = FALSE;
#endif
d787 3
a789 231
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
		return;
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return;
	}

	/* set pv_head => pmap locking */
	PMAP_HEAD_TO_MAP_LOCK();

	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	for (pve = pvh->pvh_list ; pve != NULL ; pve = pve->pv_next) {
		ptes = pmap_map_ptes(pve->pv_pmap);		/* locks pmap */

#ifdef DIAGNOSTIC
		if (pve->pv_ptp && (pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] &
				    PG_FRAME)
		    != VM_PAGE_TO_PHYS(pve->pv_ptp)) {
			printf("pmap_page_remove: pg=%p: va=%lx, pv_ptp=%p\n",
			       pg, pve->pv_va, pve->pv_ptp);
			printf("pmap_page_remove: PTP's phys addr: "
			       "actual=%x, recorded=%lx\n",
			       (pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] &
				PG_FRAME), VM_PAGE_TO_PHYS(pve->pv_ptp));
			panic("pmap_page_remove: mapped managed page has "
			      "invalid pv_ptp field");
		}
#endif

		opte = ptes[i386_btop(pve->pv_va)];
		ptes[i386_btop(pve->pv_va)] = 0;		/* zap! */

		if (opte & PG_W)
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;

		if (pmap_is_curpmap(pve->pv_pmap)) {
#if defined(I386_CPU)
			if (cpu_class == CPUCLASS_386)
				needs_update = TRUE;
			else
#endif
				pmap_update_pg(pve->pv_va);
		}

		/* sync R/M bits */
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));

		/* update the PTP reference count.  free if last reference. */
		if (pve->pv_ptp) {
			pve->pv_ptp->wire_count--;
			if (pve->pv_ptp->wire_count <= 1) {
				/* zap! */
				pve->pv_pmap->pm_pdir[pdei(pve->pv_va)] = 0;
				pmap_update_pg(((vaddr_t)ptes) +
					       pve->pv_ptp->offset);
#if defined(I386_CPU)
				needs_update = FALSE;
#endif
				pve->pv_pmap->pm_stats.resident_count--;
				/* update hint? */
				if (pve->pv_pmap->pm_ptphint == pve->pv_ptp)
					pve->pv_pmap->pm_ptphint =
					    pve->pv_pmap->pm_obj.memq.tqh_first;
				pve->pv_ptp->wire_count = 0;
				uvm_pagefree(pve->pv_ptp);
			}
		}
		pmap_unmap_ptes(pve->pv_pmap);		/* unlocks pmap */
	}
	pmap_free_pvs(NULL, pvh->pvh_list);
	pvh->pvh_list = NULL;
	simple_unlock(&pvh->pvh_lock);
	PMAP_HEAD_TO_MAP_UNLOCK();
#if defined(I386_CPU)
	if (needs_update)
		pmap_update();
#endif
}

/*
 * p m a p   a t t r i b u t e  f u n c t i o n s
 * functions that test/change managed page's attributes
 * since a page can be mapped multiple times we must check each PTE that
 * maps it by going down the pv lists.
 */

/*
 * pmap_test_attrs: test a page's attributes
 *
 * => we set pv_head => pmap locking
 */

boolean_t
pmap_test_attrs(pg, testbits)
	struct vm_page *pg;
	int testbits;
{
	int bank, off;
	char *myattrs;
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *ptes, pte;

	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_test_attrs: unmanaged page?\n");
		return(FALSE);
	}

	/*
	 * before locking: see if attributes are already set and if so,
	 * return!
	 */

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	if (*myattrs & testbits)
		return(TRUE);

	/* test to see if there is a list before bothering to lock */
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return(FALSE);
	}

	/* nope, gonna have to do it the hard way */
	PMAP_HEAD_TO_MAP_LOCK();
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	for (pve = pvh->pvh_list; pve != NULL && (*myattrs & testbits) == 0;
	     pve = pve->pv_next) {
		ptes = pmap_map_ptes(pve->pv_pmap);
		pte = ptes[i386_btop(pve->pv_va)];
		pmap_unmap_ptes(pve->pv_pmap);
		*myattrs |= pte;
	}

	/*
	 * note that we will exit the for loop with a non-null pve if
	 * we have found the bits we are testing for.
	 */

	simple_unlock(&pvh->pvh_lock);
	PMAP_HEAD_TO_MAP_UNLOCK();
	return((*myattrs & testbits) != 0);
}

/*
 * pmap_change_attrs: change a page's attributes
 *
 * => we set pv_head => pmap locking
 * => we return TRUE if we cleared one of the bits we were asked to
 */

boolean_t
pmap_change_attrs(pg, setbits, clearbits)
	struct vm_page *pg;
	int setbits, clearbits;
{
	u_int32_t result;
	int bank, off;
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *ptes, npte;
	char *myattrs;
#if defined(I386_CPU)
	boolean_t needs_update = FALSE;
#endif

	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_change_attrs: unmanaged page?\n");
		return(FALSE);
	}

	PMAP_HEAD_TO_MAP_LOCK();
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	result = *myattrs & clearbits;
	*myattrs = (*myattrs | setbits) & ~clearbits;

	for (pve = pvh->pvh_list; pve != NULL; pve = pve->pv_next) {
#ifdef DIAGNOSTIC
		if (pve->pv_va >= uvm.pager_sva && pve->pv_va < uvm.pager_eva) {
			printf("pmap_change_attrs: found pager VA on pv_list");
		}
		if (!pmap_valid_entry(pve->pv_pmap->pm_pdir[pdei(pve->pv_va)]))
			panic("pmap_change_attrs: mapping without PTP "
			      "detected");
#endif

		ptes = pmap_map_ptes(pve->pv_pmap);		/* locks pmap */
		npte = ptes[i386_btop(pve->pv_va)];
		result |= (npte & clearbits);
		npte = (npte | setbits) & ~clearbits;
		if (ptes[i386_btop(pve->pv_va)] != npte) {
			ptes[i386_btop(pve->pv_va)] = npte;	/* zap! */

			if (pmap_is_curpmap(pve->pv_pmap)) {
#if defined(I386_CPU)
				if (cpu_class == CPUCLASS_386)
					needs_update = TRUE;
				else
#endif
					pmap_update_pg(pve->pv_va);
			}
		}
		pmap_unmap_ptes(pve->pv_pmap);		/* unlocks pmap */
	}

	simple_unlock(&pvh->pvh_lock);
	PMAP_HEAD_TO_MAP_UNLOCK();

#if defined(I386_CPU)
	if (needs_update)
		pmap_update();
#endif
	return(result != 0);
d792 5
a796 12
/*
 * p m a p   p r o t e c t i o n   f u n c t i o n s
 */

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */
d799 1
a799 1
 * pmap_protect: set the protection in of the pages in a pmap
d801 2
a802 7
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_write_protect: write-protect pages in a pmap
a803 1

d805 1
a805 1
pmap_write_protect(pmap, sva, eva, prot)
d807 1
a807 2
	vaddr_t sva, eva;
	vm_prot_t prot;
d809 4
a812 14
	pt_entry_t *ptes, *spte, *epte, npte;
	struct pmap_remove_record pmap_rr, *prr;
	vaddr_t blockend, va;
	u_int32_t md_prot;

	ptes = pmap_map_ptes(pmap);		/* locks pmap */

	/* need to worry about TLB? [TLB stores protection bits] */
	if (pmap_is_curpmap(pmap)) {
		prr = &pmap_rr;
		prr->prr_npages = 0;
	} else {
		prr = NULL;
	}
a813 1
	/* should be ok, but just in case ... */
d817 10
a826 1
	for (/* null */ ; sva < eva ; sva = blockend) {
d828 10
a837 12
		blockend = (sva & PD_MASK) + NBPD;
		if (blockend > eva)
			blockend = eva;

		/*
		 * XXXCDC: our PTE mappings should never be write-protected!
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */
d839 4
a842 2
		/* XXXCDC: ugly hack to avoid freeing PDP here */
		if (pdei(sva) == PDSLOT_PTE)
d844 1
d846 9
a854 3
		/* empty block? */
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
			continue;
d856 14
a869 6
		md_prot = protection_codes[prot];
		if (sva < VM_MAXUSER_ADDRESS)
			md_prot |= PG_u;
		else if (sva < VM_MAX_ADDRESS)
			/* XXX: write-prot our PTES? never! */
			md_prot |= (PG_u | PG_RW);
d871 1
a871 2
		spte = &ptes[i386_btop(sva)];
		epte = &ptes[i386_btop(blockend)];
d873 6
a878 1
		for (/*null */; spte < epte ; spte++) {
d880 1
a880 2
			if (!pmap_valid_entry(*spte))	/* no mapping? */
				continue;
d882 9
a890 1
			npte = (*spte & ~PG_PROT) | md_prot;
d892 3
a894 2
			if (npte != *spte) {
				*spte = npte;		/* zap! */
d896 6
a901 22
				if (prr) {    /* worried about tlb flushing? */
					va = i386_ptob(spte - ptes);
					if (npte & PG_G) {
						/* PG_G requires this */
						pmap_update_pg(va);
					} else {
						if (prr->prr_npages <
						    PMAP_RR_MAX) {
							prr->prr_vas[
							    prr->prr_npages++] =
								va;
						} else {
						    if (prr->prr_npages ==
							PMAP_RR_MAX)
							/* signal an overflow */
							    prr->prr_npages++;
						}
					}
				}	/* if (prr) */
			}	/* npte != *spte */
		}	/* for loop */
	}
d903 1
a903 3
	/*
	 * if we kept a removal record and removed some pages update the TLB
	 */
d905 2
a906 5
	if (prr && prr->prr_npages) {
#if defined(I386_CPU)
		if (cpu_class == CPUCLASS_386) {
			pmap_update();
		} else
d908 2
a909 10
		{ /* not I386 */
			if (prr->prr_npages > PMAP_RR_MAX) {
				pmap_update();
			} else {
				while (prr->prr_npages) {
					pmap_update_pg(prr->prr_vas[
						       --prr->prr_npages]);
				}
			}
		} /* not I386 */
d911 3
a913 1
	pmap_unmap_ptes(pmap);		/* unlocks pmap */
d917 5
a921 1
 * end of protection functions
d923 19
d943 2
a944 5
/*
 * pmap_unwire: clear the wired bit in the PTE
 *
 * => mapping should already be in map
 */
d946 4
a949 7
void
pmap_change_wiring(pmap, va, wired)
	struct pmap *pmap;
	vaddr_t va;
	boolean_t wired;
{
	pt_entry_t *ptes;
d951 3
a953 2
	if (pmap_valid_entry(pmap->pm_pdir[pdei(va)])) {
		ptes = pmap_map_ptes(pmap);		/* locks pmap */
d955 3
a957 3
#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(ptes[i386_btop(va)]))
			panic("pmap_unwire: invalid (unmapped) va");
d959 5
a963 2
		if ((ptes[i386_btop(va)] & PG_W) != 0) {
			ptes[i386_btop(va)] &= ~PG_W;
d965 10
a974 7
		}
#if 0
#ifdef DIAGNOSITC
		else {
			printf("pmap_unwire: wiring for pmap %p va 0x%lx "
			       "didn't change!\n", pmap, va);
		}
d976 3
d980 13
a992 1
		pmap_unmap_ptes(pmap);		/* unlocks map */
d994 3
a996 5
#ifdef DIAGNOSTIC
	else {
		panic("pmap_unwire: invalid PDE");
	}
#endif
d1000 2
a1001 4
 * pmap_collect: free resources held by a pmap
 *
 * => optional function.
 * => called when a process is swapped out to free memory.
a1002 1

d1004 4
a1007 2
pmap_collect(pmap)
	struct pmap *pmap;
d1009 8
a1016 4
	/*
	 * free all of the pt pages by removing the physical mappings
	 * for its entire address space.
	 */
d1018 4
a1021 2
	pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
}
d1023 2
a1024 9
/*
 * pmap_transfer: transfer (move or copy) mapping from one pmap
 * 	to another.
 *
 * => this function is optional, it doesn't have to do anything
 * => we assume that the mapping in the src pmap is valid (i.e. that
 *    it doesn't run off the end of the map's virtual space).
 * => we assume saddr, daddr, and len are page aligned/lengthed
 */
d1026 2
a1027 16
void
pmap_transfer(dstpmap, srcpmap, daddr, len, saddr, move)
	struct pmap *dstpmap, *srcpmap;
	vaddr_t daddr, saddr;
	vsize_t len;
	boolean_t move;
{
	/* base address of PTEs, dst could be NULL */
	pt_entry_t *srcptes, *dstptes;

	struct pmap_transfer_location srcl, dstl;
	int dstvalid;		  /* # of PTEs left in dst's current PTP */
	struct pmap *mapped_pmap; /* the pmap we passed to pmap_map_ptes */
	vsize_t blklen;
	int blkpgs, toxfer;
	boolean_t ok;
a1028 1
#ifdef DIAGNOSTIC
d1030 2
a1031 2
	 * sanity check: let's make sure our len doesn't overflow our dst
	 * space.
d1033 6
d1040 9
a1048 15
	if (daddr < VM_MAXUSER_ADDRESS) {
		if (VM_MAXUSER_ADDRESS - daddr < len) {
			printf("pmap_transfer: no room in user pmap "
			       "(addr=0x%lx, len=0x%lx)\n", daddr, len);
			return;
		}
	} else if (daddr < VM_MIN_KERNEL_ADDRESS ||
		   daddr >= VM_MAX_KERNEL_ADDRESS) {
		printf("pmap_transfer: invalid transfer address 0x%lx\n",
		       daddr);
	} else {
		if (VM_MAX_KERNEL_ADDRESS - daddr < len) {
			printf("pmap_transfer: no room in kernel pmap "
			       "(addr=0x%lx, len=0x%lx)\n", daddr, len);
			return;
a1049 2
	}
#endif
d1051 9
a1059 8
	/*
	 * ideally we would like to have either src or dst pmap's be the
	 * current pmap so that we can map the other one in APTE space
	 * (if needed... one of the maps could be the kernel's pmap).
	 *
	 * however, if we can't get this, then we have to use the tmpmap
	 * (alternately we could punt).
	 */
d1061 13
a1073 13
	if (!pmap_is_curpmap(dstpmap) && !pmap_is_curpmap(srcpmap)) {
		dstptes = NULL;			/* dstptes NOT mapped */
		srcptes = pmap_map_ptes(srcpmap);   /* let's map the source */
		mapped_pmap = srcpmap;
	} else {
		if (!pmap_is_curpmap(srcpmap)) {
			srcptes = pmap_map_ptes(srcpmap);   /* possible APTE */
			dstptes = PTE_BASE;
			mapped_pmap = srcpmap;
		} else {
			dstptes = pmap_map_ptes(dstpmap);   /* possible APTE */
			srcptes = PTE_BASE;
			mapped_pmap = dstpmap;
a1074 1
	}
d1076 1
a1076 6
	/*
	 * at this point we know that the srcptes are mapped.   the dstptes
	 * are mapped if (dstptes != NULL).    if (dstptes == NULL) then we
	 * will have to map the dst PTPs page at a time using the tmpmap.
	 * [XXX: is it worth the effort, or should we just punt?]
	 */
d1078 6
a1083 10
	srcl.addr = saddr;
	srcl.pte = &srcptes[i386_btop(srcl.addr)];
	srcl.ptp = NULL;
	dstl.addr = daddr;
	if (dstptes)
		dstl.pte = &dstptes[i386_btop(dstl.addr)];
	else
		dstl.pte  = NULL;		/* we map page at a time */
	dstl.ptp = NULL;
	dstvalid = 0;		/* force us to load a new dst PTP to start */
d1085 6
a1090 1
	while (len) {
d1092 3
a1094 3
		/*
		 * compute the size of this block.
		 */
d1096 26
a1121 5
		/* length in bytes */
		blklen = i386_round_pdr(srcl.addr+1) - srcl.addr;
		if (blklen > len)
			blklen = len;
		blkpgs = i386_btop(blklen);
d1123 5
a1127 4
		/*
		 * if the block is not valid in the src pmap,
		 * then we can skip it!
		 */
d1129 2
a1130 21
		if (!pmap_valid_entry(srcpmap->pm_pdir[pdei(srcl.addr)])) {
			len = len - blklen;
			srcl.pte  = srcl.pte + blkpgs;
			srcl.addr += blklen;
			dstl.addr += blklen;
			if (blkpgs > dstvalid) {
				dstvalid = 0;
				dstl.ptp = NULL;
			} else {
				dstvalid = dstvalid - blkpgs;
			}
			if (dstptes == NULL && (len == 0 || dstvalid == 0)) {
				if (dstl.pte) {
					pmap_tmpunmap_pa();
					dstl.pte = NULL;
				}
			} else {
				dstl.pte += blkpgs;
			}
			continue;
		}
d1132 3
a1134 4
		/*
		 * we have a valid source block of "blkpgs" PTEs to transfer.
		 * if we don't have any dst PTEs ready, then get some.
		 */
d1136 6
a1141 25
		if (dstvalid == 0) {
			if (!pmap_valid_entry(dstpmap->
					      pm_pdir[pdei(dstl.addr)])) {
#ifdef DIAGNOSTIC
				if (dstl.addr >= VM_MIN_KERNEL_ADDRESS)
					panic("pmap_transfer: missing kernel "
					      "PTP at 0x%lx", dstl.addr);
#endif
				dstl.ptp = pmap_get_ptp(dstpmap,
							pdei(dstl.addr), TRUE);
				if (dstl.ptp == NULL)	/* out of RAM?  punt. */
					break;
			} else {
				dstl.ptp = NULL;
			}
			dstvalid = i386_btop(i386_round_pdr(dstl.addr+1) -
					     dstl.addr);
			if (dstptes == NULL) {
				dstl.pte = (pt_entry_t *)
					pmap_tmpmap_pa(dstpmap->
						       pm_pdir[pdei(dstl.addr)]
						       & PG_FRAME);
				dstl.pte = dstl.pte + (PTES_PER_PTP - dstvalid);
			}
		}
d1143 2
d1146 31
a1176 4
		 * we have a valid source block of "blkpgs" PTEs to transfer.
		 * we have a valid dst block of "dstvalid" PTEs ready.
		 * thus we can transfer min(blkpgs, dstvalid) PTEs now.
		 */
d1178 1
a1178 5
		srcl.ptp = NULL;	/* don't know source PTP yet */
		if (dstvalid < blkpgs)
			toxfer = dstvalid;
		else
			toxfer = blkpgs;
d1180 41
a1220 3
		if (toxfer > 0) {
			ok = pmap_transfer_ptes(srcpmap, &srcl, dstpmap, &dstl,
						toxfer, move);
d1222 2
a1223 2
			if (!ok)		/* memory shortage?  punt. */
				break;
d1225 21
a1245 5
			dstvalid -= toxfer;
			blkpgs -= toxfer;
			len -= i386_ptob(toxfer);
			if (blkpgs == 0)	/* out of src PTEs?  restart */
				continue;
d1248 3
d1252 1
a1252 6
		 * we have a valid source block of "blkpgs" PTEs left
		 * to transfer.  we have just used up our "dstvalid"
		 * PTEs, and thus must obtain more dst PTEs to finish
		 * off the src block.  since we are now going to
		 * obtain a brand new dst PTP, we know we can finish
		 * the src block in one more transfer.
d1254 3
a1256 7

#ifdef DIAGNOSTIC
		if (dstvalid)
			panic("pmap_transfer: dstvalid non-zero after drain");
		if ((dstl.addr & (NBPD-1)) != 0)
			panic("pmap_transfer: dstaddr not on PD boundary "
			      "(0x%lx)\n", dstl.addr);
d1258 1
a1258 5

		if (dstptes == NULL && dstl.pte != NULL) {
			/* dispose of old PT mapping */
			pmap_tmpunmap_pa();
			dstl.pte = NULL;
d1260 1
a1260 1

d1262 2
a1263 1
		 * get new dst PTP
d1265 3
a1267 5
		if (!pmap_valid_entry(dstpmap->pm_pdir[pdei(dstl.addr)])) {
#ifdef DIAGNOSTIC
			if (dstl.addr >= VM_MIN_KERNEL_ADDRESS)
				panic("pmap_transfer: missing kernel PTP at "
				      "0x%lx", dstl.addr);
d1269 5
a1273 5
			dstl.ptp = pmap_get_ptp(dstpmap, pdei(dstl.addr), TRUE);
			if (dstl.ptp == NULL)	/* out of free RAM?  punt. */
				break;
		} else {
			dstl.ptp = NULL;
d1275 11
d1287 13
a1299 2
		dstvalid = PTES_PER_PTP;	/* new PTP */

d1301 2
a1302 2
		 * if the dstptes are un-mapped, then we need to tmpmap in the
		 * dstl.ptp.
d1304 2
d1307 9
a1315 5
		if (dstptes == NULL) {
			dstl.pte = (pt_entry_t *)
				pmap_tmpmap_pa(dstpmap->pm_pdir[pdei(dstl.addr)]
					       & PG_FRAME);
		}
d1317 3
d1321 4
a1324 3
		 * we have a valid source block of "blkpgs" PTEs left
		 * to transfer.  we just got a brand new dst PTP to
		 * receive these PTEs.
d1326 1
d1328 3
a1330 3
#ifdef DIAGNOSTIC
		if (dstvalid < blkpgs)
			panic("pmap_transfer: too many blkpgs?");
a1331 3
		toxfer = blkpgs;
		ok = pmap_transfer_ptes(srcpmap, &srcl, dstpmap, &dstl, toxfer,
					move);
d1333 4
a1336 2
		if (!ok)		/* memory shortage?   punt. */
			break;
d1338 10
a1347 3
		dstvalid -= toxfer;
		blkpgs -= toxfer;
		len -= i386_ptob(toxfer);
d1349 10
a1358 3
		/*
		 * done src pte block
		 */
a1359 3
	if (dstptes == NULL && dstl.pte != NULL)
		pmap_tmpunmap_pa();		/* dst PTP still mapped? */
	pmap_unmap_ptes(mapped_pmap);
d1363 5
a1367 6
 * pmap_transfer_ptes: transfer PTEs from one pmap to another
 *
 * => we assume that the needed PTPs are mapped and that we will
 *	not cross a block boundary.
 * => we return TRUE if we transfered all PTEs, FALSE if we were
 *	unable to allocate a pv_entry
d1369 5
a1373 7

static boolean_t
pmap_transfer_ptes(srcpmap, srcl, dstpmap, dstl, toxfer, move)
	struct pmap *srcpmap, *dstpmap;
	struct pmap_transfer_location *srcl, *dstl;
	int toxfer;
	boolean_t move;
d1375 1
a1375 13
	pt_entry_t dstproto, opte;
	int bank, off;
	struct pv_head *pvh;
	struct pv_entry *pve, *lpve;

	/*
	 * generate "prototype" dst PTE
	 */

	if (dstl->addr < VM_MAX_ADDRESS)
		dstproto = PG_u;		/* "user" page */
	else
		dstproto = pmap_pg_g;	/* kernel page */
d1377 4
a1380 3
	/*
	 * ensure we have dst PTP for user addresses.
	 */
d1382 3
a1384 3
	if (dstl->ptp == NULL && dstl->addr < VM_MAXUSER_ADDRESS)
		dstl->ptp = PHYS_TO_VM_PAGE(dstpmap->pm_pdir[pdei(dstl->addr)] &
					    PG_FRAME);
d1386 1
d1388 2
a1389 1
	 * main loop over range
d1391 5
d1397 8
a1404 3
	for (/*null*/; toxfer > 0 ; toxfer--,
			     srcl->addr += NBPG, dstl->addr += NBPG,
			     srcl->pte++, dstl->pte++) {
d1406 12
a1417 2
		if (!pmap_valid_entry(*srcl->pte))  /* skip invalid entrys */
			continue;
d1419 3
a1421 4
#ifdef DIAGNOSTIC
		if (pmap_valid_entry(*dstl->pte))
			panic("pmap_transfer_ptes: attempt to overwrite "
			      "active entry");
d1424 2
a1425 4
		/*
		 * let's not worry about non-pvlist mappings (typically device
		 * pager mappings).
		 */
d1427 12
a1438 1
		opte = *srcl->pte;
d1440 2
a1441 2
		if ((opte & PG_PVLIST) == 0)
			continue;
d1443 13
a1455 5
		/*
		 * if we are moving the mapping, then we can just adjust the
		 * current pv_entry.    if we are copying the mapping, then we
		 * need to allocate a new pv_entry to account for it.
		 */
d1457 4
a1460 7
		if (move == FALSE) {
			pve = pmap_alloc_pv(dstpmap, ALLOCPV_TRY);
			if (pve == NULL)
				return(FALSE); 		/* punt! */
		} else {
			pve = NULL;  /* XXX: quiet gcc warning */
		}
d1462 5
a1466 4
		/*
		 * find the pv_head for this mapping.  since our mapping is
		 * on the pvlist (PG_PVLIST), there must be a pv_head.
		 */
d1468 4
a1471 5
		bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
		if (bank == -1)
			panic("pmap_transfer_ptes: PG_PVLIST PTE and "
			      "no pv_head!");
d1473 2
a1474 1
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
d1476 13
a1488 4
		/*
		 * now lock down the pvhead and find the current entry (there
		 * must be one).
		 */
d1490 4
a1493 9
		simple_lock(&pvh->pvh_lock);
		for (lpve = pvh->pvh_list ; lpve ; lpve = lpve->pv_next)
			if (lpve->pv_pmap == srcpmap &&
			    lpve->pv_va == srcl->addr)
				break;
#ifdef DIAGNOSTIC
		if (lpve == NULL)
			panic("pmap_transfer_ptes: PG_PVLIST PTE, but "
			      "entry not found");
d1495 1
d1497 19
a1515 13
		/*
		 * update src ptp.   if the ptp is null in the pventry, then
		 * we are not counting valid entrys for this ptp (this is only
		 * true for kernel PTPs).
		 */

		if (srcl->ptp == NULL)
			srcl->ptp = lpve->pv_ptp;
#ifdef DIAGNOSTIC
		if (srcl->ptp &&
		    (srcpmap->pm_pdir[pdei(srcl->addr)] & PG_FRAME) !=
		    VM_PAGE_TO_PHYS(srcl->ptp))
			panic("pmap_transfer_ptes: pm_pdir - pv_ptp mismatch!");
d1518 2
a1519 5
		/*
		 * for move, update the pve we just found (lpve) to
		 * point to its new mapping.  for copy, init the new
		 * pve and put it in the list.
		 */
d1521 1
a1521 10
		if (move == TRUE) {
			pve = lpve;
		}
		pve->pv_pmap = dstpmap;
		pve->pv_va = dstl->addr;
		pve->pv_ptp = dstl->ptp;
		if (move == FALSE) {		/* link in copy */
			pve->pv_next = lpve->pv_next;
			lpve->pv_next = pve;
		}
d1523 8
a1530 3
		/*
		 * sync the R/M bits while we are here.
		 */
d1532 2
a1533 1
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
d1535 8
a1542 20
		/*
		 * now actually update the ptes and unlock the pvlist.
		 */

		if (move) {
			*srcl->pte = 0;		/* zap! */
			if (pmap_is_curpmap(srcpmap))
				pmap_update_pg(srcl->addr);
			if (srcl->ptp)
				/* don't bother trying to free PTP */
				srcl->ptp->wire_count--;
			srcpmap->pm_stats.resident_count--;
			if (opte & PG_W)
				srcpmap->pm_stats.wired_count--;
		}
		*dstl->pte = (opte & ~(PG_u|PG_U|PG_M|PG_G|PG_W)) | dstproto;
		dstpmap->pm_stats.resident_count++;
		if (dstl->ptp)
			dstl->ptp->wire_count++;
		simple_unlock(&pvh->pvh_lock);
d1544 3
a1546 1
	return(TRUE);
d1548 3
d1553 2
a1554 4
 * pmap_copy: copy mappings from one pmap to another
 *
 * => optional function
 * void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
d1556 15
d1573 3
a1575 1
 * defined as macro call to pmap_transfer in pmap.h
d1577 4
d1582 4
a1585 6
/*
 * pmap_move: move mappings from one pmap to another
 *
 * => optional function
 * void pmap_move(dst_pmap, src_pmap, dst_addr, len, src_addr)
 */
d1587 7
a1593 3
/*
 * defined as macro call to pmap_transfer in pmap.h
 */
d1596 8
a1603 1
 * pmap_enter: enter a mapping into a pmap
d1605 3
a1607 2
 * => must be done "now" ... no lazy-evaluation
 * => we set pmap => pv_head locking
d1610 5
a1614 7
int
_pmap_enter(pmap, va, pa, prot, flags)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
a1615 6
	pt_entry_t *ptes, opte, npte;
	struct vm_page *ptp;
	struct pv_head *pvh;
	struct pv_entry *pve;
	int bank, off, error;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d1617 23
d1641 3
a1643 3
	/* sanity check: totally out of range? */
	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter: too big");
d1645 4
a1648 7
	if (va == (vaddr_t) PDP_BASE || va == (vaddr_t) APDP_BASE)
		panic("pmap_enter: trying to map over PDP/APDP!");

	/* sanity check: kernel PTPs should already have been pre-allocated */
	if (va >= VM_MIN_KERNEL_ADDRESS &&
	    !pmap_valid_entry(pmap->pm_pdir[pdei(va)]))
		panic("pmap_enter: missing kernel PTP!");
d1651 5
a1655 2
	/* get lock */
	PMAP_MAP_TO_HEAD_LOCK();
d1657 1
a1657 3
	/*
	 * map in ptes and get a pointer to our PTP (unless we are the kernel)
	 */
d1659 11
a1669 10
	ptes = pmap_map_ptes(pmap);		/* locks pmap */
	if (pmap == pmap_kernel()) {
		ptp = NULL;
	} else {
		ptp = pmap_get_ptp(pmap, pdei(va), FALSE);
		if (ptp == NULL) {
			if (flags & PMAP_CANFAIL) {
				return (KERN_RESOURCE_SHORTAGE);
			}
			panic("pmap_enter: get ptp failed");
d1671 1
a1671 8
	}
	opte = ptes[i386_btop(va)];		/* old PTE */

	/*
	 * is there currently a valid mapping at our VA?
	 */

	if (pmap_valid_entry(opte)) {
d1674 1
a1674 3
		 * first, update pm_stats.  resident count will not
		 * change since we are replacing/changing a valid
		 * mapping.  wired count might change...
d1676 1
d1678 9
a1686 4
		if (wired && (opte & PG_W) == 0)
			pmap->pm_stats.wired_count++;
		else if (!wired && (opte & PG_W) != 0)
			pmap->pm_stats.wired_count--;
d1688 6
a1693 4
		/*
		 * is the currently mapped PA the same as the one we
		 * want to map?
		 */
d1695 9
a1703 1
		if ((opte & PG_FRAME) == pa) {
d1705 9
a1713 21
			/* if this is on the PVLIST, sync R/M bit */
			if (opte & PG_PVLIST) {
				bank = vm_physseg_find(atop(pa), &off);
#ifdef DIAGNOSTIC
				if (bank == -1)
					panic("pmap_enter: PG_PVLIST mapping "
					    "with unmanaged page");
#endif
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				simple_lock(&pvh->pvh_lock);
				vm_physmem[bank].pmseg.attrs[off] |= opte;
				simple_unlock(&pvh->pvh_lock);
			} else {
				pvh = NULL;	/* ensure !PG_PVLIST */
			}
			goto enter_now;
		}

		/*
		 * changing PAs: we must remove the old one first
		 */
d1715 4
a1718 4
		/*
		 * if current mapping is on a pvlist,
		 * remove it (sync R/M bits)
		 */
d1720 6
a1725 22
		if (opte & PG_PVLIST) {
			bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
			if (bank == -1)
				panic("pmap_enter: PG_PVLIST mapping with "
				    "unmanaged page");
#endif
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pve = pmap_remove_pv(pvh, pmap, va);
			vm_physmem[bank].pmseg.attrs[off] |= opte;
			simple_unlock(&pvh->pvh_lock);
		} else {
			pve = NULL;
		}
	} else {	/* opte not valid */
		pve = NULL;
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
		if (ptp)
			ptp->wire_count++;      /* count # of valid entrys */
d1729 2
a1730 5
	 * at this point pm_stats has been updated.   pve is either NULL
	 * or points to a now-free pv_entry structure (the latter case is
	 * if we called pmap_remove_pv above).
	 *
	 * if this entry is to be on a pvlist, enter it now.
d1732 6
a1737 12

	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
		if (pve == NULL) {
			pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
			if (pve == NULL) {
				if (flags & PMAP_CANFAIL) {
					error = KERN_RESOURCE_SHORTAGE;
					goto out;
				}
				panic("pmap_enter: no pv entries available");
a1739 8
		/* lock pvh when adding */
		pmap_enter_pv(pvh, pve, pmap, va, ptp);
	} else {

		/* new mapping is not PG_PVLIST.   free pve if we've got one */
		pvh = NULL;		/* ensure !PG_PVLIST */
		if (pve)
			pmap_free_pv(pmap, pve);
d1741 2
a1742 30

enter_now:
	/*
	 * at this point pvh is !NULL if we want the PG_PVLIST bit set
	 */

	npte = pa | protection_codes[prot] | PG_V;
	if (pvh)
		npte |= PG_PVLIST;
	if (wired)
		npte |= PG_W;
	if (va < VM_MAXUSER_ADDRESS)
		npte |= PG_u;
	else if (va < VM_MAX_ADDRESS)
		npte |= (PG_u | PG_RW);	/* XXXCDC: no longer needed? */
	if (pmap == pmap_kernel())
		npte |= pmap_pg_g;

	ptes[i386_btop(va)] = npte;		/* zap! */

	if ((opte & ~(PG_M|PG_U)) != npte && pmap_is_curpmap(pmap))
		pmap_update_pg(va);

	error = KERN_SUCCESS;

out:
	pmap_unmap_ptes(pmap);
	PMAP_MAP_TO_HEAD_UNLOCK();

	return error;
d1746 3
a1748 4
 * pmap_growkernel: increase usage of KVM space
 *
 * => we allocate new PTPs for the kernel and install them in all
 *	the pmaps on the system.
d1750 4
a1753 4

vaddr_t
pmap_growkernel(maxkvaddr)
	vaddr_t maxkvaddr;
d1755 3
a1757 2
	struct pmap *kpm = pmap_kernel(), *pm;
	int needed_kpde;   /* needed number of kernel PTPs */
d1759 1
a1759 1
	paddr_t ptaddr;
d1761 10
a1770 4
	needed_kpde = (int)(maxkvaddr - VM_MIN_KERNEL_ADDRESS + (NBPD-1))
		/ NBPD;
	if (needed_kpde <= nkpde)
		goto out;		/* we are OK */
d1773 1
a1773 1
	 * whoops!   we need to add kernel PTPs
d1775 2
d1778 7
a1784 6
	s = splhigh();	/* to be safe */
	simple_lock(&kpm->pm_obj.vmobjlock);

	for (/*null*/ ; nkpde < needed_kpde ; nkpde++) {

		if (uvm.page_init_done == FALSE) {
d1787 1
a1787 3
			 * we're growing the kernel pmap early (from
			 * uvm_pageboot_alloc()).  this case must be
			 * handled a little differently.
d1789 7
d1797 4
a1800 5
			if (uvm_page_physget(&ptaddr) == FALSE)
				panic("pmap_growkernel: out of memory");

			kpm->pm_pdir[PDSLOT_KERN + nkpde] =
				ptaddr | PG_RW | PG_V;
d1802 2
a1803 3
			/* count PTP as resident */
			kpm->pm_stats.resident_count++;
			continue;
d1805 4
d1810 16
a1825 8
		/*
		 * THIS *MUST* BE CODED SO AS TO WORK IN THE
		 * pmap_initialized == FALSE CASE!  WE MAY BE
		 * INVOKED WHILE pmap_init() IS RUNNING!
		 */

		if (pmap_alloc_ptp(kpm, PDSLOT_KERN + nkpde, FALSE) == NULL) {
			panic("pmap_growkernel: alloc ptp failed");
d1827 3
d1831 7
a1837 2
		/* PG_u not for kernel */
		kpm->pm_pdir[PDSLOT_KERN + nkpde] &= ~PG_u;
d1839 8
a1846 6
		/* distribute new kernel PTP to all active pmaps */
		simple_lock(&pmaps_lock);
		for (pm = pmaps.lh_first; pm != NULL;
		     pm = pm->pm_list.le_next) {
			pm->pm_pdir[PDSLOT_KERN + nkpde] =
				kpm->pm_pdir[PDSLOT_KERN + nkpde];
a1847 1
		simple_unlock(&pmaps_lock);
d1849 1
a1849 6

	simple_unlock(&kpm->pm_obj.vmobjlock);
	splx(s);

out:
	return (VM_MIN_KERNEL_ADDRESS + (nkpde * NBPD));
d1852 1
a1852 9
#ifdef DEBUG
void pmap_dump __P((struct pmap *, vaddr_t, vaddr_t));

/*
 * pmap_dump: dump all the mappings from a pmap
 *
 * => caller should not be holding any pmap locks
 */

d1854 3
a1856 3
pmap_dump(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva, eva;
d1858 2
a1859 2
	pt_entry_t *ptes, *pte;
	vaddr_t blkendva;
d1861 4
a1864 4
	/*
	 * if end is out of range truncate.
	 * if (end == start) update to max.
	 */
d1866 13
a1878 2
	if (eva > VM_MAXUSER_ADDRESS || eva <= sva)
		eva = VM_MAXUSER_ADDRESS;
d1880 7
a1886 3
	/*
	 * we lock in the pmap => pv_head direction
	 */
d1888 16
a1903 28
	PMAP_MAP_TO_HEAD_LOCK();
	ptes = pmap_map_ptes(pmap);	/* locks pmap */

	/*
	 * dumping a range of pages: we dump in PTP sized blocks (4MB)
	 */

	for (/* null */ ; sva < eva ; sva = blkendva) {

		/* determine range of block */
		blkendva = i386_round_pdr(sva+1);
		if (blkendva > eva)
			blkendva = eva;

		/* valid block? */
		if (!pmap_valid_entry(pmap->pm_pdir[pdei(sva)]))
			continue;

		pte = &ptes[i386_btop(sva)];
		for (/* null */; sva < blkendva ; sva += NBPG, pte++) {
			if (!pmap_valid_entry(*pte))
				continue;
			printf("va %#lx -> pa %#x (pte=%#x)\n",
			       sva, *pte, *pte & PG_FRAME);
		}
	}
	pmap_unmap_ptes(pmap);
	PMAP_MAP_TO_HEAD_UNLOCK();
@


1.34.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.2 2001/04/18 16:07:21 niklas Exp $	*/
d1785 1
a1785 1
	(void) pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_pdir,
d2035 1
a2035 1
pmap_extract(pmap, va, pap)
d2054 12
d2918 1
a2918 1
pmap_unwire(pmap, va)
d2921 1
@


1.34.2.4
log
@Initial import of some SMP code from NetBSD.
Not really working here yet, but there is some work in progress.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.3 2001/07/04 10:16:39 niklas Exp $	*/
a267 2
#ifdef notyet /* XXX */

a285 2
#endif /* XXX notyet */

a901 1
#ifdef notyet /* XXX */
a909 1
#endif /* XXX notyet */
@


1.34.2.5
log
@Re-enable pmap_main_lock.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.4 2001/07/14 10:02:30 ho Exp $	*/
d268 2
d288 2
d906 1
d915 1
@


1.34.2.6
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.34.2.5 2001/07/15 13:38:16 ho Exp $	*/
/*	$NetBSD: pmap.c,v 1.91 2000/06/02 17:46:37 thorpej Exp $	*/
d78 1
a268 5
#ifdef __OpenBSD__
#define spinlockinit(lock, name, flags)  lockinit(lock, 0, name, 0, flags)
#define spinlockmgr(lock, flags, slock) lockmgr(lock, flags, slock, curproc)
#endif

d989 1
a989 1
	if (addr == 0)
d1021 1
a1021 1
	pv_cachedva = 0;   /* a VA we have allocated but not used yet */
d1156 1
a1156 1
	if (pv_cachedva == 0) {
d1159 1
a1159 1
		if (pv_cachedva == 0) {
d1197 1
a1197 1
	pv_cachedva = 0;
d1579 1
a1579 1
			    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
a1586 2
		/* stole one; zero it. */
		pmap_zero_page(VM_PAGE_TO_PHYS(ptp));
d1592 1
d2082 1
a2082 1
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
a2110 22
 * pmap_zero_page_uncached: the same, except uncached.
 */

void
pmap_zero_page_uncached(pa)
	paddr_t pa;
{
	simple_lock(&pmap_zero_page_lock);
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page_uncached: lock botch");
#endif

	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW |	/* map in */
	    ((cpu_class != CPUCLASS_386) ? PG_N : 0);
	memset(zerop, 0, NBPG);				/* zero */
	*zero_pte = 0;					/* zap! */
	pmap_update_pg((vaddr_t)zerop);			/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
}

/*
d2217 1
a2217 2
			      "PG_PVLIST, va = 0x%lx, pa = 0x%lx",
			      startva, (u_long)(opte & PG_FRAME));
d2283 1
a2283 1
			panic("pmap_remove_pte: managed page without "
d2292 1
a2292 3
		panic("pmap_remove_pte: unmanaged page marked "
		    "PG_PVLIST, va = 0x%lx, pa = 0x%lx", va,
		    (u_long)(opte & PG_FRAME));
d2719 1
a2719 1
			printf("pmap_change_attrs: found pager VA on pv_list\n");
d2923 2
a2924 1
#ifdef DIAGNOSTIC
d2930 1
d3432 1
a3432 1
pmap_enter(pmap, va, pa, prot, flags)
d3510 2
a3511 4
					panic("pmap_enter: same pa PG_PVLIST "
					      "mapping with unmanaged page "
					      "pa = 0x%lx (0x%lx)", pa,
					      atop(pa));
d3537 1
a3537 2
				      "unmanaged page "
				      "pa = 0x%lx (0x%lx)", pa, atop(pa));
a3657 1
			pmap_zero_page(ptaddr);
a3665 6

		/*
		 * THIS *MUST* BE CODED SO AS TO WORK IN THE
		 * pmap_initialized == FALSE CASE!  WE MAY BE
		 * INVOKED WHILE pmap_init() IS RUNNING!
		 */
@


1.34.2.7
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d77 3
d2119 1
a2119 1
boolean_t
a2134 2

	return (TRUE);
@


1.34.2.8
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.7 2001/11/13 21:00:52 niklas Exp $	*/
d271 5
a275 5
struct simplelock pvalloc_lock;
struct simplelock pmaps_lock;
struct simplelock pmap_copy_page_lock;
struct simplelock pmap_zero_page_lock;
struct simplelock pmap_tmpptp_lock;
d582 1
a582 1
			tlbflush();
d610 1
a610 1
 * space.   pmap_kremove are exported to MI kernel.
d671 39
a709 1
		tlbflush();
d962 1
a962 1
	tlbflush();
d1447 1
a1447 1
	struct vm_map_entry *dead_entries;
d1678 1
a1678 1
					tlbflush();
d2514 1
a2514 1
			tlbflush();
d2519 1
a2519 1
				tlbflush();
d2634 1
a2634 1
		tlbflush();
d2747 3
d2779 1
a2779 1
		tlbflush();
d2907 1
a2907 1
			tlbflush();
d2912 1
a2912 1
				tlbflush();
d3501 1
a3501 1
				return (ENOMEM);
d3601 1
a3601 1
					error = ENOMEM;
d3639 1
a3639 1
	error = 0;
@


1.34.2.9
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d63 7
a75 1
#include <sys/kernel.h>
d147 2
a148 2
 *	- plan 1: done at pmap_create() we use
 *	  uvm_km_alloc(kernel_map, PAGE_SIZE)  [fka kmem_alloc] to do this
d163 1
a163 1
 * 	call uvm_pagealloc()
d165 8
a172 2
 * 		=> failure: we are out of free vm_pages, let pmap_enter()
 *		   tell UVM about it.
d195 8
a202 1
 *	If we fail, we simply let pmap_enter() tell UVM about it.
d264 6
d287 10
d610 2
a611 2
 * space.   pmap_kremove is exported to MI kernel.  we make use of
 * the recursive PTE mappings.
d643 1
a643 1
 * => we assume the va is page aligned and the len is a multiple of PAGE_SIZE
d691 2
d702 3
d724 5
d733 1
d798 1
a798 1
		     kva += PAGE_SIZE)
d813 2
a814 2
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;	/* allocate */
	virtual_avail += PAGE_SIZE; pte++;			/* advance */
d817 1
a817 1
	virtual_avail += PAGE_SIZE; pte++;
d820 1
a820 1
	virtual_avail += PAGE_SIZE; pte++;
d823 1
a823 1
	virtual_avail += PAGE_SIZE; pte++;
d827 1
a827 1
	virtual_avail += PAGE_SIZE; pte++;
d839 3
a841 3
	virtual_avail += PAGE_SIZE; pte++;
	idt_paddr = avail_start;			/* steal a page */
	avail_start += PAGE_SIZE;
d846 1
a846 1
	virtual_avail += PAGE_SIZE; pte++;
d865 1
d872 1
d882 1
a882 1
	    &pool_allocator_nointr);
d936 1
a936 1
	int npages, lcv, i;
a963 5
		for (i = 0;
		     i < (vm_physmem[lcv].end - vm_physmem[lcv].start); i++) {
			simple_lock_init(
			    &vm_physmem[lcv].pmseg.pvhead[i].pvh_lock);
		}
d981 1
a981 1
	pv_initpage = (struct pv_page *) uvm_km_alloc(kernel_map, PAGE_SIZE);
d1354 1
a1354 1
	if (pv_nfpvents > PVE_HIWAT && TAILQ_FIRST(&pv_unusedpgs) != NULL &&
d1385 1
a1385 1
	if (pv_nfpvents > PVE_HIWAT && TAILQ_FIRST(&pv_unusedpgs) != NULL &&
d1412 3
a1414 2
	s = splvm(); /* protect kmem_map */
	pvp = TAILQ_FIRST(&pv_unusedpgs);
a1419 1

d1424 1
d1432 2
a1433 2
		uvm_unmap_remove(map, (vaddr_t)pvp, ((vaddr_t)pvp) + PAGE_SIZE,
		    &dead_entries);
d1952 1
a1952 1
 * pmap_activate: activate a process' pmap (fill in %cr3 and LDT info)
d1954 1
a1954 1
 * => called from cpu_switch()
d2033 21
d2114 1
a2114 1
	bcopy(csrcp, cdstp, PAGE_SIZE);
d2326 1
a2326 1
	if (sva + PAGE_SIZE == eva) {
d2375 1
a2375 1
					    TAILQ_FIRST(&pmap->pm_obj.memq);
d2463 1
a2463 2
				pmap->pm_ptphint =
				    TAILQ_FIRST(&pmap->pm_obj.memq);
a2534 3
		if (pve->pv_va >= uvm.pager_sva && pve->pv_va < uvm.pager_eva) {
			printf("pmap_page_remove: found pager VA on pv_list\n");
		}
d2583 1
a2583 1
					    TAILQ_FIRST(&pve->pv_pmap->pm_obj.memq);
d2905 1
a2905 1
			panic("pmap_unwire: invalid (unmapped) va 0x%lx", va);
d3396 12
a3407 1
 * defined as macro call in pmap.h
d3460 1
a3460 1
				return (KERN_RESOURCE_SHORTAGE);
d3560 1
a3560 1
					error = KERN_RESOURCE_SHORTAGE;
@


1.34.2.10
log
@Merge in -current from about a week ago
@
text
@d370 3
a372 3
static struct pv_entry	*pmap_add_pvpage(struct pv_page *, boolean_t);
static struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t);
static struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
d376 2
a377 2
static struct pv_entry	*pmap_alloc_pvpage(struct pmap *, int);
static void		 pmap_enter_pv(struct pv_head *,
d379 13
a391 13
					    vaddr_t, struct vm_page *);
static void		 pmap_free_pv(struct pmap *, struct pv_entry *);
static void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
static void		 pmap_free_pv_doit(struct pv_entry *);
static void		 pmap_free_pvpage(void);
static struct vm_page	*pmap_get_ptp(struct pmap *, int, boolean_t);
static boolean_t	 pmap_is_curpmap(struct pmap *);
static pt_entry_t	*pmap_map_ptes(struct pmap *);
static struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
					     vaddr_t);
static boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *,
					      pt_entry_t *, vaddr_t);
static void		 pmap_remove_ptes(struct pmap *,
d394 8
a401 8
					       vaddr_t, vaddr_t);
static struct vm_page	*pmap_steal_ptp(struct uvm_object *,
					     vaddr_t);
static vaddr_t		 pmap_tmpmap_pa(paddr_t);
static pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
static void		 pmap_tmpunmap_pa(void);
static void		 pmap_tmpunmap_pvepte(struct pv_entry *);
static boolean_t	 pmap_transfer_ptes(struct pmap *,
d405 2
a406 2
					 int, boolean_t);
static boolean_t	 pmap_try_steal_pv(struct pv_head *,
d408 2
a409 2
						struct pv_entry *);
static void		pmap_unmap_ptes(struct pmap *);
d411 2
a412 2
void			pmap_pinit(pmap_t);
void			pmap_release(pmap_t);
d3622 1
a3622 1
void pmap_dump(struct pmap *, vaddr_t, vaddr_t);
@


1.34.2.11
log
@Sync the SMP branch with 3.3
@
text
@a413 2
void			pmap_zero_phys(paddr_t);

a570 25
__inline static void
pmap_nxstack_account(struct pmap *pmap, vaddr_t va,
    pt_entry_t opte, pt_entry_t npte)
{
	if (((opte ^ npte) & PG_X) &&
	    va < VM_MAXUSER_ADDRESS && va >= VM_MAXUSER_ADDRESS - MAXSSIZ) {
		struct trapframe *tf = curproc->p_md.md_regs;
		struct vm_map *map = &curproc->p_vmspace->vm_map;

		if (npte & PG_X && !(opte & PG_X)) {
			if (++pmap->pm_nxpages == 1 &&
			    pmap == vm_map_pmap(map)) {
				tf->tf_cs = GSEL(GUCODE1_SEL, SEL_UPL);
				pmap_update_pg(va);
			}
		} else {
			if (!--pmap->pm_nxpages &&
			    pmap == vm_map_pmap(map)) {
				tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
				pmap_update_pg(va);
			}
		}
	}
}

d625 1
a625 1
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx",
d703 8
a710 8
	protection_codes[UVM_PROT_NONE] = 0;  			/* --- */
	protection_codes[UVM_PROT_EXEC] = PG_X;			/* --x */
	protection_codes[UVM_PROT_READ] = PG_RO;		/* -r- */
	protection_codes[UVM_PROT_RX] = PG_X;			/* -rx */
	protection_codes[UVM_PROT_WRITE] = PG_RW;		/* w-- */
	protection_codes[UVM_PROT_WX] = PG_RW|PG_X;		/* w-x */
	protection_codes[UVM_PROT_RW] = PG_RW;			/* wr- */
	protection_codes[UVM_PROT_RWX] = PG_RW|PG_X;		/* wrx */
d1507 1
a1507 1
		pmap_zero_page(ptp);
a1698 1
	pmap->pm_nxpages = 0;
d1993 2
a1994 23
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

	simple_lock(&pmap_zero_page_lock);
#ifdef DIAGNOSTIC
	if (*zero_pte)
		panic("pmap_zero_page: lock botch");
#endif

	*zero_pte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	bzero(zerop, NBPG);				/* zero */
	*zero_pte = 0;				/* zap! */
	pmap_update_pg((vaddr_t)zerop);		/* flush TLB */
	simple_unlock(&pmap_zero_page_lock);
}

/*
 * pmap_zero_phys: same as pmap_zero_page, but for use before vm_pages are
 * initialized.
 */
void
pmap_zero_phys(paddr_t pa)
d2038 2
a2039 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a2040 3
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);

a2189 2
	pmap_nxstack_account(pmap, va, opte, 0);

d2765 1
a2765 1
		for (/*null */; spte < epte ; spte++, sva += PAGE_SIZE) {
a2772 3
				/* account for executable pages on the stack */
				pmap_nxstack_account(pmap, sva, *spte, npte);

d3089 1
a3089 1
			      "(0x%lx)", dstl.addr);
a3509 1
	pmap_nxstack_account(pmap, va, opte, npte);
d3575 1
a3575 1
			pmap_zero_phys(ptaddr);
@


1.34.2.12
log
@pmap_deactivate for MP.  Per-CPU ptes for zeroing, copying and tmp ptp.
static cleanup.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.11 2003/03/27 23:26:55 niklas Exp $	*/
a72 1
#include <machine/atomic.h>
d228 11
d245 1
d248 3
a250 3

#ifdef MULTIPROCESSOR
struct lock pmap_main_lock;
a261 10
#else

#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */

#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

#endif

a344 14
 * MULTIPROCESSOR: special VA's/ PTE's are actually allocated inside a
 * I386_MAXPROCS*NPTECL array of PTE's, to avoid cache line thrashing
 * due to false sharing.
 */

#ifdef MULTIPROCESSOR
#define PTESLEW(pte, id) ((pte)+(id)*NPTECL)
#define VASLEW(va,id) ((va)+(id)*NPTECL*NBPG)
#else
#define PTESLEW(pte, id) (pte)
#define VASLEW(va,id) (va)
#endif

/*
d370 3
a372 3
struct pv_entry	*pmap_add_pvpage(struct pv_page *, boolean_t);
__inline static struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t);
__inline static struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
d377 1
a377 1
__inline static void		 pmap_enter_pv(struct pv_head *,
d380 8
a387 9
__inline static void		 pmap_free_pv(struct pmap *, struct pv_entry *);
__inline static void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
__inline static void		 pmap_free_pv_doit(struct pv_entry *);
void		 pmap_free_pvpage(void);
struct vm_page	*pmap_get_ptp(struct pmap *, int, boolean_t);
__inline static boolean_t	 pmap_is_curpmap(struct pmap *);
__inline static boolean_t	 pmap_is_active(struct pmap *, int);
__inline static pt_entry_t	*pmap_map_ptes(struct pmap *);
__inline static struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
d389 1
a389 1
boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *,
d391 1
a391 1
void		 pmap_remove_ptes(struct pmap *,
d395 1
a395 1
struct vm_page	*pmap_steal_ptp(struct uvm_object *,
d397 5
a401 5
__inline static vaddr_t		 pmap_tmpmap_pa(paddr_t);
__inline static pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
__inline static void		 pmap_tmpunmap_pa(void);
__inline static void		 pmap_tmpunmap_pvepte(struct pv_entry *);
boolean_t	 pmap_transfer_ptes(struct pmap *,
d406 1
a406 1
boolean_t	 pmap_try_steal_pv(struct pv_head *,
d409 1
a409 1
__inline static void		pmap_unmap_ptes(struct pmap *);
a433 14
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
 */

__inline static boolean_t
pmap_is_active(pmap, cpu_id)
	struct pmap *pmap;
	int cpu_id;
{

	return (pmap == pmap_kernel() ||
	    (pmap->pm_cpus & (1U << cpu_id)) != 0);
}

/*
d435 2
d443 1
a443 5
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(ptpp, id);
d445 2
a446 2
	if (*ptpte)
		panic("pmap_tmpmap_pa: ptpte in use?");
d448 2
a449 2
	*ptpte = PG_V | PG_RW | pa;		/* always a new mapping */
	return((vaddr_t)ptpva);
d454 2
a460 5
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(ptpp, id);
d462 1
a462 1
	if (!pmap_valid_entry(*ptpte))
d465 3
a467 7
	*ptpte = 0;		/* zap! */
	pmap_update_pg((vaddr_t)ptpva);
#ifdef MULTIPROCESSOR
	/*
	 * No need for tlb shootdown here, since ptp_pte is per-CPU.
	 */
#endif
d474 1
d496 2
a793 20
#ifdef MULTIPROCESSOR
	/*
	 * Waste some VA space to avoid false sharing of cache lines
	 * for page table pages: Give each possible CPU a cache line
	 * of PTE's (8) to play with, though we only need 4.  We could
	 * recycle some of this waste by putting the idle stacks here
	 * as well; we could waste less space if we knew the largest
	 * CPU ID beforehand.
	 */
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;

	cdstp = (caddr_t) virtual_avail+PAGE_SIZE;  cdst_pte = pte+1;

	zerop = (caddr_t) virtual_avail+PAGE_SIZE*2;  zero_pte = pte+2;

	ptpp = (caddr_t) virtual_avail+PAGE_SIZE*3;  ptp_pte = pte+3;

	virtual_avail += PAGE_SIZE * I386_MAXPROCS * NPTECL;
	pte += I386_MAXPROCS * NPTECL;
#else
a804 1
#endif
d808 1
a808 1
	virtual_avail += PAGE_SIZE;
a845 1
#ifdef MULTIPROCESSOR
a846 1
#endif
d849 3
d1063 1
a1063 1
struct pv_entry *
d1195 1
a1195 1
boolean_t
d1258 1
a1258 1
struct pv_entry *
d1388 1
a1388 1
void
d1558 1
a1558 1
struct vm_page *
d1659 1
a1659 1
struct vm_page *
d1835 1
a1835 4
	/*
	 * MULTIPROCESSOR -- no need to flush out of other processors'
	 * APTE space because we do that in pmap_unmap_ptes().
	 */
d1952 1
a1952 1
	if (p == curproc) {
d1954 1
a1955 6

		/*
		 * mark the pmap in use by this processor.
		 */
		i386_atomic_setbits_l(&pmap->pm_cpus, (1U << cpu_number()));
	}
d1960 2
a1967 6
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	/*
	 * mark the pmap no longer in use by this processor.
	 */
	i386_atomic_clearbits_l(&pmap->pm_cpus, (1U << cpu_number()));
d2045 1
a2045 6
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(zerop, id);

d2047 1
a2047 1
	if (*zpte)
d2051 5
a2055 8
	*zpte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	pmap_update_pg((vaddr_t)zerova);	/* flush TLB */
	bzero(zerova, NBPG);			/* zero */
	*zpte = 0;				/* zap! */
	pmap_update_pg((vaddr_t)zerova);	/* flush TLB */
#ifdef MULTIPROCESSOR
	/* Using per-cpu VA; no shootdown required here. */
#endif
d2066 1
a2066 6
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(zerop, id);

d2068 1
a2068 1
	if (*zpte)
d2072 1
a2072 1
	*zpte = (pa & PG_FRAME) | PG_V | PG_RW |	/* map in */
d2074 4
a2077 7
	pmap_update_pg((vaddr_t)zerova);		/* flush TLB */
	memset(zerova, 0, NBPG);			/* zero */
	*zpte = 0;					/* zap! */
	pmap_update_pg((vaddr_t)zerova);		/* flush TLB */
#ifdef MULTIPROCESSOR
	/* Using per-cpu VA; no shootdown required here. */
#endif
a2090 7
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *spte = PTESLEW(csrc_pte,id);
	pt_entry_t *dpte = PTESLEW(cdst_pte,id);
	caddr_t csrcva = VASLEW(csrcp, id);
	caddr_t cdstva = VASLEW(cdstp, id);
d2092 1
d2094 1
a2094 1
	if (*spte || *dpte)
d2098 6
a2103 9
	*spte = (srcpa & PG_FRAME) | PG_V | PG_RW;
	*dpte = (dstpa & PG_FRAME) | PG_V | PG_RW;
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
	bcopy(csrcva, cdstva, PAGE_SIZE);
	*spte = *dpte = 0;			/* zap! */
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
#ifdef MULTIPROCESSOR
	/* Using per-cpu VA; no shootdown required here. */
#endif
d2121 1
a2121 1
void
d2224 1
a2224 1
boolean_t
d3222 1
a3222 1
boolean_t
d3391 1
a3391 1
 * defined as macro in pmap.h
@


1.34.2.13
log
@Remove unused code.  Create TLB shootdown stubs, but only implement normal
uniprocessor functionality, call the stubs everywhere except in pmap_steal_ptp
which is tough, and rather should be removed.  TLB shootdown API from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.12 2003/04/05 20:43:38 niklas Exp $	*/
d403 2
a404 2
boolean_t	 pmap_remove_pte(struct pmap *, struct vm_page *, pt_entry_t *,
    vaddr_t, int32_t *);
d408 1
a408 1
					       vaddr_t, vaddr_t, int32_t *);
d415 5
d476 1
a476 1
		panic("pmap_tmpmap_pa: ptp_pte in use?");
a610 3
	int32_t cpumask = 0;
	int needs_update = 0;

d620 1
a620 2
				pmap_tlb_shootdown(pmap, va, opte, &cpumask);
				needs_update = 1;
d626 1
a626 2
				pmap_tlb_shootdown(pmap, va, opte, &cpumask);
				needs_update = 1;
a628 2
		if (needs_update)
			pmap_tlb_shootnow(cpumask);
d653 1
a653 1
	pt_entry_t *pte, opte, npte;
d656 4
a659 10
	npte = pa | ((prot & VM_PROT_WRITE)? PG_RW : PG_RO) | PG_V | pmap_pg_g;
	opte = i386_atomic_testset_ul(pte, npte); /* zap! */
	if (pmap_valid_entry(opte)) {
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
a660 2
#endif
	}
d679 1
a679 2
	pt_entry_t *pte, opte;
	int32_t cpumask = 0;
a683 1
		opte = i386_atomic_testset_ul(pte, 0); /* zap! */
d685 7
a691 2
		if (opte & PG_PVLIST)
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx", va);
d693 1
a693 1
		pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
d695 4
a698 1
	pmap_tlb_shootnow(cpumask);
d1254 1
a1254 2
	pt_entry_t *ptep, opte;
	int32_t cpumask = 0;
d1273 1
a1273 6
		opte = i386_atomic_testset_ul(ptep, 0); /* zap! */
#ifdef MULTIPROCESSOR
		pmap_tlb_shootdown(cpv->pv_pmap, cpv->pv_va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
a1275 1
#endif
a1622 1
	int32_t cpumask = 0;
d1663 1
a1663 2
							 ptp_i2v(idx+1),
							 &cpumask);
a1676 4
					/*
					 * XXX How de we cope with this in
					 * XXX MP cases?
					 */
a1680 4
					/*
					 * XXX How de we cope with this in
					 * XXX MP cases?
					 */
a1702 1
	pmap_tlb_shootnow(cpumask);
d2089 13
a2101 1
	pmap_zero_phys(VM_PAGE_TO_PHYS(pg));
d2119 1
a2119 1
		panic("pmap_zero_phys: lock botch");
d2213 1
a2213 1
pmap_remove_ptes(pmap, pmap_rr, ptp, ptpva, startva, endva, cpumaskp)
a2218 1
	int32_t *cpumaskp;
d2240 2
a2241 3
		/* atomically save the old PTE and zap! it */
		opte = i386_atomic_testset_ul(pte, 0);

d2246 15
a2260 2
		pmap_tlb_shootdown(pmap, startva, opte, cpumaskp);

d2316 1
a2316 1
pmap_remove_pte(pmap, ptp, pte, va, cpumaskp)
a2320 1
	int32_t *cpumaskp;
d2341 2
a2342 1
	pmap_tlb_shootdown(pmap, va, opte, cpumaskp);
d2387 1
a2387 1
	pt_entry_t *ptes, opte;
d2392 1
a2392 1
	int32_t cpumask = 0;
d2434 1
a2434 1
			    &ptes[i386_btop(sva)], sva, &cpumask);
d2442 4
a2445 23
				/* zap! */
				opte = i386_atomic_testset_ul(
				    &pmap->pm_pdir[pdei(sva)], 0);
#ifdef MULTIPROCESSOR
				/*
				 * XXXthorpej Redundant shootdown can happen
				 * here if we're using APTE space.
				 */
#endif
				pmap_tlb_shootdown(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + ptp->offset, opte,
				    &cpumask);
#ifdef MULTIPROCESSOR
				/*
				 * Always shoot down the pmap's self-mapping
				 * of the PTP.
				 * XXXthorpej Redundant shootdown can happen
				 * here if pmap == curpcb->pcb_pmap (not APTE
				 * space).
				 */
				pmap_tlb_shootdown(pmap,
				    ((vaddr_t)PTE_BASE) + ptp->offset, opte,
				    &cpumask);
d2447 4
d2459 1
a2459 1
		pmap_tlb_shootnow(cpumask);
d2465 14
d2528 2
a2529 3
		pmap_remove_ptes(pmap, 0, ptp,
				 (vaddr_t)&ptes[i386_btop(sva)], sva, blkendva,
				 &cpumask);
d2533 6
a2538 20
			/* zap! */
			opte = i386_atomic_testset_ul(
			    &pmap->pm_pdir[pdei(sva)], 0);
#if defined(MULTIPROCESSOR)
			/*
			 * XXXthorpej Redundant shootdown can happen here
			 * if we're using APTE space.
			 */
#endif
			pmap_tlb_shootdown(curpcb->pcb_pmap,
			    ((vaddr_t)ptes) + ptp->offset, opte, &cpumask);
#if defined(MULTIPROCESSOR)
			/*
			 * Always shoot down the pmap's self-mapping
			 * of the PTP.
			 * XXXthorpej Redundant shootdown can happen here
			 * if pmap == curpcb->pcb_pmap (not APTE space).
			 */
			pmap_tlb_shootdown(pmap,
			    ((vaddr_t)PTE_BASE) + ptp->offset, opte, &cpumask);
d2549 21
a2569 1
	pmap_tlb_shootnow(cpumask);
d2589 3
a2591 1
	int32_t cpumask = 0;
d2639 8
a2646 1
		pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va, opte, &cpumask);
d2656 5
a2660 14
				opte = i386_atomic_testset_ul(
				    &pve->pv_pmap->pm_pdir[pdei(pve->pv_va)],
				    0);
				pmap_tlb_shootdown(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + pve->pv_ptp->offset,
				    opte, &cpumask);
#if defined(MULTIPROCESSOR)
				/*
				 * Always shoot down the other pmap's
				 * self-mapping of the PTP.
				 */
				pmap_tlb_shootdown(pve->pv_pmap,
				    ((vaddr_t)PTE_BASE) + pve->pv_ptp->offset,
				    opte, &cpumask);
d2677 4
a2680 1
	pmap_tlb_shootnow(cpumask);
d2768 1
a2768 1
	pt_entry_t *ptes, npte, opte;
d2770 3
a2772 1
	int32_t cpumask = 0;
d2802 10
a2811 4
			opte = i386_atomic_testset_ul(
			    &ptes[i386_btop(pve->pv_va)], npte);
			pmap_tlb_shootdown(pve->pv_pmap,
			    i386_btop(pve->pv_va), opte, &cpumask);
a2817 1
	pmap_tlb_shootnow(cpumask);
d2819 4
d2858 2
a2859 1
	vaddr_t blockend;
a2860 1
	int32_t cpumask = 0;
d2864 8
d2919 44
a2962 2
				i386_atomic_testset_ul(spte, npte); /* zap! */
				pmap_tlb_shootdown(pmap, sva, *spte, &cpumask);
d2964 1
a2964 1
		}
a2965 2

	pmap_tlb_shootnow(cpumask);
d3032 443
d3671 2
a3672 12
	if ((opte & ~(PG_M|PG_U)) != npte) {
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pmap, va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		if (pmap_is_curpmap(pmap))
			pmap_update_pg(va);
#endif
	}
a3827 33


/******************** TLB shootdown code ********************/

/* XXX These are still just uniprocessor stubs.  */

void
pmap_tlb_shootnow(int32_t cpumask)
{
#ifdef I386_CPU
	if (cpu_class == CPUCLASS_386)
		tlbflush();
#endif
}

/*
 * pmap_tlb_shootdown:
 *
 *	Cause the TLB entry for pmap/va to be shot down.
 */
void
pmap_tlb_shootdown(pmap, va, pte, cpumaskp)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t pte;
	int32_t *cpumaskp;
{
#ifdef I386_CPU
	if (cpu_class != CPUCLASS_386)
#endif
		if (pmap_is_curpmap(pmap))
			pmap_update_pg(va);
}
@


1.34.2.14
log
@Add TLB shootdown logic, mostly from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.14 2003/04/14 14:02:50 niklas Exp $	*/
d132 2
a261 43
 * TLB Shootdown:
 *
 * When a mapping is changed in a pmap, the TLB entry corresponding to
 * the virtual address must be invalidated on all processors.  In order
 * to accomplish this on systems with multiple processors, messages are
 * sent from the processor which performs the mapping change to all
 * processors on which the pmap is active.  For other processors, the
 * ASN generation numbers for that processor is invalidated, so that
 * the next time the pmap is activated on that processor, a new ASN
 * will be allocated (which implicitly invalidates all TLB entries).
 *
 * Shootdown job queue entries are allocated using a simple special-
 * purpose allocator for speed.
 */
struct pmap_tlb_shootdown_job {
	TAILQ_ENTRY(pmap_tlb_shootdown_job) pj_list;
	vaddr_t pj_va;			/* virtual address */
	pmap_t pj_pmap;			/* the pmap which maps the address */
	pt_entry_t pj_pte;		/* the PTE bits */
	struct pmap_tlb_shootdown_job *pj_nextfree;
};

struct pmap_tlb_shootdown_q {
	TAILQ_HEAD(, pmap_tlb_shootdown_job) pq_head;
	int pq_pte;			/* aggregate PTE bits */
	int pq_count;			/* number of pending requests */
	struct simplelock pq_slock;	/* spin lock on queue */
	int pq_flushg;		/* pending flush global */
	int pq_flushu;		/* pending flush user */
} pmap_tlb_shootdown_q[I386_MAXPROCS];

#define	PMAP_TLB_MAXJOBS	16

void	pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *);
struct pmap_tlb_shootdown_job *pmap_tlb_shootdown_job_get(
    struct pmap_tlb_shootdown_q *);
void	pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *,
    struct pmap_tlb_shootdown_job *);

struct simplelock pmap_tlb_shootdown_job_lock;
struct pmap_tlb_shootdown_job *pj_page, *pj_free;

/*
d405 4
a408 2
void		 pmap_remove_ptes(struct pmap *, struct vm_page *, vaddr_t,
    vaddr_t, vaddr_t, int32_t *);
a729 1
	int i;
a920 11
	/*
	 * Initialize the TLB shootdown queues.
	 */

	simple_lock_init(&pmap_tlb_shootdown_job_lock);

	for (i = 0; i < I386_MAXPROCS; i++) {
		TAILQ_INIT(&pmap_tlb_shootdown_q[i].pq_head);
		simple_lock_init(&pmap_tlb_shootdown_q[i].pq_slock);
	}

a1029 9
	pj_page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
	if (pj_page == NULL)
		panic("pmap_init: pj_page");

	for (i = 0; i < PAGE_SIZE / sizeof *pj_page - 1; i++)
		pj_page[i].pj_nextfree = &pj_page[i + 1];
	pj_page[i].pj_nextfree = NULL;
	pj_free = &pj_page[0];

d1672 5
a1676 3
					pmap_remove_ptes(pmaps_hand, ptp,
					    (vaddr_t)ptes, ptp_i2v(idx),
					    ptp_i2v(idx+1), &cpumask);
d2223 1
a2223 1
pmap_remove_ptes(pmap, ptp, ptpva, startva, endva, cpumaskp)
d2225 1
d2528 3
a2530 2
		pmap_remove_ptes(pmap, ptp, (vaddr_t)&ptes[i386_btop(sva)],
		    sva, blkendva, &cpumask);
d3239 6
d3332 2
d3337 3
a3339 48
#ifdef MULTIPROCESSOR
	struct cpu_info *ci, *self;
	CPU_INFO_ITERATOR cii;
	int s;
#ifdef DIAGNOSTIC
	int count = 0;
#endif
#endif

	if (cpumask == 0)
		return;

#ifdef MULTIPROCESSOR
	self = curcpu();
	s = splipi();
	self->ci_tlb_ipi_mask = cpumask;
#endif

#ifdef notyet
	pmap_do_tlb_shootdown(0);	/* do *our* work. */
#else
	pmap_do_tlb_shootdown();	/* do *our* work. */
#endif

#ifdef MULTIPROCESSOR
	splx(s);

	/*
	 * Send the TLB IPI to other CPUs pending shootdowns.
	 */
	for (CPU_INFO_FOREACH(cii, ci)) {
		if (ci == self)
			continue;
		if (cpumask & (1U << ci->ci_cpuid))
			if (i386_send_ipi(ci, I386_IPI_TLB) != 0)
				i386_atomic_clearbits_l(&self->ci_tlb_ipi_mask,
				    (1U << ci->ci_cpuid));
	}

	while (self->ci_tlb_ipi_mask != 0)
#ifdef DIAGNOSTIC
		if (count++ > 100000000)
			panic("TLB IPI rendezvous failed (mask %x)",
			    self->ci_tlb_ipi_mask);
#else
		/* XXX insert pause instruction */
		;
#endif
a3354 38
	struct cpu_info *ci, *self;
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
	CPU_INFO_ITERATOR cii;
	int s;

	if (pmap_initialized == FALSE) {
		pmap_update_pg(va);
		return;
	}

	self = curcpu();

	s = splipi();
#if 0
	printf("dshootdown %lx\n", va);
#endif

	for (CPU_INFO_FOREACH(cii, ci)) {
		/* Note: we queue shootdown events for ourselves here! */
		if (pmap_is_active(pmap, ci->ci_cpuid) == 0)
			continue;
		if (ci != self && !(ci->ci_flags & CPUF_RUNNING))
			continue;
		pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];
		simple_lock(&pq->pq_slock);

		/*
		 * If there's a global flush already queued, or a
		 * non-global flush, and this pte doesn't have the G
		 * bit set, don't bother.
		 */
		if (pq->pq_flushg > 0 ||
		    (pq->pq_flushu > 0 && (pte & pmap_pg_g) == 0)) {
			simple_unlock(&pq->pq_slock);
			continue;
		}

d3356 1
a3356 15
		/*
		 * i386 CPUs can't invalidate a single VA, only
		 * flush the entire TLB, so don't bother allocating
		 * jobs for them -- just queue a `flushu'.
		 *
		 * XXX note that this can be executed for non-i386
		 * when called early (before identifycpu() has set
		 * cpu_class)
		 */
		if (cpu_class == CPUCLASS_386) {
			pq->pq_flushu++;
			*cpumaskp |= 1U << ci->ci_cpuid;
			simple_unlock(&pq->pq_slock);
			continue;
		}
d3358 2
a3359 174

		pj = pmap_tlb_shootdown_job_get(pq);
		pq->pq_pte |= pte;
		if (pj == NULL) {
			/*
			 * Couldn't allocate a job entry.
			 * Kill it now for this cpu, unless the failure
			 * was due to too many pending flushes; otherwise,
			 * tell other cpus to kill everything..
			 */
			if (ci == self && pq->pq_count < PMAP_TLB_MAXJOBS) {
				pmap_update_pg(va);
				simple_unlock(&pq->pq_slock);
				continue;
			} else {
				if (pq->pq_pte & pmap_pg_g)
					pq->pq_flushg++;
				else
					pq->pq_flushu++;
				/*
				 * Since we've nailed the whole thing,
				 * drain the job entries pending for that
				 * processor.
				 */
				pmap_tlb_shootdown_q_drain(pq);
				*cpumaskp |= 1U << ci->ci_cpuid;
			}
		} else {
			pj->pj_pmap = pmap;
			pj->pj_va = va;
			pj->pj_pte = pte;
			TAILQ_INSERT_TAIL(&pq->pq_head, pj, pj_list);
			*cpumaskp |= 1U << ci->ci_cpuid;
		}
		simple_unlock(&pq->pq_slock);
	}
	splx(s);
}

/*
 * pmap_do_tlb_shootdown:
 *
 *	Process pending TLB shootdown operations for this processor.
 */
void
#ifdef notyet
pmap_do_tlb_shootdown(struct cpu_info *self)
#else
pmap_do_tlb_shootdown(void)
#endif
{
	u_long cpu_id = cpu_number();
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj;
	int s;
#ifdef MULTIPROCESSOR
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
#endif

	s = splipi();

	simple_lock(&pq->pq_slock);

	if (pq->pq_flushg) {
		tlbflushg();
		pq->pq_flushg = 0;
		pq->pq_flushu = 0;
		pmap_tlb_shootdown_q_drain(pq);
	} else {
		/*
		 * TLB flushes for PTEs with PG_G set may be in the queue
		 * after a flushu, they need to be dealt with.
		 */
		if (pq->pq_flushu) {
			tlbflush();
		}
		while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
			TAILQ_REMOVE(&pq->pq_head, pj, pj_list);

			if ((!pq->pq_flushu && pmap_is_curpmap(pj->pj_pmap)) ||
			    (pj->pj_pte & pmap_pg_g))
				pmap_update_pg(pj->pj_va);

			pmap_tlb_shootdown_job_put(pq, pj);
		}

		pq->pq_flushu = pq->pq_pte = 0;
	}

#ifdef MULTIPROCESSOR
	for (CPU_INFO_FOREACH(cii, ci))
		i386_atomic_clearbits_l(&ci->ci_tlb_ipi_mask,
		    (1U << cpu_id));
#endif
	simple_unlock(&pq->pq_slock);

	splx(s);
}

/*
 * pmap_tlb_shootdown_q_drain:
 *
 *	Drain a processor's TLB shootdown queue.  We do not perform
 *	the shootdown operations.  This is merely a convenience
 *	function.
 *
 *	Note: We expect the queue to be locked.
 */
void
pmap_tlb_shootdown_q_drain(pq)
	struct pmap_tlb_shootdown_q *pq;
{
	struct pmap_tlb_shootdown_job *pj;

	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		pmap_tlb_shootdown_job_put(pq, pj);
	}
	pq->pq_pte = 0;
}

/*
 * pmap_tlb_shootdown_job_get:
 *
 *	Get a TLB shootdown job queue entry.  This places a limit on
 *	the number of outstanding jobs a processor may have.
 *
 *	Note: We expect the queue to be locked.
 */
struct pmap_tlb_shootdown_job *
pmap_tlb_shootdown_job_get(pq)
	struct pmap_tlb_shootdown_q *pq;
{
	struct pmap_tlb_shootdown_job *pj;

	if (pq->pq_count >= PMAP_TLB_MAXJOBS)
		return (NULL);

	simple_lock(&pmap_tlb_shootdown_job_lock);
	if (pj_free == NULL) {
		simple_unlock(&pmap_tlb_shootdown_job_lock);
		return NULL;
	}
	pj = pj_free;
	pj_free = pj_free->pj_nextfree;
	simple_unlock(&pmap_tlb_shootdown_job_lock);

	pq->pq_count++;
	return (pj);
}

/*
 * pmap_tlb_shootdown_job_put:
 *
 *	Put a TLB shootdown job queue entry onto the free list.
 *
 *	Note: We expect the queue to be locked.
 */
void
pmap_tlb_shootdown_job_put(pq, pj)
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
{
#ifdef DIAGNOSTIC
	if (pq->pq_count == 0)
		panic("pmap_tlb_shootdown_job_put: queue length inconsistency");
#endif
	simple_lock(&pmap_tlb_shootdown_job_lock);
	pj->pj_nextfree = pj_free;
	pj_free = pj;
	simple_unlock(&pmap_tlb_shootdown_job_lock);

	pq->pq_count--;
@


1.34.2.15
log
@Sync the SMP branch to -current, plus some ELF-related fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.14 2003/04/15 03:53:47 niklas Exp $	*/
d649 1
a649 1
	    va < VM_MAXUSER_ADDRESS && va >= I386_MAX_EXE_ADDR) {
d1260 1
a1260 2
	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg),
	    VM_PROT_READ|VM_PROT_WRITE);
@


1.34.2.16
log
@merge the trunk so we will get the genfs and locking fixes
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d642 1
a642 1
pmap_exec_account(struct pmap *pm, vaddr_t va,
d646 1
d648 2
a649 18
	if (curproc == NULL || curproc->p_vmspace == NULL ||
	    pm != vm_map_pmap(&curproc->p_vmspace->vm_map))
		return;

	if ((opte ^ npte) & PG_X) {
		pmap_tlb_shootdown(pmap, va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
	}
			
	/*
	 * Executability was removed on the last executable change.
	 * Reset the code segment to something conservative and
	 * let the trap handler deal with setting the right limit.
	 * We can't do that because of locking constraints on the vm map.
	 *
	 * XXX - floating cs - set this _really_ low.
	 */
	if ((opte & PG_X) && (npte & PG_X) == 0 && va == pm->pm_hiexec) {
d651 1
a651 1
		struct pcb *pcb = &curproc->p_addr->u_pcb;
d653 17
a669 36
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
		pm->pm_hiexec = I386_MAX_EXE_ADDR;
	}
}

/*
 * Fixup the code segment to cover all potential executable mappings.
 * returns 0 if no changes to the code segment were made.
 */
int
pmap_exec_fixup(struct vm_map *map, struct trapframe *tf, struct pcb *pcb)
{
	struct vm_map_entry *ent;
	struct pmap *pm = vm_map_pmap(map);
	vaddr_t va = 0;

	vm_map_lock(map);
	for (ent = (&map->header)->next; ent != &map->header; ent = ent->next) {
		/*
		 * This entry has greater va than the entries before.
		 * We need to make it point to the last page, not past it.
		 */
		if (ent->protection & VM_PROT_EXECUTE)
			va = trunc_page(ent->end) - PAGE_SIZE;
	}
	vm_map_unlock(map);

	if (va == pm->pm_hiexec)
		return (0);

	pm->pm_hiexec = va;

	if (pm->pm_hiexec > (vaddr_t)I386_MAX_EXE_ADDR) {
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE1_SEL, SEL_UPL);
	} else {
		pcb->pcb_cs = tf->tf_cs = GSEL(GUCODE_SEL, SEL_UPL);
a670 2
	
	return (1);
d1861 1
a1861 1
	pmap->pm_hiexec = 0;
d2390 1
a2390 1
	pmap_exec_account(pmap, va, opte, 0);
d2947 2
a2948 1
				pmap_exec_account(pmap, sva, *spte, npte);
d3204 1
a3204 1
	pmap_exec_account(pmap, va, opte, npte);
@


1.34.2.17
log
@merge typo
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.16 2003/05/16 00:29:39 niklas Exp $	*/
d652 1
a652 1
		pmap_tlb_shootdown(pm, va, opte, &cpumask);
@


1.34.2.18
log
@Go back to defining simplelocks as noops, even if MULTIPROCESSOR.  Instead use
a new real simple recursive-lock capable lock implementation for the few
necessary locks (kernel, scheduler, tlb shootdown, printf and ddb MP).
This because we cannot trust the old fine-grained locks spread out all over
our kernel, and not really tested.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.17 2003/05/16 01:03:19 niklas Exp $	*/
d236 1
a236 2
#if defined(MULTIPROCESSOR) && 0

d286 1
a286 1
	struct SIMPLELOCK pq_slock;	/* spin lock on queue */
d981 1
a981 1
#if defined(MULTIPROCESSOR) && 0
d1005 1
a1005 1
		SIMPLE_LOCK_INIT(&pmap_tlb_shootdown_q[i].pq_slock);
d2491 1
a2491 1
 	PMAP_MAP_TO_HEAD_LOCK();
d3509 1
a3509 1
		SIMPLE_LOCK(&pq->pq_slock);
d3518 1
a3518 1
			SIMPLE_UNLOCK(&pq->pq_slock);
d3535 1
a3535 1
			SIMPLE_UNLOCK(&pq->pq_slock);
d3551 1
a3551 1
				SIMPLE_UNLOCK(&pq->pq_slock);
d3573 1
a3573 1
		SIMPLE_UNLOCK(&pq->pq_slock);
d3601 1
a3601 1
	SIMPLE_LOCK(&pq->pq_slock);
d3634 1
a3634 1
	SIMPLE_UNLOCK(&pq->pq_slock);
@


1.34.2.19
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.18 2003/05/18 17:41:16 niklas Exp $	*/
d3118 1
a3118 2
				error = KERN_RESOURCE_SHORTAGE;
				goto out;
@


1.34.2.20
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2017 3
		 *
		 * No need to lock the pmap for ldt_free (or anything else),
		 * we're the last one to use it.
a2202 1
void (*pagezero)(void *, size_t) = bzero;
d2230 1
a2230 1
	pagezero(zerova, PAGE_SIZE);		/* zero */
d2232 4
d2260 1
a2260 1
	pagezero(zerova, PAGE_SIZE);				/* zero */
d2262 4
@


1.34.2.21
log
@Import NetBSD updates to NPX logic, and IPI API
@
text
@d3437 1
d3439 3
d3582 1
d3584 3
@


1.34.2.22
log
@Missed TLB shootdowns, and the locks around them were noops
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.21 2004/02/20 22:19:55 niklas Exp $	*/
d300 1
a300 1
struct SIMPLELOCK pmap_tlb_shootdown_job_lock;
d449 2
a450 1
struct vm_page	*pmap_steal_ptp(struct uvm_object *, vaddr_t);
a579 35
__inline static void
pmap_apte_flush(struct pmap *pmap)
{
#if defined(MULTIPROCESSOR)
	struct pmap_tlb_shootdown_q *pq;
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	int s;
#endif

	tlbflush();		/* flush TLB on current processor */
#if defined(MULTIPROCESSOR)
	/*
	 * Flush the APTE mapping from all other CPUs that
	 * are using the pmap we are using (who's APTE space
	 * is the one we've just modified).
	 *
	 * XXXthorpej -- find a way to defer the IPI.
	 */
	for (CPU_INFO_FOREACH(cii, ci)) {
		if (ci == self)
			continue;
		if (pmap_is_active(pmap, ci->ci_cpuid)) {
			pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];
			s = splipi();
			SIMPLE_LOCK(&pq->pq_slock);
			pq->pq_flushu++;
			SIMPLE_UNLOCK(&pq->pq_slock);
			splx(s);
			i386_send_ipi(ci, I386_IPI_TLB);
		}
	}
#endif
}

d618 1
a618 1
			pmap_apte_flush(pmap);
a636 4
#if defined(MULTIPROCESSOR)
		*APDP_PDE = 0;
		pmap_apte_flush(curpcb->pcb_pmap);
#endif
d1002 1
a1002 1
	SIMPLE_LOCK_INIT(&pmap_tlb_shootdown_job_lock);
d1786 5
a1790 1
					pmap_apte_flush(pmaps_hand);
d3668 1
a3668 1
	SIMPLE_LOCK(&pmap_tlb_shootdown_job_lock);
d3670 1
a3670 1
		SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
d3675 1
a3675 1
	SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
d3697 1
a3697 1
	SIMPLE_LOCK(&pmap_tlb_shootdown_job_lock);
d3700 1
a3700 1
	SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
@


1.34.2.23
log
@tlb shootdown fixes and optimizations, mostly from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.22 2004/03/30 09:08:38 niklas Exp $	*/
d684 2
a690 3
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;

a692 4
#else
		/* Don't bother deferring in the single CPU case. */
		pmap_update_pg(va);
#endif
d794 2
a803 1
#ifdef MULTIPROCESSOR
a804 1
#endif
d807 2
a808 5
	for ( /* null */ ; len ; len--, va += PAGE_SIZE) {
		if (va < VM_MIN_KERNEL_ADDRESS)
			pte = vtopte(va);
		else
			pte = kvtopte(va);
d814 1
a814 6
		if ((opte & (PG_V | PG_U)) == (PG_V | PG_U))
#ifdef MULTIPROCESSOR
			pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
#else
			pmap_update_pg(va);
#endif
a815 1
#ifdef MULTIPROCESSOR
a816 1
#endif
a1822 5
#ifdef MULTIPROCESSOR
/* XXX */
printf("pmap_steal_ptp got something\n");
				pmap_apte_flush(pmaps_hand);
#else
d1826 6
a1831 2
				    (*APDP_PDE & PG_FRAME) ==
				    pmaps_hand->pm_pdirpa)
d1834 1
a1834 1
#endif
d2383 1
a2383 2
		if (opte & PG_U)
			pmap_tlb_shootdown(pmap, startva, opte, cpumaskp);
d2385 1
a2385 1
		if (ptp) {
a2386 5
			/* Make sure that the PDE is flushed */
			if ((ptp->wire_count <= 1) && !(opte & PG_U))
				pmap_tlb_shootdown(pmap, startva, opte,
				    cpumaskp);
		}
d2463 1
a2463 4
	if (opte & PG_U)
		pmap_tlb_shootdown(pmap, va, opte, cpumaskp);

	if (ptp) {
a2464 3
		/* Make sure that the PDE is flushed */
		if ((ptp->wire_count <= 1) && !(opte & PG_U))
			pmap_tlb_shootdown(pmap, va, opte, cpumaskp);
d2466 1
a2466 1
	}
d2756 1
a2756 4
		/* Shootdown only if referenced */
		if (opte & PG_U)
			pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va, opte,
			    &cpumask);
a2764 8
				/*
				 * Do we have to shootdown the page just to
				 * get the pte out of the TLB ?
				 */
				if(!(opte & PG_U))
					pmap_tlb_shootdown(pve->pv_pmap,
					    pve->pv_va, opte, &cpumask);

@


1.34.2.24
log
@remove debug printout
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.23 2004/04/05 10:15:56 niklas Exp $	*/
d1839 2
@


1.34.2.25
log
@cpumask is needed only for MULTIPROCESSOR.

ok niklas@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.24 2004/04/06 13:36:43 niklas Exp $	*/
a1409 1
#ifdef MULTIPROCESSOR
a1410 1
#endif
@


1.34.2.26
log
@Don't inline here, it eats install media space.

ok niklas@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34.2.25 2004/06/06 18:21:15 grange Exp $	*/
d426 2
a427 2
struct vm_page	*pmap_alloc_ptp(struct pmap *, int, boolean_t);
struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
d431 2
a432 2
struct pv_entry	*pmap_alloc_pvpage(struct pmap *, int);
void		 pmap_enter_pv(struct pv_head *,
d435 3
a437 3
void		 pmap_free_pv(struct pmap *, struct pv_entry *);
void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
void		 pmap_free_pv_doit(struct pv_entry *);
d440 4
a443 4
boolean_t	 pmap_is_curpmap(struct pmap *);
boolean_t	 pmap_is_active(struct pmap *, int);
pt_entry_t	*pmap_map_ptes(struct pmap *);
struct pv_entry	*pmap_remove_pv(struct pv_head *, struct pmap *,
d450 4
a453 5
vaddr_t		 pmap_tmpmap_pa(paddr_t);
pt_entry_t	*pmap_tmpmap_pvepte(struct pv_entry *);
void		 pmap_tmpunmap_pa(void);
void		 pmap_tmpunmap_pvepte(struct pv_entry *);
void		 pmap_apte_flush(struct pmap *);
d457 1
a457 3
void		pmap_unmap_ptes(struct pmap *);
void		pmap_exec_account(struct pmap *, vaddr_t, pt_entry_t,
		    pt_entry_t);
d473 1
a473 1
boolean_t
d485 1
a485 1
boolean_t
d499 1
a499 1
vaddr_t
d520 1
a520 1
void
d547 1
a547 1
pt_entry_t *
d568 1
a568 1
void
d579 1
a579 1
void
d621 1
a621 1
pt_entry_t *
d661 1
a661 1
void
d680 1
a680 1
void
d1214 1
a1214 1
struct pv_entry *
d1509 1
a1509 1
void
d1544 1
a1544 1
void
d1569 1
a1569 1
void
d1668 1
a1668 1
void
d1695 1
a1695 1
struct pv_entry *
d1734 1
a1734 1
struct vm_page *
@


1.33
log
@Put the "faulting in a pt page map ..." printf behind #ifdef DEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 1999/09/03 18:00:51 art Exp $	*/
a178 2
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
a180 1
extern vm_offset_t hole_start, hole_end;
a229 3
#if defined(UVM)
	int first16q;
#endif
a237 4

	/* XXX: allow for msgbuf */
	avail_end -= i386_round_page(sizeof(struct msgbuf));

d300 2
a301 1
		p = vm_bootstrap_steal_memory((MAXKPDE-NKPDE+1) * NBPG);
a308 33

	/*                       
	 * we must call vm_page_physload() after we are done playing
	 * with virtual_avail but before we call pmap_steal_memory.
	 * [i.e. here]
	 */                               
#if defined(UVM)
	if (avail_end < (16 * 1024 * 1024))
		first16q = VM_FREELIST_DEFAULT;
	else
		first16q = VM_FREELIST_FIRST16;

	if (avail_start < hole_start)
		uvm_page_physload(atop(avail_start), atop(hole_start),
			atop(avail_start), atop(hole_start), first16q);
	if (first16q == VM_FREELIST_FIRST16) {
		uvm_page_physload(atop(hole_end), atop(16 * 1024 * 1024),
		    atop(hole_end), atop(16 * 1024 * 1024), first16q);
		uvm_page_physload(atop(16 * 1024 * 1024), atop(avail_end),
		    atop(16 * 1024 * 1024), atop(avail_end),
		    VM_FREELIST_DEFAULT);
	} else {
		uvm_page_physload(atop(hole_end), atop(avail_end),
		    atop(hole_end), atop(avail_end), first16q);
	}
#else
	if (avail_start < hole_start)
		vm_page_physload(atop(avail_start), atop(hole_start),
				 atop(avail_start), atop(hole_start));      
	vm_page_physload(atop(hole_end), atop(avail_end),
			 atop(hole_end), atop(avail_end));
#endif
	pmap_update();
@


1.32
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 1999/08/25 09:13:53 ho Exp $	*/
d1225 1
d1227 1
@


1.31
log
@Compile under UVM and versions of egcs. art@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 1999/07/02 12:25:50 niklas Exp $	*/
d661 1
a661 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE);
d1152 1
a1152 1
pmap_enter(pmap, va, pa, prot, wired)
d1158 1
@


1.30
log
@Kludge to not get locked threads with UVM, XXX
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 1999/06/04 16:37:48 mickey Exp $	*/
d304 1
d317 1
@


1.29
log
@remove old MN code, which is not in use anymore, MNN been running for
a year already, and upcoming new apm stuff is not compatible w/ the old MN.
niklas@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 1999/06/01 06:37:22 art Exp $	*/
d1231 17
a1247 1
		uvm_map_pageable(vmap, v, round_page(v+1), FALSE);
@


1.28
log
@&vm->vm_pmap -> vm->vm_map.pmap
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 1999/02/26 10:37:51 art Exp $	*/
a186 4
#if !defined(MACHINE_NEW_NONCONTIG)
char		*pmap_attributes;	/* reference and modify bits */
struct pv_entry	*pv_table;		/* array of entries, one per page */
#endif
a316 1
#if defined(MACHINE_NEW_NONCONTIG)
a346 1
#endif          
a359 1
#if defined(MACHINE_NEW_NONCONTIG)
a413 37
#else /* MACHINE_NEW_NONCONTIG */
/*
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
 */
void
pmap_init()
{
	vm_offset_t addr;
	vm_size_t s;

	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE != 1");

	npages = pmap_page_index(avail_end - 1) + 1;
	s = (vm_size_t) (sizeof(struct pv_entry) * npages + npages);
	s = round_page(s);
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
	pv_table = (struct pv_entry *) addr;
	addr += sizeof(struct pv_entry) * npages;
	pmap_attributes = (char *) addr;
	TAILQ_INIT(&pv_page_freelist);

#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %x bytes (%x pgs): tbl %x attr %x\n",
		       s, npages, pv_table, pmap_attributes);
#endif

	/*
	 * Now it is safe to enable pv_table recording.
	 */
	pmap_initialized = TRUE;
}
#endif /* MACHINE_NEW_NONCONTIG */

a486 1
#if defined(MACHINE_NEW_NONCONTIG)
a487 1
#endif
a505 1
#if defined(MACHINE_NEW_NONCONTIG)
a511 3
#else
	for (ph = &pv_table[npages - 1]; ph >= &pv_table[0]; ph--) {
#endif
a851 1
#if defined(MACHINE_NEW_NONCONTIG)
a852 3
#else
	u_int pind;
#endif
a936 1
#if defined(MACHINE_NEW_NONCONTIG)
a942 7
#else

		if ((pind = pmap_page_index(pa)) != -1) {
			pmap_attributes[pind] |= *pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, sva, &pv_table[pind]);
		}
#endif /* MACHINE_NEW_NONCONTIG */
a970 1
#if defined(MACHINE_NEW_NONCONTIG)
a971 3
#else
	u_int pind;
#endif
a979 1
#if defined(MACHINE_NEW_NONCONTIG)
a984 6
#else
	if ((pind = pmap_page_index(pa)) == -1)
		return;

	pv = ph = &pv_table[pind];
#endif
a1024 1
#if defined(MACHINE_NEW_NONCONTIG)
a1025 3
#else
		pmap_attributes[pind] |= *pte & (PG_M | PG_U);
#endif
a1158 1
#if defined(MACHINE_NEW_NONCONTIG)
a1159 3
#else
	u_int pind;
#endif
a1290 1
#if defined(MACHINE_NEW_NONCONTIG)
a1296 6
#else
		if ((pind = pmap_page_index(opa)) != -1) {
			pmap_attributes[pind] |= *pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, va, &pv_table[pind]);
		}
#endif
a1311 1
#if defined(MACHINE_NEW_NONCONTIG)
d1318 1
a1318 11
	}        
#else
	if ((pind = pmap_page_index(pa)) != -1) {
#ifdef DEBUG
		enter_stats.managed++;
#endif
		pmap_enter_pv(pmap, va, &pv_table[pind]);
		cacheable = TRUE;
	}
#endif
	else if (pmap_initialized) {
a1551 1
#if defined(MACHINE_NEW_NONCONTIG)
a1552 1
#endif
a1559 1
#if defined(MACHINE_NEW_NONCONTIG)
a1561 3
#else
	pv = &pv_table[pmap_page_index(phys)];
#endif
a1662 1
#if defined(MACHINE_NEW_NONCONTIG)
a1663 3
#else
		u_int pind;
#endif
a1684 1
#if defined(MACHINE_NEW_NONCONTIG)
a1687 8
#else
		if ((pind = pmap_page_index(pa)) == -1) {
			printf("pmap_pageable: invalid pa %x va %x\n",
				pa, sva);
			return;
		}
		pv = &pv_table[pind];
#endif
a1734 1
#if defined(MACHINE_NEW_NONCONTIG)
a1739 7
#else
	u_int pind;

	if ((pind = pmap_page_index(pa)) == -1)
		return FALSE;
	pv = &pv_table[pind];
#endif
a1744 1
#if defined(MACHINE_NEW_NONCONTIG)
a1745 3
#else
	if (pmap_attributes[pind] & setbits) {
#endif
a1780 1
#if defined(MACHINE_NEW_NONCONTIG)
a1781 3
#else
	u_int pind;
#endif
a1788 1
#if defined(MACHINE_NEW_NONCONTIG)
a1791 5
#else
	if ((pind = pmap_page_index(pa)) == -1)
		return;
	pv = &pv_table[pind];
#endif
a1797 1
#if defined(MACHINE_NEW_NONCONTIG)
a1798 3
#else
		pmap_attributes[pind] &= maskbits;
#endif
a1858 1
#if defined(MACHINE_NEW_NONCONTIG)
a1859 1
#endif
a1861 1
#if defined(MACHINE_NEW_NONCONTIG)
a1870 6
#else
	for (pv = &pv_table[pmap_page_index(pa)]; pv; pv = pv->pv_next) {
		printf(" -> pmap %x, va %x", pv->pv_pmap, pv->pv_va);
		pads(pv->pv_pmap);
	}
#endif
@


1.27
log
@deal with uvm. Mostly name changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 1998/04/25 20:31:30 mickey Exp $	*/
d1296 1
a1296 1
		    pmap != &curproc->p_vmspace->vm_pmap)
@


1.26
log
@change pmap_{de,}activate to take a struct proc *.
XXX - This should be done to other archs, but since nothing (except uvm)
      uses it right now, the interface will be changed there when
      support for uvm is added.
@
text
@d92 4
d237 11
a307 3
	/* Register the page size with the vm system */
	vm_set_page_size();

d328 5
d335 8
a342 1
			atop(avail_start), atop(hole_start));
d344 2
a345 1
			atop(hole_end), atop(avail_end));
d350 2
a351 2
		vm_page_physload(atop(hole_end), atop(avail_end),
				 atop(hole_end), atop(avail_end));
d466 4
d471 1
d515 3
d519 1
d589 3
d593 1
d772 3
d776 1
d778 4
d843 3
d847 1
d1304 3
d1308 1
d1311 3
d1315 1
d1957 4
d1965 1
d1987 3
d1991 1
@


1.25
log
@convert i386 to MNN
@
text
@a196 1
void pmap_deactivate __P((pmap_t, struct pcb *));
d822 2
a823 3
pmap_activate(pmap, pcb)
	pmap_t pmap;
	struct pcb *pcb;
d825 2
d828 3
a830 7
	if (pmap /*&& pmap->pm_pdchanged */) {
		pcb->pcb_cr3 =
		    pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_pdir);
		if (pmap == &curproc->p_vmspace->vm_pmap)
			lcr3(pcb->pcb_cr3);
		pmap->pm_pdchanged = FALSE;
	}
d834 2
a835 3
pmap_deactivate(pmap, pcb)
	pmap_t pmap;
	struct pcb *pcb;
@


1.24
log
@Some cleanup of page steals
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 1998/01/20 18:40:15 niklas Exp $	*/
d179 1
d183 1
d185 2
d195 2
a196 2
__inline void pmap_remove_pv __P((pmap_t, vm_offset_t, u_int));
__inline void pmap_enter_pv __P((pmap_t, vm_offset_t, u_int));
d310 20
d342 56
d432 1
d498 3
d519 8
d528 1
d561 1
a561 1
pmap_enter_pv(pmap, va, pind)
d564 1
a564 1
	u_int pind;
d566 1
a566 1
	register struct pv_entry *pv, *npv;
a576 2

	pv = &pv_table[pind];
d613 1
a613 1
pmap_remove_pv(pmap, va, pind)
d616 1
a616 1
	u_int pind;
d618 1
a618 1
	register struct pv_entry *pv, *npv;
a624 1
	pv = &pv_table[pind];
d857 3
d861 1
d946 9
d957 1
a957 1
			pmap_remove_pv(pmap, sva, pind);
d959 1
d988 3
d992 1
d1001 7
d1012 1
d1053 3
d1057 1
a1057 1

d1191 3
d1195 1
d1319 8
d1329 1
a1329 1
			pmap_remove_pv(pmap, va, pind);
d1331 1
d1347 9
d1360 1
a1360 1
		pmap_enter_pv(pmap, va, pind);
d1362 3
a1364 1
	} else if (pmap_initialized) {
d1598 3
d1608 4
d1613 1
d1715 3
d1719 1
d1741 5
a1745 1

a1750 1

d1752 1
d1799 8
a1807 1
	int s;
a1810 1

d1812 1
d1818 3
d1822 1
d1857 4
d1862 1
a1862 1
	int s;
d1870 5
a1876 1

d1878 1
d1885 3
d1889 1
d1941 3
d1946 11
d1961 1
@


1.23
log
@Merge bus_dma support from NetBSD, mostly by Jason Thorpe.  Only i386 uses it
 so far, the other archs gets placeholders for now.  I wrote a compatibility
layer for OpenBSD's old isadma code so we can still use our old
driver sources.  They will however get changed to native bus_dma use,
on a case by case basis.   Oh yes, I almost forgot, I kept our notion
of isadma being a device so DMA-less ISA-busses still work
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 1997/10/25 06:57:59 niklas Exp $	*/
d290 3
d298 1
a298 1
		p = pmap_steal_memory((MAXKPDE-NKPDE+1) * NBPG);
a313 1

@


1.22
log
@Boot arguments are now at physmem 0x100
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 1997/10/24 22:15:07 mickey Exp $	*/
a288 14

	/*
	 * reserve special hunk of memory for use by bus dma as a bounce
	 * buffer (contiguous virtual *and* physical memory).  XXX
	 */
#if NISA > 0 && NISADMA > 0
	if (ctob(physmem) >= 0x1000000) {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE * NBPG);
		isaphysmempgs = DMA_BOUNCE;
	} else {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE_LOW * NBPG);
		isaphysmempgs = DMA_BOUNCE_LOW;
	}
#endif
@


1.21
log
@map a piece of memory after the msgbuf and copy bootargv there.
pass cksumlen argument, sysctl it
mostly by niklas
me just did slite editing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 1997/09/24 22:28:15 niklas Exp $	*/
d231 2
a232 3
	/* XXX: allow for msgbuf and bootargv */
	avail_end -= i386_round_page(sizeof(struct msgbuf)) +
	    2 * i386_round_page(bootargc);
d279 3
a281 2
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,1		)
	SYSMAP(bootarg_t *	,bootargmap	,bootargp  ,2		)
@


1.20
log
@Revert, as we won't have enough time to test this fully before release.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 1997/08/17 17:31:37 grr Exp $	*/
d95 1
d207 1
a207 1
pt_entry_t	*msgbufmap;
d231 3
a233 2
	/* XXX: allow for msgbuf */
	avail_end -= i386_round_page(sizeof(struct msgbuf));
d281 1
@


1.19
log
@Use vm_page_alloc_memory API. Some cleanup.
@
text
@d287 14
@


1.18
log
@Back out Mickey's 8/1 pmap.c change, which was misguided and caused
stability problems with swapped/paged out processes getting segementation
vioations when reactivated.

Also add some additional paranoia about whether an allocation being changed
to pageable is actually a page-table and move some sanity checking from
#ifdef DEBUG over to #ifdef DIAGSNOTIC.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 1997/07/28 23:46:10 mickey Exp $	*/
a285 14

	/*
	 * reserve special hunk of memory for use by bus dma as a bounce
	 * buffer (contiguous virtual *and* physical memory).  XXX
	 */
#if NISA > 0 && NISADMA > 0
	if (ctob(physmem) >= 0x1000000) {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE * NBPG);
		isaphysmempgs = DMA_BOUNCE;
	} else {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE_LOW * NBPG);
		isaphysmempgs = DMA_BOUNCE_LOW;
	}
#endif
@


1.17
log
@do multipage pmap_pageable
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 1997/01/07 05:37:32 tholo Exp $	*/
d1541 1
d1561 3
a1564 2
	if (pmap != pmap_kernel() || !pageable)
		return;
d1566 4
a1569 1
	for ( ; sva < eva; sva += NBPG) {
d1572 1
a1572 2

#ifdef DEBUG
d1575 1
d1577 1
d1591 8
a1598 2
#ifdef DEBUG
		if ((pind = pmap_page_index(pa)) == -1)
d1600 1
@


1.16
log
@Fix for final ptdi panic on i386
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 1996/10/25 11:14:13 deraadt Exp $	*/
d1558 1
a1558 2
	 * Assumptions:
	 *	- we are called with only one page at a time
d1561 4
a1564 1
	if (pmap == pmap_kernel() && pageable && sva + NBPG == eva) {
@


1.15
log
@grow kvm space; fix an over-agressive pmap optimization
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1996/09/26 14:04:27 deraadt Exp $	*/
d1727 17
@


1.14
log
@-Wall happiness (not happieness, pointed by theo ;)
@
text
@d229 1
d301 13
d657 2
a658 1
	bcopy(&PTD[KPTDI], &pmap->pm_pdir[KPTDI], NKPDE * sizeof(pd_entry_t));
d661 2
a662 2
	pmap->pm_pdir[PTDPTDI] =
	    pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_pdir) | PG_V | PG_KW;
d804 7
a1131 1

d1137 2
a1138 2
		  	pmap != &curproc->p_vmspace->vm_pmap)
				panic("ptdi %x", pmap->pm_pdir[PTDPTDI]);
d1144 1
@


1.13
log
@fault in a ptp in one nasty situation. not fool proof, but helps some cases. thanks to dyson and chuck
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1996/06/16 10:24:19 deraadt Exp $	*/
d194 3
d1708 1
d1723 1
d1751 1
@


1.12
log
@print better ptdi panic diagnostics
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1996/06/02 10:44:21 mickey Exp $	*/
a1079 3
	/*
	 * Page Directory table entry not valid, we need a new PT page
	 */
d1082 45
a1126 6
		printf("ptdi panic!\n");
		printf("pte %p pmap %p va %p\n", pte, pmap, va);
		printf("pmap_pde(pmap, va) %p\n", pmap_pde(pmap, va));
		printf("pmap_pde_v(pmap_pde(pmap, va) %p\n",
		    pmap_pde_v(pmap_pde(pmap, va)));
		panic("ptdi %x", pmap->pm_pdir[PTDPTDI]);
@


1.11
log
@Fix back my fixes lost in last sync.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1996/05/30 09:30:08 deraadt Exp $	*/
d1084 6
a1089 1
	if (!pte)
d1091 1
a1091 1

@


1.10
log
@clean & sync
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1996/05/07 07:21:51 deraadt Exp $	*/
d1382 2
a1383 1
	printf("pmap_collect(%x) ", pmap);
d1391 1
a1391 1
#if 0
d1398 3
@


1.9
log
@sync with 0504; prototype changes
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1996/04/21 22:16:33 deraadt Exp $	*/
a184 1
void	i386_protection_init __P((void));
d1382 1
a1382 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%x) ", pmap);
@


1.8
log
@partial sync with netbsd 960418, more to come
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.7 1996/04/18 04:04:54 mickey Exp $	*/
/*	$NetBSD: pmap.c,v 1.35 1996/04/03 08:21:05 mycroft Exp $	*/
d83 1
d187 2
d190 5
a226 3
	extern int physmem;
	extern vm_offset_t reserve_dumppages(vm_offset_t);

d319 1
a319 1
	vm_offset_t addr, addr2;
a320 1
	int rv;
a384 1
	register int i;
d844 1
d846 1
d1024 1
d1026 1
d1099 2
a1100 1
		if (wired && !pmap_pte_w(pte) || !wired && pmap_pte_w(pte)) {
d1266 1
a1266 1
	if (wired && !pmap_pte_w(pte) || !wired && pmap_pte_w(pte)) {
a1381 6
	register vm_offset_t pa;
	register struct pv_entry *pv;
	register pt_entry_t *pte;
	vm_offset_t kpa;
	int s;

@


1.7
log
@Fix prototyping, so it's compiling again.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.6 1996/04/17 05:18:55 mickey Exp $	*/
/*	$NetBSD: pmap.c,v 1.34 1995/12/09 07:39:02 mycroft Exp $	*/
d186 1
@


1.6
log
@Cleanups & fixes from latest NetBSD primarily to run doscmd, etc.
GENERIC added to the compile/.cvsignore (it is used for 'make links'
for example), thus conf/GENERIC should appear magically ...
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d184 1
@


1.5
log
@Merging w/ NetBSD 021796.
speaker upgraded to the current.
some changes to the VM stuff (ie kern_thread.c added and so).
@
text
@d1 1
d1541 1
a1541 1

@


1.4
log
@elliminating unnecessary printf w/ DEBUG in pmap_collect.
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.33 1995/06/26 05:21:58 cgd Exp $	*/
@


1.3
log
@i386 isa bounce buffers by hannken@@eis.cs.tu-bs.de
@
text
@d1378 2
a1379 1
	printf("pmap_collect(%x) ", pmap);
@


1.2
log
@wrap isaphysmem in NISADMA; from andrew@@wipux2.wifo.uni-mannheim.de; netbsd pr#1735
@
text
@d96 1
d280 8
a287 2
#if NISADMA > 0
	isaphysmem = pmap_steal_memory(DMA_BOUNCE * NBPG);
@


1.1
log
@Initial revision
@
text
@d279 1
a279 1
#if NISA > 0
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

