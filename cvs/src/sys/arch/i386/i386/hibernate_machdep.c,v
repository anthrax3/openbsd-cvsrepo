head	1.49;
access;
symbols
	OPENBSD_6_2_BASE:1.49
	OPENBSD_6_1:1.49.0.6
	OPENBSD_6_1_BASE:1.49
	OPENBSD_6_0:1.49.0.2
	OPENBSD_6_0_BASE:1.49
	OPENBSD_5_9:1.47.0.2
	OPENBSD_5_9_BASE:1.47
	OPENBSD_5_8:1.46.0.4
	OPENBSD_5_8_BASE:1.46
	OPENBSD_5_7:1.44.0.2
	OPENBSD_5_7_BASE:1.44
	OPENBSD_5_6:1.38.0.4
	OPENBSD_5_6_BASE:1.38
	OPENBSD_5_5:1.29.0.4
	OPENBSD_5_5_BASE:1.29
	OPENBSD_5_4:1.25.0.2
	OPENBSD_5_4_BASE:1.25
	OPENBSD_5_3:1.19.0.2
	OPENBSD_5_3_BASE:1.19
	OPENBSD_5_2:1.18.0.2
	OPENBSD_5_2_BASE:1.18
	OPENBSD_5_1_BASE:1.17
	OPENBSD_5_1:1.17.0.2
	OPENBSD_5_0:1.11.0.2
	OPENBSD_5_0_BASE:1.11;
locks; strict;
comment	@ * @;


1.49
date	2016.05.20.02.30.41;	author mlarkin;	state Exp;
branches;
next	1.48;
commitid	F5BmBZzZKQUK5NHq;

1.48
date	2016.05.18.03.45.11;	author mlarkin;	state Exp;
branches;
next	1.47;
commitid	JQz4IJ6cQ8DinMqc;

1.47
date	2015.08.21.07.01.38;	author mlarkin;	state Exp;
branches;
next	1.46;
commitid	q2hUARV0iqgQ4kaw;

1.46
date	2015.04.26.11.09.32;	author kettenis;	state Exp;
branches;
next	1.45;
commitid	o9YdhMEc9lBSn0pk;

1.45
date	2015.04.12.18.37.53;	author mlarkin;	state Exp;
branches;
next	1.44;
commitid	5ST94uMTezmXYdhY;

1.44
date	2015.01.09.03.43.52;	author mlarkin;	state Exp;
branches;
next	1.43;
commitid	TzpVlzYKb3Vx3GSZ;

1.43
date	2014.12.23.01.24.50;	author deraadt;	state Exp;
branches;
next	1.42;
commitid	u5ByZ7JDYGBMbD06;

1.42
date	2014.12.22.23.59.43;	author mlarkin;	state Exp;
branches;
next	1.41;
commitid	VnzXkNOPyYOCYtPk;

1.41
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.40;
commitid	yv0ECmCdICvq576h;

1.40
date	2014.11.08.08.18.37;	author mlarkin;	state Exp;
branches;
next	1.39;
commitid	cdNw14OmxaQkiOXD;

1.39
date	2014.10.01.19.41.06;	author mlarkin;	state Exp;
branches;
next	1.38;
commitid	8DlKHWU83o4MRkyW;

1.38
date	2014.07.20.19.47.53;	author deraadt;	state Exp;
branches;
next	1.37;
commitid	ITyy4ODprXXhxf1d;

1.37
date	2014.07.20.18.05.21;	author mlarkin;	state Exp;
branches;
next	1.36;
commitid	F1K1yInguabWnn54;

1.36
date	2014.07.16.17.44.16;	author mlarkin;	state Exp;
branches;
next	1.35;
commitid	hCkTucoKUqYndTAb;

1.35
date	2014.07.09.15.03.12;	author mlarkin;	state Exp;
branches;
next	1.34;
commitid	umg7lvIvVAUp4krC;

1.34
date	2014.07.09.14.35.24;	author mlarkin;	state Exp;
branches;
next	1.33;
commitid	9gbzBlBRZ6WLeVWv;

1.33
date	2014.07.09.14.10.25;	author mlarkin;	state Exp;
branches;
next	1.32;
commitid	T9vAIJG20qGyXHaR;

1.32
date	2014.07.09.11.37.16;	author mlarkin;	state Exp;
branches;
next	1.31;
commitid	ZzCjmXn3ZAUY3nHp;

1.31
date	2014.06.11.00.30.25;	author mlarkin;	state Exp;
branches;
next	1.30;
commitid	bWxTVOsFgnDWnvhd;

1.30
date	2014.05.31.06.30.16;	author mlarkin;	state Exp;
branches;
next	1.29;

1.29
date	2013.10.20.20.03.03;	author mlarkin;	state Exp;
branches;
next	1.28;

1.28
date	2013.10.20.11.16.56;	author deraadt;	state Exp;
branches;
next	1.27;

1.27
date	2013.10.20.09.41.31;	author mlarkin;	state Exp;
branches;
next	1.26;

1.26
date	2013.08.24.23.43.36;	author mlarkin;	state Exp;
branches;
next	1.25;

1.25
date	2013.06.04.01.20.23;	author pirofti;	state Exp;
branches;
next	1.24;

1.24
date	2013.05.31.19.59.59;	author mlarkin;	state Exp;
branches;
next	1.23;

1.23
date	2013.05.30.20.16.54;	author mlarkin;	state Exp;
branches;
next	1.22;

1.22
date	2013.05.30.19.00.59;	author mlarkin;	state Exp;
branches;
next	1.21;

1.21
date	2013.04.23.16.02.10;	author pirofti;	state Exp;
branches;
next	1.20;

1.20
date	2013.03.07.01.26.54;	author mlarkin;	state Exp;
branches;
next	1.19;

1.19
date	2012.10.10.23.33.01;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2012.03.27.18.20.03;	author deraadt;	state Exp;
branches;
next	1.17;

1.17
date	2011.11.23.06.54.51;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	2011.11.16.23.52.27;	author mlarkin;	state Exp;
branches;
next	1.15;

1.15
date	2011.11.14.00.26.03;	author mlarkin;	state Exp;
branches;
next	1.14;

1.14
date	2011.11.13.18.38.10;	author mlarkin;	state Exp;
branches;
next	1.13;

1.13
date	2011.09.22.22.12.45;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	2011.09.21.02.51.23;	author mlarkin;	state Exp;
branches;
next	1.11;

1.11
date	2011.07.11.03.30.32;	author mlarkin;	state Exp;
branches;
next	1.10;

1.10
date	2011.07.09.04.40.22;	author mlarkin;	state Exp;
branches;
next	1.9;

1.9
date	2011.07.09.03.55.22;	author mlarkin;	state Exp;
branches;
next	1.8;

1.8
date	2011.07.09.03.10.27;	author mlarkin;	state Exp;
branches;
next	1.7;

1.7
date	2011.07.09.01.30.39;	author mlarkin;	state Exp;
branches;
next	1.6;

1.6
date	2011.07.09.00.55.00;	author mlarkin;	state Exp;
branches;
next	1.5;

1.5
date	2011.07.09.00.27.31;	author mlarkin;	state Exp;
branches;
next	1.4;

1.4
date	2011.07.09.00.08.04;	author mlarkin;	state Exp;
branches;
next	1.3;

1.3
date	2011.05.09.00.08.28;	author mlarkin;	state Exp;
branches;
next	1.2;

1.2
date	2011.05.08.23.04.36;	author mlarkin;	state Exp;
branches;
next	1.1;

1.1
date	2011.04.30.15.33.18;	author mlarkin;	state Exp;
branches;
next	;


desc
@@


1.49
log
@
split the ACPI resume trampoline into code and data pages, and protect
with proper permissions. Same treatment was done on amd64 last year, i386
is catching up.

This diff has been in snaps for a few days, no regressions reported.

ok deraadt@@
@
text
@/*	$OpenBSD: hibernate_machdep.c,v 1.48 2016/05/18 03:45:11 mlarkin Exp $	*/

/*
 * Copyright (c) 2011 Mike Larkin <mlarkin@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>
#include <sys/buf.h>
#include <sys/conf.h>
#include <sys/device.h>
#include <sys/disk.h>
#include <sys/disklabel.h>
#include <sys/hibernate.h>
#include <sys/timeout.h>
#include <sys/malloc.h>

#include <dev/acpi/acpivar.h>

#include <uvm/uvm_extern.h>
#include <uvm/uvm_pmemrange.h>

#include <machine/hibernate.h>
#include <machine/hibernate_var.h>
#include <machine/kcore.h>
#include <machine/pmap.h>

#ifdef MULTIPROCESSOR
#include <machine/mpbiosvar.h>
#endif /* MULTIPROCESSOR */

#include "acpi.h"
#include "wd.h"
#include "ahci.h"
#include "softraid.h"
#include "sd.h"

/* Hibernate support */
void    hibernate_enter_resume_4k_pte(vaddr_t, paddr_t);
void    hibernate_enter_resume_4k_pde(vaddr_t);
void    hibernate_enter_resume_4m_pde(vaddr_t, paddr_t);

extern	void hibernate_resume_machdep(void);
extern	void hibernate_flush(void);
extern	caddr_t start, end;
extern	int ndumpmem;
extern  struct dumpmem dumpmem[];
extern	bios_memmap_t *bios_memmap;
extern	struct hibernate_state *hibernate_state;

/*
 * Hibernate always uses non-PAE page tables during resume, so
 * redefine masks and pt_entry_t sizes in case PAE is in use.
 */
#define PAGE_MASK_L2    (NBPD - 1)
typedef uint32_t pt_entry_t;

/*
 * i386 MD Hibernate functions
 *
 * see i386 hibernate.h for lowmem layout used during hibernate
 */

/*
 * Returns the hibernate write I/O function to use on this machine
 */
hibio_fn
get_hibernate_io_function(dev_t dev)
{
	char *blkname = findblkname(major(dev));

	if (blkname == NULL)
		return NULL;

#if NWD > 0
	if (strcmp(blkname, "wd") == 0) {
		extern int wd_hibernate_io(dev_t dev, daddr_t blkno,
		    vaddr_t addr, size_t size, int op, void *page);
		return wd_hibernate_io;
	}
#endif
#if NSD > 0
	if (strcmp(blkname, "sd") == 0) {
		extern struct cfdriver sd_cd;
		extern int ahci_hibernate_io(dev_t dev, daddr_t blkno,
		    vaddr_t addr, size_t size, int op, void *page);
		extern int sr_hibernate_io(dev_t dev, daddr_t blkno,
		    vaddr_t addr, size_t size, int op, void *page);
		struct device *dv = disk_lookup(&sd_cd, DISKUNIT(dev));

#if NAHCI > 0
		if (dv && dv->dv_parent && dv->dv_parent->dv_parent &&
		    strcmp(dv->dv_parent->dv_parent->dv_cfdata->cf_driver->cd_name,
		    "ahci") == 0)
			return ahci_hibernate_io;
#endif
#if NSOFTRAID > 0
		if (dv && dv->dv_parent && dv->dv_parent->dv_parent &&
		    strcmp(dv->dv_parent->dv_parent->dv_cfdata->cf_driver->cd_name,
		    "softraid") == 0)
			return sr_hibernate_io;
	}
#endif
#endif /* NSD > 0 */
	return NULL;
}

/*
 * Gather MD-specific data and store into hiber_info
 */
int
get_hibernate_info_md(union hibernate_info *hiber_info)
{
	int i;
	bios_memmap_t *bmp;

	/* Calculate memory ranges */
	hiber_info->nranges = ndumpmem;
	hiber_info->image_size = 0;

	for (i = 0; i < ndumpmem; i++) {
		hiber_info->ranges[i].base = dumpmem[i].start * PAGE_SIZE;
		hiber_info->ranges[i].end = dumpmem[i].end * PAGE_SIZE;
		hiber_info->image_size += hiber_info->ranges[i].end -
		    hiber_info->ranges[i].base;
	}

	/* Record lowmem PTP page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = PTP0_PA;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;

#if NACPI > 0
	/* Record ACPI trampoline code page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = ACPI_TRAMPOLINE;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;

	/* Record ACPI trampoline data page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = ACPI_TRAMP_DATA;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;
#endif
#ifdef MULTIPROCESSOR
	/* Record MP trampoline code page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = MP_TRAMPOLINE;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;

	/* Record MP trampoline data page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = MP_TRAMP_DATA;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;
#endif /* MULTIPROCESSOR */

	for (bmp = bios_memmap; bmp->type != BIOS_MAP_END; bmp++) {
		/* Skip non-NVS ranges (already processed) */
		if (bmp->type != BIOS_MAP_NVS)
			continue;
		if (hiber_info->nranges >= VM_PHYSSEG_MAX)
			return (1);

		i = hiber_info->nranges;	
		hiber_info->ranges[i].base = round_page(bmp->addr);
		hiber_info->ranges[i].end = trunc_page(bmp->addr + bmp->size);
		hiber_info->image_size += hiber_info->ranges[i].end -
			hiber_info->ranges[i].base;
		hiber_info->nranges++;
	}

	hibernate_sort_ranges(hiber_info);

	return (0);
}

/*
 * Enter a mapping for va->pa in the resume pagetable, using
 * the specified size.
 *
 * size : 0 if a 4KB mapping is desired
 *        1 if a 4MB mapping is desired
 */
void
hibernate_enter_resume_mapping(vaddr_t va, paddr_t pa, int size)
{
	if (size)
		return hibernate_enter_resume_4m_pde(va, pa);
	else
		return hibernate_enter_resume_4k_pte(va, pa);
}

/*
 * Enter a 4MB PDE mapping for the supplied VA/PA into the resume-time pmap
 */
void
hibernate_enter_resume_4m_pde(vaddr_t va, paddr_t pa)
{
	pt_entry_t *pde, npde;

	pde = s4pde_4m(va);
	npde = (pa & HIB_PD_MASK) | PG_RW | PG_V | PG_M | PG_PS;
	*pde = npde;
}

/*
 * Enter a 4KB PTE mapping for the supplied VA/PA into the resume-time pmap.
 */
void
hibernate_enter_resume_4k_pte(vaddr_t va, paddr_t pa)
{
	pt_entry_t *pte, npte;

	pte = s4pte_4k(va);
	npte = (pa & PMAP_PA_MASK) | PG_RW | PG_V | PG_M;
	*pte = npte;
}

/*
 * Enter a 4KB PDE mapping for the supplied VA into the resume-time pmap.
 */
void
hibernate_enter_resume_4k_pde(vaddr_t va)
{
	pt_entry_t *pde, npde;

	pde = s4pde_4k(va);
	npde = (HIBERNATE_PT_PAGE & PMAP_PA_MASK) | PG_RW | PG_V | PG_M;
	*pde = npde;
}

/*
 * Create the resume-time page table. This table maps the image(pig) area,
 * the kernel text area, and various utility pages for use during resume,
 * since we cannot overwrite the resuming kernel's page table during inflate
 * and expect things to work properly.
 */
void
hibernate_populate_resume_pt(union hibernate_info *hib_info,
    paddr_t image_start, paddr_t image_end)
{
	int phys_page_number, i;
	paddr_t pa;
	vaddr_t kern_start_4m_va, kern_end_4m_va, page;
	vaddr_t piglet_start_va, piglet_end_va;
	struct pmap *kpm = pmap_kernel();

	/* Identity map PD, PT, and stack pages */
	pmap_kenter_pa(HIBERNATE_PT_PAGE, HIBERNATE_PT_PAGE, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PD_PAGE, HIBERNATE_PD_PAGE, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_STACK_PAGE, HIBERNATE_STACK_PAGE, PROT_MASK);
	pmap_activate(curproc);

	bzero((caddr_t)HIBERNATE_PT_PAGE, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PD_PAGE, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_STACK_PAGE, PAGE_SIZE);

	/* PDE for low pages */
	hibernate_enter_resume_4k_pde(0);

	/*
	 * Identity map low physical pages.
	 * See arch/i386/include/hibernate_var.h for page ranges used here.
	 */
	for (i = ACPI_TRAMPOLINE; i <= HIBERNATE_HIBALLOC_PAGE; i += PAGE_SIZE)
		hibernate_enter_resume_mapping(i, i, 0);

	/*
	 * Map current kernel VA range using 4M pages
	 */
	kern_start_4m_va = (vaddr_t)&start & ~(PAGE_MASK_L2);
	kern_end_4m_va = (vaddr_t)&end & ~(PAGE_MASK_L2);

	/* i386 kernels load at 2MB phys (on the 0th 4mb page) */
	phys_page_number = 0;

	for (page = kern_start_4m_va; page <= kern_end_4m_va;
	    page += NBPD, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD);
		hibernate_enter_resume_mapping(page, pa, 1);
	}

	/*
	 * Identity map the image (pig) area
	 */
	phys_page_number = image_start / NBPD;
	image_start &= ~(PAGE_MASK_L2);
	image_end &= ~(PAGE_MASK_L2);
	for (page = image_start; page <= image_end ;
	    page += NBPD, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD);
		hibernate_enter_resume_mapping(page, pa, 1);
	}

	/*
	 * Identity map the piglet using 4MB pages.
	 */
	phys_page_number = hib_info->piglet_pa / NBPD;

	/* VA == PA */
	piglet_start_va = hib_info->piglet_pa;
	piglet_end_va = piglet_start_va + HIBERNATE_CHUNK_SIZE * 4;

	for (page = piglet_start_va; page <= piglet_end_va;
	    page += NBPD, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD);
		hibernate_enter_resume_mapping(page, pa, 1);
	}

	/*
	 * Fill last 8 slots of the new PD with the PAE PDPTEs of the
	 * kernel pmap, such that we can easily switch back into
	 * non-PAE mode.  If we're running in non-PAE mode, this will
	 * just fill the slots with zeroes.
	 */
	((uint64_t *)HIBERNATE_PD_PAGE)[508] = kpm->pm_pdidx[0];
	((uint64_t *)HIBERNATE_PD_PAGE)[509] = kpm->pm_pdidx[1];
	((uint64_t *)HIBERNATE_PD_PAGE)[510] = kpm->pm_pdidx[2];
	((uint64_t *)HIBERNATE_PD_PAGE)[511] = kpm->pm_pdidx[3];

	/* Unmap MMU pages (stack remains mapped) */
	pmap_kremove(HIBERNATE_PT_PAGE, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PD_PAGE, PAGE_SIZE);
	pmap_activate(curproc);
}

/*
 * During inflate, certain pages that contain our bookkeeping information
 * (eg, the chunk table, scratch pages, etc) need to be skipped over and
 * not inflated into.
 *
 * Returns 1 if the physical page at dest should be skipped, 0 otherwise
 */
int
hibernate_inflate_skip(union hibernate_info *hib_info, paddr_t dest)
{
	if (dest >= hib_info->piglet_pa &&
	    dest <= (hib_info->piglet_pa + 4 * HIBERNATE_CHUNK_SIZE))
		return (1);

	return (0);
}

void
hibernate_enable_intr_machdep(void)
{
	enable_intr();
}

void
hibernate_disable_intr_machdep(void)
{
	disable_intr();
}

#ifdef MULTIPROCESSOR
/*
 * On i386, the APs have not been hatched at the time hibernate resume is
 * called, so there is no need to quiesce them. We do want to make sure
 * however that we are on the BSP.
 */
void
hibernate_quiesce_cpus(void)
{
	KASSERT(CPU_IS_PRIMARY(curcpu()));
}
#endif /* MULTIPROCESSOR */
@


1.48
log
@
Split i386 mp hatch trampoline into code and data pages, and protect each
with proper W^X policy. The same thing was done for amd64 late last year,
catching i386 up now. Diff has been in snaps for a few days with no
reported fallout.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.47 2015/08/21 07:01:38 mlarkin Exp $	*/
d139 9
d149 1
d153 9
@


1.47
log
@
use vaddr_t for kernel va range calculation instead of paddr_t. No binary
change but using paddr_t here wasn't correct - better to clean it up.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.46 2015/04/26 11:09:32 kettenis Exp $	*/
d149 1
d157 10
a166 1
#endif
@


1.46
log
@Disable PAE when switching to the hibernate resume pagetables.  This involves
a slightly conmplicated dance where we stash the PAE PDPTEs into the
hibernate resume pagetables and use those before turning off PAE.
Makes (un)hibernate work with the new PAE pmap.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.45 2015/04/12 18:37:53 mlarkin Exp $	*/
d272 2
a273 2
	kern_start_4m_va = (paddr_t)&start & ~(PAGE_MASK_L2);
	kern_end_4m_va = (paddr_t)&end & ~(PAGE_MASK_L2);
@


1.45
log
@
Bring PAE code back to life, in a different form. This diff (via bluhm then
to deraadt, then myself) brings the PAE pmap on i386 (not touched in any
significant way for years) closer to the current non-PAE pmap and allows
us to take a big next step toward better i386 W^X in the kernel (similar to
what we did a few months ago on amd64). Unlike the original PAE pmap, this
diff will not be supporting > 4GB physical memory on i386 - this effort is
specifically geared toward providing W^X (via NX) only.

There still seems to be a bug removing certain pmap entries when PAE is
enabled, so I'm leaving PAE mode disabled for the moment until we can
figure out what is going on, but with this diff in the tree hopefully
others can help.

The pmap functions now operate through function pointers, due to the need
to support both non-PAE and PAE forms. My unscientific testing showed
less than 0.3% (a third of a percent) slowdown with this approach during
a base build.

Discussed for months with guenther, kettenis, and deraadt.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.44 2015/01/09 03:43:52 mlarkin Exp $	*/
d247 1
d310 11
@


1.44
log
@
Cleanup some macros and #defines in i386 pmap. Previously committed and
backed out because of libkvm breakage, recommitting now with libkvm fix.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.43 2014/12/23 01:24:50 deraadt Exp $	*/
d61 7
@


1.43
log
@backout previous, because libkvm needs two pieces.  will let mike
find a different way.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.41 2014/11/16 12:30:57 deraadt Exp $	*/
d196 1
a196 1
	npde = (pa & PD_MASK) | PG_RW | PG_V | PG_M | PG_PS;
@


1.42
log
@Move PD_MASK, PT_MASK and a couple macros into pmap.c. The only other
user of these was hibernate, which now gets its own PD_MASK (since
the resume time PD_MASK is essentially disjoint from the runtime
PD_MASK). No functional change, just moving the deck chairs around in
preparation for an upcoming change.

ok deraadt
@
text
@d196 1
a196 1
	npde = (pa & HIB_PD_MASK) | PG_RW | PG_V | PG_M | PG_PS;
@


1.41
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.40 2014/11/08 08:18:37 mlarkin Exp $	*/
d196 1
a196 1
	npde = (pa & PD_MASK) | PG_RW | PG_V | PG_M | PG_PS;
@


1.40
log
@
No need to keep the temporary mappings for the MMU pages for the resume
time page table after we are done creating it in the resuming kernel.
Note the resume time stack has to remain mapped as it is used during the
initial phase of the hibernate unpack while the original page table is
still being used.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.39 2014/10/01 19:41:06 mlarkin Exp $	*/
d242 3
a244 3
	pmap_kenter_pa(HIBERNATE_PT_PAGE, HIBERNATE_PT_PAGE, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PD_PAGE, HIBERNATE_PD_PAGE, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_STACK_PAGE, HIBERNATE_STACK_PAGE, VM_PROT_ALL);
@


1.39
log
@

Move some hibernate #defines to pte.h and eliminate some duplicate defines
from hibernate code that were already defined in pte.h (with different
names). No functional change.

ok sf@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.38 2014/07/20 19:47:53 deraadt Exp $	*/
d302 5
@


1.38
log
@look up correct dev_t.  This matters for the case where a device is
underlying softraid.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.37 2014/07/20 18:05:21 mlarkin Exp $	*/
d196 1
a196 1
	npde = (pa & PMAP_PA_MASK_4M) | PG_RW | PG_V | PG_M | PG_PS;
d264 2
a265 2
	kern_start_4m_va = (paddr_t)&start & ~(PAGE_MASK_4M);
	kern_end_4m_va = (paddr_t)&end & ~(PAGE_MASK_4M);
d280 2
a281 2
	image_start &= ~(PAGE_MASK_4M);
	image_end &= ~(PAGE_MASK_4M);
@


1.37
log
@
Support hibernating to softraid crypto volumes.

much help and ok from deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.36 2014/07/16 17:44:16 mlarkin Exp $	*/
d93 1
a93 1
		struct device *dv;
a95 1
		dv = disk_lookup(&sd_cd, DISKUNIT(swdevt[0].sw_dev));
@


1.36
log
@
Save and restore NVS ranges when hibernating, as per The Spec.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.35 2014/07/09 15:03:12 mlarkin Exp $	*/
d46 1
d72 1
a72 1
get_hibernate_io_function(void)
d74 1
a74 1
	char *blkname = findblkname(major(swdevt[0].sw_dev));
d78 1
d86 1
a86 1
#if NAHCI > 0 && NSD > 0
d91 2
d95 1
d101 6
d109 1
@


1.35
log
@

Don't use the suspending kernel's VA mapping for the piglet. It's far
easier and much less error-prone to just identity map it in the resuming
kernel as we have more control over the VA space layout there (otherwise
we are at the mercy of the suspending kernel's placement of the piglet VA).

This diff also increases the size of the piglet to 4 chunks, to avoid an
overwrite issue seen in m2k14 where the start of the kernel text was
overwritten with a bounced chunk before unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.34 2014/07/09 14:35:24 mlarkin Exp $	*/
d58 1
d108 1
d122 2
d131 2
d139 15
@


1.34
log
@

Fixes a resume time page table issue on amd64 if the piglet was located
above 1GB physical (caused by using an incorrect page size mask)

Also removes some unneeded low memory mappings on both amd64 and i386 (this
is a cosmetic fix but makes things easier to debug).
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.33 2014/07/09 14:10:25 mlarkin Exp $	*/
d205 1
a205 1
	paddr_t pa, piglet_start, piglet_end;
d207 1
d257 1
a257 1
	 * Map the piglet
d260 6
a265 5
	piglet_start = hib_info->piglet_va;
	piglet_end = piglet_start + HIBERNATE_CHUNK_SIZE * 3;
	piglet_start &= ~(PAGE_MASK_4M);
	piglet_end &= ~(PAGE_MASK_4M);
	for (page = piglet_start; page <= piglet_end ;
d283 1
a283 1
	    dest <= (hib_info->piglet_pa + 3 * HIBERNATE_CHUNK_SIZE))
@


1.33
log
@

Cleanup the chunk placement routine by removing the conflict resolver.
Chunks are now sorted by ascending PA and all chunks are bounced before
unpack. This fixes an issue where the trampoline chunks were being placed
at the end of the unpack ordering, causing overwrite during unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.32 2014/07/09 11:37:16 mlarkin Exp $	*/
d222 2
a223 2
	 * Identity map 64KB-640KB physical for tramps and special utility
	 * pages using 4KB mappings
d225 2
a226 3
	for (i = 16; i < 160; i ++) {
		hibernate_enter_resume_mapping(i*PAGE_SIZE, i*PAGE_SIZE, 0);
	}
@


1.32
log
@
Fixes a hibernate issue wherein we locked the kernel lock while hatching
but then parked ourselves in real mode without completing acquisition of
said lock. Also removes the park routine from i386 since we don't need it
(the APs are already parked at the time we start unpack).

discussed with and ok kettenis@@, also ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.31 2014/06/11 00:30:25 mlarkin Exp $	*/
d112 1
a112 1
	for(i = 0; i < ndumpmem; i++) {
d133 2
@


1.31
log
@
Don't map phys pages < 64KB in the resume page table. We stopped doing this
in the kernel a few months back and there's no reason these pages need to
be mapped during unpack either.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.30 2014/05/31 06:30:16 mlarkin Exp $	*/
d300 3
a302 4
 * Quiesce CPUs in a multiprocessor machine before resuming. We need to do
 * this since the APs will be hatched (but waiting for CPUF_GO), and we don't
 * want the APs to be executing code and causing side effects during the
 * unpack operation.
d307 1
a307 7
        KASSERT(CPU_IS_PRIMARY(curcpu()));

	/* Start the hatched (but idling) APs */
	cpu_boot_secondary_processors();

	/* Now shut them down */
	acpi_sleep_mp();
@


1.30
log
@
Remove some unused code that we added at the 2013 Toronto hackathon but
don't need anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.29 2013/10/20 20:03:03 mlarkin Exp $	*/
d220 1
a220 1
	 * Identity map first 640KB physical for tramps and special utility
d223 1
a223 1
	for (i = 0; i < 160; i ++) {
@


1.29
log
@

Realmode park is causing more problems than it's solving. Remove until we
have a better handle on it.

Add an assert in i386 MP hibernate quiesce cpu function to ensure we are
running on the BSP
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.28 2013/10/20 11:16:56 deraadt Exp $	*/
a266 34
}

/*
 * MD-specific resume preparation (creating resume time pagetables,
 * stacks, etc).
 */
void
hibernate_prepare_resume_machdep(union hibernate_info *hib_info)
{
	paddr_t pa, piglet_end;
	vaddr_t va;

	/*
	 * At this point, we are sure that the piglet's phys space is going to
	 * have been unused by the suspending kernel, but the vaddrs used by
	 * the suspending kernel may or may not be available to us here in the
	 * resuming kernel, so we allocate a new range of VAs for the piglet.
	 * Those VAs will be temporary and will cease to exist as soon as we
	 * switch to the resume PT, so we need to ensure that any VAs required
	 * during inflate are also entered into that map.
	 */

        hib_info->piglet_va = (vaddr_t)km_alloc(HIBERNATE_CHUNK_SIZE*3,
	    &kv_any, &kp_none, &kd_nowait);
        if (!hib_info->piglet_va)
                panic("Unable to allocate vaddr for hibernate resume piglet\n");

	piglet_end = hib_info->piglet_pa + HIBERNATE_CHUNK_SIZE*3;

	for (pa = hib_info->piglet_pa,va = hib_info->piglet_va;
	    pa <= piglet_end; pa += PAGE_SIZE, va += PAGE_SIZE)
		pmap_kenter_pa(va, pa, VM_PROT_ALL);

	pmap_activate(curproc);
@


1.28
log
@Simplify definition of the side-effect-free wd io routine.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.27 2013/10/20 09:41:31 mlarkin Exp $	*/
d342 2
@


1.27
log
@

SMEP (on Ivy Bridge and later CPUs) require page protections that include
at least one supervisor mode (U/S bit = 0) setting in higher level paging
structures. This diff removes PG_u flags from the hibernate resume time
pmap (there was really no reason we needed it), to allow hibernate to
work on Ivy Bridge and later CPUs.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.26 2013/08/24 23:43:36 mlarkin Exp $	*/
a47 5
#if NWD > 0
#include <dev/ata/atavar.h>
#include <dev/ata/wdvar.h>
#endif

d77 3
a79 1
	if (strcmp(blkname, "wd") == 0)
d81 1
@


1.26
log
@

Remove call to sched_start_secondary_cpus in MP unhibernate case until we
can more fully understand the side-effects.

Tested by various people back at t2k13, been sitting in my tree since then.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.25 2013/06/04 01:20:23 pirofti Exp $	*/
d164 1
a164 1
	npde = (pa & PMAP_PA_MASK_4M) | PG_RW | PG_V | PG_u | PG_M | PG_PS;
d177 1
a177 1
	npte = (pa & PMAP_PA_MASK) | PG_RW | PG_V | PG_u | PG_M;
d190 1
a190 1
	npde = (HIBERNATE_PT_PAGE & PMAP_PA_MASK) | PG_RW | PG_V | PG_u | PG_M;
@


1.25
log
@Add RCS ids.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a345 1
	sched_start_secondary_cpus();
@


1.24
log
@

We need to halt the APs on MP hibernate resume or else they will be
executing code possibly causing side effects during the image unpack
operation. But before we can halt the APs, we need to complete their init
(as they will be hatched but idling, possibly with interrupts off).

Introduces MD function hibernate_quiesce_cpus to do this, called from the
MI hibernate resume code.

ok deraadt
@
text
@d1 2
@


1.23
log
@

Fix a bug in amd64 hibernate introduced when we moved the kernel to load at
16MB physical. Document the same in i386 (i386 was not affected, just
commented for diffability)
@
text
@d331 19
@


1.22
log
@

Make interrupt handling in hibernate resume MI by providing MD-specific
functions to enable and disable interrupts, if needed. If a platform doesnt
need interrupt handling in this way, the MD function can be a no-op.

discussed with pirofti and deraadt
@
text
@d232 2
@


1.21
log
@Remove obsolete comment, okay mlarkin@@.
@
text
@d317 12
@


1.20
log
@

Reword some wrong comments and some improperly formatted comments and add
information about piglet memory layout. No functional changes.
@
text
@a79 1
	/* XXX - Only support wd hibernate presently */
@


1.19
log
@Oops.  Need to handle the case of nfs diskless machines, where the block
device name is NULL.
From Tim Wiess
@
text
@d65 2
a270 15
 *
 * On i386, we use the piglet whose address is contained in hib_info
 * as per the following layout:
 *
 * offset from piglet base      use
 * -----------------------      --------------------
 * 0                            i/o allocation area
 * PAGE_SIZE                    i/o read area
 * 2*PAGE_SIZE                  temp/scratch page
 * 5*PAGE_SIZE			resume stack
 * 6*PAGE_SIZE                  hiballoc arena
 * 7*PAGE_SIZE to 87*PAGE_SIZE  zlib inflate area
 * ...
 * HIBERNATE_CHUNK_SIZE         chunk table
 * 2*HIBERNATE_CHUNK_SIZE	bounce/copy area
d279 7
a285 9
	 * At this point, we are sure that the piglet's phys
	 * space is going to have been unused by the suspending
	 * kernel, but the vaddrs used by the suspending kernel
	 * may or may not be available to us here in the
	 * resuming kernel, so we allocate a new range of VAs
	 * for the piglet. Those VAs will be temporary and will
	 * cease to exist as soon as we switch to the resume
	 * PT, so we need to ensure that any VAs required during
	 * inflate are also entered into that map.
@


1.18
log
@fix indentation
@
text
@d73 4
d79 1
a79 1
	if (strcmp(findblkname(major(swdevt[0].sw_dev)), "wd") == 0)
d83 1
a83 1
	if (strcmp(findblkname(major(swdevt[0].sw_dev)), "sd") == 0) {
@


1.17
log
@properly account for the MP tramp page in the ranges
ok mlarkin
@
text
@d82 1
a82 1
		    vaddr_t addr, size_t size, int wr, void *page);
d89 1
a89 1
		return ahci_hibernate_io;
@


1.16
log
@

Reduce use of globals in hibernate code.

discussed with deraadt@@
@
text
@d119 1
a119 1
	hiber_info->nranges ++;
d126 1
@


1.15
log
@

ahci hibernate device selection code; not yet tested

This code is from deraadt@@
@
text
@a54 1
int	hibernate_read_chunks(union hibernate_info *, paddr_t, paddr_t, size_t);
@


1.14
log
@

Fix a handful of bugs that were causing reboots and other bad behavior
during hibernate resumes.
@
text
@d43 2
d78 14
@


1.13
log
@KNF of mlarkin's code, requested by him.  Some improvements to the interface
for talking to the disk driver snuck in.
ok mlarkin
@
text
@a54 2
extern	vaddr_t hibernate_inflate_page;

a189 2

	hibernate_inflate_page = HIBERNATE_INFLATE_PAGE;
@


1.12
log
@

Perform most of the remaining refactoring of hibernate code into
MI/MD parts. This also introduces a chunk placement routine that was
originally developed at c2k11 with help from drahn and ariane.

There are still a few more things to do for hibernate, but those can be
worked on in-tree. This code is disabled by default, and not yet called.

ok deraadt@@ (and deraadt@@ said kettenis@@ also ok'ed it :) )
@
text
@a36 3
#include <dev/ata/atavar.h>
#include <dev/ata/wdvar.h>

d44 5
a58 1
extern	char *disk_readlabel(struct disklabel *, dev_t, char *, size_t);
a69 1
 *
d71 2
a72 2
void *
get_hibernate_io_function()
d78 1
a78 3
	else
		return NULL;
#else
a79 1
#endif
d94 1
a94 1
	for(i=0; i<ndumpmem; i++) {
d98 1
a98 1
			hiber_info->ranges[i].base;
d103 2
a104 2
	hiber_info->ranges[hiber_info->nranges].end = 
		hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
d110 2
a111 2
	hiber_info->ranges[hiber_info->nranges].end = 
		hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
d113 1
a113 1
#endif	
d130 2
a131 3
	else {
		return hibernate_enter_resume_4k_pte(va, pa);			
	}		
d181 1
a181 1
	paddr_t image_start, paddr_t image_end)
d203 1
a203 1
	 * Identity map first 640KB physical for tramps and special utility 
d206 1
a206 1
	for (i=0; i < 160; i ++) {
d215 1
a215 1
	phys_page_number = 0; 
d217 1
a217 1
	for (page = kern_start_4m_va ; page <= kern_end_4m_va ; 
a218 1

a230 1

d281 1
a281 1
 	 * for the piglet. Those VAs will be temporary and will
d284 1
a284 1
	 * inflate are also entered into that map. 
d288 1
a288 1
						&kv_any, &kp_none, &kd_nowait);
d294 3
a296 5
	for (pa = hib_info->piglet_pa,
		va = hib_info->piglet_va;
		pa <= piglet_end;
		pa += PAGE_SIZE, va += PAGE_SIZE)
			pmap_kenter_pa(va, pa, VM_PROT_ALL);
d316 1
a316 2
} 

@


1.11
log
@

Add hibernate_read_block and fix a couple of typos in the previous commit.
@
text
@a46 1
#ifndef SMALL_KERNEL
a47 3
int	hibernate_write_image(void);
int	hibernate_read_image(void);
void	hibernate_unpack_image(void);
d51 1
a51 2
void	hibernate_populate_resume_pt(paddr_t, paddr_t);
int	get_hibernate_info_md(union hibernate_info *);
d53 1
a53 2
union 	hibernate_info *global_hiber_info;
paddr_t global_image_start;
d55 1
a55 3
extern	void hibernate_resume_machine(void);
extern	void hibernate_activate_resume_pt(void);
extern	void hibernate_switch_stack(void);
a67 35
 * hibernate_zlib_reset
 *
 * Reset the zlib stream state and allocate a new hiballoc area for either
 * inflate or deflate. This function is called once for each hibernate chunk
 * Calling hiballoc_init multiple times is acceptable since the memory it is
 * provided is unmanaged memory (stolen).
 *
 */
int
hibernate_zlib_reset(int deflate)
{
	hibernate_state = (struct hibernate_state *)HIBERNATE_ZLIB_SCRATCH;

	bzero((caddr_t)HIBERNATE_ZLIB_START, HIBERNATE_ZLIB_SIZE);
	bzero((caddr_t)HIBERNATE_ZLIB_SCRATCH, PAGE_SIZE);

	/* Set up stream structure */
	hibernate_state->hib_stream.zalloc = (alloc_func)hibernate_zlib_alloc;
	hibernate_state->hib_stream.zfree = (free_func)hibernate_zlib_free;

	/* Initialize the hiballoc arena for zlib allocs/frees */
	hiballoc_init(&hibernate_state->hiballoc_arena,
		(caddr_t)HIBERNATE_ZLIB_START, HIBERNATE_ZLIB_SIZE);

	if (deflate) {
		return deflateInit(&hibernate_state->hib_stream,
			Z_DEFAULT_COMPRESSION);
	}
	else
		return inflateInit(&hibernate_state->hib_stream);
}

/*
 * get_hibernate_io_function
 *
a85 2
 * get_hibernate_info_md
 *
d123 1
a123 1
 * the specified size hint.
d125 1
a125 1
 * hint : 0 if a 4KB mapping is desired
d129 1
a129 1
hibernate_enter_resume_mapping(vaddr_t va, paddr_t pa, int hint)
d131 1
a131 1
	if (hint)
a133 1
		hibernate_enter_resume_4k_pde(va);
d179 3
a181 3
 * the kernel text area, and various utility pages located in low memory for
 * use during resume, since we cannot overwrite the resuming kernel's 
 * page table during inflate and expect things to work properly.
d184 2
a185 1
hibernate_populate_resume_pt(paddr_t image_start, paddr_t image_end)
d188 1
a188 1
	paddr_t pa;
d191 8
d201 4
a214 9
	 * Identity map chunk table
	 */
	for (i=0; i < HIBERNATE_CHUNK_TABLE_SIZE/PAGE_SIZE; i ++) {
		hibernate_enter_resume_mapping(
			(vaddr_t)HIBERNATE_CHUNK_TABLE_START + (i*PAGE_SIZE),
			HIBERNATE_CHUNK_TABLE_START + (i*PAGE_SIZE), 0);
	}
	
	/*
d240 14
d257 2
a258 1
 * hibernate_write_image
d260 2
a261 2
 * Write a compressed version of this machine's memory to disk, at the
 * precalculated swap offset:
d263 11
a273 15
 * end of swap - signature block size - chunk table size - memory size
 *
 * The function begins by mapping various pages into the suspending
 * kernel's pmap, then loops through each phys mem range, cutting each
 * one into 4MB chunks. These chunks are then compressed individually
 * and written out to disk, in phys mem order. Some chunks might compress
 * more than others, and for this reason, each chunk's size is recorded
 * in the chunk table, which is written to disk after the image has
 * properly been compressed and written.
 *
 * When this function is called, the machine is nearly suspended - most
 * devices are quiesced/suspended, interrupts are off, and cold has
 * been set. This means that there can be no side effects once the
 * write has started, and the write function itself can also have no
 * side effects.
a274 220
int
hibernate_write_image()
{
	union hibernate_info hiber_info;
	paddr_t range_base, range_end, inaddr, temp_inaddr;
	vaddr_t zlib_range;
	daddr_t blkctr;
	int i;
	size_t nblocks, out_remaining, used, offset;
	struct hibernate_disk_chunk *chunks;

	/* Get current running machine's hibernate info */
	bzero(&hiber_info, sizeof(hiber_info));
	if (get_hibernate_info(&hiber_info))
		return (1);

	zlib_range = HIBERNATE_ZLIB_START;

	/* Map utility pages */
	pmap_kenter_pa(HIBERNATE_ALLOC_PAGE, HIBERNATE_ALLOC_PAGE,
		VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_IO_PAGE, HIBERNATE_IO_PAGE, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_TEMP_PAGE, HIBERNATE_TEMP_PAGE,
		VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_ZLIB_SCRATCH, HIBERNATE_ZLIB_SCRATCH,
		VM_PROT_ALL);
	
	/* Map the zlib allocation ranges */
	for(zlib_range = HIBERNATE_ZLIB_START; zlib_range < HIBERNATE_ZLIB_END;
		zlib_range += PAGE_SIZE) {
		pmap_kenter_pa((vaddr_t)(zlib_range+i),
			(paddr_t)(zlib_range+i),
			VM_PROT_ALL);
	}

	/* Identity map the chunktable */
	for(i=0; i < HIBERNATE_CHUNK_TABLE_SIZE; i += PAGE_SIZE) {
		pmap_kenter_pa((vaddr_t)(HIBERNATE_CHUNK_TABLE_START+i),
			(paddr_t)(HIBERNATE_CHUNK_TABLE_START+i),
			VM_PROT_ALL);
	}

	pmap_activate(curproc);

	blkctr = hiber_info.image_offset;
	hiber_info.chunk_ctr = 0;
	offset = 0;
	chunks = (struct hibernate_disk_chunk *)HIBERNATE_CHUNK_TABLE_START;

	/* Calculate the chunk regions */
	for (i=0; i < hiber_info.nranges; i++) {
                range_base = hiber_info.ranges[i].base;
                range_end = hiber_info.ranges[i].end;

		inaddr = range_base;

		while (inaddr < range_end) {
			chunks[hiber_info.chunk_ctr].base = inaddr;
			if (inaddr + HIBERNATE_CHUNK_SIZE < range_end)
				chunks[hiber_info.chunk_ctr].end = inaddr +
					HIBERNATE_CHUNK_SIZE;
			else
				chunks[hiber_info.chunk_ctr].end = range_end;

			inaddr += HIBERNATE_CHUNK_SIZE;
			hiber_info.chunk_ctr ++;
		}
	} 

	/* Compress and write the chunks in the chunktable */
	for (i=0; i < hiber_info.chunk_ctr; i++) {
		range_base = chunks[i].base;
		range_end = chunks[i].end;

		chunks[i].offset = blkctr; 

		/* Reset zlib for deflate */
		if (hibernate_zlib_reset(1) != Z_OK)
			return (1);

		inaddr = range_base;

		/*
		 * For each range, loop through its phys mem region
		 * and write out 4MB chunks (the last chunk might be
		 * smaller than 4MB).
		 */
		while (inaddr < range_end) {
			out_remaining = PAGE_SIZE;
			while (out_remaining > 0 && inaddr < range_end) {
				pmap_kenter_pa(HIBERNATE_TEMP_PAGE2,
					inaddr & PMAP_PA_MASK, VM_PROT_ALL);
				pmap_activate(curproc);
				bcopy((caddr_t)HIBERNATE_TEMP_PAGE2,
					(caddr_t)HIBERNATE_TEMP_PAGE, PAGE_SIZE);

				/* Adjust for non page-sized regions */
				temp_inaddr = (inaddr & PAGE_MASK) +
					HIBERNATE_TEMP_PAGE;

				/* Deflate from temp_inaddr to IO page */
				inaddr += hibernate_deflate(temp_inaddr,
					&out_remaining);
			}

			if (out_remaining == 0) {
				/* Filled up the page */
				nblocks = PAGE_SIZE / hiber_info.secsize;

				if(hiber_info.io_func(hiber_info.device, blkctr,
					(vaddr_t)HIBERNATE_IO_PAGE, PAGE_SIZE,
					1, (void *)HIBERNATE_ALLOC_PAGE))
						return (1);

				blkctr += nblocks;
			}
			
		}

		if (inaddr != range_end)
			return (1);

		/*
		 * End of range. Round up to next secsize bytes
		 * after finishing compress
		 */
		if (out_remaining == 0)
			out_remaining = PAGE_SIZE;

		/* Finish compress */
		hibernate_state->hib_stream.avail_in = 0;
		hibernate_state->hib_stream.avail_out = out_remaining;
		hibernate_state->hib_stream.next_in = (caddr_t)inaddr;
		hibernate_state->hib_stream.next_out = 
			(caddr_t)HIBERNATE_IO_PAGE + (PAGE_SIZE - out_remaining);

		if (deflate(&hibernate_state->hib_stream, Z_FINISH) !=
			Z_STREAM_END)
				return (1);

		out_remaining = hibernate_state->hib_stream.avail_out;

		used = PAGE_SIZE - out_remaining;
		nblocks = used / hiber_info.secsize;

		/* Round up to next block if needed */
		if (used % hiber_info.secsize != 0)
			nblocks ++;

		/* Write final block(s) for this chunk */
		if( hiber_info.io_func(hiber_info.device, blkctr,
			(vaddr_t)HIBERNATE_IO_PAGE, nblocks*hiber_info.secsize,
			1, (void *)HIBERNATE_ALLOC_PAGE))
				return (1);

		blkctr += nblocks;

		offset = blkctr;
		chunks[i].compressed_size=
			(offset-chunks[i].offset)*hiber_info.secsize;
	}

	/* Image write complete, write the signature+chunk table and return */
	return hibernate_write_signature(&hiber_info);
}

int
hibernate_read_image()
{
	union hibernate_info hiber_info;
	int i, j;
	paddr_t range_base, range_end, addr;
	daddr_t blkctr;

	/* Get current running machine's hibernate info */
	if (get_hibernate_info(&hiber_info))
		return (1);

	pmap_kenter_pa(HIBERNATE_TEMP_PAGE, HIBERNATE_TEMP_PAGE, VM_PROT_ALL);	
	pmap_kenter_pa(HIBERNATE_ALLOC_PAGE, HIBERNATE_ALLOC_PAGE, VM_PROT_ALL);

	blkctr = hiber_info.image_offset;

	for (i=0; i < hiber_info.nranges; i++) {
		range_base = hiber_info.ranges[i].base;
		range_end = hiber_info.ranges[i].end;

		for (j=0; j < (range_end - range_base)/NBPG;
		    blkctr += (NBPG/512), j += NBPG) {
			addr = range_base + j;
			pmap_kenter_pa(HIBERNATE_TEMP_PAGE, addr,
				VM_PROT_ALL);
			hiber_info.io_func(hiber_info.device, blkctr,
				(vaddr_t)HIBERNATE_IO_PAGE, NBPG, 1,
				(void *)HIBERNATE_ALLOC_PAGE);
			bcopy((caddr_t)HIBERNATE_IO_PAGE,
				(caddr_t)HIBERNATE_TEMP_PAGE,
				NBPG);
		
		}
	}

	/* Read complete, clear the signature and return */
	return hibernate_clear_signature();
}

int
hibernate_suspend()
{
	/*
	 * On i386, the only thing to do on hibernate suspend is
	 * to zero all the unused pages and write the image.
	 */

	uvm_pmr_zero_everything();

	return hibernate_write_image();
}

/* Unpack image from resumed image to real location */
d276 1
a276 1
hibernate_unpack_image()
d278 2
a279 45
	union hibernate_info *hiber_info = global_hiber_info;
	int i, j;
	paddr_t base, end, pig_base;

	hibernate_activate_resume_pt();

	for (i=0; i<hiber_info->nranges; i++) {
		base = hiber_info->ranges[i].base;
		end = hiber_info->ranges[i].end;
		pig_base = base + global_image_start;

		for (j=base; j< (end - base)/NBPD; j++) {
			hibernate_enter_resume_mapping(base, base, 0);
			bcopy((caddr_t)pig_base, (caddr_t)base, NBPD);
		}
	}
}

void
hibernate_resume()
{
	union hibernate_info hiber_info, disk_hiber_info;
	u_int8_t *io_page;
	int s;
	paddr_t image_start;

	/* Get current running machine's hibernate info */
	if (get_hibernate_info(&hiber_info))
		return;

	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return;
	
	/* Read hibernate info from disk */
	s = splbio();
	hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
		(vaddr_t)&disk_hiber_info, 512, 0, io_page);

	free(io_page, M_DEVBUF);

	if (memcmp(&hiber_info, &disk_hiber_info,
	    sizeof(union hibernate_info)) !=0) {
		return;
	}
d282 9
a290 2
	 * On-disk and in-memory hibernate signatures match,
	 * this means we should do a resume from hibernate.
d293 12
a304 2
	disable_intr();
	cold = 1;
a305 5
	/*
	 * Add mappings for resume stack and PT page tables
	 * into the "resuming" kernel. We use these mappings
	 * during image read and copy
	 */
a306 35
	pmap_kenter_pa((vaddr_t)HIBERNATE_STACK_PAGE,
		(paddr_t)HIBERNATE_STACK_PAGE,
		VM_PROT_ALL);
	pmap_kenter_pa((vaddr_t)HIBERNATE_PT_PAGE,
		(paddr_t)HIBERNATE_PT_PAGE,
		VM_PROT_ALL);

	/* 
	 * We can't access any of this function's local variables (via 
	 * stack) after we switch stacks, so we stash hiber_info and
	 * the image start area into temporary global variables first.
	 */
	global_hiber_info = &hiber_info;
	global_image_start = image_start;

	/* Switch stacks */
	hibernate_switch_stack();

	/* Read the image from disk into the image (pig) area */
	if (hibernate_read_image())
		panic("Failed to restore the hibernate image");

	/*
	 * Image is now in high memory (pig area), copy to "correct" 
	 * location in memory. We'll eventually end up copying on top
	 * of ourself, but we are assured the kernel code here is
	 * the same between the hibernated and resuming kernel, 
	 * and we are running on our own stack
	 */
	hibernate_unpack_image();	
	
	/*
	 * Resume the loaded kernel by jumping to the S3 resume vector
	 */
	hibernate_resume_machine();
a309 2
 * hibernate_inflate_skip
 *
d317 1
a317 1
hibernate_inflate_skip(paddr_t dest)
d319 2
a320 13
	/* Chunk Table */
	if (dest >= HIBERNATE_CHUNK_TABLE_START && 
	    dest <= HIBERNATE_CHUNK_TABLE_END)
		return (1);

	/* Contiguous utility pages */
	if (dest >= HIBERNATE_STACK_PAGE &&
	    dest <= HIBERNATE_CHUNKS_PAGE)
		return (1);

	/* libz hiballoc arena */
	if (dest >= HIBERNATE_ZLIB_SCRATCH &&
	    dest <= HIBERNATE_ZLIB_END)
d325 1
a325 1
#endif /* !SMALL_KERNEL */
@


1.10
log
@

Switch to new write function that does range/chunk compression using zlib
@
text
@d305 1
d330 2
a331 1
	pmap_kenter_pa(HIBERNATE_ZLIB_SCRATCH, HIBERNATE_ZLIB_SCRATCH, VM_PROT_ALL);
d334 1
a334 1
	for(zlib_range = HIBERNATE_ZLIB_START; zlib < HIBERNATE_ZLIB_END;
d343 2
a344 2
		pmap_kenter_pa((vaddr_t)(HIBERNATE_CHUNK_TABLE+i),
			(paddr_t)(HIBERNATE_CHUNK_TABLE+i),
d353 1
a353 1
	chunks = (struct hibernate_disk_chunk *)HIBERNATE_CHUNK_TABLE;
@


1.9
log
@

Update hibernate_populate_resume_pt to include new ranges that need to be
mapped during resume
@
text
@d185 1
a185 2
 * Enter a 4MB PDE mapping for the supplied VA/PA
 * into the resume-time pmap
d198 1
a198 4
 * Enter a 4KB PTE mapping for the supplied VA/PA
 * into the resume-time pmap. This should only be
 * used to map the special pages and tramps below
 * 1MB phys
d211 1
a211 4
 * Enter a 4KB PDE mapping for the supplied VA
 * into the resume-time pmap. This should only be
 * used to map the special pages and tramps below
 * 1MB phys
d227 1
a227 1
 * page table and expect things to work properly.
d283 22
a304 1
	
d309 2
a310 2
	int i, j;
	paddr_t range_base, range_end, addr;
d312 3
d317 1
d321 5
a325 2
	pmap_kenter_pa(HIBERNATE_TEMP_PAGE, HIBERNATE_TEMP_PAGE, VM_PROT_ALL);	
	pmap_kenter_pa(HIBERNATE_ALLOC_PAGE, HIBERNATE_ALLOC_PAGE, VM_PROT_ALL);
d327 20
d349 3
d353 1
d355 2
a356 2
		range_base = hiber_info.ranges[i].base;
		range_end = hiber_info.ranges[i].end;
d358 63
a420 11
		for (j=0; j < (range_end - range_base);
		    blkctr += (NBPG/512), j += NBPG) {
			addr = range_base + j;
			pmap_kenter_pa(HIBERNATE_TEMP_PAGE, addr,
				VM_PROT_ALL);
			bcopy((caddr_t)HIBERNATE_TEMP_PAGE,
				(caddr_t)HIBERNATE_IO_PAGE,
				NBPG);
			hiber_info.io_func(hiber_info.device, blkctr,
				(vaddr_t)HIBERNATE_IO_PAGE, NBPG, 1,
				(void *)HIBERNATE_ALLOC_PAGE);
d422 42
d465 2
a466 2
	
	/* Image write complete, write the signature and return */	
@


1.8
log
@

Call (temporarily, until we have RLE page encoding) uvm_pmr_zero_everything
on suspend to ensure we get good zlib compression.

Add MI signature block (hibernate_info) comparison routine
@
text
@d55 1
a55 1
void	hibernate_populate_resume_pt(paddr_t *, paddr_t *);
d237 1
a237 1
hibernate_populate_resume_pt(paddr_t *image_start, paddr_t *image_end)
d239 2
a240 3
	int phys_page_number;
	paddr_t pa, pig_start, pig_end;
	psize_t pig_sz;
d243 2
a244 3
	/* Get the pig (largest contiguous physical range) from uvm */
	if (uvm_pmr_alloc_pig(&pig_start, pig_sz) == ENOMEM)
		panic("Insufficient memory for resume");
d246 7
a252 4
	*image_start = pig_start;
	*image_end = pig_end;

	bzero((caddr_t)HIBERNATE_PT_PAGE, PAGE_SIZE);
d255 1
a255 2
	 * Identity map first 4M physical for tramps and special utility 
	 * pages
d257 5
a261 1
	hibernate_enter_resume_mapping(0, 0, 1);
d280 4
a283 4
	phys_page_number = pig_start / NBPD;
	pig_start &= ~(PAGE_MASK_4M);
	pig_end &= ~(PAGE_MASK_4M);
	for (page = pig_start; page <= pig_end ;
d336 1
a336 1
	paddr_t range_base, range_end, addr, image_start, image_end;
a347 3
	/* Prepare the resume-time pagetable */
	hibernate_populate_resume_pt(&image_start, &image_end);

d412 1
a412 1
	paddr_t image_start, image_end;
a453 7

	/*
	 * Create the resume-time page table (ahead of when we actually
	 * need it)
	 */
	hibernate_populate_resume_pt(&image_start, &image_end);
	
@


1.7
log
@

Extract hibernate_write_signature and hibernate_clear_signature to the MI
hibernate code, and add chunk range overlap checking.
@
text
@d375 1
a375 1
	 * to write the image.
d377 2
@


1.6
log
@

Extract MI pmap function hibernate_enter_resume_mapping, refactor old i386
resume pmap code to match.

Add hibernate deflater and inflater and cache flush routines.

Code is not presently called or automatically built.
@
text
@a56 3
int	hibernate_write_signature(void);
int	hibernate_clear_signature(void);
int	hibernate_inflate_skip(paddr_t);
d324 1
a324 1
	return hibernate_write_signature();
a486 45
}

int
hibernate_write_signature()
{
	union hibernate_info hiber_info;
	u_int8_t *io_page;

	/* Get current running machine's hibernate info */
	if (get_hibernate_info(&hiber_info))
		return (1);

	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return (1);
	
	/* Write hibernate info to disk */
	hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
		(vaddr_t)&hiber_info, 512, 1, io_page);

	free(io_page, M_DEVBUF);

	return (0);
}

int
hibernate_clear_signature()
{
	union hibernate_info hiber_info;
	u_int8_t *io_page;

	/* Zero out a blank hiber_info */
	bzero(&hiber_info, sizeof(hiber_info));

	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return (1);
	
	/* Write (zeroed) hibernate info to disk */
	hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
		(vaddr_t)&hiber_info, 512, 1, io_page);

	free(io_page, M_DEVBUF);

	return (0);
@


1.5
log
@

Add zlib reset, alloc, and free functions for hibernate image compression
@
text
@d52 3
a54 1
void	hibernate_enter_resume_pte(vaddr_t, paddr_t);
d59 1
d67 1
d170 36
a205 2
 * Enter a 4MB PTE mapping for the supplied VA/PA
 * into the resume-time page table.
d208 1
a208 1
hibernate_enter_resume_pte(vaddr_t va, paddr_t pa)
d212 2
a213 2
	pte = s4pde_4m(va);
	npte = (pa & PMAP_PA_MASK_4M) | PG_RW | PG_V | PG_U | PG_M | PG_PS;
d218 16
d260 1
a260 1
	hibernate_enter_resume_pte(0, 0);	
d273 1
a273 1
		hibernate_enter_resume_pte(page, pa);
d286 1
a286 1
		hibernate_enter_resume_pte(page, pa);
d400 1
a400 1
			hibernate_enter_resume_pte(base, base);
d536 30
@


1.4
log
@

Separate some MD and MI bits and a bit of refactoring to make subsequent
commits easier.

Work in progress, hibernate will still not work for you.

ok deraadt@@
@
text
@d57 1
d68 1
d70 3
d75 7
a81 1
 * i386 MD Hibernate functions
d83 23
@


1.3
log
@Add signature read/write functions for i386 hibernate.

Also fix a missing parenthesis in the previous commit.
@
text
@d23 1
d27 2
d33 1
d44 1
a51 2
void	*get_hibernate_io_function(void);
int	get_hibernate_info(struct hibernate_info *);
d54 1
d57 1
a57 1
struct 	hibernate_info *global_hiber_info;
d73 6
d82 1
a82 2

#if NWD > 0 
d93 5
d99 1
a99 1
get_hibernate_info(struct hibernate_info *hiber_info)
a101 10
	struct disklabel dl;
	char err_string[128], *dl_ret;

	/* Determine I/O function to use */
	hiber_info->io_func = get_hibernate_io_function();
	if (hiber_info->io_func == NULL)
		return (0);

	/* Calculate hibernate device */
	hiber_info->device = swdevt[0].sw_dev;
d109 3
a111 4
		hiber_info->ranges[i].end = 
			(dumpmem[i].end * PAGE_SIZE);
		hiber_info->image_size +=
			hiber_info->ranges[i].end - hiber_info->ranges[i].base;
d128 1
a128 23
	/* Read disklabel (used to calculate signature and image offsets */
	dl_ret = disk_readlabel(&dl, hiber_info->device, err_string, 128);

	if (dl_ret) {
		printf("Hibernate error: %s\n", dl_ret);
		return (0);
	}

	/* Calculate signature block offset in swap */
	hiber_info->sig_offset = DL_BLKTOSEC(&dl, 
					(dl.d_partitions[1].p_size - 1)) * 
					DL_BLKSPERSEC(&dl);

	/* Calculate memory image offset in swap */
	hiber_info->image_offset = dl.d_partitions[1].p_offset +
				   dl.d_partitions[1].p_size -
				   (hiber_info->image_size / 512) -1;

	/* Stash kernel version information */
	bcopy(version, &hiber_info->kernel_version, 
		min(strlen(version), sizeof(hiber_info->kernel_version)));

	return (1);
d140 1
a140 1
	pte = s4pte_4m(va);
d160 1
a160 1
	if (uvm_pmr_alloc_pig(&pig_start, &pig_sz) == ENOMEM)
d205 1
a205 1
	struct hibernate_info hiber_info;
d211 2
a212 2
	if (!get_hibernate_info(&hiber_info))
		return (0);
d245 1
a245 1
	struct hibernate_info hiber_info;
d251 2
a252 2
	if (!get_hibernate_info(&hiber_info))
		return (0);
d300 1
a300 1
	struct hibernate_info *hiber_info = global_hiber_info;
d321 1
a321 1
	struct hibernate_info hiber_info, disk_hiber_info;
d327 1
a327 1
	if (!get_hibernate_info(&hiber_info))
d342 1
a342 1
	    sizeof(struct hibernate_info)) !=0) {
d386 1
a386 1
	if (!hibernate_read_image())
d407 1
a407 1
	struct hibernate_info hiber_info;
d411 2
a412 2
	if (!get_hibernate_info(&hiber_info))
		return (0);
d416 1
a416 1
		return (0);
d424 1
a424 1
	return (1);
d430 1
a430 1
	struct hibernate_info hiber_info;
d438 1
a438 1
		return (0);
d446 1
a446 1
	return (1);
@


1.2
log
@Fix a few minor issues in i386 hibernate support code relating to
improper swap dev determination and memory range calculation. Also
fix a knf issue.
@
text
@d51 2
d134 3
a136 3
	hiber_info->sig_offset = DL_BLKTOSEC(&dl,
				 	dl.d_partitions[1].p_size - 1)) *
				 	DL_BLKSPERSEC(&dl);
d256 3
a258 2
		
	return (1);
d299 3
a301 2
	
	return (1);
d421 45
@


1.1
log
@Preliminary plumbing code for i386 hibernate (suspend-to-disk).
This code is not yet called as there are still some important parts
not completed.

ok deraadt@@, kettenis@@ "looks reasonable"
@
text
@d73 1
a73 1
	if (strcmp(findblkname(major(swapdev)), "wd") == 0)
d102 1
a102 1
		hiber_info->ranges[i].base = dumpmem[i].start;
d104 1
a104 1
			(dumpmem[i].start + dumpmem[i].end * PAGE_SIZE);
d132 3
a134 2
	hiber_info->sig_offset = DL_BLKTOSEC(&dl, (dl.d_partitions[1].p_size - 1)) *
				 DL_BLKSPERSEC(&dl);
d233 1
d241 1
a241 1
		for (j=0; j < (range_end - range_base)/NBPG;
@

