head	1.39;
access;
symbols
	SMP_SYNC_A:1.39
	SMP_SYNC_B:1.39
	UBC_SYNC_A:1.39
	UBC_SYNC_B:1.39
	OPENBSD_2_9:1.37.0.4
	OPENBSD_2_9_BASE:1.37
	SMP:1.37.0.2;
locks; strict;
comment	@ * @;


1.39
date	2001.05.05.23.25.40;	author art;	state dead;
branches;
next	1.38;

1.38
date	2001.05.05.21.26.36;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2001.03.22.20.44.59;	author niklas;	state Exp;
branches
	1.37.2.1;
next	1.36;

1.36
date	2000.05.04.18.55.06;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	2000.02.22.19.27.48;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2000.01.29.21.41.51;	author mickey;	state Exp;
branches
	1.34.2.1;
next	1.33;

1.33
date	99.11.30.06.44.51;	author art;	state Exp;
branches;
next	1.32;

1.32
date	99.09.03.18.00.51;	author art;	state Exp;
branches;
next	1.31;

1.31
date	99.08.25.09.13.53;	author ho;	state Exp;
branches;
next	1.30;

1.30
date	99.07.02.12.25.50;	author niklas;	state Exp;
branches;
next	1.29;

1.29
date	99.06.04.16.37.48;	author mickey;	state Exp;
branches;
next	1.28;

1.28
date	99.06.01.06.37.22;	author art;	state Exp;
branches;
next	1.27;

1.27
date	99.02.26.10.37.51;	author art;	state Exp;
branches;
next	1.26;

1.26
date	99.02.26.10.26.57;	author art;	state Exp;
branches;
next	1.25;

1.25
date	98.04.25.20.31.30;	author mickey;	state Exp;
branches;
next	1.24;

1.24
date	98.03.20.15.40.32;	author niklas;	state Exp;
branches;
next	1.23;

1.23
date	98.01.20.18.40.15;	author niklas;	state Exp;
branches;
next	1.22;

1.22
date	97.10.25.06.57.59;	author niklas;	state Exp;
branches;
next	1.21;

1.21
date	97.10.24.22.15.07;	author mickey;	state Exp;
branches;
next	1.20;

1.20
date	97.09.24.22.28.15;	author niklas;	state Exp;
branches;
next	1.19;

1.19
date	97.09.22.21.03.54;	author niklas;	state Exp;
branches;
next	1.18;

1.18
date	97.08.17.17.31.37;	author grr;	state Exp;
branches;
next	1.17;

1.17
date	97.07.28.23.46.10;	author mickey;	state Exp;
branches;
next	1.16;

1.16
date	97.01.07.05.37.32;	author tholo;	state Exp;
branches;
next	1.15;

1.15
date	96.10.25.11.14.13;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	96.10.18.15.33.08;	author mickey;	state Exp;
branches;
next	1.13;

1.13
date	96.09.26.14.04.27;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	96.06.16.10.24.19;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	96.06.02.10.44.21;	author mickey;	state Exp;
branches;
next	1.10;

1.10
date	96.05.30.09.30.08;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	96.05.07.07.21.51;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	96.04.21.22.16.33;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	96.04.18.04.04.54;	author mickey;	state Exp;
branches;
next	1.6;

1.6
date	96.04.17.05.18.55;	author mickey;	state Exp;
branches;
next	1.5;

1.5
date	96.03.19.21.09.22;	author mickey;	state Exp;
branches;
next	1.4;

1.4
date	96.02.17.22.32.31;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	95.11.28.16.43.47;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.11.13.04.53.26;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.50.34;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.50.34;	author deraadt;	state Exp;
branches;
next	;

1.34.2.1
date	2000.03.02.07.04.28;	author niklas;	state Exp;
branches;
next	;

1.37.2.1
date	2001.04.18.16.07.22;	author niklas;	state Exp;
branches;
next	1.37.2.2;

1.37.2.2
date	2001.07.04.10.16.40;	author niklas;	state dead;
branches;
next	;


desc
@@


1.39
log
@PMAP_NEW and UVM are no longer optional on i386.
@
text
@/*	$OpenBSD: pmap.old.c,v 1.38 2001/05/05 21:26:36 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.36 1996/05/03 19:42:22 christos Exp $	*/

/*
 * Copyright (c) 1993, 1994, 1995 Charles M. Hannum.  All rights reserved.
 * Copyright (c) 1991 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department and William Jolitz of UUNET Technologies Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	7.7 (Berkeley)	5/12/91
 */

/*
 * Derived originally from an old hp300 version by Mike Hibler.  The version
 * by William Jolitz has been heavily modified to allow non-contiguous
 * mapping of physical memory by Wolfgang Solfrank, and to fix several bugs
 * and greatly speedup it up by Charles Hannum.
 * 
 * A recursive map [a pde which points to the page directory] is used to map
 * the page tables using the pagetables themselves. This is done to reduce
 * the impact on kernel virtual memory for lots of sparse address space, and
 * to reduce the cost of memory to each process.
 */

/*
 *	Manages physical address maps.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/user.h>

#include <vm/vm.h>
#include <vm/vm_kern.h>
#include <vm/vm_page.h>

#if defined(UVM)
#include <uvm/uvm.h>
#endif

#include <machine/cpu.h>

#include <dev/isa/isareg.h>
#include <stand/boot/bootarg.h>
#include <i386/isa/isa_machdep.h>

#include "isa.h"
#include "isadma.h"

/*
 * Allocate various and sundry SYSMAPs used in the days of old VM
 * and not yet converted.  XXX.
 */
#define	BSDVM_COMPAT	1

#ifdef DEBUG
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
} enter_stats;
struct {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
} remove_stats;

int pmapdebug = 0 /* 0xffff */;
#define	PDB_FOLLOW	0x0001
#define	PDB_INIT	0x0002
#define	PDB_ENTER	0x0004
#define	PDB_REMOVE	0x0008
#define	PDB_CREATE	0x0010
#define	PDB_PTPAGE	0x0020
#define	PDB_CACHE	0x0040
#define	PDB_BITS	0x0080
#define	PDB_COLLECT	0x0100
#define	PDB_PROTECT	0x0200
#define	PDB_PDRTAB	0x0400
#define	PDB_PARANOIA	0x2000
#define	PDB_WIRING	0x4000
#define	PDB_PVDUMP	0x8000
#endif

/*
 * Get PDEs and PTEs for user/kernel address space
 */
#define	pmap_pde(m, v)	(&((m)->pm_pdir[((vm_offset_t)(v) >> PDSHIFT)&1023]))

/*
 * Empty PTEs and PDEs are always 0, but checking only the valid bit allows
 * the compiler to generate `testb' rather than `testl'.
 */
#define	pmap_pde_v(pde)			(*(pde) & PG_V)
#define	pmap_pte_pa(pte)		(*(pte) & PG_FRAME)
#define	pmap_pte_w(pte)			(*(pte) & PG_W)
#define	pmap_pte_m(pte)			(*(pte) & PG_M)
#define	pmap_pte_u(pte)			(*(pte) & PG_U)
#define	pmap_pte_v(pte)			(*(pte) & PG_V)
#define	pmap_pte_set_w(pte, v)		((v) ? (*(pte) |= PG_W) : (*(pte) &= ~PG_W))
#define	pmap_pte_set_prot(pte, v)	((*(pte) &= ~PG_PROT), (*(pte) |= (v)))

/*
 * Given a map and a machine independent protection code,
 * convert to a vax protection code.
 */
pt_entry_t	protection_codes[8];

struct pmap	kernel_pmap_store;

vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
int		npages;

boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int		pv_nfree;

pt_entry_t *pmap_pte __P((pmap_t, vm_offset_t));
struct pv_entry * pmap_alloc_pv __P((void));
void pmap_free_pv __P((struct pv_entry *));
void i386_protection_init __P((void));
void pmap_collect_pv __P((void));
__inline void pmap_remove_pv __P((pmap_t, vm_offset_t, struct pv_entry *));
__inline void pmap_enter_pv __P((pmap_t, vm_offset_t, struct pv_entry *));
void pmap_remove_all __P((vm_offset_t));
void pads __P((pmap_t pm));
void pmap_dump_pvlist __P((vm_offset_t phys, char *m));
void pmap_pvdump __P((vm_offset_t pa));

#if BSDVM_COMPAT
#include <sys/msgbuf.h>

/*
 * All those kernel PT submaps that BSD is so fond of
 */
pt_entry_t	*CMAP1, *CMAP2, *XXX_mmap;
caddr_t		CADDR1, CADDR2, vmmap;
pt_entry_t	*msgbufmap, *bootargmap;
#endif	/* BSDVM_COMPAT */

/*
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, and allocate the system page table.
 *
 *	On the I386 this is called after mapping has already been enabled
 *	and just syncs the pmap module with what has already been done.
 *	[We can't call it easily with mapping off since the kernel is not
 *	mapped with PA == VA, hence we would have to relocate every address
 *	from the linked base (virtual) address to the actual (physical)
 *	address starting relative to 0]
 */

void
pmap_bootstrap(virtual_start)
	vm_offset_t virtual_start;
{
#if BSDVM_COMPAT
	vm_offset_t va;
	pt_entry_t *pte;
#endif

	/* Register the page size with the vm system */
#if defined(UVM)
	uvm_setpagesize();
#else
	vm_set_page_size();
#endif

	virtual_avail = virtual_start;
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	/*
	 * Initialize protection array.
	 */
	i386_protection_init();

#ifdef notdef
	/*
	 * Create Kernel page directory table and page maps.
	 * [ currently done in locore. i have wild and crazy ideas -wfj ]
	 */
	bzero(firstaddr, (1+NKPDE)*NBPG);
	pmap_kernel()->pm_pdir = firstaddr + VM_MIN_KERNEL_ADDRESS;
	pmap_kernel()->pm_ptab = firstaddr + VM_MIN_KERNEL_ADDRESS + NBPG;

	firstaddr += NBPG;
	for (x = i386_btod(VM_MIN_KERNEL_ADDRESS);
	     x < i386_btod(VM_MIN_KERNEL_ADDRESS) + NKPDE; x++) {
		pd_entry_t *pde;
		pde = pmap_kernel()->pm_pdir + x;
		*pde = (firstaddr + x*NBPG) | PG_V | PG_KW;
	}
#else
	pmap_kernel()->pm_pdir =
	    (pd_entry_t *)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
#endif

	simple_lock_init(&pmap_kernel()->pm_lock);
	pmap_kernel()->pm_count = 1;

#if BSDVM_COMPAT
	/*
	 * Allocate all the submaps we need
	 */
#define	SYSMAP(c, p, v, n)	\
	v = (c)va; va += ((n)*NBPG); p = pte; pte += (n);

	va = virtual_avail;
	pte = pmap_pte(pmap_kernel(), va);

	SYSMAP(caddr_t		,CMAP1		,CADDR1	   ,1		)
	SYSMAP(caddr_t		,CMAP2		,CADDR2	   ,1		)
	SYSMAP(caddr_t		,XXX_mmap	,vmmap	   ,1		)
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,btoc(MSGBUFSIZE))
	SYSMAP(bootarg_t *	,bootargmap	,bootargp  ,btoc(bootargc))
	virtual_avail = va;
#endif

	/*
	 * Reserve pmap space for mapping physical pages during dump.
	 */
	virtual_avail = reserve_dumppages(virtual_avail);

	/* flawed, no mappings?? */
	if (ctob(physmem) > 31*1024*1024 && MAXKPDE != NKPDE) {
		vm_offset_t p;
		int i;

		p = virtual_avail;
		virtual_avail += (MAXKPDE-NKPDE+1) * NBPG;
		bzero((void *)p, (MAXKPDE-NKPDE+1) * NBPG);
		p = round_page(p);
		for (i = NKPDE; i < MAXKPDE; i++, p += NBPG)
			PTD[KPTDI+i] = (pd_entry_t)p |
			    PG_V | PG_KW;
	}
}

void
pmap_virtual_space(startp, endp)
	vm_offset_t *startp;
	vm_offset_t *endp;
{
	*startp = virtual_avail;
	*endp = virtual_end;
}

/*
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
 */
void
pmap_init()
{
	vm_offset_t addr;
	vm_size_t s;
	int lcv;

	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE != 1");

	npages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) 
		npages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
	s = (vm_size_t) (sizeof(struct pv_entry) * npages + npages);
	s = round_page(s);
#if defined(UVM)
	addr = (vm_offset_t) uvm_km_zalloc(kernel_map, s);
	if (addr == NULL)
		panic("pmap_init");
#else
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
#endif

	/* allocate pv_entry stuff first */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.pvent = (struct pv_entry *) addr;
		addr = (vm_offset_t)(vm_physmem[lcv].pmseg.pvent +
			(vm_physmem[lcv].end - vm_physmem[lcv].start));
	}
	/* allocate attrs next */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.attrs = (char *) addr;
		addr = (vm_offset_t)(vm_physmem[lcv].pmseg.attrs +
			(vm_physmem[lcv].end - vm_physmem[lcv].start));
	}
	TAILQ_INIT(&pv_page_freelist);

#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %lx bytes (%x pgs)\n",
		       s, npages);
#endif

	/*
	 * Now it is safe to enable pv_entry recording.
	 */
	pmap_initialized = TRUE;
}

struct pv_entry *
pmap_alloc_pv()
{
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;

	if (pv_nfree == 0) {
#if defined(UVM)
		/* NOTE: can't lock kernel_map here */
		MALLOC(pvp, struct pv_page *, NBPG, M_VMPVENT, M_WAITOK);
#else
		pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG);
#endif
		if (pvp == 0)
			panic("pmap_alloc_pv: kmem_alloc() failed");
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		}
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
	return pv;
}

void
pmap_free_pv(pv)
	struct pv_entry *pv;
{
	register struct pv_page *pvp;

	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
#if defined(UVM)
		FREE((vaddr_t) pvp, M_VMPVENT);
#else
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
#endif
		break;
	}
}

void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;
	int bank, off;

	TAILQ_INIT(&pv_page_collectlist);

	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp, pvp_pgi.pgi_list);
			pv_nfree -= pvp->pvp_pgi.pgi_nfree;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}

	if (pv_page_collectlist.tqh_first == 0)
		return;

	if ((bank = vm_physseg_find(atop(0), &off)) == -1) { 
		printf("INVALID PA!");
		return;
	}

	for (ph = &vm_physmem[bank].pmseg.pvent[off]; ph; ph = ph->pv_next) {
		if (ph->pv_pmap == 0)
			continue;
		s = splimp();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: pgi_nfree inconsistent");
#endif
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}

	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
#if defined(UVM)
		FREE((vaddr_t) pvp, M_VMPVENT);
#else
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
#endif
	}
}

__inline void
pmap_enter_pv(pmap, va, pv)
	register pmap_t pmap;
	vm_offset_t va;
	struct pv_entry *pv;
{	
	register struct pv_entry *npv;
	int s;

	if (!pmap_initialized)
		return;

#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter_pv: pv %x: %x/%x/%x\n",
		       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
	s = splimp();

	if (pv->pv_pmap == NULL) {
		/*
		 * No entries yet, use header as the first entry
		 */
#ifdef DEBUG
		enter_stats.firstpv++;
#endif
		pv->pv_va = va;
		pv->pv_pmap = pmap;
		pv->pv_next = NULL;
	} else {
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
#ifdef DEBUG
		for (npv = pv; npv; npv = npv->pv_next)
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				panic("pmap_enter_pv: already in pv_tab");
#endif
		npv = pmap_alloc_pv();
		npv->pv_va = va;
		npv->pv_pmap = pmap;
		npv->pv_next = pv->pv_next;
		pv->pv_next = npv;
#ifdef DEBUG
		if (!npv->pv_next)
			enter_stats.secondpv++;
#endif
	}
	splx(s);
}

__inline void
pmap_remove_pv(pmap, va, pv)
	register pmap_t pmap;
	vm_offset_t va;
	struct pv_entry *pv;
{	
	register struct pv_entry *npv;
	int s;

	/*
	 * Remove from the PV table (raise IPL since we
	 * may be called at interrupt time).
	 */
	s = splimp();

	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		npv = pv->pv_next;
		if (npv) {
			*pv = *npv;
			pmap_free_pv(npv);
		} else
			pv->pv_pmap = NULL;
	} else {
		for (npv = pv->pv_next; npv; pv = npv, npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
		}
		if (npv) {
			pv->pv_next = npv->pv_next;
			pmap_free_pv(npv);
		}
	}
	splx(s);
}

/*
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 */
vm_offset_t
pmap_map(va, spa, epa, prot)
	vm_offset_t va, spa, epa;
	int prot;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%x, %x, %x, %x)\n", va, spa, epa, prot);
#endif

	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE, 0);
		va += NBPG;
		spa += NBPG;
	}
	return va;
}

/*
 *	Create and return a physical map.
 *
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
 *
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
 *
 * [ just allocate a ptd and mark it uninitialize -- should we track
 *   with a table which process has which ptd? -wfj ]
 */
pmap_t
pmap_create(size)
	vm_size_t size;
{
	register pmap_t pmap;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create(%x)\n", size);
#endif

	/*
	 * Software use map does not need a pmap
	 */
	if (size)
		return NULL;

	pmap = (pmap_t) malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return pmap;
}

/*
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
 */
void
pmap_pinit(pmap)
	register struct pmap *pmap;
{

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_pinit(%x)\n", pmap);
#endif

	/*
	 * No need to allocate page table space yet but we do need a
	 * valid page directory table.
	 */
#if defined(UVM)
	pmap->pm_pdir = (pd_entry_t *) uvm_km_zalloc(kernel_map, NBPG);
#else
	pmap->pm_pdir = (pd_entry_t *) kmem_alloc(kernel_map, NBPG);
#endif

#ifdef DIAGNOSTIC
	if (pmap->pm_pdir == NULL)
		panic("pmap_pinit: alloc failed");
#endif
	/* wire in kernel global address entries */
	bcopy(&PTD[KPTDI], &pmap->pm_pdir[KPTDI], MAXKPDE *
	    sizeof(pd_entry_t));

	/* install self-referential address mapping entry */
	pmap->pm_pdir[PTDPTDI] = pmap_extract(pmap_kernel(),
	    (vm_offset_t)pmap->pm_pdir) | PG_V | PG_KW;

	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
}

/*
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
 */
void
pmap_destroy(pmap)
	register pmap_t pmap;
{
	int count;

	if (pmap == NULL)
		return;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%x)\n", pmap);
#endif

	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		free((caddr_t)pmap, M_VMPMAP);
	}
}

/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_release(pmap)
	register struct pmap *pmap;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_release(%x)\n", pmap);
#endif

#ifdef DIAGNOSTICx
	/* sometimes 1, sometimes 0; could rearrange pmap_destroy */
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif

#if defined(UVM)
	uvm_km_free(kernel_map, (vaddr_t)pmap->pm_pdir, NBPG);
#else
	kmem_free(kernel_map, (vm_offset_t)pmap->pm_pdir, NBPG);
#endif
}

/*
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap)
	pmap_t pmap;
{

	if (pmap == NULL)
		return;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%x)", pmap);
#endif

	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}

void
pmap_activate(p)
	struct proc *p;
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

	pcb->pcb_cr3 = pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_pdir);
	if (p == curproc)
		lcr3(pcb->pcb_cr3);
}

void
pmap_deactivate(p)
	struct proc *p;
{
}

/*
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_remove(pmap, sva, eva)
	struct pmap *pmap;
	register vm_offset_t sva, eva;
{
	register pt_entry_t *pte;
	vm_offset_t pa;
	int bank, off;
	int flush = 0;

	sva &= PG_FRAME;
	eva &= PG_FRAME;

	/*
	 * We need to acquire a pointer to a page table page before entering
	 * the following loop.
	 */
	while (sva < eva) {
		pte = pmap_pte(pmap, sva);
		if (pte)
			break;
		sva = (sva & PD_MASK) + NBPD;
	}

	while (sva < eva) {
		/* only check once in a while */
		if ((sva & PT_MASK) == 0) {
			if (!pmap_pde_v(pmap_pde(pmap, sva))) {
				/* We can race ahead here, to the next pde. */
				sva += NBPD;
				pte += i386_btop(NBPD);
				continue;
			}
		}

		pte = pmap_pte(pmap, sva);
		if (pte == NULL) {
			/* We can race ahead here, to the next pde. */
			sva = (sva & PD_MASK) + NBPD;
			continue;
		}

		if (!pmap_pte_v(pte)) {
#ifdef __GNUC__
			/*
			 * Scan ahead in a tight loop for the next used PTE in
			 * this page.  We don't scan the whole region here
			 * because we don't want to zero-fill unused page table
			 * pages.
			 */
			int n, m;

			n = min(eva - sva, NBPD - (sva & PT_MASK)) >> PGSHIFT;
			__asm __volatile(
			    "cld\n\trepe\n\tscasl\n\tje 1f\n\tincl %1\n\t1:"
			    : "=D" (pte), "=c" (m)
			    : "0" (pte), "1" (n), "a" (0));
			sva += (n - m) << PGSHIFT;
			if (!m)
				continue;
			/* Overshot. */
			--pte;
#else
			goto next;
#endif
		}

		flush = 1;

		/*
		 * Update statistics
		 */
		if (pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		pa = pmap_pte_pa(pte);

		/*
		 * Invalidate the PTEs.
		 * XXX: should cluster them up and invalidate as many
		 * as possible at once.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_REMOVE)
			printf("remove: inv pte at %x(%x) ", pte, *pte);
#endif

#ifdef needednotdone
reduce wiring count on page table pages as references drop
#endif

		if ((bank = vm_physseg_find(atop(pa), &off)) != -1) {
			vm_physmem[bank].pmseg.attrs[off] |=
				*pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, sva,
				&vm_physmem[bank].pmseg.pvent[off]);
		}

		*pte = 0;

#ifndef __GNUC__
	next:
#endif
		sva += NBPG;
		pte++;
	}

	if (flush)
		pmap_update();
}

/*
 *	Routine:	pmap_remove_all
 *	Function:
 *		Removes this physical page from
 *		all physical maps in which it resides.
 *		Reflects back modify bits to the pager.
 */
void
pmap_remove_all(pa)
	vm_offset_t pa;
{
	struct pv_entry *ph, *pv, *npv;
	register pmap_t pmap;
	register pt_entry_t *pte;
	int bank, off;
	int s;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_all(%x)", pa);
	/*pmap_pvdump(pa);*/
#endif

	bank = vm_physseg_find(atop(pa), &off);
	if (bank == -1)
		return;

	pv = ph = &vm_physmem[bank].pmseg.pvent[off];
	s = splimp();

	if (ph->pv_pmap == NULL) {
		splx(s);
		return;
	}

	while (pv) {
		pmap = pv->pv_pmap;
		pte = pmap_pte(pmap, pv->pv_va);

#ifdef DEBUG
		if (!pte || !pmap_pte_v(pte) || pmap_pte_pa(pte) != pa)
			panic("pmap_remove_all: bad mapping");
#endif

		/*
		 * Update statistics
		 */
		if (pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		/*
		 * Invalidate the PTEs.
		 * XXX: should cluster them up and invalidate as many
		 * as possible at once.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_REMOVE)
			printf("remove: inv pte at %x(%x) ", pte, *pte);
#endif

#ifdef needednotdone
reduce wiring count on page table pages as references drop
#endif

		/*
		 * Update saved attributes for managed page
		 */
		vm_physmem[bank].pmseg.attrs[off] |= *pte & (PG_M | PG_U);
		*pte = 0;

		npv = pv->pv_next;
		if (pv == ph)
			ph->pv_pmap = NULL;
		else
			pmap_free_pv(pv);
		pv = npv;
	}
	splx(s);

	pmap_update();
}

/*
 *	Set the physical protection on the
 *	specified range of this map as requested.
 */
void
pmap_protect(pmap, sva, eva, prot)
	register pmap_t pmap;
	vm_offset_t sva, eva;
	vm_prot_t prot;
{
	register pt_entry_t *pte;
	register int i386prot;
	int flush = 0;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%x, %x, %x, %x)", pmap, sva, eva, prot);
#endif

	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	if (prot & VM_PROT_WRITE)
		return;

	sva &= PG_FRAME;
	eva &= PG_FRAME;

	/*
	 * We need to acquire a pointer to a page table page before entering
	 * the following loop.
	 */
	while (sva < eva) {
		pte = pmap_pte(pmap, sva);
		if (pte)
			break;
		sva = (sva & PD_MASK) + NBPD;
	}

	while (sva < eva) {
		/* only check once in a while */
		if ((sva & PT_MASK) == 0) {
			if (!pmap_pde_v(pmap_pde(pmap, sva))) {
				/* We can race ahead here, to the next pde. */
				sva += NBPD;
				pte += i386_btop(NBPD);
				continue;
			}
		}

		if (!pmap_pte_v(pte)) {
#ifdef __GNUC__
			/*
			 * Scan ahead in a tight loop for the next used PTE in
			 * this page.  We don't scan the whole region here
			 * because we don't want to zero-fill unused page table
			 * pages.
			 */
			int n, m;

			n = min(eva - sva, NBPD - (sva & PT_MASK)) >> PGSHIFT;
			__asm __volatile(
			    "cld\n\trepe\n\tscasl\n\tje 1f\n\tincl %1\n\t1:"
			    : "=D" (pte), "=c" (m)
			    : "0" (pte), "1" (n), "a" (0));
			sva += (n - m) << PGSHIFT;
			if (!m)
				continue;
			/* Overshot. */
			--pte;
#else
			goto next;
#endif
		}

		flush = 1;

		i386prot = protection_codes[prot];
		if (sva < VM_MAXUSER_ADDRESS)	/* see also pmap_enter() */
			i386prot |= PG_u;
		else if (sva < VM_MAX_ADDRESS)
			i386prot |= PG_u | PG_RW;
		pmap_pte_set_prot(pte, i386prot);

#ifndef __GNUC__
	next:
#endif
		sva += NBPG;
		pte++;
	}

	if (flush)
		pmap_update();
}

/*
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte can not be reclaimed.
 *
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
	register pmap_t pmap;
	vm_offset_t va;
	register vm_offset_t pa;
	vm_prot_t prot;
	boolean_t wired;
	vm_prot_t access_type;
{
	register pt_entry_t *pte;
	register pt_entry_t npte;
	int bank, off;
	int flush = 0;
	boolean_t cacheable;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%x, %x, %x, %x, %x)", pmap, va, pa, prot,
		    wired);
#endif

	if (pmap == NULL)
		return;

	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter: too big");
	/* also, should not muck with PTD va! */

#ifdef DEBUG
	if (pmap == pmap_kernel())
		enter_stats.kernel++;
	else
		enter_stats.user++;
#endif

	pte = pmap_pte(pmap, va);
	if (!pte) {
		/*
		 * Page Directory table entry not valid, we need a new PT page
		 *
		 * we want to vm_fault in a new zero-filled PT page for our
		 * use.   in order to do this, we want to call vm_fault()
		 * with the VA of where we want to put the PTE.   but in
		 * order to call vm_fault() we need to know which vm_map
		 * we are faulting in.    in the m68k pmap's this is easy
		 * since all PT pages live in one global vm_map ("pt_map")
		 * and we have a lot of virtual space we can use for the
		 * pt_map (since the kernel doesn't have to share its 4GB
		 * address space with processes).    but in the i386 port
		 * the kernel must live in the top part of the virtual 
		 * address space and PT pages live in their process' vm_map
		 * rather than a global one.    the problem is that we have
		 * no way of knowing which vm_map is the correct one to 
		 * fault on.
		 * 
		 * XXX: see NetBSD PR#1834 and Mycroft's posting to 
		 *	tech-kern on 7 Jan 1996.
		 *
		 * rather than always calling panic, we try and make an 
		 * educated guess as to which vm_map to use by using curproc.
		 * this is a workaround and may not fully solve the problem?
	 	 */
		struct vm_map *vmap;
		int rv;
		vm_offset_t v;

		if (curproc == NULL || curproc->p_vmspace == NULL ||
		    pmap != curproc->p_vmspace->vm_map.pmap)
			panic("ptdi %x", pmap->pm_pdir[PTDPTDI]);

		/* our guess about the vm_map was good!  fault it in.  */

		vmap = &curproc->p_vmspace->vm_map;
		v = trunc_page((vaddr_t)vtopte(va));
#ifdef DEBUG
		printf("faulting in a pt page map %x va %x\n", vmap, v);
#endif
#if defined(UVM)
		rv = uvm_fault(vmap, v, 0, VM_PROT_READ|VM_PROT_WRITE);
#else
		rv = vm_fault(vmap, v, VM_PROT_READ|VM_PROT_WRITE, FALSE);
#endif
		if (rv != KERN_SUCCESS)
			panic("ptdi2 %x", pmap->pm_pdir[PTDPTDI]);
#if defined(UVM)
		/*
		 * XXX It is possible to get here from uvm_fault with vmap
		 * locked.  uvm_map_pageable requires it to be unlocked, so
		 * try to record the state of the lock, unlock it, and then
		 * after the call, reacquire the original lock.
		 * THIS IS A GROSS HACK!
		 */
		{
			int ls = lockstatus(&vmap->lock);

			if (ls)
				lockmgr(&vmap->lock, LK_RELEASE, (void *)0,
				    curproc);
			uvm_map_pageable(vmap, v, round_page(v+1), FALSE);
			if (ls)
				lockmgr(&vmap->lock, ls, (void *)0, curproc);
		}
#else
		vm_map_pageable(vmap, v, round_page(v+1), FALSE);
#endif
		pte = pmap_pte(pmap, va);
		if (!pte) 
			panic("ptdi3 %x", pmap->pm_pdir[PTDPTDI]);
	}
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: pte %x, *pte %x ", pte, *pte);
#endif

	if (pmap_pte_v(pte)) {
		register vm_offset_t opa;

		/*
		 * Check for wiring change and adjust statistics.
		 */
		if ((wired && !pmap_pte_w(pte)) ||
		    (!wired && pmap_pte_w(pte))) {
			/*
			 * We don't worry about wiring PT pages as they remain
			 * resident as long as there are valid mappings in them.
			 * Hence, if a user page is wired, the PT page will be also.
			 */
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("enter: wiring change -> %x ", wired);
#endif
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
#ifdef DEBUG
			enter_stats.wchange++;
#endif
		}

		flush = 1;
		opa = pmap_pte_pa(pte);

		/*
		 * Mapping has not changed, must be protection or wiring change.
		 */
		if (opa == pa) {
#ifdef DEBUG
			enter_stats.pwchange++;
#endif
			goto validate;
		}
		
		/*
		 * Mapping has changed, invalidate old range and fall through to
		 * handle validating new mapping.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: removing old mapping %x pa %x ", va, opa);
#endif
		if ((bank = vm_physseg_find(atop(opa), &off)) != -1) {
			vm_physmem[bank].pmseg.attrs[off] |=
				*pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, va,
				&vm_physmem[bank].pmseg.pvent[off]);
		}
#ifdef DEBUG
		enter_stats.mchange++;
#endif
	} else {
		/*
		 * Increment counters
		 */
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
	}

	/*
	 * Enter on the PV list if part of our managed memory
	 */
	if ((bank = vm_physseg_find(atop(pa), &off)) != -1) {
#ifdef DEBUG     
		enter_stats.managed++;
#endif
		pmap_enter_pv(pmap, va, &vm_physmem[bank].pmseg.pvent[off]);
		cacheable = TRUE;
	} else if (pmap_initialized) {
#ifdef DEBUG
		enter_stats.unmanaged++;
#endif
		/*
		 * Assumption: if it is not part of our managed memory
		 * then it must be device memory which may be volatile.
		 */
		cacheable = FALSE;
	}

validate:
	/*
	 * Now validate mapping with desired protection/wiring.
	 * Assume uniform modified and referenced status for all
	 * I386 pages in a MACH page.
	 */
	npte = (pa & PG_FRAME) | protection_codes[prot] | PG_V;
	if (wired)
		npte |= PG_W;

	if (va < VM_MAXUSER_ADDRESS)	/* i.e. below USRSTACK */
		npte |= PG_u;
	else if (va < VM_MAX_ADDRESS)
		/*
		 * Page tables need to be user RW, for some reason, and the
		 * user area must be writable too.  Anything above
		 * VM_MAXUSER_ADDRESS is protected from user access by
		 * the user data and code segment descriptors, so this is OK.
		 */
		npte |= PG_u | PG_RW;

#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: new pte value %x ", npte);
#endif

	*pte = npte;
	if (flush)
		pmap_update();
}

/*
 *      pmap_page_protect:
 *
 *      Lower the permission for all mappings to a given page.
 */
void
pmap_page_protect(phys, prot)
	vm_offset_t     phys;
	vm_prot_t       prot;
{

	switch (prot) {
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_copy_on_write(phys);
		break;
	case VM_PROT_ALL:
		break;
	default:
		pmap_remove_all(phys);
		break;
	}
}

/*
 *	Routine:	pmap_change_wiring
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 */
void
pmap_change_wiring(pmap, va, wired)
	register pmap_t pmap;
	vm_offset_t va;
	boolean_t wired;
{
	register pt_entry_t *pte;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_change_wiring(%x, %x, %x)", pmap, va, wired);
#endif

	pte = pmap_pte(pmap, va);
	if (!pte)
		return;

#ifdef DEBUG
	/*
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
	 */
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_change_wiring: invalid PTE for %x ", va);
	}
#endif

	if ((wired && !pmap_pte_w(pte)) || (!wired && pmap_pte_w(pte))) {
		if (wired)
			pmap->pm_stats.wired_count++;
		else
			pmap->pm_stats.wired_count--;
		pmap_pte_set_w(pte, wired);
	}
}

/*
 *	Routine:	pmap_pte
 *	Function:
 *		Extract the page table entry associated
 *		with the given map/virtual_address pair.
 */
pt_entry_t *
pmap_pte(pmap, va)
	register pmap_t pmap;
	vm_offset_t va;
{
	pt_entry_t *ptp;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pte(%x, %x) ->\n", pmap, va);
#endif

	if (!pmap || !pmap_pde_v(pmap_pde(pmap, va)))
		return NULL;

	if ((pmap->pm_pdir[PTDPTDI] & PG_FRAME) == (PTDpde & PG_FRAME) ||
	    pmap == pmap_kernel())
		/* current address space or kernel */
		ptp = PTmap;
	else {
		/* alternate address space */
		if ((pmap->pm_pdir[PTDPTDI] & PG_FRAME) != (APTDpde & PG_FRAME)) {
			APTDpde = pmap->pm_pdir[PTDPTDI];
			pmap_update();
		}
		ptp = APTmap;
	}

	return ptp + i386_btop(va);
}

/*
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
 */
vm_offset_t
pmap_extract(pmap, va)
	register pmap_t pmap;
	vm_offset_t va;
{
	register pt_entry_t *pte;
	register vm_offset_t pa;

#ifdef DEBUGx
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%x, %x) -> ", pmap, va);
#endif

	pte = pmap_pte(pmap, va);
	if (!pte)
		return NULL;
	if (!pmap_pte_v(pte))
		return NULL;

	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("%x\n", pa);
#endif
	return pa | (va & ~PG_FRAME);
}

/*
 *	Copy the range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t dst_pmap, src_pmap;
	vm_offset_t dst_addr, src_addr;
	vm_size_t len;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%x, %x, %x, %x, %x)",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
#endif
}

/*
 *	Routine:	pmap_collect
 *	Function:
 *		Garbage collects the physical map system for
 *		pages which are no longer used.
 *		Success need not be guaranteed -- that is, there
 *		may well be pages which are not referenced, but
 *		others may be collected.
 *	Usage:
 *		Called by the pageout daemon when pages are scarce.
 * [ needs to be written -wfj ]  XXXX
 */
void
pmap_collect(pmap)
	pmap_t pmap;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%x) ", pmap);
#endif

	if (pmap != pmap_kernel())
		return;

}

#if DEBUG
void
pmap_dump_pvlist(phys, m)
	vm_offset_t phys;
	char *m;
{
	register struct pv_entry *pv;
	int bank, off;

	if (!(pmapdebug & PDB_PARANOIA))
		return;

	if (!pmap_initialized)
		return;
	printf("%s %08x:", m, phys);
	bank = vm_physseg_find(atop(phys), &off);
	pv = &vm_physmem[bank].pmseg.pvent[off];
	if (pv->pv_pmap == NULL) {
		printf(" no mappings\n");
		return;
	}
	for (; pv; pv = pv->pv_next)
		printf(" pmap %08x va %08x", pv->pv_pmap, pv->pv_va);
	printf("\n");
}
#else
#define	pmap_dump_pvlist(a,b)
#endif

/*
 *	pmap_zero_page zeros the specified by mapping it into
 *	virtual memory and using bzero to clear its contents.
 */
void
pmap_zero_page(phys)
	register vm_offset_t phys;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%x)", phys);
#endif

	pmap_dump_pvlist(phys, "pmap_zero_page: phys");
	*CMAP2 = (phys & PG_FRAME) | PG_V | PG_KW /*| PG_N*/;
	pmap_update();
	bzero(CADDR2, NBPG);
}

/*
 *	pmap_copy_page copies the specified page by mapping
 *	it into virtual memory and using bcopy to copy its
 *	contents.
 */
void
pmap_copy_page(src, dst)
	register vm_offset_t src, dst;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%x, %x)", src, dst);
#endif

	pmap_dump_pvlist(src, "pmap_copy_page: src");
	pmap_dump_pvlist(dst, "pmap_copy_page: dst");
	*CMAP1 = (src & PG_FRAME) | PG_V | PG_KR;
	*CMAP2 = (dst & PG_FRAME) | PG_V | PG_KW /*| PG_N*/;
	pmap_update();
	bcopy(CADDR1, CADDR2, NBPG);
}

/*
 *	Routine:	pmap_pageable
 *	Function:
 *		Make the specified pages (by pmap, offset)
 *		pageable (or not) as requested.
 *
 *		A page which is not pageable may not take
 *		a fault; therefore, its page table entry
 *		must remain valid for the duration.
 *
 *		This routine is merely advisory; pmap_enter
 *		will specify that these pages are to be wired
 *		down (or not) as appropriate.
 */

void
pmap_pageable(pmap, sva, eva, pageable)
	pmap_t pmap;
	vm_offset_t sva, eva;
	boolean_t pageable;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pageable(%x, %x, %x, %x)",
		       pmap, sva, eva, pageable);
#endif

	/*
	 * If we are making a PT page pageable then all valid
	 * mappings must be gone from that page.  Hence it should
	 * be all zeros and there is no need to clean it.
	 * Assumption:
	 *	- PT pages have only one pv_table entry
	 *	- PT pages are the only single-page allocations
	 *	  between the user stack and kernel va's 
	 * See also pmap_enter & pmap_protect for rehashes of this...
	 */

	if (pageable &&
	    pmap == pmap_kernel() &&
	    sva >= VM_MAXUSER_ADDRESS && eva <= VM_MAX_ADDRESS &&
	    eva - sva == NBPG) {
		register vm_offset_t pa;
		register pt_entry_t *pte;
#ifdef DIAGNOSTIC
		int bank, off;
		register struct pv_entry *pv;
#endif

#ifdef DEBUG
		if ((pmapdebug & (PDB_FOLLOW|PDB_PTPAGE)) == PDB_PTPAGE)
			printf("pmap_pageable(%x, %x, %x, %x)",
			       pmap, sva, eva, pageable);
#endif

		pte = pmap_pte(pmap, sva);
		if (!pte)
			return;
		if (!pmap_pte_v(pte))
			return;

		pa = pmap_pte_pa(pte);

#ifdef DIAGNOSTIC
		if ((*pte & (PG_u | PG_RW)) != (PG_u | PG_RW))
			printf("pmap_pageable: unexpected pte=%x va %x\n",
				*pte, sva);
		if ((bank = vm_physseg_find(atop(pa), &off)) == -1)
			return;
		pv = &vm_physmem[bank].pmseg.pvent[off];
		if (pv->pv_va != sva || pv->pv_next) {
			printf("pmap_pageable: bad PT page va %x next %x\n",
			       pv->pv_va, pv->pv_next);
			return;
		}
#endif

		/*
		 * Mark it unmodified to avoid pageout
		 */
		pmap_clear_modify(pa);

#ifdef needsomethinglikethis
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_pageable: PT page %x(%x) unmodified\n",
			       sva, *pmap_pte(pmap, sva));
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("pageable", sva);
#endif
	}
}

/*
 * Miscellaneous support routines follow
 */
void
i386_protection_init()
{

	protection_codes[VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE] = 0;
	protection_codes[VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE] =
	protection_codes[VM_PROT_NONE | VM_PROT_READ | VM_PROT_NONE] =
	protection_codes[VM_PROT_NONE | VM_PROT_READ | VM_PROT_EXECUTE] = PG_RO;
	protection_codes[VM_PROT_WRITE | VM_PROT_NONE | VM_PROT_NONE] =
	protection_codes[VM_PROT_WRITE | VM_PROT_NONE | VM_PROT_EXECUTE] =
	protection_codes[VM_PROT_WRITE | VM_PROT_READ | VM_PROT_NONE] =
	protection_codes[VM_PROT_WRITE | VM_PROT_READ | VM_PROT_EXECUTE] = PG_RW;
}

boolean_t
pmap_testbit(pa, setbits)
	register vm_offset_t pa;
	int setbits;
{
	register struct pv_entry *pv;
	register pt_entry_t *pte;
	int s;
	int bank, off;

	if ((bank = vm_physseg_find(atop(pa), &off)) == -1)
		return FALSE;
	pv = &vm_physmem[bank].pmseg.pvent[off];
	s = splimp();

	/*
	 * Check saved info first
	 */
	if (vm_physmem[bank].pmseg.attrs[off] & setbits) {
		splx(s);
		return TRUE;
	}

	/*
	 * Not found, check current mappings returning
	 * immediately if found.
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
			if (*pte & setbits) {
				splx(s);
				return TRUE;
			}
		}
	}
	splx(s);
	return FALSE;
}

/*
 * Modify pte bits for all ptes corresponding to the given physical address.
 * We use `maskbits' rather than `clearbits' because we're always passing
 * constants and the latter would require an extra inversion at run-time.
 */
void
pmap_changebit(pa, setbits, maskbits)
	register vm_offset_t pa;
	int setbits, maskbits;
{
	register struct pv_entry *pv;
	register pt_entry_t *pte;
	vm_offset_t va;
	int s;
	int bank, off;

#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(%x, %x, %x)",
		       pa, setbits, ~maskbits);
#endif

	if ((bank = vm_physseg_find(atop(pa), &off)) == -1)
		return;
	pv = &vm_physmem[bank].pmseg.pvent[off];
	s = splimp();

	/*
	 * Clear saved attributes (modify, reference)
	 */
	if (~maskbits)
		vm_physmem[bank].pmseg.attrs[off] &= maskbits;

	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			va = pv->pv_va;

			/*
			 * XXX don't write protect pager mappings
			 */
			if ((PG_RO && setbits == PG_RO) ||
			    (PG_RW && maskbits == ~PG_RW)) {
#if defined(UVM)
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
					continue;
#else
				extern vm_offset_t pager_sva, pager_eva;

				if (va >= pager_sva && va < pager_eva)
					continue;
#endif
			}

			pte = pmap_pte(pv->pv_pmap, va);
			*pte = (*pte & maskbits) | setbits;
		}
		pmap_update();
	}
	splx(s);
}

void
pmap_prefault(map, v, l)
	vm_map_t map;
	vm_offset_t v;
	vm_size_t l;
{
	vm_offset_t pv, pv2;

	for (pv = v; pv < v + l ; pv += ~PD_MASK + 1) {
		if (!pmap_pde_v(pmap_pde(map->pmap, pv))) {
			pv2 = trunc_page((vaddr_t)vtopte(pv));
#if defined(UVM)
			uvm_fault(map, pv2, 0, VM_PROT_READ);
#else
			vm_fault(map, pv2, VM_PROT_READ, FALSE);
#endif
		}
		pv &= PD_MASK;
	}
}

#ifdef DEBUG
void
pmap_pvdump(pa)
	vm_offset_t pa;
{
	register struct pv_entry *pv;
	int bank, off;

	printf("pa %x", pa);
	if ((bank = vm_physseg_find(atop(pa), &off)) == -1) {
		printf("INVALID PA!");
	} else {
		for (pv = &vm_physmem[bank].pmseg.pvent[off] ; pv ;
		     pv = pv->pv_next) {
			printf(" -> pmap %p, va %lx", pv->pv_pmap, pv->pv_va);
			       pads(pv->pv_pmap);
		}
	}
	printf(" ");
}

#ifdef notyet
void
pmap_check_wiring(str, va)
	char *str;
	vm_offset_t va;
{
	vm_map_entry_t entry;
	register int count, *pte;

	va = trunc_page(va);
	if (!pmap_pde_v(pmap_pde(pmap_kernel(), va)) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;

	if (!vm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %x not found\n", va);
		return;
	}
	count = 0;
	for (pte = (int *)va; pte < (int *)(va + NBPG); pte++)
		if (*pte)
			count++;
	if (entry->wired_count != count)
		printf("*%s*: %x: w%d/a%d\n",
		       str, va, entry->wired_count, count);
}
#endif

/* print address space of pmap*/
void
pads(pm)
	pmap_t pm;
{
	unsigned va, i, j;
	register pt_entry_t *pte;

	if (pm == pmap_kernel())
		return;
	for (i = 0; i < 1024; i++) 
		if (pmap_pde_v(&pm->pm_pdir[i]))
			for (j = 0; j < 1024 ; j++) {
				va = (i << PDSHIFT) | (j << PGSHIFT);
				if (pm == pmap_kernel() &&
				    va < VM_MIN_KERNEL_ADDRESS)
					continue;
				if (pm != pmap_kernel() &&
				    va > VM_MAX_ADDRESS)
					continue;
				pte = pmap_pte(pm, va);
				if (pmap_pte_v(pte)) 
					printf("%x:%x ", va, *pte); 
			}
}
#endif
@


1.38
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.old.c,v 1.37 2001/03/22 20:44:59 niklas Exp $	*/
@


1.37
log
@KNF a stmt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2000/05/04 18:55:06 deraadt Exp $	*/
d416 1
a416 1
	pvp = (struct pv_page *) trunc_page(pv);
d473 1
a473 1
			pvp = (struct pv_page *) trunc_page(pv);
d1179 1
a1179 1
		v = trunc_page(vtopte(va));
d1818 1
a1818 1
			pv2 = trunc_page(vtopte(pv));
@


1.37.2.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.old.c,v 1.37 2001/03/22 20:44:59 niklas Exp $	*/
@


1.37.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.old.c,v 1.37.2.1 2001/04/18 16:07:22 niklas Exp $	*/
@


1.36
log
@revision 1.31 patch was incorrect
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2000/02/22 19:27:48 deraadt Exp $	*/
d1123 2
a1124 2
		printf("pmap_enter(%x, %x, %x, %x, %x)",
		       pmap, va, pa, prot, wired);
@


1.35
log
@enlarge msgbuf, somewhat line netbsd did
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2000/01/29 21:41:51 mickey Exp $	*/
a292 1
#if !defined(UVM)
a305 1
#endif
@


1.34
log
@get usage of memory maps supplied from /boot.
gives two immidiate advances: memory holes support (two
best known are 640k-1M and 15M-16M), and bizaare apm segments placements.
/boot must be at least from 2.5 (well, some earlier might work too ;)
also, allows usage of new libkvm.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 1999/11/30 06:44:51 art Exp $	*/
d283 1
a283 2
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,
	    btoc(sizeof(struct msgbuf))		)
@


1.34.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d283 2
a284 1
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,btoc(MSGBUFSIZE))
@


1.33
log
@Put the "faulting in a pt page map ..." printf behind #ifdef DEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 1999/09/03 18:00:51 art Exp $	*/
a178 2
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
a180 1
extern vm_offset_t hole_start, hole_end;
a229 3
#if defined(UVM)
	int first16q;
#endif
a237 4

	/* XXX: allow for msgbuf */
	avail_end -= i386_round_page(sizeof(struct msgbuf));

d300 2
a301 1
		p = vm_bootstrap_steal_memory((MAXKPDE-NKPDE+1) * NBPG);
a308 33

	/*                       
	 * we must call vm_page_physload() after we are done playing
	 * with virtual_avail but before we call pmap_steal_memory.
	 * [i.e. here]
	 */                               
#if defined(UVM)
	if (avail_end < (16 * 1024 * 1024))
		first16q = VM_FREELIST_DEFAULT;
	else
		first16q = VM_FREELIST_FIRST16;

	if (avail_start < hole_start)
		uvm_page_physload(atop(avail_start), atop(hole_start),
			atop(avail_start), atop(hole_start), first16q);
	if (first16q == VM_FREELIST_FIRST16) {
		uvm_page_physload(atop(hole_end), atop(16 * 1024 * 1024),
		    atop(hole_end), atop(16 * 1024 * 1024), first16q);
		uvm_page_physload(atop(16 * 1024 * 1024), atop(avail_end),
		    atop(16 * 1024 * 1024), atop(avail_end),
		    VM_FREELIST_DEFAULT);
	} else {
		uvm_page_physload(atop(hole_end), atop(avail_end),
		    atop(hole_end), atop(avail_end), first16q);
	}
#else
	if (avail_start < hole_start)
		vm_page_physload(atop(avail_start), atop(hole_start),
				 atop(avail_start), atop(hole_start));      
	vm_page_physload(atop(hole_end), atop(avail_end),
			 atop(hole_end), atop(avail_end));
#endif
	pmap_update();
@


1.32
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 1999/08/25 09:13:53 ho Exp $	*/
d1225 1
d1227 1
@


1.31
log
@Compile under UVM and versions of egcs. art@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 1999/07/02 12:25:50 niklas Exp $	*/
d661 1
a661 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE);
d1152 1
a1152 1
pmap_enter(pmap, va, pa, prot, wired)
d1158 1
@


1.30
log
@Kludge to not get locked threads with UVM, XXX
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 1999/06/04 16:37:48 mickey Exp $	*/
d304 1
d317 1
@


1.29
log
@remove old MN code, which is not in use anymore, MNN been running for
a year already, and upcoming new apm stuff is not compatible w/ the old MN.
niklas@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 1999/06/01 06:37:22 art Exp $	*/
d1231 17
a1247 1
		uvm_map_pageable(vmap, v, round_page(v+1), FALSE);
@


1.28
log
@&vm->vm_pmap -> vm->vm_map.pmap
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 1999/02/26 10:37:51 art Exp $	*/
a186 4
#if !defined(MACHINE_NEW_NONCONTIG)
char		*pmap_attributes;	/* reference and modify bits */
struct pv_entry	*pv_table;		/* array of entries, one per page */
#endif
a316 1
#if defined(MACHINE_NEW_NONCONTIG)
a346 1
#endif          
a359 1
#if defined(MACHINE_NEW_NONCONTIG)
a413 37
#else /* MACHINE_NEW_NONCONTIG */
/*
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
 */
void
pmap_init()
{
	vm_offset_t addr;
	vm_size_t s;

	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE != 1");

	npages = pmap_page_index(avail_end - 1) + 1;
	s = (vm_size_t) (sizeof(struct pv_entry) * npages + npages);
	s = round_page(s);
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
	pv_table = (struct pv_entry *) addr;
	addr += sizeof(struct pv_entry) * npages;
	pmap_attributes = (char *) addr;
	TAILQ_INIT(&pv_page_freelist);

#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %x bytes (%x pgs): tbl %x attr %x\n",
		       s, npages, pv_table, pmap_attributes);
#endif

	/*
	 * Now it is safe to enable pv_table recording.
	 */
	pmap_initialized = TRUE;
}
#endif /* MACHINE_NEW_NONCONTIG */

a486 1
#if defined(MACHINE_NEW_NONCONTIG)
a487 1
#endif
a505 1
#if defined(MACHINE_NEW_NONCONTIG)
a511 3
#else
	for (ph = &pv_table[npages - 1]; ph >= &pv_table[0]; ph--) {
#endif
a851 1
#if defined(MACHINE_NEW_NONCONTIG)
a852 3
#else
	u_int pind;
#endif
a936 1
#if defined(MACHINE_NEW_NONCONTIG)
a942 7
#else

		if ((pind = pmap_page_index(pa)) != -1) {
			pmap_attributes[pind] |= *pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, sva, &pv_table[pind]);
		}
#endif /* MACHINE_NEW_NONCONTIG */
a970 1
#if defined(MACHINE_NEW_NONCONTIG)
a971 3
#else
	u_int pind;
#endif
a979 1
#if defined(MACHINE_NEW_NONCONTIG)
a984 6
#else
	if ((pind = pmap_page_index(pa)) == -1)
		return;

	pv = ph = &pv_table[pind];
#endif
a1024 1
#if defined(MACHINE_NEW_NONCONTIG)
a1025 3
#else
		pmap_attributes[pind] |= *pte & (PG_M | PG_U);
#endif
a1158 1
#if defined(MACHINE_NEW_NONCONTIG)
a1159 3
#else
	u_int pind;
#endif
a1290 1
#if defined(MACHINE_NEW_NONCONTIG)
a1296 6
#else
		if ((pind = pmap_page_index(opa)) != -1) {
			pmap_attributes[pind] |= *pte & (PG_M | PG_U);
			pmap_remove_pv(pmap, va, &pv_table[pind]);
		}
#endif
a1311 1
#if defined(MACHINE_NEW_NONCONTIG)
d1318 1
a1318 11
	}        
#else
	if ((pind = pmap_page_index(pa)) != -1) {
#ifdef DEBUG
		enter_stats.managed++;
#endif
		pmap_enter_pv(pmap, va, &pv_table[pind]);
		cacheable = TRUE;
	}
#endif
	else if (pmap_initialized) {
a1551 1
#if defined(MACHINE_NEW_NONCONTIG)
a1552 1
#endif
a1559 1
#if defined(MACHINE_NEW_NONCONTIG)
a1561 3
#else
	pv = &pv_table[pmap_page_index(phys)];
#endif
a1662 1
#if defined(MACHINE_NEW_NONCONTIG)
a1663 3
#else
		u_int pind;
#endif
a1684 1
#if defined(MACHINE_NEW_NONCONTIG)
a1687 8
#else
		if ((pind = pmap_page_index(pa)) == -1) {
			printf("pmap_pageable: invalid pa %x va %x\n",
				pa, sva);
			return;
		}
		pv = &pv_table[pind];
#endif
a1734 1
#if defined(MACHINE_NEW_NONCONTIG)
a1739 7
#else
	u_int pind;

	if ((pind = pmap_page_index(pa)) == -1)
		return FALSE;
	pv = &pv_table[pind];
#endif
a1744 1
#if defined(MACHINE_NEW_NONCONTIG)
a1745 3
#else
	if (pmap_attributes[pind] & setbits) {
#endif
a1780 1
#if defined(MACHINE_NEW_NONCONTIG)
a1781 3
#else
	u_int pind;
#endif
a1788 1
#if defined(MACHINE_NEW_NONCONTIG)
a1791 5
#else
	if ((pind = pmap_page_index(pa)) == -1)
		return;
	pv = &pv_table[pind];
#endif
a1797 1
#if defined(MACHINE_NEW_NONCONTIG)
a1798 3
#else
		pmap_attributes[pind] &= maskbits;
#endif
a1858 1
#if defined(MACHINE_NEW_NONCONTIG)
a1859 1
#endif
a1861 1
#if defined(MACHINE_NEW_NONCONTIG)
a1870 6
#else
	for (pv = &pv_table[pmap_page_index(pa)]; pv; pv = pv->pv_next) {
		printf(" -> pmap %x, va %x", pv->pv_pmap, pv->pv_va);
		pads(pv->pv_pmap);
	}
#endif
@


1.27
log
@deal with uvm. Mostly name changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 1998/04/25 20:31:30 mickey Exp $	*/
d1296 1
a1296 1
		    pmap != &curproc->p_vmspace->vm_pmap)
@


1.26
log
@change pmap_{de,}activate to take a struct proc *.
XXX - This should be done to other archs, but since nothing (except uvm)
      uses it right now, the interface will be changed there when
      support for uvm is added.
@
text
@d92 4
d237 11
a307 3
	/* Register the page size with the vm system */
	vm_set_page_size();

d328 5
d335 8
a342 1
			atop(avail_start), atop(hole_start));
d344 2
a345 1
			atop(hole_end), atop(avail_end));
d350 2
a351 2
		vm_page_physload(atop(hole_end), atop(avail_end),
				 atop(hole_end), atop(avail_end));
d466 4
d471 1
d515 3
d519 1
d589 3
d593 1
d772 3
d776 1
d778 4
d843 3
d847 1
d1304 3
d1308 1
d1311 3
d1315 1
d1957 4
d1965 1
d1987 3
d1991 1
@


1.25
log
@convert i386 to MNN
@
text
@a196 1
void pmap_deactivate __P((pmap_t, struct pcb *));
d822 2
a823 3
pmap_activate(pmap, pcb)
	pmap_t pmap;
	struct pcb *pcb;
d825 2
d828 3
a830 7
	if (pmap /*&& pmap->pm_pdchanged */) {
		pcb->pcb_cr3 =
		    pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_pdir);
		if (pmap == &curproc->p_vmspace->vm_pmap)
			lcr3(pcb->pcb_cr3);
		pmap->pm_pdchanged = FALSE;
	}
d834 2
a835 3
pmap_deactivate(pmap, pcb)
	pmap_t pmap;
	struct pcb *pcb;
@


1.24
log
@Some cleanup of page steals
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 1998/01/20 18:40:15 niklas Exp $	*/
d179 1
d183 1
d185 2
d195 2
a196 2
__inline void pmap_remove_pv __P((pmap_t, vm_offset_t, u_int));
__inline void pmap_enter_pv __P((pmap_t, vm_offset_t, u_int));
d310 20
d342 56
d432 1
d498 3
d519 8
d528 1
d561 1
a561 1
pmap_enter_pv(pmap, va, pind)
d564 1
a564 1
	u_int pind;
d566 1
a566 1
	register struct pv_entry *pv, *npv;
a576 2

	pv = &pv_table[pind];
d613 1
a613 1
pmap_remove_pv(pmap, va, pind)
d616 1
a616 1
	u_int pind;
d618 1
a618 1
	register struct pv_entry *pv, *npv;
a624 1
	pv = &pv_table[pind];
d857 3
d861 1
d946 9
d957 1
a957 1
			pmap_remove_pv(pmap, sva, pind);
d959 1
d988 3
d992 1
d1001 7
d1012 1
d1053 3
d1057 1
a1057 1

d1191 3
d1195 1
d1319 8
d1329 1
a1329 1
			pmap_remove_pv(pmap, va, pind);
d1331 1
d1347 9
d1360 1
a1360 1
		pmap_enter_pv(pmap, va, pind);
d1362 3
a1364 1
	} else if (pmap_initialized) {
d1598 3
d1608 4
d1613 1
d1715 3
d1719 1
d1741 5
a1745 1

a1750 1

d1752 1
d1799 8
a1807 1
	int s;
a1810 1

d1812 1
d1818 3
d1822 1
d1857 4
d1862 1
a1862 1
	int s;
d1870 5
a1876 1

d1878 1
d1885 3
d1889 1
d1941 3
d1946 11
d1961 1
@


1.23
log
@Merge bus_dma support from NetBSD, mostly by Jason Thorpe.  Only i386 uses it
 so far, the other archs gets placeholders for now.  I wrote a compatibility
layer for OpenBSD's old isadma code so we can still use our old
driver sources.  They will however get changed to native bus_dma use,
on a case by case basis.   Oh yes, I almost forgot, I kept our notion
of isadma being a device so DMA-less ISA-busses still work
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 1997/10/25 06:57:59 niklas Exp $	*/
d290 3
d298 1
a298 1
		p = pmap_steal_memory((MAXKPDE-NKPDE+1) * NBPG);
a313 1

@


1.22
log
@Boot arguments are now at physmem 0x100
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 1997/10/24 22:15:07 mickey Exp $	*/
a288 14

	/*
	 * reserve special hunk of memory for use by bus dma as a bounce
	 * buffer (contiguous virtual *and* physical memory).  XXX
	 */
#if NISA > 0 && NISADMA > 0
	if (ctob(physmem) >= 0x1000000) {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE * NBPG);
		isaphysmempgs = DMA_BOUNCE;
	} else {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE_LOW * NBPG);
		isaphysmempgs = DMA_BOUNCE_LOW;
	}
#endif
@


1.21
log
@map a piece of memory after the msgbuf and copy bootargv there.
pass cksumlen argument, sysctl it
mostly by niklas
me just did slite editing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 1997/09/24 22:28:15 niklas Exp $	*/
d231 2
a232 3
	/* XXX: allow for msgbuf and bootargv */
	avail_end -= i386_round_page(sizeof(struct msgbuf)) +
	    2 * i386_round_page(bootargc);
d279 3
a281 2
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,1		)
	SYSMAP(bootarg_t *	,bootargmap	,bootargp  ,2		)
@


1.20
log
@Revert, as we won't have enough time to test this fully before release.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 1997/08/17 17:31:37 grr Exp $	*/
d95 1
d207 1
a207 1
pt_entry_t	*msgbufmap;
d231 3
a233 2
	/* XXX: allow for msgbuf */
	avail_end -= i386_round_page(sizeof(struct msgbuf));
d281 1
@


1.19
log
@Use vm_page_alloc_memory API. Some cleanup.
@
text
@d287 14
@


1.18
log
@Back out Mickey's 8/1 pmap.c change, which was misguided and caused
stability problems with swapped/paged out processes getting segementation
vioations when reactivated.

Also add some additional paranoia about whether an allocation being changed
to pageable is actually a page-table and move some sanity checking from
#ifdef DEBUG over to #ifdef DIAGSNOTIC.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 1997/07/28 23:46:10 mickey Exp $	*/
a285 14

	/*
	 * reserve special hunk of memory for use by bus dma as a bounce
	 * buffer (contiguous virtual *and* physical memory).  XXX
	 */
#if NISA > 0 && NISADMA > 0
	if (ctob(physmem) >= 0x1000000) {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE * NBPG);
		isaphysmempgs = DMA_BOUNCE;
	} else {
		isaphysmem = pmap_steal_memory(DMA_BOUNCE_LOW * NBPG);
		isaphysmempgs = DMA_BOUNCE_LOW;
	}
#endif
@


1.17
log
@do multipage pmap_pageable
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 1997/01/07 05:37:32 tholo Exp $	*/
d1541 1
d1561 3
a1564 2
	if (pmap != pmap_kernel() || !pageable)
		return;
d1566 4
a1569 1
	for ( ; sva < eva; sva += NBPG) {
d1572 1
a1572 2

#ifdef DEBUG
d1575 1
d1577 1
d1591 8
a1598 2
#ifdef DEBUG
		if ((pind = pmap_page_index(pa)) == -1)
d1600 1
@


1.16
log
@Fix for final ptdi panic on i386
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 1996/10/25 11:14:13 deraadt Exp $	*/
d1558 1
a1558 2
	 * Assumptions:
	 *	- we are called with only one page at a time
d1561 4
a1564 1
	if (pmap == pmap_kernel() && pageable && sva + NBPG == eva) {
@


1.15
log
@grow kvm space; fix an over-agressive pmap optimization
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1996/09/26 14:04:27 deraadt Exp $	*/
d1727 17
@


1.14
log
@-Wall happiness (not happieness, pointed by theo ;)
@
text
@d229 1
d301 13
d657 2
a658 1
	bcopy(&PTD[KPTDI], &pmap->pm_pdir[KPTDI], NKPDE * sizeof(pd_entry_t));
d661 2
a662 2
	pmap->pm_pdir[PTDPTDI] =
	    pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_pdir) | PG_V | PG_KW;
d804 7
a1131 1

d1137 2
a1138 2
		  	pmap != &curproc->p_vmspace->vm_pmap)
				panic("ptdi %x", pmap->pm_pdir[PTDPTDI]);
d1144 1
@


1.13
log
@fault in a ptp in one nasty situation. not fool proof, but helps some cases. thanks to dyson and chuck
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1996/06/16 10:24:19 deraadt Exp $	*/
d194 3
d1708 1
d1723 1
d1751 1
@


1.12
log
@print better ptdi panic diagnostics
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1996/06/02 10:44:21 mickey Exp $	*/
a1079 3
	/*
	 * Page Directory table entry not valid, we need a new PT page
	 */
d1082 45
a1126 6
		printf("ptdi panic!\n");
		printf("pte %p pmap %p va %p\n", pte, pmap, va);
		printf("pmap_pde(pmap, va) %p\n", pmap_pde(pmap, va));
		printf("pmap_pde_v(pmap_pde(pmap, va) %p\n",
		    pmap_pde_v(pmap_pde(pmap, va)));
		panic("ptdi %x", pmap->pm_pdir[PTDPTDI]);
@


1.11
log
@Fix back my fixes lost in last sync.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1996/05/30 09:30:08 deraadt Exp $	*/
d1084 6
a1089 1
	if (!pte)
d1091 1
a1091 1

@


1.10
log
@clean & sync
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1996/05/07 07:21:51 deraadt Exp $	*/
d1382 2
a1383 1
	printf("pmap_collect(%x) ", pmap);
d1391 1
a1391 1
#if 0
d1398 3
@


1.9
log
@sync with 0504; prototype changes
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1996/04/21 22:16:33 deraadt Exp $	*/
a184 1
void	i386_protection_init __P((void));
d1382 1
a1382 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%x) ", pmap);
@


1.8
log
@partial sync with netbsd 960418, more to come
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.7 1996/04/18 04:04:54 mickey Exp $	*/
/*	$NetBSD: pmap.c,v 1.35 1996/04/03 08:21:05 mycroft Exp $	*/
d83 1
d187 2
d190 5
a226 3
	extern int physmem;
	extern vm_offset_t reserve_dumppages(vm_offset_t);

d319 1
a319 1
	vm_offset_t addr, addr2;
a320 1
	int rv;
a384 1
	register int i;
d844 1
d846 1
d1024 1
d1026 1
d1099 2
a1100 1
		if (wired && !pmap_pte_w(pte) || !wired && pmap_pte_w(pte)) {
d1266 1
a1266 1
	if (wired && !pmap_pte_w(pte) || !wired && pmap_pte_w(pte)) {
a1381 6
	register vm_offset_t pa;
	register struct pv_entry *pv;
	register pt_entry_t *pte;
	vm_offset_t kpa;
	int s;

@


1.7
log
@Fix prototyping, so it's compiling again.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.6 1996/04/17 05:18:55 mickey Exp $	*/
/*	$NetBSD: pmap.c,v 1.34 1995/12/09 07:39:02 mycroft Exp $	*/
d186 1
@


1.6
log
@Cleanups & fixes from latest NetBSD primarily to run doscmd, etc.
GENERIC added to the compile/.cvsignore (it is used for 'make links'
for example), thus conf/GENERIC should appear magically ...
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d184 1
@


1.5
log
@Merging w/ NetBSD 021796.
speaker upgraded to the current.
some changes to the VM stuff (ie kern_thread.c added and so).
@
text
@d1 1
d1541 1
a1541 1

@


1.4
log
@elliminating unnecessary printf w/ DEBUG in pmap_collect.
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.33 1995/06/26 05:21:58 cgd Exp $	*/
@


1.3
log
@i386 isa bounce buffers by hannken@@eis.cs.tu-bs.de
@
text
@d1378 2
a1379 1
	printf("pmap_collect(%x) ", pmap);
@


1.2
log
@wrap isaphysmem in NISADMA; from andrew@@wipux2.wifo.uni-mannheim.de; netbsd pr#1735
@
text
@d96 1
d280 8
a287 2
#if NISADMA > 0
	isaphysmem = pmap_steal_memory(DMA_BOUNCE * NBPG);
@


1.1
log
@Initial revision
@
text
@d279 1
a279 1
#if NISA > 0
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@

