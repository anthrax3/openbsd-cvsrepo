head	1.52;
access;
symbols
	OPENBSD_6_1_BASE:1.52
	OPENBSD_6_0:1.49.0.2
	OPENBSD_6_0_BASE:1.49
	OPENBSD_5_9:1.48.0.2
	OPENBSD_5_9_BASE:1.48
	OPENBSD_5_8:1.41.0.4
	OPENBSD_5_8_BASE:1.41
	OPENBSD_5_7:1.27.0.2
	OPENBSD_5_7_BASE:1.27
	OPENBSD_5_6:1.24.0.4
	OPENBSD_5_6_BASE:1.24
	OPENBSD_5_5:1.23.0.4
	OPENBSD_5_5_BASE:1.23
	OPENBSD_5_4:1.22.0.4
	OPENBSD_5_4_BASE:1.22
	OPENBSD_5_3:1.22.0.2
	OPENBSD_5_3_BASE:1.22
	OPENBSD_5_2:1.21.0.10
	OPENBSD_5_2_BASE:1.21
	OPENBSD_5_1_BASE:1.21
	OPENBSD_5_1:1.21.0.8
	OPENBSD_5_0:1.21.0.6
	OPENBSD_5_0_BASE:1.21
	OPENBSD_4_9:1.21.0.4
	OPENBSD_4_9_BASE:1.21
	OPENBSD_4_8:1.21.0.2
	OPENBSD_4_8_BASE:1.21
	OPENBSD_4_7:1.20.0.2
	OPENBSD_4_7_BASE:1.20
	OPENBSD_4_6:1.19.0.4
	OPENBSD_4_6_BASE:1.19
	OPENBSD_4_5:1.15.0.2
	OPENBSD_4_5_BASE:1.15
	OPENBSD_4_4:1.14.0.4
	OPENBSD_4_4_BASE:1.14
	OPENBSD_4_3:1.14.0.2
	OPENBSD_4_3_BASE:1.14
	OPENBSD_4_2:1.13.0.2
	OPENBSD_4_2_BASE:1.13
	OPENBSD_4_1:1.7.0.2
	OPENBSD_4_1_BASE:1.7
	OPENBSD_4_0:1.5.0.2
	OPENBSD_4_0_BASE:1.5;
locks; strict;
comment	@ * @;


1.52
date	2016.10.21.06.20.58;	author mlarkin;	state Exp;
branches;
next	1.51;
commitid	szRuKZ9HgqvwYLcM;

1.51
date	2016.09.17.07.37.57;	author mlarkin;	state Exp;
branches;
next	1.50;
commitid	aHOfHpl2qCSTgi8H;

1.50
date	2016.09.16.02.35.41;	author dlg;	state Exp;
branches;
next	1.49;
commitid	Fei4687v68qad1tP;

1.49
date	2016.03.07.05.32.47;	author naddy;	state Exp;
branches;
next	1.48;
commitid	Ht3NH0pdlkYC6Nxx;

1.48
date	2016.02.20.19.59.01;	author mlarkin;	state Exp;
branches;
next	1.47;
commitid	YdhhS211QfzgRU4H;

1.47
date	2015.10.23.09.36.09;	author kettenis;	state Exp;
branches;
next	1.46;
commitid	AJyCutJ1mVHbxB6K;

1.46
date	2015.09.03.18.49.19;	author kettenis;	state Exp;
branches;
next	1.45;
commitid	FcVXc8R49wlBBFLt;

1.45
date	2015.08.28.05.00.42;	author mlarkin;	state Exp;
branches;
next	1.44;
commitid	ukgD5217j2tpSTsJ;

1.44
date	2015.08.25.20.18.44;	author mlarkin;	state Exp;
branches;
next	1.43;
commitid	nqTT2PsTpGTkTIiB;

1.43
date	2015.08.25.04.57.31;	author mlarkin;	state Exp;
branches;
next	1.42;
commitid	9OQndlj4AhqxzfSp;

1.42
date	2015.08.22.07.16.10;	author mlarkin;	state Exp;
branches;
next	1.41;
commitid	kNcQSOWLJMgUor2a;

1.41
date	2015.08.04.06.12.16;	author mlarkin;	state Exp;
branches;
next	1.40;
commitid	LQuoqBdcrpfiymRt;

1.40
date	2015.07.10.13.06.26;	author kettenis;	state Exp;
branches;
next	1.39;
commitid	rD0fxwlCxe3YpQUu;

1.39
date	2015.07.10.11.52.59;	author kettenis;	state Exp;
branches;
next	1.38;
commitid	SobrNgy64HGluy6L;

1.38
date	2015.07.09.08.33.05;	author kettenis;	state Exp;
branches;
next	1.37;
commitid	Hj9fqfTvUold6FeU;

1.37
date	2015.07.02.16.14.43;	author kettenis;	state Exp;
branches;
next	1.36;
commitid	Pj63rbiQMvtKOpRg;

1.36
date	2015.04.26.09.48.29;	author kettenis;	state Exp;
branches;
next	1.35;
commitid	3Uurxze029jRDQJK;

1.35
date	2015.04.24.19.41.58;	author kettenis;	state Exp;
branches;
next	1.34;
commitid	VnMWpJGDCLyoYmxy;

1.34
date	2015.04.24.12.52.38;	author kettenis;	state Exp;
branches;
next	1.33;
commitid	ldM9NygmNatsq6n7;

1.33
date	2015.04.22.06.26.23;	author mlarkin;	state Exp;
branches;
next	1.32;
commitid	KKR3WvcGhVQ59IC1;

1.32
date	2015.04.21.00.07.51;	author mlarkin;	state Exp;
branches;
next	1.31;
commitid	wSzAe38Tgttxx9P3;

1.31
date	2015.04.14.05.21.51;	author mlarkin;	state Exp;
branches;
next	1.30;
commitid	ltUKhUefvGLOx98j;

1.30
date	2015.04.12.21.37.33;	author mlarkin;	state Exp;
branches;
next	1.29;
commitid	9CHPP1SHAUmxpVnl;

1.29
date	2015.04.12.19.21.32;	author mlarkin;	state Exp;
branches;
next	1.28;
commitid	zqO23f3TXoDqmWUN;

1.28
date	2015.04.12.18.37.53;	author mlarkin;	state Exp;
branches;
next	1.27;
commitid	5ST94uMTezmXYdhY;

1.27
date	2015.02.02.09.29.53;	author mlarkin;	state Exp;
branches;
next	1.26;
commitid	Gl0HfhXiwDmJ4Tqg;

1.26
date	2014.12.02.18.13.10;	author tedu;	state Exp;
branches;
next	1.25;
commitid	ZYUxNRICiD9sC1vn;

1.25
date	2014.11.19.20.09.01;	author mlarkin;	state Exp;
branches;
next	1.24;
commitid	A1R8j3OYk6zYWabw;

1.24
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.23;
commitid	7NtJNW9udCOFtDNM;

1.23
date	2014.01.06.14.29.25;	author sf;	state Exp;
branches;
next	1.22;

1.22
date	2013.02.09.20.37.41;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2010.04.22.19.02.44;	author oga;	state Exp;
branches;
next	1.20;

1.20
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.19;

1.19
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.18;

1.18
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.17;

1.17
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.16;

1.16
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.15;

1.15
date	2009.01.27.22.14.13;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2007.11.16.16.16.06;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	2007.07.20.19.48.15;	author mk;	state Exp;
branches;
next	1.12;

1.12
date	2007.04.19.16.15.41;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2007.03.26.08.43.34;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2007.03.18.14.23.57;	author mickey;	state Exp;
branches;
next	1.7;

1.7
date	2007.02.03.16.48.23;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2006.11.29.22.40.13;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2006.08.25.13.04.16;	author mickey;	state Exp;
branches;
next	1.4;

1.4
date	2006.08.17.17.09.51;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	2006.05.19.20.53.31;	author brad;	state Exp;
branches;
next	1.2;

1.2
date	2006.05.11.13.21.11;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	2006.04.27.15.37.51;	author mickey;	state Exp;
branches;
next	;


desc
@@


1.52
log
@
vmm(4) for i386. Userland changes forthcoming. Note that for the time being,
i386 hosts are limited to running only i386 guests, even if the underlying
hardware supports amd64. This is a restriction I hope to lift moving forward,
but for now please don't report problems running amd64 guests on i386 hosts.

This was a straightforward port of the in-tree amd64 code plus the old rotted
tree I had from last year for i386 support. Changes included converting 64-bit
VMREAD/VMWRITE ops to 2x32-bit ops, and fixing treatment of the TSS, which
differs on i386.

ok deraadt@@
@
text
@/*	$OpenBSD: pmapae.c,v 1.51 2016/09/17 07:37:57 mlarkin Exp $	*/

/*
 * Copyright (c) 2006-2008 Michael Shalayeff
 * All rights reserved.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF MIND, USE, DATA OR PROFITS, WHETHER IN
 * AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 *	from OpenBSD: pmap.c,v 1.85 2005/11/18 17:05:04 brad Exp
 */

/*
 * pmap.c: i386 pmap module rewrite
 * Chuck Cranor <chuck@@ccrc.wustl.edu>
 * 11-Aug-97
 *
 * history of this pmap module: in addition to my own input, i used
 *    the following references for this rewrite of the i386 pmap:
 *
 * [1] the NetBSD i386 pmap.   this pmap appears to be based on the
 *     BSD hp300 pmap done by Mike Hibler at University of Utah.
 *     it was then ported to the i386 by William Jolitz of UUNET
 *     Technologies, Inc.   Then Charles M. Hannum of the NetBSD
 *     project fixed some bugs and provided some speed ups.
 *
 * [2] the FreeBSD i386 pmap.   this pmap seems to be the
 *     Hibler/Jolitz pmap, as modified for FreeBSD by John S. Dyson
 *     and David Greenman.
 *
 * [3] the Mach pmap.   this pmap, from CMU, seems to have migrated
 *     between several processors.   the VAX version was done by
 *     Avadis Tevanian, Jr., and Michael Wayne Young.    the i386
 *     version was done by Lance Berc, Mike Kupfer, Bob Baron,
 *     David Golub, and Richard Draves.    the alpha version was
 *     done by Alessandro Forin (CMU/Mach) and Chris Demetriou
 *     (NetBSD/alpha).
 */
/*
 * PAE support
 * Michael Shalayeff <mickey@@lucifier.net>
 *
 * This module implements PAE mode for i386.
 *
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/atomic.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/user.h>
#include <sys/kernel.h>
#include <sys/mutex.h>

#include <uvm/uvm.h>

#include <machine/cpu.h>
#include <machine/specialreg.h>
#include <machine/gdt.h>

#include <dev/isa/isareg.h>
#include <i386/isa/isa_machdep.h>
#include <sys/msgbuf.h>
#include <stand/boot/bootarg.h>

#include "ksyms.h"

/*
 * this file contains the code for the "pmap module."   the module's
 * job is to manage the hardware's virtual to physical address mappings.
 * note that there are two levels of mapping in the VM system:
 *
 *  [1] the upper layer of the VM system uses vm_map's and vm_map_entry's
 *      to map ranges of virtual address space to objects/files.  for
 *      example, the vm_map may say: "map VA 0x1000 to 0x22000 read-only
 *      to the file /bin/ls starting at offset zero."   note that
 *      the upper layer mapping is not concerned with how individual
 *      vm_pages are mapped.
 *
 *  [2] the lower layer of the VM system (the pmap) maintains the mappings
 *      from virtual addresses.   it is concerned with which vm_page is
 *      mapped where.   for example, when you run /bin/ls and start
 *      at page 0x1000 the fault routine may lookup the correct page
 *      of the /bin/ls file and then ask the pmap layer to establish
 *      a mapping for it.
 *
 * note that information in the lower layer of the VM system can be
 * thrown away since it can easily be reconstructed from the info
 * in the upper layer.
 *
 * data structures we use include:
 *
 *  - struct pmap: describes the address space of one thread
 *  - struct pv_entry: describes one <PMAP,VA> mapping of a PA
 *  - struct pv_head: there is one pv_head per managed page of
 *	physical memory.   the pv_head points to a list of pv_entry
 *	structures which describe all the <PMAP,VA> pairs that this
 *      page is mapped in.    this is critical for page based operations
 *      such as pmap_page_protect() [change protection on _all_ mappings
 *      of a page]
 */
/*
 * i386 PAE hardware Page Tables structure:
 *
 * the i386 PAE Page Table is a three-level PT which maps 4GB of VA.
 * the pagesize is 4K (4096 [0x1000] bytes) or 2MB.
 *
 * the first level table is called "page directory index" and consists
 * of 4 page directory index entries (PDIE) each 64 bits in size.
 *
 * the second level table is called a "page directory" and it contains
 * 512 page directory entries (PDEs).   each PDE is
 * 8 bytes (a long long), so a PD fits in a single 4K page.   this page is
 * the page directory page (PDP).  each PDE in a PDP maps 1GB of space
 * (512 * 2MB = 1GB).   a PDE contains the physical address of the
 * second level table: the page table.   or, if 2MB pages are being used,
 * then the PDE contains the PA of the 2MB page being mapped.
 *
 * a page table consists of 512 page table entries (PTEs).  each PTE is
 * 8 bytes (a long long), so a page table also fits in a single 4K page.
 * a 4K page being used as a page table is called a page table page (PTP).
 * each PTE in a PTP maps one 4K page (512 * 4K = 2MB).   a PTE contains
 * the physical address of the page it maps and some flag bits (described
 * below).
 *
 * the processor has a special register, "cr3", which points to the
 * the PDP which is currently controlling the mappings of the virtual
 * address space.
 *
 * the following picture shows the translation process for a 4K page:
 *
 * %cr3 register [PA of PDPT]
 *  |
 *  |  bits <31-30> of VA
 *  |  index the DPE (0-3)
 *  |        |
 *  v        v
 *  +-----------+
 *  |  PDP Ptr  |
 *  | 4 entries |
 *  +-----------+
 *       |
 *    PA of PDP
 *       |
 *       |
 *       |  bits <29-21> of VA       bits <20-12> of VA   bits <11-0>
 *       |  index the PDP (0 - 512)  index the PTP        are the page offset
 *       |        |                         |                    |
 *       |        v                         |                    |
 *       +-->+---------+                    |                    |
 *           | PD Page |    PA of           v                    |
 *           |         |-----PTP----->+------------+             |
 *           | 512 PDE |              | page table |--PTE--+     |
 *           | entries |              | (aka PTP)  |       |     |
 *           +---------+              |  512 PTE   |       |     |
 *                                    |  entries   |       |     |
 *                                    +------------+       |     |
 *                                                         |     |
 *                                              bits <35-12>   bits <11-0>
 *                                               p h y s i c a l  a d d r
 *
 * the i386 caches PTEs in a TLB.   it is important to flush out old
 * TLB mappings when making a change to a mapping.   writing to the
 * %cr3 will flush the entire TLB.    newer processors also have an
 * instruction that will invalidate the mapping of a single page (which
 * is useful if you are changing a single mapping because it preserves
 * all the cached TLB entries).
 *
 * as shows, bits 31-12 of the PTE contain PA of the page being mapped.
 * the rest of the PTE is defined as follows:
 *   bit#	name	use
 *   63		NX	no-execute bit (0=ITLB, 1=DTLB), optional
 *   11		n/a	available for OS use, hardware ignores it
 *   10		n/a	available for OS use, hardware ignores it
 *   9		n/a	available for OS use, hardware ignores it
 *   8		G	global bit (see discussion below)
 *   7		PS	page size [for PDEs] (0=4k, 1=4M <if supported>)
 *   6		D	dirty (modified) page
 *   5		A	accessed (referenced) page
 *   4		PCD	cache disable
 *   3		PWT	prevent write through (cache)
 *   2		U/S	user/supervisor bit (0=supervisor only, 1=both u&s)
 *   1		R/W	read/write bit (0=read only, 1=read-write)
 *   0		P	present (valid)
 *
 * notes:
 *  - on the i386 the R/W bit is ignored if processor is in supervisor
 *    state (bug!)
 *  - PS is only supported on newer processors
 *  - PTEs with the G bit are global in the sense that they are not
 *    flushed from the TLB when %cr3 is written (to flush, use the
 *    "flush single page" instruction).   this is only supported on
 *    newer processors.    this bit can be used to keep the kernel's
 *    TLB entries around while context switching.   since the kernel
 *    is mapped into all processes at the same place it does not make
 *    sense to flush these entries when switching from one process'
 *    pmap to another.
 */
/*
 * A pmap describes a process' 4GB virtual address space.  This
 * virtual address space can be broken up into 2048 2MB regions which
 * are described by PDEs in the PDP.  The PDEs are defined as follows:
 *
 * Ranges are inclusive -> exclusive, just like vm_map_entry start/end.
 * The following assumes that KERNBASE is 0xd0000000.
 *
 * PDE#s	VA range		Usage
 * 0->1660	0x0 -> 0xcf800000	user address space, note that the
 *					max user address is 0xcfbfe000
 *					the final two pages in the last 4MB
 *					used to be reserved for the UAREA
 *					but now are no longer used.
 * 1660		0xcf800000->		recursive mapping of PDP (used for
 *			0xd0000000	linear mapping of PTPs).
 * 1664->2044	0xd0000000->		kernel address space (constant
 *			0xff800000	across all pmaps/processes).
 * 2044		0xff800000->		"alternate" recursive PDP mapping
 *			<end>		(for other pmaps).
 *
 *
 * Note: A recursive PDP mapping provides a way to map all the PTEs for
 * a 4GB address space into a linear chunk of virtual memory.  In other
 * words, the PTE for page 0 is the first 8b mapped into the 2MB recursive
 * area.  The PTE for page 1 is the second 8b.  The very last 8b in the
 * 2MB range is the PTE that maps VA 0xffffe000 (the last page in a 4GB
 * address).
 *
 * All pmaps' PDs must have the same values in slots 1660->2043 so that
 * the kernel is always mapped in every process.  These values are loaded
 * into the PD at pmap creation time.
 *
 * At any one time only one pmap can be active on a processor.  This is
 * the pmap whose PDP is pointed to by processor register %cr3.  This pmap
 * will have all its PTEs mapped into memory at the recursive mapping
 * point (slots #1660-3 as show above).  When the pmap code wants to find the
 * PTE for a virtual address, all it has to do is the following:
 *
 * Address of PTE = (1660 * 2MB) + (VA / NBPG) * sizeof(pt_entry_t)
 *                = 0xcf800000 + (VA / 4096) * 8
 *
 * What happens if the pmap layer is asked to perform an operation
 * on a pmap that is not the one which is currently active?  In that
 * case we take the PA of the PDP of the non-active pmap and put it in
 * slots 2044-7 of the active pmap.  This causes the non-active pmap's
 * PTEs to get mapped in the final 4MB of the 4GB address space
 * (e.g. starting at 0xffc00000).
 *
 * The following figure shows the effects of the recursive PDP mapping:
 *
 *   PDP (%cr3->PDPTP)
 *   +----+
 *   |   0| -> PTP#0 that maps VA 0x0 -> 0x200000
 *   |    |
 *   |    |
 *   |1660| -> points back to PDP (%cr3) mapping VA 0xcf800000 -> 0xd0000000
 *   |1661|    (PDP is 4 pages)
 *   |1662|
 *   |1663|
 *   |1664| -> first kernel PTP (maps 0xd0000000 -> 0xe0200000)
 *   |    |
 *   |2044| -> points to alternate pmap's PDP (maps 0xff800000 -> end)
 *   |2045|
 *   |2046|
 *   |2047|
 *   +----+
 *
 * Note that the PDE#1660 VA (0xcf8033e0) is defined as "PTE_BASE".
 * Note that the PDE#2044 VA (0xff803fe0) is defined as "APTE_BASE".
 *
 * Starting at VA 0xcf8033e0 the current active PDPs (%cr3) act as a
 * PDPTP and reference four consecutively mapped pages:
 *
 * PTP#1660-3 == PDP(%cr3) => maps VA 0xcf800000 -> 0xd0000000
 *   +----+
 *   |   0| -> maps the contents of PTP#0 at VA 0xcf800000->0xcf801000
 *   |    |
 *   |    |
 *   |1660| -> maps the contents of PTP#1660 (the PDP) at VA 0xcfe7c000
 *   |1661|
 *   |1662|
 *   |1663|
 *   |1664| -> maps the contents of first kernel PTP
 *   |    |
 *   |2047|
 *   +----+
 *
 * Note that mapping of the PDP at PTP#1660's VA (0xcfe7c000) is
 * defined as "PDP_BASE".... within that mapping there are two
 * defines:
 *   "PDP_PDE" (0xcfe7f3e0) is the VA of the PDE in the PDP
 *      which points back to itself.
 *   "APDP_PDE" (0xfff02fe0) is the VA of the PDE in the PDP which
 *      establishes the recursive mapping of the alternate pmap.
 *      To set the alternate PDP, one just has to put the correct
 *	PA info in *APDP_PDE.
 *
 * Note that in the APTE_BASE space, the APDP appears at VA
 * "APDP_BASE" (0xffffc000).
 *
 * unfortunately, we cannot use recursive PDPT from the page tables
 * because cr3 is only 32 bits wide.
 *
 */
#define PG_FRAME	0xffffff000ULL	/* page frame mask */
#define PG_LGFRAME	0xfffe00000ULL	/* large (2M) page frame mask */

/*
 * Redefine the PDSHIFT and NBPD macros for PAE
 */
#undef PDSHIFT
#define PDSHIFT		21		/* page directory address shift */
#undef NBPD
#define NBPD		(1U << PDSHIFT)	/* # bytes mapped by PD (2MB) */

#undef PDSLOT_PTE
#define PDSLOT_PTE	(1660U)	/* 1660: for recursive PDP map */
#undef PDSLOT_KERN
#define PDSLOT_KERN	(1664U)	/* 1664: start of kernel space */
#undef PDSLOT_APTE
#define PDSLOT_APTE	(2044U)	/* 2044: alternative recursive slot */

/*
 * The following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
 */
#define PTE_BASE	((pt_entry_t *) (PDSLOT_PTE * NBPD))
#define APTE_BASE	((pt_entry_t *) (PDSLOT_APTE * NBPD))
#define PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
#define PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define APDP_PDE	(PDP_BASE + PDSLOT_APTE)

/*
 * pdei/ptei: generate index into PDP/PTP from a VA
 */
#define PD_MASK		0xffe00000	/* page directory address bits */
#define PT_MASK		0x001ff000	/* page table address bits */
#define pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)

/*
 * Mach derived conversion macros
 */
#define i386_round_pdr(x)	((((unsigned)(x)) + ~PD_MASK) & PD_MASK)

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 */
#define vtopte(VA)	(PTE_BASE + atop((vaddr_t)VA))

/*
 * PTP macros:
 *   A PTP's index is the PD index of the PDE that points to it.
 *   A PTP's offset is the byte-offset in the PTE space that this PTP is at.
 *   A PTP's VA is the first VA mapped by that PTP.
 *
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 512 entries)
 *           NBPD == number of bytes a PTP can map (2MB)
 */

#define ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */

/*
 * Access PD and PT
 */
#define PDE(pm,i)	(((pd_entry_t *)(pm)->pm_pdir)[(i)])

/*
 * here we define the data types for PDEs and PTEs for PAE
 */
typedef u_int64_t pd_entry_t;		/* PDE */
typedef u_int64_t pt_entry_t;		/* PTE */

#define PG_NX	0x8000000000000000ULL	/* execute-disable */

/*
 * Number of PTEs per cache line. 8 byte pte, 64-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL			8

/*
 * other data structures
 */

extern u_int32_t protection_codes[];	/* maps MI prot to i386 prot code */
extern boolean_t pmap_initialized;	/* pmap_init done yet? */

/* Segment boundaries */
extern vaddr_t kernel_text, etext, __rodata_start, erodata, __data_start;
extern vaddr_t edata, __bss_start, end, ssym, esym, PTmap;

/*
 * MULTIPROCESSOR: special VA's/ PTE's are actually allocated inside a
 * MAXCPUS*NPTECL array of PTE's, to avoid cache line thrashing
 * due to false sharing.
 */

#ifdef MULTIPROCESSOR
#define PTESLEW(pte, id) ((pte)+(id)*NPTECL)
#define VASLEW(va,id) ((va)+(id)*NPTECL*NBPG)
#else
#define PTESLEW(pte, id) (pte)
#define VASLEW(va,id) (va)
#endif

/*
 * special VAs and the PTEs that map them
 */

static pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte, *flsh_pte;
extern caddr_t pmap_csrcp, pmap_cdstp, pmap_zerop, pmap_ptpp, pmap_flshp;

extern int pmap_pg_g;
extern int pmap_pg_wc;
extern struct pmap_head pmaps;

/*
 * local prototypes
 */
struct vm_page	*pmap_alloc_ptp_pae(struct pmap *, int, pt_entry_t);
struct vm_page	*pmap_get_ptp_pae(struct pmap *, int);
void		 pmap_drop_ptp_pae(struct pmap *, vaddr_t, struct vm_page *,
    pt_entry_t *);
pt_entry_t	*pmap_map_ptes_pae(struct pmap *);
void		 pmap_unmap_ptes_pae(struct pmap *);
void		 pmap_do_remove_pae(struct pmap *, vaddr_t, vaddr_t, int);
void		 pmap_remove_ptes_pae(struct pmap *, struct vm_page *,
		     vaddr_t, vaddr_t, vaddr_t, int, struct pv_entry **);
void		 pmap_sync_flags_pte_pae(struct vm_page *, pt_entry_t);

static __inline u_int
pmap_pte2flags(pt_entry_t pte)
{
	return (((pte & PG_U) ? PG_PMAP_REF : 0) |
	    ((pte & PG_M) ? PG_PMAP_MOD : 0));
}

void
pmap_sync_flags_pte_pae(struct vm_page *pg, pt_entry_t pte)
{
	if (pte & (PG_U|PG_M)) {
		atomic_setbits_int(&pg->pg_flags, pmap_pte2flags(pte));
	}
}

/*
 * pmap_map_ptes: map a pmap's PTEs into KVM and lock them in
 *
 * => we lock enough pmaps to keep things locked in
 * => must be undone with pmap_unmap_ptes before returning
 */

pt_entry_t *
pmap_map_ptes_pae(struct pmap *pmap)
{
	pd_entry_t opde;

	/* the kernel's pmap is always accessible */
	if (pmap == pmap_kernel()) {
		return(PTE_BASE);
	}

	mtx_enter(&pmap->pm_mtx);

	/* if curpmap then we are always mapped */
	if (pmap_is_curpmap(pmap)) {
		return(PTE_BASE);
	}

	mtx_enter(&curcpu()->ci_curpmap->pm_apte_mtx);

	/* need to load a new alternate pt space into curpmap? */
	opde = *APDP_PDE;
#if defined(MULTIPROCESSOR) && defined(DIAGNOSTIC)
	if (pmap_valid_entry(opde))
		panic("pmap_map_ptes_pae: APTE valid");
#endif
	if (!pmap_valid_entry(opde) || (opde & PG_FRAME) != pmap->pm_pdidx[0]) {
		APDP_PDE[0] = pmap->pm_pdidx[0] | PG_RW | PG_V | PG_U | PG_M;
		APDP_PDE[1] = pmap->pm_pdidx[1] | PG_RW | PG_V | PG_U | PG_M;
		APDP_PDE[2] = pmap->pm_pdidx[2] | PG_RW | PG_V | PG_U | PG_M;
		APDP_PDE[3] = pmap->pm_pdidx[3] | PG_RW | PG_V | PG_U | PG_M;
		if (pmap_valid_entry(opde))
			pmap_apte_flush();
	}
	return(APTE_BASE);
}

/*
 * pmap_unmap_ptes: unlock the PTE mapping of "pmap"
 */

void
pmap_unmap_ptes_pae(struct pmap *pmap)
{
	if (pmap == pmap_kernel())
		return;

	if (!pmap_is_curpmap(pmap)) {
#if defined(MULTIPROCESSOR)
		APDP_PDE[0] = 0;
		APDP_PDE[1] = 0;
		APDP_PDE[2] = 0;
		APDP_PDE[3] = 0;
		pmap_apte_flush();
#endif
		mtx_leave(&curcpu()->ci_curpmap->pm_apte_mtx);
	}

	mtx_leave(&pmap->pm_mtx);
}

u_int32_t
pmap_pte_set_pae(vaddr_t va, paddr_t pa, u_int32_t bits)
{
	pt_entry_t pte, *ptep = vtopte(va);
	uint64_t nx;

	pa &= PMAP_PA_MASK;

	if (bits & PG_X)
		nx = 0;
	else
		nx = PG_NX;

	pte = i386_atomic_testset_uq(ptep, pa | bits | nx);  /* zap! */
	return (pte & ~PG_FRAME);
}

u_int32_t
pmap_pte_setbits_pae(vaddr_t va, u_int32_t set, u_int32_t clr)
{
	pt_entry_t *ptep = vtopte(va);
	pt_entry_t pte = *ptep;

	i386_atomic_testset_uq(ptep, (pte | set) & ~(pt_entry_t)clr);
	return (pte & ~PG_FRAME);
}

u_int32_t
pmap_pte_bits_pae(vaddr_t va)
{
	pt_entry_t *ptep = vtopte(va);

	return (*ptep & ~PG_FRAME);
}

paddr_t
pmap_pte_paddr_pae(vaddr_t va)
{
	pt_entry_t *ptep = vtopte(va);

	return (*ptep & PG_FRAME);
}

/*
 * Switch over to PAE page tables
 */
void
pmap_bootstrap_pae(void)
{
	extern int cpu_pae, nkpde;
	struct pmap *kpm = pmap_kernel();
	struct vm_page *ptp;
	paddr_t ptaddr;
	u_int32_t bits;
	vaddr_t va, eva;
	pt_entry_t pte;

	if ((cpu_feature & CPUID_PAE) == 0 ||
	    (ecpu_feature & CPUID_NXE) == 0)
		return;

	cpu_pae = 1;

	va = (vaddr_t)kpm->pm_pdir;
	kpm->pm_pdidx[0] = (va + 0*NBPG - KERNBASE) | PG_V;
	kpm->pm_pdidx[1] = (va + 1*NBPG - KERNBASE) | PG_V;
	kpm->pm_pdidx[2] = (va + 2*NBPG - KERNBASE) | PG_V;
	kpm->pm_pdidx[3] = (va + 3*NBPG - KERNBASE) | PG_V;
	/* map pde recursively into itself */
	PDE(kpm, PDSLOT_PTE+0) = kpm->pm_pdidx[0] | PG_KW | PG_M | PG_U;
	PDE(kpm, PDSLOT_PTE+1) = kpm->pm_pdidx[1] | PG_KW | PG_M | PG_U;
	PDE(kpm, PDSLOT_PTE+2) = kpm->pm_pdidx[2] | PG_KW | PG_M | PG_U;
	PDE(kpm, PDSLOT_PTE+3) = kpm->pm_pdidx[3] | PG_KW | PG_M | PG_U;

	/* transfer all kernel mappings over into pae tables */
	for (va = KERNBASE, eva = va + (nkpde << 22);
	    va < eva; va += PAGE_SIZE) {
		if (!pmap_valid_entry(PDE(kpm, pdei(va)))) {
			ptp = uvm_pagealloc(&kpm->pm_obj, va, NULL,
			    UVM_PGA_ZERO);
			ptaddr = VM_PAGE_TO_PHYS(ptp);
			PDE(kpm, pdei(va)) = ptaddr | PG_KW | PG_V |
			    PG_U | PG_M;
			pmap_pte_set_86((vaddr_t)vtopte(va),
			    ptaddr, PG_KW | PG_V | PG_U | PG_M);

			/* count PTP as resident */
			kpm->pm_stats.resident_count++;
		}
		bits = pmap_pte_bits_86(va) | pmap_pg_g;

		/*
		 * At this point, ideally only kernel text should be executable.
		 * However, we need to leave the ISA hole executable to handle
		 * bios32, pcibios, and apmbios calls that may potentially
		 * happen later since we don't know (yet) which of those may be
		 * in use. Later (in biosattach), we will reset the permissions
		 * according to what we actually need.
		 */
		if ((va >= (vaddr_t)&kernel_text && va <= (vaddr_t)&etext) ||
		    (va >= (vaddr_t)atdevbase && va <=
		     (vaddr_t)(atdevbase + IOM_SIZE)))
			bits |= PG_X;
		else
			bits &= ~PG_X;

		if (pmap_valid_entry(bits))
			pmap_pte_set_pae(va, pmap_pte_paddr_86(va), bits);
	}

	if (!cpu_paenable(&kpm->pm_pdidx[0])) {
		extern struct user *proc0paddr;

		proc0paddr->u_pcb.pcb_cr3 = kpm->pm_pdirpa =
		    (vaddr_t)kpm - KERNBASE;
		kpm->pm_pdirsize = 4 * NBPG;

		csrc_pte = vtopte(pmap_csrcp);
		cdst_pte = vtopte(pmap_cdstp);
		zero_pte = vtopte(pmap_zerop);
		ptp_pte = vtopte(pmap_ptpp);
		flsh_pte = vtopte(pmap_flshp);

		nkpde *= 2;
		nkptp_max = 2048 - PDSLOT_KERN - 4;

		pmap_pte_set_p = pmap_pte_set_pae;
		pmap_pte_setbits_p = pmap_pte_setbits_pae;
		pmap_pte_bits_p = pmap_pte_bits_pae;
		pmap_pte_paddr_p = pmap_pte_paddr_pae;
		pmap_clear_attrs_p = pmap_clear_attrs_pae;
		pmap_enter_p = pmap_enter_pae;
		pmap_extract_p = pmap_extract_pae;
		pmap_growkernel_p = pmap_growkernel_pae;
		pmap_page_remove_p = pmap_page_remove_pae;
		pmap_do_remove_p = pmap_do_remove_pae;
		pmap_test_attrs_p = pmap_test_attrs_pae;
		pmap_unwire_p = pmap_unwire_pae;
		pmap_write_protect_p = pmap_write_protect_pae;
		pmap_pinit_pd_p = pmap_pinit_pd_pae;
		pmap_zero_phys_p = pmap_zero_phys_pae;
		pmap_zero_page_uncached_p = pmap_zero_page_uncached_pae;
		pmap_copy_page_p = pmap_copy_page_pae;

		bzero((void *)kpm->pm_pdir + 8, (PDSLOT_PTE-1) * 8);
		/* TODO also reclaim old PDPs */
	}

	/* Set region permissions */
	for (va = (vaddr_t)&PTmap; va < KERNBASE; va += NBPD) {
		pte = PDE(kpm, pdei(va));
		PDE(kpm, pdei(va)) = pte | PG_NX;
	}

	va = (vaddr_t)APTE_BASE;
	pte = PDE(kpm, pdei(va));
	PDE(kpm, pdei(va)) = pte | PG_NX;

	pmap_write_protect(kpm, (vaddr_t)&kernel_text, (vaddr_t)&etext,
	    PROT_READ | PROT_EXEC);
	pmap_write_protect(kpm, (vaddr_t)&__rodata_start,
	    (vaddr_t)&erodata, PROT_READ);
	pmap_write_protect(kpm, (vaddr_t)&__data_start, (vaddr_t)&edata,
	    PROT_READ | PROT_WRITE);
	pmap_write_protect(kpm, (vaddr_t)&__bss_start, (vaddr_t)&end,
	    PROT_READ | PROT_WRITE);

#if defined(DDB) || NKSYMS > 0
	pmap_write_protect(kpm, ssym, esym, PROT_READ);
#endif
}

/*
 * p t p   f u n c t i o n s
 */

/*
 * pmap_alloc_ptp: allocate a PTP for a PMAP
 *
 * => pmap should already be locked by caller
 * => we use the ptp's wire_count to count the number of active mappings
 *	in the PTP (we start it at one to prevent any chance this PTP
 *	will ever leak onto the active/inactive queues)
 * => we should not be holding any pv_head locks (in case we are forced
 *	to call pmap_steal_ptp())
 * => we may need to lock pv_head's if we have to steal a PTP
 * => just_try: true if we want a PTP, but not enough to steal one
 * 	from another pmap (e.g. during optional functions like pmap_copy)
 */

struct vm_page *
pmap_alloc_ptp_pae(struct pmap *pmap, int pde_index, pt_entry_t pde_flags)
{
	struct vm_page *ptp;

	ptp = uvm_pagealloc(&pmap->pm_obj, ptp_i2o(pde_index), NULL,
			    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (ptp == NULL)
		return (NULL);

	/* got one! */
	atomic_clearbits_int(&ptp->pg_flags, PG_BUSY);
	ptp->wire_count = 1;	/* no mappings yet */
	PDE(pmap, pde_index) = (pd_entry_t)(VM_PAGE_TO_PHYS(ptp) |
	    PG_RW | PG_V | PG_M | PG_U | pde_flags);
	pmap->pm_stats.resident_count++;	/* count PTP as resident */
	pmap->pm_ptphint = ptp;
	return(ptp);
}

/*
 * pmap_get_ptp: get a PTP (if there isn't one, allocate a new one)
 *
 * => pmap should NOT be pmap_kernel()
 * => pmap should be locked
 */

struct vm_page *
pmap_get_ptp_pae(struct pmap *pmap, int pde_index)
{
	struct vm_page *ptp;

	if (pmap_valid_entry(PDE(pmap, pde_index))) {
		/* valid... check hint (saves us a PA->PG lookup) */
		if (pmap->pm_ptphint &&
		    (PDE(pmap, pde_index) & PG_FRAME) ==
		    VM_PAGE_TO_PHYS(pmap->pm_ptphint))
			return(pmap->pm_ptphint);

		ptp = uvm_pagelookup(&pmap->pm_obj, ptp_i2o(pde_index));
#ifdef DIAGNOSTIC
		if (ptp == NULL)
			panic("pmap_get_ptp_pae: unmanaged user PTP");
#endif
		pmap->pm_ptphint = ptp;
		return(ptp);
	}

	/* allocate a new PTP (updates ptphint) */
	return (pmap_alloc_ptp_pae(pmap, pde_index, PG_u));
}

void
pmap_drop_ptp_pae(struct pmap *pm, vaddr_t va, struct vm_page *ptp,
    pt_entry_t *ptes)
{
	i386_atomic_testset_uq(&PDE(pm, pdei(va)), 0);
	pmap_tlb_shootpage(curcpu()->ci_curpmap, ((vaddr_t)ptes) + ptp->offset);
#ifdef MULTIPROCESSOR
	/*
	 * Always shoot down the other pmap's
	 * self-mapping of the PTP.
	 */
	pmap_tlb_shootpage(pm, ((vaddr_t)PTE_BASE) + ptp->offset);
#endif
	pm->pm_stats.resident_count--;
	/* update hint */
	if (pm->pm_ptphint == ptp)
		pm->pm_ptphint = RBT_ROOT(uvm_objtree, &pm->pm_obj.memt);
	ptp->wire_count = 0;
	/* Postpone free to after shootdown. */
	uvm_pagerealloc(ptp, NULL, 0);
}

/*
 * pmap_pinit_pd: given a freshly allocated pmap structure, give it a PD
 */
void
pmap_pinit_pd_pae(struct pmap *pmap)
{
	extern int nkpde;
	vaddr_t va;
	paddr_t pdidx[4];

	/* allocate PDP */
	pmap->pm_pdir = uvm_km_alloc(kernel_map, 4 * NBPG);
	if (pmap->pm_pdir == 0)
		panic("pmap_pinit_pd_pae: kernel_map out of virtual space!");
	/* page index is in the pmap! */
	pmap_extract(pmap_kernel(), (vaddr_t)pmap, &pmap->pm_pdirpa);
	va = (vaddr_t)pmap->pm_pdir;
	pmap_extract(pmap_kernel(), va + 0*NBPG, &pdidx[0]);
	pmap_extract(pmap_kernel(), va + 1*NBPG, &pdidx[1]);
	pmap_extract(pmap_kernel(), va + 2*NBPG, &pdidx[2]);
	pmap_extract(pmap_kernel(), va + 3*NBPG, &pdidx[3]);
	pmap->pm_pdidx[0] = (uint64_t)pdidx[0];
	pmap->pm_pdidx[1] = (uint64_t)pdidx[1];
	pmap->pm_pdidx[2] = (uint64_t)pdidx[2];
	pmap->pm_pdidx[3] = (uint64_t)pdidx[3];
	pmap->pm_pdidx[0] |= PG_V;
	pmap->pm_pdidx[1] |= PG_V;
	pmap->pm_pdidx[2] |= PG_V;
	pmap->pm_pdidx[3] |= PG_V;
	pmap->pm_pdirsize = 4 * NBPG;

	/* init PDP */
	/* zero init area */
	bzero((void *)pmap->pm_pdir, PDSLOT_PTE * sizeof(pd_entry_t));
	/* put in recursive PDE to map the PTEs */
	PDE(pmap, PDSLOT_PTE+0) = pmap->pm_pdidx[0] | PG_KW | PG_U |
	    PG_M | PG_V | PG_NX;
	PDE(pmap, PDSLOT_PTE+1) = pmap->pm_pdidx[1] | PG_KW | PG_U |
	    PG_M | PG_V | PG_NX;
	PDE(pmap, PDSLOT_PTE+2) = pmap->pm_pdidx[2] | PG_KW | PG_U |
	    PG_M | PG_V | PG_NX;
	PDE(pmap, PDSLOT_PTE+3) = pmap->pm_pdidx[3] | PG_KW | PG_U |
	    PG_M | PG_V | PG_NX;

	/*
	 * we need to lock pmaps_lock to prevent nkpde from changing on
	 * us.   note that there is no need to splvm to protect us from
	 * malloc since malloc allocates out of a submap and we should have
	 * already allocated kernel PTPs to cover the range...
	 */
	/* put in kernel VM PDEs */
	bcopy(&PDP_BASE[PDSLOT_KERN], &PDE(pmap, PDSLOT_KERN),
	       nkpde * sizeof(pd_entry_t));
	/* zero the rest */
	bzero(&PDE(pmap, PDSLOT_KERN + nkpde), pmap->pm_pdirsize -
	    ((PDSLOT_KERN + nkpde) * sizeof(pd_entry_t)));
	LIST_INSERT_HEAD(&pmaps, pmap, pm_list);
}

/*
 * some misc. functions
 */

/*
 * pmap_extract: extract a PA for the given VA
 */

boolean_t
pmap_extract_pae(struct pmap *pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *ptes, pte;

	if (pmap_valid_entry(PDE(pmap, pdei(va)))) {
		ptes = pmap_map_ptes_pae(pmap);
		pte = ptes[atop(va)];
		pmap_unmap_ptes_pae(pmap);
		if (!pmap_valid_entry(pte))
			return (FALSE);
		if (pap != NULL)
			*pap = (pte & PG_FRAME) | (va & ~PG_FRAME);
		return (TRUE);
	}
	return (FALSE);
}

extern void (*pagezero)(void *, size_t);

/*
 * pmap_zero_phys: same as pmap_zero_page, but for use before vm_pages are
 * initialized.
 */
void
pmap_zero_phys_pae(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(pmap_zerop, id);

#ifdef DIAGNOSTIC
	if (*zpte)
		panic("pmap_zero_phys_pae: lock botch");
#endif

	*zpte = (pa & PG_FRAME) | PG_V | PG_RW;	/* map in */
	pmap_update_pg((vaddr_t)zerova);	/* flush TLB */
	pagezero(zerova, PAGE_SIZE);		/* zero */
	*zpte = 0;
}

/*
 * pmap_zero_page_uncached: the same, except uncached.
 */

boolean_t
pmap_zero_page_uncached_pae(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(pmap_zerop, id);

#ifdef DIAGNOSTIC
	if (*zpte)
		panic("pmap_zero_page_uncached_pae: lock botch");
#endif

	*zpte = (pa & PG_FRAME) | PG_V | PG_RW | PG_N;	/* map in */
	pmap_update_pg((vaddr_t)zerova);		/* flush TLB */
	pagezero(zerova, PAGE_SIZE);		/* zero */
	*zpte = 0;

	return (TRUE);
}

/*
 * pmap_copy_page: copy a page
 */

void
pmap_copy_page_pae(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *spte = PTESLEW(csrc_pte, id);
	pt_entry_t *dpte = PTESLEW(cdst_pte, id);
	caddr_t csrcva = VASLEW(pmap_csrcp, id);
	caddr_t cdstva = VASLEW(pmap_cdstp, id);

#ifdef DIAGNOSTIC
	if (*spte || *dpte)
		panic("pmap_copy_page_pae: lock botch");
#endif

	*spte = (srcpa & PG_FRAME) | PG_V | PG_RW;
	*dpte = (dstpa & PG_FRAME) | PG_V | PG_RW;
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
	bcopy(csrcva, cdstva, PAGE_SIZE);
	*spte = *dpte = 0;
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
}

/*
 * p m a p   r e m o v e   f u n c t i o n s
 *
 * functions that remove mappings
 */

/*
 * pmap_remove_ptes: remove PTEs from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => caller must hold pmap's lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
*/

void
pmap_remove_ptes_pae(struct pmap *pmap, struct vm_page *ptp, vaddr_t ptpva,
    vaddr_t startva, vaddr_t endva, int flags, struct pv_entry **free_pvs)
{
	struct pv_entry *pve;
	pt_entry_t *pte = (pt_entry_t *) ptpva;
	struct vm_page *pg;
	pt_entry_t opte;

	/*
	 * note that ptpva points to the PTE that maps startva.   this may
	 * or may not be the first PTE in the PTP.
	 *
	 * we loop through the PTP while there are still PTEs to look at
	 * and the wire_count is greater than 1 (because we use the wire_count
	 * to keep track of the number of real PTEs in the PTP).
	 */

	for (/*null*/; startva < endva && (ptp == NULL || ptp->wire_count > 1)
			     ; pte++, startva += NBPG) {
		if (!pmap_valid_entry(*pte))
			continue;			/* VA not mapped */

		if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W))
			continue;

		/* atomically save the old PTE and zero it */
		opte = i386_atomic_testset_uq(pte, 0);

		if (opte & PG_W)
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		if (ptp)
			ptp->wire_count--;		/* dropping a PTE */

		/*
		 * Unnecessary work if not PG_PVLIST.
		 */
		pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);

		/*
		 * if we are not on a pv list we are done.
		 */
		if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
			if (pg != NULL)
				panic("pmap_remove_ptes_pae: managed page "
				     "without PG_PVLIST for 0x%lx", startva);
#endif
			continue;
		}

#ifdef DIAGNOSTIC
		if (pg == NULL)
			panic("pmap_remove_ptes_pae: unmanaged page marked "
			      "PG_PVLIST, va = 0x%lx, pa = 0x%lx",
			      startva, (u_long)(opte & PG_FRAME));
#endif

		/* sync R/M bits */
		pmap_sync_flags_pte_pae(pg, opte);
		pve = pmap_remove_pv(pg, pmap, startva);
		if (pve) {
			pve->pv_next = *free_pvs;
			*free_pvs = pve;
		}

		/* end of "for" loop: time for next pte */
	}
}

/*
 * pmap_remove: top level mapping removal function
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_do_remove_pae(struct pmap *pmap, vaddr_t sva, vaddr_t eva, int flags)
{
	pt_entry_t *ptes;
	paddr_t ptppa;
	vaddr_t blkendva;
	struct vm_page *ptp;
	struct pv_entry *pve;
	struct pv_entry *free_pvs = NULL;
	TAILQ_HEAD(, vm_page) empty_ptps;
	int shootall;
	vaddr_t va;

	TAILQ_INIT(&empty_ptps);

	ptes = pmap_map_ptes_pae(pmap);	/* locks pmap */

	/*
	 * Decide if we want to shoot the whole tlb or just the range.
	 * Right now, we simply shoot everything when we remove more
	 * than 32 pages, but never in the kernel pmap. XXX - tune.
	 */
	if ((eva - sva > 32 * PAGE_SIZE) && pmap != pmap_kernel())
		shootall = 1;
	else
		shootall = 0;

	for (va = sva ; va < eva ; va = blkendva) {
		/* determine range of block */
		blkendva = i386_round_pdr(va + 1);
		if (blkendva > eva)
			blkendva = eva;

		/*
		 * XXXCDC: our PTE mappings should never be removed
		 * with pmap_remove!  if we allow this (and why would
		 * we?) then we end up freeing the pmap's page
		 * directory page (PDP) before we are finished using
		 * it when we hit in in the recursive mapping.  this
		 * is BAD.
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		if (pdei(va) >= PDSLOT_PTE && pdei(va) <= (PDSLOT_PTE + 3))
			/* XXXCDC: ugly hack to avoid freeing PDP here */
			continue;

		if (!pmap_valid_entry(PDE(pmap, pdei(va))))
			/* valid block? */
			continue;

		/* PA of the PTP */
		ptppa = PDE(pmap, pdei(va)) & PG_FRAME;

		/* get PTP if non-kernel mapping */
		if (pmap == pmap_kernel()) {
			/* we never free kernel PTPs */
			ptp = NULL;
		} else {
			if (pmap->pm_ptphint &&
			    VM_PAGE_TO_PHYS(pmap->pm_ptphint) == ptppa) {
				ptp = pmap->pm_ptphint;
			} else {
				ptp = PHYS_TO_VM_PAGE(ptppa);
#ifdef DIAGNOSTIC
				if (ptp == NULL)
					panic("pmap_do_remove_pae: unmanaged "
					      "PTP detected");
#endif
			}
		}

		pmap_remove_ptes_pae(pmap, ptp, (vaddr_t)&ptes[atop(va)],
		    va, blkendva, flags, &free_pvs);

		/* If PTP is no longer being used, free it. */
		if (ptp && ptp->wire_count <= 1) {
			pmap_drop_ptp_pae(pmap, va, ptp, ptes);
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, pageq);
		}

		if (!shootall)
			pmap_tlb_shootrange(pmap, va, blkendva);
	}

	if (shootall)
		pmap_tlb_shoottlb();

	pmap_unmap_ptes_pae(pmap);
	pmap_tlb_shootwait();

	while ((pve = free_pvs) != NULL) {
		free_pvs = pve->pv_next;
		pool_put(&pmap_pv_pool, pve);
	}

	while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
		TAILQ_REMOVE(&empty_ptps, ptp, pageq);
		uvm_pagefree(ptp);
	}
}

/*
 * pmap_page_remove: remove a managed vm_page from all pmaps that map it
 *
 * => R/M bits are sync'd back to attrs
 */

void
pmap_page_remove_pae(struct vm_page *pg)
{
	struct pv_entry *pve;
	struct pmap *pm;
	pt_entry_t *ptes, opte;
	TAILQ_HEAD(, vm_page) empty_ptps;
	struct vm_page *ptp;

	if (pg->mdpage.pv_list == NULL)
		return;

	TAILQ_INIT(&empty_ptps);

	mtx_enter(&pg->mdpage.pv_mtx);
	while ((pve = pg->mdpage.pv_list) != NULL) {
		pmap_reference(pve->pv_pmap);
		pm = pve->pv_pmap;
		mtx_leave(&pg->mdpage.pv_mtx);

		ptes = pmap_map_ptes_pae(pve->pv_pmap);	/* locks pmap */

		/*
		 * We dropped the pvlist lock before grabbing the pmap
		 * lock to avoid lock ordering problems.  This means
		 * we have to check the pvlist again since somebody
		 * else might have modified it.  All we care about is
		 * that the pvlist entry matches the pmap we just
		 * locked.  If it doesn't, unlock the pmap and try
		 * again.
		 */
		mtx_enter(&pg->mdpage.pv_mtx);
		if ((pve = pg->mdpage.pv_list) == NULL ||
		    pve->pv_pmap != pm) {
			mtx_leave(&pg->mdpage.pv_mtx);
			pmap_unmap_ptes_pae(pm);	/* unlocks pmap */
			pmap_destroy(pm);
			mtx_enter(&pg->mdpage.pv_mtx);
			continue;
		}

		pg->mdpage.pv_list = pve->pv_next;
		mtx_leave(&pg->mdpage.pv_mtx);

#ifdef DIAGNOSTIC
		if (pve->pv_ptp && (PDE(pve->pv_pmap, pdei(pve->pv_va)) &
				    PG_FRAME)
		    != VM_PAGE_TO_PHYS(pve->pv_ptp)) {
			printf("pmap_page_remove_pae: pg=%p: va=%lx, "
				"pv_ptp=%p\n",
				pg, pve->pv_va, pve->pv_ptp);
			printf("pmap_page_remove_pae: PTP's phys addr: "
				"actual=%llx, recorded=%lx\n",
				(PDE(pve->pv_pmap, pdei(pve->pv_va)) &
				PG_FRAME), VM_PAGE_TO_PHYS(pve->pv_ptp));
			panic("pmap_page_remove_pae: mapped managed page has "
				"invalid pv_ptp field");
}
#endif
		opte = i386_atomic_testset_uq(&ptes[atop(pve->pv_va)], 0);

		if (opte & PG_W)
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;

		/* sync R/M bits */
		pmap_sync_flags_pte_pae(pg, opte);

		/* update the PTP reference count.  free if last reference. */
		if (pve->pv_ptp && --pve->pv_ptp->wire_count <= 1) {
			pmap_drop_ptp_pae(pve->pv_pmap, pve->pv_va,
			    pve->pv_ptp, ptes);
			TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp, pageq);
		}

		pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va);

		pmap_unmap_ptes_pae(pve->pv_pmap);	/* unlocks pmap */
		pmap_destroy(pve->pv_pmap);
		pool_put(&pmap_pv_pool, pve);
		mtx_enter(&pg->mdpage.pv_mtx);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	pmap_tlb_shootwait();

	while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
		TAILQ_REMOVE(&empty_ptps, ptp, pageq);
		uvm_pagefree(ptp);
	}
}

/*
 * p m a p   a t t r i b u t e  f u n c t i o n s
 * functions that test/change managed page's attributes
 * since a page can be mapped multiple times we must check each PTE that
 * maps it by going down the pv lists.
 */

/*
 * pmap_test_attrs: test a page's attributes
 *
 * => we set pv_head => pmap locking
 */

boolean_t
pmap_test_attrs_pae(struct vm_page *pg, int testbits)
{
	struct pv_entry *pve;
	pt_entry_t *ptes, pte;
	u_long mybits, testflags;
	paddr_t ptppa;

	testflags = pmap_pte2flags(testbits);

	if (pg->pg_flags & testflags)
		return (TRUE);

	mybits = 0;
	mtx_enter(&pg->mdpage.pv_mtx);
	for (pve = pg->mdpage.pv_list; pve != NULL && mybits == 0;
	    pve = pve->pv_next) {
		ptppa = PDE(pve->pv_pmap, pdei(pve->pv_va)) & PG_FRAME;
		ptes = (pt_entry_t *)pmap_tmpmap_pa(ptppa);
		pte = ptes[ptei(pve->pv_va)];
		pmap_tmpunmap_pa();
		mybits |= (pte & testbits);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	if (mybits == 0)
		return (FALSE);

	atomic_setbits_int(&pg->pg_flags, pmap_pte2flags(mybits));

	return (TRUE);
}

/*
 * pmap_clear_attrs: change a page's attributes
 *
 * => we return TRUE if we cleared one of the bits we were asked to
 */
boolean_t
pmap_clear_attrs_pae(struct vm_page *pg, int clearbits)
{
	struct pv_entry *pve;
	pt_entry_t *ptes, npte, opte;
	u_long clearflags;
	paddr_t ptppa;
	int result;

	clearflags = pmap_pte2flags(clearbits);

	result = pg->pg_flags & clearflags;
	if (result)
		atomic_clearbits_int(&pg->pg_flags, clearflags);

	mtx_enter(&pg->mdpage.pv_mtx);
	for (pve = pg->mdpage.pv_list; pve != NULL; pve = pve->pv_next) {
		ptppa = PDE(pve->pv_pmap, pdei(pve->pv_va)) & PG_FRAME;
		ptes = (pt_entry_t *)pmap_tmpmap_pa(ptppa);
#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(PDE(pve->pv_pmap, pdei(pve->pv_va))))
			panic("pmap_clear_attrs_pae: mapping without PTP "
				"detected");
#endif

		opte = ptes[ptei(pve->pv_va)];
		if (opte & clearbits) {
			result = TRUE;
			npte = opte & ~clearbits;
			opte = i386_atomic_testset_uq(
			   &ptes[ptei(pve->pv_va)], npte);
			pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va);
		}
		pmap_tmpunmap_pa();
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	pmap_tlb_shootwait();

	return (result != 0);
}


/*
 * p m a p   p r o t e c t i o n   f u n c t i o n s
 */

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_protect: set the protection in of the pages in a pmap
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_write_protect: write-protect pages in a pmap
 */

void
pmap_write_protect_pae(struct pmap *pmap, vaddr_t sva, vaddr_t eva,
    vm_prot_t prot)
{
	pt_entry_t *ptes, *spte, *epte, npte, opte;
	vaddr_t blockend;
	u_int64_t md_prot;
	vaddr_t va;
	int shootall = 0;

	ptes = pmap_map_ptes_pae(pmap);		/* locks pmap */

	/* should be ok, but just in case ... */
	sva &= PG_FRAME;
	eva &= PG_FRAME;

	if ((eva - sva > 32 * PAGE_SIZE) && pmap != pmap_kernel())
		shootall = 1;

	for (va = sva; va < eva; va = blockend) {
		blockend = (va & PD_MASK) + NBPD;
		if (blockend > eva)
			blockend = eva;

		/*
		 * XXXCDC: our PTE mappings should never be write-protected!
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		/* XXXCDC: ugly hack to avoid freeing PDP here */
		if (pdei(va) >= PDSLOT_PTE && pdei(va) <= (PDSLOT_PTE + 3))
			continue;

		/* empty block? */
		if (!pmap_valid_entry(PDE(pmap, pdei(va))))
			continue;

		md_prot = protection_codes[prot];
		if (!(prot & PROT_EXEC))
			md_prot |= PG_NX;
		if (va < VM_MAXUSER_ADDRESS)
			md_prot |= PG_u;
		else if (va < VM_MAX_ADDRESS)
			/* XXX: write-prot our PTES? never! */
			md_prot |= PG_RW;

		spte = &ptes[atop(va)];
		epte = &ptes[atop(blockend)];

		for (/*null */; spte < epte ; spte++, va += PAGE_SIZE) {

			if (!pmap_valid_entry(*spte))	/* no mapping? */
				continue;

			opte = *spte;
			npte = (opte & ~(pt_entry_t)PG_PROT) | md_prot;

			if (npte != opte) {
				pmap_exec_account(pmap, va, *spte, npte);
				i386_atomic_testset_uq(spte, npte);
			}
		}
	}
	if (shootall)
		pmap_tlb_shoottlb();
	else
		pmap_tlb_shootrange(pmap, sva, eva);

	pmap_unmap_ptes_pae(pmap);		/* unlocks pmap */
	pmap_tlb_shootwait();
}

/*
 * end of protection functions
 */

/*
 * pmap_unwire: clear the wired bit in the PTE
 *
 * => mapping should already be in map
 */

void
pmap_unwire_pae(struct pmap *pmap, vaddr_t va)
{
	pt_entry_t *ptes;

	if (pmap_valid_entry(PDE(pmap, pdei(va)))) {
		ptes = pmap_map_ptes_pae(pmap);		/* locks pmap */

#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(ptes[atop(va)]))
			panic("pmap_unwire_pae: invalid (unmapped) va "
			      "0x%lx", va);
#endif
		if ((ptes[atop(va)] & PG_W) != 0) {
			i386_atomic_testset_uq(&ptes[atop(va)],
			    ptes[atop(va)] & ~PG_W);
			pmap->pm_stats.wired_count--;
		}
#ifdef DIAGNOSTIC
		else {
			printf("pmap_unwire_pae: wiring for pmap %p va 0x%lx "
			       "didn't change!\n", pmap, va);
		}
#endif
		pmap_unmap_ptes_pae(pmap);		/* unlocks map */
	}
#ifdef DIAGNOSTIC
	else {
		panic("pmap_unwire_pae: invalid PDE");
	}
#endif
}

/*
 * pmap_copy: copy mappings from one pmap to another
 *
 * => optional function
 * void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
 */

/*
 * defined as macro in pmap.h
 */

/*
 * pmap_enter: enter a mapping into a pmap
 *
 * => must be done "now" ... no lazy-evaluation
 */

int
pmap_enter_pae(struct pmap *pmap, vaddr_t va, paddr_t pa, vm_prot_t prot,
    int flags)
{
	pt_entry_t *ptes, opte, npte;
	struct vm_page *ptp;
	struct pv_entry *pve, *opve = NULL;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	boolean_t nocache = (pa & PMAP_NOCACHE) != 0;
	boolean_t wc = (pa & PMAP_WC) != 0;
	struct vm_page *pg = NULL;
	int error, wired_count, resident_count, ptp_count;

	KASSERT(!(wc && nocache));
	pa &= PMAP_PA_MASK;	/* nuke flags from pa */

#ifdef DIAGNOSTIC
	/* sanity check: totally out of range? */
	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter_pae: too big");

	if (va == (vaddr_t) PDP_BASE || va == (vaddr_t) APDP_BASE)
		panic("pmap_enter_pae: trying to map over PDP/APDP!");

	/* sanity check: kernel PTPs should already have been pre-allocated */
	if (va >= VM_MIN_KERNEL_ADDRESS &&
	    !pmap_valid_entry(PDE(pmap, pdei(va))))
		panic("pmap_enter_pae: missing kernel PTP!");
#endif

	if (pmap_initialized)
		pve = pool_get(&pmap_pv_pool, PR_NOWAIT);
	else
		pve = NULL;
	wired_count = resident_count = ptp_count = 0;

	/*
	 * map in ptes and get a pointer to our PTP (unless we are the kernel)
	 */

	ptes = pmap_map_ptes_pae(pmap);		/* locks pmap */
	if (pmap == pmap_kernel()) {
		ptp = NULL;
	} else {
		ptp = pmap_get_ptp_pae(pmap, pdei(va));
		if (ptp == NULL) {
			if (flags & PMAP_CANFAIL) {
				error = ENOMEM;
				pmap_unmap_ptes_pae(pmap);
				goto out;
			}
			panic("pmap_enter_pae: get ptp failed");
		}
	}
	/*
	 * not allowed to sleep after here!
	 */
	opte = ptes[atop(va)];			/* old PTE */

	/*
	 * is there currently a valid mapping at our VA?
	 */

	if (pmap_valid_entry(opte)) {

		/*
		 * first, calculate pm_stats updates.  resident count will not
		 * change since we are replacing/changing a valid
		 * mapping.  wired count might change...
		 */

		if (wired && (opte & PG_W) == 0)
			wired_count++;
		else if (!wired && (opte & PG_W) != 0)
			wired_count--;

		/*
		 * is the currently mapped PA the same as the one we
		 * want to map?
		 */

		if ((opte & PG_FRAME) == pa) {

			/* if this is on the PVLIST, sync R/M bit */
			if (opte & PG_PVLIST) {
				pg = PHYS_TO_VM_PAGE(pa);
#ifdef DIAGNOSTIC
				if (pg == NULL)
					panic("pmap_enter_pae: same pa "
					     "PG_PVLIST mapping with "
					     "unmanaged page "
					     "pa = 0x%lx (0x%lx)", pa,
					     atop(pa));
#endif
				pmap_sync_flags_pte_pae(pg, opte);
			}
			goto enter_now;
		}

		/*
		 * changing PAs: we must remove the old one first
		 */

		/*
		 * if current mapping is on a pvlist,
		 * remove it (sync R/M bits)
		 */

		if (opte & PG_PVLIST) {
			pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);
#ifdef DIAGNOSTIC
			if (pg == NULL)
				panic("pmap_enter_pae: PG_PVLIST mapping with "
				      "unmanaged page "
				      "pa = 0x%lx (0x%lx)", pa, atop(pa));
#endif
			pmap_sync_flags_pte_pae(pg, opte);
			opve = pmap_remove_pv(pg, pmap, va);
			pg = NULL; /* This is not the page we are looking for */
		}
	} else {	/* opte not valid */
		resident_count++;
		if (wired)
			wired_count++;
		if (ptp)
			ptp_count++;	/* count # of valid entries */
	}

	/*
	 * pve is either NULL or points to a now-free pv_entry structure
	 * (the latter case is if we called pmap_remove_pv above).
	 *
	 * if this entry is to be on a pvlist, enter it now.
	 */

	if (pmap_initialized && pg == NULL)
		pg = PHYS_TO_VM_PAGE(pa);

	if (pg != NULL) {
		if (pve == NULL) {
			pve = opve;
			opve = NULL;
		}
		if (pve == NULL) {
			if (flags & PMAP_CANFAIL) {
				pmap_unmap_ptes_pae(pmap);
				error = ENOMEM;
				goto out;
			}
			panic("pmap_enter_pae: no pv entries available");
		}
		/* lock pg when adding */
		pmap_enter_pv(pg, pve, pmap, va, ptp);
		pve = NULL;
	}

enter_now:
	/*
	 * at this point pg is !NULL if we want the PG_PVLIST bit set
	 */

	npte = pa | protection_codes[prot] | PG_V;
	if (!(prot & PROT_EXEC))
		npte |= PG_NX;
	pmap_exec_account(pmap, va, opte, npte);
	if (wired)
		npte |= PG_W;
	if (nocache)
		npte |= PG_N;
	if (va < VM_MAXUSER_ADDRESS)
		npte |= PG_u;
	else if (va < VM_MAX_ADDRESS)
		npte |= PG_RW;	/* XXXCDC: no longer needed? */
	if (pmap == pmap_kernel())
		npte |= pmap_pg_g;
	if (flags & PROT_READ)
		npte |= PG_U;
	if (flags & PROT_WRITE)
		npte |= PG_M;
	if (pg) {
		npte |= PG_PVLIST;
		if (pg->pg_flags & PG_PMAP_WC) {
			KASSERT(nocache == 0);
			wc = TRUE;
		}
		pmap_sync_flags_pte_pae(pg, npte);
	}
	if (wc)
		npte |= pmap_pg_wc;

	opte = i386_atomic_testset_uq(&ptes[atop(va)], npte);
	if (ptp)
		ptp->wire_count += ptp_count;
	pmap->pm_stats.resident_count += resident_count;
	pmap->pm_stats.wired_count += wired_count;

	if (pmap_valid_entry(opte)) {
		if (nocache && (opte & PG_N) == 0)
			wbinvd(); /* XXX clflush before we enter? */
		pmap_tlb_shootpage(pmap, va);
	}

	pmap_unmap_ptes_pae(pmap);
	pmap_tlb_shootwait();

	error = 0;

out:
	if (pve)
		pool_put(&pmap_pv_pool, pve);
	if (opve)
		pool_put(&pmap_pv_pool, opve);

	return error;
}

/*
 * pmap_growkernel: increase usage of KVM space
 *
 * => we allocate new PTPs for the kernel and install them in all
 *	the pmaps on the system.
 */

vaddr_t
pmap_growkernel_pae(vaddr_t maxkvaddr)
{
	extern int nkpde;
	struct pmap *kpm = pmap_kernel(), *pm;
	int needed_kpde;   /* needed number of kernel PTPs */
	int s;
	paddr_t ptaddr;

	needed_kpde = (int)(maxkvaddr - VM_MIN_KERNEL_ADDRESS + (NBPD-1))
		/ NBPD;
	if (needed_kpde <= nkpde)
		goto out;		/* we are OK */

	/*
	 * whoops!   we need to add kernel PTPs
	 */

	s = splhigh();	/* to be safe */

	for (/*null*/ ; nkpde < needed_kpde ; nkpde++) {

		if (uvm.page_init_done == FALSE) {

			/*
			 * we're growing the kernel pmap early (from
			 * uvm_pageboot_alloc()).  this case must be
			 * handled a little differently.
			 */

			if (uvm_page_physget(&ptaddr) == FALSE)
				panic("pmap_growkernel: out of memory");
			pmap_zero_phys_pae(ptaddr);

			PDE(kpm, PDSLOT_KERN + nkpde) =
				ptaddr | PG_RW | PG_V | PG_U | PG_M;

			/* count PTP as resident */
			kpm->pm_stats.resident_count++;
			continue;
		}

		/*
		 * THIS *MUST* BE CODED SO AS TO WORK IN THE
		 * pmap_initialized == FALSE CASE!  WE MAY BE
		 * INVOKED WHILE pmap_init() IS RUNNING!
		 */

		while (!pmap_alloc_ptp_pae(kpm, PDSLOT_KERN + nkpde, 0))
			uvm_wait("pmap_growkernel");

		/* distribute new kernel PTP to all active pmaps */
		LIST_FOREACH(pm, &pmaps, pm_list) {
			PDE(pm, PDSLOT_KERN + nkpde) =
				PDE(kpm, PDSLOT_KERN + nkpde);
		}
	}

	splx(s);

out:
	return (VM_MIN_KERNEL_ADDRESS + (nkpde * NBPD));
}

/*
 * Pre-allocate PTP 0 for low memory, so that 1:1 mappings for various
 * trampoline code can be entered.
 */
void
pmap_prealloc_lowmem_ptp_pae(void)
{
	pt_entry_t *pte, npte;
	vaddr_t ptpva = (vaddr_t)vtopte(0);

	/* enter pa for pte 0 into recursive map */
	pte = vtopte(ptpva);
	npte = PTP0_PA | PG_RW | PG_V | PG_U | PG_M;

	i386_atomic_testset_uq(pte, npte);

	/* make sure it is clean before using */
	memset((void *)ptpva, 0, NBPG);
}

/*
 * pmap_tmpmap_pa_pae: map a page in for tmp usage
 */

vaddr_t
pmap_tmpmap_pa_pae(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(pmap_ptpp, id);
#if defined(DIAGNOSTIC)
	if (*ptpte)
		panic("pmap_tmpmap_pa_pae: ptp_pte in use?");
#endif
	*ptpte = PG_V | PG_RW | pa;	/* always a new mapping */
	return((vaddr_t)ptpva);
}

/*
 * pmap_tmpunmap_pa_pae: unmap a tmp use page (undoes pmap_tmpmap_pa_pae)
 */

void
pmap_tmpunmap_pa_pae(void)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(pmap_ptpp, id);
#if defined(DIAGNOSTIC)
	if (!pmap_valid_entry(*ptpte))
		panic("pmap_tmpunmap_pa_pae: our pte invalid?");
#endif
	*ptpte = 0;
	pmap_update_pg((vaddr_t)ptpva);
#ifdef MULTIPROCESSOR
	/*
	 * No need for tlb shootdown here, since ptp_pte is per-CPU.
	 */
#endif
}

paddr_t
vtophys_pae(vaddr_t va)
{
	return ((*vtopte(va) & PG_FRAME) | (va & ~PG_FRAME));
}

void
pmap_flush_page_pae(paddr_t pa)
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *pte = PTESLEW(flsh_pte, id);
	caddr_t va = VASLEW(pmap_flshp, id);

	KDASSERT(PHYS_TO_VM_PAGE(pa) != NULL);
#ifdef DIAGNOSTIC
	if (*pte)
		panic("pmap_flush_page_pae: lock botch");
#endif

	*pte = (pa & PG_FRAME) | PG_V | PG_RW;
	pmap_update_pg(va);
	pmap_flush_cache((vaddr_t)va, PAGE_SIZE);
	*pte = 0;
	pmap_update_pg(va);
}

int
pmap_convert(struct pmap *pmap, int mode)
{
	int ret;
	pt_entry_t *pte;
	paddr_t pml4_pa, pdpt_pa;

	pmap->pm_type = mode;

	ret = 0;
	if (mode == PMAP_TYPE_EPT) {
		pmap->pm_npt_pml4 = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
		    &kp_zero, &kd_nowait);
		if (!pmap->pm_npt_pml4) {
			ret = ENOMEM;
			goto error;
		}

		pmap->pm_npt_pdpt = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
		    &kp_zero, &kd_nowait);
		if (!pmap->pm_npt_pdpt) {
			ret = ENOMEM;
			goto error;
		}

		if (!pmap_extract(pmap_kernel(), pmap->pm_npt_pml4,
		    &pml4_pa)) {
			ret = ENOMEM;
			goto error;
		}
		pmap->pm_npt_pa = pml4_pa;

		if (!pmap_extract(pmap_kernel(), pmap->pm_npt_pdpt,
		    &pdpt_pa)) {
			ret = ENOMEM;
			goto error;
		}

		pte = (pt_entry_t *)pmap->pm_npt_pml4;
		pte[0] = (pdpt_pa & PG_FRAME) | EPT_R | EPT_W | EPT_X;
		pte = (pt_entry_t *)pmap->pm_npt_pdpt;
		pte[0] = (pmap->pm_pdidx[0] & PG_FRAME) |
		    EPT_R | EPT_W | EPT_X;
		pte[1] = (pmap->pm_pdidx[1] & PG_FRAME) |
		    EPT_R | EPT_W | EPT_X;
		pte[2] = (pmap->pm_pdidx[2] & PG_FRAME) |
		    EPT_R | EPT_W | EPT_X;
		pte[3] = (pmap->pm_pdidx[3] & PG_FRAME) |
		    EPT_R | EPT_W | EPT_X;
	}

	return (ret);

error:
	if (pmap->pm_npt_pml4)
		km_free((void *)pmap->pm_npt_pml4, PAGE_SIZE, &kv_any, &kp_zero);
	if (pmap->pm_npt_pdpt)
		km_free((void *)pmap->pm_npt_pdpt, PAGE_SIZE, &kv_any, &kp_zero);

	return (ret);
}
@


1.51
log
@
remove unused pmap_dump functions

ok kettenis, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.50 2016/09/16 02:35:41 dlg Exp $	*/
d1917 62
@


1.50
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.49 2016/03/07 05:32:47 naddy Exp $	*/
a1917 51

#ifdef DEBUG
void		 pmap_dump_pae(struct pmap *, vaddr_t, vaddr_t);
/*
 * pmap_dump: dump all the mappings from a pmap
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_dump_pae(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	pt_entry_t *ptes, *pte;
	vaddr_t blkendva;

	/*
	 * if end is out of range truncate.
	 * if (end == start) update to max.
	 */

	if (eva > VM_MAXUSER_ADDRESS || eva <= sva)
		eva = VM_MAXUSER_ADDRESS;

	ptes = pmap_map_ptes_pae(pmap);	/* locks pmap */

	/*
	 * dumping a range of pages: we dump in PTP sized blocks (4MB)
	 */

	for (/* null */ ; sva < eva ; sva = blkendva) {

		/* determine range of block */
		blkendva = i386_round_pdr(sva+1);
		if (blkendva > eva)
			blkendva = eva;

		/* valid block? */
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
			continue;

		pte = &ptes[atop(sva)];
		for (/* null */; sva < blkendva ; sva += NBPG, pte++) {
			if (!pmap_valid_entry(*pte))
				continue;
			printf("va %#lx -> pa %#llx (pte=%#llx)\n",
			       sva, *pte, *pte & PG_FRAME);
		}
	}
	pmap_unmap_ptes_pae(pmap);
}
#endif
@


1.49
log
@Sync no-argument function declaration and definition by adding (void).
ok mlarkin@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.48 2016/02/20 19:59:01 mlarkin Exp $	*/
d814 1
a814 1
		pm->pm_ptphint = RB_ROOT(&pm->pm_obj.memt);
@


1.48
log
@
Fixes a boot issue on non-ACPI i386 machines that need X permissions on
the BIOS region in the ISA hole.

Also fix a separate unrelated issue relating to placing R/O (no X)
permissions on the kernel symbol area on bsd.rd.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.47 2015/10/23 09:36:09 kettenis Exp $	*/
d1871 1
a1871 1
pmap_tmpunmap_pa_pae()
@


1.47
log
@Zap pv allocation abstraction layer.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.46 2015/09/03 18:49:19 kettenis Exp $	*/
d97 1
d101 2
d647 11
a657 2
		/* At this point, only kernel text should be executable */
		if (va >= (vaddr_t)&kernel_text && va <= (vaddr_t)&etext)
d722 2
d725 2
a726 1
 }
@


1.46
log
@Fix a race in pmap_page_remove_86() and pmap_page_remove_pae().

ok millert@@, tedu@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.45 2015/08/28 05:00:42 mlarkin Exp $	*/
d1162 1
a1162 1
		pmap_free_pv(pmap, pve);
d1256 1
a1256 1
		pmap_free_pv(NULL, pve);
d1555 1
a1555 1
		pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d1733 1
a1733 1
		pmap_free_pv(pmap, pve);
d1735 1
a1735 1
		pmap_free_pv(pmap, opve);
@


1.45
log
@
The PDE covering the APTE space should not confer exec permissions.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.44 2015/08/25 20:18:44 mlarkin Exp $	*/
d1181 1
a1192 1
		pg->mdpage.pv_list = pve->pv_next;
d1194 1
d1198 22
@


1.44
log
@
typo in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.43 2015/08/25 04:57:31 mlarkin Exp $	*/
d697 4
@


1.43
log
@
Enforce kernel w^x policy by properly setting NX (as needed) for
kernel text, PTEs, .rodata, data, bss and the symbol regions. This has
been in snaps for a while with no reported fallout.

The APTE space and MP/ACPI trampolines will be fixed next.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.42 2015/08/22 07:16:10 mlarkin Exp $	*/
d1519 1
a1519 1
		panic("pmap_enter:_pae trying to map over PDP/APDP!");
@


1.42
log
@
delete some wrong comments
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.41 2015/08/04 06:12:16 mlarkin Exp $	*/
d428 4
d557 1
d561 6
a566 1
	pte = i386_atomic_testset_uq(ptep, pa | bits);  /* zap! */
d608 1
d643 7
d691 17
a707 1
}
d837 1
a837 1
	    PG_M | PG_V;
d839 1
a839 1
	    PG_M | PG_V;
d841 1
a841 1
	    PG_M | PG_V;
d843 1
a843 1
	    PG_M | PG_V;
@


1.41
log
@
Remove some ancient code in PAE mode that was part of supporting > 4GB
physmem on i386, which we don't support anymore. And since we removed the
physmem ranges above 4GB in machdep.c, this code did nothing anyway.

ok beck@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.40 2015/07/10 13:06:26 kettenis Exp $	*/
a357 1
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
@


1.40
log
@Committed from the wrong tree.  So now for real:

Don't call pool_put(9) while holding a mutex.  Instead collect pv entries in
a list and put them back into the pool after releasing the mutex.  This
prevents a lock ordering problem between the per-pmap mutexes and the kernel
lock that arises because pool_put(9) may grab the kernel lock when it decides
to free a pool page.

This seems to make the i386 pmap mpsafe enough to run the reaper without
holding the kernel lock.

ok sthen@@ (who helped me a lot debugging this)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.38 2015/07/09 08:33:05 kettenis Exp $	*/
a598 1
	int i, pn, pe;
a672 12
		for (i = 0; i < vm_nphysseg; i++)
			if (vm_physmem[i].start > atop(0xfffff000)) {
				vm_physmem[i].avail_end = vm_physmem[i].end;
				/* free vm_pages (uvm had already zeroed 'em) */
				for (pn = 0, pe = vm_physmem[i].end -
				    vm_physmem[i].start; pn < pe ; pn++) {
					uvmexp.npages++;
					/* add page to free pool */
					uvm_pagefree(&vm_physmem[i].pgs[pn]);
				}

			}
@


1.39
log
@Don't call pool_put(9) while holding a mutex.  Instead collect pv entries in
a list and put them back into the pool after releasing the mutex.  This
prevents a lock ordering problem between the per-pmap mutexes and the kernel
lock that arises because pool_put(9) may grab the kernel lock when it decides
to free a pool page.

This seems to make the i386 pmap mpsafe enough to run the reaper without
holding the kernel lock.

ok sthen@@ (who helped me a lot debugging this)
@
text
@d465 1
a465 1
		     vaddr_t, vaddr_t, vaddr_t, int);
d965 1
a965 1
vaddr_t startva, vaddr_t endva, int flags)
d1026 4
a1029 2
		if (pve)
			pool_put(&pmap_pv_pool, pve);
d1048 2
d1118 1
a1118 1
		    va, blkendva, flags);
d1136 5
d1209 1
a1209 1
		pool_put(&pmap_pv_pool, pve);
d1508 1
a1508 1
		pve = pool_get(&pmap_pv_pool, PR_NOWAIT);
d1686 1
a1686 1
		pool_put(&pmap_pv_pool, pve);
d1688 1
a1688 1
		pool_put(&pmap_pv_pool, opve);
@


1.38
log
@Remove unused prototype.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.37 2015/07/02 16:14:43 kettenis Exp $	*/
d1027 1
a1027 1
			pmap_free_pv(NULL, pve);
d1200 1
a1200 1
		pmap_free_pv(NULL, pve);
d1499 1
a1499 1
		pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d1677 1
a1677 1
		pmap_free_pv(pmap, pve);
d1679 1
a1679 1
		pmap_free_pv(pmap, opve);
@


1.37
log
@Make the i386 pmap (almost) mpsafe by protecting the pmap itself, the pv
lists and the apte with a mutex.  Rearrange some code to avoid
sleeping/spinning with one of these locks held.  This should make
pmap_enter(9), pmap_remove(9) and pmap_page_protect(9) safe to use without
holding the kernel lock.  Unfortunately there still seems to be an issue
that causes deadlocks under pressure.  That shouldn't be an issue as
long as uvm still calls the pmap functions with the kernel lock held.

Hopefully committed this will help finding the last bugs.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.36 2015/04/26 09:48:29 kettenis Exp $	*/
a456 1
void		 pmap_free_pv_doit(struct pv_entry *);
@


1.36
log
@Only enable PAE if the CPU we're running on has NX support.  Without NX
support we're only wasting memory on the larger PAE page tables without
any real benefit.  This allows some simplifications of the low-level
assembly code.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.35 2015/04/24 19:41:58 kettenis Exp $	*/
d82 1
a91 1
#include <machine/atomic.h>
d501 2
d508 2
d545 1
d547 2
a967 1
	struct pv_entry *pv_tofree = NULL;	/* list of pv_entrys to free */
d1027 2
a1028 4
		if (pve) {
			pve->pv_next = pv_tofree;
			pv_tofree = pve;
		}
a1031 2
	if (pv_tofree)
		pmap_free_pvs(pmap, pv_tofree);
d1130 1
d1132 1
a1132 1
	pmap_unmap_ptes_pae(pmap);
d1158 6
a1163 1
	for (pve = pg->mdpage.pv_list ; pve != NULL ; pve = pve->pv_next) {
d1165 1
d1198 1
d1200 3
d1204 2
a1205 2
	pmap_free_pvs(NULL, pg->mdpage.pv_list);
	pg->mdpage.pv_list = NULL;
d1233 1
d1241 1
d1244 4
a1247 3
		ptes = pmap_map_ptes_pae(pve->pv_pmap);
		pte = ptes[atop(pve->pv_va)];
		pmap_unmap_ptes_pae(pve->pv_pmap);
d1250 1
d1271 1
d1280 1
d1282 2
a1283 1
		ptes = pmap_map_ptes_pae(pve->pv_pmap);	/* locks pmap */
d1290 1
a1290 1
		opte = ptes[atop(pve->pv_va)];
d1295 1
a1295 1
			   &ptes[atop(pve->pv_va)], npte);
d1298 1
a1298 1
		pmap_unmap_ptes_pae(pve->pv_pmap);	/* unlocks pmap */
d1300 1
d1405 1
a1406 1
	pmap_unmap_ptes_pae(pmap);		/* unlocks pmap */
d1475 1
a1475 1
	struct pv_entry *pve = NULL, *freepve;
d1500 1
a1500 1
		freepve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d1502 1
a1502 1
		freepve = NULL;
d1517 1
d1586 1
a1586 1
			pve = pmap_remove_pv(pg, pmap, va);
d1609 8
a1616 7
			pve = freepve;
			if (pve == NULL) {
				if (flags & PMAP_CANFAIL) {
					error = ENOMEM;
					goto out;
				}
				panic("pmap_enter_pae: no pv entries available");
d1618 1
a1618 1
			freepve = NULL;
d1622 1
a1622 4
	} else {
		/* new mapping is not PG_PVLIST.   free pve if we've got one */
		if (pve)
			pmap_free_pv(pmap, pve);
a1668 1
		pmap_tlb_shootwait();
d1671 3
d1677 4
a1680 3
	pmap_unmap_ptes_pae(pmap);
	if (freepve)
		pmap_free_pv(pmap, freepve);
@


1.35
log
@Make sure we keep the whole recursive mapping of the PDP instead of just the
mapping for the first page when tearing things down.  Seems to fix the last
bug mlarkin@@ has been chasing for a while.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.34 2015/04/24 12:52:38 kettenis Exp $	*/
d414 2
a427 1
pt_entry_t pg_nx;
d585 1
a585 1
pmap_bootstrap_pae()
d595 2
a596 1
	if (!(cpu_feature & CPUID_PAE)){
a597 1
	}
a599 2
	if (ecpu_feature & CPUID_NXE)
		pg_nx = (1ULL << 63);
d1355 1
a1355 1
			md_prot |= pg_nx;
d1612 1
a1612 1
		npte |= pg_nx;
@


1.34
log
@Enable the NX bit and use it in the PAE pmap code.  PAE is still disabled
while we're chasing at least one remaining bug.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.33 2015/04/22 06:26:23 mlarkin Exp $	*/
d1084 1
a1084 1
		if (pdei(va) == PDSLOT_PTE)
d1347 1
a1347 1
		if (pdei(va) == PDSLOT_PTE)
@


1.33
log
@
Reduce differences between pae and no-pae modes.

discussed with deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.32 2015/04/21 00:07:51 mlarkin Exp $	*/
d426 1
d599 2
d1319 1
a1319 1
	u_int32_t md_prot;
d1355 2
d1612 2
@


1.32
log
@
Reduce differences between i386 pmap modes.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.31 2015/04/14 05:21:51 mlarkin Exp $	*/
d45 1
a132 3
 *  - pv_page/pv_page_info: pv_entry's are allocated out of pv_page's.
 *      if we run out of pv_entry's we allocate a new pv_page and free
 *      its pv_entrys.
d361 2
a362 2
#define PTE_BASE	((pt_entry_t *) (PDSLOT_PTE * NBPD) )
#define APTE_BASE	((pt_entry_t *) (PDSLOT_APTE * NBPD) )
a384 1
 *
d398 4
a401 4
#define	ptp_i2o(I)	((I) * NBPG)	/* index => offset */
#define	ptp_o2i(O)	((O) / NBPG)	/* offset => index */
#define	ptp_i2v(I)	((I) * NBPD)	/* index => VA */
#define	ptp_v2i(V)	((V) / NBPD)	/* VA => index (same as pdei) */
d406 1
a406 1
#define	PDE(pm,i)	(((pd_entry_t *)(pm)->pm_pdir)[(i)])
d411 2
a412 2
typedef u_int64_t pd_entry_t;	/* PDE */
typedef u_int64_t pt_entry_t;	/* PTE */
d418 1
a418 1
#define	NPTECL		8
d508 1
a508 1
		panic("pmap_map_ptes: APTE valid");
d549 1
a549 1
	pte = i386_atomic_testset_uq(ptep, pa | bits);
a560 1

d713 1
a713 1
		PG_RW | PG_V | PG_M | PG_U | pde_flags);
d881 1
a881 1
	*zpte = 0;				/* zap! */
d904 2
a905 2
	pagezero(zerova, PAGE_SIZE);			/* zero */
	*zpte = 0;					/* zap! */
d915 1
a915 1
pae_copy_phys(paddr_t srcpa, paddr_t dstpa, int off, int l)
d917 2
d929 1
a929 1
		panic("pmap_copy_phys: lock botch");
d935 2
a936 4
	if (l > PAGE_SIZE - off)
		l = PAGE_SIZE - off;
	bcopy(csrcva + off, cdstva + off, l);
	*spte = *dpte = 0;			/* zap! */
a939 11
void
pmap_copy_page_pae(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
	int s = splhigh();

	pae_copy_phys(srcpa, dstpa, 0, PAGE_SIZE);
	splx(s);
}

d953 1
a953 1
 */
d957 1
a957 1
    vaddr_t startva, vaddr_t endva, int flags)
d982 1
a982 1
		/* atomically save the old PTE and zap! it */
d993 1
a993 1
		 * Unnecessary work if not PG_PVLIST
d998 1
a998 1
		 * if we are not on a pv_head list we are done.
d1004 1
a1004 1
				      "without PG_PVLIST for 0x%lx", startva);
d1103 1
a1103 1
				if (ptp == NULL) {
a1105 1
				}
d1113 1
a1113 1
		/* if PTP is no longer being used, free it. */
d1181 3
a1183 4
				pmap_drop_ptp_pae(pve->pv_pmap, pve->pv_va,
				    pve->pv_ptp, ptes);
				TAILQ_INSERT_TAIL(&empty_ptps, pve->pv_ptp,
				    pageq);
d1192 1
a1226 1

d1255 1
d1268 2
a1269 2
		npte = ptes[atop(pve->pv_va)];
		if (npte & clearbits) {
d1271 1
a1271 1
			npte &= ~clearbits;
d1314 1
a1314 1
	pt_entry_t *ptes, *spte, *epte, npte;
d1356 1
a1356 1
			md_prot |= (PG_u | PG_RW);
d1366 2
a1367 1
			npte = (*spte & ~(pt_entry_t)PG_PROT) | md_prot;
d1369 1
a1369 1
			if (npte != *spte) {
d1408 2
a1409 1
			ptes[atop(va)] &= ~PG_W;
a1558 1

d1568 1
a1568 1
			ptp_count++;
d1650 1
a1650 1
	if (freepve) {
a1651 1
	}
d1697 2
a1698 2
			PDE(kpm, PDSLOT_KERN + nkpde) = ptaddr | PG_RW | PG_V |
			    PG_U | PG_M;
d1821 1
a1821 2
void pmap_dump_pae(struct pmap *, vaddr_t, vaddr_t);

d1863 1
a1863 1
			printf("va %#lx -> pa %#x (pte=%#x)\n",
@


1.31
log
@
Reduce differences between non-PAE and PAE pmaps. This diff removes an
unneeded disable/enable_intr sequence around the PTE unmap operation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.30 2015/04/12 21:37:33 mlarkin Exp $	*/
d452 1
d461 2
a467 2
boolean_t	 pmap_remove_pte_pae(struct pmap *, struct vm_page *,
		     pt_entry_t *, vaddr_t, int);
d711 1
a711 1
		return(NULL);
d745 1
a745 1
			panic("pmap_get_ptp: unmanaged user PTP");
d755 22
d812 8
a819 4
	PDE(pmap, PDSLOT_PTE+0) = pmap->pm_pdidx[0] | PG_KW | PG_U | PG_M;
	PDE(pmap, PDSLOT_PTE+1) = pmap->pm_pdidx[1] | PG_KW | PG_U | PG_M;
	PDE(pmap, PDSLOT_PTE+2) = pmap->pm_pdidx[2] | PG_KW | PG_U | PG_M;
	PDE(pmap, PDSLOT_PTE+3) = pmap->pm_pdidx[3] | PG_KW | PG_U | PG_M;
d879 1
a879 1
		panic("pmap_zero_phys: lock botch");
d903 1
a903 1
		panic("pmap_zero_page_uncached: lock botch");
d931 1
a931 1
		panic("pmap_copy_page: lock botch");
d1008 1
a1008 1
		 * if we are not on a pv_head list we are done.
d1012 3
d1016 5
d1024 7
a1033 1

a1044 57

/*
 * pmap_remove_pte: remove a single PTE from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => caller must hold pmap's lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 * => returns true if we removed a mapping
 */

boolean_t
pmap_remove_pte_pae(struct pmap *pmap, struct vm_page *ptp, pt_entry_t *pte,
    vaddr_t va, int flags)
{
	struct pv_entry *pve;
	struct vm_page *pg;
	pt_entry_t opte;

	if (!pmap_valid_entry(*pte))
		return(FALSE);		/* VA not mapped */

	if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W))
		return (FALSE);

	opte = *pte;			/* save the old PTE */
	*pte = 0;			/* zap! */

	pmap_exec_account(pmap, va, opte, 0);

	if (opte & PG_W)
		pmap->pm_stats.wired_count--;

	pmap->pm_stats.resident_count--;

	if (ptp)
		ptp->wire_count--;		/* dropping a PTE */

	pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);


	/*
	 * if we are not on a pv_head list we are done.
	 */

	if ((opte & PG_PVLIST) == 0)
		return(TRUE);

	pmap_sync_flags_pte_pae(pg, opte);
	pve = pmap_remove_pv(pg, pmap, va);

	if (pve)
		pmap_free_pv(pmap, pve);

	return(TRUE);
}

d1054 1
a1054 1
	pt_entry_t *ptes, opte;
d1059 1
a1059 1
	int shootall = 0;
d1073 2
d1078 1
a1078 1
		blkendva = i386_round_pdr(va+1);
d1119 2
a1120 6
					printf("pmap_remove: null PTP for ptppa 0x%lx\n", ptppa);
					printf("pmap_remove: va = 0x%lx\n", va);
					printf("pmap_remove: pdei(va) = 0x%lx\n", pdei(va));
					printf("pmap_remove: PDE = 0x%llx\n", PDE(pmap, pdei(va)));
					panic("pmap_remove: unmanaged PTP "
					      "detected");
d1129 1
a1129 1
		/* if PTP is no longer being used, free it! */
d1131 1
a1131 24

			opte = i386_atomic_testset_uq(&PDE(pmap, pdei(va)), 0);
#if defined(MULTIPROCESSOR)
			/*
			 * XXXthorpej Redundant shootdown can happen here
			 * if we're using APTE space.
			 */
#endif
			pmap_tlb_shootpage(curpcb->pcb_pmap,
			    ((vaddr_t)ptes) + ptp->offset);
#if defined(MULTIPROCESSOR)
			/*
			 * Always shoot down the pmap's self-mapping
			 * of the PTP.
			 * XXXthorpej Redundant shootdown can happen here
			 * if pmap == curpcb->pcb_pmap (not APTE space).
			 */
			pmap_tlb_shootpage(pmap,
				((vaddr_t)PTE_BASE) + ptp->offset);
#endif
			pmap->pm_stats.resident_count--;
			ptp->wire_count = 0;
			/* Postpone free to after shootdown. */
			uvm_pagerealloc(ptp, NULL, 0);
a1132 3
			if (pmap->pm_ptphint == ptp)	/* update hint? */
				pmap->pm_ptphint =
				    RB_ROOT(&pmap->pm_obj.memt);
a1138 1

a1169 3
		if (pve->pv_ptp == NULL)
			continue;

d1171 15
d1187 1
d1197 2
a1198 23

				opte = i386_atomic_testset_uq(&PDE(pve->pv_pmap,
				    pdei(pve->pv_va)), 0);
				pmap_tlb_shootpage(curcpu()->ci_curpmap,
				    ((vaddr_t)ptes) + pve->pv_ptp->offset);

#if defined(MULTIPROCESSOR)
				/*
				 * Always shoot down the other pmap's
				 * self-mapping of the PTP.
				 */
				pmap_tlb_shootpage(pve->pv_pmap,
				    ((vaddr_t)PTE_BASE) + pve->pv_ptp->offset);

#endif
				pve->pv_pmap->pm_stats.resident_count--;
				/* update hint? */
				if (pve->pv_pmap->pm_ptphint == pve->pv_ptp)
					pve->pv_pmap->pm_ptphint =
					    RB_ROOT(&pve->pv_pmap->pm_obj.memt);
				pve->pv_ptp->wire_count = 0;
				/* Postpone free to after shootdown. */
				uvm_pagerealloc(pve->pv_ptp, NULL, 0);
d1202 1
d1278 6
d1419 2
a1420 1
			panic("pmap_unwire: invalid (unmapped) va 0x%lx", va);
d1428 1
a1428 1
			printf("pmap_unwire: wiring for pmap %p va 0x%lx "
d1436 1
a1436 1
		panic("pmap_unwire: invalid PDE");
d1466 2
d1471 16
a1486 1
	pa &= PMAP_PA_MASK;
d1508 1
a1508 1
			panic("pmap_enter: get ptp failed");
d1521 1
d1527 1
d1537 1
d1539 1
d1543 8
d1567 7
d1576 1
a1576 1
			pg = NULL;
d1586 7
d1604 1
a1604 1
				panic("pmap_enter: no pv entries available");
d1612 1
a1612 1
		if (pve) {
a1613 1
		}	
d1625 2
d1630 1
a1630 1
		npte |= (PG_u | PG_RW);	/* XXXCDC: no longer needed? */
d1639 4
d1645 2
d1654 3
a1656 1
	if (opte & PG_V) {
a1664 1

@


1.30
log
@
Fix some KNF, spacing, and typo issues. Moving the deck chairs around to
reduce differences between PAE and no-PAE i386 pmaps.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.29 2015/04/12 19:21:32 mlarkin Exp $	*/
a534 2
		int ef = read_eflags();
		disable_intr();
a539 1
		write_eflags(ef);
@


1.29
log
@
Fix some typos in comments, and remove an outdated comment about how
certain pmap structures are allocated.

No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.28 2015/04/12 18:37:53 mlarkin Exp $	*/
d338 2
a339 2
#define	PG_FRAME	0xffffff000ULL	/* page frame mask */
#define	PG_LGFRAME	0xfffe00000ULL	/* large (2M) page frame mask */
d342 1
a342 1
 * Redefine the PDSHIFT, NBPD
d344 11
a354 11
#undef	PDSHIFT
#define	PDSHIFT		21		/* page directory address shift */
#undef	NBPD
#define	NBPD		(1U << PDSHIFT)	/* # bytes mapped by PD (2MB) */

#undef	PDSLOT_PTE
#define	PDSLOT_PTE	(1660U)	/* 1660: for recursive PDP map */
#undef	PDSLOT_KERN
#define	PDSLOT_KERN	(1664U)	/* 1664: start of kernel space */
#undef	PDSLOT_APTE
#define	PDSLOT_APTE	(2044U)	/* 2044: alternative recursive slot */
d363 6
a368 6
#define	PTE_BASE	((pt_entry_t *) (PDSLOT_PTE * NBPD) )
#define	APTE_BASE	((pt_entry_t *) (PDSLOT_APTE * NBPD) )
#define	PDP_BASE ((pd_entry_t *)(((char *)PTE_BASE) + (PDSLOT_PTE * NBPG)))
#define	APDP_BASE ((pd_entry_t *)(((char *)APTE_BASE) + (PDSLOT_APTE * NBPG)))
#define	PDP_PDE		(PDP_BASE + PDSLOT_PTE)
#define	APDP_PDE	(PDP_BASE + PDSLOT_APTE)
d373 4
a376 4
#define	PD_MASK		0xffe00000	/* page directory address bits */
#define	PT_MASK		0x001ff000	/* page table address bits */
#define	pdei(VA)	(((VA) & PD_MASK) >> PDSHIFT)
#define	ptei(VA)	(((VA) & PT_MASK) >> PGSHIFT)
d381 1
a381 1
#define	i386_round_pdr(x)	((((unsigned)(x)) + ~PD_MASK) & PD_MASK)
d389 1
a389 2
#define	vtopte(VA)	(PTE_BASE + atop((vaddr_t)VA))

d397 2
a398 2
 * Note that NBPG == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
d412 1
a412 1
 * here we define the data types for PDEs and PTEs
d418 1
a418 1
 * Number of PTE's per cache line. 8 byte pte, 64-byte cache line
a456 7
struct pv_entry *pmap_add_pvpage(struct pv_page *, boolean_t);
struct pv_entry *pmap_alloc_pv(struct pmap *, int); /* see codes in pmap.h */
struct pv_entry *pmap_alloc_pvpage(struct pmap *, int);
void		 pmap_enter_pv(struct vm_page *, struct pv_entry *,
    struct pmap *, vaddr_t, struct vm_page *);
void		 pmap_free_pv(struct pmap *, struct pv_entry *);
void		 pmap_free_pvs(struct pmap *, struct pv_entry *);
a457 1
void		 pmap_free_pvpage(void);
d461 2
a462 1
void            pmap_do_remove_pae(struct pmap *, vaddr_t, vaddr_t, int);
a467 1
void		 pmap_unmap_ptes_pae(struct pmap *);
@


1.28
log
@
Bring PAE code back to life, in a different form. This diff (via bluhm then
to deraadt, then myself) brings the PAE pmap on i386 (not touched in any
significant way for years) closer to the current non-PAE pmap and allows
us to take a big next step toward better i386 W^X in the kernel (similar to
what we did a few months ago on amd64). Unlike the original PAE pmap, this
diff will not be supporting > 4GB physical memory on i386 - this effort is
specifically geared toward providing W^X (via NX) only.

There still seems to be a bug removing certain pmap entries when PAE is
enabled, so I'm leaving PAE mode disabled for the moment until we can
figure out what is going on, but with this diff in the tree hopefully
others can help.

The pmap functions now operate through function pointers, due to the need
to support both non-PAE and PAE forms. My unscientific testing showed
less than 0.3% (a third of a percent) slowdown with this approach during
a base build.

Discussed for months with guenther, kettenis, and deraadt.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.27 2015/02/02 09:29:53 mlarkin Exp $	*/
d197 1
a197 1
 * TLB mappings when making a change to a mappings.   writing to the
d200 1
a200 1
 * is useful if you are changing a single mappings because it preserves
d257 2
a258 2
 * words, the PTE for page 0 is the first int mapped into the 2MB recursive
 * area.  The PTE for page 1 is the second int.  The very last int in the
d277 1
a277 1
 * case we take the PA of the PDP of non-active pmap and put it in
d304 2
a305 2
 * Starting at VA 0xcf8033e0 the current active PDPs (%cr3) acts as a
 * PDPTP and references four consequetly mapped pages:
d334 2
a335 2
 * unfortunately we cannot use recursive PDPT from the page tables
 * because in their infinite wisdom they have defined cr3 32 bits!
a337 69
/*
 * memory allocation
 *
 *  - there are three data structures that we must dynamically allocate:
 *
 * [A] new process' page directory page (PDP)
 *	- plan 1: done at pmap_create() we use
 *	  uvm_km_alloc(kernel_map, PAGE_SIZE)  [fka kmem_alloc] to do this
 *	  allocation.
 *
 * if we are low in free physical memory then we sleep in
 * uvm_km_alloc -- in this case this is ok since we are creating
 * a new pmap and should not be holding any locks.
 *
 * if the kernel is totally out of virtual space
 * (i.e. uvm_km_alloc returns NULL), then we panic.
 *
 * XXX: the fork code currently has no way to return an "out of
 * memory, try again" error code since uvm_fork [fka vm_fork]
 * is a void function.
 *
 * [B] new page tables pages (PTP)
 * 	call uvm_pagealloc()
 * 		=> success: zero page, add to pm_pdir
 * 		=> failure: we are out of free vm_pages, let pmap_enter()
 *		   tell UVM about it.
 *
 * note: for kernel PTPs, we start with NKPTP of them.   as we map
 * kernel memory (at uvm_map time) we check to see if we've grown
 * the kernel pmap.   if so, we call the optional function
 * pmap_growkernel() to grow the kernel PTPs in advance.
 *
 * [C] pv_entry structures
 *	- plan 1: try to allocate one off the free list
 *		=> success: done!
 *		=> failure: no more free pv_entrys on the list
 *	- plan 2: try to allocate a new pv_page to add a chunk of
 *	pv_entrys to the free list
 *		[a] obtain a free, unmapped, VA in kmem_map.  either
 *		we have one saved from a previous call, or we allocate
 *		one now using a "vm_map_lock_try" in uvm_map
 *		=> success: we have an unmapped VA, continue to [b]
 *		=> failure: unable to lock kmem_map or out of VA in it.
 *			move on to plan 3.
 *		[b] allocate a page in kmem_object for the VA
 *		=> success: map it in, free the pv_entry's, DONE!
 *		=> failure: kmem_object locked, no free vm_pages, etc.
 *			save VA for later call to [a], go to plan 3.
 *	If we fail, we simply let pmap_enter() tell UVM about it.
 */
/*
 * locking
 *
 * we have the following locks that we must contend with:
 *
 * "simple" locks:
 *
 * - pmap lock (per pmap, part of uvm_object)
 *   this lock protects the fields in the pmap structure including
 *   the non-kernel PDEs in the PDP, and the PTEs.  it also locks
 *   in the alternate PTE space (since that is determined by the
 *   entry in the PDP).
 *
 * - pmaps_lock
 *   this lock protects the list of active pmaps (headed by "pmaps").
 *   we lock it when adding or removing pmaps from this list.
 *
 */

@


1.27
log
@
Remove some pmap locks that were #defined to be nothing (empty). Discussed
with many, ok kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.26 2014/12/02 18:13:10 tedu Exp $	*/
d4 1
a4 1
 * Copyright (c) 2006 Michael Shalayeff
a400 10
 * - pvh_lock (per pv_head)
 *   this lock protects the pv_entry list which is chained off the
 *   pv_head structure for a specific managed PA.   it is locked
 *   when traversing the list (e.g. adding/removing mappings,
 *   syncing R/M bits, etc.)
 *
 * - pvalloc_lock
 *   this lock protects the data structures which are used to manage
 *   the free list of pv_entry structures.
 *
a413 1
#define	PD_MASK		0xffe00000	/* page directory address bits */
a414 1
#define	PT_MASK		0x001ff000	/* page table address bits */
a417 3
/*
 *
 */
a438 10
#define	PTES_PER_PTP	(NBPG / sizeof(pt_entry_t))  /* # of PTEs in a PTP */

/*
 * various address macros
 *
 *  vtopte: return a pointer to the PTE mapping a VA
 *
 */
#define	vtopte(VA)	(PTE_BASE + atop((vaddr_t)VA))

d442 2
d453 9
d518 2
a519 2
static pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte;
extern caddr_t pmap_csrcp, pmap_cdstp, pmap_zerop, pmap_ptpp;
d527 11
a537 6

struct vm_page	*pmap_alloc_ptp_pae(struct pmap *, int, boolean_t);
#define ALLOCPV_NEED	0	/* need PV now */
#define ALLOCPV_TRY	1	/* just try to allocate, don't steal */
#define ALLOCPV_NONEED	2	/* don't need PV, just growing cache */
struct vm_page	*pmap_get_ptp_pae(struct pmap *, int, boolean_t);
d539 1
d541 1
a541 1
		     vaddr_t, vaddr_t, vaddr_t, int32_t *);
d543 2
a544 1
		     pt_entry_t *, vaddr_t, int32_t *);
a545 2
vaddr_t		 pmap_tmpmap_pa_pae(paddr_t);
void		 pmap_tmpunmap_pa_pae(void);
d547 2
a548 6
/*
 * pmap_tmpmap_pa: map a page in for tmp usage
 */

vaddr_t
pmap_tmpmap_pa_pae(paddr_t pa)
d550 2
a551 11
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(pmap_ptpp, id);
#if defined(DIAGNOSTIC)
	if (*ptpte)
		panic("pmap_tmpmap_pa: ptp_pte in use?");
#endif
	*ptpte = PG_V | PG_RW | pa;		/* always a new mapping */
	return((vaddr_t)ptpva);
a553 4
/*
 * pmap_tmpunmap_pa: unmap a tmp use page (undoes pmap_tmpmap_pa)
 */

d555 1
a555 1
pmap_tmpunmap_pa_pae()
d557 3
a559 16
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(pmap_ptpp, id);
#if defined(DIAGNOSTIC)
	if (!pmap_valid_entry(*ptpte))
		panic("pmap_tmpunmap_pa: our pte invalid?");
#endif
	*ptpte = 0;		/* zap! */
	pmap_update_pg((vaddr_t)ptpva);
#ifdef MULTIPROCESSOR
	/*
	 * No need for tlb shootdown here, since ptp_pte is per-CPU.
	 */
#endif
d586 4
d591 4
a594 4
		APDP_PDE[0] = pmap->pm_pdidx[0] | PG_RW | PG_V;
		APDP_PDE[1] = pmap->pm_pdidx[1] | PG_RW | PG_V;
		APDP_PDE[2] = pmap->pm_pdidx[2] | PG_RW | PG_V;
		APDP_PDE[3] = pmap->pm_pdidx[3] | PG_RW | PG_V;
d596 1
a596 1
			pmap_apte_flush(curpcb->pcb_pmap);
d613 2
d619 2
a620 1
		pmap_apte_flush(curpcb->pcb_pmap);
d630 2
a668 1
	extern paddr_t avail_end, avail_end2;
d677 1
a677 2
	if (!cpu_pae || avail_end >= avail_end2 || !(cpu_feature & CPUID_PAE)){
		avail_end2 = avail_end;
d681 2
d689 4
a692 4
	PDE(kpm, PDSLOT_PTE+0) = kpm->pm_pdidx[0] | PG_KW;
	PDE(kpm, PDSLOT_PTE+1) = kpm->pm_pdidx[1] | PG_KW;
	PDE(kpm, PDSLOT_PTE+2) = kpm->pm_pdidx[2] | PG_KW;
	PDE(kpm, PDSLOT_PTE+3) = kpm->pm_pdidx[3] | PG_KW;
d701 2
a702 1
			PDE(kpm, pdei(va)) = ptaddr | PG_KW | PG_V;
d704 1
a704 1
			    ptaddr, PG_KW | PG_V);
d724 2
a725 1
		ptp_pte  = vtopte(pmap_ptpp);
a728 2
		vm_max_address = (PDSLOT_PTE << PDSHIFT) +
		    (PDSLOT_PTE << PGSHIFT);
d734 1
a734 1
		pmap_change_attrs_p = pmap_change_attrs_pae;
d739 1
a739 1
		pmap_remove_p = pmap_remove_pae;
a746 1
		pmap_try_steal_pv_p = pmap_try_steal_pv_pae;
a761 1
		uvm_page_rehash();
a765 82
 * p v _ e n t r y   f u n c t i o n s
 */

/*
 * pv_entry allocation functions:
 *   the main pv_entry allocation functions are:
 *     pmap_alloc_pv: allocate a pv_entry structure
 *     pmap_free_pv: free one pv_entry
 *     pmap_free_pvs: free a list of pv_entrys
 *
 * the rest are helper functions
 */

/*
 * pmap_try_steal_pv: try and steal a pv_entry from a pmap
 *
 * => return true if we did it!
 */

boolean_t
pmap_try_steal_pv_pae(struct pv_head *pvh, struct pv_entry *cpv,
    struct pv_entry *prevpv)
{
	pt_entry_t *ptep, opte;
#ifdef MULTIPROCESSOR
	int32_t cpumask = 0;
#endif

	/*
	 * we never steal kernel mappings or mappings from pmaps we can't lock
	 */

	if (cpv->pv_pmap == pmap_kernel())
		return(FALSE);

	/*
	 * yes, we can try and steal it.   first we need to remove the
	 * mapping from the pmap.
	 */

	ptep = pmap_tmpmap_pvepte_pae(cpv);
	if (*ptep & PG_W) {
		ptep = NULL;	/* wired page, avoid stealing this one */
	} else {
		opte = i386_atomic_testset_uq(ptep, 0);	/* zap! */
#ifdef MULTIPROCESSOR
		pmap_tlb_shootdown(cpv->pv_pmap, cpv->pv_va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		if (pmap_is_curpmap(cpv->pv_pmap))
			pmap_update_pg(cpv->pv_va);
#endif
		pmap_tmpunmap_pvepte_pae(cpv);
	}
	if (ptep == NULL) {
		return(FALSE);	/* wired page, abort! */
	}
	cpv->pv_pmap->pm_stats.resident_count--;
	if (cpv->pv_ptp && cpv->pv_ptp->wire_count)
		/* drop PTP's wired count */
		cpv->pv_ptp->wire_count--;

	/*
	 * XXX: if wire_count goes to one the PTP could be freed, however,
	 * we'd have to lock the page queues (etc.) to do that and it could
	 * cause deadlock headaches.   besides, the pmap we just stole from
	 * may want the mapping back anyway, so leave the PTP around.
	 */

	/*
	 * now we need to remove the entry from the pvlist
	 */

	if (cpv == pvh->pvh_list)
		pvh->pvh_list = cpv->pv_next;
	else
		prevpv->pv_next = cpv->pv_next;
	return(TRUE);
}

/*
d784 1
a784 1
pmap_alloc_ptp_pae(struct pmap *pmap, int pde_index, boolean_t just_try)
d796 2
a797 2
	PDE(pmap, pde_index) =
	    (pd_entry_t)(VM_PAGE_TO_PHYS(ptp) | PG_u | PG_RW | PG_V);
d811 1
a811 1
pmap_get_ptp_pae(struct pmap *pmap, int pde_index, boolean_t just_try)
a815 1

d832 1
a832 1
	return (pmap_alloc_ptp_pae(pmap, pde_index, just_try));
d843 1
d847 1
a847 1
	if (pmap->pm_pdir == NULL)
a850 1
	/* fill out the PDPT entries */
d852 8
a859 4
	pmap_extract(pmap_kernel(), va + 0*NBPG, &pmap->pm_pdidx[0]);
	pmap_extract(pmap_kernel(), va + 1*NBPG, &pmap->pm_pdidx[1]);
	pmap_extract(pmap_kernel(), va + 2*NBPG, &pmap->pm_pdidx[2]);
	pmap_extract(pmap_kernel(), va + 3*NBPG, &pmap->pm_pdidx[3]);
d870 4
a873 4
	PDE(pmap, PDSLOT_PTE+0) = pmap->pm_pdidx[0] | PG_KW;
	PDE(pmap, PDSLOT_PTE+1) = pmap->pm_pdidx[1] | PG_KW;
	PDE(pmap, PDSLOT_PTE+2) = pmap->pm_pdidx[2] | PG_KW;
	PDE(pmap, PDSLOT_PTE+3) = pmap->pm_pdidx[3] | PG_KW;
d901 1
a901 2
	paddr_t retval;
	pt_entry_t *ptes;
d903 1
a903 1
	if (PDE(pmap, pdei(va))) {
d905 1
a905 1
		retval = (paddr_t)(ptes[atop(va)] & PG_FRAME);
d907 2
d910 1
a910 1
			*pap = retval | (va & ~PG_FRAME);
d935 1
d960 1
a960 1
	*zpte = (pa & PG_FRAME) | PG_V | PG_RW | PG_N);	/* map in */
d973 1
a973 1
pmap_copy_page_pae(struct vm_page *srcpg, struct vm_page *dstpg)
a974 2
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
d978 2
a979 2
	pt_entry_t *spte = PTESLEW(csrc_pte,id);
	pt_entry_t *dpte = PTESLEW(cdst_pte,id);
d991 3
a993 1
	bcopy(csrcva, cdstva, PAGE_SIZE);
d996 11
a1006 3
#ifdef MULTIPROCESSOR
	/* Using per-cpu VA; no shootdown required here. */
#endif
d1026 1
a1026 1
    vaddr_t startva, vaddr_t endva, int32_t *cpumaskp)
d1031 1
a1032 1
	int bank, off;
d1048 5
a1052 1
		opte = i386_atomic_testset_uq(pte, 0);	/* zap! */
d1058 1
a1058 4
		if (opte & PG_U)
			pmap_tlb_shootdown(pmap, startva, opte, cpumaskp);

		if (ptp) {
a1059 5
			/* Make sure that the PDE is flushed */
			if ((ptp->wire_count <= 1) && !(opte & PG_U))
				pmap_tlb_shootdown(pmap, startva, opte,
				    cpumaskp);
		}
d1064 1
a1066 6
#ifdef DIAGNOSTIC
			if (vm_physseg_find(atop(opte & PG_FRAME), &off)
			    != -1)
				panic("pmap_remove_ptes: managed page without "
				      "PG_PVLIST for 0x%lx", startva);
#endif
a1069 8
		bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
		if (bank == -1)
			panic("pmap_remove_ptes: unmanaged page marked "
			      "PG_PVLIST, va = 0x%lx, pa = 0x%lx",
			      startva, (u_long)(opte & PG_FRAME));
#endif

d1071 2
a1072 3
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
		pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap,
				     startva);
d1098 1
a1098 1
    vaddr_t va, int32_t *cpumaskp)
d1100 2
a1102 2
	int bank, off;
	struct pv_entry *pve;
d1107 3
d1117 1
d1120 2
a1121 2
	if (opte & PG_U)
		pmap_tlb_shootdown(pmap, va, opte, cpumaskp);
d1123 1
a1123 5
	if (ptp) {
		ptp->wire_count--;		/* dropping a PTE */
		/* Make sure that the PDE is flushed */
		if ((ptp->wire_count <= 1) && !(opte & PG_U))
			pmap_tlb_shootdown(pmap, va, opte, cpumaskp);
a1124 1
	}
d1130 1
a1130 6
	if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
		if (vm_physseg_find(atop(opte & PG_FRAME), &off) != -1)
			panic("pmap_remove_pte: managed page without "
			      "PG_PVLIST for 0x%lx", va);
#endif
a1131 9
	}

	bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
	if (bank == -1)
		panic("pmap_remove_pte: unmanaged page marked "
		    "PG_PVLIST, va = 0x%lx, pa = 0x%lx", va,
		    (u_long)(opte & PG_FRAME));
#endif
d1133 2
a1134 3
	/* sync R/M bits */
	vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
	pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap, va);
d1138 1
d1149 1
a1149 1
pmap_remove_pae(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
a1151 1
	boolean_t result;
a1154 1
	int32_t cpumask = 0;
d1156 2
a1157 4

	/*
	 * we lock in the pmap => pv_head direction
	 */
d1162 1
d1164 3
a1166 1
	 * removing one page?  take shortcut function.
d1168 2
d1171 1
a1171 81
	if (sva + PAGE_SIZE == eva) {

		if (pmap_valid_entry(PDE(pmap, pdei(sva)))) {

			/* PA of the PTP */
			ptppa = PDE(pmap, pdei(sva)) & PG_FRAME;

			/* get PTP if non-kernel mapping */

			if (pmap == pmap_kernel()) {
				/* we never free kernel PTPs */
				ptp = NULL;
			} else {
				if (pmap->pm_ptphint &&
				    VM_PAGE_TO_PHYS(pmap->pm_ptphint) ==
				    ptppa) {
					ptp = pmap->pm_ptphint;
				} else {
					ptp = PHYS_TO_VM_PAGE(ptppa);
#ifdef DIAGNOSTIC
					if (ptp == NULL)
						panic("pmap_remove: unmanaged "
						      "PTP detected");
#endif
				}
			}

			/* do it! */
			result = pmap_remove_pte_pae(pmap, ptp,
			    &ptes[atop(sva)], sva, &cpumask);

			/*
			 * if mapping removed and the PTP is no longer
			 * being used, free it!
			 */

			if (result && ptp && ptp->wire_count <= 1) {
				opte = i386_atomic_testset_uq(&PDE(pmap,
				    pdei(sva)), 0);	/* zap! */
#ifdef MULTIPROCESSOR
				/*
				 * XXXthorpej Redundant shootdown can happen
				 * here if we're using APTE space.
				 */
#endif
				pmap_tlb_shootdown(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + ptp->offset, opte,
				    &cpumask);
#ifdef MULTIPROCESSOR
				/*
				 * Always shoot down the pmap's self-mapping
				 * of the PTP.
				 * XXXthorpej Redundant shootdown can happen
				 * here if pmap == curpcb->pcb_pmap (not APTE
				 * space).
				 */
				pmap_tlb_shootdown(pmap,
				    ((vaddr_t)PTE_BASE) + ptp->offset, opte,
				    &cpumask);
#endif
				pmap->pm_stats.resident_count--;
				if (pmap->pm_ptphint == ptp)
					pmap->pm_ptphint =
					    RB_ROOT(&pmap->pm_obj.memt);
				ptp->wire_count = 0;
				/* Postpone free to after shootdown. */
				uvm_pagerealloc(ptp, NULL, 0);
				TAILQ_INSERT_TAIL(&empty_ptps, ptp, pageq);
			}
		}
		pmap_tlb_shootnow(cpumask);
		pmap_unmap_ptes_pae(pmap);		/* unlock pmap */
		while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
			TAILQ_REMOVE(&empty_ptps, ptp, pageq);
			uvm_pagefree(ptp);
		}
		return;
	}

	for (/* null */ ; sva < eva ; sva = blkendva) {

d1173 1
a1173 1
		blkendva = i386_round_pdr(sva+1);
d1191 1
a1191 1
		if (pdei(sva) == PDSLOT_PTE)
d1195 1
a1195 1
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
d1200 1
a1200 1
		ptppa = PDE(pmap, pdei(sva)) & PG_FRAME;
d1213 5
a1217 1
				if (ptp == NULL)
d1220 1
d1224 3
a1226 2
		pmap_remove_ptes_pae(pmap, ptp, (vaddr_t)&ptes[atop(sva)],
		    sva, blkendva, &cpumask);
d1230 2
a1231 1
			opte = i386_atomic_testset_uq(&PDE(pmap, pdei(sva)),0);
d1238 2
a1239 2
			pmap_tlb_shootdown(curpcb->pcb_pmap,
			    ((vaddr_t)ptes) + ptp->offset, opte, &cpumask);
d1247 2
a1248 2
			pmap_tlb_shootdown(pmap,
			    ((vaddr_t)PTE_BASE) + ptp->offset, opte, &cpumask);
a1250 3
			if (pmap->pm_ptphint == ptp)	/* update hint? */
				pmap->pm_ptphint =
				    RB_ROOT(&pmap->pm_obj.memt);
d1255 3
d1259 3
d1264 5
a1268 1
	pmap_tlb_shootnow(cpumask);
a1278 1
 * => we set pv_head => pmap locking
a1284 2
	int bank, off;
	struct pv_head *pvh;
a1286 1
	int32_t cpumask = 0;
d1290 1
a1290 4
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
a1291 1
	}
d1293 1
a1293 4
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return;
	}
d1295 3
a1297 1
	TAILQ_INIT(&empty_ptps);
a1298 1
	for (pve = pvh->pvh_list ; pve != NULL ; pve = pve->pv_next) {
d1300 1
a1300 19

#ifdef DIAGNOSTIC
		if (pve->pv_ptp && (PDE(pve->pv_pmap,
		    pdei(pve->pv_va)) & PG_FRAME) !=
		    VM_PAGE_TO_PHYS(pve->pv_ptp)) {
			printf("pmap_page_remove: pg=%p: va=%lx, pv_ptp=%p\n",
			       pg, pve->pv_va, pve->pv_ptp);
			printf("pmap_page_remove: PTP's phys addr: "
			       "actual=%llx, recorded=%llx\n",
			       (PDE(pve->pv_pmap, pdei(pve->pv_va)) &
				PG_FRAME), VM_PAGE_TO_PHYS(pve->pv_ptp));
			panic("pmap_page_remove: mapped managed page has "
			      "invalid pv_ptp field");
		}
#endif

		opte = ptes[atop(pve->pv_va)];
		ptes[atop(pve->pv_va)] = 0;			/* zap! */

a1304 5
		/* Shootdown only if referenced */
		if (opte & PG_U)
			pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va, opte,
			    &cpumask);

d1306 1
a1306 1
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
d1309 1
a1309 10
		if (pve->pv_ptp) {
			pve->pv_ptp->wire_count--;
			if (pve->pv_ptp->wire_count <= 1) {
				/*
				 * Do we have to shootdown the page just to
				 * get the pte out of the TLB ?
				 */
				if(!(opte & PG_U))
					pmap_tlb_shootdown(pve->pv_pmap,
					    pve->pv_va, opte, &cpumask);
d1313 3
a1315 3
				pmap_tlb_shootdown(curpcb->pcb_pmap,
				    ((vaddr_t)ptes) + pve->pv_ptp->offset,
				    opte, &cpumask);
d1321 3
a1323 3
				pmap_tlb_shootdown(pve->pv_pmap,
				    ((vaddr_t)PTE_BASE) + pve->pv_ptp->offset,
				    opte, &cpumask);
a1334 1
			}
d1336 1
d1339 3
a1341 3
	pmap_free_pvs(NULL, pvh->pvh_list);
	pvh->pvh_list = NULL;
	pmap_tlb_shootnow(cpumask);
a1363 3
	int bank, off;
	char *myattrs;
	struct pv_head *pvh;
d1366 1
d1368 1
a1368 6
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_test_attrs: unmanaged page?\n");
		return(FALSE);
	}
d1370 2
a1371 4
	/*
	 * before locking: see if attributes are already set and if so,
	 * return!
	 */
d1373 3
a1375 3
	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	if (*myattrs & testbits)
		return(TRUE);
a1376 9
	/* test to see if there is a list before bothering to lock */
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return(FALSE);
	}

	/* nope, gonna have to do it the hard way */
	for (pve = pvh->pvh_list; pve != NULL && (*myattrs & testbits) == 0;
	     pve = pve->pv_next) {
d1380 1
a1380 1
		*myattrs |= pte;
d1383 6
a1388 5
	/*
	 * note that we will exit the for loop with a non-null pve if
	 * we have found the bits we are testing for.
	 */
	return((*myattrs & testbits) != 0);
d1392 1
a1392 1
 * pmap_change_attrs: change a page's attributes
a1393 1
 * => we set pv_head => pmap locking
a1395 1

d1397 1
a1397 1
pmap_change_attrs_pae(struct vm_page *pg, int setbits, int clearbits)
a1398 3
	u_int32_t result;
	int bank, off;
	struct pv_head *pvh;
d1401 2
a1402 9
	char *myattrs;
	int32_t cpumask = 0;

	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_change_attrs: unmanaged page?\n");
		return(FALSE);
	}
d1404 4
a1407 12
	pvh = &vm_physmem[bank].pmseg.pvhead[off];

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	result = *myattrs & clearbits;
	*myattrs = (*myattrs | setbits) & ~clearbits;

	for (pve = pvh->pvh_list; pve != NULL; pve = pve->pv_next) {
#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(PDE(pve->pv_pmap, pdei(pve->pv_va))))
			panic("pmap_change_attrs: mapping without PTP "
			      "detected");
#endif
d1409 1
d1412 6
a1417 7
		result |= (npte & clearbits);
		npte = (npte | setbits) & ~(pt_entry_t)clearbits;
		if (ptes[atop(pve->pv_va)] != npte) {
			opte = i386_atomic_testset_uq(&ptes[atop(pve->pv_va)],
			    npte);
			pmap_tlb_shootdown(pve->pv_pmap,
			    atop(pve->pv_va), opte, &cpumask);
d1422 1
a1422 1
	pmap_tlb_shootnow(cpumask);
d1424 1
a1424 1
	return(result != 0);
d1427 1
d1452 1
d1457 1
a1457 1
	pt_entry_t *ptes, *spte, *epte, opte, npte;
d1460 2
a1461 1
	int32_t cpumask = 0;
d1469 2
a1470 1
	for (/* null */ ; sva < eva ; sva = blockend) {
d1472 2
a1473 1
		blockend = (sva & PD_MASK) + NBPD;
d1487 1
a1487 1
		if (pdei(sva) == PDSLOT_PTE)
d1491 1
a1491 1
		if (!pmap_valid_entry(PDE(pmap, pdei(sva))))
d1495 1
a1495 1
		if (sva < VM_MAXUSER_ADDRESS)
d1497 1
a1497 1
		else if (sva < VM_MAX_ADDRESS)
d1501 1
a1501 1
		spte = &ptes[atop(sva)];
d1504 1
a1504 1
		for (/*null */; spte < epte ; spte++, sva += PAGE_SIZE) {
d1512 2
a1513 4
				pmap_exec_account(pmap, sva, *spte, npte);
				opte = *spte;
				*spte = npte;
				pmap_tlb_shootdown(pmap, sva, opte, &cpumask);
d1517 4
d1522 1
a1522 1
	pmap_tlb_shootnow(cpumask);
a1581 1
 * => we set pmap => pv_head locking
d1590 1
a1590 3
	struct pv_head *pvh;
	struct pv_entry *pve;
	int bank, off, error;
d1592 4
d1597 5
a1601 13
#ifdef DIAGNOSTIC
	/* sanity check: totally out of range? */
	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter: too big");

	if (va == (vaddr_t) PDP_BASE || va == (vaddr_t) APDP_BASE)
		panic("pmap_enter: trying to map over PDP/APDP!");

	/* sanity check: kernel PTPs should already have been pre-allocated */
	if (va >= VM_MIN_KERNEL_ADDRESS &&
	    !pmap_valid_entry(PDE(pmap, pdei(va))))
		panic("pmap_enter: missing kernel PTP!");
#endif
d1611 1
a1611 1
		ptp = pmap_get_ptp_pae(pmap, pdei(va), FALSE);
d1620 3
a1629 1

d1631 1
a1631 1
		 * first, update pm_stats.  resident count will not
a1634 1

d1636 1
a1636 1
			pmap->pm_stats.wired_count++;
d1638 1
a1638 1
			pmap->pm_stats.wired_count--;
a1643 1

a1644 1

d1647 2
a1648 12
				bank = vm_physseg_find(atop(pa), &off);
#ifdef DIAGNOSTIC
				if (bank == -1)
					panic("pmap_enter: same pa PG_PVLIST "
					      "mapping with unmanaged page "
					      "pa = 0x%lx (0x%lx)", pa,
					      atop(pa));
#endif
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				vm_physmem[bank].pmseg.attrs[off] |= opte;
			} else {
				pvh = NULL;	/* ensure !PG_PVLIST */
d1663 4
a1666 12
			bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
#ifdef DIAGNOSTIC
			if (bank == -1)
				panic("pmap_enter: PG_PVLIST mapping with "
				      "unmanaged page "
				      "pa = 0x%lx (0x%lx)", pa, atop(pa));
#endif
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			pve = pmap_remove_pv(pvh, pmap, va);
			vm_physmem[bank].pmseg.attrs[off] |= opte;
		} else {
			pve = NULL;
d1669 1
a1669 2
		pve = NULL;
		pmap->pm_stats.resident_count++;
d1671 1
a1671 1
			pmap->pm_stats.wired_count++;
d1673 1
a1673 1
			ptp->wire_count++;      /* count # of valid entrys */
d1676 2
a1677 7
	/*
	 * at this point pm_stats has been updated.   pve is either NULL
	 * or points to a now-free pv_entry structure (the latter case is
	 * if we called pmap_remove_pv above).
	 *
	 * if this entry is to be on a pvlist, enter it now.
	 */
d1679 1
a1679 3
	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
d1681 1
a1681 1
			pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d1689 1
d1691 2
a1692 2
		/* lock pvh when adding */
		pmap_enter_pv(pvh, pve, pmap, va, ptp);
a1693 1

d1695 1
a1695 2
		pvh = NULL;		/* ensure !PG_PVLIST */
		if (pve)
d1697 1
d1702 1
a1702 1
	 * at this point pvh is !NULL if we want the PG_PVLIST bit set
a1706 2
	if (pvh)
		npte |= PG_PVLIST;
d1715 8
d1724 9
a1732 13
	ptes[atop(va)] = npte;			/* zap! */

	if ((opte & ~(pt_entry_t)(PG_M|PG_U)) != npte) {
#ifdef MULTIPROCESSOR
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pmap, va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		if (pmap_is_curpmap(pmap))
			pmap_update_pg(va);
#endif
d1739 5
d1786 1
a1786 1
			pmap_zero_phys(ptaddr);
d1788 2
a1789 1
			PDE(kpm, PDSLOT_KERN + nkpde) = ptaddr | PG_RW | PG_V;
d1802 1
a1802 1
		while (!pmap_alloc_ptp_pae(kpm, PDSLOT_KERN + nkpde, FALSE))
a1804 3
		/* PG_u not for kernel */
		PDE(kpm, PDSLOT_KERN + nkpde) &= ~PG_u;

d1818 93
a1932 4

	/*
	 * we lock in the pmap => pv_head direction
	 */
@


1.26
log
@delete all the simplelocks. ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.25 2014/11/19 20:09:01 mlarkin Exp $	*/
a416 10
/*
 * locking data structures
 */

#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */

#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

a1298 1
 	PMAP_MAP_TO_HEAD_LOCK();
a1375 1
		PMAP_MAP_TO_HEAD_UNLOCK();
a1468 1
	PMAP_MAP_TO_HEAD_UNLOCK();
a1506 3
	/* set pv_head => pmap locking */
	PMAP_HEAD_TO_MAP_LOCK();

a1581 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a1633 2
	PMAP_HEAD_TO_MAP_LOCK();

a1645 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a1673 1
	PMAP_HEAD_TO_MAP_LOCK();
a1699 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a1880 3
	/* get lock */
	PMAP_MAP_TO_HEAD_LOCK();

a2041 1
	PMAP_MAP_TO_HEAD_UNLOCK();
a2144 1
	PMAP_MAP_TO_HEAD_LOCK();
a2170 1
	PMAP_MAP_TO_HEAD_UNLOCK();
@


1.25
log
@
Remove some unused i386 pmap functions. Also fix two typos in comments
while I had the hood open.

ok deraadt@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.24 2014/07/11 16:35:40 jsg Exp $	*/
a629 1
		simple_lock(&pmap->pm_obj.vmobjlock);
a632 9
	/* need to lock both curpmap and pmap: use ordered locking */
	if ((unsigned) pmap < (unsigned) curpcb->pcb_pmap) {
		simple_lock(&pmap->pm_obj.vmobjlock);
		simple_lock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
	} else {
		simple_lock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
		simple_lock(&pmap->pm_obj.vmobjlock);
	}

d656 1
a656 3
	if (pmap_is_curpmap(pmap)) {
		simple_unlock(&pmap->pm_obj.vmobjlock);
	} else {
a663 2
		simple_unlock(&pmap->pm_obj.vmobjlock);
		simple_unlock(&curpcb->pcb_pmap->pm_obj.vmobjlock);
d840 1
a840 2
	if (cpv->pv_pmap == pmap_kernel() ||
	    !simple_lock_try(&cpv->pv_pmap->pm_obj.vmobjlock))
a863 1
		simple_unlock(&cpv->pv_pmap->pm_obj.vmobjlock);
a1001 1
	simple_lock(&pmaps_lock);
a1008 1
	simple_unlock(&pmaps_lock);
a1198 1
		simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
a1201 1
		simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
a1277 1
	simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
a1279 1
	simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
a1522 3
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

a1597 1
	simple_unlock(&pvh->pvh_lock);
a1651 2
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);
a1664 2

	simple_unlock(&pvh->pvh_lock);
a1695 2
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);
a1720 1
	simple_unlock(&pvh->pvh_lock);
a1959 1
				simple_lock(&pvh->pvh_lock);
a1960 1
				simple_unlock(&pvh->pvh_lock);
a1984 1
			simple_lock(&pvh->pvh_lock);
a1986 1
			simple_unlock(&pvh->pvh_lock);
a2096 1
	simple_lock(&kpm->pm_obj.vmobjlock);
a2131 1
		simple_lock(&pmaps_lock);
a2135 1
		simple_unlock(&pmaps_lock);
a2137 1
	simple_unlock(&kpm->pm_obj.vmobjlock);
@


1.24
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.23 2014/01/06 14:29:25 sf Exp $	*/
a563 1
pt_entry_t	*pmap_tmpmap_pvepte_pae(struct pv_entry *);
a564 1
void		 pmap_tmpunmap_pvepte_pae(struct pv_entry *);
a608 36
}

/*
 * pmap_tmpmap_pvepte: get a quick mapping of a PTE for a pv_entry
 *
 * => do NOT use this on kernel mappings [why?  because pv_ptp may be NULL]
 */

pt_entry_t *
pmap_tmpmap_pvepte_pae(struct pv_entry *pve)
{
#ifdef DIAGNOSTIC
	if (pve->pv_pmap == pmap_kernel())
		panic("pmap_tmpmap_pvepte: attempt to map kernel");
#endif

	/* is it current pmap?  use direct mapping... */
	if (pmap_is_curpmap(pve->pv_pmap))
		return(vtopte(pve->pv_va));

	return(((pt_entry_t *)pmap_tmpmap_pa_pae(VM_PAGE_TO_PHYS(pve->pv_ptp)))
	       + ptei((unsigned)pve->pv_va));
}

/*
 * pmap_tmpunmap_pvepte: release a mapping obtained with pmap_tmpmap_pvepte
 */

void
pmap_tmpunmap_pvepte_pae(struct pv_entry *pve)
{
	/* was it current pmap?   if so, return */
	if (pmap_is_curpmap(pve->pv_pmap))
		return;

	pmap_tmpunmap_pa_pae();
@


1.23
log
@Increase NPTECL, as cache-lines are 64-bytes nowadays.
Also move it from pmap.h to pmap.c because it is an internal detail.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.22 2013/02/09 20:37:41 miod Exp $	*/
a19 1
 *
a30 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.22
log
@Blame the right function when panic'ing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.21 2010/04/22 19:02:44 oga Exp $	*/
d519 1
a519 1
 * Number of PTE's per cache line. 8 byte pte, 32-byte cache line
d522 1
a522 1
#define	NPTECL		4
@


1.21
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.20 2009/08/06 15:28:14 oga Exp $	*/
d1033 1
a1033 1
		panic("pmap_pinit: kernel_map out of virtual space!");
@


1.20
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.19 2009/06/16 16:42:41 ariane Exp $	*/
d367 1
a367 1
 * 	call pae_pagealloc()
a555 7
 * a towards larger memory prioritised version opf uvm_pagealloc()
 */
#define	pae_pagealloc(obj, off, anon, flags) \
    uvm_pagealloc_strat((obj), (off), (anon), (flags), \
	UVM_PGA_STRAT_FALLBACK, VM_FREELIST_ABOVE4G)

/*
d797 1
a797 1
			ptp = pae_pagealloc(&kpm->pm_obj, va, NULL,
d973 1
a973 1
	ptp = pae_pagealloc(&pmap->pm_obj, ptp_i2o(pde_index), NULL,
@


1.19
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.15 2009/01/27 22:14:13 miod Exp $	*/
d1452 1
a1452 1
					    TAILQ_FIRST(&pmap->pm_obj.memq);
d1456 1
a1456 1
				TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
d1463 1
a1463 1
			TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1545 1
a1545 1
				    TAILQ_FIRST(&pmap->pm_obj.memq);
d1549 1
a1549 1
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
d1557 1
a1557 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1663 1
a1663 1
					    TAILQ_FIRST(&pve->pv_pmap->pm_obj.memq);
d1668 1
a1668 1
				    listq);
d1679 1
a1679 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
@


1.18
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.16 2009/06/01 17:42:33 ariane Exp $	*/
d1456 1
a1456 2
				TAILQ_INSERT_TAIL(&empty_ptps, ptp,
				    fq.queues.listq);
d1463 1
a1463 1
			TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
d1549 1
a1549 1
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, fq.queues.listq);
d1557 1
a1557 1
		TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
d1668 1
a1668 1
				    fq.queues.listq);
d1679 1
a1679 1
		TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
@


1.17
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1452 1
a1452 1
					    RB_ROOT(&pmap->pm_obj.memt);
d1546 1
a1546 1
				    RB_ROOT(&pmap->pm_obj.memt);
d1664 1
a1664 1
					    RB_ROOT(&pve->pv_pmap->pm_obj.memt);
@


1.16
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.15 2009/01/27 22:14:13 miod Exp $	*/
d1452 1
a1452 1
					    TAILQ_FIRST(&pmap->pm_obj.memq);
d1546 1
a1546 1
				    TAILQ_FIRST(&pmap->pm_obj.memq);
d1664 1
a1664 1
					    TAILQ_FIRST(&pve->pv_pmap->pm_obj.memq);
@


1.15
log
@Get rid of the last traces of uvm.pager_[se]va
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.14 2007/11/16 16:16:06 deraadt Exp $	*/
d1456 2
a1457 1
				TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
d1464 1
a1464 1
			TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1550 1
a1550 1
			TAILQ_INSERT_TAIL(&empty_ptps, ptp, listq);
d1558 1
a1558 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1669 1
a1669 1
				    listq);
d1680 1
a1680 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
@


1.14
log
@fix the bus_space #define nightmare, so that amd64 and i386 are much more
uniform. as a result shared code like acpi needs less #ifdef's
ok marco kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.13 2007/07/20 19:48:15 mk Exp $	*/
a1603 2
		if (pve->pv_va >= uvm.pager_sva && pve->pv_va < uvm.pager_eva)
			printf("pmap_page_remove: found pager VA on pv_list\n");
@


1.13
log
@More code that tests for CPUCLASS_386 that can go away.  Pointed out by
Charles Longeau, thanks.

Fix an indentation nit while there.

ok toby
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.12 2007/04/19 16:15:41 art Exp $	*/
d533 1
a533 1
 * I386_MAXPROCS*NPTECL array of PTE's, to avoid cache line thrashing
@


1.12
log
@Do the late freeing of ptps in pae pmap as well.

From mickey.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.11 2007/04/13 18:57:49 art Exp $	*/
d1149 1
a1149 2
	*zpte = (pa & PG_FRAME) | PG_V | PG_RW |	/* map in */
	    ((cpu_class != CPUCLASS_386) ? PG_N : 0);
d1151 1
a1151 1
	pagezero(zerova, PAGE_SIZE);				/* zero */
@


1.11
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.10 2007/04/04 17:44:45 art Exp $	*/
d1376 1
d1382 2
d1455 3
a1457 1
				uvm_pagefree(ptp);
d1463 4
d1548 3
a1550 1
			uvm_pagefree(ptp);
d1557 4
d1578 2
d1593 2
d1668 4
a1671 1
				uvm_pagefree(pve->pv_ptp);
d1681 4
@


1.10
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.9 2007/03/26 08:43:34 art Exp $	*/
d986 1
a986 1
	ptp->pg_flags &= ~PG_BUSY;	/* never busy */
@


1.9
log
@Rip out the KERN_ error codes.
ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.8 2007/03/18 14:23:57 mickey Exp $	*/
d986 1
a986 1
	ptp->flags &= ~PG_BUSY;	/* never busy */
@


1.8
log
@do not steal page table pages on page allocation failure -- it can be dealt w/ in a more simple way as other archs do; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.7 2007/02/03 16:48:23 miod Exp $	*/
d1979 1
a1979 1
				error = KERN_RESOURCE_SHORTAGE;
d2080 1
a2080 1
					error = KERN_RESOURCE_SHORTAGE;
@


1.7
log
@Remove unused functionality from lockmgr():
- LK_EXCLUPGRADE is never used.
- LK_REENABLE is never used.
- LK_SETRECURSE is never used. Because of this, the lk_recurselevel
  field is always zero, so it can be removed to.
- the spinlock version (and LK_SPIN) is never used, since it was decided
  to use different locking structure for MP-safe protection.

Tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.6 2006/11/29 22:40:13 miod Exp $	*/
a553 1
extern struct pmap *pmaps_hand;
a576 1
struct vm_page	*pmap_steal_ptp_pae(struct uvm_object *, vaddr_t);
d982 2
a983 10
	if (ptp == NULL) {
		if (just_try)
			return(NULL);
		ptp = pmap_steal_ptp_pae(&pmap->pm_obj, ptp_i2o(pde_index));
		if (ptp == NULL) {
			return (NULL);
		}
		/* stole one; zero it. */
		pmap_zero_page(ptp);
	}
a995 106
 * pmap_steal_ptp: steal a PTP from any pmap that we can access
 *
 * => obj is locked by caller.
 * => we can throw away mappings at this level (except in the kernel's pmap)
 * => stolen PTP is placed in <obj,offset> pmap
 * => we lock pv_head's
 * => hopefully, this function will be seldom used [much better to have
 *	enough free pages around for us to allocate off the free page list]
 */

struct vm_page *
pmap_steal_ptp_pae(struct uvm_object *obj, vaddr_t offset)
{
	struct vm_page *ptp = NULL;
	struct pmap *firstpmap;
	struct uvm_object *curobj;
	pt_entry_t *ptes;
	int idx, lcv;
	boolean_t caller_locked, we_locked;
	int32_t cpumask = 0;

	simple_lock(&pmaps_lock);
	if (pmaps_hand == NULL)
		pmaps_hand = LIST_FIRST(&pmaps);
	firstpmap = pmaps_hand;

	do { /* while we haven't looped back around to firstpmap */

		curobj = &pmaps_hand->pm_obj;
		we_locked = FALSE;
		caller_locked = (curobj == obj);
		if (!caller_locked) {
			we_locked = simple_lock_try(&curobj->vmobjlock);
		}
		if (caller_locked || we_locked) {
			TAILQ_FOREACH(ptp, &curobj->memq, listq) {

				/*
				 * might have found a PTP we can steal
				 * (unless it has wired pages).
				 */

				idx = ptp_o2i(ptp->offset);
#ifdef DIAGNOSTIC
				if (VM_PAGE_TO_PHYS(ptp) !=
				    (PDE(pmaps_hand, idx) & PG_FRAME))
					panic("pmap_steal_ptp: PTP mismatch!");
#endif

				ptes = (pt_entry_t *)
				    pmap_tmpmap_pa_pae(VM_PAGE_TO_PHYS(ptp));
				for (lcv = 0 ; lcv < PTES_PER_PTP ; lcv++)
					if ((ptes[lcv] & (PG_V|PG_W)) ==
					    (PG_V|PG_W))
						break;
				if (lcv == PTES_PER_PTP)
					pmap_remove_ptes_pae(pmaps_hand, ptp,
					    (vaddr_t)ptes, ptp_i2v(idx),
					    ptp_i2v(idx+1), &cpumask);
				pmap_tmpunmap_pa_pae();

				if (lcv != PTES_PER_PTP)
					/* wired, try next PTP */
					continue;

				/*
				 * got it!!!
				 */

				PDE(pmaps_hand, idx) = 0;	/* zap! */
				pmaps_hand->pm_stats.resident_count--;
#ifdef MULTIPROCESSOR
				pmap_apte_flush(pmaps_hand);
#else
				if (pmap_is_curpmap(pmaps_hand))
					pmap_apte_flush(pmaps_hand);
				else if (pmap_valid_entry(*APDP_PDE) &&
				    (*APDP_PDE & PG_FRAME) ==
				    pmaps_hand->pm_pdidx[0])
					pmap_update_pg(((vaddr_t)APTE_BASE) +
						       ptp->offset);
#endif

				/* put it in our pmap! */
				uvm_pagerealloc(ptp, obj, offset);
				break;	/* break out of "for" loop */
			}
			if (we_locked) {
				simple_unlock(&curobj->vmobjlock);
			}
		}

		/* advance the pmaps_hand */
		pmaps_hand = LIST_NEXT(pmaps_hand, pm_list);
		if (pmaps_hand == NULL) {
			pmaps_hand = LIST_FIRST(&pmaps);
		}

	} while (ptp == NULL && pmaps_hand != firstpmap);

	simple_unlock(&pmaps_lock);
	pmap_tlb_shootnow(cpumask);
	return(ptp);
}

/*
d1025 1
a1025 1
	return(pmap_alloc_ptp_pae(pmap, pde_index, just_try));
d2192 2
a2193 3
		if (pmap_alloc_ptp_pae(kpm, PDSLOT_KERN + nkpde, FALSE) == NULL) {
			panic("pmap_growkernel: alloc ptp failed");
		}
@


1.6
log
@Nuke all commons but one, and enable --warn-common in LINKFLAGS.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.5 2006/08/25 13:04:16 mickey Exp $	*/
a399 15
 * "normal" locks:
 *
 *  - pmap_main_lock
 *    this lock is used to prevent deadlock and/or provide mutex
 *    access to the pmap system.   most operations lock the pmap
 *    structure first, then they lock the pv_lists (if needed).
 *    however, some operations such as pmap_page_protect lock
 *    the pv_lists and then lock pmaps.   in order to prevent a
 *    cycle, we require a mutex lock when locking the pv_lists
 *    first.   thus, the "pmap = >pv_list" lockers must gain a
 *    read-lock on pmap_main_lock before locking the pmap.   and
 *    the "pv_list => pmap" lockers must gain a write-lock on
 *    pmap_main_lock before locking.    since only one thread
 *    can write-lock a lock at a time, this provides mutex.
 *
a427 16
#if defined(MULTIPROCESSOR) && 0

extern struct lock pmap_main_lock;

#define PMAP_MAP_TO_HEAD_LOCK() \
     spinlockmgr(&pmap_main_lock, LK_SHARED, (void *) 0)
#define PMAP_MAP_TO_HEAD_UNLOCK() \
     spinlockmgr(&pmap_main_lock, LK_RELEASE, (void *) 0)

#define PMAP_HEAD_TO_MAP_LOCK() \
     spinlockmgr(&pmap_main_lock, LK_EXCLUSIVE, (void *) 0)
#define PMAP_HEAD_TO_MAP_UNLOCK() \
     spinlockmgr(&pmap_main_lock, LK_RELEASE, (void *) 0)

#else

a432 2

#endif
@


1.5
log
@forget about >4g memory if not switched to pae in order to disable all the bounce-buffering logic
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.4 2006/08/17 17:09:51 mickey Exp $	*/
a441 3

struct simplelock pvalloc_lock;
struct simplelock pmaps_lock;
@


1.4
log
@prefer higher phys memory for page tables
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.3 2006/05/19 20:53:31 brad Exp $	*/
d822 2
a823 1
	if (!cpu_pae || avail_end >= avail_end2 || !(cpu_feature & CPUID_PAE))
d825 1
@


1.3
log
@clean out some NetBSD cruft from the pmap code.

ok mickey@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.2 2006/05/11 13:21:11 mickey Exp $	*/
d367 1
a367 1
 * 	call uvm_pagealloc()
d593 7
d840 1
a840 1
			ptp = uvm_pagealloc(&kpm->pm_obj, va, NULL,
a870 1
		avail_end = avail_end2;
d1016 1
a1016 1
	ptp = uvm_pagealloc(&pmap->pm_obj, ptp_i2o(pde_index), NULL,
@


1.2
log
@kill trainling spaces
@
text
@d1 1
a1 1
/*	$OpenBSD: pmapae.c,v 1.1 2006/04/27 15:37:51 mickey Exp $	*/
a102 4
#ifdef __NetBSD__
#include <machine/isa_machdep.h>
#endif
#ifdef __OpenBSD__
a104 1
#endif
@


1.1
log
@implement separate PAE pmap that allows access to 64g of physmem
if supported by the cpu(s). currently not enabled by default and
not compiled into ramdisks. this grows paddr_t to 64bit but yet
leaves bus_addr_t at 32bits. measures are taken to favour dmaable
memory allocation from below 4g line such that buffer cache is
already allocated form below, pool backend allocator prefers lower
memory and then finally bounce buffers are used as last resort.
PAE is engaged only if global variable cpu_pae is manually set
to non-zero and there is physical memory present above 4g.
simplify pcibios address math to use u_long as we always will
be in the 32bit space.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d156 1
a156 1
 * 
d486 1
a486 1
#define	NBPD		(1U << PDSHIFT)	/* # bytes mapped by PD (2MB) */ 
d494 1
a494 1
#define	PDSLOT_KERN	(1664U)	/* 1664: start of kernel space */  
@

