head	1.73;
access;
symbols
	OPENBSD_6_1_BASE:1.73
	OPENBSD_6_0:1.73.0.2
	OPENBSD_6_0_BASE:1.73
	OPENBSD_5_9:1.72.0.4
	OPENBSD_5_9_BASE:1.72
	OPENBSD_5_8:1.72.0.6
	OPENBSD_5_8_BASE:1.72
	OPENBSD_5_7:1.72.0.2
	OPENBSD_5_7_BASE:1.72
	OPENBSD_5_6:1.69.0.4
	OPENBSD_5_6_BASE:1.69
	OPENBSD_5_5:1.67.0.4
	OPENBSD_5_5_BASE:1.67
	OPENBSD_5_4:1.66.0.4
	OPENBSD_5_4_BASE:1.66
	OPENBSD_5_3:1.66.0.2
	OPENBSD_5_3_BASE:1.66
	OPENBSD_5_2:1.64.0.6
	OPENBSD_5_2_BASE:1.64
	OPENBSD_5_1_BASE:1.64
	OPENBSD_5_1:1.64.0.4
	OPENBSD_5_0:1.64.0.2
	OPENBSD_5_0_BASE:1.64
	OPENBSD_4_9:1.62.0.4
	OPENBSD_4_9_BASE:1.62
	OPENBSD_4_8:1.62.0.2
	OPENBSD_4_8_BASE:1.62
	OPENBSD_4_7:1.61.0.2
	OPENBSD_4_7_BASE:1.61
	OPENBSD_4_6:1.60.0.4
	OPENBSD_4_6_BASE:1.60
	OPENBSD_4_5:1.53.0.2
	OPENBSD_4_5_BASE:1.53
	OPENBSD_4_4:1.50.0.2
	OPENBSD_4_4_BASE:1.50
	OPENBSD_4_3:1.49.0.2
	OPENBSD_4_3_BASE:1.49
	OPENBSD_4_2:1.47.0.2
	OPENBSD_4_2_BASE:1.47
	OPENBSD_4_1:1.44.0.2
	OPENBSD_4_1_BASE:1.44
	OPENBSD_4_0:1.43.0.2
	OPENBSD_4_0_BASE:1.43
	OPENBSD_3_9:1.38.0.4
	OPENBSD_3_9_BASE:1.38
	OPENBSD_3_8:1.38.0.2
	OPENBSD_3_8_BASE:1.38
	OPENBSD_3_7:1.36.0.2
	OPENBSD_3_7_BASE:1.36
	OPENBSD_3_6:1.35.0.6
	OPENBSD_3_6_BASE:1.35
	SMP_SYNC_A:1.35
	SMP_SYNC_B:1.35
	OPENBSD_3_5:1.35.0.2
	OPENBSD_3_5_BASE:1.35
	OPENBSD_3_4:1.32.0.2
	OPENBSD_3_4_BASE:1.32
	UBC_SYNC_A:1.28
	OPENBSD_3_3:1.28.0.2
	OPENBSD_3_3_BASE:1.28
	OPENBSD_3_2:1.19.0.2
	OPENBSD_3_2_BASE:1.19
	OPENBSD_3_1:1.16.0.2
	OPENBSD_3_1_BASE:1.16
	UBC_SYNC_B:1.23
	UBC:1.7.0.2
	UBC_BASE:1.7
	SMP:1.5.0.4
	OPENBSD_3_0:1.5.0.2
	OPENBSD_3_0_BASE:1.5;
locks; strict;
comment	@ * @;


1.73
date	2016.05.04.18.26.12;	author kettenis;	state Exp;
branches;
next	1.72;
commitid	M6mvq0S4zf7jMQ41;

1.72
date	2015.01.09.14.23.25;	author kettenis;	state Exp;
branches;
next	1.71;
commitid	prUrIemnAyVP7Kby;

1.71
date	2014.11.16.12.30.59;	author deraadt;	state Exp;
branches;
next	1.70;
commitid	yv0ECmCdICvq576h;

1.70
date	2014.10.26.18.17.16;	author kettenis;	state Exp;
branches;
next	1.69;
commitid	MP2HqmDBczRdpLBw;

1.69
date	2014.07.12.18.44.43;	author tedu;	state Exp;
branches;
next	1.68;
commitid	uKVPYMN2MLxdZxzH;

1.68
date	2014.05.10.12.20.38;	author kettenis;	state Exp;
branches;
next	1.67;

1.67
date	2014.01.22.10.52.35;	author kettenis;	state Exp;
branches;
next	1.66;

1.66
date	2013.01.15.03.14.01;	author kettenis;	state Exp;
branches;
next	1.65;

1.65
date	2012.08.17.20.46.50;	author kettenis;	state Exp;
branches;
next	1.64;

1.64
date	2011.05.18.23.36.31;	author ariane;	state Exp;
branches;
next	1.63;

1.63
date	2011.04.07.15.30.16;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2010.04.20.23.26.59;	author deraadt;	state Exp;
branches;
next	1.61;

1.61
date	2009.08.09.13.35.43;	author oga;	state Exp;
branches;
next	1.60;

1.60
date	2009.05.04.16.48.37;	author oga;	state Exp;
branches;
next	1.59;

1.59
date	2009.05.03.13.44.05;	author kettenis;	state Exp;
branches;
next	1.58;

1.58
date	2009.05.02.18.39.27;	author kettenis;	state Exp;
branches;
next	1.57;

1.57
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.56;

1.56
date	2009.04.05.21.57.41;	author oga;	state Exp;
branches;
next	1.55;

1.55
date	2009.03.16.21.00.48;	author oga;	state Exp;
branches;
next	1.54;

1.54
date	2009.03.16.19.45.32;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2009.01.02.20.01.45;	author kettenis;	state Exp;
branches;
next	1.52;

1.52
date	2008.11.25.16.31.19;	author kettenis;	state Exp;
branches;
next	1.51;

1.51
date	2008.08.11.21.28.53;	author kettenis;	state Exp;
branches;
next	1.50;

1.50
date	2008.07.12.13.08.04;	author kettenis;	state Exp;
branches;
next	1.49;

1.49
date	2007.12.15.18.08.07;	author deraadt;	state Exp;
branches;
next	1.48;

1.48
date	2007.12.05.21.15.46;	author deraadt;	state Exp;
branches;
next	1.47;

1.47
date	2007.05.29.09.53.59;	author sobrado;	state Exp;
branches;
next	1.46;

1.46
date	2007.04.04.20.09.37;	author kettenis;	state Exp;
branches;
next	1.45;

1.45
date	2007.04.02.13.23.46;	author claudio;	state Exp;
branches;
next	1.44;

1.44
date	2007.01.26.16.53.28;	author tsi;	state Exp;
branches;
next	1.43;

1.43
date	2006.09.01.20.07.57;	author miod;	state Exp;
branches;
next	1.42;

1.42
date	2006.08.15.20.55.57;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2006.07.02.06.22.03;	author dlg;	state Exp;
branches;
next	1.40;

1.40
date	2006.07.01.16.41.26;	author deraadt;	state Exp;
branches;
next	1.39;

1.39
date	2006.06.28.20.09.15;	author deraadt;	state Exp;
branches;
next	1.38;

1.38
date	2005.06.07.20.40.01;	author kurt;	state Exp;
branches;
next	1.37;

1.37
date	2005.06.02.01.27.39;	author mickey;	state Exp;
branches;
next	1.36;

1.36
date	2004.12.25.23.02.25;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2004.03.19.21.04.00;	author miod;	state Exp;
branches;
next	1.34;

1.34
date	2003.12.20.20.08.17;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2003.12.04.21.13.37;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2003.06.11.04.00.11;	author henric;	state Exp;
branches;
next	1.31;

1.31
date	2003.06.11.03.16.12;	author henric;	state Exp;
branches;
next	1.30;

1.30
date	2003.06.11.03.07.41;	author henric;	state Exp;
branches;
next	1.29;

1.29
date	2003.05.22.21.16.29;	author henric;	state Exp;
branches;
next	1.28;

1.28
date	2003.03.06.08.26.08;	author henric;	state Exp;
branches;
next	1.27;

1.27
date	2003.02.22.23.51.39;	author jason;	state Exp;
branches;
next	1.26;

1.26
date	2003.02.21.00.47.56;	author jason;	state Exp;
branches;
next	1.25;

1.25
date	2003.02.21.00.01.17;	author jason;	state Exp;
branches;
next	1.24;

1.24
date	2003.02.17.01.29.20;	author henric;	state Exp;
branches;
next	1.23;

1.23
date	2002.10.12.01.09.43;	author krw;	state Exp;
branches;
next	1.22;

1.22
date	2002.10.07.18.35.56;	author mickey;	state Exp;
branches;
next	1.21;

1.21
date	2002.10.06.22.06.15;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2002.10.04.01.55.44;	author jason;	state Exp;
branches;
next	1.19;

1.19
date	2002.08.19.20.02.30;	author jason;	state Exp;
branches;
next	1.18;

1.18
date	2002.07.24.19.07.43;	author jason;	state Exp;
branches;
next	1.17;

1.17
date	2002.05.13.19.43.48;	author jason;	state Exp;
branches;
next	1.16;

1.16
date	2002.03.26.18.13.11;	author jason;	state Exp;
branches;
next	1.15;

1.15
date	2002.03.14.01.26.44;	author millert;	state Exp;
branches;
next	1.14;

1.14
date	2002.03.12.19.41.05;	author jason;	state Exp;
branches;
next	1.13;

1.13
date	2002.03.07.17.58.24;	author jason;	state Exp;
branches;
next	1.12;

1.12
date	2002.03.07.17.44.10;	author jason;	state Exp;
branches;
next	1.11;

1.11
date	2002.02.22.21.53.21;	author jason;	state Exp;
branches;
next	1.10;

1.10
date	2002.02.22.20.21.46;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	2002.02.22.19.35.02;	author jason;	state Exp;
branches;
next	1.8;

1.8
date	2002.02.22.16.11.59;	author jason;	state Exp;
branches;
next	1.7;

1.7
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.7.2.1;
next	1.6;

1.6
date	2001.11.06.19.53.16;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2001.10.15.03.36.16;	author jason;	state Exp;
branches
	1.5.4.1;
next	1.4;

1.4
date	2001.09.26.19.31.57;	author jason;	state Exp;
branches;
next	1.3;

1.3
date	2001.09.19.20.50.58;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2001.08.18.21.30.00;	author jason;	state Exp;
branches;
next	1.1;

1.1
date	2001.08.18.21.24.01;	author jason;	state Exp;
branches;
next	;

1.5.4.1
date	2001.10.31.03.07.57;	author nate;	state Exp;
branches;
next	1.5.4.2;

1.5.4.2
date	2001.11.13.21.04.17;	author niklas;	state Exp;
branches;
next	1.5.4.3;

1.5.4.3
date	2002.03.06.02.04.46;	author niklas;	state Exp;
branches;
next	1.5.4.4;

1.5.4.4
date	2002.03.28.10.57.11;	author niklas;	state Exp;
branches;
next	1.5.4.5;

1.5.4.5
date	2003.03.27.23.42.35;	author niklas;	state Exp;
branches;
next	1.5.4.6;

1.5.4.6
date	2003.06.07.11.14.44;	author ho;	state Exp;
branches;
next	1.5.4.7;

1.5.4.7
date	2004.02.19.10.49.59;	author niklas;	state Exp;
branches;
next	1.5.4.8;

1.5.4.8
date	2004.06.05.23.10.59;	author niklas;	state Exp;
branches;
next	;

1.7.2.1
date	2002.06.11.03.38.42;	author art;	state Exp;
branches;
next	1.7.2.2;

1.7.2.2
date	2002.10.29.00.28.11;	author art;	state Exp;
branches;
next	1.7.2.3;

1.7.2.3
date	2003.05.19.21.46.57;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.73
log
@Some hardware (such as the onboard dc(4) of the Netra X1) has a broken DMA
engine that might attempt to read beyond the end of the buffer that was
programmed.  The IOMMU catches this "DMA overrun" and throws an unrecoverable
error at us, at which point we have no choice but to panic.  To avoid this
implement a BUS_DMA_OVERRUN flag that maps an additional scratch page at the
end of the vdma address range.  DMA requests will spill over into this page,
which just returns zeroes.

Thanks to matthieu@@ for giving me access to a machine with the problem.

ok deraadt@@, beck@@
@
text
@/*	$OpenBSD: iommu.c,v 1.72 2015/01/09 14:23:25 kettenis Exp $	*/
/*	$NetBSD: iommu.c,v 1.47 2002/02/08 20:03:45 eeh Exp $	*/

/*
 * Copyright (c) 2003 Henric Jungheim
 * Copyright (c) 2001, 2002 Eduardo Horvath
 * Copyright (c) 1999, 2000 Matthew R. Green
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
 * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED
 * AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

/*
 * UltraSPARC IOMMU support; used by both the sbus and pci code.
 */
#include <sys/param.h>
#include <sys/extent.h>
#include <sys/malloc.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/device.h>
#include <sys/mbuf.h>

#include <uvm/uvm_extern.h>

#include <machine/bus.h>
#include <sparc64/sparc64/cache.h>
#include <sparc64/dev/iommureg.h>
#include <sparc64/dev/iommuvar.h>

#include <machine/autoconf.h>
#include <machine/cpu.h>

#ifdef DDB
#include <machine/db_machdep.h>
#include <ddb/db_sym.h>
#include <ddb/db_extern.h>
#endif

#ifdef DEBUG
#define IDB_BUSDMA	0x1
#define IDB_IOMMU	0x2
#define IDB_INFO	0x4
#define IDB_SYNC	0x8
#define IDB_XXX		0x10
#define IDB_PRINT_MAP	0x20
#define IDB_BREAK	0x40
int iommudebug = IDB_INFO;
#define DPRINTF(l, s)   do { if (iommudebug & l) printf s; } while (0)
#else
#define DPRINTF(l, s)
#endif

void iommu_enter(struct iommu_state *, struct strbuf_ctl *, bus_addr_t,
    paddr_t, int);
void iommu_remove(struct iommu_state *, struct strbuf_ctl *, bus_addr_t);
int iommu_dvmamap_sync_range(struct strbuf_ctl*, bus_addr_t, bus_size_t);
int iommu_strbuf_flush_done(struct iommu_map_state *);
int iommu_dvmamap_load_seg(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t, bus_dma_segment_t *, int, int, bus_size_t, bus_size_t);
int iommu_dvmamap_load_mlist(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t, struct pglist *, int, bus_size_t, bus_size_t);
int iommu_dvmamap_validate_map(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t);
void iommu_dvmamap_print_map(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t);
int iommu_dvmamap_append_range(bus_dma_tag_t, bus_dmamap_t, paddr_t,
    bus_size_t, int, bus_size_t);
int64_t iommu_tsb_entry(struct iommu_state *, bus_addr_t);
void strbuf_reset(struct strbuf_ctl *);
int iommu_iomap_insert_page(struct iommu_map_state *, paddr_t);
bus_addr_t iommu_iomap_translate(struct iommu_map_state *, paddr_t);
void iommu_iomap_load_map(struct iommu_state *, struct iommu_map_state *,
    bus_addr_t, int);
void iommu_iomap_unload_map(struct iommu_state *, struct iommu_map_state *);
struct iommu_map_state *iommu_iomap_create(int);
void iommu_iomap_destroy(struct iommu_map_state *);
void iommu_iomap_clear_pages(struct iommu_map_state *);
void _iommu_dvmamap_sync(bus_dma_tag_t, bus_dma_tag_t, bus_dmamap_t,
    bus_addr_t, bus_size_t, int);

/*
 * Initiate an STC entry flush.
 */
static inline void
iommu_strbuf_flush(struct strbuf_ctl *sb, bus_addr_t va)
{
#ifdef DEBUG
	if (sb->sb_flush == NULL) {
		printf("iommu_strbuf_flush: attempting to flush w/o STC\n");
		return;
	}
#endif

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_pgflush), va);
}

/*
 * initialise the UltraSPARC IOMMU (SBus or PCI):
 *	- allocate and setup the iotsb.
 *	- enable the IOMMU
 *	- initialise the streaming buffers (if they exist)
 *	- create a private DVMA map.
 */
void
iommu_init(char *name, struct iommu_state *is, int tsbsize, u_int32_t iovabase)
{
	psize_t size;
	vaddr_t va;
	paddr_t pa;
	struct vm_page *m;
	struct pglist mlist;

	/*
	 * Setup the iommu.
	 *
	 * The sun4u iommu is part of the SBus or PCI controller so we will
	 * deal with it here..
	 *
	 * For sysio and psycho/psycho+ the IOMMU address space always ends at
	 * 0xffffe000, but the starting address depends on the size of the
	 * map.  The map size is 1024 * 2 ^ is->is_tsbsize entries, where each
	 * entry is 8 bytes.  The start of the map can be calculated by
	 * (0xffffe000 << (8 + is->is_tsbsize)).
	 *
	 * But sabre and hummingbird use a different scheme that seems to
	 * be hard-wired, so we read the start and size from the PROM and
	 * just use those values.
	 */
	if (strncmp(name, "pyro", 4) == 0) {
		is->is_cr = IOMMUREG_READ(is, iommu_cr);
		is->is_cr &= ~IOMMUCR_FIRE_BE;
		is->is_cr |= (IOMMUCR_FIRE_SE | IOMMUCR_FIRE_CM_EN |
		    IOMMUCR_FIRE_TE);
	} else 
		is->is_cr = IOMMUCR_EN;
	is->is_tsbsize = tsbsize;
	if (iovabase == (u_int32_t)-1) {
		is->is_dvmabase = IOTSB_VSTART(is->is_tsbsize);
		is->is_dvmaend = IOTSB_VEND;
	} else {
		is->is_dvmabase = iovabase;
		is->is_dvmaend = iovabase + IOTSB_VSIZE(tsbsize) - 1;
	}

	/*
	 * Allocate memory for I/O pagetables.  They need to be physically
	 * contiguous.
	 */

	size = PAGE_SIZE << is->is_tsbsize;
	TAILQ_INIT(&mlist);
	if (uvm_pglistalloc((psize_t)size, (paddr_t)0, (paddr_t)-1,
	    (paddr_t)PAGE_SIZE, (paddr_t)0, &mlist, 1, UVM_PLA_NOWAIT) != 0)
		panic("iommu_init: no memory");

	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, &kd_nowait);
	if (va == 0)
		panic("iommu_init: no memory");
	is->is_tsb = (int64_t *)va;

	m = TAILQ_FIRST(&mlist);
	is->is_ptsb = VM_PAGE_TO_PHYS(m);

	/* Map the pages */
	for (; m != NULL; m = TAILQ_NEXT(m,pageq)) {
		pa = VM_PAGE_TO_PHYS(m);
		pmap_enter(pmap_kernel(), va, pa | PMAP_NVC,
		    PROT_READ | PROT_WRITE,
		    PROT_READ | PROT_WRITE | PMAP_WIRED);
		va += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	memset(is->is_tsb, 0, size);

	TAILQ_INIT(&mlist);
	if (uvm_pglistalloc(PAGE_SIZE, 0, -1, PAGE_SIZE, 0, &mlist, 1,
	    UVM_PLA_NOWAIT | UVM_PLA_ZERO) != 0)
		panic("%s: no memory", __func__);
	m = TAILQ_FIRST(&mlist);
	is->is_scratch = VM_PAGE_TO_PHYS(m);

#ifdef DEBUG
	if (iommudebug & IDB_INFO) {
		/* Probe the iommu */
		/* The address or contents of the regs...? */
		printf("iommu regs at: cr=%lx tsb=%lx flush=%lx\n",
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_cr),
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_tsb),
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_flush));
		printf("iommu cr=%llx tsb=%llx\n",
		    IOMMUREG_READ(is, iommu_cr),
		    IOMMUREG_READ(is, iommu_tsb));
		printf("TSB base %p phys %llx\n",
		    (void *)is->is_tsb, (unsigned long long)is->is_ptsb);
		delay(1000000); /* 1 s */
	}
#endif

	/*
	 * Now all the hardware's working we need to allocate a dvma map.
	 */
	printf("dvma map %x-%x", is->is_dvmabase, is->is_dvmaend);
#ifdef DEBUG
	printf(", iotdb %llx-%llx",
	    (unsigned long long)is->is_ptsb,
	    (unsigned long long)(is->is_ptsb + size));
#endif
	is->is_dvmamap = extent_create(name,
	    is->is_dvmabase, (u_long)is->is_dvmaend + 1,
	    M_DEVBUF, NULL, 0, EX_NOCOALESCE);
	mtx_init(&is->is_mtx, IPL_HIGH);

	/*
	 * Set the TSB size.  The relevant bits were moved to the TSB
	 * base register in the PCIe host bridges.
	 */
	if (strncmp(name, "pyro", 4) == 0)
		is->is_ptsb |= is->is_tsbsize;
	else
		is->is_cr |= (is->is_tsbsize << 16);

	/*
	 * Now actually start up the IOMMU.
	 */
	iommu_reset(is);
	printf("\n");
}

/*
 * Streaming buffers don't exist on the UltraSPARC IIi/e; we should have
 * detected that already and disabled them.  If not, we will notice that
 * they aren't there when the STRBUF_EN bit does not remain.
 */
void
iommu_reset(struct iommu_state *is)
{
	int i;

	IOMMUREG_WRITE(is, iommu_tsb, is->is_ptsb);

	/* Enable IOMMU */
	IOMMUREG_WRITE(is, iommu_cr, is->is_cr);

	for (i = 0; i < 2; ++i) {
		struct strbuf_ctl *sb = is->is_sb[i];

		if (sb == NULL)
			continue;

		sb->sb_iommu = is;
		strbuf_reset(sb);

		if (sb->sb_flush)
			printf(", STC%d enabled", i);
	}

	if (is->is_flags & IOMMU_FLUSH_CACHE)
		IOMMUREG_WRITE(is, iommu_cache_invalidate, -1ULL);
}

/*
 * Initialize one STC.
 */
void
strbuf_reset(struct strbuf_ctl *sb)
{
	if(sb->sb_flush == NULL)
		return;

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_ctl), STRBUF_EN);

	membar(Lookaside);

	/* No streaming buffers? Disable them */
	if (bus_space_read_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_ctl)) == 0) {
		sb->sb_flush = NULL;
	} else {
		/*
		 * locate the pa of the flush buffer
		 */
		if (pmap_extract(pmap_kernel(),
		    (vaddr_t)sb->sb_flush, &sb->sb_flushpa) == FALSE)
			sb->sb_flush = NULL;
		mtx_init(&sb->sb_mtx, IPL_HIGH);
	}
}

/*
 * Add an entry to the IOMMU table.
 *
 * The entry is marked streaming if an STC was detected and 
 * the BUS_DMA_STREAMING flag is set.
 */
void
iommu_enter(struct iommu_state *is, struct strbuf_ctl *sb, bus_addr_t va,
    paddr_t pa, int flags)
{
	int64_t tte;
	volatile int64_t *tte_ptr = &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)];

#ifdef DIAGNOSTIC
	if (va < is->is_dvmabase || (va + PAGE_MASK) > is->is_dvmaend)
		panic("iommu_enter: va %#lx not in DVMA space", va);

	tte = *tte_ptr;

	if (tte & IOTTE_V) {
		printf("Overwriting valid tte entry (dva %lx pa %lx "
		    "&tte %p tte %llx)\n", va, pa, tte_ptr, tte);
		extent_print(is->is_dvmamap);
		panic("IOMMU overwrite");
	}
#endif

	tte = MAKEIOTTE(pa, !(flags & BUS_DMA_NOWRITE),
	    !(flags & BUS_DMA_NOCACHE), (flags & BUS_DMA_STREAMING));

	DPRINTF(IDB_IOMMU, ("Clearing TSB slot %d for va %p\n", 
	    (int)IOTSBSLOT(va,is->is_tsbsize), (void *)(u_long)va));

	*tte_ptr = tte;

	/*
	 * Why bother to flush this va?  It should only be relevant for
	 * V ==> V or V ==> non-V transitions.  The former is illegal and
	 * the latter is never done here.  It is true that this provides
	 * some protection against a misbehaving master using an address
	 * after it should.  The IOMMU documentations specifically warns
	 * that the consequences of a simultaneous IOMMU flush and DVMA
	 * access to the same address are undefined.  (By that argument,
	 * the STC should probably be flushed as well.)   Note that if
	 * a bus master keeps using a memory region after it has been
	 * unmapped, the specific behavior of the IOMMU is likely to
	 * be the least of our worries.
	 */
	IOMMUREG_WRITE(is, iommu_flush, va);

	DPRINTF(IDB_IOMMU, ("iommu_enter: va %lx pa %lx TSB[%lx]@@%p=%lx\n",
	    va, (long)pa, (u_long)IOTSBSLOT(va,is->is_tsbsize), 
	    (void *)(u_long)&is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
	    (u_long)tte));
}

/*
 * Remove an entry from the IOMMU table.
 *
 * The entry is flushed from the STC if an STC is detected and the TSB
 * entry has the IOTTE_STREAM flags set.  It should be impossible for
 * the TSB entry to have this flag set without the BUS_DMA_STREAMING
 * flag, but better to be safe.  (The IOMMU will be ignored as long
 * as an STC entry exists.)
 */
void
iommu_remove(struct iommu_state *is, struct strbuf_ctl *sb, bus_addr_t va)
{
	int64_t *tte_ptr = &is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)];
	int64_t tte;

#ifdef DIAGNOSTIC
	if (va < is->is_dvmabase || (va + PAGE_MASK) > is->is_dvmaend)
		panic("iommu_remove: va 0x%lx not in DVMA space", (u_long)va);
	if (va != trunc_page(va)) {
		printf("iommu_remove: unaligned va: %lx\n", va);
		va = trunc_page(va);
	}
#endif
	tte = *tte_ptr;

	DPRINTF(IDB_IOMMU, ("iommu_remove: va %lx TSB[%llx]@@%p\n",
	    va, tte, tte_ptr));

#ifdef DIAGNOSTIC
	if ((tte & IOTTE_V) == 0) {
		printf("Removing invalid tte entry (dva %lx &tte %p "
		    "tte %llx)\n", va, tte_ptr, tte);
		extent_print(is->is_dvmamap);
		panic("IOMMU remove overwrite");
	}
#endif

	*tte_ptr = tte & ~IOTTE_V;

	/*
	 * IO operations are strongly ordered WRT each other.  It is
	 * unclear how they relate to normal memory accesses.
	 */
	membar(StoreStore);

	IOMMUREG_WRITE(is, iommu_flush, va);

	if (sb && (tte & IOTTE_STREAM))
		iommu_strbuf_flush(sb, va);

	/* Should we sync the iommu and stc here? */
}

/*
 * Find the physical address of a DVMA address (debug routine).
 */
paddr_t
iommu_extract(struct iommu_state *is, bus_addr_t dva)
{
	int64_t tte = 0;
	
	if (dva >= is->is_dvmabase && dva <= is->is_dvmaend)
		tte = is->is_tsb[IOTSBSLOT(dva, is->is_tsbsize)];

	return (tte & IOTTE_PAMASK);
}

/*
 * Lookup a TSB entry for a given DVMA (debug routine).
 */
int64_t
iommu_lookup_tte(struct iommu_state *is, bus_addr_t dva)
{
	int64_t tte = 0;
	
	if (dva >= is->is_dvmabase && dva <= is->is_dvmaend)
		tte = is->is_tsb[IOTSBSLOT(dva, is->is_tsbsize)];

	return (tte);
}

/*
 * Lookup a TSB entry at a given physical address (debug routine).
 */
int64_t
iommu_fetch_tte(struct iommu_state *is, paddr_t pa)
{
	int64_t tte = 0;
	
	if (pa >= is->is_ptsb && pa < is->is_ptsb +
	    (PAGE_SIZE << is->is_tsbsize)) 
		tte = ldxa(pa, ASI_PHYS_CACHED);

	return (tte);
}

/*
 * Fetch a TSB entry with some sanity checking.
 */
int64_t
iommu_tsb_entry(struct iommu_state *is, bus_addr_t dva)
{
	int64_t tte;

	if (dva < is->is_dvmabase || dva > is->is_dvmaend)
		panic("invalid dva: %llx", (long long)dva);

	tte = is->is_tsb[IOTSBSLOT(dva,is->is_tsbsize)];

	if ((tte & IOTTE_V) == 0)
		panic("iommu_tsb_entry: invalid entry %lx", dva);

	return (tte);
}

/*
 * Initiate and then block until an STC flush synchronization has completed.
 */
int 
iommu_strbuf_flush_done(struct iommu_map_state *ims)
{
	struct strbuf_ctl *sb = ims->ims_sb;
	struct strbuf_flush *sf = &ims->ims_flush;
	struct timeval cur, flushtimeout;
	struct timeval to = { 0, 500000 };
	u_int64_t flush;
	int timeout_started = 0;

#ifdef DIAGNOSTIC
	if (sb == NULL) {
		panic("iommu_strbuf_flush_done: invalid flush buffer");
	}
#endif

	mtx_enter(&sb->sb_mtx);

	/*
	 * Streaming buffer flushes:
	 * 
	 *   1 Tell strbuf to flush by storing va to strbuf_pgflush.
	 *   2 Store 0 in flag
	 *   3 Store pointer to flag in flushsync
	 *   4 wait till flushsync becomes 0x1
	 *
	 * If it takes more than .5 sec, something went very, very wrong.
	 */

	/*
	 * If we're reading from ASI_PHYS_CACHED, then we'll write to
	 * it too.  No need to tempt fate or learn about Si bugs or such.
	 * FreeBSD just uses normal "volatile" reads/writes...
	 */

	stxa(sf->sbf_flushpa, ASI_PHYS_CACHED, 0);

	/*
	 * Insure any previous strbuf operations are complete and that 
	 * memory is initialized before the IOMMU uses it.
	 * Is this Needed?  How are IO and memory operations ordered? 
	 */
	membar(StoreStore);

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
		    STRBUFREG(strbuf_flushsync), sf->sbf_flushpa);

	DPRINTF(IDB_IOMMU,
	    ("iommu_strbuf_flush_done: flush = %llx pa = %lx\n", 
		ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED), sf->sbf_flushpa));

	membar(StoreLoad | Lookaside);

	for(;;) {
		int i;

		/*
		 * Try to shave a few instruction cycles off the average
		 * latency by only checking the elapsed time every few
		 * fetches.
		 */
		for (i = 0; i < 1000; ++i) {
			membar(LoadLoad);
			/* Bypass non-coherent D$ */
			/* non-coherent...?   Huh? */
			flush = ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED);

			if (flush) {
				DPRINTF(IDB_IOMMU,
				    ("iommu_strbuf_flush_done: flushed\n"));
				mtx_leave(&sb->sb_mtx);
				return (0);
			}
		}

		microtime(&cur);

		if (timeout_started) {
			if (timercmp(&cur, &flushtimeout, >))
				panic("STC timeout at %lx (%lld)",
				    sf->sbf_flushpa, flush);
		} else {
			timeradd(&cur, &to, &flushtimeout);
			
			timeout_started = 1;
	
			DPRINTF(IDB_IOMMU,
			    ("iommu_strbuf_flush_done: flush = %llx pa = %lx "
				"now=%lx:%lx until = %lx:%lx\n", 
				ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED),
				sf->sbf_flushpa, cur.tv_sec, cur.tv_usec, 
				flushtimeout.tv_sec, flushtimeout.tv_usec));
		}
	}
}

/*
 * IOMMU DVMA operations, common to SBus and PCI.
 */

#define BUS_DMA_FIND_PARENT(t, fn)                                      \
        if (t->_parent == NULL)                                         \
                panic("null bus_dma parent (" #fn ")");                 \
        for (t = t->_parent; t->fn == NULL; t = t->_parent)             \
                if (t->_parent == NULL)                                 \
                        panic("no bus_dma " #fn " located");

int
iommu_dvmamap_create(bus_dma_tag_t t, bus_dma_tag_t t0, struct strbuf_ctl *sb,
    bus_size_t size, int nsegments, bus_size_t maxsegsz, bus_size_t boundary,
    int flags, bus_dmamap_t *dmamap)
{
	int ret;
	bus_dmamap_t map;
	struct iommu_map_state *ims;

	BUS_DMA_FIND_PARENT(t, _dmamap_create);
	ret = (*t->_dmamap_create)(t, t0, size, nsegments, maxsegsz, boundary,
	    flags, &map);

	if (ret)
		return (ret);

	ims = iommu_iomap_create(atop(round_page(size)));

	if (ims == NULL) {
		bus_dmamap_destroy(t0, map);
		return (ENOMEM);
	}

	ims->ims_sb = sb;
	map->_dm_cookie = ims;

#ifdef DIAGNOSTIC
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_create: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_create: null iommu");
#endif
	*dmamap = map;

	return (0);
}

void
iommu_dvmamap_destroy(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map)
{
	/*
	 * The specification (man page) requires a loaded
	 * map to be unloaded before it is destroyed.
	 */
	if (map->dm_nsegs)
		bus_dmamap_unload(t0, map);

        if (map->_dm_cookie)
                iommu_iomap_destroy(map->_dm_cookie);
	map->_dm_cookie = NULL;

	BUS_DMA_FIND_PARENT(t, _dmamap_destroy);
	(*t->_dmamap_destroy)(t, t0, map);
}

/*
 * Load a contiguous kva buffer into a dmamap.  The physical pages are
 * not assumed to be contiguous.  Two passes are made through the buffer
 * and both call pmap_extract() for the same va->pa translations.  It
 * is possible to run out of pa->dvma mappings; the code should be smart
 * enough to resize the iomap (when the "flags" permit allocation).  It
 * is trivial to compute the number of entries required (round the length
 * up to the page size and then divide by the page size)...
 */
int
iommu_dvmamap_load(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
    void *buf, bus_size_t buflen, struct proc *p, int flags)
{
	int err = 0;
	bus_size_t sgsize;
	u_long dvmaddr, sgstart, sgend;
	bus_size_t align, boundary;
	struct iommu_state *is;
	struct iommu_map_state *ims = map->_dm_cookie;
	pmap_t pmap;

#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_load: null map state");
#endif
#ifdef DEBUG
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_load: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_load: null iommu");
#endif /* DEBUG */
	is = ims->ims_sb->sb_iommu;

	if (map->dm_nsegs) {
		/*
		 * Is it still in use? _bus_dmamap_load should have taken care
		 * of this.
		 */
#ifdef DIAGNOSTIC
		panic("iommu_dvmamap_load: map still in use");
#endif
		bus_dmamap_unload(t0, map);
	}

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;

	if (buflen < 1 || buflen > map->_dm_size) {
		DPRINTF(IDB_BUSDMA,
		    ("iommu_dvmamap_load(): error %d > %d -- "
		     "map size exceeded!\n", (int)buflen, (int)map->_dm_size));
		return (EINVAL);
	}

	/*
	 * A boundary presented to bus_dmamem_alloc() takes precedence
	 * over boundary in the map.
	 */
	if ((boundary = (map->dm_segs[0]._ds_boundary)) == 0)
		boundary = map->_dm_boundary;
	align = MAX(map->dm_segs[0]._ds_align, PAGE_SIZE);

	pmap = p ? p->p_vmspace->vm_map.pmap : pmap_kernel();

	/* Count up the total number of pages we need */
	iommu_iomap_clear_pages(ims);
	{ /* Scope */
		bus_addr_t a, aend;
		bus_addr_t addr = (bus_addr_t)buf;
		int seg_len = buflen;

		aend = round_page(addr + seg_len);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			paddr_t pa;

			if (pmap_extract(pmap, a, &pa) == FALSE)
				panic("iomap pmap error addr 0x%lx\n", a);

			err = iommu_iomap_insert_page(ims, pa);
			if (err) {
				printf("iomap insert error: %d for "
				    "va 0x%lx pa 0x%lx "
				    "(buf %p len %ld/%lx)\n",
				    err, a, pa, buf, buflen, buflen);
				iommu_dvmamap_print_map(t, is, map);
				iommu_iomap_clear_pages(ims);
				return (EFBIG);
			}
		}
	}
	if (flags & BUS_DMA_OVERRUN) {
		err = iommu_iomap_insert_page(ims, is->is_scratch);
		if (err) {
			iommu_iomap_clear_pages(ims);
			return (EFBIG);
		}
	}
	sgsize = ims->ims_map.ipm_pagecnt * PAGE_SIZE;

	mtx_enter(&is->is_mtx);
	if (flags & BUS_DMA_24BIT) {
		sgstart = MAX(is->is_dvmamap->ex_start, 0xff000000);
		sgend = MIN(is->is_dvmamap->ex_end, 0xffffffff);
	} else {
		sgstart = is->is_dvmamap->ex_start;
		sgend = is->is_dvmamap->ex_end;
	}

	/* 
	 * If our segment size is larger than the boundary we need to 
	 * split the transfer up into little pieces ourselves.
	 */
	err = extent_alloc_subregion_with_descr(is->is_dvmamap, sgstart, sgend,
	    sgsize, align, 0, (sgsize > boundary) ? 0 : boundary, 
	    EX_NOWAIT | EX_BOUNDZERO, &ims->ims_er, (u_long *)&dvmaddr);
	mtx_leave(&is->is_mtx);

#ifdef DEBUG
	if (err || (dvmaddr == (bus_addr_t)-1))	{ 
		printf("iommu_dvmamap_load(): extent_alloc(%d, %x) failed!\n",
		    (int)sgsize, flags);
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
	}		
#endif	
	if (err != 0) {
		iommu_iomap_clear_pages(ims);
		return (err);
	}

	/* Set the active DVMA map */
	map->_dm_dvmastart = dvmaddr;
	map->_dm_dvmasize = sgsize;

	map->dm_mapsize = buflen;

#ifdef DEBUG
	iommu_dvmamap_validate_map(t, is, map);
#endif

	iommu_iomap_load_map(is, ims, dvmaddr, flags);

	{ /* Scope */
		bus_addr_t a, aend;
		bus_addr_t addr = (bus_addr_t)buf;
		int seg_len = buflen;

		aend = round_page(addr + seg_len);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			bus_addr_t pgstart;
			bus_addr_t pgend;
			paddr_t pa;
			int pglen;

			/* Yuck... Redoing the same pmap_extract... */
			if (pmap_extract(pmap, a, &pa) == FALSE)
				panic("iomap pmap error addr 0x%lx\n", a);

			pgstart = pa | (MAX(a, addr) & PAGE_MASK);
			pgend = pa | (MIN(a + PAGE_SIZE - 1,
			    addr + seg_len - 1) & PAGE_MASK);
			pglen = pgend - pgstart + 1;

			if (pglen < 1)
				continue;

			err = iommu_dvmamap_append_range(t, map, pgstart,
			    pglen, flags, boundary);
			if (err == EFBIG)
				break;
			else if (err) {
				printf("iomap load seg page: %d for "
				    "va 0x%lx pa %lx (%lx - %lx) "
				    "for %d/0x%x\n",
				    err, a, pa, pgstart, pgend, pglen, pglen);
				break;
			}
		}
	}
#ifdef DEBUG
	iommu_dvmamap_validate_map(t, is, map);

	if (err)
		printf("**** iommu_dvmamap_load failed with error %d\n",
		    err);
	
	if (err || (iommudebug & IDB_PRINT_MAP)) {
		iommu_dvmamap_print_map(t, is, map);
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
	}
#endif
	if (err)
		iommu_dvmamap_unload(t, t0, map);

	return (err);
}

/*
 * Load a dvmamap from an array of segs or an mlist (if the first
 * "segs" entry's mlist is non-null).  It calls iommu_dvmamap_load_segs()
 * or iommu_dvmamap_load_mlist() for part of the 2nd pass through the
 * mapping.  This is ugly.  A better solution would probably be to have
 * function pointers for implementing the traversal.  That way, there
 * could be one core load routine for each of the three required algorithms
 * (buffer, seg, and mlist).  That would also mean that the traversal
 * algorithm would then only need one implementation for each algorithm
 * instead of two (one for populating the iomap and one for populating
 * the dvma map).
 */
int
iommu_dvmamap_load_raw(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
    bus_dma_segment_t *segs, int nsegs, bus_size_t size, int flags)
{
	int i;
	int left;
	int err = 0;
	bus_size_t sgsize;
	bus_size_t boundary, align;
	u_long dvmaddr, sgstart, sgend;
	struct iommu_state *is;
	struct iommu_map_state *ims = map->_dm_cookie;

#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_load_raw: null map state");
#endif
#ifdef DEBUG
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_load_raw: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_load_raw: null iommu");
#endif /* DEBUG */
	is = ims->ims_sb->sb_iommu;

	if (map->dm_nsegs) {
		/* Already in use?? */
#ifdef DIAGNOSTIC
		panic("iommu_dvmamap_load_raw: map still in use");
#endif
		bus_dmamap_unload(t0, map);
	}

	/*
	 * A boundary presented to bus_dmamem_alloc() takes precedence
	 * over boundary in the map.
	 */
	if ((boundary = segs[0]._ds_boundary) == 0)
		boundary = map->_dm_boundary;

	align = MAX(segs[0]._ds_align, PAGE_SIZE);

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;

	iommu_iomap_clear_pages(ims);
	if (segs[0]._ds_mlist) {
		struct pglist *mlist = segs[0]._ds_mlist;
		struct vm_page *m;
		for (m = TAILQ_FIRST(mlist); m != NULL;
		    m = TAILQ_NEXT(m,pageq)) {
			err = iommu_iomap_insert_page(ims, VM_PAGE_TO_PHYS(m));

			if(err) {
				printf("iomap insert error: %d for "
				    "pa 0x%lx\n", err, VM_PAGE_TO_PHYS(m));
				iommu_dvmamap_print_map(t, is, map);
				iommu_iomap_clear_pages(ims);
				return (EFBIG);
			}
		}
	} else {
		/* Count up the total number of pages we need */
		for (i = 0, left = size; left > 0 && i < nsegs; i++) {
			bus_addr_t a, aend;
			bus_size_t len = segs[i].ds_len;
			bus_addr_t addr = segs[i].ds_addr;
			int seg_len = MIN(left, len);

			if (len < 1)
				continue;

			aend = round_page(addr + seg_len);
			for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {

				err = iommu_iomap_insert_page(ims, a);
				if (err) {
					printf("iomap insert error: %d for "
					    "pa 0x%lx\n", err, a);
					iommu_dvmamap_print_map(t, is, map);
					iommu_iomap_clear_pages(ims);
					return (EFBIG);
				}
			}

			left -= seg_len;
		}
	}
	if (flags & BUS_DMA_OVERRUN) {
		err = iommu_iomap_insert_page(ims, is->is_scratch);
		if (err) {
			iommu_iomap_clear_pages(ims);
			return (EFBIG);
		}
	}
	sgsize = ims->ims_map.ipm_pagecnt * PAGE_SIZE;

	mtx_enter(&is->is_mtx);
	if (flags & BUS_DMA_24BIT) {
		sgstart = MAX(is->is_dvmamap->ex_start, 0xff000000);
		sgend = MIN(is->is_dvmamap->ex_end, 0xffffffff);
	} else {
		sgstart = is->is_dvmamap->ex_start;
		sgend = is->is_dvmamap->ex_end;
	}

	/* 
	 * If our segment size is larger than the boundary we need to 
	 * split the transfer up into little pieces ourselves.
	 */
	err = extent_alloc_subregion_with_descr(is->is_dvmamap, sgstart, sgend,
	    sgsize, align, 0, (sgsize > boundary) ? 0 : boundary, 
	    EX_NOWAIT | EX_BOUNDZERO, &ims->ims_er, (u_long *)&dvmaddr);
	mtx_leave(&is->is_mtx);

	if (err != 0) {
		iommu_iomap_clear_pages(ims);
		return (err);
	}

#ifdef DEBUG
	if (dvmaddr == (bus_addr_t)-1)	{ 
		printf("iommu_dvmamap_load_raw(): extent_alloc(%d, %x) "
		    "failed!\n", (int)sgsize, flags);
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#else
		panic("");
#endif
	}		
#endif	

	/* Set the active DVMA map */
	map->_dm_dvmastart = dvmaddr;
	map->_dm_dvmasize = sgsize;

	map->dm_mapsize = size;

#ifdef DEBUG
	iommu_dvmamap_validate_map(t, is, map);
#endif

	iommu_iomap_load_map(is, ims, dvmaddr, flags);

	if (segs[0]._ds_mlist)
		err = iommu_dvmamap_load_mlist(t, is, map, segs[0]._ds_mlist,
		    flags, size, boundary);
	else
		err = iommu_dvmamap_load_seg(t, is, map, segs, nsegs,
		    flags, size, boundary);

#ifdef DEBUG
	/* The map should be valid even if the load failed */
	if (iommu_dvmamap_validate_map(t, is, map)) {
		printf("load size %ld/0x%lx\n", size, size);
		if (segs[0]._ds_mlist)
			printf("mlist %p\n", segs[0]._ds_mlist);
		else  {
			long tot_len = 0;
			long clip_len = 0;
			printf("segs %p nsegs %d\n", segs, nsegs);

			left = size;
			for(i = 0; i < nsegs; i++) {
				bus_size_t len = segs[i].ds_len;
				bus_addr_t addr = segs[i].ds_addr;
				int seg_len = MIN(left, len);

				printf("addr %lx len %ld/0x%lx seg_len "
				    "%ld/0x%lx left %ld/0xl%x\n", addr,
				    len, len, seg_len, seg_len, left, left);

				left -= seg_len;
				
				clip_len += seg_len;
				tot_len += segs[i].ds_len;
			}
			printf("total length %ld/0x%lx total seg. "
			    "length %ld/0x%lx\n", tot_len, tot_len, clip_len,
			    clip_len);
		}

		if (err == 0)
			err = 1;
	}

	if (err)
		printf("**** iommu_dvmamap_load_raw failed with error %d\n",
		    err);
	
	if (err || (iommudebug & IDB_PRINT_MAP)) {
		iommu_dvmamap_print_map(t, is, map);
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
	}
#endif
	if (err)
		iommu_dvmamap_unload(t, t0, map);

	return (err);
}

/*
 * Insert a range of addresses into a loaded map respecting the specified
 * boundary and alignment restrictions.  The range is specified by its 
 * physical address and length.  The range cannot cross a page boundary.
 * This code (along with most of the rest of the function in this file)
 * assumes that the IOMMU page size is equal to PAGE_SIZE.
 */
int
iommu_dvmamap_append_range(bus_dma_tag_t t, bus_dmamap_t map, paddr_t pa,
    bus_size_t length, int flags, bus_size_t boundary)
{
	struct iommu_map_state *ims = map->_dm_cookie;
	bus_addr_t sgstart, sgend, bd_mask;
	bus_dma_segment_t *seg = NULL;
	int i = map->dm_nsegs;

#ifdef DEBUG
	if (ims == NULL)
		panic("iommu_dvmamap_append_range: null map state");
#endif

	sgstart = iommu_iomap_translate(ims, pa);
	sgend = sgstart + length - 1;

#ifdef DIAGNOSTIC
	if (sgstart == 0 || sgstart > sgend) {
		printf("append range invalid mapping for %lx "
		    "(0x%lx - 0x%lx)\n", pa, sgstart, sgend);
		map->dm_nsegs = 0;
		return (EINVAL);
	}
#endif

#ifdef DEBUG
	if (trunc_page(sgstart) != trunc_page(sgend)) {
		printf("append range crossing page boundary! "
		    "pa %lx length %ld/0x%lx sgstart %lx sgend %lx\n",
		    pa, length, length, sgstart, sgend);
	}
#endif

	/*
	 * We will attempt to merge this range with the previous entry
	 * (if there is one).
	 */
	if (i > 0) {
		seg = &map->dm_segs[i - 1];
		if (sgstart == seg->ds_addr + seg->ds_len) {
			length += seg->ds_len;
			sgstart = seg->ds_addr;
			sgend = sgstart + length - 1;
		} else
			seg = NULL;
	}

	if (seg == NULL) {
		seg = &map->dm_segs[i];
		if (++i > map->_dm_segcnt) {
			map->dm_nsegs = 0;
			return (EFBIG);
		}
	}

	/*
	 * At this point, "i" is the index of the *next* bus_dma_segment_t
	 * (the segment count, aka map->dm_nsegs) and "seg" points to the
	 * *current* entry.  "length", "sgstart", and "sgend" reflect what
	 * we intend to put in "*seg".  No assumptions should be made about
	 * the contents of "*seg".  Only "boundary" issue can change this
	 * and "boundary" is often zero, so explicitly test for that case
	 * (the test is strictly an optimization).
	 */ 
	if (boundary != 0) {
		bd_mask = ~(boundary - 1);

		while ((sgstart & bd_mask) != (sgend & bd_mask)) {
			/*
			 * We are crossing a boundary so fill in the current
			 * segment with as much as possible, then grab a new
			 * one.
			 */

			seg->ds_addr = sgstart;
			seg->ds_len = boundary - (sgstart & ~bd_mask);

			sgstart += seg->ds_len; /* sgend stays the same */
			length -= seg->ds_len;

			seg = &map->dm_segs[i];
			if (++i > map->_dm_segcnt) {
				map->dm_nsegs = 0;
				return (EFBIG);
			}
		}
	}

	seg->ds_addr = sgstart;
	seg->ds_len = length;
	map->dm_nsegs = i;

	return (0);
}

/*
 * Populate the iomap from a bus_dma_segment_t array.  See note for
 * iommu_dvmamap_load() * regarding page entry exhaustion of the iomap.
 * This is less of a problem for load_seg, as the number of pages
 * is usually similar to the number of segments (nsegs).
 */
int
iommu_dvmamap_load_seg(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, bus_dma_segment_t *segs, int nsegs, int flags,
    bus_size_t size, bus_size_t boundary)
{
	int i;
	int left;
	int seg;

	/*
	 * This segs is made up of individual physical
	 * segments, probably by _bus_dmamap_load_uio() or
	 * _bus_dmamap_load_mbuf().  Ignore the mlist and
	 * load each one individually.
	 */

	/*
	 * Keep in mind that each segment could span
	 * multiple pages and that these are not always
	 * adjacent. The code is no longer adding dvma
	 * aliases to the IOMMU.  The STC will not cross
	 * page boundaries anyway and a IOMMU table walk
	 * vs. what may be a streamed PCI DMA to a ring
	 * descriptor is probably a wash.  It eases TLB
	 * pressure and in the worst possible case, it is
	 * only as bad a non-IOMMUed architecture.  More
	 * importantly, the code is not quite as hairy.
	 * (It's bad enough as it is.)
	 */
	left = size;
	seg = 0;
	for (i = 0; left > 0 && i < nsegs; i++) {
		bus_addr_t a, aend;
		bus_size_t len = segs[i].ds_len;
		bus_addr_t addr = segs[i].ds_addr;
		int seg_len = MIN(left, len);

		if (len < 1)
			continue;

		aend = round_page(addr + seg_len);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			bus_addr_t pgstart;
			bus_addr_t pgend;
			int pglen;
			int err;

			pgstart = MAX(a, addr);
			pgend = MIN(a + PAGE_SIZE - 1, addr + seg_len - 1);
			pglen = pgend - pgstart + 1;
			
			if (pglen < 1)
				continue;

			err = iommu_dvmamap_append_range(t, map, pgstart,
			    pglen, flags, boundary);
			if (err == EFBIG)
				return (err);
			if (err) {
				printf("iomap load seg page: %d for "
				    "pa 0x%lx (%lx - %lx for %d/%x\n",
				    err, a, pgstart, pgend, pglen, pglen);
				return (err);
			}

		}

		left -= seg_len;
	}
	return (0);
}

/*
 * Populate the iomap from an mlist.  See note for iommu_dvmamap_load()
 * regarding page entry exhaustion of the iomap.
 */
int
iommu_dvmamap_load_mlist(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, struct pglist *mlist, int flags,
    bus_size_t size, bus_size_t boundary)
{
	struct vm_page *m;
	paddr_t pa;
	int err;

	/*
	 * This was allocated with bus_dmamem_alloc.
	 * The pages are on an `mlist'.
	 */
	for (m = TAILQ_FIRST(mlist); m != NULL; m = TAILQ_NEXT(m,pageq)) {
		pa = VM_PAGE_TO_PHYS(m);

		err = iommu_dvmamap_append_range(t, map, pa,
		    MIN(PAGE_SIZE, size), flags, boundary);
		if (err == EFBIG)
			return (err);
		if (err) {
			printf("iomap load seg page: %d for pa 0x%lx "
			    "(%lx - %lx for %d/%x\n", err, pa, pa,
			    pa + PAGE_SIZE, PAGE_SIZE, PAGE_SIZE);
			return (err);
		}
		if (size < PAGE_SIZE)
			break;
		size -= PAGE_SIZE;
	}

	return (0);
}

/*
 * Unload a dvmamap.
 */
void
iommu_dvmamap_unload(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map)
{
	struct iommu_state *is;
	struct iommu_map_state *ims = map->_dm_cookie;
	bus_addr_t dvmaddr = map->_dm_dvmastart;
	bus_size_t sgsize = map->_dm_dvmasize;
	int error;

#ifdef DEBUG
	if (ims == NULL)
		panic("iommu_dvmamap_unload: null map state");
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_unload: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_unload: null iommu");
#endif /* DEBUG */

	is = ims->ims_sb->sb_iommu;

	/* Flush the iommu */
#ifdef DEBUG
	if (dvmaddr == 0) {
		printf("iommu_dvmamap_unload: No dvmastart\n");
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
		return;
	}

	iommu_dvmamap_validate_map(t, is, map);

	if (iommudebug & IDB_PRINT_MAP)
		iommu_dvmamap_print_map(t, is, map);
#endif /* DEBUG */

	/* Remove the IOMMU entries */
	iommu_iomap_unload_map(is, ims);

	/* Clear the iomap */
	iommu_iomap_clear_pages(ims);

	bus_dmamap_unload(t->_parent, map);

	/* Mark the mappings as invalid. */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	mtx_enter(&is->is_mtx);
	error = extent_free(is->is_dvmamap, dvmaddr, sgsize, EX_NOWAIT);
	map->_dm_dvmastart = 0;
	map->_dm_dvmasize = 0;
	mtx_leave(&is->is_mtx);
	if (error != 0)
		printf("warning: %ld of DVMA space lost\n", sgsize);
}

#ifdef DEBUG
/*
 * Perform internal consistency checking on a dvmamap.
 */
int
iommu_dvmamap_validate_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map)
{
	int err = 0;
	int seg;

	if (trunc_page(map->_dm_dvmastart) != map->_dm_dvmastart) {
		printf("**** dvmastart address not page aligned: %llx",
			map->_dm_dvmastart);
		err = 1;
	}
	if (trunc_page(map->_dm_dvmasize) != map->_dm_dvmasize) {
		printf("**** dvmasize not a multiple of page size: %llx",
			map->_dm_dvmasize);
		err = 1;
	}
	if (map->_dm_dvmastart < is->is_dvmabase ||
	    (round_page(map->_dm_dvmastart + map->_dm_dvmasize) - 1) >
	    is->is_dvmaend) {
		printf("dvmaddr %llx len %llx out of range %x - %x\n",
			    map->_dm_dvmastart, map->_dm_dvmasize,
			    is->is_dvmabase, is->is_dvmaend);
		err = 1;
	}
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		if (map->dm_segs[seg].ds_addr == 0 ||
		    map->dm_segs[seg].ds_len == 0) {
			printf("seg %d null segment dvmaddr %llx len %llx for "
			    "range %llx len %llx\n",
			    seg,
			    map->dm_segs[seg].ds_addr,
			    map->dm_segs[seg].ds_len,
			    map->_dm_dvmastart, map->_dm_dvmasize);
			err = 1;
		} else if (map->dm_segs[seg].ds_addr < map->_dm_dvmastart ||
		    round_page(map->dm_segs[seg].ds_addr +
			map->dm_segs[seg].ds_len) >
		    map->_dm_dvmastart + map->_dm_dvmasize) {
			printf("seg %d dvmaddr %llx len %llx out of "
			    "range %llx len %llx\n",
			    seg,
			    map->dm_segs[seg].ds_addr,
			    map->dm_segs[seg].ds_len,
			    map->_dm_dvmastart, map->_dm_dvmasize);
			err = 1;
		}
	}

	if (err) {
		iommu_dvmamap_print_map(t, is, map);
#if defined(DDB) && defined(DEBUG)
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
	}

	return (err);
}
#endif /* DEBUG */

void
iommu_dvmamap_print_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map)
{
	int seg, i;
	long full_len, source_len;
	struct mbuf *m;

	printf("DVMA %x for %x, mapping %p: dvstart %lx dvsize %lx "
	    "size %ld/%lx maxsegsz %lx boundary %lx segcnt %d "
	    "flags %x type %d source %p "
	    "cookie %p mapsize %lx nsegs %d\n",
	    is ? is->is_dvmabase : 0, is ? is->is_dvmaend : 0, map,
	    map->_dm_dvmastart, map->_dm_dvmasize,
	    map->_dm_size, map->_dm_size, map->_dm_maxsegsz, map->_dm_boundary,
	    map->_dm_segcnt, map->_dm_flags, map->_dm_type,
	    map->_dm_source, map->_dm_cookie, map->dm_mapsize,
	    map->dm_nsegs);

	full_len = 0;
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		printf("seg %d dvmaddr %lx pa %lx len %lx (tte %llx)\n",
		    seg, map->dm_segs[seg].ds_addr,
		    is ? iommu_extract(is, map->dm_segs[seg].ds_addr) : 0,
		    map->dm_segs[seg].ds_len,
		    is ? iommu_lookup_tte(is, map->dm_segs[seg].ds_addr) : 0);
		full_len += map->dm_segs[seg].ds_len;
	}
	printf("total length = %ld/0x%lx\n", full_len, full_len);

	if (map->_dm_source) switch (map->_dm_type) {
	case _DM_TYPE_MBUF:
		m = map->_dm_source;
		if (m->m_flags & M_PKTHDR)
			printf("source PKTHDR mbuf (%p) hdr len = %d/0x%x:\n",
			    m, m->m_pkthdr.len, m->m_pkthdr.len);
		else
			printf("source mbuf (%p):\n", m);

		source_len = 0;
		for ( ; m; m = m->m_next) {
			vaddr_t vaddr = mtod(m, vaddr_t);
			long len = m->m_len;
			paddr_t pa;

			if (pmap_extract(pmap_kernel(), vaddr, &pa))
				printf("kva %lx pa %lx len %ld/0x%lx\n",
				    vaddr, pa, len, len);
			else
				printf("kva %lx pa <invalid> len %ld/0x%lx\n",
				    vaddr, len, len);

			source_len += len;
		}

		if (full_len != source_len)
			printf("mbuf length %ld/0x%lx is %s than mapping "
			    "length %ld/0x%lx\n", source_len, source_len,
			    (source_len > full_len) ? "greater" : "less",
			    full_len, full_len);
		else
			printf("mbuf length %ld/0x%lx\n", source_len,
			    source_len);
		break;
	case _DM_TYPE_LOAD:
	case _DM_TYPE_SEGS:
	case _DM_TYPE_UIO:
	default:
		break;
	}

	if (map->_dm_cookie) {
		struct iommu_map_state *ims = map->_dm_cookie;
		struct iommu_page_map *ipm = &ims->ims_map;

		printf("page map (%p) of size %d with %d entries\n",
		    ipm, ipm->ipm_maxpage, ipm->ipm_pagecnt);
		for (i = 0; i < ipm->ipm_pagecnt; ++i) {
			struct iommu_page_entry *e = &ipm->ipm_map[i];
			printf("%d: vmaddr 0x%lx pa 0x%lx\n", i,
			    e->ipe_va, e->ipe_pa);
		}
	} else
		printf("iommu map state (cookie) is NULL\n");
}

void
_iommu_dvmamap_sync(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
	bus_addr_t offset, bus_size_t len, int ops)
{
	struct iommu_state *is;
	struct iommu_map_state *ims = map->_dm_cookie;
	struct strbuf_ctl *sb;
	bus_size_t count;
	int i, needsflush = 0;

	sb = ims->ims_sb;
	is = sb->sb_iommu;

	for (i = 0; i < map->dm_nsegs; i++) {
		if (offset < map->dm_segs[i].ds_len)
			break;
		offset -= map->dm_segs[i].ds_len;
	}

	if (i == map->dm_nsegs)
		panic("iommu_dvmamap_sync: too short %lu", offset);

	for (; len > 0 && i < map->dm_nsegs; i++) {
		count = MIN(map->dm_segs[i].ds_len - offset, len);
		if (count > 0 && iommu_dvmamap_sync_range(sb,
		    map->dm_segs[i].ds_addr + offset, count))
			needsflush = 1;
		len -= count;
	}

#ifdef DIAGNOSTIC
	if (i == map->dm_nsegs && len > 0)
		panic("iommu_dvmamap_sync: leftover %lu", len);
#endif

	if (needsflush)
		iommu_strbuf_flush_done(ims);
}

void
iommu_dvmamap_sync(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
    bus_addr_t offset, bus_size_t len, int ops)
{
	struct iommu_map_state *ims = map->_dm_cookie;

#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_sync: null map state");
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_sync: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_sync: null iommu");
#endif
	if (len == 0)
		return;

	if (ops & BUS_DMASYNC_PREWRITE)
		membar(MemIssue);

	if ((ims->ims_flags & IOMMU_MAP_STREAM) &&
	    (ops & (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_PREWRITE)))
		_iommu_dvmamap_sync(t, t0, map, offset, len, ops);

	if (ops & BUS_DMASYNC_POSTREAD)
		membar(MemIssue);
}

/*
 * Flush an individual dma segment, returns non-zero if the streaming buffers
 * need flushing afterwards.
 */
int
iommu_dvmamap_sync_range(struct strbuf_ctl *sb, bus_addr_t va, bus_size_t len)
{
	bus_addr_t vaend;
#ifdef DIAGNOSTIC
	struct iommu_state *is = sb->sb_iommu;

	if (va < is->is_dvmabase || va > is->is_dvmaend)
		panic("invalid va: %llx", (long long)va);

	if ((is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)] & IOTTE_STREAM) == 0) {
		printf("iommu_dvmamap_sync_range: attempting to flush "
		    "non-streaming entry\n");
		return (0);
	}
#endif

	vaend = (va + len + PAGE_MASK) & ~PAGE_MASK;
	va &= ~PAGE_MASK;

#ifdef DIAGNOSTIC
	if (va < is->is_dvmabase || (vaend - 1) > is->is_dvmaend)
		panic("invalid va range: %llx to %llx (%x to %x)",
		    (long long)va, (long long)vaend,
		    is->is_dvmabase,
		    is->is_dvmaend);
#endif

	for ( ; va <= vaend; va += PAGE_SIZE) {
		DPRINTF(IDB_BUSDMA,
		    ("iommu_dvmamap_sync_range: flushing va %p\n",
		    (void *)(u_long)va));
		iommu_strbuf_flush(sb, va);
	}

	return (1);
}

int
iommu_dvmamem_alloc(bus_dma_tag_t t, bus_dma_tag_t t0, bus_size_t size,
    bus_size_t alignment, bus_size_t boundary, bus_dma_segment_t *segs,
    int nsegs, int *rsegs, int flags)
{

	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_alloc: sz %llx align %llx "
	    "bound %llx segp %p flags %d\n", (unsigned long long)size,
	    (unsigned long long)alignment, (unsigned long long)boundary,
	    segs, flags));
	BUS_DMA_FIND_PARENT(t, _dmamem_alloc);
	return ((*t->_dmamem_alloc)(t, t0, size, alignment, boundary,
	    segs, nsegs, rsegs, flags | BUS_DMA_DVMA));
}

void
iommu_dvmamem_free(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dma_segment_t *segs,
    int nsegs)
{

	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_free: segp %p nsegs %d\n",
	    segs, nsegs));
	BUS_DMA_FIND_PARENT(t, _dmamem_free);
	(*t->_dmamem_free)(t, t0, segs, nsegs);
}

/*
 * Create a new iomap.
 */
struct iommu_map_state *
iommu_iomap_create(int n)
{
	struct iommu_map_state *ims;
	struct strbuf_flush *sbf;
	vaddr_t va;

	/* Safety for heavily fragmented data, such as mbufs */
	n += 4;
	if (n < 16)
		n = 16;

	ims = malloc(sizeof(*ims) + (n - 1) * sizeof(ims->ims_map.ipm_map[0]),
		M_DEVBUF, M_NOWAIT | M_ZERO);
	if (ims == NULL)
		return (NULL);

	/* Initialize the map. */
	ims->ims_map.ipm_maxpage = n;
	SPLAY_INIT(&ims->ims_map.ipm_tree);

	/* Initialize the flush area. */
	sbf = &ims->ims_flush;
	va = (vaddr_t)&sbf->sbf_area[0x40];
	va &= ~0x3f;
	pmap_extract(pmap_kernel(), va, &sbf->sbf_flushpa);
	sbf->sbf_flush = (void *)va;

	return (ims);
}

/*
 * Destroy an iomap.
 */
void
iommu_iomap_destroy(struct iommu_map_state *ims)
{
#ifdef DIAGNOSTIC
	if (ims->ims_map.ipm_pagecnt > 0)
		printf("iommu_iomap_destroy: %d page entries in use\n",
		    ims->ims_map.ipm_pagecnt);
#endif

	free(ims, M_DEVBUF, 0);
}

/*
 * Utility function used by splay tree to order page entries by pa.
 */
static inline int
iomap_compare(struct iommu_page_entry *a, struct iommu_page_entry *b)
{
	return ((a->ipe_pa > b->ipe_pa) ? 1 :
		(a->ipe_pa < b->ipe_pa) ? -1 : 0);
}

SPLAY_PROTOTYPE(iommu_page_tree, iommu_page_entry, ipe_node, iomap_compare);

SPLAY_GENERATE(iommu_page_tree, iommu_page_entry, ipe_node, iomap_compare);

/*
 * Insert a pa entry in the iomap.
 */
int
iommu_iomap_insert_page(struct iommu_map_state *ims, paddr_t pa)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;

	if (ipm->ipm_pagecnt >= ipm->ipm_maxpage) {
		struct iommu_page_entry ipe;

		ipe.ipe_pa = pa;
		if (SPLAY_FIND(iommu_page_tree, &ipm->ipm_tree, &ipe))
			return (0);

		return (ENOMEM);
	}

	e = &ipm->ipm_map[ipm->ipm_pagecnt];

	e->ipe_pa = pa;
	e->ipe_va = 0;

	e = SPLAY_INSERT(iommu_page_tree, &ipm->ipm_tree, e);

	/* Duplicates are okay, but only count them once. */
	if (e)
		return (0);

	++ipm->ipm_pagecnt;

	return (0);
}

/*
 * Locate the iomap by filling in the pa->va mapping and inserting it
 * into the IOMMU tables.
 */
void
iommu_iomap_load_map(struct iommu_state *is, struct iommu_map_state *ims,
    bus_addr_t vmaddr, int flags)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct strbuf_ctl *sb = ims->ims_sb;
	int i, slot;

	if (sb->sb_flush == NULL)
		flags &= ~BUS_DMA_STREAMING;

	if (flags & BUS_DMA_STREAMING)
		ims->ims_flags |= IOMMU_MAP_STREAM;
	else
		ims->ims_flags &= ~IOMMU_MAP_STREAM;

	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e) {
		e->ipe_va = vmaddr;
		iommu_enter(is, sb, e->ipe_va, e->ipe_pa, flags);

		/* Flush cache if necessary. */
		slot = IOTSBSLOT(e->ipe_va, is->is_tsbsize);
		if (is->is_flags & IOMMU_FLUSH_CACHE &&
		    (i == (ipm->ipm_pagecnt - 1) || (slot % 8) == 7))
			IOMMUREG_WRITE(is, iommu_cache_flush,
			    is->is_ptsb + slot * 8);

		vmaddr += PAGE_SIZE;
	}
}

/*
 * Remove the iomap from the IOMMU.
 */
void
iommu_iomap_unload_map(struct iommu_state *is, struct iommu_map_state *ims)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct strbuf_ctl *sb = ims->ims_sb;
	int i, slot;

	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e) {
		iommu_remove(is, sb, e->ipe_va);

		/* Flush cache if necessary. */
		slot = IOTSBSLOT(e->ipe_va, is->is_tsbsize);
		if (is->is_flags & IOMMU_FLUSH_CACHE &&
		    (i == (ipm->ipm_pagecnt - 1) || (slot % 8) == 7))
			IOMMUREG_WRITE(is, iommu_cache_flush,
			    is->is_ptsb + slot * 8);
	}
}

/*
 * Translate a physical address (pa) into a DVMA address.
 */
bus_addr_t
iommu_iomap_translate(struct iommu_map_state *ims, paddr_t pa)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct iommu_page_entry pe;
	paddr_t offset = pa & PAGE_MASK;

	pe.ipe_pa = trunc_page(pa);

	e = SPLAY_FIND(iommu_page_tree, &ipm->ipm_tree, &pe);

	if (e == NULL)
		return (0);

	return (e->ipe_va | offset);
}

/*
 * Clear the iomap table and tree.
 */
void
iommu_iomap_clear_pages(struct iommu_map_state *ims)
{
	ims->ims_map.ipm_pagecnt = 0;
	SPLAY_INIT(&ims->ims_map.ipm_tree);
}

@


1.72
log
@Fix loading memory allocated with bus_dmamem_alloc(9).  The old could would
always load all allocated pages instead of the size specified in the
bus_dmamap_load_raw(9) call.  Also fixes the corner case where a specified
boundary is less than the page size, which would always create multiple
segments, even if the specified size was smaller than the boundary.

Fixes xhci(4) on sparc64.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.71 2014/11/16 12:30:59 deraadt Exp $	*/
d198 7
d744 7
d956 7
@


1.71
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.70 2014/10/26 18:17:16 kettenis Exp $	*/
d1255 2
a1256 2
		err = iommu_dvmamap_append_range(t, map, pa, PAGE_SIZE,
		    flags, boundary);
d1265 3
@


1.70
log
@uvm_km_valloc -> km_alloc
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.69 2014/07/12 18:44:43 tedu Exp $	*/
d191 2
a192 2
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
@


1.69
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.68 2014/05/10 12:20:38 kettenis Exp $	*/
d179 1
a179 1
	va = uvm_km_valloc(kernel_map, size);
@


1.68
log
@Format string fixes for bus_addr_t and bus_size_t, which are u_long everywhere.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.67 2014/01/22 10:52:35 kettenis Exp $	*/
d1663 1
a1663 1
	free(ims, M_DEVBUF);
@


1.67
log
@Use extent_alloc_subregion_with_descr(9).  This should make bus_dmamap_load(9)
and bus_dmamap_unload(9) "mpsafe".
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.66 2013/01/15 03:14:01 kettenis Exp $	*/
d723 1
a723 1
				panic("iomap pmap error addr 0x%llx\n", a);
d728 2
a729 2
				    "va 0x%llx pa 0x%lx "
				    "(buf %p len %lld/%llx)\n",
d798 1
a798 1
				panic("iomap pmap error addr 0x%llx\n", a);
d814 1
a814 1
				    "va 0x%llx pa %lx (%llx - %llx) "
d934 1
a934 1
					    "pa 0x%llx\n", err, a);
d1004 1
a1004 1
		printf("load size %lld/0x%llx\n", size, size);
d1018 3
a1020 3
				printf("addr %llx len %lld/0x%llx seg_len "
				    "%d/0x%x left %d/0x%x\n", addr, len, len,
				    seg_len, seg_len, left, left);
d1081 1
a1081 1
		    "(0x%llx - 0x%llx)\n", pa, sgstart, sgend);
d1090 1
a1090 1
		    "pa %lx length %lld/0x%llx sgstart %llx sgend %llx\n",
d1223 1
a1223 1
				    "pa 0x%llx (%llx - %llx for %d/%x\n",
d1328 1
a1328 1
		printf("warning: %qd of DVMA space lost\n", sgsize);
d1404 2
a1405 2
	printf("DVMA %x for %x, mapping %p: dvstart %llx dvsize %llx "
	    "size %lld/%llx maxsegsz %llx boundary %llx segcnt %d "
d1407 1
a1407 1
	    "cookie %p mapsize %llx nsegs %d\n",
d1417 1
a1417 1
		printf("seg %d dvmaddr %llx pa %lx len %llx (tte %llx)\n",
d1502 1
a1502 1
		panic("iommu_dvmamap_sync: too short %llu", offset);
d1514 1
a1514 1
		panic("iommu_dvmamap_sync: leftover %llu", len);
@


1.66
log
@Fix segmentation of buffers that straddle a boundary.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.65 2012/08/17 20:46:50 kettenis Exp $	*/
d229 1
a229 1
	    M_DEVBUF, 0, 0, EX_NOWAIT);
d752 1
a752 1
	err = extent_alloc_subregion(is->is_dvmamap, sgstart, sgend,
d754 1
a754 1
	    EX_NOWAIT | EX_BOUNDZERO, (u_long *)&dvmaddr);
d959 1
a959 1
	err = extent_alloc_subregion(is->is_dvmamap, sgstart, sgend,
d961 1
a961 1
	    EX_NOWAIT | EX_BOUNDZERO, (u_long *)&dvmaddr);
@


1.65
log
@Properly initialize the IOMMU control and status register for pyro(4).
Fixes DMA problems spotted on the v445.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.64 2011/05/18 23:36:31 ariane Exp $	*/
d1137 1
a1137 1
			seg->ds_len = boundary - (sgstart & bd_mask);
@


1.64
log
@When the sparc64 iommu fails to lookup a vaddr, it prints a message and
returns EFBIG. This cannot be recovered from and is usually indicative
of a damaged pmap. Therefor, panic right here instead.

This diff meant the difference between dropping into single-user mode versus
getting a traceable panic.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.63 2011/04/07 15:30:16 miod Exp $	*/
d152 7
a158 1
	is->is_cr = IOMMUCR_EN;
@


1.63
log
@Do not use NULL in integer comparisons. No functional change.
ok matthew@@ tedu@@, also eyeballed by at least krw@@ oga@@ kettenis@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.62 2010/04/20 23:26:59 deraadt Exp $	*/
d716 2
a717 5
			if (pmap_extract(pmap, a, &pa) == FALSE) {
				printf("iomap pmap error addr 0x%llx\n", a);
				iommu_iomap_clear_pages(ims);
				return (EFBIG);
			}
d791 2
a792 5
			if (pmap_extract(pmap, a, &pa) == FALSE) {
				printf("iomap pmap error addr 0x%llx\n", a);
				err =  EFBIG;
				break;
			}
@


1.62
log
@cleanup more confusion regarding user.h before proc.h, or missing proc.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.61 2009/08/09 13:35:43 oga Exp $	*/
d1079 1
a1079 1
	if (sgstart == NULL || sgstart > sgend) {
d1702 1
a1702 1
	e->ipe_va = NULL;
d1790 1
a1790 1
		return (NULL);
@


1.61
log
@if extent_alloc() fails, we don't clear the iomap properly before
returning an error. so next time we mess around, we may get annoying
printfs.

Fix this.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.60 2009/05/04 16:48:37 oga Exp $	*/
d41 1
@


1.60
log
@type pedantry.

the type we bind to an iommu or a GART is paddr_t, by definition, on the
other hand, the type we get out of it is not a vaddr_t, it's bus_addr_t.

fix up sparc64 iommu, amd64 iommu and the sg_dma backedn that uses it to
realise this.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.59 2009/05/03 13:44:05 kettenis Exp $	*/
d763 2
a764 1
	if (err != 0)
d766 1
d963 2
a964 1
	if (err != 0)
d966 1
@


1.59
log
@Avoid clobbering error return values with the result from extent_free(), by
simply calling iommu_dvma_unload().  Solution suggested by oga@@.

While there, also unwrap a line that isn't long enough to need wrapping.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.58 2009/05/02 18:39:27 kettenis Exp $	*/
d74 4
a77 4
void iommu_enter(struct iommu_state *, struct strbuf_ctl *, vaddr_t, paddr_t,
    int);
void iommu_remove(struct iommu_state *, struct strbuf_ctl *, vaddr_t);
int iommu_dvmamap_sync_range(struct strbuf_ctl*, vaddr_t, bus_size_t);
d89 1
a89 1
int64_t iommu_tsb_entry(struct iommu_state *, vaddr_t);
d92 1
a92 1
vaddr_t iommu_iomap_translate(struct iommu_map_state *, paddr_t);
d94 1
a94 1
    vaddr_t, int);
d106 1
a106 1
iommu_strbuf_flush(struct strbuf_ctl *sb, vaddr_t va)
d309 1
a309 1
iommu_enter(struct iommu_state *is, struct strbuf_ctl *sb, vaddr_t va,
d368 1
a368 1
iommu_remove(struct iommu_state *is, struct strbuf_ctl *sb, vaddr_t va)
d415 1
a415 1
iommu_extract(struct iommu_state *is, vaddr_t dva)
d429 1
a429 1
iommu_lookup_tte(struct iommu_state *is, vaddr_t dva)
d458 1
a458 1
iommu_tsb_entry(struct iommu_state *is, vaddr_t dva)
d708 1
a708 1
		bus_addr_t addr = (vaddr_t)buf;
d780 1
a780 1
		bus_addr_t addr = (vaddr_t)buf;
d1549 1
a1549 1
iommu_dvmamap_sync_range(struct strbuf_ctl *sb, vaddr_t va, bus_size_t len)
d1551 1
a1551 1
	vaddr_t vaend;
d1716 1
a1716 1
    vaddr_t vmaddr, int flags)
d1772 1
a1772 1
vaddr_t
@


1.58
log
@Avoid clobbering error return values with the result from extent_free(), by
simply calling iommu_dvma_unload().  Solution suggested by oga@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.57 2009/04/14 16:01:04 oga Exp $	*/
d1318 1
a1318 2
	error = extent_free(is->is_dvmamap, dvmaddr, 
		sgsize, EX_NOWAIT);
@


1.57
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.56 2009/04/05 21:57:41 oga Exp $	*/
d833 2
a834 12
	if (err) {
		/* XXX keep enough state and just call unload here? */
		iommu_iomap_unload_map(is, ims);
		iommu_iomap_clear_pages(ims);
		map->dm_mapsize = 0;
		map->dm_nsegs = 0;
		mtx_enter(&is->is_mtx);
		err = extent_free(is->is_dvmamap, dvmaddr, sgsize, EX_NOWAIT);
		map->_dm_dvmastart = 0;
		map->_dm_dvmasize = 0;
		mtx_leave(&is->is_mtx);
	}
d1043 2
a1044 12
	if (err) {
		/* XXX keep enough state and just call unload here? */
		iommu_iomap_unload_map(is, ims);
		iommu_iomap_clear_pages(ims);
		map->dm_mapsize = 0;
		map->dm_nsegs = 0;
		mtx_enter(&is->is_mtx);
		err = extent_free(is->is_dvmamap, dvmaddr, sgsize, EX_NOWAIT);
		map->_dm_dvmastart = 0;
		map->_dm_dvmasize = 0;
		mtx_leave(&is->is_mtx);
	}
@


1.56
log
@In the rare case where after we've loaded the iomap into the hardware,
if we fail while assembling the dmamap due to the memory not fitting
into our constraints we'll return from the function with the iomap still
loaded, and more importantly with memory still allocated from the
extent(9). So in such a case, make sure we clean up after outselves.

In order to make this cleaner, remove an impossible condition check
(kettenis and myself are satisfied that it will never happen), and make
iomap_load_map void (it can't fail), so that we can only fail after both
the extent is allocated and the iomap is loaded, and not inbetween the
two.

I tested iommu, kettenis tested viommu.

ok kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.55 2009/03/16 21:00:48 oga Exp $	*/
d169 1
a169 1
		(paddr_t)PAGE_SIZE, (paddr_t)0, &mlist, 1, 0) != 0)
@


1.55
log
@Trivial malloc + memset -> malloc(,,M_ZERO) conversion.

As a small bonus this now actually zeroes the whole struct, not just the
non-varying sized part.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.54 2009/03/16 19:45:32 oga Exp $	*/
d93 1
a93 1
int iommu_iomap_load_map(struct iommu_state *, struct iommu_map_state *,
d95 1
a95 1
int iommu_iomap_unload_map(struct iommu_state *, struct iommu_map_state *);
a765 3
	if (dvmaddr == (bus_addr_t)-1)
		return (ENOMEM);

d776 1
a776 2
	if (iommu_iomap_load_map(is, ims, dvmaddr, flags))
		return (EFBIG);
d793 2
a794 2
				iommu_iomap_clear_pages(ims);
				return (EFBIG);
d808 2
a809 2
				return (err);
			if (err) {
d814 1
a814 1
				return (err);
a817 1

d833 12
a985 2
	if (dvmaddr == (bus_addr_t)-1)
		return (ENOMEM);
d997 1
a997 2
	if (iommu_iomap_load_map(is, ims, dvmaddr, flags))
		return (EFBIG);
a1005 3
	if (err)
		iommu_iomap_unload_map(is, ims);

d1053 12
d1735 1
a1735 1
int
a1764 2

	return (0);
d1770 1
a1770 1
int
a1787 2

	return (0);
@


1.54
log
@pmap = (conditional ? user_way : pmap = pmap_kernel());

doesn't need the second pmap =. "ok if you tested it" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.53 2009/01/02 20:01:45 kettenis Exp $	*/
d1636 1
a1636 1
		M_DEVBUF, M_NOWAIT);
a1638 2

	memset(ims, 0, sizeof *ims);
@


1.53
log
@Make IOMMU code use the generic _bus_dmamem_map() and _bus_dmamem_unmap().
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.52 2008/11/25 16:31:19 kettenis Exp $	*/
d702 1
a702 1
	pmap = p ? p->p_vmspace->vm_map.pmap : pmap = pmap_kernel();
@


1.52
log
@Fix dmesg ugliness caused by not printing iotdb stuff.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.51 2008/08/11 21:28:53 kettenis Exp $	*/
a1617 84
}

/*
 * Map the DVMA mappings into the kernel pmap.
 * Check the flags to see whether we're streaming or coherent.
 */
int
iommu_dvmamem_map(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dma_segment_t *segs,
    int nsegs, size_t size, caddr_t *kvap, int flags)
{
	struct vm_page *m;
	vaddr_t va;
	bus_addr_t addr;
	struct pglist *mlist;
	bus_addr_t cbit = 0;

	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_map: segp %p nsegs %d size %lx\n",
	    segs, nsegs, size));

	/*
	 * Allocate some space in the kernel map, and then map these pages
	 * into this space.
	 */
	size = round_page(size);
	va = uvm_km_valloc(kernel_map, size);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	/* 
	 * digest flags:
	 */
#if 0
	if (flags & BUS_DMA_COHERENT)	/* Disable vcache */
		cbit |= PMAP_NVC;
#endif
	if (flags & BUS_DMA_NOCACHE)	/* sideffects */
		cbit |= PMAP_NC;

	/*
	 * Now take this and map it into the CPU.
	 */
	mlist = segs[0]._ds_mlist;
	TAILQ_FOREACH(m, mlist, pageq) {
#ifdef DIAGNOSTIC
		if (size == 0)
			panic("iommu_dvmamem_map: size botch");
#endif
		addr = VM_PAGE_TO_PHYS(m);
		DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_map: "
		    "mapping va %lx at %llx\n", va,
		    (unsigned long long)addr | cbit));
		pmap_enter(pmap_kernel(), va, addr | cbit,
		    VM_PROT_READ | VM_PROT_WRITE,
		    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
		va += PAGE_SIZE;
		size -= PAGE_SIZE;
	}
	pmap_update(pmap_kernel());

	return (0);
}

/*
 * Unmap DVMA mappings from kernel
 */
void
iommu_dvmamem_unmap(bus_dma_tag_t t, bus_dma_tag_t t0, caddr_t kva,
    size_t size)
{
	
	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_unmap: kvm %p size %lx\n",
	    kva, size));
	    
#ifdef DIAGNOSTIC
	if ((u_long)kva & PAGE_MASK)
		panic("iommu_dvmamem_unmap");
#endif
	
	size = round_page(size);
	pmap_remove(pmap_kernel(), (vaddr_t)kva, size);
	pmap_update(pmap_kernel());
	uvm_km_free(kernel_map, (vaddr_t)kva, size);
@


1.51
log
@Only print iotdb stuff when DEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.50 2008/07/12 13:08:04 kettenis Exp $	*/
d214 1
a214 1
	printf("dvma map %x-%x, ", is->is_dvmabase, is->is_dvmaend);
d216 1
a216 1
	printf("iotdb %llx-%llx",
@


1.50
log
@Perform IOMMU cache flushes on Oberon.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.48 2007/12/05 21:15:46 deraadt Exp $	*/
d215 1
d219 1
@


1.49
log
@move some iommu_dvmamap_validate_map()'s into DEBUG instead of DIAGNOSTIC, to
slightly improve performance
ok kettenis
@
text
@d266 3
d1813 1
a1813 1
	int i;
d1826 8
d1849 1
a1849 1
	int i;
d1851 1
a1851 1
	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e)
d1853 8
@


1.48
log
@use mutexes to protect the iommu's extent map and the streaming buffer,
so that busdma is most likely MP_SAFE now.  (while there, fix an extent
map race... the ranges were selected outside splhigh)
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.47 2007/05/29 09:53:59 sobrado Exp $	*/
d818 1
a818 1
#ifdef DIAGNOSTIC
a819 1
#endif
a820 1
#ifdef DEBUG
d1000 1
a1000 1
#ifdef DIAGNOSTIC
a1034 3
#endif

#ifdef DEBUG
d1300 1
a1300 1
	/* XXX is this not supposed to be debug-only code by now? */
d1329 1
d1392 1
@


1.47
log
@use the right capitalization for `SBus'

ok jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.46 2007/04/04 20:09:37 kettenis Exp $	*/
d221 1
d293 1
d487 2
d541 1
a645 1
	int s;
d730 1
a742 1
	s = splhigh();
d746 1
a746 1
	splx(s);
d855 1
a855 1
	int i, s;
d943 1
a955 1
	s = splhigh();
d959 1
a959 1
	splx(s);
d1282 1
a1282 1
	int error, s;
d1305 1
d1324 1
a1324 1
	s = splhigh();
d1329 1
a1329 1
	splx(s);
@


1.46
log
@Correctly set the TSB size on pyro(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.45 2007/04/02 13:23:46 claudio Exp $	*/
d120 1
a120 1
 * initialise the UltraSPARC IOMMU (SBUS or PCI):
d138 1
a138 1
	 * The sun4u iommu is part of the SBUS or PCI controller so we will
d563 1
a563 1
 * IOMMU DVMA operations, common to SBUS and PCI.
@


1.45
log
@Return EFBIG in case not enough segments are present to load a DMA request
instead of ENOMEM and remove/shortcut the additional debug printfs.
Some network drivers will try to load the mbuf chain and linearize the mbufs
if EFBIG is returned.
OK miod@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.44 2007/01/26 16:53:28 tsi Exp $	*/
d151 1
a151 1
	is->is_cr = (tsbsize << 16) | IOMMUCR_EN;
d223 10
a232 1
	 * now actually start up the IOMMU
@


1.44
log
@Deal with zero wraparound in DVMA range checks;
Fix default DVMA range for Schizo's with no "virtual-dma" property;
Add TSB size indicator to Schizo & Psycho debugging messages.

ok jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.43 2006/09/01 20:07:57 miod Exp $	*/
d793 2
a1100 2
			printf("append range, out of segments (%d)\n", i);
			iommu_dvmamap_print_map(t, NULL, map);
d1102 1
a1102 1
			return (ENOMEM);
a1132 2
				printf("append range, out of segments\n");
				iommu_dvmamap_print_map(t, NULL, map);
d1208 2
d1246 2
@


1.43
log
@Standardize on EFBIG in bus_dmamap_load* if caller is too greedy; I had
done this 4.5 years ago already but regressions happened; reminded by
a similar commit in NetBSD (from mrg@@); ok damien@@ deraadt@@ jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.42 2006/08/15 20:55:57 miod Exp $	*/
d153 1
a153 1
	if (iovabase == -1) {
d158 1
a158 1
		is->is_dvmaend = iovabase + IOTSB_VSIZE(tsbsize);
d214 1
a214 1
	printf("dvma map %x-%x, ", is->is_dvmabase, is->is_dvmaend - 1);
d219 1
a219 1
	    is->is_dvmabase, is->is_dvmaend - PAGE_SIZE,
d300 1
a300 2
	if (va < is->is_dvmabase || round_page(va + PAGE_SIZE) >
	    is->is_dvmaend + 1)
d358 1
a358 2
	if (trunc_page(va) < is->is_dvmabase || round_page(va) >
	    is->is_dvmaend + 1)
d446 1
a446 1
	if (dva < is->is_dvmabase && dva > is->is_dvmaend)
d1339 2
a1340 2
	    round_page(map->_dm_dvmastart + map->_dm_dvmasize) >
	    is->is_dvmaend + 1) {
d1559 1
a1559 1
	if (va < is->is_dvmabase || vaend > is->is_dvmaend)
@


1.42
log
@Allocate as many iommu page lists as necessary to match the size request
in bus_dmamap_create() again, but this time add a few extra pages to cope
with fragmented data (such as mbufs). Tested by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.41 2006/07/02 06:22:03 dlg Exp $	*/
d702 1
a702 1
				return (E2BIG);
d713 1
a713 1
				return (E2BIG);
d764 1
a764 1
		return (E2BIG);
d782 1
a782 1
				return (E2BIG);
d898 1
a898 1
				return (E2BIG);
d921 1
a921 1
					return (E2BIG);
d977 1
a977 1
		return (E2BIG);
d1138 1
a1138 1
				return (E2BIG);
@


1.41
log
@get rid of an unused variable.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.40 2006/07/01 16:41:26 deraadt Exp $	*/
d582 1
a582 1
	ims = iommu_iomap_create(nsegments);
d896 1
d919 1
d1698 4
a1701 2
	if (n < 64)
		n = 64;
@


1.40
log
@clean dmesg output for iommu more; tested miod dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.39 2006/06/28 20:09:15 deraadt Exp $	*/
d253 1
a253 3
		if (sb->sb_flush) {
			char buf[64];
			bus_space_render_tag(sb->sb_bustag, buf, sizeof buf);
a254 1
		}
@


1.39
log
@cleanup dmesg logging for iommu goo, make it follow the standard form
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.38 2005/06/07 20:40:01 kurt Exp $	*/
d226 1
a252 1

d256 1
a256 1
			printf("STC%d on %s enabled\n", i, buf);
@


1.38
log
@revert previous commit and unbreak sparc64.

okay deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.36 2004/12/25 23:02:25 miod Exp $	*/
d68 1
a68 1
int iommudebug = 0x0;
a211 5
	 * now actually start up the IOMMU
	 */
	iommu_reset(is);

	/*
d214 2
a215 2
	printf("DVMA map: %x to %x\n", is->is_dvmabase, is->is_dvmaend);
	printf("IOTDB: %llx to %llx\n", 
d221 5
@


1.37
log
@allow for dma maps larger than 64 pages; jason@@ ok
@
text
@d585 1
a585 1
	ims = iommu_iomap_create((size + PAGE_MASK) >> PAGE_SHIFT);
d1698 3
@


1.36
log
@Use list and queue macros where applicable to make the code easier to read;
no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.35 2004/03/19 21:04:00 miod Exp $	*/
d585 1
a585 1
	ims = iommu_iomap_create(nsegments);
a1697 3

	if (n < 64)
		n = 64;
@


1.35
log
@Off-by-ones, requests for (x * PAGE_SIZE) + 1 bytes would not allocate the
last page.  Found the hard way by chris@@ and claudio@@.

ok jason@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.34 2003/12/20 20:08:17 miod Exp $	*/
d1647 1
a1647 1
	for (m = mlist->tqh_first; m != NULL; m = m->pageq.tqe_next) {
@


1.34
log
@Pass -Wformat
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.33 2003/12/04 21:13:37 miod Exp $	*/
d698 1
a698 1
		aend = round_page(addr + seg_len - 1);
d774 1
a774 1
		aend = round_page(addr + seg_len - 1);
d914 1
a914 1
			aend = round_page(addr + seg_len - 1);
d1197 2
a1198 3
		aend = addr + seg_len - 1;
		for (a = trunc_page(addr); a < round_page(aend);
		    a += PAGE_SIZE) {
@


1.33
log
@Typos
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.32 2003/06/11 04:00:11 henric Exp $	*/
d1491 1
a1491 1
		panic("iommu_dvmamap_sync: too short %lu", offset);
d1503 1
a1503 1
		panic("iommu_dvmamap_sync: leftover %lu", len);
@


1.32
log
@It is important not to forget "membar(MemIssue)".  Got missed
when extracting the minimal diff for the previous iommu change.

ok jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.31 2003/06/11 03:16:12 henric Exp $	*/
d262 1
a262 1
 * Inititalize one STC.
@


1.31
log
@Much of the sbus, psycho, and schizo bus_dma code is the same, so let's
call the real implementation functions directly instead of duplicating
lots of code that only calls the real stuff anyway.

tested by miod@@ henning@@
ok jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.30 2003/06/11 03:07:41 henric Exp $	*/
d99 2
d1472 1
a1472 1
iommu_dvmamap_sync(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
a1480 9
#ifdef DEBUG
	if (ims == NULL)
		panic("iommu_dvmamap_sync: null map state");
	if (ims->ims_sb == NULL)
                panic("iommu_dvmamap_sync: null sb");
        if (ims->ims_sb->sb_iommu == NULL)
                panic("iommu_dvmamap_sync: null iommu");
#endif /* DEBUG */

a1483 9
	if ((ims->ims_flags & IOMMU_MAP_STREAM) == 0 || (len == 0))
		return;

	if (ops & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTWRITE))
		return;

	if ((ops & (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_PREWRITE)) == 0)
		return;

d1501 1
d1504 1
d1508 28
@


1.30
log
@The "min" and "max" functions cast to "u_int", which can lead to surprising
results.  Use the MIN/MAX macros instead.

tested by miod@@
ok jason@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.29 2003/05/22 21:16:29 henric Exp $	*/
d559 8
d568 3
a570 3
iommu_dvmamap_create(bus_dma_tag_t t, struct iommu_state *is,
    struct strbuf_ctl *sb, bus_size_t size, int nsegments, bus_size_t maxsegsz,
    bus_size_t boundary, int flags, bus_dmamap_t *dmamap)
d576 3
a578 2
	ret = bus_dmamap_create(t->_parent, size, nsegments, maxsegsz,
	    boundary, flags, &map);
d586 1
a586 1
		bus_dmamap_destroy(t->_parent, map);
d592 7
d605 1
a605 1
iommu_dvmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
d612 1
a612 1
		bus_dmamap_unload(t, map);
d618 2
a619 1
	bus_dmamap_destroy(t->_parent, map);
d632 1
a632 1
iommu_dvmamap_load(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map,
d640 1
d648 7
d664 1
a664 1
		bus_dmamap_unload(t, map);
d840 2
a841 3
iommu_dvmamap_load_raw(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, bus_dma_segment_t *segs, int nsegs, int flags,
    bus_size_t size)
d849 1
d856 7
d869 1
a869 1
		bus_dmamap_unload(t, map);
d1263 1
a1263 1
iommu_dvmamap_unload(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map)
d1265 1
d1271 11
d1470 1
a1470 1
iommu_dvmamap_sync(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map,
d1473 1
d1479 1
a1479 1
#ifdef DIAGNOSTIC
d1482 6
a1487 1
#endif
d1489 1
d1567 1
a1567 1
iommu_dvmamem_alloc(bus_dma_tag_t t, struct iommu_state *is, bus_size_t size,
d1576 2
a1577 1
	return (bus_dmamem_alloc(t->_parent, size, alignment, boundary,
d1582 2
a1583 2
iommu_dvmamem_free(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *segs, int nsegs)
d1588 2
a1589 1
	bus_dmamem_free(t->_parent, segs, nsegs);
d1597 2
a1598 2
iommu_dvmamem_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *segs, int nsegs, size_t size, caddr_t *kvap, int flags)
d1658 1
a1658 1
iommu_dvmamem_unmap(bus_dma_tag_t t, struct iommu_state *is, caddr_t kva,
@


1.29
log
@There's an off-by-one in the diagnostic code that can cause grief,
especially for NIC drivers that aren't careful about handling load
failures.

Bug reported and diff tested by Holger Burde.

ok jason@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.28 2003/03/06 08:26:08 henric Exp $	*/
d660 1
a660 1
	align = max(map->dm_segs[0]._ds_align, PAGE_SIZE);
d696 2
a697 2
		sgstart = max(is->is_dvmamap->ex_start, 0xff000000);
		sgend = min(is->is_dvmamap->ex_end, 0xffffffff);
d761 2
a762 2
			pgstart = pa | (max(a, addr) & PAGE_MASK);
			pgend = pa | (min(a + PAGE_SIZE - 1,
d847 1
a847 1
	align = max(segs[0]._ds_align, PAGE_SIZE);
d875 1
a875 1
			int seg_len = min(left, len);
d898 2
a899 2
		sgstart = max(is->is_dvmamap->ex_start, 0xff000000);
		sgend = min(is->is_dvmamap->ex_end, 0xffffffff);
d971 1
a971 1
				int seg_len = min(left, len);
d1158 1
a1158 1
		int seg_len = min(left, len);
d1171 2
a1172 2
			pgstart = max(a, addr);
			pgend = min(a + PAGE_SIZE - 1, addr + seg_len - 1);
d1459 1
a1459 1
		count = min(map->dm_segs[i].ds_len - offset, len);
@


1.28
log
@The existing IOMMU code had a rounding problem that was most noticeable
on faster systems under heavy network load.  This replaces some of the
unreadable iommu functions with something a little less dense and a lot
less crash prone.

The bus_dma function pointer/cookie handling was broken.  Change them
to work like the stacked bus_space drivers (where "work" is the key
word).

Tested my many (thanks).

ok jason@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.27 2003/02/22 23:51:39 jason Exp $	*/
d1035 1
a1035 1
	if (sgstart == NULL || sgstart >= sgend) {
d1484 1
a1484 1
	if (va < is->is_dvmabase || va >= is->is_dvmaend)
d1498 1
a1498 1
	if (va < is->is_dvmabase || vaend >= is->is_dvmaend)
@


1.27
log
@don't use home grown timeval comparison when timercmp() is available; pointed out by henric
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.26 2003/02/21 00:47:56 jason Exp $	*/
d5 1
d42 1
d64 4
a67 1
#define	IDB_SYNC	0x8
d74 25
a98 3
int iommu_dvmamap_sync_seg(bus_dma_tag_t, struct iommu_state *,
    bus_dma_segment_t *, bus_addr_t, bus_size_t, int);
int iommu_dvmamap_sync_range(struct iommu_state *, vaddr_t, bus_size_t);
d100 3
d104 1
a104 1
iommu_strbuf_flush(struct iommu_state *is, vaddr_t va)
d106 6
a111 5
	int i;
	for(i = 0; i < 2; ++i) {
		struct strbuf_ctl *sb = is->is_sb[i];
		if(sb == NULL || sb->sb_flush == NULL)
			continue;
d113 2
a114 3
		bus_space_write_8(sb->sb_bustag, sb->sb_sb,
		    STRBUFREG(strbuf_pgflush), va);
	}
a116 3
int iommu_strbuf_flush_done(struct iommu_state *);
int64_t iommu_tsb_entry(struct iommu_state *, vaddr_t);

d164 1
a164 1
	size = NBPG << is->is_tsbsize;
d167 1
a167 1
		(paddr_t)NBPG, (paddr_t)0, &mlist, 1, 0) != 0)
d184 1
a184 1
		va += NBPG;
a210 1
	 * Don't start the thing until it can see all the TSB data
a211 1
	membar(MemIssue);
d222 1
a222 1
	    is->is_dvmabase, is->is_dvmaend - NBPG,
d227 1
a227 1
 * Streaming buffers don't exist on the UltraSPARC IIi; we should have
a235 2
	/* Need to do 64-bit stores */

d238 2
a239 2
	/* Enable IOMMU in diagnostic mode */
	IOMMUREG_WRITE(is, iommu_cr, is->is_cr | IOMMUCR_DE);
d241 1
a241 1
	for (i = 0; i < 2; i++) {
d244 1
a244 1
		if(sb == NULL || sb->sb_flush == NULL)
d247 3
a249 14
		/* Enable diagnostics mode? */
		bus_space_write_8(sb->sb_bustag, sb->sb_sb,
		    STRBUFREG(strbuf_ctl), STRBUF_EN);

		membar(Lookaside);

		/* No streaming buffers? Disable them */
		if (bus_space_read_8(sb->sb_bustag, sb->sb_sb,
		    STRBUFREG(strbuf_ctl)) == 0) {
			sb->sb_flush = NULL;
		} else {
			/*
			 * locate the pa of the flush buffer
			 */
d251 4
a254 2
			pmap_extract(pmap_kernel(),
			    (vaddr_t)sb->sb_flush, &sb->sb_flushpa);
d260 32
a291 1
 * Here are the iommu control routines. 
d294 2
a295 1
iommu_enter(struct iommu_state *is, vaddr_t va, int64_t pa, int flags)
d298 1
a298 1
	int strbuf = flags & BUS_DMA_STREAMING;
d301 2
a302 1
	if (va < is->is_dvmabase || va > is->is_dvmaend)
a303 1
#endif
d305 7
a311 4
	/* Is the streamcache flush really needed? */
	if (is->is_sb[0] != NULL || is->is_sb[1] != NULL) {
		iommu_strbuf_flush(is, va);
		iommu_strbuf_flush_done(is);
d313 1
a313 2
	else
		strbuf = 0;
d316 4
a319 4
	    !(flags & BUS_DMA_NOCACHE), (strbuf));
#ifdef DEBUG
	tte |= (flags & 0xff000LL) << (4 * 8); /* DEBUG */
#endif /* DEBUG */
d321 1
d323 13
a335 5
	DPRINTF(IDB_IOMMU, ("Clearing TSB slot %d for va %p\n", 
		       (int)IOTSBSLOT(va,is->is_tsbsize), (void *)(u_long)va));
	is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)] = tte;
 	/* Make is->is_tsb[] change globally visible.  Needed? */
	membar(MemIssue);
d339 57
a395 4
		       va, (long)pa, (u_long)IOTSBSLOT(va,is->is_tsbsize), 
		       (void *)(u_long)
			   &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
		       (u_long)tte));
d399 1
a399 1
 * Find the value of a DVMA address (debug routine).
d406 1
a406 1
	if (dva >= is->is_dvmabase && dva < is->is_dvmaend)
a408 2
	if ((tte & IOTTE_V) == 0)
		return ((paddr_t)-1L);
d413 30
a442 1
 * Fetch a tsb entry with some sanity checking.
d449 1
a449 1
	if (dva < is->is_dvmabase && dva >= is->is_dvmaend)
a451 2
	membar(Lookaside);

d455 1
a455 1
		panic("iommu_tsb_entry: invalid entry %llx", (long long)dva);
d461 1
a461 5
 * iommu_remove: removes mappings created by iommu_enter
 *
 * Only demap from IOMMU if flag is set.
 *
 * XXX: this function needs better internal error checking.
a462 62
void
iommu_remove(struct iommu_state *is, vaddr_t va, size_t len)
{
#ifdef DIAGNOSTIC
	if (va < is->is_dvmabase || va > is->is_dvmaend)
		panic("iommu_remove: va 0x%lx not in DVMA space", (u_long)va);
	if ((long)(va + len) < (long)va)
		panic("iommu_remove: va 0x%lx + len 0x%lx wraps", 
		      (long) va, (long) len);
	if (len & ~0xfffffff) 
		panic("iommu_remove: rediculous len 0x%lx", (u_long)len);
#endif

	va = trunc_page(va);
	DPRINTF(IDB_IOMMU, ("iommu_remove: va %lx TSB[%lx]@@%p\n",
	    va, (u_long)IOTSBSLOT(va,is->is_tsbsize), 
	    &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)]));
	while (len > 0) {
		/* NetBSD does *not* flush the streaming buffer (here, anyway) */
		DPRINTF(IDB_IOMMU,
		    ("iommu_remove: clearing TSB slot %d for va %p size %lx\n", 
		    (int)IOTSBSLOT(va,is->is_tsbsize),
		    (void *)(u_long)va, (u_long)len));
		if (is->is_sb[0] != NULL || is->is_sb[1] != NULL) {
			DPRINTF(IDB_IOMMU,
			    ("iommu_remove: flushing va %p TSB[%lx]@@%p=%lx, "
				"%lu bytes left\n", 	       
			       (void *)(u_long)va,
			       (long)IOTSBSLOT(va,is->is_tsbsize), 
			       (void *)(u_long)
				    &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
			       (long)
				   (is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)]), 
			       (u_long)len));
			iommu_strbuf_flush(is, va);
			if (len <= NBPG)
				iommu_strbuf_flush_done(is);
			DPRINTF(IDB_IOMMU,
			    ("iommu_remove: flushed va %p TSB[%lx]@@%p=%lx, "
				"%lu bytes left\n", 	       
			       (void *)(u_long)va,
			       (long)IOTSBSLOT(va,is->is_tsbsize), 
			       (void *)(u_long)
				   &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
			       (long)
				   (is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)]), 
			       (u_long)len));
		}

		if (len <= NBPG)
			len = 0;
		else
			len -= NBPG;

		/* XXX Zero-ing the entry would not require RMW */
		is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)] &= ~IOTTE_V;
		membar(MemIssue);   /* Needed? */
		IOMMUREG_WRITE(is, iommu_flush, va);
		va += NBPG;
	}
}

d464 1
a464 1
iommu_strbuf_flush_done(struct iommu_state *is)
d466 2
d470 2
a471 4
	u_int64_t flush[2];
	struct strbuf_ctl *sb[2];
	int i;
	int present[2];
d473 3
a475 4
	for(i = 0; i < 2; ++i) {
		sb[i] = is->is_sb[i];
		present[i] =
		    (sb[i] == NULL || sb[i]->sb_flush == NULL) ? 0 : 1;
d477 1
a477 3

	if (!present[0] && !present[1])
		return (0);
d482 1
a482 2
	 *   1 Tell strbuf to flush by storing va to strbuf_pgflush.  If
	 *     we're not on a cache line boundary (64-bits):
d487 1
a487 2
	 * If it takes more than .5 sec, something
	 * went wrong.
d491 1
a491 1
	 * If we're reading from the ASI_PHYS_CACHED, then we'll write to
d496 1
a496 3
	for(i = 0; i < 2; ++i)
		if(present[i])
			stxa(sb[i]->sb_flushpa, ASI_PHYS_CACHED, 0);
d500 2
a501 1
	 * memory is initialized before the IOMMU uses it
d503 1
a503 1
	membar(MemIssue);
d505 2
a506 5
	for(i = 0; i < 2; ++i) {
		if (present[i])
			bus_space_write_8(sb[i]->sb_bustag, sb[i]->sb_sb,
			    STRBUFREG(strbuf_flushsync), sb[i]->sb_flushpa);
	}
a507 3
	microtime(&cur);
	timeradd(&cur, &to, &flushtimeout);
	
d509 2
a510 9
	    ("iommu_strbuf_flush_done: flush[0] = %lx flush[1] = %lx "
		"pa[0] = %lx pa[1] = %lx now=%lx:%lx until = %lx:%lx\n", 
	       (long)present[0] ?
		   ldxa(sb[0]->sb_flushpa, ASI_PHYS_CACHED) : 1,
	       (long)present[1] ?
		   ldxa(sb[1]->sb_flushpa, ASI_PHYS_CACHED) : 1,
	       (long)sb[0]->sb_flushpa,
	       (long)sb[1]->sb_flushpa, cur.tv_sec, cur.tv_usec, 
	       flushtimeout.tv_sec, flushtimeout.tv_usec));
d512 1
a512 1
	membar(MemIssue | Lookaside);
a513 2
	/* Bypass non-coherent D$ */
	/* non-coherent...?   Huh? */
d515 1
a515 1
		membar(LoadLoad);
d517 17
a533 4
		flush[0] =
		    present[0] ? ldxa(sb[0]->sb_flushpa, ASI_PHYS_CACHED) : 1;
		flush[1] =
		    present[1] ? ldxa(sb[1]->sb_flushpa, ASI_PHYS_CACHED) : 1;
d535 1
a535 2
		if(flush[0] && flush[1])
			break;
d537 16
a552 3
		microtime(&cur);
		if (timercmp(&cur, &flushtimeout, >))
			break;
d554 13
d568 11
a578 12
#ifdef DIAGNOSTIC
	if (flush[0] == 0 || flush[1] == 0) {
		printf("iommu_strbuf_flush_done: flush timeout %p/%llx, "
		    "%p/%llx\n",
		    present[0] ? sb[0]->sb_flushpa : 0, flush[0],
		    present[1] ? sb[1]->sb_flushpa : 0, flush[1]);
		/* panic? */
#ifdef DDB
#if 0
		Debugger();
#endif
#endif
d580 23
a602 3
#endif
	DPRINTF(IDB_IOMMU, ("iommu_strbuf_flush_done: flushed\n"));
	return (flush[0] && flush[1]);
d606 7
a612 1
 * IOMMU DVMA operations, common to SBUS and PCI.
d619 1
a619 1
	int err;
a620 1
	paddr_t curaddr;
d623 1
a623 2
	vaddr_t vaddr = (vaddr_t)buf;
	int seg;
d626 5
d632 4
a635 1
		/* Already in use?? */
d641 1
d647 1
a647 1
	if (buflen > map->_dm_size) {
a653 2
	sgsize = round_page(buflen + ((int)vaddr & PGOFSET));

d660 34
a693 1
	align = max(map->dm_segs[0]._ds_align, NBPG);
d702 1
d705 1
a705 1
	 * split the transfer up int little pieces ourselves.
d718 2
a719 1
		Debugger();
a732 39
	/*
	 * Now split the DVMA range into segments, not crossing
	 * the boundary.
	 */
	seg = 0;
	sgstart = dvmaddr + (vaddr & PGOFSET);
	sgend = sgstart + buflen - 1;
	map->dm_segs[seg].ds_addr = sgstart;
	DPRINTF(IDB_INFO, ("iommu_dvmamap_load: boundary %lx boundary-1 %lx "
		"~(boundary-1) %lx\n", boundary, (boundary-1), ~(boundary-1)));
	while ((sgstart & ~(boundary - 1)) != (sgend & ~(boundary - 1))) {
		/* Oops.  We crossed a boundary.  Split the xfer. */
		DPRINTF(IDB_INFO, ("iommu_dvmamap_load: "
			"seg %d start %lx size %lx\n", seg,
			(long)map->dm_segs[seg].ds_addr, 
			map->dm_segs[seg].ds_len));
		map->dm_segs[seg].ds_len =
		    boundary - (sgstart & (boundary - 1));
		if (++seg >= map->_dm_segcnt) {
			/* Too many segments.  Fail the operation. */
			DPRINTF(IDB_INFO, ("iommu_dvmamap_load: "
				"too many segments %d\n", seg));
			s = splhigh();
			/* How can this fail?  And if it does what can we do? */
			err = extent_free(is->is_dvmamap,
				dvmaddr, sgsize, EX_NOWAIT);
			map->_dm_dvmastart = 0;
			map->_dm_dvmasize = 0;
			splx(s);
			return (E2BIG);
		}
		sgstart = roundup(sgstart, boundary);
		map->dm_segs[seg].ds_addr = sgstart;
	}
	map->dm_segs[seg].ds_len = sgend - sgstart + 1;
	DPRINTF(IDB_INFO, ("iommu_dvmamap_load: "
		"seg %d start %lx size %lx\n", seg,
		(long)map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len));
	map->dm_nsegs = seg + 1;
d735 3
a737 4
	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();
d739 2
a740 8
	for (; buflen > 0; ) {
		/*
		 * Get the physical address for this page.
		 */
		if (pmap_extract(pmap, (vaddr_t)vaddr, &curaddr) == FALSE) {
			bus_dmamap_unload(t, map);
			return (-1);
		}
d742 18
a759 6
		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = NBPG - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;
d761 18
a778 11
		DPRINTF(IDB_BUSDMA,
		    ("iommu_dvmamap_load: map %p loading va %p "
			    "dva %lx at pa %lx\n",
			    map, (void *)vaddr, (long)dvmaddr,
			    (long)(curaddr&~(NBPG-1))));
		iommu_enter(is, trunc_page(dvmaddr), trunc_page(curaddr),
		    flags | 0x4000);  /* 0x4000?  Magic...? */
			
		dvmaddr += PAGE_SIZE;
		vaddr += sgsize;
		buflen -= sgsize;
d780 1
d782 1
a782 11
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
		    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
			printf("seg %d dvmaddr %lx out of range %x - %x\n",
			    seg, (long)map->dm_segs[seg].ds_addr,
			    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
			Debugger();
#endif
		}
	}
a783 9
	return (0);
}


void
iommu_dvmamap_unload(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map)
{
	int error, s;
	bus_size_t sgsize;
a784 1
	/* Flush the iommu */
d786 6
a791 2
	if (!map->_dm_dvmastart) {
		printf("iommu_dvmamap_unload: No dvmastart is zero\n");
d793 2
a794 1
		Debugger();
a797 4
	iommu_remove(is, map->_dm_dvmastart, map->_dm_dvmasize);

	/* Flush the caches */
	bus_dmamap_unload(t->_parent, map);
d799 1
a799 16
	/* Mark the mappings as invalid. */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	sgsize = map->_dm_dvmasize;

	s = splhigh();
	error = extent_free(is->is_dvmamap, map->_dm_dvmastart, 
		map->_dm_dvmasize, EX_NOWAIT);
	map->_dm_dvmastart = 0;
	map->_dm_dvmasize = 0;
	splx(s);
	if (error != 0)
		printf("warning: %qd of DVMA space lost\n", (long long)sgsize);

	/* Clear the map */
d802 12
a813 1

d819 1
a819 2
	struct vm_page *m;
	int i, j, s;
d821 1
a821 1
	int err;
a822 1
	paddr_t pa;
d825 6
a830 3
	struct pglist *mlist;
	int pagesz = PAGE_SIZE;
	int npg = 0; /* DEBUG */
d847 1
a847 1
	align = max(segs[0]._ds_align, pagesz);
d853 41
a893 10
	/* Count up the total number of pages we need */
	pa = segs[0].ds_addr;
	sgsize = 0;
	left = size;
	for (i = 0; left && i < nsegs; i++) {
		if (round_page(pa) != round_page(segs[i].ds_addr))
			sgsize = round_page(sgsize);
		sgsize += min(left, segs[i].ds_len);
		left -= segs[i].ds_len;
		pa = segs[i].ds_addr + segs[i].ds_len;
d895 1
a895 1
	sgsize = round_page(sgsize);
d904 1
a904 1
	s = splhigh();
d909 1
d923 2
a924 1
		Debugger();
d937 1
a937 4
	if ((mlist = segs[0]._ds_mlist) == NULL) {
		u_long prev_va = NULL;
		paddr_t prev_pa = 0;
		int end = 0, offset;
a938 23
		/*
		 * This segs is made up of individual physical
		 * segments, probably by _bus_dmamap_load_uio() or
		 * _bus_dmamap_load_mbuf().  Ignore the mlist and
		 * load each one individually.
		 */
		map->dm_mapsize = size;

		j = 0;
		for (i = 0; i < nsegs; i++) {
			pa = segs[i].ds_addr;
			offset = (pa & PGOFSET);
			pa = trunc_page(pa);
			dvmaddr = trunc_page(dvmaddr);
			left = min(size, segs[i].ds_len);

			DPRINTF(IDB_INFO, ("iommu_dvamap_load_raw: converting "
			    "physseg %d start %lx size %lx\n", i,
			    (long)segs[i].ds_addr, segs[i].ds_len));

			if ((pa == prev_pa) &&
			    ((offset != 0) || (end != offset))) {
				/* We can re-use this mapping */
d940 1
a940 2
if (iommudebug & 0x10) printf("reusing dva %lx prev %lx pa %lx prev %lx\n",
    dvmaddr, prev_va, pa, prev_pa);
d942 39
a980 1
				dvmaddr = prev_va; 
d982 10
a991 2
			sgstart = dvmaddr + offset;
			sgend = sgstart + left - 1;
a992 4
			/* Are the segments virtually adjacent? */
			if ((j > 0) && (end == offset) &&
			    ((offset = 0) || (pa == prev_pa))) {
				/* Just append to the previous segment. */
d994 14
a1007 3
if (iommudebug & 0x10) {
printf("appending offset %x pa %lx, prev %lx dva %lx prev %lx\n",
    offset, pa, prev_pa, dvmaddr, prev_va);
a1008 1
#endif
d1010 15
a1024 18
				map->dm_segs[--j].ds_len += left; 
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
				    "appending seg %d start %lx size %lx\n", j,
				    (long)map->dm_segs[j].ds_addr,
				    map->dm_segs[j].ds_len));
			} else {
				if (j >= map->_dm_segcnt) {
					iommu_dvmamap_unload(t, is, map);
					return (E2BIG);
				}
				map->dm_segs[j].ds_addr = sgstart;
				map->dm_segs[j].ds_len = left;
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
				    "seg %d start %lx size %lx\n", j,
				    (long)map->dm_segs[j].ds_addr,
				    map->dm_segs[j].ds_len));
			}
			end = (offset + left) & PGOFSET;
d1026 4
a1029 18
			/* Check for boundary issues */
			while ((sgstart & ~(boundary - 1)) !=
				(sgend & ~(boundary - 1))) {
				/* Need a new segment. */
				map->dm_segs[j].ds_len =
				    boundary - (sgstart & (boundary - 1));
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
					"seg %d start %lx size %lx\n", j,
					(long)map->dm_segs[j].ds_addr, 
					(long)map->dm_segs[j].ds_len));
				if (++j >= map->_dm_segcnt) {
					iommu_dvmamap_unload(t, is, map);
					return (E2BIG);
				}
				sgstart = roundup(sgstart, boundary);
				map->dm_segs[j].ds_addr = sgstart;
				map->dm_segs[j].ds_len = sgend - sgstart + 1;
			}
d1031 2
a1032 2
			if (sgsize == 0)
				panic("iommu_dmamap_load_raw: size botch");
d1034 7
a1040 11
			/* Now map a series of pages. */
			while (dvmaddr <= sgend) {
				DPRINTF(IDB_BUSDMA,
				    ("iommu_dvamap_load_raw: map %p "
				    "loading va %lx at pa %lx\n",
				    map, (long)dvmaddr,
				    (long)(pa)));
				/* Enter if if we haven't before. */
				if (prev_va != dvmaddr)
#ifdef DEBUG
{ if (iommudebug & 0x10) printf("seg %d:5d entering dvma %lx, prev %lx pa %lx\n", i, j, dvmaddr, prev_va, pa);
d1042 1
a1042 3
					 iommu_enter(is, prev_va = dvmaddr,
					     prev_pa = pa,
					     flags | (++npg << 12)); 
d1044 5
a1048 1
} else if (iommudebug & 0x10) printf("seg %d:%d skipping dvma %lx, prev %lx\n", i, j, dvmaddr, prev_va);
d1051 55
a1105 2
				dvmaddr += pagesz;
				pa += pagesz;
d1107 24
d1132 45
a1176 3
			size -= left;
			++j;
		}
d1178 7
a1184 18
		map->dm_nsegs = j;
#ifdef DIAGNOSTIC
		{ /* Scope */
			int seg;
			for (seg = 0; seg < map->dm_nsegs; seg++) {
				if (map->dm_segs[seg].ds_addr <
				        is->is_dvmabase ||
				    map->dm_segs[seg].ds_addr >
					is->is_dvmaend) {
					printf("seg %d dvmaddr %lx out of "
					    "range %x - %x\n",
					    seg,
					    (long)map->dm_segs[seg].ds_addr,
					    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
					Debugger();
#endif
				}
d1186 1
d1188 2
a1189 2
#endif
		return (0);
d1191 16
d1211 10
a1220 22
	map->dm_mapsize = size;
	i = 0;
	sgstart = dvmaddr;
	sgend = sgstart + size - 1;
	map->dm_segs[i].ds_addr = sgstart;
	while ((sgstart & ~(boundary - 1)) != (sgend & ~(boundary - 1))) {
		/* Oops.  We crossed a boundary.  Split the xfer. */
		map->dm_segs[i].ds_len = boundary - (sgstart & (boundary - 1));
		DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
			"seg %d start %lx size %lx\n", i,
			(long)map->dm_segs[i].ds_addr,
			map->dm_segs[i].ds_len));
		if (++i >= map->_dm_segcnt) {
			/* Too many segments.  Fail the operation. */
			s = splhigh();
			/* How can this fail?  And if it does what can we do? */
			err = extent_free(is->is_dvmamap,
				dvmaddr, sgsize, EX_NOWAIT);
			map->_dm_dvmastart = 0;
			map->_dm_dvmasize = 0;
			splx(s);
			return (E2BIG);
a1221 2
		sgstart = roundup(sgstart, boundary);
		map->dm_segs[i].ds_addr = sgstart;
a1222 4
	DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
			"seg %d start %lx size %lx\n", i,
			(long)map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len));
	map->dm_segs[i].ds_len = sgend - sgstart + 1;
d1224 13
a1236 4
	for (m = TAILQ_FIRST(mlist); m != NULL; m = TAILQ_NEXT(m,pageq)) {
		if (sgsize == 0)
			panic("iommu_dmamap_load_raw: size botch");
		pa = VM_PAGE_TO_PHYS(m);
d1238 4
a1241 21
		DPRINTF(IDB_BUSDMA,
		    ("iommu_dvmamap_load_raw: map %p loading va %lx at "
			"pa %lx\n",
		    map, (long)dvmaddr, (long)(pa)));
		iommu_enter(is, dvmaddr, pa, flags | 0x8000); /* Magic 0x8000? */
			
		dvmaddr += pagesz;
		sgsize -= pagesz;
	}
	map->dm_mapsize = size;
	map->dm_nsegs = i + 1;
#ifdef DIAGNOSTIC
	{
		int seg;
		for (seg = 0; seg < map->dm_nsegs; seg++) {
			if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
			    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
				printf("seg %d dvmaddr %lx out of range %x "
				    "- %x\n",
				    seg, (long)map->dm_segs[seg].ds_addr,
				    is->is_dvmabase, is->is_dvmaend);
d1243 2
a1244 1
				Debugger();
d1246 79
a1324 1
			}
d1326 7
a1332 1
       }
d1334 89
a1422 1
	return (0);
d1429 2
d1434 13
a1446 1
	if (is->is_sb[0] == NULL && is->is_sb[1] == NULL)
d1460 2
a1461 2
		if(iommu_dvmamap_sync_seg(t, is, &map->dm_segs[i],
		    offset, count, ops))
d1470 1
a1470 1
		iommu_strbuf_flush_done(is);
a1476 1

d1478 1
a1478 1
iommu_dvmamap_sync_range(struct iommu_state *is, vaddr_t va, bus_size_t len)
d1481 2
a1483 4
	if (is->is_sb[0] == NULL && is->is_sb[1] == NULL)
		return (0);

#ifdef DIAGNOSTIC
a1485 1
#endif
d1487 3
a1489 1
	if ((is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)] & IOTTE_STREAM) == 0)
d1491 2
d1494 2
a1495 2
	vaend = (va + len + PGOFSET) & ~PGOFSET;
	va &= ~PGOFSET;
d1505 1
a1505 1
	for( ; va <= vaend; va += NBPG) {
d1509 1
a1509 1
		iommu_strbuf_flush(is, va);
a1515 26
iommu_dvmamap_sync_seg(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *seg, bus_addr_t offset, bus_size_t len, int ops)
{
	int needsflush = 0;
	vaddr_t va = seg->ds_addr + offset;

	DPRINTF(IDB_SYNC,
	    ("iommu_dvmamap_sync_seg: syncing va %p len %lu (%x)\n",
	    	(void *)(u_long)va, (u_long)len, ops));

	if (len == 0)
		return (0);

	if (ops & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTWRITE)) {
		/* Nothing to do */;
	}

	if (ops & (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_PREWRITE)) {
		if (iommu_dvmamap_sync_range(is, va, len))
			needsflush = 1;
	}

	return (needsflush);
}

int
d1570 1
d1573 1
d1613 1
a1613 1
	if ((u_long)kva & PGOFSET)
d1621 176
@


1.26
log
@Print the pa not ldxa(pa)
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.25 2003/02/21 00:01:17 jason Exp $	*/
a88 1
int iommu_tv_comp(struct timeval *, struct timeval *);
a393 15
int
iommu_tv_comp(struct timeval *t1, struct timeval *t2)
{
	if (t1->tv_sec < t2->tv_sec)
		return (-1);
	if (t1->tv_sec > t2->tv_sec)
		return (1);
	/* t1->tv_sec == t2->tv_sec */
	if (t1->tv_usec < t2->tv_usec)
		return (-1);
	if (t1->tv_usec > t2->tv_usec)
		return (1);
	return (0);
}

d478 1
a478 2
		/* Use existing time compare macros? */
		if(iommu_tv_comp(&cur, &flushtimeout) <= 0)
@


1.25
log
@- kill several unnecessary static's
- fix debugging code so it doesn't generate crashes itself
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.24 2003/02/17 01:29:20 henric Exp $	*/
d503 2
a504 4
		    present[0] ? ldxa(sb[0]->sb_flushpa, ASI_PHYS_CACHED) : 1,
		    flush[0],
		    present[1] ? ldxa(sb[1]->sb_flushpa, ASI_PHYS_CACHED) : 1,
		    flush[1]);
@


1.24
log
@
Add support for the Sun Enterprise 450
Reduce the size of a GENERIC kernel by ~190k
Remove the nasty pointer/bus_space_handle_t casts
Adds debug bus_space code including the ability to trace
    bus operations (it actually works now).

The following rules are now followed (and verfified by the debug
code):

1.  A "bus_space_handle_t" may only be used with the
    "bus_space_tag_t" that created it.
2.  Only "bus_space_map()" may create "bus_space_handle_t"s.
3.  A "bus_space_handle_t" may not be modified after it has
    been created (other than being destroyed by "bus_space_unmap()").


Thanks to help from mcbride, marc, jason, drahn, to anyone that might
have slipped my mind at the moment.

ok jason@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.23 2002/10/12 01:09:43 krw Exp $	*/
d74 1
a74 1
iommu_strbuf_flush(struct iommu_state* is, vaddr_t va)
d87 1
a87 1
static	int iommu_strbuf_flush_done(struct iommu_state *);
d89 1
a89 1
static	int iommu_tv_comp(struct timeval *, struct timeval *);
d395 1
a395 1
static int
d410 1
a410 1
static int 
d503 4
a506 2
		    (void *)sb[0]->sb_flushpa, flush[0],
		    (void *)sb[1]->sb_flushpa, flush[1]);
d509 1
d511 1
@


1.23
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.22 2002/10/07 18:35:56 mickey Exp $	*/
d71 1
d73 13
a85 12
#define iommu_strbuf_flush(i,v) do {				\
	if ((i)->is_sb[0])					\
		bus_space_write_8((i)->is_bustag,		\
			(bus_space_handle_t)(u_long)		\
			&(i)->is_sb[0]->strbuf_pgflush,		\
			0, (v));				\
	if ((i)->is_sb[1])					\
		bus_space_write_8((i)->is_bustag,		\
			(bus_space_handle_t)(u_long)		\
			&(i)->is_sb[1]->strbuf_pgflush,		\
			0, (v));				\
	} while (0)
d99 1
a99 5
iommu_init(name, is, tsbsize, iovabase)
	char *name;
	struct iommu_state *is;
	int tsbsize;
	u_int32_t iovabase;
d138 1
a138 1
	size = NBPG<<(is->is_tsbsize);
d161 1
a161 1
	bzero(is->is_tsb, size);
d164 1
a164 2
	if (iommudebug & IDB_INFO)
	{
d166 1
a166 2
		struct iommureg *regs = is->is_iommu;

d168 11
a178 5
		    (u_long)&regs->iommu_cr,
		    (u_long)&regs->iommu_tsb,
		    (u_long)&regs->iommu_flush);
		printf("iommu cr=%llx tsb=%llx\n", (unsigned long long)regs->iommu_cr, (unsigned long long)regs->iommu_tsb);
		printf("TSB base %p phys %llx\n", (void *)is->is_tsb, (unsigned long long)is->is_ptsb);
a183 7
	 * Initialize streaming buffer, if it is there.
	 */
	if (is->is_sb[0] || is->is_sb[1])
		(void)pmap_extract(pmap_kernel(), (vaddr_t)&is->is_flush[0],
		    &is->is_flushpa);

	/*
d185 1
d187 1
d193 1
d208 1
a208 2
iommu_reset(is)
	struct iommu_state *is;
a209 1
	struct iommu_strbuf *sb;
d213 3
a215 3
	bus_space_write_8(is->is_bustag, 
			  (bus_space_handle_t)(u_long)&is->is_iommu->iommu_tsb, 
			  0, is->is_ptsb);
d217 25
a241 16
	bus_space_write_8(is->is_bustag, 
			  (bus_space_handle_t)(u_long)&is->is_iommu->iommu_cr, 0, 
			  is->is_cr|IOMMUCR_DE);

	for (i=0; i<2; i++) {
		if ((sb = is->is_sb[i]) != NULL) {
			/* Enable diagnostics mode? */
			bus_space_write_8(is->is_bustag,
			    (bus_space_handle_t)(u_long)&sb->strbuf_ctl,
			    0, STRBUF_EN);

			/* No streaming buffers? Disable them */
			if (bus_space_read_8(is->is_bustag,
			    (bus_space_handle_t)(u_long)&sb->strbuf_ctl,
				0) == 0)
				is->is_sb[i] = 0;
d250 1
a250 5
iommu_enter(is, va, pa, flags)
	struct iommu_state *is;
	vaddr_t va;
	int64_t pa;
	int flags;
d253 1
a259 4
	tte = MAKEIOTTE(pa, !(flags&BUS_DMA_NOWRITE), !(flags&BUS_DMA_NOCACHE), 
			(flags&BUS_DMA_STREAMING));
tte |= (flags & 0xff000LL)<<(4*8);/* DEBUG */

d261 1
a261 1
	if (is->is_sb[0] || is->is_sb[1]) {
d265 10
d278 4
a281 2
	bus_space_write_8(is->is_bustag, (bus_space_handle_t)(u_long)
			  &is->is_iommu->iommu_flush, 0, va);
d284 2
a285 1
		       (void *)(u_long)&is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
a288 1

d293 1
a293 3
iommu_extract(is, dva)
	struct iommu_state *is;
	vaddr_t dva;
d298 1
a298 1
		tte = is->is_tsb[IOTSBSLOT(dva,is->is_tsbsize)];
d300 1
a300 1
	if ((tte&IOTTE_V) == 0)
d302 1
a302 1
	return (tte&IOTTE_PAMASK);
d309 1
a309 3
iommu_tsb_entry(is, dva)
	struct iommu_state *is;
	vaddr_t dva;
d316 2
d334 1
a334 4
iommu_remove(is, va, len)
	struct iommu_state *is;
	vaddr_t va;
	size_t len;
d351 15
a365 7
		DPRINTF(IDB_IOMMU, ("iommu_remove: clearing TSB slot %d for va %p size %lx\n", 
		    (int)IOTSBSLOT(va,is->is_tsbsize), (void *)(u_long)va, (u_long)len));
		if (is->is_sb[0] || is->is_sb[0]) {
			DPRINTF(IDB_IOMMU, ("iommu_remove: flushing va %p TSB[%lx]@@%p=%lx, %lu bytes left\n", 	       
			       (void *)(u_long)va, (long)IOTSBSLOT(va,is->is_tsbsize), 
			       (void *)(u_long)&is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
			       (long)(is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)]), 
d370 9
a378 4
			DPRINTF(IDB_IOMMU, ("iommu_remove: flushed va %p TSB[%lx]@@%p=%lx, %lu bytes left\n", 	       
			       (void *)(u_long)va, (long)IOTSBSLOT(va,is->is_tsbsize), 
			       (void *)(u_long)&is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
			       (long)(is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)]), 
d388 3
a390 3
		is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)] &= ~IOTTE_V;
		bus_space_write_8(is->is_bustag, (bus_space_handle_t)(u_long)
				  &is->is_iommu->iommu_flush, 0, va);
d396 1
a396 2
iommu_tv_comp(t1, t2)
	struct timeval *t1, *t2;
d411 1
a411 2
iommu_strbuf_flush_done(is)
	struct iommu_state *is;
d414 11
d426 1
a426 1
	if (!is->is_sb[0] && !is->is_sb[1])
d441 21
a461 18
	is->is_flush[0] = (is->is_sb[0] == NULL) ? 1 : 0;
	is->is_flush[1] = (is->is_sb[1] == NULL) ? 1 : 0;
	membar_memissue();

	if (is->is_sb[0]) {
		bus_space_write_8(is->is_bustag, (bus_space_handle_t)(u_long)
			&is->is_sb[0]->strbuf_flushsync, 0, is->is_flushpa);
		bus_space_barrier(is->is_bustag, (bus_space_handle_t)(u_long)
		    &is->is_sb[0]->strbuf_flushsync, 0, sizeof(u_int64_t),
		    BUS_SPACE_BARRIER_WRITE);
	}

	if (is->is_sb[1]) {
		bus_space_write_8(is->is_bustag, (bus_space_handle_t)(u_long)
			&is->is_sb[1]->strbuf_flushsync, 0, is->is_flushpa + 8);
		bus_space_barrier(is->is_bustag, (bus_space_handle_t)(u_long)
		    &is->is_sb[1]->strbuf_flushsync, 0, sizeof(u_int64_t),
		    BUS_SPACE_BARRIER_WRITE);
d465 1
a465 6
	flushtimeout.tv_usec = cur.tv_usec + 500000; /* 1/2 sec */
	if (flushtimeout.tv_usec >= 1000000) {
		flushtimeout.tv_usec -= 1000000;
		flushtimeout.tv_sec = cur.tv_sec + 1;
	} else
		flushtimeout.tv_sec = cur.tv_sec;
d467 12
a478 4
	DPRINTF(IDB_IOMMU, ("iommu_strbuf_flush_done: flush = %lx at va = %lx pa = %lx now=%lx:%lx until = %lx:%lx\n", 
		       (long)is->is_flush, (long)&is->is_flush, 
		       (long)is->is_flushpa, cur.tv_sec, cur.tv_usec, 
		       flushtimeout.tv_sec, flushtimeout.tv_usec));
d481 12
a492 3
	while (((ldxa(is->is_flushpa, ASI_PHYS_CACHED) == 0) ||
	        (ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED) == 0)) &&
	       (iommu_tv_comp(&cur, &flushtimeout) <= 0)) {
d494 3
d500 6
a505 6
	if (((is->is_sb[0] != NULL) && (ldxa(is->is_flushpa, ASI_PHYS_CACHED) == 0)) ||
	    ((is->is_sb[1] != NULL) && (ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED) == 0))) {
		printf("iommu_strbuf_flush_done: flush timeout %p,%p at %p\n",
			(void *)(u_long)is->is_flush[0],
			(void *)(u_long)is->is_flush[1],
			(void *)(u_long)is->is_flushpa); /* panic? */
d512 1
a512 1
	return (is->is_flush[0] && is->is_flush[1]);
d519 2
a520 8
iommu_dvmamap_load(t, is, map, buf, buflen, p, flags)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dmamap_t map;
	void *buf;
	bus_size_t buflen;
	struct proc *p;
	int flags;
a567 1
	s = splhigh();
d572 1
d575 1
a575 1
	    EX_NOWAIT|EX_BOUNDZERO, (u_long *)&dvmaddr);
d579 1
a579 2
	if (err || (dvmaddr == (bus_addr_t)-1))	
	{ 
d635 1
a635 1
	map->dm_nsegs = seg+1;
d665 1
a665 1
		    flags|0x4000);
d689 1
a689 4
iommu_dvmamap_unload(t, is, map)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dmamap_t map;
d711 3
a713 1
	
d728 3
a730 8
iommu_dvmamap_load_raw(t, is, map, segs, nsegs, flags, size)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dmamap_t map;
	bus_dma_segment_t *segs;
	int nsegs;
	int flags;
	bus_size_t size;
d769 1
a769 1
	for (i=0; left && i<nsegs; i++) {
d792 1
a792 1
	    EX_NOWAIT|EX_BOUNDZERO, (u_long *)&dvmaddr);
d799 3
a801 4
	if (dvmaddr == (bus_addr_t)-1)	
	{ 
		printf("iommu_dvmamap_load_raw(): extent_alloc(%d, %x) failed!\n",
		    (int)sgsize, flags);
d892 1
a892 1
					map->dm_segs[j].ds_len));
d918 2
a919 1
					     prev_pa = pa, flags|(++npg<<12)); 
d934 1
a934 1
		{
d937 8
a944 4
				if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
				    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
					printf("seg %d dvmaddr %lx out of range %x - %x\n",
					    seg, (long)map->dm_segs[seg].ds_addr,
d996 2
a997 1
		    ("iommu_dvmamap_load_raw: map %p loading va %lx at pa %lx\n",
d999 1
a999 1
		iommu_enter(is, dvmaddr, pa, flags|0x8000);
d1005 1
a1005 1
	map->dm_nsegs = i+1;
d1012 2
a1013 1
				printf("seg %d dvmaddr %lx out of range %x - %x\n",
d1027 2
a1028 7
iommu_dvmamap_sync(t, is, map, offset, len, ops)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dmamap_t map;
	bus_addr_t offset;
	bus_size_t len;
	int ops;
d1033 3
d1047 3
a1049 2
		needsflush += iommu_dvmamap_sync_seg(t, is, &map->dm_segs[i],
		    offset, count, ops);
d1064 1
d1066 39
a1104 7
iommu_dvmamap_sync_seg(t, is, seg, offset, len, ops)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dma_segment_t *seg;
	bus_addr_t offset;
	bus_size_t len;
	int ops;
d1109 4
d1114 1
a1114 8
		goto out;

	len += offset & PGOFSET;

	if (ops & BUS_DMASYNC_PREREAD) {
		DPRINTF(IDB_SYNC,
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
		     "BUS_DMASYNC_PREREAD\n", (void *)(u_long)va, (u_long)len));
d1116 1
d1120 3
a1122 47
	if (ops & BUS_DMASYNC_POSTREAD) {
		DPRINTF(IDB_SYNC,
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
		     "BUS_DMASYNC_POSTREAD\n", (void *)(u_long)va, (u_long)len));
		/* if we have a streaming buffer, flush it here first */
		if (is->is_sb[0] || is->is_sb[1])
			while (len > 0) {
				DPRINTF(IDB_BUSDMA,
				    ("iommu_dvmamap_sync_seg: flushing va %p, %lu "
				     "bytes left\n", (void *)(u_long)va, (u_long)len));
				if (iommu_tsb_entry(is, va) & IOTTE_STREAM) {
					iommu_strbuf_flush(is, va);
					needsflush = 1;
				}
				if (len <= NBPG)
					len = 0;
				else
					len -= NBPG;
				va += NBPG;
			}
	}
	if (ops & BUS_DMASYNC_PREWRITE) {
		DPRINTF(IDB_SYNC,
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
		     "BUS_DMASYNC_PREWRITE\n", (void *)(u_long)va, (u_long)len));
		/* if we have a streaming buffer, flush it here first */
		if (is->is_sb[0] || is->is_sb[1])
			while (len > 0) {
				DPRINTF(IDB_BUSDMA,
				    ("iommu_dvmamap_sync_seg: flushing va %p, %lu "
				     "bytes left\n", (void *)(u_long)va, (u_long)len));
				if (iommu_tsb_entry(is, va) & IOTTE_STREAM) {
					iommu_strbuf_flush(is, va);
					needsflush = 1;
				}
				if (len <= NBPG)
					len = 0;
				else
					len -= NBPG;
				va += NBPG;
			}
	}
	if (ops & BUS_DMASYNC_POSTWRITE) {
		DPRINTF(IDB_SYNC,
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
		     "BUS_DMASYNC_POSTWRITE\n", (void *)(u_long)va, (u_long)len));
		/* Nothing to do */;
a1124 1
out:
d1129 3
a1131 8
iommu_dvmamem_alloc(t, is, size, alignment, boundary, segs, nsegs, rsegs, flags)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
d1134 4
a1137 4
	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_alloc: sz %llx align %llx bound %llx "
	   "segp %p flags %d\n", (unsigned long long)size,
	   (unsigned long long)alignment, (unsigned long long)boundary,
	   segs, flags));
d1139 1
a1139 1
	    segs, nsegs, rsegs, flags|BUS_DMA_DVMA));
d1143 2
a1144 5
iommu_dvmamem_free(t, is, segs, nsegs)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dma_segment_t *segs;
	int nsegs;
d1157 2
a1158 8
iommu_dvmamem_map(t, is, segs, nsegs, size, kvap, flags)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dma_segment_t *segs;
	int nsegs;
	size_t size;
	caddr_t *kvap;
	int flags;
d1164 1
a1164 1
	int cbit;
a1182 1
	cbit = 0;
d1199 2
a1200 1
		    "mapping va %lx at %llx\n", va, (unsigned long long)addr | cbit));
d1216 2
a1217 5
iommu_dvmamem_unmap(t, is, kva, size)
	bus_dma_tag_t t;
	struct iommu_state *is;
	caddr_t kva;
	size_t size;
d1233 1
@


1.22
log
@this removes the functionality of adding allocated
pages into the queue already containing allocated pages.
breaks i386:setup_buffers() because of this.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.21 2002/10/06 22:06:15 art Exp $	*/
d313 1
a313 1
		panic("iommu_tsb_entry: invalid entry %llx\n", (long long)dva);
@


1.21
log
@No more need to initialize the result list before uvm_pglistalloc.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.20 2002/10/04 01:55:44 jason Exp $	*/
d141 1
@


1.20
log
@Aasking to load an already loaded map is not a "warn and pray" kinda thing.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.19 2002/08/19 20:02:30 jason Exp $	*/
a140 1
	TAILQ_INIT(&mlist);
@


1.19
log
@From NetBSD:
-Fix some corner cases in bus_dmamap_load_mbuf().
From Takeshi Nakayama <tn@@catvmics.ne.jp>
-Fix off-by-one error in iommu_dvmamap_load_raw() where if a DMA segment
has just one byte on a page the page is never mapped into the IOMMU.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.18 2002/07/24 19:07:43 jason Exp $	*/
d496 1
a496 1
		printf("iommu_dvmamap_load: map still in use\n");
d715 1
a715 1
		printf("iommu_dvmamap_load_raw: map still in use\n");
@


1.18
log
@No need to cast &thing to paddr_t* when thing IS a paddr_t
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.17 2002/05/13 19:43:48 jason Exp $	*/
d577 1
a577 1
		if (++seg > map->_dm_segcnt) {
d839 4
d857 1
a857 1
					sgstart & (boundary - 1);
d862 1
a862 1
				if (++j > map->_dm_segcnt) {
d875 1
a875 1
			while (dvmaddr < sgend) {
d930 1
a930 1
		map->dm_segs[i].ds_len = sgstart & (boundary - 1);
d935 1
a935 1
		if (++i > map->_dm_segcnt) {
@


1.17
log
@Deal with 24bit dvma requests
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.16 2002/03/26 18:13:11 jason Exp $	*/
d186 1
a186 1
		    (paddr_t *)&is->is_flushpa);
d401 1
a401 1
				
@


1.16
log
@Permit compiling with DEBUG; Joey Coleman <joeycoleman@@acm.org>
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.15 2002/03/14 01:26:44 millert Exp $	*/
d521 8
d534 3
a536 3
	err = extent_alloc(is->is_dvmamap, sgsize, align, 0,
		(sgsize > boundary) ? 0 : boundary, 
		EX_NOWAIT|EX_BOUNDZERO, (u_long *)&dvmaddr);
d746 7
d758 3
a760 4
	err = extent_alloc(is->is_dvmamap, sgsize, align, 0,
		(sgsize > boundary) ? 0 : boundary,
		((flags & BUS_DMA_NOWAIT) == 0 ? EX_WAITOK : EX_NOWAIT) |
		EX_BOUNDZERO, (u_long *)&dvmaddr);
@


1.15
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.14 2002/03/12 19:41:05 jason Exp $	*/
d801 1
a801 1
    dvmaddr, prev_va, pa prev_pa);
d815 1
a815 1
    offset, pa, prev_pa,l dvmaddr, prev_va);
@


1.14
log
@Look up the TTE and only do the streaming cache dance if the mapping is marked as streaming.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.13 2002/03/07 17:58:24 jason Exp $	*/
d85 2
a86 2
static	int iommu_strbuf_flush_done __P((struct iommu_state *));
int64_t iommu_tsb_entry __P((struct iommu_state *, vaddr_t));
@


1.13
log
@Remove some debugging code accidentally committed in previous
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.12 2002/03/07 17:44:10 jason Exp $	*/
d86 1
d298 21
d1005 4
d1044 2
a1045 2
				iommu_strbuf_flush(is, va);
				if (len <= NBPG) {
d1047 2
d1050 1
a1050 1
				} else
d1065 2
a1066 2
				iommu_strbuf_flush(is, va);
				if (len <= NBPG) {
d1068 2
d1071 1
a1071 1
				} else
@


1.12
log
@From NetBSD:
Fix calculation of dma segment length when the DVMA range is crossing the boundary.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.11 2002/02/22 21:53:21 jason Exp $	*/
a956 1
	bus_addr_t orig_offset = offset;
d966 1
a966 9
	if (i == map->dm_nsegs) {
		printf("map length: 0x%llx\n", (unsigned long long)map->dm_mapsize);
		printf("looking for 0x%llx/0x%llx\n",
		    (unsigned long long)orig_offset, (unsigned long long)len);
		for (i = 0; i < map->dm_nsegs; i++) {
			printf("%d: addr 0x%llx len 0x%llx\n", i,
			    (unsigned long long)map->dm_segs[i].ds_addr,
			    (unsigned long long)map->dm_segs[i].ds_len);
		}
a967 1
	}
@


1.11
log
@Be sure to wait the whole 0.5 seconds (half a second!) for the streaming
cache to flush, not some fraction of it, before giving up.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.10 2002/02/22 20:21:46 deraadt Exp $	*/
d545 2
a546 1
		map->dm_segs[seg].ds_len = sgstart & (boundary - 1);
d957 1
d967 9
a975 1
	if (i == map->dm_nsegs)
d977 1
@


1.10
log
@Debugger() is #ifdef DDB
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.9 2002/02/22 19:35:02 jason Exp $	*/
d86 1
d355 16
a376 11
#define BUMPTIME(t, usec) { \
	register volatile struct timeval *tp = (t); \
	register long us; \
 \
	tp->tv_usec = us = tp->tv_usec + (usec); \
	if (us >= 1000000) { \
		tp->tv_usec = us - 1000000; \
		tp->tv_sec++; \
	} \
}

d412 7
a418 3
	microtime(&flushtimeout); 
	cur = flushtimeout;
	BUMPTIME(&flushtimeout, 500000); /* 1/2 sec */
d428 1
a428 1
	       ((cur.tv_sec <= flushtimeout.tv_sec) && (cur.tv_usec <= flushtimeout.tv_usec))) {
@


1.9
log
@- don't call strbuf_flush_done for each segment, call it once per map (and
then only if necessary)
- When flushing a partial segment make sure we flush all of the necessary pages
(the case where len < NBPG would not correctly flush both pages if
va + off <-> va + off + len crossed a page boundary)
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.8 2002/02/22 16:11:59 jason Exp $	*/
d599 1
d601 1
d859 1
d861 1
d927 1
d929 1
@


1.8
log
@From NetBSD:
- Fix pr sparc64/15633: datafault at tlp_start causes panic
- Also clean up some additional 32-bit kernel printf issues.
- Handle the case where a DMA operation wraps from the end of a page to the
beginning of the same page properly by double-mapping that page.
- Move a brace so this will compile w/o DEBUG.
- Overhaul iommu_dvmamap_load_raw().
From Me:
- Verify the streaming cache exists before waiting on it to clear during flush.
- Force sync of flush status (hardware sets it to 1 when its done)
- Force sync flush of registers (bus_space_barrier)
- Flush ALL segments on BUS_DMA_POSTREAD operations (not just the first!)
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.7 2001/12/04 23:22:42 art Exp $	*/
d69 1
a69 1
void iommu_dvmamap_sync_seg(bus_dma_tag_t, struct iommu_state *,
a307 1

d941 1
a941 1
	int i;
d954 1
a954 1
		iommu_dvmamap_sync_seg(t, is, &map->dm_segs[i],
d961 3
d966 1
a966 1
void
d975 1
d978 4
a981 4
	/*
	 * We only support one DMA segment; supporting more makes this code
         * too unweildy.
	 */
d1003 1
a1003 1
					iommu_strbuf_flush_done(is);
d1022 1
a1022 1
					iommu_strbuf_flush_done(is);
d1035 3
@


1.7
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: iommu.c,v 1.6 2001/11/06 19:53:16 miod Exp $	*/
/*	$NetBSD: iommu.c,v 1.42 2001/10/07 20:30:40 eeh Exp $	*/
d5 1
a32 81
/*-
 * Copyright (c) 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Paul Kranenburg.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *        This product includes software developed by the NetBSD
 *        Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This software was developed by the Computer Systems Engineering group
 * at Lawrence Berkeley Laboratory under DARPA contract BG 91-66 and
 * contributed to Berkeley.
 *
 * All advertising materials mentioning features or use of this software
 * must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Lawrence Berkeley Laboratory.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	from: NetBSD: sbus.c,v 1.13 1999/05/23 07:24:02 mrg Exp
 *	from: @@(#)sbus.c	8.1 (Berkeley) 6/11/93
 */

d69 3
d110 8
a117 2
	 * The sun4u iommu is part of the SBUS or PCI controller so we
	 * will deal with it here..
d119 3
a121 4
	 * The IOMMU address space always ends at 0xffffe000, but the starting
	 * address depends on the size of the map.  The map size is 1024 * 2 ^
	 * is->is_tsbsize entries, where each entry is 8 bytes.  The start of
	 * the map can be calculated by (0xffffe000 << (8 + is->is_tsbsize)).
d125 7
a131 2
	is->is_dvmabase = iovabase;
	if (iovabase == -1) is->is_dvmabase = IOTSB_VSTART(is->is_tsbsize);
d194 3
a196 3
	printf("DVMA map: %x to %x\n", 
		(unsigned int)is->is_dvmabase,
		(unsigned int)(is->is_dvmabase+(size<<10)));
d198 2
a199 2
				       is->is_dvmabase, (u_long)IOTSB_VEND,
				       M_DEVBUF, 0, 0, EX_NOWAIT);
d211 2
d223 13
a235 26
	if (is->is_sb[0]) {

		/* Enable diagnostics mode? */
		bus_space_write_8(is->is_bustag,
			(bus_space_handle_t)(u_long)&is->is_sb[0]->strbuf_ctl,
			0, STRBUF_EN);

		/* No streaming buffers? Disable them */
		if (bus_space_read_8(is->is_bustag,
			(bus_space_handle_t)(u_long)&is->is_sb[0]->strbuf_ctl,
			0) == 0)
		is->is_sb[0] = 0;
	}

	if (is->is_sb[1]) {

		/* Enable diagnostics mode? */
		bus_space_write_8(is->is_bustag,
			(bus_space_handle_t)(u_long)&is->is_sb[1]->strbuf_ctl,
			0, STRBUF_EN);

		/* No streaming buffers? Disable them */
		if (bus_space_read_8(is->is_bustag,
			(bus_space_handle_t)(u_long)&is->is_sb[1]->strbuf_ctl,
			0) == 0)
		is->is_sb[1] = 0;
d252 1
a252 1
	if (va < is->is_dvmabase)
d258 2
a259 1
	
d287 1
a287 1
	if (dva >= is->is_dvmabase)
d310 1
a310 1
	if (va < is->is_dvmabase)
d347 2
a348 1
		is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)] = 0;
d387 3
a390 2
	is->is_flush[0] = 1;
	is->is_flush[1] = 1;
a391 1
		is->is_flush[0] = 0;
d394 3
d398 1
a399 1
		is->is_flush[0] = 1;
d402 3
d415 1
d417 3
a419 4
	while ((!ldxa(is->is_flushpa, ASI_PHYS_CACHED) ||
		!ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED)) &&
		((cur.tv_sec <= flushtimeout.tv_sec) &&
			(cur.tv_usec <= flushtimeout.tv_usec)))
d421 1
d424 2
a425 2
	if ((!ldxa(is->is_flushpa, ASI_PHYS_CACHED) ||
		!ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED))) {
d534 2
a535 1
			map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len));
d556 1
a556 1
		map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len));
d587 1
a587 1
		    flags);
d593 11
d668 1
d685 1
a685 1
	align = max(segs[0]._ds_align, NBPG);
d707 1
a707 1
	 * split the transfer up int little pieces ourselves.
d739 3
d743 2
a744 2
		 * This segs is made up of individual physical pages,
		 * probably by _bus_dmamap_load_uio() or 
d746 1
a746 1
		 * load each segment individually.
d750 23
a772 4
		i = j = 0;
		pa = segs[i].ds_addr;
		dvmaddr += (pa & PGOFSET);
		left = min(size, segs[i].ds_len);
d774 10
a783 49
		sgstart = dvmaddr;
		sgend = sgstart + left - 1;

		map->dm_segs[j].ds_addr = dvmaddr;
		map->dm_segs[j].ds_len = left;

		/* Set the size (which we will be destroying */
		map->dm_mapsize = size;

		while (size > 0) {
			int incr;

			if (left <= 0) {
				u_long offset;

				/*
				 * If the two segs are on different physical
				 * pages move to a new virtual page.
				 */
				if (trunc_page(pa) != 
					trunc_page(segs[++i].ds_addr))
					dvmaddr += NBPG;

				pa = segs[i].ds_addr;
				left = min(size, segs[i].ds_len);

				offset = (pa & PGOFSET);
				if (dvmaddr == trunc_page(dvmaddr) + offset) {
					/* We can combine segments */
					map->dm_segs[j].ds_len += left;
					sgend += left;
				} else {
					/* Need a new segment */
					dvmaddr = trunc_page(dvmaddr) + offset;
					DPRINTF(IDB_INFO,
						("iommu_dvmamap_load_raw: "
							"seg %d start %lx "
							"size %lx\n", j,
							map->dm_segs[j].ds_addr,
							map->dm_segs[j].
							ds_len));
					if (++j > map->_dm_segcnt)
						goto fail;
					map->dm_segs[j].ds_addr = dvmaddr;
					map->dm_segs[j].ds_len = left;

					sgstart = dvmaddr;
					sgend = sgstart + left - 1;
				}
d785 12
d798 1
d808 1
a808 1
					map->dm_segs[j].ds_addr, 
a810 1
fail:
d822 39
a860 12
			DPRINTF(IDB_BUSDMA,
				("iommu_dvmamap_load_raw: map %p loading va %lx at pa %lx\n",
					map, (long)dvmaddr, (long)(pa)));
			/* Enter it if we haven't before. */
			if (prev_va != trunc_page(dvmaddr))
				iommu_enter(is, prev_va = trunc_page(dvmaddr),
					trunc_page(pa), flags);
			incr = min(pagesz, left);
			dvmaddr += incr;
			pa += incr;
			left -= incr;
			size -= incr;
d862 1
a862 4
		DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
			"seg %d start %lx size %lx\n", j,
			map->dm_segs[j].ds_addr, map->dm_segs[j].ds_len));
		map->dm_nsegs = j+1;
d879 1
a879 1
			map->dm_segs[i].ds_addr,
d897 1
a897 1
			map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len));
d908 1
a908 1
		iommu_enter(is, dvmaddr, pa, flags);
d915 14
d941 33
a973 1
	vaddr_t va = map->dm_segs[0].ds_addr + offset;
d982 1
a982 1
		    ("iommu_dvmamap_sync: syncing va %p len %lu "
d987 1
d990 1
a990 1
		    ("iommu_dvmamap_sync: syncing va %p len %lu "
d996 1
a996 1
				    ("iommu_dvmamap_sync: flushing va %p, %lu "
d1009 1
a1009 1
		    ("iommu_dvmamap_sync: syncing va %p len %lu "
d1015 1
a1015 1
				    ("iommu_dvmamap_sync: flushing va %p, %lu "
d1028 1
a1028 1
		    ("iommu_dvmamap_sync: syncing va %p len %lu "
a1153 5
#if 0
	/*
	 * XXX ? is this necessary? i think so and i think other
	 * implementations are missing it.
	 */
a1154 1
#endif
@


1.7.2.1
log
@Sync UBC branch to -current
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: iommu.c,v 1.47 2002/02/08 20:03:45 eeh Exp $	*/
a4 1
 * Copyright (c) 2001, 2002 Eduardo Horvath
d32 81
a148 3
int iommu_dvmamap_sync_seg(bus_dma_tag_t, struct iommu_state *,
    bus_dma_segment_t *, bus_addr_t, bus_size_t, int);

d162 1
a162 3
static	int iommu_strbuf_flush_done(struct iommu_state *);
int64_t iommu_tsb_entry(struct iommu_state *, vaddr_t);
static	int iommu_tv_comp(struct timeval *, struct timeval *);
d187 2
a188 2
	 * The sun4u iommu is part of the SBUS or PCI controller so we will
	 * deal with it here..
d190 4
a193 9
	 * For sysio and psycho/psycho+ the IOMMU address space always ends at
	 * 0xffffe000, but the starting address depends on the size of the
	 * map.  The map size is 1024 * 2 ^ is->is_tsbsize entries, where each
	 * entry is 8 bytes.  The start of the map can be calculated by
	 * (0xffffe000 << (8 + is->is_tsbsize)).
	 *
	 * But sabre and hummingbird use a different scheme that seems to
	 * be hard-wired, so we read the start and size from the PROM and
	 * just use those values.
d197 2
a198 7
	if (iovabase == -1) {
		is->is_dvmabase = IOTSB_VSTART(is->is_tsbsize);
		is->is_dvmaend = IOTSB_VEND;
	} else {
		is->is_dvmabase = iovabase;
		is->is_dvmaend = iovabase + IOTSB_VSIZE(tsbsize);
	}
d261 3
a263 3
	printf("IOTDB: %llx to %llx\n", 
	    (unsigned long long)is->is_ptsb,
	    (unsigned long long)(is->is_ptsb + size));
d265 2
a266 2
	    is->is_dvmabase, is->is_dvmaend - NBPG,
	    M_DEVBUF, 0, 0, EX_NOWAIT);
a277 2
	struct iommu_strbuf *sb;
	int i;
d288 26
a313 13
	for (i=0; i<2; i++) {
		if ((sb = is->is_sb[i]) != NULL) {
			/* Enable diagnostics mode? */
			bus_space_write_8(is->is_bustag,
			    (bus_space_handle_t)(u_long)&sb->strbuf_ctl,
			    0, STRBUF_EN);

			/* No streaming buffers? Disable them */
			if (bus_space_read_8(is->is_bustag,
			    (bus_space_handle_t)(u_long)&sb->strbuf_ctl,
				0) == 0)
				is->is_sb[i] = 0;
		}
d330 1
a330 1
	if (va < is->is_dvmabase || va > is->is_dvmaend)
d336 1
a336 2
tte |= (flags & 0xff000LL)<<(4*8);/* DEBUG */

d364 1
a364 1
	if (dva >= is->is_dvmabase && dva < is->is_dvmaend)
a372 21
 * Fetch a tsb entry with some sanity checking.
 */
int64_t
iommu_tsb_entry(is, dva)
	struct iommu_state *is;
	vaddr_t dva;
{
	int64_t tte;

	if (dva < is->is_dvmabase && dva >= is->is_dvmaend)
		panic("invalid dva: %llx", (long long)dva);

	tte = is->is_tsb[IOTSBSLOT(dva,is->is_tsbsize)];

	if ((tte & IOTTE_V) == 0)
		panic("iommu_tsb_entry: invalid entry %llx\n", (long long)dva);

	return (tte);
}

/*
d385 1
d387 1
a387 1
	if (va < is->is_dvmabase || va > is->is_dvmaend)
d424 1
a424 2
		/* XXX Zero-ing the entry would not require RMW */
		is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)] &= ~IOTTE_V;
a430 16
static int
iommu_tv_comp(t1, t2)
	struct timeval *t1, *t2;
{
	if (t1->tv_sec < t2->tv_sec)
		return (-1);
	if (t1->tv_sec > t2->tv_sec)
		return (1);
	/* t1->tv_sec == t2->tv_sec */
	if (t1->tv_usec < t2->tv_usec)
		return (-1);
	if (t1->tv_usec > t2->tv_usec)
		return (1);
	return (0);
}

d437 11
a462 3
	is->is_flush[0] = (is->is_sb[0] == NULL) ? 1 : 0;
	is->is_flush[1] = (is->is_sb[1] == NULL) ? 1 : 0;
	membar_memissue();
d464 2
d467 1
a469 3
		bus_space_barrier(is->is_bustag, (bus_space_handle_t)(u_long)
		    &is->is_sb[0]->strbuf_flushsync, 0, sizeof(u_int64_t),
		    BUS_SPACE_BARRIER_WRITE);
a470 1

d472 1
a474 3
		bus_space_barrier(is->is_bustag, (bus_space_handle_t)(u_long)
		    &is->is_sb[1]->strbuf_flushsync, 0, sizeof(u_int64_t),
		    BUS_SPACE_BARRIER_WRITE);
d477 3
a479 7
	microtime(&cur);
	flushtimeout.tv_usec = cur.tv_usec + 500000; /* 1/2 sec */
	if (flushtimeout.tv_usec >= 1000000) {
		flushtimeout.tv_usec -= 1000000;
		flushtimeout.tv_sec = cur.tv_sec + 1;
	} else
		flushtimeout.tv_sec = cur.tv_sec;
a484 1

d486 4
a489 3
	while (((ldxa(is->is_flushpa, ASI_PHYS_CACHED) == 0) ||
	        (ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED) == 0)) &&
	       (iommu_tv_comp(&cur, &flushtimeout) <= 0)) {
a490 1
	}
d493 2
a494 2
	if (((is->is_sb[0] != NULL) && (ldxa(is->is_flushpa, ASI_PHYS_CACHED) == 0)) ||
	    ((is->is_sb[1] != NULL) && (ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED) == 0))) {
a558 8

	if (flags & BUS_DMA_24BIT) {
		sgstart = max(is->is_dvmamap->ex_start, 0xff000000);
		sgend = min(is->is_dvmamap->ex_end, 0xffffffff);
	} else {
		sgstart = is->is_dvmamap->ex_start;
		sgend = is->is_dvmamap->ex_end;
	}
d564 3
a566 3
	err = extent_alloc_subregion(is->is_dvmamap, sgstart, sgend,
	    sgsize, align, 0, (sgsize > boundary) ? 0 : boundary, 
	    EX_NOWAIT|EX_BOUNDZERO, (u_long *)&dvmaddr);
d603 2
a604 4
			(long)map->dm_segs[seg].ds_addr, 
			map->dm_segs[seg].ds_len));
		map->dm_segs[seg].ds_len =
		    boundary - (sgstart & (boundary - 1));
d624 1
a624 1
		(long)map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len));
d655 1
a655 1
		    flags|0x4000);
a660 13
#ifdef DIAGNOSTIC
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
		    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
			printf("seg %d dvmaddr %lx out of range %x - %x\n",
			    seg, (long)map->dm_segs[seg].ds_addr,
			    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
			Debugger();
#endif
		}
	}
#endif
a724 1
	int npg = 0; /* DEBUG */
d741 1
a741 1
	align = max(segs[0]._ds_align, pagesz);
a759 7
	if (flags & BUS_DMA_24BIT) {
		sgstart = max(is->is_dvmamap->ex_start, 0xff000000);
		sgend = min(is->is_dvmamap->ex_end, 0xffffffff);
	} else {
		sgstart = is->is_dvmamap->ex_start;
		sgend = is->is_dvmamap->ex_end;
	}
d763 1
a763 1
	 * split the transfer up into little pieces ourselves.
d765 4
a768 3
	err = extent_alloc_subregion(is->is_dvmamap, sgstart, sgend,
	    sgsize, align, 0, (sgsize > boundary) ? 0 : boundary, 
	    EX_NOWAIT|EX_BOUNDZERO, (u_long *)&dvmaddr);
a794 3
		paddr_t prev_pa = 0;
		int end = 0, offset;

d796 2
a797 2
		 * This segs is made up of individual physical
		 * segments, probably by _bus_dmamap_load_uio() or
d799 1
a799 1
		 * load each one individually.
d803 50
a852 23
		j = 0;
		for (i = 0; i < nsegs; i++) {
			pa = segs[i].ds_addr;
			offset = (pa & PGOFSET);
			pa = trunc_page(pa);
			dvmaddr = trunc_page(dvmaddr);
			left = min(size, segs[i].ds_len);

			DPRINTF(IDB_INFO, ("iommu_dvamap_load_raw: converting "
			    "physseg %d start %lx size %lx\n", i,
			    (long)segs[i].ds_addr, segs[i].ds_len));

			if ((pa == prev_pa) &&
			    ((offset != 0) || (end != offset))) {
				/* We can re-use this mapping */
#ifdef DEBUG
if (iommudebug & 0x10) printf("reusing dva %lx prev %lx pa %lx prev %lx\n",
    dvmaddr, prev_va, pa, prev_pa);
#endif
				dvmaddr = prev_va; 
			}
			sgstart = dvmaddr + offset;
			sgend = sgstart + left - 1;
d854 3
a856 10
			/* Are the segments virtually adjacent? */
			if ((j > 0) && (end == offset) &&
			    ((offset = 0) || (pa == prev_pa))) {
				/* Just append to the previous segment. */
#ifdef DEBUG
if (iommudebug & 0x10) {
printf("appending offset %x pa %lx, prev %lx dva %lx prev %lx\n",
    offset, pa, prev_pa, dvmaddr, prev_va);
}
#endif
a857 12
				map->dm_segs[--j].ds_len += left; 
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
				    "appending seg %d start %lx size %lx\n", j,
				    (long)map->dm_segs[j].ds_addr,
				    map->dm_segs[j].ds_len));
			} else {
				map->dm_segs[j].ds_addr = sgstart;
				map->dm_segs[j].ds_len = left;
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
				    "seg %d start %lx size %lx\n", j,
				    (long)map->dm_segs[j].ds_addr,
				    map->dm_segs[j].ds_len));
a858 1
			end = (offset + left) & PGOFSET;
d868 1
a868 1
					(long)map->dm_segs[j].ds_addr, 
d871 1
d883 12
a894 24
			/* Now map a series of pages. */
			while (dvmaddr < sgend) {
				DPRINTF(IDB_BUSDMA,
				    ("iommu_dvamap_load_raw: map %p "
				    "loading va %lx at pa %lx\n",
				    map, (long)dvmaddr,
				    (long)(pa)));
				/* Enter if if we haven't before. */
				if (prev_va != dvmaddr)
#ifdef DEBUG
{ if (iommudebug & 0x10) printf("seg %d:5d entering dvma %lx, prev %lx pa %lx\n", i, j, dvmaddr, prev_va, pa);
#endif
					 iommu_enter(is, prev_va = dvmaddr,
					     prev_pa = pa, flags|(++npg<<12)); 
#ifdef DEBUG
} else if (iommudebug & 0x10) printf("seg %d:%d skipping dvma %lx, prev %lx\n", i, j, dvmaddr, prev_va);
#endif

				dvmaddr += pagesz;
				pa += pagesz;
			}

			size -= left;
			++j;
d896 4
a899 18

		map->dm_nsegs = j;
#ifdef DIAGNOSTIC
		{
			int seg;
			for (seg = 0; seg < map->dm_nsegs; seg++) {
				if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
				    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
					printf("seg %d dvmaddr %lx out of range %x - %x\n",
					    seg, (long)map->dm_segs[seg].ds_addr,
					    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
					Debugger();
#endif
				}
			}
		}
#endif
d916 1
a916 1
			(long)map->dm_segs[i].ds_addr,
d934 1
a934 1
			(long)map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len));
d945 1
a945 1
		iommu_enter(is, dvmaddr, pa, flags|0x8000);
a951 16
#ifdef DIAGNOSTIC
	{
		int seg;
		for (seg = 0; seg < map->dm_nsegs; seg++) {
			if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
			    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
				printf("seg %d dvmaddr %lx out of range %x - %x\n",
				    seg, (long)map->dm_segs[seg].ds_addr,
				    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
				Debugger();
#endif
			}
		}
       }
#endif
d964 1
a964 2
	bus_size_t count;
	int i, needsflush = 0;
d966 4
a969 43
	for (i = 0; i < map->dm_nsegs; i++) {
		if (offset < map->dm_segs[i].ds_len)
			break;
		offset -= map->dm_segs[i].ds_len;
	}

	if (i == map->dm_nsegs)
		panic("iommu_dvmamap_sync: too short %lu", offset);

	for (; len > 0 && i < map->dm_nsegs; i++) {
		count = min(map->dm_segs[i].ds_len - offset, len);
		needsflush += iommu_dvmamap_sync_seg(t, is, &map->dm_segs[i],
		    offset, count, ops);
		len -= count;
	}

	if (i == map->dm_nsegs && len > 0)
		panic("iommu_dvmamap_sync: leftover %lu", len);

	if (needsflush)
		iommu_strbuf_flush_done(is);
}

/*
 * Flush an individual dma segment, returns non-zero if the streaming buffers
 * need flushing afterwards.
 */
int
iommu_dvmamap_sync_seg(t, is, seg, offset, len, ops)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dma_segment_t *seg;
	bus_addr_t offset;
	bus_size_t len;
	int ops;
{
	int needsflush = 0;
	vaddr_t va = seg->ds_addr + offset;

	if (len == 0)
		goto out;

	len += offset & PGOFSET;
d973 1
a973 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
a977 1

d980 1
a980 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
d986 1
a986 1
				    ("iommu_dvmamap_sync_seg: flushing va %p, %lu "
d988 3
a990 5
				if (iommu_tsb_entry(is, va) & IOTTE_STREAM) {
					iommu_strbuf_flush(is, va);
					needsflush = 1;
				}
				if (len <= NBPG)
d992 1
a992 1
				else
d999 1
a999 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
d1005 1
a1005 1
				    ("iommu_dvmamap_sync_seg: flushing va %p, %lu "
d1007 3
a1009 5
				if (iommu_tsb_entry(is, va) & IOTTE_STREAM) {
					iommu_strbuf_flush(is, va);
					needsflush = 1;
				}
				if (len <= NBPG)
d1011 1
a1011 1
				else
d1018 1
a1018 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
a1021 3

out:
	return (needsflush);
d1144 5
d1150 1
@


1.7.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.7.2.1 2002/06/11 03:38:42 art Exp $	*/
d186 1
a186 1
		    &is->is_flushpa);
d313 1
a313 1
		panic("iommu_tsb_entry: invalid entry %llx", (long long)dva);
d401 1
a401 1

d496 1
a496 1
		panic("iommu_dvmamap_load: map still in use");
d577 1
a577 1
		if (++seg >= map->_dm_segcnt) {
d715 1
a715 1
		panic("iommu_dvmamap_load_raw: map still in use");
a838 4
				if (j >= map->_dm_segcnt) {
					iommu_dvmamap_unload(t, is, map);
					return (E2BIG);
				}
d853 1
a853 1
				    boundary - (sgstart & (boundary - 1));
d858 1
a858 1
				if (++j >= map->_dm_segcnt) {
d871 1
a871 1
			while (dvmaddr <= sgend) {
d926 1
a926 1
		map->dm_segs[i].ds_len = boundary - (sgstart & (boundary - 1));
d931 1
a931 1
		if (++i >= map->_dm_segcnt) {
@


1.7.2.3
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a4 1
 * Copyright (c) 2003 Henric Jungheim
a40 1
#include <sys/mbuf.h>
d62 1
a62 4
#define IDB_SYNC	0x8
#define IDB_XXX		0x10
#define IDB_PRINT_MAP	0x20
#define IDB_BREAK	0x40
d69 2
a70 25
void iommu_enter(struct iommu_state *, struct strbuf_ctl *, vaddr_t, paddr_t,
    int);
void iommu_remove(struct iommu_state *, struct strbuf_ctl *, vaddr_t);
int iommu_dvmamap_sync_range(struct strbuf_ctl*, vaddr_t, bus_size_t);
int iommu_strbuf_flush_done(struct iommu_map_state *);
int iommu_dvmamap_load_seg(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t, bus_dma_segment_t *, int, int, bus_size_t, bus_size_t);
int iommu_dvmamap_load_mlist(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t, struct pglist *, int, bus_size_t, bus_size_t);
int iommu_dvmamap_validate_map(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t);
void iommu_dvmamap_print_map(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t);
int iommu_dvmamap_append_range(bus_dma_tag_t, bus_dmamap_t, paddr_t,
    bus_size_t, int, bus_size_t);
int64_t iommu_tsb_entry(struct iommu_state *, vaddr_t);
void strbuf_reset(struct strbuf_ctl *);
int iommu_iomap_insert_page(struct iommu_map_state *, paddr_t);
vaddr_t iommu_iomap_translate(struct iommu_map_state *, paddr_t);
int iommu_iomap_load_map(struct iommu_state *, struct iommu_map_state *,
    vaddr_t, int);
int iommu_iomap_unload_map(struct iommu_state *, struct iommu_map_state *);
struct iommu_map_state *iommu_iomap_create(int);
void iommu_iomap_destroy(struct iommu_map_state *);
void iommu_iomap_clear_pages(struct iommu_map_state *);
d72 12
a83 12
/*
 * Initiate an STC entry flush.
 */
static inline void
iommu_strbuf_flush(struct strbuf_ctl *sb, vaddr_t va)
{
#ifdef DEBUG
	if (sb->sb_flush == NULL) {
		printf("iommu_strbuf_flush: attempting to flush w/o STC\n");
		return;
	}
#endif
d85 3
a87 3
	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_pgflush), va);
}
d97 5
a101 1
iommu_init(char *name, struct iommu_state *is, int tsbsize, u_int32_t iovabase)
d140 1
a140 1
	size = PAGE_SIZE << is->is_tsbsize;
d143 1
a143 1
		(paddr_t)PAGE_SIZE, (paddr_t)0, &mlist, 1, 0) != 0)
d160 1
a160 1
		va += PAGE_SIZE;
d163 1
a163 1
	memset(is->is_tsb, 0, size);
d166 2
a167 1
	if (iommudebug & IDB_INFO) {
d169 2
a170 1
		/* The address or contents of the regs...? */
d172 5
a176 11
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_cr),
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_tsb),
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_flush));
		printf("iommu cr=%llx tsb=%llx\n",
		    IOMMUREG_READ(is, iommu_cr),
		    IOMMUREG_READ(is, iommu_tsb));
		printf("TSB base %p phys %llx\n",
		    (void *)is->is_tsb, (unsigned long long)is->is_ptsb);
d182 7
a195 1
	printf("DVMA map: %x to %x\n", is->is_dvmabase, is->is_dvmaend);
d200 1
a200 1
	    is->is_dvmabase, is->is_dvmaend - PAGE_SIZE,
d205 1
a205 1
 * Streaming buffers don't exist on the UltraSPARC IIi/e; we should have
d210 2
a211 1
iommu_reset(struct iommu_state *is)
d213 1
d216 21
a236 19
	IOMMUREG_WRITE(is, iommu_tsb, is->is_ptsb);

	/* Enable IOMMU */
	IOMMUREG_WRITE(is, iommu_cr, is->is_cr);

	for (i = 0; i < 2; ++i) {
		struct strbuf_ctl *sb = is->is_sb[i];

		if (sb == NULL)
			continue;

		sb->sb_iommu = is;
		strbuf_reset(sb);


		if (sb->sb_flush) {
			char buf[64];
			bus_space_render_tag(sb->sb_bustag, buf, sizeof buf);
			printf("STC%d on %s enabled\n", i, buf);
d242 1
a242 1
 * Inititalize one STC.
d245 5
a249 33
strbuf_reset(struct strbuf_ctl *sb)
{
	if(sb->sb_flush == NULL)
		return;

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_ctl), STRBUF_EN);

	membar(Lookaside);

	/* No streaming buffers? Disable them */
	if (bus_space_read_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_ctl)) == 0) {
		sb->sb_flush = NULL;
	} else {
		/*
		 * locate the pa of the flush buffer
		 */
		if (pmap_extract(pmap_kernel(),
		    (vaddr_t)sb->sb_flush, &sb->sb_flushpa) == FALSE)
			sb->sb_flush = NULL;
	}
}

/*
 * Add an entry to the IOMMU table.
 *
 * The entry is marked streaming if an STC was detected and 
 * the BUS_DMA_STREAMING flag is set.
 */
void
iommu_enter(struct iommu_state *is, struct strbuf_ctl *sb, vaddr_t va,
    paddr_t pa, int flags)
a251 1
	volatile int64_t *tte_ptr = &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)];
d254 1
a254 2
	if (va < is->is_dvmabase || round_page(va + PAGE_SIZE) >
	    is->is_dvmaend + 1)
d256 1
d258 8
a265 7
	tte = *tte_ptr;

	if (tte & IOTTE_V) {
		printf("Overwriting valid tte entry (dva %lx pa %lx "
		    "&tte %p tte %llx)\n", va, pa, tte_ptr, tte);
		extent_print(is->is_dvmamap);
		panic("IOMMU overwrite");
a266 5
#endif

	tte = MAKEIOTTE(pa, !(flags & BUS_DMA_NOWRITE),
	    !(flags & BUS_DMA_NOCACHE), (flags & BUS_DMA_STREAMING));

d268 4
a271 19
	    (int)IOTSBSLOT(va,is->is_tsbsize), (void *)(u_long)va));

	*tte_ptr = tte;

	/*
	 * Why bother to flush this va?  It should only be relevant for
	 * V ==> V or V ==> non-V transitions.  The former is illegal and
	 * the latter is never done here.  It is true that this provides
	 * some protection against a misbehaving master using an address
	 * after it should.  The IOMMU documentations specifically warns
	 * that the consequences of a simultaneous IOMMU flush and DVMA
	 * access to the same address are undefined.  (By that argument,
	 * the STC should probably be flushed as well.)   Note that if
	 * a bus master keeps using a memory region after it has been
	 * unmapped, the specific behavior of the IOMMU is likely to
	 * be the least of our worries.
	 */
	IOMMUREG_WRITE(is, iommu_flush, va);

d273 3
a275 3
	    va, (long)pa, (u_long)IOTSBSLOT(va,is->is_tsbsize), 
	    (void *)(u_long)&is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
	    (u_long)tte));
a277 53
/*
 * Remove an entry from the IOMMU table.
 *
 * The entry is flushed from the STC if an STC is detected and the TSB
 * entry has the IOTTE_STREAM flags set.  It should be impossible for
 * the TSB entry to have this flag set without the BUS_DMA_STREAMING
 * flag, but better to be safe.  (The IOMMU will be ignored as long
 * as an STC entry exists.)
 */
void
iommu_remove(struct iommu_state *is, struct strbuf_ctl *sb, vaddr_t va)
{
	int64_t *tte_ptr = &is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)];
	int64_t tte;

#ifdef DIAGNOSTIC
	if (trunc_page(va) < is->is_dvmabase || round_page(va) >
	    is->is_dvmaend + 1)
		panic("iommu_remove: va 0x%lx not in DVMA space", (u_long)va);
	if (va != trunc_page(va)) {
		printf("iommu_remove: unaligned va: %lx\n", va);
		va = trunc_page(va);
	}
#endif
	tte = *tte_ptr;

	DPRINTF(IDB_IOMMU, ("iommu_remove: va %lx TSB[%llx]@@%p\n",
	    va, tte, tte_ptr));

#ifdef DIAGNOSTIC
	if ((tte & IOTTE_V) == 0) {
		printf("Removing invalid tte entry (dva %lx &tte %p "
		    "tte %llx)\n", va, tte_ptr, tte);
		extent_print(is->is_dvmamap);
		panic("IOMMU remove overwrite");
	}
#endif

	*tte_ptr = tte & ~IOTTE_V;

	/*
	 * IO operations are strongly ordered WRT each other.  It is
	 * unclear how they relate to normal memory accesses.
	 */
	membar(StoreStore);

	IOMMUREG_WRITE(is, iommu_flush, va);

	if (sb && (tte & IOTTE_STREAM))
		iommu_strbuf_flush(sb, va);

	/* Should we sync the iommu and stc here? */
}
d280 1
a280 1
 * Find the physical address of a DVMA address (debug routine).
d283 3
a285 1
iommu_extract(struct iommu_state *is, vaddr_t dva)
d289 2
a290 2
	if (dva >= is->is_dvmabase && dva <= is->is_dvmaend)
		tte = is->is_tsb[IOTSBSLOT(dva, is->is_tsbsize)];
d292 3
a294 1
	return (tte & IOTTE_PAMASK);
d298 1
a298 1
 * Lookup a TSB entry for a given DVMA (debug routine).
d301 3
a303 1
iommu_lookup_tte(struct iommu_state *is, vaddr_t dva)
d305 4
a308 4
	int64_t tte = 0;
	
	if (dva >= is->is_dvmabase && dva <= is->is_dvmaend)
		tte = is->is_tsb[IOTSBSLOT(dva, is->is_tsbsize)];
d310 1
a310 2
	return (tte);
}
d312 2
a313 11
/*
 * Lookup a TSB entry at a given physical address (debug routine).
 */
int64_t
iommu_fetch_tte(struct iommu_state *is, paddr_t pa)
{
	int64_t tte = 0;
	
	if (pa >= is->is_ptsb && pa < is->is_ptsb +
	    (PAGE_SIZE << is->is_tsbsize)) 
		tte = ldxa(pa, ASI_PHYS_CACHED);
d319 5
a323 1
 * Fetch a TSB entry with some sanity checking.
d325 5
a329 2
int64_t
iommu_tsb_entry(struct iommu_state *is, vaddr_t dva)
d331 32
a362 1
	int64_t tte;
d364 4
a367 2
	if (dva < is->is_dvmabase && dva > is->is_dvmaend)
		panic("invalid dva: %llx", (long long)dva);
d369 7
a375 1
	tte = is->is_tsb[IOTSBSLOT(dva,is->is_tsbsize)];
d377 14
a390 4
	if ((tte & IOTTE_V) == 0)
		panic("iommu_tsb_entry: invalid entry %lx", dva);

	return (tte);
d393 3
a395 5
/*
 * Initiate and then block until an STC flush synchronization has completed.
 */
int 
iommu_strbuf_flush_done(struct iommu_map_state *ims)
a396 2
	struct strbuf_ctl *sb = ims->ims_sb;
	struct strbuf_flush *sf = &ims->ims_flush;
a397 3
	struct timeval to = { 0, 500000 };
	u_int64_t flush;
	int timeout_started = 0;
d399 2
a400 5
#ifdef DIAGNOSTIC
	if (sb == NULL) {
		panic("iommu_strbuf_flush_done: invalid flush buffer");
	}
#endif
d405 2
a406 1
	 *   1 Tell strbuf to flush by storing va to strbuf_pgflush.
d411 2
a412 1
	 * If it takes more than .5 sec, something went very, very wrong.
d414 37
a450 46

	/*
	 * If we're reading from ASI_PHYS_CACHED, then we'll write to
	 * it too.  No need to tempt fate or learn about Si bugs or such.
	 * FreeBSD just uses normal "volatile" reads/writes...
	 */

	stxa(sf->sbf_flushpa, ASI_PHYS_CACHED, 0);

	/*
	 * Insure any previous strbuf operations are complete and that 
	 * memory is initialized before the IOMMU uses it.
	 * Is this Needed?  How are IO and memory operations ordered? 
	 */
	membar(StoreStore);

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
		    STRBUFREG(strbuf_flushsync), sf->sbf_flushpa);

	DPRINTF(IDB_IOMMU,
	    ("iommu_strbuf_flush_done: flush = %llx pa = %lx\n", 
		ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED), sf->sbf_flushpa));

	membar(StoreLoad | Lookaside);

	for(;;) {
		int i;

		/*
		 * Try to shave a few instruction cycles off the average
		 * latency by only checking the elapsed time every few
		 * fetches.
		 */
		for (i = 0; i < 1000; ++i) {
			membar(LoadLoad);
			/* Bypass non-coherent D$ */
			/* non-coherent...?   Huh? */
			flush = ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED);

			if (flush) {
				DPRINTF(IDB_IOMMU,
				    ("iommu_strbuf_flush_done: flushed\n"));
				return (0);
			}
		}

d452 1
d454 10
a463 16
		if (timeout_started) {
			if (timercmp(&cur, &flushtimeout, >))
				panic("STC timeout at %lx (%lld)",
				    sf->sbf_flushpa, flush);
		} else {
			timeradd(&cur, &to, &flushtimeout);
			
			timeout_started = 1;
	
			DPRINTF(IDB_IOMMU,
			    ("iommu_strbuf_flush_done: flush = %llx pa = %lx "
				"now=%lx:%lx until = %lx:%lx\n", 
				ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED),
				sf->sbf_flushpa, cur.tv_sec, cur.tv_usec, 
				flushtimeout.tv_sec, flushtimeout.tv_usec));
		}
d465 3
d474 3
a476 5
iommu_dvmamap_create(bus_dma_tag_t t, struct iommu_state *is,
    struct strbuf_ctl *sb, bus_size_t size, int nsegments, bus_size_t maxsegsz,
    bus_size_t boundary, int flags, bus_dmamap_t *dmamap)
{
	int ret;
d478 4
a481 51
	struct iommu_map_state *ims;

	ret = bus_dmamap_create(t->_parent, size, nsegments, maxsegsz,
	    boundary, flags, &map);

	if (ret)
		return (ret);

	ims = iommu_iomap_create(nsegments);

	if (ims == NULL) {
		bus_dmamap_destroy(t->_parent, map);
		return (ENOMEM);
	}

	ims->ims_sb = sb;
	map->_dm_cookie = ims;
	*dmamap = map;

	return (0);
}

void
iommu_dvmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{
	/*
	 * The specification (man page) requires a loaded
	 * map to be unloaded before it is destroyed.
	 */
	if (map->dm_nsegs)
		bus_dmamap_unload(t, map);

        if (map->_dm_cookie)
                iommu_iomap_destroy(map->_dm_cookie);
	map->_dm_cookie = NULL;

	bus_dmamap_destroy(t->_parent, map);
}

/*
 * Load a contiguous kva buffer into a dmamap.  The physical pages are
 * not assumed to be contiguous.  Two passes are made through the buffer
 * and both call pmap_extract() for the same va->pa translations.  It
 * is possible to run out of pa->dvma mappings; the code should be smart
 * enough to resize the iomap (when the "flags" permit allocation).  It
 * is trivial to compute the number of entries required (round the length
 * up to the page size and then divide by the page size)...
 */
int
iommu_dvmamap_load(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map,
    void *buf, bus_size_t buflen, struct proc *p, int flags)
d484 1
a484 1
	int err = 0;
d486 1
d489 2
a490 1
	struct iommu_map_state *ims = map->_dm_cookie;
a492 5
#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_load: null map state");
#endif

d494 1
a494 4
		/*
		 * Is it still in use? _bus_dmamap_load should have taken care
		 * of this.
		 */
a499 1

d505 1
a505 1
	if (buflen < 1 || buflen > map->_dm_size) {
d512 2
d520 1
a520 34
	align = max(map->dm_segs[0]._ds_align, PAGE_SIZE);

	pmap = p ? p->p_vmspace->vm_map.pmap : pmap = pmap_kernel();

	/* Count up the total number of pages we need */
	iommu_iomap_clear_pages(ims);
	{ /* Scope */
		bus_addr_t a, aend;
		bus_addr_t addr = (vaddr_t)buf;
		int seg_len = buflen;

		aend = round_page(addr + seg_len - 1);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			paddr_t pa;

			if (pmap_extract(pmap, a, &pa) == FALSE) {
				printf("iomap pmap error addr 0x%llx\n", a);
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}

			err = iommu_iomap_insert_page(ims, pa);
			if (err) {
				printf("iomap insert error: %d for "
				    "va 0x%llx pa 0x%lx "
				    "(buf %p len %lld/%llx)\n",
				    err, a, pa, buf, buflen, buflen);
				iommu_dvmamap_print_map(t, is, map);
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}
		}
	}
	sgsize = ims->ims_map.ipm_pagecnt * PAGE_SIZE;
d529 1
a529 1

d532 1
a532 1
	 * split the transfer up into little pieces ourselves.
a533 1
	s = splhigh();
d536 1
a536 1
	    EX_NOWAIT | EX_BOUNDZERO, (u_long *)&dvmaddr);
d540 2
a541 1
	if (err || (dvmaddr == (bus_addr_t)-1))	{ 
d545 1
a545 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d559 39
d600 4
a603 3
#ifdef DEBUG
	iommu_dvmamap_validate_map(t, is, map);
#endif
d605 8
a612 2
	if (iommu_iomap_load_map(is, ims, dvmaddr, flags))
		return (E2BIG);
d614 6
a619 18
	{ /* Scope */
		bus_addr_t a, aend;
		bus_addr_t addr = (vaddr_t)buf;
		int seg_len = buflen;

		aend = round_page(addr + seg_len - 1);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			bus_addr_t pgstart;
			bus_addr_t pgend;
			paddr_t pa;
			int pglen;

			/* Yuck... Redoing the same pmap_extract... */
			if (pmap_extract(pmap, a, &pa) == FALSE) {
				printf("iomap pmap error addr 0x%llx\n", a);
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}
d621 22
a642 17
			pgstart = pa | (max(a, addr) & PAGE_MASK);
			pgend = pa | (min(a + PAGE_SIZE - 1,
			    addr + seg_len - 1) & PAGE_MASK);
			pglen = pgend - pgstart + 1;

			if (pglen < 1)
				continue;

			err = iommu_dvmamap_append_range(t, map, pgstart,
			    pglen, flags, boundary);
			if (err) {
				printf("iomap load seg page: %d for "
				    "va 0x%llx pa %lx (%llx - %llx) "
				    "for %d/0x%x\n",
				    err, a, pa, pgstart, pgend, pglen, pglen);
				return (err);
			}
d645 4
d650 8
a657 3
#ifdef DIAGNOSTIC
	iommu_dvmamap_validate_map(t, is, map);
#endif
d659 1
d661 2
a662 6
	if (err)
		printf("**** iommu_dvmamap_load failed with error %d\n",
		    err);
	
	if (err || (iommudebug & IDB_PRINT_MAP)) {
		iommu_dvmamap_print_map(t, is, map);
d664 1
a664 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d668 17
d686 1
a686 1
	return (err);
d689 1
a689 12
/*
 * Load a dvmamap from an array of segs or an mlist (if the first
 * "segs" entry's mlist is non-null).  It calls iommu_dvmamap_load_segs()
 * or iommu_dvmamap_load_mlist() for part of the 2nd pass through the
 * mapping.  This is ugly.  A better solution would probably be to have
 * function pointers for implementing the traversal.  That way, there
 * could be one core load routine for each of the three required algorithms
 * (buffer, seg, and mlist).  That would also mean that the traversal
 * algorithm would then only need one implementation for each algorithm
 * instead of two (one for populating the iomap and one for populating
 * the dvma map).
 */
d691 8
a698 3
iommu_dvmamap_load_raw(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, bus_dma_segment_t *segs, int nsegs, int flags,
    bus_size_t size)
d700 2
a701 1
	int i, s;
d703 1
a703 1
	int err = 0;
d705 1
d708 3
a710 6
	struct iommu_map_state *ims = map->_dm_cookie;

#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_load_raw: null map state");
#endif
d727 1
a727 1
	align = max(segs[0]._ds_align, PAGE_SIZE);
d733 10
a742 41

	iommu_iomap_clear_pages(ims);
	if (segs[0]._ds_mlist) {
		struct pglist *mlist = segs[0]._ds_mlist;
		struct vm_page *m;
		for (m = TAILQ_FIRST(mlist); m != NULL;
		    m = TAILQ_NEXT(m,pageq)) {
			err = iommu_iomap_insert_page(ims, VM_PAGE_TO_PHYS(m));

			if(err) {
				printf("iomap insert error: %d for "
				    "pa 0x%lx\n", err, VM_PAGE_TO_PHYS(m));
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}
		}
	} else {
		/* Count up the total number of pages we need */
		for (i = 0, left = size; left > 0 && i < nsegs; i++) {
			bus_addr_t a, aend;
			bus_size_t len = segs[i].ds_len;
			bus_addr_t addr = segs[i].ds_addr;
			int seg_len = min(left, len);

			if (len < 1)
				continue;

			aend = round_page(addr + seg_len - 1);
			for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {

				err = iommu_iomap_insert_page(ims, a);
				if (err) {
					printf("iomap insert error: %d for "
					    "pa 0x%llx\n", err, a);
					iommu_iomap_clear_pages(ims);
					return (E2BIG);
				}
			}

			left -= seg_len;
		}
d744 1
a744 1
	sgsize = ims->ims_map.ipm_pagecnt * PAGE_SIZE;
d753 1
a753 1

a757 1
	s = splhigh();
d760 1
a760 1
	    EX_NOWAIT | EX_BOUNDZERO, (u_long *)&dvmaddr);
d767 4
a770 3
	if (dvmaddr == (bus_addr_t)-1)	{ 
		printf("iommu_dvmamap_load_raw(): extent_alloc(%d, %x) "
		    "failed!\n", (int)sgsize, flags);
d772 1
a772 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d785 12
a796 1
	map->dm_mapsize = size;
d798 15
d814 2
a815 1
	iommu_dvmamap_validate_map(t, is, map);
d817 4
d822 10
a831 2
	if (iommu_iomap_load_map(is, ims, dvmaddr, flags))
		return (E2BIG);
d833 18
a850 6
	if (segs[0]._ds_mlist)
		err = iommu_dvmamap_load_mlist(t, is, map, segs[0]._ds_mlist,
		    flags, size, boundary);
	else
		err = iommu_dvmamap_load_seg(t, is, map, segs, nsegs,
		    flags, size, boundary);
d852 17
a868 28
	if (err)
		iommu_iomap_unload_map(is, ims);

#ifdef DIAGNOSTIC
	/* The map should be valid even if the load failed */
	if (iommu_dvmamap_validate_map(t, is, map)) {
		printf("load size %lld/0x%llx\n", size, size);
		if (segs[0]._ds_mlist)
			printf("mlist %p\n", segs[0]._ds_mlist);
		else  {
			long tot_len = 0;
			long clip_len = 0;
			printf("segs %p nsegs %d\n", segs, nsegs);

			left = size;
			for(i = 0; i < nsegs; i++) {
				bus_size_t len = segs[i].ds_len;
				bus_addr_t addr = segs[i].ds_addr;
				int seg_len = min(left, len);

				printf("addr %llx len %lld/0x%llx seg_len "
				    "%d/0x%x left %d/0x%x\n", addr, len, len,
				    seg_len, seg_len, left, left);

				left -= seg_len;
				
				clip_len += seg_len;
				tot_len += segs[i].ds_len;
a869 4
			printf("total length %ld/0x%lx total seg. "
			    "length %ld/0x%lx\n", tot_len, tot_len, clip_len,
			    clip_len);
		}
d871 2
a872 3
		if (err == 0)
			err = 1;
	}
d874 11
d886 2
a887 1

d889 1
a889 11
	if (err)
		printf("**** iommu_dvmamap_load_raw failed with error %d\n",
		    err);
	
	if (err || (iommudebug & IDB_PRINT_MAP)) {
		iommu_dvmamap_print_map(t, is, map);
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
	}
d892 3
a894 2
	return (err);
}
d896 3
a898 23
/*
 * Insert a range of addresses into a loaded map respecting the specified
 * boundary and alignment restrictions.  The range is specified by its 
 * physical address and length.  The range cannot cross a page boundary.
 * This code (along with most of the rest of the function in this file)
 * assumes that the IOMMU page size is equal to PAGE_SIZE.
 */
int
iommu_dvmamap_append_range(bus_dma_tag_t t, bus_dmamap_t map, paddr_t pa,
    bus_size_t length, int flags, bus_size_t boundary)
{
	struct iommu_map_state *ims = map->_dm_cookie;
	bus_addr_t sgstart, sgend, bd_mask;
	bus_dma_segment_t *seg = NULL;
	int i = map->dm_nsegs;

#ifdef DEBUG
	if (ims == NULL)
		panic("iommu_dvmamap_append_range: null map state");
#endif

	sgstart = iommu_iomap_translate(ims, pa);
	sgend = sgstart + length - 1;
d900 1
d902 10
a911 6
	if (sgstart == NULL || sgstart >= sgend) {
		printf("append range invalid mapping for %lx "
		    "(0x%llx - 0x%llx)\n", pa, sgstart, sgend);
		map->dm_nsegs = 0;
		return (EINVAL);
	}
d913 1
a913 64

#ifdef DEBUG
	if (trunc_page(sgstart) != trunc_page(sgend)) {
		printf("append range crossing page boundary! "
		    "pa %lx length %lld/0x%llx sgstart %llx sgend %llx\n",
		    pa, length, length, sgstart, sgend);
	}
#endif

	/*
	 * We will attempt to merge this range with the previous entry
	 * (if there is one).
	 */
	if (i > 0) {
		seg = &map->dm_segs[i - 1];
		if (sgstart == seg->ds_addr + seg->ds_len) {
			length += seg->ds_len;
			sgstart = seg->ds_addr;
			sgend = sgstart + length - 1;
		} else
			seg = NULL;
	}

	if (seg == NULL) {
		seg = &map->dm_segs[i];
		if (++i > map->_dm_segcnt) {
			printf("append range, out of segments (%d)\n", i);
			iommu_dvmamap_print_map(t, NULL, map);
			map->dm_nsegs = 0;
			return (ENOMEM);
		}
	}

	/*
	 * At this point, "i" is the index of the *next* bus_dma_segment_t
	 * (the segment count, aka map->dm_nsegs) and "seg" points to the
	 * *current* entry.  "length", "sgstart", and "sgend" reflect what
	 * we intend to put in "*seg".  No assumptions should be made about
	 * the contents of "*seg".  Only "boundary" issue can change this
	 * and "boundary" is often zero, so explicitly test for that case
	 * (the test is strictly an optimization).
	 */ 
	if (boundary != 0) {
		bd_mask = ~(boundary - 1);

		while ((sgstart & bd_mask) != (sgend & bd_mask)) {
			/*
			 * We are crossing a boundary so fill in the current
			 * segment with as much as possible, then grab a new
			 * one.
			 */

			seg->ds_addr = sgstart;
			seg->ds_len = boundary - (sgstart & bd_mask);

			sgstart += seg->ds_len; /* sgend stays the same */
			length -= seg->ds_len;

			seg = &map->dm_segs[i];
			if (++i > map->_dm_segcnt) {
				printf("append range, out of segments\n");
				iommu_dvmamap_print_map(t, NULL, map);
				map->dm_nsegs = 0;
				return (E2BIG);
d916 2
a918 23

	seg->ds_addr = sgstart;
	seg->ds_len = length;
	map->dm_nsegs = i;

	return (0);
}

/*
 * Populate the iomap from a bus_dma_segment_t array.  See note for
 * iommu_dvmamap_load() * regarding page entry exhaustion of the iomap.
 * This is less of a problem for load_seg, as the number of pages
 * is usually similar to the number of segments (nsegs).
 */
int
iommu_dvmamap_load_seg(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, bus_dma_segment_t *segs, int nsegs, int flags,
    bus_size_t size, bus_size_t boundary)
{
	int i;
	int left;
	int seg;

d920 2
a921 4
	 * This segs is made up of individual physical
	 * segments, probably by _bus_dmamap_load_uio() or
	 * _bus_dmamap_load_mbuf().  Ignore the mlist and
	 * load each one individually.
d923 22
a944 49

	/*
	 * Keep in mind that each segment could span
	 * multiple pages and that these are not always
	 * adjacent. The code is no longer adding dvma
	 * aliases to the IOMMU.  The STC will not cross
	 * page boundaries anyway and a IOMMU table walk
	 * vs. what may be a streamed PCI DMA to a ring
	 * descriptor is probably a wash.  It eases TLB
	 * pressure and in the worst possible case, it is
	 * only as bad a non-IOMMUed architecture.  More
	 * importantly, the code is not quite as hairy.
	 * (It's bad enough as it is.)
	 */
	left = size;
	seg = 0;
	for (i = 0; left > 0 && i < nsegs; i++) {
		bus_addr_t a, aend;
		bus_size_t len = segs[i].ds_len;
		bus_addr_t addr = segs[i].ds_addr;
		int seg_len = min(left, len);

		if (len < 1)
			continue;

		aend = addr + seg_len - 1;
		for (a = trunc_page(addr); a < round_page(aend);
		    a += PAGE_SIZE) {
			bus_addr_t pgstart;
			bus_addr_t pgend;
			int pglen;
			int err;

			pgstart = max(a, addr);
			pgend = min(a + PAGE_SIZE - 1, addr + seg_len - 1);
			pglen = pgend - pgstart + 1;
			
			if (pglen < 1)
				continue;

			err = iommu_dvmamap_append_range(t, map, pgstart,
			    pglen, flags, boundary);
			if (err) {
				printf("iomap load seg page: %d for "
				    "pa 0x%llx (%llx - %llx for %d/%x\n",
				    err, a, pgstart, pgend, pglen, pglen);
				return (err);
			}

d946 2
a947 2

		left -= seg_len;
d949 4
a952 15
	return (0);
}

/*
 * Populate the iomap from an mlist.  See note for iommu_dvmamap_load()
 * regarding page entry exhaustion of the iomap.
 */
int
iommu_dvmamap_load_mlist(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, struct pglist *mlist, int flags,
    bus_size_t size, bus_size_t boundary)
{
	struct vm_page *m;
	paddr_t pa;
	int err;
a953 4
	/*
	 * This was allocated with bus_dmamem_alloc.
	 * The pages are on an `mlist'.
	 */
d955 2
d959 7
a965 8
		err = iommu_dvmamap_append_range(t, map, pa, PAGE_SIZE,
		    flags, boundary);
		if (err) {
			printf("iomap load seg page: %d for pa 0x%lx "
			    "(%lx - %lx for %d/%x\n", err, pa, pa,
			    pa + PAGE_SIZE, PAGE_SIZE, PAGE_SIZE);
			return (err);
		}
d967 11
a977 19

	return (0);
}

/*
 * Unload a dvmamap.
 */
void
iommu_dvmamap_unload(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map)
{
	struct iommu_map_state *ims = map->_dm_cookie;
	bus_addr_t dvmaddr = map->_dm_dvmastart;
	bus_size_t sgsize = map->_dm_dvmasize;
	int error, s;

	/* Flush the iommu */
#ifdef DEBUG
	if (dvmaddr == 0) {
		printf("iommu_dvmamap_unload: No dvmastart\n");
d979 1
a979 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d981 1
a981 79
		return;
	}
	iommu_dvmamap_validate_map(t, is, map);

	if (iommudebug & IDB_PRINT_MAP)
		iommu_dvmamap_print_map(t, is, map);
#endif /* DEBUG */

	/* Remove the IOMMU entries */
	iommu_iomap_unload_map(is, ims);

	/* Clear the iomap */
	iommu_iomap_clear_pages(ims);

	bus_dmamap_unload(t->_parent, map);

	/* Mark the mappings as invalid. */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	s = splhigh();
	error = extent_free(is->is_dvmamap, dvmaddr, 
		sgsize, EX_NOWAIT);
	map->_dm_dvmastart = 0;
	map->_dm_dvmasize = 0;
	splx(s);
	if (error != 0)
		printf("warning: %qd of DVMA space lost\n", sgsize);
}

/*
 * Perform internal consistency checking on a dvmamap.
 */
int
iommu_dvmamap_validate_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map)
{
	int err = 0;
	int seg;

	if (trunc_page(map->_dm_dvmastart) != map->_dm_dvmastart) {
		printf("**** dvmastart address not page aligned: %llx",
			map->_dm_dvmastart);
		err = 1;
	}
	if (trunc_page(map->_dm_dvmasize) != map->_dm_dvmasize) {
		printf("**** dvmasize not a multiple of page size: %llx",
			map->_dm_dvmasize);
		err = 1;
	}
	if (map->_dm_dvmastart < is->is_dvmabase ||
	    round_page(map->_dm_dvmastart + map->_dm_dvmasize) >
	    is->is_dvmaend + 1) {
		printf("dvmaddr %llx len %llx out of range %x - %x\n",
			    map->_dm_dvmastart, map->_dm_dvmasize,
			    is->is_dvmabase, is->is_dvmaend);
		err = 1;
	}
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		if (map->dm_segs[seg].ds_addr == 0 ||
		    map->dm_segs[seg].ds_len == 0) {
			printf("seg %d null segment dvmaddr %llx len %llx for "
			    "range %llx len %llx\n",
			    seg,
			    map->dm_segs[seg].ds_addr,
			    map->dm_segs[seg].ds_len,
			    map->_dm_dvmastart, map->_dm_dvmasize);
			err = 1;
		} else if (map->dm_segs[seg].ds_addr < map->_dm_dvmastart ||
		    round_page(map->dm_segs[seg].ds_addr +
			map->dm_segs[seg].ds_len) >
		    map->_dm_dvmastart + map->_dm_dvmasize) {
			printf("seg %d dvmaddr %llx len %llx out of "
			    "range %llx len %llx\n",
			    seg,
			    map->dm_segs[seg].ds_addr,
			    map->dm_segs[seg].ds_len,
			    map->_dm_dvmastart, map->_dm_dvmasize);
			err = 1;
d983 1
a983 7
	}

	if (err) {
		iommu_dvmamap_print_map(t, is, map);
#if defined(DDB) && defined(DEBUG)
		if (iommudebug & IDB_BREAK)
			Debugger();
d985 1
a985 3
	}

	return (err);
d989 7
a995 88
iommu_dvmamap_print_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map)
{
	int seg, i;
	long full_len, source_len;
	struct mbuf *m;

	printf("DVMA %x for %x, mapping %p: dvstart %llx dvsize %llx "
	    "size %lld/%llx maxsegsz %llx boundary %llx segcnt %d "
	    "flags %x type %d source %p "
	    "cookie %p mapsize %llx nsegs %d\n",
	    is ? is->is_dvmabase : 0, is ? is->is_dvmaend : 0, map,
	    map->_dm_dvmastart, map->_dm_dvmasize,
	    map->_dm_size, map->_dm_size, map->_dm_maxsegsz, map->_dm_boundary,
	    map->_dm_segcnt, map->_dm_flags, map->_dm_type,
	    map->_dm_source, map->_dm_cookie, map->dm_mapsize,
	    map->dm_nsegs);

	full_len = 0;
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		printf("seg %d dvmaddr %llx pa %lx len %llx (tte %llx)\n",
		    seg, map->dm_segs[seg].ds_addr,
		    is ? iommu_extract(is, map->dm_segs[seg].ds_addr) : 0,
		    map->dm_segs[seg].ds_len,
		    is ? iommu_lookup_tte(is, map->dm_segs[seg].ds_addr) : 0);
		full_len += map->dm_segs[seg].ds_len;
	}
	printf("total length = %ld/0x%lx\n", full_len, full_len);

	if (map->_dm_source) switch (map->_dm_type) {
	case _DM_TYPE_MBUF:
		m = map->_dm_source;
		if (m->m_flags & M_PKTHDR)
			printf("source PKTHDR mbuf (%p) hdr len = %d/0x%x:\n",
			    m, m->m_pkthdr.len, m->m_pkthdr.len);
		else
			printf("source mbuf (%p):\n", m);

		source_len = 0;
		for ( ; m; m = m->m_next) {
			vaddr_t vaddr = mtod(m, vaddr_t);
			long len = m->m_len;
			paddr_t pa;

			if (pmap_extract(pmap_kernel(), vaddr, &pa))
				printf("kva %lx pa %lx len %ld/0x%lx\n",
				    vaddr, pa, len, len);
			else
				printf("kva %lx pa <invalid> len %ld/0x%lx\n",
				    vaddr, len, len);

			source_len += len;
		}

		if (full_len != source_len)
			printf("mbuf length %ld/0x%lx is %s than mapping "
			    "length %ld/0x%lx\n", source_len, source_len,
			    (source_len > full_len) ? "greater" : "less",
			    full_len, full_len);
		else
			printf("mbuf length %ld/0x%lx\n", source_len,
			    source_len);
		break;
	case _DM_TYPE_LOAD:
	case _DM_TYPE_SEGS:
	case _DM_TYPE_UIO:
	default:
		break;
	}

	if (map->_dm_cookie) {
		struct iommu_map_state *ims = map->_dm_cookie;
		struct iommu_page_map *ipm = &ims->ims_map;

		printf("page map (%p) of size %d with %d entries\n",
		    ipm, ipm->ipm_maxpage, ipm->ipm_pagecnt);
		for (i = 0; i < ipm->ipm_pagecnt; ++i) {
			struct iommu_page_entry *e = &ipm->ipm_map[i];
			printf("%d: vmaddr 0x%lx pa 0x%lx\n", i,
			    e->ipe_va, e->ipe_pa);
		}
	} else
		printf("iommu map state (cookie) is NULL\n");
}

void
iommu_dvmamap_sync(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map,
	bus_addr_t offset, bus_size_t len, int ops)
a996 2
	struct iommu_map_state *ims = map->_dm_cookie;
	struct strbuf_ctl *sb;
a999 15
#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_sync: null map state");
#endif
	sb = ims->ims_sb;

	if ((ims->ims_flags & IOMMU_MAP_STREAM) == 0 || (len == 0))
		return;

	if (ops & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTWRITE))
		return;

	if ((ops & (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_PREWRITE)) == 0)
		return;

d1011 2
a1012 3
		if (count > 0 && iommu_dvmamap_sync_range(sb,
		    map->dm_segs[i].ds_addr + offset, count))
			needsflush = 1;
d1020 1
a1020 1
		iommu_strbuf_flush_done(ims);
d1028 65
a1092 13
iommu_dvmamap_sync_range(struct strbuf_ctl *sb, vaddr_t va, bus_size_t len)
{
	vaddr_t vaend;
#ifdef DIAGNOSTIC
	struct iommu_state *is = sb->sb_iommu;

	if (va < is->is_dvmabase || va >= is->is_dvmaend)
		panic("invalid va: %llx", (long long)va);

	if ((is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)] & IOTTE_STREAM) == 0) {
		printf("iommu_dvmamap_sync_range: attempting to flush "
		    "non-streaming entry\n");
		return (0);
d1094 5
a1098 18
#endif

	vaend = (va + len + PAGE_MASK) & ~PAGE_MASK;
	va &= ~PAGE_MASK;

#ifdef DIAGNOSTIC
	if (va < is->is_dvmabase || vaend >= is->is_dvmaend)
		panic("invalid va range: %llx to %llx (%x to %x)",
		    (long long)va, (long long)vaend,
		    is->is_dvmabase,
		    is->is_dvmaend);
#endif

	for ( ; va <= vaend; va += PAGE_SIZE) {
		DPRINTF(IDB_BUSDMA,
		    ("iommu_dvmamap_sync_range: flushing va %p\n",
		    (void *)(u_long)va));
		iommu_strbuf_flush(sb, va);
d1101 2
a1102 1
	return (1);
d1106 8
a1113 3
iommu_dvmamem_alloc(bus_dma_tag_t t, struct iommu_state *is, bus_size_t size,
    bus_size_t alignment, bus_size_t boundary, bus_dma_segment_t *segs,
    int nsegs, int *rsegs, int flags)
d1116 4
a1119 4
	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_alloc: sz %llx align %llx "
	    "bound %llx segp %p flags %d\n", (unsigned long long)size,
	    (unsigned long long)alignment, (unsigned long long)boundary,
	    segs, flags));
d1121 1
a1121 1
	    segs, nsegs, rsegs, flags | BUS_DMA_DVMA));
d1125 5
a1129 2
iommu_dvmamem_free(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *segs, int nsegs)
d1142 8
a1149 2
iommu_dvmamem_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *segs, int nsegs, size_t size, caddr_t *kvap, int flags)
d1155 1
a1155 1
	bus_addr_t cbit = 0;
d1174 1
a1174 1
#if 0
a1176 1
#endif
d1191 1
a1191 2
		    "mapping va %lx at %llx\n", va,
		    (unsigned long long)addr | cbit));
d1207 5
a1211 2
iommu_dvmamem_unmap(bus_dma_tag_t t, struct iommu_state *is, caddr_t kva,
    size_t size)
d1218 1
a1218 1
	if ((u_long)kva & PAGE_MASK)
a1226 177

/*
 * Create a new iomap.
 */
struct iommu_map_state *
iommu_iomap_create(int n)
{
	struct iommu_map_state *ims;
	struct strbuf_flush *sbf;
	vaddr_t va;

	if (n < 64)
		n = 64;

	ims = malloc(sizeof(*ims) + (n - 1) * sizeof(ims->ims_map.ipm_map[0]),
		M_DEVBUF, M_NOWAIT);
	if (ims == NULL)
		return (NULL);

	memset(ims, 0, sizeof *ims);

	/* Initialize the map. */
	ims->ims_map.ipm_maxpage = n;
	SPLAY_INIT(&ims->ims_map.ipm_tree);

	/* Initialize the flush area. */
	sbf = &ims->ims_flush;
	va = (vaddr_t)&sbf->sbf_area[0x40];
	va &= ~0x3f;
	pmap_extract(pmap_kernel(), va, &sbf->sbf_flushpa);
	sbf->sbf_flush = (void *)va;

	return (ims);
}

/*
 * Destroy an iomap.
 */
void
iommu_iomap_destroy(struct iommu_map_state *ims)
{
#ifdef DIAGNOSTIC
	if (ims->ims_map.ipm_pagecnt > 0)
		printf("iommu_iomap_destroy: %d page entries in use\n",
		    ims->ims_map.ipm_pagecnt);
#endif

	free(ims, M_DEVBUF);
}

/*
 * Utility function used by splay tree to order page entries by pa.
 */
static inline int
iomap_compare(struct iommu_page_entry *a, struct iommu_page_entry *b)
{
	return ((a->ipe_pa > b->ipe_pa) ? 1 :
		(a->ipe_pa < b->ipe_pa) ? -1 : 0);
}

SPLAY_PROTOTYPE(iommu_page_tree, iommu_page_entry, ipe_node, iomap_compare);

SPLAY_GENERATE(iommu_page_tree, iommu_page_entry, ipe_node, iomap_compare);

/*
 * Insert a pa entry in the iomap.
 */
int
iommu_iomap_insert_page(struct iommu_map_state *ims, paddr_t pa)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;

	if (ipm->ipm_pagecnt >= ipm->ipm_maxpage) {
		struct iommu_page_entry ipe;

		ipe.ipe_pa = pa;
		if (SPLAY_FIND(iommu_page_tree, &ipm->ipm_tree, &ipe))
			return (0);

		return (ENOMEM);
	}

	e = &ipm->ipm_map[ipm->ipm_pagecnt];

	e->ipe_pa = pa;
	e->ipe_va = NULL;

	e = SPLAY_INSERT(iommu_page_tree, &ipm->ipm_tree, e);

	/* Duplicates are okay, but only count them once. */
	if (e)
		return (0);

	++ipm->ipm_pagecnt;

	return (0);
}

/*
 * Locate the iomap by filling in the pa->va mapping and inserting it
 * into the IOMMU tables.
 */
int
iommu_iomap_load_map(struct iommu_state *is, struct iommu_map_state *ims,
    vaddr_t vmaddr, int flags)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct strbuf_ctl *sb = ims->ims_sb;
	int i;

	if (sb->sb_flush == NULL)
		flags &= ~BUS_DMA_STREAMING;

	if (flags & BUS_DMA_STREAMING)
		ims->ims_flags |= IOMMU_MAP_STREAM;
	else
		ims->ims_flags &= ~IOMMU_MAP_STREAM;

	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e) {
		e->ipe_va = vmaddr;
		iommu_enter(is, sb, e->ipe_va, e->ipe_pa, flags);
		vmaddr += PAGE_SIZE;
	}

	return (0);
}

/*
 * Remove the iomap from the IOMMU.
 */
int
iommu_iomap_unload_map(struct iommu_state *is, struct iommu_map_state *ims)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct strbuf_ctl *sb = ims->ims_sb;
	int i;

	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e)
		iommu_remove(is, sb, e->ipe_va);

	return (0);
}

/*
 * Translate a physical address (pa) into a DVMA address.
 */
vaddr_t
iommu_iomap_translate(struct iommu_map_state *ims, paddr_t pa)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct iommu_page_entry pe;
	paddr_t offset = pa & PAGE_MASK;

	pe.ipe_pa = trunc_page(pa);

	e = SPLAY_FIND(iommu_page_tree, &ipm->ipm_tree, &pe);

	if (e == NULL)
		return (NULL);

	return (e->ipe_va | offset);
}

/*
 * Clear the iomap table and tree.
 */
void
iommu_iomap_clear_pages(struct iommu_map_state *ims)
{
	ims->ims_map.ipm_pagecnt = 0;
	SPLAY_INIT(&ims->ims_map.ipm_tree);
}

@


1.6
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.5 2001/10/15 03:36:16 jason Exp $	*/
d227 1
a227 1
	pmap_update();
d1117 1
a1117 1
	pmap_update();
d1143 1
a1143 1
	pmap_update();
@


1.5
log
@Pull in several changes from NetBSD:
- Clear the dmamap inside splhigh() protection.
- Manage both streaming caches on psycho/psycho+.
And fix a botch I created when merging iommu changes last time.
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.4 2001/09/26 19:31:57 jason Exp $	*/
a121 1
#include <vm/vm.h>
@


1.5.4.1
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.5.4.2
log
@Merge in -current
@
text
@d122 1
@


1.5.4.3
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: iommu.c,v 1.47 2002/02/08 20:03:45 eeh Exp $	*/
a4 1
 * Copyright (c) 2001, 2002 Eduardo Horvath
d32 81
a148 3
int iommu_dvmamap_sync_seg(bus_dma_tag_t, struct iommu_state *,
    bus_dma_segment_t *, bus_addr_t, bus_size_t, int);

a162 1
static	int iommu_tv_comp(struct timeval *, struct timeval *);
d187 2
a188 8
	 * The sun4u iommu is part of the SBUS or PCI controller so we will
	 * deal with it here..
	 *
	 * For sysio and psycho/psycho+ the IOMMU address space always ends at
	 * 0xffffe000, but the starting address depends on the size of the
	 * map.  The map size is 1024 * 2 ^ is->is_tsbsize entries, where each
	 * entry is 8 bytes.  The start of the map can be calculated by
	 * (0xffffe000 << (8 + is->is_tsbsize)).
d190 4
a193 3
	 * But sabre and hummingbird use a different scheme that seems to
	 * be hard-wired, so we read the start and size from the PROM and
	 * just use those values.
d197 2
a198 7
	if (iovabase == -1) {
		is->is_dvmabase = IOTSB_VSTART(is->is_tsbsize);
		is->is_dvmaend = IOTSB_VEND;
	} else {
		is->is_dvmabase = iovabase;
		is->is_dvmaend = iovabase + IOTSB_VSIZE(tsbsize);
	}
d227 1
a227 1
	pmap_update(pmap_kernel());
d261 3
a263 3
	printf("IOTDB: %llx to %llx\n", 
	    (unsigned long long)is->is_ptsb,
	    (unsigned long long)(is->is_ptsb + size));
d265 2
a266 2
	    is->is_dvmabase, is->is_dvmaend - NBPG,
	    M_DEVBUF, 0, 0, EX_NOWAIT);
a277 2
	struct iommu_strbuf *sb;
	int i;
d288 26
a313 13
	for (i=0; i<2; i++) {
		if ((sb = is->is_sb[i]) != NULL) {
			/* Enable diagnostics mode? */
			bus_space_write_8(is->is_bustag,
			    (bus_space_handle_t)(u_long)&sb->strbuf_ctl,
			    0, STRBUF_EN);

			/* No streaming buffers? Disable them */
			if (bus_space_read_8(is->is_bustag,
			    (bus_space_handle_t)(u_long)&sb->strbuf_ctl,
				0) == 0)
				is->is_sb[i] = 0;
		}
d330 1
a330 1
	if (va < is->is_dvmabase || va > is->is_dvmaend)
d336 1
a336 2
tte |= (flags & 0xff000LL)<<(4*8);/* DEBUG */

d364 1
a364 1
	if (dva >= is->is_dvmabase && dva < is->is_dvmaend)
d385 1
d387 1
a387 1
	if (va < is->is_dvmabase || va > is->is_dvmaend)
d424 1
a424 2
		/* XXX Zero-ing the entry would not require RMW */
		is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)] &= ~IOTTE_V;
a430 16
static int
iommu_tv_comp(t1, t2)
	struct timeval *t1, *t2;
{
	if (t1->tv_sec < t2->tv_sec)
		return (-1);
	if (t1->tv_sec > t2->tv_sec)
		return (1);
	/* t1->tv_sec == t2->tv_sec */
	if (t1->tv_usec < t2->tv_usec)
		return (-1);
	if (t1->tv_usec > t2->tv_usec)
		return (1);
	return (0);
}

d437 11
a462 3
	is->is_flush[0] = (is->is_sb[0] == NULL) ? 1 : 0;
	is->is_flush[1] = (is->is_sb[1] == NULL) ? 1 : 0;
	membar_memissue();
d464 2
d467 1
a469 3
		bus_space_barrier(is->is_bustag, (bus_space_handle_t)(u_long)
		    &is->is_sb[0]->strbuf_flushsync, 0, sizeof(u_int64_t),
		    BUS_SPACE_BARRIER_WRITE);
a470 1

d472 1
a474 3
		bus_space_barrier(is->is_bustag, (bus_space_handle_t)(u_long)
		    &is->is_sb[1]->strbuf_flushsync, 0, sizeof(u_int64_t),
		    BUS_SPACE_BARRIER_WRITE);
d477 3
a479 7
	microtime(&cur);
	flushtimeout.tv_usec = cur.tv_usec + 500000; /* 1/2 sec */
	if (flushtimeout.tv_usec >= 1000000) {
		flushtimeout.tv_usec -= 1000000;
		flushtimeout.tv_sec = cur.tv_sec + 1;
	} else
		flushtimeout.tv_sec = cur.tv_sec;
a484 1

d486 4
a489 3
	while (((ldxa(is->is_flushpa, ASI_PHYS_CACHED) == 0) ||
	        (ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED) == 0)) &&
	       (iommu_tv_comp(&cur, &flushtimeout) <= 0)) {
a490 1
	}
d493 2
a494 2
	if (((is->is_sb[0] != NULL) && (ldxa(is->is_flushpa, ASI_PHYS_CACHED) == 0)) ||
	    ((is->is_sb[1] != NULL) && (ldxa(is->is_flushpa + 8, ASI_PHYS_CACHED) == 0))) {
d603 1
a603 2
			(long)map->dm_segs[seg].ds_addr, 
			map->dm_segs[seg].ds_len));
d624 1
a624 1
		(long)map->dm_segs[seg].ds_addr, map->dm_segs[seg].ds_len));
d655 1
a655 1
		    flags|0x4000);
a660 13
#ifdef DIAGNOSTIC
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
		    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
			printf("seg %d dvmaddr %lx out of range %x - %x\n",
			    seg, (long)map->dm_segs[seg].ds_addr,
			    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
			Debugger();
#endif
		}
	}
#endif
a724 1
	int npg = 0; /* DEBUG */
d741 1
a741 1
	align = max(segs[0]._ds_align, pagesz);
d763 1
a763 1
	 * split the transfer up into little pieces ourselves.
a794 3
		paddr_t prev_pa = 0;
		int end = 0, offset;

d796 2
a797 2
		 * This segs is made up of individual physical
		 * segments, probably by _bus_dmamap_load_uio() or
d799 1
a799 1
		 * load each one individually.
d803 50
a852 23
		j = 0;
		for (i = 0; i < nsegs; i++) {
			pa = segs[i].ds_addr;
			offset = (pa & PGOFSET);
			pa = trunc_page(pa);
			dvmaddr = trunc_page(dvmaddr);
			left = min(size, segs[i].ds_len);

			DPRINTF(IDB_INFO, ("iommu_dvamap_load_raw: converting "
			    "physseg %d start %lx size %lx\n", i,
			    (long)segs[i].ds_addr, segs[i].ds_len));

			if ((pa == prev_pa) &&
			    ((offset != 0) || (end != offset))) {
				/* We can re-use this mapping */
#ifdef DEBUG
if (iommudebug & 0x10) printf("reusing dva %lx prev %lx pa %lx prev %lx\n",
    dvmaddr, prev_va, pa prev_pa);
#endif
				dvmaddr = prev_va; 
			}
			sgstart = dvmaddr + offset;
			sgend = sgstart + left - 1;
d854 3
a856 10
			/* Are the segments virtually adjacent? */
			if ((j > 0) && (end == offset) &&
			    ((offset = 0) || (pa == prev_pa))) {
				/* Just append to the previous segment. */
#ifdef DEBUG
if (iommudebug & 0x10) {
printf("appending offset %x pa %lx, prev %lx dva %lx prev %lx\n",
    offset, pa, prev_pa,l dvmaddr, prev_va);
}
#endif
a857 12
				map->dm_segs[--j].ds_len += left; 
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
				    "appending seg %d start %lx size %lx\n", j,
				    (long)map->dm_segs[j].ds_addr,
				    map->dm_segs[j].ds_len));
			} else {
				map->dm_segs[j].ds_addr = sgstart;
				map->dm_segs[j].ds_len = left;
				DPRINTF(IDB_INFO, ("iommu_dvmamap_load_raw: "
				    "seg %d start %lx size %lx\n", j,
				    (long)map->dm_segs[j].ds_addr,
				    map->dm_segs[j].ds_len));
a858 1
			end = (offset + left) & PGOFSET;
d868 1
a868 1
					(long)map->dm_segs[j].ds_addr, 
d871 1
d883 12
a894 24
			/* Now map a series of pages. */
			while (dvmaddr < sgend) {
				DPRINTF(IDB_BUSDMA,
				    ("iommu_dvamap_load_raw: map %p "
				    "loading va %lx at pa %lx\n",
				    map, (long)dvmaddr,
				    (long)(pa)));
				/* Enter if if we haven't before. */
				if (prev_va != dvmaddr)
#ifdef DEBUG
{ if (iommudebug & 0x10) printf("seg %d:5d entering dvma %lx, prev %lx pa %lx\n", i, j, dvmaddr, prev_va, pa);
#endif
					 iommu_enter(is, prev_va = dvmaddr,
					     prev_pa = pa, flags|(++npg<<12)); 
#ifdef DEBUG
} else if (iommudebug & 0x10) printf("seg %d:%d skipping dvma %lx, prev %lx\n", i, j, dvmaddr, prev_va);
#endif

				dvmaddr += pagesz;
				pa += pagesz;
			}

			size -= left;
			++j;
d896 4
a899 18

		map->dm_nsegs = j;
#ifdef DIAGNOSTIC
		{
			int seg;
			for (seg = 0; seg < map->dm_nsegs; seg++) {
				if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
				    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
					printf("seg %d dvmaddr %lx out of range %x - %x\n",
					    seg, (long)map->dm_segs[seg].ds_addr,
					    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
					Debugger();
#endif
				}
			}
		}
#endif
d916 1
a916 1
			(long)map->dm_segs[i].ds_addr,
d934 1
a934 1
			(long)map->dm_segs[i].ds_addr, map->dm_segs[i].ds_len));
d945 1
a945 1
		iommu_enter(is, dvmaddr, pa, flags|0x8000);
a951 16
#ifdef DIAGNOSTIC
	{
		int seg;
		for (seg = 0; seg < map->dm_nsegs; seg++) {
			if (map->dm_segs[seg].ds_addr < is->is_dvmabase ||
			    map->dm_segs[seg].ds_addr > is->is_dvmaend) {
				printf("seg %d dvmaddr %lx out of range %x - %x\n",
				    seg, (long)map->dm_segs[seg].ds_addr,
				    is->is_dvmabase, is->is_dvmaend);
#ifdef DDB
				Debugger();
#endif
			}
		}
       }
#endif
d964 1
a964 2
	bus_size_t count;
	int i, needsflush = 0;
d966 4
a969 39
	for (i = 0; i < map->dm_nsegs; i++) {
		if (offset < map->dm_segs[i].ds_len)
			break;
		offset -= map->dm_segs[i].ds_len;
	}

	if (i == map->dm_nsegs)
		panic("iommu_dvmamap_sync: too short %lu", offset);

	for (; len > 0 && i < map->dm_nsegs; i++) {
		count = min(map->dm_segs[i].ds_len - offset, len);
		needsflush += iommu_dvmamap_sync_seg(t, is, &map->dm_segs[i],
		    offset, count, ops);
		len -= count;
	}

	if (i == map->dm_nsegs && len > 0)
		panic("iommu_dvmamap_sync: leftover %lu", len);

	if (needsflush)
		iommu_strbuf_flush_done(is);
}

int
iommu_dvmamap_sync_seg(t, is, seg, offset, len, ops)
	bus_dma_tag_t t;
	struct iommu_state *is;
	bus_dma_segment_t *seg;
	bus_addr_t offset;
	bus_size_t len;
	int ops;
{
	int needsflush = 0;
	vaddr_t va = seg->ds_addr + offset;

	if (len == 0)
		goto out;

	len += offset & PGOFSET;
d973 1
a973 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
a977 1

d980 1
a980 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
d986 1
a986 1
				    ("iommu_dvmamap_sync_seg: flushing va %p, %lu "
d990 1
a990 1
					needsflush = 1;
d999 1
a999 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
d1005 1
a1005 1
				    ("iommu_dvmamap_sync_seg: flushing va %p, %lu "
d1009 1
a1009 1
					needsflush = 1;
d1018 1
a1018 1
		    ("iommu_dvmamap_sync_seg: syncing va %p len %lu "
a1021 3

out:
	return (needsflush);
d1117 1
a1117 1
	pmap_update(pmap_kernel());
d1143 6
a1148 1
	pmap_update(pmap_kernel());
d1150 1
@


1.5.4.4
log
@Merge in -current from about a week ago
@
text
@d85 1
a85 2
static	int iommu_strbuf_flush_done(struct iommu_state *);
int64_t iommu_tsb_entry(struct iommu_state *, vaddr_t);
a296 21
 * Fetch a tsb entry with some sanity checking.
 */
int64_t
iommu_tsb_entry(is, dva)
	struct iommu_state *is;
	vaddr_t dva;
{
	int64_t tte;

	if (dva < is->is_dvmabase && dva >= is->is_dvmaend)
		panic("invalid dva: %llx", (long long)dva);

	tte = is->is_tsb[IOTSBSLOT(dva,is->is_tsbsize)];

	if ((tte & IOTTE_V) == 0)
		panic("iommu_tsb_entry: invalid entry %llx\n", (long long)dva);

	return (tte);
}

/*
d545 1
a545 2
		map->dm_segs[seg].ds_len =
		    boundary - (sgstart & (boundary - 1));
a981 4
/*
 * Flush an individual dma segment, returns non-zero if the streaming buffers
 * need flushing afterwards.
 */
d1017 2
a1018 2
				if (iommu_tsb_entry(is, va) & IOTTE_STREAM) {
					iommu_strbuf_flush(is, va);
a1019 2
				}
				if (len <= NBPG)
d1021 1
a1021 1
				else
d1036 2
a1037 2
				if (iommu_tsb_entry(is, va) & IOTTE_STREAM) {
					iommu_strbuf_flush(is, va);
a1038 2
				}
				if (len <= NBPG)
d1040 1
a1040 1
				else
@


1.5.4.5
log
@Sync the SMP branch with 3.3
@
text
@a4 1
 * Copyright (c) 2003 Henric Jungheim
a40 1
#include <sys/mbuf.h>
d62 1
a62 4
#define IDB_SYNC	0x8
#define IDB_XXX		0x10
#define IDB_PRINT_MAP	0x20
#define IDB_BREAK	0x40
d69 2
a70 25
void iommu_enter(struct iommu_state *, struct strbuf_ctl *, vaddr_t, paddr_t,
    int);
void iommu_remove(struct iommu_state *, struct strbuf_ctl *, vaddr_t);
int iommu_dvmamap_sync_range(struct strbuf_ctl*, vaddr_t, bus_size_t);
int iommu_strbuf_flush_done(struct iommu_map_state *);
int iommu_dvmamap_load_seg(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t, bus_dma_segment_t *, int, int, bus_size_t, bus_size_t);
int iommu_dvmamap_load_mlist(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t, struct pglist *, int, bus_size_t, bus_size_t);
int iommu_dvmamap_validate_map(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t);
void iommu_dvmamap_print_map(bus_dma_tag_t, struct iommu_state *,
    bus_dmamap_t);
int iommu_dvmamap_append_range(bus_dma_tag_t, bus_dmamap_t, paddr_t,
    bus_size_t, int, bus_size_t);
int64_t iommu_tsb_entry(struct iommu_state *, vaddr_t);
void strbuf_reset(struct strbuf_ctl *);
int iommu_iomap_insert_page(struct iommu_map_state *, paddr_t);
vaddr_t iommu_iomap_translate(struct iommu_map_state *, paddr_t);
int iommu_iomap_load_map(struct iommu_state *, struct iommu_map_state *,
    vaddr_t, int);
int iommu_iomap_unload_map(struct iommu_state *, struct iommu_map_state *);
struct iommu_map_state *iommu_iomap_create(int);
void iommu_iomap_destroy(struct iommu_map_state *);
void iommu_iomap_clear_pages(struct iommu_map_state *);
d72 12
a83 12
/*
 * Initiate an STC entry flush.
 */
static inline void
iommu_strbuf_flush(struct strbuf_ctl *sb, vaddr_t va)
{
#ifdef DEBUG
	if (sb->sb_flush == NULL) {
		printf("iommu_strbuf_flush: attempting to flush w/o STC\n");
		return;
	}
#endif
d85 3
a87 3
	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_pgflush), va);
}
d97 5
a101 1
iommu_init(char *name, struct iommu_state *is, int tsbsize, u_int32_t iovabase)
d140 1
a140 1
	size = PAGE_SIZE << is->is_tsbsize;
d143 1
a143 1
		(paddr_t)PAGE_SIZE, (paddr_t)0, &mlist, 1, 0) != 0)
d160 1
a160 1
		va += PAGE_SIZE;
d163 1
a163 1
	memset(is->is_tsb, 0, size);
d166 2
a167 1
	if (iommudebug & IDB_INFO) {
d169 2
a170 1
		/* The address or contents of the regs...? */
d172 5
a176 11
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_cr),
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_tsb),
		    (u_long)bus_space_vaddr(is->is_bustag, is->is_iommu) +
			IOMMUREG(iommu_flush));
		printf("iommu cr=%llx tsb=%llx\n",
		    IOMMUREG_READ(is, iommu_cr),
		    IOMMUREG_READ(is, iommu_tsb));
		printf("TSB base %p phys %llx\n",
		    (void *)is->is_tsb, (unsigned long long)is->is_ptsb);
d182 7
a195 1
	printf("DVMA map: %x to %x\n", is->is_dvmabase, is->is_dvmaend);
d200 1
a200 1
	    is->is_dvmabase, is->is_dvmaend - PAGE_SIZE,
d205 1
a205 1
 * Streaming buffers don't exist on the UltraSPARC IIi/e; we should have
d210 2
a211 1
iommu_reset(struct iommu_state *is)
d213 1
d216 21
a236 19
	IOMMUREG_WRITE(is, iommu_tsb, is->is_ptsb);

	/* Enable IOMMU */
	IOMMUREG_WRITE(is, iommu_cr, is->is_cr);

	for (i = 0; i < 2; ++i) {
		struct strbuf_ctl *sb = is->is_sb[i];

		if (sb == NULL)
			continue;

		sb->sb_iommu = is;
		strbuf_reset(sb);


		if (sb->sb_flush) {
			char buf[64];
			bus_space_render_tag(sb->sb_bustag, buf, sizeof buf);
			printf("STC%d on %s enabled\n", i, buf);
d242 1
a242 1
 * Inititalize one STC.
d245 5
a249 33
strbuf_reset(struct strbuf_ctl *sb)
{
	if(sb->sb_flush == NULL)
		return;

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_ctl), STRBUF_EN);

	membar(Lookaside);

	/* No streaming buffers? Disable them */
	if (bus_space_read_8(sb->sb_bustag, sb->sb_sb,
	    STRBUFREG(strbuf_ctl)) == 0) {
		sb->sb_flush = NULL;
	} else {
		/*
		 * locate the pa of the flush buffer
		 */
		if (pmap_extract(pmap_kernel(),
		    (vaddr_t)sb->sb_flush, &sb->sb_flushpa) == FALSE)
			sb->sb_flush = NULL;
	}
}

/*
 * Add an entry to the IOMMU table.
 *
 * The entry is marked streaming if an STC was detected and 
 * the BUS_DMA_STREAMING flag is set.
 */
void
iommu_enter(struct iommu_state *is, struct strbuf_ctl *sb, vaddr_t va,
    paddr_t pa, int flags)
a251 1
	volatile int64_t *tte_ptr = &is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)];
d254 1
a254 2
	if (va < is->is_dvmabase || round_page(va + PAGE_SIZE) >
	    is->is_dvmaend + 1)
d256 1
d258 8
a265 7
	tte = *tte_ptr;

	if (tte & IOTTE_V) {
		printf("Overwriting valid tte entry (dva %lx pa %lx "
		    "&tte %p tte %llx)\n", va, pa, tte_ptr, tte);
		extent_print(is->is_dvmamap);
		panic("IOMMU overwrite");
a266 5
#endif

	tte = MAKEIOTTE(pa, !(flags & BUS_DMA_NOWRITE),
	    !(flags & BUS_DMA_NOCACHE), (flags & BUS_DMA_STREAMING));

d268 4
a271 19
	    (int)IOTSBSLOT(va,is->is_tsbsize), (void *)(u_long)va));

	*tte_ptr = tte;

	/*
	 * Why bother to flush this va?  It should only be relevant for
	 * V ==> V or V ==> non-V transitions.  The former is illegal and
	 * the latter is never done here.  It is true that this provides
	 * some protection against a misbehaving master using an address
	 * after it should.  The IOMMU documentations specifically warns
	 * that the consequences of a simultaneous IOMMU flush and DVMA
	 * access to the same address are undefined.  (By that argument,
	 * the STC should probably be flushed as well.)   Note that if
	 * a bus master keeps using a memory region after it has been
	 * unmapped, the specific behavior of the IOMMU is likely to
	 * be the least of our worries.
	 */
	IOMMUREG_WRITE(is, iommu_flush, va);

d273 3
a275 3
	    va, (long)pa, (u_long)IOTSBSLOT(va,is->is_tsbsize), 
	    (void *)(u_long)&is->is_tsb[IOTSBSLOT(va,is->is_tsbsize)],
	    (u_long)tte));
a277 53
/*
 * Remove an entry from the IOMMU table.
 *
 * The entry is flushed from the STC if an STC is detected and the TSB
 * entry has the IOTTE_STREAM flags set.  It should be impossible for
 * the TSB entry to have this flag set without the BUS_DMA_STREAMING
 * flag, but better to be safe.  (The IOMMU will be ignored as long
 * as an STC entry exists.)
 */
void
iommu_remove(struct iommu_state *is, struct strbuf_ctl *sb, vaddr_t va)
{
	int64_t *tte_ptr = &is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)];
	int64_t tte;

#ifdef DIAGNOSTIC
	if (trunc_page(va) < is->is_dvmabase || round_page(va) >
	    is->is_dvmaend + 1)
		panic("iommu_remove: va 0x%lx not in DVMA space", (u_long)va);
	if (va != trunc_page(va)) {
		printf("iommu_remove: unaligned va: %lx\n", va);
		va = trunc_page(va);
	}
#endif
	tte = *tte_ptr;

	DPRINTF(IDB_IOMMU, ("iommu_remove: va %lx TSB[%llx]@@%p\n",
	    va, tte, tte_ptr));

#ifdef DIAGNOSTIC
	if ((tte & IOTTE_V) == 0) {
		printf("Removing invalid tte entry (dva %lx &tte %p "
		    "tte %llx)\n", va, tte_ptr, tte);
		extent_print(is->is_dvmamap);
		panic("IOMMU remove overwrite");
	}
#endif

	*tte_ptr = tte & ~IOTTE_V;

	/*
	 * IO operations are strongly ordered WRT each other.  It is
	 * unclear how they relate to normal memory accesses.
	 */
	membar(StoreStore);

	IOMMUREG_WRITE(is, iommu_flush, va);

	if (sb && (tte & IOTTE_STREAM))
		iommu_strbuf_flush(sb, va);

	/* Should we sync the iommu and stc here? */
}
d280 1
a280 1
 * Find the physical address of a DVMA address (debug routine).
d283 3
a285 1
iommu_extract(struct iommu_state *is, vaddr_t dva)
d289 2
a290 2
	if (dva >= is->is_dvmabase && dva <= is->is_dvmaend)
		tte = is->is_tsb[IOTSBSLOT(dva, is->is_tsbsize)];
d292 3
a294 1
	return (tte & IOTTE_PAMASK);
d298 1
a298 1
 * Lookup a TSB entry for a given DVMA (debug routine).
d301 3
a303 1
iommu_lookup_tte(struct iommu_state *is, vaddr_t dva)
d305 4
a308 4
	int64_t tte = 0;
	
	if (dva >= is->is_dvmabase && dva <= is->is_dvmaend)
		tte = is->is_tsb[IOTSBSLOT(dva, is->is_tsbsize)];
d310 1
a310 2
	return (tte);
}
d312 2
a313 11
/*
 * Lookup a TSB entry at a given physical address (debug routine).
 */
int64_t
iommu_fetch_tte(struct iommu_state *is, paddr_t pa)
{
	int64_t tte = 0;
	
	if (pa >= is->is_ptsb && pa < is->is_ptsb +
	    (PAGE_SIZE << is->is_tsbsize)) 
		tte = ldxa(pa, ASI_PHYS_CACHED);
d319 5
a323 1
 * Fetch a TSB entry with some sanity checking.
d325 5
a329 2
int64_t
iommu_tsb_entry(struct iommu_state *is, vaddr_t dva)
d331 32
a362 1
	int64_t tte;
d364 4
a367 2
	if (dva < is->is_dvmabase && dva > is->is_dvmaend)
		panic("invalid dva: %llx", (long long)dva);
d369 7
a375 1
	tte = is->is_tsb[IOTSBSLOT(dva,is->is_tsbsize)];
d377 14
a390 4
	if ((tte & IOTTE_V) == 0)
		panic("iommu_tsb_entry: invalid entry %lx", dva);

	return (tte);
d393 3
a395 5
/*
 * Initiate and then block until an STC flush synchronization has completed.
 */
int 
iommu_strbuf_flush_done(struct iommu_map_state *ims)
a396 2
	struct strbuf_ctl *sb = ims->ims_sb;
	struct strbuf_flush *sf = &ims->ims_flush;
a397 9
	struct timeval to = { 0, 500000 };
	u_int64_t flush;
	int timeout_started = 0;

#ifdef DIAGNOSTIC
	if (sb == NULL) {
		panic("iommu_strbuf_flush_done: invalid flush buffer");
	}
#endif
d399 3
d405 2
a406 1
	 *   1 Tell strbuf to flush by storing va to strbuf_pgflush.
d411 2
a412 1
	 * If it takes more than .5 sec, something went very, very wrong.
d414 37
a450 46

	/*
	 * If we're reading from ASI_PHYS_CACHED, then we'll write to
	 * it too.  No need to tempt fate or learn about Si bugs or such.
	 * FreeBSD just uses normal "volatile" reads/writes...
	 */

	stxa(sf->sbf_flushpa, ASI_PHYS_CACHED, 0);

	/*
	 * Insure any previous strbuf operations are complete and that 
	 * memory is initialized before the IOMMU uses it.
	 * Is this Needed?  How are IO and memory operations ordered? 
	 */
	membar(StoreStore);

	bus_space_write_8(sb->sb_bustag, sb->sb_sb,
		    STRBUFREG(strbuf_flushsync), sf->sbf_flushpa);

	DPRINTF(IDB_IOMMU,
	    ("iommu_strbuf_flush_done: flush = %llx pa = %lx\n", 
		ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED), sf->sbf_flushpa));

	membar(StoreLoad | Lookaside);

	for(;;) {
		int i;

		/*
		 * Try to shave a few instruction cycles off the average
		 * latency by only checking the elapsed time every few
		 * fetches.
		 */
		for (i = 0; i < 1000; ++i) {
			membar(LoadLoad);
			/* Bypass non-coherent D$ */
			/* non-coherent...?   Huh? */
			flush = ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED);

			if (flush) {
				DPRINTF(IDB_IOMMU,
				    ("iommu_strbuf_flush_done: flushed\n"));
				return (0);
			}
		}

d452 1
d454 10
a463 16
		if (timeout_started) {
			if (timercmp(&cur, &flushtimeout, >))
				panic("STC timeout at %lx (%lld)",
				    sf->sbf_flushpa, flush);
		} else {
			timeradd(&cur, &to, &flushtimeout);
			
			timeout_started = 1;
	
			DPRINTF(IDB_IOMMU,
			    ("iommu_strbuf_flush_done: flush = %llx pa = %lx "
				"now=%lx:%lx until = %lx:%lx\n", 
				ldxa(sf->sbf_flushpa, ASI_PHYS_CACHED),
				sf->sbf_flushpa, cur.tv_sec, cur.tv_usec, 
				flushtimeout.tv_sec, flushtimeout.tv_usec));
		}
d465 3
d474 3
a476 5
iommu_dvmamap_create(bus_dma_tag_t t, struct iommu_state *is,
    struct strbuf_ctl *sb, bus_size_t size, int nsegments, bus_size_t maxsegsz,
    bus_size_t boundary, int flags, bus_dmamap_t *dmamap)
{
	int ret;
d478 4
a481 51
	struct iommu_map_state *ims;

	ret = bus_dmamap_create(t->_parent, size, nsegments, maxsegsz,
	    boundary, flags, &map);

	if (ret)
		return (ret);

	ims = iommu_iomap_create(nsegments);

	if (ims == NULL) {
		bus_dmamap_destroy(t->_parent, map);
		return (ENOMEM);
	}

	ims->ims_sb = sb;
	map->_dm_cookie = ims;
	*dmamap = map;

	return (0);
}

void
iommu_dvmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{
	/*
	 * The specification (man page) requires a loaded
	 * map to be unloaded before it is destroyed.
	 */
	if (map->dm_nsegs)
		bus_dmamap_unload(t, map);

        if (map->_dm_cookie)
                iommu_iomap_destroy(map->_dm_cookie);
	map->_dm_cookie = NULL;

	bus_dmamap_destroy(t->_parent, map);
}

/*
 * Load a contiguous kva buffer into a dmamap.  The physical pages are
 * not assumed to be contiguous.  Two passes are made through the buffer
 * and both call pmap_extract() for the same va->pa translations.  It
 * is possible to run out of pa->dvma mappings; the code should be smart
 * enough to resize the iomap (when the "flags" permit allocation).  It
 * is trivial to compute the number of entries required (round the length
 * up to the page size and then divide by the page size)...
 */
int
iommu_dvmamap_load(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map,
    void *buf, bus_size_t buflen, struct proc *p, int flags)
d484 1
a484 1
	int err = 0;
d486 1
d489 2
a490 1
	struct iommu_map_state *ims = map->_dm_cookie;
a492 5
#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_load: null map state");
#endif

d494 1
a494 4
		/*
		 * Is it still in use? _bus_dmamap_load should have taken care
		 * of this.
		 */
d496 1
a496 1
		panic("iommu_dvmamap_load: map still in use");
a499 1

d505 1
a505 1
	if (buflen < 1 || buflen > map->_dm_size) {
d512 2
d520 2
a521 43
	align = max(map->dm_segs[0]._ds_align, PAGE_SIZE);

	pmap = p ? p->p_vmspace->vm_map.pmap : pmap = pmap_kernel();

	/* Count up the total number of pages we need */
	iommu_iomap_clear_pages(ims);
	{ /* Scope */
		bus_addr_t a, aend;
		bus_addr_t addr = (vaddr_t)buf;
		int seg_len = buflen;

		aend = round_page(addr + seg_len - 1);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			paddr_t pa;

			if (pmap_extract(pmap, a, &pa) == FALSE) {
				printf("iomap pmap error addr 0x%llx\n", a);
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}

			err = iommu_iomap_insert_page(ims, pa);
			if (err) {
				printf("iomap insert error: %d for "
				    "va 0x%llx pa 0x%lx "
				    "(buf %p len %lld/%llx)\n",
				    err, a, pa, buf, buflen, buflen);
				iommu_dvmamap_print_map(t, is, map);
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}
		}
	}
	sgsize = ims->ims_map.ipm_pagecnt * PAGE_SIZE;

	if (flags & BUS_DMA_24BIT) {
		sgstart = max(is->is_dvmamap->ex_start, 0xff000000);
		sgend = min(is->is_dvmamap->ex_end, 0xffffffff);
	} else {
		sgstart = is->is_dvmamap->ex_start;
		sgend = is->is_dvmamap->ex_end;
	}

d524 1
a524 1
	 * split the transfer up into little pieces ourselves.
d526 3
a528 4
	s = splhigh();
	err = extent_alloc_subregion(is->is_dvmamap, sgstart, sgend,
	    sgsize, align, 0, (sgsize > boundary) ? 0 : boundary, 
	    EX_NOWAIT | EX_BOUNDZERO, (u_long *)&dvmaddr);
d532 2
a533 1
	if (err || (dvmaddr == (bus_addr_t)-1))	{ 
d537 1
a537 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d551 39
d592 4
a595 3
#ifdef DEBUG
	iommu_dvmamap_validate_map(t, is, map);
#endif
d597 8
a604 2
	if (iommu_iomap_load_map(is, ims, dvmaddr, flags))
		return (E2BIG);
d606 6
a611 18
	{ /* Scope */
		bus_addr_t a, aend;
		bus_addr_t addr = (vaddr_t)buf;
		int seg_len = buflen;

		aend = round_page(addr + seg_len - 1);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
			bus_addr_t pgstart;
			bus_addr_t pgend;
			paddr_t pa;
			int pglen;

			/* Yuck... Redoing the same pmap_extract... */
			if (pmap_extract(pmap, a, &pa) == FALSE) {
				printf("iomap pmap error addr 0x%llx\n", a);
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}
d613 22
a634 17
			pgstart = pa | (max(a, addr) & PAGE_MASK);
			pgend = pa | (min(a + PAGE_SIZE - 1,
			    addr + seg_len - 1) & PAGE_MASK);
			pglen = pgend - pgstart + 1;

			if (pglen < 1)
				continue;

			err = iommu_dvmamap_append_range(t, map, pgstart,
			    pglen, flags, boundary);
			if (err) {
				printf("iomap load seg page: %d for "
				    "va 0x%llx pa %lx (%llx - %llx) "
				    "for %d/0x%x\n",
				    err, a, pa, pgstart, pgend, pglen, pglen);
				return (err);
			}
d637 3
a640 3
#ifdef DIAGNOSTIC
	iommu_dvmamap_validate_map(t, is, map);
#endif
d642 10
d653 2
a654 6
	if (err)
		printf("**** iommu_dvmamap_load failed with error %d\n",
		    err);
	
	if (err || (iommudebug & IDB_PRINT_MAP)) {
		iommu_dvmamap_print_map(t, is, map);
d656 1
a656 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d660 17
d678 1
a678 1
	return (err);
d681 1
a681 12
/*
 * Load a dvmamap from an array of segs or an mlist (if the first
 * "segs" entry's mlist is non-null).  It calls iommu_dvmamap_load_segs()
 * or iommu_dvmamap_load_mlist() for part of the 2nd pass through the
 * mapping.  This is ugly.  A better solution would probably be to have
 * function pointers for implementing the traversal.  That way, there
 * could be one core load routine for each of the three required algorithms
 * (buffer, seg, and mlist).  That would also mean that the traversal
 * algorithm would then only need one implementation for each algorithm
 * instead of two (one for populating the iomap and one for populating
 * the dvma map).
 */
d683 8
a690 3
iommu_dvmamap_load_raw(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, bus_dma_segment_t *segs, int nsegs, int flags,
    bus_size_t size)
d692 2
a693 1
	int i, s;
d695 1
a695 1
	int err = 0;
d697 1
d700 3
a702 6
	struct iommu_map_state *ims = map->_dm_cookie;

#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_load_raw: null map state");
#endif
d707 1
a707 1
		panic("iommu_dvmamap_load_raw: map still in use");
d719 1
a719 1
	align = max(segs[0]._ds_align, PAGE_SIZE);
d725 12
d738 9
a746 59
	iommu_iomap_clear_pages(ims);
	if (segs[0]._ds_mlist) {
		struct pglist *mlist = segs[0]._ds_mlist;
		struct vm_page *m;
		for (m = TAILQ_FIRST(mlist); m != NULL;
		    m = TAILQ_NEXT(m,pageq)) {
			err = iommu_iomap_insert_page(ims, VM_PAGE_TO_PHYS(m));

			if(err) {
				printf("iomap insert error: %d for "
				    "pa 0x%lx\n", err, VM_PAGE_TO_PHYS(m));
				iommu_iomap_clear_pages(ims);
				return (E2BIG);
			}
		}
	} else {
		/* Count up the total number of pages we need */
		for (i = 0, left = size; left > 0 && i < nsegs; i++) {
			bus_addr_t a, aend;
			bus_size_t len = segs[i].ds_len;
			bus_addr_t addr = segs[i].ds_addr;
			int seg_len = min(left, len);

			if (len < 1)
				continue;

			aend = round_page(addr + seg_len - 1);
			for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {

				err = iommu_iomap_insert_page(ims, a);
				if (err) {
					printf("iomap insert error: %d for "
					    "pa 0x%llx\n", err, a);
					iommu_iomap_clear_pages(ims);
					return (E2BIG);
				}
			}

			left -= seg_len;
		}
	}
	sgsize = ims->ims_map.ipm_pagecnt * PAGE_SIZE;

	if (flags & BUS_DMA_24BIT) {
		sgstart = max(is->is_dvmamap->ex_start, 0xff000000);
		sgend = min(is->is_dvmamap->ex_end, 0xffffffff);
	} else {
		sgstart = is->is_dvmamap->ex_start;
		sgend = is->is_dvmamap->ex_end;
	}

	/* 
	 * If our segment size is larger than the boundary we need to 
	 * split the transfer up into little pieces ourselves.
	 */
	s = splhigh();
	err = extent_alloc_subregion(is->is_dvmamap, sgstart, sgend,
	    sgsize, align, 0, (sgsize > boundary) ? 0 : boundary, 
	    EX_NOWAIT | EX_BOUNDZERO, (u_long *)&dvmaddr);
d753 4
a756 3
	if (dvmaddr == (bus_addr_t)-1)	{ 
		printf("iommu_dvmamap_load_raw(): extent_alloc(%d, %x) "
		    "failed!\n", (int)sgsize, flags);
d758 1
a758 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d771 12
a782 1
	map->dm_mapsize = size;
d784 15
d800 2
a801 1
	iommu_dvmamap_validate_map(t, is, map);
d803 1
a803 39

	if (iommu_iomap_load_map(is, ims, dvmaddr, flags))
		return (E2BIG);

	if (segs[0]._ds_mlist)
		err = iommu_dvmamap_load_mlist(t, is, map, segs[0]._ds_mlist,
		    flags, size, boundary);
	else
		err = iommu_dvmamap_load_seg(t, is, map, segs, nsegs,
		    flags, size, boundary);

	if (err)
		iommu_iomap_unload_map(is, ims);

#ifdef DIAGNOSTIC
	/* The map should be valid even if the load failed */
	if (iommu_dvmamap_validate_map(t, is, map)) {
		printf("load size %lld/0x%llx\n", size, size);
		if (segs[0]._ds_mlist)
			printf("mlist %p\n", segs[0]._ds_mlist);
		else  {
			long tot_len = 0;
			long clip_len = 0;
			printf("segs %p nsegs %d\n", segs, nsegs);

			left = size;
			for(i = 0; i < nsegs; i++) {
				bus_size_t len = segs[i].ds_len;
				bus_addr_t addr = segs[i].ds_addr;
				int seg_len = min(left, len);

				printf("addr %llx len %lld/0x%llx seg_len "
				    "%d/0x%x left %d/0x%x\n", addr, len, len,
				    seg_len, seg_len, left, left);

				left -= seg_len;
				
				clip_len += seg_len;
				tot_len += segs[i].ds_len;
d805 2
a806 8
			printf("total length %ld/0x%lx total seg. "
			    "length %ld/0x%lx\n", tot_len, tot_len, clip_len,
			    clip_len);
		}

		if (err == 0)
			err = 1;
	}
d808 9
d819 14
a832 13
#ifdef DEBUG
	if (err)
		printf("**** iommu_dvmamap_load_raw failed with error %d\n",
		    err);
	
	if (err || (iommudebug & IDB_PRINT_MAP)) {
		iommu_dvmamap_print_map(t, is, map);
#ifdef DDB
		if (iommudebug & IDB_BREAK)
			Debugger();
#endif
	}
#endif
d834 18
a851 2
	return (err);
}
d853 2
a854 15
/*
 * Insert a range of addresses into a loaded map respecting the specified
 * boundary and alignment restrictions.  The range is specified by its 
 * physical address and length.  The range cannot cross a page boundary.
 * This code (along with most of the rest of the function in this file)
 * assumes that the IOMMU page size is equal to PAGE_SIZE.
 */
int
iommu_dvmamap_append_range(bus_dma_tag_t t, bus_dmamap_t map, paddr_t pa,
    bus_size_t length, int flags, bus_size_t boundary)
{
	struct iommu_map_state *ims = map->_dm_cookie;
	bus_addr_t sgstart, sgend, bd_mask;
	bus_dma_segment_t *seg = NULL;
	int i = map->dm_nsegs;
d856 9
d866 1
a866 14
	if (ims == NULL)
		panic("iommu_dvmamap_append_range: null map state");
#endif

	sgstart = iommu_iomap_translate(ims, pa);
	sgend = sgstart + length - 1;

#ifdef DIAGNOSTIC
	if (sgstart == NULL || sgstart >= sgend) {
		printf("append range invalid mapping for %lx "
		    "(0x%llx - 0x%llx)\n", pa, sgstart, sgend);
		map->dm_nsegs = 0;
		return (EINVAL);
	}
d868 2
a869 1

d871 1
a871 5
	if (trunc_page(sgstart) != trunc_page(sgend)) {
		printf("append range crossing page boundary! "
		    "pa %lx length %lld/0x%llx sgstart %llx sgend %llx\n",
		    pa, length, length, sgstart, sgend);
	}
d874 3
a876 13
	/*
	 * We will attempt to merge this range with the previous entry
	 * (if there is one).
	 */
	if (i > 0) {
		seg = &map->dm_segs[i - 1];
		if (sgstart == seg->ds_addr + seg->ds_len) {
			length += seg->ds_len;
			sgstart = seg->ds_addr;
			sgend = sgstart + length - 1;
		} else
			seg = NULL;
	}
d878 2
a879 7
	if (seg == NULL) {
		seg = &map->dm_segs[i];
		if (++i > map->_dm_segcnt) {
			printf("append range, out of segments (%d)\n", i);
			iommu_dvmamap_print_map(t, NULL, map);
			map->dm_nsegs = 0;
			return (ENOMEM);
a880 1
	}
d882 14
a895 31
	/*
	 * At this point, "i" is the index of the *next* bus_dma_segment_t
	 * (the segment count, aka map->dm_nsegs) and "seg" points to the
	 * *current* entry.  "length", "sgstart", and "sgend" reflect what
	 * we intend to put in "*seg".  No assumptions should be made about
	 * the contents of "*seg".  Only "boundary" issue can change this
	 * and "boundary" is often zero, so explicitly test for that case
	 * (the test is strictly an optimization).
	 */ 
	if (boundary != 0) {
		bd_mask = ~(boundary - 1);

		while ((sgstart & bd_mask) != (sgend & bd_mask)) {
			/*
			 * We are crossing a boundary so fill in the current
			 * segment with as much as possible, then grab a new
			 * one.
			 */

			seg->ds_addr = sgstart;
			seg->ds_len = boundary - (sgstart & bd_mask);

			sgstart += seg->ds_len; /* sgend stays the same */
			length -= seg->ds_len;

			seg = &map->dm_segs[i];
			if (++i > map->_dm_segcnt) {
				printf("append range, out of segments\n");
				iommu_dvmamap_print_map(t, NULL, map);
				map->dm_nsegs = 0;
				return (E2BIG);
d898 2
a900 23

	seg->ds_addr = sgstart;
	seg->ds_len = length;
	map->dm_nsegs = i;

	return (0);
}

/*
 * Populate the iomap from a bus_dma_segment_t array.  See note for
 * iommu_dvmamap_load() * regarding page entry exhaustion of the iomap.
 * This is less of a problem for load_seg, as the number of pages
 * is usually similar to the number of segments (nsegs).
 */
int
iommu_dvmamap_load_seg(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, bus_dma_segment_t *segs, int nsegs, int flags,
    bus_size_t size, bus_size_t boundary)
{
	int i;
	int left;
	int seg;

d902 2
a903 4
	 * This segs is made up of individual physical
	 * segments, probably by _bus_dmamap_load_uio() or
	 * _bus_dmamap_load_mbuf().  Ignore the mlist and
	 * load each one individually.
d905 22
a926 49

	/*
	 * Keep in mind that each segment could span
	 * multiple pages and that these are not always
	 * adjacent. The code is no longer adding dvma
	 * aliases to the IOMMU.  The STC will not cross
	 * page boundaries anyway and a IOMMU table walk
	 * vs. what may be a streamed PCI DMA to a ring
	 * descriptor is probably a wash.  It eases TLB
	 * pressure and in the worst possible case, it is
	 * only as bad a non-IOMMUed architecture.  More
	 * importantly, the code is not quite as hairy.
	 * (It's bad enough as it is.)
	 */
	left = size;
	seg = 0;
	for (i = 0; left > 0 && i < nsegs; i++) {
		bus_addr_t a, aend;
		bus_size_t len = segs[i].ds_len;
		bus_addr_t addr = segs[i].ds_addr;
		int seg_len = min(left, len);

		if (len < 1)
			continue;

		aend = addr + seg_len - 1;
		for (a = trunc_page(addr); a < round_page(aend);
		    a += PAGE_SIZE) {
			bus_addr_t pgstart;
			bus_addr_t pgend;
			int pglen;
			int err;

			pgstart = max(a, addr);
			pgend = min(a + PAGE_SIZE - 1, addr + seg_len - 1);
			pglen = pgend - pgstart + 1;
			
			if (pglen < 1)
				continue;

			err = iommu_dvmamap_append_range(t, map, pgstart,
			    pglen, flags, boundary);
			if (err) {
				printf("iomap load seg page: %d for "
				    "pa 0x%llx (%llx - %llx for %d/%x\n",
				    err, a, pgstart, pgend, pglen, pglen);
				return (err);
			}

d928 2
a929 2

		left -= seg_len;
d931 4
a934 15
	return (0);
}

/*
 * Populate the iomap from an mlist.  See note for iommu_dvmamap_load()
 * regarding page entry exhaustion of the iomap.
 */
int
iommu_dvmamap_load_mlist(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map, struct pglist *mlist, int flags,
    bus_size_t size, bus_size_t boundary)
{
	struct vm_page *m;
	paddr_t pa;
	int err;
a935 4
	/*
	 * This was allocated with bus_dmamem_alloc.
	 * The pages are on an `mlist'.
	 */
d937 2
d941 7
a947 8
		err = iommu_dvmamap_append_range(t, map, pa, PAGE_SIZE,
		    flags, boundary);
		if (err) {
			printf("iomap load seg page: %d for pa 0x%lx "
			    "(%lx - %lx for %d/%x\n", err, pa, pa,
			    pa + PAGE_SIZE, PAGE_SIZE, PAGE_SIZE);
			return (err);
		}
d949 11
a959 19

	return (0);
}

/*
 * Unload a dvmamap.
 */
void
iommu_dvmamap_unload(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map)
{
	struct iommu_map_state *ims = map->_dm_cookie;
	bus_addr_t dvmaddr = map->_dm_dvmastart;
	bus_size_t sgsize = map->_dm_dvmasize;
	int error, s;

	/* Flush the iommu */
#ifdef DEBUG
	if (dvmaddr == 0) {
		printf("iommu_dvmamap_unload: No dvmastart\n");
d961 1
a961 2
		if (iommudebug & IDB_BREAK)
			Debugger();
d963 1
a963 79
		return;
	}
	iommu_dvmamap_validate_map(t, is, map);

	if (iommudebug & IDB_PRINT_MAP)
		iommu_dvmamap_print_map(t, is, map);
#endif /* DEBUG */

	/* Remove the IOMMU entries */
	iommu_iomap_unload_map(is, ims);

	/* Clear the iomap */
	iommu_iomap_clear_pages(ims);

	bus_dmamap_unload(t->_parent, map);

	/* Mark the mappings as invalid. */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	s = splhigh();
	error = extent_free(is->is_dvmamap, dvmaddr, 
		sgsize, EX_NOWAIT);
	map->_dm_dvmastart = 0;
	map->_dm_dvmasize = 0;
	splx(s);
	if (error != 0)
		printf("warning: %qd of DVMA space lost\n", sgsize);
}

/*
 * Perform internal consistency checking on a dvmamap.
 */
int
iommu_dvmamap_validate_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map)
{
	int err = 0;
	int seg;

	if (trunc_page(map->_dm_dvmastart) != map->_dm_dvmastart) {
		printf("**** dvmastart address not page aligned: %llx",
			map->_dm_dvmastart);
		err = 1;
	}
	if (trunc_page(map->_dm_dvmasize) != map->_dm_dvmasize) {
		printf("**** dvmasize not a multiple of page size: %llx",
			map->_dm_dvmasize);
		err = 1;
	}
	if (map->_dm_dvmastart < is->is_dvmabase ||
	    round_page(map->_dm_dvmastart + map->_dm_dvmasize) >
	    is->is_dvmaend + 1) {
		printf("dvmaddr %llx len %llx out of range %x - %x\n",
			    map->_dm_dvmastart, map->_dm_dvmasize,
			    is->is_dvmabase, is->is_dvmaend);
		err = 1;
	}
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		if (map->dm_segs[seg].ds_addr == 0 ||
		    map->dm_segs[seg].ds_len == 0) {
			printf("seg %d null segment dvmaddr %llx len %llx for "
			    "range %llx len %llx\n",
			    seg,
			    map->dm_segs[seg].ds_addr,
			    map->dm_segs[seg].ds_len,
			    map->_dm_dvmastart, map->_dm_dvmasize);
			err = 1;
		} else if (map->dm_segs[seg].ds_addr < map->_dm_dvmastart ||
		    round_page(map->dm_segs[seg].ds_addr +
			map->dm_segs[seg].ds_len) >
		    map->_dm_dvmastart + map->_dm_dvmasize) {
			printf("seg %d dvmaddr %llx len %llx out of "
			    "range %llx len %llx\n",
			    seg,
			    map->dm_segs[seg].ds_addr,
			    map->dm_segs[seg].ds_len,
			    map->_dm_dvmastart, map->_dm_dvmasize);
			err = 1;
d965 1
a965 7
	}

	if (err) {
		iommu_dvmamap_print_map(t, is, map);
#if defined(DDB) && defined(DEBUG)
		if (iommudebug & IDB_BREAK)
			Debugger();
d967 1
a967 3
	}

	return (err);
d971 7
a977 88
iommu_dvmamap_print_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dmamap_t map)
{
	int seg, i;
	long full_len, source_len;
	struct mbuf *m;

	printf("DVMA %x for %x, mapping %p: dvstart %llx dvsize %llx "
	    "size %lld/%llx maxsegsz %llx boundary %llx segcnt %d "
	    "flags %x type %d source %p "
	    "cookie %p mapsize %llx nsegs %d\n",
	    is ? is->is_dvmabase : 0, is ? is->is_dvmaend : 0, map,
	    map->_dm_dvmastart, map->_dm_dvmasize,
	    map->_dm_size, map->_dm_size, map->_dm_maxsegsz, map->_dm_boundary,
	    map->_dm_segcnt, map->_dm_flags, map->_dm_type,
	    map->_dm_source, map->_dm_cookie, map->dm_mapsize,
	    map->dm_nsegs);

	full_len = 0;
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		printf("seg %d dvmaddr %llx pa %lx len %llx (tte %llx)\n",
		    seg, map->dm_segs[seg].ds_addr,
		    is ? iommu_extract(is, map->dm_segs[seg].ds_addr) : 0,
		    map->dm_segs[seg].ds_len,
		    is ? iommu_lookup_tte(is, map->dm_segs[seg].ds_addr) : 0);
		full_len += map->dm_segs[seg].ds_len;
	}
	printf("total length = %ld/0x%lx\n", full_len, full_len);

	if (map->_dm_source) switch (map->_dm_type) {
	case _DM_TYPE_MBUF:
		m = map->_dm_source;
		if (m->m_flags & M_PKTHDR)
			printf("source PKTHDR mbuf (%p) hdr len = %d/0x%x:\n",
			    m, m->m_pkthdr.len, m->m_pkthdr.len);
		else
			printf("source mbuf (%p):\n", m);

		source_len = 0;
		for ( ; m; m = m->m_next) {
			vaddr_t vaddr = mtod(m, vaddr_t);
			long len = m->m_len;
			paddr_t pa;

			if (pmap_extract(pmap_kernel(), vaddr, &pa))
				printf("kva %lx pa %lx len %ld/0x%lx\n",
				    vaddr, pa, len, len);
			else
				printf("kva %lx pa <invalid> len %ld/0x%lx\n",
				    vaddr, len, len);

			source_len += len;
		}

		if (full_len != source_len)
			printf("mbuf length %ld/0x%lx is %s than mapping "
			    "length %ld/0x%lx\n", source_len, source_len,
			    (source_len > full_len) ? "greater" : "less",
			    full_len, full_len);
		else
			printf("mbuf length %ld/0x%lx\n", source_len,
			    source_len);
		break;
	case _DM_TYPE_LOAD:
	case _DM_TYPE_SEGS:
	case _DM_TYPE_UIO:
	default:
		break;
	}

	if (map->_dm_cookie) {
		struct iommu_map_state *ims = map->_dm_cookie;
		struct iommu_page_map *ipm = &ims->ims_map;

		printf("page map (%p) of size %d with %d entries\n",
		    ipm, ipm->ipm_maxpage, ipm->ipm_pagecnt);
		for (i = 0; i < ipm->ipm_pagecnt; ++i) {
			struct iommu_page_entry *e = &ipm->ipm_map[i];
			printf("%d: vmaddr 0x%lx pa 0x%lx\n", i,
			    e->ipe_va, e->ipe_pa);
		}
	} else
		printf("iommu map state (cookie) is NULL\n");
}

void
iommu_dvmamap_sync(bus_dma_tag_t t, struct iommu_state *is, bus_dmamap_t map,
	bus_addr_t offset, bus_size_t len, int ops)
a978 2
	struct iommu_map_state *ims = map->_dm_cookie;
	struct strbuf_ctl *sb;
a981 15
#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_sync: null map state");
#endif
	sb = ims->ims_sb;

	if ((ims->ims_flags & IOMMU_MAP_STREAM) == 0 || (len == 0))
		return;

	if (ops & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTWRITE))
		return;

	if ((ops & (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_PREWRITE)) == 0)
		return;

d993 2
a994 3
		if (count > 0 && iommu_dvmamap_sync_range(sb,
		    map->dm_segs[i].ds_addr + offset, count))
			needsflush = 1;
d1002 1
a1002 1
		iommu_strbuf_flush_done(ims);
d1010 65
a1074 13
iommu_dvmamap_sync_range(struct strbuf_ctl *sb, vaddr_t va, bus_size_t len)
{
	vaddr_t vaend;
#ifdef DIAGNOSTIC
	struct iommu_state *is = sb->sb_iommu;

	if (va < is->is_dvmabase || va >= is->is_dvmaend)
		panic("invalid va: %llx", (long long)va);

	if ((is->is_tsb[IOTSBSLOT(va, is->is_tsbsize)] & IOTTE_STREAM) == 0) {
		printf("iommu_dvmamap_sync_range: attempting to flush "
		    "non-streaming entry\n");
		return (0);
d1076 5
a1080 18
#endif

	vaend = (va + len + PAGE_MASK) & ~PAGE_MASK;
	va &= ~PAGE_MASK;

#ifdef DIAGNOSTIC
	if (va < is->is_dvmabase || vaend >= is->is_dvmaend)
		panic("invalid va range: %llx to %llx (%x to %x)",
		    (long long)va, (long long)vaend,
		    is->is_dvmabase,
		    is->is_dvmaend);
#endif

	for ( ; va <= vaend; va += PAGE_SIZE) {
		DPRINTF(IDB_BUSDMA,
		    ("iommu_dvmamap_sync_range: flushing va %p\n",
		    (void *)(u_long)va));
		iommu_strbuf_flush(sb, va);
d1083 2
a1084 1
	return (1);
d1088 8
a1095 3
iommu_dvmamem_alloc(bus_dma_tag_t t, struct iommu_state *is, bus_size_t size,
    bus_size_t alignment, bus_size_t boundary, bus_dma_segment_t *segs,
    int nsegs, int *rsegs, int flags)
d1098 4
a1101 4
	DPRINTF(IDB_BUSDMA, ("iommu_dvmamem_alloc: sz %llx align %llx "
	    "bound %llx segp %p flags %d\n", (unsigned long long)size,
	    (unsigned long long)alignment, (unsigned long long)boundary,
	    segs, flags));
d1103 1
a1103 1
	    segs, nsegs, rsegs, flags | BUS_DMA_DVMA));
d1107 5
a1111 2
iommu_dvmamem_free(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *segs, int nsegs)
d1124 8
a1131 2
iommu_dvmamem_map(bus_dma_tag_t t, struct iommu_state *is,
    bus_dma_segment_t *segs, int nsegs, size_t size, caddr_t *kvap, int flags)
d1137 1
a1137 1
	bus_addr_t cbit = 0;
d1156 1
a1156 1
#if 0
a1158 1
#endif
d1173 1
a1173 2
		    "mapping va %lx at %llx\n", va,
		    (unsigned long long)addr | cbit));
d1189 5
a1193 2
iommu_dvmamem_unmap(bus_dma_tag_t t, struct iommu_state *is, caddr_t kva,
    size_t size)
d1200 1
a1200 1
	if ((u_long)kva & PAGE_MASK)
a1208 177

/*
 * Create a new iomap.
 */
struct iommu_map_state *
iommu_iomap_create(int n)
{
	struct iommu_map_state *ims;
	struct strbuf_flush *sbf;
	vaddr_t va;

	if (n < 64)
		n = 64;

	ims = malloc(sizeof(*ims) + (n - 1) * sizeof(ims->ims_map.ipm_map[0]),
		M_DEVBUF, M_NOWAIT);
	if (ims == NULL)
		return (NULL);

	memset(ims, 0, sizeof *ims);

	/* Initialize the map. */
	ims->ims_map.ipm_maxpage = n;
	SPLAY_INIT(&ims->ims_map.ipm_tree);

	/* Initialize the flush area. */
	sbf = &ims->ims_flush;
	va = (vaddr_t)&sbf->sbf_area[0x40];
	va &= ~0x3f;
	pmap_extract(pmap_kernel(), va, &sbf->sbf_flushpa);
	sbf->sbf_flush = (void *)va;

	return (ims);
}

/*
 * Destroy an iomap.
 */
void
iommu_iomap_destroy(struct iommu_map_state *ims)
{
#ifdef DIAGNOSTIC
	if (ims->ims_map.ipm_pagecnt > 0)
		printf("iommu_iomap_destroy: %d page entries in use\n",
		    ims->ims_map.ipm_pagecnt);
#endif

	free(ims, M_DEVBUF);
}

/*
 * Utility function used by splay tree to order page entries by pa.
 */
static inline int
iomap_compare(struct iommu_page_entry *a, struct iommu_page_entry *b)
{
	return ((a->ipe_pa > b->ipe_pa) ? 1 :
		(a->ipe_pa < b->ipe_pa) ? -1 : 0);
}

SPLAY_PROTOTYPE(iommu_page_tree, iommu_page_entry, ipe_node, iomap_compare);

SPLAY_GENERATE(iommu_page_tree, iommu_page_entry, ipe_node, iomap_compare);

/*
 * Insert a pa entry in the iomap.
 */
int
iommu_iomap_insert_page(struct iommu_map_state *ims, paddr_t pa)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;

	if (ipm->ipm_pagecnt >= ipm->ipm_maxpage) {
		struct iommu_page_entry ipe;

		ipe.ipe_pa = pa;
		if (SPLAY_FIND(iommu_page_tree, &ipm->ipm_tree, &ipe))
			return (0);

		return (ENOMEM);
	}

	e = &ipm->ipm_map[ipm->ipm_pagecnt];

	e->ipe_pa = pa;
	e->ipe_va = NULL;

	e = SPLAY_INSERT(iommu_page_tree, &ipm->ipm_tree, e);

	/* Duplicates are okay, but only count them once. */
	if (e)
		return (0);

	++ipm->ipm_pagecnt;

	return (0);
}

/*
 * Locate the iomap by filling in the pa->va mapping and inserting it
 * into the IOMMU tables.
 */
int
iommu_iomap_load_map(struct iommu_state *is, struct iommu_map_state *ims,
    vaddr_t vmaddr, int flags)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct strbuf_ctl *sb = ims->ims_sb;
	int i;

	if (sb->sb_flush == NULL)
		flags &= ~BUS_DMA_STREAMING;

	if (flags & BUS_DMA_STREAMING)
		ims->ims_flags |= IOMMU_MAP_STREAM;
	else
		ims->ims_flags &= ~IOMMU_MAP_STREAM;

	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e) {
		e->ipe_va = vmaddr;
		iommu_enter(is, sb, e->ipe_va, e->ipe_pa, flags);
		vmaddr += PAGE_SIZE;
	}

	return (0);
}

/*
 * Remove the iomap from the IOMMU.
 */
int
iommu_iomap_unload_map(struct iommu_state *is, struct iommu_map_state *ims)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct strbuf_ctl *sb = ims->ims_sb;
	int i;

	for (i = 0, e = ipm->ipm_map; i < ipm->ipm_pagecnt; ++i, ++e)
		iommu_remove(is, sb, e->ipe_va);

	return (0);
}

/*
 * Translate a physical address (pa) into a DVMA address.
 */
vaddr_t
iommu_iomap_translate(struct iommu_map_state *ims, paddr_t pa)
{
	struct iommu_page_map *ipm = &ims->ims_map;
	struct iommu_page_entry *e;
	struct iommu_page_entry pe;
	paddr_t offset = pa & PAGE_MASK;

	pe.ipe_pa = trunc_page(pa);

	e = SPLAY_FIND(iommu_page_tree, &ipm->ipm_tree, &pe);

	if (e == NULL)
		return (NULL);

	return (e->ipe_va | offset);
}

/*
 * Clear the iomap table and tree.
 */
void
iommu_iomap_clear_pages(struct iommu_map_state *ims)
{
	ims->ims_map.ipm_pagecnt = 0;
	SPLAY_INIT(&ims->ims_map.ipm_tree);
}

@


1.5.4.6
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: iommu.c,v 1.5.4.5 2003/03/27 23:42:35 niklas Exp $	*/
d1035 1
a1035 1
	if (sgstart == NULL || sgstart > sgend) {
d1484 1
a1484 1
	if (va < is->is_dvmabase || va > is->is_dvmaend)
d1498 1
a1498 1
	if (va < is->is_dvmabase || vaend > is->is_dvmaend)
@


1.5.4.7
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a98 2
void _iommu_dvmamap_sync(bus_dma_tag_t, bus_dma_tag_t, bus_dmamap_t,
    bus_addr_t, bus_size_t, int);
d260 1
a260 1
 * Initialize one STC.
a558 8

#define BUS_DMA_FIND_PARENT(t, fn)                                      \
        if (t->_parent == NULL)                                         \
                panic("null bus_dma parent (" #fn ")");                 \
        for (t = t->_parent; t->fn == NULL; t = t->_parent)             \
                if (t->_parent == NULL)                                 \
                        panic("no bus_dma " #fn " located");

d560 3
a562 3
iommu_dvmamap_create(bus_dma_tag_t t, bus_dma_tag_t t0, struct strbuf_ctl *sb,
    bus_size_t size, int nsegments, bus_size_t maxsegsz, bus_size_t boundary,
    int flags, bus_dmamap_t *dmamap)
d568 2
a569 3
	BUS_DMA_FIND_PARENT(t, _dmamap_create);
	ret = (*t->_dmamap_create)(t, t0, size, nsegments, maxsegsz, boundary,
	    flags, &map);
d577 1
a577 1
		bus_dmamap_destroy(t0, map);
a582 7

#ifdef DIAGNOSTIC
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_create: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_create: null iommu");
#endif
d589 1
a589 1
iommu_dvmamap_destroy(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map)
d596 1
a596 1
		bus_dmamap_unload(t0, map);
d602 1
a602 2
	BUS_DMA_FIND_PARENT(t, _dmamap_destroy);
	(*t->_dmamap_destroy)(t, t0, map);
d615 1
a615 1
iommu_dvmamap_load(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
a622 1
	struct iommu_state *is;
a629 7
#ifdef DEBUG
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_load: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_load: null iommu");
#endif /* DEBUG */
	is = ims->ims_sb->sb_iommu;
d639 1
a639 1
		bus_dmamap_unload(t0, map);
d660 1
a660 1
	align = MAX(map->dm_segs[0]._ds_align, PAGE_SIZE);
d696 2
a697 2
		sgstart = MAX(is->is_dvmamap->ex_start, 0xff000000);
		sgend = MIN(is->is_dvmamap->ex_end, 0xffffffff);
d761 2
a762 2
			pgstart = pa | (MAX(a, addr) & PAGE_MASK);
			pgend = pa | (MIN(a + PAGE_SIZE - 1,
d815 3
a817 2
iommu_dvmamap_load_raw(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
    bus_dma_segment_t *segs, int nsegs, bus_size_t size, int flags)
a824 1
	struct iommu_state *is;
a830 7
#ifdef DEBUG
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_load_raw: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_load_raw: null iommu");
#endif /* DEBUG */
	is = ims->ims_sb->sb_iommu;
d837 1
a837 1
		bus_dmamap_unload(t0, map);
d847 1
a847 1
	align = MAX(segs[0]._ds_align, PAGE_SIZE);
d875 1
a875 1
			int seg_len = MIN(left, len);
d898 2
a899 2
		sgstart = MAX(is->is_dvmamap->ex_start, 0xff000000);
		sgend = MIN(is->is_dvmamap->ex_end, 0xffffffff);
d971 1
a971 1
				int seg_len = MIN(left, len);
d1158 1
a1158 1
		int seg_len = MIN(left, len);
d1171 2
a1172 2
			pgstart = MAX(a, addr);
			pgend = MIN(a + PAGE_SIZE - 1, addr + seg_len - 1);
d1231 1
a1231 1
iommu_dvmamap_unload(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map)
a1232 1
	struct iommu_state *is;
a1237 11
#ifdef DEBUG
	if (ims == NULL)
		panic("iommu_dvmamap_unload: null map state");
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_unload: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_unload: null iommu");
#endif /* DEBUG */

	is = ims->ims_sb->sb_iommu;

d1426 1
a1426 1
_iommu_dvmamap_sync(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
a1428 1
	struct iommu_state *is;
d1434 4
d1439 9
a1447 1
	is = sb->sb_iommu;
d1456 1
a1456 1
		panic("iommu_dvmamap_sync: too short %llu", offset);
d1459 1
a1459 1
		count = MIN(map->dm_segs[i].ds_len - offset, len);
a1465 1
#ifdef DIAGNOSTIC
d1467 1
a1467 2
		panic("iommu_dvmamap_sync: leftover %llu", len);
#endif
a1472 28
void
iommu_dvmamap_sync(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dmamap_t map,
    bus_addr_t offset, bus_size_t len, int ops)
{
	struct iommu_map_state *ims = map->_dm_cookie;

#ifdef DIAGNOSTIC
	if (ims == NULL)
		panic("iommu_dvmamap_sync: null map state");
	if (ims->ims_sb == NULL)
		panic("iommu_dvmamap_sync: null sb");
	if (ims->ims_sb->sb_iommu == NULL)
		panic("iommu_dvmamap_sync: null iommu");
#endif
	if (len == 0)
		return;

	if (ops & BUS_DMASYNC_PREWRITE)
		membar(MemIssue);

	if ((ims->ims_flags & IOMMU_MAP_STREAM) &&
	    (ops & (BUS_DMASYNC_POSTREAD | BUS_DMASYNC_PREWRITE)))
		_iommu_dvmamap_sync(t, t0, map, offset, len, ops);

	if (ops & BUS_DMASYNC_POSTREAD)
		membar(MemIssue);
}

d1516 1
a1516 1
iommu_dvmamem_alloc(bus_dma_tag_t t, bus_dma_tag_t t0, bus_size_t size,
d1525 1
a1525 2
	BUS_DMA_FIND_PARENT(t, _dmamem_alloc);
	return ((*t->_dmamem_alloc)(t, t0, size, alignment, boundary,
d1530 2
a1531 2
iommu_dvmamem_free(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dma_segment_t *segs,
    int nsegs)
d1536 1
a1536 2
	BUS_DMA_FIND_PARENT(t, _dmamem_free);
	(*t->_dmamem_free)(t, t0, segs, nsegs);
d1544 2
a1545 2
iommu_dvmamem_map(bus_dma_tag_t t, bus_dma_tag_t t0, bus_dma_segment_t *segs,
    int nsegs, size_t size, caddr_t *kvap, int flags)
d1605 1
a1605 1
iommu_dvmamem_unmap(bus_dma_tag_t t, bus_dma_tag_t t0, caddr_t kva,
@


1.5.4.8
log
@Merge with the trunk
@
text
@d698 1
a698 1
		aend = round_page(addr + seg_len);
d774 1
a774 1
		aend = round_page(addr + seg_len);
d914 1
a914 1
			aend = round_page(addr + seg_len);
d1197 3
a1199 2
		aend = round_page(addr + seg_len);
		for (a = trunc_page(addr); a < aend; a += PAGE_SIZE) {
@


1.4
log
@Merge with NetBSD:
better handling of boundary conditions
add iommu_extract debugging function
@
text
@d1 2
a2 2
/*	$OpenBSD: iommu.c,v 1.3 2001/09/19 20:50:58 mickey Exp $	*/
/*	$NetBSD: iommu.c,v 1.40 2001/09/21 03:04:09 eeh Exp $	*/
d150 13
a162 2
#define iommu_strbuf_flush(i,v) bus_space_write_8((i)->is_bustag, \
	(bus_space_handle_t)(u_long)&(i)->is_sb->strbuf_pgflush, 0, (v))
d250 2
a251 2
	if (is->is_sb)
		(void)pmap_extract(pmap_kernel(), (vaddr_t)&is->is_flush,
d289 1
d291 25
a315 13
	if (!is->is_sb)
		return;

	/* Enable diagnostics mode? */
	bus_space_write_8(is->is_bustag, 
			  (bus_space_handle_t)(u_long)&is->is_sb->strbuf_ctl,
			  0, STRBUF_EN);

	/* No streaming buffers? Disable them */
	if (bus_space_read_8(is->is_bustag,
			     (bus_space_handle_t)(u_long)&is->is_sb->strbuf_ctl, 
			     0) == 0)
		is->is_sb = 0;
d339 1
a339 1
	if (is->is_sb) {
a366 2
	else
		printf("%llx dva invalid!\n", dva);
d404 1
a404 1
		if (is->is_sb) {
d449 1
a449 1
	if (!is->is_sb)
d465 12
a476 5
	is->is_flush = 0;
	membar_sync();	/* #StoreStore is prolly enuf. */
	bus_space_write_8(is->is_bustag, (bus_space_handle_t)(u_long)
			  &is->is_sb->strbuf_flushsync, 0, is->is_flushpa);
	membar_sync();	/* Prolly not needed at all. */
d487 4
a490 3
	while (!ldxa(is->is_flushpa, ASI_PHYS_CACHED) && 
	       ((cur.tv_sec <= flushtimeout.tv_sec) && 
		(cur.tv_usec <= flushtimeout.tv_usec)))
d494 6
a499 4
	if (!ldxa(is->is_flushpa, ASI_PHYS_CACHED)) {
		printf("iommu_strbuf_flush_done: flush timeout %p at %p\n",
		    (void *)(u_long)is->is_flush, 
		    (void *)(u_long)is->is_flushpa); /* panic? */
d506 1
a506 1
	return (is->is_flush);
d561 2
a562 2
	/*
	 * If our segment size is larger than the boundary we need to
d566 1
a566 1
		(sgsize > boundary) ? 0 : boundary,
d571 3
a573 2
	if (err || (dvmaddr == (bus_addr_t)-1))	{ 
		panic("iommu_dvmamap_load(): extent_alloc(%d, %x) failed!",
d575 3
a613 1
			splx(s);
d616 1
d696 2
a702 2
	map->_dm_dvmastart = 0;
	map->_dm_dvmasize = 0;
d762 4
a771 19
	/*
	 *  If our segment size is larger than the boundary we need to
	 * split the transfer up int little pieces ourselves.
	 */

	/*
	 r A boundar presented to bus_dmamem_alloc() takes precedence
	 * over boundary in the map.
	 */
	if ((boundary = segs[0]._ds_boundary) == 0)
		boundary = map->_dm_boundary;

	align = max(segs[0]._ds_align, NBPG);
	s = splhigh();
	err = extent_alloc(is->is_dvmamap, sgsize, align, 0, boundary, 
	   ((flags & BUS_DMA_NOWAIT) == 0 ? EX_WAITOK : EX_NOWAIT)|EX_BOUNDZERO, 
	    (u_long *)&dvmaddr);
	splx(s);

a924 1
			splx(s);
d927 1
d984 1
a984 1
		if (is->is_sb)
d1003 1
a1003 1
		if (is->is_sb)
@


1.3
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: iommu.c,v 1.2 2001/08/18 21:30:00 jason Exp $	*/
/*	$NetBSD: iommu.c,v 1.37 2001/08/06 22:02:58 eeh Exp $	*/
d330 21
d494 1
a494 1
	u_long dvmaddr;
d497 1
d529 4
d534 2
a535 1
	    boundary, EX_NOWAIT|EX_BOUNDZERO, (u_long *)&dvmaddr);
d539 2
a540 3
	if (err || (dvmaddr == (bus_addr_t)-1))	
	{ 
		printf("iommu_dvmamap_load(): extent_alloc(%d, %x) failed!\n",
a541 5
#ifdef DDB
		Debugger();
#else
		panic("");
#endif
d550 41
a590 3
	/*
	 * We always use just one segment.
	 */
a591 3
	map->dm_nsegs = 1;
	map->dm_segs[0].ds_addr = dvmaddr + (vaddr & PGOFSET);
	map->dm_segs[0].ds_len = buflen;
a597 1
	dvmaddr = trunc_page(map->dm_segs[0].ds_addr);
d636 1
a636 4
	vaddr_t addr, offset;
	size_t len;
	int error, s, i;
	bus_addr_t dvmaddr;
a637 1
	paddr_t pa;
d639 10
a648 22
	dvmaddr = (map->dm_segs[0].ds_addr & ~PGOFSET);
	pa = 0;
	sgsize = 0;
	for (i = 0; i<map->dm_nsegs; i++) {

		addr = trunc_page(map->dm_segs[i].ds_addr);
		offset = map->dm_segs[i].ds_addr & PGOFSET;
		len = map->dm_segs[i].ds_len;
		if (len == 0 || addr == 0)
			printf("iommu_dvmamap_unload: map = %p, i = %d, len = %d, addr = %lx\n",
				map, (int)i, (int)len, (unsigned long)addr);

		DPRINTF(IDB_BUSDMA,
			("iommu_dvmamap_unload: map %p removing va %lx size %lx\n",
			 map, (long)addr, (long)len));
		iommu_remove(is, addr, len);
		
		if (trunc_page(pa) == addr)
			sgsize += trunc_page(len + offset);
		else 
			sgsize += round_page(len + offset);
		pa = addr + offset + len;
a649 1
	}
d658 2
a659 1
	error = extent_free(is->is_dvmamap, dvmaddr, sgsize, EX_NOWAIT);
d663 4
d681 1
a681 1
	int i, s;
d687 1
a687 1
	u_long dvmaddr;
d698 10
d715 2
a716 1
	for (i=0; i<nsegs; i++) {
d719 2
a720 1
		sgsize += segs[i].ds_len;
d725 12
d738 1
a738 1
	 * A boundary presented to bus_dmamem_alloc() takes precedence
d769 4
d781 6
d788 5
a792 4
		/* We'll never end up with less segments than we got as input.
		   this gives us a chance to fail quickly */
		if (nsegs > map->_dm_segcnt)
			return (E2BIG);
d794 2
a795 5
		i = 0;
		dvmaddr += (segs[i].ds_addr & PGOFSET);
		map->dm_segs[i].ds_addr = dvmaddr;
		map->dm_segs[i].ds_len = left = segs[i].ds_len;
		pa = segs[i].ds_addr;
d797 1
a797 1
		while (left > 0) {
d800 60
d862 1
d865 1
a865 1
				 map, (long)dvmaddr, (long)(pa)));
d868 2
a869 2
				iommu_enter(is, prev_va = trunc_page(dvmaddr), 
					    trunc_page(pa), flags);
d874 1
a874 27

			/* Next segment */
			if (left <= 0 && ++i < nsegs) {
				u_long offset;

				/* 
				 * If the two segs are on different physical pages
				 * move to a new virtual page.
				 */
				offset = (segs[i].ds_addr & PGOFSET);
				if (trunc_page(pa) != trunc_page(segs[i].ds_addr))
					dvmaddr += NBPG;

				pa = segs[i].ds_addr;
				dvmaddr = trunc_page(dvmaddr) + offset;

				map->dm_segs[i].ds_len = left = segs[i].ds_len;
				map->dm_segs[i].ds_addr = dvmaddr;
			}
		}
		map->dm_nsegs = i;

		/* bail out if we created more segments than the dmamap is
		   allowed to carry */
		if (i > map->_dm_segcnt) {
			iommu_dvmamap_unload(t, is, map);
			return (E2BIG);
d876 4
a879 1

d883 2
a884 2
	 * This was allocated with bus_dmamem_alloc.  We only
	 * have one segment, and the pages are on an `mlist'.
d888 28
a915 2
	map->dm_segs[0].ds_addr = dvmaddr;
	map->dm_segs[0].ds_len = size;
d930 2
a931 1
	map->dm_nsegs = i;
@


1.2
log
@RCSids
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a122 1
#include <vm/vm_kern.h>
@


1.1
log
@compiles with minimal mods (From NetBSD)
@
text
@d1 1
@

