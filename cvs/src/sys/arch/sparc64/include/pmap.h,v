head	1.30;
access;
symbols
	OPENBSD_6_2:1.30.0.12
	OPENBSD_6_2_BASE:1.30
	OPENBSD_6_1:1.30.0.10
	OPENBSD_6_1_BASE:1.30
	OPENBSD_6_0:1.30.0.6
	OPENBSD_6_0_BASE:1.30
	OPENBSD_5_9:1.30.0.2
	OPENBSD_5_9_BASE:1.30
	OPENBSD_5_8:1.30.0.4
	OPENBSD_5_8_BASE:1.30
	OPENBSD_5_7:1.29.0.2
	OPENBSD_5_7_BASE:1.29
	OPENBSD_5_6:1.28.0.6
	OPENBSD_5_6_BASE:1.28
	OPENBSD_5_5:1.28.0.4
	OPENBSD_5_5_BASE:1.28
	OPENBSD_5_4:1.26.0.2
	OPENBSD_5_4_BASE:1.26
	OPENBSD_5_3:1.23.0.8
	OPENBSD_5_3_BASE:1.23
	OPENBSD_5_2:1.23.0.6
	OPENBSD_5_2_BASE:1.23
	OPENBSD_5_1_BASE:1.23
	OPENBSD_5_1:1.23.0.4
	OPENBSD_5_0:1.23.0.2
	OPENBSD_5_0_BASE:1.23
	OPENBSD_4_9:1.19.0.2
	OPENBSD_4_9_BASE:1.19
	OPENBSD_4_8:1.17.0.8
	OPENBSD_4_8_BASE:1.17
	OPENBSD_4_7:1.17.0.4
	OPENBSD_4_7_BASE:1.17
	OPENBSD_4_6:1.17.0.6
	OPENBSD_4_6_BASE:1.17
	OPENBSD_4_5:1.17.0.2
	OPENBSD_4_5_BASE:1.17
	OPENBSD_4_4:1.16.0.2
	OPENBSD_4_4_BASE:1.16
	OPENBSD_4_3:1.15.0.2
	OPENBSD_4_3_BASE:1.15
	OPENBSD_4_2:1.12.0.2
	OPENBSD_4_2_BASE:1.12
	OPENBSD_4_1:1.11.0.4
	OPENBSD_4_1_BASE:1.11
	OPENBSD_4_0:1.11.0.2
	OPENBSD_4_0_BASE:1.11
	OPENBSD_3_9:1.9.0.2
	OPENBSD_3_9_BASE:1.9
	OPENBSD_3_8:1.8.0.6
	OPENBSD_3_8_BASE:1.8
	OPENBSD_3_7:1.8.0.4
	OPENBSD_3_7_BASE:1.8
	OPENBSD_3_6:1.8.0.2
	OPENBSD_3_6_BASE:1.8
	SMP_SYNC_A:1.6
	SMP_SYNC_B:1.6
	OPENBSD_3_5:1.5.0.10
	OPENBSD_3_5_BASE:1.5
	OPENBSD_3_4:1.5.0.8
	OPENBSD_3_4_BASE:1.5
	UBC_SYNC_A:1.5
	OPENBSD_3_3:1.5.0.6
	OPENBSD_3_3_BASE:1.5
	OPENBSD_3_2:1.5.0.4
	OPENBSD_3_2_BASE:1.5
	OPENBSD_3_1:1.5.0.2
	OPENBSD_3_1_BASE:1.5
	UBC_SYNC_B:1.5
	UBC:1.2.0.2
	UBC_BASE:1.2
	SMP:1.1.0.4
	OPENBSD_3_0:1.1.0.2
	OPENBSD_3_0_BASE:1.1;
locks; strict;
comment	@ * @;


1.30
date	2015.04.09.16.56.52;	author kettenis;	state Exp;
branches;
next	1.29;
commitid	hrRKJaqwTtvGGgzQ;

1.29
date	2014.12.17.19.39.20;	author tedu;	state Exp;
branches;
next	1.28;
commitid	GbiCGgOHhTtM35o0;

1.28
date	2014.01.30.18.16.41;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2013.08.08.21.14.23;	author kettenis;	state Exp;
branches;
next	1.26;

1.26
date	2013.06.11.16.42.12;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	2013.03.31.17.07.03;	author deraadt;	state Exp;
branches;
next	1.24;

1.24
date	2013.03.23.16.12.28;	author deraadt;	state Exp;
branches;
next	1.23;

1.23
date	2011.05.18.21.43.22;	author ariane;	state Exp;
branches;
next	1.22;

1.22
date	2011.05.18.21.00.09;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2011.05.18.20.41.24;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2011.04.28.20.38.04;	author ariane;	state Exp;
branches;
next	1.19;

1.19
date	2010.12.26.15.41.00;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2010.12.06.20.57.19;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2009.02.12.18.53.13;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2008.06.09.20.32.53;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2007.12.14.18.32.25;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	2007.10.17.21.23.28;	author kettenis;	state Exp;
branches;
next	1.13;

1.13
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2007.06.06.17.15.13;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	2006.06.29.20.50.58;	author kettenis;	state Exp;
branches;
next	1.10;

1.10
date	2006.05.31.23.48.23;	author kettenis;	state Exp;
branches;
next	1.9;

1.9
date	2005.12.12.19.14.48;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2004.08.09.10.13.21;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2004.08.06.22.39.14;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2004.05.20.09.20.42;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2002.03.14.03.16.01;	author millert;	state Exp;
branches;
next	1.4;

1.4
date	2002.03.14.01.26.45;	author millert;	state Exp;
branches;
next	1.3;

1.3
date	2002.02.05.18.34.39;	author jason;	state Exp;
branches;
next	1.2;

1.2
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.2.2.1;
next	1.1;

1.1
date	2001.08.18.19.04.49;	author art;	state Exp;
branches
	1.1.4.1;
next	;

1.1.4.1
date	2001.10.31.03.07.58;	author nate;	state Exp;
branches;
next	1.1.4.2;

1.1.4.2
date	2002.03.06.02.04.47;	author niklas;	state Exp;
branches;
next	1.1.4.3;

1.1.4.3
date	2002.03.28.11.23.52;	author niklas;	state Exp;
branches;
next	1.1.4.4;

1.1.4.4
date	2004.06.05.23.11.00;	author niklas;	state Exp;
branches;
next	;

1.2.2.1
date	2002.06.11.03.38.43;	author art;	state Exp;
branches;
next	;


desc
@@


1.30
log
@Make the sparc64 pmap (more) mpsafe by protecting both the pmap itself and the
pv lists with a mutex.  Some minor code adjustments to prevent holding locks
too long.  This should make pmap_enter(9), pmap_remove(9) and
pmap_page_protect(9) safe to use without holding the kernel lock.
@
text
@/*	$NetBSD: pmap.h,v 1.16 2001/04/22 23:19:30 thorpej Exp $	*/

/*-
 * Copyright (C) 1995, 1996 Wolfgang Solfrank.
 * Copyright (C) 1995, 1996 TooLs GmbH.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by TooLs GmbH.
 * 4. The name of TooLs GmbH may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY TOOLS GMBH ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL TOOLS GMBH BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef	_MACHINE_PMAP_H_
#define	_MACHINE_PMAP_H_

#ifndef _LOCORE
#include <machine/pte.h>
#ifdef	_KERNEL
#include <sys/queue.h>
#endif
#include <sys/mutex.h>
#endif

/*
 * This scheme uses 2-level page tables.
 *
 * While we're still in 32-bit mode we do the following:
 *
 *   offset:						13 bits
 * 1st level: 1024 64-bit TTEs in an 8K page for	10 bits
 * 2nd level: 512 32-bit pointers in the pmap for 	 9 bits
 *							-------
 * total:						32 bits
 *
 * In 64-bit mode the Spitfire and Blackbird CPUs support only
 * 44-bit virtual addresses.  All addresses between 
 * 0x0000 07ff ffff ffff and 0xffff f800 0000 0000 are in the
 * "VA hole" and trap, so we don't have to track them.  However,
 * we do need to keep them in mind during PT walking.  If they
 * ever change the size of the address "hole" we need to rework
 * all the page table handling.
 *
 *   offset:						13 bits
 * 1st level: 1024 64-bit TTEs in an 8K page for	10 bits
 * 2nd level: 1024 64-bit pointers in an 8K page for 	10 bits
 * 3rd level: 1024 64-bit pointers in the segmap for 	10 bits
 *							-------
 * total:						43 bits
 *
 * Of course, this means for 32-bit spaces we always have a (practically)
 * wasted page for the segmap (only one entry used) and half a page wasted
 * for the page directory.  We still have need of one extra bit 8^(.
 */

#define HOLESHIFT	(43)

#define PTSZ	(PAGE_SIZE/8)
#define PDSZ	(PTSZ)
#define STSZ	(PTSZ)

#define PTSHIFT		(13)
#define	PDSHIFT		(10+PTSHIFT)
#define STSHIFT		(10+PDSHIFT)

#define PTMASK		(PTSZ-1)
#define PDMASK		(PDSZ-1)
#define STMASK		(STSZ-1)

#ifndef _LOCORE

#define va_to_seg(v)	(int)((((paddr_t)(v))>>STSHIFT)&STMASK)
#define va_to_dir(v)	(int)((((paddr_t)(v))>>PDSHIFT)&PDMASK)
#define va_to_pte(v)	(int)((((paddr_t)(v))>>PTSHIFT)&PTMASK)

#ifdef	_KERNEL

/*
 * Support for big page sizes.  This maps the page size to the
 * page bits.
 */
struct page_size_map {
	u_int64_t mask;
	u_int64_t code;
#ifdef DEBUG
	u_int64_t use;
#endif
};
extern struct page_size_map page_size_map[];

struct pmap {
	struct mutex pm_mtx;
	int pm_ctx;		/* Current context */
	int pm_refs;		/* ref count */
	/* 
	 * This contains 64-bit pointers to pages that contain 
	 * 1024 64-bit pointers to page tables.  All addresses
	 * are physical.  
	 *
	 * !!! Only touch this through pseg_get() and pseg_set() !!!
	 */
	paddr_t pm_physaddr;	/* physical address of pm_segs */
	int64_t *pm_segs;

	struct pmap_statistics pm_stats;
};

/*
 * This comes from the PROM and is used to map prom entries.
 */
struct prom_map {
	u_int64_t	vstart;
	u_int64_t	vsize;
	u_int64_t	tte;
};

#define PMAP_NC		0x001	/* Set the E bit in the page */
#define PMAP_NVC	0x002	/* Don't enable the virtual cache */
#define PMAP_NOCACHE	PMAP_NC
#define PMAP_LITTLE	0x004	/* Map in little endian mode */
/* Large page size hints -- we really should use another param to pmap_enter() */
#define PMAP_8K		0x000
#define PMAP_64K	0x008	/* Use 64K page */
#define PMAP_512K	0x010
#define PMAP_4M		0x018
#define PMAP_SZ_TO_TTE(x)	(((x)&0x018)<<58)
/* If these bits are different in va's to the same PA then there is an aliasing in the d$ */
#define VA_ALIAS_MASK	(1<<13)	/* = (VA_ALIAS_ALIGN - 1) & ~PAGE_MASK */
#define VA_ALIAS_ALIGN	(1<<14)

typedef	struct pmap *pmap_t;

/* 
 * Encode IO space for pmap_enter() 
 *
 * Since sun4u machines don't have separate IO spaces, this is a noop.
 */
#define PMAP_IOENC(io)	0

extern struct pmap kernel_pmap_;
#define	pmap_kernel()	(&kernel_pmap_)

/* int pmap_change_wiring(pmap_t pm, vaddr_t va, boolean_t wired); */
#define	pmap_resident_count(pm)		((pm)->pm_stats.resident_count)
#define	pmap_update(pm)			/* nothing (yet) */

#define pmap_proc_iflush(p,va,len)	/* nothing */

void pmap_bootstrap(u_long, u_long, u_int, u_int);
/* make sure all page mappings are modulo 16K to prevent d$ aliasing */
#define PMAP_PREFER(pa, va)	((va) + (((va) ^ (pa)) & VA_ALIAS_MASK))

/* pmap prefer alignment */
#define PMAP_PREFER_ALIGN()	(VA_ALIAS_ALIGN)
/* pmap prefer offset in alignment */
#define PMAP_PREFER_OFFSET(of)	((of) & VA_ALIAS_MASK)

#define PMAP_GROWKERNEL         /* turn on pmap_growkernel interface */

/* SPARC specific? */
void		pmap_redzone(void);
int             pmap_dumpsize(void);
int             pmap_dumpmmu(int (*)(dev_t, daddr_t, caddr_t, size_t), daddr_t);
int		pmap_pa_exists(paddr_t);
struct proc;
void		switchexit(struct proc *);

/* SPARC64 specific */
int	ctx_alloc(struct pmap*);
void	ctx_free(struct pmap*);

#endif	/* _KERNEL */

/*
 * For each struct vm_page, there is a list of all currently valid virtual
 * mappings of that page.
 */
typedef struct pv_entry {
	struct pv_entry	*pv_next;	/* next pv_entry */
	struct pmap	*pv_pmap;	/* pmap where mapping lies */
	vaddr_t	pv_va;		/* virtual address for mapping */
} *pv_entry_t;
/* PV flags encoded in the low bits of the VA of the first pv_entry */

struct vm_page_md {
	struct mutex pvmtx;
	struct pv_entry pvent;
};

#define VM_MDPAGE_INIT(pg) do {			\
	mtx_init(&(pg)->mdpage.pvmtx, IPL_VM);	\
	(pg)->mdpage.pvent.pv_next = NULL;	\
	(pg)->mdpage.pvent.pv_pmap = NULL;	\
	(pg)->mdpage.pvent.pv_va = 0;		\
} while (0)

#endif	/* _LOCORE */
#endif	/* _MACHINE_PMAP_H_ */
@


1.29
log
@minimal removal of simplelock to eliminate lock.h dependency
@
text
@d42 1
d112 1
d206 1
d211 1
@


1.28
log
@Move declaration of struct vm_page_md from <machine/vmparam.h> to
<machine/pmap.h> where it belongs, and compensate in <uvm/uvm_extern.h>
by including <uvm/uvm_pmap.h> before <uvm/uvm_page.h>. Tested on all
MACHINE_ARCH but amd64 and i386 (and hppa64).
@
text
@a121 1
	struct simplelock pm_lock;
@


1.27
log
@Add PMAP_NOCACHE as an alias for PMAP_NC for use in semi-MI code.
@
text
@d191 22
a213 1
#endif	/* _KERNEL */
@


1.26
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d138 1
@


1.25
log
@try to avoid pulling in pte.h and other more crazy things.  Checked against
the things that libkvm needs.
@
text
@d181 1
a181 1
int             pmap_dumpmmu(int (*)(dev_t, daddr64_t, caddr_t, size_t), daddr64_t);
@


1.24
log
@refactor sys/param.h and machine/param.h.  A lot of #ifdef _KERNEL is added
to keep definitions our of user space.  The MD files now follow a consistant
order -- all namespace intrusion is at the tail can be cleaned up
independently.  locore, bootblocks, and libkvm still see enough visibility to
build.  Checked on 90% of platforms...
@
text
@d39 1
d42 1
d91 6
a108 10

/*
 * Pmap stuff
 */

#define va_to_seg(v)	(int)((((paddr_t)(v))>>STSHIFT)&STMASK)
#define va_to_dir(v)	(int)((((paddr_t)(v))>>PDSHIFT)&PDMASK)
#define va_to_pte(v)	(int)((((paddr_t)(v))>>PTSHIFT)&PTMASK)

#ifdef	_KERNEL
@


1.23
log
@Turns out VA_ALIAS_MASK is indeed a mask, but with the PAGE_MASK deceptively
zeroed out.
Documented this in code and updated PMAP_PREFER_* macros to use the corrected
values.

Discussed with and ok miod@@
@
text
@d75 1
a75 1
#define PTSZ	(NBPG/8)
@


1.22
log
@Better make PMAP_PREFER_ALIGN() a rounded value, as done on other arches;
while there fix PMAP_PREFER_OFFSET() for good too, after discussion with ariane@@
@
text
@d148 2
a149 1
#define VA_ALIAS_MASK   (1<<13)	
d174 1
a174 1
#define PMAP_PREFER_ALIGN()	(VA_ALIAS_MASK)
d176 1
a176 1
#define PMAP_PREFER_OFFSET(of)	((of) & (VA_ALIAS_MASK - 1))
@


1.21
log
@Off-by-two in PMAP_PREFER_ALIGN definition; found the hard way by matthieu@@
@
text
@d173 1
a173 1
#define PMAP_PREFER_ALIGN()	(VA_ALIAS_MASK - 1)
d175 1
a175 1
#define PMAP_PREFER_OFFSET(of)	((of) & VA_ALIAS_MASK)
@


1.20
log
@Expose pmap_prefer parameters.
Enables future uvm_map code to make intelligent decisions during allocation.

No functional change.
@
text
@d173 1
a173 1
#define PMAP_PREFER_ALIGN()	(VA_ALIAS_MASK + 1)
@


1.19
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d172 5
@


1.18
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@a163 1
#define	pmap_phys_address(x)		(x)
@


1.17
log
@Add a pm_statistics struct to all pmap, and keep track of resident
pages. Use this to provide a real pmap_resident_count() function.
ok kettenis@@
@
text
@d171 1
a171 1
#define PMAP_PREFER(pa, va)	(*(va) += (((*(va)) ^ (pa)) & VA_ALIAS_MASK))
@


1.16
log
@Sparc64 MMUs have an address hole, too, and the pmap implementation we
are using has an even larger one, so implement pmap_remove_hole() to
prevent mmap() from ever reaching the hole.

feedback and ok kettenis@@
@
text
@d125 2
a161 1
int pmap_count_res(pmap_t pmap);
d163 1
a163 1
#define	pmap_resident_count(pm)		pmap_count_res((pm))
@


1.15
log
@Remove a lot of symbols from the namespace, otherwise sys/sysctl.h and
rpc/pmap_prot.h collide.. "struct pmap" from the kernel should not make
it out to userland.
@
text
@a164 1
#define	pmap_remove_holes(map)		do { /* nothing */ } while (0)
@


1.14
log
@Spin up secondary CPUs on MULTIPROCESSOR kernels.  Works on UltraSPARC-III
CPUs.
@
text
@d110 2
a156 1
#ifdef	_KERNEL
@


1.13
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d168 1
a168 1
void pmap_bootstrap(u_long kernelstart, u_long kernelend, u_int numctx);
@


1.12
log
@now that all partition size/offsets are potentially 64-bit, change the
type of all variables to daddr64_t.  this includes the APIs for XXsize()
and XXdump(), all range checks inside bio drivers, internal variables
for disklabel handling, and even uvm's swap offsets.  re-read numerous
times by otto, miod, krw, thib to look for errors
@
text
@d164 1
@


1.11
log
@Fix PMAP_PREFER and make it use VA_ALIAS_MASK to prevent me from breaking
it again.

ok miod@@, jason@@
@
text
@d176 1
a176 1
int             pmap_dumpmmu(int (*)(dev_t, daddr_t, caddr_t, size_t), daddr_t);
@


1.10
log
@It's bit 13 that's causing the address aliasing in the cache, not bit 14.
ok jason@@
@
text
@d169 1
a169 1
#define PMAP_PREFER(pa, va)	(*(va)+=(((*(va))^(pa))&(1<<(PGSHIFT+1))))
@


1.9
log
@Nuke unused pmap_from_phys_address().
@
text
@d144 1
a144 1
#define VA_ALIAS_MASK   (1<<14)	
@


1.8
log
@Needs a non-empty pmap_unuse_final() as sparc.
Spotted by: art@@ No cookie for: deraadt@@
@
text
@a161 1
#define pmap_from_phys_address(x,f)	((x)&~PGOFSET)
@


1.7
log
@rename sparc kill_user_windows() to pmap_unuse_final().  provide empty stubs
on all other architectures.  remove last architecture dependent #ifdef from
uvm code.
@
text
@a166 1
#define pmap_unuse_final(p)		/* nothing */
@


1.6
log
@Properly flush instruction cache for ptrace(PT_WRTIE_{DI}, ...) on powerpc
and m68k.
ok drahn@@, millert@@
@
text
@d167 1
@


1.5
log
@Final __P removal plus some cosmetic fixups
@
text
@d166 2
@


1.4
log
@First round of __P removal in sys
@
text
@d175 1
a175 2
int             pmap_dumpmmu __P((int (*)(dev_t, daddr_t, caddr_t, size_t),
                                 daddr_t));
@


1.3
log
@Implement vgafb_mmap() fully
From NetBSD: change bus_space_mmap() prototype to standard one
@
text
@d159 2
a160 2
int pmap_count_res __P((pmap_t pmap));
/* int pmap_change_wiring __P((pmap_t pm, vaddr_t va, boolean_t wired)); */
d166 1
a166 1
void pmap_bootstrap __P((u_long kernelstart, u_long kernelend, u_int numctx));
d173 3
a175 3
void		pmap_redzone __P((void));
int             pmap_dumpsize __P((void));
int             pmap_dumpmmu __P((int (*)__P((dev_t, daddr_t, caddr_t, size_t)),
d177 1
a177 1
int		pmap_pa_exists __P((paddr_t));
d179 1
a179 1
void		switchexit __P((struct proc *));
d182 2
a183 2
int	ctx_alloc __P((struct pmap*));
void	ctx_free __P((struct pmap*));
@


1.2
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d161 3
a163 3
#define pmap_resident_count(pm)		pmap_count_res((pm))
#define pmap_from_phys_address(x,f)	((x)>>PGSHIFT)
#define	pmap_phys_address(x)		((((paddr_t)(x))<<PGSHIFT)|PMAP_NC)
@


1.2.2.1
log
@Sync UBC branch to -current
@
text
@d159 5
a163 5
int pmap_count_res(pmap_t pmap);
/* int pmap_change_wiring(pmap_t pm, vaddr_t va, boolean_t wired); */
#define	pmap_resident_count(pm)		pmap_count_res((pm))
#define pmap_from_phys_address(x,f)	((x)&~PGOFSET)
#define	pmap_phys_address(x)		(x)
d166 1
a166 1
void pmap_bootstrap(u_long kernelstart, u_long kernelend, u_int numctx);
d173 5
a177 4
void		pmap_redzone(void);
int             pmap_dumpsize(void);
int             pmap_dumpmmu(int (*)(dev_t, daddr_t, caddr_t, size_t), daddr_t);
int		pmap_pa_exists(paddr_t);
d179 1
a179 1
void		switchexit(struct proc *);
d182 2
a183 2
int	ctx_alloc(struct pmap*);
void	ctx_free(struct pmap*);
@


1.1
log
@Some more includes from NetBSD.
No modifications.
@
text
@d164 1
a164 1
#define	pmap_update()			/* nothing (yet) */
@


1.1.4.1
log
@Sync the SMP branch to something just after 3.0
@
text
@@


1.1.4.2
log
@Merge in trunk
@
text
@d161 4
a164 4
#define	pmap_resident_count(pm)		pmap_count_res((pm))
#define pmap_from_phys_address(x,f)	((x)&~PGOFSET)
#define	pmap_phys_address(x)		(x)
#define	pmap_update(pm)			/* nothing (yet) */
@


1.1.4.3
log
@Merge in -current from about a week ago
@
text
@d159 2
a160 2
int pmap_count_res(pmap_t pmap);
/* int pmap_change_wiring(pmap_t pm, vaddr_t va, boolean_t wired); */
d166 1
a166 1
void pmap_bootstrap(u_long kernelstart, u_long kernelend, u_int numctx);
d173 5
a177 4
void		pmap_redzone(void);
int             pmap_dumpsize(void);
int             pmap_dumpmmu(int (*)(dev_t, daddr_t, caddr_t, size_t), daddr_t);
int		pmap_pa_exists(paddr_t);
d179 1
a179 1
void		switchexit(struct proc *);
d182 2
a183 2
int	ctx_alloc(struct pmap*);
void	ctx_free(struct pmap*);
@


1.1.4.4
log
@Merge with the trunk
@
text
@a165 2
#define pmap_proc_iflush(p,va,len)	/* nothing */

@


