head	1.17;
access;
symbols
	OPENBSD_6_1:1.12.0.4
	OPENBSD_6_1_BASE:1.12
	OPENBSD_6_0:1.11.0.2
	OPENBSD_6_0_BASE:1.11
	OPENBSD_5_9:1.10.0.2
	OPENBSD_5_9_BASE:1.10
	OPENBSD_5_8:1.10.0.4
	OPENBSD_5_8_BASE:1.10
	OPENBSD_5_7:1.9.0.2
	OPENBSD_5_7_BASE:1.9
	OPENBSD_5_6:1.7.0.4
	OPENBSD_5_6_BASE:1.7
	OPENBSD_5_5:1.5.0.4
	OPENBSD_5_5_BASE:1.5
	OPENBSD_5_4:1.3.0.4
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.2
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.1.0.20
	OPENBSD_5_2_BASE:1.1
	OPENBSD_5_1_BASE:1.1
	OPENBSD_5_1:1.1.0.18
	OPENBSD_5_0:1.1.0.16
	OPENBSD_5_0_BASE:1.1
	OPENBSD_4_9:1.1.0.14
	OPENBSD_4_9_BASE:1.1
	OPENBSD_4_8:1.1.0.12
	OPENBSD_4_8_BASE:1.1
	OPENBSD_4_7:1.1.0.8
	OPENBSD_4_7_BASE:1.1
	OPENBSD_4_6:1.1.0.10
	OPENBSD_4_6_BASE:1.1
	OPENBSD_4_5:1.1.0.6
	OPENBSD_4_5_BASE:1.1
	OPENBSD_4_4:1.1.0.4
	OPENBSD_4_4_BASE:1.1
	OPENBSD_4_3:1.1.0.2
	OPENBSD_4_3_BASE:1.1;
locks; strict;
comment	@ * @;


1.17
date	2017.05.29.14.19.50;	author mpi;	state Exp;
branches;
next	1.16;
commitid	4u6PWvBw90PH7UDq;

1.16
date	2017.05.27.15.11.03;	author mpi;	state Exp;
branches;
next	1.15;
commitid	cGnhYW39Xic3xNsx;

1.15
date	2017.05.25.03.19.39;	author dlg;	state Exp;
branches;
next	1.14;
commitid	gx8rjMxrMcqYnydg;

1.14
date	2017.04.30.16.45.45;	author mpi;	state Exp;
branches;
next	1.13;
commitid	2Gtqjzrin9LL2yHk;

1.13
date	2017.04.04.12.30.04;	author visa;	state Exp;
branches;
next	1.12;
commitid	rNYAjvonXVwVY68r;

1.12
date	2017.03.07.14.41.57;	author visa;	state Exp;
branches;
next	1.11;
commitid	QuoJY64mTVcW3iGY;

1.11
date	2016.03.19.11.34.22;	author mpi;	state Exp;
branches;
next	1.10;
commitid	15xZY6veDWwRM6Iq;

1.10
date	2015.06.25.00.53.19;	author dlg;	state Exp;
branches;
next	1.9;
commitid	ge9NUodOFyzzh2Cp;

1.9
date	2015.02.11.04.38.26;	author dlg;	state Exp;
branches;
next	1.8;
commitid	Spk54skBCmzpcd9b;

1.8
date	2014.11.30.22.26.14;	author kettenis;	state Exp;
branches;
next	1.7;
commitid	SuwbTdviYcmeB8QU;

1.7
date	2014.03.29.18.09.30;	author guenther;	state Exp;
branches;
next	1.6;

1.6
date	2014.03.14.01.20.44;	author dlg;	state Exp;
branches;
next	1.5;

1.5
date	2014.01.30.00.51.13;	author dlg;	state Exp;
branches;
next	1.4;

1.4
date	2013.12.05.01.28.45;	author uebayasi;	state Exp;
branches;
next	1.3;

1.3
date	2012.10.29.21.58.07;	author kettenis;	state Exp;
branches;
next	1.2;

1.2
date	2012.08.30.20.57.00;	author kettenis;	state Exp;
branches;
next	1.1;

1.1
date	2007.11.27.23.29.57;	author kettenis;	state Exp;
branches;
next	;


desc
@@


1.17
log
@Kill SPINLOCK_SPIN_HOOK, use CPU_BUSY_CYCLE() instead.

ok visa@@, kettenis@@
@
text
@/*	$OpenBSD: lock_machdep.c,v 1.16 2017/05/27 15:11:03 mpi Exp $	*/

/*
 * Copyright (c) 2007 Artur Grabowski <art@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/atomic.h>
#include <sys/param.h>
#include <sys/systm.h>

#include <machine/cpu.h>
#include <machine/psl.h>

#include <ddb/db_output.h>

void
__mp_lock_init(struct __mp_lock *mpl)
{
	memset(mpl->mpl_cpus, 0, sizeof(mpl->mpl_cpus));
	mpl->mpl_users = 0;
	mpl->mpl_ticket = 1;
}

#if defined(MP_LOCKDEBUG)
#ifndef DDB
#error "MP_LOCKDEBUG requires DDB"
#endif

/* CPU-dependent timing, needs this to be settable from ddb. */
extern int __mp_lock_spinout;
#endif

static __inline void
__mp_lock_spin(struct __mp_lock *mpl, u_int me)
{
#ifndef MP_LOCKDEBUG
	while (mpl->mpl_ticket != me)
		CPU_BUSY_CYCLE();
#else
	int nticks = __mp_lock_spinout;

	while (mpl->mpl_ticket != me) {
		CPU_BUSY_CYCLE();

		if (--nticks <= 0) {
			db_printf("__mp_lock(%p): lock spun out", mpl);
			db_enter();
			nticks = __mp_lock_spinout;
		}
	}
#endif
}

void
__mp_lock(struct __mp_lock *mpl)
{
	struct __mp_lock_cpu *cpu = &mpl->mpl_cpus[cpu_number()];
	u_int64_t s;

	s = intr_disable();
	if (cpu->mplc_depth++ == 0)
		cpu->mplc_ticket = atomic_inc_int_nv(&mpl->mpl_users);
	intr_restore(s);

	__mp_lock_spin(mpl, cpu->mplc_ticket);
	membar_enter();
}

void
__mp_unlock(struct __mp_lock *mpl)
{
	struct __mp_lock_cpu *cpu = &mpl->mpl_cpus[cpu_number()];
	u_int64_t s;

	s = intr_disable();
	if (--cpu->mplc_depth == 0) {
		membar_exit();
		mpl->mpl_ticket++;
	}
	intr_restore(s);
}

int
__mp_release_all(struct __mp_lock *mpl)
{
	struct __mp_lock_cpu *cpu = &mpl->mpl_cpus[cpu_number()];
	u_int64_t s;
	int rv;

	s = intr_disable();
	rv = cpu->mplc_depth;
	cpu->mplc_depth = 0;
	membar_exit();
	mpl->mpl_ticket++;
	intr_restore(s);

	return (rv);
}

int
__mp_release_all_but_one(struct __mp_lock *mpl)
{
	struct __mp_lock_cpu *cpu = &mpl->mpl_cpus[cpu_number()];
	u_int64_t s;
	int rv;

	s = intr_disable();
	rv = cpu->mplc_depth;
	cpu->mplc_depth = 1;
	intr_restore(s);

	return (rv - 1);
}

void
__mp_acquire_count(struct __mp_lock *mpl, int count)
{
	while (count--)
		__mp_lock(mpl);
}

int
__mp_lock_held(struct __mp_lock *mpl)
{
	struct __mp_lock_cpu *cpu = &mpl->mpl_cpus[cpu_number()];

	return (cpu->mplc_ticket == mpl->mpl_ticket && cpu->mplc_depth > 0);
}
@


1.16
log
@Move SPINLOCK_SPIN_HOOK to the header used by other archs in order to
prepare the terrain for MI locks.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.15 2017/05/25 03:19:39 dlg Exp $	*/
d23 1
a23 1
#include <machine/lock.h>
d50 1
a50 1
		SPINLOCK_SPIN_HOOK;
d55 1
a55 1
		SPINLOCK_SPIN_HOOK;
@


1.15
log
@tweak sparc64 membars as a step toward making them usable in userland.

specifically, dont rely on magic in ctlreg to implement membars. moving
that to atomic.h would add a lot of pollution to the namespace, so
move to passing the membar options to a single __membar macro.

this tweaks everything that was using the ctlreg backend to either use
an appropriate membar_foo(), or to use __membar() in the MD code.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.14 2017/04/30 16:45:45 mpi Exp $	*/
a43 44

/*
 * On processors with multiple threads we force a thread switch.
 *
 * On UltraSPARC T2 and its successors, the optimal way to do this
 * seems to be to do three nop reads of %ccr.  This works on
 * UltraSPARC T1 as well, even though three nop casx operations seem
 * to be slightly more optimal.  Since these instructions are
 * effectively nops, executing them on earlier non-CMT processors is
 * harmless, so we make this the default.
 *
 * On SPARC T4 and later, we can use the processor-specific pause
 * instruction.
 *
 * On SPARC64 VI and its successors we execute the processor-specific
 * sleep instruction.
 */
static __inline void
__mp_lock_spin_hook(void)
{
	__asm volatile(
		"999:	rd	%%ccr, %%g0			\n"
		"	rd	%%ccr, %%g0			\n"
		"	rd	%%ccr, %%g0			\n"
		"	.section .sun4v_pause_patch, \"ax\"	\n"
		"	.word	999b				\n"
		"	.word	0xb7802080	! pause	128	\n"
		"	.word	999b + 4			\n"
		"	nop					\n"
		"	.word	999b + 8			\n"
		"	nop					\n"
		"	.previous				\n"
		"	.section .sun4u_mtp_patch, \"ax\"	\n"
		"	.word	999b				\n"
		"	.word	0x81b01060	! sleep		\n"
		"	.word	999b + 4			\n"
		"	nop					\n"
		"	.word	999b + 8			\n"
		"	nop					\n"
		"	.previous				\n"
		: : : "memory");
}

#define SPINLOCK_SPIN_HOOK __mp_lock_spin_hook()
@


1.14
log
@Rename Debugger() into db_enter().

Using a name with the 'db_' prefix makes it invisible from the dynamic
profiler.

ok deraadt@@, kettenis@@, visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.13 2017/04/04 12:30:04 visa Exp $	*/
d122 1
a122 1
	sparc_membar(LoadLoad | LoadStore);
d133 1
a133 1
		sparc_membar(StoreStore | LoadStore);
d149 1
a149 1
	sparc_membar(StoreStore | LoadStore);
@


1.13
log
@Issue memory barrier before lock release, not after. This ensures
the release write becomes globally visible only after any writes
of the critical section are globally visible. In practice, the
reordering has not happened because the kernel runs in the total
store order mode.

Tested by and OK kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.12 2017/03/07 14:41:57 visa Exp $	*/
d103 1
a103 1
			Debugger();
@


1.12
log
@Keep on trying to grab the lock after leaving ddb after lock spin-out.
This restores the behaviour that preceded ticket locks. The feature can
be useful in some debug cases where the system is not totally borken.

OK guenther@@, dlg@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.11 2016/03/19 11:34:22 mpi Exp $	*/
d133 1
a134 1
		sparc_membar(StoreStore | LoadStore);
d149 1
a150 1
	sparc_membar(StoreStore | LoadStore);
@


1.11
log
@Reduces the noise around the global ``ticks'' variable by renaming
all the local ones to ``nticks''.

ok stefan@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.10 2015/06/25 00:53:19 dlg Exp $	*/
d98 1
a98 1
	while (mpl->mpl_ticket != me && --nticks > 0)
d101 5
a105 3
	if (nticks == 0) {
		db_printf("__mp_lock(%p): lock spun out", mpl);
		Debugger();
@


1.10
log
@fix format string in MP_LOCKDEBUG code
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.9 2015/02/11 04:38:26 dlg Exp $	*/
d96 1
a96 1
	int ticks = __mp_lock_spinout;
d98 1
a98 1
	while (mpl->mpl_ticket != me && --ticks > 0)
d101 1
a101 1
	if (ticks == 0) {
@


1.9
log
@dont need lockmgr locks here.
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.8 2014/11/30 22:26:14 kettenis Exp $	*/
d102 1
a102 1
		db_printf("__mp_lock(0x%x): lock spun out", mpl);
@


1.8
log
@SPARC T4 and later have a pause instruction to voluntarily pause a virtual
processor in order to give other strands a chance to run.  Use it in
__mp_lock_spin_hook() to avoid wasting CPU cycles if we're waiting for
the kernel or scheduler locks.  This is instruction is patched in, just like
we already do for the sleep instruction on SPARC64 VI processors.  We look
at the hwcap-list property of the cpu nodes in the machine description to
decide whether the pause instruction is available.
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.7 2014/03/29 18:09:30 guenther Exp $	*/
a20 1
#include <sys/lock.h>
@


1.7
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.6 2014/03/14 01:20:44 dlg Exp $	*/
d56 4
a59 1
 * On SPARC64 VI and it successors we execute the processor-specific
d69 8
@


1.6
log
@rework mplock as ticket locks instead of spinlocks. this provides
fairer access to the kernel lock (which suffers very heavy contention
at the moment).

i wrote this at n2k14 and reworked it shortly after to use the mi
atomic ops api when that was agreed on. ive been running it on my
systems ever since.

the version before the atomic op tweaks was tested by kettenis@@
n2k14 (deraadt@@ and kettenis@@ in particular if i recall correctly)
oked this going in after 5.5
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.5 2014/01/30 00:51:13 dlg Exp $	*/
d62 1
a62 1
	__asm __volatile(
@


1.5
log
@move sparc64 behind the MI atomic api.

this basically replaces sparc64_cas and sparc64_casx with atomic_cas_uint
and atomic_cas_ulong respectively. it then builds atomic_add and
atomic_sub out of those. this avoids the gcc atomic builtins that
the MI atomic_foo api uses by default, so we dont get the extra
membars that the builtins do but the atomic_foo api doesnt promise.

it also fixes up the code that used to use sparc64_{cas,casx} to
use the atomic_cas api instead.

use of the sparc64 membar() macros are left untouched for now.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.4 2013/12/05 01:28:45 uebayasi Exp $	*/
d30 1
a30 1
__mp_lock_init(struct __mp_lock *lock)
d32 3
a34 2
	lock->mpl_cpu = NULL;
	lock->mpl_count = 0;
d80 1
a80 1
__mp_lock_spin(struct __mp_lock *mpl)
d83 1
a83 1
	while (mpl->mpl_count != 0)
d88 1
a88 1
	while (mpl->mpl_count != 0 && --ticks > 0)
d101 7
a107 27
	/*
	 * Please notice that mpl_count gets incremented twice for the
	 * first lock. This is on purpose. The way we release the lock
	 * in mp_unlock is to decrement the mpl_count and then check if
	 * the lock should be released. Since mpl_count is what we're
	 * spinning on, decrementing it in mpl_unlock to 0 means that
	 * we can't clear mpl_cpu, because we're no longer holding the
	 * lock. In theory mpl_cpu doesn't need to be cleared, but it's
	 * safer to clear it and besides, setting mpl_count to 2 on the
	 * first lock makes most of this code much simpler.
	 */

	while (1) {
		u_int64_t s;

		s = intr_disable();
		if (atomic_cas_ulong(&mpl->mpl_count, 0, 1) == 0) {
			sparc_membar(LoadLoad | LoadStore);
			mpl->mpl_cpu = curcpu();
		}

		if (mpl->mpl_cpu == curcpu()) {
			mpl->mpl_count++;
			intr_restore(s);
			break;
		}
		intr_restore(s);
d109 2
a110 2
		__mp_lock_spin(mpl);
	}
d116 1
a118 7
#ifdef MP_LOCKDEBUG
	if (mpl->mpl_cpu != curcpu()) {
		db_printf("__mp_unlock(%p): not held lock\n", mpl);
		Debugger();
	}
#endif

d120 2
a121 2
	if (--mpl->mpl_count == 1) {
		mpl->mpl_cpu = NULL;
a122 1
		mpl->mpl_count = 0;
d130 1
a130 1
	int rv = mpl->mpl_count - 1;
d132 1
a132 7

#ifdef MP_LOCKDEBUG
	if (mpl->mpl_cpu != curcpu()) {
		db_printf("__mp_release_all(%p): not held lock\n", mpl);
		Debugger();
	}
#endif
d135 3
a137 1
	mpl->mpl_cpu = NULL;
a138 1
	mpl->mpl_count = 0;
d147 3
a149 1
	int rv = mpl->mpl_count - 2;
d151 4
a154 8
#ifdef MP_LOCKDEBUG
	if (mpl->mpl_cpu != curcpu()) {
		db_printf("__mp_release_all_but_one(%p): not held lock\n", mpl);
		Debugger();
	}
#endif

	mpl->mpl_count = 2;
d156 1
a156 1
	return (rv);
d169 3
a171 1
	return mpl->mpl_cpu == curcpu();
@


1.4
log
@Correct spin timeout detection in __mp_lock debug code.

OK pirofti@@ krw@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d19 1
a19 1

a23 1
#include <machine/atomic.h>
d116 1
a116 1
		if (sparc64_casx(&mpl->mpl_count, 0, 1) == 0) {
@


1.3
log
@On UltraSPARC T1/T2, block the current strand while spinning in the hope
other strands can do some useful work.  Idea stolen from Linux.
Results in a small, but measurable speedup doing a kernel build and reduces
the system time by almost 10%.
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.2 2012/08/30 20:57:00 kettenis Exp $	*/
d88 1
a88 1
	while (mpl->mpl_count != 0 && ticks-- > 0)
@


1.2
log
@On SPARC64 VI/VII CPUs, use the sleep instruction while spinning to force a
thread switch in the hope the other thread can do some useful work.
@
text
@d1 1
a1 1
/*	$OpenBSD: lock_machdep.c,v 1.1 2007/11/27 23:29:57 kettenis Exp $	*/
d46 13
d63 3
a65 1
		"999:	nop					\n"
d69 4
@


1.1
log
@Like i386 and amd64 - make the __mp_lock not spin at splhigh.

help from & ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d46 13
a58 1
#define SPINLOCK_SPIN_HOOK	/**/
@

