head	1.101;
access;
symbols
	OPENBSD_6_2_BASE:1.101
	OPENBSD_6_1:1.98.0.4
	OPENBSD_6_1_BASE:1.98
	OPENBSD_6_0:1.97.0.2
	OPENBSD_6_0_BASE:1.97
	OPENBSD_5_9:1.96.0.2
	OPENBSD_5_9_BASE:1.96
	OPENBSD_5_8:1.92.0.4
	OPENBSD_5_8_BASE:1.92
	OPENBSD_5_7:1.88.0.2
	OPENBSD_5_7_BASE:1.88
	OPENBSD_5_6:1.82.0.4
	OPENBSD_5_6_BASE:1.82
	OPENBSD_5_5:1.81.0.4
	OPENBSD_5_5_BASE:1.81
	OPENBSD_5_4:1.79.0.2
	OPENBSD_5_4_BASE:1.79
	OPENBSD_5_3:1.78.0.2
	OPENBSD_5_3_BASE:1.78
	OPENBSD_5_2:1.76.0.2
	OPENBSD_5_2_BASE:1.76
	OPENBSD_5_1_BASE:1.75
	OPENBSD_5_1:1.75.0.2
	OPENBSD_5_0:1.74.0.2
	OPENBSD_5_0_BASE:1.74
	OPENBSD_4_9:1.71.0.2
	OPENBSD_4_9_BASE:1.71
	OPENBSD_4_8:1.70.0.2
	OPENBSD_4_8_BASE:1.70
	OPENBSD_4_7:1.68.0.4
	OPENBSD_4_7_BASE:1.68
	OPENBSD_4_6:1.68.0.6
	OPENBSD_4_6_BASE:1.68
	OPENBSD_4_5:1.68.0.2
	OPENBSD_4_5_BASE:1.68
	OPENBSD_4_4:1.67.0.2
	OPENBSD_4_4_BASE:1.67
	OPENBSD_4_3:1.50.0.2
	OPENBSD_4_3_BASE:1.50
	OPENBSD_4_2:1.42.0.2
	OPENBSD_4_2_BASE:1.42
	OPENBSD_4_1:1.34.0.2
	OPENBSD_4_1_BASE:1.34
	OPENBSD_4_0:1.32.0.2
	OPENBSD_4_0_BASE:1.32
	OPENBSD_3_9:1.27.0.2
	OPENBSD_3_9_BASE:1.27
	OPENBSD_3_8:1.26.0.8
	OPENBSD_3_8_BASE:1.26
	OPENBSD_3_7:1.26.0.6
	OPENBSD_3_7_BASE:1.26
	OPENBSD_3_6:1.26.0.4
	OPENBSD_3_6_BASE:1.26
	SMP_SYNC_A:1.26
	SMP_SYNC_B:1.26
	OPENBSD_3_5:1.26.0.2
	OPENBSD_3_5_BASE:1.26
	OPENBSD_3_4:1.25.0.2
	OPENBSD_3_4_BASE:1.25
	UBC_SYNC_A:1.24
	OPENBSD_3_3:1.22.0.2
	OPENBSD_3_3_BASE:1.22
	OPENBSD_3_2:1.18.0.2
	OPENBSD_3_2_BASE:1.18
	OPENBSD_3_1:1.12.0.2
	OPENBSD_3_1_BASE:1.12
	UBC_SYNC_B:1.21
	UBC:1.8.0.2
	UBC_BASE:1.8
	SMP:1.5.0.4
	OPENBSD_3_0:1.5.0.2
	OPENBSD_3_0_BASE:1.5;
locks; strict;
comment	@ * @;


1.101
date	2017.05.27.09.03.22;	author kettenis;	state Exp;
branches;
next	1.100;
commitid	MflTTasSNUFRC8ey;

1.100
date	2017.05.03.12.27.00;	author dlg;	state Exp;
branches;
next	1.99;
commitid	sETXIFsrddhVZgAr;

1.99
date	2017.04.30.16.45.45;	author mpi;	state Exp;
branches;
next	1.98;
commitid	2Gtqjzrin9LL2yHk;

1.98
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.97;
commitid	RlO92XR575sygHqm;

1.97
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.96;
commitid	N0upL0onl7Raz5yi;

1.96
date	2015.11.27.15.34.01;	author kettenis;	state Exp;
branches;
next	1.95;
commitid	4uIZG2uCNich8yRN;

1.95
date	2015.11.25.21.13.28;	author kettenis;	state Exp;
branches;
next	1.94;
commitid	IPX4K6QyvYZ67Qs8;

1.94
date	2015.09.02.21.59.29;	author kettenis;	state Exp;
branches;
next	1.93;
commitid	5HEqMDqkPZoGPYNv;

1.93
date	2015.08.30.16.47.43;	author kettenis;	state Exp;
branches;
next	1.92;
commitid	UEwQgblwVKb0d1Wz;

1.92
date	2015.07.10.12.11.41;	author kettenis;	state Exp;
branches;
next	1.91;
commitid	4rnD01czf1JsAAGg;

1.91
date	2015.04.10.18.08.31;	author kettenis;	state Exp;
branches;
next	1.90;
commitid	t193Yy00k07u6N3G;

1.90
date	2015.04.09.16.56.52;	author kettenis;	state Exp;
branches;
next	1.89;
commitid	hrRKJaqwTtvGGgzQ;

1.89
date	2015.04.08.14.02.43;	author kettenis;	state Exp;
branches;
next	1.88;
commitid	KY2mkOl0YhD88gRZ;

1.88
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.87;
commitid	eahBabNpxnDWKzqJ;

1.87
date	2015.02.10.01.59.43;	author kettenis;	state Exp;
branches;
next	1.86;
commitid	plxP1A2rOAysnnsc;

1.86
date	2015.01.10.19.52.37;	author kettenis;	state Exp;
branches;
next	1.85;
commitid	7zAhrm88wErqcpCp;

1.85
date	2014.12.17.06.58.10;	author guenther;	state Exp;
branches;
next	1.84;
commitid	DImukoCWyTxwdbuh;

1.84
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.83;
commitid	ZxaujiOM0aYQRjFY;

1.83
date	2014.11.16.12.30.59;	author deraadt;	state Exp;
branches;
next	1.82;
commitid	yv0ECmCdICvq576h;

1.82
date	2014.07.08.17.19.25;	author deraadt;	state Exp;
branches;
next	1.81;
commitid	EF98ch02VpFassUi;

1.81
date	2014.01.30.00.51.13;	author dlg;	state Exp;
branches;
next	1.80;

1.80
date	2013.09.21.10.04.42;	author miod;	state Exp;
branches;
next	1.79;

1.79
date	2013.06.11.16.42.12;	author deraadt;	state Exp;
branches;
next	1.78;

1.78
date	2012.08.29.20.33.16;	author kettenis;	state Exp;
branches;
next	1.77;

1.77
date	2012.08.29.19.54.32;	author kettenis;	state Exp;
branches;
next	1.76;

1.76
date	2012.04.26.21.32.20;	author okan;	state Exp;
branches;
next	1.75;

1.75
date	2011.09.22.17.41.00;	author jasper;	state Exp;
branches;
next	1.74;

1.74
date	2011.05.30.22.25.22;	author oga;	state Exp;
branches;
next	1.73;

1.73
date	2011.04.26.23.50.21;	author ariane;	state Exp;
branches;
next	1.72;

1.72
date	2011.04.07.15.30.16;	author miod;	state Exp;
branches;
next	1.71;

1.71
date	2010.11.20.20.33.24;	author miod;	state Exp;
branches;
next	1.70;

1.70
date	2010.04.20.23.27.00;	author deraadt;	state Exp;
branches;
next	1.69;

1.69
date	2010.04.15.21.14.18;	author kettenis;	state Exp;
branches;
next	1.68;

1.68
date	2009.02.12.18.53.14;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2008.07.27.20.33.23;	author kettenis;	state Exp;
branches;
next	1.66;

1.66
date	2008.07.25.19.37.16;	author kettenis;	state Exp;
branches;
next	1.65;

1.65
date	2008.07.05.21.20.48;	author kettenis;	state Exp;
branches;
next	1.64;

1.64
date	2008.07.04.17.40.25;	author kettenis;	state Exp;
branches;
next	1.63;

1.63
date	2008.06.14.10.55.20;	author mk;	state Exp;
branches;
next	1.62;

1.62
date	2008.06.09.20.32.56;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2008.06.09.16.55.20;	author kettenis;	state Exp;
branches;
next	1.60;

1.60
date	2008.05.21.19.23.15;	author kettenis;	state Exp;
branches;
next	1.59;

1.59
date	2008.04.03.23.10.25;	author kettenis;	state Exp;
branches;
next	1.58;

1.58
date	2008.04.02.20.23.22;	author kettenis;	state Exp;
branches;
next	1.57;

1.57
date	2008.03.31.22.14.01;	author kettenis;	state Exp;
branches;
next	1.56;

1.56
date	2008.03.30.12.30.01;	author kettenis;	state Exp;
branches;
next	1.55;

1.55
date	2008.03.23.23.46.21;	author kettenis;	state Exp;
branches;
next	1.54;

1.54
date	2008.03.22.16.01.32;	author kettenis;	state Exp;
branches;
next	1.53;

1.53
date	2008.03.19.23.16.19;	author kettenis;	state Exp;
branches;
next	1.52;

1.52
date	2008.03.19.20.42.05;	author kettenis;	state Exp;
branches;
next	1.51;

1.51
date	2008.03.15.22.05.51;	author kettenis;	state Exp;
branches;
next	1.50;

1.50
date	2008.01.10.20.37.14;	author marco;	state Exp;
branches;
next	1.49;

1.49
date	2007.12.23.21.43.30;	author kettenis;	state Exp;
branches;
next	1.48;

1.48
date	2007.12.05.19.43.15;	author kettenis;	state Exp;
branches;
next	1.47;

1.47
date	2007.11.28.19.37.23;	author kettenis;	state Exp;
branches;
next	1.46;

1.46
date	2007.10.27.22.20.16;	author martin;	state Exp;
branches;
next	1.45;

1.45
date	2007.10.18.20.44.47;	author kettenis;	state Exp;
branches;
next	1.44;

1.44
date	2007.10.17.21.23.28;	author kettenis;	state Exp;
branches;
next	1.43;

1.43
date	2007.09.09.14.59.37;	author kettenis;	state Exp;
branches;
next	1.42;

1.42
date	2007.06.06.17.15.13;	author deraadt;	state Exp;
branches;
next	1.41;

1.41
date	2007.05.20.15.11.27;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2007.05.02.18.46.07;	author kettenis;	state Exp;
branches;
next	1.39;

1.39
date	2007.04.22.18.13.04;	author art;	state Exp;
branches;
next	1.38;

1.38
date	2007.04.21.13.43.38;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2007.04.05.20.08.30;	author claudio;	state Exp;
branches;
next	1.35;

1.35
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2007.01.12.19.19.34;	author kettenis;	state Exp;
branches;
next	1.33;

1.33
date	2007.01.06.21.08.07;	author kettenis;	state Exp;
branches;
next	1.32;

1.32
date	2006.07.01.16.23.31;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2006.06.20.20.31.04;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2006.06.16.23.04.49;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2006.06.02.19.53.32;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2006.06.02.00.26.08;	author kettenis;	state Exp;
branches;
next	1.27;

1.27
date	2005.11.11.16.38.30;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2003.11.03.07.01.33;	author david;	state Exp;
branches;
next	1.25;

1.25
date	2003.06.03.17.16.33;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2003.05.11.22.05.49;	author jason;	state Exp;
branches;
next	1.23;

1.23
date	2003.05.10.00.35.42;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2003.02.17.01.29.20;	author henric;	state Exp;
branches;
next	1.21;

1.21
date	2002.10.12.02.03.45;	author krw;	state Exp;
branches;
next	1.20;

1.20
date	2002.10.07.18.35.57;	author mickey;	state Exp;
branches;
next	1.19;

1.19
date	2002.10.06.22.06.15;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2002.09.18.10.36.50;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2002.09.10.18.29.44;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2002.08.20.19.28.55;	author jason;	state Exp;
branches;
next	1.15;

1.15
date	2002.07.24.00.48.25;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2002.07.20.20.19.11;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2002.06.11.11.16.46;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2002.03.14.03.16.01;	author millert;	state Exp;
branches;
next	1.11;

1.11
date	2002.03.14.01.26.45;	author millert;	state Exp;
branches;
next	1.10;

1.10
date	2002.01.25.15.43.59;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.8.2.1;
next	1.7;

1.7
date	2001.11.06.19.53.16;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2001.11.06.03.36.57;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2001.09.26.17.32.19;	author deraadt;	state Exp;
branches
	1.5.4.1;
next	1.4;

1.4
date	2001.09.20.23.23.59;	author jason;	state Exp;
branches;
next	1.3;

1.3
date	2001.09.19.20.50.57;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2001.08.20.20.23.53;	author jason;	state Exp;
branches;
next	1.1;

1.1
date	2001.08.19.05.21.38;	author jason;	state Exp;
branches;
next	;

1.5.4.1
date	2001.10.31.03.07.59;	author nate;	state Exp;
branches;
next	1.5.4.2;

1.5.4.2
date	2001.11.13.21.04.17;	author niklas;	state Exp;
branches;
next	1.5.4.3;

1.5.4.3
date	2002.03.06.02.04.47;	author niklas;	state Exp;
branches;
next	1.5.4.4;

1.5.4.4
date	2002.03.28.11.23.52;	author niklas;	state Exp;
branches;
next	1.5.4.5;

1.5.4.5
date	2003.03.27.23.42.37;	author niklas;	state Exp;
branches;
next	1.5.4.6;

1.5.4.6
date	2003.05.13.19.41.09;	author ho;	state Exp;
branches;
next	1.5.4.7;

1.5.4.7
date	2003.06.07.11.14.46;	author ho;	state Exp;
branches;
next	1.5.4.8;

1.5.4.8
date	2004.02.19.10.50.01;	author niklas;	state Exp;
branches;
next	;

1.8.2.1
date	2002.01.31.22.55.25;	author niklas;	state Exp;
branches;
next	1.8.2.2;

1.8.2.2
date	2002.06.11.03.38.44;	author art;	state Exp;
branches;
next	1.8.2.3;

1.8.2.3
date	2002.10.29.00.28.12;	author art;	state Exp;
branches;
next	1.8.2.4;

1.8.2.4
date	2003.05.19.21.46.58;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.101
log
@Claim physical memory allocated during early bootstrap from the PROM.
@
text
@/*	$OpenBSD: pmap.c,v 1.100 2017/05/03 12:27:00 dlg Exp $	*/
/*	$NetBSD: pmap.c,v 1.107 2001/08/31 16:47:41 eeh Exp $	*/
#undef	NO_VCACHE /* Don't forget the locked TLB in dostart */
/*
 * 
 * Copyright (C) 1996-1999 Eduardo Horvath.
 * All rights reserved.
 *
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *  
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR  ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR  BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

#include <sys/atomic.h>
#include <sys/param.h>
#include <sys/malloc.h>
#include <sys/queue.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/msgbuf.h>
#include <sys/pool.h>
#include <sys/exec.h>
#include <sys/core.h>
#include <sys/kcore.h>

#include <uvm/uvm.h>

#include <machine/pcb.h>
#include <machine/sparc64.h>
#include <machine/ctlreg.h>
#include <machine/hypervisor.h>
#include <machine/openfirm.h>
#include <machine/kcore.h>

#include "cache.h"

#ifdef DDB
#include <machine/db_machdep.h>
#include <ddb/db_command.h>
#include <ddb/db_sym.h>
#include <ddb/db_variables.h>
#include <ddb/db_extern.h>
#include <ddb/db_access.h>
#include <ddb/db_output.h>
#define db_enter()	__asm volatile("ta 1; nop");
#else
#define db_enter()
#define db_printf	printf
#endif

#define	MEG		(1<<20) /* 1MB */
#define	KB		(1<<10)	/* 1KB */

paddr_t cpu0paddr;/* XXXXXXXXXXXXXXXX */

extern int64_t asmptechk(int64_t *pseg[], int addr); /* DEBUG XXXXX */

/* These routines are in assembly to allow access thru physical mappings */
extern int64_t pseg_get(struct pmap*, vaddr_t addr);
extern int pseg_set(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);

/* XXX - temporary workaround for pmap_{copy,zero}_page api change */
void pmap_zero_phys(paddr_t pa);
void pmap_copy_phys(paddr_t src, paddr_t dst);

/*
 * Diatribe on ref/mod counting:
 *
 * First of all, ref/mod info must be non-volatile.  Hence we need to keep it
 * in the pv_entry structure for each page.  (We could bypass this for the 
 * vm_page, but that's a long story....)
 * 
 * This architecture has nice, fast traps with lots of space for software bits
 * in the TTE.  To accelerate ref/mod counts we make use of these features.
 *
 * When we map a page initially, we place a TTE in the page table.  It's 
 * inserted with the TLB_W and TLB_ACCESS bits cleared.  If a page is really
 * writeable we set the TLB_REAL_W bit for the trap handler.
 *
 * Whenever we take a TLB miss trap, the trap handler will set the TLB_ACCESS
 * bit in the approprate TTE in the page table.  Whenever we take a protection
 * fault, if the TLB_REAL_W bit is set then we flip both the TLB_W and TLB_MOD
 * bits to enable writing and mark the page as modified.
 *
 * This means that we may have ref/mod information all over the place.  The
 * pmap routines must traverse the page tables of all pmaps with a given page
 * and collect/clear all the ref/mod information and copy it into the pv_entry.
 */

#ifdef NO_VCACHE
#define FORCE_ALIAS	1
#else
#define FORCE_ALIAS	0
#endif

#define	PV_ALIAS	0x1LL
#define PV_REF		0x2LL
#define PV_MOD		0x4LL
#define PV_NVC		0x8LL
#define PV_NC		0x10LL
#define PV_WE		0x20LL		/* Debug -- track if this page was ever writable */
#define PV_MASK		(0x03fLL)
#define PV_VAMASK	(~(NBPG - 1))
#define PV_MATCH(pv,va)	(!((((pv)->pv_va) ^ (va)) & PV_VAMASK))
#define PV_SETVA(pv,va) ((pv)->pv_va = (((va) & PV_VAMASK) | (((pv)->pv_va) & PV_MASK)))

pv_entry_t	pv_table;	/* array of entries, one per page */
static struct pool pv_pool;
static struct pool pmap_pool;

pv_entry_t pmap_remove_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
pv_entry_t pmap_enter_pv(struct pmap *pm, pv_entry_t, vaddr_t va, paddr_t pa);
void	pmap_page_cache(struct pmap *pm, paddr_t pa, int mode);

void	pmap_bootstrap_cpu(paddr_t);

void	pmap_pinit(struct pmap *);
void	pmap_release(struct pmap *);
pv_entry_t pa_to_pvh(paddr_t);

u_int64_t first_phys_addr;

pv_entry_t
pa_to_pvh(paddr_t pa)
{
	struct vm_page *pg;

	pg = PHYS_TO_VM_PAGE(pa);
	return pg ? &pg->mdpage.pvent : NULL;
}

static __inline u_int
pmap_tte2flags(u_int64_t tte)
{
	if (CPU_ISSUN4V)
		return (((tte & SUN4V_TLB_ACCESS) ? PV_REF : 0) |
		    ((tte & SUN4V_TLB_MODIFY) ? PV_MOD : 0));
	else
		return (((tte & SUN4U_TLB_ACCESS) ? PV_REF : 0) |
		    ((tte & SUN4U_TLB_MODIFY) ? PV_MOD : 0));
}

/*
 * Here's the CPU TSB stuff.  It's allocated in pmap_bootstrap.
 */
pte_t *tsb_dmmu;
pte_t *tsb_immu;
int tsbsize;		/* tsbents = 512 * 2^tsbsize */
#define TSBENTS (512 << tsbsize)
#define	TSBSIZE	(TSBENTS * 16)

/*
 * The invalid tsb tag uses the fact that the last context we have is
 * never allocated.
 */
#define TSB_TAG_INVALID	(~0LL << 48)

#define TSB_DATA(g,sz,pa,priv,write,cache,aliased,valid,ie) \
  (CPU_ISSUN4V ?\
    SUN4V_TSB_DATA(g,sz,pa,priv,write,cache,aliased,valid,ie) : \
    SUN4U_TSB_DATA(g,sz,pa,priv,write,cache,aliased,valid,ie))

/* The same for sun4u and sun4v. */
#define TLB_V		SUN4U_TLB_V

/* Only used for DEBUG. */
#define TLB_NFO		(CPU_ISSUN4V ? SUN4V_TLB_NFO : SUN4U_TLB_NFO)

/*
 * UltraSPARC T1 & T2 implement only a 40-bit real address range, just
 * like older UltraSPARC CPUs.
 */
#define TLB_PA_MASK	SUN4U_TLB_PA_MASK

/* XXX */
#define TLB_TSB_LOCK	(CPU_ISSUN4V ? SUN4V_TLB_TSB_LOCK : SUN4U_TLB_TSB_LOCK)

#ifdef SUN4V
struct tsb_desc *tsb_desc;
#endif

struct pmap kernel_pmap_;

extern int physmem;
/*
 * Virtual and physical addresses of the start and end of kernel text
 * and data segments.
 */
vaddr_t ktext;
paddr_t ktextp;
vaddr_t ektext;
paddr_t ektextp;
vaddr_t kdata;
paddr_t kdatap;
vaddr_t ekdata;
paddr_t ekdatap;

static int npgs;
static struct mem_region memlist[8]; /* Pick a random size here */

vaddr_t	vmmap;			/* one reserved MI vpage for /dev/mem */

struct mem_region *mem, *avail, *orig;
int memsize;

static int memh = 0, vmemh = 0;	/* Handles to OBP devices */

static int ptelookup_va(vaddr_t va); /* sun4u */

static __inline void
tsb_invalidate(int ctx, vaddr_t va)
{
	int i;
	int64_t tag;

	i = ptelookup_va(va);
	tag = TSB_TAG(0, ctx, va);
	if (tsb_dmmu[i].tag == tag)
		atomic_cas_ulong((volatile unsigned long *)&tsb_dmmu[i].tag,
		    tag, TSB_TAG_INVALID);
	if (tsb_immu[i].tag == tag)
		atomic_cas_ulong((volatile unsigned long *)&tsb_immu[i].tag,
		    tag, TSB_TAG_INVALID);
}

struct prom_map *prom_map;
int prom_map_size;

#ifdef DEBUG
#define	PDB_BOOT	0x20000
#define	PDB_BOOT1	0x40000
int	pmapdebug = 0;

#define	BDPRINTF(n, f)	if (pmapdebug & (n)) prom_printf f
#else
#define	BDPRINTF(n, f)
#endif

/*
 *
 * A context is simply a small number that differentiates multiple mappings
 * of the same address.  Contexts on the spitfire are 13 bits, but could
 * be as large as 17 bits.
 *
 * Each context is either free or attached to a pmap.
 *
 * The context table is an array of pointers to psegs.  Just dereference
 * the right pointer and you get to the pmap segment tables.  These are
 * physical addresses, of course.
 *
 */
paddr_t *ctxbusy;	
int numctx;
#define CTXENTRY	(sizeof(paddr_t))
#define CTXSIZE		(numctx * CTXENTRY)

int pmap_get_page(paddr_t *, const char *, struct pmap *);
void pmap_free_page(paddr_t, struct pmap *);

/*
 * Support for big page sizes.  This maps the page size to the
 * page bits.  That is: these are the bits between 8K pages and
 * larger page sizes that cause aliasing.
 */
struct page_size_map page_size_map[] = {
	{ (4*1024*1024-1) & ~(8*1024-1), PGSZ_4M },
	{ (512*1024-1) & ~(8*1024-1), PGSZ_512K  },
	{ (64*1024-1) & ~(8*1024-1), PGSZ_64K  },
	{ (8*1024-1) & ~(8*1024-1), PGSZ_8K  },
	{ 0, PGSZ_8K&0  }
};

/*
 * Enter a TTE into the kernel pmap only.  Don't do anything else.
 * 
 * Use only during bootstrapping since it does no locking and 
 * can lose ref/mod info!!!!
 *
 */
static void
pmap_enter_kpage(vaddr_t va, int64_t data)
{
	paddr_t newp;

	newp = 0;
	while (pseg_set(pmap_kernel(), va, data, newp) == 1) {
		newp = 0;
		if (!pmap_get_page(&newp, NULL, pmap_kernel())) {
			prom_printf("pmap_enter_kpage: out of pages\n");
			panic("pmap_enter_kpage");
		}

		BDPRINTF(PDB_BOOT1, 
			 ("pseg_set: pm=%p va=%p data=%lx newp %lx\r\n",
			  pmap_kernel(), va, (long)data, (long)newp));
	}
}

/*
 * Check bootargs to see if we need to enable bootdebug.
 */
#ifdef DEBUG
void
pmap_bootdebug(void) 
{
	int chosen;
	char *cp;
	char buf[128];

	/*
	 * Grab boot args from PROM
	 */
	chosen = OF_finddevice("/chosen");
	/* Setup pointer to boot flags */
	OF_getprop(chosen, "bootargs", buf, sizeof(buf));
	cp = buf;
	while (*cp != '-')
		if (*cp++ == '\0')
			return;
	for (;;) 
		switch (*++cp) {
		case '\0':
			return;
		case 'V':
			pmapdebug |= PDB_BOOT|PDB_BOOT1;
			break;
		case 'D':
			pmapdebug |= PDB_BOOT1;
			break;
		}
}
#endif

/*
 * This is called during bootstrap, before the system is really initialized.
 *
 * It's called with the start and end virtual addresses of the kernel.  We
 * bootstrap the pmap allocator now.  We will allocate the basic structures we
 * need to bootstrap the VM system here: the page frame tables, the TSB, and
 * the free memory lists.
 *
 * Now all this is becoming a bit obsolete.  maxctx is still important, but by
 * separating the kernel text and data segments we really would need to
 * provide the start and end of each segment.  But we can't.  The rodata
 * segment is attached to the end of the kernel segment and has nothing to
 * delimit its end.  We could still pass in the beginning of the kernel and
 * the beginning and end of the data segment but we could also just as easily
 * calculate that all in here.
 *
 * To handle the kernel text, we need to do a reverse mapping of the start of
 * the kernel, then traverse the free memory lists to find out how big it is.
 */

void
pmap_bootstrap(u_long kernelstart, u_long kernelend, u_int maxctx, u_int numcpus)
{
	extern int data_start[], end[];	/* start of data segment */
	extern int msgbufmapped;
	struct mem_region *mp, *mp1;
	int msgbufsiz;
	int pcnt;
	size_t s, sz;
	int i, j;
	int64_t data;
	vaddr_t va;
	u_int64_t phys_msgbuf;
	paddr_t newkp;
	vaddr_t newkv, firstaddr, intstk;
	vsize_t kdsize, ktsize;

#ifdef DEBUG
	pmap_bootdebug();
#endif

	BDPRINTF(PDB_BOOT, ("Entered pmap_bootstrap.\r\n"));
	/*
	 * set machine page size
	 */
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();

	/*
	 * Find out how big the kernel's virtual address
	 * space is.  The *$#@@$ prom loses this info
	 */
	if ((vmemh = OF_finddevice("/virtual-memory")) == -1) {
		prom_printf("no virtual-memory?");
		OF_exit();
	}
	bzero((caddr_t)memlist, sizeof(memlist));
	if (OF_getprop(vmemh, "available", memlist, sizeof(memlist)) <= 0) {
		prom_printf("no vmemory avail?");
		OF_exit();
	}

#ifdef DEBUG
	if (pmapdebug & PDB_BOOT) {
		/* print out mem list */
		prom_printf("Available virtual memory:\r\n");
		for (mp = memlist; mp->size; mp++) {
			prom_printf("memlist start %p size %lx\r\n", 
				    (void *)(u_long)mp->start,
				    (u_long)mp->size);
		}
		prom_printf("End of available virtual memory\r\n");
	}
#endif
	/* 
	 * Get hold or the message buffer.
	 */
	msgbufp = (struct msgbuf *)(vaddr_t)MSGBUF_VA;
/* XXXXX -- increase msgbufsiz for uvmhist printing */
	msgbufsiz = 4*NBPG /* round_page(sizeof(struct msgbuf)) */;
	BDPRINTF(PDB_BOOT, ("Trying to allocate msgbuf at %lx, size %lx\r\n", 
			    (long)msgbufp, (long)msgbufsiz));
	if ((long)msgbufp !=
	    (long)(phys_msgbuf = prom_claim_virt((vaddr_t)msgbufp, msgbufsiz)))
		prom_printf(
		    "cannot get msgbuf VA, msgbufp=%p, phys_msgbuf=%lx\r\n", 
		    (void *)msgbufp, (long)phys_msgbuf);
	phys_msgbuf = prom_get_msgbuf(msgbufsiz, MMU_PAGE_ALIGN);
	BDPRINTF(PDB_BOOT, 
		("We should have the memory at %lx, let's map it in\r\n",
			phys_msgbuf));
	if (prom_map_phys(phys_msgbuf, msgbufsiz, (vaddr_t)msgbufp, 
			  -1/* sunos does this */) == -1)
		prom_printf("Failed to map msgbuf\r\n");
	else
		BDPRINTF(PDB_BOOT, ("msgbuf mapped at %p\r\n", 
			(void *)msgbufp));
	msgbufmapped = 1;	/* enable message buffer */
	initmsgbuf((caddr_t)msgbufp, msgbufsiz);

	/* 
	 * Record kernel mapping -- we will map these with a permanent 4MB
	 * TLB entry when we initialize the CPU later.
	 */
	BDPRINTF(PDB_BOOT, ("translating kernelstart %p\r\n", 
		(void *)kernelstart));
	ktext = kernelstart;
	ktextp = prom_vtop(kernelstart);

	kdata = (vaddr_t)data_start;
	kdatap = prom_vtop(kdata);
	ekdata = (vaddr_t)end;

	/*
	 * Find the real size of the kernel.  Locate the smallest starting
	 * address > kernelstart.
	 */
	for (mp1 = mp = memlist; mp->size; mp++) {
		/*
		 * Check whether this region is at the end of the kernel.
		 */
		if (mp->start >= ekdata && (mp1->start < ekdata || 
						mp1->start > mp->start))
			mp1 = mp;
	}
	if (mp1->start < kdata)
		prom_printf("Kernel at end of vmem???\r\n");

	BDPRINTF(PDB_BOOT1, 
		("Kernel data is mapped at %lx, next free seg: %lx, %lx\r\n",
			(long)kdata, (u_long)mp1->start, (u_long)mp1->size));

	/*
	 * We save where we can start allocating memory.
	 */
	firstaddr = (ekdata + 07) & ~ 07;	/* Longword align */

	/*
	 * We reserve 100K to grow.
	 */
	ekdata += 100*KB;

	/*
	 * And set the end of the data segment to the end of what our
	 * bootloader allocated for us, if we still fit in there.
	 */
	if (ekdata < mp1->start)
		ekdata = mp1->start;

#define	valloc(name, type, num) (name) = (type *)firstaddr; firstaddr += (num)

	/*
	 * Since we can't always give the loader the hint to align us on a 4MB
	 * boundary, we will need to do the alignment ourselves.  First
	 * allocate a new 4MB aligned segment for the kernel, then map it
	 * in, copy the kernel over, swap mappings, then finally, free the
	 * old kernel.  Then we can continue with this.
	 *
	 * We'll do the data segment up here since we know how big it is.
	 * We'll do the text segment after we've read in the PROM translations
	 * so we can figure out its size.
	 *
	 * The ctxbusy table takes about 64KB, the TSB up to 32KB, and the
	 * rest should be less than 1K, so 100KB extra should be plenty.
	 */
	kdsize = round_page(ekdata - kdata);
	BDPRINTF(PDB_BOOT1, ("Kernel data size is %lx\r\n", (long)kdsize));

	if ((kdatap & (4*MEG-1)) == 0) {
		/* We were at a 4MB boundary -- claim the rest */
		psize_t szdiff = (4*MEG - kdsize) & (4*MEG - 1);

		BDPRINTF(PDB_BOOT1, ("Need to extend dseg by %lx\r\n",
			(long)szdiff));
		if (szdiff) {
			/* Claim the rest of the physical page. */
			newkp = kdatap + kdsize;
			newkv = kdata + kdsize;
			if (newkp != prom_claim_phys(newkp, szdiff)) {
				prom_printf("pmap_bootstrap: could not claim "
					"physical dseg extension "
					"at %lx size %lx\r\n",
					newkp, szdiff);
				goto remap_data;
			}

			/* And the rest of the virtual page. */
			if (prom_claim_virt(newkv, szdiff) != newkv)
			prom_printf("pmap_bootstrap: could not claim "
				"virtual dseg extension "
				"at size %lx\r\n", newkv, szdiff);

			/* Make sure all 4MB are mapped */
			prom_map_phys(newkp, szdiff, newkv, -1);
		}
	} else {
		psize_t sz;
remap_data:
		/* 
		 * Either we're not at a 4MB boundary or we can't get the rest
		 * of the 4MB extension.  We need to move the data segment.
		 * Leave 1MB of extra fiddle space in the calculations.
		 */

		sz = (kdsize + 4*MEG - 1) & ~(4*MEG-1);
		BDPRINTF(PDB_BOOT1, 
			 ("Allocating new %lx kernel data at 4MB boundary\r\n",
			  (u_long)sz));
		if ((newkp = prom_alloc_phys(sz, 4*MEG)) == (paddr_t)-1 ) {
			prom_printf("Cannot allocate new kernel\r\n");
			OF_exit();
		}
		BDPRINTF(PDB_BOOT1, ("Allocating new va for buffer at %llx\r\n",
				     (u_int64_t)newkp));
		if ((newkv = (vaddr_t)prom_alloc_virt(sz, 8)) ==
		    (vaddr_t)-1) {
			prom_printf("Cannot allocate new kernel va\r\n");
			OF_exit();
		}
		BDPRINTF(PDB_BOOT1, ("Mapping in buffer %llx at %llx\r\n",
		    (u_int64_t)newkp, (u_int64_t)newkv));
		prom_map_phys(newkp, sz, (vaddr_t)newkv, -1); 
		BDPRINTF(PDB_BOOT1, ("Copying %ld bytes kernel data...",
			kdsize));
		bzero((void *)newkv, sz);
		bcopy((void *)kdata, (void *)newkv, kdsize);
		BDPRINTF(PDB_BOOT1, ("done.  Swapping maps..unmap new\r\n"));
		prom_unmap_virt((vaddr_t)newkv, sz);
		BDPRINTF(PDB_BOOT, ("remap old "));
#if 0
		/*
		 * calling the prom will probably require reading part of the
		 * data segment so we can't do this.  */
		prom_unmap_virt((vaddr_t)kdatap, kdsize);
#endif
		prom_map_phys(newkp, sz, kdata, -1); 
		/*
		 * we will map in 4MB, more than we allocated, to allow
		 * further allocation
		 */
		BDPRINTF(PDB_BOOT1, ("free old\r\n"));
		prom_free_phys(kdatap, kdsize);
		kdatap = newkp;
		BDPRINTF(PDB_BOOT1,
			 ("pmap_bootstrap: firstaddr is %lx virt (%lx phys)"
			  "avail for kernel\r\n", (u_long)firstaddr,
			  (u_long)prom_vtop(firstaddr)));
	}

	/*
	 * Find out how much RAM we have installed.
	 */
	BDPRINTF(PDB_BOOT, ("pmap_bootstrap: getting phys installed\r\n"));
	if ((memh = OF_finddevice("/memory")) == -1) {
		prom_printf("no memory?");
		OF_exit();
	}
	memsize = OF_getproplen(memh, "reg") + 2 * sizeof(struct mem_region);
	valloc(mem, struct mem_region, memsize);
	bzero((caddr_t)mem, memsize);
	if (OF_getprop(memh, "reg", mem, memsize) <= 0) {
		prom_printf("no memory installed?");
		OF_exit();
	}

#ifdef DEBUG
	if (pmapdebug & PDB_BOOT1) {
		/* print out mem list */
		prom_printf("Installed physical memory:\r\n");
		for (mp = mem; mp->size; mp++) {
			prom_printf("memlist start %lx size %lx\r\n",
				    (u_long)mp->start, (u_long)mp->size);
		}
	}
#endif
	BDPRINTF(PDB_BOOT1, ("Calculating physmem:"));

	for (mp = mem; mp->size; mp++)
		physmem += atop(mp->size);
	BDPRINTF(PDB_BOOT1, (" result %x or %d pages\r\n", 
			     (int)physmem, (int)physmem));

	/* 
	 * Calculate approx TSB size.
	 */
	tsbsize = 0;
#ifdef SMALL_KERNEL
	while ((physmem >> tsbsize) > atop(64 * MEG) && tsbsize < 2)
#else
	while ((physmem >> tsbsize) > atop(64 * MEG) && tsbsize < 7)
#endif
		tsbsize++;

	/*
	 * Save the prom translations
	 */
	sz = OF_getproplen(vmemh, "translations");
	valloc(prom_map, struct prom_map, sz);
	if (OF_getprop(vmemh, "translations", (void *)prom_map, sz) <= 0) {
		prom_printf("no translations installed?");
		OF_exit();
	}
	prom_map_size = sz / sizeof(struct prom_map);
#ifdef DEBUG
	if (pmapdebug & PDB_BOOT) {
		/* print out mem list */
		prom_printf("Prom xlations:\r\n");
		for (i = 0; i < prom_map_size; i++) {
			prom_printf("start %016lx size %016lx tte %016lx\r\n", 
				    (u_long)prom_map[i].vstart, 
				    (u_long)prom_map[i].vsize,
				    (u_long)prom_map[i].tte);
		}
		prom_printf("End of prom xlations\r\n");
	}
#endif
	/*
	 * Hunt for the kernel text segment and figure out it size and
	 * alignment.  
	 */
	ktsize = 0;
	for (i = 0; i < prom_map_size; i++) 
		if (prom_map[i].vstart == ktext + ktsize)
			ktsize += prom_map[i].vsize;
	if (ktsize == 0)
		panic("No kernel text segment!");
	ektext = ktext + ktsize;

	if (ktextp & (4*MEG-1)) {
		/* Kernel text is not 4MB aligned -- need to fix that */
		BDPRINTF(PDB_BOOT1, 
			 ("Allocating new %lx kernel text at 4MB boundary\r\n",
			  (u_long)ktsize));
		if ((newkp = prom_alloc_phys(ktsize, 4*MEG)) == 0 ) {
			prom_printf("Cannot allocate new kernel text\r\n");
			OF_exit();
		}
		BDPRINTF(PDB_BOOT1, ("Allocating new va for buffer at %llx\r\n",
				     (u_int64_t)newkp));
		if ((newkv = (vaddr_t)prom_alloc_virt(ktsize, 8)) ==
		    (vaddr_t)-1) {
			prom_printf("Cannot allocate new kernel text va\r\n");
			OF_exit();
		}
		BDPRINTF(PDB_BOOT1, ("Mapping in buffer %lx at %lx\r\n",
				     (u_long)newkp, (u_long)newkv));
		prom_map_phys(newkp, ktsize, (vaddr_t)newkv, -1); 
		BDPRINTF(PDB_BOOT1, ("Copying %ld bytes kernel text...",
			ktsize));
		bcopy((void *)ktext, (void *)newkv,
		    ktsize);
		BDPRINTF(PDB_BOOT1, ("done.  Swapping maps..unmap new\r\n"));
		prom_unmap_virt((vaddr_t)newkv, 4*MEG);
		BDPRINTF(PDB_BOOT, ("remap old "));
#if 0
		/*
		 * calling the prom will probably require reading part of the
		 * text segment so we can't do this.  
		 */
		prom_unmap_virt((vaddr_t)ktextp, ktsize);
#endif
		prom_map_phys(newkp, ktsize, ktext, -1); 
		/*
		 * we will map in 4MB, more than we allocated, to allow
		 * further allocation
		 */
		BDPRINTF(PDB_BOOT1, ("free old\r\n"));
		prom_free_phys(ktextp, ktsize);
		ktextp = newkp;
		
		BDPRINTF(PDB_BOOT1, 
			 ("pmap_bootstrap: firstaddr is %lx virt (%lx phys)"
			  "avail for kernel\r\n", (u_long)firstaddr,
			  (u_long)prom_vtop(firstaddr)));

		/*
		 * Re-fetch translations -- they've certainly changed.
		 */
		if (OF_getprop(vmemh, "translations", (void *)prom_map, sz) <=
			0) {
			prom_printf("no translations installed?");
			OF_exit();
		}
#ifdef DEBUG
		if (pmapdebug & PDB_BOOT) {
			/* print out mem list */
			prom_printf("New prom xlations:\r\n");
			for (i = 0; i < prom_map_size; i++) {
				prom_printf("start %016lx size %016lx tte %016lx\r\n",
					    (u_long)prom_map[i].vstart, 
					    (u_long)prom_map[i].vsize,
					    (u_long)prom_map[i].tte);
			}
			prom_printf("End of prom xlations\r\n");
		}
#endif
	} 
	ektextp = ktextp + ktsize;

	/*
	 * Here's a quick in-lined reverse bubble sort.  It gets rid of
	 * any translations inside the kernel data VA range.
	 */
	for(i = 0; i < prom_map_size; i++) {
		if (prom_map[i].vstart >= kdata &&
		    prom_map[i].vstart <= firstaddr) {
			prom_map[i].vstart = 0;
			prom_map[i].vsize = 0;
		}
		if (prom_map[i].vstart >= ktext &&
		    prom_map[i].vstart <= ektext) {
			prom_map[i].vstart = 0;
			prom_map[i].vsize = 0;
		}
		for(j = i; j < prom_map_size; j++) {
			if (prom_map[j].vstart >= kdata &&
			    prom_map[j].vstart <= firstaddr)
				continue;	/* this is inside the kernel */
			if (prom_map[j].vstart >= ktext &&
			    prom_map[j].vstart <= ektext)
				continue;	/* this is inside the kernel */
			if (prom_map[j].vstart > prom_map[i].vstart) {
				struct prom_map tmp;
				tmp = prom_map[i];
				prom_map[i] = prom_map[j];
				prom_map[j] = tmp;
			}
		}
	}
#ifdef DEBUG
	if (pmapdebug & PDB_BOOT) {
		/* print out mem list */
		prom_printf("Prom xlations:\r\n");
		for (i = 0; i < prom_map_size; i++) {
			prom_printf("start %016lx size %016lx tte %016lx\r\n", 
				    (u_long)prom_map[i].vstart, 
				    (u_long)prom_map[i].vsize,
				    (u_long)prom_map[i].tte);
		}
		prom_printf("End of prom xlations\r\n");
	}
#endif

	/*
	 * Allocate a 64KB page for the cpu_info structure now.
	 */
	if ((cpu0paddr = prom_alloc_phys(numcpus * 8*NBPG, 8*NBPG)) == 0 ) {
		prom_printf("Cannot allocate new cpu_info\r\n");
		OF_exit();
	}


	/*
	 * Now the kernel text segment is in its final location we can try to
	 * find out how much memory really is free.  
	 */
	sz = OF_getproplen(memh, "available") + sizeof(struct mem_region);
	valloc(orig, struct mem_region, sz);
	bzero((caddr_t)orig, sz);
	if (OF_getprop(memh, "available", orig, sz) <= 0) {
		prom_printf("no available RAM?");
		OF_exit();
	}
#ifdef DEBUG
	if (pmapdebug & PDB_BOOT1) {
		/* print out mem list */
		prom_printf("Available physical memory:\r\n");
		for (mp = orig; mp->size; mp++) {
			prom_printf("memlist start %lx size %lx\r\n",
				    (u_long)mp->start, (u_long)mp->size);
		}
		prom_printf("End of available physical memory\r\n");
	}
#endif
	valloc(avail, struct mem_region, sz);
	bzero((caddr_t)avail, sz);
	for (pcnt = 0, mp = orig, mp1 = avail; (mp1->size = mp->size);
	    mp++, mp1++) {
		mp1->start = mp->start;
		pcnt++;
	}

	/*
	 * Allocate and initialize a context table
	 */
	numctx = maxctx;
	valloc(ctxbusy, paddr_t, CTXSIZE);
	bzero((caddr_t)ctxbusy, CTXSIZE);

	/*
	 * Allocate our TSB.
	 *
	 * We will use the left over space to flesh out the kernel pmap.
	 */
	BDPRINTF(PDB_BOOT1, ("firstaddr before TSB=%lx\r\n", 
		(u_long)firstaddr));
	firstaddr = ((firstaddr + TSBSIZE - 1) & ~(TSBSIZE-1)); 
#ifdef DEBUG
	i = (firstaddr + (NBPG-1)) & ~(NBPG-1);	/* First, page align */
	if ((int)firstaddr < i) {
		prom_printf("TSB alloc fixup failed\r\n");
		prom_printf("frobbed i, firstaddr before TSB=%x, %lx\r\n",
		    (int)i, (u_long)firstaddr);
		panic("TSB alloc");
		OF_exit();
	}
#endif
	BDPRINTF(PDB_BOOT, ("frobbed i, firstaddr before TSB=%x, %lx\r\n", 
			    (int)i, (u_long)firstaddr));
	valloc(tsb_dmmu, pte_t, TSBSIZE);
	bzero(tsb_dmmu, TSBSIZE);
	valloc(tsb_immu, pte_t, TSBSIZE);
	bzero(tsb_immu, TSBSIZE);

	BDPRINTF(PDB_BOOT1, ("firstaddr after TSB=%lx\r\n", (u_long)firstaddr));
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void *)tsb_dmmu,
	    (int)TSBSIZE));

#ifdef SUN4V
	if (CPU_ISSUN4V) {
		valloc(tsb_desc, struct tsb_desc, sizeof(struct tsb_desc));
		bzero(tsb_desc, sizeof(struct tsb_desc));
		tsb_desc->td_idxpgsz = 0;
		tsb_desc->td_assoc = 1;
		tsb_desc->td_size = TSBENTS;
		tsb_desc->td_ctxidx = -1;
		tsb_desc->td_pgsz = 0xf;
		tsb_desc->td_pa = (paddr_t)tsb_dmmu + kdatap - kdata;
	}
#endif

	first_phys_addr = mem->start;
	BDPRINTF(PDB_BOOT1, ("firstaddr after pmap=%08lx\r\n", 
		(u_long)firstaddr));

	/*
	 * Page align all regions.  
	 * Non-page memory isn't very interesting to us.
	 * Also, sort the entries for ascending addresses.
	 * 
	 * And convert from virtual to physical addresses.
	 */
	
	BDPRINTF(PDB_BOOT, ("kernel virtual size %08lx - %08lx\r\n",
			    (u_long)kernelstart, (u_long)firstaddr));
	kdata = kdata & ~PGOFSET;
	ekdata = firstaddr;
	ekdata = (ekdata + PGOFSET) & ~PGOFSET;
	BDPRINTF(PDB_BOOT1, ("kernel virtual size %08lx - %08lx\r\n",
			     (u_long)kernelstart, (u_long)kernelend));
	ekdatap = ekdata - kdata + kdatap;
	/* Switch from vaddrs to paddrs */
	if(ekdatap > (kdatap + 4*MEG)) {
		prom_printf("Kernel size exceeds 4MB\r\n");
	}

#ifdef DEBUG
	if (pmapdebug & PDB_BOOT1) {
		/* print out mem list */
		prom_printf("Available %lx physical memory before cleanup:\r\n",
			    (u_long)avail);
		for (mp = avail; mp->size; mp++) {
			prom_printf("memlist start %lx size %lx\r\n", 
				    (u_long)mp->start, 
				    (u_long)mp->size);
		}
		prom_printf("End of available physical memory before cleanup\r\n");
		prom_printf("kernel physical text size %08lx - %08lx\r\n",
			    (u_long)ktextp, (u_long)ektextp);
		prom_printf("kernel physical data size %08lx - %08lx\r\n",
			    (u_long)kdatap, (u_long)ekdatap);
	}
#endif
	/*
	 * Here's a another quick in-lined bubble sort.
	 */
	for (i = 0; i < pcnt; i++) {
		for (j = i; j < pcnt; j++) {
			if (avail[j].start < avail[i].start) {
				struct mem_region tmp;
				tmp = avail[i];
				avail[i] = avail[j];
				avail[j] = tmp;
			}
		}
	}

	/* Throw away page zero if we have it. */
	if (avail->start == 0) {
		avail->start += NBPG;
		avail->size -= NBPG;
	}
	/*
	 * Now we need to remove the area we valloc'ed from the available
	 * memory lists.  (NB: we may have already alloc'ed the entire space).
	 */
	npgs = 0;
	for (mp = avail; mp->size; mp++) {
		/*
		 * Check whether this region holds all of the kernel.
		 */
		s = mp->start + mp->size;
		if (mp->start < kdatap && s > roundup(ekdatap, 4*MEG)) {
			avail[pcnt].start = roundup(ekdatap, 4*MEG);
			avail[pcnt++].size = s - kdatap;
			mp->size = kdatap - mp->start;
		}
		/*
		 * Look whether this regions starts within the kernel.
		 */
		if (mp->start >= kdatap && 
			mp->start < roundup(ekdatap, 4*MEG)) {
			s = ekdatap - mp->start;
			if (mp->size > s)
				mp->size -= s;
			else
				mp->size = 0;
			mp->start = roundup(ekdatap, 4*MEG);
		}
		/*
		 * Now look whether this region ends within the kernel.
		 */
		s = mp->start + mp->size;
		if (s > kdatap && s < roundup(ekdatap, 4*MEG))
			mp->size -= s - kdatap;
		/*
		 * Now page align the start of the region.
		 */
		s = mp->start % NBPG;
		if (mp->size >= s) {
			mp->size -= s;
			mp->start += s;
		}
		/*
		 * And now align the size of the region.
		 */
		mp->size -= mp->size % NBPG;
		/*
		 * Check whether some memory is left here.
		 */
		if (mp->size == 0) {
			bcopy(mp + 1, mp,
			      (pcnt - (mp - avail)) * sizeof *mp);
			pcnt--;
			mp--;
			continue;
		}
		s = mp->start;
		sz = mp->size;
		npgs += atop(sz);
		for (mp1 = avail; mp1 < mp; mp1++)
			if (s < mp1->start)
				break;
		if (mp1 < mp) {
			bcopy(mp1, mp1 + 1, (char *)mp - (char *)mp1);
			mp1->start = s;
			mp1->size = sz;
		}
		/* 
		 * In future we should be able to specify both allocated
		 * and free.
		 */
		uvm_page_physload(
			atop(mp->start),
			atop(mp->start+mp->size),
			atop(mp->start),
			atop(mp->start+mp->size), 0);
	}

#if 0
	/* finally, free up any space that valloc did not use */
	prom_unmap_virt((vaddr_t)ekdata, roundup(ekdata, 4*MEG) - ekdata);
	if (ekdatap < roundup(kdatap, 4*MEG))) {
		uvm_page_physload(atop(ekdatap), 
			atop(roundup(ekdatap, (4*MEG))),
			atop(ekdatap), 
			atop(roundup(ekdatap, (4*MEG))), 0);
	}
#endif

#ifdef DEBUG
	if (pmapdebug & PDB_BOOT) {
		/* print out mem list */
		prom_printf("Available physical memory after cleanup:\r\n");
		for (mp = avail; mp->size; mp++) {
			prom_printf("avail start %lx size %lx\r\n", 
				    (long)mp->start, (long)mp->size);
		}
		prom_printf("End of available physical memory after cleanup\r\n");
	}
#endif
	/*
	 * Allocate and clear out pmap_kernel()->pm_segs[]
	 */
	mtx_init(&pmap_kernel()->pm_mtx, IPL_VM);
	pmap_kernel()->pm_refs = 1;
	pmap_kernel()->pm_ctx = 0;
	{
		paddr_t newp;

		do {
			pmap_get_page(&newp, NULL, pmap_kernel());
		} while (!newp); /* Throw away page zero */
		pmap_kernel()->pm_segs=(int64_t *)(u_long)newp;
		pmap_kernel()->pm_physaddr = newp;
		/* mark kernel context as busy */
		((paddr_t*)ctxbusy)[0] = pmap_kernel()->pm_physaddr;
	}
	/*
	 * finish filling out kernel pmap.
	 */

	BDPRINTF(PDB_BOOT, ("pmap_kernel()->pm_physaddr = %lx\r\n",
	    (long)pmap_kernel()->pm_physaddr));
	/*
	 * Tell pmap about our mesgbuf -- Hope this works already
	 */
#ifdef DEBUG
	BDPRINTF(PDB_BOOT1, ("Calling consinit()\r\n"));
	if (pmapdebug & PDB_BOOT1) consinit();
	BDPRINTF(PDB_BOOT1, ("Inserting mesgbuf into pmap_kernel()\r\n"));
#endif
	/* it's not safe to call pmap_enter so we need to do this ourselves */
	va = (vaddr_t)msgbufp;
	prom_map_phys(phys_msgbuf, msgbufsiz, (vaddr_t)msgbufp, -1);
	while (msgbufsiz) {
		data = TSB_DATA(0 /* global */, 
			PGSZ_8K,
			phys_msgbuf,
			1 /* priv */,
			1 /* Write */,
			1 /* Cacheable */,
			FORCE_ALIAS /* ALIAS -- Disable D$ */,
			1 /* valid */,
			0 /* IE */);
		pmap_enter_kpage(va, data);
		va += PAGE_SIZE;
		msgbufsiz -= PAGE_SIZE;
		phys_msgbuf += PAGE_SIZE;
	}
	BDPRINTF(PDB_BOOT1, ("Done inserting mesgbuf into pmap_kernel()\r\n"));
	
	BDPRINTF(PDB_BOOT1, ("Inserting PROM mappings into pmap_kernel()\r\n"));
	data = 0;
	if (CPU_ISSUN4U || CPU_ISSUN4US)
		data = SUN4U_TLB_EXEC;
	for (i = 0; i < prom_map_size; i++) {
		if (prom_map[i].vstart && ((prom_map[i].vstart>>32) == 0)) {
			for (j = 0; j < prom_map[i].vsize; j += NBPG) {
				int k;
				
				for (k = 0; page_size_map[k].mask; k++) {
					if (((prom_map[i].vstart |
					      prom_map[i].tte) &
					      page_size_map[k].mask) == 0 &&
					      page_size_map[k].mask <
					      prom_map[i].vsize)
						break;
				}
				/* Enter PROM map into pmap_kernel() */
				pmap_enter_kpage(prom_map[i].vstart + j,
					(prom_map[i].tte + j)|data|
					page_size_map[k].code);
			}
		}
	}
	BDPRINTF(PDB_BOOT1, ("Done inserting PROM mappings into pmap_kernel()\r\n"));

	/*
	 * Fix up start of kernel heap.
	 */
	vmmap = (vaddr_t)roundup(ekdata, 4*MEG);
	/* Let's keep 1 page of redzone after the kernel */
	vmmap += NBPG;
	{ 
		extern vaddr_t u0[2];
		extern struct pcb* proc0paddr;
		extern void main(void);
		paddr_t pa;

		/* Initialize all the pointers to u0 */
		u0[0] = vmmap;
		/* Allocate some VAs for u0 */
		u0[1] = vmmap + 2*USPACE;

		BDPRINTF(PDB_BOOT1, 
			("Inserting stack 0 into pmap_kernel() at %p\r\n",
				vmmap));

		while (vmmap < u0[1]) {
			int64_t data;

			pmap_get_page(&pa, NULL, pmap_kernel());
			prom_map_phys(pa, NBPG, vmmap, -1);
			data = TSB_DATA(0 /* global */,
				PGSZ_8K,
				pa,
				1 /* priv */,
				1 /* Write */,
				1 /* Cacheable */,
				FORCE_ALIAS /* ALIAS -- Disable D$ */,
				1 /* valid */,
				0 /* IE */);
			pmap_enter_kpage(vmmap, data);
			vmmap += NBPG;
		}
		BDPRINTF(PDB_BOOT1, 
			 ("Done inserting stack 0 into pmap_kernel()\r\n"));

		/* Now map in and initialize our cpu_info structure */
#ifdef DIAGNOSTIC
		vmmap += NBPG; /* redzone -- XXXX do we need one? */
#endif
		intstk = vmmap = roundup(vmmap, 64*KB);
		cpus = (struct cpu_info *)(intstk + CPUINFO_VA - INTSTACK);

		BDPRINTF(PDB_BOOT1,
			("Inserting cpu_info into pmap_kernel() at %p\r\n",
				 cpus));
		/* Now map in all 8 pages of cpu_info */
		pa = cpu0paddr;
		prom_map_phys(pa, 64*KB, vmmap, -1);
		/* 
		 * Also map it in as the interrupt stack.
		 * This lets the PROM see this if needed.
		 *
		 * XXXX locore.s does not flush these mappings
		 * before installing the locked TTE.
		 */
		prom_map_phys(pa, 64*KB, CPUINFO_VA, -1);
		for (i=0; i<8; i++) {
			int64_t data;

			data = TSB_DATA(0 /* global */,
				PGSZ_8K,
				pa,
				1 /* priv */,
				1 /* Write */,
				1 /* Cacheable */,
				FORCE_ALIAS /* ALIAS -- Disable D$ */,
				1 /* valid */,
				0 /* IE */);
			pmap_enter_kpage(vmmap, data);
			vmmap += NBPG;
			pa += NBPG;
		}
		BDPRINTF(PDB_BOOT1, ("Initializing cpu_info\r\n"));

		/* Initialize our cpu_info structure */
		bzero((void *)intstk, 8*NBPG);
		cpus->ci_self = cpus;
		cpus->ci_next = NULL; /* Redundant, I know. */
		cpus->ci_curproc = &proc0;
		cpus->ci_cpcb = (struct pcb *)u0[0]; /* Need better source */
		cpus->ci_upaid = cpu_myid();
		cpus->ci_number = 0;
		cpus->ci_flags = CPUF_RUNNING;
		cpus->ci_fpproc = NULL;
		cpus->ci_spinup = main; /* Call main when we're running. */
		cpus->ci_initstack = (void *)u0[1];
		cpus->ci_paddr = cpu0paddr;
#ifdef SUN4V
		cpus->ci_mmfsa = cpu0paddr;
#endif
		proc0paddr = cpus->ci_cpcb;

		cpu0paddr += 64 * KB;

		/* The rest will be done at CPU attach time. */
		BDPRINTF(PDB_BOOT1, 
			 ("Done inserting cpu_info into pmap_kernel()\r\n"));
	}

	vmmap = (vaddr_t)reserve_dumppages((caddr_t)(u_long)vmmap);
	BDPRINTF(PDB_BOOT1, ("Finished pmap_bootstrap()\r\n"));

	pmap_bootstrap_cpu(cpus->ci_paddr);
}

void sun4u_bootstrap_cpu(paddr_t);
void sun4v_bootstrap_cpu(paddr_t);

void
pmap_bootstrap_cpu(paddr_t intstack)
{
	if (CPU_ISSUN4V)
		sun4v_bootstrap_cpu(intstack);
	else
		sun4u_bootstrap_cpu(intstack);
}

extern void sun4u_set_tsbs(void);

void
sun4u_bootstrap_cpu(paddr_t intstack)
{
	u_int64_t data;
	paddr_t pa;
	vaddr_t va;
	int index;
	int impl;

	impl = (getver() & VER_IMPL) >> VER_IMPL_SHIFT;

	/*
	 * Establish the 4MB locked mappings for kernel data and text.
	 *
	 * The text segment needs to be mapped into the DTLB too,
	 * because of .rodata.
	 */

	index = 15; /* XXX */
	for (va = ktext, pa = ktextp; va < ektext; va += 4*MEG, pa += 4*MEG) {
		data = SUN4U_TSB_DATA(0, PGSZ_4M, pa, 1, 0, 1, FORCE_ALIAS, 1, 0);
		data |= SUN4U_TLB_L;
		prom_itlb_load(index, data, va);
		prom_dtlb_load(index, data, va);
		index--;
	}

	for (va = kdata, pa = kdatap; va < ekdata; va += 4*MEG, pa += 4*MEG) {
		data = SUN4U_TSB_DATA(0, PGSZ_4M, pa, 1, 1, 1, FORCE_ALIAS, 1, 0);
		data |= SUN4U_TLB_L;
		prom_dtlb_load(index, data, va);
		index--;
	}

#ifdef MULTIPROCESSOR
	if (impl >= IMPL_OLYMPUS_C && impl <= IMPL_JUPITER) {
		/*
		 * On SPARC64-VI and SPARC64-VII processors, the MMU is
		 * shared between threads, so we can't establish a locked
		 * mapping for the interrupt stack since the mappings would
		 * conflict.  Instead we stick the address in a scratch
		 * register, like we do for sun4v.
		 */
		pa = intstack + (CPUINFO_VA - INTSTACK);
		pa += offsetof(struct cpu_info, ci_self);
		va = ldxa(pa, ASI_PHYS_CACHED);
		stxa(0x00, ASI_SCRATCH, va);

		if ((CPU_JUPITERID % 2) == 1)
			index--;

		data = SUN4U_TSB_DATA(0, PGSZ_64K, intstack, 1, 1, 1, FORCE_ALIAS, 1, 0);
		data |= SUN4U_TLB_L;
		prom_dtlb_load(index, data, va - (CPUINFO_VA - INTSTACK));

		sun4u_set_tsbs();
		return;
	}
#endif

	/*
	 * Establish the 64KB locked mapping for the interrupt stack.
	 */

	data = SUN4U_TSB_DATA(0, PGSZ_64K, intstack, 1, 1, 1, FORCE_ALIAS, 1, 0);
	data |= SUN4U_TLB_L;
	prom_dtlb_load(index, data, INTSTACK);

	sun4u_set_tsbs();
}

void
sun4v_bootstrap_cpu(paddr_t intstack)
{
#ifdef SUN4V
	u_int64_t data;
	paddr_t pa;
	vaddr_t va;
	int err;

	/*
	 * Establish the 4MB locked mappings for kernel data and text.
	 *
	 * The text segment needs to be mapped into the DTLB too,
	 * because of .rodata.
	 */

	for (va = ktext, pa = ktextp; va < ektext; va += 4*MEG, pa += 4*MEG) {
		data = SUN4V_TSB_DATA(0, PGSZ_4M, pa, 1, 0, 1, 0, 1, 0);
		data |= SUN4V_TLB_X;
		err = hv_mmu_map_perm_addr(va, data, MAP_ITLB|MAP_DTLB);
		if (err != H_EOK)
			prom_printf("err: %d\r\n", err);
	}

	for (va = kdata, pa = kdatap; va < ekdata; va += 4*MEG, pa += 4*MEG) {
		data = SUN4V_TSB_DATA(0, PGSZ_4M, pa, 1, 1, 1, 0, 1, 0);
		err = hv_mmu_map_perm_addr(va, data, MAP_DTLB);
		if (err != H_EOK)
			prom_printf("err: %d\r\n", err);
	}

#ifndef MULTIPROCESSOR
	/*
	 * Establish the 64KB locked mapping for the interrupt stack.
	 */
	data = SUN4V_TSB_DATA(0, PGSZ_64K, intstack, 1, 1, 1, 0, 1, 0);
	err = hv_mmu_map_perm_addr(INTSTACK, data, MAP_DTLB);
	if (err != H_EOK)
		prom_printf("err: %d\r\n", err);
#else
	pa = intstack + (CPUINFO_VA - INTSTACK);
	pa += offsetof(struct cpu_info, ci_self);
	stxa(0x00, ASI_SCRATCHPAD, ldxa(pa, ASI_PHYS_CACHED));
#endif

	stxa(0x10, ASI_SCRATCHPAD, intstack + (CPUINFO_VA - INTSTACK));

	err = hv_mmu_tsb_ctx0(1, (paddr_t)tsb_desc + kdatap - kdata);
	if (err != H_EOK)
		prom_printf("err: %d\r\n", err);
	err = hv_mmu_tsb_ctxnon0(1, (paddr_t)tsb_desc + kdatap - kdata);
	if (err != H_EOK)
		prom_printf("err: %d\r\n", err);
#endif
}

/*
 * Initialize anything else for pmap handling.
 * Called during uvm_init().
 */
void
pmap_init(void)
{
	BDPRINTF(PDB_BOOT1, ("pmap_init()\r\n"));
	if (PAGE_SIZE != NBPG)
		panic("pmap_init: CLSIZE!=1");

	/* Setup a pool for additional pvlist structures */
	pool_init(&pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pv_entry", NULL);
	pool_init(&pmap_pool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmappl", NULL);
}

/* Start of non-cachable physical memory on UltraSPARC-III. */
#define VM_MAXPHYS_ADDRESS	((vaddr_t)0x0000040000000000L)

static vaddr_t kbreak; /* End of kernel VA */

/*
 * How much virtual space is available to the kernel?
 */
void
pmap_virtual_space(vaddr_t *start, vaddr_t *end)
{
	/*
	 * Make sure virtual memory and physical memory don't overlap
	 * to avoid problems with ASI_PHYS_CACHED on UltraSPARC-III.
	 */
	if (vmmap < VM_MAXPHYS_ADDRESS)
		vmmap = VM_MAXPHYS_ADDRESS;

	/* Reserve two pages for pmap_copy_page && /dev/mem */
	*start = kbreak = (vaddr_t)(vmmap + 2*NBPG);
	*end = VM_MAX_KERNEL_ADDRESS;
	BDPRINTF(PDB_BOOT1, ("pmap_virtual_space: %x-%x\r\n", *start, *end));
}

/*
 * Preallocate kernel page tables to a specified VA.
 * This simply loops through the first TTE for each
 * page table from the beginning of the kernel pmap, 
 * reads the entry, and if the result is
 * zero (either invalid entry or no page table) it stores
 * a zero there, populating page tables in the process.
 * This is not the most efficient technique but i don't
 * expect it to be called that often.
 */
vaddr_t 
pmap_growkernel(vaddr_t maxkvaddr)
{
	paddr_t pg;
	struct pmap *pm = pmap_kernel();
	
	if (maxkvaddr >= VM_MAX_KERNEL_ADDRESS) {
		printf("WARNING: cannot extend kernel pmap beyond %p to %p\n",
		       (void *)VM_MAX_KERNEL_ADDRESS, (void *)maxkvaddr);
		return (kbreak);
	}

	/* Align with the start of a page table */
	for (kbreak &= (-1<<PDSHIFT); kbreak < maxkvaddr;
	     kbreak += (1<<PDSHIFT)) {
		if (pseg_get(pm, kbreak))
			continue;

		pg = 0;
		while (pseg_set(pm, kbreak, 0, pg) == 1) {
			pg = 0;
			pmap_get_page(&pg, "growk", pm);
		}
		
	}

	return (kbreak);
}

/*
 * Create and return a physical map.
 */
struct pmap *
pmap_create(void)
{
	struct pmap *pm;

	pm = pool_get(&pmap_pool, PR_WAITOK | PR_ZERO);

	mtx_init(&pm->pm_mtx, IPL_VM);
	pm->pm_refs = 1;
	pmap_get_page(&pm->pm_physaddr, "pmap_create", pm);
	pm->pm_segs = (int64_t *)(u_long)pm->pm_physaddr;
	ctx_alloc(pm);

	return (pm);
}

/*
 * Add a reference to the given pmap.
 */
void
pmap_reference(struct pmap *pm)
{
	atomic_inc_int(&pm->pm_refs);
}

/*
 * Retire the given pmap from service.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_destroy(struct pmap *pm)
{
	if (atomic_dec_int_nv(&pm->pm_refs) == 0) {
		pmap_release(pm);
		pool_put(&pmap_pool, pm);
	}
}

/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 */
void
pmap_release(struct pmap *pm)
{
	int i, j, k;
	paddr_t *pdir, *ptbl, tmp;

#ifdef DIAGNOSTIC
	if(pm == pmap_kernel())
		panic("pmap_release: releasing pmap_kernel()");
#endif

	mtx_enter(&pm->pm_mtx);
	for(i=0; i<STSZ; i++) {
		paddr_t psegentp = (paddr_t)(u_long)&pm->pm_segs[i];
		if((pdir = (paddr_t *)(u_long)ldxa((vaddr_t)psegentp,
		    ASI_PHYS_CACHED))) {
			for (k=0; k<PDSZ; k++) {
				paddr_t pdirentp = (paddr_t)(u_long)&pdir[k];
				if ((ptbl = (paddr_t *)(u_long)ldxa(
					(vaddr_t)pdirentp, ASI_PHYS_CACHED))) {
					for (j=0; j<PTSZ; j++) {
						int64_t data;
						paddr_t pa;
						pv_entry_t pv;

						data  = ldxa((vaddr_t)&ptbl[j],
							ASI_PHYS_CACHED);
						if (!(data & TLB_V))
							continue;
						pa = data & TLB_PA_MASK;
						pv = pa_to_pvh(pa);
						if (pv != NULL) {
							printf("pmap_release: pm=%p page %llx still in use\n", pm, 
							       (unsigned long long)(((u_int64_t)i<<STSHIFT)|((u_int64_t)k<<PDSHIFT)|((u_int64_t)j<<PTSHIFT)));
							db_enter();
						}
					}
					stxa(pdirentp, ASI_PHYS_CACHED, 0);
					pmap_free_page((paddr_t)ptbl, pm);
				}
			}
			stxa(psegentp, ASI_PHYS_CACHED, 0);
			pmap_free_page((paddr_t)pdir, pm);
		}
	}
	tmp = (paddr_t)(u_long)pm->pm_segs;
	pm->pm_segs = NULL;
	pmap_free_page(tmp, pm);
	mtx_leave(&pm->pm_mtx);
	ctx_free(pm);
}

/*
 * Copy the range specified by src_addr/len
 * from the source map to the range dst_addr/len
 * in the destination map.
 *
 * This routine is only advisory and need not do anything.
 */
void
pmap_copy(struct pmap *dst_pmap, struct pmap *src_pmap, vaddr_t dst_addr,
    vsize_t len, vaddr_t src_addr)
{
}

/*
 * Garbage collects the physical map system for
 * pages which are no longer used.
 * Success need not be guaranteed -- that is, there
 * may well be pages which are not referenced, but
 * others may be collected.
 * Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(struct pmap *pm)
{
#if 1
	int i, j, k, n, m, s;
	paddr_t *pdir, *ptbl;
	/* This is a good place to scan the pmaps for page tables with
	 * no valid mappings in them and free them. */
	
	/* NEVER GARBAGE COLLECT THE KERNEL PMAP */
	if (pm == pmap_kernel())
		return;

	s = splvm();
	for (i=0; i<STSZ; i++) {
		if ((pdir = (paddr_t *)(u_long)ldxa((vaddr_t)&pm->pm_segs[i], ASI_PHYS_CACHED))) {
			m = 0;
			for (k=0; k<PDSZ; k++) {
				if ((ptbl = (paddr_t *)(u_long)ldxa((vaddr_t)&pdir[k], ASI_PHYS_CACHED))) {
					m++;
					n = 0;
					for (j=0; j<PTSZ; j++) {
						int64_t data = ldxa((vaddr_t)&ptbl[j], ASI_PHYS_CACHED);
						if (data&TLB_V)
							n++;
					}
					if (!n) {
						/* Free the damn thing */
						stxa((paddr_t)(u_long)&pdir[k], ASI_PHYS_CACHED, 0);
						pmap_free_page((paddr_t)ptbl, pm);
					}
				}
			}
			if (!m) {
				/* Free the damn thing */
				stxa((paddr_t)(u_long)&pm->pm_segs[i], ASI_PHYS_CACHED, 0);
				pmap_free_page((paddr_t)pdir, pm);
			}
		}
	}
	splx(s);
#endif
}

void
pmap_zero_page(struct vm_page *pg)
{
	pmap_zero_phys(VM_PAGE_TO_PHYS(pg));
}

void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

	pmap_copy_phys(src, dst);
}

/*
 * Activate the address space for the specified process.  If the
 * process is the current process, load the new MMU context.
 */
void
pmap_activate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	int s;

	/*
	 * This is essentially the same thing that happens in cpu_switch()
	 * when the newly selected process is about to run, except that we
	 * have to make sure to clean the register windows before we set
	 * the new context.
	 */

	s = splvm();
	if (p == curproc) {
		write_user_windows();
		if (pmap->pm_ctx == 0)
			ctx_alloc(pmap);
		if (CPU_ISSUN4V)
			stxa(CTX_SECONDARY, ASI_MMU_CONTEXTID, pmap->pm_ctx);
		else
			stxa(CTX_SECONDARY, ASI_DMMU, pmap->pm_ctx);
	}
	splx(s);
}

/*
 * Deactivate the address space of the specified process.
 */
void
pmap_deactivate(struct proc *p)
{
}

/*
 * pmap_kenter_pa:		[ INTERFACE ]
 *
 *	Enter a va -> pa mapping into the kernel pmap without any
 *	physical->virtual tracking.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	struct pmap *pm = pmap_kernel();
	pte_t tte;

	KDASSERT(va < INTSTACK || va > EINTSTACK);
	KDASSERT(va < kdata || va > ekdata);

#ifdef DIAGNOSTIC
	if (pa & (PMAP_NVC|PMAP_NC|PMAP_LITTLE))
		panic("%s: illegal cache flags 0x%lx", __func__, pa);
#endif

	/*
	 * Construct the TTE.
	 */
	tte.tag = TSB_TAG(0, pm->pm_ctx,va);
	if (CPU_ISSUN4V) {
		tte.data = SUN4V_TSB_DATA(0, PGSZ_8K, pa, 1 /* Privileged */,
		    (PROT_WRITE & prot), 1, 0, 1, 0);
		/*
		 * We don't track modification on kenter mappings.
		 */
		if (prot & PROT_WRITE)
			tte.data |= SUN4V_TLB_REAL_W|SUN4V_TLB_W;
		if (prot & PROT_EXEC)
			tte.data |= SUN4V_TLB_EXEC;
		tte.data |= SUN4V_TLB_TSB_LOCK;	/* wired */
	} else {
		tte.data = SUN4U_TSB_DATA(0, PGSZ_8K, pa, 1 /* Privileged */,
		    (PROT_WRITE & prot), 1, 0, 1, 0);
		/*
		 * We don't track modification on kenter mappings.
		 */
		if (prot & PROT_WRITE)
			tte.data |= SUN4U_TLB_REAL_W|SUN4U_TLB_W;
		if (prot & PROT_EXEC)
			tte.data |= SUN4U_TLB_EXEC;
		tte.data |= SUN4U_TLB_TSB_LOCK;	/* wired */
	}
	KDASSERT((tte.data & TLB_NFO) == 0);

	/* Kernel page tables are pre-allocated. */
	if (pseg_set(pm, va, tte.data, 0) != 0)
		panic("%s: no pseg", __func__);

	/* this is correct */
	dcache_flush_page(pa);
}

/*
 * pmap_kremove:		[ INTERFACE ]
 *
 *	Remove a mapping entered with pmap_kenter_pa() starting at va,
 *	for size bytes (assumed to be page rounded).
 */
void
pmap_kremove(vaddr_t va, vsize_t size)
{
	struct pmap *pm = pmap_kernel();

	KDASSERT(va < INTSTACK || va > EINTSTACK);
	KDASSERT(va < kdata || va > ekdata);

	while (size >= NBPG) {
		/*
		 * Is this part of the permanent 4MB mapping?
		 */
#ifdef DIAGNOSTIC
		if (pm == pmap_kernel() && 
		    (va >= ktext && va < roundup(ekdata, 4*MEG)))
			panic("%s: va=0x%lx in locked TLB", __func__, va);
#endif
		/* Shouldn't need to do this if the entry's not valid. */
		if (pseg_get(pm, va)) {
			/* We need to flip the valid bit and clear the access statistics. */
			if (pseg_set(pm, va, 0, 0)) {
				printf("pmap_kremove: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}

			tsb_invalidate(pm->pm_ctx, va);
			/* Here we assume nothing can get into the TLB unless it has a PTE */
			tlb_flush_pte(va, pm->pm_ctx);
		}
		va += NBPG;
		size -= NBPG;
	}
}

/*
 * Insert physical page at pa into the given pmap at virtual address va.
 * Supports 64-bit pa so we can map I/O space.
 */
int
pmap_enter(struct pmap *pm, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	pte_t tte;
	paddr_t pg;
	int aliased = 0;
	pv_entry_t pv, npv;
	int size = 0; /* PMAP_SZ_TO_TTE(pa); */
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	/*
	 * Is this part of the permanent mappings?
	 */
	KDASSERT(pm != pmap_kernel() || va < INTSTACK || va > EINTSTACK);
	KDASSERT(pm != pmap_kernel() || va < kdata || va > ekdata);

	npv = pool_get(&pv_pool, PR_NOWAIT);
	if (npv == NULL && (flags & PMAP_CANFAIL))
		return (ENOMEM);

	/*
	 * XXXX If a mapping at this address already exists, remove it.
	 */
	mtx_enter(&pm->pm_mtx);
	tte.data = pseg_get(pm, va);
	if (tte.data & TLB_V) {
		mtx_leave(&pm->pm_mtx);
		pmap_remove(pm, va, va + NBPG-1);
		mtx_enter(&pm->pm_mtx);
		tte.data = pseg_get(pm, va);
	}

	/*
	 * Construct the TTE.
	 */
	pv = pa_to_pvh(pa);
	if (pv != NULL) {
		struct vm_page *pg = PHYS_TO_VM_PAGE(pa);

		mtx_enter(&pg->mdpage.pvmtx);
		aliased = (pv->pv_va & (PV_ALIAS|PV_NVC));
#ifdef DIAGNOSTIC
		if ((flags & PROT_MASK) & ~prot)
			panic("pmap_enter: access_type exceeds prot");
#endif
		/* If we don't have the traphandler do it, set the ref/mod bits now */
		if (flags & PROT_MASK)
			pv->pv_va |= PV_REF;
		if (flags & PROT_WRITE)
			pv->pv_va |= PV_MOD;
		pv->pv_va |= pmap_tte2flags(tte.data);
		mtx_leave(&pg->mdpage.pvmtx);
	} else {
		aliased = 0;
	}
	if (pa & PMAP_NVC)
		aliased = 1;
#ifdef NO_VCACHE
	aliased = 1; /* Disable D$ */
#endif
	if (CPU_ISSUN4V) {
		tte.data = SUN4V_TSB_DATA(0, size, pa, pm == pmap_kernel(),
		    (flags & PROT_WRITE), (!(pa & PMAP_NC)), 
		    aliased, 1, (pa & PMAP_LITTLE));
		if (prot & PROT_WRITE)
			tte.data |= SUN4V_TLB_REAL_W;
		if (prot & PROT_EXEC)
			tte.data |= SUN4V_TLB_EXEC;
		if (wired)
			tte.data |= SUN4V_TLB_TSB_LOCK;
	} else {
		tte.data = SUN4U_TSB_DATA(0, size, pa, pm == pmap_kernel(),
		    (flags & PROT_WRITE), (!(pa & PMAP_NC)), 
		    aliased, 1, (pa & PMAP_LITTLE));
		if (prot & PROT_WRITE)
			tte.data |= SUN4U_TLB_REAL_W;
		if (prot & PROT_EXEC)
			tte.data |= SUN4U_TLB_EXEC;
		if (wired)
			tte.data |= SUN4U_TLB_TSB_LOCK;
	}
	KDASSERT((tte.data & TLB_NFO) == 0);

	pg = 0;
	while (pseg_set(pm, va, tte.data, pg) == 1) {
		pg = 0;
		if (!pmap_get_page(&pg, NULL, pm)) {
			if ((flags & PMAP_CANFAIL) == 0)
				panic("pmap_enter: no memory");
			mtx_leave(&pm->pm_mtx);
			if (npv != NULL)
				pool_put(&pv_pool, npv);
			return (ENOMEM);
		}
	}

	if (pv != NULL)
		npv = pmap_enter_pv(pm, npv, va, pa);
	atomic_inc_long(&pm->pm_stats.resident_count);
	mtx_leave(&pm->pm_mtx);
	if (pm->pm_ctx || pm == pmap_kernel()) {
		tsb_invalidate(pm->pm_ctx, va);

		/* Force reload -- protections may be changed */
		tlb_flush_pte(va, pm->pm_ctx);	
	}
	/* this is correct */
	dcache_flush_page(pa);

	if (npv != NULL)
		pool_put(&pv_pool, npv);

	/* We will let the fast mmu miss interrupt load the new translation */
	return 0;
}

/*
 * Remove the given range of mapping entries.
 */
void
pmap_remove(struct pmap *pm, vaddr_t va, vaddr_t endva)
{
	pv_entry_t pv, freepvs = NULL;
	int flush = 0;
	int64_t data;
	vaddr_t flushva = va;

	/* 
	 * In here we should check each pseg and if there are no more entries,
	 * free it.  It's just that linear scans of 8K pages gets expensive.
	 */

	KDASSERT(pm != pmap_kernel() || endva < INTSTACK || va > EINTSTACK);
	KDASSERT(pm != pmap_kernel() || endva < kdata || va > ekdata);

	mtx_enter(&pm->pm_mtx);

	/* Now do the real work */
	while (va < endva) {
		/*
		 * Is this part of the permanent 4MB mapping?
		 */
#ifdef DIAGNOSTIC
		if (pm == pmap_kernel() && va >= ktext && 
			va < roundup(ekdata, 4*MEG))
			panic("pmap_remove: va=%08x in locked TLB", (u_int)va);
#endif
		/* We don't really need to do this if the valid bit is not set... */
		if ((data = pseg_get(pm, va)) && (data & TLB_V) != 0) {
			paddr_t entry;
			
			flush |= 1;
			/* First remove it from the pv_table */
			entry = (data & TLB_PA_MASK);
			pv = pa_to_pvh(entry);
			if (pv != NULL) {
				pv = pmap_remove_pv(pm, va, entry);
				if (pv != NULL) {
					pv->pv_next = freepvs;
					freepvs = pv;
				}
			}
			/* We need to flip the valid bit and clear the access statistics. */
			if (pseg_set(pm, va, 0, 0)) {
				printf("pmap_remove: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}
			atomic_dec_long(&pm->pm_stats.resident_count);
			if (!pm->pm_ctx && pm != pmap_kernel())
				continue;
			tsb_invalidate(pm->pm_ctx, va);
			/* Here we assume nothing can get into the TLB unless it has a PTE */
			tlb_flush_pte(va, pm->pm_ctx);
		}
		va += NBPG;
	}

	mtx_leave(&pm->pm_mtx);

	while ((pv = freepvs) != NULL) {
		freepvs = pv->pv_next;
		pool_put(&pv_pool, pv);
	}

	if (flush)
		cache_flush_virt(flushva, endva - flushva);
}

/*
 * Change the protection on the specified range of this pmap.
 */
void
pmap_protect(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	paddr_t pa;
	pv_entry_t pv;
	int64_t data;
	
	KDASSERT(pm != pmap_kernel() || eva < INTSTACK || sva > EINTSTACK);
	KDASSERT(pm != pmap_kernel() || eva < kdata || sva > ekdata);

	if ((prot & (PROT_WRITE | PROT_EXEC)) ==
	    (PROT_WRITE | PROT_EXEC))
		return;

	if (prot == PROT_NONE) {
		pmap_remove(pm, sva, eva);
		return;
	}
		
	mtx_enter(&pm->pm_mtx);
	sva = sva & ~PGOFSET;
	while (sva < eva) {
		/*
		 * Is this part of the permanent 4MB mapping?
		 */
		if (pm == pmap_kernel() && sva >= ktext && 
			sva < roundup(ekdata, 4*MEG)) {
			prom_printf("pmap_protect: va=%08x in locked TLB\r\n", sva);
			OF_enter();
			mtx_leave(&pm->pm_mtx);
			return;
		}

		if (((data = pseg_get(pm, sva))&TLB_V) /*&& ((data&TLB_TSB_LOCK) == 0)*/) {
			pa = data & TLB_PA_MASK;
			pv = pa_to_pvh(pa);
			if (pv != NULL) {
				struct vm_page *pg = PHYS_TO_VM_PAGE(pa);

				/* Save REF/MOD info */
				mtx_enter(&pg->mdpage.pvmtx);
				pv->pv_va |= pmap_tte2flags(data);
				mtx_leave(&pg->mdpage.pvmtx);
			}
			/* Just do the pmap and TSB, not the pv_list */
			if (CPU_ISSUN4V) {
				if ((prot & PROT_WRITE) == 0)
					data &= ~(SUN4V_TLB_W|SUN4V_TLB_REAL_W);
				if ((prot & PROT_EXEC) == 0)
					data &= ~(SUN4V_TLB_EXEC);
			} else {
				if ((prot & PROT_WRITE) == 0)
					data &= ~(SUN4U_TLB_W|SUN4U_TLB_REAL_W);
				if ((prot & PROT_EXEC) == 0)
					data &= ~(SUN4U_TLB_EXEC);
			}
			KDASSERT((data & TLB_NFO) == 0);
			if (pseg_set(pm, sva, data, 0)) {
				printf("pmap_protect: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}
			
			if (!pm->pm_ctx && pm != pmap_kernel())
				continue;
			tsb_invalidate(pm->pm_ctx, sva);
			tlb_flush_pte(sva, pm->pm_ctx);
		}
		sva += NBPG;
	}
	mtx_leave(&pm->pm_mtx);
}

/*
 * Extract the physical page address associated
 * with the given map/virtual_address pair.
 */
boolean_t
pmap_extract(struct pmap *pm, vaddr_t va, paddr_t *pap)
{
	paddr_t pa;

	if (pm == pmap_kernel() && va >= kdata && 
		va < roundup(ekdata, 4*MEG)) {
		/* Need to deal w/locked TLB entry specially. */
		pa = (paddr_t) (kdatap - kdata + va);
	} else if( pm == pmap_kernel() && va >= ktext && va < ektext ) {
		/* Need to deal w/locked TLB entry specially. */
		pa = (paddr_t) (ktextp - ktext + va);
	} else if (pm == pmap_kernel() && va >= INTSTACK && va < EINTSTACK) {
		pa = curcpu()->ci_paddr + va - INTSTACK;
	} else {
		int s;

		s = splvm();
		pa = (pseg_get(pm, va) & TLB_PA_MASK) + (va & PGOFSET);
		splx(s);
	}
	if (pa == 0)
		return (FALSE);
	if (pap != NULL)
		*pap = pa;
	return (TRUE);
}

/*
 * Return the number bytes that pmap_dumpmmu() will dump.
 */
int
pmap_dumpsize(void)
{
	int	sz;

	sz = ALIGN(sizeof(kcore_seg_t)) + ALIGN(sizeof(cpu_kcore_hdr_t));
	sz += memsize * sizeof(phys_ram_seg_t);

	return btodb(sz + DEV_BSIZE - 1);
}

/*
 * Write the mmu contents to the dump device.
 * This gets appended to the end of a crash dump since
 * there is no in-core copy of kernel memory mappings on a 4/4c machine.
 *
 * Write the core dump headers and MD data to the dump device.
 * We dump the following items:
 * 
 *	kcore_seg_t		 MI header defined in <sys/kcore.h>)
 *	cpu_kcore_hdr_t		 MD header defined in <machine/kcore.h>)
 *	phys_ram_seg_t[memsize]  physical memory segments
 */
int
pmap_dumpmmu(int (*dump)(dev_t, daddr_t, caddr_t, size_t), daddr_t blkno)
{
	kcore_seg_t	*kseg;
	cpu_kcore_hdr_t	*kcpu;
	phys_ram_seg_t	memseg;
	register int	error = 0;
	register int	i, memsegoffset;
	int		buffer[dbtob(1) / sizeof(int)];
	int		*bp, *ep;

#define EXPEDITE(p,n) do {						\
	int *sp = (int *)(p);						\
	int sz = (n);							\
	while (sz > 0) {						\
		*bp++ = *sp++;						\
		if (bp >= ep) {						\
			error = (*dump)(dumpdev, blkno,			\
					(caddr_t)buffer, dbtob(1));	\
			if (error != 0)					\
				return (error);				\
			++blkno;					\
			bp = buffer;					\
		}							\
		sz -= 4;						\
	}								\
} while (0)

	/* Setup bookkeeping pointers */
	bp = buffer;
	ep = &buffer[sizeof(buffer) / sizeof(buffer[0])];

	/* Fill in MI segment header */
	kseg = (kcore_seg_t *)bp;
	CORE_SETMAGIC(*kseg, KCORE_MAGIC, MID_MACHINE, CORE_CPU);
	kseg->c_size = dbtob(pmap_dumpsize()) - ALIGN(sizeof(kcore_seg_t));

	/* Fill in MD segment header (interpreted by MD part of libkvm) */
	kcpu = (cpu_kcore_hdr_t *)((long)bp + ALIGN(sizeof(kcore_seg_t)));
	kcpu->cputype = CPU_SUN4U;
	kcpu->kernbase = (u_int64_t)KERNBASE;
	kcpu->cpubase = (u_int64_t)CPUINFO_VA;

	/* Describe the locked text segment */
	kcpu->ktextbase = (u_int64_t)ktext;
	kcpu->ktextp = (u_int64_t)ktextp;
	kcpu->ktextsz = (u_int64_t)(roundup(ektextp, 4*MEG) - ktextp);

	/* Describe locked data segment */
	kcpu->kdatabase = (u_int64_t)kdata;
	kcpu->kdatap = (u_int64_t)kdatap;
	kcpu->kdatasz = (u_int64_t)(roundup(ekdatap, 4*MEG) - kdatap);

	/* Now the memsegs */
	kcpu->nmemseg = memsize;
	kcpu->memsegoffset = memsegoffset = ALIGN(sizeof(cpu_kcore_hdr_t));

	/* Now we need to point this at our kernel pmap. */
	kcpu->nsegmap = STSZ;
	kcpu->segmapoffset = (u_int64_t)pmap_kernel()->pm_physaddr;

	/* Note: we have assumed everything fits in buffer[] so far... */
	bp = (int *)((long)kcpu + ALIGN(sizeof(cpu_kcore_hdr_t)));

	for (i = 0; i < memsize; i++) {
		memseg.start = mem[i].start;
		memseg.size = mem[i].size;
		EXPEDITE(&memseg, sizeof(phys_ram_seg_t));
	}

	if (bp != buffer)
		error = (*dump)(dumpdev, blkno++, (caddr_t)buffer, dbtob(1));

	return (error);
}

/*
 * Determine (non)existence of physical page
 */
int pmap_pa_exists(paddr_t pa)
{
	struct mem_region *mp;

	/* Just go through physical memory list & see if we're there */
	for (mp = mem; mp->size && mp->start <= pa; mp++)
		if (mp->start <= pa && mp->start + mp->size >= pa)
			return 1;
	return 0;
}

/*
 * Lookup the appropriate TSB entry.
 *
 * Here is the full official pseudo code:
 *
 */

#ifdef NOTYET
int64 GenerateTSBPointer(
 	int64 va,		/* Missing VA			*/
 	PointerType type,	/* 8K_POINTER or 16K_POINTER	*/
 	int64 TSBBase,		/* TSB Register[63:13] << 13	*/
 	Boolean split,		/* TSB Register[12]		*/
 	int TSBSize)		/* TSB Register[2:0]		*/
{
 	int64 vaPortion;
 	int64 TSBBaseMask;
 	int64 splitMask;
 
	/* TSBBaseMask marks the bits from TSB Base Reg		*/
	TSBBaseMask = 0xffffffffffffe000 <<
		(split? (TSBsize + 1) : TSBsize);

	/* Shift va towards lsb appropriately and		*/
	/* zero out the original va page offset			*/
	vaPortion = (va >> ((type == 8K_POINTER)? 9: 12)) &
		0xfffffffffffffff0;
	
	if (split) {
		/* There's only one bit in question for split	*/
		splitMask = 1 << (13 + TSBsize);
		if (type == 8K_POINTER)
			/* Make sure we're in the lower half	*/
			vaPortion &= ~splitMask;
		else
			/* Make sure we're in the upper half	*/
			vaPortion |= splitMask;
	}
	return (TSBBase & TSBBaseMask) | (vaPortion & ~TSBBaseMask);
}
#endif
/*
 * Of course, since we are not using a split TSB or variable page sizes,
 * we can optimize this a bit.  
 *
 * The following only works for a unified 8K TSB.  It will find the slot
 * for that particular va and return it.  IT MAY BE FOR ANOTHER MAPPING!
 */
int
ptelookup_va(vaddr_t va)
{
	long tsbptr;
#define TSBBASEMASK	(0xffffffffffffe000LL<<tsbsize)

	tsbptr = (((va >> 9) & 0xfffffffffffffff0LL) & ~TSBBASEMASK );
	return (tsbptr/sizeof(pte_t));
}

/*
 * Do whatever is needed to sync the MOD/REF flags
 */

boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int changed = 0;
	pv_entry_t pv;
	
	/* Clear all mappings */
	mtx_enter(&pg->mdpage.pvmtx);
	pv = pa_to_pvh(pa);
	if (pv->pv_va & PV_MOD)
		changed |= 1;
	pv->pv_va &= ~(PV_MOD);
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			int64_t data;

			/* First clear the mod bit in the PTE and make it R/O */
			data = pseg_get(pv->pv_pmap, pv->pv_va & PV_VAMASK);

			/* Need to both clear the modify and write bits */
			if (CPU_ISSUN4V) {
				if (data & (SUN4V_TLB_MODIFY))
					changed |= 1;
				data &= ~(SUN4V_TLB_MODIFY|SUN4V_TLB_W);
			} else {
				if (data & (SUN4U_TLB_MODIFY))
					changed |= 1;
				data &= ~(SUN4U_TLB_MODIFY|SUN4U_TLB_W);
			}
			KDASSERT((data & TLB_NFO) == 0);
			if (pseg_set(pv->pv_pmap, pv->pv_va & PV_VAMASK, data, 0)) {
				printf("pmap_clear_modify: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}
			if (pv->pv_pmap->pm_ctx || pv->pv_pmap == pmap_kernel()) {
				tsb_invalidate(pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				tlb_flush_pte((pv->pv_va & PV_VAMASK),
				    pv->pv_pmap->pm_ctx);
			}
			/* Then clear the mod bit in the pv */
			if (pv->pv_va & PV_MOD)
				changed |= 1;
			pv->pv_va &= ~(PV_MOD);
			dcache_flush_page(pa);
		}
	}
	mtx_leave(&pg->mdpage.pvmtx);

	return (changed);
}

boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int changed = 0;
	pv_entry_t pv;

	/* Clear all references */
	mtx_enter(&pg->mdpage.pvmtx);
	pv = pa_to_pvh(pa);
	if (pv->pv_va & PV_REF)
		changed = 1;
	pv->pv_va &= ~(PV_REF);
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			int64_t data;

			data = pseg_get(pv->pv_pmap, pv->pv_va & PV_VAMASK);
			if (CPU_ISSUN4V) {
				if (data & SUN4V_TLB_ACCESS)
					changed = 1;
				data &= ~SUN4V_TLB_ACCESS;
			} else {
				if (data & SUN4U_TLB_ACCESS)
					changed = 1;
				data &= ~SUN4U_TLB_ACCESS;
			}
			KDASSERT((data & TLB_NFO) == 0);
			if (pseg_set(pv->pv_pmap, pv->pv_va & PV_VAMASK, data, 0)) {
				printf("pmap_clear_reference: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}
			if (pv->pv_pmap->pm_ctx || pv->pv_pmap == pmap_kernel()) {
				tsb_invalidate(pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
/*
				tlb_flush_pte(pv->pv_va & PV_VAMASK, 
					pv->pv_pmap->pm_ctx);
*/
			}
			if (pv->pv_va & PV_REF)
				changed = 1;
			pv->pv_va &= ~(PV_REF);
		}
	}
	/* Stupid here will take a cache hit even on unmapped pages 8^( */
	dcache_flush_page(VM_PAGE_TO_PHYS(pg));
	mtx_leave(&pg->mdpage.pvmtx);

	return (changed);
}

boolean_t
pmap_is_modified(struct vm_page *pg)
{
	pv_entry_t pv, npv;
	int mod = 0;

	/* Check if any mapping has been modified */
	mtx_enter(&pg->mdpage.pvmtx);
	pv = &pg->mdpage.pvent;
	if (pv->pv_va & PV_MOD)
		mod = 1;
	if (!mod && (pv->pv_pmap != NULL)) {
		for (npv = pv; mod == 0 && npv && npv->pv_pmap; npv = npv->pv_next) {
			int64_t data;
			
			data = pseg_get(npv->pv_pmap, npv->pv_va & PV_VAMASK);
			if (pmap_tte2flags(data) & PV_MOD)
				mod = 1;
			/* Migrate modify info to head pv */
			if (npv->pv_va & PV_MOD)
				mod = 1;
			npv->pv_va &= ~PV_MOD;
		}
	}
	/* Save modify info */
	if (mod)
		pv->pv_va |= PV_MOD;
	mtx_leave(&pg->mdpage.pvmtx);

	return (mod);
}

boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	pv_entry_t pv, npv;
	int ref = 0;

	/* Check if any mapping has been referenced */
	mtx_enter(&pg->mdpage.pvmtx);
	pv = &pg->mdpage.pvent;
	if (pv->pv_va & PV_REF)
		ref = 1;
	if (!ref && (pv->pv_pmap != NULL)) {
		for (npv = pv; npv; npv = npv->pv_next) {
			int64_t data;
			
			data = pseg_get(npv->pv_pmap, npv->pv_va & PV_VAMASK);
			if (pmap_tte2flags(data) & PV_REF)
				ref = 1;
			/* Migrate modify info to head pv */
			if (npv->pv_va & PV_REF)
				ref = 1;
			npv->pv_va &= ~PV_REF;
		}
	}
	/* Save ref info */
	if (ref)
		pv->pv_va |= PV_REF;
	mtx_leave(&pg->mdpage.pvmtx);

	return (ref);
}

/*
 *	Routine:	pmap_unwire
 *	Function:	Clear the wired attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 */
void
pmap_unwire(struct pmap *pmap, vaddr_t va)
{
	int64_t data;

	if (pmap == NULL)
		return;

	/*
	 * Is this part of the permanent 4MB mapping?
	 */
	if (pmap == pmap_kernel() && va >= ktext && 
		va < roundup(ekdata, 4*MEG)) {
		prom_printf("pmap_unwire: va=%08x in locked TLB\r\n", va);
		OF_enter();
		return;
	}
	mtx_enter(&pmap->pm_mtx);
	data = pseg_get(pmap, va & PV_VAMASK);

	if (CPU_ISSUN4V)
		data &= ~SUN4V_TLB_TSB_LOCK;
	else
		data &= ~SUN4U_TLB_TSB_LOCK;

	if (pseg_set(pmap, va & PV_VAMASK, data, 0)) {
		printf("pmap_unwire: gotten pseg empty!\n");
		db_enter();
		/* panic? */
	}
	mtx_leave(&pmap->pm_mtx);
}

/*
 * Lower the protection on the specified physical page.
 *
 * Never enable writing as it will break COW
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	pv_entry_t pv;
	int64_t data, clear, set;

	if (prot & PROT_WRITE)
		return;

	if (prot & (PROT_READ | PROT_EXEC)) {
		/* copy_on_write */

		set = TLB_V;
		if (CPU_ISSUN4V) {
			clear = SUN4V_TLB_REAL_W|SUN4V_TLB_W;
			if (PROT_EXEC & prot)
				set |= SUN4V_TLB_EXEC;
			else
				clear |= SUN4V_TLB_EXEC;
		} else {
			clear = SUN4U_TLB_REAL_W|SUN4U_TLB_W;
			if (PROT_EXEC & prot)
				set |= SUN4U_TLB_EXEC;
			else
				clear |= SUN4U_TLB_EXEC;
			if (PROT_EXEC == prot)
				set |= SUN4U_TLB_EXEC_ONLY;
		}

		pv = pa_to_pvh(pa);
		mtx_enter(&pg->mdpage.pvmtx);
		if (pv->pv_pmap != NULL) {
			for (; pv; pv = pv->pv_next) {
				data = pseg_get(pv->pv_pmap, pv->pv_va & PV_VAMASK);

				/* Save REF/MOD info */
				pv->pv_va |= pmap_tte2flags(data);

				data &= ~(clear);
				data |= (set);
				KDASSERT((data & TLB_NFO) == 0);
				if (pseg_set(pv->pv_pmap, pv->pv_va & PV_VAMASK, data, 0)) {
					printf("pmap_page_protect: gotten pseg empty!\n");
					db_enter();
					/* panic? */
				}
				if (pv->pv_pmap->pm_ctx || pv->pv_pmap == pmap_kernel()) {
					tsb_invalidate(pv->pv_pmap->pm_ctx,
					    (pv->pv_va & PV_VAMASK));
					tlb_flush_pte(pv->pv_va & PV_VAMASK, pv->pv_pmap->pm_ctx);
				}
			}
		}
		mtx_leave(&pg->mdpage.pvmtx);
	} else {
		pv_entry_t firstpv;
		/* remove mappings */

		firstpv = pa_to_pvh(pa);
		mtx_enter(&pg->mdpage.pvmtx);

		/* First remove the entire list of continuation pv's*/
		while ((pv = firstpv->pv_next) != NULL) {
			data = pseg_get(pv->pv_pmap, pv->pv_va & PV_VAMASK);

			/* Save REF/MOD info */
			firstpv->pv_va |= pmap_tte2flags(data);

			/* Clear mapping */
			if (pseg_set(pv->pv_pmap, pv->pv_va & PV_VAMASK, 0, 0)) {
				printf("pmap_page_protect: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}
			if (pv->pv_pmap->pm_ctx || pv->pv_pmap == pmap_kernel()) {
				tsb_invalidate(pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				tlb_flush_pte(pv->pv_va & PV_VAMASK, pv->pv_pmap->pm_ctx);
			}
			atomic_dec_long(&pv->pv_pmap->pm_stats.resident_count);

			/* free the pv */
			firstpv->pv_next = pv->pv_next;
			mtx_leave(&pg->mdpage.pvmtx);
			pool_put(&pv_pool, pv);
			mtx_enter(&pg->mdpage.pvmtx);
		}

		pv = firstpv;

		/* Then remove the primary pv */
		if (pv->pv_pmap != NULL) {
			data = pseg_get(pv->pv_pmap, pv->pv_va & PV_VAMASK);

			/* Save REF/MOD info */
			pv->pv_va |= pmap_tte2flags(data);
			if (pseg_set(pv->pv_pmap, pv->pv_va & PV_VAMASK, 0, 0)) {
				printf("pmap_page_protect: gotten pseg empty!\n");
				db_enter();
				/* panic? */
			}
			if (pv->pv_pmap->pm_ctx || pv->pv_pmap == pmap_kernel()) {
				tsb_invalidate(pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				tlb_flush_pte(pv->pv_va & PV_VAMASK,
				    pv->pv_pmap->pm_ctx);
			}
			atomic_dec_long(&pv->pv_pmap->pm_stats.resident_count);

			KASSERT(pv->pv_next == NULL);
			/* dump the first pv */
			pv->pv_pmap = NULL;
		}
		dcache_flush_page(pa);
		mtx_leave(&pg->mdpage.pvmtx);
	}
	/* We should really only flush the pages we demapped. */
}

/*
 * Allocate a context.  If necessary, steal one from someone else.
 * Changes hardware context number and loads segment map.
 *
 * This routine is only ever called from locore.s just after it has
 * saved away the previous process, so there are no active user windows.
 *
 * The new context is flushed from the TLB before returning.
 */
int
ctx_alloc(struct pmap *pm)
{
	int s, cnum;
	static int next = 0;

	if (pm == pmap_kernel()) {
#ifdef DIAGNOSTIC
		printf("ctx_alloc: kernel pmap!\n");
#endif
		return (0);
	}
	s = splvm();
	cnum = next;
	do {
		/*
		 * We use the last context as an "invalid" context in
		 * TSB tags. Never allocate (or bad things will happen).
		 */
		if (cnum >= numctx - 2)
			cnum = 0;
	} while (ctxbusy[++cnum] != 0 && cnum != next);
	if (cnum==0) cnum++; /* Never steal ctx 0 */
	if (ctxbusy[cnum]) {
		int i;
		/* We gotta steal this context */
		for (i = 0; i < TSBENTS; i++) {
			if (TSB_TAG_CTX(tsb_dmmu[i].tag) == cnum)
				tsb_dmmu[i].tag = TSB_TAG_INVALID;
			if (TSB_TAG_CTX(tsb_immu[i].tag) == cnum)
				tsb_immu[i].tag = TSB_TAG_INVALID;
		}
		tlb_flush_ctx(cnum);
	}
	ctxbusy[cnum] = pm->pm_physaddr;
	next = cnum;
	splx(s);
	pm->pm_ctx = cnum;
	return cnum;
}

/*
 * Give away a context.
 */
void
ctx_free(struct pmap *pm)
{
	int oldctx;
	
	oldctx = pm->pm_ctx;

	if (oldctx == 0)
		panic("ctx_free: freeing kernel context");
#ifdef DIAGNOSTIC
	if (ctxbusy[oldctx] == 0)
		printf("ctx_free: freeing free context %d\n", oldctx);
	if (ctxbusy[oldctx] != pm->pm_physaddr) {
		printf("ctx_free: freeing someone esle's context\n "
		       "ctxbusy[%d] = %p, pm(%p)->pm_ctx = %p\n", 
		       oldctx, (void *)(u_long)ctxbusy[oldctx], pm,
		       (void *)(u_long)pm->pm_physaddr);
		db_enter();
	}
#endif
	/* We should verify it has not been stolen and reallocated... */
	ctxbusy[oldctx] = 0;
}

/*
 * Enter the pmap and virtual address into the
 * physical to virtual map table.
 */
pv_entry_t
pmap_enter_pv(struct pmap *pmap, pv_entry_t npv, vaddr_t va, paddr_t pa)
{
	struct vm_page *pg = PHYS_TO_VM_PAGE(pa);
	pv_entry_t pv = &pg->mdpage.pvent;

	mtx_enter(&pg->mdpage.pvmtx);

	if (pv->pv_pmap == NULL) {
		/*
		 * No entries yet, use header as the first entry
		 */
		PV_SETVA(pv, va);
		pv->pv_pmap = pmap;
		pv->pv_next = NULL;

		mtx_leave(&pg->mdpage.pvmtx);
		return (npv);
	}

	if (npv == NULL)
		panic("%s: no pv entries available", __func__);

	if (!(pv->pv_va & PV_ALIAS)) {
		/*
		 * There is at least one other VA mapping this page.
		 * Check if they are cache index compatible. If not
		 * remove all mappings, flush the cache and set page
		 * to be mapped uncached. Caching will be restored
		 * when pages are mapped compatible again.
		 * XXX - caching is not currently being restored, but
		 * XXX - I haven't seen the pages uncached since
		 * XXX - using pmap_prefer().	mhitch
		 */
		if ((pv->pv_va ^ va) & VA_ALIAS_MASK) {
			pv->pv_va |= PV_ALIAS;
			pmap_page_cache(pmap, pa, 0);
		}
	}

	/*
	 * There is at least one other VA mapping this page.
	 * Place this entry after the header.
	 */
	npv->pv_va = va & PV_VAMASK;
	npv->pv_pmap = pmap;
	npv->pv_next = pv->pv_next;
	pv->pv_next = npv;

	mtx_leave(&pg->mdpage.pvmtx);
	return (NULL);
}

/*
 * Remove a physical to virtual address translation.
 */
pv_entry_t
pmap_remove_pv(struct pmap *pmap, vaddr_t va, paddr_t pa)
{
	pv_entry_t pv, opv, npv = NULL;
	struct vm_page *pg = PHYS_TO_VM_PAGE(pa);
	int64_t data = 0LL;

	opv = pv = &pg->mdpage.pvent;
	mtx_enter(&pg->mdpage.pvmtx);

	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && PV_MATCH(pv, va)) {
		/* Save modified/ref bits */
		data = pseg_get(pv->pv_pmap, pv->pv_va & PV_VAMASK);
		npv = pv->pv_next;
		if (npv) {
			/* First save mod/ref bits */
			pv->pv_va = (pv->pv_va & PV_MASK) | npv->pv_va;
			pv->pv_next = npv->pv_next;
			pv->pv_pmap = npv->pv_pmap;
		} else {
			pv->pv_pmap = NULL;
			pv->pv_next = NULL;
			pv->pv_va &= (PV_REF|PV_MOD); /* Only save ref/mod bits */
		}
	} else {
		for (npv = pv->pv_next; npv; pv = npv, npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && PV_MATCH(npv, va))
				goto found;
		}

		/* 
		 * Sometimes UVM gets confused and calls pmap_remove() instead
		 * of pmap_kremove() 
		 */
		mtx_leave(&pg->mdpage.pvmtx);
		return (NULL);
found:
		pv->pv_next = npv->pv_next;

		/*
		 * move any referenced/modified info to the base pv
		 */
		data = pseg_get(npv->pv_pmap, npv->pv_va & PV_VAMASK);

		/* 
		 * Here, if this page was aliased, we should try clear out any
		 * alias that may have occurred.  However, that's a complicated
		 * operation involving multiple scans of the pv list. 
		 */
	}

	/* Save REF/MOD info */
	opv->pv_va |= pmap_tte2flags(data);

	/* Check to see if the alias went away */
	if (opv->pv_va & PV_ALIAS) {
		opv->pv_va &= ~PV_ALIAS;
		for (pv = opv; pv; pv = pv->pv_next) {
			if ((pv->pv_va ^ opv->pv_va) & VA_ALIAS_MASK) {
				opv->pv_va |= PV_ALIAS;
			}
		}
		if (!(opv->pv_va & PV_ALIAS))
			pmap_page_cache(pmap, pa, 1);
	}

	mtx_leave(&pg->mdpage.pvmtx);
	return (npv);
}

/*
 *	pmap_page_cache:
 *
 *	Change all mappings of a page to cached/uncached.
 */
void
pmap_page_cache(struct pmap *pm, paddr_t pa, int mode)
{
	pv_entry_t pv;
	struct vm_page *pg = PHYS_TO_VM_PAGE(pa);

	if (CPU_ISSUN4US || CPU_ISSUN4V)
		return;

	pv = &pg->mdpage.pvent;
	if (pv == NULL)
		return;

	MUTEX_ASSERT_LOCKED(&pg->mdpage.pvmtx);

	while (pv) {
		vaddr_t va;

		va = (pv->pv_va & PV_VAMASK);
		if (pv->pv_va & PV_NC) {
			/* Non-cached -- I/O mapping */
			if (pseg_set(pv->pv_pmap, va,
			    pseg_get(pv->pv_pmap, va) & ~(SUN4U_TLB_CV|SUN4U_TLB_CP),
				     0)) {
				printf("pmap_page_cache: aliased pseg empty!\n");
				db_enter();
				/* panic? */
			}
		} else if (mode && (!(pv->pv_va & PV_NVC))) {
			/* Enable caching */
			if (pseg_set(pv->pv_pmap, va,
			    pseg_get(pv->pv_pmap, va) | SUN4U_TLB_CV, 0)) {
				printf("pmap_page_cache: aliased pseg empty!\n");
				db_enter();
				/* panic? */
			}
		} else {
			/* Disable caching */
			if (pseg_set(pv->pv_pmap, va,
			    pseg_get(pv->pv_pmap, va) & ~SUN4U_TLB_CV, 0)) {
				printf("pmap_page_cache: aliased pseg empty!\n");
				db_enter();
				/* panic? */
			}
		}
		if (pv->pv_pmap->pm_ctx || pv->pv_pmap == pmap_kernel()) {
			tsb_invalidate(pv->pv_pmap->pm_ctx, va);
			/* Force reload -- protections may be changed */
			tlb_flush_pte(va, pv->pv_pmap->pm_ctx);	
		}

		pv = pv->pv_next;
	}
}

int
pmap_get_page(paddr_t *pa, const char *wait, struct pmap *pm)
{
	int reserve = pm == pmap_kernel() ? UVM_PGA_USERESERVE : 0;

	if (uvm.page_init_done) {
		struct vm_page *pg;

		while ((pg = uvm_pagealloc(NULL, 0, NULL,
		    UVM_PGA_ZERO|reserve)) == NULL) {
			if (wait == NULL)
				return 0;
			uvm_wait(wait);
		}
		pg->wire_count++;
		atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
		*pa = VM_PAGE_TO_PHYS(pg);
	} else {
		uvm_page_physget(pa);
		prom_claim_phys(*pa, PAGE_SIZE);
		pmap_zero_phys(*pa);
	}

	return (1);
}

void
pmap_free_page(paddr_t pa, struct pmap *pm)
{
	struct vm_page *pg = PHYS_TO_VM_PAGE(pa);

	pg->wire_count = 0;
	uvm_pagefree(pg);
}

void
pmap_remove_holes(struct vmspace *vm)
{
	vaddr_t shole, ehole;
	struct vm_map *map = &vm->vm_map;

	/*
	 * Although the hardware only supports 44-bit virtual addresses
	 * (and thus a hole from 1 << 43 to -1 << 43), this pmap
	 * implementation itself only supports 43-bit virtual addresses,
	 * so we have to narrow the hole a bit more.
	 */
	shole = 1L << (HOLESHIFT - 1);
	ehole = -1L << (HOLESHIFT - 1);

	shole = ulmax(vm_map_min(map), shole);
	ehole = ulmin(vm_map_max(map), ehole);

	if (ehole <= shole)
		return;

	(void)uvm_map(map, &shole, ehole - shole, NULL, UVM_UNKNOWN_OFFSET, 0,
	    UVM_MAPFLAG(PROT_NONE, PROT_NONE, MAP_INHERIT_SHARE, MADV_RANDOM,
	      UVM_FLAG_NOMERGE | UVM_FLAG_HOLE | UVM_FLAG_FIXED));
}

#ifdef DDB

void
db_dump_pv(db_expr_t addr, int have_addr, db_expr_t count, char *modif)
{
	struct pv_entry *pv;

	if (!have_addr) {
		db_printf("Need addr for pv\n");
		return;
	}

	for (pv = pa_to_pvh(addr); pv; pv = pv->pv_next)
		db_printf("pv@@%p: next=%p pmap=%p va=0x%llx\n",
			  pv, pv->pv_next, pv->pv_pmap,
			  (unsigned long long)pv->pv_va);
	
}

#endif
@


1.100
log
@explicitly initialise the mutex in the kernel map.

previously it was zeroed memory from a global, which kind of looks
like a valid mutex, but we shouldnt rely on that.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2017/04/30 16:45:45 mpi Exp $	*/
d2873 1
@


1.99
log
@Rename Debugger() into db_enter().

Using a name with the 'db_' prefix makes it invisible from the dynamic
profiler.

ok deraadt@@, kettenis@@, visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2016/09/15 02:00:17 dlg Exp $	*/
d1044 1
@


1.98
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2016/06/07 06:23:19 dlg Exp $	*/
d61 1
a61 1
#define Debugger()	__asm volatile("ta 1; nop");
d63 1
a63 1
#define Debugger()
d1529 1
a1529 1
							Debugger();
d1752 1
a1752 1
				Debugger();
d1934 1
a1934 1
				Debugger();
d2020 1
a2020 1
				Debugger();
d2277 1
a2277 1
				Debugger();
d2328 1
a2328 1
				Debugger();
d2449 1
a2449 1
		Debugger();
d2504 1
a2504 1
					Debugger();
d2532 1
a2532 1
				Debugger();
d2559 1
a2559 1
				Debugger();
d2650 1
a2650 1
		Debugger();
d2823 1
a2823 1
				Debugger();
d2831 1
a2831 1
				Debugger();
d2839 1
a2839 1
				Debugger();
@


1.97
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2015/11/27 15:34:01 kettenis Exp $	*/
d1381 3
a1383 3
	pool_init(&pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pv_entry", NULL);
	pool_setipl(&pv_pool, IPL_VM);
	pool_init(&pmap_pool, sizeof(struct pmap), 0, 0, 0,
a1384 1
	pool_setipl(&pmap_pool, IPL_NONE);
@


1.96
log
@Don't panic in pmap_enter() if we deplete the pool of pv entries and the
PMAP_CANFAIL flag is set.  Return ENOMEM instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2015/11/25 21:13:28 kettenis Exp $	*/
d1383 1
a1383 1
	pool_init(&pmap_pool, sizeof(struct pmap), 0, 0, PR_WAITOK,
d1385 1
@


1.95
log
@Don't bother keeping track of the resident count for the kernel pmap.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2015/09/02 21:59:29 kettenis Exp $	*/
d1786 2
@


1.94
log
@Make pmap_kenter_pa(9) and pmap_kremove(9) mpsafe.  This required me to
use atomic operations to operate on pm_stats.resident_count.  Is it really
necessary to keep track of that for the kernel pmap?
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2015/08/30 16:47:43 kettenis Exp $	*/
a307 1
		atomic_inc_long(&pmap_kernel()->pm_stats.resident_count);
a1719 2
	atomic_inc_long(&pm->pm_stats.resident_count);

a1755 1
			atomic_dec_long(&pm->pm_stats.resident_count);
d2567 1
@


1.93
log
@The pmap_pool pool will never be used in interrupt context, so pass the
PR_WAITOK flag to pmap_init and pass NULL as the pool allocator.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2015/07/10 12:11:41 kettenis Exp $	*/
d308 1
a308 1
		pmap_kernel()->pm_stats.resident_count++;
d1677 1
a1678 2
	struct pmap *pm = pmap_kernel();
	int s;
d1685 1
a1685 1
		panic("pmap_kenter_pa: illegal cache flags %ld", pa);
d1691 1
a1691 2
	s = splvm();
	tte.tag = TSB_TAG(0,pm->pm_ctx,va);
d1718 2
a1719 2
	if (pseg_set(pmap_kernel(), va, tte.data, 0) != 0)
		panic("pmap_kenter_pa: no pseg");
d1721 1
a1721 1
	pmap_kernel()->pm_stats.resident_count++;
a1722 1
	splx(s);
a1736 2
	int64_t data;
	int s;
a1740 1
	s = splvm();
d1747 2
a1748 3
			(va >= ktext && va < roundup(ekdata, 4*MEG)))
			panic("pmap_kremove: va=%08x in locked TLB", 
				(u_int)va);
d1751 1
a1751 1
		if ((data = pseg_get(pm, va))) {
d1758 2
a1759 2
			
			pmap_kernel()->pm_stats.resident_count--;
a1766 1
	splx(s);
d1869 1
a1869 1
	pm->pm_stats.resident_count++;
d1939 1
a1939 1
			pm->pm_stats.resident_count--;
d2542 1
a2542 1
			pv->pv_pmap->pm_stats.resident_count--;
d2570 1
a2570 1
			pv->pv_pmap->pm_stats.resident_count--;
@


1.92
log
@Avoid calling pool_put(9) while holding a mutex here as well to prevent lock
order problems.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2015/04/10 18:08:31 kettenis Exp $	*/
d1384 2
a1385 2
	pool_init(&pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
@


1.91
log
@Don't use pa_to_pvh() if we already have the page.  Avoids an unused variable
for non-DIAGNOSTIC kernels.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2015/04/09 16:56:52 kettenis Exp $	*/
d127 2
a128 2
void	pmap_remove_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
void	pmap_enter_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
d1787 1
a1787 1
	pv_entry_t pv = NULL;
d1797 2
d1869 2
d1875 2
a1876 2
	if (pv)
		pmap_enter_pv(pm, va, pa);
d1888 3
d1901 1
a1928 1
			pv_entry_t pv;
d1934 7
a1940 2
			if (pv != NULL)
				pmap_remove_pv(pm, va, entry);
d1956 1
d1958 7
a1964 1
	if (flush) {
a1965 1
	}
d2670 2
a2671 2
void
pmap_enter_pv(struct pmap *pmap, vaddr_t va, paddr_t pa)
a2672 1
	pv_entry_t pv, npv = NULL;
d2674 1
a2675 1
	pv = pa_to_pvh(pa);
a2677 1
retry:
d2685 1
d2687 1
a2687 3
		if (npv)
			pool_put(&pv_pool, npv);
		return;
d2690 2
a2691 8
	if (npv == NULL) {
		mtx_leave(&pg->mdpage.pvmtx);
		npv = pool_get(&pv_pool, PR_NOWAIT);
		if (npv == NULL)
			panic("%s: no pv entries available", __func__);
		mtx_enter(&pg->mdpage.pvmtx);
		goto retry;
	}
d2720 1
d2726 1
a2726 1
void
d2733 1
a2733 1
	opv = pv = pa_to_pvh(pa);
d2767 1
a2767 1
		return; 
d2799 1
a2799 3

	if (npv)
		pool_put(&pv_pool, npv);
@


1.90
log
@Make the sparc64 pmap (more) mpsafe by protecting both the pmap itself and the
pv lists with a mutex.  Some minor code adjustments to prevent holding locks
too long.  This should make pmap_enter(9), pmap_remove(9) and
pmap_page_protect(9) safe to use without holding the kernel lock.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2015/04/08 14:02:43 kettenis Exp $	*/
d2808 1
a2808 1
	pv = pa_to_pvh(pa);
@


1.89
log
@I believe UVM is clean now and no managed pages should be in use anymore
when we call pmap_release().  Turn debug code into printf and remove the code
that tried to mop up.  Override Debugger() such that it inserts a trap
instruction directly into the code such that we get useful backtraces.

This way I don't need to worry about pmap_release() removing pv entries when
making this pmap mpsafe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2015/02/15 21:34:33 miod Exp $	*/
d1383 1
d1461 1
d1476 1
a1476 1
	pm->pm_refs++;
d1486 1
a1486 1
	if (--pm->pm_refs == 0) {
d1499 1
a1499 1
	int i, j, k, s;
d1507 1
a1507 1
	s=splvm();
d1544 1
a1544 1
	splx(s);
d1786 1
a1786 1
	int s, aliased = 0;
d1800 1
a1800 1
	s = splvm();
d1803 1
d1805 1
d1814 4
a1817 1
		aliased = (pv->pv_va&(PV_ALIAS|PV_NVC));
d1828 1
d1832 2
a1833 1
	if (pa & PMAP_NVC) aliased = 1;
d1866 1
a1866 1
			splx(s);
d1874 1
a1874 1
	splx(s);
d1894 1
a1894 1
	int s, flush = 0;
d1906 1
a1906 1
	s = splvm();
d1925 1
a1925 1
			entry = (data&TLB_PA_MASK);
d1927 1
a1927 3
			if (pv != NULL) {
				/* Save REF/MOD info */
				pv->pv_va |= pmap_tte2flags(data);
a1928 1
			}
d1944 1
a1944 1
	splx(s);
a1955 1
	int s;
d1972 1
a1972 1
	s = splvm();
d1982 1
a1982 1
			splx(s);
d1987 1
a1987 1
			pa = data&TLB_PA_MASK;
d1990 2
d1993 1
d1995 1
d2023 1
a2023 1
	splx(s);
d2048 1
a2048 1
		pa = (pseg_get(pm, va)&TLB_PA_MASK)+(va&PGOFSET);
a2241 1
	int s;
d2244 1
a2244 1
	s = splvm();
d2285 1
a2285 1
	splx(s);
a2295 1
	int s;
d2298 1
a2298 1
	s = splvm();
d2338 1
a2338 1
	splx(s);
a2347 1
	int s;
d2350 1
a2350 1
	s = splvm();
d2370 1
a2370 1
	splx(s);
a2379 1
	int s;
d2382 1
a2382 1
	s = splvm();
d2402 1
a2402 1
	splx(s);
a2417 1
	int s;
d2431 2
a2432 2
	s = splvm();
	data = pseg_get(pmap, va&PV_VAMASK);
d2439 1
a2439 1
	if (pseg_set(pmap, va&PV_VAMASK, data, 0)) {
d2444 1
a2444 1
	splx(s);
a2457 1
	int s;
d2483 1
a2483 1
		s = splvm();
d2506 1
a2506 1
		splx(s);
d2508 1
a2508 1
		pv_entry_t npv, firstpv;
d2511 2
a2512 2
		firstpv = pv = pa_to_pvh(pa);
		s = splvm();
d2515 2
a2516 3
		for (npv = pv->pv_next; npv; npv = pv->pv_next) {
			/* We're removing npv from pv->pv_next */
			data = pseg_get(npv->pv_pmap, npv->pv_va & PV_VAMASK);
d2522 1
a2522 1
			if (pseg_set(npv->pv_pmap, npv->pv_va & PV_VAMASK, 0, 0)) {
d2527 4
a2530 4
			if (npv->pv_pmap->pm_ctx || npv->pv_pmap == pmap_kernel()) {
				tsb_invalidate(npv->pv_pmap->pm_ctx,
				    (npv->pv_va & PV_VAMASK));
				tlb_flush_pte(npv->pv_va & PV_VAMASK, npv->pv_pmap->pm_ctx);
d2532 1
a2532 1
			npv->pv_pmap->pm_stats.resident_count--;
d2535 4
a2538 2
			pv->pv_next = npv->pv_next;
			pool_put(&pv_pool, npv);
d2549 1
a2549 1
			if (pseg_set(pv->pv_pmap, pv->pv_va&PV_VAMASK, 0, 0)) {
d2561 1
a2561 1
			npv = pv->pv_next;
d2563 1
a2563 10
			if (npv) {
				/* First save mod/ref bits */
				pv->pv_va |= (npv->pv_va & PV_MASK);
				pv->pv_next = npv->pv_next;
				pv->pv_pmap = npv->pv_pmap;
				pool_put(&pv_pool, npv);
			} else {
				pv->pv_pmap = NULL;
				pv->pv_next = NULL;
			}
d2566 1
a2566 1
		splx(s);
d2655 2
a2656 2
	pv_entry_t pv, npv;
	int s;
d2659 3
a2661 1
	s = splvm();
d2669 16
a2684 17
	} else {
		if (!(pv->pv_va & PV_ALIAS)) {
			/*
			 * There is at least one other VA mapping this page.
			 * Check if they are cache index compatible. If not
			 * remove all mappings, flush the cache and set page
			 * to be mapped uncached. Caching will be restored
			 * when pages are mapped compatible again.
			 * XXX - caching is not currently being restored, but
			 * XXX - I haven't seen the pages uncached since
			 * XXX - using pmap_prefer().	mhitch
			 */
			if ((pv->pv_va ^ va) & VA_ALIAS_MASK) {
				pv->pv_va |= PV_ALIAS;
				pmap_page_cache(pmap, pa, 0);
			}
		}
d2687 7
a2693 4
		 * Place this entry after the header.
		 *
		 * Note: the entry may already be in the table if
		 * we are only changing the protection bits.
d2695 3
a2697 4
		for (npv = pv; npv; npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && PV_MATCH(npv, va)) {
				goto found;
			}
a2698 12
		/* 
		 * XXXXX can this cause us to recurse forever? 
		 */
		npv = pool_get(&pv_pool, PR_NOWAIT);
		if (npv == NULL)
			panic("pmap_enter: new pv malloc() failed");
		npv->pv_va = va&PV_VAMASK;
		npv->pv_pmap = pmap;
		npv->pv_next = pv->pv_next;
		pv->pv_next = npv;
found:
		;
d2700 11
a2710 1
	splx(s);
d2719 2
a2720 1
	pv_entry_t pv, opv, npv;
a2721 1
	int s;
a2722 4
	/*
	 * Remove page from the PV table (raise IPL since we
	 * may be called at interrupt time).
	 */
d2724 1
a2724 1
	s = splvm();
a2740 1
			pool_put(&pv_pool, npv);
d2756 1
a2756 1
		splx(s);
a2770 1
		pool_put(&pv_pool, npv);
d2779 2
a2780 2
		for (npv = opv; npv; npv = npv->pv_next) {
			if ((npv->pv_va ^ opv->pv_va) & VA_ALIAS_MASK) {
d2787 5
a2791 1
	splx(s);
d2803 1
a2803 1
	int s;
d2811 2
a2812 1
	s = splvm();
a2851 2

	splx(s);
@


1.88
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2015/02/10 01:59:43 kettenis Exp $	*/
d61 1
a1525 2

#ifdef DEBUG
a1528 7
#endif
							/* Save REF/MOD info */
							pv->pv_va |= pmap_tte2flags(data);

							pmap_remove_pv(pm, 
								       (long)((u_int64_t)i<<STSHIFT)|((long)k<<PDSHIFT)|((long)j<<PTSHIFT), 
								       pa);
@


1.87
log
@Cleanup debug crap that hasn't been used in the last decade.  Sprinkle some
KNF while I'm there.  Also remove some #if 0'ed code that I'm not planning
to use in the near future.

Apologies to deraadt@@ for ignoring his diff and doing it my own way.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2015/01/10 19:52:37 kettenis Exp $	*/
d2904 1
a2904 1
pmap_remove_holes(struct vm_map *map)
d2907 1
@


1.86
log
@Bye, bye simple_lock.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2014/12/17 06:58:10 guenther Exp $	*/
a72 37
#if 0
static int pseg_check(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
static int
pseg_check(struct pmap *pm, vaddr_t addr, int64_t tte, paddr_t spare)
{
	int i, k, s;
	paddr_t *pdir, *ptbl;
	extern int pseg_set(struct pmap*, vaddr_t addr, int64_t tte,
		paddr_t spare);

	if (!spare) return pseg_set(pm, addr, tte, spare);

	s = splvm();
	if ((paddr_t)pm->pm_segs == spare) panic("pseg_check: pm_segs == %llx", spare);
	for (i=0; i<STSZ; i++) {
		if ((pdir = (paddr_t *)(u_long)ldxa((vaddr_t)&pm->pm_segs[i], ASI_PHYS_CACHED))) {
			if ((paddr_t)pdir == spare)
				panic("pseg_check: pdir %d == %llx", i,
					spare);
			for (k=0; k<PDSZ; k++) {
				if ((ptbl = (paddr_t *)(u_long)ldxa((vaddr_t)&pdir[k], ASI_PHYS_CACHED))) {
					if ((paddr_t)ptbl == spare)
				panic("pseg_check: ptbl %d:%d == %llx", i, k,
					spare);
				}
			}
		}
	}
	splx(s);
	if (addr == -1) return 0;
	return pseg_set(pm, addr, tte, spare);
}
#define pseg_check(a, b, c, d)
#define cache_flush_phys(a, b, c)
/* #define pseg_set(a, b, c, d)	pseg_check(a, b, c, d) */
#endif

a73 1
#if 1
a75 36
#else
static int64_t pseg_get(struct pmap*, vaddr_t addr);
static int pseg_set(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);

static int64_t pseg_get(struct pmap* pm, vaddr_t addr) {
	paddr_t *pdir, *ptbl;
	
	if ((pdir = (paddr_t *)ldda(&pm->pm_segs[va_to_seg(addr)],
				    ASI_PHYS_CACHED)) &&
	    (ptbl = (paddr_t *)ldda(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED)))
		return  (ldda(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED));
	return (0);
}

static int pseg_set(struct pmap* pm, vaddr_t addr, int64_t tte, paddr_t spare) {
	int i, j, k, s;
	paddr_t *pdir, *ptbl;

	if (!(pdir = (paddr_t *)ldda(&pm->pm_segs[va_to_seg(addr)],
	    ASI_PHYS_CACHED))) {
		if (!spare) return (1);
		stxa_sync(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
		pdir = spare;
		spare = NULL;
	}
	if (!(ptbl = (paddr_t *)ldda(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED))) {
		if (!spare) return (1);
		stxa_sync(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
		ptbl = spare;
		spare = NULL;
	}
	stxa_sync(&ptbl[va_to_pte(addr)], ASI_PHYS_CACHED, tte);
	return (0);
}

#endif
d105 2
a106 2
#ifdef	NO_VCACHE
#define	FORCE_ALIAS	1
d118 3
a120 3
#define PV_VAMASK	(~(NBPG-1))
#define PV_MATCH(pv,va)	(!((((pv)->pv_va)^(va))&PV_VAMASK))
#define PV_SETVA(pv,va) ((pv)->pv_va = (((va)&PV_VAMASK)|(((pv)->pv_va)&PV_MASK)))
d125 4
a128 3
extern void	pmap_remove_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
extern void	pmap_enter_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
extern void	pmap_page_cache(struct pmap *pm, paddr_t pa, int mode);
d163 2
a164 2
int tsbsize;		/* tsbents = 512 * 2^^tsbsize */
#define TSBENTS (512<<tsbsize)
d166 1
a240 16
#if notyet
static void tsb_enter(int ctx, int64_t va, int64_t data);
#endif

struct pmap_stats {
	int	ps_unlink_pvfirst;	/* # of pv_unlinks on head */
	int	ps_unlink_pvsearch;	/* # of pv_unlink searches */
	int	ps_changeprots;		/* # of calls to changeprot */
	int	ps_useless_changeprots;	/* # of changeprots for wiring */
	int	ps_enter_firstpv;	/* pv heads entered */
	int	ps_enter_secondpv;	/* pv nonheads entered */
	int	ps_useless_changewire;	/* useless wiring changes */
	int	ps_npg_prot_all;	/* # of active pages protected */
	int	ps_npg_prot_actual;	/* # pages actually affected */
} pmap_stats;

a244 42
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
	int cachehit;	/* new entry forced valid entry out */
} enter_stats;
struct {
	int calls;
	int removes;
	int flushes;
	int tflushes;	/* TLB flushes */
	int pidflushes;	/* HW pid stolen */
	int pvfirst;
	int pvsearch;
} remove_stats;
#define	PDB_CREATE	0x0001
#define	PDB_DESTROY	0x0002
#define	PDB_REMOVE	0x0004
#define	PDB_CHANGEPROT	0x0008
#define	PDB_ENTER	0x0010
#define PDB_DEMAP	0x0020
#define	PDB_REF		0x0040
#define PDB_COPY	0x0080

#define	PDB_MMU_ALLOC	0x0100
#define	PDB_MMU_STEAL	0x0200
#define	PDB_CTX_ALLOC	0x0400
#define	PDB_CTX_STEAL	0x0800
#define	PDB_MMUREG_ALLOC	0x1000
#define	PDB_MMUREG_STEAL	0x2000
#define	PDB_CACHESTUFF	0x4000
#define	PDB_ALIAS	0x8000
#define PDB_EXTRACT	0x10000
a246 1
#define	PDB_GROW	0x80000
a247 2
/* Number of H/W pages stolen for page tables */
int	pmap_pages_stolen = 0;
a249 1
#define	DPRINTF(n, f)	if (pmapdebug & (n)) printf f 
a251 26
#define	DPRINTF(n, f)
#endif

#ifdef NOTDEF_DEBUG
void pv_check(void);
void
pv_check()
{
	int i, s;
	
	s = splhigh();
	for (i = 0; i < physmem; i++) {
		struct pv_entry *pv;
		for (pv = &pv_table[i]; pv; pv = pv->pv_next) {
			if (pv->pv_pmap &&
			    !(pseg_get(pv->pv_pmap, pv->pv_va)&TLB_V)) {
		printf("pv_check(): unreferenced pv=%p pa=%p va=%p pm=%p\n",
		       i, ptoa(first_phys_addr+i), pv->pv_va, pv->pv_pmap);
				Debugger();
			}
		}
	}
	splx(s);
}
#else
#define pv_check()
d270 1
a270 1
#define CTXSIZE		(numctx*CTXENTRY)
a280 3
#ifdef DEBUG
	{ 0, PGSZ_8K&0  },	/* Disable large pages */
#endif
a294 1
static void pmap_enter_kpage(vaddr_t, int64_t);
a318 1
void pmap_bootdebug(void);
d320 1
a320 1
pmap_bootdebug() 
a349 37
#ifdef notyet
/*
 * Calculate the correct number of page colors to use.  This should be the
 * size of the E$/NBPG.  However, different CPUs can have different sized
 * E$, so we need to take the GCM of the E$ size.
 */
static int pmap_calculate_colors(void);
static int 
pmap_calculate_colors() {
	int node = 0;
	int size, assoc, color, maxcolor = 1;
	char buf[80];

	while ((node = OF_peer(node))) {
		if ((OF_getprop(node, "device_type", buf, sizeof(buf)) > 0) &&
			strcmp("cpu", buf) == 0) {
			/* Found a CPU, get the E$ info. */
			if (OF_getprop(node,"ecache-size", &size, 
				sizeof(size)) != sizeof(size)) {
				printf("pmap_calculate_colors: node %x has "
					"no ecache-size\n", node);
				/* If we can't get the E$ size, skip the node */
				continue;
			}
			if (OF_getprop(node, "ecache-associativity", &assoc,
				sizeof(assoc)) != sizeof(assoc))
				/* Fake asociativity of 1 */
				assoc = 1;
			color = size/assoc/NBPG;
			if (color > maxcolor)
				maxcolor = color;
		}
	}
	return (maxcolor);
}
#endif

d371 1
a371 3
pmap_bootstrap(kernelstart, kernelend, maxctx, numcpus)
	u_long kernelstart, kernelend;
	u_int maxctx, numcpus;
a395 3
#ifdef notyet
	uvmexp.ncolors = pmap_calculate_colors();
#endif
a498 1
#if 1
a499 4
#else
#define	valloc(name, type, num) (name) = (type *)firstaddr; firstaddr = \
	(vaddr_t)((name)+(num))
#endif
a1007 10
#ifdef DEBUG
/* Clear all memory we give to the VM system.  I want to make sure
 * the PROM isn't using it for something, so this should break the PROM.
 */
		{
			paddr_t p;
			for (p = mp->start; p < mp->start+mp->size; p += NBPG)
				pmap_zero_phys(p);
		}
#endif
d1095 2
a1096 2
	for (i = 0; i < prom_map_size; i++)
		if (prom_map[i].vstart && ((prom_map[i].vstart>>32) == 0))
a1107 3
#ifdef DEBUG
				page_size_map[k].use++;
#endif
d1113 2
d1374 1
a1374 1
pmap_init()
a1375 1

d1389 2
a1393 1
static vaddr_t kbreak; /* End of kernel VA */
d1395 1
a1395 2
pmap_virtual_space(start, end)
	vaddr_t *start, *end;
a1431 1
	DPRINTF(PDB_GROW, ("pmap_growkernel(%lx...%lx)\n", kbreak, maxkvaddr));
a1439 2
			DPRINTF(PDB_GROW, 
				("pmap_growkernel: extending %lx\n", kbreak));
a1456 2
	DPRINTF(PDB_CREATE, ("pmap_create()\n"));

a1457 1
	DPRINTF(PDB_CREATE, ("pmap_create(): created %p\n", pm));
d1481 1
a1481 2
pmap_destroy(pm)
	struct pmap *pm;
a1483 4
#ifdef DEBUG
		if (pmapdebug & PDB_DESTROY)
			printf("pmap_destroy: freeing pmap %p\n", pm);
#endif
d1494 1
a1494 2
pmap_release(pm)
	struct pmap *pm;
a1549 14
#ifdef NOTDEF_DEBUG
	for (i=0; i<physmem; i++) {
		struct pv_entry *pv;
		for (pv = &pv_table[i]; pv; pv=pv->pv_next) {
			if (pv->pv_pmap == pm) {
				printf("pmap_release(): unreferenced pv=%p pa=%p va=%p pm=%p\n",
				       i, ptoa(first_phys_addr+i), pv->pv_va, pm);
				Debugger();
				pmap_remove_pv(pm, pv->pv_va, i);
				break;
			}
		}
	}
#endif
d1562 2
a1563 4
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	struct pmap *dst_pmap, *src_pmap;
	vaddr_t dst_addr, src_addr;
	vsize_t len;
a1564 6
#ifdef DEBUG
	if (pmapdebug&PDB_CREATE)
		printf("pmap_copy(%p, %p, %p, %lx, %p)\n",
		       dst_pmap, src_pmap, (void *)(u_long)dst_addr,
		       (u_long)len, (void *)(u_long)src_addr);
#endif
d1576 1
a1576 2
pmap_collect(pm)
	struct pmap *pm;
d1585 2
a1586 1
	if (pm == pmap_kernel()) return;
a1621 1

d1639 1
a1639 2
pmap_activate(p)
	struct proc *p;
d1641 1
a1641 1
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d1668 1
a1668 2
pmap_deactivate(p)
	struct proc *p;
d1681 1
a1681 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
a1698 3
#ifdef DEBUG	     
	enter_stats.unmanaged ++;
#endif
d1743 1
a1743 3
pmap_kremove(va, size)
	vaddr_t va;
	vsize_t size;
d1747 1
a1747 1
	int s, flush = 0;
a1752 6
#ifdef DEBUG
	if (pmapdebug & PDB_DEMAP) {
		printf("pmap_kremove: start %p size %lx\n",
		    (void *)(u_long)va, size);
	}
#endif
a1770 7
#ifdef DEBUG
			if (pmapdebug & PDB_DEMAP)
				printf("pmap_kremove: clearing seg %x pdir %x pte %x\n", 
				       (int)va_to_seg(va), (int)va_to_dir(va), 
				       (int)va_to_pte(va));
			remove_stats.removes ++;
#endif
a1773 3
#ifdef DEBUG
			remove_stats.tflushes ++;
#endif
a1775 1
			flush = 1;
a1779 5
	if (flush) {
#ifdef DEBUG
		remove_stats.flushes ++;
#endif
	}
d1788 1
a1788 6
pmap_enter(pm, va, pa, prot, flags)
	struct pmap *pm;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
a1802 8
#ifdef DEBUG
	/* Trap mapping of page zero */
	if (va == 0) {
		prom_printf("pmap_enter: NULL va=%08x pa=%x:%08x\r\n", 
			    va, (int)(pa>>32), (int)pa);
		OF_enter();
	}
#endif
d1809 1
a1809 1
		pmap_remove(pm, va, va+NBPG-1);
a1828 3
#ifdef DEBUG
		enter_stats.managed ++;
#endif
a1829 3
#ifdef DEBUG	     
		enter_stats.unmanaged ++;
#endif
a1835 3
#ifdef DEBUG
	enter_stats.ci ++;
#endif
a1883 1
	pv_check();
d1891 1
a1891 3
pmap_remove(pm, va, endva)
	struct pmap *pm;
	vaddr_t va, endva;
d1893 1
a1893 1
	int s, flush=0;
a1905 6
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("pmap_remove(pm=%p, va=%p, endva=%p):", pm,
		    (void *)(u_long)va, (void *)(u_long)endva);
	remove_stats.calls ++;
#endif
a1937 5
#ifdef DEBUG
			if (pmapdebug & PDB_REMOVE)
				printf(" clearing seg %x pte %x\n", (int)va_to_seg(va), (int)va_to_pte(va));
			remove_stats.removes++;
#endif
a1940 3
#ifdef DEBUG
			remove_stats.tflushes ++;
#endif
a1947 3
#ifdef DEBUG
		remove_stats.flushes ++;
#endif
a1949 5
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("\n");
#endif
	pv_check();
d1956 1
a1956 4
pmap_protect(pm, sva, eva, prot)
	struct pmap *pm;
	vaddr_t sva, eva;
	vm_prot_t prot;
a1988 4
#ifdef DEBUG
		if (pmapdebug & PDB_CHANGEPROT)
			printf("pmap_protect: va %p\n", (void *)(u_long)sva);
#endif
a1990 6
#ifdef DEBUG
			if (pmapdebug & (PDB_CHANGEPROT|PDB_REF)) {
				printf("pmap_protect: va=%08x data=%x:%08x seg=%08x pte=%08x\r\n", 
					    (u_int)sva, (int)(pa>>32), (int)pa, (int)va_to_seg(sva), (int)va_to_pte(sva));
			}
#endif
a2022 1
	pv_check();
d2030 1
a2030 4
pmap_extract(pm, va, pap)
	register struct pmap *pm;
	vaddr_t va;
	paddr_t *pap;
a2037 5
#ifdef DEBUG
		if (pmapdebug & PDB_EXTRACT) {
			printf("pmap_extract: va=%lx pa=%llx\n", (u_long)va, (unsigned long long)pa);
		}
#endif
a2040 6
#ifdef DEBUG
		if (pmapdebug & PDB_EXTRACT) {
			printf("pmap_extract: va=%lx pa=%llx\n",
			    (u_long)va, (unsigned long long)pa);
		}
#endif
a2042 6
#ifdef DEBUG
		if (pmapdebug & PDB_EXTRACT) {
			printf("pmap_extract: va=%lx pa=%llx\n",
			    (u_long)va, (unsigned long long)pa);
		}
#endif
a2047 18
#ifdef DEBUG
		if (pmapdebug & PDB_EXTRACT) {
			paddr_t npa;

			npa = ldxa((vaddr_t)&pm->pm_segs[va_to_seg(va)], ASI_PHYS_CACHED);
			printf("pmap_extract: va=%p segs[%ld]=%llx", (void *)(u_long)va, (long)va_to_seg(va), (unsigned long long)npa);
			if (npa) {
				npa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)npa)[va_to_dir(va)], ASI_PHYS_CACHED);
				printf(" segs[%ld][%ld]=%lx", (long)va_to_seg(va), (long)va_to_dir(va), (long)npa);
			}
			if (npa) {
				npa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)npa)[va_to_pte(va)], ASI_PHYS_CACHED);
				printf(" segs[%ld][%ld][%ld]=%lx", (long)va_to_seg(va), 
				       (long)va_to_dir(va), (long)va_to_pte(va), (long)npa);
			}
			printf(" pseg_get: %lx\n", (long)npa);
		}
#endif
d2061 1
a2061 1
pmap_dumpsize()
d2084 1
a2084 3
pmap_dumpmmu(dump, blkno)
	register daddr_t blkno;
	register int (*dump)(dev_t, daddr_t, caddr_t, size_t);
d2162 1
a2162 2
int pmap_pa_exists(pa)
paddr_t pa;
d2164 1
a2164 1
	register struct mem_region *mp;
d2168 1
a2168 1
		if( mp->start <= pa && mp->start + mp->size >= pa )
d2222 1
a2222 2
ptelookup_va(va)
	vaddr_t va;
a2230 26
#if notyet
void
tsb_enter(ctx, va, data)
	int ctx;
	int64_t va;
	int64_t data;
{
	int i, s;
	int64_t pa;

	i = ptelookup_va(va);
	s = splvm();
	pa = tsb[i].data&TLB_PA_MASK;
	/* 
	 * If we use fast DMMU access fault handlers to track
	 * referenced and modified bits, we should save the 
	 * TSB entry's state here.  Since we don't, we don't.
	 */
	/* Do not use global entries */
	tsb[i].tag = TSB_TAG(0,ctx,va);
	tsb[i].data = data;
	tlb_flush_pte(va, ctx);	/* Force reload -- protections may be changed */
	splx(s);
}
#endif

d2236 1
a2236 2
pmap_clear_modify(pg)
	struct vm_page *pg;
d2240 1
a2240 3
#ifdef DEBUG
	int modified = 0;
#endif
a2241 1
	register pv_entry_t pv;
a2242 8
#ifdef DEBUG
	if (pmapdebug & (PDB_CHANGEPROT|PDB_REF))
		printf("pmap_clear_modify(%llx)\n", (unsigned long long)pa);
#endif

#if defined(DEBUG)
	modified = pmap_is_modified(pg);
#endif
a2245 4
#ifdef DEBUG
	if (pv->pv_va & PV_MOD)
		pv->pv_va |= PV_WE;	/* Remember this was modified */
#endif
d2249 1
a2249 7
#ifdef DEBUG	
	if (pv->pv_next && !pv->pv_pmap) {
		printf("pmap_clear_modify: npv but no pmap for pv %p\n", pv);
		Debugger();
	}
#endif
	if (pv->pv_pmap != NULL)
d2254 2
a2255 1
			data = pseg_get(pv->pv_pmap, pv->pv_va&PV_VAMASK);
d2267 1
a2267 1
			if (pseg_set(pv->pv_pmap, pv->pv_va&PV_VAMASK, data, 0)) {
d2284 1
d2286 1
a2286 14
	pv_check();
#ifdef DEBUG
	if (pmap_is_modified(pg)) {
		printf("pmap_clear_modify(): %p still modified!\n", pg);
		Debugger();
	}
	if (pmapdebug & (PDB_CHANGEPROT|PDB_REF))
		printf("pmap_clear_modify: page %lx %s\n", (long)pa, 
		       (changed?"was modified":"was not modified"));
	if (modified != changed) {
		printf("pmap_clear_modify: modified %d changed %d\n", modified, changed);
		Debugger();
	} else return (modified);
#endif
d2293 1
d2295 1
a2295 3
#ifdef DEBUG
	int referenced = 0;
#endif
a2296 1
	register pv_entry_t pv;
a2297 5
#ifdef DEBUG
	if (pmapdebug & (PDB_CHANGEPROT|PDB_REF))
		printf("pmap_clear_reference(%llx)\n", VM_PAGE_TO_PHYS(pg));
	referenced = pmap_is_referenced(pg);
#endif
d2300 1
a2300 6
	pv = &pg->mdpage.pvent;
#ifdef NOT_DEBUG
	if (pv->pv_va & PV_MOD)
		printf("pmap_clear_reference(): pv %p still modified\n",
		    VM_PAGE_TO_PHYS(pg));
#endif
d2302 1
a2302 1
		changed |= 1;
a2303 6
#ifdef DEBUG	
	if (pv->pv_next && !pv->pv_pmap) {
		printf("pmap_clear_reference: npv but no pmap for pv %p\n", pv);
		Debugger();
	}
#endif
d2308 1
a2308 6
			data = pseg_get(pv->pv_pmap, pv->pv_va&PV_VAMASK);
#ifdef DEBUG
			if (pmapdebug & PDB_CHANGEPROT)
				printf("clearing ref pm:%p va:%p ctx:%lx data:%x:%x\n", pv->pv_pmap,
				       (void *)(u_long)pv->pv_va, (u_long)pv->pv_pmap->pm_ctx, (int)(data>>32), (int)data);
#endif
d2311 1
a2311 1
					changed |= 1;
d2315 1
a2315 1
					changed |= 1;
d2319 1
a2319 1
			if (pseg_set(pv->pv_pmap, pv->pv_va, data, 0)) {
d2324 1
a2324 2
			if (pv->pv_pmap->pm_ctx || 
			    pv->pv_pmap == pmap_kernel()) {
d2328 1
a2328 1
				tlb_flush_pte(pv->pv_va&PV_VAMASK, 
d2333 1
a2333 1
				changed |= 1;
d2340 1
a2340 15
	pv_check();
#ifdef DEBUG
	if (pmap_is_referenced(pg)) {
		printf("pmap_clear_reference(): %p still referenced!\n", pg);
		Debugger();
	}
	if (pmapdebug & (PDB_CHANGEPROT|PDB_REF))
		printf("pmap_clear_reference: page %lx %s\n",
		    VM_PAGE_TO_PHYS(pg),
		    (changed?"was referenced":"was not referenced"));
	if (referenced != changed) {
		printf("pmap_clear_reference: referenced %d changed %d\n", referenced, changed);
		Debugger();
	} else return (referenced);
#endif
d2348 2
a2349 1
	int i=0, s;
d2354 4
a2357 9
	if (pv->pv_va&PV_MOD) i = 1;
#ifdef DEBUG	
	if (pv->pv_next && !pv->pv_pmap) {
		printf("pmap_is_modified: npv but no pmap for pv %p\n", pv);
		Debugger();
	}
#endif
	if (!i && (pv->pv_pmap != NULL))
		for (npv = pv; i == 0 && npv && npv->pv_pmap; npv = npv->pv_next) {
d2360 3
a2362 2
			data = pseg_get(npv->pv_pmap, npv->pv_va&PV_VAMASK);
			if (pmap_tte2flags(data) & PV_MOD) i = 1;
d2364 2
a2365 1
			if (npv->pv_va & PV_MOD) i = 1;
d2368 1
d2370 2
a2371 4
	if (i) pv->pv_va |= PV_MOD;
#ifdef DEBUG
	if (i) pv->pv_va |= PV_WE;
#endif
d2374 1
a2374 9
#ifdef DEBUG
	if (pmapdebug & (PDB_CHANGEPROT|PDB_REF)) {
		printf("pmap_is_modified(%llx) = %d\n",
		    (unsigned long long)VM_PAGE_TO_PHYS(pg), i);
		/* if (i) Debugger(); */
	}
#endif
	pv_check();
	return (i);
d2378 1
a2378 1
pmap_is_referenced(struct vm_page* pg)
a2379 1
	int i=0, s;
d2381 2
d2387 3
a2389 8
	if (pv->pv_va&PV_REF) i = 1;
#ifdef DEBUG	
	if (pv->pv_next && !pv->pv_pmap) {
		printf("pmap_is_referenced: npv but no pmap for pv %p\n", pv);
		Debugger();
	}
#endif
	if (!i && (pv->pv_pmap != NULL))
d2393 3
a2395 2
			data = pseg_get(npv->pv_pmap, npv->pv_va&PV_VAMASK);
			if (pmap_tte2flags(data) & PV_REF) i = 1;
d2397 2
a2398 1
			if (npv->pv_va & PV_REF) i = 1;
d2401 1
d2403 2
a2404 1
	if (i) pv->pv_va |= PV_REF;
d2407 1
a2407 9
#ifdef DEBUG
	if (pmapdebug & (PDB_CHANGEPROT|PDB_REF)) {
		printf("pmap_is_referenced(%llx) = %d\n",
		    (unsigned long long)VM_PAGE_TO_PHYS(pg), i);
		/* if (i) Debugger(); */
	}
#endif
	pv_check();
	return i;
a2409 2


d2418 1
a2418 3
pmap_unwire(pmap, va)
	register pmap_t	pmap;
	vaddr_t va;
d2423 1
a2423 6
#ifdef DEBUG
	if (pmapdebug & (PDB_MMU_STEAL)) /* XXXX Need another flag for this */
		printf("pmap_unwire(%p, %lx)\n", pmap, va);
#endif
	if (pmap == NULL) {
		pv_check();
a2424 1
	}
a2448 1
	pv_check();
a2455 1

d2457 1
a2457 3
pmap_page_protect(pg, prot)
	struct vm_page* pg;
	vm_prot_t prot;
d2460 3
a2462 4
	register pv_entry_t pv;
	register int s;
	long long clear, set;
	int64_t data = 0LL;
d2464 1
a2464 8
#ifdef DEBUG
	if (pmapdebug & PDB_CHANGEPROT)
		printf("pmap_page_protect: pa %llx prot %x\n",
			(unsigned long long)pa, prot);
#endif

	if (prot & PROT_WRITE) {
		pv_check();
a2465 1
	}
a2488 6
#ifdef DEBUG	
		if (pv->pv_next && !pv->pv_pmap) {
			printf("pmap_page_protect: npv but no pmap for pv %p\n", pv);
			Debugger();
		}
#endif
d2491 1
a2491 15
#ifdef DEBUG
				if (pmapdebug & (PDB_CHANGEPROT|PDB_REF)) {
					printf("pmap_page_protect: RO va %p of pa %p...\n",
					    (void *)(u_long)pv->pv_va, (void *)(u_long)pa);
				}
#if 0
				if (!pv->pv_pmap->pm_segs[va_to_seg(pv->pv_va&PV_VAMASK)]) {
					printf("pmap_page_protect(%x:%x,%x): pv %x va %x not in pmap %x\n",
					       (int)(pa>>32), (int)pa, prot, pv, pv->pv_va, pv->pv_pmap);
					Debugger();
					continue;
				}
#endif
#endif
				data = pseg_get(pv->pv_pmap, pv->pv_va&PV_VAMASK);
d2499 1
a2499 1
				if (pseg_set(pv->pv_pmap, pv->pv_va&PV_VAMASK, data, 0)) {
d2507 1
a2507 1
					tlb_flush_pte(pv->pv_va&PV_VAMASK, pv->pv_pmap->pm_ctx);
d2515 1
a2515 6
		
#ifdef DEBUG
		if (pmapdebug & PDB_REMOVE) 
			printf("pmap_page_protect: demapping pa %lx\n", (long)pa);
#endif
			       
d2522 1
a2522 16
#ifdef DEBUG
			if (pmapdebug & (PDB_CHANGEPROT|PDB_REF|PDB_REMOVE)) {
				printf("pmap_page_protect: demap va %p of pa %p in pmap %p...\n",
				       (void *)(u_long)npv->pv_va, (void *)(u_long)pa, npv->pv_pmap);
			}
#if 0
			if (!npv->pv_pmap->pm_segs[va_to_seg(npv->pv_va&PV_VAMASK)]) {
				printf("pmap_page_protect(%x:%x,%x): pv %x va %x not in pmap %x\n",
				       (int)(pa>>32), (int)pa, prot, npv, npv->pv_va, npv->pv_pmap);
				Debugger();
				continue;
			}
#endif
#endif
			/* clear the entry in the page table */
			data = pseg_get(npv->pv_pmap, npv->pv_va&PV_VAMASK);
d2526 1
a2526 9
			if (data & TLB_TSB_LOCK) {
#ifdef DIAGNOSTIC
				printf("pmap_page_protect: wired page pm %p va %p not removed\n",
				       npv->pv_pmap, (void *)(u_long)npv->pv_va);
				printf("vm wire count %d\n", pg->wire_count);
				pv = npv;
				continue;
#endif			
			}
d2528 1
a2528 1
			if (pseg_set(npv->pv_pmap, npv->pv_va&PV_VAMASK, 0, 0)) {
d2536 1
a2536 1
				tlb_flush_pte(npv->pv_va&PV_VAMASK, npv->pv_pmap->pm_ctx);
a2547 6
#ifdef DEBUG	
		if (pv->pv_next && !pv->pv_pmap) {
			printf("pmap_page_protect: npv but no pmap for pv %p\n", pv);
			Debugger();
		}
#endif
d2549 2
a2550 7
#ifdef DEBUG
			if (pmapdebug & (PDB_CHANGEPROT|PDB_REF|PDB_REMOVE)) {
				printf("pmap_page_protect: demap va %p of pa %lx from pm %p...\n",
				       (void *)(u_long)pv->pv_va, (long)pa, pv->pv_pmap);
			}
#endif
			data = pseg_get(pv->pv_pmap, pv->pv_va&PV_VAMASK);
d2569 1
a2569 1
				pv->pv_va |= (npv->pv_va&PV_MASK);
a2581 1
	pv_check();
d2594 1
a2594 2
ctx_alloc(pm)
	struct pmap* pm;
d2596 1
a2596 1
	register int s, cnum;
a2617 5
#ifdef DEBUG
		/* We should identify this pmap and clear it */
		printf("Warning: stealing context %d\n", cnum);
		remove_stats.pidflushes ++;
#endif
a2630 4
#ifdef DEBUG
	if (pmapdebug & PDB_CTX_ALLOC)
		printf("ctx_alloc: allocated ctx %d\n", cnum);
#endif
d2638 1
a2638 2
ctx_free(pm)
	struct pmap* pm;
a2657 6
#ifdef DEBUG
	if (pmapdebug & PDB_CTX_ALLOC) {
		printf("ctx_free: freeing ctx %d\n", oldctx);
		Debugger();
	}
#endif
d2666 1
a2666 4
pmap_enter_pv(pmap, va, pa)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
a2672 5
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter: pv %p: was %lx/%p/%p\n",
		       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
a2676 6
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("pmap_enter: first pv: pmap %p va %lx\n",
				pmap, va);
		enter_stats.firstpv++;
#endif
d2692 1
a2692 1
			if ((pv->pv_va^va)&VA_ALIAS_MASK) {
a2694 3
#ifdef DEBUG
				enter_stats.ci++;
#endif
d2706 1
a2706 11
#ifdef PARANOIADIAG
				int64_t data;

				data = pseg_get(pm, va);
				if (data >= 0 ||
				    data&TLB_PA_MASK != pa&TLB_PA_MASK)
					printf(
		"pmap_enter: found va %lx pa %lx in pv_table but != %lx\n",
						va, pa, (long)data);
#endif
				goto fnd;
a2708 5
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("pmap_enter: new pv: pmap %p va %lx\n",
				pmap, va);
#endif
d2719 1
a2719 5
#ifdef DEBUG
		if (!npv->pv_next)
			enter_stats.secondpv++;
#endif
	fnd:
a2727 1

d2729 1
a2729 4
pmap_remove_pv(pmap, va, pa)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
d2731 1
a2731 1
	register pv_entry_t pv, npv, opv;
a2734 5
#ifdef DEBUG
	if (pmapdebug & (PDB_REMOVE))
		printf("pmap_remove_pv(pm=%p, va=%p, pa=%llx)\n", pmap,
		    (void *)(u_long)va, (unsigned long long)pa);
#endif
a2738 1
	pv_check();
d2741 1
d2748 1
a2748 1
	if (pmap == pv->pv_pmap && PV_MATCH(pv,va)) {
d2750 1
a2750 1
		data = pseg_get(pv->pv_pmap, pv->pv_va&PV_VAMASK);
d2754 1
a2754 1
			pv->pv_va = (pv->pv_va&PV_MASK) | npv->pv_va;
a2762 3
#ifdef DEBUG
		remove_stats.pvfirst++;
#endif
d2765 2
a2766 5
#ifdef DEBUG
			remove_stats.pvsearch++;
#endif
			if (pmap == npv->pv_pmap && PV_MATCH(npv,va))
				goto fnd;
d2775 1
a2775 8
#ifdef DIAGNOSTIC
		printf("pmap_remove_pv(%lx, %x, %x) not found\n", (u_long)pmap, (u_int)va, (u_int)pa);
		
		Debugger();
		splx(s);
		return;
#endif
	fnd:
d2777 2
a2778 1
		/* 
d2781 2
a2782 1
		data = pseg_get(npv->pv_pmap, npv->pv_va&PV_VAMASK);
d2798 1
a2798 1
			if ((npv->pv_va^opv->pv_va)&VA_ALIAS_MASK) {
a2805 1
	pv_check();
d2814 1
a2814 4
pmap_page_cache(pm, pa, mode)
	struct pmap *pm;
	paddr_t pa;
	int mode;
a2821 5
#ifdef DEBUG
	if (pmapdebug & (PDB_ENTER))
		printf("pmap_page_uncache(%llx)\n", (unsigned long long)pa);
#endif

d2862 1
a2862 1
		
a2929 1
void db_dump_pv(db_expr_t, int, db_expr_t, char *);
d2931 1
a2931 5
db_dump_pv(addr, have_addr, count, modif)
	db_expr_t addr;
	int have_addr;
	db_expr_t count;
	char *modif;
a2946 200
#endif

#ifdef DEBUG
/*
 * Test ref/modify handling.
 */
void pmap_testout(void);
void
pmap_testout()
{
	struct vm_page *pg;
	vaddr_t va;
	volatile int *loc;
	int val = 0;
	paddr_t pa;
	int ref, mod;

	/* Allocate a page */
	va = (vaddr_t)(vmmap - NBPG);
	KDASSERT(va != 0);
	loc = (int *)va;

	pmap_get_page(&pa, NULL, pmap_kernel());
	pg = PHYS_TO_VM_PAGE(pa);
	pmap_enter(pmap_kernel(), va, pa, PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);

	/* Check it's properly cleared */
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Checking cleared page: ref %d, mod %d\n",
	       ref, mod);

	/* Reference page */
	val = *loc;

	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Referenced page: ref %d, mod %d val %x\n",
	       ref, mod, val);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);
	
	/* Modify page */
	*loc = 1;

	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Modified page: ref %d, mod %d\n",
	       ref, mod);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);

	/* Check it's properly cleared */
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Checking cleared page: ref %d, mod %d\n",
	       ref, mod);

	/* Modify page */
	*loc = 1;

	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Modified page: ref %d, mod %d\n",
	       ref, mod);

	/* Check pmap_protect() */
	pmap_protect(pmap_kernel(), va, va+1, PROT_READ);
	pmap_update(pmap_kernel());
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("pmap_protect(PROT_READ): ref %d, mod %d\n",
	       ref, mod);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);

	/* Modify page */
	pmap_enter(pmap_kernel(), va, pa, PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());
	*loc = 1;

	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Modified page: ref %d, mod %d\n",
	       ref, mod);

	/* Check pmap_protect() */
	pmap_protect(pmap_kernel(), va, va+1, PROT_NONE);
	pmap_update(pmap_kernel());
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("pmap_protect(PROT_READ): ref %d, mod %d\n",
	       ref, mod);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);

	/* Modify page */
	pmap_enter(pmap_kernel(), va, pa, PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());
	*loc = 1;

	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Modified page: ref %d, mod %d\n",
	       ref, mod);

	/* Check pmap_pag_protect() */
	pmap_page_protect(pg, PROT_READ);
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("pmap_protect(): ref %d, mod %d\n",
	       ref, mod);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);


	/* Modify page */
	pmap_enter(pmap_kernel(), va, pa, PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());
	*loc = 1;

	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Modified page: ref %d, mod %d\n",
	       ref, mod);

	/* Check pmap_pag_protect() */
	pmap_page_protect(pg, PROT_NONE);
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("pmap_protect(): ref %d, mod %d\n",
	       ref, mod);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa,
	       ref, mod);

	/* Unmap page */
	pmap_remove(pmap_kernel(), va, va+1);
	pmap_update(pmap_kernel());
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Unmapped page: ref %d, mod %d\n", ref, mod);

	/* Now clear reference and modify */
	ref = pmap_clear_reference(pg);
	mod = pmap_clear_modify(pg);
	printf("Clearing page va %p pa %lx: ref %d, mod %d\n",
	       (void *)(u_long)va, (long)pa, ref, mod);

	/* Check it's properly cleared */
	ref = pmap_is_referenced(pg);
	mod = pmap_is_modified(pg);
	printf("Checking cleared page: ref %d, mod %d\n",
	       ref, mod);

	pmap_remove(pmap_kernel(), va, va+1);
	pmap_update(pmap_kernel());
	pmap_free_page(pa, pmap_kernel());
}
@


1.85
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2014/12/15 02:24:23 guenther Exp $	*/
a36 1
#include <sys/lock.h>
a1740 1
	simple_lock(&pm->pm_lock);
a1800 1
	simple_unlock(&pm->pm_lock);
a1846 1
	simple_lock(&pm->pm_lock);
a1872 1
	simple_unlock(&pm->pm_lock);
a2021 1
	simple_lock(&pm->pm_lock);
a2070 1
	simple_unlock(&pm->pm_lock);
a2110 1
	simple_lock(&pm->pm_lock);
a2112 1
		simple_unlock(&pm->pm_lock);
a2113 1
		simple_lock(&pm->pm_lock);
a2177 1
			simple_unlock(&pm->pm_lock);
a2185 1
	simple_unlock(&pm->pm_lock);
a2221 1
	simple_lock(&pm->pm_lock);
a2275 1
	simple_unlock(&pm->pm_lock);
a2316 1
	simple_lock(&pm->pm_lock);
a2325 1
			simple_unlock(&pm->pm_lock);
a2372 1
	simple_unlock(&pm->pm_lock);
a2418 1
		simple_lock(&pm->pm_lock);
a2437 1
		simple_unlock(&pm->pm_lock);
a2694 2

			simple_lock(&pv->pv_pmap->pm_lock);
a2722 1
			simple_unlock(&pv->pv_pmap->pm_lock);
a2778 1
			simple_lock(&pv->pv_pmap->pm_lock);
a2811 1
			simple_unlock(&pv->pv_pmap->pm_lock);
a2955 1
	simple_lock(&pmap->pm_lock);
a2967 1
	simple_unlock(&pmap->pm_lock);
a3029 1
				simple_lock(&pv->pv_pmap->pm_lock);
a3061 1
				simple_unlock(&pv->pv_pmap->pm_lock);
a3079 1
			simple_lock(&npv->pv_pmap->pm_lock);
a3119 1
			simple_unlock(&npv->pv_pmap->pm_lock);
a3135 1
			simple_lock(&pv->pv_pmap->pm_lock);
a3156 1
			simple_unlock(&pv->pv_pmap->pm_lock);
a3273 2
 *
 * We enter here with the pmap locked.
a3352 5
		 *
		 * We need to drop the lock on the kernel_pmap
		 * to do memory allocation.  But that should not
		 * cause any real problems unless someone tries to
		 * touch the particular mapping we're adding.
a3505 2
		if (pv->pv_pmap != pm)
			simple_lock(&pv->pv_pmap->pm_lock);
a3531 2
		if (pv->pv_pmap != pm)
			simple_unlock(&pv->pv_pmap->pm_lock);
@


1.84
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2014/11/16 12:30:59 deraadt Exp $	*/
d3642 1
a3642 2
	    UVM_MAPFLAG(PROT_NONE, PROT_NONE, MAP_INHERIT_SHARE,
	      POSIX_MADV_RANDOM,
@


1.83
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2014/07/08 17:19:25 deraadt Exp $	*/
d3642 1
a3642 1
	    UVM_MAPFLAG(PROT_NONE, PROT_NONE, UVM_INH_SHARE,
@


1.82
log
@decouple struct uvmexp into a new file, so that uvm_extern.h and sysctl.h
don't need to be married.
ok guenther miod beck jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2014/01/30 00:51:13 dlg Exp $	*/
d1974 1
a1974 1
		    (VM_PROT_WRITE & prot), 1, 0, 1, 0);
d1978 1
a1978 1
		if (prot & VM_PROT_WRITE)
d1980 1
a1980 1
		if (prot & VM_PROT_EXECUTE)
d1985 1
a1985 1
		    (VM_PROT_WRITE & prot), 1, 0, 1, 0);
d1989 1
a1989 1
		if (prot & VM_PROT_WRITE)
d1991 1
a1991 1
		if (prot & VM_PROT_EXECUTE)
d2134 1
a2134 1
		if ((flags & VM_PROT_ALL) & ~prot)
d2138 1
a2138 1
		if (flags & VM_PROT_ALL)
d2140 1
a2140 1
		if (flags & VM_PROT_WRITE)
d2161 1
a2161 1
		    (flags & VM_PROT_WRITE), (!(pa & PMAP_NC)), 
d2163 1
a2163 1
		if (prot & VM_PROT_WRITE)
d2165 1
a2165 1
		if (prot & VM_PROT_EXECUTE)
d2171 1
a2171 1
		    (flags & VM_PROT_WRITE), (!(pa & PMAP_NC)), 
d2173 1
a2173 1
		if (prot & VM_PROT_WRITE)
d2175 1
a2175 1
		if (prot & VM_PROT_EXECUTE)
d2321 2
a2322 2
	if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) ==
	    (VM_PROT_WRITE|VM_PROT_EXECUTE))
d2325 1
a2325 1
	if (prot == VM_PROT_NONE) {
d2365 1
a2365 1
				if ((prot & VM_PROT_WRITE) == 0)
d2367 1
a2367 1
				if ((prot & VM_PROT_EXECUTE) == 0)
d2370 1
a2370 1
				if ((prot & VM_PROT_WRITE) == 0)
d2372 1
a2372 1
				if ((prot & VM_PROT_EXECUTE) == 0)
d3021 1
a3021 1
	if (prot & VM_PROT_WRITE) {
d3026 1
a3026 1
	if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
d3032 1
a3032 1
			if (VM_PROT_EXECUTE & prot)
d3038 1
a3038 1
			if (VM_PROT_EXECUTE & prot)
d3042 1
a3042 1
			if (VM_PROT_EXECUTE == prot)
d3642 2
a3643 2
	    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_SHARE,
	      UVM_ADV_RANDOM,
d3695 2
a3696 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3757 1
a3757 1
	pmap_protect(pmap_kernel(), va, va+1, VM_PROT_READ);
d3761 1
a3761 1
	printf("pmap_protect(VM_PROT_READ): ref %d, mod %d\n",
d3772 2
a3773 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3783 1
a3783 1
	pmap_protect(pmap_kernel(), va, va+1, VM_PROT_NONE);
d3787 1
a3787 1
	printf("pmap_protect(VM_PROT_READ): ref %d, mod %d\n",
d3798 2
a3799 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3809 1
a3809 1
	pmap_page_protect(pg, VM_PROT_READ);
d3824 2
a3825 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3835 1
a3835 1
	pmap_page_protect(pg, VM_PROT_NONE);
@


1.81
log
@move sparc64 behind the MI atomic api.

this basically replaces sparc64_cas and sparc64_casx with atomic_cas_uint
and atomic_cas_ulong respectively. it then builds atomic_add and
atomic_sub out of those. this avoids the gcc atomic builtins that
the MI atomic_foo api uses by default, so we dont get the extra
membars that the builtins do but the atomic_foo api doesnt promise.

it also fixes up the code that used to use sparc64_{cas,casx} to
use the atomic_cas api instead.

use of the sparc64 membar() macros are left untouched for now.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2013/09/21 10:04:42 miod Exp $	*/
d34 1
a35 1
#include <sys/systm.h>
@


1.80
log
@Create process map holes with UVM_INH_SHARE so that they don't get lost in
fork-without-exec situation (such as privsep'd binaries).
Fixes occasional SIGSEGV in syslogd and pflogd on sun4/4c/4e.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2013/06/11 16:42:12 deraadt Exp $	*/
d30 1
d307 1
a307 1
		sparc64_casx((volatile unsigned long *)&tsb_dmmu[i].tag,
d310 1
a310 1
		sparc64_casx((volatile unsigned long *)&tsb_immu[i].tag,
@


1.79
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2012/08/29 20:33:16 kettenis Exp $	*/
d3641 1
a3641 1
	    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
@


1.78
log
@The low-level guts to support MTP (Multi-Threaded Processing) on the
Fujitsu SPARC64-VI and SPARC64-VII CPUs.  Since the two threads on each core
share the TLBs of the core we cannot enter different mappings for the same
virtual address.  Instead we use a scratch register to store the per-cpu
pointer.  This is very similar to what we do on sun4v.

For now we still only attach the first thread of each SPARC64-VI/VII core
since we currently don't handle the VMT (Vertical Multi-Threading) of the
SPARC64-VI very well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2012/08/29 19:54:32 kettenis Exp $	*/
d2493 2
a2494 2
	register daddr64_t blkno;
	register int (*dump)(dev_t, daddr64_t, caddr_t, size_t);
@


1.77
log
@Make sure the interrupt stack for the boot processor is properly aligned to
a 64K boundary.  This means we don't have to worry about virtual cache aliasing
anymore since SPARC V9 CPUs have at most a 16K aliasing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2012/04/26 21:32:20 okan Exp $	*/
d1472 3
d1498 26
@


1.76
log
@va is of type vaddr_t; allow sparc64 to build with DEBUG

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2011/09/22 17:41:00 jasper Exp $	*/
d1385 1
a1385 3
		if ((vmmap ^ INTSTACK) & VA_ALIAS_MASK) 
			vmmap += NBPG; /* Matchup virtual color for D$ */
		intstk = vmmap;
@


1.75
log
@nowadays uvm_init() calls pmap_init(), not vm_init(); so update the comments.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2011/05/30 22:25:22 oga Exp $	*/
d2080 1
a2080 1
	if (va == NULL) {
d3662 1
a3662 1
	KDASSERT(va != NULL);
@


1.74
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2011/04/26 23:50:21 ariane Exp $	*/
d1567 1
a1567 1
 * Called during vm_init().
@


1.73
log
@MMU address space holes are at a fixed position (ofcourse).
Therefore set UVM_FLAG_FIXED and enforce this.

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2011/04/07 15:30:16 miod Exp $	*/
d1238 1
a1238 2
			atop(mp->start+mp->size),
			VM_FREELIST_DEFAULT);
d1248 1
a1248 2
			atop(roundup(ekdatap, (4*MEG))),
			VM_FREELIST_DEFAULT);
@


1.72
log
@Do not use NULL in integer comparisons. No functional change.
ok matthew@@ tedu@@, also eyeballed by at least krw@@ oga@@ kettenis@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2010/11/20 20:33:24 miod Exp $	*/
d3617 2
a3618 1
	      UVM_ADV_RANDOM, UVM_FLAG_NOMERGE | UVM_FLAG_HOLE));
@


1.71
log
@This is a first step towards getting rid of avail_start and avail_end in the
kernel, currently limited to low-hanging fruit: these variables were used
by bus_dma to specify the range in which to allocate memory, back when
uvm_pglistalloc() was stupid and would not walk the vm_physseg[].

Nowadays, except on some platforms for early initialization, these variables
are not used, or do not need to be global variables. Therefore:
- remove `extern' declarations of avail_start and avail_end (or close cousins,
  such as arm physical_start and physical_end) from files which no longer need
  to use them.
- make them local variables whenever possible.
- remove them when they are assigned to but no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2010/04/20 23:27:00 deraadt Exp $	*/
d464 1
a464 1
	newp = NULL;
d466 1
a466 1
		newp = NULL;
d1751 1
a1751 1
					stxa(pdirentp, ASI_PHYS_CACHED, NULL);
d1755 1
a1755 1
			stxa(psegentp, ASI_PHYS_CACHED, NULL);
d1839 1
a1839 1
						stxa((paddr_t)(u_long)&pdir[k], ASI_PHYS_CACHED, NULL);
d1846 1
a1846 1
				stxa((paddr_t)(u_long)&pm->pm_segs[i], ASI_PHYS_CACHED, NULL);
d1893 1
a1893 1
		if (pmap->pm_ctx == NULL)
d2156 1
a2156 1
	pg = NULL;
d2158 1
a2158 1
		pg = NULL;
d3214 1
a3214 1
	} while (ctxbusy[++cnum] != NULL && cnum != next);
d3274 1
a3274 1
	ctxbusy[oldctx] = NULL;
@


1.70
log
@cleanup more confusion regarding user.h before proc.h, or missing proc.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2010/04/15 21:14:18 kettenis Exp $	*/
a208 5
/*
 * First and last managed physical addresses.  XXX only used for dumping the system.
 */
paddr_t	vm_first_phys, vm_num_phys;

a294 2
paddr_t avail_start, avail_end;

a1449 6
	/*
	 * Set up bounds of allocatable memory for vmstat et al.
	 */
	avail_start = avail->start;
	for (mp = avail; mp->size; mp++)
		avail_end = mp->start+mp->size;
a1582 3

	vm_first_phys = avail_start;
	vm_num_phys = avail_end - avail_start;
@


1.69
log
@Add some missing splx()'s (and missing simple_unlock() no-ops) in pmap error
paths.  Spotted by Mike Belopuhov.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2009/02/12 18:53:14 miod Exp $	*/
d33 1
@


1.68
log
@Add a pm_statistics struct to all pmap, and keep track of resident
pages. Use this to provide a real pmap_resident_count() function.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2008/07/27 20:33:23 kettenis Exp $	*/
d2177 2
d2330 2
d3462 1
@


1.67
log
@Increase the size of the TSB on large memory machines.  This make building
a kernel 10-15% faster on the t1k.  Don't do this for SMALL_KERNEL though,
otherwise the 4MB locked mapping for the data segment will be too small to
include a ramdisk.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2008/07/25 19:37:16 kettenis Exp $	*/
d477 1
d1990 2
d2049 1
d2183 1
d2240 1
a2240 1
		if ((data = pseg_get(pm, va))) {
d2259 1
d2263 1
a2263 1
			remove_stats.removes ++;
d3133 1
d3172 1
a3191 42
}

/*
 * count pages in pmap -- this can be slow.
 */
int
pmap_count_res(pm)
	pmap_t pm;
{
	int i, j, k, n, s;
	paddr_t *pdir, *ptbl;
	/* Almost the same as pmap_collect() */

	/*
	 * XXX On the SPARC Enterprise T5120, counting the number of
	 * pages in the kernel pmap is ridiculously slow.  Since ps(1)
	 * doesn't use the information for P_SYSTEM processes, we may
	 * as well skip the counting and return zero immediately.
	 */
	if (pm == pmap_kernel())
		return 0;

	/* Don't want one of these pages reused while we're reading it. */
	s = splvm();
	simple_lock(&pm->pm_lock);
	n = 0;
	for (i=0; i<STSZ; i++) {
		if((pdir = (paddr_t *)(u_long)ldxa((vaddr_t)&pm->pm_segs[i], ASI_PHYS_CACHED))) {
			for (k=0; k<PDSZ; k++) {
				if ((ptbl = (paddr_t *)(u_long)ldxa((vaddr_t)&pdir[k], ASI_PHYS_CACHED))) {
					for (j=0; j<PTSZ; j++) {
						int64_t data = (int64_t)ldxa((vaddr_t)&ptbl[j], ASI_PHYS_CACHED);
						if (data&TLB_V)
							n++;
					}
				}
			}
		}
	}
	simple_unlock(&pm->pm_lock);
	splx(s);
	return n;
@


1.66
log
@Work around a problem on the t5120 where top(1) would lock up the system for
several minutes by making pmap_resident_count() return 0 for the kernel pmap.

discussed with miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2008/07/05 21:20:48 kettenis Exp $	*/
d848 1
d850 1
a850 1
	 * Calculate approx TSB size.  This probably needs tweaking.
d852 7
a858 6
	if (physmem < atop(64 * 1024 * 1024))
		tsbsize = 0;
	else if (physmem < atop(512 * 1024 * 1024))
		tsbsize = 1;
	else
		tsbsize = 2;
@


1.65
log
@Calculate the size of kernel text correctly if it spans multiple prom mapping
entries.  This is necessary on the v1280 where the firmware mixes 4MB and 8KB
mappings to map kernel text.

tested by miod@@ and nick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2008/07/04 17:40:25 kettenis Exp $	*/
d3194 9
@


1.64
log
@Make debug code compile.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2008/06/14 10:55:20 mk Exp $	*/
d885 1
d887 3
a889 3
		if (prom_map[i].vstart == ktext)
			break;
	if (i == prom_map_size) 
a890 1
	ktsize = prom_map[i].vsize;
@


1.63
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2008/06/09 20:32:56 miod Exp $	*/
d2330 1
a2330 1
			if (pmapdebug & (PDB_CHANGEPROT|PDB_REF))
@


1.62
log
@Sparc64 MMUs have an address hole, too, and the pmap implementation we
are using has an even larger one, so implement pmap_remove_hole() to
prevent mmap() from ever reaching the hole.

feedback and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2008/06/09 16:55:20 kettenis Exp $	*/
d1673 1
a1673 2
	pm = pool_get(&pmap_pool, PR_WAITOK);
	bzero((caddr_t)pm, sizeof *pm);
@


1.61
log
@Don't use u_int to store a 64-bit address.  Found with help from miod@@.
Makes a Sun Fire V490 boot without spending ages in bus_dmamem_alloc(9).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2008/05/21 19:23:15 kettenis Exp $	*/
d3627 25
@


1.60
log
@First step towards supporting sun4us machines with Fujitsu's SPARC64-V CPU.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2008/04/03 23:10:25 kettenis Exp $	*/
a289 1
static u_int nextavail;
d1456 1
a1456 2
	nextavail = avail->start;
	avail_start = nextavail;
@


1.59
log
@Use atomic operations to invalidate TSB entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2008/04/02 20:23:22 kettenis Exp $	*/
d1321 1
a1321 1
	if (CPU_ISSUN4U)
d3538 1
a3538 1
	if (CPU_ISSUN4V)
@


1.58
log
@For some reason using the ASI_SCRATCHPAD register at offset 0x08 makes the
t1k freak out upon reboot/halt/powerdown.  Use the register at offset 0x10
instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2008/03/31 22:14:01 kettenis Exp $	*/
d313 2
a314 1
		tsb_dmmu[i].tag = TSB_TAG_INVALID;
d316 2
a317 1
		tsb_immu[i].tag = TSB_TAG_INVALID;
@


1.57
log
@Make MULTIPROCESSOR kernels work on sun4v.  Won't gracefully halt, powerdown
or reboot yet, but that will (hopefully) be fixed in the near future.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2008/03/30 12:30:01 kettenis Exp $	*/
d1562 1
a1562 1
	stxa(0, ASI_SCRATCHPAD, ldxa(pa, ASI_PHYS_CACHED));
d1565 1
a1565 1
	stxa(8, ASI_SCRATCHPAD, intstack + (CPUINFO_VA - INTSTACK));
@


1.56
log
@More sun4v support.  GENERIC and RAMDISK kernels will now boot on both
sun4u and sun4v.  GENERIC.MP won't work yet though.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2008/03/23 23:46:21 kettenis Exp $	*/
d1559 4
d1565 1
a1565 1
	stxa(0, ASI_SCRATCHPAD, intstack + (CPUINFO_VA - INTSTACK));
@


1.55
log
@Add code to initialize CPUs on sun4v.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2008/03/22 16:01:32 kettenis Exp $	*/
d224 10
d247 21
a267 1
#define TSB_TAG_INVALID (~0LL)
d1318 3
d1339 1
a1339 1
					(prom_map[i].tte + j)|TLB_EXEC|
d1753 1
a1753 6
							if (data & TLB_ACCESS)
								pv->pv_va |=
									PV_REF;
							if (data & (TLB_MODIFY))
								pv->pv_va |=
									PV_MOD;
d1904 4
a1907 1
		stxa(CTX_SECONDARY, ASI_DMMU, pmap->pm_ctx);
d1955 23
a1977 11
	tte.data = TSB_DATA(0, PGSZ_8K, pa, 1 /* Privileged */,
				 (VM_PROT_WRITE & prot),
				 1, 0, 1, 0);
	/*
	 * We don't track modification on kenter mappings.
	 */
	if (prot & VM_PROT_WRITE)
		tte.data |= TLB_REAL_W|TLB_W;
	if (prot & VM_PROT_EXECUTE)
		tte.data |= TLB_EXEC;
	tte.data |= TLB_TSB_LOCK;	/* wired */
a2026 4
			paddr_t entry;

			flush |= 1;
			entry = (data&TLB_PA_MASK);
d2047 1
d2118 1
a2118 1
		if ((flags & VM_PROT_ALL) || (tte.data & TLB_ACCESS))
d2120 1
a2120 1
		if ((flags & VM_PROT_WRITE) || (tte.data & (TLB_MODIFY)))
d2122 1
d2139 21
a2159 9
	tte.data = TSB_DATA(0, size, pa, pm == pmap_kernel(),
		(flags & VM_PROT_WRITE), (!(pa & PMAP_NC)), 
		aliased, 1, (pa & PMAP_LITTLE));
	if (prot & VM_PROT_WRITE)
		tte.data |= TLB_REAL_W;
	if (prot & VM_PROT_EXECUTE)
		tte.data |= TLB_EXEC;
	if (wired)
		tte.data |= TLB_TSB_LOCK;
d2240 1
a2240 3
				if (data & TLB_ACCESS) pv->pv_va |= PV_REF;
				if (data & (TLB_MODIFY))  pv->pv_va |= PV_MOD;

a2329 5
/* Catch this before the assertion */
			if (data & TLB_NFO) {
				printf("pmap_protect: pm=%p  NFO mapping va=%x data=%x:%x\n",
				       pm, (u_int)sva, (int)(data>>32), (int)data);
				Debugger();
d2335 1
a2335 4
				if (data & TLB_ACCESS)
					pv->pv_va |= PV_REF;
				if (data & (TLB_MODIFY))  
					pv->pv_va |= PV_MOD;
d2338 11
a2348 4
			if ((prot & VM_PROT_WRITE) == 0)
				data &= ~(TLB_W|TLB_REAL_W);
			if ((prot & VM_PROT_EXECUTE) == 0)
				data &= ~(TLB_EXEC);
d2398 8
d2693 9
a2701 3
			if (data & (TLB_MODIFY))
				changed |= 1;
			data &= ~(TLB_MODIFY|TLB_W);
d2782 9
a2790 3
			if (data & TLB_ACCESS)
				changed |= 1;
			data &= ~TLB_ACCESS;
d2854 1
a2854 1
			if (data & (TLB_MODIFY)) i = 1;
d2898 1
a2898 1
			if (data & TLB_ACCESS) i = 1;
d2957 4
a2960 1
	data &= ~TLB_TSB_LOCK;
d3004 15
a3018 7
		clear = TLB_REAL_W|TLB_W;
		if (VM_PROT_EXECUTE & prot)
			set |= TLB_EXEC;
		else
			clear |= TLB_EXEC;
		if (VM_PROT_EXECUTE == prot)
			set |= TLB_EXEC_ONLY;
d3048 1
a3048 3
				if (data & TLB_ACCESS) pv->pv_va |= PV_REF;
				if (data & (TLB_MODIFY))  
					pv->pv_va |= PV_MOD;
d3100 2
a3101 5
			/* Save ref/mod info */
			if (data & TLB_ACCESS) 
				firstpv->pv_va |= PV_REF;
			if (data & (TLB_MODIFY))
				firstpv->pv_va |= PV_MOD;
d3147 2
a3148 5
			/* Save ref/mod info */
			if (data & TLB_ACCESS) 
				pv->pv_va |= PV_REF;
			if (data & (TLB_MODIFY))
				pv->pv_va |= PV_MOD;
d3500 2
a3501 5
	/* Save ref/mod info */
	if (data & TLB_ACCESS) 
		opv->pv_va |= PV_REF;
	if (data & (TLB_MODIFY))
		opv->pv_va |= PV_MOD;
d3532 3
d3553 2
a3554 2
			if (pseg_set(pv->pv_pmap, va, 
				     pseg_get(pv->pv_pmap, va) & ~(TLB_CV|TLB_CP), 
d3562 2
a3563 2
			if (pseg_set(pv->pv_pmap, va, 
				     pseg_get(pv->pv_pmap, va) | TLB_CV, 0)) {
d3570 2
a3571 2
			if (pseg_set(pv->pv_pmap, va, 
				     pseg_get(pv->pv_pmap, va) & ~TLB_CV, 0)) {
@


1.54
log
@Split out the code that sets the TSB registers and call it from
pmap_bootstrap_cpu().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2008/03/19 23:16:19 kettenis Exp $	*/
d46 1
d239 4
d1052 13
d1406 3
d1431 12
d1446 1
a1446 1
pmap_bootstrap_cpu(paddr_t intstack)
d1462 2
a1463 2
		data = TSB_DATA(0, PGSZ_4M, pa, 1, 0, 1, FORCE_ALIAS, 1, 0);
		data |= TLB_L;
d1470 2
a1471 2
		data = TSB_DATA(0, PGSZ_4M, pa, 1, 1, 1, FORCE_ALIAS, 1, 0);
		data |= TLB_L;
d1480 2
a1481 2
	data = TSB_DATA(0, PGSZ_64K, intstack, 1, 1, 1, FORCE_ALIAS, 1, 0);
	data |= TLB_L;
d1485 52
@


1.53
log
@Establish per-cpu locked mappings for `struct cpuinfo' through the PROM as
well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2008/03/19 20:42:05 kettenis Exp $	*/
d1410 2
d1443 4
d1450 2
@


1.52
log
@Use PROM calls to enter locked kernel text and data mappings into the TLB.
Gets rid of a big chunk of nasty asm code and makes us boot on the e10k with
multi-systemboard domains.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2008/03/15 22:05:51 kettenis Exp $	*/
d201 1
a201 1
void	pmap_bootstrap_cpu(void);
d1407 1
a1407 1
	pmap_bootstrap_cpu();
d1411 1
a1411 1
pmap_bootstrap_cpu(void)
d1440 4
@


1.51
log
@Make GENERIC.MP work on the e10k.  The e10k is a bit funky since UPA only
supports 32 ports, and a machine with up to 64 CPUs obviously needs more.
So the machine has a special ASIC that does port translation, and because
of that we need to distinguish between port ID's and interrupt target ID's.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2008/01/10 20:37:14 marco Exp $	*/
d201 2
d1407 33
@


1.50
log
@Fix spello

ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2007/12/23 21:43:30 kettenis Exp $	*/
d1379 1
a1379 1
		cpus->ci_upaid = CPU_UPAID;
@


1.49
log
@In pmap_dumpmmu(), account for the fact that we use 4MB mappings for text and
data segments.  This makes the complete mappings available in kernel dumps.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2007/12/05 19:43:15 kettenis Exp $	*/
d447 1
a447 1
 * See checp bootargs to see if we need to enable bootdebug.
@


1.48
log
@Remove some 32-bit compatibility code in pseg_get().
Completely remove pseg_find() since it isn't used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2007/11/28 19:37:23 kettenis Exp $	*/
d2324 1
a2324 1
	kcpu->ktextsz = (u_int64_t)ektextp - ktextp;
d2329 1
a2329 1
	kcpu->kdatasz = (u_int64_t)ekdatap - kdatap;
@


1.47
log
@Don't complain about removing wired pages; this is expected to happen with
ftruncate(2) on mlock(2)'ed pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2007/10/27 22:20:16 martin Exp $	*/
a111 1
extern paddr_t pseg_find(struct pmap*, vaddr_t addr, paddr_t spare);
a114 1
static paddr_t pseg_find(struct pmap*, vaddr_t addr, paddr_t spare);
a145 21

static paddr_t pseg_find(struct pmap* pm, vaddr_t addr, paddr_t spare) {
	int i, j, k, s;
	paddr_t *pdir, *ptbl;

	if (!(pdir = (paddr_t *)ldda(&pm->pm_segs[va_to_seg(addr)],
	    ASI_PHYS_CACHED))) {
		if (!spare) return (1);
		stxa_sync(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
		pdir = spare;
		spare = NULL;
	}
	if (!(ptbl = (paddr_t *)ldda(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED))) {
		if (!spare) return (1);
		stxa_sync(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
		ptbl = spare;
		spare = NULL;
	}
	return (paddr_t)(&ptbl[va_to_pte(addr)]);
}

@


1.46
log
@get rid of btoc/ctob in favor of atop/ptoa
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2007/10/18 20:44:47 kettenis Exp $	*/
a2966 6
			if (data & TLB_TSB_LOCK) {
#ifdef DIAGNOSTIC
				printf("pmap_page_protect: Removing wired page pm %p va %p\n",
				       (void *)(u_long)pv->pv_pmap, (void *)(u_long)pv->pv_va);
#endif			
			}
@


1.45
log
@Don't try to send IPIs to CPUs that aren't running (yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2007/10/17 21:23:28 kettenis Exp $	*/
d830 1
a830 1
		physmem += btoc(mp->size);
d1186 1
a1186 1
		npgs += btoc(sz);
@


1.44
log
@Spin up secondary CPUs on MULTIPROCESSOR kernels.  Works on UltraSPARC-III
CPUs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2007/09/09 14:59:37 kettenis Exp $	*/
d1404 1
@


1.43
log
@Add a few missing bits to include/cpu.h required for MULTIPROCESSOR kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2007/06/06 17:15:13 deraadt Exp $	*/
d563 1
a563 1
pmap_bootstrap(kernelstart, kernelend, maxctx)
d565 1
a565 1
	u_int maxctx;
d996 1
a996 1
	if ((cpu0paddr = prom_alloc_phys(8*NBPG, 8*NBPG)) == 0 ) {
d1409 2
@


1.42
log
@now that all partition size/offsets are potentially 64-bit, change the
type of all variables to daddr64_t.  this includes the APIs for XXsize()
and XXdump(), all range checks inside bio drivers, internal variables
for disklabel handling, and even uvm's swap offsets.  re-read numerous
times by otto, miod, krw, thib to look for errors
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2007/05/20 15:11:27 miod Exp $	*/
d1398 1
d1403 1
a1403 1
		cpus->ci_number = cpus->ci_upaid; /* How do we figure this out? */
@


1.41
log
@Replace ASSERT with KDASSERT and get rid of its definition; while there,
remove some #if 0 dead code which won't get ressurrected anytime soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2007/05/02 18:46:07 kettenis Exp $	*/
d2297 2
a2298 2
	register daddr_t blkno;
	register int (*dump)(dev_t, daddr_t, caddr_t, size_t);
@


1.40
log
@Move sparc64 to __HAVE_CPUINFO.

ok miod@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2007/04/22 18:13:04 art Exp $	*/
a175 12
#ifdef DEBUG
#ifdef __STDC__
#define	ASSERT(x)	\
	if (!(x)) panic("%s at line %d: assertion failed", #x, __LINE__);
#else
#define	ASSERT(x)	\
	if (!(x)) panic("%s at line %d: assertion failed", "x", __LINE__);
#endif
#else
#define ASSERT(x)
#endif

a1739 35
#if 0
/*
 * The two following routines are now in locore.s so I can code them in assembly
 * They can bypass the MMU or use VIS bcopy extensions for speed.
 */
/*
 * Fill the given physical page with zeroes.
 */
void
pmap_zero_page(pa)
	paddr_t pa;
{
	/* 
	 * We don't need to worry about flushing caches
	 * since all our virtual caches are write-through.
	 * All we need to do is map the page in somewhere, bzero it,
	 * and unmap it.  However, we need to be sure we don't
	 * map it in anywhere near the kernel or we may lose, badly.
	 */
	bzero((caddr_t)pa, NBPG);
}

/*
 * Copy the given physical source page to its destination.
 *
 * I will code this in assembly RSN.
 */
void
pmap_copy_page(src, dst)
	paddr_t src, dst;
{
	bcopy((caddr_t)src, (caddr_t)dst, NBPG);
}
#endif

d1795 2
a1796 2
	ASSERT(va < INTSTACK || va > EINTSTACK);
	ASSERT(va < kdata || va > ekdata);
d1822 1
a1822 1
	ASSERT((tte.data & TLB_NFO) == 0);
a1838 9
#if 0
void
pmap_kremove(va, size)
	vaddr_t va;
	vsize_t size;
{
	return pmap_remove(pmap_kernel(), va, va+size);
}
#else
d1848 2
a1849 2
	ASSERT(va < INTSTACK || va > EINTSTACK);
	ASSERT(va < kdata || va > ekdata);
a1906 1
#endif
d1930 2
a1931 2
	ASSERT(pm != pmap_kernel() || va < INTSTACK || va > EINTSTACK);
	ASSERT(pm != pmap_kernel() || va < kdata || va > ekdata);
d1994 1
a1994 1
	ASSERT((tte.data & TLB_NFO) == 0);
d2041 2
a2042 2
	ASSERT(pm != pmap_kernel() || endva < INTSTACK || va > EINTSTACK);
	ASSERT(pm != pmap_kernel() || endva < kdata || va > ekdata);
d2130 2
a2131 2
	ASSERT(pm != pmap_kernel() || eva < INTSTACK || sva > EINTSTACK);
	ASSERT(pm != pmap_kernel() || eva < kdata || sva > ekdata);
d2186 1
a2186 1
			ASSERT((data & TLB_NFO) == 0);
d2525 1
a2525 1
			ASSERT((data & TLB_NFO) == 0);
d2608 1
a2608 1
			ASSERT((data & TLB_NFO) == 0);
d2860 1
a2860 1
				ASSERT((data & TLB_NFO) == 0);
d3488 1
a3488 1
	ASSERT(va != NULL);
d3492 1
a3492 1
	pg = PHYS_TO_VM_PAGE(pg);
d3667 1
a3667 1
	pmap_free_page(pa, pm);
@


1.39
log
@- Use pmap_get_page and pmap_free_page to manage memory allocations
  for sparc64 pmap. They take care of all the magic of checking for
  initialized pages, zeroing them, etc.
- merge pmap_create and pmap_pinit.
- don't do the page allocation dance in pmap_kenter, we have growkernel.
- Clean up if I was close to something dirty.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2007/04/21 13:43:38 art Exp $	*/
a1338 2
		cpcb = (struct pcb *)vmmap;
		proc0paddr = cpcb;
d1374 1
a1374 1
		cpus = (struct cpu_info *)(intstk+CPUINFO_VA-INTSTACK);
d1419 2
@


1.38
log
@Do a simple switch to VM_PAGE_MD. Just the path of least resistance for now,
more work coming soon.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2007/04/13 18:57:49 art Exp $	*/
a171 3
extern struct vm_page *vm_page_alloc1(void);
extern void vm_page_free1(struct vm_page *);

d435 2
a436 2
#define	pmap_get_page(p)	uvm_page_physget((p));

d463 1
a463 3
pmap_enter_kpage(va, data)
	vaddr_t va;
	int64_t data;
d470 1
a470 2
		pmap_get_page(&newp);
		if (!newp) {
d474 1
a474 4
		pmap_zero_phys(newp);
#ifdef DEBUG
		enter_stats.ptpneeded ++;
#endif
a477 4
#ifdef DEBUG
		if (pmapdebug & PDB_BOOT1)
		{int i; for (i=0; i<140000000; i++) ;}
#endif
d1255 2
a1256 1
	pmap_pinit(pmap_kernel());
d1261 1
a1261 2
			pmap_get_page(&newp);
			pmap_zero_phys(newp);
d1352 1
a1352 2
			pmap_get_page(&pa);
			pmap_zero_phys(pa);
a1482 1
#ifdef PMAP_GROWKERNEL
d1494 1
a1494 2
pmap_growkernel(maxkvaddr)
        vaddr_t maxkvaddr; 
a1495 1
	int s;
d1504 2
a1505 4
	s = splvm();
	simple_lock(&pm->pm_lock);
	DPRINTF(PDB_GROW, 
		("pmap_growkernel(%lx...%lx)\n", kbreak, maxkvaddr));
d1509 2
a1510 1
		if (pseg_get(pm, kbreak)) continue;
d1517 1
a1517 16
			if (uvm.page_init_done || !uvm_page_physget(&pg)) {
				struct vm_page *page;
				DPRINTF(PDB_GROW,
("pmap_growkernel: need to alloc page\n"));
				while ((page = 
					vm_page_alloc1()) == NULL) {
					DPRINTF(PDB_GROW, 
("pmap_growkernel: calling uvm_wait()\n"));
					uvm_wait("pmap_growkernel");
				}
				pg = (paddr_t)VM_PAGE_TO_PHYS(page);
			}
			pmap_zero_phys((paddr_t)pg);
#ifdef DEBUG
			enter_stats.ptpneeded ++;
#endif 
d1521 1
a1521 2
	simple_unlock(&pm->pm_lock);
	splx(s);
a1523 1
#endif
d1529 1
a1529 1
pmap_create()
d1537 1
a1537 7
#ifdef DEBUG
	if (pmapdebug & PDB_CREATE)
		printf("pmap_create(): created %p\n", pm);
#endif
	pmap_pinit(pm);
	return pm;
}
d1539 4
a1542 7
/*
 * Initialize a preallocated and zeroed pmap structure.
 */
void
pmap_pinit(pm)
	struct pmap *pm;
{
d1544 1
a1544 34
	/*
	 * Allocate some segment registers for this pmap.
	 */
	simple_lock_init(&pm->pm_lock);
	simple_lock(&pm->pm_lock);
	pm->pm_refs = 1;
	if(pm != pmap_kernel()) {
		struct vm_page *page;
#ifdef NOTDEF_DEBUG
		printf("pmap_pinit: need to alloc page\n");
#endif
		while ((page = vm_page_alloc1()) == NULL) {
			/*
			 * Let the pager run a bit--however this may deadlock
			 */
#ifdef NOTDEF_DEBUG
			printf("pmap_pinit: calling uvm_wait()\n");
#endif
			uvm_wait("pmap_pinit");
		}
		pm->pm_physaddr = (paddr_t)VM_PAGE_TO_PHYS(page);
		pmap_zero_phys(pm->pm_physaddr);
		pm->pm_segs = (int64_t *)(u_long)pm->pm_physaddr;
		if (!pm->pm_physaddr) panic("pmap_pinit");
#ifdef NOTDEF_DEBUG
		printf("pmap_pinit: segs %p == %p\n", pm->pm_segs, (void *)page->phys_addr);
#endif
		ctx_alloc(pm);
	}
#ifdef DEBUG
	if (pmapdebug & PDB_CREATE)
		printf("pmap_pinit(%p): ctx %d\n", pm, pm->pm_ctx);
#endif
	simple_unlock(&pm->pm_lock);
d1551 1
a1551 2
pmap_reference(pm)
	struct pmap *pm;
a1552 4
	int s;

	s = splvm();
	simple_lock(&pm->pm_lock);
a1553 2
	simple_unlock(&pm->pm_lock);
	splx(s);
d1632 1
a1632 1
					vm_page_free1(PHYS_TO_VM_PAGE((paddr_t)(u_long)ptbl));
d1636 1
a1636 1
			vm_page_free1(PHYS_TO_VM_PAGE((paddr_t)(u_long)pdir));
d1641 1
a1641 1
	vm_page_free1(PHYS_TO_VM_PAGE(tmp));
d1720 1
a1720 1
						vm_page_free1(PHYS_TO_VM_PAGE((paddr_t)(u_long)ptbl));
d1727 1
a1727 1
				vm_page_free1(PHYS_TO_VM_PAGE((paddr_t)(u_long)pdir));
a1838 1
	paddr_t pg;
d1840 1
a1840 1
	int i, s;
d1870 5
a1874 31
	pg = NULL;
	while ((i = pseg_set(pm, va, tte.data, pg)) == 1) {
		pg = NULL;
		if (uvm.page_init_done || !uvm_page_physget(&pg)) {
			struct vm_page *page;
#ifdef NOTDEF_DEBUG
			printf("pmap_kenter_pa: need to alloc page\n");
#endif
			while ((page = vm_page_alloc1()) == NULL) {
				/*
				 * Let the pager run a bit--however this may deadlock
				 */
				panic("pmap_kenter_pa: no free pages");
#ifdef NOTDEF_DEBUG
				printf("pmap_kenter_pa: calling uvm_wait()\n");
#endif
				uvm_wait("pmap_kenter_pa");
			}
			pg = (paddr_t)VM_PAGE_TO_PHYS(page);
		}
		pmap_zero_phys((paddr_t)pg);
#ifdef DEBUG
		enter_stats.ptpneeded ++;
#endif
	}
	if (i == 2) {
		/* We allocated a spare page but didn't use it.  Free it. */
		printf("pmap_kenter_pa: freeing unused page %llx\n", 
		       (long long)pg);
		vm_page_free1(PHYS_TO_VM_PAGE(pg));
	}
d2052 1
a2053 4
#ifdef NOTDEF_DEBUG
	printf("pmap_enter: inserting %x:%x at %x\n", 
	       (int)(tte.data>>32), (int)tte.data, (int)va);
#endif
d2056 5
a2060 26
		if (uvm.page_init_done || !uvm_page_physget(&pg)) {
			struct vm_page *page;
#ifdef NOTDEF_DEBUG
			printf("pmap_enter: need to alloc page\n");
#endif
			while ((page = vm_page_alloc1()) == NULL) {
				/*
				 * Let the pager run a bit--however this may deadlock
				 */
				if (pm == pmap_kernel())
					panic("pmap_enter: no free pages");
#ifdef NOTDEF_DEBUG
				printf("pmap_enter: calling uvm_wait()\n");
#endif
				uvm_wait("pmap_enter");
			}
			pg = (paddr_t)VM_PAGE_TO_PHYS(page);
		} 
		pmap_zero_phys((paddr_t)pg);
#ifdef DEBUG
		enter_stats.ptpneeded ++;
#endif
#ifdef NOTDEF_DEBUG
	printf("pmap_enter: inserting %x:%x at %x with %x\n", 
	       (int)(tte.data>>32), (int)tte.data, (int)va, (int)pg);
#endif
d3468 2
a3469 7
/*
 *	vm_page_alloc1:
 *
 *	Allocate and return a memory cell with no associated object.
 */
struct vm_page *
vm_page_alloc1()
d3471 12
a3482 3
	struct vm_page *pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (pg) {
		pg->wire_count = 1;	/* no mappings yet */
d3484 4
d3489 2
a3490 1
	return pg;
a3492 8
/*
 *	vm_page_free1:
 *
 *	Returns the given page to the free list,
 *	disassociating it with any VM object.
 *
 *	Object and page must be locked prior to entry.
 */
d3494 1
a3494 2
vm_page_free1(mem)
	struct vm_page *mem;
d3496 4
a3499 9
	if (mem->pg_flags != (PG_CLEAN|PG_FAKE)) {
		printf("Freeing invalid page %p\n", mem);
		printf("pa = %llx\n", (unsigned long long)VM_PAGE_TO_PHYS(mem));
		Debugger();
		return;
	}
	atomic_setbits_int(&mem->pg_flags, PG_BUSY);
	mem->wire_count = 0;
	uvm_pagefree(mem);
d3536 1
a3540 1
	struct vm_page *pg;
d3548 2
a3549 2
	pg = vm_page_alloc1();
	pa = (paddr_t)VM_PAGE_TO_PHYS(pg);
d3724 1
a3724 1
	vm_page_free1(pg);
@


1.37
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2007/04/05 20:08:30 claudio Exp $	*/
a191 12
 * For each struct vm_page, there is a list of all currently valid virtual
 * mappings of that page.  An entry is a pv_entry_t, the list is pv_table.
 * XXX really should do this as a part of the higher level code.
 */
typedef struct pv_entry {
	struct pv_entry	*pv_next;	/* next pv_entry */
	struct pmap	*pv_pmap;	/* pmap where mapping lies */
	vaddr_t	pv_va;		/* virtual address for mapping */
} *pv_entry_t;
/* PV flags encoded in the low bits of the VA of the first pv_entry */

/*
d253 1
a253 1
	int bank, pg;
d255 2
a256 5
	bank = vm_physseg_find(atop(pa), &pg);
	if (bank == -1)
		return (NULL);
	else
		return (pv_entry_t)&vm_physmem[bank].pmseg.pvent[pg];
a1458 8
	struct vm_page *m;
	paddr_t pa;
	psize_t size;
	vaddr_t va;
	struct pglist mlist;
	vsize_t		s;
	int		bank;
	struct pv_entry	*pvh;
a1463 43
	size = sizeof(struct pv_entry) * physmem;
	TAILQ_INIT(&mlist);
	if (uvm_pglistalloc((psize_t)size, (paddr_t)0, (paddr_t)-1,
		(paddr_t)NBPG, (paddr_t)0, &mlist, 1, 0) != 0)
		panic("cpu_start: no memory");

	va = uvm_km_valloc(kernel_map, size);
	if (va == 0)
		panic("cpu_start: no memory");

	pv_table = (struct pv_entry *)va;
	m = TAILQ_FIRST(&mlist);

	/* Map the pages */
	for (; m != NULL; m = TAILQ_NEXT(m,pageq)) {
		u_int64_t data;

		pa = VM_PAGE_TO_PHYS(m);
		pmap_zero_page(m);
		data = TSB_DATA(0 /* global */, 
			PGSZ_8K,
			pa,
			1 /* priv */,
			1 /* Write */,
			1 /* Cacheable */,
			FORCE_ALIAS /* ALIAS -- Disable D$ */,
			1 /* valid */,
			0 /* IE */);
		pmap_enter_kpage(va, data);
		va += NBPG;
	}

	/*
	 * Memory for the pv heads has already been allocated.
	 * Initialize the physical memory segments.
	 */
	pvh = pv_table;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		s = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pvh;
		pvh += s;
	}

d2755 1
a2755 2
pmap_clear_reference(pg)
	struct vm_page* pg;
a2756 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d2766 1
a2766 1
		printf("pmap_clear_reference(%llx)\n", (unsigned long long)pa);
d2771 1
a2771 1
	pv = pa_to_pvh(pa);
d2774 2
a2775 1
		printf("pmap_clear_reference(): pv %p still modified\n", (long)pa);
d2822 1
a2822 1
	dcache_flush_page(pa);
d2831 3
a2833 2
		printf("pmap_clear_reference: page %lx %s\n", (long)pa, 
		       (changed?"was referenced":"was not referenced"));
d2843 1
a2843 2
pmap_is_modified(pg)
	struct vm_page* pg;
d2845 1
a2845 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a2846 1
	register pv_entry_t pv, npv;
d2850 1
a2850 1
	pv = pa_to_pvh(pa);
d2877 2
a2878 1
		printf("pmap_is_modified(%llx) = %d\n", (unsigned long long)pa, i);
d2887 1
a2887 2
pmap_is_referenced(pg)
	struct vm_page* pg;
a2888 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d2890 1
a2890 1
	register pv_entry_t pv, npv;
d2894 1
a2894 1
	pv = pa_to_pvh(pa);
d2918 2
a2919 1
		printf("pmap_is_referenced(%llx) = %d\n", (unsigned long long)pa, i);
@


1.36
log
@Fix a comment. 64KB is enough for the cpu_info structure. 64MB would be a
bit lavish. OK jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2007/04/04 17:44:45 art Exp $	*/
d3682 1
a3682 1
		pg->pg_flags &= ~PG_BUSY;	/* never busy */
d3705 1
a3705 1
	mem->pg_flags |= PG_BUSY;
@


1.35
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2007/01/12 19:19:34 kettenis Exp $	*/
d1034 1
a1034 1
	 * Allocate a 64MB page for the cpu_info structure now.
@


1.34
log
@Move kernel virtual address space to make sure it doesn't overlap with
physical memory to avoid problems on UltraSPARC-III and III+.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2007/01/06 21:08:07 kettenis Exp $	*/
d3682 1
a3682 1
		pg->flags &= ~PG_BUSY;	/* never busy */
d3699 1
a3699 1
	if (mem->flags != (PG_CLEAN|PG_FAKE)) {
d3705 1
a3705 1
	mem->flags |= PG_BUSY;
@


1.33
log
@Use VM_MAX_KERNEL_ADDRESS instead of KERNEND in pmap_growkernel().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2006/07/01 16:23:31 miod Exp $	*/
d1539 3
d1551 2
a1552 1
	 * Reserve one segment for kernel virtual memory
d1554 3
@


1.32
log
@Add PMAP_LITTLE to the list of forbidden pa bits in pmap_kenter_pa().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2006/06/20 20:31:04 miod Exp $	*/
d1575 1
a1575 1
	if (maxkvaddr >= KERNEND) {
d1577 1
a1577 1
		       (void *)KERNEND, (void *)maxkvaddr);
@


1.31
log
@Fix tsbsize computation; from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2006/06/16 23:04:49 miod Exp $	*/
d1990 1
a1990 1
	if (pa & (PMAP_NVC|PMAP_NC))
d3450 1
a3450 1
				    data&TLB_PA_MASK != pa)
@


1.30
log
@When checking if a page is managed or not, do not invoke IS_VM_PHYSADDR
and then pa_to_pvh, since they amount to the same work; instead let pa_to_pvh
return NULL for unmanaged pages and test for this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2006/06/02 19:53:32 miod Exp $	*/
d876 1
a876 1
	if (physmem < 64 * 1024 * 1024)
d878 1
a878 1
	else if (physmem < 512 * 1024 * 1024)
@


1.29
log
@All I wanted was to fix the bogus PG_SZ macro, but this is better:
Do not try to map the message buffer with large pages, it's just as easy to
simply use as many 8KB pages as necessary. From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2006/06/02 00:26:08 kettenis Exp $	*/
a70 2
#define IS_VM_PHYSADDR(PA) (vm_physseg_find(atop(PA), NULL) != -1)

d253 1
a260 8
#define pa_index(pa)		atop((pa) - first_phys_addr)
#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	(pv_entry_t)&vm_physmem[bank_].pmseg.pvent[pg_];		\
})
d262 11
d1738 1
a1738 1
			ASI_PHYS_CACHED))) {
d1745 3
d1750 5
a1754 4
						if (data&TLB_V && 
						    IS_VM_PHYSADDR(data&TLB_PA_MASK)) {
							paddr_t pa;
							pv_entry_t pv;
a1761 2
							pa = data&TLB_PA_MASK;
							pv = pa_to_pvh(pa);
d2184 2
a2185 2
	if (IS_VM_PHYSADDR(pa)) {
		pv = pa_to_pvh(pa);
d2317 1
d2322 2
a2323 3
			if (IS_VM_PHYSADDR(entry)) {
				pv_entry_t pv;

a2324 1
				pv = pa_to_pvh(entry);
d2378 1
d2424 2
a2425 3
			if (IS_VM_PHYSADDR(pa)) {
				pv_entry_t pv;

a2426 1
				pv = pa_to_pvh(pa);
a3611 2
	if (!IS_VM_PHYSADDR(pa))
		return;
d3614 2
@


1.28
log
@Remove unused local variable from debug code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2005/11/11 16:38:30 miod Exp $	*/
a470 20
 * Calculate the largest page size that will map this.
 *
 * You really need to do this both on VA and PA.
 */
#define	PMAP_PAGE_SIZE(va, pa, len, pgsz, pglen)			\
do {									\
	for ((pgsz) = PGSZ_4M; (pgsz); (pgsz)--) {			\
		(pglen) = PG_SZ(pgsz);					\
									\
		if (((len) >= (pgsz)) &&				\
			((pa) & ((pglen)-1) & ~PG_SZ(PGSZ_8K)) == 0 &&	\
			((va) & ((pglen)-1) & ~PG_SZ(PGSZ_8K)) == 0)	\
			break;						\
	}								\
	(pgsz) = 0;							\
	(pglen) = PG_SZ(pgsz);						\
} while (0)


/*
a1311 4
		int pgsz;
		psize_t psize;

		PMAP_PAGE_SIZE(va, phys_msgbuf, msgbufsiz, pgsz, psize);
d1313 1
a1313 1
			pgsz,
d1321 4
a1324 6
		do {
			pmap_enter_kpage(va, data);
			va += NBPG;
			msgbufsiz -= NBPG;
			phys_msgbuf += NBPG;
		} while (psize-=NBPG);
@


1.27
log
@Simplify a computation in a can't happen diagnostic message.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2003/11/03 07:01:33 david Exp $	*/
d413 1
a413 1
	int i, j, s;
@


1.26
log
@spelling fixes (in the comments)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2003/06/03 17:16:33 art Exp $	*/
d3194 1
a3194 2
				printf("vm wire count %d\n", 
					PHYS_TO_VM_PAGE(pa)->wire_count);
@


1.25
log
@Play it safe - when invalidating the tsb, only touch the tag and
always set it to an invalid tag. To actually have an invalid tag make
sure that we never allocate the last context in ctx_alloc. Move all
that magic into a simple tsb_invalidate inline function.

Fixes weird race conditions in memory handling where the page in the tlb
didn't match the page tables.

After discussion with henric@@. Tested for a while and noone objected.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2003/05/11 22:05:49 jason Exp $	*/
d2649 1
a2649 1
 * Determine (non)existance of physical page
@


1.24
log
@nuke common: physmem
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2003/05/10 00:35:42 art Exp $	*/
d281 5
d317 15
d2094 1
a2094 1
	int i, s, flush = 0;
a2119 1
			int64_t tag;
d2137 1
a2137 18
			i = ptelookup_va(va);
			tag = TSB_TAG(0, pm->pm_ctx, va);
			/*
			 * Invalidate the TSB 
			 *
			 * While we can invalidate it by clearing the
			 * valid bit:
			 *
			 * ptp->data_v = 0;
			 *
			 * it's faster to do store 1 doubleword.
			 */
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = 0LL;
			}
			if (tsb_immu[i].tag == tag) {
				tsb_immu[i].data = 0LL;
			}
d2171 1
a2171 1
	int i, s, aliased = 0;
a2282 1
	i = ptelookup_va(va);
d2284 1
a2284 1
		int64_t tag = TSB_TAG(0, pm->pm_ctx, va);
a2285 6
		if (tsb_dmmu[i].tag == tag) {
			tsb_dmmu[i].data = 0LL;
		}
		if (tsb_immu[i].tag == tag) {
			tsb_immu[i].data = 0LL;
		}
d2305 2
a2306 2
	int i, s, flush=0;
	int64_t data, tag;
d2364 3
a2366 9
			if (!pm->pm_ctx && pm != pmap_kernel()) continue;
			i = ptelookup_va(va);
			tag = TSB_TAG(0, pm->pm_ctx, va);
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = 0LL;
			}
			if (tsb_immu[i].tag == tag) {
				tsb_immu[i].data = 0LL;
			}
d2399 1
a2399 1
	int i, s;
d2401 1
a2401 1
	int64_t data, tag;
d2468 3
a2470 12
			if (!pm->pm_ctx && pm != pmap_kernel()) continue;
			i = ptelookup_va(sva);
			tag = TSB_TAG(0, pm->pm_ctx, sva);
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = data;
			}
			if (tsb_immu[i].tag == tag) {
				if ((data & TLB_EXEC) != 0)
					tsb_immu[i].data = data;
				else
					tsb_immu[i].data = 0LL;
			}
d2761 1
a2761 1
	int i, s;
d2807 1
a2807 4
				int64_t tag;

				i = ptelookup_va(pv->pv_va & PV_VAMASK);
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
a2808 4
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;/* data */
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;/* data */
d2846 1
a2846 1
	int i, s;
d2892 2
a2893 10
				int64_t tag;

				i = ptelookup_va(pv->pv_va & PV_VAMASK);
				/* Invalidate our TSB entry since ref info is in the PTE */
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va&PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0;
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0;
d3076 1
a3076 1
	register int i, s;
d3144 1
a3144 4
					int64_t tag;
					i = ptelookup_va(pv->pv_va&PV_VAMASK);
					/* since we already know the va for each mapping we don't need to scan the entire TSB */
					tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
a3145 4
					if (tsb_dmmu[i].tag == tag)
						tsb_dmmu[i].data = /* data */ 0LL;
					if (tsb_immu[i].tag == tag)
						tsb_immu[i].data = /* data */ 0LL;
d3207 1
a3207 5
				int64_t tag;

				/* clear the entry in the TSB */
				i = ptelookup_va(npv->pv_va&PV_VAMASK);
				tag = TSB_TAG(0, npv->pv_pmap->pm_ctx,
a3208 5
				/* since we already know the va for each mapping we don't need to scan the entire TSB */
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;			
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;			
d3253 1
a3253 5
				int64_t tag;

				i = ptelookup_va(pv->pv_va&PV_VAMASK);
				/* since we already know the va for each mapping we don't need to scan the entire TSB */
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
a3254 4
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;			
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;			
d3337 5
a3341 1
		if (cnum >= numctx-1) 
d3355 1
a3355 1
				tsb_dmmu[i].data = 0LL;
d3357 1
a3357 1
				tsb_immu[i].data = 0LL;
d3631 1
a3631 1
	int i, s;
d3678 1
a3678 7
			int64_t tag;
			i = ptelookup_va(va);
			tag = TSB_TAG(0, pv->pv_pmap->pm_ctx, va);
			if (tsb_dmmu[i].tag == tag)
				tsb_dmmu[i].data = 0LL; 
			if (tsb_immu[i].tag == tag)
				tsb_immu[i].data = 0LL; 
@


1.23
log
@Remove some exec mappings in the pmap test code (ifdef DEBUG).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2003/02/17 01:29:20 henric Exp $	*/
d284 1
a284 1
int physmem;
@


1.22
log
@
Add support for the Sun Enterprise 450
Reduce the size of a GENERIC kernel by ~190k
Remove the nasty pointer/bus_space_handle_t casts
Adds debug bus_space code including the ability to trace
    bus operations (it actually works now).

The following rules are now followed (and verfified by the debug
code):

1.  A "bus_space_handle_t" may only be used with the
    "bus_space_tag_t" that created it.
2.  Only "bus_space_map()" may create "bus_space_handle_t"s.
3.  A "bus_space_handle_t" may not be modified after it has
    been created (other than being destroyed by "bus_space_unmap()").


Thanks to help from mcbride, marc, jason, drahn, to anyone that might
have slipped my mind at the moment.

ok jason@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2002/10/12 02:03:45 krw Exp $	*/
d3837 2
a3838 1
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_ALL, VM_PROT_ALL);
d3914 2
a3915 1
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_ALL, VM_PROT_ALL);
d3940 2
a3941 1
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_ALL, VM_PROT_ALL);
d3966 2
a3967 1
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_ALL, VM_PROT_ALL);
@


1.21
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Last bits of diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2002/10/07 18:35:57 mickey Exp $	*/
d137 1
a137 1
		stda(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
d143 1
a143 1
		stda(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
d147 1
a147 1
	stda(&ptbl[va_to_pte(addr)], ASI_PHYS_CACHED, tte);
d158 1
a158 1
		stda(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
d164 1
a164 1
		stda(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
d1103 1
a1103 1
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void *)tsb,
d2193 2
a2194 1
	if ((tte.data = pseg_get(pm, va))<0) {
@


1.20
log
@this removes the functionality of adding allocated
pages into the queue already containing allocated pages.
breaks i386:setup_buffers() because of this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2002/10/06 22:06:15 art Exp $	*/
d86 1
a86 1
	if ((paddr_t)pm->pm_segs == spare) panic("pseg_check: pm_segs == %llx\n", spare);
d90 1
a90 1
				panic("pseg_check: pdir %d == %llx\n", i,
d95 1
a95 1
				panic("pseg_check: ptbl %d:%d == %llx\n", i, k,
d184 1
a184 1
	if (!(x)) panic("%s at line %d: assertion failed\n", #x, __LINE__);
d187 1
a187 1
	if (!(x)) panic("%s at line %d: assertion failed\n", "x", __LINE__);
d912 1
a912 1
		panic("No kernel text segment!\r\n");
d1091 1
a1091 1
		panic("TSB alloc\n");
d2094 1
a2094 1
			panic("pmap_kremove: va=%08x in locked TLB\r\n", 
d2338 1
a2338 1
			panic("pmap_remove: va=%08x in locked TLB\r\n", (u_int)va);
@


1.19
log
@No more need to initialize the result list before uvm_pglistalloc.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2002/09/18 10:36:50 art Exp $	*/
d1492 1
@


1.18
log
@Don't forget to map the prom executable. Otherwise we'll have trouble
starting X after the prom has been bumped from the iTLB.

matthieu@@ jason@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2002/09/10 18:29:44 art Exp $	*/
a1491 1
	TAILQ_INIT(&mlist);
@


1.17
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2002/08/20 19:28:55 jason Exp $	*/
d1353 1
a1353 1
					(prom_map[i].tte + j)|
@


1.16
log
@Two fixes from NetBSD (this allows my u60 to boot multiuser):
- Use paddr_t for avail_start and avail_end so we can handle machines with
RAM above the 2GB mark.
- Do not truncate the kernel pmap physical address to an `int' before
sticking it in the context lookup table.  Fixes a booting issue on
Netra T1125s.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2002/07/24 00:48:25 art Exp $	*/
d177 3
d493 1
a493 1
		pmap_zero_page(newp);
d1240 1
a1240 1
				pmap_zero_page(p);
d1287 1
a1287 1
			pmap_zero_page(newp);
d1385 1
a1385 1
			pmap_zero_page(pa);
d1509 1
a1509 1
		pmap_zero_page(pa);
d1610 1
a1610 1
			pmap_zero_page((paddr_t)pg);
d1672 1
a1672 1
		pmap_zero_page(pm->pm_physaddr);
d1881 16
d2036 1
a2036 1
		pmap_zero_page((paddr_t)pg);
d2266 1
a2266 1
		pmap_zero_page((paddr_t)pg);
@


1.15
log
@Support for non-exec page mappings.
 - split the one TSB into two - one for dmmu, one for immu.
 - don't load pages without PG_EXEC into the immu TSB.
 - support for setting correct permissions on exec faults.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2002/07/20 20:19:11 art Exp $	*/
d306 1
a306 1
int avail_start, avail_end;	/* These are used by ps & family */
d1289 1
a1289 1
		((paddr_t*)ctxbusy)[0] = (int)pmap_kernel()->pm_physaddr;
d2518 7
a2524 5
			pa = ldxa((vaddr_t)&pm->pm_segs[va_to_seg(va)], ASI_PHYS_CACHED);
			printf("pmap_extract: va=%p segs[%ld]=%llx", (void *)(u_long)va, (long)va_to_seg(va), (unsigned long long)pa);
			if (pa) {
				pa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)pa)[va_to_dir(va)], ASI_PHYS_CACHED);
				printf(" segs[%ld][%ld]=%lx", (long)va_to_seg(va), (long)va_to_dir(va), (long)pa);
d2526 2
a2527 2
			if (pa)	{
				pa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)pa)[va_to_pte(va)], ASI_PHYS_CACHED);
d2529 1
a2529 1
				       (long)va_to_dir(va), (long)va_to_pte(va), (long)pa);
d2531 1
a2531 1
			printf(" pseg_get: %lx\n", (long)pa);
@


1.14
log
@Always HWREF. Don't have it as an option because the slow-slow version
doesn't make any sense (slow and doesn't work follow the rules).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2002/06/11 11:16:46 art Exp $	*/
d273 2
a274 1
pte_t *tsb;
d1094 4
a1097 2
	valloc(tsb, pte_t, TSBSIZE);
	bzero(tsb, TSBSIZE);
a1980 4
#if 0
	/* Not needed -- all operations are atomic. */
	simple_lock(&pm->pm_lock);
#endif
d1988 4
a1991 2
	/* We don't track modification here. */
	if (VM_PROT_WRITE & prot)
d1993 2
a2027 22
#ifdef DEBUG
	i = ptelookup_va(va);
	if( pmapdebug & PDB_ENTER )
		prom_printf("pmap_kenter_pa: va=%08x tag=%x:%08x data=%08x:%08x tsb[%d]=%08x\r\n", va,
			    (int)(tte.tag>>32), (int)tte.tag, 
			    (int)(tte.data>>32), (int)tte.data, 
			    i, &tsb[i]);
	if( pmapdebug & PDB_MMU_STEAL && tsb[i].data ) {
		prom_printf("pmap_kenter_pa: evicting entry tag=%x:%08x data=%08x:%08x tsb[%d]=%08x\r\n",
			    (int)(tsb[i].tag>>32), (int)tsb[i].tag, 
			    (int)(tsb[i].data>>32), (int)tsb[i].data, 
			    i, &tsb[i]);
		prom_printf("with va=%08x tag=%x:%08x data=%08x:%08x tsb[%d]=%08x\r\n", va,
			    (int)(tte.tag>>32), (int)tte.tag, 
			    (int)(tte.data>>32), (int)tte.data, 
			    i, &tsb[i]);
	}
#endif
#if 0
/* Not needed -- all operations are atomic. */
	simple_unlock(&pm->pm_lock);
#endif
a2028 1
	ASSERT((tsb[i].data & TLB_NFO) == 0);
d2081 2
a2082 1
			
d2100 16
a2115 20
			if (tsb[i].tag > 0 
			    && tsb[i].tag == TSB_TAG(0,pm->pm_ctx,va))
			{
				/* 
				 * Invalidate the TSB 
				 * 
				 * While we can invalidate it by clearing the
				 * valid bit:
				 *
				 * ptp->data_v = 0;
				 *
				 * it's faster to do store 1 doubleword.
				 */
#ifdef DEBUG
				if (pmapdebug & PDB_DEMAP)
					printf(" clearing TSB [%d]\n", i);
#endif
				tsb[i].data = 0LL; 
				ASSERT((tsb[i].data & TLB_NFO) == 0);
				/* Flush the TLB */
d2194 1
a2194 1
		if (flags & VM_PROT_WRITE || (tte.data & (TLB_MODIFY)))
d2217 2
a2261 17
#ifdef DEBUG
	if( pmapdebug & PDB_ENTER )
		prom_printf("pmap_enter: va=%08x tag=%x:%08x data=%08x:%08x tsb[%d]=%08x\r\n", va,
			    (int)(tte.tag>>32), (int)tte.tag, 
			    (int)(tte.data>>32), (int)tte.data, 
			    i, &tsb[i]);
	if( pmapdebug & PDB_MMU_STEAL && tsb[i].data ) {
		prom_printf("pmap_enter: evicting entry tag=%x:%08x data=%08x:%08x tsb[%d]=%08x\r\n",
			    (int)(tsb[i].tag>>32), (int)tsb[i].tag, 
			    (int)(tsb[i].data>>32), (int)tsb[i].data, 
			    i, &tsb[i]);
		prom_printf("with va=%08x tag=%x:%08x data=%08x:%08x tsb[%d]=%08x\r\n", va,
			    (int)(tte.tag>>32), (int)tte.tag, 
			    (int)(tte.data>>32), (int)tte.data, 
			    i, &tsb[i]);
	}
#endif
d2263 7
a2269 14
		if (tsb[i].tag > 0 && 
		    tsb[i].tag == TSB_TAG(0,pm->pm_ctx,va)) {
			/* 
			 * Invalidate the TSB 
			 * 
			 * While we can invalidate it by clearing the
			 * valid bit:
			 *
			 * ptp->data_v = 0;
			 *
			 * it's faster to do store 1 doubleword.
			 */
			tsb[i].data = 0LL; 
			ASSERT((tsb[i].data & TLB_NFO) == 0);
a2272 1
		ASSERT((tsb[i].data & TLB_NFO) == 0);
d2291 1
a2291 1
	int64_t data;
d2351 3
a2353 20
			if (tsb[i].tag > 0 
			    && tsb[i].tag == TSB_TAG(0,pm->pm_ctx,va))
			{
				/* 
				 * Invalidate the TSB 
				 * 
				 * While we can invalidate it by clearing the
				 * valid bit:
				 *
				 * ptp->data_v = 0;
				 *
				 * it's faster to do store 1 doubleword.
				 */
#ifdef DEBUG
				if (pmapdebug & PDB_REMOVE)
					printf(" clearing TSB [%d]\n", i);
#endif
				tsb[i].data = 0LL; 
				ASSERT((tsb[i].data & TLB_NFO) == 0);
				/* Flush the TLB */
d2355 2
a2356 9
#ifdef NOTDEF_DEBUG
			else if (pmapdebug & PDB_REMOVE) {
				printf("TSB[%d] has ctx %d va %x: ",
				       i,
				       TSB_TAG_CTX(tsb[i].tag),
				       (int)(TSB_TAG_VA(tsb[i].tag)|(i<<13)));
				printf("%08x:%08x %08x:%08x\n",
				       (int)(tsb[i].tag>>32), (int)tsb[i].tag, 
				       (int)(tsb[i].data>>32), (int)tsb[i].data);			       
a2357 1
#endif
d2392 1
a2392 1
	int64_t data;
d2397 2
a2398 1
	if (prot & VM_PROT_WRITE) 
d2442 2
a2443 1
				if (data & TLB_ACCESS) pv->pv_va |= PV_REF;
d2448 4
a2451 1
			data &= ~(TLB_W|TLB_REAL_W);
d2461 9
a2469 5
			if (tsb[i].tag > 0 
			    && tsb[i].tag == TSB_TAG(0,pm->pm_ctx,sva)) {
				tsb[i].data = data;
				ASSERT((tsb[i].data & TLB_NFO) == 0);
				
d2805 11
a2815 5
				i = ptelookup_va(pv->pv_va&PV_VAMASK);
				if (tsb[i].tag == TSB_TAG(0, pv->pv_pmap->pm_ctx, pv->pv_va&PV_VAMASK))
					tsb[i].data = /* data */ 0;
				tlb_flush_pte(pv->pv_va&PV_VAMASK, 
					pv->pv_pmap->pm_ctx);
d2896 4
a2899 2
				pv->pv_pmap == pmap_kernel()) {
				i = ptelookup_va(pv->pv_va&PV_VAMASK);
d2901 6
a2906 4
				if (tsb[i].tag == 
					TSB_TAG(0,pv->pv_pmap->pm_ctx,pv->pv_va&
						PV_VAMASK))
					tsb[i].data = 0;
d3157 1
d3160 6
a3165 2
					if (tsb[i].tag == TSB_TAG(0, pv->pv_pmap->pm_ctx, pv->pv_va&PV_VAMASK))
						tsb[i].data = /* data */ 0;
d3227 2
d3231 2
d3234 4
a3237 2
				if (tsb[i].tag == TSB_TAG(0, npv->pv_pmap->pm_ctx, npv->pv_va&PV_VAMASK))
					tsb[i].data = 0LL;			
d3282 2
d3286 8
a3293 3
				if (tsb[i].tag == TSB_TAG(0, pv->pv_pmap->pm_ctx, pv->pv_va&PV_VAMASK))
					tsb[i].data = 0LL;			
				tlb_flush_pte(pv->pv_va&PV_VAMASK, pv->pv_pmap->pm_ctx);
d3387 4
a3390 2
			if (TSB_TAG_CTX(tsb[i].tag) == cnum)
				tsb[i].data = 0LL;
d3711 1
d3713 5
a3717 15
			if (tsb[i].tag > 0 && tsb[i].tag == 
			    TSB_TAG(0, pv->pv_pmap->pm_ctx, va)) {
				/* 
				 * Invalidate the TSB 
				 * 
				 * While we can invalidate it by clearing the
				 * valid bit:
				 *
				 * ptp->data_v = 0;
				 *
				 * it's faster to do store 1 doubleword.
				 */
				tsb[i].data = 0LL; 
				ASSERT((tsb[i].data & TLB_NFO) == 0);
			}
@


1.13
log
@never allow uncached mappings to pmap_kenter_pa
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2002/03/14 03:16:01 millert Exp $	*/
a3 1
#define	HWREF
d1990 2
a1991 1
	if (VM_PROT_WRITE & prot) tte.data |= TLB_REAL_W|TLB_W; /* HWREF -- XXXX */
d2238 4
a2241 15
#ifdef HWREF
	if (prot & VM_PROT_WRITE) tte.data |= TLB_REAL_W;
#else
	/* If it needs ref accounting do nothing. */
	if (!(flags&VM_PROT_READ)) {
		simple_unlock(&pm->pm_lock);
		splx(s);
		if (wired) {
			printf("pmap_enter: wired but not readable\n");
			Debugger();
		}
		return 0;
	}
#endif
	if (wired) tte.data |= TLB_TSB_LOCK;
a2858 1
#ifdef HWREF
a2859 3
#else
			data &= ~(TLB_MODIFY|TLB_W|TLB_REAL_W);
#endif
a2941 1
#ifdef HWREF
a2944 5
#else
			if (data < 0)
				changed |= 1;
			data = 0;
#endif
a3001 1
#ifdef HWREF
a3022 1
#endif
a3046 1
#ifdef HWREF 
a3064 1
#endif
@


1.12
log
@Final __P removal plus some cosmetic fixups
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2002/03/14 01:26:45 millert Exp $	*/
d1970 5
a1985 4
#ifdef DEBUG
	if (pa & (PMAP_NVC|PMAP_NC)) 
		enter_stats.ci ++;
#endif
d1989 1
a1989 1
				 (!(pa & PMAP_NC)), pa & (PMAP_NVC), 1, 0);
@


1.11
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 2002/01/25 15:43:59 art Exp $	*/
d883 1
a883 1
	if (OF_getprop(vmemh, "translations", (void*)prom_map, sz) <= 0) {
d963 1
a963 1
		if (OF_getprop(vmemh, "translations", (void*)prom_map, sz) <=
d1098 1
a1098 1
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void*)tsb,
d1671 1
a1671 1
		printf("pmap_pinit: segs %p == %p\n", pm->pm_segs, (void*)page->phys_addr);
d3876 1
a3876 1
	loc = (int*)va;
@


1.10
log
@allocate pmaps with pool.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 2002/01/23 00:39:47 art Exp $	*/
d70 1
a70 1
extern int64_t asmptechk __P((int64_t *pseg[], int addr)); /* DEBUG XXXXX */
d75 1
a75 1
static int pseg_check __P((struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare));
d81 2
a82 2
	extern int pseg_set __P((struct pmap*, vaddr_t addr, int64_t tte,
		paddr_t spare));
d113 3
a115 3
extern int64_t pseg_get __P((struct pmap*, vaddr_t addr));
extern int pseg_set __P((struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare));
extern paddr_t pseg_find __P((struct pmap*, vaddr_t addr, paddr_t spare));
d117 3
a119 3
static int64_t pseg_get __P((struct pmap*, vaddr_t addr));
static int pseg_set __P((struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare));
static paddr_t pseg_find __P((struct pmap*, vaddr_t addr, paddr_t spare));
d175 2
a176 2
extern struct vm_page *vm_page_alloc1 __P((void));
extern void vm_page_free1 __P((struct vm_page *));
d247 3
a249 3
extern void	pmap_remove_pv __P((struct pmap *pm, vaddr_t va, paddr_t pa));
extern void	pmap_enter_pv __P((struct pmap *pm, vaddr_t va, paddr_t pa));
extern void	pmap_page_cache __P((struct pmap *pm, paddr_t pa, int mode));
d251 2
a252 2
void	pmap_pinit __P((struct pmap *));
void	pmap_release __P((struct pmap *));
d308 1
a308 1
static int ptelookup_va __P((vaddr_t va)); /* sun4u */
d310 1
a310 1
static void tsb_enter __P((int ctx, int64_t va, int64_t data));
d386 1
a386 1
void pv_check __P((void));
d474 1
a474 1
static void pmap_enter_kpage __P((vaddr_t, int64_t));
d508 1
a508 1
void pmap_bootdebug __P((void));
d546 1
a546 1
static int pmap_calculate_colors __P((void));
d1362 1
a1362 1
		extern void main __P((void));
d2643 1
a2643 1
	register int (*dump)	__P((dev_t, daddr_t, caddr_t, size_t));
d3834 1
a3834 1
void db_dump_pv __P((db_expr_t, int, db_expr_t, char *));
d3862 1
a3862 1
void pmap_testout __P((void));
@


1.9
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2001/12/04 23:22:42 art Exp $	*/
d246 1
d1531 2
d1628 1
a1628 1
	pm = (struct pmap *)malloc(sizeof *pm, M_VMPMAP, M_WAITOK);
d1712 1
a1712 1
		free((caddr_t)pm, M_VMPMAP);
@


1.8
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 2001/11/06 19:53:16 miod Exp $	*/
d1529 1
a1529 2
	pool_init(&pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pv_entry", 0,
		  NULL, NULL, 0);
@


1.8.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 2002/01/25 15:43:59 art Exp $	*/
a245 1
static struct pool pmap_pool;
d1529 2
a1530 3
	pool_init(&pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pv_entry", NULL);
	pool_init(&pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
d1626 1
a1626 1
	pm = pool_get(&pmap_pool, PR_WAITOK);
d1710 1
a1710 1
		pool_put(&pmap_pool, pm);
@


1.8.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8.2.1 2002/01/31 22:55:25 niklas Exp $	*/
d70 1
a70 1
extern int64_t asmptechk(int64_t *pseg[], int addr); /* DEBUG XXXXX */
d75 1
a75 1
static int pseg_check(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
d81 2
a82 2
	extern int pseg_set(struct pmap*, vaddr_t addr, int64_t tte,
		paddr_t spare);
d113 3
a115 3
extern int64_t pseg_get(struct pmap*, vaddr_t addr);
extern int pseg_set(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
extern paddr_t pseg_find(struct pmap*, vaddr_t addr, paddr_t spare);
d117 3
a119 3
static int64_t pseg_get(struct pmap*, vaddr_t addr);
static int pseg_set(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
static paddr_t pseg_find(struct pmap*, vaddr_t addr, paddr_t spare);
d175 2
a176 2
extern struct vm_page *vm_page_alloc1(void);
extern void vm_page_free1(struct vm_page *);
d247 3
a249 3
extern void	pmap_remove_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
extern void	pmap_enter_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
extern void	pmap_page_cache(struct pmap *pm, paddr_t pa, int mode);
d251 2
a252 2
void	pmap_pinit(struct pmap *);
void	pmap_release(struct pmap *);
d308 1
a308 1
static int ptelookup_va(vaddr_t va); /* sun4u */
d310 1
a310 1
static void tsb_enter(int ctx, int64_t va, int64_t data);
d386 1
a386 1
void pv_check(void);
d474 1
a474 1
static void pmap_enter_kpage(vaddr_t, int64_t);
d508 1
a508 1
void pmap_bootdebug(void);
d546 1
a546 1
static int pmap_calculate_colors(void);
d883 1
a883 1
	if (OF_getprop(vmemh, "translations", (void *)prom_map, sz) <= 0) {
d963 1
a963 1
		if (OF_getprop(vmemh, "translations", (void *)prom_map, sz) <=
d1098 1
a1098 1
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void *)tsb,
d1362 1
a1362 1
		extern void main(void);
d1671 1
a1671 1
		printf("pmap_pinit: segs %p == %p\n", pm->pm_segs, (void *)page->phys_addr);
d2643 1
a2643 1
	register int (*dump)(dev_t, daddr_t, caddr_t, size_t);
d3834 1
a3834 1
void db_dump_pv(db_expr_t, int, db_expr_t, char *);
d3862 1
a3862 1
void pmap_testout(void);
d3876 1
a3876 1
	loc = (int *)va;
@


1.8.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8.2.2 2002/06/11 03:38:44 art Exp $	*/
d4 1
d87 1
a87 1
	if ((paddr_t)pm->pm_segs == spare) panic("pseg_check: pm_segs == %llx", spare);
d91 1
a91 1
				panic("pseg_check: pdir %d == %llx", i,
d96 1
a96 1
				panic("pseg_check: ptbl %d:%d == %llx", i, k,
a177 3
/* XXX - temporary workaround for pmap_{copy,zero}_page api change */
void pmap_zero_phys(paddr_t pa);
void pmap_copy_phys(paddr_t src, paddr_t dst);
d182 1
a182 1
	if (!(x)) panic("%s at line %d: assertion failed", #x, __LINE__);
d185 1
a185 1
	if (!(x)) panic("%s at line %d: assertion failed", "x", __LINE__);
d274 1
a274 2
pte_t *tsb_dmmu;
pte_t *tsb_immu;
d306 1
a306 1
paddr_t avail_start, avail_end;
d490 1
a490 1
		pmap_zero_phys(newp);
d909 1
a909 1
		panic("No kernel text segment!");
d1088 1
a1088 1
		panic("TSB alloc");
d1094 2
a1095 4
	valloc(tsb_dmmu, pte_t, TSBSIZE);
	bzero(tsb_dmmu, TSBSIZE);
	valloc(tsb_immu, pte_t, TSBSIZE);
	bzero(tsb_immu, TSBSIZE);
d1235 1
a1235 1
				pmap_zero_phys(p);
d1282 1
a1282 1
			pmap_zero_phys(newp);
d1287 1
a1287 1
		((paddr_t*)ctxbusy)[0] = pmap_kernel()->pm_physaddr;
d1348 1
a1348 1
					(prom_map[i].tte + j)|TLB_EXEC|
d1380 1
a1380 1
			pmap_zero_phys(pa);
d1504 1
a1504 1
		pmap_zero_page(m);
d1605 1
a1605 1
			pmap_zero_phys((paddr_t)pg);
d1667 1
a1667 1
		pmap_zero_phys(pm->pm_physaddr);
a1875 16
void
pmap_zero_page(struct vm_page *pg)
{

	pmap_zero_phys(VM_PAGE_TO_PHYS(pg));
}

void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

	pmap_copy_phys(src, dst);
}

a1969 5
#ifdef DIAGNOSTIC
	if (pa & (PMAP_NVC|PMAP_NC))
		panic("pmap_kenter_pa: illegal cache flags %ld", pa);
#endif

d1974 4
d1981 4
d1988 3
a1990 8
				 1, 0, 1, 0);
	/*
	 * We don't track modification on kenter mappings.
	 */
	if (prot & VM_PROT_WRITE)
		tte.data |= TLB_REAL_W|TLB_W;
	if (prot & VM_PROT_EXECUTE)
		tte.data |= TLB_EXEC;
d2013 1
a2013 1
		pmap_zero_phys((paddr_t)pg);
d2024 22
d2047 1
d2094 1
a2094 1
			panic("pmap_kremove: va=%08x in locked TLB", 
d2100 1
a2100 2
			int64_t tag;

d2118 20
a2137 16
			tag = TSB_TAG(0, pm->pm_ctx, va);
			/*
			 * Invalidate the TSB 
			 *
			 * While we can invalidate it by clearing the
			 * valid bit:
			 *
			 * ptp->data_v = 0;
			 *
			 * it's faster to do store 1 doubleword.
			 */
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = 0LL;
			}
			if (tsb_immu[i].tag == tag) {
				tsb_immu[i].data = 0LL;
d2216 1
a2216 1
		if ((flags & VM_PROT_WRITE) || (tte.data & (TLB_MODIFY)))
d2237 15
a2251 6
	if (prot & VM_PROT_WRITE)
		tte.data |= TLB_REAL_W;
	if (prot & VM_PROT_EXECUTE)
		tte.data |= TLB_EXEC;
	if (wired)
		tte.data |= TLB_TSB_LOCK;
d2278 1
a2278 1
		pmap_zero_phys((paddr_t)pg);
d2293 17
d2311 14
a2324 7
		int64_t tag = TSB_TAG(0, pm->pm_ctx, va);

		if (tsb_dmmu[i].tag == tag) {
			tsb_dmmu[i].data = 0LL;
		}
		if (tsb_immu[i].tag == tag) {
			tsb_immu[i].data = 0LL;
d2328 1
d2347 1
a2347 1
	int64_t data, tag;
d2375 1
a2375 1
			panic("pmap_remove: va=%08x in locked TLB", (u_int)va);
d2407 20
a2426 3
			tag = TSB_TAG(0, pm->pm_ctx, va);
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = 0LL;
d2428 9
a2436 2
			if (tsb_immu[i].tag == tag) {
				tsb_immu[i].data = 0LL;
d2438 1
d2473 1
a2473 1
	int64_t data, tag;
d2478 1
a2478 2
	if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) ==
	    (VM_PROT_WRITE|VM_PROT_EXECUTE))
d2522 1
a2522 2
				if (data & TLB_ACCESS)
					pv->pv_va |= PV_REF;
d2527 1
a2527 4
			if ((prot & VM_PROT_WRITE) == 0)
				data &= ~(TLB_W|TLB_REAL_W);
			if ((prot & VM_PROT_EXECUTE) == 0)
				data &= ~(TLB_EXEC);
d2537 5
a2541 9
			tag = TSB_TAG(0, pm->pm_ctx, sva);
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = data;
			}
			if (tsb_immu[i].tag == tag) {
				if ((data & TLB_EXEC) != 0)
					tsb_immu[i].data = data;
				else
					tsb_immu[i].data = 0LL;
d2590 5
a2594 7
			paddr_t npa;

			npa = ldxa((vaddr_t)&pm->pm_segs[va_to_seg(va)], ASI_PHYS_CACHED);
			printf("pmap_extract: va=%p segs[%ld]=%llx", (void *)(u_long)va, (long)va_to_seg(va), (unsigned long long)npa);
			if (npa) {
				npa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)npa)[va_to_dir(va)], ASI_PHYS_CACHED);
				printf(" segs[%ld][%ld]=%lx", (long)va_to_seg(va), (long)va_to_dir(va), (long)npa);
d2596 2
a2597 2
			if (npa) {
				npa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)npa)[va_to_pte(va)], ASI_PHYS_CACHED);
d2599 1
a2599 1
				       (long)va_to_dir(va), (long)va_to_pte(va), (long)npa);
d2601 1
a2601 1
			printf(" pseg_get: %lx\n", (long)npa);
d2869 1
d2871 3
d2881 5
a2885 11
				int64_t tag;

				i = ptelookup_va(pv->pv_va & PV_VAMASK);
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;/* data */
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;/* data */
				tlb_flush_pte((pv->pv_va & PV_VAMASK),
				    pv->pv_pmap->pm_ctx);
d2956 1
d2960 5
d2972 2
a2973 4
			    pv->pv_pmap == pmap_kernel()) {
				int64_t tag;

				i = ptelookup_va(pv->pv_va & PV_VAMASK);
d2975 4
a2978 6
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va&PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0;
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0;
d3022 1
d3044 1
d3069 1
d3088 1
a3232 1
					int64_t tag;
d3235 2
a3236 6
					tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
					    (pv->pv_va & PV_VAMASK));
					if (tsb_dmmu[i].tag == tag)
						tsb_dmmu[i].data = /* data */ 0LL;
					if (tsb_immu[i].tag == tag)
						tsb_immu[i].data = /* data */ 0LL;
a3297 2
				int64_t tag;

a3299 2
				tag = TSB_TAG(0, npv->pv_pmap->pm_ctx,
				    (npv->pv_va & PV_VAMASK));
d3301 2
a3302 4
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;			
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;			
a3346 2
				int64_t tag;

d3349 3
a3351 8
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;			
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;			
				tlb_flush_pte(pv->pv_va & PV_VAMASK,
				    pv->pv_pmap->pm_ctx);
d3445 2
a3446 4
			if (TSB_TAG_CTX(tsb_dmmu[i].tag) == cnum)
				tsb_dmmu[i].data = 0LL;
			if (TSB_TAG_CTX(tsb_immu[i].tag) == cnum)
				tsb_immu[i].data = 0LL;
a3766 1
			int64_t tag;
d3768 15
a3782 5
			tag = TSB_TAG(0, pv->pv_pmap->pm_ctx, va);
			if (tsb_dmmu[i].tag == tag)
				tsb_dmmu[i].data = 0LL; 
			if (tsb_immu[i].tag == tag)
				tsb_immu[i].data = 0LL; 
@


1.8.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d137 1
a137 1
		stxa_sync(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
d143 1
a143 1
		stxa_sync(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
d147 1
a147 1
	stxa_sync(&ptbl[va_to_pte(addr)], ASI_PHYS_CACHED, tte);
d158 1
a158 1
		stxa_sync(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
d164 1
a164 1
		stxa_sync(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
d284 1
a284 1
extern int physmem;
d1103 1
a1103 1
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void *)tsb_dmmu,
d2193 1
a2193 2
	tte.data = pseg_get(pm, va);
	if (tte.data & TLB_V) {
d3836 1
a3836 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3912 1
a3912 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3937 1
a3937 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3962 1
a3962 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
@


1.7
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 2001/11/06 03:36:57 art Exp $	*/
d3879 1
a3879 1
	pmap_update();
d3940 1
a3940 1
	pmap_update();
d3955 1
a3955 1
	pmap_update();
d3965 1
a3965 1
	pmap_update();
d3980 1
a3980 1
	pmap_update();
d4005 1
a4005 1
	pmap_update();
d4029 1
a4029 1
	pmap_update();
d4047 1
a4047 1
	pmap_update();
@


1.6
log
@Need local prototypes for pmap_pinit and pmap_release.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 2001/09/26 17:32:19 deraadt Exp $	*/
a41 1
#include <vm/vm.h>
@


1.5
log
@change *int64*_t types to long long; ok art/jason
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 2001/09/20 23:23:59 jason Exp $	*/
d250 3
@


1.5.4.1
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.5.4.2
log
@Merge in -current
@
text
@d42 1
a249 3

void	pmap_pinit __P((struct pmap *));
void	pmap_release __P((struct pmap *));
@


1.5.4.3
log
@Merge in trunk
@
text
@a245 1
static struct pool pmap_pool;
d1529 2
a1530 3
	pool_init(&pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pv_entry", NULL);
	pool_init(&pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
d1626 1
a1626 1
	pm = pool_get(&pmap_pool, PR_WAITOK);
d1710 1
a1710 1
		pool_put(&pmap_pool, pm);
d3879 1
a3879 1
	pmap_update(pmap_kernel());
d3940 1
a3940 1
	pmap_update(pmap_kernel());
d3955 1
a3955 1
	pmap_update(pmap_kernel());
d3965 1
a3965 1
	pmap_update(pmap_kernel());
d3980 1
a3980 1
	pmap_update(pmap_kernel());
d4005 1
a4005 1
	pmap_update(pmap_kernel());
d4029 1
a4029 1
	pmap_update(pmap_kernel());
d4047 1
a4047 1
	pmap_update(pmap_kernel());
@


1.5.4.4
log
@Merge in -current from about a week ago
@
text
@d70 1
a70 1
extern int64_t asmptechk(int64_t *pseg[], int addr); /* DEBUG XXXXX */
d75 1
a75 1
static int pseg_check(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
d81 2
a82 2
	extern int pseg_set(struct pmap*, vaddr_t addr, int64_t tte,
		paddr_t spare);
d113 3
a115 3
extern int64_t pseg_get(struct pmap*, vaddr_t addr);
extern int pseg_set(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
extern paddr_t pseg_find(struct pmap*, vaddr_t addr, paddr_t spare);
d117 3
a119 3
static int64_t pseg_get(struct pmap*, vaddr_t addr);
static int pseg_set(struct pmap*, vaddr_t addr, int64_t tte, paddr_t spare);
static paddr_t pseg_find(struct pmap*, vaddr_t addr, paddr_t spare);
d175 2
a176 2
extern struct vm_page *vm_page_alloc1(void);
extern void vm_page_free1(struct vm_page *);
d247 3
a249 3
extern void	pmap_remove_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
extern void	pmap_enter_pv(struct pmap *pm, vaddr_t va, paddr_t pa);
extern void	pmap_page_cache(struct pmap *pm, paddr_t pa, int mode);
d251 2
a252 2
void	pmap_pinit(struct pmap *);
void	pmap_release(struct pmap *);
d308 1
a308 1
static int ptelookup_va(vaddr_t va); /* sun4u */
d310 1
a310 1
static void tsb_enter(int ctx, int64_t va, int64_t data);
d386 1
a386 1
void pv_check(void);
d474 1
a474 1
static void pmap_enter_kpage(vaddr_t, int64_t);
d508 1
a508 1
void pmap_bootdebug(void);
d546 1
a546 1
static int pmap_calculate_colors(void);
d883 1
a883 1
	if (OF_getprop(vmemh, "translations", (void *)prom_map, sz) <= 0) {
d963 1
a963 1
		if (OF_getprop(vmemh, "translations", (void *)prom_map, sz) <=
d1098 1
a1098 1
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void *)tsb,
d1362 1
a1362 1
		extern void main(void);
d1671 1
a1671 1
		printf("pmap_pinit: segs %p == %p\n", pm->pm_segs, (void *)page->phys_addr);
d2643 1
a2643 1
	register int (*dump)(dev_t, daddr_t, caddr_t, size_t);
d3834 1
a3834 1
void db_dump_pv(db_expr_t, int, db_expr_t, char *);
d3862 1
a3862 1
void pmap_testout(void);
d3876 1
a3876 1
	loc = (int *)va;
@


1.5.4.5
log
@Sync the SMP branch with 3.3
@
text
@d4 1
d87 1
a87 1
	if ((paddr_t)pm->pm_segs == spare) panic("pseg_check: pm_segs == %llx", spare);
d91 1
a91 1
				panic("pseg_check: pdir %d == %llx", i,
d96 1
a96 1
				panic("pseg_check: ptbl %d:%d == %llx", i, k,
d138 1
a138 1
		stxa_sync(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
d144 1
a144 1
		stxa_sync(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
d148 1
a148 1
	stxa_sync(&ptbl[va_to_pte(addr)], ASI_PHYS_CACHED, tte);
d159 1
a159 1
		stxa_sync(&pm->pm_segs[va_to_seg(addr)], ASI_PHYS_CACHED, spare);
d165 1
a165 1
		stxa_sync(&pdir[va_to_dir(addr)], ASI_PHYS_CACHED, spare);
a177 3
/* XXX - temporary workaround for pmap_{copy,zero}_page api change */
void pmap_zero_phys(paddr_t pa);
void pmap_copy_phys(paddr_t src, paddr_t dst);
d182 1
a182 1
	if (!(x)) panic("%s at line %d: assertion failed", #x, __LINE__);
d185 1
a185 1
	if (!(x)) panic("%s at line %d: assertion failed", "x", __LINE__);
d274 1
a274 2
pte_t *tsb_dmmu;
pte_t *tsb_immu;
d306 1
a306 1
paddr_t avail_start, avail_end;
d490 1
a490 1
		pmap_zero_phys(newp);
d909 1
a909 1
		panic("No kernel text segment!");
d1088 1
a1088 1
		panic("TSB alloc");
d1094 2
a1095 4
	valloc(tsb_dmmu, pte_t, TSBSIZE);
	bzero(tsb_dmmu, TSBSIZE);
	valloc(tsb_immu, pte_t, TSBSIZE);
	bzero(tsb_immu, TSBSIZE);
d1098 1
a1098 1
	BDPRINTF(PDB_BOOT1, ("TSB allocated at %p size %08x\r\n", (void *)tsb_dmmu,
d1235 1
a1235 1
				pmap_zero_phys(p);
d1282 1
a1282 1
			pmap_zero_phys(newp);
d1287 1
a1287 1
		((paddr_t*)ctxbusy)[0] = pmap_kernel()->pm_physaddr;
d1348 1
a1348 1
					(prom_map[i].tte + j)|TLB_EXEC|
d1380 1
a1380 1
			pmap_zero_phys(pa);
d1504 1
a1504 1
		pmap_zero_page(m);
d1605 1
a1605 1
			pmap_zero_phys((paddr_t)pg);
d1667 1
a1667 1
		pmap_zero_phys(pm->pm_physaddr);
a1875 16
void
pmap_zero_page(struct vm_page *pg)
{

	pmap_zero_phys(VM_PAGE_TO_PHYS(pg));
}

void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

	pmap_copy_phys(src, dst);
}

a1969 5
#ifdef DIAGNOSTIC
	if (pa & (PMAP_NVC|PMAP_NC))
		panic("pmap_kenter_pa: illegal cache flags %ld", pa);
#endif

d1974 4
d1981 4
d1988 3
a1990 8
				 1, 0, 1, 0);
	/*
	 * We don't track modification on kenter mappings.
	 */
	if (prot & VM_PROT_WRITE)
		tte.data |= TLB_REAL_W|TLB_W;
	if (prot & VM_PROT_EXECUTE)
		tte.data |= TLB_EXEC;
d2013 1
a2013 1
		pmap_zero_phys((paddr_t)pg);
d2024 22
d2047 1
d2094 1
a2094 1
			panic("pmap_kremove: va=%08x in locked TLB", 
d2100 1
a2100 2
			int64_t tag;

d2118 20
a2137 16
			tag = TSB_TAG(0, pm->pm_ctx, va);
			/*
			 * Invalidate the TSB 
			 *
			 * While we can invalidate it by clearing the
			 * valid bit:
			 *
			 * ptp->data_v = 0;
			 *
			 * it's faster to do store 1 doubleword.
			 */
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = 0LL;
			}
			if (tsb_immu[i].tag == tag) {
				tsb_immu[i].data = 0LL;
d2196 1
a2196 2
	tte.data = pseg_get(pm, va);
	if (tte.data & TLB_V) {
d2216 1
a2216 1
		if ((flags & VM_PROT_WRITE) || (tte.data & (TLB_MODIFY)))
d2237 15
a2251 6
	if (prot & VM_PROT_WRITE)
		tte.data |= TLB_REAL_W;
	if (prot & VM_PROT_EXECUTE)
		tte.data |= TLB_EXEC;
	if (wired)
		tte.data |= TLB_TSB_LOCK;
d2278 1
a2278 1
		pmap_zero_phys((paddr_t)pg);
d2293 17
d2311 14
a2324 7
		int64_t tag = TSB_TAG(0, pm->pm_ctx, va);

		if (tsb_dmmu[i].tag == tag) {
			tsb_dmmu[i].data = 0LL;
		}
		if (tsb_immu[i].tag == tag) {
			tsb_immu[i].data = 0LL;
d2328 1
d2347 1
a2347 1
	int64_t data, tag;
d2375 1
a2375 1
			panic("pmap_remove: va=%08x in locked TLB", (u_int)va);
d2407 20
a2426 3
			tag = TSB_TAG(0, pm->pm_ctx, va);
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = 0LL;
d2428 9
a2436 2
			if (tsb_immu[i].tag == tag) {
				tsb_immu[i].data = 0LL;
d2438 1
d2473 1
a2473 1
	int64_t data, tag;
d2478 1
a2478 2
	if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) ==
	    (VM_PROT_WRITE|VM_PROT_EXECUTE))
d2522 1
a2522 2
				if (data & TLB_ACCESS)
					pv->pv_va |= PV_REF;
d2527 1
a2527 4
			if ((prot & VM_PROT_WRITE) == 0)
				data &= ~(TLB_W|TLB_REAL_W);
			if ((prot & VM_PROT_EXECUTE) == 0)
				data &= ~(TLB_EXEC);
d2537 5
a2541 9
			tag = TSB_TAG(0, pm->pm_ctx, sva);
			if (tsb_dmmu[i].tag == tag) {
				tsb_dmmu[i].data = data;
			}
			if (tsb_immu[i].tag == tag) {
				if ((data & TLB_EXEC) != 0)
					tsb_immu[i].data = data;
				else
					tsb_immu[i].data = 0LL;
d2590 5
a2594 7
			paddr_t npa;

			npa = ldxa((vaddr_t)&pm->pm_segs[va_to_seg(va)], ASI_PHYS_CACHED);
			printf("pmap_extract: va=%p segs[%ld]=%llx", (void *)(u_long)va, (long)va_to_seg(va), (unsigned long long)npa);
			if (npa) {
				npa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)npa)[va_to_dir(va)], ASI_PHYS_CACHED);
				printf(" segs[%ld][%ld]=%lx", (long)va_to_seg(va), (long)va_to_dir(va), (long)npa);
d2596 2
a2597 2
			if (npa) {
				npa = (paddr_t)ldxa((vaddr_t)&((paddr_t*)(u_long)npa)[va_to_pte(va)], ASI_PHYS_CACHED);
d2599 1
a2599 1
				       (long)va_to_dir(va), (long)va_to_pte(va), (long)npa);
d2601 1
a2601 1
			printf(" pseg_get: %lx\n", (long)npa);
d2869 1
d2871 3
d2881 5
a2885 11
				int64_t tag;

				i = ptelookup_va(pv->pv_va & PV_VAMASK);
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;/* data */
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;/* data */
				tlb_flush_pte((pv->pv_va & PV_VAMASK),
				    pv->pv_pmap->pm_ctx);
d2956 1
d2960 5
d2972 2
a2973 4
			    pv->pv_pmap == pmap_kernel()) {
				int64_t tag;

				i = ptelookup_va(pv->pv_va & PV_VAMASK);
d2975 4
a2978 6
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va&PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0;
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0;
d3022 1
d3044 1
d3069 1
d3088 1
a3232 1
					int64_t tag;
d3235 2
a3236 6
					tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
					    (pv->pv_va & PV_VAMASK));
					if (tsb_dmmu[i].tag == tag)
						tsb_dmmu[i].data = /* data */ 0LL;
					if (tsb_immu[i].tag == tag)
						tsb_immu[i].data = /* data */ 0LL;
a3297 2
				int64_t tag;

a3299 2
				tag = TSB_TAG(0, npv->pv_pmap->pm_ctx,
				    (npv->pv_va & PV_VAMASK));
d3301 2
a3302 4
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;			
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;			
a3346 2
				int64_t tag;

d3349 3
a3351 8
				tag = TSB_TAG(0, pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
				if (tsb_dmmu[i].tag == tag)
					tsb_dmmu[i].data = 0LL;			
				if (tsb_immu[i].tag == tag)
					tsb_immu[i].data = 0LL;			
				tlb_flush_pte(pv->pv_va & PV_VAMASK,
				    pv->pv_pmap->pm_ctx);
d3445 2
a3446 4
			if (TSB_TAG_CTX(tsb_dmmu[i].tag) == cnum)
				tsb_dmmu[i].data = 0LL;
			if (TSB_TAG_CTX(tsb_immu[i].tag) == cnum)
				tsb_immu[i].data = 0LL;
a3766 1
			int64_t tag;
d3768 15
a3782 5
			tag = TSB_TAG(0, pv->pv_pmap->pm_ctx, va);
			if (tsb_dmmu[i].tag == tag)
				tsb_dmmu[i].data = 0LL; 
			if (tsb_immu[i].tag == tag)
				tsb_immu[i].data = 0LL; 
@


1.5.4.6
log
@Sync the SMP branch to -current.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5.4.5 2003/03/27 23:42:37 niklas Exp $	*/
d284 1
a284 1
extern int physmem;
d3837 1
a3837 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3913 1
a3913 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3938 1
a3938 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
d3963 1
a3963 2
	pmap_enter(pmap_kernel(), va, pa, VM_PROT_READ|VM_PROT_WRITE,
	    VM_PROT_READ|VM_PROT_WRITE);
@


1.5.4.7
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5.4.6 2003/05/13 19:41:09 ho Exp $	*/
a280 5
/*
 * The invalid tsb tag uses the fact that the last context we have is
 * never allocated.
 */
#define TSB_TAG_INVALID (~0LL)
a311 15

static __inline void
tsb_invalidate(int ctx, vaddr_t va)
{
	int i;
	int64_t tag;

	i = ptelookup_va(va);
	tag = TSB_TAG(0, ctx, va);
	if (tsb_dmmu[i].tag == tag)
		tsb_dmmu[i].tag = TSB_TAG_INVALID;
	if (tsb_immu[i].tag == tag)
		tsb_immu[i].tag = TSB_TAG_INVALID;
}

d2074 1
a2074 1
	int s, flush = 0;
d2100 1
d2118 18
a2135 1
			tsb_invalidate(pm->pm_ctx, va);
d2169 1
a2169 1
	int s, aliased = 0;
d2281 1
d2283 1
a2283 1
		tsb_invalidate(pm->pm_ctx, va);
d2285 6
d2310 2
a2311 2
	int s, flush=0;
	int64_t data;
d2369 9
a2377 3
			if (!pm->pm_ctx && pm != pmap_kernel())
				continue;
			tsb_invalidate(pm->pm_ctx, va);
d2410 1
a2410 1
	int s;
d2412 1
a2412 1
	int64_t data;
d2479 12
a2490 3
			if (!pm->pm_ctx && pm != pmap_kernel())
				continue;
			tsb_invalidate(pm->pm_ctx, sva);
d2781 1
a2781 1
	int s;
d2827 4
a2830 1
				tsb_invalidate(pv->pv_pmap->pm_ctx,
d2832 4
d2873 1
a2873 1
	int s;
d2919 10
a2928 2
				tsb_invalidate(pv->pv_pmap->pm_ctx,
				    (pv->pv_va & PV_VAMASK));
d3111 1
a3111 1
	register int s;
d3179 4
a3182 1
					tsb_invalidate(pv->pv_pmap->pm_ctx,
d3184 4
d3249 5
a3253 1
				tsb_invalidate(npv->pv_pmap->pm_ctx,
d3255 5
d3304 5
a3308 1
				tsb_invalidate(pv->pv_pmap->pm_ctx,
d3310 4
d3396 1
a3396 5
		/*
		 * We use the last context as an "invalid" context in
		 * TSB tags. Never allocate (or bad things will happen).
		 */
		if (cnum >= numctx - 2)
d3410 1
a3410 1
				tsb_dmmu[i].tag = TSB_TAG_INVALID;
d3412 1
a3412 1
				tsb_immu[i].tag = TSB_TAG_INVALID;
d3686 1
a3686 1
	int s;
d3733 7
a3739 1
			tsb_invalidate(pv->pv_pmap->pm_ctx, va);
@


1.5.4.8
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d2649 1
a2649 1
 * Determine (non)existence of physical page
@


1.4
log
@Three changes from NetBSD:
o in pmap_clear_modify(), we need to clear the TLB_W bit (ie. the hardware write-enable bit) to re-enable modify-bit emulation even if we're doing HWREF.
o Fix locking problem.
o Change data segment size calculations so we don't need to resize if the bootloader did it for us.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 2001/09/19 20:50:57 mickey Exp $	*/
d1281 1
a1281 1
		pmap_kernel()->pm_segs=(paddr_t *)(u_long)newp;
d1664 1
a1664 1
		pm->pm_segs = (paddr_t *)(u_long)pm->pm_physaddr;
d2162 1
a2162 1
	u_int64_t pa;
@


1.3
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.2 2001/08/20 20:23:53 jason Exp $	*/
/*	$NetBSD: pmap.c,v 1.106 2001/08/09 01:01:31 eeh Exp $	*/
d710 3
a712 2
	/* 
	 * This it bogus and will be changed when the kernel is rounded to 4MB.
d716 12
d750 1
d754 1
a754 1
		psize_t szdiff = 4*MEG - (kdsize & (4*MEG-1));
d756 2
a757 4
		if (szdiff < 100*KB /* ~100KB slack */ ) {
			/* We've overflowed, or soon will, get another page */
			szdiff += 4*MEG;
		}
d788 1
a788 1
		sz = (kdsize + 4*MEG + 100*KB) & ~(4*MEG-1);
d2866 1
a2866 1
			data &= ~(TLB_MODIFY);
a3249 1
		if (firstpv->pv_pmap) simple_lock(&firstpv->pv_pmap->pm_lock);
d3318 1
@


1.2
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a42 1
#include <vm/vm_kern.h>
@


1.1
log
@Lot of stuff... Some from NetBSD, some from OpenBSD, minor modifications
@
text
@d1 1
@

