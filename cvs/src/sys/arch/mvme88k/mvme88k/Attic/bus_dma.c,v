head	1.19;
access;
symbols
	OPENBSD_5_5:1.18.0.12
	OPENBSD_5_5_BASE:1.18
	OPENBSD_5_4:1.18.0.8
	OPENBSD_5_4_BASE:1.18
	OPENBSD_5_3:1.18.0.6
	OPENBSD_5_3_BASE:1.18
	OPENBSD_5_2:1.18.0.4
	OPENBSD_5_2_BASE:1.18
	OPENBSD_5_1_BASE:1.18
	OPENBSD_5_1:1.18.0.2
	OPENBSD_5_0:1.17.0.2
	OPENBSD_5_0_BASE:1.17
	OPENBSD_4_9:1.16.0.2
	OPENBSD_4_9_BASE:1.16
	OPENBSD_4_8:1.15.0.2
	OPENBSD_4_8_BASE:1.15
	OPENBSD_4_7:1.13.0.2
	OPENBSD_4_7_BASE:1.13
	OPENBSD_4_6:1.13.0.4
	OPENBSD_4_6_BASE:1.13
	OPENBSD_4_5:1.9.0.4
	OPENBSD_4_5_BASE:1.9
	OPENBSD_4_4:1.8.0.2
	OPENBSD_4_4_BASE:1.8
	OPENBSD_4_3:1.7.0.2
	OPENBSD_4_3_BASE:1.7
	OPENBSD_4_2:1.5.0.4
	OPENBSD_4_2_BASE:1.5
	OPENBSD_4_1:1.5.0.2
	OPENBSD_4_1_BASE:1.5
	OPENBSD_4_0:1.4.0.4
	OPENBSD_4_0_BASE:1.4
	OPENBSD_3_9:1.4.0.2
	OPENBSD_3_9_BASE:1.4
	OPENBSD_3_8:1.3.0.4
	OPENBSD_3_8_BASE:1.3
	OPENBSD_3_7:1.3.0.2
	OPENBSD_3_7_BASE:1.3
	OPENBSD_3_6:1.1.0.4
	OPENBSD_3_6_BASE:1.1
	SMP_SYNC_A:1.1
	SMP_SYNC_B:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.19
date	2014.03.18.22.36.36;	author miod;	state dead;
branches;
next	1.18;

1.18
date	2011.10.09.17.08.22;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2011.06.23.20.44.39;	author ariane;	state Exp;
branches;
next	1.16;

1.16
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2010.06.26.23.24.44;	author guenther;	state Exp;
branches;
next	1.14;

1.14
date	2010.03.29.19.21.58;	author oga;	state Exp;
branches;
next	1.13;

1.13
date	2009.06.07.16.02.41;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2009.04.20.00.42.06;	author oga;	state Exp;
branches;
next	1.11;

1.11
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.10;

1.10
date	2009.03.07.15.34.34;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2009.02.01.00.52.19;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2008.06.26.05.42.12;	author ray;	state Exp;
branches;
next	1.7;

1.7
date	2007.10.06.23.12.17;	author krw;	state Exp;
branches;
next	1.6;

1.6
date	2007.09.03.01.09.09;	author krw;	state Exp;
branches;
next	1.5;

1.5
date	2007.02.11.12.49.38;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2005.11.25.22.18.18;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2004.12.25.23.02.25;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2004.11.09.19.17.01;	author claudio;	state Exp;
branches;
next	1.1;

1.1
date	2004.05.07.18.10.28;	author miod;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2004.06.05.23.09.50;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.19
log
@Retire hp300, mvme68k and mvme88k ports. These ports have no users, keeping
this hardware alive is becoming increasingly difficult, and I should heed the
message sent by the three disks which have died on me over the last few days.

Noone sane will mourn these ports anyway. So long, and thanks for the fish.
@
text
@/*      $OpenBSD: bus_dma.c,v 1.18 2011/10/09 17:08:22 miod Exp $	*/
/*      $NetBSD: bus_dma.c,v 1.2 2001/06/10 02:31:25 briggs Exp $        */

/*-
 * Copyright (c) 1996, 1997, 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/proc.h>
#include <sys/extent.h>
#include <sys/buf.h>
#include <sys/device.h>
#include <sys/systm.h>
#include <sys/conf.h>
#include <sys/file.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/mount.h>

#include <uvm/uvm_extern.h>

#include <machine/bus.h>
#include <machine/cmmu.h>
#include <machine/intr.h>

int     _bus_dmamap_load_buffer(bus_dma_tag_t, bus_dmamap_t, void *,
	    bus_size_t, struct proc *, int, paddr_t *, int *, int);

int	_bus_dmamem_alloc_range(bus_dma_tag_t, bus_size_t, bus_size_t,
	    bus_size_t, bus_dma_segment_t *, int, int *, int, paddr_t, paddr_t);

/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
int
bus_dmamap_create(t, size, nsegments, maxsegsz, boundary, flags, dmamp)
        bus_dma_tag_t t;
        bus_size_t size;
        int nsegments;
        bus_size_t maxsegsz;
        bus_size_t boundary;
        int flags;
        bus_dmamap_t *dmamp;
{
        struct m88k_bus_dmamap *map;
        void *mapstore;
        size_t mapsize;

        /*
         * Allocate and initialize the DMA map.  The end of the map
         * is a variable-sized array of segments, so we allocate enough
         * room for them in one shot.
         *
         * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
         * of ALLOCNOW notifies others that we've reserved these resources,
         * and they are not to be freed.
         *
         * The bus_dmamap_t includes one bus_dma_segment_t, hence
         * the (nsegments - 1).
         */
        mapsize = sizeof(struct m88k_bus_dmamap) +
            (sizeof(bus_dma_segment_t) * (nsegments - 1));
        if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
                return (ENOMEM);

        map = (struct m88k_bus_dmamap *)mapstore;
        map->_dm_size = size;
        map->_dm_segcnt = nsegments;
        map->_dm_maxsegsz = maxsegsz;
        map->_dm_boundary = boundary;
        map->dm_mapsize = 0;                /* no valid mappings */
        map->dm_nsegs = 0;

        *dmamp = map;
        return (0);
}

/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
void
bus_dmamap_destroy(t, map)
        bus_dma_tag_t t;
        bus_dmamap_t map;
{

        free(map, M_DEVBUF);
}

/*
 * Utility function to load a linear buffer.  lastaddrp holds state
 * between invocations (for multiple-buffer loads).  segp contains
 * the starting segment on entrance, and the ending segment on exit.
 * first indicates if this is the first invocation of this function.
 */
int
_bus_dmamap_load_buffer(t, map, buf, buflen, p, flags, lastaddrp, segp, first)
        bus_dma_tag_t t;
        bus_dmamap_t map;
        void *buf;
        bus_size_t buflen;
        struct proc *p;
        int flags;
        paddr_t *lastaddrp;
        int *segp;
        int first;
{
        bus_size_t sgsize;
        bus_addr_t curaddr, lastaddr, baddr, bmask;
        vaddr_t vaddr = (vaddr_t)buf;
        int seg;
	pmap_t pmap;

	if (p != NULL)
		pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	else
		pmap = pmap_kernel();

        lastaddr = *lastaddrp;
        bmask = ~(map->_dm_boundary - 1);

        for (seg = *segp; buflen > 0 ; ) {
                /*
                 * Get the physical address for this segment.
                 */
		if (pmap_extract(pmap, vaddr, (paddr_t *)&curaddr) == FALSE)
			return (EINVAL);

                /*
                 * Compute the segment size, and adjust counts.
                 */
                sgsize = PAGE_SIZE - ((u_long)vaddr & PGOFSET);
                if (buflen < sgsize)
                        sgsize = buflen;

                /*
                 * Make sure we don't cross any boundaries.
                 */
                if (map->_dm_boundary > 0) {
                        baddr = (curaddr + map->_dm_boundary) & bmask;
                        if (sgsize > (baddr - curaddr))
                                sgsize = (baddr - curaddr);
                }

                /*
                 * Insert chunk into a segment, coalescing with
                 * the previous segment if possible.
                 */
                if (first) {
                        map->dm_segs[seg].ds_addr = curaddr;
                        map->dm_segs[seg].ds_len = sgsize;
                        first = 0;
                } else {
                        if (curaddr == lastaddr &&
                            (map->dm_segs[seg].ds_len + sgsize) <=
                             map->_dm_maxsegsz &&
                            (map->_dm_boundary == 0 ||
                             (map->dm_segs[seg].ds_addr & bmask) ==
                             (curaddr & bmask)))
                                map->dm_segs[seg].ds_len += sgsize;
                        else {
                                if (++seg >= map->_dm_segcnt)
                                        break;
                                map->dm_segs[seg].ds_addr = curaddr;
                                map->dm_segs[seg].ds_len = sgsize;
                        }
                }

                lastaddr = curaddr + sgsize;
                vaddr += sgsize;
                buflen -= sgsize;
        }

        *segp = seg;
        *lastaddrp = lastaddr;

        /*
         * Did we fit?
         */
        if (buflen != 0)
                return (EFBIG);                /* XXX better return value here? */

        return (0);
}

/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
int
bus_dmamap_load(t, map, buf, buflen, p, flags)
        bus_dma_tag_t t;
        bus_dmamap_t map;
        void *buf;
        bus_size_t buflen;
        struct proc *p;
        int flags;
{
        paddr_t lastaddr;
        int seg, error;

        /*
         * Make sure that on error condition we return "no valid mappings".
         */
        map->dm_mapsize = 0;
        map->dm_nsegs = 0;

        if (buflen > map->_dm_size)
                return (EINVAL);

        seg = 0;
        error = _bus_dmamap_load_buffer(t, map, buf, buflen, p, flags,
                &lastaddr, &seg, 1);
        if (error == 0) {
                map->dm_mapsize = buflen;
                map->dm_nsegs = seg + 1;
        }
        return (error);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
bus_dmamap_load_mbuf(t, map, m0, flags)
        bus_dma_tag_t t;
        bus_dmamap_t map;
        struct mbuf *m0;
        int flags;
{
        paddr_t lastaddr;
        int seg, error, first;
        struct mbuf *m;

        /*
         * Make sure that on error condition we return "no valid mappings."
         */
        map->dm_mapsize = 0;
        map->dm_nsegs = 0;

#ifdef DIAGNOSTIC
        if ((m0->m_flags & M_PKTHDR) == 0)
                panic("bus_dmamap_load_mbuf: no packet header");
#endif

        if (m0->m_pkthdr.len > map->_dm_size)
                return (EINVAL);

        first = 1;
        seg = 0;
        error = 0;
        for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
                error = _bus_dmamap_load_buffer(t, map, m->m_data, m->m_len,
                    NULL, flags, &lastaddr, &seg, first);
                first = 0;
        }
        if (error == 0) {
                map->dm_mapsize = m0->m_pkthdr.len;
                map->dm_nsegs = seg + 1;
        }
        return (error);
}

/*
 * Like _bus_dmamap_load(), but for uios.
 */
int
bus_dmamap_load_uio(t, map, uio, flags)
        bus_dma_tag_t t;
        bus_dmamap_t map;
        struct uio *uio;
        int flags;
{
        paddr_t lastaddr;
        int seg, i, error, first;
        bus_size_t minlen, resid;
        struct proc *p = NULL;
        struct iovec *iov;
        caddr_t addr;

        /*
         * Make sure that on error condition we return "no valid mappings."
         */
        map->dm_mapsize = 0;
        map->dm_nsegs = 0;

        resid = uio->uio_resid;
        iov = uio->uio_iov;

	if (resid > map->_dm_size)
		return (EINVAL);

        if (uio->uio_segflg == UIO_USERSPACE) {
                p = uio->uio_procp;
#ifdef DIAGNOSTIC
                if (p == NULL)
                        panic("bus_dmamap_load_uio: USERSPACE but no proc");
#endif
        }

        first = 1;
        seg = 0;
        error = 0;
        for (i = 0; i < uio->uio_iovcnt && resid != 0 && error == 0; i++) {
                /*
                 * Now at the first iovec to load.  Load each iovec
                 * until we have exhausted the residual count.
                 */
                minlen = resid < iov[i].iov_len ? resid : iov[i].iov_len;
                addr = (caddr_t)iov[i].iov_base;

                error = _bus_dmamap_load_buffer(t, map, addr, minlen,
                    p, flags, &lastaddr, &seg, first);
                first = 0;

                resid -= minlen;
        }
        if (error == 0) {
                map->dm_mapsize = uio->uio_resid;
                map->dm_nsegs = seg + 1;
        }
        return (error);
}

/*
 * Like bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
bus_dmamap_load_raw(t, map, segs, nsegs, size, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_dma_segment_t *segs;
	int nsegs;
	bus_size_t size;
	int flags;
{
	if (nsegs > map->_dm_segcnt || size > map->_dm_size)
		return (EINVAL);

	/*
	 * Make sure we don't cross any boundaries.
	 */
	if (map->_dm_boundary) {
		bus_addr_t bmask = ~(map->_dm_boundary - 1);
		int i;

		for (i = 0; i < nsegs; i++) {
			if (segs[i].ds_len > map->_dm_maxsegsz)
				return (EINVAL);
			if ((segs[i].ds_addr & bmask) !=
			    ((segs[i].ds_addr + segs[i].ds_len - 1) & bmask))
				return (EINVAL);
		}
	}

	bcopy(segs, map->dm_segs, nsegs * sizeof(*segs));
	map->dm_nsegs = nsegs;
	return (0);
}

/*
 * Common function for unloading a DMA map.  May be called by
 * chipset-specific DMA map unload functions.
 */
void
bus_dmamap_unload(t, map)
        bus_dma_tag_t t;
        bus_dmamap_t map;
{

        /*
         * No resources to free; just mark the mappings as
         * invalid.
         */
        map->dm_mapsize = 0;
        map->dm_nsegs = 0;
}

/*
 * Common function for DMA map synchronization.  May be called
 * by chipset-specific DMA map synchronization functions.
 */

void
bus_dmamap_sync(t, map, offset, len, op)
        bus_dma_tag_t t;
        bus_dmamap_t map;
	bus_addr_t offset;
	bus_size_t len;
	int op;
{
	u_int nsegs;
	bus_dma_segment_t *seg;

	if (op & BUS_DMASYNC_PREREAD)
		op = DMA_CACHE_SYNC_INVAL;
	else if (op & BUS_DMASYNC_PREWRITE)
		op = DMA_CACHE_SYNC;
	else if (op & BUS_DMASYNC_POSTREAD)
		op = DMA_CACHE_INV;
	else
		return;

	nsegs = map->dm_nsegs;
	seg = map->dm_segs;
	while (nsegs != 0 && len != 0) {
		if (offset >= seg->ds_len) {
			offset -= seg->ds_len;
		} else {
			bus_addr_t addr;
			bus_size_t sublen;

			addr = seg->ds_addr + offset;
			sublen = seg->ds_len - offset;
			if (sublen > len)
				sublen = len;

			dma_cachectl(addr, sublen, op);

			offset = 0;
			len -= sublen;
		}
		seg++;
		nsegs--;
	}
}

/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */
int
bus_dmamem_alloc(t, size, alignment, boundary, segs, nsegs, rsegs, flags)
        bus_dma_tag_t t;
        bus_size_t size, alignment, boundary;
        bus_dma_segment_t *segs;
        int nsegs;
        int *rsegs;
        int flags;
{
        return _bus_dmamem_alloc_range(t, size, alignment, boundary, segs,
            nsegs, rsegs, flags, 0, -1);
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
bus_dmamem_free(t, segs, nsegs)
        bus_dma_tag_t t;
        bus_dma_segment_t *segs;
        int nsegs;
{
        struct vm_page *m;
        bus_addr_t addr;
        struct pglist mlist;
        int curseg;

        /*
         * Build a list of pages to free back to the VM system.
         */
        TAILQ_INIT(&mlist);
        for (curseg = 0; curseg < nsegs; curseg++) {
                for (addr = segs[curseg].ds_addr;
                    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
                    addr += PAGE_SIZE) {
                        m = PHYS_TO_VM_PAGE(addr);
                        TAILQ_INSERT_TAIL(&mlist, m, pageq);
                }
        }

        uvm_pglistfree(&mlist);
}

/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
int
bus_dmamem_map(t, segs, nsegs, size, kvap, flags)
        bus_dma_tag_t t;
        bus_dma_segment_t *segs;
        int nsegs;
        size_t size;
        caddr_t *kvap;
        int flags;
{
        vaddr_t va, sva;
        size_t ssize;
        bus_addr_t addr;
        int curseg, error;

        size = round_page(size);

        va = uvm_km_valloc(kernel_map, size);

        if (va == 0)
                return (ENOMEM);

        *kvap = (caddr_t)va;

	sva = va;
	ssize = size;
        for (curseg = 0; curseg < nsegs; curseg++) {
                for (addr = segs[curseg].ds_addr;
                    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
                    addr += PAGE_SIZE, va += PAGE_SIZE, size -= PAGE_SIZE) {
                        if (size == 0)
                                panic("bus_dmamem_map: size botch");
                        error = pmap_enter(pmap_kernel(), va, addr,
                            VM_PROT_READ | VM_PROT_WRITE, VM_PROT_READ |
                            VM_PROT_WRITE | PMAP_WIRED | PMAP_CANFAIL);
                        if (error) {
                               pmap_update(pmap_kernel());
                               uvm_km_free(kernel_map, sva, ssize);
                               return (error);
                        }
			if (flags & BUS_DMA_COHERENT)
				pmap_page_uncache(addr);
                }
        }
	pmap_update(pmap_kernel());

        return (0);
}

/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
void
bus_dmamem_unmap(t, kva, size)
        bus_dma_tag_t t;
        caddr_t kva;
        size_t size;
{

#ifdef DIAGNOSTIC
        if ((u_long)kva & PGOFSET)
                panic("bus_dmamem_unmap");
#endif

        size = round_page(size);
        uvm_km_free(kernel_map, (vaddr_t)kva, size);
}

/*
 * Common function for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
paddr_t
bus_dmamem_mmap(t, segs, nsegs, off, prot, flags)
        bus_dma_tag_t t;
        bus_dma_segment_t *segs;
        int nsegs;
        off_t off;
        int prot, flags;
{
        int i;

        for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
                if (off & PGOFSET)
                        panic("bus_dmamem_mmap: offset unaligned");
                if (segs[i].ds_addr & PGOFSET)
                        panic("bus_dmamem_mmap: segment unaligned");
                if (segs[i].ds_len & PGOFSET)
                        panic("bus_dmamem_mmap: segment size not multiple"
                            " of page size");
#endif
                if (off >= segs[i].ds_len) {
                        off -= segs[i].ds_len;
                        continue;
                }

                return (segs[i].ds_addr + off);
        }

        /* Page not found. */
        return (-1);
}

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
int
_bus_dmamem_alloc_range(t, size, alignment, boundary, segs, nsegs, rsegs,
    flags, low, high)
        bus_dma_tag_t t;
        bus_size_t size, alignment, boundary;
        bus_dma_segment_t *segs;
        int nsegs;
        int *rsegs;
        int flags;
        paddr_t low;
        paddr_t high;
{
        paddr_t curaddr, lastaddr;
        struct vm_page *m;
        struct pglist mlist;
        int curseg, error, plaflag;

        /* Always round the size. */
        size = round_page(size);

        /*
         * Allocate pages from the VM system.
         */
	plaflag = flags & BUS_DMA_NOWAIT ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & BUS_DMA_ZERO)
		plaflag |= UVM_PLA_ZERO;

        TAILQ_INIT(&mlist);
        error = uvm_pglistalloc(size, low, high, alignment, boundary,
            &mlist, nsegs, plaflag);
        if (error)
                return (error);

        /*
         * Compute the location, size, and number of segments actually
         * returned by the VM code.
         */
        m = TAILQ_FIRST(&mlist);
        curseg = 0;
        lastaddr = segs[curseg].ds_addr = VM_PAGE_TO_PHYS(m);
        segs[curseg].ds_len = PAGE_SIZE;
	m = TAILQ_NEXT(m, pageq);

	for (; m != TAILQ_END(&mlist); m = TAILQ_NEXT(m, pageq)) {
                curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
                if (curaddr < low || curaddr >= high) {
                        panic("_bus_dmamem_alloc_range: uvm_pglistalloc "
			    "returned non-sensical address 0x%lx\n", curaddr);
                }
#endif
                if (curaddr == (lastaddr + PAGE_SIZE))
                        segs[curseg].ds_len += PAGE_SIZE;
                else {
                        curseg++;
                        segs[curseg].ds_addr = curaddr;
                        segs[curseg].ds_len = PAGE_SIZE;
                }
                lastaddr = curaddr;
        }

        *rsegs = curseg + 1;

        return (0);
}
@


1.18
log
@Let BUS_DMA_COHERENT allocations return cache-inhibited pages.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.17 2011/06/23 20:44:39 ariane Exp $	*/
@


1.17
log
@Fix the error path in bus_dmamem_map.
As discussed on icb: remove the comment,
remove pmap_remove (uvm_km_free does that for us).

ok oga@@, deraadt@@
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.16 2010/12/26 15:40:59 miod Exp $	*/
d551 2
@


1.16
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.15 2010/06/26 23:24:44 guenther Exp $	*/
a546 4
                               /*
                                * Clean up after ourselves.
                                * XXX uvm_wait on WAITOK
                                */
d548 1
a548 1
                               uvm_km_free(kernel_map, va, ssize);
@


1.15
log
@Don't #include <sys/user.h> into files that don't need the stuff
it defines.  In some cases, this means pulling in uvm.h or pcb.h
instead, but most of the inclusions were just noise.  Tested on
alpha, amd64, armish, hppa, i386, macpcc, sgi, sparc64, and vax,
mostly by krw and naddy.
ok krw@@
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.14 2010/03/29 19:21:58 oga Exp $	*/
d611 1
a611 1
                return (atop(segs[i].ds_addr + off));
@


1.14
log
@PMAP_CANFAIL for bus_dmamem_map on all other architectures (and some
whitespace tweaks on i386 so that it matches).

ok kettenis@@
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.13 2009/06/07 16:02:41 miod Exp $	*/
a35 1
#include <sys/user.h>
@


1.13
log
@Do not look at the avail_{start,end} members from vm_physmem[] anymore.
These values were used to eventually pass ranges to uvm_pglistalloc(),
which has been fixed to correctly skip no-memory ranges a lot of time ago;
however mvme68k would still use the computed range and osiop would no
longer attach; this repairs it.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.12 2009/04/20 00:42:06 oga Exp $	*/
d522 2
a523 1
        vaddr_t va;
d525 1
a525 1
        int curseg;
d536 2
d544 12
a555 3
                        pmap_enter(pmap_kernel(), va, addr,
                            VM_PROT_READ | VM_PROT_WRITE,
                            VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
@


1.12
log
@Add a BUS_DMA_ZERO flag for bus_dmamem_alloc() to return zeroed memory.

Saves every damned driver calling bzero(), and continues the M_ZERO,
PR_ZERO symmetry.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.11 2009/04/14 16:01:04 oga Exp $	*/
a473 10
        paddr_t avail_start = (paddr_t)-1, avail_end = 0;
        int bank;

        for (bank = 0; bank < vm_nphysseg; bank++) {
                if (avail_start > vm_physmem[bank].avail_start << PGSHIFT)
                        avail_start = vm_physmem[bank].avail_start << PGSHIFT;
                if (avail_end < vm_physmem[bank].avail_end << PGSHIFT)
                        avail_end = vm_physmem[bank].avail_end << PGSHIFT;
        }

@


1.11
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.10 2009/03/07 15:34:34 miod Exp $	*/
d645 2
@


1.10
log
@When allocating memory in bus_dmamem_alloc() with uvm_pglistalloc(), do not
try to be smart for the address range, uvm_pglistalloc() is smart enough
nowadays.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.9 2009/02/01 00:52:19 miod Exp $	*/
d636 1
a636 1
        int curseg, error;
d644 2
d648 1
a648 1
            &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.9
log
@Remove dma_cachectl() and rename dma_cachectl_pa() to dma_cachectl() now that
the old vs(4) code is gone.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.8 2008/06/26 05:42:12 ray Exp $	*/
d485 1
a485 1
            nsegs, rsegs, flags, avail_start, avail_end - PAGE_SIZE);
@


1.8
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.7 2007/10/06 23:12:17 krw Exp $	*/
d451 1
a451 1
			dma_cachectl_pa(addr, sublen, op);
@


1.7
log
@Some archs used memset() rather than bzero(). So duplicate diff
previously applied to other archs deleting a memset() this time. e.g.

-	if ((mapstore = malloc(mapsize, M_DEVBUF,
-	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
+	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
+	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
 		return (ENOMEM);

-	memset(mapstore, 0, mapsize);
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.6 2007/09/03 01:09:09 krw Exp $	*/
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by the NetBSD
 *      Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.6
log
@Typos from miod. 'functin' -> 'functin' in some comments.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.5 2007/02/11 12:49:38 miod Exp $	*/
d98 2
a99 2
        if ((mapstore = malloc(mapsize, M_DEVBUF,
            (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
a101 1
        memset(mapstore, 0, mapsize);
@


1.5
log
@Rework the cache handling routines again. We now try to operate on the exact
address range we've been given, rounded to cache line boundaries, instead
of being lazy and operating on pages as soon as the range was large enough.

Also, since the ranges we'll be invoked for are reasonably small, it does
not make sense to check for segment sizes - we're always smaller, really.

While there, hardcode the size in cmmu_flush_data_cache(), which becomes
cmmu_flush_data_page(), since it was always invoked for complete pages.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.4 2005/11/25 22:18:18 miod Exp $	*/
d590 1
a590 1
 * Common functin for mmap(2)'ing DMA-safe memory.  May be called by
@


1.4
log
@Replace utterly wrong bus_dmamap_sync() with a much better version.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.3 2004/12/25 23:02:25 miod Exp $	*/
d436 3
a438 2
	switch (op) {
	case BUS_DMASYNC_PREWRITE:
d440 3
a442 6
		break;
	case BUS_DMASYNC_PREREAD:
	case BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE:
		op = DMA_CACHE_SYNC_INVAL;
		break;
	default:
a443 1
	}
d459 1
a459 2
			if (dma_cachectl_pa(addr, sublen, op) != 0)
				break;
@


1.3
log
@Use list and queue macros where applicable to make the code easier to read;
no functional change.
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.2 2004/11/09 19:17:01 claudio Exp $	*/
d433 2
a434 1
	int i;
d448 23
a470 2
	for (i = map->dm_nsegs; i--; )
		dma_cachectl_pa(map->dm_segs[i].ds_addr, len, op);
@


1.2
log
@Do not map empty mbufs (m_len == 0) in bus_dmamap_load_mbuf() as these mappings
may disturb the dma as seen in ipw(4). Emtpy mbufs are at the beginning of the
mbuf chain and are as example a "side-effect" of a previous m_adj() call.
OK miod@@ mickey@@ jason@@ markus@@
@
text
@d1 1
a1 1
/*      $OpenBSD: bus_dma.c,v 1.1 2004/05/07 18:10:28 miod Exp $	*/
d644 1
a644 1
        m = mlist.tqh_first;
d648 1
a648 1
        m = m->pageq.tqe_next;
d650 1
a650 1
        for (; m != NULL; m = m->pageq.tqe_next) {
@


1.1
log
@bus_dma(9) implementation for mvme88k, mostly based upon powerpc.
@
text
@d1 1
a1 1
/*      $OpenBSD$	*/
d291 2
@


1.1.2.1
log
@Merge with the trunk
@
text
@@

