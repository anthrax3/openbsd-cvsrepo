head	1.118;
access;
symbols
	SMP_SYNC_A:1.115
	SMP_SYNC_B:1.115
	OPENBSD_3_5:1.110.0.2
	OPENBSD_3_5_BASE:1.110
	OPENBSD_3_4:1.74.0.2
	OPENBSD_3_4_BASE:1.74
	UBC_SYNC_A:1.68
	OPENBSD_3_3:1.68.0.2
	OPENBSD_3_3_BASE:1.68
	OPENBSD_3_2:1.67.0.2
	OPENBSD_3_2_BASE:1.67
	OPENBSD_3_1:1.65.0.2
	OPENBSD_3_1_BASE:1.65
	UBC_SYNC_B:1.67
	UBC:1.55.0.2
	UBC_BASE:1.55
	OPENBSD_3_0:1.41.0.2
	OPENBSD_3_0_BASE:1.41
	OPENBSD_2_9:1.25.0.2
	OPENBSD_2_9_BASE:1.25
	OPENBSD_2_8:1.13.0.4
	OPENBSD_2_8_BASE:1.13
	OPENBSD_2_7:1.13.0.2
	OPENBSD_2_7_BASE:1.13
	SMP:1.12.0.4
	SMP_BASE:1.12
	kame_19991208:1.12
	OPENBSD_2_6:1.12.0.2
	OPENBSD_2_6_BASE:1.12
	OPENBSD_2_5:1.6.0.2
	OPENBSD_2_5_BASE:1.6
	OPENBSD_2_4:1.3.0.8
	OPENBSD_2_4_BASE:1.3
	OPENBSD_2_3:1.3.0.6
	OPENBSD_2_3_BASE:1.3
	OPENBSD_2_2:1.3.0.4
	OPENBSD_2_2_BASE:1.3
	OPENBSD_2_1:1.3.0.2
	OPENBSD_2_1_BASE:1.3
	mvme88kport:1.1.1.1
	OPENBSD_2_0:1.1.0.2
	OPENBSD_2_0_BASE:1.1;
locks; strict;
comment	@ * @;


1.118
date	2004.07.25.11.06.43;	author miod;	state dead;
branches;
next	1.117;

1.117
date	2004.06.24.19.59.14;	author miod;	state Exp;
branches;
next	1.116;

1.116
date	2004.06.22.05.04.59;	author miod;	state Exp;
branches;
next	1.115;

1.115
date	2004.05.23.21.09.50;	author miod;	state Exp;
branches;
next	1.114;

1.114
date	2004.05.07.15.30.04;	author miod;	state Exp;
branches;
next	1.113;

1.113
date	2004.05.07.08.00.16;	author miod;	state Exp;
branches;
next	1.112;

1.112
date	2004.04.24.20.35.27;	author miod;	state Exp;
branches;
next	1.111;

1.111
date	2004.04.14.13.43.48;	author miod;	state Exp;
branches;
next	1.110;

1.110
date	2004.01.28.13.04.57;	author miod;	state Exp;
branches;
next	1.109;

1.109
date	2004.01.15.23.36.08;	author miod;	state Exp;
branches;
next	1.108;

1.108
date	2004.01.09.00.31.01;	author miod;	state Exp;
branches;
next	1.107;

1.107
date	2004.01.08.20.31.44;	author miod;	state Exp;
branches;
next	1.106;

1.106
date	2004.01.07.23.43.54;	author miod;	state Exp;
branches;
next	1.105;

1.105
date	2004.01.05.20.07.03;	author miod;	state Exp;
branches;
next	1.104;

1.104
date	2004.01.04.22.51.55;	author miod;	state Exp;
branches;
next	1.103;

1.103
date	2004.01.02.23.25.18;	author miod;	state Exp;
branches;
next	1.102;

1.102
date	2004.01.02.17.14.40;	author miod;	state Exp;
branches;
next	1.101;

1.101
date	2004.01.02.17.08.58;	author miod;	state Exp;
branches;
next	1.100;

1.100
date	2003.12.30.06.45.55;	author miod;	state Exp;
branches;
next	1.99;

1.99
date	2003.12.28.14.10.58;	author miod;	state Exp;
branches;
next	1.98;

1.98
date	2003.12.23.00.53.17;	author miod;	state Exp;
branches;
next	1.97;

1.97
date	2003.12.19.22.30.18;	author miod;	state Exp;
branches;
next	1.96;

1.96
date	2003.12.19.21.25.03;	author miod;	state Exp;
branches;
next	1.95;

1.95
date	2003.12.19.18.08.23;	author miod;	state Exp;
branches;
next	1.94;

1.94
date	2003.12.14.22.08.02;	author miod;	state Exp;
branches;
next	1.93;

1.93
date	2003.12.14.22.06.39;	author miod;	state Exp;
branches;
next	1.92;

1.92
date	2003.11.03.06.54.26;	author david;	state Exp;
branches;
next	1.91;

1.91
date	2003.10.28.21.43.44;	author miod;	state Exp;
branches;
next	1.90;

1.90
date	2003.10.28.17.33.01;	author miod;	state Exp;
branches;
next	1.89;

1.89
date	2003.10.24.17.44.51;	author miod;	state Exp;
branches;
next	1.88;

1.88
date	2003.10.19.18.12.00;	author miod;	state Exp;
branches;
next	1.87;

1.87
date	2003.10.16.23.04.09;	author miod;	state Exp;
branches;
next	1.86;

1.86
date	2003.10.13.18.45.17;	author miod;	state Exp;
branches;
next	1.85;

1.85
date	2003.10.11.23.54.17;	author miod;	state Exp;
branches;
next	1.84;

1.84
date	2003.10.11.22.08.35;	author miod;	state Exp;
branches;
next	1.83;

1.83
date	2003.10.10.14.25.42;	author miod;	state Exp;
branches;
next	1.82;

1.82
date	2003.10.06.14.59.29;	author miod;	state Exp;
branches;
next	1.81;

1.81
date	2003.10.06.06.31.28;	author miod;	state Exp;
branches;
next	1.80;

1.80
date	2003.09.29.20.29.04;	author miod;	state Exp;
branches;
next	1.79;

1.79
date	2003.09.29.13.05.56;	author miod;	state Exp;
branches;
next	1.78;

1.78
date	2003.09.27.13.05.30;	author miod;	state Exp;
branches;
next	1.77;

1.77
date	2003.09.26.22.27.26;	author miod;	state Exp;
branches;
next	1.76;

1.76
date	2003.09.19.23.12.22;	author miod;	state Exp;
branches;
next	1.75;

1.75
date	2003.09.16.20.52.22;	author miod;	state Exp;
branches;
next	1.74;

1.74
date	2003.08.20.19.35.50;	author miod;	state Exp;
branches;
next	1.73;

1.73
date	2003.08.20.19.29.12;	author miod;	state Exp;
branches;
next	1.72;

1.72
date	2003.08.08.21.36.33;	author miod;	state Exp;
branches;
next	1.71;

1.71
date	2003.08.01.23.15.31;	author miod;	state Exp;
branches;
next	1.70;

1.70
date	2003.08.01.18.39.12;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2003.08.01.07.48.24;	author miod;	state Exp;
branches;
next	1.68;

1.68
date	2003.01.24.09.57.44;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.66;

1.66
date	2002.05.07.00.54.37;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2002.03.14.03.15.57;	author millert;	state Exp;
branches;
next	1.64;

1.64
date	2002.03.14.01.26.40;	author millert;	state Exp;
branches;
next	1.63;

1.63
date	2002.02.05.23.07.38;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2001.12.27.22.33.46;	author miod;	state Exp;
branches;
next	1.60;

1.60
date	2001.12.24.04.12.40;	author miod;	state Exp;
branches;
next	1.59;

1.59
date	2001.12.24.00.25.17;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2001.12.22.10.22.13;	author smurph;	state Exp;
branches;
next	1.57;

1.57
date	2001.12.22.09.49.39;	author smurph;	state Exp;
branches;
next	1.56;

1.56
date	2001.12.22.07.35.43;	author smurph;	state Exp;
branches;
next	1.55;

1.55
date	2001.12.19.07.04.42;	author smurph;	state Exp;
branches
	1.55.2.1;
next	1.54;

1.54
date	2001.12.16.23.49.47;	author miod;	state Exp;
branches;
next	1.53;

1.53
date	2001.12.13.08.55.52;	author smurph;	state Exp;
branches;
next	1.52;

1.52
date	2001.12.12.19.34.23;	author miod;	state Exp;
branches;
next	1.51;

1.51
date	2001.12.09.01.13.17;	author miod;	state Exp;
branches;
next	1.50;

1.50
date	2001.12.08.21.41.34;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2001.12.08.20.08.27;	author miod;	state Exp;
branches;
next	1.48;

1.48
date	2001.11.28.15.34.16;	author art;	state Exp;
branches;
next	1.47;

1.47
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.46;

1.46
date	2001.11.28.13.47.38;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2001.11.27.05.39.02;	author miod;	state Exp;
branches;
next	1.44;

1.44
date	2001.11.20.19.54.28;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2001.11.07.22.32.29;	author miod;	state Exp;
branches;
next	1.42;

1.42
date	2001.11.06.19.53.15;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2001.09.19.20.50.57;	author mickey;	state Exp;
branches;
next	1.40;

1.40
date	2001.08.26.14.31.12;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2001.08.12.19.28.38;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2001.08.12.00.17.45;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2001.07.25.13.25.32;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2001.07.22.19.58.17;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2001.07.18.10.47.04;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.07.05.07.20.45;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.06.30.12.14.43;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2001.06.27.06.19.51;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.06.27.04.29.20;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.06.16.22.31.50;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2001.06.14.21.30.46;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2001.06.08.08.09.14;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.05.16.12.49.47;	author ho;	state Exp;
branches;
next	1.26;

1.26
date	2001.05.09.15.31.25;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.03.22.01.15.35;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2001.03.14.22.02.16;	author deraadt;	state Exp;
branches;
next	1.23;

1.23
date	2001.03.09.05.44.42;	author smurph;	state Exp;
branches;
next	1.22;

1.22
date	2001.03.08.22.26.00;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2001.03.07.23.52.33;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2001.02.12.08.16.25;	author smurph;	state Exp;
branches;
next	1.19;

1.19
date	2001.02.01.03.38.21;	author smurph;	state Exp;
branches;
next	1.18;

1.18
date	2001.01.14.20.25.25;	author smurph;	state Exp;
branches;
next	1.17;

1.17
date	2001.01.13.05.18.59;	author smurph;	state Exp;
branches;
next	1.16;

1.16
date	2001.01.12.07.29.26;	author smurph;	state Exp;
branches;
next	1.15;

1.15
date	2000.12.28.21.21.24;	author smurph;	state Exp;
branches;
next	1.14;

1.14
date	2000.12.21.16.54.56;	author aaron;	state Exp;
branches;
next	1.13;

1.13
date	2000.02.22.19.27.56;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	99.09.27.19.13.23;	author smurph;	state Exp;
branches
	1.12.4.1;
next	1.11;

1.11
date	99.09.03.18.01.31;	author art;	state Exp;
branches;
next	1.10;

1.10
date	99.07.18.18.00.07;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	99.07.18.16.45.55;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	99.07.18.16.23.47;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	99.05.29.04.41.46;	author smurph;	state Exp;
branches;
next	1.6;

1.6
date	99.02.09.06.36.30;	author smurph;	state Exp;
branches;
next	1.5;

1.5
date	99.01.11.05.11.44;	author millert;	state Exp;
branches;
next	1.4;

1.4
date	98.12.15.05.11.02;	author smurph;	state Exp;
branches;
next	1.3;

1.3
date	97.03.03.20.21.45;	author rahnds;	state Exp;
branches;
next	1.2;

1.2
date	97.03.03.19.08.12;	author rahnds;	state dead;
branches;
next	1.1;

1.1
date	95.10.18.12.32.30;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	97.03.03.19.32.13;	author rahnds;	state Exp;
branches;
next	;

1.12.4.1
date	2000.03.02.07.04.31;	author niklas;	state Exp;
branches;
next	1.12.4.2;

1.12.4.2
date	2001.04.18.16.11.38;	author niklas;	state Exp;
branches;
next	1.12.4.3;

1.12.4.3
date	2001.07.04.10.20.19;	author niklas;	state Exp;
branches;
next	1.12.4.4;

1.12.4.4
date	2001.10.31.03.01.19;	author nate;	state Exp;
branches;
next	1.12.4.5;

1.12.4.5
date	2001.11.13.21.04.15;	author niklas;	state Exp;
branches;
next	1.12.4.6;

1.12.4.6
date	2001.12.05.00.39.12;	author niklas;	state Exp;
branches;
next	1.12.4.7;

1.12.4.7
date	2002.03.06.02.04.45;	author niklas;	state Exp;
branches;
next	1.12.4.8;

1.12.4.8
date	2002.03.28.10.36.02;	author niklas;	state Exp;
branches;
next	1.12.4.9;

1.12.4.9
date	2003.03.27.23.32.18;	author niklas;	state Exp;
branches;
next	1.12.4.10;

1.12.4.10
date	2004.02.19.10.49.08;	author niklas;	state Exp;
branches;
next	1.12.4.11;

1.12.4.11
date	2004.06.05.23.09.51;	author niklas;	state Exp;
branches;
next	;

1.55.2.1
date	2002.01.31.22.55.19;	author niklas;	state Exp;
branches;
next	1.55.2.2;

1.55.2.2
date	2002.06.11.03.37.11;	author art;	state Exp;
branches;
next	1.55.2.3;

1.55.2.3
date	2002.10.29.00.28.07;	author art;	state Exp;
branches;
next	1.55.2.4;

1.55.2.4
date	2003.05.19.21.45.53;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.118
log
@Merge luna88k and mvme88k pmap. The pmap will now assume that the memory
below the kernel text is reserved for the PROM, instead of using fixed
(but different) values between luna88k and mvme88k.

Tested on mvme88k by myself, on luna88k by aoyama@@
@
text
@/*	$OpenBSD: pmap.c,v 1.117 2004/06/24 19:59:14 miod Exp $	*/
/*
 * Copyright (c) 2001, 2002, 2003 Miodrag Vallat
 * Copyright (c) 1998-2001 Steve Murphree, Jr.
 * Copyright (c) 1996 Nivas Madhur
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Nivas Madhur.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 */
/*
 * Mach Operating System
 * Copyright (c) 1991 Carnegie Mellon University
 * Copyright (c) 1991 OMRON Corporation
 * All Rights Reserved.
 *
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 */

#include <sys/types.h>
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/simplelock.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/msgbuf.h>
#include <sys/user.h>

#include <uvm/uvm.h>

#include <machine/asm_macro.h>
#include <machine/board.h>
#include <machine/cmmu.h>
#include <machine/cpu_number.h>
#include <machine/pmap_table.h>

/*
 * VM externals
 */
extern vaddr_t	avail_start;
extern vaddr_t	virtual_avail, virtual_end;
extern vaddr_t	last_addr;

/*
 * Macros to operate pm_cpus field
 */
#define SETBIT_CPUSET(cpu_number, cpuset) (*(cpuset)) |= (1 << (cpu_number));
#define CLRBIT_CPUSET(cpu_number, cpuset) (*(cpuset)) &= ~(1 << (cpu_number));

#ifdef	DEBUG
/*
 * Static variables, functions and variables for debugging
 */

/*
 * conditional debugging
 */
#define CD_FULL		0x02

#define CD_ACTIVATE	0x0000004	/* pmap_activate */
#define CD_KMAP		0x0000008	/* pmap_expand_kmap */
#define CD_MAP		0x0000010	/* pmap_map */
#define CD_CACHE	0x0000020	/* pmap_cache_ctrl */
#define CD_INIT		0x0000080	/* pmap_init */
#define CD_CREAT	0x0000100	/* pmap_create */
#define CD_FREE		0x0000200	/* pmap_release */
#define CD_DESTR	0x0000400	/* pmap_destroy */
#define CD_RM		0x0000800	/* pmap_remove */
#define CD_RMAL		0x0001000	/* pmap_remove_all */
#define CD_PROT		0x0002000	/* pmap_protect */
#define CD_EXP		0x0004000	/* pmap_expand */
#define CD_ENT		0x0008000	/* pmap_enter */
#define CD_UPD		0x0010000	/* pmap_update */
#define CD_COL		0x0020000	/* pmap_collect */
#define CD_CBIT		0x0040000	/* pmap_changebit */
#define CD_TBIT		0x0080000	/* pmap_testbit */
#define CD_USBIT	0x0100000	/* pmap_unsetbit */
#define CD_PGMV		0x0200000	/* pagemove */
#define CD_ALL		0x0FFFFFC

int pmap_con_dbg = 0;

/*
 * Alignment checks for pages (must lie on page boundaries).
 */
#define PAGE_ALIGNED(ad)	(((vaddr_t)(ad) & PAGE_MASK) == 0)
#define	CHECK_PAGE_ALIGN(ad, who) \
	if (!PAGE_ALIGNED(ad)) \
		printf("%s: addr %x not page aligned.\n", who, ad)

#else	/* DEBUG */

#define	CHECK_PAGE_ALIGN(ad, who)

#endif	/* DEBUG */

struct pool pmappool, pvpool;

caddr_t vmmap;
pt_entry_t *vmpte, *msgbufmap;

struct pmap kernel_pmap_store;
pmap_t kernel_pmap = &kernel_pmap_store;

typedef struct kpdt_entry *kpdt_entry_t;
struct kpdt_entry {
	kpdt_entry_t	next;
	paddr_t		phys;
};
#define	KPDT_ENTRY_NULL		((kpdt_entry_t)0)

kpdt_entry_t	kpdt_free;

/*
 * Two pages of scratch space per cpu.
 * Used in pmap_copy_page() and pmap_zero_page().
 */
vaddr_t phys_map_vaddr, phys_map_vaddr_end;

#define PV_ENTRY_NULL	((pv_entry_t) 0)

static pv_entry_t pg_to_pvh(struct vm_page *);

static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}

/*
 *	Locking primitives
 */

/*
 *	We raise the interrupt level to splvm, to block interprocessor
 *	interrupts during pmap operations.
 */
#define	SPLVM(spl)	spl = splvm()
#define	SPLX(spl)	splx(spl)

#define PMAP_LOCK(pmap,spl) \
	do { \
		SPLVM(spl); \
		simple_lock(&(pmap)->pm_lock); \
	} while (0)
#define PMAP_UNLOCK(pmap, spl) \
	do { \
		simple_unlock(&(pmap)->pm_lock); \
		SPLX(spl); \
	} while (0)

#define ETHERPAGES 16
void *etherbuf = NULL;
int etherlen;

#ifdef	PMAP_USE_BATC

/*
 * number of BATC entries used
 */
int batc_used;

/*
 * keep track BATC mapping
 */
batc_entry_t batc_entry[BATC_MAX];

#endif	/* PMAP_USE_BATC */

vaddr_t kmapva = 0;

/*
 * Internal routines
 */
static void flush_atc_entry(long, vaddr_t, boolean_t);
pt_entry_t *pmap_expand_kmap(vaddr_t, vm_prot_t);
void pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *);
void pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
void pmap_expand(pmap_t, vaddr_t);
void pmap_release(pmap_t);
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, u_int);
pt_entry_t *pmap_pte(pmap_t, vaddr_t);
void pmap_remove_all(struct vm_page *);
void pmap_changebit(struct vm_page *, int, int);
boolean_t pmap_unsetbit(struct vm_page *, int);
boolean_t pmap_testbit(struct vm_page *, int);

/*
 * quick PTE field checking macros
 */
#define	pmap_pte_w(pte)		(*(pte) & PG_W)
#define	pmap_pte_prot(pte)	(*(pte) & PG_PROT)

#define	pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define	pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

#define	m88k_protection(prot)	((prot) & VM_PROT_WRITE ? PG_RW : PG_RO)

/*
 * Routine:	FLUSH_ATC_ENTRY
 *
 * Function:
 *	Flush atc(TLB) which maps given virtual address, in the CPUs which
 *	are specified by 'users', for the operating mode specified by
 *	'kernel'.
 *
 * Parameters:
 *	users	bit patterns of the CPUs which may hold the TLB, and
 *		should be flushed
 *	va	virtual address that should be flushed
 *	kernel	TRUE if supervisor mode, FALSE if user mode
 */
static __inline__ void
flush_atc_entry(long users, vaddr_t va, boolean_t kernel)
{
#if NCPUS > 1
	int cpu;

	if (users == 0)
		return;

#ifdef DEBUG
	if (ff1(users) >= MAX_CPUS) {
		panic("flush_atc_entry: invalid ff1 users = %d", ff1(users));
	}
#endif

	while ((cpu = ff1(users)) != 32) {
		if (cpu_sets[cpu]) { /* just checking to make sure */
			cmmu_flush_tlb(cpu, kernel, va, PAGE_SIZE);
		}
		users &= ~(1 << cpu);
	}
#else
	if (users != 0)
		cmmu_flush_tlb(cpu_number(), kernel, va, PAGE_SIZE);
#endif
}

/*
 * Routine:	PMAP_PTE
 *
 * Function:
 *	Given a map and a virtual address, compute a (virtual) pointer
 *	to the page table entry (PTE) which maps the address .
 *	If the page table associated with the address does not
 *	exist, PT_ENTRY_NULL is returned (and the map may need to grow).
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	virt	virtual address for which page table entry is desired
 *
 *    Otherwise the page table address is extracted from the segment table,
 *    the page table index is added, and the result is returned.
 */
pt_entry_t *
pmap_pte(pmap_t pmap, vaddr_t virt)
{
	sdt_entry_t *sdt;

#ifdef DEBUG
	/*XXX will this change if physical memory is not contiguous? */
	/* take a look at PDTIDX XXXnivas */
	if (pmap == PMAP_NULL)
		panic("pmap_pte: pmap is NULL");
#endif

	sdt = SDTENT(pmap, virt);
	/*
	 * Check whether page table exists.
	 */
	if (!SDT_VALID(sdt))
		return (PT_ENTRY_NULL);

	return (pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES)) << PDT_SHIFT) +
		PDTIDX(virt);
}

/*
 * Routine:	PMAP_EXPAND_KMAP (internal)
 *
 * Function:
 *    Allocate a page descriptor table (pte_table) and validate associated
 * segment table entry, returning pointer to page table entry. This is
 * much like 'pmap_expand', except that table space is acquired
 * from an area set up by pmap_bootstrap, instead of through
 * uvm_km_zalloc. (Obviously, because uvm_km_zalloc uses the kernel map
 * for allocation - which we can't do when trying to expand the
 * kernel map!) Note that segment tables for the kernel map were
 * all allocated at pmap_bootstrap time, so we only need to worry
 * about the page table here.
 *
 * Parameters:
 *	virt	VA for which translation tables are needed
 *	prot	protection attributes for segment entries
 *
 * Extern/Global:
 *	kpdt_free	kernel page table free queue
 *
 * This routine simply dequeues a table from the kpdt_free list,
 * initializes all its entries (invalidates them), and sets the
 * corresponding segment table entry to point to it. If the kpdt_free
 * list is empty - we panic (no other places to get memory, sorry). (Such
 * a panic indicates that pmap_bootstrap is not allocating enough table
 * space for the kernel virtual address space).
 *
 */
pt_entry_t *
pmap_expand_kmap(vaddr_t virt, vm_prot_t prot)
{
	sdt_entry_t template, *sdt;
	kpdt_entry_t kpdt_ent;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
		printf("(pmap_expand_kmap: %x) v %x\n", curproc, virt);
#endif

	template = m88k_protection(prot) | PG_M | SG_V;

	/* segment table entry derivate from map and virt. */
	sdt = SDTENT(kernel_pmap, virt);
#ifdef DEBUG
	if (SDT_VALID(sdt))
		panic("pmap_expand_kmap: segment table entry VALID");
#endif

	kpdt_ent = kpdt_free;
	if (kpdt_ent == KPDT_ENTRY_NULL)
		panic("pmap_expand_kmap: Ran out of kernel pte tables");

	kpdt_free = kpdt_free->next;
	/* physical table */
	*sdt = kpdt_ent->phys | template;
	/* virtual table */
	*(sdt + SDT_ENTRIES) = (vaddr_t)kpdt_ent | template;

	/* Reinitialize this kpdt area to zero */
	bzero((void *)kpdt_ent, PDT_SIZE);

	return (pt_entry_t *)(kpdt_ent) + PDTIDX(virt);
}

/*
 * Routine:	PMAP_MAP
 *
 * Function:
 *    Map memory at initialization. The physical addresses being
 * mapped are not managed and are never unmapped.
 *
 * Parameters:
 *	virt	virtual address of range to map
 *	start	physical address of range to map
 *	end	physical address of end of range
 *	prot	protection attributes
 *	cmode	cache control attributes
 *
 * Calls:
 *	pmap_pte
 *	pmap_expand_kmap
 *
 * Special Assumptions
 *	For now, VM is already on, only need to map the specified
 * memory. Used only by pmap_bootstrap() and vm_page_startup().
 *
 * For each page that needs mapping:
 *	pmap_pte is called to obtain the address of the page table
 *	table entry (PTE). If the page table does not exist,
 *	pmap_expand_kmap is called to allocate it. Finally, the page table
 *	entry is set to point to the physical page.
 *
 *	initialize template with paddr, prot, dt
 *	look for number of phys pages in range
 *	{
 *		pmap_pte(virt)	- expand if necessary
 *		stuff pte from template
 *		increment virt one page
 *		increment template paddr one page
 *	}
 *
 */
vaddr_t
pmap_map(vaddr_t virt, paddr_t start, paddr_t end, vm_prot_t prot, u_int cmode)
{
	u_int npages;
	u_int num_phys_pages;
	pt_entry_t template, *pte;
	paddr_t	 page;
#ifdef	PMAP_USE_BATC
	u_int32_t batctmp;
	int i;
#endif

#ifdef DEBUG
	if (pmap_con_dbg & CD_MAP)
		printf ("(pmap_map: %x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
			curproc, start, end, virt, prot, cmode);
#endif

#ifdef DEBUG
	/* Check for zero if we map the very end of the address space... */
	if (start > end && end != 0) {
		panic("pmap_map: start greater than end address");
	}
#endif

	template = m88k_protection(prot) | cmode | PG_V;
#ifdef M88110
	if (cputyp == CPU_88110 && m88k_protection(prot) != PG_RO)
		template |= PG_M;
#endif

#ifdef	PMAP_USE_BATC
	batctmp = BATC_SO | BATC_V;
	if (template & CACHE_WT)
		batctmp |= BATC_WT;
	if (template & CACHE_GLOBAL)
		batctmp |= BATC_GLOBAL;
	if (template & CACHE_INH)
		batctmp |= BATC_INH;
	if (template & PG_PROT)
		batctmp |= BATC_PROT;
#endif

	page = trunc_page(start);
	npages = atop(round_page(end) - page);
	for (num_phys_pages = npages; num_phys_pages != 0; num_phys_pages--) {
#ifdef	PMAP_USE_BATC

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			printf("(pmap_map: %x) num_phys_pg=%x, virt=%x, "
			    "align V=%d, page=%x, align P=%d\n",
			    curproc, num_phys_pages, virt,
			    BATC_BLK_ALIGNED(virt), page,
			    BATC_BLK_ALIGNED(page));
#endif

		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) &&
		     num_phys_pages >= BATC_BLKBYTES/PAGE_SIZE &&
		     batc_used < BATC_MAX ) {
			/*
			 * map by BATC
			 */
			batctmp |= M88K_BTOBLK(virt) << BATC_VSHIFT;
			batctmp |= M88K_BTOBLK(page) << BATC_PSHIFT;

			for (i = 0; i < MAX_CPUS; i++)
				if (cpu_sets[i])
					cmmu_set_pair_batc_entry(i, batc_used,
					    batctmp);
			batc_entry[batc_used] = batctmp;
#ifdef DEBUG
			if (pmap_con_dbg & CD_MAP) {
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp);
				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE) {
					pte = pmap_pte(kernel_pmap, virt + i);
					if (PDT_VALID(pte))
						printf("(pmap_map: %x) va %x is already mapped: pte %x\n",
						    curproc, virt + i, *pte);
				}
			}
#endif
			batc_used++;
			virt += BATC_BLKBYTES;
			page += BATC_BLKBYTES;
			num_phys_pages -= BATC_BLKBYTES/PAGE_SIZE;
			continue;
		}
#endif	/* PMAP_USE_BATC */

		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			pte = pmap_expand_kmap(virt,
			    VM_PROT_READ | VM_PROT_WRITE);

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			if (PDT_VALID(pte))
				printf("(pmap_map: %x) pte @@ 0x%p already valid\n", curproc, pte);
#endif

		*pte = template | page;
		virt += PAGE_SIZE;
		page += PAGE_SIZE;
	}
	return virt;
}

/*
 * Routine:	PMAP_CACHE_CONTROL
 *
 * Function:
 *	Set the cache-control bits in the page table entries(PTE) which maps
 *	the specified virtual address range.
 *
 * Parameters:
 *	pmap_t		pmap
 *	vaddr_t		s
 *	vaddr_t		e
 *	u_int		mode
 *
 * Calls:
 *	pmap_pte
 *	invalidate_pte
 *	flush_atc_entry
 *
 *  This routine sequences through the pages of the specified range.
 * For each, it calls pmap_pte to acquire a pointer to the page table
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
 * Otherwise, the cache-control bits in the PTE's are adjusted as specified.
 *
 */
void
pmap_cache_ctrl(pmap_t pmap, vaddr_t s, vaddr_t e, u_int mode)
{
	int spl;
	pt_entry_t *pte;
	vaddr_t va;
	paddr_t pa;
	boolean_t kflush;
	int cpu;
	u_int users;

#ifdef DEBUG
	if ((mode & CACHE_MASK) != mode) {
		printf("(cache_ctrl) illegal mode %x\n", mode);
		return;
	}
	if (pmap_con_dbg & CD_CACHE) {
		printf("(pmap_cache_ctrl: %x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
	}

	if (pmap == PMAP_NULL)
		panic("pmap_cache_ctrl: pmap is NULL");
#endif /* DEBUG */

	PMAP_LOCK(pmap, spl);

	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

	for (va = s; va < e; va += PAGE_SIZE) {
		if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
			continue;
#ifdef DEBUG
		if (pmap_con_dbg & CD_CACHE) {
			printf("(cache_ctrl) pte@@0x%p\n", pte);
		}
#endif /* DEBUG */
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 * XXX
		 */
		*pte = (invalidate_pte(pte) & ~CACHE_MASK) | mode;
		flush_atc_entry(users, va, kflush);

		/*
		 * Data cache should be copied back and invalidated.
		 */
		pa = ptoa(PG_PFNUM(*pte));
		for (cpu = 0; cpu < MAX_CPUS; cpu++)
			if (cpu_sets[cpu])
				cmmu_flush_cache(cpu, pa, PAGE_SIZE);
	}
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_BOOTSTRAP
 *
 * Function:
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, allocate the kernel
 *	translation table space, and map control registers
 *	and other IO addresses.
 *
 * Parameters:
 *	load_start	PA where kernel was loaded
 *
 * Extern/Global:
 *
 *	PAGE_SIZE	VM (software) page size
 *	kernelstart	start symbol of kernel text
 *	etext		end of kernel text
 *	phys_map_vaddr	VA of page mapped arbitrarily for debug/IO
 *
 * Calls:
 *	simple_lock_init
 *	pmap_map
 *
 *    The physical address 'load_start' is mapped at
 * VM_MIN_KERNEL_ADDRESS, which maps the kernel code and data at the
 * virtual address for which it was (presumably) linked. Immediately
 * following the end of the kernel code/data, sufficient page of
 * physical memory are reserved to hold translation tables for the kernel
 * address space.
 *
 *    A pair of virtual pages per cpu are reserved for debugging and
 * IO purposes. They are arbitrarily mapped when needed. They are used,
 * for example, by pmap_copy_page and pmap_zero_page.
 *
 * For m88k, we have to map BUG memory also. This is a read only
 * mapping for 0x10000 bytes. We will end up having load_start as
 * 0 and VM_MIN_KERNEL_ADDRESS as 0 - yes sir, we have one-to-one
 * mapping!!!
 */

void
pmap_bootstrap(vaddr_t load_start)
{
	kpdt_entry_t kpdt_virt;
	sdt_entry_t *kmap;
	vaddr_t vaddr, virt;
	paddr_t s_text, e_text, kpdt_phys;
	pt_entry_t *pte;
	unsigned int kernel_pmap_size, pdt_size;
	int i;
	pmap_table_t ptable;
	extern void *kernelstart, *etext;

	simple_lock_init(&kernel_pmap->pm_lock);

	/*
	 * Allocate the kernel page table from the front of available
	 * physical memory, i.e. just after where the kernel image was loaded.
	 */
	/*
	 * The calling sequence is
	 *    ...
	 *  pmap_bootstrap(&kernelstart, ...);
	 * kernelstart being the first symbol in the load image.
	 * The kernel is linked such that &kernelstart == 0x10000 (size of
	 * BUG reserved memory area).
	 * The expression (&kernelstart - load_start) will end up as
	 *	0, making virtual_avail == avail_start, giving a 1-to-1 map)
	 */

	avail_start = round_page(avail_start);
	virtual_avail = avail_start +
	    (trunc_page((vaddr_t)&kernelstart) - load_start);

	/*
	 * Initialize kernel_pmap structure
	 */
	kernel_pmap->pm_count = 1;
	kernel_pmap->pm_cpus = 0;
	kmap = (sdt_entry_t *)(avail_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)virtual_avail;
	kmapva = virtual_avail;

	/*
	 * Reserve space for segment table entries.
	 * One for the regular segment table and one for the shadow table
	 * The shadow table keeps track of the virtual address of page
	 * tables. This is used in virtual-to-physical address translation
	 * functions. Remember, MMU cares only for physical addresses of
	 * segment and page table addresses. For kernel page tables, we
	 * really don't need this virtual stuff (since the kernel will
	 * be mapped 1-to-1) but for user page tables, this is required.
	 * Just to be consistent, we will maintain the shadow table for
	 * kernel pmap also.
	 */
	kernel_pmap_size = 2 * SDT_SIZE;

#ifdef DEBUG
	printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
#endif
	/* init all segment descriptors to zero */
	bzero(kernel_pmap->pm_stab, kernel_pmap_size);

	avail_start += kernel_pmap_size;
	virtual_avail += kernel_pmap_size;

	/* make sure page tables are page aligned!! XXX smurph */
	avail_start = round_page(avail_start);
	virtual_avail = round_page(virtual_avail);

	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = avail_start;
	kpdt_virt = (kpdt_entry_t)virtual_avail;

	/* Compute how much space we need for the kernel page table */
	pdt_size = atop(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS)
	    * sizeof(pt_entry_t);
	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		pdt_size += atop(ptable->size) * sizeof(pt_entry_t);
	pdt_size = round_page(pdt_size);
	kernel_pmap_size += pdt_size;
	avail_start += pdt_size;
	virtual_avail += pdt_size;

	/* init all page descriptors to zero */
	bzero((void *)kpdt_phys, pdt_size);
#ifdef DEBUG
	printf("--------------------------------------\n");
	printf("        kernel page start = 0x%x\n", kpdt_phys);
	printf("   kernel page table size = 0x%x\n", pdt_size);
	printf("          kernel page end = 0x%x\n", avail_start);

	printf("kpdt_virt = 0x%x\n", kpdt_virt);
#endif
	/*
	 * init the kpdt queue
	 */
	kpdt_free = kpdt_virt;
	for (i = pdt_size / PDT_SIZE; i != 0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vaddr_t)kpdt_virt + PDT_SIZE);
		kpdt_virt->phys = kpdt_phys;
		kpdt_virt = kpdt_virt->next;
		kpdt_phys += PDT_SIZE;
	}
	kpdt_virt->next = KPDT_ENTRY_NULL; /* terminate the list */

	/*
	 * Map the kernel image into virtual space
	 */

	s_text = load_start;	/* paddr of text */
	e_text = load_start +
	    ((vaddr_t)&etext - trunc_page((vaddr_t)&kernelstart));
	/* paddr of end of text section*/
	e_text = round_page(e_text);

	/* map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
	vaddr = pmap_map(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_INH);

	/* map the kernel text read only */
	vaddr = pmap_map(trunc_page((vaddr_t)&kernelstart),
	    s_text, e_text, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_GLOBAL);	/* shouldn't it be RO? XXX*/

	vaddr = pmap_map(vaddr, e_text, (paddr_t)kmap,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);

	/*
	 * Map system segment & page tables - should be cache inhibited?
	 * 88200 manual says that CI bit is driven on the Mbus while accessing
	 * the translation tree. I don't think we need to map it CACHE_INH
	 * here...
	 */
	if (kmapva != vaddr) {
		while (vaddr < (virtual_avail - kernel_pmap_size))
			vaddr = round_page(vaddr + 1);
	}
	vaddr = pmap_map(vaddr, (paddr_t)kmap, avail_start,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);

#if defined (MVME187) || defined (MVME197)
	/*
	 * Get ethernet buffer - need etherlen bytes physically contiguous.
	 * 1 to 1 mapped as well???. There is actually a bug in the macros
	 * used by the 1x7 ethernet driver. Remove this when that is fixed.
	 * XXX -nivas
	 */
	if (brdtyp == BRD_187 || brdtyp == BRD_8120 || brdtyp == BRD_197) {
		avail_start = vaddr;
		etherlen = ETHERPAGES * PAGE_SIZE;
		etherbuf = (void *)vaddr;

		vaddr = pmap_map(vaddr, avail_start, avail_start + etherlen,
		    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);

		virtual_avail += etherlen;
		avail_start += etherlen;

		if (vaddr != virtual_avail) {
			virtual_avail = vaddr;
			avail_start = round_page(avail_start);
		}
	}

#endif /* defined (MVME187) || defined (MVME197) */

	virtual_avail = round_page(virtual_avail);
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	/*
	 * Map two pages per cpu for copying/zeroing.
	 */

	phys_map_vaddr = virtual_avail;
	phys_map_vaddr_end = virtual_avail + 2 * (max_cpus << PAGE_SHIFT);
	avail_start += 2 * (max_cpus << PAGE_SHIFT);
	virtual_avail += 2 * (max_cpus << PAGE_SHIFT);

	/*
	 * Map all IO space 1-to-1. Ideally, I would like to not do this
	 * but have va for the given IO address dynamically allocated. But
	 * on the 88200, 2 of the BATCs are hardwired to map the IO space
	 * 1-to-1; I decided to map the rest of the IO space 1-to-1.
	 * And bug ROM & the SRAM need to be mapped 1-to-1 if we ever want to
	 * execute bug system calls after the MMU has been turned on.
	 * OBIO should be mapped cache inhibited.
	 */

	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		if (ptable->size != 0) {
			pmap_map(ptable->virt_start, ptable->phys_start,
			    ptable->phys_start + ptable->size,
			    ptable->prot, ptable->cacheability);
		}

	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physical pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -
	 * else we will be referencing the wrong page.
	 */

#define	SYSMAP(c, p, v, n)	\
({ \
	v = (c)virt; \
	if ((p = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL) \
		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
	virt += ((n) * PAGE_SIZE); \
})

	virt = virtual_avail;

	SYSMAP(caddr_t, vmpte, vmmap, 1);
	invalidate_pte(vmpte);

	SYSMAP(struct msgbuf *, msgbufmap, msgbufp, btoc(MSGBUFSIZE));

	virtual_avail = virt;

	/*
	 * Set translation for UPAGES at UADDR. The idea is we want to
	 * have translations set up for UADDR. Later on, the ptes for
	 * for this address will be set so that kstack will refer
	 * to the u area. Make sure pmap knows about this virtual
	 * address by doing vm_findspace on kernel_map.
	 */

	for (i = 0, virt = UADDR; i < UPAGES; i++, virt += PAGE_SIZE) {
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE);
	}

	/*
	 * Switch to using new page tables
	 */

	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) |
	    CACHE_GLOBAL | CACHE_WT | APR_V;

	/* Invalidate entire kernel TLB and get ready for address translation */
	for (i = 0; i < MAX_CPUS; i++)
		if (cpu_sets[i]) {
			cmmu_flush_tlb(i, TRUE, VM_MIN_KERNEL_ADDRESS,
			    VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS);
			/* Load supervisor pointer to segment table. */
			cmmu_set_sapr(i, kernel_pmap->pm_apr);
#ifdef DEBUG
			printf("cpu%d: running virtual\n", i);
#endif
			SETBIT_CPUSET(i, &kernel_pmap->pm_cpus);
		}
}

/*
 * Routine:	PMAP_INIT
 *
 * Function:
 *	Initialize the pmap module. It is called by vm_init, to initialize
 *	any structures that the pmap system needs to map virtual memory.
 *
 * Calls:
 *	pool_init
 *
 *   This routine does not really have much to do. It initializes
 * pools for pmap structures and pv_entry structures.
 */
void
pmap_init(void)
{
#ifdef DEBUG
	if (pmap_con_dbg & CD_INIT)
		printf("pmap_init()\n");
#endif

	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", NULL);
} /* pmap_init() */

/*
 * Routine:	PMAP_ZERO_PAGE
 *
 * Function:
 *	Zeroes the specified page.
 *
 * Parameters:
 *	pg		page to zero
 *
 * Extern/Global:
 *	phys_map_vaddr
 *
 * Special Assumptions:
 *	no locking required
 *
 *	This routine maps the physical pages at the 'phys_map' virtual
 * address set up in pmap_bootstrap. It flushes the TLB to make the new
 * mappings effective, and zeros all the bits.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	vaddr_t va;
	int spl;
	int cpu = cpu_number();
	pt_entry_t *pte;

	CHECK_PAGE_ALIGN(pa, "pmap_zero_page");

	va = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	pte = pmap_pte(kernel_pmap, va);

	SPLVM(spl);

	*pte = m88k_protection(VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_M /* 88110 */ | PG_V | pa;

	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_flush_tlb(cpu, TRUE, va, PAGE_SIZE);

	/*
	 * The page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the pa flushed after the filling.
	 */
	bzero((void *)va, PAGE_SIZE);
	cmmu_flush_data_cache(cpu, pa, PAGE_SIZE);

	SPLX(spl);
}

/*
 * Routine:	PMAP_CREATE
 *
 * Function:
 *	Create and return a physical map. If the size specified for the
 *	map is zero, the map is an actual physical map, and may be referenced
 *	by the hardware. If the size specified is non-zero, the map will be
 *	used in software only, and is bounded by that size.
 *
 *  This routines allocates a pmap structure.
 */
pmap_t
pmap_create(void)
{
	pmap_t pmap;
	sdt_entry_t *segdt;
	paddr_t stpa;
	u_int s;
#ifdef	PMAP_USE_BATC
	int i;
#endif

	pmap = pool_get(&pmappool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));

	/*
	 * Allocate memory for *actual* segment table and *shadow* table.
	 */
	s = round_page(2 * SDT_SIZE);
#ifdef DEBUG
	if (pmap_con_dbg & CD_CREAT) {
		printf("(pmap_create: %x) need %d pages for sdt\n",
		    curproc, atop(s));
	}
#endif

	segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, s);
	if (segdt == NULL)
		panic("pmap_create: uvm_km_zalloc failure");

	/*
	 * Initialize pointer to segment table both virtual and physical.
	 */
	pmap->pm_stab = segdt;
	if (pmap_extract(kernel_pmap, (vaddr_t)segdt,
	    (paddr_t *)&stpa) == FALSE)
		panic("pmap_create: pmap_extract failed!");
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) | CACHE_GLOBAL | APR_V;

#ifdef DEBUG
	if (!PAGE_ALIGNED(stpa))
		panic("pmap_create: sdt_table 0x%x not aligned on page boundary",
		    (int)stpa);

	if (pmap_con_dbg & CD_CREAT) {
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x (pa 0x%x)\n",
		    curproc, pmap, pmap->pm_stab, stpa);
	}
#endif

	/* memory for page tables should not be writeback or local */
	pmap_cache_ctrl(kernel_pmap,
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_GLOBAL | CACHE_WT);

	/*
	 * Initialize SDT_ENTRIES.
	 */
	/*
	 * There is no need to clear segment table, since uvm_km_zalloc
	 * provides us clean pages.
	 */

	/*
	 * Initialize pmap structure.
	 */
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
	pmap->pm_cpus = 0;

#ifdef	PMAP_USE_BATC
	/* initialize block address translation cache */
	for (i = 0; i < BATC_MAX; i++) {
		pmap->pm_ibatc[i].bits = 0;
		pmap->pm_dbatc[i].bits = 0;
	}
#endif

	return pmap;
}

/*
 * Routine:	PMAP_RELEASE
 *
 *	Internal procedure used by pmap_destroy() to actualy deallocate
 *	the tables.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_pte
 *	uvm_km_free
 *
 * Special Assumptions:
 *	No locking is needed, since this is only called which the
 * 	pm_count field of the pmap structure goes to zero.
 *
 * This routine sequences of through the user address space, releasing
 * all translation table space back to the system using uvm_km_free.
 * The loops are indexed by the virtual address space
 * ranges represented by the table group sizes(PDT_VA_SPACE).
 *
 */
void
pmap_release(pmap_t pmap)
{
	unsigned long sdt_va;	/* outer loop index */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */

#ifdef DEBUG
	if (pmap_con_dbg & CD_FREE)
		printf("(pmap_release: %x) pmap %x\n", curproc, pmap);
#endif

	/* Segment table Loop */
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE) {
		if ((gdttbl = pmap_pte(pmap, (vaddr_t)sdt_va)) != PT_ENTRY_NULL) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
				printf("(pmap_release: %x) free page table = 0x%x\n",
				    curproc, gdttbl);
#endif
			uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
		}
	}

	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
	sdttbl = pmap->pm_stab;		/* addr of segment table */
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
		printf("(pmap_release: %x) free segment table = 0x%x\n",
		    curproc, sdttbl);
#endif
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2 * SDT_SIZE));

#ifdef DEBUG
	if (pmap_con_dbg & CD_FREE)
		printf("(pmap_release: %x) pm_count = 0\n", curproc);
#endif
}

/*
 * Routine:	PMAP_DESTROY
 *
 * Function:
 *	Retire the given physical map from service. Should only be called
 *	if the map contains no valid mappings.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_release
 *	pool_put
 *
 * Special Assumptions:
 *	Map contains no valid mappings.
 *
 *  This routine decrements the reference count in the pmap
 * structure. If it goes to zero, pmap_release is called to release
 * the memory space to the system. Then, call pool_put to free the
 * pmap structure.
 */
void
pmap_destroy(pmap_t pmap)
{
	int count;

#ifdef DEBUG
	if (pmap == kernel_pmap)
		panic("pmap_destroy: Attempt to destroy kernel pmap");
#endif

	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		pool_put(&pmappool, pmap);
	}
}


/*
 * Routine:	PMAP_REFERENCE
 *
 * Function:
 *	Add a reference to the specified pmap.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Under a pmap read lock, the pm_count field of the pmap structure
 * is incremented. The function then returns.
 */
void
pmap_reference(pmap_t pmap)
{

	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}

/*
 * Routine:	PMAP_REMOVE_PTE (internal)
 *
 * Function:
 *	Invalidate a given page table entry associated with the
 *	given virtual address.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	va		virtual address of page to remove
 *	pte		existing pte
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pool_put
 *	invalidate_pte
 *	flush_atc_entry
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *  If the PTE is valid, the routine must invalidate the entry. The
 * 'modified' bit, if on, is referenced to the VM, and into the appropriate
 * entry in the PV list entry. Next, the function must find the PV
 * list entry associated with this pmap/va (if it doesn't exist - the function
 * panics). The PV list entry is unlinked from the list, and returned to
 * its zone.
 */
void
pmap_remove_pte(pmap_t pmap, vaddr_t va, pt_entry_t *pte)
{
	pt_entry_t opte;
	pv_entry_t prev, cur, pvl;
	struct vm_page *pg;
	paddr_t pa;
	u_int users;
	boolean_t kflush;

#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove_pte: %x) pmap kernel va %x\n", curproc, va);
		else
			printf("(pmap_remove_pte: %x) pmap %x va %x\n", curproc, pmap, va);
	}
#endif

	if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
		return;	 	/* no page mapping, nothing to do! */
	}

	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

	/*
	 * Update statistics.
	 */
	pmap->pm_stats.resident_count--;
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;

	pa = ptoa(PG_PFNUM(*pte));

	/*
	 * Invalidate the pte.
	 */

	opte = invalidate_pte(pte) & (PG_U | PG_M);
	flush_atc_entry(users, va, kflush);

	pg = PHYS_TO_VM_PAGE(pa);

	/* If this isn't a managed page, just return. */
	if (pg == NULL)
		return;

	/*
	 * Remove the mapping from the pvlist for
	 * this physical page.
	 */
	pvl = pg_to_pvh(pg);

#ifdef DIAGNOSTIC
	if (pvl->pv_pmap == PMAP_NULL)
		panic("pmap_remove_pte: null pv_list");
#endif

	prev = PV_ENTRY_NULL;
	for (cur = pvl; cur != PV_ENTRY_NULL; cur = cur->pv_next) {
		if (cur->pv_va == va && cur->pv_pmap == pmap)
			break;
		prev = cur;
	}
	if (cur == PV_ENTRY_NULL) {
		panic("pmap_remove_pte: mapping for va "
		    "0x%lx (pa 0x%lx) not in pv list at 0x%p",
		    va, pa, pvl);
	}

	if (prev == PV_ENTRY_NULL) {
		/*
		 * Hander is the pv_entry. Copy the next one
		 * to hander and free the next one (we can't
		 * free the hander)
		 */
		cur = cur->pv_next;
		if (cur != PV_ENTRY_NULL) {
			cur->pv_flags = pvl->pv_flags;
			*pvl = *cur;
			pool_put(&pvpool, cur);
		} else {
			pvl->pv_pmap = PMAP_NULL;
		}
	} else {
		prev->pv_next = cur->pv_next;
		pool_put(&pvpool, cur);
	}

	/* Update saved attributes for managed page */
	pvl->pv_flags |= opte;
}

/*
 * Routine:	PMAP_REMOVE_RANGE (internal)
 *
 * Function:
 *	Invalidate page table entries associated with the
 *	given virtual address range. The entries given are the first
 *	(inclusive) and last (exclusive) entries for the VM pages.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		virtual address of start of range to remove
 *	e		virtual address of end of range to remove
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pmap_remove_pte
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *   This routine sequences through the pages defined by the given
 * range. For each page, the associated page table entry (PTE) is
 * invalidated via pmap_remove_pte().
 *
 * Empty segments are skipped for performance.
 */
void
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	vaddr_t va;

#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove_range: %x) pmap kernel s %x e %x\n", curproc, s, e);
		else
			printf("(pmap_remove_range: %x) pmap %x s %x e %x\n", curproc, pmap, s, e);
	}
#endif

	/*
	 * Loop through the range in vm_page_size increments.
	 */
	for (va = s; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;

		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
		}

		pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
	}
}

/*
 * Routine:	PMAP_REMOVE
 *
 * Function:
 *	Remove the given range of addresses from the specified map.
 *	It is assumed that start and end are properly rounded to the VM page
 *	size.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s
 *	e
 *
 * Special Assumptions:
 *	Assumes not all entries must be valid in specified range.
 *
 * Calls:
 *	pmap_remove_range
 *
 *  After taking pmap read lock, pmap_remove_range is called to do the
 * real work.
 */
void
pmap_remove(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	int spl;

	if (pmap == PMAP_NULL)
		return;

#ifdef DEBUG
	if (s >= e)
		panic("pmap_remove: start greater than end address");
#endif

	PMAP_LOCK(pmap, spl);
	pmap_remove_range(pmap, s, e);
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_REMOVE_ALL
 *
 * Function:
 *	Removes this physical page from all physical maps in which it
 *	resides. Reflects back modify bits to the pager.
 *
 * Parameters:
 *	pg		physical pages which is to
 *			be removed from all maps
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	simple_lock
 *	pmap_pte
 *	pool_put
 *
 *  If the page specified by the given address is not a managed page,
 * this routine simply returns. Otherwise, the PV list associated with
 * that page is traversed. For each pmap/va pair pmap_pte is called to
 * obtain a pointer to the page table entry (PTE) associated with the
 * va (the PTE must exist and be valid, otherwise the routine panics).
 * The hardware 'modified' bit in the PTE is examined. If it is on, the
 * corresponding bit in the PV list entry corresponding
 * to the physical page is set to 1.
 * Then, the PTE is invalidated, and the PV list entry is unlinked and
 * freed.
 *
 *  At the end of this function, the PV list for the specified page
 * will be null.
 */
void
pmap_remove_all(struct vm_page *pg)
{
	pt_entry_t *pte;
	pv_entry_t pvl;
	vaddr_t va;
	pmap_t pmap;
	int spl;

	if (pg == NULL) {
		/* not a managed page. */
#ifdef DEBUG
		if (pmap_con_dbg & CD_RMAL)
			printf("(pmap_remove_all: %x) vm page 0x%x not a managed page\n", curproc, pg);
#endif
		return;
	}

#ifdef DEBUG
	if (pmap_con_dbg & CD_RMAL)
		printf("(pmap_remove_all: %x) va %x\n", curproc, pg, pg_to_pvh(pg)->pv_va);
#endif

	SPLVM(spl);
	/*
	 * Walk down PV list, removing all mappings.
	 * We don't have to lock the pv list, since we have the entire pmap
	 * system.
	 */
remove_all_Retry:

	pvl = pg_to_pvh(pg);

	/*
	 * Loop for each entry on the pv list
	 */
	while (pvl != PV_ENTRY_NULL && (pmap = pvl->pv_pmap) != PMAP_NULL) {
		if (!simple_lock_try(&pmap->pm_lock))
			goto remove_all_Retry;

		va = pvl->pv_va;
		pte = pmap_pte(pmap, va);

		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			pvl = pvl->pv_next;
			goto next;	/* no page mapping */
		}
		if (pmap_pte_w(pte)) {
#ifdef DEBUG
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
				    pg);
#endif
			pvl = pvl->pv_next;
			goto next;
		}

		pmap_remove_pte(pmap, va, pte);

		/*
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
		 */
next:
		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);
}

/*
 * Routine:	PMAP_PROTECT
 *
 * Function:
 *	Sets the physical protection on the specified range of this map
 *	as requested.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		start address of start of range
 *	e		end address of end of range
 *	prot		desired protection attributes
 *
 *	Calls:
 *		PMAP_LOCK, PMAP_UNLOCK
 *		CHECK_PAGE_ALIGN
 *		pmap_pte
 *		PDT_VALID
 *
 *  This routine sequences through the pages of the specified range.
 * For each, it calls pmap_pte to acquire a pointer to the page table
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
 * Otherwise, the PTE's protection attributes are adjusted as specified.
 */
void
pmap_protect(pmap_t pmap, vaddr_t s, vaddr_t e, vm_prot_t prot)
{
	int spl;
	pt_entry_t *pte, ap;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

#ifdef DEBUG
	if (s >= e)
		panic("pmap_protect: start grater than end address");
#endif

	if ((prot & VM_PROT_READ) == 0) {
		pmap_remove(pmap, s, e);
		return;
	}

	ap = m88k_protection(prot);

	PMAP_LOCK(pmap, spl);

	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

	CHECK_PAGE_ALIGN(s, "pmap_protect");

	/*
	 * Loop through the range in vm_page_size increments.
	 */
	for (va = s; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;

		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
		}

		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}

		/*
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
		 * written back by any other cpu.
		 */
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
		flush_atc_entry(users, va, kflush);
		pte++;
	}
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_EXPAND
 *
 * Function:
 *	Expands a pmap to be able to map the specified virtual address.
 *	New kernel virtual memory is allocated for a page table.
 *
 *	Must be called with the pmap system and the pmap unlocked, since
 *	these must be unlocked to use vm_allocate or vm_deallocate (via
 *	uvm_km_zalloc). Thus it must be called in a unlock/lock loop
 *	that checks whether the map has been expanded enough. (We won't loop
 *	forever, since page table aren't shrunk.)
 *
 * Parameters:
 *	pmap	point to pmap structure
 *	v	VA indicating which tables are needed
 *
 * Extern/Global:
 *	user_pt_map
 *	kernel_pmap
 *
 * Calls:
 *	pmap_pte
 *	uvm_km_free
 *	uvm_km_zalloc
 *	pmap_extract
 *
 * Special Assumptions
 *	no pmap locks held
 *	pmap != kernel_pmap
 *
 * 1:	This routine immediately allocates space for a page table.
 *
 * 2:	The page table entries (PTEs) are initialized (set invalid), and
 *	the corresponding segment table entry is set to point to the new
 *	page table.
 */
void
pmap_expand(pmap_t pmap, vaddr_t v)
{
	int spl;
	vaddr_t pdt_vaddr;
	paddr_t pdt_paddr;
	sdt_entry_t *sdt;
	pt_entry_t *pte;

#ifdef DEBUG
	if (pmap_con_dbg & CD_EXP)
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, pmap, v);
#endif

	CHECK_PAGE_ALIGN(v, "pmap_expand");

	/* XXX */
	pdt_vaddr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
	if (pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr) == FALSE)
		panic("pmap_expand: pmap_extract failed");

	/* memory for page tables should not be writeback or local */
	pmap_cache_ctrl(kernel_pmap,
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_GLOBAL | CACHE_WT);

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, v)) != PT_ENTRY_NULL) {
		/*
		 * Someone else caused us to expand
		 * during our vm_allocate.
		 */
		simple_unlock(&pmap->pm_lock);
		uvm_km_free(kernel_map, pdt_vaddr, PAGE_SIZE);

#ifdef DEBUG
		if (pmap_con_dbg & CD_EXP)
			printf("(pmap_expand: %x) table has already been allocated\n", curproc);
#endif
		splx(spl);
		return;
	}
	/*
	 * Apply a mask to V to obtain the vaddr of the beginning of
	 * its containing page 'table group', i.e. the group of
	 * page tables that fit eithin a single VM page.
	 * Using that, obtain the segment table pointer that references the
	 * first page table in the group, and initialize all the
	 * segment table descriptions for the page 'table group'.
	 */
	v &= ~((1 << (PDT_BITS + PG_BITS)) - 1);

	sdt = SDTENT(pmap, v);

	/*
	 * Init each of the segment entries to point the freshly allocated
	 * page tables.
	 */
	*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
	*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;

	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_ENTER
 *
 * Function:
 *	Insert the given physical page (p) at the specified virtual
 *	address (v) in the target phisical map with the protecton requested.
 *	If specified, the page will be wired down, meaning that the
 *	related pte can not be reclaimed.
 *
 * N.B.: This is the only routine which MAY NOT lazy-evaluation or lose
 *	information. That is, this routine must actually insert this page
 *	into the given map NOW.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	va	VA of page to be mapped
 *	pa	PA of page to be mapped
 *	prot	protection attributes for page
 *	wired	wired attribute for page
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pmap_expand
 *	pmap_remove_pte
 *
 *	This routine starts off by calling pmap_pte to obtain a (virtual)
 * pointer to the page table entry corresponding to given virtual
 * address. If the page table itself does not exist, pmap_expand is
 * called to allocate it.
 *
 *	If the page table entry (PTE) already maps the given physical page,
 * all that is needed is to set the protection and wired attributes as
 * given. TLB entries are flushed and pmap_enter returns.
 *
 *	If the page table entry (PTE) maps a different physical page than
 * that given, the old mapping is removed by a call to map_remove_range.
 * And execution of pmap_enter continues.
 *
 *	To map the new physical page, the routine first inserts a new
 * entry in the PV list exhibiting the given pmap and virtual address.
 * It then inserts the physical page address, protection attributes, and
 * wired attributes into the page table entry (PTE).
 *
 *
 *	get machine-dependent prot code
 *	get the pte for this page
 *	if necessary pmap_expand(pmap, v)
 *	if (changing wired attribute or protection) {
 * 		flush entry from TLB
 *		update template
 *		for (ptes per vm page)
 *			stuff pte
 *	} else if (mapped at wrong addr)
 *		flush entry from TLB
 *		pmap_remove_pte
 *	} else {
 *		enter mapping in pv_list
 *		setup template and stuff ptes
 *	}
 *
 */
int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	int spl;
	pt_entry_t *pte, template;
	paddr_t old_pa;
	pv_entry_t pv_e, pvl;
	u_int users;
	boolean_t kflush;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	struct vm_page *pg;

	CHECK_PAGE_ALIGN(va, "pmap_entry - va");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - pa");

#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT) {
		if (pmap == kernel_pmap)
			printf("(pmap_enter: %x) pmap kernel va %x pa %x\n", curproc, va, pa);
		else
			printf("(pmap_enter: %x) pmap %x va %x pa %x\n", curproc, pmap, va, pa);
	}

	/* copying/zeroing pages are magic */
	if (pmap == kernel_pmap &&
	    va >= phys_map_vaddr && va < phys_map_vaddr_end) {
		return 0;
	}
#endif

	template = m88k_protection(prot);

	PMAP_LOCK(pmap, spl);
	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;

	/*
	 * Expand pmap to include this pte.
	 */
	while ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		if (pmap == kernel_pmap) {
			pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
		} else {
			/*
			 * Must unlock to expand the pmap.
			 */
			simple_unlock(&pmap->pm_lock);
			pmap_expand(pmap, va);
			simple_lock(&pmap->pm_lock);
		}
	}
	/*
	 * Special case if the physical page is already mapped at this address.
	 */
	old_pa = ptoa(PG_PFNUM(*pte));
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, *pte);
#endif
	if (old_pa == pa) {
		/* May be changing its wired attributes or protection */
		if (wired && !(pmap_pte_w(pte)))
			pmap->pm_stats.wired_count++;
		else if (!wired && pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;

		pvl = NULL;
	} else { /* if (pa == old_pa) */
		/* Remove old mapping from the PV list if necessary. */
		pmap_remove_pte(pmap, va, pte);

		pg = PHYS_TO_VM_PAGE(pa);
		if (pg != NULL) {
			/*
			 *	Enter the mapping in the PV list for this
			 *	physical page.
			 */
			pvl = pg_to_pvh(pg);

			if (pvl->pv_pmap == PMAP_NULL) {
				/*
				 *	No mappings yet
				 */
				pvl->pv_va = va;
				pvl->pv_pmap = pmap;
				pvl->pv_next = PV_ENTRY_NULL;
				pvl->pv_flags = 0;

			} else {
#ifdef DEBUG
				/*
				 * Check that this mapping is not already there
				 */
				for (pv_e = pvl; pv_e; pv_e = pv_e->pv_next)
					if (pv_e->pv_pmap == pmap &&
					    pv_e->pv_va == va)
						panic("pmap_enter: already in pv_list");
#endif
				/*
				 * Add new pv_entry after header.
				 */
				pv_e = pool_get(&pvpool, PR_NOWAIT);
				if (pv_e == NULL) {
					if (flags & PMAP_CANFAIL) {
						PMAP_UNLOCK(pmap, spl);
						return (ENOMEM);
					} else
						panic("pmap_enter: "
						    "pvpool exhausted");
				}
				pv_e->pv_va = va;
				pv_e->pv_pmap = pmap;
				pv_e->pv_next = pvl->pv_next;
				pv_e->pv_flags = 0;
				pvl->pv_next = pv_e;
			}
		}

		/*
		 * And count the mapping.
		 */
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
	} /* if (pa == old_pa) ... else */

	template |= PG_V;
	if (wired)
		template |= PG_W;

	if ((unsigned long)pa >= last_addr)
		template |= CACHE_INH;
	else
		template |= CACHE_GLOBAL;

	if (flags & VM_PROT_WRITE)
		template |= PG_U | PG_M;
	else if (flags & VM_PROT_ALL)
		template |= PG_U;

	/*
	 * Invalidate pte temporarily to avoid being written
	 * back the modified bit and/or the reference bit by
	 * any other cpu.
	 */
	template |= invalidate_pte(pte) & (PG_U | PG_M);
	*pte = template | pa;
	flush_atc_entry(users, va, kflush);
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) set pte to %x\n", *pte);
#endif

	/*
	 * Cache attribute flags
	 */
	if (pvl != NULL)
		pvl->pv_flags |= (template & (PG_U | PG_M));

	PMAP_UNLOCK(pmap, spl);

	return 0;
}

/*
 * Routine:	pmap_unwire
 *
 * Function:	Change the wiring attributes for a map/virtual-address pair.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	v	virtual address of page to be unwired
 *
 * Calls:
 *	pmap_pte
 *
 * Special Assumptions:
 *	The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap_t pmap, vaddr_t v)
{
	pt_entry_t *pte;
	int spl;

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
		panic("pmap_unwire: pte missing");

	if (pmap_pte_w(pte)) {
		/* unwired mapping */
		pmap->pm_stats.wired_count--;
		*pte &= ~PG_W;
	}

	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_EXTRACT
 *
 * Function:
 *	Extract the physical page address associoated
 *	with the given map/virtual_address pair.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	va		virtual address
 *	pap		storage for result.
 *
 * Calls:
 *	PMAP_LOCK, PMAP_UNLOCK
 *	pmap_pte
 *
 * If BATC mapping is enabled and the specified pmap is kernel_pmap,
 * batc_entry is scanned to find out the mapping.
 *
 * Then the routine calls pmap_pte to get a (virtual) pointer to
 * the page table entry (PTE) associated with the given virtual
 * address. If the page table does not exist, or if the PTE is not valid,
 * then 0 address is returned. Otherwise, the physical page address from
 * the PTE is returned.
 */
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *pte;
	paddr_t pa;
	int spl;
	boolean_t rv = FALSE;

#ifdef	PMAP_USE_BATC
	int i;
#endif

#ifdef DIAGNOSTIC
	if (pmap == PMAP_NULL)
		panic("pmap_extract: pmap is NULL");
#endif

#ifdef	PMAP_USE_BATC
	/*
	 * check BATC first
	 */
	if (pmap == kernel_pmap && batc_used != 0)
		for (i = batc_used - 1; i != 0; i--)
			if (batc_entry[i].lba == M88K_BTOBLK(va)) {
				if (pap != NULL)
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) |
						(va & BATC_BLKMASK);
				return TRUE;
			}
#endif

	PMAP_LOCK(pmap, spl);

	pte = pmap_pte(pmap, va);
	if (pte != PT_ENTRY_NULL && PDT_VALID(pte)) {
		rv = TRUE;
		if (pap != NULL) {
			pa = ptoa(PG_PFNUM(*pte));
			pa |= (va & PAGE_MASK); /* offset within page */
			*pap = pa;
		}
	}

	PMAP_UNLOCK(pmap, spl);
	return rv;
}

/*
 * Routine:	PMAP_COLLECT
 *
 * Runction:
 *	Garbage collects the physical map system for pages which are
 *	no longer used. there may well be pages which are not
 *	referenced, but others may be collected as well.
 *	Called by the pageout daemon when pages are scarce.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_pte
 *	pmap_remove_range
 *	uvm_km_free
 *
 *	The intent of this routine is to release memory pages being used
 * by translation tables. They can be release only if they contain no
 * valid mappings, and their parent table entry has been invalidated.
 *
 *	The routine sequences through the entries user address space,
 * inspecting page-sized groups of page tables for wired entries. If
 * a full page of tables has no wired enties, any otherwise valid
 * entries are invalidated (via pmap_remove_range). Then, the segment
 * table entries corresponding to this group of page tables are
 * invalidated. Finally, uvm_km_free is called to return the page to the
 * system.
 *
 *	If all entries in a segment table are invalidated, it too can
 * be returned to the system.
 */
void
pmap_collect(pmap_t pmap)
{
	vaddr_t sdt_va;		/* outer loop index */
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t *gdttblend;	/* ptr to byte after last entry in
				   table group */
	pt_entry_t *gdtp;	/* ptr to index into a page table */
	boolean_t found_gdt_wired; /* flag indicating a wired page exists
				   in a page table's address range */
	int spl;

#ifdef DEBUG
	if (pmap_con_dbg & CD_COL)
		printf ("(pmap_collect: %x) pmap %x\n", curproc, pmap);
#endif

	PMAP_LOCK(pmap, spl);

	sdtp = pmap->pm_stab; /* addr of segment table */

	/* Segment table loop */
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE, sdtp++) {
		gdttbl = pmap_pte(pmap, sdt_va);
		if (gdttbl == PT_ENTRY_NULL)
			continue; /* no maps in this range */

		gdttblend = gdttbl + PDT_ENTRIES;

		/* scan page maps for wired pages */
		found_gdt_wired = FALSE;
		for (gdtp = gdttbl; gdtp < gdttblend; gdtp++) {
			if (pmap_pte_w(gdtp)) {
				found_gdt_wired = TRUE;
				break;
			}
		}

		if (found_gdt_wired)
			continue; /* can't free this range */

		/* invalidate all maps in this range */
		pmap_remove_range(pmap, sdt_va, sdt_va + PDT_VA_SPACE);

		/*
		 * we can safely deallocate the page map(s)
		 */
		*((sdt_entry_t *) sdtp) = 0;
		*((sdt_entry_t *)(sdtp + SDT_ENTRIES)) = 0;

		/*
		 * we have to unlock before freeing the table, since
		 * uvm_km_free will invoke another pmap routine
		 */
		simple_unlock(&pmap->pm_lock);
		uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
		simple_lock(&pmap->pm_lock);
	}

	PMAP_UNLOCK(pmap, spl);

#ifdef DEBUG
	if (pmap_con_dbg & CD_COL)
		printf("(pmap_collect: %x) done\n", curproc);
#endif
}

/*
 * Routine:	PMAP_ACTIVATE
 *
 * Function:
 * 	Binds the pmap associated to the process to the current processor.
 *
 * Parameters:
 * 	p	pointer to proc structure
 *
 * Notes:
 *	If the specified pmap is not kernel_pmap, this routine stores its
 *	apr template into UAPR (user area pointer register) in the
 *	CMMUs connected to the specified CPU.
 *
 *	Then it flushes the TLBs mapping user virtual space, in the CMMUs
 *	connected to the specified CPU.
 */
void
pmap_activate(struct proc *p)
{
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();
#ifdef	PMAP_USE_BATC
	int n;
#endif

#ifdef DEBUG
	if (pmap_con_dbg & CD_ACTIVATE)
		printf("(pmap_activate: %x) pmap 0x%p\n", p, pmap);
#endif

	if (pmap != kernel_pmap) {
		/*
		 * Lock the pmap to put this cpu in its active set.
		 */
		simple_lock(&pmap->pm_lock);

#ifdef	PMAP_USE_BATC
		/*
		 * cmmu_pmap_activate will set the uapr and the batc entries,
		 * then flush the *USER* TLB. IF THE KERNEL WILL EVER CARE
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE
		 * FLUSHED AS WELL.
		 */
		cmmu_pmap_activate(cpu, pmap->pm_apr,
		    pmap->pm_ibatc, pmap->pm_dbatc);
		for (n = 0; n < BATC_MAX; n++)
			*(register_t *)&batc_entry[n] = pmap->pm_ibatc[n].bits;
#else
		cmmu_set_uapr(pmap->pm_apr);
		cmmu_flush_tlb(cpu, FALSE, VM_MIN_ADDRESS,
		    VM_MAX_ADDRESS - VM_MIN_ADDRESS);
#endif	/* PMAP_USE_BATC */

		/*
		 * Mark that this cpu is using the pmap.
		 */
		SETBIT_CPUSET(cpu, &(pmap->pm_cpus));
		simple_unlock(&pmap->pm_lock);
	}
}

/*
 * Routine:	PMAP_DEACTIVATE
 *
 * Function:
 *	Unbinds the pmap associated to the process from the current processor.
 *
 * Parameters:
 *	p		pointer to proc structure
 */
void
pmap_deactivate(struct proc *p)
{
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();

	if (pmap != kernel_pmap) {
		/*
		 * we expect the spl is already raised to sched level.
		 */
		simple_lock(&pmap->pm_lock);
		CLRBIT_CPUSET(cpu, &(pmap->pm_cpus));
		simple_unlock(&pmap->pm_lock);
	}
}

/*
 * Routine:	PMAP_COPY_PAGE
 *
 * Function:
 *	Copies the specified pages.
 *
 * Parameters:
 *	src	PA of source page
 *	dst	PA of destination page
 *
 * Extern/Global:
 *	phys_map_vaddr
 *
 * Special Assumptions:
 *	no locking required
 *
 * This routine maps the physical pages at the 'phys_map' virtual
 * addresses set up in pmap_bootstrap. It flushes the TLB to make the
 * new mappings effective, and performs the copy.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
	vaddr_t dstva, srcva;
	int spl;
	pt_entry_t *dstpte, *srcpte;
	int cpu = cpu_number();

	CHECK_PAGE_ALIGN(src, "pmap_copy_page - src");
	CHECK_PAGE_ALIGN(dst, "pmap_copy_page - dst");

	dstva = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	srcva = dstva + PAGE_SIZE;
	dstpte = pmap_pte(kernel_pmap, dstva);
	srcpte = pmap_pte(kernel_pmap, srcva);

	SPLVM(spl);

	*dstpte = m88k_protection(VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_M /* 88110 */ | PG_V | dst;
	*srcpte = m88k_protection(VM_PROT_READ) |
	    CACHE_GLOBAL | PG_V | src;

	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_flush_tlb(cpu, TRUE, dstva, 2 * PAGE_SIZE);

	/*
	 * The source page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the source pa flushed before the copy is
	 * attempted, and the destination pa flushed afterwards.
	 */
	cmmu_flush_data_cache(cpu, src, PAGE_SIZE);
	bcopy((const void *)srcva, (void *)dstva, PAGE_SIZE);
	cmmu_flush_data_cache(cpu, dst, PAGE_SIZE);

	SPLX(spl);
}

/*
 * Routine:	PMAP_CHANGEBIT
 *
 * Function:
 *	Update the pte bits on the specified physical page.
 *
 * Parameters:
 *	pg	physical page
 *	set	bits to set
 *	mask	bits to mask
 *
 * Extern/Global:
 *	pv_lists
 *
 * Calls:
 *	pmap_pte
 *
 * The pte bits corresponding to the page's frame index will be changed as
 * requested. The PV list will be traversed.
 * For each pmap/va the hardware the necessary bits in the page descriptor
 * table entry will be altered as well if necessary. If any bits were changed,
 * a TLB flush will be performed.
 */
void
pmap_changebit(struct vm_page *pg, int set, int mask)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, npte, opte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	SPLVM(spl);

changebit_Retry:
	pvl = pg_to_pvh(pg);

	/*
	 * Clear saved attributes (modify, reference)
	 */
	pvl->pv_flags &= mask;

	if (pvl->pv_pmap == PMAP_NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_CBIT)
			printf("(pmap_changebit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return;
	}

	/* for each listed pmap, update the affected bits */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		if (!simple_lock_try(&pmap->pm_lock)) {
			goto changebit_Retry;
		}
		users = pmap->pm_cpus;
		kflush = pmap == kernel_pmap;

		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		/*
		 * Check for existing and valid pte
		 */
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_changebit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
#endif

		/*
		 * Update bits
		 */
		opte = *pte;
		npte = (opte | set) & mask;

		/*
		 * Flush TLB of which cpus using pmap.
		 *
		 * Invalidate pte temporarily to avoid the modified bit
		 * and/or the reference being written back by any other cpu.
		 */
		if (npte != opte) {
			invalidate_pte(pte);
			*pte = npte;
			flush_atc_entry(users, va, kflush);
		}
next:
		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);
}

/*
 * Routine:	PMAP_TESTBIT
 *
 * Function:
 *	Test the modified/referenced bits of a physical page.
 *
 * Parameters:
 *	pg	physical page
 *	bit	bit to test
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	simple_lock, simple_unlock
 *	pmap_pte
 *
 * If the attribute list for the given page has the bit, this routine
 * returns TRUE.
 *
 * Otherwise, this routine walks the PV list corresponding to the
 * given page. For each pmap/va pair, the page descriptor table entry is
 * examined. If the selected bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list), and updates the
 * attribute list.
 */
boolean_t
pmap_testbit(struct vm_page *pg, int bit)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte;
	int spl;

	SPLVM(spl);

testbit_Retry:
	pvl = pg_to_pvh(pg);

	if (pvl->pv_flags & bit) {
		/* we've already cached this flag for this page,
		   no use looking further... */
#ifdef DEBUG
		if (pmap_con_dbg & CD_TBIT)
			printf("(pmap_testbit: %x) already cached a %x flag for this page\n",
			    curproc, bit);
#endif
		SPLX(spl);
		return (TRUE);
	}

	if (pvl->pv_pmap == PMAP_NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_TBIT)
			printf("(pmap_testbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return (FALSE);
	}

	/* for each listed pmap, check modified bit for given page */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		if (!simple_lock_try(&pvep->pv_pmap->pm_lock)) {
			goto testbit_Retry;
		}

		pte = pmap_pte(pvep->pv_pmap, pvep->pv_va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;
		}

#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_testbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pvep->pv_pmap, pvep->pv_pmap == kernel_pmap ? 1 : 0, pg, VM_PAGE_TO_PHYS(pg));
#endif

		if ((*pte & bit) != 0) {
			simple_unlock(&pvep->pv_pmap->pm_lock);
			pvl->pv_flags |= bit;
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_TBIT | CD_FULL)) == (CD_TBIT | CD_FULL))
				printf("(pmap_testbit: %x) true on page pte@@0x%p\n", curproc, pte);
#endif
			SPLX(spl);
			return (TRUE);
		}
next:
		simple_unlock(&pvep->pv_pmap->pm_lock);
	}

	SPLX(spl);
	return (FALSE);
}

/*
 * Routine:	PMAP_UNSETBIT
 *
 * Function:
 *	Clears a pte bit and returns its previous state, for the
 *	specified physical page.
 *	This is an optimized version of:
 *		rv = pmap_testbit(pg, bit);
 *		pmap_changebit(pg, 0, ~bit);
 *		return rv;
 */
boolean_t
pmap_unsetbit(struct vm_page *pg, int bit)
{
	boolean_t rv = FALSE;
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, opte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	SPLVM(spl);

unsetbit_Retry:
	pvl = pg_to_pvh(pg);

	/*
	 * Clear saved attributes
	 */
	pvl->pv_flags &= ~bit;

	if (pvl->pv_pmap == PMAP_NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_USBIT)
			printf("(pmap_unsetbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return (FALSE);
	}

	/* for each listed pmap, update the specified bit */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		if (!simple_lock_try(&pmap->pm_lock)) {
			goto unsetbit_Retry;
		}
		users = pmap->pm_cpus;
		kflush = pmap == kernel_pmap;

		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		/*
		 * Check for existing and valid pte
		 */
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_unsetbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
#endif

		/*
		 * Update bits
		 */
		opte = *pte;
		if (opte & bit) {
			/*
			 * Flush TLB of which cpus using pmap.
			 *
			 * Invalidate pte temporarily to avoid the specified
			 * bit being written back by any other cpu.
			 */
			invalidate_pte(pte);
			*pte = opte ^ bit;
			flush_atc_entry(users, va, kflush);
		} else
			rv = TRUE;
next:
		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);

	return (rv);
}

/*
 * Routine:	PMAP_IS_MODIFIED
 *
 * Function:
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
	return pmap_testbit(pg, PG_M);
}

/*
 * Routine:	PMAP_IS_REFERENCED
 *
 * Function:
 *	Return whether or not the specified physical page is referenced by
 *	any physical maps.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	return pmap_testbit(pg, PG_U);
}

/*
 * Routine:	PMAP_PAGE_PROTECT
 *
 * Calls:
 *	pmap_changebit
 *	pmap_remove_all
 *
 *	Lower the permission for all mappings to a given page.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	if ((prot & VM_PROT_READ) == VM_PROT_NONE)
		pmap_remove_all(pg);
	else if ((prot & VM_PROT_WRITE) == VM_PROT_NONE)
		pmap_changebit(pg, PG_RO, ~0);
}

void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = virtual_avail;
	*endp = virtual_end;
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	int spl;
	pt_entry_t template, *pte;
	u_int users;

	CHECK_PAGE_ALIGN(va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_kenter_pa - PA");

#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT) {
		printf ("(pmap_kenter_pa: %x) va %x pa %x\n", curproc, va, pa);
	}
#endif

	PMAP_LOCK(kernel_pmap, spl);
	users = kernel_pmap->pm_cpus;

	template = m88k_protection(prot);
#ifdef M88110
	if (cputyp == CPU_88110 && m88k_protection(prot) != PG_RO)
		template |= PG_M;
#endif

	/*
	 * Expand pmap to include this pte.
	 */
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL)
		pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);

	/*
	 * And count the mapping.
	 */
	kernel_pmap->pm_stats.resident_count++;
	kernel_pmap->pm_stats.wired_count++;

	invalidate_pte(pte);
	if ((unsigned long)pa >= last_addr)
		template |= CACHE_INH | PG_V | PG_W;
	else
		template |= CACHE_GLOBAL | PG_V | PG_W;
	*pte = template | pa;
	flush_atc_entry(users, va, TRUE);

	PMAP_UNLOCK(kernel_pmap, spl);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	int spl;
	u_int users;
	vaddr_t e;

#ifdef DEBUG
	if (pmap_con_dbg & CD_RM)
		printf("(pmap_kremove: %x) va %x len %x\n", curproc, va, len);
#endif

	CHECK_PAGE_ALIGN(va, "pmap_kremove addr");
	CHECK_PAGE_ALIGN(len, "pmap_kremove len");

	PMAP_LOCK(kernel_pmap, spl);
	users = kernel_pmap->pm_cpus;

	e = va + len;
	for (; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;
		pt_entry_t *pte;

		sdt = SDTENT(kernel_pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
		}

		pte = pmap_pte(kernel_pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}

		/*
		 * Update the counts
		 */
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;

		invalidate_pte(pte);
		flush_atc_entry(users, va, TRUE);
	}
	PMAP_UNLOCK(map, spl);
}
@


1.117
log
@Typos in DEBUG code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116 2004/06/22 05:04:59 miod Exp $	*/
@


1.116
log
@Turn m88k_protection() into a macro again, compensating for 88110
quirks locally where necessary; pmap_enter() does most of the dirty work.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.115 2004/05/23 21:09:50 miod Exp $	*/
d1238 1
a1238 1
			printf("(pmap_remove: %x) pmap %x va %x\n", curproc, pmap, va);
d1354 1
a1354 1
			printf("(pmap_remove: %x) pmap kernel s %x e %x\n", curproc, s, e);
d1356 1
a1356 1
			printf("(pmap_remove: %x) pmap %x s %x e %x\n", curproc, pmap, s, e);
@


1.115
log
@Put back the fixed NCPUS == 1 version of flush_atc_entry().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.114 2004/05/07 15:30:04 miod Exp $	*/
d204 1
a204 1
void flush_atc_entry(long, vaddr_t, boolean_t);
d226 1
a226 29
/*
 * Convert machine-independent protection code to M88K protection bits.
 */
static __inline u_int32_t
m88k_protection(pmap_t pmap, vm_prot_t prot)
{
	pt_entry_t p;

	p = (prot & VM_PROT_WRITE) ? PG_RW : PG_RO;
	/*
	 * XXX this should not be necessary anymore now that pmap_enter
	 * does the correct thing... -- miod
	 */
#ifdef M88110
	if (cputyp == CPU_88110) {
		p |= PG_U;
		/* if the map is the kernel's map and since this
		 * is not a paged kernel, we go ahead and mark
		 * the page as modified to avoid an exception
		 * upon writing to the page the first time.  XXX smurph
		 */
		if (pmap == kernel_pmap) {
			if ((p & PG_RO) == 0)
				p |= PG_M;
		}
	}
#endif
	return p;
}
d242 1
a242 1
void
d253 1
a253 1
		panic("flush_atc_entry: invalid ff1 users = %d", ff1(tusers));
a328 3
 * Calls:
 *	m88k_protection
 *
d348 1
a348 1
	template = m88k_protection(kernel_pmap, prot) | SG_V;
d436 5
a440 1
	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;
a931 3
 * Calls:
 *	m88k_protection
 *
d955 2
a956 2
	*pte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V | pa;
d1022 1
a1022 2
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) |
	    CACHE_GLOBAL | APR_V;
d1564 1
a1564 1
	ap = m88k_protection(pmap, prot) & PG_PROT;
a1729 1
 *	m88k_protection
d1800 1
a1800 1
	template = m88k_protection(pmap, prot);
a2241 3
 * Calls:
 *	m88k_protection
 *
d2269 3
a2271 3
	*dstpte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V | dst;
	*srcpte = m88k_protection(kernel_pmap, VM_PROT_READ) |
d2650 5
a2654 1
	template = m88k_protection(kernel_pmap, prot);
@


1.114
log
@Do not rely upon a fictitious MAXPHYSMEM value, but rather the actual
physical memory size, to decide the end of /dev/*mem, as well as default
cacheability for mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.113 2004/05/07 08:00:16 miod Exp $	*/
d273 1
d275 3
a277 1
	long tusers = users;
d280 1
a280 1
	if ((tusers != 0) && (ff1(tusers) >= MAX_CPUS)) {
d285 1
a285 1
	while ((cpu = ff1(tusers)) != 32) {
d289 1
a289 1
		tusers &= ~(1 << cpu);
d291 4
@


1.113
log
@Compute the size of the kernel page table at runtime, depending upon the
board we run on, and its default mappings from pmap_table, rather than
trying to compute a "fits all" value at compile time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.112 2004/04/24 20:35:27 miod Exp $	*/
d71 1
d1925 1
a1925 1
	if ((unsigned long)pa >= MAXPHYSMEM)
d2693 1
a2693 1
	if ((unsigned long)pa >= MAXPHYSMEM)
@


1.112
log
@G/c CD_BOOT and the last related debug printf.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.111 2004/04/14 13:43:48 miod Exp $	*/
a142 35
 * MAX_KERNEL_VA_SIZE must fit into the virtual address space between
 * VM_MIN_KERNEL_ADDRESS and VM_MAX_KERNEL_ADDRESS.
 */

#define	MAX_KERNEL_VA_SIZE	(256*1024*1024)	/* 256 Mb */

/*
 * Size of kernel page tables, which is enough to map MAX_KERNEL_VA_SIZE
 */
#define	KERNEL_PDT_SIZE	(atop(MAX_KERNEL_VA_SIZE) * sizeof(pt_entry_t))

/*
 * Size of kernel page tables for mapping onboard IO space.
 */
#if defined(MVME188)
#define	M188_PDT_SIZE	(atop(UTIL_SIZE) * sizeof(pt_entry_t))
#else
#define	M188_PDT_SIZE 0
#endif

#if defined(MVME187) || defined(MVME197)
#define	M1x7_PDT_SIZE	(atop(OBIO_SIZE) * sizeof(pt_entry_t))
#else
#define	M1x7_PDT_SIZE 0
#endif

#if defined(MVME188) && (defined(MVME187) || defined(MVME197))
#define	OBIO_PDT_SIZE	((brdtyp == BRD_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
#else
#define	OBIO_PDT_SIZE	MAX(M188_PDT_SIZE, M1x7_PDT_SIZE)
#endif

#define MAX_KERNEL_PDT_SIZE	(KERNEL_PDT_SIZE + OBIO_PDT_SIZE)

/*
d662 1
a662 1
	vaddr_t vaddr, virt, kernel_pmap_size, pdt_size;
d665 1
d731 6
a736 2
	/* might as well round up to a page - XXX smurph */
	pdt_size = round_page(MAX_KERNEL_PDT_SIZE);
d846 2
a847 4
	ptable = pmap_table_build();

	for (; ptable->size != (vsize_t)(-1); ptable++){
		if (ptable->size) {
a851 1
	}
@


1.111
log
@Simplify pmap_bootstrap() interface.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.110 2004/01/28 13:04:57 miod Exp $	*/
a91 1
#define CD_BOOT		0x0000040	/* pmap_bootstrap */
a920 5
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("setting up mapping for Upage %d @@ %x\n", i, virt);
		}
#endif
@


1.110
log
@Use write-back mappings whenever possible again, but make sure that
vmapbuf() returns write-through mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.109 2004/01/15 23:36:08 miod Exp $	*/
d69 1
a69 1
extern vaddr_t	avail_start, avail_end;
a663 4
 *	&phys_start	PA of first available physical page
 *	&phys_end	PA of last available physical page
 *	&virtual_avail	VA of first available page (after kernel bss)
 *	&virtual_end	VA of last available page (end of kernel address space)
d681 1
a681 3
 * address space. The 'phys_start' parameter is adjusted upward to
 * reflect this allocation. This space is mapped in virtual memory
 * immediately following the kernel code/data map.
d694 1
a694 2
pmap_bootstrap(vaddr_t load_start, paddr_t *phys_start, paddr_t *phys_end,
    vaddr_t *virt_start, vaddr_t *virt_end)
a704 8
#ifdef DEBUG
	if (pmap_con_dbg & CD_BOOT) {
		printf("pmap_bootstrap: \"load_start\" 0x%x\n", load_start);
	}
	if (!PAGE_ALIGNED(load_start))
		panic("pmap_bootstrap: \"load_start\" not on the m88k page boundary: 0x%x", load_start);
#endif

d715 5
a719 5
	 *  kernelstart is the first symbol in the load image.
	 *  We link the kernel such that &kernelstart == 0x10000 (size of
	 *							BUG ROM)
	 *  The expression (&kernelstart - load_start) will end up as
	 *	0, making *virt_start == *phys_start, giving a 1-to-1 map)
d722 2
a723 2
	*phys_start = round_page(*phys_start);
	*virt_start = *phys_start +
d731 3
a733 10
	kmap = (sdt_entry_t *)(*phys_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)(*virt_start);
	kmapva = *virt_start;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kernel_pmap->pm_stab = 0x%x (pa 0x%x)\n",
		    kernel_pmap->pm_stab, kmap);
	}
#endif
d750 1
a750 3
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
	}
d755 2
a756 2
	*phys_start += kernel_pmap_size;
	*virt_start += kernel_pmap_size;
d759 2
a760 2
	*phys_start = round_page(*phys_start);
	*virt_start = round_page(*virt_start);
d763 2
a764 2
	kpdt_phys = *phys_start;
	kpdt_virt = (kpdt_entry_t)*virt_start;
d769 2
a770 2
	*phys_start += pdt_size;
	*virt_start += pdt_size;
d775 4
a778 7
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("--------------------------------------\n");
		printf("        kernel page start = 0x%x\n", kpdt_phys);
		printf("   kernel page table size = 0x%x\n", pdt_size);
		printf("          kernel page end = 0x%x\n", *phys_start);
	}
#endif
d780 1
a780 7
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kpdt_phys = 0x%x\n", kpdt_phys);
		printf("kpdt_virt = 0x%x\n", kpdt_virt);
		printf("end of kpdt at (virt)0x%08x, (phys)0x%08x\n",
		    *virt_start, *phys_start);
	}
d823 1
a823 6
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("(pmap_bootstrap) correcting vaddr\n");
		}
#endif
		while (vaddr < (*virt_start - kernel_pmap_size))
d826 1
a826 1
	vaddr = pmap_map(vaddr, (paddr_t)kmap, *phys_start,
d837 1
a837 1
		*phys_start = vaddr;
d841 1
a841 1
		vaddr = pmap_map(vaddr, *phys_start, *phys_start + etherlen,
d844 2
a845 2
		*virt_start += etherlen;
		*phys_start += etherlen;
d847 3
a849 9
		if (vaddr != *virt_start) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("2: vaddr %x *virt_start %x *phys_start %x\n", vaddr,
				    *virt_start, *phys_start);
			}
#endif
			*virt_start = vaddr;
			*phys_start = round_page(*phys_start);
d855 2
a856 2
	*virt_start = round_page(*virt_start);
	*virt_end = VM_MAX_KERNEL_ADDRESS;
d862 4
a865 4
	phys_map_vaddr = *virt_start;
	phys_map_vaddr_end = *virt_start + 2 * (max_cpus << PAGE_SHIFT);
	*phys_start += 2 * (max_cpus << PAGE_SHIFT);
	*virt_start += 2 * (max_cpus << PAGE_SHIFT);
a877 5
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("pmap_bootstrap: -> pmap_table_build\n");
	}
#endif
d904 1
a904 1
	virt = *virt_start;
d911 1
a911 1
	*virt_start = virt;
d937 1
a937 5
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		show_apr(kernel_pmap->pm_apr);
	}
#endif
d946 1
a946 3
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("Processor %d running virtual.\n", i);
			}
a949 6

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("running virtual - avail_next 0x%x\n", *phys_start);
	}
#endif
@


1.109
log
@Disable write-back mappings for userland entirely; pmap_copy_page() still
misbehaves in some circumstances if they are enabled, despite the code
doing (apparenty) The Right Thing.

Reasonable performance hit, to be offset by further diffs in the pipeline;
discussed with deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.108 2004/01/09 00:31:01 miod Exp $	*/
a66 5
 * Define this to allow write-back caching. Use at your own risk.
 */
#undef	PMAP_USE_WRITEBACK

/*
a1091 1
#ifdef	PMAP_USE_WRITEBACK
a1092 1
#endif
d1145 1
a1145 4
	    CACHE_GLOBAL | CACHE_WT | APR_V;
#ifdef	PMAP_USE_WRITEBACK
	pmap->pm_apr &= ~CACHE_WT;	/* enable writeback */
#endif
a1157 1
#ifdef	PMAP_USE_WRITEBACK
a1160 1
#endif
a1785 1
#ifdef	PMAP_USE_WRITEBACK
a1788 1
#endif
d2390 1
a2390 1
	srcva = (vaddr_t)(phys_map_vaddr + PAGE_SIZE + 2 * (cpu << PAGE_SHIFT));
a2412 1
#ifdef	PMAP_USE_WRITEBACK
a2413 1
#endif
a2414 1
#ifdef	PMAP_USE_WRITEBACK
a2415 1
#endif
@


1.108
log
@Put back the pmap_zero_page() and pmap_copy_page() fixes, svnd work again.

We need to map the temporary pages write through _and_ flush the data cache,
for split user/supervisor designs.

Bring back a few minor style and cosmetic fixes while there, too.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.107 2004/01/08 20:31:44 miod Exp $	*/
d67 5
d1083 6
a1088 1
	    CACHE_GLOBAL | CACHE_WT | PG_V | pa;
d1097 1
d1099 1
d1151 5
a1155 1
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) | CACHE_GLOBAL | APR_V;
d1168 1
d1172 1
d1798 1
d1802 1
d2411 1
a2411 1
	    CACHE_GLOBAL | CACHE_WT | PG_V | dst;
d2413 6
a2418 1
	    CACHE_GLOBAL | CACHE_WT | PG_V | src;
d2427 1
d2429 1
d2431 1
d2433 1
@


1.107
log
@The va for the few pages used for copying/zeroing pages are magic, but only
in the kernel_pmap; and they are perfectly legal in user pmaps, so treat them
correctly.

While there, allocate them differently so that pmap_copy_page() only needs
one double-size tlb flush, rather than two single-size.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.106 2004/01/07 23:43:54 miod Exp $	*/
a866 16
#ifdef DEBUG
	if (vaddr != *virt_start) {
		/*
		 * This should never happen because we now round the PDT
		 * table size up to a page boundry in the quest to get
		 * mc88110 working. - XXX smurph
		 */
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("1: vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
			    *virt_start, *phys_start);
		}
		*virt_start = vaddr;
		*phys_start = round_page(*phys_start);
	}
#endif

d984 2
a985 1
	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) | CACHE_WT | APR_V;
d994 2
a995 1
			cmmu_flush_tlb(i, 1, 0, -1);
d1065 1
a1065 1
	vaddr_t srcva;
d1067 2
a1068 2
	int cpu;
	pt_entry_t *srcpte;
d1072 2
a1073 3
	cpu = cpu_number();
	srcva = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	srcpte = pmap_pte(kernel_pmap, srcva);
a1075 5
	cmmu_flush_tlb(cpu, TRUE, srcva, PAGE_SIZE);
	*srcpte = pa |
	    m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
	SPLX(spl);
d1077 10
a1086 2
	bzero((void *)srcva, PAGE_SIZE);
	/* force the data out */
d1088 2
d1154 1
a1154 1
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_WT | CACHE_GLOBAL);
d1782 1
a1782 1
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_WT | CACHE_GLOBAL);
d2310 2
a2311 1
		cmmu_flush_tlb(cpu, FALSE, 0, -1);
d2377 1
a2377 1
	pt_entry_t template, *dstpte, *srcpte;
a2382 3
	template = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;

d2389 5
a2394 3
	*srcpte = template | src;
	*dstpte = template | dst;
	SPLX(spl);
d2396 7
a2403 2
	/* flush source, dest out of cache? */
	cmmu_flush_data_cache(cpu, src, PAGE_SIZE);
d2405 2
d2809 1
a2809 1
	e = va + round_page(len);
@


1.106
log
@Revert the pmap machinery to 20031228. The changes made since all produce
different subtle races, and need more tinkering until they are really safe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2003/12/23 00:53:17 miod Exp $	*/
d182 1
a182 1
vaddr_t phys_map_vaddr1, phys_map_vaddr2, phys_map_vaddr_end;
d674 1
a674 2
 *	phys_map_vaddr1 VA of page mapped arbitrarily for debug/IO
 *	phys_map_vaddr2 VA of page mapped arbitrarily for debug/IO
d919 1
a919 1
	 * Map a few more pages for phys routines and debugger.
d922 4
a925 11
	phys_map_vaddr1 = round_page(*virt_start);
	phys_map_vaddr2 = phys_map_vaddr1 + (max_cpus << PAGE_SHIFT);
	phys_map_vaddr_end = phys_map_vaddr2 + 2 * (max_cpus << PAGE_SHIFT);

	/*
	 * To make 1:1 mapping of virt:phys, throw away a few phys pages.
	 * XXX what is this? nivas
	 */

	*phys_start += 2 * PAGE_SIZE * max_cpus;
	*virt_start += 2 * PAGE_SIZE * max_cpus;
d1006 1
a1006 2
	/* Invalidate entire kernel TLB. */

a1008 1
			/* Invalidate entire kernel TLB. */
a1009 12
			/* still physical */
			/*
			 * Set valid bit to DT_INVALID so that the very first
			 * pmap_enter() on these won't barf in
			 * pmap_remove_range().
			 */
			pte = pmap_pte(kernel_pmap,
			    phys_map_vaddr1 + (i << PAGE_SHIFT));
			invalidate_pte(pte);
			pte = pmap_pte(kernel_pmap,
			    phys_map_vaddr2 + (i << PAGE_SHIFT));
			invalidate_pte(pte);
d1063 1
a1063 1
 *	phys_map_vaddr1
d1087 1
a1087 1
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
d1920 6
d1967 1
a1967 4
		if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end)
			flush_atc_entry(users, va, TRUE);
		else
			pmap_remove_pte(pmap, va, pte);
a1969 12
#ifdef DEBUG
		if (pmap_con_dbg & CD_ENT) {
			if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end) {
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n",
				    phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
				    pg != NULL ? 1 : 0);
				printf("pte %x pfn %x valid %x\n",
				    pte, PG_PFNUM(*pte), PDT_VALID(pte));
			}
		}
#endif

a1970 7
#ifdef DEBUG
			if (pmap_con_dbg & CD_ENT) {
				if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end) {
					printf("va 0x%x and managed pa 0x%x\n", va, pa);
				}
			}
#endif
d2367 1
a2367 2
 *	phys_map_vaddr1
 *	phys_map_vaddr2
d2395 3
a2397 6
	/*
	 * Map source physical address.
	 */
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	dstva = (vaddr_t)(phys_map_vaddr2 + (cpu << PAGE_SHIFT));

a2398 1
	dstpte = pmap_pte(kernel_pmap, dstva);
d2401 1
a2401 1
	cmmu_flush_tlb(cpu, TRUE, srcva, PAGE_SIZE);
a2402 5

	/*
	 * Map destination physical address.
	 */
	cmmu_flush_tlb(cpu, TRUE, dstva, PAGE_SIZE);
d2406 1
a2406 1
	bcopy((void *)srcva, (void *)dstva, PAGE_SIZE);
d2556 2
a2557 2
			printf("(pmap_testbit: %x) already cached a modify flag for this page\n",
			    curproc);
@


1.105
log
@Stop masquerading the M8120 (no-slot MVME187) as a real MVME187, and keep
its value in brdtyp.

Compensate by checking for BRD_8120 everywhere BRD_187 was checked.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.104 2004/01/04 22:51:55 miod Exp $	*/
d488 1
a488 1
#ifdef DIAGNOSTIC
d1008 1
a1008 2
	kernel_pmap->pm_apr =
	    (atop(kmap) << PG_SHIFT) | CACHE_WT | CACHE_GLOBAL | APR_V;
d1019 1
a1019 2
			cmmu_flush_tlb(i, TRUE, VM_MIN_KERNEL_ADDRESS,
			    VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS);
d1171 1
a1171 2
	pmap->pm_apr =
	    (atop(stpa) << PG_SHIFT) | CACHE_WT | CACHE_GLOBAL | APR_V;
d1498 1
a1498 1
	vaddr_t va, seva;
d1512 1
a1512 1
	for (va = s; va < e;) {
a1515 1
		seva = (va & SDT_MASK) + (1 << SDT_SHIFT);
d1519 2
a1520 1
			va = seva;
d1524 1
a1524 4
		if (seva > e)
			seva = e;
		for (; va < seva; va += PAGE_SIZE)
			pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
d1558 1
a1558 1
#ifdef DIAGNOSTIC
d1645 12
a1656 3
			pvl = pvl->pv_next;	/* no page mapping */
		} else {
			pmap_remove_pte(pmap, va, pte);
d1658 1
a1658 5
			/*
			 * Do not free any page tables,
			 * leave that for when VM calls pmap_collect().
			 */
		}
d1660 5
d1699 1
a1699 1
	vaddr_t va, seva;
d1703 1
a1703 1
#ifdef DIAGNOSTIC
d1708 1
a1708 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
d1725 1
a1725 1
	for (va = s; va < e;) {
a1728 1
		seva = (va & SDT_MASK) + (1 << SDT_SHIFT);
d1732 2
a1733 1
			va = seva;
d1737 4
a1740 7
		if (seva > e)
			seva = e;
		for (; va < seva; va += PAGE_SIZE) {
			pte = pmap_pte(pmap, va);
			if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
				continue;	 /* no page mapping */
			}
d1742 8
a1749 11
			if ((*pte & PG_PROT) ^ ap) {
				/*
				 * Invalidate pte temporarily to avoid the
				 * modified bit and/or the reference bit being
				 * written back by any other cpu.
				 */
				*pte = (invalidate_pte(pte) ^ PG_PROT);
				flush_atc_entry(users, va, kflush);
			}
			pte++;
		}
d1924 1
a1924 1
	pt_entry_t *pte, opte, template;
d1968 1
a1968 2
	opte = *pte;
	old_pa = ptoa(PG_PFNUM(opte));
d1971 1
a1971 1
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, opte);
d1975 1
a1975 1
		if (wired && !(pmap_pte_w(&opte)))
d1977 1
a1977 1
		else if (!wired && pmap_pte_w(&opte))
d1979 2
d1982 5
a1986 4
		if (old_pa != NULL) {
			if (va < phys_map_vaddr1 || va >= phys_map_vaddr_end)
				pmap_remove_pte(pmap, va, pte);
		}
d1996 1
a1996 1
				    pte, PG_PFNUM(opte), PDT_VALID(&opte));
a2051 8

			/*
			 * Cache attribute flags
			 */
			if (flags & VM_PROT_WRITE)
				pvl->pv_flags |= PG_U | PG_M;
			else if (flags & VM_PROT_ALL)
				pvl->pv_flags |= PG_U;
d2071 5
a2075 1
	template |= (opte & (PG_U | PG_M)) | pa;
d2077 3
a2079 1
	 * No need to flush anything if it's just a wiring change...
d2081 3
a2083 12
	if ((template ^ opte) == PG_W) {
		*pte = template;
	} else {
		/*
		 * Invalidate pte temporarily to avoid being written
		 * back the modified bit and/or the reference bit by
		 * any other cpu.
		 */
		invalidate_pte(pte);
		*pte = template;
		flush_atc_entry(users, va, kflush);
	}
d2086 1
a2086 1
		printf("(pmap_enter) set pte to %x\n", template);
d2089 6
d2123 1
a2123 3
	pte = pmap_pte(pmap, v);
#ifdef DEBUG
	if (pte == PT_ENTRY_NULL)
a2124 1
#endif
d2127 1
a2127 1
		/* unwire mapping */
d2172 1
a2172 1
#ifdef DEBUG
d2322 1
a2322 1
 *	Then, it flushes the TLBs mapping user virtual space, in the CMMUs
d2358 1
a2366 6
#ifdef	PMAP_USE_BATC
	else
#endif

	cmmu_flush_tlb(cpu, FALSE, VM_MIN_ADDRESS,
	    VM_MAX_ADDRESS - VM_MIN_ADDRESS);
d2847 1
a2847 1
	vaddr_t e, seva;
d2860 2
a2861 1
	for (e = va + len; va < e;) {
a2865 1
		seva = (va & SDT_MASK) + (1 << SDT_SHIFT);
d2869 2
a2870 1
			va = seva;
d2874 4
a2877 7
		if (seva > e)
			seva = e;
		for (; va < seva; va += PAGE_SIZE) {
			pte = pmap_pte(kernel_pmap, va);
			if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
				continue;	 /* no page mapping */
			}
d2879 5
a2883 5
			/*
			 * Update the counts
			 */
			kernel_pmap->pm_stats.resident_count--;
			kernel_pmap->pm_stats.wired_count--;
d2885 2
a2886 3
			invalidate_pte(pte);
			flush_atc_entry(users, va, TRUE);
		}
@


1.104
log
@In pmap_activate(), be sure to always flush the user tlb, even if we are
scheduling a kernel thread, as we depend upon this behaviour now.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.103 2004/01/02 23:25:18 miod Exp $	*/
d891 1
a891 1
	if (brdtyp == BRD_187 || brdtyp == BRD_197) {
@


1.103
log
@Revert 1.100 and 1.102 for now - they cause page table corruption (bloody hell!)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2003/12/28 14:10:58 miod Exp $	*/
d1020 2
a1021 1
			cmmu_flush_tlb(i, 1, 0, -1);
d2330 2
a2331 2
 *	If kernel_pmap is specified, only flushes the TLBs mapping kernel
 *	virtual space, in the CMMUs connected to the specified CPU.
a2365 1
		cmmu_flush_tlb(cpu, FALSE, 0, -1);
d2374 6
@


1.102
log
@The scratch pages used in pmap_zero_page() and pmap_copy_page() being special
mappings, they still need the dcache to be invalidated after use.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.101 2004/01/02 17:08:58 miod Exp $	*/
d1009 1
a1009 1
	    (atop(kmap) << PG_SHIFT) | CACHE_GLOBAL | CACHE_WT | APR_V;
d1102 1
a1102 1
	vaddr_t va;
d1104 2
a1105 2
	int cpu = cpu_number();
	pt_entry_t *pte;
d1109 3
a1111 2
	va = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	pte = pmap_pte(kernel_pmap, va);
d1114 4
a1117 5

	cmmu_flush_tlb(cpu, TRUE, va, PAGE_SIZE);
	*pte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_WT | CACHE_GLOBAL | PG_V | pa;

d1120 2
a1121 1
	bzero((void *)va, PAGE_SIZE);
d2432 1
a2432 1
	pt_entry_t *dstpte, *srcpte;
d2438 6
d2446 1
a2450 1

d2452 1
a2452 2
	*srcpte = m88k_protection(kernel_pmap, VM_PROT_READ) |
	    CACHE_WT | CACHE_GLOBAL | PG_V | src;
d2454 3
d2458 1
a2458 3
	*dstpte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_WT | CACHE_GLOBAL | PG_V | dst;

d2462 1
@


1.101
log
@When both cmmu_dofoo() and cmmu_remote_dofoo() exist, kill the first one,
and rename the second one to the first one, i.e. have the cmmu_dofoo()
functions always take a cpu# parameter.

No functional change, simply makes code more readable and saves a few
call frames.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2003/12/30 06:45:55 miod Exp $	*/
d1121 1
d2455 2
@


1.100
log
@In pmap_copy_page() and pmap_zero_page(), it is not enough to flush the
cache for the affected pages; force the ptes in write through instead.

This gets rid of the corrupted svnd issues encountered randomly (but
frequently) on mvme88k.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2003/12/28 14:10:58 miod Exp $	*/
d319 1
a319 1
			cmmu_flush_remote_tlb(cpu, kernel, va, PAGE_SIZE);
d648 1
a648 1
				cmmu_flush_remote_cache(cpu, pa, PAGE_SIZE);
d1020 1
a1020 1
			cmmu_flush_remote_tlb(i, 1, 0, -1);
d1034 1
a1034 1
			cmmu_remote_set_sapr(i, kernel_pmap->pm_apr);
d1114 1
a1114 1
	cmmu_flush_tlb(TRUE, va, PAGE_SIZE);
d2363 1
a2363 1
		cmmu_flush_tlb(FALSE, 0, -1);
d2443 1
a2443 1
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
d2447 1
a2447 1
	cmmu_flush_tlb(TRUE, dstva, PAGE_SIZE);
@


1.99
log
@More optimizations borrowed from the m68k pmap:
- in loops over va space, do the empty segment test only once per segment
- do not flush tlb for wiring-only changes
While there:
- in pmap_remove_all(), do not treat wired pages special.
- move more paranoid tests from DIAGNOSTIC to DEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2003/12/23 00:53:17 miod Exp $	*/
d893 1
a893 1
		etherlen = ETHERPAGES * NBPG;
d1008 2
a1009 1
	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) | CACHE_WT | APR_V;
d1102 1
a1102 1
	vaddr_t srcva;
d1104 2
a1105 2
	int cpu;
	pt_entry_t *srcpte;
d1109 2
a1110 3
	cpu = cpu_number();
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	srcpte = pmap_pte(kernel_pmap, srcva);
d1113 5
a1117 4
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
	*srcpte = pa |
	    m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
d1120 1
a1120 3
	bzero((void *)srcva, PAGE_SIZE);
	/* force the data out */
	cmmu_flush_remote_data_cache(cpu, pa, PAGE_SIZE);
d1170 2
a1171 1
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) | CACHE_GLOBAL | APR_V;
d2430 1
a2430 1
	pt_entry_t template, *dstpte, *srcpte;
a2435 6
	template = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;

	/*
	 * Map source physical address.
	 */
a2437 1

d2442 1
d2444 2
a2445 1
	*srcpte = template | src;
a2446 3
	/*
	 * Map destination physical address.
	 */
d2448 3
a2450 1
	*dstpte = template | dst;
a2453 3
	/* flush source, dest out of cache? */
	cmmu_flush_remote_data_cache(cpu, src, PAGE_SIZE);
	cmmu_flush_remote_data_cache(cpu, dst, PAGE_SIZE);
@


1.98
log
@style
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2003/12/19 22:30:18 miod Exp $	*/
d346 1
a346 1
#ifdef DIAGNOSTIC
d980 1
a980 1
	*vmpte = PG_NV;
d1135 1
a1135 1
struct pmap *
d1306 1
a1306 1
#ifdef DIAGNOSTIC
d1498 1
a1498 1
	vaddr_t va;
d1512 1
a1512 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1516 1
d1520 1
a1520 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d1524 4
a1527 1
		pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
d1648 8
a1655 11
			pvl = pvl->pv_next;
			goto next;	/* no page mapping */
		}
		if (pmap_pte_w(pte)) {
#ifdef DEBUG
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
				    pg);
#endif
			pvl = pvl->pv_next;
			goto next;
a1657 7
		pmap_remove_pte(pmap, va, pte);

		/*
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
		 */
next:
d1692 1
a1692 1
	vaddr_t va;
d1696 6
a1701 1
	if ((prot & VM_PROT_READ) == 0) {
a1705 5
#ifdef DIAGNOSTIC
	if (s > e)
		panic("pmap_protect: start grater than end address");
#endif

d1718 1
a1718 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1722 1
d1726 1
a1726 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d1730 18
a1747 3
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
a1748 9

		/*
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
		 * written back by any other cpu.
		 */
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
		flush_atc_entry(users, va, kflush);
		pte++;
d1923 1
a1923 1
	pt_entry_t *pte, template;
d1967 2
a1968 1
	old_pa = ptoa(PG_PFNUM(*pte));
d1971 1
a1971 1
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, *pte);
d1975 1
a1975 1
		if (wired && !(pmap_pte_w(pte)))
d1977 1
a1977 1
		else if (!wired && pmap_pte_w(pte))
a1978 2

		pvl = NULL;
d1980 4
a1983 5
		/* Remove old mapping from the PV list if necessary. */
		if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end)
			flush_atc_entry(users, va, TRUE);
		else
			pmap_remove_pte(pmap, va, pte);
d1993 1
a1993 1
				    pte, PG_PFNUM(*pte), PDT_VALID(pte));
d2049 8
d2076 1
a2076 5
	if (flags & VM_PROT_WRITE)
		template |= PG_U | PG_M;
	else if (flags & VM_PROT_ALL)
		template |= PG_U;

d2078 1
a2078 3
	 * Invalidate pte temporarily to avoid being written
	 * back the modified bit and/or the reference bit by
	 * any other cpu.
d2080 12
a2091 3
	template |= invalidate_pte(pte) & (PG_U | PG_M);
	*pte = template | pa;
	flush_atc_entry(users, va, kflush);
d2094 1
a2094 1
		printf("(pmap_enter) set pte to %x\n", *pte);
a2096 6
	/*
	 * Cache attribute flags
	 */
	if (pvl != NULL)
		pvl->pv_flags |= (template & (PG_U | PG_M));

d2125 3
a2127 1
	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
d2129 1
d2132 1
a2132 1
		/* unwired mapping */
d2177 1
a2177 1
#ifdef DIAGNOSTIC
d2790 1
a2790 1
	if (!(prot & VM_PROT_READ))
d2792 1
a2792 2
	else if (!(prot & VM_PROT_WRITE))
		/* copy on write */
d2852 1
a2852 1
	vaddr_t e;
d2865 1
a2865 2
	e = va + round_page(len);
	for (; va < e; va += PAGE_SIZE) {
d2870 1
d2874 1
a2874 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d2878 7
a2884 4
		pte = pmap_pte(kernel_pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}
d2886 5
a2890 5
		/*
		 * Update the counts
		 */
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;
d2892 3
a2894 2
		invalidate_pte(pte);
		flush_atc_entry(users, va, TRUE);
@


1.97
log
@Pass -Wformat, and silence a few vme* debug printf.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2003/12/19 21:25:03 miod Exp $	*/
d624 1
a624 5
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}
d1396 1
a1396 5
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}
d1718 1
a1718 5
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}
d1948 1
a1948 6

	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}
d1974 1
a1974 4
		/*
		 * May be changing its wired attributes or protection
		 */

d1980 1
a1980 1
		pg = NULL;
d1982 2
a1983 6

		/*
		 * Remove old mapping from the PV list if necessary.
		 */

		if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end) {
d1985 1
a1985 1
		} else {
a1986 1
		}
d2027 1
a2027 1
				 * check that this mapping is not already there
d2029 4
a2032 8
				{
					pv_entry_t e = pvl;
					while (e != PV_ENTRY_NULL) {
						if (e->pv_pmap == pmap && e->pv_va == va)
							panic("pmap_enter: already in pv_list");
						e = e->pv_next;
					}
				}
d2092 1
a2092 2
	if (pg != NULL) {
		pvl = pg_to_pvh(pg);
a2093 1
	}
d2521 1
a2521 5
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}
d2706 1
a2706 5
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}
@


1.96
log
@The physical address of a pmap's segment table is only used to compute an
apr value used in pmap_activate(). So, rather than storing the address in the
pmap structure, store the precomputed apr value...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2003/12/19 18:08:23 miod Exp $	*/
d1447 1
a1447 1
		    "0x%x (pa 0x%x) not in pv list at 0x%p",
d1961 6
a1990 6
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}

d2000 1
d2569 1
a2569 1
			panic("pmap_changebit: pte %x in pmap %x %d doesn't point to page %x %x",
d2669 1
a2669 1
			panic("pmap_testbit: pte %x in pmap %x %d doesn't point to page %x %x",
d2758 1
a2758 1
			panic("pmap_unsetbit: pte %x in pmap %x %d doesn't point to page %x %x",
@


1.95
log
@When temporarily unlocking a pmap, do not release the spl as well.
While there, get rid of that ugly PT_FREE() macro.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2003/12/14 22:08:02 miod Exp $	*/
a711 1
	u_int32_t apr_data;
d751 1
a751 1
	kernel_pmap->pm_stpa = kmap = (sdt_entry_t *)(*phys_start);
d757 2
a758 2
		printf("kernel_pmap->pm_stpa = 0x%x\n", kernel_pmap->pm_stpa);
		printf("kernel_pmap->pm_stab = 0x%x\n", kernel_pmap->pm_stab);
a777 1
		printf("     kernel segment start = 0x%x\n", kernel_pmap->pm_stpa);
a778 1
		printf("       kernel segment end = 0x%x\n", ((paddr_t)kernel_pmap->pm_stpa) + kernel_pmap_size);
d1012 1
a1012 1
	apr_data = (atop(kernel_pmap->pm_stpa) << PG_SHIFT) | CACHE_WT | APR_V;
d1015 1
a1015 1
		show_apr(apr_data);
a1018 5
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("invalidating tlb %x\n", apr_data);
	}
#endif
d1037 1
a1037 1
			cmmu_remote_set_sapr(i, apr_data);
d1144 1
d1173 1
a1173 1
	    (paddr_t *)&pmap->pm_stpa) == FALSE)
d1175 1
d1178 1
a1178 1
	if (!PAGE_ALIGNED(pmap->pm_stpa))
d1180 1
a1180 1
		    (int)pmap->pm_stpa);
d1183 2
a1184 2
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x, pm_stpa=0x%x\n",
		    curproc, pmap, pmap->pm_stab, pmap->pm_stpa);
d2342 1
a2342 6
 * 	Binds the given physical map to the given
 *	processor, and returns a hardware map description.
 *	In a mono-processor implementation the cpu
 *	argument is ignored, and the PMAP_ACTIVATE macro
 *	simply sets the MMU root pointer element of the PCB
 *	to the physical address of the segment descriptor table.
d2348 2
a2349 2
 *	If the specified pmap is not kernel_pmap, this routine makes arp
 *	template and stores it into UAPR (user area pointer register) in the
a2353 1
 *
a2357 1
	u_int32_t apr_data;
a2359 1

a2372 1

a2373 2
		apr_data = (atop(pmap->pm_stpa) << PG_SHIFT) |
		    CACHE_GLOBAL | APR_V;
d2382 1
a2382 1
		cmmu_pmap_activate(cpu, apr_data,
d2387 1
a2387 1
		cmmu_set_uapr(apr_data);
a2394 1

d2403 1
a2403 2
 *	Unbinds the given physical map from the given processor,
 *	i.e. the pmap i no longer is use on the processor.
a2406 3
 *
 * pmap_deactive simply clears the pm_cpus field in given pmap structure.
 *
d2442 1
a2442 1
 *	no locking reauired
@


1.94
log
@Replace pmap_testbit/pmap_changebit sequences with a tailor-made pmap_unsetbit
routine, in order to speed up pmap_clear_modify() and pmap_clear_reference().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2003/12/14 22:06:39 miod Exp $	*/
d355 1
a355 1
	 * Check whether page table is exist or not.
d1173 1
a1173 1
		panic("pmap_create: kmem_alloc failure");
d1202 2
a1203 2
	 * There is no need to clear segment table, since kmem_alloc would
	 * provide us clean pages.
a1235 1
 *	PT_FREE
d1242 1
a1242 1
 * all translation table space back to the system using PT_FREE.
d1268 1
a1268 1
			PT_FREE(gdttbl);
d1782 1
a1782 1
 *	that checks whether the map has been expanded enough. ( We won't loop
d1841 1
a1841 2
		PMAP_UNLOCK(pmap, spl);
		/* XXX */
d1848 1
a1900 1
 *	PT_FREE
d1977 1
a1977 1
			PMAP_UNLOCK(pmap, spl);
d1979 1
a1979 1
			PMAP_LOCK(pmap, spl);
d2006 1
a2006 1
	} else { /* if ( pa == old_pa) */
a2255 1
 *	PT_FREE
d2258 1
d2269 1
a2269 1
 * invalidated. Finally, PT_FREE is called to return the page to the
a2273 4
 *
 *	[Note: depending upon compilation options, tables may be in zones
 * or allocated through kmem_alloc. In the former case, the
 * module deals with a single table at a time.]
d2328 2
a2329 3
		 * we have to unlock before freeing the table, since PT_FREE
		 * calls uvm_km_free or free, which will invoke another
		 * pmap routine
d2331 3
a2333 3
		PMAP_UNLOCK(pmap, spl);
		PT_FREE(gdttbl);
		PMAP_LOCK(pmap, spl);
@


1.93
log
@Remove unnecessary trunc_page() calls, and change pmap_enter() logic to
match m68k.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2003/11/03 06:54:26 david Exp $	*/
d106 1
a106 1
#define CD_CREF		0x0100000	/* pmap_clear_reference */
d249 1
a2624 16
 * Routine:	PMAP_CLEAR_MODIFY
 *
 * Function:
 *	Clears the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	boolean_t rv;

	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
	return rv;
}

/*
d2659 1
a2660 1
testbit_Retry:
d2720 1
a2720 1
 * Routine:	PMAP_IS_MODIFIED
d2723 6
a2728 2
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d2731 1
a2731 1
pmap_is_modified(struct vm_page *pg)
d2733 79
a2811 1
	return pmap_testbit(pg, PG_M);
d2815 1
a2815 1
 * Routine:	PMAP_CLEAR_REFERENCE
d2818 2
a2819 1
 *	Clear the reference bit on the specified physical page.
d2822 1
a2822 1
pmap_clear_reference(struct vm_page *pg)
d2824 1
a2824 5
	boolean_t rv;

	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
	return rv;
@


1.92
log
@spelling fixes (in the comments)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2003/10/28 21:43:44 miod Exp $	*/
a114 1

d116 1
a116 1
#define	CHECK_PAGE_ALIGN(ad,who) \
d122 1
a122 1
#define	CHECK_PAGE_ALIGN(ad,who)
d1039 1
a1039 1
			*pte = PG_NV;
d1042 1
a1042 1
			*pte = PG_NV;
d1117 2
d1125 1
a1125 1
	*srcpte = trunc_page(pa) |
d1386 1
a1386 1
	pv_entry_t prev, cur, pvl = PV_ENTRY_NULL;
d1430 9
a1438 6
	if (pg != NULL) {
		/*
		 * Remove the mapping from the pvlist for
		 * this physical page.
		 */
		pvl = pg_to_pvh(pg);
d1441 2
a1442 2
		if (pvl->pv_pmap == PMAP_NULL)
			panic("pmap_remove_pte: null pv_list");
d1445 11
a1455 12
		prev = PV_ENTRY_NULL;
		for (cur = pvl; cur != PV_ENTRY_NULL;
		    cur = cur->pv_next) {
			if (cur->pv_va == va && cur->pv_pmap == pmap)
				break;
			prev = cur;
		}
		if (cur == PV_ENTRY_NULL) {
			panic("pmap_remove_pte: mapping for va "
			    "0x%x (pa 0x%x) not in pv list at 0x%p",
			    va, pa, pvl);
		}
d1457 11
a1467 14
		if (prev == PV_ENTRY_NULL) {
			/*
			 * Hander is the pv_entry. Copy the next one
			 * to hander and free the next one (we can't
			 * free the hander)
			 */
			cur = cur->pv_next;
			if (cur != PV_ENTRY_NULL) {
				cur->pv_flags = pvl->pv_flags;
				*pvl = *cur;
				pool_put(&pvpool, cur);
			} else {
				pvl->pv_pmap = PMAP_NULL;
			}
d1469 1
a1469 2
			prev->pv_next = cur->pv_next;
			pool_put(&pvpool, cur);
d1471 4
a1474 1
	} /* if (pg != NULL) */
d1476 2
a1477 7
	/*
	 * Reflect modify bits to pager.
	 */

	if (opte != 0 && pvl != PV_ENTRY_NULL) {
		pvl->pv_flags |= opte;
	}
d1943 1
a1943 1
	pt_entry_t *pte, ap, template;
d1951 2
a1952 2
	CHECK_PAGE_ALIGN(va, "pmap_entry - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - PA");
d1963 1
a1963 1
	ap = m88k_protection(pmap, prot);
d1984 1
a1984 2
	 *	Special case if the physical page is already mapped
	 *	at this address.
d2007 1
a2007 6
		if ((unsigned long)pa >= MAXPHYSMEM)
			template = CACHE_INH | PG_V;
		else
			template = CACHE_GLOBAL | PG_V;
		if (wired)
			template |= PG_W;
a2009 21
		 * If there is a same mapping, we have nothing to do.
		 */
		if (!PDT_VALID(pte) || pmap_pte_w_chg(pte, template & PG_W) ||
		    (pmap_pte_prot_chg(pte, ap & PG_PROT))) {

			/*
			 * Invalidate pte temporarily to avoid being written
			 * back the modified bit and/or the reference bit by
			 * any other cpu.
			 */
			template |= (invalidate_pte(pte) & (PG_M | PG_U));
			*pte = template | ap | trunc_page(pa);
			flush_atc_entry(users, va, kflush);
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) update pte to %x\n", *pte);
#endif
		}

	} else { /* if ( pa == old_pa) */
		/*
d2041 1
a2041 1
			 *	Enter the mappimg in the PV list for this
d2095 1
d2097 8
a2104 6
		if ((unsigned long)pa >= MAXPHYSMEM)
			template = CACHE_INH | PG_V;
		else
			template = CACHE_GLOBAL | PG_V;
		if (wired)
			template |= PG_W;
d2106 4
a2109 4
		if (flags & VM_PROT_WRITE)
			template |= PG_U | PG_M;
		else if (flags & VM_PROT_ALL)
			template |= PG_U;
d2111 8
a2118 1
		*pte = template | ap | trunc_page(pa);
d2124 7
a2130 1
	} /* if (pa == old_pa) ... else */
d2486 3
d2503 1
a2503 1
	*srcpte = template | trunc_page(src);
d2509 1
a2509 1
	*dstpte = template | trunc_page(dst);
d2603 1
a2603 1
		opte = invalidate_pte(pte);
a2611 1
		*pte = npte;
d2613 2
d2840 1
a2840 1
	*pte = template | trunc_page(pa);
@


1.91
log
@Split pmap_remove_range() into its main loop and a new function,
pmap_remove_pte(), which takes the va and a pte.

Use pmap_remove_pte() instead of pmap_remove_range() in strategic places,
in order to save some cycles by avoiding unnecessary pte address
recomputations.

pmap_remove_range() is still preferred for large amounts of memory, as it
will skip empty segments.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2003/10/28 17:33:01 miod Exp $	*/
d593 1
a593 1
 * entry (PTE). If the PTE is invalid, or non-existant, nothing is done.
d1708 1
a1708 1
 * entry (PTE). If the PTE is invalid, or non-existant, nothing is done.
@


1.90
log
@In pmap_remove_range(), be sure to propagate pvlist flags when removing
the head entry.

While there, invalidate the affected pte earlier for safety, and store its
PG_U bit in the pvlist flags as well.

And also get rid of a pdt table group leftover in pmap_collect().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2003/10/24 17:44:51 miod Exp $	*/
d67 1
a67 1
 *  VM externals
d69 2
a70 2
extern vaddr_t      avail_start, avail_end;
extern vaddr_t      virtual_avail, virtual_end;
d119 1
a119 1
		printf("%s: addr  %x not page aligned.\n", who, ad)
d242 1
d297 1
a297 1
 *      'kernel'.
d303 1
a303 1
 *      kernel  TRUE if supervisor mode, FALSE if user mode
d404 1
a404 1
		printf("(pmap_expand_kmap: %x) v %x\n", curproc,virt);
d610 1
a610 1
		printf("(cache_ctrl) illegal mode %x\n",mode);
d735 1
a735 1
	 *  pmap_bootstrap(&kernelstart,...);
d758 2
a759 2
		printf("kernel_pmap->pm_stpa = 0x%x\n",kernel_pmap->pm_stpa);
		printf("kernel_pmap->pm_stab = 0x%x\n",kernel_pmap->pm_stab);
d817 2
a818 2
		printf("kpdt_phys = 0x%x\n",kpdt_phys);
		printf("kpdt_virt = 0x%x\n",kpdt_virt);
d820 1
a820 1
		       *virt_start,*phys_start);
d839 1
a839 1
	s_text = load_start;	     /* paddr of text */
d852 1
a852 1
	    CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/
d884 1
a884 1
			       *virt_start, *phys_start);
d893 4
a896 4
	 *  Get ethernet buffer - need etherlen bytes physically contiguous.
	 *  1 to 1 mapped as well???. There is actually a bug in the macros
	 *  used by the 1x7 ethernet driver. Remove this when that is fixed.
	 *  XXX -nivas
d913 1
a913 1
				       *virt_start, *phys_start);
d979 2
a980 2
    	if ((p = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL) \
    		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
d1165 1
a1165 1
		       curproc, atop(s));
d1188 1
a1188 1
		       curproc, pmap, pmap->pm_stab, pmap->pm_stpa);
d1265 1
a1265 1
				       curproc, gdttbl);
d1274 1
a1274 1
	sdttbl = pmap->pm_stab;    /* addr of segment table */
d1278 1
a1278 1
		       curproc, sdttbl);
d1334 1
a1334 1
 *	Add a reference to the specified  pmap.
d1352 1
a1352 1
 * Routine:	PMAP_REMOVE_RANGE (internal)
d1355 2
a1356 3
 *	Invalidate page table entries associated with the
 *	given virtual address range. The entries given are the first
 *	(inclusive) and last (exclusive) entries for the VM pages.
d1360 2
a1361 2
 *	s		virtual address of start of range to remove
 *	e		virtual address of end of range to remove
a1366 1
 *	pmap_pte
d1374 1
a1374 7
 *   This routine sequences through the pages defined by the given
 * range. For each page, pmap_pte is called to obtain a (virtual)
 * pointer to the page table  entry (PTE) associated with the page's
 * virtual address. If the page table entry does not exist, or is invalid,
 * nothing need be done.
 *
 *  If the PTE is valid, the routine must invalidated the entry. The
d1382 1
a1382 1
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
d1384 1
a1384 1
	pt_entry_t *pte, opte;
a1387 1
	vaddr_t va;
d1394 1
a1394 1
			printf("(pmap_remove: %x) pmap kernel s %x e %x\n", curproc, s, e);
d1396 1
a1396 1
			printf("(pmap_remove: %x) pmap %x s %x e %x\n", curproc, pmap, s, e);
d1400 4
a1403 3
	/*
	 * pmap has been locked by the caller.
	 */
d1412 1
a1412 1
	 * Loop through the range in vm_page_size increments.
d1414 3
a1416 2
	for (va = s; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;
d1418 1
a1418 1
		sdt = SDTENT(pmap,va);
d1420 3
a1422 11
		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
		}

		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}
d1424 2
a1425 6
		/*
		 * Update statistics.
		 */
		pmap->pm_stats.resident_count--;
		if (pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;
d1427 1
a1427 1
		pa = ptoa(PG_PFNUM(*pte));
d1429 1
d1431 2
a1432 1
		 * Invalidate the pte.
d1434 1
d1436 4
a1439 2
		opte = invalidate_pte(pte) & (PG_U | PG_M);
		flush_atc_entry(users, va, kflush);
d1441 12
a1452 1
		pg = PHYS_TO_VM_PAGE(pa);
d1454 1
a1454 1
		if (pg != NULL) {
d1456 3
a1458 2
			 * Remove the mapping from the pvlist for
			 * this physical page.
d1460 56
a1515 1
			pvl = pg_to_pvh(pg);
d1517 7
a1523 3
#ifdef DIAGNOSTIC
			if (pvl->pv_pmap == PMAP_NULL)
				panic("pmap_remove_range: null pv_list");
d1526 5
a1530 12
			prev = PV_ENTRY_NULL;
			for (cur = pvl; cur != PV_ENTRY_NULL;
			    cur = cur->pv_next) {
				if (cur->pv_va == va && cur->pv_pmap == pmap)
					break;
				prev = cur;
			}
			if (cur == PV_ENTRY_NULL) {
				panic("pmap_remove_range: mapping for va "
				    "0x%x (pa 0x%x) not in pv list at 0x%p",
				    va, pa, pvl);
			}
d1532 1
a1532 19
			if (prev == PV_ENTRY_NULL) {
				/*
				 * Hander is the pv_entry. Copy the next one
				 * to hander and free the next one (we can't
				 * free the hander)
				 */
				cur = cur->pv_next;
				if (cur != PV_ENTRY_NULL) {
					cur->pv_flags = pvl->pv_flags;
					*pvl = *cur;
					pool_put(&pvpool, cur);
				} else {
					pvl->pv_pmap = PMAP_NULL;
				}
			} else {
				prev->pv_next = cur->pv_next;
				pool_put(&pvpool, cur);
			}
		} /* if (pg != NULL) */
d1534 5
a1538 6
		/*
		 * Reflect modify bits to pager.
		 */

		if (opte != 0 && pvl != PV_ENTRY_NULL) {
			pvl->pv_flags |= opte;
d1541 2
a1542 1
	} /* for (va = s; ...) */
d1675 1
a1675 1
		pmap_remove_range(pmap, va, va + PAGE_SIZE);
d1749 1
a1749 1
		sdt = SDTENT(pmap,va);
d1804 1
a1804 1
 *      pmap != kernel_pmap
d1856 2
a1857 2
	 * its containing page 'table group',i.e. the group of
	 * page  tables that fit eithin a single VM page.
d1864 1
a1864 1
	sdt = SDTENT(pmap,v);
d1903 1
a1903 1
 *	pmap_remove_range
d1911 1
a1911 1
 *      If the page table entry (PTE) already maps the given physical page,
d1927 1
a1927 1
 *	if necessary pmap expand(pmap,v)
d1935 1
a1935 1
 *		pmap_remove_range
d2046 1
a2046 1
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
d2054 2
a2055 2
				       phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
				       pg != NULL ? 1 : 0);
d2057 1
a2057 1
				       pte, PG_PFNUM(*pte), PDT_VALID(pte));
d2415 1
a2415 1
		 * then flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE
d2445 1
a2445 1
 *     	p		pointer to proc structure
d2520 1
a2520 1
	*dstpte  = template | trunc_page(dst);
d2603 1
a2603 1
			goto next;	 /*  no page mapping */
d2879 1
a2879 1
		sdt = SDTENT(kernel_pmap,va);
@


1.89
log
@No \n in panic() messages...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2003/10/19 18:12:00 miod Exp $	*/
d1392 1
a1392 1
	pv_entry_t prev, cur, pvl;
d1446 8
d1468 14
a1481 1
			if (pvl->pv_va == va && pvl->pv_pmap == pmap) {
d1487 1
a1487 1
				cur = pvl->pv_next;
d1489 1
d1493 1
a1493 1
					pvl->pv_pmap =  PMAP_NULL;
a1494 1

a1495 12

				for (cur = pvl; cur != PV_ENTRY_NULL;
				    cur = cur->pv_next) {
					if (cur->pv_va == va && cur->pv_pmap == pmap)
						break;
					prev = cur;
				}
				if (cur == PV_ENTRY_NULL) {
					panic("pmap_remove_range: mapping for va "
					       "0x%x (pa 0x%x) not in pv list at 0x%p\n", va, pa, pvl);
				}

d1502 1
a1502 2
		 * Reflect modify bits to pager and zero (invalidate,
		 * remove) the pte entry.
d1505 2
a1506 8
		opte = invalidate_pte(pte);
		flush_atc_entry(users, va, kflush);

		if (opte & PG_M) {
			if (pg != NULL) {
				/* keep track ourselves too */
				pvl->pv_flags |= PG_M;
			}
a2264 1
	vaddr_t sdt_vt;		/* end of segment */
a2303 3
		/* figure out end of range. Watch for wraparound */
		sdt_vt = MIN(sdt_va + PDT_VA_SPACE, VM_MAX_ADDRESS);

d2305 1
a2305 1
		pmap_remove_range(pmap, sdt_va, sdt_vt);
a2552 1
		va = pvep->pv_va;
d2563 1
d2574 2
a2575 2
			panic("pmap_changebit: pte %x doesn't point to page %x %x",
			    *pte, pg, VM_PAGE_TO_PHYS(pg));
d2655 1
a2655 1
		/* we've already cached a this flag for this page,
d2683 11
a2693 1
		if (pte != PT_ENTRY_NULL && (*pte & bit) != 0) {
d2703 1
@


1.88
log
@Simplify pmap DEBUG test constructs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2003/10/16 23:04:09 miod Exp $	*/
d2576 1
a2576 1
			panic("pmap_changebit: pte %x doesn't point to page %x %x\n",
@


1.87
log
@Better DEBUG output.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2003/10/13 18:45:17 miod Exp $	*/
a85 2
#define CD_NONE		0x00
#define CD_NORM		0x01
d110 1
a110 1
int pmap_con_dbg = CD_NONE;
d482 1
a482 1
	if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM))
d537 1
a537 1
			if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM)) {
a538 2
			}
			if (pmap_con_dbg & CD_MAP)
d545 1
d612 1
a612 1
	if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
d633 1
a633 1
		if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
d718 1
a718 1
	if ((pmap_con_dbg & (CD_BOOT | CD_NORM)) == (CD_BOOT | CD_NORM)) {
d1077 1
a1077 1
	if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
d1162 1
a1162 1
	if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
d1185 1
a1185 1
	if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
d1253 1
a1253 1
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
d1282 1
a1282 1
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
d1400 1
a1400 1
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM)) {
d1787 1
a1787 1
	if ((pmap_con_dbg & (CD_EXP | CD_NORM)) == (CD_EXP | CD_NORM))
d1923 1
a1923 1
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
d1957 1
a1957 1
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM))
d1998 1
a1998 1
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM))
d2016 1
a2016 1
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
d2029 1
a2029 1
			if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
d2105 1
a2105 1
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM))
d2274 1
a2274 1
	if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
d2328 1
a2328 1
	if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
d2368 1
a2368 1
	if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
d2544 1
a2544 1
		if ((pmap_con_dbg & (CD_CBIT | CD_NORM)) == (CD_CBIT | CD_NORM))
d2660 1
a2660 1
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
d2670 1
a2670 1
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
d2781 1
a2781 1
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
d2822 1
a2822 1
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
@


1.86
log
@Get rid of the "page table group" stuff. "groups" used to be one logical page
big anyways.

However, getting rid of the various constants (expanding to 0 for logs and
shifts, and 1 for size) allows us to do further simplifications in some pmap
loops, as there is no risk anymore of address wrapround.

While there, fix some typos and comments.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2003/10/11 23:54:17 miod Exp $	*/
a256 1
#define	pmap_pte_m(pte)		(*(pte) & PG_M)
d1402 9
a1542 5
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
		printf("(pmap_remove: %x) map %x s %x e %x\n", curproc, pmap, s, e);
#endif

d1604 5
d1959 4
d2000 4
d2107 4
@


1.85
log
@Better sync comment with reality after last commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2003/10/11 22:08:35 miod Exp $	*/
d355 1
a355 1
	sdt = SDTENT(pmap,virt);
d1204 1
a1204 1
	 * provides us clean pages.
d1246 1
a1246 1
 * ranges represented by the table group sizes(PDT_TABLE_GROUP_VA_SPACE).
a1254 1
	u_int i, j;
a1260 11
	sdttbl = pmap->pm_stab;    /* addr of segment table */
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
	  is near 0xffffffff
	*/
	i = VM_MIN_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	if (j < 1024)
		j++;

d1262 2
a1263 2
	for (; i < j; i++) {
		sdt_va = PDT_TABLE_GROUP_VA_SPACE * i;
d1272 1
a1272 1
	} /* Segment Loop */
d1274 4
d1283 1
a1283 4
	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2*SDT_SIZE));
a1414 2
	 * Do not assume that either start or end fail on any
	 * kind of page boundary (though this may be true!?).
a1415 1

d1421 1
d1423 2
a1424 5
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
d1429 1
a1429 2

		if (!PDT_VALID(pte)) {
a1491 4
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 */
d1621 1
a1621 11
		/*
		 * Do a few consistency checks to make sure
		 * the PV list and the pmap are in synch.
		 */
#ifdef DIAGNOSTIC
		if (pte == PT_ENTRY_NULL) {
			panic("pmap_remove_all: null pte for vm page %p pmap %x va %x",
			       curproc, pg, pmap, va);
		}
#endif	/* DIAGNOSTIC */
		if (!PDT_VALID(pte)) {
a1663 1
 *		SDT_NEXT
d1704 1
a1704 1
	 * Loop through the range in vm_page_size increment.
d1707 8
a1714 14
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1 << SDT_SHIFT)) {
				/* no page table, skip to next seg entry */
				va += (1 << SDT_SHIFT) - PAGE_SIZE;
			} else {
				/* wrap around */
				break;
			}
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
				printf("(pmap_protect: %x) no page table: skip to 0x%x\n", curproc, va + PAGE_SIZE);
#endif
d1718 3
a1720 6
		if (!PDT_VALID(pte)) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
				printf("(pmap_protect: %x) pte invalid pte @@ 0x%x\n", curproc, pte);
#endif
			continue;	 /*  no page mapping */
d1740 1
a1740 1
 *	New kernel virtual memory is allocated for a page table
d1775 1
a1775 1
	int i, spl;
d1822 1
a1822 1
	v &= ~((1 << (LOG2_PDT_TABLE_GROUP_SIZE + PDT_BITS + PG_BITS)) - 1);
d1830 3
a1832 7
	for (i = PDT_TABLE_GROUP_SIZE; i>0; i--) {
		*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
		*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;
		sdt++;
		pdt_paddr += PDT_SIZE;
		pdt_vaddr += PDT_SIZE;
	}
d2192 7
a2198 8
	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL) {
		if (PDT_VALID(pte)) {
			rv = TRUE;
			if (pap != NULL) {
				pa = ptoa(PG_PFNUM(*pte));
				pa |= (va & PAGE_MASK); /* offset within page */
				*pap = pa;
			}
a2247 1
	sdt_entry_t *sdt;	/* ptr to index into segment table */
a2254 1
	u_int i, j;
a2264 10
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
	  is near 0xffffffff
	*/
	i = VM_MIN_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	if (j < 1024)
		j++;

d2266 2
a2267 3
	for (; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE) {
		sdt_va = PDT_TABLE_GROUP_VA_SPACE * i;

a2268 1

d2272 1
a2272 1
		gdttblend = gdttbl + (PDT_ENTRIES * PDT_TABLE_GROUP_SIZE);
d2287 1
a2287 2
		sdt_vt = sdt_va <= VM_MAX_ADDRESS - PDT_TABLE_GROUP_VA_SPACE ?
		    sdt_va + PDT_TABLE_GROUP_VA_SPACE : VM_MAX_ADDRESS;
d2295 2
a2296 4
		for (sdt = sdtp; sdt < (sdtp + PDT_TABLE_GROUP_SIZE); sdt++) {
			*((sdt_entry_t *) sdt) = 0;
			*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = 0;
		}
d2554 1
a2554 6
#ifdef DIAGNOSTIC
		if (pte == PT_ENTRY_NULL)
			panic("pmap_changebit: null pte for va %x pmap %x",
			    va, pmap);
#endif
		if (!PDT_VALID(pte)) {
d2622 1
a2622 1
 * given page. For each pmap/va pair, the page descripter table entry is
d2668 1
a2668 6
#ifdef DIAGNOSTIC
		if (pte == PT_ENTRY_NULL) {
			panic("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->pv_va);
		}
#endif
		if (*pte & bit) {
d2820 1
a2820 1
		sdt = SDTENT(kernel_pmap, va);
d2822 1
d2824 2
a2825 5
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
d2829 5
a2839 1
		pte = pmap_pte(kernel_pmap, va);
@


1.84
log
@Page tables can be cached again on 187 and 188, as long as they are cached
write through and global (for 188).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2003/10/10 14:25:42 miod Exp $	*/
d1195 1
a1195 1
	/* memory for page tables should be CACHE DISABLED */
d1834 1
a1834 1
	/* memory for page tables should be CACHE DISABLED */
@


1.83
log
@In pmap_expand_kmap(), be sure to initialize new page tables.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2003/10/06 14:59:29 miod Exp $	*/
d313 1
a313 1
#ifdef DIAGNOSTIC
d315 1
a315 2
		printf("ff1 users = %d!\n", ff1(tusers));
		panic("bogus amount of users!!!");
d1197 1
a1197 1
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_INH);
d1495 2
a1496 3
					printf("pmap_remove_range: looking for VA "
					       "0x%x (pa 0x%x) PV list at 0x%p\n", va, pa, pvl);
					panic("pmap_remove_range: mapping not in pv_list");
a1611 3
#ifdef DEBUG
	int dbgcnt = 0;
#endif
d1648 2
a1649 5
#ifdef DEBUG
			printf("(pmap_remove_all: %p) vm page %p pmap %x va %x dbgcnt %x\n",
			       curproc, pg, pmap, va, dbgcnt);
#endif
			panic("pmap_remove_all: pte NULL");
a1667 3
#ifdef DEBUG
		dbgcnt++;
#endif
a1693 1
 *		panic
d1836 1
a1836 1
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_INH);
d2619 2
a2620 1
			panic("pmap_changebit: bad pv list entry.");
d2851 1
a2851 1
		pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
@


1.82
log
@Correctly handle sparse mappings in pmap_changebit() and pmap_testbit().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2003/10/06 06:31:28 miod Exp $	*/
a257 1
#define	pmap_pte_u(pte)		(*(pte) & PG_U)
d428 3
a430 4
#ifdef DEBUG	/* XXX - necessary? */
	kpdt_ent->phys = (paddr_t)0;
	kpdt_ent->next = NULL;
#endif
d559 1
a559 1
	
d1993 1
a1993 1
			pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
d2042 1
a2042 1
			template |= (invalidate_pte(pte) & PG_M);
d2861 1
a2861 1
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL) {
a2862 1
	}
@


1.81
log
@More vm_offset_t removal I forgot to check in; spotted by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2003/09/29 20:29:04 miod Exp $	*/
d415 1
d418 1
d429 1
d432 1
a2627 1
#ifdef DIAGNOSTIC
d2631 1
d2634 5
a2638 3
		if (!PDT_VALID(pte))
			printf("pmap_changebit: invalid pte %x pg %x %x\n",
			    *pte, pg, VM_PAGE_TO_PHYS(pg));
d2660 1
a2660 1

d2743 1
a2743 2
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
d2749 1
d2751 1
a2751 2
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->pv_va);
			panic("pmap_testbit: bad pv list entry");
d2753 1
d2759 1
a2759 1
				printf("(pmap_testbit: %x) modified page pte@@0x%p\n", curproc, pte);
a2764 1
		pvep = pvep->pv_next;
@


1.80
log
@Disable cache on page tables unconditionnaly, not only for MVME188: magically
solves the last 187 issues...

Although the slowdown is surprisingly small, this is only a temporary measure,
there is room for improvement...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2003/09/29 13:05:56 miod Exp $	*/
d118 1
a118 1
#define PAGE_ALIGNED(ad)	(((vm_offset_t)(ad) & PAGE_MASK) == 0)
d791 1
a791 1
	
d795 1
a795 1
	
d799 1
a799 1
	
d961 1
a961 1
	for (; ptable->size != (size_t)(-1); ptable++){
d1174 1
a1174 1
	
d2480 1
a2480 1
	
@


1.79
log
@Better use of phys_map_vaddr* (does not matter until we start using more than
one cpu).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2003/09/27 13:05:30 miod Exp $	*/
a617 1
#endif /* DEBUG */
a618 1
#ifdef DIAGNOSTIC
d621 1
a621 1
#endif
d696 2
a697 2
 *    A pair of virtual pages are reserved for debugging and IO
 * purposes. They are arbitrarily mapped when needed. They are used,
a1069 9
 * Parameters:
 *	phys_start	physical address of first available page
 *			(was last set by pmap_bootstrap)
 *	phys_end	physical address of last available page
 *
 * Extern/Globals
 *	pmap_phys_start
 *	pmap_phys_end
 *
d1194 4
a1197 9
#ifdef MVME188
	if (brdtyp == BRD_188) {
		/*
		 * memory for page tables should be CACHE DISABLED on MVME188
		 */
		pmap_cache_ctrl(kernel_pmap,
		    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_INH);
	}
#endif
d1844 3
a1846 8
#ifdef MVME188
	if (brdtyp == BRD_188) {
		/*
		 * the pages for page tables should be CACHE DISABLED on MVME188
		 */
		pmap_cache_ctrl(kernel_pmap, pdt_vaddr, pdt_vaddr+PAGE_SIZE, CACHE_INH);
	}
#endif
@


1.78
log
@Do not panic in pmap_map() if DIAGNOSTIC, when we are mapping the very end
of the address space (i.e. "end" wraps to zero).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2003/09/26 22:27:26 miod Exp $	*/
d182 1
a182 1
 * Two pages of scratch space.
d185 1
a185 1
vaddr_t phys_map_vaddr1, phys_map_vaddr2;
d935 2
a936 1
	phys_map_vaddr2 = phys_map_vaddr1 + PAGE_SIZE * max_cpus;
d1042 2
a1043 1
			pte = pmap_pte(kernel_pmap, phys_map_vaddr1);
d1045 2
a1046 1
			pte = pmap_pte(kernel_pmap, phys_map_vaddr2);
d1132 1
a1132 1
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu * PAGE_SIZE));
d2071 1
a2071 1
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
d2080 1
a2080 1
			if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
d2093 1
a2093 1
				if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
d2652 2
a2653 1
			panic("pmap_changebit: invalid pte");
d2655 2
a2656 1
			panic("pmap_changebit: pte doesn't point to page");
@


1.77
log
@Death to the bitfields, this time cmmu_apr_t and batc_entry_t. In the
process, remove duplicate batc defines.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2003/09/19 23:12:22 miod Exp $	*/
d490 2
a491 1
	if (start > end)
d493 1
@


1.76
log
@In pmap_changebit(), do not flush tlb entries unless really necessary.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2003/09/16 20:52:22 miod Exp $	*/
d113 15
a127 1
#endif /* DEBUG */
d479 1
a479 1
	batc_template_t	batctmp;
d497 1
a497 2
	batctmp.bits = 0;
	batctmp.field.sup = 1;	     /* supervisor */
d499 1
a499 1
		batctmp.field.wt = 1;	 /* write through */
d501 1
a501 1
		batctmp.field.g = 1;     /* global */
d503 1
a503 1
		batctmp.field.ci = 1;	 /* cache inhibit */
d505 1
a505 2
		batctmp.field.wp = 1; /* protection */
	batctmp.field.v = 1;	     /* valid */
d528 2
a529 2
			batctmp.field.lba = M88K_BTOBLK(virt);
			batctmp.field.pba = M88K_BTOBLK(page);
d534 2
a535 2
								 batctmp.bits);
			batc_entry[batc_used] = batctmp.field;
d538 1
a538 1
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
d609 1
a609 1
	if (mode & CACHE_MASK) {
d645 1
a645 1
		*pte = (invalidate_pte(pte) & CACHE_MASK) | mode;
d714 1
a714 1
	apr_template_t apr_data;
a723 2
#endif
#ifdef DIAGNOSTIC
d1016 1
a1016 6
	apr_data.bits = 0;
	apr_data.field.st_base = atop(kernel_pmap->pm_stpa);
	apr_data.field.wt = 1;
	apr_data.field.g  = 1;
	apr_data.field.ci = 0;
	apr_data.field.te = 1; /* Translation enable */
d1019 1
a1019 1
		show_apr(apr_data.bits);
d1025 1
a1025 1
		printf("invalidating tlb %x\n", apr_data.bits);
d1044 1
a1044 1
			cmmu_remote_set_sapr(i, apr_data.bits);
d1189 1
a1193 1
#ifdef DEBUG
d2431 1
a2431 1
	apr_template_t apr_data;
d2450 2
a2451 6
		apr_data.bits = 0;
		apr_data.field.st_base = atop(pmap->pm_stpa);
		apr_data.field.wt = 0;
		apr_data.field.g  = 1;
		apr_data.field.ci = 0;
		apr_data.field.te = 1;
d2460 2
a2461 2
		cmmu_pmap_activate(cpu, apr_data.bits,
				   pmap->pm_ibatc, pmap->pm_dbatc);
d2465 1
a2465 1
		cmmu_set_uapr(apr_data.bits);
@


1.75
log
@Remove unused material from include files, as well as the annoying U() macro
which is only used to obfuscate things.

Doing this points out that the BUG memory is not at the same address on
187 and 197 (the 197 BUG is inside obio), so provide distinct constants,
and treat 187 and 197 slightly differently in pmap_bootstrap(). However, we
now need to map the 197 flash memory as well...

While there, simplify and constify pmap_table_build() and its associated data.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2003/08/20 19:35:50 miod Exp $	*/
d2596 1
a2596 1
	pt_entry_t *pte, npte;
d2654 2
a2655 2
		*pte = invalidate_pte(pte);
		npte = (*pte | set) & mask;
d2663 2
a2664 2
		if (npte != *pte) {
			*pte = npte;
@


1.74
log
@Fix the write status test for the 88110 kluge in m88k_protection() [oops]
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2003/08/20 19:29:12 miod Exp $	*/
a223 3
extern vaddr_t bugromva;
extern vaddr_t sramva;
extern vaddr_t obiova;
d590 2
a591 1
	vaddr_t va, pteva;
d639 1
a639 1
		pteva = ptoa(PG_PFNUM(*pte));
d642 1
a642 1
				cmmu_flush_remote_cache(cpu, pteva, PAGE_SIZE);
d867 1
a873 1
#ifdef DEBUG
a877 1
#endif
d881 1
d943 1
a943 1
	ptable = pmap_table_build(0);
a951 3
			/*
			 * size-1, 'cause pmap_map rounds up to next pagenumber
			 */
d953 1
a953 1
			    ptable->phys_start + (ptable->size - 1),
d1001 1
d1201 1
a1201 2
		    (vaddr_t)segdt, (vaddr_t)segdt+ (SDT_SIZE*2),
		    CACHE_INH);
@


1.73
log
@Revert the flust_atc_entry() shortcut if NCPUS == 1 - it will not work on
multiprocessor boards where the master cpu is not the first one.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2003/08/08 21:36:33 miod Exp $	*/
d275 1
a275 1
			if (p & PG_PROT)
@


1.72
log
@Fix harmless address computation buglet in pmap_collect().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2003/08/01 23:15:31 miod Exp $	*/
d231 1
a296 2
#if NCPUS > 1
void flush_atc_entry(long, vaddr_t, boolean_t);
a316 4
#else
#define	flush_atc_entry(users,va,kernel) \
	cmmu_flush_remote_tlb(0, (kernel), (va), PAGE_SIZE)
#endif
@


1.71
log
@The pmap potpourri du jour, while hunting for evil bugs:
- provide a simpler flush_atc_entry() in the NCPUS == 1 case
- remove some can't happen tests in pmap_protect()
- handle pool_get failure and PMAP_CANFAIL correctly in pmap_enter()
- don't forget to initialize pv_flags in new pv_entry items
- de-cretinize pmap_testbit() and pmap_page_protect()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2003/08/01 18:39:12 miod Exp $	*/
d1290 1
a1290 1
		sdt_va = PDT_TABLE_GROUP_VA_SPACE*i;
d2320 1
a2320 1
pmap_collect( pmap_t pmap)
a2323 1
	sdt_entry_t *sdttbl;	/* ptr to first entry in seg table */
d2342 1
a2342 2
	sdttbl = pmap->pm_stab; /* addr of segment table */
	sdtp = sdttbl;
d2356 1
a2356 1
		sdt_va = VM_MIN_ADDRESS + PDT_TABLE_GROUP_VA_SPACE * i;
@


1.70
log
@pmap_kremove was so broken I'd rather pretend I did not write it...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2003/08/01 07:48:24 miod Exp $	*/
a230 1
void flush_atc_entry(long, vaddr_t, boolean_t);
d296 2
d318 4
d1654 1
a1654 2
	while ((pmap = pvl->pv_pmap) != PMAP_NULL) {
		va = pvl->pv_va;
d1658 1
a1737 2
	if (pmap == PMAP_NULL || prot & VM_PROT_WRITE)
		return;
a2003 9
	/*
	 * Must allocate a new pvlist entry while we're unlocked;
	 * zalloc may cause pageout (which will lock the pmap system).
	 * If we determine we need a pvlist entry, we will unlock
	 * and allocate one. Then will retry, throwing away
	 * the allocated entry later (if we no longer need it).
	 */
	pv_e = PV_ENTRY_NULL;

a2006 1
Retry:
d2062 1
a2062 1
			*pte++ = template | ap | trunc_page(pa);
d2071 6
a2088 5
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(users, va, TRUE);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}
d2111 1
d2130 8
a2137 3
				if (pv_e == PV_ENTRY_NULL) {
					pv_e = pool_get(&pvpool, PR_NOWAIT);
					goto Retry;
d2142 1
a2143 4
				/*
				 * Remember that we used the pvlist entry.
				 */
				pv_e = PV_ENTRY_NULL;
a2171 3
	if (pv_e != PV_ENTRY_NULL)
		pool_put(&pvpool, pv_e);

a2732 1
	boolean_t rv;
a2751 3
		/* unmapped page - get info from attribute array
		   maintained by pmap_remove_range/pmap_remove_all */
		rv = (boolean_t)(pvl->pv_flags & bit);
d2758 1
a2758 1
		return (rv);
d2845 3
a2847 3
	switch (prot) {
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
a2849 8
		break;
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
		break;
	default:
		pmap_remove_all(pg);
		break;
	}
@


1.69
log
@No semicolon at the end of macros supposed to be atomic statements.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2003/01/24 09:57:44 miod Exp $	*/
d159 1
a159 1
#if defined(MVME188) && defined(MVME187) || defined(MVME197)
d2927 1
d2940 2
a2941 2
	for (len >>= PAGE_SHIFT; len != 0; len--, va += PAGE_SIZE) {
		vaddr_t e = va + PAGE_SIZE;
@


1.68
log
@Convert m88k pmap from physseg to VM_PAGE_MD.

This allows us to remove some ambiguities on how some functions are called,
remove some diagnostic checks for conditions that can never happen and
remove the ugly hack with "pmap_initialized".

Then, rework internal function interfaces and some logic so as to stop
fetching vm_page from a pa and the reverse every now and then - this makes
some pmap operations run much faster.

While there, various KNF and whitespace fixes, and rename some structure
fields to be closer to the m68k pmap.

per art@@'s idea.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2002/09/10 18:29:43 art Exp $	*/
d191 2
a192 2
#define	SPLVM(spl)	spl = splvm();
#define	SPLX(spl)	splx(spl);
@


1.67
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2002/05/07 00:54:37 miod Exp $	*/
d3 1
a3 1
 * Copyright (c) 2001, 2002 Miodrag Vallat
d73 1
a73 1
 * Macros to operate cpus_using field
d147 1
a147 1
#if defined(MVME188) 
d151 1
a151 1
#endif 
d157 1
a157 1
#endif 
a172 9
/*
 * Cached page flags
 *
 * This is an array, one byte per physical page, which keeps track
 * of some flags for pages which are no longer containd in any
 * pmap (for mapped pages, the modified flags are in the PTE).
 */
u_int8_t  *pmap_cached_attributes;

d175 1
a175 1
pv_entry_t pv_head_table; /* array of entries, one per page */
d177 5
a181 17
#define	PMAP_MANAGED(pa) \
	(pmap_initialized && IS_VM_PHYSADDR(pa))

#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})
#define	pa_to_attribute(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
})
d197 1
a197 1
		simple_lock(&(pmap)->lock); \
d201 1
a201 1
		simple_unlock(&(pmap)->lock); \
a204 2
#define PV_TABLE_SIZE(n)	((vsize_t)((n) * sizeof(struct pv_entry)))

d206 2
a207 4
void  *etherbuf=NULL;
int   etherlen;

boolean_t   pmap_initialized = FALSE;/* Has pmap_init completed? */
d214 1
a214 1
int   batc_used;
d238 3
a240 3
void pmap_remove_all(paddr_t);
void pmap_changebit(paddr_t, int, int);
boolean_t pmap_testbit(paddr_t, int);
d257 1
a257 1
m88k_protection(pmap_t map, vm_prot_t prot)
d269 4
a272 4
		/* if the map is the kernel's map and since this 
		 * is not a paged kernel, we go ahead and mark 
		 * the page as modified to avoid an exception 
		 * upon writing to the page the first time.  XXX smurph 
d274 1
a274 1
		if (map == kernel_pmap) { 
d279 3
a281 3
#endif 
	return (p);
} /* m88k_protection */
d297 2
a298 5
void 
flush_atc_entry(users, va, kernel)
	long users;
	vaddr_t va;
	boolean_t kernel;
d300 2
a301 2
	int	cpu;
	long	tusers = users;
d308 2
a309 1
#endif 
d335 1
a335 3
pmap_pte(map, virt)
	pmap_t map;
	vaddr_t virt;
d337 1
a337 1
	sdt_entry_t	*sdt;
d342 1
a342 1
	if (map == PMAP_NULL)
d346 1
a346 1
	sdt = SDTENT(map,virt);
d353 3
a355 4
	return ((pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES))<<PDT_SHIFT) +
		PDTIDX(virt));

} /* pmap_pte */
d390 1
a390 3
pmap_expand_kmap(virt, prot)
	vaddr_t virt;
	vm_prot_t prot;
d392 2
a393 2
	sdt_entry_t	template, *sdt;
	kpdt_entry_t	kpdt_ent;
d399 1
d402 1
a402 1
	/*  segment table entry derivate from map and virt. */
d419 2
a420 2
	return ((pt_entry_t *)(kpdt_ent) + PDTIDX(virt));
} /* pmap_expand_kmap() */
d461 6
a466 10
pmap_map(virt, start, end, prot, cmode)
	vaddr_t virt;
	paddr_t start, end;
	vm_prot_t prot;
	u_int cmode;
{
	u_int		npages;
	u_int		num_phys_pages;
	pt_entry_t	template, *pte;
	paddr_t		page;
d469 1
a469 1
	int		i;
a485 1

a496 1

a501 1

d513 1
a513 1
		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) && 
d524 1
a524 1
					cmmu_set_pair_batc_entry(i, batc_used, 
d562 1
a562 1
} /* pmap_map() */
d572 1
a572 1
 *	pmap_t		map
d589 1
a589 4
pmap_cache_ctrl(pmap, s, e, mode)
	pmap_t pmap;
	vaddr_t s, e;
	u_int mode;
d591 6
a596 6
	int		spl;
	pt_entry_t	*pte;
	vaddr_t		va, pteva;
	boolean_t	kflush;
	int		cpu;
	u_int		users;
d615 1
a615 1
	users = pmap->cpus_using;
d647 1
a647 1
} /* pmap_cache_ctrl */
d690 1
a690 1
 * For m88k, we have to map BUG memory also. This is a read only 
d697 12
a708 19
pmap_bootstrap(load_start, phys_start, phys_end, virt_start, virt_end)
	vaddr_t load_start;
	paddr_t *phys_start, *phys_end;
	vaddr_t *virt_start, *virt_end;
{
	kpdt_entry_t	kpdt_virt;
	sdt_entry_t	*kmap;
	vaddr_t		vaddr,
			virt,
			kernel_pmap_size,
			pdt_size;
	paddr_t		s_text,
			e_text,
			kpdt_phys;
	apr_template_t	apr_data;
	pt_entry_t	*pte;
	int		i;
	pmap_table_t	ptable;
	extern void	*kernelstart, *etext;
d710 1
a710 1
#ifdef DEBUG 
d720 1
a720 1
	simple_lock_init(&kernel_pmap->lock);
d724 1
a724 2
	 * physical memory,
	 * i.e. just after where the kernel image was loaded.
d727 1
a727 1
	 * The calling sequence is 
d729 1
a729 1
	 *  pmap_bootstrap(&kernelstart,...) 
d744 4
a747 4
	kernel_pmap->ref_count = 1;
	kernel_pmap->cpus_using = 0;
	kernel_pmap->sdt_paddr = kmap = (sdt_entry_t *)(*phys_start);
	kernel_pmap->sdt_vaddr = (sdt_entry_t *)(*virt_start);
d752 2
a753 2
		printf("kernel_pmap->sdt_paddr = 0x%x\n",kernel_pmap->sdt_paddr);
		printf("kernel_pmap->sdt_vaddr = 0x%x\n",kernel_pmap->sdt_vaddr);
a754 3
	/* init double-linked list of pmap structure */
	kernel_pmap->next = kernel_pmap;
	kernel_pmap->prev = kernel_pmap;
d757 1
a757 1
	/* 
d769 1
a769 1
	kernel_pmap_size = 2*SDT_SIZE;
d773 1
a773 1
		printf("     kernel segment start = 0x%x\n", kernel_pmap->sdt_paddr);
d775 1
a775 1
		printf("       kernel segment end = 0x%x\n", ((paddr_t)kernel_pmap->sdt_paddr) + kernel_pmap_size);
d777 1
a777 1
#endif 
d779 1
a779 1
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);
d807 1
a807 1
#endif 
d834 2
a835 2
	e_text = load_start + ((vaddr_t)&etext -
	    trunc_page((vaddr_t)&kernelstart));
d870 1
a870 1
		/* 
d872 1
a872 1
		 * table size up to a page boundry in the quest to get 
d901 1
a901 1
		*phys_start += etherlen; 
d945 1
a945 1
	ptable = pmap_table_build(0);	 /* see pmap_table.c for details */
d1011 1
a1011 1
	apr_data.field.st_base = atop(kernel_pmap->sdt_paddr);
d1020 1
a1020 1
#endif 
d1034 2
a1035 2
			 * Set valid bit to DT_INVALID so that the very first 
			 * pmap_enter() on these won't barf in 
d1049 1
a1049 1
			SETBIT_CPUSET(i, &kernel_pmap->cpus_using);
d1057 1
a1057 1
} /* pmap_bootstrap() */
a1071 2
 *	pv_head_table
 *	pmap_cached_attributes
a1073 1
 *	pmap_initialized
d1076 1
a1076 1
 *	uvm_km_zalloc
d1078 2
a1079 13
 *   This routine does not really have much to do. It allocates space
 * for the pv_head_table, pmap_cached_attributes; and sets these
 * pointers. It also initializes zones for pmap structures, pv_entry
 * structures, and segment tables.
 *
 *  Last, it sets the pmap_phys_start and pmap_phys_end global
 * variables. These define the range of pages 'managed' by pmap. These
 * are pages for which pmap must maintain the PV list and the modify
 * list. (All other pages are kernel-specific and are permanently
 * wired.)
 *
 *	uvm_km_zalloc() memory for pv_table
 * 	uvm_km_zalloc() memory for modify_bits
a1083 7
	long		npages;
	vaddr_t		addr;
	vsize_t		s;
	pv_entry_t	pv;
	u_int8_t	*attr;
	int		bank;

a1087 39
	/*
	 * Allocate memory for the pv_head_table,
	 * the modify bit array, and the pte_page table.
	 */
	for (npages = 0, bank = 0; bank < vm_nphysseg; bank++)
		npages += vm_physmem[bank].end - vm_physmem[bank].start;

	s = PV_TABLE_SIZE(npages);		/* pv_list */
	s += npages * sizeof(u_int8_t);		/* pmap_cached_attributes */

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
		printf("(pmap_init) nbr of managed pages = %x\n", npages);
		printf("(pmap_init) size of pv_list = %x\n",
		       npages * sizeof(struct pv_entry));
	}
#endif

	s = round_page(s);
	addr = (vaddr_t)uvm_km_zalloc(kernel_map, s);

	pv_head_table = (pv_entry_t)addr;
	addr += PV_TABLE_SIZE(npages);

	pmap_cached_attributes = (u_int8_t *)addr;

	/*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_head_table;
	attr = pmap_cached_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}
a1091 2

	pmap_initialized = TRUE;
d1101 1
a1101 1
 *	phys		PA of page to zero
d1119 5
a1123 5
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
	vaddr_t		srcva;
	int		spl;
	int		cpu;
	pt_entry_t	*srcpte;
d1131 1
a1131 1
	*srcpte = trunc_page(phys) |
d1138 2
a1139 2
	cmmu_flush_remote_data_cache(cpu, phys, PAGE_SIZE);
} /* pmap_zero_page() */
d1155 3
a1157 4
	pmap_t		p;
	sdt_entry_t	*segdt;
	u_int		s;

d1159 1
a1159 1
	int		i;
d1162 2
a1163 2
	p = pool_get(&pmappool, PR_WAITOK);
	bzero(p, sizeof(*p));
d1183 1
a1183 1
	p->sdt_vaddr = segdt;
d1185 1
a1185 1
	    (paddr_t *)&p->sdt_paddr) == FALSE)
d1188 1
a1188 1
	if (!PAGE_ALIGNED(p->sdt_paddr))
d1190 1
a1190 1
		    (int)p->sdt_paddr);
d1194 2
a1195 2
		printf("(pmap_create: %x) pmap=0x%p, sdt_vaddr=0x%x, sdt_paddr=0x%x\n",
		       curproc, p, p->sdt_vaddr, p->sdt_paddr);
d1220 3
a1222 3
	p->ref_count = 1;
	simple_lock_init(&p->lock);
	p->cpus_using = 0;
d1227 2
a1228 2
		p->i_batc[i].bits = 0;
		p->d_batc[i].bits = 0;
d1232 2
a1233 10
#ifdef DEBUG
	/* link into list of pmaps, just after kernel pmap */
	p->next = kernel_pmap->next;
	p->prev = kernel_pmap;
	kernel_pmap->next = p;
	p->next->prev = p;
#endif

	return (p);
} /* pmap_create() */
d1251 1
a1251 1
 * 	ref_count field of the pmap structure goes to zero.
d1260 1
a1260 2
pmap_release(pmap)
	pmap_t pmap;
d1262 4
a1265 4
	unsigned long	sdt_va;  /*  outer loop index */
	sdt_entry_t	*sdttbl; /*  ptr to first entry in the segment table */
	pt_entry_t	*gdttbl; /*  ptr to first entry in a page table */
	u_int		i,j;
d1272 4
a1275 4
	sdttbl = pmap->sdt_vaddr;    /* addr of segment table */
	/* 
	  This contortion is here instead of the natural loop 
	  because of integer overflow/wraparound if VM_MAX_ADDRESS 
d1308 1
a1308 4
		printf("(pmap_release: %x) ref_count = 0\n", curproc);
	/* unlink from list of pmap structs */
	pmap->prev->next = pmap->next;
	pmap->next->prev = pmap->prev;
d1310 1
a1310 1
} /* pmap_release() */
d1335 1
a1335 2
pmap_destroy(p)
	pmap_t p;
d1340 1
a1340 1
	if (p == kernel_pmap)
d1344 3
a1346 3
	simple_lock(&p->lock);
	count = --p->ref_count;
	simple_unlock(&p->lock);
d1348 2
a1349 2
		pmap_release(p);
		pool_put(&pmappool, p);
d1351 1
a1351 1
} /* pmap_destroy() */
d1363 1
a1363 1
 * Under a pmap read lock, the ref_count field of the pmap structure
d1367 1
a1367 2
pmap_reference(p)
	pmap_t p;
d1370 4
a1373 4
	simple_lock(&p->lock);
	p->ref_count++;
	simple_unlock(&p->lock);
} /* pmap_reference */
a1389 1
 *	pmap_cached_attributes
d1408 1
a1408 1
 * entry in the pmap_cached_attributes. Next, the function must find the PV
d1414 1
a1414 3
pmap_remove_range(pmap, s, e)
	pmap_t pmap;
	vaddr_t s, e;
d1416 7
a1422 8
	pt_entry_t	*pte;
	pv_entry_t	prev, cur;
	pv_entry_t	pvl;
	paddr_t		pa;
	vaddr_t		va;
	u_int		users;
	pt_entry_t	opte;
	boolean_t	kflush;
d1425 1
a1425 1
	 * Pmap has been locked by the caller.
d1427 1
a1427 1
	users = pmap->cpus_using;
d1454 1
a1454 1
		pte = pmap_pte(pmap,va);
d1463 1
a1463 1
		pmap->stats.resident_count--;
d1465 1
a1465 1
			pmap->stats.wired_count--;
d1468 1
d1470 1
a1470 1
		if (PMAP_MANAGED(pa)) {
d1475 1
a1475 1
			pvl = pa_to_pvh(pa);
d1478 1
a1478 1
			if (pvl->pmap == PMAP_NULL)
d1482 1
a1482 1
			if (pvl->va == va && pvl->pmap == pmap) {
d1488 1
a1488 1
				cur = pvl->next;
d1493 1
a1493 1
					pvl->pmap =  PMAP_NULL;
d1499 2
a1500 2
				    cur = cur->next) {
					if (cur->va == va && cur->pmap == pmap)
d1510 1
a1510 1
				prev->next = cur->next;
d1513 1
a1513 1
		} /* if PMAP_MANAGED */
d1528 1
a1528 1
			if (PMAP_MANAGED(pa)) {
d1530 1
a1530 1
				*pa_to_attribute(pa) |= PG_M;
d1534 2
a1535 2
	} /* end for( va = s; ...) */
} /* pmap_remove_range */
d1546 1
a1546 1
 *	map		pointer to pmap structure
d1560 1
a1560 3
pmap_remove(map, s, e)
	pmap_t map;
	vaddr_t s, e;
d1564 1
a1564 1
	if (map == PMAP_NULL)
d1569 1
a1569 1
		printf("(pmap_remove: %x) map %x  s %x  e %x\n", curproc, map, s, e);
d1577 4
a1580 4
	PMAP_LOCK(map, spl);
	pmap_remove_range(map, s, e);
	PMAP_UNLOCK(map, spl);
} /* pmap_remove() */
d1590 1
a1590 1
 *	phys		physical address of pages which is to
d1594 1
a1594 2
 *	pv_head_array, pv lists
 *	pmap_cached_attributes
d1607 1
a1607 1
 * corresponding bit in the pmap_cached_attributes entry corresponding
d1616 1
a1616 2
pmap_remove_all(phys)
	paddr_t phys;
d1618 5
a1622 5
	pv_entry_t	pvl;
	pt_entry_t	*pte;
	vaddr_t		va;
	pmap_t		pmap;
	int		spl;
d1624 1
a1624 1
	int		dbgcnt = 0;
d1627 1
a1627 1
	if (!PMAP_MANAGED(phys)) {
d1631 1
a1631 1
			printf("(pmap_remove_all: %x) phys addr 0x%x not a managed page\n", curproc, phys);
d1639 1
a1639 1
	 * We don't have to lock the pv_head, since we have the entire pmap
d1644 1
a1644 1
	pvl = pa_to_pvh(phys);
d1649 3
a1651 3
	while ((pmap = pvl->pmap) != PMAP_NULL) {
		va = pvl->va;
		if (!simple_lock_try(&pmap->lock))
d1663 2
a1664 2
			printf("(pmap_remove_all: %p) phys %p pmap %x va %x dbgcnt %x\n",
			       curproc, phys, pmap, va, dbgcnt);
d1670 1
a1670 1
			pvl = pvl->next;
d1677 1
a1677 1
				    phys);
d1679 1
a1679 1
			pvl = pvl->next;
d1693 1
a1693 1
		simple_unlock(&pmap->lock);
d1696 1
a1696 1
} /* pmap_remove_all() */
d1725 1
a1725 4
pmap_protect(pmap, s, e, prot)
	pmap_t pmap;
	vaddr_t s, e;
	vm_prot_t prot;
d1727 5
a1731 5
	int		spl;
	pt_entry_t	ap, *pte;
	vaddr_t		va;
	u_int		users;
	boolean_t	kflush;
d1749 1
a1749 1
	users = pmap->cpus_using;
d1788 2
a1789 2
		 * Invalidate pte temporarily to avoid the 
		 * modified bit and/or the reference bit being 
d1797 1
a1797 1
} /* pmap_protect() */
d1813 1
a1813 1
 *	map	point to map structure
a1834 6
 *
 *
 *	if (kernel_pmap)
 *		pmap_expand_kmap()
 *	ptva = kmem_alloc(user_pt_map)
 *
d1837 1
a1837 3
pmap_expand(map, v)
	pmap_t map;
	vaddr_t v;
d1839 5
a1843 10
	int		i, spl;
	vaddr_t		pdt_vaddr;
	paddr_t		pdt_paddr;
	sdt_entry_t	*sdt;
	pt_entry_t	*pte;

#ifdef DIAGNOSTIC
	if (map == PMAP_NULL)
		panic("pmap_expand: pmap is NULL");
#endif
d1847 1
a1847 1
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, map, v);
d1850 1
a1850 1
	CHECK_PAGE_ALIGN (v, "pmap_expand");
d1855 1
a1855 1
		panic("pmap_expand: pmap_extract failed!");
d1866 1
a1866 1
	PMAP_LOCK(map, spl);
d1868 1
a1868 1
	if ((pte = pmap_pte(map, v)) != PT_ENTRY_NULL) {
d1873 1
a1873 1
		PMAP_UNLOCK(map, spl);
d1893 1
a1893 1
	sdt = SDTENT(map,v);
d1906 2
a1907 2
	PMAP_UNLOCK(map, spl);
} /* pmap_expand() */
d1930 1
a1930 2
 *	pv_head_array, pv lists
 *	pmap_cached_attributes
d1976 1
a1976 6
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d1978 8
a1985 9
	int		spl;
	pv_entry_t	pv_e;
	pt_entry_t	ap, *pte;
	paddr_t		old_pa;
	pt_entry_t	template;
	pv_entry_t	pvl;
	u_int		users;
	boolean_t	kflush;
	boolean_t	wired = (flags & PMAP_WIRED) != 0;
a1989 4
	/*
	 *	Range check no longer use, since we use whole address space
	 */

d1995 1
a1995 1
			printf("(pmap_enter: %x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
d1998 1
d2002 5
a2006 5
	 *	Must allocate a new pvlist entry while we're unlocked;
	 *	zalloc may cause pageout (which will lock the pmap system).
	 *	If we determine we need a pvlist entry, we will unlock
	 *	and allocate one. Then will retry, throwing away
	 *	the allocated entry later (if we no longer need it).
d2011 1
a2011 1
	users = pmap->cpus_using;
d2046 1
a2046 1
			pmap->stats.wired_count++;
d2048 1
a2048 1
			pmap->stats.wired_count--;
d2077 2
d2082 1
a2082 1
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
d2084 1
a2084 1
				       PMAP_MANAGED(pa) ? 1 : 0);
d2096 1
a2096 1
		if (PMAP_MANAGED(pa)) {
d2108 1
a2108 1
			pvl = pa_to_pvh(pa);
d2110 1
a2110 1
			if (pvl->pmap == PMAP_NULL) {
d2114 3
a2116 3
				pvl->va = va;
				pvl->pmap = pmap;
				pvl->next = PV_ENTRY_NULL;
d2126 1
a2126 1
						if (e->pmap == pmap && e->va == va)
d2128 1
a2128 1
						e = e->next;
d2139 4
a2142 4
				pv_e->va = va;
				pv_e->pmap = pmap;
				pv_e->next = pvl->next;
				pvl->next = pv_e;
d2153 1
a2153 1
		pmap->stats.resident_count++;
d2155 1
a2155 1
			pmap->stats.wired_count++;
d2171 1
a2171 1
	} /* if ( pa == old_pa ) ... else */
d2178 2
a2179 2
	return (0);
} /* pmap_enter */
d2197 1
a2197 3
pmap_unwire(map, v)
	pmap_t map;
	vaddr_t v;
d2199 2
a2200 2
	pt_entry_t  *pte;
	int      spl;
d2202 1
a2202 1
	PMAP_LOCK(map, spl);
d2204 1
a2204 1
	if ((pte = pmap_pte(map, v)) == PT_ENTRY_NULL)
d2209 1
a2209 1
		map->stats.wired_count--;
d2213 2
a2214 2
	PMAP_UNLOCK(map, spl);
} /* pmap_unwire() */
d2233 1
a2233 1
 * batc_entry is scanned to find out the mapping. 
d2242 1
a2242 4
pmap_extract(pmap, va, pap)
	pmap_t pmap;
	vaddr_t va;
	paddr_t *pap;
d2244 4
a2247 4
	pt_entry_t	*pte;
	paddr_t		pa;
	int		spl;
	boolean_t	rv = FALSE;
d2250 1
a2250 1
	int		i;
d2266 1
a2266 1
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) | 
d2268 1
a2268 1
				return (TRUE);
d2286 2
a2287 30
	return (rv);
} /* pmap_extract() */

/*
 * Routine:	PMAP_COPY
 *
 * Function:
 *	Copy the range specigfied by src_adr/len from the source map
 *	to the range dst_addr/len in the destination map. This routine
 *	is only advisory and need not do anything.
 *
 * Parameters:
 *	dst_pmap	pointer to destination  pmap structure
 *	src_pmap	pointer to source pmap structure
 *	dst_addr	VA in destination map
 *	len		length of address space being copied
 *	src_addr	VA in source map
 *
 * At this time, the 88200 pmap implementation does nothing in this
 * function. Translation tables in the destination map will be allocated
 * at VM fault time.
 */
/* ARGSUSED */
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t dst_pmap, src_pmap;
	vaddr_t dst_addr, src_addr;
	vsize_t len;
{
}/* pmap_copy() */
d2326 1
a2326 2
pmap_collect(pmap)
	pmap_t pmap;
d2328 13
a2340 21
	vaddr_t		sdt_va;		/* outer loop index */
	vaddr_t		sdt_vt;		/* end of segment */
	sdt_entry_t	*sdttbl;	/* ptr to first entry in seg table */
	sdt_entry_t	*sdtp;		/* ptr to index into segment table */
	sdt_entry_t	*sdt;		/* ptr to index into segment table */
	pt_entry_t	*gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t	*gdttblend;	/* ptr to byte after last entry in
					   table group */
	pt_entry_t	*gdtp;		/* ptr to index into a page table */
	boolean_t	found_gdt_wired; /* flag indicating a wired page exists 
					   in a page table's address range */
	int		spl;
	u_int		i, j;

#ifdef DIAGNOSTIC
	if (pmap == PMAP_NULL)
		panic("pmap_collect: pmap is NULL");

	if (pmap == kernel_pmap)
		panic("pmap_collect attempted on kernel pmap");
#endif
d2349 1
a2349 1
	sdttbl = pmap->sdt_vaddr; /* addr of segment table */
d2352 3
a2354 3
	/* 
	  This contortion is here instead of the natural loop 
	  because of integer overflow/wraparound if VM_MAX_ADDRESS 
d2402 1
a2402 1
		 * calls uvm_km_free or free, which will invoke another 
d2408 1
a2408 2

	} /* Segment table Loop */
d2414 1
a2414 1
		printf  ("(pmap_collect: %x) done\n", curproc);
d2416 1
a2416 1
} /* pmap collect() */
d2420 1
a2420 1
 * 
d2442 1
a2442 2
pmap_activate(p)
	struct proc *p;
d2444 3
a2446 3
	apr_template_t	apr_data;
	pmap_t		pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int		cpu = cpu_number();  
d2449 1
a2449 1
	int		n;
d2459 1
a2459 1
		 *	Lock the pmap to put this cpu in its active set.
d2462 1
a2462 1
		simple_lock(&pmap->lock);
d2464 1
a2464 1
		apr_data.field.st_base = atop(pmap->sdt_paddr); 
d2472 3
a2474 3
		 * cmmu_pmap_activate will set the uapr and the batc entries, 
		 * then flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE 
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE 
d2477 2
a2478 2
		cmmu_pmap_activate(cpu, apr_data.bits, 
				   pmap->i_batc, pmap->d_batc);
d2480 1
a2480 1
			*(register_t *)&batc_entry[n] = pmap->i_batc[n].bits;
d2489 1
a2489 1
		SETBIT_CPUSET(cpu, &(pmap->cpus_using));
d2491 1
a2491 1
		simple_unlock(&pmap->lock);
d2493 1
a2493 1
} /* pmap_activate() */
d2505 1
a2505 1
 * pmap_deactive simply clears the cpus_using field in given pmap structure.
d2509 1
a2509 2
pmap_deactivate(p)
	struct proc *p;
d2511 2
a2512 2
	pmap_t	pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int	cpu = cpu_number();  
d2518 3
a2520 3
		simple_lock(&pmap->lock);
		CLRBIT_CPUSET(cpu, &(pmap->cpus_using));
		simple_unlock(&pmap->lock);
d2522 1
a2522 1
} /* pmap_deactivate() */
d2553 4
a2556 4
	vaddr_t		dstva, srcva;
	int		spl;
	pt_entry_t	template, *dstpte, *srcpte;
	int		cpu = cpu_number();
d2562 1
a2562 1
	 *	Map source physical address.
d2575 1
a2575 1
	 *	Map destination physical address.
d2585 1
a2585 1
} /* pmap_copy_page() */
d2594 1
a2594 1
 *	pg	vm_page
d2599 1
a2599 2
 *	pv_head_table, pv_lists
 *	pmap_cached_attributes
a2601 1
 *	pa_to_pvh
d2611 9
a2619 17
pmap_changebit(pg, set, mask)
	paddr_t pg;
	int set, mask;
{
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*pte, npte;
	pmap_t		pmap;
	int		spl;
	vaddr_t		va;
	u_int		users;
	boolean_t	kflush;

#ifdef DIAGNOSTIC
	if (!PMAP_MANAGED(pg))
		panic("pmap_changebit: not managed?");
#endif
d2624 1
a2624 1
	pvl = pa_to_pvh(pg);
d2629 1
a2629 1
	*pa_to_attribute(pg) &= mask;
d2631 1
a2631 1
	if (pvl->pmap == PMAP_NULL) {
d2634 1
a2634 1
			printf("(pmap_changebit: %x) phys addr 0x%x not mapped\n",
d2642 4
a2645 4
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->next) {
		pmap = pvep->pmap;
		va = pvep->va;
		if (!simple_lock_try(&pmap->lock)) {
d2648 1
a2648 1
		users = pmap->cpus_using;
d2665 1
a2665 1
		if (ptoa(PG_PFNUM(*pte)) != pg)
d2678 1
a2678 1
		 * Invalidate pte temporarily to avoid the modified bit 
d2686 1
a2686 1
		simple_unlock(&pmap->lock);
d2689 1
a2689 1
} /* pmap_changebit() */
d2698 1
a2698 2
pmap_clear_modify(pg)
	struct vm_page *pg;
a2699 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d2702 2
a2703 2
	rv = pmap_testbit(pa, PG_M);
	pmap_changebit(pa, 0, ~PG_M);
d2714 1
a2714 1
 *	pg	vm_page
d2718 1
a2718 2
 *	pv_head_array, pv lists
 *	pmap_cached_attributes
a2721 1
 *	pa_to_pvh
d2734 6
a2739 14
pmap_testbit(pg, bit)
	paddr_t pg;
	int bit;
{
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*ptep;
	int		spl;
	boolean_t	rv;

#ifdef DIAGNOSTIC
	if (!PMAP_MANAGED(pg))
		panic("pmap_testbit: not managed?");
#endif
d2743 1
a2743 1
	pvl = pa_to_pvh(pg);
d2746 1
a2746 1
	if (*pa_to_attribute(pg) & bit) {
d2758 1
a2758 1
	if (pvl->pmap == PMAP_NULL) {
d2761 1
a2761 1
		rv = (boolean_t)(*pa_to_attribute(pg) & bit);
d2764 1
a2764 1
			printf("(pmap_testbit: %x) phys addr 0x%x not mapped\n",
d2774 1
a2774 1
		if (!simple_lock_try(&pvep->pmap->lock)) {
d2778 3
a2780 3
		ptep = pmap_pte(pvep->pmap, pvep->va);
		if (ptep == PT_ENTRY_NULL) {
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->va);
d2783 3
a2785 3
		if (*ptep & bit) {
			simple_unlock(&pvep->pmap->lock);
			*pa_to_attribute(pg) |= bit;
d2788 1
a2788 1
				printf("(pmap_testbit: %x) modified page pte@@0x%p\n", curproc, ptep);
d2793 2
a2794 2
		simple_unlock(&pvep->pmap->lock);
		pvep = pvep->next;
d2799 1
a2799 1
} /* pmap_testbit() */
d2809 1
a2809 2
pmap_is_modified(pg)
	struct vm_page *pg;
d2811 2
a2812 4
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);

	return (pmap_testbit(phys, PG_M));
} /* pmap_is_modified() */
d2821 1
a2821 2
pmap_clear_reference(pg)
	struct vm_page *pg;
a2822 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d2825 2
a2826 2
	rv = pmap_testbit(pa, PG_U);
	pmap_changebit(pa, 0, ~PG_U);
d2838 1
a2838 2
pmap_is_referenced(pg)
	struct vm_page *pg;
d2840 1
a2840 3
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);

	return (pmap_testbit(phys, PG_U));
d2853 1
a2853 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
a2854 2
	paddr_t phys = VM_PAGE_TO_PHYS(pg);

d2859 1
a2859 1
		pmap_changebit(phys, PG_RO, ~0);
d2865 1
a2865 1
		pmap_remove_all(phys);
d2871 1
a2871 2
pmap_virtual_space(startp, endp)
	vaddr_t *startp, *endp;
d2878 1
a2878 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d2880 3
a2882 3
	int		spl;
	pt_entry_t	template, *pte;
	u_int		users;
d2884 2
a2885 2
	CHECK_PAGE_ALIGN (va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN (pa, "pmap_kenter_pa - PA");
d2894 1
a2894 1
	users = kernel_pmap->cpus_using;
d2908 2
a2909 2
	kernel_pmap->stats.resident_count++;
	kernel_pmap->stats.wired_count++;
d2923 1
a2923 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d2925 2
a2926 2
	int		spl;
	u_int		users;
d2937 1
a2937 1
	users = kernel_pmap->cpus_using;
d2958 2
a2959 2
		kernel_pmap->stats.resident_count--;
		kernel_pmap->stats.wired_count--;
@


1.66
log
@Spring pmap cleaning:
- update and fix comments
- clean the batc code (still not used by default)
- there was some KNF left to do
- fix incorrect locking behaviour in pmap_remove_all()
- fix incorrect pmap_remove_range() call which would cause the kernel to
  enter an infinite loop sometimes at shutdown
- fix an off-by-one page loop in pmap_protect()

This should bring the extra bits of stability I need to resume working on the
compiler...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2002/03/14 03:15:57 millert Exp $	*/
d1231 1
a1231 2
pmap_zero_page(phys)
	paddr_t phys;
d1233 1
d2754 1
a2754 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d2756 2
@


1.65
log
@Final __P removal plus some cosmetic fixups
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2002/03/14 01:26:40 millert Exp $	*/
a47 3
/* don't want to make them general yet. */
#define OMRON_PMAP

d93 17
a109 18
#define CD_MAPB		0x0000020	/* pmap_map_batc */
#define CD_CACHE	0x0000040	/* pmap_cache_ctrl */
#define CD_BOOT		0x0000080	/* pmap_bootstrap */
#define CD_INIT		0x0000100	/* pmap_init */
#define CD_CREAT	0x0000200	/* pmap_create */
#define CD_FREE		0x0000400	/* pmap_release */
#define CD_DESTR	0x0000800	/* pmap_destroy */
#define CD_RM		0x0001000	/* pmap_remove */
#define CD_RMAL		0x0002000	/* pmap_remove_all */
#define CD_PROT		0x0004000	/* pmap_protect */
#define CD_EXP		0x0008000	/* pmap_expand */
#define CD_ENT		0x0010000	/* pmap_enter */
#define CD_UPD		0x0020000	/* pmap_update */
#define CD_COL		0x0040000	/* pmap_collect */
#define CD_CBIT		0x0080000	/* pmap_changebit */
#define CD_TBIT		0x0100000	/* pmap_testbit */
#define CD_CREF		0x0200000	/* pmap_clear_reference */
#define CD_PGMV		0x0400000	/* pagemove */
d153 1
a153 1
#if (defined(MVME187) || defined(MVME197))
d159 1
d161 4
d174 1
a174 1
 * The Modify List
d177 2
a178 2
 * of modified flags for pages which are no longer containd in any
 * pmap. (for mapped pages, the modified flags are in the PTE.)
d180 1
a180 1
char  *pmap_modify_list;
d234 2
d246 2
d261 1
a261 2
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int);
vaddr_t pmap_map_batc(vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int);
d429 1
a429 1
		printf("(pmap_expand_kmap :%x) v %x\n", curproc,virt);
d439 3
a441 4
	if (kpdt_ent == KPDT_ENTRY_NULL) {
		printf("pmap_expand_kmap: Ran out of kernel pte tables\n");
		return (PT_ENTRY_NULL);
	}
d447 2
a448 2
	(unsigned)(kpdt_ent->phys) = 0;
	(unsigned)(kpdt_ent->next) = 0;
d496 1
a496 1
	unsigned int cmode;
d498 2
a499 2
	unsigned	npages;
	unsigned	num_phys_pages;
d502 4
d509 1
a509 1
		printf ("(pmap_map :%x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
d520 1
a520 88
	page = trunc_page(start);
	npages = atop(round_page(end) - page);
	for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
				panic("pmap_map: Cannot allocate pte table");

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			if (PDT_VALID(pte))
				printf("(pmap_map :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
#endif
		*pte = template | page;
		page += PAGE_SIZE;
		virt += PAGE_SIZE;
	}
	return virt;
} /* pmap_map() */

/*
 * Routine:	PMAP_MAP_BATC
 *
 * Function:
 *    Map memory using BATC at initialization. The physical addresses being
 * mapped are not managed and are never unmapped.
 *
 * Parameters:
 *	virt	virtual address of range to map
 *	start	physical address of range to map
 *	end	physical address of end of range
 *	prot	protection attributes
 *	cmode	cache control attributes
 *
 * External & Global:
 *	batc_used	number of BATC used
 *
 * Calls:
 *	m88k_protection
 *	pmap_pte
 *	pmap_expand_kmap
 *
 * For each page that needs mapping:
 *	If both virt and phys are on the BATC block boundary, map using BATC.
 *	Else make mapping in the same manner as pmap_map.
 *
 *	initialize BATC and pte template
 *	look for number of phys pages in range
 *	{
 *		if virt and phys are on BATC block boundary
 *		{
 *			map using BATC
 *			increment virt and phys one BATC block
 *			continue outer loop
 *		}
 *		pmap_pte(virt)	- expand if necessary
 *		stuff pte from template
 *		increment virt one page
 *		increment template paddr one page
 *	}
 *
 */
vaddr_t
pmap_map_batc(virt, start, end, prot, cmode)
	vaddr_t virt;
	paddr_t start, end;
	vm_prot_t prot;
	unsigned int cmode;
{
	unsigned	npages;
	unsigned	num_phys_pages;
	pt_entry_t	template, *pte;
	paddr_t		page;
	batc_template_t	batctmp;
	int		i;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
		printf ("(pmap_map_batc :%x) phys address from %x to %x mapped at virtual %x, prot %x\n", curproc,
			start, end, virt, prot);
#endif

#ifdef DIAGNOSTIC
	if (start > end)
		panic("pmap_map_batc: start greater than end address");
#endif


	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;
d534 2
d538 4
a541 1
	for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {
d543 2
a544 2
		if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
			printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, "
d560 1
a560 1
			for (i = 0; i < max_cpus; i++)
d566 2
a567 2
			if ((pmap_con_dbg & (CD_MAPB | CD_NORM)) == (CD_MAPB | CD_NORM)) {
				printf("(pmap_map_batc :%x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
d569 3
a571 4
			if (pmap_con_dbg & CD_MAPB) {

				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE ) {
					pte = pmap_pte(kernel_pmap, virt+i);
d573 2
a574 1
						printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, *pte);
a575 1
			}
d583 2
d586 2
a587 2
			if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
				panic("pmap_map_batc: Cannot allocate pte table");
d589 2
a590 2
#ifdef	DEBUG
		if (pmap_con_dbg & CD_MAPB)
d592 1
a592 1
				printf("(pmap_map_batc :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
d595 1
a595 1
		*pte = template | trunc_page(page);
d600 1
a600 1
} /* pmap_map_batc() */
d613 1
a613 1
 *	unsigned	mode
d630 1
a630 1
	unsigned mode;
d634 1
a634 1
	vaddr_t		va;
d637 1
a637 1
	unsigned	users;
d645 1
a645 1
		printf("(pmap_cache_ctrl :%x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
d668 1
a668 1
			printf("(cache_ctrl) pte@@0x%08x\n",(unsigned)pte);
d682 2
a683 1
		for (cpu = 0; cpu < max_cpus; cpu++)
d685 1
a685 2
				cmmu_flush_remote_cache(cpu, ptoa(PG_PFNUM(*pte)),
							PAGE_SIZE);
d716 1
a716 1
 *	pmap_map or pmap_map_batc
d756 1
a756 1
	extern char	*kernelstart, *etext;
d760 1
a760 1
		printf("pmap_bootstrap : \"load_start\" 0x%x\n", load_start);
d765 1
a765 1
		panic("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x", load_start);
d788 1
a788 1
	    (trunc_page((unsigned)&kernelstart) - load_start);
d827 1
a827 1
		printf("       kernel segment end = 0x%x\n", ((unsigned)kernel_pmap->sdt_paddr) + kernel_pmap_size);
a843 1
	pdt_size = MAX_KERNEL_PDT_SIZE;
d845 1
a845 1
	pdt_size = round_page(pdt_size);
d873 1
a873 1
	for (i = pdt_size / PDT_SIZE; i > 0; i--) {
d886 2
a887 2
	e_text = load_start + ((unsigned)&etext -
	    trunc_page((unsigned)&kernelstart));
d891 3
a893 8
#ifdef OMRON_PMAP
#define PMAPER	pmap_map
#else
#define PMAPER	pmap_map_batc
#endif

	/*  map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
	vaddr = PMAPER(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d895 2
a896 2
	/*  map the kernel text read only */
	vaddr = PMAPER((vaddr_t)trunc_page(((unsigned)&kernelstart)),
d900 1
a900 1
	vaddr = PMAPER(vaddr, e_text, (paddr_t)kmap,
d918 1
a918 1
	vaddr = PMAPER(vaddr, (paddr_t)kmap, *phys_start,
d929 1
a929 1
			printf("1:vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
d949 1
a949 1
		vaddr = PMAPER(vaddr, *phys_start, *phys_start + etherlen,
d958 1
a958 1
				printf("2:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
d1004 1
a1004 1
	for (; ptable->size != 0xffffffffU; ptable++){
d1009 1
a1009 1
			PMAPER(ptable->virt_start, ptable->phys_start,
a1013 1
#undef PMAPER
d1125 1
a1125 1
 *	pmap_modify_list
d1134 1
a1134 1
 * for the pv_head_table, pmap_modify_list; and sets these
d1154 1
a1154 1
	char		*attr;
d1169 1
a1169 1
	s += npages * sizeof(char);		/* pmap_modify_list */
d1185 1
a1185 1
	pmap_modify_list = (char *)addr;
d1192 1
a1192 1
	attr = pmap_modify_list;
d1244 1
a1244 1
	cmmu_flush_tlb(1, srcva, PAGE_SIZE);
d1269 7
a1275 4
	pmap_t			p;
	sdt_entry_t		*segdt;
	int			i;
	unsigned int		s;
d1286 1
a1286 1
		printf("(pmap_create :%x) need %d pages for sdt\n",
d1299 3
a1301 1
	pmap_extract(kernel_pmap, (vaddr_t)segdt, (paddr_t *)&p->sdt_paddr);
d1309 2
a1310 2
		printf("(pmap_create :%x) pmap=0x%x, sdt_vaddr=0x%x, sdt_paddr=0x%x\n",
		       curproc, (unsigned)p, p->sdt_vaddr, p->sdt_paddr);
d1339 1
a1339 1
#ifdef OMRON_PMAP
d1389 1
a1389 1
	unsigned int	i,j;
d1393 1
a1393 1
		printf("(pmap_release :%x) pmap %x\n", curproc, pmap);
d1413 1
a1413 1
				printf("(pmap_release :%x) free page table = 0x%x\n",
d1422 1
a1422 1
		printf("(pmap_release :%x) free segment table = 0x%x\n",
d1428 1
a1428 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, 2*SDT_SIZE);
d1432 1
a1432 1
		printf("(pmap_release :%x) ref_count = 0\n", curproc);
d1515 1
a1515 1
 *	e		virtual address of start of range to remove
d1519 1
a1519 1
 *	pmap_modify_list
d1538 2
a1539 2
 * entry in the pmap_modify_list. Next, the function must find the PV list
 * entry associated with this pmap/va (if it doesn't exist - the function
d1553 1
a1553 1
	unsigned	users;
d1638 1
a1638 1
					       "0x%x (pa 0x%x) PV list at 0x%x\n", va, pa, (unsigned)pvl);
a1643 1
				/*pvl = pa_to_pvh(pa);*/
d1698 1
a1698 1
	if (map == PMAP_NULL) {
d1700 1
a1700 1
	}
d1703 1
a1703 1
		printf("(pmap_remove :%x) map %x  s %x  e %x\n", curproc, map, s, e);
d1707 1
a1707 1
	if (s > e)
d1729 1
a1729 1
 *	pmap_modify_list
d1742 2
a1743 1
 * pmap_modify_list entry corresponding to the physical page is set to 1.
d1767 1
a1767 1
			printf("(pmap_remove_all :%x) phys addr 0x%x not a managed page\n", curproc, phys);
d1775 2
a1776 3
	 * We have to do the same work as in pmap_remove_pte_page
	 * since that routine locks the pv_head. We don't have
	 * to lock the pv_head, since we have the entire pmap system.
d1799 2
a1800 2
			printf("(pmap_remove_all :%x) phys %x pmap %x va %x dbgcnt %x\n",
			       (unsigned)curproc, phys, (unsigned)pmap, va, dbgcnt);
d1807 1
a1807 1
			continue;	/* no page mapping */
d1816 1
a1816 1
			continue;
d1819 1
a1819 1
		pmap_remove_range(pmap, va, va);
d1821 3
d1828 1
a1828 1

a1829 3
#ifdef DEBUG
		dbgcnt++;
#endif
d1869 1
a1869 1
	unsigned	users;
a1898 2
	 * Do not assume that either start or end fall on any
	 * kind of page boundary (though this may be true ?!).
d1900 1
a1900 1
	for (va = s; va <= e; va += PAGE_SIZE) {
d1904 5
a1908 3
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
d1910 1
d1913 1
a1913 1
				printf("(pmap_protect :%x) no page table :: skip to 0x%x\n", curproc, va + PAGE_SIZE);
d1921 1
a1921 1
				printf("(pmap_protect :%x) pte invalid pte @@ 0x%x\n", curproc, pte);
d1971 1
a1971 1
 * 2:   The page table entries (PTEs) are initialized (set invalid), and
d1999 1
a1999 1
		printf ("(pmap_expand :%x) map %x v %x\n", curproc, map, v);
d2006 2
a2007 1
	pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr);
d2031 1
a2031 1
			printf("(pmap_expand :%x) table has already been allocated\n", curproc);
d2040 1
a2040 1
	 * first page table in the group, and initilize all the
d2043 1
a2043 1
	v &= ~((1<<(LOG2_PDT_TABLE_GROUP_SIZE+PDT_BITS+PG_BITS))-1);
d2070 1
a2070 1
 * N.B.: This is only routine which MAY NOT lazy-evaluation or lose
d2083 1
a2083 1
 *	pmap_modify_list
d2142 2
a2143 2
	unsigned	users;
	int		kflush;
d2156 1
a2156 1
			printf("(pmap_enter :%x) pmap kernel va %x pa %x\n", curproc, va, pa);
d2158 1
a2158 1
			printf("(pmap_enter :%x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
d2181 1
a2181 3
			if (pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE) ==
			    PT_ENTRY_NULL)
				panic("pmap_expand: Cannot allocate kernel pte table");
a2238 5
		if (old_pa != (paddr_t)-1) {
			/*
			 *	Invalidate the translation buffer,
			 *	then remove the mapping.
			 */
d2240 7
a2246 8
			if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
				if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
					printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
					       phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
					       PMAP_MANAGED(pa) ? 1 : 0);
					printf("pte %x pfn %x valid %x\n",
					       pte, PG_PFNUM(*pte), PDT_VALID(pte));
				}
d2248 1
d2250 4
a2253 5
			if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				flush_atc_entry(users, va, TRUE);
			} else {
				pmap_remove_range(pmap, va, va + PAGE_SIZE);
			}
d2342 1
a2342 1
 *	Routine:	pmap_unwire
d2344 1
a2344 5
 *	Function:	Change the wiring attributes for a map/virtual-address
 *			Pair.
 *	Parameters:
 *		pmap		pointer to pmap structure
 *		v		virtual address of page to be unwired
d2346 6
a2351 2
 *	Calls:
 *		pmap_pte
d2353 2
a2354 2
 *	Special Assumptions:
 *		The mapping must already exist in the pmap.
d2394 2
d2397 1
a2397 4
 *	This routine checks BATC mapping first. BATC has been used and 
 * the specified pmap is kernel_pmap, batc_entry is scanned to find out
 * the mapping. 
 * 	Then the routine calls pmap_pte to get a (virtual) pointer to
a2410 1
	int		i;
d2414 4
d2423 1
d2427 2
a2428 2
	if (pmap == kernel_pmap && batc_used > 0)
		for (i = batc_used - 1; i > 0; i--)
d2435 1
d2455 1
a2455 1
 *	Routine:	PMAP_COPY
d2457 4
a2460 11
 *	Function:
 *		Copy the range specigfied by src_adr/len from the source map
 *		to the range dst_addr/len in the destination map. This routine
 *		is only advisory and need not do anything.
 *
 *	Parameters:
 *		dst_pmap	pointer to destination  pmap structure
 *		src_pmap	pointer to source pmap structure
 *		dst_addr	VA in destination map
 *		len		length of address space being copied
 *		src_addr	VA in source map
d2462 8
a2469 1
 *	At this time, the 88200 pmap implementation does nothing in this
d2534 1
a2534 1
	unsigned int	i, j;
d2546 1
a2546 1
		printf ("(pmap_collect :%x) pmap %x\n", curproc, pmap);
d2566 1
a2566 1
		sdt_va = VM_MIN_ADDRESS + PDT_TABLE_GROUP_VA_SPACE*i;
d2588 2
a2589 4

		sdt_vt = sdt_va <= VM_MAX_ADDRESS-PDT_TABLE_GROUP_VA_SPACE ?
			 sdt_va+PDT_TABLE_GROUP_VA_SPACE : 
			 VM_MAX_ADDRESS;
d2595 1
a2595 1
		 * we can safely deallocated the page map(s)
d2599 1
a2599 1
			*((sdt_entry_t *)(sdt+SDT_ENTRIES)) = 0;
d2617 1
a2617 1
		printf  ("(pmap_collect :%x) done \n", curproc);
d2649 4
a2652 2
#ifdef notyet
#ifdef OMRON_PMAP
a2654 3
#endif
	pmap_t		pmap = p->p_vmspace->vm_map.pmap;
	int		cpu = cpu_number();  
d2658 1
a2658 1
		printf("(pmap_activate :%x) pmap 0x%x\n", p, (unsigned)pmap);
d2673 2
a2674 2
#ifdef notyet
#ifdef OMRON_PMAP
d2684 1
a2684 5
			*(unsigned*)&batc_entry[n] = pmap->i_batc[n].bits;
#else
		cmmu_set_uapr(apr_data.bits);
		cmmu_flush_tlb(0, 0, -1);
#endif
a2685 4
		/*
		 * I am forcing it to not program the BATC at all. pmap.c module
		 * needs major, major cleanup. XXX nivas
		 */
d2687 2
a2688 2
		cmmu_flush_tlb(0, 0, -1);
#endif /* notyet */
a2695 10
	} else {
		/*
		 * kernel_pmap must be always active.
		 */

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
			printf("(pmap_activate :%x) called for kernel_pmap\n", curproc);
#endif

d2716 1
a2716 1
	pmap_t	pmap = p->p_vmspace->vm_map.pmap;
d2730 4
a2733 1
 *	Routine:	PMAP_COPY_PAGE
d2735 3
a2737 2
 *	Function:
 *		Copies the specified pages.
d2739 3
a2741 7
 *	Parameters:
 *		src		PA of source page
 *		dst		PA of destination page
 *
 *	Extern/Global:
 *		phys_map_vaddr1
 *		phys_map_vaddr2
d2743 2
a2744 2
 *	Calls:
 *		m88kprotection
d2746 2
a2747 2
 *	Special Assumptions:
 *		no locking reauired
d2749 1
a2749 1
 *	This routine maps the physical pages at the 'phys_map' virtual
d2775 1
a2775 1
	cmmu_flush_tlb(1, srcva, PAGE_SIZE);
d2781 1
a2781 1
	cmmu_flush_tlb(1, dstva, PAGE_SIZE);
d2792 1
a2792 1
 *	Routine:	PMAP_CHANGEBIT
d2794 7
a2800 2
 *	Function:
 *		Update the pte bits on the specified physical page.
d2802 3
a2804 8
 *	Parameters:
 *		pg	vm_page
 *		set	bits to set
 *		mask	bits to mask
 *
 *	Extern/Global:
 *		pv_head_table, pv_lists
 *		pmap_modify_list
d2806 3
a2808 3
 *	Calls:
 *		pa_to_pvh
 *		pmap_pte
d2827 1
a2827 1
	unsigned	users;
a2842 1
	/* *pa_to_attribute(pg) |= set; */
d2848 1
a2848 1
			printf("(pmap_changebit :%x) phys addr 0x%x not mapped\n",
d2855 1
a2855 1
	/* for each listed pmap, turn off the page modified bit */
d2906 1
a2906 1
 *	Routine:	PMAP_CLEAR_MODIFY
d2908 2
a2909 2
 *	Function:
 *		Clears the modify bits on the specified physical page.
d2924 1
a2924 1
 *	Routine:	PMAP_TESTBIT
d2926 6
a2931 2
 *	Function:
 *		Test the modified/referenced bits of a physical page.
d2933 3
a2935 7
 *	Parameters:
 *		pg		vm_page
 *		bit		bit to test
 *
 *	Extern/Global:
 *		pv_head_array, pv lists
 *		pmap_modify_list
d2937 4
a2940 6
 *	Calls:
 *		simple_lock, simple_unlock
 *		SPLVM, SPLX
 *		PMAP_MANAGED
 *		pa_to_pvh
 *		pmap_pte
d2942 1
a2942 1
 *	If the attribute list for the given page has the bit, this routine
d2945 1
a2945 1
 *	Otherwise, this routine walks the PV list corresponding to the
d2977 1
a2977 1
			printf("(pmap_testbit :%x) already cached a modify flag for this page\n",
d2990 1
a2990 1
			printf("(pmap_testbit :%x) phys addr 0x%x not mapped\n",
d3014 1
a3014 1
				printf("(pmap_testbit :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d3028 1
a3028 1
 *	Routine:	PMAP_IS_MODIFIED
d3030 3
a3032 3
 *	Function:
 *		Return whether or not the specified physical page is modified
 *		by any physical maps.
d3044 1
a3044 1
 *	Routine:	PMAP_CLEAR_REFERENCE
d3046 2
a3047 2
 *	Function:
 *		Clear the reference bit on the specified physical page.
d3124 1
a3124 1
	unsigned	users;
d3131 1
a3131 1
		printf ("(pmap_kenter_pa :%x) va %x pa %x\n", curproc, va, pa);
d3144 1
a3144 2
		if (pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE) == PT_ENTRY_NULL)
			panic("pmap_kenter_pa: Cannot allocate kernel pte table");
d3170 1
a3170 1
	unsigned	users;
d3174 1
a3174 1
		printf("(pmap_kremove :%x) va %x len %x\n", curproc, va, len);
d3183 1
a3183 1
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
@


1.64
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2002/02/05 23:07:38 miod Exp $	*/
d1331 1
a1331 1
	bzero((void*)srcva, PAGE_SIZE);
d2885 1
a2885 1
	bcopy((void*)srcva, (void*)dstva, PAGE_SIZE);
@


1.63
log
@- factorize insane amounts of code
- do not do some paranoid sanity checks unless #ifdef DIAGNOSTIC
- remove some of the less-usefull, most screen-eating debug information
- and fix a few typos in the process.
This pmap is slowly starting to have a decent looking shape... Really.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2002/01/23 00:39:47 art Exp $	*/
d251 11
a261 11
void flush_atc_entry __P((long, vaddr_t, boolean_t));
pt_entry_t *pmap_expand_kmap __P((vaddr_t, vm_prot_t));
void pmap_remove_range __P((pmap_t, vaddr_t, vaddr_t));
void pmap_expand __P((pmap_t, vaddr_t));
void pmap_release __P((pmap_t));
vaddr_t pmap_map __P((vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int));
vaddr_t pmap_map_batc __P((vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int));
pt_entry_t *pmap_pte __P((pmap_t, vaddr_t));
void pmap_remove_all __P((paddr_t));
void pmap_changebit __P((paddr_t, int, int));
boolean_t pmap_testbit __P((paddr_t, int));
@


1.62
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2001/12/27 22:33:46 miod Exp $	*/
d3 1
a3 1
 * Copyright (c) 2001 Miodrag Vallat
d105 9
a113 13
#define CD_COW		0x0004000	/* pmap_copy_on_write */
#define CD_PROT		0x0008000	/* pmap_protect */
#define CD_EXP		0x0010000	/* pmap_expand */
#define CD_ENT		0x0020000	/* pmap_enter */
#define CD_UPD		0x0040000	/* pmap_update */
#define CD_COL		0x0080000	/* pmap_collect */
#define CD_CMOD		0x0100000	/* pmap_clear_modify */
#define CD_IMOD		0x0200000	/* pmap_is_modified */
#define CD_CREF		0x0400000	/* pmap_clear_reference */
#define CD_PGMV		0x0800000	/* pagemove */
#define CD_CHKPV	0x1000000	/* check_pv_list */
#define CD_CHKPM	0x2000000	/* check_pmap_consistency */
#define CD_CHKM		0x4000000	/* check_map */
a233 21
 * Consistency checks.
 * These checks are disabled by default; enabled by setting CD_FULL
 * in pmap_con_dbg.
 */

#ifdef DEBUG

void check_pv_list __P((vaddr_t, pv_entry_t, char *));
void check_pmap_consistency __P((char *));
void check_map __P((pmap_t, vaddr_t, vaddr_t, char *));

#define CHECK_PV_LIST(phys,pv_h,who) \
	if (pmap_con_dbg & CD_CHKPV) check_pv_list(phys,pv_h,who)
#define CHECK_PMAP_CONSISTENCY(who) \
	if (pmap_con_dbg & CD_CHKPM) check_pmap_consistency(who)
#else
#define CHECK_PV_LIST(phys,pv_h,who)
#define CHECK_PMAP_CONSISTENCY(who)
#endif /* DEBUG */

/*
a253 1
void pmap_copy_on_write __P((paddr_t));
d260 2
a1350 1
	pmap_statistics_t	stats;
d1355 1
a1355 3
	CHECK_PMAP_CONSISTENCY("pmap_create");

	p = (struct pmap *)pool_get(&pmappool, PR_WAITOK);
a1422 7
	/*
	 * Initialize statistics.
	 */
	stats = &p->stats;
	stats->resident_count = 0;
	stats->wired_count = 0;

d1541 1
a1541 9
	int	c, s;

	if (p == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
			printf("(pmap_destroy :%x) pmap is NULL\n", curproc);
#endif
		return;
	}
d1548 4
a1551 7
	CHECK_PMAP_CONSISTENCY("pmap_destroy");

	PMAP_LOCK(p, s);
	c = --p->ref_count;
	PMAP_UNLOCK(p, s);

	if (c == 0) {
d1567 1
a1567 4
 * Calls:
 *	PMAP_LOCK, PMAP_UNLOCK
 *
 *   Under a pmap read lock, the ref_count field of the pmap structure
a1573 7
	int	s;

	if (p != PMAP_NULL) {
		PMAP_LOCK(p, s);
		p->ref_count++;
		PMAP_UNLOCK(p, s);
	}
d1575 3
a1648 2
	CHECK_PAGE_ALIGN(s, "pmap_remove_range - start addr");

a1683 1
			CHECK_PV_LIST(pa, pvl, "pmap_remove_range before");
d1706 3
a1708 2
				for (prev = pvl; (cur = prev->next) != PV_ENTRY_NULL; prev = cur) {
					if (cur->va == va && cur->pmap == pmap) {
d1710 1
a1710 1
					}
d1720 1
a1721 2

			CHECK_PV_LIST(pa, pvl, "pmap_remove_range after");
d1739 1
a1739 1
				*pa_to_attribute(pa) = TRUE;
d1751 2
a1752 1
 *	It is assumed that start is properly rounded to the VM page size.
a1782 2
	CHECK_PAGE_ALIGN(s, "pmap_remove start addr");

d1830 1
a1830 1
	pv_entry_t	pvl, cur;
a1834 3
	unsigned	users;
	pt_entry_t	opte;
	boolean_t	kflush;
a1857 1
	CHECK_PV_LIST(phys, pvl, "pmap_remove_all before");
d1864 1
a1864 1
		if (!simple_lock_try(&pmap->lock)) {
a1865 7
		}
		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}
d1873 1
d1881 14
a1894 8
		if (!PDT_VALID(pte))
			panic("pmap_remove_all: pte invalid");
		if (ptoa(PG_PFNUM(*pte)) != phys)
			panic("pmap_remove_all: pte doesn't point to page");
		if (pmap_pte_w(pte))
			panic("pmap_remove_all: removing a wired page");

		pmap->stats.resident_count--;
d1896 1
a1896 20
		if ((cur = pvl->next) != PV_ENTRY_NULL) {
			*pvl = *cur;
			pool_put(&pvpool, cur);
		} else
			pvl->pmap = PMAP_NULL;

		/*
		 * Reflect modified pages to pager.
		 *
		 * Invalidate pte temporarily to avoid the modified 
		 * bit and/or the reference bit being written back
		 * by any other cpu.
		 */
		opte = invalidate_pte(pte);
		flush_atc_entry(users, va, kflush);

		if (opte & PG_M) {
			/* keep track ourselves too */
			*pa_to_attribute(phys) = TRUE;
		}
a1907 2
	CHECK_PV_LIST(phys, pvl, "pmap_remove_all after");

a1911 103
 * Routine:	PMAP_COPY_ON_WRITE
 *
 * Function:
 *	Remove write privileges from all physical maps for this physical page.
 *
 * Parameters:
 *	phys		physical address of page to be read-protected.
 *
 * Calls:
 *	pa_to_pvh
 *	simple_lock, simple_unlock
 *	pmap_pte
 *
 * Special Assumptions:
 *	All mapings of the page are user-space mappings.
 *
 *  This routine walks the PV list. For each pmap/va pair it locates
 * the page table entry (the PTE), and sets the hardware enforced
 * read-only bit. The TLB is appropriately flushed.
 */
void
pmap_copy_on_write(phys)
	paddr_t phys;
{
	pv_entry_t	pv_e;
	pt_entry_t	*pte;
	int		spl;
	unsigned	users;
	boolean_t	kflush;

	if (!PMAP_MANAGED(phys)) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_COW)
			printf("(pmap_copy_on_write :%x) phys addr 0x%x not managed \n", curproc, phys);
#endif
		return;
	}

	SPLVM(spl);

copy_on_write_Retry:
	pv_e = pa_to_pvh(phys);
	CHECK_PV_LIST(phys, pv_e, "pmap_copy_on_write before");
	
	if (pv_e->pmap  == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_COW | CD_NORM)) == (CD_COW | CD_NORM))
			printf("(pmap_copy_on_write :%x) phys addr 0x%x not mapped\n", curproc, phys);
#endif
		SPLX(spl);
		return;		/* no mappings */
	}

	/*
	 * Run down the list of mappings to this physical page,
	 * disabling write privileges on each one.
	 */

	while (pv_e != PV_ENTRY_NULL) {
		pmap_t	pmap = pv_e->pmap;
		vaddr_t	va = pv_e->va;

		if (!simple_lock_try(&pmap->lock)) {
			goto copy_on_write_Retry;
		}

		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}

		/*
		 * Check for existing and valid pte
		 */
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL)
			panic("pmap_copy_on_write: pte from pv_list not in map");
		if (!PDT_VALID(pte))
			panic("pmap_copy_on_write: invalid pte");
		if (ptoa(PG_PFNUM(*pte)) != phys)
			panic("pmap_copy_on_write: pte doesn't point to page");

		/*
		 * Flush TLBs of which cpus using pmap.
		 *
		 * Invalidate pte temporarily to avoid the modified 
		 * bit and/or the reference bit being written back
		 * by any other cpu.
		 */
		*pte = invalidate_pte(pte) | PG_PROT;
		flush_atc_entry(users, va, kflush);
		
		simple_unlock(&pmap->lock);
		pv_e = pv_e->next;
	}
	CHECK_PV_LIST(phys, pa_to_pvh(phys), "pmap_copy_on_write");

	SPLX(spl);
} /* pmap_copy_on_write */

/*
a2351 1
			CHECK_PV_LIST (pa, pvl, "pmap_enter before");
a2622 2
	CHECK_PMAP_CONSISTENCY ("pmap_collect");

a2699 2

	CHECK_PMAP_CONSISTENCY("pmap_collect");
d2892 1
a2892 1
 *	Routine:	PMAP_CLEAR_MODIFY
d2895 1
a2895 1
 *		Clear the modify bits on the specified physical page.
d2899 2
d2910 5
a2914 5
 *	The modify_list entry corresponding to the
 * page's frame index will be zeroed. The PV list will be traversed.
 * For each pmap/va the hardware 'modified' bit in the page descripter table
 * entry inspected - and turned off if  necessary. If any of the
 * inspected bits were found on, an TLB flush will be performed.
d2916 4
a2919 3
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
d2923 1
a2923 1
	pt_entry_t	*pte;
a2928 4
	boolean_t	ret;
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);

	ret = pmap_is_modified(pg);
d2931 2
a2932 2
	if (!PMAP_MANAGED(phys))
		panic("pmap_clear_modify: not managed?");
d2937 2
a2938 3
clear_modify_Retry:
	pvl = pa_to_pvh(phys);
	CHECK_PV_LIST(phys, pvl, "pmap_clear_modify");
d2940 5
a2944 2
	/* update corresponding pmap_modify_list element */
	*pa_to_attribute(phys) = FALSE;
d2948 3
a2950 2
		if ((pmap_con_dbg & (CD_CMOD | CD_NORM)) == (CD_CMOD | CD_NORM))
			printf("(pmap_clear_modify :%x) phys addr 0x%x not mapped\n", curproc, phys);
d2953 1
a2953 1
		return (ret);
d2961 1
a2961 1
			goto clear_modify_Retry;
d2971 5
d2977 6
a2982 1
			panic("pmap_clear_modify: bad pv list entry.");
d2985 8
d2996 4
a2999 3
		/* clear modified bit */
		*pte = invalidate_pte(pte) & ~PG_M;
		flush_atc_entry(users, va, kflush);
d3004 1
d3006 17
a3022 2
	return (ret);
} /* pmap_clear_modify() */
d3025 1
a3025 1
 *	Routine:	PMAP_IS_MODIFIED
d3028 1
a3028 3
 *		Return whether or not the specified physical page is modified
 *		by any physical maps. That is, whether the hardware has
 *		stored data into the page.
d3032 1
d3045 2
a3046 4
 *	If the entry in the modify list, corresponding to the given page,
 * is TRUE, this routine return TRUE. (This means at least one mapping
 * has been invalidated where the MMU had set the modified bit in the
 * page descripter table entry (PTE).
d3050 3
a3052 2
 * examined. If a modified bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list).
d3055 3
a3057 2
pmap_is_modified(pg)
	struct vm_page *pg;
d3063 1
a3063 2
	boolean_t	modified_flag;
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
d3066 2
a3067 2
	if (!PMAP_MANAGED(phys))
		panic("pmap_is_modified: not managed?");
d3072 2
a3073 3
	pvl = pa_to_pvh(phys);
	CHECK_PV_LIST (phys, pvl, "pmap_is_modified");
is_mod_Retry:
d3075 2
a3076 2
	if ((boolean_t)*pa_to_attribute(phys)) {
		/* we've already cached a modify flag for this page,
d3079 3
a3081 2
		if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
			printf("(pmap_is_modified :%x) already cached a modify flag for this page\n", curproc);
d3088 7
a3094 6
		/* unmapped page - get info from page_modified array
		   maintained by pmap_remove_range/ pmap_remove_all */
		modified_flag = (boolean_t)*pa_to_attribute(phys);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
			printf("(pmap_is_modified :%x) phys addr 0x%x not mapped\n", curproc, phys);
d3097 1
a3097 1
		return (modified_flag);
d3104 1
a3104 1
			goto is_mod_Retry;
d3109 2
a3110 2
			printf("pmap_is_modified: pte from pv_list not in map virt = 0x%x\n", pvep->va);
			panic("pmap_is_modified: bad pv list entry");
d3112 1
a3112 1
		if (pmap_pte_m(ptep)) {
d3114 1
d3116 2
a3117 2
			if ((pmap_con_dbg & (CD_IMOD | CD_FULL)) == (CD_IMOD | CD_FULL))
				printf("(pmap_is_modified :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d3128 16
d3147 1
a3147 1
 *	Routine:	PMAP_CLEAR_REFERECE
d3150 1
a3150 15
 *		Clear the reference bits on the specified physical page.
 *
 *	Parameters:
 *		pg		vm_page
 *
 *	Calls:
 *		pa_to_pvh
 *		pmap_pte
 *
 *	Extern/Global:
 *		pv_head_array, pv lists
 *
 * For each pmap/va the hardware 'used' bit in the page table entry
 * inspected - and turned off if necessary. If any of the inspected bits
 * were found on, a TLB flush will be performed.
d3156 2
a3157 10
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*pte;
	pmap_t		pmap;
	int		spl;
	vaddr_t		va;
	unsigned	users;
	boolean_t	kflush;
	paddr_t		phys;
	boolean_t	ret;
d3159 4
a3162 55
	phys = VM_PAGE_TO_PHYS(pg);

#ifdef DIAGNOSTIC
	if (!PMAP_MANAGED(phys))
		panic("pmap_clear_reference: not managed?");
#endif
	ret = pmap_is_referenced(pg);

	SPLVM(spl);

clear_reference_Retry:
	pvl = pa_to_pvh(phys);
	CHECK_PV_LIST(phys, pvl, "pmap_clear_reference");

	if (pvl->pmap == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_CREF | CD_NORM)) == (CD_CREF | CD_NORM))
			printf("(pmap_clear_reference :%x) phys addr 0x%x not mapped\n", curproc,phys);
#endif
		SPLX(spl);
		return (ret);
	}

	/* for each listed pmap, turn off the page refrenced bit */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->next) {
		pmap = pvep->pmap;
		va = pvep->va;
		if (!simple_lock_try(&pmap->lock)) {
			goto clear_reference_Retry;
		}
		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}

		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL)
			panic("pmap_clear_reference: bad pv list entry.");

		/*
		 * Invalidate pte temporarily to avoid the modified bit 
		 * and/or the reference being written back by any other cpu.
		 */
		/* clear reference bit */
		*pte = invalidate_pte(pte) & ~PG_U;
		flush_atc_entry(users, va, kflush);

		simple_unlock(&pmap->lock);
	}
	SPLX(spl);

	return (ret);
} /* pmap_clear_reference() */
d3169 1
a3169 16
 *	any physical maps. That is, whether the hardware has touched the page.
 *
 * Parameters:
 *	pg		vm_page
 *
 * Extern/Global:
 *	pv_head_array, pv lists
 *
 * Calls:
 *	pa_to_pvh
 *	pmap_pte
 *
 *	This routine walks the PV list corresponding to the
 * given page. For each pmap/va/ pair, the page descripter table entry is
 * examined. If a used bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list).
a3174 4
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*ptep;
	int		spl;
d3177 2
a3178 35
#ifdef DIAGNOSTIC
	if (!PMAP_MANAGED(phys))
		panic("pmap_is_referenced: not managed?");
#endif

	SPLVM(spl);

	pvl = pa_to_pvh(phys);
	CHECK_PV_LIST(phys, pvl, "pmap_is_referenced");

is_ref_Retry:
	if (pvl->pmap == PMAP_NULL) {
		SPLX(spl);
		return (FALSE);
	}

	/* for each listed pmap, check used bit for given page */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->next) {
		if (!simple_lock_try(&pvep->pmap->lock)) {
			goto is_ref_Retry;
		}
		ptep = pmap_pte(pvep->pmap, pvep->va);
		if (ptep == PT_ENTRY_NULL)
			panic("pmap_is_referenced: bad pv list entry.");
		if (pmap_pte_u(ptep)) {
			simple_unlock(&pvep->pmap->lock);
			SPLX(spl);
			return (TRUE);
		}
		simple_unlock(&pvep->pmap->lock);
	}

	SPLX(spl);
	return (FALSE);
} /* pmap_is referenced() */
d3184 1
a3184 1
 *	pmap_copy_on_write
d3199 2
a3200 1
		pmap_copy_on_write(phys);
a3209 303

#ifdef DEBUG
/*
 *  DEBUGGING ROUTINES - check_pv_list and check_pmp_consistency are used
 *		only for debugging.  They are invoked only
 *		through the macros CHECK_PV_LIST AND CHECK_PMAP_CONSISTENCY
 *		defined early in this sourcefile.
 */

/*
 *	Routine:	CHECK_PV_LIST (internal)
 *
 *	Function:
 *		Debug-mode routine to check consistency of a PV list. First, it
 *		makes sure every map thinks the physical page is the same. This
 *		should be called by all routines which touch a PV list.
 *
 *	Parameters:
 *		phys	physical address of page whose PV list is
 *			to be checked
 *		pv_h	pointer to head to the PV list
 *		who	string containing caller's name to be
 *			printed if a panic arises
 *
 *	Extern/Global:
 *		pv_head_array, pv lists
 *
 *	Calls:
 *		pmap_extract
 *
 *	Special Assumptions:
 *		No locking is required.
 *
 *	This function walks the given PV list. For each pmap/va pair,
 * pmap_extract is called to obtain the physical address of the page from
 * the pmap in question. If the returned physical address does not match
 * that for the PV list being perused, the function panics.
 */
void
check_pv_list(phys, pv_h, who)
	paddr_t phys;
	pv_entry_t pv_h;
	char *who;
{
	pv_entry_t	pv_e;
	pt_entry_t	*pte;
	paddr_t		pa;

	if (pv_h != pa_to_pvh(phys)) {
		printf("check_pv_list: incorrect pv_h supplied.\n");
		panic(who);
	}

	if (!PAGE_ALIGNED(phys)) {
		printf("check_pv_list: supplied phys addr not page aligned.\n");
		panic(who);
	}

	if (pv_h->pmap == PMAP_NULL) {
		if (pv_h->next != PV_ENTRY_NULL) {
			printf("check_pv_list: first entry has null pmap, but list non-empty.\n");
			panic(who);
		} else	return;	    /* proper empty list */
	}

	pv_e = pv_h;
	while (pv_e != PV_ENTRY_NULL) {
		if (!PAGE_ALIGNED(pv_e->va)) {
			printf("check_pv_list: non-aligned VA in entry at 0x%x.\n", pv_e);
			panic(who);
		}
		/*
		 * We can't call pmap_extract since it requires lock.
		 */
		if ((pte = pmap_pte(pv_e->pmap, pv_e->va)) == PT_ENTRY_NULL)
			pa = 0;
		else
			pa = ptoa(PG_PFNUM(*pte)) | (pv_e->va & PAGE_MASK);

		if (pa != phys) {
			printf("check_pv_list: phys addr diff in entry at 0x%x.\n", pv_e);
			panic(who);
		}

		pv_e = pv_e->next;
	}
} /* check_pv_list() */

/*
 *	Routine:	CHECK_MAP (internal)
 *
 *	Function:
 *		Debug mode routine to check consistency of map.
 *		Called by check_pmap_consistency only.
 *
 *	Parameters:
 *		map	pointer to pmap structure
 *		s	start of range to be checked
 *		e	end of range to be checked
 *		who	string containing caller's name to be
 *			printed if a panic arises
 *
 *	Extern/Global:
 *		pv_head_array, pv lists
 *
 *	Calls:
 *		pmap_pte
 *
 *	Special Assumptions:
 *		No locking required.
 *
 *	This function sequences through the given range of addresses. For
 * each page, pmap_pte is called to obtain the page table entry. If
 * its valid, and the physical page it maps is managed, the PV list is
 * searched for the corresponding pmap/va entry. If not found, or if
 * duplicate PV list entries are found, the function panics.
 */
void
check_map(map, s, e, who)
	pmap_t map;
	vaddr_t s, e;
	char *who;
{
	vaddr_t		va,
			old_va;
	paddr_t		phys;
	pv_entry_t	pv_h,
			pv_e,
			saved_pv_e;
	pt_entry_t	*ptep;
	boolean_t	found;
	int		loopcnt;
	int		bank;
	unsigned	npages;

	/*
	 * for each page in the address space, check to see if there's
	 * a valid mapping. If so makes sure it's listed in the PV_list.
	 */

	if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
		printf("(check_map) checking map at 0x%x\n", map);

	old_va = s;
	for (va = s; va < e; va += PAGE_SIZE) {
		/* check for overflow - happens if e=0xffffffff */
		if (va < old_va)
			break;
		else
			old_va = va;

		if (va == phys_map_vaddr1 || va == phys_map_vaddr2)
			/* don't try anything with these */
			continue;

		ptep = pmap_pte(map, va);

		if (ptep == PT_ENTRY_NULL) {
			/* no page table, skip to next segment entry */
			va = SDT_NEXT(va)-PAGE_SIZE;
			continue;
		}

		if (!PDT_VALID(ptep))
			continue;      /* no page mapping */

		phys = ptoa(PG_PFNUM(*ptep));  /* pick up phys addr */

		if (!PMAP_MANAGED(phys))
			continue;      /* no PV list */

		/* note: vm_page_startup allocates some memory for itself
		   through pmap_map before pmap_init is run. However,
		   it doesn't adjust the physical start of memory.
		   So, pmap thinks those pages are managed - but they're
		   not actually under it's control. So, the following
		   conditional is a hack to avoid those addresses
		   reserved by vm_page_startup */
		/* pmap_init also allocate some memory for itself. */

		for (npages = 0, bank = 0; bank < vm_nphysseg; bank++)
			npages += vm_physmem[bank].end - vm_physmem[bank].start;
		if (map == kernel_pmap &&
		    va < round_page((vaddr_t)(pmap_modify_list + npages)))
			continue;
		pv_h = pa_to_pvh(phys);
		found = FALSE;

		if (pv_h->pmap != PMAP_NULL) {
			loopcnt = 10000;  /* loop limit */
			pv_e = pv_h;
			while (pv_e != PV_ENTRY_NULL) {

				if (loopcnt-- < 0) {
					printf("check_map: loop in PV list at PVH 0x%x (for phys 0x%x)\n", pv_h, phys);
					panic(who);
				}

				if (pv_e->pmap == map && pv_e->va == va) {
					if (found) {
						printf("check_map: Duplicate PV list entries at 0x%x and 0x%x in PV list 0x%x.\n", saved_pv_e, pv_e, pv_h);
						printf("check_map: for pmap 0x%x, VA 0x%x,phys 0x%x.\n", map, va, phys);
						panic(who);
					} else {
						found = TRUE;
						saved_pv_e = pv_e;
					}
				}
				pv_e = pv_e->next;
			}
		}

		if (!found) {
			printf("check_map: Mapping for pmap 0x%x VA 0x%x Phys 0x%x does not appear in PV list 0x%x.\n", map, va, phys, pv_h);
		}
	}

	if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
		printf("(check_map) done \n");

} /* check_map() */

/*
 *	Routine:	CHECK_PMAP_CONSISTENCY (internal)
 *
 *	Function:
 *		Debug mode routine which walks all pmap, checking for internal
 *		consistency. We are called UNLOCKED, so we'll take the write
 *		lock.
 *
 *	Parameters:
 *		who		string containing caller's name tobe
 *				printed if a panic arises
 *
 *	Extern/Global:
 *		list of pmap structures
 *
 *	Calls:
 *		check map
 *		check pv_list
 *
 *	This function obtains the pmap write lock. Then, for each pmap
 * structure in the pmap struct queue, it calls check_map to verify the
 * consistency of its translation table hierarchy.
 *
 *	Once all pmaps have been checked, check_pv_list is called to check
 * consistency of the PV lists for each managed page.
 *
 *	There are some pages which do not appear in PV list. These pages
 * are allocated for pv structures by uvm_km_zalloc called in pmap_init.
 * Though they are in the range of pmap_phys_start to pmap_phys_end,
 * PV manipulations had not been activated when these pages were alloceted.
 *
 */
void
check_pmap_consistency(who)
	char *who;
{
	pmap_t		p;
	int		i;
	paddr_t		phys;
	pv_entry_t	pv_h;
	int		spl;
	int		bank;
	unsigned	npages;

	if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
		printf("check_pmap_consistency (%s :%x) start.\n", who, curproc);

	if (pv_head_table == PV_ENTRY_NULL) {
		printf("check_pmap_consistency (%s) PV head table not initialized.\n", who);
		return;
	}

	SPLVM(spl);

	p = kernel_pmap;
	check_map(p, VM_MIN_KERNEL_ADDRESS, VM_MAX_KERNEL_ADDRESS, who);

	/* run through all pmaps. check consistency of each one... */
	i = 512;
	for (p = kernel_pmap->next;p != kernel_pmap; p = p->next, i--) {
		if (i == 0) { /* can not read pmap list */
			printf("check_pmap_consistency: pmap struct loop error.\n");
			panic(who);
		}
		check_map(p, VM_MIN_ADDRESS, VM_MAX_ADDRESS, who);
	}

	/* run through all managed paes, check pv_list for each one */
	for (npages = 0, bank = 0; bank < vm_nphysseg; bank++) {
		for (phys = ptoa(vm_physmem[bank].start); phys < ptoa(vm_physmem[bank].end); phys += PAGE_SIZE) {
			pv_h = pa_to_pvh(phys);
			check_pv_list(phys, pv_h, who);
		}
	}

	SPLX(spl);

	if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
		printf("check_pmap consistency (%s :%x): done.\n",who, curproc);
} /* check_pmap_consistency() */
#endif /* DEBUG */
@


1.61
log
@Don't mess with the PMAP_PHYSSEG flags there. It's UVM playground, not really
ours.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2001/12/24 04:12:40 miod Exp $	*/
d1306 3
a1308 4
	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl", 0,
	    pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", 0,
	    NULL, NULL, M_VMPVENT);
@


1.60
log
@- completely change the mmu segment and page table structure definitions,
to use constant bitmasks instead of bitfields.
- remove unnecessary (as long as we are not running SMP) locks on the
physsegs.
- update the pmap code to take these changes into account, and gratuitously
change several names and code paths to be closer to existing m68k pmaps. It's
a bit faster now.
- change pmap.c's usage of vm_{offset,size}_t to {p,v}{addr,size}_t.
- remove dead or unused stuff from pmap.c, fix typos, etc

Tested on 187 and 188, should not make things worse for 197.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2001/12/24 00:25:17 miod Exp $	*/
a1793 5
				struct vm_page *pg;

				pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);
				pg->flags &= ~PG_CLEAN;

a1973 4
			struct vm_page *pg;

			pg = PHYS_TO_VM_PAGE(phys);
			pg->flags &= ~PG_CLEAN;
@


1.59
log
@Be more TLB-coherency friendly in pmap_k* functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2001/12/22 10:22:13 smurph Exp $	*/
d3 2
a67 1
#include <machine/pte.h>
d72 2
a73 2
extern vm_offset_t      avail_start, avail_end;
extern vm_offset_t      virtual_avail, virtual_end;
d101 1
a101 1
#define CD_FREE		0x0000400	/* pmap_free_tables */
d125 2
a126 2
caddr_t  vmmap;
pt_entry_t  *vmpte, *msgbufmap;
d128 1
a128 1
struct pmap	kernel_pmap_store;
d133 2
a134 2
   kpdt_entry_t   next;
   vm_offset_t phys;
d138 1
a138 1
kpdt_entry_t     kpdt_free;
d174 1
a174 1
vm_offset_t phys_map_vaddr1, phys_map_vaddr2;
d177 1
a177 1
 *	The Modify List
a186 2
struct simplelock *pv_lock_table; /* array */

d189 2
a190 2
#define	PMAP_MANAGED(pa) (pmap_initialized &&			\
			 vm_physseg_find(atop((pa)), NULL) != -1)
d192 1
a192 1
#define	PA_TO_PVH(pa)							\
d199 1
a199 22
#define	LOCK_PVH(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	simple_lock(&vm_physmem[bank_].pmseg.plock[pg_]);		\
})
#define	UNLOCK_PVH(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	simple_unlock(&vm_physmem[bank_].pmseg.plock[pg_]);		\
})
#define	PA_TO_ATTRIB(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	vm_physmem[bank_].pmseg.attrs[pg_];				\
})
#define	SET_ATTRIB(pa, attr)						\
d204 1
a204 1
	vm_physmem[bank_].pmseg.attrs[pg_] = (attr);			\
d208 1
a208 1
 *	Locking and TLB invalidation primitives
a211 38
 *	Locking Protocols:
 *
 *	There are two structures in the pmap module that need locking:
 *	the pmaps themselves, and the per-page pv_lists (which are locked
 *	by locking the pv_lock_table entry that corresponds to the pv_head
 *	for the list in question.)  Most routines want to lock a pmap and
 *	then do operations in it that require pv_list locking -- however
 *	pmap_remove_all and pmap_copy_on_write operate on a physical page
 *	basis and want to do the locking in the reverse order, i.e. lock
 *	a pv_list and then go through all the pmaps referenced by that list.
 *	To protect against deadlock between these two cases, the pmap_lock
 *	is used.  There are three different locking protocols as a result:
 *
 *  1.  pmap operations only (pmap_extract, pmap_access, ...)  Lock only
 *		the pmap.
 *
 *  2.  pmap-based operations (pmap_enter, pmap_remove, ...)  Get a read
 *		lock on the pmap_lock (shared read), then lock the pmap
 *		and finally the pv_lists as needed [i.e. pmap lock before
 *		pv_list lock.]
 *
 *  3.  pv_list-based operations (pmap_remove_all, pmap_copy_on_write, ...)
 *		Get a write lock on the pmap_lock (exclusive write); this
 *		also guaranteees exclusive access to the pv_lists.  Lock the
 *		pmaps as needed.
 *
 *	At no time may any routine hold more than one pmap lock or more than
 *	one pv_list lock.  Because interrupt level routines can allocate
 *	mbufs and cause pmap_enter's, the pmap_lock and the lock on the
 *	kernel_pmap can only be held at splvm.
 */
/* DCR: 12/18/91 - The above explanation is no longer true.  The pmap
 *	system lock has been removed in favor of a backoff strategy to
 *	avoid deadlock.  Now, pv_list-based operations first get the
 *	pv_list lock, then try to get the pmap lock, but if they can't,
 *	they release the pv_list lock and retry the whole operation.
 */
/*
d217 1
d229 1
a229 2
#define PV_LOCK_TABLE_SIZE(n)	((vm_size_t)((n) * sizeof(struct simplelock)))
#define PV_TABLE_SIZE(n)	((vm_size_t)((n) * sizeof(struct pv_entry)))
d245 1
a245 1
void check_pv_list __P((vm_offset_t, pv_entry_t, char *));
d247 1
d249 1
a249 1
   #define CHECK_PV_LIST(phys,pv_h,who) \
d251 1
a251 1
   #define CHECK_PMAP_CONSISTENCY(who) \
d254 2
a255 2
   #define CHECK_PV_LIST(phys,pv_h,who)
   #define CHECK_PMAP_CONSISTENCY(who)
d268 13
a280 14
vm_offset_t kmapva = 0;
extern vm_offset_t bugromva;
extern vm_offset_t sramva;
extern vm_offset_t obiova;

void flush_atc_entry __P((long, vm_offset_t, boolean_t));
unsigned int m88k_protection __P((pmap_t, vm_prot_t));
pt_entry_t *pmap_expand_kmap __P((vm_offset_t, vm_prot_t));
void pmap_free_tables __P((pmap_t));
void pmap_remove_range __P((pmap_t, vm_offset_t, vm_offset_t));
void pmap_copy_on_write __P((vm_offset_t));
void pmap_expand __P((pmap_t, vm_offset_t));
void cache_flush_loop __P((int, vm_offset_t, int));
void pmap_pinit __P((pmap_t));
d282 45
d340 1
a340 1
 *      kernel  nonzero if supervisor mode, zero if user mode
d343 4
a346 1
flush_atc_entry(long users, vm_offset_t va, boolean_t kernel)
d348 2
a349 2
	register int cpu;
	long tusers = users;
a365 28
 *	Convert machine-independent protection code to M88K protection bits.
 */

unsigned int
m88k_protection(pmap_t map, vm_prot_t prot)
{
	pte_template_t p;

	p.bits = 0;
	p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;
#ifdef M88110
	if (cputyp == CPU_88110) {
		p.pte.pg_used = 1;
		p.pte.modified = p.pte.prot ? 0 : 1;
		/* if the map is the kernel's map and since this 
		 * is not a paged kernel, we go ahead and mark 
		 * the page as modified to avoid an exception 
		 * upon writing to the page the first time.  XXX smurph 
		 */
		if (map == kernel_pmap) { 
			p.pte.modified = p.pte.prot ? 0 : 1;
		}
	}
#endif 
	return (p.bits);
} /* m88k_protection */

/*
a379 5
 *
 * Calls:
 *	SDTENT
 *	SDT_VALID
 *	PDT_IDX
a380 1

d382 3
a384 1
pmap_pte(pmap_t map, vm_offset_t virt)
d386 2
a387 1
	sdt_entry_t *sdt;
d401 3
a403 3
	else
		return ((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
			PDTIDX(virt));
a429 3
 *	SDTENT
 *	SDT_VALID
 *	PDT_IDX
a438 1

d440 3
a442 1
pmap_expand_kmap(vm_offset_t virt, vm_prot_t prot)
d444 2
a445 4
	int        aprot;
	sdt_entry_t      *sdt;
	kpdt_entry_t  kpdt_ent;
	pmap_t     map = kernel_pmap;
d451 1
a451 1
	aprot = m88k_protection (map, prot);
d454 1
a454 1
	sdt = SDTENT(map, virt);
d465 1
a465 1
	((sdt_entry_template_t *)sdt)->bits = kpdt_ent->phys | aprot | DT_VALID;
d467 1
a467 1
	((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = (vm_offset_t)kpdt_ent | aprot | DT_VALID;
d472 1
a472 1
}/* pmap_expand_kmap() */
a501 1
 *
d512 1
a512 2

vm_offset_t
d514 2
a515 1
	vm_offset_t virt, start, end;
a518 1
	int		aprot;
d521 2
a522 2
	pt_entry_t	*pte;
	pte_template_t	template;
d535 1
a535 5
	aprot = m88k_protection (kernel_pmap, prot);

	template.bits = trunc_page(start) | aprot | cmode | DT_VALID;

	npages = atop(round_page(end) - trunc_page(start));
d537 2
d546 1
a546 1
			if (pte->dtype)
d549 2
a550 1
		*pte = template.pte;
a551 1
		template.bits += PAGE_SIZE;
d553 1
a553 1
	return (virt);
a574 1
 *	BATC_BLK_ALIGNED
d598 1
a598 1
vm_offset_t
d600 2
a601 1
	vm_offset_t virt, start, end;
d605 1
a605 1
	int		aprot;
d607 2
a608 3
	vm_offset_t	phys;
	pt_entry_t	*pte;
	pte_template_t	template;
d610 1
a610 1
	register int	i;
d623 3
a625 3
	aprot = m88k_protection (kernel_pmap, prot);
	template.bits = trunc_page(start) | aprot | DT_VALID | cmode;
	phys = start;
d628 8
a635 4
	batctmp.field.wt = template.pte.wt;	 /* write through */
	batctmp.field.g = template.pte.g;     /* global */
	batctmp.field.ci = template.pte.ci;	 /* cache inhibit */
	batctmp.field.wp = template.pte.prot; /* protection */
d638 3
a640 4
	num_phys_pages = atop(round_page(end) - 
				   trunc_page(start));

	while (num_phys_pages > 0) {
d643 5
a647 2
			printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, aligne V=%d, phys=%x, aligne P=%d\n", curproc,
			       num_phys_pages, virt, BATC_BLK_ALIGNED(virt), phys, BATC_BLK_ALIGNED(phys));
d650 1
a650 1
		if ( BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(phys) && 
d657 1
a657 1
			batctmp.field.pba = M88K_BTOBLK(phys);
d672 2
a673 2
					if (pte->dtype)
						printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, ((pte_template_t *)pte)->bits);
d679 1
a679 2
			phys += BATC_BLKBYTES;
			template.pte.pfn = atop(phys);
d689 1
a689 1
			if (pte->dtype)
d693 1
a693 1
		*pte = template.pte;
d695 1
a695 3
		phys += PAGE_SIZE;
		template.bits += PAGE_SIZE;
		num_phys_pages--;
d697 1
a697 1
	return (round_page(virt));
d705 1
a705 6
 *	the specifid virutal address range.
 *
 * 	mode
 *		writethrough	0x200
 *		global		0x80
 *		cache inhibit	0x40
d709 2
a710 2
 *	vm_offset_t	s
 *	vm_offset_t	e
a713 2
 *	PMAP_LOCK
 *	PMAP_UNLOCK
a716 1
 *	dcachefall
d725 4
a728 1
pmap_cache_ctrl(pmap_t pmap, vm_offset_t s, vm_offset_t e, unsigned mode)
d732 1
a732 1
	vm_offset_t	va;
d735 1
a735 2
	register unsigned	users;
	register pte_template_t	opte;
d771 2
a772 2
		 * the modified bit and/or the reference bit by other cpu.
		 *  XXX
d774 1
a774 2
		opte.bits = invalidate_pte(pte);
		((pte_template_t *)pte)->bits = (opte.bits & CACHE_MASK) | mode;
d780 1
a780 1
		for (cpu=0; cpu<max_cpus; cpu++)
d782 1
a782 1
				cmmu_flush_remote_cache(cpu, ptoa(pte->pfn),
d814 1
a814 2
 *	pmap_map
 *	pmap_map_batc
d836 4
a839 5
pmap_bootstrap(vm_offset_t load_start,
	       vm_offset_t   *phys_start,
	       vm_offset_t   *phys_end,
	       vm_offset_t   *virt_start,
	       vm_offset_t   *virt_end)
d843 1
a843 1
	vm_offset_t	vaddr,
a844 3
			kpdt_phys,
			s_text,
			e_text,
d847 3
d972 2
a973 2
	for (i = pdt_size/PDT_SIZE; i > 0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vm_offset_t)kpdt_virt + PDT_SIZE);
d991 1
a991 1
   #define PMAPER	pmap_map
d993 1
a993 1
   #define PMAPER	pmap_map_batc
d997 1
a997 5
	vaddr = PMAPER(0,
		      0,
		      0x10000,
		       VM_PROT_WRITE | VM_PROT_READ,
		       CACHE_INH);
d1000 7
a1006 11
	vaddr = PMAPER((vm_offset_t)trunc_page(((unsigned)&kernelstart)),
		      s_text,
		      e_text,
		       VM_PROT_WRITE | VM_PROT_READ,
		       CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(vaddr,
		      e_text,
		      (vm_offset_t)kmap,
		       VM_PROT_WRITE | VM_PROT_READ,
		       CACHE_GLOBAL);
d1022 2
a1023 2
	vaddr = PMAPER(vaddr, (vm_offset_t)kmap, *phys_start,
		      VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1048 1
a1048 1
	if (brdtyp != BRD_188) { /*  != BRD_188 */
d1054 1
a1054 1
			      VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1113 3
a1115 4
			PMAPER(ptable->virt_start,
			       ptable->phys_start,
			       ptable->phys_start + (ptable->size - 1),
			       ptable->prot, ptable->cacheability);
d1117 11
a1127 9
		}
		/*
		 * Allocate all the submaps we need. Note that SYSMAP just allocates
		 * kernel virtual address with no physical backing memory. The idea
		 * is physical memory will be mapped at this va before using that va.
		 * This means that if different physical pages are going to be mapped
		 * at different times, we better do a tlb flush before using it -	         
		 * else we will be referencing the wrong page.
		 */
d1133 1
a1133 1
    		pmap_expand_kmap(virt, (VM_PROT_READ|VM_PROT_WRITE)|(CACHE_GLOBAL << 16)); \
d1140 1
a1140 2
	vmpte->pfn = -1;
	vmpte->dtype = DT_INVALID;
d1161 1
a1161 1
			pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
d1196 1
a1196 2
			pte->pfn = -1;
			pte->dtype = DT_INVALID;
d1198 1
a1198 2
			pte->dtype = DT_INVALID;
			pte->pfn = -1;
a1215 1

a1229 1
 *	pv_lock_table
d1239 1
a1239 1
 * for the pv_head_table, pv_lock_table, pmap_modify_list; and sets these
a1248 1
 *
a1250 1
 *
d1255 6
a1260 8
	register long		npages;
	register vm_offset_t	addr;
	register vm_size_t	s;
	register int		i;
	struct pv_entry		*pv;
	char			*attr;
	struct simplelock	*lock;
	int			bank;
d1267 1
a1267 1
	 * Allocate memory for the pv_head_table and its lock bits,
a1273 1
	s += PV_LOCK_TABLE_SIZE(npages);	/* pv_lock_table */
a1289 6
	/*
	 * Assume that 'simple_lock' is used to lock pv_lock_table
	 */
	pv_lock_table = (struct simplelock *)addr; /* XXX */
	addr += PV_LOCK_TABLE_SIZE(npages);

d1293 1
a1293 7
	* Initialize pv_lock_table
	*/
	for (i = 0; i < npages; i++)
		simple_lock_init(&(pv_lock_table[i]));

	/*
	 * Now that the pv, attribute, and lock tables have been allocated,
a1296 1
	lock = pv_lock_table;
a1301 1
		vm_physmem[bank].pmseg.plock = lock;
a1302 1
		lock += npages;
a1327 2
 *	cmmu_sflush_page
 *	bzero
d1337 2
a1338 1
pmap_zero_page(vm_offset_t phys)
d1340 2
a1341 3
	vm_offset_t	srcva;
	pte_template_t	template;
	unsigned int	spl;
d1346 1
a1346 1
	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu * PAGE_SIZE));
a1348 4
	template.bits = trunc_page(phys)
			| m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
			| DT_VALID | CACHE_GLOBAL;

d1351 3
a1353 1
	*srcpte = template.pte;
d1355 1
a1369 3
 * Parameters:
 *	size		size of the map
 *
d1375 5
a1379 1
	struct pmap *p;
a1384 2
	pmap_pinit(p);
	return (p);
a1385 10
} /* pmap_create() */

void
pmap_pinit(pmap_t p)
{
	pmap_statistics_t stats;
	sdt_entry_t *segdt;
	int i;
	unsigned int s;
	
d1424 2
a1425 3
				(vm_offset_t)segdt,
				(vm_offset_t)segdt+ (SDT_SIZE*2),
				CACHE_INH);
d1466 2
a1467 1
} /* pmap_pinit() */
d1470 1
a1470 1
 * Routine:	PMAP_FREE_TABLES (internal)
d1480 1
a1480 1
 *	kmem_free
d1494 2
a1495 1
pmap_free_tables(pmap_t pmap)
d1504 1
a1504 1
		printf("(pmap_free_tables :%x) pmap %x\n", curproc, pmap);
d1521 1
a1521 1
		if ((gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va)) != PT_ENTRY_NULL) {
d1524 1
a1524 1
				printf("(pmap_free_tables :%x) free page table = 0x%x\n",
d1533 1
a1533 1
		printf("(pmap_free_tables :%x) free segment table = 0x%x\n",
d1539 1
a1539 2
	uvm_km_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
} /* pmap_free_tables() */
a1540 4
void
pmap_release(register pmap_t p)
{
	pmap_free_tables(p);
d1542 2
a1543 2
	if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
		printf("(pmap_destroy :%x) ref_count = 0\n", curproc);
d1545 2
a1546 2
	p->prev->next = p->next;
	p->next->prev = p->prev;
d1548 1
a1548 1
}
d1561 2
a1562 4
 *	CHECK_PMAP_CONSISTENCY
 *	PMAP_LOCK, PMAP_UNLOCK
 *	pmap_free_tables
 *	free
d1568 2
a1569 2
 * structure. If it goes to zero, pmap_free_tables is called to release
 * the memory space to the system. Then, call kmem_free to free the
d1573 2
a1574 1
pmap_destroy(pmap_t p)
d1576 1
a1576 1
	register int  c, s;
a1600 1

d1620 2
a1621 1
pmap_reference(pmap_t p)
d1623 1
a1623 1
	int     s;
a1650 4
 *	CHECK_PAGE_ALIGN
 *	SDTENT
 *	SDT_VALID
 *	SDT_NEXT
d1652 1
a1652 7
 *	PDT_VALID
 *	PMAP_MANAGED
 *	LOCK_PVH
 *	UNLOCK_PVH
 *	PA_TO_PVH
 *	CHECK_PV_LIST
 *	free
a1654 1
 *	PHYS_TO_VM_PAGE
a1671 1

d1673 3
a1675 1
pmap_remove_range(pmap_t pmap, vm_offset_t s, vm_offset_t e)
a1676 1
	int		pfn;
d1680 2
a1681 1
	vm_offset_t	pa, va;
d1683 1
a1683 1
	pte_template_t	opte;
a1704 1

d1728 1
a1728 1
		if (pte->wired)
d1731 1
a1731 2
		pfn = pte->pfn;
		pa = ptoa(pfn);
a1733 1
			LOCK_PVH(pa);
d1738 1
a1738 1
			pvl = PA_TO_PVH(pa);
a1746 1

a1777 2
			UNLOCK_PVH(pa);

d1787 1
a1787 1
		 * the modified bit and/or the reference bit by other cpu.
d1789 1
a1789 1
		opte.bits = invalidate_pte(pte);
d1792 1
a1792 1
		if (opte.pte.modified) {
d1796 1
a1796 1
				pg = PHYS_TO_VM_PAGE(opte.bits & ~PAGE_MASK);
d1800 1
a1800 1
				SET_ATTRIB(pa, 1);
d1815 3
a1817 1
 *	pmap		pointer to pmap structure
a1822 2
 *	CHECK_PAGE_ALIGN
 *	PMAP_LOCK, PMAP_UNLOCK
a1823 1
 *	panic
d1829 3
a1831 1
pmap_remove(pmap_t map, vm_offset_t s, vm_offset_t e)
d1833 1
a1833 1
	int     spl;
a1870 4
 *	PMAP_MANAGED
 *	SPLVM, SPLX
 *	PA_TO_PVH
 *	CHECK_PV_LIST
a1871 1
 *	PDT_VALID
d1873 1
a1873 2
 *	PHYS_TO_VM_PAGE
 *	free
d1889 2
a1890 1
pmap_remove_all(vm_offset_t phys)
d1894 1
a1894 1
	vm_offset_t	va;
d1898 1
a1898 1
	pte_template_t	opte;
d1912 1
d1922 1
a1922 1
	pvl = PA_TO_PVH(phys);
a1923 1
	LOCK_PVH(phys);
a1930 1
			UNLOCK_PVH(phys);
d1955 1
a1955 1
		if (ptoa(pte->pfn) != phys)
d1957 1
a1957 1
		if (pte->wired)
d1963 1
a1963 1
			*pvl  = *cur;
d1973 1
a1973 1
		 * by other cpu.
d1975 1
a1975 1
		opte.bits = invalidate_pte(pte);
d1978 1
a1978 1
		if (opte.pte.modified) {
d1984 1
a1984 1
			SET_ATTRIB(phys, 1);
a1998 1
	UNLOCK_PVH(phys);
d2012 3
a2014 7
 *		SPLVM, SPLX
 *		PA_TO_PVH
 *		CHECK_PV_LIST
 *		simple_lock, simple_unlock
 *		panic
 *		PDT_VALID
 *		pmap_pte
d2024 2
a2025 1
pmap_copy_on_write(vm_offset_t phys)
a2030 1
	pte_template_t	opte;
d2044 1
a2044 1
	pv_e = PA_TO_PVH(phys);
a2045 1
	LOCK_PVH(phys);
d2052 2
a2053 1
		goto out;	/* no mappings */
d2062 2
a2063 5
		pmap_t      pmap;
		vm_offset_t va;

		pmap = pv_e->pmap;
		va = pv_e->va;
a2065 1
			UNLOCK_PVH(phys);
d2084 1
a2084 1
		if (ptoa(pte->pfn) != phys)
d2092 1
a2092 1
		 * by other cpu.
d2094 1
a2094 3
		opte.bits = invalidate_pte(pte);
		opte.pte.prot = M88K_RO;
		((pte_template_t *)pte)->bits = opte.bits;
d2100 1
a2100 1
	CHECK_PV_LIST(phys, PA_TO_PVH(phys), "pmap_copy_on_write");
a2101 2
out:
	UNLOCK_PVH(phys);
d2132 4
a2135 1
pmap_protect(pmap_t pmap, vm_offset_t s, vm_offset_t e, vm_prot_t prot)
a2136 2
	pte_template_t	maprot;
	unsigned	ap;
d2138 2
a2139 2
	pt_entry_t	*pte;
	vm_offset_t	va;
a2140 1
	pte_template_t	opte;
d2155 1
a2155 2
	maprot.bits = m88k_protection(pmap, prot);
	ap = maprot.pte.prot;
d2195 1
a2195 3
#if 0
		printf("(pmap_protect :%x) pte good\n", curproc);
#endif
d2199 1
a2199 1
		 * written back by other cpu.
d2201 1
a2201 3
		opte.bits = invalidate_pte(pte);
		opte.pte.prot = ap;
		((pte_template_t *)pte)->bits = opte.bits;
d2231 2
a2232 4
 *	kmem_alloc
 *	kmem_free
 *	zalloc
 *	free
d2252 3
a2254 1
pmap_expand(pmap_t map, vm_offset_t v)
d2257 2
a2258 1
	vm_offset_t	pdt_vaddr, pdt_paddr;
d2281 1
a2281 1
		 * the page for page tables should be CACHE DISABLED on MVME188
d2300 1
a2300 1
			printf("(pmap_expand :%x) table has already allocated\n", curproc);
d2321 2
a2322 2
		((sdt_entry_template_t *)sdt)->bits = pdt_paddr | M88K_RW | DT_VALID;
		((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = pdt_vaddr | M88K_RW | DT_VALID;
d2359 1
a2359 1
 *	free
d2398 6
a2403 3
pmap_enter(pmap_t pmap, vm_offset_t va, vm_offset_t pa,
	   vm_prot_t prot,
	   int flags)
a2404 1
	int		ap;
d2407 3
a2409 3
	pt_entry_t	*pte;
	vm_offset_t	old_pa;
	pte_template_t	template;
a2411 1
	pte_template_t	opte;
d2466 1
a2466 1
	old_pa = ptoa(pte->pfn);
d2478 1
a2478 1
		if (wired && !pte->wired)
d2480 1
a2480 1
		else if (!wired && pte->wired)
d2484 1
a2484 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_INH;
d2486 1
a2486 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_GLOBAL;
d2488 1
a2488 1
			template.pte.wired = 1;
d2493 2
a2494 2
		if ( !PDT_VALID(pte) || (pte->wired != template.pte.wired)
		     || (pte->prot != template.pte.prot)) {
d2497 3
a2499 2
			 * Invalidate pte temporarily to avoid being written back
			 * the modified bit and/or the reference bit by other cpu.
d2501 2
a2502 3
			opte.bits = invalidate_pte(pte);
			template.pte.modified = opte.pte.modified;
			*pte++ = template.pte;
d2510 1
a2510 1
		if (old_pa != (vm_offset_t)-1) {
d2522 1
a2522 1
					       pte, pte->pfn, pte->dtype);
d2545 1
a2545 2
			LOCK_PVH(pa);
			pvl = PA_TO_PVH(pa);
a2573 1
					UNLOCK_PVH(pa);
d2582 1
a2582 1
				 * Remeber that we used the pvlist entry.
a2585 1
			UNLOCK_PVH(pa);
d2596 1
a2596 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_INH;
d2598 3
a2600 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_GLOBAL;
d2602 4
a2605 2
		if (wired)
			template.pte.wired = 1;
d2607 1
a2607 1
		*(int *)pte = template.bits;
a2627 3
 *	Extern/Global:
 *		pte_per_vm_page
 *
a2628 1
 *		PMAP_LOCK, PMAP_UNLOCK
a2629 1
 *		panic
d2635 3
a2637 1
pmap_unwire(pmap_t map, vm_offset_t v)
d2647 1
a2647 1
	if (pte->wired) {
d2650 1
a2650 1
		pte->wired = 0;
a2653 1

d2683 4
a2686 1
pmap_extract(pmap_t pmap, vm_offset_t va, paddr_t *pap)
d2688 5
a2692 4
	pt_entry_t  *pte;
	paddr_t pa;
	int   i;
	int         spl;
d2703 1
a2703 1
		for (i = batc_used-1; i > 0; i--)
d2705 3
a2707 2
				*pap = (batc_entry[i].pba << BATC_BLKSHIFT) | 
					(va & BATC_BLKMASK );
d2713 9
a2721 7
	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		goto fail;
	} else {
		if (PDT_VALID(pte))
			pa = ptoa(pte->pfn);
		else
			goto fail;
a2723 3
	pa |= (va & PAGE_MASK); /* offset within page */
	*pap = pa;

d2725 1
a2725 4
	return (TRUE);
fail:
	PMAP_UNLOCK(pmap, spl);
	return (FALSE);
d2749 4
a2752 2
pmap_copy(pmap_t dst_pmap, pmap_t src_pmap, vm_offset_t dst_addr,
	  vm_size_t len, vm_offset_t src_addr)
a2753 1

a2768 3
 *	CHECK_PMAP_CONSISTENCY
 *	panic
 *	PMAP_LOCK, PMAP_UNLOCK
d2793 2
a2794 1
pmap_collect(pmap_t pmap)
d2796 10
a2805 10

	vm_offset_t sdt_va;	/* outer loop index */
	vm_offset_t sdt_vt;	/* end of segment */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	sdt_entry_t *sdt;	/* ptr to index into segment table */
	pt_entry_t  *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t  *gdttblend;	/* ptr to byte after last entry in table group */
	pt_entry_t  *gdtp;	/* ptr to index into a page table */
	boolean_t   found_gdt_wired;	/* flag indicating a wired page exists 
d2807 2
a2808 2
	int     spl;
	unsigned int i,j;
a2834 1

d2853 2
a2854 2
		for (gdtp=gdttbl; gdtp <gdttblend; gdtp++) {
			if (gdtp->wired) {
d2876 2
a2877 2
			((sdt_entry_template_t *) sdt) -> bits = 0;
			((sdt_entry_template_t *)(sdt+SDT_ENTRIES))->bits = 0;
d2925 2
a2926 1
pmap_activate(struct proc *p)
d2928 1
a2928 1
	apr_template_t   apr_data;
d2931 1
a2931 1
	int     n;
d2934 2
a2935 2
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	int cpu = cpu_number();  
d2955 1
a2955 1
	#ifdef OMRON_PMAP
d2966 1
a2966 1
	#else
d2969 2
a2970 2
	#endif
#endif /* notyet */
d2977 1
d2980 1
a2980 1
		 *	Mark that this cpu is using the pmap.
a2985 1

d3012 2
a3013 1
pmap_deactivate(struct proc *p)
d3015 2
a3016 2
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	int cpu = cpu_number();  
a3018 1

a3043 2
 *		cmmu_sflush_page
 *		bcopy
d3053 2
a3054 1
pmap_copy_page(vm_offset_t src, vm_offset_t dst)
d3056 7
a3062 6
	vm_offset_t dstva, srcva;
	unsigned int spl;
	int      aprot;
	pte_template_t template;
	pt_entry_t  *dstpte, *srcpte;
	int      cpu = cpu_number();
d3067 2
a3068 4
	aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);

	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu << PAGE_SHIFT));
a3072 4
	template.bits = trunc_page(src) | aprot | 
		DT_VALID | CACHE_GLOBAL;

	/* do we need to write back dirty bits */
d3075 1
a3075 1
	*srcpte = template.pte;
a3079 2
	template.bits = trunc_page(dst) | aprot | 
		CACHE_GLOBAL | DT_VALID;
d3081 1
a3081 1
	*dstpte  = template.pte;
a3087 1

d3104 1
a3104 5
 *		PMAP_MANAGED
 *		SPLVM, SPLX
 *		PA_TO_PVH
 *		CHECK_PV_LIST
 *		simple_lock, simple_unlock
a3105 1
 *		panic
d3114 2
a3115 1
pmap_clear_modify(struct vm_page *pg)
d3122 1
a3122 1
	vm_offset_t	va;
a3123 1
	pte_template_t	opte;
d3138 1
a3138 1
	pvl = PA_TO_PVH(phys);
a3139 1
	LOCK_PVH(phys);
d3142 1
a3142 1
	SET_ATTRIB(phys, 0);
a3148 1
		UNLOCK_PVH(phys);
a3157 1
			UNLOCK_PVH(phys);
d3173 1
a3173 1
		 * and/or the reference being written back by other cpu.
a3174 1
		opte.bits = invalidate_pte(pte);
d3176 1
a3176 2
		opte.pte.modified = 0;
		((pte_template_t *)pte)->bits = opte.bits;
a3180 1
	UNLOCK_PVH(phys);
d3205 1
a3205 1
 *		PA_TO_PVH
d3219 2
a3220 1
pmap_is_modified(struct vm_page *pg)
d3222 6
a3227 6
	pv_entry_t  pvl;
	pv_entry_t  pvep;
	pt_entry_t  *ptep;
	int      spl;
	boolean_t   modified_flag;
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d3236 1
a3236 1
	pvl = PA_TO_PVH(phys);
d3240 1
a3240 1
	if ((boolean_t)PA_TO_ATTRIB(phys)) {
a3249 1
	LOCK_PVH(phys);
d3254 1
a3254 1
		modified_flag = (boolean_t) PA_TO_ATTRIB(phys);
a3258 1
		UNLOCK_PVH(phys);
a3266 1
			UNLOCK_PVH(phys);
d3275 1
a3275 1
		if (ptep->modified) {
a3280 1
			UNLOCK_PVH(phys);
a3287 1
	UNLOCK_PVH(phys);
a3289 1

d3302 1
a3302 5
 *		PMAP_MANAGED
 *		SPLVM, SPLX
 *		PA_TO_PVH
 *		CHECK_PV_LIST
 *		simple_lock
a3303 1
 *		panic
d3313 2
a3314 1
pmap_clear_reference(struct vm_page *pg)
d3321 1
a3321 1
	vm_offset_t	va;
a3322 1
	pte_template_t	opte;
d3337 2
a3338 3
	clear_reference_Retry:
	LOCK_PVH(phys);
	pvl = PA_TO_PVH(phys);
a3340 1

a3345 1
		UNLOCK_PVH(phys);
d3351 1
a3351 2
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
a3354 1
			UNLOCK_PVH(phys);
d3370 1
a3370 1
		 * and/or the reference being written back by other cpu.
a3371 1
		opte.bits = invalidate_pte(pte);
d3373 1
a3373 2
		opte.pte.pg_used = 0;
		((pte_template_t *)pte)->bits = opte.bits;
a3376 1
		pvep = pvep->next;
a3377 1
	UNLOCK_PVH(phys);
d3397 1
a3397 5
 *	PMAP_MANAGED
 *	SPLVM
 *	PA_TO_PVH
 *	CHECK_PV_LIST
 *	simple_lock
a3404 1

d3406 2
a3407 1
pmap_is_referenced(struct vm_page *pg)
d3409 5
a3413 5
	pv_entry_t pvl;
	pv_entry_t pvep;
	pt_entry_t *ptep;
	int     spl;
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d3422 1
a3422 1
	pvl = PA_TO_PVH(phys);
a3430 2
	LOCK_PVH(phys);

d3432 1
a3432 2
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
a3433 1
			UNLOCK_PVH(phys);
d3439 1
a3439 1
		if (ptep->pg_used) {
a3440 1
			UNLOCK_PVH(phys);
a3444 1
		pvep = pvep->next;
a3446 1
	UNLOCK_PVH(phys);
d3461 3
a3463 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
a3480 155
#ifdef FUTURE_MAYBE
/*
 * Routine:	PAGEMOVE
 *
 * Function:
 *	Move pages from one kernel virtual address to another.
 *
 * Parameters:
 *	from	kernel virtual address of source
 *	to	kernel virtual address of destination
 *	size	size in bytes
 *
 * Calls:
 *	PMAP_LOCK
 *	PMAP_UNLOCK
 *	LOCK_PVH
 *	UNLOCK_PVH
 *	CHECK_PV_LIST
 *	pmap_pte
 *	pmap_expand_kmap
 *	cmmu_sflush
 *
 * Special Assumptions:
 *	size must be a multiple of CLBYTES (?)
 */
void
pagemove(vm_offset_t from, vm_offset_t to, int size)
{
	vm_offset_t	pa;
	pt_entry_t	*srcpte, *dstpte;
	pv_entry_t	pvl;
	int		spl;
	unsigned	users;
	pte_template_t	opte;

	PMAP_LOCK(kernel_pmap, spl);

	users = kernel_pmap->cpus_using;

	while (size > 0) {
		/*
		 * check if the source addr is mapped
		 */
		if ((srcpte = pmap_pte(kernel_pmap, from)) == PT_ENTRY_NULL) {
			panic("pagemove: source addr 0x%x not mapped", from);
		}

		/*
		 *
		 */
		if ((dstpte = pmap_pte(kernel_pmap, to)) == PT_ENTRY_NULL)
			if ((dstpte = pmap_expand_kmap(to,
			    VM_PROT_READ | VM_PROT_WRITE)) == PT_ENTRY_NULL)
				panic("pagemove: Cannot allocate destination pte");
		/*
		 *
		 */
		if (dstpte->dtype == DT_VALID) {
			panic("pagemove: destination pte 0x%x (vaddr 0x%x)"
			    " already valid", *((unsigned *)dstpte), to);
		}

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_PGMV | CD_NORM)) == (CD_PGMV | CD_NORM))
			printf("(pagemove :%x) from 0x%x to 0x%x\n", curproc, from, to);
		if ((pmap_con_dbg & (CD_PGMV | CD_FULL)) == (CD_PGMV | CD_FULL))
			printf("(pagemove :%x) srcpte @@ 0x%x = %x dstpte @@ 0x%x = %x\n", curproc, (unsigned)srcpte, *(unsigned *)srcpte, (unsigned)dstpte, *(unsigned *)dstpte);

#endif /* DEBUG */

		/*
		 * Update pv_list
		 */
		pa = ptoa(srcpte->pfn);
		if (PMAP_MANAGED(pa)) {
			LOCK_PVH(pa);
			pvl = PA_TO_PVH(pa);
			CHECK_PV_LIST(pa, pvl, "pagemove");
			pvl->va = to;
			UNLOCK_PVH(pa);
		}

		/*
		 * copy pte
		 *
		 * Invalidate pte temporarily to avoid the modified bit 
		 * and/or the reference being written back by other cpu.
		 */
		opte.bits = invalidate_pte(srcpte);
		flush_atc_entry(users, from, TRUE);
		((pte_template_t *)dstpte)->bits = opte.bits;
		from += PAGE_SIZE;
		to += PAGE_SIZE;

		size -= PAGE_SIZE;
	}
	PMAP_UNLOCK(kernel_pmap, spl);
} /* pagemove */

#endif /* FUTURE_MAYBE */

void
cache_flush_loop(int mode, vm_offset_t pa, int size)
{
	int	i;
	int	ncpus;
	void	(*cfunc)(int cpu, vm_offset_t physaddr, int size);

	switch (mode) {
	default:
		panic("bad cache_flush_loop mode");
		return;

	case FLUSH_CACHE:   /* All caches, all CPUs */
		ncpus = max_cpus;
		cfunc = cmmu->cmmu_flush_remote_cache_func;
		break;

	case FLUSH_CODE_CACHE: /* Instruction caches, all CPUs */
		ncpus = max_cpus;
		cfunc = cmmu->cmmu_flush_remote_inst_cache_func;
		break;

	case FLUSH_DATA_CACHE: /* Data caches, all CPUs */
		ncpus = max_cpus;
		cfunc = cmmu->cmmu_flush_remote_data_cache_func;
		break;

	case FLUSH_LOCAL_CACHE:	     /* Both caches, my CPU */
		ncpus = 1;
		cfunc = cmmu->cmmu_flush_remote_cache_func;
		break;

	case FLUSH_LOCAL_CODE_CACHE: /* Instruction cache, my CPU */
		ncpus = 1;
		cfunc = cmmu->cmmu_flush_remote_inst_cache_func;
		break;

	case FLUSH_LOCAL_DATA_CACHE: /* Data cache, my CPU */
		ncpus = 1;
		cfunc = cmmu->cmmu_flush_remote_data_cache_func;
		break;
	}

	if (ncpus == 1) {
		(*cfunc)(cpu_number(), pa, size);
	} else {
		for (i=0; i<max_cpus; i++) {
			if (cpu_sets[i]) {
				(*cfunc)(i, pa, size);
			}
		}
	}
}

a3517 1

d3519 4
a3522 1
check_pv_list(vm_offset_t phys, pv_entry_t pv_h, char *who)
d3524 3
a3526 3
	pv_entry_t  pv_e;
	pt_entry_t  *pte;
	vm_offset_t pa;
d3528 1
a3528 1
	if (pv_h != PA_TO_PVH(phys)) {
d3542 1
a3542 1
		} else	return;	    /* proper empry lst */
d3557 1
a3557 1
			pa = ptoa(pte->pfn) | (pv_e->va & PAGE_MASK);
a3565 1

a3596 1

d3598 4
a3601 1
check_map(pmap_t map, vm_offset_t s, vm_offset_t e, char *who)
d3603 11
a3613 11
	vm_offset_t va,
	old_va,
	phys;
	pv_entry_t  pv_h,
	pv_e,
	saved_pv_e;
	pt_entry_t  *ptep;
	boolean_t   found;
	int      loopcnt;
	int bank;
	unsigned      npages;
d3646 1
a3646 1
		phys = ptoa(ptep->pfn);  /* pick up phys addr */
d3663 1
a3663 1
		    va < round_page((vm_offset_t)(pmap_modify_list + npages)))
d3665 1
a3665 1
		pv_h = PA_TO_PVH(phys);
a3668 1

a3733 1

d3735 2
a3736 1
check_pmap_consistency(char *who)
d3738 7
a3744 7
	pmap_t      p;
	int      i;
	vm_offset_t phys;
	pv_entry_t  pv_h;
	int      spl;
	int bank;
	unsigned      npages;
a3749 1

d3772 1
a3772 1
			pv_h = PA_TO_PVH(phys);
a3780 1

a3783 24
/*
 * PMAP PRINT MACROS AND ROUTINES FOR DEBUGGING
 * These routines are called only from the debugger.
 */

#define	PRINT_SDT(p)							\
		printf("%08x : ",					\
			((sdt_entry_template_t *)p)-> bits);		\
		printf("table adress=0x%x, prot=%d, dtype=%d\n",	\
			ptoa(p->table_addr),			        \
			p->prot,					\
			p->dtype);

#define	PRINT_PDT(p)							\
		printf("%08x : ",					\
			((pte_template_t *)p)-> bits);			\
		printf("frame num=0x%x, prot=%d, dtype=%d, wired=%d, modified=%d, pg_used=%d\n",	\
			p->pfn,						\
			p->prot,					\
			p->dtype,					\
			p->wired,					\
			p->modified,					\
			p->pg_used);

d3785 2
a3786 1
pmap_virtual_space(vm_offset_t *startp, vm_offset_t *endp)
a3791 5
#ifdef USING_BATC
   #ifdef OMRON_PMAP
/*
 *      Set BATC
 */
d3793 4
a3796 73
pmap_set_batc(
	     pmap_t pmap,
	     boolean_t data,
	     int i,
	     vm_offset_t va,
	     vm_offset_t pa,
	     boolean_t super,
	     boolean_t wt,
	     boolean_t global,
	     boolean_t ci,
	     boolean_t wp,
	     boolean_t valid)
{
	register batc_template_t batctmp;

	if (i < 0 || i > (BATC_MAX - 1)) {
		panic("pmap_set_batc: illegal batc number");
		/* bad number */
		return;
	}

	batctmp.field.lba = va >> 19;
	batctmp.field.pba = pa >> 19;
	batctmp.field.sup = super;
	batctmp.field.wt = wt;
	batctmp.field.g = global;
	batctmp.field.ci = ci;
	batctmp.field.wp = wp;
	batctmp.field.v = valid;

	if (data) {
		pmap->d_batc[i].bits = batctmp.bits;
	} else {
		pmap->i_batc[i].bits = batctmp.bits;
	}
}

void 
use_batc(task_t task,
	 boolean_t data,	 /* for data-cmmu ? */
	 int i,			 /* batc number */
	 vm_offset_t va,	 /* virtual address */
	 vm_offset_t pa,	 /* physical address */
	 boolean_t s,		 /* for super-mode ? */
	 boolean_t wt,		 /* is writethrough */
	 boolean_t g,		 /* is global ? */
	 boolean_t ci,		 /* is cache inhibited ? */
	 boolean_t wp,		 /* is write-protected ? */
	 boolean_t v)		 /* is valid ? */
{
	pmap_t pmap;
	pmap = vm_map_pmap(task->map);
	pmap_set_batc(pmap, data, i, va, pa, s, wt, g, ci, wp, v);
}

   #endif
#endif /* USING_BATC */
#ifdef FUTURE_MAYBE
/*
 *	Machine-level page attributes
 *
 *	The only attribute that may be controlled right now is cacheability.
 *
 *	Obviously these attributes will be used in a sparse
 *	fashion, so we use a simple sorted list of address ranges
 *	which possess the attribute.
 */

/*
 *	Destroy an attribute list.
 */
void
pmap_destroy_ranges(pmap_range_t *ranges)
a3797 130
	pmap_range_t this, next;

	this = *ranges;
	while (this != 0) {
		next = this->next;
		pmap_range_free(this);
		this = next;
	}
	*ranges = 0;
}

/*
 *	Lookup an address in a sorted range list.
 */
boolean_t
pmap_range_lookup(pmap_range_t *ranges, vm_offset_t address)
{
	pmap_range_t range;

	for (range = *ranges; range != 0; range = range->next) {
		if (address < range->start)
			return FALSE;
		if (address < range->end)
			return TRUE;
	}
	return FALSE;
}

/*
 *	Add a range to a list.
 *	The pmap must be locked.
 */
void
pmap_range_add(pmap_range_t *ranges, vm_offset_t start, vm_offset_t end)
{
	pmap_range_t range, *prev;

	/* look for the start address */

	for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
		if (start < range->start)
			break;
		if (start <= range->end)
			goto start_overlaps;
	}

	/* start address is not present */

	if ((range == 0) || (end < range->start)) {
		/* no overlap; allocate a new range */

		range = pmap_range_alloc();
		range->start = start;
		range->end = end;
		range->next = *prev;
		*prev = range;
		return;
	}

	/* extend existing range forward to start */

	range->start = start;

start_overlaps:
	/* delete redundant ranges */

	while ((range->next != 0) && (range->next->start <= end)) {
		pmap_range_t old;

		old = range->next;
		range->next = old->next;
		range->end = old->end;
		pmap_range_free(old);
	}

	/* extend existing range backward to end */

	if (range->end < end)
		range->end = end;
}

/*
 *	Remove a range from a list.
 *	The pmap must be locked.
 */
void
pmap_range_remove(pmap_range_t *ranges, vm_offset_t start, vm_offset_t end)
{
	pmap_range_t range, *prev;

	/* look for start address */

	for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
		if (start <= range->start)
			break;
		if (start < range->end) {
			if (end < range->end) {
				pmap_range_t new;
				/* split this range */
				new = pmap_range_alloc();
				new->next = range->next;
				new->start = end;
				new->end = range->end;

				range->next = new;
				range->end = start;
				return;
			}
			/* truncate this range */
			range->end = start;
		}
	}

	/* start address is not in the middle of a range */

	while ((range != 0) && (range->end <= end)) {
		*prev = range->next;
		pmap_range_free(range);
		range = *prev;
	}

	if ((range != 0) && (range->start < end))
		range->start = end;
}
#endif /* FUTURE_MAYBE */

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	int		ap;
d3799 1
a3799 2
	pt_entry_t	*pte;
	pte_template_t	template;
d3814 1
a3814 1
	ap = m88k_protection(kernel_pmap, prot);
d3830 1
d3832 1
a3832 1
		template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_INH;
d3834 2
a3835 6
		template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_GLOBAL;

	template.pte.wired = 1;

	invalidate_pte(pte);
	*(int *)pte = template.bits;
d3842 3
a3844 1
pmap_kremove(vaddr_t va, vsize_t len)
@


1.58
log
@correct pmap_map.  mc88110 modifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2001/12/22 09:49:39 smurph Exp $	*/
d4417 1
d4419 1
d4444 1
d4463 2
@


1.57
log
@masive cmmu overhaul.  function pointers now control cmmu functionality
instead of case statements based on cpu type.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2001/12/22 07:35:43 smurph Exp $	*/
d533 1
a559 3
#ifdef MVME197
extern void m197_load_patc(int, vm_offset_t, vm_offset_t, int);
#endif
d562 4
a565 1
pmap_map(vm_offset_t virt, vm_offset_t start, vm_offset_t end, vm_prot_t prot)
a569 1
	unsigned	cmode;
a571 10
#ifdef MVME197
	static int m197_atc_initialized = FALSE;
#endif
	/*
	 * cache mode is passed in the top 16 bits.
	 * extract it from there. And clear the top
	 * 16 bits from prot.
	 */
	cmode = (prot & 0xffff0000) >> 16;
	prot &= 0x0000ffff;
a590 1

a600 11
#ifdef MVME197
		/* hack for MVME197 */
		if (brdtyp == BRD_197 && m197_atc_initialized == FALSE) {
			int i;

			for (i = 0; i < 32; i++)
				m197_load_patc(i, virt, 
					       (vm_offset_t)template.bits, 1);
			m197_atc_initialized = TRUE;
		}
#endif 
a603 1

a604 1

d651 4
a654 2
pmap_map_batc(vm_offset_t virt, vm_offset_t start, vm_offset_t end,
	      vm_prot_t prot, unsigned cmode)
a689 1

a745 1

a746 1

a823 1

a839 1

a840 1

a841 1

d906 2
a907 1
			kernel_pmap_size;
a912 1
	extern void	cmmu_go_virt(void);
d980 5
a984 2
	printf("kernel segment table from 0x%x to 0x%x\n", kernel_pmap->sdt_vaddr, 
	       kernel_pmap->sdt_vaddr + kernel_pmap_size);
d986 3
a988 4
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = (*phys_start + kernel_pmap_size);
	kpdt_virt = (kpdt_entry_t)(*virt_start + kernel_pmap_size);
	kernel_pmap_size += MAX_KERNEL_PDT_SIZE;
d991 15
d1007 2
a1008 2
	/* init all segment and page descriptor to zero */
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);
d1010 6
a1015 1
	printf("kernel page table to 0x%x\n", kernel_pmap->sdt_vaddr + kernel_pmap_size);
d1020 3
a1022 3
		printf("kpdt_phys = %x\n",kpdt_phys);
		printf("kpdt_virt = %x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x  ; (phys)0x%08x\n",
d1030 1
a1030 1
	for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i > 0; i--) {
d1055 1
a1055 2
	vaddr = PMAPER(
		      0,
d1058 2
a1059 1
		      (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_INH <<16));
d1062 1
a1062 2
	vaddr = PMAPER(
		      (vm_offset_t)trunc_page(((unsigned)&kernelstart)),
d1065 2
a1066 1
		      (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_GLOBAL<<16));  /* shouldn't it be RO? XXX*/
d1068 1
a1068 2
	vaddr = PMAPER(
		      vaddr,
d1071 2
a1072 1
		      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
d1088 2
a1089 5
	vaddr = PMAPER(
		      vaddr,
		      (vm_offset_t)kmap,
		      *phys_start,
		      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1092 5
d1099 1
a1099 1
			printf("1:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
d1119 2
a1120 5
		vaddr = PMAPER(
			      vaddr,
			      *phys_start,
			      *phys_start + etherlen,
			      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1174 1
a1174 1
	for (; ptable->size != 0xffffffffU; ptable++)
d1182 2
a1183 1
			       ptable->prot|(ptable->cacheability << 16));
a1184 1

a1228 1

a1254 5
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("After cmmu_flush_remote_tlb()\n");
			}
#endif
d1271 1
a1271 1
				printf("After cmmu_remote_set_sapr()\n");
a1503 1

a2261 1
 *		m88k_protection
d3027 1
a3027 1
			((sdt_entry_template_t *) sdt+SDT_ENTRIES) -> bits = 0;
@


1.56
log
@rename m882xx.h to m8820x.h
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2001/12/19 07:04:42 smurph Exp $	*/
a61 1
#include <machine/mmu.h>
a64 1
#include <machine/m8820x.h>		/* CMMU stuff */
a430 1

d511 1
a511 1

d513 1
d3809 1
a3809 1
		cfunc = cmmu_flush_remote_cache;
d3814 1
a3814 1
		cfunc = cmmu_flush_remote_inst_cache;
d3819 1
a3819 1
		cfunc = cmmu_flush_remote_data_cache;
d3824 1
a3824 1
		cfunc = cmmu_flush_remote_cache;
d3829 1
a3829 1
		cfunc = cmmu_flush_remote_inst_cache;
d3834 1
a3834 1
		cfunc = cmmu_flush_remote_data_cache;
@


1.55
log
@Introduce brdtyp and change what cputyp means.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2001/12/16 23:49:47 miod Exp $	*/
d66 1
a66 1
#include <machine/m882xx.h>		/* CMMU stuff */
@


1.55.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2002/01/23 00:39:47 art Exp $	*/
a2 2
 * Copyright (c) 2001 Miodrag Vallat
 * Copyright (c) 1998-2001 Steve Murphree, Jr.
d62 1
d66 1
d68 1
d73 2
a74 2
extern vaddr_t      avail_start, avail_end;
extern vaddr_t      virtual_avail, virtual_end;
d102 1
a102 1
#define CD_FREE		0x0000400	/* pmap_release */
d126 2
a127 2
caddr_t vmmap;
pt_entry_t *vmpte, *msgbufmap;
d129 1
a129 1
struct pmap kernel_pmap_store;
d134 2
a135 2
	kpdt_entry_t	next;
	paddr_t		phys;
d139 1
a139 1
kpdt_entry_t	kpdt_free;
d175 1
a175 1
vaddr_t phys_map_vaddr1, phys_map_vaddr2;
d178 1
a178 1
 * The Modify List
d188 2
d192 2
a193 2
#define	PMAP_MANAGED(pa) \
	(pmap_initialized && IS_VM_PHYSADDR(pa))
d195 1
a195 1
#define	pa_to_pvh(pa)							\
d202 22
a223 1
#define	pa_to_attribute(pa)						\
d228 1
a228 1
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
d232 1
a232 1
 *	Locking primitives
d236 38
a278 1

d290 2
a291 1
#define PV_TABLE_SIZE(n)	((vsize_t)((n) * sizeof(struct pv_entry)))
d307 1
a307 1
void check_pv_list __P((vaddr_t, pv_entry_t, char *));
a308 1
void check_map __P((pmap_t, vaddr_t, vaddr_t, char *));
d310 1
a310 1
#define CHECK_PV_LIST(phys,pv_h,who) \
d312 1
a312 1
#define CHECK_PMAP_CONSISTENCY(who) \
d315 2
a316 2
#define CHECK_PV_LIST(phys,pv_h,who)
#define CHECK_PMAP_CONSISTENCY(who)
d329 14
a342 13
vaddr_t kmapva = 0;
extern vaddr_t bugromva;
extern vaddr_t sramva;
extern vaddr_t obiova;

/*
 * Internal routines
 */
void flush_atc_entry __P((long, vaddr_t, boolean_t));
pt_entry_t *pmap_expand_kmap __P((vaddr_t, vm_prot_t));
void pmap_remove_range __P((pmap_t, vaddr_t, vaddr_t));
void pmap_copy_on_write __P((paddr_t));
void pmap_expand __P((pmap_t, vaddr_t));
a343 45
vaddr_t pmap_map __P((vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int));
vaddr_t pmap_map_batc __P((vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int));
pt_entry_t *pmap_pte __P((pmap_t, vaddr_t));
void pmap_remove_all __P((paddr_t));

/*
 * quick PTE field checking macros
 */
#define	pmap_pte_w(pte)		(*(pte) & PG_W)
#define	pmap_pte_m(pte)		(*(pte) & PG_M)
#define	pmap_pte_u(pte)		(*(pte) & PG_U)
#define	pmap_pte_prot(pte)	(*(pte) & PG_PROT)

#define	pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define	pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

/*
 * Convert machine-independent protection code to M88K protection bits.
 */
static __inline u_int32_t
m88k_protection(pmap_t map, vm_prot_t prot)
{
	pt_entry_t p;

	p = (prot & VM_PROT_WRITE) ? PG_RW : PG_RO;
	/*
	 * XXX this should not be necessary anymore now that pmap_enter
	 * does the correct thing... -- miod
	 */
#ifdef M88110
	if (cputyp == CPU_88110) {
		p |= PG_U;
		/* if the map is the kernel's map and since this 
		 * is not a paged kernel, we go ahead and mark 
		 * the page as modified to avoid an exception 
		 * upon writing to the page the first time.  XXX smurph 
		 */
		if (map == kernel_pmap) { 
			if (p & PG_PROT)
				p |= PG_M;
		}
	}
#endif 
	return (p);
} /* m88k_protection */
d357 1
a357 1
 *      kernel  TRUE if supervisor mode, FALSE if user mode
d360 1
a360 4
flush_atc_entry(users, va, kernel)
	long users;
	vaddr_t va;
	boolean_t kernel;
d362 2
a363 2
	int	cpu;
	long	tusers = users;
d380 28
d422 5
d428 1
d430 1
a430 3
pmap_pte(map, virt)
	pmap_t map;
	vaddr_t virt;
d432 1
a432 1
	sdt_entry_t	*sdt;
d447 3
a449 3

	return ((pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES))<<PDT_SHIFT) +
		PDTIDX(virt));
d476 3
d488 1
d490 1
a490 3
pmap_expand_kmap(virt, prot)
	vaddr_t virt;
	vm_prot_t prot;
d492 4
a495 2
	sdt_entry_t	template, *sdt;
	kpdt_entry_t	kpdt_ent;
d501 1
a501 1
	template = m88k_protection(kernel_pmap, prot) | SG_V;
d504 1
a504 1
	sdt = SDTENT(kernel_pmap, virt);
d514 3
a516 4
	/* physical table */
	*sdt = kpdt_ent->phys | template;
	/* virtual table */
	*(sdt + SDT_ENTRIES) = (vaddr_t)kpdt_ent | template;
d521 1
a521 1
} /* pmap_expand_kmap() */
a534 1
 *	cmode	cache control attributes
d550 1
d561 6
a566 6
vaddr_t
pmap_map(virt, start, end, prot, cmode)
	vaddr_t virt;
	paddr_t start, end;
	vm_prot_t prot;
	unsigned int cmode;
d568 1
d571 13
a583 2
	pt_entry_t	template, *pte;
	paddr_t		page;
d596 5
a600 1
	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;
a601 2
	page = trunc_page(start);
	npages = atop(round_page(end) - page);
d603 1
d610 1
a610 1
			if (PDT_VALID(pte))
d613 12
a624 2
		*pte = template | page;
		page += PAGE_SIZE;
d626 1
d628 3
a630 1
	return virt;
d652 1
d676 3
a678 6
vaddr_t
pmap_map_batc(virt, start, end, prot, cmode)
	vaddr_t virt;
	paddr_t start, end;
	vm_prot_t prot;
	unsigned int cmode;
d680 1
a680 1
	unsigned	npages;
d682 3
a684 2
	pt_entry_t	template, *pte;
	paddr_t		page;
d686 1
a686 1
	int		i;
d699 3
a701 3

	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;

d704 4
a707 8
	if (template & CACHE_WT)
		batctmp.field.wt = 1;	 /* write through */
	if (template & CACHE_GLOBAL)
		batctmp.field.g = 1;     /* global */
	if (template & CACHE_INH)
		batctmp.field.ci = 1;	 /* cache inhibit */
	if (template & PG_PROT)
		batctmp.field.wp = 1; /* protection */
d710 5
a714 3
	page = trunc_page(start);
	npages = atop(round_page(end) - page);
	for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {
d717 2
a718 5
			printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, "
			    "align V=%d, page=%x, align P=%d\n",
			    curproc, num_phys_pages, virt,
			    BATC_BLK_ALIGNED(virt), page,
			    BATC_BLK_ALIGNED(page));
d721 1
a721 1
		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) && 
d728 1
a728 1
			batctmp.field.pba = M88K_BTOBLK(page);
d743 2
a744 2
					if (PDT_VALID(pte))
						printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, *pte);
d750 2
a751 1
			page += BATC_BLKBYTES;
d761 1
a761 1
			if (PDT_VALID(pte))
d765 1
a765 1
		*pte = template | trunc_page(page);
d767 3
a769 1
		page += PAGE_SIZE;
d771 3
a773 1
	return virt;
d781 6
a786 1
 *	the specified virtual address range.
d790 2
a791 2
 *	vaddr_t		s
 *	vaddr_t		e
d795 2
d800 1
d809 1
a809 4
pmap_cache_ctrl(pmap, s, e, mode)
	pmap_t pmap;
	vaddr_t s, e;
	unsigned mode;
d813 1
a813 1
	vaddr_t		va;
d816 2
a817 1
	unsigned	users;
d851 1
d854 2
a855 2
		 * the modified bit and/or the reference bit by any other cpu.
		 * XXX
d857 2
a858 1
		*pte = (invalidate_pte(pte) & CACHE_MASK) | mode;
d864 1
a864 1
		for (cpu = 0; cpu < max_cpus; cpu++)
d866 1
a866 1
				cmmu_flush_remote_cache(cpu, ptoa(PG_PFNUM(*pte)),
d868 1
d870 1
d872 1
d901 2
a902 1
 *	pmap_map or pmap_map_batc
d924 5
a928 4
pmap_bootstrap(load_start, phys_start, phys_end, virt_start, virt_end)
	vaddr_t load_start;
	paddr_t *phys_start, *phys_end;
	vaddr_t *virt_start, *virt_end;
d932 1
a932 1
	vaddr_t		vaddr,
d934 2
a935 3
			kernel_pmap_size,
			pdt_size;
	paddr_t		s_text,
d937 1
a937 1
			kpdt_phys;
d943 1
d1011 2
a1012 5
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("     kernel segment start = 0x%x\n", kernel_pmap->sdt_paddr);
		printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
		printf("       kernel segment end = 0x%x\n", ((unsigned)kernel_pmap->sdt_paddr) + kernel_pmap_size);
	}
d1014 4
a1017 3
	/* init all segment descriptors to zero */
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);

a1019 15
	
	/* make sure page tables are page aligned!! XXX smurph */
	*phys_start = round_page(*phys_start);
	*virt_start = round_page(*virt_start);
	
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = *phys_start;
	kpdt_virt = (kpdt_entry_t)*virt_start;
	
	pdt_size = MAX_KERNEL_PDT_SIZE;
	/* might as well round up to a page - XXX smurph */
	pdt_size = round_page(pdt_size);
	kernel_pmap_size += pdt_size;
	*phys_start += pdt_size;
	*virt_start += pdt_size;
d1021 2
a1022 2
	/* init all page descriptors to zero */
	bzero((void *)kpdt_phys, pdt_size);
d1024 1
a1024 6
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("--------------------------------------\n");
		printf("        kernel page start = 0x%x\n", kpdt_phys);
		printf("   kernel page table size = 0x%x\n", pdt_size);
		printf("          kernel page end = 0x%x\n", *phys_start);
	}
d1029 3
a1031 3
		printf("kpdt_phys = 0x%x\n",kpdt_phys);
		printf("kpdt_virt = 0x%x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x, (phys)0x%08x\n",
d1039 2
a1040 2
	for (i = pdt_size / PDT_SIZE; i > 0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vaddr_t)kpdt_virt + PDT_SIZE);
d1058 1
a1058 1
#define PMAPER	pmap_map
d1060 1
a1060 1
#define PMAPER	pmap_map_batc
d1064 5
a1068 1
	vaddr = PMAPER(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1071 11
a1081 7
	vaddr = PMAPER((vaddr_t)trunc_page(((unsigned)&kernelstart)),
	    s_text, e_text, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(vaddr, e_text, (paddr_t)kmap,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);

d1097 5
a1101 2
	vaddr = PMAPER(vaddr, (paddr_t)kmap, *phys_start,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
a1103 5
		/* 
		 * This should never happen because we now round the PDT
		 * table size up to a page boundry in the quest to get 
		 * mc88110 working. - XXX smurph
		 */
d1106 1
a1106 1
			printf("1:vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
d1121 1
a1121 1
	if (brdtyp == BRD_187 || brdtyp == BRD_197) {
d1126 5
a1130 2
		vaddr = PMAPER(vaddr, *phys_start, *phys_start + etherlen,
		    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1184 1
a1184 1
	for (; ptable->size != 0xffffffffU; ptable++){
d1189 4
a1192 3
			PMAPER(ptable->virt_start, ptable->phys_start,
			    ptable->phys_start + (ptable->size - 1),
			    ptable->prot, ptable->cacheability);
a1193 2
	}
#undef PMAPER
d1195 8
a1202 8
	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physical pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -
	 * else we will be referencing the wrong page.
	 */
d1208 1
a1208 1
    		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
d1215 2
a1216 1
	*vmpte = PG_NV;
d1237 1
a1237 1
			pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE);
d1239 1
d1266 5
d1278 2
a1279 1
			*pte = PG_NV;
d1281 2
a1282 1
			*pte = PG_NV;
d1287 1
a1287 1
				printf("Processor %d running virtual.\n", i);
d1300 1
d1315 1
d1325 1
a1325 1
 * for the pv_head_table, pmap_modify_list; and sets these
d1335 1
d1338 1
d1343 8
a1350 6
	long		npages;
	vaddr_t		addr;
	vsize_t		s;
	pv_entry_t	pv;
	char		*attr;
	int		bank;
d1357 1
a1357 1
	 * Allocate memory for the pv_head_table,
d1364 1
d1381 6
d1390 7
a1396 1
	 * Now that the pv and attribute tables have been allocated,
d1400 1
d1406 1
d1408 1
d1412 4
a1415 3
	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", NULL);
d1434 2
d1445 1
a1445 2
pmap_zero_page(phys)
	paddr_t phys;
d1447 3
a1449 2
	vaddr_t		srcva;
	int		spl;
d1454 1
a1454 1
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu * PAGE_SIZE));
d1457 4
d1463 1
a1463 3
	*srcpte = trunc_page(phys) |
	    m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
a1464 1

d1479 3
d1487 1
a1487 5
	pmap_t			p;
	pmap_statistics_t	stats;
	sdt_entry_t		*segdt;
	int			i;
	unsigned int		s;
d1493 4
d1498 8
d1520 1
d1545 3
a1547 2
		    (vaddr_t)segdt, (vaddr_t)segdt+ (SDT_SIZE*2),
		    CACHE_INH);
d1588 1
a1588 2
	return (p);
} /* pmap_create() */
d1591 1
a1591 1
 * Routine:	PMAP_RELEASE
d1601 1
a1601 1
 *	uvm_km_free
d1615 1
a1615 2
pmap_release(pmap)
	pmap_t pmap;
d1624 1
a1624 1
		printf("(pmap_release :%x) pmap %x\n", curproc, pmap);
d1641 1
a1641 1
		if ((gdttbl = pmap_pte(pmap, (vaddr_t)sdt_va)) != PT_ENTRY_NULL) {
d1644 1
a1644 1
				printf("(pmap_release :%x) free page table = 0x%x\n",
d1653 1
a1653 1
		printf("(pmap_release :%x) free segment table = 0x%x\n",
d1659 2
a1660 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, 2*SDT_SIZE);
d1662 4
d1667 2
a1668 2
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
		printf("(pmap_release :%x) ref_count = 0\n", curproc);
d1670 2
a1671 2
	pmap->prev->next = pmap->next;
	pmap->next->prev = pmap->prev;
d1673 1
a1673 1
} /* pmap_release() */
d1686 4
a1689 2
 *	pmap_release
 *	pool_put
d1695 2
a1696 2
 * structure. If it goes to zero, pmap_release is called to release
 * the memory space to the system. Then, call pool_put to free the
d1700 1
a1700 2
pmap_destroy(p)
	pmap_t p;
d1702 1
a1702 1
	int	c, s;
d1727 1
d1747 1
a1747 2
pmap_reference(p)
	pmap_t p;
d1749 1
a1749 1
	int	s;
d1777 4
d1782 7
a1788 1
 *	pool_put
d1791 1
d1809 1
d1811 1
a1811 3
pmap_remove_range(pmap, s, e)
	pmap_t pmap;
	vaddr_t s, e;
d1813 1
d1817 1
a1817 2
	paddr_t		pa;
	vaddr_t		va;
d1819 1
a1819 1
	pt_entry_t	opte;
d1841 1
d1865 1
a1865 1
		if (pmap_pte_w(pte))
d1868 2
a1869 1
		pa = ptoa(PG_PFNUM(*pte));
d1872 1
d1877 1
a1877 1
			pvl = pa_to_pvh(pa);
d1886 1
d1918 2
d1929 1
a1929 1
		 * the modified bit and/or the reference bit by any other cpu.
d1931 1
a1931 1
		opte = invalidate_pte(pte);
d1934 1
a1934 1
		if (opte & PG_M) {
d1936 5
d1942 1
a1942 1
				*pa_to_attribute(pa) = TRUE;
d1957 1
a1957 3
 *	map		pointer to pmap structure
 *	s
 *	e
d1963 2
d1966 1
d1972 1
a1972 3
pmap_remove(map, s, e)
	pmap_t map;
	vaddr_t s, e;
d1974 1
a1974 1
	int spl;
d2012 4
d2017 1
d2019 2
a2020 1
 *	pool_put
d2036 1
a2036 2
pmap_remove_all(phys)
	paddr_t phys;
d2040 1
a2040 1
	vaddr_t		va;
d2044 1
a2044 1
	pt_entry_t	opte;
a2057 1

d2067 1
a2067 1
	pvl = pa_to_pvh(phys);
d2069 1
d2077 1
d2102 1
a2102 1
		if (ptoa(PG_PFNUM(*pte)) != phys)
d2104 1
a2104 1
		if (pmap_pte_w(pte))
d2110 1
a2110 1
			*pvl = *cur;
d2120 1
a2120 1
		 * by any other cpu.
d2122 1
a2122 1
		opte = invalidate_pte(pte);
d2125 5
a2129 1
		if (opte & PG_M) {
d2131 1
a2131 1
			*pa_to_attribute(phys) = TRUE;
d2146 1
d2160 7
a2166 3
 *	pa_to_pvh
 *	simple_lock, simple_unlock
 *	pmap_pte
d2176 1
a2176 2
pmap_copy_on_write(phys)
	paddr_t phys;
d2182 1
d2196 1
a2196 1
	pv_e = pa_to_pvh(phys);
d2198 1
d2205 1
a2205 2
		SPLX(spl);
		return;		/* no mappings */
d2214 5
a2218 2
		pmap_t	pmap = pv_e->pmap;
		vaddr_t	va = pv_e->va;
d2221 1
d2240 1
a2240 1
		if (ptoa(PG_PFNUM(*pte)) != phys)
d2248 1
a2248 1
		 * by any other cpu.
d2250 3
a2252 1
		*pte = invalidate_pte(pte) | PG_PROT;
d2258 1
a2258 1
	CHECK_PV_LIST(phys, pa_to_pvh(phys), "pmap_copy_on_write");
d2260 2
d2279 1
d2293 1
a2293 4
pmap_protect(pmap, s, e, prot)
	pmap_t pmap;
	vaddr_t s, e;
	vm_prot_t prot;
d2295 2
d2298 2
a2299 2
	pt_entry_t	ap, *pte;
	vaddr_t		va;
d2301 1
d2316 2
a2317 1
	ap = m88k_protection(pmap, prot) & PG_PROT;
d2357 3
a2359 1

d2363 1
a2363 1
		 * written back by any other cpu.
d2365 3
a2367 1
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
d2397 4
a2400 2
 *	uvm_km_free
 *	uvm_km_zalloc
d2420 1
a2420 3
pmap_expand(map, v)
	pmap_t map;
	vaddr_t v;
d2423 1
a2423 2
	vaddr_t		pdt_vaddr;
	paddr_t		pdt_paddr;
d2446 1
a2446 1
		 * the pages for page tables should be CACHE DISABLED on MVME188
d2465 1
a2465 1
			printf("(pmap_expand :%x) table has already been allocated\n", curproc);
d2486 2
a2487 2
		*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
		*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;
d2524 1
a2524 1
 *	PT_FREE
d2563 3
a2565 6
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d2567 1
d2570 3
a2572 3
	pt_entry_t	ap, *pte;
	paddr_t		old_pa;
	pt_entry_t	template;
d2575 1
d2630 1
a2630 1
	old_pa = ptoa(PG_PFNUM(*pte));
d2642 1
a2642 1
		if (wired && !(pmap_pte_w(pte)))
d2644 1
a2644 1
		else if (!wired && pmap_pte_w(pte))
d2648 1
a2648 1
			template = CACHE_INH | PG_V;
d2650 1
a2650 1
			template = CACHE_GLOBAL | PG_V;
d2652 1
a2652 1
			template |= PG_W;
d2657 2
a2658 2
		if (!PDT_VALID(pte) || pmap_pte_w_chg(pte, template & PG_W) ||
		    (pmap_pte_prot_chg(pte, ap & PG_PROT))) {
d2661 2
a2662 3
			 * Invalidate pte temporarily to avoid being written
			 * back the modified bit and/or the reference bit by
			 * any other cpu.
d2664 3
a2666 2
			template |= (invalidate_pte(pte) & PG_M);
			*pte++ = template | ap | trunc_page(pa);
d2674 1
a2674 1
		if (old_pa != (paddr_t)-1) {
d2686 1
a2686 1
					       pte, PG_PFNUM(*pte), PDT_VALID(pte));
d2709 2
a2710 1
			pvl = pa_to_pvh(pa);
d2739 1
d2748 1
a2748 1
				 * Remember that we used the pvlist entry.
d2752 1
d2763 1
a2763 1
			template = CACHE_INH | PG_V;
d2765 2
a2766 1
			template = CACHE_GLOBAL | PG_V;
d2768 1
a2768 1
			template |= PG_W;
d2770 1
a2770 6
		if (flags & VM_PROT_WRITE)
			template |= PG_U | PG_M;
		else if (flags & VM_PROT_ALL)
			template |= PG_U;

		*pte = template | ap | trunc_page(pa);
d2791 3
d2795 1
d2797 1
d2803 1
a2803 3
pmap_unwire(map, v)
	pmap_t map;
	vaddr_t v;
d2813 1
a2813 1
	if (pmap_pte_w(pte)) {
d2816 1
a2816 1
		*pte &= ~PG_W;
d2820 1
d2850 1
a2850 4
pmap_extract(pmap, va, pap)
	pmap_t pmap;
	vaddr_t va;
	paddr_t *pap;
d2852 4
a2855 5
	pt_entry_t	*pte;
	paddr_t		pa;
	int		i;
	int		spl;
	boolean_t	rv = FALSE;
d2866 1
a2866 1
		for (i = batc_used - 1; i > 0; i--)
d2868 2
a2869 3
				if (pap != NULL)
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) | 
						(va & BATC_BLKMASK);
d2875 7
a2881 9
	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL) {
		if (PDT_VALID(pte)) {
			rv = TRUE;
			if (pap != NULL) {
				pa = ptoa(PG_PFNUM(*pte));
				pa |= (va & PAGE_MASK); /* offset within page */
				*pap = pa;
			}
		}
d2884 3
d2888 4
a2891 1
	return (rv);
d2915 2
a2916 4
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t dst_pmap, src_pmap;
	vaddr_t dst_addr, src_addr;
	vsize_t len;
d2918 1
d2934 3
d2961 1
a2961 2
pmap_collect(pmap)
	pmap_t pmap;
d2963 10
a2972 10
	vaddr_t		sdt_va;		/* outer loop index */
	vaddr_t		sdt_vt;		/* end of segment */
	sdt_entry_t	*sdttbl;	/* ptr to first entry in seg table */
	sdt_entry_t	*sdtp;		/* ptr to index into segment table */
	sdt_entry_t	*sdt;		/* ptr to index into segment table */
	pt_entry_t	*gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t	*gdttblend;	/* ptr to byte after last entry in
					   table group */
	pt_entry_t	*gdtp;		/* ptr to index into a page table */
	boolean_t	found_gdt_wired; /* flag indicating a wired page exists 
d2974 2
a2975 2
	int		spl;
	unsigned int	i, j;
d3002 1
d3021 2
a3022 2
		for (gdtp = gdttbl; gdtp < gdttblend; gdtp++) {
			if (pmap_pte_w(gdtp)) {
d3044 2
a3045 2
			*((sdt_entry_t *) sdt) = 0;
			*((sdt_entry_t *)(sdt+SDT_ENTRIES)) = 0;
d3093 1
a3093 2
pmap_activate(p)
	struct proc *p;
d3095 1
a3095 1
	apr_template_t	apr_data;
d3098 1
a3098 1
	int		n;
d3101 2
a3102 2
	pmap_t		pmap = p->p_vmspace->vm_map.pmap;
	int		cpu = cpu_number();  
d3122 1
a3122 1
#ifdef OMRON_PMAP
d3133 1
a3133 1
#else
d3136 2
a3137 2
#endif
#else
a3143 1
#endif /* notyet */
d3146 1
a3146 1
		 * Mark that this cpu is using the pmap.
d3152 1
d3179 1
a3179 2
pmap_deactivate(p)
	struct proc *p;
d3181 2
a3182 2
	pmap_t	pmap = p->p_vmspace->vm_map.pmap;
	int	cpu = cpu_number();  
d3185 1
d3211 2
d3222 1
a3222 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d3224 6
a3229 7
	vaddr_t		dstva, srcva;
	int		spl;
	pt_entry_t	template, *dstpte, *srcpte;
	int		cpu = cpu_number();

	template = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
d3234 4
a3237 2
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	dstva = (vaddr_t)(phys_map_vaddr2 + (cpu << PAGE_SHIFT));
d3242 4
d3248 1
a3248 1
	*srcpte = template | trunc_page(src);
d3253 2
d3256 1
a3256 1
	*dstpte  = template | trunc_page(dst);
d3263 1
d3280 5
a3284 1
 *		pa_to_pvh
d3286 1
d3295 1
a3295 2
pmap_clear_modify(pg)
	struct vm_page *pg;
d3302 1
a3302 1
	vaddr_t		va;
d3304 1
d3319 1
a3319 1
	pvl = pa_to_pvh(phys);
d3321 1
d3324 1
a3324 1
	*pa_to_attribute(phys) = FALSE;
d3331 1
d3341 1
d3357 1
a3357 1
		 * and/or the reference being written back by any other cpu.
d3359 1
d3361 2
a3362 1
		*pte = invalidate_pte(pte) & ~PG_M;
d3367 1
d3392 1
a3392 1
 *		pa_to_pvh
d3406 1
a3406 2
pmap_is_modified(pg)
	struct vm_page *pg;
d3408 6
a3413 6
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*ptep;
	int		spl;
	boolean_t	modified_flag;
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
d3422 1
a3422 1
	pvl = pa_to_pvh(phys);
d3426 1
a3426 1
	if ((boolean_t)*pa_to_attribute(phys)) {
d3436 1
d3441 1
a3441 1
		modified_flag = (boolean_t)*pa_to_attribute(phys);
d3446 1
d3455 1
d3464 1
a3464 1
		if (pmap_pte_m(ptep)) {
d3470 1
d3478 1
d3481 1
d3494 5
a3498 1
 *		pa_to_pvh
d3500 1
d3510 1
a3510 2
pmap_clear_reference(pg)
	struct vm_page *pg;
d3517 1
a3517 1
	vaddr_t		va;
d3519 1
d3534 3
a3536 2
clear_reference_Retry:
	pvl = pa_to_pvh(phys);
d3539 1
d3545 1
d3551 2
a3552 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->next) {
d3556 1
d3572 1
a3572 1
		 * and/or the reference being written back by any other cpu.
d3574 1
d3576 2
a3577 1
		*pte = invalidate_pte(pte) & ~PG_U;
d3581 1
d3583 1
d3603 5
a3607 1
 *	pa_to_pvh
d3615 1
d3617 1
a3617 2
pmap_is_referenced(pg)
	struct vm_page *pg;
d3619 5
a3623 5
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*ptep;
	int		spl;
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
d3632 1
a3632 1
	pvl = pa_to_pvh(phys);
d3641 2
d3644 2
a3645 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->next) {
d3647 1
d3653 1
a3653 1
		if (pmap_pte_u(ptep)) {
d3655 1
d3660 1
d3663 1
d3678 1
a3678 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
d3696 155
d3888 1
d3890 1
a3890 4
check_pv_list(phys, pv_h, who)
	paddr_t phys;
	pv_entry_t pv_h;
	char *who;
d3892 3
a3894 3
	pv_entry_t	pv_e;
	pt_entry_t	*pte;
	paddr_t		pa;
d3896 1
a3896 1
	if (pv_h != pa_to_pvh(phys)) {
d3910 1
a3910 1
		} else	return;	    /* proper empty list */
d3925 1
a3925 1
			pa = ptoa(PG_PFNUM(*pte)) | (pv_e->va & PAGE_MASK);
d3934 1
d3966 1
d3968 1
a3968 4
check_map(map, s, e, who)
	pmap_t map;
	vaddr_t s, e;
	char *who;
d3970 11
a3980 11
	vaddr_t		va,
			old_va;
	paddr_t		phys;
	pv_entry_t	pv_h,
			pv_e,
			saved_pv_e;
	pt_entry_t	*ptep;
	boolean_t	found;
	int		loopcnt;
	int		bank;
	unsigned	npages;
d4013 1
a4013 1
		phys = ptoa(PG_PFNUM(*ptep));  /* pick up phys addr */
d4030 1
a4030 1
		    va < round_page((vaddr_t)(pmap_modify_list + npages)))
d4032 1
a4032 1
		pv_h = pa_to_pvh(phys);
d4036 1
d4102 1
d4104 1
a4104 2
check_pmap_consistency(who)
	char *who;
d4106 7
a4112 7
	pmap_t		p;
	int		i;
	paddr_t		phys;
	pv_entry_t	pv_h;
	int		spl;
	int		bank;
	unsigned	npages;
d4118 1
d4141 1
a4141 1
			pv_h = pa_to_pvh(phys);
d4150 1
d4154 24
d4179 1
a4179 2
pmap_virtual_space(startp, endp)
	vaddr_t *startp, *endp;
d4185 77
d4263 130
a4392 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d4394 1
d4396 2
a4397 1
	pt_entry_t	template, *pte;
d4412 1
a4412 1
	template = m88k_protection(kernel_pmap, prot);
a4427 1
	invalidate_pte(pte);
d4429 1
a4429 1
		template |= CACHE_INH | PG_V | PG_W;
d4431 5
a4435 3
		template |= CACHE_GLOBAL | PG_V | PG_W;
	*pte = template | trunc_page(pa);
	flush_atc_entry(users, va, TRUE);
d4441 1
a4441 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
a4459 1
		pt_entry_t *pte;
a4477 2
		pte = pmap_pte(kernel_pmap, va);
		invalidate_pte(pte);
@


1.55.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55.2.1 2002/01/31 22:55:19 niklas Exp $	*/
d3 1
a3 1
 * Copyright (c) 2001, 2002 Miodrag Vallat
d48 3
d96 22
a117 17
#define CD_CACHE	0x0000020	/* pmap_cache_ctrl */
#define CD_BOOT		0x0000040	/* pmap_bootstrap */
#define CD_INIT		0x0000080	/* pmap_init */
#define CD_CREAT	0x0000100	/* pmap_create */
#define CD_FREE		0x0000200	/* pmap_release */
#define CD_DESTR	0x0000400	/* pmap_destroy */
#define CD_RM		0x0000800	/* pmap_remove */
#define CD_RMAL		0x0001000	/* pmap_remove_all */
#define CD_PROT		0x0002000	/* pmap_protect */
#define CD_EXP		0x0004000	/* pmap_expand */
#define CD_ENT		0x0008000	/* pmap_enter */
#define CD_UPD		0x0010000	/* pmap_update */
#define CD_COL		0x0020000	/* pmap_collect */
#define CD_CBIT		0x0040000	/* pmap_changebit */
#define CD_TBIT		0x0080000	/* pmap_testbit */
#define CD_CREF		0x0100000	/* pmap_clear_reference */
#define CD_PGMV		0x0200000	/* pagemove */
d161 1
a161 1
#if defined(MVME187) || defined(MVME197)
a166 1
#if defined(MVME188) && defined(MVME187) || defined(MVME197)
a167 4
#else
#define	OBIO_PDT_SIZE	MAX(M188_PDT_SIZE, M1x7_PDT_SIZE)
#endif

d177 1
a177 1
 * Cached page flags
d180 2
a181 2
 * of some flags for pages which are no longer containd in any
 * pmap (for mapped pages, the modified flags are in the PTE).
d183 1
a183 1
u_int8_t  *pmap_cached_attributes;
d237 20
a256 1
#ifdef	PMAP_USE_BATC
a267 2
#endif	/* PMAP_USE_BATC */

d276 10
a285 10
void flush_atc_entry(long, vaddr_t, boolean_t);
pt_entry_t *pmap_expand_kmap(vaddr_t, vm_prot_t);
void pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
void pmap_expand(pmap_t, vaddr_t);
void pmap_release(pmap_t);
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, u_int);
pt_entry_t *pmap_pte(pmap_t, vaddr_t);
void pmap_remove_all(paddr_t);
void pmap_changebit(paddr_t, int, int);
boolean_t pmap_testbit(paddr_t, int);
d449 1
a449 1
		printf("(pmap_expand_kmap: %x) v %x\n", curproc,virt);
d459 4
a462 3
	if (kpdt_ent == KPDT_ENTRY_NULL)
		panic("pmap_expand_kmap: Ran out of kernel pte tables");

d468 2
a469 2
	kpdt_ent->phys = (paddr_t)0;
	kpdt_ent->next = NULL;
d517 1
a517 1
	u_int cmode;
d519 2
a520 2
	u_int		npages;
	u_int		num_phys_pages;
a522 4
#ifdef	PMAP_USE_BATC
	batc_template_t	batctmp;
	int		i;
#endif
d526 1
a526 1
		printf ("(pmap_map: %x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
d537 88
a624 1
#ifdef	PMAP_USE_BATC
a637 2
#endif

d640 1
a640 4
	for (num_phys_pages = npages; num_phys_pages != 0; num_phys_pages--) {

#ifdef	PMAP_USE_BATC

d642 2
a643 2
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			printf("(pmap_map: %x) num_phys_pg=%x, virt=%x, "
d659 1
a659 1
			for (i = 0; i < MAX_CPUS; i++)
d665 2
a666 2
			if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM)) {
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
d668 4
a671 3
			if (pmap_con_dbg & CD_MAP)
				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE) {
					pte = pmap_pte(kernel_pmap, virt + i);
d673 1
a673 2
						printf("(pmap_map: %x) va %x is already mapped: pte %x\n",
						    curproc, virt + i, *pte);
d675 1
a682 2
#endif	/* PMAP_USE_BATC */
	
d684 2
a685 2
			pte = pmap_expand_kmap(virt,
			    VM_PROT_READ | VM_PROT_WRITE);
d687 2
a688 2
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
d690 1
a690 1
				printf("(pmap_map: %x) pte @@ 0x%p already valid\n", curproc, pte);
d693 1
a693 1
		*pte = template | page;
d698 1
a698 1
} /* pmap_map() */
d711 1
a711 1
 *	u_int		mode
d728 1
a728 1
	u_int mode;
d732 1
a732 1
	vaddr_t		va, pteva;
d735 1
a735 1
	u_int		users;
d743 1
a743 1
		printf("(pmap_cache_ctrl: %x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
d766 1
a766 1
			printf("(cache_ctrl) pte@@0x%p\n", pte);
d780 1
a780 2
		pteva = ptoa(PG_PFNUM(*pte));
		for (cpu = 0; cpu < MAX_CPUS; cpu++)
d782 2
a783 1
				cmmu_flush_remote_cache(cpu, pteva, PAGE_SIZE);
d814 1
a814 1
 *	pmap_map
d854 1
a854 1
	extern void	*kernelstart, *etext;
d858 1
a858 1
		printf("pmap_bootstrap: \"load_start\" 0x%x\n", load_start);
d863 1
a863 1
		panic("pmap_bootstrap: \"load_start\" not on the m88k page boundary: 0x%x", load_start);
d886 1
a886 1
	    (trunc_page((vaddr_t)&kernelstart) - load_start);
d925 1
a925 1
		printf("       kernel segment end = 0x%x\n", ((paddr_t)kernel_pmap->sdt_paddr) + kernel_pmap_size);
d942 1
d944 1
a944 1
	pdt_size = round_page(MAX_KERNEL_PDT_SIZE);
d972 1
a972 1
	for (i = pdt_size / PDT_SIZE; i != 0; i--) {
d985 2
a986 2
	e_text = load_start + ((vaddr_t)&etext -
	    trunc_page((vaddr_t)&kernelstart));
d990 5
a994 3
	/* map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
	vaddr = pmap_map(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_INH);
d996 5
a1000 2
	/* map the kernel text read only */
	vaddr = pmap_map(trunc_page((vaddr_t)&kernelstart),
d1004 1
a1004 1
	vaddr = pmap_map(vaddr, e_text, (paddr_t)kmap,
d1022 1
a1022 1
	vaddr = pmap_map(vaddr, (paddr_t)kmap, *phys_start,
d1033 1
a1033 1
			printf("1: vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
d1053 1
a1053 1
		vaddr = pmap_map(vaddr, *phys_start, *phys_start + etherlen,
d1062 1
a1062 1
				printf("2: vaddr %x *virt_start %x *phys_start %x\n", vaddr,
d1108 1
a1108 1
	for (; ptable->size != (size_t)(-1); ptable++){
d1113 1
a1113 1
			pmap_map(ptable->virt_start, ptable->phys_start,
d1118 1
d1230 1
a1230 1
 *	pmap_cached_attributes
d1239 1
a1239 1
 * for the pv_head_table, pmap_cached_attributes; and sets these
d1259 1
a1259 1
	u_int8_t	*attr;
d1274 1
a1274 1
	s += npages * sizeof(u_int8_t);		/* pmap_cached_attributes */
d1290 1
a1290 1
	pmap_cached_attributes = (u_int8_t *)addr;
d1297 1
a1297 1
	attr = pmap_cached_attributes;
d1349 1
a1349 1
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
d1355 1
a1355 1
	bzero((void *)srcva, PAGE_SIZE);
d1374 5
a1378 3
	pmap_t		p;
	sdt_entry_t	*segdt;
	u_int		s;
d1380 1
a1380 3
#ifdef	PMAP_USE_BATC
	int		i;
#endif
d1382 1
a1382 1
	p = pool_get(&pmappool, PR_WAITOK);
d1391 1
a1391 1
		printf("(pmap_create: %x) need %d pages for sdt\n",
d1404 1
a1404 3
	if (pmap_extract(kernel_pmap, (vaddr_t)segdt,
	    (paddr_t *)&p->sdt_paddr) == FALSE)
		panic("pmap_create: pmap_extract failed!");
d1412 2
a1413 2
		printf("(pmap_create: %x) pmap=0x%p, sdt_vaddr=0x%x, sdt_paddr=0x%x\n",
		       curproc, p, p->sdt_vaddr, p->sdt_paddr);
d1442 1
a1442 1
#ifdef	PMAP_USE_BATC
d1450 7
d1499 1
a1499 1
	u_int		i,j;
d1503 1
a1503 1
		printf("(pmap_release: %x) pmap %x\n", curproc, pmap);
d1523 1
a1523 1
				printf("(pmap_release: %x) free page table = 0x%x\n",
d1532 1
a1532 1
		printf("(pmap_release: %x) free segment table = 0x%x\n",
d1538 1
a1538 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2*SDT_SIZE));
d1542 1
a1542 1
		printf("(pmap_release: %x) ref_count = 0\n", curproc);
d1575 9
a1583 1
	int count;
d1590 7
a1596 4
	simple_lock(&p->lock);
	count = --p->ref_count;
	simple_unlock(&p->lock);
	if (count == 0) {
d1612 4
a1615 1
 * Under a pmap read lock, the ref_count field of the pmap structure
d1622 7
a1629 3
	simple_lock(&p->lock);
	p->ref_count++;
	simple_unlock(&p->lock);
d1643 1
a1643 1
 *	e		virtual address of end of range to remove
d1647 1
a1647 1
 *	pmap_cached_attributes
d1666 2
a1667 2
 * entry in the pmap_cached_attributes. Next, the function must find the PV
 * list entry associated with this pmap/va (if it doesn't exist - the function
d1681 1
a1681 1
	u_int		users;
d1701 2
d1738 1
d1761 2
a1762 3
				for (cur = pvl; cur != PV_ENTRY_NULL;
				    cur = cur->next) {
					if (cur->va == va && cur->pmap == pmap)
d1764 1
a1764 1
					prev = cur;
d1768 1
a1768 1
					       "0x%x (pa 0x%x) PV list at 0x%p\n", va, pa, pvl);
d1775 2
d1794 1
a1794 1
				*pa_to_attribute(pa) |= PG_M;
d1806 1
a1806 2
 *	It is assumed that start and end are properly rounded to the VM page
 *	size.
d1829 1
a1829 1
	if (map == PMAP_NULL)
d1831 1
a1831 1

d1834 1
a1834 1
		printf("(pmap_remove: %x) map %x  s %x  e %x\n", curproc, map, s, e);
d1837 2
d1840 1
a1840 1
	if (s >= e)
d1862 1
a1862 1
 *	pmap_cached_attributes
d1875 1
a1875 2
 * corresponding bit in the pmap_cached_attributes entry corresponding
 * to the physical page is set to 1.
d1886 1
a1886 1
	pv_entry_t	pvl;
d1891 3
d1902 1
a1902 1
			printf("(pmap_remove_all: %x) phys addr 0x%x not a managed page\n", curproc, phys);
d1910 3
a1912 2
	 * We don't have to lock the pv_head, since we have the entire pmap
	 * system.
d1917 1
d1924 1
a1924 1
		if (!simple_lock_try(&pmap->lock))
d1926 7
a1939 1
#ifdef DIAGNOSTIC
d1942 2
a1943 2
			printf("(pmap_remove_all: %p) phys %p pmap %x va %x dbgcnt %x\n",
			       curproc, phys, pmap, va, dbgcnt);
d1947 28
a1974 4
#endif	/* DIAGNOSTIC */
		if (!PDT_VALID(pte)) {
			pvl = pvl->next;
			goto next;	/* no page mapping */
d1976 48
a2023 1
		if (pmap_pte_w(pte)) {
d2025 2
a2026 3
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
				    phys);
d2028 2
a2029 3
			pvl = pvl->next;
			goto next;
		}
d2031 1
a2031 1
		pmap_remove_range(pmap, va, va + PAGE_SIZE);
d2033 5
d2039 2
a2040 1
		dbgcnt++;
d2042 35
d2078 5
a2082 2
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
d2084 3
a2086 1
next:
d2088 1
d2090 2
d2093 1
a2093 1
} /* pmap_remove_all() */
d2130 1
a2130 1
	u_int		users;
d2160 2
d2163 1
a2163 1
	for (va = s; va < e; va += PAGE_SIZE) {
d2167 3
a2169 5
			if (va <= e - (1 << SDT_SHIFT)) {
				/* no page table, skip to next seg entry */
				va += (1 << SDT_SHIFT) - PAGE_SIZE;
			} else {
				/* wrap around */
a2170 1
			}
d2173 1
a2173 1
				printf("(pmap_protect: %x) no page table: skip to 0x%x\n", curproc, va + PAGE_SIZE);
d2181 1
a2181 1
				printf("(pmap_protect: %x) pte invalid pte @@ 0x%x\n", curproc, pte);
d2231 1
a2231 1
 * 2:	The page table entries (PTEs) are initialized (set invalid), and
d2259 1
a2259 1
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, map, v);
d2266 1
a2266 2
	if (pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr) == FALSE)
		panic("pmap_expand: pmap_extract failed!");
d2290 1
a2290 1
			printf("(pmap_expand: %x) table has already been allocated\n", curproc);
d2299 1
a2299 1
	 * first page table in the group, and initialize all the
d2302 1
a2302 1
	v &= ~((1 << (LOG2_PDT_TABLE_GROUP_SIZE + PDT_BITS + PG_BITS)) - 1);
d2329 1
a2329 1
 * N.B.: This is the only routine which MAY NOT lazy-evaluation or lose
d2342 1
a2342 1
 *	pmap_cached_attributes
d2401 2
a2402 2
	u_int		users;
	boolean_t	kflush;
d2415 1
a2415 1
			printf("(pmap_enter: %x) pmap kernel va %x pa %x\n", curproc, va, pa);
d2417 1
a2417 1
			printf("(pmap_enter: %x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
d2440 3
a2442 1
			pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
d2500 5
d2506 10
a2515 1
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
d2517 3
a2519 5
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
				       phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
				       PMAP_MANAGED(pa) ? 1 : 0);
				printf("pte %x pfn %x valid %x\n",
				       pte, PG_PFNUM(*pte), PDT_VALID(pte));
a2521 6
#endif
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(users, va, TRUE);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}
d2536 1
d2610 1
a2610 1
 * Routine:	pmap_unwire
d2612 5
a2616 1
 * Function:	Change the wiring attributes for a map/virtual-address pair.
d2618 2
a2619 6
 * Parameters:
 *	pmap	pointer to pmap structure
 *	v	virtual address of page to be unwired
 *
 * Calls:
 *	pmap_pte
d2621 2
a2622 2
 * Special Assumptions:
 *	The mapping must already exist in the pmap.
a2661 2
 * If BATC mapping is enabled and the specified pmap is kernel_pmap,
 * batc_entry is scanned to find out the mapping. 
d2663 4
a2666 1
 * Then the routine calls pmap_pte to get a (virtual) pointer to
d2680 1
a2683 4
#ifdef	PMAP_USE_BATC
	int		i;
#endif

a2688 1
#ifdef	PMAP_USE_BATC
d2692 2
a2693 2
	if (pmap == kernel_pmap && batc_used != 0)
		for (i = batc_used - 1; i != 0; i--)
a2699 1
#endif
d2719 1
a2719 1
 * Routine:	PMAP_COPY
d2721 11
a2731 11
 * Function:
 *	Copy the range specigfied by src_adr/len from the source map
 *	to the range dst_addr/len in the destination map. This routine
 *	is only advisory and need not do anything.
 *
 * Parameters:
 *	dst_pmap	pointer to destination  pmap structure
 *	src_pmap	pointer to source pmap structure
 *	dst_addr	VA in destination map
 *	len		length of address space being copied
 *	src_addr	VA in source map
d2733 1
a2733 1
 * At this time, the 88200 pmap implementation does nothing in this
d2798 1
a2798 1
	u_int		i, j;
d2808 2
d2812 1
a2812 1
		printf ("(pmap_collect: %x) pmap %x\n", curproc, pmap);
d2832 1
a2832 1
		sdt_va = VM_MIN_ADDRESS + PDT_TABLE_GROUP_VA_SPACE * i;
d2854 4
a2857 2
		sdt_vt = sdt_va <= VM_MAX_ADDRESS - PDT_TABLE_GROUP_VA_SPACE ?
		    sdt_va + PDT_TABLE_GROUP_VA_SPACE : VM_MAX_ADDRESS;
d2863 1
a2863 1
		 * we can safely deallocate the page map(s)
d2867 1
a2867 1
			*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = 0;
d2885 1
a2885 1
		printf  ("(pmap_collect: %x) done\n", curproc);
d2887 2
d2919 2
a2920 4
	pmap_t		pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int		cpu = cpu_number();  

#ifdef	PMAP_USE_BATC
d2923 3
d2929 1
a2929 1
		printf("(pmap_activate: %x) pmap 0x%p\n", p, pmap);
d2944 2
a2945 2

#ifdef	PMAP_USE_BATC
d2955 1
a2955 1
			*(register_t *)&batc_entry[n] = pmap->i_batc[n].bits;
d2958 10
a2967 2
		cmmu_flush_tlb(FALSE, 0, -1);
#endif	/* PMAP_USE_BATC */
d2975 10
d3005 1
a3005 1
	pmap_t	pmap = vm_map_pmap(&p->p_vmspace->vm_map);
d3019 1
a3019 4
 * Routine:	PMAP_COPY_PAGE
 *
 * Function:
 *	Copies the specified pages.
d3021 2
a3022 3
 * Parameters:
 *	src	PA of source page
 *	dst	PA of destination page
d3024 7
a3030 3
 * Extern/Global:
 *	phys_map_vaddr1
 *	phys_map_vaddr2
d3032 2
a3033 2
 * Calls:
 *	m88k_protection
d3035 2
a3036 2
 * Special Assumptions:
 *	no locking reauired
d3038 1
a3038 1
 * This routine maps the physical pages at the 'phys_map' virtual
d3064 1
a3064 1
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
d3070 1
a3070 1
	cmmu_flush_tlb(TRUE, dstva, PAGE_SIZE);
d3074 1
a3074 1
	bcopy((void *)srcva, (void *)dstva, PAGE_SIZE);
d3081 1
a3081 1
 * Routine:	PMAP_CHANGEBIT
d3083 2
a3084 2
 * Function:
 *	Update the pte bits on the specified physical page.
d3086 2
a3087 4
 * Parameters:
 *	pg	vm_page
 *	set	bits to set
 *	mask	bits to mask
d3089 3
a3091 3
 * Extern/Global:
 *	pv_head_table, pv_lists
 *	pmap_cached_attributes
d3093 3
a3095 3
 * Calls:
 *	pa_to_pvh
 *	pmap_pte
d3097 5
a3101 5
 * The pte bits corresponding to the page's frame index will be changed as
 * requested. The PV list will be traversed.
 * For each pmap/va the hardware the necessary bits in the page descriptor
 * table entry will be altered as well if necessary. If any bits were changed,
 * a TLB flush will be performed.
d3103 3
a3105 4
void
pmap_changebit(pg, set, mask)
	paddr_t pg;
	int set, mask;
d3109 1
a3109 1
	pt_entry_t	*pte, npte;
d3113 1
a3113 1
	u_int		users;
d3115 4
d3121 2
a3122 2
	if (!PMAP_MANAGED(pg))
		panic("pmap_changebit: not managed?");
d3127 3
a3129 2
changebit_Retry:
	pvl = pa_to_pvh(pg);
d3131 2
a3132 4
	/*
	 * Clear saved attributes (modify, reference)
	 */
	*pa_to_attribute(pg) &= mask;
d3136 2
a3137 3
		if ((pmap_con_dbg & (CD_CBIT | CD_NORM)) == (CD_CBIT | CD_NORM))
			printf("(pmap_changebit: %x) phys addr 0x%x not mapped\n",
			    curproc, pg);
d3140 1
a3140 1
		return;
d3143 1
a3143 1
	/* for each listed pmap, update the affected bits */
d3148 1
a3148 1
			goto changebit_Retry;
a3157 5

#ifdef DIAGNOSTIC
		/*
		 * Check for existing and valid pte
		 */
d3159 1
a3159 6
			panic("pmap_changebit: bad pv list entry.");
		if (!PDT_VALID(pte))
			panic("pmap_changebit: invalid pte");
		if (ptoa(PG_PFNUM(*pte)) != pg)
			panic("pmap_changebit: pte doesn't point to page");
#endif
a3161 8
		 * Update bits
		 */
		*pte = invalidate_pte(pte);
		npte = (*pte | set) & mask;

		/*
		 * Flush TLB of which cpus using pmap.
		 *
d3165 3
a3167 4
		if (npte != *pte) {
			*pte = npte;
			flush_atc_entry(users, va, kflush);
		}
a3171 1
} /* pmap_changebit() */
d3173 2
a3174 17
/*
 * Routine:	PMAP_CLEAR_MODIFY
 *
 * Function:
 *	Clears the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;

	rv = pmap_testbit(pa, PG_M);
	pmap_changebit(pa, 0, ~PG_M);
	return rv;
}
d3177 1
a3177 1
 * Routine:	PMAP_TESTBIT
d3179 4
a3182 2
 * Function:
 *	Test the modified/referenced bits of a physical page.
d3184 2
a3185 3
 * Parameters:
 *	pg	vm_page
 *	bit	bit to test
d3187 3
a3189 3
 * Extern/Global:
 *	pv_head_array, pv lists
 *	pmap_cached_attributes
d3191 6
a3196 4
 * Calls:
 *	simple_lock, simple_unlock
 *	pa_to_pvh
 *	pmap_pte
d3198 4
a3201 2
 * If the attribute list for the given page has the bit, this routine
 * returns TRUE.
d3203 1
a3203 1
 * Otherwise, this routine walks the PV list corresponding to the
d3205 2
a3206 3
 * examined. If the selected bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list), and updates the
 * attribute list.
d3209 2
a3210 3
pmap_testbit(pg, bit)
	paddr_t pg;
	int bit;
d3216 2
a3217 1
	boolean_t	rv;
d3220 2
a3221 2
	if (!PMAP_MANAGED(pg))
		panic("pmap_testbit: not managed?");
d3226 3
a3228 2
	pvl = pa_to_pvh(pg);
testbit_Retry:
d3230 2
a3231 2
	if (*pa_to_attribute(pg) & bit) {
		/* we've already cached a this flag for this page,
d3234 2
a3235 3
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
			printf("(pmap_testbit: %x) already cached a modify flag for this page\n",
			    curproc);
d3242 6
a3247 7
		/* unmapped page - get info from attribute array
		   maintained by pmap_remove_range/pmap_remove_all */
		rv = (boolean_t)(*pa_to_attribute(pg) & bit);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
			printf("(pmap_testbit: %x) phys addr 0x%x not mapped\n",
			    curproc, pg);
d3250 1
a3250 1
		return (rv);
d3257 1
a3257 1
			goto testbit_Retry;
d3262 2
a3263 2
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->va);
			panic("pmap_testbit: bad pv list entry");
d3265 1
a3265 1
		if (*ptep & bit) {
a3266 1
			*pa_to_attribute(pg) |= bit;
d3268 2
a3269 2
			if ((pmap_con_dbg & (CD_TBIT | CD_FULL)) == (CD_TBIT | CD_FULL))
				printf("(pmap_testbit: %x) modified page pte@@0x%p\n", curproc, ptep);
d3280 1
a3280 1
} /* pmap_testbit() */
d3283 7
a3289 1
 * Routine:	PMAP_IS_MODIFIED
d3291 10
a3300 3
 * Function:
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d3303 1
a3303 1
pmap_is_modified(pg)
d3306 51
a3356 1
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
d3358 7
a3364 2
	return (pmap_testbit(phys, PG_M));
} /* pmap_is_modified() */
d3366 3
a3368 12
/*
 * Routine:	PMAP_CLEAR_REFERENCE
 *
 * Function:
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;
d3370 2
a3371 4
	rv = pmap_testbit(pa, PG_U);
	pmap_changebit(pa, 0, ~PG_U);
	return rv;
}
d3378 16
a3393 1
 *	any physical maps.
d3399 4
d3405 35
a3439 2
	return (pmap_testbit(phys, PG_U));
}
d3445 1
a3445 1
 *	pmap_changebit
d3460 1
a3460 2
		/* copy on write */
		pmap_changebit(phys, PG_RO, ~0);
d3471 303
d3790 1
a3790 1
	u_int		users;
d3797 1
a3797 1
		printf ("(pmap_kenter_pa: %x) va %x pa %x\n", curproc, va, pa);
d3810 2
a3811 1
		pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
d3837 1
a3837 1
	u_int		users;
d3841 1
a3841 1
		printf("(pmap_kremove: %x) va %x len %x\n", curproc, va, len);
d3850 1
a3850 1
	for (len >>= PAGE_SHIFT; len != 0; len--, va += PAGE_SIZE) {
@


1.55.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55.2.2 2002/06/11 03:37:11 art Exp $	*/
d1231 2
a1232 1
pmap_zero_page(struct vm_page *pg)
a1233 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d2754 2
a2755 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a2756 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
@


1.55.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d3 1
a3 1
 * Copyright (c) 2001, 2002, 2003 Miodrag Vallat
d73 1
a73 1
 * Macros to operate pm_cpus field
d147 1
a147 1
#if defined(MVME188)
d151 1
a151 1
#endif
d157 1
a157 1
#endif
d173 9
d184 1
a184 1
static pv_entry_t pg_to_pvh(struct vm_page *);
d186 17
a202 5
static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}
d218 1
a218 1
		simple_lock(&(pmap)->pm_lock); \
d222 1
a222 1
		simple_unlock(&(pmap)->pm_lock); \
d226 2
d229 4
a232 2
void *etherbuf = NULL;
int etherlen;
d239 1
a239 1
int batc_used;
d263 3
a265 3
void pmap_remove_all(struct vm_page *);
void pmap_changebit(struct vm_page *, int, int);
boolean_t pmap_testbit(struct vm_page *, int);
d282 1
a282 1
m88k_protection(pmap_t pmap, vm_prot_t prot)
d294 4
a297 4
		/* if the map is the kernel's map and since this
		 * is not a paged kernel, we go ahead and mark
		 * the page as modified to avoid an exception
		 * upon writing to the page the first time.  XXX smurph
d299 1
a299 1
		if (pmap == kernel_pmap) {
d304 3
a306 3
#endif
	return p;
}
d322 5
a326 2
void
flush_atc_entry(long users, vaddr_t va, boolean_t kernel)
d328 2
a329 2
	int cpu;
	long tusers = users;
d336 1
a336 2
#endif

d362 3
a364 1
pmap_pte(pmap_t pmap, vaddr_t virt)
d366 1
a366 1
	sdt_entry_t *sdt;
d371 1
a371 1
	if (pmap == PMAP_NULL)
d375 1
a375 1
	sdt = SDTENT(pmap,virt);
d382 4
a385 3
	return (pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES)) << PDT_SHIFT) +
		PDTIDX(virt);
}
d420 3
a422 1
pmap_expand_kmap(vaddr_t virt, vm_prot_t prot)
d424 2
a425 2
	sdt_entry_t template, *sdt;
	kpdt_entry_t kpdt_ent;
a430 1

d433 1
a433 1
	/* segment table entry derivate from map and virt. */
d450 2
a451 2
	return (pt_entry_t *)(kpdt_ent) + PDTIDX(virt);
}
d492 10
a501 6
pmap_map(vaddr_t virt, paddr_t start, paddr_t end, vm_prot_t prot, u_int cmode)
{
	u_int npages;
	u_int num_phys_pages;
	pt_entry_t template, *pte;
	paddr_t	 page;
d504 1
a504 1
	int i;
d521 1
d533 1
d539 1
d551 1
a551 1
		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) &&
d562 1
a562 1
					cmmu_set_pair_batc_entry(i, batc_used,
d600 1
a600 1
}
d610 1
a610 1
 *	pmap_t		pmap
d627 4
a630 1
pmap_cache_ctrl(pmap_t pmap, vaddr_t s, vaddr_t e, u_int mode)
d632 6
a637 6
	int spl;
	pt_entry_t *pte;
	vaddr_t va, pteva;
	boolean_t kflush;
	int cpu;
	u_int users;
d656 1
a656 1
	users = pmap->pm_cpus;
d688 1
a688 1
}
d731 1
a731 1
 * For m88k, we have to map BUG memory also. This is a read only
d738 19
a756 12
pmap_bootstrap(vaddr_t load_start, paddr_t *phys_start, paddr_t *phys_end,
    vaddr_t *virt_start, vaddr_t *virt_end)
{
	kpdt_entry_t kpdt_virt;
	sdt_entry_t *kmap;
	vaddr_t vaddr, virt, kernel_pmap_size, pdt_size;
	paddr_t s_text, e_text, kpdt_phys;
	apr_template_t apr_data;
	pt_entry_t *pte;
	int i;
	pmap_table_t ptable;
	extern void *kernelstart, *etext;
d758 1
a758 1
#ifdef DEBUG
d768 1
a768 1
	simple_lock_init(&kernel_pmap->pm_lock);
d772 2
a773 1
	 * physical memory, i.e. just after where the kernel image was loaded.
d776 1
a776 1
	 * The calling sequence is
d778 1
a778 1
	 *  pmap_bootstrap(&kernelstart,...);
d793 4
a796 4
	kernel_pmap->pm_count = 1;
	kernel_pmap->pm_cpus = 0;
	kernel_pmap->pm_stpa = kmap = (sdt_entry_t *)(*phys_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)(*virt_start);
d801 2
a802 2
		printf("kernel_pmap->pm_stpa = 0x%x\n",kernel_pmap->pm_stpa);
		printf("kernel_pmap->pm_stab = 0x%x\n",kernel_pmap->pm_stab);
d804 3
d809 1
a809 1
	/*
d821 1
a821 1
	kernel_pmap_size = 2 * SDT_SIZE;
d825 1
a825 1
		printf("     kernel segment start = 0x%x\n", kernel_pmap->pm_stpa);
d827 1
a827 1
		printf("       kernel segment end = 0x%x\n", ((paddr_t)kernel_pmap->pm_stpa) + kernel_pmap_size);
d829 1
a829 1
#endif
d831 1
a831 1
	bzero(kernel_pmap->pm_stab, kernel_pmap_size);
d859 1
a859 1
#endif
d886 2
a887 2
	e_text = load_start +
	    ((vaddr_t)&etext - trunc_page((vaddr_t)&kernelstart));
d922 1
a922 1
		/*
d924 1
a924 1
		 * table size up to a page boundry in the quest to get
d953 1
a953 1
		*phys_start += etherlen;
d997 1
a997 1
	ptable = pmap_table_build(0);
d1063 1
a1063 1
	apr_data.field.st_base = atop(kernel_pmap->pm_stpa);
d1072 1
a1072 1
#endif
d1086 2
a1087 2
			 * Set valid bit to DT_INVALID so that the very first
			 * pmap_enter() on these won't barf in
d1101 1
a1101 1
			SETBIT_CPUSET(i, &kernel_pmap->pm_cpus);
d1109 1
a1109 1
}
d1124 2
d1128 1
d1131 1
a1131 1
 *	pool_init
d1133 13
a1145 2
 *   This routine does not really have much to do. It initializes
 * pools for pmap structures and pv_entry structures.
d1150 7
d1161 39
d1204 2
d1215 1
a1215 1
 *	pg		page to zero
d1233 5
a1237 5
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	vaddr_t srcva;
	int spl;
	int cpu;
	pt_entry_t *srcpte;
d1245 1
a1245 1
	*srcpte = trunc_page(pa) |
d1252 2
a1253 2
	cmmu_flush_remote_data_cache(cpu, pa, PAGE_SIZE);
}
d1269 4
a1272 3
	pmap_t pmap;
	sdt_entry_t *segdt;
	u_int s;
d1274 1
a1274 1
	int i;
d1277 2
a1278 2
	pmap = pool_get(&pmappool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));
d1298 1
a1298 1
	pmap->pm_stab = segdt;
d1300 1
a1300 1
	    (paddr_t *)&pmap->pm_stpa) == FALSE)
d1303 1
a1303 1
	if (!PAGE_ALIGNED(pmap->pm_stpa))
d1305 1
a1305 1
		    (int)pmap->pm_stpa);
d1309 2
a1310 2
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x, pm_stpa=0x%x\n",
		       curproc, pmap, pmap->pm_stab, pmap->pm_stpa);
d1335 3
a1337 3
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
	pmap->pm_cpus = 0;
d1342 2
a1343 2
		pmap->pm_ibatc[i].bits = 0;
		pmap->pm_dbatc[i].bits = 0;
d1347 10
a1356 2
	return pmap;
}
d1374 1
a1374 1
 * 	pm_count field of the pmap structure goes to zero.
d1383 2
a1384 1
pmap_release(pmap_t pmap)
d1386 4
a1389 4
	unsigned long sdt_va;	/* outer loop index */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	u_int i, j;
d1396 4
a1399 4
	sdttbl = pmap->pm_stab;    /* addr of segment table */
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
d1432 4
a1435 1
		printf("(pmap_release: %x) pm_count = 0\n", curproc);
d1437 1
a1437 1
}
d1462 2
a1463 1
pmap_destroy(pmap_t pmap)
d1468 1
a1468 1
	if (pmap == kernel_pmap)
d1472 3
a1474 3
	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
d1476 2
a1477 2
		pmap_release(pmap);
		pool_put(&pmappool, pmap);
d1479 1
a1479 1
}
d1491 1
a1491 1
 * Under a pmap read lock, the pm_count field of the pmap structure
d1495 2
a1496 1
pmap_reference(pmap_t pmap)
d1499 4
a1502 4
	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}
d1519 1
d1538 1
a1538 1
 * entry in the PV list entry. Next, the function must find the PV
d1544 3
a1546 1
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
d1548 8
a1555 7
	pt_entry_t *pte, opte;
	pv_entry_t prev, cur, pvl;
	struct vm_page *pg;
	paddr_t pa;
	vaddr_t va;
	u_int users;
	boolean_t kflush;
d1558 1
a1558 1
	 * pmap has been locked by the caller.
d1560 1
a1560 1
	users = pmap->pm_cpus;
d1587 1
a1587 1
		pte = pmap_pte(pmap, va);
d1596 1
a1596 1
		pmap->pm_stats.resident_count--;
d1598 1
a1598 1
			pmap->pm_stats.wired_count--;
a1600 1
		pg = PHYS_TO_VM_PAGE(pa);
d1602 1
a1602 1
		if (pg != NULL) {
d1607 1
a1607 1
			pvl = pg_to_pvh(pg);
d1610 1
a1610 1
			if (pvl->pv_pmap == PMAP_NULL)
d1614 1
a1614 1
			if (pvl->pv_va == va && pvl->pv_pmap == pmap) {
d1620 1
a1620 1
				cur = pvl->pv_next;
d1625 1
a1625 1
					pvl->pv_pmap =  PMAP_NULL;
d1631 2
a1632 2
				    cur = cur->pv_next) {
					if (cur->pv_va == va && cur->pv_pmap == pmap)
d1642 1
a1642 1
				prev->pv_next = cur->pv_next;
d1645 1
a1645 1
		} /* if (pg != NULL) */
d1660 1
a1660 1
			if (pg != NULL) {
d1662 1
a1662 1
				pvl->pv_flags |= PG_M;
d1666 2
a1667 2
	} /* for (va = s; ...) */
}
d1678 1
a1678 1
 *	pmap		pointer to pmap structure
d1692 3
a1694 1
pmap_remove(pmap_t pmap, vaddr_t s, vaddr_t e)
d1698 1
a1698 1
	if (pmap == PMAP_NULL)
d1703 1
a1703 1
		printf("(pmap_remove: %x) map %x s %x e %x\n", curproc, pmap, s, e);
d1711 4
a1714 4
	PMAP_LOCK(pmap, spl);
	pmap_remove_range(pmap, s, e);
	PMAP_UNLOCK(pmap, spl);
}
d1724 1
a1724 1
 *	pg		physical pages which is to
d1728 2
a1729 1
 *	pv lists
d1742 1
a1742 1
 * corresponding bit in the PV list entry corresponding
d1751 2
a1752 1
pmap_remove_all(struct vm_page *pg)
d1754 5
a1758 5
	pt_entry_t *pte;
	pv_entry_t pvl;
	vaddr_t va;
	pmap_t pmap;
	int spl;
d1760 1
a1760 1
	int dbgcnt = 0;
d1763 1
a1763 1
	if (pg == NULL) {
d1767 1
a1767 1
			printf("(pmap_remove_all: %x) vm page 0x%x not a managed page\n", curproc, pg);
d1775 1
a1775 1
	 * We don't have to lock the pv list, since we have the entire pmap
d1780 1
a1780 1
	pvl = pg_to_pvh(pg);
d1785 3
a1787 3
	while ((pmap = pvl->pv_pmap) != PMAP_NULL) {
		va = pvl->pv_va;
		if (!simple_lock_try(&pmap->pm_lock))
d1799 2
a1800 2
			printf("(pmap_remove_all: %p) vm page %p pmap %x va %x dbgcnt %x\n",
			       curproc, pg, pmap, va, dbgcnt);
d1806 1
a1806 1
			pvl = pvl->pv_next;
d1813 1
a1813 1
				    pg);
d1815 1
a1815 1
			pvl = pvl->pv_next;
d1829 1
a1829 1
		simple_unlock(&pmap->pm_lock);
d1832 1
a1832 1
}
d1861 4
a1864 1
pmap_protect(pmap_t pmap, vaddr_t s, vaddr_t e, vm_prot_t prot)
d1866 5
a1870 5
	int spl;
	pt_entry_t *pte, ap;
	vaddr_t va;
	u_int users;
	boolean_t kflush;
d1888 1
a1888 1
	users = pmap->pm_cpus;
d1927 2
a1928 2
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
d1936 1
a1936 1
}
d1952 1
a1952 1
 *	pmap	point to pmap structure
d1974 6
d1982 3
a1984 1
pmap_expand(pmap_t pmap, vaddr_t v)
d1986 10
a1995 5
	int i, spl;
	vaddr_t pdt_vaddr;
	paddr_t pdt_paddr;
	sdt_entry_t *sdt;
	pt_entry_t *pte;
d1999 1
a1999 1
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, pmap, v);
d2002 1
a2002 1
	CHECK_PAGE_ALIGN(v, "pmap_expand");
d2007 1
a2007 1
		panic("pmap_expand: pmap_extract failed");
d2018 1
a2018 1
	PMAP_LOCK(pmap, spl);
d2020 1
a2020 1
	if ((pte = pmap_pte(pmap, v)) != PT_ENTRY_NULL) {
d2025 1
a2025 1
		PMAP_UNLOCK(pmap, spl);
d2045 1
a2045 1
	sdt = SDTENT(pmap,v);
d2058 2
a2059 2
	PMAP_UNLOCK(pmap, spl);
}
d2082 2
a2083 1
 *	pv lists
d2129 6
a2134 1
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
d2136 9
a2144 8
	int spl;
	pt_entry_t *pte, ap, template;
	paddr_t old_pa;
	pv_entry_t pv_e, pvl;
	u_int users;
	boolean_t kflush;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	struct vm_page *pg;
d2149 4
d2158 1
a2158 1
			printf("(pmap_enter: %x) pmap %x va %x pa %x\n", curproc, pmap, va, pa);
a2160 1

d2164 5
a2168 5
	 * Must allocate a new pvlist entry while we're unlocked;
	 * zalloc may cause pageout (which will lock the pmap system).
	 * If we determine we need a pvlist entry, we will unlock
	 * and allocate one. Then will retry, throwing away
	 * the allocated entry later (if we no longer need it).
d2173 1
a2173 1
	users = pmap->pm_cpus;
d2208 1
a2208 1
			pmap->pm_stats.wired_count++;
d2210 1
a2210 1
			pmap->pm_stats.wired_count--;
a2238 2

		pg = PHYS_TO_VM_PAGE(pa);
d2242 1
a2242 1
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n",
d2244 1
a2244 1
				       pg != NULL ? 1 : 0);
d2256 1
a2256 1
		if (pg != NULL) {
d2268 1
a2268 1
			pvl = pg_to_pvh(pg);
d2270 1
a2270 1
			if (pvl->pv_pmap == PMAP_NULL) {
d2274 3
a2276 3
				pvl->pv_va = va;
				pvl->pv_pmap = pmap;
				pvl->pv_next = PV_ENTRY_NULL;
d2286 1
a2286 1
						if (e->pv_pmap == pmap && e->pv_va == va)
d2288 1
a2288 1
						e = e->pv_next;
d2299 4
a2302 4
				pv_e->pv_va = va;
				pv_e->pv_pmap = pmap;
				pv_e->pv_next = pvl->pv_next;
				pvl->pv_next = pv_e;
d2313 1
a2313 1
		pmap->pm_stats.resident_count++;
d2315 1
a2315 1
			pmap->pm_stats.wired_count++;
d2331 1
a2331 1
	} /* if (pa == old_pa) ... else */
d2338 2
a2339 2
	return 0;
}
d2357 3
a2359 1
pmap_unwire(pmap_t pmap, vaddr_t v)
d2361 2
a2362 2
	pt_entry_t *pte;
	int spl;
d2364 1
a2364 1
	PMAP_LOCK(pmap, spl);
d2366 1
a2366 1
	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
d2371 1
a2371 1
		pmap->pm_stats.wired_count--;
d2375 2
a2376 2
	PMAP_UNLOCK(pmap, spl);
}
d2395 1
a2395 1
 * batc_entry is scanned to find out the mapping.
d2404 4
a2407 1
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
d2409 4
a2412 4
	pt_entry_t *pte;
	paddr_t pa;
	int spl;
	boolean_t rv = FALSE;
d2415 1
a2415 1
	int i;
d2431 1
a2431 1
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) |
d2433 1
a2433 1
				return TRUE;
d2451 30
a2480 2
	return rv;
}
d2519 2
a2520 1
pmap_collect( pmap_t pmap)
d2522 21
a2542 13
	vaddr_t sdt_va;		/* outer loop index */
	vaddr_t sdt_vt;		/* end of segment */
	sdt_entry_t *sdttbl;	/* ptr to first entry in seg table */
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	sdt_entry_t *sdt;	/* ptr to index into segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t *gdttblend;	/* ptr to byte after last entry in
				   table group */
	pt_entry_t *gdtp;	/* ptr to index into a page table */
	boolean_t found_gdt_wired; /* flag indicating a wired page exists
				   in a page table's address range */
	int spl;
	u_int i, j;
d2551 1
a2551 1
	sdttbl = pmap->pm_stab; /* addr of segment table */
d2554 3
a2556 3
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
d2604 1
a2604 1
		 * calls uvm_km_free or free, which will invoke another
d2610 2
a2611 1
	}
d2617 1
a2617 1
		printf("(pmap_collect: %x) done\n", curproc);
d2619 1
a2619 1
}
d2623 1
a2623 1
 *
d2645 2
a2646 1
pmap_activate(struct proc *p)
d2648 3
a2650 3
	apr_template_t apr_data;
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();
d2653 1
a2653 1
	int n;
d2663 1
a2663 1
		 * Lock the pmap to put this cpu in its active set.
d2666 1
a2666 1
		simple_lock(&pmap->pm_lock);
d2668 1
a2668 1
		apr_data.field.st_base = atop(pmap->pm_stpa);
d2676 3
a2678 3
		 * cmmu_pmap_activate will set the uapr and the batc entries,
		 * then flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE
d2681 2
a2682 2
		cmmu_pmap_activate(cpu, apr_data.bits,
				   pmap->pm_ibatc, pmap->pm_dbatc);
d2684 1
a2684 1
			*(register_t *)&batc_entry[n] = pmap->pm_ibatc[n].bits;
d2693 1
a2693 1
		SETBIT_CPUSET(cpu, &(pmap->pm_cpus));
d2695 1
a2695 1
		simple_unlock(&pmap->pm_lock);
d2697 1
a2697 1
}
d2709 1
a2709 1
 * pmap_deactive simply clears the pm_cpus field in given pmap structure.
d2713 2
a2714 1
pmap_deactivate(struct proc *p)
d2716 2
a2717 2
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();
d2723 3
a2725 3
		simple_lock(&pmap->pm_lock);
		CLRBIT_CPUSET(cpu, &(pmap->pm_cpus));
		simple_unlock(&pmap->pm_lock);
d2727 1
a2727 1
}
d2758 4
a2761 4
	vaddr_t dstva, srcva;
	int spl;
	pt_entry_t template, *dstpte, *srcpte;
	int cpu = cpu_number();
d2767 1
a2767 1
	 * Map source physical address.
d2780 1
a2780 1
	 * Map destination physical address.
d2790 1
a2790 1
}
d2799 1
a2799 1
 *	pg	physical page
d2804 2
a2805 1
 *	pv_lists
d2808 1
d2818 17
a2834 9
pmap_changebit(struct vm_page *pg, int set, int mask)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, npte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;
d2839 1
a2839 1
	pvl = pg_to_pvh(pg);
d2844 1
a2844 1
	pvl->pv_flags &= mask;
d2846 1
a2846 1
	if (pvl->pv_pmap == PMAP_NULL) {
d2849 1
a2849 1
			printf("(pmap_changebit: %x) vm page 0x%x not mapped\n",
d2857 4
a2860 4
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		va = pvep->pv_va;
		if (!simple_lock_try(&pmap->pm_lock)) {
d2863 1
a2863 1
		users = pmap->pm_cpus;
d2880 1
a2880 1
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
d2893 1
a2893 1
		 * Invalidate pte temporarily to avoid the modified bit
d2901 1
a2901 1
		simple_unlock(&pmap->pm_lock);
d2904 1
a2904 1
}
d2913 2
a2914 1
pmap_clear_modify(struct vm_page *pg)
d2916 1
d2919 2
a2920 2
	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
d2931 1
a2931 1
 *	pg	physical page
d2935 2
a2936 1
 *	pv lists
d2940 1
d2953 14
a2966 6
pmap_testbit(struct vm_page *pg, int bit)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte;
	int spl;
	boolean_t rv;
d2970 1
a2970 1
	pvl = pg_to_pvh(pg);
d2973 1
a2973 1
	if (pvl->pv_flags & bit) {
d2985 1
a2985 1
	if (pvl->pv_pmap == PMAP_NULL) {
d2988 1
a2988 1
		rv = (boolean_t)(pvl->pv_flags & bit);
d2991 1
a2991 1
			printf("(pmap_testbit: %x) vm page 0x%x not mapped\n",
d3001 1
a3001 1
		if (!simple_lock_try(&pvep->pv_pmap->pm_lock)) {
d3005 3
a3007 3
		pte = pmap_pte(pvep->pv_pmap, pvep->pv_va);
		if (pte == PT_ENTRY_NULL) {
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->pv_va);
d3010 3
a3012 3
		if (*pte & bit) {
			simple_unlock(&pvep->pv_pmap->pm_lock);
			pvl->pv_flags |= bit;
d3015 1
a3015 1
				printf("(pmap_testbit: %x) modified page pte@@0x%p\n", curproc, pte);
d3020 2
a3021 2
		simple_unlock(&pvep->pv_pmap->pm_lock);
		pvep = pvep->pv_next;
d3026 1
a3026 1
}
d3036 2
a3037 1
pmap_is_modified(struct vm_page *pg)
d3039 4
a3042 2
	return pmap_testbit(pg, PG_M);
}
d3051 2
a3052 1
pmap_clear_reference(struct vm_page *pg)
d3054 1
d3057 2
a3058 2
	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
d3070 2
a3071 1
pmap_is_referenced(struct vm_page *pg)
d3073 3
a3075 1
	return pmap_testbit(pg, PG_U);
d3088 3
a3090 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
d3092 2
d3098 1
a3098 1
		pmap_changebit(pg, PG_RO, ~0);
d3104 1
a3104 1
		pmap_remove_all(pg);
d3110 2
a3111 1
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
d3118 4
a3121 1
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
d3123 3
a3125 3
	int spl;
	pt_entry_t template, *pte;
	u_int users;
d3127 2
a3128 2
	CHECK_PAGE_ALIGN(va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_kenter_pa - PA");
d3137 1
a3137 1
	users = kernel_pmap->pm_cpus;
d3151 2
a3152 2
	kernel_pmap->pm_stats.resident_count++;
	kernel_pmap->pm_stats.wired_count++;
d3166 3
a3168 1
pmap_kremove(vaddr_t va, vsize_t len)
d3170 2
a3171 2
	int spl;
	u_int users;
d3182 1
a3182 1
	users = kernel_pmap->pm_cpus;
d3203 2
a3204 2
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;
@


1.54
log
@Revert the mvme88k to 20011212. Recent changes had not been merged correctly,
and I am fed up with dissecting diffs to put back code that disappeared.
This will likely be fixed shortly.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2001/12/09 01:13:17 miod Exp $	*/
d168 1
a168 1
#define	OBIO_PDT_SIZE	((cputyp == CPU_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
d390 14
a403 1

a404 1

d616 1
a616 1
		if (cputyp == CPU_197 && m197_atc_initialized == FALSE) {
d988 2
a989 2
		printf("kernel_pmap->sdt_paddr = %x\n",kernel_pmap->sdt_paddr);
		printf("kernel_pmap->sdt_vaddr = %x\n",kernel_pmap->sdt_vaddr);
d1008 1
a1009 1
	kernel_pmap_size = 2*SDT_SIZE;
d1121 1
a1121 1
	if (cputyp != CPU_188) { /*  != CPU_188 */
d1540 1
a1540 1
	if (cputyp == CPU_188) {
d2444 1
a2444 1
	if (cputyp == CPU_188) {
@


1.53
log
@Support for MVME197 completed.  Fix SPL defs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2001/12/12 19:34:23 miod Exp $	*/
d66 1
d121 1
a121 1
int pmap_con_dbg = CD_NONE; /* CD_FULL | CD_NORM | CD_BOOT | CD_MAP; */
d168 1
a168 1
#define	OBIO_PDT_SIZE	((brdtyp == BRD_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
a333 2
/* FORWARDS */
unsigned int m88k_protection __P((pmap_t map, vm_prot_t prot));
d335 1
a343 9
vm_offset_t pmap_map __P((vm_offset_t, vm_offset_t, vm_offset_t,
    vm_prot_t, unsigned int));
vm_offset_t pmap_map_batc __P((vm_offset_t, vm_offset_t, vm_offset_t,
    vm_prot_t, unsigned int));
void pmap_set_batc __P((pmap_t, boolean_t, int, vm_offset_t, vm_offset_t,
    boolean_t, boolean_t, boolean_t, boolean_t, boolean_t, boolean_t));
pt_entry_t *pmap_pte __P((pmap_t, vm_offset_t));
void pmap_remove_all __P((vm_offset_t));
void check_map __P((pmap_t map, vm_offset_t s, vm_offset_t e, char *who));
d390 1
a390 13
#ifdef M88110
	if (cputyp == CPU_88110) {
		p.pte.pg_used = 1;
		/* if the map is the kernel's map and since this 
		 * is not a paged kernel, we go ahead and mark 
		 * the page as modified to avoid an exception 
		 * upon writing to the page the first time.  XXX smurph 
		 */
		if (map == kernel_pmap) { 
			p.pte.modified = p.pte.prot ? 0 : 1;
		}
	}
#endif 
d392 1
d421 1
d436 1
a436 1
		return ((pt_entry_t *)(((sdt+SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
d463 1
d489 1
a489 1
	aprot = m88k_protection(map, prot);
d502 1
a502 1
	/* physical table */
a503 1
	/* virtual table */
a522 1
 *	cmode	cache control attributes
d554 1
a554 4
pmap_map(virt, start, end, prot, cmode)
	vm_offset_t virt, start, end;
	vm_prot_t prot;
	unsigned int cmode;
d559 1
d562 10
d584 2
a585 2
	aprot = m88k_protection(kernel_pmap, prot);
	
d587 1
a587 1
	
d591 1
d602 11
d616 1
d618 1
d639 1
d665 2
a666 4
pmap_map_batc(virt, start, end, prot, cmode)
	vm_offset_t virt, start, end;
	vm_prot_t prot;
	unsigned int cmode;
d687 1
a687 1
	aprot = m88k_protection(kernel_pmap, prot);
d702 1
d759 1
d761 1
d839 1
d856 1
d858 1
d860 1
d925 1
a925 2
			kernel_pmap_size,
			pdt_size;
d931 1
d976 2
a977 2
		printf("kernel_pmap->sdt_paddr = 0x%x\n",kernel_pmap->sdt_paddr);
		printf("kernel_pmap->sdt_vaddr = 0x%x\n",kernel_pmap->sdt_vaddr);
d996 1
a997 1

d999 2
a1000 5
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("     kernel segment start = 0x%x\n", kernel_pmap->sdt_paddr);
		printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
		printf("       kernel segment end = 0x%x\n", ((unsigned)kernel_pmap->sdt_paddr) + kernel_pmap_size);
	}
d1002 4
a1005 3
	/* init all segment descriptors to zero */
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);

a1007 15
	
	/* make sure page tables are page aligned!! XXX smurph */
	*phys_start = round_page(*phys_start);
	*virt_start = round_page(*virt_start);
	
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = *phys_start;
	kpdt_virt = (kpdt_entry_t)*virt_start;
	
	pdt_size = MAX_KERNEL_PDT_SIZE;
	/* might as well round up to a page - XXX smurph */
	pdt_size = round_page(pdt_size);
	kernel_pmap_size += pdt_size;
	*phys_start += pdt_size;
	*virt_start += pdt_size;
d1009 2
a1010 2
	/* init all page descriptors to zero */
	bzero((void *)kpdt_phys, pdt_size);
d1012 1
a1012 6
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("--------------------------------------\n");
		printf("        kernel page start = 0x%x\n", kpdt_phys);
		printf("   kernel page table size = 0x%x\n", pdt_size);
		printf("          kernel page end = 0x%x\n", *phys_start);
	}
d1017 3
a1019 3
		printf("kpdt_phys = 0x%x\n",kpdt_phys);
		printf("kpdt_virt = 0x%x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x, (phys)0x%08x\n",
d1027 1
a1027 1
	for (i = pdt_size/PDT_SIZE; i > 0; i--) {
d1052 5
a1056 5
	vaddr = PMAPER(0,
		       0,
		       0x10000,
		       VM_PROT_WRITE | VM_PROT_READ,
		       CACHE_INH);
d1059 11
a1069 11
	vaddr = PMAPER((vm_offset_t)trunc_page(((unsigned)&kernelstart)),
		       s_text,
		       e_text,
		       VM_PROT_WRITE | VM_PROT_READ,
		       CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(vaddr,
		       e_text,
		       (vm_offset_t)kmap,
		       VM_PROT_WRITE | VM_PROT_READ,
		       CACHE_GLOBAL);
d1085 5
a1089 2
	vaddr = PMAPER(vaddr, (vm_offset_t)kmap, *phys_start,
		      VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
a1091 5
		/* 
		 * This should never happen because we now round the PDT
		 * table size up to a page boundry in the quest to get 
		 * mc88110 working. - XXX smurph
		 */
d1094 1
a1094 1
			printf("1:vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
d1109 1
a1109 1
	if (brdtyp != BRD_188) { /*  != BRD_188 */
d1114 5
a1118 2
		vaddr = PMAPER(vaddr, *phys_start, *phys_start + etherlen,
			      VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1172 1
a1172 1
	for (; ptable->size != 0xffffffffU; ptable++){
d1180 1
a1180 1
			       ptable->prot, ptable->cacheability);
d1182 9
a1190 9
	}
	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physical pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -	         
	 * else we will be referencing the wrong page.
	 */
d1209 1
a1209 1
	
d1227 1
d1235 1
a1235 1
	apr_data.field.g  = 0;
d1254 5
d1275 1
a1275 1
				printf("Processor %d running virtual.\n", i);
d1421 1
d1505 1
a1505 1
	segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, s); 
d1507 2
a1508 1
		panic("pmap_create: kmem_alloc failure"); 
d1528 1
a1528 1
	if (brdtyp == BRD_188) {
d2267 1
d2432 1
a2432 1
	if (brdtyp == BRD_188) {
d2508 1
d3031 3
a3033 3
		for (sdt = sdtp; sdt < (sdtp+PDT_TABLE_GROUP_SIZE); sdt++) {
			((sdt_entry_template_t *) sdt)->bits = 0;
			((sdt_entry_template_t *)(sdt+SDT_ENTRIES))->bits = 0;
d3214 1
d3219 5
d3230 1
a3230 4
	/*
	 *	Map source physical address.
	 */
	template.bits = trunc_page(src) | PG_RW | 
d3241 2
a3242 2
	template.bits = trunc_page(dst) | PG_RW | 
		DT_VALID | CACHE_GLOBAL;
d3799 1
a3799 1
		cfunc = cmmu->cmmu_flush_remote_cache_func;
d3804 1
a3804 1
		cfunc = cmmu->cmmu_flush_remote_inst_cache_func;
d3809 1
a3809 1
		cfunc = cmmu->cmmu_flush_remote_data_cache_func;
d3814 1
a3814 1
		cfunc = cmmu->cmmu_flush_remote_cache_func;
d3819 1
a3819 1
		cfunc = cmmu->cmmu_flush_remote_inst_cache_func;
d3824 1
a3824 1
		cfunc = cmmu->cmmu_flush_remote_data_cache_func;
d4214 18
@


1.52
log
@- turn m88k_protection() into a trivial macro.
- de-uglify pmap_map() interface.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2001/12/09 01:13:17 miod Exp $	*/
a65 1
#include <machine/m882xx.h>		/* CMMU stuff */
d120 1
a120 1
int pmap_con_dbg = CD_NONE;
d167 1
a167 1
#define	OBIO_PDT_SIZE	((cputyp == CPU_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
d333 2
d352 1
a352 6

/*
 * PTE field macros
 */

#define	m88k_protection(prot)	((prot) & VM_PROT_WRITE ? PG_RW : PG_RO)
d389 27
a440 1

d455 1
a455 1
		return ((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
d507 1
a507 1
	aprot = m88k_protection(prot);
d520 1
a520 1

d522 1
a583 3
#ifdef MVME197
	static int m197_atc_initialized = FALSE;
#endif
d596 2
a597 2
	aprot = m88k_protection(prot);

d599 1
a599 1

a602 1

a612 11
#ifdef MVME197
		/* hack for MVME197 */
		if (cputyp == CPU_197 && m197_atc_initialized == FALSE) {
			int i;

			for (i = 0; i < 32; i++)
				m197_load_patc(i, virt, 
					       (vm_offset_t)template.bits, 1);
			m197_atc_initialized = TRUE;
		}
#endif 
a615 1

a616 1

d686 1
a686 1
	aprot = m88k_protection(prot);
a700 1

a756 1

a757 1

a834 1

a850 1

a851 1

a852 1

d917 2
a918 1
			kernel_pmap_size;
a923 1
	extern void	cmmu_go_virt(void);
d968 2
a969 2
		printf("kernel_pmap->sdt_paddr = %x\n",kernel_pmap->sdt_paddr);
		printf("kernel_pmap->sdt_vaddr = %x\n",kernel_pmap->sdt_vaddr);
d988 1
a989 1
	kernel_pmap_size = 2*SDT_SIZE;
d991 5
a995 2
	printf("kernel segment table from 0x%x to 0x%x\n", kernel_pmap->sdt_vaddr, 
	       kernel_pmap->sdt_vaddr + kernel_pmap_size);
d997 3
a999 4
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = (*phys_start + kernel_pmap_size);
	kpdt_virt = (kpdt_entry_t)(*virt_start + kernel_pmap_size);
	kernel_pmap_size += MAX_KERNEL_PDT_SIZE;
d1002 15
d1018 2
a1019 2
	/* init all segment and page descriptor to zero */
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);
d1021 6
a1026 1
	printf("kernel page table to 0x%x\n", kernel_pmap->sdt_vaddr + kernel_pmap_size);
d1031 3
a1033 3
		printf("kpdt_phys = %x\n",kpdt_phys);
		printf("kpdt_virt = %x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x  ; (phys)0x%08x\n",
d1041 1
a1041 1
	for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i > 0; i--) {
d1066 5
a1070 1
	vaddr = PMAPER(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1074 10
a1083 5
		      s_text, e_text,
		      VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(vaddr, e_text, (vm_offset_t)kmap,
		      VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);
d1103 5
d1110 1
a1110 1
			printf("1:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
d1125 1
a1125 1
	if (cputyp != CPU_188) { /*  != CPU_188 */
d1185 1
a1185 1
	for (; ptable->size != 0xffffffffU; ptable++)
d1195 9
a1203 9

		/*
		 * Allocate all the submaps we need. Note that SYSMAP just allocates
		 * kernel virtual address with no physical backing memory. The idea
		 * is physical memory will be mapped at this va before using that va.
		 * This means that if different physical pages are going to be mapped
		 * at different times, we better do a tlb flush before using it -	         
		 * else we will be referencing the wrong page.
		 */
d1222 1
a1222 1

a1239 1

d1247 1
a1247 1
	apr_data.field.g  = 1;
a1265 5
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("After cmmu_flush_remote_tlb()\n");
			}
#endif
d1282 1
a1282 1
				printf("After cmmu_remote_set_sapr()\n");
d1452 1
a1452 1
			| m88k_protection(VM_PROT_READ | VM_PROT_WRITE)
d1511 1
a1511 1
	segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, s);
d1513 1
a1513 2
		panic("pmap_create: kmem_alloc failure");

d1533 1
a1533 1
	if (cputyp == CPU_188) {
d2308 1
a2308 1
	maprot.bits = m88k_protection(prot);
d2436 1
a2436 1
	if (cputyp == CPU_188) {
d2585 1
a2585 1
	ap = m88k_protection(prot);
d3034 3
a3036 3
		for (sdt = sdtp; sdt < (sdtp + PDT_TABLE_GROUP_SIZE); sdt++) {
			((sdt_entry_template_t *) sdt) -> bits = 0;
			((sdt_entry_template_t *) sdt+SDT_ENTRIES) -> bits = 0;
d3799 1
a3799 1
		cfunc = cmmu_flush_remote_cache;
d3804 1
a3804 1
		cfunc = cmmu_flush_remote_inst_cache;
d3809 1
a3809 1
		cfunc = cmmu_flush_remote_data_cache;
d3814 1
a3814 1
		cfunc = cmmu_flush_remote_cache;
d3819 1
a3819 1
		cfunc = cmmu_flush_remote_inst_cache;
d3824 1
a3824 1
		cfunc = cmmu_flush_remote_data_cache;
d4382 1
a4382 1
	ap = m88k_protection(prot);
@


1.51
log
@- change flush_atc_entry() prototype, the third argument is indeed a boolean_t,
  and update callers.
- move the kernel_pmap case out of pmap_expand(), and explicitely handle this
  in pmap_enter(). This saves an unlock/lock cycle in pmap_enter().
- put more diagnostic code in #ifdef DIAGNOSTIC/#endif pairs.
- fix a few style issues and correct a few comments.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2001/12/08 21:41:34 miod Exp $	*/
a334 1
unsigned int m88k_protection __P((pmap_t, vm_prot_t));
d343 14
a392 16
 *	Convert machine-independent protection code to M88K protection bits.
 */

unsigned int
m88k_protection(pmap_t map, vm_prot_t prot)
{
	pte_template_t p;

	p.bits = 0;
	p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;

	return (p.bits);

} /* m88k_protection */

/*
a459 1
 *	m88k_protection
d485 1
a485 1
	aprot = m88k_protection (map, prot);
d519 1
d551 4
a554 1
pmap_map(vm_offset_t virt, vm_offset_t start, vm_offset_t end, vm_prot_t prot)
a558 1
	unsigned	cmode;
a563 7
	/*
	 * cache mode is passed in the top 16 bits.
	 * extract it from there. And clear the top
	 * 16 bits from prot.
	 */
	cmode = (prot & 0xffff0000) >> 16;
	prot &= 0x0000ffff;
d576 1
a576 1
	aprot = m88k_protection (kernel_pmap, prot);
a630 1
 *	m88k_protection
d656 4
a659 2
pmap_map_batc(vm_offset_t virt, vm_offset_t start, vm_offset_t end,
	      vm_prot_t prot, unsigned cmode)
d680 1
a680 1
	aprot = m88k_protection (kernel_pmap, prot);
d1045 1
a1045 5
	vaddr = PMAPER(
		      0,
		      0,
		      0x10000,
		      (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_INH <<16));
d1048 6
a1053 11
	vaddr = PMAPER(
		      (vm_offset_t)trunc_page(((unsigned)&kernelstart)),
		      s_text,
		      e_text,
		      (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_GLOBAL<<16));  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(
		      vaddr,
		      e_text,
		      (vm_offset_t)kmap,
		      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
d1069 2
a1070 5
	vaddr = PMAPER(
		      vaddr,
		      (vm_offset_t)kmap,
		      *phys_start,
		      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1095 2
a1096 5
		vaddr = PMAPER(
			      vaddr,
			      *phys_start,
			      *phys_start + etherlen,
			      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1158 1
a1158 1
			       ptable->prot|(ptable->cacheability << 16));
a1398 1
 *	m88k_protection
d1423 1
a1423 1
			| m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
a2243 1
 *		m88k_protection
d2280 1
a2280 1
	maprot.bits = m88k_protection(pmap, prot);
a2483 1
 *	m88k_protection
d2557 1
a2557 1
	ap = m88k_protection(pmap, prot);
a3188 1
	int      aprot;
a3192 5
	/*
	 *	Map source physical address.
	 */
	aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);

d3199 4
a3202 1
	template.bits = trunc_page(src) | aprot | 
d3213 2
a3214 2
	template.bits = trunc_page(dst) | aprot | 
		CACHE_GLOBAL | DT_VALID;
a4187 18
void 
use_batc(task_t task,
	 boolean_t data,	 /* for data-cmmu ? */
	 int i,			 /* batc number */
	 vm_offset_t va,	 /* virtual address */
	 vm_offset_t pa,	 /* physical address */
	 boolean_t s,		 /* for super-mode ? */
	 boolean_t wt,		 /* is writethrough */
	 boolean_t g,		 /* is global ? */
	 boolean_t ci,		 /* is cache inhibited ? */
	 boolean_t wp,		 /* is write-protected ? */
	 boolean_t v)		 /* is valid ? */
{
	pmap_t pmap;
	pmap = vm_map_pmap(task->map);
	pmap_set_batc(pmap, data, i, va, pa, s, wt, g, ci, wp, v);
}

d4354 1
a4354 1
	ap = m88k_protection(kernel_pmap, prot);
@


1.50
log
@Better pmap_kenter_pa() and pmap_kremove() implementation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2001/12/08 20:08:27 miod Exp $	*/
a69 3
#include <mvme88k/dev/pcctworeg.h>
#include <mvme88k/dev/clreg.h>

d90 30
a119 30
   #define CD_NONE		0x00
   #define CD_NORM		0x01
   #define CD_FULL		0x02

   #define CD_ACTIVATE		0x0000004	/* pmap_activate */
   #define CD_KMAP		0x0000008	/* pmap_expand_kmap */
   #define CD_MAP		0x0000010	/* pmap_map */
   #define CD_MAPB		0x0000020	/* pmap_map_batc */
   #define CD_CACHE		0x0000040	/* pmap_cache_ctrl */
   #define CD_BOOT		0x0000080	/* pmap_bootstrap */
   #define CD_INIT		0x0000100	/* pmap_init */
   #define CD_CREAT		0x0000200	/* pmap_create */
   #define CD_FREE		0x0000400	/* pmap_free_tables */
   #define CD_DESTR		0x0000800	/* pmap_destroy */
   #define CD_RM		0x0001000	/* pmap_remove */
   #define CD_RMAL		0x0002000	/* pmap_remove_all */
   #define CD_COW		0x0004000	/* pmap_copy_on_write */
   #define CD_PROT		0x0008000	/* pmap_protect */
   #define CD_EXP		0x0010000	/* pmap_expand */
   #define CD_ENT		0x0020000	/* pmap_enter */
   #define CD_UPD		0x0040000	/* pmap_update */
   #define CD_COL		0x0080000	/* pmap_collect */
   #define CD_CMOD		0x0100000	/* pmap_clear_modify */
   #define CD_IMOD		0x0200000	/* pmap_is_modified */
   #define CD_CREF		0x0400000	/* pmap_clear_reference */
   #define CD_PGMV		0x0800000	/* pagemove */
   #define CD_CHKPV		0x1000000	/* check_pv_list */
   #define CD_CHKPM		0x2000000	/* check_pmap_consistency */
   #define CD_CHKM		0x4000000	/* check_map */
   #define CD_ALL		0x0FFFFFC
d334 1
a334 1
void flush_atc_entry __P((long, vm_offset_t, int));
d360 1
a360 1
flush_atc_entry(long users, vm_offset_t va, int kernel)
d579 1
d582 1
d594 1
a594 1
				panic ("pmap_map: Cannot allocate pte table");
d682 1
d685 1
d718 1
a718 1
			for ( i = 0; i < max_cpus; i++)
d745 1
a745 1
				panic ("pmap_map_batc: Cannot allocate pte table");
d802 1
a802 1
	int		kflush;
d817 2
a818 1
	if (pmap == PMAP_NULL) {
d820 1
a820 1
	}
d826 1
a826 1
		kflush = 1;
d828 1
a828 1
		kflush = 0;
d938 2
a939 1
	if (!PAGE_ALIGNED(load_start)) {
d941 1
a941 1
	}
d963 1
a963 1
		      (trunc_page((unsigned)&kernelstart) - load_start);
d1041 1
a1041 1
			       trunc_page((unsigned)&kernelstart));
d1102 1
a1102 1
#if defined(MVME187) || defined (MVME197)
d1135 1
a1135 1
#endif /* defined(MVME187) || defined (MVME197) */
d1172 1
a1172 1
	for (  ; ptable->size != 0xffffffffU; ptable++)
d1366 1
a1366 1
	pv_head_table =  (pv_entry_t)addr;
d1516 3
a1518 4
	if (!PAGE_ALIGNED(p->sdt_paddr)) {
		printf("pmap_create: std table = %x\n",(int)p->sdt_paddr);
		panic("pmap_create: sdt_table not aligned on page boundary");
	}
d1623 2
a1624 1
	if ( j < 1024 )	j++;
d1627 1
a1627 1
	for ( ; i < j; i++) {
d1700 2
a1701 1
	if (p == kernel_pmap) {
d1703 1
a1703 1
	}
d1801 8
a1808 13
	int			pfn;
	int			num_removed = 0;
	int			num_unwired = 0;
	pt_entry_t		*pte;
	pv_entry_t		prev, cur;
	pv_entry_t		pvl;
	vm_offset_t		pa, va, tva;
	register unsigned	users;
	register pte_template_t	opte;
	int			kflush;

	if (e < s)
		panic("pmap_remove_range: end < start");
d1811 1
a1811 1
	 * Pmap has been locked by pmap_remove.
d1815 1
a1815 1
		kflush = 1;
d1817 1
a1817 1
		kflush = 0;
d1849 4
a1852 2
		num_removed++;

d1854 1
a1854 1
			num_unwired++;
d1868 2
a1869 1
			if (pvl->pmap == PMAP_NULL) {
d1871 1
a1871 1
			}
a1913 1
		tva = va;
d1920 1
a1920 1
		flush_atc_entry(users, tva, kflush);
d1934 1
a1934 8
	} /* end for ( va = s; ...) */

	/*
	 * Update the counts
	 */
	pmap->stats.resident_count -= num_removed;
	pmap->stats.wired_count -= num_unwired;

d1974 1
d1977 1
d2026 11
a2036 9
	pv_entry_t		pvl, cur;
	register pt_entry_t	*pte;
	register vm_offset_t	va;
	register pmap_t		pmap;
	int			spl;
	int			dbgcnt = 0;
	register unsigned	users;
	register pte_template_t	opte;
	int			kflush;
d2070 1
a2070 1
			kflush = 1;
d2072 1
a2072 1
			kflush = 0;
d2082 1
d2085 1
d2093 1
a2093 1
			panic("pmap_remove_all: removing  a wired page");
d2128 1
d2130 1
a2135 1

d2166 6
a2171 6
	register pv_entry_t  pv_e;
	register pt_entry_t  *pte;
	int                  spl;
	register unsigned    users;
	register pte_template_t      opte;
	int                  kflush;
d2215 1
a2215 1
			kflush = 1;
d2217 1
a2217 1
			kflush = 0;
d2283 8
a2290 8
	pte_template_t		maprot;
	unsigned		ap;
	int			spl;
	pt_entry_t		*pte;
	vm_offset_t		va;
	register unsigned	users;
	register pte_template_t	opte;
	int			kflush;
d2299 1
d2302 1
d2311 1
a2311 1
		kflush = 1;
d2313 1
a2313 1
		kflush = 0;
d2393 1
d2415 2
a2416 1
	if (map == PMAP_NULL) {
d2418 1
a2418 1
	}
a2426 15
	/*
	 * Handle kernel pmap in pmap_expand_kmap().
	 */
	if (map == kernel_pmap) {
		PMAP_LOCK(map, spl);
		if (pmap_expand_kmap(v, VM_PROT_READ|VM_PROT_WRITE) == PT_ENTRY_NULL)
			panic ("pmap_expand: Cannot allocate kernel pte table");
		PMAP_UNLOCK(map, spl);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_EXP | CD_FULL)) == (CD_EXP | CD_FULL))
			printf("(pmap_expand :%x) kernel_pmap\n", curproc);
#endif
		return;
	}

d2562 2
a2563 2
	register unsigned	users;
	register pte_template_t	opte;
d2565 1
a2565 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d2567 2
a2568 2
	CHECK_PAGE_ALIGN (va, "pmap_entry - VA");
	CHECK_PAGE_ALIGN (pa, "pmap_entry - PA");
d2577 1
a2577 1
			printf ("(pmap_enter :%x) pmap kernel va %x pa %x\n", curproc, va, pa);
d2579 1
a2579 1
			printf ("(pmap_enter :%x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
d2582 1
a2582 1
	ap = m88k_protection (pmap, prot);
a2591 1
Retry:
d2596 1
d2601 12
a2612 6
		/*
		 * Must unlock to expand the pmap.
		 */
		PMAP_UNLOCK(pmap, spl);
		pmap_expand(pmap, va);
		PMAP_LOCK(pmap, spl);
d2621 1
a2621 1
			kflush = 1;
d2623 1
a2623 1
			kflush = 0;
d2679 1
a2679 1
				flush_atc_entry(users, va, 1);
a2727 1
					PMAP_UNLOCK(pmap, spl);
d2799 1
a2799 1
		panic ("pmap_unwire: pte missing");
d2845 1
d2848 1
d2965 3
d2969 1
a2969 8

	if (pmap == PMAP_NULL) {
		panic("pmap_collect: pmap is NULL");
	}
	if (pmap == kernel_pmap) {
#ifdef MACH_KERNEL
		return;
#else
a2971 1
	}
d2993 2
a2994 1
	if ( j < 1024 )	j++;
d2997 1
a2997 1
	for ( ; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE) {
d3000 1
a3000 1
		gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va);
d3026 1
a3026 1
		pmap_remove_range (pmap, (vm_offset_t)sdt_va,(vm_offset_t)sdt_vt);
d3031 1
a3031 1
		for (sdt = sdtp; sdt < (sdtp+PDT_TABLE_GROUP_SIZE); sdt++) {
d3222 1
a3222 1
	aprot = m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
d3285 10
a3294 9
	pv_entry_t    pvl;
	pv_entry_t    pvep;
	pt_entry_t    *pte;
	pmap_t     pmap;
	int        spl;
	vm_offset_t      va;
	unsigned      users;
	pte_template_t   opte;
	int        kflush;
a3295 1
	boolean_t	ret;
d3300 1
a3300 1
	if (!PMAP_MANAGED(phys)) {
a3301 1
	}
d3308 1
a3308 1
	CHECK_PV_LIST (phys, pvl, "pmap_clear_modify");
d3334 1
a3334 1
			kflush = 1;
d3336 1
a3336 1
			kflush = 0;
d3404 1
a3404 1
	if (!PMAP_MANAGED(phys)) {
a3405 1
	}
d3414 1
a3414 1
	if ((boolean_t) PA_TO_ATTRIB(phys)) {
d3508 1
a3508 1
	int		kflush;
d3515 1
a3515 1
	if (!PMAP_MANAGED(phys)) {
a3516 1
	}
d3549 1
a3549 1
			kflush = 1;
d3551 1
a3551 1
			kflush = 0;
d3712 6
a3717 6
	vm_offset_t      pa;
	pt_entry_t    *srcpte, *dstpte;
	pv_entry_t    pvl;
	int        spl;
	unsigned      users;
	pte_template_t   opte;
a3723 1

d3727 2
a3728 3
		if ((srcpte = pmap_pte(kernel_pmap, (vm_offset_t)from)) == PT_ENTRY_NULL) {
			printf("pagemove: source vaddr 0x%x\n", from);
			panic("pagemove: source addr not mapped");
d3734 3
a3736 3
		if ((dstpte = pmap_pte(kernel_pmap, (vm_offset_t)to)) == PT_ENTRY_NULL)
			if ((dstpte = pmap_expand_kmap((vm_offset_t)to, VM_PROT_READ | VM_PROT_WRITE))
			    == PT_ENTRY_NULL)
d3738 3
a3740 3
			/*
			 *
			 */
d3742 2
a3743 2
			printf("pagemove: destination vaddr 0x%x, pte = 0x%x\n", to, *((unsigned *)dstpte));
			panic("pagemove: destination pte already valid");
d3762 1
a3762 1
			pvl->va = (vm_offset_t)to;
d3773 1
a3773 1
		flush_atc_entry(users, from, 1);
d3788 3
a3790 3
	int     i;
	int     ncpus;
	void    (*cfunc)(int cpu, vm_offset_t physaddr, int size);
d3911 1
a3911 1
			pa = (vm_offset_t)0;
d3948 1
a3948 1
 *	This function sequences through the given range of  addresses. For
d4407 1
a4407 1
			panic ("pmap_kenter_pa: Cannot allocate kernel pte table");
d4466 1
a4466 2
		flush_atc_entry(users, va, 1);

@


1.49
log
@Use PMAP_MANAGED() macro everywhere instead of hand-expanding it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2001/11/28 15:34:16 art Exp $	*/
d4394 44
a4437 1
	pmap_enter(pmap_kernel(), va, pa, prot, VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
d4443 14
d4458 22
a4479 1
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
d4481 1
@


1.48
log
@Make pmap_update functions into nops so that we can have a consistent
pmap_update API (right now it's nop).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2001/11/28 14:13:06 art Exp $	*/
d1922 1
a1922 1
			if (vm_physseg_find(atop(pa), NULL) != -1) {
d1927 3
a1930 3
			/* keep track ourselves too */
			if (PMAP_MANAGED(pa))
				SET_ATTRIB(pa, 1);
@


1.47
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2001/11/28 13:47:38 art Exp $	*/
a2910 34


/*
 * Routine:	PMAP_UPDATE
 *
 * Function:
 *	Require that all active physical maps contain no incorrect entries
 *	NOW. [This update includes forcing updates of any address map
 *	cashing]
 *	Generally used to ensure that thread about to run will see a
 *	semantically correct world.
 *
 * Parameters:
 *	none
 *
 * Call:
 *	cmmuflush
 *
 *	The 88200 pmap implementation does not defer any operations.
 * Therefore, the translation table trees are always consistent while the
 * pmap lock is not held. Therefore, there is really no work to do in
 * this function other than to flush the TLB.
 */
void
pmap_update(void)
{
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_UPD | CD_FULL)) == (CD_UPD | CD_FULL))
		printf("(pmap_update :%x) Called \n", curproc);
#endif

}/* pmap_update() */


@


1.46
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2001/11/27 05:39:02 miod Exp $	*/
a4428 12
}

void
pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs)
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	}
@


1.45
log
@Use pools for pmap and pv_entry structures.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2001/11/20 19:54:28 miod Exp $	*/
d2772 1
a2772 1
	return (KERN_SUCCESS);
@


1.44
log
@All sensitive pmap operations should operate at splvm, NOT splimp.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2001/11/07 22:32:29 miod Exp $	*/
a48 1
/*#define DEBUG 1*/
d55 1
d127 2
d1396 6
d1476 1
a1476 2
	p = (struct pmap *)malloc(sizeof(*p), M_VMPMAP, M_WAITOK);

d1709 1
a1709 1
		free((caddr_t)p, M_VMPMAP);
d1881 1
a1881 1
					free((caddr_t)cur, M_VMPVENT);
d1900 1
a1900 1
				free((caddr_t)cur, M_VMPVENT);
d2099 1
a2099 1
			free((caddr_t)cur, M_VMPVENT);
d2733 1
a2733 3
					pv_e = (pv_entry_t) malloc(sizeof *pv_e,
								   M_VMPVENT,
								   M_NOWAIT);
d2770 1
a2770 1
		free((caddr_t) pv_e, M_VMPVENT);
@


1.43
log
@Let those compile.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2001/11/06 19:53:15 miod Exp $	*/
d62 1
d278 12
a289 10
#define	SPLVM(spl)	{ spl = splvm(); }
#define	SPLX(spl)	{ splx(spl); }
#define PMAP_LOCK(pmap,spl) { \
	SPLVM(spl); \
	simple_lock(&(pmap)->lock); \
}
#define PMAP_UNLOCK(pmap, spl) { \
	simple_unlock(&(pmap)->lock); \
	SPLX(spl); \
}
d796 1
a796 1
	int		spl, spl_sav;
a840 1
		spl_sav = splimp();
a843 1
		splx(spl_sav);
d1426 1
a1426 1
	unsigned int	spl_sav;
d1438 1
a1438 1
	spl_sav = splimp();
d1441 1
a1441 1
	splx(spl_sav);
d2160 1
a2160 1
	int                  spl, spl_sav;
a2229 1
		spl_sav = splimp();
a2233 1
		splx(spl_sav);
d2277 1
a2277 1
	int			spl, spl_sav;
a2342 1
		spl_sav = splimp();
a2346 1
		splx(spl_sav);
d2559 1
a2559 1
	int		spl, spl_sav;
a2648 1
			spl_sav = splimp();
a2652 1
			splx(spl_sav);
d3249 1
a3249 1
	unsigned int spl_sav;
d3270 1
a3270 1
	spl_sav = splimp();
d3281 1
a3281 1
	splx(spl_sav);
d3325 1
a3325 1
	int        spl, spl_sav;
a3383 1
		spl_sav = splimp();
a3388 1
		splx(spl_sav);
d3542 1
a3542 1
	int		spl, spl_sav;
a3600 1
		spl_sav = splimp();
a3605 1
		splx(spl_sav);
@


1.42
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2001/09/19 20:50:57 mickey Exp $	*/
d188 1
a188 1
static struct simplelock *pv_lock_table; /* array */
d190 1
a190 1
static pv_entry_t pv_head_table; /* array of entries, one per page */
d305 2
a306 2
static void check_pv_list __P((vm_offset_t, pv_entry_t, char *));
static void check_pmap_consistency __P((char *));
d340 2
d344 1
a344 1
 * Rooutine:	FLUSH_ATC_ENTRY
d1191 1
a1191 1
	virt += ((n)*NBPG); \
d1196 1
a1196 3
	SYSMAP(caddr_t, vmpte , vmmap, 1);
	SYSMAP(struct msgbuf *, msgbufmap ,msgbufp, btoc(MSGBUFSIZE));

d1199 2
@


1.41
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2001/08/26 14:31:12 miod Exp $	*/
a58 1
#include <vm/vm.h>
@


1.40
log
@Add prototypes, fix compilation warnings, random style fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2001/08/12 19:28:38 miod Exp $	*/
a59 1
#include <vm/vm_kern.h>			/* vm/vm_kern.h */
@


1.39
log
@Replace VM_{MAX,MIN}_USER_ADDRESS with VM_{MAX,MIN}_ADDRESS.
Change a while() loop with a clearer for() loop.
Fix pmap_page_protect to correctly handle the VM_PROT_READ|VM_PROT_WRITE
case.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2001/08/12 00:17:45 miod Exp $	*/
d69 1
d77 1
a77 1
extern vm_offset_t      avail_start, avail_next, avail_end;
d334 9
d561 1
a561 1
	static m197_atc_initialized = FALSE;
a636 1
 *	cmmu_store
a1279 1
	avail_next = *phys_start;
@


1.38
log
@Remove asserts, there is enough diag code there.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2001/07/25 13:25:32 art Exp $	*/
d1600 1
a1600 1
	  because of integer overflow/wraparound if VM_MAX_USER_ADDRESS 
d1603 2
a1604 2
	i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
d3021 1
a3021 1
	  because of integer overflow/wraparound if VM_MAX_USER_ADDRESS 
d3025 2
a3026 2
	i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
d3031 1
a3031 1
		sdt_va = VM_MIN_USER_ADDRESS + PDT_TABLE_GROUP_VA_SPACE*i;
d3054 1
a3054 1
		sdt_vt = sdt_va <= VM_MAX_USER_ADDRESS-PDT_TABLE_GROUP_VA_SPACE ?
d3056 1
a3056 1
			 VM_MAX_USER_ADDRESS;
d3359 1
a3359 2
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
a3389 1
		pvep = pvep->next;
d3715 1
d4165 1
a4165 1
		check_map(p, VM_MIN_USER_ADDRESS, VM_MAX_USER_ADDRESS, who);
@


1.37
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2001/07/22 19:58:17 miod Exp $	*/
a63 1
#include <machine/assert.h>
a1042 2
	assert(vaddr == trunc_page((unsigned)&kernelstart));

a4359 2
	assert((range->start <= start) && (start <= range->end));

@


1.36
log
@pmap_clear_modify returns boolean_t
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2001/07/18 10:47:04 art Exp $	*/
d2551 1
a2551 1
void
d2553 2
a2554 2
	   vm_prot_t prot, boolean_t wired,
	   vm_prot_t access_type)
d2566 1
a2566 4

	if (pmap == PMAP_NULL) {
		panic("pmap_enter: pmap is NULL");
	}
d2767 1
d4430 1
a4430 1
	pmap_enter(pmap_kernel(), va, pa, prot, 1, VM_PROT_READ|VM_PROT_WRITE);
d4440 2
a4441 2
			VM_PROT_READ|VM_PROT_WRITE, 1,
			VM_PROT_READ|VM_PROT_WRITE);
@


1.35
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2001/07/05 07:20:45 art Exp $	*/
d3320 1
a3320 1
void
@


1.34
log
@Don't use some macros from vm_page.h.
This was the only user and I want to clean those up.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2001/06/30 12:14:43 miod Exp $	*/
d1455 2
a1456 2
pmap_t
pmap_create(vm_size_t size)
d1458 1
a1458 7
	pmap_t      p;

	/*
	 * A software use-only map doesn't even need a map.
	 */
	if (size != 0)
		return (PMAP_NULL);
d1462 1
a1462 1
	p = (pmap_t)malloc(sizeof(*p), M_VMPMAP, M_WAITOK);
d3299 1
a3299 1
 *		phys	physical address of page
d3314 1
a3314 1
 *	For managed pages, the modify_list entry corresponding to the
d3321 1
a3321 1
pmap_clear_modify(vm_offset_t phys)
d3332 4
d3337 1
d3339 2
a3340 3
#ifdef DEBUG
		if (pmap_con_dbg & CD_CMOD)
			printf("(pmap_clear_modify :%x) phys addr 0x%x not managed \n", curproc, phys);
a3341 2
		return;
	}
d3360 1
a3360 1
		return;
d3400 2
d3413 1
a3413 1
 *		phys		physical address og a page
a3425 3
 *	If the physical address specified is not a managed page, this
 * routine simply returns TRUE (looks like it is returning FALSE XXX).
 *
d3437 1
a3437 1
pmap_is_modified(vm_offset_t phys)
d3444 1
d3446 1
d3448 2
a3449 3
#ifdef DEBUG
		if (pmap_con_dbg & CD_IMOD)
			printf("(pmap_is_modified :%x) phys addr 0x%x not managed\n", curproc, phys);
a3450 2
		return (FALSE);
	}
d3523 1
a3523 1
 *		phys		physical address of page
a3536 1
 *	For managed pages, the coressponding PV list will be traversed.
d3541 2
a3542 2
void
pmap_clear_reference(vm_offset_t phys)
d3544 13
a3556 9
	pv_entry_t    pvl;
	pv_entry_t    pvep;
	pt_entry_t    *pte;
	pmap_t     pmap;
	int        spl, spl_sav;
	vm_offset_t      va;
	unsigned      users;
	pte_template_t   opte;
	int        kflush;
d3558 1
d3560 2
a3561 4
#ifdef DEBUG
		if (pmap_con_dbg & CD_CREF) {
			printf("(pmap_clear_reference :%x) phys addr 0x%x not managed\n", curproc,phys);
		}
d3563 1
a3563 2
		return;
	}
d3580 1
a3580 1
		return;
d3620 2
d3632 1
a3632 1
 *	phys		physical address of a page
d3645 1
a3645 4
 *	If the physical address specified is not a managed page, this
 * routine simply returns TRUE.
 *
 *	Otherwise, this routine walks the PV list corresponding to the
d3652 1
a3652 1
pmap_is_referenced(vm_offset_t phys)
d3658 1
d3660 1
d3662 2
a3663 1
		return (FALSE);
d3713 1
a3713 1
pmap_page_protect(vm_offset_t phys, vm_prot_t prot)
d3715 2
d4428 26
@


1.33
log
@Repair 197 support (oops)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2001/06/27 06:19:51 art Exp $	*/
a1767 1
 *	vm_page_set_modified
d1780 3
a1782 4
 * 'modified' bit, if on, is referenced to the VM through the
 * 'vm_page_set_modified' macro, and into the appropriate entry in the
 * pmap_modify_list. Next, the function must find the PV list entry
 * associated with this pmap/va (if it doesn't exist - the function
d1915 5
a1919 3
			if (IS_VM_PHYSADDR(pa)) {
				vm_page_set_modified(
				    PHYS_TO_VM_PAGE(opte.bits & ~PAGE_MASK));
a2003 1
 *	vm_page_set_modified
d2107 4
a2110 1
			vm_page_set_modified((vm_page_t)PHYS_TO_VM_PAGE(phys));
@


1.32
log
@MNN is no longer an option.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2001/06/27 04:29:20 art Exp $	*/
d552 1
a552 1
	static mvme197_atc_initialized = FALSE;
@


1.31
log
@rip old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2001/06/16 22:31:50 miod Exp $	*/
a193 24
#if !defined(MACHINE_NEW_NONCONTIG)
/*
 * First and last physical address that we maintain any information
 * for. Initialized to zero so that pmap operations done before
 * pmap_init won't touch any non-existent structures.
 */
static vm_offset_t   pmap_phys_start   = (vm_offset_t) 0;
static vm_offset_t   pmap_phys_end  = (vm_offset_t) 0;

/*
 * Index into pv_head table, its lock bits, and the modify bits
 * starting at pmap_phys_start.
 */
#define PFIDX(pa)		(atop(pa - pmap_phys_start))
#define PFIDX_TO_PVH(pfidx)	(&pv_head_table[pfidx])
#define	PA_TO_PVH(pa)		(&pv_head_table[PFIDX(pa)])
#define PMAP_MANAGED(pa)	(pmap_initialized && \
	((pa) >= pmap_phys_start && (pa) < pmap_phys_end))
#define LOCK_PVH(pa)		simple_lock(&(pv_lock_table[PFIDX(pa)]))
#define UNLOCK_PVH(pa)		simple_unlock(&(pv_lock_table[PFIDX(pa)]))
#define	PA_TO_ATTRIB(pa)	(pmap_modify_list[PFIDX(pa)])
#define	SET_ATTRIB(pa, attr)	(pmap_modify_list[PFIDX(pa)] = (attr))

#else
a231 1
#endif /* !defined(MACHINE_NEW_NONCONTIG) */
a1276 34
/*
 * Bootstrap memory allocator. This function allows for early dynamic
 * memory allocation until the virtual memory system has been bootstrapped.
 * After that point, either uvm_km_zalloc or malloc should be used. This
 * function works by stealing pages from the (to be) managed page pool,
 * stealing virtual address space, then mapping the pages and zeroing them.
 *
 * It should be used from pmap_bootstrap till vm_page_startup, afterwards
 * it cannot be used, and will generate a panic if tried. Note that this
 * memory will never be freed, and in essence it is wired down.
 */

#if !defined(MACHINE_NEW_NONCONTIG)
void *
pmap_bootstrap_alloc(int size)
{
	register void *mem;

	size = round_page(size);
	mem = (void *)virtual_avail;
	virtual_avail = pmap_map(virtual_avail, avail_start,
				 avail_start + size,
				 VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
	avail_start += size;
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("pmap_bootstrap_alloc: size %x virtual_avail %x avail_start %x\n",
		       size, virtual_avail, avail_start);
	}
#endif
	bzero((void *)mem, size);
	return (mem);
}
#endif /* !defined(MACHINE_NEW_NONCONTIG) */
a1316 1
#ifdef MACHINE_NEW_NONCONTIG
a1390 66
#else
void
pmap_init(vm_offset_t phys_start, vm_offset_t phys_end)
{
	register long		npages;
	register vm_offset_t	addr;
	register vm_size_t	s;
	register int		i;
	vm_size_t		pvl_table_size;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
		printf("(pmap_init) phys_start %x  phys_end %x\n", phys_start, phys_end);
#endif

	/*
	 * Allocate memory for the pv_head_table and its lock bits,
	 * the modify bit array, and the pte_page table.
	 */
	npages = atop(phys_end - phys_start);
	s = PV_TABLE_SIZE(npages);		/* pv_list */
	s += PV_LOCK_TABLE_SIZE(npages);	/* pv_lock_table */
	s += npages * sizeof(char);		/* pmap_modify_list */

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
		printf("(pmap_init) nbr of managed pages = %x\n", npages);
		printf("(pmap_init) size of pv_list = %x\n",
		       npages * sizeof(struct pv_entry));
	}
#endif
	s = round_page(s);
	addr = (vm_offset_t)kmem_alloc(kernel_map, s);

	pv_head_table =  (pv_entry_t)addr;
	addr += PV_TABLE_SIZE(npages);

	/*
	 * Assume that 'simple_lock' is used to lock pv_lock_table
	 */
	pv_lock_table = (struct simplelock *)addr; /* XXX */
	addr += PV_LOCK_TABLE_SIZE(npages);

	pmap_modify_list = (char *)addr;

	/*
	* Initialize pv_lock_table
	*/
	for (i = 0; i < npages; i++)
		simple_lock_init(&(pv_lock_table[i]));

	/*
	  * Only now, when all of the data structures are allocated,
	  * can we set pmap_phys_start and pmap_phys_end. If we set them
	  * too soon, the kmem_alloc above will blow up when it causes
	  * a call to pmap_enter, and pmap_enter tries to manipulate the
	  * (not yet existing) pv_list.
	  */
	pmap_phys_start = phys_start;
	pmap_phys_end = phys_end;

	pmap_initialized = TRUE;

} /* pmap_init() */
#endif 

a4013 1
#if defined(MACHINE_NEW_NONCONTIG)
a4015 1
#endif 
a4061 1
#if defined(MACHINE_NEW_NONCONTIG)
a4066 5
#else
		if (map == kernel_pmap &&
		    va < round_page((vm_offset_t)(pmap_modify_list + (pmap_phys_end - pmap_phys_start))))
			continue;
#endif 
a4145 1
#ifdef MACHINE_NEW_NONCONTIG
d4148 1
a4148 1
#endif 
a4173 1
#if defined(MACHINE_NEW_NONCONTIG)
a4179 6
#else
	for (phys = pmap_phys_start; phys < pmap_phys_end; phys += PAGE_SIZE) {
		pv_h = PA_TO_PVH(phys);
		check_pv_list(phys, pv_h, who);
	}
#endif /* defined(MACHINE_NEW_NONCONTIG) */
a4218 21

#if !defined(MACHINE_NEW_NONCONTIG)

unsigned int
pmap_free_pages(void)
{
	return atop(avail_end - avail_next);
}

boolean_t
pmap_next_page(vm_offset_t *addrp)
{
	if (avail_next == avail_end)
		return FALSE;

	*addrp = avail_next;
	avail_next += PAGE_SIZE;
	return TRUE;
}

#endif
@


1.30
log
@More cleanink of unused code and incorrect comments.
Replace a ton of english typos with (fewer) miodish typos, too...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/06/14 21:30:46 miod Exp $	*/
a60 1
#if defined(UVM)
a61 1
#endif
a1412 1
#if defined(UVM)
a1413 3
#else
	addr = (vm_offset_t)kmem_alloc(kernel_map, s);
#endif
a1620 1
#if defined(UVM)
a1621 3
#else
	segdt = (sdt_entry_t *)kmem_alloc(kernel_map, s);
#endif
a1624 12
#if !defined(UVM)
	/* uvm_km_zalloc gives zeroed memory */
	/* use pmap zero page to zero it out */
	addr = (vm_offset_t)segdt;
	for (i=0; i<atop(s); i++) {
		paddr_t pa;

		pmap_extract(kernel_pmap, addr, &pa);
		pmap_zero_page(pa);
		addr += PAGE_SIZE;
	}
#endif
a1762 1
#if defined(UVM)
a1763 4
#else
	kmem_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
#endif

a2558 1
#if defined(UVM)
a2559 3
#else
	pdt_vaddr = kmem_alloc (kernel_map, PAGE_SIZE);
#endif
a2579 1
#if defined(UVM)
a2580 3
#else
		kmem_free (kernel_map, pdt_vaddr, PAGE_SIZE);
#endif
@


1.29
log
@Big cleanup of VM issues:
o get rid of m88k_foo macros when there is an mi foo macro
o remove the ability, for the pmap module, to handle a native mmu page
  size different from the vm module page size. This allows some
  optimizations in pmap.c
o remove dead stuff from <machine/vmparam.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2001/06/08 08:09:14 art Exp $	*/
d68 1
a81 7
extern vm_offset_t   pcc2consvaddr;
extern vm_offset_t   clconsvaddr;

extern void *iomapbase;
int iiomapsize;

extern int max_cpus;
d88 1
d92 1
a92 3
#ifdef	DEBUG
   #define	STATIC
   #define	DBG
a95 1

d126 1
a127 4
#else

   #define	STATIC		static

d143 1
a143 1
STATIC kpdt_entry_t     kpdt_free;
d177 1
a177 1
 * Used in copy_to_phys(), pmap_copy_page() and pmap_zero_page().
a180 2
#define PMAP_MAX	512

a323 5
/*
 *	This variable extract vax's pmap.c.
 *	pmap_init initialize this.
 *	'90.7.17	Fuzzy
 */
a363 2
 * Author:	N. Sugai
 *
d366 2
a367 1
 *	are spcified by 'users', using the function 'flush'.
d373 1
a373 6
 *	flush	pointer to the function actually flushes the TLB
 *
 * Special Assumptions:
 *	(*flush)() has two arguments, 1st one specifies the CPU number,
 *	and 2nd one specifies the virtual address should be flushed.
 *	
a395 1
 *  Author: Joe Uemura
a396 7
 *
 *	History:
 *	'90.8.3	Fuzzy
 *			if defined TEST, 'static' undeclared.
 *	'90.8.30	Fuzzy
 *			delete "if defined TEST, 'static' undeclared."
 *
d399 1
a399 1
STATIC unsigned int
d438 1
d443 1
d465 1
a465 1
 * kmem_alloc. (Obviously, because kmem_alloc uses the kernel map
d493 1
a493 1
STATIC pt_entry_t *
d535 4
a538 4
 *	virt	virtual address of range to map    (IN)
 *	start	physical address of range to map   (IN)
 *	end	physical address of end of range   (IN)
 *	prot	protection attributes              (IN)
d565 3
a567 1
void m197_load_patc(int, vm_offset_t, vm_offset_t, int);
d579 1
a579 1
	static unsigned	i = 0;
d618 4
a621 2
		if (cputyp == CPU_197) {
			if (i < 32) {
d624 1
a624 2
				i++;
			}
d643 5
a647 5
 *	virt	virtual address of range to map    (IN)
 *	start	physical address of range to map   (IN)
 *	end	physical address of end of range   (IN)
 *	prot	protection attributes              (IN)
 *	cmode	cache control attributes	   (IN)
d650 1
a650 1
 *	batc_used	number of BATC used	(IN/OUT)
a658 1
 *
d821 1
a821 1
	if ( mode & CACHE_MASK ) {
d830 1
a830 1
	if ( pmap == PMAP_NULL ) {
a835 3
	/*
	 * 
	 */
d881 1
a881 1
 *	Bootstarp the system enough to run with virtual memory.
d887 3
a889 3
 *	load_start	PA where kernel was loaded (IN)
 *	&phys_start	PA of first available physical page (IN/OUT)
 *	&phys_end	PA of last available physical page (IN)
d895 5
a899 5
 *	PAGE_SIZE	VM (software) page size (IN)
 *	kernelstart	start symbol of kernel text (IN)
 *	etext		end of kernel text (IN)
 *	phys_map_vaddr1 VA of page mapped arbitrarily for debug/IO (OUT)
 *	phys_map_vaddr2 VA of page mapped arbitrarily for debug/IO (OUT)
d909 1
a909 1
 * following the end of the kernel code/data, sufficent page of
d926 5
a930 5
pmap_bootstrap(vm_offset_t load_start, /* IN */
	       vm_offset_t   *phys_start,   /* IN/OUT */
	       vm_offset_t   *phys_end,	 /* IN */
	       vm_offset_t   *virt_start,   /* OUT */
	       vm_offset_t   *virt_end)	 /* OUT */
d1040 1
a1040 1
	for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i>0; i--) {
d1202 1
a1202 1
		 * This means that if different physcal pages are going to be mapped
a1254 1
		void show_apr(unsigned value);
a1301 1
	return;
d1307 1
a1307 1
 * After that point, either kmem_alloc or malloc should be used. This
d1352 6
a1357 6
 *	pv_head_table (OUT)
 *	pv_lock_table (OUT)
 *	pmap_modify_list (OUT)
 *	pmap_phys_start (OUT)
 *	pmap_phys_end (OUT)
 *	pmap_initialized(OUT)
d1360 1
a1360 2
 *	kmem_alloc
 *	zinit
d1374 2
a1375 4
 *	kmem_alloc() memory for pv_table
 * 	kmem_alloc() memory for modify_bits
 *	zinit(pmap_zone)
 *	zinit(segment zone)
a1525 10
 * History:
 *	'90.7.13	Fuzzy
 *	'90.9.05	Fuzzy
 *		Bug: template page invalid --> template page valid
 *
 *	template = trunc_page(phys)
 *	   | m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
 *	   | DT_VALID;
 *	     ^^^^^^^^ add
 *
d1527 1
a1527 1
 *	Zeros the specified (machine independent) page.
a1535 1
 *	trunc_page
d1543 1
a1543 1
 *	This routine maps the physical pages ath the 'phys_map' virtual
d1553 1
a1553 1
	int		my_cpu;
d1556 2
a1557 2
	my_cpu = cpu_number();
	srcva = (vm_offset_t)(phys_map_vaddr1 + (my_cpu * PAGE_SIZE));
d1570 1
a1570 1
	cmmu_flush_remote_data_cache(my_cpu,phys, PAGE_SIZE);
a1575 13
 * Author:	Fuzzy
 *
 * History:
 *	'90.7.13	Fuzzy	level 1 --> segment exchange
 *	'90.7.16	Fuzzy	PT_ALIGNED --> PAGE_ALIGNED exchange
 *					l1_utemplate	delete
 *	'90.7.20	Fuzzy	kernel segment entries in segment table
 *				entries for user space address delete.
 *				copying kernel segment entries
 *				to user pmap segment entries delete.
 *				all user segment table entries initialize
 *				to zero (invalid).
 *
d1582 1
a1582 1
 * Paramerters:
d1741 1
a1741 1
STATIC void
d1820 1
a1820 1
 *	zfree
d1855 1
a1855 1
		free((caddr_t)p,M_VMPMAP);
a1912 1
 *	ptoa
d1918 1
a1918 1
 *	zfree
d1942 1
a1942 1
STATIC void
d2054 1
a2054 1
		} /* if PAGE_MANAGED */
a2154 1
 *	ptoa
d2159 1
a2159 1
 *	zfree
a2296 1
 *		ptoa
d2306 1
a2306 1
STATIC void
d2318 1
a2318 1
		if (pmap_con_dbg & CD_CMOD)
d2336 1
a2336 3
		UNLOCK_PVH(phys);
		SPLX(spl);
		return;	    /* no mappings */
d2393 1
a2395 1

d2516 1
a2516 1
 *	kmem_alloc, zalloc). Thus it must be called in a unlock/lock loop
d2533 1
a2533 1
 *	zfree
d2551 1
a2551 1
STATIC void
a2651 11
 *
 * Update:
 *	July 13,90 - JUemura
 *		initial porting
 *         *****TO CHECK*****
 *              locks removed since we don't have to allocate
 *		level 2 tables anymore. locks needed?
 *	'90.7.26	Fuzzy	VM_MIN_KERNEL_ADDRESS -> VM_MAX_USER_ADDRESS
 *	'90.8.17	Fuzzy	Debug message added(PV no mapped at VA)
 *	'90.8.31	Sugai	Remove redundant message output
 *
d2678 1
a2678 1
 *	zfree
d2767 1
a2767 3
	 * Expand pmap to include this pte. Assume that
	 * pmap is always expanded to include enough M88K
	 * pages to map one VM page.
a2939 2
 *	Author:		Fuzzy
 *
d2942 1
a2942 1
 *	Prameterts:
d2944 1
a2944 1
 *		v		virtual address of page to be wired/unwired
a2980 2
 * Author:	Fuzzy
 *
a3047 42
  a version for the kernel debugger 
*/

vm_offset_t
pmap_extract_unlocked(pmap_t pmap, vm_offset_t va)
{
	pt_entry_t *pte;
	vm_offset_t   pa;
	int     i;

	if (pmap == PMAP_NULL)
		panic("pmap_extract: pmap is NULL");

	/*
	 * check BATC first
	 */
	if (pmap == kernel_pmap && batc_used > 0)
		for (i = batc_used-1; i > 0; i--)
			if (batc_entry[i].lba == M88K_BTOBLK(va)) {
				pa = (batc_entry[i].pba << BATC_BLKSHIFT) | 
					(va & BATC_BLKMASK );
				return (pa);
			}

	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
		pa = (vm_offset_t) 0;
	else {
		if (PDT_VALID(pte))
			pa = ptoa(pte->pfn);
		else
			pa = (vm_offset_t) 0;
	}

	if (pa)
		pa |= (va & PAGE_MASK); /* offset within page */

	return (pa);

} /* pamp_extract_unlocked() */


/*
d3066 1
a3070 4
#ifdef lint
	dst_pmap++; src_pmap++; dst_addr++; len++; src_addr++;
#endif

d3241 1
a3241 1
		 * calls kmem_free or zfree, which will invoke another 
d3266 1
a3266 1
 *	In a mono-processor implementation the my_cpu
d3293 1
a3293 1
	int my_cpu = cpu_number();  
d3320 1
a3320 1
		cmmu_pmap_activate(my_cpu, apr_data.bits, 
d3339 1
a3339 1
		SETBIT_CPUSET(my_cpu, &(pmap->cpus_using));
d3366 1
a3366 1
 * _pmap_deactive simply clears the cpus_using field in given pmap structure.
d3373 1
a3373 1
	int my_cpu = cpu_number();  
d3381 1
a3381 1
		CLRBIT_CPUSET(my_cpu, &(pmap->cpus_using));
d3390 1
a3390 1
 *		Copies the specified (machine independent) pages.
a3401 1
 *		trunc_page
d3408 1
a3408 1
 *	This routine maps the phsical pages at the 'phys_map' virtual
d3420 1
a3420 1
	int      my_cpu = cpu_number();
d3427 2
a3428 2
	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu_number() * PAGE_SIZE));
	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
d3452 2
a3453 2
	cmmu_flush_remote_data_cache(my_cpu, src, PAGE_SIZE);
	cmmu_flush_remote_data_cache(my_cpu, dst, PAGE_SIZE);
a3457 180
 *	copy_to_phys
 *
 *	Copy virtual memory to physical memory by mapping the physical
 *	memory into virtual memory and then doing a virtual to virtual
 *	copy with bcopy.
 *
 *	Parameters:
 *		srcva		VA of source page
 *		dstpa		PA of destination page
 *		bytecount	copy byte size
 *
 *	Extern/Global:
 *		phys_map_vaddr2
 *
 *	Calls:
 *		m88kprotection
 *		trunc_page
 *		cmmu_sflush_page
 *		bcopy
 *
 */
void
copy_to_phys(vm_offset_t srcva, vm_offset_t dstpa, int bytecount)
{
	vm_offset_t   dstva;
	pt_entry_t *dstpte;
	int     copy_size,
	offset,
	aprot;
	pte_template_t  template;

	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
	dstpte = pmap_pte(kernel_pmap, dstva);
	copy_size = PAGE_SIZE;
	offset = dstpa - trunc_page(dstpa);
	dstpa -= offset;

	aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
	while (bytecount > 0) {
		copy_size = PAGE_SIZE - offset;
		if (copy_size > bytecount)
			copy_size = bytecount;
		/*
		 * Map destination physical address. 
		 */
		template.bits = trunc_page(dstpa) | aprot | CACHE_WT | DT_VALID;
		cmmu_flush_tlb(1, dstva, PAGE_SIZE);
		*dstpte = template.pte;

		dstva += offset;
		bcopy((void*)srcva, (void*)dstva, copy_size);
		srcva += copy_size;
		dstva += copy_size;
		dstpa += PAGE_SIZE;
		bytecount -= copy_size;
		offset = 0;
	}
}

/*
 *	copy_from_phys
 *
 *	Copy physical memory to virtual memory by mapping the physical
 *	memory into virtual memory and then doing a virtual to virtual
 *	copy with bcopy.
 *
 *	Parameters:
 *		srcpa		PA of source page
 *		dstva		VA of destination page
 *		bytecount	copy byte size
 *
 *	Extern/Global:
 *		phys_map_vaddr2
 *
 *	Calls:
 *		m88kprotection
 *		trunc_page
 *		cmmu_sflush_page
 *		bcopy
 *
 */
void
copy_from_phys(vm_offset_t srcpa, vm_offset_t dstva, int bytecount)
{
	register vm_offset_t   srcva;
	register pt_entry_t *srcpte;
	register int     copy_size, offset;
	int             aprot;
	pte_template_t  template;

	srcva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
	srcpte = pmap_pte(kernel_pmap, srcva);
	copy_size = PAGE_SIZE;
	offset = srcpa - trunc_page(srcpa);
	srcpa -= offset;

	aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
	while (bytecount > 0) {
		copy_size = PAGE_SIZE - offset;
		if (copy_size > bytecount)
			copy_size = bytecount;
		/*
		 * Map destination physical address.
		 */
		template.bits = trunc_page(srcpa) | aprot | CACHE_WT | DT_VALID;
		cmmu_flush_tlb(1, srcva, PAGE_SIZE);
		*srcpte = template.pte;

		srcva += offset;
		bcopy((void*)srcva, (void*)dstva, copy_size);
		srcpa += PAGE_SIZE;
		dstva += copy_size;
		srcva += copy_size;
		bytecount -= copy_size;
		offset = 0;
		/* cache flush source? */
	}
}

/*
 * Routine:	PMAP_REDZONE
 *
 * Function:
 *	Give the kernel read-only access to the specified address. This
 *	is used to detect stack overflows. It is assumed that the address
 *	specified is the last possible kernel stack address. Therefore, we
 *	round up to the nearest machine dependent page.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	addr		virtual address of page to which access should
 *			be restricted to read-only
 *
 * Calls:
 *	round_page
 *	PMAP_LOCK
 *	pmap_pte
 *	PDT_VALID
 *
 *	This function calls pmap_pte to obtain a pointer to the page
 * table entry associated with the given virtual address. If there is a
 * page entry, and it is valid, its write protect bit will be set.
 */
void
pmap_redzone(pmap_t pmap, vm_offset_t va)
{
	pt_entry_t    *pte;
	int        spl, spl_sav;
	unsigned      users;
	pte_template_t   opte;
	int        kflush;

	va = round_page(va);
	PMAP_LOCK(pmap, spl);

	users = pmap->cpus_using;
	if (pmap == kernel_pmap) {
		kflush = 1;
	} else {
		kflush = 0;
	}

	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL && PDT_VALID(pte)) {
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by other cpu.
		 */
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		opte.pte.prot = M88K_RO;
		((pte_template_t *)pte)->bits = opte.bits;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);
	}

	PMAP_UNLOCK(pmap, spl);

} /* pmap_redzone() */

/*
d3513 1
a3513 1
	/* update correspoinding pmap_modify_list element */
d3526 1
a3526 1
	/* for each listed pmap, trun off the page modified bit */
d3621 1
a3621 1
	is_mod_Retry:
a3683 6
 *	History:
 *		'90. 7.16	Fuzzy	unchanged
 *		'90. 7.19	Fuzzy	comment "Calls:' add
 *		'90. 8.21	Fuzzy	Debugging message add
 *		'93. 3. 1	jfriedl Added call to LOCK_PVH
 *
d3705 1
a3705 1
 * werw found on, an TLB flush will be performed.
a3788 4
 * History:
 *	'90. 7.16	Fuzzy
 *	'90. 7.19	Fuzzy	comment	'Calls:' add
 *
d3790 1
a3790 1
 *	Retrun whether or not the specifeid physical page is referenced by
d3832 1
a3832 2
	is_ref_Retry:

d3890 1
a3890 1
#if FUTURE_MAYBE
d3899 1
a3899 1
 *	to	kernel virtual address of distination
d3936 1
a3936 1
			panic("pagemove: Source addr not mapped");
d3945 1
a3945 1
				panic("pagemove: Cannot allocate distination pte");
d3950 2
a3951 2
			printf("pagemove: distination vaddr 0x%x, pte = 0x%x\n", to, *((unsigned *)dstpte));
			panic("pagemove: Distination pte already valid");
a3991 32
/*
 *	Routine:	icache_flush
 *
 *	Function:
 *		Invalidate instruction cache for all CPUs on specified
 *		physical address.  Called when a page is removed from a
 *		vm_map.  This is done because the Instruction CMMUs are not
 *		snooped, and if a page is subsequently used as a text page,
 *		we want the CMMUs to re-load the cache for the page.
 *
 *	Parameters:
 *		pa	physical address of the (vm) page
 *
 *	Calls:
 *		cachefall
 *
 *	Called by:
 *		vm_remove_page
 *
 */
void
icache_flush(vm_offset_t pa)
{
	int  cpu = 0;

	for (cpu=0; cpu<max_cpus; cpu++) {
		if (cpu_sets[cpu]) {
			cmmu_flush_remote_inst_cache(cpu, pa, 
						     PAGE_SIZE);
		}
	}
} /* icache_flush */
a3992 17
/*
 *	Routine:	pmap_dcache_flush
 *
 *	Function:
 *		Flush DATA cache on specified virtual address.
 *
 *	Parameters:
 *		pmap	specify pmap
 *		va	virtual address of the (vm) page to be flushed
 *
 *	Extern/globals:
 *		pmap_pte
 *
 *	Calls:
 *		dcacheflush
 *
 */
a3993 19
pmap_dcache_flush(pmap_t pmap, vm_offset_t va)
{
	vm_offset_t pa;
	int     spl;

	if (pmap == PMAP_NULL)
		panic("pmap_dcache_flush: pmap is NULL");

	PMAP_LOCK(pmap, spl);

	pa = ptoa((pmap_pte(pmap, va))->pfn);
	cmmu_flush_data_cache(pa, PAGE_SIZE);

	PMAP_UNLOCK(pmap, spl);


} /* pmap_dcache_flush */

STATIC void
a4046 31
/*
 * pmap_cache_flush
 *	Internal function.
 */
void
pmap_cache_flush(pmap_t pmap, vm_offset_t virt, int bytes, int mode)
{
	vm_offset_t pa;
	vm_offset_t va;
	int     spl;

	if (pmap == PMAP_NULL)
		panic("pmap_cache_flush: NULL pmap");

	/*
	 * If it is more than a couple of pages, just blow the whole cache
	 * because of the number of cycles involved.
	 */
	if (bytes > 2*PAGE_SIZE) {
		cache_flush_loop(mode, 0, -1);
		return;
	}

	PMAP_LOCK(pmap, spl);
	for (va = virt; bytes > 0; bytes -= PAGE_SIZE,va += PAGE_SIZE) {
		pa = ptoa((pmap_pte(pmap, va))->pfn);
		cache_flush_loop(mode, pa, PAGE_SIZE);
	}
	PMAP_UNLOCK(pmap, spl);
} /* pmap_cache_flush */

d4081 1
a4081 1
 * the pmap in question. If the retruned physical address does not match
d4085 1
a4085 1
STATIC void
d4134 1
a4134 1
 *	Routine:	CHECK_MAP (itnernal)
d4159 2
a4160 3
 * searched for the corresponding pmap/va entry. If not found, the
 * function panics. If duplicate PV list entries are found, the function
 * panics.
d4163 1
a4163 1
STATIC void
d4300 2
a4301 3
 *	NOTE: Added by Sugai 10/29/90
 *	There are some pages do not appaer in PV list. These pages are
 * allocated for pv structures by kmem_alloc called in pmap_init.
d4303 1
a4303 1
 * PV maniupulations had not been activated when these pages were alloceted.
d4307 1
a4307 1
STATIC void
d4334 2
a4335 2
	i = PMAP_MAX;
	for (p = kernel_pmap->next;p != kernel_pmap; p = p->next) {
d4337 1
a4337 1
			printf("check_pmap_consistency: pmap strcut loop error.\n");
a4389 211
/*
 *	Routine:	PMAP_PRINT
 *
 *  	History:
 *
 *	Function:
 *		Print pmap stucture, including segment table.
 *
 *	Parameters:
 *		pmap		pointer to pmap structure
 *
 *	Special Assumptions:
 *		No locking required.
 *
 *	This function prints the fields of the pmap structure, then
 * iterates through the segment translation table, printing each entry.
 */
void
pmap_print(pmap_t pmap)
{
	sdt_entry_t   *sdtp;
	sdt_entry_t   *sdtv;
	int     i;

	printf("Pmap @@ 0x%x:\n", (unsigned)pmap);
	sdtp = pmap->sdt_paddr;
	sdtv = pmap->sdt_vaddr;
	printf("	sdt_paddr: 0x%x; sdt_vaddr: 0x%x; ref_count: %d;\n",
	       (unsigned)sdtp, (unsigned)sdtv,
	       pmap->ref_count);

#ifdef	statistics_not_yet_maintained
	printf("	statistics: pagesize %d: free_count %d; "
	       "active_count %d; inactive_count %d; wire_count %d\n",
	       pmap->stats.pagesize,
	       pmap->stats.free_count,
	       pmap->stats.active_count,
	       pmap->stats.inactive_count,
	       pmap->stats.wire_count);

	printf("	zero_fill_count %d; reactiveations %d; "
	       "pageins %d; pageouts %d; faults %d\n",
	       pmap->stats.zero_fill_count,
	       pmap->stats.reactivations,
	       pmap->stats.pageins,
	       pmap->stats.pageouts,
	       pmap->stats.fault);

	printf("	cow_faults %d, lookups %d, hits %d\n",
	       pmap->stats.cow_faults,
	       pmap->stats.loopups,
	       pmap->stats.faults);
#endif
	sdtp = (sdt_entry_t *) pmap->sdt_vaddr;	 /* addr of physical table */
	sdtv = sdtp + SDT_ENTRIES;	/* shadow table with virt address */
	if (sdtp == (sdt_entry_t *)0)
		printf("Error in pmap - sdt_paddr is null.\n");
	else {
		int   count = 0;
		printf("	Segment table at 0x%x (0x%x):\n",
		       (unsigned)sdtp, (unsigned)sdtv);
		for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
			if ((sdtp->table_addr != 0 ) || (sdtv->table_addr != 0)) {
				if (count != 0)
					printf("sdt entry %d skip !!\n", count);
				count = 0;
				printf("   (%x)phys: ", i);
				PRINT_SDT(sdtp);
				printf("   (%x)virt: ", i);
				PRINT_SDT(sdtv);
			} else
				count++;
		}
		if (count != 0)
			printf("sdt entry %d skip !!\n", count);
	}

} /* pmap_print() */

/*
 * Routine:	PMAP_PRINT_TRACE
 *
 * Function:
 *	Using virt addr, derive phys addr, printing pmap tables along the way.
 *
 * Parameters:
 *	pmap		pointer to pmap strucuture
 *	va		virtual address whose translation is to be trace
 *	long_format	flag indicating long from output is desired
 *
 * Special Assumptions:
 *	No locking required.
 *
 *	This function chases down through the translation tree as
 * appropriate for the given virtual address. each table entry
 * encoutered is printed. If the long_format is desired, all entries of
 * each table are printed, with special indication of the entries used in
 * the translation.
 */
void
pmap_print_trace (pmap_t pmap, vm_offset_t va, boolean_t long_format)
{
	sdt_entry_t   *sdtp;   /* ptr to sdt table of physical addresses */
	sdt_entry_t   *sdtv;   /* ptr to sdt shadow table of virtual addresses */
	pt_entry_t *ptep;   /* ptr to pte table of physical page addresses */

	int     i; /* table loop index */
	unsigned long prev_entry; /* keep track of value of previous table entry */
	int     n_dup_entries; /* count contiguous duplicate entries */

	printf("Trace of virtual address 0x%08x. Pmap @@ 0x%08x.\n",
	       va, (unsigned)pmap);

	/*** SDT TABLES ***/
	/* get addrs of sdt tables */
	sdtp = (sdt_entry_t *)pmap->sdt_vaddr;
	sdtv = sdtp + SDT_ENTRIES;

	if (sdtp == SDT_ENTRY_NULL) {
		printf("    Segment table pointer (pmap.sdt_paddr) null, trace stops.\n");
		return;
	}

	n_dup_entries = 0;
	prev_entry = 0xFFFFFFFF;

	if (long_format) {
		printf("    Segment table at 0x%08x (virt shadow at 0x%08x)\n",
		       (unsigned)sdtp, (unsigned)sdtv);
		for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
			if (prev_entry == ((sdt_entry_template_t *)sdtp)->bits
			    && SDTIDX(va) != i && i != SDT_ENTRIES-1) {
				n_dup_entries++;
				continue;  /* suppress duplicate entry */
			}
			if (n_dup_entries != 0) {
				printf("    - %d duplicate entries skipped -\n",n_dup_entries);
				n_dup_entries = 0;
			}
			prev_entry = ((pte_template_t *)sdtp)->bits;
			if (SDTIDX(va) == i) {
				printf("    >> (%x)phys: ", i);
			} else {
				printf("       (%x)phys: ", i);
			}
			PRINT_SDT(sdtp);
			if (SDTIDX(va) == i) {
				printf("    >> (%x)virt: ", i);
			} else {
				printf("       (%x)virt: ", i);
			}
			PRINT_SDT(sdtv);
		} /* for */
	} else {
		/* index into both tables for given VA */
		sdtp += SDTIDX(va);
		sdtv += SDTIDX(va);
		printf("    SDT entry index 0x%x at 0x%x (virt shadow at 0x%x)\n",
		       SDTIDX(va), (unsigned)sdtp, (unsigned)sdtv);
		printf("    phys:  ");
		PRINT_SDT(sdtp);
		printf("    virt:  ");
		PRINT_SDT(sdtv);
	}

	/*** PTE TABLES ***/
	/* get addrs of page (pte) table (no shadow table) */

	sdtp = ((sdt_entry_t *)pmap->sdt_vaddr) + SDTIDX(va);
#ifdef DEBUG
	printf("*** DEBUG (sdtp) ");
	PRINT_SDT(sdtp);
#endif
	sdtv = sdtp + SDT_ENTRIES;
	ptep = (pt_entry_t *)(ptoa(sdtv->table_addr));
	if (sdtp->dtype != DT_VALID) {
		printf("    segment table entry invlid, trace stops.\n");
		return;
	}

	n_dup_entries = 0;
	prev_entry = 0xFFFFFFFF;
	if (long_format) {
		printf("        page table (ptes) at 0x%x\n", (unsigned)ptep);
		for (i = 0; i < PDT_ENTRIES; i++, ptep++) {
			if (prev_entry == ((pte_template_t *)ptep)->bits
			    && PDTIDX(va) != i && i != PDT_ENTRIES-1) {
				n_dup_entries++;
				continue;  /* suppress suplicate entry */
			}
			if (n_dup_entries != 0) {
				printf("    - %d duplicate entries skipped -\n",n_dup_entries);
				n_dup_entries = 0;
			}
			prev_entry = ((pte_template_t *)ptep)->bits;
			if (PDTIDX(va) == i) {
				printf("    >> (%x)pte: ", i);
			} else {
				printf("       (%x)pte: ", i);
			}
			PRINT_PDT(ptep);
		} /* for */
	} else {
		/* index into page table */
		ptep += PDTIDX(va);
		printf("    pte index 0x%x\n", PDTIDX(va));
		printf("    pte: ");
		PRINT_PDT(ptep);
	}
} /* pmap_print_trace() */

d4418 1
a4418 1
#if USING_BATC
d4481 1
a4481 1
#if FUTURE_MAYBE
@


1.28
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/05/16 12:49:47 ho Exp $	*/
a46 3
#ifdef luna88k
   #define OMRON_PMAP
#endif
d166 1
a166 1
#define	KERNEL_PDT_SIZE	(M88K_BTOP(MAX_KERNEL_VA_SIZE) * sizeof(pt_entry_t))
d172 1
a172 1
#define	M188_PDT_SIZE	(M88K_BTOP(UTIL_SIZE) * sizeof(pt_entry_t))
d178 1
a178 1
#define	M1x7_PDT_SIZE	(M88K_BTOP(OBIO_SIZE) * sizeof(pt_entry_t))
a191 2
int      ptes_per_vm_page; /* no. of ptes required to map one VM page */

a338 1
 *	pmap_verify_free refer to this.
d413 1
a413 1
			cmmu_flush_remote_tlb(cpu, kernel, va, M88K_PGBYTES);
d628 1
a628 1
	template.bits = M88K_TRUNC_PAGE(start) | aprot | cmode | DT_VALID;
d630 1
a630 1
	npages = M88K_BTOP(M88K_ROUND_PAGE(end) - M88K_TRUNC_PAGE(start));
d654 2
a655 2
		virt += M88K_PGBYTES;
		template.bits += M88K_PGBYTES;
d729 1
a729 1
	template.bits = M88K_TRUNC_PAGE(start) | aprot | DT_VALID | cmode;
d739 2
a740 2
	num_phys_pages = M88K_BTOP(M88K_ROUND_PAGE(end) - 
				   M88K_TRUNC_PAGE(start));
d751 1
a751 1
		     num_phys_pages >= BATC_BLKBYTES/M88K_PGBYTES &&
d770 1
a770 1
				for (i = 0; i < BATC_BLKBYTES; i += M88K_PGBYTES ) {
d780 2
a781 2
			template.pte.pfn = M88K_BTOP(phys);
			num_phys_pages -= BATC_BLKBYTES/M88K_PGBYTES;
d795 3
a797 3
		virt += M88K_PGBYTES;
		phys += M88K_PGBYTES;
		template.bits += M88K_PGBYTES;
d801 1
a801 1
	return (M88K_ROUND_PAGE(virt));
d874 1
a874 1
	for (va = s; va < e; va += M88K_PGBYTES) {
d899 2
a900 4
				/*cmmu_flush_remote_data_cache(cpu, 
				M88K_PTOB(pte->pfn),M88K_PGBYTES);*/
				cmmu_flush_remote_cache(cpu, M88K_PTOB(pte->pfn),
							M88K_PGBYTES);
a982 4
	ptes_per_vm_page = PAGE_SIZE >> M88K_PGSHIFT;
	if (ptes_per_vm_page == 0) {
		panic("pmap_bootstrap: VM page size < MACHINE page size");
	}
d1005 1
a1005 1
	*phys_start = M88K_ROUND_PAGE(*phys_start);
d1007 1
a1007 1
		      (M88K_TRUNC_PAGE((unsigned)&kernelstart) - load_start);
d1085 1
a1085 1
			       M88K_TRUNC_PAGE((unsigned)&kernelstart));
d1087 1
a1087 1
	e_text = M88K_ROUND_PAGE(e_text);
d1102 1
a1102 1
	assert(vaddr == M88K_TRUNC_PAGE((unsigned)&kernelstart));
d1106 1
a1106 1
		      (vm_offset_t)M88K_TRUNC_PAGE(((unsigned)&kernelstart)),
d1129 1
a1129 1
			vaddr = M88K_ROUND_PAGE(vaddr + 1);
d1279 1
a1279 1
	apr_data.field.st_base = M88K_BTOP(kernel_pmap->sdt_paddr);
d1567 1
a1567 1
 *	template = M88K_TRUNC_PAGE(phys)
d1582 1
a1582 1
 *	M88K_TRUNC_PAGE
a1584 1
 *	DO_PTES
a1598 1
	unsigned int	i;
d1607 11
a1617 13
	for (i = 0; i < ptes_per_vm_page; i++, phys += M88K_PGBYTES) {
		template.bits = M88K_TRUNC_PAGE(phys)
				| m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
				| DT_VALID | CACHE_GLOBAL;

		spl_sav = splimp();
		cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
		*srcpte = template.pte;
		splx(spl_sav);
		bzero((void*)srcva, M88K_PGBYTES);
		/* force the data out */
		cmmu_flush_remote_data_cache(my_cpu,phys, M88K_PGBYTES);
	}
a1674 1
	vm_offset_t addr;
d1679 1
a1679 1
	s = M88K_ROUND_PAGE(2 * SDT_SIZE);
d1973 1
a1973 1
 *	M88K_PTOB
a2008 1
	register int		i;
d2065 1
a2065 1
		pa = M88K_PTOB(pfn);
d2118 1
a2118 3
		 * For each pte in vm_page (NOTE: vm_page, not
		 * M88K (machine dependent) page !! ), reflect
		 * modify bits to pager and zero (invalidate,
a2121 1
		for (i = ptes_per_vm_page; i > 0; i--) {
d2123 6
a2128 6
			/*
			 * Invalidate pte temporarily to avoid being written back
			 * the modified bit and/or the reference bit by other cpu.
			 */
			opte.bits = invalidate_pte(pte);
			flush_atc_entry(users, tva, kflush);
d2130 4
a2133 7
			if (opte.pte.modified) {
				if (IS_VM_PHYSADDR(pa)) {
					vm_page_set_modified(PHYS_TO_VM_PAGE(opte.bits & M88K_PGMASK));
				}
				/* keep track ourselves too */
				if (PMAP_MANAGED(pa))
					SET_ATTRIB(pa, 1);
d2135 3
a2137 2
			pte++;
			tva += M88K_PGBYTES;
d2216 1
a2216 1
 *	M88K_PTOB
a2240 1
	register int		i;
d2299 1
a2299 1
		if (M88K_PTOB(pte->pfn) != phys)
d2314 4
d2319 2
a2320 8
		for (i = ptes_per_vm_page; i>0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified 
			 * bit and/or the reference bit being written back
			 * by other cpu.
			 */
			opte.bits = invalidate_pte(pte);
			flush_atc_entry(users, va, kflush);
d2322 4
a2325 7
			if (opte.pte.modified) {
				vm_page_set_modified((vm_page_t)PHYS_TO_VM_PAGE(phys));
				/* keep track ourselves too */
				SET_ATTRIB(phys, 1);
			}
			pte++;
			va += M88K_PGBYTES;
d2359 1
a2359 1
 *		M88K_PTOB
a2373 1
	register int         i;
d2436 1
a2436 1
		if (M88K_PTOB(pte->pfn) != phys)
d2441 4
d2446 7
a2452 16

		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified 
			 * bit and/or the reference bit being written back
			 * by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			opte.pte.prot = M88K_RO;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va += M88K_PGBYTES;
		}
a2495 1
	register int		i;
d2497 1
a2497 1
	vm_offset_t		va, tva;
d2556 12
a2567 16
		tva = va;
		for (i = ptes_per_vm_page; i>0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the 
			 * modified bit and/or the reference bit being 
			 * written back by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			opte.pte.prot = ap;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, tva, kflush);
			splx(spl_sav);
			pte++;
			tva += M88K_PGBYTES;
		}
a2802 1
	register int	i;
d2859 1
a2859 1
	old_pa = M88K_PTOB(pte->pfn);
d2877 1
a2877 1
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
d2879 1
a2879 1
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
d2889 10
a2898 15
			for (i = ptes_per_vm_page; i>0; i--) {

				/*
				 * Invalidate pte temporarily to avoid being written back
				 * the modified bit and/or the reference bit by other cpu.
				 */
				spl_sav = splimp();
				opte.bits = invalidate_pte(pte);
				template.pte.modified = opte.pte.modified;
				*pte++ = template.pte;
				flush_atc_entry(users, va, kflush);
				splx(spl_sav);
				template.bits += M88K_PGBYTES;
				va += M88K_PGBYTES;
			}
d2997 1
a2997 1
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
d2999 1
a2999 1
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
d3004 1
a3004 1
		DO_PTES (pte, template.bits);
a3040 1
	int      i;
d3048 1
a3048 1
	if (pte->wired)
d3051 2
a3052 3

	for (i = ptes_per_vm_page; i>0; i--)
		(pte++)->wired = 0;
d3114 1
a3114 1
			pa = M88K_PTOB(pte->pfn);
d3119 1
a3119 1
	pa |= (va & M88K_PGOFSET); /* offset within page */
d3158 1
a3158 1
			pa = M88K_PTOB(pte->pfn);
d3164 1
a3164 1
		pa |= (va & M88K_PGOFSET); /* offset within page */
d3434 1
a3434 1
		apr_data.field.st_base = M88K_BTOP(pmap->sdt_paddr); 
d3529 1
a3529 1
 *		M88K_TRUNC_PAGE
a3530 1
 *		DO_PTES
a3544 1
	int i;
d3561 2
a3562 18
	for (i=0; i < ptes_per_vm_page; i++, 
	     src += M88K_PGBYTES, dst += M88K_PGBYTES) {
		template.bits = M88K_TRUNC_PAGE(src) | aprot | 
			DT_VALID | CACHE_GLOBAL;

		/* do we need to write back dirty bits */
		spl_sav = splimp();
		cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
		*srcpte = template.pte;

		/*
		 *	Map destination physical address.
		 */
		template.bits = M88K_TRUNC_PAGE(dst) | aprot | 
			CACHE_GLOBAL | DT_VALID;
		cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
		*dstpte  = template.pte;
		splx(spl_sav);
d3564 18
a3581 5
		bcopy((void*)srcva, (void*)dstva, M88K_PGBYTES);
		/* flush source, dest out of cache? */
		cmmu_flush_remote_data_cache(my_cpu, src, M88K_PGBYTES);
		cmmu_flush_remote_data_cache(my_cpu, dst, M88K_PGBYTES);
	}
d3602 1
a3602 1
 *		M88K_TRUNC_PAGE
a3603 1
 *		DO_PTES
a3614 1
	unsigned int i;
d3619 2
a3620 2
	copy_size = M88K_PGBYTES;
	offset = dstpa - M88K_TRUNC_PAGE(dstpa);
d3625 1
a3625 1
		copy_size = M88K_PGBYTES - offset;
d3631 11
a3641 13
		for (i = 0; i < ptes_per_vm_page; i++) {
			template.bits = M88K_TRUNC_PAGE(dstpa) | aprot | CACHE_WT | DT_VALID;
			cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
			*dstpte = template.pte;

			dstva += offset;
			bcopy((void*)srcva, (void*)dstva, copy_size);
			srcva += copy_size;
			dstva += copy_size;
			dstpa += M88K_PGBYTES;
			bytecount -= copy_size;
			offset = 0;
		}
d3662 1
a3662 1
 *		M88K_TRUNC_PAGE
a3663 1
 *		DO_PTES
a3673 1
	unsigned int i;
d3678 2
a3679 2
	copy_size = M88K_PGBYTES;
	offset = srcpa - M88K_TRUNC_PAGE(srcpa);
d3684 1
a3684 1
		copy_size = M88K_PGBYTES - offset;
d3690 12
a3701 14
		for (i=0; i < ptes_per_vm_page; i++) {
			template.bits = M88K_TRUNC_PAGE(srcpa) | aprot | CACHE_WT | DT_VALID;
			cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
			*srcpte = template.pte;

			srcva += offset;
			bcopy((void*)srcva, (void*)dstva, copy_size);
			srcpa += M88K_PGBYTES;
			dstva += copy_size;
			srcva += copy_size;
			bytecount -= copy_size;
			offset = 0;
			/* cache flush source? */
		}
d3720 1
a3720 1
 *	M88K_ROUND_PAGE
a3733 1
	int        i;
d3738 1
a3738 1
	va = M88K_ROUND_PAGE(va);
d3748 12
a3759 16
	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL && PDT_VALID(pte))
		for (i = ptes_per_vm_page; i > 0; i--) {

			/*
			 * Invalidate pte temporarily to avoid being written back
			 * the modified bit and/or the reference bit by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			opte.pte.prot = M88K_RO;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va +=M88K_PGBYTES;
		}
a3801 1
	int        i;
d3854 12
a3865 15
		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified bit 
			 * and/or the reference being written back by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			/* clear modified bit */
			opte.pte.modified = 0;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va += M88K_PGBYTES;
		}
a3914 1
	int      i;
d3969 2
a3970 3
		for (i = ptes_per_vm_page; i > 0; i--) {
			if (ptep->modified) {
				simple_unlock(&pvep->pmap->lock);
d3972 2
a3973 2
				if ((pmap_con_dbg & (CD_IMOD | CD_FULL)) == (CD_IMOD | CD_FULL))
					printf("(pmap_is_modified :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d3975 3
a3977 5
				UNLOCK_PVH(phys);
				SPLX(spl);
				return (TRUE);
			}
			ptep++;
a4029 1
	int        i;
d4081 11
a4091 15
		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified bit 
			 * and/or the reference being written back by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			/* clear reference bit */
			opte.pte.pg_used = 0;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va += M88K_PGBYTES;
		}
a4140 1
	int     i;
d4169 5
a4173 8
		for (i = ptes_per_vm_page; i > 0; i--) {
			if (ptep->pg_used) {
				simple_unlock(&pvep->pmap->lock);
				UNLOCK_PVH(phys);
				SPLX(spl);
				return (TRUE);
			}
			ptep++;
a4184 66
 *	Routine:	PMAP_VERIFY_FREE
 *
 *	History:
 *	'90. 7.17	Fuzzy	This routine extract vax's pmap.c.
 *				This do not exit in m68k's pmap.c.
 *				vm_page_alloc calls this.
 *				Variables changed below,
 *					vm_first_phys --> pmap_phys_start
 *					vm_last_phys  --> pmap_phys_end
 *
 *	Calls:
 *		SPLVM, SPLX
 *		PA_TO_PVH
 *
 *	Global/Extern:
 *		pmap_initialized
 *		pmap_phys_start
 *		pmap_phys_end
 *		TRUE, FALSE
 *		PMAP_NULL
 *
 *	This routine check physical address if that have pmap modules.
 *	It returns TRUE/FALSE.
 */

boolean_t
pmap_verify_free(vm_offset_t phys)
{
	pv_entry_t  pv_h;
	int      spl;
	boolean_t   result;

	if (!pmap_initialized)
		return (TRUE);

	if (!PMAP_MANAGED(phys))
		return (FALSE);

	SPLVM(spl);

	pv_h = PA_TO_PVH(phys);
	LOCK_PVH(phys);

	result = (pv_h->pmap == PMAP_NULL);
	UNLOCK_PVH(phys);
	SPLX(spl);

	return (result);
} /* pmap_verify_free */

/*
 *	Routine:	PMAP_VALID_PAGE
 *
 *	The physical address space is dense... there are no holes.
 *	All addresses provided to vm_page_startup() are valid.
 */
boolean_t
pmap_valid_page(vm_offset_t p)
{
#ifdef lint
	p++;
#endif
	return (TRUE);
}  /* pmap_valid_page() */

/*
a4240 1
	int        i;
d4284 1
a4284 1
		pa = M88K_PTOB(srcpte->pfn);
d4295 3
d4299 6
a4304 12
		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified bit 
			 * and/or the reference being written back by other cpu.
			 */
			opte.bits = invalidate_pte(srcpte);
			flush_atc_entry(users, from, 1);
			((pte_template_t *)dstpte)->bits = opte.bits;
			from += M88K_PGBYTES;
			to += M88K_PGBYTES;
			srcpte++; dstpte++;
		}
a4323 3
 *	Extern/globals:
 *		ptes_per_vm_page
 *
a4333 1
	int  i;
d4336 4
a4339 6
	for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
		for (cpu=0; cpu<max_cpus; cpu++) {
			if (cpu_sets[cpu]) {
				cmmu_flush_remote_inst_cache(cpu, pa, 
							     M88K_PGBYTES);
			}
a4355 1
 *		ptes_per_vm_page
a4364 1
	int  i;
d4372 2
a4373 4
	pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
	for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
		cmmu_flush_data_cache(pa, M88K_PGBYTES);
	}
a4442 1
	int  i;
d4452 1
a4452 1
	if (bytes > 2*M88K_PGBYTES) {
d4458 3
a4460 5
	for (va = virt; bytes > 0; bytes -= M88K_PGBYTES,va += M88K_PGBYTES) {
		pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
		for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
			cache_flush_loop(mode, pa, M88K_PGBYTES);
		}
d4539 1
a4539 1
			pa = M88K_PTOB(pte->pfn) | (pv_e->va & M88K_PGOFSET);
d4630 1
a4630 1
		phys = M88K_PTOB(ptep->pfn);  /* pick up phys addr */
d4795 1
a4795 1
			M88K_PTOB(p->table_addr),			\
d4984 1
a4984 1
	ptep = (pt_entry_t *)(M88K_PTOB(sdtv->table_addr));
a5019 51

/*
 * Check whether the current transaction being looked at by dodexc()
 *  could have been the one that caused a fault.  Given the virtual
 *  address, map, and transaction type, checks whether the page at that
 *  address is valid, and, for write transactions, whether it has write
 *  permission.
 */
boolean_t
pmap_check_transaction(pmap_t pmap, vm_offset_t va, vm_prot_t type)
{
	pt_entry_t *pte;
	sdt_entry_t   *sdt;
	int                         spl;

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		PMAP_UNLOCK(pmap, spl);
		return FALSE;
	}

	if (!PDT_VALID(pte)) {
		PMAP_UNLOCK(pmap, spl);
		return FALSE;
	}

	/*
	 * Valid pte.  If the transaction was a read, there is no way it
	 *  could have been a fault, so return true.  For now, assume
	 *  that a write transaction could have caused a fault.  We need
	 *  to check pte and sdt entries for write permission to really
	 *  tell.
	 */

	if (type == VM_PROT_READ) {
		PMAP_UNLOCK(pmap, spl);
		return TRUE;
	} else {
		sdt = SDTENT(pmap,va);
		if (sdt->prot || pte->prot) {
			PMAP_UNLOCK(pmap, spl);
			return FALSE;
		} else {
			PMAP_UNLOCK(pmap, spl);
			return TRUE;
		}
	}
}

/* New functions to satisfy rpd - contributed by danner */
@


1.27
log
@No need to check M_WAIT/M_WAITOK malloc return values. (art@@ ok)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2001/05/09 15:31:25 art Exp $	*/
d1712 2
d1717 4
a1720 1
		pmap_zero_page(pmap_extract(kernel_pmap, addr));
d1723 1
d1729 1
a1729 1
	p->sdt_paddr = (sdt_entry_t *)pmap_extract(kernel_pmap,(vm_offset_t)segdt);
a2662 1
	vm_offset_t	pmap_extract();
d2696 1
a2696 1
	pdt_paddr = pmap_extract(kernel_pmap, pdt_vaddr);
d3118 1
d3134 2
a3135 2
vm_offset_t
pmap_extract(pmap_t pmap, vm_offset_t va)
d3137 3
a3139 3
	register pt_entry_t  *pte;
	register vm_offset_t pa;
	register int   i;
d3151 1
a3151 1
				pa = (batc_entry[i].pba << BATC_BLKSHIFT) | 
d3153 1
a3153 1
				return (pa);
d3158 3
a3160 3
	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
		pa = (vm_offset_t) 0;
	else {
d3164 1
a3164 1
			pa = (vm_offset_t) 0;
d3167 2
a3168 2
	if (pa)
		pa |= (va & M88K_PGOFSET); /* offset within page */
d3171 5
a3175 2
	return (pa);
} /* pamp_extract() */
@


1.26
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2001/03/22 01:15:35 miod Exp $	*/
a1676 3
	if (p == PMAP_NULL) {
		panic("pmap_create: cannot allocate a pmap");
	}
@


1.25
log
@Get pmap->cpus_using before the pmap structure is touched. Besides, we
need to do this before playing with pmap_expand().
Solves a few more pmap data corruption problems.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2001/03/14 22:02:16 deraadt Exp $	*/
d3060 1
a3060 1
 *	Routine:	pmap_change_wiring
a3068 1
 *		wired		flag indicating new wired state
d3082 1
a3082 1
pmap_change_wiring(pmap_t map, vm_offset_t v, boolean_t wired)
d3091 1
a3091 1
		panic ("pmap_change_wiring: pte missing");
d3093 1
a3093 5
	if (wired && !pte->wired)
		/* wiring mapping */
		map->stats.wired_count++;

	else if (!wired && pte->wired)
d3098 1
a3098 1
		(pte++)->wired = wired;
d3102 1
a3102 1
} /* pmap_change_wiring() */
a3758 32

/*
 * Routine:	PMAP_PAGEABLE
 *
 * History:
 *	'90.7.16	Fuzzy
 *
 * Function:
 *	Make the specified pages (by pmap, offset) pageable (or not) as
 *	requested. A page which is not pageable may not take a fault;
 *	therefore, its page table entry must remain valid for the duration.
 *	this routine is merely advisory; pmap_enter will specify that
 *	these pages are to be wired down (or not) as appropriate.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	start		virtual address of start of range
 *	end		virtual address of end of range
 *	pageable	flag indicating whether range is to be pageable.
 *
 *	This routine currently does nothing in the 88100 implemetation.
 */
void
pmap_pageable(pmap_t pmap, vm_offset_t start, vm_offset_t end,
	      boolean_t pageable)
{
#ifdef lint
	pmap++; start++; end++; pageable++;
#endif
} /* pmap_pagealbe() */


@


1.24
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2001/03/09 05:44:42 smurph Exp $	*/
d2879 1
a2899 2

		users = pmap->cpus_using;
d5276 2
d5294 2
@


1.23
log
@kernel will compile with -Werror.  Added intr.h
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2001/03/08 22:26:00 miod Exp $	*/
d3230 1
a3230 1
 *		dst_addr	VA in destionation map
@


1.22
log
@Include <machine/asm_macro.h>, reorder include files list.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2001/03/07 23:52:33 miod Exp $	*/
d55 1
d1630 1
a1630 1
		bzero (srcva, M88K_PGBYTES);
@


1.21
log
@Correct the diagnostic code in flush_atc_entry.
This function may be called for a pmap which is not marked as tied to
any cpu, which is acceptable behaviour.
While there, some warning hunting.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2001/02/12 08:16:25 smurph Exp $	*/
a53 1
#include <machine/board.h>
d55 6
a60 1
#include <machine/m882xx.h>		/* CMMU stuff */
d67 1
a67 5
#include <sys/simplelock.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/msgbuf.h>
#include <sys/user.h>
d69 1
d71 1
@


1.20
log
@correct buginstat(), statclock now working for '188, systat vmstat now works,
serial driver for '188 working better.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2001/02/01 03:38:21 smurph Exp $	*/
d392 2
a393 2
 *	users	bit paterns of the CPUs which may hold the TLB shoule be
 *		flushed
d409 1
a409 1
	if (ff1(tusers) > 4) { /* can't be more than 4 */
d609 1
d611 1
d975 1
a975 2
			kernel_pmap_size,
			etherpa;
a980 1
	extern char	*kernel_sdt;
d1689 1
a1689 1
	int i, spl;
a1691 4
	sdt_entry_t *sdt;
	pt_entry_t  *pte;
	pte_template_t    template;
	int        aprot;
d3459 2
d3462 2
@


1.19
log
@Major changes to get MVME188 working.  More header and code cleanups.  The
kernel is tested on MVME188A/2P256 and MVME188A/1P64.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2001/01/14 20:25:25 smurph Exp $	*/
d353 1
a353 1
#ifdef	DEBUG
d408 1
a408 1
#if 0
a445 1

a466 1
vm_offset_t va_tmp = (vm_offset_t)0xDEADBEEF;
a478 3
	if (virt == va_tmp) {
		printf("check sdt @@ 0x%x\n", sdt);
	}
d534 1
a534 1
#if	DEBUG
a537 1

d618 1
a618 1
#if	DEBUG
d639 1
a639 1
#ifdef	DEBUG
d720 1
a720 1
#if	DEBUG
d1179 1
a1179 1
   #ifdef DEBUG
d1184 1
a1184 1
   #endif
d1436 1
a1436 1
#ifdef	DEBUG
d1451 1
a1451 1
#ifdef	DEBUG
d1512 1
a1512 1
#ifdef	DEBUG
d1526 1
a1526 1
#ifdef	DEBUG
d1701 1
a1701 1
#ifdef	DEBUG
d1734 1
a1734 1
#ifdef	DEBUG
d1741 1
d1751 1
d1782 1
a1782 1
#ifdef	DEBUG
d1824 1
a1824 1
#if	DEBUG
d1843 1
a1843 1
#ifdef	DEBUG
d1852 1
a1852 1
#ifdef	DEBUG
d1872 1
a1872 1
#ifdef	DBG
a1878 1

d1911 1
a1911 1
#ifdef	DEBUG
d2273 1
a2273 1
#ifdef	DEBUG
d2408 1
a2408 1
#ifdef	DEBUG
d2423 1
a2423 1
#ifdef	DEBUG
d2573 1
a2573 1
#ifdef	DEBUG
d2581 1
a2581 1
#ifdef	DEBUG
d2667 1
a2667 1
#ifdef	DEBUG
d2682 1
a2682 1
#ifdef	DEBUG
d2697 1
d2704 1
d2721 1
a2721 1
#ifdef	DEBUG
d2859 1
a2859 1
#ifdef	DEBUG
a2866 1

d2956 1
a2956 1
#ifdef	DEBUG
d2975 1
a2975 1
#ifdef	DEBUG
d2999 1
a2999 1
#ifdef	DEBUG
d3243 1
a3243 1
#ifdef        lint
d3275 1
a3275 1
#ifdef	DBG
d3355 1
a3355 1
#ifdef	DBG
d3428 1
a3428 1
#ifdef	DBG
a3634 1

d3788 1
a3788 1
#ifdef	lint
d3903 1
a3903 1
#ifdef	DBG
d3921 1
a3921 1
#ifdef	DEBUG
d4018 1
a4018 1
#ifdef	DBG
d4034 1
a4034 1
#ifdef	DBG
d4047 1
a4047 1
#ifdef	DBG
d4072 1
a4072 1
#ifdef	DBG
d4139 1
a4139 1
#ifdef	DBG
d4156 1
a4156 1
#ifdef	DBG
d4356 1
a4356 1
#ifdef	lint
d4452 1
a4452 1
   #ifdef DEBUG
d4458 1
a4458 1
   #endif /* DEBUG */
d4660 1
a4660 1
#ifdef	DEBUG
a4933 1

a5057 1

d5174 1
a5174 1
#ifdef DBG
a5497 1

@


1.18
log
@Complete move to UVM virtual memory system.  More header fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2001/01/13 05:18:59 smurph Exp $	*/
d104 1
d106 1
a106 1
   #define CD_FULL 	0x02
d108 1
a108 1
   #define CD_ACTIVATE	0x0000004	/* _pmap_activate */
d112 1
a112 1
   #define CD_CACHE	0x0000040	/* pmap_cache_ctrl */
d115 1
a115 1
   #define CD_CREAT	0x0000200	/* pmap_create */
d117 1
a117 1
   #define CD_DESTR	0x0000800	/* pmap_destroy */
d130 2
a131 2
   #define CD_CHKPV	0x1000000	/* check_pv_list */
   #define CD_CHKPM	0x2000000	/* check_pmap_consistency */
d134 1
a134 4
int pmap_con_dbg = CD_NORM;
/*
int pmap_con_dbg = CD_FULL| CD_NORM | CD_PROT | CD_BOOT | CD_CHKPV | CD_CHKPM | CD_CHKM;
int pmap_con_dbg = CD_NORM;*/
d171 8
a178 2
#if defined(MVME188) && !(defined(MVME187) || defined(MVME197))
#define	OBIO_PDT_SIZE	0
d180 4
a183 2
#define	OBIO_PDT_SIZE	((cputyp == CPU_188) ? 0 : (M88K_BTOP(OBIO_SIZE) * sizeof(pt_entry_t)))
#endif
d405 2
a406 2
   register int cpu;
   long tusers = users;
d409 4
a412 4
   if (ff1(tusers) > 4) { /* can't be more than 4 */
      printf("ff1 users = %d!\n", ff1(tusers));
      panic("bogus amount of users!!!");
   }
d414 6
a419 6
   while ((cpu = ff1(tusers)) != 32) {
      if (cpu_sets[cpu]) { /* just checking to make sure */
         cmmu_flush_remote_tlb(cpu, kernel, va, M88K_PGBYTES);
      }
      tusers &= ~(1 << cpu);
   }
a422 130
 * Routine:	_PMAP_ACTIVATE
 *
 * Author:	N. Sugai
 *
 * Function:
 *	Binds the given physical map to the given processor.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	p	pointer to proc structure
 *	cpu	CPU number
 *
 * If the specified pmap is not kernel_pmap, this routine makes arp
 * template and stores it into UAPR (user area pointer register) in the
 * CMMUs connected to the specified CPU.
 *
 * If kernel_pmap is specified, only flushes the TLBs mapping kernel
 * virtual space, in the CMMUs connected to the specified CPU.
 *
 * NOTE:
 * All of the code of this function extracted from macro PMAP_ACTIVATE
 * to make debugging easy. Accordingly, PMAP_ACTIVATE simlpy call
 * _pmap_activate.
 *
 */
#if 0
void
_pmap_activate(pmap_t pmap, pcb_t pcb, int my_cpu)
{
   apr_template_t   apr_data;
   int     n;

#ifdef DEBUG
   if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
      printf("(_pmap_activate :%x) pmap 0x%x\n", curproc, (unsigned)pmap);
#endif

   if (pmap != kernel_pmap) {
      /*
       *	Lock the pmap to put this cpu in its active set.
       */
      simple_lock(&pmap->lock);

      apr_data.bits = 0;
      apr_data.field.st_base = M88K_BTOP(pmap->sdt_paddr); 
      apr_data.field.wt = 0;
      apr_data.field.g  = 1;
      apr_data.field.ci = 0;
      apr_data.field.te = 1;
#ifdef notyet
   #ifdef OMRON_PMAP
      /*
       * cmmu_pmap_activate will set the uapr and the batc entries, then
       * flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE ABOUT THE
       * BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE FLUSHED AS WELL.
       */
      cmmu_pmap_activate(my_cpu, apr_data.bits, pmap->i_batc, pmap->d_batc);
      for (n = 0; n < BATC_MAX; n++)
         *(unsigned*)&batc_entry[n] = pmap->i_batc[n].bits;
   #else
      cmmu_set_uapr(apr_data.bits);
      cmmu_flush_tlb(0, 0, -1);
   #endif
#endif /* notyet */
      /*
       * I am forcing it to not program the BATC at all. pmap.c module
       * needs major, major cleanup. XXX nivas
       */
      cmmu_set_uapr(apr_data.bits);
      cmmu_flush_tlb(0, 0, -1);

      /*
       *	Mark that this cpu is using the pmap.
       */
      SETBIT_CPUSET(my_cpu, &(pmap->cpus_using));

      simple_unlock(&pmap->lock);

   } else {

      /*
       * kernel_pmap must be always active.
       */

#ifdef DEBUG
      if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
         printf("(_pmap_activate :%x) called for kernel_pmap\n", curproc);
#endif

   }
} /* _pmap_activate */
#endif 

/*
 * Routine:	_PMAP_DEACTIVATE
 *
 * Author:	N. Sugai
 *
 * Function:
 *	Unbinds the given physical map to the given processor.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	th	pointer to thread structure
 *	cpu	CPU number
 *
 * _pmap_deactive simply clears the cpus_using field in given pmap structure.
 *
 * NOTE:
 * All of the code of this function extracted from macro PMAP_DEACTIVATE
 * to make debugging easy. Accordingly, PMAP_DEACTIVATE simlpy call
 * _pmap_deactivate.
 *
 */
#if 0
void
_pmap_deactivate(pmap_t pmap, pcb_t pcb, int my_cpu)
{
   if (pmap != kernel_pmap) {

      /*
       * we expect the spl is already raised to sched level.
       */
      simple_lock(&pmap->lock);
      CLRBIT_CPUSET(my_cpu, &(pmap->cpus_using));
      simple_unlock(&pmap->lock);
   }
}
#endif 
/*
d437 1
a437 1
   pte_template_t p;
d439 2
a440 2
   p.bits = 0;
   p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;
d442 1
a442 1
   return (p.bits);
d468 1
d473 1
a473 1
   sdt_entry_t *sdt;
d475 17
a491 15
   /*XXX will this change if physical memory is not contiguous? */
   /* take a look at PDTIDX XXXnivas */
   if (map == PMAP_NULL)
      panic("pmap_pte: pmap is NULL");

   sdt = SDTENT(map,virt);

   /*
    * Check whether page table is exist or not.
    */
   if (!SDT_VALID(sdt))
      return (PT_ENTRY_NULL);
   else
      return ((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
              PDTIDX(virt));
a494 1

d534 4
a537 4
   int        aprot;
   sdt_entry_t      *sdt;
   kpdt_entry_t  kpdt_ent;
   pmap_t     map = kernel_pmap;
d540 2
a541 2
   if ((pmap_con_dbg & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
      printf("(pmap_expand_kmap :%x) v %x\n", curproc,virt);
d544 13
a556 1
   aprot = m88k_protection (map, prot);
d558 4
a561 16
   /*  segment table entry derivate from map and virt. */
   sdt = SDTENT(map, virt);
   if (SDT_VALID(sdt))
      panic("pmap_expand_kmap: segment table entry VALID");

   kpdt_ent = kpdt_free;
   if (kpdt_ent == KPDT_ENTRY_NULL) {
      printf("pmap_expand_kmap: Ran out of kernel pte tables\n");
      return (PT_ENTRY_NULL);
   }
   kpdt_free = kpdt_free->next;

   ((sdt_entry_template_t *)sdt)->bits = kpdt_ent->phys | aprot | DT_VALID;
   ((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = (vm_offset_t)kpdt_ent | aprot | DT_VALID;
   (unsigned)(kpdt_ent->phys) = 0;
   (unsigned)(kpdt_ent->next) = 0;
d563 1
a563 1
   return ((pt_entry_t *)(kpdt_ent) + PDTIDX(virt));
d609 14
a622 14
   int        aprot;
   unsigned      npages;
   unsigned      num_phys_pages;
   unsigned      cmode;
   pt_entry_t    *pte;
   pte_template_t    template;
   static unsigned i = 0;
   /*
    * cache mode is passed in the top 16 bits.
    * extract it from there. And clear the top
    * 16 bits from prot.
    */
   cmode = (prot & 0xffff0000) >> 16;
   prot &= 0x0000ffff;
d625 3
a627 3
   if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
      printf ("(pmap_map :%x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
              curproc, start, end, virt, prot, cmode);
d630 2
a631 2
   if (start > end)
      panic("pmap_map: start greater than end address");
d633 1
a633 1
   aprot = m88k_protection (kernel_pmap, prot);
d635 1
a635 1
   template.bits = M88K_TRUNC_PAGE(start) | aprot | cmode | DT_VALID;
d637 1
a637 1
   npages = M88K_BTOP(M88K_ROUND_PAGE(end) - M88K_TRUNC_PAGE(start));
d639 1
a639 1
   for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {
d641 3
a643 3
      if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
         if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
            panic ("pmap_map: Cannot allocate pte table");
d646 3
a648 3
      if (pmap_con_dbg & CD_MAP)
         if (pte->dtype)
            printf("(pmap_map :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
d650 1
a650 2

      *pte = template.pte;
d652 8
a659 7
      /* hack for MVME197 */
      if (cputyp == CPU_197) {
         if (i < 32) {
            m197_load_patc(i, virt, (vm_offset_t)template.bits, 1);
            i++;
         }
      }
d661 3
a663 3
      virt += M88K_PGBYTES;
      template.bits += M88K_PGBYTES;
   }
d665 1
a665 1
   return (virt);
d716 1
a716 1
              vm_prot_t prot, unsigned cmode)
d718 7
a724 7
   int        aprot;
   unsigned      num_phys_pages;
   vm_offset_t      phys;
   pt_entry_t    *pte;
   pte_template_t   template;
   batc_template_t  batctmp;
   register int  i;
d727 18
a744 18
   if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
      printf ("(pmap_map_batc :%x) phys address from %x to %x mapped at virtual %x, prot %x\n", curproc,
              start, end, virt, prot);
#endif

   if (start > end)
      panic("pmap_map_batc: start greater than end address");

   aprot = m88k_protection (kernel_pmap, prot);
   template.bits = M88K_TRUNC_PAGE(start) | aprot | DT_VALID | cmode;
   phys = start;
   batctmp.bits = 0;
   batctmp.field.sup = 1;       /* supervisor */
   batctmp.field.wt = template.pte.wt;      /* write through */
   batctmp.field.g = template.pte.g;     /* global */
   batctmp.field.ci = template.pte.ci;      /* cache inhibit */
   batctmp.field.wp = template.pte.prot; /* protection */
   batctmp.field.v = 1;         /* valid */
d746 2
a747 1
   num_phys_pages = M88K_BTOP(M88K_ROUND_PAGE(end) - M88K_TRUNC_PAGE(start));
d749 1
a749 1
   while (num_phys_pages > 0) {
d752 19
a770 21
      if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
         printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, aligne V=%d, phys=%x, aligne P=%d\n", curproc,
                num_phys_pages, virt, BATC_BLK_ALIGNED(virt), phys, BATC_BLK_ALIGNED(phys));
#endif

      if ( BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(phys) && 
           num_phys_pages >= BATC_BLKBYTES/M88K_PGBYTES &&
           batc_used < BATC_MAX ) {

         /*
          * map by BATC
          */
         batctmp.field.lba = M88K_BTOBLK(virt);
         batctmp.field.pba = M88K_BTOBLK(phys);

         for ( i = 0; i < max_cpus; i++)
            if (cpu_sets[i])
               cmmu_set_pair_batc_entry(i, batc_used, batctmp.bits);

         batc_entry[batc_used] = batctmp.field;

d772 22
a793 22
         if ((pmap_con_dbg & (CD_MAPB | CD_NORM)) == (CD_MAPB | CD_NORM)) {
            printf("(pmap_map_batc :%x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
         }
         if (pmap_con_dbg & CD_MAPB) {

            for (i = 0; i < BATC_BLKBYTES; i += M88K_PGBYTES ) {
               pte = pmap_pte(kernel_pmap, virt+i);
               if (pte->dtype)
                  printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, ((pte_template_t *)pte)->bits);
            }
         }
#endif
         batc_used++;
         virt += BATC_BLKBYTES;
         phys += BATC_BLKBYTES;
         template.pte.pfn = M88K_BTOP(phys);
         num_phys_pages -= BATC_BLKBYTES/M88K_PGBYTES;
         continue;
      }
      if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
         if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
            panic ("pmap_map_batc: Cannot allocate pte table");
d796 3
a798 3
      if (pmap_con_dbg & CD_MAPB)
         if (pte->dtype)
            printf("(pmap_map_batc :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
d801 6
a806 6
      *pte = template.pte;
      virt += M88K_PGBYTES;
      phys += M88K_PGBYTES;
      template.bits += M88K_PGBYTES;
      num_phys_pages--;
   }
d808 1
a808 1
   return (M88K_ROUND_PAGE(virt));
d847 7
a853 7
   int     spl, spl_sav;
   pt_entry_t *pte;
   vm_offset_t   va;
   int           kflush;
   int           cpu;
   register unsigned      users;
   register pte_template_t   opte;
d856 7
a862 7
   if ( mode & CACHE_MASK ) {
      printf("(cache_ctrl) illegal mode %x\n",mode);
      return;
   }
   if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
      printf("(pmap_cache_ctrl :%x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
   }
d865 19
a883 19
   if ( pmap == PMAP_NULL ) {
      panic("pmap_cache_ctrl: pmap is NULL");
   }

   PMAP_LOCK(pmap, spl);

   /*
    * 
    */
   users = pmap->cpus_using;
   if (pmap == kernel_pmap) {
      kflush = 1;
   } else {
      kflush = 0;
   }

   for (va = s; va < e; va += M88K_PGBYTES) {
      if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
         continue;
d885 3
a887 3
      if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
         printf("(cache_ctrl) pte@@0x%08x\n",(unsigned)pte);
      }
d890 20
a909 18
      /*
       * Invalidate pte temporarily to avoid being written back
       * the modified bit and/or the reference bit by other cpu.
       *  XXX
       */
      spl_sav = splimp();
      opte.bits = invalidate_pte(pte);
      ((pte_template_t *)pte)->bits = (opte.bits & CACHE_MASK) | mode;
      flush_atc_entry(users, va, kflush);
      splx(spl_sav);

      /*
       * Data cache should be copied back and invalidated.
       */
      for (cpu=0; cpu<max_cpus; cpu++)
         if (cpu_sets[cpu])
            /*cmmu_flush_remote_data_cache(cpu, M88K_PTOB(pte->pfn),M88K_PGBYTES);*/
            cmmu_flush_remote_cache(cpu, M88K_PTOB(pte->pfn), M88K_PGBYTES);
d911 1
a911 1
   }
d913 1
a913 1
   PMAP_UNLOCK(pmap, spl);
d967 21
a987 22
               vm_offset_t   *phys_start,   /* IN/OUT */
               vm_offset_t   *phys_end,  /* IN */
               vm_offset_t   *virt_start,   /* OUT */
               vm_offset_t   *virt_end)  /* OUT */
{
   kpdt_entry_t  kpdt_virt;
   sdt_entry_t      *kmap;
   vm_offset_t      vaddr,
   virt,
   kpdt_phys,
   s_text,
   e_text,
   kernel_pmap_size,
   etherpa;
   apr_template_t   apr_data;
   pt_entry_t    *pte;
   int        i;
   u_long     foo;
   pmap_table_t   ptable;
   extern char   *kernelstart, *etext;
   extern char    *kernel_sdt;
   extern void cmmu_go_virt(void);
d990 42
a1031 42
   if ((pmap_con_dbg & (CD_BOOT | CD_NORM)) == (CD_BOOT | CD_NORM)) {
      printf("pmap_bootstrap : \"load_start\" 0x%x\n", load_start);
   }
#endif
   ptes_per_vm_page = PAGE_SIZE >> M88K_PGSHIFT;
   if (ptes_per_vm_page == 0) {
      panic("pmap_bootstrap: VM page size < MACHINE page size");
   }
   if (!PAGE_ALIGNED(load_start)) {
      panic("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x", load_start);
   }

   simple_lock_init(&kernel_pmap->lock);

   /*
    * Allocate the kernel page table from the front of available
    * physical memory,
    * i.e. just after where the kernel image was loaded.
    */
   /*
    * The calling sequence is 
    *    ...
    *  pmap_bootstrap(&kernelstart,...) 
    *  kernelstart is the first symbol in the load image.
    *  We link the kernel such that &kernelstart == 0x10000 (size of
    *							BUG ROM)
    *  The expression (&kernelstart - load_start) will end up as
    *	0, making *virt_start == *phys_start, giving a 1-to-1 map)
    */

   *phys_start = M88K_ROUND_PAGE(*phys_start);
   *virt_start = *phys_start +
                 (M88K_TRUNC_PAGE((unsigned)&kernelstart) - load_start);

   /*
    * Initialize kernel_pmap structure
    */
   kernel_pmap->ref_count = 1;
   kernel_pmap->cpus_using = 0;
   kernel_pmap->sdt_paddr = kmap = (sdt_entry_t *)(*phys_start);
   kernel_pmap->sdt_vaddr = (sdt_entry_t *)(*virt_start);
   kmapva = *virt_start;
d1034 21
a1054 21
   if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
      printf("kernel_pmap->sdt_paddr = %x\n",kernel_pmap->sdt_paddr);
      printf("kernel_pmap->sdt_vaddr = %x\n",kernel_pmap->sdt_vaddr);
   }
   /* init double-linked list of pmap structure */
   kernel_pmap->next = kernel_pmap;
   kernel_pmap->prev = kernel_pmap;
#endif

   /* 
    * Reserve space for segment table entries.
    * One for the regular segment table and one for the shadow table
    * The shadow table keeps track of the virtual address of page
    * tables. This is used in virtual-to-physical address translation
    * functions. Remember, MMU cares only for physical addresses of
    * segment and page table addresses. For kernel page tables, we
    * really don't need this virtual stuff (since the kernel will
    * be mapped 1-to-1) but for user page tables, this is required.
    * Just to be consistent, we will maintain the shadow table for
    * kernel pmap also.
    */
d1056 1
a1056 1
   kernel_pmap_size = 2*SDT_SIZE;
d1058 2
a1059 2
   printf("kernel segment table from 0x%x to 0x%x\n", kernel_pmap->sdt_vaddr, 
          kernel_pmap->sdt_vaddr + kernel_pmap_size);
d1061 6
a1066 6
   /* save pointers to where page table entries start in physical memory */
   kpdt_phys = (*phys_start + kernel_pmap_size);
   kpdt_virt = (kpdt_entry_t)(*virt_start + kernel_pmap_size);
   kernel_pmap_size += MAX_KERNEL_PDT_SIZE;
   *phys_start += kernel_pmap_size;
   *virt_start += kernel_pmap_size;
d1068 2
a1069 2
   /* init all segment and page descriptor to zero */
   bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);
d1071 1
a1071 1
   printf("kernel page table to 0x%x\n", kernel_pmap->sdt_vaddr + kernel_pmap_size);
d1075 28
a1102 28
   if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
      printf("kpdt_phys = %x\n",kpdt_phys);
      printf("kpdt_virt = %x\n",kpdt_virt);
      printf("end of kpdt at (virt)0x%08x  ; (phys)0x%08x\n",
             *virt_start,*phys_start);
   }
#endif
   /*
    * init the kpdt queue
    */
   kpdt_free = kpdt_virt;
   for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i>0; i--) {
      kpdt_virt->next = (kpdt_entry_t)((vm_offset_t)kpdt_virt + PDT_SIZE);
      kpdt_virt->phys = kpdt_phys;
      kpdt_virt = kpdt_virt->next;
      kpdt_phys += PDT_SIZE;
   }
   kpdt_virt->next = KPDT_ENTRY_NULL; /* terminate the list */

   /*
    * Map the kernel image into virtual space
    */

   s_text = load_start;         /* paddr of text */
   e_text = load_start + ((unsigned)&etext -
                          M88K_TRUNC_PAGE((unsigned)&kernelstart));
   /* paddr of end of text section*/
   e_text = M88K_ROUND_PAGE(e_text);
d1110 28
a1137 31
#if 1 /* defined(MVME187) || defined (MVME197) */
   /*  map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
   if (cputyp != CPU_188) { /*  != CPU_188 */
      vaddr = PMAPER(
                    0,
                    0,
                    0x10000,
                    (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_INH <<16));
      assert(vaddr == M88K_TRUNC_PAGE((unsigned)&kernelstart));
   }
#endif /* defined(MVME187) || defined (MVME197) */

   vaddr = PMAPER(
                 (vm_offset_t)M88K_TRUNC_PAGE(((unsigned)&kernelstart)),
                 s_text,
                 e_text,
                 (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL<<16));  /* shouldn't it be RO? XXX*/

   vaddr = PMAPER(
                 vaddr,
                 e_text,
                 (vm_offset_t)kmap,
                 (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));

   /*
    * Map system segment & page tables - should be cache inhibited?
    * 88200 manual says that CI bit is driven on the Mbus while accessing
    * the translation tree. I don't think we need to map it CACHE_INH
    * here...
    */
   if (kmapva != vaddr) {
d1139 12
a1150 12
      if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
         printf("(pmap_bootstrap) correcting vaddr\n");
      }
#endif
      while (vaddr < (*virt_start - kernel_pmap_size))
         vaddr = M88K_ROUND_PAGE(vaddr + 1);
   }
   vaddr = PMAPER(
                 vaddr,
                 (vm_offset_t)kmap,
                 *phys_start,
                 (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1152 1
a1152 1
   if (vaddr != *virt_start) {
d1154 8
a1161 8
      if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
         printf("1:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
                *virt_start, *phys_start);
      }
#endif
      *virt_start = vaddr;
      *phys_start = round_page(*phys_start);
   }
d1164 16
a1179 16
   /*
    *  Get ethernet buffer - need etherlen bytes physically contiguous.
    *  1 to 1 mapped as well???. There is actually a bug in the macros
    *  used by the 1x7 ethernet driver. Remove this when that is fixed.
    *  XXX -nivas
    */
   if (cputyp != CPU_188) { /*  != CPU_188 */
      *phys_start = vaddr;
      etherlen = ETHERPAGES * NBPG;
      etherbuf = (void *)vaddr;

      vaddr = PMAPER(
                    vaddr,
                    *phys_start,
                    *phys_start + etherlen,
                    (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1181 2
a1182 2
      *virt_start += etherlen;
      *phys_start += etherlen; 
d1184 1
a1184 1
      if (vaddr != *virt_start) {
d1186 4
a1189 4
         if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
            printf("2:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
                   *virt_start, *phys_start);
         }
d1191 4
a1194 4
         *virt_start = vaddr;
         *phys_start = round_page(*phys_start);
      }
   }
d1198 2
a1199 2
   *virt_start = round_page(*virt_start);
   *virt_end = VM_MAX_KERNEL_ADDRESS;
d1201 24
a1224 24
   /*
    * Map a few more pages for phys routines and debugger.
    */

   phys_map_vaddr1 = round_page(*virt_start);
   phys_map_vaddr2 = phys_map_vaddr1 + PAGE_SIZE * max_cpus;

   /*
    * To make 1:1 mapping of virt:phys, throw away a few phys pages.
    * XXX what is this? nivas
    */

   *phys_start += 2 * PAGE_SIZE * max_cpus;
   *virt_start += 2 * PAGE_SIZE * max_cpus;

   /*
    * Map all IO space 1-to-1. Ideally, I would like to not do this
    * but have va for the given IO address dynamically allocated. But
    * on the 88200, 2 of the BATCs are hardwired to map the IO space
    * 1-to-1; I decided to map the rest of the IO space 1-to-1.
    * And bug ROM & the SRAM need to be mapped 1-to-1 if we ever want to
    * execute bug system calls after the MMU has been turned on.
    * OBIO should be mapped cache inhibited.
    */
d1226 1
a1226 1
   ptable = pmap_table_build(avail_end);    /* see pmap_table.c for details */
d1228 24
a1251 21
   printf("pmap_bootstrap: -> pmap_table_build\n");
#endif 
   for (  ; ptable->size != 0xffffffffU; ptable++)
      if (ptable->size) {
         /*
          * size-1, 'cause pmap_map rounds up to next pagenumber
          */
         PMAPER(ptable->virt_start,
                ptable->phys_start,
                ptable->phys_start + (ptable->size - 1),
                ptable->prot|(ptable->cacheability << 16));
   }

   /*
    * Allocate all the submaps we need. Note that SYSMAP just allocates
    * kernel virtual address with no physical backing memory. The idea
    * is physical memory will be mapped at this va before using that va.
    * This means that if different physcal pages are going to be mapped
    * at different times, we better do a tlb flush before using it -	         
    * else we will be referencing the wrong page.
    */
d1261 1
a1261 1
   virt = *virt_start;
d1263 2
a1264 2
   SYSMAP(caddr_t, vmpte , vmmap, 1);
   SYSMAP(struct msgbuf *, msgbufmap ,msgbufp, btoc(MSGBUFSIZE));
d1266 2
a1267 2
   vmpte->pfn = -1;
   vmpte->dtype = DT_INVALID;
d1269 1
a1269 1
   *virt_start = virt;
d1271 7
a1277 7
   /*
    * Set translation for UPAGES at UADDR. The idea is we want to
    * have translations set up for UADDR. Later on, the ptes for
    * for this address will be set so that kstack will refer
    * to the u area. Make sure pmap knows about this virtual
    * address by doing vm_findspace on kernel_map.
    */
d1279 1
a1279 1
   for (i = 0, virt = UADDR; i < UPAGES; i++, virt += PAGE_SIZE) {
d1281 18
a1298 18
      if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
         printf("setting up mapping for Upage %d @@ %x\n", i, virt);
      }
#endif
      if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
         pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
   }

   /*
    * Switch to using new page tables
    */

   apr_data.bits = 0;
   apr_data.field.st_base = M88K_BTOP(kernel_pmap->sdt_paddr);
   apr_data.field.wt = 1;
   apr_data.field.g  = 1;
   apr_data.field.ci = 0;
   apr_data.field.te = 1; /* Translation enable */
d1300 4
a1303 4
   if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
      void show_apr(unsigned value);
      show_apr(apr_data.bits);
   }
d1305 1
a1305 1
   /* Invalidate entire kernel TLB. */
d1307 3
a1309 3
   if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
      printf("invalidating tlb %x\n", apr_data.bits);
   }
d1312 4
a1315 4
   for (i = 0; i < MAX_CPUS; i++)
      if (cpu_sets[i]) {
         /* Invalidate entire kernel TLB. */
         cmmu_flush_remote_tlb(i, 1, 0, -1);
d1317 18
a1334 17
         if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
            printf("After cmmu_flush_remote_tlb()\n");
         }
#endif
         /* still physical */
         /*
          * Set valid bit to DT_INVALID so that the very first pmap_enter()
          * on these won't barf in pmap_remove_range().
          */
         pte = pmap_pte(kernel_pmap, phys_map_vaddr1);
         pte->pfn = -1;
         pte->dtype = DT_INVALID;
         pte = pmap_pte(kernel_pmap, phys_map_vaddr2);
         pte->dtype = DT_INVALID;
         pte->pfn = -1;
         /* Load supervisor pointer to segment table. */
         cmmu_remote_set_sapr(i, apr_data.bits);
d1336 6
a1341 7
         if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
            printf("After cmmu_remote_set_sapr()\n");
         }
#endif
         SETBIT_CPUSET(i, &kernel_pmap->cpus_using);
         /* Load supervisor pointer to segment table. */
      }
d1344 3
a1346 3
   if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
      printf("running virtual - avail_next 0x%x\n", *phys_start);
   }
d1348 2
a1349 4
   avail_next = *phys_start;

   return;

d1368 1
a1368 1
   register void *mem;
d1370 6
a1375 6
   size = round_page(size);
   mem = (void *)virtual_avail;
   virtual_avail = pmap_map(virtual_avail, avail_start,
                            avail_start + size,
                            VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
   avail_start += size;
d1377 4
a1380 4
   if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
      printf("pmap_bootstrap_alloc: size %x virtual_avail %x avail_start %x\n",
             size, virtual_avail, avail_start);
   }
d1382 2
a1383 2
   bzero((void *)mem, size);
   return (mem);
d1433 8
a1440 8
   register long    npages;
   register vm_offset_t	addr;
   register vm_size_t	s;
   register int     i;
   struct pv_entry *pv;
   char	*attr;
   struct simplelock *lock;
   int	bank;
d1443 2
a1444 2
   if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
	   printf("pmap_init()\n");
d1446 10
a1455 11

   /*
   * Allocate memory for the pv_head_table and its lock bits,
    * the modify bit array, and the pte_page table.
    */
   for (npages = 0, bank = 0; bank < vm_nphysseg; bank++)
      npages += vm_physmem[bank].end - vm_physmem[bank].start;
   
   s = PV_TABLE_SIZE(npages);			/* pv_list */
   s += PV_LOCK_TABLE_SIZE(npages);		/* pv_lock_table */
   s += npages * sizeof(char);			/* pmap_modify_list */
d1458 5
a1462 5
   if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
      printf("(pmap_init) nbr of managed pages = %x\n", npages);
      printf("(pmap_init) size of pv_list = %x\n",
             npages * sizeof(struct pv_entry));
   }
d1465 1
a1465 1
   s = round_page(s);
d1467 1
a1467 1
   addr = (vaddr_t)uvm_km_zalloc(kernel_map, s);
d1469 1
a1469 1
   addr = (vm_offset_t)kmem_alloc(kernel_map, s);
d1472 2
a1473 35
   pv_head_table =  (pv_entry_t)addr;
   addr += PV_TABLE_SIZE(npages);

   /*
    * Assume that 'simple_lock' is used to lock pv_lock_table
    */
   pv_lock_table = (struct simplelock *)addr; /* XXX */
   addr += PV_LOCK_TABLE_SIZE(npages);

   pmap_modify_list = (char *)addr;

   /*
   * Initialize pv_lock_table
   */
   for (i = 0; i < npages; i++)
      simple_lock_init(&(pv_lock_table[i]));
   
   /*
    * Now that the pv, attribute, and lock tables have been allocated,
    * assign them to the memory segments.
    */
   pv = pv_head_table;
   lock = pv_lock_table;
   attr = pmap_modify_list;
   for (bank = 0; bank < vm_nphysseg; bank++) {
	   npages = vm_physmem[bank].end - vm_physmem[bank].start;
	   vm_physmem[bank].pmseg.pvent = pv;
	   vm_physmem[bank].pmseg.attrs = attr;
	   vm_physmem[bank].pmseg.plock = lock;
	   pv += npages;
	   lock += npages;
	   attr += npages;
   }

   pmap_initialized = TRUE;
d1475 31
d1512 5
a1516 5
   register long    npages;
   register vm_offset_t      addr;
   register vm_size_t     s;
   register int     i;
   vm_size_t        pvl_table_size;
d1519 2
a1520 2
   if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
      printf("(pmap_init) phys_start %x  phys_end %x\n", phys_start, phys_end);
d1523 8
a1530 8
   /*
   * Allocate memory for the pv_head_table and its lock bits,
    * the modify bit array, and the pte_page table.
    */
   npages = atop(phys_end - phys_start);
   s = PV_TABLE_SIZE(npages);			/* pv_list */
   s += PV_LOCK_TABLE_SIZE(npages);		/* pv_lock_table */
   s += npages * sizeof(char);			/* pmap_modify_list */
d1533 8
a1540 36
   if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
      printf("(pmap_init) nbr of managed pages = %x\n", npages);
      printf("(pmap_init) size of pv_list = %x\n",
             npages * sizeof(struct pv_entry));
   }
#endif

   s = round_page(s);
   addr = (vm_offset_t)kmem_alloc(kernel_map, s);

   pv_head_table =  (pv_entry_t)addr;
   addr += PV_TABLE_SIZE(npages);

   /*
    * Assume that 'simple_lock' is used to lock pv_lock_table
    */
   pv_lock_table = (struct simplelock *)addr; /* XXX */
   addr += PV_LOCK_TABLE_SIZE(npages);

   pmap_modify_list = (char *)addr;

   /*
   * Initialize pv_lock_table
   */
   for (i = 0; i < npages; i++)
      simple_lock_init(&(pv_lock_table[i]));

   /*
     * Only now, when all of the data structures are allocated,
     * can we set pmap_phys_start and pmap_phys_end. If we set them
     * too soon, the kmem_alloc above will blow up when it causes
     * a call to pmap_enter, and pmap_enter tries to manipulate the
     * (not yet existing) pv_list.
     */
   pmap_phys_start = phys_start;
   pmap_phys_end = phys_end;
d1542 28
a1569 1
   pmap_initialized = TRUE;
d1613 24
a1636 26
   vm_offset_t srcva;
   pte_template_t    template   ;
   unsigned int i;
   unsigned int spl_sav;
   int   my_cpu;
   pt_entry_t  *srcpte;

   my_cpu = cpu_number();
   srcva = (vm_offset_t)(phys_map_vaddr1 + (my_cpu * PAGE_SIZE));
   srcpte = pmap_pte(kernel_pmap, srcva);

   for (i = 0; i < ptes_per_vm_page; i++, phys += M88K_PGBYTES) {
      template.bits = M88K_TRUNC_PAGE(phys)
                      | m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
                      | DT_VALID | CACHE_GLOBAL;


      spl_sav = splimp();
      cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
      *srcpte = template.pte;
      splx(spl_sav);
      bzero (srcva, M88K_PGBYTES);
      /* force the data out */
      cmmu_flush_remote_data_cache(my_cpu,phys, M88K_PGBYTES);
   }

a1638 1

d1669 14
a1682 1
   pmap_t      p;
d1684 3
a1686 16
   /*
    * A software use-only map doesn't even need a map.
    */
   if (size != 0)
      return (PMAP_NULL);

   CHECK_PMAP_CONSISTENCY("pmap_create");

   p = (pmap_t)malloc(sizeof(*p), M_VMPMAP, M_WAITOK);
   if (p == PMAP_NULL) {
      panic("pmap_create: cannot allocate a pmap");
   }

   bzero(p, sizeof(*p));
   pmap_pinit(p);
   return (p);
d1693 21
a1713 7
   pmap_statistics_t   stats;
   sdt_entry_t      *segdt;
   int                 i;

   /*
    * Allocate memory for *actual* segment table and *shadow* table.
    */
d1715 1
a1715 1
   segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, 2 * SDT_SIZE);
d1717 1
a1717 1
   segdt = (sdt_entry_t *)kmem_alloc(kernel_map, 2 * SDT_SIZE);
d1719 2
a1720 2
   if (segdt == NULL)
      panic("pmap_create: kmem_alloc failure");
d1722 17
a1738 21
#if 0
   /* maybe, we can use bzero to zero out the segdt. XXX nivas */
   bzero(segdt, 2 * SDT_SIZE); 
#endif /* 0 */
   /* use pmap zero page to zero it out */
   pmap_zero_page(pmap_extract(kernel_pmap,(vm_offset_t)segdt));
   if (PAGE_SIZE == SDT_SIZE)  /* only got half */
      pmap_zero_page(pmap_extract(kernel_pmap,(vm_offset_t)segdt+PAGE_SIZE));
   if (PAGE_SIZE < 2*SDT_SIZE) /* get remainder */
      bzero((vm_offset_t)segdt+PAGE_SIZE, (2*SDT_SIZE)-PAGE_SIZE);

   /*
    * Initialize pointer to segment table both virtual and physical.
    */
   p->sdt_vaddr = segdt;
   p->sdt_paddr = (sdt_entry_t *)pmap_extract(kernel_pmap,(vm_offset_t)segdt);

   if (!PAGE_ALIGNED(p->sdt_paddr)) {
      printf("pmap_create: std table = %x\n",(int)p->sdt_paddr);
      panic("pmap_create: sdt_table not aligned on page boundary");
   }
d1741 29
a1769 29
   if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
      printf("(pmap_create :%x) pmap=0x%x, sdt_vaddr=0x%x, sdt_paddr=0x%x\n",
             curproc, (unsigned)p, p->sdt_vaddr, p->sdt_paddr);
   }
#endif

#if notneeded
   /*
    * memory for page tables should be CACHE DISABLED?
    */
   pmap_cache_ctrl(kernel_pmap,
                   (vm_offset_t)segdt,
                   (vm_offset_t)segdt+ (SDT_SIZE*2),
                   CACHE_INH);
#endif
   /*
    * Initialize SDT_ENTRIES.
    */
   /*
    * There is no need to clear segment table, since kmem_alloc would
    * provides us clean pages.
    */

   /*
    * Initialize pmap structure.
    */
   p->ref_count = 1;
   simple_lock_init(&p->lock);
   p->cpus_using = 0;
d1772 5
a1776 5
   /* initialize block address translation cache */
   for (i = 0; i < BATC_MAX; i++) {
      p->i_batc[i].bits = 0;
      p->d_batc[i].bits = 0;
   }
d1779 6
a1784 6
   /*
    * Initialize statistics.
    */
   stats = &p->stats;
   stats->resident_count = 0;
   stats->wired_count = 0;
d1787 5
a1791 5
   /* link into list of pmaps, just after kernel pmap */
   p->next = kernel_pmap->next;
   p->prev = kernel_pmap;
   kernel_pmap->next = p;
   p->next->prev = p;
a1819 1

d1823 4
a1826 4
   unsigned long sdt_va;  /*  outer loop index */
   sdt_entry_t   *sdttbl; /*  ptr to first entry in the segment table */
   pt_entry_t  *gdttbl; /*  ptr to first entry in a page table */
   unsigned int i,j;
d1829 2
a1830 2
   if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
      printf("(pmap_free_tables :%x) pmap %x\n", curproc, pmap);
d1833 14
a1846 15
   sdttbl = pmap->sdt_vaddr;    /* addr of segment table */

   /* 
     This contortion is here instead of the natural loop 
     because of integer overflow/wraparound if VM_MAX_USER_ADDRESS is near 0xffffffff
   */

   i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
   j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
   if ( j < 1024 ) j++;

   /* Segment table Loop */
   for ( ; i < j; i++) {
      sdt_va = PDT_TABLE_GROUP_VA_SPACE*i;
      if ((gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va)) != PT_ENTRY_NULL) {
d1848 7
a1854 7
         if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
            printf("(pmap_free_tables :%x) free page table = 0x%x\n", curproc, gdttbl);
#endif
         PT_FREE(gdttbl);
      }

   } /* Segment Loop */
d1857 7
a1863 6
   if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
      printf("(pmap_free_tables :%x) free segment table = 0x%x\n", curproc, sdttbl);
#endif
   /*
    * Freeing both *actual* and *shadow* segment tables
    */
d1865 1
a1865 1
   uvm_km_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
d1867 1
a1867 1
   kmem_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
a1871 1

d1875 1
a1875 1
   pmap_free_tables(p);
d1877 5
a1881 5
   if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
   printf("(pmap_destroy :%x) ref_count = 0\n", curproc);
   /* unlink from list of pmap structs */
   p->prev->next = p->next;
   p->next->prev = p->prev;
d1913 1
a1913 1
   register int  c, s;
d1915 1
a1915 1
   if (p == PMAP_NULL) {
d1917 2
a1918 2
      if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
         printf("(pmap_destroy :%x) pmap is NULL\n", curproc);
d1920 6
a1925 2
      return;
   }
d1927 10
a1936 14
   if (p == kernel_pmap) {
      panic("pmap_destroy: Attempt to destroy kernel pmap");
   }

   CHECK_PMAP_CONSISTENCY("pmap_destroy");

   PMAP_LOCK(p, s);
   c = --p->ref_count;
   PMAP_UNLOCK(p, s);

   if (c == 0) {
      pmap_release(p);
      free((caddr_t)p,M_VMPMAP);
   }
d1959 1
a1959 1
   int     s;
d1961 5
a1965 5
   if (p != PMAP_NULL) {
      PMAP_LOCK(p, s);
      p->ref_count++;
      PMAP_UNLOCK(p, s);
   }
a1968 1

d2026 147
a2172 147
   int        pfi;
   int        pfn;
   int        num_removed = 0,
   num_unwired = 0;
   register int  i;
   pt_entry_t    *pte;
   pv_entry_t    prev, cur;
   pv_entry_t    pvl;
   vm_offset_t      pa, va, tva;
   register unsigned      users;
   register pte_template_t   opte;
   int           kflush;

   if (e < s)
      panic("pmap_remove_range: end < start");

   /*
    * Pmap has been locked by pmap_remove.
    */
   users = pmap->cpus_using;
   if (pmap == kernel_pmap) {
      kflush = 1;
   } else {
      kflush = 0;
   }

   /*
    * Loop through the range in vm_page_size increments.
    * Do not assume that either start or end fail on any
    * kind of page boundary (though this may be true!?).
    */

   CHECK_PAGE_ALIGN(s, "pmap_remove_range - start addr");

   for (va = s; va < e; va += PAGE_SIZE) {

      sdt_entry_t *sdt;

      sdt = SDTENT(pmap,va);

      if (!SDT_VALID(sdt)) {
         va &= SDT_MASK; /* align to segment */
         if (va <= e - (1<<SDT_SHIFT))
            va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
         else /* wrap around */
            break;
         continue;
      }

      pte = pmap_pte(pmap,va);

      if (!PDT_VALID(pte)) {
         continue;        /* no page mapping */
      }

      num_removed++;

      if (pte->wired)
         num_unwired++;

      pfn = pte->pfn;
      pa = M88K_PTOB(pfn);

      if (PMAP_MANAGED(pa)) {
         LOCK_PVH(pa);
         /*
          * Remove the mapping from the pvlist for
          * this physical page.
          */
         pvl = PA_TO_PVH(pa);
         CHECK_PV_LIST(pa, pvl, "pmap_remove_range before");

         if (pvl->pmap == PMAP_NULL)
            panic("pmap_remove_range: null pv_list");

         if (pvl->va == va && pvl->pmap == pmap) {

            /*
             * Hander is the pv_entry. Copy the next one
             * to hander and free the next one (we can't
             * free the hander)
             */
            cur = pvl->next;
            if (cur != PV_ENTRY_NULL) {
               *pvl = *cur;
               free((caddr_t)cur, M_VMPVENT);
            } else {
               pvl->pmap =  PMAP_NULL;
            }

         } else {

            for (prev = pvl; (cur = prev->next) != PV_ENTRY_NULL; prev = cur) {
               if (cur->va == va && cur->pmap == pmap) {
                  break;
               }
            }
            if (cur == PV_ENTRY_NULL) {
               printf("pmap_remove_range: looking for VA "
                      "0x%x (pa 0x%x) PV list at 0x%x\n", va, pa, (unsigned)pvl);
               panic("pmap_remove_range: mapping not in pv_list");
            }

            prev->next = cur->next;
            free((caddr_t)cur, M_VMPVENT);
         }

         CHECK_PV_LIST(pa, pvl, "pmap_remove_range after");
         UNLOCK_PVH(pa);

      } /* if PAGE_MANAGED */

      /*
       * For each pte in vm_page (NOTE: vm_page, not
       * M88K (machine dependent) page !! ), reflect
       * modify bits to pager and zero (invalidate,
       * remove) the pte entry.
       */
      tva = va;
      for (i = ptes_per_vm_page; i > 0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         opte.bits = invalidate_pte(pte);
         flush_atc_entry(users, tva, kflush);

         if (opte.pte.modified) {
            if (IS_VM_PHYSADDR(pa)) {
               vm_page_set_modified(PHYS_TO_VM_PAGE(opte.bits & M88K_PGMASK));
            }
            /* keep track ourselves too */
            if (PMAP_MANAGED(pa))
               SET_ATTRIB(pa, 1);
         }
         pte++;
         tva += M88K_PGBYTES;
      }

   } /* end for ( va = s; ...) */

   /*
    * Update the counts
    */
   pmap->stats.resident_count -= num_removed;
   pmap->stats.wired_count -= num_unwired;
d2201 1
a2201 5
   int     spl;

   if (map == PMAP_NULL) {
      return;
   }
d2203 3
d2207 2
a2208 2
   if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
      printf("(pmap_remove :%x) map %x  s %x  e %x\n", curproc, map, s, e);
d2211 1
a2211 1
   CHECK_PAGE_ALIGN(s, "pmap_remove start addr");
d2213 2
a2214 2
   if (s>e)
      panic("pmap_remove: start greater than end address");
d2216 3
a2218 3
   PMAP_LOCK(map, spl);
   pmap_remove_range(map, s, e);
   PMAP_UNLOCK(map, spl);
a2220 1

d2265 10
a2274 11
   pv_entry_t       pvl, cur;
   register pt_entry_t    *pte;
   int           pfi;
   register int     i;
   register vm_offset_t   va;
   register pmap_t     pmap;
   int           spl;
   int           dbgcnt = 0;
   register unsigned      users;
   register pte_template_t   opte;
   int           kflush;
d2276 2
a2277 2
   if (!PMAP_MANAGED(phys)) {
      /* not a managed page. */
d2279 2
a2280 2
      if (pmap_con_dbg & CD_RMAL)
         printf("(pmap_remove_all :%x) phys addr 0x%x not a managed page\n", curproc, phys);
d2282 30
a2311 2
      return;
   }
d2313 1
a2313 1
   SPLVM(spl);
d2315 23
a2337 84
   /*
    * Walk down PV list, removing all mappings.
    * We have to do the same work as in pmap_remove_pte_page
    * since that routine locks the pv_head. We don't have
    * to lock the pv_head, since we have the entire pmap system.
    */
   remove_all_Retry:

   pvl = PA_TO_PVH(phys);
   CHECK_PV_LIST(phys, pvl, "pmap_remove_all before");
   LOCK_PVH(phys);

   /*
    * Loop for each entry on the pv list
    */
   while ((pmap = pvl->pmap) != PMAP_NULL) {
      va = pvl->va;
      if (!simple_lock_try(&pmap->lock)) {
         UNLOCK_PVH(phys);
         goto remove_all_Retry;
      }
      users = pmap->cpus_using;
      if (pmap == kernel_pmap) {
         kflush = 1;
      } else {
         kflush = 0;
      }

      pte = pmap_pte(pmap, va);

      /*
       * Do a few consistency checks to make sure
       * the PV list and the pmap are in synch.
       */
      if (pte == PT_ENTRY_NULL) {
         printf("(pmap_remove_all :%x) phys %x pmap %x va %x dbgcnt %x\n",
                (unsigned)curproc, phys, (unsigned)pmap, va, dbgcnt);
         panic("pmap_remove_all: pte NULL");
      }
      if (!PDT_VALID(pte))
         panic("pmap_remove_all: pte invalid");
      if (M88K_PTOB(pte->pfn) != phys)
         panic("pmap_remove_all: pte doesn't point to page");
      if (pte->wired)
         panic("pmap_remove_all: removing  a wired page");

      pmap->stats.resident_count--;

      if ((cur = pvl->next) != PV_ENTRY_NULL) {
         *pvl  = *cur;
         free((caddr_t)cur, M_VMPVENT);
      } else
         pvl->pmap = PMAP_NULL;

      /*
       * Reflect modified pages to pager.
       */
      for (i = ptes_per_vm_page; i>0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         opte.bits = invalidate_pte(pte);
         flush_atc_entry(users, va, kflush);

         if (opte.pte.modified) {
            vm_page_set_modified((vm_page_t)PHYS_TO_VM_PAGE(phys));
            /* keep track ourselves too */
            SET_ATTRIB(phys, 1);
         }
         pte++;
         va += M88K_PGBYTES;
      }

      /*
       * Do not free any page tables,
       * leaves that for when VM calls pmap_collect().
       */

      simple_unlock(&pmap->lock);
      dbgcnt++;
   }
   CHECK_PV_LIST(phys, pvl, "pmap_remove_all after");
d2339 33
a2371 2
   UNLOCK_PVH(phys);
   SPLX(spl);
d2404 7
a2410 7
   register pv_entry_t    pv_e;
   register pt_entry_t    *pte;
   register int     i;
   int           spl, spl_sav;
   register unsigned      users;
   register pte_template_t   opte;
   int           kflush;
d2412 1
a2412 1
   if (!PMAP_MANAGED(phys)) {
d2414 2
a2415 2
      if (pmap_con_dbg & CD_CMOD)
         printf("(pmap_copy_on_write :%x) phys addr 0x%x not managed \n", curproc, phys);
d2417 2
a2418 2
      return;
   }
d2420 1
a2420 1
   SPLVM(spl);
d2423 5
a2427 5
   pv_e = PA_TO_PVH(phys);
   CHECK_PV_LIST(phys, pv_e, "pmap_copy_on_write before");
   LOCK_PVH(phys);
   if (pv_e->pmap  == PMAP_NULL) {

d2429 2
a2430 2
      if ((pmap_con_dbg & (CD_COW | CD_NORM)) == (CD_COW | CD_NORM))
         printf("(pmap_copy_on_write :%x) phys addr 0x%x not mapped\n", curproc, phys);
d2432 4
d2437 23
a2459 2
      UNLOCK_PVH(phys);
      SPLX(spl);
d2461 34
a2494 62
      return;     /* no mappings */
   }

   /*
    * Run down the list of mappings to this physical page,
    * disabling write privileges on each one.
    */

   while (pv_e != PV_ENTRY_NULL) {
      pmap_t      pmap;
      vm_offset_t va;

      pmap = pv_e->pmap;
      va = pv_e->va;

      if (!simple_lock_try(&pmap->lock)) {
         UNLOCK_PVH(phys);
         goto copy_on_write_Retry;
      }

      users = pmap->cpus_using;
      if (pmap == kernel_pmap) {
         kflush = 1;
      } else {
         kflush = 0;
      }

      /*
       * Check for existing and valid pte
       */
      pte = pmap_pte(pmap, va);
      if (pte == PT_ENTRY_NULL)
         panic("pmap_copy_on_write: pte from pv_list not in map");
      if (!PDT_VALID(pte))
         panic("pmap_copy_on_write: invalid pte");
      if (M88K_PTOB(pte->pfn) != phys)
         panic("pmap_copy_on_write: pte doesn't point to page");

      /*
       * Flush TLBs of which cpus using pmap.
       */

      for (i = ptes_per_vm_page; i > 0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         spl_sav = splimp();
         opte.bits = invalidate_pte(pte);
         opte.pte.prot = M88K_RO;
         ((pte_template_t *)pte)->bits = opte.bits;
         flush_atc_entry(users, va, kflush);
         splx(spl_sav);
         pte++;
         va += M88K_PGBYTES;
      }

      simple_unlock(&pmap->lock);
      pv_e = pv_e->next;
   }
   CHECK_PV_LIST(phys, PA_TO_PVH(phys), "pmap_copy_on_write");
d2496 2
a2497 2
   UNLOCK_PVH(phys);
   SPLX(spl);
d2531 33
a2563 50
   pte_template_t      maprot;
   unsigned         ap;
   int           spl, spl_sav;
   register int     i;
   pt_entry_t       *pte;
   vm_offset_t         va, tva;
   register unsigned      users;
   register pte_template_t   opte;
   int           kflush;

   if (pmap == PMAP_NULL || prot & VM_PROT_WRITE)
      return;
   if ((prot & VM_PROT_READ) == 0) {
      pmap_remove(pmap, s, e);
      return;
   }

   if (s > e)
      panic("pmap_protect: start grater than end address");

   maprot.bits = m88k_protection(pmap, prot);
   ap = maprot.pte.prot;

   PMAP_LOCK(pmap, spl);

   users = pmap->cpus_using;
   if (pmap == kernel_pmap) {
      kflush = 1;
   } else {
      kflush = 0;
   }

   CHECK_PAGE_ALIGN(s, "pmap_protect");

   /*
    * Loop through the range in vm_page_size increment.
    * Do not assume that either start or end fall on any
    * kind of page boundary (though this may be true ?!).
    */
   for (va = s; va <= e; va += PAGE_SIZE) {

      pte = pmap_pte(pmap, va);

      if (pte == PT_ENTRY_NULL) {

         va &= SDT_MASK; /* align to segment */
         if (va <= e - (1<<SDT_SHIFT))
            va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
         else /* wrap around */
            break;
d2565 13
d2579 2
a2580 2
         if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
            printf("(pmap_protect :%x) no page table :: skip to 0x%x\n", curproc, va + PAGE_SIZE);
d2582 2
a2583 2
         continue;
      }
d2585 1
a2585 1
      if (!PDT_VALID(pte)) {
d2587 2
a2588 2
         if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
            printf("(pmap_protect :%x) pte invalid pte @@ 0x%x\n", curproc, pte);
d2590 2
a2591 2
         continue;        /*  no page mapping */
      }
d2593 1
a2593 1
      printf("(pmap_protect :%x) pte good\n", curproc);
d2595 18
a2612 21

      tva = va;
      for (i = ptes_per_vm_page; i>0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         spl_sav = splimp();
         opte.bits = invalidate_pte(pte);
         opte.pte.prot = ap;
         ((pte_template_t *)pte)->bits = opte.bits;
         flush_atc_entry(users, tva, kflush);
         splx(spl_sav);
         pte++;
         tva += M88K_PGBYTES;
      }
   }

   PMAP_UNLOCK(pmap, spl);

a2614 2


d2662 9
a2670 12
   int      i,
   spl;
   vm_offset_t pdt_vaddr,
   pdt_paddr;

   sdt_entry_t *sdt;
   pt_entry_t     *pte;
   vm_offset_t pmap_extract();

   if (map == PMAP_NULL) {
      panic("pmap_expand: pmap is NULL");
   }
d2673 2
a2674 2
   if ((pmap_con_dbg & (CD_EXP | CD_NORM)) == (CD_EXP | CD_NORM))
      printf ("(pmap_expand :%x) map %x v %x\n", curproc, map, v);
d2677 1
a2677 1
   CHECK_PAGE_ALIGN (v, "pmap_expand");
d2679 8
a2686 8
   /*
    * Handle kernel pmap in pmap_expand_kmap().
    */
   if (map == kernel_pmap) {
      PMAP_LOCK(map, spl);
      if (pmap_expand_kmap(v, VM_PROT_READ|VM_PROT_WRITE) == PT_ENTRY_NULL)
         panic ("pmap_expand: Cannot allocate kernel pte table");
      PMAP_UNLOCK(map, spl);
d2688 2
a2689 2
      if ((pmap_con_dbg & (CD_EXP | CD_FULL)) == (CD_EXP | CD_FULL))
         printf("(pmap_expand :%x) kernel_pmap\n", curproc);
d2691 2
a2692 2
      return;
   }
d2694 1
a2694 6
   /* XXX */
#ifdef MACH_KERNEL
   if (kmem_alloc_wired(kernel_map, &pdt_vaddr, PAGE_SIZE) != KERN_SUCCESS)
      panic("pmap_enter: kmem_alloc failure");
   pmap_zero_page(pmap_extract(kernel_pmap, pdt_vaddr));
#else
d2696 1
a2696 1
   pdt_vaddr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
d2698 1
a2698 2
   pdt_vaddr = kmem_alloc (kernel_map, PAGE_SIZE);
#endif
d2700 1
d2702 6
a2707 1
   pdt_paddr = pmap_extract(kernel_pmap, pdt_vaddr);
d2709 1
a2709 6
#if notneeded
   /*
    * the page for page tables should be CACHE DISABLED
    */
   pmap_cache_ctrl(kernel_pmap, pdt_vaddr, pdt_vaddr+PAGE_SIZE, CACHE_INH);
#endif
d2711 7
a2717 9
   PMAP_LOCK(map, spl);

   if ((pte = pmap_pte(map, v)) != PT_ENTRY_NULL) {
      /*
       * Someone else caused us to expand
       * during our vm_allocate.
       */
      PMAP_UNLOCK(map, spl);
      /* XXX */
d2719 1
a2719 1
      uvm_km_free(kernel_map, pdt_vaddr, PAGE_SIZE);
d2721 1
a2721 1
      kmem_free (kernel_map, pdt_vaddr, PAGE_SIZE);
d2723 1
a2723 1
      
d2725 2
a2726 2
      if (pmap_con_dbg & CD_EXP)
         printf("(pmap_expand :%x) table has already allocated\n", curproc);
d2728 26
a2753 30
      return;
   }

   /*
    * Apply a mask to V to obtain the vaddr of the beginning of
    * its containing page 'table group',i.e. the group of
    * page  tables that fit eithin a single VM page.
    * Using that, obtain the segment table pointer that references the
    * first page table in the group, and initilize all the
    * segment table descriptions for the page 'table group'.
    */
   v &= ~((1<<(LOG2_PDT_TABLE_GROUP_SIZE+PDT_BITS+PG_BITS))-1);

   sdt = SDTENT(map,v);

   /*
    * Init each of the segment entries to point the freshly allocated
    * page tables.
    */

   for (i = PDT_TABLE_GROUP_SIZE; i>0; i--) {
      ((sdt_entry_template_t *)sdt)->bits = pdt_paddr | M88K_RW | DT_VALID;
      ((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = pdt_vaddr | M88K_RW | DT_VALID;
      sdt++;
      pdt_paddr += PDT_SIZE;
      pdt_vaddr += PDT_SIZE;
   }

   PMAP_UNLOCK(map, spl);

a2755 2


d2836 2
a2837 2
		vm_prot_t prot, boolean_t wired,
	        vm_prot_t access_type)
d2839 22
a2860 23
   int           ap;
   int           spl, spl_sav;
   pv_entry_t       pv_e;
   pt_entry_t       *pte;
   vm_offset_t         old_pa;
   pte_template_t      template;
   register int     i;
   int           pfi;
   pv_entry_t       pvl;
   register unsigned      users;
   register pte_template_t   opte;
   int           kflush;

   if (pmap == PMAP_NULL) {
      panic("pmap_enter: pmap is NULL");
   }

   CHECK_PAGE_ALIGN (va, "pmap_entry - VA");
   CHECK_PAGE_ALIGN (pa, "pmap_entry - PA");

   /*
    *	Range check no longer use, since we use whole address space
    */
d2863 18
a2880 18
   if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
      if (pmap == kernel_pmap)
         printf ("(pmap_enter :%x) pmap kernel va %x pa %x\n", curproc, va, pa);
      else
         printf ("(pmap_enter :%x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
   }
#endif

   ap = m88k_protection (pmap, prot);

   /*
    *	Must allocate a new pvlist entry while we're unlocked;
    *	zalloc may cause pageout (which will lock the pmap system).
    *	If we determine we need a pvlist entry, we will unlock
    *	and allocate one. Then will retry, throwing away
    *	the allocated entry later (if we no longer need it).
    */
   pv_e = PV_ENTRY_NULL;
d2883 67
a2949 1
   PMAP_LOCK(pmap, spl);
d2951 9
a2959 77
   /*
    * Expand pmap to include this pte. Assume that
    * pmap is always expanded to include enough M88K
    * pages to map one VM page.
    */
   while ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
      /*
       * Must unlock to expand the pmap.
       */
      PMAP_UNLOCK(pmap, spl);
      pmap_expand(pmap, va);
      PMAP_LOCK(pmap, spl);
   }

   /*
    *	Special case if the physical page is already mapped
    *	at this address.
    */
   old_pa = M88K_PTOB(pte->pfn);
   if (old_pa == pa) {

      users = pmap->cpus_using;
      if (pmap == kernel_pmap) {
         kflush = 1;
      } else {
         kflush = 0;
      }

      /*
       * May be changing its wired attributes or protection
       */

      if (wired && !pte->wired)
         pmap->stats.wired_count++;
      else if (!wired && pte->wired)
         pmap->stats.wired_count--;

      if ((unsigned long)pa >= MAXPHYSMEM)
         template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
      else
         template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
      if (wired)
         template.pte.wired = 1;

      /*
       * If there is a same mapping, we have nothing to do.
       */
      if ( !PDT_VALID(pte) || (pte->wired != template.pte.wired)
           || (pte->prot != template.pte.prot)) {

         for (i = ptes_per_vm_page; i>0; i--) {

            /*
             * Invalidate pte temporarily to avoid being written back
             * the modified bit and/or the reference bit by other cpu.
             */
            spl_sav = splimp();
            opte.bits = invalidate_pte(pte);
            template.pte.modified = opte.pte.modified;
            *pte++ = template.pte;
            flush_atc_entry(users, va, kflush);
            splx(spl_sav);
            template.bits += M88K_PGBYTES;
            va += M88K_PGBYTES;
         }
      }

   } else { /* if ( pa == old_pa) */

      /*
       * Remove old mapping from the PV list if necessary.
       */
      if (old_pa != (vm_offset_t)-1) {
         /*
          *	Invalidate the translation buffer,
          *	then remove the mapping.
          */
d2961 16
a2976 16
         if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
            if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
               printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
                      phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
                      PMAP_MANAGED(pa) ? 1 : 0);
               printf("pte %x pfn %x valid %x\n",
                      pte, pte->pfn, pte->dtype);
            }
         }
#endif
         if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
            flush_atc_entry(users, va, 1);
         } else {
            pmap_remove_range(pmap, va, va + PAGE_SIZE);
         }
      }
d2978 1
a2978 1
      if (PMAP_MANAGED(pa)) {
d2980 21
a3000 22
         if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
            if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
               printf("va 0x%x and managed pa 0x%x\n", va, pa);
            }
         }
#endif
         /*
          *	Enter the mappimg in the PV list for this
          *	physical page.
          */
         LOCK_PVH(pa);
         pvl = PA_TO_PVH(pa);
         CHECK_PV_LIST (pa, pvl, "pmap_enter before");

         if (pvl->pmap == PMAP_NULL) {

            /*
             *	No mappings yet
             */
            pvl->va = va;
            pvl->pmap = pmap;
            pvl->next = PV_ENTRY_NULL;
d3002 1
a3002 1
         } else {
d3004 34
a3037 48
            /*
             * check that this mapping is not already there
             */
            {
               pv_entry_t e = pvl;
               while (e != PV_ENTRY_NULL) {
                  if (e->pmap == pmap && e->va == va)
                     panic ("pmap_enter: already in pv_list");
                  e = e->next;
               }
            }
#endif
            /*
             *	Add new pv_entry after header.
             */
            if (pv_e == PV_ENTRY_NULL) {
               UNLOCK_PVH(pa);
               PMAP_UNLOCK(pmap, spl);
               pv_e = (pv_entry_t) malloc(sizeof *pv_e, M_VMPVENT,
                                          M_NOWAIT);
               goto Retry;
            }
            pv_e->va = va;
            pv_e->pmap = pmap;
            pv_e->next = pvl->next;
            pvl->next = pv_e;
            /*
             *		Remeber that we used the pvlist entry.
             */
            pv_e = PV_ENTRY_NULL;
         }
         UNLOCK_PVH(pa);
      }

      /*
       * And count the mapping.
       */
      pmap->stats.resident_count++;
      if (wired)
         pmap->stats.wired_count++;

      if ((unsigned long)pa >= MAXPHYSMEM)
         template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
      else
         template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;

      if (wired)
         template.pte.wired = 1;
d3039 11
a3049 1
      DO_PTES (pte, template.bits);
d3051 2
a3052 1
   } /* if ( pa == old_pa ) ... else */
d3054 1
a3054 1
   PMAP_UNLOCK(pmap, spl);
d3056 1
a3056 2
   if (pv_e != PV_ENTRY_NULL)
      free((caddr_t) pv_e, M_VMPVENT);
d3058 1
a3058 1
} /* pmap_enter */
d3060 2
d3063 1
d3091 3
a3093 20
   pt_entry_t  *pte;
   int      i;
   int      spl;

   PMAP_LOCK(map, spl);

   if ((pte = pmap_pte(map, v)) == PT_ENTRY_NULL)
      panic ("pmap_change_wiring: pte missing");

   if (wired && !pte->wired)
      /*
       *	wiring mapping
       */
      map->stats.wired_count++;

   else if (!wired && pte->wired)
      /*
       *	unwired mapping
       */
      map->stats.wired_count--;
d3095 1
a3095 2
   for (i = ptes_per_vm_page; i>0; i--)
      (pte++)->wired = wired;
d3097 2
a3098 1
   PMAP_UNLOCK(map, spl);
d3100 7
a3106 1
} /* pmap_change_wiring() */
d3108 2
d3111 3
d3145 29
a3173 31
   register pt_entry_t  *pte;
   register vm_offset_t pa;
   register int   i;
   int         spl;

   if (pmap == PMAP_NULL)
      panic("pmap_extract: pmap is NULL");

   /*
    * check BATC first
    */
   if (pmap == kernel_pmap && batc_used > 0)
      for (i = batc_used-1; i > 0; i--)
         if (batc_entry[i].lba == M88K_BTOBLK(va)) {
            pa = (batc_entry[i].pba << BATC_BLKSHIFT) | (va & BATC_BLKMASK );
            return (pa);
         }

   PMAP_LOCK(pmap, spl);

   if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
      pa = (vm_offset_t) 0;
   else {
      if (PDT_VALID(pte))
         pa = M88K_PTOB(pte->pfn);
      else
         pa = (vm_offset_t) 0;
   }

   if (pa)
      pa |= (va & M88K_PGOFSET); /* offset within page */
d3175 2
a3176 6
   PMAP_UNLOCK(pmap, spl);

#if 0
   printf("pmap_extract ret %x\n", pa);
#endif /* 0 */
   return (pa);
d3178 2
d3189 26
a3214 25
   pt_entry_t *pte;
   vm_offset_t   pa;
   int     i;

   if (pmap == PMAP_NULL)
      panic("pmap_extract: pmap is NULL");

   /*
    * check BATC first
    */
   if (pmap == kernel_pmap && batc_used > 0)
      for (i = batc_used-1; i > 0; i--)
         if (batc_entry[i].lba == M88K_BTOBLK(va)) {
            pa = (batc_entry[i].pba << BATC_BLKSHIFT) | (va & BATC_BLKMASK );
            return (pa);
         }

   if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
      pa = (vm_offset_t) 0;
   else {
      if (PDT_VALID(pte))
         pa = M88K_PTOB(pte->pfn);
      else
         pa = (vm_offset_t) 0;
   }
d3216 2
a3217 2
   if (pa)
      pa |= (va & M88K_PGOFSET); /* offset within page */
d3219 1
a3219 1
   return (pa);
d3245 1
a3245 1
          vm_size_t len, vm_offset_t src_addr)
d3248 1
a3248 1
   dst_pmap++; src_pmap++; dst_addr++; len++; src_addr++;
d3280 2
a3281 2
   if ((pmap_con_dbg & (CD_UPD | CD_FULL)) == (CD_UPD | CD_FULL))
      printf("(pmap_update :%x) Called \n", curproc);
d3331 12
a3342 12
   vm_offset_t      sdt_va;  /* outer loop index */
   vm_offset_t      sdt_vt; /* end of segment */
   sdt_entry_t   *sdttbl; /* ptr to first entry in the segment table */
   sdt_entry_t   *sdtp;      /* ptr to index into segment table */
   sdt_entry_t   *sdt;    /* ptr to index into segment table */
   pt_entry_t *gdttbl; /* ptr to first entry in a page table */
   pt_entry_t *gdttblend; /* ptr to byte after last entry in table group */
   pt_entry_t *gdtp;      /* ptr to index into a page table */
   boolean_t  found_gdt_wired; /* flag indicating a wired page exists in */
   /* a page table's address range 	   */
   int     spl;
   unsigned int i,j;
d3346 4
a3349 4
   if (pmap == PMAP_NULL) {
      panic("pmap_collect: pmap is NULL");
   }
   if (pmap == kernel_pmap) {
d3351 1
a3351 1
      return;
d3353 1
a3353 1
      panic("pmap_collect attempted on kernel pmap");
d3355 1
a3355 1
   }
d3357 1
a3357 1
   CHECK_PMAP_CONSISTENCY ("pmap_collect");
d3360 2
a3361 2
   if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
      printf ("(pmap_collect :%x) pmap %x\n", curproc, pmap);
d3364 1
a3364 1
   PMAP_LOCK(pmap, spl);
d3366 2
a3367 2
   sdttbl = pmap->sdt_vaddr; /* addr of segment table */
   sdtp = sdttbl;
d3369 5
a3373 4
   /* 
     This contortion is here instead of the natural loop 
     because of integer overflow/wraparound if VM_MAX_USER_ADDRESS is near 0xffffffff
   */
d3375 3
a3377 3
   i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
   j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
   if ( j < 1024 ) j++;
d3379 3
a3381 3
   /* Segment table loop */
   for ( ; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE) {
      sdt_va = VM_MIN_USER_ADDRESS + PDT_TABLE_GROUP_VA_SPACE*i;
d3383 1
a3383 1
      gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va);
d3385 2
a3386 2
      if (gdttbl == PT_ENTRY_NULL)
         continue; /* no maps in this range */
d3388 1
a3388 1
      gdttblend = gdttbl + (PDT_ENTRIES * PDT_TABLE_GROUP_SIZE);
d3390 8
a3397 8
      /* scan page maps for wired pages */
      found_gdt_wired = FALSE;
      for (gdtp=gdttbl; gdtp <gdttblend; gdtp++) {
         if (gdtp->wired) {
            found_gdt_wired = TRUE;
            break;
         }
      }
d3399 2
a3400 2
      if (found_gdt_wired)
         continue; /* can't free this range */
d3402 1
a3402 1
      /* figure out end of range. Watch for wraparound */
d3404 3
a3406 3
      sdt_vt = sdt_va <= VM_MAX_USER_ADDRESS-PDT_TABLE_GROUP_VA_SPACE ?
               sdt_va+PDT_TABLE_GROUP_VA_SPACE : 
               VM_MAX_USER_ADDRESS;
d3408 2
a3409 2
      /* invalidate all maps in this range */
      pmap_remove_range (pmap, (vm_offset_t)sdt_va, (vm_offset_t)sdt_vt);
d3411 7
a3417 7
      /*
       * we can safely deallocated the page map(s)
       */
      for (sdt = sdtp; sdt < (sdtp+PDT_TABLE_GROUP_SIZE); sdt++) {
         ((sdt_entry_template_t *) sdt) -> bits = 0;
         ((sdt_entry_template_t *) sdt+SDT_ENTRIES) -> bits = 0;
      }
d3419 8
a3426 7
      /*
       * we have to unlock before freeing the table, since PT_FREE
       * calls kmem_free or zfree, which will invoke another pmap routine
       */
      PMAP_UNLOCK(pmap, spl);
      PT_FREE(gdttbl);
      PMAP_LOCK(pmap, spl);
d3428 1
a3428 1
   } /* Segment table Loop */
d3430 1
a3430 1
   PMAP_UNLOCK(pmap, spl);
d3433 2
a3434 2
   if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
      printf  ("(pmap_collect :%x) done \n", curproc);
d3437 1
a3437 1
   CHECK_PMAP_CONSISTENCY("pmap_collect");
d3441 9
a3449 1
 *	Routine:	PMAP_ACTIVATE
d3451 10
a3460 7
 *	Function:
 *		Binds the given physical map to the given
 *		processor, and returns a hardware map description.
 *		In a mono-processor implementation the my_cpu
 *		argument is ignored, and the PMAP_ACTIVATE macro
 *		simply sets the MMU root pointer element of the PCB
 *		to the physical address of the segment descriptor table.
a3461 2
 *	Parameters:
 *		p		pointer to proc structure
d3473 1
a3473 1
		printf("(_pmap_activate :%x) pmap 0x%x\n", p, (unsigned)pmap);
d3491 4
a3494 3
		 * cmmu_pmap_activate will set the uapr and the batc entries, then
		 * flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE ABOUT THE
		 * BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE FLUSHED AS WELL.
d3496 2
a3497 1
		cmmu_pmap_activate(my_cpu, apr_data.bits, pmap->i_batc, pmap->d_batc);
d3526 1
a3526 1
			printf("(_pmap_activate :%x) called for kernel_pmap\n", curproc);
a3531 2


d3533 1
a3533 1
 *	Routine:	PMAP_DEACTIVATE
d3535 8
a3542 4
 *	Function:
 *		Unbinds the given physical map from the given processor,
 *		i.e. the pmap i no longer is use on the processor.
 *		In a mono-processor the PMAP_DEACTIVATE macro is null.
a3543 2
 *	Parameters:
 *		p		pointer to proc structure
a3561 15

/*
 *	Routine:	PMAP_KERNEL
 *
 *	Function:
 *		Retruns a pointer to the kernel pmap.
 */
#if 0 /* Now a macro XXX smurph */
pmap_t
pmap_kernel(void)
{
   return (kernel_pmap);
}/* pmap_kernel() */
#endif 

d3593 43
a3635 40
   vm_offset_t dstva, srcva;
   unsigned int spl_sav;
   int i;
   int      aprot;
   pte_template_t template;
   pt_entry_t  *dstpte, *srcpte;
   int      my_cpu = cpu_number();

   /*
    *	Map source physical address.
    */
   aprot = m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);

   srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu_number() * PAGE_SIZE));
   dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));

   srcpte = pmap_pte(kernel_pmap, srcva);
   dstpte = pmap_pte(kernel_pmap, dstva);

   for (i=0; i < ptes_per_vm_page; i++, src += M88K_PGBYTES, dst += M88K_PGBYTES) {
      template.bits = M88K_TRUNC_PAGE(src) | aprot | DT_VALID | CACHE_GLOBAL;

      /* do we need to write back dirty bits */
      spl_sav = splimp();
      cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
      *srcpte = template.pte;

      /*
       *	Map destination physical address.
       */
      template.bits = M88K_TRUNC_PAGE(dst) | aprot | CACHE_GLOBAL | DT_VALID;
      cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
      *dstpte  = template.pte;
      splx(spl_sav);

      bcopy((void*)srcva, (void*)dstva, M88K_PGBYTES);
      /* flush source, dest out of cache? */
      cmmu_flush_remote_data_cache(my_cpu, src, M88K_PGBYTES);
      cmmu_flush_remote_data_cache(my_cpu, dst, M88K_PGBYTES);
   }
d3666 36
a3701 38
   vm_offset_t   dstva;
   pt_entry_t *dstpte;
   int     copy_size,
   offset,
   aprot;
   unsigned int i;
   pte_template_t  template;

   dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
   dstpte = pmap_pte(kernel_pmap, dstva);
   copy_size = M88K_PGBYTES;
   offset = dstpa - M88K_TRUNC_PAGE(dstpa);
   dstpa -= offset;

   aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
   while (bytecount > 0) {
      copy_size = M88K_PGBYTES - offset;
      if (copy_size > bytecount)
         copy_size = bytecount;

      /*
       *      Map distation physical address. 
       */

      for (i = 0; i < ptes_per_vm_page; i++) {
         template.bits = M88K_TRUNC_PAGE(dstpa) | aprot | CACHE_WT | DT_VALID;
         cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
         *dstpte = template.pte;

         dstva += offset;
         bcopy((void*)srcva, (void*)dstva, copy_size);
         srcva += copy_size;
         dstva += copy_size;
         dstpa += M88K_PGBYTES;
         bytecount -= copy_size;
         offset = 0;
      }
   }
d3730 36
a3765 38
   register vm_offset_t   srcva;
   register pt_entry_t *srcpte;
   register int     copy_size, offset;
   int             aprot;
   unsigned int i;
   pte_template_t  template;

   srcva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
   srcpte = pmap_pte(kernel_pmap, srcva);
   copy_size = M88K_PGBYTES;
   offset = srcpa - M88K_TRUNC_PAGE(srcpa);
   srcpa -= offset;

   aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
   while (bytecount > 0) {
      copy_size = M88K_PGBYTES - offset;
      if (copy_size > bytecount)
         copy_size = bytecount;

      /*
       *      Map destnation physical address.
       */

      for (i=0; i < ptes_per_vm_page; i++) {
         template.bits = M88K_TRUNC_PAGE(srcpa) | aprot | CACHE_WT | DT_VALID;
         cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
         *srcpte = template.pte;

         srcva += offset;
         bcopy((void*)srcva, (void*)dstva, copy_size);
         srcpa += M88K_PGBYTES;
         dstva += copy_size;
         srcva += copy_size;
         bytecount -= copy_size;
         offset = 0;
         /* cache flush source? */
      }
   }
d3791 1
a3791 1
              boolean_t pageable)
d3794 1
a3794 1
   pmap++; start++; end++; pageable++;
d3827 16
a3842 33
   pt_entry_t    *pte;
   int        spl, spl_sav;
   int        i;
   unsigned      users;
   pte_template_t   opte;
   int        kflush;

   va = M88K_ROUND_PAGE(va);
   PMAP_LOCK(pmap, spl);

   users = pmap->cpus_using;
   if (pmap == kernel_pmap) {
      kflush = 1;
   } else {
      kflush = 0;
   }

   if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL && PDT_VALID(pte))
      for (i = ptes_per_vm_page; i > 0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         spl_sav = splimp();
         opte.bits = invalidate_pte(pte);
         opte.pte.prot = M88K_RO;
         ((pte_template_t *)pte)->bits = opte.bits;
         flush_atc_entry(users, va, kflush);
         splx(spl_sav);
         pte++;
         va +=M88K_PGBYTES;
      }
d3844 2
a3845 1
   PMAP_UNLOCK(pmap, spl);
d3847 13
a3859 1
} /* pmap_redzone() */
d3861 1
d3863 1
d3896 10
a3905 11
   pv_entry_t    pvl;
   int        pfi;
   pv_entry_t    pvep;
   pt_entry_t    *pte;
   pmap_t     pmap;
   int        spl, spl_sav;
   vm_offset_t      va;
   int        i;
   unsigned      users;
   pte_template_t   opte;
   int        kflush;
d3907 1
a3907 1
   if (!PMAP_MANAGED(phys)) {
d3909 2
a3910 2
      if (pmap_con_dbg & CD_CMOD)
         printf("(pmap_clear_modify :%x) phys addr 0x%x not managed \n", curproc, phys);
d3912 2
a3913 2
      return;
   }
d3915 1
a3915 1
   SPLVM(spl);
d3918 3
a3920 3
   pvl = PA_TO_PVH(phys);
   CHECK_PV_LIST (phys, pvl, "pmap_clear_modify");
   LOCK_PVH(phys);
d3922 2
a3923 2
   /* update correspoinding pmap_modify_list element */
   SET_ATTRIB(phys, 0);
d3925 1
a3925 1
   if (pvl->pmap == PMAP_NULL) {
d3927 2
a3928 2
      if ((pmap_con_dbg & (CD_CMOD | CD_NORM)) == (CD_CMOD | CD_NORM))
         printf("(pmap_clear_modify :%x) phys addr 0x%x not mapped\n", curproc, phys);
d3930 4
d3935 40
a3974 48
      UNLOCK_PVH(phys);
      SPLX(spl);
      return;
   }

   /* for each listed pmap, trun off the page modified bit */
   pvep = pvl;
   while (pvep != PV_ENTRY_NULL) {
      pmap = pvep->pmap;
      va = pvep->va;
      if (!simple_lock_try(&pmap->lock)) {
         UNLOCK_PVH(phys);
         goto clear_modify_Retry;
      }
      users = pmap->cpus_using;
      if (pmap == kernel_pmap) {
         kflush = 1;
      } else {
         kflush = 0;
      }

      pte = pmap_pte(pmap, va);
      if (pte == PT_ENTRY_NULL)
         panic("pmap_clear_modify: bad pv list entry.");

      for (i = ptes_per_vm_page; i > 0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         spl_sav = splimp();
         opte.bits = invalidate_pte(pte);
         /* clear modified bit */
         opte.pte.modified = 0;
         ((pte_template_t *)pte)->bits = opte.bits;
         flush_atc_entry(users, va, kflush);
         splx(spl_sav);
         pte++;
         va += M88K_PGBYTES;
      }
      simple_unlock(&pmap->lock);
      pvep = pvep->next;
   }

   UNLOCK_PVH(phys);
   SPLX(spl);

a3976 2


d4015 6
a4020 7
   pv_entry_t  pvl;
   int      pfi;
   pv_entry_t  pvep;
   pt_entry_t  *ptep;
   int      spl;
   int      i;
   boolean_t   modified_flag;
d4022 1
a4022 1
   if (!PMAP_MANAGED(phys)) {
d4024 2
a4025 2
      if (pmap_con_dbg & CD_IMOD)
         printf("(pmap_is_modified :%x) phys addr 0x%x not managed\n", curproc, phys);
d4027 2
a4028 4
      return (FALSE);
   }

   SPLVM(spl);
d4030 1
a4030 3
   pvl = PA_TO_PVH(phys);
   CHECK_PV_LIST (phys, pvl, "pmap_is_modified");
is_mod_Retry:
d4032 7
a4038 3
   if ((boolean_t) PA_TO_ATTRIB(phys)) {
      /* we've already cached a modify flag for this page,
         no use looking further... */
d4040 2
a4041 2
      if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
         printf("(pmap_is_modified :%x) already cached a modify flag for this page\n", curproc);
d4043 9
a4051 9
      SPLX(spl);
      return (TRUE);
   }
   LOCK_PVH(phys);

   if (pvl->pmap == PMAP_NULL) {
      /* unmapped page - get info from page_modified array
         maintained by pmap_remove_range/ pmap_remove_all */
      modified_flag = (boolean_t) PA_TO_ATTRIB(phys);
d4053 2
a4054 2
      if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
         printf("(pmap_is_modified :%x) phys addr 0x%x not mapped\n", curproc, phys);
d4056 21
a4076 21
      UNLOCK_PVH(phys);
      SPLX(spl);
      return (modified_flag);
   }

   /* for each listed pmap, check modified bit for given page */
   pvep = pvl;
   while (pvep != PV_ENTRY_NULL) {
      if (!simple_lock_try(&pvep->pmap->lock)) {
         UNLOCK_PVH(phys);
         goto is_mod_Retry;
      }

      ptep = pmap_pte(pvep->pmap, pvep->va);
      if (ptep == PT_ENTRY_NULL) {
         printf("pmap_is_modified: pte from pv_list not in map virt = 0x%x\n", pvep->va);
         panic("pmap_is_modified: bad pv list entry");
      }
      for (i = ptes_per_vm_page; i > 0; i--) {
         if (ptep->modified) {
            simple_unlock(&pvep->pmap->lock);
d4078 2
a4079 2
            if ((pmap_con_dbg & (CD_IMOD | CD_FULL)) == (CD_IMOD | CD_FULL))
               printf("(pmap_is_modified :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d4081 13
a4093 13
            UNLOCK_PVH(phys);
            SPLX(spl);
            return (TRUE);
         }
         ptep++;
      }
      simple_unlock(&pvep->pmap->lock);
      pvep = pvep->next;
   }

   UNLOCK_PVH(phys);
   SPLX(spl);
   return (FALSE);
a4096 2


d4132 10
a4141 11
   pv_entry_t    pvl;
   int        pfi;
   pv_entry_t    pvep;
   pt_entry_t    *pte;
   pmap_t     pmap;
   int        spl, spl_sav;
   vm_offset_t      va;
   int        i;
   unsigned      users;
   pte_template_t   opte;
   int        kflush;
d4143 1
a4143 1
   if (!PMAP_MANAGED(phys)) {
d4145 3
a4147 3
      if (pmap_con_dbg & CD_CREF) {
         printf("(pmap_clear_reference :%x) phys addr 0x%x not managed\n", curproc,phys);
      }
d4149 2
a4150 2
      return;
   }
d4152 1
a4152 1
   SPLVM(spl);
d4154 4
a4157 4
clear_reference_Retry:
   LOCK_PVH(phys);
   pvl = PA_TO_PVH(phys);
   CHECK_PV_LIST(phys, pvl, "pmap_clear_reference");
d4160 1
a4160 1
   if (pvl->pmap == PMAP_NULL) {
d4162 2
a4163 2
      if ((pmap_con_dbg & (CD_CREF | CD_NORM)) == (CD_CREF | CD_NORM))
         printf("(pmap_clear_reference :%x) phys addr 0x%x not mapped\n", curproc,phys);
d4165 4
a4168 45
      UNLOCK_PVH(phys);
      SPLX(spl);
      return;
   }

   /* for each listed pmap, turn off the page refrenced bit */
   pvep = pvl;
   while (pvep != PV_ENTRY_NULL) {
      pmap = pvep->pmap;
      va = pvep->va;
      if (!simple_lock_try(&pmap->lock)) {
         UNLOCK_PVH(phys);
         goto clear_reference_Retry;
      }
      users = pmap->cpus_using;
      if (pmap == kernel_pmap) {
         kflush = 1;
      } else {
         kflush = 0;
      }

      pte = pmap_pte(pmap, va);
      if (pte == PT_ENTRY_NULL)
         panic("pmap_clear_reference: bad pv list entry.");

      for (i = ptes_per_vm_page; i > 0; i--) {

         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         spl_sav = splimp();
         opte.bits = invalidate_pte(pte);
         /* clear reference bit */
         opte.pte.pg_used = 0;
         ((pte_template_t *)pte)->bits = opte.bits;
         flush_atc_entry(users, va, kflush);
         splx(spl_sav);
         pte++;
         va += M88K_PGBYTES;
      }

      simple_unlock(&pmap->lock);
      pvep = pvep->next;
   }
d4170 35
a4204 2
   UNLOCK_PVH(phys);
   SPLX(spl);
d4206 5
a4212 2


d4246 1
d4250 49
a4298 50
   pv_entry_t pvl;
   int     pfi;
   pv_entry_t pvep;
   pt_entry_t *ptep;
   int     spl;
   int     i;

   if (!PMAP_MANAGED(phys))
      return (FALSE);

   SPLVM(spl);

   pvl = PA_TO_PVH(phys);
   CHECK_PV_LIST(phys, pvl, "pmap_is_referenced");

is_ref_Retry:

   if (pvl->pmap == PMAP_NULL) {
      SPLX(spl);
      return (FALSE);
   }

   LOCK_PVH(phys);

   /* for each listed pmap, check used bit for given page */
   pvep = pvl;
   while (pvep != PV_ENTRY_NULL) {
      if (!simple_lock_try(&pvep->pmap->lock)) {
         UNLOCK_PVH(phys);
         goto is_ref_Retry;
      }
      ptep = pmap_pte(pvep->pmap, pvep->va);
      if (ptep == PT_ENTRY_NULL)
         panic("pmap_is_referenced: bad pv list entry.");
      for (i = ptes_per_vm_page; i > 0; i--) {
         if (ptep->pg_used) {
            simple_unlock(&pvep->pmap->lock);
            UNLOCK_PVH(phys);
            SPLX(spl);
            return (TRUE);
         }
         ptep++;
      }
      simple_unlock(&pvep->pmap->lock);
      pvep = pvep->next;
   }

   UNLOCK_PVH(phys);
   SPLX(spl);
   return (FALSE);
d4330 3
a4332 6
   pv_entry_t  pv_h;
   int      spl;
   boolean_t   result;

   if (!pmap_initialized)
      return (TRUE);
d4334 2
a4335 2
   if (!PMAP_MANAGED(phys))
      return (FALSE);
d4337 2
a4338 1
   SPLVM(spl);
d4340 1
a4340 2
   pv_h = PA_TO_PVH(phys);
   LOCK_PVH(phys);
d4342 2
a4343 3
   result = (pv_h->pmap == PMAP_NULL);
   UNLOCK_PVH(phys);
   SPLX(spl);
d4345 3
a4347 1
   return (result);
d4349 1
a4351 1

d4362 1
a4362 1
   p++;
d4364 1
a4364 1
   return (TRUE);
d4379 11
a4389 11
   switch (prot) {
      case VM_PROT_READ:
      case VM_PROT_READ|VM_PROT_EXECUTE:
         pmap_copy_on_write(phys);
         break;
      case VM_PROT_ALL:
         break;
      default:
         pmap_remove_all(phys);
         break;
   }
d4420 36
a4455 37
   vm_offset_t      pa;
   pt_entry_t    *srcpte, *dstpte;
   int        pfi;
   pv_entry_t    pvl;
   int        spl;
   int        i;
   unsigned      users;
   pte_template_t   opte;

   PMAP_LOCK(kernel_pmap, spl);

   users = kernel_pmap->cpus_using;

   while (size > 0) {

      /*
       * check if the source addr is mapped
       */
      if ((srcpte = pmap_pte(kernel_pmap, (vm_offset_t)from)) == PT_ENTRY_NULL) {
         printf("pagemove: source vaddr 0x%x\n", from);
         panic("pagemove: Source addr not mapped");
      }

      /*
       *
       */
      if ((dstpte = pmap_pte(kernel_pmap, (vm_offset_t)to)) == PT_ENTRY_NULL)
         if ((dstpte = pmap_expand_kmap((vm_offset_t)to, VM_PROT_READ | VM_PROT_WRITE))
             == PT_ENTRY_NULL)
            panic("pagemove: Cannot allocate distination pte");
         /*
          *
          */
      if (dstpte->dtype == DT_VALID) {
         printf("pagemove: distination vaddr 0x%x, pte = 0x%x\n", to, *((unsigned *)dstpte));
         panic("pagemove: Distination pte already valid");
      }
d4458 4
a4461 4
      if ((pmap_con_dbg & (CD_PGMV | CD_NORM)) == (CD_PGMV | CD_NORM))
         printf("(pagemove :%x) from 0x%x to 0x%x\n", curproc, from, to);
      if ((pmap_con_dbg & (CD_PGMV | CD_FULL)) == (CD_PGMV | CD_FULL))
         printf("(pagemove :%x) srcpte @@ 0x%x = %x dstpte @@ 0x%x = %x\n", curproc, (unsigned)srcpte, *(unsigned *)srcpte, (unsigned)dstpte, *(unsigned *)dstpte);
d4465 11
a4475 31
      /*
       * Update pv_list
       */
      pa = M88K_PTOB(srcpte->pfn);
      if (PMAP_MANAGED(pa)) {
         LOCK_PVH(pa);
         pvl = PA_TO_PVH(pa);
         CHECK_PV_LIST(pa, pvl, "pagemove");
         pvl->va = (vm_offset_t)to;
         UNLOCK_PVH(pa);
      }

      /*
       * copy pte
       */
      for (i = ptes_per_vm_page; i > 0; i--) {
         /*
          * Invalidate pte temporarily to avoid being written back
          * the modified bit and/or the reference bit by other cpu.
          */
         opte.bits = invalidate_pte(srcpte);
         flush_atc_entry(users, from, 1);
         ((pte_template_t *)dstpte)->bits = opte.bits;
         from += M88K_PGBYTES;
         to += M88K_PGBYTES;
         srcpte++; dstpte++;
      }
      size -= PAGE_SIZE;
   }

   PMAP_UNLOCK(kernel_pmap, spl);
d4477 18
d4524 2
a4525 10
   int  i;
   int  cpu = 0;

   for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
      for (cpu=0; cpu<max_cpus; cpu++) {
         if (cpu_sets[cpu]) {
            cmmu_flush_remote_inst_cache(cpu, pa, M88K_PGBYTES);
         }
      }
   }
d4527 8
d4558 13
a4570 8
   vm_offset_t pa;
   int  i;
   int     spl;

   if (pmap == PMAP_NULL)
      panic("pmap_dcache_flush: pmap is NULL");

   PMAP_LOCK(pmap, spl);
d4572 1
a4572 6
   pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
   for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
      cmmu_flush_data_cache(pa, M88K_PGBYTES);
   }

   PMAP_UNLOCK(pmap, spl);
d4580 49
a4628 49
   int     i;
   int     ncpus;
   void    (*cfunc)(int cpu, vm_offset_t physaddr, int size);

   switch (mode) {
      default:
         panic("bad cache_flush_loop mode");
         return;

      case FLUSH_CACHE:   /* All caches, all CPUs */
         ncpus = max_cpus;
         cfunc = cmmu_flush_remote_cache;
         break;

      case FLUSH_CODE_CACHE: /* Instruction caches, all CPUs */
         ncpus = max_cpus;
         cfunc = cmmu_flush_remote_inst_cache;
         break;

      case FLUSH_DATA_CACHE: /* Data caches, all CPUs */
         ncpus = max_cpus;
         cfunc = cmmu_flush_remote_data_cache;
         break;

      case FLUSH_LOCAL_CACHE:      /* Both caches, my CPU */
         ncpus = 1;
         cfunc = cmmu_flush_remote_cache;
         break;

      case FLUSH_LOCAL_CODE_CACHE: /* Instruction cache, my CPU */
         ncpus = 1;
         cfunc = cmmu_flush_remote_inst_cache;
         break;

      case FLUSH_LOCAL_DATA_CACHE: /* Data cache, my CPU */
         ncpus = 1;
         cfunc = cmmu_flush_remote_data_cache;
         break;
   }

   if (ncpus == 1) {
      (*cfunc)(cpu_number(), pa, size);
   } else {
      for (i=0; i<max_cpus; i++) {
         if (cpu_sets[i]) {
            (*cfunc)(i, pa, size);
         }
      }
   }
d4638 25
a4662 25
   vm_offset_t pa;
   vm_offset_t va;
   int  i;
   int     spl;

   if (pmap == PMAP_NULL)
      panic("pmap_dcache_flush: NULL pmap");

   /*
    * If it is more than a couple of pages, just blow the whole cache
    * because of the number of cycles involved.
    */
   if (bytes > 2*M88K_PGBYTES) {
      cache_flush_loop(mode, 0, -1);
      return;
   }

   PMAP_LOCK(pmap, spl);
   for (va = virt; bytes > 0; bytes -= M88K_PGBYTES,va += M88K_PGBYTES) {
      pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
      for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
         cache_flush_loop(mode, pa, M88K_PGBYTES);
      }
   }
   PMAP_UNLOCK(pmap, spl);
d4706 20
a4725 39
   pv_entry_t  pv_e;
   pt_entry_t  *pte;
   vm_offset_t pa;

   if (pv_h != PA_TO_PVH(phys)) {
      printf("check_pv_list: incorrect pv_h supplied.\n");
      panic(who);
   }

   if (!PAGE_ALIGNED(phys)) {
      printf("check_pv_list: supplied phys addr not page aligned.\n");
      panic(who);
   }

   if (pv_h->pmap == PMAP_NULL) {
      if (pv_h->next != PV_ENTRY_NULL) {
         printf("check_pv_list: first entry has null pmap, but list non-empty.\n");
         panic(who);
      } else  return;     /* proper empry lst */
   }

   pv_e = pv_h;
   while (pv_e != PV_ENTRY_NULL) {
      if (!PAGE_ALIGNED(pv_e->va)) {
         printf("check_pv_list: non-aligned VA in entry at 0x%x.\n", pv_e);
         panic(who);
      }
      /*
       * We can't call pmap_extract since it requires lock.
       */
      if ((pte = pmap_pte(pv_e->pmap, pv_e->va)) == PT_ENTRY_NULL)
         pa = (vm_offset_t)0;
      else
         pa = M88K_PTOB(pte->pfn) | (pv_e->va & M88K_PGOFSET);

      if (pa != phys) {
         printf("check_pv_list: phys addr diff in entry at 0x%x.\n", pv_e);
         panic(who);
      }
d4727 21
a4747 2
      pv_e = pv_e->next;
   }
d4785 9
a4793 9
   vm_offset_t va,
   old_va,
   phys;
   pv_entry_t  pv_h,
   pv_e,
   saved_pv_e;
   pt_entry_t  *ptep;
   boolean_t   found;
   int      loopcnt;
d4795 2
a4796 2
   int bank;
   unsigned      npages;
d4799 44
a4842 44
   /*
    * for each page in the address space, check to see if there's
    * a valid mapping. If so makes sure it's listed in the PV_list.
    */

   if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
      printf("(check_map) checking map at 0x%x\n", map);

   old_va = s;
   for (va = s; va < e; va += PAGE_SIZE) {
      /* check for overflow - happens if e=0xffffffff */
      if (va < old_va)
         break;
      else
         old_va = va;

      if (va == phys_map_vaddr1 || va == phys_map_vaddr2)
         /* don't try anything with these */
         continue;

      ptep = pmap_pte(map, va);

      if (ptep == PT_ENTRY_NULL) {
         /* no page table, skip to next segment entry */
         va = SDT_NEXT(va)-PAGE_SIZE;
         continue;
      }

      if (!PDT_VALID(ptep))
         continue;      /* no page mapping */

      phys = M88K_PTOB(ptep->pfn);  /* pick up phys addr */

      if (!PMAP_MANAGED(phys))
         continue;      /* no PV list */

      /* note: vm_page_startup allocates some memory for itself
         through pmap_map before pmap_init is run. However,
         it doesn't adjust the physical start of memory.
         So, pmap thinks those pages are managed - but they're
         not actually under it's control. So, the following
         conditional is a hack to avoid those addresses
         reserved by vm_page_startup */
      /* pmap_init also allocate some memory for itself. */
d4845 5
a4849 5
      for (npages = 0, bank = 0; bank < vm_nphysseg; bank++)
         npages += vm_physmem[bank].end - vm_physmem[bank].start;
      if (map == kernel_pmap &&
          va < round_page((vm_offset_t)(pmap_modify_list + npages)))
         continue;
d4851 3
a4853 3
      if (map == kernel_pmap &&
          va < round_page((vm_offset_t)(pmap_modify_list + (pmap_phys_end - pmap_phys_start))))
         continue;
d4855 2
a4856 2
      pv_h = PA_TO_PVH(phys);
      found = FALSE;
d4858 1
a4858 1
      if (pv_h->pmap != PMAP_NULL) {
d4860 27
a4886 27
         loopcnt = 10000;  /* loop limit */
         pv_e = pv_h;
         while (pv_e != PV_ENTRY_NULL) {

            if (loopcnt-- < 0) {
               printf("check_map: loop in PV list at PVH 0x%x (for phys 0x%x)\n", pv_h, phys);
               panic(who);
            }

            if (pv_e->pmap == map && pv_e->va == va) {
               if (found) {
                  printf("check_map: Duplicate PV list entries at 0x%x and 0x%x in PV list 0x%x.\n", saved_pv_e, pv_e, pv_h);
                  printf("check_map: for pmap 0x%x, VA 0x%x,phys 0x%x.\n", map, va, phys);
                  panic(who);
               } else {
                  found = TRUE;
                  saved_pv_e = pv_e;
               }
            }
            pv_e = pv_e->next;
         }
      }

      if (!found) {
         printf("check_map: Mapping for pmap 0x%x VA 0x%x Phys 0x%x does not appear in PV list 0x%x.\n", map, va, phys, pv_h);
      }
   }
d4888 2
a4889 2
   if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
      printf("(check_map) done \n");
d4930 5
a4934 5
   pmap_t      p;
   int      i;
   vm_offset_t phys;
   pv_entry_t  pv_h;
   int      spl;
d4936 2
a4937 2
   int bank;
   unsigned      npages;
d4940 2
a4941 2
   if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
      printf("check_pmap_consistency (%s :%x) start.\n", who, curproc);
d4943 1
a4943 1
   if (pv_head_table == PV_ENTRY_NULL) {
d4945 3
a4947 18
      printf("check_pmap_consistency (%s) PV head table not initialized.\n", who);
      return;
   }

   SPLVM(spl);

   p = kernel_pmap;
   check_map(p, VM_MIN_KERNEL_ADDRESS, VM_MAX_KERNEL_ADDRESS, who);

   /* run through all pmaps. check consistency of each one... */
   i = PMAP_MAX;
   for (p = kernel_pmap->next;p != kernel_pmap; p = p->next) {
      if (i == 0) { /* can not read pmap list */
         printf("check_pmap_consistency: pmap strcut loop error.\n");
         panic(who);
      }
      check_map(p, VM_MIN_USER_ADDRESS, VM_MAX_USER_ADDRESS, who);
   }
d4949 16
a4964 1
   /* run through all managed paes, check pv_list for each one */
d4966 5
a4970 4
    for (npages = 0, bank = 0; bank < vm_nphysseg; bank++){
        for (phys = ptoa(vm_physmem[bank].start); phys < ptoa(vm_physmem[bank].end); phys += PAGE_SIZE) {
            pv_h = PA_TO_PVH(phys);
	    check_pv_list(phys, pv_h, who);
a4971 1
    }
d4973 4
a4976 4
   for (phys = pmap_phys_start; phys < pmap_phys_end; phys += PAGE_SIZE) {
      pv_h = PA_TO_PVH(phys);
      check_pv_list(phys, pv_h, who);
   }
d4979 1
a4979 1
   SPLX(spl);
d4981 2
a4982 2
   if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
      printf("check_pmap consistency (%s :%x): done.\n",who, curproc);
d5008 1
a5008 1
			p->modified,				\
d5031 10
a5040 10
   sdt_entry_t   *sdtp;
   sdt_entry_t   *sdtv;
   int     i;

   printf("Pmap @@ 0x%x:\n", (unsigned)pmap);
   sdtp = pmap->sdt_paddr;
   sdtv = pmap->sdt_vaddr;
   printf("	sdt_paddr: 0x%x; sdt_vaddr: 0x%x; ref_count: %d;\n",
          (unsigned)sdtp, (unsigned)sdtv,
          pmap->ref_count);
d5043 45
a5087 45
   printf("	statistics: pagesize %d: free_count %d; "
          "active_count %d; inactive_count %d; wire_count %d\n",
          pmap->stats.pagesize,
          pmap->stats.free_count,
          pmap->stats.active_count,
          pmap->stats.inactive_count,
          pmap->stats.wire_count);

   printf("	zero_fill_count %d; reactiveations %d; "
          "pageins %d; pageouts %d; faults %d\n",
          pmap->stats.zero_fill_count,
          pmap->stats.reactivations,
          pmap->stats.pageins,
          pmap->stats.pageouts,
          pmap->stats.fault);

   printf("	cow_faults %d, lookups %d, hits %d\n",
          pmap->stats.cow_faults,
          pmap->stats.loopups,
          pmap->stats.faults);
#endif

   sdtp = (sdt_entry_t *) pmap->sdt_vaddr;  /* addr of physical table */
   sdtv = sdtp + SDT_ENTRIES;      /* shadow table with virt address */
   if (sdtp == (sdt_entry_t *)0)
      printf("Error in pmap - sdt_paddr is null.\n");
   else {
      int   count = 0;
      printf("	Segment table at 0x%x (0x%x):\n",
             (unsigned)sdtp, (unsigned)sdtv);
      for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
         if ((sdtp->table_addr != 0 ) || (sdtv->table_addr != 0)) {
            if (count != 0)
               printf("sdt entry %d skip !!\n", count);
            count = 0;
            printf("   (%x)phys: ", i);
            PRINT_SDT(sdtp);
            printf("   (%x)virt: ", i);
            PRINT_SDT(sdtv);
         } else
            count++;
      }
      if (count != 0)
         printf("sdt entry %d skip !!\n", count);
   }
d5114 20
a5133 62
   sdt_entry_t   *sdtp;   /* ptr to sdt table of physical addresses */
   sdt_entry_t   *sdtv;   /* ptr to sdt shadow table of virtual addresses */
   pt_entry_t *ptep;   /* ptr to pte table of physical page addresses */

   int     i; /* table loop index */
   unsigned long prev_entry; /* keep track of value of previous table entry */
   int     n_dup_entries; /* count contiguous duplicate entries */

   printf("Trace of virtual address 0x%08x. Pmap @@ 0x%08x.\n",
          va, (unsigned)pmap);

   /*** SDT TABLES ***/
   /* get addrs of sdt tables */
   sdtp = (sdt_entry_t *)pmap->sdt_vaddr;
   sdtv = sdtp + SDT_ENTRIES;

   if (sdtp == SDT_ENTRY_NULL) {
      printf("    Segment table pointer (pmap.sdt_paddr) null, trace stops.\n");
      return;
   }

   n_dup_entries = 0;
   prev_entry = 0xFFFFFFFF;

   if (long_format) {
      printf("    Segment table at 0x%08x (virt shadow at 0x%08x)\n",
             (unsigned)sdtp, (unsigned)sdtv);
      for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
         if (prev_entry == ((sdt_entry_template_t *)sdtp)->bits
             && SDTIDX(va) != i && i != SDT_ENTRIES-1) {
            n_dup_entries++;
            continue;  /* suppress duplicate entry */
         }
         if (n_dup_entries != 0) {
            printf("    - %d duplicate entries skipped -\n",n_dup_entries);
            n_dup_entries = 0;
         }
         prev_entry = ((pte_template_t *)sdtp)->bits;
         if (SDTIDX(va) == i) {
            printf("    >> (%x)phys: ", i);
         } else {
            printf("       (%x)phys: ", i);
         }
         PRINT_SDT(sdtp);
         if (SDTIDX(va) == i) {
            printf("    >> (%x)virt: ", i);
         } else {
            printf("       (%x)virt: ", i);
         }
         PRINT_SDT(sdtv);
      } /* for */
   } else {
      /* index into both tables for given VA */
      sdtp += SDTIDX(va);
      sdtv += SDTIDX(va);
      printf("    SDT entry index 0x%x at 0x%x (virt shadow at 0x%x)\n",
             SDTIDX(va), (unsigned)sdtp, (unsigned)sdtv);
      printf("    phys:  ");
      PRINT_SDT(sdtp);
      printf("    virt:  ");
      PRINT_SDT(sdtv);
   }
d5135 2
a5136 2
   /*** PTE TABLES ***/
   /* get addrs of page (pte) table (no shadow table) */
d5138 43
a5180 1
   sdtp = ((sdt_entry_t *)pmap->sdt_vaddr) + SDTIDX(va);
d5182 2
a5183 2
   printf("*** DEBUG (sdtp) ");
   PRINT_SDT(sdtp);
d5185 36
a5220 36
   sdtv = sdtp + SDT_ENTRIES;
   ptep = (pt_entry_t *)(M88K_PTOB(sdtv->table_addr));
   if (sdtp->dtype != DT_VALID) {
      printf("    segment table entry invlid, trace stops.\n");
      return;
   }

   n_dup_entries = 0;
   prev_entry = 0xFFFFFFFF;
   if (long_format) {
      printf("        page table (ptes) at 0x%x\n", (unsigned)ptep);
      for (i = 0; i < PDT_ENTRIES; i++, ptep++) {
         if (prev_entry == ((pte_template_t *)ptep)->bits
             && PDTIDX(va) != i && i != PDT_ENTRIES-1) {
            n_dup_entries++;
            continue;  /* suppress suplicate entry */
         }
         if (n_dup_entries != 0) {
            printf("    - %d duplicate entries skipped -\n",n_dup_entries);
            n_dup_entries = 0;
         }
         prev_entry = ((pte_template_t *)ptep)->bits;
         if (PDTIDX(va) == i) {
            printf("    >> (%x)pte: ", i);
         } else {
            printf("       (%x)pte: ", i);
         }
         PRINT_PDT(ptep);
      } /* for */
   } else {
      /* index into page table */
      ptep += PDTIDX(va);
      printf("    pte index 0x%x\n", PDTIDX(va));
      printf("    pte: ");
      PRINT_PDT(ptep);
   }
d5233 37
a5269 37
   pt_entry_t *pte;
   sdt_entry_t   *sdt;
   int                         spl;

   PMAP_LOCK(pmap, spl);

   if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
      PMAP_UNLOCK(pmap, spl);
      return FALSE;
   }

   if (!PDT_VALID(pte)) {
      PMAP_UNLOCK(pmap, spl);
      return FALSE;
   }

   /*
    * Valid pte.  If the transaction was a read, there is no way it
    *  could have been a fault, so return true.  For now, assume
    *  that a write transaction could have caused a fault.  We need
    *  to check pte and sdt entries for write permission to really
    *  tell.
    */

   if (type == VM_PROT_READ) {
      PMAP_UNLOCK(pmap, spl);
      return TRUE;
   } else {
      sdt = SDTENT(pmap,va);
      if (sdt->prot || pte->prot) {
         PMAP_UNLOCK(pmap, spl);
         return FALSE;
      } else {
         PMAP_UNLOCK(pmap, spl);
         return TRUE;
      }
   }
d5277 2
a5278 2
   *startp = virtual_avail;
   *endp = virtual_end;
d5284 1
a5284 1
   return atop(avail_end - avail_next);
d5290 2
a5291 2
   if (avail_next == avail_end)
      return FALSE;
d5293 3
a5295 3
   *addrp = avail_next;
   avail_next += PAGE_SIZE;
   return TRUE;
d5305 34
a5338 34
             pmap_t pmap,
             boolean_t data,
             int i,
             vm_offset_t va,
             vm_offset_t pa,
             boolean_t super,
             boolean_t wt,
             boolean_t global,
             boolean_t ci,
             boolean_t wp,
             boolean_t valid)
{
   register batc_template_t batctmp;

   if (i < 0 || i > (BATC_MAX - 1)) {
      panic("pmap_set_batc: illegal batc number");
      /* bad number */
      return;
   }

   batctmp.field.lba = va >> 19;
   batctmp.field.pba = pa >> 19;
   batctmp.field.sup = super;
   batctmp.field.wt = wt;
   batctmp.field.g = global;
   batctmp.field.ci = ci;
   batctmp.field.wp = wp;
   batctmp.field.v = valid;

   if (data) {
      pmap->d_batc[i].bits = batctmp.bits;
   } else {
      pmap->i_batc[i].bits = batctmp.bits;
   }
d5341 16
a5356 16
void use_batc(
             task_t task,  
             boolean_t data,         /* for data-cmmu ? */
             int i,                  /* batc number */
             vm_offset_t va,         /* virtual address */
             vm_offset_t pa,         /* physical address */
             boolean_t s,            /* for super-mode ? */
             boolean_t wt,           /* is writethrough */
             boolean_t g,            /* is global ? */
             boolean_t ci,           /* is cache inhibited ? */
             boolean_t wp,           /* is write-protected ? */
             boolean_t v)            /* is valid ? */
{
   pmap_t pmap;
   pmap = vm_map_pmap(task->map);
   pmap_set_batc(pmap, data, i, va, pa, s, wt, g, ci, wp, v);
d5378 1
a5378 1
   pmap_range_t this, next;
d5380 7
a5386 7
   this = *ranges;
   while (this != 0) {
      next = this->next;
      pmap_range_free(this);
      this = next;
   }
   *ranges = 0;
d5395 1
a5395 1
   pmap_range_t range;
d5397 7
a5403 7
   for (range = *ranges; range != 0; range = range->next) {
      if (address < range->start)
         return FALSE;
      if (address < range->end)
         return TRUE;
   }
   return FALSE;
d5413 1
a5413 1
   pmap_range_t range, *prev;
d5415 1
a5415 1
   /* look for the start address */
d5417 6
a5422 6
   for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
      if (start < range->start)
         break;
      if (start <= range->end)
         goto start_overlaps;
   }
d5424 1
a5424 1
   /* start address is not present */
d5426 2
a5427 2
   if ((range == 0) || (end < range->start)) {
      /* no overlap; allocate a new range */
d5429 7
a5435 7
      range = pmap_range_alloc();
      range->start = start;
      range->end = end;
      range->next = *prev;
      *prev = range;
      return;
   }
d5437 1
a5437 1
   /* extend existing range forward to start */
d5439 1
a5439 1
   range->start = start;
d5442 1
a5442 1
   assert((range->start <= start) && (start <= range->end));
d5444 1
a5444 1
   /* delete redundant ranges */
d5446 2
a5447 2
   while ((range->next != 0) && (range->next->start <= end)) {
      pmap_range_t old;
d5449 5
a5453 5
      old = range->next;
      range->next = old->next;
      range->end = old->end;
      pmap_range_free(old);
   }
d5455 1
a5455 1
   /* extend existing range backward to end */
d5457 2
a5458 2
   if (range->end < end)
      range->end = end;
d5468 1
a5468 1
   pmap_range_t range, *prev;
d5470 1
a5470 1
   /* look for start address */
d5472 20
a5491 6
   for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
      if (start <= range->start)
         break;
      if (start < range->end) {
         if (end < range->end) {
            pmap_range_t new;
d5493 1
a5493 1
            /* split this range */
d5495 5
a5499 23
            new = pmap_range_alloc();
            new->next = range->next;
            new->start = end;
            new->end = range->end;

            range->next = new;
            range->end = start;
            return;
         }

         /* truncate this range */

         range->end = start;
      }
   }

   /* start address is not in the middle of a range */

   while ((range != 0) && (range->end <= end)) {
      *prev = range->next;
      pmap_range_free(range);
      range = *prev;
   }
d5501 2
a5502 2
   if ((range != 0) && (range->start < end))
      range->start = end;
@


1.17
log
@Booting kernel with MACHINE_NEW_NONCONTIG.  UVM code added but not working.
New stand config.  Lots of header fixes.  Can now cross-compile i386->m88k.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2001/01/12 07:29:26 smurph Exp $	*/
d99 1
a99 1

d133 1
a133 1
int pmap_con_dbg = CD_FULL | CD_ALL;
d173 5
a177 2
#define	OBIO_PDT_SIZE	(M88K_BTOP(OBIO_SIZE) * sizeof(pt_entry_t))

d267 1
a267 2

#endif 
d2000 1
a2000 1
   DEBUG ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
d2331 1
a2331 1
#if	DEBUG
a2434 1

d2493 1
a2584 1

d3039 1
a3039 1
   Retry:
d3530 1
a3530 1
#if	DBG
d3601 1
a3601 1
#if	DBG
a3608 2


d3640 1
a3641 1

a3675 1

d4094 1
a4094 1
   clear_modify_Retry:
a4121 1

a4148 1

a4149 1

a4271 1

d4341 1
a4341 1
   clear_reference_Retry:
a4365 1

a4394 1

a4471 1

a4484 1

d5638 1
a5638 1
   start_overlaps:
@


1.16
log
@Update vm interface to MACHIN_NEW_NONCONTIG.  Fix compile warning in pcctwo.c
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2000/12/28 21:21:24 smurph Exp $	*/
d59 3
d146 1
a146 1
STATIC struct pmap   kernel_pmap_store;
a368 3
int   maxcmmu_pb = 4;   /* max number of CMMUs per processors pbus	*/
int   n_cmmus_pb = 1;   /* number of CMMUs per processors pbus		*/

d1829 3
d1833 1
d1984 3
d1988 1
d2843 3
d2848 1
d2868 3
d2872 2
a3722 1

d3729 1
d3735 1
a3735 1

@


1.15
log
@mvme88k updates to -current.  finally!
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2000/12/21 16:54:56 aaron Exp $	*/
d64 1
a192 14

/* 	The PV (Physical to virtual) List.
 *
 * For each vm_page_t, pmap keeps a list of all currently valid virtual
 * mappings of that page. An entry is a pv_entry_t; the list is the
 * pv_head_table. This is used by things like pmap_remove, when we must
 * find and remove all mappings for a particular physical page.
 */
typedef  struct pv_entry {
   struct pv_entry   *next;      /* next pv_entry */
   pmap_t      pmap;    /* pmap where mapping lies */
   vm_offset_t va;      /* virtual address for mapping */
} *pv_entry_t;

d198 10
d214 7
d222 41
d322 1
a322 2
#define LOCK_PVH(index)		simple_lock(&(pv_lock_table[index]))
#define UNLOCK_PVH(index)	simple_unlock(&(pv_lock_table[index]))
a328 11
 * First and last physical address that we maintain any information
 * for. Initialized to zero so that pmap operations done before
 * pmap_init won't touch any non-existent structures.
 */

static vm_offset_t   pmap_phys_start   = (vm_offset_t) 0;
static vm_offset_t   pmap_phys_end  = (vm_offset_t) 0;

#define PMAP_MANAGED(pa) (pmap_initialized && ((pa) >= pmap_phys_start && (pa) < pmap_phys_end))

/*
d440 1
d506 1
d529 1
d543 1
a543 1

d1486 1
d1507 1
d1551 83
d1653 3
a1655 4
   pvl_table_size = PV_LOCK_TABLE_SIZE(npages);
   s = (vm_size_t)(npages * sizeof(struct pv_entry)  /* pv_list */
                   + pvl_table_size           /* pv_lock_table */
                   + npages);              /* pmap_modify_list */
d1669 1
a1669 1
   addr = (vm_offset_t)(pv_head_table + npages);
d1675 1
a1675 1
   addr = (vm_offset_t)pv_lock_table + pvl_table_size;
d1698 1
a1698 1

a2108 1
 *	PFIDX
d2111 1
a2111 1
 *	PFIDX_TO_PVH
d2204 1
a2204 2
         pfi = PFIDX(pa);
         LOCK_PVH(pfi);
d2209 1
a2209 1
         pvl = PFIDX_TO_PVH(pfi);
d2248 1
a2248 1
         UNLOCK_PVH(pfi);
d2274 1
a2274 1
               pmap_modify_list[pfi] = 1;
d2355 1
a2355 2
 *	PFIDX
 *	PFIDX_TO_PVH
d2412 1
a2412 2
   pfi = PFIDX(phys);
   pvl = PFIDX_TO_PVH(pfi);
d2414 1
a2414 1
   LOCK_PVH(pfi);
d2422 1
a2422 1
         UNLOCK_PVH(pfi);
d2474 1
a2474 1
            pmap_modify_list[pfi] = 1;
d2489 1
a2489 1
   UNLOCK_PVH(pfi);
d2505 1
a2505 1
 *		PFIDX_TO_PVH
d2541 2
a2542 2
   copy_on_write_Retry:
   pv_e = PFIDX_TO_PVH(PFIDX(phys));
d2544 1
a2544 1
   LOCK_PVH(PFIDX(phys));
d2552 1
a2552 1
      UNLOCK_PVH(PFIDX(phys));
d2571 1
a2571 1
         UNLOCK_PVH(PFIDX(phys));
d2617 1
a2617 1
   CHECK_PV_LIST(phys, PFIDX_TO_PVH(PFIDX(phys)), "pmap_copy_on_write");
d2619 1
a2619 1
   UNLOCK_PVH(PFIDX(phys));
d3132 2
a3133 3
         pfi = PFIDX(pa);
         LOCK_PVH(pfi);
         pvl = PFIDX_TO_PVH(pfi);
d3163 1
a3163 1
               UNLOCK_PVH(pfi);
d3178 1
a3178 1
         UNLOCK_PVH(pfi);
d3605 1
a3605 3
 *		pmap		pointer to pmap structure
 *		pcbp		pointer to current pcb
 *		cpu		CPU number
d3608 1
a3608 1
pmap_activate(pmap_t pmap, pcb_t pcb, int cpu)
d3610 60
a3669 2
#ifdef	lint
   my_cpu++;
d3671 2
a3672 2
   cpu = cpu_number();  /* hack to fix bogus cpu number */
   PMAP_ACTIVATE(pmap, pcb, cpu);
d3686 1
a3686 3
 *		pmap		pointer to pmap structure
 *		pcb		pointer to pcb
 *		cpu		CPU number
d3689 1
a3689 1
pmap_deactivate(pmap_t pmap, pcb_t pcb,int cpu)
d3691 12
a3702 4
#ifdef	lint
   pmap++; th++; which_cpu++;
#endif
   PMAP_DEACTIVATE(pmap, pcb, cpu);
d4042 1
a4042 2
 *		PFIDX
 *		PFIDX_TO_PVH
d4080 1
a4080 2
   pfi = PFIDX(phys);
   pvl = PFIDX_TO_PVH(pfi);
d4082 1
a4082 2
   LOCK_PVH(pfi);

d4085 1
a4085 1
   pmap_modify_list[pfi] = 0;
d4093 1
a4093 1
      UNLOCK_PVH(pfi);
d4104 1
a4104 1
         UNLOCK_PVH(pfi);
d4141 1
a4141 1
   UNLOCK_PVH(pfi);
d4167 1
a4167 2
 *		PFIDX
 *		PFIDX_TO_PVH
d4204 1
a4204 2
   pfi = PFIDX(phys);
   pvl = PFIDX_TO_PVH(pfi);
d4206 1
a4206 1
   is_mod_Retry:
d4208 1
a4208 1
   if ((boolean_t) pmap_modify_list[pfi]) {
d4218 1
a4218 1
   LOCK_PVH(pfi);
d4223 1
a4223 1
      modified_flag = (boolean_t) pmap_modify_list[pfi];
d4228 1
a4228 1
      UNLOCK_PVH(pfi);
d4237 1
a4237 1
         UNLOCK_PVH(pfi);
d4253 1
a4253 1
            UNLOCK_PVH(pfi);
d4264 1
a4264 1
   UNLOCK_PVH(pfi);
d4290 1
a4290 2
 *		PFIDX
 *		PFIDX_TO_PVH
d4331 2
a4332 3
   pfi = PFIDX(phys);
   LOCK_PVH(pfi);
   pvl = PFIDX_TO_PVH(pfi);
d4341 1
a4341 1
      UNLOCK_PVH(pfi);
d4352 1
a4352 1
         UNLOCK_PVH(pfi);
d4389 1
a4389 1
   UNLOCK_PVH(pfi);
d4416 1
a4416 2
 *	PFIDX
 *	PFIDX_TO_PVH
d4444 1
a4444 2
   pfi = PFIDX(phys);
   pvl = PFIDX_TO_PVH(pfi);
d4447 1
a4447 1
   is_ref_Retry:
d4454 1
a4454 1
   LOCK_PVH(pfi);
d4460 1
a4460 1
         UNLOCK_PVH(pfi);
d4470 1
a4470 1
            UNLOCK_PVH(pfi);
d4481 1
a4481 1
   UNLOCK_PVH(pfi);
a4495 3
 *				Macro chnged below,
 *					pa_index   --> PFIDX
 *					pai_to_pvh --> PFI_TO_PVH
d4499 1
a4499 2
 *		PFIDX
 *		PFI_TO_PVH
d4527 2
a4528 2
   pv_h = PFIDX_TO_PVH(PFIDX(phys));
   LOCK_PVH(PFIDX(phys));
d4531 1
a4531 1
   UNLOCK_PVH(PFIDX(phys));
d4658 2
a4659 3
         pfi = PFIDX(pa);
         LOCK_PVH(pfi);
         pvl = PFIDX_TO_PVH(pfi);
d4662 1
a4662 1
         UNLOCK_PVH(pfi);
d4900 1
a4900 1
   if (pv_h != PFIDX_TO_PVH(PFIDX(phys))) {
d4984 4
a4987 1

d5034 7
d5044 2
a5045 2

      pv_h = PFIDX_TO_PVH(PFIDX(phys));
d5125 4
d5155 8
d5164 1
a5164 1
      pv_h = PFIDX_TO_PVH(PFIDX(phys));
d5167 1
@


1.14
log
@People have difficulty spelling 'initial' and derivatives thereof (too many
"i's" I guess).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2000/02/22 19:27:56 deraadt Exp $	*/
d736 1
d744 1
a744 1

@


1.13
log
@enlarge msgbuf, somewhat line netbsd did
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/09/27 19:13:23 smurph Exp $	*/
d286 1
a286 1
 * for. Initalized to zero so that pmap operations done before
d607 1
a607 1
 * initalizes all its entries (invalidates them), and sets the
d654 1
a654 1
 *    Map memory at initalization. The physical addresses being
d756 1
a756 1
 *    Map memory using BATC at initalization. The physical addresses being
d1108 1
a1108 1
    * Initialilze kernel_pmap structure
d1751 1
a1751 1
    * Initalize SDT_ENTRIES.
@


1.12
log
@Added to support MVME188 and MVME197
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1999/09/03 18:01:31 art Exp $	*/
d1347 1
a1347 1
   SYSMAP(struct msgbuf *, msgbufmap ,msgbufp, 1);
@


1.12.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1347 1
a1347 1
   SYSMAP(struct msgbuf *, msgbufmap ,msgbufp, btoc(MSGBUFSIZE));
@


1.12.4.2
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2001/03/22 01:15:35 miod Exp $	*/
d54 1
d56 4
a59 1
#include <sys/systm.h>
a63 9
#include <sys/user.h>

#include <vm/vm.h>
#include <vm/vm_kern.h>			/* vm/vm_kern.h */
#if defined(UVM)
#include <uvm/uvm.h>
#endif

#include <machine/asm_macro.h>
a64 1
#include <machine/board.h>
a65 1
#include <machine/m882xx.h>		/* CMMU stuff */
d95 1
a95 1
   #define	DBG
a99 1
   #define CD_NONE		0x00
d101 1
a101 1
   #define CD_FULL		0x02
d103 1
a103 1
   #define CD_ACTIVATE		0x0000004	/* pmap_activate */
d107 1
a107 1
   #define CD_CACHE		0x0000040	/* pmap_cache_ctrl */
d110 1
a110 1
   #define CD_CREAT		0x0000200	/* pmap_create */
d112 1
a112 1
   #define CD_DESTR		0x0000800	/* pmap_destroy */
d125 2
a126 2
   #define CD_CHKPV		0x1000000	/* check_pv_list */
   #define CD_CHKPM		0x2000000	/* check_pmap_consistency */
d129 4
a132 1
int pmap_con_dbg = CD_NONE;
d142 1
a142 1
struct pmap	kernel_pmap_store;
d169 1
a169 5
#if defined(MVME188) 
#define	M188_PDT_SIZE	(M88K_BTOP(UTIL_SIZE) * sizeof(pt_entry_t))
#else
#define	M188_PDT_SIZE 0
#endif 
a170 7
#if (defined(MVME187) || defined(MVME197))
#define	M1x7_PDT_SIZE	(M88K_BTOP(OBIO_SIZE) * sizeof(pt_entry_t))
#else
#define	M1x7_PDT_SIZE 0
#endif 

#define	OBIO_PDT_SIZE	((cputyp == CPU_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
d192 14
a210 10

#if !defined(MACHINE_NEW_NONCONTIG)
/*
 * First and last physical address that we maintain any information
 * for. Initialized to zero so that pmap operations done before
 * pmap_init won't touch any non-existent structures.
 */
static vm_offset_t   pmap_phys_start   = (vm_offset_t) 0;
static vm_offset_t   pmap_phys_end  = (vm_offset_t) 0;

a216 7
#define	PA_TO_PVH(pa)		(&pv_head_table[PFIDX(pa)])
#define PMAP_MANAGED(pa)	(pmap_initialized && \
	((pa) >= pmap_phys_start && (pa) < pmap_phys_end))
#define LOCK_PVH(pa)		simple_lock(&(pv_lock_table[PFIDX(pa)]))
#define UNLOCK_PVH(pa)		simple_unlock(&(pv_lock_table[PFIDX(pa)]))
#define	PA_TO_ATTRIB(pa)	(pmap_modify_list[PFIDX(pa)])
#define	SET_ATTRIB(pa, attr)	(pmap_modify_list[PFIDX(pa)] = (attr))
a217 40
#else
#define	PMAP_MANAGED(pa) (pmap_initialized &&			\
			 vm_physseg_find(atop((pa)), NULL) != -1)

#define	PA_TO_PVH(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})
#define	LOCK_PVH(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	simple_lock(&vm_physmem[bank_].pmseg.plock[pg_]);		\
})
#define	UNLOCK_PVH(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	simple_unlock(&vm_physmem[bank_].pmseg.plock[pg_]);		\
})
#define	PA_TO_ATTRIB(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	vm_physmem[bank_].pmseg.attrs[pg_];				\
})
#define	SET_ATTRIB(pa, attr)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	vm_physmem[bank_].pmseg.attrs[pg_] = (attr);			\
})
#endif /* !defined(MACHINE_NEW_NONCONTIG) */
d277 2
a278 1
#define PV_TABLE_SIZE(n)	((vm_size_t)((n) * sizeof(struct pv_entry)))
d285 11
d309 1
a309 1
#ifdef DEBUG
d333 3
d351 2
a352 2
 *	users	bit patterns of the CPUs which may hold the TLB, and
 *		should be flushed
d364 2
a365 2
	register int cpu;
	long tusers = users;
d367 5
a371 5
#ifdef DIAGNOSTIC
	if ((tusers != 0) && (ff1(tusers) >= MAX_CPUS)) {
		printf("ff1 users = %d!\n", ff1(tusers));
		panic("bogus amount of users!!!");
	}
d373 133
a505 6
	while ((cpu = ff1(tusers)) != 32) {
		if (cpu_sets[cpu]) { /* just checking to make sure */
			cmmu_flush_remote_tlb(cpu, kernel, va, M88K_PGBYTES);
		}
		tusers &= ~(1 << cpu);
	}
d523 1
a523 1
	pte_template_t p;
d525 2
a526 2
	p.bits = 0;
	p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;
d528 1
a528 1
	return (p.bits);
d532 1
d558 1
a558 1
	sdt_entry_t *sdt;
d560 15
a574 14
	/*XXX will this change if physical memory is not contiguous? */
	/* take a look at PDTIDX XXXnivas */
	if (map == PMAP_NULL)
		panic("pmap_pte: pmap is NULL");

	sdt = SDTENT(map,virt);
	/*
	 * Check whether page table is exist or not.
	 */
	if (!SDT_VALID(sdt))
		return (PT_ENTRY_NULL);
	else
		return ((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
			PDTIDX(virt));
d578 1
d607 1
a607 1
 * initializes all its entries (invalidates them), and sets the
d618 28
a645 4
	int        aprot;
	sdt_entry_t      *sdt;
	kpdt_entry_t  kpdt_ent;
	pmap_t     map = kernel_pmap;
d647 1
a647 24
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
		printf("(pmap_expand_kmap :%x) v %x\n", curproc,virt);
#endif
	aprot = m88k_protection (map, prot);

	/*  segment table entry derivate from map and virt. */
	sdt = SDTENT(map, virt);
	if (SDT_VALID(sdt))
		panic("pmap_expand_kmap: segment table entry VALID");

	kpdt_ent = kpdt_free;
	if (kpdt_ent == KPDT_ENTRY_NULL) {
		printf("pmap_expand_kmap: Ran out of kernel pte tables\n");
		return (PT_ENTRY_NULL);
	}
	kpdt_free = kpdt_free->next;

	((sdt_entry_template_t *)sdt)->bits = kpdt_ent->phys | aprot | DT_VALID;
	((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = (vm_offset_t)kpdt_ent | aprot | DT_VALID;
	(unsigned)(kpdt_ent->phys) = 0;
	(unsigned)(kpdt_ent->next) = 0;

	return ((pt_entry_t *)(kpdt_ent) + PDTIDX(virt));
d654 1
a654 1
 *    Map memory at initialization. The physical addresses being
d693 35
a727 16
	int		aprot;
	unsigned	npages;
	unsigned	num_phys_pages;
	unsigned	cmode;
	pt_entry_t	*pte;
	pte_template_t	template;
#ifdef MVME197
	static unsigned	i = 0;
#endif
	/*
	 * cache mode is passed in the top 16 bits.
	 * extract it from there. And clear the top
	 * 16 bits from prot.
	 */
	cmode = (prot & 0xffff0000) >> 16;
	prot &= 0x0000ffff;
d729 18
a746 8
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM))
		printf ("(pmap_map :%x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
			curproc, start, end, virt, prot, cmode);
#endif

	if (start > end)
		panic("pmap_map: start greater than end address");
d748 1
a748 33
	aprot = m88k_protection (kernel_pmap, prot);

	template.bits = M88K_TRUNC_PAGE(start) | aprot | cmode | DT_VALID;

	npages = M88K_BTOP(M88K_ROUND_PAGE(end) - M88K_TRUNC_PAGE(start));

	for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {

		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
				panic ("pmap_map: Cannot allocate pte table");

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			if (pte->dtype)
				printf("(pmap_map :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
#endif
		*pte = template.pte;
#ifdef MVME197
		/* hack for MVME197 */
		if (cputyp == CPU_197) {
			if (i < 32) {
				m197_load_patc(i, virt, 
					       (vm_offset_t)template.bits, 1);
				i++;
			}
		}
#endif 
		virt += M88K_PGBYTES;
		template.bits += M88K_PGBYTES;
	}

	return (virt);
d756 1
a756 1
 *    Map memory using BATC at initialization. The physical addresses being
d799 1
a799 1
	      vm_prot_t prot, unsigned cmode)
d801 77
a877 76
	int		aprot;
	unsigned	num_phys_pages;
	vm_offset_t	phys;
	pt_entry_t	*pte;
	pte_template_t	template;
	batc_template_t	batctmp;
	register int	i;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
		printf ("(pmap_map_batc :%x) phys address from %x to %x mapped at virtual %x, prot %x\n", curproc,
			start, end, virt, prot);
#endif

	if (start > end)
		panic("pmap_map_batc: start greater than end address");

	aprot = m88k_protection (kernel_pmap, prot);
	template.bits = M88K_TRUNC_PAGE(start) | aprot | DT_VALID | cmode;
	phys = start;
	batctmp.bits = 0;
	batctmp.field.sup = 1;	     /* supervisor */
	batctmp.field.wt = template.pte.wt;	 /* write through */
	batctmp.field.g = template.pte.g;     /* global */
	batctmp.field.ci = template.pte.ci;	 /* cache inhibit */
	batctmp.field.wp = template.pte.prot; /* protection */
	batctmp.field.v = 1;	     /* valid */

	num_phys_pages = M88K_BTOP(M88K_ROUND_PAGE(end) - 
				   M88K_TRUNC_PAGE(start));

	while (num_phys_pages > 0) {

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
			printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, aligne V=%d, phys=%x, aligne P=%d\n", curproc,
			       num_phys_pages, virt, BATC_BLK_ALIGNED(virt), phys, BATC_BLK_ALIGNED(phys));
#endif

		if ( BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(phys) && 
		     num_phys_pages >= BATC_BLKBYTES/M88K_PGBYTES &&
		     batc_used < BATC_MAX ) {
			/*
			 * map by BATC
			 */
			batctmp.field.lba = M88K_BTOBLK(virt);
			batctmp.field.pba = M88K_BTOBLK(phys);

			for ( i = 0; i < max_cpus; i++)
				if (cpu_sets[i])
					cmmu_set_pair_batc_entry(i, batc_used, 
								 batctmp.bits);
			batc_entry[batc_used] = batctmp.field;
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_MAPB | CD_NORM)) == (CD_MAPB | CD_NORM)) {
				printf("(pmap_map_batc :%x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
			}
			if (pmap_con_dbg & CD_MAPB) {

				for (i = 0; i < BATC_BLKBYTES; i += M88K_PGBYTES ) {
					pte = pmap_pte(kernel_pmap, virt+i);
					if (pte->dtype)
						printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, ((pte_template_t *)pte)->bits);
				}
			}
#endif
			batc_used++;
			virt += BATC_BLKBYTES;
			phys += BATC_BLKBYTES;
			template.pte.pfn = M88K_BTOP(phys);
			num_phys_pages -= BATC_BLKBYTES/M88K_PGBYTES;
			continue;
		}
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
				panic ("pmap_map_batc: Cannot allocate pte table");
d880 3
a882 3
		if (pmap_con_dbg & CD_MAPB)
			if (pte->dtype)
				printf("(pmap_map_batc :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
d885 6
a890 6
		*pte = template.pte;
		virt += M88K_PGBYTES;
		phys += M88K_PGBYTES;
		template.bits += M88K_PGBYTES;
		num_phys_pages--;
	}
d892 1
a892 1
	return (M88K_ROUND_PAGE(virt));
d931 16
a946 16
	int		spl, spl_sav;
	pt_entry_t	*pte;
	vm_offset_t	va;
	int		kflush;
	int		cpu;
	register unsigned	users;
	register pte_template_t	opte;

#ifdef DEBUG
	if ( mode & CACHE_MASK ) {
		printf("(cache_ctrl) illegal mode %x\n",mode);
		return;
	}
	if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
		printf("(pmap_cache_ctrl :%x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
	}
d949 23
a971 23
	if ( pmap == PMAP_NULL ) {
		panic("pmap_cache_ctrl: pmap is NULL");
	}

	PMAP_LOCK(pmap, spl);

	/*
	 * 
	 */
	users = pmap->cpus_using;
	if (pmap == kernel_pmap) {
		kflush = 1;
	} else {
		kflush = 0;
	}

	for (va = s; va < e; va += M88K_PGBYTES) {
		if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
			continue;
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
			printf("(cache_ctrl) pte@@0x%08x\n",(unsigned)pte);
		}
d974 18
a991 20
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by other cpu.
		 *  XXX
		 */
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		((pte_template_t *)pte)->bits = (opte.bits & CACHE_MASK) | mode;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);

		/*
		 * Data cache should be copied back and invalidated.
		 */
		for (cpu=0; cpu<max_cpus; cpu++)
			if (cpu_sets[cpu])
				/*cmmu_flush_remote_data_cache(cpu, 
				M88K_PTOB(pte->pfn),M88K_PGBYTES);*/
				cmmu_flush_remote_cache(cpu, M88K_PTOB(pte->pfn),
							M88K_PGBYTES);
d993 1
a993 1
	}
d995 1
a995 1
	PMAP_UNLOCK(pmap, spl);
d1049 22
a1070 19
	       vm_offset_t   *phys_start,   /* IN/OUT */
	       vm_offset_t   *phys_end,	 /* IN */
	       vm_offset_t   *virt_start,   /* OUT */
	       vm_offset_t   *virt_end)	 /* OUT */
{
	kpdt_entry_t	kpdt_virt;
	sdt_entry_t	*kmap;
	vm_offset_t	vaddr,
			virt,
			kpdt_phys,
			s_text,
			e_text,
			kernel_pmap_size;
	apr_template_t	apr_data;
	pt_entry_t	*pte;
	int		i;
	pmap_table_t	ptable;
	extern char	*kernelstart, *etext;
	extern void	cmmu_go_virt(void);
d1073 65
a1137 65
	if ((pmap_con_dbg & (CD_BOOT | CD_NORM)) == (CD_BOOT | CD_NORM)) {
		printf("pmap_bootstrap : \"load_start\" 0x%x\n", load_start);
	}
#endif
	ptes_per_vm_page = PAGE_SIZE >> M88K_PGSHIFT;
	if (ptes_per_vm_page == 0) {
		panic("pmap_bootstrap: VM page size < MACHINE page size");
	}
	if (!PAGE_ALIGNED(load_start)) {
		panic("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x", load_start);
	}

	simple_lock_init(&kernel_pmap->lock);

	/*
	 * Allocate the kernel page table from the front of available
	 * physical memory,
	 * i.e. just after where the kernel image was loaded.
	 */
	/*
	 * The calling sequence is 
	 *    ...
	 *  pmap_bootstrap(&kernelstart,...) 
	 *  kernelstart is the first symbol in the load image.
	 *  We link the kernel such that &kernelstart == 0x10000 (size of
	 *							BUG ROM)
	 *  The expression (&kernelstart - load_start) will end up as
	 *	0, making *virt_start == *phys_start, giving a 1-to-1 map)
	 */

	*phys_start = M88K_ROUND_PAGE(*phys_start);
	*virt_start = *phys_start +
		      (M88K_TRUNC_PAGE((unsigned)&kernelstart) - load_start);

	/*
	 * Initialize kernel_pmap structure
	 */
	kernel_pmap->ref_count = 1;
	kernel_pmap->cpus_using = 0;
	kernel_pmap->sdt_paddr = kmap = (sdt_entry_t *)(*phys_start);
	kernel_pmap->sdt_vaddr = (sdt_entry_t *)(*virt_start);
	kmapva = *virt_start;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kernel_pmap->sdt_paddr = %x\n",kernel_pmap->sdt_paddr);
		printf("kernel_pmap->sdt_vaddr = %x\n",kernel_pmap->sdt_vaddr);
	}
	/* init double-linked list of pmap structure */
	kernel_pmap->next = kernel_pmap;
	kernel_pmap->prev = kernel_pmap;
#endif

	/* 
	 * Reserve space for segment table entries.
	 * One for the regular segment table and one for the shadow table
	 * The shadow table keeps track of the virtual address of page
	 * tables. This is used in virtual-to-physical address translation
	 * functions. Remember, MMU cares only for physical addresses of
	 * segment and page table addresses. For kernel page tables, we
	 * really don't need this virtual stuff (since the kernel will
	 * be mapped 1-to-1) but for user page tables, this is required.
	 * Just to be consistent, we will maintain the shadow table for
	 * kernel pmap also.
	 */
d1139 1
a1139 1
	kernel_pmap_size = 2*SDT_SIZE;
d1141 2
a1142 2
	printf("kernel segment table from 0x%x to 0x%x\n", kernel_pmap->sdt_vaddr, 
	       kernel_pmap->sdt_vaddr + kernel_pmap_size);
d1144 6
a1149 6
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = (*phys_start + kernel_pmap_size);
	kpdt_virt = (kpdt_entry_t)(*virt_start + kernel_pmap_size);
	kernel_pmap_size += MAX_KERNEL_PDT_SIZE;
	*phys_start += kernel_pmap_size;
	*virt_start += kernel_pmap_size;
d1151 2
a1152 2
	/* init all segment and page descriptor to zero */
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);
d1154 1
a1154 1
	printf("kernel page table to 0x%x\n", kernel_pmap->sdt_vaddr + kernel_pmap_size);
d1158 28
a1185 28
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kpdt_phys = %x\n",kpdt_phys);
		printf("kpdt_virt = %x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x  ; (phys)0x%08x\n",
		       *virt_start,*phys_start);
	}
#endif
	/*
	 * init the kpdt queue
	 */
	kpdt_free = kpdt_virt;
	for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i>0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vm_offset_t)kpdt_virt + PDT_SIZE);
		kpdt_virt->phys = kpdt_phys;
		kpdt_virt = kpdt_virt->next;
		kpdt_phys += PDT_SIZE;
	}
	kpdt_virt->next = KPDT_ENTRY_NULL; /* terminate the list */

	/*
	 * Map the kernel image into virtual space
	 */

	s_text = load_start;	     /* paddr of text */
	e_text = load_start + ((unsigned)&etext -
			       M88K_TRUNC_PAGE((unsigned)&kernelstart));
	/* paddr of end of text section*/
	e_text = M88K_ROUND_PAGE(e_text);
d1193 11
a1203 41
	/*  map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
	vaddr = PMAPER(
		      0,
		      0,
		      0x10000,
		      (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_INH <<16));

	assert(vaddr == M88K_TRUNC_PAGE((unsigned)&kernelstart));

	/*  map the kernel text read only */
	vaddr = PMAPER(
		      (vm_offset_t)M88K_TRUNC_PAGE(((unsigned)&kernelstart)),
		      s_text,
		      e_text,
		      (VM_PROT_WRITE | VM_PROT_READ)|(CACHE_GLOBAL<<16));  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(
		      vaddr,
		      e_text,
		      (vm_offset_t)kmap,
		      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
	/*
	 * Map system segment & page tables - should be cache inhibited?
	 * 88200 manual says that CI bit is driven on the Mbus while accessing
	 * the translation tree. I don't think we need to map it CACHE_INH
	 * here...
	 */
	if (kmapva != vaddr) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("(pmap_bootstrap) correcting vaddr\n");
		}
#endif
		while (vaddr < (*virt_start - kernel_pmap_size))
			vaddr = M88K_ROUND_PAGE(vaddr + 1);
	}
	vaddr = PMAPER(
		      vaddr,
		      (vm_offset_t)kmap,
		      *phys_start,
		      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
d1205 43
a1247 10
	if (vaddr != *virt_start) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("1:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
			       *virt_start, *phys_start);
		}
#endif
		*virt_start = vaddr;
		*phys_start = round_page(*phys_start);
	}
d1250 31
a1280 31
	/*
	 *  Get ethernet buffer - need etherlen bytes physically contiguous.
	 *  1 to 1 mapped as well???. There is actually a bug in the macros
	 *  used by the 1x7 ethernet driver. Remove this when that is fixed.
	 *  XXX -nivas
	 */
	if (cputyp != CPU_188) { /*  != CPU_188 */
		*phys_start = vaddr;
		etherlen = ETHERPAGES * NBPG;
		etherbuf = (void *)vaddr;

		vaddr = PMAPER(
			      vaddr,
			      *phys_start,
			      *phys_start + etherlen,
			      (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));

		*virt_start += etherlen;
		*phys_start += etherlen; 

		if (vaddr != *virt_start) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("2:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
				       *virt_start, *phys_start);
			}
#endif
			*virt_start = vaddr;
			*phys_start = round_page(*phys_start);
		}
	}
d1284 2
a1285 2
	*virt_start = round_page(*virt_start);
	*virt_end = VM_MAX_KERNEL_ADDRESS;
d1287 24
a1310 24
	/*
	 * Map a few more pages for phys routines and debugger.
	 */

	phys_map_vaddr1 = round_page(*virt_start);
	phys_map_vaddr2 = phys_map_vaddr1 + PAGE_SIZE * max_cpus;

	/*
	 * To make 1:1 mapping of virt:phys, throw away a few phys pages.
	 * XXX what is this? nivas
	 */

	*phys_start += 2 * PAGE_SIZE * max_cpus;
	*virt_start += 2 * PAGE_SIZE * max_cpus;

	/*
	 * Map all IO space 1-to-1. Ideally, I would like to not do this
	 * but have va for the given IO address dynamically allocated. But
	 * on the 88200, 2 of the BATCs are hardwired to map the IO space
	 * 1-to-1; I decided to map the rest of the IO space 1-to-1.
	 * And bug ROM & the SRAM need to be mapped 1-to-1 if we ever want to
	 * execute bug system calls after the MMU has been turned on.
	 * OBIO should be mapped cache inhibited.
	 */
d1312 1
a1312 1
	ptable = pmap_table_build(0);	 /* see pmap_table.c for details */
d1314 21
a1334 24
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("pmap_bootstrap: -> pmap_table_build\n");
	}
#endif

	for (  ; ptable->size != 0xffffffffU; ptable++)
		if (ptable->size) {
			/*
			 * size-1, 'cause pmap_map rounds up to next pagenumber
			 */
			PMAPER(ptable->virt_start,
			       ptable->phys_start,
			       ptable->phys_start + (ptable->size - 1),
			       ptable->prot|(ptable->cacheability << 16));
		}

		/*
		 * Allocate all the submaps we need. Note that SYSMAP just allocates
		 * kernel virtual address with no physical backing memory. The idea
		 * is physical memory will be mapped at this va before using that va.
		 * This means that if different physcal pages are going to be mapped
		 * at different times, we better do a tlb flush before using it -	         
		 * else we will be referencing the wrong page.
		 */
d1344 1
a1344 7
	virt = *virt_start;

	SYSMAP(caddr_t, vmpte , vmmap, 1);
	SYSMAP(struct msgbuf *, msgbufmap ,msgbufp, btoc(MSGBUFSIZE));

	vmpte->pfn = -1;
	vmpte->dtype = DT_INVALID;
d1346 2
a1347 1
	*virt_start = virt;
d1349 2
a1350 7
	/*
	 * Set translation for UPAGES at UADDR. The idea is we want to
	 * have translations set up for UADDR. Later on, the ptes for
	 * for this address will be set so that kstack will refer
	 * to the u area. Make sure pmap knows about this virtual
	 * address by doing vm_findspace on kernel_map.
	 */
d1352 35
a1386 25
	for (i = 0, virt = UADDR; i < UPAGES; i++, virt += PAGE_SIZE) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("setting up mapping for Upage %d @@ %x\n", i, virt);
		}
#endif
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
	}

	/*
	 * Switch to using new page tables
	 */

	apr_data.bits = 0;
	apr_data.field.st_base = M88K_BTOP(kernel_pmap->sdt_paddr);
	apr_data.field.wt = 1;
	apr_data.field.g  = 1;
	apr_data.field.ci = 0;
	apr_data.field.te = 1; /* Translation enable */
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		void show_apr(unsigned value);
		show_apr(apr_data.bits);
	}
d1388 1
a1388 1
	/* Invalidate entire kernel TLB. */
d1390 40
a1429 3
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("invalidating tlb %x\n", apr_data.bits);
	}
d1431 1
d1433 1
a1433 30
	for (i = 0; i < MAX_CPUS; i++)
		if (cpu_sets[i]) {
			/* Invalidate entire kernel TLB. */
			cmmu_flush_remote_tlb(i, 1, 0, -1);
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("After cmmu_flush_remote_tlb()\n");
			}
#endif
			/* still physical */
			/*
			 * Set valid bit to DT_INVALID so that the very first 
			 * pmap_enter() on these won't barf in 
			 * pmap_remove_range().
			 */
			pte = pmap_pte(kernel_pmap, phys_map_vaddr1);
			pte->pfn = -1;
			pte->dtype = DT_INVALID;
			pte = pmap_pte(kernel_pmap, phys_map_vaddr2);
			pte->dtype = DT_INVALID;
			pte->pfn = -1;
			/* Load supervisor pointer to segment table. */
			cmmu_remote_set_sapr(i, apr_data.bits);
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("After cmmu_remote_set_sapr()\n");
			}
#endif
			SETBIT_CPUSET(i, &kernel_pmap->cpus_using);
		}
a1434 7
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("running virtual - avail_next 0x%x\n", *phys_start);
	}
#endif
	avail_next = *phys_start;
	return;
a1448 1
#if !defined(MACHINE_NEW_NONCONTIG)
d1452 1
a1452 1
	register void *mem;
d1454 11
a1464 11
	size = round_page(size);
	mem = (void *)virtual_avail;
	virtual_avail = pmap_map(virtual_avail, avail_start,
				 avail_start + size,
				 VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
	avail_start += size;
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("pmap_bootstrap_alloc: size %x virtual_avail %x avail_start %x\n",
		       size, virtual_avail, avail_start);
	}
d1466 2
a1467 2
	bzero((void *)mem, size);
	return (mem);
a1468 1
#endif /* !defined(MACHINE_NEW_NONCONTIG) */
a1511 1
#ifdef MACHINE_NEW_NONCONTIG
d1513 1
a1513 1
pmap_init(void)
d1515 5
a1519 8
	register long		npages;
	register vm_offset_t	addr;
	register vm_size_t	s;
	register int		i;
	struct pv_entry		*pv;
	char			*attr;
	struct simplelock	*lock;
	int			bank;
d1521 3
a1523 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
		printf("pmap_init()\n");
a1524 10
	/*
	 * Allocate memory for the pv_head_table and its lock bits,
	 * the modify bit array, and the pte_page table.
	 */
	for (npages = 0, bank = 0; bank < vm_nphysseg; bank++)
		npages += vm_physmem[bank].end - vm_physmem[bank].start;

	s = PV_TABLE_SIZE(npages);		/* pv_list */
	s += PV_LOCK_TABLE_SIZE(npages);	/* pv_lock_table */
	s += npages * sizeof(char);		/* pmap_modify_list */
d1526 9
a1534 7
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
		printf("(pmap_init) nbr of managed pages = %x\n", npages);
		printf("(pmap_init) size of pv_list = %x\n",
		       npages * sizeof(struct pv_entry));
	}
#endif
d1536 37
a1572 6
	s = round_page(s);
#if defined(UVM)
	addr = (vaddr_t)uvm_km_zalloc(kernel_map, s);
#else
	addr = (vm_offset_t)kmem_alloc(kernel_map, s);
#endif
d1574 1
a1574 2
	pv_head_table =  (pv_entry_t)addr;
	addr += PV_TABLE_SIZE(npages);
a1575 31
	/*
	 * Assume that 'simple_lock' is used to lock pv_lock_table
	 */
	pv_lock_table = (struct simplelock *)addr; /* XXX */
	addr += PV_LOCK_TABLE_SIZE(npages);

	pmap_modify_list = (char *)addr;

	/*
	* Initialize pv_lock_table
	*/
	for (i = 0; i < npages; i++)
		simple_lock_init(&(pv_lock_table[i]));

	/*
	 * Now that the pv, attribute, and lock tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_head_table;
	lock = pv_lock_table;
	attr = pmap_modify_list;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		vm_physmem[bank].pmseg.plock = lock;
		pv += npages;
		lock += npages;
		attr += npages;
	}
	pmap_initialized = TRUE;
a1577 65
#else
void
pmap_init(vm_offset_t phys_start, vm_offset_t phys_end)
{
	register long		npages;
	register vm_offset_t	addr;
	register vm_size_t	s;
	register int		i;
	vm_size_t		pvl_table_size;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
		printf("(pmap_init) phys_start %x  phys_end %x\n", phys_start, phys_end);
#endif

	/*
	 * Allocate memory for the pv_head_table and its lock bits,
	 * the modify bit array, and the pte_page table.
	 */
	npages = atop(phys_end - phys_start);
	s = PV_TABLE_SIZE(npages);		/* pv_list */
	s += PV_LOCK_TABLE_SIZE(npages);	/* pv_lock_table */
	s += npages * sizeof(char);		/* pmap_modify_list */

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
		printf("(pmap_init) nbr of managed pages = %x\n", npages);
		printf("(pmap_init) size of pv_list = %x\n",
		       npages * sizeof(struct pv_entry));
	}
#endif
	s = round_page(s);
	addr = (vm_offset_t)kmem_alloc(kernel_map, s);

	pv_head_table =  (pv_entry_t)addr;
	addr += PV_TABLE_SIZE(npages);

	/*
	 * Assume that 'simple_lock' is used to lock pv_lock_table
	 */
	pv_lock_table = (struct simplelock *)addr; /* XXX */
	addr += PV_LOCK_TABLE_SIZE(npages);

	pmap_modify_list = (char *)addr;

	/*
	* Initialize pv_lock_table
	*/
	for (i = 0; i < npages; i++)
		simple_lock_init(&(pv_lock_table[i]));

	/*
	  * Only now, when all of the data structures are allocated,
	  * can we set pmap_phys_start and pmap_phys_end. If we set them
	  * too soon, the kmem_alloc above will blow up when it causes
	  * a call to pmap_enter, and pmap_enter tries to manipulate the
	  * (not yet existing) pv_list.
	  */
	pmap_phys_start = phys_start;
	pmap_phys_end = phys_end;

	pmap_initialized = TRUE;

} /* pmap_init() */
#endif 
d1618 26
a1643 24
	vm_offset_t	srcva;
	pte_template_t	template;
	unsigned int	i;
	unsigned int	spl_sav;
	int		my_cpu;
	pt_entry_t	*srcpte;

	my_cpu = cpu_number();
	srcva = (vm_offset_t)(phys_map_vaddr1 + (my_cpu * PAGE_SIZE));
	srcpte = pmap_pte(kernel_pmap, srcva);

	for (i = 0; i < ptes_per_vm_page; i++, phys += M88K_PGBYTES) {
		template.bits = M88K_TRUNC_PAGE(phys)
				| m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
				| DT_VALID | CACHE_GLOBAL;

		spl_sav = splimp();
		cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
		*srcpte = template.pte;
		splx(spl_sav);
		bzero((void*)srcva, M88K_PGBYTES);
		/* force the data out */
		cmmu_flush_remote_data_cache(my_cpu,phys, M88K_PGBYTES);
	}
d1646 1
d1677 1
a1677 1
	pmap_t      p;
d1679 16
a1694 16
	/*
	 * A software use-only map doesn't even need a map.
	 */
	if (size != 0)
		return (PMAP_NULL);

	CHECK_PMAP_CONSISTENCY("pmap_create");

	p = (pmap_t)malloc(sizeof(*p), M_VMPMAP, M_WAITOK);
	if (p == PMAP_NULL) {
		panic("pmap_create: cannot allocate a pmap");
	}

	bzero(p, sizeof(*p));
	pmap_pinit(p);
	return (p);
d1701 10
a1710 16
	pmap_statistics_t stats;
	sdt_entry_t *segdt;
	int i;
	unsigned int s;
	vm_offset_t addr;
	
	/*
	 * Allocate memory for *actual* segment table and *shadow* table.
	 */
	s = M88K_ROUND_PAGE(2 * SDT_SIZE);
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
		printf("(pmap_create :%x) need %d pages for sdt\n",
		       curproc, atop(s));
	}
#endif
d1712 21
a1732 7
#if defined(UVM)
	segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, s);
#else
	segdt = (sdt_entry_t *)kmem_alloc(kernel_map, s);
#endif
	if (segdt == NULL)
		panic("pmap_create: kmem_alloc failure");
d1734 30
a1763 50
	/* use pmap zero page to zero it out */
	addr = (vm_offset_t)segdt;
	for (i=0; i<atop(s); i++) {
		pmap_zero_page(pmap_extract(kernel_pmap, addr));
		addr += PAGE_SIZE;
	}
	
	/*
	 * Initialize pointer to segment table both virtual and physical.
	 */
	p->sdt_vaddr = segdt;
	p->sdt_paddr = (sdt_entry_t *)pmap_extract(kernel_pmap,(vm_offset_t)segdt);

	if (!PAGE_ALIGNED(p->sdt_paddr)) {
		printf("pmap_create: std table = %x\n",(int)p->sdt_paddr);
		panic("pmap_create: sdt_table not aligned on page boundary");
	}

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
		printf("(pmap_create :%x) pmap=0x%x, sdt_vaddr=0x%x, sdt_paddr=0x%x\n",
		       curproc, (unsigned)p, p->sdt_vaddr, p->sdt_paddr);
	}
#endif

#ifdef MVME188
	if (cputyp == CPU_188) {
		/*
		 * memory for page tables should be CACHE DISABLED on MVME188
		 */
		pmap_cache_ctrl(kernel_pmap,
				(vm_offset_t)segdt,
				(vm_offset_t)segdt+ (SDT_SIZE*2),
				CACHE_INH);
	}
#endif
	/*
	 * Initialize SDT_ENTRIES.
	 */
	/*
	 * There is no need to clear segment table, since kmem_alloc would
	 * provides us clean pages.
	 */

	/*
	 * Initialize pmap structure.
	 */
	p->ref_count = 1;
	simple_lock_init(&p->lock);
	p->cpus_using = 0;
d1766 5
a1770 5
	/* initialize block address translation cache */
	for (i = 0; i < BATC_MAX; i++) {
		p->i_batc[i].bits = 0;
		p->d_batc[i].bits = 0;
	}
d1773 6
a1778 6
	/*
	 * Initialize statistics.
	 */
	stats = &p->stats;
	stats->resident_count = 0;
	stats->wired_count = 0;
d1780 6
a1785 6
#ifdef DEBUG
	/* link into list of pmaps, just after kernel pmap */
	p->next = kernel_pmap->next;
	p->prev = kernel_pmap;
	kernel_pmap->next = p;
	p->next->prev = p;
d1814 1
d1818 28
a1845 8
	unsigned long	sdt_va;  /*  outer loop index */
	sdt_entry_t	*sdttbl; /*  ptr to first entry in the segment table */
	pt_entry_t	*gdttbl; /*  ptr to first entry in a page table */
	unsigned int	i,j;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
		printf("(pmap_free_tables :%x) pmap %x\n", curproc, pmap);
d1847 2
d1850 1
a1850 22
	sdttbl = pmap->sdt_vaddr;    /* addr of segment table */
	/* 
	  This contortion is here instead of the natural loop 
	  because of integer overflow/wraparound if VM_MAX_USER_ADDRESS 
	  is near 0xffffffff
	*/
	i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	if ( j < 1024 )	j++;

	/* Segment table Loop */
	for ( ; i < j; i++) {
		sdt_va = PDT_TABLE_GROUP_VA_SPACE*i;
		if ((gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va)) != PT_ENTRY_NULL) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
				printf("(pmap_free_tables :%x) free page table = 0x%x\n",
				       curproc, gdttbl);
#endif
			PT_FREE(gdttbl);
		}
	} /* Segment Loop */
d1852 3
a1854 12
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
		printf("(pmap_free_tables :%x) free segment table = 0x%x\n",
		       curproc, sdttbl);
#endif
	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
#if defined(UVM)
	uvm_km_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
#else
	kmem_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
d1856 4
d1863 1
d1867 7
a1873 7
	pmap_free_tables(p);
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
		printf("(pmap_destroy :%x) ref_count = 0\n", curproc);
	/* unlink from list of pmap structs */
	p->prev->next = p->next;
	p->next->prev = p->prev;
d1875 1
d1905 1
a1905 1
	register int  c, s;
d1907 4
a1910 4
	if (p == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
			printf("(pmap_destroy :%x) pmap is NULL\n", curproc);
d1912 2
a1913 2
		return;
	}
d1915 14
a1928 14
	if (p == kernel_pmap) {
		panic("pmap_destroy: Attempt to destroy kernel pmap");
	}

	CHECK_PMAP_CONSISTENCY("pmap_destroy");

	PMAP_LOCK(p, s);
	c = --p->ref_count;
	PMAP_UNLOCK(p, s);

	if (c == 0) {
		pmap_release(p);
		free((caddr_t)p,M_VMPMAP);
	}
d1951 1
a1951 1
	int     s;
d1953 5
a1957 5
	if (p != PMAP_NULL) {
		PMAP_LOCK(p, s);
		p->ref_count++;
		PMAP_UNLOCK(p, s);
	}
d1961 1
d1988 1
d1991 1
a1991 1
 *	PA_TO_PVH
d2020 148
a2167 147
	int			pfn;
	int			num_removed = 0;
	int			num_unwired = 0;
	register int		i;
	pt_entry_t		*pte;
	pv_entry_t		prev, cur;
	pv_entry_t		pvl;
	vm_offset_t		pa, va, tva;
	register unsigned	users;
	register pte_template_t	opte;
	int			kflush;

	if (e < s)
		panic("pmap_remove_range: end < start");

	/*
	 * Pmap has been locked by pmap_remove.
	 */
	users = pmap->cpus_using;
	if (pmap == kernel_pmap) {
		kflush = 1;
	} else {
		kflush = 0;
	}

	/*
	 * Loop through the range in vm_page_size increments.
	 * Do not assume that either start or end fail on any
	 * kind of page boundary (though this may be true!?).
	 */

	CHECK_PAGE_ALIGN(s, "pmap_remove_range - start addr");

	for (va = s; va < e; va += PAGE_SIZE) {

		sdt_entry_t *sdt;

		sdt = SDTENT(pmap,va);

		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
			continue;
		}

		pte = pmap_pte(pmap,va);

		if (!PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}

		num_removed++;

		if (pte->wired)
			num_unwired++;

		pfn = pte->pfn;
		pa = M88K_PTOB(pfn);

		if (PMAP_MANAGED(pa)) {
			LOCK_PVH(pa);
			/*
			 * Remove the mapping from the pvlist for
			 * this physical page.
			 */
			pvl = PA_TO_PVH(pa);
			CHECK_PV_LIST(pa, pvl, "pmap_remove_range before");

			if (pvl->pmap == PMAP_NULL) {
				panic("pmap_remove_range: null pv_list");
			}

			if (pvl->va == va && pvl->pmap == pmap) {

				/*
				 * Hander is the pv_entry. Copy the next one
				 * to hander and free the next one (we can't
				 * free the hander)
				 */
				cur = pvl->next;
				if (cur != PV_ENTRY_NULL) {
					*pvl = *cur;
					free((caddr_t)cur, M_VMPVENT);
				} else {
					pvl->pmap =  PMAP_NULL;
				}

			} else {

				for (prev = pvl; (cur = prev->next) != PV_ENTRY_NULL; prev = cur) {
					if (cur->va == va && cur->pmap == pmap) {
						break;
					}
				}
				if (cur == PV_ENTRY_NULL) {
					printf("pmap_remove_range: looking for VA "
					       "0x%x (pa 0x%x) PV list at 0x%x\n", va, pa, (unsigned)pvl);
					panic("pmap_remove_range: mapping not in pv_list");
				}

				prev->next = cur->next;
				free((caddr_t)cur, M_VMPVENT);
			}

			CHECK_PV_LIST(pa, pvl, "pmap_remove_range after");
			UNLOCK_PVH(pa);

		} /* if PAGE_MANAGED */

		/*
		 * For each pte in vm_page (NOTE: vm_page, not
		 * M88K (machine dependent) page !! ), reflect
		 * modify bits to pager and zero (invalidate,
		 * remove) the pte entry.
		 */
		tva = va;
		for (i = ptes_per_vm_page; i > 0; i--) {

			/*
			 * Invalidate pte temporarily to avoid being written back
			 * the modified bit and/or the reference bit by other cpu.
			 */
			opte.bits = invalidate_pte(pte);
			flush_atc_entry(users, tva, kflush);

			if (opte.pte.modified) {
				if (IS_VM_PHYSADDR(pa)) {
					vm_page_set_modified(PHYS_TO_VM_PAGE(opte.bits & M88K_PGMASK));
				}
				/* keep track ourselves too */
				if (PMAP_MANAGED(pa))
					SET_ATTRIB(pa, 1);
			}
			pte++;
			tva += M88K_PGBYTES;
		}

	} /* end for ( va = s; ...) */

	/*
	 * Update the counts
	 */
	pmap->stats.resident_count -= num_removed;
	pmap->stats.wired_count -= num_unwired;
d2196 5
a2200 1
	int     spl;
d2202 3
a2204 6
	if (map == PMAP_NULL) {
		return;
	}
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
		printf("(pmap_remove :%x) map %x  s %x  e %x\n", curproc, map, s, e);
d2207 1
a2207 1
	CHECK_PAGE_ALIGN(s, "pmap_remove start addr");
d2209 2
a2210 2
	if (s > e)
		panic("pmap_remove: start greater than end address");
d2212 3
a2214 3
	PMAP_LOCK(map, spl);
	pmap_remove_range(map, s, e);
	PMAP_UNLOCK(map, spl);
d2217 1
d2236 2
a2237 1
 *	PA_TO_PVH
d2263 11
a2273 10
	pv_entry_t		pvl, cur;
	register pt_entry_t	*pte;
	register int		i;
	register vm_offset_t	va;
	register pmap_t		pmap;
	int			spl;
	int			dbgcnt = 0;
	register unsigned	users;
	register pte_template_t	opte;
	int			kflush;
d2275 5
a2279 5
	if (!PMAP_MANAGED(phys)) {
		/* not a managed page. */
#ifdef DEBUG
		if (pmap_con_dbg & CD_RMAL)
			printf("(pmap_remove_all :%x) phys addr 0x%x not a managed page\n", curproc, phys);
d2281 90
a2370 87
		return;
	}
	SPLVM(spl);
	/*
	 * Walk down PV list, removing all mappings.
	 * We have to do the same work as in pmap_remove_pte_page
	 * since that routine locks the pv_head. We don't have
	 * to lock the pv_head, since we have the entire pmap system.
	 */
remove_all_Retry:

	pvl = PA_TO_PVH(phys);
	CHECK_PV_LIST(phys, pvl, "pmap_remove_all before");
	LOCK_PVH(phys);

	/*
	 * Loop for each entry on the pv list
	 */
	while ((pmap = pvl->pmap) != PMAP_NULL) {
		va = pvl->va;
		if (!simple_lock_try(&pmap->lock)) {
			UNLOCK_PVH(phys);
			goto remove_all_Retry;
		}
		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = 1;
		} else {
			kflush = 0;
		}

		pte = pmap_pte(pmap, va);

		/*
		 * Do a few consistency checks to make sure
		 * the PV list and the pmap are in synch.
		 */
		if (pte == PT_ENTRY_NULL) {
			printf("(pmap_remove_all :%x) phys %x pmap %x va %x dbgcnt %x\n",
			       (unsigned)curproc, phys, (unsigned)pmap, va, dbgcnt);
			panic("pmap_remove_all: pte NULL");
		}
		if (!PDT_VALID(pte))
			panic("pmap_remove_all: pte invalid");
		if (M88K_PTOB(pte->pfn) != phys)
			panic("pmap_remove_all: pte doesn't point to page");
		if (pte->wired)
			panic("pmap_remove_all: removing  a wired page");

		pmap->stats.resident_count--;

		if ((cur = pvl->next) != PV_ENTRY_NULL) {
			*pvl  = *cur;
			free((caddr_t)cur, M_VMPVENT);
		} else
			pvl->pmap = PMAP_NULL;

		/*
		 * Reflect modified pages to pager.
		 */
		for (i = ptes_per_vm_page; i>0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified 
			 * bit and/or the reference bit being written back
			 * by other cpu.
			 */
			opte.bits = invalidate_pte(pte);
			flush_atc_entry(users, va, kflush);

			if (opte.pte.modified) {
				vm_page_set_modified((vm_page_t)PHYS_TO_VM_PAGE(phys));
				/* keep track ourselves too */
				SET_ATTRIB(phys, 1);
			}
			pte++;
			va += M88K_PGBYTES;
		}

		/*
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
		 */

		simple_unlock(&pmap->lock);
		dbgcnt++;
	}
	CHECK_PV_LIST(phys, pvl, "pmap_remove_all after");
d2372 2
a2373 2
	UNLOCK_PVH(phys);
	SPLX(spl);
d2388 1
a2388 1
 *		PA_TO_PVH
d2406 7
a2412 7
	register pv_entry_t  pv_e;
	register pt_entry_t  *pte;
	register int         i;
	int                  spl, spl_sav;
	register unsigned    users;
	register pte_template_t      opte;
	int                  kflush;
d2414 4
a2417 4
	if (!PMAP_MANAGED(phys)) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_CMOD)
			printf("(pmap_copy_on_write :%x) phys addr 0x%x not managed \n", curproc, phys);
d2419 2
a2420 2
		return;
	}
d2422 1
a2422 1
	SPLVM(spl);
d2424 9
a2432 9
copy_on_write_Retry:
	pv_e = PA_TO_PVH(phys);
	CHECK_PV_LIST(phys, pv_e, "pmap_copy_on_write before");
	LOCK_PVH(phys);
	
	if (pv_e->pmap  == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_COW | CD_NORM)) == (CD_COW | CD_NORM))
			printf("(pmap_copy_on_write :%x) phys addr 0x%x not mapped\n", curproc, phys);
a2433 63
		UNLOCK_PVH(phys);
		SPLX(spl);
		return;	    /* no mappings */
	}

	/*
	 * Run down the list of mappings to this physical page,
	 * disabling write privileges on each one.
	 */

	while (pv_e != PV_ENTRY_NULL) {
		pmap_t      pmap;
		vm_offset_t va;

		pmap = pv_e->pmap;
		va = pv_e->va;

		if (!simple_lock_try(&pmap->lock)) {
			UNLOCK_PVH(phys);
			goto copy_on_write_Retry;
		}

		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = 1;
		} else {
			kflush = 0;
		}

		/*
		 * Check for existing and valid pte
		 */
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL)
			panic("pmap_copy_on_write: pte from pv_list not in map");
		if (!PDT_VALID(pte))
			panic("pmap_copy_on_write: invalid pte");
		if (M88K_PTOB(pte->pfn) != phys)
			panic("pmap_copy_on_write: pte doesn't point to page");

		/*
		 * Flush TLBs of which cpus using pmap.
		 */

		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified 
			 * bit and/or the reference bit being written back
			 * by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			opte.pte.prot = M88K_RO;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va += M88K_PGBYTES;
		}
		simple_unlock(&pmap->lock);
		pv_e = pv_e->next;
	}
	CHECK_PV_LIST(phys, PA_TO_PVH(phys), "pmap_copy_on_write");
d2435 69
a2503 2
	UNLOCK_PVH(phys);
	SPLX(spl);
d2537 54
a2590 50
	pte_template_t		maprot;
	unsigned		ap;
	int			spl, spl_sav;
	register int		i;
	pt_entry_t		*pte;
	vm_offset_t		va, tva;
	register unsigned	users;
	register pte_template_t	opte;
	int			kflush;

	if (pmap == PMAP_NULL || prot & VM_PROT_WRITE)
		return;
	if ((prot & VM_PROT_READ) == 0) {
		pmap_remove(pmap, s, e);
		return;
	}

	if (s > e)
		panic("pmap_protect: start grater than end address");

	maprot.bits = m88k_protection(pmap, prot);
	ap = maprot.pte.prot;

	PMAP_LOCK(pmap, spl);

	users = pmap->cpus_using;
	if (pmap == kernel_pmap) {
		kflush = 1;
	} else {
		kflush = 0;
	}

	CHECK_PAGE_ALIGN(s, "pmap_protect");

	/*
	 * Loop through the range in vm_page_size increment.
	 * Do not assume that either start or end fall on any
	 * kind of page boundary (though this may be true ?!).
	 */
	for (va = s; va <= e; va += PAGE_SIZE) {
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
				printf("(pmap_protect :%x) no page table :: skip to 0x%x\n", curproc, va + PAGE_SIZE);
d2592 2
a2593 2
			continue;
		}
d2595 4
a2598 4
		if (!PDT_VALID(pte)) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
				printf("(pmap_protect :%x) pte invalid pte @@ 0x%x\n", curproc, pte);
d2600 2
a2601 2
			continue;	 /*  no page mapping */
		}
d2603 1
a2603 1
		printf("(pmap_protect :%x) pte good\n", curproc);
d2605 21
a2625 18
		tva = va;
		for (i = ptes_per_vm_page; i>0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the 
			 * modified bit and/or the reference bit being 
			 * written back by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			opte.pte.prot = ap;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, tva, kflush);
			splx(spl_sav);
			pte++;
			tva += M88K_PGBYTES;
		}
	}
	PMAP_UNLOCK(pmap, spl);
d2628 2
d2677 12
a2688 9
	int		i, spl;
	vm_offset_t	pdt_vaddr, pdt_paddr;
	sdt_entry_t	*sdt;
	pt_entry_t	*pte;
	vm_offset_t	pmap_extract();

	if (map == PMAP_NULL) {
		panic("pmap_expand: pmap is NULL");
	}
d2690 3
a2692 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_EXP | CD_NORM)) == (CD_EXP | CD_NORM))
		printf ("(pmap_expand :%x) map %x v %x\n", curproc, map, v);
d2695 1
a2695 1
	CHECK_PAGE_ALIGN (v, "pmap_expand");
d2697 11
a2707 11
	/*
	 * Handle kernel pmap in pmap_expand_kmap().
	 */
	if (map == kernel_pmap) {
		PMAP_LOCK(map, spl);
		if (pmap_expand_kmap(v, VM_PROT_READ|VM_PROT_WRITE) == PT_ENTRY_NULL)
			panic ("pmap_expand: Cannot allocate kernel pte table");
		PMAP_UNLOCK(map, spl);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_EXP | CD_FULL)) == (CD_EXP | CD_FULL))
			printf("(pmap_expand :%x) kernel_pmap\n", curproc);
d2709 2
a2710 2
		return;
	}
d2712 5
a2716 3
	/* XXX */
#if defined(UVM)
	pdt_vaddr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
d2718 1
a2718 1
	pdt_vaddr = kmem_alloc (kernel_map, PAGE_SIZE);
a2719 1
	pdt_paddr = pmap_extract(kernel_pmap, pdt_vaddr);
d2721 7
a2727 22
#ifdef MVME188
	if (cputyp == CPU_188) {
		/*
		 * the page for page tables should be CACHE DISABLED on MVME188
		 */
		pmap_cache_ctrl(kernel_pmap, pdt_vaddr, pdt_vaddr+PAGE_SIZE, CACHE_INH);
	}
#endif

	PMAP_LOCK(map, spl);

	if ((pte = pmap_pte(map, v)) != PT_ENTRY_NULL) {
		/*
		 * Someone else caused us to expand
		 * during our vm_allocate.
		 */
		PMAP_UNLOCK(map, spl);
		/* XXX */
#if defined(UVM)
		uvm_km_free(kernel_map, pdt_vaddr, PAGE_SIZE);
#else
		kmem_free (kernel_map, pdt_vaddr, PAGE_SIZE);
d2730 13
a2742 3
#ifdef DEBUG
		if (pmap_con_dbg & CD_EXP)
			printf("(pmap_expand :%x) table has already allocated\n", curproc);
d2744 30
a2773 26
		return;
	}
	/*
	 * Apply a mask to V to obtain the vaddr of the beginning of
	 * its containing page 'table group',i.e. the group of
	 * page  tables that fit eithin a single VM page.
	 * Using that, obtain the segment table pointer that references the
	 * first page table in the group, and initilize all the
	 * segment table descriptions for the page 'table group'.
	 */
	v &= ~((1<<(LOG2_PDT_TABLE_GROUP_SIZE+PDT_BITS+PG_BITS))-1);

	sdt = SDTENT(map,v);

	/*
	 * Init each of the segment entries to point the freshly allocated
	 * page tables.
	 */
	for (i = PDT_TABLE_GROUP_SIZE; i>0; i--) {
		((sdt_entry_template_t *)sdt)->bits = pdt_paddr | M88K_RW | DT_VALID;
		((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = pdt_vaddr | M88K_RW | DT_VALID;
		sdt++;
		pdt_paddr += PDT_SIZE;
		pdt_vaddr += PDT_SIZE;
	}
	PMAP_UNLOCK(map, spl);
d2776 2
d2858 2
a2859 2
	   vm_prot_t prot, boolean_t wired,
	   vm_prot_t access_type)
d2861 23
a2883 22
	int		ap;
	int		spl, spl_sav;
	pv_entry_t	pv_e;
	pt_entry_t	*pte;
	vm_offset_t	old_pa;
	pte_template_t	template;
	register int	i;
	pv_entry_t	pvl;
	register unsigned	users;
	register pte_template_t	opte;
	int		kflush;

	if (pmap == PMAP_NULL) {
		panic("pmap_enter: pmap is NULL");
	}

	CHECK_PAGE_ALIGN (va, "pmap_entry - VA");
	CHECK_PAGE_ALIGN (pa, "pmap_entry - PA");

	/*
	 *	Range check no longer use, since we use whole address space
	 */
d2885 117
a3001 113
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
		if (pmap == kernel_pmap)
			printf ("(pmap_enter :%x) pmap kernel va %x pa %x\n", curproc, va, pa);
		else
			printf ("(pmap_enter :%x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
	}
#endif
	ap = m88k_protection (pmap, prot);

	/*
	 *	Must allocate a new pvlist entry while we're unlocked;
	 *	zalloc may cause pageout (which will lock the pmap system).
	 *	If we determine we need a pvlist entry, we will unlock
	 *	and allocate one. Then will retry, throwing away
	 *	the allocated entry later (if we no longer need it).
	 */
	pv_e = PV_ENTRY_NULL;
Retry:

	PMAP_LOCK(pmap, spl);
	users = pmap->cpus_using;

	/*
	 * Expand pmap to include this pte. Assume that
	 * pmap is always expanded to include enough M88K
	 * pages to map one VM page.
	 */
	while ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		/*
		 * Must unlock to expand the pmap.
		 */
		PMAP_UNLOCK(pmap, spl);
		pmap_expand(pmap, va);
		PMAP_LOCK(pmap, spl);
	}
	/*
	 *	Special case if the physical page is already mapped
	 *	at this address.
	 */
	old_pa = M88K_PTOB(pte->pfn);
	if (old_pa == pa) {
		if (pmap == kernel_pmap) {
			kflush = 1;
		} else {
			kflush = 0;
		}

		/*
		 * May be changing its wired attributes or protection
		 */

		if (wired && !pte->wired)
			pmap->stats.wired_count++;
		else if (!wired && pte->wired)
			pmap->stats.wired_count--;

		if ((unsigned long)pa >= MAXPHYSMEM)
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
		else
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
		if (wired)
			template.pte.wired = 1;

		/*
		 * If there is a same mapping, we have nothing to do.
		 */
		if ( !PDT_VALID(pte) || (pte->wired != template.pte.wired)
		     || (pte->prot != template.pte.prot)) {

			for (i = ptes_per_vm_page; i>0; i--) {

				/*
				 * Invalidate pte temporarily to avoid being written back
				 * the modified bit and/or the reference bit by other cpu.
				 */
				spl_sav = splimp();
				opte.bits = invalidate_pte(pte);
				template.pte.modified = opte.pte.modified;
				*pte++ = template.pte;
				flush_atc_entry(users, va, kflush);
				splx(spl_sav);
				template.bits += M88K_PGBYTES;
				va += M88K_PGBYTES;
			}
		}

	} else { /* if ( pa == old_pa) */
		/*
		 * Remove old mapping from the PV list if necessary.
		 */
		if (old_pa != (vm_offset_t)-1) {
			/*
			 *	Invalidate the translation buffer,
			 *	then remove the mapping.
			 */
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
				if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
					printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
					       phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
					       PMAP_MANAGED(pa) ? 1 : 0);
					printf("pte %x pfn %x valid %x\n",
					       pte, pte->pfn, pte->dtype);
				}
			}
#endif
			if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				flush_atc_entry(users, va, 1);
			} else {
				pmap_remove_range(pmap, va, va + PAGE_SIZE);
			}
		}
d3003 25
a3027 23
		if (PMAP_MANAGED(pa)) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
				if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
					printf("va 0x%x and managed pa 0x%x\n", va, pa);
				}
			}
#endif
			/*
			 *	Enter the mappimg in the PV list for this
			 *	physical page.
			 */
			LOCK_PVH(pa);
			pvl = PA_TO_PVH(pa);
			CHECK_PV_LIST (pa, pvl, "pmap_enter before");

			if (pvl->pmap == PMAP_NULL) {
				/*
				 *	No mappings yet
				 */
				pvl->va = va;
				pvl->pmap = pmap;
				pvl->next = PV_ENTRY_NULL;
d3029 50
a3078 51
			} else {
#ifdef DEBUG
				/*
				 * check that this mapping is not already there
				 */
				{
					pv_entry_t e = pvl;
					while (e != PV_ENTRY_NULL) {
						if (e->pmap == pmap && e->va == va)
							panic("pmap_enter: already in pv_list");
						e = e->next;
					}
				}
#endif
				/*
				 * Add new pv_entry after header.
				 */
				if (pv_e == PV_ENTRY_NULL) {
					UNLOCK_PVH(pa);
					PMAP_UNLOCK(pmap, spl);
					pv_e = (pv_entry_t) malloc(sizeof *pv_e,
								   M_VMPVENT,
								   M_NOWAIT);
					goto Retry;
				}
				pv_e->va = va;
				pv_e->pmap = pmap;
				pv_e->next = pvl->next;
				pvl->next = pv_e;
				/*
				 * Remeber that we used the pvlist entry.
				 */
				pv_e = PV_ENTRY_NULL;
			}
			UNLOCK_PVH(pa);
		}

		/*
		 * And count the mapping.
		 */
		pmap->stats.resident_count++;
		if (wired)
			pmap->stats.wired_count++;

		if ((unsigned long)pa >= MAXPHYSMEM)
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
		else
			template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;

		if (wired)
			template.pte.wired = 1;
d3080 1
a3080 1
		DO_PTES (pte, template.bits);
d3082 1
a3082 1
	} /* if ( pa == old_pa ) ... else */
d3084 1
a3084 1
	PMAP_UNLOCK(pmap, spl);
d3086 2
a3087 2
	if (pv_e != PV_ENTRY_NULL)
		free((caddr_t) pv_e, M_VMPVENT);
d3091 2
d3119 20
a3138 3
	pt_entry_t  *pte;
	int      i;
	int      spl;
d3140 2
a3141 1
	PMAP_LOCK(map, spl);
d3143 1
a3143 2
	if ((pte = pmap_pte(map, v)) == PT_ENTRY_NULL)
		panic ("pmap_change_wiring: pte missing");
d3145 1
a3145 7
	if (wired && !pte->wired)
		/* wiring mapping */
		map->stats.wired_count++;

	else if (!wired && pte->wired)
		/* unwired mapping */
		map->stats.wired_count--;
a3146 2
	for (i = ptes_per_vm_page; i>0; i--)
		(pte++)->wired = wired;
a3147 3
	PMAP_UNLOCK(map, spl);

} /* pmap_change_wiring() */
d3179 31
a3209 29
	register pt_entry_t  *pte;
	register vm_offset_t pa;
	register int   i;
	int         spl;

	if (pmap == PMAP_NULL)
		panic("pmap_extract: pmap is NULL");

	/*
	 * check BATC first
	 */
	if (pmap == kernel_pmap && batc_used > 0)
		for (i = batc_used-1; i > 0; i--)
			if (batc_entry[i].lba == M88K_BTOBLK(va)) {
				pa = (batc_entry[i].pba << BATC_BLKSHIFT) | 
					(va & BATC_BLKMASK );
				return (pa);
			}

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
		pa = (vm_offset_t) 0;
	else {
		if (PDT_VALID(pte))
			pa = M88K_PTOB(pte->pfn);
		else
			pa = (vm_offset_t) 0;
	}
d3211 6
a3216 2
	if (pa)
		pa |= (va & M88K_PGOFSET); /* offset within page */
a3217 2
	PMAP_UNLOCK(pmap, spl);
	return (pa);
d3227 25
a3251 26
	pt_entry_t *pte;
	vm_offset_t   pa;
	int     i;

	if (pmap == PMAP_NULL)
		panic("pmap_extract: pmap is NULL");

	/*
	 * check BATC first
	 */
	if (pmap == kernel_pmap && batc_used > 0)
		for (i = batc_used-1; i > 0; i--)
			if (batc_entry[i].lba == M88K_BTOBLK(va)) {
				pa = (batc_entry[i].pba << BATC_BLKSHIFT) | 
					(va & BATC_BLKMASK );
				return (pa);
			}

	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
		pa = (vm_offset_t) 0;
	else {
		if (PDT_VALID(pte))
			pa = M88K_PTOB(pte->pfn);
		else
			pa = (vm_offset_t) 0;
	}
d3253 2
a3254 2
	if (pa)
		pa |= (va & M88K_PGOFSET); /* offset within page */
d3256 1
a3256 1
	return (pa);
d3272 1
a3272 1
 *		dst_addr	VA in destination map
d3282 1
a3282 1
	  vm_size_t len, vm_offset_t src_addr)
d3284 2
a3285 2
#ifdef lint
	dst_pmap++; src_pmap++; dst_addr++; len++; src_addr++;
d3316 3
a3318 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_UPD | CD_FULL)) == (CD_UPD | CD_FULL))
		printf("(pmap_update :%x) Called \n", curproc);
d3368 12
a3379 12
	vm_offset_t sdt_va;	/* outer loop index */
	vm_offset_t sdt_vt;	/* end of segment */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	sdt_entry_t *sdt;	/* ptr to index into segment table */
	pt_entry_t  *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t  *gdttblend;	/* ptr to byte after last entry in table group */
	pt_entry_t  *gdtp;	/* ptr to index into a page table */
	boolean_t   found_gdt_wired;	/* flag indicating a wired page exists 
					   in a page table's address range */
	int     spl;
	unsigned int i,j;
d3383 4
a3386 4
	if (pmap == PMAP_NULL) {
		panic("pmap_collect: pmap is NULL");
	}
	if (pmap == kernel_pmap) {
d3388 1
a3388 1
		return;
d3390 1
a3390 1
		panic("pmap_collect attempted on kernel pmap");
d3392 1
a3392 1
	}
d3394 1
a3394 1
	CHECK_PMAP_CONSISTENCY ("pmap_collect");
d3396 3
a3398 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
		printf ("(pmap_collect :%x) pmap %x\n", curproc, pmap);
d3401 1
a3401 1
	PMAP_LOCK(pmap, spl);
d3403 2
a3404 2
	sdttbl = pmap->sdt_vaddr; /* addr of segment table */
	sdtp = sdttbl;
d3406 4
a3409 5
	/* 
	  This contortion is here instead of the natural loop 
	  because of integer overflow/wraparound if VM_MAX_USER_ADDRESS 
	  is near 0xffffffff
	*/
d3411 3
a3413 3
	i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	if ( j < 1024 )	j++;
d3415 3
a3417 3
	/* Segment table loop */
	for ( ; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE) {
		sdt_va = VM_MIN_USER_ADDRESS + PDT_TABLE_GROUP_VA_SPACE*i;
d3419 1
a3419 1
		gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va);
d3421 2
a3422 2
		if (gdttbl == PT_ENTRY_NULL)
			continue; /* no maps in this range */
d3424 1
a3424 1
		gdttblend = gdttbl + (PDT_ENTRIES * PDT_TABLE_GROUP_SIZE);
d3426 8
a3433 8
		/* scan page maps for wired pages */
		found_gdt_wired = FALSE;
		for (gdtp=gdttbl; gdtp <gdttblend; gdtp++) {
			if (gdtp->wired) {
				found_gdt_wired = TRUE;
				break;
			}
		}
d3435 2
a3436 2
		if (found_gdt_wired)
			continue; /* can't free this range */
d3438 1
a3438 1
		/* figure out end of range. Watch for wraparound */
d3440 3
a3442 3
		sdt_vt = sdt_va <= VM_MAX_USER_ADDRESS-PDT_TABLE_GROUP_VA_SPACE ?
			 sdt_va+PDT_TABLE_GROUP_VA_SPACE : 
			 VM_MAX_USER_ADDRESS;
d3444 2
a3445 2
		/* invalidate all maps in this range */
		pmap_remove_range (pmap, (vm_offset_t)sdt_va,(vm_offset_t)sdt_vt);
d3447 7
a3453 7
		/*
		 * we can safely deallocated the page map(s)
		 */
		for (sdt = sdtp; sdt < (sdtp+PDT_TABLE_GROUP_SIZE); sdt++) {
			((sdt_entry_template_t *) sdt) -> bits = 0;
			((sdt_entry_template_t *) sdt+SDT_ENTRIES) -> bits = 0;
		}
d3455 7
a3461 8
		/*
		 * we have to unlock before freeing the table, since PT_FREE
		 * calls kmem_free or zfree, which will invoke another 
		 * pmap routine
		 */
		PMAP_UNLOCK(pmap, spl);
		PT_FREE(gdttbl);
		PMAP_LOCK(pmap, spl);
d3463 1
a3463 1
	} /* Segment table Loop */
d3465 1
a3465 1
	PMAP_UNLOCK(pmap, spl);
d3467 3
a3469 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
		printf  ("(pmap_collect :%x) done \n", curproc);
d3472 1
a3472 1
	CHECK_PMAP_CONSISTENCY("pmap_collect");
d3475 2
d3478 1
a3478 9
 * Routine:	PMAP_ACTIVATE
 * 
 * Function:
 * 	Binds the given physical map to the given
 *	processor, and returns a hardware map description.
 *	In a mono-processor implementation the my_cpu
 *	argument is ignored, and the PMAP_ACTIVATE macro
 *	simply sets the MMU root pointer element of the PCB
 *	to the physical address of the segment descriptor table.
d3480 7
a3486 10
 * Parameters:
 * 	p	pointer to proc structure
 *
 * Notes:
 *	If the specified pmap is not kernel_pmap, this routine makes arp
 *	template and stores it into UAPR (user area pointer register) in the
 *	CMMUs connected to the specified CPU.
 *
 *	If kernel_pmap is specified, only flushes the TLBs mapping kernel
 *	virtual space, in the CMMUs connected to the specified CPU.
d3488 4
d3494 1
a3494 1
pmap_activate(struct proc *p)
d3496 2
a3497 4
	apr_template_t   apr_data;
#ifdef notyet
#ifdef OMRON_PMAP
	int     n;
d3499 3
a3501 3
#endif
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	int my_cpu = cpu_number();  
a3502 4
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
		printf("(pmap_activate :%x) pmap 0x%x\n", p, (unsigned)pmap);
#endif
a3503 47
	if (pmap != kernel_pmap) {
		/*
		 *	Lock the pmap to put this cpu in its active set.
		 */

		simple_lock(&pmap->lock);
		apr_data.bits = 0;
		apr_data.field.st_base = M88K_BTOP(pmap->sdt_paddr); 
		apr_data.field.wt = 0;
		apr_data.field.g  = 1;
		apr_data.field.ci = 0;
		apr_data.field.te = 1;
#ifdef notyet
	#ifdef OMRON_PMAP
		/*
		 * cmmu_pmap_activate will set the uapr and the batc entries, 
		 * then flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE 
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE 
		 * FLUSHED AS WELL.
		 */
		cmmu_pmap_activate(my_cpu, apr_data.bits, 
				   pmap->i_batc, pmap->d_batc);
		for (n = 0; n < BATC_MAX; n++)
			*(unsigned*)&batc_entry[n] = pmap->i_batc[n].bits;
	#else
		cmmu_set_uapr(apr_data.bits);
		cmmu_flush_tlb(0, 0, -1);
	#endif
#endif /* notyet */
		/*
		 * I am forcing it to not program the BATC at all. pmap.c module
		 * needs major, major cleanup. XXX nivas
		 */
		cmmu_set_uapr(apr_data.bits);
		cmmu_flush_tlb(0, 0, -1);

		/*
		 *	Mark that this cpu is using the pmap.
		 */
		SETBIT_CPUSET(my_cpu, &(pmap->cpus_using));

		simple_unlock(&pmap->lock);
	} else {

		/*
		 * kernel_pmap must be always active.
		 */
d3505 18
a3522 3
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
			printf("(pmap_activate :%x) called for kernel_pmap\n", curproc);
d3524 3
a3527 2
	}
} /* pmap_activate() */
d3530 1
a3530 10
 * Routine:	PMAP_DEACTIVATE
 *
 * Function:
 *	Unbinds the given physical map from the given processor,
 *	i.e. the pmap i no longer is use on the processor.
 *
 * Parameters:
 *     	p		pointer to proc structure
 *
 * _pmap_deactive simply clears the cpus_using field in given pmap structure.
d3532 2
d3535 2
a3536 2
void
pmap_deactivate(struct proc *p)
d3538 3
a3540 13
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	int my_cpu = cpu_number();  
	
	if (pmap != kernel_pmap) {

		/*
		 * we expect the spl is already raised to sched level.
		 */
		simple_lock(&pmap->lock);
		CLRBIT_CPUSET(my_cpu, &(pmap->cpus_using));
		simple_unlock(&pmap->lock);
	}
} /* pmap_deactivate() */
d3573 40
a3612 43
	vm_offset_t dstva, srcva;
	unsigned int spl_sav;
	int i;
	int      aprot;
	pte_template_t template;
	pt_entry_t  *dstpte, *srcpte;
	int      my_cpu = cpu_number();

	/*
	 *	Map source physical address.
	 */
	aprot = m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);

	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu_number() * PAGE_SIZE));
	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));

	srcpte = pmap_pte(kernel_pmap, srcva);
	dstpte = pmap_pte(kernel_pmap, dstva);

	for (i=0; i < ptes_per_vm_page; i++, 
	     src += M88K_PGBYTES, dst += M88K_PGBYTES) {
		template.bits = M88K_TRUNC_PAGE(src) | aprot | 
			DT_VALID | CACHE_GLOBAL;

		/* do we need to write back dirty bits */
		spl_sav = splimp();
		cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
		*srcpte = template.pte;

		/*
		 *	Map destination physical address.
		 */
		template.bits = M88K_TRUNC_PAGE(dst) | aprot | 
			CACHE_GLOBAL | DT_VALID;
		cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
		*dstpte  = template.pte;
		splx(spl_sav);

		bcopy((void*)srcva, (void*)dstva, M88K_PGBYTES);
		/* flush source, dest out of cache? */
		cmmu_flush_remote_data_cache(my_cpu, src, M88K_PGBYTES);
		cmmu_flush_remote_data_cache(my_cpu, dst, M88K_PGBYTES);
	}
d3616 1
d3643 38
a3680 36
	vm_offset_t   dstva;
	pt_entry_t *dstpte;
	int     copy_size,
	offset,
	aprot;
	unsigned int i;
	pte_template_t  template;

	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
	dstpte = pmap_pte(kernel_pmap, dstva);
	copy_size = M88K_PGBYTES;
	offset = dstpa - M88K_TRUNC_PAGE(dstpa);
	dstpa -= offset;

	aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
	while (bytecount > 0) {
		copy_size = M88K_PGBYTES - offset;
		if (copy_size > bytecount)
			copy_size = bytecount;
		/*
		 * Map destination physical address. 
		 */
		for (i = 0; i < ptes_per_vm_page; i++) {
			template.bits = M88K_TRUNC_PAGE(dstpa) | aprot | CACHE_WT | DT_VALID;
			cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
			*dstpte = template.pte;

			dstva += offset;
			bcopy((void*)srcva, (void*)dstva, copy_size);
			srcva += copy_size;
			dstva += copy_size;
			dstpa += M88K_PGBYTES;
			bytecount -= copy_size;
			offset = 0;
		}
	}
d3709 38
a3746 36
	register vm_offset_t   srcva;
	register pt_entry_t *srcpte;
	register int     copy_size, offset;
	int             aprot;
	unsigned int i;
	pte_template_t  template;

	srcva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
	srcpte = pmap_pte(kernel_pmap, srcva);
	copy_size = M88K_PGBYTES;
	offset = srcpa - M88K_TRUNC_PAGE(srcpa);
	srcpa -= offset;

	aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
	while (bytecount > 0) {
		copy_size = M88K_PGBYTES - offset;
		if (copy_size > bytecount)
			copy_size = bytecount;
		/*
		 * Map destination physical address.
		 */
		for (i=0; i < ptes_per_vm_page; i++) {
			template.bits = M88K_TRUNC_PAGE(srcpa) | aprot | CACHE_WT | DT_VALID;
			cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
			*srcpte = template.pte;

			srcva += offset;
			bcopy((void*)srcva, (void*)dstva, copy_size);
			srcpa += M88K_PGBYTES;
			dstva += copy_size;
			srcva += copy_size;
			bytecount -= copy_size;
			offset = 0;
			/* cache flush source? */
		}
	}
d3772 1
a3772 1
	      boolean_t pageable)
d3774 2
a3775 2
#ifdef lint
	pmap++; start++; end++; pageable++;
d3808 33
a3840 33
	pt_entry_t    *pte;
	int        spl, spl_sav;
	int        i;
	unsigned      users;
	pte_template_t   opte;
	int        kflush;

	va = M88K_ROUND_PAGE(va);
	PMAP_LOCK(pmap, spl);

	users = pmap->cpus_using;
	if (pmap == kernel_pmap) {
		kflush = 1;
	} else {
		kflush = 0;
	}

	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL && PDT_VALID(pte))
		for (i = ptes_per_vm_page; i > 0; i--) {

			/*
			 * Invalidate pte temporarily to avoid being written back
			 * the modified bit and/or the reference bit by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			opte.pte.prot = M88K_RO;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va +=M88K_PGBYTES;
		}
d3842 1
a3842 1
	PMAP_UNLOCK(pmap, spl);
d3846 2
d3864 2
a3865 1
 *		PA_TO_PVH
d3880 31
a3910 10
	pv_entry_t    pvl;
	pv_entry_t    pvep;
	pt_entry_t    *pte;
	pmap_t     pmap;
	int        spl, spl_sav;
	vm_offset_t      va;
	int        i;
	unsigned      users;
	pte_template_t   opte;
	int        kflush;
d3912 4
a3915 4
	if (!PMAP_MANAGED(phys)) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_CMOD)
			printf("(pmap_clear_modify :%x) phys addr 0x%x not managed \n", curproc, phys);
a3916 2
		return;
	}
d3918 44
a3961 1
	SPLVM(spl);
d3963 2
a3964 4
clear_modify_Retry:
	pvl = PA_TO_PVH(phys);
	CHECK_PV_LIST (phys, pvl, "pmap_clear_modify");
	LOCK_PVH(phys);
d3966 2
a3967 2
	/* update correspoinding pmap_modify_list element */
	SET_ATTRIB(phys, 0);
a3968 50
	if (pvl->pmap == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_CMOD | CD_NORM)) == (CD_CMOD | CD_NORM))
			printf("(pmap_clear_modify :%x) phys addr 0x%x not mapped\n", curproc, phys);
#endif
		UNLOCK_PVH(phys);
		SPLX(spl);
		return;
	}

	/* for each listed pmap, trun off the page modified bit */
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
		pmap = pvep->pmap;
		va = pvep->va;
		if (!simple_lock_try(&pmap->lock)) {
			UNLOCK_PVH(phys);
			goto clear_modify_Retry;
		}
		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = 1;
		} else {
			kflush = 0;
		}

		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL)
			panic("pmap_clear_modify: bad pv list entry.");

		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified bit 
			 * and/or the reference being written back by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			/* clear modified bit */
			opte.pte.modified = 0;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va += M88K_PGBYTES;
		}
		simple_unlock(&pmap->lock);
		pvep = pvep->next;
	}
	UNLOCK_PVH(phys);
	SPLX(spl);
d3971 2
d3992 2
a3993 1
 *		PA_TO_PVH
d4012 82
a4093 6
	pv_entry_t  pvl;
	pv_entry_t  pvep;
	pt_entry_t  *ptep;
	int      spl;
	int      i;
	boolean_t   modified_flag;
d4095 1
a4095 9
	if (!PMAP_MANAGED(phys)) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_IMOD)
			printf("(pmap_is_modified :%x) phys addr 0x%x not managed\n", curproc, phys);
#endif
		return (FALSE);
	}

	SPLVM(spl);
a4096 62
	pvl = PA_TO_PVH(phys);
	CHECK_PV_LIST (phys, pvl, "pmap_is_modified");
	is_mod_Retry:

	if ((boolean_t) PA_TO_ATTRIB(phys)) {
		/* we've already cached a modify flag for this page,
		   no use looking further... */
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
			printf("(pmap_is_modified :%x) already cached a modify flag for this page\n", curproc);
#endif
		SPLX(spl);
		return (TRUE);
	}
	LOCK_PVH(phys);

	if (pvl->pmap == PMAP_NULL) {
		/* unmapped page - get info from page_modified array
		   maintained by pmap_remove_range/ pmap_remove_all */
		modified_flag = (boolean_t) PA_TO_ATTRIB(phys);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
			printf("(pmap_is_modified :%x) phys addr 0x%x not mapped\n", curproc, phys);
#endif
		UNLOCK_PVH(phys);
		SPLX(spl);
		return (modified_flag);
	}

	/* for each listed pmap, check modified bit for given page */
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
		if (!simple_lock_try(&pvep->pmap->lock)) {
			UNLOCK_PVH(phys);
			goto is_mod_Retry;
		}

		ptep = pmap_pte(pvep->pmap, pvep->va);
		if (ptep == PT_ENTRY_NULL) {
			printf("pmap_is_modified: pte from pv_list not in map virt = 0x%x\n", pvep->va);
			panic("pmap_is_modified: bad pv list entry");
		}
		for (i = ptes_per_vm_page; i > 0; i--) {
			if (ptep->modified) {
				simple_unlock(&pvep->pmap->lock);
#ifdef DEBUG
				if ((pmap_con_dbg & (CD_IMOD | CD_FULL)) == (CD_IMOD | CD_FULL))
					printf("(pmap_is_modified :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
#endif
				UNLOCK_PVH(phys);
				SPLX(spl);
				return (TRUE);
			}
			ptep++;
		}
		simple_unlock(&pvep->pmap->lock);
		pvep = pvep->next;
	}

	UNLOCK_PVH(phys);
	SPLX(spl);
	return (FALSE);
a4097 1
} /* pmap_is_modified() */
d4117 2
a4118 1
 *		PA_TO_PVH
d4135 77
a4211 10
	pv_entry_t    pvl;
	pv_entry_t    pvep;
	pt_entry_t    *pte;
	pmap_t     pmap;
	int        spl, spl_sav;
	vm_offset_t      va;
	int        i;
	unsigned      users;
	pte_template_t   opte;
	int        kflush;
d4213 4
a4216 8
	if (!PMAP_MANAGED(phys)) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_CREF) {
			printf("(pmap_clear_reference :%x) phys addr 0x%x not managed\n", curproc,phys);
		}
#endif
		return;
	}
d4218 2
a4219 1
	SPLVM(spl);
d4221 1
a4221 4
	clear_reference_Retry:
	LOCK_PVH(phys);
	pvl = PA_TO_PVH(phys);
	CHECK_PV_LIST(phys, pvl, "pmap_clear_reference");
a4223 52
	if (pvl->pmap == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_CREF | CD_NORM)) == (CD_CREF | CD_NORM))
			printf("(pmap_clear_reference :%x) phys addr 0x%x not mapped\n", curproc,phys);
#endif
		UNLOCK_PVH(phys);
		SPLX(spl);
		return;
	}

	/* for each listed pmap, turn off the page refrenced bit */
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
		pmap = pvep->pmap;
		va = pvep->va;
		if (!simple_lock_try(&pmap->lock)) {
			UNLOCK_PVH(phys);
			goto clear_reference_Retry;
		}
		users = pmap->cpus_using;
		if (pmap == kernel_pmap) {
			kflush = 1;
		} else {
			kflush = 0;
		}

		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL)
			panic("pmap_clear_reference: bad pv list entry.");

		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified bit 
			 * and/or the reference being written back by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			/* clear reference bit */
			opte.pte.pg_used = 0;
			((pte_template_t *)pte)->bits = opte.bits;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
			pte++;
			va += M88K_PGBYTES;
		}

		simple_unlock(&pmap->lock);
		pvep = pvep->next;
	}
	UNLOCK_PVH(phys);
	SPLX(spl);
} /* pmap_clear_reference() */
d4245 2
a4246 1
 *	PA_TO_PVH
a4258 1

d4262 53
a4314 49
	pv_entry_t pvl;
	pv_entry_t pvep;
	pt_entry_t *ptep;
	int     spl;
	int     i;

	if (!PMAP_MANAGED(phys))
		return (FALSE);

	SPLVM(spl);

	pvl = PA_TO_PVH(phys);
	CHECK_PV_LIST(phys, pvl, "pmap_is_referenced");

	is_ref_Retry:

	if (pvl->pmap == PMAP_NULL) {
		SPLX(spl);
		return (FALSE);
	}

	LOCK_PVH(phys);

	/* for each listed pmap, check used bit for given page */
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
		if (!simple_lock_try(&pvep->pmap->lock)) {
			UNLOCK_PVH(phys);
			goto is_ref_Retry;
		}
		ptep = pmap_pte(pvep->pmap, pvep->va);
		if (ptep == PT_ENTRY_NULL)
			panic("pmap_is_referenced: bad pv list entry.");
		for (i = ptes_per_vm_page; i > 0; i--) {
			if (ptep->pg_used) {
				simple_unlock(&pvep->pmap->lock);
				UNLOCK_PVH(phys);
				SPLX(spl);
				return (TRUE);
			}
			ptep++;
		}
		simple_unlock(&pvep->pmap->lock);
		pvep = pvep->next;
	}

	UNLOCK_PVH(phys);
	SPLX(spl);
	return (FALSE);
d4327 3
d4333 2
a4334 1
 *		PA_TO_PVH
d4350 6
a4355 3
	pv_entry_t  pv_h;
	int      spl;
	boolean_t   result;
d4357 2
a4358 2
	if (!pmap_initialized)
		return (TRUE);
d4360 1
a4360 2
	if (!PMAP_MANAGED(phys))
		return (FALSE);
d4362 2
a4363 1
	SPLVM(spl);
d4365 3
a4367 2
	pv_h = PA_TO_PVH(phys);
	LOCK_PVH(phys);
d4369 1
a4369 3
	result = (pv_h->pmap == PMAP_NULL);
	UNLOCK_PVH(phys);
	SPLX(spl);
a4370 1
	return (result);
d4373 1
d4383 2
a4384 2
#ifdef lint
	p++;
d4386 1
a4386 1
	return (TRUE);
d4401 11
a4411 11
	switch (prot) {
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_copy_on_write(phys);
		break;
	case VM_PROT_ALL:
		break;
	default:
		pmap_remove_all(phys);
		break;
	}
d4442 76
a4517 42
	vm_offset_t      pa;
	pt_entry_t    *srcpte, *dstpte;
	pv_entry_t    pvl;
	int        spl;
	int        i;
	unsigned      users;
	pte_template_t   opte;

	PMAP_LOCK(kernel_pmap, spl);

	users = kernel_pmap->cpus_using;

	while (size > 0) {

		/*
		 * check if the source addr is mapped
		 */
		if ((srcpte = pmap_pte(kernel_pmap, (vm_offset_t)from)) == PT_ENTRY_NULL) {
			printf("pagemove: source vaddr 0x%x\n", from);
			panic("pagemove: Source addr not mapped");
		}

		/*
		 *
		 */
		if ((dstpte = pmap_pte(kernel_pmap, (vm_offset_t)to)) == PT_ENTRY_NULL)
			if ((dstpte = pmap_expand_kmap((vm_offset_t)to, VM_PROT_READ | VM_PROT_WRITE))
			    == PT_ENTRY_NULL)
				panic("pagemove: Cannot allocate distination pte");
			/*
			 *
			 */
		if (dstpte->dtype == DT_VALID) {
			printf("pagemove: distination vaddr 0x%x, pte = 0x%x\n", to, *((unsigned *)dstpte));
			panic("pagemove: Distination pte already valid");
		}

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_PGMV | CD_NORM)) == (CD_PGMV | CD_NORM))
			printf("(pagemove :%x) from 0x%x to 0x%x\n", curproc, from, to);
		if ((pmap_con_dbg & (CD_PGMV | CD_FULL)) == (CD_PGMV | CD_FULL))
			printf("(pagemove :%x) srcpte @@ 0x%x = %x dstpte @@ 0x%x = %x\n", curproc, (unsigned)srcpte, *(unsigned *)srcpte, (unsigned)dstpte, *(unsigned *)dstpte);
d4519 1
a4519 1
#endif /* DEBUG */
a4520 30
		/*
		 * Update pv_list
		 */
		pa = M88K_PTOB(srcpte->pfn);
		if (PMAP_MANAGED(pa)) {
			LOCK_PVH(pa);
			pvl = PA_TO_PVH(pa);
			CHECK_PV_LIST(pa, pvl, "pagemove");
			pvl->va = (vm_offset_t)to;
			UNLOCK_PVH(pa);
		}

		/*
		 * copy pte
		 */
		for (i = ptes_per_vm_page; i > 0; i--) {
			/*
			 * Invalidate pte temporarily to avoid the modified bit 
			 * and/or the reference being written back by other cpu.
			 */
			opte.bits = invalidate_pte(srcpte);
			flush_atc_entry(users, from, 1);
			((pte_template_t *)dstpte)->bits = opte.bits;
			from += M88K_PGBYTES;
			to += M88K_PGBYTES;
			srcpte++; dstpte++;
		}
		size -= PAGE_SIZE;
	}
	PMAP_UNLOCK(kernel_pmap, spl);
d4550 10
a4559 2
	int  i;
	int  cpu = 0;
a4560 8
	for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
		for (cpu=0; cpu<max_cpus; cpu++) {
			if (cpu_sets[cpu]) {
				cmmu_flush_remote_inst_cache(cpu, pa, 
							     M88K_PGBYTES);
			}
		}
	}
d4584 3
a4586 3
	vm_offset_t pa;
	int  i;
	int     spl;
d4588 2
a4589 2
	if (pmap == PMAP_NULL)
		panic("pmap_dcache_flush: pmap is NULL");
d4591 1
a4591 1
	PMAP_LOCK(pmap, spl);
d4593 4
a4596 4
	pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
	for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
		cmmu_flush_data_cache(pa, M88K_PGBYTES);
	}
d4598 1
a4598 1
	PMAP_UNLOCK(pmap, spl);
d4606 49
a4654 49
	int     i;
	int     ncpus;
	void    (*cfunc)(int cpu, vm_offset_t physaddr, int size);

	switch (mode) {
	default:
		panic("bad cache_flush_loop mode");
		return;

	case FLUSH_CACHE:   /* All caches, all CPUs */
		ncpus = max_cpus;
		cfunc = cmmu_flush_remote_cache;
		break;

	case FLUSH_CODE_CACHE: /* Instruction caches, all CPUs */
		ncpus = max_cpus;
		cfunc = cmmu_flush_remote_inst_cache;
		break;

	case FLUSH_DATA_CACHE: /* Data caches, all CPUs */
		ncpus = max_cpus;
		cfunc = cmmu_flush_remote_data_cache;
		break;

	case FLUSH_LOCAL_CACHE:	     /* Both caches, my CPU */
		ncpus = 1;
		cfunc = cmmu_flush_remote_cache;
		break;

	case FLUSH_LOCAL_CODE_CACHE: /* Instruction cache, my CPU */
		ncpus = 1;
		cfunc = cmmu_flush_remote_inst_cache;
		break;

	case FLUSH_LOCAL_DATA_CACHE: /* Data cache, my CPU */
		ncpus = 1;
		cfunc = cmmu_flush_remote_data_cache;
		break;
	}

	if (ncpus == 1) {
		(*cfunc)(cpu_number(), pa, size);
	} else {
		for (i=0; i<max_cpus; i++) {
			if (cpu_sets[i]) {
				(*cfunc)(i, pa, size);
			}
		}
	}
d4664 25
a4688 25
	vm_offset_t pa;
	vm_offset_t va;
	int  i;
	int     spl;

	if (pmap == PMAP_NULL)
		panic("pmap_cache_flush: NULL pmap");

	/*
	 * If it is more than a couple of pages, just blow the whole cache
	 * because of the number of cycles involved.
	 */
	if (bytes > 2*M88K_PGBYTES) {
		cache_flush_loop(mode, 0, -1);
		return;
	}

	PMAP_LOCK(pmap, spl);
	for (va = virt; bytes > 0; bytes -= M88K_PGBYTES,va += M88K_PGBYTES) {
		pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
		for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
			cache_flush_loop(mode, pa, M88K_PGBYTES);
		}
	}
	PMAP_UNLOCK(pmap, spl);
d4691 1
a4691 1
#ifdef DEBUG
d4732 39
a4770 39
	pv_entry_t  pv_e;
	pt_entry_t  *pte;
	vm_offset_t pa;

	if (pv_h != PA_TO_PVH(phys)) {
		printf("check_pv_list: incorrect pv_h supplied.\n");
		panic(who);
	}

	if (!PAGE_ALIGNED(phys)) {
		printf("check_pv_list: supplied phys addr not page aligned.\n");
		panic(who);
	}

	if (pv_h->pmap == PMAP_NULL) {
		if (pv_h->next != PV_ENTRY_NULL) {
			printf("check_pv_list: first entry has null pmap, but list non-empty.\n");
			panic(who);
		} else	return;	    /* proper empry lst */
	}

	pv_e = pv_h;
	while (pv_e != PV_ENTRY_NULL) {
		if (!PAGE_ALIGNED(pv_e->va)) {
			printf("check_pv_list: non-aligned VA in entry at 0x%x.\n", pv_e);
			panic(who);
		}
		/*
		 * We can't call pmap_extract since it requires lock.
		 */
		if ((pte = pmap_pte(pv_e->pmap, pv_e->va)) == PT_ENTRY_NULL)
			pa = (vm_offset_t)0;
		else
			pa = M88K_PTOB(pte->pfn) | (pv_e->va & M88K_PGOFSET);

		if (pa != phys) {
			printf("check_pv_list: phys addr diff in entry at 0x%x.\n", pv_e);
			panic(who);
		}
d4772 2
a4773 2
		pv_e = pv_e->next;
	}
d4811 92
a4902 72
	vm_offset_t va,
	old_va,
	phys;
	pv_entry_t  pv_h,
	pv_e,
	saved_pv_e;
	pt_entry_t  *ptep;
	boolean_t   found;
	int      loopcnt;
#if defined(MACHINE_NEW_NONCONTIG)
	int bank;
	unsigned      npages;
#endif 

	/*
	 * for each page in the address space, check to see if there's
	 * a valid mapping. If so makes sure it's listed in the PV_list.
	 */

	if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
		printf("(check_map) checking map at 0x%x\n", map);

	old_va = s;
	for (va = s; va < e; va += PAGE_SIZE) {
		/* check for overflow - happens if e=0xffffffff */
		if (va < old_va)
			break;
		else
			old_va = va;

		if (va == phys_map_vaddr1 || va == phys_map_vaddr2)
			/* don't try anything with these */
			continue;

		ptep = pmap_pte(map, va);

		if (ptep == PT_ENTRY_NULL) {
			/* no page table, skip to next segment entry */
			va = SDT_NEXT(va)-PAGE_SIZE;
			continue;
		}

		if (!PDT_VALID(ptep))
			continue;      /* no page mapping */

		phys = M88K_PTOB(ptep->pfn);  /* pick up phys addr */

		if (!PMAP_MANAGED(phys))
			continue;      /* no PV list */

		/* note: vm_page_startup allocates some memory for itself
		   through pmap_map before pmap_init is run. However,
		   it doesn't adjust the physical start of memory.
		   So, pmap thinks those pages are managed - but they're
		   not actually under it's control. So, the following
		   conditional is a hack to avoid those addresses
		   reserved by vm_page_startup */
		/* pmap_init also allocate some memory for itself. */

#if defined(MACHINE_NEW_NONCONTIG)
		for (npages = 0, bank = 0; bank < vm_nphysseg; bank++)
			npages += vm_physmem[bank].end - vm_physmem[bank].start;
		if (map == kernel_pmap &&
		    va < round_page((vm_offset_t)(pmap_modify_list + npages)))
			continue;
#else
		if (map == kernel_pmap &&
		    va < round_page((vm_offset_t)(pmap_modify_list + (pmap_phys_end - pmap_phys_start))))
			continue;
#endif 
		pv_h = PA_TO_PVH(phys);
		found = FALSE;
d4904 2
a4905 32
		if (pv_h->pmap != PMAP_NULL) {

			loopcnt = 10000;  /* loop limit */
			pv_e = pv_h;
			while (pv_e != PV_ENTRY_NULL) {

				if (loopcnt-- < 0) {
					printf("check_map: loop in PV list at PVH 0x%x (for phys 0x%x)\n", pv_h, phys);
					panic(who);
				}

				if (pv_e->pmap == map && pv_e->va == va) {
					if (found) {
						printf("check_map: Duplicate PV list entries at 0x%x and 0x%x in PV list 0x%x.\n", saved_pv_e, pv_e, pv_h);
						printf("check_map: for pmap 0x%x, VA 0x%x,phys 0x%x.\n", map, va, phys);
						panic(who);
					} else {
						found = TRUE;
						saved_pv_e = pv_e;
					}
				}
				pv_e = pv_e->next;
			}
		}

		if (!found) {
			printf("check_map: Mapping for pmap 0x%x VA 0x%x Phys 0x%x does not appear in PV list 0x%x.\n", map, va, phys, pv_h);
		}
	}

	if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
		printf("(check_map) done \n");
d4946 8
a4953 11
	pmap_t      p;
	int      i;
	vm_offset_t phys;
	pv_entry_t  pv_h;
	int      spl;
#ifdef MACHINE_NEW_NONCONTIG
	int bank;
	unsigned      npages;
#endif 
	if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
		printf("check_pmap_consistency (%s :%x) start.\n", who, curproc);
d4955 1
a4955 1
	if (pv_head_table == PV_ENTRY_NULL) {
d4957 24
a4980 33
		printf("check_pmap_consistency (%s) PV head table not initialized.\n", who);
		return;
	}

	SPLVM(spl);

	p = kernel_pmap;
	check_map(p, VM_MIN_KERNEL_ADDRESS, VM_MAX_KERNEL_ADDRESS, who);

	/* run through all pmaps. check consistency of each one... */
	i = PMAP_MAX;
	for (p = kernel_pmap->next;p != kernel_pmap; p = p->next) {
		if (i == 0) { /* can not read pmap list */
			printf("check_pmap_consistency: pmap strcut loop error.\n");
			panic(who);
		}
		check_map(p, VM_MIN_USER_ADDRESS, VM_MAX_USER_ADDRESS, who);
	}

	/* run through all managed paes, check pv_list for each one */
#if defined(MACHINE_NEW_NONCONTIG)
	for (npages = 0, bank = 0; bank < vm_nphysseg; bank++) {
		for (phys = ptoa(vm_physmem[bank].start); phys < ptoa(vm_physmem[bank].end); phys += PAGE_SIZE) {
			pv_h = PA_TO_PVH(phys);
			check_pv_list(phys, pv_h, who);
		}
	}
#else
	for (phys = pmap_phys_start; phys < pmap_phys_end; phys += PAGE_SIZE) {
		pv_h = PA_TO_PVH(phys);
		check_pv_list(phys, pv_h, who);
	}
#endif /* defined(MACHINE_NEW_NONCONTIG) */
d4982 1
a4982 1
	SPLX(spl);
d4984 2
a4985 2
	if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
		printf("check_pmap consistency (%s :%x): done.\n",who, curproc);
d5011 1
a5011 1
			p->modified,					\
d5034 10
a5043 10
	sdt_entry_t   *sdtp;
	sdt_entry_t   *sdtv;
	int     i;

	printf("Pmap @@ 0x%x:\n", (unsigned)pmap);
	sdtp = pmap->sdt_paddr;
	sdtv = pmap->sdt_vaddr;
	printf("	sdt_paddr: 0x%x; sdt_vaddr: 0x%x; ref_count: %d;\n",
	       (unsigned)sdtp, (unsigned)sdtv,
	       pmap->ref_count);
d5046 45
a5090 44
	printf("	statistics: pagesize %d: free_count %d; "
	       "active_count %d; inactive_count %d; wire_count %d\n",
	       pmap->stats.pagesize,
	       pmap->stats.free_count,
	       pmap->stats.active_count,
	       pmap->stats.inactive_count,
	       pmap->stats.wire_count);

	printf("	zero_fill_count %d; reactiveations %d; "
	       "pageins %d; pageouts %d; faults %d\n",
	       pmap->stats.zero_fill_count,
	       pmap->stats.reactivations,
	       pmap->stats.pageins,
	       pmap->stats.pageouts,
	       pmap->stats.fault);

	printf("	cow_faults %d, lookups %d, hits %d\n",
	       pmap->stats.cow_faults,
	       pmap->stats.loopups,
	       pmap->stats.faults);
#endif
	sdtp = (sdt_entry_t *) pmap->sdt_vaddr;	 /* addr of physical table */
	sdtv = sdtp + SDT_ENTRIES;	/* shadow table with virt address */
	if (sdtp == (sdt_entry_t *)0)
		printf("Error in pmap - sdt_paddr is null.\n");
	else {
		int   count = 0;
		printf("	Segment table at 0x%x (0x%x):\n",
		       (unsigned)sdtp, (unsigned)sdtv);
		for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
			if ((sdtp->table_addr != 0 ) || (sdtv->table_addr != 0)) {
				if (count != 0)
					printf("sdt entry %d skip !!\n", count);
				count = 0;
				printf("   (%x)phys: ", i);
				PRINT_SDT(sdtp);
				printf("   (%x)virt: ", i);
				PRINT_SDT(sdtv);
			} else
				count++;
		}
		if (count != 0)
			printf("sdt entry %d skip !!\n", count);
	}
d5117 107
a5223 107
	sdt_entry_t   *sdtp;   /* ptr to sdt table of physical addresses */
	sdt_entry_t   *sdtv;   /* ptr to sdt shadow table of virtual addresses */
	pt_entry_t *ptep;   /* ptr to pte table of physical page addresses */

	int     i; /* table loop index */
	unsigned long prev_entry; /* keep track of value of previous table entry */
	int     n_dup_entries; /* count contiguous duplicate entries */

	printf("Trace of virtual address 0x%08x. Pmap @@ 0x%08x.\n",
	       va, (unsigned)pmap);

	/*** SDT TABLES ***/
	/* get addrs of sdt tables */
	sdtp = (sdt_entry_t *)pmap->sdt_vaddr;
	sdtv = sdtp + SDT_ENTRIES;

	if (sdtp == SDT_ENTRY_NULL) {
		printf("    Segment table pointer (pmap.sdt_paddr) null, trace stops.\n");
		return;
	}

	n_dup_entries = 0;
	prev_entry = 0xFFFFFFFF;

	if (long_format) {
		printf("    Segment table at 0x%08x (virt shadow at 0x%08x)\n",
		       (unsigned)sdtp, (unsigned)sdtv);
		for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
			if (prev_entry == ((sdt_entry_template_t *)sdtp)->bits
			    && SDTIDX(va) != i && i != SDT_ENTRIES-1) {
				n_dup_entries++;
				continue;  /* suppress duplicate entry */
			}
			if (n_dup_entries != 0) {
				printf("    - %d duplicate entries skipped -\n",n_dup_entries);
				n_dup_entries = 0;
			}
			prev_entry = ((pte_template_t *)sdtp)->bits;
			if (SDTIDX(va) == i) {
				printf("    >> (%x)phys: ", i);
			} else {
				printf("       (%x)phys: ", i);
			}
			PRINT_SDT(sdtp);
			if (SDTIDX(va) == i) {
				printf("    >> (%x)virt: ", i);
			} else {
				printf("       (%x)virt: ", i);
			}
			PRINT_SDT(sdtv);
		} /* for */
	} else {
		/* index into both tables for given VA */
		sdtp += SDTIDX(va);
		sdtv += SDTIDX(va);
		printf("    SDT entry index 0x%x at 0x%x (virt shadow at 0x%x)\n",
		       SDTIDX(va), (unsigned)sdtp, (unsigned)sdtv);
		printf("    phys:  ");
		PRINT_SDT(sdtp);
		printf("    virt:  ");
		PRINT_SDT(sdtv);
	}

	/*** PTE TABLES ***/
	/* get addrs of page (pte) table (no shadow table) */

	sdtp = ((sdt_entry_t *)pmap->sdt_vaddr) + SDTIDX(va);
#ifdef DEBUG
	printf("*** DEBUG (sdtp) ");
	PRINT_SDT(sdtp);
#endif
	sdtv = sdtp + SDT_ENTRIES;
	ptep = (pt_entry_t *)(M88K_PTOB(sdtv->table_addr));
	if (sdtp->dtype != DT_VALID) {
		printf("    segment table entry invlid, trace stops.\n");
		return;
	}

	n_dup_entries = 0;
	prev_entry = 0xFFFFFFFF;
	if (long_format) {
		printf("        page table (ptes) at 0x%x\n", (unsigned)ptep);
		for (i = 0; i < PDT_ENTRIES; i++, ptep++) {
			if (prev_entry == ((pte_template_t *)ptep)->bits
			    && PDTIDX(va) != i && i != PDT_ENTRIES-1) {
				n_dup_entries++;
				continue;  /* suppress suplicate entry */
			}
			if (n_dup_entries != 0) {
				printf("    - %d duplicate entries skipped -\n",n_dup_entries);
				n_dup_entries = 0;
			}
			prev_entry = ((pte_template_t *)ptep)->bits;
			if (PDTIDX(va) == i) {
				printf("    >> (%x)pte: ", i);
			} else {
				printf("       (%x)pte: ", i);
			}
			PRINT_PDT(ptep);
		} /* for */
	} else {
		/* index into page table */
		ptep += PDTIDX(va);
		printf("    pte index 0x%x\n", PDTIDX(va));
		printf("    pte: ");
		PRINT_PDT(ptep);
	}
d5236 37
a5272 37
	pt_entry_t *pte;
	sdt_entry_t   *sdt;
	int                         spl;

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		PMAP_UNLOCK(pmap, spl);
		return FALSE;
	}

	if (!PDT_VALID(pte)) {
		PMAP_UNLOCK(pmap, spl);
		return FALSE;
	}

	/*
	 * Valid pte.  If the transaction was a read, there is no way it
	 *  could have been a fault, so return true.  For now, assume
	 *  that a write transaction could have caused a fault.  We need
	 *  to check pte and sdt entries for write permission to really
	 *  tell.
	 */

	if (type == VM_PROT_READ) {
		PMAP_UNLOCK(pmap, spl);
		return TRUE;
	} else {
		sdt = SDTENT(pmap,va);
		if (sdt->prot || pte->prot) {
			PMAP_UNLOCK(pmap, spl);
			return FALSE;
		} else {
			PMAP_UNLOCK(pmap, spl);
			return TRUE;
		}
	}
d5280 2
a5281 2
	*startp = virtual_avail;
	*endp = virtual_end;
a5283 2
#if !defined(MACHINE_NEW_NONCONTIG)

d5287 1
a5287 1
	return atop(avail_end - avail_next);
d5293 2
a5294 2
	if (avail_next == avail_end)
		return FALSE;
d5296 3
a5298 3
	*addrp = avail_next;
	avail_next += PAGE_SIZE;
	return TRUE;
a5300 2
#endif

d5308 34
a5341 34
	     pmap_t pmap,
	     boolean_t data,
	     int i,
	     vm_offset_t va,
	     vm_offset_t pa,
	     boolean_t super,
	     boolean_t wt,
	     boolean_t global,
	     boolean_t ci,
	     boolean_t wp,
	     boolean_t valid)
{
	register batc_template_t batctmp;

	if (i < 0 || i > (BATC_MAX - 1)) {
		panic("pmap_set_batc: illegal batc number");
		/* bad number */
		return;
	}

	batctmp.field.lba = va >> 19;
	batctmp.field.pba = pa >> 19;
	batctmp.field.sup = super;
	batctmp.field.wt = wt;
	batctmp.field.g = global;
	batctmp.field.ci = ci;
	batctmp.field.wp = wp;
	batctmp.field.v = valid;

	if (data) {
		pmap->d_batc[i].bits = batctmp.bits;
	} else {
		pmap->i_batc[i].bits = batctmp.bits;
	}
d5344 16
a5359 16
void 
use_batc(task_t task,
	 boolean_t data,	 /* for data-cmmu ? */
	 int i,			 /* batc number */
	 vm_offset_t va,	 /* virtual address */
	 vm_offset_t pa,	 /* physical address */
	 boolean_t s,		 /* for super-mode ? */
	 boolean_t wt,		 /* is writethrough */
	 boolean_t g,		 /* is global ? */
	 boolean_t ci,		 /* is cache inhibited ? */
	 boolean_t wp,		 /* is write-protected ? */
	 boolean_t v)		 /* is valid ? */
{
	pmap_t pmap;
	pmap = vm_map_pmap(task->map);
	pmap_set_batc(pmap, data, i, va, pa, s, wt, g, ci, wp, v);
d5381 1
a5381 1
	pmap_range_t this, next;
d5383 7
a5389 7
	this = *ranges;
	while (this != 0) {
		next = this->next;
		pmap_range_free(this);
		this = next;
	}
	*ranges = 0;
d5398 1
a5398 1
	pmap_range_t range;
d5400 7
a5406 7
	for (range = *ranges; range != 0; range = range->next) {
		if (address < range->start)
			return FALSE;
		if (address < range->end)
			return TRUE;
	}
	return FALSE;
d5416 1
a5416 1
	pmap_range_t range, *prev;
d5418 1
a5418 1
	/* look for the start address */
d5420 6
a5425 6
	for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
		if (start < range->start)
			break;
		if (start <= range->end)
			goto start_overlaps;
	}
d5427 1
a5427 1
	/* start address is not present */
d5429 2
a5430 2
	if ((range == 0) || (end < range->start)) {
		/* no overlap; allocate a new range */
d5432 7
a5438 7
		range = pmap_range_alloc();
		range->start = start;
		range->end = end;
		range->next = *prev;
		*prev = range;
		return;
	}
d5440 1
a5440 1
	/* extend existing range forward to start */
d5442 1
a5442 1
	range->start = start;
d5444 2
a5445 2
start_overlaps:
	assert((range->start <= start) && (start <= range->end));
d5447 1
a5447 1
	/* delete redundant ranges */
d5449 2
a5450 2
	while ((range->next != 0) && (range->next->start <= end)) {
		pmap_range_t old;
d5452 5
a5456 5
		old = range->next;
		range->next = old->next;
		range->end = old->end;
		pmap_range_free(old);
	}
d5458 1
a5458 1
	/* extend existing range backward to end */
d5460 2
a5461 2
	if (range->end < end)
		range->end = end;
d5471 12
a5482 1
	pmap_range_t range, *prev;
d5484 4
a5487 1
	/* look for start address */
d5489 4
a5492 28
	for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
		if (start <= range->start)
			break;
		if (start < range->end) {
			if (end < range->end) {
				pmap_range_t new;
				/* split this range */
				new = pmap_range_alloc();
				new->next = range->next;
				new->start = end;
				new->end = range->end;

				range->next = new;
				range->end = start;
				return;
			}
			/* truncate this range */
			range->end = start;
		}
	}

	/* start address is not in the middle of a range */

	while ((range != 0) && (range->end <= end)) {
		*prev = range->next;
		pmap_range_free(range);
		range = *prev;
	}
d5494 16
a5509 2
	if ((range != 0) && (range->start < end))
		range->start = end;
d5512 1
@


1.12.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12.4.2 2001/04/18 16:11:38 niklas Exp $	*/
d47 3
d64 1
d66 1
a70 1
#include <machine/cmmu.h>
d84 7
a96 1
#ifdef	DEBUG
d100 3
a102 1

d106 1
d137 4
a141 1
int pmap_con_dbg = CD_NONE;
d157 1
a157 1
kpdt_entry_t     kpdt_free;
d169 1
a169 1
#define	KERNEL_PDT_SIZE	(atop(MAX_KERNEL_VA_SIZE) * sizeof(pt_entry_t))
d175 1
a175 1
#define	M188_PDT_SIZE	(atop(UTIL_SIZE) * sizeof(pt_entry_t))
d181 1
a181 1
#define	M1x7_PDT_SIZE	(atop(OBIO_SIZE) * sizeof(pt_entry_t))
d191 1
a191 1
 * Used in pmap_copy_page() and pmap_zero_page().
d195 4
d214 24
d276 1
d342 6
d388 2
d392 1
a392 2
 *	are specified by 'users', for the operating mode specified by
 *      'kernel'.
d398 6
a403 1
 *      kernel  nonzero if supervisor mode, zero if user mode
d419 1
a419 1
			cmmu_flush_remote_tlb(cpu, kernel, va, PAGE_SIZE);
d426 1
d428 7
d437 1
a437 1
unsigned int
a475 1
#ifdef DIAGNOSTIC
a479 1
#endif
d501 1
a501 1
 * uvm_km_zalloc. (Obviously, because uvm_km_zalloc uses the kernel map
d529 1
a529 1
pt_entry_t *
d571 4
a574 4
 *	virt	virtual address of range to map
 *	start	physical address of range to map
 *	end	physical address of end of range
 *	prot	protection attributes
d601 1
a601 3
#ifdef MVME197
extern void m197_load_patc(int, vm_offset_t, vm_offset_t, int);
#endif
d613 1
a613 1
	static m197_atc_initialized = FALSE;
d634 1
a634 1
	template.bits = trunc_page(start) | aprot | cmode | DT_VALID;
d636 1
a636 1
	npages = atop(round_page(end) - trunc_page(start));
d652 2
a653 4
		if (cputyp == CPU_197 && m197_atc_initialized == FALSE) {
			int i;

			for (i = 0; i < 32; i++)
d656 2
a657 1
			m197_atc_initialized = TRUE;
d660 2
a661 2
		virt += PAGE_SIZE;
		template.bits += PAGE_SIZE;
d676 5
a680 5
 *	virt	virtual address of range to map
 *	start	physical address of range to map
 *	end	physical address of end of range
 *	prot	protection attributes
 *	cmode	cache control attributes
d683 1
a683 1
 *	batc_used	number of BATC used
d692 1
d735 1
a735 1
	template.bits = trunc_page(start) | aprot | DT_VALID | cmode;
d745 2
a746 2
	num_phys_pages = atop(round_page(end) - 
				   trunc_page(start));
d757 1
a757 1
		     num_phys_pages >= BATC_BLKBYTES/PAGE_SIZE &&
d776 1
a776 1
				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE ) {
d786 2
a787 2
			template.pte.pfn = atop(phys);
			num_phys_pages -= BATC_BLKBYTES/PAGE_SIZE;
d801 3
a803 3
		virt += PAGE_SIZE;
		phys += PAGE_SIZE;
		template.bits += PAGE_SIZE;
d807 1
a807 1
	return (round_page(virt));
d855 1
a855 1
	if (mode & CACHE_MASK) {
d864 1
a864 1
	if (pmap == PMAP_NULL) {
d870 3
d880 1
a880 1
	for (va = s; va < e; va += PAGE_SIZE) {
d905 4
a908 2
				cmmu_flush_remote_cache(cpu, ptoa(pte->pfn),
							PAGE_SIZE);
d920 1
a920 1
 *	Bootstrap the system enough to run with virtual memory.
d926 3
a928 3
 *	load_start	PA where kernel was loaded
 *	&phys_start	PA of first available physical page
 *	&phys_end	PA of last available physical page
d934 5
a938 5
 *	PAGE_SIZE	VM (software) page size
 *	kernelstart	start symbol of kernel text
 *	etext		end of kernel text
 *	phys_map_vaddr1 VA of page mapped arbitrarily for debug/IO
 *	phys_map_vaddr2 VA of page mapped arbitrarily for debug/IO
d948 1
a948 1
 * following the end of the kernel code/data, sufficient page of
d965 5
a969 5
pmap_bootstrap(vm_offset_t load_start,
	       vm_offset_t   *phys_start,
	       vm_offset_t   *phys_end,
	       vm_offset_t   *virt_start,
	       vm_offset_t   *virt_end)
d991 4
d1017 1
a1017 1
	*phys_start = round_page(*phys_start);
d1019 1
a1019 1
		      (trunc_page((unsigned)&kernelstart) - load_start);
d1083 1
a1083 1
	for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i > 0; i--) {
d1097 1
a1097 1
			       trunc_page((unsigned)&kernelstart));
d1099 1
a1099 1
	e_text = round_page(e_text);
d1114 1
a1114 1
	assert(vaddr == trunc_page((unsigned)&kernelstart));
d1118 1
a1118 1
		      (vm_offset_t)trunc_page(((unsigned)&kernelstart)),
d1141 1
a1141 1
			vaddr = round_page(vaddr + 1);
d1245 1
a1245 1
		 * This means that if different physical pages are going to be mapped
d1291 1
a1291 1
	apr_data.field.st_base = atop(kernel_pmap->sdt_paddr);
d1298 1
d1346 1
d1349 34
d1397 6
a1402 6
 *	pv_head_table
 *	pv_lock_table
 *	pmap_modify_list
 *	pmap_phys_start
 *	pmap_phys_end
 *	pmap_initialized
d1405 2
a1406 1
 *	uvm_km_zalloc
d1420 4
a1423 2
 *	uvm_km_zalloc() memory for pv_table
 * 	uvm_km_zalloc() memory for modify_bits
d1426 1
d1463 1
d1465 3
d1505 66
d1574 10
d1585 1
a1585 1
 *	Zeroes the specified page.
d1594 1
d1597 1
d1603 1
a1603 1
 *	This routine maps the physical pages at the 'phys_map' virtual
d1612 1
d1614 1
a1614 1
	int		cpu;
d1617 2
a1618 2
	cpu = cpu_number();
	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu * PAGE_SIZE));
d1621 13
a1633 11
	template.bits = trunc_page(phys)
			| m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
			| DT_VALID | CACHE_GLOBAL;

	spl_sav = splimp();
	cmmu_flush_tlb(1, srcva, PAGE_SIZE);
	*srcpte = template.pte;
	splx(spl_sav);
	bzero((void*)srcva, PAGE_SIZE);
	/* force the data out */
	cmmu_flush_remote_data_cache(cpu, phys, PAGE_SIZE);
d1639 13
d1658 1
a1658 1
 * Parameters:
d1677 3
d1694 1
d1699 1
a1699 1
	s = round_page(2 * SDT_SIZE);
d1707 1
d1709 3
d1715 6
d1726 1
a1726 1
	pmap_extract(kernel_pmap, (vaddr_t)segdt, (paddr_t *)&p->sdt_paddr);
d1815 1
a1815 1
void
d1859 1
d1861 4
d1894 1
a1894 1
 *	free
d1929 1
a1929 1
		free((caddr_t)p, M_VMPMAP);
d1987 1
d1993 1
a1993 1
 *	free
d2017 1
a2017 1
void
d2023 1
d2080 1
a2080 1
		pa = ptoa(pfn);
d2130 1
a2130 1
		} /* if PMAP_MANAGED */
d2133 3
a2135 1
		 * Reflect modify bits to pager and zero (invalidate,
d2139 1
d2141 6
a2146 6
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by other cpu.
		 */
		opte.bits = invalidate_pte(pte);
		flush_atc_entry(users, tva, kflush);
d2148 7
a2154 4
		if (opte.pte.modified) {
			if (IS_VM_PHYSADDR(pa)) {
				vm_page_set_modified(
				    PHYS_TO_VM_PAGE(opte.bits & ~PAGE_MASK));
d2156 2
a2157 3
			/* keep track ourselves too */
			if (PMAP_MANAGED(pa))
				SET_ATTRIB(pa, 1);
d2236 1
d2241 1
a2241 1
 *	free
d2261 1
d2320 1
a2320 1
		if (ptoa(pte->pfn) != phys)
a2334 4
		 *
		 * Invalidate pte temporarily to avoid the modified 
		 * bit and/or the reference bit being written back
		 * by other cpu.
d2336 8
a2343 2
		opte.bits = invalidate_pte(pte);
		flush_atc_entry(users, va, kflush);
d2345 7
a2351 4
		if (opte.pte.modified) {
			vm_page_set_modified((vm_page_t)PHYS_TO_VM_PAGE(phys));
			/* keep track ourselves too */
			SET_ATTRIB(phys, 1);
d2385 1
d2395 1
a2395 1
void
d2400 1
d2408 1
a2408 1
		if (pmap_con_dbg & CD_COW)
d2426 3
a2428 1
		goto out;	/* no mappings */
d2463 1
a2463 1
		if (ptoa(pte->pfn) != phys)
a2467 4
		 *
		 * Invalidate pte temporarily to avoid the modified 
		 * bit and/or the reference bit being written back
		 * by other cpu.
d2469 16
a2484 7
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		opte.pte.prot = M88K_RO;
		((pte_template_t *)pte)->bits = opte.bits;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);
		
a2489 1
out:
d2492 1
d2528 1
d2530 1
a2530 1
	vm_offset_t		va;
d2589 16
a2604 12
		/*
		 * Invalidate pte temporarily to avoid the 
		 * modified bit and/or the reference bit being 
		 * written back by other cpu.
		 */
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		opte.pte.prot = ap;
		((pte_template_t *)pte)->bits = opte.bits;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);
		pte++;
d2618 1
a2618 1
 *	uvm_km_zalloc). Thus it must be called in a unlock/lock loop
d2635 1
a2635 1
 *	free
d2653 1
a2653 1
void
d2660 1
d2689 1
d2691 4
a2694 1
	pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr);
d2714 1
d2716 3
d2755 11
d2792 1
a2792 1
 *	free
d2841 1
d2882 3
a2884 1
	 * Expand pmap to include this pte.
d2898 1
a2898 1
	old_pa = ptoa(pte->pfn);
d2916 1
a2916 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_INH;
d2918 1
a2918 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_GLOBAL;
d2928 15
a2942 10
			/*
			 * Invalidate pte temporarily to avoid being written back
			 * the modified bit and/or the reference bit by other cpu.
			 */
			spl_sav = splimp();
			opte.bits = invalidate_pte(pte);
			template.pte.modified = opte.pte.modified;
			*pte++ = template.pte;
			flush_atc_entry(users, va, kflush);
			splx(spl_sav);
d3041 1
a3041 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_INH;
d3043 1
a3043 1
			template.bits = DT_VALID | ap | trunc_page(pa) | CACHE_GLOBAL;
d3048 1
a3048 1
		*(int *)pte = template.bits;
d3060 3
a3062 1
 *	Routine:	pmap_unwire
d3066 1
a3066 1
 *	Parameters:
d3068 2
a3069 1
 *		v		virtual address of page to be unwired
d3083 1
a3083 1
pmap_unwire(pmap_t map, vm_offset_t v)
d3086 1
d3092 1
a3092 1
		panic ("pmap_unwire: pte missing");
d3094 5
a3098 1
	if (pte->wired) {
d3101 3
a3103 2
		pte->wired = 0;
	}
d3107 1
a3107 1
} /* pmap_unwire() */
d3112 2
a3120 1
 *	pap		storage for result.
d3136 2
a3137 2
boolean_t
pmap_extract(pmap_t pmap, vm_offset_t va, paddr_t *pap)
d3139 3
a3141 3
	pt_entry_t  *pte;
	paddr_t pa;
	int   i;
d3153 1
a3153 1
				*pap = (batc_entry[i].pba << BATC_BLKSHIFT) | 
d3155 1
a3155 1
				return (TRUE);
d3160 3
a3162 3
	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		goto fail;
	} else {
d3164 1
a3164 1
			pa = ptoa(pte->pfn);
d3166 1
a3166 1
			goto fail;
d3169 2
a3170 2
	pa |= (va & PAGE_MASK); /* offset within page */
	*pap = pa;
d3173 44
a3216 5
	return (TRUE);
fail:
	PMAP_UNLOCK(pmap, spl);
	return (FALSE);
} /* pmap_extract() */
a3236 1
/* ARGSUSED */
d3241 4
d3415 1
a3415 1
		 * calls uvm_km_free or free, which will invoke another 
d3440 1
a3440 1
 *	In a mono-processor implementation the cpu
d3467 1
a3467 1
	int cpu = cpu_number();  
d3481 1
a3481 1
		apr_data.field.st_base = atop(pmap->sdt_paddr); 
d3494 1
a3494 1
		cmmu_pmap_activate(cpu, apr_data.bits, 
d3513 1
a3513 1
		SETBIT_CPUSET(cpu, &(pmap->cpus_using));
d3540 1
a3540 1
 * pmap_deactive simply clears the cpus_using field in given pmap structure.
d3547 1
a3547 1
	int cpu = cpu_number();  
d3555 1
a3555 1
		CLRBIT_CPUSET(cpu, &(pmap->cpus_using));
d3564 1
a3564 1
 *		Copies the specified pages.
d3576 1
d3578 1
d3584 1
a3584 1
 *	This routine maps the physical pages at the 'phys_map' virtual
d3593 1
d3597 1
a3597 1
	int      cpu = cpu_number();
d3604 2
a3605 2
	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu << PAGE_SHIFT));
d3610 18
a3627 2
	template.bits = trunc_page(src) | aprot | 
		DT_VALID | CACHE_GLOBAL;
d3629 5
a3633 18
	/* do we need to write back dirty bits */
	spl_sav = splimp();
	cmmu_flush_tlb(1, srcva, PAGE_SIZE);
	*srcpte = template.pte;

	/*
	 *	Map destination physical address.
	 */
	template.bits = trunc_page(dst) | aprot | 
		CACHE_GLOBAL | DT_VALID;
	cmmu_flush_tlb(1, dstva, PAGE_SIZE);
	*dstpte  = template.pte;
	splx(spl_sav);

	bcopy((void*)srcva, (void*)dstva, PAGE_SIZE);
	/* flush source, dest out of cache? */
	cmmu_flush_remote_data_cache(cpu, src, PAGE_SIZE);
	cmmu_flush_remote_data_cache(cpu, dst, PAGE_SIZE);
d3638 1
a3638 1
 *	Routine:	PMAP_CLEAR_MODIFY
d3640 3
a3642 2
 *	Function:
 *		Clear the modify bits on the specified physical page.
d3645 3
a3647 1
 *		phys	physical address of page
d3650 224
a3873 2
 *		pv_head_table, pv_lists
 *		pmap_modify_list
d3899 1
d3919 1
a3919 1
	/* update corresponding pmap_modify_list element */
d3932 1
a3932 1
	/* for each listed pmap, turn off the page modified bit */
d3952 15
a3966 12
		/*
		 * Invalidate pte temporarily to avoid the modified bit 
		 * and/or the reference being written back by other cpu.
		 */
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		/* clear modified bit */
		opte.pte.modified = 0;
		((pte_template_t *)pte)->bits = opte.bits;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);

d4016 1
d4031 1
a4031 1
is_mod_Retry:
d4071 3
a4073 2
		if (ptep->modified) {
			simple_unlock(&pvep->pmap->lock);
d4075 2
a4076 2
			if ((pmap_con_dbg & (CD_IMOD | CD_FULL)) == (CD_IMOD | CD_FULL))
				printf("(pmap_is_modified :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d4078 5
a4082 3
			UNLOCK_PVH(phys);
			SPLX(spl);
			return (TRUE);
d4097 6
d4124 1
a4124 1
 * were found on, a TLB flush will be performed.
d4135 1
d4187 15
a4201 11
		/*
		 * Invalidate pte temporarily to avoid the modified bit 
		 * and/or the reference being written back by other cpu.
		 */
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		/* clear reference bit */
		opte.pte.pg_used = 0;
		((pte_template_t *)pte)->bits = opte.bits;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);
d4213 4
d4218 1
a4218 1
 *	Return whether or not the specified physical page is referenced by
d4251 1
d4261 2
a4262 1
is_ref_Retry:
d4280 8
a4287 5
		if (ptep->pg_used) {
			simple_unlock(&pvep->pmap->lock);
			UNLOCK_PVH(phys);
			SPLX(spl);
			return (TRUE);
d4299 66
d4389 1
a4389 1
#ifdef FUTURE_MAYBE
d4398 1
a4398 1
 *	to	kernel virtual address of destination
d4421 1
d4436 1
a4436 1
			panic("pagemove: source addr not mapped");
d4445 1
a4445 1
				panic("pagemove: Cannot allocate destination pte");
d4450 2
a4451 2
			printf("pagemove: destination vaddr 0x%x, pte = 0x%x\n", to, *((unsigned *)dstpte));
			panic("pagemove: destination pte already valid");
d4465 1
a4465 1
		pa = ptoa(srcpte->pfn);
a4475 3
		 *
		 * Invalidate pte temporarily to avoid the modified bit 
		 * and/or the reference being written back by other cpu.
d4477 12
a4488 6
		opte.bits = invalidate_pte(srcpte);
		flush_atc_entry(users, from, 1);
		((pte_template_t *)dstpte)->bits = opte.bits;
		from += PAGE_SIZE;
		to += PAGE_SIZE;

d4495 28
d4524 28
d4553 22
d4628 34
d4696 1
a4696 1
 * the pmap in question. If the returned physical address does not match
d4700 1
a4700 1
void
d4736 1
a4736 1
			pa = ptoa(pte->pfn) | (pv_e->va & PAGE_MASK);
d4749 1
a4749 1
 *	Routine:	CHECK_MAP (internal)
d4774 3
a4776 2
 * searched for the corresponding pmap/va entry. If not found, or if
 * duplicate PV list entries are found, the function panics.
d4779 1
a4779 1
void
d4791 1
d4794 1
d4827 1
a4827 1
		phys = ptoa(ptep->pfn);  /* pick up phys addr */
d4841 1
d4847 5
d4916 3
a4918 2
 *	There are some pages which do not appear in PV list. These pages
 * are allocated for pv structures by uvm_km_zalloc called in pmap_init.
d4920 1
a4920 1
 * PV manipulations had not been activated when these pages were alloceted.
d4924 1
a4924 1
void
d4932 1
d4935 1
a4935 1

d4951 2
a4952 2
	i = 512;
	for (p = kernel_pmap->next;p != kernel_pmap; p = p->next, i--) {
d4954 1
a4954 1
			printf("check_pmap_consistency: pmap struct loop error.\n");
d4961 1
d4968 6
d4992 1
a4992 1
			ptoa(p->table_addr),			        \
d5007 262
d5276 22
a5297 1
#ifdef USING_BATC
d5360 1
a5360 1
#ifdef FUTURE_MAYBE
@


1.12.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12.4.3 2001/07/04 10:20:19 niklas Exp $	*/
d60 1
d64 1
a69 1
#include <machine/pte.h>
d77 1
a77 1
extern vm_offset_t      avail_start, avail_end;
a333 9
void flush_atc_entry __P((long, vm_offset_t, int));
unsigned int m88k_protection __P((pmap_t, vm_prot_t));
pt_entry_t *pmap_expand_kmap __P((vm_offset_t, vm_prot_t));
void pmap_free_tables __P((pmap_t));
void pmap_remove_range __P((pmap_t, vm_offset_t, vm_offset_t));
void pmap_copy_on_write __P((vm_offset_t));
void pmap_expand __P((pmap_t, vm_offset_t));
void cache_flush_loop __P((int, vm_offset_t, int));

d552 1
a552 1
	static int m197_atc_initialized = FALSE;
d628 1
d1044 2
d1274 1
d1455 2
a1456 2
struct pmap *
pmap_create(void)
d1458 7
a1464 1
	struct pmap *p;
d1468 1
a1468 1
	p = (struct pmap *)malloc(sizeof(*p), M_VMPMAP, M_WAITOK);
d1609 1
a1609 1
	  because of integer overflow/wraparound if VM_MAX_ADDRESS 
d1612 2
a1613 2
	i = VM_MIN_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
d1768 1
d1781 4
a1784 3
 * 'modified' bit, if on, is referenced to the VM, and into the appropriate
 * entry in the pmap_modify_list. Next, the function must find the PV list
 * entry associated with this pmap/va (if it doesn't exist - the function
d1917 3
a1919 5
			if (vm_physseg_find(atop(pa), NULL) != -1) {
				struct vm_page *pg;

				pg = PHYS_TO_VM_PAGE(opte.bits & ~PAGE_MASK);
				pg->flags &= ~PG_CLEAN;
d2004 1
d2108 1
a2108 4
			struct vm_page *pg;

			pg = PHYS_TO_VM_PAGE(phys);
			pg->flags &= ~PG_CLEAN;
d2555 1
a2555 1
int
d2557 2
a2558 2
	   vm_prot_t prot,
	   int flags)
d2570 4
a2573 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
a2773 1
	return (KERN_SUCCESS);
d3030 1
a3030 1
	  because of integer overflow/wraparound if VM_MAX_ADDRESS 
d3034 2
a3035 2
	i = VM_MIN_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
d3040 1
a3040 1
		sdt_va = VM_MIN_ADDRESS + PDT_TABLE_GROUP_VA_SPACE*i;
d3063 1
a3063 1
		sdt_vt = sdt_va <= VM_MAX_ADDRESS-PDT_TABLE_GROUP_VA_SPACE ?
d3065 1
a3065 1
			 VM_MAX_ADDRESS;
d3303 1
a3303 1
 *		pg	vm_page
d3318 1
a3318 1
 *	The modify_list entry corresponding to the
d3324 2
a3325 2
boolean_t
pmap_clear_modify(struct vm_page *pg)
a3335 4
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
	boolean_t	ret;

	ret = pmap_is_modified(pg);
a3336 1
#ifdef DIAGNOSTIC
d3338 5
a3342 1
		panic("pmap_clear_modify: not managed?");
a3343 1
#endif
d3362 1
a3362 1
		return (ret);
d3366 2
a3367 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->next) {
d3398 1
a3401 2

	return (ret);
d3413 1
a3413 1
 *		pg		vm_page
d3426 3
d3440 1
a3440 1
pmap_is_modified(struct vm_page *pg)
a3446 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
a3447 1
#ifdef DIAGNOSTIC
d3449 5
a3453 1
		panic("pmap_is_modified: not managed?");
a3454 1
#endif
d3527 1
a3527 1
 *		pg		vm_page
d3541 1
d3546 2
a3547 2
boolean_t
pmap_clear_reference(struct vm_page *pg)
d3549 9
a3557 13
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*pte;
	pmap_t		pmap;
	int		spl, spl_sav;
	vm_offset_t	va;
	unsigned	users;
	pte_template_t	opte;
	int		kflush;
	paddr_t		phys;
	boolean_t	ret;

	phys = VM_PAGE_TO_PHYS(pg);
a3558 1
#ifdef DIAGNOSTIC
d3560 6
a3565 1
		panic("pmap_clear_reference: not managed?");
a3566 2
#endif
	ret = pmap_is_referenced(pg);
d3583 1
a3583 1
		return (ret);
a3622 2

	return (ret);
d3633 1
a3633 1
 *	pg		vm_page
d3646 4
a3649 1
 *	This routine walks the PV list corresponding to the
d3656 1
a3656 1
pmap_is_referenced(struct vm_page *pg)
a3661 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
a3662 1
#ifdef DIAGNOSTIC
d3664 1
a3664 2
		panic("pmap_is_referenced: not managed?");
#endif
d3714 1
a3714 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
a3715 2
	paddr_t phys = VM_PAGE_TO_PHYS(pg);

a3720 1
	case VM_PROT_READ|VM_PROT_WRITE:
d4170 1
a4170 1
		check_map(p, VM_MIN_ADDRESS, VM_MAX_ADDRESS, who);
d4364 2
a4426 26

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pmap_enter(pmap_kernel(), va, pa, prot, VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
}

void
pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs)
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	}
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
@


1.12.4.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d59 1
d189 1
a189 1
struct simplelock *pv_lock_table; /* array */
d191 1
a191 1
pv_entry_t pv_head_table; /* array of entries, one per page */
d306 2
a307 2
void check_pv_list __P((vm_offset_t, pv_entry_t, char *));
void check_pmap_consistency __P((char *));
a340 2
void pmap_pinit __P((pmap_t));
void pmap_release __P((pmap_t));
d343 1
a343 1
 * Routine:	FLUSH_ATC_ENTRY
d1190 1
a1190 1
	virt += ((n) * PAGE_SIZE); \
d1195 3
a1197 1
	SYSMAP(caddr_t, vmpte, vmmap, 1);
a1199 2

	SYSMAP(struct msgbuf *, msgbufmap, msgbufp, btoc(MSGBUFSIZE));
@


1.12.4.6
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12.4.5 2001/11/13 21:04:15 niklas Exp $	*/
d49 1
a55 1
#include <sys/pool.h>
a61 1
#include <machine/mmu.h>
a125 2
struct pool pmappool, pvpool;

d277 10
a286 12
#define	SPLVM(spl)	spl = splvm();
#define	SPLX(spl)	splx(spl);
#define PMAP_LOCK(pmap,spl) \
	do { \
		SPLVM(spl); \
		simple_lock(&(pmap)->lock); \
	} while (0)
#define PMAP_UNLOCK(pmap, spl) \
	do { \
		simple_unlock(&(pmap)->lock); \
		SPLX(spl); \
	} while (0)
d793 1
a793 1
	int		spl;
d838 1
d842 1
a1392 6

	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl", 0,
	    pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", 0,
	    NULL, NULL, M_VMPVENT);

d1425 1
a1425 1
	unsigned int	spl;
d1437 1
a1437 1
	SPLVM(spl);
d1440 1
a1440 1
	SPLX(spl);
d1467 2
a1468 1
	p = (struct pmap *)pool_get(&pmappool, PR_WAITOK);
d1701 1
a1701 1
		pool_put(&pmappool, p);
d1873 1
a1873 1
					pool_put(&pvpool, cur);
d1892 1
a1892 1
				pool_put(&pvpool, cur);
d2091 1
a2091 1
			pool_put(&pvpool, cur);
d2159 1
a2159 1
	int                  spl;
d2229 1
d2234 1
d2278 1
a2278 1
	int			spl;
d2344 1
d2349 1
d2562 1
a2562 1
	int		spl;
d2652 1
d2657 1
d2731 3
a2733 1
					pv_e = pool_get(&pvpool, PR_NOWAIT);
d2770 1
a2770 1
		pool_put(&pvpool, pv_e);
d2772 1
a2772 1
	return (0);
d2912 34
d3254 1
a3254 1
	unsigned int spl;
d3275 1
a3275 1
	SPLVM(spl);
d3286 1
a3286 1
	SPLX(spl);
d3330 1
a3330 1
	int        spl;
d3389 1
d3395 1
d3549 1
a3549 1
	int		spl;
d3608 1
d3614 1
d4433 12
@


1.12.4.7
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2 2
 * Copyright (c) 2001, 2002 Miodrag Vallat
 * Copyright (c) 1998-2001 Steve Murphree, Jr.
d62 1
d66 1
d68 4
d76 2
a77 2
extern vaddr_t      avail_start, avail_end;
extern vaddr_t      virtual_avail, virtual_end;
d93 30
a122 26
#define CD_NONE		0x00
#define CD_NORM		0x01
#define CD_FULL		0x02

#define CD_ACTIVATE	0x0000004	/* pmap_activate */
#define CD_KMAP		0x0000008	/* pmap_expand_kmap */
#define CD_MAP		0x0000010	/* pmap_map */
#define CD_MAPB		0x0000020	/* pmap_map_batc */
#define CD_CACHE	0x0000040	/* pmap_cache_ctrl */
#define CD_BOOT		0x0000080	/* pmap_bootstrap */
#define CD_INIT		0x0000100	/* pmap_init */
#define CD_CREAT	0x0000200	/* pmap_create */
#define CD_FREE		0x0000400	/* pmap_release */
#define CD_DESTR	0x0000800	/* pmap_destroy */
#define CD_RM		0x0001000	/* pmap_remove */
#define CD_RMAL		0x0002000	/* pmap_remove_all */
#define CD_PROT		0x0004000	/* pmap_protect */
#define CD_EXP		0x0008000	/* pmap_expand */
#define CD_ENT		0x0010000	/* pmap_enter */
#define CD_UPD		0x0020000	/* pmap_update */
#define CD_COL		0x0040000	/* pmap_collect */
#define CD_CBIT		0x0080000	/* pmap_changebit */
#define CD_TBIT		0x0100000	/* pmap_testbit */
#define CD_CREF		0x0200000	/* pmap_clear_reference */
#define CD_PGMV		0x0400000	/* pagemove */
#define CD_ALL		0x0FFFFFC
d129 2
a130 2
caddr_t vmmap;
pt_entry_t *vmpte, *msgbufmap;
d132 1
a132 1
struct pmap kernel_pmap_store;
d137 2
a138 2
	kpdt_entry_t	next;
	paddr_t		phys;
d142 1
a142 1
kpdt_entry_t	kpdt_free;
d171 1
a171 1
#define	OBIO_PDT_SIZE	((brdtyp == BRD_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
d178 1
a178 1
vaddr_t phys_map_vaddr1, phys_map_vaddr2;
d181 1
a181 1
 * The Modify List
d191 2
d195 2
a196 2
#define	PMAP_MANAGED(pa) \
	(pmap_initialized && IS_VM_PHYSADDR(pa))
d198 1
a198 1
#define	pa_to_pvh(pa)							\
d205 22
a226 1
#define	pa_to_attribute(pa)						\
d231 1
a231 1
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
d235 1
a235 1
 *	Locking primitives
d239 38
a281 1

d293 2
a294 1
#define PV_TABLE_SIZE(n)	((vsize_t)((n) * sizeof(struct pv_entry)))
d303 3
a305 1
 * number of BATC entries used
a306 1
int   batc_used;
d308 1
a308 4
/*
 * keep track BATC mapping
 */
batc_entry_t batc_entry[BATC_MAX];
d310 2
a311 4
vaddr_t kmapva = 0;
extern vaddr_t bugromva;
extern vaddr_t sramva;
extern vaddr_t obiova;
d313 8
a320 14
/*
 * Internal routines
 */
void flush_atc_entry __P((long, vaddr_t, boolean_t));
pt_entry_t *pmap_expand_kmap __P((vaddr_t, vm_prot_t));
void pmap_remove_range __P((pmap_t, vaddr_t, vaddr_t));
void pmap_expand __P((pmap_t, vaddr_t));
void pmap_release __P((pmap_t));
vaddr_t pmap_map __P((vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int));
vaddr_t pmap_map_batc __P((vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int));
pt_entry_t *pmap_pte __P((pmap_t, vaddr_t));
void pmap_remove_all __P((paddr_t));
void pmap_changebit __P((paddr_t, int, int));
boolean_t pmap_testbit __P((paddr_t, int));
d323 1
a323 1
 * quick PTE field checking macros
d325 1
a325 7
#define	pmap_pte_w(pte)		(*(pte) & PG_W)
#define	pmap_pte_m(pte)		(*(pte) & PG_M)
#define	pmap_pte_u(pte)		(*(pte) & PG_U)
#define	pmap_pte_prot(pte)	(*(pte) & PG_PROT)

#define	pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define	pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))
d328 1
a328 1
 * Convert machine-independent protection code to M88K protection bits.
d330 1
a330 4
static __inline u_int32_t
m88k_protection(pmap_t map, vm_prot_t prot)
{
	pt_entry_t p;
d332 15
a346 21
	p = (prot & VM_PROT_WRITE) ? PG_RW : PG_RO;
	/*
	 * XXX this should not be necessary anymore now that pmap_enter
	 * does the correct thing... -- miod
	 */
#ifdef M88110
	if (cputyp == CPU_88110) {
		p |= PG_U;
		/* if the map is the kernel's map and since this 
		 * is not a paged kernel, we go ahead and mark 
		 * the page as modified to avoid an exception 
		 * upon writing to the page the first time.  XXX smurph 
		 */
		if (map == kernel_pmap) { 
			if (p & PG_PROT)
				p |= PG_M;
		}
	}
#endif 
	return (p);
} /* m88k_protection */
d360 1
a360 1
 *      kernel  TRUE if supervisor mode, FALSE if user mode
d363 1
a363 4
flush_atc_entry(users, va, kernel)
	long users;
	vaddr_t va;
	boolean_t kernel;
d365 2
a366 2
	int	cpu;
	long	tusers = users;
d383 16
d413 5
d419 1
d421 1
a421 3
pmap_pte(map, virt)
	pmap_t map;
	vaddr_t virt;
d423 1
a423 1
	sdt_entry_t	*sdt;
d438 3
a440 3

	return ((pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES))<<PDT_SHIFT) +
		PDTIDX(virt));
d467 3
d479 1
d481 1
a481 3
pmap_expand_kmap(virt, prot)
	vaddr_t virt;
	vm_prot_t prot;
d483 4
a486 2
	sdt_entry_t	template, *sdt;
	kpdt_entry_t	kpdt_ent;
d492 1
a492 1
	template = m88k_protection(kernel_pmap, prot) | SG_V;
d495 1
a495 1
	sdt = SDTENT(kernel_pmap, virt);
d505 3
a507 4
	/* physical table */
	*sdt = kpdt_ent->phys | template;
	/* virtual table */
	*(sdt + SDT_ENTRIES) = (vaddr_t)kpdt_ent | template;
d512 1
a512 1
} /* pmap_expand_kmap() */
a525 1
 *	cmode	cache control attributes
d541 1
d552 6
a557 6
vaddr_t
pmap_map(virt, start, end, prot, cmode)
	vaddr_t virt;
	paddr_t start, end;
	vm_prot_t prot;
	unsigned int cmode;
d559 1
d562 13
a574 2
	pt_entry_t	template, *pte;
	paddr_t		page;
a581 1
#ifdef DIAGNOSTIC
a583 1
#endif
d585 5
a589 1
	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;
a590 2
	page = trunc_page(start);
	npages = atop(round_page(end) - page);
d592 1
d595 1
a595 1
				panic("pmap_map: Cannot allocate pte table");
d599 1
a599 1
			if (PDT_VALID(pte))
d602 12
a613 2
		*pte = template | page;
		page += PAGE_SIZE;
d615 1
d617 3
a619 1
	return virt;
d641 1
d665 3
a667 6
vaddr_t
pmap_map_batc(virt, start, end, prot, cmode)
	vaddr_t virt;
	paddr_t start, end;
	vm_prot_t prot;
	unsigned int cmode;
d669 1
a669 1
	unsigned	npages;
d671 3
a673 2
	pt_entry_t	template, *pte;
	paddr_t		page;
d675 1
a675 1
	int		i;
a682 1
#ifdef DIAGNOSTIC
a684 4
#endif


	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;
d686 3
d691 4
a694 8
	if (template & CACHE_WT)
		batctmp.field.wt = 1;	 /* write through */
	if (template & CACHE_GLOBAL)
		batctmp.field.g = 1;     /* global */
	if (template & CACHE_INH)
		batctmp.field.ci = 1;	 /* cache inhibit */
	if (template & PG_PROT)
		batctmp.field.wp = 1; /* protection */
d697 5
a701 3
	page = trunc_page(start);
	npages = atop(round_page(end) - page);
	for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {
d704 2
a705 5
			printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, "
			    "align V=%d, page=%x, align P=%d\n",
			    curproc, num_phys_pages, virt,
			    BATC_BLK_ALIGNED(virt), page,
			    BATC_BLK_ALIGNED(page));
d708 1
a708 1
		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) && 
d715 1
a715 1
			batctmp.field.pba = M88K_BTOBLK(page);
d717 1
a717 1
			for (i = 0; i < max_cpus; i++)
d730 2
a731 2
					if (PDT_VALID(pte))
						printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, *pte);
d737 2
a738 1
			page += BATC_BLKBYTES;
d744 1
a744 1
				panic("pmap_map_batc: Cannot allocate pte table");
d748 1
a748 1
			if (PDT_VALID(pte))
d752 1
a752 1
		*pte = template | trunc_page(page);
d754 3
a756 1
		page += PAGE_SIZE;
d758 3
a760 1
	return virt;
d768 6
a773 1
 *	the specified virtual address range.
d777 2
a778 2
 *	vaddr_t		s
 *	vaddr_t		e
d782 2
d787 1
d796 1
a796 4
pmap_cache_ctrl(pmap, s, e, mode)
	pmap_t pmap;
	vaddr_t s, e;
	unsigned mode;
d800 2
a801 2
	vaddr_t		va;
	boolean_t	kflush;
d803 2
a804 1
	unsigned	users;
d816 1
a816 2
#ifdef DIAGNOSTIC
	if (pmap == PMAP_NULL)
d818 1
a818 1
#endif
d824 1
a824 1
		kflush = TRUE;
d826 1
a826 1
		kflush = FALSE;
d837 1
d840 2
a841 2
		 * the modified bit and/or the reference bit by any other cpu.
		 * XXX
d843 2
a844 1
		*pte = (invalidate_pte(pte) & CACHE_MASK) | mode;
d850 1
a850 1
		for (cpu = 0; cpu < max_cpus; cpu++)
d852 1
a852 1
				cmmu_flush_remote_cache(cpu, ptoa(PG_PFNUM(*pte)),
d854 1
d856 1
d858 1
d887 2
a888 1
 *	pmap_map or pmap_map_batc
d910 5
a914 4
pmap_bootstrap(load_start, phys_start, phys_end, virt_start, virt_end)
	vaddr_t load_start;
	paddr_t *phys_start, *phys_end;
	vaddr_t *virt_start, *virt_end;
d918 1
a918 1
	vaddr_t		vaddr,
d920 2
a921 3
			kernel_pmap_size,
			pdt_size;
	paddr_t		s_text,
d923 1
a923 1
			kpdt_phys;
d929 1
d936 1
a936 2
#ifdef DIAGNOSTIC
	if (!PAGE_ALIGNED(load_start))
d938 1
a938 1
#endif
d960 1
a960 1
	    (trunc_page((unsigned)&kernelstart) - load_start);
d973 2
a974 2
		printf("kernel_pmap->sdt_paddr = 0x%x\n",kernel_pmap->sdt_paddr);
		printf("kernel_pmap->sdt_vaddr = 0x%x\n",kernel_pmap->sdt_vaddr);
d993 1
a994 1

d996 2
a997 5
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("     kernel segment start = 0x%x\n", kernel_pmap->sdt_paddr);
		printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
		printf("       kernel segment end = 0x%x\n", ((unsigned)kernel_pmap->sdt_paddr) + kernel_pmap_size);
	}
d999 4
a1002 3
	/* init all segment descriptors to zero */
	bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);

a1004 15
	
	/* make sure page tables are page aligned!! XXX smurph */
	*phys_start = round_page(*phys_start);
	*virt_start = round_page(*virt_start);
	
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = *phys_start;
	kpdt_virt = (kpdt_entry_t)*virt_start;
	
	pdt_size = MAX_KERNEL_PDT_SIZE;
	/* might as well round up to a page - XXX smurph */
	pdt_size = round_page(pdt_size);
	kernel_pmap_size += pdt_size;
	*phys_start += pdt_size;
	*virt_start += pdt_size;
d1006 2
a1007 2
	/* init all page descriptors to zero */
	bzero((void *)kpdt_phys, pdt_size);
d1009 1
a1009 6
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("--------------------------------------\n");
		printf("        kernel page start = 0x%x\n", kpdt_phys);
		printf("   kernel page table size = 0x%x\n", pdt_size);
		printf("          kernel page end = 0x%x\n", *phys_start);
	}
d1014 3
a1016 3
		printf("kpdt_phys = 0x%x\n",kpdt_phys);
		printf("kpdt_virt = 0x%x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x, (phys)0x%08x\n",
d1024 2
a1025 2
	for (i = pdt_size / PDT_SIZE; i > 0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vaddr_t)kpdt_virt + PDT_SIZE);
d1038 1
a1038 1
	    trunc_page((unsigned)&kernelstart));
d1043 1
a1043 1
#define PMAPER	pmap_map
d1045 1
a1045 1
#define PMAPER	pmap_map_batc
d1049 5
a1053 1
	vaddr = PMAPER(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1056 11
a1066 7
	vaddr = PMAPER((vaddr_t)trunc_page(((unsigned)&kernelstart)),
	    s_text, e_text, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/

	vaddr = PMAPER(vaddr, e_text, (paddr_t)kmap,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);

d1082 5
a1086 2
	vaddr = PMAPER(vaddr, (paddr_t)kmap, *phys_start,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
a1088 5
		/* 
		 * This should never happen because we now round the PDT
		 * table size up to a page boundry in the quest to get 
		 * mc88110 working. - XXX smurph
		 */
d1091 1
a1091 1
			printf("1:vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
d1099 1
a1099 1
#if defined (MVME187) || defined (MVME197)
d1106 1
a1106 1
	if (brdtyp == BRD_187 || brdtyp == BRD_197) {
d1111 5
a1115 2
		vaddr = PMAPER(vaddr, *phys_start, *phys_start + etherlen,
		    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);
d1132 1
a1132 1
#endif /* defined (MVME187) || defined (MVME197) */
d1169 1
a1169 1
	for (; ptable->size != 0xffffffffU; ptable++){
d1174 4
a1177 3
			PMAPER(ptable->virt_start, ptable->phys_start,
			    ptable->phys_start + (ptable->size - 1),
			    ptable->prot, ptable->cacheability);
a1178 2
	}
#undef PMAPER
d1180 8
a1187 8
	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physical pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -
	 * else we will be referencing the wrong page.
	 */
d1193 1
a1193 1
    		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
d1200 2
a1201 1
	*vmpte = PG_NV;
d1222 1
a1222 1
			pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE);
d1224 1
d1251 5
d1263 2
a1264 1
			*pte = PG_NV;
d1266 2
a1267 1
			*pte = PG_NV;
d1272 1
a1272 1
				printf("Processor %d running virtual.\n", i);
d1285 1
d1300 1
d1310 1
a1310 1
 * for the pv_head_table, pmap_modify_list; and sets these
d1320 1
d1323 1
d1328 8
a1335 6
	long		npages;
	vaddr_t		addr;
	vsize_t		s;
	pv_entry_t	pv;
	char		*attr;
	int		bank;
d1342 1
a1342 1
	 * Allocate memory for the pv_head_table,
d1349 1
d1363 1
a1363 1
	pv_head_table = (pv_entry_t)addr;
d1366 6
d1375 7
a1381 1
	 * Now that the pv and attribute tables have been allocated,
d1385 1
d1391 1
d1393 1
d1397 4
a1400 3
	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", NULL);
d1419 2
d1430 1
a1430 2
pmap_zero_page(phys)
	paddr_t phys;
d1432 3
a1434 2
	vaddr_t		srcva;
	int		spl;
d1439 1
a1439 1
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu * PAGE_SIZE));
d1442 4
d1448 1
a1448 3
	*srcpte = trunc_page(phys) |
	    m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
a1449 1

d1464 3
d1472 3
a1474 4
	pmap_t			p;
	sdt_entry_t		*segdt;
	int			i;
	unsigned int		s;
d1476 1
a1476 1
	p = pool_get(&pmappool, PR_WAITOK);
d1478 4
d1483 8
d1505 1
d1513 4
a1516 3
	if (!PAGE_ALIGNED(p->sdt_paddr))
		panic("pmap_create: sdt_table 0x%x not aligned on page boundary",
		    (int)p->sdt_paddr);
d1526 1
a1526 1
	if (brdtyp == BRD_188) {
d1531 3
a1533 2
		    (vaddr_t)segdt, (vaddr_t)segdt+ (SDT_SIZE*2),
		    CACHE_INH);
d1559 7
d1574 1
a1574 2
	return (p);
} /* pmap_create() */
d1577 1
a1577 1
 * Routine:	PMAP_RELEASE
d1587 1
a1587 1
 *	uvm_km_free
d1601 1
a1601 2
pmap_release(pmap)
	pmap_t pmap;
d1610 1
a1610 1
		printf("(pmap_release :%x) pmap %x\n", curproc, pmap);
d1621 1
a1621 2
	if (j < 1024)
		j++;
d1624 1
a1624 1
	for (; i < j; i++) {
d1626 1
a1626 1
		if ((gdttbl = pmap_pte(pmap, (vaddr_t)sdt_va)) != PT_ENTRY_NULL) {
d1629 1
a1629 1
				printf("(pmap_release :%x) free page table = 0x%x\n",
d1638 1
a1638 1
		printf("(pmap_release :%x) free segment table = 0x%x\n",
d1644 2
a1645 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, 2*SDT_SIZE);
d1647 4
d1652 2
a1653 2
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
		printf("(pmap_release :%x) ref_count = 0\n", curproc);
d1655 2
a1656 2
	pmap->prev->next = pmap->next;
	pmap->next->prev = pmap->prev;
d1658 1
a1658 1
} /* pmap_release() */
d1671 4
a1674 2
 *	pmap_release
 *	pool_put
d1680 2
a1681 2
 * structure. If it goes to zero, pmap_release is called to release
 * the memory space to the system. Then, call pool_put to free the
d1685 1
a1685 2
pmap_destroy(p)
	pmap_t p;
d1687 9
a1695 1
	int count;
d1697 1
a1697 2
#ifdef DIAGNOSTIC
	if (p == kernel_pmap)
d1699 7
a1705 1
#endif
d1707 1
a1707 4
	simple_lock(&p->lock);
	count = --p->ref_count;
	simple_unlock(&p->lock);
	if (count == 0) {
d1711 1
d1724 4
a1727 1
 * Under a pmap read lock, the ref_count field of the pmap structure
d1731 1
a1731 2
pmap_reference(p)
	pmap_t p;
d1733 7
a1740 3
	simple_lock(&p->lock);
	p->ref_count++;
	simple_unlock(&p->lock);
d1761 4
d1766 7
a1772 1
 *	pool_put
d1775 1
d1793 1
d1795 1
a1795 3
pmap_remove_range(pmap, s, e)
	pmap_t pmap;
	vaddr_t s, e;
d1797 13
a1809 8
	pt_entry_t	*pte;
	pv_entry_t	prev, cur;
	pv_entry_t	pvl;
	paddr_t		pa;
	vaddr_t		va;
	unsigned	users;
	pt_entry_t	opte;
	boolean_t	kflush;
d1812 1
a1812 1
	 * Pmap has been locked by the caller.
d1816 1
a1816 1
		kflush = TRUE;
d1818 1
a1818 1
		kflush = FALSE;
d1827 2
d1830 1
d1850 4
a1853 6
		/*
		 * Update statistics.
		 */
		pmap->stats.resident_count--;
		if (pmap_pte_w(pte))
			pmap->stats.wired_count--;
d1855 2
a1856 1
		pa = ptoa(PG_PFNUM(*pte));
d1859 1
d1864 2
a1865 1
			pvl = pa_to_pvh(pa);
d1867 1
a1867 2
#ifdef DIAGNOSTIC
			if (pvl->pmap == PMAP_NULL)
d1869 1
a1869 1
#endif
d1872 1
d1888 2
a1889 3
				for (cur = pvl; cur != PV_ENTRY_NULL;
				    cur = cur->next) {
					if (cur->va == va && cur->pmap == pmap)
d1891 1
a1891 1
					prev = cur;
a1900 1
				/*pvl = pa_to_pvh(pa);*/
d1902 4
d1912 1
d1916 1
a1916 1
		 * the modified bit and/or the reference bit by any other cpu.
d1918 6
a1923 2
		opte = invalidate_pte(pte);
		flush_atc_entry(users, va, kflush);
d1925 2
a1926 4
		if (opte & PG_M) {
			if (PMAP_MANAGED(pa)) {
				/* keep track ourselves too */
				*pa_to_attribute(pa) |= PG_M;
d1928 3
d1933 8
a1940 1
	} /* end for( va = s; ...) */
d1948 1
a1948 2
 *	It is assumed that start and end are properly rounded to the VM page
 *	size.
d1951 1
a1951 3
 *	map		pointer to pmap structure
 *	s
 *	e
d1957 2
d1960 1
d1966 1
a1966 3
pmap_remove(map, s, e)
	pmap_t map;
	vaddr_t s, e;
d1968 1
a1968 1
	int spl;
d1978 2
a1979 1
#ifdef DIAGNOSTIC
a1981 1
#endif
d2004 4
d2009 1
d2011 2
a2012 1
 *	pool_put
d2028 1
a2028 2
pmap_remove_all(phys)
	paddr_t phys;
d2030 9
a2038 8
	pv_entry_t	pvl;
	pt_entry_t	*pte;
	vaddr_t		va;
	pmap_t		pmap;
	int		spl;
#ifdef DEBUG
	int		dbgcnt = 0;
#endif
a2047 1

d2057 3
a2059 1
	pvl = pa_to_pvh(phys);
d2066 2
a2067 1
		if (!simple_lock_try(&pmap->lock))
d2069 7
a2082 1
#ifdef DIAGNOSTIC
a2083 1
#ifdef DEBUG
a2085 1
#endif
d2088 32
a2119 13
#endif	/* DIAGNOSTIC */
		if (!PDT_VALID(pte)) {
			pvl = pvl->next;
			continue;	/* no page mapping */
		}
		if (pmap_pte_w(pte)) {
#ifdef DEBUG
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
				    phys);
#endif
			pvl = pvl->next;
			continue;
a2121 2
		pmap_remove_range(pmap, va, va);

a2127 1
#ifdef DEBUG
a2128 1
#endif
d2130 3
d2134 1
d2138 115
d2266 1
d2280 1
a2280 4
pmap_protect(pmap, s, e, prot)
	pmap_t pmap;
	vaddr_t s, e;
	vm_prot_t prot;
d2282 8
a2289 5
	int		spl;
	pt_entry_t	ap, *pte;
	vaddr_t		va;
	unsigned	users;
	boolean_t	kflush;
a2297 1
#ifdef DIAGNOSTIC
a2299 1
#endif
d2301 2
a2302 1
	ap = m88k_protection(pmap, prot) & PG_PROT;
d2308 1
a2308 1
		kflush = TRUE;
d2310 1
a2310 1
		kflush = FALSE;
d2342 3
a2344 1

d2348 1
a2348 1
		 * written back by any other cpu.
d2350 3
a2352 1
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
d2382 4
a2385 2
 *	uvm_km_free
 *	uvm_km_zalloc
a2389 1
 *      pmap != kernel_pmap
d2404 1
a2404 3
pmap_expand(map, v)
	pmap_t map;
	vaddr_t v;
d2407 1
a2407 2
	vaddr_t		pdt_vaddr;
	paddr_t		pdt_paddr;
d2411 1
a2411 2
#ifdef DIAGNOSTIC
	if (map == PMAP_NULL)
d2413 1
a2413 1
#endif
d2422 15
d2442 1
a2442 1
	if (brdtyp == BRD_188) {
d2444 1
a2444 1
		 * the pages for page tables should be CACHE DISABLED on MVME188
d2463 1
a2463 1
			printf("(pmap_expand :%x) table has already been allocated\n", curproc);
d2484 2
a2485 2
		*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
		*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;
d2522 1
a2522 1
 *	PT_FREE
d2561 3
a2563 6
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d2565 1
d2568 3
a2570 3
	pt_entry_t	ap, *pte;
	paddr_t		old_pa;
	pt_entry_t	template;
d2572 2
a2573 1
	unsigned	users;
d2575 1
a2575 1
	boolean_t	wired = (flags & PMAP_WIRED) != 0;
d2577 2
a2578 2
	CHECK_PAGE_ALIGN(va, "pmap_entry - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - PA");
d2587 1
a2587 1
			printf("(pmap_enter :%x) pmap kernel va %x pa %x\n", curproc, va, pa);
d2589 1
a2589 1
			printf("(pmap_enter :%x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
d2592 1
a2592 1
	ap = m88k_protection(pmap, prot);
d2602 1
a2606 1
Retry:
d2611 6
a2616 12
		if (pmap == kernel_pmap) {
			if (pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE) ==
			    PT_ENTRY_NULL)
				panic("pmap_expand: Cannot allocate kernel pte table");
		} else {
			/*
			 * Must unlock to expand the pmap.
			 */
			PMAP_UNLOCK(pmap, spl);
			pmap_expand(pmap, va);
			PMAP_LOCK(pmap, spl);
		}
d2622 1
a2622 1
	old_pa = ptoa(PG_PFNUM(*pte));
d2625 1
a2625 1
			kflush = TRUE;
d2627 1
a2627 1
			kflush = FALSE;
d2634 1
a2634 1
		if (wired && !(pmap_pte_w(pte)))
d2636 1
a2636 1
		else if (!wired && pmap_pte_w(pte))
d2640 1
a2640 1
			template = CACHE_INH | PG_V;
d2642 1
a2642 1
			template = CACHE_GLOBAL | PG_V;
d2644 1
a2644 1
			template |= PG_W;
d2649 2
a2650 2
		if (!PDT_VALID(pte) || pmap_pte_w_chg(pte, template & PG_W) ||
		    (pmap_pte_prot_chg(pte, ap & PG_PROT))) {
d2653 2
a2654 3
			 * Invalidate pte temporarily to avoid being written
			 * back the modified bit and/or the reference bit by
			 * any other cpu.
d2656 3
a2658 2
			template |= (invalidate_pte(pte) & PG_M);
			*pte++ = template | ap | trunc_page(pa);
d2666 1
a2666 1
		if (old_pa != (paddr_t)-1) {
d2678 1
a2678 1
					       pte, PG_PFNUM(*pte), PDT_VALID(pte));
d2683 1
a2683 1
				flush_atc_entry(users, va, TRUE);
d2701 3
a2703 1
			pvl = pa_to_pvh(pa);
d2731 2
d2741 1
a2741 1
				 * Remember that we used the pvlist entry.
d2745 1
d2756 1
a2756 1
			template = CACHE_INH | PG_V;
d2758 2
a2759 1
			template = CACHE_GLOBAL | PG_V;
d2761 1
a2761 6
			template |= PG_W;

		if (flags & VM_PROT_WRITE)
			template |= PG_U | PG_M;
		else if (flags & VM_PROT_ALL)
			template |= PG_U;
d2763 1
a2763 1
		*pte = template | ap | trunc_page(pa);
d2784 3
d2788 1
d2790 1
d2796 1
a2796 3
pmap_unwire(map, v)
	pmap_t map;
	vaddr_t v;
d2804 1
a2804 1
		panic("pmap_unwire: pte missing");
d2806 1
a2806 1
	if (pmap_pte_w(pte)) {
d2809 1
a2809 1
		*pte &= ~PG_W;
d2813 1
d2843 1
a2843 4
pmap_extract(pmap, va, pap)
	pmap_t pmap;
	vaddr_t va;
	paddr_t *pap;
d2845 4
a2848 5
	pt_entry_t	*pte;
	paddr_t		pa;
	int		i;
	int		spl;
	boolean_t	rv = FALSE;
a2849 1
#ifdef DIAGNOSTIC
a2851 1
#endif
d2857 1
a2857 1
		for (i = batc_used - 1; i > 0; i--)
d2859 2
a2860 3
				if (pap != NULL)
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) | 
						(va & BATC_BLKMASK);
d2866 7
a2872 9
	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL) {
		if (PDT_VALID(pte)) {
			rv = TRUE;
			if (pap != NULL) {
				pa = ptoa(PG_PFNUM(*pte));
				pa |= (va & PAGE_MASK); /* offset within page */
				*pap = pa;
			}
		}
d2875 3
d2879 4
a2882 1
	return (rv);
d2906 2
a2907 4
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t dst_pmap, src_pmap;
	vaddr_t dst_addr, src_addr;
	vsize_t len;
d2909 1
d2925 3
d2952 1
a2952 2
pmap_collect(pmap)
	pmap_t pmap;
d2954 10
a2963 10
	vaddr_t		sdt_va;		/* outer loop index */
	vaddr_t		sdt_vt;		/* end of segment */
	sdt_entry_t	*sdttbl;	/* ptr to first entry in seg table */
	sdt_entry_t	*sdtp;		/* ptr to index into segment table */
	sdt_entry_t	*sdt;		/* ptr to index into segment table */
	pt_entry_t	*gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t	*gdttblend;	/* ptr to byte after last entry in
					   table group */
	pt_entry_t	*gdtp;		/* ptr to index into a page table */
	boolean_t	found_gdt_wired; /* flag indicating a wired page exists 
d2965 4
a2968 2
	int		spl;
	unsigned int	i, j;
d2970 1
a2970 2
#ifdef DIAGNOSTIC
	if (pmap == PMAP_NULL)
d2972 5
a2976 2

	if (pmap == kernel_pmap)
d2979 3
d2998 1
d3001 1
a3001 2
	if (j < 1024)
		j++;
d3004 1
a3004 1
	for (; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE) {
d3007 1
a3007 1
		gdttbl = pmap_pte(pmap, sdt_va);
d3016 2
a3017 2
		for (gdtp = gdttbl; gdtp < gdttblend; gdtp++) {
			if (pmap_pte_w(gdtp)) {
d3033 1
a3033 1
		pmap_remove_range(pmap, sdt_va, sdt_vt);
d3038 3
a3040 3
		for (sdt = sdtp; sdt < (sdtp + PDT_TABLE_GROUP_SIZE); sdt++) {
			*((sdt_entry_t *) sdt) = 0;
			*((sdt_entry_t *)(sdt+SDT_ENTRIES)) = 0;
d3060 2
d3088 1
a3088 2
pmap_activate(p)
	struct proc *p;
d3090 1
a3090 1
	apr_template_t	apr_data;
d3093 1
a3093 1
	int		n;
d3096 2
a3097 2
	pmap_t		pmap = p->p_vmspace->vm_map.pmap;
	int		cpu = cpu_number();  
d3117 1
a3117 1
#ifdef OMRON_PMAP
d3128 1
a3128 1
#else
d3131 2
a3132 2
#endif
#else
a3138 1
#endif /* notyet */
d3141 1
a3141 1
		 * Mark that this cpu is using the pmap.
d3147 1
d3174 1
a3174 2
pmap_deactivate(p)
	struct proc *p;
d3176 2
a3177 2
	pmap_t	pmap = p->p_vmspace->vm_map.pmap;
	int	cpu = cpu_number();  
d3180 1
d3206 2
d3217 1
a3217 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d3219 6
a3224 7
	vaddr_t		dstva, srcva;
	int		spl;
	pt_entry_t	template, *dstpte, *srcpte;
	int		cpu = cpu_number();

	template = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
d3229 4
a3232 2
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	dstva = (vaddr_t)(phys_map_vaddr2 + (cpu << PAGE_SHIFT));
d3237 4
d3243 1
a3243 1
	*srcpte = template | trunc_page(src);
d3248 2
d3251 1
a3251 1
	*dstpte  = template | trunc_page(dst);
d3258 1
d3262 1
a3262 1
 *	Routine:	PMAP_CHANGEBIT
d3265 1
a3265 1
 *		Update the pte bits on the specified physical page.
a3268 2
 *		set	bits to set
 *		mask	bits to mask
d3275 5
a3279 1
 *		pa_to_pvh
d3281 1
d3283 5
a3287 5
 * The pte bits corresponding to the page's frame index will be changed as
 * requested. The PV list will be traversed.
 * For each pmap/va the hardware the necessary bits in the page descriptor
 * table entry will be altered as well if necessary. If any bits were changed,
 * a TLB flush will be performed.
d3289 2
a3290 4
void
pmap_changebit(pg, set, mask)
	paddr_t pg;
	int set, mask;
d3292 13
a3304 8
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*pte, npte;
	pmap_t		pmap;
	int		spl;
	vaddr_t		va;
	unsigned	users;
	boolean_t	kflush;
d3307 3
a3309 2
	if (!PMAP_MANAGED(pg))
		panic("pmap_changebit: not managed?");
d3314 4
a3317 2
changebit_Retry:
	pvl = pa_to_pvh(pg);
d3319 2
a3320 5
	/*
	 * Clear saved attributes (modify, reference)
	 */
	/* *pa_to_attribute(pg) |= set; */
	*pa_to_attribute(pg) &= mask;
d3324 2
a3325 3
		if ((pmap_con_dbg & (CD_CBIT | CD_NORM)) == (CD_CBIT | CD_NORM))
			printf("(pmap_changebit :%x) phys addr 0x%x not mapped\n",
			    curproc, pg);
d3327 1
d3329 1
a3329 1
		return;
d3337 2
a3338 1
			goto changebit_Retry;
d3342 1
a3342 1
			kflush = TRUE;
d3344 1
a3344 1
			kflush = FALSE;
a3347 5

#ifdef DIAGNOSTIC
		/*
		 * Check for existing and valid pte
		 */
d3349 1
a3349 6
			panic("pmap_changebit: bad pv list entry.");
		if (!PDT_VALID(pte))
			panic("pmap_changebit: invalid pte");
		if (ptoa(PG_PFNUM(*pte)) != pg)
			panic("pmap_changebit: pte doesn't point to page");
#endif
a3351 8
		 * Update bits
		 */
		*pte = invalidate_pte(pte);
		npte = (*pte | set) & mask;

		/*
		 * Flush TLB of which cpus using pmap.
		 *
d3353 1
a3353 1
		 * and/or the reference being written back by any other cpu.
d3355 5
a3359 4
		if (npte != *pte) {
			*pte = npte;
			flush_atc_entry(users, va, kflush);
		}
d3363 1
a3364 14
} /* pmap_changebit() */

/*
 *	Routine:	PMAP_CLEAR_MODIFY
 *
 *	Function:
 *		Clears the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;
d3366 2
a3367 4
	rv = pmap_testbit(pa, PG_M);
	pmap_changebit(pa, 0, ~PG_M);
	return rv;
}
d3370 1
a3370 1
 *	Routine:	PMAP_TESTBIT
d3373 3
a3375 1
 *		Test the modified/referenced bits of a physical page.
a3378 1
 *		bit		bit to test
d3388 1
a3388 1
 *		pa_to_pvh
d3391 4
a3394 2
 *	If the attribute list for the given page has the bit, this routine
 * returns TRUE.
d3398 2
a3399 3
 * examined. If the selected bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list), and updates the
 * attribute list.
d3402 1
a3402 3
pmap_testbit(pg, bit)
	paddr_t pg;
	int bit;
d3404 6
a3409 5
	pv_entry_t	pvl;
	pv_entry_t	pvep;
	pt_entry_t	*ptep;
	int		spl;
	boolean_t	rv;
d3412 3
a3414 2
	if (!PMAP_MANAGED(pg))
		panic("pmap_testbit: not managed?");
d3419 3
a3421 2
	pvl = pa_to_pvh(pg);
testbit_Retry:
d3423 2
a3424 2
	if (*pa_to_attribute(pg) & bit) {
		/* we've already cached a this flag for this page,
d3427 2
a3428 3
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
			printf("(pmap_testbit :%x) already cached a modify flag for this page\n",
			    curproc);
d3433 1
d3436 6
a3441 7
		/* unmapped page - get info from attribute array
		   maintained by pmap_remove_range/pmap_remove_all */
		rv = (boolean_t)(*pa_to_attribute(pg) & bit);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
			printf("(pmap_testbit :%x) phys addr 0x%x not mapped\n",
			    curproc, pg);
d3443 1
d3445 1
a3445 1
		return (rv);
d3452 2
a3453 1
			goto testbit_Retry;
d3458 2
a3459 2
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->va);
			panic("pmap_testbit: bad pv list entry");
d3461 1
a3461 1
		if (*ptep & bit) {
a3462 1
			*pa_to_attribute(pg) |= bit;
d3464 2
a3465 2
			if ((pmap_con_dbg & (CD_TBIT | CD_FULL)) == (CD_TBIT | CD_FULL))
				printf("(pmap_testbit :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d3467 1
d3475 1
d3478 2
a3479 1
} /* pmap_testbit() */
d3482 1
a3482 1
 *	Routine:	PMAP_IS_MODIFIED
d3485 20
a3504 2
 *		Return whether or not the specified physical page is modified
 *		by any physical maps.
d3507 1
a3507 2
pmap_is_modified(pg)
	struct vm_page *pg;
d3509 11
a3519 1
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
d3521 1
a3521 2
	return (pmap_testbit(phys, PG_M));
} /* pmap_is_modified() */
d3523 6
a3528 12
/*
 *	Routine:	PMAP_CLEAR_REFERENCE
 *
 *	Function:
 *		Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(pg)
	struct vm_page *pg;
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;
d3530 1
a3530 4
	rv = pmap_testbit(pa, PG_U);
	pmap_changebit(pa, 0, ~PG_U);
	return rv;
}
d3532 4
a3535 12
/*
 * Routine:	PMAP_IS_REFERENCED
 *
 * Function:
 *	Return whether or not the specified physical page is referenced by
 *	any physical maps.
 */
boolean_t
pmap_is_referenced(pg)
	struct vm_page *pg;
{
	paddr_t		phys = VM_PAGE_TO_PHYS(pg);
d3537 128
a3664 2
	return (pmap_testbit(phys, PG_U));
}
d3670 1
a3670 1
 *	pmap_changebit
d3676 1
a3676 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
d3683 1
a3683 2
		/* copy on write */
		pmap_changebit(phys, PG_RO, ~0);
d3694 103
d3798 1
a3798 2
pmap_virtual_space(startp, endp)
	vaddr_t *startp, *endp;
d3800 49
a3848 2
	*startp = virtual_avail;
	*endp = virtual_end;
d3851 38
d3890 1
a3890 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d3892 34
a3925 3
	int		spl;
	pt_entry_t	template, *pte;
	unsigned	users;
d3927 4
a3930 2
	CHECK_PAGE_ALIGN (va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN (pa, "pmap_kenter_pa - PA");
d3932 1
a3932 3
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
		printf ("(pmap_kenter_pa :%x) va %x pa %x\n", curproc, va, pa);
a3933 1
#endif
d3935 31
a3965 2
	PMAP_LOCK(kernel_pmap, spl);
	users = kernel_pmap->cpus_using;
d3967 14
a3980 1
	template = m88k_protection(kernel_pmap, prot);
d3983 2
a3984 1
	 * Expand pmap to include this pte.
d3986 239
a4224 3
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL) {
		if (pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE) == PT_ENTRY_NULL)
			panic("pmap_kenter_pa: Cannot allocate kernel pte table");
d4226 32
d4259 16
a4274 5
	/*
	 * And count the mapping.
	 */
	kernel_pmap->stats.resident_count++;
	kernel_pmap->stats.wired_count++;
d4276 7
a4282 7
	invalidate_pte(pte);
	if ((unsigned long)pa >= MAXPHYSMEM)
		template |= CACHE_INH | PG_V | PG_W;
	else
		template |= CACHE_GLOBAL | PG_V | PG_W;
	*pte = template | trunc_page(pa);
	flush_atc_entry(users, va, TRUE);
d4284 7
a4290 1
	PMAP_UNLOCK(kernel_pmap, spl);
d4293 4
d4298 1
a4298 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d4300 33
a4332 2
	int		spl;
	unsigned	users;
d4334 5
a4338 4
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
		printf("(pmap_kremove :%x) va %x len %x\n", curproc, va, len);
#endif
d4340 1
a4340 2
	CHECK_PAGE_ALIGN(va, "pmap_kremove addr");
	CHECK_PAGE_ALIGN(len, "pmap_kremove len");
d4342 3
a4344 2
	PMAP_LOCK(kernel_pmap, spl);
	users = kernel_pmap->cpus_using;
d4346 8
a4353 4
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		vaddr_t e = va + PAGE_SIZE;
		sdt_entry_t *sdt;
		pt_entry_t *pte;
d4355 1
a4355 1
		sdt = SDTENT(kernel_pmap, va);
d4357 18
a4374 7
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
			continue;
d4376 9
d4386 10
a4395 5
		/*
		 * Update the counts
		 */
		kernel_pmap->stats.resident_count--;
		kernel_pmap->stats.wired_count--;
d4397 5
a4401 3
		pte = pmap_pte(kernel_pmap, va);
		invalidate_pte(pte);
		flush_atc_entry(users, va, TRUE);
a4402 1
	PMAP_UNLOCK(map, spl);
@


1.12.4.8
log
@Merge in -current from about a week ago
@
text
@d251 11
a261 11
void flush_atc_entry(long, vaddr_t, boolean_t);
pt_entry_t *pmap_expand_kmap(vaddr_t, vm_prot_t);
void pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
void pmap_expand(pmap_t, vaddr_t);
void pmap_release(pmap_t);
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int);
vaddr_t pmap_map_batc(vaddr_t, paddr_t, paddr_t, vm_prot_t, unsigned int);
pt_entry_t *pmap_pte(pmap_t, vaddr_t);
void pmap_remove_all(paddr_t);
void pmap_changebit(paddr_t, int, int);
boolean_t pmap_testbit(paddr_t, int);
d1331 1
a1331 1
	bzero((void *)srcva, PAGE_SIZE);
d2885 1
a2885 1
	bcopy((void *)srcva, (void *)dstva, PAGE_SIZE);
@


1.12.4.9
log
@Sync the SMP branch with 3.3
@
text
@d3 1
a3 1
 * Copyright (c) 2001, 2002, 2003 Miodrag Vallat
d48 3
d76 1
a76 1
 * Macros to operate pm_cpus field
d96 18
a113 17
#define CD_CACHE	0x0000020	/* pmap_cache_ctrl */
#define CD_BOOT		0x0000040	/* pmap_bootstrap */
#define CD_INIT		0x0000080	/* pmap_init */
#define CD_CREAT	0x0000100	/* pmap_create */
#define CD_FREE		0x0000200	/* pmap_release */
#define CD_DESTR	0x0000400	/* pmap_destroy */
#define CD_RM		0x0000800	/* pmap_remove */
#define CD_RMAL		0x0001000	/* pmap_remove_all */
#define CD_PROT		0x0002000	/* pmap_protect */
#define CD_EXP		0x0004000	/* pmap_expand */
#define CD_ENT		0x0008000	/* pmap_enter */
#define CD_UPD		0x0010000	/* pmap_update */
#define CD_COL		0x0020000	/* pmap_collect */
#define CD_CBIT		0x0040000	/* pmap_changebit */
#define CD_TBIT		0x0080000	/* pmap_testbit */
#define CD_CREF		0x0100000	/* pmap_clear_reference */
#define CD_PGMV		0x0200000	/* pagemove */
d151 1
a151 1
#if defined(MVME188)
d155 1
a155 1
#endif
d157 1
a157 1
#if defined(MVME187) || defined(MVME197)
d161 1
a161 1
#endif
a162 1
#if defined(MVME188) && defined(MVME187) || defined(MVME197)
a163 4
#else
#define	OBIO_PDT_SIZE	MAX(M188_PDT_SIZE, M1x7_PDT_SIZE)
#endif

d172 9
d183 4
a186 1
static pv_entry_t pg_to_pvh(struct vm_page *);
d188 14
a201 5
static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}
d217 1
a217 1
		simple_lock(&(pmap)->pm_lock); \
d221 1
a221 1
		simple_unlock(&(pmap)->pm_lock); \
d225 2
d228 2
a229 2
void *etherbuf = NULL;
int etherlen;
d231 1
a231 1
#ifdef	PMAP_USE_BATC
d236 1
a236 1
int batc_used;
a242 2
#endif	/* PMAP_USE_BATC */

d256 2
a257 1
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, u_int);
d259 3
a261 3
void pmap_remove_all(struct vm_page *);
void pmap_changebit(struct vm_page *, int, int);
boolean_t pmap_testbit(struct vm_page *, int);
d278 1
a278 1
m88k_protection(pmap_t pmap, vm_prot_t prot)
d290 4
a293 4
		/* if the map is the kernel's map and since this
		 * is not a paged kernel, we go ahead and mark
		 * the page as modified to avoid an exception
		 * upon writing to the page the first time.  XXX smurph
d295 1
a295 1
		if (pmap == kernel_pmap) {
d300 3
a302 3
#endif
	return p;
}
d318 5
a322 2
void
flush_atc_entry(long users, vaddr_t va, boolean_t kernel)
d324 2
a325 2
	int cpu;
	long tusers = users;
d332 1
a332 2
#endif

d358 3
a360 1
pmap_pte(pmap_t pmap, vaddr_t virt)
d362 1
a362 1
	sdt_entry_t *sdt;
d367 1
a367 1
	if (pmap == PMAP_NULL)
d371 1
a371 1
	sdt = SDTENT(pmap,virt);
d378 4
a381 3
	return (pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES)) << PDT_SHIFT) +
		PDTIDX(virt);
}
d416 3
a418 1
pmap_expand_kmap(vaddr_t virt, vm_prot_t prot)
d420 2
a421 2
	sdt_entry_t template, *sdt;
	kpdt_entry_t kpdt_ent;
d425 1
a425 1
		printf("(pmap_expand_kmap: %x) v %x\n", curproc,virt);
a426 1

d429 1
a429 1
	/* segment table entry derivate from map and virt. */
d435 4
a438 3
	if (kpdt_ent == KPDT_ENTRY_NULL)
		panic("pmap_expand_kmap: Ran out of kernel pte tables");

d444 2
a445 2
	kpdt_ent->phys = (paddr_t)0;
	kpdt_ent->next = NULL;
d447 2
a448 2
	return (pt_entry_t *)(kpdt_ent) + PDTIDX(virt);
}
d489 10
a498 10
pmap_map(vaddr_t virt, paddr_t start, paddr_t end, vm_prot_t prot, u_int cmode)
{
	u_int npages;
	u_int num_phys_pages;
	pt_entry_t template, *pte;
	paddr_t	 page;
#ifdef	PMAP_USE_BATC
	batc_template_t	batctmp;
	int i;
#endif
d502 1
a502 1
		printf ("(pmap_map: %x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
d513 89
a601 1
#ifdef	PMAP_USE_BATC
a612 1
#endif
d616 1
a616 3
	for (num_phys_pages = npages; num_phys_pages != 0; num_phys_pages--) {
#ifdef	PMAP_USE_BATC

d618 2
a619 2
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			printf("(pmap_map: %x) num_phys_pg=%x, virt=%x, "
d626 1
a626 1
		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) &&
d635 1
a635 1
			for (i = 0; i < MAX_CPUS; i++)
d637 1
a637 1
					cmmu_set_pair_batc_entry(i, batc_used,
d641 2
a642 2
			if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM)) {
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
d644 4
a647 3
			if (pmap_con_dbg & CD_MAP)
				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE) {
					pte = pmap_pte(kernel_pmap, virt + i);
d649 1
a649 2
						printf("(pmap_map: %x) va %x is already mapped: pte %x\n",
						    curproc, virt + i, *pte);
d651 1
a658 2
#endif	/* PMAP_USE_BATC */
	
d660 2
a661 2
			pte = pmap_expand_kmap(virt,
			    VM_PROT_READ | VM_PROT_WRITE);
d663 2
a664 2
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
d666 1
a666 1
				printf("(pmap_map: %x) pte @@ 0x%p already valid\n", curproc, pte);
d669 1
a669 1
		*pte = template | page;
d674 1
a674 1
}
d684 1
a684 1
 *	pmap_t		pmap
d687 1
a687 1
 *	u_int		mode
d701 4
a704 1
pmap_cache_ctrl(pmap_t pmap, vaddr_t s, vaddr_t e, u_int mode)
d706 6
a711 6
	int spl;
	pt_entry_t *pte;
	vaddr_t va, pteva;
	boolean_t kflush;
	int cpu;
	u_int users;
d719 1
a719 1
		printf("(pmap_cache_ctrl: %x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
d730 1
a730 1
	users = pmap->pm_cpus;
d742 1
a742 1
			printf("(cache_ctrl) pte@@0x%p\n", pte);
d756 1
a756 2
		pteva = ptoa(PG_PFNUM(*pte));
		for (cpu = 0; cpu < MAX_CPUS; cpu++)
d758 2
a759 1
				cmmu_flush_remote_cache(cpu, pteva, PAGE_SIZE);
d762 1
a762 1
}
d790 1
a790 1
 *	pmap_map
d805 1
a805 1
 * For m88k, we have to map BUG memory also. This is a read only
d812 19
a830 12
pmap_bootstrap(vaddr_t load_start, paddr_t *phys_start, paddr_t *phys_end,
    vaddr_t *virt_start, vaddr_t *virt_end)
{
	kpdt_entry_t kpdt_virt;
	sdt_entry_t *kmap;
	vaddr_t vaddr, virt, kernel_pmap_size, pdt_size;
	paddr_t s_text, e_text, kpdt_phys;
	apr_template_t apr_data;
	pt_entry_t *pte;
	int i;
	pmap_table_t ptable;
	extern void *kernelstart, *etext;
d832 1
a832 1
#ifdef DEBUG
d834 1
a834 1
		printf("pmap_bootstrap: \"load_start\" 0x%x\n", load_start);
d839 1
a839 1
		panic("pmap_bootstrap: \"load_start\" not on the m88k page boundary: 0x%x", load_start);
d842 1
a842 1
	simple_lock_init(&kernel_pmap->pm_lock);
d846 2
a847 1
	 * physical memory, i.e. just after where the kernel image was loaded.
d850 1
a850 1
	 * The calling sequence is
d852 1
a852 1
	 *  pmap_bootstrap(&kernelstart,...);
d862 1
a862 1
	    (trunc_page((vaddr_t)&kernelstart) - load_start);
d867 4
a870 4
	kernel_pmap->pm_count = 1;
	kernel_pmap->pm_cpus = 0;
	kernel_pmap->pm_stpa = kmap = (sdt_entry_t *)(*phys_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)(*virt_start);
d875 2
a876 2
		printf("kernel_pmap->pm_stpa = 0x%x\n",kernel_pmap->pm_stpa);
		printf("kernel_pmap->pm_stab = 0x%x\n",kernel_pmap->pm_stab);
d878 3
d883 1
a883 1
	/*
d895 1
a895 1
	kernel_pmap_size = 2 * SDT_SIZE;
d899 1
a899 1
		printf("     kernel segment start = 0x%x\n", kernel_pmap->pm_stpa);
d901 1
a901 1
		printf("       kernel segment end = 0x%x\n", ((paddr_t)kernel_pmap->pm_stpa) + kernel_pmap_size);
d903 1
a903 1
#endif
d905 1
a905 1
	bzero(kernel_pmap->pm_stab, kernel_pmap_size);
d918 1
d920 1
a920 1
	pdt_size = round_page(MAX_KERNEL_PDT_SIZE);
d934 1
a934 1
#endif
d948 1
a948 1
	for (i = pdt_size / PDT_SIZE; i != 0; i--) {
d961 2
a962 2
	e_text = load_start +
	    ((vaddr_t)&etext - trunc_page((vaddr_t)&kernelstart));
d966 8
a973 3
	/* map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
	vaddr = pmap_map(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_INH);
d975 2
a976 2
	/* map the kernel text read only */
	vaddr = pmap_map(trunc_page((vaddr_t)&kernelstart),
d980 1
a980 1
	vaddr = pmap_map(vaddr, e_text, (paddr_t)kmap,
d998 1
a998 1
	vaddr = pmap_map(vaddr, (paddr_t)kmap, *phys_start,
d1002 1
a1002 1
		/*
d1004 1
a1004 1
		 * table size up to a page boundry in the quest to get
d1009 1
a1009 1
			printf("1: vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
d1029 1
a1029 1
		vaddr = pmap_map(vaddr, *phys_start, *phys_start + etherlen,
d1033 1
a1033 1
		*phys_start += etherlen;
d1038 1
a1038 1
				printf("2: vaddr %x *virt_start %x *phys_start %x\n", vaddr,
d1077 1
a1077 1
	ptable = pmap_table_build(0);
d1084 1
a1084 1
	for (; ptable->size != (size_t)(-1); ptable++){
d1089 1
a1089 1
			pmap_map(ptable->virt_start, ptable->phys_start,
d1094 1
d1144 1
a1144 1
	apr_data.field.st_base = atop(kernel_pmap->pm_stpa);
d1153 1
a1153 1
#endif
d1167 2
a1168 2
			 * Set valid bit to DT_INVALID so that the very first
			 * pmap_enter() on these won't barf in
d1182 1
a1182 1
			SETBIT_CPUSET(i, &kernel_pmap->pm_cpus);
d1190 1
a1190 1
}
d1205 2
d1209 1
d1212 1
a1212 1
 *	pool_init
d1214 13
a1226 2
 *   This routine does not really have much to do. It initializes
 * pools for pmap structures and pv_entry structures.
d1231 7
d1242 39
d1285 2
d1296 1
a1296 1
 *	pg		page to zero
d1312 2
a1313 1
pmap_zero_page(struct vm_page *pg)
d1315 4
a1318 5
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	vaddr_t srcva;
	int spl;
	int cpu;
	pt_entry_t *srcpte;
d1325 2
a1326 2
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
	*srcpte = trunc_page(pa) |
d1333 2
a1334 2
	cmmu_flush_remote_data_cache(cpu, pa, PAGE_SIZE);
}
d1350 4
a1353 6
	pmap_t pmap;
	sdt_entry_t *segdt;
	u_int s;
#ifdef	PMAP_USE_BATC
	int i;
#endif
d1355 2
a1356 2
	pmap = pool_get(&pmappool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));
d1364 1
a1364 1
		printf("(pmap_create: %x) need %d pages for sdt\n",
d1376 2
a1377 4
	pmap->pm_stab = segdt;
	if (pmap_extract(kernel_pmap, (vaddr_t)segdt,
	    (paddr_t *)&pmap->pm_stpa) == FALSE)
		panic("pmap_create: pmap_extract failed!");
d1379 1
a1379 1
	if (!PAGE_ALIGNED(pmap->pm_stpa))
d1381 1
a1381 1
		    (int)pmap->pm_stpa);
d1385 2
a1386 2
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x, pm_stpa=0x%x\n",
		       curproc, pmap, pmap->pm_stab, pmap->pm_stpa);
d1411 3
a1413 3
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
	pmap->pm_cpus = 0;
d1415 1
a1415 1
#ifdef	PMAP_USE_BATC
d1418 2
a1419 2
		pmap->pm_ibatc[i].bits = 0;
		pmap->pm_dbatc[i].bits = 0;
d1423 10
a1432 2
	return pmap;
}
d1450 1
a1450 1
 * 	pm_count field of the pmap structure goes to zero.
d1459 2
a1460 1
pmap_release(pmap_t pmap)
d1462 4
a1465 4
	unsigned long sdt_va;	/* outer loop index */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	u_int i, j;
d1469 1
a1469 1
		printf("(pmap_release: %x) pmap %x\n", curproc, pmap);
d1472 4
a1475 4
	sdttbl = pmap->pm_stab;    /* addr of segment table */
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
d1489 1
a1489 1
				printf("(pmap_release: %x) free page table = 0x%x\n",
d1498 1
a1498 1
		printf("(pmap_release: %x) free segment table = 0x%x\n",
d1504 1
a1504 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2*SDT_SIZE));
d1508 4
a1511 1
		printf("(pmap_release: %x) pm_count = 0\n", curproc);
d1513 1
a1513 1
}
d1538 2
a1539 1
pmap_destroy(pmap_t pmap)
d1544 1
a1544 1
	if (pmap == kernel_pmap)
d1548 3
a1550 3
	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
d1552 2
a1553 2
		pmap_release(pmap);
		pool_put(&pmappool, pmap);
d1555 1
a1555 1
}
d1567 1
a1567 1
 * Under a pmap read lock, the pm_count field of the pmap structure
d1571 2
a1572 1
pmap_reference(pmap_t pmap)
d1575 4
a1578 4
	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}
d1591 1
a1591 1
 *	e		virtual address of end of range to remove
d1595 1
d1614 2
a1615 2
 * entry in the PV list entry. Next, the function must find the PV
 * list entry associated with this pmap/va (if it doesn't exist - the function
d1620 3
a1622 1
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
d1624 8
a1631 7
	pt_entry_t *pte, opte;
	pv_entry_t prev, cur, pvl;
	struct vm_page *pg;
	paddr_t pa;
	vaddr_t va;
	u_int users;
	boolean_t kflush;
d1634 1
a1634 1
	 * pmap has been locked by the caller.
d1636 1
a1636 1
	users = pmap->pm_cpus;
d1663 1
a1663 1
		pte = pmap_pte(pmap, va);
d1672 1
a1672 1
		pmap->pm_stats.resident_count--;
d1674 1
a1674 1
			pmap->pm_stats.wired_count--;
a1676 1
		pg = PHYS_TO_VM_PAGE(pa);
d1678 1
a1678 1
		if (pg != NULL) {
d1683 1
a1683 1
			pvl = pg_to_pvh(pg);
d1686 1
a1686 1
			if (pvl->pv_pmap == PMAP_NULL)
d1690 1
a1690 1
			if (pvl->pv_va == va && pvl->pv_pmap == pmap) {
d1696 1
a1696 1
				cur = pvl->pv_next;
d1701 1
a1701 1
					pvl->pv_pmap =  PMAP_NULL;
d1707 2
a1708 2
				    cur = cur->pv_next) {
					if (cur->pv_va == va && cur->pv_pmap == pmap)
d1714 1
a1714 1
					       "0x%x (pa 0x%x) PV list at 0x%p\n", va, pa, pvl);
d1718 1
a1718 1
				prev->pv_next = cur->pv_next;
d1720 1
d1722 1
a1722 1
		} /* if (pg != NULL) */
d1737 1
a1737 1
			if (pg != NULL) {
d1739 1
a1739 1
				pvl->pv_flags |= PG_M;
d1743 2
a1744 2
	} /* for (va = s; ...) */
}
d1755 1
a1755 1
 *	pmap		pointer to pmap structure
d1769 3
a1771 1
pmap_remove(pmap_t pmap, vaddr_t s, vaddr_t e)
d1775 1
a1775 1
	if (pmap == PMAP_NULL)
d1777 1
a1777 1

d1780 1
a1780 1
		printf("(pmap_remove: %x) map %x s %x e %x\n", curproc, pmap, s, e);
d1784 1
a1784 1
	if (s >= e)
d1788 4
a1791 4
	PMAP_LOCK(pmap, spl);
	pmap_remove_range(pmap, s, e);
	PMAP_UNLOCK(pmap, spl);
}
d1801 1
a1801 1
 *	pg		physical pages which is to
d1805 2
a1806 1
 *	pv lists
d1819 1
a1819 2
 * corresponding bit in the PV list entry corresponding
 * to the physical page is set to 1.
d1827 2
a1828 1
pmap_remove_all(struct vm_page *pg)
d1830 5
a1834 5
	pt_entry_t *pte;
	pv_entry_t pvl;
	vaddr_t va;
	pmap_t pmap;
	int spl;
d1836 1
a1836 1
	int dbgcnt = 0;
d1839 1
a1839 1
	if (pg == NULL) {
d1843 1
a1843 1
			printf("(pmap_remove_all: %x) vm page 0x%x not a managed page\n", curproc, pg);
d1851 3
a1853 2
	 * We don't have to lock the pv list, since we have the entire pmap
	 * system.
d1857 1
a1857 1
	pvl = pg_to_pvh(pg);
d1862 3
a1864 3
	while ((pmap = pvl->pv_pmap) != PMAP_NULL) {
		va = pvl->pv_va;
		if (!simple_lock_try(&pmap->pm_lock))
d1876 2
a1877 2
			printf("(pmap_remove_all: %p) vm page %p pmap %x va %x dbgcnt %x\n",
			       curproc, pg, pmap, va, dbgcnt);
d1883 2
a1884 2
			pvl = pvl->pv_next;
			goto next;	/* no page mapping */
d1890 1
a1890 1
				    pg);
d1892 2
a1893 2
			pvl = pvl->pv_next;
			goto next;
d1896 1
a1896 1
		pmap_remove_range(pmap, va, va + PAGE_SIZE);
a1897 3
#ifdef DEBUG
		dbgcnt++;
#endif
d1902 5
a1906 2
next:
		simple_unlock(&pmap->pm_lock);
d1909 1
a1909 1
}
d1938 4
a1941 1
pmap_protect(pmap_t pmap, vaddr_t s, vaddr_t e, vm_prot_t prot)
d1943 5
a1947 5
	int spl;
	pt_entry_t *pte, ap;
	vaddr_t va;
	u_int users;
	boolean_t kflush;
d1965 1
a1965 1
	users = pmap->pm_cpus;
d1976 2
d1979 1
a1979 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1983 3
a1985 5
			if (va <= e - (1 << SDT_SHIFT)) {
				/* no page table, skip to next seg entry */
				va += (1 << SDT_SHIFT) - PAGE_SIZE;
			} else {
				/* wrap around */
a1986 1
			}
d1989 1
a1989 1
				printf("(pmap_protect: %x) no page table: skip to 0x%x\n", curproc, va + PAGE_SIZE);
d1997 1
a1997 1
				printf("(pmap_protect: %x) pte invalid pte @@ 0x%x\n", curproc, pte);
d2003 2
a2004 2
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
d2012 1
a2012 1
}
d2028 1
a2028 1
 *	pmap	point to pmap structure
d2047 1
a2047 1
 * 2:	The page table entries (PTEs) are initialized (set invalid), and
d2050 6
d2058 3
a2060 1
pmap_expand(pmap_t pmap, vaddr_t v)
d2062 10
a2071 5
	int i, spl;
	vaddr_t pdt_vaddr;
	paddr_t pdt_paddr;
	sdt_entry_t *sdt;
	pt_entry_t *pte;
d2075 1
a2075 1
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, pmap, v);
d2078 1
a2078 1
	CHECK_PAGE_ALIGN(v, "pmap_expand");
d2082 1
a2082 2
	if (pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr) == FALSE)
		panic("pmap_expand: pmap_extract failed");
d2093 1
a2093 1
	PMAP_LOCK(pmap, spl);
d2095 1
a2095 1
	if ((pte = pmap_pte(pmap, v)) != PT_ENTRY_NULL) {
d2100 1
a2100 1
		PMAP_UNLOCK(pmap, spl);
d2106 1
a2106 1
			printf("(pmap_expand: %x) table has already been allocated\n", curproc);
d2115 1
a2115 1
	 * first page table in the group, and initialize all the
d2118 1
a2118 1
	v &= ~((1 << (LOG2_PDT_TABLE_GROUP_SIZE + PDT_BITS + PG_BITS)) - 1);
d2120 1
a2120 1
	sdt = SDTENT(pmap,v);
d2133 2
a2134 2
	PMAP_UNLOCK(pmap, spl);
}
d2145 1
a2145 1
 * N.B.: This is the only routine which MAY NOT lazy-evaluation or lose
d2157 2
a2158 1
 *	pv lists
d2204 6
a2209 1
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
d2211 9
a2219 8
	int spl;
	pt_entry_t *pte, ap, template;
	paddr_t old_pa;
	pv_entry_t pv_e, pvl;
	u_int users;
	boolean_t kflush;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	struct vm_page *pg;
d2224 4
d2231 1
a2231 1
			printf("(pmap_enter: %x) pmap kernel va %x pa %x\n", curproc, va, pa);
d2233 1
a2233 1
			printf("(pmap_enter: %x) pmap %x va %x pa %x\n", curproc, pmap, va, pa);
a2235 1

d2239 5
a2243 5
	 * Must allocate a new pvlist entry while we're unlocked;
	 * zalloc may cause pageout (which will lock the pmap system).
	 * If we determine we need a pvlist entry, we will unlock
	 * and allocate one. Then will retry, throwing away
	 * the allocated entry later (if we no longer need it).
d2248 1
a2248 1
	users = pmap->pm_cpus;
d2256 3
a2258 1
			pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
d2285 1
a2285 1
			pmap->pm_stats.wired_count++;
d2287 1
a2287 1
			pmap->pm_stats.wired_count--;
d2316 5
a2320 2

		pg = PHYS_TO_VM_PAGE(pa);
d2322 10
a2331 1
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
d2333 3
a2335 5
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n",
				       phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
				       pg != NULL ? 1 : 0);
				printf("pte %x pfn %x valid %x\n",
				       pte, PG_PFNUM(*pte), PDT_VALID(pte));
a2337 6
#endif
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(users, va, TRUE);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}
d2339 1
a2339 1
		if (pg != NULL) {
d2351 1
a2351 1
			pvl = pg_to_pvh(pg);
d2353 1
a2353 1
			if (pvl->pv_pmap == PMAP_NULL) {
d2357 3
a2359 3
				pvl->pv_va = va;
				pvl->pv_pmap = pmap;
				pvl->pv_next = PV_ENTRY_NULL;
d2369 1
a2369 1
						if (e->pv_pmap == pmap && e->pv_va == va)
d2371 1
a2371 1
						e = e->pv_next;
d2382 4
a2385 4
				pv_e->pv_va = va;
				pv_e->pv_pmap = pmap;
				pv_e->pv_next = pvl->pv_next;
				pvl->pv_next = pv_e;
d2396 1
a2396 1
		pmap->pm_stats.resident_count++;
d2398 1
a2398 1
			pmap->pm_stats.wired_count++;
d2414 1
a2414 1
	} /* if (pa == old_pa) ... else */
d2421 2
a2422 2
	return 0;
}
d2425 1
a2425 1
 * Routine:	pmap_unwire
d2427 5
a2431 1
 * Function:	Change the wiring attributes for a map/virtual-address pair.
d2433 2
a2434 3
 * Parameters:
 *	pmap	pointer to pmap structure
 *	v	virtual address of page to be unwired
d2436 2
a2437 5
 * Calls:
 *	pmap_pte
 *
 * Special Assumptions:
 *	The mapping must already exist in the pmap.
d2440 3
a2442 1
pmap_unwire(pmap_t pmap, vaddr_t v)
d2444 2
a2445 2
	pt_entry_t *pte;
	int spl;
d2447 1
a2447 1
	PMAP_LOCK(pmap, spl);
d2449 1
a2449 1
	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
d2454 1
a2454 1
		pmap->pm_stats.wired_count--;
d2458 2
a2459 2
	PMAP_UNLOCK(pmap, spl);
}
a2476 2
 * If BATC mapping is enabled and the specified pmap is kernel_pmap,
 * batc_entry is scanned to find out the mapping.
d2478 4
a2481 1
 * Then the routine calls pmap_pte to get a (virtual) pointer to
d2488 4
a2491 1
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
d2493 5
a2497 8
	pt_entry_t *pte;
	paddr_t pa;
	int spl;
	boolean_t rv = FALSE;

#ifdef	PMAP_USE_BATC
	int i;
#endif
a2503 1
#ifdef	PMAP_USE_BATC
d2507 2
a2508 2
	if (pmap == kernel_pmap && batc_used != 0)
		for (i = batc_used - 1; i != 0; i--)
d2511 1
a2511 1
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) |
d2513 1
a2513 1
				return TRUE;
a2514 1
#endif
d2530 30
a2559 2
	return rv;
}
d2598 2
a2599 1
pmap_collect( pmap_t pmap)
d2601 21
a2621 13
	vaddr_t sdt_va;		/* outer loop index */
	vaddr_t sdt_vt;		/* end of segment */
	sdt_entry_t *sdttbl;	/* ptr to first entry in seg table */
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	sdt_entry_t *sdt;	/* ptr to index into segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t *gdttblend;	/* ptr to byte after last entry in
				   table group */
	pt_entry_t *gdtp;	/* ptr to index into a page table */
	boolean_t found_gdt_wired; /* flag indicating a wired page exists
				   in a page table's address range */
	int spl;
	u_int i, j;
d2625 1
a2625 1
		printf ("(pmap_collect: %x) pmap %x\n", curproc, pmap);
d2630 1
a2630 1
	sdttbl = pmap->pm_stab; /* addr of segment table */
d2633 3
a2635 3
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
d2645 1
a2645 1
		sdt_va = VM_MIN_ADDRESS + PDT_TABLE_GROUP_VA_SPACE * i;
d2667 4
a2670 2
		sdt_vt = sdt_va <= VM_MAX_ADDRESS - PDT_TABLE_GROUP_VA_SPACE ?
		    sdt_va + PDT_TABLE_GROUP_VA_SPACE : VM_MAX_ADDRESS;
d2676 1
a2676 1
		 * we can safely deallocate the page map(s)
d2680 1
a2680 1
			*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = 0;
d2685 1
a2685 1
		 * calls uvm_km_free or free, which will invoke another
d2691 2
a2692 1
	}
d2698 1
a2698 1
		printf("(pmap_collect: %x) done\n", curproc);
d2700 1
a2700 1
}
d2704 1
a2704 1
 *
d2726 2
a2727 1
pmap_activate(struct proc *p)
d2729 5
a2733 6
	apr_template_t apr_data;
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();

#ifdef	PMAP_USE_BATC
	int n;
d2735 2
d2740 1
a2740 1
		printf("(pmap_activate: %x) pmap 0x%p\n", p, pmap);
d2745 1
a2745 1
		 * Lock the pmap to put this cpu in its active set.
d2748 1
a2748 1
		simple_lock(&pmap->pm_lock);
d2750 1
a2750 1
		apr_data.field.st_base = atop(pmap->pm_stpa);
d2755 2
a2756 2

#ifdef	PMAP_USE_BATC
d2758 3
a2760 3
		 * cmmu_pmap_activate will set the uapr and the batc entries,
		 * then flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE
d2763 2
a2764 2
		cmmu_pmap_activate(cpu, apr_data.bits,
				   pmap->pm_ibatc, pmap->pm_dbatc);
d2766 5
a2770 1
			*(register_t *)&batc_entry[n] = pmap->pm_ibatc[n].bits;
d2772 4
d2777 2
a2778 2
		cmmu_flush_tlb(FALSE, 0, -1);
#endif	/* PMAP_USE_BATC */
d2783 12
a2794 1
		SETBIT_CPUSET(cpu, &(pmap->pm_cpus));
a2795 1
		simple_unlock(&pmap->pm_lock);
d2797 1
a2797 1
}
d2809 1
a2809 1
 * pmap_deactive simply clears the pm_cpus field in given pmap structure.
d2813 2
a2814 1
pmap_deactivate(struct proc *p)
d2816 2
a2817 2
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();
d2823 3
a2825 3
		simple_lock(&pmap->pm_lock);
		CLRBIT_CPUSET(cpu, &(pmap->pm_cpus));
		simple_unlock(&pmap->pm_lock);
d2827 1
a2827 1
}
d2830 1
a2830 1
 * Routine:	PMAP_COPY_PAGE
d2832 2
a2833 2
 * Function:
 *	Copies the specified pages.
d2835 3
a2837 3
 * Parameters:
 *	src	PA of source page
 *	dst	PA of destination page
d2839 3
a2841 3
 * Extern/Global:
 *	phys_map_vaddr1
 *	phys_map_vaddr2
d2843 2
a2844 2
 * Calls:
 *	m88k_protection
d2846 2
a2847 2
 * Special Assumptions:
 *	no locking reauired
d2849 1
a2849 1
 * This routine maps the physical pages at the 'phys_map' virtual
d2854 2
a2855 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
d2857 4
a2860 6
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
	vaddr_t dstva, srcva;
	int spl;
	pt_entry_t template, *dstpte, *srcpte;
	int cpu = cpu_number();
d2866 1
a2866 1
	 * Map source physical address.
d2875 1
a2875 1
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
d2879 1
a2879 1
	 * Map destination physical address.
d2881 1
a2881 1
	cmmu_flush_tlb(TRUE, dstva, PAGE_SIZE);
d2889 1
a2889 1
}
d2892 1
a2892 1
 * Routine:	PMAP_CHANGEBIT
d2894 2
a2895 2
 * Function:
 *	Update the pte bits on the specified physical page.
d2897 4
a2900 4
 * Parameters:
 *	pg	physical page
 *	set	bits to set
 *	mask	bits to mask
d2902 3
a2904 2
 * Extern/Global:
 *	pv_lists
d2906 3
a2908 2
 * Calls:
 *	pmap_pte
d2917 17
a2933 9
pmap_changebit(struct vm_page *pg, int set, int mask)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, npte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;
d2938 1
a2938 1
	pvl = pg_to_pvh(pg);
d2943 2
a2944 1
	pvl->pv_flags &= mask;
d2946 1
a2946 1
	if (pvl->pv_pmap == PMAP_NULL) {
d2949 1
a2949 1
			printf("(pmap_changebit: %x) vm page 0x%x not mapped\n",
d2956 5
a2960 5
	/* for each listed pmap, update the affected bits */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		va = pvep->pv_va;
		if (!simple_lock_try(&pmap->pm_lock)) {
d2963 1
a2963 1
		users = pmap->pm_cpus;
d2980 1
a2980 1
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
d2993 1
a2993 1
		 * Invalidate pte temporarily to avoid the modified bit
d3001 1
a3001 1
		simple_unlock(&pmap->pm_lock);
d3004 1
a3004 1
}
d3007 1
a3007 1
 * Routine:	PMAP_CLEAR_MODIFY
d3009 2
a3010 2
 * Function:
 *	Clears the modify bits on the specified physical page.
d3013 2
a3014 1
pmap_clear_modify(struct vm_page *pg)
d3016 1
d3019 2
a3020 2
	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
d3025 1
a3025 1
 * Routine:	PMAP_TESTBIT
d3027 2
a3028 2
 * Function:
 *	Test the modified/referenced bits of a physical page.
d3030 7
a3036 3
 * Parameters:
 *	pg	physical page
 *	bit	bit to test
d3038 6
a3043 6
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	simple_lock, simple_unlock
 *	pmap_pte
d3045 1
a3045 1
 * If the attribute list for the given page has the bit, this routine
d3048 1
a3048 1
 * Otherwise, this routine walks the PV list corresponding to the
d3055 14
a3068 6
pmap_testbit(struct vm_page *pg, int bit)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte;
	int spl;
	boolean_t rv;
d3072 1
a3072 1
	pvl = pg_to_pvh(pg);
d3075 1
a3075 1
	if (pvl->pv_flags & bit) {
d3080 1
a3080 1
			printf("(pmap_testbit: %x) already cached a modify flag for this page\n",
d3087 1
a3087 1
	if (pvl->pv_pmap == PMAP_NULL) {
d3090 1
a3090 1
		rv = (boolean_t)(pvl->pv_flags & bit);
d3093 1
a3093 1
			printf("(pmap_testbit: %x) vm page 0x%x not mapped\n",
d3103 1
a3103 1
		if (!simple_lock_try(&pvep->pv_pmap->pm_lock)) {
d3107 3
a3109 3
		pte = pmap_pte(pvep->pv_pmap, pvep->pv_va);
		if (pte == PT_ENTRY_NULL) {
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->pv_va);
d3112 3
a3114 3
		if (*pte & bit) {
			simple_unlock(&pvep->pv_pmap->pm_lock);
			pvl->pv_flags |= bit;
d3117 1
a3117 1
				printf("(pmap_testbit: %x) modified page pte@@0x%p\n", curproc, pte);
d3122 2
a3123 2
		simple_unlock(&pvep->pv_pmap->pm_lock);
		pvep = pvep->pv_next;
d3128 1
a3128 1
}
d3131 1
a3131 1
 * Routine:	PMAP_IS_MODIFIED
d3133 3
a3135 3
 * Function:
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d3138 2
a3139 1
pmap_is_modified(struct vm_page *pg)
d3141 4
a3144 2
	return pmap_testbit(pg, PG_M);
}
d3147 1
a3147 1
 * Routine:	PMAP_CLEAR_REFERENCE
d3149 2
a3150 2
 * Function:
 *	Clear the reference bit on the specified physical page.
d3153 2
a3154 1
pmap_clear_reference(struct vm_page *pg)
d3156 1
d3159 2
a3160 2
	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
d3172 2
a3173 1
pmap_is_referenced(struct vm_page *pg)
d3175 3
a3177 1
	return pmap_testbit(pg, PG_U);
d3190 3
a3192 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
d3194 2
d3200 1
a3200 1
		pmap_changebit(pg, PG_RO, ~0);
d3206 1
a3206 1
		pmap_remove_all(pg);
d3212 2
a3213 1
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
d3220 4
a3223 1
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
d3225 3
a3227 3
	int spl;
	pt_entry_t template, *pte;
	u_int users;
d3229 2
a3230 2
	CHECK_PAGE_ALIGN(va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_kenter_pa - PA");
d3234 1
a3234 1
		printf ("(pmap_kenter_pa: %x) va %x pa %x\n", curproc, va, pa);
d3239 1
a3239 1
	users = kernel_pmap->pm_cpus;
d3247 2
a3248 1
		pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
d3254 2
a3255 2
	kernel_pmap->pm_stats.resident_count++;
	kernel_pmap->pm_stats.wired_count++;
d3269 3
a3271 1
pmap_kremove(vaddr_t va, vsize_t len)
d3273 2
a3274 2
	int spl;
	u_int users;
d3278 1
a3278 1
		printf("(pmap_kremove: %x) va %x len %x\n", curproc, va, len);
d3285 1
a3285 1
	users = kernel_pmap->pm_cpus;
d3287 1
a3287 1
	for (len >>= PAGE_SHIFT; len != 0; len--, va += PAGE_SIZE) {
d3306 2
a3307 2
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;
@


1.12.4.10
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d67 1
a67 1
 * VM externals
d69 2
a70 2
extern vaddr_t	avail_start, avail_end;
extern vaddr_t	virtual_avail, virtual_end;
d86 2
d108 1
a108 1
#define CD_USBIT	0x0100000	/* pmap_unsetbit */
d112 2
a113 15
int pmap_con_dbg = 0;

/*
 * Alignment checks for pages (must lie on page boundaries).
 */
#define PAGE_ALIGNED(ad)	(((vaddr_t)(ad) & PAGE_MASK) == 0)
#define	CHECK_PAGE_ALIGN(ad, who) \
	if (!PAGE_ALIGNED(ad)) \
		printf("%s: addr %x not page aligned.\n", who, ad)

#else	/* DEBUG */

#define	CHECK_PAGE_ALIGN(ad, who)

#endif	/* DEBUG */
d159 1
a159 1
#if defined(MVME188) && (defined(MVME187) || defined(MVME197))
d168 1
a168 1
 * Two pages of scratch space per cpu.
d171 1
a171 1
vaddr_t phys_map_vaddr, phys_map_vaddr_end;
d191 2
a192 2
#define	SPLVM(spl)	spl = splvm()
#define	SPLX(spl)	splx(spl)
d224 3
a232 1
void pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *);
a239 1
boolean_t pmap_unsetbit(struct vm_page *, int);
d246 2
d275 1
a275 1
			if ((p & PG_RO) == 0)
d289 1
a289 1
 *	'kernel'.
d295 1
a295 1
 *	kernel	TRUE if supervisor mode, FALSE if user mode
d303 1
a303 1
#ifdef DEBUG
d305 2
a306 1
		panic("flush_atc_entry: invalid ff1 users = %d", ff1(tusers));
d312 1
a312 1
			cmmu_flush_tlb(cpu, kernel, va, PAGE_SIZE);
d339 1
a339 1
#ifdef DEBUG
d346 1
a346 1
	sdt = SDTENT(pmap, virt);
d348 1
a348 1
	 * Check whether page table exists.
d397 1
a397 1
		printf("(pmap_expand_kmap: %x) v %x\n", curproc, virt);
a403 1
#ifdef DEBUG
a405 1
#endif
d416 2
a417 3

	/* Reinitialize this kpdt area to zero */
	bzero((void *)kpdt_ent, PDT_SIZE);
d468 1
a468 1
	u_int32_t batctmp;
d473 1
a473 1
	if (pmap_con_dbg & CD_MAP)
d478 2
a479 3
#ifdef DEBUG
	/* Check for zero if we map the very end of the address space... */
	if (start > end && end != 0) {
a480 1
	}
d486 2
a487 1
	batctmp = BATC_SO | BATC_V;
d489 1
a489 1
		batctmp |= BATC_WT;
d491 1
a491 1
		batctmp |= BATC_GLOBAL;
d493 1
a493 1
		batctmp |= BATC_INH;
d495 2
a496 1
		batctmp |= BATC_PROT;
d519 2
a520 2
			batctmp |= M88K_BTOBLK(virt) << BATC_VSHIFT;
			batctmp |= M88K_BTOBLK(page) << BATC_PSHIFT;
d525 2
a526 2
					    batctmp);
			batc_entry[batc_used] = batctmp;
d528 4
a531 2
			if (pmap_con_dbg & CD_MAP) {
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp);
a537 1
			}
d546 1
a546 1

d584 1
a584 1
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
d593 1
a593 2
	vaddr_t va;
	paddr_t pa;
d599 2
a600 2
	if ((mode & CACHE_MASK) != mode) {
		printf("(cache_ctrl) illegal mode %x\n", mode);
d603 1
a603 1
	if (pmap_con_dbg & CD_CACHE) {
d606 1
d608 1
d611 1
a611 1
#endif /* DEBUG */
d616 5
a620 1
	kflush = pmap == kernel_pmap;
d626 1
a626 1
		if (pmap_con_dbg & CD_CACHE) {
d635 1
a635 1
		*pte = (invalidate_pte(pte) & ~CACHE_MASK) | mode;
d641 1
a641 1
		pa = ptoa(PG_PFNUM(*pte));
d644 1
a644 1
				cmmu_flush_cache(cpu, pa, PAGE_SIZE);
d670 2
a671 1
 *	phys_map_vaddr	VA of page mapped arbitrarily for debug/IO
d686 2
a687 2
 *    A pair of virtual pages per cpu are reserved for debugging and
 * IO purposes. They are arbitrarily mapped when needed. They are used,
d704 1
d711 1
a711 1
	if (pmap_con_dbg & CD_BOOT) {
d714 2
d729 1
a729 1
	 *  pmap_bootstrap(&kernelstart, ...);
d746 1
a746 1
	kmap = (sdt_entry_t *)(*phys_start);
d752 2
a753 2
		printf("kernel_pmap->pm_stab = 0x%x (pa 0x%x)\n",
		    kernel_pmap->pm_stab, kmap);
d773 1
d775 1
d783 1
a783 1

d787 1
a787 1

d791 1
a791 1

d811 2
a812 2
		printf("kpdt_phys = 0x%x\n", kpdt_phys);
		printf("kpdt_virt = 0x%x\n", kpdt_virt);
d814 1
a814 1
		    *virt_start, *phys_start);
d833 1
a833 1
	s_text = load_start;	/* paddr of text */
d846 1
a846 1
	    CACHE_GLOBAL);	/* shouldn't it be RO? XXX*/
d869 16
d887 4
a890 4
	 * Get ethernet buffer - need etherlen bytes physically contiguous.
	 * 1 to 1 mapped as well???. There is actually a bug in the macros
	 * used by the 1x7 ethernet driver. Remove this when that is fixed.
	 * XXX -nivas
d892 1
a892 1
	if (brdtyp == BRD_187 || brdtyp == BRD_8120 || brdtyp == BRD_197) {
d894 1
a894 1
		etherlen = ETHERPAGES * PAGE_SIZE;
d907 1
a907 1
				    *virt_start, *phys_start);
d921 9
a929 1
	 * Map two pages per cpu for copying/zeroing.
d932 2
a933 4
	phys_map_vaddr = *virt_start;
	phys_map_vaddr_end = *virt_start + 2 * (max_cpus << PAGE_SHIFT);
	*phys_start += 2 * (max_cpus << PAGE_SHIFT);
	*virt_start += 2 * (max_cpus << PAGE_SHIFT);
d945 1
a945 1
	ptable = pmap_table_build();
d952 1
a952 1
	for (; ptable->size != (vsize_t)(-1); ptable++){
d954 3
d958 1
a958 1
			    ptable->phys_start + ptable->size,
d975 2
a976 2
	if ((p = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL) \
		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
d983 1
a983 1
	invalidate_pte(vmpte);
a1005 1

d1010 12
a1021 2
	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) |
	    CACHE_GLOBAL | CACHE_WT | APR_V;
d1024 1
a1024 1
		show_apr(kernel_pmap->pm_apr);
d1027 1
a1027 1
	/* Invalidate entire kernel TLB and get ready for address translation */
d1030 12
a1041 2
			cmmu_flush_tlb(i, TRUE, VM_MIN_KERNEL_ADDRESS,
			    VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS);
d1043 1
a1043 1
			cmmu_set_sapr(i, kernel_pmap->pm_apr);
d1066 9
d1085 1
a1085 1
	if (pmap_con_dbg & CD_INIT)
d1104 1
a1104 1
 *	phys_map_vaddr
d1120 1
a1120 1
	vaddr_t va;
d1122 2
a1123 2
	int cpu = cpu_number();
	pt_entry_t *pte;
d1125 3
a1127 4
	CHECK_PAGE_ALIGN(pa, "pmap_zero_page");

	va = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	pte = pmap_pte(kernel_pmap, va);
d1130 5
d1136 3
a1138 18
	*pte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V | pa;

	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_flush_tlb(cpu, TRUE, va, PAGE_SIZE);

	/*
	 * The page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the pa flushed after the filling.
	 */
	bzero((void *)va, PAGE_SIZE);
	cmmu_flush_data_cache(cpu, pa, PAGE_SIZE);

	SPLX(spl);
d1152 1
a1152 1
pmap_t
a1156 1
	paddr_t stpa;
d1170 1
a1170 1
	if (pmap_con_dbg & CD_CREAT) {
d1172 1
a1172 1
		    curproc, atop(s));
d1178 2
a1179 2
		panic("pmap_create: uvm_km_zalloc failure");

d1185 1
a1185 1
	    (paddr_t *)&stpa) == FALSE)
a1186 2
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) |
	    CACHE_GLOBAL | APR_V;
d1188 1
a1188 2
#ifdef DEBUG
	if (!PAGE_ALIGNED(stpa))
d1190 1
a1190 1
		    (int)stpa);
d1192 4
a1195 3
	if (pmap_con_dbg & CD_CREAT) {
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x (pa 0x%x)\n",
		    curproc, pmap, pmap->pm_stab, stpa);
d1199 10
a1208 4
	/* memory for page tables should not be writeback or local */
	pmap_cache_ctrl(kernel_pmap,
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_GLOBAL | CACHE_WT);

d1213 1
a1213 1
	 * There is no need to clear segment table, since uvm_km_zalloc
d1247 1
d1254 1
a1254 1
 * all translation table space back to the system using uvm_km_free.
d1256 1
a1256 1
 * ranges represented by the table group sizes(PDT_VA_SPACE).
d1265 1
d1268 1
a1268 1
	if (pmap_con_dbg & CD_FREE)
d1272 11
d1284 2
a1285 2
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE) {
d1290 1
a1290 1
				    curproc, gdttbl);
d1292 1
a1292 1
			uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
d1294 1
a1294 1
	}
a1295 4
	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
	sdttbl = pmap->pm_stab;		/* addr of segment table */
d1299 1
a1299 1
		    curproc, sdttbl);
d1301 4
a1304 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2 * SDT_SIZE));
d1307 1
a1307 1
	if (pmap_con_dbg & CD_FREE)
d1339 1
a1339 1
#ifdef DEBUG
d1358 1
a1358 1
 *	Add a reference to the specified pmap.
d1376 1
a1376 1
 * Routine:	PMAP_REMOVE_PTE (internal)
d1379 3
a1381 2
 *	Invalidate a given page table entry associated with the
 *	given virtual address.
d1385 2
a1386 2
 *	va		virtual address of page to remove
 *	pte		existing pte
d1392 1
d1400 7
a1406 1
 *  If the PTE is valid, the routine must invalidate the entry. The
d1414 1
a1414 1
pmap_remove_pte(pmap_t pmap, vaddr_t va, pt_entry_t *pte)
d1416 1
a1416 1
	pt_entry_t opte;
d1420 1
d1424 8
a1431 6
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove_pte: %x) pmap kernel va %x\n", curproc, va);
		else
			printf("(pmap_remove: %x) pmap %x va %x\n", curproc, pmap, va);
a1432 1
#endif
d1434 5
a1438 3
	if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
		return;	 	/* no page mapping, nothing to do! */
	}
d1440 2
a1441 2
	users = pmap->pm_cpus;
	kflush = pmap == kernel_pmap;
d1443 1
a1443 6
	/*
	 * Update statistics.
	 */
	pmap->pm_stats.resident_count--;
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
d1445 8
a1452 1
	pa = ptoa(PG_PFNUM(*pte));
d1454 1
a1454 3
	/*
	 * Invalidate the pte.
	 */
d1456 3
a1458 2
	opte = invalidate_pte(pte) & (PG_U | PG_M);
	flush_atc_entry(users, va, kflush);
d1460 6
a1465 1
	pg = PHYS_TO_VM_PAGE(pa);
d1467 2
a1468 3
	/* If this isn't a managed page, just return. */
	if (pg == NULL)
		return;
d1470 6
a1475 5
	/*
	 * Remove the mapping from the pvlist for
	 * this physical page.
	 */
	pvl = pg_to_pvh(pg);
d1478 2
a1479 2
	if (pvl->pv_pmap == PMAP_NULL)
		panic("pmap_remove_pte: null pv_list");
d1482 13
a1494 11
	prev = PV_ENTRY_NULL;
	for (cur = pvl; cur != PV_ENTRY_NULL; cur = cur->pv_next) {
		if (cur->pv_va == va && cur->pv_pmap == pmap)
			break;
		prev = cur;
	}
	if (cur == PV_ENTRY_NULL) {
		panic("pmap_remove_pte: mapping for va "
		    "0x%lx (pa 0x%lx) not in pv list at 0x%p",
		    va, pa, pvl);
	}
d1496 1
a1496 18
	if (prev == PV_ENTRY_NULL) {
		/*
		 * Hander is the pv_entry. Copy the next one
		 * to hander and free the next one (we can't
		 * free the hander)
		 */
		cur = cur->pv_next;
		if (cur != PV_ENTRY_NULL) {
			cur->pv_flags = pvl->pv_flags;
			*pvl = *cur;
			pool_put(&pvpool, cur);
		} else {
			pvl->pv_pmap = PMAP_NULL;
		}
	} else {
		prev->pv_next = cur->pv_next;
		pool_put(&pvpool, cur);
	}
d1498 11
a1508 3
	/* Update saved attributes for managed page */
	pvl->pv_flags |= opte;
}
d1510 4
a1513 33
/*
 * Routine:	PMAP_REMOVE_RANGE (internal)
 *
 * Function:
 *	Invalidate page table entries associated with the
 *	given virtual address range. The entries given are the first
 *	(inclusive) and last (exclusive) entries for the VM pages.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		virtual address of start of range to remove
 *	e		virtual address of end of range to remove
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pmap_remove_pte
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *   This routine sequences through the pages defined by the given
 * range. For each page, the associated page table entry (PTE) is
 * invalidated via pmap_remove_pte().
 *
 * Empty segments are skipped for performance.
 */
void
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	vaddr_t va;
d1515 4
a1518 8
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove: %x) pmap kernel s %x e %x\n", curproc, s, e);
		else
			printf("(pmap_remove: %x) pmap %x s %x e %x\n", curproc, pmap, s, e);
	}
#endif
d1520 6
a1525 5
	/*
	 * Loop through the range in vm_page_size increments.
	 */
	for (va = s; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;
d1527 5
a1531 7
		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
d1534 1
a1534 2
		pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
	}
d1568 5
d1623 3
a1635 5
#ifdef DEBUG
	if (pmap_con_dbg & CD_RMAL)
		printf("(pmap_remove_all: %x) va %x\n", curproc, pg, pg_to_pvh(pg)->pv_va);
#endif

d1649 2
a1650 1
	while (pvl != PV_ENTRY_NULL && (pmap = pvl->pv_pmap) != PMAP_NULL) {
a1653 1
		va = pvl->pv_va;
d1656 14
a1669 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d1683 1
a1683 1
		pmap_remove_pte(pmap, va, pte);
d1685 3
d1714 1
d1716 1
d1721 1
a1721 1
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
d1733 2
a1734 5
#ifdef DEBUG
	if (s >= e)
		panic("pmap_protect: start grater than end address");
#endif

d1740 5
d1750 5
a1754 1
	kflush = pmap == kernel_pmap;
d1759 1
a1759 1
	 * Loop through the range in vm_page_size increments.
d1762 14
a1775 8
		sdt_entry_t *sdt;

		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d1779 6
a1784 3
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
d1804 1
a1804 1
 *	New kernel virtual memory is allocated for a page table.
d1809 1
a1809 1
 *	that checks whether the map has been expanded enough. (We won't loop
d1828 1
a1828 1
 *	pmap != kernel_pmap
d1839 1
a1839 1
	int spl;
d1846 1
a1846 1
	if (pmap_con_dbg & CD_EXP)
d1857 8
a1864 3
	/* memory for page tables should not be writeback or local */
	pmap_cache_ctrl(kernel_pmap,
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_GLOBAL | CACHE_WT);
d1873 2
a1874 1
		simple_unlock(&pmap->pm_lock);
a1880 1
		splx(spl);
d1885 2
a1886 2
	 * its containing page 'table group', i.e. the group of
	 * page tables that fit eithin a single VM page.
d1891 1
a1891 1
	v &= ~((1 << (PDT_BITS + PG_BITS)) - 1);
d1893 1
a1893 1
	sdt = SDTENT(pmap, v);
d1899 7
a1905 3
	*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
	*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;

d1936 2
a1937 1
 *	pmap_remove_pte
d1944 1
a1944 1
 *	If the page table entry (PTE) already maps the given physical page,
d1960 1
a1960 1
 *	if necessary pmap_expand(pmap, v)
d1968 1
a1968 1
 *		pmap_remove_pte
d1979 1
a1979 1
	pt_entry_t *pte, template;
d1987 2
a1988 2
	CHECK_PAGE_ALIGN(va, "pmap_entry - va");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - pa");
d1991 1
a1991 1
	if (pmap_con_dbg & CD_ENT) {
a1996 6

	/* copying/zeroing pages are magic */
	if (pmap == kernel_pmap &&
	    va >= phys_map_vaddr && va < phys_map_vaddr_end) {
		return 0;
	}
d1999 10
a2008 1
	template = m88k_protection(pmap, prot);
a2011 1
	kflush = pmap == kernel_pmap;
d2013 1
d2019 1
a2019 1
			pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
d2024 1
a2024 1
			simple_unlock(&pmap->pm_lock);
d2026 1
a2026 1
			simple_lock(&pmap->pm_lock);
d2030 2
a2031 1
	 * Special case if the physical page is already mapped at this address.
a2033 4
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, *pte);
#endif
d2035 10
a2044 1
		/* May be changing its wired attributes or protection */
d2050 27
a2076 4
		pvl = NULL;
	} else { /* if (pa == old_pa) */
		/* Remove old mapping from the PV list if necessary. */
		pmap_remove_pte(pmap, va, pte);
d2079 17
d2097 7
d2105 1
a2105 1
			 *	Enter the mapping in the PV list for this
a2116 1
				pvl->pv_flags = 0;
d2121 1
a2121 1
				 * Check that this mapping is not already there
d2123 8
a2130 4
				for (pv_e = pvl; pv_e; pv_e = pv_e->pv_next)
					if (pv_e->pv_pmap == pmap &&
					    pv_e->pv_va == va)
						panic("pmap_enter: already in pv_list");
d2135 3
a2137 8
				pv_e = pool_get(&pvpool, PR_NOWAIT);
				if (pv_e == NULL) {
					if (flags & PMAP_CANFAIL) {
						PMAP_UNLOCK(pmap, spl);
						return (ENOMEM);
					} else
						panic("pmap_enter: "
						    "pvpool exhausted");
a2141 1
				pv_e->pv_flags = 0;
d2143 4
a2155 1
	} /* if (pa == old_pa) ... else */
d2157 6
a2162 3
	template |= PG_V;
	if (wired)
		template |= PG_W;
d2164 4
a2167 4
	if ((unsigned long)pa >= MAXPHYSMEM)
		template |= CACHE_INH;
	else
		template |= CACHE_GLOBAL;
d2169 1
a2169 4
	if (flags & VM_PROT_WRITE)
		template |= PG_U | PG_M;
	else if (flags & VM_PROT_ALL)
		template |= PG_U;
d2171 1
a2171 12
	/*
	 * Invalidate pte temporarily to avoid being written
	 * back the modified bit and/or the reference bit by
	 * any other cpu.
	 */
	template |= invalidate_pte(pte) & (PG_U | PG_M);
	*pte = template | pa;
	flush_atc_entry(users, va, kflush);
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) set pte to %x\n", *pte);
#endif
d2173 1
a2173 5
	/*
	 * Cache attribute flags
	 */
	if (pvl != NULL)
		pvl->pv_flags |= (template & (PG_U | PG_M));
d2175 2
a2176 1
	PMAP_UNLOCK(pmap, spl);
d2274 8
a2281 7
	pte = pmap_pte(pmap, va);
	if (pte != PT_ENTRY_NULL && PDT_VALID(pte)) {
		rv = TRUE;
		if (pap != NULL) {
			pa = ptoa(PG_PFNUM(*pte));
			pa |= (va & PAGE_MASK); /* offset within page */
			*pap = pa;
d2302 1
a2304 1
 *	uvm_km_free
d2315 1
a2315 1
 * invalidated. Finally, uvm_km_free is called to return the page to the
d2320 4
d2326 1
a2326 1
pmap_collect(pmap_t pmap)
d2329 2
d2332 1
d2340 1
d2343 1
a2343 1
	if (pmap_con_dbg & CD_COL)
d2349 12
a2360 1
	sdtp = pmap->pm_stab; /* addr of segment table */
d2363 3
a2365 2
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE, sdtp++) {
d2367 1
d2371 1
a2371 1
		gdttblend = gdttbl + PDT_ENTRIES;
d2385 4
d2390 1
a2390 1
		pmap_remove_range(pmap, sdt_va, sdt_va + PDT_VA_SPACE);
d2395 4
a2398 2
		*((sdt_entry_t *) sdtp) = 0;
		*((sdt_entry_t *)(sdtp + SDT_ENTRIES)) = 0;
d2401 3
a2403 2
		 * we have to unlock before freeing the table, since
		 * uvm_km_free will invoke another pmap routine
d2405 3
a2407 3
		simple_unlock(&pmap->pm_lock);
		uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
		simple_lock(&pmap->pm_lock);
d2413 1
a2413 1
	if (pmap_con_dbg & CD_COL)
d2422 6
a2427 1
 * 	Binds the pmap associated to the process to the current processor.
d2433 2
a2434 2
 *	If the specified pmap is not kernel_pmap, this routine stores its
 *	apr template into UAPR (user area pointer register) in the
d2437 3
a2439 2
 *	Then it flushes the TLBs mapping user virtual space, in the CMMUs
 *	connected to the specified CPU.
d2444 1
d2447 1
d2453 1
a2453 1
	if (pmap_con_dbg & CD_ACTIVATE)
d2461 1
d2463 6
d2473 1
a2473 1
		 * then flush the *USER* TLB. IF THE KERNEL WILL EVER CARE
d2477 2
a2478 2
		cmmu_pmap_activate(cpu, pmap->pm_apr,
		    pmap->pm_ibatc, pmap->pm_dbatc);
d2482 2
a2483 3
		cmmu_set_uapr(pmap->pm_apr);
		cmmu_flush_tlb(cpu, FALSE, VM_MIN_ADDRESS,
		    VM_MAX_ADDRESS - VM_MIN_ADDRESS);
d2490 1
d2499 2
a2500 1
 *	Unbinds the pmap associated to the process from the current processor.
d2503 4
a2506 1
 *	p		pointer to proc structure
d2513 1
a2513 1

d2535 2
a2536 1
 *	phys_map_vaddr
d2542 1
a2542 1
 *	no locking required
d2555 1
a2555 1
	pt_entry_t *dstpte, *srcpte;
d2558 2
a2559 2
	CHECK_PAGE_ALIGN(src, "pmap_copy_page - src");
	CHECK_PAGE_ALIGN(dst, "pmap_copy_page - dst");
d2561 7
a2567 2
	dstva = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	srcva = dstva + PAGE_SIZE;
a2568 1
	srcpte = pmap_pte(kernel_pmap, srcva);
d2571 2
a2572 5

	*dstpte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V | dst;
	*srcpte = m88k_protection(kernel_pmap, VM_PROT_READ) |
	    CACHE_GLOBAL | PG_V | src;
d2575 1
a2575 2
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
d2577 3
a2579 11
	cmmu_flush_tlb(cpu, TRUE, dstva, 2 * PAGE_SIZE);

	/*
	 * The source page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the source pa flushed before the copy is
	 * attempted, and the destination pa flushed afterwards.
	 */
	cmmu_flush_data_cache(cpu, src, PAGE_SIZE);
	bcopy((const void *)srcva, (void *)dstva, PAGE_SIZE);
	cmmu_flush_data_cache(cpu, dst, PAGE_SIZE);
d2581 4
a2584 1
	SPLX(spl);
d2614 1
a2614 1
	pt_entry_t *pte, npte, opte;
d2633 1
a2633 1
		if (pmap_con_dbg & CD_CBIT)
d2644 1
d2649 5
a2653 1
		kflush = pmap == kernel_pmap;
a2654 1
		va = pvep->pv_va;
d2657 1
d2661 4
a2664 4
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
#ifdef DIAGNOSTIC
d2666 1
a2666 2
			panic("pmap_changebit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
d2672 2
a2673 2
		opte = *pte;
		npte = (opte | set) & mask;
d2681 1
a2681 2
		if (npte != opte) {
			invalidate_pte(pte);
d2685 1
a2685 1
next:
d2692 16
d2728 1
a2728 1
 * given page. For each pmap/va pair, the page descriptor table entry is
d2739 1
d2743 1
a2744 1
	pvl = pg_to_pvh(pg);
d2747 1
a2747 1
		/* we've already cached this flag for this page,
d2750 3
a2752 3
		if (pmap_con_dbg & CD_TBIT)
			printf("(pmap_testbit: %x) already cached a %x flag for this page\n",
			    curproc, bit);
d2759 3
d2763 1
a2763 1
		if (pmap_con_dbg & CD_TBIT)
d2768 1
a2768 1
		return (FALSE);
d2772 2
a2773 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
d2779 3
a2781 2
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;
d2783 1
a2783 8

#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_testbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pvep->pv_pmap, pvep->pv_pmap == kernel_pmap ? 1 : 0, pg, VM_PAGE_TO_PHYS(pg));
#endif

		if ((*pte & bit) != 0) {
d2788 1
a2788 1
				printf("(pmap_testbit: %x) true on page pte@@0x%p\n", curproc, pte);
a2792 1
next:
d2794 1
d2802 1
a2802 1
 * Routine:	PMAP_UNSETBIT
d2805 2
a2806 6
 *	Clears a pte bit and returns its previous state, for the
 *	specified physical page.
 *	This is an optimized version of:
 *		rv = pmap_testbit(pg, bit);
 *		pmap_changebit(pg, 0, ~bit);
 *		return rv;
d2809 1
a2809 1
pmap_unsetbit(struct vm_page *pg, int bit)
d2811 1
a2811 75
	boolean_t rv = FALSE;
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, opte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	SPLVM(spl);

unsetbit_Retry:
	pvl = pg_to_pvh(pg);

	/*
	 * Clear saved attributes
	 */
	pvl->pv_flags &= ~bit;

	if (pvl->pv_pmap == PMAP_NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_USBIT)
			printf("(pmap_unsetbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return (FALSE);
	}

	/* for each listed pmap, update the specified bit */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		if (!simple_lock_try(&pmap->pm_lock)) {
			goto unsetbit_Retry;
		}
		users = pmap->pm_cpus;
		kflush = pmap == kernel_pmap;

		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		/*
		 * Check for existing and valid pte
		 */
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_unsetbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
#endif

		/*
		 * Update bits
		 */
		opte = *pte;
		if (opte & bit) {
			/*
			 * Flush TLB of which cpus using pmap.
			 *
			 * Invalidate pte temporarily to avoid the specified
			 * bit being written back by any other cpu.
			 */
			invalidate_pte(pte);
			*pte = opte ^ bit;
			flush_atc_entry(users, va, kflush);
		} else
			rv = TRUE;
next:
		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);

	return (rv);
d2815 1
a2815 1
 * Routine:	PMAP_IS_MODIFIED
d2818 1
a2818 2
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d2821 1
a2821 1
pmap_is_modified(struct vm_page *pg)
d2823 5
a2827 1
	return pmap_testbit(pg, PG_M);
d2855 10
a2864 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE)
d2866 2
a2867 2
	else if ((prot & VM_PROT_WRITE) == VM_PROT_NONE)
		pmap_changebit(pg, PG_RO, ~0);
d2888 1
a2888 1
	if (pmap_con_dbg & CD_ENT) {
d2901 3
a2903 2
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL)
		pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
d2916 1
a2916 1
	*pte = template | pa;
a2926 1
	vaddr_t e;
d2929 1
a2929 1
	if (pmap_con_dbg & CD_RM)
d2939 2
a2940 2
	e = va + len;
	for (; va < e; va += PAGE_SIZE) {
a2945 1
		/* If no segment table, skip a whole segment */
d2947 5
a2951 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
a2954 5
		pte = pmap_pte(kernel_pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}

d2961 1
@


1.12.4.11
log
@Merge with the trunk
@
text
@d69 1
a69 1
extern vaddr_t	avail_start;
a70 1
extern vaddr_t	last_addr;
d92 1
d144 35
a307 1
#if NCPUS > 1
d309 1
a309 3

	if (users == 0)
		return;
d312 1
a312 1
	if (ff1(users) >= MAX_CPUS) {
d317 1
a317 1
	while ((cpu = ff1(users)) != 32) {
d321 1
a321 1
		users &= ~(1 << cpu);
a322 4
#else
	if (users != 0)
		cmmu_flush_tlb(cpu_number(), kernel, va, PAGE_SIZE);
#endif
d664 4
d685 3
a687 1
 * address space.
d700 2
a701 1
pmap_bootstrap(vaddr_t load_start)
d705 1
a705 1
	vaddr_t vaddr, virt;
a707 1
	unsigned int kernel_pmap_size, pdt_size;
d712 8
d730 5
a734 5
	 * kernelstart being the first symbol in the load image.
	 * The kernel is linked such that &kernelstart == 0x10000 (size of
	 * BUG reserved memory area).
	 * The expression (&kernelstart - load_start) will end up as
	 *	0, making virtual_avail == avail_start, giving a 1-to-1 map)
d737 2
a738 2
	avail_start = round_page(avail_start);
	virtual_avail = avail_start +
d746 10
a755 3
	kmap = (sdt_entry_t *)(avail_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)virtual_avail;
	kmapva = virtual_avail;
d772 3
a774 1
	printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
d779 2
a780 2
	avail_start += kernel_pmap_size;
	virtual_avail += kernel_pmap_size;
d783 2
a784 2
	avail_start = round_page(avail_start);
	virtual_avail = round_page(virtual_avail);
d787 2
a788 2
	kpdt_phys = avail_start;
	kpdt_virt = (kpdt_entry_t)virtual_avail;
d790 2
a791 6
	/* Compute how much space we need for the kernel page table */
	pdt_size = atop(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS)
	    * sizeof(pt_entry_t);
	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		pdt_size += atop(ptable->size) * sizeof(pt_entry_t);
	pdt_size = round_page(pdt_size);
d793 2
a794 2
	avail_start += pdt_size;
	virtual_avail += pdt_size;
d799 7
a805 4
	printf("--------------------------------------\n");
	printf("        kernel page start = 0x%x\n", kpdt_phys);
	printf("   kernel page table size = 0x%x\n", pdt_size);
	printf("          kernel page end = 0x%x\n", avail_start);
d807 7
a813 1
	printf("kpdt_virt = 0x%x\n", kpdt_virt);
d856 6
a861 1
		while (vaddr < (virtual_avail - kernel_pmap_size))
d864 1
a864 1
	vaddr = pmap_map(vaddr, (paddr_t)kmap, avail_start,
d875 1
a875 1
		avail_start = vaddr;
d879 1
a879 1
		vaddr = pmap_map(vaddr, avail_start, avail_start + etherlen,
d882 2
a883 2
		virtual_avail += etherlen;
		avail_start += etherlen;
d885 9
a893 3
		if (vaddr != virtual_avail) {
			virtual_avail = vaddr;
			avail_start = round_page(avail_start);
d899 2
a900 2
	virtual_avail = round_page(virtual_avail);
	virtual_end = VM_MAX_KERNEL_ADDRESS;
d906 4
a909 4
	phys_map_vaddr = virtual_avail;
	phys_map_vaddr_end = virtual_avail + 2 * (max_cpus << PAGE_SHIFT);
	avail_start += 2 * (max_cpus << PAGE_SHIFT);
	virtual_avail += 2 * (max_cpus << PAGE_SHIFT);
d921 9
a929 2
	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		if (ptable->size != 0) {
d934 1
d953 1
a953 1
	virt = virtual_avail;
d960 1
a960 1
	virtual_avail = virt;
d971 5
d986 5
a990 1

d999 3
a1001 1
			printf("cpu%d: running virtual\n", i);
d1005 6
d2024 1
a2024 1
	if ((unsigned long)pa >= last_addr)
d2792 1
a2792 1
	if ((unsigned long)pa >= last_addr)
@


1.11
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1999/07/18 18:00:07 deraadt Exp $	*/
d48 1
a48 1
#  define OMRON_PMAP
d50 1
a50 1
#  define OMRON_PMAP
d52 1
d60 1
d65 2
d71 3
a73 3
 /*
  *  VM externals
  */
d77 2
a78 2
extern vm_offset_t	pcc2consvaddr;
extern vm_offset_t	clconsvaddr;
d80 1
a80 1
char *iiomapbase;
d83 7
d94 1
a94 1
#define	STATIC
d100 2
a101 2
#define CD_NORM		0x01
#define CD_FULL 	0x02
d103 30
a132 29
#define CD_ACTIVATE	0x0000004	/* _pmap_activate */
#define CD_KMAP		0x0000008	/* pmap_expand_kmap */
#define CD_MAP		0x0000010	/* pmap_map */
#define CD_MAPB		0x0000020	/* pmap_map_batc */
#define CD_CACHE	0x0000040	/* pmap_cache_ctrl */
#define CD_BOOT		0x0000080	/* pmap_bootstrap */
#define CD_INIT		0x0000100	/* pmap_init */
#define CD_CREAT	0x0000200	/* pmap_create */
#define CD_FREE		0x0000400	/* pmap_free_tables */
#define CD_DESTR	0x0000800	/* pmap_destroy */
#define CD_RM		0x0001000	/* pmap_remove */
#define CD_RMAL		0x0002000	/* pmap_remove_all */
#define CD_COW		0x0004000	/* pmap_copy_on_write */
#define CD_PROT		0x0008000	/* pmap_protect */
#define CD_EXP		0x0010000	/* pmap_expand */
#define CD_ENT		0x0020000	/* pmap_enter */
#define CD_UPD		0x0040000	/* pmap_update */
#define CD_COL		0x0080000	/* pmap_collect */
#define CD_CMOD		0x0100000	/* pmap_clear_modify */
#define CD_IMOD		0x0200000	/* pmap_is_modified */
#define CD_CREF		0x0400000	/* pmap_clear_reference */
#define CD_PGMV		0x0800000	/* pagemove */
#define CD_CHKPV	0x1000000	/* check_pv_list */
#define CD_CHKPM	0x2000000	/* check_pmap_consistency */
#define CD_CHKM		0x4000000	/* check_map */
#define CD_ALL		0x0FFFFFC

/*int pmap_con_dbg = CD_ALL | CD_FULL | CD_COW | CD_BOOT;*/
int pmap_con_dbg = CD_NORM;
d135 1
a135 1
#define	STATIC		static
d139 2
a140 2
caddr_t	vmmap;
pt_entry_t	*vmpte, *msgbufmap;
d142 1
a142 1
STATIC struct pmap 	kernel_pmap_store;
d147 2
a148 2
		kpdt_entry_t	next;
		vm_offset_t	phys;
d152 1
a152 1
STATIC kpdt_entry_t		kpdt_free;
d177 1
a177 1
vm_offset_t	phys_map_vaddr1, phys_map_vaddr2;
d179 1
a179 1
int		ptes_per_vm_page; /* no. of ptes required to map one VM page */
d190 1
a190 1
char	*pmap_modify_list;
d200 4
a203 4
typedef	struct pv_entry {
	struct pv_entry	*next;		/* next pv_entry */
	pmap_t		pmap;		/* pmap where mapping lies */
	vm_offset_t	va;		/* virtual address for mapping */
d208 1
a208 1
static pv_entry_t	pv_head_table;	/* array of entries, one per page */
d210 1
d261 4
a264 1

d267 8
d276 3
a278 6
#define PMAP_LOCK(pmap, spl)	SPLVM(spl)
#define PMAP_UNLOCK(pmap, spl)	SPLX(spl)

#define PV_LOCK_TABLE_SIZE(n)	0
#define LOCK_PVH(index)
#define UNLOCK_PVH(index)
d281 2
a282 2
void	*etherbuf=NULL;
int	etherlen;
d290 2
a291 2
static vm_offset_t	pmap_phys_start	= (vm_offset_t) 0;
static vm_offset_t	pmap_phys_end	= (vm_offset_t) 0;
d301 1
a301 1
boolean_t	pmap_initialized = FALSE;/* Has pmap_init completed? */
d314 1
a314 1
#define CHECK_PV_LIST(phys,pv_h,who) \
d316 1
a316 1
#define CHECK_PMAP_CONSISTENCY(who) \
d319 2
a320 2
#define CHECK_PV_LIST(phys,pv_h,who)
#define CHECK_PMAP_CONSISTENCY(who)
d326 1
a326 1
int	batc_used;
d333 2
a334 4
int 	maxcmmu_pb = 4;	/* max number of CMMUs per processors pbus	*/
int 	n_cmmus_pb = 1;	/* number of CMMUs per processors pbus		*/

#define cpu_number()	0	/* just being lazy, should be taken out -nivas*/
d341 22
a362 2
STATIC void
flush_atc_entry(unsigned users, vm_offset_t va, int kernel)
d364 15
a378 1
	cmmu_flush_remote_tlb(cpu_number(), kernel, va, M88K_PGBYTES);
d410 2
a411 2
    apr_template_t	apr_data;
    int 		n;
d414 2
a415 2
    if ((pmap_con_dbg & (CD_ACTIVATE | CD_FULL)) == (CD_ACTIVATE | CD_NORM))
	printf("(_pmap_activate :%x) pmap 0x%x\n", curproc, (unsigned)pmap);
d417 13
a429 13
    
    if (pmap != kernel_pmap) {
	/*
	 *	Lock the pmap to put this cpu in its active set.
	 */
	simple_lock(&pmap->lock);

	apr_data.bits = 0;
	apr_data.field.st_base = M88K_BTOP(pmap->sdt_paddr); 
	apr_data.field.wt = 0;
	apr_data.field.g  = 1;
	apr_data.field.ci = 0;
	apr_data.field.te = 1;
d431 13
a443 13
#ifdef OMRON_PMAP
	/*
	 * cmmu_pmap_activate will set the uapr and the batc entries, then
	 * flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE ABOUT THE
	 * BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE FLUSHED AS WELL.
	 */
	cmmu_pmap_activate(my_cpu, apr_data.bits, pmap->i_batc, pmap->d_batc);
        for (n = 0; n < BATC_MAX; n++)
	    *(unsigned*)&batc_entry[n] = pmap->i_batc[n].bits;
#else
	cmmu_set_uapr(apr_data.bits);
	cmmu_flush_tlb(0, 0, -1);
#endif
d445 19
a463 17
	/*
	 * I am forcing it to not program the BATC at all. pmap.c module
	 * needs major, major cleanup. XXX nivas
	 */
	cmmu_set_uapr(apr_data.bits);
	cmmu_flush_tlb(0, 0, -1);

	/*
	 *	Mark that this cpu is using the pmap.
	 */
	simple_unlock(&pmap->lock);

    } else {

	/*
	 * kernel_pmap must be always active.
	 */
d466 2
a467 2
    if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
	printf("(_pmap_activate :%x) called for kernel_pmap\n", curproc);
d470 1
a470 1
    }
d497 9
a505 3
    if (pmap != kernel_pmap) {
	/* Nothing to do */
    }
d523 1
a523 1
	pte_template_t p;
d525 2
a526 2
	p.bits = 0;
	p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;
d528 1
a528 1
	return(p.bits);
d558 8
a565 1
	sdt_entry_t	*sdt;
d567 8
a574 15
	/*XXX will this change if physical memory is not contiguous? */
	/* take a look at PDTIDX XXXnivas */
	if (map == PMAP_NULL)
		panic("pmap_pte: pmap is NULL");

	sdt = SDTENT(map,virt);

	/*
	 * Check whether page table is exist or not.
	 */
	if (!SDT_VALID(sdt))
		return(PT_ENTRY_NULL);
	else
		return((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
				PDTIDX(virt));
d618 4
a621 4
    int			aprot;
    sdt_entry_t		*sdt;
    kpdt_entry_t	kpdt_ent;
    pmap_t		map = kernel_pmap;
d624 2
a625 2
    if ((pmap_con_dbg & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
	printf("(pmap_expand_kmap :%x) v %x\n", curproc,virt);
d628 13
a640 1
    aprot = m88k_protection (map, prot);
d642 4
a645 16
    /*  segment table entry derivate from map and virt. */
    sdt = SDTENT(map, virt);
    if (SDT_VALID(sdt))
	panic("pmap_expand_kmap: segment table entry VALID");

    kpdt_ent = kpdt_free;
    if (kpdt_ent == KPDT_ENTRY_NULL) {
	printf("pmap_expand_kmap: Ran out of kernel pte tables\n");
	return(PT_ENTRY_NULL);
    }
    kpdt_free = kpdt_free->next;

    ((sdt_entry_template_t *)sdt)->bits = kpdt_ent->phys | aprot | DT_VALID;
    ((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = (vm_offset_t)kpdt_ent | aprot | DT_VALID;
    (unsigned)(kpdt_ent->phys) = 0;
    (unsigned)(kpdt_ent->next) = 0;
d647 1
a647 1
    return((pt_entry_t *)(kpdt_ent) + PDTIDX(virt));
d688 2
d693 14
a706 14
    int			aprot;
    unsigned		npages;
    unsigned		num_phys_pages;
    unsigned		cmode;
    pt_entry_t		*pte;
    pte_template_t	 template;

    /*
     * cache mode is passed in the top 16 bits.
     * extract it from there. And clear the top
     * 16 bits from prot.
     */
    cmode = (prot & 0xffff0000) >> 16;
    prot &= 0x0000ffff;
d709 3
a711 3
    if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
	printf ("(pmap_map :%x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
		 curproc, start, end, virt, prot, cmode);
d714 2
a715 2
    if (start > end)
	panic("pmap_map: start greater than end address");
d717 1
a717 1
    aprot = m88k_protection (kernel_pmap, prot);
d719 1
a719 1
    template.bits = M88K_TRUNC_PAGE(start) | aprot | DT_VALID | cmode;
d721 1
a721 1
    npages = M88K_BTOP(M88K_ROUND_PAGE(end) - M88K_TRUNC_PAGE(start));
d723 1
a723 1
    for (num_phys_pages = npages; num_phys_pages > 0; num_phys_pages--) {
d725 3
a727 3
	if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
	    if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
		panic ("pmap_map: Cannot allocate pte table");
d730 3
a732 3
	if (pmap_con_dbg & CD_MAP)
	  if (pte->dtype)
		printf("(pmap_map :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
d735 12
a746 4
	*pte = template.pte;
	virt += M88K_PGBYTES;
	template.bits += M88K_PGBYTES;
    }
d748 1
a748 1
    return(virt);
d799 1
a799 1
					vm_prot_t prot, unsigned cmode)
d801 7
a807 7
    int			aprot;
    unsigned		num_phys_pages;
    vm_offset_t		phys;
    pt_entry_t		*pte;
    pte_template_t	template;
    batc_template_t	batctmp;
    register int	i;
d810 1
a810 1
    if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
d812 1
a812 1
	      start, end, virt, prot);
d815 1
a815 1
    if (start > end)
d818 10
a827 10
    aprot = m88k_protection (kernel_pmap, prot);
    template.bits = M88K_TRUNC_PAGE(start) | aprot | DT_VALID | cmode;
    phys = start;
    batctmp.bits = 0;
    batctmp.field.sup = 1;			/* supervisor */
    batctmp.field.wt = template.pte.wt;		/* write through */
    batctmp.field.g = template.pte.g;		/* global */
    batctmp.field.ci = template.pte.ci;		/* cache inhibit */
    batctmp.field.wp = template.pte.prot;	/* protection */
    batctmp.field.v = 1;			/* valid */
d829 1
a829 1
    num_phys_pages = M88K_BTOP(M88K_ROUND_PAGE(end) - M88K_TRUNC_PAGE(start));
d831 1
a831 1
    while (num_phys_pages > 0) {
d834 18
a851 16
	if ((pmap_con_dbg & (CD_MAPB | CD_FULL)) == (CD_MAPB | CD_FULL))
	  printf("(pmap_map_batc :%x) num_phys_pg=%x, virt=%x, aligne V=%d, phys=%x, aligne P=%d\n", curproc,
	       num_phys_pages, virt, BATC_BLK_ALIGNED(virt), phys, BATC_BLK_ALIGNED(phys));
#endif

	if ( BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(phys) && 
	    num_phys_pages >= BATC_BLKBYTES/M88K_PGBYTES &&
	    batc_used < BATC_MAX ) {

	    /*
	     * map by BATC
	     */
	    batctmp.field.lba = M88K_BTOBLK(virt);
	    batctmp.field.pba = M88K_BTOBLK(phys);

	    cmmu_set_pair_batc_entry(0, batc_used, batctmp.bits);
d853 1
a853 1
	    batc_entry[batc_used] = batctmp.field;
d856 22
a877 22
	    if ((pmap_con_dbg & (CD_MAPB | CD_NORM)) == (CD_MAPB | CD_NORM)) {
		printf("(pmap_map_batc :%x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
	    }
	    if (pmap_con_dbg & CD_MAPB) {

		for (i = 0; i < BATC_BLKBYTES; i += M88K_PGBYTES ) {
		    pte = pmap_pte(kernel_pmap, virt+i);
		    if (pte->dtype)
		      printf("(pmap_map_batc :%x) va %x is already mapped : pte %x\n", curproc, virt+i, ((pte_template_t *)pte)->bits);
		}
	    }
#endif
	    batc_used++;
    	    virt += BATC_BLKBYTES;
	    phys += BATC_BLKBYTES;
	    template.pte.pfn = M88K_BTOP(phys);
	    num_phys_pages -= BATC_BLKBYTES/M88K_PGBYTES;
	    continue;
	}
	if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
	  if ((pte = pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE)) == PT_ENTRY_NULL)
	    panic ("pmap_map_batc: Cannot allocate pte table");
d880 3
a882 3
	if (pmap_con_dbg & CD_MAPB)
	  if (pte->dtype)
		printf("(pmap_map_batc :%x) pte @@ 0x%x already valid\n", curproc, (unsigned)pte);
d885 6
a890 6
	*pte = template.pte;
	virt += M88K_PGBYTES;
	phys += M88K_PGBYTES;
	template.bits += M88K_PGBYTES;
	num_phys_pages--;
    }
d892 1
a892 1
    return(M88K_ROUND_PAGE(virt));
d931 7
a937 6
    int		spl, spl_sav;
    pt_entry_t	*pte;
    vm_offset_t	va;
    int				kflush;
    int				cpu;
    register pte_template_t	opte;
d940 7
a946 7
    if ( mode & CACHE_MASK ) {
	printf("(cache_ctrl) illegal mode %x\n",mode);
	return;
    }
    if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
	printf("(pmap_cache_ctrl :%x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
    }
d949 19
a967 15
    if ( pmap == PMAP_NULL ) {
	panic("pmap_cache_ctrl: pmap is NULL");
    }

    PMAP_LOCK(pmap, spl);

    if (pmap == kernel_pmap) {
	kflush = 1;
    } else {
	kflush = 0;
    }

    for (va = s; va < e; va += M88K_PGBYTES) {
	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
	    continue;
d969 3
a971 3
    if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
	printf("(cache_ctrl) pte@@0x%08x\n",(unsigned)pte);
    }
d974 18
a991 16
	/*
	 * Invalidate pte temporarily to avoid being written back
	 * the modified bit and/or the reference bit by other cpu.
	 *  XXX
	 */
	spl_sav = splimp();
	opte.bits = invalidate_pte(pte);
	((pte_template_t *)pte)->bits = (opte.bits & CACHE_MASK) | mode;
	flush_atc_entry(0, va, kflush);
	splx(spl_sav);

	/*
	 * Data cache should be copied back and invalidated.
	 */
        cmmu_flush_remote_cache(0, M88K_PTOB(pte->pfn), M88K_PGBYTES);
    }
d993 3
a995 1
    PMAP_UNLOCK(pmap, spl);
a998 1

d1048 23
a1070 21
pmap_bootstrap(vm_offset_t	load_start,	/* IN */
	       vm_offset_t	*phys_start,	/* IN/OUT */
	       vm_offset_t	*phys_end,	/* IN */
	       vm_offset_t	*virt_start,	/* OUT */
	       vm_offset_t	*virt_end)	/* OUT */
{
    kpdt_entry_t	kpdt_virt;
    sdt_entry_t		*kmap;
    vm_offset_t		vaddr,
    			virt,
    			kpdt_phys,
    			s_text,
    			e_text,
    			kernel_pmap_size,
    			etherpa;
    apr_template_t	apr_data;
    pt_entry_t		*pte;
    int			i;
    u_long		foo;
    extern char	*kernelstart, *etext;
    extern void cmmu_go_virt(void);
d1073 29
a1101 27
    if ((pmap_con_dbg & (CD_BOOT | CD_NORM)) == (CD_BOOT | CD_NORM)) {
	printf("pmap_bootstrap : \"load_start\" 0x%x\n", load_start);
    }
#endif
    ptes_per_vm_page = PAGE_SIZE >> M88K_PGSHIFT;
    if (ptes_per_vm_page == 0){
	panic("pmap_bootstrap: VM page size < MACHINE page size");
    }
    if (!PAGE_ALIGNED(load_start)) {
	panic("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x", load_start);
    }

    /*
     * Allocate the kernel page table from the front of available
     * physical memory,
     * i.e. just after where the kernel image was loaded.
     */
    /*
     * The calling sequence is 
     *    ...
     *  pmap_bootstrap(&kernelstart,...) 
     *  kernelstart is the first symbol in the load image.
     *  We link the kernel such that &kernelstart == 0x10000 (size of
     *							BUG ROM)
     *  The expression (&kernelstart - load_start) will end up as
     *	0, making *virt_start == *phys_start, giving a 1-to-1 map)
     */
d1103 3
a1105 3
    *phys_start = M88K_ROUND_PAGE(*phys_start);
    *virt_start = *phys_start +
   		 (M88K_TRUNC_PAGE((unsigned)&kernelstart) - load_start);
d1107 8
a1114 7
    /*
     * Initialilze kernel_pmap structure
     */
    kernel_pmap->ref_count = 1;
    kernel_pmap->sdt_paddr = kmap = (sdt_entry_t *)(*phys_start);
    kernel_pmap->sdt_vaddr = (sdt_entry_t *)(*virt_start);
    kmapva = *virt_start;
d1117 21
a1137 21
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	printf("kernel_pmap->sdt_paddr = %x\n",kernel_pmap->sdt_paddr);
	printf("kernel_pmap->sdt_vaddr = %x\n",kernel_pmap->sdt_vaddr);
    }
    /* init double-linked list of pmap structure */
    kernel_pmap->next = kernel_pmap;
    kernel_pmap->prev = kernel_pmap;
#endif

    /* 
     * Reserve space for segment table entries.
     * One for the regular segment table and one for the shadow table
     * The shadow table keeps track of the virtual address of page
     * tables. This is used in virtual-to-physical address translation
     * functions. Remember, MMU cares only for physical addresses of
     * segment and page table addresses. For kernel page tables, we
     * really don't need this virtual stuff (since the kernel will
     * be mapped 1-to-1) but for user page tables, this is required.
     * Just to be consistent, we will maintain the shadow table for
     * kernel pmap also.
     */
d1139 11
a1149 1
    kernel_pmap_size = 2*SDT_SIZE;
d1151 5
a1155 9
    /* save pointers to where page table entries start in physical memory */
    kpdt_phys = (*phys_start + kernel_pmap_size);
    kpdt_virt = (kpdt_entry_t)(*virt_start + kernel_pmap_size);
    kernel_pmap_size += MAX_KERNEL_PDT_SIZE;
    *phys_start += kernel_pmap_size;
    *virt_start += kernel_pmap_size;

    /* init all segment and page descriptor to zero */
    bzero(kernel_pmap->sdt_vaddr, kernel_pmap_size);
d1158 6
a1163 6
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	printf("kpdt_phys = %x\n",kpdt_phys);
	printf("kpdt_virt = %x\n",kpdt_virt);
    	printf("end of kpdt at (virt)0x%08x  ; (phys)0x%08x\n",
		*virt_start,*phys_start);
    }
d1165 11
a1175 11
    /*
     * init the kpdt queue
     */
    kpdt_free = kpdt_virt;
    for (i = MAX_KERNEL_PDT_SIZE/PDT_SIZE; i>0; i--) {
	kpdt_virt->next = (kpdt_entry_t)((vm_offset_t)kpdt_virt + PDT_SIZE);
	kpdt_virt->phys = kpdt_phys;
	kpdt_virt = kpdt_virt->next;
	kpdt_phys += PDT_SIZE;
    }
    kpdt_virt->next = KPDT_ENTRY_NULL; /* terminate the list */
d1177 3
a1179 3
    /*
     * Map the kernel image into virtual space
     */
d1181 5
a1185 5
    s_text = load_start;			/* paddr of text */
    e_text = load_start + ((unsigned)&etext -
		M88K_TRUNC_PAGE((unsigned)&kernelstart));
					/* paddr of end of text section*/
    e_text = M88K_ROUND_PAGE(e_text);
d1188 1
a1188 1
#define PMAPER	pmap_map
d1190 1
a1190 1
#define PMAPER	pmap_map_batc
d1193 30
a1222 27
    /*  map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
    vaddr = PMAPER(
		0,
		0,
		0x10000,
		(VM_PROT_WRITE | VM_PROT_READ)|(CACHE_INH <<16));

    assert(vaddr == M88K_TRUNC_PAGE((unsigned)&kernelstart));

    vaddr = PMAPER(
		(vm_offset_t)M88K_TRUNC_PAGE(((unsigned)&kernelstart)),
		s_text,
		e_text,
		VM_PROT_WRITE | VM_PROT_READ|(CACHE_GLOBAL<<16));	/* shouldn't it be RO? XXX*/

    vaddr = PMAPER(
		vaddr,
		e_text,
		(vm_offset_t)kmap,
		(VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));

    /*
     * Map system segment & page tables - should be cache inhibited?
     * 88200 manual says that CI bit is driven on the Mbus while accessing
     * the translation tree. I don't think we need to map it CACHE_INH
     * here...
     */
d1226 2
a1227 2
	      printf("(pmap_bootstrap) correcting vaddr\n");
	   }
d1229 2
a1230 2
	   while (vaddr < (*virt_start - kernel_pmap_size))
	      vaddr = M88K_ROUND_PAGE(vaddr + 1);
d1232 5
a1237 3
   vaddr = PMAPER(vaddr,(vm_offset_t)kmap,*phys_start,
	   (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
    
d1239 1
a1239 1
 #ifdef DEBUG
d1242 1
a1242 1
         *virt_start, *phys_start);
d1244 1
a1244 1
 #endif
a1247 1
	
d1249 32
a1280 26
    /*
     *  Get ethernet buffer - need etherlen bytes physically contiguous.
     *  1 to 1 mapped as well???. There is actually a bug in the macros
     *  used by the 1x7 ethernet driver. Remove this when that is fixed.
     *  XXX -nivas
     */
    etherlen = (ETHERPAGES * NBPG);
    *phys_start = vaddr;
   
    etherbuf = (void *)vaddr;
   
    vaddr = PMAPER(vaddr,*phys_start,*phys_start + etherlen,
     (VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));
    *virt_start += etherlen;
    *phys_start += etherlen; 
   
    if (vaddr != *virt_start) {
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
   	      printf("2:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
   		      *virt_start, *phys_start);
        }
#endif
   	*virt_start = vaddr;
   	*phys_start = round_page(*phys_start);
    }
d1282 1
d1292 1
a1292 1
   phys_map_vaddr2 = phys_map_vaddr1 + PAGE_SIZE;
d1298 3
a1300 3
   
   *phys_start += 2 * PAGE_SIZE;
   *virt_start += 2 * PAGE_SIZE;
d1305 1
a1305 1
    * on the 88200, 2 of the BATCs are hardwired to do map the IO space
d1312 14
a1325 17
   PMAPER(
            BUGROM_START,
            BUGROM_START,
            BUGROM_START + BUGROM_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));

   PMAPER(
            SRAM_START,
            SRAM_START,
            SRAM_START + SRAM_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_GLOBAL << 16));

   PMAPER(
            OBIO_START,
            OBIO_START,
            OBIO_START + OBIO_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));
d1327 6
a1332 26
#if 0
   PMAPER(
            VMEA16,
            VMEA16,
            VMEA16 + VMEA16_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));
    
	PMAPER(
            VMEA32D16,
            VMEA32D16,
            VMEA32D16 + VMEA32D16_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));
	
   PMAPER(
            IOMAP_MAP_START,
            IOMAP_MAP_START,
            IOMAP_MAP_START + IOMAP_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));
#endif /* 0 */

	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physcal pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -	         
d1334 1
a1334 1
	 */
d1340 1
a1340 1
    		pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16)); \
d1344 4
a1347 1
	virt = *virt_start;
d1349 2
a1350 2
	SYSMAP(caddr_t, vmpte , vmmap, 1);
	SYSMAP(struct msgbuf *,	msgbufmap ,msgbufp, 1);
d1352 9
a1360 12
	vmpte->pfn = -1;
	vmpte->dtype = DT_INVALID;
	
	*virt_start = virt;

    /*
     * Set translation for UPAGES at UADDR. The idea is we want to
     * have translations set up for UADDR. Later on, the ptes for
     * for this address will be set so that kstack will refer
     * to the u area. Make sure pmap knows about this virtual
     * address by doing vm_findspace on kernel_map.
     */
d1362 1
a1362 1
    for (i = 0, virt = UADDR; i < UPAGES; i++, virt += PAGE_SIZE) {
d1364 7
a1370 7
    	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("setting up mapping for Upage %d @@ %x\n", i, virt);
    	}
#endif
    	if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
    		pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
    }
d1372 3
a1374 3
    /*
     * Switch to using new page tables
     */
d1376 6
a1381 6
    apr_data.bits = 0;
    apr_data.field.st_base = M88K_BTOP(kernel_pmap->sdt_paddr);
    apr_data.field.wt = 1;
    apr_data.field.g  = 1;
    apr_data.field.ci = 0;
    apr_data.field.te = 1;	/* Translation enable */
d1383 4
a1386 4
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	void show_apr(unsigned value);
	show_apr(apr_data.bits);
    }
d1388 1
a1388 1
    /* Invalidate entire kernel TLB. */
d1390 3
a1392 3
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	printf("invalidating tlb %x\n", apr_data.bits);
    }
d1395 22
a1416 2
    cmmu_flush_remote_tlb(0, 1, 0, -1);

d1418 3
a1420 3
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	printf("done invalidating tlb %x\n", apr_data.bits);
    }
d1422 3
a1425 15
    /*
     * Set valid bit to DT_INVALID so that the very first pmap_enter()
     * on these won't barf in pmap_remove_range().
     */
    pte = pmap_pte(kernel_pmap, phys_map_vaddr1);
    pte->pfn = -1;
    pte->dtype = DT_INVALID;
    pte = pmap_pte(kernel_pmap, phys_map_vaddr2);
    pte->dtype = DT_INVALID;
    pte->pfn = -1;

    /* still physical */ 
    /* Load supervisor pointer to segment table. */
    cmmu_remote_set_sapr(0, apr_data.bits);
    /* virtual now on */
d1427 3
a1429 3
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
        printf("running virtual - avail_next 0x%x\n", *phys_start);
    }
d1431 3
a1433 1
    avail_next = *phys_start;
a1434 2
    return;
    
d1452 1
a1452 1
	register void *mem;
d1454 6
a1459 6
	size = round_page(size);
	mem = (void *)virtual_avail;
	virtual_avail = pmap_map(virtual_avail, avail_start,
				avail_start + size,
				VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
	avail_start += size;
d1461 4
a1464 4
    	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("pmap_bootstrap_alloc: size %x virtual_avail %x avail_start %x\n",
			size, virtual_avail, avail_start);
    	}
d1466 2
a1467 2
	bzero((void *)mem, size);
	return (mem);
d1500 1
a1500 1
 * variables. These define the range of pages 'managed' be pmap. These
d1515 5
a1519 5
    register long		npages;
    register vm_offset_t    	addr;
    register vm_size_t		s;
    register int		i;
    vm_size_t			pvl_table_size;
d1522 2
a1523 2
    if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
	printf("(pmap_init) phys_start %x  phys_end %x\n", phys_start, phys_end);
d1526 9
a1534 8
    /*
     * Allocate memory for the pv_head_table,
     * the modify bit array, and the pte_page table.
     */
    npages = atop(phys_end - phys_start);
    pvl_table_size = PV_LOCK_TABLE_SIZE(npages);
    s = (vm_size_t)(npages * sizeof(struct pv_entry)	/* pv_list */
	+ npages);					/* pmap_modify_list */
d1537 5
a1541 5
    if ((pmap_con_dbg & (CD_INIT | CD_FULL)) == (CD_INIT | CD_FULL)) {
	printf("(pmap_init) nbr of managed pages = %x\n", npages);
	printf("(pmap_init) size of pv_list = %x\n",
	       npages * sizeof(struct pv_entry));
    }
d1544 2
a1545 2
    s = round_page(s);
    addr = (vm_offset_t)kmem_alloc(kernel_map, s);
d1547 2
a1548 2
    pv_head_table =  (pv_entry_t)addr;
    addr = (vm_offset_t)(pv_head_table + npages);
d1550 13
a1562 1
    pmap_modify_list = (char *)addr;
d1564 1
a1564 1
    /*
d1571 2
a1572 2
    pmap_phys_start = phys_start;
    pmap_phys_end = phys_end;
d1574 1
a1574 1
    pmap_initialized = TRUE;
d1618 25
a1642 26
	vm_offset_t	srcva;
	pte_template_t	template;
	unsigned int i;
	unsigned int spl_sav;

	register int	my_cpu = cpu_number();
	pt_entry_t	*srcpte;

	srcva = (vm_offset_t)(phys_map_vaddr1 + (my_cpu * PAGE_SIZE));
	srcpte = pmap_pte(kernel_pmap, srcva);

	for (i = 0; i < ptes_per_vm_page; i++, phys += M88K_PGBYTES)
	  {
	    template.bits = M88K_TRUNC_PAGE(phys)
	      | m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE)
		| DT_VALID | CACHE_GLOBAL;


	    spl_sav = splimp();
	    cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
	    *srcpte = template.pte;
	    splx(spl_sav);
	    bzero (srcva, M88K_PGBYTES);
	    /* force the data out */
	    cmmu_flush_remote_data_cache(my_cpu,phys, M88K_PGBYTES);
	  }
d1677 1
a1677 1
    	pmap_t		p;
d1679 7
a1685 5
    /*
     * A software use-only map doesn't even need a map.
     */
    if (size != 0)
	return(PMAP_NULL);
d1687 4
a1690 1
    CHECK_PMAP_CONSISTENCY("pmap_create");
d1692 3
a1694 8
    p = (pmap_t)malloc(sizeof(*p), M_VMPMAP, M_WAITOK);
    if (p == PMAP_NULL) {
	panic("pmap_create: cannot allocate a pmap");
    }

    bzero(p, sizeof(*p));
    pmap_pinit(p);
    return(p);
d1701 3
a1703 3
    pmap_statistics_t	stats;
    sdt_entry_t		*segdt;
    int                	i;
d1705 6
a1710 6
    /*
     * Allocate memory for *actual* segment table and *shadow* table.
     */
    segdt = (sdt_entry_t *)kmem_alloc(kernel_map, 2 * SDT_SIZE);
    if (segdt == NULL)
	      panic("pmap_create: kmem_alloc failure");
d1713 2
a1714 2
    /* maybe, we can use bzero to zero out the segdt. XXX nivas */
    bzero(segdt, 2 * SDT_SIZE); */
d1716 3
a1718 3
    /* use pmap zero page to zero it out */
    pmap_zero_page(pmap_extract(kernel_pmap,(vm_offset_t)segdt));
    if (PAGE_SIZE == SDT_SIZE)  /* only got half */
d1720 1
a1720 1
    if (PAGE_SIZE < 2*SDT_SIZE) /* get remainder */
d1723 5
a1727 5
    /*
     * Initialize pointer to segment table both virtual and physical.
     */
    p->sdt_vaddr = segdt;
    p->sdt_paddr = (sdt_entry_t *)pmap_extract(kernel_pmap,(vm_offset_t)segdt);
d1729 4
a1732 4
    if (!PAGE_ALIGNED(p->sdt_paddr)) {
	printf("pmap_create: std table = %x\n",(int)p->sdt_paddr);
	panic("pmap_create: sdt_table not aligned on page boundary");
    }
d1735 4
a1738 4
    if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
	printf("(pmap_create :%x) pmap=0x%x, sdt_vaddr=0x%x, sdt_paddr=0x%x\n",
	     curproc, (unsigned)p, p->sdt_vaddr, p->sdt_paddr);
    }
d1742 7
a1748 7
    /*
     * memory for page tables should be CACHE DISABLED?
     */
    pmap_cache_ctrl(kernel_pmap,
		   (vm_offset_t)segdt,
		   (vm_offset_t)segdt+SDT_SIZE,
		   CACHE_INH);
d1750 7
a1756 7
    /*
     * Initalize SDT_ENTRIES.
     */
    /*
     * There is no need to clear segment table, since kmem_alloc would
     * provides us clean pages.
     */
d1758 6
a1763 4
    /*
     * Initialize pmap structure.
     */
    p->ref_count = 1;
d1766 5
a1770 5
     /* initialize block address translation cache */
     for (i = 0; i < BATC_MAX; i++) {
	 p->i_batc[i].bits = 0;
	 p->d_batc[i].bits = 0;
     }
d1773 6
a1778 6
    /*
     * Initialize statistics.
     */
    stats = &p->stats;
    stats->resident_count = 0;
    stats->wired_count = 0;
d1781 5
a1785 5
    /* link into list of pmaps, just after kernel pmap */
    p->next = kernel_pmap->next;
    p->prev = kernel_pmap;
    kernel_pmap->next = p;
    p->next->prev = p;
d1818 4
a1821 4
    unsigned long	sdt_va;	/*  outer loop index */
    sdt_entry_t	*sdttbl; /*  ptr to first entry in the segment table */
    pt_entry_t  *gdttbl; /*  ptr to first entry in a page table */
    unsigned int i,j;
d1824 2
a1825 2
    if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
	printf("(pmap_free_tables :%x) pmap %x\n", curproc, pmap);
d1828 1
a1828 1
    sdttbl = pmap->sdt_vaddr;		/* addr of segment table */
d1830 4
a1833 4
    /* 
      This contortion is here instead of the natural loop 
      because of integer overflow/wraparound if VM_MAX_USER_ADDRESS is near 0xffffffff
    */
d1835 3
a1837 3
    i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
    j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
    if ( j < 1024 ) j++;
d1839 4
a1842 5
    /* Segment table Loop */
    for ( ; i < j; i++)
      {
	sdt_va = PDT_TABLE_GROUP_VA_SPACE*i;
	if ((gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va)) != PT_ENTRY_NULL) {
d1844 2
a1845 2
	    if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
		printf("(pmap_free_tables :%x) free page table = 0x%x\n", curproc, gdttbl);
d1847 2
a1848 2
		PT_FREE(gdttbl);
	}
d1850 1
a1850 1
    } /* Segment Loop */
d1853 1
a1853 1
    if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
d1856 4
a1859 4
    /*
     * Freeing both *actual* and *shadow* segment tables
     */
    kmem_free(kernel_map, (vm_offset_t)sdttbl, 2*SDT_SIZE);
d1867 1
a1867 1
	pmap_free_tables(p);
d1869 5
a1873 5
	DEBUG ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
	  printf("(pmap_destroy :%x) ref_count = 0\n", curproc);
	/* unlink from list of pmap structs */
	p->prev->next = p->next;
	p->next->prev = p->prev;
d1905 1
a1905 1
    register int	c, s;
d1907 1
a1907 1
    if (p == PMAP_NULL) {
d1909 2
a1910 2
	if ((pmap_con_dbg & (CD_DESTR | CD_NORM)) == (CD_DESTR | CD_NORM))
	  printf("(pmap_destroy :%x) pmap is NULL\n", curproc);
d1912 8
a1919 2
	return;
     }
d1921 8
a1928 14
    if (p == kernel_pmap) {
	panic("pmap_destroy: Attempt to destroy kernel pmap");
    }

    CHECK_PMAP_CONSISTENCY("pmap_destroy");

    PMAP_LOCK(p, s);
    c = --p->ref_count;
    PMAP_UNLOCK(p, s);

    if (c == 0) {
	pmap_release(p);
	free((caddr_t)p,M_VMPMAP);
    }
d1951 1
a1951 1
    int		s;
d1953 5
a1957 5
    if (p != PMAP_NULL) {
	PMAP_LOCK(p, s);
	p->ref_count++;
	PMAP_UNLOCK(p, s);
    }
d2020 12
a2031 12
    int			pfi;
    int			pfn;
    int			num_removed = 0,
    			num_unwired = 0;
    register int	i;
    pt_entry_t		*pte;
    pv_entry_t		prev, cur;
    pv_entry_t		pvl;
    vm_offset_t		pa, va, tva;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;
d2033 1
a2033 1
    if (e < s)
d2036 40
a2075 14
    /*
     * Pmap has been locked by pmap_remove.
     */
    if (pmap == kernel_pmap) {
	kflush = 1;
    } else {
	kflush = 0;
    }

    /*
     * Loop through the range in vm_page_size increments.
     * Do not assume that either start or end fail on any
     * kind of page boundary (though this may be true!?).
     */
d2077 2
a2078 1
    CHECK_PAGE_ALIGN(s, "pmap_remove_range - start addr");
d2080 2
a2081 1
    for (va = s; va < e; va += PAGE_SIZE) {
d2083 58
a2140 1
	sdt_entry_t *sdt;
d2142 18
a2159 1
	sdt = SDTENT(pmap,va);
d2161 1
a2161 100
	if (!SDT_VALID(sdt)) {
	    va &= SDT_MASK; /* align to segment */
	    if (va <= e - (1<<SDT_SHIFT))
	      va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
	    else /* wrap around */
	      break;
	    continue;
	}

	pte = pmap_pte(pmap,va);

	if (!PDT_VALID(pte)) {
	    continue;			/* no page mapping */
	}

	num_removed++;

	if (pte->wired)
	  num_unwired++;

	pfn = pte->pfn;
	pa = M88K_PTOB(pfn);

	if (PMAP_MANAGED(pa)) {
	    pfi = PFIDX(pa);
	    /*
	     * Remove the mapping from the pvlist for
	     * this physical page.
	     */
	    pvl = PFIDX_TO_PVH(pfi);
	    CHECK_PV_LIST(pa, pvl, "pmap_remove_range before");

	    if (pvl->pmap == PMAP_NULL)
	      panic("pmap_remove: null pv_list");

	    if (pvl->va == va && pvl->pmap == pmap) {

		/*
		 * Hander is the pv_entry. Copy the next one
		 * to hander and free the next one (we can't
		 * free the hander)
		 */
		cur = pvl->next;
		if (cur != PV_ENTRY_NULL) {
		    *pvl = *cur;
		    free((caddr_t)cur, M_VMPVENT);
		} else {
		    pvl->pmap =  PMAP_NULL;
		}

	    } else {

		for (prev = pvl; (cur = prev->next) != PV_ENTRY_NULL; prev = cur) {
		    if (cur->va == va && cur->pmap == pmap) {
			break;
		    }
		}
		if (cur == PV_ENTRY_NULL) {
		    printf("pmap_remove_range: looking for VA "
			   "0x%x (pa 0x%x) PV list at 0x%x\n", va, pa, (unsigned)pvl);
		    panic("pmap_remove_range: mapping not in pv_list");
		}

		prev->next = cur->next;
	        free((caddr_t)cur, M_VMPVENT);
	    }

	    CHECK_PV_LIST(pa, pvl, "pmap_remove_range after");

	} /* if PAGE_MANAGED */

	/*
	 * For each pte in vm_page (NOTE: vm_page, not
	 * M88K (machine dependent) page !! ), reflect
	 * modify bits to pager and zero (invalidate,
	 * remove) the pte entry.
	 */
	tva = va;
	for (i = ptes_per_vm_page; i > 0; i--) {

	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    opte.bits = invalidate_pte(pte);
	    flush_atc_entry(0, tva, kflush);

	    if (opte.pte.modified) {
		if (IS_VM_PHYSADDR(pa)) {
			vm_page_set_modified(PHYS_TO_VM_PAGE(opte.bits & M88K_PGMASK));
		}
		/* keep track ourselves too */
		if (PMAP_MANAGED(pa))
		  pmap_modify_list[pfi] = 1;
	    }
	    pte++;
	    tva += M88K_PGBYTES;
	}
	
    } /* end for ( va = s; ...) */
d2163 5
a2167 5
    /*
     * Update the counts
     */
    pmap->stats.resident_count -= num_removed;
    pmap->stats.wired_count -= num_unwired;
d2196 1
a2196 1
    int 		spl;
d2198 3
a2200 3
    if (map == PMAP_NULL) {
	 return;
     }
d2203 2
a2204 2
    if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
	printf("(pmap_remove :%x) map %x  s %x  e %x\n", curproc, map, s, e);
d2207 1
a2207 1
    CHECK_PAGE_ALIGN(s, "pmap_remove start addr");
d2209 2
a2210 2
    if (s>e)
	panic("pmap_remove: start greater than end address");
d2212 3
a2214 1
    pmap_remove_range(map, s, e);
d2263 11
a2273 11
    pv_entry_t			pvl, cur;
    register pt_entry_t		*pte;
    int				pfi;
    register int		i;
    register vm_offset_t	va;
    register pmap_t		pmap;
    int				spl;
    int				dbgcnt = 0;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;
d2275 2
a2276 2
    if (!PMAP_MANAGED(phys)) {
	/* not a managed page. */
d2278 2
a2279 2
	if (pmap_con_dbg & CD_RMAL)
	  printf("(pmap_remove_all :%x) phys addr 0x%x not a managed page\n", curproc, phys);
d2281 27
a2307 2
	return;
    }
d2309 6
a2314 1
    SPLVM(spl);
d2316 1
a2316 7
    /*
     * Walk down PV list, removing all mappings.
     * We have to do the same work as in pmap_remove_pte_page
     * since that routine locks the pv_head. We don't have
     * to lock the pv_head, since we have the entire pmap system.
     */
remove_all_Retry:
d2318 44
a2361 3
    pfi = PFIDX(phys);
    pvl = PFIDX_TO_PVH(pfi);
    CHECK_PV_LIST(phys, pvl, "pmap_remove_all before");
d2363 8
a2370 67
    /*
     * Loop for each entry on the pv list
     */
    while ((pmap = pvl->pmap) != PMAP_NULL) {
	va = pvl->va;
	users = 0;
	if (pmap == kernel_pmap) {
	    kflush = 1;
	} else {
	    kflush = 0;
	}

	pte = pmap_pte(pmap, va);

	/*
	 * Do a few consistency checks to make sure
	 * the PV list and the pmap are in synch.
	 */
	if (pte == PT_ENTRY_NULL) {
	    printf("(pmap_remove_all :%x) phys %x pmap %x va %x dbgcnt %x\n",
		 (unsigned)curproc, phys, (unsigned)pmap, va, dbgcnt);
	    panic("pmap_remove_all: pte NULL");
	}
	if (!PDT_VALID(pte))
	    panic("pmap_remove_all: pte invalid");
	if (M88K_PTOB(pte->pfn) != phys)
	    panic("pmap_remove_all: pte doesn't point to page");
	if (pte->wired)
	    panic("pmap_remove_all: removing  a wired page");

	pmap->stats.resident_count--;

	if ((cur = pvl->next) != PV_ENTRY_NULL) {
	    *pvl  = *cur;
	    free((caddr_t)cur, M_VMPVENT);
	}
	else
	    pvl->pmap = PMAP_NULL;

	/*
	 * Reflect modified pages to pager.
	 */
	for (i = ptes_per_vm_page; i>0; i--) {

	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    opte.bits = invalidate_pte(pte);
	    flush_atc_entry(users, va, kflush);

	    if (opte.pte.modified) {
		vm_page_set_modified((vm_page_t)PHYS_TO_VM_PAGE(phys));
		/* keep track ourselves too */
		pmap_modify_list[pfi] = 1;
	    }
	    pte++;
	    va += M88K_PGBYTES;
	}

	/*
	 * Do not free any page tables,
	 * leaves that for when VM calls pmap_collect().
	 */
	dbgcnt++;
    }
    CHECK_PV_LIST(phys, pvl, "pmap_remove_all after");
d2372 2
a2373 1
    SPLX(spl);
d2406 7
a2412 7
    register pv_entry_t		pv_e;
    register pt_entry_t 	*pte;
    register int		i;
    int				spl, spl_sav;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;
d2414 1
a2414 1
    if (!PMAP_MANAGED(phys)) {
d2416 2
a2417 2
	if (pmap_con_dbg & CD_CMOD)
	  printf("(pmap_copy_on_write :%x) phys addr 0x%x not managed \n", curproc, phys);
d2419 2
a2420 2
	return;
    }
d2422 1
a2422 1
    SPLVM(spl);
d2424 5
a2428 3
    pv_e = PFIDX_TO_PVH(PFIDX(phys));
    CHECK_PV_LIST(phys, pv_e, "pmap_copy_on_write before");
    if (pv_e->pmap  == PMAP_NULL) {
d2431 2
a2432 2
	if ((pmap_con_dbg & (CD_COW | CD_NORM)) == (CD_COW | CD_NORM))
	  printf("(pmap_copy_on_write :%x) phys addr 0x%x not mapped\n", curproc, phys);
d2435 22
a2456 1
	SPLX(spl);
a2457 2
	return;		/* no mappings */
    }
d2459 23
a2481 4
    /*
     * Run down the list of mappings to this physical page,
     * disabling write privileges on each one.
     */
d2483 13
a2495 48
    while (pv_e != PV_ENTRY_NULL) {
	pmap_t		pmap;
	vm_offset_t	va;

	pmap = pv_e->pmap;
	va = pv_e->va;

	users = 0;
	if (pmap == kernel_pmap) {
	    kflush = 1;
	} else {
	    kflush = 0;
	}

	/*
	 * Check for existing and valid pte
	 */
	pte = pmap_pte(pmap, va);
	if (pte == PT_ENTRY_NULL)
	    panic("pmap_copy_on_write: pte from pv_list not in map");
	if (!PDT_VALID(pte))
	    panic("pmap_copy_on_write: invalid pte");
	if (M88K_PTOB(pte->pfn) != phys)
	    panic("pmap_copy_on_write: pte doesn't point to page");

	/*
	 * Flush TLBs of which cpus using pmap.
	 */

	for (i = ptes_per_vm_page; i > 0; i--) {

	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    spl_sav = splimp();
	    opte.bits = invalidate_pte(pte);
	    opte.pte.prot = M88K_RO;
	    ((pte_template_t *)pte)->bits = opte.bits;
	    flush_atc_entry(users, va, kflush);
	    splx(spl_sav);
	    pte++;
	    va += M88K_PGBYTES;
	}

	pv_e = pv_e->next;
    }
    CHECK_PV_LIST(phys, PFIDX_TO_PVH(PFIDX(phys)), "pmap_copy_on_write");
d2497 7
a2503 1
    SPLX(spl);
d2537 42
a2578 38
    pte_template_t		maprot;
    unsigned			ap;
    int				spl, spl_sav;
    register int		i;
    pt_entry_t			*pte;
    vm_offset_t			va, tva;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;

    if (pmap == PMAP_NULL || prot & VM_PROT_WRITE)
	return;
    if ((prot & VM_PROT_READ) == 0) {
    	pmap_remove(pmap, s, e);
    	return;
    }
    if (s > e)
	panic("pmap_protect: start grater than end address");

    maprot.bits = m88k_protection(pmap, prot);
    ap = maprot.pte.prot;

    PMAP_LOCK(pmap, spl);

    if (pmap == kernel_pmap) {
	kflush = 1;
    } else {
	kflush = 0;
    }

    CHECK_PAGE_ALIGN(s, "pmap_protect");

    /*
     * Loop through the range in vm_page_size increment.
     * Do not assume that either start or end fall on any
     * kind of page boundary (though this may be true ?!).
     */
    for (va = s; va <= e; va += PAGE_SIZE) {
d2580 1
a2580 1
	pte = pmap_pte(pmap, va);
d2582 5
a2586 1
	if (pte == PT_ENTRY_NULL) {
a2587 6
	    va &= SDT_MASK; /* align to segment */
	    if (va <= e - (1<<SDT_SHIFT))
	      va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
	    else /* wrap around */
	      break;
	    
d2589 2
a2590 2
	    if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
	      printf("(pmap_protect :%x) no page table :: skip to 0x%x\n", curproc, va + PAGE_SIZE);
d2592 2
a2593 2
	    continue;
	}
d2595 1
a2595 1
	if (!PDT_VALID(pte)) {
d2597 2
a2598 2
	    if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
	      printf("(pmap_protect :%x) pte invalid pte @@ 0x%x\n", curproc, pte);
d2600 2
a2601 2
	    continue;			/*  no page mapping */
	}
d2603 1
a2603 1
	printf("(pmap_protect :%x) pte good\n", curproc);
d2606 2
a2607 2
	tva = va;
	for (i = ptes_per_vm_page; i>0; i--) {
d2609 14
a2622 14
	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    spl_sav = splimp();
	    opte.bits = invalidate_pte(pte);
	    opte.pte.prot = ap;
	    ((pte_template_t *)pte)->bits = opte.bits;
	    flush_atc_entry(0, tva, kflush);
	    splx(spl_sav);
	    pte++;
	    tva += M88K_PGBYTES;
	}
    }
d2624 1
a2624 1
    PMAP_UNLOCK(pmap, spl);
d2677 12
a2688 12
	int		i,
			spl;
	vm_offset_t	pdt_vaddr,
			pdt_paddr;

	sdt_entry_t *sdt;
	pt_entry_t		*pte;
	vm_offset_t pmap_extract();

	if (map == PMAP_NULL) {
	    panic("pmap_expand: pmap is NULL");
	}
d2691 2
a2692 2
	if ((pmap_con_dbg & (CD_EXP | CD_NORM)) == (CD_EXP | CD_NORM))
		printf ("(pmap_expand :%x) map %x v %x\n", curproc, map, v);
d2695 1
a2695 1
	CHECK_PAGE_ALIGN (v, "pmap_expand");
d2697 8
a2704 8
	/*
	 * Handle kernel pmap in pmap_expand_kmap().
	 */
	if (map == kernel_pmap) {
		PMAP_LOCK(map, spl);
		if (pmap_expand_kmap(v, VM_PROT_READ|VM_PROT_WRITE) == PT_ENTRY_NULL)
			panic ("pmap_expand: Cannot allocate kernel pte table");
		PMAP_UNLOCK(map, spl);
d2706 2
a2707 2
		if ((pmap_con_dbg & (CD_EXP | CD_FULL)) == (CD_EXP | CD_FULL))
			printf("(pmap_expand :%x) kernel_pmap\n", curproc);
d2709 2
a2710 2
		return;
	}
d2712 1
a2712 1
	/* XXX */
d2714 3
a2716 3
        if (kmem_alloc_wired(kernel_map, &pdt_vaddr, PAGE_SIZE) != KERN_SUCCESS)
          panic("pmap_enter: kmem_alloc failure");
	pmap_zero_page(pmap_extract(kernel_pmap, pdt_vaddr));
d2718 1
a2718 1
	pdt_vaddr = kmem_alloc (kernel_map, PAGE_SIZE);
d2721 1
a2721 1
	pdt_paddr = pmap_extract(kernel_pmap, pdt_vaddr);
d2724 16
a2739 16
	/*
	 * the page for page tables should be CACHE DISABLED
	 */
	pmap_cache_ctrl(kernel_pmap, pdt_vaddr, pdt_vaddr+PAGE_SIZE, CACHE_INH);
#endif

	PMAP_LOCK(map, spl);

	if ((pte = pmap_pte(map, v)) != PT_ENTRY_NULL) {
		/*
		 * Someone else caused us to expand
		 * during our vm_allocate.
		 */
		PMAP_UNLOCK(map, spl);
		/* XXX */
		kmem_free (kernel_map, pdt_vaddr, PAGE_SIZE);
d2741 2
a2742 2
		if (pmap_con_dbg & CD_EXP)
			printf("(pmap_expand :%x) table has already allocated\n", curproc);
d2744 2
a2745 2
		return;
	}
d2747 9
a2755 24
	/*
	 * Apply a mask to V to obtain the vaddr of the beginning of
	 * its containing page 'table group',i.e. the group of
	 * page  tables that fit eithin a single VM page.
	 * Using that, obtain the segment table pointer that references the
	 * first page table in the group, and initilize all the
	 * segment table descriptions for the page 'table group'.
	 */
	v &= ~((1<<(LOG2_PDT_TABLE_GROUP_SIZE+PDT_BITS+PG_BITS))-1);

	sdt = SDTENT(map,v);

	/*
	 * Init each of the segment entries to point the freshly allocated
	 * page tables.
	 */

	for (i = PDT_TABLE_GROUP_SIZE; i>0; i--) {
		((sdt_entry_template_t *)sdt)->bits = pdt_paddr | M88K_RW | DT_VALID;
		((sdt_entry_template_t *)(sdt + SDT_ENTRIES))->bits = pdt_vaddr | M88K_RW | DT_VALID;
		sdt++;
		pdt_paddr += PDT_SIZE;
		pdt_vaddr += PDT_SIZE;
	}
d2757 16
a2772 1
	PMAP_UNLOCK(map, spl);
d2858 2
a2859 2
					vm_prot_t prot, boolean_t wired,
	                                vm_prot_t access_type)
d2861 12
a2872 16
    int				ap;
    int				spl, spl_sav;
    pv_entry_t			pv_e;
    pt_entry_t			*pte;
    vm_offset_t			old_pa;
    pte_template_t		template;
    register int		i;
    int				pfi;
    pv_entry_t			pvl;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;

    if (pmap == PMAP_NULL) {
	panic("pmap_enter: pmap is NULL");
    }
d2874 6
a2879 2
    CHECK_PAGE_ALIGN (va, "pmap_entry - VA");
    CHECK_PAGE_ALIGN (pa, "pmap_entry - PA");
d2881 3
a2883 3
    /*
     *	Range check no longer use, since we use whole address space
     */
d2886 49
a2934 19
    if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
	if (pmap == kernel_pmap)
	  printf ("(pmap_enter :%x) pmap kernel va %x pa %x\n", curproc, va, pa);
	else
	  printf ("(pmap_enter :%x) pmap %x  va %x pa %x\n", curproc, pmap, va, pa);
    }
#endif

    ap = m88k_protection (pmap, prot);

    /*
     *	Must allocate a new pvlist entry while we're unlocked;
     *	zalloc may cause pageout (which will lock the pmap system).
     *	If we determine we need a pvlist entry, we will unlock
     *	and allocate one. Then will retry, throwing away
     *	the allocated entry later (if we no longer need it).
     */
    pv_e = PV_ENTRY_NULL;
  Retry:
d2936 38
a2973 1
    PMAP_LOCK(pmap, spl);
d2975 1
a2975 20
    /*
     * Expand pmap to include this pte. Assume that
     * pmap is always expanded to include enough M88K
     * pages to map one VM page.
     */
    while ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
	/*
	 * Must unlock to expand the pmap.
	 */
	PMAP_UNLOCK(pmap, spl);
	pmap_expand(pmap, va);
	PMAP_LOCK(pmap, spl);
    }

    /*
     *	Special case if the physical page is already mapped
     *	at this address.
     */
    old_pa = M88K_PTOB(pte->pfn);
    if (old_pa == pa) {
d2977 8
a2984 56
	users = 0;
	if (pmap == kernel_pmap) {
	    kflush = 1;
	} else {
	    kflush = 0;
	}
	
	/*
	 * May be changing its wired attributes or protection
	 */

	if (wired && !pte->wired)
	  pmap->stats.wired_count++;
	else if (!wired && pte->wired)
	  pmap->stats.wired_count--;

	if ((unsigned long)pa >= MAXPHYSMEM)
	    template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
	else
	    template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
	if (wired)
	  template.pte.wired = 1;

	/*
	 * If there is a same mapping, we have nothing to do.
	 */
	if ( !PDT_VALID(pte) || (pte->wired != template.pte.wired)
	    || (pte->prot != template.pte.prot)) {

	    for (i = ptes_per_vm_page; i>0; i--) {
		
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by other cpu.
		 */
		spl_sav = splimp();
		opte.bits = invalidate_pte(pte);
		template.pte.modified = opte.pte.modified;
		*pte++ = template.pte;
		flush_atc_entry(users, va, kflush);
		splx(spl_sav);
		template.bits += M88K_PGBYTES;
		va += M88K_PGBYTES;
	    }
	}
	
    } else { /* if ( pa == old_pa) */

	/*
	 * Remove old mapping from the PV list if necessary.
	 */
	if (old_pa != (vm_offset_t)-1) {
	    /*
	     *	Invalidate the translation buffer,
	     *	then remove the mapping.
	     */
d2986 16
a3001 16
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
			if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
				phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
					PMAP_MANAGED(pa) ? 1 : 0);
				printf("pte %x pfn %x valid %x\n",
					pte, pte->pfn, pte->dtype);
	    		}
		}
#endif
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(0, va, 1);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}
	}
d3003 1
a3003 1
	if (PMAP_MANAGED(pa)) {
d3005 23
a3027 22
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
	    		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				printf("va 0x%x and managed pa 0x%x\n", va, pa);
	    		}
    		}
#endif
	    /*
	     *	Enter the mappimg in the PV list for this
	     *	physical page.
	     */
	    pfi = PFIDX(pa);
	    pvl = PFIDX_TO_PVH(pfi);
	    CHECK_PV_LIST (pa, pvl, "pmap_enter before");

	    if (pvl->pmap == PMAP_NULL) {

		/*
		 *	No mappings yet
		 */
		pvl->va = va;
		pvl->pmap = pmap;
		pvl->next = PV_ENTRY_NULL;
d3029 1
a3029 1
	    } else {
d3031 33
a3063 46
		/*
		 * check that this mapping is not already there
		 */
		{
		    pv_entry_t e = pvl;
		    while (e != PV_ENTRY_NULL) {
			if (e->pmap == pmap && e->va == va)
			  panic ("pmap_enter: already in pv_list");
			e = e->next;
		    }
		}
#endif
		/*
		 *	Add new pv_entry after header.
		 */
		if (pv_e == PV_ENTRY_NULL) {
		    PMAP_UNLOCK(pmap, spl);
		    pv_e = (pv_entry_t) malloc(sizeof *pv_e, M_VMPVENT,
				M_NOWAIT);
		    goto Retry;
		}
		pv_e->va = va;
		pv_e->pmap = pmap;
		pv_e->next = pvl->next;
		pvl->next = pv_e;
		/*
		 *		Remeber that we used the pvlist entry.
		 */
		pv_e = PV_ENTRY_NULL;
	    }
	}

	/*
	 * And count the mapping.
	 */
	pmap->stats.resident_count++;
	if (wired)
	  pmap->stats.wired_count++;

	if ((unsigned long)pa >= MAXPHYSMEM)
	    template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_INH;
	else
	    template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;

	if (wired)
	  template.pte.wired = 1;
d3065 6
a3070 1
	DO_PTES (pte, template.bits);
d3072 4
a3075 1
    } /* if ( pa == old_pa ) ... else */
d3077 2
a3078 1
    PMAP_UNLOCK(pmap, spl);
d3080 7
a3086 1
    if (pv_e != PV_ENTRY_NULL)
d3119 20
a3138 20
	pt_entry_t	*pte;
	int		i;
	int		spl;

	PMAP_LOCK(map, spl);

	if ((pte = pmap_pte(map, v)) == PT_ENTRY_NULL)
		panic ("pmap_change_wiring: pte missing");

	if (wired && !pte->wired)
		/*
		 *	wiring mapping
		 */
		map->stats.wired_count++;

	else if (!wired && pte->wired)
		/*
		 *	unwired mapping
		 */
		map->stats.wired_count--;
d3140 2
a3141 2
	for (i = ptes_per_vm_page; i>0; i--)
		(pte++)->wired = wired;
d3143 1
a3143 1
	PMAP_UNLOCK(map, spl);
d3179 33
a3211 31
    register pt_entry_t	 *pte;
    register vm_offset_t pa;
    register int	 i;
    int			 spl;
    
    if (pmap == PMAP_NULL)
	panic("pmap_extract: pmap is NULL");
    
    /*
     * check BATC first
     */
    if (pmap == kernel_pmap && batc_used > 0)
	for (i = batc_used-1; i > 0; i--)
	    if (batc_entry[i].lba == M88K_BTOBLK(va)) {
		pa = (batc_entry[i].pba << BATC_BLKSHIFT) | (va & BATC_BLKMASK );
		return(pa);
	    }

    PMAP_LOCK(pmap, spl);
    
    if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
	pa = (vm_offset_t) 0;
    else {
	if (PDT_VALID(pte))
	    pa = M88K_PTOB(pte->pfn);
	else
	    pa = (vm_offset_t) 0;
    }
    
    if (pa)
	pa |= (va & M88K_PGOFSET);	/* offset within page */
a3212 2
    PMAP_UNLOCK(pmap, spl);
    
d3214 1
a3214 1
    printf("pmap_extract ret %x\n", pa);
d3216 2
a3217 2
    return(pa);
    
d3227 30
a3256 28
    pt_entry_t	*pte;
    vm_offset_t	pa;
    int		i;
    
    if (pmap == PMAP_NULL)
	panic("pmap_extract: pmap is NULL");
    
    /*
     * check BATC first
     */
    if (pmap == kernel_pmap && batc_used > 0)
	for (i = batc_used-1; i > 0; i--)
	    if (batc_entry[i].lba == M88K_BTOBLK(va)) {
		pa = (batc_entry[i].pba << BATC_BLKSHIFT) | (va & BATC_BLKMASK );
		return(pa);
	    }

    if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
	pa = (vm_offset_t) 0;
    else {
	if (PDT_VALID(pte))
	    pa = M88K_PTOB(pte->pfn);
	else
	    pa = (vm_offset_t) 0;
    }
    
    if (pa)
	pa |= (va & M88K_PGOFSET);	/* offset within page */
a3257 2
    return(pa);
    
d3282 1
a3282 1
				vm_size_t len, vm_offset_t src_addr)
d3285 1
a3285 1
      dst_pmap++; src_pmap++; dst_addr++; len++; src_addr++;
d3317 2
a3318 2
    if ((pmap_con_dbg & (CD_UPD | CD_FULL)) == (CD_UPD | CD_FULL))
	printf("(pmap_update :%x) Called \n", curproc);
d3368 12
a3379 12
    vm_offset_t		sdt_va;	/* outer loop index */
    vm_offset_t		sdt_vt; /* end of segment */
    sdt_entry_t	*sdttbl;	/* ptr to first entry in the segment table */
    sdt_entry_t	*sdtp;		/* ptr to index into segment table */
    sdt_entry_t	*sdt;		/* ptr to index into segment table */
    pt_entry_t	*gdttbl;	/* ptr to first entry in a page table */
    pt_entry_t	*gdttblend;	/* ptr to byte after last entry in table group */
    pt_entry_t	*gdtp;		/* ptr to index into a page table */
    boolean_t	found_gdt_wired; /* flag indicating a wired page exists in */
    				 /* a page table's address range 	   */
    int		spl;
    unsigned int i,j;
d3383 4
a3386 4
    if (pmap == PMAP_NULL) {
	panic("pmap_collect: pmap is NULL");
    }
    if (pmap == kernel_pmap) {
d3388 1
a3388 1
	return;
d3390 1
a3390 1
	panic("pmap_collect attempted on kernel pmap");
d3392 3
a3394 3
    }
    
    CHECK_PMAP_CONSISTENCY ("pmap_collect");
d3397 1
a3397 1
    if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
d3401 1
a3401 1
    PMAP_LOCK(pmap, spl);
d3403 2
a3404 2
    sdttbl = pmap->sdt_vaddr;	/* addr of segment table */
    sdtp = sdttbl;
d3406 28
a3433 4
    /* 
      This contortion is here instead of the natural loop 
      because of integer overflow/wraparound if VM_MAX_USER_ADDRESS is near 0xffffffff
    */
d3435 2
a3436 3
    i = VM_MIN_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
    j = VM_MAX_USER_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
    if ( j < 1024 ) j++;
d3438 1
a3438 4
    /* Segment table loop */
    for ( ; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE)
      {
	sdt_va = VM_MIN_USER_ADDRESS + PDT_TABLE_GROUP_VA_SPACE*i;
d3440 14
a3453 24
	gdttbl = pmap_pte(pmap, (vm_offset_t)sdt_va);

	if (gdttbl == PT_ENTRY_NULL)
	  continue;	/* no maps in this range */

	gdttblend = gdttbl + (PDT_ENTRIES * PDT_TABLE_GROUP_SIZE);

	/* scan page maps for wired pages */
	found_gdt_wired = FALSE;
	for (gdtp=gdttbl; gdtp <gdttblend; gdtp++) {
	    if (gdtp->wired) {
		found_gdt_wired = TRUE;
		break;
	    }
	}

	if (found_gdt_wired)
	  continue;	/* can't free this range */

	/* figure out end of range. Watch for wraparound */

	sdt_vt = sdt_va <= VM_MAX_USER_ADDRESS-PDT_TABLE_GROUP_VA_SPACE ?
	         sdt_va+PDT_TABLE_GROUP_VA_SPACE : 
	         VM_MAX_USER_ADDRESS;
d3455 7
a3461 2
	/* invalidate all maps in this range */
	pmap_remove_range (pmap, (vm_offset_t)sdt_va, (vm_offset_t)sdt_vt);
d3463 1
a3463 7
	/*
	 * we can safely deallocated the page map(s)
	 */
	for (sdt = sdtp; sdt < (sdtp+PDT_TABLE_GROUP_SIZE); sdt++) {
	    ((sdt_entry_template_t *) sdt) -> bits = 0;
	    ((sdt_entry_template_t *) sdt+SDT_ENTRIES) -> bits = 0;
	}
d3465 1
a3465 11
	/*
	 * we have to unlock before freeing the table, since PT_FREE
	 * calls kmem_free or zfree, which will invoke another pmap routine
	 */
	PMAP_UNLOCK(pmap, spl);
	PT_FREE(gdttbl);
	PMAP_LOCK(pmap, spl);

    } /* Segment table Loop */

    PMAP_UNLOCK(pmap, spl);
d3468 1
a3468 1
    if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
d3472 1
a3472 1
    CHECK_PMAP_CONSISTENCY("pmap_collect");
d3494 1
a3494 1
pmap_activate(pmap_t pmap, pcb_t pcb)
d3497 1
a3497 1
	my_cpu++;
d3499 2
a3500 1
	PMAP_ACTIVATE(pmap, pcb, 0);
d3516 1
d3519 1
a3519 1
pmap_deactivate(pmap_t pmap, pcb_t pcb)
d3522 1
a3522 1
	pmap++; th++; which_cpu++;
d3524 1
a3524 1
	PMAP_DEACTIVATE(pmap, pcb, 0);
d3538 1
a3538 1
	return (kernel_pmap);
d3573 40
a3612 41
	vm_offset_t	dstva, srcva;
  	unsigned int spl_sav;
  	int i;
	int		aprot;
	pte_template_t	template;
	pt_entry_t	*dstpte, *srcpte;
	int 		my_cpu = cpu_number();

	/*
	 *	Map source physical address.
	 */
	aprot = m88k_protection (kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);

	srcva = (vm_offset_t)(phys_map_vaddr1 + (cpu_number() * PAGE_SIZE));
	dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));

	srcpte = pmap_pte(kernel_pmap, srcva);
	dstpte = pmap_pte(kernel_pmap, dstva);

	for (i=0; i < ptes_per_vm_page; i++, src += M88K_PGBYTES, dst += M88K_PGBYTES)
	  {
	    template.bits = M88K_TRUNC_PAGE(src) | aprot | DT_VALID | CACHE_GLOBAL;

	    /* do we need to write back dirty bits */
	    spl_sav = splimp();
	    cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
	    *srcpte = template.pte;

	    /*
	     *	Map destination physical address.
	     */
	    template.bits = M88K_TRUNC_PAGE(dst) | aprot | CACHE_GLOBAL | DT_VALID;
	    cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
	    *dstpte  = template.pte;
	    splx(spl_sav);

	    bcopy((void*)srcva, (void*)dstva, M88K_PGBYTES);
	    /* flush source, dest out of cache? */
	    cmmu_flush_remote_data_cache(my_cpu, src, M88K_PGBYTES);
	    cmmu_flush_remote_data_cache(my_cpu, dst, M88K_PGBYTES);
	  }
d3643 36
a3678 38
    vm_offset_t	dstva;
    pt_entry_t	*dstpte;
    int		copy_size,
   		offset,
    		aprot;
    unsigned int i;
    pte_template_t  template;

    dstva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
    dstpte = pmap_pte(kernel_pmap, dstva);
    copy_size = M88K_PGBYTES;
    offset = dstpa - M88K_TRUNC_PAGE(dstpa);
    dstpa -= offset;

    aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
    while (bytecount > 0){
	copy_size = M88K_PGBYTES - offset;
	if (copy_size > bytecount)
	  copy_size = bytecount;

	/*
	 *      Map distation physical address. 
	 */

	for (i = 0; i < ptes_per_vm_page; i++)
	  {
	    template.bits = M88K_TRUNC_PAGE(dstpa) | aprot | CACHE_WT | DT_VALID;
	    cmmu_flush_tlb(1, dstva, M88K_PGBYTES);
	    *dstpte = template.pte;

	    dstva += offset;
	    bcopy((void*)srcva, (void*)dstva, copy_size);
	    srcva += copy_size;
	    dstva += copy_size;
	    dstpa += M88K_PGBYTES;
	    bytecount -= copy_size;
	    offset = 0;
	  }
d3680 1
d3709 36
a3744 38
    register vm_offset_t	srcva;
    register pt_entry_t	*srcpte;
    register int		copy_size, offset;
    int             aprot;
    unsigned int i;
    pte_template_t  template;

    srcva = (vm_offset_t)(phys_map_vaddr2 + (cpu_number() * PAGE_SIZE));
    srcpte = pmap_pte(kernel_pmap, srcva);
    copy_size = M88K_PGBYTES;
    offset = srcpa - M88K_TRUNC_PAGE(srcpa);
    srcpa -= offset;

    aprot = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE);
    while (bytecount > 0){
	copy_size = M88K_PGBYTES - offset;
	if (copy_size > bytecount)
	  copy_size = bytecount;

	/*
	 *      Map destnation physical address.
	 */

	for (i=0; i < ptes_per_vm_page; i++)
	  {
	    template.bits = M88K_TRUNC_PAGE(srcpa) | aprot | CACHE_WT | DT_VALID;
	    cmmu_flush_tlb(1, srcva, M88K_PGBYTES);
	    *srcpte = template.pte;

	    srcva += offset;
	    bcopy((void*)srcva, (void*)dstva, copy_size);
	    srcpa += M88K_PGBYTES;
	    dstva += copy_size;
	    srcva += copy_size;
	    bytecount -= copy_size;
	    offset = 0;
	    /* cache flush source? */
	  }
d3746 1
d3772 1
a3772 1
							boolean_t pageable)
d3775 1
a3775 1
	pmap++; start++; end++; pageable++;
d3808 16
a3823 16
    pt_entry_t		*pte;
    int			spl, spl_sav;
    int			i;
    unsigned		users;
    pte_template_t	opte;
    int			kflush;

    va = M88K_ROUND_PAGE(va);
    PMAP_LOCK(pmap, spl);

    users = 0;
    if (pmap == kernel_pmap) {
	kflush = 1;
    } else {
	kflush = 0;
    }
d3825 1
a3825 1
    if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL && PDT_VALID(pte))
d3828 12
a3839 12
	  /*
	   * Invalidate pte temporarily to avoid being written back
	   * the modified bit and/or the reference bit by other cpu.
	   */
	  spl_sav = splimp();
	  opte.bits = invalidate_pte(pte);
	  opte.pte.prot = M88K_RO;
	  ((pte_template_t *)pte)->bits = opte.bits;
	  flush_atc_entry(users, va, kflush);
	  splx(spl_sav);
	  pte++;
	  va +=M88K_PGBYTES;
d3842 1
a3842 1
    PMAP_UNLOCK(pmap, spl);
d3880 11
a3890 11
    pv_entry_t		pvl;
    int			pfi;
    pv_entry_t		pvep;
    pt_entry_t		*pte;
    pmap_t		pmap;
    int			spl, spl_sav;
    vm_offset_t		va;
    int			i;
    unsigned		users;
    pte_template_t	opte;
    int			kflush;
d3892 1
a3892 1
    if (!PMAP_MANAGED(phys)) {
d3894 2
a3895 2
	if (pmap_con_dbg & CD_CMOD)
	  printf("(pmap_clear_modify :%x) phys addr 0x%x not managed \n", curproc, phys);
d3897 2
a3898 2
	return;
    }
d3900 1
a3900 1
    SPLVM(spl);
d3902 5
a3906 4
clear_modify_Retry:
    pfi = PFIDX(phys);
    pvl = PFIDX_TO_PVH(pfi);
    CHECK_PV_LIST (phys, pvl, "pmap_clear_modify");
a3907 2
    /* update correspoinding pmap_modify_list element */
    pmap_modify_list[pfi] = 0;
d3909 4
a3912 1
    if (pvl->pmap == PMAP_NULL) {
d3914 2
a3915 2
	if ((pmap_con_dbg & (CD_CMOD | CD_NORM)) == (CD_CMOD | CD_NORM))
	  printf("(pmap_clear_modify :%x) phys addr 0x%x not mapped\n", curproc, phys);
d3918 4
a3921 40
	SPLX(spl);
	return;
    }

    /* for each listed pmap, trun off the page modified bit */
    pvep = pvl;
    while (pvep != PV_ENTRY_NULL) {
	pmap = pvep->pmap;
	va = pvep->va;
	if (!simple_lock_try(&pmap->lock)) {
		goto clear_modify_Retry;
	}

	users = 0;
	if (pmap == kernel_pmap) {
	    kflush = 1;
	} else {
	    kflush = 0;
	}

	pte = pmap_pte(pmap, va);
	if (pte == PT_ENTRY_NULL)
	  panic("pmap_clear_modify: bad pv list entry.");

	for (i = ptes_per_vm_page; i > 0; i--) {

	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    spl_sav = splimp();
	    opte.bits = invalidate_pte(pte);
	    /* clear modified bit */
	    opte.pte.modified = 0;
	    ((pte_template_t *)pte)->bits = opte.bits;
	    flush_atc_entry(users, va, kflush);
	    splx(spl_sav);
	    pte++;
	    va += M88K_PGBYTES;
	}
d3923 20
a3942 1
	simple_unlock(&pmap->lock);
d3944 16
a3959 2
	pvep = pvep->next;
    }
d3961 7
a3967 1
    SPLX(spl);
d4012 7
a4018 7
	pv_entry_t	pvl;
	int		pfi;
	pv_entry_t	pvep;
	pt_entry_t	*ptep;
	int		spl;
	int		i;
	boolean_t	modified_flag;
d4020 1
a4020 1
	if (!PMAP_MANAGED(phys)) {
d4022 2
a4023 2
	    if (pmap_con_dbg & CD_IMOD)
		printf("(pmap_is_modified :%x) phys addr 0x%x not managed\n", curproc, phys);
d4025 2
a4026 2
		return(FALSE);
	}
d4028 1
a4028 1
	SPLVM(spl);
d4030 8
a4037 8
	pfi = PFIDX(phys);
	pvl = PFIDX_TO_PVH(pfi);
	CHECK_PV_LIST (phys, pvl, "pmap_is_modified");
is_mod_Retry:

	if ((boolean_t) pmap_modify_list[pfi]) {
		/* we've already cached a modify flag for this page,
			no use looking further... */
d4039 2
a4040 2
	    if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
		printf("(pmap_is_modified :%x) already cached a modify flag for this page\n", curproc);
d4042 9
a4050 8
		SPLX(spl);
		return(TRUE);
	}

	if (pvl->pmap == PMAP_NULL) {
		/* unmapped page - get info from page_modified array
			maintained by pmap_remove_range/ pmap_remove_all */
		modified_flag = (boolean_t) pmap_modify_list[pfi];
d4052 2
a4053 2
		if ((pmap_con_dbg & (CD_IMOD | CD_NORM)) == (CD_IMOD | CD_NORM))
		  printf("(pmap_is_modified :%x) phys addr 0x%x not mapped\n", curproc, phys);
d4055 21
a4075 20
		SPLX(spl);
		return(modified_flag);
	}

	/* for each listed pmap, check modified bit for given page */
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
		if (!simple_lock_try(&pvep->pmap->lock)) {
			UNLOCK_PVH(pfi);
			goto is_mod_Retry;
		}

		ptep = pmap_pte(pvep->pmap, pvep->va);
		if (ptep == PT_ENTRY_NULL) {
			printf("pmap_is_modified: pte from pv_list not in map virt = 0x%x\n", pvep->va);
			panic("pmap_is_modified: bad pv list entry");
		}
		for (i = ptes_per_vm_page; i > 0; i--) {
			if (ptep->modified) {
				simple_unlock(&pvep->pmap->lock);
d4077 2
a4078 2
				if ((pmap_con_dbg & (CD_IMOD | CD_FULL)) == (CD_IMOD | CD_FULL))
				  printf("(pmap_is_modified :%x) modified page pte@@0x%x\n", curproc, (unsigned)ptep);
d4080 7
a4086 6
				SPLX(spl);
				return(TRUE);
			}
			ptep++;
		}
		simple_unlock(&pvep->pmap->lock);
d4088 2
a4089 2
		pvep = pvep->next;
	}
d4091 3
a4093 2
	SPLX(spl);
	return(FALSE);
d4135 11
a4145 11
    pv_entry_t		pvl;
    int			pfi;
    pv_entry_t		pvep;
    pt_entry_t		*pte;
    pmap_t		pmap;
    int			spl, spl_sav;
    vm_offset_t		va;
    int			i;
    unsigned		users;
    pte_template_t	opte;
    int			kflush;
d4147 1
a4147 1
    if (!PMAP_MANAGED(phys)) {
d4149 3
a4151 3
	if (pmap_con_dbg & CD_CREF) {
	    printf("(pmap_clear_reference :%x) phys addr 0x%x not managed\n", curproc,phys);
	}
d4153 2
a4154 2
	return;
    }
d4156 1
a4156 1
    SPLVM(spl);
d4158 5
a4162 4
clear_reference_Retry:
    pfi = PFIDX(phys);
    pvl = PFIDX_TO_PVH(pfi);
    CHECK_PV_LIST(phys, pvl, "pmap_clear_reference");
d4165 1
a4165 1
    if (pvl->pmap == PMAP_NULL) {
d4167 2
a4168 2
	if ((pmap_con_dbg & (CD_CREF | CD_NORM)) == (CD_CREF | CD_NORM))
	  printf("(pmap_clear_reference :%x) phys addr 0x%x not mapped\n", curproc,phys);
d4170 27
a4196 40
	SPLX(spl);
	return;
    }

    /* for each listed pmap, turn off the page refrenced bit */
    pvep = pvl;
    while (pvep != PV_ENTRY_NULL) {
	pmap = pvep->pmap;
	va = pvep->va;
	if (!simple_lock_try(&pmap->lock)) {
		goto clear_reference_Retry;
	}

	users = 0;
	if (pmap == kernel_pmap) {
	    kflush = 1;
	} else {
	    kflush = 0;
	}

	pte = pmap_pte(pmap, va);
	if (pte == PT_ENTRY_NULL)
	  panic("pmap_clear_reference: bad pv list entry.");

	for (i = ptes_per_vm_page; i > 0; i--) {

	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    spl_sav = splimp();
	    opte.bits = invalidate_pte(pte);
	    /* clear reference bit */
	    opte.pte.pg_used = 0;
	    ((pte_template_t *)pte)->bits = opte.bits;
	    flush_atc_entry(users, va, kflush);
	    splx(spl_sav);
	    pte++;
	    va += M88K_PGBYTES;
	}
d4198 16
a4213 1
	simple_unlock(&pmap->lock);
d4215 2
a4216 2
	pvep = pvep->next;
    }
d4218 2
a4219 1
    SPLX(spl);
d4262 53
a4314 49
    pv_entry_t	pvl;
    int		pfi;
    pv_entry_t	pvep;
    pt_entry_t	*ptep;
    int		spl;
    int		i;
    
    if (!PMAP_MANAGED(phys))
      return(FALSE);
    
    SPLVM(spl);
    
    pfi = PFIDX(phys);
    pvl = PFIDX_TO_PVH(pfi);
    CHECK_PV_LIST(phys, pvl, "pmap_is_referenced");
    
is_ref_Retry:

    if (pvl->pmap == PMAP_NULL) {
	SPLX(spl);
	return(FALSE);
    }

    /* for each listed pmap, check used bit for given page */
    pvep = pvl;
    while (pvep != PV_ENTRY_NULL) {
	if (!simple_lock_try(&pvep->pmap->lock)) {
		UNLOCK_PVH(pfi);
		goto is_ref_Retry;
	}

	ptep = pmap_pte(pvep->pmap, pvep->va);
	if (ptep == PT_ENTRY_NULL)
	  panic("pmap_is_referenced: bad pv list entry.");
	for (i = ptes_per_vm_page; i > 0; i--) {
	    if (ptep->pg_used) {
		simple_unlock(&pvep->pmap->lock);
		SPLX(spl);
		return(TRUE);
	    }
	    ptep++;
	}
	simple_unlock(&pvep->pmap->lock);

	pvep = pvep->next;
    }
    
    SPLX(spl);
    return(FALSE);
d4350 3
a4352 3
	pv_entry_t	pv_h;
	int		spl;
	boolean_t	result;
d4354 2
a4355 2
	if (!pmap_initialized)
		return(TRUE);
d4357 2
a4358 2
	if (!PMAP_MANAGED(phys))
		return(FALSE);
d4360 1
a4360 1
	SPLVM(spl);
d4362 2
a4363 1
	pv_h = PFIDX_TO_PVH(PFIDX(phys));
d4365 3
a4367 2
	result = (pv_h->pmap == PMAP_NULL);
	SPLX(spl);
d4369 1
a4369 1
	return(result);
d4384 1
a4384 1
	p++;
d4386 2
a4387 2
	return(TRUE);
}	/* pmap_valid_page() */
d4401 11
a4411 11
	switch (prot) {
		case VM_PROT_READ:
		case VM_PROT_READ|VM_PROT_EXECUTE:
			pmap_copy_on_write(phys);
			break;
		case VM_PROT_ALL:
			break;
		default:
			pmap_remove_all(phys);
			break;
	}
d4442 22
a4463 37
    vm_offset_t		pa;
    pt_entry_t		*srcpte, *dstpte;
    int			pfi;
    pv_entry_t		pvl;
    int			spl;
    int			i;
    unsigned		users;
    pte_template_t	opte;

    PMAP_LOCK(kernel_pmap, spl);

    users = 0;

    while (size > 0) {

	/*
	 * check if the source addr is mapped
	 */
	if ((srcpte = pmap_pte(kernel_pmap, (vm_offset_t)from)) == PT_ENTRY_NULL) {
	    printf("pagemove: source vaddr 0x%x\n", from);
	    panic("pagemove: Source addr not mapped");
	}

	/*
	 *
	 */
	if ((dstpte = pmap_pte(kernel_pmap, (vm_offset_t)to)) == PT_ENTRY_NULL)
	  if ((dstpte = pmap_expand_kmap((vm_offset_t)to, VM_PROT_READ | VM_PROT_WRITE))
	      == PT_ENTRY_NULL)
	    panic("pagemove: Cannot allocate distination pte");
	/*
	 *
	 */
	if (dstpte->dtype == DT_VALID) {
	    printf("pagemove: distination vaddr 0x%x, pte = 0x%x\n", to, *((unsigned *)dstpte));
	    panic("pagemove: Distination pte already valid");
	}
d4465 14
a4478 5
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_PGMV | CD_NORM)) == (CD_PGMV | CD_NORM))
	  printf("(pagemove :%x) from 0x%x to 0x%x\n", curproc, from, to);
	if ((pmap_con_dbg & (CD_PGMV | CD_FULL)) == (CD_PGMV | CD_FULL))
	  printf("(pagemove :%x) srcpte @@ 0x%x = %x dstpte @@ 0x%x = %x\n", curproc, (unsigned)srcpte, *(unsigned *)srcpte, (unsigned)dstpte, *(unsigned *)dstpte);
d4480 20
a4499 1
#endif /* DEBUG */
d4501 17
a4517 28
	/*
	 * Update pv_list
	 */
	pa = M88K_PTOB(srcpte->pfn);
	if (PMAP_MANAGED(pa)) {
	    pfi = PFIDX(pa);
	    pvl = PFIDX_TO_PVH(pfi);
	    CHECK_PV_LIST(pa, pvl, "pagemove");
	    pvl->va = (vm_offset_t)to;
	}

	/*
	 * copy pte
	 */
	for (i = ptes_per_vm_page; i > 0; i--) {
	    /*
	     * Invalidate pte temporarily to avoid being written back
	     * the modified bit and/or the reference bit by other cpu.
	     */
	    opte.bits = invalidate_pte(srcpte);
	    flush_atc_entry(users, from, 1);
	    ((pte_template_t *)dstpte)->bits = opte.bits;
	    from += M88K_PGBYTES;
	    to += M88K_PGBYTES;
	    srcpte++; dstpte++;
	}
	size -= PAGE_SIZE;
    }
d4519 1
a4519 1
    PMAP_UNLOCK(kernel_pmap, spl);
d4550 2
a4551 2
    int 	i;
    int 	cpu = 0;
d4553 7
a4559 3
    for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
	 cmmu_flush_remote_inst_cache(cpu, pa, M88K_PGBYTES);
    }
d4584 13
a4596 13
    vm_offset_t pa;
    int 	i;
    int		spl;

    if (pmap == PMAP_NULL)
	panic("pmap_dcache_flush: pmap is NULL");

    PMAP_LOCK(pmap, spl);

    pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
    for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
	cmmu_flush_data_cache(pa, M88K_PGBYTES);
    }
d4598 1
a4598 1
    PMAP_UNLOCK(pmap, spl);
d4606 49
a4654 48
    int		i;
    int		ncpus;
    void 	(*cfunc)(int cpu, vm_offset_t physaddr, int size);

    switch (mode) {
    default:
	panic("bad cache_flush_loop mode");
	return;

    case FLUSH_CACHE:	/* All caches, all CPUs */
	ncpus = NCPUS;
	cfunc = cmmu_flush_remote_cache;
	break;

    case FLUSH_CODE_CACHE:	/* Instruction caches, all CPUs */
	ncpus = NCPUS;
	cfunc = cmmu_flush_remote_inst_cache;
	break;
    
    case FLUSH_DATA_CACHE:	/* Data caches, all CPUs */
	ncpus = NCPUS;
	cfunc = cmmu_flush_remote_data_cache;
	break;

    case FLUSH_LOCAL_CACHE:		/* Both caches, my CPU */
	ncpus = 1;
	cfunc = cmmu_flush_remote_cache;
	break;
    
    case FLUSH_LOCAL_CODE_CACHE:	/* Instruction cache, my CPU */
	ncpus = 1;
	cfunc = cmmu_flush_remote_inst_cache;
	break;

    case FLUSH_LOCAL_DATA_CACHE:	/* Data cache, my CPU */
	ncpus = 1;
	cfunc = cmmu_flush_remote_data_cache;
	break;
    }

    if (ncpus == 1) {
	(*cfunc)(cpu_number(), pa, size);
    }
    else {
	for (i=0; i<NCPUS; i++) {
		(*cfunc)(i, pa, size);
	}
    }
d4664 25
a4688 25
    vm_offset_t pa;
    vm_offset_t va;
    int 	i;
    int		spl;

    if (pmap == PMAP_NULL)
	panic("pmap_dcache_flush: NULL pmap");

    /*
     * If it is more than a couple of pages, just blow the whole cache
     * because of the number of cycles involved.
     */
    if (bytes > 2*M88K_PGBYTES) {
	cache_flush_loop(mode, 0, -1);
	return;
    }

    PMAP_LOCK(pmap, spl);
    for(va = virt; bytes > 0; bytes -= M88K_PGBYTES,va += M88K_PGBYTES) {
        pa = M88K_PTOB((pmap_pte(pmap, va))->pfn);
        for (i = ptes_per_vm_page; i > 0; i--, pa += M88K_PGBYTES) {
	    cache_flush_loop(mode, pa, M88K_PGBYTES);
        }
    }
    PMAP_UNLOCK(pmap, spl);
d4732 39
a4770 40
	pv_entry_t	pv_e;
	pt_entry_t 	*pte;
	vm_offset_t	pa;

	if (pv_h != PFIDX_TO_PVH(PFIDX(phys))) {
		printf("check_pv_list: incorrect pv_h supplied.\n");
		panic(who);
	}

	if (!PAGE_ALIGNED(phys)) {
		printf("check_pv_list: supplied phys addr not page aligned.\n");
		panic(who);
	}

	if (pv_h->pmap == PMAP_NULL) {
		if (pv_h->next != PV_ENTRY_NULL) {
			printf("check_pv_list: first entry has null pmap, but list non-empty.\n");
			panic(who);
		}
		else	return;		/* proper empry lst */
	}

	pv_e = pv_h;
	while (pv_e != PV_ENTRY_NULL) {
		if (!PAGE_ALIGNED(pv_e->va)) {
			printf("check_pv_list: non-aligned VA in entry at 0x%x.\n", pv_e);
			panic(who);
		}
		/*
		 * We can't call pmap_extract since it requires lock.
		 */
		if ((pte = pmap_pte(pv_e->pmap, pv_e->va)) == PT_ENTRY_NULL)
		  pa = (vm_offset_t)0;
		else
		  pa = M88K_PTOB(pte->pfn) | (pv_e->va & M88K_PGOFSET);

		if (pa != phys) {
			printf("check_pv_list: phys addr diff in entry at 0x%x.\n", pv_e);
			panic(who);
		}
d4772 2
a4773 2
		pv_e = pv_e->next;
	}
d4811 92
a4902 93
	vm_offset_t	va,
			old_va,
			phys;
	pv_entry_t	pv_h,
			pv_e,
			saved_pv_e;
	pt_entry_t	*ptep;
	boolean_t	found;
	int		loopcnt;


	/*
	 * for each page in the address space, check to see if there's
	 * a valid mapping. If so makes sure it's listed in the PV_list.
	 */

	if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
		printf("(check_map) checking map at 0x%x\n", map);

	old_va = s;
	for (va = s; va < e; va += PAGE_SIZE) {
		/* check for overflow - happens if e=0xffffffff */
		if (va < old_va)
			break;
		else
			old_va = va;

		if (va == phys_map_vaddr1 || va == phys_map_vaddr2)
			/* don't try anything with these */
			continue;

		ptep = pmap_pte(map, va);

		if (ptep == PT_ENTRY_NULL) {
			/* no page table, skip to next segment entry */
			va = SDT_NEXT(va)-PAGE_SIZE;
			continue;
		}

		if (!PDT_VALID(ptep))
			continue;		/* no page mapping */

		phys = M88K_PTOB(ptep->pfn);	/* pick up phys addr */

		if (!PMAP_MANAGED(phys))
			continue;		/* no PV list */

		/* note: vm_page_startup allocates some memory for itself
			through pmap_map before pmap_init is run. However,
			it doesn't adjust the physical start of memory.
			So, pmap thinks those pages are managed - but they're
			not actually under it's control. So, the following
			conditional is a hack to avoid those addresses
			reserved by vm_page_startup */
		/* pmap_init also allocate some memory for itself. */

		if (map == kernel_pmap &&
		    va < round_page((vm_offset_t)(pmap_modify_list + (pmap_phys_end - pmap_phys_start))))
			continue;

		pv_h = PFIDX_TO_PVH(PFIDX(phys));
		found = FALSE;

		if (pv_h->pmap != PMAP_NULL) {

			loopcnt = 10000;	/* loop limit */
			pv_e = pv_h;
			while(pv_e != PV_ENTRY_NULL) {

				if (loopcnt-- < 0) {
					printf("check_map: loop in PV list at PVH 0x%x (for phys 0x%x)\n", pv_h, phys);
					panic(who);
				}

				if (pv_e->pmap == map && pv_e->va == va) {
					if (found) {
						printf("check_map: Duplicate PV list entries at 0x%x and 0x%x in PV list 0x%x.\n", saved_pv_e, pv_e, pv_h);
						printf("check_map: for pmap 0x%x, VA 0x%x,phys 0x%x.\n", map, va, phys);
						panic(who);
					}
					else {
						found = TRUE;
						saved_pv_e = pv_e;
					}
				}
				pv_e = pv_e->next;
			}
		}

		if (!found) {
				printf("check_map: Mapping for pmap 0x%x VA 0x%x Phys 0x%x does not appear in PV list 0x%x.\n", map, va, phys, pv_h);
		}
	}
d4904 2
a4905 2
	if ((pmap_con_dbg & (CD_CHKM | CD_NORM)) == (CD_CHKM | CD_NORM))
		printf("(check_map) done \n");
d4946 19
a4964 35
	pmap_t		p;
	int		i;
	vm_offset_t	phys;
	pv_entry_t	pv_h;
	int		spl;

	if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
		printf("check_pmap_consistency (%s :%x) start.\n", who, curproc);

	if (pv_head_table == PV_ENTRY_NULL) {

		printf("check_pmap_consistency (%s) PV head table not initialized.\n", who);
		return;
	}

	SPLVM(spl);

	p = kernel_pmap;
	check_map(p, VM_MIN_KERNEL_ADDRESS, VM_MAX_KERNEL_ADDRESS, who);

	/* run through all pmaps. check consistency of each one... */
	i = PMAP_MAX;
	for (p = kernel_pmap->next;p != kernel_pmap; p = p->next) {
		if (i == 0) { /* can not read pmap list */
			printf("check_pmap_consistency: pmap strcut loop error.\n");
			panic(who);
		}
		check_map(p, VM_MIN_USER_ADDRESS, VM_MAX_USER_ADDRESS, who);
	}

	/* run through all managed paes, check pv_list for each one */
	for (phys = pmap_phys_start; phys < pmap_phys_end; phys += PAGE_SIZE) {
		pv_h = PFIDX_TO_PVH(PFIDX(phys));
		check_pv_list(phys, pv_h, who);
	}
d4966 15
a4980 1
	SPLX(spl);
d4982 4
a4985 2
	if ((pmap_con_dbg & (CD_CHKPM | CD_NORM)) == (CD_CHKPM | CD_NORM))
		printf("check_pmap consistency (%s :%x): done.\n",who, curproc);
d5034 10
a5043 10
    sdt_entry_t	*sdtp;
    sdt_entry_t	*sdtv;
    int		i;

    printf("Pmap @@ 0x%x:\n", (unsigned)pmap);
    sdtp = pmap->sdt_paddr;
    sdtv = pmap->sdt_vaddr;
    printf("	sdt_paddr: 0x%x; sdt_vaddr: 0x%x; ref_count: %d;\n",
	    (unsigned)sdtp, (unsigned)sdtv,
	    pmap->ref_count);
d5046 45
a5090 46
    printf("	statistics: pagesize %d: free_count %d; "
	   "active_count %d; inactive_count %d; wire_count %d\n",
		pmap->stats.pagesize,
		pmap->stats.free_count,
		pmap->stats.active_count,
		pmap->stats.inactive_count,
		pmap->stats.wire_count);

    printf("	zero_fill_count %d; reactiveations %d; "
	   "pageins %d; pageouts %d; faults %d\n",
		pmap->stats.zero_fill_count,
		pmap->stats.reactivations,
		pmap->stats.pageins,
		pmap->stats.pageouts,
		pmap->stats.fault);

    printf("	cow_faults %d, lookups %d, hits %d\n",
		pmap->stats.cow_faults,
		pmap->stats.loopups,
		pmap->stats.faults);
#endif

    sdtp = (sdt_entry_t *) pmap->sdt_vaddr;	/* addr of physical table */
    sdtv = sdtp + SDT_ENTRIES;		/* shadow table with virt address */
    if (sdtp == (sdt_entry_t *)0)
	printf("Error in pmap - sdt_paddr is null.\n");
    else {
	int	count = 0;
	printf("	Segment table at 0x%x (0x%x):\n",
	    (unsigned)sdtp, (unsigned)sdtv);
	for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
	    if ((sdtp->table_addr != 0 ) || (sdtv->table_addr != 0)) {
		if (count != 0)
			printf("sdt entry %d skip !!\n", count);
		count = 0;
		printf("   (%x)phys: ", i);
		PRINT_SDT(sdtp);
		printf("   (%x)virt: ", i);
		PRINT_SDT(sdtv);
	    }
	    else
		count++;
	}
	if (count != 0)
	    printf("sdt entry %d skip !!\n", count);
    }
d5117 107
a5223 107
    sdt_entry_t	*sdtp;	/* ptr to sdt table of physical addresses */
    sdt_entry_t	*sdtv;	/* ptr to sdt shadow table of virtual addresses */
    pt_entry_t	*ptep;	/* ptr to pte table of physical page addresses */

    int		i;	/* table loop index */
    unsigned long prev_entry;	/* keep track of value of previous table entry */
    int		n_dup_entries;	/* count contiguous duplicate entries */

    printf("Trace of virtual address 0x%08x. Pmap @@ 0x%08x.\n",
	va, (unsigned)pmap);

    /*** SDT TABLES ***/
    /* get addrs of sdt tables */
    sdtp = (sdt_entry_t *)pmap->sdt_vaddr;
    sdtv = sdtp + SDT_ENTRIES;

    if (sdtp == SDT_ENTRY_NULL) {
	printf("    Segment table pointer (pmap.sdt_paddr) null, trace stops.\n");
	return;
    }

    n_dup_entries = 0;
    prev_entry = 0xFFFFFFFF;

    if (long_format) {
	printf("    Segment table at 0x%08x (virt shadow at 0x%08x)\n",
		(unsigned)sdtp, (unsigned)sdtv);
	for (i = 0; i < SDT_ENTRIES; i++, sdtp++, sdtv++) {
	    if (prev_entry == ((sdt_entry_template_t *)sdtp)->bits
		&& SDTIDX(va) != i && i != SDT_ENTRIES-1) {
		    n_dup_entries++;
		    continue;	/* suppress duplicate entry */
	    }
	    if (n_dup_entries != 0) {
		printf("    - %d duplicate entries skipped -\n",n_dup_entries);
		n_dup_entries = 0;
	    }
	    prev_entry = ((pte_template_t *)sdtp)->bits;
	    if (SDTIDX(va) == i) {
		printf("    >> (%x)phys: ", i);
	    } else {
		printf("       (%x)phys: ", i);
	    }
	    PRINT_SDT(sdtp);
	    if (SDTIDX(va) == i) {
		printf("    >> (%x)virt: ", i);
	    } else {
		printf("       (%x)virt: ", i);
	    }
	    PRINT_SDT(sdtv);
	} /* for */
    } else {
	/* index into both tables for given VA */
	sdtp += SDTIDX(va);
	sdtv += SDTIDX(va);
	printf("    SDT entry index 0x%x at 0x%x (virt shadow at 0x%x)\n",
	       SDTIDX(va), (unsigned)sdtp, (unsigned)sdtv);
	printf("    phys:  ");
	PRINT_SDT(sdtp);
	printf("    virt:  ");
	PRINT_SDT(sdtv);
    }

    /*** PTE TABLES ***/
    /* get addrs of page (pte) table (no shadow table) */

    sdtp = ((sdt_entry_t *)pmap->sdt_vaddr) + SDTIDX(va);
    #ifdef DBG
	    printf("*** DEBUG (sdtp) ");
	    PRINT_SDT(sdtp);
    #endif
    sdtv = sdtp + SDT_ENTRIES;
    ptep = (pt_entry_t *)(M88K_PTOB(sdtv->table_addr));
    if (sdtp->dtype != DT_VALID) {
	printf("    segment table entry invlid, trace stops.\n");
	return;
    }

    n_dup_entries = 0;
    prev_entry = 0xFFFFFFFF;
    if (long_format) {
	printf("        page table (ptes) at 0x%x\n", (unsigned)ptep);
	for (i = 0; i < PDT_ENTRIES; i++, ptep++) {
	    if (prev_entry == ((pte_template_t *)ptep)->bits
		&& PDTIDX(va) != i && i != PDT_ENTRIES-1) {
		    n_dup_entries++;
		    continue;	/* suppress suplicate entry */
	    }
	    if (n_dup_entries != 0) {
		printf("    - %d duplicate entries skipped -\n",n_dup_entries);
		n_dup_entries = 0;
	    }
	    prev_entry = ((pte_template_t *)ptep)->bits;
	    if (PDTIDX(va) == i) {
		printf("    >> (%x)pte: ", i);
	    } else	{
		printf("       (%x)pte: ", i);
	    }
	    PRINT_PDT(ptep);
	} /* for */
    } else {
	/* index into page table */
	ptep += PDTIDX(va);
	printf("    pte index 0x%x\n", PDTIDX(va));
	printf("    pte: ");
	PRINT_PDT(ptep);
    }
d5236 23
a5258 23
    pt_entry_t	*pte;
    sdt_entry_t	*sdt;
    int                         spl;

    PMAP_LOCK(pmap, spl);

    if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
	PMAP_UNLOCK(pmap, spl);
	return FALSE;
    }
    
    if (!PDT_VALID(pte)) {
	PMAP_UNLOCK(pmap, spl);
	return FALSE;
    }

    /*
     * Valid pte.  If the transaction was a read, there is no way it
     *  could have been a fault, so return true.  For now, assume
     *  that a write transaction could have caused a fault.  We need
     *  to check pte and sdt entries for write permission to really
     *  tell.
     */
d5260 13
a5272 13
    if (type == VM_PROT_READ) {
	PMAP_UNLOCK(pmap, spl);
	return TRUE;
    } else {
	sdt = SDTENT(pmap,va);
	if (sdt->prot || pte->prot) {
	    PMAP_UNLOCK(pmap, spl);
	    return FALSE;
	} else {
	    PMAP_UNLOCK(pmap, spl);
	    return TRUE;
	}
    }
d5280 2
a5281 2
    *startp = virtual_avail;
    *endp = virtual_end;
d5287 1
a5287 1
        return atop(avail_end - avail_next);
d5293 2
a5294 2
        if (avail_next == avail_end)
                return FALSE;
d5296 3
a5298 3
        *addrp = avail_next;
        avail_next += PAGE_SIZE;
        return TRUE;
d5302 1
a5302 1
#ifdef OMRON_PMAP
d5308 34
a5341 34
    pmap_t pmap,
    boolean_t data,
    int i,
    vm_offset_t va,
    vm_offset_t pa,
    boolean_t super,
    boolean_t wt,
    boolean_t global,
    boolean_t ci,
    boolean_t wp,
    boolean_t valid)
{
    register batc_template_t batctmp;

    if (i < 0 || i > (BATC_MAX - 1)) {
        panic("pmap_set_batc: illegal batc number");
        /* bad number */
        return;
    }

    batctmp.field.lba = va >> 19;
    batctmp.field.pba = pa >> 19;
    batctmp.field.sup = super;
    batctmp.field.wt = wt;
    batctmp.field.g = global;
    batctmp.field.ci = ci;
    batctmp.field.wp = wp;
    batctmp.field.v = valid;

    if (data) {
        pmap->d_batc[i].bits = batctmp.bits;
    } else {
        pmap->i_batc[i].bits = batctmp.bits;
    }
d5345 15
a5359 15
    task_t task,	
    boolean_t data,         /* for data-cmmu ? */
    int i,                  /* batc number */
    vm_offset_t va,         /* virtual address */
    vm_offset_t pa,         /* physical address */
    boolean_t s,            /* for super-mode ? */
    boolean_t wt,           /* is writethrough */
    boolean_t g,            /* is global ? */
    boolean_t ci,           /* is cache inhibited ? */
    boolean_t wp,           /* is write-protected ? */
    boolean_t v)            /* is valid ? */
{
    pmap_t pmap;
    pmap = vm_map_pmap(task->map);
    pmap_set_batc(pmap, data, i, va, pa, s, wt, g, ci, wp, v);
d5362 1
a5362 1
#endif
d5381 1
a5381 1
	pmap_range_t this, next;
d5383 7
a5389 7
	this = *ranges;
	while (this != 0) {
		next = this->next;
		pmap_range_free(this);
		this = next;
	      }
	*ranges = 0;
d5398 1
a5398 1
	pmap_range_t range;
d5400 7
a5406 7
	for (range = *ranges; range != 0; range = range->next) {
		if (address < range->start)
			return FALSE;
		if (address < range->end)
			return TRUE;
	}
	return FALSE;
d5416 1
a5416 1
	pmap_range_t range, *prev;
d5418 1
a5418 1
	/* look for the start address */
d5420 6
a5425 6
	for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
		if (start < range->start)
			break;
		if (start <= range->end)
			goto start_overlaps;
	}
d5427 1
a5427 1
	/* start address is not present */
d5429 2
a5430 2
	if ((range == 0) || (end < range->start)) {
		/* no overlap; allocate a new range */
d5432 7
a5438 7
		range = pmap_range_alloc();
		range->start = start;
		range->end = end;
		range->next = *prev;
		*prev = range;
		return;
	}
d5440 1
a5440 1
	/* extend existing range forward to start */
d5442 1
a5442 1
	range->start = start;
d5444 2
a5445 2
    start_overlaps:
	assert((range->start <= start) && (start <= range->end));
d5447 1
a5447 1
	/* delete redundant ranges */
d5449 2
a5450 2
	while ((range->next != 0) && (range->next->start <= end)) {
		pmap_range_t old;
d5452 5
a5456 5
		old = range->next;
		range->next = old->next;
		range->end = old->end;
		pmap_range_free(old);
	}
d5458 1
a5458 1
	/* extend existing range backward to end */
d5460 2
a5461 2
	if (range->end < end)
		range->end = end;
d5471 1
a5471 1
	pmap_range_t range, *prev;
d5473 1
a5473 1
	/* look for start address */
d5475 6
a5480 6
	for (prev = ranges; (range = *prev) != 0; prev = &range->next) {
		if (start <= range->start)
			break;
		if (start < range->end) {
			if (end < range->end) {
				pmap_range_t new;
d5482 1
a5482 1
				/* split this range */
d5484 4
a5487 4
				new = pmap_range_alloc();
				new->next = range->next;
				new->start = end;
				new->end = range->end;
d5489 4
a5492 4
				range->next = new;
				range->end = start;
				return;
			}
d5494 1
a5494 1
			/* truncate this range */
d5496 3
a5498 3
			range->end = start;
		}
	}
d5500 1
a5500 1
	/* start address is not in the middle of a range */
d5502 5
a5506 5
	while ((range != 0) && (range->end <= end)) {
		*prev = range->next;
		pmap_range_free(range);
		range = *prev;
	}
d5508 2
a5509 2
	if ((range != 0) && (range->start < end))
		range->start = end;
d5512 1
@


1.10
log
@pmap_activate() and pmap_deactivate() are MD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 1999/05/29 04:41:46 smurph Exp $	*/
d2732 2
a2733 1
					vm_prot_t prot, boolean_t wired)
@


1.9
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1999/07/18 16:23:47 deraadt Exp $	*/
d3365 1
a3365 1
pmap_activate(struct proc *p)
a3366 3
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

d3388 1
a3388 1
pmap_deactivate(p)
a3389 3
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

@


1.8
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 1999/05/29 04:41:46 smurph Exp $	*/
d3365 1
a3365 1
pmap_activate(pmap_t pmap)
d3368 1
d3391 1
a3391 1
pmap_deactivate(pmap_t pmap)
d3394 1
@


1.7
log
@Added vme bus device drivers. MVME328, MVME376, MVME332
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 1999/02/09 06:36:30 smurph Exp $	*/
d3365 1
a3365 1
pmap_activate(pmap_t pmap, pcb_t pcb)
d3367 2
d3390 1
a3390 1
pmap_deactivate(pmap_t pmap, pcb_t pcb)
d3392 2
@


1.6
log
@Added kernel support for user debugging.  Fixed file ID's
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 1995/04/19 22:37:27 smurph Exp $	*/
d260 1
a260 1
void	*etherbuf;
d1128 1
a1128 1
    if (kmapva != vaddr) {
d1130 3
a1132 3
    	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	    printf("(pmap_bootstrap) correcting vaddr\n");
	}
d1134 3
a1136 3
	while (vaddr < (*virt_start - kernel_pmap_size))
	  vaddr = M88K_ROUND_PAGE(vaddr + 1);
    }
d1138 13
a1150 5
    vaddr = PMAPER(
		vaddr,
		(vm_offset_t)kmap,
		*phys_start,
		(VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
a1151 1
    etherlen = ETHERPAGES * NBPG;
d1154 1
a1154 1
     *	Get ethernet buffer - need etherlen bytes physically contiguous.
d1159 1
a1159 12

    if (vaddr != *virt_start) {
#ifdef DEBUG
        if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
 	    printf("1:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
		*virt_start, *phys_start);
	}
#endif
	*virt_start = vaddr;
	*phys_start = round_page(*phys_start);
    }

d1161 1
a1161 1

d1163 3
a1165 7

    vaddr = PMAPER(
		vaddr,
		*phys_start,
		*phys_start + etherlen,
		(VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));

d1167 2
a1168 2
    *phys_start += etherlen;

d1171 4
a1174 4
        if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	     printf("2:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
		*virt_start, *phys_start);
	}
d1176 2
a1177 2
	*virt_start = vaddr;
	*phys_start = round_page(*phys_start);
a1179 2
    *virt_start = round_page(*virt_start);
    *virt_end = VM_MAX_KERNEL_ADDRESS;
d1181 2
a1182 11
    /*
     * Map a few more pages for phys routines and debugger.
     */

    phys_map_vaddr1 = round_page(*virt_start);
    phys_map_vaddr2 = phys_map_vaddr1 + PAGE_SIZE;

    /*
     * To make 1:1 mapping of virt:phys, throw away a few phys pages.
     * XXX what is this? nivas
     */
d1184 11
d1196 2
a1197 2
    *phys_start += 2 * PAGE_SIZE;
    *virt_start += 2 * PAGE_SIZE;
d1199 9
a1207 9
    /*
     * Map all IO space 1-to-1. Ideally, I would like to not do this
     * but have va for the given IO address dynamically allocated. But
     * on the 88200, 2 of the BATCs are hardwired to do map the IO space
     * 1-to-1; I decided to map the rest of the IO space 1-to-1.
     * And bug ROM & the SRAM need to be mapped 1-to-1 if we ever want to
     * execute bug system calls after the MMU has been turned on.
     * OBIO should be mapped cache inhibited.
     */
d1209 1
a1209 1
    PMAPER(
d1215 1
a1215 1
    PMAPER(
d1221 1
a1221 1
    PMAPER(
d1227 20
d1252 2
a1253 1
	 * at different times, we better do a tlb flush before using it -	         * else we will be referencing the wrong page.
@


1.5
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
@


1.4
log
@Commit for the first working mvme88k port.
@
text
@d998 1
a998 1
	panic("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x\n", load_start);
d5158 1
a5158 1
        panic("pmap_set_batc: illegal batc number\n");
@


1.3
log
@Cleanup after import. This also seems to bring up the current version.
@
text
@d53 1
a53 1
#include <vm/pmap.h>
d55 1
a57 1
#include <sys/param.h>
d75 3
d118 2
a119 1
int pmap_con_dbg = CD_FULL|CD_NORM;
d286 1
d994 3
a996 3
    if (ptes_per_vm_page == 0)
		panic("pmap_bootstrap: VM page size < MACHINE page size");

d1105 1
a1105 1
		(VM_PROT_READ|VM_PROT_WRITE)|(CACHE_INH <<16));
d1288 6
a1293 1

d1300 1
d1302 1
d2462 3
@


1.2
log
@This is a remove to get rid of the old mvme88k port which was incomplete
to replace it with a working version. The kernel compiles and works
at least.  The new version will be imported shortly.
@
text
@d2 41
a42 1
 * HISTORY
d54 1
a54 1
#include <machine/m882xx.h>/* CMMU stuff */
a55 5
#include <assym.s>

/*#ifdef luna88k*/
# define splblock splhigh
/*#endif */
a57 1
#include <sys/time.h>
d63 2
d72 2
a73 6
#if 0
/*
 * Machine configuration stuff 
 */
pmap_table_t pmap_table_build();
#endif /* 0 */
d79 1
a79 5
#define	static

boolean_t code_cache_enable = TRUE;
boolean_t data_cache_enable = TRUE;
boolean_t kernel_text_ro = FALSE;	/* If TRUE kernel text set READ ONLY */
d116 1
d118 1
a118 1
#endif /* DBG */
d120 1
a120 11
int cmmumap = 0;
int mapallio = 1;
int mapextra = 1;
int mydebug = 0;
extern proc0paddr;

/*
 * All those kernel PT submaps that BSD is so fond of
 */
caddr_t	CADDR1, CADDR2, vmmap;
u_int	*CMAP1, *CMAP2, *vmpte, *msgbufmap;
d122 2
a123 15
/*
 * PHYS_TO_VM_PAGE and vm_page_set_modified, called by pmap_remove_range
 * and pmap_remove_all, are still stubbed out.
 *
 * VM-routines would keep truck of the page status through calling
 * pmap_is_modified.
 */

#ifndef PHYS_TO_VM_PAGE
#define	PHYS_TO_VM_PAGE(pa)
#endif

#ifndef vm_page_set_modified
#define	vm_page_set_modified(m)
#endif
d125 1
a125 1
static struct pmap 	kernel_pmap_store;
d131 1
a131 1
		vm_offset_t		phys;
d135 1
a135 1
static kpdt_entry_t		kpdt_free;
d138 1
a138 1
 * MAX_KERNEL_VA_SIZE must be fit into the virtual address space between
a140 1
#define	MAX_KERNEL_VA_SIZE	(256*1024*1024)
d142 1
d147 1
a147 1
#define	MAX_KERNEL_PDT_SIZE	(M88K_BTOP(MAX_KERNEL_VA_SIZE) * sizeof(pt_entry_t))
d149 6
d162 1
a162 2
int		ptes_per_vm_page; /* number of M88K ptes required to map one VM page */

d179 3
a181 3
 * mappings of that page. An entry is a pv_entry_t; the list is the pv_table.
 * This is used by things like pmap_remove, when we must find and remove all
 * mappings for a particular physical page.
d254 4
d267 1
a267 1
#define PMAP_MANAGED(pa) ((pa) >= pmap_phys_start && (pa) < pmap_phys_end)
d283 4
d312 3
d316 2
a317 1
static void flush_atc_entry(unsigned users, vm_offset_t va, int kernel)
a318 3
		/* always flush cpu 0 TLB till we understand if this
		   is required XXX -nivas */
/*    if (users) */
d349 1
a349 4
_pmap_activate(
    register pmap_t pmap,
    register pcb_t pcb,
    register int my_cpu)
d351 2
a352 2
    register apr_template_t	apr_data;
    register int n;
d371 1
d385 7
d434 1
a434 4
_pmap_deactivate(
    register pmap_t	pmap,
    register pcb_t	pcb,
    register int	my_cpu)
d453 2
a454 3
static int unsigned m88k_protection(
    pmap_t	map,
    vm_prot_t	prot)
d456 1
a456 1
    register pte_template_t p;
d458 2
a459 2
    p.bits = 0;
    p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;
d461 1
a461 1
    return(p.bits);
a468 2
 * Author: Joe Uemura
 *
a485 3
 *
 * History:
 *		90/9/12	Fuzzy	if pmap == PMAP_NULL, panic
d487 3
a489 3
pt_entry_t * pmap_pte(
     pmap_t	map,
     vm_offset_t virt)
d491 1
a491 1
    sdt_entry_t	*sdt;
d493 4
a496 2
    if (map == PMAP_NULL)
      panic("pmap_pte: pmap is NULL");
d498 1
a498 1
    sdt = SDTENT(map,virt);
d500 8
a507 7
    /*
     * Check whether page table is exist or not.
     */
    if (!SDT_VALID(sdt))
      return(PT_ENTRY_NULL);
    else
      return((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) + PDTIDX(virt));
a514 16
 *	History:
 *	'90.8.3		Fuzzy
 *		if defined TEST, 'static' undeclared.
 *	'90.8.27	Fuzzy
 *		allocated pte entry clear
 *	'90.8.28	Fuzzy
 *		Bug: No free kernel page table process
 *		panic("pmap_expand_kmap:...");
 *		--> #ifdef DBG
 *		      printf("Warnning: Ran out of page table entry VALID\n");
 *		    #endif
 *	'90.8.30		Fuzzy
 *		delete "if defined TEST, 'static' undeclared."
 *
 * Author: Fuzzy
 *
d548 2
a549 3
static pt_entry_t * pmap_expand_kmap(
    vm_offset_t		virt,
    vm_prot_t		prot)
a619 5
 *
 *	History:
 *	90/09/12 Fuzzy	calculation of allocating page table entry number
 *	90/09/12 Fuzzy	When mapped VA map again, output warinning message.
 *
d621 2
a622 9
vm_offset_t pmap_map(
    register vm_offset_t	virt,
    register vm_offset_t	start,
    register vm_offset_t	end,
    register vm_prot_t		prot
#ifdef OMRON_PMAP
    , register unsigned           cmode
#endif /* OMRON */
    )
d627 1
d631 8
d641 2
a642 2
	printf ("(pmap_map :%x) phys address from %x to %x mapped at virtual %x, prot %x\n",
		 curproc, start, end, virt, prot);
a649 1
#ifdef OMRON_PMAP
a650 3
#else /* OMRON */
    template.bits = M88K_TRUNC_PAGE(start) | aprot | DT_VALID;
#endif /* OMRON */
a718 3
 * Author: Sugai
 *	Oct 25 '90	Initial virsion
 *
d721 2
a722 6
pmap_map_batc (
    register vm_offset_t	virt,
    register vm_offset_t	start,
    register vm_offset_t	end,
    register vm_prot_t		prot,
    register unsigned		cmode)
a819 2
 * Author:	Sugai		90/09/07
 *
d849 2
a850 5
void pmap_cache_ctrl(
    pmap_t	pmap,
    vm_offset_t	s,
    vm_offset_t	e,
    unsigned	mode)
d885 1
d887 1
d895 1
a895 1
	spl_sav = splblock();
a914 5
 * Author: Fuzzy	'90.7.12
 *
 * 	90.7.23. JU - changed blkclr to bzero
 *
 *
d961 5
a965 6
pmap_bootstrap(
    vm_offset_t	load_start,	/* IN */
    vm_offset_t	*phys_start,	/* IN/OUT */
    vm_offset_t	*phys_end,	/* IN */
    vm_offset_t	*virt_start,	/* OUT */
    vm_offset_t	*virt_end)	/* OUT */
d974 2
a975 1
    			kernel_pmap_size;
d979 1
d981 1
a981 3
#if 0
    pmap_table_t ptable;
#endif /* 0 */
d983 5
a987 1
    printf("pmap_bootstrap : \"load_start\" 0x%x\n", load_start);
d992 2
a993 2
    if ( ! PAGE_ALIGNED(load_start)) {
	printf("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x\n", load_start);
d1013 2
a1014 1
    *virt_start = *phys_start + ((unsigned)&kernelstart - GOOFYLDOFFSET - load_start);
d1084 3
a1086 1
    e_text = load_start + ((unsigned)&etext - (unsigned)&kernelstart - GOOFYLDOFFSET);	/* paddr of end of text section*/
d1089 5
a1093 5
    #ifdef OMRON_PMAP
     #define PMAPER	pmap_map
    #else
     #define PMAPER	pmap_map_batc
    #endif
d1095 1
a1095 1
    /*  map the first 64k (BUG ROM) read only, cache inhibited */
d1100 1
a1100 2
		VM_PROT_READ|VM_PROT_WRITE,
		CACHE_INH);
d1102 1
a1102 1
    assert(vaddr == (unsigned)&kernelstart - GOOFYLDOFFSET);
d1105 1
a1105 1
		(vm_offset_t)((unsigned)&kernelstart - GOOFYLDOFFSET),
d1108 1
a1108 2
		VM_PROT_WRITE | VM_PROT_READ,	/* shouldn't it be RO? XXX*/
		CACHE_INH);
d1114 1
a1114 2
		VM_PROT_WRITE|VM_PROT_READ,
		CACHE_GLOBAL);
d1117 4
a1120 1
     * Map system segment & page tables - should be cache inhibited.
d1123 5
a1127 1
	printf("(pmap_bootstrap) correcting vaddr\n");
d1136 10
a1145 2
		VM_PROT_WRITE|VM_PROT_READ,
		CACHE_INH);
d1148 6
d1158 23
d1193 2
a1194 1
     * To make 1:1 mapping of virt:phys, throw away a few phys pages
d1197 1
d1202 7
a1208 1
     * establish mapping for code and data cmmu
a1210 1
    if (cmmumap) {
d1212 4
a1215 5
	CMMU_I,
	CMMU_I,
	CMMU_I + 0x1000,
	VM_PROT_WRITE|VM_PROT_READ,
	CACHE_INH);
d1218 4
a1221 24
	CMMU_D,
	CMMU_D,
	CMMU_D + 0x1000,
	VM_PROT_WRITE|VM_PROT_READ,
	CACHE_INH);
    }
#if 0
    if (mapextra) {
	PMAPER(
            0x01000000,
            0x01000000,
            0x02000000,
            VM_PROT_WRITE|VM_PROT_READ,
            CACHE_INH);
    }
#endif /* 0 */
    if (mapallio) {
	PMAPER(
            0xFF800000,
            0xFF800000,
            0xFFFF0000,
            VM_PROT_WRITE|VM_PROT_READ,
            CACHE_INH);
    }
d1223 5
a1227 12
#if 0
    ptable = pmap_table_build(avail_end);

    for (  ; ptable->size != 0xffffffffU; ptable++)
      if (ptable->size)
        PMAPER(ptable->virt_start,
                 ptable->phys_start,
                 ptable->phys_start + ptable->size,
                 ptable->prot,
                 ptable->cacheability);

#endif /* 0 */
d1230 5
a1234 1
	 * Allocate all the submaps we need
d1236 1
d1241 1
a1241 1
    		pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE); \
d1247 5
a1251 4
	SYSMAP(caddr_t		,CMAP1		,CADDR1	   ,1		);
	SYSMAP(caddr_t		,CMAP2		,CADDR2	   ,1		);
	SYSMAP(caddr_t		,vmpte		,vmmap	   ,1		);
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,1		);
d1254 1
d1270 1
a1270 1
    		pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE);
d1272 1
d1276 1
d1281 1
a1281 1
    apr_data.field.ci = 1;
d1297 11
a1307 4
    if (mydebug) {
    	pmap_print(kernel_pmap);
    	pmap_print_trace(kernel_pmap, (vm_offset_t)0xFFF00000, 1);
    }
d1313 3
a1315 1
    printf("running virtual - avail_next 0x%x\n", *phys_start);
a1317 3
    if (mydebug) {
         pmap_print_trace(kernel_pmap, proc0paddr, 1);
    }
d1319 2
d1343 2
a1344 1
	    avail_start + size, VM_PROT_READ|VM_PROT_WRITE, CACHE_INH);
a1358 6
 * History
 * June 13 90	Fri.		Fuzzy
 *				Rewrite		lvl1 --> segment
 *						lvl3 --> page
 *	'90.7.19	Fuzzy		sdt_zone unused
 *
d1398 2
a1399 1
void pmap_init(vm_offset_t phys_start, vm_offset_t phys_end)
a1418 3
	#if 0
	+ pvl_table_size				/* pv_lock_table */
	#endif /* 0 */
d1488 2
a1489 1
void pmap_zero_page(vm_offset_t phys)
d1509 1
a1509 1
	    spl_sav = splblock();
d1546 1
a1546 10
 * Calls:
 *	zalloc
 *	simple_lock_init
 *
 *  This routines allocates a pmap structure and segment translation
 * table from the zones set up by pmap_init. The segment table entries
 * for user space addresses are initalized to zero (invalid).
 * The pmap structure is initalized with the virtual and physical
 * addresses of the segment table. The address (virtual) of the
 * pmap structure is returned.
d1548 2
a1549 1
pmap_t pmap_create(vm_size_t size)
d1551 1
a1551 1
    register pmap_t		p;
d1575 3
a1577 3
    register pmap_statistics_t	stats;
    sdt_entry_t			*segdt;
    int                         i;
d1582 1
a1582 1
    segdt = kmem_alloc(kernel_map, 2 * SDT_SIZE);
d1587 1
a1587 1
    /* maybe, we can use bzero to zero out the segdt. */
d1615 1
d1617 1
a1617 1
     * memory for page tables should be CACHE DISABLED
d1623 1
a1664 14
 *	History:
 *	'90. 7.16	Fuzzy		level 3 --> page discriptor table
 *					level 1 --> segment discriptor table
 *	90/07/20	N.Sugai		sdt_zone no longer exist. We must
 *					use kmem_free instead of zfree.
 *	'90. 7.26	Fuzzy	VM_MIN_ADDRESS -> VM_MIN_USER_ADDRESS
 *				VM_MIN_KERNEL_ADDRESS -> VM_MAX_USER_ADDRESS
 *	'90.8.3		Fuzzy
 *			if defined TEST, 'static' undeclared.
 *	'90.8.22	Fuzzy	Debugging message add
 *	'90.8.30		Fuzzy
 *			delete "if defined TEST, 'static' undeclared."
 *	'90. 9.11	Fuzzy	sdt_va: vm_offset_t --> unsigned long
 *
d1687 2
a1688 1
static void pmap_free_tables(pmap_t pmap)
a1753 3
 *	History:
 * 		'90. 7.16	Fuzzy
 *
d1775 2
a1776 1
void pmap_destroy(register pmap_t p)
a1808 2
 * Author:	Fuzzy
 *
d1821 2
a1822 1
void pmap_reference(register pmap_t p)
a1837 9
 * Update:
 *
 *	July 16, 90 - JUemura	initial porting
 *	'90.7.27	Fuzzy	Calls: add Macros
 *	'90.8.3		Fuzzy	if defined TEST, 'static' undeclared.
 *	'90.8.29	Fuzzy	line 112 (if (pte == PT_ENTRY_NULL) { ...)
 *						delete (check sdt invalid).
 *	'90.8.30	Fuzzy	delete  "if defined TEST, 'static' undeclared."
 *
d1890 2
a1891 1
static void pmap_remove_range(pmap_t pmap, vm_offset_t s, vm_offset_t e)
d1906 1
a1906 1
    if (e <= s)
d1991 1
a1991 1
			   "0x%x PV list at 0x%x\n", va, (unsigned)pvl);
d2020 3
a2022 1
		vm_page_set_modified(PHYS_TO_VM_PAGE(opte.bits & M88K_PGMASK));
a2043 5
 * History:
 *	'90.7.16	Fuzzy	Unchanged
 *	'90.7.26	Fuzzy	VM_MIN_KERNEL_ADDRESS -> VM_MAX_USER_ADDRESS
 *	'90.8.23	Fuzzy	add Debugging message
 *
a2088 4
 *	History:
 *		'90.7.27	Fuzzy	'Calls:' modify
 *		'90.8.28	Fuzzy	add Debugging message
 *
a2237 3



a2240 10
 * History:
 *	'90. 7.16	Fuzzy	level 3 --> page table changed
 *	'90. 7.19	Fuzzy	Comment 'Calls' add
 *	'90. 7.26	Fuzzy	VM_MIN_KERNEL_ADDRESS -> VM_MAX_USER_ADDRESS
 *	'90. 8.18	Fuzzy	Add Debugging Message (PA no mappings)
 *	'90. 8.18	Fuzzy	Bug Fixs
 *		for (i=ptes_per_vm_page; i>0; i++) {
 *					       ^^
 *		for (i=ptes_per_vm_page; i>0; i--) {
 *
d2264 2
a2265 1
static void pmap_copy_on_write(vm_offset_t phys)
d2339 1
a2339 1
	    spl_sav = splblock();
a2356 2


a2359 5
 * History:
 *	'90.7.16	Fuzzy
 *	'90.7.26	Fuzzy	VM_MIN_KERNEL_ADDRESS -> VM_MAX_USER_ADDRESS
 *	'90.8.21	Fuzzy	Debugging message add
 *
d2384 2
a2385 5
void pmap_protect(
    pmap_t	pmap,
    vm_offset_t	s,
    vm_offset_t e,
    vm_prot_t	prot)
d2458 1
a2458 1
	    spl_sav = splblock();
a2477 9
 *	History:
 *	'90.8.3	Fuzzy
 *			if defined TEST, 'static' undeclared.
 *	'90.8.16	Fuzzy
 *			Extern/Global	no --> user_pt_map, kernel_pmap
 *			added Debug message
 *	'90.8.30	Fuzzy
 *			delete "if defined TEST, 'static' undeclared."
 *
d2519 2
a2520 1
static void pmap_expand(pmap_t map, vm_offset_t v)
d2568 1
d2573 1
d2701 3
a2703 6
void pmap_enter(
    register pmap_t	pmap,
    vm_offset_t		va,
    vm_offset_t		pa,
    vm_prot_t		prot,
    boolean_t		wired)
d2789 1
a2789 2
/*#ifdef luna88k*/ /* KLUDGE (or is it?) */ /* is it for dealing with IO mem? */
	if (pa >= MAXPHYSMEM)
d2792 1
a2792 2
/*#endif*/
	template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL /*XXX*/;
d2808 1
a2808 1
		spl_sav = splblock();
d2824 1
a2824 1
	if (old_pa != (vm_offset_t) 0) {
d2829 16
a2844 1
	    pmap_remove_range(pmap, va, va + PAGE_SIZE);
d2848 7
a2854 1

d2913 1
a2913 2
/*#ifdef luna88k */ /* KLUDGE (or is it?) */
	if (pa >= MAXPHYSMEM)
d2916 1
a2916 3
		/* SHOULDN't THE NEXT THING HAVE CACHE_GLOBAL? */
/*#endif */
	template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa);
d2957 6
a2962 8
void pmap_change_wiring(
    pmap_t map,
    vm_offset_t	v,
    boolean_t	wired)
{
	register pt_entry_t	*pte;
	register int		i;
	int			spl;
d3017 2
a3018 1
vm_offset_t pmap_extract(pmap_t pmap, vm_offset_t va)
d3065 2
a3066 1
vm_offset_t pmap_extract_unlocked(pmap_t pmap, vm_offset_t va)
d3068 3
a3070 3
    register pt_entry_t	 *pte;
    register vm_offset_t pa;
    register int	 i;
a3104 3
 *	History:
 *		'90.7.16	Fuzzy
 *
d3121 3
a3123 6
void pmap_copy(
    pmap_t	dst_pmap,
    pmap_t	src_pmap,
    vm_offset_t	dst_addr,
    vm_size_t	len,
    vm_offset_t	src_addr)
a3135 4
 * History:
 *	'90.7.16		Fuzzy
 *	'90.8.27		Fuzzy	Debugging message add
 *
d3154 2
a3155 1
void pmap_update(void)
a3168 8
 * History:
 *	'90. 7.16	Fuzzy
 *	'90. 7.26	Fuzzy	VM_MIN_ADDRESS	-->	VM_MIN_USER_ADDRESS
 *				VM_MIN_KERNEL_ADDRESS --> VM_MAX_USER_ADDRESS
 *	'90. 7.27	Fuzzy	Calls: add Macro
 *	'90. 8.22	Fuzzy	add Debugging message
 *	'90. 9.11	Fuzzy	sdt_va: vm_offset_t --> unsigned long
 *
d3205 2
a3206 1
void pmap_collect(pmap_t pmap)
a3321 2
 *	Author:		Fuzzy
 *
d3335 2
a3336 1
void pmap_activate(pmap_t pmap, pcb_t pcb)
a3348 2
 *	Author:		Fuzzy
 *
d3358 2
a3359 1
void pmap_deactivate(pmap_t pmap, pcb_t pcb)
a3371 3
 *	History:
 *		'90. 7.16	Fuzzy	unchanged
 *
d3375 2
a3376 1
pmap_t pmap_kernel(void)
a3384 4
 *	History:
 *		'90.7.16	Fuzzy	M68K --> M88K
 *					DT_PAGE --> DT_VALID
 *
d3410 2
a3411 1
void pmap_copy_page(vm_offset_t src, vm_offset_t dst)
d3437 1
a3437 1
	    spl_sav = splblock();
a3460 4
 *	Author:	Fuzzy
 *	History:
 *		10/17/90	take out of pmap.c of SUN3, and modify for m88k
 *
d3481 2
a3482 4
void copy_to_phys(
    register vm_offset_t	srcva,
    register vm_offset_t	dstpa,
    register int		bytecount)
d3484 5
a3488 4
    register vm_offset_t	dstva;
    register pt_entry_t	*dstpte;
    register int		copy_size, offset;
    int             aprot;
a3527 3
 *	Author:	David Rudolph
 *	History:
 *
d3548 2
a3549 4
void copy_from_phys(
    register vm_offset_t	srcpa,
    register vm_offset_t	dstva,
    register int		bytecount)
d3613 3
a3615 5
void pmap_pageable(
    pmap_t	pmap,
    vm_offset_t	start,
    vm_offset_t	end,
    boolean_t	pageable)
a3626 4
 * History:
 *	'90.7.16	Fuzzy	m68k --> m88K
 *				pte protection & supervisor bit
 *
d3648 2
a3649 1
void pmap_redzone(pmap_t pmap, vm_offset_t va)
d3651 6
a3656 6
    pt_entry_t			*pte;
    int				spl, spl_sav;
    register int		i;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;
d3675 1
a3675 1
	  spl_sav = splblock();
a3693 6
 *	Author:		Fuzzy
 *
 *	History:
 *		'90.7.24	Fuzzy
 *		'90.8.21	Fuzzy	Debugging message	add
 *
d3720 2
a3721 1
void pmap_clear_modify(vm_offset_t phys)
d3723 11
a3733 11
    pv_entry_t			pvl;
    int				pfi;
    pv_entry_t			pvep;
    pt_entry_t			*pte;
    pmap_t			pmap;
    int				spl, spl_sav;
    register vm_offset_t	va;
    register int		i;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;
d3789 1
a3789 1
	    spl_sav = splblock();
a3813 6
 *	History:
 *		'90. 7.16	Fuzzy
 *		'90. 7.19	Fuzzy	comments 'Calls'
 *		'90. 8.20	Fuzzy	Added debugging message
 *		'90. 8.20	Fuzzy	when panic, print virt_address
 *
d3847 2
a3848 1
boolean_t pmap_is_modified(vm_offset_t phys)
d3966 2
a3967 1
void pmap_clear_reference(vm_offset_t phys)
d3969 11
a3979 11
    pv_entry_t			pvl;
    int				pfi;
    pv_entry_t			pvep;
    pt_entry_t			*pte;
    pmap_t			pmap;
    int				spl, spl_sav;
    register vm_offset_t	va;
    register int		i;
    register unsigned		users;
    register pte_template_t	opte;
    int				kflush;
d4033 1
a4033 1
	    spl_sav = splblock();
d4089 2
a4090 1
boolean_t pmap_is_referenced(vm_offset_t phys)
d4173 2
a4174 1
boolean_t pmap_verify_free(vm_offset_t phys)
a4200 4
 *	History:
 *	'90.7.18	Fuzzy	This function do not exist in m68K pmap list.
 *				vm_page_startup() routine calls this.
 *
d4204 2
a4205 1
boolean_t pmap_valid_page(vm_offset_t p)
a4215 3
 * History:
 *	'90.8.4	Fuzzy	extract vax pmap.c
 *
d4222 2
a4223 1
void pmap_page_protect(vm_offset_t phys, vm_prot_t prot)
d4225 11
a4235 11
    switch (prot) {
      case VM_PROT_READ:
      case VM_PROT_READ|VM_PROT_EXECUTE:
	pmap_copy_on_write(phys);
	break;
      case VM_PROT_ALL:
	break;
      default:
	pmap_remove_all(phys);
	break;
    }
d4238 1
a4238 1
#if 0
a4241 4
 * History:
 *
 *	11/08/09	N.Sugai		Initial version
 *
d4263 2
a4264 1
void pagemove(vm_offset_t from, vm_offset_t to, int size)
d4266 8
a4273 8
    vm_offset_t			pa;
    pt_entry_t			*srcpte, *dstpte;
    int				pfi;
    pv_entry_t			pvl;
    int				spl;
    register int		i;
    register unsigned		users;
    register pte_template_t	opte;
d4304 1
a4304 1
#ifdef DBG
d4310 1
a4310 1
#endif /* DBG */
d4345 1
a4345 1
#endif /* 0 */
d4369 2
a4370 1
void icache_flush(vm_offset_t pa)
d4372 2
a4373 2
    register int i;
    register int cpu = 0;
d4399 2
a4400 1
void pmap_dcache_flush(pmap_t pmap, vm_offset_t va)
d4402 2
a4403 2
    register vm_offset_t pa;
    register int i;
d4421 2
a4422 1
static void cache_flush_loop(int mode, vm_offset_t pa, int size)
d4424 3
a4426 3
    register int	i;
    register int	ncpus;
    void (*cfunc)(int cpu, vm_offset_t physaddr, int size);
d4478 2
a4479 5
void pmap_cache_flush(
    pmap_t pmap,
    vm_offset_t virt,
    int bytes,
    int mode)
d4481 3
a4483 3
    register vm_offset_t pa;
    register vm_offset_t va;
    register int i;
d4506 1
a4506 56
} /* pmap_ccacheflush */


#ifdef JUNK
/*
 *	Machine-level page attributes
 *
 *	This implementation was lifted from the MIPS pmap module.
 *	We currently only use it to invalidate the I-Cache for
 *	debugger use.
 *
 *	These are infrequently used features of the M88K CMMU,
 *	basically cache control functions.  The cachability
 *	property of mappings must be remembered across paging
 *	operations, so that they can be restored on need.
 *
 *	Obviously these attributes will be used in a sparse
 *	fashion, so we use a simple list of attribute-value
 *	pairs.
 *
 *	Some notes on the cache management based upon my quick
 *	calculation and previous experience.
 *	We must carefully weigh the cost of cache invalidate time to
 *	cache refill time.  If "cachefall()" is called for more than
 *	two pages, it is usually faster to simply invalidate the entire
 *	cache and let it refill, since the number of cycles required to
 *	perform the invalidate becomes greater than the number to refill.
 *	If we are only performing an invalidate for something like a
 *	debugger breakpoint, it becomes worthwhile to only perform a
 *	line invalidate.  Remember, we must account for the amount of
 *	time required to perform the pmap lookups.
 */
/*
 *	pmap_attributes:
 *
 *	Set/Get special memory attributes
 *
 *	This is currently only used to invalidate the I-cache when a
 *	breakpoint is set by the debugger.
 *
 */
int pmap_attribute(
    pmap_t			pmap,
    vm_offset_t			address,
    vm_size_t			size,
    vm_machine_attribute_t	attribute,
    vm_machine_attribute_val_t* value)		/* IN/OUT */
{
	register vm_offset_t 	start, end;
	int		ret;
#ifdef notyet
	pmap_attribute_t	a;
#endif

	if (attribute != MATTR_CACHE)
		return KERN_INVALID_ARGUMENT;
a4507 29
	if (pmap == PMAP_NULL)
		return KERN_SUCCESS;

	start = trunc_page(address);
	end = round_page(address + size);
	ret = KERN_SUCCESS;


	/* All we are looking for right now is an instruction cache flush.
	*/
	switch(*value) {
	case MATTR_VAL_CACHE_FLUSH:
		pmap_cache_flush(pmap, start, size, FLUSH_CACHE);
		break;
	case MATTR_VAL_DCACHE_FLUSH:
		pmap_cache_flush(pmap, start, size, FLUSH_DATA_CACHE);
		break;
	case MATTR_VAL_ICACHE_FLUSH:
		pmap_cache_flush(pmap, start, size, FLUSH_CODE_CACHE);
		/* ptrace_user_iflush(pmap, start, size); */
		break;

	default:
		ret = KERN_INVALID_ARGUMENT;
	}

	return ret;
}
#endif /* JUNK */
a4518 7
 *	History:
 *		'90.7.13	Fuzzy
 *	'90.8.3	Fuzzy
 *			if defined TEST, 'static' undeclared.
 *	'90.8.30	Fuzzy
 *			delete "if defined TEST, 'static' undeclared."
 *
d4546 2
a4547 1
static void check_pv_list(vm_offset_t phys, pv_entry_t pv_h, char *who)
a4597 10
 *	History:
 *	June 13 '90		Fuzzy
 *				Rewrite		level 1 --> segment
 *						level 3 --> page
 *	'90.8.3	Fuzzy
 *			if defined TEST, 'static' undeclared.
 *	'90.8.30	Fuzzy
 *			delete "if defined TEST, 'static' undeclared."
 *
 *
d4626 2
a4627 5
static void check_map(
    pmap_t	map,
    vm_offset_t	s,
    vm_offset_t	e,
    char	*who)
a4730 7
 *	History:
 *		'90. 7.16	Fuzzy
 *	'90.8.3	Fuzzy
 *			if defined TEST, 'static' undeclared.
 *	'90.8.30	Fuzzy
 *			delete "if defined TEST, 'static' undeclared."
 *
d4762 2
a4763 1
static void check_pmap_consistency(char *who)
d4807 1
a4807 1
#endif /* DBG */
a4811 2
 * (No locking required.)
 *	usually found in pmap.c			Fuzzy '90.7.12
d4836 1
a4836 4
 *	Author:		Fuzzy			'90.7.12
 *
 *  History:
 *		'90.7.25	Fuzzy	Null sdt entry skip, and skip count print.
d4850 2
a4851 1
void pmap_print (pmap_t pmap)
d4934 2
a4935 4
void pmap_print_trace (
    pmap_t	pmap,
    vm_offset_t	va,
    boolean_t	long_format)
d5053 2
a5054 4
boolean_t pmap_check_transaction(
    pmap_t pmap,
    vm_offset_t va,
    vm_prot_t type)
d5097 2
a5098 3
void pmap_virtual_space(
    vm_offset_t *startp,
    vm_offset_t *endp)
d5104 2
a5105 1
unsigned int pmap_free_pages(void)
d5110 2
a5111 1
boolean_t pmap_next_page(vm_offset_t *addrp)
d5121 1
a5121 1
#if 0
d5126 2
a5127 1
void pmap_set_batc(
d5183 2
a5184 2
#endif /* 0 */
#ifdef notyet
d5198 2
a5199 1
void pmap_destroy_ranges(pmap_range_t *ranges)
d5201 1
a5201 1
	register pmap_range_t this, next;
d5215 2
a5216 3
boolean_t pmap_range_lookup(
	pmap_range_t *ranges,
	vm_offset_t address)
d5218 1
a5218 1
	register pmap_range_t range;
d5233 2
a5234 4
void pmap_range_add(
	pmap_range_t *ranges,
	vm_offset_t start,
	vm_offset_t end)
d5236 1
a5236 1
	register pmap_range_t range, *prev;
d5288 2
a5289 4
void pmap_range_remove(
	pmap_range_t *ranges,
	vm_offset_t start,
	vm_offset_t end)
d5291 1
a5291 1
	register pmap_range_t range, *prev;
d5331 1
a5331 1
#endif /* notyet */
@


1.1
log
@moved from m88k directory
@
text
@@


1.1.1.1
log
@Third try at importing the mvme88k port. This is a working kernel
from nivas.
Userland and compiler still need to be worked on.
Make certain what directory the import is done from.
@
text
@d2 1
a2 41
 * Copyright (c) 1996 Nivas Madhur
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Nivas Madhur.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 */
/*
 * Mach Operating System
 * Copyright (c) 1991 Carnegie Mellon University
 * Copyright (c) 1991 OMRON Corporation
 * All Rights Reserved.
 *
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
d14 1
a14 1
#include <machine/m882xx.h>		/* CMMU stuff */
d16 5
d23 1
a28 2
#include <mvme88k/dev/pcctworeg.h>
#include <mvme88k/dev/clreg.h>
d36 6
a41 2
extern vm_offset_t	pcc2consvaddr;
extern vm_offset_t	clconsvaddr;
d47 5
a51 1
#define	STATIC
a87 1
#else
d89 13
a101 1
#define	STATIC		static
d103 11
a113 1
#endif /* DEBUG */
d115 3
a117 2
caddr_t	vmmap;
pt_entry_t	*vmpte, *msgbufmap;
d119 1
a119 1
STATIC struct pmap 	kernel_pmap_store;
d125 1
a125 1
		vm_offset_t	phys;
d129 1
a129 1
STATIC kpdt_entry_t		kpdt_free;
d132 1
a132 1
 * MAX_KERNEL_VA_SIZE must fit into the virtual address space between
d135 1
a136 1
#define	MAX_KERNEL_VA_SIZE	(256*1024*1024)	/* 256 Mb */
d141 1
a141 1
#define	KERNEL_PDT_SIZE	(M88K_BTOP(MAX_KERNEL_VA_SIZE) * sizeof(pt_entry_t))
a142 6
/*
 * Size of kernel page tables for mapping onboard IO space.
 */
#define	OBIO_PDT_SIZE	(M88K_BTOP(OBIO_SIZE) * sizeof(pt_entry_t))

#define MAX_KERNEL_PDT_SIZE	(KERNEL_PDT_SIZE + OBIO_PDT_SIZE)
d150 2
a151 1
int		ptes_per_vm_page; /* no. of ptes required to map one VM page */
d168 3
a170 3
 * mappings of that page. An entry is a pv_entry_t; the list is the
 * pv_head_table. This is used by things like pmap_remove, when we must
 * find and remove all mappings for a particular physical page.
a242 4
#define ETHERPAGES 16
void	*etherbuf;
int	etherlen;

d252 1
a252 1
#define PMAP_MANAGED(pa) (pmap_initialized && ((pa) >= pmap_phys_start && (pa) < pmap_phys_end))
a267 4

static void check_pv_list __P((vm_offset_t, pv_entry_t, char *));
static void check_pmap_consistency __P((char *));

a292 3
extern vm_offset_t bugromva;
extern vm_offset_t sramva;
extern vm_offset_t obiova;
d294 1
a294 2
STATIC void
flush_atc_entry(unsigned users, vm_offset_t va, int kernel)
d296 3
d329 4
a332 1
_pmap_activate(pmap_t pmap, pcb_t pcb, int my_cpu)
d334 2
a335 2
    apr_template_t	apr_data;
    int 		n;
a353 1
#ifdef notyet
a366 7
#endif /* notyet */
	/*
	 * I am forcing it to not program the BATC at all. pmap.c module
	 * needs major, major cleanup. XXX nivas
	 */
	cmmu_set_uapr(apr_data.bits);
	cmmu_flush_tlb(0, 0, -1);
d409 4
a412 1
_pmap_deactivate(pmap_t pmap, pcb_t pcb, int my_cpu)
d431 3
a433 2
STATIC unsigned int
m88k_protection(pmap_t map, vm_prot_t prot)
d435 1
a435 1
	pte_template_t p;
d437 2
a438 2
	p.bits = 0;
	p.pte.prot = (prot & VM_PROT_WRITE) ? 0 : 1;
d440 1
a440 1
	return(p.bits);
d448 2
d467 3
d471 3
a473 3

pt_entry_t *
pmap_pte(pmap_t map, vm_offset_t virt)
d475 1
a475 1
	sdt_entry_t	*sdt;
d477 2
a478 4
	/*XXX will this change if physical memory is not contiguous? */
	/* take a look at PDTIDX XXXnivas */
	if (map == PMAP_NULL)
		panic("pmap_pte: pmap is NULL");
d480 1
a480 1
	sdt = SDTENT(map,virt);
d482 7
a488 8
	/*
	 * Check whether page table is exist or not.
	 */
	if (!SDT_VALID(sdt))
		return(PT_ENTRY_NULL);
	else
		return((pt_entry_t *)(((sdt + SDT_ENTRIES)->table_addr)<<PDT_SHIFT) +
				PDTIDX(virt));
d496 16
d545 3
a547 2
STATIC pt_entry_t *
pmap_expand_kmap(vm_offset_t virt, vm_prot_t prot)
d618 5
d624 9
a632 2
vm_offset_t
pmap_map(vm_offset_t virt, vm_offset_t start, vm_offset_t end, vm_prot_t prot)
a636 1
    unsigned		cmode;
a639 8
    /*
     * cache mode is passed in the top 16 bits.
     * extract it from there. And clear the top
     * 16 bits from prot.
     */
    cmode = (prot & 0xffff0000) >> 16;
    prot &= 0x0000ffff;

d642 2
a643 2
	printf ("(pmap_map :%x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
		 curproc, start, end, virt, prot, cmode);
d651 1
d653 3
d724 3
d729 6
a734 2
pmap_map_batc(vm_offset_t virt, vm_offset_t start, vm_offset_t end,
					vm_prot_t prot, unsigned cmode)
d832 2
d863 5
a867 2
void
pmap_cache_ctrl(pmap_t pmap, vm_offset_t s, vm_offset_t e, unsigned mode)
a901 1
    if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
a902 1
    }
d910 1
a910 1
	spl_sav = splimp();
d930 5
d981 6
a986 5
pmap_bootstrap(vm_offset_t	load_start,	/* IN */
	       vm_offset_t	*phys_start,	/* IN/OUT */
	       vm_offset_t	*phys_end,	/* IN */
	       vm_offset_t	*virt_start,	/* OUT */
	       vm_offset_t	*virt_end)	/* OUT */
d995 1
a995 2
    			kernel_pmap_size,
    			etherpa;
a998 1
    u_long		foo;
d1000 3
a1002 1
    extern void cmmu_go_virt(void);
d1004 1
a1004 5
#ifdef DEBUG 
    if ((pmap_con_dbg & (CD_BOOT | CD_NORM)) == (CD_BOOT | CD_NORM)) {
	printf("pmap_bootstrap : \"load_start\" 0x%x\n", load_start);
    }
#endif
d1009 2
a1010 2
    if (!PAGE_ALIGNED(load_start)) {
	panic("pmap_bootstrap : \"load_start\" not on the m88k page boundary : 0x%x\n", load_start);
d1030 1
a1030 2
    *virt_start = *phys_start +
   		 (M88K_TRUNC_PAGE((unsigned)&kernelstart) - load_start);
d1100 1
a1100 3
    e_text = load_start + ((unsigned)&etext -
		M88K_TRUNC_PAGE((unsigned)&kernelstart));
					/* paddr of end of text section*/
d1103 5
a1107 5
#ifdef OMRON_PMAP
#define PMAPER	pmap_map
#else
#define PMAPER	pmap_map_batc
#endif
d1109 1
a1109 1
    /*  map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
d1114 2
a1115 1
		(VM_PROT_READ|VM_PROT_WRITE)|(CACHE_INH <<16));
d1117 1
a1117 1
    assert(vaddr == M88K_TRUNC_PAGE((unsigned)&kernelstart));
d1120 1
a1120 1
		(vm_offset_t)M88K_TRUNC_PAGE(((unsigned)&kernelstart)),
d1123 2
a1124 1
		VM_PROT_WRITE | VM_PROT_READ|(CACHE_GLOBAL<<16));	/* shouldn't it be RO? XXX*/
d1130 2
a1131 1
		(VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
d1134 1
a1134 4
     * Map system segment & page tables - should be cache inhibited?
     * 88200 manual says that CI bit is driven on the Mbus while accessing
     * the translation tree. I don't think we need to map it CACHE_INH
     * here...
d1137 1
a1137 5
#ifdef DEBUG
    	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	    printf("(pmap_bootstrap) correcting vaddr\n");
	}
#endif
d1146 2
a1147 10
		(VM_PROT_WRITE|VM_PROT_READ)|(CACHE_GLOBAL << 16));
	
    etherlen = ETHERPAGES * NBPG;

    /*
     *	Get ethernet buffer - need etherlen bytes physically contiguous.
     *  1 to 1 mapped as well???. There is actually a bug in the macros
     *  used by the 1x7 ethernet driver. Remove this when that is fixed.
     *  XXX -nivas
     */
a1149 6
#ifdef DEBUG
        if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
 	    printf("1:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
		*virt_start, *phys_start);
	}
#endif
a1153 23
    *phys_start = vaddr;

    etherbuf = (void *)vaddr;

    vaddr = PMAPER(
		vaddr,
		*phys_start,
		*phys_start + etherlen,
		(VM_PROT_WRITE|VM_PROT_READ)|(CACHE_INH << 16));

    *virt_start += etherlen;
    *phys_start += etherlen;

    if (vaddr != *virt_start) {
#ifdef DEBUG
        if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
	     printf("2:vaddr %x *virt_start %x *phys_start %x\n", vaddr,
		*virt_start, *phys_start);
	}
#endif
	*virt_start = vaddr;
	*phys_start = round_page(*phys_start);
    }
d1166 1
a1166 2
     * To make 1:1 mapping of virt:phys, throw away a few phys pages.
     * XXX what is this? nivas
a1168 1
   
d1173 1
a1173 7
     * Map all IO space 1-to-1. Ideally, I would like to not do this
     * but have va for the given IO address dynamically allocated. But
     * on the 88200, 2 of the BATCs are hardwired to do map the IO space
     * 1-to-1; I decided to map the rest of the IO space 1-to-1.
     * And bug ROM & the SRAM need to be mapped 1-to-1 if we ever want to
     * execute bug system calls after the MMU has been turned on.
     * OBIO should be mapped cache inhibited.
d1176 1
d1178 5
a1182 4
            BUGROM_START,
            BUGROM_START,
            BUGROM_START + BUGROM_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));
d1185 35
a1219 4
            SRAM_START,
            SRAM_START,
            SRAM_START + SRAM_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_GLOBAL << 16));
d1221 1
a1221 5
    PMAPER(
            OBIO_START,
            OBIO_START,
            OBIO_START + OBIO_SIZE,
            VM_PROT_WRITE|VM_PROT_READ|(CACHE_INH << 16));
d1224 1
a1224 5
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physcal pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -	         * else we will be referencing the wrong page.
a1225 1

d1230 1
a1230 1
    		pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16)); \
d1236 4
a1239 5
	SYSMAP(caddr_t, vmpte , vmmap, 1);
	SYSMAP(struct msgbuf *,	msgbufmap ,msgbufp, 1);

	vmpte->pfn = -1;
	vmpte->dtype = DT_INVALID;
a1241 1

d1257 1
a1257 1
    		pmap_expand_kmap(virt, VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
a1258 1

a1261 1

d1266 1
a1266 1
    apr_data.field.ci = 0;
d1282 4
a1285 11
    /*
     * Set valid bit to DT_INVALID so that the very first pmap_enter()
     * on these won't barf in pmap_remove_range().
     */
    pte = pmap_pte(kernel_pmap, phys_map_vaddr1);
    pte->pfn = -1;
    pte->dtype = DT_INVALID;
    pte = pmap_pte(kernel_pmap, phys_map_vaddr2);
    pte->dtype = DT_INVALID;
    pte->pfn = -1;

d1291 1
a1291 3
    if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
        printf("running virtual - avail_next 0x%x\n", *phys_start);
    }
d1294 3
a1297 2
    return;
    
d1320 1
a1320 2
				avail_start + size,
				VM_PROT_READ|VM_PROT_WRITE|(CACHE_GLOBAL << 16));
d1335 6
d1380 1
a1380 2
void
pmap_init(vm_offset_t phys_start, vm_offset_t phys_end)
d1400 3
d1472 1
a1472 2
void
pmap_zero_page(vm_offset_t phys)
d1492 1
a1492 1
	    spl_sav = splimp();
d1529 10
a1538 1
 *  This routines allocates a pmap structure.
d1540 1
a1540 2
pmap_t
pmap_create(vm_size_t size)
d1542 1
a1542 1
    	pmap_t		p;
d1566 3
a1568 3
    pmap_statistics_t	stats;
    sdt_entry_t		*segdt;
    int                	i;
d1573 1
a1573 1
    segdt = (sdt_entry_t *)kmem_alloc(kernel_map, 2 * SDT_SIZE);
d1578 1
a1578 1
    /* maybe, we can use bzero to zero out the segdt. XXX nivas */
a1605 1
#if notneeded
d1607 1
a1607 1
     * memory for page tables should be CACHE DISABLED?
a1612 1
#endif
d1654 14
d1690 1
a1690 2
STATIC void
pmap_free_tables(pmap_t pmap)
d1756 3
d1780 1
a1780 2
void
pmap_destroy(pmap_t p)
d1813 2
d1827 1
a1827 2
void
pmap_reference(pmap_t p)
d1843 9
d1904 1
a1904 2
STATIC void
pmap_remove_range(pmap_t pmap, vm_offset_t s, vm_offset_t e)
d1919 1
a1919 1
    if (e < s)
d2004 1
a2004 1
			   "0x%x (pa 0x%x) PV list at 0x%x\n", va, pa, (unsigned)pvl);
d2033 1
a2033 3
		if (IS_VM_PHYSADDR(pa)) {
			vm_page_set_modified(PHYS_TO_VM_PAGE(opte.bits & M88K_PGMASK));
		}
d2055 5
d2105 4
d2258 3
d2264 10
d2297 1
a2297 2
STATIC void
pmap_copy_on_write(vm_offset_t phys)
d2371 1
a2371 1
	    spl_sav = splimp();
d2389 2
d2394 5
d2423 5
a2427 2
void
pmap_protect(pmap_t pmap, vm_offset_t s, vm_offset_t e, vm_prot_t prot)
d2500 1
a2500 1
	    spl_sav = splimp();
d2520 9
d2570 1
a2570 2
STATIC void
pmap_expand(pmap_t map, vm_offset_t v)
a2617 1
#if notneeded
a2621 1
#endif
d2749 6
a2754 3
void
pmap_enter(pmap_t pmap, vm_offset_t va, vm_offset_t pa,
					vm_prot_t prot, boolean_t wired)
d2840 2
a2841 1
	if ((unsigned long)pa >= MAXPHYSMEM)
d2844 2
a2845 1
	    template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
d2861 1
a2861 1
		spl_sav = splimp();
d2877 1
a2877 1
	if (old_pa != (vm_offset_t)-1) {
d2882 1
a2882 16
#ifdef	DEBUG
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
			if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n", 
				phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
					PMAP_MANAGED(pa) ? 1 : 0);
				printf("pte %x pfn %x valid %x\n",
					pte, pte->pfn, pte->dtype);
	    		}
		}
#endif
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(0, va, 1);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}
d2886 1
a2886 7
#ifdef	DEBUG
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
	    		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				printf("va 0x%x and managed pa 0x%x\n", va, pa);
	    		}
    		}
#endif
d2945 2
a2946 1
	if ((unsigned long)pa >= MAXPHYSMEM)
d2949 3
a2951 1
	    template.bits = DT_VALID | ap | M88K_TRUNC_PAGE(pa) | CACHE_GLOBAL;
d2992 8
a2999 6
void
pmap_change_wiring(pmap_t map, vm_offset_t v, boolean_t wired)
{
	pt_entry_t	*pte;
	int		i;
	int		spl;
d3054 1
a3054 2
vm_offset_t
pmap_extract(pmap_t pmap, vm_offset_t va)
d3101 1
a3101 2
vm_offset_t
pmap_extract_unlocked(pmap_t pmap, vm_offset_t va)
d3103 3
a3105 3
    pt_entry_t	*pte;
    vm_offset_t	pa;
    int		i;
d3140 3
d3159 6
a3164 3
void
pmap_copy(pmap_t dst_pmap, pmap_t src_pmap, vm_offset_t dst_addr,
				vm_size_t len, vm_offset_t src_addr)
d3177 4
d3199 1
a3199 2
void
pmap_update(void)
d3213 8
d3257 1
a3257 2
void
pmap_collect(pmap_t pmap)
d3373 2
d3388 1
a3388 2
void
pmap_activate(pmap_t pmap, pcb_t pcb)
d3401 2
d3412 1
a3412 2
void
pmap_deactivate(pmap_t pmap, pcb_t pcb)
d3425 3
d3431 1
a3431 2
pmap_t
pmap_kernel(void)
d3440 4
d3469 1
a3469 2
void
pmap_copy_page(vm_offset_t src, vm_offset_t dst)
d3495 1
a3495 1
	    spl_sav = splimp();
d3519 4
d3543 4
a3546 2
void
copy_to_phys(vm_offset_t srcva, vm_offset_t dstpa, int bytecount)
d3548 4
a3551 5
    vm_offset_t	dstva;
    pt_entry_t	*dstpte;
    int		copy_size,
   		offset,
    		aprot;
d3591 3
d3614 4
a3617 2
void
copy_from_phys(vm_offset_t srcpa, vm_offset_t dstva, int bytecount)
d3681 5
a3685 3
void
pmap_pageable(pmap_t pmap, vm_offset_t start, vm_offset_t end,
							boolean_t pageable)
d3697 4
d3722 1
a3722 2
void
pmap_redzone(pmap_t pmap, vm_offset_t va)
d3724 6
a3729 6
    pt_entry_t		*pte;
    int			spl, spl_sav;
    int			i;
    unsigned		users;
    pte_template_t	opte;
    int			kflush;
d3748 1
a3748 1
	  spl_sav = splimp();
d3767 6
d3799 1
a3799 2
void
pmap_clear_modify(vm_offset_t phys)
d3801 11
a3811 11
    pv_entry_t		pvl;
    int			pfi;
    pv_entry_t		pvep;
    pt_entry_t		*pte;
    pmap_t		pmap;
    int			spl, spl_sav;
    vm_offset_t		va;
    int			i;
    unsigned		users;
    pte_template_t	opte;
    int			kflush;
d3867 1
a3867 1
	    spl_sav = splimp();
d3892 6
d3931 1
a3931 2
boolean_t
pmap_is_modified(vm_offset_t phys)
d4049 1
a4049 2
void
pmap_clear_reference(vm_offset_t phys)
d4051 11
a4061 11
    pv_entry_t		pvl;
    int			pfi;
    pv_entry_t		pvep;
    pt_entry_t		*pte;
    pmap_t		pmap;
    int			spl, spl_sav;
    vm_offset_t		va;
    int			i;
    unsigned		users;
    pte_template_t	opte;
    int			kflush;
d4115 1
a4115 1
	    spl_sav = splimp();
d4171 1
a4171 2
boolean_t
pmap_is_referenced(vm_offset_t phys)
d4254 1
a4254 2
boolean_t
pmap_verify_free(vm_offset_t phys)
d4281 4
d4288 1
a4288 2
boolean_t
pmap_valid_page(vm_offset_t p)
d4299 3
d4308 1
a4308 2
void
pmap_page_protect(vm_offset_t phys, vm_prot_t prot)
d4310 11
a4320 11
	switch (prot) {
		case VM_PROT_READ:
		case VM_PROT_READ|VM_PROT_EXECUTE:
			pmap_copy_on_write(phys);
			break;
		case VM_PROT_ALL:
			break;
		default:
			pmap_remove_all(phys);
			break;
	}
d4323 1
a4323 1
#if FUTURE_MAYBE
d4327 4
d4352 1
a4352 2
void
pagemove(vm_offset_t from, vm_offset_t to, int size)
d4354 8
a4361 8
    vm_offset_t		pa;
    pt_entry_t		*srcpte, *dstpte;
    int			pfi;
    pv_entry_t		pvl;
    int			spl;
    int			i;
    unsigned		users;
    pte_template_t	opte;
d4392 1
a4392 1
#ifdef DEBUG
d4398 1
a4398 1
#endif /* DEBUG */
d4433 1
a4433 1
#endif /* FUTURE_MAYBE */
d4457 1
a4457 2
void
icache_flush(vm_offset_t pa)
d4459 2
a4460 2
    int 	i;
    int 	cpu = 0;
d4486 1
a4486 2
void
pmap_dcache_flush(pmap_t pmap, vm_offset_t va)
d4488 2
a4489 2
    vm_offset_t pa;
    int 	i;
d4507 1
a4507 2
STATIC void
cache_flush_loop(int mode, vm_offset_t pa, int size)
d4509 3
a4511 3
    int		i;
    int		ncpus;
    void 	(*cfunc)(int cpu, vm_offset_t physaddr, int size);
d4563 5
a4567 2
void
pmap_cache_flush(pmap_t pmap, vm_offset_t virt, int bytes, int mode)
d4569 3
a4571 3
    vm_offset_t pa;
    vm_offset_t va;
    int 	i;
d4594 56
a4649 1
} /* pmap_cache_flush */
d4651 29
d4691 7
d4725 1
a4725 2
STATIC void
check_pv_list(vm_offset_t phys, pv_entry_t pv_h, char *who)
d4776 10
d4814 5
a4818 2
STATIC void
check_map(pmap_t map, vm_offset_t s, vm_offset_t e, char *who)
d4922 7
d4960 1
a4960 2
STATIC void
check_pmap_consistency(char *who)
d5004 1
a5004 1
#endif /* DEBUG */
d5009 2
d5035 4
a5038 1
 *  	History:
d5052 1
a5052 2
void
pmap_print(pmap_t pmap)
d5135 4
a5138 2
void
pmap_print_trace (pmap_t pmap, vm_offset_t va, boolean_t long_format)
d5256 4
a5259 2
boolean_t
pmap_check_transaction(pmap_t pmap, vm_offset_t va, vm_prot_t type)
d5302 3
a5304 2
void
pmap_virtual_space(vm_offset_t *startp, vm_offset_t *endp)
d5310 1
a5310 2
unsigned int
pmap_free_pages(void)
d5315 1
a5315 2
boolean_t
pmap_next_page(vm_offset_t *addrp)
d5325 1
a5325 1
#if USING_BATC
d5330 1
a5330 2
void
pmap_set_batc(
d5386 2
a5387 2
#endif /* USING_BATC */
#if FUTURE_MAYBE
d5401 1
a5401 2
void
pmap_destroy_ranges(pmap_range_t *ranges)
d5403 1
a5403 1
	pmap_range_t this, next;
d5417 3
a5419 2
boolean_t
pmap_range_lookup(pmap_range_t *ranges, vm_offset_t address)
d5421 1
a5421 1
	pmap_range_t range;
d5436 4
a5439 2
void
pmap_range_add(pmap_range_t *ranges, vm_offset_t start, vm_offset_t end)
d5441 1
a5441 1
	pmap_range_t range, *prev;
d5493 4
a5496 2
void
pmap_range_remove(pmap_range_t *ranges, vm_offset_t start, vm_offset_t end)
d5498 1
a5498 1
	pmap_range_t range, *prev;
d5538 1
a5538 1
#endif /* FUTURE_MAYBE */
@
