head	1.38;
access;
symbols
	SMP_SYNC_A:1.38
	SMP_SYNC_B:1.38
	UBC_SYNC_A:1.38
	UBC_SYNC_B:1.38
	OPENBSD_3_0:1.29.0.2
	OPENBSD_3_0_BASE:1.29
	OPENBSD_2_9:1.16.0.12
	OPENBSD_2_9_BASE:1.16
	NIKLAS_UNDEAD:1.16.0.10
	OPENBSD_2_8:1.16.0.8
	OPENBSD_2_8_BASE:1.16
	OPENBSD_2_7:1.16.0.6
	OPENBSD_2_7_BASE:1.16
	SMP:1.16.0.4
	SMP_BASE:1.16
	kame_19991208:1.16
	OPENBSD_2_6:1.16.0.2
	OPENBSD_2_6_BASE:1.16
	OPENBSD_2_5:1.11.0.2
	OPENBSD_2_5_BASE:1.11
	OPENBSD_2_4:1.10.0.2
	OPENBSD_2_4_BASE:1.10
	OPENBSD_2_3:1.9.0.6
	OPENBSD_2_3_BASE:1.9
	OPENBSD_2_2:1.9.0.4
	OPENBSD_2_2_BASE:1.9
	OPENBSD_2_1:1.9.0.2
	OPENBSD_2_1_BASE:1.9
	OPENBSD_2_0:1.5.0.2
	OPENBSD_2_0_BASE:1.5
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.38
date	2002.01.10.22.25.40;	author miod;	state dead;
branches;
next	1.37;

1.37
date	2001.12.20.19.02.28;	author miod;	state Exp;
branches;
next	1.36;

1.36
date	2001.11.30.21.02.00;	author miod;	state dead;
branches;
next	1.35;

1.35
date	2001.11.28.16.13.28;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.11.28.15.34.16;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.11.28.13.47.38;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.11.07.01.18.00;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.06.02.07.59;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.09.19.20.50.56;	author mickey;	state Exp;
branches;
next	1.28;

1.28
date	2001.07.31.15.31.25;	author beck;	state Exp;
branches;
next	1.27;

1.27
date	2001.07.25.13.25.32;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.07.18.10.47.04;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.06.27.04.22.38;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.06.27.04.02.05;	author beck;	state Exp;
branches;
next	1.23;

1.23
date	2001.06.24.04.20.44;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.06.23.21.42.23;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.06.08.08.09.02;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.06.08.02.47.39;	author aaron;	state Exp;
branches;
next	1.19;

1.19
date	2001.05.09.15.31.25;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.05.08.17.30.41;	author aaron;	state Exp;
branches;
next	1.17;

1.17
date	2001.05.05.21.26.38;	author art;	state Exp;
branches;
next	1.16;

1.16
date	99.09.03.18.01.14;	author art;	state Exp;
branches
	1.16.4.1;
next	1.15;

1.15
date	99.07.18.18.00.05;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	99.07.18.16.45.51;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	99.07.18.16.23.47;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	99.04.24.06.39.41;	author downsj;	state Exp;
branches;
next	1.11;

1.11
date	99.01.11.05.11.36;	author millert;	state Exp;
branches;
next	1.10;

1.10
date	98.05.03.07.16.51;	author gene;	state Exp;
branches;
next	1.9;

1.9
date	97.02.10.12.16.56;	author downsj;	state Exp;
branches;
next	1.8;

1.8
date	97.02.10.12.01.49;	author downsj;	state Exp;
branches;
next	1.7;

1.7
date	97.01.24.01.35.51;	author briggs;	state Exp;
branches;
next	1.6;

1.6
date	96.10.28.14.55.33;	author briggs;	state Exp;
branches;
next	1.5;

1.5
date	96.05.26.18.36.28;	author briggs;	state Exp;
branches;
next	1.4;

1.4
date	96.05.26.18.14.33;	author briggs;	state Exp;
branches;
next	1.3;

1.3
date	95.12.14.11.36.09;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.11.30.18.40.26;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.51.08;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.51.08;	author deraadt;	state Exp;
branches;
next	;

1.16.4.1
date	2001.07.04.10.18.40;	author niklas;	state Exp;
branches;
next	1.16.4.2;

1.16.4.2
date	2001.10.31.03.01.15;	author nate;	state Exp;
branches;
next	1.16.4.3;

1.16.4.3
date	2001.11.13.21.00.53;	author niklas;	state Exp;
branches;
next	1.16.4.4;

1.16.4.4
date	2001.12.05.00.39.11;	author niklas;	state dead;
branches;
next	;


desc
@@


1.38
log
@Bring back pmap_motorola for mac68k.
@
text
@/*	$OpenBSD: pmap.c,v 1.37 2001/12/20 19:02:28 miod Exp $	*/
/*	$NetBSD: pmap.c,v 1.55 1999/04/22 04:24:53 chs Exp $	*/

/*
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.6 (Berkeley) 5/27/94
 */

/*
 * Derived from HP9000/300 series physical map management code.
 *
 * Supports:
 *	68020 with 68851 MMU	Mac II
 *	68030 with on-chip MMU	IIcx, etc.
 *	68040 with on-chip MMU	Quadras, etc.
 *
 * Notes:
 *	Don't even pay lip service to multiprocessor support.
 *
 *	We assume TLB entries don't have process tags (except for the
 *	supervisor/user distinction) so we only invalidate TLB entries
 *	when changing mappings for the current (or kernel) pmap.  This is
 *	technically not true for the 68551 but we flush the TLB on every
 *	context switch, so it effectively winds up that way.
 *
 *	Bitwise and/or operations are significantly faster than bitfield
 *	references so we use them when accessing STE/PTEs in the pmap_pte_*
 *	macros.  Note also that the two are not always equivalent; e.g.:
 *		(*pte & PG_PROT)[4] != pte->pg_prot[1]
 *	and a couple of routines that deal with protection and wiring take
 *	some shortcuts that assume the and/or definitions.
 *
 *	This implementation will only work for PAGE_SIZE == NBPG
 *	(i.e. 4096 bytes).
 */

/*
 *	Manages physical address maps.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/user.h>
#include <sys/pool.h>

#include <machine/pte.h>

#include <uvm/uvm.h>

#include <machine/cpu.h>

#ifdef DEBUG
#define PDB_FOLLOW	0x0001
#define PDB_INIT	0x0002
#define PDB_ENTER	0x0004
#define PDB_REMOVE	0x0008
#define PDB_CREATE	0x0010
#define PDB_PTPAGE	0x0020
#define PDB_CACHE	0x0040
#define PDB_BITS	0x0080
#define PDB_COLLECT	0x0100
#define PDB_PROTECT	0x0200
#define PDB_SEGTAB	0x0400
#define PDB_MULTIMAP	0x0800
#define PDB_PARANOIA	0x2000
#define PDB_WIRING	0x4000
#define PDB_PVDUMP	0x8000

int debugmap = 0;
int pmapdebug = PDB_PARANOIA;

#define	PMAP_DPRINTF(l, x)	if (pmapdebug & (l)) printf x

#if defined(M68040)
int dowriteback = 1;	/* 68040: enable writeback caching */
int dokwriteback = 1;	/* 68040: enable writeback caching of kernel AS */
#endif
#else /* ! DEBUG */
#define	PMAP_DPRINTF(l, x)	/* nothing */
#endif /* DEBUG */

/*
 * Get STEs and PTEs for user/kernel address space
 */
#if defined(M68040)
#define	pmap_ste1(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
/* XXX assumes physically contiguous ST pages (if more than one) */
#define pmap_ste2(m, v) \
	(&((m)->pm_stab[(st_entry_t *)(*(u_int *)pmap_ste1(m, v) & SG4_ADDR1) \
			- (m)->pm_stpa + (((v) & SG4_MASK2) >> SG4_SHIFT2)]))
#define	pmap_ste(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) \
			>> (mmutype == MMU_68040 ? SG4_SHIFT1 : SG_ISHIFT)]))
#define pmap_ste_v(m, v) \
	(mmutype == MMU_68040 \
	 ? ((*pmap_ste1(m, v) & SG_V) && \
	    (*pmap_ste2(m, v) & SG_V)) \
	 : (*pmap_ste(m, v) & SG_V))
#else
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
#define pmap_ste_v(m, v) (*pmap_ste(m, v) & SG_V)
#endif

#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))
#define pmap_pte_pa(pte)	(*(pte) & PG_FRAME)
#define pmap_pte_w(pte)		(*(pte) & PG_W)
#define pmap_pte_ci(pte)	(*(pte) & PG_CI)
#define pmap_pte_m(pte)		(*(pte) & PG_M)
#define pmap_pte_u(pte)		(*(pte) & PG_U)
#define pmap_pte_prot(pte)	(*(pte) & PG_PROT)
#define pmap_pte_v(pte)		(*(pte) & PG_V)

#define pmap_pte_set_w(pte, v) \
	if (v) *(pte) |= PG_W; else *(pte) &= ~PG_W
#define pmap_pte_set_prot(pte, v) \
	if (v) *(pte) |= PG_PROT; else *(pte) &= ~PG_PROT
#define pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

/*
 * Given a map and a machine independent protection code,
 * convert to a m68k protection code.
 */
#define pte_prot(m, p)	(protection_codes[p])
int	protection_codes[8];

/*
 * Kernel page table page management.
 */
struct kpt_page {
	struct kpt_page *kpt_next;	/* link on either used or free list */
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
};
struct kpt_page *kpt_free_list, *kpt_used_list;
struct kpt_page *kpt_pages;

/*
 * Kernel segment/page table and page table map.
 * The page table map gives us a level of indirection we need to dynamically
 * expand the page table.  It is essentially a copy of the segment table
 * with PTEs instead of STEs.  All are initialized in locore at boot time.
 * Sysmap will initially contain VM_KERNEL_PT_PAGES pages of PTEs.
 * Segtabzero is an empty segment table which all processes share til they
 * reference something.
 */
st_entry_t	*Sysseg;
pt_entry_t	*Sysmap, *Sysptmap;
st_entry_t	*Segtabzero, *Segtabzeropa;
vsize_t		Sysptsize = VM_KERNEL_PT_PAGES;

struct pmap	kernel_pmap_store;
struct vm_map	*st_map, *pt_map;
struct vm_map	st_map_store, pt_map_store;

paddr_t		avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vsize_t		mem_size;	/* memory size in bytes */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
int		page_cnt;	/* number of pages managed by VM system */

boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
struct pv_entry	*pv_table;
char		*pmap_attributes;	/* reference and modify bits */
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int		pv_nfree;

/* The following four variables are defined in pmap_bootstrap.c */
extern int		vidlen;
#define VIDMAPSIZE	btoc(vidlen)

#if defined(M68040)
int		protostfree;	/* prototype (default) free ST map */
#endif

extern caddr_t	CADDR1, CADDR2;

pt_entry_t	*caddr1_pte;	/* PTE for CADDR1 */
pt_entry_t	*caddr2_pte;	/* PTE for CADDR2 */

struct pool	pmap_pmap_pool;	/* memory pool for pmap structures */

struct pv_entry *pmap_alloc_pv __P((void));
void	pmap_free_pv __P((struct pv_entry *));
void	pmap_collect_pv __P((void));

#define	PAGE_IS_MANAGED(pa)	(pmap_initialized &&			\
				 vm_physseg_find(atop((pa)), NULL) != -1)

#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})

#define	pa_to_attribute(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
})

/*
 * Internal routines
 */
void	pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));
boolean_t pmap_testbit	__P((paddr_t, int));
void	pmap_changebit	__P((paddr_t, int, int));
void	pmap_enter_ptpage	__P((pmap_t, vaddr_t));
void	pmap_collect1	__P((pmap_t, paddr_t, vaddr_t));
void	pmap_pinit __P((pmap_t));
void	pmap_release __P((pmap_t));

#ifdef DEBUG
void	pmap_pvdump          __P((paddr_t));
void	pmap_check_wiring    __P((char *, vaddr_t));
#endif

/* pmap_remove_mapping flags */
#define	PRM_TFLUSH	1
#define	PRM_CFLUSH	2

/*
 * pmap_virtual_space:		[ INTERFACE ]
 *
 *	Report the range of available kernel virtual address
 *	space to the VM system during bootstrap.
 *
 *	This is only an interface function if we do not use
 *	pmap_steal_memory()!
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_virtual_space(vstartp, vendp)
	vaddr_t *vstartp, *vendp;
{

	*vstartp = virtual_avail;
	*vendp = virtual_end;
}

/*
 * pmap_init:			[ INTERFACE ]
 *
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_init()
{
	vaddr_t addr, addr2;
	vsize_t s;
	struct pv_entry *pv;
	char *attr;
	int rv;
	int npages;
	int bank;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));

	/*
	 * Before we do anything else, initialize the PTE pointers
	 * used by pmap_zero_page() and pmap_copy_page().
	 */
	caddr1_pte = pmap_pte(pmap_kernel(), CADDR1);
	caddr2_pte = pmap_pte(pmap_kernel(), CADDR2);

	/*
	 * Now that kernel map has been allocated, we can mark as
	 * unavailable regions which we have mapped in pmap_bootstrap().
	 */
	addr = (vaddr_t)IOBase;
	if (uvm_map(kernel_map, &addr,
		    m68k_ptob(IIOMAPSIZE + ROMMAPSIZE + VIDMAPSIZE),
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)))
		goto bogons;
	addr = (vaddr_t)Sysmap;
	if (uvm_map(kernel_map, &addr, MAC_MAX_PTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED))) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
 bogons:
		panic("pmap_init: bogons in the VM system!\n");
	}

	PMAP_DPRINTF(PDB_INIT,
	    ("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
	    Sysseg, Sysmap, Sysptmap));
	PMAP_DPRINTF(PDB_INIT,
	    ("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
	    avail_start, avail_end, virtual_avail, virtual_end));

	/*
	 * Allocate memory for random pmap data structures.  Includes the
	 * initial segment table, pv_head_table and pmap_attributes.
	 */
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++)
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
	s = MAC_STSIZE;					/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */
	s = round_page(s);
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: can't allocate data structures");

	Segtabzero = (st_entry_t *)addr;
	pmap_extract(pmap_kernel(), addr, (paddr_t *)&Segtabzeropa);
	addr += MAC_STSIZE;

	pv_table = (struct pv_entry *)addr;
	addr += page_cnt * sizeof(struct pv_entry);

	pmap_attributes = (char *)addr;

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
	    "tbl %p atr %p\n",
	    s, page_cnt, Segtabzero, Segtabzeropa,
	    pv_table, pmap_attributes));

	/*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}

	/*
	 * Allocate physical memory for kernel PT pages and their management.
	 * We need 1 PT page per possible task plus some slop.
	 */
	npages = min(atop(MAC_MAX_KPTSIZE), maxproc+16);
	s = ptoa(npages) + round_page(npages * sizeof(struct kpt_page));

	/*
	 * Verify that space will be allocated in region for which
	 * we already have kernel PT pages.
	 */
	addr = 0;
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv || (addr + s) >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	uvm_unmap(kernel_map, addr, addr + s);

	/*
	 * Now allocate the space and link the pages together to
	 * form the KPT free list.
	 */
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
	s = ptoa(npages);
	addr2 = addr + s;
	kpt_pages = &((struct kpt_page *)addr2)[npages];
	kpt_free_list = (struct kpt_page *)0;
	do {
		addr2 -= NBPG;
		(--kpt_pages)->kpt_next = kpt_free_list;
		kpt_free_list = kpt_pages;
		kpt_pages->kpt_va = addr2;
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
	} while (addr != addr2);

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: KPT: %ld pages from %lx to %lx\n",
	    atop(s), addr, addr + s));

	/*
	 * Allocate the segment table map and the page table map.
	 */
	s = maxproc * MAC_STSIZE;
	st_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, TRUE,
	    FALSE, &st_map_store);

	addr = MAC_PTBASE;
	if ((MAC_PTMAXSIZE / MAC_MAX_PTSIZE) < maxproc) {
		s = MAC_PTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(2).
		 */
		maxproc = (MAC_PTMAXSIZE / MAC_MAX_PTSIZE);
	} else
		s = (maxproc * MAC_MAX_PTSIZE);
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, TRUE,
	    TRUE, &pt_map_store);

#if defined(M68040)
	if (mmutype == MMU_68040) {
		protostfree = ~l2tobm(0);
		for (rv = MAXUL2SIZE; rv < sizeof(protostfree)*NBBY; rv++)
			protostfree &= ~l2tobm(rv);
	}
#endif

	/*
	 * Initialize the pmap pools.
	 */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

	/*
	 * Now it is safe to enable pv_table recording.
	 */
	pmap_initialized = TRUE;
}

/*
 * pmap_alloc_pv:
 *
 *	Allocate a pv_entry.
 */
struct pv_entry *
pmap_alloc_pv()
{
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;

	if (pv_nfree == 0) {
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		}
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
	return pv;
}

/*
 * pmap_free_pv:
 *
 *	Free a pv_entry.
 */
void
pmap_free_pv(pv)
	struct pv_entry *pv;
{
	struct pv_page *pvp;

	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
		break;
	}
}

/*
 * pmap_collect_pv:
 *
 *	Perform compaction on the PV list, called via pmap_collect().
 */
void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;

	TAILQ_INIT(&pv_page_collectlist);

	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp,
			    pvp_pgi.pgi_list);
			pv_nfree -= NPVPPG;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}

	if (pv_page_collectlist.tqh_first == 0)
		return;

	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
		if (ph->pv_pmap == 0)
			continue;
		s = splimp();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp,
					    pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: pgi_nfree inconsistent");
#endif
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}

	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
	}
}

/*
 * pmap_map:
 *
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
 */
vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));

	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
		va += NBPG;
		spa += NBPG;
	}
	return (va);
}

/*
 * pmap_create:			[ INTERFACE ]
 *
 *	Create and return a physical map.
 *
 *	Note: no locking is necessary in this function.
 */
struct pmap *
pmap_create(void)
{
	struct pmap *pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create(%lx)\n", size));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);

	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return (pmap);
}

/*
 * pmap_pinit:
 *
 *	Initialize a preallocated and zeroed pmap structure.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_create()!
 */
void
pmap_pinit(pmap)
	struct pmap *pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_pinit(%p)\n", pmap));

	/*
	 * No need to allocate page table space yet but we do need a
	 * valid segment table.  Initially, we point everyone at the
	 * "null" segment table.  On the first pmap_enter, a real
	 * segment table will be allocated.
	 */
	pmap->pm_stab = Segtabzero;
	pmap->pm_stpa = Segtabzeropa;
#if defined(M68040)
	if (mmutype == MMU_68040)
		pmap->pm_stfree = protostfree;
#endif
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
}

/*
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
 */
void
pmap_destroy(pmap)
	pmap_t pmap;
{
	int count;

	if (pmap == NULL)
		return;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));

	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		pool_put(&pmap_pmap_pool, pmap);
	}
}

/*
 * pmap_release:
 *
 *	Relese the resources held by a pmap.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_destroy().
 */
void
pmap_release(pmap)
	struct pmap *pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_release(%p)\n", pmap));

#ifdef notdef /* DIAGNOSTIC */
	/* count would be 0 from pmap_destroy... */
	simple_lock(&pmap->pm_lock);
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif

	if (pmap->pm_ptab)
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
		    MAC_MAX_PTSIZE);
	if (pmap->pm_stab != Segtabzero)
		uvm_km_free_wakeup(st_map, (vaddr_t)pmap->pm_stab,
		    MAC_STSIZE);
}

/*
 * pmap_reference:		[ INTERFACE ]
 *
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap)
	pmap_t	pmap;
{

	if (pmap == NULL)
		return;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_reference(%p)\n", pmap));

	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}

 /*
  * pmap_activate:		[ INTERFACE ]
  *
  *	Activate the pmap used by the specified process.  This includes
  *	reloading the MMU context if the current process, and marking
  *	the pmap in use by the processor.
  *
  *	Note: we may only use spin locks here, since we are called
  *	by a critical section in cpu_switch()!
  */
void
pmap_activate(p)
	struct proc *p;
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_SEGTAB,
	    ("pmap_activate(%p)\n", p));

	PMAP_ACTIVATE(pmap, p == curproc);
}

/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.
 */
void
pmap_deactivate(p)
	struct proc *p;
{

	/* No action necessary in this pmap implementation. */
}

/*
 * pmap_remove:			[ INTERFACE ]
 *
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_remove(pmap, sva, eva)
	pmap_t pmap;
	vaddr_t sva, eva;
{
	vaddr_t nssva;
	pt_entry_t *pte;
	boolean_t firstpage, needcflush;
	int flags;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva));

	if (pmap == NULL)
		return;

	firstpage = TRUE;
	needcflush = FALSE;
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
	while (sva < eva) {
		nssva = mac68k_trunc_seg(sva) + MAC_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}
		/*
		 * Invalidate every valid mapping within this segment.
		 */
		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte)) {
				pmap_remove_mapping(pmap, sva, pte, flags);
				firstpage = FALSE;
			}
			pte++;
			sva += NBPG;
		}
	}
	/*
	 * Didn't do anything, no need for cache flushes
	 */
	if (firstpage)
		return;
}

/*
 * pmap_page_protect:		[ INTERFACE ]
 *
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	paddr_t		pa;
	struct pv_entry *pv;
	int s;

	pa = VM_PAGE_TO_PHYS(pg);

#ifdef DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    (prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE)))
		printf("pmap_page_protect(%lx, %x)\n", pa, prot);
#endif
	if (PAGE_IS_MANAGED(pa) == 0)
		return;

	switch (prot) {
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
		return;
	/* copy_on_write */
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_changebit(pa, PG_RO, ~0);
		return;
	/* remove_all */
	default:
		break;
	}
	pv = pa_to_pvh(pa);
	s = splimp();
	while (pv->pv_pmap != NULL) {
		pt_entry_t *pte;

		pte = pmap_pte(pv->pv_pmap, pv->pv_va);
#ifdef DEBUG
		if (!pmap_ste_v(pv->pv_pmap, pv->pv_va) ||
		    pmap_pte_pa(pte) != pa)
			panic("pmap_page_protect: bad mapping");
#endif
		if (!pmap_pte_w(pte))
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
					    pte, PRM_TFLUSH|PRM_CFLUSH);
		else {
			pv = pv->pv_next;
#ifdef DEBUG
			if (pmapdebug & PDB_PARANOIA)
				printf("%s wired mapping for %lx not removed\n",
				       "pmap_page_protect:", pa);
#endif
			if (pv == NULL)
				break;
		}
	}
	splx(s);
}

/*
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protectoin on the specified range of this map
 *	as requested.
 */
void
pmap_protect(pmap, sva, eva, prot)
	pmap_t		pmap;
	vaddr_t		sva, eva;
	vm_prot_t	prot;
{
	vaddr_t nssva;
	pt_entry_t *pte;
	boolean_t firstpage, needtflush;
	int isro;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_protect(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, prot));

	if (pmap == NULL)
		return;

	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}
	if (prot & VM_PROT_WRITE)
		return;

	isro = pte_prot(pmap, prot);
	needtflush = active_pmap(pmap);
	firstpage = TRUE;
	while (sva < eva) {
		nssva = mac68k_trunc_seg(sva) + MAC_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}
		/*
		 * Change protection on mapping if it is valid and doesn't
		 * already have the correct protection.
		 */
		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte) && pmap_pte_prot_chg(pte, isro)) {
#if defined(M68040)
				/*
				 * Clear caches if making RO (see section
				 * "7.3 Cache Coherency" in the manual).
				 */
				if (isro && mmutype == MMU_68040) {
					paddr_t pa = pmap_pte_pa(pte);

					DCFP(pa);
					ICPP(pa);
				}
#endif
				pmap_pte_set_prot(pte, isro);
				if (needtflush)
					TBIS(sva);
				firstpage = FALSE;
			}
			pte++;
			sva += NBPG;
		}
	}
}

void
mac68k_set_pte(va, pge)
	vm_offset_t va;
	vm_offset_t pge;
{
extern	vm_offset_t tmp_vpages[];
	register pt_entry_t *pte;

	if (va != tmp_vpages[0])
		return;

	pte = pmap_pte(pmap_kernel(), va);
	*pte = (pt_entry_t) pge;
}

/*
 * pmap_enter:			[ INTERFACE ]
 *
 *	Insert the given physical page (pa) at
 *	the specified virtual address (va) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte cannot be reclaimed.
 *
 *	Note: This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  Thatis, this routine must actually
 *	insert this page into the given map NOW.
 */
int
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	pt_entry_t *pte;
	int npte;
	paddr_t opa;
	boolean_t cacheable = TRUE;
	boolean_t checkpv = TRUE;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, %lx, %lx, %x, %x)\n",
	    pmap, va, pa, prot, wired));

#ifdef DIAGNOSTIC
	/*
	 * pmap_enter() should never be used for CADDR1 and CADDR2.
	 */
	if (pmap == pmap_kernel() &&
	    (va == (vaddr_t)CADDR1 || va == (vaddr_t)CADDR2))
		panic("pmap_enter: used for CADDR1 or CADDR2");
#endif

	/*
	 * For user mapping, allocate kernel VM resources if necessary.
	 */
	if (pmap->pm_ptab == NULL)
		pmap->pm_ptab = (pt_entry_t *)
		    uvm_km_valloc_wait(pt_map, MAC_MAX_PTSIZE);

	/*
	 * Segment table entry not valid, we need a new PT page
	 */
	if (!pmap_ste_v(pmap, va))
		pmap_enter_ptpage(pmap, va);

	pa = m68k_trunc_page(pa);
	pte = pmap_pte(pmap, va);
	opa = pmap_pte_pa(pte);

	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));

	/*
	 * Mapping has not changed, must be protection or wiring change.
	 */
	if (opa == pa) {
		/*
		 * Wiring change, just update stats.
		 * We don't worry about wiring PT pages as they remain
		 * resident as long as there are valid mappings in them.
		 * Hence, if a user page is wired, the PT page will be also.
		 */
		if (pmap_pte_w_chg(pte, wired ? PG_W : 0)) {
			PMAP_DPRINTF(PDB_ENTER,
			    ("enter: wiring change -> %x\n", wired));
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
		}
		/*
		 * Retain cache inhibition status
		 */
		checkpv = FALSE;
		if (pmap_pte_ci(pte))
			cacheable = FALSE;
		goto validate;
	}

	/*
	 * Mapping has changed, invalidate old range and fall through to
	 * handle validating new mapping.
	 */
	if (opa) {
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: removing old mapping %lx\n", va));
		pmap_remove_mapping(pmap, va, pte, PRM_TFLUSH|PRM_CFLUSH);
	}

	/*
	 * If this is a new user mapping, increment the wiring count
	 * on this PT page.  PT pages are wired down as long as there
	 * is a valid mapping in the page.
	 */
	if (pmap != pmap_kernel())
		(void)uvm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)(pte+1)), FALSE, FALSE);

	/*
	 * Enter on the PV list if part of our managed memory
	 * Note that we raise IPL while manipulating pv_table
	 * since pmap_enter can be called at interrupt time.
	 */
	if (PAGE_IS_MANAGED(pa)) {
		struct pv_entry *pv, *npv;
		int s;

		pv = pa_to_pvh(pa);
		s = splimp();
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: pv at %p: %lx/%p/%p\n",
		    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
		/*
		 * No entries yet, use header as the first entry
		 */
		if (pv->pv_pmap == NULL) {
			pv->pv_va = va;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
			pv->pv_ptste = NULL;
			pv->pv_ptpmap = NULL;
			pv->pv_flags = 0;
		}
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
		else {
#ifdef DEBUG
			for (npv = pv; npv; npv = npv->pv_next)
				if (pmap == npv->pv_pmap && va == npv->pv_va)
					panic("pmap_enter: already in pv_tab");
#endif
			npv = pmap_alloc_pv();
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_ptste = NULL;
			npv->pv_ptpmap = NULL;
			npv->pv_flags = 0;
			pv->pv_next = npv;
		}

		/*
		 * Speed pmap_is_referenced() or pmap_is_modified() based
		 * on the hint provided in access_type.
		 */
#ifdef DIAGNOSTIC
		if ((flags & VM_PROT_ALL) & ~prot)
			panic("pmap_enter: access_type exceeds prot");
#endif
		if (flags & VM_PROT_WRITE)
			*pa_to_attribute(pa) |= (PG_U|PG_M);
		else if (flags & VM_PROT_ALL)
			*pa_to_attribute(pa) |= PG_U;
		splx(s);
	}
	/*
	 * Assumption: if it is not part of our managed memory
	 * then it must be device memory which may be volitile.
	 */
	else if (pmap_initialized) {
		checkpv = cacheable = FALSE;
	}

	/*
	 * Increment counters
	 */
	pmap->pm_stats.resident_count++;
	if (wired)
		pmap->pm_stats.wired_count++;

validate:
	/*
	 * Build the new PTE.
	 */
	npte = pa | pte_prot(pmap, prot) | (*pte & (PG_M|PG_U)) | PG_V;
	if (wired)
		npte |= PG_W;

	/* Don't cache if process can't take it, like SunOS ones.  */
	if (mmutype == MMU_68040 && pmap != pmap_kernel() &&
	    (curproc->p_md.md_flags & MDP_UNCACHE_WX) &&
	    (prot & VM_PROT_EXECUTE) && (prot & VM_PROT_WRITE))
	        checkpv = cacheable = FALSE;

	if (!checkpv && !cacheable)
		npte |= PG_CI;
#if defined(M68040)
	if (mmutype == MMU_68040 && (npte & (PG_PROT|PG_CI)) == PG_RW)
#ifdef DEBUG
		if (dowriteback && (dokwriteback || pmap != pmap_kernel()))
#endif
		npte |= PG_CCB;
#endif

	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));

	/*
	 * Remember if this was a wiring-only change.
	 * If so, we need not flush the TLB and caches.
	 */
	wired = ((*pte ^ npte) == PG_W);
#if defined(M68040)
	if (mmutype == MMU_68040 && !wired) {
		DCFP(pa);
		ICPP(pa);
	}
#endif
	*pte = npte;
	if (!wired && active_pmap(pmap))
		TBIS(va);
#ifdef DEBUG
	if ((pmapdebug & PDB_WIRING) && pmap != pmap_kernel())
		pmap_check_wiring("enter", trunc_page((vaddr_t)pmap_pte(pmap, va)));
#endif

	return (0);
}

/*
 * pmap_unwire:		[ INTERFACE ]
 *
 *	Change the wiring attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap, va)
	pmap_t		pmap;
	vaddr_t		va;
{
	pt_entry_t *pte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %lx)\n", pmap, va));

	if (pmap == NULL)
		return;

	pte = pmap_pte(pmap, va);
#ifdef DEBUG
	/*
	 * Page table page is not allocated.
	 * Should this ever happen?  Ignore it for now,
	 * we don't want to force allocation of unnecessary PTE pages.
	 */
	if (!pmap_ste_v(pmap, va)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid STE for %lx\n", va);
		return;
	}
	/*
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
	 */
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid PTE for %lx\n", va);
	}
#endif
	/*
	 * If wiring actually changed (always?) set the wire bit and
	 * update the wire count.  Note that wiring is not a hardware
	 * characteristic so there is no need to invalidate the TLB.
	 */
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, 0);
		pmap->pm_stats.wired_count--;
	}
}

/*
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
 */
boolean_t
pmap_extract(pmap, va, pap)
	pmap_t	pmap;
	vaddr_t	va;
	paddr_t *pap;
{
	paddr_t pa;

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_extract(%p, %lx) -> ", pmap, va));

	if (pmap && pmap_ste_v(pmap, va))
		pa = *pmap_pte(pmap, va);
	else
		return (FALSE);

	pa = (pa & PG_FRAME) | (va & ~PG_FRAME);

	PMAP_DPRINTF(PDB_FOLLOW, ("%lx\n", pa));

	*pap = pa;
	return (TRUE);
}

/*
 * pmap_copy:			[ INTERFACE ]
 *
 *	Copy the mapping range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t		dst_pmap;
	pmap_t		src_pmap;
	vaddr_t		dst_addr;
	vsize_t		len;
	vaddr_t		src_addr;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
	    dst_pmap, src_pmap, dst_addr, len, src_addr));
}

/*
 * pmap_collect:		[ INTERFACE ]
 *
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap)
	pmap_t		pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_collect(%p)\n", pmap));

	if (pmap == pmap_kernel()) {
		int bank, s;

		/*
		 * XXX This is very bogus.  We should handle kernel PT
		 * XXX pages much differently.
		 */

		s = splimp();
		for (bank = 0; bank < vm_nphysseg; bank++)
			pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			    ptoa(vm_physmem[bank].end));
		splx(s);
	} else {
		/*
		 * This process is about to be swapped out; free all of
		 * the PT pages by removing the physical mappings for its
		 * entire address space.  Note: pmap_remove() performs
		 * all necessary locking.
		 */
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
	}

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
#endif
}

/*
 * pmap_collect1:
 *
 *	Garbage-collect KPT pages.  Helper for the above (bogus)
 *	pmap_collect().
 *
 *	Note: THIS SHOULD GO AWAY, AND BE REPLACED WITH A BETTER
 *	WAY OF HANDLING PT PAGES!
 */
void
pmap_collect1(pmap, startpa, endpa)
	pmap_t		pmap;
	paddr_t		startpa, endpa;
{
	paddr_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	paddr_t kpa;
#ifdef DEBUG
	st_entry_t *ste;
	int opmapdebug = 0 /* XXX initialize to quiet gcc -Wall */;
#endif

	for (pa = startpa; pa < endpa; pa += NBPG) {
		struct kpt_page *kpt, **pkpt;

		/*
		 * Locate physical pages which are being used as kernel
		 * page table pages.
		 */
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptste && pv->pv_ptpmap == pmap_kernel())
				break;
		} while ((pv = pv->pv_next));
		if (pv == NULL)
			continue;
#ifdef DEBUG
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + MAC_MAX_PTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (pt_entry_t *)(pv->pv_va + NBPG);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf("collect: freeing KPT page at %lx (ste %x@@%p)\n",
			       pv->pv_va, *pv->pv_ptste, pv->pv_ptste);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
		}

		ste = pv->pv_ptste;
#endif
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove_entry to take care of invalidating
		 * ST and Sysptmap entries.
		 */
		pmap_extract(pmap, pv->pv_va, &kpa);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				    PRM_TFLUSH|PRM_CFLUSH);
		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != (struct kpt_page *)0;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef DEBUG
		if (kpt == (struct kpt_page *)0)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			       kpt->kpt_va, kpa);
#endif
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste != SG_NV)
			printf("collect: kernel STE at %p still valid (%x)\n",
			       ste, *ste);
		ste = &Sysptmap[ste - pmap_ste(pmap_kernel(), 0)];
		if (*ste != SG_NV)
			printf("collect: kernel PTmap at %p still valid (%x)\n",
			       ste, *ste);
#endif
	}
}

/*
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and using bzero to clear its contents, one
 *	machine dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
 */
void
pmap_zero_page(phys)
	paddr_t phys;
{
	int s, npte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%lx)\n", phys));

	npte = phys | PG_V;

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		/*
		 * Set copyback caching on the page; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte |= PG_CCB;
	}
#endif

	s = splimp();

	*caddr1_pte = npte;
	TBIS((vaddr_t)CADDR1);

	zeropage(CADDR1);

#ifdef DEBUG
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);
#endif

	splx(s);
}

/*
 * pmap_copy_page:		[ INTERFACE ]
 *
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using bcopy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
 */
void
pmap_copy_page(src, dst)
	paddr_t src, dst;
{
	int s, npte1, npte2;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_copy_page(%lx, %lx)\n", src, dst));

	npte1 = src | PG_RO | PG_V;
	npte2 = dst | PG_V;

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		/*
		 * Set copyback caching on the pages; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte1 |= PG_CCB;
		npte2 |= PG_CCB;
	}
#endif

	s = splimp();

	*caddr1_pte = npte1;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = npte2;
	TBIS((vaddr_t)CADDR2);

	copypage(CADDR1, CADDR2);

#ifdef DEBUG
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = PG_NV;
	TBIS((vaddr_t)CADDR2);
#endif

	splx(s);
}

/*
 * pmap_clear_modify:		[ INTERFACE ]
 *
 *	Clear the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;

	ret = pmap_is_modified(pg);

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_M);

	return (ret);
}

/*
 * pmap_clear_reference:	[ INTERFACE ]
 *
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;

	ret = pmap_is_referenced(pg);

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_U);

	return (ret);
}

/*
 * pmap_is_referenced:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pa, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pa, "FT"[rv]);
		return(rv);
	}
#endif
	return(pmap_testbit(pa, PG_U));
}

/*
 * pmap_is_modified:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pa, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pa, "FT"[rv]);
		return(rv);
	}
#endif
	return(pmap_testbit(pa, PG_M));
}

/*
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
pmap_phys_address(ppn)
	int ppn;
{
	return(m68k_ptob(ppn));
}

/*
 * Miscellaneous support routines follow
 */

/*
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
 */
/* static */
void
pmap_remove_mapping(pmap, va, pte, flags)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
	int flags;
{
	paddr_t pa;
	struct pv_entry *pv, *npv;
	pmap_t ptpmap;
	st_entry_t *ste;
	int s, bits;
#ifdef DEBUG
	pt_entry_t opte;
#endif

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    pmap, va, pte, flags));

	/*
	 * PTE not provided, compute it from pmap and va.
	 */
	if (pte == PT_ENTRY_NULL) {
		pte = pmap_pte(pmap, va);
		if (*pte == PG_NV)
			return;
	}

	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	opte = *pte;
#endif
	/*
	 * Update statistics
	 */
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */
	PMAP_DPRINTF(PDB_REMOVE, ("remove: invalidating pte at %p\n", pte));
	bits = *pte & (PG_U|PG_M);
	*pte = PG_NV;
	if ((flags & PRM_TFLUSH) && active_pmap(pmap))
		TBIS(va);
	/*
	 * For user mappings decrement the wiring count on
	 * the PT page.  We do this after the PTE has been
	 * invalidated because vm_map_pageable winds up in
	 * pmap_pageable which clears the modify bit for the
	 * PT page.
	 */
	if (pmap != pmap_kernel()) {
		(void)uvm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)(pte+1)), TRUE, FALSE);
#ifdef DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", (vaddr_t)trunc_page(pte));
#endif
	}
	/*
	 * If this isn't a managed page, we are all done.
	 */
	if (PAGE_IS_MANAGED(pa) == 0)
		return;
	/*
	 * Otherwise remove it from the PV table
	 * (raise IPL since we may be called at interrupt time).
	 */
	pv = pa_to_pvh(pa);
	ste = ST_ENTRY_NULL;
	s = splimp();
	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		ste = pv->pv_ptste;
		ptpmap = pv->pv_ptpmap;
		npv = pv->pv_next;
		if (npv) {
			npv->pv_flags = pv->pv_flags;
			*pv = *npv;
			pmap_free_pv(npv);
		} else
			pv->pv_pmap = NULL;
	} else {
		for (npv = pv->pv_next; npv; npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
			pv = npv;
		}
#ifdef DEBUG
		if (npv == NULL)
			panic("pmap_remove: PA not in pv_tab");
#endif
		ste = npv->pv_ptste;
		ptpmap = npv->pv_ptpmap;
		pv->pv_next = npv->pv_next;
		pmap_free_pv(npv);
		pv = pa_to_pvh(pa);
	}

	/*
	 * If this was a PT page we must also remove the
	 * mapping from the associated segment table.
	 */
	if (ste) {
		PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
		    ("remove: ste was %x@@%p pte was %x@@%p\n",
		    *ste, ste, opte, pmap_pte(pmap, va)));
#if defined(M68040)
		if (mmutype == MMU_68040) {
			st_entry_t *este = &ste[NPTEPG/SG4_LEV3SIZE];

			while (ste < este)
				*ste++ = SG_NV;
#ifdef DEBUG
			ste -= NPTEPG/SG4_LEV3SIZE;
#endif
		} else
#endif
		*ste = SG_NV;
		/*
		 * If it was a user PT page, we decrement the
		 * reference count on the segment table as well,
		 * freeing it if it is now empty.
		 */
		if (ptpmap != pmap_kernel()) {
			PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
			    ("remove: stab %p, refcnt %d\n",
			    ptpmap->pm_stab, ptpmap->pm_sref - 1));
#ifdef DEBUG
			if ((pmapdebug & PDB_PARANOIA) &&
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
				panic("remove: bogus ste");
#endif
			if (--(ptpmap->pm_sref) == 0) {
				PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
				    ("remove: free stab %p\n",
				    ptpmap->pm_stab));
				uvm_km_free_wakeup(st_map,
				    (vaddr_t)ptpmap->pm_stab, MAC_STSIZE);
				ptpmap->pm_stab = Segtabzero;
				ptpmap->pm_stpa = Segtabzeropa;
#if defined(M68040)
				if (mmutype == MMU_68040)
					ptpmap->pm_stfree = protostfree;
#endif
				/*
				 * XXX may have changed segment table
				 * pointer for current process so
				 * update now to reload hardware.
				 */
				if (active_user_pmap(ptpmap))
					PMAP_ACTIVATE(ptpmap, 1);
			}
#ifdef DEBUG
			else if (ptpmap->pm_sref < 0)
				panic("remove: sref < 0");
#endif
		}
#if 0
		/*
		 * XXX this should be unnecessary as we have been
		 * flushing individual mappings as we go.
		 */
		if (ptpmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
#endif
		pv->pv_flags &= ~PV_PTPAGE;
		ptpmap->pm_ptpages--;
	}
	/*
	 * Update saved attributes for managed page
	 */
	*pa_to_attribute(pa) |= bits;
	splx(s);
}

/*
 * pmap_testbit:
 *
 *	Test the modified/referenced bits of a physical page.
 */
/* static */
boolean_t
pmap_testbit(pa, bit)
	paddr_t pa;
	int bit;
{
	struct pv_entry *pv;
	pt_entry_t *pte;
	int s;

	if (PAGE_IS_MANAGED(pa) == 0)
		return (FALSE);

	pv = pa_to_pvh(pa);
	s = splimp();
	/*
	 * Check saved info first
	 */
	if (*pa_to_attribute(pa) & bit) {
		splx(s);
		return(TRUE);
	}

	/*
	 * Not found.  Check current mappings, returning immediately if
	 * found.  Cache a hit to speed future lookups.
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
			if (*pte & bit) {
				*pa_to_attribute(pa) |= bit;
				splx(s);
				return(TRUE);
			}
		}
	}
	splx(s);
	return(FALSE);
}

/*
 * pmap_changebit:
 *
 *	Change the modified/referenced bits, or other PTE bits,
 *	for a physical page.
 */
/* static */
void
pmap_changebit(pa, set, mask)
	paddr_t pa;
	int set, mask;
{
	struct pv_entry *pv;
	pt_entry_t *pte, npte;
	vaddr_t va;
	int s;
#if defined(M68040)
	boolean_t firstpage = TRUE;
#endif

	PMAP_DPRINTF(PDB_BITS,
	    ("pmap_changebit(%lx, %x, %x)\n", pa, set, mask));

	if (PAGE_IS_MANAGED(pa) == 0)
		return;

	pv = pa_to_pvh(pa);
	s = splimp();

	/*
	 * Clear saved attributes (modify, reference)
	 */
	*pa_to_attribute(pa) &= mask;

	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */
	if (pv->pv_pmap != NULL) {
#ifdef DEBUG
		int toflush = 0;
#endif
		for (; pv; pv = pv->pv_next) {
#ifdef DEBUG
			toflush |= (pv->pv_pmap == pmap_kernel()) ? 2 : 1;
#endif
			va = pv->pv_va;

			/*
			 * XXX don't write protect pager mappings
			 */
			if (set == PG_RO) {
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
					continue;
			}

			pte = pmap_pte(pv->pv_pmap, va);
			npte = (*pte | set) & mask;
			if (*pte != npte) {
#if defined(M68040)
				/*
				 * If we are changing caching status or
				 * protection make sure the caches are
				 * flushed (but only once).
				 */
				if (firstpage && (mmutype == MMU_68040) &&
				    ((set == PG_RO) ||
				     (set & PG_CMASK) ||
				     (mask & PG_CMASK) == 0)) {
					firstpage = FALSE;
					DCFP(pa);
					ICPP(pa);
				}
#endif
				*pte = npte;
				if (active_pmap(pv->pv_pmap))
					TBIS(va);
			}
		}
	}
	splx(s);
}

/*
 * pmap_enter_ptpage:
 *
 *	Allocate and map a PT page for the specified pmap/va pair.
 */
/* static */
void
pmap_enter_ptpage(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	paddr_t ptpa;
	struct pv_entry *pv;
	st_entry_t *ste;
	int s;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE,
	    ("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va));

	/*
	 * Allocate a segment table if necessary.  Note that it is allocated
	 * from a private map and not pt_map.  This keeps user page tables
	 * aligned on segment boundaries in the kernel address space.
	 * The segment table is wired down.  It will be freed whenever the
	 * reference count drops to zero.
	 */
	if (pmap->pm_stab == Segtabzero) {
		pmap->pm_stab = (st_entry_t *)
		    uvm_km_zalloc(st_map, MAC_STSIZE);
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab,
			(paddr_t *)&pmap->pm_stpa);
#if defined(M68040)
		if (mmutype == MMU_68040) {
#ifdef DEBUG
			if (dowriteback && dokwriteback)
#endif
			pmap_changebit((paddr_t)pmap->pm_stpa, 0, ~PG_CCB);
			pmap->pm_stfree = protostfree;
		}
#endif
		/*
		 * XXX may have changed segment table pointer for current
		 * process so update now to reload hardware.
		 */
		if (active_user_pmap(pmap))
			PMAP_ACTIVATE(pmap, 1);

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: pmap %p stab %p(%p)\n",
		    pmap, pmap->pm_stab, pmap->pm_stpa));
	}

	ste = pmap_ste(pmap, va);
#if defined(M68040)
	/*
	 * Allocate level 2 descriptor block if necessary
	 */
	if (mmutype == MMU_68040) {
		if (*ste == SG_NV) {
			int ix;
			caddr_t addr;

			ix = bmtol2(pmap->pm_stfree);
			if (ix == -1)
				panic("enter: out of address space"); /* XXX */
			pmap->pm_stfree &= ~l2tobm(ix);
			addr = (caddr_t)&pmap->pm_stab[ix*SG4_LEV2SIZE];
			bzero(addr, SG4_LEV2SIZE*sizeof(st_entry_t));
			addr = (caddr_t)&pmap->pm_stpa[ix*SG4_LEV2SIZE];
			*ste = (u_int)addr | SG_RW | SG_U | SG_V;

			PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
			    ("enter: alloc ste2 %d(%p)\n", ix, addr));
		}
		ste = pmap_ste2(pmap, va);
		/*
		 * Since a level 2 descriptor maps a block of SG4_LEV3SIZE
		 * level 3 descriptors, we need a chunk of NPTEPG/SG4_LEV3SIZE
		 * (16) such descriptors (NBPG/SG4_LEV3SIZE bytes) to map a
		 * PT page--the unit of allocation.  We set `ste' to point
		 * to the first entry of that chunk which is validated in its
		 * entirety below.
		 */
		ste = (st_entry_t *)((int)ste & ~(NBPG/SG4_LEV3SIZE-1));

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: ste2 %p (%p)\n", pmap_ste2(pmap, va), ste));
	}
#endif
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));

	/*
	 * In the kernel we allocate a page from the kernel PT page
	 * free list and map it into the kernel page table map (via
	 * pmap_enter).
	 */
	if (pmap == pmap_kernel()) {
		struct kpt_page *kpt;

		s = splimp();
		if ((kpt = kpt_free_list) == (struct kpt_page *)0) {
			/*
			 * No PT pages available.
			 * Try once to free up unused ones.
			 */
			PMAP_DPRINTF(PDB_COLLECT,
			    ("enter: no KPT pages, collecting...\n"));
			pmap_collect(pmap_kernel());
			if ((kpt = kpt_free_list) == (struct kpt_page *)0)
				panic("pmap_enter_ptpage: can't get KPT page");
		}
		kpt_free_list = kpt->kpt_next;
		kpt->kpt_next = kpt_used_list;
		kpt_used_list = kpt;
		ptpa = kpt->kpt_pa;
		bzero((caddr_t)kpt->kpt_va, NBPG);
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT,
		    VM_PROT_DEFAULT|PMAP_WIRED);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE)) {
			int ix = pmap_ste(pmap, va) - pmap_ste(pmap, 0);

			printf("enter: add &Sysptmap[%d]: %x (KPT page %lx)\n",
			       ix, Sysptmap[ix], kpt->kpt_va);
		}
#endif
		splx(s);
	}
	/*
	 * For user processes we just simulate a fault on that location
	 * letting the VM system allocate a zero-filled page.
	 */
	else {
		/*
		 * Count the segment table reference now so that we won't
		 * lose the segment table when low on memory.
		 */
		pmap->pm_sref++;
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
		    ("enter: about to fault UPT pg at %lx\n", va));
		s = uvm_fault(pt_map, va, 0, VM_PROT_READ|VM_PROT_WRITE);
		if (s) {
			printf("uvm_fault(pt_map, 0x%lx, 0, RW) -> %d\n",
			    va, s);
			panic("pmap_enter: uvm_fault failed");
		}
		pmap_extract(pmap_kernel(), va, &ptpa);
	}
#if defined(M68040)
	/*
	 * Turn off copyback caching of page table pages,
	 * could get ugly otherwise.
	 */
#ifdef DEBUG
	if (dowriteback && dokwriteback)
#endif
	if (mmutype == MMU_68040) {
#ifdef DEBUG
		pt_entry_t *pte = pmap_pte(pmap_kernel(), va);
		if ((pmapdebug & PDB_PARANOIA) && (*pte & PG_CCB) == 0)
			printf("%s PT no CCB: kva=%lx ptpa=%lx pte@@%p=%x\n",
			       pmap == pmap_kernel() ? "Kernel" : "User",
			       va, ptpa, pte, *pte);
#endif
		pmap_changebit(ptpa, 0, ~PG_CCB);
	}
#endif
	/*
	 * Locate the PV entry in the kernel for this PT page and
	 * record the STE address.  This is so that we can invalidate
	 * the STE when we remove the mapping for the page.
	 */
	pv = pa_to_pvh(ptpa);
	s = splimp();
	if (pv) {
		pv->pv_flags |= PV_PTPAGE;
		do {
			if (pv->pv_pmap == pmap_kernel() && pv->pv_va == va)
				break;
		} while ((pv = pv->pv_next));
	}
#ifdef DEBUG
	if (pv == NULL)
		panic("pmap_enter_ptpage: PT page not entered");
#endif
	pv->pv_ptste = ste;
	pv->pv_ptpmap = pmap;

	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
	    ("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste));

	/*
	 * Map the new PT page into the segment table.
	 * Also increment the reference count on the segment table if this
	 * was a user page table page.  Note that we don't use vm_map_pageable
	 * to keep the count like we do for PT pages, this is mostly because
	 * it would be difficult to identify ST pages in pmap_pageable to
	 * release them.  We also avoid the overhead of vm_map_pageable.
	 */
#if defined(M68040)
	if (mmutype == MMU_68040) {
		st_entry_t *este;

		for (este = &ste[NPTEPG/SG4_LEV3SIZE]; ste < este; ste++) {
			*ste = ptpa | SG_U | SG_RW | SG_V;
			ptpa += SG4_LEV3SIZE * sizeof(st_entry_t);
		}
	} else
#endif
	*ste = (ptpa & SG_FRAME) | SG_RW | SG_V;
	if (pmap != pmap_kernel()) {
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: stab %p refcnt %d\n",
		    pmap->pm_stab, pmap->pm_sref));
	}
#if 0
	/*
	 * Flush stale TLB info.
	 */
	if (pmap == pmap_kernel())
		TBIAS();
	else
		TBIAU();
#endif
	pmap->pm_ptpages++;
	splx(s);
}

#ifdef DEBUG
/*
 * pmap_pvdump:
 *
 *	Dump the contents of the PV list for the specified physical page.
 */
/* static */
void
pmap_pvdump(pa)
	paddr_t pa;
{
	struct pv_entry *pv;

	printf("pa %lx", pa);
	for (pv = pa_to_pvh(pa); pv; pv = pv->pv_next)
		printf(" -> pmap %p, va %lx, ptste %p, ptpmap %p, flags %x",
		       pv->pv_pmap, pv->pv_va, pv->pv_ptste, pv->pv_ptpmap,
		       pv->pv_flags);
	printf("\n");
}

/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
/* static */
void
pmap_check_wiring(str, va)
	char *str;
	vaddr_t va;
{
	struct vm_map_entry *entry;
	int count;
	pt_entry_t *pte;

	va = trunc_page(va);
	if (!pmap_ste_v(pmap_kernel(), va) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;

	if (!uvm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
		return;
	}
	count = 0;
	for (pte = (pt_entry_t *)va; pte < (pt_entry_t *)(va + NBPG); pte++)
		if (*pte)
			count++;
	if (entry->wired_count != count)
		printf("*%s*: %lx: w%d/a%d\n",
		       str, va, entry->wired_count, count);
}
#endif /* DEBUG */

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pmap_enter(pmap_kernel(), va, pa, prot, VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
@


1.37
log
@Temporarily revert the pmap_motorola changes, as they may account for
some problems as well.
Requested by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2001/11/28 16:13:28 art Exp $	*/
@


1.36
log
@Not needed anymore.
@
text
@@


1.35
log
@zap some typedefs.
vm_map_t -> struct vm_map *
vm_map_entry_t -> struct vm_map_entry *
simple_lock_data_t -> struct simplelock

(uvm not done yet, coming in the next commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2001/11/28 15:34:16 art Exp $	*/
@


1.34
log
@Make pmap_update functions into nops so that we can have a consistent
pmap_update API (right now it's nop).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2001/11/28 14:13:06 art Exp $	*/
d212 1
a212 1
vm_map_t	st_map, pt_map;
d2316 1
a2316 1
	vm_map_entry_t entry;
@


1.33
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2001/11/28 13:47:38 art Exp $	*/
a1364 16
}

/*
 * pmap_update:
 *
 *	Require that all active physical maps contain no
 *	incorrect entires NOW, by processing any deferred
 *	pmap operations.
 */
void
pmap_update()
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_update()\n"));

	TBIA();		/* XXX should not be here. */
@


1.32
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2001/11/07 01:18:00 art Exp $	*/
a2358 12
}

void
pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs)
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	}
@


1.31
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2001/11/06 02:07:59 art Exp $	*/
d344 1
a344 1
				UVM_FLAG_FIXED)) != KERN_SUCCESS)
d351 1
a351 1
				UVM_FLAG_FIXED)) != KERN_SUCCESS) {
d425 1
a425 1
	if (rv != KERN_SUCCESS || (addr + s) >= (vaddr_t)Sysmap)
d427 1
a427 3
	rv = uvm_unmap(kernel_map, addr, addr + s);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: uvm_unmap failed");
d1260 1
a1260 1
	return (KERN_SUCCESS);
d2211 1
a2211 1
		if (s != KERN_SUCCESS) {
@


1.30
log
@Redundant includes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/09/19 20:50:56 mickey Exp $	*/
d341 1
a341 1
		    NULL, UVM_UNKNOWN_OFFSET,
d348 1
a348 1
		    NULL, UVM_UNKNOWN_OFFSET,
d422 1
a422 1
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET,
@


1.29
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2001/07/31 15:31:25 beck Exp $	*/
a104 3

#include <vm/vm.h>
#include <vm/vm_page.h>
@


1.28
log
@type mismatch fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/07/25 13:25:32 art Exp $	*/
a106 1
#include <vm/vm_kern.h>
@


1.27
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2001/07/18 10:47:04 art Exp $	*/
d1670 1
a1670 1
void
@


1.26
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2001/06/27 04:22:38 art Exp $	*/
d652 1
a652 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE, 0);
d1054 2
a1055 2
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
d1060 1
a1060 2
	boolean_t wired;
	vm_prot_t access_type;
d1067 1
a1072 3
	if (pmap == NULL)
		return;

d1197 1
a1197 1
		if (access_type & ~prot)
d1200 1
a1200 1
		if (access_type & VM_PROT_WRITE)
d1202 1
a1202 1
		else if (access_type & VM_PROT_ALL)
d1265 2
d2192 2
a2193 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE,
		    VM_PROT_DEFAULT);
d2364 1
a2364 1
	pmap_enter(pmap_kernel(), va, pa, prot, 1, VM_PROT_READ|VM_PROT_WRITE);
d2374 2
a2375 2
			VM_PROT_READ|VM_PROT_WRITE, 1,
			VM_PROT_READ|VM_PROT_WRITE);
@


1.25
log
@old vm no more
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2001/06/27 04:02:05 beck Exp $	*/
d666 2
a667 3
pmap_t
pmap_create(size)
	vsize_t	size;
d669 1
a669 1
	pmap_t pmap;
a673 6
	/*
	 * Software use map does not need a pmap
	 */
	if (size)
		return (NULL);

d892 2
a893 1
pmap_page_protect(pa, prot)
a894 2
	vm_prot_t	prot;
{
d898 2
d1651 2
a1652 3
void
pmap_clear_modify(pa)
	paddr_t	pa;
d1654 4
d1662 2
d1672 1
a1672 2
pmap_clear_reference(pa)
	paddr_t	pa;
d1674 4
d1682 2
d1693 1
a1693 2
pmap_is_referenced(pa)
	paddr_t	pa;
d1695 2
d1714 1
a1714 2
pmap_is_modified(pa)
	paddr_t	pa;
d1716 2
d2361 26
@


1.24
log
@fix mac pool_get panics on forks - ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2001/06/24 04:20:44 art Exp $	*/
a109 1
#if defined(UVM)
a110 1
#endif
a216 1
#if defined(UVM)
a217 1
#endif
a341 1
#if defined(UVM)
a363 20
#else
	addr = (vaddr_t)IOBase;
	(void)vm_map_find(kernel_map, NULL, (vaddr_t)0, &addr,
	    m68k_ptob(IIOMAPSIZE + ROMMAPSIZE + VIDMAPSIZE), FALSE);
	if (addr != (vaddr_t)IOBase)
		goto bogons;

	addr = (vaddr_t)Sysmap;
	vm_object_reference(kernel_object);
	(void)vm_map_find(kernel_map, kernel_object, addr,
			   &addr, MAC_MAX_PTSIZE, FALSE);
	/*
	 * If this fails it is probably because the static portion of
	 * the kernel page table isn't big enough and we overran the
	 * page table map.   Need to adjust pmap_size() in mac68k_init.c.
	 */
	if (addr != (vaddr_t)Sysmap)
 bogons:
		panic("pmap_init: bogons in the VM system!");
#endif /* UVM */
a381 1
#if defined(UVM)
a384 3
#else
	addr = kmem_alloc(kernel_map, s);
#endif
a424 1
#if defined(UVM)
a433 7
#else
	addr = 0;
	rv = vm_map_find(kernel_map, NULL, 0, &addr, s, TRUE);
	if (rv != KERN_SUCCESS || addr + s >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	vm_map_remove(kernel_map, addr, addr + s);
#endif
a438 1
#if defined(UVM)
a441 3
#else
	addr = kmem_alloc(kernel_map, s);
#endif
a456 1
#if defined(UVM)
a477 39
#else
	/*
	 * Allocate the segment table map
	 */
	s = maxproc * MAC_STSIZE;
	st_map = kmem_suballoc(kernel_map, &addr, &addr2, s, TRUE);

	/*
	 * Slightly modified version of kmem_suballoc() to get page table
	 * map where we want it.
	 */
	addr = MAC_PTBASE;
	if ((MAC_PTMAXSIZE / MAC_MAX_PTSIZE) < maxproc) {
		s = MAC_PTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(2).
		 */
		maxproc = (MAC_PTMAXSIZE / MAC_MAX_PTSIZE);
	} else
		s = (maxproc * MAC_MAX_PTSIZE);
	addr2 = addr + s;
	rv = vm_map_find(kernel_map, NULL, 0, &addr, s, TRUE);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: cannot allocate space for PT map");
	pmap_reference(vm_map_pmap(kernel_map));
	pt_map = vm_map_create(vm_map_pmap(kernel_map), addr, addr2, TRUE);
	if (pt_map == NULL)
		panic("pmap_init: cannot create pt_map");
	rv = vm_map_submap(kernel_map, addr, addr2, pt_map);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: cannot map range to pt_map");
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: pt_map [%lx - %lx)\n", addr, addr2);
#endif
#endif /* UVM */
a511 1
#if defined(UVM)
a514 5
#else
		pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: kmem_alloc() failed");
#endif
a560 1
#if defined(UVM)
a561 3
#else
		kmem_free(kernel_map, (vaddr_t)pvp, NBPG);
#endif
a625 1
#if defined(UVM)
a626 3
#else
		kmem_free(kernel_map, (vaddr_t)pvp, NBPG);
#endif
a766 1
#if defined(UVM)
a768 4
#else
		kmem_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
		    MAC_MAX_PTSIZE);
#endif
a769 1
#if defined(UVM)
a771 4
#else
		kmem_free_wakeup(st_map, (vaddr_t)pmap->pm_stab,
		    MAC_STSIZE);
#endif
a1094 1
#if defined(UVM)
a1096 4
#else
		pmap->pm_ptab = (pt_entry_t *)
		    kmem_alloc_wait(pt_map, MAC_MAX_PTSIZE);
#endif
a1152 1
#if defined(UVM)
a1154 4
#else
		(void)vm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)(pte+1)), FALSE);
#endif
a1810 1
#if defined(UVM)
a1812 4
#else
		(void)vm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)(pte+1)), TRUE);
#endif
a1900 1
#if defined(UVM)
a1902 4
#else
				kmem_free_wakeup(st_map,
				    (vaddr_t)ptpmap->pm_stab, MAC_STSIZE);
#endif
a2039 1
#if defined(UVM)
a2041 6
#else
				extern vaddr_t pager_sva, pager_eva;

				if (va >= pager_sva && va < pager_eva)
					continue;
#endif
a2097 1
#if defined(UVM)
a2099 3
#else
		pmap->pm_stab = (st_entry_t *)kmem_alloc(st_map, MAC_STSIZE);
#endif
a2210 1
#if defined(UVM)
a2216 7
#else
		s = vm_fault(pt_map, va, VM_PROT_READ|VM_PROT_WRITE, FALSE);
		if (s != KERN_SUCCESS) {
			printf("vm_fault(pt_map, %lx, RW, 0) -> %d\n", va, s);
			panic("pmap_enter: vm_fault failed");
		}
#endif
a2217 5
#if !defined (UVM)
#ifdef DEBUG
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_PTPAGE;
#endif
#endif
a2341 1
#if defined(UVM)
a2345 6
#else
	if (!vm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
		return;
	}
#endif
@


1.23
log
@oops. another one.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2001/06/23 21:42:23 art Exp $	*/
a2361 6
		/*
		 * Mark the page clean now to avoid its pageout (and
		 * hence creation of a pager) between now and when it
		 * is wired; i.e. while it is on a paging queue.
		 */
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_CLEAN;
@


1.22
log
@oops. unbreak after the last change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2001/06/08 08:09:02 art Exp $	*/
d2237 1
a2237 1
			(paddr_t *)pmap->pm_stpa);
@


1.21
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2001/06/08 02:47:39 aaron Exp $	*/
d416 1
a416 1
	pmap_extract(pmap_kernel(), addr, (paddr_t *)Segtabzeropa);
@


1.20
log
@Adjust to recent locking protocol changes; beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2001/05/09 15:31:25 art Exp $	*/
d416 1
a416 1
	Segtabzeropa = (st_entry_t *)pmap_extract(pmap_kernel(), addr);
d492 1
a492 1
		kpt_pages->kpt_pa = pmap_extract(pmap_kernel(), addr2);
d1448 2
a1449 2
paddr_t
pmap_extract(pmap, va)
d1452 1
a1458 1
	pa = 0;
d1461 4
a1464 2
	if (pa)
		pa = (pa & PG_FRAME) | (va & ~PG_FRAME);
d1468 2
a1469 1
	return (pa);
d1627 1
a1627 1
		kpa = pmap_extract(pmap, pv->pv_va);
d2236 2
a2237 2
		pmap->pm_stpa = (st_entry_t *)
		    pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab);
d2361 1
a2361 1
		ptpa = pmap_extract(pmap_kernel(), va);
@


1.19
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2001/05/08 17:30:41 aaron Exp $	*/
d1265 1
a1265 1
		    round_page((vaddr_t)(pte+1)), FALSE);
d1925 1
a1925 1
		    round_page((vaddr_t)(pte+1)), TRUE);
@


1.18
log
@Substantial update from NetBSD, most notably gives us UVM support; deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2001/05/05 21:26:38 art Exp $	*/
d1392 1
a1392 1
 * pmap_change_wiring:		[ INTERFACE ]
d1399 1
a1399 1
pmap_change_wiring(pmap, va, wired)
a1401 1
	boolean_t	wired;
d1405 1
a1405 2
	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_change_wiring(%p, %lx, %x)\n", pmap, va, wired));
d1419 1
a1419 1
			printf("pmap_change_wiring: invalid STE for %lx\n", va);
d1428 1
a1428 1
			printf("pmap_change_wiring: invalid PTE for %lx\n", va);
d1436 3
a1438 6
	if (pmap_pte_w_chg(pte, wired ? PG_W : 0)) {
		pmap_pte_set_w(pte, wired);
		if (wired)
			pmap->pm_stats.wired_count++;
		else
			pmap->pm_stats.wired_count--;
a1761 73
}

/*
 * pmap_pageable:		[ INTERFACE ]
 *
 *	Make the specified pages (by pmap, offset) pageable (or not) as
 *	requested.
 *
 *	A page which is not pageable may not take a fault; therefore,
 *	its page table entry must remain valid for the duration.
 *
 *	This routine is merely advisory; pmap_enter() will specify that
 *	these pages are to be wired down (or not) as appropriate.
 */
void
pmap_pageable(pmap, sva, eva, pageable)
	pmap_t		pmap;
	vaddr_t		sva, eva;
	boolean_t	pageable;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_pageable(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, pageable));

	/*
	 * If we are making a PT page pageable then all valid
	 * mappings must be gone from that page.  Hence it should
	 * be all zeros and there is no need to clean it.
	 * Assumptions:
	 *	- we are called with only one page at a time
	 *	- PT pages have only one pv_table entry
	 */
	if (pmap == pmap_kernel() && pageable && sva + NBPG == eva) {
		struct pv_entry *pv;
		paddr_t pa;

#ifdef DEBUG
		if ((pmapdebug & (PDB_FOLLOW|PDB_PTPAGE)) == PDB_PTPAGE)
			printf("pmap_pageable(%p, %lx, %lx, %x)\n",
			       pmap, sva, eva, pageable);
#endif
		if (!pmap_ste_v(pmap, sva))
			return;
		pa = pmap_pte_pa(pmap_pte(pmap, sva));
		if (PAGE_IS_MANAGED(pa) == 0)
			return;
		pv = pa_to_pvh(pa);
		if (pv->pv_ptste == NULL)
			return;
#ifdef DEBUG
		if (pv->pv_va != sva || pv->pv_next) {
			printf("pmap_pageable: bad PT page va %lx next %p\n",
			       pv->pv_va, pv->pv_next);
			return;
		}
#endif
		/*
		 * page is unused, free it now!
		 */
		pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
				    NULL, PRM_TFLUSH|PRM_CFLUSH);
#if defined(UVM)
		uvm_pagefree(PHYS_TO_VM_PAGE(pa));
#else
		vm_page_free(PHYS_TO_VM_PAGE(pa));
#endif
#ifdef DEBUG
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_pageable: PT page %lx(%x) freed\n",
			       sva, *pmap_pte(pmap, sva));
#endif
	}
@


1.17
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.16 1999/09/03 18:01:14 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.28 1996/10/21 05:42:27 scottr Exp $	*/
d4 1
a4 1
/* 
d63 1
a63 1
 *		(*pte & PG_PROT) [4] != pte->pg_prot [1]
d102 1
d110 4
a115 49
#ifdef PMAPSTATS
struct {
	int collectscans;
	int collectpages;
	int kpttotal;
	int kptinuse;
	int kptmaxuse;
} kpt_stats;
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int nochange;	/* no change at all */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int pchange;	/* no mapping change, just protection */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
} enter_stats;
struct {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
} remove_stats;
struct {
	int calls;
	int changed;
	int alreadyro;
	int alreadyrw;
} protect_stats;
struct chgstats {
	int setcalls;
	int sethits;
	int setmiss;
	int clrcalls;
	int clrhits;
	int clrmiss;
} changebit_stats[16];
#endif

a116 2
int debugmap = 0;
int pmapdebug = 0x2000;
d133 5
d142 3
a144 3

extern vm_offset_t pager_sva, pager_eva;
#endif
d150 2
a151 2
#define pmap_ste1(m, v) \
	(&((m)->pm_stab[(vm_offset_t)(v) >> SG4_SHIFT1]))
d156 2
a157 2
#define pmap_ste(m, v) \
	(&((m)->pm_stab[(vm_offset_t)(v) \
d160 3
a162 3
	(mmutype == MMU_68040			\
	 ? ((*pmap_ste1(m, v) & SG_V) &&	\
	    (*pmap_ste2(m, v) & SG_V))  	\
d165 2
a166 2
#define	pmap_ste(m, v)   (&((m)->pm_stab[(vm_offset_t)(v) >> SG_ISHIFT]))
#define	pmap_ste_v(m, v) (*pmap_ste(m, v) & SG_V)
d169 1
a169 1
#define pmap_pte(m, v)	(&((m)->pm_ptab[(vm_offset_t)(v) >> PG_SHIFT]))
d178 1
a178 1
#define pmap_pte_set_w(pte, v)	  \
a184 4
#define pmap_valid_page(pa)	(pmap_initialized && pmap_page_index(pa) >= 0)

int pmap_page_index(vm_offset_t pa);

d197 2
a198 2
	vm_offset_t	kpt_va;		/* always valid kernel VA */
	vm_offset_t	kpt_pa;		/* PA of this page (for speed) */
d215 1
a215 1
vm_size_t	Sysptsize = VM_KERNEL_PT_PAGES;
d219 3
d223 6
a228 10
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_next;	/* Next available physical page		*/
int		avail_remaining;/* Number of physical free pages left	*/
int		avail_range;	/* Range avail_next is in		*/
vm_offset_t	avail_end;	/* Set for ps and friends as		*/
				/*       avail_start + avail_remaining. */
vm_size_t	mem_size;	/* memory size in bytes */
vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
int		npages;
d236 3
a238 4
/* The following three variables are defined in pmap_bootstrap.c */
extern int		numranges;
extern unsigned long	low[8];
extern unsigned long	high[8];
d244 30
d277 8
a284 4
void	pmap_remove_mapping  __P((pmap_t, vm_offset_t, pt_entry_t *, int));
boolean_t	pmap_testbit __P((vm_offset_t, int));
void	pmap_changebit       __P((vm_offset_t, int, boolean_t));
void	pmap_enter_ptpage    __P((pmap_t, vm_offset_t));
d286 2
a287 2
void	pmap_pvdump          __P((vm_offset_t));
void	pmap_check_wiring    __P((char *, vm_offset_t));
d295 17
a311 32
 * Bootstrap memory allocator. This function allows for early dynamic
 * memory allocation until the virtual memory system has been bootstrapped.
 * After that point, either kmem_alloc or malloc should be used. This
 * function works by stealing pages from the (to be) managed page pool,
 * stealing virtual address space, then mapping the pages and zeroing them.
 *
 * It should be used from pmap_bootstrap till vm_page_startup, afterwards
 * it cannot be used, and will generate a panic if tried. Note that this
 * memory will never be freed, and in essence it is wired down.
 */
void *
pmap_bootstrap_alloc(size)
	int size;
{
	extern boolean_t vm_page_startup_initialized;
	vm_offset_t val;
	
	if (vm_page_startup_initialized)
		panic("pmap_bootstrap_alloc: called after startup initialized");
	size = round_page(size);
	val = virtual_avail;

	virtual_avail = pmap_map(virtual_avail, avail_start,
		avail_start + size, VM_PROT_READ|VM_PROT_WRITE);
	avail_start += size;

	avail_remaining -= m68k_btop (size);
	/* XXX hope this doesn't pop it into the next range: */
	avail_next += size;

	bzero ((caddr_t) val, size);
	return ((void *) val);
d315 6
a320 3
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
d325 16
a340 3
	vm_offset_t	addr, addr2;
	vm_size_t	s;
	int		rv;
a341 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_init()\n");
#endif
d344 1
a344 1
	 * unavailable regions which we have mapped in pmap_bootstrap.
d346 29
a374 6
	addr = (vm_offset_t) IOBase;
	(void) vm_map_find(kernel_map, NULL, (vm_offset_t) 0, &addr,
			   m68k_ptob(IIOMAPSIZE + ROMMAPSIZE + NBMAPSIZE),
			   FALSE);
	if (addr != (vm_offset_t)IOBase)
		panic("pmap_init: I/O space not mapped!");
d376 1
a376 1
	addr = (vm_offset_t) Sysmap;
d378 1
a378 1
	(void) vm_map_find(kernel_map, kernel_object, addr,
d385 2
a386 1
	if (addr != (vm_offset_t)Sysmap)
d388 1
d390 6
a395 8
#ifdef DEBUG
	if (pmapdebug & PDB_INIT) {
		printf("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
		       Sysseg, Sysmap, Sysptmap);
		printf("  pstart %lx, plast %x, vstart %lx, vend %lx\n",
		    avail_start, avail_remaining, virtual_avail, virtual_end);
	}
#endif
d401 5
a405 5
	/*
	 * This is wasteful on MACHINE_NONCONTIG.  Is it avoidable?
	 */
	npages = atop(high[numranges - 1] - 1);
	s = (vm_size_t) (MAC_STSIZE + sizeof(struct pv_entry)*npages + npages);
d407 10
a416 3
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
	Segtabzero = (st_entry_t *) addr;
	Segtabzeropa = (st_entry_t *) pmap_extract(pmap_kernel(), addr);
d418 24
a441 9
	pv_table = (struct pv_entry *) addr;
	addr += sizeof(struct pv_entry) * npages;
	pmap_attributes = (char *) addr;
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %lx bytes: npages %x s0 %p(%p) tbl %p atr %p\n",
		       s, npages, Segtabzero, Segtabzeropa,
		       pv_table, pmap_attributes);
#endif
d454 11
d467 1
a467 1
	if (rv != KERN_SUCCESS || addr + s >= (vm_offset_t)Sysmap)
d470 1
d476 7
a482 1
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
d486 1
a486 1
	kpt_free_list = (struct kpt_page *) 0;
a493 8
#ifdef PMAPSTATS
	kpt_stats.kpttotal = atop(s);
#endif
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: KPT: %ld pages from %lx to %lx\n",
		       atop(s), addr, addr + s);
#endif
d495 26
d532 11
a542 2
	s = (MAC_PTMAXSIZE / MAC_MAX_PTSIZE < maxproc) ?
		MAC_PTMAXSIZE : (maxproc * MAC_MAX_PTSIZE);
d558 1
d569 6
d580 6
a585 4
static struct pv_entry *pmap_alloc_pv __P((void));
static void	pmap_free_pv __P((struct pv_entry *));

static struct pv_entry *
d588 3
a590 3
	struct pv_page	*pvp;
	struct pv_entry	*pv;
	int		i;
d593 5
d601 1
d625 6
a630 1
static void
d634 1
a634 1
	register struct pv_page *pvp;
d648 5
a652 1
		kmem_free(kernel_map, (vm_offset_t) pvp, NBPG);
d657 5
a661 2
void	pmap_collect_pv __P((void));

d678 3
a680 2
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp, pvp_pgi.pgi_list);
			pv_nfree -= pvp->pvp_pgi.pgi_nfree;
d688 1
a688 1
	for (ph = &pv_table[npages - 1]; ph >= &pv_table[0]; ph--) {
d697 2
a698 1
					TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
d717 5
a721 1
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
d726 2
d733 2
d736 1
a736 1
vm_offset_t
d738 3
a740 2
	vm_offset_t	va, spa, epa;
	int		prot;
d743 2
a744 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot);
#endif
d751 1
a751 1
	return(va);
d755 2
d759 1
a759 8
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
 *
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
d763 1
a763 1
	vm_size_t	size;
d765 1
a765 1
	register pmap_t pmap;
d767 2
a768 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create(%lx)\n", size);
#endif
d774 3
a776 1
		return(NULL);
a777 6
	/* XXX: is it ok to wait here? */
	pmap = (pmap_t) malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
#ifdef notifwewait
	if (pmap == NULL)
		panic("pmap_create: cannot allocate a pmap");
#endif
d784 5
a788 2
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
d792 1
a792 1
	register struct pmap *pmap;
d795 2
a796 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_pinit(%p)\n", pmap);
#endif
a809 1
	pmap->pm_stchanged = TRUE;
d815 4
a818 3
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
d822 1
a822 1
	register pmap_t pmap;
d829 1
a829 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%p)\n", pmap);
#endif
d836 1
a836 1
		free((caddr_t)pmap, M_VMPMAP);
d841 5
a845 3
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
d849 1
a849 1
	register struct pmap *pmap;
d852 1
a852 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_release(%p)\n", pmap);
#endif
d862 7
a868 2
		kmem_free_wakeup(pt_map, (vm_offset_t)pmap->pm_ptab,
				 MAC_MAX_PTSIZE);
d870 7
a876 2
		kmem_free_wakeup(st_map, (vm_offset_t)pmap->pm_stab,
				MAC_STSIZE);
d880 2
d892 1
a892 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%p)\n", pmap);
#endif
d899 10
a908 3
void	loadustp __P((vm_offset_t));
void	pmap_activate __P((register pmap_t, struct pcb *));

d910 2
a911 3
pmap_activate(pmap, pcbp)
	register pmap_t pmap;
	struct pcb *pcbp;
d913 1
d915 2
a916 7
	if (pmap == NULL)
		return;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_SEGTAB))
		printf("pmap_activate(%p, %p)\n", pmap, pcbp);
#endif
d918 1
a918 1
	PMAP_ACTIVATE(pmap, pcbp, pmap == curproc->p_vmspace->vm_map.pmap);
d921 9
a929 2
void	pmap_deactivate __P((register pmap_t, struct pcb *));

d931 2
a932 3
pmap_deactivate(pmap, pcb)
	register pmap_t pmap;
	struct pcb *pcb;
d934 2
d939 2
d948 2
a949 2
	register pmap_t pmap;
	vm_offset_t sva, eva;
d951 2
a952 2
	register vm_offset_t nssva;
	register pt_entry_t *pte;
d956 2
a957 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
#endif
a961 3
#ifdef PMAPSTATS
	remove_stats.calls++;
#endif
d998 1
a998 1
 *	pmap_page_protect:
d1000 2
a1001 1
 *	Lower the permission for all mappings to a given page.
d1005 1
a1005 1
	vm_offset_t	pa;
d1008 1
a1008 1
	register struct pv_entry *pv;
d1016 1
a1016 1
	if (!pmap_valid_page (pa))
d1026 1
a1026 1
		pmap_changebit(pa, PG_RO, TRUE);
d1035 1
a1035 1
		register pt_entry_t *pte;
d1051 1
a1051 1
					"pmap_page_protect:", pa);
d1053 2
d1061 4
a1064 2
 *	Set the physical protection on the
 *	specified range of this map as requested.
d1068 3
a1070 3
	register pmap_t		pmap;
	register vm_offset_t	sva, eva;
	vm_prot_t		prot;
d1072 2
a1073 2
	register vm_offset_t nssva;
	register pt_entry_t *pte;
d1077 3
a1079 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%p, %lx, %lx, %x)\n", pmap, sva, eva, prot);
#endif
a1083 3
#ifdef PMAPSTATS
	protect_stats.calls++;
#endif
d1119 1
a1119 1
					vm_offset_t pa = pmap_pte_pa(pte);
a1127 3
#ifdef PMAPSTATS
				protect_stats.changed++;
#endif
a1129 8
#ifdef PMAPSTATS
			else if (pmap_pte_v(pte)) {
				if (isro)
					protect_stats.alreadyro++;
				else
					protect_stats.alreadyrw++;
			}
#endif
d1152 4
a1155 2
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
d1159 1
a1159 1
 *	that the related pte can not be reclaimed.
d1161 2
a1162 2
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
d1167 3
a1169 3
	register pmap_t pmap;
	vm_offset_t va;
	register vm_offset_t pa;
d1174 3
a1176 3
	register pt_entry_t *pte;
	register int npte;
	vm_offset_t opa;
d1180 4
a1183 5
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
		       pmap, va, pa, prot, wired);
#endif
d1187 7
a1193 5
#ifdef PMAPSTATS
	if (pmap == pmap_kernel())
		enter_stats.kernel++;
	else
		enter_stats.user++;
d1195 1
d1199 5
a1203 1
	if (pmap->pm_ptab == NULL) {
d1205 2
a1206 2
			kmem_alloc_wait(pt_map, MAC_MAX_PTSIZE);
	}
d1217 2
a1218 4
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: pte %p, *pte %x\n", pte, *pte);
#endif
a1223 3
#ifdef PMAPSTATS
		enter_stats.pwchange++;
#endif
d1231 2
a1232 4
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("enter: wiring change -> %x\n", wired);
#endif
a1236 4
#ifdef PMAPSTATS
			if (pmap_pte_prot(pte) == pte_prot(pmap, prot))
				enter_stats.wchange++;
#endif
a1237 6
#ifdef PMAPSTATS
		else if (pmap_pte_prot(pte) != pte_prot(pmap, prot))
			enter_stats.pchange++;
		else
			enter_stats.nochange++;
#endif
d1252 2
a1253 4
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: removing old mapping %lx\n", va);
#endif
a1254 3
#ifdef PMAPSTATS
		enter_stats.mchange++;
#endif
d1263 7
a1269 2
		(void) vm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
				       round_page((vaddr_t)(pte+1)), FALSE);
d1276 2
a1277 2
	if (pmap_valid_page (pa)) {
		register struct pv_entry *pv, *npv;
a1279 3
#ifdef PMAPSTATS
		enter_stats.managed++;
#endif
d1282 3
a1284 5
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: pv at %p: %lx/%p/%p\n",
			       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
a1288 3
#ifdef PMAPSTATS
			enter_stats.firstpv++;
#endif
d1314 9
a1322 3
#ifdef PMAPSTATS
			if (!npv->pv_next)
				enter_stats.secondpv++;
d1324 4
a1327 1
		}
a1335 3
#ifdef PMAPSTATS
		enter_stats.unmanaged++;
#endif
d1368 3
a1370 4
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: new pte value %x\n", npte);
#endif
d1392 5
a1396 5
 *	Routine:	pmap_change_wiring
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
d1400 2
a1401 2
	register pmap_t	pmap;
	vm_offset_t	va;
d1404 4
a1407 1
	register pt_entry_t *pte;
a1408 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_change_wiring(%p, %lx, %x)\n", pmap, va, wired);
#endif
d1434 1
a1434 1
	 * If wiring actually changes (always?) set the wire bit and
d1436 1
a1436 1
	 * characteristic, so there is no need to invalidate the TLB.
d1448 4
a1451 4
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
d1453 1
a1453 2

vm_offset_t
d1455 2
a1456 2
	register pmap_t	pmap;
	vm_offset_t va;
d1458 4
a1461 1
	register vm_offset_t pa;
a1462 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%p, %lx) -> ", pmap, va);
#endif
d1468 4
a1471 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("%lx\n", pa);
#endif
	return(pa);
d1475 3
a1477 1
 *	Copy the range specified by src_addr/len
d1483 2
a1484 1
void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
d1487 3
a1489 3
	vm_offset_t	dst_addr;
	vm_size_t	len;
	vm_offset_t	src_addr;
d1491 4
a1494 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
#endif
d1498 2
d1501 14
a1514 2
 *	incorrect entries NOW.  [This update includes
 *	forcing updates of any address map caching.]
d1516 6
a1521 2
 *	Generally used to insure that a thread about
 *	to run will see a semantically correct world.
d1523 3
a1525 1
void pmap_update()
d1527 29
a1555 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_update()\n");
a1556 1
	TBIA();
d1560 7
a1566 9
 *	Routine:	pmap_collect
 *	Function:
 *		Garbage collects the physical map system for
 *		pages which are no longer used.
 *		Success need not be guaranteed -- that is, there
 *		may well be pages which are not referenced, but
 *		others may be collected.
 *	Usage:
 *		Called by the pageout daemon when pages are scarce.
d1569 1
a1569 1
pmap_collect(pmap)
d1571 1
d1573 92
a1664 1
	return; /* XXX -- do we need to do anything here? */
d1668 9
a1676 4
 *	pmap_zero_page zeros the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bzero to clear its contents, one machine dependent page
 *	at a time.
d1680 1
a1680 1
	vm_offset_t phys;
d1682 23
a1704 2
	register vm_offset_t kva;
	extern caddr_t CADDR1;
d1707 2
a1708 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%lx)\n", phys);
d1710 2
a1711 6
	kva = (vm_offset_t) CADDR1;
	pmap_enter(pmap_kernel(), kva, phys, VM_PROT_READ|VM_PROT_WRITE, TRUE,
		   0);
	zeropage((caddr_t)kva);
	pmap_remove_mapping(pmap_kernel(), kva, PT_ENTRY_NULL,
			    PRM_TFLUSH|PRM_CFLUSH);
d1715 9
a1723 4
 *	pmap_copy_page copies the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bcopy to copy the page, one machine dependent page at a
 *	time.
d1727 1
a1727 1
	vm_offset_t src, dst;
d1729 28
a1756 2
	register vm_offset_t skva, dkva;
	extern caddr_t CADDR1, CADDR2;
d1759 5
a1763 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%lx, %lx)\n", src, dst);
d1765 2
a1766 8
	skva = (vm_offset_t) CADDR1;
	dkva = (vm_offset_t) CADDR2;
	pmap_enter(pmap_kernel(), skva, src, VM_PROT_READ, TRUE, 0);
	pmap_enter(pmap_kernel(), dkva, dst, VM_PROT_READ|VM_PROT_WRITE, TRUE,
		   0);
	copypage((caddr_t)skva, (caddr_t)dkva);
	/* CADDR1 and CADDR2 are virtually contiguous */
	pmap_remove(pmap_kernel(), skva, skva + (2 * NBPG));
a1768 1

d1770 4
a1773 4
 *	Routine:	pmap_pageable
 *	Function:
 *		Make the specified pages (by pmap, offset)
 *		pageable (or not) as requested.
d1775 2
a1776 3
 *		A page which is not pageable may not take
 *		a fault; therefore, its page table entry
 *		must remain valid for the duration.
d1778 2
a1779 3
 *		This routine is merely advisory; pmap_enter
 *		will specify that these pages are to be wired
 *		down (or not) as appropriate.
d1784 1
a1784 1
	vm_offset_t	sva, eva;
d1787 5
a1791 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pageable(%p, %lx, %lx, %x)\n",
		       pmap, sva, eva, pageable);
#endif
d1801 2
a1802 2
		register struct pv_entry *pv;
		register vm_offset_t pa;
d1812 1
a1812 1
		if (!pmap_valid_page (pa))
d1825 1
a1825 1
		 * Mark it unmodified to avoid pageout
d1827 7
a1833 1
		pmap_changebit(pa, PG_M, FALSE);
a1834 5
		if ((PHYS_TO_VM_PAGE(pa)->flags & PG_CLEAN) == 0) {
			printf("pa %lx: flags=%x: not clean\n",
				pa, PHYS_TO_VM_PAGE(pa)->flags);
			PHYS_TO_VM_PAGE(pa)->flags |= PG_CLEAN;
		}
d1836 1
a1836 1
			printf("pmap_pageable: PT page %lx(%x) unmodified\n",
a1837 2
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("pageable", sva);
d1843 2
a1846 1

d1849 1
a1849 1
	vm_offset_t	pa;
d1851 4
a1854 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%lx)\n", pa);
#endif
	pmap_changebit(pa, PG_M, FALSE);
d1858 1
a1858 1
 *	pmap_clear_reference:
d1862 4
d1867 3
a1869 8
void pmap_clear_reference(pa)
	vm_offset_t	pa;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%lx)\n", pa);
#endif
	pmap_changebit(pa, PG_U, FALSE);
d1873 1
a1873 1
 *	pmap_is_referenced:
a1877 1

d1880 1
a1880 1
	vm_offset_t	pa;
d1893 1
a1893 1
 *	pmap_is_modified:
a1897 1

d1900 1
a1900 1
	vm_offset_t	pa;
d1912 10
a1921 1
vm_offset_t
d1933 10
a1942 4
 * Invalidate a single page denoted by pmap/va.
 * If (pte != NULL), it is the already computed PTE for the page.
 * If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 * If (flags & PRM_CFLUSH), we must flush/invalidate any cache information.
d1947 3
a1949 3
	register pmap_t pmap;
	register vm_offset_t va;
	register pt_entry_t *pte;
d1952 2
a1953 2
	register vm_offset_t pa;
	register struct pv_entry *pv, *npv;
d1959 1
d1961 3
a1963 4
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_mapping(%p, %lx, %p, %x)\n",
		       pmap, va, pte, flags);
#endif
d1973 1
a1977 3
#ifdef PMAPSTATS
	remove_stats.removes++;
#endif
d1988 1
a1988 4
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("remove: invalidating pte at %p\n", pte);
#endif
d2001 7
a2007 2
		(void) vm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
				       round_page((vaddr_t)(pte+1)), TRUE);
d2016 1
a2016 1
	if (!pmap_valid_page (pa))
a2040 3
#ifdef PMAPSTATS
		remove_stats.pvfirst++;
#endif
a2042 3
#ifdef PMAPSTATS
			remove_stats.pvsearch++;
#endif
d2057 1
d2063 3
a2065 8
#ifdef PMAPSTATS
		remove_stats.ptinvalid++;
#endif
#ifdef DEBUG
		if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
			printf("remove: ste was %x@@%p pte was %x@@%p\n",
			       *ste, ste, opte, pmap_pte(pmap, va));
#endif
d2084 3
a2087 3
			if (pmapdebug & (PDB_REMOVE|PDB_SEGTAB))
				printf("remove: stab %p, refcnt %d\n",
				       ptpmap->pm_stab, ptpmap->pm_sref - 1);
d2093 9
a2101 4
#ifdef DEBUG
				if (pmapdebug&(PDB_REMOVE|PDB_SEGTAB))
					printf("remove: free stab %p\n",
					       ptpmap->pm_stab);
a2102 3
				kmem_free_wakeup(st_map,
						 (vm_offset_t)ptpmap->pm_stab,
						 MAC_STSIZE);
a2108 1
				ptpmap->pm_stchanged = TRUE;
d2114 2
a2115 4
				if (curproc != NULL &&
				    ptpmap == curproc->p_vmspace->vm_map.pmap)
					PMAP_ACTIVATE(ptpmap,
					    &curproc->p_addr->u_pcb, 1);
d2138 1
a2138 1
	pmap_attributes[pmap_page_index(pa)] |= bits;
d2142 5
d2150 1
a2150 1
	register vm_offset_t pa;
d2153 2
a2154 2
	register struct pv_entry *pv;
	register pt_entry_t *pte;
d2157 2
a2158 2
	if (!pmap_valid_page (pa))
		return FALSE;
d2165 1
a2165 1
	if (pmap_attributes[pmap_page_index(pa)] & bit) {
d2169 1
d2171 2
a2172 2
	 * Not found, check current mappings returning
	 * immediately if found.
d2178 1
d2188 6
d2196 3
a2198 4
pmap_changebit(pa, bit, setem)
	register vm_offset_t pa;
	int bit;
	boolean_t setem;
d2200 3
a2202 3
	register struct pv_entry *pv;
	register pt_entry_t *pte, npte;
	vm_offset_t va;
a2206 3
#ifdef PMAPSTATS
	struct chgstats *chgp;
#endif
d2208 4
a2211 6
#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(%lx, %x, %s)\n",
		       pa, bit, setem ? "set" : "clear");
#endif
	if (!pmap_valid_page (pa))
a2213 7
#ifdef PMAPSTATS
	chgp = &changebit_stats[(bit>>2)-1];
	if (setem)
		chgp->setcalls++;
	else
		chgp->clrcalls++;
#endif
d2216 1
d2220 2
a2221 2
	if (!setem)
		pmap_attributes[pmap_page_index(pa)] &= ~bit;
d2224 1
d2239 6
a2244 2
			if (bit == PG_RO) {
				extern vm_offset_t pager_sva, pager_eva;
d2248 1
d2252 1
a2252 4
			if (setem)
				npte = *pte | bit;
			else
				npte = *pte & ~bit;
d2260 4
a2263 3
				if (firstpage && mmutype == MMU_68040 &&
				    ((bit == PG_RO && setem) ||
				     (bit & PG_CMASK))) {
a2271 6
#ifdef PMAPSTATS
				if (setem)
					chgp->sethits++;
				else
					chgp->clrhits++;
#endif
a2272 8
#ifdef PMAPSTATS
			else {
					if (setem)
						chgp->setmiss++;
					else
						chgp->clrmiss++;
			}
#endif
d2278 5
d2286 2
a2287 2
	register pmap_t pmap;
	register vm_offset_t va;
d2289 2
a2290 2
	register vm_offset_t ptpa;
	register struct pv_entry *pv;
d2294 3
a2296 7
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE))
		printf("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va);
#endif
#ifdef PMAPSTATS
	enter_stats.ptpneeded++;
#endif
d2305 1
d2307 4
a2310 1
				kmem_alloc(st_map, MAC_STSIZE);
d2312 1
a2312 1
			pmap_extract(pmap_kernel(), (vm_offset_t)pmap->pm_stab);
d2318 1
a2318 1
			pmap_changebit((vm_offset_t)pmap->pm_stpa, PG_CCB, 0);
a2321 1
		pmap->pm_stchanged = TRUE;
d2326 6
a2331 7
		if (pmap == curproc->p_vmspace->vm_map.pmap)
			PMAP_ACTIVATE(pmap, &curproc->p_addr->u_pcb, 1);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: pmap %p stab %p(%p)\n",
			       pmap, pmap->pm_stab, pmap->pm_stpa);
#endif
d2352 3
a2354 4
#ifdef DEBUG
			if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
				printf("enter: alloc ste2 %d(%p)\n", ix, addr);
#endif
d2366 3
a2368 5
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: ste2 %p (%p)\n",
				pmap_ste2(pmap, va), ste);
#endif
d2371 1
a2371 1
	va = trunc_page((vm_offset_t)pmap_pte(pmap, va));
d2379 1
a2379 1
		register struct kpt_page *kpt;
d2387 2
a2388 4
#ifdef DEBUG
			if (pmapdebug & PDB_COLLECT)
				printf("enter: no KPT pages, collecting...\n");
#endif
a2392 4
#ifdef PMAPSTATS
		if (++kpt_stats.kptinuse > kpt_stats.kptmaxuse)
			kpt_stats.kptmaxuse = kpt_stats.kptinuse;
#endif
d2399 1
a2399 1
			   VM_PROT_DEFAULT);
d2420 10
a2429 4
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
			printf("enter: about to fault UPT pg at %lx\n", va);
#endif
d2432 1
a2432 1
			printf("vm_fault(pt_map, 0x%lx, RW, 0) -> %d\n",va,s);
d2435 1
d2443 1
d2447 1
d2462 2
a2463 2
				pmap == pmap_kernel() ? "Kernel" : "User",
				va, ptpa, pte, *pte);
d2465 1
a2465 1
		pmap_changebit(ptpa, PG_CCB, 0);
d2480 1
a2480 1
		} while ((pv = pv->pv_next) != NULL);
d2488 3
a2490 4
#ifdef DEBUG
	if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
		printf("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste);
#endif
d2512 3
a2514 5
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: stab %p refcnt %d\n",
			       pmap->pm_stab, pmap->pm_sref);
#endif
d2530 5
d2538 1
a2538 1
	vm_offset_t pa;
d2540 1
a2540 1
	register struct pv_entry *pv;
d2550 7
d2561 1
a2561 1
	vm_offset_t va;
d2564 2
a2565 2
	register int count;
	register pt_entry_t *pte;
d2572 6
d2582 1
d2591 1
a2591 89
#endif

/*
 * LAK: These functions are from NetBSD/i386 and are used for
 *  the non-contiguous memory machines, such as the IIci, IIsi, and IIvx.
 *  See the functions in sys/vm that #ifdef MACHINE_NONCONTIG.
 */

/*
 * pmap_free_pages()
 *
 *   Returns the number of free physical pages left.
 */

unsigned int
pmap_free_pages()
{
	/* printf ("pmap_free_pages(): returning %d\n", avail_remaining); */
	return avail_remaining;
}

/*
 * pmap_next_page()
 *
 *   Stores in *addrp the next available page, skipping the hole between
 *   bank A and bank B.
 */

int
pmap_next_page(addrp)
	vm_offset_t *addrp;
{
	if (avail_next == high[avail_range]) {
		avail_range++;
		if (avail_range >= numranges) {
			/* printf ("pmap_next_page(): returning FALSE\n"); */
			return FALSE;
		}
		avail_next = low[avail_range];
	}

	*addrp = avail_next;
	/* printf ("pmap_next_page(): returning 0x%x\n", avail_next); */
	avail_next += NBPG;
	avail_remaining--;
	return TRUE;
}

/*
 * pmap_page_index()
 *
 *   Given a physical address, return the page number that it is in
 *   the block of free memory.
 */

int
pmap_page_index(pa)
	vm_offset_t pa;
{
	/*
	 * XXX LAK: This routine is called quite a bit.  We should go
	 *  back and try to optimize it a bit.
	 */

	int	i, index;

	index = 0;
	for (i = 0; i < numranges; i++) {
		if (pa >= low[i] && pa < high[i]) {
			index += m68k_btop (pa - low[i]);
			/* printf ("pmap_page_index(0x%x): returning %d\n", */
				/* (int)pa, index); */
			return index;
		}
		index += m68k_btop (high[i] - low[i]);
	}

	return -1;
}

void
pmap_virtual_space(startp, endp)
	vm_offset_t *startp, *endp;
{
	/* printf ("pmap_virtual_space(): returning 0x%x and 0x%x\n", */
		/* virtual_avail, virtual_end); */
	*startp = virtual_avail;
	*endp = virtual_end;
}
@


1.16
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 1999/07/18 18:00:05 deraadt Exp $	*/
d542 1
a542 1
	pvp = (struct pv_page *) trunc_page(pv);
d591 1
a591 1
			pvp = (struct pv_page *) trunc_page(pv);
d1175 2
a1176 2
		(void) vm_map_pageable(pt_map, trunc_page(pte),
				       round_page(pte+1), FALSE);
d1297 1
a1297 1
		pmap_check_wiring("enter", trunc_page(pmap_pte(pmap, va)));
d1725 2
a1726 2
		(void) vm_map_pageable(pt_map, trunc_page(pte),
				       round_page(pte+1), TRUE);
d1729 1
a1729 1
			pmap_check_wiring("remove", trunc_page(pte));
d1818 1
a1818 1
			    ptpmap->pm_stab != (st_entry_t *)trunc_page(ste))
@


1.16.4.1
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.c,v 1.55 1999/04/22 04:24:53 chs Exp $	*/
d4 1
a4 1
/*
d63 1
a63 1
 *		(*pte & PG_PROT)[4] != pte->pg_prot[1]
a101 1
#include <sys/pool.h>
d109 1
a109 1
#include <uvm/uvm.h>
d111 48
a158 1
#include <machine/cpu.h>
d161 2
a178 5
int debugmap = 0;
int pmapdebug = PDB_PARANOIA;

#define	PMAP_DPRINTF(l, x)	if (pmapdebug & (l)) printf x

d183 3
a185 3
#else /* ! DEBUG */
#define	PMAP_DPRINTF(l, x)	/* nothing */
#endif /* DEBUG */
d191 2
a192 2
#define	pmap_ste1(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
d197 2
a198 2
#define	pmap_ste(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) \
d201 3
a203 3
	(mmutype == MMU_68040 \
	 ? ((*pmap_ste1(m, v) & SG_V) && \
	    (*pmap_ste2(m, v) & SG_V)) \
d206 2
a207 2
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
#define pmap_ste_v(m, v) (*pmap_ste(m, v) & SG_V)
d210 1
a210 1
#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))
d219 1
a219 1
#define pmap_pte_set_w(pte, v) \
d226 4
d242 2
a243 2
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
d260 1
a260 1
vsize_t		Sysptsize = VM_KERNEL_PT_PAGES;
a263 1
struct vm_map	st_map_store, pt_map_store;
d265 10
a274 6
paddr_t		avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vsize_t		mem_size;	/* memory size in bytes */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
int		page_cnt;	/* number of pages managed by VM system */
d282 4
a285 3
/* The following four variables are defined in pmap_bootstrap.c */
extern int		vidlen;
#define VIDMAPSIZE	btoc(vidlen)
a290 30
extern caddr_t	CADDR1, CADDR2;

pt_entry_t	*caddr1_pte;	/* PTE for CADDR1 */
pt_entry_t	*caddr2_pte;	/* PTE for CADDR2 */

struct pool	pmap_pmap_pool;	/* memory pool for pmap structures */

struct pv_entry *pmap_alloc_pv __P((void));
void	pmap_free_pv __P((struct pv_entry *));
void	pmap_collect_pv __P((void));

#define	PAGE_IS_MANAGED(pa)	(pmap_initialized &&			\
				 vm_physseg_find(atop((pa)), NULL) != -1)

#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})

#define	pa_to_attribute(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
})

d294 4
a297 8
void	pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));
boolean_t pmap_testbit	__P((paddr_t, int));
void	pmap_changebit	__P((paddr_t, int, int));
void	pmap_enter_ptpage	__P((pmap_t, vaddr_t));
void	pmap_collect1	__P((pmap_t, paddr_t, vaddr_t));
void	pmap_pinit __P((pmap_t));
void	pmap_release __P((pmap_t));

d299 2
a300 2
void	pmap_pvdump          __P((paddr_t));
void	pmap_check_wiring    __P((char *, vaddr_t));
d308 32
a339 17
 * pmap_virtual_space:		[ INTERFACE ]
 *
 *	Report the range of available kernel virtual address
 *	space to the VM system during bootstrap.
 *
 *	This is only an interface function if we do not use
 *	pmap_steal_memory()!
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_virtual_space(vstartp, vendp)
	vaddr_t *vstartp, *vendp;
{

	*vstartp = virtual_avail;
	*vendp = virtual_end;
d343 3
a345 6
 * pmap_init:			[ INTERFACE ]
 *
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
 *
 *	Note: no locking is necessary in this function.
d350 3
a352 16
	vaddr_t addr, addr2;
	vsize_t s;
	struct pv_entry *pv;
	char *attr;
	int rv;
	int npages;
	int bank;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));

	/*
	 * Before we do anything else, initialize the PTE pointers
	 * used by pmap_zero_page() and pmap_copy_page().
	 */
	caddr1_pte = pmap_pte(pmap_kernel(), CADDR1);
	caddr2_pte = pmap_pte(pmap_kernel(), CADDR2);
d354 4
d360 1
a360 1
	 * unavailable regions which we have mapped in pmap_bootstrap().
d362 25
a386 21
	addr = (vaddr_t)IOBase;
	if (uvm_map(kernel_map, &addr,
		    m68k_ptob(IIOMAPSIZE + ROMMAPSIZE + VIDMAPSIZE),
		    NULL, UVM_UNKNOWN_OFFSET,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)) != KERN_SUCCESS)
		goto bogons;
	addr = (vaddr_t)Sysmap;
	if (uvm_map(kernel_map, &addr, MAC_MAX_PTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)) != KERN_SUCCESS) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
 bogons:
		panic("pmap_init: bogons in the VM system!\n");
d388 1
a388 7

	PMAP_DPRINTF(PDB_INIT,
	    ("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
	    Sysseg, Sysmap, Sysptmap));
	PMAP_DPRINTF(PDB_INIT,
	    ("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
	    avail_start, avail_end, virtual_avail, virtual_end));
d394 5
a398 5
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++)
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
	s = MAC_STSIZE;					/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */
d400 3
a402 6
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: can't allocate data structures");

	Segtabzero = (st_entry_t *)addr;
	pmap_extract(pmap_kernel(), addr, (paddr_t *)&Segtabzeropa);
d404 9
a412 24

	pv_table = (struct pv_entry *)addr;
	addr += page_cnt * sizeof(struct pv_entry);

	pmap_attributes = (char *)addr;

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
	    "tbl %p atr %p\n",
	    s, page_cnt, Segtabzero, Segtabzeropa,
	    pv_table, pmap_attributes));

	/*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}
d426 2
a427 4
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv != KERN_SUCCESS || (addr + s) >= (vaddr_t)Sysmap)
d429 1
a429 3
	rv = uvm_unmap(kernel_map, addr, addr + s);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: uvm_unmap failed");
d435 1
a435 3
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
d439 1
a439 1
	kpt_free_list = (struct kpt_page *)0;
d445 1
a445 1
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
d447 8
a454 3

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: KPT: %ld pages from %lx to %lx\n",
	    atop(s), addr, addr + s));
d457 1
a457 1
	 * Allocate the segment table map and the page table map.
d460 1
a460 2
	st_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, TRUE,
	    FALSE, &st_map_store);
d462 4
d467 17
a483 13
	if ((MAC_PTMAXSIZE / MAC_MAX_PTSIZE) < maxproc) {
		s = MAC_PTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(2).
		 */
		maxproc = (MAC_PTMAXSIZE / MAC_MAX_PTSIZE);
	} else
		s = (maxproc * MAC_MAX_PTSIZE);
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, TRUE,
	    TRUE, &pt_map_store);
a493 6
	 * Initialize the pmap pools.
	 */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);

	/*
d499 4
a502 6
/*
 * pmap_alloc_pv:
 *
 *	Allocate a pv_entry.
 */
struct pv_entry *
d505 3
a507 3
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;
d510 1
a510 1
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG);
d512 1
a512 1
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
d536 1
a536 6
/*
 * pmap_free_pv:
 *
 *	Free a pv_entry.
 */
void
d540 1
a540 1
	struct pv_page *pvp;
d542 1
a542 1
	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
d554 1
a554 1
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
d559 2
a560 5
/*
 * pmap_collect_pv:
 *
 *	Perform compaction on the PV list, called via pmap_collect().
 */
d577 2
a578 3
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp,
			    pvp_pgi.pgi_list);
			pv_nfree -= NPVPPG;
d586 1
a586 1
	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
d591 1
a591 1
			pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
d595 1
a595 2
					TAILQ_REMOVE(&pv_page_freelist, pvp,
					    pvp_pgi.pgi_list);
d614 1
a614 1
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
a618 2
 * pmap_map:
 *
a623 2
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
d625 1
a625 1
vaddr_t
d627 2
a628 3
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
d631 4
a634 2
	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));
d641 1
a641 1
	return (va);
a644 2
 * pmap_create:			[ INTERFACE ]
 *
d647 8
a654 1
 *	Note: no locking is necessary in this function.
d658 1
a658 1
	vsize_t	size;
d660 1
a660 1
	pmap_t pmap;
d662 4
a665 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create(%lx)\n", size));
d671 1
a671 3
		return (NULL);

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
d673 6
d685 2
a686 5
 * pmap_pinit:
 *
 *	Initialize a preallocated and zeroed pmap structure.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_create()!
d690 1
a690 1
	struct pmap *pmap;
d693 4
a696 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_pinit(%p)\n", pmap));
d710 1
d716 3
a718 4
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
d722 1
a722 1
	pmap_t pmap;
d729 4
a732 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));
d739 1
a739 1
		pool_put(&pmap_pmap_pool, pmap);
d744 3
a746 5
 * pmap_release:
 *
 *	Relese the resources held by a pmap.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_destroy().
d750 1
a750 1
	struct pmap *pmap;
d753 4
a756 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_release(%p)\n", pmap));
d766 2
a767 2
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
		    MAC_MAX_PTSIZE);
d769 2
a770 2
		uvm_km_free_wakeup(st_map, (vaddr_t)pmap->pm_stab,
		    MAC_STSIZE);
a773 2
 * pmap_reference:		[ INTERFACE ]
 *
d784 4
a787 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_reference(%p)\n", pmap));
d794 3
a796 10
 /*
  * pmap_activate:		[ INTERFACE ]
  *
  *	Activate the pmap used by the specified process.  This includes
  *	reloading the MMU context if the current process, and marking
  *	the pmap in use by the processor.
  *
  *	Note: we may only use spin locks here, since we are called
  *	by a critical section in cpu_switch()!
  */
d798 3
a800 2
pmap_activate(p)
	struct proc *p;
a801 1
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d803 7
a809 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_SEGTAB,
	    ("pmap_activate(%p)\n", p));
d811 1
a811 1
	PMAP_ACTIVATE(pmap, p == curproc);
d814 2
a815 9
/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.
 */
d817 3
a819 2
pmap_deactivate(p)
	struct proc *p;
a820 2

	/* No action necessary in this pmap implementation. */
a823 2
 * pmap_remove:			[ INTERFACE ]
 *
d831 2
a832 2
	pmap_t pmap;
	vaddr_t sva, eva;
d834 2
a835 2
	vaddr_t nssva;
	pt_entry_t *pte;
d839 4
a842 2
	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva));
d847 3
d886 1
a886 1
 * pmap_page_protect:		[ INTERFACE ]
d888 1
a888 2
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
d892 1
a892 1
	paddr_t		pa;
d895 1
a895 1
	struct pv_entry *pv;
d903 1
a903 1
	if (PAGE_IS_MANAGED(pa) == 0)
d913 1
a913 1
		pmap_changebit(pa, PG_RO, ~0);
d922 1
a922 1
		pt_entry_t *pte;
d938 1
a938 1
				       "pmap_page_protect:", pa);
a939 2
			if (pv == NULL)
				break;
d946 2
a947 4
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protectoin on the specified range of this map
 *	as requested.
d951 3
a953 3
	pmap_t		pmap;
	vaddr_t		sva, eva;
	vm_prot_t	prot;
d955 2
a956 2
	vaddr_t nssva;
	pt_entry_t *pte;
d960 4
a963 3
	PMAP_DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_protect(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, prot));
d968 3
d1006 1
a1006 1
					paddr_t pa = pmap_pte_pa(pte);
d1015 3
d1020 8
d1050 2
a1051 4
 * pmap_enter:			[ INTERFACE ]
 *
 *	Insert the given physical page (pa) at
 *	the specified virtual address (va) in the
d1055 1
a1055 1
 *	that the related pte cannot be reclaimed.
d1057 2
a1058 2
 *	Note: This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  Thatis, this routine must actually
d1063 3
a1065 3
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
d1070 3
a1072 3
	pt_entry_t *pte;
	int npte;
	paddr_t opa;
d1076 5
a1080 4
	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, %lx, %lx, %x, %x)\n",
	    pmap, va, pa, prot, wired));

d1084 5
a1088 7
#ifdef DIAGNOSTIC
	/*
	 * pmap_enter() should never be used for CADDR1 and CADDR2.
	 */
	if (pmap == pmap_kernel() &&
	    (va == (vaddr_t)CADDR1 || va == (vaddr_t)CADDR2))
		panic("pmap_enter: used for CADDR1 or CADDR2");
a1089 1

d1093 1
a1093 1
	if (pmap->pm_ptab == NULL)
d1095 2
a1096 1
		    uvm_km_valloc_wait(pt_map, MAC_MAX_PTSIZE);
d1107 4
a1110 2

	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));
d1116 3
d1126 4
a1129 2
			PMAP_DPRINTF(PDB_ENTER,
			    ("enter: wiring change -> %x\n", wired));
d1134 4
d1139 6
d1159 4
a1162 2
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: removing old mapping %lx\n", va));
d1164 3
d1175 2
a1176 2
		(void)uvm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)(pte+1)), FALSE, FALSE);
d1183 2
a1184 2
	if (PAGE_IS_MANAGED(pa)) {
		struct pv_entry *pv, *npv;
d1187 3
d1192 5
a1196 3
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: pv at %p: %lx/%p/%p\n",
		    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
d1201 3
d1229 4
a1233 13

		/*
		 * Speed pmap_is_referenced() or pmap_is_modified() based
		 * on the hint provided in access_type.
		 */
#ifdef DIAGNOSTIC
		if (access_type & ~prot)
			panic("pmap_enter: access_type exceeds prot");
#endif
		if (access_type & VM_PROT_WRITE)
			*pa_to_attribute(pa) |= (PG_U|PG_M);
		else if (access_type & VM_PROT_ALL)
			*pa_to_attribute(pa) |= PG_U;
d1242 3
d1277 4
a1280 3

	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));

d1297 1
a1297 1
		pmap_check_wiring("enter", trunc_page((vaddr_t)pmap_pte(pmap, va)));
d1302 5
a1306 5
 * pmap_unwire:		[ INTERFACE ]
 *
 *	Change the wiring attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
d1309 4
a1312 3
pmap_unwire(pmap, va)
	pmap_t		pmap;
	vaddr_t		va;
d1314 1
a1314 3
	pt_entry_t *pte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %lx)\n", pmap, va));
d1316 4
d1332 1
a1332 1
			printf("pmap_unwire: invalid STE for %lx\n", va);
d1341 1
a1341 1
			printf("pmap_unwire: invalid PTE for %lx\n", va);
d1345 1
a1345 1
	 * If wiring actually changed (always?) set the wire bit and
d1347 1
a1347 1
	 * characteristic so there is no need to invalidate the TLB.
d1349 6
a1354 3
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, 0);
		pmap->pm_stats.wired_count--;
d1359 4
a1362 4
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
d1364 5
a1368 5
boolean_t
pmap_extract(pmap, va, pap)
	pmap_t	pmap;
	vaddr_t	va;
	paddr_t *pap;
d1370 1
a1370 4
	paddr_t pa;

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_extract(%p, %lx) -> ", pmap, va));
d1372 5
d1379 7
a1385 9
	else
		return (FALSE);

	pa = (pa & PG_FRAME) | (va & ~PG_FRAME);

	PMAP_DPRINTF(PDB_FOLLOW, ("%lx\n", pa));

	*pap = pa;
	return (TRUE);
d1389 1
a1389 3
 * pmap_copy:			[ INTERFACE ]
 *
 *	Copy the mapping range specified by src_addr/len
d1395 1
a1395 2
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
d1398 3
a1400 3
	vaddr_t		dst_addr;
	vsize_t		len;
	vaddr_t		src_addr;
d1402 5
a1406 4

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
	    dst_pmap, src_pmap, dst_addr, len, src_addr));
d1410 3
a1412 1
 * pmap_update:
d1414 2
a1415 3
 *	Require that all active physical maps contain no
 *	incorrect entires NOW, by processing any deferred
 *	pmap operations.
d1417 1
a1417 2
void
pmap_update()
d1419 5
a1423 4

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_update()\n"));

	TBIA();		/* XXX should not be here. */
d1427 9
a1435 8
 * pmap_collect:		[ INTERFACE ]
 *
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
d1441 1
a1441 30

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_collect(%p)\n", pmap));

	if (pmap == pmap_kernel()) {
		int bank, s;

		/*
		 * XXX This is very bogus.  We should handle kernel PT
		 * XXX pages much differently.
		 */

		s = splimp();
		for (bank = 0; bank < vm_nphysseg; bank++)
			pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			    ptoa(vm_physmem[bank].end));
		splx(s);
	} else {
		/*
		 * This process is about to be swapped out; free all of
		 * the PT pages by removing the physical mappings for its
		 * entire address space.  Note: pmap_remove() performs
		 * all necessary locking.
		 */
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
	}

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
#endif
d1445 4
a1448 7
 * pmap_collect1:
 *
 *	Garbage-collect KPT pages.  Helper for the above (bogus)
 *	pmap_collect().
 *
 *	Note: THIS SHOULD GO AWAY, AND BE REPLACED WITH A BETTER
 *	WAY OF HANDLING PT PAGES!
d1451 2
a1452 3
pmap_collect1(pmap, startpa, endpa)
	pmap_t		pmap;
	paddr_t		startpa, endpa;
d1454 2
a1455 8
	paddr_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	paddr_t kpa;
#ifdef DEBUG
	st_entry_t *ste;
	int opmapdebug = 0 /* XXX initialize to quiet gcc -Wall */;
#endif
a1456 16
	for (pa = startpa; pa < endpa; pa += NBPG) {
		struct kpt_page *kpt, **pkpt;

		/*
		 * Locate physical pages which are being used as kernel
		 * page table pages.
		 */
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptste && pv->pv_ptpmap == pmap_kernel())
				break;
		} while ((pv = pv->pv_next));
		if (pv == NULL)
			continue;
d1458 2
a1459 49
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + MAC_MAX_PTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (pt_entry_t *)(pv->pv_va + NBPG);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf("collect: freeing KPT page at %lx (ste %x@@%p)\n",
			       pv->pv_va, *pv->pv_ptste, pv->pv_ptste);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
		}

		ste = pv->pv_ptste;
#endif
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove_entry to take care of invalidating
		 * ST and Sysptmap entries.
		 */
		pmap_extract(pmap, pv->pv_va, &kpa);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				    PRM_TFLUSH|PRM_CFLUSH);
		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != (struct kpt_page *)0;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef DEBUG
		if (kpt == (struct kpt_page *)0)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			       kpt->kpt_va, kpa);
d1461 6
a1466 16
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste != SG_NV)
			printf("collect: kernel STE at %p still valid (%x)\n",
			       ste, *ste);
		ste = &Sysptmap[ste - pmap_ste(pmap_kernel(), 0)];
		if (*ste != SG_NV)
			printf("collect: kernel PTmap at %p still valid (%x)\n",
			       ste, *ste);
#endif
	}
d1470 4
a1473 9
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and using bzero to clear its contents, one
 *	machine dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
d1476 2
a1477 2
pmap_zero_page(phys)
	paddr_t phys;
d1479 2
a1480 23
	int s, npte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%lx)\n", phys));

	npte = phys | PG_V;

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		/*
		 * Set copyback caching on the page; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte |= PG_CCB;
	}
#endif

	s = splimp();

	*caddr1_pte = npte;
	TBIS((vaddr_t)CADDR1);

	zeropage(CADDR1);
d1483 2
a1484 2
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);
d1486 9
a1495 2
	splx(s);
}
d1498 4
a1501 1
 * pmap_copy_page:		[ INTERFACE ]
d1503 7
a1509 7
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using bcopy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 *	      (Actually, we go to splimp(), and since we don't
 *	      support multiple processors, this is sufficient.)
d1512 4
a1515 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d1517 16
a1532 1
	int s, npte1, npte2;
d1534 20
a1553 7
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_copy_page(%lx, %lx)\n", src, dst));

	npte1 = src | PG_RO | PG_V;
	npte2 = dst | PG_V;

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
d1555 1
a1555 3
		 * Set copyback caching on the pages; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
d1557 1
a1557 15
		npte1 |= PG_CCB;
		npte2 |= PG_CCB;
	}
#endif

	s = splimp();

	*caddr1_pte = npte1;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = npte2;
	TBIS((vaddr_t)CADDR2);

	copypage(CADDR1, CADDR2);

d1559 10
a1568 5
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = PG_NV;
	TBIS((vaddr_t)CADDR2);
d1570 1
a1570 2

	splx(s);
a1573 2
 * pmap_clear_modify:		[ INTERFACE ]
 *
d1576 1
d1579 1
a1579 1
	paddr_t	pa;
d1581 5
a1585 4

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_M);
d1589 1
a1589 1
 * pmap_clear_reference:	[ INTERFACE ]
d1593 3
a1595 3
void
pmap_clear_reference(pa)
	paddr_t	pa;
d1597 5
a1601 4

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));

	pmap_changebit(pa, 0, ~PG_U);
d1605 1
a1605 1
 * pmap_is_referenced:		[ INTERFACE ]
d1610 1
d1613 1
a1613 1
	paddr_t	pa;
d1626 1
a1626 1
 * pmap_is_modified:		[ INTERFACE ]
d1631 1
d1634 1
a1634 1
	paddr_t	pa;
d1646 1
a1646 10
/*
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
d1658 4
a1661 10
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
d1666 3
a1668 3
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
d1671 2
a1672 2
	paddr_t pa;
	struct pv_entry *pv, *npv;
d1678 4
a1683 4
	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    pmap, va, pte, flags));

a1691 1

d1696 3
d1709 4
a1712 1
	PMAP_DPRINTF(PDB_REMOVE, ("remove: invalidating pte at %p\n", pte));
d1725 2
a1726 2
		(void)uvm_map_pageable(pt_map, trunc_page((vaddr_t)pte),
		    round_page((vaddr_t)(pte+1)), TRUE, FALSE);
d1729 1
a1729 1
			pmap_check_wiring("remove", (vaddr_t)trunc_page(pte));
d1735 1
a1735 1
	if (PAGE_IS_MANAGED(pa) == 0)
d1760 3
d1765 3
a1781 1

d1787 8
a1794 3
		PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
		    ("remove: ste was %x@@%p pte was %x@@%p\n",
		    *ste, ste, opte, pmap_pte(pmap, va)));
a1812 3
			PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
			    ("remove: stab %p, refcnt %d\n",
			    ptpmap->pm_stab, ptpmap->pm_sref - 1));
d1814 3
d1818 1
a1818 1
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
d1822 8
a1829 5
				PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
				    ("remove: free stab %p\n",
				    ptpmap->pm_stab));
				uvm_km_free_wakeup(st_map,
				    (vaddr_t)ptpmap->pm_stab, MAC_STSIZE);
d1836 1
d1842 4
a1845 2
				if (active_user_pmap(ptpmap))
					PMAP_ACTIVATE(ptpmap, 1);
d1868 1
a1868 1
	*pa_to_attribute(pa) |= bits;
a1871 5
/*
 * pmap_testbit:
 *
 *	Test the modified/referenced bits of a physical page.
 */
d1875 1
a1875 1
	paddr_t pa;
d1878 2
a1879 2
	struct pv_entry *pv;
	pt_entry_t *pte;
d1882 2
a1883 2
	if (PAGE_IS_MANAGED(pa) == 0)
		return (FALSE);
d1890 1
a1890 1
	if (*pa_to_attribute(pa) & bit) {
a1893 1

d1895 2
a1896 2
	 * Not found.  Check current mappings, returning immediately if
	 * found.  Cache a hit to speed future lookups.
a1901 1
				*pa_to_attribute(pa) |= bit;
a1910 6
/*
 * pmap_changebit:
 *
 *	Change the modified/referenced bits, or other PTE bits,
 *	for a physical page.
 */
d1913 4
a1916 3
pmap_changebit(pa, set, mask)
	paddr_t pa;
	int set, mask;
d1918 3
a1920 3
	struct pv_entry *pv;
	pt_entry_t *pte, npte;
	vaddr_t va;
d1925 3
d1929 6
a1934 4
	PMAP_DPRINTF(PDB_BITS,
	    ("pmap_changebit(%lx, %x, %x)\n", pa, set, mask));

	if (PAGE_IS_MANAGED(pa) == 0)
d1937 7
a1945 1

d1949 2
a1950 2
	*pa_to_attribute(pa) &= mask;

a1952 1
	 * If setting RO do we need to clear the VAC?
d1967 4
a1970 2
			if (set == PG_RO) {
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
d1975 4
a1978 1
			npte = (*pte | set) & mask;
d1986 3
a1988 4
				if (firstpage && (mmutype == MMU_68040) &&
				    ((set == PG_RO) ||
				     (set & PG_CMASK) ||
				     (mask & PG_CMASK) == 0)) {
d1997 6
d2004 8
a2016 5
/*
 * pmap_enter_ptpage:
 *
 *	Allocate and map a PT page for the specified pmap/va pair.
 */
d2020 2
a2021 2
	pmap_t pmap;
	vaddr_t va;
d2023 2
a2024 2
	paddr_t ptpa;
	struct pv_entry *pv;
d2028 7
a2034 3
	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE,
	    ("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va));

d2044 3
a2046 3
		    uvm_km_zalloc(st_map, MAC_STSIZE);
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab,
			(paddr_t *)&pmap->pm_stpa);
d2052 1
a2052 1
			pmap_changebit((paddr_t)pmap->pm_stpa, 0, ~PG_CCB);
d2056 1
d2061 7
a2067 6
		if (active_user_pmap(pmap))
			PMAP_ACTIVATE(pmap, 1);

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: pmap %p stab %p(%p)\n",
		    pmap, pmap->pm_stab, pmap->pm_stpa));
d2088 4
a2091 3

			PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
			    ("enter: alloc ste2 %d(%p)\n", ix, addr));
d2103 5
a2107 3

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: ste2 %p (%p)\n", pmap_ste2(pmap, va), ste));
d2110 1
a2110 1
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));
d2118 1
a2118 1
		struct kpt_page *kpt;
d2126 4
a2129 2
			PMAP_DPRINTF(PDB_COLLECT,
			    ("enter: no KPT pages, collecting...\n"));
d2134 4
d2144 1
a2144 1
		    VM_PROT_DEFAULT);
d2165 5
a2169 3
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
		    ("enter: about to fault UPT pg at %lx\n", va));
		s = uvm_fault(pt_map, va, 0, VM_PROT_READ|VM_PROT_WRITE);
d2171 2
a2172 3
			printf("uvm_fault(pt_map, 0x%lx, 0, RW) -> %d\n",
			    va, s);
			panic("pmap_enter: uvm_fault failed");
d2174 10
a2183 1
		pmap_extract(pmap_kernel(), va, &ptpa);
d2198 2
a2199 2
			       pmap == pmap_kernel() ? "Kernel" : "User",
			       va, ptpa, pte, *pte);
d2201 1
a2201 1
		pmap_changebit(ptpa, 0, ~PG_CCB);
d2216 1
a2216 1
		} while ((pv = pv->pv_next));
d2224 4
a2227 3

	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
	    ("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste));
d2249 5
a2253 3
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: stab %p refcnt %d\n",
		    pmap->pm_stab, pmap->pm_sref));
a2268 5
/*
 * pmap_pvdump:
 *
 *	Dump the contents of the PV list for the specified physical page.
 */
d2272 1
a2272 1
	paddr_t pa;
d2274 1
a2274 1
	struct pv_entry *pv;
a2283 7
/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
d2288 1
a2288 1
	vaddr_t va;
d2291 2
a2292 2
	int count;
	pt_entry_t *pte;
d2299 1
a2299 1
	if (!uvm_map_lookup_entry(pt_map, va, &entry)) {
d2311 89
a2399 1
#endif /* DEBUG */
@


1.16.4.2
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16.4.1 2001/07/04 10:18:40 niklas Exp $	*/
d107 1
d652 1
a652 1
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
d666 3
a668 2
struct pmap *
pmap_create(void)
d670 1
a670 1
	struct pmap *pmap;
d675 6
d899 3
a901 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
a902 1
	paddr_t		pa;
a905 2
	pa = VM_PAGE_TO_PHYS(pg);

d1060 2
a1061 2
int
pmap_enter(pmap, va, pa, prot, flags)
d1066 2
a1067 1
	int flags;
a1073 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d1079 3
d1206 1
a1206 1
		if ((flags & VM_PROT_ALL) & ~prot)
d1209 1
a1209 1
		if (flags & VM_PROT_WRITE)
d1211 1
a1211 1
		else if (flags & VM_PROT_ALL)
a1273 2

	return (KERN_SUCCESS);
d1657 3
a1659 2
boolean_t
pmap_clear_modify(struct vm_page *pg)
a1660 4
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;

	ret = pmap_is_modified(pg);
a1664 2

	return (ret);
d1672 3
a1674 2
boolean_t
pmap_clear_reference(struct vm_page *pg)
a1675 4
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;

	ret = pmap_is_referenced(pg);
a1679 2

	return (ret);
d1689 2
a1690 1
pmap_is_referenced(struct vm_page *pg)
a1691 2
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);

d1709 2
a1710 1
pmap_is_modified(struct vm_page *pg)
a1711 2
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);

d2187 2
a2188 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT,
		    VM_PROT_DEFAULT|PMAP_WIRED);
a2354 26

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pmap_enter(pmap_kernel(), va, pa, prot, VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
}

void
pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs)
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	}
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
@


1.16.4.3
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d106 3
d344 1
a344 1
		    NULL, UVM_UNKNOWN_OFFSET, 0,
d351 1
a351 1
		    NULL, UVM_UNKNOWN_OFFSET, 0,
d425 1
a425 1
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
@


1.16.4.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16.4.3 2001/11/13 21:00:53 niklas Exp $	*/
@


1.15
log
@pmap_activate() and pmap_deactivate() are MD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/04/24 06:39:41 downsj Exp $	*/
d637 1
a637 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE);
d1062 1
a1062 1
pmap_enter(pmap, va, pa, prot, wired)
d1068 1
d1462 2
a1463 1
	pmap_enter(pmap_kernel(), kva, phys, VM_PROT_READ|VM_PROT_WRITE, TRUE);
d1488 3
a1490 2
	pmap_enter(pmap_kernel(), skva, src, VM_PROT_READ, TRUE);
	pmap_enter(pmap_kernel(), dkva, dst, VM_PROT_READ|VM_PROT_WRITE, TRUE);
d2143 2
a2144 1
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE);
@


1.14
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1999/07/18 16:23:47 deraadt Exp $	*/
d795 1
d798 3
a800 2
pmap_activate(p)
	struct proc *p;
a801 2
	struct pcb *pcbp = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d814 2
d817 3
a819 2
pmap_deactivate(p)
	struct proc *p;
@


1.13
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/04/24 06:39:41 downsj Exp $	*/
a794 1
void	pmap_activate __P((register pmap_t));
d797 2
a798 2
pmap_activate(pmap)
	register pmap_t pmap;
d801 1
a813 2
void	pmap_deactivate __P((register pmap_t));

d815 2
a816 2
pmap_deactivate(pmap)
	register pmap_t pmap;
@


1.12
log
@Use the generic m68k param.h.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1999/01/11 05:11:36 millert Exp $	*/
d795 1
a795 1
void	pmap_activate __P((register pmap_t, struct pcb *));
d798 1
a798 1
pmap_activate(pmap, pcbp)
a799 1
	struct pcb *pcbp;
d801 1
d814 1
a814 1
void	pmap_deactivate __P((register pmap_t, struct pcb *));
d817 1
a817 1
pmap_deactivate(pmap, pcb)
a818 1
	struct pcb *pcb;
@


1.11
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1998/05/03 07:16:51 gene Exp $	*/
d334 1
a334 1
	avail_remaining -= mac68k_btop (size);
d364 1
a364 1
			   mac68k_ptob(IIOMAPSIZE + ROMMAPSIZE + NBMAPSIZE),
d1103 1
a1103 1
	pa = mac68k_trunc_page(pa);
d1647 1
a1647 1
	return(mac68k_ptob(ppn));
d2376 1
a2376 1
			index += mac68k_btop (pa - low[i]);
d2381 1
a2381 1
		index += mac68k_btop (high[i] - low[i]);
@


1.10
log
@Fallout from recent merge of macinfo.h to cpu.h.
Also, fallout from interrupt glue structure becoming mroe dynamic using
locore-changeable variables to store ipl levels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1997/02/10 12:16:56 downsj Exp $	*/
d367 1
a367 1
		panic("pmap_init: I/O space not mapped!\n");
d379 1
a379 1
		panic("pmap_init: bogons in the VM system!\n");
@


1.9
log
@kill extra decl
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1997/02/10 12:01:49 downsj Exp $	*/
a109 1
#include <machine/macinfo.h>
@


1.8
log
@mac68k copypage/zeropage changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 1997/01/24 01:35:51 briggs Exp $	*/
a1477 1
void copypage __P((caddr_t, caddr_t));
@


1.7
log
@Sync w/ NETBSD_CURRENT_971122.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 1996/10/28 14:55:33 briggs Exp $	*/
d1463 1
a1463 1
	bzero((caddr_t)kva, NBPG);
d1492 1
a1492 1
	pmap_remove(pmap_kernel(), skva, skva+2*NBPG);
@


1.6
log
@Include macinfo.h.
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.5 1996/05/26 18:36:28 briggs Exp $	*/
/*	$NetBSD: pmap.c,v 1.25 1996/05/07 01:45:22 briggs Exp $	*/
d384 1
a384 1
		printf("pmap_init: Sysseg %x, Sysmap %x, Sysptmap %x\n",
d386 1
a386 1
		printf("  pstart %x, plast %x, vstart %x, vend %x\n",
d410 1
a410 1
		printf("pmap_init: %x bytes: npages %x s0 %x(%x) tbl %x atr %x\n",
d453 1
a453 1
		printf("pmap_init: KPT: %d pages from %x to %x\n",
d483 1
a483 1
		printf("pmap_init: pt_map [%x - %x)\n", addr, addr2);
d634 1
a634 1
		printf("pmap_map(%x, %x, %x, %x)\n", va, spa, epa, prot);
d665 1
a665 1
		printf("pmap_create(%x)\n", size);
d696 1
a696 1
		printf("pmap_pinit(%x)\n", pmap);
d732 1
a732 1
		printf("pmap_destroy(%x)\n", pmap);
d756 1
a756 1
		printf("pmap_release(%x)\n", pmap);
d787 1
a787 1
		printf("pmap_reference(%x)\n", pmap);
d809 1
a809 1
		printf("pmap_activate(%x, %x)\n", pmap, pcbp);
d842 1
a842 1
		printf("pmap_remove(%x, %x, %x)\n", pmap, sva, eva);
d901 2
a902 2
	    prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE))
		printf("pmap_page_protect(%x, %x)\n", pa, prot);
d938 1
a938 1
				printf("%s wired mapping for %x not removed\n",
d963 1
a963 1
		printf("pmap_protect(%x, %x, %x, %x)\n", pmap, sva, eva, prot);
d1078 1
a1078 1
		printf("pmap_enter(%x, %x, %x, %x, %x)\n",
d1109 1
a1109 1
		printf("enter: pte %x, *pte %x\n", pte, *pte);
d1161 1
a1161 1
			printf("enter: removing old mapping %x\n", va);
d1194 1
a1194 1
			printf("enter: pv at %x: %x/%x/%x\n",
d1318 1
a1318 1
		printf("pmap_change_wiring(%x, %x, %x)\n", pmap, va, wired);
d1332 1
a1332 1
			printf("pmap_change_wiring: invalid STE for %x\n", va);
d1341 1
a1341 1
			printf("pmap_change_wiring: invalid PTE for %x\n", va);
d1374 1
a1374 1
		printf("pmap_extract(%x, %x) -> ", pmap, va);
d1383 1
a1383 1
		printf("%x\n", pa);
d1404 1
a1404 1
		printf("pmap_copy(%x, %x, %x, %x, %x)\n",
d1459 1
a1459 1
		printf("pmap_zero_page(%x)\n", phys);
d1484 1
a1484 1
		printf("pmap_copy_page(%x, %x)\n", src, dst);
d1518 1
a1518 1
		printf("pmap_pageable(%x, %x, %x, %x)\n",
d1535 1
a1535 1
			printf("pmap_pageable(%x, %x, %x, %x)\n",
d1548 1
a1548 1
			printf("pmap_pageable: bad PT page va %x next %x\n",
d1559 1
a1559 1
			printf("pa %x: flags=%x: not clean\n",
d1564 1
a1564 1
			printf("pmap_pageable: PT page %x(%x) unmodified\n",
d1582 1
a1582 1
		printf("pmap_clear_modify(%x)\n", pa);
d1598 1
a1598 1
		printf("pmap_clear_reference(%x)\n", pa);
d1617 1
a1617 1
		printf("pmap_is_referenced(%x) -> %c\n", pa, "FT"[rv]);
d1638 1
a1638 1
		printf("pmap_is_modified(%x) -> %c\n", pa, "FT"[rv]);
d1679 1
a1679 1
		printf("pmap_remove_mapping(%x, %x, %x, %x)\n",
d1710 1
a1710 1
		printf("remove: invalidating pte at %x\n", pte);
d1791 1
a1791 1
			printf("remove: ste was %x@@%x pte was %x@@%x\n",
d1814 1
a1814 1
				printf("remove: stab %x, refcnt %d\n",
d1823 1
a1823 1
					printf("remove: free stab %x\n",
d1930 1
a1930 1
		printf("pmap_changebit(%x, %x, %s)\n",
d2029 1
a2029 1
		printf("pmap_enter_ptpage: pmap %x, va %x\n", pmap, va);
d2064 1
a2064 1
			printf("enter: pmap %x stab %x(%x)\n",
d2089 1
a2089 1
				printf("enter: alloc ste2 %d(%x)\n", ix, addr);
d2104 1
a2104 1
			printf("enter: ste2 %x (%x)\n",
d2147 1
a2147 1
			printf("enter: add &Sysptmap[%d]: %x (KPT page %x)\n",
d2165 1
a2165 1
			printf("enter: about to fault UPT pg at %x\n", va);
d2195 1
a2195 1
			printf("%s PT no CCB: kva=%x ptpa=%x pte@@%x=%x\n",
d2224 1
a2224 1
		printf("enter: new PT page at PA %x, ste at %x\n", ptpa, ste);
d2249 1
a2249 1
			printf("enter: stab %x refcnt %d\n",
d2274 1
a2274 1
	printf("pa %x", pa);
d2276 1
a2276 1
		printf(" -> pmap %x, va %x, ptste %x, ptpmap %x, flags %x",
d2298 1
a2298 1
		printf("wired_check: entry for %x not found\n", va);
d2306 1
a2306 1
		printf("*%s*: %x: w%d/a%d\n",
@


1.5
log
@Add OpenBSD Id string.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d110 1
@


1.4
log
@Sync with NetBSD-current.
@
text
@d1 1
@


1.3
log
@update from netbsd (without losing local changes)
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.22 1995/12/03 13:52:50 briggs Exp $	*/
a177 8
#ifdef HAVEVAC
int pmapvacflush = 0;
#define	PVF_ENTER	0x01
#define	PVF_REMOVE	0x02
#define	PVF_PROTECT	0x04
#define	PVF_TOTAL	0x80
#endif

a285 3
#ifdef HAVEVAC
int		pmap_aliasmask;	/* seperation at which VA aliasing ok */
#endif
d498 4
a501 1
struct pv_entry *
d535 1
a535 1
void
a539 1
	register int i;
d558 2
d793 3
d813 2
a869 23
#ifdef HAVEVAC
				if (pmap_aliasmask) {
					/*
					 * Purge kernel side of VAC to ensure
					 * we get the correct state of any
					 * hardware maintained bits.
					 */
					if (firstpage) {
						DCIS();
#ifdef PMAPSTATS
						remove_stats.sflushes++;
#endif
					}
					/*
					 * Remember if we may need to
					 * flush the VAC due to a non-CI
					 * mapping.
					 */
					if (!needcflush && !pmap_pte_ci(pte))
						needcflush = TRUE;

				}
#endif
a881 36
#ifdef HAVEVAC
	/*
	 * In a couple of cases, we don't need to worry about flushing
	 * the VAC:
	 * 	1. if this is a kernel mapping,
	 *	   we have already done it
	 *	2. if it is a user mapping not for the current process,
	 *	   it won't be there
	 */
	if (pmap_aliasmask &&
	    (pmap == pmap_kernel() || pmap != curproc->p_vmspace->vm_map.pmap))
		needcflush = FALSE;
#ifdef DEBUG
	if (pmap_aliasmask && (pmapvacflush & PVF_REMOVE)) {
		if (pmapvacflush & PVF_TOTAL)
			DCIA();
		else if (pmap == pmap_kernel())
			DCIS();
		else
			DCIU();
	} else
#endif
	if (needcflush) {
		if (pmap == pmap_kernel()) {
			DCIS();
#ifdef PMAPSTATS
			remove_stats.sflushes++;
#endif
		} else {
			DCIU();
#ifdef PMAPSTATS
			remove_stats.uflushes++;
#endif
		}
	}
#endif
a998 12
#ifdef HAVEVAC
				/*
				 * Purge kernel side of VAC to ensure we
				 * get the correct state of any hardware
				 * maintained bits.
				 *
				 * XXX do we need to clear the VAC in
				 * general to reflect the new protection?
				 */
				if (firstpage && pmap_aliasmask)
					DCIS();
#endif
d1031 15
a1045 10
#if defined(HAVEVAC) && defined(DEBUG)
	if (pmap_aliasmask && (pmapvacflush & PVF_PROTECT)) {
		if (pmapvacflush & PVF_TOTAL)
			DCIA();
		else if (pmap == pmap_kernel())
			DCIS();
		else
			DCIU();
	}
#endif
a1230 50
#ifdef HAVEVAC
			/*
			 * Since there is another logical mapping for the
			 * same page we may need to cache-inhibit the
			 * descriptors on those CPUs with external VACs.
			 * We don't need to CI if:
			 *
			 * - No two mappings belong to the same user pmaps.
			 *   Since the cache is flushed on context switches
			 *   there is no problem between user processes.
			 *
			 * - Mappings within a single pmap are a certain
			 *   magic distance apart.  VAs at these appropriate
			 *   boundaries map to the same cache entries or
			 *   otherwise don't conflict.
			 *
			 * To keep it simple, we only check for these special
			 * cases if there are only two mappings, otherwise we
			 * punt and always CI.
			 *
			 * Note that there are no aliasing problems with the
			 * on-chip data-cache when the WA bit is set.
			 */
			if (pmap_aliasmask) {
				if (pv->pv_flags & PV_CI) {
#ifdef DEBUG
					if (pmapdebug & PDB_CACHE)
					printf("enter: pa %x already CI'ed\n",
					       pa);
#endif
					checkpv = cacheable = FALSE;
				} else if (npv->pv_next ||
					   ((pmap == pv->pv_pmap ||
					     pmap == pmap_kernel() ||
					     pv->pv_pmap == pmap_kernel()) &&
					    ((pv->pv_va & pmap_aliasmask) !=
					     (va & pmap_aliasmask)))) {
#ifdef DEBUG
					if (pmapdebug & PDB_CACHE)
					printf("enter: pa %x CI'ing all\n",
					       pa);
#endif
					cacheable = FALSE;
					pv->pv_flags |= PV_CI;
#ifdef PMAPSTATS
					enter_stats.ci++;
#endif
				}
			}
#endif
a1252 8
#ifdef HAVEVAC
	/*
	 * Purge kernel side of VAC to ensure we get correct state
	 * of HW bits so we don't clobber them.
	 */
	if (pmap_aliasmask)
		DCIS();
#endif
a1292 31
#ifdef HAVEVAC
	/*
	 * The following is executed if we are entering a second
	 * (or greater) mapping for a physical page and the mappings
	 * may create an aliasing problem.  In this case we must
	 * cache inhibit the descriptors involved and flush any
	 * external VAC.
	 */
	if (checkpv && !cacheable) {
		pmap_changebit(pa, PG_CI, TRUE);
		DCIA();
#ifdef PMAPSTATS
		enter_stats.flushes++;
#endif
#ifdef DEBUG
		if ((pmapdebug & (PDB_CACHE|PDB_PVDUMP)) ==
		    (PDB_CACHE|PDB_PVDUMP))
			pmap_pvdump(pa);
#endif
	}
#ifdef DEBUG
	else if (pmapvacflush & PVF_ENTER) {
		if (pmapvacflush & PVF_TOTAL)
			DCIA();
		else if (pmap == pmap_kernel())
			DCIS();
		else
			DCIU();
	}
#endif
#endif
d1476 1
a1688 24
#ifdef HAVEVAC
	if (pmap_aliasmask && (flags & PRM_CFLUSH)) {
		/*
		 * Purge kernel side of VAC to ensure we get the correct
		 * state of any hardware maintained bits.
		 */
		DCIS();
#ifdef PMAPSTATS
		remove_stats.sflushes++;
#endif
		/*
		 * If this is a non-CI user mapping for the current process,
		 * flush the VAC.  Note that the kernel side was flushed
		 * above so we don't worry about non-CI kernel mappings.
		 */
		if (pmap == curproc->p_vmspace->vm_map.pmap &&
		    !pmap_pte_ci(pte)) {
			DCIU();
#ifdef PMAPSTATS
			remove_stats.uflushes++;
#endif
		}
	}
#endif
a1778 19
#ifdef HAVEVAC
	/*
	 * If only one mapping left we no longer need to cache inhibit
	 */
	if (pmap_aliasmask &&
	    pv->pv_pmap && pv->pv_next == NULL && (pv->pv_flags & PV_CI)) {
#ifdef DEBUG
		if (pmapdebug & PDB_CACHE)
			printf("remove: clearing CI for pa %x\n", pa);
#endif
		pv->pv_flags &= ~PV_CI;
		pmap_changebit(pa, PG_CI, FALSE);
#ifdef DEBUG
		if ((pmapdebug & (PDB_CACHE|PDB_PVDUMP)) ==
		    (PDB_CACHE|PDB_PVDUMP))
			pmap_pvdump(pa);
#endif
	}
#endif
a1890 7
#ifdef HAVEVAC
	/*
	 * Flush VAC to get correct state of any hardware maintained bits.
	 */
	if (pmap_aliasmask && (bit & (PG_U|PG_M)))
		DCIS();
#endif
d1919 1
d1921 1
a1949 1
	 * If setting RO do we need to clear the VAC?
a1971 10
#ifdef HAVEVAC
			/*
			 * Flush VAC to ensure we get correct state of HW bits
			 * so we don't clobber them.
			 */
			if (firstpage && pmap_aliasmask) {
				firstpage = FALSE;
				DCIS();
			}
#endif
d1984 1
a1984 1
				    (bit == PG_RO && setem ||
a2009 10
#if defined(HAVEVAC) && defined(DEBUG)
		if (setem && bit == PG_RO && (pmapvacflush & PVF_PROTECT)) {
			if ((pmapvacflush & PVF_TOTAL) || toflush == 3)
				DCIA();
			else if (toflush == 2)
				DCIS();
			else
				DCIU();
		}
#endif
d2167 1
a2167 1
			printf("vm_fault(pt_map, %x, RW, 0) -> %d\n", va, s);
d2190 1
a2191 1
#ifdef DEBUG
d2212 1
a2212 1
		} while (pv = pv->pv_next);
@


1.2
log
@integrate 040 MDP_UNCACHE_WX cache changes by niklas & deraadt
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.21 1995/10/10 04:14:30 briggs Exp $	*/
d477 2
a478 1
	s = min(MAC_PTMAXSIZE, maxproc*MAC_MAX_PTSIZE);
d1390 1
a1390 1
		checkpv = cacheable = FALSE;
@


1.1
log
@Initial revision
@
text
@d1384 7
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
