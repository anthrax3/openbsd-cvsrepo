head	1.29;
access;
symbols
	OPENBSD_5_9:1.28.0.2
	OPENBSD_5_9_BASE:1.28
	OPENBSD_5_8:1.27.0.6
	OPENBSD_5_8_BASE:1.27
	OPENBSD_5_7:1.27.0.2
	OPENBSD_5_7_BASE:1.27
	OPENBSD_5_6:1.23.0.12
	OPENBSD_5_6_BASE:1.23
	OPENBSD_5_5:1.23.0.10
	OPENBSD_5_5_BASE:1.23
	OPENBSD_5_4:1.23.0.6
	OPENBSD_5_4_BASE:1.23
	OPENBSD_5_3:1.23.0.4
	OPENBSD_5_3_BASE:1.23
	OPENBSD_5_2:1.23.0.2
	OPENBSD_5_2_BASE:1.23
	OPENBSD_5_1_BASE:1.22
	OPENBSD_5_1:1.22.0.2
	OPENBSD_5_0:1.20.0.2
	OPENBSD_5_0_BASE:1.20
	OPENBSD_4_9:1.13.0.4
	OPENBSD_4_9_BASE:1.13
	OPENBSD_4_8:1.13.0.2
	OPENBSD_4_8_BASE:1.13
	OPENBSD_4_7:1.8.0.2
	OPENBSD_4_7_BASE:1.8
	OPENBSD_4_6:1.7.0.4
	OPENBSD_4_6_BASE:1.7
	OPENBSD_4_5:1.6.0.4
	OPENBSD_4_5_BASE:1.6
	OPENBSD_4_4:1.6.0.2
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.5.0.4
	OPENBSD_4_3_BASE:1.5
	OPENBSD_4_2:1.5.0.2
	OPENBSD_4_2_BASE:1.5
	OPENBSD_4_1:1.3.0.6
	OPENBSD_4_1_BASE:1.3
	OPENBSD_4_0:1.3.0.4
	OPENBSD_4_0_BASE:1.3
	OPENBSD_3_9:1.3.0.2
	OPENBSD_3_9_BASE:1.3
	OPENBSD_3_8:1.1.0.2
	OPENBSD_3_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.29
date	2016.05.11.21.52.50;	author deraadt;	state dead;
branches;
next	1.28;
commitid	VpgRpYXqYSJy4P7J;

1.28
date	2015.09.06.11.06.48;	author kettenis;	state Exp;
branches;
next	1.27;
commitid	P8Qw56gQPXKdw8A3;

1.27
date	2015.02.11.03.24.47;	author miod;	state Exp;
branches;
next	1.26;
commitid	7ZFXK9dyPt1QY5po;

1.26
date	2014.12.17.15.26.21;	author deraadt;	state Exp;
branches;
next	1.25;
commitid	CSUPb3DX3PWNCAdG;

1.25
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.24;
commitid	yv0ECmCdICvq576h;

1.24
date	2014.10.12.20.39.46;	author miod;	state Exp;
branches;
next	1.23;
commitid	n8DZa9w0X2LVgDkM;

1.23
date	2012.06.03.13.28.40;	author jsing;	state Exp;
branches;
next	1.22;

1.22
date	2011.09.18.11.55.23;	author kettenis;	state Exp;
branches;
next	1.21;

1.21
date	2011.09.18.11.02.17;	author kettenis;	state Exp;
branches;
next	1.20;

1.20
date	2011.08.01.22.21.42;	author kettenis;	state Exp;
branches;
next	1.19;

1.19
date	2011.07.07.22.50.42;	author kettenis;	state Exp;
branches;
next	1.18;

1.18
date	2011.07.07.18.40.12;	author kettenis;	state Exp;
branches;
next	1.17;

1.17
date	2011.07.04.17.07.27;	author kettenis;	state Exp;
branches;
next	1.16;

1.16
date	2011.05.30.22.25.21;	author oga;	state Exp;
branches;
next	1.15;

1.15
date	2011.04.14.19.34.55;	author kettenis;	state Exp;
branches;
next	1.14;

1.14
date	2011.04.14.13.24.04;	author jsing;	state Exp;
branches;
next	1.13;

1.13
date	2010.07.24.16.25.33;	author kettenis;	state Exp;
branches;
next	1.12;

1.12
date	2010.07.02.22.47.54;	author jsing;	state Exp;
branches;
next	1.11;

1.11
date	2010.07.01.04.18.36;	author jsing;	state Exp;
branches;
next	1.10;

1.10
date	2010.05.24.15.06.05;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	2010.03.31.19.46.27;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2009.12.16.16.54.42;	author jasper;	state Exp;
branches;
next	1.7;

1.7
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.6;

1.6
date	2008.04.27.17.48.10;	author martin;	state Exp;
branches;
next	1.5;

1.5
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2005.12.25.21.39.06;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2005.10.26.18.35.45;	author martin;	state Exp;
branches;
next	1.1;

1.1
date	2005.04.01.10.40.47;	author mickey;	state Exp;
branches;
next	;


desc
@@


1.29
log
@remove hppa64 port, which we never got going beyond broken single users.
hppa reverse-stack gives us a valuable test case, but most developers don't
have a 2nd one to proceed further with this.
ok kettenis
@
text
@/*	$OpenBSD: pmap.c,v 1.28 2015/09/06 11:06:48 kettenis Exp $	*/

/*
 * Copyright (c) 2005 Michael Shalayeff
 * All rights reserved.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF MIND, USE, DATA OR PROFITS, WHETHER IN
 * AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#ifndef SMALL_KERNEL
#define	PMAPDEBUG
#endif

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/lock.h>
#include <sys/proc.h>
#include <sys/user.h>
#include <sys/pool.h>
#include <sys/extent.h>

#include <uvm/uvm.h>

#include <machine/iomod.h>

#include <dev/rndvar.h>

#ifdef PMAPDEBUG
#define	DPRINTF(l,s)	do {		\
	if ((pmapdebug & (l)) == (l))	\
		printf s;		\
} while(0)
#define	PDB_FOLLOW	0x00000001
#define	PDB_INIT	0x00000002
#define	PDB_ENTER	0x00000004
#define	PDB_REMOVE	0x00000008
#define	PDB_CREATE	0x00000010
#define	PDB_PTPAGE	0x00000020
#define	PDB_CACHE	0x00000040
#define	PDB_BITS	0x00000080
#define	PDB_COLLECT	0x00000100
#define	PDB_PROTECT	0x00000200
#define	PDB_EXTRACT	0x00000400
#define	PDB_VP		0x00000800
#define	PDB_PV		0x00001000
#define	PDB_PARANOIA	0x00002000
#define	PDB_WIRING	0x00004000
#define	PDB_PMAP	0x00008000
#define	PDB_STEAL	0x00010000
#define	PDB_PHYS	0x00020000
#define	PDB_POOL	0x00040000
int pmapdebug = 0
	| PDB_INIT
/*	| PDB_FOLLOW */
/*	| PDB_VP */
/*	| PDB_PV */
/*	| PDB_ENTER */
/*	| PDB_REMOVE */
/*	| PDB_STEAL */
/*	| PDB_PROTECT */
/*	| PDB_PHYS */
	;
#else
#define	DPRINTF(l,s)	/* */
#endif

paddr_t physical_steal, physical_end;

struct pmap	kernel_pmap_store;
struct pool	pmap_pmap_pool;
struct pool	pmap_pv_pool;
int		pmap_pvlowat = 252;
int 		pmap_initialized;
int		pmap_nkpdes = 32;

pt_entry_t	hppa_prot[8];
#define	pmap_prot(m,vp) (hppa_prot[(vp)] | ((m) == pmap_kernel()? 0 : PTE_USER))

pt_entry_t	kernel_ptes[] = {
	PTE_EXEC  | PTE_ORDER | PTE_PREDICT | PTE_WIRED |
	    TLB_PAGE(0x000000) | PTE_PG4M,
	PTE_WRITE | PTE_ORDER | PTE_DIRTY   | PTE_WIRED |
	    TLB_PAGE(0x400000) | PTE_PG4M,
	PTE_WRITE | PTE_ORDER | PTE_DIRTY   | PTE_WIRED |
	    TLB_PAGE(0x800000) | PTE_PG4M,
	PTE_WRITE | PTE_ORDER | PTE_DIRTY   | PTE_WIRED |
	    TLB_PAGE(0xc00000) | PTE_PG4M
};

#define	pmap_pvh_attrs(a) \
	(((a) & PTE_DIRTY) | ((a) ^ PTE_REFTRAP))

struct vm_page	*pmap_pagealloc(struct uvm_object *obj, voff_t off);
volatile pt_entry_t *pmap_pde_get(volatile u_int32_t *pd, vaddr_t va);
void		 pmap_pde_set(struct pmap *pm, vaddr_t va, paddr_t ptp);
void		 pmap_pte_flush(struct pmap *pmap, vaddr_t va, pt_entry_t pte);
pt_entry_t *	 pmap_pde_alloc(struct pmap *pm, vaddr_t va,
		    struct vm_page **pdep);
#ifdef DDB
void		 pmap_dump_table(pa_space_t space, vaddr_t sva);
void		 pmap_dump_pv(paddr_t pa);
#endif
int		 pmap_check_alias(struct pv_entry *pve, vaddr_t va,
		    pt_entry_t pte);
void		 pmap_pv_free(struct pv_entry *pv);
void		 pmap_pv_enter(struct vm_page *pg, struct pv_entry *pve,
		    struct pmap *pm, vaddr_t va, struct vm_page *pdep);
struct pv_entry *pmap_pv_remove(struct vm_page *pg, struct pmap *pmap,
		    vaddr_t va);
void		 pmap_maphys(paddr_t spa, paddr_t epa);

#define	IS_IOPAGE(pa)	((pa) >= (HPPA_IOBEGIN & HPPA_PHYSMAP))

struct vm_page *
pmap_pagealloc(struct uvm_object *obj, voff_t off)
{
	struct vm_page *pg;

	if ((pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO)) == NULL)
		printf("pmap_pagealloc fail\n");

	return (pg);
}

volatile pt_entry_t *
pmap_pde_get(volatile u_int32_t *pd, vaddr_t va)
{
	int i;

	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_get(%p, 0x%lx)\n", pd, va));

	i = (va & PIE_MASK) >> PIE_SHIFT;
	if (i) {
		pd = (volatile u_int32_t *)((u_int64_t)pd[i] << PAGE_SHIFT);

		if (!pd)
			return (NULL);
	} else
		pd += PAGE_SIZE / sizeof(*pd);

	i = (va & PDE_MASK) >> PDE_SHIFT;
	return (pt_entry_t *)((u_int64_t)pd[i] << PAGE_SHIFT);
}

void
pmap_pde_set(struct pmap *pm, vaddr_t va, paddr_t ptp)
{
	volatile u_int32_t *pd = pm->pm_pdir;
	int i;

	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_set(%p, 0x%lx, 0x%lx)\n", pm, va, ptp));

	i = (va & PIE_MASK) >> PIE_SHIFT;
	if (i)
		pd = (volatile u_int32_t *)((u_int64_t)pd[i] << PAGE_SHIFT);
	else
		pd += PAGE_SIZE / sizeof(*pd);

	i = (va & PDE_MASK) >> PDE_SHIFT;
	pd[i] = ptp >> PAGE_SHIFT;
}

pt_entry_t *
pmap_pde_alloc(struct pmap *pm, vaddr_t va, struct vm_page **pdep)
{
	struct vm_page *pg;
	paddr_t pa;

	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_alloc(%p, 0x%lx, %p)\n", pm, va, pdep));

	if ((pg = pmap_pagealloc(&pm->pm_obj, va)) == NULL)
		return (NULL);

	pa = VM_PAGE_TO_PHYS(pg);

	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pde_alloc: pde %lx\n", pa));

	atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
	pg->wire_count = 1;		/* no mappings yet */
	pmap_pde_set(pm, va, pa);
	pm->pm_stats.resident_count++;	/* count PTP as resident */
	pm->pm_ptphint = pg;
	if (pdep)
		*pdep = pg;
	return ((pt_entry_t *)pa);
}

static __inline struct vm_page *
pmap_pde_ptp(struct pmap *pm, volatile pt_entry_t *pde)
{
	paddr_t pa = (paddr_t)pde;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp(%p, %p)\n", pm, pde));

	if (pm->pm_ptphint && VM_PAGE_TO_PHYS(pm->pm_ptphint) == pa)
		return (pm->pm_ptphint);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp: lookup 0x%lx\n", pa));

	return (PHYS_TO_VM_PAGE(pa));
}

static __inline void
pmap_pde_release(struct pmap *pmap, vaddr_t va, struct vm_page *ptp)
{
	paddr_t pa;

	DPRINTF(PDB_FOLLOW|PDB_PV,
	    ("pmap_pde_release(%p, 0x%lx, %p)\n", pmap, va, ptp));

	if (pmap != pmap_kernel() && --ptp->wire_count <= 1) {
		DPRINTF(PDB_FOLLOW|PDB_PV,
		    ("pmap_pde_release: disposing ptp %p\n", ptp));

		pmap_pde_set(pmap, va, 0);
		pmap->pm_stats.resident_count--;
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = RB_ROOT(&pmap->pm_obj.memt);
		ptp->wire_count = 0;
#ifdef DIAGNOSTIC
		if (ptp->pg_flags & PG_BUSY)
			panic("pmap_pde_release: busy page table page");
#endif
		pa = VM_PAGE_TO_PHYS(ptp);
		pdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
		pdtlb(HPPA_SID_KERNEL, pa);
		uvm_pagefree(ptp);
	}
}

static __inline pt_entry_t
pmap_pte_get(volatile pt_entry_t *pde, vaddr_t va)
{
	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pte_get(%p, 0x%lx)\n", pde, va));

	return (pde[(va & PTE_MASK) >> PTE_SHIFT]);
}

static __inline void
pmap_pte_set(volatile pt_entry_t *pde, vaddr_t va, pt_entry_t pte)
{
	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pte_set(%p, 0x%lx, 0x%lx)\n", pde, va, (long)pte));

	pde[(va & PTE_MASK) >> PTE_SHIFT] = pte;
}

void
pmap_pte_flush(struct pmap *pmap, vaddr_t va, pt_entry_t pte)
{
	fdcache(pmap->pm_space, va, PAGE_SIZE);
	if (pte & PTE_EXEC) {
		ficache(pmap->pm_space, va, PAGE_SIZE);
		pdtlb(pmap->pm_space, va);
		pitlb(pmap->pm_space, va);
	} else
		pdtlb(pmap->pm_space, va);
}

static __inline pt_entry_t
pmap_vp_find(struct pmap *pm, vaddr_t va)
{
	volatile pt_entry_t *pde;

	if (!(pde = pmap_pde_get(pm->pm_pdir, va)))
		return (0);

	return (pmap_pte_get(pde, va));
}

#ifdef DDB
void
pmap_dump_table(pa_space_t space, vaddr_t sva)
{
	volatile pt_entry_t *pde;
	volatile u_int32_t *pd;
	pt_entry_t pte;
	vaddr_t va, pdemask;

	if (space)
		pd = (u_int32_t *)mfctl(CR_VTOP);
	else
		pd = pmap_kernel()->pm_pdir;

	for (pdemask = 1, va = sva ? sva : 0;
	    va < VM_MAX_ADDRESS; va += PAGE_SIZE) {
		if (pdemask != (va & (PDE_MASK|PIE_MASK))) {
			pdemask = va & (PDE_MASK|PIE_MASK);
			if (!(pde = pmap_pde_get(pd, va))) {
				va = pdemask + PTE_MASK;
				continue;
			}
			printf("%x:%8p:\n", space, pde);
		}

		if (!(pte = pmap_pte_get(pde, va)))
			continue;

		printf("0x%08lx-0x%08llx:%lb\n",
		    va, PTE_PAGE(pte), (long)PTE_GETBITS(pte), PTE_BITS);
	}
}

void
pmap_dump_pv(paddr_t pa)
{
	struct vm_page *pg;
	struct pv_entry *pve;

	pg = PHYS_TO_VM_PAGE(pa);
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
		printf("%x:%lx\n", pve->pv_pmap->pm_space, pve->pv_va);
}
#endif

int
pmap_check_alias(struct pv_entry *pve, vaddr_t va, pt_entry_t pte)
{
	int ret;

	/* check for non-equ aliased mappings */
	for (ret = 0; pve; pve = pve->pv_next) {
		pte |= pmap_vp_find(pve->pv_pmap, pve->pv_va);
		if ((va & HPPA_PGAOFF) != (pve->pv_va & HPPA_PGAOFF) &&
		    (pte & PTE_WRITE)) {
#ifdef PMAPDEBUG
			printf("pmap_check_alias: "
			    "aliased writable mapping 0x%x:0x%lx\n",
			    pve->pv_pmap->pm_space, pve->pv_va);
			ret++;
#endif
		}
	}

	return (ret);
}

static __inline struct pv_entry *
pmap_pv_alloc(void)
{
	struct pv_entry *pv;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc()\n"));

	pv = pool_get(&pmap_pv_pool, PR_NOWAIT);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc: %p\n", pv));

	return (pv);
}

void
pmap_pv_free(struct pv_entry *pv)
{
	if (pv->pv_ptp)
		pmap_pde_release(pv->pv_pmap, pv->pv_va, pv->pv_ptp);

	pool_put(&pmap_pv_pool, pv);
}

void
pmap_pv_enter(struct vm_page *pg, struct pv_entry *pve, struct pmap *pm,
    vaddr_t va, struct vm_page *pdep)
{
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_enter(%p, %p, %p, 0x%lx, %p)\n",
	    pg, pve, pm, va, pdep));
	pve->pv_pmap	= pm;
	pve->pv_va	= va;
	pve->pv_ptp	= pdep;
	pve->pv_next	= pg->mdpage.pvh_list;
	pg->mdpage.pvh_list = pve;
#ifdef PMAPDEBUG
	if (pmap_check_alias(pve, va, 0))
		Debugger();
#endif
}

struct pv_entry *
pmap_pv_remove(struct vm_page *pg, struct pmap *pmap, vaddr_t va)
{
	struct pv_entry **pve, *pv;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_remove(%p, %p, 0x%lx)\n",
	    pg, pmap, va));

	for(pv = *(pve = &pg->mdpage.pvh_list);
	    pv; pv = *(pve = &(*pve)->pv_next))
		if (pv->pv_pmap == pmap && pv->pv_va == va) {
			*pve = pv->pv_next;
			break;
		}
	return (pv);
}

const pt_entry_t hppa_pgs[] = {
	PTE_PG4K,
	PTE_PG16K,
	PTE_PG64K,
	PTE_PG256K,
	PTE_PG1M,
	PTE_PG4M,
	PTE_PG16M,
	PTE_PG64M
};

void
pmap_maphys(paddr_t spa, paddr_t epa)
{
	volatile pt_entry_t *pde, *epde, pte;
	paddr_t pa, tpa;
	int s, e, i;

	DPRINTF(PDB_INIT, ("pmap_maphys: mapping 0x%lx - 0x%lx\n", spa, epa));

	s = ffs(spa) - 12;
	e = ffs(epa) - 12;

	if (s < e || (s == e && s / 2 < nitems(hppa_pgs))) {
		i = s / 2;
		if (i > nitems(hppa_pgs))
			i = nitems(hppa_pgs);
		pa = spa;
		spa = tpa = 0x1000 << ((i + 1) * 2);
	} else if (s > e) {
		i = e / 2;
		if (i > nitems(hppa_pgs))
			i = nitems(hppa_pgs);
		epa = pa = epa & (0xfffff000 << ((i + 1) * 2));
		tpa = epa;
	} else {
		i = s / 2;
		if (i > nitems(hppa_pgs))
			i = nitems(hppa_pgs);
		pa = spa;
		spa = tpa = epa;
	}

printf("pa 0x%lx tpa 0x%lx\n", pa, tpa);
	while (pa < tpa) {
		pte = TLB_PAGE(pa) | hppa_pgs[i - 1] |
		    PTE_WRITE | PTE_ORDER | PTE_DIRTY | PTE_WIRED;
		pde = pmap_pde_get(pmap_kernel()->pm_pdir, pa);
		epde = pde + (PTE_MASK >> PTE_SHIFT) + 1;
		if (pa + (PTE_MASK + (1 << PTE_SHIFT)) > tpa)
			epde = pde + ((tpa & PTE_MASK) >> PTE_SHIFT);
printf("pde %p epde %p pte 0x%lx\n", pde, epde, (long)pte);
		for (pde += (pa & PTE_MASK) >> PTE_SHIFT; pde < epde;)
			*pde++ = pte;
		pa += PTE_MASK + (1 << PTE_SHIFT);
		pa &= ~(PTE_MASK | PAGE_MASK);
	}

	if (spa < epa)
		pmap_maphys(spa, epa);
}

void
pmap_bootstrap(vaddr_t vstart)
{
	extern int resvphysmem, __rodata_end, __data_start;
	vaddr_t va, eaddr, addr = round_page(vstart);
	struct pmap *kpm;

	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_bootstrap(0x%lx)\n", vstart));

	uvmexp.pagesize = PAGE_SIZE;
	uvm_setpagesize();

	hppa_prot[PROT_NONE]  = PTE_ORDER|PTE_ACC_NONE;
	hppa_prot[PROT_READ]  = PTE_ORDER|PTE_READ;
	hppa_prot[PROT_WRITE] = PTE_ORDER|PTE_WRITE;
	hppa_prot[PROT_READ | PROT_WRITE] = PTE_ORDER|PTE_READ|PTE_WRITE;
	hppa_prot[PROT_EXEC]  = PTE_ORDER|PTE_EXEC;
	hppa_prot[PROT_READ | PROT_EXEC] = PTE_ORDER|PTE_READ|PTE_EXEC;
	hppa_prot[PROT_WRITE | PROT_EXEC] = PTE_ORDER|PTE_WRITE|PTE_EXEC;
	hppa_prot[PROT_READ | PROT_WRITE | PROT_EXEC] =
	    PTE_ORDER|PTE_READ|PTE_WRITE|PTE_EXEC;

	/*
	 * Initialize kernel pmap
	 */
	kpm = &kernel_pmap_store;
	bzero(kpm, sizeof(*kpm));
	uvm_objinit(&kpm->pm_obj, NULL, 1);
	kpm->pm_space = HPPA_SID_KERNEL;
	TAILQ_INIT(&kpm->pm_pglist);
	kpm->pm_pdir = (u_int32_t *)mfctl(CR_VTOP);
	fdcache(HPPA_SID_KERNEL, (vaddr_t)kpm->pm_pdir, 5 * PAGE_SIZE);

	/*
	 * Allocate various tables and structures.
	 */

	if (&__rodata_end < &__data_start) {
		physical_steal = (vaddr_t)&__rodata_end;
		physical_end = (vaddr_t)&__data_start;
		DPRINTF(PDB_INIT, ("physpool: 0x%lx @@ 0x%lx\n",
		    physical_end - physical_steal, physical_steal));
	}

	/* map enough PDEs to map initial physmem */
	for (va = 0x1000000, eaddr = ptoa(physmem);
	    va < eaddr; addr += PAGE_SIZE, va += 1 << PDE_SHIFT) {
		bzero((void *)addr, PAGE_SIZE);
		fdcache(HPPA_SID_KERNEL, addr, PAGE_SIZE);
		pmap_pde_set(kpm, va, addr);
		kpm->pm_stats.resident_count++;	/* count PTP as resident */
	}

	/* map a little of initial kmem */
	for (va = VM_MIN_KERNEL_ADDRESS + ((pmap_nkpdes - 1) << PDE_SHIFT);
	    va >= VM_MIN_KERNEL_ADDRESS;
	    addr += PAGE_SIZE, va -= 1 << PDE_SHIFT) {
		bzero((void *)addr, PAGE_SIZE);
		fdcache(HPPA_SID_KERNEL, addr, PAGE_SIZE);
		pmap_pde_set(kpm, va, addr);
		kpm->pm_stats.resident_count++;	/* count PTP as resident */
	}

	pmap_maphys(0x1000000, ptoa(physmem));

	eaddr = physmem - atop(round_page(MSGBUFSIZE));
	resvphysmem = atop(addr);
	DPRINTF(PDB_INIT, ("physmem: 0x%x - 0x%lx\n", resvphysmem, eaddr));
	uvm_page_physload(0, physmem, resvphysmem, eaddr, 0);
}

void
pmap_init(void)
{
	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init()\n"));

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, PR_WAITOK,
	    "pmappl", NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry),0, 0, 0, "pmappv",
	    NULL);
	pool_setlowat(&pmap_pv_pool, pmap_pvlowat);
	pool_sethiwat(&pmap_pv_pool, pmap_pvlowat * 32);

	pmap_initialized = 1;

	/*
	 * map SysCall gateways page once for everybody
	 * NB: we'll have to remap the phys memory
	 *     if we have any at SYSCALLGATE address (;
	 */
	{
		extern void gateway_page(void);
		volatile pt_entry_t *pde;

		if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, SYSCALLGATE)) &&
		    !(pde = pmap_pde_alloc(pmap_kernel(), SYSCALLGATE, NULL)))
			panic("pmap_init: cannot allocate pde");

#if 0
		pmap_pte_set(pde, SYSCALLGATE,
		    TLB_PAGE((paddr_t)gateway_page) | PTE_GATEWAY);
#else
		pmap_pte_set(pde, SYSCALLGATE,
		    TLB_PAGE((paddr_t)0x81000) | PTE_GATEWAY);
#endif
	}

	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init(): done\n"));
}

#ifdef PMAP_STEAL_MEMORY
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
{
	vaddr_t va;
	int npg;

	DPRINTF(PDB_FOLLOW|PDB_PHYS,
	    ("pmap_steal_memory(0x%lx, %p, %p)\n", size, vstartp, vendp));

	size = round_page(size);
	npg = atop(size);

	if (vm_physmem[0].avail_end - vm_physmem[0].avail_start < npg)
		panic("pmap_steal_memory: no more");

	if (vstartp)
		*vstartp = VM_MIN_KERNEL_ADDRESS;
	if (vendp)
		*vendp = VM_MAX_KERNEL_ADDRESS;

	vm_physmem[0].end -= npg;
	vm_physmem[0].avail_end -= npg;
	va = ptoa(vm_physmem[0].avail_end);
	bzero((void *)va, size);

	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_steal_memory: 0x%lx\n", va));

	return (va);
}
#else
void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = VM_MIN_KERNEL_ADDRESS;
	*endp = VM_MAX_KERNEL_ADDRESS;
}
#endif /* PMAP_STEAL_MEMORY */

#ifdef PMAP_GROWKERNEL
vaddr_t
pmap_growkernel(vaddr_t kva)
{
	vaddr_t va;

	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_growkernel(0x%lx)\n", kva));

	va = VM_MIN_KERNEL_ADDRESS + (pmap_nkpdes << PDE_SHIFT);
	DPRINTF(PDB_PHYS, ("pmap_growkernel: was va 0x%lx\n", va));
	if (va < kva) {
		for ( ; va < kva ; pmap_nkpdes++, va += 1 << PDE_SHIFT)
			if (uvm.page_init_done) {
				if (!pmap_pde_alloc(pmap_kernel(), va, NULL))
					break;
			} else {
				paddr_t pa;

				pa = pmap_steal_memory(PAGE_SIZE, NULL, NULL);
				if (pa)
					panic("pmap_growkernel: out of memory");
				pmap_pde_set(pmap_kernel(), va, pa);
				pmap_kernel()->pm_stats.resident_count++;
			}
	}
	DPRINTF(PDB_PHYS|PDB_VP, ("pmap_growkernel: now va 0x%lx\n", va));
	return (va);
}
#endif /* PMAP_GROWKERNEL */

struct pmap *
pmap_create(void)
{
	struct pmap *pmap;
	struct vm_page *pg;
	static pa_space_t space = 0x200;
	paddr_t pa;

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_create()\n"));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);

	uvm_objinit(&pmap->pm_obj, NULL, 1);

	pmap->pm_ptphint = NULL;

	TAILQ_INIT(&pmap->pm_pglist);
	if (uvm_pglistalloc(2 * PAGE_SIZE, 0, VM_MIN_KERNEL_ADDRESS - 1,
	    PAGE_SIZE, 2 * PAGE_SIZE, &pmap->pm_pglist, 1, UVM_PLA_WAITOK))
		panic("pmap_create: no pages");

	pg = TAILQ_FIRST(&pmap->pm_pglist);
	atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_CLEAN);
	pmap->pm_pdir = (u_int32_t *)(pa = VM_PAGE_TO_PHYS(pg));
	bzero((void *)pa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);

	/* set the first PIE that's covering low 2g of the address space */
	pg = TAILQ_LAST(&pmap->pm_pglist, pglist);
	atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_CLEAN);
	*pmap->pm_pdir = (pa = VM_PAGE_TO_PHYS(pg)) >> PAGE_SHIFT;
	bzero((void *)pa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);

/* TODO	for (space = 1 + (arc4random() & HPPA_SID_MAX);
	    pmap_sdir_get(space); space = (space + 1) % HPPA_SID_MAX); */
	pmap->pm_space = space;
	space += 0x200;

	pmap->pm_stats.resident_count = 2;
	pmap->pm_stats.wired_count = 0;

	return (pmap);
}

void
pmap_destroy(struct pmap *pmap)
{
	struct vm_page *pg;
	paddr_t pa;
	int refs;

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_destroy(%p)\n", pmap));

	refs = --pmap->pm_obj.uo_refs;

	if (refs > 0)
		return;

#ifdef DIAGNOSTIC
	while ((pg = RB_ROOT(&pmap->pm_obj.memt))) {
		pt_entry_t *pde, *epde;
		struct vm_page *spg;
		struct pv_entry *pv, *npv;

		KASSERT(pg != TAILQ_FIRST(&pmap->pm_pglist));
		KASSERT(pg != TAILQ_LAST(&pmap->pm_pglist, pglist));
		pa = VM_PAGE_TO_PHYS(pg);
#ifdef PMAPDEBUG
		printf("pmap_destroy(%p): stray ptp 0x%lx w/ %u ents:",
		    pmap, pa, pg->wire_count - 1);
#endif

		pde = (pt_entry_t *)pa;
		epde = (pt_entry_t *)(pa + PAGE_SIZE);
		for (; pde < epde; pde++) {
			if (*pde == 0)
				continue;

			spg = PHYS_TO_VM_PAGE(PTE_PAGE(*pde));
			if (spg == NULL)
				continue;
			for (pv = spg->mdpage.pvh_list; pv != NULL; pv = npv) {
				npv = pv->pv_next;
				if (pv->pv_pmap == pmap) {
#ifdef PMAPDEBUG
					printf(" 0x%lx", pv->pv_va);
#endif
					pmap_remove(pmap, pv->pv_va,
					    pv->pv_va + PAGE_SIZE);
				}
			}
		}
#ifdef PMAPDEBUG
		printf("\n");
#endif
	}
#endif

	pg = TAILQ_FIRST(&pmap->pm_pglist);
	pa = VM_PAGE_TO_PHYS(pg);
	pdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, pa);

	pg = TAILQ_LAST(&pmap->pm_pglist, pglist);
	pa = VM_PAGE_TO_PHYS(pg);
	pdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, pa);

	uvm_pglistfree(&pmap->pm_pglist);
	TAILQ_INIT(&pmap->pm_pglist);
	pool_put(&pmap_pmap_pool, pmap);
}

/*
 * Add a reference to the specified pmap.
 */
void
pmap_reference(struct pmap *pmap)
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_reference(%p)\n", pmap));

	pmap->pm_obj.uo_refs++;
}

void
pmap_collect(struct pmap *pmap)
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_collect(%p)\n", pmap));
	/* nothing yet */
}

int
pmap_enter(struct pmap *pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	struct vm_page *pg, *ptp = NULL;
	struct pv_entry *pve = NULL;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, 0x%lx, 0x%lx, 0x%x, 0x%x)\n",
	    pmap, va, pa, prot, flags));

	if (!(pde = pmap_pde_get(pmap->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap, va, &ptp))) {
		if (flags & PMAP_CANFAIL)
			return (ENOMEM);

		panic("pmap_enter: cannot allocate pde");
	}

	if (!ptp)
		ptp = pmap_pde_ptp(pmap, pde);

	if ((pte = pmap_pte_get(pde, va))) {

		DPRINTF(PDB_ENTER,
		    ("pmap_enter: remapping 0x%lx -> 0x%lx\n", (long)pte, pa));

		pmap_pte_flush(pmap, va, pte);
		if (wired && !(pte & PTE_WIRED))
			pmap->pm_stats.wired_count++;
		else if (!wired && (pte & PTE_WIRED))
			pmap->pm_stats.wired_count--;

		if (PTE_PAGE(pte) == pa) {
			DPRINTF(PDB_FOLLOW|PDB_ENTER,
			    ("pmap_enter: same page\n"));
			goto enter;
		}

		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
		if (pg != NULL) {
			pve = pmap_pv_remove(pg, pmap, va);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
		}
	} else {
		DPRINTF(PDB_ENTER,
		    ("pmap_enter: new mapping 0x%lx -> 0x%lx\n", va, pa));
		pte = PTE_REFTRAP;
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
		if (ptp)
			ptp->wire_count++;
	}

	if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(pa))) {
		if (!pve && !(pve = pmap_pv_alloc())) {
			if (flags & PMAP_CANFAIL)
				return (ENOMEM);
			panic("pmap_enter: no pv entries available");
		}
		pte |= pmap_prot(pmap, prot);
		if (pmap_check_alias(pg->mdpage.pvh_list, va, pte))
			pmap_page_remove(pg);
		pmap_pv_enter(pg, pve, pmap, va, ptp);
	} else if (pve)
		pmap_pv_free(pve);

enter:
	/* preserve old ref & mod */
	pte = TLB_PAGE(pa) | pmap_prot(pmap, prot) |
	    (pte & (PTE_UNCACHABLE|PTE_DIRTY|PTE_REFTRAP));
	if (IS_IOPAGE(pa))
		pte |= PTE_UNCACHABLE | PTE_ORDER;
	if (wired)
		pte |= PTE_WIRED;
	pmap_pte_set(pde, va, pte);

	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_enter: leaving\n"));

	return (0);
}

void
pmap_remove(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	struct pv_entry *pve;
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	struct vm_page *pg, *ptp;
	vaddr_t pdemask;
	int batch;

	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_remove(%p, 0x%lx, 0x%lx)\n", pmap, sva, eva));

	for (batch = 0; sva < eva; sva += PAGE_SIZE) {
		pdemask = sva & (PDE_MASK|PIE_MASK);
		if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
			sva = pdemask + PTE_MASK;
			continue;
		}
		if (pdemask == sva) {
			if (sva + (PTE_MASK + PAGE_SIZE) <= eva)
				batch = 1;
			else
				batch = 0;
		}

		if ((pte = pmap_pte_get(pde, sva))) {

			/* TODO measure here the speed tradeoff
			 * for flushing whole PT vs per-page
			 * in case of non-complete pde fill
			 */
			pmap_pte_flush(pmap, sva, pte);
			if (pte & PTE_WIRED)
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;

			/* iff properly accounted pde will be dropped anyway */
			if (!batch)
				pmap_pte_set(pde, sva, 0);

			if (pmap_initialized &&
			    (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {

				pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
				if ((pve = pmap_pv_remove(pg, pmap, sva)))
					pmap_pv_free(pve);
			} else {
				if (IS_IOPAGE(PTE_PAGE(pte))) {
					ptp = pmap_pde_ptp(pmap, pde);
					if (ptp != NULL)
						pmap_pde_release(pmap, sva, ptp);
				}
			}
		}
	}

	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_remove: leaving\n"));
}

void
pmap_write_protect(struct pmap *pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	struct vm_page *pg;
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	u_int tlbprot, pdemask;

	DPRINTF(PDB_FOLLOW|PDB_PMAP,
	    ("pmap_write_protect(%p, %lx, %lx, %x)\n", pmap, sva, eva, prot));

	sva = trunc_page(sva);
	tlbprot = pmap_prot(pmap, prot);

	for (pdemask = 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & (PDE_MASK|PIE_MASK))) {
			pdemask = sva & (PDE_MASK|PIE_MASK);
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva = pdemask + PTE_MASK;
				continue;
			}
		}
		if ((pte = pmap_pte_get(pde, sva))) {

			DPRINTF(PDB_PMAP,
			    ("pmap_write_protect: va=0x%lx pte=0x%lx\n",
			    sva,  (long)pte));
			/*
			 * Determine if mapping is changing.
			 * If not, nothing to do.
			 */
			if ((pte & PTE_ACC_MASK) == tlbprot)
				continue;

			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			if (pg != NULL)
				pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);

			pmap_pte_flush(pmap, sva, pte);
			pte &= ~PTE_ACC_MASK;
			pte |= tlbprot;
			pmap_pte_set(pde, sva, pte);
		}
	}
}

void
pmap_page_remove(struct vm_page *pg)
{
	struct pv_entry *pve, *ppve;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove(%p)\n", pg));

	if (pg->mdpage.pvh_list == NULL)
		return;

	for (pve = pg->mdpage.pvh_list; pve;
	     pve = (ppve = pve)->pv_next, pmap_pv_free(ppve)) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		volatile pt_entry_t *pde;
		pt_entry_t pte;

		pde = pmap_pde_get(pmap->pm_pdir, va);
		pte = pmap_pte_get(pde, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);

		pmap_pte_flush(pmap, va, pte);
		if (pte & PTE_WIRED)
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		pmap_pte_set(pde, va, 0);
	}
	pg->mdpage.pvh_list = NULL;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove: leaving\n"));

}

void
pmap_unwire(struct pmap *pmap, vaddr_t	va)
{
	volatile pt_entry_t *pde;
	pt_entry_t pte = 0;

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire(%p, 0x%lx)\n", pmap, va));

	if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
		pte = pmap_pte_get(pde, va);

		if (pte & PTE_WIRED) {
			pte &= ~PTE_WIRED;
			pmap->pm_stats.wired_count--;
			pmap_pte_set(pde, va, pte);
		}
	}

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire: leaving\n"));

#ifdef DIAGNOSTIC
	if (!pte)
		panic("pmap_unwire: invalid va 0x%lx", va);
#endif
}

boolean_t
pmap_changebit(struct vm_page *pg, pt_entry_t set, pt_entry_t clear)
{
	struct pv_entry *pve;
	pt_entry_t res;

	DPRINTF(PDB_FOLLOW|PDB_BITS,
	    ("pmap_changebit(%p, %lx, %lx)\n", pg, (long)set, (long)clear));

	res = pg->mdpage.pvh_attrs = 0;
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		volatile pt_entry_t *pde;
		pt_entry_t opte, pte;

		if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
			opte = pte = pmap_pte_get(pde, va);
#ifdef PMAPDEBUG
			if (!pte) {
				printf("pmap_changebit: zero pte for 0x%lx\n",
				    va);
				continue;
			}
#endif
			pte &= ~clear;
			pte |= set;
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			res |= pmap_pvh_attrs(opte);

			if (opte != pte) {
				pmap_pte_flush(pmap, va, opte);
				pmap_pte_set(pde, va, pte);
			}
		}
	}

	return ((res & (clear | set)) != 0);
}

boolean_t
pmap_testbit(struct vm_page *pg, pt_entry_t bit)
{
	struct pv_entry *pve;
	pt_entry_t pte;

	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %lx)\n", pg, (long)bit));

	for(pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bit) && pve;
	    pve = pve->pv_next) {
		pte = pmap_vp_find(pve->pv_pmap, pve->pv_va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
	}

	return ((pg->mdpage.pvh_attrs & bit) != 0);
}

boolean_t
pmap_extract(struct pmap *pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t pte;
	vaddr_t mask;

	DPRINTF(PDB_FOLLOW|PDB_EXTRACT, ("pmap_extract(%p, %lx)\n", pmap, va));

	pte = pmap_vp_find(pmap, va);

	if (pte) {
		if (pap) {
			mask = PTE_PAGE_SIZE(pte) - 1;
			*pap = PTE_PAGE(pte) | (va & mask);
		}
		return (TRUE);
	}

	return (FALSE);
}

void
pmap_activate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	struct pcb *pcb = &p->p_addr->u_pcb;

	pcb->pcb_space = pmap->pm_space;
}

void
pmap_deactivate(struct proc *p)
{

}

static __inline void
pmap_flush_page(struct vm_page *pg, int purge)
{
	struct pv_entry *pve;

	/* purge cache for all possible mappings for the pa */
	for (pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
		if (purge)
			pdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		else
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		ficache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		pdtlb(pve->pv_pmap->pm_space, pve->pv_va);
		pitlb(pve->pv_pmap->pm_space, pve->pv_va);
	}
}

void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%lx)\n", pa));

	pmap_flush_page(pg, 1);
	bzero((void *)pa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, pa);
}

void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t spa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dpa = VM_PAGE_TO_PHYS(dstpg);
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_copy_page(%lx, %lx)\n", spa, dpa));

	pmap_flush_page(srcpg, 0);
	pmap_flush_page(dstpg, 1);
	bcopy((void *)spa, (void *)dpa, PAGE_SIZE);
	pdcache(HPPA_SID_KERNEL, spa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, dpa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, spa);
	pdtlb(HPPA_SID_KERNEL, dpa);
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	volatile pt_entry_t *pde;
	pt_entry_t pte, opte;

	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%lx, %lx, %x)\n", va, pa, prot));

	if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap_kernel(), va, NULL)))
		panic("pmap_kenter_pa: cannot allocate pde for va=0x%lx", va);
	opte = pmap_pte_get(pde, va);
	pte = TLB_PAGE(pa) | PTE_WIRED | PTE_REFTRAP |
	    pmap_prot(pmap_kernel(), prot);
	if (IS_IOPAGE(pa))
		pte |= PTE_UNCACHABLE | PTE_ORDER;
	if (opte)
		pmap_pte_flush(pmap_kernel(), va, opte);
	pmap_pte_set(pde, va, pte);
	pmap_kernel()->pm_stats.wired_count++;
	pmap_kernel()->pm_stats.resident_count++;

#ifdef PMAPDEBUG
	{
		struct vm_page *pg;

		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {

			if (pmap_check_alias(pg->mdpage.pvh_list, va, pte))
				Debugger();
		}
	}
#endif
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_kenter_pa: leaving\n"));
}

void
pmap_kremove(vaddr_t va, vsize_t size)
{
	struct pv_entry *pve;
	vaddr_t eva, pdemask;
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	struct vm_page *pg;

	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_kremove(%lx, %lx)\n", va, size));
#ifdef PMAPDEBUG
	if (va < ptoa(physmem)) {
		printf("pmap_kremove(%lx, %lx): unmapping physmem\n", va, size);
		return;
	}
#endif

	for (pdemask = 1, eva = va + size; va < eva; va += PAGE_SIZE) {
		if (pdemask != (va & (PDE_MASK|PIE_MASK))) {
			pdemask = va & (PDE_MASK|PIE_MASK);
			if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va))) {
				va = pdemask + PTE_MASK;
				continue;
			}
		}
		if (!(pte = pmap_pte_get(pde, va))) {
#ifdef DEBUG
			printf("pmap_kremove: unmapping unmapped 0x%lx\n", va);
#endif
			continue;
		}

		pmap_pte_flush(pmap_kernel(), va, pte);
		pmap_pte_set(pde, va, 0);
		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {

			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			/* just in case we have enter/kenter mismatch */
			if ((pve = pmap_pv_remove(pg, pmap_kernel(), va)))
				pmap_pv_free(pve);
		}
	}

	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_kremove: leaving\n"));
}

void
pmap_proc_iflush(struct proc *p, vaddr_t va, vsize_t len)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	fdcache(pmap->pm_space, va, len);
	sync_caches();
	ficache(pmap->pm_space, va, len);
	sync_caches();
}

struct vm_page *
pmap_unmap_direct(vaddr_t va)
{
	fdcache(HPPA_SID_KERNEL, va, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, va);
	return (PHYS_TO_VM_PAGE(va));
}
@


1.28
log
@PR_WAITOK for the pmap_pmap_pool here as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2015/02/11 03:24:47 miod Exp $	*/
@


1.27
log
@Backout non-wanted changed introduced by accident as part of 1.24.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2014/12/17 15:26:21 deraadt Exp $	*/
d548 4
a551 3
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry),0,0,0, "pmappv", NULL);
@


1.26
log
@delete simplelocks
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2014/11/16 12:30:57 deraadt Exp $	*/
d583 1
a583 1
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp, int zero)
d605 1
a605 2
	if (zero)
		bzero((void *)va, size);
d638 1
a638 1
				pa = pmap_steal_memory(PAGE_SIZE, NULL, NULL, 1);
@


1.25
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2014/10/12 20:39:46 miod Exp $	*/
a325 1
	simple_lock(&pg->mdpage.pvh_lock);
a327 1
	simple_unlock(&pg->mdpage.pvh_lock);
a400 1
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
a406 1
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
a631 2
		simple_lock(&pmap_kernel()->pm_obj.vmobjlock);

a644 2

		simple_unlock(&pmap_kernel()->pm_obj.vmobjlock);
a704 1
	simple_lock(&pmap->pm_lock);
a705 1
	simple_unlock(&pmap->pm_lock);
a772 1
	simple_lock(&pmap->pm_lock);
a773 1
	simple_unlock(&pmap->pm_lock);
a795 2
	simple_lock(&pmap->pm_lock);

d798 1
a798 2
		if (flags & PMAP_CANFAIL) {
			simple_unlock(&pmap->pm_lock);
a799 1
		}
a825 1
			simple_lock(&pg->mdpage.pvh_lock);
a827 1
			simple_unlock(&pg->mdpage.pvh_lock);
d842 1
a842 3
			if (flags & PMAP_CANFAIL) {
				simple_unlock(&pg->mdpage.pvh_lock);
				simple_unlock(&pmap->pm_lock);
a843 1
			}
a846 1
		simple_lock(&pg->mdpage.pvh_lock);
a849 1
		simple_unlock(&pg->mdpage.pvh_lock);
a862 2
	simple_unlock(&pmap->pm_lock);

a880 2
	simple_lock(&pmap->pm_lock);

a911 1
				simple_lock(&pg->mdpage.pvh_lock);
a914 1
				simple_unlock(&pg->mdpage.pvh_lock);
a924 2
	simple_unlock(&pmap->pm_lock);

a941 2
	simple_lock(&pmap->pm_lock);

d963 1
a963 2
			if (pg != NULL) {
				simple_lock(&pg->mdpage.pvh_lock);
a964 2
				simple_unlock(&pg->mdpage.pvh_lock);
			}
a971 2

	simple_unlock(&pmap->pm_lock);
a983 1
	simple_lock(&pg->mdpage.pvh_lock);
a990 2
		simple_lock(&pmap->pm_lock);

a1000 1
		simple_unlock(&pmap->pm_lock);
a1002 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1015 1
	simple_lock(&pmap->pm_lock);
a1024 1
	simple_unlock(&pmap->pm_lock);
a1042 1
	simple_lock(&pg->mdpage.pvh_lock);
a1049 1
		simple_lock(&pmap->pm_lock);
a1068 1
		simple_unlock(&pmap->pm_lock);
a1069 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1081 1
	simple_lock(&pg->mdpage.pvh_lock);
a1083 1
		simple_lock(&pve->pv_pmap->pm_lock);
a1084 1
		simple_unlock(&pve->pv_pmap->pm_lock);
a1086 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1098 1
	simple_lock(&pmap->pm_lock);
a1099 1
	simple_unlock(&pmap->pm_lock);
a1132 1
	simple_lock(&pg->mdpage.pvh_lock);
a1141 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1181 2
	simple_lock(&pmap->pm_lock);

a1201 1
			simple_lock(&pg->mdpage.pvh_lock);
a1203 1
			simple_unlock(&pg->mdpage.pvh_lock);
a1206 2
	simple_unlock(&pmap->pm_lock);

a1227 2
	simple_lock(&pmap->pm_lock);

a1246 1
			simple_lock(&pg->mdpage.pvh_lock);
a1250 1
			simple_unlock(&pg->mdpage.pvh_lock);
a1252 2

	simple_unlock(&pmap->pm_lock);
@


1.24
log
@Rough sync with hppa to make this compile again.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2012/06/03 13:28:40 jsing Exp $	*/
d488 9
a496 8
	hppa_prot[UVM_PROT_NONE]  = PTE_ORDER|PTE_ACC_NONE;
	hppa_prot[UVM_PROT_READ]  = PTE_ORDER|PTE_READ;
	hppa_prot[UVM_PROT_WRITE] = PTE_ORDER|PTE_WRITE;
	hppa_prot[UVM_PROT_RW]    = PTE_ORDER|PTE_READ|PTE_WRITE;
	hppa_prot[UVM_PROT_EXEC]  = PTE_ORDER|PTE_EXEC;
	hppa_prot[UVM_PROT_RX]    = PTE_ORDER|PTE_READ|PTE_EXEC;
	hppa_prot[UVM_PROT_WX]    = PTE_ORDER|PTE_WRITE|PTE_EXEC;
	hppa_prot[UVM_PROT_RWX]   = PTE_ORDER|PTE_READ|PTE_WRITE|PTE_EXEC;
@


1.23
log
@Avoid the unlikely but possible use of an uninitialised variable.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2011/08/01 22:21:42 kettenis Exp $	*/
d20 1
d22 1
d258 1
a258 1
	    ("pmap_pte_set(%p, 0x%lx, 0x%lx)\n", pde, va, pte));
d314 2
a315 2
		printf("0x%08lx-0x%08lx:%b\n",
		    va, PTE_PAGE(pte), PTE_GETBITS(pte), PTE_BITS);
d465 1
a465 1
printf("pde %p epde %p pte 0x%lx\n", pde, epde, pte);
d485 1
d542 1
a542 1
	DPRINTF(PDB_INIT, ("physmem: 0x%lx - 0x%lx\n", resvphysmem, eaddr));
d586 1
a586 1
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
d608 2
a609 1
	bzero((void *)va, size);
d644 1
a644 1
				pa = pmap_steal_memory(PAGE_SIZE, NULL, NULL);
d746 1
a746 1
					printf(" 0x%x", pv->pv_va);
d825 1
a825 1
		    ("pmap_enter: remapping 0x%lx -> 0x%lx\n", pte, pa));
d986 1
a986 1
			    sva,  pte));
d1085 1
a1085 1
	    ("pmap_changebit(%p, %lx, %lx)\n", pg, set, clear));
d1128 1
a1128 1
	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %lx)\n", pg, bit));
@


1.22
log
@Use an uvm object to keep track of the page table pages, just like we do on
hppa to reduce the diffs between the two pmaps.  Also add cache flushing
in places where hppa does them.
@
text
@d796 1
a796 1
	struct pv_entry *pve;
a845 1
		pve = NULL;
@


1.21
log
@Take into account the PIE bits when masking PDEs.
@
text
@d101 1
a101 1
struct vm_page	*pmap_pagealloc(int wait);
d123 1
a123 1
pmap_pagealloc(int wait)
d127 1
a127 1
	if ((pg = uvm_pagealloc(NULL, 0, NULL,
d183 1
a183 1
	if ((pg = pmap_pagealloc(0)) == NULL)
d230 1
a230 1
			pmap->pm_ptphint = NULL;
d499 1
a499 2
	simple_lock_init(&kpm->pm_lock);
	kpm->pm_refcount = 1;
d503 1
d520 1
d530 1
d666 2
a667 2
	simple_lock_init(&pmap->pm_lock);
	pmap->pm_refcount = 1;
d679 1
d686 1
d702 2
d709 1
a709 1
	refs = --pmap->pm_refcount;
d715 50
d779 1
a779 1
	pmap->pm_refcount++;
@


1.20
log
@Synch with hppa.  Seems to fix at least one of the remaining pmap bugs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2011/07/07 22:50:42 kettenis Exp $	*/
a287 1
	pa_space_t sp;
d303 1
a303 1
				va += ~PDE_MASK + 1 - PAGE_SIZE;
d306 1
a306 1
			printf("%x:%8p:\n", sp, pde);
d849 1
a849 1
		pdemask = sva & PDE_MASK;
d851 1
a851 1
			sva = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
d855 1
a855 1
			if (sva + (~PDE_MASK + 1) <= eva)
d916 2
a917 2
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
d919 1
a919 1
				sva = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
d1234 2
a1235 2
		if (pdemask != (va & PDE_MASK)) {
			pdemask = va & PDE_MASK;
d1237 1
a1237 1
				va = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
@


1.19
log
@Add missing bits to the syscall entry path.  Not perfect yet, but it works
for at least the open(2), write(2) and exit(2) system calls.  Map the syscall
entry page publically.  For now the physical address is hardcoded because
of toolchain problems.  It is somewhat likely to say the same as long as we
don't add stuff to the start of locore.S.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2011/07/07 18:40:12 kettenis Exp $	*/
d120 2
d218 2
d226 1
d236 3
a238 2
		pdcache(HPPA_SID_KERNEL, (vaddr_t)ptp, PAGE_SIZE);
		pdtlb(HPPA_SID_KERNEL, (vaddr_t)ptp);
a331 1
#ifdef PMAPDEBUG
d342 1
d347 1
a352 1
#endif
a772 1
		pte &= PTE_UNCACHABLE|PTE_DIRTY|PTE_REFTRAP;
d781 6
a786 3
		simple_lock(&pg->mdpage.pvh_lock);
		pve = pmap_pv_remove(pg, pmap, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
a796 1
		simple_lock(&pg->mdpage.pvh_lock);
d808 4
d813 1
a815 1
	simple_unlock(&pg->mdpage.pvh_lock);
d819 4
a822 1
	pte = TLB_PAGE(pa) | pmap_prot(pmap, prot);
d840 1
a840 1
	struct vm_page *pg;
d849 11
a859 8
	for (batch = 0, pdemask = 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
			batch = pdemask == sva && sva + ~PDE_MASK + 1 <= eva;
d885 6
d920 1
a920 1
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
d937 5
a941 3
			simple_lock(&pg->mdpage.pvh_lock);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pg->mdpage.pvh_lock);
d1138 1
a1138 1
}
d1188 1
a1188 1
	if (pa >= 0xf0000000ULL /* TODO (HPPA_IOBEGIN & HPPA_PHYSMAP) */)
d1190 2
a1191 2
	DPRINTF(PDB_ENTER, ("pmap_kenter_pa: pde %p va %lx pte %lx\n",
	    pde, va, pte));
a1194 2
	if (opte)
		pmap_pte_flush(pmap_kernel(), va, opte);
d1238 1
a1238 1
				va += ~PDE_MASK + 1 - PAGE_SIZE;
@


1.18
log
@Bring over a few more cache flushing and TLB purging fixes from hppa.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2011/07/04 17:07:27 kettenis Exp $	*/
d548 22
@


1.17
log
@Give each pmap its own space ID.  The current algorithm is silly, as we'll wrap
and might end up with duplicates, but that's not an issue until we're
multi-user.  Change the TLB miss code to enter mappings with the right
protection ID.  Properly switch pmaps on context switches.  This makes the
copyins we do before starting init actually work instead of failing with
EFAULT.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2011/05/30 22:25:21 oga Exp $	*/
d232 1
d258 1
d261 1
d263 2
a264 3
	}
	fdcache(pmap->pm_space, va, PAGE_SIZE);
	pdtlb(pmap->pm_space, va);
d1083 1
a1083 1
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
d1088 4
d1105 1
d1120 2
d1220 19
@


1.16
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2011/04/14 19:34:55 kettenis Exp $	*/
d628 1
a628 1
	pa_space_t space;
d658 1
@


1.15
log
@Get rid if pcb_uva, like we did on hppa a while ago since it creates evil
non-equivalent aliases.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2011/04/14 13:24:04 jsing Exp $	*/
d531 1
a531 2
	uvm_page_physload(0, physmem,
	    resvphysmem, eaddr, VM_FREELIST_DEFAULT);
@


1.14
log
@Ansify.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2010/07/02 22:47:54 jsing Exp $	*/
a1065 1
	pcb->pcb_uva = (vaddr_t)p->p_addr;
@


1.13
log
@Make pmap_extract() work for large pages.
@
text
@d468 1
a468 2
pmap_bootstrap(vstart)
	vaddr_t vstart;
d536 1
a536 1
pmap_init()
d625 1
a625 1
pmap_create()
d667 1
a667 2
pmap_destroy(pmap)
	struct pmap *pmap;
d706 1
a706 6
pmap_enter(pmap, va, pa, prot, flags)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d797 1
a797 4
pmap_remove(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
d854 1
a854 5
pmap_write_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
	vm_prot_t prot;
d905 1
a905 2
pmap_page_remove(pg)
	struct vm_page *pg;
d944 1
a944 3
pmap_unwire(pmap, va)
	struct pmap *pmap;
	vaddr_t	va;
d1037 1
a1037 4
pmap_extract(pmap, va, pap)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t *pap;
d1117 1
a1117 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d1162 1
a1162 3
pmap_kremove(va, size)
	vaddr_t va;
	vsize_t size;
@


1.12
log
@When we steal memory, return the VA for the memory that we have stolen and
not the memory prior to it.

Lots of help from oga@@

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2010/07/01 04:18:36 jsing Exp $	*/
d1060 1
d1069 4
a1072 2
		if (pap)
			*pap = PTE_PAGE(pte) | (va & PAGE_MASK);
@


1.11
log
@Add missing function prototypes and fix order of includes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 2010/05/24 15:06:05 deraadt Exp $	*/
d575 1
a575 1
	va = ptoa(vm_physmem[0].avail_end) - size;
@


1.10
log
@sync to hppa: Add missing prototypes
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 2010/03/31 19:46:27 miod Exp $	*/
d25 1
a26 1
#include <sys/proc.h>
d101 3
a103 1
struct vm_page	*pmap_pagealloc(struct uvm_object *obj, voff_t off);
d105 2
d113 6
@


1.9
log
@Make sure the boundaries of uvm_pglistalloc() calls are set up with low
being page-aligned, and high being end of page (i.e.
high & PAGE_MASK == PAGE_MASK) everywhere, for consistency. Future code
will depend on this.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2009/12/16 16:54:42 jasper Exp $	*/
d100 9
@


1.8
log
@zap two more handrolled equivalents of nitems().

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 2009/04/14 16:01:04 oga Exp $	*/
d623 1
a623 1
	if (uvm_pglistalloc(2 * PAGE_SIZE, 0, VM_MIN_KERNEL_ADDRESS,
@


1.7
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 2008/04/27 17:48:10 martin Exp $	*/
a395 1
#define	nhppa_pgs	sizeof(hppa_pgs)/sizeof(hppa_pgs[0])
d409 1
a409 1
	if (s < e || (s == e && s / 2 < nhppa_pgs)) {
d411 2
a412 2
		if (i > nhppa_pgs)
			i = nhppa_pgs;
d417 2
a418 2
		if (i > nhppa_pgs)
			i = nhppa_pgs;
d423 2
a424 2
		if (i > nhppa_pgs)
			i = nhppa_pgs;
@


1.6
log
@replace ctob/btoc by ptoa/atop
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 2007/04/13 18:57:49 art Exp $	*/
d625 1
a625 1
	    PAGE_SIZE, 2 * PAGE_SIZE, &pmap->pm_pglist, 1, 1))
@


1.5
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 2007/04/04 17:44:45 art Exp $	*/
d509 1
a509 1
	pmap_maphys(0x1000000, ctob(physmem));
@


1.4
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 2005/12/25 21:39:06 miod Exp $	*/
d169 1
a169 1
	pg->pg_flags &= ~PG_BUSY;		/* never busy */
d629 1
a629 1
	pg->pg_flags &= ~(PG_BUSY|PG_CLEAN);
d635 1
a635 1
	pg->pg_flags &= ~(PG_BUSY|PG_CLEAN);
@


1.3
log
@KERN_RESOURCE_SHORTAGE -> ENOMEM
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 2005/10/26 18:35:45 martin Exp $	*/
d169 1
a169 1
	pg->flags &= ~PG_BUSY;		/* never busy */
d209 1
a209 1
		if (ptp->flags & PG_BUSY)
d629 1
a629 1
	pg->flags &= ~(PG_BUSY|PG_CLEAN);
d635 1
a635 1
	pg->flags &= ~(PG_BUSY|PG_CLEAN);
@


1.2
log
@no more hppa_round_page() and hppa_trunc_page() macros

ok mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.1 2005/04/01 10:40:47 mickey Exp $	*/
d713 1
a713 1
			return (KERN_RESOURCE_SHORTAGE);
d762 1
a762 1
				return (KERN_RESOURCE_SHORTAGE);
@


1.1
log
@small batch early bottling hppa64 port
matured in mighty ukrainian oak for 23 months
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d454 1
a454 1
	vaddr_t va, eaddr, addr = hppa_round_page(vstart);
d860 1
a860 1
	sva = hppa_trunc_page(sva);
@

