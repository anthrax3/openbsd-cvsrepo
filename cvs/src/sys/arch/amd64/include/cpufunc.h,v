head	1.19;
access;
symbols
	OPENBSD_6_1:1.13.0.4
	OPENBSD_6_1_BASE:1.13
	OPENBSD_6_0:1.12.0.6
	OPENBSD_6_0_BASE:1.12
	OPENBSD_5_9:1.12.0.2
	OPENBSD_5_9_BASE:1.12
	OPENBSD_5_8:1.12.0.4
	OPENBSD_5_8_BASE:1.12
	OPENBSD_5_7:1.11.0.2
	OPENBSD_5_7_BASE:1.11
	OPENBSD_5_6:1.11.0.4
	OPENBSD_5_6_BASE:1.11
	OPENBSD_5_5:1.10.0.4
	OPENBSD_5_5_BASE:1.10
	OPENBSD_5_4:1.8.0.4
	OPENBSD_5_4_BASE:1.8
	OPENBSD_5_3:1.8.0.2
	OPENBSD_5_3_BASE:1.8
	OPENBSD_5_2:1.7.0.6
	OPENBSD_5_2_BASE:1.7
	OPENBSD_5_1_BASE:1.7
	OPENBSD_5_1:1.7.0.4
	OPENBSD_5_0:1.7.0.2
	OPENBSD_5_0_BASE:1.7
	OPENBSD_4_9:1.6.0.2
	OPENBSD_4_9_BASE:1.6
	OPENBSD_4_8:1.4.0.4
	OPENBSD_4_8_BASE:1.4
	OPENBSD_4_7:1.4.0.2
	OPENBSD_4_7_BASE:1.4
	OPENBSD_4_6:1.3.0.8
	OPENBSD_4_6_BASE:1.3
	OPENBSD_4_5:1.3.0.4
	OPENBSD_4_5_BASE:1.3
	OPENBSD_4_4:1.3.0.2
	OPENBSD_4_4_BASE:1.3
	OPENBSD_4_3:1.2.0.6
	OPENBSD_4_3_BASE:1.2
	OPENBSD_4_2:1.2.0.4
	OPENBSD_4_2_BASE:1.2
	OPENBSD_4_1:1.2.0.2
	OPENBSD_4_1_BASE:1.2
	OPENBSD_4_0:1.1.0.12
	OPENBSD_4_0_BASE:1.1
	OPENBSD_3_9:1.1.0.14
	OPENBSD_3_9_BASE:1.1
	OPENBSD_3_8:1.1.0.10
	OPENBSD_3_8_BASE:1.1
	OPENBSD_3_7:1.1.0.8
	OPENBSD_3_7_BASE:1.1
	OPENBSD_3_6:1.1.0.6
	OPENBSD_3_6_BASE:1.1
	SMP_SYNC_A:1.1
	SMP_SYNC_B:1.1
	OPENBSD_3_5:1.1.0.4
	OPENBSD_3_5_BASE:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.19
date	2017.05.30.15.11.32;	author deraadt;	state Exp;
branches;
next	1.18;
commitid	KepHUzDSsoNf5ym4;

1.18
date	2017.05.27.12.21.50;	author tedu;	state Exp;
branches;
next	1.17;
commitid	bXNHKOqIkb2LnYuD;

1.17
date	2017.05.27.10.22.50;	author tedu;	state Exp;
branches;
next	1.16;
commitid	8spPUfnlRM8lGUjr;

1.16
date	2017.05.25.03.54.10;	author visa;	state Exp;
branches;
next	1.15;
commitid	J0PLAPzKuJAH1OX8;

1.15
date	2017.04.30.13.04.49;	author mpi;	state Exp;
branches;
next	1.14;
commitid	xDPbcPU6tYP39nZG;

1.14
date	2017.04.27.06.16.39;	author mlarkin;	state Exp;
branches;
next	1.13;
commitid	ZHkhUOyGvsrTGUP9;

1.13
date	2016.09.04.09.22.28;	author mpi;	state Exp;
branches
	1.13.4.1;
next	1.12;
commitid	jBolvsPoQ0BaYiLs;

1.12
date	2015.03.21.20.42.38;	author kettenis;	state Exp;
branches;
next	1.11;
commitid	tilqEUDj6duaLWpC;

1.11
date	2014.03.29.18.09.28;	author guenther;	state Exp;
branches;
next	1.10;

1.10
date	2013.12.06.22.56.20;	author kettenis;	state Exp;
branches;
next	1.9;

1.9
date	2013.10.05.16.58.30;	author guenther;	state Exp;
branches;
next	1.8;

1.8
date	2012.12.05.23.20.10;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	2011.03.23.16.54.34;	author pirofti;	state Exp;
branches;
next	1.6;

1.6
date	2010.12.07.22.12.44;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	2010.08.19.19.31.53;	author kettenis;	state Exp;
branches;
next	1.4;

1.4
date	2009.12.09.14.28.46;	author oga;	state Exp;
branches;
next	1.3;

1.3
date	2008.06.26.05.42.09;	author ray;	state Exp;
branches;
next	1.2;

1.2
date	2007.02.17.17.35.43;	author tom;	state Exp;
branches;
next	1.1;

1.1
date	2004.01.28.01.39.39;	author mickey;	state Exp;
branches;
next	;

1.13.4.1
date	2017.05.03.02.29.16;	author jsg;	state Exp;
branches;
next	;
commitid	G8fCSoM66ex6Gjxp;


desc
@@


1.19
log
@Support for SMAP is pretty small, so don't exclude it from the RAMDISKS.
ok jsg visa
@
text
@/*	$OpenBSD: cpufunc.h,v 1.18 2017/05/27 12:21:50 tedu Exp $	*/
/*	$NetBSD: cpufunc.h,v 1.3 2003/05/08 10:27:43 fvdl Exp $	*/

/*-
 * Copyright (c) 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Charles M. Hannum.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _MACHINE_CPUFUNC_H_
#define	_MACHINE_CPUFUNC_H_

/*
 * Functions to provide access to i386-specific instructions.
 */

#include <sys/types.h>

#include <machine/specialreg.h>

#ifdef _KERNEL

extern int cpu_feature;

static __inline void 
invlpg(u_int64_t addr)
{ 
        __asm volatile("invlpg (%0)" : : "r" (addr) : "memory");
}  

static __inline void
lidt(void *p)
{
	__asm volatile("lidt (%0)" : : "r" (p) : "memory");
}

static __inline void
lldt(u_short sel)
{
	__asm volatile("lldt %0" : : "r" (sel));
}

static __inline void
ltr(u_short sel)
{
	__asm volatile("ltr %0" : : "r" (sel));
}

static __inline void
lcr8(u_int val)
{
	u_int64_t val64 = val;
	__asm volatile("movq %0,%%cr8" : : "r" (val64));
}

/*
 * Upper 32 bits are reserved anyway, so just keep this 32bits.
 */
static __inline void
lcr0(u_int val)
{
	u_int64_t val64 = val;
	__asm volatile("movq %0,%%cr0" : : "r" (val64));
}

static __inline u_int
rcr0(void)
{
	u_int64_t val64;
	u_int val;
	__asm volatile("movq %%cr0,%0" : "=r" (val64));
	val = val64;
	return val;
}

static __inline u_int64_t
rcr2(void)
{
	u_int64_t val;
	__asm volatile("movq %%cr2,%0" : "=r" (val));
	return val;
}

static __inline void
lcr3(u_int64_t val)
{
	__asm volatile("movq %0,%%cr3" : : "r" (val));
}

static __inline u_int64_t
rcr3(void)
{
	u_int64_t val;
	__asm volatile("movq %%cr3,%0" : "=r" (val));
	return val;
}

/*
 * Same as for cr0. Don't touch upper 32 bits.
 */
static __inline void
lcr4(u_int val)
{
	u_int64_t val64 = val;

	__asm volatile("movq %0,%%cr4" : : "r" (val64));
}

static __inline u_int
rcr4(void)
{
	u_int64_t val64;
	__asm volatile("movq %%cr4,%0" : "=r" (val64));
	return (u_int) val64;
}

static __inline void
tlbflush(void)
{
	u_int64_t val;
	__asm volatile("movq %%cr3,%0" : "=r" (val));
	__asm volatile("movq %0,%%cr3" : : "r" (val));
}

#ifdef notyet
void	setidt(int idx, /*XXX*/caddr_t func, int typ, int dpl);
#endif


/* XXXX ought to be in psl.h with spl() functions */

static __inline void
disable_intr(void)
{
	__asm volatile("cli");
}

static __inline void
enable_intr(void)
{
	__asm volatile("sti");
}

static __inline u_long
read_rflags(void)
{
	u_long	ef;

	__asm volatile("pushfq; popq %0" : "=r" (ef));
	return (ef);
}

static __inline void
write_rflags(u_long ef)
{
	__asm volatile("pushq %0; popfq" : : "r" (ef));
}

static __inline u_long
intr_disable(void)
{
	u_long ef;

	ef = read_rflags();
	disable_intr();
	return (ef);
}

static __inline void
intr_restore(u_long ef)
{
	write_rflags(ef);
}

static __inline u_int64_t
rdmsr(u_int msr)
{
	uint32_t hi, lo;
	__asm volatile("rdmsr" : "=d" (hi), "=a" (lo) : "c" (msr));
	return (((uint64_t)hi << 32) | (uint64_t) lo);
}

static __inline void
wrmsr(u_int msr, u_int64_t newval)
{
	__asm volatile("wrmsr" :
	    : "a" (newval & 0xffffffff), "d" (newval >> 32), "c" (msr));
}

/* 
 * Some of the undocumented AMD64 MSRs need a 'passcode' to access.
 *
 * See LinuxBIOSv2: src/cpu/amd/model_fxx/model_fxx_init.c
 */

#define	OPTERON_MSR_PASSCODE	0x9c5a203a
 
static __inline u_int64_t
rdmsr_locked(u_int msr, u_int code)
{
	uint32_t hi, lo;
	__asm volatile("rdmsr"
	    : "=d" (hi), "=a" (lo)
	    : "c" (msr), "D" (code));
	return (((uint64_t)hi << 32) | (uint64_t) lo);
}

static __inline void
wrmsr_locked(u_int msr, u_int code, u_int64_t newval)
{
	__asm volatile("wrmsr" :
	    : "a" (newval & 0xffffffff), "d" (newval >> 32), "c" (msr), "D" (code));
}

static __inline void
wbinvd(void)
{
	__asm volatile("wbinvd");
}

static __inline void
clflush(u_int64_t addr)
{
	__asm volatile("clflush %0" : "+m" (*(volatile char *)addr));
}

static __inline void
mfence(void)
{
	__asm volatile("mfence" : : : "memory");
}

static __inline u_int64_t
rdtsc(void)
{
	uint32_t hi, lo;

	__asm volatile("rdtsc" : "=d" (hi), "=a" (lo));
	return (((uint64_t)hi << 32) | (uint64_t) lo);
}

static __inline u_int64_t
rdpmc(u_int pmc)
{
	uint32_t hi, lo;

	__asm volatile("rdpmc" : "=d" (hi), "=a" (lo) : "c" (pmc));
	return (((uint64_t)hi << 32) | (uint64_t) lo);
}

static __inline void
monitor(const volatile void *addr, u_long extensions, u_int hints)
{

	__asm volatile("monitor"
	    : : "a" (addr), "c" (extensions), "d" (hints));
}

static __inline void
mwait(u_long extensions, u_int hints)
{

	__asm volatile("mwait" : : "a" (hints), "c" (extensions));
}

static __inline void
xsetbv(uint32_t reg, uint64_t mask)
{
	uint32_t lo, hi;

	lo = mask;
	hi = mask >> 32;
	__asm volatile("xsetbv" :: "c" (reg), "a" (lo), "d" (hi) : "memory");
}

static __inline uint64_t
xgetbv(uint32_t reg)
{
	uint32_t lo, hi;

	__asm volatile("xgetbv" : "=a" (lo), "=d" (hi) : "c" (reg));

	return (((uint64_t)hi << 32) | (uint64_t)lo);
}

/* Break into DDB. */
static __inline void
breakpoint(void)
{
	__asm volatile("int $3");
}

#define read_psl()	read_rflags()
#define write_psl(x)	write_rflags(x)

void amd64_errata(struct cpu_info *);

#endif /* _KERNEL */

#endif /* !_MACHINE_CPUFUNC_H_ */
@


1.18
log
@manually inline tlbflushg. it's short and there's only one caller.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.17 2017/05/27 10:22:50 tedu Exp $	*/
a134 1
	u_int val;
d137 1
a137 2
	val = val64;
	return val;
@


1.17
log
@there shouldn't be any need to check for PGE on amd64. ok deraadt mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.16 2017/05/25 03:54:10 visa Exp $	*/
a147 13
}

static __inline void
tlbflushg(void)
{
	/*
	 * Big hammer: flush all TLB entries, including ones from PTE's
	 * with the G bit set.  This should only be necessary if TLB
	 * shootdown falls far behind.
	 */
	u_int cr4 = rcr4();
	lcr4(cr4 & ~CR4_PGE);
	lcr4(cr4);
@


1.16
log
@Replace the only usage of x86_pause() with SPINLOCK_SPIN_HOOK.

OK dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.15 2017/04/30 13:04:49 mpi Exp $	*/
a156 14
	 *
	 * Intel Architecture Software Developer's Manual, Volume 3,
	 *	System Programming, section 9.10, "Invalidating the
	 * Translation Lookaside Buffers (TLBS)":
	 * "The following operations invalidate all TLB entries, irrespective
	 * of the setting of the G flag:
	 * ...
	 * "(P6 family processors only): Writing to control register CR4 to
	 * modify the PSE, PGE, or PAE flag."
	 *
	 * (the alternatives not quoted above are not an option here.)
	 *
	 * If PGE is not in use, we reload CR3 for the benefit of
	 * pre-P6-family processors.
d158 3
a160 7

	if (cpu_feature & CPUID_PGE) {
		u_int cr4 = rcr4();
		lcr4(cr4 & ~CR4_PGE);
		lcr4(cr4);
	} else
		tlbflush();
@


1.15
log
@Unifdef KGDB.

It doesn't compile und hasn't been working during the last decade.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.14 2017/04/27 06:16:39 mlarkin Exp $	*/
a42 6

static __inline void
x86_pause(void)
{
	/* nothing */
}
@


1.14
log
@vmm(4): proper save/restore of FPU context during entry/exit.

tested by reyk, dcoppa, and a few others.

ok kettenis@@ on the fpu bits
ok deraadt@@ on the vmm bits
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.13 2016/09/04 09:22:28 mpi Exp $	*/
d348 1
a348 1
/* Break into DDB/KGDB. */
@


1.13
log
@Introduce Dynamic Profiling, a ddb(4) based & gprof compatible kernel
profiling framework.

Code patching is used to enable probes when entering functions.  The
probes will call a mcount()-like function to match the behavior of a
GPROF kernel.

Currently only available on amd64 and guarded under DDBPROF.  Support
for other archs will follow soon.

A new sysctl knob, ddb.console, need to be set to 1 in securelevel 0
to be able to use this feature.

Inputs and ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.12 2015/03/21 20:42:38 kettenis Exp $	*/
d336 10
@


1.13.4.1
log
@OpenBSD 6.1 errata 002, May 2, 2017

vmm(4) mismanaged floating point contexts.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.13 2016/09/04 09:22:28 mpi Exp $	*/
a335 10
}

static __inline uint64_t
xgetbv(uint32_t reg)
{
	uint32_t lo, hi;

	__asm volatile("xgetbv" : "=a" (lo), "=d" (hi) : "c" (reg));

	return (((uint64_t)hi << 32) | (uint64_t)lo);
@


1.12
log
@Add support for saving/restoring FPU state using the XSAVE/XRSTOR.  Limit
support to the X87, SSE and AVX state.

This gives us (almost) full AVX support.  The AVX state isn't saved by
signal handlers yet, and ptrace(2) support is still missing.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.11 2014/03/29 18:09:28 guenther Exp $	*/
d219 16
@


1.11
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.10 2013/12/06 22:56:20 kettenis Exp $	*/
d310 10
@


1.10
log
@Make clflush() flush the cache line specified by the address we pass it
instead of the cache line containing the local variable used to specify the
address.  Fixes the gnome corruption and hangs people have been experiencing
for the last couple of months or so.

ok deraadt@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.9 2013/10/05 16:58:30 guenther Exp $	*/
d57 1
a57 1
        __asm __volatile("invlpg (%0)" : : "r" (addr) : "memory");
d63 1
a63 1
	__asm __volatile("lidt (%0)" : : "r" (p) : "memory");
d69 1
a69 1
	__asm __volatile("lldt %0" : : "r" (sel));
d75 1
a75 1
	__asm __volatile("ltr %0" : : "r" (sel));
d82 1
a82 1
	__asm __volatile("movq %0,%%cr8" : : "r" (val64));
d92 1
a92 1
	__asm __volatile("movq %0,%%cr0" : : "r" (val64));
d100 1
a100 1
	__asm __volatile("movq %%cr0,%0" : "=r" (val64));
d109 1
a109 1
	__asm __volatile("movq %%cr2,%0" : "=r" (val));
d116 1
a116 1
	__asm __volatile("movq %0,%%cr3" : : "r" (val));
d123 1
a123 1
	__asm __volatile("movq %%cr3,%0" : "=r" (val));
d135 1
a135 1
	__asm __volatile("movq %0,%%cr4" : : "r" (val64));
d143 1
a143 1
	__asm __volatile("movq %%cr4,%0" : "=r" (val64));
d152 2
a153 2
	__asm __volatile("movq %%cr3,%0" : "=r" (val));
	__asm __volatile("movq %0,%%cr3" : : "r" (val));
d197 1
a197 1
	__asm __volatile("cli");
d203 1
a203 1
	__asm __volatile("sti");
d211 1
a211 1
	__asm __volatile("pushfq; popq %0" : "=r" (ef));
d218 1
a218 1
	__asm __volatile("pushq %0; popfq" : : "r" (ef));
d225 1
a225 1
	__asm __volatile("rdmsr" : "=d" (hi), "=a" (lo) : "c" (msr));
d232 1
a232 1
	__asm __volatile("wrmsr" :
d248 1
a248 1
	__asm __volatile("rdmsr"
d257 1
a257 1
	__asm __volatile("wrmsr" :
d264 1
a264 1
	__asm __volatile("wbinvd");
d270 1
a270 1
	__asm __volatile("clflush %0" : "+m" (*(volatile char *)addr));
d276 1
a276 1
	__asm __volatile("mfence" : : : "memory");
d284 1
a284 1
	__asm __volatile("rdtsc" : "=d" (hi), "=a" (lo));
d293 1
a293 1
	__asm __volatile("rdpmc" : "=d" (hi), "=a" (lo) : "c" (pmc));
d301 1
a301 1
	__asm __volatile("monitor"
d309 1
a309 1
	__asm __volatile("mwait" : : "a" (hints), "c" (extensions));
d316 1
a316 1
	__asm __volatile("int $3");
@


1.9
log
@Use monitor/mwait to idle when available.  Make cpu_unidle() do nothing
if can tell the target CPU isn't actually idling and introduce cpu_kick()
for the cases where we want to force a non-idle CPU into the kernel.

critical review of early versions by weingart@@; testing by many
ok haesbaert@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.8 2012/12/05 23:20:10 deraadt Exp $	*/
d270 1
a270 1
	__asm __volatile("clflush %0" : "+m" (addr));
@


1.8
log
@Remove excessive sys/cdefs.h inclusion
ok guenther millert kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.7 2011/03/23 16:54:34 pirofti Exp $	*/
d295 15
@


1.7
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.6 2010/12/07 22:12:44 deraadt Exp $	*/
a39 1
#include <sys/cdefs.h>
@


1.6
log
@The rdmsr_locked/wrmsr_locked functions were using "=A" as a constraint for
the 64-bit output/input.  On i386, this means that the 64-bit value is in
eax:edx, but on amd64 gcc this is not the case (might be a gcc bug, or might
be intentionally different and annoying?).  The consequence is that amd64
errata were not always being matched (and then "corrected" using the magic
repair code).  We need to compose/decompose the 64-bit value like the
un-locked msr functions do.
originally pointed out by ragge, ok kettenis jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.5 2010/08/19 19:31:53 kettenis Exp $	*/
d33 2
a34 2
#ifndef _AMD64_CPUFUNC_H_
#define	_AMD64_CPUFUNC_H_
d312 1
a312 1
#endif /* !_AMD64_CPUFUNC_H_ */
@


1.5
log
@Add "memory" clobber to lidt inline asm, to prevent the GCC optimizer from
getting stupid ideas like optimizing away stores to the descriptor that we're
setting.  This may be overkill, but this code is far from performance
critical and it may prevent future surprises.  Fixes instant reboots
with bsd.rd on Pentiums with the F00F bug.

Thanks to espie@@, for narrowing the issue down enough for me to find the
problem.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.4 2009/12/09 14:28:46 oga Exp $	*/
d248 3
a250 3
	uint64_t rv;
	__asm volatile("rdmsr"
	    : "=A" (rv)
d252 1
a252 1
	return (rv);
d258 2
a259 3
	__asm volatile("wrmsr"
	    :
	    : "A" (newval), "c" (msr), "D" (code));
@


1.4
log
@add cpufunc functions for the clflush instruction and the mfence
instruction.

ok kettenis@@ as part of a larger diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.3 2008/06/26 05:42:09 ray Exp $	*/
d64 1
a64 1
	__asm __volatile("lidt (%0)" : : "r" (p));
@


1.3
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.2 2007/02/17 17:35:43 tom Exp $	*/
d267 12
@


1.2
log
@Add code to check for the AMD amd64 errata, and correct them where
possible.  Taken from NetBSD.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cpufunc.h,v 1.1 2004/01/28 01:39:39 mickey Exp $	*/
a18 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *        This product includes software developed by the NetBSD
 *        Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.1
log
@an amd64 arch support.
hacked by art@@ from netbsd sources and then later debugged
by me into the shape where it can host itself.
no bootloader yet as needs redoing from the
recent advanced i386 sources (anyone? ;)
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d244 26
d303 2
@

