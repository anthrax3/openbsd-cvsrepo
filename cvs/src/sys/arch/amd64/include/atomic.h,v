head	1.20;
access;
symbols
	OPENBSD_6_2:1.20.0.4
	OPENBSD_6_2_BASE:1.20
	OPENBSD_6_1:1.18.0.4
	OPENBSD_6_1_BASE:1.18
	OPENBSD_6_0:1.17.0.8
	OPENBSD_6_0_BASE:1.17
	OPENBSD_5_9:1.17.0.4
	OPENBSD_5_9_BASE:1.17
	OPENBSD_5_8:1.17.0.6
	OPENBSD_5_8_BASE:1.17
	OPENBSD_5_7:1.17.0.2
	OPENBSD_5_7_BASE:1.17
	OPENBSD_5_6:1.13.0.4
	OPENBSD_5_6_BASE:1.13
	OPENBSD_5_5:1.10.0.4
	OPENBSD_5_5_BASE:1.10
	OPENBSD_5_4:1.9.0.4
	OPENBSD_5_4_BASE:1.9
	OPENBSD_5_3:1.9.0.2
	OPENBSD_5_3_BASE:1.9
	OPENBSD_5_2:1.8.0.6
	OPENBSD_5_2_BASE:1.8
	OPENBSD_5_1_BASE:1.8
	OPENBSD_5_1:1.8.0.4
	OPENBSD_5_0:1.8.0.2
	OPENBSD_5_0_BASE:1.8
	OPENBSD_4_9:1.7.0.2
	OPENBSD_4_9_BASE:1.7
	OPENBSD_4_8:1.6.0.14
	OPENBSD_4_8_BASE:1.6
	OPENBSD_4_7:1.6.0.10
	OPENBSD_4_7_BASE:1.6
	OPENBSD_4_6:1.6.0.12
	OPENBSD_4_6_BASE:1.6
	OPENBSD_4_5:1.6.0.8
	OPENBSD_4_5_BASE:1.6
	OPENBSD_4_4:1.6.0.6
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.6.0.4
	OPENBSD_4_3_BASE:1.6
	OPENBSD_4_2:1.6.0.2
	OPENBSD_4_2_BASE:1.6
	OPENBSD_4_1:1.5.0.2
	OPENBSD_4_1_BASE:1.5
	OPENBSD_4_0:1.3.0.10
	OPENBSD_4_0_BASE:1.3
	OPENBSD_3_9:1.3.0.12
	OPENBSD_3_9_BASE:1.3
	OPENBSD_3_8:1.3.0.8
	OPENBSD_3_8_BASE:1.3
	OPENBSD_3_7:1.3.0.6
	OPENBSD_3_7_BASE:1.3
	OPENBSD_3_6:1.3.0.4
	OPENBSD_3_6_BASE:1.3
	SMP_SYNC_A:1.3
	SMP_SYNC_B:1.3
	OPENBSD_3_5:1.3.0.2
	OPENBSD_3_5_BASE:1.3
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.20
date	2017.05.27.20.12.12;	author kettenis;	state Exp;
branches;
next	1.19;
commitid	wnLzXin7FL6GumIs;

1.19
date	2017.05.12.08.47.03;	author mpi;	state Exp;
branches;
next	1.18;
commitid	mIca02cRX9qZT1Jj;

1.18
date	2016.08.17.11.56.42;	author kettenis;	state Exp;
branches;
next	1.17;
commitid	0tTTCo8nkm1xFGwi;

1.17
date	2015.01.06.00.38.32;	author dlg;	state Exp;
branches;
next	1.16;
commitid	W3kfFzaKNOKHuHue;

1.16
date	2014.10.08.19.40.28;	author sf;	state Exp;
branches;
next	1.15;
commitid	tjRVuxiDN7VnYLrQ;

1.15
date	2014.09.07.22.19.32;	author kettenis;	state Exp;
branches;
next	1.14;
commitid	hHm0PAJnyC3a2NMx;

1.14
date	2014.09.01.03.39.15;	author guenther;	state Exp;
branches;
next	1.13;
commitid	6O8Ub2HDNAEjeQwh;

1.13
date	2014.07.18.10.40.14;	author dlg;	state Exp;
branches
	1.13.4.1;
next	1.12;
commitid	GoJdpESN6XUgaZcc;

1.12
date	2014.03.29.18.09.28;	author guenther;	state Exp;
branches;
next	1.11;

1.11
date	2014.03.27.10.24.40;	author dlg;	state Exp;
branches;
next	1.10;

1.10
date	2014.02.17.10.01.32;	author dlg;	state Exp;
branches
	1.10.4.1;
next	1.9;

1.9
date	2012.11.19.15.18.06;	author pirofti;	state Exp;
branches;
next	1.8;

1.8
date	2011.03.23.16.54.34;	author pirofti;	state Exp;
branches;
next	1.7;

1.7
date	2010.12.27.20.22.23;	author guenther;	state Exp;
branches;
next	1.6;

1.6
date	2007.05.25.16.22.11;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2007.02.19.17.18.42;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2007.02.06.17.13.33;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2004.02.23.06.25.14;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2004.02.23.05.57.08;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	2004.01.28.01.39.39;	author mickey;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2004.06.05.23.09.25;	author niklas;	state Exp;
branches;
next	;

1.10.4.1
date	2014.12.09.13.03.17;	author sf;	state Exp;
branches;
next	;
commitid	ux5h60hA6HQhMlAQ;

1.13.4.1
date	2014.12.09.12.53.03;	author sf;	state Exp;
branches;
next	;
commitid	j4gSsn64Ru5hB56g;


desc
@@


1.20
log
@On i386 and amd64, atomic instructions include an implicit memory barrier.

ok mikeb@@, visa@@, mpi@@
@
text
@/*	$OpenBSD: atomic.h,v 1.19 2017/05/12 08:47:03 mpi Exp $	*/
/*	$NetBSD: atomic.h,v 1.1 2003/04/26 18:39:37 fvdl Exp $	*/

/*
 * Copyright 2002 (c) Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Frank van der Linden for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed for the NetBSD Project by
 *      Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _MACHINE_ATOMIC_H_
#define _MACHINE_ATOMIC_H_

/*
 * Perform atomic operations on memory. Should be atomic with respect
 * to interrupts and multiple processors.
 *
 * void atomic_setbits_int(volatile u_int *a, u_int mask) { *a |= mask; }
 * void atomic_clearbits_int(volatile u_int *a, u_int mas) { *a &= ~mask; }
 */

#if !defined(_LOCORE)

#if defined(MULTIPROCESSOR) || !defined(_KERNEL)
#define _LOCK "lock"
#else
#define _LOCK
#endif

static inline unsigned int
_atomic_cas_uint(volatile unsigned int *p, unsigned int e, unsigned int n)
{
	__asm volatile(_LOCK " cmpxchgl %2, %1"
	    : "=a" (n), "=m" (*p)
	    : "r" (n), "a" (e), "m" (*p));

	return (n);
}
#define atomic_cas_uint(_p, _e, _n) _atomic_cas_uint((_p), (_e), (_n))

static inline unsigned long
_atomic_cas_ulong(volatile unsigned long *p, unsigned long e, unsigned long n)
{
	__asm volatile(_LOCK " cmpxchgq %2, %1"
	    : "=a" (n), "=m" (*p)
	    : "r" (n), "a" (e), "m" (*p));

	return (n);
}
#define atomic_cas_ulong(_p, _e, _n) _atomic_cas_ulong((_p), (_e), (_n))

static inline void *
_atomic_cas_ptr(volatile void *p, void *e, void *n)
{
	__asm volatile(_LOCK " cmpxchgq %2, %1"
	    : "=a" (n), "=m" (*(unsigned long *)p)
	    : "r" (n), "a" (e), "m" (*(unsigned long *)p));

	return (n);
}
#define atomic_cas_ptr(_p, _e, _n) _atomic_cas_ptr((_p), (_e), (_n))

static inline unsigned int
_atomic_swap_uint(volatile unsigned int *p, unsigned int n)
{
	__asm volatile("xchgl %0, %1"
	    : "=a" (n), "=m" (*p)
	    : "0" (n), "m" (*p));

	return (n);
}
#define atomic_swap_uint(_p, _n) _atomic_swap_uint((_p), (_n))
#define atomic_swap_32(_p, _n) _atomic_swap_uint((_p), (_n))

static inline unsigned long
_atomic_swap_ulong(volatile unsigned long *p, unsigned long n)
{
	__asm volatile("xchgq %0, %1"
	    : "=a" (n), "=m" (*p)
	    : "0" (n), "m" (*p));

	return (n);
}
#define atomic_swap_ulong(_p, _n) _atomic_swap_ulong((_p), (_n))

static inline uint64_t
_atomic_swap_64(volatile uint64_t *p, uint64_t n)
{
	__asm volatile("xchgq %0, %1"
	    : "=a" (n), "=m" (*p)
	    : "0" (n), "m" (*p));

	return (n);
}
#define atomic_swap_64(_p, _n) _atomic_swap_64((_p), (_n))

static inline void *
_atomic_swap_ptr(volatile void *p, void *n)
{
	__asm volatile("xchgq %0, %1"
	    : "=a" (n), "=m" (*(unsigned long *)p)
	    : "0" (n), "m" (*(unsigned long *)p));

	return (n);
}
#define atomic_swap_ptr(_p, _n) _atomic_swap_ptr((_p), (_n))

static inline void
_atomic_inc_int(volatile unsigned int *p)
{
	__asm volatile(_LOCK " incl %0"
	    : "+m" (*p));
}
#define atomic_inc_int(_p) _atomic_inc_int(_p)

static inline void
_atomic_inc_long(volatile unsigned long *p)
{
	__asm volatile(_LOCK " incq %0"
	    : "+m" (*p));
}
#define atomic_inc_long(_p) _atomic_inc_long(_p)

static inline void
_atomic_dec_int(volatile unsigned int *p)
{
	__asm volatile(_LOCK " decl %0"
	    : "+m" (*p));
}
#define atomic_dec_int(_p) _atomic_dec_int(_p)

static inline void
_atomic_dec_long(volatile unsigned long *p)
{
	__asm volatile(_LOCK " decq %0"
	    : "+m" (*p));
}
#define atomic_dec_long(_p) _atomic_dec_long(_p)

static inline void
_atomic_add_int(volatile unsigned int *p, unsigned int v)
{
	__asm volatile(_LOCK " addl %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_add_int(_p, _v) _atomic_add_int(_p, _v)

static inline void
_atomic_add_long(volatile unsigned long *p, unsigned long v)
{
	__asm volatile(_LOCK " addq %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_add_long(_p, _v) _atomic_add_long(_p, _v)

static inline void
_atomic_sub_int(volatile unsigned int *p, unsigned int v)
{
	__asm volatile(_LOCK " subl %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_sub_int(_p, _v) _atomic_sub_int(_p, _v)

static inline void
_atomic_sub_long(volatile unsigned long *p, unsigned long v)
{
	__asm volatile(_LOCK " subq %1,%0"
	    : "+m" (*p)
	    : "a" (v));
}
#define atomic_sub_long(_p, _v) _atomic_sub_long(_p, _v)


static inline unsigned long
_atomic_add_int_nv(volatile unsigned int *p, unsigned int v)
{
	unsigned int rv = v;

	__asm volatile(_LOCK " xaddl %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv + v);
}
#define atomic_add_int_nv(_p, _v) _atomic_add_int_nv(_p, _v)

static inline unsigned long
_atomic_add_long_nv(volatile unsigned long *p, unsigned long v)
{
	unsigned long rv = v;

	__asm volatile(_LOCK " xaddq %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv + v);
}
#define atomic_add_long_nv(_p, _v) _atomic_add_long_nv(_p, _v)

static inline unsigned long
_atomic_sub_int_nv(volatile unsigned int *p, unsigned int v)
{
	unsigned int rv = 0 - v;

	__asm volatile(_LOCK " xaddl %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv - v);
}
#define atomic_sub_int_nv(_p, _v) _atomic_sub_int_nv(_p, _v)

static inline unsigned long
_atomic_sub_long_nv(volatile unsigned long *p, unsigned long v)
{
	unsigned long rv = 0 - v;

	__asm volatile(_LOCK " xaddq %0,%1"
	    : "+a" (rv), "+m" (*p));

	return (rv - v);
}
#define atomic_sub_long_nv(_p, _v) _atomic_sub_long_nv(_p, _v)

/*
 * The AMD64 architecture is rather strongly ordered.  When accessing
 * normal write-back cachable memory, only reads may be reordered with
 * older writes to different locations.  There are a few instructions
 * (clfush, non-temporal move instructions) that obey weaker ordering
 * rules, but those instructions will only be used in (inline)
 * assembly code where we can add the necessary fence instructions
 * ourselves.
 */

#define __membar(_f) do { __asm __volatile(_f ::: "memory"); } while (0)

#if defined(MULTIPROCESSOR) || !defined(_KERNEL)
#define membar_enter()		__membar("mfence")
#define membar_exit()		__membar("")
#define membar_producer()	__membar("")
#define membar_consumer()	__membar("")
#define membar_sync()		__membar("mfence")
#else
#define membar_enter()		__membar("")
#define membar_exit()		__membar("")
#define membar_producer()	__membar("")
#define membar_consumer()	__membar("")
#define membar_sync()		__membar("")
#endif

#define membar_enter_after_atomic()	__membar("")
#define membar_exit_before_atomic()	__membar("")

#ifdef _KERNEL

/* virtio needs MP membars even on SP kernels */
#define virtio_membar_producer()	__membar("")
#define virtio_membar_consumer()	__membar("")
#define virtio_membar_sync()		__membar("mfence")

static __inline void
x86_atomic_setbits_u32(volatile u_int32_t *ptr, u_int32_t bits)
{
	__asm volatile(_LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
}

static __inline void
x86_atomic_clearbits_u32(volatile u_int32_t *ptr, u_int32_t bits)
{
	__asm volatile(_LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (~bits));
}

static __inline void
x86_atomic_setbits_u64(volatile u_int64_t *ptr, u_int64_t bits)
{
	__asm volatile(_LOCK " orq %1,%0" :  "=m" (*ptr) : "er" (bits));
}

static __inline void
x86_atomic_clearbits_u64(volatile u_int64_t *ptr, u_int64_t bits)
{
	__asm volatile(_LOCK " andq %1,%0" :  "=m" (*ptr) : "er" (~bits));
}

#define x86_atomic_testset_ul	x86_atomic_testset_u64
#define x86_atomic_setbits_ul	x86_atomic_setbits_u64
#define x86_atomic_clearbits_ul	x86_atomic_clearbits_u64

#define atomic_setbits_int x86_atomic_setbits_u32
#define atomic_clearbits_int x86_atomic_clearbits_u32

#endif /* _KERNEL */

#undef _LOCK

#endif /* !defined(_LOCORE) */
#endif /* _MACHINE_ATOMIC_H_ */
@


1.19
log
@Make atomic.h ready to be included in userland.

- prefix the LOCK macro with an underscore
- keep setbits/clearbits and virtio barriers inside _KERNEL

ok dlg@@, kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.18 2016/08/17 11:56:42 kettenis Exp $	*/
d278 3
@


1.18
log
@Fix x86_atomic_{set|clear}bits_u64() by using the "er" constraint instead
of "ir" as the orq and andq instructions take a 32-bit immedate argument that
gets sign-extended.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.17 2015/01/06 00:38:32 dlg Exp $	*/
d50 1
a50 1
#if defined(_KERNEL) && !defined(_LOCORE)
d52 2
a53 2
#ifdef MULTIPROCESSOR
#define LOCK "lock"
d55 1
a55 1
#define LOCK
d61 1
a61 1
	__asm volatile(LOCK " cmpxchgl %2, %1"
d72 1
a72 1
	__asm volatile(LOCK " cmpxchgq %2, %1"
d83 1
a83 1
	__asm volatile(LOCK " cmpxchgq %2, %1"
d139 1
a139 1
	__asm volatile(LOCK " incl %0"
d147 1
a147 1
	__asm volatile(LOCK " incq %0"
d155 1
a155 1
	__asm volatile(LOCK " decl %0"
d163 1
a163 1
	__asm volatile(LOCK " decq %0"
d171 1
a171 1
	__asm volatile(LOCK " addl %1,%0"
d180 1
a180 1
	__asm volatile(LOCK " addq %1,%0"
d189 1
a189 1
	__asm volatile(LOCK " subl %1,%0"
d198 1
a198 1
	__asm volatile(LOCK " subq %1,%0"
d210 1
a210 1
	__asm volatile(LOCK " xaddl %0,%1"
d222 1
a222 1
	__asm volatile(LOCK " xaddq %0,%1"
d234 1
a234 1
	__asm volatile(LOCK " xaddl %0,%1"
d246 1
a246 1
	__asm volatile(LOCK " xaddq %0,%1"
d265 1
a265 1
#ifdef MULTIPROCESSOR
d279 2
d289 1
a289 1
	__asm volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d295 1
a295 1
	__asm volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (~bits));
d301 1
a301 1
	__asm volatile(LOCK " orq %1,%0" :  "=m" (*ptr) : "er" (bits));
d307 1
a307 1
	__asm volatile(LOCK " andq %1,%0" :  "=m" (*ptr) : "er" (~bits));
d317 3
a319 1
#undef LOCK
d321 1
a321 1
#endif /* defined(_KERNEL) && !defined(_LOCORE) */
@


1.17
log
@implement atomic_swap_{uint,ulong,ptr) and some md variants. use these
to replace x86_atomic_testset_{u32,u64}.

help from guenther@@ kettenis@@
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.16 2014/10/08 19:40:28 sf Exp $	*/
a295 7
/*
 * XXX XXX XXX
 * theoretically 64bit cannot be used as
 * an "i" and thus if we ever try to give
 * these anything from the high dword there
 * is an asm error pending
 */
d299 1
a299 1
	__asm volatile(LOCK " orq %1,%0" :  "=m" (*ptr) : "ir" (bits));
d305 1
a305 1
	__asm volatile(LOCK " andq %1,%0" :  "=m" (*ptr) : "ir" (~bits));
@


1.16
log
@Add virtio_membar_* macros

Virtio needs the MP memory barriers even on SP kernels.

OK kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.15 2014/09/07 22:19:32 kettenis Exp $	*/
d91 45
a282 15

static __inline u_int64_t
x86_atomic_testset_u64(volatile u_int64_t *ptr, u_int64_t val)
{
	__asm__ volatile ("xchgq %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
}

static __inline u_int32_t
x86_atomic_testset_u32(volatile u_int32_t *ptr, u_int32_t val)
{
	__asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
}

@


1.15
log
@Implement membar(9) API for amd64.

ok dlg@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.14 2014/09/01 03:39:15 guenther Exp $	*/
d233 5
@


1.14
log
@Add implementations of atomic_{inc,dec,add,sub}_{int,long}() and
atomic_{add,sub}_{int,long}_nv(), based on Solaris and translated
through the mind of dlg@@

Remove the 'memory' constraint from atomic_cas_{uint,ulong,ptr}()
now that rw_{enter,exit}*() use membars.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.13 2014/07/18 10:40:14 dlg Exp $	*/
d207 26
@


1.13
log
@pass atomic_{cas,swap}_uint a volatile void * instead of a volatile
void **. the latter is really hard to cast for, and not what what
solaris does.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.12 2014/03/29 18:09:28 guenther Exp $	*/
d63 1
a63 2
	    : "r" (n), "a" (e), "m" (*p)
	    : "memory");
d74 1
a74 2
	    : "r" (n), "a" (e), "m" (*p)
	    : "memory");
d85 1
a85 2
	    : "r" (n), "a" (e), "m" (*(unsigned long *)p)
	    : "memory");
d90 117
@


1.13.4.1
log
@Backport arch/amd64/include/atomic.h 1.16
         arch/i386/include/atomic.h  1.14

Add virtio_membar_* macros

Virtio needs the MP memory barriers even on SP kernels.

no objections kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.13 2014/07/18 10:40:14 dlg Exp $	*/
a56 7

#define __membar(_f) do { __asm __volatile(_f ::: "memory"); } while (0)

/* virtio needs MP membars even on SP kernels */
#define virtio_membar_producer()	__membar("")
#define virtio_membar_consumer()	__membar("")
#define virtio_membar_sync()		__membar("mfence")
@


1.12
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.11 2014/03/27 10:24:40 dlg Exp $	*/
d83 1
a83 1
_atomic_cas_ptr(volatile void **p, void *e, void *n)
d86 2
a87 2
	    : "=a" (n), "=m" (*p)
	    : "r" (n), "a" (e), "m" (*p)
@


1.11
log
@replace x86_atomic_cas_things with atomic_cas_foo equivalents.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.10 2014/02/17 10:01:32 dlg Exp $	*/
d61 1
a61 1
	__asm __volatile(LOCK " cmpxchgl %2, %1"
d73 1
a73 1
	__asm __volatile(LOCK " cmpxchgq %2, %1"
d85 1
a85 1
	__asm __volatile(LOCK " cmpxchgq %2, %1"
d112 1
a112 1
	__asm __volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d118 1
a118 1
	__asm __volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (~bits));
d131 1
a131 1
	__asm __volatile(LOCK " orq %1,%0" :  "=m" (*ptr) : "ir" (bits));
d137 1
a137 1
	__asm __volatile(LOCK " andq %1,%0" :  "=m" (*ptr) : "ir" (~bits));
@


1.10
log
@x86_atomic_testset_i32 and x86_atomic_testset_i are not used, so remove
them to unmuddy the waters a bit.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.9 2012/11/19 15:18:06 pirofti Exp $	*/
d58 36
a118 22
}

static __inline int
x86_atomic_cas_int32(volatile int32_t *ptr, int32_t expect, int32_t set)
{
	int res;

	__asm volatile(LOCK " cmpxchgl %2, %1" : "=a" (res), "=m" (*ptr)
	    : "r" (set), "a" (expect), "m" (*ptr) : "memory");

	return (res);
}

static __inline u_long
x86_atomic_cas_ul(volatile u_long *ptr, u_long expect, u_long set)
{
	u_long res;

	__asm volatile(LOCK " cmpxchgq %2, %1" : "=a" (res), "=m" (*ptr)
	    : "r" (set), "a" (expect), "m" (*ptr) : "memory");

	return (res);
@


1.10.4.1
log
@Backport arch/amd64/include/atomic.h 1.16
         arch/i386/include/atomic.h  1.14

Add virtio_membar_* macros

Virtio needs the MP memory barriers even on SP kernels.

no objections kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.10 2014/02/17 10:01:32 dlg Exp $	*/
a56 7

#define __membar(_f) do { __asm __volatile(_f ::: "memory"); } while (0)

/* virtio needs MP membars even on SP kernels */
#define virtio_membar_producer()	__membar("")
#define virtio_membar_consumer()	__membar("")
#define virtio_membar_sync()		__membar("mfence")
@


1.9
log
@Add atomic 32-bit cas operations.

This is needed for future acpi global locking routines.

Okay kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.8 2011/03/23 16:54:34 pirofti Exp $	*/
a71 8
static __inline int32_t
x86_atomic_testset_i32(volatile int32_t *ptr, int32_t val)
{
	__asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
	return val;
}


a126 1
#define x86_atomic_testset_i	x86_atomic_testset_i32
@


1.8
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.7 2010/12/27 20:22:23 guenther Exp $	*/
d91 11
@


1.7
log
@Correct x86_atomic_*_ul() to operate on 64bit integers, eliminate
x86_atomic_*_l(), and update the callers of all of those to use the
underlying x86_atomic_*_u32() functions that they were expecting anyway.
Whether the x86_atomic_*_ul() macros should be eliminated has been deferred.

ok kettenis@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.6 2007/05/25 16:22:11 art Exp $	*/
d39 2
a40 2
#ifndef _AMD64_ATOMIC_H
#define _AMD64_ATOMIC_H
d134 1
a134 1
#endif /* _AMD64_ATOMIC_H_ */
@


1.6
log
@Change the old slow and complicated TLB shootdown code to new, fast and
simple. This is basically the same code as on i386 and basically the same
performance improvements.

This change also includes code to delay the freeing of ptps until they
have been properly shot.

in snaps for a week, no problems reported.
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.5 2007/02/19 17:18:42 deraadt Exp $	*/
d123 1
a123 1
#define x86_atomic_testset_ul	x86_atomic_testset_u32
d125 2
a126 4
#define x86_atomic_setbits_l	x86_atomic_setbits_u32
#define x86_atomic_setbits_ul	x86_atomic_setbits_u32
#define x86_atomic_clearbits_l	x86_atomic_clearbits_u32
#define x86_atomic_clearbits_ul	x86_atomic_clearbits_u32
@


1.5
log
@only make this interface available to the kernel for now, discussed witha
rt and such; tested and ok miod drahn
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.4 2007/02/06 17:13:33 art Exp $	*/
d93 10
@


1.4
log
@Add machine/atomic.h to all architectures and define two operations
right now that are supposed to be atomic with respect to interrupts and
SMP: atomic_setbits_int and atomic_clearbits_int.

All architectures other than i386 and amd64 get dummy implementations
since at first we'll be replacing operations that are done with
"a |= bit" and "a &= ~bit" today. More proper implementations will follow

kettenis@@, miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.3 2004/02/23 06:25:14 mickey Exp $	*/
d39 2
a40 2
#ifndef _ATOMIC_H
#define _ATOMIC_H
d50 1
a50 1
#ifndef _LOCORE
d59 4
a62 3
x86_atomic_testset_u64(volatile u_int64_t *ptr, u_int64_t val) {
    __asm__ volatile ("xchgq %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
d66 4
a69 3
x86_atomic_testset_u32(volatile u_int32_t *ptr, u_int32_t val) {
    __asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
a71 2


d73 4
a76 3
x86_atomic_testset_i32(volatile int32_t *ptr, int32_t val) {
    __asm__ volatile ("xchgl %0,(%2)" :"=r" (val):"0" (val),"r" (ptr));
    return val;
d82 3
a84 2
x86_atomic_setbits_u32(volatile u_int32_t *ptr, u_int32_t bits) {
    __asm __volatile(LOCK " orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d88 3
a90 2
x86_atomic_clearbits_u32(volatile u_int32_t *ptr, u_int32_t bits) {
    __asm __volatile(LOCK " andl %1,%0" :  "=m" (*ptr) : "ir" (~bits));
d102 3
a104 2
x86_atomic_setbits_u64(volatile u_int64_t *ptr, u_int64_t bits) {
    __asm __volatile(LOCK " orq %1,%0" :  "=m" (*ptr) : "ir" (bits));
d108 3
a110 2
x86_atomic_clearbits_u64(volatile u_int64_t *ptr, u_int64_t bits) {
    __asm __volatile(LOCK " andq %1,%0" :  "=m" (*ptr) : "ir" (~bits));
d125 2
a126 2
#endif
#endif
@


1.3
log
@fix a pasto
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.2 2004/02/23 05:57:08 mickey Exp $	*/
d42 8
d52 6
d82 1
a82 1
    __asm __volatile("lock ; orl %1,%0" :  "=m" (*ptr) : "ir" (bits));
d87 1
a87 1
    __asm __volatile("lock ; andl %1,%0" :  "=m" (*ptr) : "ir" (~bits));
d100 1
a100 1
    __asm __volatile("lock ; orq %1,%0" :  "=m" (*ptr) : "ir" (bits));
d105 1
a105 1
    __asm __volatile("lock ; andq %1,%0" :  "=m" (*ptr) : "ir" (~bits));
d114 5
@


1.2
log
@add a comment on set/clr 64bit ops
@
text
@d1 1
a1 1
/*	$OpenBSD: atomic.h,v 1.1 2004/01/28 01:39:39 mickey Exp $	*/
d86 1
a86 1
    __asm __volatile("lock ; orq %1,%0" :  "=m" (*ptr) : "ir" (~bits));
@


1.1
log
@an amd64 arch support.
hacked by art@@ from netbsd sources and then later debugged
by me into the shape where it can host itself.
no bootloader yet as needs redoing from the
recent advanced i386 sources (anyone? ;)
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d77 7
a83 1

@


1.1.2.1
log
@Merge with the trunk
@
text
@d77 1
a77 7
/*
 * XXX XXX XXX
 * theoretically 64bit cannot be used as
 * an "i" and thus if we ever try to give
 * these anything from the high dword there
 * is an asm error pending
 */
d80 1
a80 1
    __asm __volatile("lock ; orq %1,%0" :  "=m" (*ptr) : "ir" (bits));
@


