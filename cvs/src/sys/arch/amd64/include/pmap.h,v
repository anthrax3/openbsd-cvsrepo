head	1.62;
access;
symbols
	OPENBSD_6_2_BASE:1.62
	OPENBSD_6_1:1.62.0.8
	OPENBSD_6_1_BASE:1.62
	OPENBSD_6_0:1.62.0.4
	OPENBSD_6_0_BASE:1.62
	OPENBSD_5_9:1.62.0.2
	OPENBSD_5_9_BASE:1.62
	OPENBSD_5_8:1.57.0.4
	OPENBSD_5_8_BASE:1.57
	OPENBSD_5_7:1.54.0.2
	OPENBSD_5_7_BASE:1.54
	OPENBSD_5_6:1.45.0.4
	OPENBSD_5_6_BASE:1.45
	OPENBSD_5_5:1.42.0.4
	OPENBSD_5_5_BASE:1.42
	OPENBSD_5_4:1.41.0.2
	OPENBSD_5_4_BASE:1.41
	OPENBSD_5_3:1.39.0.8
	OPENBSD_5_3_BASE:1.39
	OPENBSD_5_2:1.39.0.6
	OPENBSD_5_2_BASE:1.39
	OPENBSD_5_1_BASE:1.39
	OPENBSD_5_1:1.39.0.4
	OPENBSD_5_0:1.39.0.2
	OPENBSD_5_0_BASE:1.39
	OPENBSD_4_9:1.37.0.2
	OPENBSD_4_9_BASE:1.37
	OPENBSD_4_8:1.33.0.2
	OPENBSD_4_8_BASE:1.33
	OPENBSD_4_7:1.30.0.2
	OPENBSD_4_7_BASE:1.30
	OPENBSD_4_6:1.27.0.4
	OPENBSD_4_6_BASE:1.27
	OPENBSD_4_5:1.20.0.2
	OPENBSD_4_5_BASE:1.20
	OPENBSD_4_4:1.19.0.2
	OPENBSD_4_4_BASE:1.19
	OPENBSD_4_3:1.17.0.2
	OPENBSD_4_3_BASE:1.17
	OPENBSD_4_2:1.16.0.2
	OPENBSD_4_2_BASE:1.16
	OPENBSD_4_1:1.10.0.4
	OPENBSD_4_1_BASE:1.10
	OPENBSD_4_0:1.10.0.2
	OPENBSD_4_0_BASE:1.10
	OPENBSD_3_9:1.9.0.4
	OPENBSD_3_9_BASE:1.9
	OPENBSD_3_8:1.8.0.2
	OPENBSD_3_8_BASE:1.8
	OPENBSD_3_7:1.5.0.4
	OPENBSD_3_7_BASE:1.5
	OPENBSD_3_6:1.5.0.2
	OPENBSD_3_6_BASE:1.5
	SMP_SYNC_A:1.3
	SMP_SYNC_B:1.3
	OPENBSD_3_5:1.2.0.2
	OPENBSD_3_5_BASE:1.2
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.62
date	2016.02.08.18.23.04;	author stefan;	state Exp;
branches;
next	1.61;
commitid	KlnooGxekwRiYYw9;

1.61
date	2015.11.13.07.52.20;	author mlarkin;	state Exp;
branches;
next	1.60;
commitid	KdsjYlqLpqhwOUHf;

1.60
date	2015.11.10.08.57.39;	author mlarkin;	state Exp;
branches;
next	1.59;
commitid	vpkDcHyyMuFIvij0;

1.59
date	2015.08.22.07.19.03;	author mlarkin;	state Exp;
branches;
next	1.58;
commitid	nQ8Za1QKKejvXUXx;

1.58
date	2015.08.20.03.43.29;	author mlarkin;	state Exp;
branches;
next	1.57;
commitid	uhvW5z9wGAxHDgXd;

1.57
date	2015.06.29.02.54.51;	author mlarkin;	state Exp;
branches;
next	1.56;
commitid	KTKDVjpb0K1F3Sxs;

1.56
date	2015.06.24.07.18.13;	author mlarkin;	state Exp;
branches;
next	1.55;
commitid	5Tbe3dOZE466jNzy;

1.55
date	2015.03.10.20.12.39;	author kettenis;	state Exp;
branches;
next	1.54;
commitid	IuMVdmBhCuJLJqCN;

1.54
date	2015.02.19.03.19.11;	author mlarkin;	state Exp;
branches;
next	1.53;
commitid	qZ2VSccCasYXUYhw;

1.53
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.52;
commitid	eahBabNpxnDWKzqJ;

1.52
date	2015.02.07.01.46.27;	author kettenis;	state Exp;
branches;
next	1.51;
commitid	7b6QyxnUYRNhvNHU;

1.51
date	2014.12.15.05.05.24;	author guenther;	state Exp;
branches;
next	1.50;
commitid	RhpMnRcBXz94hFDC;

1.50
date	2014.12.15.04.54.44;	author tedu;	state Exp;
branches;
next	1.49;
commitid	LTR5gzASo8KdOa3m;

1.49
date	2014.12.02.18.13.10;	author tedu;	state Exp;
branches;
next	1.48;
commitid	ZYUxNRICiD9sC1vn;

1.48
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.47;
commitid	yv0ECmCdICvq576h;

1.47
date	2014.10.06.20.34.58;	author sf;	state Exp;
branches;
next	1.46;
commitid	TLzvOnmkOFxCXfko;

1.46
date	2014.09.16.18.57.51;	author sf;	state Exp;
branches;
next	1.45;
commitid	86xaiPmbmOFQjqPd;

1.45
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.44;
commitid	7NtJNW9udCOFtDNM;

1.44
date	2014.03.16.07.28.55;	author guenther;	state Exp;
branches;
next	1.43;

1.43
date	2014.03.07.16.56.57;	author guenther;	state Exp;
branches;
next	1.42;

1.42
date	2014.01.30.18.16.41;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2013.03.31.17.07.02;	author deraadt;	state Exp;
branches;
next	1.40;

1.40
date	2013.03.21.15.50.28;	author deraadt;	state Exp;
branches;
next	1.39;

1.39
date	2011.06.30.22.18.01;	author jsg;	state Exp;
branches;
next	1.38;

1.38
date	2011.03.23.16.54.34;	author pirofti;	state Exp;
branches;
next	1.37;

1.37
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.36;

1.36
date	2010.11.30.19.30.16;	author kettenis;	state Exp;
branches;
next	1.35;

1.35
date	2010.10.26.05.49.10;	author guenther;	state Exp;
branches;
next	1.34;

1.34
date	2010.09.06.17.36.49;	author guenther;	state Exp;
branches;
next	1.33;

1.33
date	2010.05.13.19.27.24;	author oga;	state Exp;
branches;
next	1.32;

1.32
date	2010.05.08.16.54.07;	author oga;	state Exp;
branches;
next	1.31;

1.31
date	2010.04.22.07.55.02;	author jasper;	state Exp;
branches;
next	1.30;

1.30
date	2009.12.09.18.45.49;	author deraadt;	state Exp;
branches;
next	1.29;

1.29
date	2009.12.09.14.31.57;	author oga;	state Exp;
branches;
next	1.28;

1.28
date	2009.08.11.17.15.54;	author oga;	state Exp;
branches;
next	1.27;

1.27
date	2009.06.09.02.56.38;	author krw;	state Exp;
branches;
next	1.26;

1.26
date	2009.06.06.23.45.36;	author guenther;	state Exp;
branches;
next	1.25;

1.25
date	2009.06.05.10.51.45;	author guenther;	state Exp;
branches;
next	1.24;

1.24
date	2009.05.28.09.05.33;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2009.04.27.17.48.25;	author deraadt;	state Exp;
branches;
next	1.22;

1.22
date	2009.04.23.07.42.02;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2009.03.30.16.09.36;	author oga;	state Exp;
branches;
next	1.20;

1.20
date	2009.02.05.01.15.20;	author oga;	state Exp;
branches;
next	1.19;

1.19
date	2008.06.10.02.55.39;	author weingart;	state Exp;
branches;
next	1.18;

1.18
date	2008.05.23.15.39.43;	author jasper;	state Exp;
branches;
next	1.17;

1.17
date	2007.09.10.18.49.44;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2007.07.06.11.46.48;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2007.05.29.02.36.19;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2007.05.27.21.06.05;	author jason;	state Exp;
branches;
next	1.13;

1.13
date	2007.05.27.08.58.31;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2007.05.25.16.22.11;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2007.05.15.16.38.33;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2006.03.15.20.01.58;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2005.10.26.18.46.07;	author martin;	state Exp;
branches;
next	1.8;

1.8
date	2005.07.12.21.18.11;	author hshoexer;	state Exp;
branches;
next	1.7;

1.7
date	2005.06.01.14.36.36;	author brad;	state Exp;
branches;
next	1.6;

1.6
date	2005.05.27.19.32.39;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2004.08.06.22.39.12;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2004.07.19.15.09.05;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2004.05.20.09.20.41;	author kettenis;	state Exp;
branches;
next	1.2;

1.2
date	2004.02.23.08.32.36;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	2004.01.28.01.39.39;	author mickey;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2004.06.05.23.09.25;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.62
log
@Set EPT bits of guest pages in pmap_enter instead of doing it
after an uvm_fault: uvm_fault maps in neighboring pages of
the faulting page. We want EPT bits set for those as soon as
possible as well. This avoids additional EPT violations
causing further uvm_faults when the guest accesses the
neighboring pages.

discussion with and ok mlarkin@@
@
text
@/*	$OpenBSD: pmap.h,v 1.61 2015/11/13 07:52:20 mlarkin Exp $	*/
/*	$NetBSD: pmap.h,v 1.1 2003/04/26 18:39:46 fvdl Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright (c) 2001 Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Frank van der Linden for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed for the NetBSD Project by
 *      Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * pmap.h: see pmap.c for the history of this pmap module.
 */

#ifndef	_MACHINE_PMAP_H_
#define	_MACHINE_PMAP_H_

#ifndef _LOCORE
#ifdef _KERNEL
#include <machine/cpufunc.h>
#include <machine/segments.h>
#endif /* _KERNEL */
#include <sys/mutex.h>
#include <uvm/uvm_object.h>
#include <machine/pte.h>
#endif

/*
 * The x86_64 pmap module closely resembles the i386 one. It uses
 * the same recursive entry scheme. See the i386 pmap.h for a
 * description. The alternate area trick for accessing non-current
 * pmaps has been removed, though, because it performs badly on SMP
 * systems.
 * The most obvious difference to i386 is that 2 extra levels of page
 * table need to be dealt with. The level 1 page table pages are at:
 *
 * l1: 0x00007f8000000000 - 0x00007fffffffffff     (39 bits, needs PML4 entry)
 *
 * The other levels are kept as physical pages in 3 UVM objects and are
 * temporarily mapped for virtual access when needed.
 *
 * The other obvious difference from i386 is that it has a direct map of all
 * physical memory in the VA range:
 *
 *     0xffffff0000000000 - 0xffffff7fffffffff
 *
 * The direct map is used in some cases to access PTEs of non-current pmaps.
 *
 * Note that address space is signed, so the layout for 48 bits is:
 *
 *  +---------------------------------+ 0xffffffffffffffff
 *  |         Kernel Image            |
 *  +---------------------------------+ 0xffffff8000000000
 *  |         Direct Map              |
 *  +---------------------------------+ 0xffffff0000000000
 *  ~                                 ~
 *  |                                 |
 *  |         Kernel Space            |
 *  |                                 |
 *  |                                 |
 *  +---------------------------------+ 0xffff800000000000 = 0x0000800000000000
 *  |    L1 table (PTE pages)         |
 *  +---------------------------------+ 0x00007f8000000000
 *  ~                                 ~
 *  |                                 |
 *  |         User Space              |
 *  |                                 |
 *  |                                 |
 *  +---------------------------------+ 0x0000000000000000
 *
 * In other words, there is a 'VA hole' at 0x0000800000000000 -
 * 0xffff800000000000 which will trap, just as on, for example,
 * sparcv9.
 *
 * The unused space can be used if needed, but it adds a little more
 * complexity to the calculations.
 */

/*
 * Mask to get rid of the sign-extended part of addresses.
 */
#define VA_SIGN_MASK		0xffff000000000000
#define VA_SIGN_NEG(va)		((va) | VA_SIGN_MASK)
/*
 * XXXfvdl this one's not right.
 */
#define VA_SIGN_POS(va)		((va) & ~VA_SIGN_MASK)

#define L4_SLOT_PTE		255
#define L4_SLOT_KERN		256
#define L4_SLOT_KERNBASE	511
#define L4_SLOT_DIRECT		510

#define PDIR_SLOT_KERN		L4_SLOT_KERN
#define PDIR_SLOT_PTE		L4_SLOT_PTE
#define PDIR_SLOT_DIRECT	L4_SLOT_DIRECT

/*
 * the following defines give the virtual addresses of various MMU
 * data structures:
 * PTE_BASE: the base VA of the linear PTE mappings
 * PDP_PDE: the VA of the PDE that points back to the PDP
 *
 */

#define PTE_BASE  ((pt_entry_t *) (L4_SLOT_PTE * NBPD_L4))
#define PMAP_DIRECT_BASE	(VA_SIGN_NEG((L4_SLOT_DIRECT * NBPD_L4)))
#define PMAP_DIRECT_END		(VA_SIGN_NEG(((L4_SLOT_DIRECT + 1) * NBPD_L4)))

#define L1_BASE		PTE_BASE

#define L2_BASE ((pd_entry_t *)((char *)L1_BASE + L4_SLOT_PTE * NBPD_L3))
#define L3_BASE ((pd_entry_t *)((char *)L2_BASE + L4_SLOT_PTE * NBPD_L2))
#define L4_BASE ((pd_entry_t *)((char *)L3_BASE + L4_SLOT_PTE * NBPD_L1))

#define PDP_PDE		(L4_BASE + PDIR_SLOT_PTE)

#define PDP_BASE	L4_BASE

#define NKL4_MAX_ENTRIES	(unsigned long)1
#define NKL3_MAX_ENTRIES	(unsigned long)(NKL4_MAX_ENTRIES * 512)
#define NKL2_MAX_ENTRIES	(unsigned long)(NKL3_MAX_ENTRIES * 512)
#define NKL1_MAX_ENTRIES	(unsigned long)(NKL2_MAX_ENTRIES * 512)

#define NKL4_KIMG_ENTRIES	1
#define NKL3_KIMG_ENTRIES	1
#define NKL2_KIMG_ENTRIES	16

#define NDML4_ENTRIES		1
#define NDML3_ENTRIES		1
#define NDML2_ENTRIES		4	/* 4GB */

/*
 * Since kva space is below the kernel in its entirety, we start off
 * with zero entries on each level.
 */
#define NKL4_START_ENTRIES	0
#define NKL3_START_ENTRIES	0
#define NKL2_START_ENTRIES	0
#define NKL1_START_ENTRIES	0	/* XXX */

#define NTOPLEVEL_PDES		(PAGE_SIZE / (sizeof (pd_entry_t)))

#define NPDPG			(PAGE_SIZE / sizeof (pd_entry_t))

/*
 * pl*_pi: index in the ptp page for a pde mapping a VA.
 * (pl*_i below is the index in the virtual array of all pdes per level)
 */
#define pl1_pi(VA)	(((VA_SIGN_POS(VA)) & L1_MASK) >> L1_SHIFT)
#define pl2_pi(VA)	(((VA_SIGN_POS(VA)) & L2_MASK) >> L2_SHIFT)
#define pl3_pi(VA)	(((VA_SIGN_POS(VA)) & L3_MASK) >> L3_SHIFT)
#define pl4_pi(VA)	(((VA_SIGN_POS(VA)) & L4_MASK) >> L4_SHIFT)

/*
 * pl*_i: generate index into pde/pte arrays in virtual space
 */
#define pl1_i(VA)	(((VA_SIGN_POS(VA)) & L1_FRAME) >> L1_SHIFT)
#define pl2_i(VA)	(((VA_SIGN_POS(VA)) & L2_FRAME) >> L2_SHIFT)
#define pl3_i(VA)	(((VA_SIGN_POS(VA)) & L3_FRAME) >> L3_SHIFT)
#define pl4_i(VA)	(((VA_SIGN_POS(VA)) & L4_FRAME) >> L4_SHIFT)
#define pl_i(va, lvl) \
        (((VA_SIGN_POS(va)) & ptp_masks[(lvl)-1]) >> ptp_shifts[(lvl)-1])

#define PTP_MASK_INITIALIZER	{ L1_FRAME, L2_FRAME, L3_FRAME, L4_FRAME }
#define PTP_SHIFT_INITIALIZER	{ L1_SHIFT, L2_SHIFT, L3_SHIFT, L4_SHIFT }
#define NKPTP_INITIALIZER	{ NKL1_START_ENTRIES, NKL2_START_ENTRIES, \
				  NKL3_START_ENTRIES, NKL4_START_ENTRIES }
#define NKPTPMAX_INITIALIZER	{ NKL1_MAX_ENTRIES, NKL2_MAX_ENTRIES, \
				  NKL3_MAX_ENTRIES, NKL4_MAX_ENTRIES }
#define NBPD_INITIALIZER	{ NBPD_L1, NBPD_L2, NBPD_L3, NBPD_L4 }
#define PDES_INITIALIZER	{ L2_BASE, L3_BASE, L4_BASE }

/*
 * PTP macros:
 *   a PTP's index is the PD index of the PDE that points to it
 *   a PTP's offset is the byte-offset in the PTE space that this PTP is at
 *   a PTP's VA is the first VA mapped by that PTP
 *
 * note that PAGE_SIZE == number of bytes in a PTP (4096 bytes == 1024 entries)
 *           NBPD == number of bytes a PTP can map (4MB)
 */

#define ptp_va2o(va, lvl)	(pl_i(va, (lvl)+1) * PAGE_SIZE)

#define PTP_LEVELS	4

/*
 * PG_AVAIL usage: we make use of the ignored bits of the PTE
 */

#define PG_W		PG_AVAIL1	/* "wired" mapping */
#define PG_PVLIST	PG_AVAIL2	/* mapping has entry on pvlist */
/* PG_AVAIL3 not used */

/*
 * Number of PTE's per cache line.  8 byte pte, 64-byte cache line
 * Used to avoid false sharing of cache lines.
 */
#define NPTECL		8


#if defined(_KERNEL) && !defined(_LOCORE)
/*
 * pmap data structures: see pmap.c for details of locking.
 */

struct pmap;
typedef struct pmap *pmap_t;

/*
 * we maintain a list of all non-kernel pmaps
 */

LIST_HEAD(pmap_head, pmap); /* struct pmap_head: head of a pmap list */

/*
 * the pmap structure
 *
 * note that the pm_obj contains the reference count,
 * page list, and number of PTPs within the pmap.
 */

#define PMAP_TYPE_NORMAL	1
#define PMAP_TYPE_EPT		2
#define PMAP_TYPE_RVI		3
#define pmap_nested(pm) ((pm)->pm_type != PMAP_TYPE_NORMAL)

struct pmap {
	struct mutex pm_mtx;
	struct uvm_object pm_obj[PTP_LEVELS-1]; /* objects for lvl >= 1) */
	LIST_ENTRY(pmap) pm_list;	/* list (lck by pm_list lock) */
	pd_entry_t *pm_pdir;		/* VA of PD (lck by object lock) */
	paddr_t pm_pdirpa;		/* PA of PD (read-only after create) */
	struct vm_page *pm_ptphint[PTP_LEVELS-1];
					/* pointer to a PTP in our pmap */
	struct pmap_statistics pm_stats;  /* pmap stats (lck by object lock) */

	u_int64_t pm_cpus;		/* mask of CPUs using pmap */
	int pm_type;			/* Type of pmap this is (PMAP_TYPE_x) */
};

/*
 * MD flags that we use for pmap_enter (in the pa):
 */
#define PMAP_PA_MASK	~((paddr_t)PAGE_MASK) /* to remove the flags */
#define	PMAP_NOCACHE	0x1 /* set the non-cacheable bit. */
#define	PMAP_WC		0x2 /* set page write combining. */

/*
 * We keep mod/ref flags in struct vm_page->pg_flags.
 */
#define	PG_PMAP_MOD	PG_PMAP0
#define	PG_PMAP_REF	PG_PMAP1
#define	PG_PMAP_WC      PG_PMAP2

/*
 * for each managed physical page we maintain a list of <PMAP,VA>'s
 * which it is mapped at.
 */
struct pv_entry {			/* locked by its list's pvh_lock */
	struct pv_entry *pv_next;	/* next entry */
	struct pmap *pv_pmap;		/* the pmap */
	vaddr_t pv_va;			/* the virtual address */
	struct vm_page *pv_ptp;		/* the vm_page of the PTP */
};

/*
 * global kernel variables
 */

/* PTDpaddr: is the physical address of the kernel's PDP */
extern u_long PTDpaddr;

extern struct pmap kernel_pmap_store;	/* kernel pmap */

extern paddr_t ptp_masks[];
extern int ptp_shifts[];
extern long nkptp[], nbpd[], nkptpmax[];

/*
 * macros
 */

#define	pmap_kernel()			(&kernel_pmap_store)
#define	pmap_resident_count(pmap)	((pmap)->pm_stats.resident_count)
#define	pmap_wired_count(pmap)		((pmap)->pm_stats.wired_count)
#define	pmap_update(pmap)		/* nothing (yet) */

#define pmap_clear_modify(pg)		pmap_clear_attrs(pg, PG_M)
#define pmap_clear_reference(pg)	pmap_clear_attrs(pg, PG_U)
#define pmap_copy(DP,SP,D,L,S)		
#define pmap_is_modified(pg)		pmap_test_attrs(pg, PG_M)
#define pmap_is_referenced(pg)		pmap_test_attrs(pg, PG_U)
#define pmap_move(DP,SP,D,L,S)		
#define pmap_valid_entry(E) 		((E) & PG_V) /* is PDE or PTE valid? */

#define pmap_proc_iflush(p,va,len)	/* nothing */
#define pmap_unuse_final(p)		/* nothing */
#define	pmap_remove_holes(vm)		do { /* nothing */ } while (0)


/*
 * prototypes
 */

paddr_t		pmap_bootstrap(paddr_t, paddr_t);
boolean_t	pmap_clear_attrs(struct vm_page *, unsigned long);
static void	pmap_page_protect(struct vm_page *, vm_prot_t);
void		pmap_page_remove (struct vm_page *);
static void	pmap_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
void		pmap_remove(struct pmap *, vaddr_t, vaddr_t);
boolean_t	pmap_test_attrs(struct vm_page *, unsigned);
static void	pmap_update_pg(vaddr_t);
static void	pmap_update_2pg(vaddr_t,vaddr_t);
void		pmap_write_protect(struct pmap *, vaddr_t,
				vaddr_t, vm_prot_t);
void		pmap_fix_ept(struct pmap *, vaddr_t);

vaddr_t reserve_dumppages(vaddr_t); /* XXX: not a pmap fn */

paddr_t	pmap_prealloc_lowmem_ptps(paddr_t);

void	pagezero(vaddr_t);

int	pmap_convert(struct pmap *, int);

/* 
 * functions for flushing the cache for vaddrs and pages.
 * these functions are not part of the MI pmap interface and thus
 * should not be used as such.
 */
void	pmap_flush_cache(vaddr_t, vsize_t);
#define pmap_flush_page(paddr) do {					\
	KDASSERT(PHYS_TO_VM_PAGE(paddr) != NULL);			\
	pmap_flush_cache(PMAP_DIRECT_MAP(paddr), PAGE_SIZE);		\
} while (/* CONSTCOND */ 0)

#define	PMAP_STEAL_MEMORY	/* enable pmap_steal_memory() */
#define PMAP_GROWKERNEL		/* turn on pmap_growkernel interface */

/*
 * inline functions
 */

static __inline void
pmap_remove_all(struct pmap *pmap)
{
	/* Nothing. */
}

/*
 * pmap_update_pg: flush one page from the TLB (or flush the whole thing
 *	if hardware doesn't support one-page flushing)
 */

__inline static void
pmap_update_pg(vaddr_t va)
{
	invlpg(va);
}

/*
 * pmap_update_2pg: flush two pages from the TLB
 */

__inline static void
pmap_update_2pg(vaddr_t va, vaddr_t vb)
{
	invlpg(va);
	invlpg(vb);
}

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => this function is a frontend for pmap_page_remove/pmap_clear_attrs
 * => we only have to worry about making the page more protected.
 *	unprotecting a page is done on-demand at fault time.
 */

__inline static void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	if ((prot & PROT_WRITE) == 0) {
		if (prot & (PROT_READ | PROT_EXEC)) {
			(void) pmap_clear_attrs(pg, PG_RW);
		} else {
			pmap_page_remove(pg);
		}
	}
}

/*
 * pmap_protect: change the protection of pages in a pmap
 *
 * => this function is a frontend for pmap_remove/pmap_write_protect
 * => we only have to worry about making the page more protected.
 *	unprotecting a page is done on-demand at fault time.
 */

__inline static void
pmap_protect(struct pmap *pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	if ((prot & PROT_WRITE) == 0) {
		if (prot & (PROT_READ| PROT_EXEC)) {
			pmap_write_protect(pmap, sva, eva, prot);
		} else {
			pmap_remove(pmap, sva, eva);
		}
	}
}

/*
 * various address inlines
 *
 *  vtopte: return a pointer to the PTE mapping a VA, works only for
 *  user and PT addresses
 *
 *  kvtopte: return a pointer to the PTE mapping a kernel VA
 */

static __inline pt_entry_t *
vtopte(vaddr_t va)
{
	return (PTE_BASE + pl1_i(va));
}

static __inline pt_entry_t *
kvtopte(vaddr_t va)
{
#ifdef LARGEPAGES
	{
		pd_entry_t *pde;

		pde = L1_BASE + pl2_i(va);
		if (*pde & PG_PS)
			return ((pt_entry_t *)pde);
	}
#endif

	return (PTE_BASE + pl1_i(va));
}

#define PMAP_DIRECT_MAP(pa)	((vaddr_t)PMAP_DIRECT_BASE + (pa))
#define PMAP_DIRECT_UNMAP(va)	((paddr_t)(va) - PMAP_DIRECT_BASE)
#define pmap_map_direct(pg)	PMAP_DIRECT_MAP(VM_PAGE_TO_PHYS(pg))
#define pmap_unmap_direct(va)	PHYS_TO_VM_PAGE(PMAP_DIRECT_UNMAP(va))

#define __HAVE_PMAP_DIRECT

#endif /* _KERNEL && !_LOCORE */

#ifndef _LOCORE
struct pv_entry;
struct vm_page_md {
	struct mutex pv_mtx;
	struct pv_entry *pv_list;
};

#define VM_MDPAGE_INIT(pg) do {		\
	mtx_init(&(pg)->mdpage.pv_mtx, IPL_VM); \
	(pg)->mdpage.pv_list = NULL;	\
} while (0)
#endif	/* !_LOCORE */

#endif	/* _MACHINE_PMAP_H_ */
@


1.61
log
@
vmm(4) kernel code

circulated on hackers@@, no objections. Disabled by default.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.60 2015/11/10 08:57:39 mlarkin Exp $	*/
d372 1
a372 1
int		pmap_fix_ept(struct pmap *, vaddr_t, int *);
@


1.60
log
@
pmap changes required for vmm. Changes include addition of pm_type to
track type of pmap and various conversion and pte bit manipulation
functions for EPT.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.59 2015/08/22 07:19:03 mlarkin Exp $	*/
d372 1
@


1.59
log
@
delete a wrong comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.58 2015/08/20 03:43:29 mlarkin Exp $	*/
d277 5
d293 1
d378 2
@


1.58
log
@
Remove an unused #include file from i386 and amd64 pmap.h

ok miod@@, millert@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.57 2015/06/29 02:54:51 mlarkin Exp $	*/
a154 1
 * PTD_BASE: the base VA of the recursive mapping of the PTD
@


1.57
log
@
Remove some unused #defines

ok guenther@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.56 2015/06/24 07:18:13 mlarkin Exp $	*/
a72 1
#include <sys/mman.h>
@


1.56
log
@
Remove a couple of unused and old #defines that discussed phys and virt
address widths in 1st-gen amd64 cpus.

ok kettenis, deraadt, guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.55 2015/03/10 20:12:39 kettenis Exp $	*/
a281 3
#define pm_obj_l1 pm_obj[0]
#define pm_obj_l2 pm_obj[1]
#define pm_obj_l3 pm_obj[2]
@


1.55
log
@Make the amd64 pmap (more) mpsafe by protecting both the pmap itself and the
pv lists with a mutex.  Rearange some code to avoid sleeping/spinning with
one of these locks held, and also take care that we don't allocate/free
any memory in that case.  This should make pmap_enter(9), pmap_remove(9) and
pmap_page_protect(9) safe to use without holding the kernel lock.

Other architectures will follow.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.54 2015/02/19 03:19:11 mlarkin Exp $	*/
a131 11

/*
 * The first generation of Hammer processors can use 48 bits of
 * virtual memory, and 40 bits of physical memory. This will be
 * more for later generations. These defines can be changed to
 * variable names containing the # of bits, extracted from an
 * extended cpuid instruction (variables are harder to use during
 * bootstrap, though)
 */
#define VIRT_BITS	48
#define PHYS_BITS	40
@


1.54
log
@
remove unused struct and #define

ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.53 2015/02/15 21:34:33 miod Exp $	*/
d77 1
a287 4
 *
 * pm_lock is the same as the spinlock for vm object 0. Changes to
 * the other objects may only be made if that lock has been taken
 * (the other object locks are only used when uvm_pagealloc is called)
d291 1
a292 1
#define	pm_lock	pm_obj[0].vmobjlock
d519 1
d524 1
@


1.53
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.52 2015/02/07 01:46:27 kettenis Exp $	*/
a331 12
};

/*
 * pmap_remove_record: a record of VAs that have been unmapped, used to
 * flush TLB.  if we have more than PMAP_RR_MAX then we stop recording.
 */

#define PMAP_RR_MAX	16	/* max of 16 pages (64K) */

struct pmap_remove_record {
	int prr_npages;
	vaddr_t prr_vas[PMAP_RR_MAX];
@


1.52
log
@Tedu the old idle page zeroing code.

ok tedu@@, guenther@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.51 2014/12/15 05:05:24 guenther Exp $	*/
d378 1
a378 1
#define	pmap_remove_holes(map)		do { /* nothing */ } while (0)
@


1.51
log
@fully parenthesize the PMAP_DIRECT_* macros

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.50 2014/12/15 04:54:44 tedu Exp $	*/
a416 6

/*
 * Do idle page zero'ing uncached to avoid polluting the cache.
 */
boolean_t	pmap_pageidlezero(struct vm_page *);
#define	PMAP_PAGEIDLEZERO(pg)	pmap_pageidlezero((pg))
@


1.50
log
@move needed macros to pmap.c and delete stale ones. ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.49 2014/12/02 18:13:10 tedu Exp $	*/
d528 2
a529 2
#define PMAP_DIRECT_MAP(pa)	((vaddr_t)PMAP_DIRECT_BASE + pa)
#define PMAP_DIRECT_UNMAP(va)	((paddr_t)va - PMAP_DIRECT_BASE)
@


1.49
log
@delete all the simplelocks. ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.48 2014/11/16 12:30:56 deraadt Exp $	*/
a526 6

#define pmap_pte_set(p, n)		x86_atomic_testset_u64(p, n)
#define pmap_pte_clearbits(p, b)	x86_atomic_clearbits_u64(p, b)
#define pmap_pte_setbits(p, b)		x86_atomic_setbits_u64(p, b)
#define pmap_cpu_has_pg_n()		(1)
#define pmap_cpu_has_invlpg		(1)
@


1.48
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.47 2014/10/06 20:34:58 sf Exp $	*/
d285 1
a285 1
 * note that the pm_obj contains the simple_lock, the reference count,
@


1.47
log
@Make amd64 pmap more efficient on multi-processor

With the current implementation, when accessing an inactive pmap, its
ptes are mapped in the APTE range. This has the problem that the APTE
range is mapped on all CPUs and changes to the APTE must therefore be
followed by a remote TLB flush on all CPUs. This is very inefficient
because the costs increase quadratically with the number of CPUs.

Therefore, the code is changed to remove the APTE mechanism completely
and instead switch the pmap locally. A remote TLB flush is then only
done if the pmap is in use on the remote CPU. In the common case, this
will replace one TLB flush on all CPUs with two local TLB flushes.

An additional optimization is done in cases where only a single PTE of
an inactive pmap is accessed: The requested PTE is found by walking the
page tables manually via the direct mapping. This makes some more TLB
flushes unnecessary.

Furthermore, some code is reordered so that the TLB-shootdown-IPIs are
sent first, then more local processing takes place, and only afterwards
the CPU waits for the remote TLB-shootdowns to finish.

This diff is based on a patch for i386 by Artur Grabowski <art blahonga org>
from 2008. Some additional bits were taken from a different patch by
Artur from 2005.

Tested by many. OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.46 2014/09/16 18:57:51 sf Exp $	*/
d73 1
d468 2
a469 2
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
d488 2
a489 2
	if ((prot & VM_PROT_WRITE) == 0) {
		if (prot & (VM_PROT_READ|VM_PROT_EXECUTE)) {
@


1.46
log
@Simple cleanups for amd64 pmap

- use __func__ in panics/printfs (fixes some out of sync function names)
- tell the compiler that code paths where we print diagnostics are unlikely
- use pmap_valid_entry() in some places
- remove KERNSPACE, which is not used anywhere

OK guenther@@ mlarkin@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.45 2014/07/11 16:35:40 jsg Exp $	*/
d82 6
a87 5
 * the same recursive entry scheme, and the same alternate area
 * trick for accessing non-current pmaps. See the i386 pmap.h
 * for a description. The first obvious difference is that 2 extra
 * levels of page table need to be dealt with. The level 1 page
 * table pages are at:
a90 4
 * The alternate space is at:
 *
 * l1: 0xffffff0000000000 - 0xffffff7fffffffff     (39 bits, needs PML4 entry)
 *
d97 3
a99 1
 *     0xfffffe8000000000 - 0xfffffeffffffffff
d106 1
a106 1
 *  |    alt.L1 table (PTE pages)     |
a107 2
 *  |         Direct Map              |
 *  +---------------------------------+ 0xfffffe8000000000
d155 1
a155 2
#define L4_SLOT_APTE		510
#define L4_SLOT_DIRECT		509
a158 1
#define PDIR_SLOT_APTE		L4_SLOT_APTE
d164 3
a166 3
 * PTE_BASE and APTE_BASE: the base VA of the linear PTE mappings
 * PTD_BASE and APTD_BASE: the base VA of the recursive mapping of the PTD
 * PDP_PDE and APDP_PDE: the VA of the PDE that points back to the PDP/APDP
a170 1
#define APTE_BASE ((pt_entry_t *) (VA_SIGN_NEG((L4_SLOT_APTE * NBPD_L4))))
a174 1
#define AL1_BASE	APTE_BASE
a179 4
#define AL2_BASE ((pd_entry_t *)((char *)AL1_BASE + L4_SLOT_PTE * NBPD_L3))
#define AL3_BASE ((pd_entry_t *)((char *)AL2_BASE + L4_SLOT_PTE * NBPD_L2))
#define AL4_BASE ((pd_entry_t *)((char *)AL3_BASE + L4_SLOT_PTE * NBPD_L1))

a180 1
#define APDP_PDE	(L4_BASE + PDIR_SLOT_APTE)
a182 1
#define APDP_BASE	AL4_BASE
a236 1
#define APDES_INITIALIZER	{ AL2_BASE, AL3_BASE, AL4_BASE }
a397 9

void	pmap_tlb_shootpage(struct pmap *, vaddr_t);
void	pmap_tlb_shootrange(struct pmap *, vaddr_t, vaddr_t);
void	pmap_tlb_shoottlb(void);
#ifdef MULTIPROCESSOR
void	pmap_tlb_shootwait(void);
#else
#define	pmap_tlb_shootwait()
#endif
@


1.45
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.44 2014/03/16 07:28:55 guenther Exp $	*/
a219 2

#define KERNSPACE		(NKL4_ENTRIES * NBPD_L4)
@


1.44
log
@Comment fixes: document the direct map and the kernel image, correct
the address of the alt L1 PTE, etc.
Remove extern declaration of pdes, as it doesn't exist as a global.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.43 2014/03/07 16:56:57 guenther Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgment:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.43
log
@We already assume global mappings (PG_G) are supported, so pmap_pg_g is
unnecessary

ok krw@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.42 2014/01/30 18:16:41 miod Exp $	*/
d91 1
a91 1
 * for a description. The obvious difference is that 3 extra
d99 1
a99 1
 * l1: 0xffffff8000000000 - 0xffffffffffffffff     (39 bits, needs PML4 entry)
d101 1
a101 1
 * The rest is kept as physical pages in 3 UVM objects, and is
d104 5
d112 2
a113 1
 *  |                                 |
d115 3
a117 2
 *  |                                 |
 *  +---------------------------------+ 0xffffff8000000000
d124 1
a124 3
 *  |                                 |
 *  |    L1 table (PTE pages)	      |
 *  |                                 |
a379 1
extern pd_entry_t *pdes[];
@


1.42
log
@Move declaration of struct vm_page_md from <machine/vmparam.h> to
<machine/pmap.h> where it belongs, and compensate in <uvm/uvm_extern.h>
by including <uvm/uvm_pmap.h> before <uvm/uvm_page.h>. Tested on all
MACHINE_ARCH but amd64 and i386 (and hppa64).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.41 2013/03/31 17:07:02 deraadt Exp $	*/
a370 1
extern int pmap_pg_g;			/* do we support PG_G? */
@


1.41
log
@try to avoid pulling in pte.h and other more crazy things.  Checked against
the things that libkvm needs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.40 2013/03/21 15:50:28 deraadt Exp $	*/
d570 12
@


1.40
log
@move kernel after 16MB. needed for additional growth that might come in a
little while...
diff from martynas who is not around now
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.39 2011/06/30 22:18:01 jsg Exp $	*/
d79 1
a80 1
#include <machine/pte.h>
d82 1
d84 1
@


1.39
log
@ansi, no binary change
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.38 2011/03/23 16:54:34 pirofti Exp $	*/
d204 1
a204 1
#define NKL2_KIMG_ENTRIES	8
@


1.38
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.37 2010/12/26 15:40:59 miod Exp $	*/
d466 1
a466 2
pmap_update_pg(va)
	vaddr_t va;
d476 1
a476 2
pmap_update_2pg(va, vb)
	vaddr_t va, vb;
d512 1
a512 4
pmap_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
@


1.37
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.36 2010/11/30 19:30:16 kettenis Exp $	*/
d75 2
a76 2
#ifndef	_AMD64_PMAP_H_
#define	_AMD64_PMAP_H_
d573 1
a573 1
#endif	/* _AMD64_PMAP_H_ */
@


1.36
log
@Extend bitmasks to 64-bit such that we can support up to 64 CPU cores.

tested by dlg@@, ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.35 2010/10/26 05:49:10 guenther Exp $	*/
a390 1
#define pmap_phys_address(ppn)		ptoa(ppn)
@


1.35
log
@The LDT is only used by dead compat code now, so load the ldt
register with the null selector (disabling use of it), stop reloading
it on every context switch, and blow away the table itself, as well
as the pcb and pmap bits that were used to track it.  Also, delete
two other unused pcb members: pcb_usersp and pcb_flags.  (Deleting
pcb_usersp also keeps the pcb_savefpu member aligned properly.)
Finally, delete the defines for the unimplemented AMD64_{GET,SET}_LDT
sysarch() calls.

Tested by various with both AMD and Intel chips
ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.34 2010/09/06 17:36:49 guenther Exp $	*/
d321 1
a321 1
	u_int32_t pm_cpus;		/* mask of CPUs using pmap */
@


1.34
log
@Stop sending so many superfluous IPIs: zero out pm_cpus in pmap_create(),
don't set the current cpu's bit in pmap_activate() unless we actually
set %cr3, and add a DIAGNOSTIC printf to pmap_destroy() to catch if we
ever stop tracking them accurately again.  Also, GC the unused pm_flags
member.

ok deraadt@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.33 2010/05/13 19:27:24 oga Exp $	*/
a320 3
	union descriptor *pm_ldt;	/* user-set LDT */
	int pm_ldt_len;			/* number of LDT entries */
	int pm_ldt_sel;			/* LDT selector */
@


1.33
log
@Synchronise amd64 more with other PMAP_DIRECT architectures. (step 1,
more to come later)

Specfically, there is no reason to reserve a special virtual address just so we
can do boot dump, we have a direct map of every page anyway.

since pmap_map is deprecated and MD only anyway, this means we can remove that
interface too. If anything this should increase reliability since pmap_enter
won't fail under memory pressure during dump (unlikely but possible). It is also
simpler and smaller ;)

Tested by myself and ckuethe, no regressions.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.32 2010/05/08 16:54:07 oga Exp $	*/
a319 2

	int pm_flags;			/* see below */
@


1.32
log
@Page Attribute Tables (PAT) support for x86.

PAT allows setting per-mapping cachability bits. Our main interest in it
for write combining mappings so we do not have to rely so heaviliy on
mtrrs (which are stupidly set up on more and more machines). MD flags to
pmap allow setting these bits (which bus_space now uses for PREFETCHABLE
maps), if a vm page has a bit set, then we will use WC for all mappings
of a page (used for userland mappings). We also check for known errata
and fall back to UC- mappings in that case.

comments from kettenis@@, tedu@@ and william@@. kettenis@@, tedu@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.31 2010/04/22 07:55:02 jasper Exp $	*/
a569 2

vaddr_t	pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t);
@


1.31
log
@- remove ptei(), which was verified to be unused.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.30 2009/12/09 18:45:49 deraadt Exp $	*/
d334 1
d339 3
a341 2
#define PG_PMAP_MOD	PG_PMAP0
#define PG_PMAP_REF	PG_PMAP1
@


1.30
log
@sloppy oga, tsk tsk, you are scaring us
spotted by ckuethe and must be in immediately before my build gets there..
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.29 2009/12/09 14:31:57 oga Exp $	*/
a223 2

#define ptei(VA)	(((VA_SIGN_POS(VA)) & L1_MASK) >> L1_SHIFT)
@


1.29
log
@add two new MD only pmap apis to amd64 and i386 (not to be used in MI
code):

pmap_flush_cache(vaddr_t, vsize_t) and pmap_flush_page(paddr_t) to flush
the cache for virtual addresses and physical pages respectively using
the clflush instruction. These apis will shortly be used by the agp
bus_dma functions to avoid doing a wbinvd on each dmamap_sync.

ok kettenis@@, some comments from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.28 2009/08/11 17:15:54 oga Exp $	*/
d443 1
a443 1
	KDASSERT(PHYS_TO_VM_PAGE(paddr) != NULL);			\	
@


1.28
log
@fix some stupidity in x86 bus_space_map.

right now, we do a pmap_kenter_pa(), we then get the pte (behind pmap's
back) and check for the cache inhibit bit (if needed). If it isn't what
we want (this is the normal case) then we change it ourselves, and do a
manual tlb shootdown (i386 was a bit more stupid about it than amd64,
too).

Instead, make it so that like on some other archs (sparc64 comes to
mind) you can pass in flags in the low bits of the physical address,
pmap then does everything correctly for you.

Discovered this when I had some code doing a lot of bus_space_maps(), it
was incredibly slow, and profilling was dominated by
pmap_tlb_shootwait();

discussed with kettenis@@, miod@@, toby@@ and art@@.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.27 2009/06/09 02:56:38 krw Exp $	*/
d435 11
@


1.27
log
@revert guenther@@'s un-revert of art's curpmap.

My

bios0: ASUSTeK Computer INC. P5K-E
cpu0: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.74 MHz
cpu1: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.46 MHz
cpu2: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.46 MHz
cpu3: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.46 MHz

can't boot with this in. It always hangs somewhere in fsck'ing if
any, or between netstart and local daemons if no fsck'ing. Also
fubars theo's real amd machine.

Much more testing needed for this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.26 2009/06/06 23:45:36 guenther Exp $	*/
d332 1
a332 1
 * MD flags that we use for pmap_enter:
d334 2
a335 1
#define	PMAP_NOCACHE	PMAP_MD0 /* set the non-cacheable bit. */
@


1.26
log
@Unrevert the curpmap change with the addition of correct %gs handling
in the IPI handler so that it works when it interrupts userspace,
waiting for the droppmap IPI to complete when destroying it, and
(most importantly) don't call pmap_tlb_droppmap() from cpu_exit().
Tested by myself and ckuethe, as our machines choked on the original.

ok @@art
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.24 2009/05/28 09:05:33 art Exp $	*/
d328 1
a418 2
void 		pmap_switch(struct proc *, struct proc *);

a425 1
void	pmap_tlb_droppmap(struct pmap *);
a427 1
#define	pmap_tlb_droppmap(pm)
@


1.25
log
@Revert the curpmap change.  We know the IPI is broken on both ends,
but even with proposed fixes, the reaper panics are back.
@
text
@a327 1
	u_int32_t pm_cpus;		/* mask of CPUs using pmap */
d418 2
d427 1
d430 1
@


1.24
log
@Bring back the curpmap change. It was missing a reload of the pmap on
curcpu when we were freeing a pmap. Tested and working for a few weeks
now, but I was a bit too busy to commit it earlier.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.22 2009/04/23 07:42:02 art Exp $	*/
d328 1
a418 2
void 		pmap_switch(struct proc *, struct proc *);

a425 1
void	pmap_tlb_droppmap(struct pmap *);
a427 1
#define	pmap_tlb_droppmap(pm)
@


1.23
log
@turning pmap_deactivate into a NOP brought back the reaper panics, probably
because the reaper is running on the mappings of pmap from the process it
is about to unmap.  back it out until ht is fixed right; don't let this sit
in the tree waiting for a fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.21 2009/03/30 16:09:36 oga Exp $	*/
a327 1
	u_int32_t pm_cpus;		/* mask of CPUs using pmap */
d418 2
d427 1
d430 1
@


1.22
log
@Make pmap_deactivate a NOP.

Instead of keeping a bitmask of on which cpu the pmap might be active which
we clear in pmap_deactivate, always keep a pointer to the currently loaded
pmap in cpu_info. We can now optimize a context switch to the kernel pmap
(idle and kernel threads) to keep the previously loaded pmap still loaded
and then reuse that pmap if we context switch back to the same process.

Introduce a new IPI to force a pmap reload before the pmap is destroyed.

Clean up cpu_switchto.

toby@@ ok
@
text
@d328 1
a418 2
void 		pmap_switch(struct proc *, struct proc *);

a425 1
void	pmap_tlb_droppmap(struct pmap *);
a427 1
#define	pmap_tlb_droppmap(pm)
@


1.21
log
@Remove the direct uncached map. All its users have been removed.

In the future, we need to mark the correct parts of the direct map
uncached, but that's another diff.

art@@, kettenis@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.20 2009/02/05 01:15:20 oga Exp $	*/
a327 1
	u_int32_t pm_cpus;		/* mask of CPUs using pmap */
d418 2
d427 1
d430 1
@


1.20
log
@Mirroring the i386 commit just made. Add MD PMAP_NOCACHE flag to pmap,
and use it to implement BUS_DMA_NOCACHE for uncached mappings of dma
memory. Needed for some broken hardware.

Discussion with art, miod, kettenis and toby, ok miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.19 2008/06/10 02:55:39 weingart Exp $	*/
a159 1
#define L4_SLOT_DIRECT_NC	508
a164 1
#define PDIR_SLOT_DIRECT_NC	L4_SLOT_DIRECT_NC
a178 2
#define PMAP_DIRECT_BASE_NC	(VA_SIGN_NEG((L4_SLOT_DIRECT_NC * NBPD_L4)))
#define PMAP_DIRECT_END_NC	(VA_SIGN_NEG(((L4_SLOT_DIRECT_NC + 1) * NBPD_L4)))
a564 5

#define PMAP_DIRECT_NC_MAP(pa)	((vaddr_t)PMAP_DIRECT_BASE_NC + pa)
#define PMAP_DIRECT_NC_UNMAP(va) ((paddr_t)va - PMAP_DIRECT_BASE_NC)
#define pmap_map_nc_direct(pg)		PMAP_DIRECT_NC_MAP(VM_PAGE_TO_PHYS(pg))
#define pmap_unmap_nc_direct(va)	PHYS_TO_VM_PAGE(PMAP_DIRECT_NC_UNMAP(va))
@


1.19
log
@All your memory belong to us.  This has been in snaps for a while,
and seems to work.  If it breaks, people had plenty of chances to
complain.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.18 2008/05/23 15:39:43 jasper Exp $	*/
d334 5
@


1.18
log
@- remove USER_LDT, it was never in a state where it would copile, nor will
we support i386-compat mode on amd64.

agreed by beck@@, dlg@@, kettenis@@
ok deraadt@@, tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.17 2007/09/10 18:49:44 miod Exp $	*/
d406 1
a406 1
void		pmap_bootstrap(vaddr_t, paddr_t);
d430 1
a430 1
void	pmap_prealloc_lowmem_ptps(void);
@


1.17
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.16 2007/07/06 11:46:48 art Exp $	*/
a334 3
/* pm_flags */
#define	PMF_USER_LDT	0x01	/* pmap has user-set LDT */

a558 5

#if 0   /* XXXfvdl was USER_LDT, need to check if that can be supported */
void	pmap_ldt_cleanup(struct proc *);
#define	PMAP_FORK
#endif /* USER_LDT */
@


1.16
log
@Fix a misleading comment.
noted by Constantine Kousoulos <wuwei@@freemail.gr>
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.15 2007/05/29 02:36:19 art Exp $	*/
d402 1
@


1.15
log
@PMAP_STEAL_MEMORY for amd64
drahn@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.14 2007/05/27 21:06:05 jason Exp $	*/
d116 1
a116 1
 *  |    alt.L1 table (PTE pages)     |
@


1.14
log
@add pmap_(un)map_nc_direct() macro's to match the non-nc versions
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.13 2007/05/27 08:58:31 art Exp $	*/
d436 1
@


1.13
log
@- Redo the way we set up the direct map. Map the first 4GB of it
  in locore so that we can use the direct map in pmap_bootstrap when
  setting up the initial page tables.

- Introduce a second direct map (I love large address spaces) with
  uncached pages.

jason@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.12 2007/05/25 16:22:11 art Exp $	*/
d568 2
d573 2
a574 3

#define pmap_map_direct(pg)	PMAP_DIRECT_MAP(VM_PAGE_TO_PHYS(pg))
#define pmap_unmap_direct(va)	PHYS_TO_VM_PAGE(PMAP_DIRECT_UNMAP(va))
@


1.12
log
@Change the old slow and complicated TLB shootdown code to new, fast and
simple. This is basically the same code as on i386 and basically the same
performance improvements.

This change also includes code to delay the freeing of ptps until they
have been properly shot.

in snaps for a week, no problems reported.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.11 2007/05/15 16:38:33 art Exp $	*/
d160 1
d166 1
d181 2
d210 4
d568 3
@


1.11
log
@Switch amd64 to VM_PAGE_MD. Mostly just imitating i386. flags in pg_flags.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.10 2006/03/15 20:01:58 miod Exp $	*/
d415 9
a423 3
void	pmap_tlb_shootdown(pmap_t, vaddr_t, pt_entry_t, int32_t *);
void	pmap_tlb_shootnow(int32_t);
void	pmap_do_tlb_shootdown(struct cpu_info *);
@


1.10
log
@Remove address range KASSERT check in vtopte() and kvtopte(), callers are
supposed to know which one of the two is applicable when the address is not
guaranteed to be a kernel address.
ok kettenis@@ mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.9 2005/10/26 18:46:07 martin Exp $	*/
d331 6
d338 1
a338 4
 * which it is mapped at.  the list is headed by a pv_head structure.
 * there is one pv_head per managed phys page (allocated at boot time).
 * the pv_head structure points to a list of pv_entry structures (each
 * describes one mapping).
a339 8

struct pv_entry;

struct pv_head {
	struct simplelock pvh_lock;	/* locks every pv on this list */
	struct pv_entry *pvh_list;	/* head of list (locked by pvh_lock) */
};

d401 1
a401 1
boolean_t	pmap_clear_attrs(struct vm_page *, unsigned);
@


1.9
log
@goodbye more Mach macros

help toby, ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.8 2005/07/12 21:18:11 hshoexer Exp $	*/
a521 2
#include <lib/libkern/libkern.h>

a524 3

	KASSERT(va < (L4_SLOT_KERN * NBPD_L4));

a530 3

	KASSERT(va >= (L4_SLOT_KERN * NBPD_L4));

@


1.8
log
@fix comment, describe the VA hole correclty

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.7 2005/06/01 14:36:36 brad Exp $	*/
d394 1
a394 1
#define pmap_phys_address(ppn)		ptob(ppn)
@


1.7
log
@as Jason requested, be gone vtophys().

ok deraadt@@ marco@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.6 2005/05/27 19:32:39 art Exp $	*/
d114 1
a114 1
 *  +---------------------------------+ 0xffff800000000000 = 0x0000008000000000
d126 1
a126 1
 * In other words, there is a 'VA hole' at 0x0000008000000000 -
@


1.6
log
@Stop pretending that amd64 is i386. We're insulting the cpu by not even
pretending to use all the address space it gives us.

 - Map all physical memory 1-1 and implement PMAP_DIRECT
 - Remove the vast magic we do to map pages for pmap_zero_page,
   pmap_copy_page, pv allocation, magic while bootstrapping,
   reading of /dev/mem, etc.
 - implement a fast pmap_zero_page based on sse instructions.

I love removing code. More to come.

deraadt@@ ok tested by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.5 2004/08/06 22:39:12 deraadt Exp $	*/
a557 1
paddr_t vtophys(vaddr_t);
a563 5

/* 
 * Hooks for the pool allocator.
 */
/* #define	POOL_VTOPHYS(va)	vtophys((vaddr_t) (va)) */
@


1.5
log
@rename sparc kill_user_windows() to pmap_unuse_final().  provide empty stubs
on all other architectures.  remove last architecture dependent #ifdef from
uvm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.4 2004/07/19 15:09:05 art Exp $	*/
a352 30
 * pv_entrys are dynamically allocated in chunks from a single page.
 * we keep track of how many pv_entrys are in use for each page and
 * we can free pv_entry pages if needed.  there is one lock for the
 * entire allocation system.
 */

struct pv_page_info {
	TAILQ_ENTRY(pv_page) pvpi_list;
	struct pv_entry *pvpi_pvfree;
	int pvpi_nfree;
};

/*
 * number of pv_entry's in a pv_page
 * (note: won't work on systems where NPBG isn't a constant)
 */

#define PVE_PER_PVPAGE ((PAGE_SIZE - sizeof(struct pv_page_info)) / \
			sizeof(struct pv_entry))

/*
 * a pv_page: where pv_entrys are allocated from
 */

struct pv_page {
	struct pv_page_info pvinfo;
	struct pv_entry pvents[PVE_PER_PVPAGE];
};

/*
d425 2
d571 5
a575 4
#define pmap_map_direct(pg)	\
	((vaddr_t)PMAP_DIRECT_BASE + VM_PAGE_TO_PHYS(pg))
#define pmap_unmap_direct(va)	\
	PHYS_TO_VM_PAGE(va - PMAP_DIRECT_BASE)
@


1.4
log
@Implement __HAVE_PMAP_DIRECT on amd64 using large pages. At this moment
it's limited to 512GB (one L4 page table entry) physical memory. Only
used carefully at this moment, but more improvements are in the pipeline.

tested by many, deraadt@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.3 2004/05/20 09:20:41 kettenis Exp $	*/
d428 1
@


1.3
log
@Properly flush instruction cache for ptrace(PT_WRTIE_{DI}, ...) on powerpc
and m68k.
ok drahn@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.2 2004/02/23 08:32:36 mickey Exp $	*/
d159 1
d161 4
a164 3
#define PDIR_SLOT_KERN	L4_SLOT_KERN
#define PDIR_SLOT_PTE	L4_SLOT_PTE
#define PDIR_SLOT_APTE	L4_SLOT_APTE
d177 2
d434 1
a434 1
void		pmap_bootstrap(vaddr_t);
d597 7
@


1.2
log
@get use of NX; partially from netbsd; passes the regress; deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.h,v 1.1 2004/01/28 01:39:39 mickey Exp $	*/
d422 2
@


1.1
log
@an amd64 arch support.
hacked by art@@ from netbsd sources and then later debugged
by me into the shape where it can host itself.
no bootloader yet as needs redoing from the
recent advanced i386 sources (anyone? ;)
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d575 1
@


1.1.2.1
log
@Merge with the trunk
@
text
@a422 2
#define pmap_proc_iflush(p,va,len)	/* nothing */

a574 1
#define pmap_pte_setbits(p, b)		x86_atomic_setbits_u64(p, b)
@


