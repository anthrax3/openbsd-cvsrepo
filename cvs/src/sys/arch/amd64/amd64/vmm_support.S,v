head	1.8;
access;
symbols
	OPENBSD_6_1:1.7.0.4
	OPENBSD_6_1_BASE:1.7
	OPENBSD_6_0:1.4.0.2
	OPENBSD_6_0_BASE:1.4
	OPENBSD_5_9:1.2.0.2
	OPENBSD_5_9_BASE:1.2;
locks; strict;
comment	@# @;


1.8
date	2017.04.27.06.16.39;	author mlarkin;	state Exp;
branches;
next	1.7;
commitid	ZHkhUOyGvsrTGUP9;

1.7
date	2017.03.25.15.25.20;	author kettenis;	state Exp;
branches;
next	1.6;
commitid	GzkzE0nA0FRkXxNU;

1.6
date	2017.02.20.07.36.20;	author mlarkin;	state Exp;
branches;
next	1.5;
commitid	FHlFYjJ8E3g13IeF;

1.5
date	2016.12.20.07.05.24;	author kettenis;	state Exp;
branches;
next	1.4;
commitid	VEUvJ5Ux5U7g4CCG;

1.4
date	2016.05.16.01.51.23;	author deraadt;	state Exp;
branches;
next	1.3;
commitid	uDwjy0S0Zftty0Z3;

1.3
date	2016.03.15.20.50.22;	author krw;	state Exp;
branches;
next	1.2;
commitid	JZR2bOwahEjnBJaG;

1.2
date	2015.11.16.10.08.41;	author mpi;	state Exp;
branches;
next	1.1;
commitid	KZ30ShVeuLAwERed;

1.1
date	2015.11.13.07.52.20;	author mlarkin;	state Exp;
branches;
next	;
commitid	KdsjYlqLpqhwOUHf;


desc
@@


1.8
log
@vmm(4): proper save/restore of FPU context during entry/exit.

tested by reyk, dcoppa, and a few others.

ok kettenis@@ on the fpu bits
ok deraadt@@ on the vmm bits
@
text
@/*	$OpenBSD: vmm_support.S,v 1.7 2017/03/25 15:25:20 kettenis Exp $	*/
/*
 * Copyright (c) 2014 Mike Larkin <mlarkin@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include "assym.h"
#include <machine/asm.h>
#include <machine/psl.h>
#include <machine/specialreg.h>

/*
 * XXX duplicated in vmmvar.h due to song-and-dance with sys/rwlock.h inclusion
 * here 
 */
#define VMX_FAIL_LAUNCH_UNKNOWN 1
#define VMX_FAIL_LAUNCH_INVALID_VMCS 2
#define VMX_FAIL_LAUNCH_VALID_VMCS 3

	.global _C_LABEL(vmxon)
	.global _C_LABEL(vmxoff)
	.global _C_LABEL(vmclear)
	.global _C_LABEL(vmptrld)
	.global _C_LABEL(vmptrst)
	.global _C_LABEL(vmwrite)
	.global _C_LABEL(vmread)
	.global _C_LABEL(invvpid)
	.global _C_LABEL(invept)
	.global _C_LABEL(vmx_enter_guest)
	.global _C_LABEL(vmm_dispatch_intr)
	.global _C_LABEL(svm_enter_guest)

	.text
	.code64
	.align 16,0xcc
_C_LABEL(vmm_dispatch_intr):
	movq	%rsp, %r11	/* r11 = temporary register */
	andq    $0xFFFFFFFFFFFFFFF0, %rsp
	movw	%ss, %ax
	pushq   %rax
	pushq	%r11
	pushfq
	movw	%cs, %ax
	pushq   %rax
	cli
	callq	*%rdi
	ret

_C_LABEL(vmxon):
	vmxon	(%rdi)
	jz	failed_on
	jc	failed_on
	xorq	%rax, %rax
	ret
failed_on:
	movq	$0x01, %rax
	ret

_C_LABEL(vmxoff):
	vmxoff
	jz	failed_off
	jc	failed_off
	xorq	%rax, %rax
	ret
failed_off:
	movq	$0x01, %rax
	ret

_C_LABEL(vmclear):
	vmclear	(%rdi)
	jz	failed_clear
	jc	failed_clear
	xorq	%rax, %rax
	ret
failed_clear:
	movq	$0x01, %rax
	ret

_C_LABEL(vmptrld):
	vmptrld	(%rdi)
	jz	failed_ptrld
	jc	failed_ptrld
	xorq	%rax, %rax
	ret
failed_ptrld:
	movq	$0x01, %rax
	ret

_C_LABEL(vmptrst):
	vmptrst	(%rdi)
	jz	failed_ptrst
	jc	failed_ptrst
	xorq	%rax, %rax
	ret
failed_ptrst:
	movq	$0x01, %rax
	ret

_C_LABEL(vmwrite):
	vmwrite	%rsi, %rdi
	jz	failed_write
	jc	failed_write
	xorq	%rax, %rax
	ret
failed_write:
	movq	$0x01, %rax
	ret

_C_LABEL(vmread):
	vmread	%rdi, (%rsi)
	jz	failed_read
	jc	failed_read
	xorq	%rax, %rax
	ret
failed_read:
	movq	$0x01, %rax
	ret

_C_LABEL(invvpid):
	invvpid (%rsi), %rdi
	ret

_C_LABEL(invept):
	invept (%rsi), %rdi
	ret

_C_LABEL(vmx_enter_guest):
	movq	%rdx, %r8	/* resume flag */
	testq	%r8, %r8
	jnz skip_init

	/*
	 * XXX make vmx_exit_handler a global and put this in the per-vcpu
	 * init code
	 */
	movq	$VMCS_HOST_IA32_RIP, %rdi
	movq	$vmx_exit_handler_asm, %rax
	vmwrite %rax, %rdi	/* Host RIP */

skip_init:
	/*
	 * XXX use msr list here for restore instead of all this
	 * stack jiggery-pokery
	 */

	pushfq
	popq	%rax
	andq	$(~PSL_I), %rax
	pushq	%rax

	/*
	 * Save (possibly) lazy-switched selectors
	 */
	movw	%es, %ax
	pushw	%ax
	movw	%ds, %ax
	pushw	%ax
	movw	%ss, %ax
	pushw	%ax

	movq	$MSR_FSBASE, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx
	pushw	%fs
	movq	$MSR_GSBASE, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx
	pushw	%gs
	movq	$MSR_KERNELGSBASE, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	/*
	 * Save various MSRs
	 */
	movq	$MSR_STAR, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	movq	$MSR_LSTAR, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	/* XXX - unused? */
	movq	$MSR_CSTAR, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	movq	$MSR_SFMASK, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	/* Preserve callee-preserved registers as per AMD64 ABI */
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbp
	pushq	%rbx
	pushq	%rsi		/* Guest Regs Pointer */

	movq	$VMCS_HOST_IA32_RSP, %rdi
	movq	%rsp, %rax
	vmwrite	%rax, %rdi	/* Host RSP */

	testq	%r8, %r8
	jnz	do_resume

	/* Restore guest registers */
	movq	0x78(%rsi), %rax
	movq	%rax, %cr2
	movq	0x70(%rsi), %r15
	movq	0x68(%rsi), %r14
	movq	0x60(%rsi), %r13
	movq	0x58(%rsi), %r12
	movq	0x50(%rsi), %r11
	movq	0x48(%rsi), %r10
	movq	0x40(%rsi), %r9
	movq	0x38(%rsi), %r8
	movq	0x30(%rsi), %rbp
	movq	0x28(%rsi), %rdi
	movq	0x20(%rsi), %rdx
	movq	0x18(%rsi), %rcx
	movq	0x10(%rsi), %rbx
	movq	0x08(%rsi), %rax
	movq	0x00(%rsi), %rsi

	vmlaunch
	jmp	fail_launch_or_resume
do_resume:
	/* Restore guest registers */
	movq	0x78(%rsi), %rax
	movq	%rax, %cr2
	movq	0x70(%rsi), %r15
	movq	0x68(%rsi), %r14
	movq	0x60(%rsi), %r13
	movq	0x58(%rsi), %r12
	movq	0x50(%rsi), %r11
	movq	0x48(%rsi), %r10
	movq	0x40(%rsi), %r9
	movq	0x38(%rsi), %r8
	movq	0x30(%rsi), %rbp
	movq	0x28(%rsi), %rdi
	movq	0x20(%rsi), %rdx
	movq	0x18(%rsi), %rcx
	movq	0x10(%rsi), %rbx
	movq	0x08(%rsi), %rax
	movq	0x00(%rsi), %rsi
	vmresume
fail_launch_or_resume:
	/* Failed launch/resume (fell through) */
	jc fail_launch_invalid_vmcs	/* Invalid VMCS */
	jz fail_launch_valid_vmcs	/* Valid VMCS, failed launch/resume */

	/* Unknown failure mode (not documented as per Intel SDM) */
fail_launch_unknown:
	movq	$VMX_FAIL_LAUNCH_UNKNOWN, %rdi
	popq	%rsi
	jmp	restore_host

fail_launch_invalid_vmcs:
	movq	$VMX_FAIL_LAUNCH_INVALID_VMCS, %rdi
	popq	%rsi
	jmp	restore_host

fail_launch_valid_vmcs:
	movq	$VMCS_INSTRUCTION_ERROR, %rdi
	popq	%rsi
	vmread	%rdi, %rax
	/* XXX check failure of vmread */
	movl	%eax, 0x80(%rsi)
	movq	$VMX_FAIL_LAUNCH_VALID_VMCS, %rdi
	jmp	restore_host

vmx_exit_handler_asm:
	/* Preserve guest registers not saved in VMCS */
	pushq	%rsi
	pushq	%rdi
	movq	0x10(%rsp), %rdi
	movq	0x8(%rsp), %rsi
	movq	%rsi, (%rdi)
	popq	%rdi
	popq	%rsi	/* discard */

	popq	%rsi
	movq	%rax, 0x8(%rsi)
	movq	%rbx, 0x10(%rsi)
	movq	%rcx, 0x18(%rsi)
	movq	%rdx, 0x20(%rsi)
	movq	%rdi, 0x28(%rsi)
	movq	%rbp, 0x30(%rsi)
	movq	%r8, 0x38(%rsi)
	movq	%r9, 0x40(%rsi)
	movq	%r10, 0x48(%rsi)
	movq	%r11, 0x50(%rsi)
	movq	%r12, 0x58(%rsi)
	movq	%r13, 0x60(%rsi)
	movq	%r14, 0x68(%rsi)
	movq	%r15, 0x70(%rsi)
	movq	%cr2, %rax
	movq	%rax, 0x78(%rsi)

	/* %rdi = 0 means we took an exit */
	xorq	%rdi, %rdi

restore_host:
	popq	%rbx
	popq	%rbp
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	
	/*
	 * Restore saved MSRs
	 */
	popq	%rdx
	popq	%rax
	movq	$MSR_SFMASK, %rcx
	wrmsr

	/* XXX - unused? */
	popq	%rdx
	popq	%rax
	movq	$MSR_CSTAR, %rcx
	wrmsr
	
	popq	%rdx
	popq	%rax
	movq	$MSR_LSTAR, %rcx
	wrmsr

	popq	%rdx
	popq	%rax
	movq	$MSR_STAR, %rcx
	wrmsr

	/*
	 * popw %gs will reset gsbase to 0, so preserve it
	 * first. This is to accommodate possibly lazy-switched
	 * selectors from above
	 */
	popq	%rdx
	popq	%rax
	movq	$MSR_KERNELGSBASE, %rcx
	wrmsr

	popw	%gs
	popq	%rdx
	popq	%rax
	movq	$MSR_GSBASE, %rcx
	wrmsr	

	popw	%fs
	popq	%rdx
	popq	%rax
	movq	$MSR_FSBASE, %rcx
	wrmsr

	popw	%ax
	movw	%ax, %ss
	popw	%ax
	movw	%ax, %ds
	popw	%ax
	movw	%ax, %es

	popfq

	movq	%rdi, %rax
	ret	
	
_C_LABEL(svm_enter_guest):
	clgi
	movq	%rdi, %r8
	pushfq

	pushq	%rdx	/* gdt pointer */

	/*
	 * Save (possibly) lazy-switched selectors
	 */
	strw	%ax
	pushw	%ax
	movw	%es, %ax
	pushw	%ax
	movw	%ds, %ax
	pushw	%ax
	movw	%ss, %ax
	pushw	%ax

	movq	$MSR_FSBASE, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx
	pushw	%fs
	movq	$MSR_GSBASE, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx
	pushw	%gs
	movq	$MSR_KERNELGSBASE, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	/*
	 * Save various MSRs
	 */
	movq	$MSR_STAR, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	movq	$MSR_LSTAR, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	/* XXX - unused? */
	movq	$MSR_CSTAR, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	movq	$MSR_SFMASK, %rcx
	rdmsr
	pushq	%rax
	pushq	%rdx

	/* Preserve callee-preserved registers as per AMD64 ABI */
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbp
	pushq	%rbx
	pushq	%rsi		/* Guest Regs Pointer */

	/* Restore guest registers */
	movq	%r8, %rax	/* rax = vmcb pa */
	movq	0x78(%rsi), %r8
	movq	%r8, %cr2
	movq	0x70(%rsi), %r15
	movq	0x68(%rsi), %r14
	movq	0x60(%rsi), %r13
	movq	0x58(%rsi), %r12
	movq	0x50(%rsi), %r11
	movq	0x48(%rsi), %r10
	movq	0x40(%rsi), %r9
	movq	0x38(%rsi), %r8
	movq	0x30(%rsi), %rbp
	movq	0x28(%rsi), %rdi
	movq	0x20(%rsi), %rdx
	movq	0x18(%rsi), %rcx
	movq	0x10(%rsi), %rbx
	/* %rax at 0x08(%rsi) is not needed in SVM */
	movq	0x00(%rsi), %rsi

	vmload	%rax
	vmrun	%rax
	vmsave	%rax

	/* Preserve guest registers not saved in VMCB */
	pushq	%rsi
	pushq	%rdi
	movq	0x10(%rsp), %rdi
	movq	0x8(%rsp), %rsi
	movq	%rsi, (%rdi)
	popq	%rdi
	popq	%rsi	/* discard */

	popq	%rsi
	/* %rax at 0x08(%rsi) is not needed in SVM */
	movq	%rbx, 0x10(%rsi)
	movq	%rcx, 0x18(%rsi)
	movq	%rdx, 0x20(%rsi)
	movq	%rdi, 0x28(%rsi)
	movq	%rbp, 0x30(%rsi)
	movq	%r8, 0x38(%rsi)
	movq	%r9, 0x40(%rsi)
	movq	%r10, 0x48(%rsi)
	movq	%r11, 0x50(%rsi)
	movq	%r12, 0x58(%rsi)
	movq	%r13, 0x60(%rsi)
	movq	%r14, 0x68(%rsi)
	movq	%r15, 0x70(%rsi)
	movq	%cr2, %rax
	movq	%rax, 0x78(%rsi)

	/* %rdi = 0 means we took an exit */
	xorq	%rdi, %rdi

restore_host_svm:
	popq	%rbx
	popq	%rbp
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	
	/*
	 * Restore saved MSRs
	 */
	popq	%rdx
	popq	%rax
	movq	$MSR_SFMASK, %rcx
	wrmsr

	/* XXX - unused? */
	popq	%rdx
	popq	%rax
	movq	$MSR_CSTAR, %rcx
	wrmsr
	
	popq	%rdx
	popq	%rax
	movq	$MSR_LSTAR, %rcx
	wrmsr

	popq	%rdx
	popq	%rax
	movq	$MSR_STAR, %rcx
	wrmsr

	/*
	 * popw %gs will reset gsbase to 0, so preserve it
	 * first. This is to accommodate possibly lazy-switched
	 * selectors from above
	 */
	cli	/* XXX not needed on amd due to implicit clgi on #vmexit */
	popq	%rdx
	popq	%rax
	movq	$MSR_KERNELGSBASE, %rcx
	wrmsr

	popw	%gs
	popq	%rdx
	popq	%rax
	movq	$MSR_GSBASE, %rcx
	wrmsr	

	popw	%fs
	popq	%rdx
	popq	%rax
	movq	$MSR_FSBASE, %rcx
	wrmsr

	popw	%ax
	movw	%ax, %ss
	popw	%ax
	movw	%ax, %ds
	popw	%ax
	movw	%ax, %es

	xorq	%rax, %rax
	popw	%ax		/* ax = saved TR */

	popq	%rdx
	addq	$0x2, %rdx
	movq	(%rdx), %rdx

	/* rdx = GDTR base addr */
	andb	$0xF9, 5(%rdx, %rax)
	
	ltrw	%ax

	popfq

	movq	%rdi, %rax
	stgi
	sti

	ret	
@


1.7
log
@Use explicit operand with SVM instructions as clang doesn't recognize the
implicit form.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm_support.S,v 1.6 2017/02/20 07:36:20 mlarkin Exp $	*/
d20 1
d158 3
a360 1
	cli
a376 1
	sti
@


1.6
log
@SVM: asm support for SVM/RVI
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm_support.S,v 1.5 2016/12/20 07:05:24 kettenis Exp $	*/
d475 3
a477 3
	vmload
	vmrun
	vmsave
@


1.5
log
@Fix operand of pushq instruction; clang's integrated assembler is less
forgiving than gas and insists that we use a 64-bit integer.  No binary
change.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm_support.S,v 1.4 2016/05/16 01:51:23 deraadt Exp $	*/
d41 1
d388 202
@


1.4
log
@place .globals further up, to reduce confusion
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm_support.S,v 1.3 2016/03/15 20:50:22 krw Exp $	*/
d49 1
a49 1
	pushq   %ax
d53 1
a53 1
	pushq   %ax
@


1.3
log
@'accomodate' -> 'accommodate' in comments.

Started by diff from Mical Mazurek.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm_support.S,v 1.2 2015/11/16 10:08:41 mpi Exp $	*/
a29 3
	.text
	.code64
	.align 16
d41 4
@


1.2
log
@Some minor tweaks:

- Add $OpenBSD$ tag,
- constify "struct cfattach",
- Use <uvm/uvm_extern.h> rather than <uvm/uvm.h>, it's enough.
- Keep the "struct vm" private.  This allows us to not pull <uvm/uvm_extern.h>
  in <macine/vmmvar.h>
- Prefer DPRINTF() for debug macro as dprintf(3) is a standard function name.
- Add vmm_debug and fix VMM_DEBUG build
- Remove unneeded <sys/rwlock.h> from <machine/vmmvar.h>
- Kill whitespaces

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d352 1
a352 1
	 * first. This is to accomodate possibly lazy-switched
@


1.1
log
@
vmm(4) kernel code

circulated on hackers@@, no objections. Disabled by default.
@
text
@d1 1
@

