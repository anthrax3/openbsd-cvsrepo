head	1.38;
access;
symbols
	OPENBSD_6_1:1.38.0.8
	OPENBSD_6_1_BASE:1.38
	OPENBSD_6_0:1.38.0.4
	OPENBSD_6_0_BASE:1.38
	OPENBSD_5_9:1.38.0.2
	OPENBSD_5_9_BASE:1.38
	OPENBSD_5_8:1.37.0.4
	OPENBSD_5_8_BASE:1.37
	OPENBSD_5_7:1.35.0.2
	OPENBSD_5_7_BASE:1.35
	OPENBSD_5_6:1.28.0.4
	OPENBSD_5_6_BASE:1.28
	OPENBSD_5_5:1.19.0.4
	OPENBSD_5_5_BASE:1.19
	OPENBSD_5_4:1.13.0.2
	OPENBSD_5_4_BASE:1.13
	OPENBSD_5_3:1.4.0.2
	OPENBSD_5_3_BASE:1.4
	OPENBSD_5_2:1.1.0.2
	OPENBSD_5_2_BASE:1.1;
locks; strict;
comment	@ * @;


1.38
date	2015.08.21.07.01.38;	author mlarkin;	state Exp;
branches;
next	1.37;
commitid	q2hUARV0iqgQ4kaw;

1.37
date	2015.05.05.02.13.46;	author guenther;	state Exp;
branches;
next	1.36;
commitid	dNPv28CJI5BxtRGW;

1.36
date	2015.03.14.03.38.46;	author jsg;	state Exp;
branches;
next	1.35;
commitid	p4LJxGKbi0BU2cG6;

1.35
date	2015.02.11.00.56.14;	author dlg;	state Exp;
branches;
next	1.34;
commitid	cupWxGdBwcE1dOxu;

1.34
date	2014.12.18.05.33.48;	author mlarkin;	state Exp;
branches;
next	1.33;
commitid	JVgpC5JUPLFOXy8N;

1.33
date	2014.12.08.07.12.37;	author mlarkin;	state Exp;
branches;
next	1.32;
commitid	9gsjQWzKEbWkbtZI;

1.32
date	2014.11.22.18.31.46;	author mlarkin;	state Exp;
branches;
next	1.31;
commitid	OZhulzwbZ5ey9UWV;

1.31
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.30;
commitid	yv0ECmCdICvq576h;

1.30
date	2014.11.08.08.18.37;	author mlarkin;	state Exp;
branches;
next	1.29;
commitid	cdNw14OmxaQkiOXD;

1.29
date	2014.10.01.19.41.06;	author mlarkin;	state Exp;
branches;
next	1.28;
commitid	8DlKHWU83o4MRkyW;

1.28
date	2014.07.20.19.47.53;	author deraadt;	state Exp;
branches;
next	1.27;
commitid	ITyy4ODprXXhxf1d;

1.27
date	2014.07.20.18.05.21;	author mlarkin;	state Exp;
branches;
next	1.26;
commitid	F1K1yInguabWnn54;

1.26
date	2014.07.16.17.44.16;	author mlarkin;	state Exp;
branches;
next	1.25;
commitid	hCkTucoKUqYndTAb;

1.25
date	2014.07.09.15.03.12;	author mlarkin;	state Exp;
branches;
next	1.24;
commitid	umg7lvIvVAUp4krC;

1.24
date	2014.07.09.14.35.24;	author mlarkin;	state Exp;
branches;
next	1.23;
commitid	9gbzBlBRZ6WLeVWv;

1.23
date	2014.07.09.14.10.25;	author mlarkin;	state Exp;
branches;
next	1.22;
commitid	T9vAIJG20qGyXHaR;

1.22
date	2014.07.09.11.37.16;	author mlarkin;	state Exp;
branches;
next	1.21;
commitid	ZzCjmXn3ZAUY3nHp;

1.21
date	2014.06.11.00.30.25;	author mlarkin;	state Exp;
branches;
next	1.20;
commitid	bWxTVOsFgnDWnvhd;

1.20
date	2014.05.31.06.30.16;	author mlarkin;	state Exp;
branches;
next	1.19;

1.19
date	2014.01.10.22.34.41;	author mlarkin;	state Exp;
branches;
next	1.18;

1.18
date	2014.01.05.23.06.54;	author mlarkin;	state Exp;
branches;
next	1.17;

1.17
date	2013.10.20.20.03.03;	author mlarkin;	state Exp;
branches;
next	1.16;

1.16
date	2013.10.20.11.16.56;	author deraadt;	state Exp;
branches;
next	1.15;

1.15
date	2013.10.20.09.41.31;	author mlarkin;	state Exp;
branches;
next	1.14;

1.14
date	2013.08.24.23.43.36;	author mlarkin;	state Exp;
branches;
next	1.13;

1.13
date	2013.06.04.01.20.23;	author pirofti;	state Exp;
branches;
next	1.12;

1.12
date	2013.06.01.22.22.50;	author mlarkin;	state Exp;
branches;
next	1.11;

1.11
date	2013.06.01.19.07.47;	author mlarkin;	state Exp;
branches;
next	1.10;

1.10
date	2013.05.31.19.59.59;	author mlarkin;	state Exp;
branches;
next	1.9;

1.9
date	2013.05.30.20.16.54;	author mlarkin;	state Exp;
branches;
next	1.8;

1.8
date	2013.05.30.19.00.59;	author mlarkin;	state Exp;
branches;
next	1.7;

1.7
date	2013.05.30.17.15.38;	author mlarkin;	state Exp;
branches;
next	1.6;

1.6
date	2013.04.23.16.02.10;	author pirofti;	state Exp;
branches;
next	1.5;

1.5
date	2013.03.07.01.26.54;	author mlarkin;	state Exp;
branches;
next	1.4;

1.4
date	2013.01.16.23.10.03;	author mlarkin;	state Exp;
branches;
next	1.3;

1.3
date	2012.10.19.16.38.30;	author mlarkin;	state Exp;
branches;
next	1.2;

1.2
date	2012.10.10.23.32.58;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	2012.07.13.10.30.37;	author mlarkin;	state Exp;
branches;
next	;


desc
@@


1.38
log
@
use vaddr_t for kernel va range calculation instead of paddr_t. No binary
change but using paddr_t here wasn't correct - better to clean it up.
@
text
@/*	$OpenBSD: hibernate_machdep.c,v 1.37 2015/05/05 02:13:46 guenther Exp $	*/

/*
 * Copyright (c) 2012 Mike Larkin <mlarkin@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>
#include <sys/buf.h>
#include <sys/conf.h>
#include <sys/device.h>
#include <sys/disk.h>
#include <sys/disklabel.h>
#include <sys/hibernate.h>
#include <sys/timeout.h>
#include <sys/malloc.h>
#include <sys/kcore.h>
#include <sys/atomic.h>

#include <dev/acpi/acpivar.h>

#include <uvm/uvm_extern.h>
#include <uvm/uvm_pmemrange.h>

#include <machine/cpu.h>
#include <machine/hibernate_var.h>
#include <machine/pte.h>
#include <machine/pmap.h>

#ifdef MULTIPROCESSOR
#include <machine/mpbiosvar.h>
#endif /* MULTIPROCESSOR */

#include "acpi.h"
#include "wd.h"
#include "ahci.h"
#include "softraid.h"
#include "sd.h"

/* Hibernate support */
void    hibernate_enter_resume_4k_pte(vaddr_t, paddr_t);
void    hibernate_enter_resume_2m_pde(vaddr_t, paddr_t);

extern	void hibernate_resume_machdep(void);
extern	void hibernate_flush(void);
extern	caddr_t start, end;
extern	int mem_cluster_cnt;
extern  phys_ram_seg_t mem_clusters[];
extern	bios_memmap_t *bios_memmap;
extern	struct hibernate_state *hibernate_state;

/*
 * amd64 MD Hibernate functions
 *
 * see amd64 hibernate.h for lowmem layout used during hibernate
 */

/*
 * Returns the hibernate write I/O function to use on this machine
 */
hibio_fn
get_hibernate_io_function(dev_t dev)
{
	char *blkname = findblkname(major(dev));

	if (blkname == NULL)
		return NULL;

#if NWD > 0
	if (strcmp(blkname, "wd") == 0) {
		extern int wd_hibernate_io(dev_t dev, daddr_t blkno,
		    vaddr_t addr, size_t size, int op, void *page);
		return wd_hibernate_io;
	}
#endif
#if NSD > 0
	if (strcmp(blkname, "sd") == 0) {
		extern struct cfdriver sd_cd;
		extern int ahci_hibernate_io(dev_t dev, daddr_t blkno,
		    vaddr_t addr, size_t size, int op, void *page);
		extern int sr_hibernate_io(dev_t dev, daddr_t blkno,
		    vaddr_t addr, size_t size, int op, void *page);
		struct device *dv = disk_lookup(&sd_cd, DISKUNIT(dev));

#if NAHCI > 0
		if (dv && dv->dv_parent && dv->dv_parent->dv_parent &&
		    strcmp(dv->dv_parent->dv_parent->dv_cfdata->cf_driver->cd_name,
		    "ahci") == 0)
			return ahci_hibernate_io;
#endif
#if NSOFTRAID > 0
		if (dv && dv->dv_parent && dv->dv_parent->dv_parent &&
		    strcmp(dv->dv_parent->dv_parent->dv_cfdata->cf_driver->cd_name,
		    "softraid") == 0)
			return sr_hibernate_io;
	}
#endif
#endif /* NSD > 0 */
	return NULL;
}

/*
 * Gather MD-specific data and store into hiber_info
 */
int
get_hibernate_info_md(union hibernate_info *hiber_info)
{
	int i;
	bios_memmap_t *bmp;

	/* Calculate memory ranges */
	hiber_info->nranges = mem_cluster_cnt;
	hiber_info->image_size = 0;

	for (i = 0; i < mem_cluster_cnt; i++) {
		hiber_info->ranges[i].base = mem_clusters[i].start;
		hiber_info->ranges[i].end = mem_clusters[i].size + mem_clusters[i].start;
		hiber_info->image_size += hiber_info->ranges[i].end -
		    hiber_info->ranges[i].base;
	}

#if NACPI > 0
	/* Record ACPI trampoline code page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = ACPI_TRAMPOLINE;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;

	/* Record ACPI trampoline data page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = ACPI_TRAMP_DATA;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;
#endif
#ifdef MULTIPROCESSOR
	/* Record MP trampoline code page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base = MP_TRAMPOLINE;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;

	/* Record MP trampoline data page */
	if (hiber_info->nranges >= VM_PHYSSEG_MAX)
		return (1);
	hiber_info->ranges[hiber_info->nranges].base =
		MP_TRAMP_DATA;
	hiber_info->ranges[hiber_info->nranges].end =
	    hiber_info->ranges[hiber_info->nranges].base + PAGE_SIZE;
	hiber_info->image_size += PAGE_SIZE;
	hiber_info->nranges++;
#endif

	for (bmp = bios_memmap; bmp->type != BIOS_MAP_END; bmp++) {
		/* Skip non-NVS ranges (already processed) */
		if (bmp->type != BIOS_MAP_NVS)
			continue;
		if (hiber_info->nranges >= VM_PHYSSEG_MAX)
			return (1);

		i = hiber_info->nranges;	
		hiber_info->ranges[i].base = round_page(bmp->addr);
		hiber_info->ranges[i].end = trunc_page(bmp->addr + bmp->size);
		hiber_info->image_size += hiber_info->ranges[i].end -
			hiber_info->ranges[i].base;
		hiber_info->nranges++;
	}

	hibernate_sort_ranges(hiber_info);

	return (0);
}

/*
 * Enter a mapping for va->pa in the resume pagetable, using
 * the specified size.
 *
 * size : 0 if a 4KB mapping is desired
 *        1 if a 2MB mapping is desired
 */
void
hibernate_enter_resume_mapping(vaddr_t va, paddr_t pa, int size)
{
	if (size)
		return hibernate_enter_resume_2m_pde(va, pa);
	else
		return hibernate_enter_resume_4k_pte(va, pa);
}

/*
 * Enter a 2MB PDE mapping for the supplied VA/PA into the resume-time pmap
 */
void
hibernate_enter_resume_2m_pde(vaddr_t va, paddr_t pa)
{
	pt_entry_t *pde, npde;

	if (va < NBPD_L4) {
		if (va < NBPD_L3) {
			/* First 512GB and 1GB are already mapped */
			pde = (pt_entry_t *)(HIBERNATE_PD_LOW +
				(pl2_pi(va) * sizeof(pt_entry_t)));
			npde = (pa & PG_LGFRAME) | 
				PG_RW | PG_V | PG_M | PG_PS | PG_U;
			*pde = npde;
		} else {
			/* Map the 1GB containing region */
			pde = (pt_entry_t *)(HIBERNATE_PDPT_LOW +
				(pl3_pi(va) * sizeof(pt_entry_t)));
			npde = (HIBERNATE_PD_LOW2) | PG_RW | PG_V;
			*pde = npde;

			/* Map 2MB page */
			pde = (pt_entry_t *)(HIBERNATE_PD_LOW2 +
				(pl2_pi(va) * sizeof(pt_entry_t)));
			npde = (pa & PG_LGFRAME) |
				PG_RW | PG_V | PG_M | PG_PS | PG_U;
			*pde = npde; 
		}
	} else {
		/* First map the 512GB containing region */
		pde = (pt_entry_t *)(HIBERNATE_PML4T +
			(pl4_pi(va) * sizeof(pt_entry_t)));
		npde = (HIBERNATE_PDPT_HI) | PG_RW | PG_V;
		*pde = npde;

		/* Map the 1GB containing region */
		pde = (pt_entry_t *)(HIBERNATE_PDPT_HI +
			(pl3_pi(va) * sizeof(pt_entry_t)));
		npde = (HIBERNATE_PD_HI) | PG_RW | PG_V;
		*pde = npde;

		/* Map the 2MB page */
		pde = (pt_entry_t *)(HIBERNATE_PD_HI +
			(pl2_pi(va) * sizeof(pt_entry_t)));
		npde = (pa & PG_LGFRAME) | PG_RW | PG_V | PG_PS;
		*pde = npde;
	}
}

/*
 * Enter a 4KB PTE mapping for the supplied VA/PA into the resume-time pmap.
 */
void
hibernate_enter_resume_4k_pte(vaddr_t va, paddr_t pa)
{
	pt_entry_t *pde, npde;

	/* Mappings entered here must be in the first 2MB VA */
	KASSERT(va < NBPD_L2);

	/* Map the page */
	pde = (pt_entry_t *)(HIBERNATE_PT_LOW +
		(pl1_pi(va) * sizeof(pt_entry_t)));
	npde = (pa & PMAP_PA_MASK) | PG_RW | PG_V | PG_M | PG_U;
	*pde = npde;
}

/*
 * Create the resume-time page table. This table maps the image(pig) area,
 * the kernel text area, and various utility pages for use during resume,
 * since we cannot overwrite the resuming kernel's page table during inflate
 * and expect things to work properly.
 */
void
hibernate_populate_resume_pt(union hibernate_info *hib_info,
    paddr_t image_start, paddr_t image_end)
{
	int phys_page_number, i;
	paddr_t pa;
	vaddr_t kern_start_2m_va, kern_end_2m_va, page;
	vaddr_t piglet_start_va, piglet_end_va;
	pt_entry_t *pde, npde;

	/* Identity map MMU pages */
	pmap_kenter_pa(HIBERNATE_PML4T, HIBERNATE_PML4T, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PDPT_LOW, HIBERNATE_PDPT_LOW, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PDPT_HI, HIBERNATE_PDPT_HI, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PD_LOW, HIBERNATE_PD_LOW, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PD_LOW2, HIBERNATE_PD_LOW2, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PD_HI, HIBERNATE_PD_HI, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PT_LOW, HIBERNATE_PT_LOW, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PT_LOW2, HIBERNATE_PT_LOW2, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_PT_HI, HIBERNATE_PT_HI, PROT_MASK);

	/* Identity map 3 pages for stack */
	pmap_kenter_pa(HIBERNATE_STACK_PAGE, HIBERNATE_STACK_PAGE, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_STACK_PAGE - PAGE_SIZE,
		HIBERNATE_STACK_PAGE - PAGE_SIZE, PROT_MASK);
	pmap_kenter_pa(HIBERNATE_STACK_PAGE - 2*PAGE_SIZE,
		HIBERNATE_STACK_PAGE - 2*PAGE_SIZE, PROT_MASK);
	pmap_activate(curproc);

	bzero((caddr_t)HIBERNATE_PML4T, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDPT_LOW, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDPT_HI, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PD_LOW, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PD_LOW2, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PD_HI, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PT_LOW, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PT_LOW2, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PT_HI, PAGE_SIZE);
	bzero((caddr_t)(HIBERNATE_STACK_PAGE - 3*PAGE_SIZE) , 3*PAGE_SIZE);

	/* First 512GB PML4E */
	pde = (pt_entry_t *)(HIBERNATE_PML4T +
		(pl4_pi(0) * sizeof(pt_entry_t)));
	npde = (HIBERNATE_PDPT_LOW) | PG_RW | PG_V;
	*pde = npde;

	/* First 1GB PDPTE */
	pde = (pt_entry_t *)(HIBERNATE_PDPT_LOW +
		(pl3_pi(0) * sizeof(pt_entry_t)));
	npde = (HIBERNATE_PD_LOW) | PG_RW | PG_V;
	*pde = npde;
	
	/* PD for first 2MB */
	pde = (pt_entry_t *)(HIBERNATE_PD_LOW +
		(pl2_pi(0) * sizeof(pt_entry_t)));
	npde = (HIBERNATE_PT_LOW) | PG_RW | PG_V;
	*pde = npde;

	/*
	 * Identity map low physical pages.
	 * See arch/amd64/include/hibernate_var.h for page ranges used here.
	 */
	for (i = ACPI_TRAMPOLINE; i <= HIBERNATE_HIBALLOC_PAGE; i += PAGE_SIZE)
		hibernate_enter_resume_mapping(i, i, 0);

	/*
	 * Map current kernel VA range using 2MB pages
	 */
	kern_start_2m_va = (vaddr_t)&start & ~(PAGE_MASK_L2);
	kern_end_2m_va = (vaddr_t)&end & ~(PAGE_MASK_L2);

	/* amd64 kernels load at 16MB phys (on the 8th 2mb page) */
	phys_page_number = 8;

	for (page = kern_start_2m_va; page <= kern_end_2m_va;
	    page += NBPD_L2, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD_L2);
		hibernate_enter_resume_mapping(page, pa, 1);
	}

	/*
	 * Identity map the piglet using 2MB pages.
	 */
	phys_page_number = hib_info->piglet_pa / NBPD_L2;

	/* VA == PA */
	piglet_start_va = hib_info->piglet_pa;
	piglet_end_va = piglet_start_va + HIBERNATE_CHUNK_SIZE * 4;

	for (page = piglet_start_va; page <= piglet_end_va;
	    page += NBPD_L2, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD_L2);
		hibernate_enter_resume_mapping(page, pa, 1);
	}

	/* Unmap MMU pages (stack remains mapped) */
	pmap_kremove(HIBERNATE_PML4T, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PDPT_LOW, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PDPT_HI, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PD_LOW, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PD_LOW2, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PD_HI, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PT_LOW, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PT_LOW2, PAGE_SIZE);
	pmap_kremove(HIBERNATE_PT_HI, PAGE_SIZE);

	pmap_activate(curproc);
}

/*
 * During inflate, certain pages that contain our bookkeeping information
 * (eg, the chunk table, scratch pages, etc) need to be skipped over and
 * not inflated into.
 *
 * Returns 1 if the physical page at dest should be skipped, 0 otherwise
 */
int
hibernate_inflate_skip(union hibernate_info *hib_info, paddr_t dest)
{
	if (dest >= hib_info->piglet_pa &&
	    dest <= (hib_info->piglet_pa + 4 * HIBERNATE_CHUNK_SIZE))
		return (1);

	return (0);
}

void
hibernate_enable_intr_machdep(void)
{
	enable_intr();
}

void
hibernate_disable_intr_machdep(void)
{
	disable_intr();
}

#ifdef MULTIPROCESSOR
/*
 * Quiesce CPUs in a multiprocessor machine before resuming. We need to do
 * this since the APs will be hatched (but waiting for CPUF_GO), and we don't
 * want the APs to be executing code and causing side effects during the
 * unpack operation.
 */
void
hibernate_quiesce_cpus(void)
{
	struct cpu_info *ci;
	u_long i;

	KASSERT(CPU_IS_PRIMARY(curcpu()));

	pmap_kenter_pa(ACPI_TRAMPOLINE, ACPI_TRAMPOLINE, PROT_READ | PROT_EXEC);
	pmap_kenter_pa(ACPI_TRAMP_DATA, ACPI_TRAMP_DATA,
		PROT_READ | PROT_WRITE);

	for (i = 0; i < MAXCPUS; i++) {
		ci = cpu_info[i];
		if (ci == NULL)
			continue;
		if (ci->ci_idle_pcb == NULL)
			continue;
		if ((ci->ci_flags & CPUF_PRESENT) == 0)
			continue;
		if (ci->ci_flags & (CPUF_BSP | CPUF_SP | CPUF_PRIMARY))
			continue;
		atomic_setbits_int(&ci->ci_flags, CPUF_GO | CPUF_PARK);
	}

	/* Wait a bit for the APs to park themselves */
	delay(500000);

	pmap_kremove(ACPI_TRAMPOLINE, PAGE_SIZE);
	pmap_kremove(ACPI_TRAMP_DATA, PAGE_SIZE);
}
#endif /* MULTIPROCESSOR */
@


1.37
log
@emul_native is only used for kernel threads which can't dump core, so
delete coredump_trad(), uvm_coredump(), cpu_coredump(), struct md_coredump,
and various #includes that are superfluous.

This leaves compat_linux processes without a coredump callback.  If that
ability is desired, someone should update it to use coredump_elf32() and
verify the results...

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.36 2015/03/14 03:38:46 jsg Exp $	*/
d352 2
a353 2
	kern_start_2m_va = (paddr_t)&start & ~(PAGE_MASK_L2);
	kern_end_2m_va = (paddr_t)&end & ~(PAGE_MASK_L2);
@


1.36
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.35 2015/02/11 00:56:14 dlg Exp $	*/
a37 1
#include <machine/kcore.h>
@


1.35
log
@need sys/atomic.h to get atomic_{set,clear}bits_int
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.34 2014/12/18 05:33:48 mlarkin Exp $	*/
a36 1
#include <machine/hibernate.h>
@


1.34
log
@
Unmap the MP hatch and ACPI resume trampolines when not in active use.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.33 2014/12/08 07:12:37 mlarkin Exp $	*/
d29 1
@


1.33
log
@
Split the ACPI resume trampoline into code and data, move the data page to
.rodata (kernel copies to the RW page), protect the code page with RX
permissions, protect the code page with RW permissions.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.32 2014/11/22 18:31:46 mlarkin Exp $	*/
d438 4
d457 3
@


1.32
log
@
Split the MP trampoline into two pages, one for code and one for data/stack
and then protect the code page as RX and the data/stack page as RW (NX).

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.31 2014/11/16 12:30:56 deraadt Exp $	*/
d135 1
d139 9
@


1.31
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.30 2014/11/08 08:18:37 mlarkin Exp $	*/
d144 1
d148 10
@


1.30
log
@
No need to keep the temporary mappings for the MMU pages for the resume
time page table after we are done creating it in the resuming kernel.
Note the resume time stack has to remain mapped as it is used during the
initial phase of the hibernate unpack while the original page table is
still being used.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.29 2014/10/01 19:41:06 mlarkin Exp $	*/
d275 9
a283 9
	pmap_kenter_pa(HIBERNATE_PML4T, HIBERNATE_PML4T, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDPT_LOW, HIBERNATE_PDPT_LOW, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDPT_HI, HIBERNATE_PDPT_HI, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PD_LOW, HIBERNATE_PD_LOW, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PD_LOW2, HIBERNATE_PD_LOW2, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PD_HI, HIBERNATE_PD_HI, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PT_LOW, HIBERNATE_PT_LOW, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PT_LOW2, HIBERNATE_PT_LOW2, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PT_HI, HIBERNATE_PT_HI, VM_PROT_ALL);
d286 1
a286 1
	pmap_kenter_pa(HIBERNATE_STACK_PAGE, HIBERNATE_STACK_PAGE, VM_PROT_ALL);
d288 1
a288 1
		HIBERNATE_STACK_PAGE - PAGE_SIZE, VM_PROT_ALL);
d290 1
a290 1
		HIBERNATE_STACK_PAGE - 2*PAGE_SIZE, VM_PROT_ALL);
@


1.29
log
@

Move some hibernate #defines to pte.h and eliminate some duplicate defines
from hibernate code that were already defined in pte.h (with different
names). No functional change.

ok sf@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.28 2014/07/20 19:47:53 deraadt Exp $	*/
d358 13
@


1.28
log
@look up correct dev_t.  This matters for the case where a device is
underlying softraid.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.27 2014/07/20 18:05:21 mlarkin Exp $	*/
d332 2
a333 2
	kern_start_2m_va = (paddr_t)&start & ~(PAGE_MASK_2M);
	kern_end_2m_va = (paddr_t)&end & ~(PAGE_MASK_2M);
@


1.27
log
@
Support hibernating to softraid crypto volumes.

much help and ok from deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.26 2014/07/16 17:44:16 mlarkin Exp $	*/
d95 1
a95 1
		struct device *dv;
a97 1
		dv = disk_lookup(&sd_cd, DISKUNIT(swdevt[0].sw_dev));
@


1.26
log
@
Save and restore NVS ranges when hibernating, as per The Spec.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.25 2014/07/09 15:03:12 mlarkin Exp $	*/
d49 1
d74 1
a74 1
get_hibernate_io_function(void)
d76 1
a76 1
	char *blkname = findblkname(major(swdevt[0].sw_dev));
d88 1
a88 1
#if NAHCI > 0 && NSD > 0
d93 2
d97 1
d103 6
d111 1
@


1.25
log
@

Don't use the suspending kernel's VA mapping for the piglet. It's far
easier and much less error-prone to just identity map it in the resuming
kernel as we have more control over the VA space layout there (otherwise
we are at the mercy of the suspending kernel's placement of the piglet VA).

This diff also increases the size of the piglet to 4 chunks, to avoid an
overwrite issue seen in m2k14 where the start of the kernel text was
overwritten with a bounced chunk before unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.24 2014/07/09 14:35:24 mlarkin Exp $	*/
d60 1
d111 1
d125 2
d134 2
d142 15
@


1.24
log
@

Fixes a resume time page table issue on amd64 if the piglet was located
above 1GB physical (caused by using an incorrect page size mask)

Also removes some unneeded low memory mappings on both amd64 and i386 (this
is a cosmetic fix but makes things easier to debug).
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.23 2014/07/09 14:10:25 mlarkin Exp $	*/
d238 1
a238 1
	paddr_t pa, piglet_start, piglet_end;
d240 1
d314 1
a314 1
	 * Map the piglet
a316 4
	piglet_start = hib_info->piglet_va;
	piglet_end = piglet_start + HIBERNATE_CHUNK_SIZE * 3;
	piglet_start &= ~(PAGE_MASK_2M);
	piglet_end &= ~(PAGE_MASK_2M);
d318 5
a322 1
	for (page = piglet_start; page <= piglet_end ;
d340 1
a340 1
	    dest <= (hib_info->piglet_pa + 3 * HIBERNATE_CHUNK_SIZE))
@


1.23
log
@

Cleanup the chunk placement routine by removing the conflict resolver.
Chunks are now sorted by ascending PA and all chunks are bounced before
unpack. This fixes an issue where the trampoline chunks were being placed
at the end of the unpack ordering, causing overwrite during unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.22 2014/07/09 11:37:16 mlarkin Exp $	*/
d171 2
a172 2
			npde = (pa & L2_MASK) | 
				PG_RW | PG_V | PG_M | PG_PS;
d181 1
a181 1
			/* Map 2MB region */
d184 2
a185 2
			npde = (pa & L2_MASK) |
				PG_RW | PG_V | PG_M | PG_PS;
d201 1
a201 1
		/* Map the requested 2MB region */
d204 1
a204 1
		npde = (pa & L2_MASK) | PG_RW | PG_V | PG_PS;
d217 3
d223 1
a223 1
	npde = (pa & PMAP_PA_MASK) | PG_RW | PG_V;
d291 2
a292 2
	 * Identity map 64KB-640KB physical for tramps and special utility
	 * pages using 4KB mappings
d294 2
a295 3
	for (i = 16; i < 160; i ++) {
		hibernate_enter_resume_mapping(i*PAGE_SIZE, i*PAGE_SIZE, 0);
	}
@


1.22
log
@
Fixes a hibernate issue wherein we locked the kernel lock while hatching
but then parked ourselves in real mode without completing acquisition of
said lock. Also removes the park routine from i386 since we don't need it
(the APs are already parked at the time we start unpack).

discussed with and ok kettenis@@, also ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.21 2014/06/11 00:30:25 mlarkin Exp $	*/
d115 1
a115 1
	for(i = 0; i < mem_cluster_cnt; i++) {
d136 2
@


1.21
log
@
Don't map phys pages < 64KB in the resume page table. We stopped doing this
in the kernel a few months back and there's no reason these pages need to
be mapped during unpack either.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.20 2014/05/31 06:30:16 mlarkin Exp $	*/
d35 1
d363 3
d368 12
a379 5
	/* Start the hatched (but idling) APs */
	cpu_boot_secondary_processors();

	/* Demote the APs to real mode */
        x86_broadcast_ipi(X86_IPI_HALT_REALMODE);
d382 1
a382 2
	delay(1000000);

@


1.20
log
@
Remove some unused code that we added at the 2013 Toronto hackathon but
don't need anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.19 2014/01/10 22:34:41 mlarkin Exp $	*/
d285 1
a285 1
	 * Identity map first 640KB physical for tramps and special utility
d288 1
a288 1
	for (i = 0; i < 160; i ++) {
@


1.19
log
@

Resurrect the "park APs in realmode" idea that we explored back at t2k13
(and which didn't work at that time due to a bug which has since been
fixed). The APs are now demoted to real mode and placed in a HLT loop
while the hibernated image is being unpacked.

Helps my x230 significantly, no more spurious reboots on resume.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.18 2014/01/05 23:06:54 mlarkin Exp $	*/
a320 34
}

/*
 * MD-specific resume preparation (creating resume time pagetables,
 * stacks, etc).
 */
void
hibernate_prepare_resume_machdep(union hibernate_info *hib_info)
{
	paddr_t pa, piglet_end;
	vaddr_t va;

	/*
	 * At this point, we are sure that the piglet's phys space is going to
	 * have been unused by the suspending kernel, but the vaddrs used by
	 * the suspending kernel may or may not be available to us here in the
	 * resuming kernel, so we allocate a new range of VAs for the piglet.
	 * Those VAs will be temporary and will cease to exist as soon as we
	 * switch to the resume PT, so we need to ensure that any VAs required
	 * during inflate are also entered into that map.
	 */

        hib_info->piglet_va = (vaddr_t)km_alloc(HIBERNATE_CHUNK_SIZE*3,
	    &kv_any, &kp_none, &kd_nowait);
        if (!hib_info->piglet_va)
                panic("Unable to allocate vaddr for hibernate resume piglet\n");

	piglet_end = hib_info->piglet_pa + HIBERNATE_CHUNK_SIZE*3;

	for (pa = hib_info->piglet_pa,va = hib_info->piglet_va;
	    pa <= piglet_end; pa += PAGE_SIZE, va += PAGE_SIZE)
		pmap_kenter_pa(va, pa, VM_PROT_ALL);

	pmap_activate(curproc);
@


1.18
log
@

HIBERNATE_SELTABLE is not used anymore. Remove, and reclaim its stolen
page.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.17 2013/10/20 20:03:03 mlarkin Exp $	*/
d401 6
a406 2
	/* Now shut them down */
	acpi_sleep_mp();	
@


1.17
log
@

Realmode park is causing more problems than it's solving. Remove until we
have a better handle on it.

Add an assert in i386 MP hibernate quiesce cpu function to ensure we are
running on the BSP
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.16 2013/10/20 11:16:56 deraadt Exp $	*/
a245 1
	pmap_kenter_pa(HIBERNATE_SELTABLE, HIBERNATE_SELTABLE, VM_PROT_ALL);
a263 1
	bzero((caddr_t)HIBERNATE_SELTABLE, PAGE_SIZE);
@


1.16
log
@Simplify definition of the side-effect-free wd io routine.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.15 2013/10/20 09:41:31 mlarkin Exp $	*/
a397 2
	int i;

d403 2
a404 13
	/* 
	 * Wait for cpus to halt so we know their FPU state has been
	 * saved and their caches have been written back.
	 */
	x86_broadcast_ipi(X86_IPI_HALT_REALMODE);
	for (i = 0; i < ncpus; i++) {
		struct cpu_info *ci = cpu_info[i];

		if (CPU_IS_PRIMARY(ci))
			continue;
		while (ci->ci_flags & CPUF_RUNNING)
			;
	}
@


1.15
log
@

SMEP (on Ivy Bridge and later CPUs) require page protections that include
at least one supervisor mode (U/S bit = 0) setting in higher level paging
structures. This diff removes PG_u flags from the hibernate resume time
pmap (there was really no reason we needed it), to allow hibernate to
work on Ivy Bridge and later CPUs.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.14 2013/08/24 23:43:36 mlarkin Exp $	*/
a49 5
#if NWD > 0
#include <dev/ata/atavar.h>
#include <dev/ata/wdvar.h>
#endif

d79 3
a81 1
	if (strcmp(blkname, "wd") == 0)
d83 1
@


1.14
log
@

Remove call to sched_start_secondary_cpus in MP unhibernate case until we
can more fully understand the side-effects.

Tested by various people back at t2k13, been sitting in my tree since then.
@
text
@d1 1
a1 1
/*	$OpenBSD: hibernate_machdep.c,v 1.13 2013/06/04 01:20:23 pirofti Exp $	*/
d171 1
a171 1
				PG_RW | PG_V | PG_u | PG_M | PG_PS;
d177 1
a177 1
			npde = (HIBERNATE_PD_LOW2) | PG_RW | PG_V | PG_u;
d184 1
a184 1
				PG_RW | PG_V | PG_u | PG_M | PG_PS;
d191 1
a191 1
		npde = (HIBERNATE_PDPT_HI) | PG_RW | PG_V | PG_u;
d197 1
a197 1
		npde = (HIBERNATE_PD_HI) | PG_RW | PG_V | PG_u;
d203 1
a203 1
		npde = (pa & L2_MASK) | PG_RW | PG_V | PG_u | PG_PS;
d219 1
a219 1
	npde = (pa & PMAP_PA_MASK) | PG_RW | PG_V | PG_u;
d273 1
a273 1
	npde = (HIBERNATE_PDPT_LOW) | PG_RW | PG_V | PG_u;
d279 1
a279 1
	npde = (HIBERNATE_PD_LOW) | PG_RW | PG_V | PG_u;
d285 1
a285 1
	npde = (HIBERNATE_PT_LOW) | PG_RW | PG_V | PG_u;
@


1.13
log
@Add RCS ids.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a405 1
	sched_start_secondary_cpus();
@


1.12
log
@

Call x86_ipi_halt_realmode to park APs in real mode before unpacking image

ok deraadt@@
@
text
@d1 2
@


1.11
log
@

Fix offset error when clearing the resume time stack pages
@
text
@d398 4
d406 13
a418 2
	/* Now shut them down */
	acpi_sleep_mp();
@


1.10
log
@

We need to halt the APs on MP hibernate resume or else they will be
executing code possibly causing side effects during the image unpack
operation. But before we can halt the APs, we need to complete their init
(as they will be hatched but idling, possibly with interrupts off).

Introduces MD function hibernate_quiesce_cpus to do this, called from the
MI hibernate resume code.

ok deraadt
@
text
@d266 1
a266 1
	bzero((caddr_t)HIBERNATE_STACK_PAGE, PAGE_SIZE*3);
@


1.9
log
@

Fix a bug in amd64 hibernate introduced when we moved the kernel to load at
16MB physical. Document the same in i386 (i386 was not affected, just
commented for diffability)
@
text
@d387 19
@


1.8
log
@

Make interrupt handling in hibernate resume MI by providing MD-specific
functions to enable and disable interrupts, if needed. If a platform doesnt
need interrupt handling in this way, the MD function can be a no-op.

discussed with pirofti and deraadt
@
text
@d299 3
a301 1
	phys_page_number = 0;
@


1.7
log
@

Even though we reserve 3 pages for the amd64 hibernate stack, we set the
resume time stack pointer to the first page reserved, effectively giving us
only one page to work with. Found when I was debugging some unrelated stack
issues in the resume code.
@
text
@d374 11
@


1.6
log
@Remove obsolete comment, okay mlarkin@@.
@
text
@d250 4
a253 4
	pmap_kenter_pa(HIBERNATE_STACK_PAGE + PAGE_SIZE,
		HIBERNATE_STACK_PAGE + PAGE_SIZE, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_STACK_PAGE + 2*PAGE_SIZE,
		HIBERNATE_STACK_PAGE + 2*PAGE_SIZE, VM_PROT_ALL);
@


1.5
log
@

Reword some wrong comments and some improperly formatted comments and add
information about piglet memory layout. No functional changes.
@
text
@a81 1
	/* XXX - Only support wd hibernate presently */
@


1.4
log
@

MD hibernate goo for amd64
@
text
@d66 2
a326 15
 *
 * On amd64, we use the piglet whose address is contained in hib_info
 * as per the following layout:
 *
 * offset from piglet base      use
 * -----------------------      --------------------
 * 0                            i/o allocation area
 * PAGE_SIZE                    i/o read area
 * 2*PAGE_SIZE                  temp/scratch page
 * 5*PAGE_SIZE			resume stack
 * 6*PAGE_SIZE                  hiballoc arena
 * 7*PAGE_SIZE to 87*PAGE_SIZE  zlib inflate area
 * ...
 * HIBERNATE_CHUNK_SIZE         chunk table
 * 2*HIBERNATE_CHUNK_SIZE	bounce/copy area
d335 7
a341 9
	 * At this point, we are sure that the piglet's phys
	 * space is going to have been unused by the suspending
	 * kernel, but the vaddrs used by the suspending kernel
	 * may or may not be available to us here in the
	 * resuming kernel, so we allocate a new range of VAs
	 * for the piglet. Those VAs will be temporary and will
	 * cease to exist as soon as we switch to the resume
	 * PT, so we need to ensure that any VAs required during
	 * inflate are also entered into that map.
@


1.3
log
@

amd64 hibernate "unpack-time" mmu/pmap code and asm goo. Work in
progress.
@
text
@d36 1
a54 1
void    hibernate_enter_resume_4k_pde(vaddr_t);
d162 6
a167 4
	if (pa < HIBERNATE_512GB) {
		if (pa < HIBERNATE_1GB) {
			pde = s4pde_2m_low(va);
			npde = (pa & PMAP_PA_MASK_2M) |
d171 4
a174 7
			/*
			 * pa in first 512GB, but not first 1GB - first map
			 * the page's 1GB containing region
			 */
			pde = s4pde_1g_low2(va);
			npde = (pa & PMAP_PA_MASK_1G) |
				PG_RW | PG_V | PG_u | PG_PS;
d177 4
a180 3
			/* Finally, map the page's region (2MB) */
			pde = s4pde_2m_low2(va);
			npde = (pa & PMAP_PA_MASK_2M) |
d183 1
a183 1
		}	
d185 4
a188 7
		/*
		 * pa not in first 512GB - first map the page's 512GB
		 * containing region
		 */
		pde = s4pde_512g(va);
		npde = (pa & PMAP_PA_MASK_512G) |
			PG_RW | PG_V | PG_u;
d191 4
a194 4
		/* Next, map the page's 1GB containing region */
		pde = s4pde_1g_hi(va);
		npde = (pa & PMAP_PA_MASK_1G) |
			PG_RW | PG_V | PG_u | PG_PS;
d197 4
a200 4
		/* Finally, map the page's region (2MB) */
		pde = s4pde_2m_hi(va);
		npde = (pa & PMAP_PA_MASK_2M) |
			PG_RW | PG_V | PG_u | PG_M | PG_PS;
d213 4
a216 76
	if (pa < HIBERNATE_512GB) {
		if (pa < HIBERNATE_1GB) {
			/* Map the 2MB region containing the page */
			pde = s4pde_2m_low(va);
			npde = (pa & PMAP_PA_MASK_2M) |
				PG_RW | PG_V | PG_u | PG_M | PG_PS;
			*pde = npde;

			/* Map the page */
			pde = s4pte_4k_low(va);
			npde = (pa & PMAP_PA_MASK) |
				PG_RW | PG_V | PG_u | PG_M | PG_PS;
			*pde = npde;
		} else {
			/*
			 * pa in first 512GB, but not first 1GB - first map
			 * the page's 1GB containing region
			 */
			pde = s4pde_1g_low2(va);
			npde = (pa & PMAP_PA_MASK_1G) |
				PG_RW | PG_V | PG_u | PG_PS;
			*pde = npde;

			/* Next, map the page's region (2MB) */
			pde = s4pde_2m_low2(va);
			npde = (pa & PMAP_PA_MASK_2M) |
				PG_RW | PG_V | PG_u | PG_M | PG_PS;
			*pde = npde; 

			/* Finally, map the page */
			pde = s4pte_4k_low2(va);
			npde = (pa & PMAP_PA_MASK) |
				PG_RW | PG_V | PG_u | PG_M | PG_PS;
			*pde = npde;
		}	
	} else {
		/*
		 * pa not in first 512GB - first map the page's 512GB
		 * containing region
		 */
		pde = s4pde_512g(va);
		npde = (pa & PMAP_PA_MASK_512G) |
			PG_RW | PG_V | PG_u;
		*pde = npde;

		/* Next, map the page's 1GB containing region */
		pde = s4pde_1g_hi(va);
		npde = (pa & PMAP_PA_MASK_1G) |
			PG_RW | PG_V | PG_u | PG_PS;
		*pde = npde;

		/* Next, map the page's region (2MB) */
		pde = s4pde_2m_hi(va);
		npde = (pa & PMAP_PA_MASK_2M) |
			PG_RW | PG_V | PG_u | PG_M | PG_PS;
		*pde = npde;

		/* Finally, map the page */
		pde = s4pte_4k_hi(va);
		npde = (pa & PMAP_PA_MASK) |
			PG_RW | PG_V | PG_u | PG_M | PG_PS;
		*pde = npde;
	}
}

/*
 * Enter a 4KB PDE mapping for the supplied VA into the resume-time pmap.
 * Note - on amd64, this is only used for low pages (< 2MB phys)
 */
void
hibernate_enter_resume_4k_pde(vaddr_t va)
{
	pt_entry_t *pde, npde;

	pde = s4pte_4k_low(va);
	npde = (HIBERNATE_PDE_LOW & PMAP_PA_MASK) | PG_RW | PG_V | PG_u | PG_M;
d233 1
d235 13
a247 10
	/* Identity map MMU and stack pages */
	pmap_kenter_pa(HIBERNATE_PML4_PAGE, HIBERNATE_PML4_PAGE, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PML4E_LOW, HIBERNATE_PML4E_LOW, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PML4E_HI, HIBERNATE_PML4E_HI, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDPTE_LOW, HIBERNATE_PDPTE_LOW, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDPTE_LOW2, HIBERNATE_PDPTE_LOW2, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDPTE_HI, HIBERNATE_PDPTE_HI, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDE_LOW, HIBERNATE_PDE_LOW, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDE_LOW2, HIBERNATE_PDE_LOW2, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PDE_HI, HIBERNATE_PDE_HI, VM_PROT_ALL);
d249 4
d255 17
a271 10
	bzero((caddr_t)HIBERNATE_PML4_PAGE, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PML4E_LOW, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PML4E_HI, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDPTE_LOW, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDPTE_LOW2, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDPTE_HI, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDE_LOW, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDE_LOW2, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PDE_HI, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_STACK_PAGE, PAGE_SIZE);
d273 11
a283 2
	/* PDE for low pages */
	hibernate_enter_resume_4k_pde(0);
d294 1
a294 1
	 * Map current kernel VA range using 2M pages
a306 12
	 * Identity map the image (pig) area
	 */
	phys_page_number = image_start / NBPD_L2;
	image_start &= ~(PAGE_MASK_2M);
	image_end &= ~(PAGE_MASK_2M);
	for (page = image_start; page <= image_end ;
	    page += NBPD_L2, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD_L2);
		hibernate_enter_resume_mapping(page, pa, 1);
	}

	/*
d314 1
d389 1
@


1.2
log
@Oops.  Need to handle the case of nfs diskless machines, where the block
device name is NULL.
From Tim Wiess
@
text
@d55 1
a55 1
void    hibernate_enter_resume_4m_pde(vaddr_t, paddr_t);
d65 1
a65 1
 * i386 MD Hibernate functions
d143 1
a143 1
 *        1 if a 4MB mapping is desired
d149 1
a149 1
		return hibernate_enter_resume_4m_pde(va, pa);
d155 1
a155 1
 * Enter a 4MB PDE mapping for the supplied VA/PA into the resume-time pmap
d158 1
a158 1
hibernate_enter_resume_4m_pde(vaddr_t va, paddr_t pa)
d162 44
a205 3
	pde = s4pde_4m(va);
	npde = (pa & PMAP_PA_MASK_4M) | PG_RW | PG_V | PG_u | PG_M | PG_PS;
	*pde = npde;
d214 1
a214 1
	pt_entry_t *pte, npte;
d216 63
a278 3
	pte = s4pte_4k(va);
	npte = (pa & PMAP_PA_MASK) | PG_RW | PG_V | PG_u | PG_M;
	*pte = npte;
d283 1
d290 2
a291 2
	pde = s4pde_4k(va);
	npde = (HIBERNATE_PT_PAGE & PMAP_PA_MASK) | PG_RW | PG_V | PG_u | PG_M;
d307 1
a307 1
	vaddr_t kern_start_4m_va, kern_end_4m_va, page;
d309 10
a318 3
	/* Identity map PD, PT, and stack pages */
	pmap_kenter_pa(HIBERNATE_PT_PAGE, HIBERNATE_PT_PAGE, VM_PROT_ALL);
	pmap_kenter_pa(HIBERNATE_PD_PAGE, HIBERNATE_PD_PAGE, VM_PROT_ALL);
d322 9
a330 2
	bzero((caddr_t)HIBERNATE_PT_PAGE, PAGE_SIZE);
	bzero((caddr_t)HIBERNATE_PD_PAGE, PAGE_SIZE);
d345 1
a345 1
	 * Map current kernel VA range using 4M pages
d347 2
a348 2
	kern_start_4m_va = (paddr_t)&start & ~(PAGE_MASK_4M);
	kern_end_4m_va = (paddr_t)&end & ~(PAGE_MASK_4M);
d351 3
a353 3
	for (page = kern_start_4m_va; page <= kern_end_4m_va;
	    page += NBPD, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD);
d360 3
a362 3
	phys_page_number = image_start / NBPD;
	image_start &= ~(PAGE_MASK_4M);
	image_end &= ~(PAGE_MASK_4M);
d364 2
a365 2
	    page += NBPD, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD);
d372 1
a372 1
	phys_page_number = hib_info->piglet_pa / NBPD;
d375 2
a376 2
	piglet_start &= ~(PAGE_MASK_4M);
	piglet_end &= ~(PAGE_MASK_4M);
d378 2
a379 2
	    page += NBPD, phys_page_number++) {
		pa = (paddr_t)(phys_page_number * NBPD);
@


1.1
log
@

Starting point for amd64 hibernate ... some goo copied from i386.
This does not yet work for amd64 - getting the structure into the tree so
others can help.
@
text
@d74 5
d81 1
a81 1
	if (strcmp(findblkname(major(swdevt[0].sw_dev)), "wd") == 0)
d85 1
a85 1
	if (strcmp(findblkname(major(swdevt[0].sw_dev)), "sd") == 0) {
@

