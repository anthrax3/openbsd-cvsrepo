head	1.49;
access;
symbols
	OPENBSD_6_1_BASE:1.49
	OPENBSD_6_0:1.49.0.4
	OPENBSD_6_0_BASE:1.49
	OPENBSD_5_9:1.49.0.2
	OPENBSD_5_9_BASE:1.49
	OPENBSD_5_8:1.48.0.6
	OPENBSD_5_8_BASE:1.48
	OPENBSD_5_7:1.48.0.2
	OPENBSD_5_7_BASE:1.48
	OPENBSD_5_6:1.45.0.4
	OPENBSD_5_6_BASE:1.45
	OPENBSD_5_5:1.41.0.4
	OPENBSD_5_5_BASE:1.41
	OPENBSD_5_4:1.40.0.4
	OPENBSD_5_4_BASE:1.40
	OPENBSD_5_3:1.40.0.2
	OPENBSD_5_3_BASE:1.40
	OPENBSD_5_2:1.38.0.6
	OPENBSD_5_2_BASE:1.38
	OPENBSD_5_1_BASE:1.38
	OPENBSD_5_1:1.38.0.4
	OPENBSD_5_0:1.38.0.2
	OPENBSD_5_0_BASE:1.38
	OPENBSD_4_9:1.35.0.2
	OPENBSD_4_9_BASE:1.35
	OPENBSD_4_8:1.32.0.2
	OPENBSD_4_8_BASE:1.32
	OPENBSD_4_7:1.29.0.2
	OPENBSD_4_7_BASE:1.29
	OPENBSD_4_6:1.28.0.4
	OPENBSD_4_6_BASE:1.28
	OPENBSD_4_5:1.14.0.2
	OPENBSD_4_5_BASE:1.14
	OPENBSD_4_4:1.12.0.2
	OPENBSD_4_4_BASE:1.12
	OPENBSD_4_3:1.10.0.2
	OPENBSD_4_3_BASE:1.10
	OPENBSD_4_2:1.8.0.2
	OPENBSD_4_2_BASE:1.8
	OPENBSD_4_1:1.7.0.2
	OPENBSD_4_1_BASE:1.7
	OPENBSD_4_0:1.6.0.2
	OPENBSD_4_0_BASE:1.6
	OPENBSD_3_9:1.4.0.2
	OPENBSD_3_9_BASE:1.4
	OPENBSD_3_8:1.2.0.4
	OPENBSD_3_8_BASE:1.2
	OPENBSD_3_7:1.2.0.2
	OPENBSD_3_7_BASE:1.2
	OPENBSD_3_6:1.1.0.6
	OPENBSD_3_6_BASE:1.1
	SMP_SYNC_A:1.1
	SMP_SYNC_B:1.1
	OPENBSD_3_5:1.1.0.4
	OPENBSD_3_5_BASE:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.49
date	2015.12.17.17.16.04;	author tedu;	state Exp;
branches;
next	1.48;
commitid	P4aSHLw81n2bU141;

1.48
date	2015.01.27.05.10.30;	author dlg;	state Exp;
branches;
next	1.47;
commitid	BOnmhejIUV5eckqc;

1.47
date	2015.01.24.15.13.55;	author kettenis;	state Exp;
branches;
next	1.46;
commitid	Ol0pICcR5PormuCL;

1.46
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.45;
commitid	yv0ECmCdICvq576h;

1.45
date	2014.07.12.18.44.41;	author tedu;	state Exp;
branches;
next	1.44;
commitid	uKVPYMN2MLxdZxzH;

1.44
date	2014.07.11.09.36.25;	author mpi;	state Exp;
branches;
next	1.43;
commitid	vsYjSRfS3Y783BvW;

1.43
date	2014.05.04.20.09.15;	author sf;	state Exp;
branches;
next	1.42;

1.42
date	2014.04.01.09.05.03;	author mpi;	state Exp;
branches;
next	1.41;

1.41
date	2013.12.12.21.04.50;	author kettenis;	state Exp;
branches;
next	1.40;

1.40
date	2012.12.08.12.04.21;	author mpi;	state Exp;
branches;
next	1.39;

1.39
date	2012.12.05.23.20.10;	author deraadt;	state Exp;
branches;
next	1.38;

1.38
date	2011.07.03.18.31.02;	author oga;	state Exp;
branches;
next	1.37;

1.37
date	2011.06.23.20.44.38;	author ariane;	state Exp;
branches;
next	1.36;

1.36
date	2011.04.02.16.37.39;	author beck;	state Exp;
branches;
next	1.35;

1.35
date	2010.12.26.15.40.58;	author miod;	state Exp;
branches;
next	1.34;

1.34
date	2010.11.20.20.33.23;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2010.09.06.19.05.48;	author kettenis;	state Exp;
branches;
next	1.32;

1.32
date	2010.05.20.05.46.53;	author oga;	state Exp;
branches;
next	1.31;

1.31
date	2010.04.08.00.55.25;	author oga;	state Exp;
branches;
next	1.30;

1.30
date	2010.03.29.19.21.58;	author oga;	state Exp;
branches;
next	1.29;

1.29
date	2009.08.11.17.15.54;	author oga;	state Exp;
branches;
next	1.28;

1.28
date	2009.06.07.02.30.34;	author oga;	state Exp;
branches;
next	1.27;

1.27
date	2009.04.21.17.05.29;	author oga;	state Exp;
branches;
next	1.26;

1.26
date	2009.04.20.00.42.05;	author oga;	state Exp;
branches;
next	1.25;

1.25
date	2009.04.15.03.35.26;	author oga;	state Exp;
branches;
next	1.24;

1.24
date	2009.04.15.02.03.33;	author oga;	state Exp;
branches;
next	1.23;

1.23
date	2009.04.15.01.58.27;	author oga;	state Exp;
branches;
next	1.22;

1.22
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.21;

1.21
date	2009.04.09.03.08.36;	author dlg;	state Exp;
branches;
next	1.20;

1.20
date	2009.04.09.03.06.35;	author dlg;	state Exp;
branches;
next	1.19;

1.19
date	2009.04.08.20.58.24;	author marco;	state Exp;
branches;
next	1.18;

1.18
date	2009.04.04.13.48.55;	author dlg;	state Exp;
branches;
next	1.17;

1.17
date	2009.03.30.18.12.24;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	2009.03.10.15.03.16;	author oga;	state Exp;
branches;
next	1.15;

1.15
date	2009.03.07.15.34.34;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2009.02.05.01.15.20;	author oga;	state Exp;
branches;
next	1.13;

1.13
date	2008.12.03.15.46.06;	author oga;	state Exp;
branches;
next	1.12;

1.12
date	2008.06.26.05.42.09;	author ray;	state Exp;
branches;
next	1.11;

1.11
date	2008.06.23.00.27.11;	author dlg;	state Exp;
branches;
next	1.10;

1.10
date	2007.09.17.15.34.38;	author chl;	state Exp;
branches;
next	1.9;

1.9
date	2007.09.03.01.09.09;	author krw;	state Exp;
branches;
next	1.8;

1.8
date	2007.05.29.21.00.50;	author jason;	state Exp;
branches;
next	1.7;

1.7
date	2007.01.15.23.19.05;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2006.06.08.03.18.08;	author weingart;	state Exp;
branches;
next	1.5;

1.5
date	2006.05.10.12.36.39;	author krw;	state Exp;
branches;
next	1.4;

1.4
date	2006.03.01.21.51.37;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	2005.10.26.18.46.06;	author martin;	state Exp;
branches;
next	1.2;

1.2
date	2004.11.09.19.17.01;	author claudio;	state Exp;
branches;
next	1.1;

1.1
date	2004.01.28.01.39.38;	author mickey;	state Exp;
branches;
next	;


desc
@@


1.49
log
@add a size to free. from Mathieu
@
text
@/*	$OpenBSD: bus_dma.c,v 1.48 2015/01/27 05:10:30 dlg Exp $	*/
/*	$NetBSD: bus_dma.c,v 1.3 2003/05/07 21:33:58 fvdl Exp $	*/

/*-
 * Copyright (c) 1996, 1997, 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Charles M. Hannum and by Jason R. Thorpe of the Numerical Aerospace
 * Simulation Facility, NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * The following is included because _bus_dma_uiomove is derived from
 * uiomove() in kern_subr.c.
 */

/*
 * Copyright (c) 1982, 1986, 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This software was developed by the Computer Systems Engineering group
 * at Lawrence Berkeley Laboratory under DARPA contract BG 91-66 and
 * contributed to Berkeley.
 *
 * All advertising materials mentioning features or use of this software
 * must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Lawrence Berkeley Laboratory.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/proc.h>

#include <machine/bus.h>

#include <uvm/uvm_extern.h>

int _bus_dmamap_load_buffer(bus_dma_tag_t, bus_dmamap_t, void *, bus_size_t,
    struct proc *, int, paddr_t *, int *, int);

/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
int
_bus_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
    bus_size_t maxsegsz, bus_size_t boundary, int flags, bus_dmamap_t *dmamp)
{
	struct bus_dmamap *map;
	void *mapstore;
	size_t mapsize;

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
	mapsize = sizeof(struct bus_dmamap) +
	    (sizeof(bus_dma_segment_t) * (nsegments - 1));
	if ((mapstore = malloc(mapsize, M_DEVBUF,
	    (flags & BUS_DMA_NOWAIT) ?
	        (M_NOWAIT|M_ZERO) : (M_WAITOK|M_ZERO))) == NULL)
		return (ENOMEM);

	map = (struct bus_dmamap *)mapstore;
	map->_dm_size = size;
	map->_dm_segcnt = nsegments;
	map->_dm_maxsegsz = maxsegsz;
	map->_dm_boundary = boundary;
	map->_dm_flags = flags & ~(BUS_DMA_WAITOK|BUS_DMA_NOWAIT);

	*dmamp = map;
	return (0);
}

/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
void
_bus_dmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{
	size_t mapsize;
	
	mapsize = sizeof(struct bus_dmamap) +
		(sizeof(bus_dma_segment_t) * (map->_dm_segcnt - 1));
	free(map, M_DEVBUF, mapsize);
}

/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
int
_bus_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags)
{
	bus_addr_t lastaddr = 0;
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	seg = 0;
	error = _bus_dmamap_load_buffer(t, map, buf, buflen, p, flags,
	    &lastaddr, &seg, 1);
	if (error == 0) {
		map->dm_mapsize = buflen;
		map->dm_nsegs = seg + 1;
	}
	return (error);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
_bus_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map, struct mbuf *m0,
    int flags)
{
	paddr_t lastaddr = 0;
	int seg, error, first;
	struct mbuf *m;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic("_bus_dmamap_load_mbuf: no packet header");
#endif

	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	first = 1;
	seg = 0;
	error = 0;
	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		error = _bus_dmamap_load_buffer(t, map, m->m_data, m->m_len,
		    NULL, flags, &lastaddr, &seg, first);
		first = 0;
	}
	if (error == 0) {
		map->dm_mapsize = m0->m_pkthdr.len;
		map->dm_nsegs = seg + 1;
	}
	return (error);
}

/*
 * Like _bus_dmamap_load(), but for uios.
 */
int
_bus_dmamap_load_uio(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio,
    int flags)
{
	paddr_t lastaddr = 0;
	int seg, i, error, first;
	bus_size_t minlen, resid;
	struct proc *p = NULL;
	struct iovec *iov;
	caddr_t addr;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	resid = uio->uio_resid;
	iov = uio->uio_iov;

	if (uio->uio_segflg == UIO_USERSPACE) {
		p = uio->uio_procp;
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic("_bus_dmamap_load_uio: USERSPACE but no proc");
#endif
	}

	first = 1;
	seg = 0;
	error = 0;
	for (i = 0; i < uio->uio_iovcnt && resid != 0 && error == 0; i++) {
		/*
		 * Now at the first iovec to load.  Load each iovec
		 * until we have exhausted the residual count.
		 */
		minlen = resid < iov[i].iov_len ? resid : iov[i].iov_len;
		addr = (caddr_t)iov[i].iov_base;

		error = _bus_dmamap_load_buffer(t, map, addr, minlen,
		    p, flags, &lastaddr, &seg, first);
		first = 0;

		resid -= minlen;
	}
	if (error == 0) {
		map->dm_mapsize = uio->uio_resid;
		map->dm_nsegs = seg + 1;
	}
	return (error);
}

/*
 * Like _bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
_bus_dmamap_load_raw(bus_dma_tag_t t, bus_dmamap_t map, bus_dma_segment_t *segs,
    int nsegs, bus_size_t size, int flags)
{
	bus_addr_t paddr, baddr, bmask, lastaddr = 0;
	bus_size_t plen, sgsize, mapsize;
	int first = 1;
	int i, seg = 0;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (nsegs > map->_dm_segcnt || size > map->_dm_size)
		return (EINVAL);

	mapsize = size;
	bmask  = ~(map->_dm_boundary - 1);

	for (i = 0; i < nsegs && size > 0; i++) {
		paddr = segs[i].ds_addr;
		plen = MIN(segs[i].ds_len, size);

		while (plen > 0) {
			/*
			 * Compute the segment size, and adjust counts.
			 */
			sgsize = PAGE_SIZE - ((u_long)paddr & PGOFSET);
			if (plen < sgsize)
				sgsize = plen;

			if (paddr > dma_constraint.ucr_high)
				panic("Non dma-reachable buffer at paddr %#lx(raw)",
				    paddr);

			/*
			 * Make sure we don't cross any boundaries.
			 */
			if (map->_dm_boundary > 0) {
				baddr = (paddr + map->_dm_boundary) & bmask;
				if (sgsize > (baddr - paddr))
					sgsize = (baddr - paddr);
			}

			/*
			 * Insert chunk into a segment, coalescing with
			 * previous segment if possible.
			 */
			if (first) {
				map->dm_segs[seg].ds_addr = paddr;
				map->dm_segs[seg].ds_len = sgsize;
				first = 0;
			} else {
				if (paddr == lastaddr &&
				    (map->dm_segs[seg].ds_len + sgsize) <=
				     map->_dm_maxsegsz &&
				    (map->_dm_boundary == 0 ||
				     (map->dm_segs[seg].ds_addr & bmask) ==
				     (paddr & bmask)))
					map->dm_segs[seg].ds_len += sgsize;
				else {
					if (++seg >= map->_dm_segcnt)
						return (EINVAL);
					map->dm_segs[seg].ds_addr = paddr;
					map->dm_segs[seg].ds_len = sgsize;
				}
			}

			paddr += sgsize;
			plen -= sgsize;
			size -= sgsize;

			lastaddr = paddr;
		}
	}

	map->dm_mapsize = mapsize;
	map->dm_nsegs = seg + 1;
	return (0);
}

/*
 * Common function for unloading a DMA map.  May be called by
 * bus-specific DMA map unload functions.
 */
void
_bus_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{
	/*
	 * No resources to free; just mark the mappings as
	 * invalid.
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;
}

/*
 * Common function for DMA map synchronization.  May be called
 * by bus-specific DMA map synchronization functions.
 */
void
_bus_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t addr,
    bus_size_t size, int op)
{
	/* Nothing to do here. */
}

/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */
int
_bus_dmamem_alloc(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags)
{

	/*
	 * XXX in the presence of decent (working) iommus and bouncebuffers
	 * we can then fallback this allocation to a range of { 0, -1 }.
	 * However for now  we err on the side of caution and allocate dma
	 * memory under the 4gig boundary.
	 */
	return (_bus_dmamem_alloc_range(t, size, alignment, boundary,
	    segs, nsegs, rsegs, flags, (bus_addr_t)0, (bus_addr_t)0xffffffff));
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
_bus_dmamem_free(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs)
{
	struct vm_page *m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE(addr);
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}

	uvm_pglistfree(&mlist);
}

/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
int
_bus_dmamem_map(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs,
    size_t size, caddr_t *kvap, int flags)
{
	vaddr_t va, sva;
	size_t ssize;
	bus_addr_t addr;
	int curseg, pmapflags = 0, error;
	const struct kmem_dyn_mode *kd;

	if (nsegs == 1 && (flags & BUS_DMA_NOCACHE) == 0) {
		*kvap = (caddr_t)PMAP_DIRECT_MAP(segs[0].ds_addr);
		return (0);
	}

	if (flags & BUS_DMA_NOCACHE)
		pmapflags |= PMAP_NOCACHE;

	size = round_page(size);
	kd = flags & BUS_DMA_NOWAIT ? &kd_trylock : &kd_waitok;
	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, kd);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	sva = va;
	ssize = size;
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE, va += PAGE_SIZE, size -= PAGE_SIZE) {
			if (size == 0)
				panic("_bus_dmamem_map: size botch");
			error = pmap_enter(pmap_kernel(), va, addr | pmapflags,
			    PROT_READ | PROT_WRITE,
			    PROT_READ | PROT_WRITE | PMAP_WIRED | PMAP_CANFAIL);
			if (error) {
				pmap_update(pmap_kernel());
				km_free((void *)sva, ssize, &kv_any, &kp_none);
				return (error);
			}
		}
	}
	pmap_update(pmap_kernel());

	return (0);
}

/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
void
_bus_dmamem_unmap(bus_dma_tag_t t, caddr_t kva, size_t size)
{

#ifdef DIAGNOSTIC
	if ((u_long)kva & PGOFSET)
		panic("_bus_dmamem_unmap");
#endif
	if (kva >= (caddr_t)PMAP_DIRECT_BASE && kva <= (caddr_t)PMAP_DIRECT_END)
		return;

	km_free(kva, round_page(size), &kv_any, &kp_none);
}

/*
 * Common function for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
paddr_t
_bus_dmamem_mmap(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs, off_t off,
    int prot, int flags)
{
	int i, pmapflags = 0;

	if (flags & BUS_DMA_NOCACHE)
		pmapflags |= PMAP_NOCACHE;

	for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
		if (off & PGOFSET)
			panic("_bus_dmamem_mmap: offset unaligned");
		if (segs[i].ds_addr & PGOFSET)
			panic("_bus_dmamem_mmap: segment unaligned");
		if (segs[i].ds_len & PGOFSET)
			panic("_bus_dmamem_mmap: segment size not multiple"
			    " of page size");
#endif
		if (off >= segs[i].ds_len) {
			off -= segs[i].ds_len;
			continue;
		}

		return ((segs[i].ds_addr + off) | pmapflags);
	}

	/* Page not found. */
	return (-1);
}

/**********************************************************************
 * DMA utility functions
 **********************************************************************/
/*
 * Utility function to load a linear buffer.  lastaddrp holds state
 * between invocations (for multiple-buffer loads).  segp contains
 * the starting segment on entrance, and the ending segment on exit.
 * first indicates if this is the first invocation of this function.
 */
int
_bus_dmamap_load_buffer(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, paddr_t *lastaddrp, int *segp,
    int first)
{
	bus_size_t sgsize;
	bus_addr_t curaddr, lastaddr, baddr, bmask;
	vaddr_t vaddr = (vaddr_t)buf;
	int seg;
	pmap_t pmap;

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = *lastaddrp;
	bmask  = ~(map->_dm_boundary - 1);

	for (seg = *segp; buflen > 0 ; ) {
		/*
		 * Get the physical address for this segment.
		 */
		pmap_extract(pmap, vaddr, (paddr_t *)&curaddr);

		if (curaddr > dma_constraint.ucr_high)
			panic("Non dma-reachable buffer at curaddr %#lx(raw)",
			    curaddr);

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = PAGE_SIZE - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = (curaddr + map->_dm_boundary) & bmask;
			if (sgsize > (baddr - curaddr))
				sgsize = (baddr - curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr = curaddr;
			map->dm_segs[seg].ds_len = sgsize;
			first = 0;
		} else {
			if (curaddr == lastaddr &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			    (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     (curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr = curaddr;
				map->dm_segs[seg].ds_len = sgsize;
			}
		}

		lastaddr = curaddr + sgsize;
		vaddr += sgsize;
		buflen -= sgsize;
	}

	*segp = seg;
	*lastaddrp = lastaddr;

	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */
	return (0);
}

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
int
_bus_dmamem_alloc_range(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags, bus_addr_t low, bus_addr_t high)
{
	paddr_t curaddr, lastaddr;
	struct vm_page *m;
	struct pglist mlist;
	int curseg, error, plaflag;

	/* Always round the size. */
	size = round_page(size);

	segs[0]._ds_boundary = boundary;
	segs[0]._ds_align = alignment;

	/*
	 * Allocate pages from the VM system.
	 */
	plaflag = flags & BUS_DMA_NOWAIT ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & BUS_DMA_ZERO)
		plaflag |= UVM_PLA_ZERO;

	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, low, high, alignment, boundary,
	    &mlist, nsegs, plaflag);
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = TAILQ_FIRST(&mlist);
	curseg = 0;
	lastaddr = segs[curseg].ds_addr = VM_PAGE_TO_PHYS(m);
	segs[curseg].ds_len = PAGE_SIZE;

	for (m = TAILQ_NEXT(m, pageq); m != NULL; m = TAILQ_NEXT(m, pageq)) {
		curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
		if (curseg == nsegs) {
			printf("uvm_pglistalloc returned too many\n");
			panic("_bus_dmamem_alloc_range");
		}
		if (curaddr < low || curaddr >= high) {
			printf("uvm_pglistalloc returned non-sensical"
			    " address 0x%lx\n", curaddr);
			panic("_bus_dmamem_alloc_range");
		}
#endif
		if (curaddr == (lastaddr + PAGE_SIZE))
			segs[curseg].ds_len += PAGE_SIZE;
		else {
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	return (0);
}

@


1.48
log
@this code doesnt need to know about interrupts, so i can trim some
headers and types.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.47 2015/01/24 15:13:55 kettenis Exp $	*/
d152 5
a156 2

	free(map, M_DEVBUF, 0);
@


1.47
log
@Add bus_dmamem_alloc_range(9) to allow drivers to allocate DMA'able memory
within a range that is more (or less) restrictive than the default range.

ok deraadt@@, stsp@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.46 2014/11/16 12:30:56 deraadt Exp $	*/
a97 3
#include <dev/isa/isareg.h>
#include <dev/isa/isavar.h>

a99 7
#include "ioapic.h"

#if NIOAPIC > 0
#include <machine/i82093var.h>
#include <machine/mpbiosvar.h>
#endif

a101 4

#define	IDTVEC(name)	__CONCAT(X,name)
typedef void (vector)(void);
extern vector *IDTVEC(intr)[];
@


1.46
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.45 2014/07/12 18:44:41 tedu Exp $	*/
d427 1
a427 1
	    segs, nsegs, rsegs, flags, (paddr_t)0, (paddr_t)0xffffffff));
d665 1
a665 1
    int flags, paddr_t low, paddr_t high)
@


1.45
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.44 2014/07/11 09:36:25 mpi Exp $	*/
d497 2
a498 2
			    VM_PROT_READ | VM_PROT_WRITE, VM_PROT_READ |
			    VM_PROT_WRITE | PMAP_WIRED | PMAP_CANFAIL);
@


1.44
log
@Convert bus_dmamem_map(9) to km_alloc(9) in order to make it fail and
not sleep if the allocator cannot obtain a lock when BUS_DMA_NOWAIT is
specified.

idea and inputs from kettenis@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.43 2014/05/04 20:09:15 sf Exp $	*/
d167 1
a167 1
	free(map, M_DEVBUF);
@


1.43
log
@format string fixes for bus_addr_t and bus_size_t

bus_addr_t and bus_size_t are u_long everywhere

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.42 2014/04/01 09:05:03 mpi Exp $	*/
d470 1
d481 2
a482 1
	va = uvm_km_valloc(kernel_map, size);
d501 1
a501 1
				uvm_km_free(kernel_map, sva, ssize);
d526 1
a526 2
	size = round_page(size);
	uvm_km_free(kernel_map, (vaddr_t)kva, size);
@


1.42
log
@More <uvm/uvm.h> -> <uvm/uvm_extern.h> cleaning.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.41 2013/12/12 21:04:50 kettenis Exp $	*/
d335 1
a335 1
				panic("Non dma-reachable buffer at paddr %p(raw)",
d598 1
a598 1
			panic("Non dma-reachable buffer at curaddr %p(raw)",
@


1.41
log
@Remove the scatter/gather dma implementation as it is no longer used.

ok krw@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.40 2012/12/08 12:04:21 mpi Exp $	*/
d101 1
a101 1
#include <uvm/uvm.h>
@


1.40
log
@Make bus_dmamem_mmap(9) honor BUS_DMA_NOCACHE.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.39 2012/12/05 23:20:10 deraadt Exp $	*/
a675 4
	if (flags & BUS_DMA_SG) {
		boundary = 0;
		alignment = 0;
	}
@


1.39
log
@Remove excessive sys/cdefs.h inclusion
ok guenther millert kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.38 2011/07/03 18:31:02 oga Exp $	*/
d536 4
a539 1
	int i;
d556 1
a556 1
		return (segs[i].ds_addr + off);
@


1.38
log
@Do as all other direct archs do and map contig memory through the direct
map in bus_dmamem_map().

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.37 2011/06/23 20:44:38 ariane Exp $	*/
a32 2

#include <sys/cdefs.h>
@


1.37
log
@Fix the error path in bus_dmamem_map.
As discussed on icb: remove the comment,
remove pmap_remove (uvm_km_free does that for us).

ok oga@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.36 2011/04/02 16:37:39 beck Exp $	*/
d473 5
d523 2
@


1.36
log
@Constraint checking - ensure that physical addresses for dma are below
the top of the dma constraint range and panic if they are not.
ok deraadt@@, thib@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.35 2010/12/26 15:40:58 miod Exp $	*/
a494 4
				/*
				 * Clean up after ourselves.
				 * XXX uvm_wait on WAITOK
				 */
d496 1
a496 1
				uvm_km_free(kernel_map, va, ssize);
@


1.35
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.34 2010/11/20 20:33:23 miod Exp $	*/
d103 1
a103 1
#include <uvm/uvm_extern.h>
d336 4
d592 4
@


1.34
log
@This is a first step towards getting rid of avail_start and avail_end in the
kernel, currently limited to low-hanging fruit: these variables were used
by bus_dma to specify the range in which to allocate memory, back when
uvm_pglistalloc() was stupid and would not walk the vm_physseg[].

Nowadays, except on some platforms for early initialization, these variables
are not used, or do not need to be global variables. Therefore:
- remove `extern' declarations of avail_start and avail_end (or close cousins,
  such as arm physical_start and physical_end) from files which no longer need
  to use them.
- make them local variables whenever possible.
- remove them when they are assigned to but no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.33 2010/09/06 19:05:48 kettenis Exp $	*/
d548 1
a548 1
		return (atop(segs[i].ds_addr + off));
@


1.33
log
@Make sure bus_dmamap_sync() always involves a function call, to prevent the
compiler from doing stupid things like reordering stores around it.  There is
some debate whether this will be enough for newer versions of GCC and LLVM.
If this is indeed deemed necessary, this will be addressed in a future diff.

ok miod@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.32 2010/05/20 05:46:53 oga Exp $	*/
a110 2

extern	paddr_t avail_end;
@


1.32
log
@Force max physical memory allocation for bus_dmamem_alloc() to be 4gig
for now.

When we get bouncebuffers/decent iommu this can be revised to either
fall back (bouncebuffers) or just grab any memory (iommu), but for now
it is one less thing to worry about for turning bigmem back on.

ok kettenis@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.31 2010/04/08 00:55:25 oga Exp $	*/
a406 1

@


1.31
log
@On amd64, move the bus_dma buftype stuff that is only used by sg_dma
into the sg_dma code instead of main bus_dma. Add identical code to i386
since this will be used in the next commit.

ok kettenis@@ back in december.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.30 2010/03/29 19:21:58 oga Exp $	*/
d421 6
d428 1
a428 1
	    segs, nsegs, rsegs, flags, (paddr_t)0, (paddr_t)-1));
@


1.30
log
@PMAP_CANFAIL for bus_dmamem_map on all other architectures (and some
whitespace tweaks on i386 so that it matches).

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.29 2009/08/11 17:15:54 oga Exp $	*/
a199 3
		map->_dm_origbuf = buf;
		map->_dm_buftype = BUS_BUFTYPE_LINEAR;
		map->_dm_proc = p;
a241 2
		map->_dm_origbuf = m0;
		map->_dm_buftype = BUS_BUFTYPE_MBUF;
a296 2
		map->_dm_origbuf = uio;
		map->_dm_buftype = BUS_BUFTYPE_UIO;
a380 2
	map->_dm_origbuf = segs;
	map->_dm_buftype = BUS_BUFTYPE_RAW;
a390 1

a396 3
	map->_dm_buftype = BUS_BUFTYPE_INVALID;
	map->_dm_origbuf = NULL;
	map->_dm_proc = NULL;
@


1.29
log
@fix some stupidity in x86 bus_space_map.

right now, we do a pmap_kenter_pa(), we then get the pte (behind pmap's
back) and check for the cache inhibit bit (if needed). If it isn't what
we want (this is the normal case) then we change it ourselves, and do a
manual tlb shootdown (i386 was a bit more stupid about it than amd64,
too).

Instead, make it so that like on some other archs (sparc64 comes to
mind) you can pass in flags in the low bits of the physical address,
pmap then does everything correctly for you.

Discovered this when I had some code doing a lot of bus_space_maps(), it
was incredibly slow, and profilling was dominated by
pmap_tlb_shootwait();

discussed with kettenis@@, miod@@, toby@@ and art@@.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.28 2009/06/07 02:30:34 oga Exp $	*/
d474 2
a475 1
	vaddr_t va;
d477 1
a477 1
	int curseg, pmapflags = 0;
d489 2
d497 1
a497 1
			pmap_enter(pmap_kernel(), va, addr | pmapflags,
d499 10
a508 1
			    VM_PROT_WRITE | PMAP_WIRED);
@


1.28
log
@on amd64, store a pointer to the loaded data and the type. This will
eventually be needed by bouncebuffers, and I need it for some of my evil
graphics shitz.

ok kettenis@@ with some tweaks
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.27 2009/04/21 17:05:29 oga Exp $	*/
d476 1
a476 1
	int curseg, pmapflags = VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED;
d494 3
a496 2
			pmap_enter(pmap_kernel(), va, addr,
			    VM_PROT_READ | VM_PROT_WRITE, pmapflags);
@


1.27
log
@add a sg_dma backend for amd64 bus_dma. This is a lot more clever about
mapping to the gart than the old code, and shouldn't conflict with
bouncebuffers when they're added.

This is essentially the sparc64 iommu code that's been modularised a bit
so I can eventually use the same code for agp-based dma for memory
managed drm drivers.

Now, this would overflow ramdiskA, so iommu and sg_dma are now #ifndef
SMALL_KERNEL.

ok kettenis@@, marco@@. SMALL_KERNEL discussions with deraadt.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.26 2009/04/20 00:42:05 oga Exp $	*/
a157 2
	map->dm_mapsize = 0;		/* no valid mappings */
	map->dm_nsegs = 0;
d200 3
d245 2
d302 2
d388 2
d407 3
@


1.26
log
@Add a BUS_DMA_ZERO flag for bus_dmamem_alloc() to return zeroed memory.

Saves every damned driver calling bzero(), and continues the M_ZERO,
PR_ZERO symmetry.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.25 2009/04/15 03:35:26 oga Exp $	*/
d648 7
@


1.25
log
@fix properly; size gets decremented while we check the segments, so save it so
when we setmapsize it's not zero.

*sigh*
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.24 2009/04/15 02:03:33 oga Exp $	*/
d653 2
@


1.24
log
@make load_raw do the same as the others and set mapsize and nsegs to
zero so that we return an empty map on error.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.23 2009/04/15 01:58:27 oga Exp $	*/
d312 1
a312 1
	bus_size_t plen, sgsize;
d325 1
d381 1
a381 1
	map->dm_mapsize = size;
@


1.23
log
@bus_dmamap_load_raw didn't set map->dm_mapsize on successful load.

I just spent five hours looking in the wrong place because of this.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.22 2009/04/14 16:01:04 oga Exp $	*/
d315 6
@


1.22
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.21 2009/04/09 03:08:36 dlg Exp $	*/
d374 1
@


1.21
log
@in bus_dmamap_load_raw, only map the requested number of bytes into the
dmamap rather than all the bytes that are described by the sg list we're
mapping.

tested on iwn by me and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.18 2009/04/04 13:48:55 dlg Exp $	*/
d636 1
a636 1
	int curseg, error;
a640 2
	TAILQ_INIT(&mlist);

d644 3
d648 1
a648 1
	    &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.20
log
@unrevert marco's revert of my bus_dmamap_load_raw change now that i found
the bug in it. bugfix will be committed next.

make bus_dmamap_load_raw respect the constraints of the dmamap we're
loading the raw memory into, particularly the segment size constraint.
@
text
@d321 1
a321 1
	for (i = 0; i < nsegs; i++) {
d323 1
a323 1
		plen = segs[i].ds_len;
a365 1
			lastaddr = paddr + sgsize;
d368 3
@


1.19
log
@Rever _raw change from dlg because it breaks iwn.

ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.17 2009/03/30 18:12:24 deraadt Exp $	*/
d311 5
d319 50
a368 13
	/*
	 * Make sure we don't cross any boundaries.
	 */
	if (map->_dm_boundary) {
		bus_addr_t bmask = ~(map->_dm_boundary - 1);
		int i;

		for (i = 0; i < nsegs; i++) {
			if (segs[i].ds_len > map->_dm_maxsegsz)
				return (EINVAL);
			if ((segs[i].ds_addr & bmask) !=
			    ((segs[i].ds_addr + segs[i].ds_len - 1) & bmask))
				return (EINVAL);
d372 1
a372 2
	bcopy(segs, map->dm_segs, nsegs * sizeof(*segs));
	map->dm_nsegs = nsegs;
@


1.18
log
@make bus_dmamap_load_raw respect teh constraints of the dmamap we're
loading the raw memory into, particularly the segment size constraint.

written in june/july last year, but my studies held me back from handling
it.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.10 2007/09/17 15:34:38 chl Exp $	*/
a310 5
	bus_addr_t paddr, baddr, bmask, lastaddr = 0;
	bus_size_t plen, sgsize;
	int first = 1;
	int i, seg = 0;

d314 13
a326 50
	bmask  = ~(map->_dm_boundary - 1);

	for (i = 0; i < nsegs; i++) {
		paddr = segs[i].ds_addr;
		plen = segs[i].ds_len;

		while (plen > 0) {
			/*
			 * Compute the segment size, and adjust counts.
			 */
			sgsize = PAGE_SIZE - ((u_long)paddr & PGOFSET);
			if (plen < sgsize)
				sgsize = plen;

			/*
			 * Make sure we don't cross any boundaries.
			 */
			if (map->_dm_boundary > 0) {
				baddr = (paddr + map->_dm_boundary) & bmask;
				if (sgsize > (baddr - paddr))
					sgsize = (baddr - paddr);
			}

			/*
			 * Insert chunk into a segment, coalescing with
			 * previous segment if possible.
			 */
			if (first) {
				map->dm_segs[seg].ds_addr = paddr;
				map->dm_segs[seg].ds_len = sgsize;
				first = 0;
			} else {
				if (paddr == lastaddr &&
				    (map->dm_segs[seg].ds_len + sgsize) <=
				     map->_dm_maxsegsz &&
				    (map->_dm_boundary == 0 ||
				     (map->dm_segs[seg].ds_addr & bmask) ==
				     (paddr & bmask)))
					map->dm_segs[seg].ds_len += sgsize;
				else {
					if (++seg >= map->_dm_segcnt)
						return (EINVAL);
					map->dm_segs[seg].ds_addr = paddr;
					map->dm_segs[seg].ds_len = sgsize;
				}
			}

			lastaddr = paddr + sgsize;
			paddr += sgsize;
			plen -= sgsize;
d330 2
a331 1
	map->dm_nsegs = seg + 1;
@


1.17
log
@make the code look the same; ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.16 2009/03/10 15:03:16 oga Exp $	*/
d311 5
d319 50
a368 13
	/*
	 * Make sure we don't cross any boundaries.
	 */
	if (map->_dm_boundary) {
		bus_addr_t bmask = ~(map->_dm_boundary - 1);
		int i;

		for (i = 0; i < nsegs; i++) {
			if (segs[i].ds_len > map->_dm_maxsegsz)
				return (EINVAL);
			if ((segs[i].ds_addr & bmask) !=
			    ((segs[i].ds_addr + segs[i].ds_len - 1) & bmask))
				return (EINVAL);
d372 1
a372 2
	bcopy(segs, map->dm_segs, nsegs * sizeof(*segs));
	map->dm_nsegs = nsegs;
@


1.16
log
@remove the _BUS_DMA_PRIVATE define from amd64 and i386.

a define needed to get to ``private'' functions that needs to be defined
5 or more times isn't much use and may cause namespace issues anyway.
Other archs will probably follow.

Discussed in portugal.  "Hell yes" weingart@@, ok kettenis@@, no
objections miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.15 2009/03/07 15:34:34 miod Exp $	*/
d415 1
a415 1
	int curseg, pmapflags = 0;
d434 1
a434 3
			    VM_PROT_READ | VM_PROT_WRITE,
			    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED |
			    pmapflags);
@


1.15
log
@When allocating memory in bus_dmamem_alloc() with uvm_pglistalloc(), do not
try to be smart for the address range, uvm_pglistalloc() is smart enough
nowadays.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.14 2009/02/05 01:15:20 oga Exp $	*/
a97 1
#define _BUS_DMA_PRIVATE
@


1.14
log
@Mirroring the i386 commit just made. Add MD PMAP_NOCACHE flag to pmap,
and use it to implement BUS_DMA_NOCACHE for uncached mappings of dma
memory. Needed for some broken hardware.

Discussion with art, miod, kettenis and toby, ok miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.13 2008/12/03 15:46:06 oga Exp $	*/
d375 1
a375 1
	    segs, nsegs, rsegs, flags, 0, trunc_page(avail_end)));
@


1.13
log
@Remove the x86 and i386 prefixes to the bus_dma types. It's really quite
pointless and just makes the code different for no reason. This moves i386 and
amd64 bus_dma to being a lot closer to identical.

suggestion to just remove the prefix instead of merge them from deraadt@@.

no objections art@@, kettenis@@, ok weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.12 2008/06/26 05:42:09 ray Exp $	*/
d416 4
a419 1
	int curseg;
d436 2
a437 1
			    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
@


1.12
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.11 2008/06/23 00:27:11 dlg Exp $	*/
d98 1
a98 1
#define _X86_BUS_DMA_PRIVATE
d130 1
a130 1
	struct x86_bus_dmamap *map;
d146 1
a146 1
	mapsize = sizeof(struct x86_bus_dmamap) +
d153 1
a153 1
	map = (struct x86_bus_dmamap *)mapstore;
@


1.11
log
@amd64s bus_dma internals use a pointer to a lastaddr variable to keep
track of where it was up to when building dmamaps. that lastaddr variable
is not initialised every time you start to load a dmamap, so it was using
random stack garbage as state when first writing the sg list.

fortunately the way it was used meant it was extremely unlikely to cause a
problem, but why allow even that possibility? this inits lastaddr to 0 in
all the callers of load_buffer.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.10 2007/09/17 15:34:38 chl Exp $	*/
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.10
log
@MALLOC/FREE -> malloc/free and M_ZERO changes

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.9 2007/09/03 01:09:09 krw Exp $	*/
d192 1
a192 1
	bus_addr_t lastaddr;
d221 1
a221 1
	paddr_t lastaddr;
d263 1
a263 1
	paddr_t lastaddr;
@


1.9
log
@Typos from miod. 'functin' -> 'functin' in some comments.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.8 2007/05/29 21:00:50 jason Exp $	*/
d156 2
a157 1
	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
a159 1
	bzero(mapstore, mapsize);
@


1.8
log
@s/entrace/entrance (not obvious that the code was cut/paste =)
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.7 2007/01/15 23:19:05 jsg Exp $	*/
d466 1
a466 1
 * Common functin for mmap(2)'ing DMA-safe memory.  May be called by
@


1.7
log
@ansi/deregister
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.6 2006/06/08 03:18:08 weingart Exp $	*/
d503 1
a503 1
 * the starting segment on entrace, and the ending segment on exit.
@


1.6
log
@Move ISA logic of allocation functions to better place.
Thanks for krw@@ for testing isa floppies.  brad@@ ok,
jason@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.5 2006/05/10 12:36:39 krw Exp $	*/
d134 2
a135 8
_bus_dmamap_create(t, size, nsegments, maxsegsz, boundary, flags, dmamp)
	bus_dma_tag_t t;
	bus_size_t size;
	int nsegments;
	bus_size_t maxsegsz;
	bus_size_t boundary;
	int flags;
	bus_dmamap_t *dmamp;
d178 1
a178 3
_bus_dmamap_destroy(t, map)
	bus_dma_tag_t t;
	bus_dmamap_t map;
d189 2
a190 7
_bus_dmamap_load(t, map, buf, buflen, p, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	void *buf;
	bus_size_t buflen;
	struct proc *p;
	int flags;
d218 2
a219 5
_bus_dmamap_load_mbuf(t, map, m0, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct mbuf *m0;
	int flags;
d260 2
a261 5
_bus_dmamap_load_uio(t, map, uio, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct uio *uio;
	int flags;
d316 2
a317 7
_bus_dmamap_load_raw(t, map, segs, nsegs, size, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_dma_segment_t *segs;
	int nsegs;
	bus_size_t size;
	int flags;
d348 1
a348 3
_bus_dmamap_unload(t, map)
	bus_dma_tag_t t;
	bus_dmamap_t map;
d364 2
a365 6
_bus_dmamap_sync(t, map, addr, size, op)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_addr_t addr;
	bus_size_t size;
	int op;
d376 3
a378 7
_bus_dmamem_alloc(t, size, alignment, boundary, segs, nsegs, rsegs, flags)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
d390 1
a390 4
_bus_dmamem_free(t, segs, nsegs)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
d418 2
a419 7
_bus_dmamem_map(t, segs, nsegs, size, kvap, flags)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
	size_t size;
	caddr_t *kvap;
	int flags;
d453 1
a453 4
_bus_dmamem_unmap(t, kva, size)
	bus_dma_tag_t t;
	caddr_t kva;
	size_t size;
d470 2
a471 6
_bus_dmamem_mmap(t, segs, nsegs, off, prot, flags)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
	off_t off;
	int prot, flags;
d507 3
a509 10
_bus_dmamap_load_buffer(t, map, buf, buflen, p, flags, lastaddrp, segp, first)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	void *buf;
	bus_size_t buflen;
	struct proc *p;
	int flags;
	paddr_t *lastaddrp;
	int *segp;
	int first;
d592 3
a594 10
_bus_dmamem_alloc_range(t, size, alignment, boundary, segs, nsegs, rsegs,
    flags, low, high)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
	paddr_t low;
	paddr_t high;
@


1.5
log
@Missing $OpenBSD$ tags.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d672 2
a673 5
	if (high <= ISA_DMA_BOUNCE_THRESHOLD || (error = uvm_pglistalloc(size,
	    round_page(ISA_DMA_BOUNCE_THRESHOLD), high, alignment, boundary,
	    &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0)))
		error = uvm_pglistalloc(size, low, high, alignment, boundary,
		    &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.4
log
@from mickey, like on i386:
prioritise <16m memory for isa bus_dma'ble memorble memory allocations
why?
this is now needed because the nvidia ethernet running with jumbo frames
is allocating much more memory, and was robbing isadma of bounce buffers.
@
text
@d1 1
@


1.3
log
@goodbye more Mach macros

help toby, ok deraadt@@
@
text
@d666 2
d671 5
a675 3
	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, low, high,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.2
log
@Do not map empty mbufs (m_len == 0) in bus_dmamap_load_mbuf() as these mappings
may disturb the dma as seen in ipw(4). Emtpy mbufs are at the beginning of the
mbuf chain and are as example a "side-effect" of a previous m_adj() call.
OK miod@@ mickey@@ jason@@ markus@@
@
text
@d538 1
a538 1
		return (x86_btop((caddr_t)segs[i].ds_addr + off));
@


1.1
log
@an amd64 arch support.
hacked by art@@ from netbsd sources and then later debugged
by me into the shape where it can host itself.
no bootloader yet as needs redoing from the
recent advanced i386 sources (anyone? ;)
@
text
@d258 2
@

