head	1.13;
access;
symbols
	OPENBSD_6_2:1.13.0.4
	OPENBSD_6_2_BASE:1.13
	OPENBSD_6_1:1.11.0.4
	OPENBSD_6_1_BASE:1.11
	OPENBSD_6_0:1.9.0.14
	OPENBSD_6_0_BASE:1.9
	OPENBSD_5_9:1.9.0.10
	OPENBSD_5_9_BASE:1.9
	OPENBSD_5_8:1.9.0.12
	OPENBSD_5_8_BASE:1.9
	OPENBSD_5_7:1.9.0.4
	OPENBSD_5_7_BASE:1.9
	OPENBSD_5_6:1.9.0.8
	OPENBSD_5_6_BASE:1.9
	OPENBSD_5_5:1.9.0.6
	OPENBSD_5_5_BASE:1.9
	OPENBSD_5_4:1.9.0.2
	OPENBSD_5_4_BASE:1.9
	OPENBSD_5_3:1.8.0.10
	OPENBSD_5_3_BASE:1.8
	OPENBSD_5_2:1.8.0.8
	OPENBSD_5_2_BASE:1.8
	OPENBSD_5_1_BASE:1.8
	OPENBSD_5_1:1.8.0.6
	OPENBSD_5_0:1.8.0.4
	OPENBSD_5_0_BASE:1.8
	OPENBSD_4_9:1.8.0.2
	OPENBSD_4_9_BASE:1.8
	OPENBSD_4_8:1.7.0.4
	OPENBSD_4_8_BASE:1.7
	OPENBSD_4_7:1.7.0.2
	OPENBSD_4_7_BASE:1.7
	OPENBSD_4_6:1.6.0.4
	OPENBSD_4_6_BASE:1.6
	OPENBSD_4_5:1.4.0.16
	OPENBSD_4_5_BASE:1.4
	OPENBSD_4_4:1.4.0.14
	OPENBSD_4_4_BASE:1.4
	OPENBSD_4_3:1.4.0.12
	OPENBSD_4_3_BASE:1.4
	OPENBSD_4_2:1.4.0.10
	OPENBSD_4_2_BASE:1.4
	OPENBSD_4_1:1.4.0.8
	OPENBSD_4_1_BASE:1.4
	OPENBSD_4_0:1.4.0.4
	OPENBSD_4_0_BASE:1.4
	OPENBSD_3_9:1.4.0.6
	OPENBSD_3_9_BASE:1.4
	OPENBSD_3_8:1.4.0.2
	OPENBSD_3_8_BASE:1.4
	OPENBSD_3_7:1.2.0.4
	OPENBSD_3_7_BASE:1.2
	OPENBSD_3_6:1.2.0.2
	OPENBSD_3_6_BASE:1.2;
locks; strict;
comment	@# @;


1.13
date	2017.06.29.17.17.28;	author deraadt;	state Exp;
branches;
next	1.12;
commitid	W8lruZ7GPI2dyHQY;

1.12
date	2017.04.20.13.57.29;	author visa;	state Exp;
branches;
next	1.11;
commitid	RHJVP52IiQkInZzu;

1.11
date	2017.03.07.14.03.22;	author visa;	state Exp;
branches;
next	1.10;
commitid	4JP9fO12Ulby6hlV;

1.10
date	2017.03.07.13.59.51;	author visa;	state Exp;
branches;
next	1.9;
commitid	h1lKqoVKEoS0Rbr5;

1.9
date	2013.06.02.01.55.52;	author kettenis;	state Exp;
branches;
next	1.8;

1.8
date	2010.09.24.13.21.30;	author matthew;	state Exp;
branches;
next	1.7;

1.7
date	2009.08.13.13.24.55;	author weingart;	state Exp;
branches;
next	1.6;

1.6
date	2009.04.27.21.48.56;	author kettenis;	state Exp;
branches;
next	1.5;

1.5
date	2009.04.25.20.14.42;	author weingart;	state Exp;
branches;
next	1.4;

1.4
date	2005.07.26.08.11.11;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2005.04.08.07.07.06;	author jolan;	state Exp;
branches;
next	1.2;

1.2
date	2004.08.02.20.03.58;	author art;	state Exp;
branches;
next	1.1;

1.1
date	2004.07.20.20.17.16;	author art;	state Exp;
branches;
next	;


desc
@@


1.13
log
@Put asm-generated strings into .rodata
ok millert
@
text
@/*	$OpenBSD: mutex.S,v 1.12 2017/04/20 13:57:29 visa Exp $	*/

/*
 * Copyright (c) 2004 Artur Grabowski <art@@openbsd.org>
 * All rights reserved. 
 *
 * Redistribution and use in source and binary forms, with or without 
 * modification, are permitted provided that the following conditions 
 * are met: 
 *
 * 1. Redistributions of source code must retain the above copyright 
 *    notice, this list of conditions and the following disclaimer. 
 * 2. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission. 
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,
 * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
 * AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL
 * THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL  DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
 */

#include "assym.h"

#include <machine/param.h>
#include <machine/asm.h>
#include <machine/segments.h>
#include <machine/specialreg.h>
#include <machine/trap.h>
#include <machine/frameasm.h>

/*
 * Yeah, we don't really need to implement mtx_init here, but let's keep
 * all the functions in the same place.
 */
ENTRY(__mtx_init)
	movl	%esi, MTX_WANTIPL(%rdi)
	movl	$0, MTX_OLDIPL(%rdi)
	movq	$0, MTX_OWNER(%rdi)
	ret

ENTRY(__mtx_enter)
1:	movl	MTX_WANTIPL(%rdi), %eax
	movq	CPUVAR(SELF), %rcx
	movl	CPU_INFO_ILEVEL(%rcx), %edx	# oipl = cpl;
	cmpl	%eax, %edx			# if (cpl < mtx->mtx_wantipl)
	cmovge	%edx, %eax
	movl	%eax, CPU_INFO_ILEVEL(%rcx)	#	cpl = mtx->mtx_wantipl;
	/*
	 * %edx - the old ipl
	 * %rcx - curcpu()
	 */
	xorq	%rax, %rax
#ifdef MULTIPROCESSOR
	lock
#endif
	cmpxchgq	%rcx, MTX_OWNER(%rdi)	# test_and_set(mtx->mtx_owner)
	jne	2f
	movl	%edx, MTX_OLDIPL(%rdi)
#ifdef DIAGNOSTIC
	incl	CPU_INFO_MUTEX_LEVEL(%rcx)
#endif
	ret

	/* We failed to obtain the lock. splx, spin and retry. */
2:	pushq	%rdi
	movl	%edx, %edi
	call	_C_LABEL(spllower)
	popq	%rdi
3:
	pause
#ifdef DIAGNOSTIC
	movq	CPUVAR(SELF), %rcx
	cmpq	MTX_OWNER(%rdi), %rcx
	je	4f
#endif
	movq	MTX_OWNER(%rdi), %rax
	testq	%rax, %rax
	jz	1b
	jmp	3b
#ifdef DIAGNOSTIC
4:	movq	$mtx_lockingself, %rdi
	call	_C_LABEL(panic)

	.section .rodata
mtx_lockingself:
	.asciz	"mtx_enter: locking against myself"
	.text
#endif

ENTRY(__mtx_enter_try)
1:	movl	MTX_WANTIPL(%rdi), %eax
	movq	CPUVAR(SELF), %rcx
	movl	CPU_INFO_ILEVEL(%rcx), %edx	# oipl = cpl;
	cmpl	%eax, %edx			# if (cpl < mtx->mtx_wantipl)
	cmovge	%edx, %eax
	movl	%eax, CPU_INFO_ILEVEL(%rcx)	#	cpl = mtx->mtx_wantipl;
	/*
	 * %edx - the old ipl
	 * %rcx - curcpu()
	 */
	xorq	%rax, %rax
#ifdef MULTIPROCESSOR
	lock
#endif
	cmpxchgq	%rcx, MTX_OWNER(%rdi)	# test_and_set(mtx->mtx_owner)
	jne	2f
	movl	%edx, MTX_OLDIPL(%rdi)
#ifdef DIAGNOSTIC
	incl	CPU_INFO_MUTEX_LEVEL(%rcx)
#endif
	movq	$1, %rax
	ret

	/* We failed to obtain the lock. splx and return 0. */
2:	pushq	%rdi
	movl	%edx, %edi
	call	_C_LABEL(spllower)
	popq	%rdi
#ifdef DIAGNOSTIC
	movq	CPUVAR(SELF), %rcx
	cmpq	MTX_OWNER(%rdi), %rcx
	je	3f
#endif
	xorq	%rax, %rax
	ret

#ifdef DIAGNOSTIC
3:	movq	$mtx_lockingtry, %rdi
	call	_C_LABEL(panic)

	.section .rodata
mtx_lockingtry:
	.asciz	"mtx_enter_try: locking against myself"
	.text
#endif


ENTRY(__mtx_leave)
	movq	%rdi, %rax
#ifdef DIAGNOSTIC
	movq	CPUVAR(SELF), %rcx
	cmpq	MTX_OWNER(%rax), %rcx
	jne	2f
	decl	CPU_INFO_MUTEX_LEVEL(%rcx)
#endif
	xorq	%rcx, %rcx
	movl	MTX_OLDIPL(%rax), %edi
	movl	%ecx, MTX_OLDIPL(%rax)
	movq	%rcx, MTX_OWNER(%rax)
	cmpl	%edi, CPUVAR(ILEVEL)
	je	1f
	call	_C_LABEL(spllower)
1:
	ret

#ifdef DIAGNOSTIC
2:	movq	$mtx_leave_held, %rdi
	call	_C_LABEL(panic)

	.section .rodata
mtx_leave_held:
	.asciz	"mtx_leave: lock not held"
	.text
#endif
@


1.12
log
@Hook up mutex(9) to witness(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.11 2017/03/07 14:03:22 visa Exp $	*/
d87 1
a87 1
4:	movq	$5f, %rdi
d89 5
a93 1
5:	.asciz	"mtx_enter: locking against myself"
d134 1
a134 1
3:	movq	$4f, %rdi
d136 5
a140 1
4:	.asciz	"mtx_enter_try: locking against myself"
d163 1
a163 1
2:	movq	$3f, %rdi
d165 5
a169 1
3:	.asciz	"mtx_leave: lock not held"
@


1.11
log
@Use the pause instruction on the slow path. This improves
performance a bit.

OK mikeb@@, kettenis@@, mpi@@, tom@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.10 2017/03/07 13:59:51 visa Exp $	*/
d47 1
a47 1
ENTRY(mtx_enter)
d92 1
a92 1
ENTRY(mtx_enter_try)
d136 1
a136 1
ENTRY(mtx_leave)
@


1.10
log
@Make the slow path similar to i386's by checking mutex owner
on every iteration.

OK mikeb@@, kettenis@@, mpi@@, tom@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.9 2013/06/02 01:55:52 kettenis Exp $	*/
d76 1
@


1.9
log
@To prevent lock ordering problems with the kernel lock, we need to make sure
we block all interrupts that can grab the kernel lock.  The simplest way to
achieve this is to make sure mutexes always raise the ipl to the highest
level that has interrupts that grab the kernel lock.  This will allow us
to have "mpsafe" interrupt handlers at lower priority levels.

No change for non-MULTIPROCESSOR kernels.

discussed with many
ok mpi@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.8 2010/09/24 13:21:30 matthew Exp $	*/
d75 1
a80 1
3:
@


1.8
log
@Add stricter asserts to DIAGNOSTIC kernels to help catch mutex and
rwlock misuse.  In particular, this commit makes the following
changes:

  1. i386 and amd64 now count the number of active mutexes so that
assertwaitok(9) can detect attempts to sleep while holding a mutex.

  2. i386 and amd64 check that we actually hold mutexes when passed to
mtx_leave().

  3. Calls to rw_exit*() now call rw_assert_{rd,wr}lock() as
appropriate.

ok krw@@, oga@@; "sounds good to me" deraadt@@; assembly bits double
checked by pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.7 2009/08/13 13:24:55 weingart Exp $	*/
d41 1
a41 1
ENTRY(mtx_init)
@


1.7
log
@A new(er) mtx_enter_try().

Ok oga@@, "the time is now" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.6 2009/04/27 21:48:56 kettenis Exp $	*/
d65 3
d109 3
d131 1
a131 1
4:	.asciz	"mtx_enter: locking against myself"
d137 6
d152 6
@


1.6
log
@Revert mtx_enter_try.  It didn't compile on hppa, it doesn't compile on
landisk, and the sparc implementation is obviously wrong.  That's where I
stopped looking, so who knows what else was broken.  A simple comparison of
the existing mtx_enter with the new mtx_enter_try would have told anybody.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.4 2005/07/26 08:11:11 art Exp $	*/
d87 42
a128 1
	
@


1.5
log
@Enter mtx_enter_try.  In part for completeness, things may start
using this soon(ish).  Ok oga@@, sorta yes kettenis@@.
@
text
@d87 1
a87 42

ENTRY(mtx_enter_try)
1:	movl	MTX_WANTIPL(%rdi), %eax
	movq	CPUVAR(SELF), %rcx
	movl	CPU_INFO_ILEVEL(%rcx), %edx	# oipl = cpl;
	cmpl	%eax, %edx			# if (cpl < mtx->mtx_wantipl)
	cmovge	%edx, %eax
	movl	%eax, CPU_INFO_ILEVEL(%rcx)	#	cpl = mtx->mtx_wantipl;
	/*
	 * %edx - the old ipl
	 * %rcx - curcpu()
	 */
	xorq	%rax, %rax
#ifdef MULTIPROCESSOR
	lock
#endif
	cmpxchgq	%rcx, MTX_OWNER(%rdi)	# test_and_set(mtx->mtx_owner)
	jne	2f
	movl	%edx, MTX_OLDIPL(%rdi)
	movq	$1, %rax
	ret

	/* We failed to obtain the lock. splx and return 0. */
2:	pushq	%rdi
	movl	%edx, %edi
	call	_C_LABEL(spllower)
	popq	%rdi
#ifdef DIAGNOSTIC
	movq	CPUVAR(SELF), %rcx
	cmpq	MTX_OWNER(%rdi), %rcx
	je	3f
#endif
	xorq	%rax, %rax
	ret

#ifdef DIAGNOSTIC
3:	movq	$4f, %rdi
	call	_C_LABEL(panic)
4:	.asciz	"mtx_enter: locking against myself"
#endif


@


1.4
log
@Microoptimizations.

 - Use cmov instead of jmp in mtx_enter
 - Don't spllower in mtx_leave unless we really have to.

tested by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.S,v 1.3 2005/04/08 07:07:06 jolan Exp $	*/
d87 42
a128 1
	
@


1.3
log
@add rcs ids
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d51 2
a52 2
	cmpl	%edx, %eax			# if (cpl < mtx->mtx_wantipl)
	jle	2f
d54 1
a54 1
2:	/*
d63 1
a63 1
	jne	3f
d68 1
a68 1
3:	pushq	%rdi
d75 1
a75 1
	je	5f
d77 1
a77 1
4:
d81 1
a81 1
	jmp	4b
d83 1
a83 1
5:	movq	$6f, %rdi
d85 1
a85 1
6:	.asciz	"mtx_enter: locking against myself"
d94 2
d97 1
@


1.2
log
@Remove a completly unnecessary compare that sneaked in while
I was debugging some completly other problem in this function.
cmpxchgq already does an equivalent compare.

Discussed on icb a few days ago.
@
text
@d1 2
@


1.1
log
@MD mutex implementation on amd64.
@
text
@a60 1
	cmpq	$0, %rax
@

