head	1.131;
access;
symbols
	OPENBSD_6_1:1.131.0.2
	OPENBSD_6_1_BASE:1.131
	OPENBSD_6_0:1.71.0.2
	OPENBSD_6_0_BASE:1.71
	OPENBSD_5_9:1.38.0.2
	OPENBSD_5_9_BASE:1.38;
locks; strict;
comment	@ * @;


1.131
date	2017.03.28.21.38.44;	author mlarkin;	state Exp;
branches;
next	1.130;
commitid	HpE70ru2yypE3gDN;

1.130
date	2017.03.27.19.00.38;	author mlarkin;	state Exp;
branches;
next	1.129;
commitid	taKygi2vgMAMpdRE;

1.129
date	2017.03.26.21.47.34;	author mlarkin;	state Exp;
branches;
next	1.128;
commitid	aZhJoV5vP5Lcw2OM;

1.128
date	2017.03.26.18.34.55;	author mlarkin;	state Exp;
branches;
next	1.127;
commitid	HCobn084yGI8qVcM;

1.127
date	2017.03.26.18.29.58;	author mlarkin;	state Exp;
branches;
next	1.126;
commitid	DlTjYCrjiSIuiLA6;

1.126
date	2017.03.25.22.24.01;	author deraadt;	state Exp;
branches;
next	1.125;
commitid	0BdiEUSnzOoly0V9;

1.125
date	2017.03.24.09.06.02;	author mlarkin;	state Exp;
branches;
next	1.124;
commitid	RAxTE6oWZZfESbB7;

1.124
date	2017.03.24.08.52.53;	author mlarkin;	state Exp;
branches;
next	1.123;
commitid	jioHJ2iQR0hrLV6T;

1.123
date	2017.03.24.08.02.02;	author mlarkin;	state Exp;
branches;
next	1.122;
commitid	QJdGvy6AIcX03Uuf;

1.122
date	2017.03.21.02.57.38;	author mlarkin;	state Exp;
branches;
next	1.121;
commitid	rzXsaIH4GBpCtB0x;

1.121
date	2017.03.21.02.30.33;	author mlarkin;	state Exp;
branches;
next	1.120;
commitid	AG9My1mf6gpICoAX;

1.120
date	2017.03.19.03.42.38;	author mlarkin;	state Exp;
branches;
next	1.119;
commitid	xJ1i4OtmV0v8ECfI;

1.119
date	2017.03.02.07.26.31;	author mlarkin;	state Exp;
branches;
next	1.118;
commitid	CIKtFWx81m0BnVMw;

1.118
date	2017.03.02.03.21.44;	author mlarkin;	state Exp;
branches;
next	1.117;
commitid	aywmj9ZFt7fPIizG;

1.117
date	2017.03.02.02.57.35;	author mlarkin;	state Exp;
branches;
next	1.116;
commitid	dXZDZAhws1CaqkXu;

1.116
date	2017.03.02.02.34.56;	author mlarkin;	state Exp;
branches;
next	1.115;
commitid	8xYf5XKcBjmgfK35;

1.115
date	2017.02.20.08.12.47;	author mlarkin;	state Exp;
branches;
next	1.114;
commitid	F4T5WM4C3kZJVUPQ;

1.114
date	2017.02.20.07.22.50;	author mlarkin;	state Exp;
branches;
next	1.113;
commitid	Je1AtaLWjcWCJo8G;

1.113
date	2017.02.20.07.21.47;	author mlarkin;	state Exp;
branches;
next	1.112;
commitid	w1r9JlJ7u8ndbRfn;

1.112
date	2017.02.20.07.14.45;	author mlarkin;	state Exp;
branches;
next	1.111;
commitid	DQnBeBSMnTiDTh16;

1.111
date	2017.01.19.23.18.11;	author mlarkin;	state Exp;
branches;
next	1.110;
commitid	yuBL6bOKMDCDcnKR;

1.110
date	2017.01.19.05.53.40;	author mlarkin;	state Exp;
branches;
next	1.109;
commitid	jqUwhD8sBcg6IE3K;

1.109
date	2017.01.19.01.46.20;	author mlarkin;	state Exp;
branches;
next	1.108;
commitid	9MPdEvPC5yByOou5;

1.108
date	2017.01.19.01.33.45;	author mlarkin;	state Exp;
branches;
next	1.107;
commitid	QtJNxfg6Ki9fn2L9;

1.107
date	2017.01.19.00.03.52;	author mlarkin;	state Exp;
branches;
next	1.106;
commitid	qfoiOcR7PzCwVvnd;

1.106
date	2017.01.13.02.38.41;	author mlarkin;	state Exp;
branches;
next	1.105;
commitid	96yxME3QzaqMJtwl;

1.105
date	2017.01.12.09.02.47;	author mlarkin;	state Exp;
branches;
next	1.104;
commitid	38nlqcIpQulk95ZY;

1.104
date	2017.01.12.08.35.53;	author mlarkin;	state Exp;
branches;
next	1.103;
commitid	WuyJdmPqa9MpkqDM;

1.103
date	2017.01.10.09.02.29;	author mlarkin;	state Exp;
branches;
next	1.102;
commitid	Wl6IS8cz04nzuK5h;

1.102
date	2017.01.08.22.16.04;	author mlarkin;	state Exp;
branches;
next	1.101;
commitid	FdRG8gBsSXROaqMH;

1.101
date	2017.01.07.23.01.27;	author mlarkin;	state Exp;
branches;
next	1.100;
commitid	kp6r7DV6Y8dgz28e;

1.100
date	2017.01.07.22.37.03;	author mlarkin;	state Exp;
branches;
next	1.99;
commitid	cIi5h8LKIZ6xd1e7;

1.99
date	2017.01.03.09.48.15;	author mlarkin;	state Exp;
branches;
next	1.98;
commitid	rOPSd8O2Di7WnNkY;

1.98
date	2016.12.26.08.55.09;	author mlarkin;	state Exp;
branches;
next	1.97;
commitid	FszgXQ4OAu93Fc2F;

1.97
date	2016.11.15.11.25.38;	author jsg;	state Exp;
branches;
next	1.96;
commitid	Yp8uCP4NvzmqLyIR;

1.96
date	2016.11.08.07.10.55;	author mlarkin;	state Exp;
branches;
next	1.95;
commitid	CVVNnacDVOdQRN7j;

1.95
date	2016.10.29.09.24.54;	author reyk;	state Exp;
branches;
next	1.94;
commitid	uN0fOg6vmq4J7JSu;

1.94
date	2016.10.18.15.16.55;	author naddy;	state Exp;
branches;
next	1.93;
commitid	PSQjG9TWBYSL3INJ;

1.93
date	2016.10.13.19.36.25;	author martijn;	state Exp;
branches;
next	1.92;
commitid	hw7D4Jnj0S4fKrmP;

1.92
date	2016.10.06.18.52.09;	author reyk;	state Exp;
branches;
next	1.91;
commitid	U2wpdUyYrOpBvTXu;

1.91
date	2016.10.06.07.51.10;	author mlarkin;	state Exp;
branches;
next	1.90;
commitid	S0ghUhn1gM4Arvey;

1.90
date	2016.10.06.07.37.51;	author mlarkin;	state Exp;
branches;
next	1.89;
commitid	ZI7DVHMVrS3IZX7i;

1.89
date	2016.10.05.08.04.14;	author mlarkin;	state Exp;
branches;
next	1.88;
commitid	Gw1SpSunJEhXncao;

1.88
date	2016.10.03.04.53.54;	author mlarkin;	state Exp;
branches;
next	1.87;
commitid	SL6Z19PRyrIxqi88;

1.87
date	2016.09.25.08.20.40;	author mlarkin;	state Exp;
branches;
next	1.86;
commitid	WOwJSvZ3MFP1mnoi;

1.86
date	2016.09.25.07.45.02;	author mlarkin;	state Exp;
branches;
next	1.85;
commitid	O8qnLFTww6NqBaoC;

1.85
date	2016.09.17.06.43.38;	author jsg;	state Exp;
branches;
next	1.84;
commitid	X90n7VLPp4JcTBVS;

1.84
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.83;
commitid	RlO92XR575sygHqm;

1.83
date	2016.09.14.18.23.06;	author mlarkin;	state Exp;
branches;
next	1.82;
commitid	W75zdGKj3TjV5EN6;

1.82
date	2016.09.10.23.39.42;	author mlarkin;	state Exp;
branches;
next	1.81;
commitid	AC9qvk4Vmhy6yWiB;

1.81
date	2016.09.10.17.15.44;	author mlarkin;	state Exp;
branches;
next	1.80;
commitid	5UB7hcr8j6jKKV3d;

1.80
date	2016.09.07.15.35.13;	author mlarkin;	state Exp;
branches;
next	1.79;
commitid	xFIvg7PK98Ekh4ae;

1.79
date	2016.09.07.15.30.45;	author mlarkin;	state Exp;
branches;
next	1.78;
commitid	9v6ggeuDN4M2yuKU;

1.78
date	2016.09.05.07.50.04;	author mlarkin;	state Exp;
branches;
next	1.77;
commitid	ifI8e4FPZLmpckMs;

1.77
date	2016.09.04.08.49.18;	author mlarkin;	state Exp;
branches;
next	1.76;
commitid	QwHgCl0y5KPqqx14;

1.76
date	2016.09.03.14.01.17;	author mlarkin;	state Exp;
branches;
next	1.75;
commitid	lu1YP25JPOrZTnJJ;

1.75
date	2016.09.03.11.35.19;	author mlarkin;	state Exp;
branches;
next	1.74;
commitid	YjZRDYNzICsi1PWV;

1.74
date	2016.09.01.16.04.47;	author stefan;	state Exp;
branches;
next	1.73;
commitid	xgCRvP7hLEqkM3Qf;

1.73
date	2016.09.01.15.01.45;	author stefan;	state Exp;
branches;
next	1.72;
commitid	1e4MbaS66tHETpf4;

1.72
date	2016.09.01.14.45.36;	author mlarkin;	state Exp;
branches;
next	1.71;
commitid	K7ioSO1L1IqvraJ9;

1.71
date	2016.07.23.07.25.29;	author mlarkin;	state Exp;
branches;
next	1.70;
commitid	ujKYy78LQ4FEYSkd;

1.70
date	2016.07.23.07.17.21;	author mlarkin;	state Exp;
branches;
next	1.69;
commitid	HMKUu4jMJGhrSH3Y;

1.69
date	2016.07.23.07.00.39;	author mlarkin;	state Exp;
branches;
next	1.68;
commitid	1t9RpmocvsTnN8eP;

1.68
date	2016.07.16.18.36.41;	author mlarkin;	state Exp;
branches;
next	1.67;
commitid	Ph0CYRouN1va27Id;

1.67
date	2016.07.16.06.32.18;	author mlarkin;	state Exp;
branches;
next	1.66;
commitid	4dLfFpyLMBTUVXV2;

1.66
date	2016.07.13.06.57.35;	author mlarkin;	state Exp;
branches;
next	1.65;
commitid	Hx9wODA9cVbuM6Xf;

1.65
date	2016.06.28.05.50.55;	author mlarkin;	state Exp;
branches;
next	1.64;
commitid	WJfcDQtA1hTu8s9b;

1.64
date	2016.06.10.16.37.16;	author stefan;	state Exp;
branches;
next	1.63;
commitid	CoKzeU4gWMUOOnIe;

1.63
date	2016.06.07.16.19.06;	author stefan;	state Exp;
branches;
next	1.62;
commitid	N4IUFQQXbAPAUf9e;

1.62
date	2016.04.27.15.25.36;	author mlarkin;	state Exp;
branches;
next	1.61;
commitid	DVh0qncVV018WUv2;

1.61
date	2016.04.26.16.11.27;	author mlarkin;	state Exp;
branches;
next	1.60;
commitid	3EU0Is5pylP7akee;

1.60
date	2016.04.26.15.57.09;	author mlarkin;	state Exp;
branches;
next	1.59;
commitid	mdTcZEnOVnQ5oQmU;

1.59
date	2016.04.26.11.59.21;	author mlarkin;	state Exp;
branches;
next	1.58;
commitid	6ZEihuKlCGhWHC4O;

1.58
date	2016.04.25.19.53.45;	author mlarkin;	state Exp;
branches;
next	1.57;
commitid	3i7ZwaPQ7QAFU0pB;

1.57
date	2016.04.25.19.26.27;	author mlarkin;	state Exp;
branches;
next	1.56;
commitid	G4NfAyE0DrxGrca5;

1.56
date	2016.04.25.17.50.21;	author mlarkin;	state Exp;
branches;
next	1.55;
commitid	3emxXAo0TW6xsTks;

1.55
date	2016.04.25.15.24.55;	author mlarkin;	state Exp;
branches;
next	1.54;
commitid	IjKO3WbSR226gdka;

1.54
date	2016.04.17.00.15.28;	author dlg;	state Exp;
branches;
next	1.53;
commitid	A7VUrDfJ1kT7tSkC;

1.53
date	2016.04.13.04.44.41;	author mlarkin;	state Exp;
branches;
next	1.52;
commitid	8YgFac1TIWBnLgE6;

1.52
date	2016.04.12.06.41.09;	author mlarkin;	state Exp;
branches;
next	1.51;
commitid	XARJvG0XeU7jtlcX;

1.51
date	2016.04.12.04.42.58;	author mlarkin;	state Exp;
branches;
next	1.50;
commitid	EPDqWmC7qDaXfsw6;

1.50
date	2016.04.11.07.34.55;	author mlarkin;	state Exp;
branches;
next	1.49;
commitid	sHpHDEGESyB4Jiwh;

1.49
date	2016.04.11.06.58.34;	author mlarkin;	state Exp;
branches;
next	1.48;
commitid	XSJESUEwU78tzeZc;

1.48
date	2016.04.06.06.15.06;	author mlarkin;	state Exp;
branches;
next	1.47;
commitid	cX1Avd5qqK2AWhez;

1.47
date	2016.04.05.09.33.05;	author mlarkin;	state Exp;
branches;
next	1.46;
commitid	klY9sBeThwZpzkLp;

1.46
date	2016.04.04.16.47.34;	author stefan;	state Exp;
branches;
next	1.45;
commitid	EMw2bGjOuR7D4fL8;

1.45
date	2016.03.13.13.11.47;	author stefan;	state Exp;
branches;
next	1.44;
commitid	tNFHzbetplEWT4Tg;

1.44
date	2016.03.09.07.41.25;	author mlarkin;	state Exp;
branches;
next	1.43;
commitid	15Gd0EkE94rYo8uo;

1.43
date	2016.03.08.08.43.50;	author mlarkin;	state Exp;
branches;
next	1.42;
commitid	8aSwhfLl1hZd0fI1;

1.42
date	2016.03.08.08.36.40;	author mlarkin;	state Exp;
branches;
next	1.41;
commitid	zEMtLqCUIrfs78Hq;

1.41
date	2016.03.08.07.10.01;	author mlarkin;	state Exp;
branches;
next	1.40;
commitid	bSQiSU7XHkl7bhxY;

1.40
date	2016.03.03.18.45.42;	author stefan;	state Exp;
branches;
next	1.39;
commitid	ZNi5sdycU7JFcYUE;

1.39
date	2016.03.03.18.23.06;	author stefan;	state Exp;
branches;
next	1.38;
commitid	INHo6B2BmEPyyQ6Z;

1.38
date	2016.02.23.17.17.31;	author stefan;	state Exp;
branches
	1.38.2.1;
next	1.37;
commitid	220IvlhfxCR2w6EZ;

1.37
date	2016.02.23.17.15.09;	author stefan;	state Exp;
branches;
next	1.36;
commitid	aYJEWb6qotjqbqk7;

1.36
date	2016.02.20.20.49.08;	author mlarkin;	state Exp;
branches;
next	1.35;
commitid	J9Nf8wKYYiNBOlDr;

1.35
date	2016.02.16.18.59.30;	author stefan;	state Exp;
branches;
next	1.34;
commitid	wwRmEfOnB4C4MiVx;

1.34
date	2016.02.08.18.23.04;	author stefan;	state Exp;
branches;
next	1.33;
commitid	KlnooGxekwRiYYw9;

1.33
date	2016.01.29.00.47.51;	author jsg;	state Exp;
branches;
next	1.32;
commitid	xcvMKxZUakvBqIcu;

1.32
date	2016.01.25.12.44.16;	author jsg;	state Exp;
branches;
next	1.31;
commitid	pwPJ6ySUOyDgVKsO;

1.31
date	2016.01.10.18.18.25;	author stefan;	state Exp;
branches;
next	1.30;
commitid	LhXwmph3IQdIYLr4;

1.30
date	2016.01.08.11.20.58;	author reyk;	state Exp;
branches;
next	1.29;
commitid	75vkfVJpzZUMdF67;

1.29
date	2016.01.04.01.35.56;	author mlarkin;	state Exp;
branches;
next	1.28;
commitid	CdiRbflraxv1GBxf;

1.28
date	2015.12.24.09.40.27;	author mlarkin;	state Exp;
branches;
next	1.27;
commitid	evPznelLbmpfgJxE;

1.27
date	2015.12.24.09.26.45;	author mlarkin;	state Exp;
branches;
next	1.26;
commitid	XLUOsiswLlRj2kQm;

1.26
date	2015.12.17.09.29.28;	author mlarkin;	state Exp;
branches;
next	1.25;
commitid	8J3NuBrhKPtN9MxY;

1.25
date	2015.12.15.03.24.26;	author mlarkin;	state Exp;
branches;
next	1.24;
commitid	veyv44RoEo6KPmug;

1.24
date	2015.12.15.01.56.51;	author mlarkin;	state Exp;
branches;
next	1.23;
commitid	i1tEkWoVWghB5C5j;

1.23
date	2015.12.14.07.46.03;	author mlarkin;	state Exp;
branches;
next	1.22;
commitid	0PmVXiI1pg00Rs8q;

1.22
date	2015.12.14.06.59.07;	author mlarkin;	state Exp;
branches;
next	1.21;
commitid	u2MfphuvnzQceP1z;

1.21
date	2015.12.09.02.29.09;	author deraadt;	state Exp;
branches;
next	1.20;
commitid	irwnvT2t0NkzqrAX;

1.20
date	2015.12.06.20.12.15;	author mlarkin;	state Exp;
branches;
next	1.19;
commitid	Bx2myBQrNd8DpMOW;

1.19
date	2015.12.06.20.04.25;	author mlarkin;	state Exp;
branches;
next	1.18;
commitid	uVXjzRwyq1DlN9My;

1.18
date	2015.12.06.18.42.18;	author mlarkin;	state Exp;
branches;
next	1.17;
commitid	h4AC5DP46uwef1DR;

1.17
date	2015.12.06.18.31.26;	author mlarkin;	state Exp;
branches;
next	1.16;
commitid	AF4v64F9YkM0iMpE;

1.16
date	2015.12.06.01.16.58;	author mlarkin;	state Exp;
branches;
next	1.15;
commitid	tuWELREcxbiJZlI1;

1.15
date	2015.12.01.12.03.55;	author mpi;	state Exp;
branches;
next	1.14;
commitid	E6cvGDKhQMqi9Dec;

1.14
date	2015.12.01.12.01.38;	author mpi;	state Exp;
branches;
next	1.13;
commitid	xL9Ekkeu98OtbMn7;

1.13
date	2015.12.01.10.18.35;	author mpi;	state Exp;
branches;
next	1.12;
commitid	9gN10ZNs7LkNftet;

1.12
date	2015.12.01.10.14.05;	author mpi;	state Exp;
branches;
next	1.11;
commitid	uyTIcb4gkUJtJ08X;

1.11
date	2015.12.01.10.12.15;	author mpi;	state Exp;
branches;
next	1.10;
commitid	E4OTShwVg7rQ15CV;

1.10
date	2015.12.01.10.08.10;	author mpi;	state Exp;
branches;
next	1.9;
commitid	F6BcE6u0PNiFkOWX;

1.9
date	2015.11.26.08.32.09;	author jsg;	state Exp;
branches;
next	1.8;
commitid	bGLbuS3i4JLCXt2G;

1.8
date	2015.11.26.08.26.48;	author reyk;	state Exp;
branches;
next	1.7;
commitid	GRMyPzYJL6q2Nvq3;

1.7
date	2015.11.24.09.07.09;	author mlarkin;	state Exp;
branches;
next	1.6;
commitid	4NI0RC7HUdwRvjYm;

1.6
date	2015.11.21.11.16.30;	author mpi;	state Exp;
branches;
next	1.5;
commitid	pF74PlS6Ys5DdjEm;

1.5
date	2015.11.21.11.08.58;	author mpi;	state Exp;
branches;
next	1.4;
commitid	c4E2J5Hq3XKNsNKd;

1.4
date	2015.11.21.11.03.14;	author mpi;	state Exp;
branches;
next	1.3;
commitid	F0QJgQJk0WJwPbZ7;

1.3
date	2015.11.16.10.08.41;	author mpi;	state Exp;
branches;
next	1.2;
commitid	KZ30ShVeuLAwERed;

1.2
date	2015.11.14.06.15.37;	author mlarkin;	state Exp;
branches;
next	1.1;
commitid	wXw982nbMFMWmX8m;

1.1
date	2015.11.13.07.52.20;	author mlarkin;	state Exp;
branches;
next	;
commitid	KdsjYlqLpqhwOUHf;

1.38.2.1
date	2016.07.14.17.28.38;	author stefan;	state Exp;
branches;
next	;
commitid	Yza0ve6fAQG6RRw1;


desc
@@


1.131
log
@Properly handle VMX entry controls governing guest processor mode.

Before seabios, this didn't matter much but now it does since various
bootloaders/kernels need such treatment.

ok deraadt
@
text
@/*	$OpenBSD: vmm.c,v 1.130 2017/03/27 19:00:38 mlarkin Exp $	*/
/*
 * Copyright (c) 2014 Mike Larkin <mlarkin@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/types.h>
#include <sys/signalvar.h>
#include <sys/malloc.h>
#include <sys/device.h>
#include <sys/pool.h>
#include <sys/proc.h>
#include <sys/ioctl.h>
#include <sys/queue.h>
#include <sys/rwlock.h>
#include <sys/pledge.h>
#include <sys/memrange.h>

#include <uvm/uvm_extern.h>

#include <machine/pmap.h>
#include <machine/biosvar.h>
#include <machine/segments.h>
#include <machine/cpufunc.h>
#include <machine/vmmvar.h>
#include <machine/i82489reg.h>

#include <dev/isa/isareg.h>

/* #define VMM_DEBUG */

#ifdef VMM_DEBUG
int vmm_debug = 0;
#define DPRINTF(x...)	do { if (vmm_debug) printf(x); } while(0)
#else
#define DPRINTF(x...)
#endif /* VMM_DEBUG */

#define DEVNAME(s)  ((s)->sc_dev.dv_xname)

#define CTRL_DUMP(x,y,z) printf("     %s: Can set:%s Can clear:%s\n", #z , \
				vcpu_vmx_check_cap(x, IA32_VMX_##y ##_CTLS, \
				IA32_VMX_##z, 1) ? "Yes" : "No", \
				vcpu_vmx_check_cap(x, IA32_VMX_##y ##_CTLS, \
				IA32_VMX_##z, 0) ? "Yes" : "No");

#define VMX_EXIT_INFO_HAVE_RIP		0x1
#define VMX_EXIT_INFO_HAVE_REASON	0x2
#define VMX_EXIT_INFO_COMPLETE				\
    (VMX_EXIT_INFO_HAVE_RIP | VMX_EXIT_INFO_HAVE_REASON)

struct vm {
	vm_map_t		 vm_map;
	uint32_t		 vm_id;
	pid_t			 vm_creator_pid;
	size_t			 vm_nmemranges;
	size_t			 vm_memory_size;
	char			 vm_name[VMM_MAX_NAME_LEN];
	struct vm_mem_range	 vm_memranges[VMM_MAX_MEM_RANGES];

	struct vcpu_head	 vm_vcpu_list;
	uint32_t		 vm_vcpu_ct;
	u_int			 vm_vcpus_running;
	struct rwlock		 vm_vcpu_lock;

	SLIST_ENTRY(vm)		 vm_link;
};

SLIST_HEAD(vmlist_head, vm);

struct vmm_softc {
	struct device		sc_dev;

	/* Capabilities */
	uint32_t		nr_vmx_cpus;
	uint32_t		nr_svm_cpus;
	uint32_t		nr_rvi_cpus;
	uint32_t		nr_ept_cpus;

	/* Managed VMs */
	struct vmlist_head	vm_list;

	int			mode;

	struct rwlock		vm_lock;
	size_t			vm_ct;		/* number of in-memory VMs */
	size_t			vm_idx;		/* next unique VM index */
};

int vmm_enabled(void);
int vmm_probe(struct device *, void *, void *);
void vmm_attach(struct device *, struct device *, void *);
int vmmopen(dev_t, int, int, struct proc *);
int vmmioctl(dev_t, u_long, caddr_t, int, struct proc *);
int vmmclose(dev_t, int, int, struct proc *);
int vmm_start(void);
int vmm_stop(void);
size_t vm_create_check_mem_ranges(struct vm_create_params *);
int vm_create(struct vm_create_params *, struct proc *);
int vm_run(struct vm_run_params *);
int vm_terminate(struct vm_terminate_params *);
int vm_get_info(struct vm_info_params *);
int vm_resetcpu(struct vm_resetcpu_params *);
int vm_intr_pending(struct vm_intr_params *);
int vm_rwregs(struct vm_rwregs_params *, int);
int vm_find(uint32_t, struct vm **);
int vcpu_readregs_vmx(struct vcpu *, uint64_t, struct vcpu_reg_state *);
int vcpu_readregs_svm(struct vcpu *, uint64_t, struct vcpu_reg_state *);
int vcpu_writeregs_vmx(struct vcpu *, uint64_t, int, struct vcpu_reg_state *);
int vcpu_writeregs_svm(struct vcpu *, uint64_t, struct vcpu_reg_state *);
int vcpu_reset_regs(struct vcpu *, struct vcpu_reg_state *);
int vcpu_reset_regs_vmx(struct vcpu *, struct vcpu_reg_state *);
int vcpu_reset_regs_svm(struct vcpu *, struct vcpu_reg_state *);
int vcpu_reload_vmcs_vmx(uint64_t *);
int vcpu_init(struct vcpu *);
int vcpu_init_vmx(struct vcpu *);
int vcpu_init_svm(struct vcpu *);
int vcpu_must_stop(struct vcpu *);
int vcpu_run_vmx(struct vcpu *, struct vm_run_params *);
int vcpu_run_svm(struct vcpu *, struct vm_run_params *);
void vcpu_deinit(struct vcpu *);
void vcpu_deinit_vmx(struct vcpu *);
void vcpu_deinit_svm(struct vcpu *);
int vm_impl_init(struct vm *, struct proc *);
int vm_impl_init_vmx(struct vm *, struct proc *);
int vm_impl_init_svm(struct vm *, struct proc *);
void vm_impl_deinit(struct vm *);
void vm_impl_deinit_vmx(struct vm *);
void vm_impl_deinit_svm(struct vm *);
void vm_teardown(struct vm *);
int vcpu_vmx_check_cap(struct vcpu *, uint32_t, uint32_t, int);
int vcpu_vmx_compute_ctrl(uint64_t, uint16_t, uint32_t, uint32_t, uint32_t *);
int vmx_get_exit_info(uint64_t *, uint64_t *);
int vmx_handle_exit(struct vcpu *);
int vmm_handle_cpuid(struct vcpu *);
int vmx_handle_rdmsr(struct vcpu *);
int vmx_handle_wrmsr(struct vcpu *);
int vmx_handle_cr0_write(struct vcpu *, uint64_t);
int vmx_handle_cr4_write(struct vcpu *, uint64_t);
int vmx_handle_cr(struct vcpu *);
int vmx_handle_inout(struct vcpu *);
int vmx_handle_hlt(struct vcpu *);
void vmx_handle_intr(struct vcpu *);
void vmx_handle_intwin(struct vcpu *);
int vmm_get_guest_memtype(struct vm *, paddr_t);
int vmm_get_guest_faulttype(void);
int vmx_get_guest_faulttype(void);
int svm_get_guest_faulttype(void);
int vmx_get_exit_qualification(uint64_t *);
int vmx_fault_page(struct vcpu *, paddr_t);
int vmx_handle_np_fault(struct vcpu *);
const char *vcpu_state_decode(u_int);
const char *vmx_exit_reason_decode(uint32_t);
const char *vmx_instruction_error_decode(uint32_t);
void svm_setmsrbr(struct vcpu *, uint32_t);
void svm_setmsrbw(struct vcpu *, uint32_t);
void svm_setmsrbrw(struct vcpu *, uint32_t);
void vmx_setmsrbr(struct vcpu *, uint32_t);
void vmx_setmsrbw(struct vcpu *, uint32_t);
void vmx_setmsrbrw(struct vcpu *, uint32_t);

#ifdef VMM_DEBUG
void dump_vcpu(struct vcpu *);
void vmx_vcpu_dump_regs(struct vcpu *);
void vmx_dump_vmcs(struct vcpu *);
const char *msr_name_decode(uint32_t);
void vmm_segment_desc_decode(uint64_t);
void vmm_decode_cr0(uint64_t);
void vmm_decode_cr3(uint64_t);
void vmm_decode_cr4(uint64_t);
void vmm_decode_msr_value(uint64_t, uint64_t);
void vmm_decode_apicbase_msr_value(uint64_t);
void vmm_decode_ia32_fc_value(uint64_t);
void vmm_decode_mtrrcap_value(uint64_t);
void vmm_decode_perf_status_value(uint64_t);
void vmm_decode_perf_ctl_value(uint64_t);
void vmm_decode_mtrrdeftype_value(uint64_t);
void vmm_decode_efer_value(uint64_t);

extern int mtrr2mrt(int);

struct vmm_reg_debug_info {
	uint64_t	vrdi_bit;
	const char	*vrdi_present;
	const char	*vrdi_absent;
};
#endif /* VMM_DEBUG */

const char *vmm_hv_signature = VMM_HV_SIGNATURE;

const struct kmem_pa_mode vmm_kp_contig = {
	.kp_constraint = &no_constraint,
	.kp_maxseg = 1,
	.kp_align = 4096,
	.kp_zero = 1,
};

struct cfdriver vmm_cd = {
	NULL, "vmm", DV_DULL
};

const struct cfattach vmm_ca = {
	sizeof(struct vmm_softc), vmm_probe, vmm_attach, NULL, NULL
};

/*
 * Helper struct to easily get the VMCS field IDs needed in vmread/vmwrite
 * to access the individual fields of the guest segment registers. This
 * struct is indexed by VCPU_REGS_* id.
 */
const struct {
	uint64_t selid;
	uint64_t limitid;
	uint64_t arid;
	uint64_t baseid;
} vmm_vmx_sreg_vmcs_fields[] = {
	{ VMCS_GUEST_IA32_CS_SEL, VMCS_GUEST_IA32_CS_LIMIT,
	  VMCS_GUEST_IA32_CS_AR, VMCS_GUEST_IA32_CS_BASE },
	{ VMCS_GUEST_IA32_DS_SEL, VMCS_GUEST_IA32_DS_LIMIT,
	  VMCS_GUEST_IA32_DS_AR, VMCS_GUEST_IA32_DS_BASE },
	{ VMCS_GUEST_IA32_ES_SEL, VMCS_GUEST_IA32_ES_LIMIT,
	  VMCS_GUEST_IA32_ES_AR, VMCS_GUEST_IA32_ES_BASE },
	{ VMCS_GUEST_IA32_FS_SEL, VMCS_GUEST_IA32_FS_LIMIT,
	  VMCS_GUEST_IA32_FS_AR, VMCS_GUEST_IA32_FS_BASE },
	{ VMCS_GUEST_IA32_GS_SEL, VMCS_GUEST_IA32_GS_LIMIT,
	  VMCS_GUEST_IA32_GS_AR, VMCS_GUEST_IA32_GS_BASE },
	{ VMCS_GUEST_IA32_SS_SEL, VMCS_GUEST_IA32_SS_LIMIT,
	  VMCS_GUEST_IA32_SS_AR, VMCS_GUEST_IA32_SS_BASE },
	{ VMCS_GUEST_IA32_LDTR_SEL, VMCS_GUEST_IA32_LDTR_LIMIT,
	  VMCS_GUEST_IA32_LDTR_AR, VMCS_GUEST_IA32_LDTR_BASE },
	{ VMCS_GUEST_IA32_TR_SEL, VMCS_GUEST_IA32_TR_LIMIT,
	  VMCS_GUEST_IA32_TR_AR, VMCS_GUEST_IA32_TR_BASE }
};

/* Pools for VMs and VCPUs */
struct pool vm_pool;
struct pool vcpu_pool;

struct vmm_softc *vmm_softc;

/* IDT information used when populating host state area */
extern vaddr_t idt_vaddr;
extern struct gate_descriptor *idt;

/* Constants used in "CR access exit" */
#define CR_WRITE	0
#define CR_READ		1
#define CR_CLTS		2
#define CR_LMSW		3

/*
 * vmm_enabled
 *
 * Checks if we have at least one CPU with either VMX or SVM.
 * Returns 1 if we have at least one of either type, but not both, 0 otherwise.
 */
int
vmm_enabled(void)
{
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
	int found_vmx = 0, found_svm = 0, vmm_disabled = 0;

	/* Check if we have at least one CPU with either VMX or SVM */
	CPU_INFO_FOREACH(cii, ci) {
		if (ci->ci_vmm_flags & CI_VMM_VMX)
			found_vmx = 1;
		if (ci->ci_vmm_flags & CI_VMM_SVM)
			found_svm = 1;
		if (ci->ci_vmm_flags & CI_VMM_DIS)
			vmm_disabled = 1;
	}

	/* Don't support both SVM and VMX at the same time */
	if (found_vmx && found_svm)
		return (0);

	/* SVM is not implemented yet */
	if (found_vmx)
		return 1;

	return 0;
}

int
vmm_probe(struct device *parent, void *match, void *aux)
{
	const char **busname = (const char **)aux;

	if (strcmp(*busname, vmm_cd.cd_name) != 0)
		return (0);
	return (1);
}

/*
 * vmm_attach
 *
 * Calculates how many of each type of CPU we have, prints this into dmesg
 * during attach. Initializes various locks, pools, and list structures for the
 * VMM.
 */
void
vmm_attach(struct device *parent, struct device *self, void *aux)
{
	struct vmm_softc *sc = (struct vmm_softc *)self;
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;

	sc->nr_vmx_cpus = 0;
	sc->nr_svm_cpus = 0;
	sc->nr_rvi_cpus = 0;
	sc->nr_ept_cpus = 0;
	sc->vm_ct = 0;
	sc->vm_idx = 0;

	/* Calculate CPU features */
	CPU_INFO_FOREACH(cii, ci) {
		if (ci->ci_vmm_flags & CI_VMM_VMX)
			sc->nr_vmx_cpus++;
		if (ci->ci_vmm_flags & CI_VMM_SVM)
			sc->nr_svm_cpus++;
		if (ci->ci_vmm_flags & CI_VMM_RVI)
			sc->nr_rvi_cpus++;
		if (ci->ci_vmm_flags & CI_VMM_EPT)
			sc->nr_ept_cpus++;
	}

	SLIST_INIT(&sc->vm_list);
	rw_init(&sc->vm_lock, "vmlistlock");

	if (sc->nr_ept_cpus) {
		printf(": VMX/EPT\n");
		sc->mode = VMM_MODE_EPT;
	} else if (sc->nr_vmx_cpus) {
		printf(": VMX\n");
		sc->mode = VMM_MODE_VMX;
	} else if (sc->nr_rvi_cpus) {
		printf(": SVM/RVI\n");
		sc->mode = VMM_MODE_RVI;
	} else if (sc->nr_svm_cpus) {
		printf(": SVM\n");
		sc->mode = VMM_MODE_SVM;
	} else {
		printf(": unknown\n");
		sc->mode = VMM_MODE_UNKNOWN;
	}

	pool_init(&vm_pool, sizeof(struct vm), 0, IPL_NONE, PR_WAITOK,
	    "vmpool", NULL);
	pool_init(&vcpu_pool, sizeof(struct vcpu), 0, IPL_NONE, PR_WAITOK,
	    "vcpupl", NULL);

	vmm_softc = sc;
}

/*
 * vmmopen
 *
 * Called during open of /dev/vmm. Presently unused.
 */
int
vmmopen(dev_t dev, int flag, int mode, struct proc *p)
{
	/* Don't allow open if we didn't attach */
	if (vmm_softc == NULL)
		return (ENODEV);

	/* Don't allow open if we didn't detect any supported CPUs */
	/* XXX presently this means EPT until SP and SVM are back */
	if (vmm_softc->mode != VMM_MODE_EPT)
		return (ENODEV);

	return 0;
}

/*
 * vmmioctl
 *
 * Main ioctl dispatch routine for /dev/vmm. Parses ioctl type and calls
 * appropriate lower level handler routine. Returns result to ioctl caller.
 */
int
vmmioctl(dev_t dev, u_long cmd, caddr_t data, int flag, struct proc *p)
{
	int ret;

	switch (cmd) {
	case VMM_IOC_CREATE:
		if ((ret = vmm_start()) != 0) {
			vmm_stop();
			break;
		}
		ret = vm_create((struct vm_create_params *)data, p);
		break;
	case VMM_IOC_RUN:
		ret = vm_run((struct vm_run_params *)data);
		break;
	case VMM_IOC_INFO:
		ret = vm_get_info((struct vm_info_params *)data);
		break;
	case VMM_IOC_TERM:
		ret = vm_terminate((struct vm_terminate_params *)data);
		break;
	case VMM_IOC_RESETCPU:
		ret = vm_resetcpu((struct vm_resetcpu_params *)data);
		break;
	case VMM_IOC_INTR:
		ret = vm_intr_pending((struct vm_intr_params *)data);
		break;
	case VMM_IOC_READREGS:
		ret = vm_rwregs((struct vm_rwregs_params *)data, 0);
		break;
	case VMM_IOC_WRITEREGS:
		ret = vm_rwregs((struct vm_rwregs_params *)data, 1);
		break;
	default:
		DPRINTF("vmmioctl: unknown ioctl code 0x%lx\n", cmd);
		ret = ENOTTY;
	}

	return (ret);
}

/*
 * pledge_ioctl_vmm
 *
 * Restrict the allowed ioctls in a pledged process context.
 * Is called from pledge_ioctl().
 */
int
pledge_ioctl_vmm(struct proc *p, long com)
{
	switch (com) {
	case VMM_IOC_CREATE:
	case VMM_IOC_INFO:
		/* The "parent" process in vmd forks and manages VMs */
		if (p->p_p->ps_pledge & PLEDGE_PROC)
			return (0);
		break;
	case VMM_IOC_TERM:
		/* XXX VM processes should only terminate themselves */
	case VMM_IOC_RUN:
	case VMM_IOC_RESETCPU:
	case VMM_IOC_INTR:
	case VMM_IOC_READREGS:
	case VMM_IOC_WRITEREGS:
		return (0);
	}

	return (EPERM);
}

/*
 * vmmclose
 *
 * Called when /dev/vmm is closed. Presently unused.
 */
int
vmmclose(dev_t dev, int flag, int mode, struct proc *p)
{
	return 0;
}

/*
 * vm_resetcpu
 *
 * Resets the vcpu defined in 'vrp' to power-on-init register state
 *
 * Parameters:
 *  vrp: ioctl structure defining the vcpu to reset (see vmmvar.h)
 *
 * Returns 0 if successful, or various error codes on failure:
 *  ENOENT if the VM id contained in 'vrp' refers to an unknown VM or
 *      if vrp describes an unknown vcpu for this VM
 *  EBUSY if the indicated VCPU is not stopped
 *  EIO if the indicated VCPU failed to reset
 */
int
vm_resetcpu(struct vm_resetcpu_params *vrp)
{
	struct vm *vm;
	struct vcpu *vcpu;
	int error;

	/* Find the desired VM */
	rw_enter_read(&vmm_softc->vm_lock);
	error = vm_find(vrp->vrp_vm_id, &vm);
	rw_exit_read(&vmm_softc->vm_lock);

	/* Not found? exit. */
	if (error != 0) {
		DPRINTF("vm_resetcpu: vm id %u not found\n",
		    vrp->vrp_vm_id);
		return (error);
	}

	rw_enter_read(&vm->vm_vcpu_lock);
	SLIST_FOREACH(vcpu, &vm->vm_vcpu_list, vc_vcpu_link) {
		if (vcpu->vc_id == vrp->vrp_vcpu_id)
			break;
	}
	rw_exit_read(&vm->vm_vcpu_lock);

	if (vcpu == NULL) {
		DPRINTF("vm_resetcpu: vcpu id %u of vm %u not found\n",
		    vrp->vrp_vcpu_id, vrp->vrp_vm_id);
		return (ENOENT);
	}

	if (vcpu->vc_state != VCPU_STATE_STOPPED) {
		DPRINTF("vm_resetcpu: reset of vcpu %u on vm %u attempted "
		    "while vcpu was in state %u (%s)\n", vrp->vrp_vcpu_id,
		    vrp->vrp_vm_id, vcpu->vc_state,
		    vcpu_state_decode(vcpu->vc_state));
		
		return (EBUSY);
	}

	DPRINTF("vm_resetcpu: resetting vm %d vcpu %d to power on defaults\n",
	    vm->vm_id, vcpu->vc_id);

	if (vcpu_reset_regs(vcpu, &vrp->vrp_init_state)) {
		printf("vm_resetcpu: failed\n");
#ifdef VMM_DEBUG
		dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
		return (EIO);
	}

	return (0);
}

/*
 * vm_intr_pending
 *
 * IOCTL handler routine for VMM_IOC_INTR messages, sent from vmd when an
 * interrupt is pending and needs acknowledgment
 *
 * Parameters:
 *  vip: Describes the vm/vcpu for which the interrupt is pending
 *
 * Return values:
 *  0: if successful
 *  ENOENT: if the VM/VCPU defined by 'vip' cannot be found
 */
int
vm_intr_pending(struct vm_intr_params *vip)
{
	struct vm *vm;
	struct vcpu *vcpu;
	int error;
	
	/* Find the desired VM */
	rw_enter_read(&vmm_softc->vm_lock);
	error = vm_find(vip->vip_vm_id, &vm);

	/* Not found? exit. */
	if (error != 0) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (error);
	}

	rw_enter_read(&vm->vm_vcpu_lock);
	SLIST_FOREACH(vcpu, &vm->vm_vcpu_list, vc_vcpu_link) {
		if (vcpu->vc_id == vip->vip_vcpu_id)
			break;
	}
	rw_exit_read(&vm->vm_vcpu_lock);
	rw_exit_read(&vmm_softc->vm_lock);

	if (vcpu == NULL)
		return (ENOENT);

	vcpu->vc_intr = vip->vip_intr;

#ifdef MULTIPROCESSOR
	/*
	 * If the vcpu is running on another PCPU, attempt to force it
	 * to exit to process the pending interrupt. This could race as
	 * it could be running when we do the check but be stopped by the
	 * time we send the IPI. In this case, there is a small extra
	 * overhead to process the IPI but no other side effects.
	 *
	 * There is also a chance that the vcpu may have interrupts blocked.
	 * That's ok as that condition will be checked on exit, and we will
	 * simply re-enter the guest. This "fast notification" is done only
	 * as an optimization.
	 */
	if (vcpu->vc_state == VCPU_STATE_RUNNING &&
	    vip->vip_intr == 1)
		x86_send_ipi(vcpu->vc_last_pcpu, X86_IPI_NOP);
#endif /* MULTIPROCESSOR */

	return (0);
}

/*
 * vm_readregs
 *
 * IOCTL handler to read/write the current register values of a guest VCPU.
 * The VCPU must not be running.
 *
 * Parameters:
 *   vrwp: Describes the VM and VCPU to get/set the registers from. The
 *   register values are returned here as well.
 *   dir: 0 for reading, 1 for writing
 *
 * Return values:
 *  0: if successful
 *  ENOENT: if the VM/VCPU defined by 'vgp' cannot be found
 *  EINVAL: if an error occured reading the registers of the guest
 */
int
vm_rwregs(struct vm_rwregs_params *vrwp, int dir)
{
	struct vm *vm;
	struct vcpu *vcpu;
	struct vcpu_reg_state *vrs = &vrwp->vrwp_regs;
	int error;

	/* Find the desired VM */
	rw_enter_read(&vmm_softc->vm_lock);
	error = vm_find(vrwp->vrwp_vm_id, &vm);

	/* Not found? exit. */
	if (error != 0) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (error);
	}

	rw_enter_read(&vm->vm_vcpu_lock);
	SLIST_FOREACH(vcpu, &vm->vm_vcpu_list, vc_vcpu_link) {
		if (vcpu->vc_id == vrwp->vrwp_vcpu_id)
			break;
	}
	rw_exit_read(&vm->vm_vcpu_lock);
	rw_exit_read(&vmm_softc->vm_lock);

	if (vcpu == NULL)
		return (ENOENT);

	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT)
		return (dir == 0) ?
		    vcpu_readregs_vmx(vcpu, vrwp->vrwp_mask, vrs) :
		    vcpu_writeregs_vmx(vcpu, vrwp->vrwp_mask, 1, vrs);
	else if (vmm_softc->mode == VMM_MODE_SVM ||
	    vmm_softc->mode == VMM_MODE_RVI)
		return (dir == 0) ?
		    vcpu_readregs_svm(vcpu, vrwp->vrwp_mask, vrs) :
		    vcpu_writeregs_svm(vcpu, vrwp->vrwp_mask, vrs);
	else
		panic("unknown vmm mode\n");
}

/*
 * vm_find
 *
 * Function to find an existing VM by its identifier.
 * Must be called under the global vm_lock.
 *
 * Parameters:
 *  id: The VM identifier.
 *  *res: A pointer to the VM or NULL if not found
 *
 * Return values:
 *  0: if successful
 *  ENOENT: if the VM defined by 'id' cannot be found
 *  EPERM: if the VM cannot be accessed by the current process
 */
int
vm_find(uint32_t id, struct vm **res)
{
	struct proc *p = curproc;
	struct vm *vm;

	*res = NULL;
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == id) {
			/* 
			 * In the pledged VM process, only allow to find
			 * the VM that is running in the current process.
			 * The managing vmm parent process can lookup all
			 * all VMs and is indicated by PLEDGE_PROC.
			 */
			if (((p->p_p->ps_pledge &
			    (PLEDGE_VMM|PLEDGE_PROC)) == PLEDGE_VMM) &&
			    (vm->vm_creator_pid != p->p_p->ps_pid))
				return (pledge_fail(p, EPERM, PLEDGE_VMM));
			*res = vm;
			return (0);
		}
	}

	return (ENOENT);
}

/*
 * vmm_start
 *
 * Starts VMM mode on the system
 */
int
vmm_start(void)
{
	struct cpu_info *self = curcpu();
	int ret = 0;
#ifdef MULTIPROCESSOR
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
	int i;
#endif

	/* VMM is already running */
	if (self->ci_flags & CPUF_VMM)
		return (0);

#ifdef MULTIPROCESSOR
	/* Broadcast start VMM IPI */
	x86_broadcast_ipi(X86_IPI_START_VMM);

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self)
			continue;
		for (i = 100000; (!(ci->ci_flags & CPUF_VMM)) && i>0;i--)
			delay(10);
		if (!(ci->ci_flags & CPUF_VMM)) {
			printf("%s: failed to enter VMM mode\n",
				ci->ci_dev->dv_xname);
			ret = EIO;
		}
	}
#endif /* MULTIPROCESSOR */

	/* Start VMM on this CPU */
	start_vmm_on_cpu(self);
	if (!(self->ci_flags & CPUF_VMM)) {
		printf("%s: failed to enter VMM mode\n",
			self->ci_dev->dv_xname);
		ret = EIO;
	}

	return (ret);
}

/*
 * vmm_stop
 *
 * Stops VMM mode on the system
 */
int
vmm_stop(void)
{
	struct cpu_info *self = curcpu();
	int ret = 0;
#ifdef MULTIPROCESSOR
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
	int i;
#endif

	/* VMM is not running */
	if (!(self->ci_flags & CPUF_VMM))
		return (0);

#ifdef MULTIPROCESSOR
	/* Stop VMM on other CPUs */
	x86_broadcast_ipi(X86_IPI_STOP_VMM);

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self)
			continue;
		for (i = 100000; (ci->ci_flags & CPUF_VMM) && i>0 ;i--)
			delay(10);
		if (ci->ci_flags & CPUF_VMM) {
			printf("%s: failed to exit VMM mode\n",
				ci->ci_dev->dv_xname);
			ret = EIO;
		}
	}
#endif /* MULTIPROCESSOR */

	/* Stop VMM on this CPU */
	stop_vmm_on_cpu(self);
	if (self->ci_flags & CPUF_VMM) {
		printf("%s: failed to exit VMM mode\n",
			self->ci_dev->dv_xname);
		ret = EIO;
	}

	return (ret);
}

/*
 * start_vmm_on_cpu
 *
 * Starts VMM mode on 'ci' by executing the appropriate CPU-specific insn
 * sequence to enter VMM mode (eg, VMXON)
 */
void
start_vmm_on_cpu(struct cpu_info *ci)
{
	uint64_t msr;
	uint32_t cr4;

	/* No VMM mode? exit. */
	if ((ci->ci_vmm_flags & CI_VMM_VMX) == 0 &&
	    (ci->ci_vmm_flags & CI_VMM_SVM) == 0)
		return;

	/*
	 * AMD SVM
	 */
	if (ci->ci_vmm_flags & CI_VMM_SVM) {
		msr = rdmsr(MSR_EFER);
		msr |= EFER_SVME;
		wrmsr(MSR_EFER, msr);
	}

	/*
	 * Intel VMX
	 */
	if (ci->ci_vmm_flags & CI_VMM_VMX) {
		if (ci->ci_vmxon_region == 0)
			return;
		else {
			bzero(ci->ci_vmxon_region, PAGE_SIZE);
			ci->ci_vmxon_region->vr_revision =
			    ci->ci_vmm_cap.vcc_vmx.vmx_vmxon_revision;

			/* Set CR4.VMXE */
			cr4 = rcr4();
			cr4 |= CR4_VMXE;
			lcr4(cr4);

			/* Enable VMX */
			msr = rdmsr(MSR_IA32_FEATURE_CONTROL);
			if (msr & IA32_FEATURE_CONTROL_LOCK) {
				if (!(msr & IA32_FEATURE_CONTROL_VMX_EN))
					return;
			} else {
				msr |= IA32_FEATURE_CONTROL_VMX_EN |
				    IA32_FEATURE_CONTROL_LOCK;
				wrmsr(MSR_IA32_FEATURE_CONTROL, msr);
			}

			/* Enter VMX mode */
			if (vmxon((uint64_t *)&ci->ci_vmxon_region_pa))
				return;
		}
	}

	ci->ci_flags |= CPUF_VMM;
}

/*
 * stop_vmm_on_cpu
 *
 * Stops VMM mode on 'ci' by executing the appropriate CPU-specific insn
 * sequence to exit VMM mode (eg, VMXOFF)
 */
void
stop_vmm_on_cpu(struct cpu_info *ci)
{
	uint64_t msr;
	uint32_t cr4;

	if (!(ci->ci_flags & CPUF_VMM))
		return;

	/*
	 * AMD SVM
	 */
	if (ci->ci_vmm_flags & CI_VMM_SVM) {
		msr = rdmsr(MSR_EFER);
		msr &= ~EFER_SVME;
		wrmsr(MSR_EFER, msr);
	}

	/*
	 * Intel VMX
	 */
	if (ci->ci_vmm_flags & CI_VMM_VMX) {
		if (vmxoff())
			panic("VMXOFF failed\n");

		cr4 = rcr4();
		cr4 &= ~CR4_VMXE;
		lcr4(cr4);
	}

	ci->ci_flags &= ~CPUF_VMM;
}

/*
 * vm_create_check_mem_ranges:
 *
 * Make sure that the guest physical memory ranges given by the user process
 * do not overlap and are in ascending order.
 *
 * The last physical address may not exceed VMM_MAX_VM_MEM_SIZE.
 *
 * Return Values:
 *   The total memory size in MB if the checks were successful
 *   0: One of the memory ranges was invalid, or VMM_MAX_VM_MEM_SIZE was
 *   exceeded
 */
size_t
vm_create_check_mem_ranges(struct vm_create_params *vcp)
{
	int disjunct_range;
	size_t i, memsize = 0;
	struct vm_mem_range *vmr, *pvmr;
	const paddr_t maxgpa = (uint64_t)VMM_MAX_VM_MEM_SIZE * 1024 * 1024;

	if (vcp->vcp_nmemranges == 0 ||
	    vcp->vcp_nmemranges > VMM_MAX_MEM_RANGES)
		return (0);

	for (i = 0; i < vcp->vcp_nmemranges; i++) {
		vmr = &vcp->vcp_memranges[i];

		/* Only page-aligned addresses and sizes are permitted */
		if ((vmr->vmr_gpa & PAGE_MASK) || (vmr->vmr_va & PAGE_MASK) ||
		    (vmr->vmr_size & PAGE_MASK) || vmr->vmr_size == 0)
			return (0);

		/* Make sure that VMM_MAX_VM_MEM_SIZE is not exceeded */
		if (vmr->vmr_gpa >= maxgpa ||
		    vmr->vmr_size > maxgpa - vmr->vmr_gpa)
			return (0);

		/*
		 * Make sure that all virtual addresses are within the address
		 * space of the process and that they do not wrap around.
		 * Calling uvm_share() when creating the VM will take care of
		 * further checks.
		 */
		if (vmr->vmr_va < VM_MIN_ADDRESS ||
		    vmr->vmr_va >= VM_MAXUSER_ADDRESS ||
		    vmr->vmr_size >= VM_MAXUSER_ADDRESS - vmr->vmr_va)
			return (0);

		/* Specifying ranges within the PCI MMIO space is forbidden */
		disjunct_range = (vmr->vmr_gpa > VMM_PCI_MMIO_BAR_END) ||
		    (vmr->vmr_gpa + vmr->vmr_size <= VMM_PCI_MMIO_BAR_BASE);
		if (!disjunct_range)
			return (0);

		/*
		 * Make sure that guest physcal memory ranges do not overlap
		 * and that they are ascending.
		 */
		if (i > 0 && pvmr->vmr_gpa + pvmr->vmr_size > vmr->vmr_gpa)
			return (0);

		memsize += vmr->vmr_size;
		pvmr = vmr;
	}

	if (memsize % (1024 * 1024) != 0)
		return (0);
	memsize /= 1024 * 1024;
	return (memsize);
}

/*
 * vm_create
 *
 * Creates the in-memory VMM structures for the VM defined by 'vcp'. The
 * parent of this VM shall be the process defined by 'p'.
 * This function does not start the VCPU(s) - see vm_start.
 *
 * Return Values:
 *  0: the create operation was successful
 *  ENOMEM: out of memory
 *  various other errors from vcpu_init/vm_impl_init
 */
int
vm_create(struct vm_create_params *vcp, struct proc *p)
{
	int i, ret;
	size_t memsize;
	struct vm *vm;
	struct vcpu *vcpu;

	if (!(curcpu()->ci_flags & CPUF_VMM))
		return (EINVAL);

	memsize = vm_create_check_mem_ranges(vcp);
	if (memsize == 0)
		return (EINVAL);

	/* XXX - support UP only (for now) */
	if (vcp->vcp_ncpus != 1)
		return (EINVAL);

	vm = pool_get(&vm_pool, PR_WAITOK | PR_ZERO);
	SLIST_INIT(&vm->vm_vcpu_list);
	rw_init(&vm->vm_vcpu_lock, "vcpulock");

	vm->vm_creator_pid = p->p_p->ps_pid;
	vm->vm_nmemranges = vcp->vcp_nmemranges;
	memcpy(vm->vm_memranges, vcp->vcp_memranges,
	    vm->vm_nmemranges * sizeof(vm->vm_memranges[0]));
	vm->vm_memory_size = memsize;
	strncpy(vm->vm_name, vcp->vcp_name, VMM_MAX_NAME_LEN);

	if (vm_impl_init(vm, p)) {
		printf("failed to init arch-specific features for vm 0x%p\n",
		    vm);
		vm_teardown(vm);
		return (ENOMEM);
	}

	rw_enter_write(&vmm_softc->vm_lock);
	vmm_softc->vm_ct++;
	vmm_softc->vm_idx++;

	/*
	 * XXX we use the vm_id for the VPID/ASID, so we need to prevent
	 * wrapping around 65536/4096 entries here
	 */
	vm->vm_id = vmm_softc->vm_idx;
	vm->vm_vcpu_ct = 0;
	vm->vm_vcpus_running = 0;

	/* Initialize each VCPU defined in 'vcp' */
	for (i = 0; i < vcp->vcp_ncpus; i++) {
		vcpu = pool_get(&vcpu_pool, PR_WAITOK | PR_ZERO);
		vcpu->vc_parent = vm;
		if ((ret = vcpu_init(vcpu)) != 0) {
			printf("failed to init vcpu %d for vm 0x%p\n", i, vm);
			vm_teardown(vm);
			vmm_softc->vm_ct--;
			vmm_softc->vm_idx--;
			rw_exit_write(&vmm_softc->vm_lock);
			return (ret);
		}
		rw_enter_write(&vm->vm_vcpu_lock);
		vcpu->vc_id = vm->vm_vcpu_ct;
		vm->vm_vcpu_ct++;
		SLIST_INSERT_HEAD(&vm->vm_vcpu_list, vcpu, vc_vcpu_link);
		rw_exit_write(&vm->vm_vcpu_lock);
	}

	/* XXX init various other hardware parts (vlapic, vioapic, etc) */

	SLIST_INSERT_HEAD(&vmm_softc->vm_list, vm, vm_link);
	rw_exit_write(&vmm_softc->vm_lock);

	vcp->vcp_id = vm->vm_id;

	return (0);
}

/*
 * vm_impl_init_vmx
 *
 * Intel VMX specific VM initialization routine
 *
 * Parameters:
 *  vm: the VM being initialized
 *   p: vmd process owning the VM
 *
 * Return values:
 *  0: the initialization was successful
 *  ENOMEM: the initialization failed (lack of resources)
 */
int
vm_impl_init_vmx(struct vm *vm, struct proc *p)
{
	int i, ret;
	vaddr_t mingpa, maxgpa;
	struct pmap *pmap;
	struct vm_mem_range *vmr;

	/* If not EPT, nothing to do here */
	if (vmm_softc->mode != VMM_MODE_EPT)
		return (0);

	/* Create a new pmap for this VM */
	pmap = pmap_create();
	if (!pmap) {
		printf("vm_impl_init_vmx: pmap_create failed\n");
		return (ENOMEM);
	}

	/*
	 * Create a new UVM map for this VM, and assign it the pmap just
	 * created.
	 */
	vmr = &vm->vm_memranges[0];
	mingpa = vmr->vmr_gpa;
	vmr = &vm->vm_memranges[vm->vm_nmemranges - 1];
	maxgpa = vmr->vmr_gpa + vmr->vmr_size;
	vm->vm_map = uvm_map_create(pmap, mingpa, maxgpa,
	    VM_MAP_ISVMSPACE | VM_MAP_PAGEABLE);

	if (!vm->vm_map) {
		printf("vm_impl_init_vmx: uvm_map_create failed\n");
		pmap_destroy(pmap);
		return (ENOMEM);
	}

	/* Map the new map with an anon */
	DPRINTF("vm_impl_init_vmx: created vm_map @@ %p\n", vm->vm_map);
	for (i = 0; i < vm->vm_nmemranges; i++) {
		vmr = &vm->vm_memranges[i];
		ret = uvm_share(vm->vm_map, vmr->vmr_gpa,
		    PROT_READ | PROT_WRITE | PROT_EXEC,
		    &p->p_vmspace->vm_map, vmr->vmr_va, vmr->vmr_size);
		if (ret) {
			printf("vm_impl_init_vmx: uvm_share failed (%d)\n",
			    ret);
			/* uvm_map_deallocate calls pmap_destroy for us */
			uvm_map_deallocate(vm->vm_map);
			vm->vm_map = NULL;
			return (ENOMEM);
		}
	}

	/* Convert the low 512GB of the pmap to EPT */
	ret = pmap_convert(pmap, PMAP_TYPE_EPT);
	if (ret) {
		printf("vm_impl_init_vmx: pmap_convert failed\n");
		/* uvm_map_deallocate calls pmap_destroy for us */
		uvm_map_deallocate(vm->vm_map);
		vm->vm_map = NULL;
		return (ENOMEM);
	}

	return (0);
}

/*
 * vm_impl_init_svm
 *
 * AMD SVM specific VM initialization routine
 *
 * Parameters:
 *  vm: the VM being initialized
 *   p: vmd process owning the VM
 *
 * Return values:
 *  0: the initialization was successful
 *  ENOMEM: the initialization failed (lack of resources)
 */
int
vm_impl_init_svm(struct vm *vm, struct proc *p)
{
	int i, ret;
	vaddr_t mingpa, maxgpa;
	struct pmap *pmap;
	struct vm_mem_range *vmr;

	/* If not RVI, nothing to do here */
	if (vmm_softc->mode != VMM_MODE_RVI)
		return (0);

	/* Create a new pmap for this VM */
	pmap = pmap_create();
	if (!pmap) {
		printf("vm_impl_init_svm: pmap_create failed\n");
		return (ENOMEM);
	}

	DPRINTF("%s: RVI pmap allocated @@ %p\n", __func__, pmap);

	/*
	 * Create a new UVM map for this VM, and assign it the pmap just
	 * created.
	 */
	vmr = &vm->vm_memranges[0];
	mingpa = vmr->vmr_gpa;
	vmr = &vm->vm_memranges[vm->vm_nmemranges - 1];
	maxgpa = vmr->vmr_gpa + vmr->vmr_size;
	vm->vm_map = uvm_map_create(pmap, mingpa, maxgpa,
	    VM_MAP_ISVMSPACE | VM_MAP_PAGEABLE);

	if (!vm->vm_map) {
		printf("vm_impl_init_svm: uvm_map_create failed\n");
		pmap_destroy(pmap);
		return (ENOMEM);
	}

	/* Map the new map with an anon */
	DPRINTF("vm_impl_init_svm: created vm_map @@ %p\n", vm->vm_map);
	for (i = 0; i < vm->vm_nmemranges; i++) {
		vmr = &vm->vm_memranges[i];
		ret = uvm_share(vm->vm_map, vmr->vmr_gpa,
		    PROT_READ | PROT_WRITE | PROT_EXEC,
		    &p->p_vmspace->vm_map, vmr->vmr_va, vmr->vmr_size);
		if (ret) {
			printf("vm_impl_init_svm: uvm_share failed (%d)\n",
			    ret);
			/* uvm_map_deallocate calls pmap_destroy for us */
			uvm_map_deallocate(vm->vm_map);
			vm->vm_map = NULL;
			return (ENOMEM);
		}
	}

	return (0);
}

/*
 * vm_impl_init
 *
 * Calls the architecture-specific VM init routine
 */
int
vm_impl_init(struct vm *vm, struct proc *p)
{
	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT)
		return vm_impl_init_vmx(vm, p);
	else if	(vmm_softc->mode == VMM_MODE_SVM ||
		 vmm_softc->mode == VMM_MODE_RVI)
		return vm_impl_init_svm(vm, p);
	else
		panic("unknown vmm mode\n");
}

/*
 * vm_impl_deinit_vmx
 *
 * Intel VMX specific VM initialization routine
 */
void
vm_impl_deinit_vmx(struct vm *vm)
{
	/* Unused */
}

/*
 * vm_impl_deinit_svm
 *
 * AMD SVM specific VM initialization routine
 */
void
vm_impl_deinit_svm(struct vm *vm)
{
	/* Unused */
}

/*
 * vm_impl_deinit
 *
 * Calls the architecture-specific VM init routine
 */
void
vm_impl_deinit(struct vm *vm)
{
	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT)
		vm_impl_deinit_vmx(vm);
	else if	(vmm_softc->mode == VMM_MODE_SVM ||
		 vmm_softc->mode == VMM_MODE_RVI)
		vm_impl_deinit_svm(vm);
	else
		panic("unknown vmm mode\n");
}

/*
 * vcpu_reload_vmcs_vmx
 *
 * Loads 'vmcs' on the current CPU, possibly flushing any old vmcs state
 * of the previous occupant.
 *
 * Parameters:
 *  vmcs: Pointer to uint64_t containing the PA of the vmcs to load
 *
 * Return values:
 *  0: if successful
 *  EINVAL: an error occurred during flush or reload
 */ 
int
vcpu_reload_vmcs_vmx(uint64_t *vmcs)
{
	uint64_t old;

	/* Flush any old state */
	if (!vmptrst(&old)) {
		if (old != 0xFFFFFFFFFFFFFFFFULL) {
			if (vmclear(&old))
				return (EINVAL);
		}
	} else
		return (EINVAL);

	/*
	 * Load the VMCS onto this PCPU
	 */
	if (vmptrld(vmcs))
		return (EINVAL);

	return (0);
}

/*
 * vcpu_readregs_vmx
 *
 * Reads 'vcpu's registers
 *
 * Parameters:
 *  vcpu: the vcpu to read register values from
 *  regmask: the types of registers to read
 *  vrs: output parameter where register values are stored
 *
 * Return values:
 *  0: if successful
 *  EINVAL: an error reading registers occured
 */
int
vcpu_readregs_vmx(struct vcpu *vcpu, uint64_t regmask,
    struct vcpu_reg_state *vrs)
{
	int i, ret = 0;
	uint64_t sel, limit, ar;
	uint64_t *gprs = vrs->vrs_gprs;
	uint64_t *crs = vrs->vrs_crs;
	struct vcpu_segment_info *sregs = vrs->vrs_sregs;

	if (vcpu_reload_vmcs_vmx(&vcpu->vc_control_pa))
		return (EINVAL);

	if (regmask & VM_RWREGS_GPRS) {
		gprs[VCPU_REGS_RAX] = vcpu->vc_gueststate.vg_rax;
		gprs[VCPU_REGS_RBX] = vcpu->vc_gueststate.vg_rbx;
		gprs[VCPU_REGS_RCX] = vcpu->vc_gueststate.vg_rcx;
		gprs[VCPU_REGS_RDX] = vcpu->vc_gueststate.vg_rdx;
		gprs[VCPU_REGS_RSI] = vcpu->vc_gueststate.vg_rsi;
		gprs[VCPU_REGS_RDI] = vcpu->vc_gueststate.vg_rdi;
		gprs[VCPU_REGS_R8] = vcpu->vc_gueststate.vg_r8;
		gprs[VCPU_REGS_R9] = vcpu->vc_gueststate.vg_r9;
		gprs[VCPU_REGS_R10] = vcpu->vc_gueststate.vg_r10;
		gprs[VCPU_REGS_R11] = vcpu->vc_gueststate.vg_r11;
		gprs[VCPU_REGS_R12] = vcpu->vc_gueststate.vg_r12;
		gprs[VCPU_REGS_R13] = vcpu->vc_gueststate.vg_r13;
		gprs[VCPU_REGS_R14] = vcpu->vc_gueststate.vg_r14;
		gprs[VCPU_REGS_R15] = vcpu->vc_gueststate.vg_r15;
		gprs[VCPU_REGS_RBP] = vcpu->vc_gueststate.vg_rbp;
		gprs[VCPU_REGS_RIP] = vcpu->vc_gueststate.vg_rip;
		if (vmread(VMCS_GUEST_IA32_RSP, &gprs[VCPU_REGS_RSP]))
			goto errout;
                if (vmread(VMCS_GUEST_IA32_RFLAGS, &gprs[VCPU_REGS_RFLAGS]))
			goto errout;
        }
	if (regmask & VM_RWREGS_SREGS) {
		for (i = 0; i < nitems(vmm_vmx_sreg_vmcs_fields); i++) {
			if (vmread(vmm_vmx_sreg_vmcs_fields[i].selid, &sel))
				goto errout;
			if (vmread(vmm_vmx_sreg_vmcs_fields[i].limitid, &limit))
				goto errout;
			if (vmread(vmm_vmx_sreg_vmcs_fields[i].arid, &ar))
				goto errout;
			if (vmread(vmm_vmx_sreg_vmcs_fields[i].baseid,
			   &sregs[i].vsi_base))
				goto errout;

			sregs[i].vsi_sel = sel;
			sregs[i].vsi_limit = limit;
			sregs[i].vsi_ar = ar;
		}

		if (vmread(VMCS_GUEST_IA32_GDTR_LIMIT, &limit))
			goto errout;
		if (vmread(VMCS_GUEST_IA32_GDTR_BASE,
		    &vrs->vrs_gdtr.vsi_base))
			goto errout;
		vrs->vrs_gdtr.vsi_limit = limit;

		if (vmread(VMCS_GUEST_IA32_IDTR_LIMIT, &limit))
			goto errout;
		if (vmread(VMCS_GUEST_IA32_IDTR_BASE,
		    &vrs->vrs_idtr.vsi_base))
			goto errout;
		vrs->vrs_idtr.vsi_limit = limit;
	}
	if (regmask & VM_RWREGS_CRS) {
		crs[VCPU_REGS_CR2] = vcpu->vc_gueststate.vg_cr2;
		if (vmread(VMCS_GUEST_IA32_CR0, &crs[VCPU_REGS_CR0]))
			goto errout;
		if (vmread(VMCS_GUEST_IA32_CR3, &crs[VCPU_REGS_CR3]))
			goto errout;
		if (vmread(VMCS_GUEST_IA32_CR4, &crs[VCPU_REGS_CR4]))
			goto errout;
	}

	goto out;

errout:
	ret = EINVAL;
out:
	if (vmclear(&vcpu->vc_control_pa))
		ret = EINVAL;
	return (ret);
}

/*
 * vcpu_readregs_svm
 *
 * XXX - unimplemented
 */
int
vcpu_readregs_svm(struct vcpu *vcpu, uint64_t regmask,
    struct vcpu_reg_state *regs)
{
	return (0);
}

/*
 * vcpu_writeregs_vmx
 *
 * Writes 'vcpu's registers
 *
 * Parameters:
 *  vcpu: the vcpu that has to get its registers written to
 *  regmask: the types of registers to write
 *  loadvmcs: bit to indicate whether the VMCS has to be loaded first
 *  vrs: the register values to write
 *
 * Return values:
 *  0: if successful
 *  EINVAL an error writing registers occured
 */
int
vcpu_writeregs_vmx(struct vcpu *vcpu, uint64_t regmask, int loadvmcs,
    struct vcpu_reg_state *vrs)
{
	int i, ret = 0;
	uint16_t sel;
	uint64_t limit, ar;
	uint64_t *gprs = vrs->vrs_gprs;
	uint64_t *crs = vrs->vrs_crs;
	struct vcpu_segment_info *sregs = vrs->vrs_sregs;

	if (loadvmcs) {
		if (vcpu_reload_vmcs_vmx(&vcpu->vc_control_pa))
			return (EINVAL);
	}

	if (regmask & VM_RWREGS_GPRS) {
		vcpu->vc_gueststate.vg_rax = gprs[VCPU_REGS_RAX];
		vcpu->vc_gueststate.vg_rbx = gprs[VCPU_REGS_RBX];
		vcpu->vc_gueststate.vg_rcx = gprs[VCPU_REGS_RCX];
		vcpu->vc_gueststate.vg_rdx = gprs[VCPU_REGS_RDX];
		vcpu->vc_gueststate.vg_rsi = gprs[VCPU_REGS_RSI];
		vcpu->vc_gueststate.vg_rdi = gprs[VCPU_REGS_RDI];
		vcpu->vc_gueststate.vg_r8 = gprs[VCPU_REGS_R8];
		vcpu->vc_gueststate.vg_r9 = gprs[VCPU_REGS_R9];
		vcpu->vc_gueststate.vg_r10 = gprs[VCPU_REGS_R10];
		vcpu->vc_gueststate.vg_r11 = gprs[VCPU_REGS_R11];
		vcpu->vc_gueststate.vg_r12 = gprs[VCPU_REGS_R12];
		vcpu->vc_gueststate.vg_r13 = gprs[VCPU_REGS_R13];
		vcpu->vc_gueststate.vg_r14 = gprs[VCPU_REGS_R14];
		vcpu->vc_gueststate.vg_r15 = gprs[VCPU_REGS_R15];
		vcpu->vc_gueststate.vg_rbp = gprs[VCPU_REGS_RBP];
		vcpu->vc_gueststate.vg_rip = gprs[VCPU_REGS_RIP];
		if (vmwrite(VMCS_GUEST_IA32_RIP, gprs[VCPU_REGS_RIP]))
			goto errout;
		if (vmwrite(VMCS_GUEST_IA32_RSP, gprs[VCPU_REGS_RSP]))
			goto errout;
                if (vmwrite(VMCS_GUEST_IA32_RFLAGS, gprs[VCPU_REGS_RFLAGS]))
			goto errout;
	}
	if (regmask & VM_RWREGS_SREGS) {
		for (i = 0; i < nitems(vmm_vmx_sreg_vmcs_fields); i++) {
			sel = sregs[i].vsi_sel;
			limit = sregs[i].vsi_limit;
			ar = sregs[i].vsi_ar;

			if (vmwrite(vmm_vmx_sreg_vmcs_fields[i].selid, sel))
				goto errout;
			if (vmwrite(vmm_vmx_sreg_vmcs_fields[i].limitid, limit))
				goto errout;
			if (vmwrite(vmm_vmx_sreg_vmcs_fields[i].arid, ar))
				goto errout;
			if (vmwrite(vmm_vmx_sreg_vmcs_fields[i].baseid,
			   sregs[i].vsi_base))
				goto errout;
		}

		if (vmwrite(VMCS_GUEST_IA32_GDTR_LIMIT,
		    vrs->vrs_gdtr.vsi_limit))
			goto errout;
		if (vmwrite(VMCS_GUEST_IA32_GDTR_BASE,
		    vrs->vrs_gdtr.vsi_base))
			goto errout;
		if (vmwrite(VMCS_GUEST_IA32_IDTR_LIMIT,
		    vrs->vrs_idtr.vsi_limit))
			goto errout;
		if (vmwrite(VMCS_GUEST_IA32_IDTR_BASE,
		    vrs->vrs_idtr.vsi_base))
			goto errout;
	}
	if (regmask & VM_RWREGS_CRS) {
		if (vmwrite(VMCS_GUEST_IA32_CR0, crs[VCPU_REGS_CR0]))
			goto errout;
		if (vmwrite(VMCS_GUEST_IA32_CR3, crs[VCPU_REGS_CR3]))
			goto errout;
		if (vmwrite(VMCS_GUEST_IA32_CR4, crs[VCPU_REGS_CR4]))
			goto errout;
	}

	goto out;

errout:
	ret = EINVAL;
out:
	if (loadvmcs) {
		if (vmclear(&vcpu->vc_control_pa))
			ret = EINVAL;
	}
	return (ret);
}

/*
 * vcpu_writeregs_svm
 *
 * Writes 'vcpu's registers
 *
 * Parameters:
 *  vcpu: the vcpu that has to get its registers written to
 *  regmask: the types of registers to write
 *  vrs: the register values to write
 *
 * Return values:
 *  0: if successful
 *  EINVAL an error writing registers occured
 */
int
vcpu_writeregs_svm(struct vcpu *vcpu, uint64_t regmask,
    struct vcpu_reg_state *vrs)
{
	uint64_t *gprs = vrs->vrs_gprs;
	uint64_t *crs = vrs->vrs_crs;
	uint16_t attr;
	struct vcpu_segment_info *sregs = vrs->vrs_sregs;
	struct vmcb *vmcb = (struct vmcb *)vcpu->vc_control_va;

	if (regmask & VM_RWREGS_GPRS) {
		vcpu->vc_gueststate.vg_rax = gprs[VCPU_REGS_RAX];
		vcpu->vc_gueststate.vg_rbx = gprs[VCPU_REGS_RBX];
		vcpu->vc_gueststate.vg_rcx = gprs[VCPU_REGS_RCX];
		vcpu->vc_gueststate.vg_rdx = gprs[VCPU_REGS_RDX];
		vcpu->vc_gueststate.vg_rsi = gprs[VCPU_REGS_RSI];
		vcpu->vc_gueststate.vg_rdi = gprs[VCPU_REGS_RDI];
		vcpu->vc_gueststate.vg_r8 = gprs[VCPU_REGS_R8];
		vcpu->vc_gueststate.vg_r9 = gprs[VCPU_REGS_R9];
		vcpu->vc_gueststate.vg_r10 = gprs[VCPU_REGS_R10];
		vcpu->vc_gueststate.vg_r11 = gprs[VCPU_REGS_R11];
		vcpu->vc_gueststate.vg_r12 = gprs[VCPU_REGS_R12];
		vcpu->vc_gueststate.vg_r13 = gprs[VCPU_REGS_R13];
		vcpu->vc_gueststate.vg_r14 = gprs[VCPU_REGS_R14];
		vcpu->vc_gueststate.vg_r15 = gprs[VCPU_REGS_R15];
		vcpu->vc_gueststate.vg_rbp = gprs[VCPU_REGS_RBP];
		vcpu->vc_gueststate.vg_rip = gprs[VCPU_REGS_RIP];

		vmcb->v_rip = gprs[VCPU_REGS_RIP];
		vmcb->v_rsp = gprs[VCPU_REGS_RSP];
		vmcb->v_rflags = gprs[VCPU_REGS_RFLAGS];
	}

	if (regmask & VM_RWREGS_SREGS) {
		vmcb->v_cs.vs_sel = sregs[VCPU_REGS_CS].vsi_sel;
		vmcb->v_cs.vs_lim = sregs[VCPU_REGS_CS].vsi_limit;
		attr = sregs[VCPU_REGS_CS].vsi_ar;
		vmcb->v_cs.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_cs.vs_base = sregs[VCPU_REGS_CS].vsi_base;
		vmcb->v_ds.vs_sel = sregs[VCPU_REGS_DS].vsi_sel;
		vmcb->v_ds.vs_lim = sregs[VCPU_REGS_DS].vsi_limit;
		attr = sregs[VCPU_REGS_DS].vsi_ar;
		vmcb->v_ds.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_ds.vs_base = sregs[VCPU_REGS_DS].vsi_base;
		vmcb->v_es.vs_sel = sregs[VCPU_REGS_ES].vsi_sel;
		vmcb->v_es.vs_lim = sregs[VCPU_REGS_ES].vsi_limit;
		attr = sregs[VCPU_REGS_ES].vsi_ar;
		vmcb->v_es.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_es.vs_base = sregs[VCPU_REGS_ES].vsi_base;
		vmcb->v_fs.vs_sel = sregs[VCPU_REGS_FS].vsi_sel;
		vmcb->v_fs.vs_lim = sregs[VCPU_REGS_FS].vsi_limit;
		attr = sregs[VCPU_REGS_FS].vsi_ar;
		vmcb->v_fs.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_fs.vs_base = sregs[VCPU_REGS_FS].vsi_base;
		vmcb->v_gs.vs_sel = sregs[VCPU_REGS_GS].vsi_sel;
		vmcb->v_gs.vs_lim = sregs[VCPU_REGS_GS].vsi_limit;
		attr = sregs[VCPU_REGS_GS].vsi_ar;
		vmcb->v_gs.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_gs.vs_base = sregs[VCPU_REGS_GS].vsi_base;
		vmcb->v_ss.vs_sel = sregs[VCPU_REGS_SS].vsi_sel;
		vmcb->v_ss.vs_lim = sregs[VCPU_REGS_SS].vsi_limit;
		attr = sregs[VCPU_REGS_SS].vsi_ar;
		vmcb->v_ss.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_ss.vs_base = sregs[VCPU_REGS_SS].vsi_base;
		vmcb->v_ldtr.vs_sel = sregs[VCPU_REGS_LDTR].vsi_sel;
		vmcb->v_ldtr.vs_lim = sregs[VCPU_REGS_LDTR].vsi_limit;
		attr = sregs[VCPU_REGS_LDTR].vsi_ar;
		vmcb->v_ldtr.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_ldtr.vs_base = sregs[VCPU_REGS_LDTR].vsi_base;
		vmcb->v_tr.vs_sel = sregs[VCPU_REGS_TR].vsi_sel;
		vmcb->v_tr.vs_lim = sregs[VCPU_REGS_TR].vsi_limit;
		attr = sregs[VCPU_REGS_TR].vsi_ar;
		vmcb->v_tr.vs_attr = (attr & 0xff) | ((attr >> 4) & 0xf00);
		vmcb->v_tr.vs_base = sregs[VCPU_REGS_TR].vsi_base;
		vmcb->v_gdtr.vs_lim = vrs->vrs_gdtr.vsi_limit;
		vmcb->v_gdtr.vs_base = vrs->vrs_gdtr.vsi_base;
		vmcb->v_idtr.vs_lim = vrs->vrs_idtr.vsi_limit;
		vmcb->v_idtr.vs_base = vrs->vrs_idtr.vsi_base;
	}

	if (regmask & VM_RWREGS_CRS) {
		vmcb->v_cr0 = crs[VCPU_REGS_CR0];
		vmcb->v_cr3 = crs[VCPU_REGS_CR3];
		vmcb->v_cr4 = crs[VCPU_REGS_CR4];
	}

	return (0);
}

/*
 * vcpu_reset_regs_svm
 *
 * Initializes 'vcpu's registers to supplied state
 *
 * Parameters:
 *  vcpu: the vcpu whose register state is to be initialized
 *  vrs: the register state to set
 * 
 * Return values:
 *  0: registers init'ed successfully
 *  EINVAL: an error occurred setting register state
 */
int
vcpu_reset_regs_svm(struct vcpu *vcpu, struct vcpu_reg_state *vrs)
{
	struct vmcb *vmcb;
	int ret;

	vmcb = (struct vmcb *)vcpu->vc_control_va;

	/*
	 * Intercept controls
	 *
	 * External Interrupt exiting (SVM_INTERCEPT_INTR)
	 * External NMI exiting (SVM_INTERCEPT_NMI)
	 * CPUID instruction (SVM_INTERCEPT_CPUID)
	 * HLT instruction (SVM_INTERCEPT_HLT)
	 * I/O instructions (SVM_INTERCEPT_INOUT)
	 * MSR access (SVM_INTERCEPT_MSR)
	 *
	 * VMRUN instruction (SVM_INTERCEPT_VMRUN)
	 * VMMCALL instruction (SVM_INTERCEPT_VMMCALL)
	 * VMLOAD instruction (SVM_INTERCEPT_VMLOAD)
	 * VMSAVE instruction (SVM_INTERCEPT_VMSAVE)
	 * STGI instruction (SVM_INTERCEPT_STGI)
	 * CLGI instruction (SVM_INTERCEPT_CLGI)
	 * SKINIT instruction (SVM_INTERCEPT_SKINIT)
	 * ICEBP instruction (SVM_INTERCEPT_ICEBP)
	 * MWAIT instruction (SVM_INTERCEPT_MWAIT_UNCOND)
	 */
	vmcb->v_intercept1 = SVM_INTERCEPT_INTR | SVM_INTERCEPT_NMI |
	    SVM_INTERCEPT_CPUID | SVM_INTERCEPT_HLT | SVM_INTERCEPT_INOUT |
	    SVM_INTERCEPT_MSR;

	vmcb->v_intercept2 = SVM_INTERCEPT_VMRUN | SVM_INTERCEPT_VMMCALL |
	    SVM_INTERCEPT_VMLOAD | SVM_INTERCEPT_VMSAVE | SVM_INTERCEPT_STGI |
	    SVM_INTERCEPT_CLGI | SVM_INTERCEPT_SKINIT | SVM_INTERCEPT_ICEBP |
	    SVM_INTERCEPT_MWAIT_UNCOND;

	/* Setup I/O bitmap */
	memset((uint8_t *)vcpu->vc_svm_ioio_va, 0xFF, 3 * PAGE_SIZE);
	vmcb->v_iopm_pa = (uint64_t)(vcpu->vc_svm_ioio_pa);

	/* Setup MSR bitmap */
	memset((uint8_t *)vcpu->vc_msr_bitmap_va, 0xFF, 2 * PAGE_SIZE);
	vmcb->v_msrpm_pa = (uint64_t)(vcpu->vc_msr_bitmap_pa);
	svm_setmsrbrw(vcpu, MSR_IA32_FEATURE_CONTROL);
	svm_setmsrbrw(vcpu, MSR_SYSENTER_CS);
	svm_setmsrbrw(vcpu, MSR_SYSENTER_ESP);
	svm_setmsrbrw(vcpu, MSR_SYSENTER_EIP);
	svm_setmsrbrw(vcpu, MSR_STAR);
	svm_setmsrbrw(vcpu, MSR_LSTAR);
	svm_setmsrbrw(vcpu, MSR_CSTAR);
	svm_setmsrbrw(vcpu, MSR_SFMASK);
	svm_setmsrbrw(vcpu, MSR_FSBASE);
	svm_setmsrbrw(vcpu, MSR_GSBASE);
	svm_setmsrbrw(vcpu, MSR_KERNELGSBASE);

	/* EFER is R/O so we can ensure the guest always has SVME */
	svm_setmsrbr(vcpu, MSR_EFER);

	/* Guest VCPU ASID */
	vmcb->v_asid = vcpu->vc_parent->vm_id;

	/* TLB Control */
	vmcb->v_tlb_control = 2;	/* Flush this guest's TLB entries */

	/* NPT */
	if (vmm_softc->mode == VMM_MODE_RVI) {
		vmcb->v_np_enable = 1;
		vmcb->v_n_cr3 = vcpu->vc_parent->vm_map->pmap->pm_pdirpa;
	}

	/* Enable SVME in EFER (must always be set) */
	vmcb->v_efer |= EFER_SVME;

	ret = vcpu_writeregs_svm(vcpu, VM_RWREGS_ALL, vrs);

	vmcb->v_efer |= (EFER_LME | EFER_LMA);
	vmcb->v_cr4 |= CR4_PAE;

	return ret;
}

/*
 * svm_setmsrbr
 *
 * Allow read access to the specified msr on the supplied vcpu.
 *
 * Parameters:
 *  vcpu: the VCPU to allow access
 *  msr: the MSR number to allow access to
 */
void
svm_setmsrbr(struct vcpu *vcpu, uint32_t msr)
{
	uint8_t *msrs;
	uint16_t idx;

	msrs = (uint8_t *)vcpu->vc_msr_bitmap_va;

	/*
	 * MSR Read bitmap layout:
	 * Pentium MSRs (0x0 - 0x1fff) @@ 0x0
	 * Gen6 and Syscall MSRs (0xc0000000 - 0xc0001fff) @@ 0x800
	 * Gen7 and Gen8 MSRs (0xc0010000 - 0xc0011fff) @@ 0x1000
	 *
	 * Read enable bit is low order bit of 2-bit pair
	 * per MSR (eg, MSR 0x0 write bit is at bit 0 @@ 0x0)
	 */
	if (msr <= 0x1fff) {
		idx = SVM_MSRIDX(msr);
		msrs[idx] &= ~(SVM_MSRBIT_R(msr));
	} else if (msr >= 0xc0000000 && msr <= 0xc0001fff) {
		idx = SVM_MSRIDX(msr - 0xc0000000) + 0x800;
		msrs[idx] &= ~(SVM_MSRBIT_R(msr - 0xc0000000));
	} else if (msr >= 0xc0010000 && msr <= 0xc0011fff) {
		idx = SVM_MSRIDX(msr - 0xc0010000) + 0x1000;
		msrs[idx] &= ~(SVM_MSRBIT_R(msr - 0xc0010000));
	} else {
		printf("%s: invalid msr 0x%x\n", __func__, msr);
		return;
	}
}

/*
 * svm_setmsrbw
 *
 * Allow write access to the specified msr on the supplied vcpu
 *
 * Parameters:
 *  vcpu: the VCPU to allow access
 *  msr: the MSR number to allow access to
 */
void
svm_setmsrbw(struct vcpu *vcpu, uint32_t msr)
{
	uint8_t *msrs;
	uint16_t idx;

	msrs = (uint8_t *)vcpu->vc_msr_bitmap_va;

	/*
	 * MSR Write bitmap layout:
	 * Pentium MSRs (0x0 - 0x1fff) @@ 0x0
	 * Gen6 and Syscall MSRs (0xc0000000 - 0xc0001fff) @@ 0x800
	 * Gen7 and Gen8 MSRs (0xc0010000 - 0xc0011fff) @@ 0x1000
	 *
	 * Write enable bit is high order bit of 2-bit pair
	 * per MSR (eg, MSR 0x0 write bit is at bit 1 @@ 0x0)
	 */
	if (msr <= 0x1fff) {
		idx = SVM_MSRIDX(msr);
		msrs[idx] &= ~(SVM_MSRBIT_W(msr));
	} else if (msr >= 0xc0000000 && msr <= 0xc0001fff) {
		idx = SVM_MSRIDX(msr - 0xc0000000) + 0x800;
		msrs[idx] &= ~(SVM_MSRBIT_W(msr - 0xc0000000));
	} else if (msr >= 0xc0010000 && msr <= 0xc0011fff) {
		idx = SVM_MSRIDX(msr - 0xc0010000) + 0x1000;
		msrs[idx] &= ~(SVM_MSRBIT_W(msr - 0xc0010000));
	} else {
		printf("%s: invalid msr 0x%x\n", __func__, msr);
		return;
	}
}

/*
 * svm_setmsrbrw
 *
 * Allow read/write access to the specified msr on the supplied vcpu
 *
 * Parameters:
 *  vcpu: the VCPU to allow access
 *  msr: the MSR number to allow access to
 */
void
svm_setmsrbrw(struct vcpu *vcpu, uint32_t msr)
{
	svm_setmsrbr(vcpu, msr);
	svm_setmsrbw(vcpu, msr);
}

/*
 * vmx_setmsrbr
 *
 * Allow read access to the specified msr on the supplied vcpu.
 *
 * Parameters:
 *  vcpu: the VCPU to allow access
 *  msr: the MSR number to allow access to
 */
void
vmx_setmsrbr(struct vcpu *vcpu, uint32_t msr)
{
	uint8_t *msrs;
	uint16_t idx;

	msrs = (uint8_t *)vcpu->vc_msr_bitmap_va;

	/*
	 * MSR Read bitmap layout:
	 * "Low" MSRs (0x0 - 0x1fff) @@ 0x0
	 * "High" MSRs (0xc0000000 - 0xc0001fff) @@ 0x400
	 */
	if (msr <= 0x1fff) {
		idx = VMX_MSRIDX(msr);
		msrs[idx] &= ~(VMX_MSRBIT(msr));
	} else if (msr >= 0xc0000000 && msr <= 0xc0001fff) {
		idx = VMX_MSRIDX(msr - 0xc0000000) + 0x400;
		msrs[idx] &= ~(VMX_MSRBIT(msr - 0xc0000000));
	} else
		printf("%s: invalid msr 0x%x\n", __func__, msr);
}

/*
 * vmx_setmsrbw
 *
 * Allow write access to the specified msr on the supplied vcpu
 *
 * Parameters:
 *  vcpu: the VCPU to allow access
 *  msr: the MSR number to allow access to
 */
void
vmx_setmsrbw(struct vcpu *vcpu, uint32_t msr)
{
	uint8_t *msrs;
	uint16_t idx;

	msrs = (uint8_t *)vcpu->vc_msr_bitmap_va;

	/*
	 * MSR Write bitmap layout:
	 * "Low" MSRs (0x0 - 0x1fff) @@ 0x800
	 * "High" MSRs (0xc0000000 - 0xc0001fff) @@ 0xc00
	 */
	if (msr <= 0x1fff) {
		idx = VMX_MSRIDX(msr) + 0x800;
		msrs[idx] &= ~(VMX_MSRBIT(msr));
	} else if (msr >= 0xc0000000 && msr <= 0xc0001fff) {
		idx = VMX_MSRIDX(msr - 0xc0000000) + 0xc00;
		msrs[idx] &= ~(VMX_MSRBIT(msr - 0xc0000000));
	} else
		printf("%s: invalid msr 0x%x\n", __func__, msr);
}

/*
 * vmx_setmsrbrw
 *
 * Allow read/write access to the specified msr on the supplied vcpu
 *
 * Parameters:
 *  vcpu: the VCPU to allow access
 *  msr: the MSR number to allow access to
 */
void
vmx_setmsrbrw(struct vcpu *vcpu, uint32_t msr)
{
	vmx_setmsrbr(vcpu, msr);
	vmx_setmsrbw(vcpu, msr);
}

/*
 * vcpu_reset_regs_vmx
 *
 * Initializes 'vcpu's registers to supplied state
 *
 * Parameters:
 *  vcpu: the vcpu whose register state is to be initialized
 *  vrs: the register state to set
 * 
 * Return values:
 *  0: registers init'ed successfully
 *  EINVAL: an error occurred setting register state
 */
int
vcpu_reset_regs_vmx(struct vcpu *vcpu, struct vcpu_reg_state *vrs)
{
	int ret, ug;
	uint32_t cr0, cr4;
	uint32_t pinbased, procbased, procbased2, exit, entry;
	uint32_t want1, want0;
	uint64_t msr, ctrlval, eptp, cr3;
	uint16_t ctrl;
	struct vmx_msr_store *msr_store;

	ret = 0;
	ug = 0;

	if (vcpu_reload_vmcs_vmx(&vcpu->vc_control_pa))
		return (EINVAL);

	/* Compute Basic Entry / Exit Controls */
	vcpu->vc_vmx_basic = rdmsr(IA32_VMX_BASIC);
	vcpu->vc_vmx_entry_ctls = rdmsr(IA32_VMX_ENTRY_CTLS);
	vcpu->vc_vmx_exit_ctls = rdmsr(IA32_VMX_EXIT_CTLS);
	vcpu->vc_vmx_pinbased_ctls = rdmsr(IA32_VMX_PINBASED_CTLS);
	vcpu->vc_vmx_procbased_ctls = rdmsr(IA32_VMX_PROCBASED_CTLS);

	/* Compute True Entry / Exit Controls (if applicable) */
	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		vcpu->vc_vmx_true_entry_ctls = rdmsr(IA32_VMX_TRUE_ENTRY_CTLS);
		vcpu->vc_vmx_true_exit_ctls = rdmsr(IA32_VMX_TRUE_EXIT_CTLS);
		vcpu->vc_vmx_true_pinbased_ctls =
		    rdmsr(IA32_VMX_TRUE_PINBASED_CTLS);
		vcpu->vc_vmx_true_procbased_ctls =
		    rdmsr(IA32_VMX_TRUE_PROCBASED_CTLS);
	}

	/* Compute Secondary Procbased Controls (if applicable) */
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1))
		vcpu->vc_vmx_procbased2_ctls = rdmsr(IA32_VMX_PROCBASED2_CTLS);

	/*
	 * Pinbased ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_EXTERNAL_INT_EXITING - exit on host interrupt
	 * IA32_VMX_NMI_EXITING - exit on host NMI
	 */
	want1 = IA32_VMX_EXTERNAL_INT_EXITING |
	    IA32_VMX_NMI_EXITING;
	want0 = 0;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_PINBASED_CTLS;
		ctrlval = vcpu->vc_vmx_true_pinbased_ctls;
	} else {
		ctrl = IA32_VMX_PINBASED_CTLS;
		ctrlval = vcpu->vc_vmx_pinbased_ctls;
	}

	if (vcpu_vmx_compute_ctrl(ctrlval, ctrl, want1, want0, &pinbased)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_PINBASED_CTLS, pinbased)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Procbased ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_HLT_EXITING - exit on HLT instruction
	 * IA32_VMX_MWAIT_EXITING - exit on MWAIT instruction
	 * IA32_VMX_UNCONDITIONAL_IO_EXITING - exit on I/O instructions
	 * IA32_VMX_USE_MSR_BITMAPS - exit on various MSR accesses
	 * IA32_VMX_CR8_LOAD_EXITING - guest TPR access
	 * IA32_VMX_CR8_STORE_EXITING - guest TPR access
	 * IA32_VMX_USE_TPR_SHADOW - guest TPR access (shadow)
	 *
	 * If we have EPT, we must be able to clear the following
	 * IA32_VMX_CR3_LOAD_EXITING - don't care about guest CR3 accesses
	 * IA32_VMX_CR3_STORE_EXITING - don't care about guest CR3 accesses
	 */
	want1 = IA32_VMX_HLT_EXITING |
	    IA32_VMX_MWAIT_EXITING |
	    IA32_VMX_UNCONDITIONAL_IO_EXITING |
	    IA32_VMX_USE_MSR_BITMAPS |
	    IA32_VMX_CR8_LOAD_EXITING |
	    IA32_VMX_CR8_STORE_EXITING |
	    IA32_VMX_USE_TPR_SHADOW;
	want0 = 0;

	if (vmm_softc->mode == VMM_MODE_EPT) {
		want1 |= IA32_VMX_ACTIVATE_SECONDARY_CONTROLS;
		want0 |= IA32_VMX_CR3_LOAD_EXITING |
		    IA32_VMX_CR3_STORE_EXITING;
	}

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_PROCBASED_CTLS;
		ctrlval = vcpu->vc_vmx_true_procbased_ctls;
	} else {
		ctrl = IA32_VMX_PROCBASED_CTLS;
		ctrlval = vcpu->vc_vmx_procbased_ctls;
	}

	if (vcpu_vmx_compute_ctrl(ctrlval, ctrl, want1, want0, &procbased)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_PROCBASED_CTLS, procbased)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Secondary Procbased ctrls
	 *
	 * We want to be able to set the following, if available:
	 * IA32_VMX_ENABLE_VPID - use VPIDs where available
	 *
	 * If we have EPT, we must be able to set the following:
	 * IA32_VMX_ENABLE_EPT - enable EPT
	 *
	 * If we have unrestricted guest capability, we must be able to set
	 * the following:
	 * IA32_VMX_UNRESTRICTED_GUEST - enable unrestricted guest
	 */
	want1 = 0;

	/* XXX checking for 2ndary controls can be combined here */
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VPID, 1))
			want1 |= IA32_VMX_ENABLE_VPID;
	}

	if (vmm_softc->mode == VMM_MODE_EPT)
		want1 |= IA32_VMX_ENABLE_EPT;

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_UNRESTRICTED_GUEST, 1)) {
			want1 |= IA32_VMX_UNRESTRICTED_GUEST;
			ug = 1;
		}
	}

	want0 = ~want1;
	ctrlval = vcpu->vc_vmx_procbased2_ctls;
	ctrl = IA32_VMX_PROCBASED2_CTLS;

	if (vcpu_vmx_compute_ctrl(ctrlval, ctrl, want1, want0, &procbased2)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_PROCBASED2_CTLS, procbased2)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Exit ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_HOST_SPACE_ADDRESS_SIZE - exit to long mode
	 * IA32_VMX_ACKNOWLEDGE_INTERRUPT_ON_EXIT - ack interrupt on exit
	 * XXX clear save_debug_ctrls on exit ?
	 */
	want1 = IA32_VMX_HOST_SPACE_ADDRESS_SIZE |
	    IA32_VMX_ACKNOWLEDGE_INTERRUPT_ON_EXIT;
	want0 = 0;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_EXIT_CTLS;
		ctrlval = vcpu->vc_vmx_true_exit_ctls;
	} else {
		ctrl = IA32_VMX_EXIT_CTLS;
		ctrlval = vcpu->vc_vmx_exit_ctls;
	}

	if (vcpu_vmx_compute_ctrl(ctrlval, ctrl, want1, want0, &exit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_EXIT_CTLS, exit)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Entry ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_IA32E_MODE_GUEST (if no unrestricted guest)
	 * We must be able to clear the following:
	 * IA32_VMX_ENTRY_TO_SMM - enter to SMM
	 * IA32_VMX_DEACTIVATE_DUAL_MONITOR_TREATMENT
	 * IA32_VMX_LOAD_DEBUG_CONTROLS
	 * IA32_VMX_LOAD_IA32_PERF_GLOBAL_CTRL_ON_ENTRY
	 */
	if (ug == 1)
		want1 = 0;
	else
		want1 = IA32_VMX_IA32E_MODE_GUEST;

	want0 = IA32_VMX_ENTRY_TO_SMM |
	    IA32_VMX_DEACTIVATE_DUAL_MONITOR_TREATMENT |
	    IA32_VMX_LOAD_DEBUG_CONTROLS |
	    IA32_VMX_LOAD_IA32_PERF_GLOBAL_CTRL_ON_ENTRY;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_ENTRY_CTLS;
		ctrlval = vcpu->vc_vmx_true_entry_ctls;
	} else {
		ctrl = IA32_VMX_ENTRY_CTLS;
		ctrlval = vcpu->vc_vmx_entry_ctls;
	}

	if (vcpu_vmx_compute_ctrl(ctrlval, ctrl, want1, want0, &entry)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_ENTRY_CTLS, entry)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmm_softc->mode == VMM_MODE_EPT) {
		eptp = vcpu->vc_parent->vm_map->pmap->pm_pdirpa;
		msr = rdmsr(IA32_VMX_EPT_VPID_CAP);
		if (msr & IA32_EPT_VPID_CAP_PAGE_WALK_4) {
			/* Page walk length 4 supported */
			eptp |= ((IA32_EPT_PAGE_WALK_LENGTH - 1) << 3);
		} else {
			DPRINTF("EPT page walk length 4 not supported");
			ret = EINVAL;
			goto exit;
		}

		if (msr & IA32_EPT_VPID_CAP_WB) {
			/* WB cache type supported */
			eptp |= IA32_EPT_PAGING_CACHE_TYPE_WB;
		}

		DPRINTF("guest eptp = 0x%llx\n", eptp);
		if (vmwrite(VMCS_GUEST_IA32_EPTP, eptp)) {
			ret = EINVAL;
			goto exit;
		}
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VPID, 1))
			if (vmwrite(VMCS_GUEST_VPID,
			    (uint16_t)vcpu->vc_parent->vm_id)) {
				ret = EINVAL;
				goto exit;
			}
	}

	/*
	 * Determine which bits in CR0 have to be set to a fixed
	 * value as per Intel SDM A.7.
	 * CR0 bits in the vrs parameter must match these.
	 */

	want1 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed0) &
	    (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed1);
	want0 = ~(curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed0) &
	    ~(curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed1);

	/*
	 * CR0_FIXED0 and CR0_FIXED1 may report the CR0_PG and CR0_PE bits as
	 * fixed to 1 even if the CPU supports the unrestricted guest
	 * feature. Update want1 and want0 accordingly to allow
	 * any value for CR0_PG and CR0_PE in vrs->vrs_crs[VCPU_REGS_CR0] if
	 * the CPU has the unrestricted guest capability.
	 */
	cr0 = vrs->vrs_crs[VCPU_REGS_CR0];

	if (ug) {
		want1 &= ~(CR0_PG | CR0_PE);
		want0 &= ~(CR0_PG | CR0_PE);
		cr0 &= ~(CR0_PG | CR0_PE);
	}

	/*
	 * VMX may require some bits to be set that userland should not have
	 * to care about. Set those here.
	 */
	if (want1 & CR0_NE)
		cr0 |= CR0_NE;

	if ((cr0 & want1) != want1) {
		ret = EINVAL;
		goto exit;
	}
	if ((~cr0 & want0) != want0) {
		ret = EINVAL;
		goto exit;
	}

	if (ug)
		cr3 = 0;
	else
		cr3 = vrs->vrs_crs[VCPU_REGS_CR3];

	/*
	 * Determine default CR4 as per Intel SDM A.8
	 * All flexible bits are set to 0
	 */
	cr4 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed0) &
	    (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed1);

	/*
	 * If we are starting in restricted guest mode, enable PAE
	 */
	if (ug == 0)
		cr4 |= CR4_PAE;

	vrs->vrs_crs[VCPU_REGS_CR0] = cr0;
	vrs->vrs_crs[VCPU_REGS_CR3] = cr3;
	vrs->vrs_crs[VCPU_REGS_CR4] = cr4;

	/*
	 * Select host MSRs to be loaded on exit
	 */
	msr_store = (struct vmx_msr_store *)vcpu->vc_vmx_msr_exit_load_va;
	msr_store[0].vms_index = MSR_EFER;
	msr_store[0].vms_data = rdmsr(MSR_EFER);
	msr_store[1].vms_index = MSR_STAR;
	msr_store[1].vms_data = rdmsr(MSR_STAR);
	msr_store[2].vms_index = MSR_LSTAR;
	msr_store[2].vms_data = rdmsr(MSR_LSTAR);
	msr_store[3].vms_index = MSR_CSTAR;
	msr_store[3].vms_data = rdmsr(MSR_CSTAR);
	msr_store[4].vms_index = MSR_SFMASK;
	msr_store[4].vms_data = rdmsr(MSR_SFMASK);
	msr_store[5].vms_index = MSR_KERNELGSBASE;
	msr_store[5].vms_data = rdmsr(MSR_KERNELGSBASE);

	/*
	 * Select guest MSRs to be loaded on entry / saved on exit
	 */
	msr_store = (struct vmx_msr_store *)vcpu->vc_vmx_msr_exit_save_va;

	/*
	 * Make sure LME is enabled in EFER if restricted guest mode is
	 * needed.
	 */
	msr_store[0].vms_index = MSR_EFER;
	if (ug == 1)
		msr_store[0].vms_data = 0ULL;	/* Initial value */
	else
		msr_store[0].vms_data = EFER_LME;

	msr_store[1].vms_index = MSR_STAR;
	msr_store[1].vms_data = 0ULL;		/* Initial value */
	msr_store[2].vms_index = MSR_LSTAR;
	msr_store[2].vms_data = 0ULL;		/* Initial value */
	msr_store[3].vms_index = MSR_CSTAR;
	msr_store[3].vms_data = 0ULL;		/* Initial value */
	msr_store[4].vms_index = MSR_SFMASK;
	msr_store[4].vms_data = 0ULL;		/* Initial value */
	msr_store[5].vms_index = MSR_KERNELGSBASE;
	msr_store[5].vms_data = 0ULL;		/* Initial value */

	/*
	 * Currently we have the same count of entry/exit MSRs loads/stores
	 * but this is not an architectural requirement.
	 */
	if (vmwrite(VMCS_EXIT_MSR_STORE_COUNT, VMX_NUM_MSR_STORE)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_EXIT_MSR_LOAD_COUNT, VMX_NUM_MSR_STORE)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_ENTRY_MSR_LOAD_COUNT, VMX_NUM_MSR_STORE)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_EXIT_STORE_MSR_ADDRESS,
	    vcpu->vc_vmx_msr_exit_save_pa)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_EXIT_LOAD_MSR_ADDRESS,
	    vcpu->vc_vmx_msr_exit_load_pa)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_ENTRY_LOAD_MSR_ADDRESS,
	    vcpu->vc_vmx_msr_exit_save_pa)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_MSR_BITMAP_ADDRESS,
	    vcpu->vc_msr_bitmap_pa)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_CR4_MASK, CR4_VMXE)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_CR0_MASK, CR0_NE)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Set up the VMCS for the register state we want during VCPU start.
	 * This matches what the CPU state would be after a bootloader
	 * transition to 'start'.
	 */
	ret = vcpu_writeregs_vmx(vcpu, VM_RWREGS_ALL, 0, vrs);

	/*
	 * Set up the MSR bitmap
	 */
	memset((uint8_t *)vcpu->vc_msr_bitmap_va, 0xFF, PAGE_SIZE);
	vmx_setmsrbrw(vcpu, MSR_IA32_FEATURE_CONTROL);
	vmx_setmsrbrw(vcpu, MSR_SYSENTER_CS);
	vmx_setmsrbrw(vcpu, MSR_SYSENTER_ESP);
	vmx_setmsrbrw(vcpu, MSR_SYSENTER_EIP);
	vmx_setmsrbrw(vcpu, MSR_EFER);
	vmx_setmsrbrw(vcpu, MSR_STAR);
	vmx_setmsrbrw(vcpu, MSR_LSTAR);
	vmx_setmsrbrw(vcpu, MSR_CSTAR);
	vmx_setmsrbrw(vcpu, MSR_SFMASK);
	vmx_setmsrbrw(vcpu, MSR_FSBASE);
	vmx_setmsrbrw(vcpu, MSR_GSBASE);
	vmx_setmsrbrw(vcpu, MSR_KERNELGSBASE);

	/* XXX CR0 shadow */
	/* XXX CR4 shadow */

	/* Flush the VMCS */
	if (vmclear(&vcpu->vc_control_pa)) {
		ret = EINVAL;
		goto exit;
	}

exit:
	return (ret);
}

/*
 * vcpu_init_vmx
 *
 * Intel VMX specific VCPU initialization routine.
 *
 * This function allocates various per-VCPU memory regions, sets up initial
 * VCPU VMCS controls, and sets initial register values.
 *
 * Parameters:
 *  vcpu: the VCPU structure being initialized
 *
 * Return values:
 *  0: the VCPU was initialized successfully
 *  ENOMEM: insufficient resources
 *  EINVAL: an error occurred during VCPU initialization
 */
int
vcpu_init_vmx(struct vcpu *vcpu)
{
	struct vmcs *vmcs;
	uint32_t cr0, cr4;
	int ret;

	ret = 0;

	/* Allocate VMCS VA */
	vcpu->vc_control_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page, &kp_zero,
	    &kd_waitok);

	if (!vcpu->vc_control_va)
		return (ENOMEM);

	/* Compute VMCS PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_control_va,
	    (paddr_t *)&vcpu->vc_control_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	/* Allocate MSR bitmap VA */
	vcpu->vc_msr_bitmap_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page, &kp_zero,
	    &kd_waitok);

	if (!vcpu->vc_msr_bitmap_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR bitmap PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_msr_bitmap_va,
	    (paddr_t *)&vcpu->vc_msr_bitmap_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	/* Allocate MSR exit load area VA */
	vcpu->vc_vmx_msr_exit_load_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_vmx_msr_exit_load_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR exit load area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_vmx_msr_exit_load_va,
	    &vcpu->vc_vmx_msr_exit_load_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	/* Allocate MSR exit save area VA */
	vcpu->vc_vmx_msr_exit_save_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_vmx_msr_exit_save_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR exit save area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_vmx_msr_exit_save_va,
	    &vcpu->vc_vmx_msr_exit_save_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	/* Allocate MSR entry load area VA */
	vcpu->vc_vmx_msr_entry_load_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_vmx_msr_entry_load_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR entry load area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_vmx_msr_entry_load_va,
	    &vcpu->vc_vmx_msr_entry_load_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	vmcs = (struct vmcs *)vcpu->vc_control_va;
	vmcs->vmcs_revision = curcpu()->ci_vmm_cap.vcc_vmx.vmx_vmxon_revision;

	/*
	 * Load the VMCS onto this PCPU so we can write registers
	 */
	if (vmptrld(&vcpu->vc_control_pa)) {
		ret = EINVAL;
		goto exit;
	}

	/* Host CR0 */
	cr0 = rcr0();
	if (vmwrite(VMCS_HOST_IA32_CR0, cr0)) {
		ret = EINVAL;
		goto exit;
	}

	/* Host CR4 */
	cr4 = rcr4();
	if (vmwrite(VMCS_HOST_IA32_CR4, cr4)) {
		ret = EINVAL;
		goto exit;
	}

	/* Host Segment Selectors */
	if (vmwrite(VMCS_HOST_IA32_CS_SEL, GSEL(GCODE_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_DS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_ES_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_FS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_GS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_SS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_TR_SEL, GSYSSEL(GPROC0_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	/* Host IDTR base */
	if (vmwrite(VMCS_HOST_IA32_IDTR_BASE, idt_vaddr)) {
		ret = EINVAL;
		goto exit;
	}

	/* VMCS link */
	if (vmwrite(VMCS_LINK_POINTER, 0xFFFFFFFFFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

exit:
	if (ret) {
		if (vcpu->vc_control_va)
			km_free((void *)vcpu->vc_control_va, PAGE_SIZE,
			    &kv_page, &kp_zero);
		if (vcpu->vc_msr_bitmap_va)
			km_free((void *)vcpu->vc_msr_bitmap_va, PAGE_SIZE,
			    &kv_page, &kp_zero);
		if (vcpu->vc_vmx_msr_exit_save_va)
			km_free((void *)vcpu->vc_vmx_msr_exit_save_va,
			    PAGE_SIZE, &kv_page, &kp_zero);
		if (vcpu->vc_vmx_msr_exit_load_va)
			km_free((void *)vcpu->vc_vmx_msr_exit_load_va,
			    PAGE_SIZE, &kv_page, &kp_zero);
		if (vcpu->vc_vmx_msr_entry_load_va)
			km_free((void *)vcpu->vc_vmx_msr_entry_load_va,
			    PAGE_SIZE, &kv_page, &kp_zero);
	} else {
		if (vmclear(&vcpu->vc_control_pa))
			ret = EINVAL;
	}

	return (ret);
}

/*
 * vcpu_reset_regs
 *
 * Resets a vcpu's registers to the provided state
 *
 * Parameters:
 *  vcpu: the vcpu whose registers shall be reset
 *  vrs: the desired register state
 *
 * Return values:
 *  0: the vcpu's registers were successfully reset
 *  !0: the vcpu's registers could not be reset (see arch-specific reset
 *      function for various values that can be returned here)
 */
int 
vcpu_reset_regs(struct vcpu *vcpu, struct vcpu_reg_state *vrs)
{
	int ret;

	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT)
		ret = vcpu_reset_regs_vmx(vcpu, vrs);
	else if (vmm_softc->mode == VMM_MODE_SVM ||
		 vmm_softc->mode == VMM_MODE_RVI)
		ret = vcpu_reset_regs_svm(vcpu, vrs);
	else
		panic("unknown vmm mode\n");

	return (ret);
}

/*
 * vcpu_init_svm
 *
 * AMD SVM specific VCPU initialization routine.
 *
 * This function allocates various per-VCPU memory regions, sets up initial
 * VCPU VMCB controls, and sets initial register values.
 *
 * Parameters:
 *  vcpu: the VCPU structure being initialized
 *
 * Return values:
 *  0: the VCPU was initialized successfully
 *  ENOMEM: insufficient resources
 *  EINVAL: an error occurred during VCPU initialization
 */
int
vcpu_init_svm(struct vcpu *vcpu)
{
	int ret;

	ret = 0;

	/* Allocate VMCB VA */
	vcpu->vc_control_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page, &kp_zero,
	    &kd_waitok);

	if (!vcpu->vc_control_va)
		return (ENOMEM);

	/* Compute VMCB PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_control_va,
	    (paddr_t *)&vcpu->vc_control_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	DPRINTF("%s: VMCB va @@ 0x%llx, pa @@ 0x%llx\n", __func__,
	    (uint64_t)vcpu->vc_control_va,
	    (uint64_t)vcpu->vc_control_pa);


	/* Allocate MSR bitmap VA (2 pages) */
	vcpu->vc_msr_bitmap_va = (vaddr_t)km_alloc(2 * PAGE_SIZE, &kv_any,
	    &vmm_kp_contig, &kd_waitok);

	if (!vcpu->vc_msr_bitmap_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR bitmap PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_msr_bitmap_va,
	    (paddr_t *)&vcpu->vc_msr_bitmap_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	DPRINTF("%s: MSR bitmap va @@ 0x%llx, pa @@ 0x%llx\n", __func__,
	    (uint64_t)vcpu->vc_msr_bitmap_va,
	    (uint64_t)vcpu->vc_msr_bitmap_pa);

	/* Allocate host state area VA */
	vcpu->vc_svm_hsa_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_svm_hsa_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute host state area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_svm_hsa_va,
	    &vcpu->vc_svm_hsa_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	DPRINTF("%s: HSA va @@ 0x%llx, pa @@ 0x%llx\n", __func__,
	    (uint64_t)vcpu->vc_svm_hsa_va,
	    (uint64_t)vcpu->vc_svm_hsa_pa);

	/* Allocate IOIO area VA (3 pages) */
	vcpu->vc_svm_ioio_va = (vaddr_t)km_alloc(3 * PAGE_SIZE, &kv_any,
	   &vmm_kp_contig, &kd_waitok);

	if (!vcpu->vc_svm_ioio_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute IOIO area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_svm_ioio_va,
	    &vcpu->vc_svm_ioio_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	DPRINTF("%s: IOIO va @@ 0x%llx, pa @@ 0x%llx\n", __func__,
	    (uint64_t)vcpu->vc_svm_ioio_va,
	    (uint64_t)vcpu->vc_svm_ioio_pa);

exit:
	if (ret) {
		if (vcpu->vc_control_va)
			km_free((void *)vcpu->vc_control_va, PAGE_SIZE,
			    &kv_page, &kp_zero);
		if (vcpu->vc_msr_bitmap_va)
			km_free((void *)vcpu->vc_msr_bitmap_va, 2 * PAGE_SIZE,
			    &kv_any, &vmm_kp_contig);
		if (vcpu->vc_svm_hsa_va)
			km_free((void *)vcpu->vc_svm_hsa_va, PAGE_SIZE,
			    &kv_page, &kp_zero);
		if (vcpu->vc_svm_ioio_va)
			km_free((void *)vcpu->vc_svm_ioio_va,
			    3 * PAGE_SIZE, &kv_any, &vmm_kp_contig);
	}

	return (ret);
}

/*
 * vcpu_init
 *
 * Calls the architecture-specific VCPU init routine
 */
int
vcpu_init(struct vcpu *vcpu)
{
	int ret = 0;

	vcpu->vc_virt_mode = vmm_softc->mode;
	vcpu->vc_state = VCPU_STATE_STOPPED;
	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT)
		ret = vcpu_init_vmx(vcpu);
	else if (vmm_softc->mode == VMM_MODE_SVM ||
		 vmm_softc->mode == VMM_MODE_RVI)
		ret = vcpu_init_svm(vcpu);
	else
		panic("unknown vmm mode\n");

	return (ret);
}

/*
 * vcpu_deinit_vmx
 *
 * Deinitializes the vcpu described by 'vcpu'
 *
 * Parameters:
 *  vcpu: the vcpu to be deinited
 */
void
vcpu_deinit_vmx(struct vcpu *vcpu)
{
	if (vcpu->vc_control_va)
		km_free((void *)vcpu->vc_control_va, PAGE_SIZE,
		    &kv_page, &kp_zero);
	if (vcpu->vc_vmx_msr_exit_save_va)
		km_free((void *)vcpu->vc_vmx_msr_exit_save_va,
		    PAGE_SIZE, &kv_page, &kp_zero);
	if (vcpu->vc_vmx_msr_exit_load_va)
		km_free((void *)vcpu->vc_vmx_msr_exit_load_va,
		    PAGE_SIZE, &kv_page, &kp_zero);
	if (vcpu->vc_vmx_msr_entry_load_va)
		km_free((void *)vcpu->vc_vmx_msr_entry_load_va,
		    PAGE_SIZE, &kv_page, &kp_zero);
}

/*
 * vcpu_deinit_svm
 *
 * Deinitializes the vcpu described by 'vcpu'
 *
 * Parameters:
 *  vcpu: the vcpu to be deinited
 */
void
vcpu_deinit_svm(struct vcpu *vcpu)
{
	if (vcpu->vc_control_va)
		km_free((void *)vcpu->vc_control_va, PAGE_SIZE, &kv_page,
		    &kp_zero);
	if (vcpu->vc_msr_bitmap_va)
		km_free((void *)vcpu->vc_msr_bitmap_va, 2 * PAGE_SIZE, &kv_any,
		    &vmm_kp_contig);
	if (vcpu->vc_svm_hsa_va)
		km_free((void *)vcpu->vc_svm_hsa_va, PAGE_SIZE, &kv_page,
		    &kp_zero);
	if (vcpu->vc_svm_ioio_va)
		km_free((void *)vcpu->vc_svm_ioio_va, 3 * PAGE_SIZE, &kv_any,
		    &vmm_kp_contig);
}

/*
 * vcpu_deinit
 *
 * Calls the architecture-specific VCPU deinit routine
 *
 * Parameters:
 *  vcpu: the vcpu to be deinited
 */
void
vcpu_deinit(struct vcpu *vcpu)
{
	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT)
		vcpu_deinit_vmx(vcpu);
	else if	(vmm_softc->mode == VMM_MODE_SVM ||
		 vmm_softc->mode == VMM_MODE_RVI)
		vcpu_deinit_svm(vcpu);
	else
		panic("unknown vmm mode\n");
}

/*
 * vm_teardown
 *
 * Tears down (destroys) the vm indicated by 'vm'.
 */
void
vm_teardown(struct vm *vm)
{
	struct vcpu *vcpu, *tmp;

	/* Free VCPUs */
	rw_enter_write(&vm->vm_vcpu_lock);
	SLIST_FOREACH_SAFE(vcpu, &vm->vm_vcpu_list, vc_vcpu_link, tmp) {
		SLIST_REMOVE(&vm->vm_vcpu_list, vcpu, vcpu, vc_vcpu_link);
		vcpu_deinit(vcpu);
		pool_put(&vcpu_pool, vcpu);
	}

	vm_impl_deinit(vm);

	/* teardown guest vmspace */
	if (vm->vm_map != NULL)
		uvm_map_deallocate(vm->vm_map);

	vmm_softc->vm_ct--;
	if (vmm_softc->vm_ct < 1)
		vmm_stop();
	rw_exit_write(&vm->vm_vcpu_lock);
	pool_put(&vm_pool, vm);
}

/*
 * vcpu_vmx_check_cap
 *
 * Checks if the 'cap' bit in the 'msr' MSR can be set or cleared (set = 1
 * or set = 0, respectively).
 *
 * When considering 'msr', we check to see if true controls are available,
 * and use those if so.
 *
 * Returns 1 of 'cap' can be set/cleared as requested, 0 otherwise.
 */
int
vcpu_vmx_check_cap(struct vcpu *vcpu, uint32_t msr, uint32_t cap, int set)
{
	uint64_t ctl;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		switch (msr) {
		case IA32_VMX_PINBASED_CTLS:
			ctl = vcpu->vc_vmx_true_pinbased_ctls;
			break;
		case IA32_VMX_PROCBASED_CTLS:
			ctl = vcpu->vc_vmx_true_procbased_ctls;
			break;
		case IA32_VMX_PROCBASED2_CTLS:
			ctl = vcpu->vc_vmx_procbased2_ctls;
			break;
		case IA32_VMX_ENTRY_CTLS:
			ctl = vcpu->vc_vmx_true_entry_ctls;
			break;
		case IA32_VMX_EXIT_CTLS:
			ctl = vcpu->vc_vmx_true_exit_ctls;
			break;
		default:
			return (0);
		}
	} else {
		switch (msr) {
		case IA32_VMX_PINBASED_CTLS:
			ctl = vcpu->vc_vmx_pinbased_ctls;
			break;
		case IA32_VMX_PROCBASED_CTLS:
			ctl = vcpu->vc_vmx_procbased_ctls;
			break;
		case IA32_VMX_PROCBASED2_CTLS:
			ctl = vcpu->vc_vmx_procbased2_ctls;
			break;
		case IA32_VMX_ENTRY_CTLS:
			ctl = vcpu->vc_vmx_entry_ctls;
			break;
		case IA32_VMX_EXIT_CTLS:
			ctl = vcpu->vc_vmx_exit_ctls;
			break;
		default:
			return (0);
		}
	}

	if (set) {
		/* Check bit 'cap << 32', must be !0 */
		return (ctl & ((uint64_t)cap << 32)) != 0;
	} else {
		/* Check bit 'cap', must be 0 */
		return (ctl & cap) == 0;
	}
}

/*
 * vcpu_vmx_compute_ctrl
 *
 * Computes the appropriate control value, given the supplied parameters
 * and CPU capabilities.
 *
 * Intel has made somewhat of a mess of this computation - it is described
 * using no fewer than three different approaches, spread across many
 * pages of the SDM. Further compounding the problem is the fact that now
 * we have "true controls" for each type of "control", and each needs to
 * be examined to get the calculation right, but only if "true" controls
 * are present on the CPU we're on.
 *
 * Parameters:
 *  ctrlval: the control value, as read from the CPU MSR
 *  ctrl: which control is being set (eg, pinbased, procbased, etc)
 *  want0: the set of desired 0 bits
 *  want1: the set of desired 1 bits
 *  out: (out) the correct value to write into the VMCS for this VCPU,
 *      for the 'ctrl' desired.
 *
 * Returns 0 if successful, or EINVAL if the supplied parameters define
 *     an unworkable control setup.
 */
int
vcpu_vmx_compute_ctrl(uint64_t ctrlval, uint16_t ctrl, uint32_t want1,
	uint32_t want0, uint32_t *out)
{
	int i, set, clear;

	/*
	 * The Intel SDM gives three formulae for determining which bits to
	 * set/clear for a given control and desired functionality. Formula
	 * 1 is the simplest but disallows use of newer features that are
	 * enabled by functionality in later CPUs.
	 *
	 * Formulas 2 and 3 allow such extra functionality. We use formula
	 * 2 - this requires us to know the identity of controls in the
	 * "default1" class for each control register, but allows us to not
	 * have to pass along and/or query both sets of capability MSRs for
	 * each control lookup. This makes the code slightly longer,
	 * however.
	 */
	for (i = 0; i < 32; i++) {
		/* Figure out if we can set and / or clear this bit */
		set = (ctrlval & (1ULL << (i + 32))) != 0;
		clear = ((1ULL << i) & ((uint64_t)ctrlval)) == 0;

		/* If the bit can't be set nor cleared, something's wrong */
		if (!set && !clear)
			return (EINVAL);

		/*
		 * Formula 2.c.i - "If the relevant VMX capability MSR
		 * reports that a control has a single setting, use that
		 * setting."
		 */
		if (set && !clear) {
			if (want0 & (1ULL << i))
				return (EINVAL);
			else 
				*out |= (1ULL << i);
		} else if (clear && !set) {
			if (want1 & (1ULL << i))
				return (EINVAL);
			else
				*out &= ~(1ULL << i);
		} else {
			/*
			 * 2.c.ii - "If the relevant VMX capability MSR
			 * reports that a control can be set to 0 or 1
			 * and that control's meaning is known to the VMM,
			 * set the control based on the functionality desired."
			 */
			if (want1 & (1ULL << i))
				*out |= (1ULL << i);
			else if (want0 & (1 << i))
				*out &= ~(1ULL << i);
			else {
				/*
				 * ... assuming the control's meaning is not
				 * known to the VMM ...
				 *
				 * 2.c.iii - "If the relevant VMX capability
				 * MSR reports that a control can be set to 0
			 	 * or 1 and the control is not in the default1
				 * class, set the control to 0."
				 *
				 * 2.c.iv - "If the relevant VMX capability
				 * MSR reports that a control can be set to 0
				 * or 1 and the control is in the default1
				 * class, set the control to 1."
				 */
				switch (ctrl) {
				case IA32_VMX_PINBASED_CTLS:
				case IA32_VMX_TRUE_PINBASED_CTLS:
					/*
					 * A.3.1 - default1 class of pinbased
					 * controls comprises bits 1,2,4
					 */
					switch (i) {
						case 1:
						case 2:
						case 4:
							*out |= (1ULL << i);
							break;
						default:
							*out &= ~(1ULL << i);
							break;
					}
					break;
				case IA32_VMX_PROCBASED_CTLS:
				case IA32_VMX_TRUE_PROCBASED_CTLS:
					/*
					 * A.3.2 - default1 class of procbased
					 * controls comprises bits 1, 4-6, 8,
					 * 13-16, 26
					 */
					switch (i) {
						case 1:
						case 4 ... 6:
						case 8:
						case 13 ... 16:
						case 26:
							*out |= (1ULL << i);
							break;
						default:
							*out &= ~(1ULL << i);
							break;
					}
					break;
					/*
					 * Unknown secondary procbased controls
					 * can always be set to 0
					 */
				case IA32_VMX_PROCBASED2_CTLS:
					*out &= ~(1ULL << i);
					break;
				case IA32_VMX_EXIT_CTLS:
				case IA32_VMX_TRUE_EXIT_CTLS:
					/*
					 * A.4 - default1 class of exit
					 * controls comprises bits 0-8, 10,
					 * 11, 13, 14, 16, 17
					 */
					switch (i) {
						case 0 ... 8:
						case 10 ... 11:
						case 13 ... 14:
						case 16 ... 17:
							*out |= (1ULL << i);
							break;
						default:
							*out &= ~(1ULL << i);
							break;
					}
					break;
				case IA32_VMX_ENTRY_CTLS:
				case IA32_VMX_TRUE_ENTRY_CTLS:
					/*
					 * A.5 - default1 class of entry
					 * controls comprises bits 0-8, 12
					 */
					switch (i) {
						case 0 ... 8:
						case 12:
							*out |= (1ULL << i);
							break;
						default:
							*out &= ~(1ULL << i);
							break;
					}
					break;
				}
			}
		}
	}

	return (0);
}

/*
 * vm_get_info
 *
 * Returns information about the VM indicated by 'vip'.
 */
int
vm_get_info(struct vm_info_params *vip)
{
	struct vm_info_result *out;
	struct vm *vm;
	struct vcpu *vcpu;
	int i, j;
	size_t need;

	rw_enter_read(&vmm_softc->vm_lock);
	need = vmm_softc->vm_ct * sizeof(struct vm_info_result);
	if (vip->vip_size < need) {
		vip->vip_info_ct = 0;
		vip->vip_size = need;
		rw_exit_read(&vmm_softc->vm_lock);
		return (0);
	}

	out = malloc(need, M_DEVBUF, M_NOWAIT|M_ZERO);
	if (out == NULL) {
		vip->vip_info_ct = 0;
		rw_exit_read(&vmm_softc->vm_lock);
		return (ENOMEM);
	}

	i = 0;
	vip->vip_info_ct = vmm_softc->vm_ct;
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		out[i].vir_memory_size = vm->vm_memory_size;
		out[i].vir_used_size =
		    pmap_resident_count(vm->vm_map->pmap) * PAGE_SIZE;
		out[i].vir_ncpus = vm->vm_vcpu_ct;
		out[i].vir_id = vm->vm_id;
		out[i].vir_creator_pid = vm->vm_creator_pid;
		strncpy(out[i].vir_name, vm->vm_name, VMM_MAX_NAME_LEN);
		rw_enter_read(&vm->vm_vcpu_lock);
		for (j = 0; j < vm->vm_vcpu_ct; j++) {
			out[i].vir_vcpu_state[j] = VCPU_STATE_UNKNOWN;
			SLIST_FOREACH(vcpu, &vm->vm_vcpu_list,
			    vc_vcpu_link) {
				if (vcpu->vc_id == j)
					out[i].vir_vcpu_state[j] =
					    vcpu->vc_state;
			}
		}
		rw_exit_read(&vm->vm_vcpu_lock);
		i++;
	}
	rw_exit_read(&vmm_softc->vm_lock);
	if (copyout(out, vip->vip_info, need) == EFAULT) {
		free(out, M_DEVBUF, need);
		return (EFAULT);
	}

	free(out, M_DEVBUF, need);
	return (0);
}

/*
 * vm_terminate
 *
 * Terminates the VM indicated by 'vtp'.
 */
int
vm_terminate(struct vm_terminate_params *vtp)
{
	struct vm *vm;
	struct vcpu *vcpu;
	u_int old, next;
	int error;

	/*
	 * Find desired VM
	 */
	rw_enter_read(&vmm_softc->vm_lock);
	error = vm_find(vtp->vtp_vm_id, &vm);

	if (error == 0) {
		rw_enter_read(&vm->vm_vcpu_lock);
		SLIST_FOREACH(vcpu, &vm->vm_vcpu_list, vc_vcpu_link) {
			do {
				old = vcpu->vc_state;
				if (old == VCPU_STATE_RUNNING)
					next = VCPU_STATE_REQTERM;
				else if (old == VCPU_STATE_STOPPED)
					next = VCPU_STATE_TERMINATED;
				else /* must be REQTERM or TERMINATED */
					break;
			} while (old != atomic_cas_uint(&vcpu->vc_state,
			    old, next));
		}
		rw_exit_read(&vm->vm_vcpu_lock);
	}
	rw_exit_read(&vmm_softc->vm_lock);

	if (error != 0)
		return (error);

	/* XXX possible race here two threads terminating the same vm? */
	rw_enter_write(&vmm_softc->vm_lock);
	SLIST_REMOVE(&vmm_softc->vm_list, vm, vm, vm_link);
	rw_exit_write(&vmm_softc->vm_lock);
	if (vm->vm_vcpus_running == 0)
		vm_teardown(vm);

	return (0);
}

/*
 * vm_run
 *
 * Run the vm / vcpu specified by 'vrp'
 */
int
vm_run(struct vm_run_params *vrp)
{
	struct vm *vm;
	struct vcpu *vcpu;
	int ret = 0, error;
	u_int old, next;

	/*
	 * Find desired VM
	 */
	rw_enter_read(&vmm_softc->vm_lock);
	error = vm_find(vrp->vrp_vm_id, &vm);

	/*
	 * Attempt to locate the requested VCPU. If found, attempt to
	 * to transition from VCPU_STATE_STOPPED -> VCPU_STATE_RUNNING.
	 * Failure to make the transition indicates the VCPU is busy.
	 */
	if (error == 0) {
		rw_enter_read(&vm->vm_vcpu_lock);
		SLIST_FOREACH(vcpu, &vm->vm_vcpu_list, vc_vcpu_link) {
			if (vcpu->vc_id == vrp->vrp_vcpu_id)
				break;
		}

		if (vcpu != NULL) {
			old = VCPU_STATE_STOPPED;
			next = VCPU_STATE_RUNNING;

			if (atomic_cas_uint(&vcpu->vc_state, old, next) != old)
				ret = EBUSY;
			else
				atomic_inc_int(&vm->vm_vcpus_running);
		}
		rw_exit_read(&vm->vm_vcpu_lock);

		if (vcpu == NULL)
			ret = ENOENT;
	}
	rw_exit_read(&vmm_softc->vm_lock);

	if (error != 0)
		ret = error;

	/* Bail if errors detected in the previous steps */
	if (ret)
		return (ret);

	/*
	 * We may be returning from userland helping us from the last exit.
	 * If so (vrp_continue == 1), copy in the exit data from vmd. The
	 * exit data will be consumed before the next entry (this typically
	 * comprises VCPU register changes as the result of vmd(8)'s actions).
	 */
	if (vrp->vrp_continue) {
		if (copyin(vrp->vrp_exit, &vcpu->vc_exit,
		    sizeof(union vm_exit)) == EFAULT) {
			return (EFAULT);
		}
	}

	/* Run the VCPU specified in vrp */
	if (vcpu->vc_virt_mode == VMM_MODE_VMX ||
	    vcpu->vc_virt_mode == VMM_MODE_EPT) {
		ret = vcpu_run_vmx(vcpu, vrp);
	} else if (vcpu->vc_virt_mode == VMM_MODE_SVM ||
		   vcpu->vc_virt_mode == VMM_MODE_RVI) {
		ret = vcpu_run_svm(vcpu, vrp);
	}

	/*
	 * We can set the VCPU states here without CAS because once
	 * a VCPU is in state RUNNING or REQTERM, only the VCPU itself
	 * can switch the state.
	 */
	atomic_dec_int(&vm->vm_vcpus_running);
	if (vcpu->vc_state == VCPU_STATE_REQTERM) {
		vrp->vrp_exit_reason = VM_EXIT_TERMINATED;
		vcpu->vc_state = VCPU_STATE_TERMINATED;
		if (vm->vm_vcpus_running == 0)
			vm_teardown(vm);
		ret = 0;
	} else if (ret == EAGAIN) {
		/* If we are exiting, populate exit data so vmd can help. */
		vrp->vrp_exit_reason = vcpu->vc_gueststate.vg_exit_reason;
		vrp->vrp_irqready = vcpu->vc_irqready;
		vcpu->vc_state = VCPU_STATE_STOPPED;

		if (copyout(&vcpu->vc_exit, vrp->vrp_exit,
		    sizeof(union vm_exit)) == EFAULT) {
			ret = EFAULT;
		} else
			ret = 0;
	} else if (ret == 0) {
		vrp->vrp_exit_reason = VM_EXIT_NONE;
		vcpu->vc_state = VCPU_STATE_STOPPED;
	} else {
		vrp->vrp_exit_reason = VM_EXIT_TERMINATED;
		vcpu->vc_state = VCPU_STATE_TERMINATED;
	}

	return (ret);
}

/*
 * vcpu_must_stop
 *
 * Check if we need to (temporarily) stop running the VCPU for some reason,
 * such as:
 * - the VM was requested to terminate
 * - the proc running this VCPU has pending signals
 */
int
vcpu_must_stop(struct vcpu *vcpu)
{
	struct proc *p = curproc;

	if (vcpu->vc_state == VCPU_STATE_REQTERM)
		return (1);
	if (CURSIG(p) != 0)
		return (1);
	return (0);
}

/*
 * vcpu_run_vmx
 *
 * VMX main loop used to run a VCPU.
 *
 * Parameters:
 *  vcpu: The VCPU to run
 *  vrp: run parameters
 *
 * Return values:
 *  0: The run loop exited and no help is needed from vmd
 *  EAGAIN: The run loop exited and help from vmd is needed
 *  EINVAL: an error occured
 */
int
vcpu_run_vmx(struct vcpu *vcpu, struct vm_run_params *vrp)
{
	int ret = 0, resume, locked, exitinfo;
	struct region_descriptor gdt;
	struct cpu_info *ci;
	uint64_t exit_reason, cr3, vmcs_ptr, insn_error;
	struct schedstate_percpu *spc;
	struct vmx_invvpid_descriptor vid;
	uint64_t eii, procbased, int_st;
	uint16_t irq;

	resume = 0;
	irq = vrp->vrp_irq;

	/*
	 * If we are returning from userspace (vmd) because we exited
	 * last time, fix up any needed vcpu state first. Which state
	 * needs to be fixed up depends on what vmd populated in the
	 * exit data structure.
	 */
	if (vrp->vrp_continue) {
		switch (vcpu->vc_gueststate.vg_exit_reason) {
		case VMX_EXIT_IO:
			vcpu->vc_gueststate.vg_rax =
			    vcpu->vc_exit.vei.vei_data;
			break;
		case VMX_EXIT_HLT:
			break;
		case VMX_EXIT_INT_WINDOW:
			break;
		case VMX_EXIT_EXTINT:
			break;
		case VMX_EXIT_EPT_VIOLATION:
			break;
		case VMX_EXIT_CPUID:
			break;
#ifdef VMM_DEBUG
		case VMX_EXIT_TRIPLE_FAULT:
			DPRINTF("%s: vm %d vcpu %d triple fault\n",
			    __func__, vcpu->vc_parent->vm_id,
			    vcpu->vc_id);
			vmx_vcpu_dump_regs(vcpu);
			dump_vcpu(vcpu);
			vmx_dump_vmcs(vcpu);
			break;
		case VMX_EXIT_ENTRY_FAILED_GUEST_STATE:
			DPRINTF("%s: vm %d vcpu %d failed entry "
			    "due to invalid guest state\n",
			    __func__, vcpu->vc_parent->vm_id,
			    vcpu->vc_id);
			vmx_vcpu_dump_regs(vcpu);
			dump_vcpu(vcpu);
			return EINVAL;
		default:
			DPRINTF("%s: unimplemented exit type %d (%s)\n",
			    __func__,
			    vcpu->vc_gueststate.vg_exit_reason,
			    vmx_exit_reason_decode(
				vcpu->vc_gueststate.vg_exit_reason));
			vmx_vcpu_dump_regs(vcpu);
			dump_vcpu(vcpu);
			break;
#endif /* VMM_DEBUG */
		}
	}

	while (ret == 0) {
		if (!resume) {
			/*
			 * We are launching for the first time, or we are
			 * resuming from a different pcpu, so we need to
			 * reset certain pcpu-specific values.
			 */
			ci = curcpu();
			setregion(&gdt, ci->ci_gdt, GDT_SIZE - 1);

			vcpu->vc_last_pcpu = ci;

			if (vmptrld(&vcpu->vc_control_pa)) {
				ret = EINVAL;
				break;
			}

			if (gdt.rd_base == 0) {
				ret = EINVAL;
				break;
			}

			/* Host GDTR base */
			if (vmwrite(VMCS_HOST_IA32_GDTR_BASE, gdt.rd_base)) {
				ret = EINVAL;
				break;
			}

			/* Host TR base */
			if (vmwrite(VMCS_HOST_IA32_TR_BASE,
			    (uint64_t)curcpu()->ci_tss)) {
				ret = EINVAL;
				break;
			}

			/* Host CR3 */
			cr3 = rcr3();
			if (vmwrite(VMCS_HOST_IA32_CR3, cr3)) {
				ret = EINVAL;
				break;
			}
		}

		/* Handle vmd(8) injected interrupts */
		/* Is there an interrupt pending injection? */
		if (irq != 0xFFFF) {
			if (vmread(VMCS_GUEST_INTERRUPTIBILITY_ST, &int_st)) {
				printf("%s: can't get interruptibility state\n",
				    __func__);
				ret = EINVAL;
				break;
			}

			/* Interruptbility state 0x3 covers NMIs and STI */
			if (!(int_st & 0x3) && vcpu->vc_irqready) {
				eii = (irq & 0xFF);
				eii |= (1ULL << 31);	/* Valid */
				eii |= (0ULL << 8);	/* Hardware Interrupt */
				if (vmwrite(VMCS_ENTRY_INTERRUPTION_INFO, eii)) {
					printf("vcpu_run_vmx: can't vector "
					    "interrupt to guest\n");
					ret = EINVAL;
					break;
				}

				irq = 0xFFFF;
			}
		} else if (!vcpu->vc_intr) {
			/*
			 * Disable window exiting
			 */
			if (vmread(VMCS_PROCBASED_CTLS, &procbased)) {
				printf("vcpu_run_vmx: can't read"
				    "procbased ctls on exit\n");
				ret = EINVAL;
				break;
			} else {
				procbased &= ~IA32_VMX_INTERRUPT_WINDOW_EXITING;
				if (vmwrite(VMCS_PROCBASED_CTLS, procbased)) {
					printf("vcpu_run_vmx: can't write"
					   " procbased ctls on exit\n");
					ret = EINVAL;
					break;
				}
			}
		}

		/* Invalidate old TLB mappings */
		vid.vid_vpid = vcpu->vc_parent->vm_id;
		vid.vid_addr = 0;
		invvpid(IA32_VMX_INVVPID_SINGLE_CTX_GLB, &vid);

		/* Start / resume the VCPU */
		KERNEL_ASSERT_LOCKED();
		KERNEL_UNLOCK();
		ret = vmx_enter_guest(&vcpu->vc_control_pa,
		    &vcpu->vc_gueststate, resume);

		exit_reason = VM_EXIT_NONE;
		if (ret == 0) {
			/*
			 * ret == 0 implies we entered the guest, and later
			 * exited for some valid reason
			 */
			exitinfo = vmx_get_exit_info(
			    &vcpu->vc_gueststate.vg_rip, &exit_reason);
			if (vmread(VMCS_GUEST_IA32_RFLAGS,
			    &vcpu->vc_gueststate.vg_rflags)) {
				printf("vcpu_run_vmx: can't read guest rflags"
				   " during exit\n");
				ret = EINVAL;
				break;
                        }
		}

		if (ret || exitinfo != VMX_EXIT_INFO_COMPLETE ||
		    exit_reason != VMX_EXIT_EXTINT) {
			KERNEL_LOCK();
			locked = 1;
		} else
			locked = 0;

		/* If we exited successfully ... */
		if (ret == 0) {
			resume = 1;
			if (!(exitinfo & VMX_EXIT_INFO_HAVE_RIP)) {
				printf("vcpu_run_vmx: cannot read guest rip\n");
				ret = EINVAL;
				break;
			}

			if (!(exitinfo & VMX_EXIT_INFO_HAVE_REASON)) {
				printf("vcpu_run_vmx: cant read exit reason\n");
				ret = EINVAL;
				break;
			}

			/*
			 * Handle the exit. This will alter "ret" to EAGAIN if
			 * the exit handler determines help from vmd is needed.
			 */
			vcpu->vc_gueststate.vg_exit_reason = exit_reason;
			ret = vmx_handle_exit(vcpu);

			/*
			 * When the guest exited due to an external interrupt,
			 * we do not yet hold the kernel lock: we need to
			 * handle interrupts first before grabbing the lock:
			 * the interrupt handler might do work that
			 * another CPU holding the kernel lock waits for.
			 *
			 * Example: the TLB shootdown code in the pmap module
			 * sends an IPI to all other CPUs and busy-waits for
			 * them to decrement tlb_shoot_wait to zero. While
			 * busy-waiting, the kernel lock is held.
			 *
			 * If this code here attempted to grab the kernel lock
			 * before handling the interrupt, it would block
			 * forever.
			 */
			if (!locked)
				KERNEL_LOCK();

			if (vcpu->vc_gueststate.vg_rflags & PSL_I)
				vcpu->vc_irqready = 1;
			else
				vcpu->vc_irqready = 0;

			/*
			 * If not ready for interrupts, but interrupts pending,
			 * enable interrupt window exiting.
			 */
			if (vcpu->vc_irqready == 0 && vcpu->vc_intr) {
				if (vmread(VMCS_PROCBASED_CTLS, &procbased)) {
					printf("vcpu_run_vmx: can't read"
					   " procbased ctls on intwin exit\n");
					ret = EINVAL;
					break;
				}

				procbased |= IA32_VMX_INTERRUPT_WINDOW_EXITING;
				if (vmwrite(VMCS_PROCBASED_CTLS, procbased)) {
					printf("vcpu_run_vmx: can't write"
					   " procbased ctls on intwin exit\n");
					ret = EINVAL;
					break;
				}
			}

			/*
			 * Exit to vmd if we are terminating, failed to enter,
			 * or need help (device I/O)
			 */
			if (ret || vcpu_must_stop(vcpu))
				break;

			if (vcpu->vc_intr && vcpu->vc_irqready) {
				ret = EAGAIN;
				break;
			}

			/* Check if we should yield - don't hog the cpu */
			spc = &ci->ci_schedstate;
			if (spc->spc_schedflags & SPCF_SHOULDYIELD) {
				resume = 0;
				if (vmclear(&vcpu->vc_control_pa)) {
					ret = EINVAL;
					break;
				}
				yield();
			}
		} else if (ret == VMX_FAIL_LAUNCH_INVALID_VMCS) {
			printf("vcpu_run_vmx: failed launch with invalid "
			    "vmcs\n");
#ifdef VMM_DEBUG
			vmx_vcpu_dump_regs(vcpu);
			dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
			ret = EINVAL;
		} else if (ret == VMX_FAIL_LAUNCH_VALID_VMCS) {
			exit_reason = vcpu->vc_gueststate.vg_exit_reason;
			printf("vcpu_run_vmx: failed launch with valid "
			    "vmcs, code=%lld (%s)\n", exit_reason,
			    vmx_instruction_error_decode(exit_reason));

			if (vmread(VMCS_INSTRUCTION_ERROR, &insn_error)) {
				printf("vcpu_run_vmx: can't read"
				    " insn error field\n");
			} else
				printf("vcpu_run_vmx: insn error code = "
				    "%lld\n", insn_error);
#ifdef VMM_DEBUG
			vmx_vcpu_dump_regs(vcpu);
			dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
			ret = EINVAL;
		} else {
			printf("vcpu_run_vmx: failed launch for unknown "
			    "reason %d\n", ret);
#ifdef VMM_DEBUG
			vmx_vcpu_dump_regs(vcpu);
			dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
			ret = EINVAL;
		}
	}

	/*
	 * We are heading back to userspace (vmd), either because we need help
	 * handling an exit, a guest interrupt is pending, or we failed in some
	 * way to enter the guest. Clear any current VMCS pointer as we may end
	 * up coming back on a different CPU.
	 */
	if (!vmptrst(&vmcs_ptr)) {
		if (vmcs_ptr != 0xFFFFFFFFFFFFFFFFULL)
			if (vmclear(&vcpu->vc_control_pa))
				ret = EINVAL;
	} else
		ret = EINVAL;

	return (ret);
}

/*
 * vmx_handle_intr
 *
 * Handle host (external) interrupts. We read which interrupt fired by
 * extracting the vector from the VMCS and dispatch the interrupt directly
 * to the host using vmm_dispatch_intr.
 */
void
vmx_handle_intr(struct vcpu *vcpu)
{
	uint8_t vec;
	uint64_t eii;
	struct gate_descriptor *idte;
	vaddr_t handler;

	if (vmread(VMCS_EXIT_INTERRUPTION_INFO, &eii)) {
		printf("vmx_handle_intr: can't obtain intr info\n");
		return;
	}

	vec = eii & 0xFF;

	/* XXX check "error valid" code in eii, abort if 0 */
	idte=&idt[vec];
	handler = idte->gd_looffset + ((uint64_t)idte->gd_hioffset << 16);
	vmm_dispatch_intr(handler);
}

/*
 * vmx_handle_hlt
 *
 * Handle HLT exits
 */
int
vmx_handle_hlt(struct vcpu *vcpu)
{
	uint64_t insn_length;

	if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
		printf("%s: can't obtain instruction length\n", __func__);
		return (EINVAL);
	}

	/* All HLT insns are 1 byte */
	KASSERT(insn_length == 1);

	vcpu->vc_gueststate.vg_rip += insn_length;
	return (EAGAIN);
}

/*
 * vmx_get_exit_info
 *
 * Returns exit information containing the current guest RIP and exit reason
 * in rip and exit_reason. The return value is a bitmask indicating whether
 * reading the RIP and exit reason was successful.
 */
int
vmx_get_exit_info(uint64_t *rip, uint64_t *exit_reason)
{
	int rv = 0;

	if (vmread(VMCS_GUEST_IA32_RIP, rip) == 0) {
		rv |= VMX_EXIT_INFO_HAVE_RIP;
		if (vmread(VMCS_EXIT_REASON, exit_reason) == 0)
			rv |= VMX_EXIT_INFO_HAVE_REASON;
	}
	return (rv);
}

/*
 * vmx_handle_exit
 *
 * Handle exits from the VM by decoding the exit reason and calling various
 * subhandlers as needed.
 */
int
vmx_handle_exit(struct vcpu *vcpu)
{
	uint64_t exit_reason, rflags, istate;
	int update_rip, ret = 0;

	update_rip = 0;
	exit_reason = vcpu->vc_gueststate.vg_exit_reason;
	rflags = vcpu->vc_gueststate.vg_rflags;

	switch (exit_reason) {
	case VMX_EXIT_INT_WINDOW:
		if (!(rflags & PSL_I)) {
			DPRINTF("vmx_handle_exit: impossible interrupt window"
			   " exit config\n");
			ret = EINVAL;
			break;
		}

		ret = EAGAIN;
		update_rip = 0;
		break;
	case VMX_EXIT_EPT_VIOLATION:
		ret = vmx_handle_np_fault(vcpu);
		break;
	case VMX_EXIT_CPUID:
		ret = vmm_handle_cpuid(vcpu);
		update_rip = 1;
		break;
	case VMX_EXIT_IO:
		ret = vmx_handle_inout(vcpu);
		update_rip = 1;
		break;
	case VMX_EXIT_EXTINT:
		vmx_handle_intr(vcpu);
		update_rip = 0;
		break;
	case VMX_EXIT_CR_ACCESS:
		ret = vmx_handle_cr(vcpu);
		update_rip = 1;
		break;
	case VMX_EXIT_HLT:
		ret = vmx_handle_hlt(vcpu);
		update_rip = 1;
		break;
	case VMX_EXIT_RDMSR:
		ret = vmx_handle_rdmsr(vcpu);
		update_rip = 1;
		break;
	case VMX_EXIT_WRMSR:
		ret = vmx_handle_wrmsr(vcpu);
		update_rip = 1;
		break;
	case VMX_EXIT_TRIPLE_FAULT:
#ifdef VMM_DEBUG
		DPRINTF("vmx_handle_exit: vm %d vcpu %d triple fault\n",
		    vcpu->vc_parent->vm_id, vcpu->vc_id);
		vmx_vcpu_dump_regs(vcpu);
		dump_vcpu(vcpu);
		vmx_dump_vmcs(vcpu);
#endif /* VMM_DEBUG */
		ret = EAGAIN;
		update_rip = 0;
		break;
	default:
		DPRINTF("vmx_handle_exit: unhandled exit %lld (%s)\n",
		    exit_reason, vmx_exit_reason_decode(exit_reason));
		return (EINVAL);
	}

	if (update_rip) {
		if (vmwrite(VMCS_GUEST_IA32_RIP,
		    vcpu->vc_gueststate.vg_rip)) {
			printf("vmx_handle_exit: can't advance rip\n");
			return (EINVAL);
		}

		if (vmread(VMCS_GUEST_INTERRUPTIBILITY_ST,
		    &istate)) {
			printf("%s: can't read interruptibility state\n",
			    __func__);
			return (EINVAL);
		}

		/* Interruptibilty state 0x3 covers NMIs and STI */
		istate &= ~0x3;

		if (vmwrite(VMCS_GUEST_INTERRUPTIBILITY_ST,
		    istate)) {
			printf("%s: can't write interruptibility state\n",
			    __func__);
			return (EINVAL);
		}
	}

	return (ret);
}

/*
 * vmm_get_guest_memtype
 *
 * Returns the type of memory 'gpa' refers to in the context of vm 'vm'
 */
int
vmm_get_guest_memtype(struct vm *vm, paddr_t gpa)
{
	int i;
	struct vm_mem_range *vmr;

	if (gpa >= VMM_PCI_MMIO_BAR_BASE && gpa <= VMM_PCI_MMIO_BAR_END) {
		DPRINTF("guest mmio access @@ 0x%llx\n", (uint64_t)gpa);
		return (VMM_MEM_TYPE_REGULAR);
	}

	/* XXX Use binary search? */
	for (i = 0; i < vm->vm_nmemranges; i++) {
		vmr = &vm->vm_memranges[i];

		/*
		 * vm_memranges are ascending. gpa can no longer be in one of
		 * the memranges
		 */
		if (gpa < vmr->vmr_gpa)
			break;

		if (gpa < vmr->vmr_gpa + vmr->vmr_size)
			return (VMM_MEM_TYPE_REGULAR);
	}

	DPRINTF("guest memtype @@ 0x%llx unknown\n", (uint64_t)gpa);
	return (VMM_MEM_TYPE_UNKNOWN);
}

/*
 * vmm_get_guest_faulttype
 *
 * Determines the type (R/W/X) of the last fault on the VCPU last run on
 * this PCPU. Calls the appropriate architecture-specific subroutine.
 */
int
vmm_get_guest_faulttype(void)
{
	if (vmm_softc->mode == VMM_MODE_EPT)
		return vmx_get_guest_faulttype();
	else if (vmm_softc->mode == VMM_MODE_RVI)
		return vmx_get_guest_faulttype();
	else
		panic("unknown vmm mode\n");
}

/*
 * vmx_get_exit_qualification
 *
 * Return the current VMCS' exit qualification information
 */
int
vmx_get_exit_qualification(uint64_t *exit_qualification)
{
	if (vmread(VMCS_GUEST_EXIT_QUALIFICATION, exit_qualification)) {
		printf("vmm_get_exit_qualification: cant extract exit qual\n");
		return (EINVAL);
	}

	return (0);
}

/*
 * vmx_get_guest_faulttype
 *
 * Determines the type (R/W/X) of the last fault on the VCPU last run on
 * this PCPU.
 */
int
vmx_get_guest_faulttype(void)
{
	uint64_t exit_qualification;
	uint64_t presentmask = IA32_VMX_EPT_FAULT_WAS_READABLE |
	    IA32_VMX_EPT_FAULT_WAS_WRITABLE | IA32_VMX_EPT_FAULT_WAS_EXECABLE;
	uint64_t protmask = IA32_VMX_EPT_FAULT_READ |
	    IA32_VMX_EPT_FAULT_WRITE | IA32_VMX_EPT_FAULT_EXEC;

	if (vmx_get_exit_qualification(&exit_qualification))
		return (-1);

	if ((exit_qualification & presentmask) == 0)
		return VM_FAULT_INVALID;
	if (exit_qualification & protmask)
		return VM_FAULT_PROTECT;
	return (-1);
}

/*
 * svm_get_guest_faulttype
 *
 * Determines the type (R/W/X) of the last fault on the VCPU last run on
 * this PCPU.
 */
int
svm_get_guest_faulttype(void)
{
	/* XXX removed due to rot */
	return (-1);
}

/*
 * vmx_fault_page
 *
 * Request a new page to be faulted into the UVM map of the VM owning 'vcpu'
 * at address 'gpa'.
 */
int
vmx_fault_page(struct vcpu *vcpu, paddr_t gpa)
{
	int fault_type, ret;

	fault_type = vmx_get_guest_faulttype();
	if (fault_type == -1) {
		printf("vmx_fault_page: invalid fault type\n");
		return (EINVAL);
	}

	ret = uvm_fault(vcpu->vc_parent->vm_map, gpa, fault_type,
	    PROT_READ | PROT_WRITE | PROT_EXEC);
	if (ret)
		printf("vmx_fault_page: uvm_fault returns %d\n", ret);

	return (ret);
}

/*
 * vmx_handle_np_fault
 *
 * High level nested paging handler for VMX. Verifies that a fault is for a
 * valid memory region, then faults a page, or aborts otherwise.
 */
int
vmx_handle_np_fault(struct vcpu *vcpu)
{
	uint64_t gpa;
	int gpa_memtype, ret;

	ret = 0;
	if (vmread(VMCS_GUEST_PHYSICAL_ADDRESS, &gpa)) {
		printf("vmm_handle_np_fault: cannot extract faulting pa\n");
		return (EINVAL);
	}

	gpa_memtype = vmm_get_guest_memtype(vcpu->vc_parent, gpa);
	switch (gpa_memtype) {
	case VMM_MEM_TYPE_REGULAR:
		ret = vmx_fault_page(vcpu, gpa);
		break;
	default:
		printf("unknown memory type %d for GPA 0x%llx\n",
		    gpa_memtype, gpa);
		return (EINVAL);
	}

	return (ret);
}

/*
 * vmx_handle_inout
 *
 * Exit handler for IN/OUT instructions.
 *
 * The vmm can handle certain IN/OUTS without exiting to vmd, but most of these
 * will be passed to vmd for completion.
 */
int
vmx_handle_inout(struct vcpu *vcpu)
{
	uint64_t insn_length, exit_qual;
	int ret;

	if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
		printf("vmx_handle_inout: can't obtain instruction length\n");
		return (EINVAL);
	}

	if (vmx_get_exit_qualification(&exit_qual)) {
		printf("vmx_handle_inout: can't get exit qual\n");
		return (EINVAL);
	}

	/* Bits 0:2 - size of exit */
	vcpu->vc_exit.vei.vei_size = (exit_qual & 0x7) + 1;
	/* Bit 3 - direction */
	vcpu->vc_exit.vei.vei_dir = (exit_qual & 0x8) >> 3;
	/* Bit 4 - string instruction? */
	vcpu->vc_exit.vei.vei_string = (exit_qual & 0x10) >> 4;
	/* Bit 5 - REP prefix? */
	vcpu->vc_exit.vei.vei_rep = (exit_qual & 0x20) >> 5;
	/* Bit 6 - Operand encoding */
	vcpu->vc_exit.vei.vei_encoding = (exit_qual & 0x40) >> 6;
	/* Bit 16:31 - port */
	vcpu->vc_exit.vei.vei_port = (exit_qual & 0xFFFF0000) >> 16;
	/* Data */
	vcpu->vc_exit.vei.vei_data = (uint32_t)vcpu->vc_gueststate.vg_rax;

	vcpu->vc_gueststate.vg_rip += insn_length;

	/*
	 * The following ports usually belong to devices owned by vmd.
	 * Return EAGAIN to signal help needed from userspace (vmd).
	 * Return 0 to indicate we don't care about this port.
	 *
	 * XXX something better than a hardcoded list here, maybe
	 * configure via vmd via the device list in vm create params?
	 *
	 * XXX handle not eax target
	 */
	switch (vcpu->vc_exit.vei.vei_port) {
	case IO_ICU1 ... IO_ICU1 + 1:
	case 0x40 ... 0x43:
	case IO_RTC ... IO_RTC + 1:
	case IO_ICU2 ... IO_ICU2 + 1:
	case 0x3f8 ... 0x3ff:
	case 0xcf8:
	case 0xcfc ... 0xcff:
	case VMM_PCI_IO_BAR_BASE ... VMM_PCI_IO_BAR_END:
		ret = EAGAIN;
		break;
	default:
		/* Read from unsupported ports returns FFs */
		if (vcpu->vc_exit.vei.vei_dir == 1) {
			if (vcpu->vc_exit.vei.vei_size == 4)
				vcpu->vc_gueststate.vg_rax = 0xFFFFFFFF;
			else if (vcpu->vc_exit.vei.vei_size == 2)
				vcpu->vc_gueststate.vg_rax |= 0xFFFF;
			else if (vcpu->vc_exit.vei.vei_size == 1)
				vcpu->vc_gueststate.vg_rax |= 0xFF;
		}
		ret = 0;
	}

	return (ret);
}

/*
 * vmx_handle_cr0_write
 *
 * Write handler for CR0. This function ensures valid values are written into
 * CR0 for the cpu/vmm mode in use (cr0 must-be-0 and must-be-1 bits, etc).
 *
 * Parameters
 *  vcpu: The vcpu taking the cr0 write exit
 *     r: The guest's desired (incoming) cr0 value
 *
 * Return values:
 *  0: if succesful
 *  EINVAL: if an error occurred
 */ 
int
vmx_handle_cr0_write(struct vcpu *vcpu, uint64_t r)
{
	struct vmx_msr_store *msr_store;
	uint64_t ectls;

	/*
	 * XXX this is the place to place handling of the must1,must0 bits
	 *     and the cr0 write shadow
	 */

	/* CR0 must always have NE set */
	r |= CR0_NE;

	if (vmwrite(VMCS_GUEST_IA32_CR0, r)) {
		printf("%s: can't write guest cr0\n", __func__);
		return (EINVAL);
	}

	/* If the guest hasn't enabled paging, nothing more to do. */
	if (!(r & CR0_PG))
		return (0);

	/*
	 * If the guest has enabled paging, then the IA32_VMX_IA32E_MODE_GUEST
	 * control must be set to the same as EFER_LME.
	 */
	msr_store = (struct vmx_msr_store *) vcpu->vc_vmx_msr_exit_save_va;

	if (vmread(VMCS_ENTRY_CTLS, &ectls)) {
		printf("%s: can't read entry controls", __func__);
		return (EINVAL);
	}

	if (msr_store[0].vms_data & EFER_LME)
		ectls |= IA32_VMX_IA32E_MODE_GUEST;
	else
		ectls &= ~IA32_VMX_IA32E_MODE_GUEST;

	if (vmwrite(VMCS_ENTRY_CTLS, ectls)) {
		printf("%s: can't write entry controls", __func__);
		return (EINVAL);
	}

	return (0);
}

/*
 * vmx_handle_cr4_write
 *
 * Write handler for CR4. This function ensures valid values are written into
 * CR4 for the cpu/vmm mode in use (cr4 must-be-0 and must-be-1 bits, etc).
 *
 * Parameters
 *  vcpu: The vcpu taking the cr4 write exit
 *     r: The guest's desired (incoming) cr4 value
 *
 * Return values:
 *  0: if succesful
 *  EINVAL: if an error occurred
 */ 
int
vmx_handle_cr4_write(struct vcpu *vcpu, uint64_t r)
{
	/*
	 * XXX this is the place to place handling of the must1,must0 bits
	 *     and the cr4 write shadow
	 */

	/* CR4_VMXE must always be enabled */
	r |= CR4_VMXE;

	if (vmwrite(VMCS_GUEST_IA32_CR4, r)) {
		printf("%s: can't write guest cr4\n", __func__);
		return (EINVAL);
	}

	return (0);
}

/*
 * vmx_handle_cr
 *
 * Handle reads/writes to control registers (except CR3)
 */
int
vmx_handle_cr(struct vcpu *vcpu)
{
	uint64_t insn_length, exit_qual, r;
	uint8_t crnum, dir, reg;
	
	if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
		printf("%s: can't obtain instruction length\n", __func__);
		return (EINVAL);
	}

	if (vmx_get_exit_qualification(&exit_qual)) {
		printf("%s: can't get exit qual\n", __func__);
		return (EINVAL);
	}

	/* Low 4 bits of exit_qual represent the CR number */
	crnum = exit_qual & 0xf;

	/*
	 * Bits 5:4 indicate the direction of operation (or special CR-modifying
	 * instruction
	 */
	dir = (exit_qual & 0x30) >> 4;

	/* Bits 11:8 encode the source/target register */
	reg = (exit_qual & 0xf00) >> 8;

	switch (dir) {
	case CR_WRITE:
		DPRINTF("%s: mov to cr%d @@ %llx, data=%llx\n", __func__, crnum,
		    vcpu->vc_gueststate.vg_rip, r);
		if (crnum == 0 || crnum == 4) {
			switch (reg) {
			case 0: r = vcpu->vc_gueststate.vg_rax; break;
			case 1: r = vcpu->vc_gueststate.vg_rcx; break;
			case 2: r = vcpu->vc_gueststate.vg_rdx; break;
			case 3: r = vcpu->vc_gueststate.vg_rbx; break;
			case 4: if (vmread(VMCS_GUEST_IA32_RSP, &r)) {
					printf("%s: unable to read guest "
					    "RSP\n", __func__);
					return (EINVAL);
				}
				break;
			case 5: r = vcpu->vc_gueststate.vg_rbp; break;
			case 6: r = vcpu->vc_gueststate.vg_rsi; break;
			case 7: r = vcpu->vc_gueststate.vg_rdi; break;
			case 8: r = vcpu->vc_gueststate.vg_r8; break;
			case 9: r = vcpu->vc_gueststate.vg_r9; break;
			case 10: r = vcpu->vc_gueststate.vg_r10; break;
			case 11: r = vcpu->vc_gueststate.vg_r11; break;
			case 12: r = vcpu->vc_gueststate.vg_r12; break;
			case 13: r = vcpu->vc_gueststate.vg_r13; break;
			case 14: r = vcpu->vc_gueststate.vg_r14; break;
			case 15: r = vcpu->vc_gueststate.vg_r15; break;
			}
		}

		if (crnum == 0)
			vmx_handle_cr0_write(vcpu, r);
			
		if (crnum == 4)
			vmx_handle_cr4_write(vcpu, r);

		break;
	case CR_READ:
		DPRINTF("vmx_handle_cr: mov from cr%d @@ %llx\n",
		    crnum, vcpu->vc_gueststate.vg_rip);
		break;
	case CR_CLTS:
		DPRINTF("vmx_handle_cr: clts instruction @@ %llx\n",
		    vcpu->vc_gueststate.vg_rip);
		break;
	case CR_LMSW:
		DPRINTF("vmx_handle_cr: lmsw instruction @@ %llx\n",
		    vcpu->vc_gueststate.vg_rip);
		break;
	default:
		DPRINTF("vmx_handle_cr: unknown cr access @@ %llx\n",
		    vcpu->vc_gueststate.vg_rip);
	}

	vcpu->vc_gueststate.vg_rip += insn_length;

	return (0);
}

/*
 * vmx_handle_rdmsr
 *
 * Handler for rdmsr instructions. Bitmap MSRs are allowed implicit access
 * and won't end up here. This handler is primarily intended to catch otherwise
 * unknown MSR access for possible later inclusion in the bitmap list. For
 * each MSR access that ends up here, we log the access (when VMM_DEBUG is
 * enabled)
 *
 * Parameters:
 *  vcpu: vcpu structure containing instruction info causing the exit
 *
 * Return value:
 *  0: The operation was successful
 *  EINVAL: An error occurred
 */
int
vmx_handle_rdmsr(struct vcpu *vcpu)
{
	uint64_t insn_length;
	uint64_t *rax, *rdx;
#ifdef VMM_DEBUG
	uint64_t *rcx;
#endif /* VMM_DEBUG */

	if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
		printf("%s: can't obtain instruction length\n", __func__);
		return (EINVAL);
	}

	/* All RDMSR instructions are 0x0F 0x32 */
	KASSERT(insn_length == 2);

	rax = &vcpu->vc_gueststate.vg_rax;
	rdx = &vcpu->vc_gueststate.vg_rdx;

	*rax = 0;
	*rdx = 0;

#ifdef VMM_DEBUG
	/* Log the access, to be able to identify unknown MSRs */
	rcx = &vcpu->vc_gueststate.vg_rcx;
	DPRINTF("%s: rdmsr exit, msr=0x%llx, data returned to "
	    "guest=0x%llx:0x%llx\n", __func__, *rcx, *rdx, *rax);
#endif /* VMM_DEBUG */

	vcpu->vc_gueststate.vg_rip += insn_length;

	return (0);
}

/*
 * vmx_handle_wrmsr
 *
 * Handler for wrmsr instructions. This handler logs the access, and discards
 * the written data (when VMM_DEBUG is enabled). Any valid wrmsr will not end
 * up here (it will be whitelisted in the MSR bitmap).
 *
 * Parameters:
 *  vcpu: vcpu structure containing instruction info causing the exit
 *
 * Return value:
 *  0: The operation was successful
 *  EINVAL: An error occurred
 */
int
vmx_handle_wrmsr(struct vcpu *vcpu)
{
	uint64_t insn_length;
	uint64_t *rax, *rdx;
#ifdef VMM_DEBUG
	uint64_t *rcx;
#endif /* VMM_DEBUG */

	if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
		printf("%s: can't obtain instruction length\n", __func__);
		return (EINVAL);
	}

	/* All WRMSR instructions are 0x0F 0x30 */
	KASSERT(insn_length == 2);

	rax = &vcpu->vc_gueststate.vg_rax;
	rdx = &vcpu->vc_gueststate.vg_rdx;

#ifdef VMM_DEBUG
	/* Log the access, to be able to identify unknown MSRs */
	rcx = &vcpu->vc_gueststate.vg_rcx;
	DPRINTF("%s: wrmsr exit, msr=0x%llx, discarding data written from "
	    "guest=0x%llx:0x%llx\n", __func__, *rcx, *rdx, *rax);
#endif /* VMM_DEBUG */

	vcpu->vc_gueststate.vg_rip += insn_length;

	return (0);
}

/*
 * vmm_handle_cpuid
 *
 * Exit handler for CPUID instruction
 *
 * Parameters:
 *  vcpu: vcpu causing the CPUID exit
 *
 * Return value:
 *  0: the exit was processed successfully
 *  EINVAL: error occurred validating the CPUID instruction arguments
 */
int
vmm_handle_cpuid(struct vcpu *vcpu)
{
	uint64_t insn_length;
	uint64_t *rax, *rbx, *rcx, *rdx;

	if (vmm_softc->mode == VMM_MODE_VMX ||
	    vmm_softc->mode == VMM_MODE_EPT) {
		if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
			DPRINTF("%s: can't obtain instruction length\n",
			    __func__);
			return (EINVAL);
		}

		/* All CPUID instructions are 0x0F 0xA2 */
		KASSERT(insn_length == 2);
	}

	rax = &vcpu->vc_gueststate.vg_rax;
	rbx = &vcpu->vc_gueststate.vg_rbx;
	rcx = &vcpu->vc_gueststate.vg_rcx;
	rdx = &vcpu->vc_gueststate.vg_rdx;

	switch (*rax) {
	case 0x00:	/* Max level and vendor ID */
		*rax = 0x07; /* cpuid_level */
		*rbx = *((uint32_t *)&cpu_vendor);
		*rdx = *((uint32_t *)&cpu_vendor + 1);
		*rcx = *((uint32_t *)&cpu_vendor + 2);
		break;
	case 0x01:	/* Version, brand, feature info */
		*rax = cpu_id;
		/* mask off host's APIC ID, reset to vcpu id */
		*rbx = cpu_ebxfeature & 0x0000FFFF;
		*rbx |= (vcpu->vc_id & 0xFF) << 24;
		/*
		 * clone host capabilities minus:
		 *  debug store (CPUIDECX_DTES64, CPUIDECX_DSCPL, CPUID_DS)
		 *  monitor/mwait (CPUIDECX_MWAIT)
		 *  vmx (CPUIDECX_VMX)
		 *  smx (CPUIDECX_SMX)
		 *  speedstep (CPUIDECX_EST)
		 *  thermal (CPUIDECX_TM2, CPUID_ACPI, CPUID_TM)
		 *  context id (CPUIDECX_CNXTID)
		 *  silicon debug (CPUIDECX_SDBG)
		 *  xTPR (CPUIDECX_XTPR)
		 *  AVX (CPUIDECX_AVX)
		 *  perf/debug (CPUIDECX_PDCM)
		 *  pcid (CPUIDECX_PCID)
		 *  direct cache access (CPUIDECX_DCA)
		 *  x2APIC (CPUIDECX_X2APIC)
		 *  apic deadline (CPUIDECX_DEADLINE)
		 *  timestamp (CPUID_TSC)
		 *  apic (CPUID_APIC)
		 *  psn (CPUID_PSN)
		 *  self snoop (CPUID_SS)
		 *  hyperthreading (CPUID_HTT)
		 *  pending break enabled (CPUID_PBE)
		 *  MTRR (CPUID_MTRR)
		 *  PAT (CPUID_PAT)
		 * plus:
		 *  hypervisor (CPUIDECX_HV)
		 */
		*rcx = (cpu_ecxfeature | CPUIDECX_HV) &
		    ~(CPUIDECX_EST | CPUIDECX_TM2 |
		    CPUIDECX_MWAIT | CPUIDECX_PDCM |
		    CPUIDECX_VMX | CPUIDECX_DTES64 |
		    CPUIDECX_DSCPL | CPUIDECX_SMX |
		    CPUIDECX_CNXTID | CPUIDECX_SDBG |
		    CPUIDECX_XTPR | CPUIDECX_AVX |
		    CPUIDECX_PCID | CPUIDECX_DCA |
		    CPUIDECX_X2APIC | CPUIDECX_DEADLINE);
		*rdx = curcpu()->ci_feature_flags &
		    ~(CPUID_ACPI | CPUID_TM | CPUID_TSC |
		      CPUID_HTT | CPUID_DS | CPUID_APIC |
		      CPUID_PSN | CPUID_SS | CPUID_PBE |
		      CPUID_MTRR | CPUID_PAT);
		break;
	case 0x02:	/* Cache and TLB information (not supported) */
		DPRINTF("%s: function 0x02 (cache/TLB) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x03:	/* Processor serial number (not supported) */
		DPRINTF("%s: function 0x03 (processor serial number) not "
		"supported\n", __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x04:
		DPRINTF("%s: function 0x04 (deterministic cache info) not "
		    "supported\n", __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x05:	/* MONITOR/MWAIT (not supported) */
		DPRINTF("%s: function 0x05 (monitor/mwait) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x06:	/* Thermal / Power management (not supported) */
		DPRINTF("%s: function 0x06 (thermal/power mgt) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x07:	/* SEFF */
		if (*rcx == 0) {
			/*
			 * SEFF flags - copy from host minus:
			 *  SGX (SEFF0EBX_SGX)
			 *  HLE (SEFF0EBX_HLE)
			 *  INVPCID (SEFF0EBX_INVPCID)
			 *  RTM (SEFF0EBX_RTM)
			 *  PQM (SEFF0EBX_PQM)
			 *  MPX (SEFF0EBX_MPX)
			 *  PCOMMIT (SEFF0EBX_PCOMMIT)
			 *  PT (SEFF0EBX_PT)
			 */
			*rax = 0;	/* Highest subleaf supported */
			*rbx = curcpu()->ci_feature_sefflags_ebx &
			    ~(SEFF0EBX_SGX | SEFF0EBX_HLE | SEFF0EBX_INVPCID |
			      SEFF0EBX_RTM | SEFF0EBX_PQM | SEFF0EBX_MPX |
			      SEFF0EBX_PCOMMIT | SEFF0EBX_PT);
			*rcx = curcpu()->ci_feature_sefflags_ecx;
			*rdx = 0;
		} else {
			/* Unsupported subleaf */
			DPRINTF("%s: function 0x07 (SEFF) unsupported subleaf "
			    "0x%llx not supported\n", __func__, *rcx);
			*rax = 0;
			*rbx = 0;
			*rcx = 0;
			*rdx = 0;
		}
		break;
	case 0x09:	/* Direct Cache Access (not supported) */
		DPRINTF("%s: function 0x09 (direct cache access) not "
		    "supported\n", __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x0a:	/* Architectural perf monitoring (not supported) */
		DPRINTF("%s: function 0x0a (arch. perf mon) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x0b:	/* Extended topology enumeration (not supported) */
		DPRINTF("%s: function 0x0b (topology enumeration) not "
		    "supported\n", __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x0d:	/* Processor ext. state information (not supported) */
		DPRINTF("%s: function 0x0d (ext. state info) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x0f:	/* QoS info (not supported) */
		DPRINTF("%s: function 0x0f (QoS info) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x14:	/* Processor Trace info (not supported) */
		DPRINTF("%s: function 0x14 (processor trace info) not "
		    "supported\n", __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x15:	/* TSC / Core Crystal Clock info (not supported) */
		DPRINTF("%s: function 0x15 (TSC / CCC info) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x16:	/* Processor frequency info (not supported) */
		DPRINTF("%s: function 0x16 (frequency info) not supported\n",
		    __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x40000000:	/* Hypervisor information */
		*rax = 0;
		*rbx = *((uint32_t *)&vmm_hv_signature[0]);
		*rcx = *((uint32_t *)&vmm_hv_signature[4]);
		*rdx = *((uint32_t *)&vmm_hv_signature[8]);
		break;
	case 0x80000000:	/* Extended function level */
		*rax = 0x80000007; /* curcpu()->ci_pnfeatset */
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	case 0x80000001: 	/* Extended function info */
		*rax = curcpu()->ci_efeature_eax;
		*rbx = 0;	/* Reserved */
		*rcx = curcpu()->ci_efeature_ecx;
		*rdx = curcpu()->ci_feature_eflags;
		break;
	case 0x80000002:	/* Brand string */
		*rax = curcpu()->ci_brand[0];
		*rbx = curcpu()->ci_brand[1];
		*rcx = curcpu()->ci_brand[2];
		*rdx = curcpu()->ci_brand[3];
		break;
	case 0x80000003:	/* Brand string */
		*rax = curcpu()->ci_brand[4];
		*rbx = curcpu()->ci_brand[5];
		*rcx = curcpu()->ci_brand[6];
		*rdx = curcpu()->ci_brand[7];
		break;
	case 0x80000004:	/* Brand string */
		*rax = curcpu()->ci_brand[8];
		*rbx = curcpu()->ci_brand[9];
		*rcx = curcpu()->ci_brand[10];
		*rdx = curcpu()->ci_brand[11];
		break;
	case 0x80000005:	/* Reserved (Intel), cacheinfo (AMD) */
		*rax = curcpu()->ci_amdcacheinfo[0];
		*rbx = curcpu()->ci_amdcacheinfo[1];
		*rcx = curcpu()->ci_amdcacheinfo[2];
		*rdx = curcpu()->ci_amdcacheinfo[3];
		break;
	case 0x80000006:	/* ext. cache info */
		*rax = curcpu()->ci_extcacheinfo[0];
		*rbx = curcpu()->ci_extcacheinfo[1];
		*rcx = curcpu()->ci_extcacheinfo[2];
		*rdx = curcpu()->ci_extcacheinfo[3];
		break;
	case 0x80000007:	/* apmi */
		*rax = 0;	/* Reserved */
		*rbx = 0;	/* Reserved */
		*rcx = 0;	/* Reserved */
		*rdx = 0;	/* unsupported ITSC */
		break;
	case 0x80000008:	/* Phys bits info and topology (AMD) */
		DPRINTF("%s: function 0x80000008 (phys bits info) not "
		    "supported\n", __func__);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
		break;
	default:
		DPRINTF("%s: unsupported rax=0x%llx\n", __func__, *rax);
		*rax = 0;
		*rbx = 0;
		*rcx = 0;
		*rdx = 0;
	}

	vcpu->vc_gueststate.vg_rip += insn_length;

	return (0);
}

/*
 * vcpu_run_svm
 *
 * VMM main loop used to run a VCPU.
 */
int
vcpu_run_svm(struct vcpu *vcpu, struct vm_run_params *vrp)
{
	/* XXX removed due to rot */
	return (0);
}

/*
 * vmx_exit_reason_decode
 *
 * Returns a human readable string describing exit type 'code'
 */
const char *
vmx_exit_reason_decode(uint32_t code)
{
	switch (code) {
	case VMX_EXIT_NMI: return "NMI";
	case VMX_EXIT_EXTINT: return "external interrupt";
	case VMX_EXIT_TRIPLE_FAULT: return "triple fault";
	case VMX_EXIT_INIT: return "INIT signal";
	case VMX_EXIT_SIPI: return "SIPI signal";
	case VMX_EXIT_IO_SMI: return "I/O SMI";
	case VMX_EXIT_OTHER_SMI: return "other SMI";
	case VMX_EXIT_INT_WINDOW: return "interrupt window";
	case VMX_EXIT_NMI_WINDOW: return "NMI window";
	case VMX_EXIT_TASK_SWITCH: return "task switch";
	case VMX_EXIT_CPUID: return "CPUID instruction";
	case VMX_EXIT_GETSEC: return "GETSEC instruction";
	case VMX_EXIT_HLT: return "HLT instruction";
	case VMX_EXIT_INVD: return "INVD instruction";
	case VMX_EXIT_INVLPG: return "INVLPG instruction";
	case VMX_EXIT_RDPMC: return "RDPMC instruction";
	case VMX_EXIT_RDTSC: return "RDTSC instruction";
	case VMX_EXIT_RSM: return "RSM instruction";
	case VMX_EXIT_VMCALL: return "VMCALL instruction";
	case VMX_EXIT_VMCLEAR: return "VMCLEAR instruction";
	case VMX_EXIT_VMLAUNCH: return "VMLAUNCH instruction";
	case VMX_EXIT_VMPTRLD: return "VMPTRLD instruction";
	case VMX_EXIT_VMPTRST: return "VMPTRST instruction";
	case VMX_EXIT_VMREAD: return "VMREAD instruction";
	case VMX_EXIT_VMRESUME: return "VMRESUME instruction";
	case VMX_EXIT_VMWRITE: return "VMWRITE instruction";
	case VMX_EXIT_VMXOFF: return "VMXOFF instruction";
	case VMX_EXIT_VMXON: return "VMXON instruction";
	case VMX_EXIT_CR_ACCESS: return "CR access";
	case VMX_EXIT_MOV_DR: return "MOV DR instruction";
	case VMX_EXIT_IO: return "I/O instruction";
	case VMX_EXIT_RDMSR: return "RDMSR instruction";
	case VMX_EXIT_WRMSR: return "WRMSR instruction";
	case VMX_EXIT_ENTRY_FAILED_GUEST_STATE: return "guest state invalid";
	case VMX_EXIT_ENTRY_FAILED_MSR_LOAD: return "MSR load failed";
	case VMX_EXIT_MWAIT: return "MWAIT instruction";
	case VMX_EXIT_MTF: return "monitor trap flag";
	case VMX_EXIT_MONITOR: return "MONITOR instruction";
	case VMX_EXIT_PAUSE: return "PAUSE instruction";
	case VMX_EXIT_ENTRY_FAILED_MCE: return "MCE during entry";
	case VMX_EXIT_TPR_BELOW_THRESHOLD: return "TPR below threshold";
	case VMX_EXIT_APIC_ACCESS: return "APIC access";
	case VMX_EXIT_VIRTUALIZED_EOI: return "virtualized EOI";
	case VMX_EXIT_GDTR_IDTR: return "GDTR/IDTR access";
	case VMX_EXIT_LDTR_TR: return "LDTR/TR access";
	case VMX_EXIT_EPT_VIOLATION: return "EPT violation";
	case VMX_EXIT_EPT_MISCONFIGURATION: return "EPT misconfiguration";
	case VMX_EXIT_INVEPT: return "INVEPT instruction";
	case VMX_EXIT_RDTSCP: return "RDTSCP instruction";
	case VMX_EXIT_VMX_PREEMPTION_TIMER_EXPIRED:
	    return "preemption timer expired";
	case VMX_EXIT_INVVPID: return "INVVPID instruction";
	case VMX_EXIT_WBINVD: return "WBINVD instruction";
	case VMX_EXIT_XSETBV: return "XSETBV instruction";
	case VMX_EXIT_APIC_WRITE: return "APIC write";
	case VMX_EXIT_RDRAND: return "RDRAND instruction";
	case VMX_EXIT_INVPCID: return "INVPCID instruction";
	case VMX_EXIT_VMFUNC: return "VMFUNC instruction";
	case VMX_EXIT_RDSEED: return "RDSEED instruction";
	case VMX_EXIT_XSAVES: return "XSAVES instruction";
	case VMX_EXIT_XRSTORS: return "XRSTORS instruction";
	default: return "unknown";
	}
}

/*
 * vmx_instruction_error_decode
 *
 * Returns a human readable string describing the instruction error in 'code'
 */
const char *
vmx_instruction_error_decode(uint32_t code)
{
	switch (code) {
	case 1: return "VMCALL: unsupported in VMX root";
	case 2: return "VMCLEAR: invalid paddr";
	case 3: return "VMCLEAR: VMXON pointer";
	case 4: return "VMLAUNCH: non-clear VMCS";
	case 5: return "VMRESUME: non-launched VMCS";
	case 6: return "VMRESUME: executed after VMXOFF";
	case 7: return "VM entry: invalid control field(s)";
	case 8: return "VM entry: invalid host state field(s)";
	case 9: return "VMPTRLD: invalid paddr";
	case 10: return "VMPTRLD: VMXON pointer";
	case 11: return "VMPTRLD: incorrect VMCS revid";
	case 12: return "VMREAD/VMWRITE: unsupported VMCS field";
	case 13: return "VMWRITE: RO VMCS field";
	case 15: return "VMXON: unsupported in VMX root";
	case 20: return "VMCALL: invalid VM exit control fields";
	case 26: return "VM entry: blocked by MOV SS";
	case 28: return "Invalid operand to INVEPT/INVVPID";
	default: return "unknown";
	}
}

/*
 * vcpu_state_decode
 *
 * Returns a human readable string describing the vcpu state in 'state'.
 */
const char *
vcpu_state_decode(u_int state)
{
	switch (state) {
	case VCPU_STATE_STOPPED: return "stopped";
	case VCPU_STATE_RUNNING: return "running";
	case VCPU_STATE_REQTERM: return "requesting termination";
	case VCPU_STATE_TERMINATED: return "terminated";
	case VCPU_STATE_UNKNOWN: return "unknown";
	default: return "invalid";
	}
}

#ifdef VMM_DEBUG
/*
 * dump_vcpu
 *
 * Dumps the VMX capabilites of vcpu 'vcpu'
 */
void
dump_vcpu(struct vcpu *vcpu)
{
	printf("vcpu @@ %p\n", vcpu);
	printf("    parent vm @@ %p\n", vcpu->vc_parent);
	printf("    mode: ");
	if (vcpu->vc_virt_mode == VMM_MODE_VMX ||
	    vcpu->vc_virt_mode == VMM_MODE_EPT) {
		printf("VMX\n");
		printf("    pinbased ctls: 0x%llx\n",
		    vcpu->vc_vmx_pinbased_ctls);
		printf("    true pinbased ctls: 0x%llx\n",
		    vcpu->vc_vmx_true_pinbased_ctls);
		CTRL_DUMP(vcpu, PINBASED, EXTERNAL_INT_EXITING);
		CTRL_DUMP(vcpu, PINBASED, NMI_EXITING);
		CTRL_DUMP(vcpu, PINBASED, VIRTUAL_NMIS);
		CTRL_DUMP(vcpu, PINBASED, ACTIVATE_VMX_PREEMPTION_TIMER);
		CTRL_DUMP(vcpu, PINBASED, PROCESS_POSTED_INTERRUPTS);
		printf("    procbased ctls: 0x%llx\n",
		    vcpu->vc_vmx_procbased_ctls);
		printf("    true procbased ctls: 0x%llx\n",
		    vcpu->vc_vmx_true_procbased_ctls);
		CTRL_DUMP(vcpu, PROCBASED, INTERRUPT_WINDOW_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, USE_TSC_OFFSETTING);
		CTRL_DUMP(vcpu, PROCBASED, HLT_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, INVLPG_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, MWAIT_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, RDPMC_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, RDTSC_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, CR3_LOAD_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, CR3_STORE_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, CR8_LOAD_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, CR8_STORE_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, USE_TPR_SHADOW);
		CTRL_DUMP(vcpu, PROCBASED, NMI_WINDOW_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, MOV_DR_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, UNCONDITIONAL_IO_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, USE_IO_BITMAPS);
		CTRL_DUMP(vcpu, PROCBASED, MONITOR_TRAP_FLAG);
		CTRL_DUMP(vcpu, PROCBASED, USE_MSR_BITMAPS);
		CTRL_DUMP(vcpu, PROCBASED, MONITOR_EXITING);
		CTRL_DUMP(vcpu, PROCBASED, PAUSE_EXITING);
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
		    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
			printf("    procbased2 ctls: 0x%llx\n",
			    vcpu->vc_vmx_procbased2_ctls);
			CTRL_DUMP(vcpu, PROCBASED2, VIRTUALIZE_APIC);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_EPT);
			CTRL_DUMP(vcpu, PROCBASED2, DESCRIPTOR_TABLE_EXITING);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_RDTSCP);
			CTRL_DUMP(vcpu, PROCBASED2, VIRTUALIZE_X2APIC_MODE);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_VPID);
			CTRL_DUMP(vcpu, PROCBASED2, WBINVD_EXITING);
			CTRL_DUMP(vcpu, PROCBASED2, UNRESTRICTED_GUEST);
			CTRL_DUMP(vcpu, PROCBASED2,
			    APIC_REGISTER_VIRTUALIZATION);
			CTRL_DUMP(vcpu, PROCBASED2,
			    VIRTUAL_INTERRUPT_DELIVERY);
			CTRL_DUMP(vcpu, PROCBASED2, PAUSE_LOOP_EXITING);
			CTRL_DUMP(vcpu, PROCBASED2, RDRAND_EXITING);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_INVPCID);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_VM_FUNCTIONS);
			CTRL_DUMP(vcpu, PROCBASED2, VMCS_SHADOWING);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_ENCLS_EXITING);
			CTRL_DUMP(vcpu, PROCBASED2, RDSEED_EXITING);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_PML);
			CTRL_DUMP(vcpu, PROCBASED2, EPT_VIOLATION_VE);
			CTRL_DUMP(vcpu, PROCBASED2, CONCEAL_VMX_FROM_PT);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_XSAVES_XRSTORS);
			CTRL_DUMP(vcpu, PROCBASED2, ENABLE_TSC_SCALING);
		}
		printf("    entry ctls: 0x%llx\n",
		    vcpu->vc_vmx_entry_ctls);
		printf("    true entry ctls: 0x%llx\n",
		    vcpu->vc_vmx_true_entry_ctls);
		CTRL_DUMP(vcpu, ENTRY, LOAD_DEBUG_CONTROLS);
		CTRL_DUMP(vcpu, ENTRY, IA32E_MODE_GUEST);
		CTRL_DUMP(vcpu, ENTRY, ENTRY_TO_SMM);
		CTRL_DUMP(vcpu, ENTRY, DEACTIVATE_DUAL_MONITOR_TREATMENT);
		CTRL_DUMP(vcpu, ENTRY, LOAD_IA32_PERF_GLOBAL_CTRL_ON_ENTRY);
		CTRL_DUMP(vcpu, ENTRY, LOAD_IA32_PAT_ON_ENTRY);
		CTRL_DUMP(vcpu, ENTRY, LOAD_IA32_EFER_ON_ENTRY);
		CTRL_DUMP(vcpu, ENTRY, LOAD_IA32_BNDCFGS_ON_ENTRY);
		CTRL_DUMP(vcpu, ENTRY, CONCEAL_VM_ENTRIES_FROM_PT);
		printf("    exit ctls: 0x%llx\n",
		    vcpu->vc_vmx_exit_ctls);
		printf("    true exit ctls: 0x%llx\n",
		    vcpu->vc_vmx_true_exit_ctls);
		CTRL_DUMP(vcpu, EXIT, SAVE_DEBUG_CONTROLS);
		CTRL_DUMP(vcpu, EXIT, HOST_SPACE_ADDRESS_SIZE);
		CTRL_DUMP(vcpu, EXIT, LOAD_IA32_PERF_GLOBAL_CTRL_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, ACKNOWLEDGE_INTERRUPT_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, SAVE_IA32_PAT_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, LOAD_IA32_PAT_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, SAVE_IA32_EFER_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, LOAD_IA32_EFER_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, SAVE_VMX_PREEMPTION_TIMER);
		CTRL_DUMP(vcpu, EXIT, CLEAR_IA32_BNDCFGS_ON_EXIT);
		CTRL_DUMP(vcpu, EXIT, CONCEAL_VM_EXITS_FROM_PT);
	}
}

/*
 * vmx_dump_vmcs_field
 *
 * Debug function to dump the contents of a single VMCS field
 *
 * Parameters:
 *  fieldid: VMCS Field ID
 *  msg: string to display
 */
void
vmx_dump_vmcs_field(uint16_t fieldid, const char *msg)
{
	uint8_t width;
	uint64_t val;


	DPRINTF("%s (0x%04x): ", msg, fieldid);
	if (vmread(fieldid, &val))
		DPRINTF("???? ");
	else {
		/*
		 * Field width encoding : bits 13:14
		 *
		 * 0: 16-bit
		 * 1: 64-bit
		 * 2: 32-bit
		 * 3: natural width
		 */
		width = (fieldid >> 13) & 0x3;
		switch (width) {
			case 0: DPRINTF("0x%04llx ", val); break;
			case 1:
			case 3: DPRINTF("0x%016llx ", val); break;
			case 2: DPRINTF("0x%08llx ", val);
		}
	}
}

/*
 * vmx_dump_vmcs
 *
 * Debug function to dump the contents of the current VMCS.
 */
void
vmx_dump_vmcs(struct vcpu *vcpu)
{
	int has_sec, i;
	uint32_t cr3_tgt_ct;

	/* XXX save and load new vmcs, restore at end */

	DPRINTF("--CURRENT VMCS STATE--\n");
	DPRINTF("VMXON revision : 0x%x\n",
	    curcpu()->ci_vmm_cap.vcc_vmx.vmx_vmxon_revision);
	DPRINTF("CR0 fixed0: 0x%llx\n",
	    curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed0);
	DPRINTF("CR0 fixed1: 0x%llx\n",
	    curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed1);
	DPRINTF("CR4 fixed0: 0x%llx\n",
	    curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed0);
	DPRINTF("CR4 fixed1: 0x%llx\n",
	    curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed1);
	DPRINTF("MSR table size: 0x%x\n",
	    512 * (curcpu()->ci_vmm_cap.vcc_vmx.vmx_msr_table_size + 1));
	
	has_sec = vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1);
	
	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VPID, 1)) {
			vmx_dump_vmcs_field(VMCS_GUEST_VPID, "VPID");
		}
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PINBASED_CTLS,
	    IA32_VMX_PROCESS_POSTED_INTERRUPTS, 1)) {
		vmx_dump_vmcs_field(VMCS_POSTED_INT_NOTIF_VECTOR,
		    "Posted Int Notif Vec");
	}

	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_EPT_VIOLATION_VE, 1)) {
			vmx_dump_vmcs_field(VMCS_EPTP_INDEX, "EPTP idx");
		}
	}

	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_ES_SEL, "G.ES");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CS_SEL, "G.CS");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SS_SEL, "G.SS");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_DS_SEL, "G.DS");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_FS_SEL, "G.FS");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_GS_SEL, "G.GS");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_LDTR_SEL, "LDTR");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_TR_SEL, "G.TR");

	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_VIRTUAL_INTERRUPT_DELIVERY, 1)) {
			vmx_dump_vmcs_field(VMCS_GUEST_INTERRUPT_STATUS,
			    "Int sts");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_PML, 1)) {
			vmx_dump_vmcs_field(VMCS_GUEST_PML_INDEX, "PML Idx");
		}
	}

	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_ES_SEL, "H.ES");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_CS_SEL, "H.CS");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_SS_SEL, "H.SS");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_DS_SEL, "H.DS");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_FS_SEL, "H.FS");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_GS_SEL, "H.GS");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_IO_BITMAP_A, "I/O Bitmap A");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_IO_BITMAP_B, "I/O Bitmap B");
	DPRINTF("\n");

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_USE_MSR_BITMAPS, 1)) {
		vmx_dump_vmcs_field(VMCS_MSR_BITMAP_ADDRESS, "MSR Bitmap");
		DPRINTF("\n");
	}

	vmx_dump_vmcs_field(VMCS_EXIT_STORE_MSR_ADDRESS, "Exit Store MSRs");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_EXIT_LOAD_MSR_ADDRESS, "Exit Load MSRs");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_ENTRY_LOAD_MSR_ADDRESS, "Entry Load MSRs");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_EXECUTIVE_VMCS_POINTER, "Exec VMCS Ptr");
	DPRINTF("\n");

	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_PML, 1)) {
			vmx_dump_vmcs_field(VMCS_PML_ADDRESS, "PML Addr");
			DPRINTF("\n");
		}
	}

	vmx_dump_vmcs_field(VMCS_TSC_OFFSET, "TSC Offset");
	DPRINTF("\n");

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_USE_TPR_SHADOW, 1)) {
		vmx_dump_vmcs_field(VMCS_VIRTUAL_APIC_ADDRESS,
		    "Virtual APIC Addr");
		DPRINTF("\n");
	}

	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_VIRTUALIZE_APIC, 1)) {
			vmx_dump_vmcs_field(VMCS_APIC_ACCESS_ADDRESS,
			    "APIC Access Addr");
			DPRINTF("\n");
		}
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PINBASED_CTLS,
	    IA32_VMX_PROCESS_POSTED_INTERRUPTS, 1)) {
		vmx_dump_vmcs_field(VMCS_POSTED_INTERRUPT_DESC,
		    "Posted Int Desc Addr");
		DPRINTF("\n");
	}

	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VM_FUNCTIONS, 1)) {
			vmx_dump_vmcs_field(VMCS_VM_FUNCTION_CONTROLS,
			    "VM Function Controls");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_EPT, 1)) {
			vmx_dump_vmcs_field(VMCS_GUEST_IA32_EPTP,
			    "EPT Pointer");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_VIRTUAL_INTERRUPT_DELIVERY, 1)) {
			vmx_dump_vmcs_field(VMCS_EOI_EXIT_BITMAP_0,
			    "EOI Exit Bitmap 0");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_EOI_EXIT_BITMAP_1,
			    "EOI Exit Bitmap 1");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_EOI_EXIT_BITMAP_2,
			    "EOI Exit Bitmap 2");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_EOI_EXIT_BITMAP_3,
			    "EOI Exit Bitmap 3");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VM_FUNCTIONS, 1)) {
			/* We assume all CPUs have the same VMFUNC caps */
			if (curcpu()->ci_vmm_cap.vcc_vmx.vmx_vm_func & 0x1) {
				vmx_dump_vmcs_field(VMCS_EPTP_LIST_ADDRESS,
				    "EPTP List Addr");
				DPRINTF("\n");
			}
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_VMCS_SHADOWING, 1)) {
			vmx_dump_vmcs_field(VMCS_VMREAD_BITMAP_ADDRESS,
			    "VMREAD Bitmap Addr");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_VMWRITE_BITMAP_ADDRESS,
			    "VMWRITE Bitmap Addr");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_EPT_VIOLATION_VE, 1)) {
			vmx_dump_vmcs_field(VMCS_VIRTUALIZATION_EXC_ADDRESS,
			    "#VE Addr");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_XSAVES_XRSTORS, 1)) {
			vmx_dump_vmcs_field(VMCS_XSS_EXITING_BITMAP,
			    "XSS exiting bitmap addr");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_ENCLS_EXITING, 1)) {
			vmx_dump_vmcs_field(VMCS_ENCLS_EXITING_BITMAP,
			    "Encls exiting bitmap addr");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_TSC_SCALING, 1)) {
			vmx_dump_vmcs_field(VMCS_TSC_MULTIPLIER,
			    "TSC scaling factor");
			DPRINTF("\n");
		}

		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_EPT, 1)) {
			vmx_dump_vmcs_field(VMCS_GUEST_PHYSICAL_ADDRESS,
			    "Guest PA");
			DPRINTF("\n");
		}
	}

	vmx_dump_vmcs_field(VMCS_LINK_POINTER, "VMCS Link Pointer");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_DEBUGCTL, "Guest DEBUGCTL");
	DPRINTF("\n");

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_ENTRY_CTLS,
	    IA32_VMX_LOAD_IA32_PAT_ON_ENTRY, 1) ||
	    vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_SAVE_IA32_PAT_ON_EXIT, 1)) {
		vmx_dump_vmcs_field(VMCS_GUEST_IA32_PAT,
		    "Guest PAT");
		DPRINTF("\n");
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_ENTRY_CTLS,
	    IA32_VMX_LOAD_IA32_EFER_ON_ENTRY, 1) ||
	    vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_SAVE_IA32_EFER_ON_EXIT, 1)) {
		vmx_dump_vmcs_field(VMCS_GUEST_IA32_EFER,
		    "Guest EFER");
		DPRINTF("\n");
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_ENTRY_CTLS,
	    IA32_VMX_LOAD_IA32_PERF_GLOBAL_CTRL_ON_ENTRY, 1)) {
		vmx_dump_vmcs_field(VMCS_GUEST_IA32_PERF_GBL_CTRL,
		    "Guest Perf Global Ctrl");
		DPRINTF("\n");
	}

	if (has_sec) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_EPT, 1)) {
			vmx_dump_vmcs_field(VMCS_GUEST_PDPTE0, "Guest PDPTE0");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_GUEST_PDPTE1, "Guest PDPTE1");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_GUEST_PDPTE2, "Guest PDPTE2");
			DPRINTF("\n");
			vmx_dump_vmcs_field(VMCS_GUEST_PDPTE3, "Guest PDPTE3");
			DPRINTF("\n");
		}
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_ENTRY_CTLS,
	    IA32_VMX_LOAD_IA32_BNDCFGS_ON_ENTRY, 1) ||
	    vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_CLEAR_IA32_BNDCFGS_ON_EXIT, 1)) {
		vmx_dump_vmcs_field(VMCS_GUEST_IA32_BNDCFGS,
		    "Guest BNDCFGS");
		DPRINTF("\n");
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_LOAD_IA32_PAT_ON_EXIT, 1)) {
		vmx_dump_vmcs_field(VMCS_HOST_IA32_PAT,
		    "Host PAT");
		DPRINTF("\n");
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_LOAD_IA32_EFER_ON_EXIT, 1)) {
		vmx_dump_vmcs_field(VMCS_HOST_IA32_EFER,
		    "Host EFER");
		DPRINTF("\n");
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_LOAD_IA32_PERF_GLOBAL_CTRL_ON_EXIT, 1)) {
		vmx_dump_vmcs_field(VMCS_HOST_IA32_PERF_GBL_CTRL,
		    "Host Perf Global Ctrl");
		DPRINTF("\n");
	}

	vmx_dump_vmcs_field(VMCS_PINBASED_CTLS, "Pinbased Ctrls");
	vmx_dump_vmcs_field(VMCS_PROCBASED_CTLS, "Procbased Ctrls");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_EXCEPTION_BITMAP, "Exception Bitmap");
	vmx_dump_vmcs_field(VMCS_PF_ERROR_CODE_MASK, "#PF Err Code Mask");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_PF_ERROR_CODE_MATCH, "#PF Err Code Match");
	vmx_dump_vmcs_field(VMCS_CR3_TARGET_COUNT, "CR3 Tgt Count");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_EXIT_CTLS, "Exit Ctrls");
	vmx_dump_vmcs_field(VMCS_EXIT_MSR_STORE_COUNT, "Exit MSR Store Ct");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_EXIT_MSR_LOAD_COUNT, "Exit MSR Load Ct");
	vmx_dump_vmcs_field(VMCS_ENTRY_CTLS, "Entry Ctrls");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_ENTRY_MSR_LOAD_COUNT, "Entry MSR Load Ct");
	vmx_dump_vmcs_field(VMCS_ENTRY_INTERRUPTION_INFO, "Entry Int. Info");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_ENTRY_EXCEPTION_ERROR_CODE,
	    "Entry Ex. Err Code");
	vmx_dump_vmcs_field(VMCS_ENTRY_INSTRUCTION_LENGTH, "Entry Insn Len");
	DPRINTF("\n");

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_USE_TPR_SHADOW, 1)) {
		vmx_dump_vmcs_field(VMCS_TPR_THRESHOLD, "TPR Threshold");
		DPRINTF("\n");
	}

	if (has_sec) {
		vmx_dump_vmcs_field(VMCS_PROCBASED2_CTLS, "2ndary Ctrls");
		DPRINTF("\n");
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_PAUSE_LOOP_EXITING, 1)) {
			vmx_dump_vmcs_field(VMCS_PLE_GAP, "PLE Gap");
			vmx_dump_vmcs_field(VMCS_PLE_WINDOW, "PLE Window");
		}
		DPRINTF("\n");
	}

	vmx_dump_vmcs_field(VMCS_INSTRUCTION_ERROR, "Insn Error");
	vmx_dump_vmcs_field(VMCS_EXIT_REASON, "Exit Reason");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_EXIT_INTERRUPTION_INFO, "Exit Int. Info");
	vmx_dump_vmcs_field(VMCS_EXIT_INTERRUPTION_ERR_CODE,
	    "Exit Int. Err Code");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_IDT_VECTORING_INFO, "IDT vect info");
	vmx_dump_vmcs_field(VMCS_IDT_VECTORING_ERROR_CODE,
	    "IDT vect err code");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_INSTRUCTION_LENGTH, "Insn Len");
	vmx_dump_vmcs_field(VMCS_EXIT_INSTRUCTION_INFO, "Exit Insn Info");
	DPRINTF("\n");
	
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_ES_LIMIT, "G. ES Lim");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CS_LIMIT, "G. CS Lim");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SS_LIMIT, "G. SS Lim");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_DS_LIMIT, "G. DS Lim");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_FS_LIMIT, "G. FS Lim");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_GS_LIMIT, "G. GS Lim");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_LDTR_LIMIT, "G. LDTR Lim");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_TR_LIMIT, "G. TR Lim");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_GDTR_LIMIT, "G. GDTR Lim");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_IDTR_LIMIT, "G. IDTR Lim");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_ES_AR, "G. ES AR");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CS_AR, "G. CS AR");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SS_AR, "G. SS AR");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_DS_AR, "G. DS AR");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_FS_AR, "G. FS AR");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_GS_AR, "G. GS AR");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_IA32_LDTR_AR, "G. LDTR AR");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_TR_AR, "G. TR AR");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_INTERRUPTIBILITY_ST, "G. Int St.");
	vmx_dump_vmcs_field(VMCS_GUEST_ACTIVITY_STATE, "G. Act St.");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_GUEST_SMBASE, "G. SMBASE");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SYSENTER_CS, "G. SYSENTER CS");
	DPRINTF("\n");

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PINBASED_CTLS,
	    IA32_VMX_ACTIVATE_VMX_PREEMPTION_TIMER, 1)) {
		vmx_dump_vmcs_field(VMCS_VMX_PREEMPTION_TIMER_VAL,
		    "VMX Preempt Timer");
		DPRINTF("\n");
	}

	vmx_dump_vmcs_field(VMCS_HOST_IA32_SYSENTER_CS, "H. SYSENTER CS");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_CR0_MASK, "CR0 Mask");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_CR4_MASK, "CR4 Mask");
	DPRINTF("\n");

	vmx_dump_vmcs_field(VMCS_CR0_READ_SHADOW, "CR0 RD Shadow");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_CR4_READ_SHADOW, "CR4 RD Shadow");
	DPRINTF("\n");

	/* We assume all CPUs have the same max CR3 target ct */
	cr3_tgt_ct = curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr3_tgt_count;
	DPRINTF("Max CR3 target count: 0x%x\n", cr3_tgt_ct);
	if (cr3_tgt_ct <= VMX_MAX_CR3_TARGETS) {
		for (i = 0 ; i < cr3_tgt_ct; i++) {
			vmx_dump_vmcs_field(VMCS_CR3_TARGET_0 + (2 * i),
			    "CR3 Target");
			DPRINTF("\n");
		}
	} else {
		DPRINTF("(Bogus CR3 Target Count > %d", VMX_MAX_CR3_TARGETS);
	}

	vmx_dump_vmcs_field(VMCS_GUEST_EXIT_QUALIFICATION, "G. Exit Qual");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_IO_RCX, "I/O RCX");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_IO_RSI, "I/O RSI");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_IO_RDI, "I/O RDI");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_IO_RIP, "I/O RIP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_LINEAR_ADDRESS, "G. Lin Addr");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CR0, "G. CR0");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CR3, "G. CR3");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CR4, "G. CR4");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_ES_BASE, "G. ES Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_CS_BASE, "G. CS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SS_BASE, "G. SS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_DS_BASE, "G. DS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_FS_BASE, "G. FS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_GS_BASE, "G. GS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_LDTR_BASE, "G. LDTR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_TR_BASE, "G. TR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_GDTR_BASE, "G. GDTR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_IDTR_BASE, "G. IDTR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_DR7, "G. DR7");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_RSP, "G. RSP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_RIP, "G. RIP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_RFLAGS, "G. RFLAGS");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_PENDING_DBG_EXC, "G. Pend Dbg Exc");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SYSENTER_ESP, "G. SYSENTER ESP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_GUEST_IA32_SYSENTER_EIP, "G. SYSENTER EIP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_CR0, "H. CR0");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_CR3, "H. CR3");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_CR4, "H. CR4");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_FS_BASE, "H. FS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_GS_BASE, "H. GS Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_TR_BASE, "H. TR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_GDTR_BASE, "H. GDTR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_IDTR_BASE, "H. IDTR Base");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_SYSENTER_ESP, "H. SYSENTER ESP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_SYSENTER_EIP, "H. SYSENTER EIP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_RSP, "H. RSP");
	DPRINTF("\n");
	vmx_dump_vmcs_field(VMCS_HOST_IA32_RIP, "H. RIP");
	DPRINTF("\n");
}

/*
 * vmx_vcpu_dump_regs
 *
 * Debug function to print vcpu regs from the current vcpu
 *  note - vmcs for 'vcpu' must be on this pcpu.
 *
 * Parameters:
 *  vcpu - vcpu whose registers should be dumped
 */
void
vmx_vcpu_dump_regs(struct vcpu *vcpu)
{
	uint64_t r;
	int i;
	struct vmx_msr_store *msr_store;

	/* XXX reformat this for 32 bit guest as needed */
	DPRINTF("vcpu @@ %p\n", vcpu);
	DPRINTF(" rax=0x%016llx rbx=0x%016llx rcx=0x%016llx\n",
	    vcpu->vc_gueststate.vg_rax, vcpu->vc_gueststate.vg_rbx,
	    vcpu->vc_gueststate.vg_rcx);
	DPRINTF(" rdx=0x%016llx rbp=0x%016llx rdi=0x%016llx\n",
	    vcpu->vc_gueststate.vg_rdx, vcpu->vc_gueststate.vg_rbp,
	    vcpu->vc_gueststate.vg_rdi);
	DPRINTF(" rsi=0x%016llx  r8=0x%016llx  r9=0x%016llx\n",
	    vcpu->vc_gueststate.vg_rsi, vcpu->vc_gueststate.vg_r8,
	    vcpu->vc_gueststate.vg_r9);
	DPRINTF(" r10=0x%016llx r11=0x%016llx r12=0x%016llx\n",
	    vcpu->vc_gueststate.vg_r10, vcpu->vc_gueststate.vg_r11,
	    vcpu->vc_gueststate.vg_r12);
	DPRINTF(" r13=0x%016llx r14=0x%016llx r15=0x%016llx\n",
	    vcpu->vc_gueststate.vg_r13, vcpu->vc_gueststate.vg_r14,
	    vcpu->vc_gueststate.vg_r15);

	DPRINTF(" rip=0x%016llx rsp=", vcpu->vc_gueststate.vg_rip);
	if (vmread(VMCS_GUEST_IA32_RSP, &r))
		DPRINTF("(error reading)\n");
	else
		DPRINTF("0x%016llx\n", r);

	DPRINTF(" cr0=");
	if (vmread(VMCS_GUEST_IA32_CR0, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%016llx ", r);
		vmm_decode_cr0(r);
	}

	DPRINTF(" cr2=0x%016llx\n", vcpu->vc_gueststate.vg_cr2);

	DPRINTF(" cr3=");
	if (vmread(VMCS_GUEST_IA32_CR3, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%016llx ", r);
		vmm_decode_cr3(r);
	}

	DPRINTF(" cr4=");
	if (vmread(VMCS_GUEST_IA32_CR4, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%016llx ", r);
		vmm_decode_cr4(r);
	}

	DPRINTF(" --Guest Segment Info--\n");

	DPRINTF(" cs=");
	if (vmread(VMCS_GUEST_IA32_CS_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx rpl=%lld", r, r & 0x3);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_CS_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_CS_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_CS_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	

	DPRINTF(" ds=");
	if (vmread(VMCS_GUEST_IA32_DS_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx rpl=%lld", r, r & 0x3);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_DS_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_DS_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_DS_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	

	DPRINTF(" es=");
	if (vmread(VMCS_GUEST_IA32_ES_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx rpl=%lld", r, r & 0x3);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_ES_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_ES_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_ES_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	

	DPRINTF(" fs=");
	if (vmread(VMCS_GUEST_IA32_FS_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx rpl=%lld", r, r & 0x3);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_FS_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_FS_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_FS_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}

	DPRINTF(" gs=");
	if (vmread(VMCS_GUEST_IA32_GS_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx rpl=%lld", r, r & 0x3);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_GS_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_GS_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_GS_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	

	DPRINTF(" ss=");
	if (vmread(VMCS_GUEST_IA32_SS_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx rpl=%lld", r, r & 0x3);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_SS_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_SS_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_SS_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	

	DPRINTF(" tr=");
	if (vmread(VMCS_GUEST_IA32_TR_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx", r);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_TR_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_TR_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_TR_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	
		
	DPRINTF(" gdtr base=");
	if (vmread(VMCS_GUEST_IA32_GDTR_BASE, &r))
		DPRINTF("(error reading)   ");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_GDTR_LIMIT, &r))
		DPRINTF("(error reading)\n");
	else
		DPRINTF("0x%016llx\n", r);

	DPRINTF(" idtr base=");
	if (vmread(VMCS_GUEST_IA32_IDTR_BASE, &r))
		DPRINTF("(error reading)   ");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_IDTR_LIMIT, &r))
		DPRINTF("(error reading)\n");
	else
		DPRINTF("0x%016llx\n", r);
	
	DPRINTF(" ldtr=");
	if (vmread(VMCS_GUEST_IA32_LDTR_SEL, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%04llx", r);

	DPRINTF(" base=");
	if (vmread(VMCS_GUEST_IA32_LDTR_BASE, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" limit=");
	if (vmread(VMCS_GUEST_IA32_LDTR_LIMIT, &r))
		DPRINTF("(error reading)");
	else
		DPRINTF("0x%016llx", r);

	DPRINTF(" a/r=");
	if (vmread(VMCS_GUEST_IA32_LDTR_AR, &r))
		DPRINTF("(error reading)\n");
	else {
		DPRINTF("0x%04llx\n  ", r);
		vmm_segment_desc_decode(r);
	}	

	DPRINTF(" --Guest MSRs @@ 0x%016llx (paddr: 0x%016llx)--\n",
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_va,
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_pa);

	msr_store = (struct vmx_msr_store *)vcpu->vc_vmx_msr_exit_save_va;

	for (i = 0; i < VMX_NUM_MSR_STORE; i++) {
		DPRINTF("  MSR %d @@ %p : 0x%08llx (%s), "
		    "value=0x%016llx ",
		    i, &msr_store[i], msr_store[i].vms_index,
		    msr_name_decode(msr_store[i].vms_index),
		    msr_store[i].vms_data); 
		vmm_decode_msr_value(msr_store[i].vms_index,
		    msr_store[i].vms_data);
	}

	DPRINTF(" last PIC irq=%d\n", vcpu->vc_intr);
}

/*
 * msr_name_decode
 *
 * Returns a human-readable name for the MSR supplied in 'msr'.
 *
 * Parameters:
 *  msr - The MSR to decode
 *
 * Return value:
 *  NULL-terminated character string containing the name of the MSR requested
 */
const char *
msr_name_decode(uint32_t msr)
{
	/*
	 * Add as needed. Also consider adding a decode function when
	 * adding to this table.
	 */

	switch (msr) {
	case MSR_TSC: return "TSC";
	case MSR_APICBASE: return "APIC base";
	case MSR_IA32_FEATURE_CONTROL: return "IA32 feature control";
	case MSR_PERFCTR0: return "perf counter 0";
	case MSR_PERFCTR1: return "perf counter 1";
	case MSR_TEMPERATURE_TARGET: return "temperature target";
	case MSR_MTRRcap: return "MTRR cap";
	case MSR_PERF_STATUS: return "perf status";
	case MSR_PERF_CTL: return "perf control";
	case MSR_MTRRvarBase: return "MTRR variable base";
	case MSR_MTRRfix64K_00000: return "MTRR fixed 64K";
	case MSR_MTRRfix16K_80000: return "MTRR fixed 16K";
	case MSR_MTRRfix4K_C0000: return "MTRR fixed 4K";
	case MSR_CR_PAT: return "PAT";
	case MSR_MTRRdefType: return "MTRR default type";
	case MSR_EFER: return "EFER";
	case MSR_STAR: return "STAR";
	case MSR_LSTAR: return "LSTAR";
	case MSR_CSTAR: return "CSTAR";
	case MSR_SFMASK: return "SFMASK";
	case MSR_FSBASE: return "FSBASE";
	case MSR_GSBASE: return "GSBASE";
	case MSR_KERNELGSBASE: return "KGSBASE";
	default: return "Unknown MSR";
	}
}

/*
 * vmm_segment_desc_decode
 *
 * Debug function to print segment information for supplied descriptor
 *
 * Parameters:
 *  val - The A/R bytes for the segment descriptor to decode
 */
void
vmm_segment_desc_decode(uint64_t val)
{
	uint16_t ar;
	uint8_t g, type, s, dpl, p, dib, l;
	uint32_t unusable;

	/* Exit early on unusable descriptors */
	unusable = val & 0x10000;
	if (unusable) {
		DPRINTF("(unusable)\n");
		return;
	}

	ar = (uint16_t)val;

	g = (ar & 0x8000) >> 15;
	dib = (ar & 0x4000) >> 14;
	l = (ar & 0x2000) >> 13;
	p = (ar & 0x80) >> 7;
	dpl = (ar & 0x60) >> 5;
	s = (ar & 0x10) >> 4;
	type = (ar & 0xf);

	DPRINTF("granularity=%d dib=%d l(64 bit)=%d present=%d sys=%d ",
	    g, dib, l, p, s);

	DPRINTF("type=");
	if (!s) {
		switch (type) {
		case SDT_SYSLDT: DPRINTF("ldt\n"); break;
		case SDT_SYS386TSS: DPRINTF("tss (available)\n"); break;
		case SDT_SYS386BSY: DPRINTF("tss (busy)\n"); break;
		case SDT_SYS386CGT: DPRINTF("call gate\n"); break;
		case SDT_SYS386IGT: DPRINTF("interrupt gate\n"); break;
		case SDT_SYS386TGT: DPRINTF("trap gate\n"); break;
		/* XXX handle 32 bit segment types by inspecting mode */
		default: DPRINTF("unknown");
		}
	} else {
		switch (type + 16) {
		case SDT_MEMRO: DPRINTF("data, r/o\n"); break;
		case SDT_MEMROA: DPRINTF("data, r/o, accessed\n"); break;
		case SDT_MEMRW: DPRINTF("data, r/w\n"); break;
		case SDT_MEMRWA: DPRINTF("data, r/w, accessed\n"); break;
		case SDT_MEMROD: DPRINTF("data, r/o, expand down\n"); break;
		case SDT_MEMRODA: DPRINTF("data, r/o, expand down, "
		    "accessed\n");
			break;
		case SDT_MEMRWD: DPRINTF("data, r/w, expand down\n"); break;
		case SDT_MEMRWDA: DPRINTF("data, r/w, expand down, "
		    "accessed\n");
			break;
		case SDT_MEME: DPRINTF("code, x only\n"); break;
		case SDT_MEMEA: DPRINTF("code, x only, accessed\n");
		case SDT_MEMER: DPRINTF("code, r/x\n"); break;
		case SDT_MEMERA: DPRINTF("code, r/x, accessed\n"); break;
		case SDT_MEMEC: DPRINTF("code, x only, conforming\n"); break;
		case SDT_MEMEAC: DPRINTF("code, x only, conforming, "
		    "accessed\n");
			break;
		case SDT_MEMERC: DPRINTF("code, r/x, conforming\n"); break;
		case SDT_MEMERAC: DPRINTF("code, r/x, conforming, accessed\n");
			break;
		}
	}
}

void
vmm_decode_cr0(uint64_t cr0)
{
	struct vmm_reg_debug_info cr0_info[11] = {
		{ CR0_PG, "PG ", "pg " },
		{ CR0_CD, "CD ", "cd " },
		{ CR0_NW, "NW ", "nw " },
		{ CR0_AM, "AM ", "am " },
		{ CR0_WP, "WP ", "wp " },
		{ CR0_NE, "NE ", "ne " },
		{ CR0_ET, "ET ", "et " },
		{ CR0_TS, "TS ", "ts " },
		{ CR0_EM, "EM ", "em " },
		{ CR0_MP, "MP ", "mp " },
		{ CR0_PE, "PE", "pe" }
	};

	uint8_t i;

	DPRINTF("(");
	for (i = 0; i < 11; i++)
		if (cr0 & cr0_info[i].vrdi_bit)
			DPRINTF(cr0_info[i].vrdi_present);
		else
			DPRINTF(cr0_info[i].vrdi_absent);
	
	DPRINTF(")\n");
}

void
vmm_decode_cr3(uint64_t cr3)
{
	struct vmm_reg_debug_info cr3_info[2] = {
		{ CR3_PWT, "PWT ", "pwt "},
		{ CR3_PCD, "PCD", "pcd"}
	};

	uint64_t cr4;
	uint8_t i;

	if (vmread(VMCS_GUEST_IA32_CR4, &cr4)) {
		DPRINTF("(error)\n");
		return;
	}

	/* If CR4.PCIDE = 0, interpret CR3.PWT and CR3.PCD */
	if ((cr4 & CR4_PCIDE) == 0) {
		DPRINTF("(");
		for (i = 0 ; i < 2 ; i++)
			if (cr3 & cr3_info[i].vrdi_bit)
				DPRINTF(cr3_info[i].vrdi_present);
			else
				DPRINTF(cr3_info[i].vrdi_absent);

		DPRINTF(")\n");
	} else {
		DPRINTF("(pcid=0x%llx)\n", cr3 & 0xFFF);
	}
}

void
vmm_decode_cr4(uint64_t cr4)
{
	struct vmm_reg_debug_info cr4_info[19] = {
		{ CR4_PKE, "PKE ", "pke "},
		{ CR4_SMAP, "SMAP ", "smap "},
		{ CR4_SMEP, "SMEP ", "smep "},
		{ CR4_OSXSAVE, "OSXSAVE ", "osxsave "},
		{ CR4_PCIDE, "PCIDE ", "pcide "},
		{ CR4_FSGSBASE, "FSGSBASE ", "fsgsbase "},
		{ CR4_SMXE, "SMXE ", "smxe "},
		{ CR4_VMXE, "VMXE ", "vmxe "},
		{ CR4_OSXMMEXCPT, "OSXMMEXCPT ", "osxmmexcpt "},
		{ CR4_OSFXSR, "OSFXSR ", "osfxsr "},
		{ CR4_PCE, "PCE ", "pce "},
		{ CR4_PGE, "PGE ", "pge "},
		{ CR4_MCE, "MCE ", "mce "},
		{ CR4_PAE, "PAE ", "pae "},
		{ CR4_PSE, "PSE ", "pse "},
		{ CR4_DE, "DE ", "de "},
		{ CR4_TSD, "TSD ", "tsd "},
		{ CR4_PVI, "PVI ", "pvi "},
		{ CR4_VME, "VME", "vme"}
	};

	uint8_t i;

	DPRINTF("(");
	for (i = 0; i < 19; i++)
		if (cr4 & cr4_info[i].vrdi_bit)
			DPRINTF(cr4_info[i].vrdi_present);
		else
			DPRINTF(cr4_info[i].vrdi_absent);
	
	DPRINTF(")\n");
}

void
vmm_decode_apicbase_msr_value(uint64_t apicbase)
{
	struct vmm_reg_debug_info apicbase_info[3] = {
		{ APICBASE_BSP, "BSP ", "bsp "},
		{ APICBASE_ENABLE_X2APIC, "X2APIC ", "x2apic "},
		{ APICBASE_GLOBAL_ENABLE, "GLB_EN", "glb_en"}
	};

	uint8_t i;

	DPRINTF("(");
	for (i = 0; i < 3; i++)
		if (apicbase & apicbase_info[i].vrdi_bit)
			DPRINTF(apicbase_info[i].vrdi_present);
		else
			DPRINTF(apicbase_info[i].vrdi_absent);
	
	DPRINTF(")\n");
}

void
vmm_decode_ia32_fc_value(uint64_t fcr)
{
	struct vmm_reg_debug_info fcr_info[4] = {
		{ IA32_FEATURE_CONTROL_LOCK, "LOCK ", "lock "},
		{ IA32_FEATURE_CONTROL_SMX_EN, "SMX ", "smx "},
		{ IA32_FEATURE_CONTROL_VMX_EN, "VMX ", "vmx "},
		{ IA32_FEATURE_CONTROL_SENTER_EN, "SENTER ", "senter "}
	};

	uint8_t i;

	DPRINTF("(");
	for (i = 0; i < 4; i++)
		if (fcr & fcr_info[i].vrdi_bit)
			DPRINTF(fcr_info[i].vrdi_present);
		else
			DPRINTF(fcr_info[i].vrdi_absent);

	if (fcr & IA32_FEATURE_CONTROL_SENTER_EN)
		DPRINTF(" [SENTER param = 0x%llx]",
		    (fcr & IA32_FEATURE_CONTROL_SENTER_PARAM_MASK) >> 8);

	DPRINTF(")\n");
}

void
vmm_decode_mtrrcap_value(uint64_t val)
{
	struct vmm_reg_debug_info mtrrcap_info[3] = {
		{ MTRRcap_FIXED, "FIXED ", "fixed "},
		{ MTRRcap_WC, "WC ", "wc "},
		{ MTRRcap_SMRR, "SMRR ", "smrr "}
	};

	uint8_t i;

	DPRINTF("(");
	for (i = 0; i < 3; i++)
		if (val & mtrrcap_info[i].vrdi_bit)
			DPRINTF(mtrrcap_info[i].vrdi_present);
		else
			DPRINTF(mtrrcap_info[i].vrdi_absent);

	if (val & MTRRcap_FIXED)
		DPRINTF(" [nr fixed ranges = 0x%llx]",
		    (val & 0xff));

	DPRINTF(")\n");
}

void
vmm_decode_perf_status_value(uint64_t val)
{
	DPRINTF("(pstate ratio = 0x%llx)\n", (val & 0xffff));
}

void vmm_decode_perf_ctl_value(uint64_t val)
{
	DPRINTF("(%s ", (val & PERF_CTL_TURBO) ? "TURBO" : "turbo");
	DPRINTF("pstate req = 0x%llx)\n", (val & 0xfffF));
}

void
vmm_decode_mtrrdeftype_value(uint64_t mtrrdeftype)
{
	struct vmm_reg_debug_info mtrrdeftype_info[2] = {
		{ MTRRdefType_FIXED_ENABLE, "FIXED ", "fixed "},
		{ MTRRdefType_ENABLE, "ENABLED ", "enabled "},
	};

	uint8_t i;
	int type;

	DPRINTF("(");
	for (i = 0; i < 2; i++)
		if (mtrrdeftype & mtrrdeftype_info[i].vrdi_bit)
			DPRINTF(mtrrdeftype_info[i].vrdi_present);
		else
			DPRINTF(mtrrdeftype_info[i].vrdi_absent);

	DPRINTF("type = ");
	type = mtrr2mrt(mtrrdeftype & 0xff);
	switch (type) {
	case MDF_UNCACHEABLE: DPRINTF("UC"); break;
	case MDF_WRITECOMBINE: DPRINTF("WC"); break;
	case MDF_WRITETHROUGH: DPRINTF("WT"); break;
	case MDF_WRITEPROTECT: DPRINTF("RO"); break;
	case MDF_WRITEBACK: DPRINTF("WB"); break;
	case MDF_UNKNOWN:
	default:
		DPRINTF("??");
		break;
	}

	DPRINTF(")\n");
}

void
vmm_decode_efer_value(uint64_t efer)
{
	struct vmm_reg_debug_info efer_info[4] = {
		{ EFER_SCE, "SCE ", "sce "},
		{ EFER_LME, "LME ", "lme "},
		{ EFER_LMA, "LMA ", "lma "},
		{ EFER_NXE, "NXE", "nxe"},
	};

	uint8_t i;

	DPRINTF("(");
	for (i = 0; i < 4; i++)
		if (efer & efer_info[i].vrdi_bit)
			DPRINTF(efer_info[i].vrdi_present);
		else
			DPRINTF(efer_info[i].vrdi_absent);

	DPRINTF(")\n");
}

void
vmm_decode_msr_value(uint64_t msr, uint64_t val)
{
	switch (msr) {
	case MSR_APICBASE: vmm_decode_apicbase_msr_value(val); break;
	case MSR_IA32_FEATURE_CONTROL: vmm_decode_ia32_fc_value(val); break;
	case MSR_MTRRcap: vmm_decode_mtrrcap_value(val); break;
	case MSR_PERF_STATUS: vmm_decode_perf_status_value(val); break;
	case MSR_PERF_CTL: vmm_decode_perf_ctl_value(val); break;
	case MSR_MTRRdefType: vmm_decode_mtrrdeftype_value(val); break;
	case MSR_EFER: vmm_decode_efer_value(val); break;
	default: DPRINTF("\n");
	}
}
#endif /* VMM_DEBUG */
@


1.130
log
@typo in debug build
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.129 2017/03/26 21:47:34 mlarkin Exp $	*/
d151 2
d4118 94
d4219 1
a4219 1
	uint64_t insn_length, exit_qual, regval, ectls, r;
a4220 1
	struct vmx_msr_store *msr_store;
d4246 2
a4247 2
		DPRINTF("%s: mov to cr%d @@ %llx\n", __func__, crnum,
		    vcpu->vc_gueststate.vg_rip);
d4274 5
a4278 6
		if (crnum == 0) {
			r |= CR0_NE;
			if (vmwrite(VMCS_GUEST_IA32_CR0, r)) {
				printf("%s: can't write guest cr0\n", __func__);
				return (EINVAL);
			}
a4279 27
			if (r & CR0_PG) {
				msr_store = (struct vmx_msr_store *)
				    vcpu->vc_vmx_msr_exit_load_va;
				if (msr_store[0].vms_data & EFER_LME) {
					msr_store[0].vms_data |= EFER_LMA;
					if (vmread(VMCS_ENTRY_CTLS, &ectls)) {
						printf("%s: can't read entry "
					 	    "controls", __func__);
						return (EINVAL);
					}
					ectls |= IA32_VMX_IA32E_MODE_GUEST;
					if (vmwrite(VMCS_ENTRY_CTLS, ectls)) {
						printf("%s: can't write entry "
						    "controls", __func__);
						return (EINVAL);
					}
				}
			}
		}

		if (crnum == 4) {
			regval |= CR4_VMXE;
			if (vmwrite(VMCS_GUEST_IA32_CR4, regval)) {
				printf("%s: can't write guest cr4\n", __func__);
				return (EINVAL);
			}
		}
@


1.129
log
@discard MSR reads from unknown MSRs instead of passing them through. That
behaviour was needed during early development but not anymore. Suppress
the printf that accompanied these exits since linux guests go probing
wildly into msr-land on each boot.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.128 2017/03/26 18:34:55 mlarkin Exp $	*/
d4307 1
a4307 1
	uin64_t *rcx;
@


1.128
log
@Add "AVX" to the comment above the previous commit. Spotted by reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.127 2017/03/26 18:29:58 mlarkin Exp $	*/
d4242 2
a4243 1
 * each MSR access that ends up here, we log the access.
d4250 1
a4250 1
 *  1: An error occurred
d4256 4
a4259 1
	uint64_t *rax, *rcx, *rdx, msr;
a4269 1
	rcx = &vcpu->vc_gueststate.vg_rcx;
d4272 2
a4273 3
	msr = rdmsr(*rcx);
	*rax = msr & 0xFFFFFFFFULL;
	*rdx = msr >> 32;
d4275 4
a4278 2
	/* XXX log the access for now, to be able to identify unknown MSRs */
	printf("%s: rdmsr exit, msr=0x%llx, data returned to "
d4280 1
d4291 2
a4292 2
 * the written data. Any valid wrmsr will not end up here (it will be
 * whitelisted in the MSR bitmap).
d4299 1
a4299 1
 *  1: An error occurred
d4305 4
a4308 1
	uint64_t *rax, *rcx, *rdx;
a4318 1
	rcx = &vcpu->vc_gueststate.vg_rcx;
d4321 4
a4324 2
	/* XXX log the access for now, to be able to identify unknown MSRs */
	printf("%s: wrmsr exit, msr=0x%llx, discarding data written from "
d4326 1
@


1.127
log
@Suppress AVX from the extended CPUID flags. Our AVX treatment is currently
incomplete and enabling it leads ubuntu guests to try and use the feature,
with incorrect results. We can re-enable this at a later date when AVX
is properly handled.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.126 2017/03/25 22:24:01 deraadt Exp $	*/
d4381 1
@


1.126
log
@Split vmm_probe() into a vmm_enabled() function, to better follow the
probe/attach approach used by mainbus.
ok mlarkin kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.125 2017/03/24 09:06:02 mlarkin Exp $	*/
d4403 1
a4403 1
		    CPUIDECX_XTPR |
@


1.125
log
@Handle guest interruptibility state - Reset the interruptibility state
VMCS field on vmentry when we advanced %rip on the last exit (simulating
a real processor's behaviour). Handles guest "sti ; hlt" instruction
sequences, which is used in seabios as a primitive idle loop construct.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.124 2017/03/24 08:52:53 mlarkin Exp $	*/
d103 1
d263 1
a263 1
 * vmm_probe
d269 1
a269 1
vmm_probe(struct device *parent, void *match, void *aux)
d273 1
a273 10
	const char **busname = (const char **)aux;
	int found_vmx, found_svm, vmm_disabled;

	/* Check if this probe is for us */
	if (strcmp(*busname, vmm_cd.cd_name) != 0)
		return (0);

	found_vmx = 0;
	found_svm = 0;
	vmm_disabled = 0;
d293 2
a294 2
	if (vmm_disabled)
		printf("vmm disabled by firmware\n");
d296 8
a303 1
	return 0;
@


1.124
log
@Exit to vmd on byte size PCI accesses.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.123 2017/03/24 08:02:02 mlarkin Exp $	*/
d3380 1
a3380 1
	uint64_t eii, procbased;
d3407 1
a3407 1
			break;	
d3484 3
a3486 3
			if (!vcpu->vc_irqready) {
				printf("vcpu_run_vmx: error - irq injected"
				    " while not ready\n");
d3491 13
a3503 8
			eii = (irq & 0xFF);
			eii |= (1ULL << 31);	/* Valid */
			eii |= (0ULL << 8);	/* Hardware Interrupt */
			if (vmwrite(VMCS_ENTRY_INTERRUPTION_INFO, eii)) {
				printf("vcpu_run_vmx: can't vector "
				    "interrupt to guest\n");
				ret = EINVAL;
				break;
a3504 2

			irq = 0xFFFF;
d3780 1
a3780 1
	uint64_t exit_reason, rflags;
d3851 17
@


1.123
log
@Allow returns from vmd after handling cpuid exits (handles the case where
a cpuid instruction was emulated at the same time there was an interrupt
pending)
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.122 2017/03/21 02:57:38 mlarkin Exp $	*/
d4076 1
a4076 1
	case 0xcfc:
d4082 8
a4089 2
		if (vcpu->vc_exit.vei.vei_dir == 1)
			vcpu->vc_gueststate.vg_rax = 0xFFFFFFFF;
@


1.122
log
@Don't allow the guest to clear CR0_NE or CR4_VMXE. While we should be
using the "must be clear / must be set" masks for these registers, I'd
like to know (for now) when guest VMs manipulate bits in these registers
in an unexpected way. This is needed for Linux guests, as they
unconditionally set CR0 without NE, and CR4 without VMXE.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.121 2017/03/21 02:30:33 mlarkin Exp $	*/
d3406 2
@


1.121
log
@CPUID feature 0x80000000 emulation fell through to 0x80000001, which
resulted in wrong cpu information being passed to the guest. Specifically
this breaks Linux guests as with the fallthrough, CPUID_LONG was cleared,
and Linux thought it was runinng on a machine incapable of 64-bit mode.
OpenBSD/NetBSD guests don't check this flag and thus weren't affected.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.120 2017/03/19 03:42:38 mlarkin Exp $	*/
d2340 10
d4096 4
a4099 3
	uint64_t insn_length, exit_qual;
	uint8_t crnum, dir;

d4101 1
a4101 1
		printf("vmx_handle_cr: can't obtain instruction length\n");
d4106 1
a4106 1
		printf("vmx_handle_cr: can't get exit qual\n");
d4113 4
d4119 3
d4124 62
a4185 2
		DPRINTF("vmx_handle_cr: mov to cr%d @@ %llx\n",
	    	    crnum, vcpu->vc_gueststate.vg_rip);
@


1.120
log
@Handle master/slave PIC vector base properly. OpenBSD uses 0x20/0x28
(respectively). Seabios uses 0x8/0x78 and linux uses 0x30/0x38. Respond
properly to PIC vector base assignment and calculate VMX injection
vectors based on current values, instead of always assuming OpenBSD
defaults.

Needed for both seabios serial console support as well as linux guest
support. Tested on -current as is, does not break existing OpenBSD
guest support.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.119 2017/03/02 07:26:31 mlarkin Exp $	*/
d4452 1
@


1.119
log
@log attempts to access cpuid leaf function 0x03, "processor serial number".

matches other log messages for other unimplemented cpuid leaf functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.118 2017/03/02 03:21:44 mlarkin Exp $	*/
a3469 2
		/* XXX - 0x20 should be changed to PIC's vector base */

d3479 1
a3479 1
			eii = (irq & 0xFF) + 0x20;
@


1.118
log
@reduce some more differences in vmm between i386 and amd64 that didn't
get picked up previously. i386 gets some changes relating to EFER
treatment and amd64 gets a whitespace fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.117 2017/03/02 02:57:35 mlarkin Exp $	*/
d4318 2
@


1.117
log
@reduce differences in vmm between amd64 and i386 (i386 picks up a handful
of recent fixes for SVM that were missed). No functional change on amd64
(just an added comment)
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.116 2017/03/02 02:34:56 mlarkin Exp $	*/
d2573 1
a2573 1
	}	
@


1.116
log
@Refactor cpuid exit handler to make it easier to bolt on SVM support
shortly (instead of having two nearly identical functions.)

ok reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.115 2017/02/20 08:12:47 mlarkin Exp $	*/
d1295 1
@


1.115
log
@VMX: assert that the supplied instruction length matches what is expected for
HLT exits
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.114 2017/02/20 07:22:50 mlarkin Exp $	*/
d147 1
a147 1
int vmx_handle_cpuid(struct vcpu *);
d3789 1
a3789 1
		ret = vmx_handle_cpuid(vcpu);
d4220 1
a4220 1
 * vmx_handle_cpuid
d4223 7
d4232 1
a4232 1
vmx_handle_cpuid(struct vcpu *vcpu)
d4237 10
a4246 3
	if (vmread(VMCS_INSTRUCTION_LENGTH, &insn_length)) {
		printf("vmx_handle_cpuid: can't obtain instruction length\n");
		return (EINVAL);
a4248 3
	/* All CPUID instructions are 0x0F 0xA2 */
	KASSERT(insn_length == 2);

d4308 3
a4310 3
	case 0x02:	/* Cache and TLB information */
		DPRINTF("vmx_handle_cpuid: function 0x02 (cache/TLB) not"
		    " supported\n");
d4323 2
a4324 2
		DPRINTF("vmx_handle_cpuid: function 0x04 (deterministic "
		    "cache info) not supported\n");
d4331 2
d4338 3
a4340 1
	case 0x06:	/* Thermal / Power management */
d4368 2
d4377 2
a4378 2
		DPRINTF("vmx_handle_cpuid: function 0x09 (direct cache access)"
		    " not supported\n");
d4384 3
a4386 1
	case 0x0a:	/* Architectural performance monitoring */
d4393 2
a4394 2
		DPRINTF("vmx_handle_cpuid: function 0x0b (topology enumeration)"
		    " not supported\n");
d4401 2
a4402 2
		DPRINTF("vmx_handle_cpuid: function 0x0d (ext. state info)"
		    " not supported\n");
d4409 2
a4410 2
		DPRINTF("vmx_handle_cpuid: function 0x0f (QoS info)"
		    " not supported\n");
d4417 2
a4418 2
		DPRINTF("vmx_handle_cpuid: function 0x14 (processor trace info)"
		    " not supported\n");
d4425 2
a4426 2
		DPRINTF("vmx_handle_cpuid: function 0x15 (TSC / CCC info)"
		    " not supported\n");
d4433 2
a4434 2
		DPRINTF("vmx_handle_cpuid: function 0x16 (frequency info)"
		    " not supported\n");
d4494 2
a4495 2
		DPRINTF("vmx_handle_cpuid: function 0x80000008 (phys bits info)"
		    " not supported\n");
d4502 1
a4502 1
		DPRINTF("vmx_handle_cpuid: unsupported rax=0x%llx\n", *rax);
@


1.114
log
@typo in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.113 2017/02/20 07:21:47 mlarkin Exp $	*/
d3726 1
a3726 1
		printf("vmx_handle_hlt: can't obtain instruction length\n");
d3729 3
@


1.113
log
@SVM: fix wrong treatment of MSRs, especially EFER.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.112 2017/02/20 07:14:45 mlarkin Exp $	*/
d3349 1
a3349 1
 * VMM main loop used to run a VCPU.
@


1.112
log
@SVM: fix segment A/R bits formatting
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.111 2017/01/19 23:18:11 mlarkin Exp $	*/
d1689 1
a1689 1
	vmcb->v_iopm_pa = (uint64_t)(vcpu->vc_msr_bitmap_pa);
a1693 1
	svm_setmsrbrw(vcpu, MSR_EFER);
d1702 3
d1717 3
d1722 3
d1762 1
a1762 1
		msrs[idx] &= ~(SVM_MSRBIT_R(msr - 0xc0000000));
a1766 3

	DPRINTF("%s: set msr read bitmap, msr=0x%x, idx=0x%x, "
	    "msrs[0x%x]=0x%x\n", __func__, msr, idx, idx, msrs[idx]);
d1802 1
a1802 1
		idx = SVM_MSRIDX(msr - 0xc0000000) + 0x1000;
a1807 3

	DPRINTF("%s: set msr write bitmap, msr=0x%x, idx=0x%x, "
	    "msrs[0x%x]=0x%x\n", __func__, msr, idx, idx, msrs[idx]);
@


1.111
log
@SVM: register reset and intercept setup code
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.110 2017/01/19 05:53:40 mlarkin Exp $	*/
d1550 1
a1574 2
		DPRINTF("%s: set vcpu GPRs (rip=0x%llx rsp=0x%llx)\n",
		    __func__, vmcb->v_rip, vmcb->v_rsp);
d1580 2
a1581 1
		vmcb->v_cs.vs_attr = sregs[VCPU_REGS_CS].vsi_ar;
d1585 2
a1586 1
		vmcb->v_ds.vs_attr = sregs[VCPU_REGS_DS].vsi_ar;
d1590 2
a1591 1
		vmcb->v_es.vs_attr = sregs[VCPU_REGS_ES].vsi_ar;
d1595 2
a1596 1
		vmcb->v_fs.vs_attr = sregs[VCPU_REGS_FS].vsi_ar;
d1600 2
a1601 1
		vmcb->v_gs.vs_attr = sregs[VCPU_REGS_GS].vsi_ar;
d1605 2
a1606 1
		vmcb->v_ss.vs_attr = sregs[VCPU_REGS_SS].vsi_ar;
d1610 2
a1611 1
		vmcb->v_ldtr.vs_attr = sregs[VCPU_REGS_LDTR].vsi_ar;
d1615 2
a1616 1
		vmcb->v_tr.vs_attr = sregs[VCPU_REGS_TR].vsi_ar;
a1621 4

		DPRINTF("%s: set vcpu seg regs (gdt.base=0x%llx, "
		    "cs.sel=0x%llx)\n", __func__, vmcb->v_gdtr.vs_base,
		    (uint64_t)vmcb->v_cs.vs_sel);
a1627 4

		DPRINTF("%s: set vcpu CRs (cr0=0x%llx cr3=0x%llx "
		    "cr4=0x%llx)\n", __func__, vmcb->v_cr0, vmcb->v_cr3,
		    vmcb->v_cr4);
@


1.110
log
@rename a couple of macros that are causing me a merge headache with the
next SVM diff
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.109 2017/01/19 01:46:20 mlarkin Exp $	*/
d165 3
d1533 10
a1542 1
 * XXX - unimplemented
d1548 83
d1637 9
a1645 1
 * XXX - unimplemented
d1650 173
a1822 1
	return (0);
@


1.109
log
@SVM: matching vcpu deinit functions for previous commits
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.108 2017/01/19 01:33:45 mlarkin Exp $	*/
d1573 2
a1574 2
		idx = MSRIDX(msr);
		msrs[idx] &= ~(MSRBIT(msr));
d1576 2
a1577 2
		idx = MSRIDX(msr - 0xc0000000) + 0x400;
		msrs[idx] &= ~(MSRBIT(msr - 0xc0000000));
d1605 2
a1606 2
		idx = MSRIDX(msr) + 0x800;
		msrs[idx] &= ~(MSRBIT(msr));
d1608 2
a1609 2
		idx = MSRIDX(msr - 0xc0000000) + 0xc00;
		msrs[idx] &= ~(MSRBIT(msr - 0xc0000000));
@


1.108
log
@SVM: vcpu_init_svm - allocate memory for control structures (vmcb,
msr bitmap, ioio bitmap, and host state save area)
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.107 2017/01/19 00:03:52 mlarkin Exp $	*/
d2481 3
d2506 3
d2513 12
a2524 1
	/* Unused */
d2531 3
@


1.107
log
@delete some empty lines found when diffing amd64 vs i386
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.106 2017/01/13 02:38:41 mlarkin Exp $	*/
d198 7
d1177 2
d2107 8
d2336 11
d2351 100
a2450 2
	/* XXX removed due to rot */
	return (0);
@


1.106
log
@SVM/RVI: vm_impl_init_svm implementation, create pmap for RVI VMs
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.105 2017/01/12 09:02:47 mlarkin Exp $	*/
a1672 1

a2076 1
	
@


1.105
log
@Remove vc_hsa_stack_va, it has not been used in a long time and is no
longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.104 2017/01/12 08:35:53 mlarkin Exp $	*/
d1142 8
d1154 51
a1204 2
	/* XXX removed due to rot */
	return (-1);
@


1.104
log
@add a couple missing comments, no code change
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.103 2017/01/10 09:02:29 mlarkin Exp $	*/
a2281 5
	vcpu->vc_hsa_stack_va = (vaddr_t)malloc(PAGE_SIZE, M_DEVBUF,
	    M_NOWAIT|M_ZERO);
	if (!vcpu->vc_hsa_stack_va)
		return (ENOMEM);

a2292 3
	if (ret)
		free((void *)vcpu->vc_hsa_stack_va, M_DEVBUF, PAGE_SIZE);

a2315 2
	if (vcpu->vc_hsa_stack_va)
		free((void *)vcpu->vc_hsa_stack_va, M_DEVBUF, PAGE_SIZE);
@


1.103
log
@ensure EPT page walk length 4. We don't support other page walk lengths
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.102 2017/01/08 22:16:04 mlarkin Exp $	*/
d1063 8
@


1.102
log
@Revert previous commit as it failed to handle unset vmcs pointers properly

noticed by Josh Grosse
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.101 2017/01/07 23:01:27 mlarkin Exp $	*/
d1811 4
@


1.101
log
@reduce statement nesting depth in a couple places, suggested by
Michael Bombardieri
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.100 2017/01/07 22:37:03 mlarkin Exp $	*/
d1218 5
a1222 7
	/* Flush any old state */
	if (vmptrst(&old))
		return (EINVAL);

	if (old != 0xFFFFFFFFFFFFFFFFULL) {
		if (vmclear(&old))
			return (EINVAL);
@


1.100
log
@use 16 bit segment selectors. also reduces differences with i386 vmm
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.99 2017/01/03 09:48:15 mlarkin Exp $	*/
d1219 6
a1224 5
	if (!vmptrst(&old)) {
		if (old != 0xFFFFFFFFFFFFFFFFULL) {
			if (vmclear(&old))
				return (EINVAL);
		}
@


1.99
log
@
reduce differences between amd64 and i386 vmm
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.98 2016/12/26 08:55:09 mlarkin Exp $	*/
d1368 2
a1369 1
	uint64_t sel, limit, ar;
@


1.98
log
@Pass through cacheline size information to guest. This was pointed out
to me by Christian Barthel who was encountering failures running Java
on amd64 vmm(4) guests. Apparently Java queries the cacheline info
and if we report "0", Java aborts.

Verified on amd64 vmm(4) guest.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.97 2016/11/15 11:25:38 jsg Exp $	*/
d263 1
a263 1
	int found_vmx, found_svm, vm_disabled;
d271 1
a271 1
	vm_disabled = 0;
d280 1
a280 1
			vm_disabled = 1;
d290 2
a291 1
	if (vm_disabled)
d293 1
@


1.97
log
@Avoid an uninitialised value in rev 1.93.
ok martijn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.96 2016/11/08 07:10:55 mlarkin Exp $	*/
d3769 2
a3770 2
		*rbx = cpu_ebxfeature & 0x00FFFFFF;
		*rbx &= (vcpu->vc_id & 0xFF) << 24;
@


1.96
log
@
fix debug build
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.95 2016/10/29 09:24:54 reyk Exp $	*/
d271 1
@


1.95
log
@Further improve vmm's security model by restricting pledged vmm
processes to only do VMM_IOC_ ioctls on their associated VM (these
ioctls are _RUN, _RESETCPU, _INTR, _READREGS, or _WRITEREGS at
present).  The vmm monitor (parent) process or any non-pledged
processes can still do ioctls on any VM.  For example, a VM can only
terminate itself but vmctl or the monitor can terminate any VM.

This prevents reachover into other VMs: while escaping from a VM to
the host side (eg. through a bug in virtio etc.) pledge already kept
the attacker in a pledged and privsep'ed process, but now it also
prevents vmm ioctls on "other VMs".

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.94 2016/10/18 15:16:55 naddy Exp $	*/
d5110 1
a5110 1
		DPRINTF("  MSR %d @@ %p : 0x%08x (%s), "
@


1.94
log
@SVM on AMD CPUs is not supported yet, so don't attach there; ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.93 2016/10/13 19:36:25 martijn Exp $	*/
d118 1
d482 1
d486 1
a486 4
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vrp->vrp_vm_id)
			break;
	}
d490 1
a490 1
	if (vm == NULL) {
d493 1
a493 1
		return (ENOENT);
d550 1
d554 1
a554 4
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vip->vip_vm_id)
			break;
	}
d557 1
a557 1
	if (vm == NULL) {
d559 1
a559 1
		return (ENOENT);
d618 1
d622 1
a622 4
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vrwp->vrwp_vm_id)
			break;
	}
d625 1
a625 1
	if (vm == NULL) {
d627 1
a627 1
		return (ENOENT);
d656 42
d2695 1
d2701 1
a2701 4
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vtp->vtp_vm_id)
			break;
	}
d2703 1
a2703 1
	if (vm != NULL) {
d2721 2
a2722 2
	if (vm == NULL)
		return (ENOENT);
d2744 1
a2744 1
	int ret = 0;
d2751 1
a2751 5

	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vrp->vrp_vm_id)
			break;
	}
d2758 1
a2758 1
	if (vm != NULL) {
d2781 2
a2782 2
	if (vm == NULL)
		ret = ENOENT;
@


1.93
log
@Add an extra debug line when virtualization is disabled in the firmware.
This line would have saved me about an hour of hairpulling.

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.92 2016/10/06 18:52:09 reyk Exp $	*/
d285 2
a286 1
	if (found_vmx || found_svm)
@


1.92
log
@Update the list of vmm ioctls that are allowed by pledge.

OK mlarkin@@ stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.91 2016/10/06 07:51:10 mlarkin Exp $	*/
d262 1
a262 1
	int found_vmx, found_svm;
d277 2
d285 5
a289 1
	return (found_vmx || found_svm);
@


1.91
log
@
turn off vmm(4) debug mode
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.90 2016/10/06 07:37:51 mlarkin Exp $	*/
d435 3
@


1.90
log
@
add a debug function that was useful in finding the previous
broadwell/skylake bug.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.89 2016/10/05 08:04:14 mlarkin Exp $	*/
d43 1
a43 1
#define VMM_DEBUG
d46 1
a46 1
int vmm_debug = 1;
@


1.89
log
@
Add a missing flush; this appears to fix the broadwell/skylake "failed to
launch with valid vmcs" issue some people have been seeing when launching
vmm(4) VMs.

tested by reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.88 2016/10/03 04:53:54 mlarkin Exp $	*/
d171 1
d2879 1
d3296 1
d4112 2
a4113 2
	printf("vcpu @@ 0x%llx\n", (uint64_t)vcpu);
	printf("    parent vm @@ 0x%llx\n", (uint64_t)vcpu->vc_parent);
d4209 551
@


1.88
log
@
disable PAT and MTRR in guest VMs
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.87 2016/09/25 08:20:40 mlarkin Exp $	*/
d2158 4
a2161 1
	}
@


1.87
log
@
refactor vmcs flush and reload into one function, and remove another
flush that wasn't needed
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.86 2016/09/25 07:45:02 mlarkin Exp $	*/
a1557 29
# if 0
	/* XXX not needed now with MSR list */

	/* Default Guest PAT (if applicable) */
	if ((vcpu_vmx_check_cap(vcpu, IA32_VMX_ENTRY_CTLS,
	    IA32_VMX_LOAD_IA32_PAT_ON_ENTRY, 1)) ||
	    vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_SAVE_IA32_PAT_ON_EXIT, 1)) {
		pat_default = PATENTRY(0, PAT_WB) | PATENTRY(1, PAT_WT) |
		    PATENTRY(2, PAT_UCMINUS) | PATENTRY(3, PAT_UC) |
		    PATENTRY(4, PAT_WB) | PATENTRY(5, PAT_WT) |
		    PATENTRY(6, PAT_UCMINUS) | PATENTRY(7, PAT_UC);
		if (vmwrite(VMCS_GUEST_IA32_PAT, pat_default)) {
			ret = EINVAL;
			goto exit;
		}
	}

	/* Host PAT (if applicable) */
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_LOAD_IA32_PAT_ON_EXIT, 1)) {
		msr = rdmsr(MSR_CR_PAT);
		if (vmwrite(VMCS_HOST_IA32_PAT, msr)) {
			ret = EINVAL;
			goto exit;
		}
	}
#endif

d1850 1
a1850 1
	 * Select MSRs to be loaded on exit
d1855 10
a1864 12
	msr_store[1].vms_index = MSR_CR_PAT;
	msr_store[1].vms_data = rdmsr(MSR_CR_PAT);
	msr_store[2].vms_index = MSR_STAR;
	msr_store[2].vms_data = rdmsr(MSR_STAR);
	msr_store[3].vms_index = MSR_LSTAR;
	msr_store[3].vms_data = rdmsr(MSR_LSTAR);
	msr_store[4].vms_index = MSR_CSTAR;
	msr_store[4].vms_data = rdmsr(MSR_CSTAR);
	msr_store[5].vms_index = MSR_SFMASK;
	msr_store[5].vms_data = rdmsr(MSR_SFMASK);
	msr_store[6].vms_index = MSR_KERNELGSBASE;
	msr_store[6].vms_data = rdmsr(MSR_KERNELGSBASE);
d1867 1
a1867 1
	 * Select MSRs to be loaded on entry / saved on exit
d1881 1
a1881 1
	msr_store[1].vms_index = MSR_CR_PAT;
d1883 1
a1883 1
	msr_store[2].vms_index = MSR_STAR;
d1885 1
a1885 1
	msr_store[3].vms_index = MSR_LSTAR;
d1887 1
a1887 1
	msr_store[4].vms_index = MSR_CSTAR;
d1889 1
a1889 1
	msr_store[5].vms_index = MSR_SFMASK;
a1890 2
	msr_store[6].vms_index = MSR_KERNELGSBASE;
	msr_store[6].vms_data = 0ULL;		/* Initial value */
a1946 1
	vmx_setmsrbrw(vcpu, MSR_MTRRcap);
a1949 3
	vmx_setmsrbrw(vcpu, MSR_MTRRvarBase);
	vmx_setmsrbrw(vcpu, MSR_CR_PAT);
	vmx_setmsrbrw(vcpu, MSR_MTRRdefType);
d3120 1
d3746 1
d3763 1
a3763 1
		      CPUID_MTRR);
@


1.86
log
@
remove an extraneous vmcs flush
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.85 2016/09/17 06:43:38 jsg Exp $	*/
d125 1
d1150 36
d1204 1
a1204 1
	uint64_t sel, limit, ar, vmcs_ptr;
d1209 1
a1209 17
	/* Flush any old state */
	if (!vmptrst(&vmcs_ptr)) {
		if (vmcs_ptr != 0xFFFFFFFFFFFFFFFFULL) {
			if (vmclear(&vmcs_ptr))
				return (EINVAL);
		}
	} else
		return (EINVAL);

	/* Flush the VMCS */
	if (vmclear(&vcpu->vc_control_pa))
		return (EINVAL);

	/*
	 * Load the VMCS onto this PCPU so we can write registers
	 */
	if (vmptrld(&vcpu->vc_control_pa))
d1317 1
a1317 1
	uint64_t sel, limit, ar, vmcs_ptr;
d1323 1
a1323 17
		/* Flush any old state */
		if (!vmptrst(&vmcs_ptr)) {
			if (vmcs_ptr != 0xFFFFFFFFFFFFFFFFULL) {
				if (vmclear(&vmcs_ptr))
					return (EINVAL);
			}
		} else
			return (EINVAL);

		/* Flush the VMCS */
		if (vmclear(&vcpu->vc_control_pa))
			return (EINVAL);

		/*
		 * Load the VMCS onto this PCPU so we can write registers
		 */
		if (vmptrld(&vcpu->vc_control_pa))
d1525 1
a1525 1
	uint64_t msr, ctrlval, eptp, vmcs_ptr, cr3;
d1532 2
a1533 26
	/* Flush any old state */
	if (!vmptrst(&vmcs_ptr)) {
		if (vmcs_ptr != 0xFFFFFFFFFFFFFFFFULL) {
			if (vmclear(&vmcs_ptr)) {
				ret = EINVAL;
				goto exit;
			}
		}
	} else {
		ret = EINVAL;
		goto exit;
	}

	/* Flush the VMCS */
	if (vmclear(&vcpu->vc_control_pa)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Load the VMCS onto this PCPU so we can write registers
	 */
	if (vmptrld(&vcpu->vc_control_pa)) {
		ret = EINVAL;
		goto exit;
	}
@


1.85
log
@remove duplicated CPUIDECX_PDCM use
ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.84 2016/09/15 02:00:16 dlg Exp $	*/
a2125 6

	/* Flush the VMCS */
	if (vmclear(&vcpu->vc_control_pa)) {
		ret = EINVAL;
		goto exit;
	}
@


1.84
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.83 2016/09/14 18:23:06 mlarkin Exp $	*/
a3799 1
		 *  performance monitoring (CPUIDECX_PDCM)
d3816 1
a3816 1
		    CPUIDECX_XTPR | CPUIDECX_PDCM |
@


1.83
log
@
don't dump vcpu state on ept violations, these happen with sufficient
frequency that they cause a lot of dmesg spam and aren't errors to begin
with
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.82 2016/09/10 23:39:42 mlarkin Exp $	*/
d337 4
a340 6
	pool_init(&vm_pool, sizeof(struct vm), 0, 0, PR_WAITOK, "vmpool",
	    NULL);
	pool_setipl(&vm_pool, IPL_NONE);
	pool_init(&vcpu_pool, sizeof(struct vcpu), 0, 0, PR_WAITOK, "vcpupl",
	    NULL);
	pool_setipl(&vcpu_pool, IPL_NONE);
@


1.82
log
@
Skip printing a debug message when an external interrupt is received
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.81 2016/09/10 17:15:44 mlarkin Exp $	*/
d2929 2
@


1.81
log
@Enable VMM debug and add a few new controls
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.80 2016/09/07 15:35:13 mlarkin Exp $	*/
d2927 3
a2929 1
			break; 
@


1.80
log
@
dump some extra vcpu state if failure to launch is detected
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.79 2016/09/07 15:30:45 mlarkin Exp $	*/
d43 2
d46 1
a46 1
int vmm_debug = 0;
d4200 4
d4225 3
d4229 3
d4244 2
d4259 2
@


1.79
log
@
fix a wrong printf in a dump/debug function
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.78 2016/09/05 07:50:04 mlarkin Exp $	*/
d2901 1
a2901 1
	uint64_t exit_reason, cr3, vmcs_ptr;
d3178 6
@


1.78
log
@
Dump vcpu and vmcs info on various fail to enter scenarios
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.77 2016/09/04 08:49:18 mlarkin Exp $	*/
d4218 1
a4218 1
		    vcpu->vc_vmx_true_procbased_ctls);
@


1.77
log
@
Restrict MSR access to supported ones, log invalid accesses.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.76 2016/09/03 14:01:17 mlarkin Exp $	*/
d3168 4
d3178 4
d3186 4
@


1.76
log
@
suppress some cpuid bits for hardware we either don't support yet or
doesn't make sense in a vm
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.75 2016/09/03 11:35:19 mlarkin Exp $	*/
d144 2
d161 3
d1421 80
d1778 1
d1994 22
a2015 1
	/* XXX msr bitmap - set restrictions */
a2041 1
	paddr_t control_pa;
d2054 2
a2055 1
	if (!pmap_extract(pmap_kernel(), vcpu->vc_control_va, &control_pa)) {
a2059 2
	vcpu->vc_control_pa = (uint64_t)control_pa;

a2060 1
	/* XXX dont need this if no msr bitmap support */
d2070 2
a2071 1
	if (!pmap_extract(pmap_kernel(), vcpu->vc_msr_bitmap_va, &control_pa)) {
a2075 2
	vcpu->vc_msr_bitmap_pa = (uint64_t)control_pa;

a2076 1
	/* XXX may not need this with MSR bitmaps */
a2092 1
	/* XXX may not need this with MSR bitmaps */
a2108 1
	/* XXX may not need this with MSR bitmaps */
d3316 8
d3640 87
d3785 1
d3801 2
a3802 1
		      CPUID_PSN | CPUID_SS | CPUID_PBE);
@


1.75
log
@
provide better interrupt responsiveness by exiting to vmd whenever
vcpu->vc_intr is asserted and the guest is ready for interrupts, instead
of waiting until the next time we would have exited to vmd for some other
reason.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.74 2016/09/01 16:04:47 stefan Exp $	*/
d3569 4
a3573 2
		 *  vmx (CPUIDECX_VMX)
		 *  xsave (CPUIDECX_XSAVE)
d3575 8
a3582 2
		 *  XXX - timestamp (CPUID_TSC)
		 *  monitor/mwait (CPUIDECX_MWAIT)
d3584 4
d3589 1
d3596 6
a3601 1
		    CPUIDECX_VMX | CPUIDECX_XSAVE);
d3603 3
a3605 1
		    ~(CPUID_ACPI | CPUID_TM | CPUID_TSC | CPUID_HTT);
d3636 1
a3636 2
		/* Only ARAT is exposed in function 0x06 */
		*rax = TPM_ARAT;
d3643 11
d3655 4
a3658 1
			*rbx = curcpu()->ci_feature_sefflags_ebx;
d3782 1
a3782 1
		*rdx = cpu_apmi_edx;
@


1.74
log
@Make vcpu_reset_regs use new writeregs code

Makes reset code a little simpler. ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.73 2016/09/01 15:01:45 stefan Exp $	*/
a2804 1
	uint8_t from_exit;
a2807 1
	from_exit = vrp->vrp_continue;
d2810 45
a2897 50
		/*
		 * If we are returning from userspace (vmd) because we exited
		 * last time, fix up any needed vcpu state first. Which state
		 * needs to be fixed up depends on what vmd populated in the
		 * exit data structure.
		 */
		if (from_exit) {
			from_exit = 0;
			switch (vcpu->vc_gueststate.vg_exit_reason) {
			case VMX_EXIT_IO:
				vcpu->vc_gueststate.vg_rax =
				    vcpu->vc_exit.vei.vei_data;
				break;
			case VMX_EXIT_HLT:
				break;
			case VMX_EXIT_INT_WINDOW:
				break; 
			case VMX_EXIT_TRIPLE_FAULT:
				DPRINTF("%s: vm %d vcpu %d triple fault\n",
				    __func__, vcpu->vc_parent->vm_id,
				    vcpu->vc_id);
#ifdef VMM_DEBUG
				vmx_vcpu_dump_regs(vcpu);
				dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
				break;
			case VMX_EXIT_ENTRY_FAILED_GUEST_STATE:
				DPRINTF("%s: vm %d vcpu %d failed entry "
				    "due to invalid guest state\n",
				    __func__, vcpu->vc_parent->vm_id,
				    vcpu->vc_id);
#ifdef VMM_DEBUG
				vmx_vcpu_dump_regs(vcpu);
				dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
				return EINVAL;
			default:
				printf("%s: unimplemented exit type %d (%s)\n",
				    __func__,
				    vcpu->vc_gueststate.vg_exit_reason,
				    vmx_exit_reason_decode(
					vcpu->vc_gueststate.vg_exit_reason));
#ifdef VMM_DEBUG
				vmx_vcpu_dump_regs(vcpu);
				dump_vcpu(vcpu);
#endif /* VMM_DEBUG */
				break;
			}
		}

d3043 4
a3046 1
			/* Exit to vmd if we are terminating or failed enter */
d3049 5
@


1.73
log
@Add ioctls to get/set VCPU registers

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.72 2016/09/01 14:45:36 mlarkin Exp $	*/
d120 3
a122 3
int vcpu_reset_regs(struct vcpu *, struct vcpu_init_state *);
int vcpu_reset_regs_vmx(struct vcpu *, struct vcpu_init_state *);
int vcpu_reset_regs_svm(struct vcpu *, struct vcpu_init_state *);
d1410 1
a1410 1
vcpu_reset_regs_svm(struct vcpu *vcpu, struct vcpu_init_state *vis)
d1422 1
a1422 1
 *  vis: the register state to set
d1429 1
a1429 1
vcpu_reset_regs_vmx(struct vcpu *vcpu, struct vcpu_init_state *vis)
a1747 16
	 * The next portion of code sets up the VMCS for the register state
	 * we want during VCPU start. This matches what the CPU state would
	 * be after a bootloader transition to 'start'.
	 */
	if (vmwrite(VMCS_GUEST_IA32_RFLAGS, vis->vis_rflags)) {
		ret = EINVAL;
		goto exit;
	}

	vcpu->vc_gueststate.vg_rip = vis->vis_rip;
	if (vmwrite(VMCS_GUEST_IA32_RIP, vis->vis_rip)) {
		ret = EINVAL;
		goto exit;
	}

	/*
d1750 1
a1750 1
	 * CR0 bits in the vis parameter must match these.
d1762 2
a1763 2
	 * any value for CR0_PG and CR0_PE in vis->vis_cr0 if the CPU has
	 * the unrestricted guest capability.
d1765 1
a1765 1
	cr0 = vis->vis_cr0;
a1772 1

a1788 5
	if (vmwrite(VMCS_GUEST_IA32_CR0, cr0)) {
		ret = EINVAL;
		goto exit;
	}

d1792 1
a1792 6
		cr3 = vis->vis_cr3;

	if (vmwrite(VMCS_GUEST_IA32_CR3, cr3)) {
		ret = EINVAL;
		goto exit;
	}
d1807 3
a1809 189
	if (vmwrite(VMCS_GUEST_IA32_CR4, cr4)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_RSP, vis->vis_rsp)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_SEL, vis->vis_ss.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_LIMIT, vis->vis_ss.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_AR, vis->vis_ss.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_BASE, vis->vis_ss.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_SEL, vis->vis_ds.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_LIMIT, vis->vis_ds.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_AR, vis->vis_ds.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_BASE, vis->vis_ds.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_SEL, vis->vis_es.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_LIMIT, vis->vis_es.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_AR, vis->vis_es.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_BASE, vis->vis_es.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_SEL, vis->vis_fs.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_LIMIT, vis->vis_fs.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_AR, vis->vis_fs.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_BASE, vis->vis_fs.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_SEL, vis->vis_gs.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_LIMIT, vis->vis_gs.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_AR, vis->vis_gs.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_BASE, vis->vis_gs.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_SEL, vis->vis_cs.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_LIMIT, vis->vis_cs.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_AR, vis->vis_cs.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_BASE, vis->vis_cs.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GDTR_LIMIT, vis->vis_gdtr.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GDTR_BASE, vis->vis_gdtr.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_IDTR_LIMIT, vis->vis_idtr.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_IDTR_BASE, vis->vis_idtr.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_SEL, vis->vis_ldtr.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_LIMIT, vis->vis_ldtr.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_AR, vis->vis_ldtr.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_BASE, vis->vis_ldtr.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_SEL, vis->vis_tr.vsi_sel)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_LIMIT, vis->vis_tr.vsi_limit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_AR, vis->vis_tr.vsi_ar)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_BASE, vis->vis_tr.vsi_base)) {
		ret = EINVAL;
		goto exit;
	}
d1901 7
d2132 1
a2132 1
 *  vis: the desired register state
d2140 1
a2140 1
vcpu_reset_regs(struct vcpu *vcpu, struct vcpu_init_state *vis)
d2146 1
a2146 1
		ret = vcpu_reset_regs_vmx(vcpu, vis);
d2149 1
a2149 1
		ret = vcpu_reset_regs_svm(vcpu, vis);
@


1.72
log
@
Remove the clock hack, and properly handle interrupts generated from vmd(8)'s
forthcoming emulated interrupt controller.

ok stefan
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.71 2016/07/23 07:25:29 mlarkin Exp $	*/
d115 5
d196 29
d394 6
d583 61
d1142 262
@


1.71
log
@
Fix a few CPUID emulation issues:

Don't advertise a hyperthreaded CPU. This doesn't make a lot of sense now
as we only provide UP guest support. This, combined with the other CPUID
issues fixed, fooled NetBSD's topology enumeration code into thinking we
had an unsupportable core/thread/package configuration.

Also fixed the unsupported CPUID functions by returning 0 in the return
registers instead of leaving whatever trash happened to be there before
the call was made.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.70 2016/07/23 07:17:21 mlarkin Exp $	*/
d43 1
a43 1
#ifdef VMM_DEBUG 
d122 2
a123 2
int vcpu_run_vmx(struct vcpu *, uint8_t, int16_t *);
int vcpu_run_svm(struct vcpu *, uint8_t);
a200 4
/* XXX Temporary hack for the PIT clock */
#define CLOCK_BIAS 8192
uint64_t vmmclk = 0;

d480 1
a480 1
 * interrupt is pending and needs acknowledgment.
d534 2
a535 1
	if (vcpu->vc_state == VCPU_STATE_RUNNING) {
a536 1
	}
a1393 9
	/*
	 * XXX -
	 * vg_rip gets special treatment here since we will rewrite
	 * it just before vmx_enter_guest, so it needs to match.
	 * we could just set vg_rip here and be done with (no vmwrite
	 * here) but that would require us to have proper resume
	 * handling (resume=1) in the exit handler, so for now we
	 * will just end up doing an extra vmwrite here.
	 */
d2564 1
a2564 1
		ret = vcpu_run_vmx(vcpu, vrp->vrp_continue, &vrp->vrp_injint);
d2567 1
a2567 1
		ret = vcpu_run_svm(vcpu, vrp->vrp_continue);
d2585 1
d2631 1
a2631 5
 *  from_exit: 1 if returning directly from an exit to vmd during the
 *      previous run, or 0 if we exited last time without needing to
 *      exit to vmd.
 *  injint: Interrupt that should be injected during this run, or -1 if
 *      no interrupt should be injected.
d2639 1
a2639 1
vcpu_run_vmx(struct vcpu *vcpu, uint8_t from_exit, int16_t *injint)
d2647 3
a2649 1
	uint64_t rflags, eii;
d2652 2
d2700 3
a2702 1
		 * last time, fix up any needed vcpu state first.
d2713 2
d2748 2
a2749 20
		/*
		 * XXX - clock hack. We don't track host clocks while not
		 * running inside a VM, and thus we lose many clocks while
		 * the OS is running other processes. For now, approximate
		 * when a clock should be injected by injecting one clock
		 * per CLOCK_BIAS exits.
		 *
		 * This should be changed to track host clocks to know if
		 * a clock tick was missed, and "catch up" clock interrupt
		 * injections later as needed.
		 *
		 * Note that checking injint here and not injecting the
		 * clock interrupt if injint is set also violates interrupt
		 * priority, until this hack is fixed.
		 */
		vmmclk++;
		eii = 0xFFFFFFFFFFFFFFFFULL;

		if (vmmclk % CLOCK_BIAS == 0)
			eii = 0x20;
d2751 8
a2758 2
		if (*injint != -1)
			eii = *injint + 0x20;
d2760 8
a2767 4
		if (eii != 0xFFFFFFFFFFFFFFFFULL) {
			if (vmread(VMCS_GUEST_IA32_RFLAGS, &rflags)) {
				printf("intr: can't read guest rflags\n");
				rflags = 0;
d2770 17
a2786 6
			if (rflags & PSL_I) {
				eii |= (1ULL << 31);	/* Valid */
				eii |= (0ULL << 8);	/* Hardware Interrupt */
				if (vmwrite(VMCS_ENTRY_INTERRUPTION_INFO, eii)) {
					printf("intr: can't vector clock "
					    "interrupt to guest\n");
a2787 2
				if (*injint != -1)
					*injint = -1;
a2790 2
		/* XXX end clock hack */

d2796 1
a2796 1
		/* Start / resume the VM / VCPU */
d2803 5
a2807 1
		if (ret == 0)
d2810 8
d2867 26
d2919 1
a2919 1
			    "reason\n");
a2921 1

d2926 3
a2928 3
	 * handling an exit, or we failed in some way to enter the guest.
	 * Clear any current VMCS pointer as we may end up coming back on
	 * a different CPU.
a2965 1

d3016 1
a3016 1
	uint64_t exit_reason;
d3021 1
d3024 11
d3300 1
d3302 2
a3309 8
	case IO_RTC ... IO_RTC + 1:
		/* We can directly read the RTC on behalf of the guest */
		if (vcpu->vc_exit.vei.vei_dir == 1) {
			vcpu->vc_gueststate.vg_rax =
			    inb(vcpu->vc_exit.vei.vei_port);
		}
		ret = 0;
		break;
d3620 1
a3620 1
vcpu_run_svm(struct vcpu *vcpu, uint8_t from_exit)
@


1.70
log
@
Dump vcpu state on unknown exit type, and add a diagnostic message
(including vcpu state dump) on failure to enter due to an incorrect
guest state.

Added as a debug facility when diagnosing interruptibility state
problems seen while testing NetBSD guest VMs.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.69 2016/07/23 07:00:39 mlarkin Exp $	*/
d3384 1
d3393 1
a3393 1
		    ~(CPUID_ACPI | CPUID_TM | CPUID_TSC);
d3398 4
d3412 4
d3447 4
d3461 4
d3469 4
d3477 4
d3485 4
d3493 4
d3501 4
d3562 4
d3569 4
@


1.69
log
@
Ensure some undesirable entry controls are cleared, instead of relying
on the default settings.

Noticed when booting a NetBSD guest VM.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.68 2016/07/16 18:36:41 mlarkin Exp $	*/
d43 1
a43 1
#ifdef VMM_DEBUG
d2724 7
d2732 10
d2743 2
a2744 2
				printf("vcpu_run_vmx: returning from exit "
				    "with unknown reason %d (%s)\n",
d2748 4
@


1.68
log
@
remove an unused parameter that wasn't handled during a previous refactor
(reducing the number of XXXs in vmm)
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.67 2016/07/16 06:32:18 mlarkin Exp $	*/
d1327 2
a1328 1
	 * XXX clear load debug_ctrls on entry ?
d1335 3
a1337 1
	    IA32_VMX_DEACTIVATE_DUAL_MONITOR_TREATMENT;
@


1.67
log
@
remove unused vmm_activate function. all vmm-specific suspend/resume needs
are handled during cpu hatch.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.66 2016/07/13 06:57:35 mlarkin Exp $	*/
d135 1
a135 2
int vcpu_vmx_compute_ctrl(struct vcpu *, uint64_t, uint16_t, uint32_t,
    uint32_t, uint32_t *);
d1181 1
a1181 2
	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0,
	    &pinbased)) {
d1230 1
a1230 2
	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0,
	    &procbased)) {
d1279 1
a1279 2
	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0,
	    &procbased2)) {
d1309 1
a1309 1
	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0, &exit)) {
d1344 1
a1344 1
	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0, &entry)) {
a2217 1
 *  vcpu: the vcpu for which controls are to be computed. (XXX now unused)
d2229 2
a2230 2
vcpu_vmx_compute_ctrl(struct vcpu *vcpu, uint64_t ctrlval, uint16_t ctrl,
    uint32_t want1, uint32_t want0, uint32_t *out)
@


1.66
log
@
CPUID vendor ID was already register-swapped during initial query, so no
need to do it again.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.65 2016/06/28 05:50:55 mlarkin Exp $	*/
a102 1
int vmm_activate(struct device *, int);
d189 1
a189 1
	sizeof(struct vmm_softc), vmm_probe, vmm_attach, NULL, vmm_activate
a308 13
}

/*
 * vmm_activate
 *
 * Autoconf routine used during activate/deactivate.
 *
 * XXX need this for suspend/resume
 */
int
vmm_activate(struct device *self, int act)
{
	return 0;
@


1.65
log
@
Don't panic on null vmxon region or vmxon failure - this will be handled
in the calling function which will return EIO back up the stack to vmd(8).

Fixes a panic when CPUs fail to spin up for other reasons during boot,
noticed by reyk.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.64 2016/06/10 16:37:16 stefan Exp $	*/
d3362 2
a3363 2
		*rcx = *((uint32_t *)&cpu_vendor + 1);
		*rdx = *((uint32_t *)&cpu_vendor + 2);
@


1.64
log
@Remove readpage and writepage ioctls.

They are not needed anymore now that guest memory is allocated
by and shared with the host.
ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.63 2016/06/07 16:19:06 stefan Exp $	*/
d688 1
a688 1
			panic("NULL vmxon region specified\n");
d712 1
a712 1
				panic("VMXON failed\n");
@


1.63
log
@Allocate RAM for guest VM in vmd(8) and pass it to vmm(4)

vmm(4) then maps the pages allocated by the vmd(8) user process into the
address space of the guest. This gives vmm(4) and vmd(8) a shared view of
the guest RAM. This will allow us to have faster guest<->host data
exchange by ordinary memory loads/stores later, as well as remove the
vm_readpage and vm_writepage ioctls next.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.62 2016/04/27 15:25:36 mlarkin Exp $	*/
a113 2
int vm_writepage(struct vm_writepage_params *);
int vm_readpage(struct vm_readpage_params *);
a372 6
	case VMM_IOC_WRITEPAGE:
		ret = vm_writepage((struct vm_writepage_params *)data);
		break;
	case VMM_IOC_READPAGE:
		ret = vm_readpage((struct vm_readpage_params *)data);
		break;
a405 2
	case VMM_IOC_WRITEPAGE:
	case VMM_IOC_READPAGE:
a424 83
 * vm_readpage
 *
 * Reads a region (PAGE_SIZE max) of guest physical memory using the parameters
 * defined in 'vrp'.
 *
 * Returns 0 if successful, or various error codes on failure:
 *  ENOENT if the VM id contained in 'vrp' refers to an unknown VM
 *  EINVAL if the memory region described by vrp is not regular memory
 *  EFAULT if the memory region described by vrp has not yet been faulted in
 *      by the guest
 */
int
vm_readpage(struct vm_readpage_params *vrp)
{
	struct vm *vm;
	paddr_t host_pa;
	void *kva;
	vaddr_t vr_page;

	/* Find the desired VM */
	rw_enter_read(&vmm_softc->vm_lock);
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vrp->vrp_vm_id)
			break;
	}

	/* Not found? exit. */
	if (vm == NULL) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (ENOENT);
	}

	/* Check that the data to be read is within a page */
	if (vrp->vrp_len > (PAGE_SIZE - (vrp->vrp_paddr & PAGE_MASK))) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (EINVAL);
	}

	/* Calculate page containing vrp->vrp_paddr */
	vr_page = vrp->vrp_paddr & ~PAGE_MASK;

	/* If not regular memory, exit. */
	if (vmm_get_guest_memtype(vm, vr_page) != VMM_MEM_TYPE_REGULAR) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (EINVAL);
	}

	/* Find the phys page where this guest page exists in real memory */
	if (!pmap_extract(vm->vm_map->pmap, vr_page, &host_pa)) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (EFAULT);
	}

	/* Allocate temporary KVA for the guest page */
	kva = km_alloc(PAGE_SIZE, &kv_any, &kp_none, &kd_nowait);
	if (!kva) {
		DPRINTF("vm_readpage: can't alloc kva\n");
		rw_exit_read(&vmm_softc->vm_lock);
		return (EFAULT);
	}

	/* Enter the mapping in the kernel pmap and copyout */
	pmap_kenter_pa((vaddr_t)kva, host_pa, PROT_READ);

	if (copyout(kva + ((vaddr_t)vrp->vrp_paddr & PAGE_MASK),
	    vrp->vrp_data, vrp->vrp_len) == EFAULT) {
		DPRINTF("vm_readpage: can't copyout\n");
		pmap_kremove((vaddr_t)kva, PAGE_SIZE);
		km_free(kva, PAGE_SIZE, &kv_any, &kp_none);
		rw_exit_read(&vmm_softc->vm_lock);
		return (EFAULT);
	}

	/* Cleanup and exit */
	pmap_kremove((vaddr_t)kva, PAGE_SIZE);
	km_free(kva, PAGE_SIZE, &kv_any, &kp_none);

	rw_exit_read(&vmm_softc->vm_lock);

	return (0);
}

/*
a556 106

	return (0);
}

/*
 * vm_writepage
 *
 * Writes a region (PAGE_SIZE max) of guest physical memory using the parameters
 * defined in 'vrp'.
 *
 * Returns 0 if successful, or various error codes on failure:
 *  ENOENT if the VM id contained in 'vrp' refers to an unknown VM
 *  EINVAL if the memory region described by vrp is not regular memory
 *  EFAULT if the source data in vrp contains an invalid address
 *  ENOMEM if a memory allocation error occurs
 */
int
vm_writepage(struct vm_writepage_params *vwp)
{
	char *pagedata;
	struct vm *vm;
	paddr_t host_pa;
	void *kva;
	int ret;
	vaddr_t vw_page, dst;

	/* Find the desired VM */
	rw_enter_read(&vmm_softc->vm_lock);
	SLIST_FOREACH(vm, &vmm_softc->vm_list, vm_link) {
		if (vm->vm_id == vwp->vwp_vm_id)
			break;
	}

	/* Not found? exit. */
	if (vm == NULL) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (ENOENT);
	}

	/* Check that the data to be written is within a page */
	if (vwp->vwp_len > (PAGE_SIZE - (vwp->vwp_paddr & PAGE_MASK))) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (EINVAL);
	}

	/* Calculate page containing vwp->vwp_paddr */
	vw_page = vwp->vwp_paddr & ~PAGE_MASK;

	/* If not regular memory, exit. */
	if (vmm_get_guest_memtype(vm, vw_page) != VMM_MEM_TYPE_REGULAR) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (EINVAL);
	}

	/* Allocate temporary region to copyin into */
	pagedata = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT|M_ZERO);
	if (pagedata == NULL) {
		rw_exit_read(&vmm_softc->vm_lock);
		return (ENOMEM);
	}

	/* Copy supplied data to kernel */
	if (copyin(vwp->vwp_data, pagedata, vwp->vwp_len) == EFAULT) {
		free(pagedata, M_DEVBUF, PAGE_SIZE);
		rw_exit_read(&vmm_softc->vm_lock);
		return (EFAULT);
	}

	/* Find the phys page where this guest page exists in real memory */
	if (!pmap_extract(vm->vm_map->pmap, vw_page, &host_pa)) {
		/* page not present */
		ret = uvm_fault(vm->vm_map, vw_page,
		    VM_FAULT_INVALID, PROT_READ | PROT_WRITE | PROT_EXEC);
		if (ret) {
			free(pagedata, M_DEVBUF, PAGE_SIZE);
			rw_exit_read(&vmm_softc->vm_lock);
			return (EFAULT);
		}

		if (!pmap_extract(vm->vm_map->pmap, vw_page, &host_pa)) {
			panic("vm_writepage: still not mapped GPA 0x%llx\n",
			    (uint64_t)vwp->vwp_paddr);
		}
	}

	/* Allocate kva for guest page */
	kva = km_alloc(PAGE_SIZE, &kv_any, &kp_none, &kd_nowait);
	if (kva == NULL) {
		DPRINTF("vm_writepage: can't alloc kva\n");
		free(pagedata, M_DEVBUF, PAGE_SIZE);
		rw_exit_read(&vmm_softc->vm_lock);
		return (ENOMEM);
	}

	/* Enter mapping and copy data */
	pmap_kenter_pa((vaddr_t)kva, host_pa, PROT_READ | PROT_WRITE);
	dst = (vaddr_t)kva + ((vaddr_t)vwp->vwp_paddr & PAGE_MASK);
	memcpy((void *)dst, pagedata, vwp->vwp_len);

	/* Cleanup */
	pmap_kremove((vaddr_t)kva, PAGE_SIZE);
	km_free(kva, PAGE_SIZE, &kv_any, &kp_none);

	free(pagedata, M_DEVBUF, PAGE_SIZE);

	rw_exit_read(&vmm_softc->vm_lock);
@


1.62
log
@
minor spacing nit
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.61 2016/04/26 16:11:27 mlarkin Exp $	*/
d130 3
a132 3
int vm_impl_init(struct vm *);
int vm_impl_init_vmx(struct vm *);
int vm_impl_init_svm(struct vm *);
d986 1
a986 1
		if ((vmr->vmr_gpa & PAGE_MASK) ||
d995 11
d1071 1
a1071 1
	if (vm_impl_init(vm)) {
d1125 1
a1125 1
vm_impl_init_vmx(struct vm *vm)
a1128 1
	vaddr_t startp;
d1164 1
a1164 3
		startp = vmr->vmr_gpa;
		ret = uvm_mapanon(vm->vm_map, &startp, vmr->vmr_size, 0,
		    UVM_MAPFLAG(PROT_READ | PROT_WRITE | PROT_EXEC,
d1166 1
a1166 3
		    MAP_INHERIT_NONE,
		    MADV_NORMAL,
		    UVM_FLAG_FIXED | UVM_FLAG_COPYONW));
d1168 1
a1168 1
			printf("vm_impl_init_vmx: uvm_mapanon failed (%d)\n",
d1196 1
a1196 1
vm_impl_init_svm(struct vm *vm)
d1208 1
a1208 1
vm_impl_init(struct vm *vm)
d1212 1
a1212 1
		return vm_impl_init_vmx(vm);
d1215 1
a1215 1
		return vm_impl_init_svm(vm);
@


1.61
log
@
missed a block of code while merging the previous change.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.60 2016/04/26 15:57:09 mlarkin Exp $	*/
d4616 1
a4616 1
		{ EFER_NXE, "NXE ", "nxe "},
@


1.60
log
@
Add decode functions for some of the MSRs that are commonly used. Only
compiled when VMM_DEBUG is enabled, and only used during VM crash.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.59 2016/04/26 11:59:21 mlarkin Exp $	*/
d4259 1
a4259 1
		    "value=0x%016llx\n",
d4263 2
@


1.59
log
@
KNF / spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.58 2016/04/25 19:53:45 mlarkin Exp $	*/
d30 1
d167 10
d4282 4
a4285 1
	/* Add as needed ... */
d4486 156
@


1.58
log
@
when returning from vmd with an unknown exit reason, print the name of the
exit in addition to the exit code.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.57 2016/04/25 19:26:27 mlarkin Exp $	*/
d635 3
a637 3
        if (vcpu->vc_state == VCPU_STATE_RUNNING) {
                x86_send_ipi(vcpu->vc_last_pcpu, X86_IPI_NOP);
        }
a638 1

@


1.57
log
@
spacing / KNF error in earlier commit
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.56 2016/04/25 17:50:21 mlarkin Exp $	*/
d2926 4
a2929 2
				    "with unknown reason %d\n",
				    vcpu->vc_gueststate.vg_exit_reason);
@


1.56
log
@
cr0, cr3, cr4 diagnostics / debug functions (used when VMs crash)
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.55 2016/04/25 15:24:55 mlarkin Exp $	*/
d170 1
a170 1
	const char 	*vrdi_absent;
@


1.55
log
@
add a few new exit types found in newer cpus
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.54 2016/04/17 00:15:28 dlg Exp $	*/
d163 9
d3981 6
a3986 3
		DPRINTF("(error reading)   ");
	else
		DPRINTF("0x%016llx", r);
d3991 6
a3996 3
		DPRINTF("(error reading)   ");
	else
		DPRINTF("0x%016llx", r);
d4000 4
a4003 2
	else
		DPRINTF("0x%016llx\n", r);
d4374 97
@


1.54
log
@add pool_setipl after pool_init.

ok mlarkin@@ stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.53 2016/04/13 04:44:41 mlarkin Exp $	*/
d3781 3
@


1.53
log
@
add missing arguments to debug printf
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.52 2016/04/12 06:41:09 mlarkin Exp $	*/
d286 1
d289 1
@


1.52
log
@
Call debug register dump functions in error conditions (if VMM_DEBUG set)
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.51 2016/04/12 04:42:58 mlarkin Exp $	*/
d3186 2
a3187 1
		DPRINTF("vmx_handle_exit: vm %d vcpu %d triple fault\n");
@


1.51
log
@
Only build the vcpu register dump functions if VMM_DEBUG is enabled, no
need for these in non-debug scenarios
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.50 2016/04/11 07:34:55 mlarkin Exp $	*/
d555 5
a559 1
	if (vcpu_reset_regs(vcpu, &vrp->vrp_init_state))
d561 1
d2745 1
d3029 1
d3185 5
@


1.50
log
@
Add functions to dump vcpu register state. Needed for some upcoming diffs.
Not presently used.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.49 2016/04/11 06:58:34 mlarkin Exp $	*/
d157 2
d163 1
d3818 1
d4340 1
@


1.49
log
@
Clarify some comments
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.48 2016/04/06 06:15:06 mlarkin Exp $	*/
d158 3
d3905 429
@


1.48
log
@
define number of exit/entry save/load MSRs as a #define instead of a magic
number.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.47 2016/04/05 09:33:05 mlarkin Exp $	*/
d2703 5
d2739 3
a2741 1
	 * If so (vrp_continue == 1), copy in the exit data from vmd.
@


1.47
log
@
Support processors without unrestricted guest capability.

ok stefan
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.46 2016/04/04 16:47:34 stefan Exp $	*/
d1897 5
a1901 1
	if (vmwrite(VMCS_EXIT_MSR_STORE_COUNT, 0x7)) {
d1906 1
a1906 1
	if (vmwrite(VMCS_EXIT_MSR_LOAD_COUNT, 0x7)) {
d1911 1
a1911 1
	if (vmwrite(VMCS_ENTRY_MSR_LOAD_COUNT, 0x7)) {
@


1.46
log
@Deallocate guest vm_map when the guest gets terminated.

That way we no longer leak uvm data structures after a
guest VM was shut down.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.45 2016/03/13 13:11:47 stefan Exp $	*/
d1248 1
a1248 1
	int ret;
d1252 1
a1252 1
	uint64_t msr, ctrlval, eptp, vmcs_ptr;
d1257 1
d1447 1
a1447 1
		    IA32_VMX_UNRESTRICTED_GUEST, 1))
d1449 2
d1501 2
d1508 4
a1511 1
	want1 = 0;
a1540 1

d1607 6
a1612 6
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_UNRESTRICTED_GUEST, 1))
			want1 &= ~(CR0_PG | CR0_PE);
			want0 &= ~(CR0_PG | CR0_PE);
a1614 1
	cr0 = vis->vis_cr0;
d1637 6
a1642 1
	if (vmwrite(VMCS_GUEST_IA32_CR3, vis->vis_cr3)) {
d1654 6
d1873 5
d1879 5
a1883 1
	msr_store[0].vms_data = 0ULL;		/* Initial value */
@


1.45
log
@Introduce memory ranges to support VMs with >= 4G RAM

Kernel bits:
- When creating a VM, a list of memory ranges has to be specified,
  similar to the BIOS memory map. This is necessary for VMs with
  RAM sizes approaching 4G because we'll need PCI MMIO space in
  the higher parts of the 32 bit address space.

vmctl and vmd bits:
- Construct appropriate memory ranges to create a VM with a given
  RAM size
- Construct a corresponding BIOS memory map from the memory ranges
  and update the boot params page accordingly.
- Make sure that all variables that represent guest physical addresses
  match the address width of the target CPU instead of using uint32_t.
- Fix some integer promotion glitches that actually restricted VM
  RAM size to 2G.

This changes the VM create ioctl interface, so update your kernel,
vmd, and vmctl.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.44 2016/03/09 07:41:25 mlarkin Exp $	*/
d2275 3
a2277 1
	/* XXX teardown guest vmspace, free pages */
@


1.44
log
@
Induce an exit in a running vcpu if an interrupt is asserted (pending).
Needed for ongoing interrupt controller work.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.43 2016/03/08 08:43:50 mlarkin Exp $	*/
d66 2
a67 1
	uint32_t		 vm_memory_size;
d69 1
d108 1
d926 61
d1002 1
d1009 4
d1022 4
a1025 1
	vm->vm_memory_size = vcp->vcp_memory_size;
d1084 3
d1088 1
a1088 3
	size_t memsize;
	vaddr_t startp;
	int ret;
a1100 3
	startp = 0;
	memsize = vm->vm_memory_size * 1024 * 1024;

d1105 5
a1109 1
	vm->vm_map = uvm_map_create(pmap, 0, memsize,
d1120 17
a1136 12
	ret = uvm_mapanon(vm->vm_map, &startp, memsize, 0,
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE | PROT_EXEC,
	    PROT_READ | PROT_WRITE | PROT_EXEC,
	    MAP_INHERIT_NONE,
	    MADV_NORMAL,
	    UVM_FLAG_FIXED | UVM_FLAG_COPYONW));
	if (ret) {
		printf("vm_impl_init_vmx: uvm_mapanon failed (%d)\n", ret);
		/* uvm_map_deallocate calls pmap_destroy for us */
		uvm_map_deallocate(vm->vm_map);
		vm->vm_map = NULL;
		return (ENOMEM);
d3161 2
d3169 13
a3181 5
	if (gpa < vm->vm_memory_size * (1024 * 1024))
		return (VMM_MEM_TYPE_REGULAR);
	else {
		DPRINTF("guest memtype @@ 0x%llx unknown\n", (uint64_t)gpa);
		return (VMM_MEM_TYPE_UNKNOWN);
d3183 3
@


1.43
log
@
Change where we note the 'last' pcpu we ran on. This is needed for upcoming
work in revamping the interrupt controller code.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.42 2016/03/08 08:36:40 mlarkin Exp $	*/
d596 19
@


1.42
log
@
Remove an outdated comment and fix some error case printfs that were
printing the wrong function name.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.41 2016/03/08 07:10:01 mlarkin Exp $	*/
d2716 2
a2841 1
			vcpu->vc_last_pcpu = ci;
@


1.41
log
@
additional debug printfs
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.40 2016/03/03 18:45:42 stefan Exp $	*/
a1476 4
	 *
	 * This can now change from the hardcoded value of 0x1000160
	 * to the marks[start] from vmd's bootloader. That needs to
	 * be hoisted up into vcpu create parameters via vm create params.
d2763 1
a2763 1
				printf("vmx_enter_guest: returning from exit "
d2893 1
a2893 1
			printf("vmx_enter_guest: failed launch with invalid "
d2898 1
a2898 1
			printf("vmx_enter_guest: failed launch with valid "
d2903 1
a2903 1
			printf("vmx_enter_guest: failed launch for unknown "
@


1.40
log
@VM guest memory is allocated via an uvm anon memory range.
Allocate management data structures (amap) lazily by specifying
the UVM_FLAG_COPYONW flag instead of UVM_FLAG_OVERLAY when
creating the anon.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.39 2016/03/03 18:23:06 stefan Exp $	*/
d151 1
d357 1
d515 3
a517 1
	if (vm == NULL)
d519 1
d528 3
a530 1
	if (vcpu == NULL)
d532 1
d534 6
a539 1
	if (vcpu->vc_state != VCPU_STATE_STOPPED)
d541 1
d3644 18
@


1.39
log
@Re-introduce computation of memory used by a VM.
This got accidently removed by me in r1.31.

noticed by and ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.38 2016/02/23 17:17:31 stefan Exp $	*/
d1018 1
a1018 1
	    UVM_FLAG_FIXED | UVM_FLAG_OVERLAY));
@


1.38
log
@Fix previous: Need to free vc_msr_bitmap_va, not
vc_msr_bitmap_pa.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.37 2016/02/23 17:15:09 stefan Exp $	*/
a66 1
	size_t			 vm_used_size;
d2459 2
a2460 1
		out[i].vir_used_size = vm->vm_used_size;
@


1.38.2.1
log
@Add the vmm bits of previous commit to 5.9 -stable branch:

Make sure that amap slot calculation does not overflow

This prevents too small amaps from being allocated by
forcing the allocation of a large number of slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

This is a different patch for 5.9 that addresses the same
issue as r1.75 of uvm/uvm_amap.c. It also makes sure that
vmm(4) cannot make such large amap allocation requests.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.38 2016/02/23 17:17:31 stefan Exp $	*/
d1019 1
a1019 1
	    UVM_FLAG_FIXED));
@


1.37
log
@Plug memleak: make sure that the MSR bitmap is freed in the VCPU init
error path.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.36 2016/02/20 20:49:08 mlarkin Exp $	*/
d2003 2
a2004 2
		if (vcpu->vc_msr_bitmap_pa)
			km_free((void *)vcpu->vc_msr_bitmap_pa, PAGE_SIZE,
@


1.36
log
@
Add "interrupt pending on vcpu" ioctl to vmm. Needed for upcoming interrupt
controller work in vmd(8).

ok stefan@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.35 2016/02/16 18:59:30 stefan Exp $	*/
d2002 3
@


1.35
log
@Allow userland to initialize CR0 when resetting a VCPU instead of
hardcoding it. Be careful to obey VMX's must-be-0 and must-be-1
restrictions for CR0.
This gives us the opportunity later to start VCPUs in real-mode, etc.
(for those CPUs that support unrestricted guest).

Be sure to update your vmd(8) also, the ioctl interface has
changed.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.34 2016/02/08 18:23:04 stefan Exp $	*/
d114 1
d353 3
d535 48
@


1.34
log
@Set EPT bits of guest pages in pmap_enter instead of doing it
after an uvm_fault: uvm_fault maps in neighboring pages of
the faulting page. We want EPT bits set for those as soon as
possible as well. This avoids additional EPT violations
causing further uvm_faults when the guest accesses the
neighboring pages.

discussion with and ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.33 2016/01/29 00:47:51 jsg Exp $	*/
d1424 3
a1426 2
	 * Determine default CR0 as per Intel SDM A.7
	 * All flexible bits are set to 0
d1428 2
a1429 1
	cr0 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed0) &
d1431 2
a1432 1
	cr0 |= (CR0_CD | CR0_NW | CR0_ET);
d1434 7
d1445 20
a1464 2
//			cr0 &= ~(CR0_PG);
			cr0 &= ~(CR0_PG | CR0_PE);
@


1.33
log
@Move a pool_put() to avoid a use after free.
ok mlarkin@@ stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.32 2016/01/25 12:44:16 jsg Exp $	*/
a150 1
int vmx_fix_ept_pte(struct pmap *, vaddr_t);
a635 7
	/* Fixup the EPT map for this page */
	if (vmx_fix_ept_pte(vm->vm_map->pmap, vw_page)) {
		DPRINTF("vm_writepage: cant fixup ept pte for gpa 0x%llx\n",
		    (uint64_t)vwp->vwp_paddr);
		rw_exit_read(&vmm_softc->vm_lock);
		return (EFAULT);
	}
a3063 1
	struct pmap *pmap;
d3073 1
a3073 7
	if (!ret) {
		pmap = vcpu->vc_parent->vm_map->pmap;
		if (vmx_fix_ept_pte(pmap, gpa)) {
			printf("vmx_fault_page: ept fixup failure\n");
			ret = EINVAL;
		}
	} else {
a3074 1
	}
a3445 16
	return (0);
}

/*
 * vmx_fix_ept_pte
 *
 * Fixes up the pmap PTE entry for 'addr' to reflect proper EPT format
 */
int
vmx_fix_ept_pte(struct pmap *pmap, vaddr_t addr)
{
	int offs, level;

	level = pmap_fix_ept(pmap, addr, &offs);
	KASSERT(level == 0);

@


1.32
log
@Zero when malloc'ing a buffer to be copyed out to userland to avoid
an information leak when not all of the buffer is written to.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.31 2016/01/10 18:18:25 stefan Exp $	*/
a2094 2
	pool_put(&vm_pool, vm);

d2099 1
@


1.31
log
@Page fault handling tweaks for vmm:
- compute fault reason for uvm_fault() (e.g. page not present,
  protection violation) instead of passing a protection code
- a page does not need to be zero'd after faulting it in. uvm_fault() does
  that for fresh anon pages already, and we also do not want a page that
  is swapped back in have its contents wiped.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.30 2016/01/08 11:20:58 reyk Exp $	*/
d2375 1
a2375 1
	out = malloc(need, M_DEVBUF, M_NOWAIT);
@


1.30
log
@Add "vmm" pledge to allow restricted ioctl access to /dev/vmm.

This will allow to pledge vmd(8)'s vmm and vm processes, so that VMs
themselves run "sandboxed", including their host-side virtio layer.
It will remain disabled for now (in userland) to not get into the way
of ongoing development and upcoming changes in vmd and the ioctl
interface.

OK mlarkin@@ deraadt@@ "kernel side in, but not the callers in userland"
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.29 2016/01/04 01:35:56 mlarkin Exp $	*/
d604 1
a604 1
		    PROT_WRITE, PROT_READ | PROT_WRITE | PROT_EXEC);
d3035 4
d3041 1
a3041 1
		return (EINVAL);
d3043 5
a3047 8
	if (exit_qualification & IA32_VMX_EPT_FAULT_WRITE)
		return (PROT_WRITE);
	else if (exit_qualification & IA32_VMX_EPT_FAULT_READ)
		return (PROT_READ);
	else if (exit_qualification & IA32_VMX_EPT_FAULT_EXEC)
		return (PROT_EXEC);
	else
		return (EINVAL);
d3060 1
a3060 1
	return (EINVAL);
a3072 2
	vaddr_t kva;
	paddr_t host_pa;
d3076 1
a3076 1
	if (fault_type == EINVAL) {
d3085 1
a3085 21
		if (!vmx_fix_ept_pte(pmap, gpa)) {
			if (pmap_extract(pmap, (vaddr_t)gpa, &host_pa)) {
				kva = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any, 
				    &kp_none, &kd_nowait);
				if (kva) {
					pmap_kenter_pa(kva, host_pa,
					    PROT_READ | PROT_WRITE);
					bzero((void *)kva, PAGE_SIZE);
					pmap_kremove(kva, PAGE_SIZE);
					km_free((void *)kva, PAGE_SIZE, &kv_any,
					    &kp_none);
					vcpu->vc_parent->vm_used_size += PAGE_SIZE;
				} else {
					printf("vmx_fault_page: kva failure\n");
					ret = ENOMEM;
				}
			} else {
				printf("vmx_fault_page: extract failure\n");
				ret = EFAULT;
			}
		} else {
@


1.29
log
@
Do proper termination of VMs by doing proper VCPU run state management.

This should fix some of the odd termination errors people have been seeing
(vmctl status showing running VMs after they have exited/crashed, and
invalid instruction panics on vmptrld during certain races)

This diff also implements dropping the biglock when running a VCPU, and
reacquiring the lock as needed based on the type of exit (normal vs.
external interrupt)

diff supplied by Stefan Kempf <sn.kempf at t-online.de>, many thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.28 2015/12/24 09:40:27 mlarkin Exp $	*/
d29 1
d358 28
@


1.28
log
@
Normalize return values from various exit functions.

Another diff from Stefan Kempf <sn.kempf at t-online.de>.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.27 2015/12/24 09:26:45 mlarkin Exp $	*/
d21 1
d55 6
d71 1
d119 1
d135 2
a136 1
int vmx_handle_exit(struct vcpu *, int *);
a332 2
		if (vmm_softc->vm_ct < 1)
			vmm_stop();
a341 2
		if (vmm_softc->vm_ct < 1)
			vmm_stop();
d868 1
d1975 1
a2053 2
	/* XXX coordinate a stop of all VCPUs first */

a2060 1
	rw_exit_write(&vm->vm_vcpu_lock);
d2067 5
d2395 1
d2409 10
a2418 1
			vcpu->vc_state = VCPU_STATE_REQSTOP;
a2428 1
	vmm_softc->vm_ct--;
d2431 2
a2432 1
	vm_teardown(vm);
d2448 1
d2468 4
a2471 1
			if (vcpu->vc_state != VCPU_STATE_STOPPED)
d2474 1
a2474 1
				vcpu->vc_state = VCPU_STATE_RUNNING;
d2509 14
a2522 2
	/* If we are exiting, populate exit data so vmd can help */
	if (ret == EAGAIN) {
d2524 1
d2531 1
a2531 1
	} else
d2533 5
d2543 20
d2583 1
a2583 1
	int ret, resume, exit_handled;
a2590 1
	exit_handled = 1;
d2593 1
a2593 1
	while (exit_handled) {
d2605 1
a2605 1
				goto exit;
d2610 1
a2610 1
				goto exit;
d2616 1
a2616 1
				goto exit;
d2623 1
a2623 1
				goto exit;
d2630 1
a2630 1
				goto exit;
d2707 2
a2708 1
		/* XXX unlock the biglock here */
d2711 12
a2722 1
		/* XXX lock the biglock here */
d2728 1
a2728 2
			if (vmread(VMCS_GUEST_IA32_RIP,
			    &vcpu->vc_gueststate.vg_rip)) {
d2731 1
a2731 2
				exit_handled = 0;
				goto exit;
d2734 1
a2734 1
			if (vmread(VMCS_EXIT_REASON, &exit_reason)) {
d2737 1
a2737 2
				exit_handled = 0;
				goto exit;
d2745 23
a2767 1
			exit_handled = vmx_handle_exit(vcpu, &ret);
d2775 1
a2775 1
					goto exit;
a2782 1
			exit_handled = 0;
a2788 1
			exit_handled = 0;
a2792 1
			exit_handled = 0;
a2795 1
	vcpu->vc_state = VCPU_STATE_STOPPED;
a2796 1
exit:
d2839 1
d2858 21
a2878 1
	return (0);
d2888 1
a2888 1
vmx_handle_exit(struct vcpu *vcpu, int *result)
d2891 1
a2891 1
	int update_rip, handled;
a2893 1
	handled = 1;
d2898 1
a2898 3
		*result = vmx_handle_np_fault(vcpu);
		if (*result)
			handled = 0;
d2901 1
a2901 1
		*result = vmx_handle_cpuid(vcpu);
d2905 1
a2905 1
		*result = vmx_handle_inout(vcpu);
a2906 2
		if (*result)
			handled = 0;
d2913 1
a2913 1
		*result = vmx_handle_cr(vcpu);
d2917 1
a2917 1
		*result = vmx_handle_hlt(vcpu);
a2918 1
		handled = 0;
d2921 1
a2921 1
		*result = EAGAIN;
a2922 1
		handled = 0;
d2927 1
a2927 2
		*result = EINVAL;
		return (0);
d2934 1
a2934 2
			*result = EINVAL;
			return (0);
d2938 1
a2938 1
	return (handled);
@


1.27
log
@
Make sure we don't overflow a page during vm_readpage/vm_writepage.

Noticed over a month ago by Stefan Kempf <sn.kempf at t-online.de>, and I
shamefully just got around to committing it. Thanks Stefan.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.26 2015/12/17 09:29:28 mlarkin Exp $	*/
d2766 1
a2766 1
		return (1);
d3037 1
a3037 1
		return (1);
d3042 1
a3042 1
		return (1);
d3111 1
a3111 1
		return (1);
d3116 1
a3116 1
		return (1);
d3164 1
a3164 1
		return (1);
@


1.26
log
@
Move vcpu register state init to vmd. Allows vmd bootloader to make the
decision as to how the vcpu should be set up for initial start and
reset. Also removes some hardcoded register constants from vmm(4).

ok jsing@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.25 2015/12/15 03:24:26 mlarkin Exp $	*/
d397 6
d534 6
@


1.25
log
@
remove some not interesting debug printfs
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.24 2015/12/15 01:56:51 mlarkin Exp $	*/
d105 3
a107 3
int vcpu_reset_regs(struct vcpu *);
int vcpu_reset_regs_vmx(struct vcpu *);
int vcpu_reset_regs_svm(struct vcpu *);
d489 1
a489 1
	if (vcpu_reset_regs(vcpu))
d1024 1
a1024 1
vcpu_reset_regs_svm(struct vcpu *vcpu)
d1032 1
a1032 1
 * Initializes 'vcpu's registers to default power-on state
d1036 1
d1043 1
a1043 1
vcpu_reset_regs_vmx(struct vcpu *vcpu)
d1359 1
a1359 1
	if (vmwrite(VMCS_GUEST_IA32_RFLAGS, 0x2)) {
d1377 2
a1378 2
	vcpu->vc_gueststate.vg_rip = 0x01000160;
	if (vmwrite(VMCS_GUEST_IA32_RIP, 0x01000160)) {
d1404 1
a1404 1
	if (vmwrite(VMCS_GUEST_IA32_CR3, 0x0)) {
d1421 1
a1421 2
	/* Set guest stack for 0x10000 - sizeof(bootloader stack setup) */
	if (vmwrite(VMCS_GUEST_IA32_RSP, 0xFFDC)) {
d1426 1
a1426 1
	if (vmwrite(VMCS_GUEST_IA32_SS_SEL, 0x10)) {
d1431 1
a1431 1
	if (vmwrite(VMCS_GUEST_IA32_SS_LIMIT, 0xFFFFFFFF)) {
d1436 1
a1436 1
	if (vmwrite(VMCS_GUEST_IA32_SS_AR, 0xC093)) {
d1441 1
a1441 1
	if (vmwrite(VMCS_GUEST_IA32_SS_BASE, 0x0)) {
d1446 1
a1446 1
	if (vmwrite(VMCS_GUEST_IA32_DS_SEL, 0x10)) {
d1451 1
a1451 1
	if (vmwrite(VMCS_GUEST_IA32_DS_LIMIT, 0xFFFFFFFF)) {
d1456 1
a1456 1
	if (vmwrite(VMCS_GUEST_IA32_DS_AR, 0xC093)) {
d1461 1
a1461 1
	if (vmwrite(VMCS_GUEST_IA32_DS_BASE, 0x0)) {
d1466 1
a1466 1
	if (vmwrite(VMCS_GUEST_IA32_ES_SEL, 0x10)) {
d1471 1
a1471 1
	if (vmwrite(VMCS_GUEST_IA32_ES_LIMIT, 0xFFFFFFFF)) {
d1476 1
a1476 1
	if (vmwrite(VMCS_GUEST_IA32_ES_AR, 0xC093)) {
d1481 1
a1481 1
	if (vmwrite(VMCS_GUEST_IA32_ES_BASE, 0x0)) {
d1486 1
a1486 1
	if (vmwrite(VMCS_GUEST_IA32_FS_SEL, 0x10)) {
d1491 1
a1491 1
	if (vmwrite(VMCS_GUEST_IA32_FS_LIMIT, 0xFFFFFFFF)) {
d1496 1
a1496 1
	if (vmwrite(VMCS_GUEST_IA32_FS_AR, 0xC093)) {
d1501 1
a1501 1
	if (vmwrite(VMCS_GUEST_IA32_FS_BASE, 0x0)) {
d1506 1
a1506 1
	if (vmwrite(VMCS_GUEST_IA32_GS_SEL, 0x10)) {
d1511 1
a1511 1
	if (vmwrite(VMCS_GUEST_IA32_GS_LIMIT, 0xFFFFFFFF)) {
d1516 1
a1516 1
	if (vmwrite(VMCS_GUEST_IA32_GS_AR, 0xC093)) {
d1521 1
a1521 1
	if (vmwrite(VMCS_GUEST_IA32_GS_BASE, 0x0)) {
d1526 1
a1526 1
	if (vmwrite(VMCS_GUEST_IA32_CS_SEL, 0x8)) {
d1531 1
a1531 1
	if (vmwrite(VMCS_GUEST_IA32_CS_LIMIT, 0xFFFFFFFF)) {
d1536 1
a1536 1
	if (vmwrite(VMCS_GUEST_IA32_CS_AR, 0xC09F)) {
d1541 1
a1541 1
	if (vmwrite(VMCS_GUEST_IA32_CS_BASE, 0x0)) {
d1546 1
a1546 1
	if (vmwrite(VMCS_GUEST_IA32_GDTR_LIMIT, 0xFFFF)) {
d1551 1
a1551 1
	if (vmwrite(VMCS_GUEST_IA32_GDTR_BASE, 0x10000)) {
d1556 1
a1556 1
	if (vmwrite(VMCS_GUEST_IA32_IDTR_LIMIT, 0xFFFF)) {
d1561 1
a1561 1
	if (vmwrite(VMCS_GUEST_IA32_IDTR_BASE, 0x0)) {
d1566 1
a1566 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_SEL, 0x0)) {
d1571 1
a1571 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_LIMIT, 0xFFFF)) {
d1576 1
a1576 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_AR, 0x0082)) {
d1581 1
a1581 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_BASE, 0x0)) {
d1586 1
a1586 1
	if (vmwrite(VMCS_GUEST_IA32_TR_SEL, 0x0)) {
d1591 1
a1591 1
	if (vmwrite(VMCS_GUEST_IA32_TR_LIMIT, 0xFFFF)) {
d1596 1
a1596 1
	if (vmwrite(VMCS_GUEST_IA32_TR_AR, 0x008B)) {
d1601 1
a1601 1
	if (vmwrite(VMCS_GUEST_IA32_TR_BASE, 0x0)) {
a1877 6
	/* Initialize default register state */
	if (vcpu_reset_regs(vcpu)) {
		ret = EINVAL;
		goto exit;
	}

d1900 1
a1900 1
 * Resets a vcpu's registers to factory power-on state
d1904 1
d1912 1
a1912 1
vcpu_reset_regs(struct vcpu *vcpu)
d1918 1
a1918 1
		ret = vcpu_reset_regs_vmx(vcpu);
d1921 1
a1921 1
		ret = vcpu_reset_regs_svm(vcpu);
@


1.24
log
@
support reset vcpu by triple fault (kernel part, userland fix will come
later).

discussed with deraadt@@ and reyk@@ at length.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.23 2015/12/14 07:46:03 mlarkin Exp $	*/
a1797 16

	DPRINTF("exit save va/pa  0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_va,
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_pa);
	DPRINTF("exit load va/pa  0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vmx_msr_exit_load_va,
	    (uint64_t)vcpu->vc_vmx_msr_exit_load_pa);
	DPRINTF("entry load va/pa  0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vmx_msr_entry_load_va,
	    (uint64_t)vcpu->vc_vmx_msr_entry_load_pa);
	DPRINTF("vlapic va/pa 0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vlapic_va,
	    (uint64_t)vcpu->vc_vlapic_pa);
	DPRINTF("msr bitmap va/pa 0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_msr_bitmap_va,
	    (uint64_t)vcpu->vc_msr_bitmap_pa);
@


1.23
log
@
Support only one vcpu for now, until we implement SMP support.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.22 2015/12/14 06:59:07 mlarkin Exp $	*/
d104 4
a108 1
int vcpu_init_regs_vmx(struct vcpu *);
d343 3
d442 54
d1018 10
d1030 1
a1030 1
 * vcpu_init_regs_vmx
d1042 1
a1042 1
vcpu_init_regs_vmx(struct vcpu *vcpu)
d1046 5
d1054 15
a1068 6
	/*
	 * The next portion of code sets up the VMCS for the register state
	 * we want during VCPU start. This matches what the CPU state would
	 * be after a bootloader transition to 'start'.
	 */
	if (vmwrite(VMCS_GUEST_IA32_RFLAGS, 0x2)) {
d1074 1
a1074 11
	 * XXX -
	 * vg_rip gets special treatment here since we will rewrite
	 * it just before vmx_enter_guest, so it needs to match.
	 * we could just set vg_rip here and be done with (no vmwrite
	 * here) but that would require us to have proper resume
	 * handling (resume=1) in the exit handler, so for now we
	 * will just end up doing an extra vmwrite here.
	 *
	 * This can now change from the hardcoded value of 0x1000160
	 * to the marks[start] from vmd's bootloader. That needs to
	 * be hoisted up into vcpu create parameters via vm create params.
d1076 1
a1076 2
	vcpu->vc_gueststate.vg_rip = 0x01000160;
	if (vmwrite(VMCS_GUEST_IA32_RIP, 0x01000160)) {
d1081 52
d1134 5
a1138 2
	 * Determine default CR0 as per Intel SDM A.7
	 * All flexible bits are set to 0
d1140 3
a1142 3
	cr0 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed0) &
	    (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed1);
	cr0 |= (CR0_CD | CR0_NW | CR0_ET);
d1144 6
a1149 6
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_UNRESTRICTED_GUEST, 1))
//			cr0 &= ~(CR0_PG);
			cr0 &= ~(CR0_PG | CR0_PE);
d1152 2
a1153 1
	if (vmwrite(VMCS_GUEST_IA32_CR0, cr0)) {
d1158 1
a1158 1
	if (vmwrite(VMCS_GUEST_IA32_CR3, 0x0)) {
d1164 14
a1177 2
	 * Determine default CR4 as per Intel SDM A.8
	 * All flexible bits are set to 0
d1179 8
a1186 2
	cr4 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed0) &
	    (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed1);
d1188 4
a1191 3
	if (vmwrite(VMCS_GUEST_IA32_CR4, cr4)) {
		ret = EINVAL;
		goto exit;
d1194 6
a1199 4
	/* Set guest stack for 0x10000 - sizeof(bootloader stack setup) */
	if (vmwrite(VMCS_GUEST_IA32_RSP, 0xFFDC)) {
		ret = EINVAL;
		goto exit;
d1202 2
a1203 1
	if (vmwrite(VMCS_GUEST_IA32_SS_SEL, 0x10)) {
d1208 1
a1208 1
	if (vmwrite(VMCS_GUEST_IA32_SS_LIMIT, 0xFFFFFFFF)) {
d1213 21
a1233 3
	if (vmwrite(VMCS_GUEST_IA32_SS_AR, 0xC093)) {
		ret = EINVAL;
		goto exit;
d1236 8
a1243 3
	if (vmwrite(VMCS_GUEST_IA32_SS_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
d1246 3
a1248 4
	if (vmwrite(VMCS_GUEST_IA32_DS_SEL, 0x10)) {
		ret = EINVAL;
		goto exit;
	}
d1250 2
a1251 1
	if (vmwrite(VMCS_GUEST_IA32_DS_LIMIT, 0xFFFFFFFF)) {
d1256 1
a1256 1
	if (vmwrite(VMCS_GUEST_IA32_DS_AR, 0xC093)) {
d1261 21
a1281 1
	if (vmwrite(VMCS_GUEST_IA32_DS_BASE, 0x0)) {
d1286 1
a1286 1
	if (vmwrite(VMCS_GUEST_IA32_ES_SEL, 0x10)) {
d1291 21
a1311 1
	if (vmwrite(VMCS_GUEST_IA32_ES_LIMIT, 0xFFFFFFFF)) {
d1316 1
a1316 1
	if (vmwrite(VMCS_GUEST_IA32_ES_AR, 0xC093)) {
d1321 38
a1358 1
	if (vmwrite(VMCS_GUEST_IA32_ES_BASE, 0x0)) {
d1363 15
a1377 1
	if (vmwrite(VMCS_GUEST_IA32_FS_SEL, 0x10)) {
d1382 17
a1398 1
	if (vmwrite(VMCS_GUEST_IA32_FS_LIMIT, 0xFFFFFFFF)) {
d1403 1
a1403 1
	if (vmwrite(VMCS_GUEST_IA32_FS_AR, 0xC093)) {
d1408 8
a1415 1
	if (vmwrite(VMCS_GUEST_IA32_FS_BASE, 0x0)) {
d1420 2
a1421 1
	if (vmwrite(VMCS_GUEST_IA32_GS_SEL, 0x10)) {
d1426 1
a1426 1
	if (vmwrite(VMCS_GUEST_IA32_GS_LIMIT, 0xFFFFFFFF)) {
d1431 1
a1431 1
	if (vmwrite(VMCS_GUEST_IA32_GS_AR, 0xC093)) {
d1436 1
a1436 1
	if (vmwrite(VMCS_GUEST_IA32_GS_BASE, 0x0)) {
d1441 1
a1441 1
	if (vmwrite(VMCS_GUEST_IA32_CS_SEL, 0x8)) {
d1446 1
a1446 1
	if (vmwrite(VMCS_GUEST_IA32_CS_LIMIT, 0xFFFFFFFF)) {
d1451 1
a1451 1
	if (vmwrite(VMCS_GUEST_IA32_CS_AR, 0xC09F)) {
d1456 1
a1456 1
	if (vmwrite(VMCS_GUEST_IA32_CS_BASE, 0x0)) {
d1461 1
a1461 1
	if (vmwrite(VMCS_GUEST_IA32_GDTR_LIMIT, 0xFFFF)) {
d1466 1
a1466 1
	if (vmwrite(VMCS_GUEST_IA32_GDTR_BASE, 0x10000)) {
d1471 1
a1471 1
	if (vmwrite(VMCS_GUEST_IA32_IDTR_LIMIT, 0xFFFF)) {
d1476 1
a1476 1
	if (vmwrite(VMCS_GUEST_IA32_IDTR_BASE, 0x0)) {
d1481 1
a1481 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_SEL, 0x0)) {
d1486 1
a1486 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_LIMIT, 0xFFFF)) {
d1491 1
a1491 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_AR, 0x0082)) {
d1496 1
a1496 1
	if (vmwrite(VMCS_GUEST_IA32_LDTR_BASE, 0x0)) {
d1501 1
a1501 1
	if (vmwrite(VMCS_GUEST_IA32_TR_SEL, 0x0)) {
d1506 1
a1506 1
	if (vmwrite(VMCS_GUEST_IA32_TR_LIMIT, 0xFFFF)) {
d1511 1
a1511 1
	if (vmwrite(VMCS_GUEST_IA32_TR_AR, 0x008B)) {
d1516 1
a1516 1
	if (vmwrite(VMCS_GUEST_IA32_TR_BASE, 0x0)) {
d1521 4
a1524 3
exit:
	return (ret);
}
d1526 4
a1529 23
/*
 * vcpu_init_vmx
 *
 * Intel VMX specific VCPU initialization routine.
 *
 * This function allocates various per-VCPU memory regions, sets up initial
 * VCPU VMCS controls, and sets initial register values.
 *
 * This function is very long but is only performing a bunch of register
 * setups, over and over.
 */
int
vcpu_init_vmx(struct vcpu *vcpu)
{
	struct vmcs *vmcs;
	uint16_t ctrl;
	uint64_t pat_default, msr, ctrlval, eptp;
	uint32_t pinbased, procbased, procbased2, exit, entry;
	uint32_t want1, want0;
	uint32_t cr0, cr4;
	paddr_t control_pa;
	int ret;
	struct vmx_msr_store *msr_store;
d1531 4
a1534 2
	ret = 0;
	pat_default = 0;
d1536 4
a1539 3
	/* Allocate VMCS VA */
	vcpu->vc_control_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page, &kp_zero,
	    &kd_waitok);
d1541 2
a1542 6
	if (!vcpu->vc_control_va)
		return (ENOMEM);

	/* Compute VMCS PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_control_va, &control_pa)) {
		ret = ENOMEM;
d1546 2
a1547 9
	vcpu->vc_control_pa = (uint64_t)control_pa;

	/* Allocate MSR bitmap VA */
	/* XXX dont need this if no msr bitmap support */
	vcpu->vc_msr_bitmap_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page, &kp_zero,
	    &kd_waitok);

	if (!vcpu->vc_msr_bitmap_va) {
		ret = ENOMEM;
d1551 2
a1552 3
	/* Compute MSR bitmap PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_msr_bitmap_va, &control_pa)) {
		ret = ENOMEM;
d1556 2
a1557 9
	vcpu->vc_msr_bitmap_pa = (uint64_t)control_pa;

	/* Allocate MSR exit load area VA */
	/* XXX may not need this with MSR bitmaps */
	vcpu->vc_vmx_msr_exit_load_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_vmx_msr_exit_load_va) {
		ret = ENOMEM;
d1561 2
a1562 4
	/* Compute MSR exit load area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_vmx_msr_exit_load_va,
	    &vcpu->vc_vmx_msr_exit_load_pa)) {
		ret = ENOMEM;
d1566 1
a1566 55
	/* Allocate MSR exit save area VA */
	/* XXX may not need this with MSR bitmaps */
	vcpu->vc_vmx_msr_exit_save_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_vmx_msr_exit_save_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR exit save area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_vmx_msr_exit_save_va,
	    &vcpu->vc_vmx_msr_exit_save_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	/* Allocate MSR entry load area VA */
	/* XXX may not need this with MSR bitmaps */
	vcpu->vc_vmx_msr_entry_load_va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_page,
	   &kp_zero, &kd_waitok);

	if (!vcpu->vc_vmx_msr_entry_load_va) {
		ret = ENOMEM;
		goto exit;
	}

	/* Compute MSR entry load area PA */
	if (!pmap_extract(pmap_kernel(), vcpu->vc_vmx_msr_entry_load_va,
	    &vcpu->vc_vmx_msr_entry_load_pa)) {
		ret = ENOMEM;
		goto exit;
	}

	DPRINTF("exit save va/pa  0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_va,
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_pa);
	DPRINTF("exit load va/pa  0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vmx_msr_exit_load_va,
	    (uint64_t)vcpu->vc_vmx_msr_exit_load_pa);
	DPRINTF("entry load va/pa  0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vmx_msr_entry_load_va,
	    (uint64_t)vcpu->vc_vmx_msr_entry_load_pa);
	DPRINTF("vlapic va/pa 0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_vlapic_va,
	    (uint64_t)vcpu->vc_vlapic_pa);
	DPRINTF("msr bitmap va/pa 0x%llx  0x%llx\n",
	    (uint64_t)vcpu->vc_msr_bitmap_va,
	    (uint64_t)vcpu->vc_msr_bitmap_pa);

	vmcs = (struct vmcs *)vcpu->vc_control_va;
	vmcs->vmcs_revision = curcpu()->ci_vmm_cap.vcc_vmx.vmx_vmxon_revision;

	/* Clear the VMCS */
	if (vmclear(&vcpu->vc_control_pa)) {
d1571 1
a1571 4
	/*
	 * Load the VMCS onto this PCPU so we can write registers and controls
	 */
	if (vmptrld(&vcpu->vc_control_pa)) {
d1576 1
a1576 55
	/* Compute Basic Entry / Exit Controls */
	vcpu->vc_vmx_basic = rdmsr(IA32_VMX_BASIC);
	vcpu->vc_vmx_entry_ctls = rdmsr(IA32_VMX_ENTRY_CTLS);
	vcpu->vc_vmx_exit_ctls = rdmsr(IA32_VMX_EXIT_CTLS);
	vcpu->vc_vmx_pinbased_ctls = rdmsr(IA32_VMX_PINBASED_CTLS);
	vcpu->vc_vmx_procbased_ctls = rdmsr(IA32_VMX_PROCBASED_CTLS);

	/* Compute True Entry / Exit Controls (if applicable) */
	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		vcpu->vc_vmx_true_entry_ctls = rdmsr(IA32_VMX_TRUE_ENTRY_CTLS);
		vcpu->vc_vmx_true_exit_ctls = rdmsr(IA32_VMX_TRUE_EXIT_CTLS);
		vcpu->vc_vmx_true_pinbased_ctls =
		    rdmsr(IA32_VMX_TRUE_PINBASED_CTLS);
		vcpu->vc_vmx_true_procbased_ctls =
		    rdmsr(IA32_VMX_TRUE_PROCBASED_CTLS);
	}

	/* Compute Secondary Procbased Controls (if applicable) */
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1))
		vcpu->vc_vmx_procbased2_ctls = rdmsr(IA32_VMX_PROCBASED2_CTLS);


# if 0
	/* XXX not needed now with MSR list */

	/* Default Guest PAT (if applicable) */
	if ((vcpu_vmx_check_cap(vcpu, IA32_VMX_ENTRY_CTLS,
	    IA32_VMX_LOAD_IA32_PAT_ON_ENTRY, 1)) ||
	    vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_SAVE_IA32_PAT_ON_EXIT, 1)) {
		pat_default = PATENTRY(0, PAT_WB) | PATENTRY(1, PAT_WT) |
		    PATENTRY(2, PAT_UCMINUS) | PATENTRY(3, PAT_UC) |
		    PATENTRY(4, PAT_WB) | PATENTRY(5, PAT_WT) |
		    PATENTRY(6, PAT_UCMINUS) | PATENTRY(7, PAT_UC);
		if (vmwrite(VMCS_GUEST_IA32_PAT, pat_default)) {
			ret = EINVAL;
			goto exit;
		}
	}

	/* Host PAT (if applicable) */
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_EXIT_CTLS,
	    IA32_VMX_LOAD_IA32_PAT_ON_EXIT, 1)) {
		msr = rdmsr(MSR_CR_PAT);
		if (vmwrite(VMCS_HOST_IA32_PAT, msr)) {
			ret = EINVAL;
			goto exit;
		}
	}
#endif

	/* Host CR0 */
	cr0 = rcr0();
	if (vmwrite(VMCS_HOST_IA32_CR0, cr0)) {
d1581 1
a1581 3
	/* Host CR4 */
	cr4 = rcr4();
	if (vmwrite(VMCS_HOST_IA32_CR4, cr4)) {
d1586 1
a1586 7
	/* Host Segment Selectors */
	if (vmwrite(VMCS_HOST_IA32_CS_SEL, GSEL(GCODE_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_DS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
d1591 3
a1593 255
	if (vmwrite(VMCS_HOST_IA32_ES_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_FS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_GS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_SS_SEL, GSEL(GDATA_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_HOST_IA32_TR_SEL, GSYSSEL(GPROC0_SEL, SEL_KPL))) {
		ret = EINVAL;
		goto exit;
	}

	/* Host IDTR base */
	if (vmwrite(VMCS_HOST_IA32_IDTR_BASE, idt_vaddr)) {
		ret = EINVAL;
		goto exit;
	}

	/* VMCS link */
	if (vmwrite(VMCS_LINK_POINTER, 0xFFFFFFFFFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Pinbased ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_EXTERNAL_INT_EXITING - exit on host interrupt
	 * IA32_VMX_NMI_EXITING - exit on host NMI
	 */
	want1 = IA32_VMX_EXTERNAL_INT_EXITING |
	    IA32_VMX_NMI_EXITING;
	want0 = 0;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_PINBASED_CTLS;
		ctrlval = vcpu->vc_vmx_true_pinbased_ctls;
	} else {
		ctrl = IA32_VMX_PINBASED_CTLS;
		ctrlval = vcpu->vc_vmx_pinbased_ctls;
	}

	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0,
	    &pinbased)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_PINBASED_CTLS, pinbased)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Procbased ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_HLT_EXITING - exit on HLT instruction
	 * IA32_VMX_MWAIT_EXITING - exit on MWAIT instruction
	 * IA32_VMX_UNCONDITIONAL_IO_EXITING - exit on I/O instructions
	 * IA32_VMX_USE_MSR_BITMAPS - exit on various MSR accesses
	 * IA32_VMX_CR8_LOAD_EXITING - guest TPR access
	 * IA32_VMX_CR8_STORE_EXITING - guest TPR access
	 * IA32_VMX_USE_TPR_SHADOW - guest TPR access (shadow)
	 *
	 * If we have EPT, we must be able to clear the following
	 * IA32_VMX_CR3_LOAD_EXITING - don't care about guest CR3 accesses
	 * IA32_VMX_CR3_STORE_EXITING - don't care about guest CR3 accesses
	 */
	want1 = IA32_VMX_HLT_EXITING |
	    IA32_VMX_MWAIT_EXITING |
	    IA32_VMX_UNCONDITIONAL_IO_EXITING |
	    IA32_VMX_USE_MSR_BITMAPS |
	    IA32_VMX_CR8_LOAD_EXITING |
	    IA32_VMX_CR8_STORE_EXITING |
	    IA32_VMX_USE_TPR_SHADOW;
	want0 = 0;

	if (vmm_softc->mode == VMM_MODE_EPT) {
		want1 |= IA32_VMX_ACTIVATE_SECONDARY_CONTROLS;
		want0 |= IA32_VMX_CR3_LOAD_EXITING |
		    IA32_VMX_CR3_STORE_EXITING;
	}

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_PROCBASED_CTLS;
		ctrlval = vcpu->vc_vmx_true_procbased_ctls;
	} else {
		ctrl = IA32_VMX_PROCBASED_CTLS;
		ctrlval = vcpu->vc_vmx_procbased_ctls;
	}

	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0,
	    &procbased)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_PROCBASED_CTLS, procbased)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Secondary Procbased ctrls
	 *
	 * We want to be able to set the following, if available:
	 * IA32_VMX_ENABLE_VPID - use VPIDs where available
	 *
	 * If we have EPT, we must be able to set the following:
	 * IA32_VMX_ENABLE_EPT - enable EPT
	 *
	 * If we have unrestricted guest capability, we must be able to set
	 * the following:
	 * IA32_VMX_UNRESTRICTED_GUEST - enable unrestricted guest
	 */
	want1 = 0;

	/* XXX checking for 2ndary controls can be combined here */
	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VPID, 1))
			want1 |= IA32_VMX_ENABLE_VPID;
	}

	if (vmm_softc->mode == VMM_MODE_EPT)
		want1 |= IA32_VMX_ENABLE_EPT;

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_UNRESTRICTED_GUEST, 1))
			want1 |= IA32_VMX_UNRESTRICTED_GUEST;
	}

	want0 = ~want1;
	ctrlval = vcpu->vc_vmx_procbased2_ctls;
	ctrl = IA32_VMX_PROCBASED2_CTLS;

	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0,
	    &procbased2)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_PROCBASED2_CTLS, procbased2)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Exit ctrls
	 *
	 * We must be able to set the following:
	 * IA32_VMX_HOST_SPACE_ADDRESS_SIZE - exit to long mode
	 * IA32_VMX_ACKNOWLEDGE_INTERRUPT_ON_EXIT - ack interrupt on exit
	 * XXX clear save_debug_ctrls on exit ?
	 */
	want1 = IA32_VMX_HOST_SPACE_ADDRESS_SIZE |
	    IA32_VMX_ACKNOWLEDGE_INTERRUPT_ON_EXIT;
	want0 = 0;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_EXIT_CTLS;
		ctrlval = vcpu->vc_vmx_true_exit_ctls;
	} else {
		ctrl = IA32_VMX_EXIT_CTLS;
		ctrlval = vcpu->vc_vmx_exit_ctls;
	}

	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0, &exit)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_EXIT_CTLS, exit)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Entry ctrls
	 *
	 * We must be able to clear the following:
	 * IA32_VMX_ENTRY_TO_SMM - enter to SMM
	 * IA32_VMX_DEACTIVATE_DUAL_MONITOR_TREATMENT
	 * XXX clear load debug_ctrls on entry ?
	 */
	want1 = 0;
	want0 = IA32_VMX_ENTRY_TO_SMM |
	    IA32_VMX_DEACTIVATE_DUAL_MONITOR_TREATMENT;

	if (vcpu->vc_vmx_basic & IA32_VMX_TRUE_CTLS_AVAIL) {
		ctrl = IA32_VMX_TRUE_ENTRY_CTLS;
		ctrlval = vcpu->vc_vmx_true_entry_ctls;
	} else {
		ctrl = IA32_VMX_ENTRY_CTLS;
		ctrlval = vcpu->vc_vmx_entry_ctls;
	}

	if (vcpu_vmx_compute_ctrl(vcpu, ctrlval, ctrl, want1, want0, &entry)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_ENTRY_CTLS, entry)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmm_softc->mode == VMM_MODE_EPT) {
		eptp = vcpu->vc_parent->vm_map->pmap->pm_pdirpa;
		msr = rdmsr(IA32_VMX_EPT_VPID_CAP);
		if (msr & IA32_EPT_VPID_CAP_PAGE_WALK_4) {
			/* Page walk length 4 supported */
			eptp |= ((IA32_EPT_PAGE_WALK_LENGTH - 1) << 3);
		}


		if (msr & IA32_EPT_VPID_CAP_WB) {
			/* WB cache type supported */
			eptp |= IA32_EPT_PAGING_CACHE_TYPE_WB;
		}

		DPRINTF("guest eptp = 0x%llx\n", eptp);
		if (vmwrite(VMCS_GUEST_IA32_EPTP, eptp)) {
			ret = EINVAL;
			goto exit;
		}
	}

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_ENABLE_VPID, 1))
			if (vmwrite(VMCS_GUEST_VPID,
			    (uint16_t)vcpu->vc_parent->vm_id)) {
				ret = EINVAL;
				goto exit;
			}
d1596 1
a1596 2
	/* Initialize default register state */
	if (vcpu_init_regs_vmx(vcpu)) {
d1601 4
a1604 11
	/*
	 * Select MSRs to be saved on exit
	 */
	msr_store = (struct vmx_msr_store *)vcpu->vc_vmx_msr_exit_save_va;
	msr_store[0].vms_index = MSR_EFER;
	msr_store[1].vms_index = MSR_CR_PAT;
	msr_store[2].vms_index = MSR_STAR;
	msr_store[3].vms_index = MSR_LSTAR;
	msr_store[4].vms_index = MSR_CSTAR;
	msr_store[5].vms_index = MSR_SFMASK;
	msr_store[6].vms_index = MSR_KERNELGSBASE;
d1626 1
a1626 1
	 * Select MSRs to be loaded on entry
d1628 1
a1628 1
	msr_store = (struct vmx_msr_store *)vcpu->vc_vmx_msr_entry_load_va;
d1632 1
a1632 1
	msr_store[1].vms_data = pat_default;	/* Initial value */
d1687 132
a1818 1
	/* Flush content of VMCS to memory */
d1824 76
d1920 30
d2595 2
d2825 5
@


1.22
log
@
track used memory in each VM. This is passed back to vmctl status.

ok reyk@@, beck@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.21 2015/12/09 02:29:09 deraadt Exp $	*/
d759 4
@


1.21
log
@successful vmm enter/leave does not need to be reported
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.20 2015/12/06 20:12:15 mlarkin Exp $	*/
d59 1
d2241 1
d2856 1
@


1.20
log
@
add function prototype
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.19 2015/12/06 20:04:25 mlarkin Exp $	*/
d574 1
a574 2
		} else
			printf("%s: entered VMM mode\n", ci->ci_dev->dv_xname);
d584 1
a584 2
	} else
		printf("%s: entered VMM mode\n", self->ci_dev->dv_xname);
d622 1
a622 2
		} else
			printf("%s: exited VMM mode\n", ci->ci_dev->dv_xname);
d632 1
a632 2
	} else
		printf("%s: exited VMM mode\n", self->ci_dev->dv_xname);
@


1.19
log
@
move around some deck chairs in preparation for supporting VM reboot. this
diff moves the cpu register init code into its own function which will
soon be called from vmd to reset processor state.

no functional change with this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.18 2015/12/06 18:42:18 mlarkin Exp $	*/
d104 1
@


1.18
log
@
Don't bother printing out the count of what type of cpu we have. Instead
just print the feature being used (eg, VMX/EPT).

suggested by and ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.17 2015/12/06 18:31:26 mlarkin Exp $	*/
d958 277
d1712 2
a1713 249
	/*
	 * The next portion of code sets up the VMCS for the register state
	 * we want during VCPU start. This matches what the CPU state would
	 * be after a bootloader transition to 'start'.
	 */
	if (vmwrite(VMCS_GUEST_IA32_RFLAGS, 0x2)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * XXX -
	 * vg_rip gets special treatment here since we will rewrite
	 * it just before vmx_enter_guest, so it needs to match.
	 * we could just set vg_rip here and be done with (no vmwrite
	 * here) but that would require us to have proper resume
	 * handling (resume=1) in the exit handler, so for now we
	 * will just end up doing an extra vmwrite here.
	 *
	 * This can now change from the hardcoded value of 0x1000160
	 * to the marks[start] from vmd's bootloader. That needs to
	 * be hoisted up into vcpu create parameters via vm create params.
	 */
	vcpu->vc_gueststate.vg_rip = 0x01000160;
	if (vmwrite(VMCS_GUEST_IA32_RIP, 0x01000160)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Determine default CR0 as per Intel SDM A.7
	 * All flexible bits are set to 0
	 */
	cr0 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed0) &
	    (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr0_fixed1);
	cr0 |= (CR0_CD | CR0_NW | CR0_ET);

	if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED_CTLS,
	    IA32_VMX_ACTIVATE_SECONDARY_CONTROLS, 1)) {
		if (vcpu_vmx_check_cap(vcpu, IA32_VMX_PROCBASED2_CTLS,
		    IA32_VMX_UNRESTRICTED_GUEST, 1))
//			cr0 &= ~(CR0_PG);
			cr0 &= ~(CR0_PG | CR0_PE);
	}

	if (vmwrite(VMCS_GUEST_IA32_CR0, cr0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CR3, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	/*
	 * Determine default CR4 as per Intel SDM A.8
	 * All flexible bits are set to 0
	 */
	cr4 = (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed0) &
	    (curcpu()->ci_vmm_cap.vcc_vmx.vmx_cr4_fixed1);

	if (vmwrite(VMCS_GUEST_IA32_CR4, cr4)) {
		ret = EINVAL;
		goto exit;
	}

	/* Set guest stack for 0x10000 - sizeof(bootloader stack setup) */
	if (vmwrite(VMCS_GUEST_IA32_RSP, 0xFFDC)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_SEL, 0x10)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_LIMIT, 0xFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_AR, 0xC093)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_SS_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_SEL, 0x10)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_LIMIT, 0xFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_AR, 0xC093)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_DS_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_SEL, 0x10)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_LIMIT, 0xFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_AR, 0xC093)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_ES_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_SEL, 0x10)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_LIMIT, 0xFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_AR, 0xC093)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_FS_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_SEL, 0x10)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_LIMIT, 0xFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_AR, 0xC093)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GS_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_SEL, 0x8)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_LIMIT, 0xFFFFFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_AR, 0xC09F)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_CS_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GDTR_LIMIT, 0xFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_GDTR_BASE, 0x10000)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_IDTR_LIMIT, 0xFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_IDTR_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_SEL, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_LIMIT, 0xFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_AR, 0x0082)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_LDTR_BASE, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_SEL, 0x0)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_LIMIT, 0xFFFF)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_AR, 0x008B)) {
		ret = EINVAL;
		goto exit;
	}

	if (vmwrite(VMCS_GUEST_IA32_TR_BASE, 0x0)) {
@


1.17
log
@
don't allow opening of /dev/vmm if we are in an unsupported configuration
or if vmm0 didn't attach, prevents later panics if we try to use vmm in
such a state.

reported by many on tech/misc
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.16 2015/12/06 01:16:58 mlarkin Exp $	*/
d242 16
a257 8
	if (sc->nr_vmx_cpus)
		printf(": %u VMX capable CPU%s, %u %s EPT capable\n",
		    sc->nr_vmx_cpus, (sc->nr_vmx_cpus > 1) ? "s" : "",
		    sc->nr_ept_cpus, (sc->nr_ept_cpus > 1) ? "are" : "is");
	else if (sc->nr_svm_cpus)
		printf(": %u SVM capable CPU%s, %u %s RVI capable\n",
		    sc->nr_svm_cpus, (sc->nr_svm_cpus > 1) ? "s" : "",
		    sc->nr_rvi_cpus, (sc->nr_rvi_cpus > 1) ? "are" : "is");
a262 10

	sc->mode = VMM_MODE_UNKNOWN;
	if (sc->nr_ept_cpus > 0)
		sc->mode = VMM_MODE_EPT;
	else if (sc->nr_vmx_cpus > 0)
		sc->mode = VMM_MODE_VMX;
	else if (sc->nr_rvi_cpus > 0)
		sc->mode = VMM_MODE_RVI;
	else
		sc->mode = VMM_MODE_SVM;
@


1.16
log
@
restore VMM mode after resume from suspend/hibernate
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.15 2015/12/01 12:03:55 mpi Exp $	*/
d290 9
a311 4

	/* Don't allow ioctls if we have no supported CPUs */
	if (vmm_softc->mode == VMM_MODE_UNKNOWN)
		return (ENOTTY);
@


1.15
log
@Pass M_NOWAIT when allocating a temporary page in vm_writepage() to be
coherent with the rest of the allocations.

While here report the correct errno if an allocation fails.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.14 2015/12/01 12:01:38 mpi Exp $	*/
d650 2
a651 1
	if (ci->ci_flags & CPUF_VMM)
@


1.14
log
@Instead of using a supplementary variable to check if we found a VCPU
or a VM in an iteration, check against NULL.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.13 2015/12/01 10:18:35 mpi Exp $	*/
d475 2
a476 3
	pagedata = malloc(PAGE_SIZE, M_DEVBUF, M_WAITOK | M_ZERO);

	if (!pagedata) {
d507 1
a507 1
	if (!kva) {
d511 1
a511 1
		return (EFAULT);
@


1.13
log
@Do not wait when allocating a page in vcpu_init().

Should help with the possible hang when trying to create a VM when
the host is out of memory.  It also improves coherency as all the
allocations in vmm(4) are done without sleeping.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.12 2015/12/01 10:14:05 mpi Exp $	*/
a370 1
	int found;
a374 1
	found = 0;
d376 1
a376 2
		if (vm->vm_id == vrp->vrp_vm_id) {
			found = 1;
a377 1
		}
d381 1
a381 1
	if (!found) {
d449 1
a449 1
	int found, ret;
a453 1
	found = 0;
d455 1
a455 2
		if (vm->vm_id == vwp->vwp_vm_id) {
			found = 1;
a456 1
		}
d460 1
a460 1
	if (!found) {
d469 1
a469 2
	if (vmm_get_guest_memtype(vm, vw_page) !=
	    VMM_MEM_TYPE_REGULAR) {
d2245 1
a2245 1
	struct vm *vm, *found_vm;
a2247 2
	found_vm = NULL;

a2251 1

d2254 1
a2254 1
			found_vm = vm;
d2257 3
a2259 3
	if (found_vm) {
		rw_enter_read(&found_vm->vm_vcpu_lock);
		SLIST_FOREACH(vcpu, &found_vm->vm_vcpu_list, vc_vcpu_link) {
d2262 1
a2262 1
		rw_exit_read(&found_vm->vm_vcpu_lock);
a2263 1

d2266 1
a2266 1
	if (!found_vm)
d2272 1
a2272 1
	SLIST_REMOVE(&vmm_softc->vm_list, found_vm, vm, vm_link);
d2274 1
a2274 1
	vm_teardown(found_vm);
d2287 3
a2289 7
	struct vm *vm, *found_vm;
	struct vcpu *vcpu, *found_vcpu;
	int ret;

	found_vm = NULL;
	found_vcpu = NULL;
	ret = 0;
d2298 1
a2298 1
			found_vm = vm;
d2301 12
a2312 11
	if (found_vm) {
		rw_enter_read(&found_vm->vm_vcpu_lock);
		SLIST_FOREACH(vcpu, &found_vm->vm_vcpu_list, vc_vcpu_link) {
			if (vcpu->vc_id == vrp->vrp_vcpu_id) {
				found_vcpu = vcpu;
				if (found_vcpu->vc_state != VCPU_STATE_STOPPED)
					ret = EBUSY;
				else
					found_vcpu->vc_state =
					    VCPU_STATE_RUNNING;
			}
d2314 1
a2314 1
		rw_exit_read(&found_vm->vm_vcpu_lock);
d2316 1
a2316 1
		if (!found_vcpu)
a2318 1

d2321 1
a2321 1
	if (!found_vm)
d2332 1
a2332 1
		if (copyin(vrp->vrp_exit, &found_vcpu->vc_exit,
d2339 6
a2344 7
	if (found_vcpu->vc_virt_mode == VMM_MODE_VMX ||
	    found_vcpu->vc_virt_mode == VMM_MODE_EPT) {
		ret = vcpu_run_vmx(found_vcpu, vrp->vrp_continue,
		    &vrp->vrp_injint);
	} else if (found_vcpu->vc_virt_mode == VMM_MODE_SVM ||
		   found_vcpu->vc_virt_mode == VMM_MODE_RVI) {
		ret = vcpu_run_svm(found_vcpu, vrp->vrp_continue);
d2349 1
a2349 2
		vrp->vrp_exit_reason =
		    found_vcpu->vc_gueststate.vg_exit_reason;
d2351 2
a2352 2
		if (copyout(&found_vcpu->vc_exit,
		    vrp->vrp_exit, sizeof(union vm_exit)) == EFAULT) {
@


1.12
log
@KNF

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.11 2015/12/01 10:12:15 mpi Exp $	*/
d1831 1
a1831 1
	int ret;
d1833 2
a1834 3
	ret = 0;
	vcpu->vc_hsa_stack_va = (vaddr_t)malloc(PAGE_SIZE,
		M_DEVBUF, M_WAITOK | M_ZERO);
d1840 1
a1840 1
	    vmm_softc->mode == VMM_MODE_EPT) {
d1842 2
a1843 5
		if (ret)
			free((void *)vcpu->vc_hsa_stack_va, M_DEVBUF,
			    PAGE_SIZE);
	} else if (vmm_softc->mode == VMM_MODE_SVM ||
		 vmm_softc->mode == VMM_MODE_RVI) {
d1845 1
a1845 4
		if (ret)
			free((void *)vcpu->vc_hsa_stack_va, M_DEVBUF,
			    PAGE_SIZE);
	} else
d1847 3
@


1.11
log
@Prettify dmesg output.

ok reyk@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.10 2015/12/01 10:08:10 mpi Exp $	*/
d308 1
a308 1
	switch(cmd) {
d917 1
a917 1
			return vm_impl_init_svm(vm);
d957 1
a957 1
			vm_impl_deinit_svm(vm);
d1846 1
a1846 2
	}
	else if	(vmm_softc->mode == VMM_MODE_SVM ||
d1848 5
a1852 6
			ret = vcpu_init_svm(vcpu);
			if (ret)
				free((void *)vcpu->vc_hsa_stack_va, M_DEVBUF,
				    PAGE_SIZE);
	}
	else
d1906 1
a1906 1
			vcpu_deinit_svm(vcpu);
d2361 2
a2362 1
		ret = vcpu_run_vmx(found_vcpu, vrp->vrp_continue, &vrp->vrp_injint);
d2365 1
a2365 2
			ret = vcpu_run_svm(found_vcpu,
			    vrp->vrp_continue);
d2884 1
a2884 1
	switch(gpa_memtype) {
d2948 1
a2948 1
	switch(vcpu->vc_exit.vei.vei_port) {
d3248 1
a3248 1
	return (0);	
d3259 1
a3259 1
	switch(code) {
d3330 1
a3330 1
	switch(code) {
@


1.10
log
@Properly disable SVM until its support is added back.

Should prevent a NULL dereference when initializing VM.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.9 2015/11/26 08:32:09 jsg Exp $	*/
d216 1
a216 1
	struct vmm_softc *sc;
a219 1
	sc = (struct vmm_softc *)self;
a241 2
	printf(": initialized\n");

d243 7
a249 5
		printf("%s: %u VMX capable CPU(s), %u are EPT capable\n",
			DEVNAME(sc), sc->nr_vmx_cpus, sc->nr_ept_cpus);
	if (sc->nr_svm_cpus)
		printf("%s: %u SVM capable CPU(s), %u are RVI capable\n",
			DEVNAME(sc), sc->nr_svm_cpus, sc->nr_rvi_cpus);
@


1.9
log
@init pat_default to 0 as it may potentially be used uninitialised
ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.8 2015/11/26 08:26:48 reyk Exp $	*/
d902 1
a902 1
	return (0);
@


1.8
log
@Automatically start vmm(4) when the first VM is created and after the
last VM is terminated.  This allows to remove the explicit "vmm
enable" / "vmm disable" (VMM_IOC_START / VMM_IOC_STOP) ioctls.  You'll
have to update kernel and userland for this change, as the kernel ABI
changes.

OK mpi@@ mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.7 2015/11/24 09:07:09 mlarkin Exp $	*/
d988 1
@


1.7
log
@
Don't loop forever trying to handle NP faults in certain failure cases.

reported by Stefan Kempf with supplied patch, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.6 2015/11/21 11:16:30 mpi Exp $	*/
d310 2
a311 3
	case VMM_IOC_START:
		ret = vmm_start();
		if (ret)
d313 2
a314 8
		break;
	case VMM_IOC_STOP:
		if (vmm_softc->vm_ct > 0)
			ret = EAGAIN;
		else
			ret = vmm_stop();
		break;
	case VMM_IOC_CREATE:
d316 2
d327 2
a556 1

d561 5
d567 1
a606 1

d611 5
d617 1
@


1.6
log
@Prevent disabling vmm mode if a VM is still running.

Return EAGAIN in this case:

# vmmctl -d
vmmctl: disable VMM command failed (35) - Resource temporarily unavailable

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.5 2015/11/21 11:08:58 mpi Exp $	*/
d2669 2
d2886 1
a2886 1
		break;
@


1.5
log
@Do not create a VM if vmm mode hasn't been enable.

Currently one MUST do "vmmctl -e" before creating a vm with "vmm -S ...".
Lately this could be done automagically by vmd(8) but the kernel should
not allow things that wont fly.

While here, disable vmm mode in error path if at least one of the CPUs
failed to enable it.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.4 2015/11/21 11:03:14 mpi Exp $	*/
d85 2
a86 2
	size_t			vm_ct;
	size_t			vm_idx;
d316 4
a319 1
		ret = vmm_stop();
d558 1
a558 1
	struct cpu_info *self;
a564 1
#endif /* MULTIPROCESSOR */
a565 2
	self = curcpu();
#ifdef MULTIPROCESSOR
a598 2
 *
 * XXX should restrict this function to not stop VMM mode while VMs are running
@


1.4
log
@Do not use boolean_t outside of /sys/uvm

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vmm.c,v 1.3 2015/11/16 10:08:41 mpi Exp $	*/
d312 2
d605 1
a605 1
	struct cpu_info *self;
a611 1
#endif /* MULTIPROCESSOR */
a612 2
	self = curcpu();
#ifdef MULTIPROCESSOR
d761 3
d776 1
a776 1
		return ENOMEM;
@


1.3
log
@Some minor tweaks:

- Add $OpenBSD$ tag,
- constify "struct cfattach",
- Use <uvm/uvm_extern.h> rather than <uvm/uvm.h>, it's enough.
- Keep the "struct vm" private.  This allows us to not pull <uvm/uvm_extern.h>
  in <macine/vmmvar.h>
- Prefer DPRINTF() for debug macro as dprintf(3) is a standard function name.
- Add vmm_debug and fix VMM_DEBUG build
- Remove unneeded <sys/rwlock.h> from <machine/vmmvar.h>
- Kill whitespaces

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d182 1
a182 1
	boolean_t found_vmx, found_svm;
d188 2
a189 2
	found_vmx = FALSE;
	found_svm = FALSE;
d194 1
a194 1
			found_vmx = TRUE;
d196 1
a196 1
			found_svm = TRUE;
@


1.2
log
@
Fix a missing unlock.

From Stefan Kempf sn.kempf at t-online.de
@
text
@d1 1
d28 3
a30 1
#include <uvm/uvm.h>
d37 1
d40 7
d54 13
d146 2
a147 3
struct cfattach vmm_ca = {
	sizeof(struct vmm_softc), vmm_probe, vmm_attach, NULL,
	vmm_activate
d231 1
a231 1
			sc->nr_vmx_cpus++;		
d393 1
a393 2
	if (vmm_get_guest_memtype(vm, vr_page) !=
	    VMM_MEM_TYPE_REGULAR) {
d407 1
a407 1
		dprintf("vm_readpage: can't alloc kva\n");
d417 1
a417 1
		dprintf("vm_readpage: can't copyout\n");
d516 1
a516 1
		dprintf("vm_writepage: can't alloc kva\n");
d535 1
a535 1
		dprintf("vm_writepage: cant fixup ept pte for gpa 0x%llx\n",
d578 1
a578 1
	}		 
d628 1
a628 1
	}		 
d734 1
a734 1
		
d858 1
a858 1
	dprintf(("vm_impl_init_vmx: created vm_map @@ %p\n", vm->vm_map));
d1066 1
a1066 1
	dprintf(("exit save va/pa  0x%llx  0x%llx\n",
d1068 2
a1069 2
	    (uint64_t)vcpu->vc_vmx_msr_exit_save_pa));
	dprintf(("exit load va/pa  0x%llx  0x%llx\n",
d1071 2
a1072 2
	    (uint64_t)vcpu->vc_vmx_msr_exit_load_pa));
	dprintf(("entry load va/pa  0x%llx  0x%llx\n",
d1074 2
a1075 2
	    (uint64_t)vcpu->vc_vmx_msr_entry_load_pa));
	dprintf(("vlapic va/pa 0x%llx  0x%llx\n",
d1077 2
a1078 2
	    (uint64_t)vcpu->vc_vlapic_pa));
	dprintf(("msr bitmap va/pa 0x%llx  0x%llx\n",
d1080 2
a1081 2
	    (uint64_t)vcpu->vc_msr_bitmap_pa));
	
d1347 1
a1347 1
	 * XXX clear save_debug_ctrls on exit ? 
d1405 1
a1405 1
			/* Page walk length 4 supported */ 
d1415 1
a1415 1
		dprintf(("guest eptp = 0x%llx\n", eptp));
d1469 1
a1469 1
	
d1735 1
a1735 1
	
d1991 1
a1991 1
	
d2119 1
a2119 1
					 */	
d2176 1
a2176 1
			}	
d2274 1
a2274 1
	
d2333 1
a2333 1
	
d2443 1
a2443 1
	
d2454 1
a2454 1
	 	 * last time, fix up any needed vcpu state first.
d2582 1
a2582 1
	
d2694 2
a2695 2
		dprintf(("vmx_handle_exit: unhandled exit %lld (%s)\n",
		    exit_reason, vmx_exit_reason_decode(exit_reason)));
d2705 1
a2705 1
			return (0);	
d2722 1
a2722 1
		dprintf(("guest mmio access @@ 0x%llx\n", (uint64_t)gpa));
d2729 1
a2729 1
		dprintf(("guest memtype @@ 0x%llx unknown\n", (uint64_t)gpa));
a2748 1
		
d2853 1
a2853 1
	}	
d2856 1
a2856 1
} 
d2995 2
a2996 2
		dprintf(("vmx_handle_cr: mov to cr%d @@ %llx\n",
	    	    crnum, vcpu->vc_gueststate.vg_rip));
d2999 2
a3000 2
		dprintf(("vmx_handle_cr: mov from cr%d @@ %llx\n",
		    crnum, vcpu->vc_gueststate.vg_rip));
d3003 2
a3004 2
		dprintf(("vmx_handle_cr: clts instruction @@ %llx\n",
		    vcpu->vc_gueststate.vg_rip));
d3006 3
a3008 3
	case CR_LMSW: 
		dprintf(("vmx_handle_cr: lmsw instruction @@ %llx\n",
		    vcpu->vc_gueststate.vg_rip));
d3011 2
a3012 2
		dprintf(("vmx_handle_cr: unknown cr access @@ %llx\n",
		    vcpu->vc_gueststate.vg_rip));
d3067 1
a3067 1
		 */  
d3076 2
a3077 2
		dprintf(("vmx_handle_cpuid: function 0x02 (cache/TLB) not"
		    " supported\n"));
d3086 2
a3087 2
		dprintf(("vmx_handle_cpuid: function 0x04 (deterministic "
		    "cache info) not supported\n"));
d3117 2
a3118 2
		dprintf(("vmx_handle_cpuid: function 0x09 (direct cache access)"
		    " not supported\n"));
d3124 1
a3124 1
		*rdx = 0;	
d3127 2
a3128 2
		dprintf(("vmx_handle_cpuid: function 0x0b (topology enumeration)"
		    " not supported\n"));
d3131 2
a3132 2
		dprintf(("vmx_handle_cpuid: function 0x0d (ext. state info)"
		    " not supported\n"));
d3135 2
a3136 2
		dprintf(("vmx_handle_cpuid: function 0x0f (QoS info)"
		    " not supported\n"));
d3139 2
a3140 2
		dprintf(("vmx_handle_cpuid: function 0x14 (processor trace info)"
		    " not supported\n"));
d3143 2
a3144 2
		dprintf(("vmx_handle_cpuid: function 0x15 (TSC / CCC info)"
		    " not supported\n"));
d3146 3
a3148 3
	case 0x16:	/* Processor frequency info (not supported) */		
		dprintf(("vmx_handle_cpuid: function 0x16 (frequency info)"
		    " not supported\n"));
d3204 2
a3205 2
		dprintf(("vmx_handle_cpuid: function 0x80000008 (phys bits info)"
		    " not supported\n"));
d3208 1
a3208 1
		dprintf(("vmx_handle_cpuid: unsupported rax=0x%llx\n", *rax));
@


1.1
log
@
vmm(4) kernel code

circulated on hackers@@, no objections. Disabled by default.
@
text
@d378 1
@

