head	1.105;
access;
symbols
	OPENBSD_6_2:1.105.0.4
	OPENBSD_6_2_BASE:1.105
	OPENBSD_6_1:1.103.0.4
	OPENBSD_6_1_BASE:1.103
	OPENBSD_6_0:1.99.0.2
	OPENBSD_6_0_BASE:1.99
	OPENBSD_5_9:1.98.0.2
	OPENBSD_5_9_BASE:1.98
	OPENBSD_5_8:1.94.0.4
	OPENBSD_5_8_BASE:1.94
	OPENBSD_5_7:1.88.0.2
	OPENBSD_5_7_BASE:1.88
	OPENBSD_5_6:1.71.0.4
	OPENBSD_5_6_BASE:1.71
	OPENBSD_5_5:1.67.0.4
	OPENBSD_5_5_BASE:1.67
	OPENBSD_5_4:1.65.0.2
	OPENBSD_5_4_BASE:1.65
	OPENBSD_5_3:1.63.0.8
	OPENBSD_5_3_BASE:1.63
	OPENBSD_5_2:1.63.0.6
	OPENBSD_5_2_BASE:1.63
	OPENBSD_5_1_BASE:1.63
	OPENBSD_5_1:1.63.0.4
	OPENBSD_5_0:1.63.0.2
	OPENBSD_5_0_BASE:1.63
	OPENBSD_4_9:1.60.0.2
	OPENBSD_4_9_BASE:1.60
	OPENBSD_4_8:1.55.0.2
	OPENBSD_4_8_BASE:1.55
	OPENBSD_4_7:1.52.0.2
	OPENBSD_4_7_BASE:1.52
	OPENBSD_4_6:1.49.0.4
	OPENBSD_4_6_BASE:1.49
	OPENBSD_4_5:1.37.0.2
	OPENBSD_4_5_BASE:1.37
	OPENBSD_4_4:1.32.0.2
	OPENBSD_4_4_BASE:1.32
	OPENBSD_4_3:1.30.0.2
	OPENBSD_4_3_BASE:1.30
	OPENBSD_4_2:1.27.0.2
	OPENBSD_4_2_BASE:1.27
	OPENBSD_4_1:1.17.0.2
	OPENBSD_4_1_BASE:1.17
	OPENBSD_4_0:1.15.0.2
	OPENBSD_4_0_BASE:1.15
	OPENBSD_3_9:1.14.0.4
	OPENBSD_3_9_BASE:1.14
	OPENBSD_3_8:1.11.0.2
	OPENBSD_3_8_BASE:1.11
	OPENBSD_3_7:1.7.0.4
	OPENBSD_3_7_BASE:1.7
	OPENBSD_3_6:1.7.0.2
	OPENBSD_3_6_BASE:1.7
	SMP_SYNC_A:1.3
	SMP_SYNC_B:1.3
	OPENBSD_3_5:1.3.0.2
	OPENBSD_3_5_BASE:1.3
	SMP:1.2.0.2;
locks; strict;
comment	@ * @;


1.105
date	2017.07.24.15.31.14;	author robert;	state Exp;
branches;
next	1.104;
commitid	gHx0ylCdB3WO0FOt;

1.104
date	2017.05.29.14.19.49;	author mpi;	state Exp;
branches;
next	1.103;
commitid	4u6PWvBw90PH7UDq;

1.103
date	2017.01.02.07.41.18;	author tedu;	state Exp;
branches;
next	1.102;
commitid	0WqtOgjp7ebdcPdn;

1.102
date	2016.09.17.07.37.57;	author mlarkin;	state Exp;
branches;
next	1.101;
commitid	aHOfHpl2qCSTgi8H;

1.101
date	2016.09.16.02.35.41;	author dlg;	state Exp;
branches;
next	1.100;
commitid	Fei4687v68qad1tP;

1.100
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.99;
commitid	RlO92XR575sygHqm;

1.99
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.98;
commitid	N0upL0onl7Raz5yi;

1.98
date	2016.02.08.18.23.04;	author stefan;	state Exp;
branches;
next	1.97;
commitid	KlnooGxekwRiYYw9;

1.97
date	2015.11.10.08.57.39;	author mlarkin;	state Exp;
branches;
next	1.96;
commitid	vpkDcHyyMuFIvij0;

1.96
date	2015.09.08.21.28.35;	author kettenis;	state Exp;
branches;
next	1.95;
commitid	WSD3bUAMn8qMj0PM;

1.95
date	2015.09.03.18.47.36;	author kettenis;	state Exp;
branches;
next	1.94;
commitid	zVkDwIrLpxhjo8ig;

1.94
date	2015.07.10.10.08.49;	author kettenis;	state Exp;
branches;
next	1.93;
commitid	zZzMDxMN6ozpVHMQ;

1.93
date	2015.06.30.08.40.55;	author mlarkin;	state Exp;
branches;
next	1.92;
commitid	keMh76iZ2HLKxyiG;

1.92
date	2015.04.30.15.49.02;	author mlarkin;	state Exp;
branches;
next	1.91;
commitid	u129MbGMP9Y3fZih;

1.91
date	2015.04.15.03.52.45;	author mlarkin;	state Exp;
branches;
next	1.90;
commitid	kJgauzXHtef5Di7D;

1.90
date	2015.03.14.03.38.46;	author jsg;	state Exp;
branches;
next	1.89;
commitid	p4LJxGKbi0BU2cG6;

1.89
date	2015.03.10.20.12.39;	author kettenis;	state Exp;
branches;
next	1.88;
commitid	IuMVdmBhCuJLJqCN;

1.88
date	2015.02.07.01.46.27;	author kettenis;	state Exp;
branches;
next	1.87;
commitid	7b6QyxnUYRNhvNHU;

1.87
date	2015.02.02.09.29.53;	author mlarkin;	state Exp;
branches;
next	1.86;
commitid	Gl0HfhXiwDmJ4Tqg;

1.86
date	2015.01.28.02.56.50;	author mlarkin;	state Exp;
branches;
next	1.85;
commitid	xYPYT76H2Qi859Zm;

1.85
date	2015.01.15.15.30.17;	author sf;	state Exp;
branches;
next	1.84;
commitid	kvzm4IH1Bv9o7MxC;

1.84
date	2015.01.06.00.38.32;	author dlg;	state Exp;
branches;
next	1.83;
commitid	W3kfFzaKNOKHuHue;

1.83
date	2014.12.23.07.42.46;	author tedu;	state Exp;
branches;
next	1.82;
commitid	4x5948uFKv4U2HgF;

1.82
date	2014.12.15.04.54.44;	author tedu;	state Exp;
branches;
next	1.81;
commitid	LTR5gzASo8KdOa3m;

1.81
date	2014.11.21.06.41.17;	author mlarkin;	state Exp;
branches;
next	1.80;
commitid	wIBjvTSaDlwqmM2F;

1.80
date	2014.11.20.06.51.41;	author mlarkin;	state Exp;
branches;
next	1.79;
commitid	rZAY1nGaAOyEEKdK;

1.79
date	2014.11.20.06.45.45;	author mlarkin;	state Exp;
branches;
next	1.78;
commitid	zgDLgqgsFh3gZiO1;

1.78
date	2014.11.16.12.30.56;	author deraadt;	state Exp;
branches;
next	1.77;
commitid	yv0ECmCdICvq576h;

1.77
date	2014.11.07.03.20.02;	author mlarkin;	state Exp;
branches;
next	1.76;
commitid	BQiWE1kfTcL2AvAy;

1.76
date	2014.10.31.04.33.51;	author mlarkin;	state Exp;
branches;
next	1.75;
commitid	2LjUr7lbHh4LyM2R;

1.75
date	2014.10.18.17.28.34;	author kettenis;	state Exp;
branches;
next	1.74;
commitid	h6dhwnjoo12OL9oK;

1.74
date	2014.10.06.20.34.58;	author sf;	state Exp;
branches;
next	1.73;
commitid	TLzvOnmkOFxCXfko;

1.73
date	2014.09.16.18.57.51;	author sf;	state Exp;
branches;
next	1.72;
commitid	86xaiPmbmOFQjqPd;

1.72
date	2014.08.24.17.55.14;	author sf;	state Exp;
branches;
next	1.71;
commitid	kfCCMdbmD8YKcRVL;

1.71
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.70;
commitid	7NtJNW9udCOFtDNM;

1.70
date	2014.06.15.11.43.24;	author sf;	state Exp;
branches;
next	1.69;
commitid	Y0AWLKcfCeF28jP4;

1.69
date	2014.03.27.10.24.40;	author dlg;	state Exp;
branches;
next	1.68;

1.68
date	2014.03.07.16.56.57;	author guenther;	state Exp;
branches;
next	1.67;

1.67
date	2013.11.19.04.12.17;	author guenther;	state Exp;
branches;
next	1.66;

1.66
date	2013.11.01.01.09.43;	author dlg;	state Exp;
branches;
next	1.65;

1.65
date	2013.06.02.16.45.12;	author guenther;	state Exp;
branches;
next	1.64;

1.64
date	2013.06.02.16.38.05;	author guenther;	state Exp;
branches;
next	1.63;

1.63
date	2011.05.17.18.06.13;	author ariane;	state Exp;
branches;
next	1.62;

1.62
date	2011.04.15.15.16.57;	author chl;	state Exp;
branches;
next	1.61;

1.61
date	2011.03.14.00.05.46;	author guenther;	state Exp;
branches;
next	1.60;

1.60
date	2010.11.30.19.30.16;	author kettenis;	state Exp;
branches;
next	1.59;

1.59
date	2010.11.20.20.33.23;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2010.11.13.04.16.42;	author guenther;	state Exp;
branches;
next	1.57;

1.57
date	2010.10.26.05.49.10;	author guenther;	state Exp;
branches;
next	1.56;

1.56
date	2010.09.06.17.36.49;	author guenther;	state Exp;
branches;
next	1.55;

1.55
date	2010.05.13.19.27.24;	author oga;	state Exp;
branches;
next	1.54;

1.54
date	2010.05.08.16.54.07;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2010.04.30.21.56.39;	author oga;	state Exp;
branches;
next	1.52;

1.52
date	2009.12.09.14.31.57;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2009.08.11.17.15.54;	author oga;	state Exp;
branches;
next	1.50;

1.50
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.49;

1.49
date	2009.06.16.16.42.40;	author ariane;	state Exp;
branches;
next	1.48;

1.48
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.47;

1.47
date	2009.06.09.02.56.38;	author krw;	state Exp;
branches;
next	1.46;

1.46
date	2009.06.06.23.45.35;	author guenther;	state Exp;
branches;
next	1.45;

1.45
date	2009.06.05.10.51.44;	author guenther;	state Exp;
branches;
next	1.44;

1.44
date	2009.06.02.23.00.18;	author oga;	state Exp;
branches;
next	1.43;

1.43
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.42;

1.42
date	2009.05.28.09.05.33;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2009.04.27.17.48.22;	author deraadt;	state Exp;
branches;
next	1.40;

1.40
date	2009.04.23.07.42.02;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2009.03.30.16.09.36;	author oga;	state Exp;
branches;
next	1.38;

1.38
date	2009.03.23.13.25.11;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2009.02.16.20.26.58;	author kurt;	state Exp;
branches;
next	1.36;

1.36
date	2009.02.05.01.15.20;	author oga;	state Exp;
branches;
next	1.35;

1.35
date	2008.12.18.14.18.29;	author kurt;	state Exp;
branches;
next	1.34;

1.34
date	2008.12.18.13.43.24;	author kurt;	state Exp;
branches;
next	1.33;

1.33
date	2008.12.04.15.48.19;	author weingart;	state Exp;
branches;
next	1.32;

1.32
date	2008.06.10.02.55.39;	author weingart;	state Exp;
branches;
next	1.31;

1.31
date	2008.05.23.15.39.43;	author jasper;	state Exp;
branches;
next	1.30;

1.30
date	2007.12.09.00.24.04;	author tedu;	state Exp;
branches;
next	1.29;

1.29
date	2007.11.03.22.23.35;	author mikeb;	state Exp;
branches;
next	1.28;

1.28
date	2007.09.04.23.20.23;	author thib;	state Exp;
branches;
next	1.27;

1.27
date	2007.06.01.20.10.04;	author tedu;	state Exp;
branches;
next	1.26;

1.26
date	2007.05.29.02.37.04;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2007.05.29.02.36.19;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2007.05.27.08.58.31;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2007.05.25.16.22.11;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2007.05.18.14.41.55;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2007.05.15.16.38.33;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2007.04.13.10.36.00;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2007.02.03.16.48.23;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2007.01.15.23.19.05;	author jsg;	state Exp;
branches;
next	1.15;

1.15
date	2006.06.29.10.22.25;	author mickey;	state Exp;
branches;
next	1.14;

1.14
date	2005.11.28.16.56.31;	author martin;	state Exp;
branches;
next	1.13;

1.13
date	2005.10.26.18.46.06;	author martin;	state Exp;
branches;
next	1.12;

1.12
date	2005.09.25.20.48.18;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2005.07.26.08.38.29;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2005.06.01.14.36.36;	author brad;	state Exp;
branches;
next	1.9;

1.9
date	2005.05.27.19.32.39;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.7;

1.7
date	2004.07.22.15.50.18;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2004.07.22.11.19.14;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2004.07.19.15.09.05;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2004.06.25.11.03.27;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2004.02.23.08.32.36;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2004.02.09.22.41.14;	author mickey;	state Exp;
branches
	1.2.2.1;
next	1.1;

1.1
date	2004.01.28.01.39.39;	author mickey;	state Exp;
branches;
next	;

1.2.2.1
date	2004.02.22.22.08.18;	author niklas;	state Exp;
branches;
next	1.2.2.2;

1.2.2.2
date	2004.06.05.23.09.24;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.105
log
@add some missing MULTIPROCESSOR ifdefs to not compile in code on
non-multiprocessor kernels to avoid unused function warnings by clang
@
text
@/*	$OpenBSD: pmap.c,v 1.104 2017/05/29 14:19:49 mpi Exp $	*/
/*	$NetBSD: pmap.c,v 1.3 2003/05/08 18:13:13 thorpej Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * Copyright 2001 (c) Wasabi Systems, Inc.
 * All rights reserved.
 *
 * Written by Frank van der Linden for Wasabi Systems, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed for the NetBSD Project by
 *      Wasabi Systems, Inc.
 * 4. The name of Wasabi Systems, Inc. may not be used to endorse
 *    or promote products derived from this software without specific prior
 *    written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY WASABI SYSTEMS, INC. ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL WASABI SYSTEMS, INC
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * This is the i386 pmap modified and generalized to support x86-64
 * as well. The idea is to hide the upper N levels of the page tables
 * inside pmap_get_ptp, pmap_free_ptp and pmap_growkernel. The rest
 * is mostly untouched, except that it uses some more generalized
 * macros and interfaces.
 *
 * This pmap has been tested on the i386 as well, and it can be easily
 * adapted to PAE.
 *
 * fvdl@@wasabisystems.com 18-Jun-2001
 */

/*
 * pmap.c: i386 pmap module rewrite
 * Chuck Cranor <chuck@@ccrc.wustl.edu>
 * 11-Aug-97
 *
 * history of this pmap module: in addition to my own input, i used
 *    the following references for this rewrite of the i386 pmap:
 *
 * [1] the NetBSD i386 pmap.   this pmap appears to be based on the
 *     BSD hp300 pmap done by Mike Hibler at University of Utah.
 *     it was then ported to the i386 by William Jolitz of UUNET
 *     Technologies, Inc.   Then Charles M. Hannum of the NetBSD
 *     project fixed some bugs and provided some speed ups.
 *
 * [2] the FreeBSD i386 pmap.   this pmap seems to be the
 *     Hibler/Jolitz pmap, as modified for FreeBSD by John S. Dyson
 *     and David Greenman.
 *
 * [3] the Mach pmap.   this pmap, from CMU, seems to have migrated
 *     between several processors.   the VAX version was done by
 *     Avadis Tevanian, Jr., and Michael Wayne Young.    the i386
 *     version was done by Lance Berc, Mike Kupfer, Bob Baron,
 *     David Golub, and Richard Draves.    the alpha version was
 *     done by Alessandro Forin (CMU/Mach) and Chris Demetriou
 *     (NetBSD/alpha).
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/atomic.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/user.h>
#include <sys/kernel.h>
#include <sys/mutex.h>
#include <sys/sched.h>

#include <uvm/uvm.h>

#include <machine/cpu.h>
#include <machine/specialreg.h>
#ifdef MULTIPROCESSOR
#include <machine/i82489reg.h>
#include <machine/i82489var.h>
#endif


#include <machine/isa_machdep.h>

#include "acpi.h"

/*
 * general info:
 *
 *  - for an explanation of how the i386 MMU hardware works see
 *    the comments in <machine/pte.h>.
 *
 *  - for an explanation of the general memory structure used by
 *    this pmap (including the recursive mapping), see the comments
 *    in <machine/pmap.h>.
 *
 * this file contains the code for the "pmap module."   the module's
 * job is to manage the hardware's virtual to physical address mappings.
 * note that there are two levels of mapping in the VM system:
 *
 *  [1] the upper layer of the VM system uses vm_map's and vm_map_entry's
 *      to map ranges of virtual address space to objects/files.  for
 *      example, the vm_map may say: "map VA 0x1000 to 0x22000 read-only
 *      to the file /bin/ls starting at offset zero."   note that
 *      the upper layer mapping is not concerned with how individual
 *      vm_pages are mapped.
 *
 *  [2] the lower layer of the VM system (the pmap) maintains the mappings
 *      from virtual addresses.   it is concerned with which vm_page is
 *      mapped where.   for example, when you run /bin/ls and start
 *      at page 0x1000 the fault routine may lookup the correct page
 *      of the /bin/ls file and then ask the pmap layer to establish
 *      a mapping for it.
 *
 * note that information in the lower layer of the VM system can be
 * thrown away since it can easily be reconstructed from the info
 * in the upper layer.
 *
 * data structures we use include:
 *  - struct pmap: describes the address space of one process
 *  - struct pv_entry: describes one <PMAP,VA> mapping of a PA
 *  - struct pg_to_free: a list of virtual addresses whose mappings
 *	have been changed.   used for TLB flushing.
 */

/*
 * memory allocation
 *
 *  - there are three data structures that we must dynamically allocate:
 *
 * [A] new process' page directory page (PDP)
 *	- plan 1: done at pmap_create() we use
 *	  uvm_km_alloc(kernel_map, PAGE_SIZE)  [fka kmem_alloc] to do this
 *	  allocation.
 *
 * if we are low in free physical memory then we sleep in
 * uvm_km_alloc -- in this case this is ok since we are creating
 * a new pmap and should not be holding any locks.
 *
 * if the kernel is totally out of virtual space
 * (i.e. uvm_km_alloc returns NULL), then we panic.
 *
 * XXX: the fork code currently has no way to return an "out of
 * memory, try again" error code since uvm_fork [fka vm_fork]
 * is a void function.
 *
 * [B] new page tables pages (PTP)
 * 	call uvm_pagealloc()
 * 		=> success: zero page, add to pm_pdir
 * 		=> failure: we are out of free vm_pages, let pmap_enter()
 *		   tell UVM about it.
 *
 * note: for kernel PTPs, we start with NKPTP of them.   as we map
 * kernel memory (at uvm_map time) we check to see if we've grown
 * the kernel pmap.   if so, we call the optional function
 * pmap_growkernel() to grow the kernel PTPs in advance.
 *
 * [C] pv_entry structures
 *	- try to allocate one from the pool.
 *	If we fail, we simply let pmap_enter() tell UVM about it.
 */

vaddr_t ptp_masks[] = PTP_MASK_INITIALIZER;
int ptp_shifts[] = PTP_SHIFT_INITIALIZER;
long nkptp[] = NKPTP_INITIALIZER;
long nkptpmax[] = NKPTPMAX_INITIALIZER;
long nbpd[] = NBPD_INITIALIZER;
pd_entry_t *normal_pdes[] = PDES_INITIALIZER;

#define pmap_pte_set(p, n)		atomic_swap_64(p, n)
#define pmap_pte_clearbits(p, b)	x86_atomic_clearbits_u64(p, b)
#define pmap_pte_setbits(p, b)		x86_atomic_setbits_u64(p, b)

/*
 * global data structures
 */

struct pmap kernel_pmap_store;	/* the kernel's pmap (proc0) */

/*
 * pmap_pg_wc: if our processor supports PAT then we set this
 * to be the pte bits for Write Combining. Else we fall back to
 * UC- so mtrrs can override the cacheability;
 */
int pmap_pg_wc = PG_UCMINUS;

/*
 * other data structures
 */

pt_entry_t protection_codes[8];     /* maps MI prot to i386 prot code */
boolean_t pmap_initialized = FALSE; /* pmap_init done yet? */

/*
 * pv management structures.
 */
struct pool pmap_pv_pool;

/*
 * linked list of all non-kernel pmaps
 */

struct pmap_head pmaps;

/*
 * pool that pmap structures are allocated from
 */

struct pool pmap_pmap_pool;

/*
 * When we're freeing a ptp, we need to delay the freeing until all
 * tlb shootdown has been done. This is the list of the to-be-freed pages.
 */
TAILQ_HEAD(pg_to_free, vm_page);

/*
 * pool that PDPs are allocated from
 */

struct pool pmap_pdp_pool;
void pmap_pdp_ctor(pd_entry_t *);

extern vaddr_t msgbuf_vaddr;
extern paddr_t msgbuf_paddr;

extern vaddr_t idt_vaddr;			/* we allocate IDT early */
extern paddr_t idt_paddr;

extern vaddr_t lo32_vaddr;
extern vaddr_t lo32_paddr;

vaddr_t virtual_avail;
extern int end;

/*
 * local prototypes
 */

void pmap_enter_pv(struct vm_page *, struct pv_entry *, struct pmap *,
    vaddr_t, struct vm_page *);
struct vm_page *pmap_get_ptp(struct pmap *, vaddr_t, pd_entry_t **);
struct vm_page *pmap_find_ptp(struct pmap *, vaddr_t, paddr_t, int);
int pmap_find_pte_direct(struct pmap *pm, vaddr_t va, pt_entry_t **pd, int *offs);
void pmap_free_ptp(struct pmap *, struct vm_page *,
    vaddr_t, pt_entry_t *, pd_entry_t **, struct pg_to_free *);
void pmap_freepage(struct pmap *, struct vm_page *, int, struct pg_to_free *);
#ifdef MULTIPROCESSOR
static boolean_t pmap_is_active(struct pmap *, int);
#endif
void pmap_map_ptes(struct pmap *, pt_entry_t **, pd_entry_t ***, paddr_t *);
struct pv_entry *pmap_remove_pv(struct vm_page *, struct pmap *, vaddr_t);
void pmap_do_remove(struct pmap *, vaddr_t, vaddr_t, int);
boolean_t pmap_remove_pte(struct pmap *, struct vm_page *, pt_entry_t *,
    vaddr_t, int, struct pv_entry **);
void pmap_remove_ptes(struct pmap *, struct vm_page *, vaddr_t,
    vaddr_t, vaddr_t, int, struct pv_entry **);
#define PMAP_REMOVE_ALL		0	/* remove all mappings */
#define PMAP_REMOVE_SKIPWIRED	1	/* skip wired mappings */

void pmap_unmap_ptes(struct pmap *, paddr_t);
boolean_t pmap_get_physpage(vaddr_t, int, paddr_t *);
boolean_t pmap_pdes_valid(vaddr_t, pd_entry_t **, pd_entry_t *);
void pmap_alloc_level(pd_entry_t **, vaddr_t, int, long *);

void pmap_sync_flags_pte(struct vm_page *, u_long);

void pmap_tlb_shootpage(struct pmap *, vaddr_t, int);
void pmap_tlb_shootrange(struct pmap *, vaddr_t, vaddr_t, int);
void pmap_tlb_shoottlb(struct pmap *, int);
#ifdef MULTIPROCESSOR
void pmap_tlb_shootwait(void);
#else
#define	pmap_tlb_shootwait()
#endif


/*
 * p m a p   i n l i n e   h e l p e r   f u n c t i o n s
 */

/*
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
 */

static __inline boolean_t
pmap_is_curpmap(struct pmap *pmap)
{
	return((pmap == pmap_kernel()) ||
	       (pmap->pm_pdirpa == (paddr_t) rcr3()));
}

/*
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
 */

#ifdef MULTIPROCESSOR
static __inline boolean_t
pmap_is_active(struct pmap *pmap, int cpu_id)
{
	return (pmap == pmap_kernel() ||
	    (pmap->pm_cpus & (1ULL << cpu_id)) != 0);
}
#endif

static __inline u_int
pmap_pte2flags(u_long pte)
{
	return (((pte & PG_U) ? PG_PMAP_REF : 0) |
	    ((pte & PG_M) ? PG_PMAP_MOD : 0));
}

void
pmap_sync_flags_pte(struct vm_page *pg, u_long pte)
{
	if (pte & (PG_U|PG_M)) {
		atomic_setbits_int(&pg->pg_flags, pmap_pte2flags(pte));
	}
}

/*
 * pmap_map_ptes: map a pmap's PTEs into KVM
 */

void
pmap_map_ptes(struct pmap *pmap, pt_entry_t **ptepp, pd_entry_t ***pdeppp, paddr_t *save_cr3)
{
	paddr_t cr3 = rcr3();

	/* the kernel's pmap is always accessible */
	if (pmap == pmap_kernel() || pmap->pm_pdirpa == cr3) {
		*save_cr3 = 0;
	} else {
		*save_cr3 = cr3;

		/*
		 * Not sure if we need this, but better be safe.
		 * We don't have the current pmap in order to unset its
		 * active bit, but this just means that we may receive
		 * an unneccessary cross-CPU TLB flush now and then.
		 */
		x86_atomic_setbits_u64(&pmap->pm_cpus, (1ULL << cpu_number()));

		lcr3(pmap->pm_pdirpa);
	}

	if (pmap != pmap_kernel())
		mtx_enter(&pmap->pm_mtx);

	*ptepp = PTE_BASE;
	*pdeppp = normal_pdes;
}

void
pmap_unmap_ptes(struct pmap *pmap, paddr_t save_cr3)
{
	if (pmap != pmap_kernel())
		mtx_leave(&pmap->pm_mtx);

	if (save_cr3 != 0) {
		x86_atomic_clearbits_u64(&pmap->pm_cpus, (1ULL << cpu_number()));
		lcr3(save_cr3);
	}
}

/*
 * pmap_fix_ept
 *
 * Fixes up an EPT PTE for vaddr 'va' by reconfiguring the low bits to
 * conform to the EPT format (separate R/W/X bits and various "must be
 * 0 bits")
 *
 * Parameters:
 *  pm: The pmap in question
 *  va: The VA to fix up
 */
void
pmap_fix_ept(struct pmap *pm, vaddr_t va)
{
	u_long mask, shift;
	pd_entry_t pde, *pd;
	paddr_t pdpa;
	int lev, offs;

	pdpa = pm->pm_pdirpa;
	shift = L4_SHIFT;
	mask = L4_MASK;
	for (lev = PTP_LEVELS; lev > 0; lev--) {
		pd = (pd_entry_t *)PMAP_DIRECT_MAP(pdpa);
		offs = (VA_SIGN_POS(va) & mask) >> shift;

		pd[offs] |= EPT_R | EPT_W | EPT_X;
		/*
		 * Levels 3-4 have bits 3:7 'must be 0'
		 * Level 2 has bits 3:6 'must be 0', and bit 7 is always
		 * 0 in our EPT format (thus, bits 3:7 == 0)
		 */
		switch(lev) {
		case 4:
		case 3:
		case 2:
			/* Bits 3:7 = 0 */
			pd[offs] &= ~(0xF8);
			break;
		case 1: pd[offs] |= EPT_WB;
			break;
		}
		
		pde = pd[offs];

		/* Large pages are different, break early if we run into one. */
		if ((pde & (PG_PS|PG_V)) != PG_V)
			panic("pmap_fix_ept: large page in EPT");

		pdpa = (pd[offs] & PG_FRAME);
		/* 4096/8 == 512 == 2^9 entries per level */
		shift -= 9;
		mask >>= 9;
	}
}

int
pmap_find_pte_direct(struct pmap *pm, vaddr_t va, pt_entry_t **pd, int *offs)
{
	u_long mask, shift;
	pd_entry_t pde;
	paddr_t pdpa;
	int lev;

	pdpa = pm->pm_pdirpa;
	shift = L4_SHIFT;
	mask = L4_MASK;
	for (lev = PTP_LEVELS; lev > 0; lev--) {
		*pd = (pd_entry_t *)PMAP_DIRECT_MAP(pdpa);
		*offs = (VA_SIGN_POS(va) & mask) >> shift;
		pde = (*pd)[*offs];

		/* Large pages are different, break early if we run into one. */
		if ((pde & (PG_PS|PG_V)) != PG_V)
			return (lev - 1);

		pdpa = ((*pd)[*offs] & PG_FRAME);
		/* 4096/8 == 512 == 2^9 entries per level */
		shift -= 9;
		mask >>= 9;
	}

	return (0);
}


/*
 * p m a p   k e n t e r   f u n c t i o n s
 *
 * functions to quickly enter/remove pages from the kernel address
 * space.   pmap_kremove is exported to MI kernel.  we make use of
 * the recursive PTE mappings.
 */

/*
 * pmap_kenter_pa: enter a kernel mapping without R/M (pv_entry) tracking
 *
 * => no need to lock anything, assume va is already allocated
 * => should be faster than normal pmap enter function
 */

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *pte, opte, npte;

	pte = kvtopte(va);

	npte = (pa & PMAP_PA_MASK) | ((prot & PROT_WRITE) ? PG_RW : PG_RO) |
	    ((pa & PMAP_NOCACHE) ? PG_N : 0) |
	    ((pa & PMAP_WC) ? pmap_pg_wc : 0) | PG_V;

	/* special 1:1 mappings in the first 2MB must not be global */
	if (va >= (vaddr_t)NBPD_L2)
		npte |= PG_G;

	if (!(prot & PROT_EXEC))
		npte |= pg_nx;
	opte = pmap_pte_set(pte, npte);
#ifdef LARGEPAGES
	/* XXX For now... */
	if (opte & PG_PS)
		panic("%s: PG_PS", __func__);
#endif
	if (pmap_valid_entry(opte)) {
		if (pa & PMAP_NOCACHE && (opte & PG_N) == 0)
			wbinvd();
		/* This shouldn't happen */
		pmap_tlb_shootpage(pmap_kernel(), va, 1);
		pmap_tlb_shootwait();
	}
}

/*
 * pmap_kremove: remove a kernel mapping(s) without R/M (pv_entry) tracking
 *
 * => no need to lock anything
 * => caller must dispose of any vm_page mapped in the va range
 * => note: not an inline function
 * => we assume the va is page aligned and the len is a multiple of PAGE_SIZE
 * => we assume kernel only unmaps valid addresses and thus don't bother
 *    checking the valid bit before doing TLB flushing
 */

void
pmap_kremove(vaddr_t sva, vsize_t len)
{
	pt_entry_t *pte, opte;
	vaddr_t va, eva;

	eva = sva + len;

	for (va = sva; va != eva; va += PAGE_SIZE) {
		pte = kvtopte(va);

		opte = pmap_pte_set(pte, 0);
#ifdef LARGEPAGES
		KASSERT((opte & PG_PS) == 0);
#endif
		KASSERT((opte & PG_PVLIST) == 0);
	}

	pmap_tlb_shootrange(pmap_kernel(), sva, eva, 1);
	pmap_tlb_shootwait();
}

/*
 * p m a p   i n i t   f u n c t i o n s
 *
 * pmap_bootstrap and pmap_init are called during system startup
 * to init the pmap module.   pmap_bootstrap() does a low level
 * init just to get things rolling.   pmap_init() finishes the job.
 */

/*
 * pmap_bootstrap: get the system in a state where it can run with VM
 *	properly enabled (called before main()).   the VM system is
 *      fully init'd later...
 *
 * => on i386, locore.s has already enabled the MMU by allocating
 *	a PDP for the kernel, and nkpde PTP's for the kernel.
 * => kva_start is the first free virtual address in kernel space
 */

paddr_t
pmap_bootstrap(paddr_t first_avail, paddr_t max_pa)
{
	vaddr_t kva, kva_end, kva_start = VM_MIN_KERNEL_ADDRESS;
	struct pmap *kpm;
	int i;
	unsigned long p1i;
	long ndmpdp;
	paddr_t dmpd, dmpdp;

	/*
	 * define the boundaries of the managed kernel virtual address
	 * space.
	 */

	virtual_avail = kva_start;		/* first free KVA */

	/*
	 * set up protection_codes: we need to be able to convert from
	 * a MI protection code (some combo of VM_PROT...) to something
	 * we can jam into a i386 PTE.
	 */

	protection_codes[PROT_NONE] = pg_nx;			/* --- */
	protection_codes[PROT_EXEC] = PG_RO;			/* --x */
	protection_codes[PROT_READ] = PG_RO | pg_nx;		/* -r- */
	protection_codes[PROT_READ | PROT_EXEC] = PG_RO;	/* -rx */
	protection_codes[PROT_WRITE] = PG_RW | pg_nx;		/* w-- */
	protection_codes[PROT_WRITE | PROT_EXEC] = PG_RW;	/* w-x */
	protection_codes[PROT_WRITE | PROT_READ] = PG_RW | pg_nx; /* wr- */
	protection_codes[PROT_READ | PROT_WRITE | PROT_EXEC] = PG_RW;	/* wrx */

	/*
	 * now we init the kernel's pmap
	 *
	 * the kernel pmap's pm_obj is not used for much.   however, in
	 * user pmaps the pm_obj contains the list of active PTPs.
	 * the pm_obj currently does not have a pager.   it might be possible
	 * to add a pager that would allow a process to read-only mmap its
	 * own page tables (fast user level vtophys?).   this may or may not
	 * be useful.
	 */

	kpm = pmap_kernel();
	for (i = 0; i < PTP_LEVELS - 1; i++) {
		uvm_objinit(&kpm->pm_obj[i], NULL, 1);
		kpm->pm_ptphint[i] = NULL;
	}
	memset(&kpm->pm_list, 0, sizeof(kpm->pm_list));  /* pm_list not used */
	kpm->pm_pdir = (pd_entry_t *)(proc0.p_addr->u_pcb.pcb_cr3 + KERNBASE);
	kpm->pm_pdirpa = proc0.p_addr->u_pcb.pcb_cr3;
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count =
		atop(kva_start - VM_MIN_KERNEL_ADDRESS);

	kpm->pm_type = PMAP_TYPE_NORMAL;

	/*
	 * the above is just a rough estimate and not critical to the proper
	 * operation of the system.
	 */

	curpcb->pcb_pmap = kpm;	/* proc0's pcb */

	/*
	 * enable global TLB entries.
	 */
	/* add PG_G attribute to already mapped kernel pages */
#if KERNBASE == VM_MIN_KERNEL_ADDRESS
	for (kva = VM_MIN_KERNEL_ADDRESS ; kva < virtual_avail ;
#else
	kva_end = roundup((vaddr_t)&end, PAGE_SIZE);
	for (kva = KERNBASE; kva < kva_end ;
#endif
	     kva += PAGE_SIZE) {
		p1i = pl1_i(kva);
		if (pmap_valid_entry(PTE_BASE[p1i]))
			PTE_BASE[p1i] |= PG_G;
	}

	/*
	 * Map the direct map. The first 4GB were mapped in locore, here
	 * we map the rest if it exists. We actually use the direct map
	 * here to set up the page tables, we're assuming that we're still
	 * operating in the lower 4GB of memory.
	 */
	ndmpdp = (max_pa + NBPD_L3 - 1) >> L3_SHIFT;
	if (ndmpdp < NDML2_ENTRIES)
		ndmpdp = NDML2_ENTRIES;		/* At least 4GB */

	dmpdp = kpm->pm_pdir[PDIR_SLOT_DIRECT] & PG_FRAME;

	dmpd = first_avail; first_avail += ndmpdp * PAGE_SIZE;

	for (i = NDML2_ENTRIES; i < NPDPG * ndmpdp; i++) {
		paddr_t pdp;
		vaddr_t va;

		pdp = (paddr_t)&(((pd_entry_t *)dmpd)[i]);
		va = PMAP_DIRECT_MAP(pdp);

		*((pd_entry_t *)va) = ((paddr_t)i << L2_SHIFT);
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_PS | PG_G | PG_U |
		    PG_M | pg_nx;
	}

	for (i = NDML2_ENTRIES; i < ndmpdp; i++) {
		paddr_t pdp;
		vaddr_t va;

		pdp = (paddr_t)&(((pd_entry_t *)dmpdp)[i]);
		va = PMAP_DIRECT_MAP(pdp);

		*((pd_entry_t *)va) = dmpd + (i << PAGE_SHIFT);
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_U | PG_M | pg_nx;
	}

	kpm->pm_pdir[PDIR_SLOT_DIRECT] = dmpdp | PG_V | PG_KW | PG_U |
	    PG_M | pg_nx;

	tlbflush();

	msgbuf_vaddr = virtual_avail;
	virtual_avail += round_page(MSGBUFSIZE);

	idt_vaddr = virtual_avail;
	virtual_avail += 2 * PAGE_SIZE;
	idt_paddr = first_avail;			/* steal a page */
	first_avail += 2 * PAGE_SIZE;

#if defined(MULTIPROCESSOR) || \
    (NACPI > 0 && !defined(SMALL_KERNEL))
	/*
	 * Grab a page below 4G for things that need it (i.e.
	 * having an initial %cr3 for the MP trampoline).
	 */
	lo32_vaddr = virtual_avail;
	virtual_avail += PAGE_SIZE;
	lo32_paddr = first_avail;
	first_avail += PAGE_SIZE;
#endif

	/*
	 * init the global lists.
	 */
	LIST_INIT(&pmaps);

	/*
	 * initialize the pmap pool.
	 */

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmappl", NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pvpl", &pool_allocator_single);
	pool_sethiwat(&pmap_pv_pool, 32 * 1024);

	/*
	 * initialize the PDE pool.
	 */

	pool_init(&pmap_pdp_pool, PAGE_SIZE, 0, IPL_NONE, PR_WAITOK,
	    "pdppl", NULL);

	/*
	 * ensure the TLB is sync'd with reality by flushing it...
	 */

	tlbflush();

	return first_avail;
}

/*
 * Pre-allocate PTPs for low memory, so that 1:1 mappings for various
 * trampoline code can be entered.
 */
paddr_t
pmap_prealloc_lowmem_ptps(paddr_t first_avail)
{
	pd_entry_t *pdes;
	int level;
	paddr_t newp;

	pdes = pmap_kernel()->pm_pdir;
	level = PTP_LEVELS;
	for (;;) {
		newp = first_avail; first_avail += PAGE_SIZE;
		memset((void *)PMAP_DIRECT_MAP(newp), 0, PAGE_SIZE);
		pdes[pl_i(0, level)] = (newp & PG_FRAME) | PG_V | PG_RW;
		level--;
		if (level <= 1)
			break;
		pdes = normal_pdes[level - 2];
	}

	return first_avail;
}

/*
 * pmap_init: called from uvm_init, our job is to get the pmap
 * system ready to manage mappings... this mainly means initing
 * the pv_entry stuff.
 */

void
pmap_init(void)
{
	/*
	 * done: pmap module is up (and ready for business)
	 */

	pmap_initialized = TRUE;
}

/*
 * p v _ e n t r y   f u n c t i o n s
 */

/*
 * main pv_entry manipulation functions:
 *   pmap_enter_pv: enter a mapping onto a pv list
 *   pmap_remove_pv: remove a mapping from a pv list
 */

/*
 * pmap_enter_pv: enter a mapping onto a pv list
 *
 * => caller should adjust ptp's wire_count before calling
 *
 * pve: preallocated pve for us to use
 * ptp: PTP in pmap that maps this VA
 */

void
pmap_enter_pv(struct vm_page *pg, struct pv_entry *pve, struct pmap *pmap,
    vaddr_t va, struct vm_page *ptp)
{
	pve->pv_pmap = pmap;
	pve->pv_va = va;
	pve->pv_ptp = ptp;			/* NULL for kernel pmap */
	mtx_enter(&pg->mdpage.pv_mtx);
	pve->pv_next = pg->mdpage.pv_list;	/* add to ... */
	pg->mdpage.pv_list = pve;		/* ... list */
	mtx_leave(&pg->mdpage.pv_mtx);
}

/*
 * pmap_remove_pv: try to remove a mapping from a pv_list
 *
 * => caller should adjust ptp's wire_count and free PTP if needed
 * => we return the removed pve
 */

struct pv_entry *
pmap_remove_pv(struct vm_page *pg, struct pmap *pmap, vaddr_t va)
{
	struct pv_entry *pve, **prevptr;

	mtx_enter(&pg->mdpage.pv_mtx);
	prevptr = &pg->mdpage.pv_list;
	while ((pve = *prevptr) != NULL) {
		if (pve->pv_pmap == pmap && pve->pv_va == va) {	/* match? */
			*prevptr = pve->pv_next;		/* remove it! */
			break;
		}
		prevptr = &pve->pv_next;		/* previous pointer */
	}
	mtx_leave(&pg->mdpage.pv_mtx);
	return(pve);				/* return removed pve */
}

/*
 * p t p   f u n c t i o n s
 */

struct vm_page *
pmap_find_ptp(struct pmap *pmap, vaddr_t va, paddr_t pa, int level)
{
	int lidx = level - 1;
	struct vm_page *pg;

	if (pa != (paddr_t)-1 && pmap->pm_ptphint[lidx] &&
	    pa == VM_PAGE_TO_PHYS(pmap->pm_ptphint[lidx]))
		return (pmap->pm_ptphint[lidx]);

	pg = uvm_pagelookup(&pmap->pm_obj[lidx], ptp_va2o(va, level));

	return pg;
}

void
pmap_freepage(struct pmap *pmap, struct vm_page *ptp, int level,
    struct pg_to_free *pagelist)
{
	int lidx;
	struct uvm_object *obj;

	lidx = level - 1;

	obj = &pmap->pm_obj[lidx];
	pmap->pm_stats.resident_count--;
	if (pmap->pm_ptphint[lidx] == ptp)
		pmap->pm_ptphint[lidx] = RBT_ROOT(uvm_objtree, &obj->memt);
	ptp->wire_count = 0;
	uvm_pagerealloc(ptp, NULL, 0);
	TAILQ_INSERT_TAIL(pagelist, ptp, pageq);
}

void
pmap_free_ptp(struct pmap *pmap, struct vm_page *ptp, vaddr_t va,
    pt_entry_t *ptes, pd_entry_t **pdes, struct pg_to_free *pagelist)
{
	unsigned long index;
	int level;
	vaddr_t invaladdr;
	pd_entry_t opde;

	level = 1;
	do {
		pmap_freepage(pmap, ptp, level, pagelist);
		index = pl_i(va, level + 1);
		opde = pmap_pte_set(&pdes[level - 1][index], 0);
		invaladdr = level == 1 ? (vaddr_t)ptes :
		    (vaddr_t)pdes[level - 2];
		pmap_tlb_shootpage(curpcb->pcb_pmap,
		    invaladdr + index * PAGE_SIZE,
		    pmap_is_curpmap(curpcb->pcb_pmap));
#if defined(MULTIPROCESSOR)
		invaladdr = level == 1 ? (vaddr_t)PTE_BASE :
		    (vaddr_t)normal_pdes[level - 2];
		pmap_tlb_shootpage(pmap, invaladdr + index * PAGE_SIZE,
		    pmap_is_curpmap(curpcb->pcb_pmap));
#endif
		if (level < PTP_LEVELS - 1) {
			ptp = pmap_find_ptp(pmap, va, (paddr_t)-1, level + 1);
			ptp->wire_count--;
			if (ptp->wire_count > 1)
				break;
		}
	} while (++level < PTP_LEVELS);
}

/*
 * pmap_get_ptp: get a PTP (if there isn't one, allocate a new one)
 *
 * => pmap should NOT be pmap_kernel()
 */


struct vm_page *
pmap_get_ptp(struct pmap *pmap, vaddr_t va, pd_entry_t **pdes)
{
	struct vm_page *ptp, *pptp;
	int i;
	unsigned long index;
	pd_entry_t *pva;
	paddr_t ppa, pa;
	struct uvm_object *obj;

	ptp = NULL;
	pa = (paddr_t)-1;

	/*
	 * Loop through all page table levels seeing if we need to
	 * add a new page to that level.
	 */
	for (i = PTP_LEVELS; i > 1; i--) {
		/*
		 * Save values from previous round.
		 */
		pptp = ptp;
		ppa = pa;

		index = pl_i(va, i);
		pva = pdes[i - 2];

		if (pmap_valid_entry(pva[index])) {
			ppa = pva[index] & PG_FRAME;
			ptp = NULL;
			continue;
		}

		obj = &pmap->pm_obj[i-2];
		ptp = uvm_pagealloc(obj, ptp_va2o(va, i - 1), NULL,
		    UVM_PGA_USERESERVE|UVM_PGA_ZERO);

		if (ptp == NULL)
			return NULL;

		atomic_clearbits_int(&ptp->pg_flags, PG_BUSY);
		ptp->wire_count = 1;
		pmap->pm_ptphint[i - 2] = ptp;
		pa = VM_PAGE_TO_PHYS(ptp);
		pva[index] = (pd_entry_t) (pa | PG_u | PG_RW | PG_V);
		pmap->pm_stats.resident_count++;
		/*
		 * If we're not in the top level, increase the
		 * wire count of the parent page.
		 */
		if (i < PTP_LEVELS) {
			if (pptp == NULL)
				pptp = pmap_find_ptp(pmap, va, ppa, i);
#ifdef DIAGNOSTIC
			if (pptp == NULL)
				panic("%s: pde page disappeared", __func__);
#endif
			pptp->wire_count++;
		}
	}

	/*
	 * ptp is not NULL if we just allocated a new ptp. If it's
	 * still NULL, we must look up the existing one.
	 */
	if (ptp == NULL) {
		ptp = pmap_find_ptp(pmap, va, ppa, 1);
#ifdef DIAGNOSTIC
		if (ptp == NULL) {
			printf("va %lx ppa %lx\n", (unsigned long)va,
			    (unsigned long)ppa);
			panic("%s: unmanaged user PTP", __func__);
		}
#endif
	}

	pmap->pm_ptphint[0] = ptp;
	return(ptp);
}

/*
 * p m a p  l i f e c y c l e   f u n c t i o n s
 */

/*
 * pmap_pdp_ctor: constructor for the PDP cache.
 */

void
pmap_pdp_ctor(pd_entry_t *pdir)
{
	paddr_t pdirpa;
	int npde;

	/* fetch the physical address of the page directory. */
	(void) pmap_extract(pmap_kernel(), (vaddr_t) pdir, &pdirpa);

	/* zero init area */
	memset(pdir, 0, PDIR_SLOT_PTE * sizeof(pd_entry_t));

	/* put in recursive PDE to map the PTEs */
	pdir[PDIR_SLOT_PTE] = pdirpa | PG_V | PG_KW | pg_nx;

	npde = nkptp[PTP_LEVELS - 1];

	/* put in kernel VM PDEs */
	memcpy(&pdir[PDIR_SLOT_KERN], &PDP_BASE[PDIR_SLOT_KERN],
	    npde * sizeof(pd_entry_t));

	/* zero the rest */
	memset(&pdir[PDIR_SLOT_KERN + npde], 0,
	    (NTOPLEVEL_PDES - (PDIR_SLOT_KERN + npde)) * sizeof(pd_entry_t));

	pdir[PDIR_SLOT_DIRECT] = pmap_kernel()->pm_pdir[PDIR_SLOT_DIRECT];

#if VM_MIN_KERNEL_ADDRESS != KERNBASE
	pdir[pl4_pi(KERNBASE)] = PDP_BASE[pl4_pi(KERNBASE)];
#endif
}

/*
 * pmap_create: create a pmap
 *
 * => note: old pmap interface took a "size" args which allowed for
 *	the creation of "software only" pmaps (not in bsd).
 */

struct pmap *
pmap_create(void)
{
	struct pmap *pmap;
	int i;

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);

	mtx_init(&pmap->pm_mtx, IPL_VM);

	/* init uvm_object */
	for (i = 0; i < PTP_LEVELS - 1; i++) {
		uvm_objinit(&pmap->pm_obj[i], NULL, 1);
		pmap->pm_ptphint[i] = NULL;
	}
	pmap->pm_stats.wired_count = 0;
	pmap->pm_stats.resident_count = 1;	/* count the PDP allocd below */
	pmap->pm_cpus = 0;
	pmap->pm_type = PMAP_TYPE_NORMAL;

	/* allocate PDP */

	/*
	 * note that there is no need to splvm to protect us from
	 * malloc since malloc allocates out of a submap and we should
	 * have already allocated kernel PTPs to cover the range...
	 */

	pmap->pm_pdir = pool_get(&pmap_pdp_pool, PR_WAITOK);
	pmap_pdp_ctor(pmap->pm_pdir);

	pmap->pm_pdirpa = pmap->pm_pdir[PDIR_SLOT_PTE] & PG_FRAME;

	LIST_INSERT_HEAD(&pmaps, pmap, pm_list);
	return (pmap);
}

/*
 * pmap_destroy: drop reference count on pmap.   free pmap if
 *	reference count goes to zero.
 */

void
pmap_destroy(struct pmap *pmap)
{
	struct vm_page *pg;
	int refs;
	int i;

	/*
	 * drop reference count
	 */

	refs = atomic_dec_int_nv(&pmap->pm_obj[0].uo_refs);
	if (refs > 0) {
		return;
	}

	/*
	 * reference count is zero, free pmap resources and then free pmap.
	 */

#ifdef DIAGNOSTIC
	if (__predict_false(pmap->pm_cpus != 0))
		printf("%s: pmap %p cpus=0x%llx\n", __func__,
		    (void *)pmap, pmap->pm_cpus);
#endif

	/*
	 * remove it from global list of pmaps
	 */
	LIST_REMOVE(pmap, pm_list);

	/*
	 * free any remaining PTPs
	 */

	for (i = 0; i < PTP_LEVELS - 1; i++) {
		while ((pg = RBT_ROOT(uvm_objtree,
		    &pmap->pm_obj[i].memt)) != NULL) {
			KASSERT((pg->pg_flags & PG_BUSY) == 0);

			pg->wire_count = 0;
			uvm_pagefree(pg);
		}
	}

	/* XXX: need to flush it out of other processor's space? */
	pool_put(&pmap_pdp_pool, pmap->pm_pdir);

	pool_put(&pmap_pmap_pool, pmap);
}

/*
 *	Add a reference to the specified pmap.
 */

void
pmap_reference(struct pmap *pmap)
{
	atomic_inc_int(&pmap->pm_obj[0].uo_refs);
}

/*
 * pmap_activate: activate a process' pmap (fill in %cr3)
 *
 * => called from cpu_fork() and when switching pmaps during exec
 * => if p is the curproc, then load it into the MMU
 */

void
pmap_activate(struct proc *p)
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	pcb->pcb_pmap = pmap;
	pcb->pcb_cr3 = pmap->pm_pdirpa;
	if (p == curproc) {
		lcr3(pcb->pcb_cr3);

		/*
		 * mark the pmap in use by this processor.
		 */
		x86_atomic_setbits_u64(&pmap->pm_cpus, (1ULL << cpu_number()));
	}
}

/*
 * pmap_deactivate: deactivate a process' pmap
 */

void
pmap_deactivate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	/*
	 * mark the pmap no longer in use by this processor.
	 */
	x86_atomic_clearbits_u64(&pmap->pm_cpus, (1ULL << cpu_number()));
}

/*
 * end of lifecycle functions
 */

/*
 * some misc. functions
 */

boolean_t
pmap_pdes_valid(vaddr_t va, pd_entry_t **pdes, pd_entry_t *lastpde)
{
	int i;
	unsigned long index;
	pd_entry_t pde;

	for (i = PTP_LEVELS; i > 1; i--) {
		index = pl_i(va, i);
		pde = pdes[i - 2][index];
		if (!pmap_valid_entry(pde))
			return FALSE;
	}
	if (lastpde != NULL)
		*lastpde = pde;
	return TRUE;
}

/*
 * pmap_extract: extract a PA for the given VA
 */

boolean_t
pmap_extract(struct pmap *pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *ptes;
	int level, offs;

	if (pmap == pmap_kernel() && va >= PMAP_DIRECT_BASE &&
	    va < PMAP_DIRECT_END) {
		*pap = va - PMAP_DIRECT_BASE;
		return (TRUE);
	}

	level = pmap_find_pte_direct(pmap, va, &ptes, &offs);

	if (__predict_true(level == 0 && pmap_valid_entry(ptes[offs]))) {
		if (pap != NULL)
			*pap = (ptes[offs] & PG_FRAME) | (va & PAGE_MASK);
		return (TRUE);
	}
	if (level == 1 && (ptes[offs] & (PG_PS|PG_V)) == (PG_PS|PG_V)) {
		if (pap != NULL)
			*pap = (ptes[offs] & PG_LGFRAME) | (va & PAGE_MASK_L2);
		return (TRUE);
	}

	return FALSE;
}

/*
 * pmap_zero_page: zero a page
 */

void
pmap_zero_page(struct vm_page *pg)
{
	pagezero(pmap_map_direct(pg));
}

/*
 * pmap_flush_cache: flush the cache for a virtual address.
 */
void
pmap_flush_cache(vaddr_t addr, vsize_t len)
{
	vaddr_t	i;

	if (curcpu()->ci_cflushsz == 0) {
		wbinvd();
		return;
	}

	/* all cpus that have clflush also have mfence. */
	mfence();
	for (i = addr; i < addr + len; i += curcpu()->ci_cflushsz)
		clflush(i);
	mfence();
}

/*
 * pmap_copy_page: copy a page
 */

void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	vaddr_t srcva = pmap_map_direct(srcpg);
	vaddr_t dstva = pmap_map_direct(dstpg);

	memcpy((void *)dstva, (void *)srcva, PAGE_SIZE);
}

/*
 * p m a p   r e m o v e   f u n c t i o n s
 *
 * functions that remove mappings
 */

/*
 * pmap_remove_ptes: remove PTEs from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 */

void
pmap_remove_ptes(struct pmap *pmap, struct vm_page *ptp, vaddr_t ptpva,
    vaddr_t startva, vaddr_t endva, int flags, struct pv_entry **free_pvs)
{
	struct pv_entry *pve;
	pt_entry_t *pte = (pt_entry_t *) ptpva;
	struct vm_page *pg;
	pt_entry_t opte;

	/*
	 * note that ptpva points to the PTE that maps startva.   this may
	 * or may not be the first PTE in the PTP.
	 *
	 * we loop through the PTP while there are still PTEs to look at
	 * and the wire_count is greater than 1 (because we use the wire_count
	 * to keep track of the number of real PTEs in the PTP).
	 */

	for (/*null*/; startva < endva && (ptp == NULL || ptp->wire_count > 1)
			     ; pte++, startva += PAGE_SIZE) {
		if (!pmap_valid_entry(*pte))
			continue;			/* VA not mapped */
		if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W)) {
			continue;
		}

		/* atomically save the old PTE and zap! it */
		opte = pmap_pte_set(pte, 0);

		if (opte & PG_W)
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		if (ptp)
			ptp->wire_count--;		/* dropping a PTE */

		pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);

		/*
		 * if we are not on a pv list we are done.
		 */

		if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
			if (pg != NULL)
				panic("%s: managed page without PG_PVLIST "
				      "for 0x%lx", __func__, startva);
#endif
			continue;
		}

#ifdef DIAGNOSTIC
		if (pg == NULL)
			panic("%s: unmanaged page marked PG_PVLIST, "
			      "va = 0x%lx, pa = 0x%lx", __func__,
			      startva, (u_long)(opte & PG_FRAME));
#endif

		/* sync R/M bits */
		pmap_sync_flags_pte(pg, opte);
		pve = pmap_remove_pv(pg, pmap, startva);
		if (pve) {
			pve->pv_next = *free_pvs;
			*free_pvs = pve;
		}

		/* end of "for" loop: time for next pte */
	}
}


/*
 * pmap_remove_pte: remove a single PTE from a PTP
 *
 * => must have proper locking on pmap_master_lock
 * => PTP must be mapped into KVA
 * => PTP should be null if pmap == pmap_kernel()
 * => returns true if we removed a mapping
 */

boolean_t
pmap_remove_pte(struct pmap *pmap, struct vm_page *ptp, pt_entry_t *pte,
    vaddr_t va, int flags, struct pv_entry **free_pvs)
{
	struct pv_entry *pve;
	struct vm_page *pg;
	pt_entry_t opte;

	if (!pmap_valid_entry(*pte))
		return(FALSE);		/* VA not mapped */
	if ((flags & PMAP_REMOVE_SKIPWIRED) && (*pte & PG_W)) {
		return(FALSE);
	}

	/* atomically save the old PTE and zap! it */
	opte = pmap_pte_set(pte, 0);

	if (opte & PG_W)
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	if (ptp)
		ptp->wire_count--;		/* dropping a PTE */

	pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);

	/*
	 * if we are not on a pv list we are done.
	 */
	if ((opte & PG_PVLIST) == 0) {
#ifdef DIAGNOSTIC
		if (pg != NULL)
			panic("%s: managed page without PG_PVLIST for 0x%lx",
			      __func__, va);
#endif
		return(TRUE);
	}

#ifdef DIAGNOSTIC
	if (pg == NULL)
		panic("%s: unmanaged page marked PG_PVLIST, va = 0x%lx, "
		      "pa = 0x%lx", __func__, va, (u_long)(opte & PG_FRAME));
#endif

	/* sync R/M bits */
	pmap_sync_flags_pte(pg, opte);
	pve = pmap_remove_pv(pg, pmap, va);
	if (pve) {
		pve->pv_next = *free_pvs;
		*free_pvs = pve;
	}

	return(TRUE);
}

/*
 * pmap_remove: top level mapping removal function
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_remove(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	pmap_do_remove(pmap, sva, eva, PMAP_REMOVE_ALL);
}

/*
 * pmap_do_remove: mapping removal guts
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_do_remove(struct pmap *pmap, vaddr_t sva, vaddr_t eva, int flags)
{
	pt_entry_t *ptes;
	pd_entry_t **pdes, pde;
	boolean_t result;
	paddr_t ptppa;
	vaddr_t blkendva;
	struct vm_page *ptp;
	struct pv_entry *pve;
	struct pv_entry *free_pvs = NULL;
	vaddr_t va;
	int shootall = 0, shootself;
	struct pg_to_free empty_ptps;
	paddr_t scr3;

	TAILQ_INIT(&empty_ptps);

	pmap_map_ptes(pmap, &ptes, &pdes, &scr3);
	shootself = (scr3 == 0);

	/*
	 * removing one page?  take shortcut function.
	 */

	if (sva + PAGE_SIZE == eva) {
		if (pmap_pdes_valid(sva, pdes, &pde)) {

			/* PA of the PTP */
			ptppa = pde & PG_FRAME;

			/* get PTP if non-kernel mapping */

			if (pmap == pmap_kernel()) {
				/* we never free kernel PTPs */
				ptp = NULL;
			} else {
				ptp = pmap_find_ptp(pmap, sva, ptppa, 1);
#ifdef DIAGNOSTIC
				if (ptp == NULL)
					panic("%s: unmanaged PTP detected",
					      __func__);
#endif
			}

			/* do it! */
			result = pmap_remove_pte(pmap, ptp,
			    &ptes[pl1_i(sva)], sva, flags, &free_pvs);

			/*
			 * if mapping removed and the PTP is no longer
			 * being used, free it!
			 */

			if (result && ptp && ptp->wire_count <= 1)
				pmap_free_ptp(pmap, ptp, sva, ptes, pdes,
				    &empty_ptps);
			pmap_tlb_shootpage(pmap, sva, shootself);
			pmap_unmap_ptes(pmap, scr3);
			pmap_tlb_shootwait();
		} else {
			pmap_unmap_ptes(pmap, scr3);
		}

		goto cleanup;
	}

	if ((eva - sva > 32 * PAGE_SIZE) && pmap != pmap_kernel())
		shootall = 1;

	for (va = sva; va < eva; va = blkendva) {
		/* determine range of block */
		blkendva = x86_round_pdr(va + 1);
		if (blkendva > eva)
			blkendva = eva;

		/*
		 * XXXCDC: our PTE mappings should never be removed
		 * with pmap_remove!  if we allow this (and why would
		 * we?) then we end up freeing the pmap's page
		 * directory page (PDP) before we are finished using
		 * it when we hit in in the recursive mapping.  this
		 * is BAD.
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		if (pl_i(va, PTP_LEVELS) == PDIR_SLOT_PTE)
			/* XXXCDC: ugly hack to avoid freeing PDP here */
			continue;

		if (!pmap_pdes_valid(va, pdes, &pde))
			continue;

		/* PA of the PTP */
		ptppa = pde & PG_FRAME;

		/* get PTP if non-kernel mapping */
		if (pmap == pmap_kernel()) {
			/* we never free kernel PTPs */
			ptp = NULL;
		} else {
			ptp = pmap_find_ptp(pmap, va, ptppa, 1);
#ifdef DIAGNOSTIC
			if (ptp == NULL)
				panic("%s: unmanaged PTP detected", __func__);
#endif
		}
		pmap_remove_ptes(pmap, ptp, (vaddr_t)&ptes[pl1_i(va)],
		    va, blkendva, flags, &free_pvs);

		/* if PTP is no longer being used, free it! */
		if (ptp && ptp->wire_count <= 1) {
			pmap_free_ptp(pmap, ptp, va, ptes, pdes, &empty_ptps);
		}
	}

	if (shootall)
		pmap_tlb_shoottlb(pmap, shootself);
	else
		pmap_tlb_shootrange(pmap, sva, eva, shootself);

	pmap_unmap_ptes(pmap, scr3);
	pmap_tlb_shootwait();

cleanup:
	while ((pve = free_pvs) != NULL) {
		free_pvs = pve->pv_next;
		pool_put(&pmap_pv_pool, pve);
	}

	while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
		TAILQ_REMOVE(&empty_ptps, ptp, pageq);
		uvm_pagefree(ptp);
	}
}

/*
 * pmap_page_remove: remove a managed vm_page from all pmaps that map it
 *
 * => R/M bits are sync'd back to attrs
 */

void
pmap_page_remove(struct vm_page *pg)
{
	struct pv_entry *pve;
	struct pmap *pm;
	pt_entry_t *ptes, opte;
	pd_entry_t **pdes;
#ifdef DIAGNOSTIC
	pd_entry_t pde;
#endif
	struct pg_to_free empty_ptps;
	struct vm_page *ptp;
	paddr_t scr3;
	int shootself;

	TAILQ_INIT(&empty_ptps);

	mtx_enter(&pg->mdpage.pv_mtx);
	while ((pve = pg->mdpage.pv_list) != NULL) {
		pmap_reference(pve->pv_pmap);
		pm = pve->pv_pmap;
		mtx_leave(&pg->mdpage.pv_mtx);

		/* XXX use direct map? */
		pmap_map_ptes(pm, &ptes, &pdes, &scr3);	/* locks pmap */
		shootself = (scr3 == 0);

		/*
		 * We dropped the pvlist lock before grabbing the pmap
		 * lock to avoid lock ordering problems.  This means
		 * we have to check the pvlist again since somebody
		 * else might have modified it.  All we care about is
		 * that the pvlist entry matches the pmap we just
		 * locked.  If it doesn't, unlock the pmap and try
		 * again.
		 */
		mtx_enter(&pg->mdpage.pv_mtx);
		if ((pve = pg->mdpage.pv_list) == NULL ||
		    pve->pv_pmap != pm) {
			mtx_leave(&pg->mdpage.pv_mtx);
			pmap_unmap_ptes(pm, scr3);	/* unlocks pmap */
			pmap_destroy(pm);
			mtx_enter(&pg->mdpage.pv_mtx);
			continue;
		}

		pg->mdpage.pv_list = pve->pv_next;
		mtx_leave(&pg->mdpage.pv_mtx);

#ifdef DIAGNOSTIC
		if (pve->pv_ptp && pmap_pdes_valid(pve->pv_va, pdes, &pde) &&
		   (pde & PG_FRAME) != VM_PAGE_TO_PHYS(pve->pv_ptp)) {
			printf("%s: pg=%p: va=%lx, pv_ptp=%p\n", __func__,
			       pg, pve->pv_va, pve->pv_ptp);
			printf("%s: PTP's phys addr: "
			       "actual=%lx, recorded=%lx\n", __func__,
			       (unsigned long)(pde & PG_FRAME),
				VM_PAGE_TO_PHYS(pve->pv_ptp));
			panic("%s: mapped managed page has "
			      "invalid pv_ptp field", __func__);
		}
#endif

		/* atomically save the old PTE and zap it */
		opte = pmap_pte_set(&ptes[pl1_i(pve->pv_va)], 0);

		if (opte & PG_W)
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;

		pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va, shootself);

		pmap_sync_flags_pte(pg, opte);

		/* update the PTP reference count.  free if last reference. */
		if (pve->pv_ptp) {
			pve->pv_ptp->wire_count--;
			if (pve->pv_ptp->wire_count <= 1) {
				pmap_free_ptp(pve->pv_pmap, pve->pv_ptp,
				    pve->pv_va, ptes, pdes, &empty_ptps);
			}
		}
		pmap_unmap_ptes(pve->pv_pmap, scr3);	/* unlocks pmap */
		pmap_destroy(pve->pv_pmap);
		pool_put(&pmap_pv_pool, pve);
		mtx_enter(&pg->mdpage.pv_mtx);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	pmap_tlb_shootwait();

	while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
		TAILQ_REMOVE(&empty_ptps, ptp, pageq);
		uvm_pagefree(ptp);
	}
}

/*
 * p m a p   a t t r i b u t e  f u n c t i o n s
 * functions that test/change managed page's attributes
 * since a page can be mapped multiple times we must check each PTE that
 * maps it by going down the pv lists.
 */

/*
 * pmap_test_attrs: test a page's attributes
 */

boolean_t
pmap_test_attrs(struct vm_page *pg, unsigned int testbits)
{
	struct pv_entry *pve;
	pt_entry_t *ptes;
	int level, offs;
	u_long mybits, testflags;

	testflags = pmap_pte2flags(testbits);

	if (pg->pg_flags & testflags)
		return (TRUE);

	mybits = 0;
	mtx_enter(&pg->mdpage.pv_mtx);
	for (pve = pg->mdpage.pv_list; pve != NULL && mybits == 0;
	    pve = pve->pv_next) {
		level = pmap_find_pte_direct(pve->pv_pmap, pve->pv_va, &ptes,
		    &offs);
		mybits |= (ptes[offs] & testbits);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	if (mybits == 0)
		return (FALSE);

	atomic_setbits_int(&pg->pg_flags, pmap_pte2flags(mybits));

	return (TRUE);
}

/*
 * pmap_clear_attrs: change a page's attributes
 *
 * => we return TRUE if we cleared one of the bits we were asked to
 */

boolean_t
pmap_clear_attrs(struct vm_page *pg, unsigned long clearbits)
{
	struct pv_entry *pve;
	pt_entry_t *ptes, opte;
	u_long clearflags;
	int result, level, offs;

	clearflags = pmap_pte2flags(clearbits);

	result = pg->pg_flags & clearflags;
	if (result)
		atomic_clearbits_int(&pg->pg_flags, clearflags);

	mtx_enter(&pg->mdpage.pv_mtx);
	for (pve = pg->mdpage.pv_list; pve != NULL; pve = pve->pv_next) {
		level = pmap_find_pte_direct(pve->pv_pmap, pve->pv_va, &ptes,
		    &offs);
		opte = ptes[offs];
		if (opte & clearbits) {
			result = 1;
			pmap_pte_clearbits(&ptes[offs], (opte & clearbits));
			pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va,
				pmap_is_curpmap(pve->pv_pmap));
		}
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	pmap_tlb_shootwait();

	return (result != 0);
}

/*
 * p m a p   p r o t e c t i o n   f u n c t i o n s
 */

/*
 * pmap_page_protect: change the protection of all recorded mappings
 *	of a managed page
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_protect: set the protection in of the pages in a pmap
 *
 * => NOTE: this is an inline function in pmap.h
 */

/* see pmap.h */

/*
 * pmap_write_protect: write-protect pages in a pmap
 */

void
pmap_write_protect(struct pmap *pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	pt_entry_t nx, *ptes, *spte, *epte;
	pd_entry_t **pdes;
	vaddr_t blockend;
	int shootall = 0, shootself;
	vaddr_t va;
	paddr_t scr3;

	pmap_map_ptes(pmap, &ptes, &pdes, &scr3);
	shootself = (scr3 == 0);

	/* should be ok, but just in case ... */
	sva &= PG_FRAME;
	eva &= PG_FRAME;

	nx = 0;
	if (!(prot & PROT_EXEC))
		nx = pg_nx;

	if ((eva - sva > 32 * PAGE_SIZE) && pmap != pmap_kernel())
		shootall = 1;

	for (va = sva; va < eva ; va = blockend) {
		blockend = (va & L2_FRAME) + NBPD_L2;
		if (blockend > eva)
			blockend = eva;

		/*
		 * XXXCDC: our PTE mappings should never be write-protected!
		 *
		 * long term solution is to move the PTEs out of user
		 * address space.  and into kernel address space (up
		 * with APTE).  then we can set VM_MAXUSER_ADDRESS to
		 * be VM_MAX_ADDRESS.
		 */

		/* XXXCDC: ugly hack to avoid freeing PDP here */
		if (pl_i(va, PTP_LEVELS) == PDIR_SLOT_PTE)
			continue;

		/* empty block? */
		if (!pmap_pdes_valid(va, pdes, NULL))
			continue;

#ifdef DIAGNOSTIC
		if (va >= VM_MAXUSER_ADDRESS && va < VM_MAX_ADDRESS)
			panic("%s: PTE space", __func__);
#endif

		spte = &ptes[pl1_i(va)];
		epte = &ptes[pl1_i(blockend)];

		for (/*null */; spte < epte ; spte++) {
			if (!pmap_valid_entry(*spte))
				continue;
			pmap_pte_clearbits(spte, PG_RW);
			pmap_pte_setbits(spte, nx);
		}
	}

	if (shootall)
		pmap_tlb_shoottlb(pmap, shootself);
	else
		pmap_tlb_shootrange(pmap, sva, eva, shootself);

	pmap_unmap_ptes(pmap, scr3);
	pmap_tlb_shootwait();
}

/*
 * end of protection functions
 */

/*
 * pmap_unwire: clear the wired bit in the PTE
 *
 * => mapping should already be in map
 */

void
pmap_unwire(struct pmap *pmap, vaddr_t va)
{
	pt_entry_t *ptes;
	int level, offs;

	level = pmap_find_pte_direct(pmap, va, &ptes, &offs);

	if (level == 0) {

#ifdef DIAGNOSTIC
		if (!pmap_valid_entry(ptes[offs]))
			panic("%s: invalid (unmapped) va 0x%lx", __func__, va);
#endif
		if (__predict_true((ptes[offs] & PG_W) != 0)) {
			pmap_pte_clearbits(&ptes[offs], PG_W);
			pmap->pm_stats.wired_count--;
		}
#ifdef DIAGNOSTIC
		else {
			printf("%s: wiring for pmap %p va 0x%lx "
			       "didn't change!\n", __func__, pmap, va);
		}
#endif
	}
#ifdef DIAGNOSTIC
	else {
		panic("%s: invalid PDE", __func__);
	}
#endif
}

/*
 * pmap_collect: free resources held by a pmap
 *
 * => optional function.
 * => called when a process is swapped out to free memory.
 */

void
pmap_collect(struct pmap *pmap)
{
	/*
	 * free all of the pt pages by removing the physical mappings
	 * for its entire address space.
	 */

/*	pmap_do_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS,
	    PMAP_REMOVE_SKIPWIRED);
*/
}

/*
 * pmap_copy: copy mappings from one pmap to another
 *
 * => optional function
 * void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
 */

/*
 * defined as macro in pmap.h
 */

/*
 * pmap_enter: enter a mapping into a pmap
 *
 * => must be done "now" ... no lazy-evaluation
 */

int
pmap_enter(struct pmap *pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	pt_entry_t *ptes, opte, npte;
	pd_entry_t **pdes;
	struct vm_page *ptp, *pg = NULL;
	struct pv_entry *pve, *opve = NULL;
	int ptpdelta, wireddelta, resdelta;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	boolean_t nocache = (pa & PMAP_NOCACHE) != 0;
	boolean_t wc = (pa & PMAP_WC) != 0;
	int error, shootself;
	paddr_t scr3;

	KASSERT(!(wc && nocache));
	pa &= PMAP_PA_MASK;

#ifdef DIAGNOSTIC
	if (va == (vaddr_t) PDP_BASE)
		panic("%s: trying to map over PDP!", __func__);

	/* sanity check: kernel PTPs should already have been pre-allocated */
	if (va >= VM_MIN_KERNEL_ADDRESS &&
	    !pmap_valid_entry(pmap->pm_pdir[pl_i(va, PTP_LEVELS)]))
		panic("%s: missing kernel PTP for va %lx!", __func__, va);

#endif

	pve = pool_get(&pmap_pv_pool, PR_NOWAIT);
	if (pve == NULL) {
		if (flags & PMAP_CANFAIL) {
			error = ENOMEM;
			goto out;
		}
		panic("%s: no pv entries available", __func__);
	}

	/*
	 * map in ptes and get a pointer to our PTP (unless we are the kernel)
	 */

	pmap_map_ptes(pmap, &ptes, &pdes, &scr3);
	shootself = (scr3 == 0);
	if (pmap == pmap_kernel()) {
		ptp = NULL;
	} else {
		ptp = pmap_get_ptp(pmap, va, pdes);
		if (ptp == NULL) {
			if (flags & PMAP_CANFAIL) {
				pmap_unmap_ptes(pmap, scr3);
				error = ENOMEM;
				goto out;
			}
			panic("%s: get ptp failed", __func__);
		}
	}
	opte = ptes[pl1_i(va)];		/* old PTE */

	/*
	 * is there currently a valid mapping at our VA?
	 */

	if (pmap_valid_entry(opte)) {
		/*
		 * first, calculate pm_stats updates.  resident count will not
		 * change since we are replacing/changing a valid mapping.
		 * wired count might change...
		 */

		resdelta = 0;
		if (wired && (opte & PG_W) == 0)
			wireddelta = 1;
		else if (!wired && (opte & PG_W) != 0)
			wireddelta = -1;
		else
			wireddelta = 0;
		ptpdelta = 0;

		/*
		 * is the currently mapped PA the same as the one we
		 * want to map?
		 */

		if ((opte & PG_FRAME) == pa) {

			/* if this is on the PVLIST, sync R/M bit */
			if (opte & PG_PVLIST) {
				pg = PHYS_TO_VM_PAGE(pa);
#ifdef DIAGNOSTIC
				if (pg == NULL)
					panic("%s: same pa PG_PVLIST "
					      "mapping with unmanaged page "
					      "pa = 0x%lx (0x%lx)", __func__,
					      pa, atop(pa));
#endif
				pmap_sync_flags_pte(pg, opte);
			} else {
#ifdef DIAGNOSTIC
				if (PHYS_TO_VM_PAGE(pa) != NULL)
					panic("%s: same pa, managed "
					    "page, no PG_VLIST pa: 0x%lx\n",
					    __func__, pa);
#endif
			}
			goto enter_now;
		}

		/*
		 * changing PAs: we must remove the old one first
		 */

		/*
		 * if current mapping is on a pvlist,
		 * remove it (sync R/M bits)
		 */

		if (opte & PG_PVLIST) {
			pg = PHYS_TO_VM_PAGE(opte & PG_FRAME);
#ifdef DIAGNOSTIC
			if (pg == NULL)
				panic("%s: PG_PVLIST mapping with unmanaged "
				      "page pa = 0x%lx (0x%lx)",
				      __func__, pa, atop(pa));
#endif
			pmap_sync_flags_pte(pg, opte);
			opve = pmap_remove_pv(pg, pmap, va);
			pg = NULL; /* This is not the page we are looking for */
		}
	} else {	/* opte not valid */
		resdelta = 1;
		if (wired)
			wireddelta = 1;
		else
			wireddelta = 0;
		if (ptp)
			ptpdelta = 1;
		else
			ptpdelta = 0;
	}

	/*
	 * pve is either NULL or points to a now-free pv_entry structure
	 * (the latter case is if we called pmap_remove_pv above).
	 *
	 * if this entry is to be on a pvlist, enter it now.
	 */

	if (pmap_initialized)
		pg = PHYS_TO_VM_PAGE(pa);

	if (pg != NULL) {
		pmap_enter_pv(pg, pve, pmap, va, ptp);
		pve = NULL;
	}

enter_now:
	/*
	 * at this point pg is !NULL if we want the PG_PVLIST bit set
	 */

	pmap->pm_stats.resident_count += resdelta;
	pmap->pm_stats.wired_count += wireddelta;
	if (ptp)
		ptp->wire_count += ptpdelta;

	KASSERT(pg == PHYS_TO_VM_PAGE(pa));

	npte = pa | protection_codes[prot] | PG_V;
	if (pg != NULL) {
		npte |= PG_PVLIST;
		/*
		 * make sure that if the page is write combined all
		 * instances of pmap_enter make it so.
		 */
		if (pg->pg_flags & PG_PMAP_WC) {
			KASSERT(nocache == 0);
			wc = TRUE;
		}
	}
	if (wc)
		npte |= pmap_pg_wc;
	if (wired)
		npte |= PG_W;
	if (nocache)
		npte |= PG_N;
	if (va < VM_MAXUSER_ADDRESS)
		npte |= PG_u;
	else if (va < VM_MAX_ADDRESS)
		npte |= (PG_u | PG_RW);	/* XXXCDC: no longer needed? */
	if (pmap == pmap_kernel())
		npte |= PG_G;

	ptes[pl1_i(va)] = npte;		/* zap! */

	/*
	 * If we changed anything other than modified/used bits,
	 * flush the TLB.  (is this overkill?)
	 */
	if (pmap_valid_entry(opte)) {
		if (nocache && (opte & PG_N) == 0)
			wbinvd();
		pmap_tlb_shootpage(pmap, va, shootself);
	}

	pmap_unmap_ptes(pmap, scr3);
	pmap_tlb_shootwait();

	error = 0;

	if (pmap->pm_type == PMAP_TYPE_EPT)
		pmap_fix_ept(pmap, va);

out:
	if (pve)
		pool_put(&pmap_pv_pool, pve);
	if (opve)
		pool_put(&pmap_pv_pool, opve);

	return error;
}

boolean_t
pmap_get_physpage(vaddr_t va, int level, paddr_t *paddrp)
{
	struct vm_page *ptp;
	struct pmap *kpm = pmap_kernel();

	if (uvm.page_init_done == FALSE) {
		vaddr_t va;

		/*
		 * we're growing the kernel pmap early (from
		 * uvm_pageboot_alloc()).  this case must be
		 * handled a little differently.
		 */

		va = pmap_steal_memory(PAGE_SIZE, NULL, NULL);
		*paddrp = PMAP_DIRECT_UNMAP(va);
	} else {
		ptp = uvm_pagealloc(&kpm->pm_obj[level - 1],
				    ptp_va2o(va, level), NULL,
				    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
		if (ptp == NULL)
			panic("%s: out of memory", __func__);
		atomic_clearbits_int(&ptp->pg_flags, PG_BUSY);
		ptp->wire_count = 1;
		*paddrp = VM_PAGE_TO_PHYS(ptp);
	}
	kpm->pm_stats.resident_count++;
	return TRUE;
}

/*
 * Allocate the amount of specified ptps for a ptp level, and populate
 * all levels below accordingly, mapping virtual addresses starting at
 * kva.
 *
 * Used by pmap_growkernel.
 */
void
pmap_alloc_level(pd_entry_t **pdes, vaddr_t kva, int lvl, long *needed_ptps)
{
	unsigned long i;
	vaddr_t va;
	paddr_t pa;
	unsigned long index, endindex;
	int level;
	pd_entry_t *pdep;

	for (level = lvl; level > 1; level--) {
		if (level == PTP_LEVELS)
			pdep = pmap_kernel()->pm_pdir;
		else
			pdep = pdes[level - 2];
		va = kva;
		index = pl_i(kva, level);
		endindex = index + needed_ptps[level - 1];
		/*
		 * XXX special case for first time call.
		 */
		if (nkptp[level - 1] != 0)
			index++;
		else
			endindex--;

		for (i = index; i <= endindex; i++) {
			pmap_get_physpage(va, level - 1, &pa);
			pdep[i] = pa | PG_RW | PG_V | pg_nx;
			nkptp[level - 1]++;
			va += nbpd[level - 1];
		}
	}
}

/*
 * pmap_growkernel: increase usage of KVM space
 *
 * => we allocate new PTPs for the kernel and install them in all
 *	the pmaps on the system.
 */

static vaddr_t pmap_maxkvaddr = VM_MIN_KERNEL_ADDRESS;

vaddr_t
pmap_growkernel(vaddr_t maxkvaddr)
{
	struct pmap *kpm = pmap_kernel(), *pm;
	int s, i;
	unsigned newpdes;
	long needed_kptp[PTP_LEVELS], target_nptp, old;

	if (maxkvaddr <= pmap_maxkvaddr)
		return pmap_maxkvaddr;

	maxkvaddr = x86_round_pdr(maxkvaddr);
	old = nkptp[PTP_LEVELS - 1];
	/*
	 * This loop could be optimized more, but pmap_growkernel()
	 * is called infrequently.
	 */
	for (i = PTP_LEVELS - 1; i >= 1; i--) {
		target_nptp = pl_i(maxkvaddr, i + 1) -
		    pl_i(VM_MIN_KERNEL_ADDRESS, i + 1);
		/*
		 * XXX only need to check toplevel.
		 */
		if (target_nptp > nkptpmax[i])
			panic("%s: out of KVA space", __func__);
		needed_kptp[i] = target_nptp - nkptp[i] + 1;
	}


	s = splhigh();	/* to be safe */
	pmap_alloc_level(normal_pdes, pmap_maxkvaddr, PTP_LEVELS,
	    needed_kptp);

	/*
	 * If the number of top level entries changed, update all
	 * pmaps.
	 */
	if (needed_kptp[PTP_LEVELS - 1] != 0) {
		newpdes = nkptp[PTP_LEVELS - 1] - old;
		LIST_FOREACH(pm, &pmaps, pm_list) {
			memcpy(&pm->pm_pdir[PDIR_SLOT_KERN + old],
			       &kpm->pm_pdir[PDIR_SLOT_KERN + old],
			       newpdes * sizeof (pd_entry_t));
		}
	}
	pmap_maxkvaddr = maxkvaddr;
	splx(s);

	return maxkvaddr;
}

vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *start, vaddr_t *end)
{
	int segno;
	u_int npg;
	vaddr_t va;
	paddr_t pa;
	struct vm_physseg *seg;

	size = round_page(size);
	npg = atop(size);

	for (segno = 0, seg = vm_physmem; segno < vm_nphysseg; segno++, seg++) {
		if (seg->avail_end - seg->avail_start < npg)
			continue;
		/*
		 * We can only steal at an ``unused'' segment boundary,
		 * i.e. either at the start or at the end.
		 */
		if (seg->avail_start == seg->start ||
		    seg->avail_end == seg->end)
			break;
	}
	if (segno == vm_nphysseg) {
		panic("%s: out of memory", __func__);
	} else {
		if (seg->avail_start == seg->start) {
			pa = ptoa(seg->avail_start);
			seg->avail_start += npg;
			seg->start += npg;
		} else {
			pa = ptoa(seg->avail_end) - size;
			seg->avail_end -= npg;
			seg->end -= npg;
		}
		/*
		 * If all the segment has been consumed now, remove it.
		 * Note that the crash dump code still knows about it
		 * and will dump it correctly.
		 */
		if (seg->start == seg->end) {
			if (vm_nphysseg-- == 1)
				panic("%s: out of memory", __func__);
			while (segno < vm_nphysseg) {
				seg[0] = seg[1]; /* struct copy */
				seg++;
				segno++;
			}
		}

		va = PMAP_DIRECT_MAP(pa);
		memset((void *)va, 0, size);
	}

	if (start != NULL)
		*start = virtual_avail;
	if (end != NULL)
		*end = VM_MAX_KERNEL_ADDRESS;

	return (va);
}

void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = virtual_avail;
	*vendp = VM_MAX_KERNEL_ADDRESS;
}

/*
 * pmap_convert
 *
 * Converts 'pmap' to the new 'mode'.
 *
 * Parameters:
 *  pmap: the pmap to convert
 *  mode: the new mode (see pmap.h, PMAP_TYPE_xxx)
 *
 * Return value:
 *  always 0
 */
int
pmap_convert(struct pmap *pmap, int mode)
{
	pt_entry_t *pte;

	pmap->pm_type = mode;

	if (mode == PMAP_TYPE_EPT) {
		/* Clear low 512GB region (first PML4E) */
		pte = (pt_entry_t *)pmap->pm_pdir;
		*pte = 0;
	}

	return (0);	
}

#ifdef MULTIPROCESSOR
/*
 * Locking for tlb shootdown.
 *
 * We lock by setting tlb_shoot_wait to the number of cpus that will
 * receive our tlb shootdown. After sending the IPIs, we don't need to
 * worry about locking order or interrupts spinning for the lock because
 * the call that grabs the "lock" isn't the one that releases it. And
 * there is nothing that can block the IPI that releases the lock.
 *
 * The functions are organized so that we first count the number of
 * cpus we need to send the IPI to, then we grab the counter, then
 * we send the IPIs, then we finally do our own shootdown.
 *
 * Our shootdown is last to make it parallel with the other cpus
 * to shorten the spin time.
 *
 * Notice that we depend on failures to send IPIs only being able to
 * happen during boot. If they happen later, the above assumption
 * doesn't hold since we can end up in situations where noone will
 * release the lock if we get an interrupt in a bad moment.
 */

volatile long tlb_shoot_wait;

volatile vaddr_t tlb_shoot_addr1;
volatile vaddr_t tlb_shoot_addr2;

void
pmap_tlb_shootpage(struct pmap *pm, vaddr_t va, int shootself)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	long wait = 0;
	u_int64_t mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
		    !(ci->ci_flags & CPUF_RUNNING))
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait > 0) {
		int s = splvm();

		while (atomic_cas_ulong(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}
		tlb_shoot_addr1 = va;
		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (x86_fast_ipi(ci, LAPIC_IPI_INVLPG) != 0)
				panic("%s: ipi failed", __func__);
		}
		splx(s);
	}

	if (shootself)
		pmap_update_pg(va);
}

void
pmap_tlb_shootrange(struct pmap *pm, vaddr_t sva, vaddr_t eva, int shootself)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	long wait = 0;
	u_int64_t mask = 0;
	vaddr_t va;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
		    !(ci->ci_flags & CPUF_RUNNING))
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait > 0) {
		int s = splvm();

		while (atomic_cas_ulong(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}
		tlb_shoot_addr1 = sva;
		tlb_shoot_addr2 = eva;
		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (x86_fast_ipi(ci, LAPIC_IPI_INVLRANGE) != 0)
				panic("%s: ipi failed", __func__);
		}
		splx(s);
	}

	if (shootself)
		for (va = sva; va < eva; va += PAGE_SIZE)
			pmap_update_pg(va);
}

void
pmap_tlb_shoottlb(struct pmap *pm, int shootself)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	long wait = 0;
	u_int64_t mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) || !(ci->ci_flags & CPUF_RUNNING))
			continue;
		mask |= (1ULL << ci->ci_cpuid);
		wait++;
	}

	if (wait) {
		int s = splvm();

		while (atomic_cas_ulong(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				CPU_BUSY_CYCLE();
		}

		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & (1ULL << ci->ci_cpuid)) == 0)
				continue;
			if (x86_fast_ipi(ci, LAPIC_IPI_INVLTLB) != 0)
				panic("%s: ipi failed", __func__);
		}
		splx(s);
	}

	if (shootself)
		tlbflush();
}

void
pmap_tlb_shootwait(void)
{
	while (tlb_shoot_wait != 0)
		CPU_BUSY_CYCLE();
}

#else

void
pmap_tlb_shootpage(struct pmap *pm, vaddr_t va, int shootself)
{
	if (shootself)
		pmap_update_pg(va);

}

void
pmap_tlb_shootrange(struct pmap *pm, vaddr_t sva, vaddr_t eva, int shootself)
{
	vaddr_t va;

	if (!shootself)
		return;

	for (va = sva; va < eva; va += PAGE_SIZE)
		pmap_update_pg(va);

}

void
pmap_tlb_shoottlb(struct pmap *pm, int shootself)
{
	if (shootself)
		tlbflush();
}
#endif /* MULTIPROCESSOR */
@


1.104
log
@Kill SPINLOCK_SPIN_HOOK, use CPU_BUSY_CYCLE() instead.

ok visa@@, kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.103 2017/01/02 07:41:18 tedu Exp $	*/
d290 1
d292 1
d340 1
d347 1
@


1.103
log
@delete obsolete (disabled) pool_cache_invalidate. pool caches were removed
many years ago, and the just reimported version doesn't cache constructed
objects.
from Nick Gonella
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.102 2016/09/17 07:37:57 mlarkin Exp $	*/
a116 1
#include <machine/lock.h>
d2475 1
a2475 1
				SPINLOCK_SPIN_HOOK;
d2513 1
a2513 1
				SPINLOCK_SPIN_HOOK;
d2551 1
a2551 1
				SPINLOCK_SPIN_HOOK;
d2571 1
a2571 1
		SPINLOCK_SPIN_HOOK;
@


1.102
log
@
remove unused pmap_dump functions

ok kettenis, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.101 2016/09/16 02:35:41 dlg Exp $	*/
a2322 5

		/* Invalidate the PDP cache. */
#if 0
		pool_cache_invalidate(&pmap_pdp_cache);
#endif
@


1.101
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2016/09/15 02:00:16 dlg Exp $	*/
a2395 54

#ifdef DEBUG
void pmap_dump(struct pmap *, vaddr_t, vaddr_t);

/*
 * pmap_dump: dump all the mappings from a pmap
 *
 * => caller should not be holding any pmap locks
 */

void
pmap_dump(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	pt_entry_t *ptes, *pte;
	pd_entry_t **pdes;
	vaddr_t blkendva;
	paddr_t scr3;

	/*
	 * if end is out of range truncate.
	 * if (end == start) update to max.
	 */

	if (eva > VM_MAXUSER_ADDRESS || eva <= sva)
		eva = VM_MAXUSER_ADDRESS;

	pmap_map_ptes(pmap, &ptes, &pdes, &scr3);

	/*
	 * dumping a range of pages: we dump in PTP sized blocks (4MB)
	 */

	for (/* null */ ; sva < eva ; sva = blkendva) {

		/* determine range of block */
		blkendva = x86_round_pdr(sva+1);
		if (blkendva > eva)
			blkendva = eva;

		/* valid block? */
		if (!pmap_pdes_valid(sva, pdes, NULL))
			continue;

		pte = &ptes[pl1_i(sva)];
		for (/* null */; sva < blkendva ; sva += PAGE_SIZE, pte++) {
			if (!pmap_valid_entry(*pte))
				continue;
			printf("va %#lx -> pa %#llx (pte=%#llx)\n",
			       sva, *pte, *pte & PG_FRAME);
		}
	}
	pmap_unmap_ptes(pmap, scr3);
}
#endif
@


1.100
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2016/06/07 06:23:19 dlg Exp $	*/
d892 1
a892 1
		pmap->pm_ptphint[lidx] = RB_ROOT(&obj->memt);
d1144 2
a1145 1
		while ((pg = RB_ROOT(&pmap->pm_obj[i].memt)) != NULL) {
@


1.99
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2016/02/08 18:23:04 stefan Exp $	*/
d740 1
a740 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,
d742 2
a743 4
	pool_setipl(&pmap_pmap_pool, IPL_NONE);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
	    &pool_allocator_single);
	pool_setipl(&pmap_pv_pool, IPL_VM);
d750 2
a751 2
	pool_init(&pmap_pdp_pool, PAGE_SIZE, 0, 0, PR_WAITOK, "pdppl", NULL);
	pool_setipl(&pmap_pdp_pool, IPL_NONE);
@


1.98
log
@Set EPT bits of guest pages in pmap_enter instead of doing it
after an uvm_fault: uvm_fault maps in neighboring pages of
the faulting page. We want EPT bits set for those as soon as
possible as well. This avoids additional EPT violations
causing further uvm_faults when the guest accesses the
neighboring pages.

discussion with and ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2015/11/10 08:57:39 mlarkin Exp $	*/
d740 1
a740 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, PR_WAITOK,
d742 1
d745 1
a746 1
	pool_setipl(&pmap_pv_pool, IPL_VM);
d753 1
@


1.97
log
@
pmap changes required for vmm. Changes include addition of pm_type to
track type of pmap and various conversion and pte bit manipulation
functions for EPT.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2015/09/08 21:28:35 kettenis Exp $	*/
a415 4
 *  offs: (out) return the offset into the PD/PT for this va
 *
 * Return value:
 *  int value corresponding to the level this VA was found
d417 2
a418 2
int
pmap_fix_ept(struct pmap *pm, vaddr_t va, int *offs)
d423 1
a423 1
	int lev;
d430 1
a430 1
		*offs = (VA_SIGN_POS(va) & mask) >> shift;
d432 1
a432 1
		pd[*offs] |= EPT_R | EPT_W | EPT_X;
d443 1
a443 1
			pd[*offs] &= ~(0xF8);
d445 1
a445 1
		case 1: pd[*offs] |= EPT_WB;
d449 1
a449 1
		pde = pd[*offs];
d453 1
a453 1
			return (lev - 1);
d455 1
a455 1
		pdpa = (pd[*offs] & PG_FRAME);
a459 2

	return (0);
d2183 3
@


1.96
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2015/09/03 18:47:36 kettenis Exp $	*/
d406 62
d650 2
d1090 1
d2458 28
@


1.95
log
@Fix a race in pmap_page_remove().

This should get mail from the mailing lists flowing again.  Thanks to
millert@@ for helping me tracking this down.

ok millert@@, tedu@@, mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2015/07/10 10:08:49 kettenis Exp $	*/
d685 1
a685 1
	    &pool_allocator_nointr);
@


1.94
log
@Avoid calling pool_put(9) while holding a mutex here as well to prevent lock
order problems.

ok sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2015/06/30 08:40:55 mlarkin Exp $	*/
d1567 1
a1581 1
		pg->mdpage.pv_list = pve->pv_next;
d1583 1
d1587 1
a1587 1
		pmap_map_ptes(pve->pv_pmap, &ptes, &pdes, &scr3);
d1590 22
d1645 1
a1645 1
		pmap_unmap_ptes(pve->pv_pmap, scr3);
@


1.93
log
@
Clean up a needless check in an if statement.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2015/04/30 15:49:02 mlarkin Exp $	*/
d296 1
a296 1
    vaddr_t, int);
d298 1
a298 1
    vaddr_t, vaddr_t, int);
d1266 1
a1266 1
    vaddr_t startva, vaddr_t endva, int flags)
a1324 1

d1326 2
a1327 1
			pool_put(&pmap_pv_pool, pve);
d1346 1
a1346 1
    vaddr_t va, int flags)
d1391 5
a1395 2
	if (pve)
		pool_put(&pmap_pv_pool, pve);
d1426 2
d1464 1
a1464 1
			    &ptes[pl1_i(sva)], sva, flags);
d1481 1
a1481 6
		while ((ptp = TAILQ_FIRST(&empty_ptps)) != NULL) {
			TAILQ_REMOVE(&empty_ptps, ptp, pageq);
			uvm_pagefree(ptp);
                }

		return;
d1528 2
a1529 2
		pmap_remove_ptes(pmap, ptp,
		    (vaddr_t)&ptes[pl1_i(va)], va, blkendva, flags);
d1544 6
@


1.92
log
@
Clean up some spacing. No functional change
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2015/04/15 03:52:45 mlarkin Exp $	*/
d814 1
a814 1
	    pa == VM_PAGE_TO_PHYS(pmap->pm_ptphint[lidx])) {
d816 3
a818 6
	}
	if (lidx == 0)
		pg = uvm_pagelookup(&pmap->pm_obj[lidx], ptp_va2o(va, level));
	else {
		pg = uvm_pagelookup(&pmap->pm_obj[lidx], ptp_va2o(va, level));
	}
@


1.91
log
@
Unneeded return at the end of a void function.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2015/03/14 03:38:46 jsg Exp $	*/
d283 1
a283 1
void  pmap_enter_pv(struct vm_page *, struct pv_entry *, struct pmap *,
d309 3
a311 3
void	pmap_tlb_shootpage(struct pmap *, vaddr_t, int);
void	pmap_tlb_shootrange(struct pmap *, vaddr_t, vaddr_t, int);
void	pmap_tlb_shoottlb(struct pmap *, int);
d313 1
a313 1
void	pmap_tlb_shootwait(void);
@


1.90
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2015/03/10 20:12:39 kettenis Exp $	*/
a391 1
	return;
@


1.89
log
@Make the amd64 pmap (more) mpsafe by protecting both the pmap itself and the
pv lists with a mutex.  Rearange some code to avoid sleeping/spinning with
one of these locks held, and also take care that we don't allocate/free
any memory in that case.  This should make pmap_enter(9), pmap_remove(9) and
pmap_page_protect(9) safe to use without holding the kernel lock.

Other architectures will follow.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2015/02/07 01:46:27 kettenis Exp $	*/
a125 1
#include <dev/isa/isareg.h>
@


1.88
log
@Tedu the old idle page zeroing code.

ok tedu@@, guenther@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2015/02/02 09:29:53 mlarkin Exp $	*/
d106 1
a116 1
#include <machine/atomic.h>
d388 3
d399 3
d689 1
d774 1
d777 1
d792 1
d801 1
d1021 2
d1065 1
a1065 1
	refs = --pmap->pm_obj[0].uo_refs;
d1111 1
a1111 1
	pmap->pm_obj[0].uo_refs++;
a1437 1

d1578 1
d1581 2
d1622 1
d1624 1
d1626 1
d1661 1
d1668 1
d1698 1
d1710 1
a1809 1

d1898 1
a1898 1
	struct pv_entry *pve = NULL;
d1920 9
d1941 1
d2017 1
a2017 1
			pve = pmap_remove_pv(pg, pmap, va);
a2020 1
		pve = NULL;
a2042 10
		if (pve == NULL) {
			pve = pool_get(&pmap_pv_pool, PR_NOWAIT);
			if (pve == NULL) {
				if (flags & PMAP_CANFAIL) {
					error = ENOMEM;
					goto out;
				}
				panic("%s: no pv entries available", __func__);
			}
		}
d2044 1
a2044 4
	} else {
		/* new mapping is not PG_PVLIST.   free pve if we've got one */
		if (pve)
			pool_put(&pmap_pv_pool, pve);
a2093 1
		pmap_tlb_shootwait();
d2096 3
d2102 4
a2105 1
	pmap_unmap_ptes(pmap, scr3);
@


1.87
log
@
Remove some pmap locks that were #defined to be nothing (empty). Discussed
with many, ok kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2015/01/28 02:56:50 mlarkin Exp $	*/
a1226 39
}

/*
 * pmap_pagezeroidle: the same, for the idle loop page zero'er.
 * Returns TRUE if the page was zero'd, FALSE if we aborted for
 * some reason.
 */

boolean_t
pmap_pageidlezero(struct vm_page *pg)
{
	vaddr_t va = pmap_map_direct(pg);
	boolean_t rv = TRUE;
	long *ptr;
	int i;

	/*
	 * XXX - We'd really like to do this uncached. But at this moment
 	 *       we're never called, so just pretend that this works.
	 *       It shouldn't be too hard to create a second direct map
	 *       with uncached mappings.
	 */
	for (i = 0, ptr = (long *) va; i < PAGE_SIZE / sizeof(long); i++) {
		if (!curcpu_is_idle()) {

			/*
			 * A process has become ready.  Abort now,
			 * so we don't keep it waiting while we
			 * do slow memory access to finish this
			 * page.
			 */

			rv = FALSE;
			break;
		}
		*ptr++ = 0;
	}

	return (rv);
@


1.86
log
@
Remove an unused macro and a stale comment. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2015/01/15 15:30:17 sf Exp $	*/
a213 6
#define PMAP_MAP_TO_HEAD_LOCK()		/* null */
#define PMAP_MAP_TO_HEAD_UNLOCK()	/* null */

#define PMAP_HEAD_TO_MAP_LOCK()		/* null */
#define PMAP_HEAD_TO_MAP_UNLOCK()	/* null */

a217 1

a1460 1
	PMAP_MAP_TO_HEAD_LOCK();
a1506 1
		PMAP_MAP_TO_HEAD_UNLOCK();
a1575 1
	PMAP_MAP_TO_HEAD_UNLOCK();
a1604 2
	PMAP_HEAD_TO_MAP_LOCK();

a1648 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a1680 1
	PMAP_HEAD_TO_MAP_LOCK();
a1687 1
	PMAP_HEAD_TO_MAP_UNLOCK();
a1712 2
	PMAP_HEAD_TO_MAP_LOCK();

a1728 2
	PMAP_HEAD_TO_MAP_UNLOCK();

a1937 3
	/* get lock */
	PMAP_MAP_TO_HEAD_LOCK();

a2122 1
	PMAP_MAP_TO_HEAD_UNLOCK();
a2351 2

	PMAP_MAP_TO_HEAD_LOCK();
a2377 1
	PMAP_MAP_TO_HEAD_UNLOCK();
@


1.85
log
@Don't include i82489 headers in cpu.h

Only pmap.c was not including them explicitly. Fix that.

"The direction is good" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2015/01/06 00:38:32 dlg Exp $	*/
a213 2
/* int nkpde = NKPTP; */

a218 2

#define COUNT(x)	/* nothing */
@


1.84
log
@implement atomic_swap_{uint,ulong,ptr) and some md variants. use these
to replace x86_atomic_testset_{u32,u64}.

help from guenther@@ kettenis@@
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2014/12/23 07:42:46 tedu Exp $	*/
d120 5
@


1.83
log
@change pmap pools from nointr to waitok. pvpool left alone for now.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2014/12/15 04:54:44 tedu Exp $	*/
d219 1
a219 1
#define pmap_pte_set(p, n)		x86_atomic_testset_u64(p, n)
@


1.82
log
@move needed macros to pmap.c and delete stale ones. ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2014/11/21 06:41:17 mlarkin Exp $	*/
d684 2
a685 2
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
d694 1
a694 2
	pool_init(&pmap_pdp_pool, PAGE_SIZE, 0, 0, 0, "pdppl",
	    &pool_allocator_nointr);
@


1.81
log
@
Add NX (if available) to the mid-level page tables (PDE, etc).

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2014/11/20 06:51:41 mlarkin Exp $	*/
d218 5
@


1.80
log
@
Move previous PTE permission fixup code into locore, and fixup some more
ranges while we're there.

ok deraadt@@, tested by many and in snaps
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2014/11/20 06:45:45 mlarkin Exp $	*/
d978 1
a978 1
	pdir[PDIR_SLOT_PTE] = pdirpa | PG_V | PG_KW;
d2211 1
a2211 1
			pdep[i] = pa | PG_RW | PG_V;
@


1.79
log
@
Revert the previous changes that enabled various PTE permissions fixups
in pmap.c - equivalent code will shortly be committed to locore.S that
accomplishes the same thing.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2014/11/16 12:30:56 deraadt Exp $	*/
d463 2
a464 2
	if ((cpu_feature & CPUID_NXE) && !(prot & PROT_EXEC))
		npte |= PG_NX;
a537 1
	pt_entry_t pg_nx = (cpu_feature & CPUID_NXE? PG_NX : 0);
d631 1
a631 1
		    PG_M;
d642 1
a642 1
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_U | PG_M;
d1791 2
a1792 2
	if ((cpu_feature & CPUID_NXE) && !(prot & PROT_EXEC))
		nx = PG_NX;
@


1.78
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2014/11/07 03:20:02 mlarkin Exp $	*/
a280 41
extern vaddr_t __rodata_start;
extern int __rodata_size;
extern vaddr_t erodata;

/*
 * NX range definitions
 */
struct nx_range_description {
	vaddr_t start;
	size_t size;
	vm_prot_t prot;
	int is_ptr;
};

static const struct nx_range_description nx_ranges[] = {
	/*
	 * List of ranges to map as NX (non-execute) if the processor supports
	 * NX. Each range consists of a start vaddr and size (in bytes), and a
	 * protection value (eg, PROT_READ or PROT_READ | PROT_WRITE).
	 *
	 * The list also includes an 'is_ptr' field in each element to denote
	 * if the 'start' value is a constant (is_ptr == 0) or should be
	 * interpreted as an address containing the real value (is_ptr == 1).
	 *
	 * The range includes the page containing [start va] and extends through
	 * and including the page containing [end va].
	 */
	{ /* .rodata range */
	    (vaddr_t)&__rodata_start,
	    (size_t)&__rodata_size,
	    PROT_READ,
	    0
	},
	{ /* ISA hole */
	    (vaddr_t)&atdevbase,
	    IOM_SIZE,
	    PROT_READ | PROT_WRITE,
	    1
	}
};

d534 1
a534 1
	vaddr_t kva, kva_end, kva_start = VM_MIN_KERNEL_ADDRESS, nx_start;
d540 1
a540 1
	paddr_t dmpd, dmpdp, nx_paddr;
a691 19

	/*
	 * Enable NX ranges, resets page permissions on already mapped ranges
	 */
	 if (pg_nx) {
		for (i = 0 ; i < nitems(nx_ranges); i++ ) {
			if (nx_ranges[i].is_ptr)
				nx_start = *(vaddr_t *)(nx_ranges[i].start);
			else
				nx_start = nx_ranges[i].start;

			for (kva = nx_start; kva < nx_start + nx_ranges[i].size;
			    kva += PAGE_SIZE) {
				if(pmap_extract(pmap_kernel(), kva, &nx_paddr))
					pmap_kenter_pa(kva, nx_paddr,
					    nx_ranges[i].prot);
			}
		}	
	}
@


1.77
log
@
Map .rodata and the KVA range corresponding to the ISA hole as NX on amd64
processors that support NX.

Tested on amd64 machines with and without NX capability.

Additional NX ranges can be added as desired to the table defined by
this diff.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2014/10/31 04:33:51 mlarkin Exp $	*/
d299 1
a299 1
	 * protection value (eg, VM_PROT_READ or VM_PROT_READ | VM_PROT_WRITE).
d311 1
a311 1
	    VM_PROT_READ,
d317 1
a317 1
	    VM_PROT_READ | VM_PROT_WRITE,
d496 1
a496 1
	npte = (pa & PMAP_PA_MASK) | ((prot & VM_PROT_WRITE) ? PG_RW : PG_RO) |
d504 1
a504 1
	if ((cpu_feature & CPUID_NXE) && !(prot & VM_PROT_EXECUTE))
d596 8
a603 9
	protection_codes[VM_PROT_NONE] = pg_nx;			/* --- */
	protection_codes[VM_PROT_EXECUTE] = PG_RO;		/* --x */
	protection_codes[VM_PROT_READ] = PG_RO | pg_nx;		/* -r- */
	protection_codes[VM_PROT_READ|VM_PROT_EXECUTE] = PG_RO;	/* -rx */
	protection_codes[VM_PROT_WRITE] = PG_RW | pg_nx;	/* w-- */
	protection_codes[VM_PROT_WRITE|VM_PROT_EXECUTE] = PG_RW;/* w-x */
	protection_codes[VM_PROT_WRITE|VM_PROT_READ] = PG_RW | pg_nx;
								/* wr- */
	protection_codes[VM_PROT_ALL] = PG_RW;			/* wrx */
d1852 1
a1852 1
	if ((cpu_feature & CPUID_NXE) && !(prot & VM_PROT_EXECUTE))
@


1.76
log
@
Fix a missing include in amd64 pmap.c that resulted in an erroneous memory
map entry being entered in uniprocessor (UP) kernels. Multiprocessor (MP)
kernels not affected.

ok guenther, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2014/10/18 17:28:34 kettenis Exp $	*/
d281 41
d575 1
a575 1
	vaddr_t kva, kva_end, kva_start = VM_MIN_KERNEL_ADDRESS;
d581 1
a581 1
	paddr_t dmpd, dmpdp;
d734 19
@


1.75
log
@Make sure the direct map isn't executable on hardware that allows us to do so.
Enforcing W^X in the kernel like this mitigates at least some ret2dir attacks.

ok mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2014/10/06 20:34:58 sf Exp $	*/
d123 2
@


1.74
log
@Make amd64 pmap more efficient on multi-processor

With the current implementation, when accessing an inactive pmap, its
ptes are mapped in the APTE range. This has the problem that the APTE
range is mapped on all CPUs and changes to the APTE must therefore be
followed by a remote TLB flush on all CPUs. This is very inefficient
because the costs increase quadratically with the number of CPUs.

Therefore, the code is changed to remove the APTE mechanism completely
and instead switch the pmap locally. A remote TLB flush is then only
done if the pmap is in use on the remote CPU. In the common case, this
will replace one TLB flush on all CPUs with two local TLB flushes.

An additional optimization is done in cases where only a single PTE of
an inactive pmap is accessed: The requested PTE is found by walking the
page tables manually via the direct mapping. This makes some more TLB
flushes unnecessary.

Furthermore, some code is reordered so that the TLB-shootdown-IPIs are
sent first, then more local processing takes place, and only afterwards
the CPU waits for the remote TLB-shootdowns to finish.

This diff is based on a patch for i386 by Artur Grabowski <art blahonga org>
from 2008. Some additional bits were taken from a different patch by
Artur from 2005.

Tested by many. OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2014/09/16 18:57:51 sf Exp $	*/
d646 1
a646 1
	    PG_M;
@


1.73
log
@Simple cleanups for amd64 pmap

- use __func__ in panics/printfs (fixes some out of sync function names)
- tell the compiler that code paths where we print diagnostics are unlikely
- use pmap_valid_entry() in some places
- remove KERNSPACE, which is not used anywhere

OK guenther@@ mlarkin@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2014/08/24 17:55:14 sf Exp $	*/
a205 1
pd_entry_t *alternate_pdes[] = APDES_INITIALIZER;
d287 1
d292 1
a292 1
void pmap_map_ptes(struct pmap *, pt_entry_t **, pd_entry_t ***);
d302 1
a302 1
void pmap_unmap_ptes(struct pmap *);
a305 1
void pmap_apte_flush(struct pmap *pmap);
d309 10
a360 7
void
pmap_apte_flush(struct pmap *pmap)
{
	pmap_tlb_shoottlb();
	pmap_tlb_shootwait();
}

a362 3
 *
 * => we lock enough pmaps to keep things locked in
 * => must be undone with pmap_unmap_ptes before returning
d366 1
a366 1
pmap_map_ptes(struct pmap *pmap, pt_entry_t **ptepp, pd_entry_t ***pdeppp)
d368 15
a382 1
	pd_entry_t opde, npde;
d384 1
a384 5
	/* if curpmap then we are always mapped */
	if (pmap_is_curpmap(pmap)) {
		*ptepp = PTE_BASE;
		*pdeppp = normal_pdes;
		return;
d387 11
a397 7
	/* need to load a new alternate pt space into curpmap? */
	opde = *APDP_PDE;
	if (!pmap_valid_entry(opde) || (opde & PG_FRAME) != pmap->pm_pdirpa) {
		npde = (pd_entry_t) (pmap->pm_pdirpa | PG_RW | PG_V);
		*APDP_PDE = npde;
		if (pmap_valid_entry(opde))
			pmap_apte_flush(curpcb->pcb_pmap);
a398 2
	*ptepp = APTE_BASE;
	*pdeppp = alternate_pdes;
d401 2
a402 2
void
pmap_unmap_ptes(struct pmap *pmap)
d404 22
a425 2
	if (pmap_is_curpmap(pmap))
		return;
d427 1
a427 5
#if defined(MULTIPROCESSOR)
	*APDP_PDE = 0;
	pmap_apte_flush(curpcb->pcb_pmap);
#endif
	COUNT(apdp_pde_unmap);
d430 1
d473 1
a473 1
		pmap_tlb_shootpage(pmap_kernel(), va);
d507 1
a507 1
	pmap_tlb_shootrange(pmap_kernel(), sva, eva);
d853 2
a854 1
		    invaladdr + index * PAGE_SIZE);
d858 2
a859 1
		pmap_tlb_shootpage(pmap, invaladdr + index * PAGE_SIZE);
d1087 1
a1087 5
	/*
	 * MULTIPROCESSOR -- no need to flush out of other processors'
	 * APTE space because we do that in pmap_unmap_ptes().
	 */
	/* XXX: need to flush it out of other processor's APTE space? */
d1176 2
a1177 2
	pt_entry_t *ptes, pte;
	pd_entry_t pde, **pdes;
d1185 1
a1185 4
	pmap_map_ptes(pmap, &ptes, &pdes);
	if (pmap_pdes_valid(va, pdes, &pde) == FALSE) {
		return FALSE;
	}
d1187 1
a1187 1
	if (pde & PG_PS) {
d1189 1
a1189 2
			*pap = (pde & PG_LGFRAME) | (va & 0x1fffff);
		pmap_unmap_ptes(pmap);
d1192 1
a1192 5

	pte = ptes[pl1_i(va)];
	pmap_unmap_ptes(pmap);

	if (__predict_true(pmap_valid_entry(pte))) {
d1194 1
a1194 1
			*pap = (pte & PG_FRAME) | (va & 0xfff);
d1457 1
a1457 1
	int shootall = 0;
d1459 1
d1464 3
a1466 1
	pmap_map_ptes(pmap, &ptes, &pdes);
d1504 5
a1508 1
			pmap_tlb_shootpage(pmap, sva);
a1509 3

		pmap_tlb_shootwait();
		pmap_unmap_ptes(pmap);
d1574 1
a1574 1
		pmap_tlb_shoottlb();
d1576 1
a1576 1
		pmap_tlb_shootrange(pmap, sva, eva);
d1578 1
a1579 2

	pmap_unmap_ptes(pmap);
d1605 2
d1615 3
a1617 1
		pmap_map_ptes(pve->pv_pmap, &ptes, &pdes);
d1640 1
a1640 1
		pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va);
d1652 1
a1652 1
		pmap_unmap_ptes(pve->pv_pmap);
d1680 2
a1681 2
	pt_entry_t *ptes, pte;
	pd_entry_t **pdes;
d1693 3
a1695 4
		pmap_map_ptes(pve->pv_pmap, &ptes, &pdes);
		pte = ptes[pl1_i(pve->pv_va)];
		pmap_unmap_ptes(pve->pv_pmap);
		mybits |= (pte & testbits);
a1717 1
	pd_entry_t **pdes;
d1719 1
a1719 1
	int result;
d1730 3
a1732 7
		pmap_map_ptes(pve->pv_pmap, &ptes, &pdes);
#ifdef DIAGNOSTIC
		if (!pmap_pdes_valid(pve->pv_va, pdes, NULL))
			panic("%s: mapping without PTP detected", __func__);
#endif

		opte = ptes[pl1_i(pve->pv_va)];
d1735 3
a1737 3
			pmap_pte_clearbits(&ptes[pl1_i(pve->pv_va)],
			    (opte & clearbits));
			pmap_tlb_shootpage(pve->pv_pmap, pve->pv_va);
a1738 1
		pmap_unmap_ptes(pve->pv_pmap);
d1779 1
a1779 1
	int shootall = 0;
d1781 1
d1783 2
a1784 1
	pmap_map_ptes(pmap, &ptes, &pdes);
d1836 1
a1836 1
		pmap_tlb_shoottlb();
d1838 3
a1840 1
		pmap_tlb_shootrange(pmap, sva, eva);
a1842 2

	pmap_unmap_ptes(pmap);
d1859 1
a1859 1
	pd_entry_t **pdes;
d1861 1
a1861 1
	pmap_map_ptes(pmap, &ptes, &pdes);
d1863 1
a1863 1
	if (pmap_pdes_valid(va, pdes, NULL)) {
d1866 1
a1866 1
		if (!pmap_valid_entry(ptes[pl1_i(va)]))
d1869 2
a1870 2
		if (__predict_true((ptes[pl1_i(va)] & PG_W) != 0)) {
			pmap_pte_clearbits(&ptes[pl1_i(va)], PG_W);
a1878 1
		pmap_unmap_ptes(pmap);
d1935 2
a1936 1
	int error;
d1942 2
a1943 2
	if (va == (vaddr_t) PDP_BASE || va == (vaddr_t) APDP_BASE)
		panic("%s: trying to map over PDP/APDP!", __func__);
d1959 2
a1960 1
	pmap_map_ptes(pmap, &ptes, &pdes);
d2132 1
a2132 1
		pmap_tlb_shootpage(pmap, va);
d2139 1
a2139 1
	pmap_unmap_ptes(pmap);
d2360 1
d2372 1
a2372 1
	pmap_map_ptes(pmap, &ptes, &pdes);
d2397 1
a2397 1
	pmap_unmap_ptes(pmap);
d2438 1
a2438 1
pmap_tlb_shootpage(struct pmap *pm, vaddr_t va)
d2470 1
a2470 1
	if (pmap_is_curpmap(pm))
d2475 1
a2475 1
pmap_tlb_shootrange(struct pmap *pm, vaddr_t sva, vaddr_t eva)
d2509 1
a2509 1
	if (pmap_is_curpmap(pm))
d2515 1
a2515 1
pmap_tlb_shoottlb(void)
d2523 1
a2523 1
		if (ci == self || !(ci->ci_flags & CPUF_RUNNING))
d2546 2
a2547 1
	tlbflush();
d2560 1
a2560 1
pmap_tlb_shootpage(struct pmap *pm, vaddr_t va)
d2562 1
a2562 1
	if (pmap_is_curpmap(pm))
d2568 1
a2568 1
pmap_tlb_shootrange(struct pmap *pm, vaddr_t sva, vaddr_t eva)
d2572 3
d2581 1
a2581 1
pmap_tlb_shoottlb(void)
d2583 2
a2584 1
	tlbflush();
@


1.72
log
@remove trailing whitespace
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2014/07/11 16:35:40 jsg Exp $	*/
d439 1
a439 1
		panic("pmap_kenter_pa: PG_PS");
d902 1
a902 1
				panic("pde page disappeared");
d918 1
a918 1
			panic("pmap_get_ptp: unmanaged user PTP");
d1034 2
a1035 2
	if (pmap->pm_cpus != 0)
		printf("pmap_destroy: pmap %p cpus=0x%llx\n",
d1135 1
a1135 1
		if ((pde & PG_V) == 0)
d1174 1
a1174 1
	if (__predict_true((pte & PG_V) != 0)) {
d1324 2
a1325 2
				panic("pmap_remove_ptes: managed page without "
				      "PG_PVLIST for 0x%lx", startva);
d1332 2
a1333 2
			panic("pmap_remove_ptes: unmanaged page marked "
			      "PG_PVLIST, va = 0x%lx, pa = 0x%lx",
d1391 2
a1392 2
			panic("pmap_remove_pte: managed page without "
			      "PG_PVLIST for 0x%lx", va);
d1399 2
a1400 3
		panic("pmap_remove_pte: unmanaged page marked "
		    "PG_PVLIST, va = 0x%lx, pa = 0x%lx", va,
		    (u_long)(opte & PG_FRAME));
d1466 2
a1467 2
					panic("pmap_remove: unmanaged "
					      "PTP detected");
d1539 1
a1539 2
				panic("pmap_remove: unmanaged PTP "
				      "detected");
d1597 1
a1597 1
			printf("pmap_page_remove: pg=%p: va=%lx, pv_ptp=%p\n",
d1599 2
a1600 2
			printf("pmap_page_remove: PTP's phys addr: "
			       "actual=%lx, recorded=%lx\n",
d1603 2
a1604 2
			panic("pmap_page_remove: mapped managed page has "
			      "invalid pv_ptp field");
d1710 1
a1710 2
			panic("pmap_change_attrs: mapping without PTP "
			      "detected");
d1801 1
a1801 1
			panic("pmap_write_protect: PTE space");
d1808 1
a1808 1
			if (!(*spte & PG_V))
d1847 1
a1847 1
			panic("pmap_unwire: invalid (unmapped) va 0x%lx", va);
d1849 1
a1849 1
		if ((ptes[pl1_i(va)] & PG_W) != 0) {
d1855 2
a1856 2
			printf("pmap_unwire: wiring for pmap %p va 0x%lx "
			       "didn't change!\n", pmap, va);
d1863 1
a1863 1
		panic("pmap_unwire: invalid PDE");
d1923 1
a1923 1
		panic("pmap_enter: trying to map over PDP/APDP!");
d1928 1
a1928 1
		panic("pmap_enter: missing kernel PTP for va %lx!", va);
d1949 1
a1949 1
			panic("pmap_enter: get ptp failed");
d1986 1
a1986 1
					panic("pmap_enter: same pa PG_PVLIST "
d1988 2
a1989 2
					      "pa = 0x%lx (0x%lx)", pa,
					      atop(pa));
d1995 1
a1995 1
					panic("pmap_enter: same pa, managed "
d1997 1
a1997 1
					    pa);
d2016 3
a2018 3
				panic("pmap_enter: PG_PVLIST mapping with "
				      "unmanaged page "
				      "pa = 0x%lx (0x%lx)", pa, atop(pa));
d2055 1
a2055 1
				panic("pmap_enter: no pv entries available");
d2075 1
a2075 2
	if (pg != PHYS_TO_VM_PAGE(pa))
		panic("wtf?");
d2108 1
a2108 1
	if (opte & PG_V) {
d2146 1
a2146 1
			panic("pmap_get_physpage: out of memory");
d2230 1
a2230 1
			panic("out of KVA space");
d2286 1
a2286 1
		panic("pmap_steal_memory: out of memory");
d2304 1
a2304 1
				panic("pmap_steal_memory: out of memory");
d2443 1
a2443 1
				panic("pmap_tlb_shootpage: ipi failed");
d2482 1
a2482 1
				panic("pmap_tlb_shootrange: ipi failed");
d2519 1
a2519 1
				panic("pmap_tlb_shoottlb: ipi failed");
@


1.71
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2014/06/15 11:43:24 sf Exp $	*/
d212 1
a212 1
  
d215 1
a215 1
 
d730 2
a731 2
 * pve: preallocated pve for us to use 
 * ptp: PTP in pmap that maps this VA 
d1112 1
a1112 1
	 * mark the pmap no longer in use by this processor. 
d2085 1
a2085 1
		 * make sure that if the page is write combined all 
d2256 1
a2256 1
#if 0  
d2554 1
a2554 1
		pmap_update_pg(va);	
@


1.70
log
@Fix a few format string bugs with -DDEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2014/03/27 10:24:40 dlg Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.69
log
@replace x86_atomic_cas_things with atomic_cas_foo equivalents.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2014/03/07 16:56:57 guenther Exp $	*/
d2382 1
a2382 1
			printf("va %#lx -> pa %#lx (pte=%#lx)\n",
@


1.68
log
@We already assume global mappings (PG_G) are supported, so pmap_pg_g is
unnecessary

ok krw@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2013/11/19 04:12:17 guenther Exp $	*/
d2445 1
a2445 1
		while (x86_atomic_cas_ul(&tlb_shoot_wait, 0, wait) != 0) {
d2483 1
a2483 1
		while (x86_atomic_cas_ul(&tlb_shoot_wait, 0, wait) != 0) {
d2521 1
a2521 1
		while (x86_atomic_cas_ul(&tlb_shoot_wait, 0, wait) != 0) {
@


1.67
log
@format string fixes picked up with -Wformat=2

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2013/11/01 01:09:43 dlg Exp $	*/
a231 7
 * pmap_pg_g: if our processor supports PG_G in the PTE then we
 * set pmap_pg_g to PG_G (otherwise it is zero).
 */

int pmap_pg_g = 0;

/*
d438 1
a438 1
		npte |= pmap_pg_g;
a573 2
	pmap_pg_g = PG_G;		/* enable software */

d2111 1
a2111 1
		npte |= pmap_pg_g;
@


1.66
log
@get rid of pmap_pdp_cache_generation. it cannt change between where it is
set and where it is tested again.

instead of using pool ctors, just init the memory out of pool_get.

ive been running this on a variety of amd64s without issue for a while now.
deraadt also had no problems running it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2013/06/02 16:45:12 guenther Exp $	*/
d1051 1
a1051 1
		printf("pmap_destroy: pmap %p cpus=0x%lx\n",
@


1.65
log
@Fix descriptions of structures
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2013/06/02 16:38:05 guenther Exp $	*/
d280 1
a280 3
u_int pmap_pdp_cache_generation;

int	pmap_pdp_ctor(void *, void *, int);
d678 1
a678 3
		  &pool_allocator_nointr);
	pool_set_ctordtor(&pmap_pdp_pool, pmap_pdp_ctor, NULL, NULL);

d951 2
a952 2
int
pmap_pdp_ctor(void *arg, void *object, int flags)
a953 1
	pd_entry_t *pdir = object;
a980 2

	return (0);
a994 1
	u_int gen;
a1014 2
try_again:
	gen = pmap_pdp_cache_generation;
d1016 1
a1016 5

	if (gen != pmap_pdp_cache_generation) {
		pool_put(&pmap_pdp_pool, pmap->pm_pdir);
		goto try_again;
	}
a2274 1
		pmap_pdp_cache_generation++;
@


1.64
log
@Don't need gdt.h here
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2011/05/17 18:06:13 ariane Exp $	*/
d164 1
a164 2
 *
 *  - struct pmap: describes the address space of one thread
d166 1
a166 1
 * - pmap_remove_record: a list of virtual addresses whose mappings
@


1.63
log
@Use the same define checks as used in machdep.c for the trampoline code.
from Brad

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2011/04/15 15:16:57 chl Exp $	*/
a126 1
#include <machine/gdt.h>
@


1.62
log
@Remove dead assignment and newly created unused variable.

Found by LLVM/Clang Static Analyzer.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2011/03/14 00:05:46 guenther Exp $	*/
a291 1
#ifdef _LP64
a293 1
#endif
d650 2
a651 1
#ifdef _LP64
d661 1
@


1.61
log
@Fix spelling in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2010/11/30 19:30:16 kettenis Exp $	*/
d1793 1
a1793 1
	pt_entry_t nx, opte, *ptes, *spte, *epte;
a1844 1
			opte = *spte;
@


1.60
log
@Extend bitmasks to 64-bit such that we can support up to 64 CPU cores.

tested by dlg@@, ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2010/11/20 20:33:23 miod Exp $	*/
d2439 1
a2439 1
 * Our shootdown is last to make it parallell with the other cpus
@


1.59
log
@This is a first step towards getting rid of avail_start and avail_end in the
kernel, currently limited to low-hanging fruit: these variables were used
by bus_dma to specify the range in which to allocate memory, back when
uvm_pglistalloc() was stupid and would not walk the vm_physseg[].

Nowadays, except on some platforms for early initialization, these variables
are not used, or do not need to be global variables. Therefore:
- remove `extern' declarations of avail_start and avail_end (or close cousins,
  such as arm physical_start and physical_end) from files which no longer need
  to use them.
- make them local variables whenever possible.
- remove them when they are assigned to but no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2010/11/13 04:16:42 guenther Exp $	*/
d354 1
a354 1
	    (pmap->pm_cpus & (1U << cpu_id)) != 0);
d1130 1
a1130 1
		x86_atomic_setbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1146 1
a1146 2
	x86_atomic_clearbits_ul(&pmap->pm_cpus, (1U << cpu_number()));

d2459 1
a2459 1
	int mask = 0;
d2465 1
a2465 1
		mask |= 1 << ci->ci_cpuid;
d2478 1
a2478 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
d2496 1
a2496 1
	int mask = 0;
d2503 1
a2503 1
		mask |= 1 << ci->ci_cpuid;
d2517 1
a2517 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
d2536 1
a2536 1
	int mask = 0;
d2541 1
a2541 1
		mask |= 1 << ci->ci_cpuid;
d2554 1
a2554 1
			if ((mask & 1 << ci->ci_cpuid) == 0)
@


1.58
log
@Switch from TSS-per-process to TSS-per-CPU, placing the TSS right
next to the cpu's GDT, also making the double-fault stack per-CPU,
leaving it at the top of the page of the CPU's idle process.  Inline
pmap_activate() and pmap_deactivate() into the asm cpu_switchto
routine, adding a check for the new pmap already being marked as
active on the CPU.  Garbage collect the hasn't-been-used-in-years
GDT update IPI.

Tested by many; ok mikeb@@, kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2010/10/26 05:49:10 guenther Exp $	*/
a245 9

/*
 * i386 physical memory comes in a big contig chunk with a small
 * hole toward the front of it...  the following 4 paddr_t's
 * (shared with machdep.c) describe the physical address space
 * of this machine.
 */
paddr_t avail_start;	/* PA of first available physical page */
paddr_t avail_end;	/* PA of last available physical page */
@


1.57
log
@The LDT is only used by dead compat code now, so load the ldt
register with the null selector (disabling use of it), stop reloading
it on every context switch, and blow away the table itself, as well
as the pcb and pmap bits that were used to track it.  Also, delete
two other unused pcb members: pcb_usersp and pcb_flags.  (Deleting
pcb_usersp also keeps the pcb_savefpu member aligned properly.)
Finally, delete the defines for the unimplemented AMD64_{GET,SET}_LDT
sysarch() calls.

Tested by various with both AMD and Intel chips
ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2010/09/06 17:36:49 guenther Exp $	*/
d1121 1
a1121 1
 * => called from cpu_switch()
@


1.56
log
@Stop sending so many superfluous IPIs: zero out pm_cpus in pmap_create(),
don't set the current cpu's bit in pmap_activate() unless we actually
set %cr3, and add a DIAGNOSTIC printf to pmap_destroy() to catch if we
ever stop tracking them accurately again.  Also, GC the unused pm_flags
member.

ok deraadt@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2010/05/13 19:27:24 oga Exp $	*/
d124 1
a1025 5
	/* init the LDT */
	pmap->pm_ldt = NULL;
	pmap->pm_ldt_len = 0;
	pmap->pm_ldt_sel = GSYSSEL(GLDT_SEL, SEL_KPL);

d1119 1
a1119 1
 * pmap_activate: activate a process' pmap (fill in %cr3 and LDT info)
a1131 1
	pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
a1140 2
	if (pcb == curpcb)
		lldt(pcb->pcb_ldt_sel);
@


1.55
log
@Synchronise amd64 more with other PMAP_DIRECT architectures. (step 1,
more to come later)

Specfically, there is no reason to reserve a special virtual address just so we
can do boot dump, we have a direct map of every page anyway.

since pmap_map is deprecated and MD only anyway, this means we can remove that
interface too. If anything this should increase reliability since pmap_enter
won't fail under memory pressure during dump (unlikely but possible). It is also
simpler and smaller ;)

Tested by myself and ckuethe, no regressions.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2010/05/08 16:54:07 oga Exp $	*/
d1023 1
a1023 1
	pmap->pm_flags = 0;
d1078 6
d1138 1
a1138 1
	if (p == curproc)
d1140 6
a1147 5

	/*
	 * mark the pmap in use by this processor.
	 */
	x86_atomic_setbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
@


1.54
log
@Page Attribute Tables (PAT) support for x86.

PAT allows setting per-mapping cachability bits. Our main interest in it
for write combining mappings so we do not have to rely so heaviliy on
mtrrs (which are stupidly set up on more and more machines). MD flags to
pmap allow setting these bits (which bus_space now uses for PREFETCHABLE
maps), if a vm page has a bit set, then we will use WC for all mappings
of a page (used for userland mappings). We also check for known errata
and fall back to UC- mappings in that case.

comments from kettenis@@, tedu@@ and william@@. kettenis@@, tedu@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2010/04/30 21:56:39 oga Exp $	*/
a207 4
/*
 * XXX: would be nice to have per-CPU VAs for the above 4
 */

a669 7

	/*
	 * now we reserve some VM for mapping pages when doing a crash dump
	 */

	virtual_avail = reserve_dumppages(virtual_avail);

a1223 20

/*
 * pmap_map: map a range of PAs into kvm
 *
 * => used during crash dump
 * => XXX: pmap_map() should be phased out?
 */

vaddr_t
pmap_map(vaddr_t va, paddr_t spa, paddr_t epa, vm_prot_t prot)
{
	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
		va += PAGE_SIZE;
		spa += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	return va;
}

@


1.53
log
@Right now, if anything internal changes with a uvm object, diverse
places in the tree need to be touched to update the object
initialisation with respect to that.

So, make a function (uvm_initobj) that takes the refcount, object and
pager ops and does this initialisation for us. This should save on
maintainance in the future.

looked good to fgs@@. Tedu complained about the British spelling but OKed
it anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2009/12/09 14:31:57 oga Exp $	*/
d244 7
d458 2
a459 1
	    ((pa & PMAP_NOCACHE) ? PG_N : 0) | PG_V;
d545 1
a545 1
	 * define the voundaries of the managed kernel virtual address
d1992 1
d1995 1
d2156 1
a2156 1
	if (pg != NULL)
d2158 11
@


1.52
log
@add two new MD only pmap apis to amd64 and i386 (not to be used in MI
code):

pmap_flush_cache(vaddr_t, vsize_t) and pmap_flush_page(paddr_t) to flush
the cache for virtual addresses and physical pages respectively using
the clflush instruction. These apis will shortly be used by the agp
bus_dma functions to avoid doing a wbinvd on each dmamap_sync.

ok kettenis@@, some comments from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2009/08/11 17:15:54 oga Exp $	*/
d572 1
a572 4
		kpm->pm_obj[i].pgops = NULL;
		RB_INIT(&kpm->pm_obj[i].memt);
		kpm->pm_obj[i].uo_npages = 0;
		kpm->pm_obj[i].uo_refs = 1;
d1021 1
a1021 4
		pmap->pm_obj[i].pgops = NULL;	/* not a mappable object */
		RB_INIT(&pmap->pm_obj[i].memt);
		pmap->pm_obj[i].uo_npages = 0;
		pmap->pm_obj[i].uo_refs = 1;
@


1.51
log
@fix some stupidity in x86 bus_space_map.

right now, we do a pmap_kenter_pa(), we then get the pte (behind pmap's
back) and check for the cache inhibit bit (if needed). If it isn't what
we want (this is the normal case) then we change it ourselves, and do a
manual tlb shootdown (i386 was a bit more stupid about it than amd64,
too).

Instead, make it so that like on some other archs (sparc64 comes to
mind) you can pass in flags in the low bits of the physical address,
pmap then does everything correctly for you.

Discovered this when I had some code doing a lot of bus_space_maps(), it
was incredibly slow, and profilling was dominated by
pmap_tlb_shootwait();

discussed with kettenis@@, miod@@, toby@@ and art@@.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2009/08/06 15:28:14 oga Exp $	*/
d1262 20
@


1.50
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2009/06/16 16:42:40 ariane Exp $	*/
d450 2
a451 1
	npte = pa | ((prot & VM_PROT_WRITE) ? PG_RW : PG_RO) | PG_V;
d466 2
d1969 1
d1972 2
d2136 1
a2136 1
	if (flags & PMAP_NOCACHE)
d2152 2
@


1.49
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2009/05/28 09:05:33 art Exp $	*/
d570 1
a570 1
		TAILQ_INIT(&kpm->pm_obj[i].memq);
d835 1
a835 1
		pmap->pm_ptphint[lidx] = TAILQ_FIRST(&obj->memq);
d838 1
a838 1
	TAILQ_INSERT_TAIL(pagelist, ptp, listq);
d1022 1
a1022 1
		TAILQ_INIT(&pmap->pm_obj[i].memq);
d1094 1
a1094 1
		while ((pg = TAILQ_FIRST(&pmap->pm_obj[i].memq)) != NULL) {
d1540 1
a1540 1
			TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1612 1
a1612 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1685 1
a1685 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
@


1.48
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2009/06/01 17:42:33 ariane Exp $	*/
d838 1
a838 1
	TAILQ_INSERT_TAIL(pagelist, ptp, fq.queues.listq);
d1540 1
a1540 1
			TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
d1612 1
a1612 1
		TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
d1685 1
a1685 1
		TAILQ_REMOVE(&empty_ptps, ptp, fq.queues.listq);
@


1.47
log
@revert guenther@@'s un-revert of art's curpmap.

My

bios0: ASUSTeK Computer INC. P5K-E
cpu0: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.74 MHz
cpu1: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.46 MHz
cpu2: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.46 MHz
cpu3: Intel(R) Core(TM)2 Quad CPU Q6600 @@ 2.40GHz, 2405.46 MHz

can't boot with this in. It always hangs somewhere in fsck'ing if
any, or between netstart and local daemons if no fsck'ing. Also
fubars theo's real amd machine.

Much more testing needed for this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2009/06/06 23:45:35 guenther Exp $	*/
d570 1
a570 1
		RB_INIT(&kpm->pm_obj[i].memt);
d835 1
a835 1
		pmap->pm_ptphint[lidx] = RB_ROOT(&obj->memt);
d1022 1
a1022 1
		RB_INIT(&pmap->pm_obj[i].memt);
d1094 1
a1094 1
		while ((pg = RB_ROOT(&pmap->pm_obj[i].memt)) != NULL) {
@


1.46
log
@Unrevert the curpmap change with the addition of correct %gs handling
in the IPI handler so that it works when it interrupts userspace,
waiting for the droppmap IPI to complete when destroying it, and
(most importantly) don't call pmap_tlb_droppmap() from cpu_exit().
Tested by myself and ckuethe, as our machines choked on the original.

ok @@art
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2009/06/02 23:00:18 oga Exp $	*/
d316 1
a316 1
static boolean_t pmap_is_active(struct pmap *, struct cpu_info *);
d331 1
a331 1
void pmap_apte_flush(void);
d340 2
a341 1
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
d345 1
a345 1
pmap_is_active(struct pmap *pmap, struct cpu_info *ci)
d347 2
a348 1
	return (pmap == pmap_kernel() || ci->ci_curpmap == pmap);
d352 1
a352 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d356 1
a356 1
pmap_is_curpmap(struct pmap *pmap)
d358 2
a359 1
	return (pmap_is_active(pmap, curcpu()));
a361 1

d378 1
a378 1
pmap_apte_flush(void)
d409 1
a409 1
			pmap_apte_flush();
d423 1
a423 1
	pmap_apte_flush();
d857 1
a857 1
		pmap_tlb_shootpage(curcpu()->ci_curpmap,
a1083 3
	/* Make sure it's not used by some other cpu. */
	pmap_tlb_droppmap(pmap);

d1102 5
d1108 1
d1124 3
d1128 1
d1132 2
a1133 2
	KASSERT(p == curproc);
	KASSERT(&p->p_addr->u_pcb == curpcb);
d1135 12
a1146 1
	pmap_switch(NULL, p);
d1156 1
a1156 19
}

u_int64_t nlazy_cr3;
u_int64_t nlazy_cr3_hit;

void
pmap_switch(struct proc *o, struct proc *n)
{
	struct pmap *npmap, *opmap;
	struct pcb *npcb;

	npmap = n->p_vmspace->vm_map.pmap;

	npcb = &n->p_addr->u_pcb;
	npcb->pcb_pmap = npmap;
	npcb->pcb_ldt_sel = npmap->pm_ldt_sel;
	npcb->pcb_cr3 = npmap->pm_pdirpa;

	opmap = curcpu()->ci_curpmap;
d1159 1
a1159 2
	 * Don't reload cr3 if we're switching to the same pmap or
	 * when we're not exiting and switching to kernel pmap.
d1161 1
a1161 9
	if (opmap == npmap) {
		if (npmap != pmap_kernel())
			nlazy_cr3_hit++;
	} else if (o != NULL && npmap == pmap_kernel()) {
		nlazy_cr3++;
	} else {
		curcpu()->ci_curpmap = npmap;
		lcr3(npmap->pm_pdirpa);
	}
a1162 1
	lldt(npcb->pcb_ldt_sel);
d2460 1
a2460 1
		if (ci == self || !pmap_is_active(pm, ci) ||
d2498 1
a2498 1
		if (ci == self || !pmap_is_active(pm, ci) ||
a2560 39
}

void
pmap_tlb_droppmap(struct pmap *pm)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	long wait = 0;
	int mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !(ci->ci_flags & CPUF_RUNNING) ||
		    ci->ci_curpmap != pm)
			continue;
		mask |= 1 << ci->ci_cpuid;
		wait++;
	}

	if (wait) {
		int s = splvm();

		while (x86_atomic_cas_ul(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				SPINLOCK_SPIN_HOOK;
		}

		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & 1 << ci->ci_cpuid) == 0)
				continue;
			if (x86_fast_ipi(ci, LAPIC_IPI_RELOADCR3) != 0)
				panic("pmap_tlb_shoottlb: ipi failed");
		}
		splx(s);
	}

	if (self->ci_curpmap == pm)
		pmap_activate(curproc);
	if (wait)
		pmap_tlb_shootwait();
@


1.45
log
@Revert the curpmap change.  We know the IPI is broken on both ends,
but even with proposed fixes, the reaper panics are back.
@
text
@d316 1
a316 1
static boolean_t pmap_is_active(struct pmap *, int);
d331 1
a331 1
void pmap_apte_flush(struct pmap *pmap);
d340 1
a340 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d344 1
a344 1
pmap_is_curpmap(struct pmap *pmap)
d346 1
a346 2
	return((pmap == pmap_kernel()) ||
	       (pmap->pm_pdirpa == (paddr_t) rcr3()));
d350 2
a351 1
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
d355 1
a355 1
pmap_is_active(struct pmap *pmap, int cpu_id)
d357 1
a357 2
	return (pmap == pmap_kernel() ||
	    (pmap->pm_cpus & (1U << cpu_id)) != 0);
d360 1
d377 1
a377 1
pmap_apte_flush(struct pmap *pmap)
d408 1
a408 1
			pmap_apte_flush(curpcb->pcb_pmap);
d422 1
a422 1
	pmap_apte_flush(curpcb->pcb_pmap);
d856 1
a856 1
		pmap_tlb_shootpage(curpcb->pcb_pmap,
d1083 3
a1103 5
	/*
	 * MULTIPROCESSOR -- no need to flush out of other processors'
	 * APTE space because we do that in pmap_unmap_ptes().
	 */
	/* XXX: need to flush it out of other processor's APTE space? */
a1104 1

a1119 3
 *
 * => called from cpu_switch()
 * => if p is the curproc, then load it into the MMU
a1120 1

d1124 2
a1125 2
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
d1127 1
a1127 12
	pcb->pcb_pmap = pmap;
	pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
	pcb->pcb_cr3 = pmap->pm_pdirpa;
	if (p == curproc)
		lcr3(pcb->pcb_cr3);
	if (pcb == curpcb)
		lldt(pcb->pcb_ldt_sel);

	/*
	 * mark the pmap in use by this processor.
	 */
	x86_atomic_setbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1137 19
a1155 1
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
d1158 2
a1159 1
	 * mark the pmap no longer in use by this processor. 
d1161 9
a1169 1
	x86_atomic_clearbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1171 1
d2469 1
a2469 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2507 1
a2507 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2570 39
@


1.44
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2009/06/01 17:42:33 ariane Exp $	*/
d316 1
a316 1
static boolean_t pmap_is_active(struct pmap *, struct cpu_info *);
d331 1
a331 1
void pmap_apte_flush(void);
d340 2
a341 1
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
d345 1
a345 1
pmap_is_active(struct pmap *pmap, struct cpu_info *ci)
d347 2
a348 1
	return (pmap == pmap_kernel() || ci->ci_curpmap == pmap);
d352 1
a352 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d356 1
a356 1
pmap_is_curpmap(struct pmap *pmap)
d358 2
a359 1
	return (pmap_is_active(pmap, curcpu()));
a361 1

d378 1
a378 1
pmap_apte_flush(void)
d409 1
a409 1
			pmap_apte_flush();
d423 1
a423 1
	pmap_apte_flush();
d857 1
a857 1
		pmap_tlb_shootpage(curcpu()->ci_curpmap,
a1083 2
	/* Make sure it's not used by some other cpu. */
	pmap_tlb_droppmap(pmap);
d1102 5
d1108 1
d1124 3
d1128 1
d1132 10
a1141 2
	KASSERT(p == curproc);
	KASSERT(&p->p_addr->u_pcb == curpcb);
d1143 4
a1146 1
	pmap_switch(NULL, p);
d1156 1
a1156 19
}

u_int64_t nlazy_cr3;
u_int64_t nlazy_cr3_hit;

void
pmap_switch(struct proc *o, struct proc *n)
{
	struct pmap *npmap, *opmap;
	struct pcb *npcb;

	npmap = n->p_vmspace->vm_map.pmap;

	npcb = &n->p_addr->u_pcb;
	npcb->pcb_pmap = npmap;
	npcb->pcb_ldt_sel = npmap->pm_ldt_sel;
	npcb->pcb_cr3 = npmap->pm_pdirpa;

	opmap = curcpu()->ci_curpmap;
d1159 1
a1159 2
	 * Don't reload cr3 if we're switching to the same pmap or
	 * when we're not exiting and switching to kernel pmap.
d1161 1
a1161 9
	if (opmap == npmap) {
		if (npmap != pmap_kernel())
			nlazy_cr3_hit++;
	} else if (o != NULL && npmap == pmap_kernel()) {
		nlazy_cr3++;
	} else {
		curcpu()->ci_curpmap = npmap;
		lcr3(npmap->pm_pdirpa);
	}
a1162 1
	lldt(npcb->pcb_ldt_sel);
d2460 1
a2460 1
		if (ci == self || !pmap_is_active(pm, ci) ||
d2498 1
a2498 1
		if (ci == self || !pmap_is_active(pm, ci) ||
a2560 36
}

void
pmap_tlb_droppmap(struct pmap *pm)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	long wait = 0;
	int mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !(ci->ci_flags & CPUF_RUNNING) ||
		    ci->ci_curpmap != pm)
			continue;
		mask |= 1 << ci->ci_cpuid;
		wait++;
	}

	if (wait) {
		int s = splvm();

		while (x86_atomic_cas_ul(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				SPINLOCK_SPIN_HOOK;
		}

		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & 1 << ci->ci_cpuid) == 0)
				continue;
			if (x86_fast_ipi(ci, LAPIC_IPI_RELOADCR3) != 0)
				panic("pmap_tlb_shoottlb: ipi failed");
		}
		splx(s);
	}

	pmap_activate(curproc);
@


1.43
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2009/05/28 09:05:33 art Exp $	*/
d569 1
a569 1
		TAILQ_INIT(&kpm->pm_obj[i].memq);
d834 1
a834 1
		pmap->pm_ptphint[lidx] = TAILQ_FIRST(&obj->memq);
d1021 1
a1021 1
		TAILQ_INIT(&pmap->pm_obj[i].memq);
d1095 1
a1095 1
		while ((pg = TAILQ_FIRST(&pmap->pm_obj[i].memq)) != NULL) {
@


1.42
log
@Bring back the curpmap change. It was missing a reload of the pmap on
curcpu when we were freeing a pmap. Tested and working for a few weeks
now, but I was a bit too busy to commit it earlier.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2009/04/23 07:42:02 art Exp $	*/
d837 1
a837 1
	TAILQ_INSERT_TAIL(pagelist, ptp, listq);
d1548 1
a1548 1
			TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1620 1
a1620 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
d1693 1
a1693 1
		TAILQ_REMOVE(&empty_ptps, ptp, listq);
@


1.41
log
@turning pmap_deactivate into a NOP brought back the reaper panics, probably
because the reaper is running on the mappings of pmap from the process it
is about to unmap.  back it out until ht is fixed right; don't let this sit
in the tree waiting for a fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2009/03/30 16:09:36 oga Exp $	*/
d316 1
a316 1
static boolean_t pmap_is_active(struct pmap *, int);
d331 1
a331 1
void pmap_apte_flush(struct pmap *pmap);
d340 1
a340 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d344 1
a344 1
pmap_is_curpmap(struct pmap *pmap)
d346 1
a346 2
	return((pmap == pmap_kernel()) ||
	       (pmap->pm_pdirpa == (paddr_t) rcr3()));
d350 2
a351 1
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
d355 1
a355 1
pmap_is_active(struct pmap *pmap, int cpu_id)
d357 1
a357 2
	return (pmap == pmap_kernel() ||
	    (pmap->pm_cpus & (1U << cpu_id)) != 0);
d360 1
d377 1
a377 1
pmap_apte_flush(struct pmap *pmap)
d408 1
a408 1
			pmap_apte_flush(curpcb->pcb_pmap);
d422 1
a422 1
	pmap_apte_flush(curpcb->pcb_pmap);
d856 1
a856 1
		pmap_tlb_shootpage(curpcb->pcb_pmap,
d1083 2
a1102 5
	/*
	 * MULTIPROCESSOR -- no need to flush out of other processors'
	 * APTE space because we do that in pmap_unmap_ptes().
	 */
	/* XXX: need to flush it out of other processor's APTE space? */
a1103 1

a1118 3
 *
 * => called from cpu_switch()
 * => if p is the curproc, then load it into the MMU
a1119 1

d1123 2
a1124 10
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	pcb->pcb_pmap = pmap;
	pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
	pcb->pcb_cr3 = pmap->pm_pdirpa;
	if (p == curproc)
		lcr3(pcb->pcb_cr3);
	if (pcb == curpcb)
		lldt(pcb->pcb_ldt_sel);
d1126 1
a1126 4
	/*
	 * mark the pmap in use by this processor.
	 */
	x86_atomic_setbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1136 19
a1154 1
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
d1157 2
a1158 1
	 * mark the pmap no longer in use by this processor. 
d1160 9
a1168 1
	x86_atomic_clearbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1170 1
d2468 1
a2468 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2506 1
a2506 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2569 36
@


1.40
log
@Make pmap_deactivate a NOP.

Instead of keeping a bitmask of on which cpu the pmap might be active which
we clear in pmap_deactivate, always keep a pointer to the currently loaded
pmap in cpu_info. We can now optimize a context switch to the kernel pmap
(idle and kernel threads) to keep the previously loaded pmap still loaded
and then reuse that pmap if we context switch back to the same process.

Introduce a new IPI to force a pmap reload before the pmap is destroyed.

Clean up cpu_switchto.

toby@@ ok
@
text
@d316 1
a316 1
static boolean_t pmap_is_active(struct pmap *, struct cpu_info *);
d331 1
a331 1
void pmap_apte_flush(void);
d340 2
a341 1
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
d345 1
a345 1
pmap_is_active(struct pmap *pmap, struct cpu_info *ci)
d347 2
a348 1
	return (pmap == pmap_kernel() || ci->ci_curpmap == pmap);
d352 1
a352 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d356 1
a356 1
pmap_is_curpmap(struct pmap *pmap)
d358 2
a359 1
	return (pmap_is_active(pmap, curcpu()));
a361 1

d378 1
a378 1
pmap_apte_flush(void)
d409 1
a409 1
			pmap_apte_flush();
d423 1
a423 1
	pmap_apte_flush();
d857 1
a857 1
		pmap_tlb_shootpage(curcpu()->ci_curpmap,
a1083 2
	/* Make sure it's not used by some other cpu. */
	pmap_tlb_droppmap(pmap);
d1102 5
d1108 1
d1124 3
d1128 1
d1132 10
a1141 2
	KASSERT(p == curproc);
	KASSERT(&p->p_addr->u_pcb == curpcb);
d1143 4
a1146 1
	pmap_switch(NULL, p);
d1156 1
a1156 19
}

u_int64_t nlazy_cr3;
u_int64_t nlazy_cr3_hit;

void
pmap_switch(struct proc *o, struct proc *n)
{
	struct pmap *npmap, *opmap;
	struct pcb *npcb;

	npmap = n->p_vmspace->vm_map.pmap;

	npcb = &n->p_addr->u_pcb;
	npcb->pcb_pmap = npmap;
	npcb->pcb_ldt_sel = npmap->pm_ldt_sel;
	npcb->pcb_cr3 = npmap->pm_pdirpa;

	opmap = curcpu()->ci_curpmap;
d1159 1
a1159 2
	 * Don't reload cr3 if we're switching to the same pmap or
	 * when we're not exiting and switching to kernel pmap.
d1161 1
a1161 9
	if (opmap == npmap) {
		if (npmap != pmap_kernel())
			nlazy_cr3_hit++;
	} else if (o != NULL && npmap == pmap_kernel()) {
		nlazy_cr3++;
	} else {
		curcpu()->ci_curpmap = npmap;
		lcr3(npmap->pm_pdirpa);
	}
a1162 1
	lldt(npcb->pcb_ldt_sel);
d2460 1
a2460 1
		if (ci == self || !pmap_is_active(pm, ci) ||
d2498 1
a2498 1
		if (ci == self || !pmap_is_active(pm, ci) ||
a2560 34
}

void
pmap_tlb_droppmap(struct pmap *pm)
{
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	long wait = 0;
	int mask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self || !(ci->ci_flags & CPUF_RUNNING) ||
		    ci->ci_curpmap != pm)
			continue;
		mask |= 1 << ci->ci_cpuid;
		wait++;
	}

	if (wait) {
		int s = splvm();

		while (x86_atomic_cas_ul(&tlb_shoot_wait, 0, wait) != 0) {
			while (tlb_shoot_wait != 0)
				SPINLOCK_SPIN_HOOK;
		}

		CPU_INFO_FOREACH(cii, ci) {
			if ((mask & 1 << ci->ci_cpuid) == 0)
				continue;
			if (x86_fast_ipi(ci, LAPIC_IPI_RELOADCR3) != 0)
				panic("pmap_tlb_shoottlb: ipi failed");
		}
		splx(s);
	}
@


1.39
log
@Remove the direct uncached map. All its users have been removed.

In the future, we need to mark the correct parts of the direct map
uncached, but that's another diff.

art@@, kettenis@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2009/03/23 13:25:11 art Exp $	*/
d316 1
a316 1
static boolean_t pmap_is_active(struct pmap *, int);
d331 1
a331 1
void pmap_apte_flush(struct pmap *pmap);
d340 1
a340 2
 * pmap_is_curpmap: is this pmap the one currently loaded [in %cr3]?
 *		of course the kernel is always loaded
d344 1
a344 1
pmap_is_curpmap(struct pmap *pmap)
d346 1
a346 2
	return((pmap == pmap_kernel()) ||
	       (pmap->pm_pdirpa == (paddr_t) rcr3()));
d350 2
a351 1
 * pmap_is_active: is this pmap loaded into the specified processor's %cr3?
d355 1
a355 1
pmap_is_active(struct pmap *pmap, int cpu_id)
d357 1
a357 2
	return (pmap == pmap_kernel() ||
	    (pmap->pm_cpus & (1U << cpu_id)) != 0);
d360 1
d377 1
a377 1
pmap_apte_flush(struct pmap *pmap)
d408 1
a408 1
			pmap_apte_flush(curpcb->pcb_pmap);
d422 1
a422 1
	pmap_apte_flush(curpcb->pcb_pmap);
d856 1
a856 1
		pmap_tlb_shootpage(curpcb->pcb_pmap,
d1083 2
a1102 5
	/*
	 * MULTIPROCESSOR -- no need to flush out of other processors'
	 * APTE space because we do that in pmap_unmap_ptes().
	 */
	/* XXX: need to flush it out of other processor's APTE space? */
a1103 1

a1118 3
 *
 * => called from cpu_switch()
 * => if p is the curproc, then load it into the MMU
a1119 1

d1123 2
a1124 10
	struct pcb *pcb = &p->p_addr->u_pcb;
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

	pcb->pcb_pmap = pmap;
	pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
	pcb->pcb_cr3 = pmap->pm_pdirpa;
	if (p == curproc)
		lcr3(pcb->pcb_cr3);
	if (pcb == curpcb)
		lldt(pcb->pcb_ldt_sel);
d1126 1
a1126 4
	/*
	 * mark the pmap in use by this processor.
	 */
	x86_atomic_setbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1136 19
a1154 1
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
d1157 2
a1158 1
	 * mark the pmap no longer in use by this processor. 
d1160 9
a1168 1
	x86_atomic_clearbits_ul(&pmap->pm_cpus, (1U << cpu_number()));
d1170 1
d2468 1
a2468 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2506 1
a2506 1
		if (ci == self || !pmap_is_active(pm, ci->ci_cpuid) ||
d2569 34
@


1.38
log
@Processor affinity for processes.
 - Split up run queues so that every cpu has one.
 - Make setrunqueue choose the cpu where we want to make this process
   runnable (this should be refined and less brutal in the future).
 - When choosing the cpu where we want to run, make some kind of educated
   guess where it will be best to run (very naive right now).
Other:
 - Set operations for sets of cpus.
 - load average calculations per cpu.
 - sched_is_idle() -> curcpu_is_idle()

tested, debugged and prodded by many@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2009/02/16 20:26:58 kurt Exp $	*/
a645 36
	/*
	 * Now do the same thing, but for the direct uncached map.
	 */
	ndmpdp = (max_pa + NBPD_L3 - 1) >> L3_SHIFT;
	if (ndmpdp < NDML2_ENTRIES)
		ndmpdp = NDML2_ENTRIES;		/* At least 4GB */

	dmpdp = first_avail;	first_avail += PAGE_SIZE;
	dmpd = first_avail;	first_avail += ndmpdp * PAGE_SIZE;

	for (i = 0; i < NPDPG * ndmpdp; i++) {
		paddr_t pdp;
		vaddr_t va;

		pdp = (paddr_t)&(((pd_entry_t *)dmpd)[i]);
		va = PMAP_DIRECT_MAP(pdp);

		*((pd_entry_t *)va) = (paddr_t)i << L2_SHIFT;
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_PS | PG_G | PG_N |
		    PG_U | PG_M;
	}

	for (i = 0; i < ndmpdp; i++) {
		paddr_t pdp;
		vaddr_t va;

		pdp = (paddr_t)&(((pd_entry_t *)dmpdp)[i]);
		va = PMAP_DIRECT_MAP(pdp);

		*((pd_entry_t *)va) = dmpd + (i << PAGE_SHIFT);
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_U | PG_M;
	}

	kpm->pm_pdir[PDIR_SLOT_DIRECT_NC] = dmpdp | PG_V | PG_KW | PG_U |
	    PG_M;
	
a994 1
	pdir[PDIR_SLOT_DIRECT_NC] = pmap_kernel()->pm_pdir[PDIR_SLOT_DIRECT_NC];
a1203 6
		return (TRUE);
	}

	if (pmap == pmap_kernel() && va >= PMAP_DIRECT_BASE_NC &&
	    va < PMAP_DIRECT_END_NC) {
		*pap = va - PMAP_DIRECT_BASE_NC;
@


1.37
log
@remove incorrect cast of pcb_cr3. okay kettenis@@ weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2009/02/05 01:15:20 oga Exp $	*/
d1325 1
a1325 1
		if (!sched_is_idle()) {
@


1.36
log
@Mirroring the i386 commit just made. Add MD PMAP_NOCACHE flag to pmap,
and use it to implement BUS_DMA_NOCACHE for uncached mappings of dma
memory. Needed for some broken hardware.

Discussion with art, miod, kettenis and toby, ok miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2008/12/18 14:18:29 kurt Exp $	*/
d577 1
a577 1
	kpm->pm_pdirpa = (u_int32_t) proc0.p_addr->u_pcb.pcb_cr3;
@


1.35
log
@Don't set the global bit PG_G for kernel pmap low memory mappings.

ok  deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2008/12/18 13:43:24 kurt Exp $	*/
d2173 2
@


1.34
log
@use atomic operations to update ptes in pmap_unwire(). okay weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2008/12/04 15:48:19 weingart Exp $	*/
d450 6
a455 2
	npte = pa | ((prot & VM_PROT_WRITE) ? PG_RW : PG_RO) |
	     PG_V | pmap_pg_g;
@


1.33
log
@Fix "fp_save ipi didn't" panic, and move i386/amd64 closer in the process.
Positive test results by a handful of people.  Ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2008/06/10 02:55:39 weingart Exp $	*/
d1941 1
a1941 1
			ptes[pl1_i(va)] &= ~PG_W;
@


1.32
log
@All your memory belong to us.  This has been in snaps for a while,
and seems to work.  If it breaks, people had plenty of chances to
complain.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2008/05/23 15:39:43 jasper Exp $	*/
a2486 3

/* XXX */
#define SPINLOCK_SPIN_HOOK __asm __volatile("pause": : :"memory")
@


1.31
log
@- remove USER_LDT, it was never in a state where it would copile, nor will
we support i386-compat mode on amd64.

agreed by beck@@, dlg@@, kettenis@@
ok deraadt@@, tom@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2007/12/09 00:24:04 tedu Exp $	*/
d518 2
a519 2
void
pmap_bootstrap(vaddr_t kva_start, paddr_t max_pa)
d521 1
a521 1
	vaddr_t kva, kva_end;
d614 1
a614 1
	dmpd = avail_start; avail_start += ndmpdp * PAGE_SIZE;
d649 2
a650 2
	dmpdp = avail_start;	avail_start += PAGE_SIZE;
	dmpd = avail_start;	avail_start += ndmpdp * PAGE_SIZE;
d685 2
a686 2
	idt_paddr = avail_start;			/* steal a page */
	avail_start += 2 * PAGE_SIZE;
d695 2
a696 2
	lo32_paddr = avail_start;
	avail_start += PAGE_SIZE;
d734 2
d742 2
a743 2
void
pmap_prealloc_lowmem_ptps(void)
d752 1
a752 2
		newp = avail_start;
		avail_start += PAGE_SIZE;
d760 2
@


1.30
log
@big patch to simplify pool code.

remove pool_cache code.  it was barely used, and quite complex.  it's
silly to have both a "fast" and "faster" allocation interface.  provide
a ctor/dtor interface, and convert the few cache users to use it.  no
caching at this time.

use mutexes to protect pools.  they should be initialized with pool_setipl
if the pool may be used in an interrupt context, without existing spl
protection.

ok art deraadt thib
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2007/11/03 22:23:35 mikeb Exp $	*/
a1138 15
#ifdef USER_LDT
	if (pmap->pm_flags & PMF_USER_LDT) {
		/*
		 * no need to switch the LDT; this address space is gone,
		 * nothing is using it.
		 *
		 * No need to lock the pmap for ldt_free (or anything else),
		 * we're the last one to use it.
		 */
		ldt_free(pmap);
		uvm_km_free(kernel_map, (vaddr_t)pmap->pm_ldt,
			    pmap->pm_ldt_len);
	}
#endif

a1150 59

#if defined(PMAP_FORK)
/*
 * pmap_fork: perform any necessary data structure manipulation when
 * a VM space is forked.
 */

void
pmap_fork(struct pmap *pmap1, struct pmap *pmap2)
{
#ifdef USER_LDT
	/* Copy the LDT, if necessary. */
	if (pmap1->pm_flags & PMF_USER_LDT) {
		char *new_ldt;
		size_t len;

		len = pmap1->pm_ldt_len;
		new_ldt = (char *)uvm_km_alloc(kernel_map, len);
		memcpy(new_ldt, pmap1->pm_ldt, len);
		pmap2->pm_ldt = new_ldt;
		pmap2->pm_ldt_len = pmap1->pm_ldt_len;
		pmap2->pm_flags |= PMF_USER_LDT;
		ldt_alloc(pmap2, new_ldt, len);
	}
#endif /* USER_LDT */
}
#endif /* PMAP_FORK */

#ifdef USER_LDT
/*
 * pmap_ldt_cleanup: if the pmap has a local LDT, deallocate it, and
 * restore the default.
 */

void
pmap_ldt_cleanup(struct proc *p)
{
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->->p_vmspace->vm_map.pmap;
	char *old_ldt = NULL;
	size_t len = 0;

	if (pmap->pm_flags & PMF_USER_LDT) {
		ldt_free(pmap);
		pmap->pm_ldt_sel = GSYSSEL(GLDT_SEL, SEL_KPL);
		pcb->pcb_ldt_sel = pmap->pm_ldt_sel;
		if (pcb == curpcb)
			lldt(pcb->pcb_ldt_sel);
		old_ldt = pmap->pm_ldt;
		len = pmap->pm_ldt_len;
		pmap->pm_ldt = NULL;
		pmap->pm_ldt_len = 0;
		pmap->pm_flags &= ~PMF_USER_LDT;
	}

	if (old_ldt != NULL)
		uvm_km_free(kernel_map, (vaddr_t)old_ldt, len);
}
#endif /* USER_LDT */
@


1.29
log
@Fix LKM support for amd64.

ok deraadt weingart
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2007/09/04 23:20:23 thib Exp $	*/
d283 1
a283 1
 * pool and cache that PDPs are allocated from
a286 1
struct pool_cache pmap_pdp_cache;
d721 1
a721 1
	 * initialize the PDE pool and cache.
d726 2
a727 2
	pool_cache_init(&pmap_pdp_cache, &pmap_pdp_pool,
			pmap_pdp_ctor, NULL, NULL);
d1076 1
a1076 1
	pmap->pm_pdir = pool_cache_get(&pmap_pdp_cache, PR_WAITOK);
d1079 1
a1079 1
		pool_cache_destruct_object(&pmap_pdp_cache, pmap->pm_pdir);
d1137 1
a1137 1
	pool_cache_put(&pmap_pdp_cache, pmap->pm_pdir);
d2395 1
d2397 1
@


1.28
log
@Zap the simplelock goo and general cleanup of comments.
also, theres no need to check first if we're the kernel pmap
and then if we're the curmap in two different if statements,
pmap_is_curpmap() check's if we're the kernel pmap, so nuke
those tests.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2007/06/01 20:10:04 tedu Exp $	*/
a2079 4
	/* sanity check: totally out of range? */
	if (va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_enter: too big");

@


1.27
log
@set hiwat mark for some of the more popular pools to reduce bouncing
ok art bob
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2007/05/29 02:37:04 art Exp $	*/
a208 16
 * locking
 *
 * we have the following locks that we must contend with:
 *
 * "simple" locks:
 *
 * - pmap lock (per pmap, part of uvm_object)
 *   this lock protects the fields in the pmap structure including
 *   the non-kernel PDEs in the PDP, and the PTEs.  it also locks
 *   in the alternate PTE space (since that is determined by the
 *   entry in the PDP).
 *
 * - pmaps_lock
 *   this lock protects the list of active pmaps (headed by "pmaps").
 *   we lock it when adding or removing pmaps from this list.
 *
a211 4
/*
 * locking data structures
 */

a221 2
struct simplelock pmaps_lock;

d386 1
a386 1
 * pmap_map_ptes: map a pmap's PTEs into KVM and lock them in
a396 7
	/* the kernel's pmap is always accessible */
	if (pmap == pmap_kernel()) {
		*ptepp = PTE_BASE;
		*pdeppp = normal_pdes;
		return;
	}

a398 1
		simple_lock(&pmap->pm_lock);
a403 9
	/* need to lock both curpmap and pmap: use ordered locking */
	if ((unsigned long) pmap < (unsigned long) curpcb->pcb_pmap) {
		simple_lock(&pmap->pm_lock);
		simple_lock(&curpcb->pcb_pmap->pm_lock);
	} else {
		simple_lock(&curpcb->pcb_pmap->pm_lock);
		simple_lock(&pmap->pm_lock);
	}

a415 4
/*
 * pmap_unmap_ptes: unlock the PTE mapping of "pmap"
 */

d419 1
a419 1
	if (pmap == pmap_kernel()) {
d421 1
a421 4
	}
	if (pmap_is_curpmap(pmap)) {
		simple_unlock(&pmap->pm_lock);
	} else {
d423 2
a424 2
		*APDP_PDE = 0;
		pmap_apte_flush(curpcb->pcb_pmap);
d426 1
a426 4
		COUNT(apdp_pde_unmap);
		simple_unlock(&pmap->pm_lock);
		simple_unlock(&curpcb->pcb_pmap->pm_lock);
	}
a565 1
		simple_lock_init(&kpm->pm_obj[i].vmobjlock);
d707 1
a707 1
	 * init the static-global locks and global lists.
a708 2

	simple_lock_init(&pmaps_lock);
a790 1
 * => caller should have pmap locked
a810 1
 * => pmap should be locked
a847 1
		simple_lock(&pmap->pm_obj[lidx].vmobjlock);
a848 1
		simple_unlock(&pmap->pm_obj[lidx].vmobjlock);
a906 1
 * => pmap should be locked
a943 5
		/*
		 * XXX pm_obj[0] is pm_lock, which is already locked.
		 */
		if (i != 2)
			simple_lock(&obj->vmobjlock);
a945 2
		if (i != 2)
			simple_unlock(&obj->vmobjlock);
a1004 5
	/*
	 * NOTE: The `pmap_lock' is held when the PDP is allocated.
	 * WE MUST NOT BLOCK!
	 */

a1051 1
		simple_lock_init(&pmap->pm_obj[i].vmobjlock);
d1070 1
a1070 2
	 * we need to lock pmaps_lock to prevent nkpde from changing on
	 * us.  note that there is no need to splvm to protect us from
a1072 3
	 *
	 * NOTE: WE MUST NOT BLOCK WHILE HOLDING THE `pmap_lock', nor
	 * ust we call pmap_growkernel() while holding it!
a1078 2
	simple_lock(&pmaps_lock);

a1079 1
		simple_unlock(&pmaps_lock);
a1086 3

	simple_unlock(&pmaps_lock);

a1105 1
	simple_lock(&pmap->pm_lock);
a1106 1
	simple_unlock(&pmap->pm_lock);
a1117 2

	simple_lock(&pmaps_lock);
a1118 1
	simple_unlock(&pmaps_lock);
a1164 1
	simple_lock(&pmap->pm_lock);
a1165 1
	simple_unlock(&pmap->pm_lock);
a1176 3
	simple_lock(&pmap1->pm_lock);
	simple_lock(&pmap2->pm_lock);

a1191 3

	simple_unlock(&pmap2->pm_lock);
	simple_unlock(&pmap1->pm_lock);
a1208 2
	simple_lock(&pmap->pm_lock);

a1221 2
	simple_unlock(&pmap->pm_lock);

a1433 1
 * => caller must hold pmap's lock
a1512 1
 * => caller must hold pmap's lock
d1605 1
a1605 1
	pmap_map_ptes(pmap, &ptes, &pdes);	/* locks pmap */
d1647 1
a1647 1
		pmap_unmap_ptes(pmap);		/* unlock pmap */
d1753 1
a1753 1
		pmap_map_ptes(pve->pv_pmap, &ptes, &pdes);	/* locks pmap */
d1788 1
a1788 1
		pmap_unmap_ptes(pve->pv_pmap);		/* unlocks pmap */
d1868 1
a1868 1
		pmap_map_ptes(pve->pv_pmap, &ptes, &pdes);	/* locks pmap */
d1882 1
a1882 1
		pmap_unmap_ptes(pve->pv_pmap);		/* unlocks pmap */
d1926 1
a1926 1
	pmap_map_ptes(pmap, &ptes, &pdes);		/* locks pmap */
d1985 1
a1985 1
	pmap_unmap_ptes(pmap);		/* unlocks pmap */
d2004 1
a2004 1
	pmap_map_ptes(pmap, &ptes, &pdes);		/* locks pmap */
d2022 1
a2022 1
		pmap_unmap_ptes(pmap);		/* unlocks map */
d2101 1
a2101 1
	pmap_map_ptes(pmap, &ptes, &pdes);		/* locks pmap */
a2383 1
	simple_lock(&kpm->pm_lock);
a2392 1
		simple_lock(&pmaps_lock);
a2401 2

		simple_unlock(&pmaps_lock);
a2403 1
	simple_unlock(&kpm->pm_lock);
d2497 1
a2497 1
	pmap_map_ptes(pmap, &ptes, &pdes);	/* locks pmap */
@


1.26
log
@Remove a debugging printf left behind by accident.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2007/05/29 02:36:19 art Exp $	*/
d771 1
@


1.25
log
@PMAP_STEAL_MEMORY for amd64
drahn@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2007/05/27 08:58:31 art Exp $	*/
a661 2

printf("ndmpdp: %ld\n", ndmpdp);
@


1.24
log
@- Redo the way we set up the direct map. Map the first 4GB of it
  in locore so that we can use the direct map in pmap_bootstrap when
  setting up the initial page tables.

- Introduce a second direct map (I love large address spaces) with
  uncached pages.

jason@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2007/05/25 16:22:11 art Exp $	*/
d2379 2
d2387 2
a2388 3
		if (uvm_page_physget(paddrp) == FALSE)
			panic("pmap_get_physpage: out of memory");
		memset((void *)PMAP_DIRECT_MAP(*paddrp), 0, PAGE_SIZE);
d2512 62
@


1.23
log
@Change the old slow and complicated TLB shootdown code to new, fast and
simple. This is basically the same code as on i386 and basically the same
performance improvements.

This change also includes code to delay the freeing of ptps until they
have been properly shot.

in snaps for a week, no problems reported.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2007/05/18 14:41:55 art Exp $	*/
a239 5
/*
 * Direct map.
 */
paddr_t DMPDpa;

a572 2
	pt_entry_t *tmppte;
	vaddr_t tmpva;
d654 4
a657 1
	 * Temporary mapping for setting up the direct map.
d659 35
a693 3
	tmpva = (KERNBASE + NKL2_KIMG_ENTRIES * NBPD_L2);
	virtual_avail += PAGE_SIZE;
	tmppte = PTE_BASE + pl1_i(tmpva);
d696 1
a696 3
	 * Map the direct map. We steal pages for the page tables from
	 * avail_start, then we create temporary mappings using the
	 * early_zerop. Scary, slow, but we only do it once.
d699 2
a700 2
	if (ndmpdp < 4)
		ndmpdp = 4;	/* At least 4GB */
a706 1
		paddr_t off;
d710 1
a710 3
		off = pdp - trunc_page(pdp);
		*tmppte = (trunc_page(pdp) & PG_FRAME) | PG_V | PG_RW;
		pmap_update_pg(tmpva);
a711 1
		va = tmpva + off;
d713 2
a714 1
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_PS | PG_G;
a718 1
		paddr_t off;
d722 1
a722 3
		off = pdp - trunc_page(pdp);
		*tmppte = (trunc_page(pdp) & PG_FRAME) | PG_V | PG_RW;
		pmap_update_pg(tmpva);
a723 1
		va = tmpva + off;
d725 1
a725 1
		*((pd_entry_t *)va) |= PG_RW | PG_V | PG_U;
d727 3
a729 1
	*tmppte = 0;
a730 3
	DMPDpa = dmpdp;
	kpm->pm_pdir[PDIR_SLOT_DIRECT] = DMPDpa | PG_V | PG_KW | PG_U;

d1094 2
a1095 1
	pdir[PDIR_SLOT_DIRECT] = DMPDpa | PG_V | PG_KW | PG_U;
d1407 6
@


1.22
log
@Instead of checking whichqs directly, add a "sched_is_idle()" macro to
sys/sched.h and use that to check if there's something to do.

kettenis@@ thib@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2007/05/15 16:38:33 art Exp $	*/
a257 49
 * TLB Shootdown:
 *
 * When a mapping is changed in a pmap, the TLB entry corresponding to
 * the virtual address must be invalidated on all processors.  In order
 * to accomplish this on systems with multiple processors, messages are
 * sent from the processor which performs the mapping change to all
 * processors on which the pmap is active.  For other processors, the
 * ASN generation numbers for that processor is invalidated, so that
 * the next time the pmap is activated on that processor, a new ASN
 * will be allocated (which implicitly invalidates all TLB entries).
 *
 * Shootdown job queue entries are allocated using a simple special-
 * purpose allocator for speed.
 */
struct pmap_tlb_shootdown_job {
	TAILQ_ENTRY(pmap_tlb_shootdown_job) pj_list;
	vaddr_t pj_va;			/* virtual address */
	pmap_t pj_pmap;			/* the pmap which maps the address */
	pt_entry_t pj_pte;		/* the PTE bits */
	struct pmap_tlb_shootdown_job *pj_nextfree;
};

#define PMAP_TLB_SHOOTDOWN_JOB_ALIGN 64
union pmap_tlb_shootdown_job_al {
	struct pmap_tlb_shootdown_job pja_job;
	char pja_align[PMAP_TLB_SHOOTDOWN_JOB_ALIGN];
};

struct pmap_tlb_shootdown_q {
	TAILQ_HEAD(, pmap_tlb_shootdown_job) pq_head;
	int pq_pte;			/* aggregate PTE bits */
	int pq_count;			/* number of pending requests */
	struct mutex pq_mutex;	/* spin lock on queue */
	int pq_flushg;		/* pending flush global */
	int pq_flushu;		/* pending flush user */
} pmap_tlb_shootdown_q[X86_MAXPROCS];

#define	PMAP_TLB_MAXJOBS	16

void	pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *);
struct pmap_tlb_shootdown_job *pmap_tlb_shootdown_job_get
	    (struct pmap_tlb_shootdown_q *);
void	pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *,
	    struct pmap_tlb_shootdown_job *);

struct mutex pmap_tlb_shootdown_job_mutex = MUTEX_INITIALIZER(IPL_NONE);
union pmap_tlb_shootdown_job_al *pj_page, *pj_free;

/*
d303 5
d342 2
a343 2
    vaddr_t, pt_entry_t *, pd_entry_t **, int32_t *);
void pmap_freepage(struct pmap *, struct vm_page *, int);
d349 1
a349 1
    vaddr_t, int32_t *, int);
d351 1
a351 1
    vaddr_t, vaddr_t, int32_t *, int);
d408 2
a409 27
#if defined(MULTIPROCESSOR)
	struct pmap_tlb_shootdown_q *pq;
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
#endif

	tlbflush();		/* flush TLB on current processor */
#if defined(MULTIPROCESSOR)
	/*
	 * Flush the APTE mapping from all other CPUs that
	 * are using the pmap we are using (who's APTE space
	 * is the one we've just modified).
	 *
	 * XXXthorpej -- find a way to defer the IPI.
	 */
	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self)
			continue;
		if (pmap_is_active(pmap, ci->ci_cpuid)) {
			pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];
			mtx_enter(&pq->pq_mutex);
			pq->pq_flushu++;
			mtx_leave(&pq->pq_mutex);
			x86_send_ipi(ci, X86_IPI_TLB);
		}
	}
#endif
d503 1
a503 4
	if (va < VM_MIN_KERNEL_ADDRESS)
		pte = vtopte(va);
	else
		pte = kvtopte(va);
d509 1
a509 1
	opte = pmap_pte_set(pte, npte); /* zap! */
d516 3
a518 9
#if defined(MULTIPROCESSOR)
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		pmap_update_pg(va);
#endif
d534 1
a534 1
pmap_kremove(vaddr_t va, vsize_t len)
d537 6
a542 1
	int32_t cpumask = 0;
d544 1
a544 7
	len >>= PAGE_SHIFT;
	for ( /* null */ ; len ; len--, va += PAGE_SIZE) {
		if (va < VM_MIN_KERNEL_ADDRESS)
			pte = vtopte(va);
		else
			pte = kvtopte(va);
		opte = pmap_pte_set(pte, 0); /* zap! */
d546 1
a546 8
		/* XXX For now... */
		if (opte & PG_PS)
			panic("pmap_kremove: PG_PS");
#endif
#ifdef DIAGNOSTIC
		if (opte & PG_PVLIST)
			panic("pmap_kremove: PG_PVLIST mapping for 0x%lx",
			      va);
d548 1
a548 1
		pmap_tlb_shootdown(pmap_kernel(), va, opte, &cpumask);
d550 3
a552 1
	pmap_tlb_shootnow(cpumask);
a756 9
	 * Initialize the TLB shootdown queues.
	 */

	for (i = 0; i < X86_MAXPROCS; i++) {
		TAILQ_INIT(&pmap_tlb_shootdown_q[i].pq_head);
		mtx_init(&pmap_tlb_shootdown_q[i].pq_mutex, IPL_IPI);
	}

	/*
a805 15
	struct vm_page *pg;
	int i;

	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (pg == NULL)
		panic("pmap_init: pj_page");
	pj_page = (void *)pmap_map_direct(pg);

	for (i = 0;
	     i < (PAGE_SIZE / sizeof (union pmap_tlb_shootdown_job_al) - 1);
	     i++)
		pj_page[i].pja_job.pj_nextfree = &pj_page[i + 1].pja_job;
	pj_page[i].pja_job.pj_nextfree = NULL;
	pj_free = &pj_page[0];

d893 2
a894 1
pmap_freepage(struct pmap *pmap, struct vm_page *ptp, int level)
a902 2
	if (lidx != 0)
		simple_lock(&obj->vmobjlock);
d906 2
a907 3
	uvm_pagefree(ptp);
	if (lidx != 0)
		simple_unlock(&obj->vmobjlock);
d912 1
a912 1
    pt_entry_t *ptes, pd_entry_t **pdes, int32_t *cpumaskp)
d921 1
a921 1
		pmap_freepage(pmap, ptp, level);
d926 2
a927 3
		pmap_tlb_shootdown(curpcb->pcb_pmap,
		    invaladdr + index * PAGE_SIZE,
		    opde, cpumaskp);
d931 1
a931 2
		pmap_tlb_shootdown(pmap, invaladdr + index * PAGE_SIZE, opde,
		    cpumaskp);
d1514 1
a1514 1
    vaddr_t startva, vaddr_t endva, int32_t *cpumaskp, int flags)
a1544 2
		pmap_tlb_shootdown(pmap, startva, opte, cpumaskp);

d1595 1
a1595 1
    vaddr_t va, int32_t *cpumaskp, int flags)
a1616 2
	pmap_tlb_shootdown(pmap, va, opte, cpumaskp);

d1673 5
a1677 1
	int32_t cpumask = 0;
d1708 1
a1708 1
			    &ptes[pl1_i(sva)], sva, &cpumask, flags);
d1717 2
a1718 1
				    &cpumask);
d1721 1
a1721 1
		pmap_tlb_shootnow(cpumask);
d1724 6
d1733 2
a1734 3
	cpumask = 0;

	for (/* null */ ; sva < eva ; sva = blkendva) {
d1736 1
d1738 1
a1738 1
		blkendva = x86_round_pdr(sva+1);
d1756 1
a1756 1
		if (pl_i(sva, PTP_LEVELS) == PDIR_SLOT_PTE)
d1760 1
a1760 1
		if (!pmap_pdes_valid(sva, pdes, &pde))
d1771 1
a1771 1
			ptp = pmap_find_ptp(pmap, sva, ptppa, 1);
d1779 1
a1779 1
		    (vaddr_t)&ptes[pl1_i(sva)], sva, blkendva, &cpumask, flags);
d1783 1
a1783 2
			pmap_free_ptp(pmap, ptp, sva, ptes,pdes,
			    &cpumask);
d1787 7
a1793 1
	pmap_tlb_shootnow(cpumask);
d1796 5
d1818 4
a1821 1
	int32_t cpumask = 0;
d1851 1
a1851 1
		pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va, opte, &cpumask);
d1860 1
a1860 1
					      pve->pv_va, ptes, pdes, &cpumask);
d1868 6
a1873 1
	pmap_tlb_shootnow(cpumask);
a1930 1
	int32_t cpumask = 0;
d1955 1
a1955 2
			pmap_tlb_shootdown(pve->pv_pmap, pve->pv_va, opte,
			    &cpumask);
d1962 1
a1962 1
	pmap_tlb_shootnow(cpumask);
d1998 2
a1999 1
	int32_t cpumask = 0;
d2011 2
a2012 1
	for (/* null */ ; sva < eva ; sva = blockend) {
d2014 2
a2015 1
		blockend = (sva & L2_FRAME) + NBPD_L2;
d2029 1
a2029 1
		if (pl_i(sva, PTP_LEVELS) == PDIR_SLOT_PTE)
d2033 1
a2033 1
		if (!pmap_pdes_valid(sva, pdes, NULL))
d2037 1
a2037 1
		if (sva >= VM_MAXUSER_ADDRESS && sva < VM_MAX_ADDRESS)
d2041 1
a2041 1
		spte = &ptes[pl1_i(sva)];
a2049 3
			if (opte != *spte)
				pmap_tlb_shootdown(pmap, ptoa(spte - ptes),
				    *spte, &cpumask);
d2053 7
a2059 1
	pmap_tlb_shootnow(cpumask);
d2333 3
a2335 11
	if ((opte & ~(PG_M|PG_U)) != npte) {
#if defined(MULTIPROCESSOR)
		int32_t cpumask = 0;

		pmap_tlb_shootdown(pmap, va, opte, &cpumask);
		pmap_tlb_shootnow(cpumask);
#else
		/* Don't bother deferring in the single CPU case. */
		if (pmap_is_curpmap(pmap))
			pmap_update_pg(va);
#endif
a2543 3
/******************** TLB shootdown code ********************/


d2545 1
a2545 1
pmap_tlb_shootnow(int32_t cpumask)
d2547 4
d2552 21
a2572 7
	struct cpu_info *ci, *self;
	CPU_INFO_ITERATOR cii;
	int s;
#ifdef DIAGNOSTIC
	int count = 0;
#endif
#endif
d2574 1
a2574 2
	if (cpumask == 0)
		return;
d2576 2
a2577 5
#ifdef MULTIPROCESSOR
	self = curcpu();
	s = splipi();
	self->ci_tlb_ipi_mask = cpumask;
#endif
d2579 2
a2580 1
	pmap_do_tlb_shootdown(0);	/* do *our* work. */
d2582 7
a2588 2
#ifdef MULTIPROCESSOR
	splx(s);
a2589 3
	/*
	 * Send the TLB IPI to other CPUs pending shootdowns.
	 */
d2591 2
a2592 1
		if (ci == self)
d2594 19
a2612 4
		if (cpumask & (1U << ci->ci_cpuid))
			if (x86_send_ipi(ci, X86_IPI_TLB) != 0)
			     x86_atomic_clearbits_ul(&self->ci_tlb_ipi_mask,
				    (1U << ci->ci_cpuid));
d2615 2
a2616 10
	while (self->ci_tlb_ipi_mask != 0)
#ifdef DIAGNOSTIC
		if (count++ > 1000000000)
			panic("TLB IPI rendezvous failed (mask %x)",
			    self->ci_tlb_ipi_mask);
#else
		/* XXX insert pause instruction */
		;
#endif
#endif
a2618 5
/*
 * pmap_tlb_shootdown:
 *
 *	Cause the TLB entry for pmap/va to be shot down.
 */
d2620 1
a2620 1
pmap_tlb_shootdown(pmap_t pmap, vaddr_t va, pt_entry_t pte, int32_t *cpumaskp)
a2622 2
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
d2624 3
d2628 6
a2633 8
#ifdef LARGEPAGES
	if (pte & PG_PS)
		va &= PG_LGFRAME;
#endif

	if (pmap_initialized == FALSE || cpus_attached == 0) {
		pmap_update_pg(va);
		return;
d2636 2
a2637 12
#if 0
	printf("doshootdown %lx\n", va);
#endif

	CPU_INFO_FOREACH(cii, ci) {
		/* Note: we queue shootdown events for ourselves here! */
		if (pmap_is_active(pmap, ci->ci_cpuid) == 0)
			continue;
		if (ci != self && !(ci->ci_flags & CPUF_RUNNING))
			continue;
		pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];
		mtx_enter(&pq->pq_mutex);
d2639 3
a2641 9
		/*
		 * If there's a global flush already queued, or a
		 * non-global flush, and this pte doesn't have the G
		 * bit set, don't bother.
		 */
		if (pq->pq_flushg > 0 ||
		    (pq->pq_flushu > 0 && (pte & pmap_pg_g) == 0)) {
			mtx_leave(&pq->pq_mutex);
			continue;
d2643 7
a2649 31

		pj = pmap_tlb_shootdown_job_get(pq);
		pq->pq_pte |= pte;
		if (pj == NULL) {
			/*
			 * Couldn't allocate a job entry.
			 * Kill it now for this cpu, unless the failure
			 * was due to too many pending flushes; otherwise,
			 * tell other cpus to kill everything..
			 */
			if (ci == self && pq->pq_count < PMAP_TLB_MAXJOBS) {
				pmap_update_pg(va);
			} else {
				if (pq->pq_pte & pmap_pg_g)
					pq->pq_flushg++;
				else
					pq->pq_flushu++;
				/*
				 * Since we've nailed the whole thing,
				 * drain the job entries pending for that
				 * processor.
				 */
				pmap_tlb_shootdown_q_drain(pq);
				*cpumaskp |= 1U << ci->ci_cpuid;
			}
		} else {
			pj->pj_pmap = pmap;
			pj->pj_va = va;
			pj->pj_pte = pte;
			TAILQ_INSERT_TAIL(&pq->pq_head, pj, pj_list);
			*cpumaskp |= 1U << ci->ci_cpuid;
d2651 1
a2651 1
		mtx_leave(&pq->pq_mutex);
d2653 4
a2658 5
/*
 * pmap_do_tlb_shootdown:
 *
 *	Process pending TLB shootdown operations for this processor.
 */
d2660 1
a2660 1
pmap_do_tlb_shootdown(struct cpu_info *self)
d2662 1
a2662 5
	u_long cpu_id = cpu_number();
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj;
#ifdef MULTIPROCESSOR
	struct cpu_info *ci;
d2664 9
a2672 1
#endif
d2674 2
a2675 1
	mtx_enter(&pq->pq_mutex);
d2677 3
a2679 14
	if (pq->pq_flushg) {
		COUNT(flushg);
		tlbflushg();
		pq->pq_flushg = 0;
		pq->pq_flushu = 0;
		pmap_tlb_shootdown_q_drain(pq);
	} else {
		/*
		 * TLB flushes for PTEs with PG_G set may be in the queue
		 * after a flushu, they need to be dealt with.
		 */
		if (pq->pq_flushu) {
			COUNT(flushu);
			tlbflush();
a2680 2
		while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
			TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
d2682 5
a2686 5
			if ((!pq->pq_flushu && pmap_is_curpmap(pj->pj_pmap)) ||
			    (pj->pj_pte & pmap_pg_g))
				pmap_update_pg(pj->pj_va);

			pmap_tlb_shootdown_job_put(pq, pj);
d2688 1
a2688 2

		pq->pq_flushu = pq->pq_pte = 0;
d2691 1
a2691 6
#ifdef MULTIPROCESSOR
	CPU_INFO_FOREACH(cii, ci)
		x86_atomic_clearbits_ul(&ci->ci_tlb_ipi_mask,
		    (1U << cpu_id));
#endif
	mtx_leave(&pq->pq_mutex);
a2693 10

/*
 * pmap_tlb_shootdown_q_drain:
 *
 *	Drain a processor's TLB shootdown queue.  We do not perform
 *	the shootdown operations.  This is merely a convenience
 *	function.
 *
 *	Note: We expect the queue to be locked.
 */
d2695 1
a2695 1
pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *pq)
d2697 3
a2699 1
	struct pmap_tlb_shootdown_job *pj;
d2701 1
a2701 6
	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		pmap_tlb_shootdown_job_put(pq, pj);
	}
	pq->pq_pte = 0;
}
d2703 2
a2704 10
/*
 * pmap_tlb_shootdown_job_get:
 *
 *	Get a TLB shootdown job queue entry.  This places a limit on
 *	the number of outstanding jobs a processor may have.
 *
 *	Note: We expect the queue to be locked.
 */
struct pmap_tlb_shootdown_job *
pmap_tlb_shootdown_job_get(struct pmap_tlb_shootdown_q *pq)
d2706 2
a2707 16
	struct pmap_tlb_shootdown_job *pj;

	if (pq->pq_count >= PMAP_TLB_MAXJOBS)
		return (NULL);

	mtx_enter(&pmap_tlb_shootdown_job_mutex);

	if (pj_free == NULL) {
		mtx_leave(&pmap_tlb_shootdown_job_mutex);
		return NULL;
	}
	pj = &pj_free->pja_job;
	pj_free =
	    (union pmap_tlb_shootdown_job_al *)pj_free->pja_job.pj_nextfree;

	mtx_leave(&pmap_tlb_shootdown_job_mutex);
a2708 2
	pq->pq_count++;
	return (pj);
a2710 7
/*
 * pmap_tlb_shootdown_job_put:
 *
 *	Put a TLB shootdown job queue entry onto the free list.
 *
 *	Note: We expect the queue to be locked.
 */
d2712 1
a2712 2
pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *pq,
    struct pmap_tlb_shootdown_job *pj)
d2714 1
d2716 2
a2717 8
#ifdef DIAGNOSTIC
	if (pq->pq_count == 0)
		panic("pmap_tlb_shootdown_job_put: queue length inconsistency");
#endif
	mtx_enter(&pmap_tlb_shootdown_job_mutex);
	pj->pj_nextfree = &pj_free->pja_job;
	pj_free = (union pmap_tlb_shootdown_job_al *)pj;
	mtx_leave(&pmap_tlb_shootdown_job_mutex);
a2718 1
	pq->pq_count--;
d2722 1
a2722 1
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
d2724 1
a2724 2
	*vstartp = virtual_avail;
	*vendp = VM_MAX_KERNEL_ADDRESS;
d2726 1
@


1.21
log
@Switch amd64 to VM_PAGE_MD. Mostly just imitating i386. flags in pg_flags.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2007/04/13 18:57:49 art Exp $	*/
d119 1
d1578 1
a1578 1
		if (whichqs != 0) {
@


1.20
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2007/04/13 10:36:00 miod Exp $	*/
a165 6
 *  - struct pv_head: there is one pv_head per managed page of
 *	physical memory.   the pv_head points to a list of pv_entry
 *	structures which describe all the <PMAP,VA> pairs that this
 *      page is mapped in.    this is critical for page based operations
 *      such as pmap_page_protect() [change protection on _all_ mappings
 *      of a page]
a219 6
 * - pvh_lock (per pv_head)
 *   this lock protects the pv_entry list which is chained off the
 *   pv_head structure for a specific managed PA.   it is locked
 *   when traversing the list (e.g. adding/removing mappings,
 *   syncing R/M bits, etc.)
 *
d380 1
a380 1
void  pmap_enter_pv(struct pv_head *, struct pv_entry *, struct pmap *,
d389 1
a389 1
struct pv_entry *pmap_remove_pv(struct pv_head *, struct pmap *, vaddr_t);
d404 2
d433 15
a897 2
	int lcv;
	unsigned int npages, i;
d899 1
a899 59
	vaddr_t addr;
	vsize_t s;

	/*
	 * compute the number of pages we have and then allocate RAM
	 * for each pages' pv_head and saved attributes.
	 */

	npages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		npages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
	s = (vsize_t) (sizeof(struct pv_head) * npages +
		       sizeof(char) * npages);
	s = round_page(s);
	addr = (vaddr_t) uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: unable to allocate pv_heads");

	/*
	 * init all pv_head's and attrs in one memset
	 */

	/* allocate pv_head stuff first */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.pvhead = (struct pv_head *) addr;
		addr = (vaddr_t)(vm_physmem[lcv].pmseg.pvhead +
				 (vm_physmem[lcv].end - vm_physmem[lcv].start));
		for (i = 0;
		     i < (vm_physmem[lcv].end - vm_physmem[lcv].start); i++) {
			simple_lock_init(
			    &vm_physmem[lcv].pmseg.pvhead[i].pvh_lock);
		}
	}

	/* now allocate attrs */
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		vm_physmem[lcv].pmseg.attrs = (unsigned char *)addr;
		addr = (vaddr_t)(vm_physmem[lcv].pmseg.attrs +
				 (vm_physmem[lcv].end - vm_physmem[lcv].start));
	}

#ifdef LOCKDEBUG
	/*
	 * Now, initialize all the pv_head locks.
	 * We only do this if LOCKDEBUG because we know that initialized locks
	 * are always all-zero if !LOCKDEBUG.
	 */
	for (lcv = 0; lcv < vm_nphysseg ; lcv++) {
		int off, npages;
		struct pmap_physseg *pmsegp;

		npages = vm_physmem[lcv].end - vm_physmem[lcv].start;
		pmsegp = &vm_physmem[lcv].pmseg;

		for (off = 0; off <npages; off++)
			simple_lock_init(&pmsegp->pvhead[off].pvh_lock);

	}
#endif
d926 2
a927 5
 *   pmap_enter_pv: enter a mapping onto a pv_head list
 *   pmap_remove_pv: remove a mapping from a pv_head list
 *
 * NOTE: pmap_enter_pv expects to lock the pvh itself
 *       pmap_remove_pv expects the caller to lock the pvh before calling
d931 1
a931 1
 * pmap_enter_pv: enter a mapping onto a pv_head lst
a933 1
 * => we will gain the lock on the pv_head and allocate the new pv_entry
d941 1
a941 1
pmap_enter_pv(struct pv_head *pvh, struct pv_entry *pve, struct pmap *pmap,
d947 2
a948 4
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	pve->pv_next = pvh->pvh_list;		/* add to ... */
	pvh->pvh_list = pve;			/* ... locked list */
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
a954 1
 * => caller should hold lock on pv_head [so that attrs can be adjusted]
d960 1
a960 1
pmap_remove_pv(struct pv_head *pvh, struct pmap *pmap, vaddr_t va)
d964 2
a965 3
	prevptr = &pvh->pvh_list;		/* previous pv_entry pointer */
	pve = *prevptr;
	while (pve) {
a970 1
		pve = pve->pv_next;			/* advance */
d1629 1
a1630 1
	int bank, off;
d1661 2
d1664 1
a1664 1
		 * if we are not on a pv_head list we are done.
d1669 1
a1669 2
			if (vm_physseg_find(atop(opte & PG_FRAME), &off)
			    != -1)
a1675 1
		bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
d1677 1
a1677 1
		if (bank == -1)
d1684 2
a1685 5
		simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
		pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap,
				     startva);
		simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
d1710 2
a1712 2
	int bank, off;
	struct pv_entry *pve;
d1732 2
d1735 1
a1735 1
	 * if we are not on a pv_head list we are done.
a1736 1

d1739 1
a1739 1
		if (vm_physseg_find(atop(opte & PG_FRAME), &off) != -1) {
a1741 1
		}
a1745 1
	bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
d1747 1
a1747 1
	if (bank == -1)
d1754 2
a1755 5
	simple_lock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);
	vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
	pve = pmap_remove_pv(&vm_physmem[bank].pmseg.pvhead[off], pmap, va);
	simple_unlock(&vm_physmem[bank].pmseg.pvhead[off].pvh_lock);

a1789 4
	/*
	 * we lock in the pmap => pv_head direction
	 */

a1899 1
 * => we set pv_head => pmap locking
d1906 1
a1906 3
	int bank, off;
	struct pv_head *pvh;
	struct pv_entry *pve, *npve, **prevptr;
a1913 13
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
		return;
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return;
	}

	/* set pv_head => pmap locking */
d1916 2
a1917 2
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);
a1918 3
	for (prevptr = &pvh->pvh_list, pve = pvh->pvh_list;
	    pve != NULL; pve = npve) {
		npve = pve->pv_next;
d1935 1
a1935 1
		/* atomically save the old PTE and zap! it */
d1944 1
a1944 2
		/* sync R/M bits */
		vm_physmem[bank].pmseg.attrs[off] |= (opte & (PG_U|PG_M));
a1954 1
		*prevptr = npve;			/* remove it */
a1957 2
	pvh->pvh_list = NULL;
	simple_unlock(&pvh->pvh_lock);
a1970 2
 *
 * => we set pv_head => pmap locking
d1974 1
a1974 1
pmap_test_attrs(struct vm_page *pg, unsigned testbits)
a1975 3
	int bank, off;
	unsigned char *myattrs;
	struct pv_head *pvh;
d1979 1
d1981 1
a1981 6
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_test_attrs: unmanaged page?\n");
		return(FALSE);
	}
d1983 2
a1984 14
	/*
	 * before locking: see if attributes are already set and if so,
	 * return!
	 */

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	if (*myattrs & testbits)
		return(TRUE);

	/* test to see if there is a list before bothering to lock */
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL) {
		return(FALSE);
	}
a1985 1
	/* nope, gonna have to do it the hard way */
d1987 3
a1989 5
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	for (pve = pvh->pvh_list; pve != NULL && (*myattrs & testbits) == 0;
	     pve = pve->pv_next) {
d1993 1
a1993 1
		*myattrs |= pte;
d1995 1
d1997 4
a2000 4
	/*
	 * note that we will exit the for loop with a non-null pve if
	 * we have found the bits we are testing for.
	 */
d2002 1
a2002 3
	simple_unlock(&pvh->pvh_lock);
	PMAP_HEAD_TO_MAP_UNLOCK();
	return((*myattrs & testbits) != 0);
a2007 1
 * => we set pv_head => pmap locking
d2012 1
a2012 1
pmap_clear_attrs(struct vm_page *pg, unsigned clearbits)
a2013 3
	int bank, off;
	unsigned result;
	struct pv_head *pvh;
a2016 1
	unsigned char *myattrs;
d2018 2
d2021 1
a2021 6
	/* XXX: vm_page should either contain pv_head or have a pointer to it */
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_change_attrs: unmanaged page?\n");
		return(FALSE);
	}
a2023 7
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	/* XXX: needed if we hold head->map lock? */
	simple_lock(&pvh->pvh_lock);

	myattrs = &vm_physmem[bank].pmseg.attrs[off];
	result = *myattrs & clearbits;
	*myattrs &= ~clearbits;
d2025 5
a2029 1
	for (pve = pvh->pvh_list; pve != NULL; pve = pve->pv_next) {
d2039 1
a2039 1
			result |= (opte & clearbits);
a2047 1
	simple_unlock(&pvh->pvh_lock);
d2052 1
a2052 1
	return(result != 0);
a2222 1
 * => we set pmap => pv_head locking
d2230 2
a2231 4
	struct vm_page *ptp;
	struct pv_head *pvh;
	struct pv_entry *pve;
	int bank, off, error;
d2234 1
d2302 1
a2302 1
				bank = vm_physseg_find(atop(pa), &off);
d2304 1
a2304 1
				if (bank == -1)
d2310 1
a2310 4
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				simple_lock(&pvh->pvh_lock);
				vm_physmem[bank].pmseg.attrs[off] |= opte;
				simple_unlock(&pvh->pvh_lock);
d2312 6
a2317 1
				pvh = NULL;	/* ensure !PG_PVLIST */
d2332 1
a2332 1
			bank = vm_physseg_find(atop(opte & PG_FRAME), &off);
d2334 1
a2334 1
			if (bank == -1)
d2339 3
a2341 7
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pve = pmap_remove_pv(pvh, pmap, va);
			vm_physmem[bank].pmseg.attrs[off] |= opte;
			simple_unlock(&pvh->pvh_lock);
		} else {
			pve = NULL;
d2363 4
a2366 3
	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
d2377 1
a2377 2
		/* lock pvh when adding */
		pmap_enter_pv(pvh, pve, pmap, va, ptp);
a2378 1

a2379 1
		pvh = NULL;		/* ensure !PG_PVLIST */
d2386 1
a2386 1
	 * at this point pvh is !NULL if we want the PG_PVLIST bit set
d2393 4
d2398 1
a2398 1
	if (pvh)
a2601 3
	/*
	 * we lock in the pmap => pv_head direction
	 */
@


1.19
log
@map peeing -> mapping
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2007/04/04 17:44:45 art Exp $	*/
d1172 1
a1172 1
		ptp->pg_flags &= ~PG_BUSY; /* never busy */
d2596 1
a2596 1
		ptp->pg_flags &= ~PG_BUSY;
@


1.18
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2007/02/03 16:48:23 miod Exp $	*/
d982 1
a982 1
 *   pmap_remove_pv: remove a mappiing from a pv_head list
@


1.17
log
@Remove unused functionality from lockmgr():
- LK_EXCLUPGRADE is never used.
- LK_REENABLE is never used.
- LK_SETRECURSE is never used. Because of this, the lk_recurselevel
  field is always zero, so it can be removed to.
- the spinlock version (and LK_SPIN) is never used, since it was decided
  to use different locking structure for MP-safe protection.

Tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2007/01/15 23:19:05 jsg Exp $	*/
d1172 1
a1172 1
		ptp->flags &= ~PG_BUSY; /* never busy */
d1368 1
a1368 1
			KASSERT((pg->flags & PG_BUSY) == 0);
d2596 1
a2596 1
		ptp->flags &= ~PG_BUSY;
@


1.16
log
@ansi/deregister
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2006/06/29 10:22:25 mickey Exp $	*/
a217 15
 * "normal" locks:
 *
 *  - pmap_main_lock
 *    this lock is used to prevent deadlock and/or provide mutex
 *    access to the pmap system.   most operations lock the pmap
 *    structure first, then they lock the pv_lists (if needed).
 *    however, some operations such as pmap_page_protect lock
 *    the pv_lists and then lock pmaps.   in order to prevent a
 *    cycle, we require a mutex lock when locking the pv_lists
 *    first.   thus, the "pmap = >pv_list" lockers must gain a
 *    read-lock on pmap_main_lock before locking the pmap.   and
 *    the "pv_list => pmap" lockers must gain a write-lock on
 *    pmap_main_lock before locking.    since only one thread
 *    can write-lock a lock at a time, this provides mutex.
 *
a259 14
#if (defined(MULTIPROCESSOR) || defined(LOCKDEBUG)) && 0
struct lock pmap_main_lock;
#define PMAP_MAP_TO_HEAD_LOCK() \
     (void) spinlockmgr(&pmap_main_lock, LK_SHARED, NULL)
#define PMAP_MAP_TO_HEAD_UNLOCK() \
     (void) spinlockmgr(&pmap_main_lock, LK_RELEASE, NULL)

#define PMAP_HEAD_TO_MAP_LOCK() \
     (void) spinlockmgr(&pmap_main_lock, LK_EXCLUSIVE, NULL)
#define PMAP_HEAD_TO_MAP_UNLOCK() \
     (void) spinlockmgr(&pmap_main_lock, LK_RELEASE, NULL)

#else

a265 2
#endif

a821 3
#if (defined(MULTIPROCESSOR) || defined(LOCKDEBUG)) && 0
	spinlockinit(&pmap_main_lock, "pmaplk", 0);
#endif
a990 1
 * => caller should hold the proper lock on pmap_main_lock
a1014 1
 * => caller should hold proper lock on pmap_main_lock
@


1.15
log
@make compile w/ debug; from danielcavanagh@@aanet.com.au
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2005/11/28 16:56:31 martin Exp $	*/
d514 1
a514 4
pmap_map_ptes(pmap, ptepp, pdeppp)
	struct pmap *pmap;
	pt_entry_t **ptepp;
	pd_entry_t ***pdeppp;
d593 1
a593 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d637 1
a637 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d1029 3
d1035 2
a1036 6
pmap_enter_pv(pvh, pve, pmap, va, ptp)
	struct pv_head *pvh;
	struct pv_entry *pve;	/* preallocated pve for us to use */
	struct pmap *pmap;
	vaddr_t va;
	struct vm_page *ptp;	/* PTP in pmap that maps this VA */
d1058 1
a1058 4
pmap_remove_pv(pvh, pmap, va)
	struct pv_head *pvh;
	struct pmap *pmap;
	vaddr_t va;
d1121 1
a1121 1
	      pt_entry_t *ptes, pd_entry_t **pdes, int32_t *cpumaskp)
d1304 1
a1304 1
pmap_create()
d1369 1
a1369 2
pmap_destroy(pmap)
	struct pmap *pmap;
d1441 1
a1441 2
pmap_reference(pmap)
	struct pmap *pmap;
d1455 1
a1455 2
pmap_fork(pmap1, pmap2)
	struct pmap *pmap1, *pmap2;
d1591 1
a1591 4
pmap_extract(pmap, va, pap)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t *pap;
d1634 1
a1634 4
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	vm_prot_t prot;
d1724 2
a1725 7
pmap_remove_ptes(pmap, ptp, ptpva, startva, endva, cpumaskp, flags)
	struct pmap *pmap;
	struct vm_page *ptp;
	vaddr_t ptpva;
	vaddr_t startva, endva;
	int32_t *cpumaskp;
	int flags;
d1810 2
a1811 7
pmap_remove_pte(pmap, ptp, pte, va, cpumaskp, flags)
	struct pmap *pmap;
	struct vm_page *ptp;
	pt_entry_t *pte;
	vaddr_t va;
	int32_t *cpumaskp;
	int flags;
d1875 1
a1875 3
pmap_remove(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva, eva;
d1887 1
a1887 4
pmap_do_remove(pmap, sva, eva, flags)
	struct pmap *pmap;
	vaddr_t sva, eva;
	int flags;
d2016 1
a2016 2
pmap_page_remove(pg)
	struct vm_page *pg;
d2110 1
a2110 3
pmap_test_attrs(pg, testbits)
	struct vm_page *pg;
	unsigned testbits;
d2172 1
a2172 3
pmap_clear_attrs(pg, clearbits)
	struct vm_page *pg;
	unsigned clearbits;
d2252 1
a2252 4
pmap_write_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
d2327 1
a2327 3
pmap_unwire(pmap, va)
	struct pmap *pmap;
	vaddr_t va;
d2367 1
a2367 2
pmap_collect(pmap)
	struct pmap *pmap;
d2398 1
a2398 6
pmap_enter(pmap, va, pa, prot, flags)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d2611 1
a2611 4
pmap_get_physpage(va, level, paddrp)
	vaddr_t va;
	int level;
	paddr_t *paddrp;
d2648 1
a2648 5
pmap_alloc_level(pdes, kva, lvl, needed_ptps)
	pd_entry_t **pdes;
	vaddr_t kva;
	int lvl;
	long *needed_ptps;
d2692 1
a2692 2
pmap_growkernel(maxkvaddr)
	vaddr_t maxkvaddr;
d2761 1
a2761 3
pmap_dump(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva, eva;
d2869 1
a2869 5
pmap_tlb_shootdown(pmap, va, pte, cpumaskp)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t pte;
	int32_t *cpumaskp;
d3010 1
a3010 2
pmap_tlb_shootdown_q_drain(pq)
	struct pmap_tlb_shootdown_q *pq;
@


1.14
log
@- consistently use x86_round_pdr() allowing us to remove the superfluous
  round_pdr() macro
- while there remove two more unused Mach macros
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2005/10/26 18:46:06 martin Exp $	*/
d2851 1
a2851 1
		blkendva = round_pdr(sva+1);
@


1.13
log
@goodbye more Mach macros

help toby, ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2005/09/25 20:48:18 miod Exp $	*/
d2762 1
a2762 1
	maxkvaddr = round_pdr(maxkvaddr);
@


1.12
log
@Turn CPU_INFO_FOREACH into a real construct, like all queue(3) iterators,
instead of the contents of a for() loop. No functional change.
From the m88k SMP tree; ok art@@ deraadt@@

[complete diff this time]
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2005/07/26 08:38:29 art Exp $	*/
d750 1
a750 1
		btop(kva_start - VM_MIN_KERNEL_ADDRESS);
d1793 1
a1793 1
			if (vm_physseg_find(btop(opte & PG_FRAME), &off)
d1801 1
a1801 1
		bank = vm_physseg_find(btop(opte & PG_FRAME), &off);
d1872 1
a1872 1
		if (vm_physseg_find(btop(opte & PG_FRAME), &off) != -1) {
d1880 1
a1880 1
	bank = vm_physseg_find(btop(opte & PG_FRAME), &off);
d2351 1
a2351 1
				pmap_tlb_shootdown(pmap, ptob(spte - ptes),
@


1.11
log
@Instead of juggling around with cr4 and enabling parts of it sometimes,
other parts later, etc. Just set it to the same default value everywhere.
We won't survive without PSE and tt's not like someone will suddenly make
an amd64 that doesn't support PGE.

This will allow us to make the bootstrap process slightly more sane.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 2005/06/01 14:36:36 brad Exp $	*/
d492 1
a492 1
	for (CPU_INFO_FOREACH(cii, ci)) {
d1027 1
a1027 1
 *       pmap_remove_pv expects te caller to lock the pvh before calling
d2904 1
a2904 1
	for (CPU_INFO_FOREACH(cii, ci)) {
d2956 1
a2956 1
	for (CPU_INFO_FOREACH(cii, ci)) {
d3058 1
a3058 1
	for (CPU_INFO_FOREACH(cii, ci))
@


1.10
log
@as Jason requested, be gone vtophys().

ok deraadt@@ marco@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 2005/05/27 19:32:39 art Exp $	*/
a761 2
	lcr4(rcr4() | CR4_PGE);	/* enable hardware (via %cr4) */
	tlbflush();
a782 6

	/*
	 * Enable large pages.
	 */
	lcr4(rcr4() | CR4_PSE);
	tlbflush();
@


1.9
log
@Stop pretending that amd64 is i386. We're insulting the cpu by not even
pretending to use all the address space it gives us.

 - Map all physical memory 1-1 and implement PMAP_DIRECT
 - Remove the vast magic we do to map pages for pmap_zero_page,
   pmap_copy_page, pv allocation, magic while bootstrapping,
   reading of /dev/mem, etc.
 - implement a fast pmap_zero_page based on sse instructions.

I love removing code. More to come.

deraadt@@ ok tested by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2005/05/24 21:11:47 tedu Exp $	*/
a1650 18


/*
 * vtophys: virtual address to physical address.  For use by
 * machine-dependent code only.
 */

paddr_t
vtophys(va)
	vaddr_t va;
{
	paddr_t pa;

	if (pmap_extract(pmap_kernel(), va, &pa) == TRUE)
		return (pa);
	return (0);
}

@


1.8
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 2004/07/22 15:50:18 art Exp $	*/
a171 3
 *  - pv_page/pv_page_info: pv_entry's are allocated out of pv_page's.
 *      if we run out of pv_entry's we allocate a new pv_page and free
 *      its pv_entrys.
d209 1
a209 15
 *	- plan 1: try to allocate one off the free list
 *		=> success: done!
 *		=> failure: no more free pv_entrys on the list
 *	- plan 2: try to allocate a new pv_page to add a chunk of
 *	pv_entrys to the free list
 *		[a] obtain a free, unmapped, VA in kmem_map.  either
 *		we have one saved from a previous call, or we allocate
 *		one now using a "vm_map_lock_try" in uvm_map
 *		=> success: we have an unmapped VA, continue to [b]
 *		=> failure: unable to lock kmem_map or out of VA in it.
 *			move on to plan 3.
 *		[b] allocate a page for the VA
 *		=> success: map it in, free the pv_entry's, DONE!
 *		=> failure: no free vm_pages, etc.
 *			save VA for later call to [a], go to plan 3.
a246 4
 * - pvalloc_lock
 *   this lock protects the data structures which are used to manage
 *   the free list of pv_entry structures.
 *
a272 1
struct simplelock pvalloc_lock;
d378 1
a378 1
 * pv_page management structures: locked by pvalloc_lock
d380 1
a380 11

TAILQ_HEAD(pv_pagelist, pv_page);
struct pv_pagelist pv_freepages;	/* list of pv_pages with free entrys */
struct pv_pagelist pv_unusedpgs; /* list of unused pv_pages */
unsigned int pv_nfpvents;	/* # of free pv entries */
struct pv_page *pv_initpage;	/* bootstrap page from kernel_map */
vaddr_t pv_cachedva;		/* cached VA for later use */

#define PVE_LOWAT (PVE_PER_PVPAGE / 2)	/* free pv_entry low water mark */
#define PVE_HIWAT (PVE_LOWAT + (PVE_PER_PVPAGE * 2))
					/* high water mark */
a395 20
 * MULTIPROCESSOR: special VA's/ PTE's are actually allocated inside a
 * X86_MAXPROCS*NPTECL array of PTE's, to avoid cache line thrashing
 * due to false sharing.
 */

#ifdef MULTIPROCESSOR
#define PTESLEW(pte, id) ((pte)+(id)*NPTECL)
#define VASLEW(va,id) ((va)+(id)*NPTECL*PAGE_SIZE)
#else
#define PTESLEW(pte, id) (pte)
#define VASLEW(va,id) (va)
#endif

/*
 * special VAs and the PTEs that map them
 */
pt_entry_t *csrc_pte, *cdst_pte, *zero_pte, *ptp_pte, *early_zero_pte;
caddr_t csrcp, cdstp, zerop, ptpp, early_zerop;

/*
a404 2
caddr_t vmmap; /* XXX: used by mem.c... it should really uvm_map_reserve it */

a418 5
#if defined(I586_CPU)
/* stuff to fix the pentium f00f bug */
extern vaddr_t pentium_idt_vaddr;
#endif

d423 5
a427 15
static struct pv_entry	*pmap_add_pvpage(struct pv_page *, boolean_t);
static struct pv_entry	*pmap_alloc_pv(struct pmap *, int); /* see codes below */
#define ALLOCPV_NEED	0	/* need PV now */
#define ALLOCPV_TRY	1	/* just try to allocate, don't steal */
#define ALLOCPV_NONEED	2	/* don't need PV, just growing cache */
struct pv_entry	*pmap_alloc_pvpage(struct pmap *, int);
static void	 pmap_enter_pv(struct pv_head *,
    struct pv_entry *, struct pmap *, vaddr_t, struct vm_page *);
static void	 pmap_free_pv(struct pmap *, struct pv_entry *);
static void	 pmap_free_pvs(struct pmap *, struct pv_entry *);
static void	 pmap_free_pv_doit(struct pv_entry *);
void	 pmap_free_pvpage(void);
struct vm_page	*pmap_get_ptp(struct pmap *, vaddr_t, pd_entry_t **);
static struct vm_page	*pmap_find_ptp(struct pmap *, vaddr_t, paddr_t, int);
void	 pmap_free_ptp(struct pmap *, struct vm_page *,
d429 1
a429 2
static void	 pmap_freepage(struct pmap *, struct vm_page *, int);
static boolean_t pmap_is_curpmap(struct pmap *);
d431 3
a433 3
static void pmap_map_ptes(struct pmap *, pt_entry_t **, pd_entry_t ***);
static struct pv_entry *pmap_remove_pv(struct pv_head *, struct pmap *, vaddr_t);
void	pmap_do_remove(struct pmap *, vaddr_t, vaddr_t, int);
d436 1
a436 1
void	pmap_remove_ptes(struct pmap *, struct vm_page *, vaddr_t,
d441 1
a441 5
static vaddr_t	pmap_tmpmap_pa(paddr_t);
static pt_entry_t *pmap_tmpmap_pvepte(struct pv_entry *);
static void	pmap_tmpunmap_pa(void);
static void	pmap_tmpunmap_pvepte(struct pv_entry *);
static void	pmap_unmap_ptes(struct pmap *);
d444 2
a445 1
void	pmap_alloc_level(pd_entry_t **, vaddr_t, int, long *);
d456 2
a457 3
__inline static boolean_t
pmap_is_curpmap(pmap)
	struct pmap *pmap;
d467 2
a468 4
__inline static boolean_t
pmap_is_active(pmap, cpu_id)
	struct pmap *pmap;
	int cpu_id;
a469 1

d474 1
a474 85
/*
 * pmap_tmpmap_pa: map a page in for tmp usage
 */

__inline static vaddr_t
pmap_tmpmap_pa(pa)
	paddr_t pa;
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(ptpp, id);
#if defined(DIAGNOSTIC)
	if (*ptpte)
		panic("pmap_tmpmap_pa: ptp_pte in use?");
#endif
	*ptpte = PG_V | PG_RW | pa;		/* always a new mapping */
	return((vaddr_t)ptpva);
}

/*
 * pmap_tmpunmap_pa: unmap a tmp use page (undoes pmap_tmpmap_pa)
 */

__inline static void
pmap_tmpunmap_pa()
{
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *ptpte = PTESLEW(ptp_pte, id);
	caddr_t ptpva = VASLEW(ptpp, id);
#if defined(DIAGNOSTIC)
	if (!pmap_valid_entry(*ptp_pte))
		panic("pmap_tmpunmap_pa: our pte invalid?");
#endif
	*ptpte = 0;		/* zap! */
	pmap_update_pg((vaddr_t)ptpva);
#ifdef MULTIPROCESSOR
	/*
	 * No need for tlb shootdown here, since ptp_pte is per-CPU.
	 */
#endif  
}

/*
 * pmap_tmpmap_pvepte: get a quick mapping of a PTE for a pv_entry
 *
 * => do NOT use this on kernel mappings [why?  because pv_ptp may be NULL]
 */

__inline static pt_entry_t *
pmap_tmpmap_pvepte(pve)
	struct pv_entry *pve;
{
#ifdef DIAGNOSTIC
	if (pve->pv_pmap == pmap_kernel())
		panic("pmap_tmpmap_pvepte: attempt to map kernel");
#endif

	/* is it current pmap?  use direct mapping... */
	if (pmap_is_curpmap(pve->pv_pmap))
		return(vtopte(pve->pv_va));

	return(((pt_entry_t *)pmap_tmpmap_pa(VM_PAGE_TO_PHYS(pve->pv_ptp)))
	       + ptei((unsigned long)pve->pv_va));
}

/*
 * pmap_tmpunmap_pvepte: release a mapping obtained with pmap_tmpmap_pvepte
 */

__inline static void
pmap_tmpunmap_pvepte(pve)
	struct pv_entry *pve;
{
	/* was it current pmap?   if so, return */
	if (pmap_is_curpmap(pve->pv_pmap))
		return;

	pmap_tmpunmap_pa();
}

__inline static void
d513 1
a513 1
__inline static void
d561 2
a562 3
__inline static void
pmap_unmap_ptes(pmap)
	struct pmap *pmap;
d695 2
a696 1
	pt_entry_t *pte;
d778 1
d780 1
a780 5
	 * zero_pte is stuck at the end of mapped space for the kernel
	 * image (disjunct from kva space). This is done so that it
	 * can safely be used in pmap_growkernel (pmap_get_physpage),
	 * when it's called for the first time.
	 * XXXfvdl fix this for MULTIPROCESSOR later.
d782 3
a784 3

	early_zerop = (caddr_t)(KERNBASE + NKL2_KIMG_ENTRIES * NBPD_L2);
	early_zero_pte = PTE_BASE + pl1_i((unsigned long)early_zerop);
d811 2
a812 2
		*early_zero_pte = (trunc_page(pdp) & PG_FRAME) | PG_V | PG_RW;
		pmap_update_pg((vaddr_t)early_zerop);
d814 1
a814 1
		va = (vaddr_t)early_zerop + off;
d826 2
a827 2
		*early_zero_pte = (trunc_page(pdp) & PG_FRAME) | PG_V | PG_RW;
		pmap_update_pg((vaddr_t)early_zerop);
d829 1
a829 1
		va = (vaddr_t)early_zerop + off;
d833 1
d840 1
a840 56
	/*
	 * now we allocate the "special" VAs which are used for tmp mappings
	 * by the pmap (and other modules).    we allocate the VAs by advancing
	 * virtual_avail (note that there are no pages mapped at these VAs).
	 * we find the PTE that maps the allocated VA via the linear PTE
	 * mapping.
	 */

	pte = PTE_BASE + pl1_i(virtual_avail);

#ifdef MULTIPROCESSOR
	/*
	 * Waste some VA space to avoid false sharing of cache lines
	 * for page table pages: Give each possible CPU a cache line
	 * of PTE's (8) to play with, though we only need 4.  We could
	 * recycle some of this waste by putting the idle stacks here
	 * as well; we could waste less space if we knew the largest
	 * CPU ID beforehand.
	 */
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;

	cdstp = (caddr_t) virtual_avail+PAGE_SIZE;  cdst_pte = pte+1;

	zerop = (caddr_t) virtual_avail+PAGE_SIZE*2;  zero_pte = pte+2;

	ptpp = (caddr_t) virtual_avail+PAGE_SIZE*3;  ptp_pte = pte+3;

	virtual_avail += PAGE_SIZE * X86_MAXPROCS * NPTECL;
	pte += X86_MAXPROCS * NPTECL;
#else
	csrcp = (caddr_t) virtual_avail;  csrc_pte = pte;	/* allocate */
	virtual_avail += PAGE_SIZE; pte++;			/* advance */

	cdstp = (caddr_t) virtual_avail;  cdst_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;

	zerop = (caddr_t) virtual_avail;  zero_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;

	ptpp = (caddr_t) virtual_avail;  ptp_pte = pte;
	virtual_avail += PAGE_SIZE; pte++;
#endif

#if VM_MIN_KERNEL_ADDRESS == KERNBASE
	early_zerop = zerop;
	early_zero_pte = zero_pte;
#endif

	pte = (void *)0xdeadbeef;

	/* XXX: vmmap used by mem.c... should be uvm_map_reserve */
	/* XXXfvdl PTEs not needed here */
	vmmap = (char *)virtual_avail;			/* don't need pte */
	virtual_avail += PAGE_SIZE; pte++;

	msgbuf_vaddr = virtual_avail;			/* don't need pte */
a841 1
	pte += x86_btop(round_page(MSGBUFSIZE));
d843 2
a844 2
	idt_vaddr = virtual_avail;			/* don't need pte */
	virtual_avail += 2 * PAGE_SIZE; pte += 2;
a847 6
#if defined(I586_CPU)
	/* pentium f00f bug stuff */
	pentium_idt_vaddr = virtual_avail;		/* don't need pte */
	virtual_avail += PAGE_SIZE; pte++;
#endif

d854 1
a854 1
	virtual_avail += PAGE_SIZE; pte++;
a871 1
	simple_lock_init(&pvalloc_lock);
a873 2
	TAILQ_INIT(&pv_freepages);
	TAILQ_INIT(&pv_unusedpgs);
d880 3
a882 1
		  &pool_allocator_nointr);
d925 1
a925 3
		*early_zero_pte = (newp & PG_FRAME) | PG_V | PG_RW;
		pmap_update_pg((vaddr_t)early_zerop);
		memset(early_zerop, 0, PAGE_SIZE);
d941 1
a941 1
pmap_init()
d945 1
d1006 2
a1007 17
	/*
	 * now we need to free enough pv_entry structures to allow us to get
	 * the kmem_map allocated and inited (done after this
	 * function is finished).  to do this we allocate one bootstrap page out
	 * of kernel_map and use it to provide an initial pool of pv_entry
	 * structures.   we never free this page.
	 */

	pv_initpage = (struct pv_page *) uvm_km_alloc(kernel_map, PAGE_SIZE);
	if (pv_initpage == NULL)
		panic("pmap_init: pv_initpage");
	pv_cachedva = 0;   /* a VA we have allocated but not used yet */
	pv_nfpvents = 0;
	(void) pmap_add_pvpage(pv_initpage, FALSE);

	pj_page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
	if (pj_page == NULL)
d1009 1
a1029 334
 * pv_entry allocation functions:
 *   the main pv_entry allocation functions are:
 *     pmap_alloc_pv: allocate a pv_entry structure
 *     pmap_free_pv: free one pv_entry
 *     pmap_free_pvs: free a list of pv_entrys
 *
 * the rest are helper functions
 */

/*
 * pmap_alloc_pv: inline function to allocate a pv_entry structure
 * => we lock pvalloc_lock
 * => if we fail, we call out to pmap_alloc_pvpage
 * => 3 modes:
 *    ALLOCPV_NEED   = we really need a pv_entry, even if we have to steal it
 *    ALLOCPV_TRY    = we want a pv_entry, but not enough to steal
 *    ALLOCPV_NONEED = we are trying to grow our free list, don't really need
 *			one now
 *
 * "try" is for optional functions like pmap_copy().
 */

__inline static struct pv_entry *
pmap_alloc_pv(pmap, mode)
	struct pmap *pmap;
	int mode;
{
	struct pv_page *pvpage;
	struct pv_entry *pv;

	simple_lock(&pvalloc_lock);

	pvpage = TAILQ_FIRST(&pv_freepages);
	if (pvpage != NULL) {
		pvpage->pvinfo.pvpi_nfree--;
		if (pvpage->pvinfo.pvpi_nfree == 0) {
			/* nothing left in this one? */
			TAILQ_REMOVE(&pv_freepages, pvpage, pvinfo.pvpi_list);
		}
		pv = pvpage->pvinfo.pvpi_pvfree;
		KASSERT(pv);
		pvpage->pvinfo.pvpi_pvfree = pv->pv_next;
		pv_nfpvents--;  /* took one from pool */
	} else {
		pv = NULL;		/* need more of them */
	}

	/*
	 * if below low water mark or we didn't get a pv_entry we try and
	 * create more pv_entrys ...
	 */

	if (pv_nfpvents < PVE_LOWAT || pv == NULL) {
		if (pv == NULL)
			pv = pmap_alloc_pvpage(pmap, (mode == ALLOCPV_TRY) ?
					       mode : ALLOCPV_NEED);
		else
			(void) pmap_alloc_pvpage(pmap, ALLOCPV_NONEED);
	}

	simple_unlock(&pvalloc_lock);
	return(pv);
}

/*
 * pmap_alloc_pvpage: maybe allocate a new pvpage
 *
 * if need_entry is false: try and allocate a new pv_page
 * if need_entry is true: try and allocate a new pv_page and return a
 *	new pv_entry from it.   if we are unable to allocate a pv_page
 *	we make a last ditch effort to steal a pv_page from some other
 *	mapping.    if that fails, we panic...
 *
 * => we assume that the caller holds pvalloc_lock
 */

struct pv_entry *
pmap_alloc_pvpage(pmap, mode)
	struct pmap *pmap;
	int mode;
{
	struct vm_page *pg;
	struct pv_page *pvpage;
	struct pv_entry *pv;
	int s;

	/*
	 * if we need_entry and we've got unused pv_pages, allocate from there
	 */

	pvpage = TAILQ_FIRST(&pv_unusedpgs);
	if (mode != ALLOCPV_NONEED && pvpage != NULL) {

		/* move it to pv_freepages list */
		TAILQ_REMOVE(&pv_unusedpgs, pvpage, pvinfo.pvpi_list);
		TAILQ_INSERT_HEAD(&pv_freepages, pvpage, pvinfo.pvpi_list);

		/* allocate a pv_entry */
		pvpage->pvinfo.pvpi_nfree--;	/* can't go to zero */
		pv = pvpage->pvinfo.pvpi_pvfree;
		KASSERT(pv);
		pvpage->pvinfo.pvpi_pvfree = pv->pv_next;
		pv_nfpvents--;  /* took one from pool */
		return(pv);
	}

	/*
	 *  see if we've got a cached unmapped VA that we can map a page in.
	 * if not, try to allocate one.
	 */

	s = splvm();	/* must protect kmem_map with splvm! */
	if (pv_cachedva == 0) {
		pv_cachedva = uvm_km_kmemalloc(kmem_map, uvmexp.kmem_object,
		    PAGE_SIZE, UVM_KMF_TRYLOCK|UVM_KMF_VALLOC);
		if (pv_cachedva == 0) {
			splx(s);
			return (NULL);
		}
	}

	/*
	 * we have a VA, now let's try and allocate a page.
	 */
	if (!simple_lock_try(&uvmexp.kmem_object->vmobjlock)) {
		splx(s);
		return (NULL);
	}

	pg = uvm_pagealloc(uvmexp.kmem_object, pv_cachedva -
	    vm_map_min(kernel_map), NULL, UVM_PGA_USERESERVE);
	if (pg)
		pg->flags &= ~PG_BUSY;	/* never busy */

	simple_unlock(&uvmexp.kmem_object->vmobjlock);
	splx(s);

	if (pg == NULL)
		return (NULL);

	/*
	 * add a mapping for our new pv_page and free its entrys (save one!)
	 *
	 * NOTE: If we are allocating a PV page for the kernel pmap, the
	 * pmap is already locked!  (...but entering the mapping is safe...)
	 */

	pmap_kenter_pa(pv_cachedva, VM_PAGE_TO_PHYS(pg),
	    VM_PROT_READ | VM_PROT_WRITE);
	pmap_update(pmap_kernel());
	pvpage = (struct pv_page *) pv_cachedva;
	pv_cachedva = 0;
	return (pmap_add_pvpage(pvpage, mode != ALLOCPV_NONEED));
}

/*
 * pmap_add_pvpage: add a pv_page's pv_entrys to the free list
 *
 * => caller must hold pvalloc_lock
 * => if need_entry is true, we allocate and return one pv_entry
 */

static struct pv_entry *
pmap_add_pvpage(pvp, need_entry)
	struct pv_page *pvp;
	boolean_t need_entry;
{
	int tofree, lcv;

	/* do we need to return one? */
	tofree = (need_entry) ? PVE_PER_PVPAGE - 1 : PVE_PER_PVPAGE;

	pvp->pvinfo.pvpi_pvfree = NULL;
	pvp->pvinfo.pvpi_nfree = tofree;
	for (lcv = 0 ; lcv < tofree ; lcv++) {
		pvp->pvents[lcv].pv_next = pvp->pvinfo.pvpi_pvfree;
		pvp->pvinfo.pvpi_pvfree = &pvp->pvents[lcv];
	}
	if (need_entry)
		TAILQ_INSERT_TAIL(&pv_freepages, pvp, pvinfo.pvpi_list);
	else
		TAILQ_INSERT_TAIL(&pv_unusedpgs, pvp, pvinfo.pvpi_list);
	pv_nfpvents += tofree;
	return((need_entry) ? &pvp->pvents[lcv] : NULL);
}

/*
 * pmap_free_pv_doit: actually free a pv_entry
 *
 * => do not call this directly!  instead use either
 *    1. pmap_free_pv ==> free a single pv_entry
 *    2. pmap_free_pvs => free a list of pv_entrys
 * => we must be holding pvalloc_lock
 */

__inline static void
pmap_free_pv_doit(pv)
	struct pv_entry *pv;
{
	struct pv_page *pvp;

	pvp = (struct pv_page *) x86_trunc_page(pv);
	pv_nfpvents++;
	pvp->pvinfo.pvpi_nfree++;

	/* nfree == 1 => fully allocated page just became partly allocated */
	if (pvp->pvinfo.pvpi_nfree == 1) {
		TAILQ_INSERT_HEAD(&pv_freepages, pvp, pvinfo.pvpi_list);
	}

	/* free it */
	pv->pv_next = pvp->pvinfo.pvpi_pvfree;
	pvp->pvinfo.pvpi_pvfree = pv;

	/*
	 * are all pv_page's pv_entry's free?  move it to unused queue.
	 */

	if (pvp->pvinfo.pvpi_nfree == PVE_PER_PVPAGE) {
		TAILQ_REMOVE(&pv_freepages, pvp, pvinfo.pvpi_list);
		TAILQ_INSERT_HEAD(&pv_unusedpgs, pvp, pvinfo.pvpi_list);
	}
}

/*
 * pmap_free_pv: free a single pv_entry
 *
 * => we gain the pvalloc_lock
 */

__inline static void
pmap_free_pv(pmap, pv)
	struct pmap *pmap;
	struct pv_entry *pv;
{
	simple_lock(&pvalloc_lock);
	pmap_free_pv_doit(pv);

	/*
	 * Can't free the PV page if the PV entries were associated with
	 * the kernel pmap; the pmap is already locked.
	 */
	if (pv_nfpvents > PVE_HIWAT && TAILQ_FIRST(&pv_unusedpgs) != NULL &&
	    pmap != pmap_kernel())
		pmap_free_pvpage();

	simple_unlock(&pvalloc_lock);
}

/*
 * pmap_free_pvs: free a list of pv_entrys
 *
 * => we gain the pvalloc_lock
 */

__inline static void
pmap_free_pvs(pmap, pvs)
	struct pmap *pmap;
	struct pv_entry *pvs;
{
	struct pv_entry *nextpv;

	simple_lock(&pvalloc_lock);

	for ( /* null */ ; pvs != NULL ; pvs = nextpv) {
		nextpv = pvs->pv_next;
		pmap_free_pv_doit(pvs);
	}

	/*
	 * Can't free the PV page if the PV entries were associated with
	 * the kernel pmap; the pmap is already locked.
	 */
	if (pv_nfpvents > PVE_HIWAT && TAILQ_FIRST(&pv_unusedpgs) != NULL &&
	    pmap != pmap_kernel())
		pmap_free_pvpage();

	simple_unlock(&pvalloc_lock);
}


/*
 * pmap_free_pvpage: try and free an unused pv_page structure
 *
 * => assume caller is holding the pvalloc_lock and that
 *	there is a page on the pv_unusedpgs list
 * => if we can't get a lock on the kmem_map we try again later
 */

void
pmap_free_pvpage()
{
	int s;
	struct vm_map *map;
	struct vm_map_entry *dead_entries;
	struct pv_page *pvp;

	s = splvm(); /* protect kmem_map */

	pvp = TAILQ_FIRST(&pv_unusedpgs);

	/*
	 * note: watch out for pv_initpage which is allocated out of
	 * kernel_map rather than kmem_map.
	 */

	if (pvp == pv_initpage)
		map = kernel_map;
	else
		map = kmem_map;
	if (vm_map_lock_try(map)) {

		/* remove pvp from pv_unusedpgs */
		TAILQ_REMOVE(&pv_unusedpgs, pvp, pvinfo.pvpi_list);

		/* unmap the page */
		dead_entries = NULL;
		uvm_unmap_remove(map, (vaddr_t)pvp, ((vaddr_t)pvp) + PAGE_SIZE,
		    &dead_entries, NULL);
		vm_map_unlock(map);

		if (dead_entries != NULL)
			uvm_unmap_detach(dead_entries, 0);

		pv_nfpvents -= PVE_PER_PVPAGE;  /* update free count */
	}
	if (pvp == pv_initpage)
		/* no more initpage, we've freed it */
		pv_initpage = NULL;

	splx(s);
}

/*
d1047 1
a1047 1
__inline static void
d1074 1
a1074 1
__inline static struct pv_entry *
d1099 1
a1099 1
static __inline struct vm_page *
d1119 1
a1119 1
static __inline void
d1700 1
a1700 20
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(zerop, id);

#ifdef DIAGNOSTIC
	if (*zpte)
		panic("pmap_zero_page: lock botch");
#endif

	*zpte = (pa & PG_FRAME) | PG_V | PG_RW;		/* map in */
	pmap_update_pg((vaddr_t)zerova);		/* flush TLB */

	memset(zerova, 0, PAGE_SIZE);			/* zero */
#ifdef DIAGNOSTIC
	*zpte = 0;					/* zap! */
#endif
d1712 1
a1712 6
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *zpte = PTESLEW(zero_pte, id);
	caddr_t zerova = VASLEW(zerop, id);
d1714 2
a1715 2
	int *ptr;
	unsigned int i;
d1717 7
a1723 7
#ifdef DIAGNOSTIC
	if (*zpte)
		panic("pmap_zero_page_uncached: lock botch");
#endif
	*zpte = (pa & PG_FRAME) | PG_V | PG_RW | PG_N;	/* map in */
	pmap_update_pg((vaddr_t)zerova);		/* flush TLB */
	for (i = 0, ptr = (int *) zerova; i < PAGE_SIZE / sizeof(int); i++) {
a1738 3
#ifdef DIAGNOSTIC
	*zpte = 0;					/* zap! */
#endif
d1749 2
a1750 14
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
#ifdef MULTIPROCESSOR
	int id = cpu_number();
#endif
	pt_entry_t *spte = PTESLEW(csrc_pte,id);
	pt_entry_t *dpte = PTESLEW(cdst_pte,id);
	caddr_t csrcva = VASLEW(csrcp, id);
	caddr_t cdstva = VASLEW(cdstp, id);

#ifdef DIAGNOSTIC
	if (*spte || *dpte)
		panic("pmap_copy_page: lock botch");
#endif
d1752 1
a1752 7
	*spte = (srcpa & PG_FRAME) | PG_V | PG_RW;
	*dpte = (dstpa & PG_FRAME) | PG_V | PG_RW;
	pmap_update_2pg((vaddr_t)csrcva, (vaddr_t)cdstva);
	memcpy(cdstva, csrcva, PAGE_SIZE);
#ifdef DIAGNOSTIC
	*spte = *dpte = 0;			/* zap! */
#endif
a1778 1
	struct pv_entry *pv_tofree = NULL;	/* list of pv_entrys to free */
d1843 1
a1843 2
			pve->pv_next = pv_tofree;
			pv_tofree = pve;
a1847 2
	if (pv_tofree)
		pmap_free_pvs(pmap, pv_tofree);
d1921 1
a1921 1
		pmap_free_pv(pmap, pve);
d2083 1
a2083 1
	struct pv_entry *pve, *npve, **prevptr, *killlist = NULL;
d2150 1
a2150 2
		pve->pv_next = killlist;		/* mark it for death */
		killlist = pve;
d2152 1
a2152 1
	pmap_free_pvs(NULL, killlist);
d2620 1
a2620 1
			pve = pmap_alloc_pv(pmap, ALLOCPV_NEED);
d2636 1
a2636 1
			pmap_free_pv(pmap, pve);
d2706 1
a2706 3
		*early_zero_pte = (*paddrp & PG_FRAME) | PG_V | PG_RW;
		pmap_update_pg((vaddr_t)early_zerop);
		memset(early_zerop, 0, PAGE_SIZE);
@


1.7
log
@Change a printf+Debugger pair into a panic.
Debugger calls in the middle of the code are evil because they don't
respect the ddb.panic sysctl and might leave critical machines hung instead
of quickly rebooting them.

requested by tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 2004/07/22 11:19:14 art Exp $	*/
d1589 1
a1589 1
		    &dead_entries);
@


1.6
log
@Use mutex instead of SIMPLELOCK for locking the tlb shootdown queues
and related structures.

tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 2004/07/19 15:09:05 art Exp $	*/
d2523 2
a2524 3
			printf("pmap_remove_pte: managed page without "
			      "PG_PVLIST for 0x%lx\n", va);
			Debugger();
@


1.5
log
@Implement __HAVE_PMAP_DIRECT on amd64 using large pages. At this moment
it's limited to 512GB (one L4 page table entry) physical memory. Only
used carefully at this moment, but more improvements are in the pipeline.

tested by many, deraadt@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 2004/06/25 11:03:27 art Exp $	*/
d118 1
d354 1
a354 1
	struct SIMPLELOCK pq_slock;	/* spin lock on queue */
d367 1
a367 1
struct SIMPLELOCK pmap_tlb_shootdown_job_lock;
a641 1
	int s;
d658 1
a658 4
			s = splipi();
#ifdef MULTIPROCESSOR
			SIMPLE_LOCK(&pq->pq_slock);
#endif
d660 1
a660 4
#ifdef MULTIPROCESSOR
			SIMPLE_UNLOCK(&pq->pq_slock);
#endif
			splx(s);
a1113 2
	SIMPLE_LOCK_INIT(&pmap_tlb_shootdown_job_lock);

d1116 1
a1116 1
		SIMPLE_LOCK_INIT(&pmap_tlb_shootdown_q[i].pq_slock);
a3594 1
	int s;
a3605 1
	s = splipi();
d3607 1
a3607 1
	printf("dshootdown %lx\n", va);
d3617 1
a3617 3
#if defined(MULTIPROCESSOR)
		SIMPLE_LOCK(&pq->pq_slock);
#endif
d3626 1
a3626 3
#if defined(MULTIPROCESSOR)
			SIMPLE_UNLOCK(&pq->pq_slock);
#endif
a3629 17
#ifdef I386_CPU
		/*
		 * i386 CPUs can't invalidate a single VA, only
		 * flush the entire TLB, so don't bother allocating
		 * jobs for them -- just queue a `flushu'.
		 *
		 * XXX note that this can be executed for non-i386
		 * when called * early (before identifycpu() has set
		 * cpu_class)
		 */
		if (cpu_class == CPUCLASS_386) {
			pq->pq_flushu++;
			*cpumaskp |= 1U << ci->ci_cpuid;
			continue;
		}
#endif

a3640 4
#if defined(MULTIPROCESSOR)
				SIMPLE_LOCK(&pq->pq_slock);
#endif
				continue;
d3661 1
a3661 3
#if defined(MULTIPROCESSOR)
		SIMPLE_UNLOCK(&pq->pq_slock);
#endif
a3662 1
	splx(s);
a3675 1
	int s;
d3681 1
a3681 5
	s = splipi();

#ifdef MULTIPROCESSOR
	SIMPLE_LOCK(&pq->pq_slock);
#endif
a3714 1
	SIMPLE_UNLOCK(&pq->pq_slock);
d3716 1
a3716 2

	splx(s);
d3751 1
a3751 2
pmap_tlb_shootdown_job_get(pq)
	struct pmap_tlb_shootdown_q *pq;
d3758 2
a3759 3
#ifdef MULTIPROCESSOR
	SIMPLE_LOCK(&pmap_tlb_shootdown_job_lock);
#endif
d3761 1
a3761 3
#ifdef MULTIPROCESSOR
		SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
#endif
d3767 2
a3768 3
#ifdef MULTIPROCESSOR
	SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
#endif
d3782 2
a3783 3
pmap_tlb_shootdown_job_put(pq, pj)
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
d3790 1
a3790 3
#ifdef MULTIPROCESSOR
	SIMPLE_LOCK(&pmap_tlb_shootdown_job_lock);
#endif
d3793 1
a3793 3
#ifdef MULTIPROCESSOR
	SIMPLE_UNLOCK(&pmap_tlb_shootdown_job_lock);
#endif
@


1.4
log
@SMP support. Big parts from NetBSD, but with some really serious debugging
done by me, niklas and others. Especially wrt. NXE support.

Still needs some polishing, especially in dmesg messages, but we're now
building kernel faster than ever.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 2004/02/23 08:32:36 mickey Exp $	*/
d286 5
a381 9
#ifdef LARGEPAGES
/*
 * pmap_largepages: if our processor supports PG_PS and we are
 * using it, this is set to TRUE.
 */

int pmap_largepages;
#endif

d859 1
a859 2
pmap_bootstrap(kva_start)
	vaddr_t kva_start;
d867 2
d927 1
a927 1
	 * enable global TLB entries if they are supported
d929 3
d933 1
a933 5
	if (cpu_feature & CPUID_PGE) {
		lcr4(rcr4() | CR4_PGE);	/* enable hardware (via %cr4) */
		pmap_pg_g = PG_G;		/* enable software */

		/* add PG_G attribute to already mapped kernel pages */
d935 1
a935 1
		for (kva = VM_MIN_KERNEL_ADDRESS ; kva < virtual_avail ;
d937 2
a938 2
		kva_end = roundup((vaddr_t)&end, PAGE_SIZE);
		for (kva = KERNBASE; kva < kva_end ;
d940 4
a943 5
		     kva += PAGE_SIZE) {
			p1i = pl1_i(kva);
			if (pmap_valid_entry(PTE_BASE[p1i]))
				PTE_BASE[p1i] |= PG_G;
		}
a944 39

#if defined(LARGEPAGES) && 0	/* XXX non-functional right now */
	/*
	 * enable large pages of they are supported.
	 */

	if (cpu_feature & CPUID_PSE) {
		paddr_t pa;
		pd_entry_t *pde;
		extern char _etext;

		lcr4(rcr4() | CR4_PSE);	/* enable hardware (via %cr4) */
		pmap_largepages = 1;	/* enable software */

		/*
		 * the TLB must be flushed after enabling large pages
		 * on Pentium CPUs, according to section 3.6.2.2 of
		 * "Intel Architecture Software Developer's Manual,
		 * Volume 3: System Programming".
		 */
		tlbflush();

		/*
		 * now, remap the kernel text using large pages.  we
		 * assume that the linker has properly aligned the
		 * .data segment to a 4MB boundary.
		 */
		kva_end = roundup((vaddr_t)&_etext, NBPD);
		for (pa = 0, kva = KERNBASE; kva < kva_end;
		     kva += NBPD, pa += NBPD) {
			pde = &kpm->pm_pdir[pdei(kva)];
			*pde = pa | pmap_pg_g | PG_PS |
			    PG_KR | PG_V;	/* zap! */
			tlbflush();
		}
	}
#endif /* LARGEPAGES */

#if VM_MIN_KERNEL_ADDRESS != KERNBASE
d955 53
a1007 1
#endif
d1877 1
a1877 1
	/* put in recursibve PDE to map the PTEs */
d1890 2
d2205 6
a2212 1
		pmap_unmap_ptes(pmap);
a2214 2
	pte = ptes[pl1_i(va)];
	pmap_unmap_ptes(pmap);
a2215 1
#ifdef LARGEPAGES
d2219 1
a2221 1
#endif
d2223 2
@


1.3
log
@get use of NX; partially from netbsd; passes the regress; deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 2004/02/09 22:41:14 mickey Exp $	*/
d291 1
a291 1
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
d348 1
a348 1
	struct simplelock pq_slock;	/* spin lock on queue */
d361 1
a361 1
struct simplelock pmap_tlb_shootdown_job_lock;
d664 1
a664 1
			__cpu_simple_lock(&pq->pq_slock);
d668 1
a668 1
			__cpu_simple_unlock(&pq->pq_slock);
d1092 1
a1092 1
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
d1112 1
a1112 1
	simple_lock_init(&pmap_tlb_shootdown_job_lock);
d1116 1
a1116 1
		simple_lock_init(&pmap_tlb_shootdown_q[i].pq_slock);
d3563 1
a3563 1
		if (count++ > 10000000)
d3614 1
a3614 1
		simple_lock(&pq->pq_slock);
d3625 1
a3625 1
			simple_unlock(&pq->pq_slock);
d3659 1
a3659 1
				simple_unlock(&pq->pq_slock);
d3683 1
a3683 1
		simple_unlock(&pq->pq_slock);
d3709 1
a3709 1
	simple_lock(&pq->pq_slock);
d3744 1
a3744 1
	simple_unlock(&pq->pq_slock);
d3791 1
a3791 1
	simple_lock(&pmap_tlb_shootdown_job_lock);
d3795 1
a3795 1
		simple_unlock(&pmap_tlb_shootdown_job_lock);
d3803 1
a3803 1
	simple_unlock(&pmap_tlb_shootdown_job_lock);
d3828 1
a3828 1
	simple_lock(&pmap_tlb_shootdown_job_lock);
d3833 1
a3833 1
	simple_unlock(&pmap_tlb_shootdown_job_lock);
@


1.2
log
@remove debugging printf
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.1 2004/01/28 01:39:39 mickey Exp $	*/
d782 2
d871 1
d886 1
a886 1
	protection_codes[VM_PROT_NONE] = 0;  			/* --- */
d888 1
a888 1
	protection_codes[VM_PROT_READ] = PG_RO;			/* -r- */
d890 1
a890 1
	protection_codes[VM_PROT_WRITE] = PG_RW;		/* w-- */
d892 2
a893 1
	protection_codes[VM_PROT_WRITE|VM_PROT_READ] = PG_RW;	/* wr- */
d2206 1
a2206 1
			*pap = (pde & PG_LGFRAME) | (va & ~PG_LGFRAME);
d2214 1
a2214 1
			*pap = (pte & PG_FRAME) | (va & ~PG_FRAME);
d2944 1
a2944 1
	pt_entry_t *ptes, *spte, *epte;
d2955 4
d2983 1
a2983 2
		if (sva >= VM_MAXUSER_ADDRESS &&
		    sva < VM_MAX_ADDRESS)
d2991 6
a2996 2
			if ((*spte & (PG_RW|PG_V)) == (PG_RW|PG_V)) {
				pmap_pte_clearbits(spte, PG_RW);
a2998 1
			}
@


1.2.2.1
log
@The merge of these files were done to another date than the rest, fix.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2305 1
printf("pmap_pageidlezero()\n");
@


1.2.2.2
log
@Merge with the trunk
@
text
@a781 2
	if ((cpu_feature & CPUID_NXE) && !(prot & VM_PROT_EXECUTE))
		npte |= PG_NX;
a868 1
	pt_entry_t pg_nx = (cpu_feature & CPUID_NXE? PG_NX : 0);
d883 1
a883 1
	protection_codes[VM_PROT_NONE] = pg_nx;			/* --- */
d885 1
a885 1
	protection_codes[VM_PROT_READ] = PG_RO | pg_nx;		/* -r- */
d887 1
a887 1
	protection_codes[VM_PROT_WRITE] = PG_RW | pg_nx;	/* w-- */
d889 1
a889 2
	protection_codes[VM_PROT_WRITE|VM_PROT_READ] = PG_RW | pg_nx;
								/* wr- */
d2202 1
a2202 1
			*pap = (pde & PG_LGFRAME) | (va & 0x1fffff);
d2210 1
a2210 1
			*pap = (pte & PG_FRAME) | (va & 0xfff);
d2306 1
d2941 1
a2941 1
	pt_entry_t nx, opte, *ptes, *spte, *epte;
a2951 4
	nx = 0;
	if ((cpu_feature & CPUID_NXE) && !(prot & VM_PROT_EXECUTE))
		nx = PG_NX;

d2976 2
a2977 1
		if (sva >= VM_MAXUSER_ADDRESS && sva < VM_MAX_ADDRESS)
d2985 2
a2986 6
			if (!(*spte & PG_V))
				continue;
			opte = *spte;
			pmap_pte_clearbits(spte, PG_RW);
			pmap_pte_setbits(spte, nx);
			if (opte != *spte)
d2989 1
@


1.1
log
@an amd64 arch support.
hacked by art@@ from netbsd sources and then later debugged
by me into the shape where it can host itself.
no bootloader yet as needs redoing from the
recent advanced i386 sources (anyone? ;)
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a2305 1
printf("pmap_pageidlezero()\n");
@

