head	1.18;
access;
symbols
	SMP_SYNC_A:1.18
	SMP_SYNC_B:1.18
	UBC_SYNC_A:1.18
	UBC_SYNC_B:1.18
	OPENBSD_2_9:1.13.0.4
	OPENBSD_2_9_BASE:1.13
	OPENBSD_2_8:1.13.0.2
	OPENBSD_2_8_BASE:1.13
	OPENBSD_2_7:1.11.0.6
	OPENBSD_2_7_BASE:1.11
	SMP:1.11.0.4
	SMP_BASE:1.11
	kame_19991208:1.11
	OPENBSD_2_6:1.11.0.2
	OPENBSD_2_6_BASE:1.11
	OPENBSD_2_5:1.9.0.2
	OPENBSD_2_5_BASE:1.9
	OPENBSD_2_4:1.8.0.2
	OPENBSD_2_4_BASE:1.8
	OPENBSD_2_3:1.4.0.4
	OPENBSD_2_3_BASE:1.4
	OPENBSD_2_2:1.4.0.2
	OPENBSD_2_2_BASE:1.4
	OPENBSD_2_1:1.3.0.2
	OPENBSD_2_1_BASE:1.3
	OPENBSD_2_0:1.2.0.2
	OPENBSD_2_0_BASE:1.2
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.18
date	2001.06.24.23.48.48;	author deraadt;	state dead;
branches;
next	1.17;

1.17
date	2001.06.08.08.09.18;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.05.06.00.45.54;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.05.05.21.26.40;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.05.05.20.56.49;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2000.06.08.22.25.22;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	2000.06.05.11.03.03;	author art;	state Exp;
branches;
next	1.11;

1.11
date	99.09.03.18.01.44;	author art;	state Exp;
branches
	1.11.4.1;
next	1.10;

1.10
date	99.08.17.10.32.17;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	99.01.11.05.11.52;	author millert;	state Exp;
branches;
next	1.8;

1.8
date	98.10.15.21.30.17;	author imp;	state Exp;
branches;
next	1.7;

1.7
date	98.09.15.10.53.55;	author pefo;	state Exp;
branches;
next	1.6;

1.6
date	98.07.28.00.13.50;	author millert;	state Exp;
branches;
next	1.5;

1.5
date	98.05.18.00.28.32;	author millert;	state Exp;
branches;
next	1.4;

1.4
date	97.06.10.14.20.14;	author graichen;	state Exp;
branches;
next	1.3;

1.3
date	96.12.22.15.18.41;	author graichen;	state Exp;
branches;
next	1.2;

1.2
date	96.09.15.21.13.19;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.51.34;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.51.34;	author deraadt;	state Exp;
branches;
next	;

1.11.4.1
date	2001.04.18.16.12.50;	author niklas;	state Exp;
branches;
next	1.11.4.2;

1.11.4.2
date	2001.07.04.10.21.56;	author niklas;	state dead;
branches;
next	;


desc
@@


1.18
log
@bye bye, with prejudice
@
text
@/*	$NetBSD: vm_machdep.c,v 1.15 1997/05/25 10:16:17 jonathan Exp $	*/

/*
 * Copyright (c) 1988 University of Utah.
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department and Ralph Campbell.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * from: Utah Hdr: vm_machdep.c 1.21 91/04/06
 *
 *	@@(#)vm_machdep.c	8.3 (Berkeley) 1/4/94
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/signalvar.h>
#include <sys/malloc.h>
#include <sys/buf.h>
#include <sys/vnode.h>
#include <sys/user.h>
#include <sys/core.h>
#include <sys/exec.h>

#include <vm/vm.h>
#include <vm/vm_kern.h>
#include <vm/vm_page.h>

#include <machine/pte.h>
#include <machine/vmparam.h>
#include <machine/locore.h>
#include <machine/machConst.h>

#include <machine/locore.h>

extern int  copykstack __P((struct user *up));
extern void MachSaveCurFPState __P((struct proc *p));
extern int switch_exit __P((void)); /* XXX never returns? */

extern vm_offset_t kvtophys __P((vm_offset_t kva));	/* XXX */

/*
 * Finish a fork operation, with process p2 nearly set up.
 * Copy and update the kernel stack and pcb, making the child
 * ready to run, and marking it so that it can return differently
 * than the parent.  Returns 1 in the child process, 0 in the parent.
 * We currently double-map the user area so that the stack is at the same
 * address in each process; in the future we will probably relocate
 * the frame pointers on the stack after copying.
 */
int
cpu_fork(p1, p2, stack, stacksize)
	register struct proc *p1, *p2;
	void *stack;
	size_t stacksize;
{
	register struct user *up = p2->p_addr;
	register pt_entry_t *pte;
	register int i;
	extern struct proc *machFPCurProcPtr;

	p2->p_md.md_regs = up->u_pcb.pcb_regs;
	p2->p_md.md_flags = p1->p_md.md_flags & MDP_FPUSED;

	/*
	 * Cache the PTEs for the user area in the machine dependent
	 * part of the proc struct so cpu_switch() can quickly map in
	 * the user struct and kernel stack. Note: if the virtual address
	 * translation changes (e.g. swapout) we have to update this.
	 */
	pte = kvtopte(up);
	for (i = 0; i < UPAGES; i++) {
		p2->p_md.md_upte[i] = pte->pt_entry & ~PG_G;
		pte++;
	}

	/*
	 * Copy floating point state from the FP chip if this process
	 * has state stored there.
	 */
	if (p1 == machFPCurProcPtr)
		MachSaveCurFPState(p1);

	/*
	 * Copy pcb and stack from proc p1 to p2. 
	 * We do this as cheaply as possible, copying only the active
	 * part of the stack.  The stack and pcb need to agree;
	 */
	p2->p_addr->u_pcb = p1->p_addr->u_pcb;
	/* cache segtab for ULTBMiss() */
	p2->p_addr->u_pcb.pcb_segtab = (void *)p2->p_vmspace->vm_map.pmap->pm_segtab;

#ifdef notyet
	/*
	 * If specified, give the child a different stack.
	 */
	if (stack != NULL)
		/* XXX How??? */;
#endif

	/*
	 * Arrange for a non-local goto when the new process
	 * is started, to resume here, returning nonzero from setjmp.
	 */
#ifdef DIAGNOSTIC
	if (p1 != curproc)
		panic("cpu_fork: curproc");
#endif
	if (copykstack(up)) {
		/*
		 * Return 1 in child.
		 */
		return (1);
	}
	return (0);
}

/*
 * Finish a swapin operation.
 * We neded to update the cached PTEs for the user area in the
 * machine dependent part of the proc structure.
 */
void
cpu_swapin(p)
	register struct proc *p;
{
	register struct user *up = p->p_addr;
	register pt_entry_t *pte;
	register int i;

	/*
	 * Cache the PTEs for the user area in the machine dependent
	 * part of the proc struct so cpu_switch() can quickly map in
	 * the user struct and kernel stack.
	 */
	pte = kvtopte(up);
	for (i = 0; i < UPAGES; i++) {
		p->p_md.md_upte[i] = pte->pt_entry & ~PG_G;
		pte++;
	}
}

/*
 * cpu_exit is called as the last action during exit.
 * We release the address space and machine-dependent resources,
 * including the memory for the user structure and kernel stack.
 * Once finished, we call switch_exit, which switches to a temporary
 * pcb and stack and never returns.  We block memory allocation
 * until switch_exit has made things safe again.
 */
void cpu_exit(p)
	struct proc *p;
{
	extern struct proc *machFPCurProcPtr;

	if (machFPCurProcPtr == p)
		machFPCurProcPtr = (struct proc *)0;

	(void) splhigh();
	exit2(p);		/* XXX - probably very wrong. */
	switch_exit();
	/* NOTREACHED */
}

/*
 * Dump the machine specific segment at the start of a core dump.
 */     
int
cpu_coredump(p, vp, cred, chdr)
	struct proc *p;
	struct vnode *vp;
	struct ucred *cred;
	struct core *chdr;
{
	int error;
	/*register struct user *up = p->p_addr;*/
	struct coreseg cseg;
	extern struct proc *machFPCurProcPtr;

	CORE_SETMAGIC(*chdr, COREMAGIC, MID_MIPS, 0);
	chdr->c_hdrsize = ALIGN(sizeof(*chdr));
	chdr->c_seghdrsize = ALIGN(sizeof(cseg));
	chdr->c_cpusize = sizeof (p -> p_addr -> u_pcb.pcb_regs);

	/*
	 * Copy floating point state from the FP chip if this process
	 * has state stored there.
	 */
	if (p == machFPCurProcPtr)
		MachSaveCurFPState(p);

	CORE_SETMAGIC(cseg, CORESEGMAGIC, MID_MIPS, CORE_CPU);
	cseg.c_addr = 0;
	cseg.c_size = chdr->c_cpusize;

	error = vn_rdwr(UIO_WRITE, vp, (caddr_t)&cseg, chdr->c_seghdrsize,
	    (off_t)chdr->c_hdrsize, UIO_SYSSPACE,
	    IO_NODELOCKED|IO_UNIT, cred, NULL, p);
	if (error)
		return error;

	error = vn_rdwr(UIO_WRITE, vp,
			(caddr_t)(&(p -> p_addr -> u_pcb.pcb_regs)),
			(off_t)chdr -> c_cpusize,
	    		(off_t)(chdr->c_hdrsize + chdr->c_seghdrsize),
			UIO_SYSSPACE, IO_NODELOCKED|IO_UNIT, cred, NULL, p);

	if (!error)
		chdr->c_nseg++;

	return error;
}

/*
 * Move pages from one kernel virtual address to another.
 * Both addresses are assumed to reside in the Sysmap.
 */
void
pagemove(from, to, size)
	register caddr_t from, to;
	size_t size;
{
	register pt_entry_t *fpte, *tpte;

#ifdef DIAGNOSTIC
	if ((size & PAGE_MASK) != 0)
		panic("pagemove");
#endif
	fpte = kvtopte(from);
	tpte = kvtopte(to);
	while (size > 0) {
		MachTLBFlushAddr((vm_offset_t)from);
		MachTLBUpdate( (u_int)to,
			       (u_int) (*fpte).pt_entry);    /* XXX casts? */
		*tpte++ = *fpte;
		fpte->pt_entry = 0;
		fpte++;
		size -= NBPG;
		from += NBPG;
		to += NBPG;
	}
}

extern vm_map_t phys_map;

/*
 * Map an IO request into kernel virtual address space.  Requests fall into
 * one of five catagories:
 *
 *	B_PHYS|B_UAREA:	User u-area swap.
 *			Address is relative to start of u-area (p_addr).
 *	B_PHYS|B_PAGET:	User page table swap.
 *			Address is a kernel VA in usrpt (Usrptmap).
 *	B_PHYS|B_DIRTY:	Dirty page push.
 *			Address is a VA in proc2's address space.
 *	B_PHYS|B_PGIN:	Kernel pagein of user pages.
 *			Address is VA in user's address space.
 *	B_PHYS:		User "raw" IO request.
 *			Address is VA in user's address space.
 *
 * All requests are (re)mapped into kernel VA space via the phys_map
 */
/*ARGSUSED*/
void
vmapbuf(bp, len)
	register struct buf *bp;
	vm_size_t len;
{
	register vm_offset_t faddr, taddr, off;
	vm_offset_t pa;
	struct proc *p;

	if ((bp->b_flags & B_PHYS) == 0)
		panic("vmapbuf");
	p = bp->b_proc;
	faddr = trunc_page((vaddr_t)(bp->b_saveaddr = bp->b_data));
	off = (vm_offset_t)bp->b_data - faddr;
	len = round_page(off + len);
	taddr = kmem_alloc_wait(phys_map, len);
	bp->b_data = (caddr_t) (taddr + off);
	len = atop(len);
	while (len--) {
		if (pmap_extract(vm_map_pmap(&p->p_vmspace->vm_map), faddr,
		    &pa) == FALSE)
			panic("vmapbuf: null page frame");
		pmap_enter(vm_map_pmap(phys_map), taddr, trunc_page(pa),
			VM_PROT_READ|VM_PROT_WRITE, TRUE, 0);
		faddr += PAGE_SIZE;
		taddr += PAGE_SIZE;
	}
}

/*
 * Free the io map PTEs associated with this IO operation.
 * We also invalidate the TLB entries and restore the original b_addr.
 */
/*ARGSUSED*/
void
vunmapbuf(bp, len)
	register struct buf *bp;
	vm_size_t len;
{
	register vm_offset_t addr, off;

	if ((bp->b_flags & B_PHYS) == 0)
		panic("vunmapbuf");
	addr = trunc_page((vaddr_t)bp->b_data);
	off = (vm_offset_t)bp->b_data - addr;
	len = round_page(off + len);
	kmem_free_wakeup(phys_map, addr, len);
	bp->b_data = bp->b_saveaddr;
	bp->b_saveaddr = NULL;
}

/*XXX*/

/*
 * Map a (kernel) virtual address to a physical address.
 * There are four cases: 
 * A kseg0 kernel "virtual address" for the   cached physical address space;
 * A kseg1 kernel "virtual address" for the uncached physical address space;
 * A kseg2 normal kernel "virtual address" for the kernel stack or
 *   "u area".  These ARE NOT necessarily in sysmap, since processes 0
 *    and 1 are handcrafted before the sysmap is set up.
 * A kseg2 normal kernel "virtual address" mapped via the TLB, which
 *   IS NOT in Sysmap (eg., an mbuf).
 * The first two are so cheap they could just be macros. The last two
 * overlap, so we must check for UADDR pages first.
 *
 * XXX the double-mapped u-area holding the current process's kernel stack
 * and u-area at a fixed address should be fixed.
 */
vm_offset_t
kvtophys(vm_offset_t kva)
{
	pt_entry_t *pte;
	vm_offset_t phys;

        if (kva >= MIPS_KSEG0_START && kva < MIPS_KSEG1_START)
	{
		return (MIPS_KSEG0_TO_PHYS(kva));
	}
	else if (kva >= MIPS_KSEG1_START && kva < MIPS_KSEG2_START) {
		return (MIPS_KSEG1_TO_PHYS(kva));
	}
	else if (kva >= UADDR && kva < KERNELSTACK) {
		int upage = (kva - UADDR) >> PGSHIFT;

		pte = (pt_entry_t *)&curproc->p_md.md_upte[upage];
		phys = (pte->pt_entry & PG_FRAME) |
			(kva & PGOFSET);
	}
	else if (kva >= MIPS_KSEG2_START /*&& kva < VM_MAX_KERNEL_ADDRESS*/) {
		pte = kvtopte(kva);

		if ((pte - Sysmap) > Sysmapsize)  {
			printf("oops: Sysmap overrun, max %d index %d\n",
			       Sysmapsize, pte - Sysmap);
		}
		if ((pte->pt_entry & PG_V) == 0) {
			printf("kvtophys: pte not valid for %lx\n", kva);
		}
		phys = (pte->pt_entry & PG_FRAME) |
			(kva & PGOFSET);
#ifdef DEBUG_VIRTUAL_TO_PHYSICAL
		printf("kvtophys: kv %p, phys %x", kva, phys);
#endif
	}
	else {
		printf("Virtual address %lx: cannot map to physical\n",
		       kva);
                phys = 0;
		/*panic("non-kernel address to kvtophys");*/
		return(kva); /* XXX -- while debugging ASC */
        }
        return(phys);
}
@


1.17
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@@


1.16
log
@Update some comments wrt. the CLSIZE changes.
And remove that memory price comment from 1981. It is amusing, but also
confusing because the math in there is only correct on vax.
@
text
@d300 2
a301 1
	register vm_offset_t faddr, taddr, off, pa;
d314 2
a315 2
		pa = pmap_extract(vm_map_pmap(&p->p_vmspace->vm_map), faddr);
		if (pa == 0)
@


1.15
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d247 1
a247 2
 * Both addresses are assumed to reside in the Sysmap,
 * and size must be a multiple of CLSIZE.
@


1.14
log
@Get rid of CLSIZE and all related stuff.
CLSIZE -> 1
CLBYTES -> PAGE_SIZE
OLOFSET -> PAGE_MASK
etc.
At the same time some archs needed some cleaning in vmparam.h so that
goes in at the same time.
@
text
@d307 1
a307 1
	faddr = trunc_page(bp->b_saveaddr = bp->b_data);
d338 1
a338 1
	addr = trunc_page(bp->b_data);
@


1.13
log
@Add explicit inclusions of signalvar.h to files actually using syms defined
there but relying on an indirect inclusion
@
text
@d257 2
a258 1
	if (size % CLBYTES)
d260 1
@


1.12
log
@Changes to exit handling.

cpu_exit no longer frees the vmspace and u-area. This is now handled by a
separate kernel thread "reaper". This is to avoid sleeping locks in the
critical path of cpu_exit where we're not allowed to sleep.

From NetBSD
@
text
@d48 1
@


1.11
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@a188 2
	vmspace_free(p->p_vmspace);

d190 1
a190 1
	kmem_free(kernel_map, (vm_offset_t)p->p_addr, ctob(UPAGES));
@


1.11.4.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@a47 1
#include <sys/signalvar.h>
d189 2
d192 1
a192 1
	exit2(p);		/* XXX - probably very wrong. */
@


1.11.4.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@@


1.10
log
@New cpu_fork API to take a stack in which you point the child's stackpointer
to, at the bottom or the top, depending on your architecture's stack growth
direction.  This is in preparation for Linux' clone(2) emulation.
port maintainers, please check that I did the work right.
@
text
@d317 1
a317 1
			VM_PROT_READ|VM_PROT_WRITE, TRUE);
@


1.9
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d82 1
a82 1
cpu_fork(p1, p2)
d84 2
d122 8
@


1.8
log
@
Back out PEFO's trap change.  It breaks everybody, it seems.
@
text
@d393 1
a393 1
		/*panic("non-kernel address to kvtophys\n");*/
@


1.7
log
@Real trapframe and no more __FORK_BRAINDAMAGE
@
text
@a71 3
extern void child_return __P((struct proc *));
extern void proc_trampoline __P((void));

d74 6
d81 1
a81 1
void
d83 1
a83 1
	struct proc *p1, *p2;
d85 3
a87 3
	pt_entry_t *pte;
	struct pcb *pcb;
	int i;
d90 1
a90 1
	p2->p_md.md_regs = &p2->p_addr->u_pcb.pcb_regs;
d93 7
a99 1
	pte = kvtopte(p2->p_addr);
d105 4
d112 5
a116 1
	pcb = &p2->p_addr->u_pcb;
d118 2
d121 15
a135 14
	pcb->pcb_segtab = (void *)p2->p_vmspace->vm_map.pmap->pm_segtab;

	pcb->pcb_context.val[10] = (int)proc_trampoline;
	pcb->pcb_context.val[8] = (int)(KERNELSTACK - 24);
	pcb->pcb_context.val[0] = (int)child_return;
	pcb->pcb_context.val[1] = (int)p2;
}

void
cpu_set_kpc(p, pc)
	struct proc *p;
	void (*pc) __P((struct proc *));
{
	p->p_addr->u_pcb.pcb_context.val[0] = (int)pc;
@


1.6
log
@Return EINVAL when msg_iovlen or iovcnt <= 0; Make uio_resid unsigned (size_t) and don't return EINVAL if it is < 0 in sys_{read,write}.  Remove check for uio_resid < 0 uiomove() now that uio_resid is unsigned and brack remaining panics with #ifdef DIAGNOSTIC.  vn_rdwr() must now take a size_t * as its 9th argument so change that and clean up uses of vn_rdwr().  Fixes 549 + more
@
text
@d72 3
a76 6
 * Copy and update the kernel stack and pcb, making the child
 * ready to run, and marking it so that it can return differently
 * than the parent.  Returns 1 in the child process, 0 in the parent.
 * We currently double-map the user area so that the stack is at the same
 * address in each process; in the future we will probably relocate
 * the frame pointers on the stack after copying.
d78 1
a78 1
int
d80 1
a80 1
	register struct proc *p1, *p2;
d82 3
a84 3
	register struct user *up = p2->p_addr;
	register pt_entry_t *pte;
	register int i;
d87 1
a87 1
	p2->p_md.md_regs = up->u_pcb.pcb_regs;
d90 1
a90 7
	/*
	 * Cache the PTEs for the user area in the machine dependent
	 * part of the proc struct so cpu_switch() can quickly map in
	 * the user struct and kernel stack. Note: if the virtual address
	 * translation changes (e.g. swapout) we have to update this.
	 */
	pte = kvtopte(up);
a95 4
	/*
	 * Copy floating point state from the FP chip if this process
	 * has state stored there.
	 */
d99 1
a99 5
	/*
	 * Copy pcb and stack from proc p1 to p2. 
	 * We do this as cheaply as possible, copying only the active
	 * part of the stack.  The stack and pcb need to agree;
	 */
a100 2
	/* cache segtab for ULTBMiss() */
	p2->p_addr->u_pcb.pcb_segtab = (void *)p2->p_vmspace->vm_map.pmap->pm_segtab;
d102 14
a115 15
	/*
	 * Arrange for a non-local goto when the new process
	 * is started, to resume here, returning nonzero from setjmp.
	 */
#ifdef DIAGNOSTIC
	if (p1 != curproc)
		panic("cpu_fork: curproc");
#endif
	if (copykstack(up)) {
		/*
		 * Return 1 in child.
		 */
		return (1);
	}
	return (0);
@


1.5
log
@new cpureg.h from NetBSD plus modifications to use the changed macro names
@
text
@d220 1
a220 1
	    IO_NODELOCKED|IO_UNIT, cred, (int *)NULL, p);
d228 1
a228 2
			UIO_SYSSPACE, IO_NODELOCKED|IO_UNIT,
			cred, (int *)NULL, p);
@


1.4
log
@update the pmax specific stuff to NetBSD-current as of about 970608 - some
cosmetic things will follow together with an update to an exact NETBSD_CU-
RRENT_xxxxxx level
@
text
@d360 1
a360 1
        if (kva >= MACH_CACHED_MEMORY_ADDR && kva < MACH_UNCACHED_MEMORY_ADDR)
d362 1
a362 1
		return (MACH_CACHED_TO_PHYS(kva));
d364 2
a365 2
	else if (kva >= MACH_UNCACHED_MEMORY_ADDR && kva < MACH_KSEG2_ADDR) {
		return (MACH_UNCACHED_TO_PHYS(kva));
d374 1
a374 1
	else if (kva >= MACH_KSEG2_ADDR /*&& kva < VM_MAX_KERNEL_ADDRESS*/) {
@


1.3
log
@update the pmax stuff to NetBSD 961107 - this version i got somehow
compiled on my decstation 2100 (PLUTO) - but it will not fully work
out of the box - but i want to bring it into the tree because i get
my own pmax on 961228 - so that i have a good startpoint then :-)

all the OpenBSD changes to the pmax tree will follow in the next commit
@
text
@d1 1
a1 1
/*	$NetBSD: vm_machdep.c,v 1.13 1996/10/13 03:39:57 christos Exp $	*/
d70 2
d119 1
a119 1
	p2->p_addr->u_pcb.pcb_segtab = (void *)p2->p_vmspace->vm_pmap.pm_segtab;
d351 2
a352 1
 * XXX the u-area mappng should all change anyway.
@


1.2
log
@sync to netbsd; for graichen
@
text
@d1 1
a1 1
/*	$NetBSD: vm_machdep.c,v 1.11 1996/05/19 15:55:31 jonathan Exp $	*/
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: vm_machdep.c,v 1.7 1995/09/25 20:36:23 jonathan Exp $	*/
d61 1
d64 6
d79 1
d196 1
a196 1
	register struct user *up = p->p_addr;
d240 1
d243 1
a243 1
	int size;
d252 3
a254 2
		MachTLBFlushAddr(from);
		MachTLBUpdate(to, *fpte);
d283 3
a285 1
vmapbuf(bp)
d287 1
d289 1
a289 2
	register caddr_t addr;
	register vm_size_t sz;
a290 3
	int off;
	vm_offset_t kva;
	register vm_offset_t pa;
a293 2
	addr = bp->b_saveaddr = bp->b_un.b_addr;
	off = (int)addr & PGOFSET;
d295 8
a302 7
	sz = round_page(bp->b_bcount + off);
	kva = kmem_alloc_wait(phys_map, sz);
	bp->b_un.b_addr = (caddr_t) (kva + off);
	sz = atop(sz);
	while (sz--) {
		pa = pmap_extract(vm_map_pmap(&p->p_vmspace->vm_map),
			(vm_offset_t)addr);
d305 1
a305 1
		pmap_enter(vm_map_pmap(phys_map), kva, trunc_page(pa),
d307 2
a308 2
		addr += PAGE_SIZE;
		kva += PAGE_SIZE;
d316 3
a318 1
vunmapbuf(bp)
d320 1
d322 1
a322 3
	register caddr_t addr = bp->b_un.b_addr;
	register vm_size_t sz;
	vm_offset_t kva;
d326 5
a330 4
	sz = round_page(bp->b_bcount + ((int)addr & PGOFSET));
	kva = (vm_offset_t)((int)addr & ~PGOFSET);
	kmem_free_wakeup(phys_map, kva, sz);
	bp->b_un.b_addr = bp->b_saveaddr;
a377 1
kernelmapped:
d379 1
a379 1
			printf("kvtophys: pte not valid for %x\n", kva);
d384 1
a384 1
		printf("kvtophys: kv %x, phys %x", kva, phys);
d388 1
a388 1
		printf("Virtual address %x: cannot map to physical\n",
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
