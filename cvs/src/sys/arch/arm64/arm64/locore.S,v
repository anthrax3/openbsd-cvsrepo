head	1.15;
access;
symbols
	OPENBSD_6_1:1.15.0.2
	OPENBSD_6_1_BASE:1.15;
locks; strict;
comment	@# @;


1.15
date	2017.02.17.19.14.58;	author patrick;	state Exp;
branches;
next	1.14;
commitid	cHUgMLSDbEfl8iaP;

1.14
date	2017.02.08.09.18.24;	author patrick;	state Exp;
branches;
next	1.13;
commitid	Qs36oCn0rzyBLZSt;

1.13
date	2017.02.07.21.56.07;	author patrick;	state Exp;
branches;
next	1.12;
commitid	y0ZHZPNvxxCvdImJ;

1.12
date	2017.02.06.19.23.45;	author patrick;	state Exp;
branches;
next	1.11;
commitid	5it27ZRIjA5PXf5f;

1.11
date	2017.02.05.13.08.03;	author patrick;	state Exp;
branches;
next	1.10;
commitid	tX6oLbcGitAmAkaJ;

1.10
date	2017.02.03.13.39.49;	author patrick;	state Exp;
branches;
next	1.9;
commitid	8iztMPG4oSbXA99P;

1.9
date	2017.02.03.10.46.19;	author patrick;	state Exp;
branches;
next	1.8;
commitid	XFRC1g0vr5HN6B67;

1.8
date	2017.02.03.10.34.21;	author patrick;	state Exp;
branches;
next	1.7;
commitid	eIpBd2S6lMzlFsZN;

1.7
date	2017.02.03.10.20.42;	author patrick;	state Exp;
branches;
next	1.6;
commitid	Ud0qgsbUiUQhNTPq;

1.6
date	2017.01.23.13.43.50;	author patrick;	state Exp;
branches;
next	1.5;
commitid	NQpb5xg2yvJxSNA7;

1.5
date	2017.01.23.13.39.24;	author patrick;	state Exp;
branches;
next	1.4;
commitid	5sNOTUxfSiCz1PBv;

1.4
date	2017.01.15.18.16.55;	author patrick;	state Exp;
branches;
next	1.3;
commitid	exWG4QjccevfpwON;

1.3
date	2017.01.15.18.13.56;	author patrick;	state Exp;
branches;
next	1.2;
commitid	OgZklJ84X6xiNaxR;

1.2
date	2016.12.18.14.40.25;	author patrick;	state Exp;
branches;
next	1.1;
commitid	Pj557aWhjAUQGvhw;

1.1
date	2016.12.17.23.38.33;	author patrick;	state Exp;
branches;
next	;
commitid	uicSPzuCfsvjSNQ4;


desc
@@


1.15
log
@Use a proper memory attribute for write-through instead of reusing
the nocache attribute.
@
text
@/* $OpenBSD: locore.S,v 1.14 2017/02/08 09:18:24 patrick Exp $ */
/*-
 * Copyright (c) 2012-2014 Andrew Turner
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * $FreeBSD: head/sys/arm64/arm64/locore.S 282867 2015-05-13 18:57:03Z zbb $
 */

#include "assym.h"
#include <sys/syscall.h>
#include <machine/asm.h>
#include <machine/armreg.h>
#include <machine/hypervisor.h>
#include <machine/param.h>
#include <machine/pte.h>

#define	VIRT_BITS	39

#define	DEVICE_MEM	0
#define	NORMAL_UNCACHED	1
#define	NORMAL_MEM	2

/*
 * We assume:
 *  MMU      on with an identity map, or off
 *  D-Cache: off
 *  I-Cache: on or off
 *  We are loaded at a 2MiB aligned address
 */

#define	INIT_STACK_SIZE	(PAGE_SIZE * 4)

	.text
	.globl _start
_start:
	mov x21, x0
	mov x22, x1
	mov x23, x2

	/* Drop to EL1 */
	bl	drop_to_el1

	/*
	 * Disable the MMU. We may have entered the kernel with it on and
	 * will need to update the tables later. If this has been set up
	 * with anything other than a VA == PA map then this will fail,
	 * but in this case the code to find where we are running from
	 * would have also failed.
	 */
	dsb	sy
	mrs	x2, sctlr_el1
	bic	x2, x2, SCTLR_M
	msr	sctlr_el1, x2
	isb

	/* Set the context id */
	msr	contextidr_el1, xzr

	/* Get the virt -> phys offset */
	bl	get_virt_delta

	/* Store symbol value. */
	adr	x0, .Lesym
	ldr	x0, [x0]
	sub	x0, x0, x29
	add	x21, x21, x29
	str	x21, [x0]

	/*
	 * At this point:
	 * x29 = PA - VA
	 * x28 = Our physical load address
	 */

	/* Create the page tables */
	bl	create_pagetables

	/*
	 * At this point:
	 * x27 = TTBR0 table
	 * x26 = TTBR1 table
	 */

	/* Enable the mmu */
	bl	start_mmu

	/* Jump to the virtual address space */
	ldr	x15, .Lvirtdone
	br	x15

.Linitstack:
	.xword initstack
.Linitstack_end:
	.xword initstack_end
virtdone:
	/* Set up the stack */
	adr	x25, .Linitstack_end
	ldr	x25, [x25]
	mov	sp, x25
	mov	x8, #TRAPFRAME_SIZEOF
	sub     x8, x8, (STACKALIGNBYTES)
	and     x8, x8, ~(STACKALIGNBYTES)

	// pass base of kernel stack as proc0
	adr	x25, .Linitstack
	ldr	x25, [x25]

	sub	sp, sp, x8

	/* Zero the BSS */
	ldr	x15, .Lbss
	ldr	x14, .Lend
1:
	str	xzr, [x15], #8
	cmp	x15, x14
	b.lo	1b

	/* Backup the module pointer */
	mov	x1, x0

	/* Make the page table base a virtual address */
	sub	x26, x26, x29

	// XXX - shouldn't this be 8 * 5 (struct grew from 4 -> 5)
	sub	sp, sp, #(64 * 4)
	mov	x0, sp

	/* Negate the delta so it is VA -> PA */
	neg	x29, x29

	str	x1,  [x0]	/* modulep */
	str	x26, [x0, 8]	/* kern_l1pt */
	str	x29, [x0, 16]	/* kern_delta */
	str	x25, [x0, 24]	/* kern_stack */
	str	x21, [x0, 32]	/* ? (x0 arg on boot) */
	str	x22, [x0, 40]	/* ? (x1 arg on boot) */
	str	x23, [x0, 48]	/* fdt (x2 arg on boot) */

	/* trace back starts here */
	mov	fp, #0
	/* Branch to C code */
	bl	initarm
	bl	_C_LABEL(main)

	/* We should not get here */
	brk	0

	.align 3
.Lvirtdone:
	.quad	virtdone
.Lbss:
	.quad	__bss_start
.Lstart:
	.quad	_start
.Lend:
	.quad	_end
.Lcpu_info_primary:
	.quad	_C_LABEL(cpu_info_primary)

/*
 * If we are started in EL2, configure the required hypervisor
 * registers and drop to EL1.
 */
drop_to_el1:
	mrs	x1, CurrentEL
	lsr	x1, x1, #2
	cmp	x1, #0x2
	b.eq	1f
	ret
1:
	/* Configure the Hypervisor */
	mov	x2, #(HCR_RW)
	msr	hcr_el2, x2

	/* Load the Virtualization Process ID Register */
	mrs	x2, midr_el1
	msr	vpidr_el2, x2

	/* Load the Virtualization Multiprocess ID Register */
	mrs	x2, mpidr_el1
	msr	vmpidr_el2, x2

	/* Set the bits that need to be 1 in sctlr_el1 */
	ldr	x2, .Lsctlr_res1
	msr	sctlr_el1, x2

	/* Don't trap to EL2 for exceptions */
	mov	x2, #CPTR_RES1
	msr	cptr_el2, x2

	/* Don't trap to EL2 for CP15 traps */
	msr	hstr_el2, xzr

	/* Enable access to the physical timers at EL1 */
	mrs	x2, cnthctl_el2
	orr	x2, x2, #(CNTHCTL_EL1PCTEN | CNTHCTL_EL1PCEN)
	msr	cnthctl_el2, x2

	/* Set the counter offset to a known value */
	msr	cntvoff_el2, xzr

	/* Hypervisor trap functions */
	adr	x2, hyp_vectors
	msr	vbar_el2, x2

	mov	x2, #(PSR_F | PSR_I | PSR_A | PSR_D | PSR_M_EL1h)
	msr	spsr_el2, x2

	/* Configure GICv3 CPU interface */
	mrs	x2, id_aa64pfr0_el1
	/* Extract GIC bits from the register */
	ubfx	x2, x2, #ID_AA64PFR0_GIC_SHIFT, #ID_AA64PFR0_GIC_BITS
	/* GIC[3:0] == 0001 - GIC CPU interface via special regs. supported */
	cmp	x2, #(ID_AA64PFR0_GIC_CPUIF_EN >> ID_AA64PFR0_GIC_SHIFT)
	b.ne	2f

	mrs	x2, icc_sre_el2
	orr	x2, x2, #ICC_SRE_EL2_EN	/* Enable access from insecure EL1 */
	orr	x2, x2, #ICC_SRE_EL2_SRE	/* Enable system registers */
	msr	icc_sre_el2, x2
2:

	/* Set the address to return to our return address */
	msr	elr_el2, x30
	isb

	eret

	.align 3
.Lsctlr_res1:
	.quad SCTLR_RES1

#define	VECT_EMPTY	\
	.align 7;	\
	1:	b	1b

	.align 11
hyp_vectors:
	VECT_EMPTY	/* Synchronous EL2t */
	VECT_EMPTY	/* IRQ EL2t */
	VECT_EMPTY	/* FIQ EL2t */
	VECT_EMPTY	/* Error EL2t */

	VECT_EMPTY	/* Synchronous EL2h */
	VECT_EMPTY	/* IRQ EL2h */
	VECT_EMPTY	/* FIQ EL2h */
	VECT_EMPTY	/* Error EL2h */

	VECT_EMPTY	/* Synchronous 64-bit EL1 */
	VECT_EMPTY	/* IRQ 64-bit EL1 */
	VECT_EMPTY	/* FIQ 64-bit EL1 */
	VECT_EMPTY	/* Error 64-bit EL1 */

	VECT_EMPTY	/* Synchronous 32-bit EL1 */
	VECT_EMPTY	/* IRQ 32-bit EL1 */
	VECT_EMPTY	/* FIQ 32-bit EL1 */
	VECT_EMPTY	/* Error 32-bit EL1 */

/*
 * Get the delta between the physical address we were loaded to and the
 * virtual address we expect to run from. This is used when building the
 * initial page table.
 */
	.globl get_virt_delta
get_virt_delta:
	/* Load the physical address of virt_map */
	adr	x28, virt_map
	/* Load the virtual address of virt_map stored in virt_map */
	ldr	x29, [x28]
	/* Find PA - VA as PA' = VA' - VA + PA = VA' + (PA - VA) = VA' + x29 */
	sub	x29, x29, x28
	and	x28, x28, #~0x0003ffff // should be 2MB?

	ret

	.align 3
virt_map:
	.quad	virt_map

/*
 * This builds the page tables containing the identity map, and the kernel
 * virtual map.
 *
 * It relys on:
 *  We were loaded to an address that is on a 2MiB boundary
 *  All the memory must not cross a 1GiB boundaty
 *  x28 contains the physical address we were loaded from
 *
 *  There are 3 pages before that address for the page tables
 *  These pages are allocated aligned in .data
 *   The pages used are:
 *    - The identity (PA = VA) table (TTBR0)
 *    - The Kernel L1 table          (TTBR1)
 *    -  The PA == VA L2 table for kernel
 */
.Lpagetable:
	.xword pagetable
.Lpagetable_end:
	.xword pagetable_end

.Lesym:
	.xword esym

create_pagetables:
	/* Save the Link register */
	mov	x5, x30

	/* Clean the page table */
	adr	x6, .Lpagetable
	ldr	x6, [x6]
	sub	x6, x6, x29 // VA -> PA
	mov	x26, x6
	adr	x27, .Lpagetable_end
	ldr	x27, [x27]
	sub	x27, x27, x29 // VA -> PA
1:
	stp	xzr, xzr, [x6], #16
	stp	xzr, xzr, [x6], #16
	stp	xzr, xzr, [x6], #16
	stp	xzr, xzr, [x6], #16
	cmp	x6, x27
	b.lo	1b

	/*
	 * Build the TTBR1 maps.
	 */

	/* Find the size of the kernel */
	adr	x6, .Lstart
	ldr	x6, [x6]
	sub	x6, x6, x29

	/* End is the symbol address */
	adr	x7, .Lesym
	ldr	x7, [x7]
	sub	x7, x7, x29
	ldr	x7, [x7]
	sub	x7, x7, x29

	/* Find the end - begin */
	sub	x8, x7, x6
	/* Get the number of l2 pages to allocate, rounded down */
	lsr	x10, x8, #(L2_SHIFT)
	/* Add 4 MiB for any rounding above and the module data */
	add	x10, x10, #2

	/* Create the kernel space L2 table */
	mov	x6, x26				// pagetable:
	mov	x7, #NORMAL_MEM
	add	x8, x28, x29
	mov	x9, x28
	bl	build_l2_block_pagetable

	/* Move to the l1 table */
	add	x26, x26, #PAGE_SIZE*2		// pagetable_l1_ttbr1:

	/* Link the l1 -> l2 table */
	mov	x9, x6
	mov	x6, x26
	bl	link_l1_pagetable

	/*
	 * Build the TTBR0 maps.
	 */
	add	x27, x26, #PAGE_SIZE * 2	// pagetable_l1_ttbr0:

	mov	x6, x27		/* The initial page table */
#if defined(SOCDEV_PA) && defined(SOCDEV_VA)
	/* Create a table for the UART */
	mov	x7, #DEVICE_MEM
	mov	x8, #(SOCDEV_VA)	/* VA start */
	mov	x9, #(SOCDEV_PA)	/* PA start */
	mov	x10, #1
	bl	build_l1_block_pagetable
#endif

	/* Create the VA = PA map */
	mov	x7, #NORMAL_MEM // #NORMAL
	mov	x9, x27
	mov	x8, x9		/* VA start (== PA start) */
	mov	x10, #1
	bl	build_l1_block_pagetable

	/* Create a mapping for the FDT */
	mov	x7, #NORMAL_MEM // #NORMAL
	mov	x9, x23
	mov	x8, x9		/* VA start (== PA start) */
	mov	x10, #1
	bl	build_l1_block_pagetable

	/* Move to the l0 table */
	add	x27, x27, #PAGE_SIZE * 2	// pagetable_l0_ttbr0:

	/* Link the l0 -> l1 table */
	mov	x9, x6
	mov	x6, x27
	mov	x10, #1
	bl	link_l0_pagetable

	/* Restore the Link register */
	mov	x30, x5
	ret

/*
 * Builds an L0 -> L1 table descriptor
 *
 * This is a link for a 512GiB block of memory with up to 1GiB regions mapped
 * within it by build_l1_block_pagetable.
 *
 *  x6  = L0 table
 *  x8  = Virtual Address
 *  x9  = L1 PA (trashed)
 *  x10 = Entry count
 *  x11, x12 and x13 are trashed
 */
link_l0_pagetable:
	/*
	 * Link an L0 -> L1 table entry.
	 */
	/* Find the table index */
	lsr	x11, x8, #L0_SHIFT
	and	x11, x11, #Ln_ADDR_MASK

	/* Build the L0 block entry */
	mov	x12, #L0_TABLE

	/* Only use the output address bits */
	lsr	x9, x9, #PAGE_SHIFT
1:	orr	x13, x12, x9, lsl #PAGE_SHIFT

	/* Store the entry */
	str	x13, [x6, x11, lsl #3]

	sub	x10, x10, #1
	add	x11, x11, #1
	add	x9, x9, #1
	cbnz	x10, 1b

	ret

/*
 * Builds an L1 -> L2 table descriptor
 *
 * This is a link for a 1GiB block of memory with up to 2MiB regions mapped
 * within it by build_l2_block_pagetable.
 *
 *  x6  = L1 table
 *  x8  = Virtual Address
 *  x9  = L2 PA (trashed)
 *  x11, x12 and x13 are trashed
 */
link_l1_pagetable:
	/*
	 * Link an L1 -> L2 table entry.
	 */
	/* Find the table index */
	lsr	x11, x8, #L1_SHIFT
	and	x11, x11, #Ln_ADDR_MASK

	/* Build the L1 block entry */
	mov	x12, #L1_TABLE

	/* Only use the output address bits */
	lsr	x9, x9, #PAGE_SHIFT
	orr	x13, x12, x9, lsl #PAGE_SHIFT

	/* Store the entry */
	str	x13, [x6, x11, lsl #3]

	ret

/*
 * Builds count 1 GiB page table entry
 *  x6  = L1 table
 *  x7  = Type (0 = Device, 1 = Normal)
 *  x8  = VA start
 *  x9  = PA start (trashed)
 *  x10 = Entry count
 *  x11, x12 and x13 are trashed
 */
build_l1_block_pagetable:
	/*
	 * Build the L1 table entry.
	 */
	/* Find the table index */
	lsr	x11, x8, #L1_SHIFT
	and	x11, x11, #Ln_ADDR_MASK

	/* Build the L1 block entry */
	lsl	x12, x7, #2
	orr	x12, x12, #L1_BLOCK
	orr	x12, x12, #(ATTR_AF)
	orr	x12, x12, ATTR_SH(SH_INNER)

	/* Only use the output address bits */
	lsr	x9, x9, #L1_SHIFT

	/* Set the physical address for this virtual address */
1:	orr	x13, x12, x9, lsl #L1_SHIFT

	/* Store the entry */
	str	x13, [x6, x11, lsl #3]

	sub	x10, x10, #1
	add	x11, x11, #1
	add	x9, x9, #1
	cbnz	x10, 1b

	ret

/*
 * Builds count 2 MiB page table entry
 *  x6  = L2 table
 *  x7  = Type (0 = Device, 1 = Normal)
 *  x8  = VA start
 *  x9  = PA start (trashed)
 *  x10 = Entry count
 *  x11, x12 and x13 are trashed
 */
build_l2_block_pagetable:
	/*
	 * Build the L2 table entry.
	 */
	/* Find the table index */
	lsr	x11, x8, #L2_SHIFT
	and	x11, x11, #Ln_ADDR_MASK

	/* Build the L2 block entry */
	lsl	x12, x7, #2
	orr	x12, x12, #L2_BLOCK
	orr	x12, x12, #(ATTR_AF)
	orr	x12, x12, ATTR_SH(SH_INNER)

	/* Only use the output address bits */
	lsr	x9, x9, #L2_SHIFT

	/* Set the physical address for this virtual address */
1:	orr	x13, x12, x9, lsl #L2_SHIFT

	/* Store the entry */
	str	x13, [x6, x11, lsl #3]

	sub	x10, x10, #1
	add	x11, x11, #1
	add	x9, x9, #1
	cbnz	x10, 1b

	ret

start_mmu:
	dsb	sy

	/* Load the exception vectors */
	ldr	x2, =exception_vectors
	msr	vbar_el1, x2

	/* Load ttbr0 and ttbr1 */
	msr	ttbr0_el1, x27
	msr	ttbr1_el1, x26
	isb

	/* Clear the Monitor Debug System control register */
	msr	mdscr_el1, xzr

	/* Invalidate the TLB */
	tlbi	vmalle1is

	ldr	x2, mair
	msr	mair_el1, x2

	/*
	 * Setup TCR according to PARange bits from ID_AA64MMFR0_EL1.
	 * Some machines have physical memory mapped >512GiB, which can not
	 * be identity-mapped using the default 39 VA bits. Thus, use
	 * 48 VA bits for now and switch back to 39 after the VA jump.
	 */
	ldr	x2, tcr
	mrs	x3, id_aa64mmfr0_el1
	bfi	x2, x3, #32, #3
	msr	tcr_el1, x2

	/* Setup SCTLR */
	ldr	x2, sctlr_set
	ldr	x3, sctlr_clear
	mrs	x1, sctlr_el1
	bic	x1, x1, x3	/* Clear the required bits */
	orr	x1, x1, x2	/* Set the required bits */
	msr	sctlr_el1, x1
	isb

	ret
	.globl switch_mmu_kernel
switch_mmu_kernel:
	dsb	sy
	/* Invalidate the TLB */
	tlbi	vmalle1is
	/* Load ttbr1 (kernel) */
	msr	ttbr1_el1, x0
	isb
	ret



	.align 3
mair:
	/* Device | Normal (no cache, write-back, write-through) */
	.quad	MAIR_ATTR(0x00, 0) |	\
		MAIR_ATTR(0x44, 1) |	\
		MAIR_ATTR(0xff, 2) |	\
		MAIR_ATTR(0x88, 3)
tcr:
	.quad (TCR_T1SZ(64 - VIRT_BITS) | TCR_T0SZ(64 - 48) | \
	    TCR_ASID_16 | TCR_TG1_4K | TCR_CACHE_ATTRS | TCR_SMP_ATTRS)
sctlr_set:
	/* Bits to set */
	.quad (SCTLR_UCI | SCTLR_nTWE | SCTLR_nTWI | SCTLR_UCT | SCTLR_DZE | \
	    SCTLR_I | SCTLR_SED | SCTLR_C | SCTLR_M)
sctlr_clear:
	/* Bits to clear */
	.quad (SCTLR_EE | SCTLR_EOE | SCTLR_WXN | SCTLR_UMA | SCTLR_ITD | \
	    SCTLR_THEE | SCTLR_CP15BEN | SCTLR_SA0 | SCTLR_SA | SCTLR_A)

	.globl abort
abort:
	b abort

	// First entries in data must be esym
	// so that bootloader can find them easily.
	.data
	.global _C_LABEL(esym)
_C_LABEL(esym): .xword   _C_LABEL(end)

	//.section .init_pagetable
data_align_pad:
	.space 32
	.align 12 /* 4KiB aligned */
	/*
	 * 3 initial tables (in the following order):
	 *           L2 for kernel (High addresses)
	 *           L1 for kernel
	 *           L1 for user   (Low addresses)
	 */
	.globl	pagetable
pagetable:
	.space	PAGE_SIZE * 2	// allocate 2 pages for pmapvp2
pagetable_l1_ttbr1:
	.space	PAGE_SIZE * 2	// allocate 2 pages for pmapvp1
pagetable_l1_ttbr0:
	.space	PAGE_SIZE * 2	// allocate 2 pages, reused later in pmap
pagetable_l0_ttbr0:
	.space	PAGE_SIZE
pagetable_end:

	.text
#if 0
	.globl init_pt_va
init_pt_va:
	.quad pagetable		/* XXX: Keep page tables VA */
#endif

	.bss
	.align	4
	.globl initstack
initstack:
	.space	USPACE
initstack_end:

	.text
ENTRY(sigcode)
	mov     x0, sp
	add     x0, x0, #SF_SC

1:
	mov     x8, #SYS_sigreturn
	svc     0
	.globl  _C_LABEL(sigcoderet)
_C_LABEL(sigcoderet):

	/* sigreturn failed, exit */
	mov     x8, #SYS_exit
	svc     0

	b       1b
END(sigcode)
	/* This may be copied to the stack, keep it 16-byte aligned */
	.align  3
        .global _C_LABEL(esigcode)
_C_LABEL(esigcode):

	.globl	sigfill
sigfill:
	.word	0xa000f7f0		/* FIXME: illegal on all cpus? */
esigfill:

	.data
	.globl	sigfillsiz
sigfillsiz:
	.word	esigfill - sigfill

	.text
@


1.14
log
@Pass the physical address to the end of symbols to the kernel.  From
armv7 we inherited the mechanism to manually modify the kernel symbol
table to change the value of esym.  We don't use this, but instead use
the virtual address that is passed to the kernel.  This change makes us
only work on and hand out physical addresses.  Bump the efiboot version
to make this ABI change more visible.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.13 2017/02/07 21:56:07 patrick Exp $ */
d627 5
a631 2
		/* Device            Normal, no cache     Normal, write-back */
	.quad	MAIR_ATTR(0x00, 0) | MAIR_ATTR(0x44, 1) | MAIR_ATTR(0xff, 2)
@


1.13
log
@For consistency sake, apply the inner shareable attribute to the bootstrap
pagetables as well.  Also replace the number for write-back with a proper
define.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.12 2017/02/06 19:23:45 patrick Exp $ */
d87 1
@


1.12
log
@Move cache and tlb flush functions, which were mostly inline assembly,
into separate functions.  This makes them reusable from other parts in
the kernel.  Assembly and header are taken from FreeBSD, but modified
to fit our requirements and with some unnecessary stuff removed.  While
there remove micro optimization for uniprocessor kernels.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.11 2017/02/05 13:08:03 patrick Exp $ */
d513 1
d552 1
@


1.11
log
@Implement another pagetable level for bootstrapping machines that have
their memory mapped above 39 bits of address space.  Since our pmap is
configured to use a 3 level pagetable userland we need to reconfigure
the size back to 39 bits as soon are in virtual address space and have
finished using the FDT via the 1:1 map.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.10 2017/02/03 13:39:49 patrick Exp $ */
a97 12

	mrs	x0, DCZID_EL0
	tbnz	x0, 4, 1f
	mov	x1, #1
	and	x0, x0, 0xf
	lsl	x1, x1, x0
	ldr	x0, =dczva_line_size
	// adjust virtual address to physical
	sub	x0, x0, x29

	str	x1, [x0]
1:
a103 1

a646 2
	.global _C_LABEL(dczva_line_size)
_C_LABEL(dczva_line_size):     .xword   0
@


1.10
log
@Implement a helper that creates an L0 pagetable entry pointing to
a L1 pagetable.  Needed for machines that need 4 level pagetables
on bootup.

From FreeBSD
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.9 2017/02/03 10:46:19 patrick Exp $ */
d424 9
d602 6
a607 1
	/* Setup TCR according to PARange bits from ID_AA64MMFR0_EL1 */
d640 2
a641 2
	.quad (TCR_TxSZ(64 - VIRT_BITS) | TCR_ASID_16 | TCR_TG1_4K | \
	    TCR_CACHE_ATTRS | TCR_SMP_ATTRS)
d680 2
@


1.9
log
@Set the context id and counter offset to a known value.  Enable access
to the physical timers at EL1.

From FreeBSD
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.8 2017/02/03 10:34:21 patrick Exp $ */
d426 37
@


1.8
log
@Use PAGE_SHIFT instead of encoding the number.

From FreeBSD
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.7 2017/02/03 10:20:42 patrick Exp $ */
d77 2
d226 8
@


1.7
log
@Cleanup pagetable creation code in preparation for adding another level
of pagetables to bootstrap machines with physical memory mapped outside
of a 39 bits address space.

From FreeBSD
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.6 2017/01/23 13:43:50 patrick Exp $ */
d441 2
a442 2
	lsr	x9, x9, #12
	orr	x12, x12, x9, lsl #12
d445 1
a445 1
	str	x12, [x6, x11, lsl #3]
a450 1
 *
@


1.6
log
@Create a mapping for the FDT if it happens to be on a different 1 GiB
mapping than the kernel.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.5 2017/01/23 13:39:24 patrick Exp $ */
d375 1
a375 1
	bl	build_block_pagetable
a384 1

d388 1
a388 1
	add	x27, x26, #PAGE_SIZE *2 	 // pagetable_l1_ttbr0:
d390 1
a392 1
	mov	x6, x27		/* The initial page table */
d396 2
a397 1
	bl	build_section_pagetable
a400 1
	mov	x6, x27		/* The initial page table */
d404 2
a405 1
	bl	build_section_pagetable
a407 1
	mov	x6, x27		/* The initial page table */
d411 2
a412 1
	bl	build_section_pagetable
d419 8
a426 5
 * Builds a 1 GiB page table entry
 *  x6 = L1 table
 *  x7 = Type (0 = Device, 1 = Normal)
 *  x8 = VA start
 *  x9 = PA start (trashed)
d429 1
a429 1
build_section_pagetable:
d431 1
a431 1
	 * Build the L1 table entry.
d438 1
a438 3
	lsl	x12, x7, #2
	orr	x12, x12, #L1_BLOCK
	orr	x12, x12, #(ATTR_AF)
d441 2
a442 2
	lsr	x9, x9, #L1_SHIFT
	orr	x12, x12, x9, lsl #L1_SHIFT
d450 1
a450 4
 * Builds an L1 -> L2 table descriptor
 *
 * This is a link for a 1GiB block of memory with up to 2MiB regions mapped
 * within it by build_block_pagetable.
d453 4
a456 2
 *  x8  = Virtual Address
 *  x9  = L2 PA (trashed)
d459 1
a459 2
.global link_l1_pagetable
link_l1_pagetable:
d461 1
a461 1
	 * Link an L1 -> L2 table entry.
d468 3
a470 1
	mov	x12, #L1_TABLE
d473 4
a476 2
	lsr	x9, x9, #12
	orr	x12, x12, x9, lsl #12
d479 6
a484 1
	str	x12, [x6, x11, lsl #3]
d494 1
a494 1
 *  x10 = Entry count (TODO)
d497 1
a497 2
.global build_block_pagetable
build_block_pagetable:
d514 1
a514 1
1:	orr	x12, x12, x9, lsl #L2_SHIFT
d517 1
a517 4
	str	x12, [x6, x11, lsl #3]

	/* Clear the address bits */
	and	x12, x12, #ATTR_MASK_L
a523 1
2:
@


1.5
log
@In comparison to _start, where we only need the address of the
function, we need another indirection for esym.  The address of
esym is simply the address of the variable.  But in the case of
esym we want to read and modify its value.

ok kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.4 2017/01/15 18:16:55 patrick Exp $ */
d404 7
@


1.4
log
@When dropping to EL1 ensure we have written to all special registers by
moving the instruction barrier to just before we drop exception level.
Additionally, enable system register access for EL2.

From FreeBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.3 2017/01/15 18:13:56 patrick Exp $ */
a60 4
	/* Store symbol value. */
	adr	x0, .Lesym
	str	x21, [x0]

d81 6
d358 2
@


1.3
log
@Address _start relative to the current program counter, like we already
do for esym.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.2 2016/12/18 14:40:25 patrick Exp $ */
d240 1
a241 1
	isb
d246 1
@


1.2
log
@Adjust OpenBSD/arm64 files with FreeBSD origin to show the upstream
revision.  While there, update a few of those files.

Prompted by mikeb@@.
@
text
@d1 1
a1 1
/* $OpenBSD: locore.S,v 1.1 2016/12/17 23:38:33 patrick Exp $ */
d349 2
a350 1
	ldr	x6, .Lstart
@


1.1
log
@Import of OpenBSD/arm64

This commit contains all the kernel files related to the OpenBSD/arm64
port.  It is based on the PowerPC pmap, loongson, arm/armv7 code and
FreeBSD aarch64 code.  Hard work done by Dale Rahn.
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d27 1
a27 1
 * $FreeBSD$
d565 1
a565 1
		TCR_SH0(3)|TCR_SH1(3)|TCR_ORGNx(1)|TCR_IRGNx(1))
@

