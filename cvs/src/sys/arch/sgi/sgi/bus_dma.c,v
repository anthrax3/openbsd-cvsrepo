head	1.40;
access;
symbols
	OPENBSD_6_0:1.40.0.4
	OPENBSD_6_0_BASE:1.40
	OPENBSD_5_9:1.40.0.2
	OPENBSD_5_9_BASE:1.40
	OPENBSD_5_8:1.39.0.6
	OPENBSD_5_8_BASE:1.39
	OPENBSD_5_7:1.39.0.2
	OPENBSD_5_7_BASE:1.39
	OPENBSD_5_6:1.37.0.4
	OPENBSD_5_6_BASE:1.37
	OPENBSD_5_5:1.30.0.8
	OPENBSD_5_5_BASE:1.30
	OPENBSD_5_4:1.30.0.4
	OPENBSD_5_4_BASE:1.30
	OPENBSD_5_3:1.30.0.2
	OPENBSD_5_3_BASE:1.30
	OPENBSD_5_2:1.29.0.2
	OPENBSD_5_2_BASE:1.29
	OPENBSD_5_1_BASE:1.22
	OPENBSD_5_1:1.22.0.4
	OPENBSD_5_0:1.22.0.2
	OPENBSD_5_0_BASE:1.22
	OPENBSD_4_9:1.20.0.2
	OPENBSD_4_9_BASE:1.20
	OPENBSD_4_8:1.19.0.2
	OPENBSD_4_8_BASE:1.19
	OPENBSD_4_7:1.17.0.2
	OPENBSD_4_7_BASE:1.17
	OPENBSD_4_6:1.13.0.2
	OPENBSD_4_6_BASE:1.13
	OPENBSD_4_5:1.6.0.4
	OPENBSD_4_5_BASE:1.6
	OPENBSD_4_4:1.6.0.2
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.3.0.2
	OPENBSD_4_3_BASE:1.3
	OPENBSD_4_2:1.2.0.2
	OPENBSD_4_2_BASE:1.2;
locks; strict;
comment	@ * @;


1.40
date	2015.09.05.21.13.24;	author miod;	state Exp;
branches;
next	1.39;
commitid	1Tj9UMvH0jts1O49;

1.39
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.38;
commitid	yv0ECmCdICvq576h;

1.38
date	2014.09.13.16.06.37;	author doug;	state Exp;
branches;
next	1.37;
commitid	jdBY2kKXhfcoQitp;

1.37
date	2014.07.17.19.51.58;	author miod;	state Exp;
branches;
next	1.36;
commitid	o86FcMSRojJgDsY0;

1.36
date	2014.07.12.18.44.42;	author tedu;	state Exp;
branches;
next	1.35;
commitid	uKVPYMN2MLxdZxzH;

1.35
date	2014.07.11.09.36.26;	author mpi;	state Exp;
branches;
next	1.34;
commitid	vsYjSRfS3Y783BvW;

1.34
date	2014.05.19.21.18.42;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2014.04.03.08.07.16;	author mpi;	state Exp;
branches;
next	1.32;

1.32
date	2014.03.21.21.49.45;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2014.03.10.21.32.15;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2012.10.03.22.46.09;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2012.05.27.14.27.10;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2012.05.20.11.41.11;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2012.04.21.12.20.30;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2012.04.05.21.49.58;	author miod;	state Exp;
branches;
next	1.25;

1.25
date	2012.03.28.20.35.41;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2012.03.25.13.52.52;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2012.03.15.18.57.22;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2011.06.23.20.44.39;	author ariane;	state Exp;
branches;
next	1.21;

1.21
date	2011.04.03.22.33.55;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2010.12.26.15.41.00;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2010.06.26.23.24.44;	author guenther;	state Exp;
branches;
next	1.18;

1.18
date	2010.03.29.19.21.58;	author oga;	state Exp;
branches;
next	1.17;

1.17
date	2010.01.09.23.34.29;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2009.12.25.21.02.18;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2009.10.14.21.26.54;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2009.07.17.18.06.51;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2009.05.24.17.31.07;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2009.05.08.20.55.20;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2009.05.08.18.42.06;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2009.04.20.00.42.06;	author oga;	state Exp;
branches;
next	1.9;

1.9
date	2009.04.19.18.34.36;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.7;

1.7
date	2009.03.07.15.34.34;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2008.07.16.15.49.22;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2008.04.07.22.30.49;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2008.04.07.17.25.20;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2007.10.02.00.59.12;	author krw;	state Exp;
branches;
next	1.2;

1.2
date	2007.07.18.20.03.51;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2007.06.21.20.17.10;	author miod;	state Exp;
branches;
next	;


desc
@@


1.40
log
@Give up trying to map DMA descriptor in uncached memory on ECC flavours of the
IP22 motherboard (IP26, IP28). Instead, do not ask for a BUS_DMA_COHERENT
mapping, but perform explicit cache operations.

This removes the need for the memory controller to switch between `fast' and
`slow' mode every time a DMA descriptor is updated.

Tested on IP22 and IP28.
@
text
@/*	$OpenBSD: bus_dma.c,v 1.39 2014/11/16 12:30:58 deraadt Exp $ */

/*
 * Copyright (c) 2003-2004 Opsycon AB  (www.opsycon.se / www.opsycon.com)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */
/*-
 * Copyright (c) 1996, 1997, 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>

#include <uvm/uvm_extern.h>

#include <mips64/cache.h>
#include <machine/cpu.h>

#include <machine/bus.h>

#if defined(TGT_INDY) || defined(TGT_INDIGO2)
#include <sgi/sgi/ip22.h>
#endif

/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
int
_dmamap_create(bus_dma_tag_t t, bus_size_t size, int nsegments,
    bus_size_t maxsegsz, bus_size_t boundary, int flags, bus_dmamap_t *dmamp)
{
	struct machine_bus_dmamap *map;
	void *mapstore;
	size_t mapsize;

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
	mapsize = sizeof(struct machine_bus_dmamap) +
	    (sizeof(bus_dma_segment_t) * (nsegments - 1));
	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
		return (ENOMEM);

	map = (struct machine_bus_dmamap *)mapstore;
	map->_dm_size = size;
	map->_dm_segcnt = nsegments;
	map->_dm_maxsegsz = maxsegsz;
	map->_dm_boundary = boundary;
	map->_dm_flags = flags & ~(BUS_DMA_WAITOK|BUS_DMA_NOWAIT);

	*dmamp = map;
	return (0);
}

/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
void
_dmamap_destroy(bus_dma_tag_t t, bus_dmamap_t map)
{
	free(map, M_DEVBUF, 0);
}

/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
int
_dmamap_load(bus_dma_tag_t t, bus_dmamap_t map, void *buf, bus_size_t buflen,
    struct proc *p, int flags)
{
	paddr_t lastaddr;
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	seg = 0;
	error = (*t->_dmamap_load_buffer)(t, map, buf, buflen, p, flags,
	    &lastaddr, &seg, 1);
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = buflen;
	}

	return (error);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
_dmamap_load_mbuf(bus_dma_tag_t t, bus_dmamap_t map, struct mbuf *m0, int flags)
{
	paddr_t lastaddr;
	int seg, error, first;
	struct mbuf *m;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic("_dmamap_load_mbuf: no packet header");
#endif

	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	first = 1;
	seg = 0;
	error = 0;
	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		error = (*t->_dmamap_load_buffer)(t, map, m->m_data, m->m_len,
		    NULL, flags, &lastaddr, &seg, first);
		first = 0;
	}
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = m0->m_pkthdr.len;
	}

	return (error);
}

/*
 * Like _dmamap_load(), but for uios.
 */
int
_dmamap_load_uio(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio, int flags)
{
	paddr_t lastaddr;
	int seg, i, error, first;
	bus_size_t minlen, resid;
	struct proc *p = NULL;
	struct iovec *iov;
	void *addr;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;

	resid = uio->uio_resid;
	iov = uio->uio_iov;

	if (uio->uio_segflg == UIO_USERSPACE) {
		p = uio->uio_procp;
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic("_dmamap_load_uio: USERSPACE but no proc");
#endif
	}

	first = 1;
	seg = 0;
	error = 0;
	for (i = 0; i < uio->uio_iovcnt && resid != 0 && error == 0; i++) {
		/*
		 * Now at the first iovec to load.  Load each iovec
		 * until we have exhausted the residual count.
		 */
		minlen = resid < iov[i].iov_len ? resid : iov[i].iov_len;
		addr = (void *)iov[i].iov_base;

		error = (*t->_dmamap_load_buffer)(t, map, addr, minlen,
		    p, flags, &lastaddr, &seg, first);
		first = 0;

		resid -= minlen;
	}
	if (error == 0) {
		map->dm_nsegs = seg + 1;
		map->dm_mapsize = uio->uio_resid;
	}

	return (error);
}

/*
 * Like _dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
_dmamap_load_raw(bus_dma_tag_t t, bus_dmamap_t map, bus_dma_segment_t *segs,
    int nsegs, bus_size_t size, int flags)
{
	if (nsegs > map->_dm_segcnt || size > map->_dm_size)
		return (EINVAL);

	/*
	 * Make sure we don't cross any boundaries.
	 */
	if (map->_dm_boundary) {
		bus_addr_t bmask = ~(map->_dm_boundary - 1);
		int i;

		if (t->_dma_mask != 0)
			bmask &= t->_dma_mask;
		for (i = 0; i < nsegs; i++) {
			if (segs[i].ds_len > map->_dm_maxsegsz)
				return (EINVAL);
			if ((segs[i].ds_addr & bmask) !=
			    ((segs[i].ds_addr + segs[i].ds_len - 1) & bmask))
				return (EINVAL);
		}
	}

	bcopy(segs, map->dm_segs, nsegs * sizeof(*segs));
	map->dm_nsegs = nsegs;
	map->dm_mapsize = size;
	return (0);
}

/*
 * Common function for unloading a DMA map.  May be called by
 * bus-specific DMA map unload functions.
 */
void
_dmamap_unload(bus_dma_tag_t t, bus_dmamap_t map)
{
	/*
	 * No resources to free; just mark the mappings as
	 * invalid.
	 */
	map->dm_nsegs = 0;
	map->dm_mapsize = 0;
}

/*
 * Common function for DMA map synchronization.  May be called
 * by bus-specific DMA map synchronization functions.
 */
void
_dmamap_sync(bus_dma_tag_t t, bus_dmamap_t map, bus_addr_t addr,
    bus_size_t size, int op)
{
	int nsegs;
	int curseg;
	int how;
	struct cpu_info *ci;

#ifdef TGT_COHERENT
	/* we only need to writeback here */
	if ((op & BUS_DMASYNC_PREWRITE) == 0)
		return;
	else
		how = CACHE_SYNC_W;
#else
	/*
	 * If only PREWRITE is requested, writeback.
	 * PREWRITE with PREREAD writebacks and invalidates (since noncoherent)
	 * *all* cache levels.
	 * Otherwise, just invalidate (since noncoherent).
	 */
	if (op & BUS_DMASYNC_PREWRITE) {
		if (op & BUS_DMASYNC_PREREAD)
			how = CACHE_SYNC_X;
		else
			how = CACHE_SYNC_W;
	} else {
		if (op & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTREAD))
			how = CACHE_SYNC_R;
		else
			return;
	}
#endif

	ci = curcpu();
	nsegs = map->dm_nsegs;
	curseg = 0;

	while (size && nsegs) {
		paddr_t paddr;
		vaddr_t vaddr;
		bus_size_t ssize;

		ssize = map->dm_segs[curseg].ds_len;
		paddr = map->dm_segs[curseg]._ds_paddr;
		vaddr = map->dm_segs[curseg]._ds_vaddr;

		if (addr != 0) {
			if (addr >= ssize) {
				addr -= ssize;
				ssize = 0;
			} else {
				vaddr += addr;
				paddr += addr;
				ssize -= addr;
				addr = 0;
			}
		}
		if (ssize > size)
			ssize = size;

#ifndef TGT_COHERENT
		if (IS_XKPHYS(vaddr) && XKPHYS_TO_CCA(vaddr) == CCA_NC) {
			size -= ssize;
			ssize = 0;
		}
#endif

		if (ssize != 0) {
			Mips_IOSyncDCache(ci, vaddr, ssize, how);
#if defined(TGT_INDY) || defined(TGT_INDIGO2)
			/*
			 * Also flush external L2 if available - this could
			 * (and used to) be done in Mips_IOSyncDCache, but
			 * as the external L2 is physically addressed, this
			 * would require the physical address to be
			 * recomputed, although we know it here.
			 */
			if (ip22_extsync != NULL)
				(*ip22_extsync)(ci, paddr, ssize, how);
#endif
			size -= ssize;
		}
		curseg++;
		nsegs--;
	}

#ifdef DIAGNOSTIC
	if (size != 0)
		panic("_dmamap_sync: ran off map!");
#endif
}

/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */
int
_dmamem_alloc(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags)
{
	return _dmamem_alloc_range(t, size, alignment, boundary,
	    segs, nsegs, rsegs, flags,
	    dma_constraint.ucr_low, dma_constraint.ucr_high);
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
_dmamem_free(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs)
{
	vm_page_t m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE((*t->_device_to_pa)(addr));
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}

	uvm_pglistfree(&mlist);
}

/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
int
_dmamem_map(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs, size_t size,
    caddr_t *kvap, int flags)
{
	vaddr_t va, sva;
	size_t ssize;
	paddr_t pa;
	bus_addr_t addr;
	int curseg, error, pmap_flags;
	const struct kmem_dyn_mode *kd;

#if defined(TGT_INDIGO2)
	/*
	 * On ECC MC systems, which do not allow uncached writes to memory
	 * during regular operation, fail requests for uncached (coherent)
	 * memory.
	 */
	if ((flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE)) && ip22_ecc)
		return EINVAL;
#endif

#ifdef TGT_COHERENT
	/* coherent mappings do not need to be uncached on these platforms */
	flags &= ~BUS_DMA_COHERENT;
#endif

	if (nsegs == 1) {
		pa = (*t->_device_to_pa)(segs[0].ds_addr);
		if (flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE))
			*kvap = (caddr_t)PHYS_TO_XKPHYS(pa, CCA_NC);
		else
			*kvap = (caddr_t)PHYS_TO_XKPHYS(pa, CCA_CACHED);
		return (0);
	}

	size = round_page(size);
	kd = flags & BUS_DMA_NOWAIT ? &kd_trylock : &kd_waitok;
	va = (vaddr_t)km_alloc(size, &kv_any, &kp_none, kd);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	sva = va;
	ssize = size;
	pmap_flags = PMAP_WIRED | PMAP_CANFAIL;
	if (flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE))
		pmap_flags |= PMAP_NOCACHE;
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += NBPG, va += NBPG, size -= NBPG) {
#ifdef DIAGNOSTIC
			if (size == 0)
				panic("_dmamem_map: size botch");
#endif
			pa = (*t->_device_to_pa)(addr);
			error = pmap_enter(pmap_kernel(), va, pa,
			    PROT_READ | PROT_WRITE,
			    PROT_READ | PROT_WRITE | pmap_flags);
			if (error) {
				pmap_update(pmap_kernel());
				km_free((void *)sva, ssize, &kv_any, &kp_none);
				return (error);
			}

			/*
			 * This is redundant with what pmap_enter() did
			 * above, but will take care of forcing other
			 * mappings of the same page (if any) to be
			 * uncached.
			 * If there are no multiple mappings of that
			 * page, this amounts to a noop.
			 */
			if (flags & (BUS_DMA_COHERENT | BUS_DMA_NOCACHE))
				pmap_page_cache(PHYS_TO_VM_PAGE(pa),
				    PGF_UNCACHED);
		}
		pmap_update(pmap_kernel());
	}

	return (0);
}

/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
void
_dmamem_unmap(bus_dma_tag_t t, caddr_t kva, size_t size)
{
	if (IS_XKPHYS((vaddr_t)kva))
		return;

	km_free(kva, round_page(size), &kv_any, &kp_none);
}

/*
 * Common function for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
paddr_t
_dmamem_mmap(bus_dma_tag_t t, bus_dma_segment_t *segs, int nsegs, off_t off,
    int prot, int flags)
{
	int i;

	for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
		if (off & PAGE_MASK)
			panic("_dmamem_mmap: offset unaligned");
		if (segs[i].ds_addr & PAGE_MASK)
			panic("_dmamem_mmap: segment unaligned");
		if (segs[i].ds_len & PAGE_MASK)
			panic("_dmamem_mmap: segment size not multiple"
			    " of page size");
#endif
		if (off >= segs[i].ds_len) {
			off -= segs[i].ds_len;
			continue;
		}

		return ((*t->_device_to_pa)(segs[i].ds_addr) + off);
	}

	/* Page not found. */
	return (-1);
}

/**********************************************************************
 * DMA utility functions
 **********************************************************************/

/*
 * Utility function to load a linear buffer.  lastaddrp holds state
 * between invocations (for multiple-buffer loads).  segp contains
 * the starting segment on entrance, and the ending segment on exit.
 * first indicates if this is the first invocation of this function.
 */
int
_dmamap_load_buffer(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, paddr_t *lastaddrp,
    int *segp, int first)
{
	bus_size_t sgsize;
	bus_addr_t lastaddr, baddr, bmask;
	paddr_t curaddr;
	vaddr_t vaddr = (vaddr_t)buf;
	int seg;
	pmap_t pmap;

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = *lastaddrp;
	bmask  = ~(map->_dm_boundary - 1);
	if (t->_dma_mask != 0)
		bmask &= t->_dma_mask;

	for (seg = *segp; buflen > 0; ) {
		/*
		 * Get the physical address for this segment.
		 */
		if (pmap_extract(pmap, vaddr, &curaddr) == FALSE)
			panic("_dmapmap_load_buffer: pmap_extract(%p, %p) failed!",
			    pmap, (void *)vaddr);

#ifdef DIAGNOSTIC
		if (curaddr > dma_constraint.ucr_high ||
		    curaddr < dma_constraint.ucr_low)
			panic("Non DMA-reachable buffer at addr %p (raw)",
			    (void *)curaddr);
#endif

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = NBPG - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = ((bus_addr_t)curaddr + map->_dm_boundary) &
			    bmask;
			if (sgsize > (baddr - (bus_addr_t)curaddr))
				sgsize = (baddr - (bus_addr_t)curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr =
			    (*t->_pa_to_device)(curaddr);
			map->dm_segs[seg].ds_len = sgsize;
			map->dm_segs[seg]._ds_paddr = curaddr;
			map->dm_segs[seg]._ds_vaddr = vaddr;
			first = 0;
		} else {
			if ((bus_addr_t)curaddr == lastaddr + 1 &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			     (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     ((bus_addr_t)curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr =
				    (*t->_pa_to_device)(curaddr);
				map->dm_segs[seg].ds_len = sgsize;
				map->dm_segs[seg]._ds_paddr = curaddr;
				map->dm_segs[seg]._ds_vaddr = vaddr;
			}
		}

		lastaddr = (bus_addr_t)curaddr + sgsize - 1;
		vaddr += sgsize;
		buflen -= sgsize;
	}

	*segp = seg;
	*lastaddrp = lastaddr;

	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */

	return (0);
}

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
int
_dmamem_alloc_range(bus_dma_tag_t t, bus_size_t size, bus_size_t alignment,
    bus_size_t boundary, bus_dma_segment_t *segs, int nsegs, int *rsegs,
    int flags, paddr_t low, paddr_t high)
{
	paddr_t curaddr, lastaddr;
	vm_page_t m;
	struct pglist mlist;
	int curseg, error, plaflag;

	/* Always round the size. */
	size = round_page(size);

	/*
	 * Allocate pages from the VM system.
	 */
	plaflag = flags & BUS_DMA_NOWAIT ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & BUS_DMA_ZERO)
		plaflag |= UVM_PLA_ZERO;

	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, low, high, alignment, boundary,
	    &mlist, nsegs, plaflag);
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = TAILQ_FIRST(&mlist);
	curseg = 0;
	lastaddr = segs[curseg].ds_addr =
	    (*t->_pa_to_device)(VM_PAGE_TO_PHYS(m));
	segs[curseg].ds_len = PAGE_SIZE;
	m = TAILQ_NEXT(m, pageq);

	for (; m != NULL; m = TAILQ_NEXT(m, pageq)) {
		curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
		if (curaddr < low || curaddr >= high) {
			printf("vm_page_alloc_memory returned non-sensical"
			    " address 0x%lx\n", curaddr);
			panic("_dmamem_alloc_range");
		}
#endif
		curaddr = (*t->_pa_to_device)(curaddr);
		if (curaddr == (lastaddr + PAGE_SIZE))
			segs[curseg].ds_len += PAGE_SIZE;
		else {
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	return (0);
}
@


1.39
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.38 2014/09/13 16:06:37 doug Exp $ */
d463 1
a463 2
	 * memory, unless the caller tells us it is aware of this and will
	 * do the right thing, by passing BUS_DMA_BUS1 as well.
d465 1
a465 2
	if ((flags & (BUS_DMA_COHERENT | BUS_DMA_BUS1)) == BUS_DMA_COHERENT &&
	    ip22_ecc)
@


1.38
log
@Replace all queue *_END macro calls except CIRCLEQ_END with NULL.

CIRCLEQ_* is deprecated and not called in the tree.  The other queue types
have *_END macros which were added for symmetry with CIRCLEQ_END.  They are
defined as NULL.  There's no reason to keep the other *_END macro calls.

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.37 2014/07/17 19:51:58 miod Exp $ */
d508 2
a509 2
			    VM_PROT_READ | VM_PROT_WRITE, VM_PROT_READ |
			    VM_PROT_WRITE | pmap_flags);
@


1.37
log
@Rework management of the external L2 cache on the few Indy/Indigo2 systems
which have it.

Instead of implementing external L2 maintainance at the cache routine level,
let bus_dmamap_sync(9) know about the possible existence of an external L2,
and invoke a dedicated routine to perform the necessary cache operations.

This way, the external L2 dmamap_sync function pointer can get invoked with
the physical address to operate on; this saves the pmap_extract() calls the
previous cache routine had to do.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.36 2014/07/12 18:44:42 tedu Exp $ */
d731 1
a731 1
	for (; m != TAILQ_END(&mlist); m = TAILQ_NEXT(m, pageq)) {
@


1.36
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.35 2014/07/11 09:36:26 mpi Exp $ */
d71 1
a71 1
#if defined(TGT_INDIGO2)
d314 1
d318 1
d321 20
d378 2
a379 4
#ifdef TGT_COHERENT
			/* we only need to writeback here */
			Mips_IOSyncDCache(ci, vaddr, ssize, CACHE_SYNC_W);
#else
d381 5
a385 4
			 * If only PREWRITE is requested, writeback.
			 * PREWRITE with PREREAD writebacks
			 * and invalidates (if noncoherent) *all* cache levels.
			 * Otherwise, just invalidate (if noncoherent).
d387 2
a388 12
			if (op & BUS_DMASYNC_PREWRITE) {
				if (op & BUS_DMASYNC_PREREAD)
					Mips_IOSyncDCache(ci, vaddr,
					    ssize, CACHE_SYNC_X);
				else
					Mips_IOSyncDCache(ci, vaddr,
					    ssize, CACHE_SYNC_W);
			} else
			if (op & (BUS_DMASYNC_PREREAD | BUS_DMASYNC_POSTREAD)) {
				Mips_IOSyncDCache(ci, vaddr,
				    ssize, CACHE_SYNC_R);
			}
@


1.35
log
@Convert bus_dmamem_map(9) to km_alloc(9) in order to make it fail and
not sleep if the allocator cannot obtain a lock when BUS_DMA_NOWAIT is
specified.

idea and inputs from kettenis@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.34 2014/05/19 21:18:42 miod Exp $ */
d123 1
a123 1
	free(map, M_DEVBUF);
@


1.34
log
@Format string fixes and removal of -Wno-format for sgi. Based upon an
initial diff from jasper@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.33 2014/04/03 08:07:16 mpi Exp $ */
d446 1
d475 2
a476 1
	va = uvm_km_valloc(kernel_map, size);
d501 1
a501 1
				uvm_km_free(kernel_map, sva, ssize);
d533 1
a533 2
	size = round_page(size);
	uvm_km_free(kernel_map, (vaddr_t)kva, size);
@


1.33
log
@Moar <uvm/uvm.h> -> <uvm/uvm_extern.h> love.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.32 2014/03/21 21:49:45 miod Exp $ */
d604 2
a605 2
			panic("_dmapmap_load_buffer: pmap_extract(%x, %x) failed!",
			    pmap, vaddr);
d610 2
a611 2
			panic("Non DMA-reachable buffer at curaddr %p (raw)",
			    curaddr);
@


1.32
log
@Rename the symbolic constants for the pmap-specific vm_pag pg_flags from
PV_xxx to PGF_xxx for consistency (these are not stored in pvlist entries
anymore since years). The PG_ prefix can't be used here because of name
conflicts with <machine/pte.h> names, and I'd rather not rename the pte
constants.

No functional change. But it makes my life easier.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.31 2014/03/10 21:32:15 miod Exp $ */
d64 1
a64 1
#include <uvm/uvm.h>
@


1.31
log
@Support BUS_DMA_NOCACHE in bus_dma(9). Memory allocations done with
BUS_DMA_NOCACHE (or BUS_DMA_COHERENT if the platform does not have coherent
caches) will use PMAP_NOCACHE when invoking pmap_enter(), to avoid creating
cached mappings, and then evicting them from the cache.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.30 2012/10/03 22:46:09 miod Exp $ */
d513 1
a513 1
				    PV_UNCACHED);
@


1.30
log
@Don't include <mips64/archtype.h> unless you really need it.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.29 2012/05/27 14:27:10 miod Exp $ */
d348 1
d353 1
d445 1
a445 1
	int curseg, error;
d459 5
d466 1
a466 2
#ifndef TGT_COHERENT
		if (flags & BUS_DMA_COHERENT)
a468 1
#endif
d482 3
d496 1
a496 1
			    VM_PROT_WRITE | PMAP_WIRED | PMAP_CANFAIL);
d503 9
a511 2
#ifndef TGT_COHERENT
			if (flags & BUS_DMA_COHERENT)
a513 1
#endif
d687 1
a687 1
	vaddr_t curaddr, lastaddr;
@


1.29
log
@Proper support for the so-called `fast mode' of the Indigo2 ECC memory
controller. In this mode, access to physical memory are not allowed to
bypass the cache, and this allows the memory subsystem to run faster.

Of course, some device drivers will require uncached memory access (e.g.
for proper HPC DMA descriptor operation).

New ip22-specific functions to switch between `fast mode' and `slow mode'
are introduced.

hpc(4) now provides read and write routines to fetch a dma descriptor from
uncached memory into a local copy, and update it from said modified copy.
On systems without the ECC MC, these will do nothing and operation will
continue to access the uncached memory directly. On systems with the ECC MC,
they will perform a copy, and the writeback will be done in slow mode.

bus_dmamem_map() requests for DMA memory with BUS_DMA_COHERENT set in flags,
which would return uncached memory, will now always fail on systems with
the ECC memory controller. Drivers which really need uncached memory, and
are aware of this particular setup, will now pass
BUS_DMA_COHERENT | BUS_DMA_BUS1, which will let the request succeed.

sq(4) will use all of the above to work mostly unmodified on ECC MC systems
in fast mode.

Finally, fast mode is enabled after autoconf.

Tested on IP22 and IP28.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.28 2012/05/20 11:41:11 miod Exp $ */
a65 1
#include <mips64/archtype.h>
a67 1
#include <machine/autoconf.h>
@


1.28
log
@Make sure the generic bus_dmamem_alloc() routine restricts its allocation to
the dma_constraint range. This allows the xbridge(4) bus_dma_tag_t to use the
generic routines instead of rolling its own, now that the ATE code has been
removed.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.27 2012/04/21 12:20:30 miod Exp $ */
d73 4
d446 12
@


1.27
log
@Rework the signature of the cache handling routines again. It makes more sense
to pass both the virtual and physical addresses of the page to clean to
SyncDCachePage, which is the only routine using `Index' operations on the data
cache, which might be virtually indexed at some levels but physically indexed
at others. On the other hand, it does not make any sense to pass a physical
address to routines using `Hit' operations (and they were discarding them
anyway).

In addition to making things cleaner, this fixes sporadic userland misbehaviour
(read: SIGSGEV) on RM7000 O2 systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.26 2012/04/05 21:49:58 miod Exp $ */
d397 2
a398 1
	    segs, nsegs, rsegs, flags, (vaddr_t)0, (vaddr_t)-1);
d657 1
a657 1
    int flags, vaddr_t low, vaddr_t high)
@


1.26
log
@Bail out of bus_dmamap_sync() earlier in the non-PREWRITE cases on TGT_COHERENT
kernels. No need to walk the whole dma map doing nothing.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.25 2012/03/28 20:35:41 miod Exp $ */
d354 1
a354 2
			Mips_IOSyncDCache(ci, vaddr, paddr,
			    ssize, CACHE_SYNC_W);
d364 1
a364 1
					Mips_IOSyncDCache(ci, vaddr, paddr,
d367 1
a367 1
					Mips_IOSyncDCache(ci, vaddr, paddr,
d371 1
a371 1
				Mips_IOSyncDCache(ci, vaddr, paddr,
@


1.25
log
@Allow dma map boundary smaller than the kernel page size to work in
bus_dmamap_load*().
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.24 2012/03/25 13:52:52 miod Exp $ */
d312 1
a312 1
	struct cpu_info *ci = curcpu();
d314 6
d352 5
a363 4
#ifdef TGT_COHERENT
				Mips_IOSyncDCache(ci, vaddr, paddr,
				    ssize, CACHE_SYNC_W);
#else
a369 1
#endif
a371 2
#ifdef TGT_COHERENT
#else
d374 1
a375 1
			}
d382 2
a383 1
	if (size != 0) {
d385 1
a385 1
	}
d467 1
d470 1
d519 1
a519 1
		if (off & PGOFSET)
d521 1
a521 1
		if (segs[i].ds_addr & PGOFSET)
d523 1
a523 1
		if (segs[i].ds_len & PGOFSET)
d583 1
a583 1
			    curaddr);                                           
@


1.24
log
@Move cache handling routines related definitions to a dedicated header file,
rather than abusing <machine/cpu.h>.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.23 2012/03/15 18:57:22 miod Exp $ */
d608 1
a608 1
			if ((bus_addr_t)curaddr == lastaddr &&
d626 1
a626 1
		lastaddr = (bus_addr_t)curaddr + sgsize;
@


1.23
log
@uncached_base was introduced early in IP27 support, since these designs use
subspaces in the CCA_NC uncached memory space. However, being coherent,
there was never a need for bus_dma to use uncached addresses.

This means that, on the only systems where uncached_base was not set to
PHYS_TO_XKPHYS(0, CCA_NC), it was never used.

Remove the variable, and replace PHYS_TO_UNCACHED() with
PHYS_TO_XKPHYS(, CCA_NC). No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.22 2011/06/23 20:44:39 ariane Exp $ */
d67 1
a309 3
#define SYNC_R 0	/* WB invalidate, WT invalidate */
#define SYNC_W 1	/* WB writeback + invalidate, WT unaffected */
#define SYNC_X 2	/* WB writeback + invalidate, WT invalidate */
d355 1
a355 1
				    ssize, SYNC_W);
d359 1
a359 1
					    ssize, SYNC_X);
d362 1
a362 1
					    ssize, SYNC_W);
d369 1
a369 1
				    ssize, SYNC_R);
@


1.22
log
@Fix the error path in bus_dmamem_map.
As discussed on icb: remove the comment,
remove pmap_remove (uvm_km_free does that for us).

ok oga@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.21 2011/04/03 22:33:55 miod Exp $ */
a439 5
#ifdef TGT_COHERENT
	if (ISSET(flags, BUS_DMA_COHERENT))
		CLR(flags, BUS_DMA_COHERENT);
#endif

d442 1
d444 1
a444 1
			*kvap = (caddr_t)PHYS_TO_UNCACHED(pa);
d446 1
d476 1
d480 1
@


1.21
log
@Add a consistency check for the value returned by pmap_extract() against the
dma_constraints range in _dmamap_load_buffer. From and ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.20 2010/12/26 15:41:00 miod Exp $ */
a473 4
				/*
				 * Clean up after ourselves.
				 * XXX uvm_wait on WAITOK
				 */
d475 1
a475 1
				uvm_km_free(kernel_map, va, ssize);
@


1.20
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.19 2010/06/26 23:24:44 guenther Exp $ */
d64 1
a64 1
#include <uvm/uvm_extern.h>
d578 7
@


1.19
log
@Don't #include <sys/user.h> into files that don't need the stuff
it defines.  In some cases, this means pulling in uvm.h or pcb.h
instead, but most of the inclusions were just noise.  Tested on
alpha, amd64, armish, hppa, i386, macpcc, sgi, sparc64, and vax,
mostly by krw and naddy.
ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.18 2010/03/29 19:21:58 oga Exp $ */
d532 1
a532 1
		return (atop((*t->_device_to_pa)(segs[i].ds_addr) + off));
@


1.18
log
@PMAP_CANFAIL for bus_dmamem_map on all other architectures (and some
whitespace tweaks on i386 so that it matches).

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.17 2010/01/09 23:34:29 miod Exp $ */
a62 1
#include <sys/user.h>
@


1.17
log
@Move cache information from global variables to per-cpu_info fields; this
allows processors with different cache sizes to be used.

Cache management routines now take a struct cpu_info * as first parameter.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.16 2009/12/25 21:02:18 miod Exp $ */
d435 2
a436 1
	vaddr_t va;
d439 1
a439 1
	int curseg;
d462 2
d471 12
a482 3
			pmap_enter(pmap_kernel(), va, pa,
			    VM_PROT_READ | VM_PROT_WRITE,
			    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
a504 2
	pmap_remove(pmap_kernel(), (vaddr_t)kva, (vaddr_t)kva + size);
	pmap_update(pmap_kernel());
@


1.16
log
@Pass both the virtual address and the physical address of the memory range
when invoking the cache functions. The physical address is needed when
operating on physically-indexed caches, such as the L2 cache on Loongson
processors.

Preprocessor abuse makes sure that the physical address computation gets
compiled out when running on a kernel compiled for virtually-indexed
caches only, such as the sgi kernel.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.15 2009/10/14 21:26:54 miod Exp $ */
d315 1
d357 2
a358 1
				Mips_IOSyncDCache(vaddr, paddr, ssize, SYNC_W);
d361 2
a362 2
					Mips_IOSyncDCache(vaddr, paddr, ssize,
					    SYNC_X);
d364 2
a365 2
					Mips_IOSyncDCache(vaddr, paddr, ssize,
					    SYNC_W);
d371 2
a372 1
				Mips_IOSyncDCache(vaddr, paddr, ssize, SYNC_R);
@


1.15
log
@On coherent systems, all bus_dmamap_sync() needs to do is writebacks, no
invalidation is necessary. Help jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.14 2009/07/17 18:06:51 miod Exp $ */
d320 2
a321 1
		bus_addr_t vaddr;
d325 1
d334 1
d356 1
a356 1
				Mips_IOSyncDCache(vaddr, ssize, SYNC_W);
d359 2
a360 1
					Mips_IOSyncDCache(vaddr, ssize, SYNC_X);
d362 2
a363 1
					Mips_IOSyncDCache(vaddr, ssize, SYNC_W);
d369 1
a369 1
				Mips_IOSyncDCache(vaddr, ssize, SYNC_R);
d592 2
a593 1
			map->dm_segs[seg]._ds_vaddr = (vaddr_t)vaddr;
d609 2
a610 1
				map->dm_segs[seg]._ds_vaddr = (vaddr_t)vaddr;
@


1.14
log
@Update bus_dma to the better codebase found on almost all other platforms,
where the common part to all bus_dmamap_load*() functions is implemented in
in an internal load_buffer routine.

This allows the xbridge-specific dma code to only provide this function,
instead of three; and this also brings us a working bus_dmamap_load_uio()
on all supported sgi machines, which in turns make crpyto(4) devices really
work. Tested with hifn(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.13 2009/05/24 17:31:07 miod Exp $ */
a318 9
#ifdef DEBUG_BUSDMASYNC
	printf("dmasync %p:%p:%p:", map, addr, size);
	if (op & BUS_DMASYNC_PREWRITE) printf("PRW ");
	if (op & BUS_DMASYNC_PREREAD) printf("PRR ");
	if (op & BUS_DMASYNC_POSTWRITE) printf("POW ");
	if (op & BUS_DMASYNC_POSTREAD) printf("POR ");
	printf("\n");
#endif

a344 8
#ifdef DEBUG_BUSDMASYNC_FRAG
	printf(" syncing %p:%p ", vaddr, ssize);
	if (op & BUS_DMASYNC_PREWRITE) printf("PRW ");
	if (op & BUS_DMASYNC_PREREAD) printf("PRR ");
	if (op & BUS_DMASYNC_POSTWRITE) printf("POW ");
	if (op & BUS_DMASYNC_POSTREAD) printf("POR ");
	printf("\n");
#endif
d346 4
a349 6
			 *  If only PREWRITE is requested, writeback and
			 *  invalidate. PREWRITE with PREREAD writebacks
			 *  and invalidates *all* cache levels.
			 *  Otherwise, just invalidate.
			 *  POSTREAD and POSTWRITE are no-ops since
			 *  we are not bouncing data.
d352 3
d359 5
a363 1
			} else if (op & (BUS_DMASYNC_PREREAD|BUS_DMASYNC_POSTREAD)) {
d365 1
@


1.13
log
@Revert the memory range restriction code added to _dmamem_alloc(); devices
which require it will provide their own _dmamem_alloc() in their own
bus_dma_tag_t.

While there, rename bus_dma_segment_t ds_vaddr member to _ds_vaddr to make
it clear this is an internal member.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.12 2009/05/08 20:55:20 miod Exp $ */
d28 29
a71 1
#include <sgi/sgi/ip30.h>
d78 2
a79 8
_dmamap_create(t, size, nsegments, maxsegsz, boundary, flags, dmamp)
	bus_dma_tag_t t;
	bus_size_t size;
	int nsegments;
	bus_size_t maxsegsz;
	bus_size_t boundary;
	int flags;
	bus_dmamap_t *dmamp;
d129 2
a130 7
_dmamap_load(t, map, buf, buflen, p, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	void *buf;
	bus_size_t buflen;
	struct proc *p;
	int flags;
d132 2
a133 6
	bus_size_t sgsize;
	bus_addr_t curaddr, lastaddr, baddr, bmask;
	caddr_t vaddr = buf;
	int first, seg;
	pmap_t pmap;
	bus_size_t saved_buflen;
d144 6
a149 67
	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = ~0;		/* XXX gcc */
	bmask  = ~(map->_dm_boundary - 1);
	if (t->_dma_mask != 0)
		bmask &= t->_dma_mask;

	saved_buflen = buflen;
	for (first = 1, seg = 0; buflen > 0; ) {
		/*
		 * Get the physical address for this segment.
		 */
		if (pmap_extract(pmap, (vaddr_t)vaddr, (paddr_t *)&curaddr) ==
		    FALSE)
			panic("_dmapmap_load: pmap_extract(%x, %x) failed!",
			    pmap, vaddr);

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = NBPG - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = (curaddr + map->_dm_boundary) & bmask;
			if (sgsize > (baddr - curaddr))
				sgsize = (baddr - curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr =
			    (*t->_pa_to_device)(curaddr);
			map->dm_segs[seg].ds_len = sgsize;
			map->dm_segs[seg]._ds_vaddr = (vaddr_t)vaddr;
			first = 0;
		} else {
			if (curaddr == lastaddr &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			     (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     (curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr =
				    (*t->_pa_to_device)(curaddr);
				map->dm_segs[seg].ds_len = sgsize;
				map->dm_segs[seg]._ds_vaddr = (vaddr_t)vaddr;
			}
		}

		lastaddr = curaddr + sgsize;
		vaddr += sgsize;
		buflen -= sgsize;
d152 1
a152 9
	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */

	map->dm_nsegs = seg + 1;
	map->dm_mapsize = saved_buflen;
	return (0);
d159 4
a162 3
_dmamap_load_mbuf(t, map, m, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
a163 4
	int flags;
{
	int i;
	size_t len;
d165 3
d171 21
a191 35
	i = 0;
	len = 0;
	while (m) {
		vaddr_t vaddr = mtod(m, vaddr_t);
		long buflen = (long)m->m_len;

		len += buflen;
		while (buflen > 0 && i < map->_dm_segcnt) {
			paddr_t pa;
			long incr;

			incr = min(buflen, NBPG);
			buflen -= incr;
			if (pmap_extract(pmap_kernel(), vaddr, &pa) == FALSE)
				panic("_dmamap_load_mbuf: pmap_extract(%x, %x) failed!",
				    pmap_kernel(), vaddr);

			if (i > 0 && pa == (*t->_device_to_pa)(map->dm_segs[i-1].ds_addr + map->dm_segs[i-1].ds_len)
			    && ((map->dm_segs[i-1].ds_len + incr) < map->_dm_maxsegsz)) {
				/* Hey, waddyaknow, they're contiguous */
				map->dm_segs[i-1].ds_len += incr;
				continue;
			}
			map->dm_segs[i].ds_addr =
			    (*t->_pa_to_device)(pa);
			map->dm_segs[i]._ds_vaddr = vaddr;
			map->dm_segs[i].ds_len = incr;
			i++;
			vaddr += incr;
		}
		m = m->m_next;
		if (m && i >= map->_dm_segcnt) {
			/* Exceeded the size of our dmamap */
			return EFBIG;
		}
d193 2
a194 3
	map->dm_nsegs = i;
	map->dm_mapsize = len;
	return (0);
d201 1
a201 5
_dmamap_load_uio(t, map, uio, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct uio *uio;
	int flags;
d203 47
a249 1
	return (EOPNOTSUPP);
d257 2
a258 7
_dmamap_load_raw(t, map, segs, nsegs, size, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_dma_segment_t *segs;
	int nsegs;
	bus_size_t size;
	int flags;
d292 1
a292 3
_dmamap_unload(t, map)
	bus_dma_tag_t t;
	bus_dmamap_t map;
a293 1

d307 2
a308 6
_dmamap_sync(t, map, addr, size, op)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_addr_t addr;
	bus_size_t size;
	int op;
d394 3
a396 7
_dmamem_alloc(t, size, alignment, boundary, segs, nsegs, rsegs, flags)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
d407 1
a407 4
_dmamem_free(t, segs, nsegs)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
d435 2
a436 7
_dmamem_map(t, segs, nsegs, size, kvap, flags)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
	size_t size;
	caddr_t *kvap;
	int flags;
d490 1
a490 4
_dmamem_unmap(t, kva, size)
	bus_dma_tag_t t;
	caddr_t kva;
	size_t size;
d506 2
a507 6
_dmamem_mmap(t, segs, nsegs, off, prot, flags)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
	off_t off;
	int prot, flags;
d538 98
d640 3
a642 10
_dmamem_alloc_range(t, size, alignment, boundary, segs, nsegs, rsegs,
    flags, low, high)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
	vaddr_t low;
	vaddr_t high;
@


1.12
log
@In _dmamem_alloc_range() DIAGNOSTIC code, check for the address being within
the expected range, before invoking pa_to_device().
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.10 2009/04/20 00:42:06 oga Exp $ */
d175 1
a175 1
			map->dm_segs[seg].ds_vaddr = (vaddr_t)vaddr;
d191 1
a191 1
				map->dm_segs[seg].ds_vaddr = (vaddr_t)vaddr;
d252 1
a252 1
			map->dm_segs[i].ds_vaddr = vaddr;
d374 1
a374 1
		vaddr = map->dm_segs[curseg].ds_vaddr;
a442 28
	vaddr_t low;
	vaddr_t high;

	/*
	 * Limit bus_dma'able memory to the first 2GB of physical memory.
	 * XXX This should be lifted if flags & BUS_DMA_64BIT for
	 * XXX drivers which do not need 32 bit DMA on IP27/30/35.
	 */
	switch (sys_config.system_type) {
#if defined(TGT_OCTANE)
	case SGI_OCTANE:
		low = IP30_MEMORY_BASE;
		high = (2U << 30) - 1 + IP30_MEMORY_BASE;
		break;
#endif
#if defined(TGT_ORIGIN200) || defined(TGT_ORIGIN2000)
	case SGI_O200:
	case SGI_O300:
		low = 0;
		high = (2U << 30) - 1;
		break;
#endif
	default:
		low = 0;
		high = (vaddr_t)-1;
		break;
	}

d444 1
a444 1
	    segs, nsegs, rsegs, flags, low, high);
d496 5
a502 1
#ifndef TGT_COHERENT
a505 1
#endif
a526 1
			segs[curseg].ds_vaddr = va;
d528 1
a528 2
			if (flags & BUS_DMA_COHERENT &&
			    sys_config.system_type == SGI_O2)
@


1.11
log
@Add a new page freelist, to which memory suitable for 32-bit dma on
xbridge(4) is assigned. Then, make bus_dmamem_alloc() allocate from this
range only.

This is transparent on O2, and makes sure the bus_dma memory address
will fit in the 2GB direct map of xbridge(4) chips - this is necessary for
PCI devices which do not handle 64 bit dma addresses.
@
text
@d673 1
a673 1
		curaddr = (*t->_pa_to_device)(VM_PAGE_TO_PHYS(m));
d681 1
@


1.10
log
@Add a BUS_DMA_ZERO flag for bus_dmamem_alloc() to return zeroed memory.

Saves every damned driver calling bzero(), and continues the M_ZERO,
PR_ZERO symmetry.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.9 2009/04/19 18:34:36 miod Exp $ */
d43 1
d443 30
a472 2
	return (_dmamem_alloc_range(t, size, alignment, boundary,
	    segs, nsegs, rsegs, flags, (paddr_t)0, (paddr_t)-1));
@


1.9
log
@Correctly handle bus_dma_tag with _dma_mask == 0.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.8 2009/04/14 16:01:04 oga Exp $ */
d623 2
@


1.8
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.7 2009/03/07 15:34:34 miod Exp $ */
d137 2
a138 1
	bmask &= t->_dma_mask;
d303 2
a304 1
		bmask &= t->_dma_mask;
d526 1
a526 1
			    sys_config.system_type == SGI_O2) 
@


1.7
log
@When allocating memory in bus_dmamem_alloc() with uvm_pglistalloc(), do not
try to be smart for the address range, uvm_pglistalloc() is smart enough
nowadays.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.6 2008/07/16 15:49:22 miod Exp $ */
d612 1
a612 1
	int curseg, error;
d620 2
d623 2
a624 2
	error = uvm_pglistalloc(size, low, high,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
@


1.6
log
@Now that uvm_pglistalloc() does not lose on large memory gaps, do not
restrict the memory allocation range in _dmamem_alloc().
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.5 2008/04/07 22:30:49 miod Exp $ */
d441 1
a441 1
	    segs, nsegs, rsegs, flags, 0, -1));
@


1.5
log
@Use CCA_CACHED as the default CCA for all cached mappings and addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.4 2008/04/07 17:25:20 miod Exp $ */
d441 1
a441 1
	    segs, nsegs, rsegs, flags, 0, 0xf0000000));
@


1.4
log
@In _dmamem_map(), be sure to convert the address from the device view to
a real physical address in the single-segment short-circuit code.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.3 2007/10/02 00:59:12 krw Exp $ */
d495 1
d497 1
a497 1
			*kvap = (caddr_t)PHYS_TO_XKPHYS(pa, CCA_NC);
d499 2
a500 1
			*kvap = (caddr_t)PHYS_TO_XKPHYS(pa, CCA_NONCOHERENT);
@


1.3
log
@Apply (with slight variants) this elimination of bzero() with M_ZERO:

-	if ((mapstore = malloc(mapsize, M_DEVBUF,
-	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
+	if ((mapstore = malloc(mapsize, M_DEVBUF, (flags & BUS_DMA_NOWAIT) ?
+	    (M_NOWAIT | M_ZERO) : (M_WAITOK | M_ZERO))) == NULL)
 		return (ENOMEM);

-	bzero(mapstore, mapsize);
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.2 2007/07/18 20:03:51 miod Exp $ */
d494 1
d496 1
a496 2
			*kvap = (caddr_t)PHYS_TO_XKPHYS(segs[0].ds_addr,
			    CCA_NC);
d498 1
a498 2
			*kvap = (caddr_t)PHYS_TO_XKPHYS(segs[0].ds_addr,
			    CCA_NONCOHERENT);
@


1.2
log
@bus_dmamem_map() maps with a single segment in directly-translated XKPHYS
space, either cache coherent for regular mappings and uncached for
BUS_DMA_COHERENT mappings, as done on all other platforms with direct mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: bus_dma.c,v 1.1 2007/06/21 20:17:10 miod Exp $ */
d76 2
a77 2
	if ((mapstore = malloc(mapsize, M_DEVBUF,
	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
a79 1
	bzero(mapstore, mapsize);
@


1.1
log
@Extent sgi bus_dma to cope with different views of memory: non-contiguous
for the cpu, contiguous from different bases for devices. This allows
memory above 256MB to be used with bus_dma (and we had really been lucky
with the first few large-memory builds).
Information about memory accesses taken from Linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: busdma.c,v 1.13 2007/05/30 19:44:26 miod Exp $ */
d374 1
a374 1
		if (addr > 0) {
d384 1
a384 1
		if (ssize > size) {
d386 4
d392 1
a392 1
		if (ssize) {
d490 1
d494 10
d517 2
a518 1
			pmap_enter(pmap_kernel(), va, (*t->_device_to_pa)(addr),
d525 2
a526 1
				pmap_page_cache(PHYS_TO_VM_PAGE((*t->_device_to_pa)(addr)), PV_UNCACHED);
d528 1
d544 2
a545 5

#ifdef DIAGNOSTIC
	if ((u_long)kva & PGOFSET)
		panic("_dmamem_unmap");
#endif
d548 2
d554 1
a554 1
 * Common functin for mmap(2)'ing DMA-safe memory.  May be called by
@

