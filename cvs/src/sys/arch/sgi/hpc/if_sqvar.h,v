head	1.6;
access;
symbols
	OPENBSD_6_2:1.6.0.10
	OPENBSD_6_2_BASE:1.6
	OPENBSD_6_1:1.6.0.8
	OPENBSD_6_1_BASE:1.6
	OPENBSD_6_0:1.6.0.4
	OPENBSD_6_0_BASE:1.6
	OPENBSD_5_9:1.6.0.2
	OPENBSD_5_9_BASE:1.6
	OPENBSD_5_8:1.4.0.16
	OPENBSD_5_8_BASE:1.4
	OPENBSD_5_7:1.4.0.8
	OPENBSD_5_7_BASE:1.4
	OPENBSD_5_6:1.4.0.12
	OPENBSD_5_6_BASE:1.4
	OPENBSD_5_5:1.4.0.10
	OPENBSD_5_5_BASE:1.4
	OPENBSD_5_4:1.4.0.6
	OPENBSD_5_4_BASE:1.4
	OPENBSD_5_3:1.4.0.4
	OPENBSD_5_3_BASE:1.4
	OPENBSD_5_2:1.4.0.2
	OPENBSD_5_2_BASE:1.4;
locks; strict;
comment	@ * @;


1.6
date	2015.09.18.20.50.02;	author miod;	state Exp;
branches;
next	1.5;
commitid	WbAU9whMk9BmFUgM;

1.5
date	2015.09.05.21.13.24;	author miod;	state Exp;
branches;
next	1.4;
commitid	1Tj9UMvH0jts1O49;

1.4
date	2012.05.28.17.03.36;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2012.05.27.14.27.08;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2012.04.30.21.31.03;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2012.03.28.20.44.23;	author miod;	state Exp;
branches;
next	;


desc
@@


1.6
log
@Go back to the previous approach when managing individual HPC DMA descriptors:
provide again an optional storage for a copy of the descriptor in the `sync'
(fetch) function, and use the returned address afterwards.

On IP22 systems (in the broader sense of the term, thus IP20/IP22/IP24),
descriptors will remain in uncached memory and no local copies need to be made.
On IP28 systems, descriptors will remain in cached memory (so as to avoid
switching to `slow mode'), but a local copy will be performed with the necessary
cache eviction work, so that speculative code execution on R10000 will not
touch the real descriptor.

With this in place, all the explicit descriptor cache operations in if_sq,
some of them being redundant or operating on the wrong number of
descriptors, can be removed, with the HPC DMA wrappers taking care of doing
the right thing.

Tested on IP22 and IP28. IP26 still unhappy but no worse than before.
@
text
@/*	$OpenBSD: if_sqvar.h,v 1.5 2015/09/05 21:13:24 miod Exp $	*/
/*	$NetBSD: sqvar.h,v 1.12 2011/01/25 13:12:39 tsutsui Exp $	*/

/*
 * Copyright (c) 2001 Rafal K. Boni
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/* Note, these must be powers of two for the magic NEXT/PREV macros to work */
#define SQ_NRXDESC		64
#define SQ_NTXDESC		64

#define	SQ_NRXDESC_MASK		(SQ_NRXDESC - 1)
#define	SQ_NEXTRX(x)		((x + 1) & SQ_NRXDESC_MASK)
#define	SQ_PREVRX(x)		((x - 1) & SQ_NRXDESC_MASK)

#define	SQ_NTXDESC_MASK		(SQ_NTXDESC - 1)
#define	SQ_NEXTTX(x)		((x + 1) & SQ_NTXDESC_MASK)
#define	SQ_PREVTX(x)		((x - 1) & SQ_NTXDESC_MASK)

/*
 * We pack all DMA control structures into one container so we can alloc just
 * one chunk of DMA-safe memory and pack them into it.  Otherwise, we'd have to
 * allocate a page for each descriptor, since the bus_dmamem_alloc() interface
 * does not allow us to allocate smaller chunks.
 */
struct sq_control {
	/* Receive descriptors */
	struct hpc_dma_desc	rx_desc[SQ_NRXDESC];

	/* Transmit descriptors */
	struct hpc_dma_desc	tx_desc[SQ_NTXDESC];
};

#define	SQ_CDOFF(x)		offsetof(struct sq_control, x)
#define	SQ_CDTXOFF(x)		SQ_CDOFF(tx_desc[(x)])
#define	SQ_CDRXOFF(x)		SQ_CDOFF(rx_desc[(x)])

#define	SQ_TYPE_8003		0
#define	SQ_TYPE_80C03		1

/* Trace Actions */
#define SQ_RESET		1
#define SQ_ADD_TO_DMA		2
#define SQ_START_DMA		3
#define SQ_DONE_DMA		4
#define SQ_RESTART_DMA		5
#define SQ_TXINTR_ENTER		6
#define SQ_TXINTR_EXIT		7
#define SQ_TXINTR_BUSY		8
#define SQ_IOCTL		9
#define SQ_ENQUEUE		10

struct sq_action_trace {
	int action;
	int line;
	int bufno;
	int status;
	int freebuf;
};

#ifdef SQ_DEBUG
#define SQ_TRACEBUF_SIZE	100

#define SQ_TRACE(act, sc, buf, stat) do {				\
	(sc)->sq_trace[(sc)->sq_trace_idx].action = (act);		\
	(sc)->sq_trace[(sc)->sq_trace_idx].line = __LINE__;		\
	(sc)->sq_trace[(sc)->sq_trace_idx].bufno = (buf);		\
	(sc)->sq_trace[(sc)->sq_trace_idx].status = (stat);		\
	(sc)->sq_trace[(sc)->sq_trace_idx].freebuf = (sc)->sc_nfreetx;	\
	if (++(sc)->sq_trace_idx == SQ_TRACEBUF_SIZE)			\
		(sc)->sq_trace_idx = 0;					\
} while (/* CONSTCOND */0)
#else
#define SQ_TRACE(act, sc, buf, stat) do { } while (/* CONSTCOND */0)
#endif

struct sq_softc {
	struct device		sc_dev;

	/* HPC registers */
	bus_space_tag_t		sc_hpct;
	bus_space_handle_t	sc_hpcbh;	/* HPC base, for IOC access */
	bus_space_handle_t	sc_hpch;

	/* HPC external Ethernet registers: aka Seeq 8003 registers */
	bus_space_tag_t		sc_regt;
	bus_space_handle_t	sc_regh;

	bus_dma_tag_t		sc_dmat;

	struct arpcom		sc_ac;
	uint8_t			sc_enaddr[ETHER_ADDR_LEN];
	struct ifmedia		sc_ifmedia;

	int			sc_type;
	int			sc_flags;
#define	SQF_LINKUP			0x00000001
#define	SQF_NOLINKDOWN			0x00000002

	struct sq_control*	sc_control;
#define	sc_rxdesc		sc_control->rx_desc
#define	sc_txdesc		sc_control->tx_desc

	/* DMA structures for control data (DMA RX/TX descriptors) */
	int			sc_ncdseg;
	bus_dma_segment_t	sc_cdseg;
	bus_dmamap_t		sc_cdmap;
#define	sc_cddma		sc_cdmap->dm_segs[0].ds_addr

	int			sc_nextrx;

	/* DMA structures for RX packet data */
	bus_dma_segment_t	sc_rxseg[SQ_NRXDESC];
	bus_dmamap_t		sc_rxmap[SQ_NRXDESC];
	struct mbuf*		sc_rxmbuf[SQ_NRXDESC];

	int			sc_nexttx;
	int			sc_prevtx;
	int			sc_nfreetx;

	/* DMA structures for TX packet data */
	bus_dma_segment_t	sc_txseg[SQ_NTXDESC];
	bus_dmamap_t		sc_txmap[SQ_NTXDESC];
	struct mbuf*		sc_txmbuf[SQ_NTXDESC];

	uint8_t			sc_txcmd;	/* current value of TXCMD */
	uint8_t			sc_rxcmd;	/* prototype rxcmd */

	struct hpc_values       *hpc_regs;      /* HPC register definitions */

#ifdef SQ_DEBUG
	int			sq_trace_idx;
	struct sq_action_trace	sq_trace[SQ_TRACEBUF_SIZE];
#endif
};

#define	SQ_CDTXADDR(sc, x)	((sc)->sc_cddma + SQ_CDTXOFF((x)))
#define	SQ_CDRXADDR(sc, x)	((sc)->sc_cddma + SQ_CDRXOFF((x)))

static inline void
SQ_INIT_RXDESC(struct sq_softc *sc, unsigned int x)
{
	struct hpc_dma_desc *__rxd, *__active, __store;
	struct mbuf *__m = (sc)->sc_rxmbuf[(x)];

	__rxd = &(sc)->sc_rxdesc[(x)];
	__active = hpc_sync_dma_desc(__rxd, &__store);
	__m->m_data = __m->m_ext.ext_buf;
	if (sc->hpc_regs->revision == 3) {
		__active->hpc3_hdd_bufptr =
		    (sc)->sc_rxmap[(x)]->dm_segs[0].ds_addr;
		__active->hpc3_hdd_ctl = __m->m_ext.ext_size |
		    HPC3_HDD_CTL_OWN | HPC3_HDD_CTL_INTR |
		    HPC3_HDD_CTL_EOPACKET |
		    ((x) == (SQ_NRXDESC  - 1) ? HPC3_HDD_CTL_EOCHAIN : 0);
	} else {
		__active->hpc1_hdd_bufptr =
		    (sc)->sc_rxmap[(x)]->dm_segs[0].ds_addr |
		    ((x) == (SQ_NRXDESC - 1) ? HPC1_HDD_CTL_EOCHAIN : 0);
		__active->hpc1_hdd_ctl = __m->m_ext.ext_size |
		    HPC1_HDD_CTL_OWN | HPC1_HDD_CTL_INTR |
		    HPC1_HDD_CTL_EOPACKET;
	}
	__active->hdd_descptr = SQ_CDRXADDR((sc), SQ_NEXTRX((x)));
	hpc_update_dma_desc(__rxd, __active);
}
@


1.5
log
@Give up trying to map DMA descriptor in uncached memory on ECC flavours of the
IP22 motherboard (IP26, IP28). Instead, do not ask for a BUS_DMA_COHERENT
mapping, but perform explicit cache operations.

This removes the need for the memory controller to switch between `fast' and
`slow' mode every time a DMA descriptor is updated.

Tested on IP22 and IP28.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_sqvar.h,v 1.4 2012/05/28 17:03:36 miod Exp $	*/
a163 27
SQ_CDTXSYNC(struct sq_softc *sc, int __x, int __n, int ops)
{
	if (!ip22_ecc)
		return;

	/* If it will wrap around, sync to the end of the ring. */
	if ((__x + __n) > SQ_NTXDESC) {
		bus_dmamap_sync((sc)->sc_dmat, (sc)->sc_cdmap,
		    SQ_CDTXOFF(__x), sizeof(struct hpc_dma_desc) *
		    (SQ_NTXDESC - __x), (ops));
		__n -= (SQ_NTXDESC - __x);
		__x = 0;
	}

	/* Now sync whatever is left. */
	bus_dmamap_sync((sc)->sc_dmat, (sc)->sc_cdmap,
	    SQ_CDTXOFF(__x), sizeof(struct hpc_dma_desc) * __n, (ops));
}

#define	SQ_CDRXSYNC(sc, x, ops)						\
do {									\
	if (ip22_ecc)							\
		bus_dmamap_sync((sc)->sc_dmat, (sc)->sc_cdmap,		\
		    SQ_CDRXOFF((x)), sizeof(struct hpc_dma_desc), (ops)); \
} while (0)

static inline void
d166 1
a166 1
	struct hpc_dma_desc *__rxd;
d170 1
a170 1
	hpc_sync_dma_desc(__rxd);
d173 1
a173 1
		__rxd->hpc3_hdd_bufptr =
d175 3
a177 2
		__rxd->hpc3_hdd_ctl = __m->m_ext.ext_size | HPC3_HDD_CTL_OWN |
		    HPC3_HDD_CTL_INTR | HPC3_HDD_CTL_EOPACKET |
d180 1
a180 1
		__rxd->hpc1_hdd_bufptr =
d183 3
a185 2
		__rxd->hpc1_hdd_ctl = __m->m_ext.ext_size | HPC1_HDD_CTL_OWN |
		    HPC1_HDD_CTL_INTR | HPC1_HDD_CTL_EOPACKET;
d187 2
a188 3
	__rxd->hdd_descptr = SQ_CDRXADDR((sc), SQ_NEXTRX((x)));
	hpc_update_dma_desc(__rxd);
	SQ_CDRXSYNC((sc), (x), BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
@


1.4
log
@The link state code does not work correctly on Indigo (IP20) and E++ GIO boards
and will report the link being down too aggressively. Better to always report
the link as up - these systems and boards are single media only so it won't
harm much.

Unbreaks dhcp in the installer on these interfaces; found the hard way by
sebastia@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_sqvar.h,v 1.3 2012/05/27 14:27:08 miod Exp $	*/
a162 1
#if 0 /* not necessary as this memory is mapped uncached */
d166 3
d184 5
a188 6
	bus_dmamap_sync((sc)->sc_dmat, (sc)->sc_cdmap,			\
	    SQ_CDRXOFF((x)), sizeof(struct hpc_dma_desc), (ops))
#else
#define	SQ_CDTXSYNC(sc, x, n, ops)	do { } while (0)
#define	SQ_CDRXSYNC(sc, x, ops)		do { } while (0)
#endif
d193 1
a193 1
	struct hpc_dma_desc *__rxd, rxd_store;
d196 2
a197 1
	__rxd = hpc_read_dma_desc(&(sc)->sc_rxdesc[(x)], &rxd_store);
d213 1
a213 1
	hpc_write_dma_desc(&(sc)->sc_rxdesc[(x)], __rxd);
@


1.3
log
@Proper support for the so-called `fast mode' of the Indigo2 ECC memory
controller. In this mode, access to physical memory are not allowed to
bypass the cache, and this allows the memory subsystem to run faster.

Of course, some device drivers will require uncached memory access (e.g.
for proper HPC DMA descriptor operation).

New ip22-specific functions to switch between `fast mode' and `slow mode'
are introduced.

hpc(4) now provides read and write routines to fetch a dma descriptor from
uncached memory into a local copy, and update it from said modified copy.
On systems without the ECC MC, these will do nothing and operation will
continue to access the uncached memory directly. On systems with the ECC MC,
they will perform a copy, and the writeback will be done in slow mode.

bus_dmamem_map() requests for DMA memory with BUS_DMA_COHERENT set in flags,
which would return uncached memory, will now always fail on systems with
the ECC memory controller. Drivers which really need uncached memory, and
are aware of this particular setup, will now pass
BUS_DMA_COHERENT | BUS_DMA_BUS1, which will let the request succeed.

sq(4) will use all of the above to work mostly unmodified on ECC MC systems
in fast mode.

Finally, fast mode is enabled after autoconf.

Tested on IP22 and IP28.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_sqvar.h,v 1.2 2012/04/30 21:31:03 miod Exp $	*/
d121 1
@


1.2
log
@Add ifmedia support to sq(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: if_sqvar.h,v 1.1 2012/03/28 20:44:23 miod Exp $	*/
d162 1
d183 4
d191 1
a191 1
	struct hpc_dma_desc* __rxd = &(sc)->sc_rxdesc[(x)];
d194 1
d210 1
@


1.1
log
@Work in progress support for the SGI Indigo, Indigo 2 and Indy systems
(IP20, IP22, IP24) in 64-bit mode, adapated from NetBSD. Currently limited
to headless operation, input and video drivers will get ported soon.

Should work on all R4000, R4440 and R5000 based systems. L2 cache on R5000SC
Indy not supported yet (coming soon), R4600 not supported yet either (coming
soon as well).

Tested to boot multiuser on: Indigo2 R4000SC, Indy R4000PC, Indy R4000SC,
Indy R5000SC, Indigo2 R4400SC. There are still glitches in the Ethernet driver
which are being looked at.

Expansion support is limited to the GIO E++ board; GIO boards with PCI-GIO
bridges not ported yet due to the lack of hardware, and this kind of driver
does not port blindly.

Most of this work comes from NetBSD, polishing and integration work, as well
as putting as many ``R4x00 in 64-bit mode'' erratas as necessary, by yours
truly.

More work is coming, as well as trying to get some easy way to boot install
kernels (as older PROM can only boot ECOFF binaries, which won't do for the
kernel).
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d105 1
d116 1
d119 2
d148 1
@

