head	1.14;
access;
symbols
	OPENBSD_6_1:1.14.0.12
	OPENBSD_6_1_BASE:1.14
	OPENBSD_6_0:1.14.0.10
	OPENBSD_6_0_BASE:1.14
	OPENBSD_5_9:1.14.0.6
	OPENBSD_5_9_BASE:1.14
	OPENBSD_5_8:1.14.0.8
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.14.0.2
	OPENBSD_5_7_BASE:1.14
	OPENBSD_5_6:1.14.0.4
	OPENBSD_5_6_BASE:1.14
	OPENBSD_5_5:1.12.0.18
	OPENBSD_5_5_BASE:1.12
	OPENBSD_5_4:1.12.0.14
	OPENBSD_5_4_BASE:1.12
	OPENBSD_5_3:1.12.0.12
	OPENBSD_5_3_BASE:1.12
	OPENBSD_5_2:1.12.0.10
	OPENBSD_5_2_BASE:1.12
	OPENBSD_5_1_BASE:1.12
	OPENBSD_5_1:1.12.0.8
	OPENBSD_5_0:1.12.0.6
	OPENBSD_5_0_BASE:1.12
	OPENBSD_4_9:1.12.0.4
	OPENBSD_4_9_BASE:1.12
	OPENBSD_4_8:1.12.0.2
	OPENBSD_4_8_BASE:1.12
	OPENBSD_4_7:1.11.0.6
	OPENBSD_4_7_BASE:1.11
	OPENBSD_4_6:1.11.0.8
	OPENBSD_4_6_BASE:1.11
	OPENBSD_4_5:1.11.0.4
	OPENBSD_4_5_BASE:1.11
	OPENBSD_4_4:1.11.0.2
	OPENBSD_4_4_BASE:1.11
	OPENBSD_4_3:1.10.0.8
	OPENBSD_4_3_BASE:1.10
	OPENBSD_4_2:1.10.0.6
	OPENBSD_4_2_BASE:1.10
	OPENBSD_4_1:1.10.0.4
	OPENBSD_4_1_BASE:1.10
	OPENBSD_4_0:1.10.0.2
	OPENBSD_4_0_BASE:1.10
	OPENBSD_3_9:1.4.0.6
	OPENBSD_3_9_BASE:1.4
	OPENBSD_3_8:1.4.0.4
	OPENBSD_3_8_BASE:1.4
	OPENBSD_3_7:1.4.0.2
	OPENBSD_3_7_BASE:1.4
	OPENBSD_3_6:1.3.0.4
	OPENBSD_3_6_BASE:1.3
	SMP_SYNC_A:1.3
	SMP_SYNC_B:1.3
	OPENBSD_3_5:1.3.0.2
	OPENBSD_3_5_BASE:1.3
	OPENBSD_3_4:1.2.0.12
	OPENBSD_3_4_BASE:1.2
	UBC_SYNC_A:1.2
	OPENBSD_3_3:1.2.0.10
	OPENBSD_3_3_BASE:1.2
	OPENBSD_3_2:1.2.0.8
	OPENBSD_3_2_BASE:1.2
	OPENBSD_3_1:1.2.0.6
	OPENBSD_3_1_BASE:1.2
	UBC_SYNC_B:1.2
	UBC:1.2.0.4
	UBC_BASE:1.2
	OPENBSD_3_0:1.2.0.2
	OPENBSD_3_0_BASE:1.2
	OPENBSD_2_9:1.1.0.4
	OPENBSD_2_9_BASE:1.1
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.14
date	2014.07.11.12.55.32;	author dlg;	state Exp;
branches;
next	1.13;
commitid	ISORLwV6pOrgBpXH;

1.13
date	2014.03.31.21.10.10;	author kettenis;	state Exp;
branches;
next	1.12;

1.12
date	2010.04.10.13.46.12;	author oga;	state Exp;
branches;
next	1.11;

1.11
date	2008.06.26.05.42.08;	author ray;	state Exp;
branches;
next	1.10;

1.10
date	2006.05.21.01.42.43;	author brad;	state Exp;
branches;
next	1.9;

1.9
date	2006.05.21.01.26.19;	author brad;	state Exp;
branches;
next	1.8;

1.8
date	2006.05.12.20.48.19;	author brad;	state Exp;
branches;
next	1.7;

1.7
date	2006.04.13.14.41.08;	author brad;	state Exp;
branches;
next	1.6;

1.6
date	2006.03.27.20.46.44;	author brad;	state Exp;
branches;
next	1.5;

1.5
date	2006.03.13.19.50.07;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2004.11.09.19.17.01;	author claudio;	state Exp;
branches;
next	1.3;

1.3
date	2004.01.13.00.12.15;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2001.06.08.08.08.40;	author art;	state Exp;
branches;
next	1.1;

1.1
date	2001.03.21.17.26.38;	author art;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2001.04.18.16.00.42;	author niklas;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2001.07.04.10.14.33;	author niklas;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2004.02.19.09.59.34;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.14
log
@sgmap loading didnt respect the dmamaps max number of segments.
this let it wanter off writing segment descriptors off in memory
it didnt own, which led to some pretty awesome memory corruption.

if you had a network card with a small number of tx descriptors per
packet, a lot of memory, and a heavily fragmented packet (ie, ssh)
you were basically guaranteed a confusing panic.

ok miod@@
@
text
@/* $OpenBSD: sgmap_typedep.c,v 1.13 2014/03/31 21:10:10 kettenis Exp $ */
/* $NetBSD: sgmap_typedep.c,v 1.17 2001/07/19 04:27:37 thorpej Exp $ */

/*-
 * Copyright (c) 1997, 1998 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifdef SGMAP_DEBUG
int			__C(SGMAP_TYPE,_debug) = 0;
#endif

SGMAP_PTE_TYPE		__C(SGMAP_TYPE,_prefetch_spill_page_pte);

int			__C(SGMAP_TYPE,_load_buffer)(bus_dma_tag_t,
			    bus_dmamap_t, void *buf, size_t buflen,
			    struct proc *, int, int, struct alpha_sgmap *);

void
__C(SGMAP_TYPE,_init_spill_page_pte)(void)
{

	__C(SGMAP_TYPE,_prefetch_spill_page_pte) =
	    (alpha_sgmap_prefetch_spill_page_pa >>
	     SGPTE_PGADDR_SHIFT) | SGPTE_VALID;
}

int
__C(SGMAP_TYPE,_load_buffer)(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    size_t buflen, struct proc *p, int flags, int seg,
    struct alpha_sgmap *sgmap)
{
	vaddr_t endva, va = (vaddr_t)buf;
	paddr_t pa;
	pmap_t pmap;
	bus_addr_t dmaoffset, sgva;
	bus_size_t sgvalen, boundary, alignment;
	struct extent_region *regions = map->_dm_cookie;
	SGMAP_PTE_TYPE *pte, *page_table = sgmap->aps_pt;
	int pteidx, error, spill;

	if (seg >= map->_dm_segcnt)
		return (EFBIG);

	/* Initialize the spill page PTE if it hasn't been already. */
	if (__C(SGMAP_TYPE,_prefetch_spill_page_pte) == 0)
		__C(SGMAP_TYPE,_init_spill_page_pte)();

	/*
	 * Remember the offset into the first page and the total
	 * transfer length.
	 */
	dmaoffset = ((u_long)buf) & PGOFSET;

#ifdef SGMAP_DEBUG
	if (__C(SGMAP_TYPE,_debug)) {
		printf("sgmap_load: ----- buf = %p -----\n", buf);
		printf("sgmap_load: dmaoffset = 0x%lx, buflen = 0x%lx\n",
		    dmaoffset, buflen);
	}
#endif
	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	/*
	 * Allocate the necessary virtual address space for the
	 * mapping.  Round the size, since we deal with whole pages.
	 */

	/*
	 * XXX Always allocate a spill page for now.  Note
	 * the spill page is not needed for an in-bound-only
	 * transfer.
	 */
	if ((flags & BUS_DMA_READ) == 0)
		spill = 1;
	else
		spill = 0;

	endva = round_page(va + buflen);
	va = trunc_page(va);

	boundary = map->_dm_boundary;
	alignment = PAGE_SIZE;

	sgvalen = (endva - va);
	if (spill) {
		sgvalen += PAGE_SIZE;

		/*
		 * ARGH!  If the addition of the spill page bumped us
		 * over our boundary, we have to 2x the boundary limit.
		 */
		if (boundary && boundary < sgvalen) {
			alignment = boundary;
			do {
				boundary <<= 1;
			} while (boundary < sgvalen);
		}
	}

#if 0
	printf("len 0x%lx -> 0x%lx, boundary 0x%lx -> 0x%lx -> ",
	    (endva - va), sgvalen, map->_dm_boundary, boundary);
#endif

	mtx_enter(&sgmap->aps_mtx);
	error = extent_alloc_with_descr(sgmap->aps_ex, sgvalen, alignment,
	    0, boundary, EX_NOWAIT, &regions[seg], &sgva);
	mtx_leave(&sgmap->aps_mtx);
	if (error)
		return (error);

#if 0
	printf("error %d sgva 0x%lx\n", error, sgva);
#endif

	pteidx = sgva >> SGMAP_ADDR_PTEIDX_SHIFT;
	pte = &page_table[pteidx * SGMAP_PTE_SPACING];

#ifdef SGMAP_DEBUG
	if (__C(SGMAP_TYPE,_debug))
		printf("sgmap_load: sgva = 0x%lx, pteidx = %d, "
		    "pte = %p (pt = %p)\n", sgva, pteidx, pte,
		    page_table);
#endif

	/* Generate the DMA address. */
	map->dm_segs[seg].ds_addr = sgmap->aps_wbase | sgva | dmaoffset;
	map->dm_segs[seg].ds_len = buflen;

#ifdef SGMAP_DEBUG
	if (__C(SGMAP_TYPE,_debug))
		printf("sgmap_load: wbase = 0x%lx, vpage = 0x%x, "
		    "dma addr = 0x%lx\n", sgmap->aps_wbase, sgva,
		    map->dm_segs[seg].ds_addr);
#endif

	for (; va < endva; va += PAGE_SIZE, pteidx++,
	     pte = &page_table[pteidx * SGMAP_PTE_SPACING]) {
		/* Get the physical address for this segment. */
		(void)pmap_extract(pmap, va, &pa);

		/* Load the current PTE with this page. */
		*pte = (pa >> SGPTE_PGADDR_SHIFT) | SGPTE_VALID;
#ifdef SGMAP_DEBUG
		if (__C(SGMAP_TYPE,_debug))
			printf("sgmap_load:     pa = 0x%lx, pte = %p, "
			    "*pte = 0x%lx\n", pa, pte, (u_long)(*pte));
#endif
	}

	if (spill) {
		/* ...and the prefetch-spill page. */
		*pte = __C(SGMAP_TYPE,_prefetch_spill_page_pte);
#ifdef SGMAP_DEBUG
		if (__C(SGMAP_TYPE,_debug)) {
			printf("sgmap_load:     spill page, pte = %p, "
			    "*pte = 0x%lx\n", pte, *pte);
			printf("sgmap_load:     pte count = %d\n",
			    map->_dm_ptecnt);
		}
#endif
	}

	return (0);
}

int
__C(SGMAP_TYPE,_load)(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, struct alpha_sgmap *sgmap)
{
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	KASSERT((map->_dm_flags & (BUS_DMA_READ|BUS_DMA_WRITE)) == 0);
	KASSERT((flags & (BUS_DMA_READ|BUS_DMA_WRITE)) !=
	    (BUS_DMA_READ|BUS_DMA_WRITE));

	map->_dm_flags |= flags & (BUS_DMA_READ|BUS_DMA_WRITE);

	seg = 0;
	error = __C(SGMAP_TYPE,_load_buffer)(t, map, buf, buflen, p,
	    flags, seg, sgmap);

	alpha_mb();

#if defined(SGMAP_DEBUG) && defined(DDB)
	if (__C(SGMAP_TYPE,_debug) > 1)
		Debugger();
#endif

	if (error == 0) {
		map->dm_mapsize = buflen;
		map->dm_nsegs = 1;
		map->_dm_window = t;
	} else {
		map->_dm_flags &= ~(BUS_DMA_READ|BUS_DMA_WRITE);
		if (t->_next_window != NULL) {
			/* Give the next window a chance. */
			error = bus_dmamap_load(t->_next_window, map, buf,
			    buflen, p, flags);
		}
	}
	return (error);
}

int
__C(SGMAP_TYPE,_load_mbuf)(bus_dma_tag_t t, bus_dmamap_t map,
    struct mbuf *m0, int flags, struct alpha_sgmap *sgmap)
{
	struct mbuf *m;
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic(__S(__C(SGMAP_TYPE,_load_mbuf)) ": no packet header");
#endif

	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	KASSERT((map->_dm_flags & (BUS_DMA_READ|BUS_DMA_WRITE)) == 0);
	KASSERT((flags & (BUS_DMA_READ|BUS_DMA_WRITE)) !=
	    (BUS_DMA_READ|BUS_DMA_WRITE));

	map->_dm_flags |= flags & (BUS_DMA_READ|BUS_DMA_WRITE);

	seg = 0;
	error = 0;
	for (m = m0; m != NULL && error == 0; m = m->m_next) {
		if (m->m_len == 0)
			continue;
		error = __C(SGMAP_TYPE,_load_buffer)(t, map,
		    m->m_data, m->m_len, NULL, flags, seg, sgmap);
		seg++;
	}

	alpha_mb();

#if defined(SGMAP_DEBUG) && defined(DDB)
	if (__C(SGMAP_TYPE,_debug) > 1)
		Debugger();
#endif

	if (error == 0) {
		map->dm_mapsize = m0->m_pkthdr.len;
		map->dm_nsegs = seg;
		map->_dm_window = t;
	} else {
		/* Need to back out what we've done so far. */
		map->dm_nsegs = seg - 1;
		__C(SGMAP_TYPE,_unload)(t, map, sgmap);
		map->_dm_flags &= ~(BUS_DMA_READ|BUS_DMA_WRITE);
		if (t->_next_window != NULL) {
			/* Give the next window a chance. */
			error = bus_dmamap_load_mbuf(t->_next_window, map,
			    m0, flags);
		}
	}

	return (error);
}

int
__C(SGMAP_TYPE,_load_uio)(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio,
    int flags, struct alpha_sgmap *sgmap)
{
	bus_size_t minlen, resid;
	struct proc *p = NULL;
	struct iovec *iov;
	caddr_t addr;
	int i, seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	KASSERT((map->_dm_flags & (BUS_DMA_READ|BUS_DMA_WRITE)) == 0);
	KASSERT((flags & (BUS_DMA_READ|BUS_DMA_WRITE)) !=
 	    (BUS_DMA_READ|BUS_DMA_WRITE));

	map->_dm_flags |= flags & (BUS_DMA_READ|BUS_DMA_WRITE);

	resid = uio->uio_resid;
	iov = uio->uio_iov;

	if (uio->uio_segflg == UIO_USERSPACE) {
		p = uio->uio_procp;
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic(__S(__C(SGMAP_TYPE,_load_uio))
			    ": USERSPACE but no proc");
#endif
	}

	seg = 0;
	error = 0;
	for (i = 0; i < uio->uio_iovcnt && resid != 0 && error == 0;
	     i++, seg++) {
		/*
		 * Now at the first iovec to load.  Load each iovec
		 * until we have exhausted the residual count.
		 */
		minlen = resid < iov[i].iov_len ? resid : iov[i].iov_len;
		addr = (caddr_t)iov[i].iov_base;

		error = __C(SGMAP_TYPE,_load_buffer)(t, map,
		    addr, minlen, p, flags, seg, sgmap);

		resid -= minlen;
	}

	alpha_mb();

#if defined(SGMAP_DEBUG) && defined(DDB)
	if (__C(SGMAP_TYPE,_debug) > 1)
		Debugger();
#endif

	if (error == 0) {
		map->dm_mapsize = uio->uio_resid;
		map->dm_nsegs = seg;
	} else {
		/* Need to back out what we've done so far. */
		map->dm_nsegs = seg - 1;
		__C(SGMAP_TYPE,_unload)(t, map, sgmap);
		map->_dm_flags &= ~(BUS_DMA_READ|BUS_DMA_WRITE);
		if (t->_next_window != NULL) {
			/* Give the next window a chance. */
			error = bus_dmamap_load_uio(t->_next_window, map,
			    uio, flags);
		}
	}

	return (error);
}

int
__C(SGMAP_TYPE,_load_raw)(bus_dma_tag_t t, bus_dmamap_t map,
    bus_dma_segment_t *segs, int nsegs, bus_size_t size, int flags,
    struct alpha_sgmap *sgmap)
{
	KASSERT((map->_dm_flags & (BUS_DMA_READ|BUS_DMA_WRITE)) == 0);
	KASSERT((flags & (BUS_DMA_READ|BUS_DMA_WRITE)) !=
	    (BUS_DMA_READ|BUS_DMA_WRITE));

	panic(__S(__C(SGMAP_TYPE,_load_raw)) ": not implemented");
}

void
__C(SGMAP_TYPE,_unload)(bus_dma_tag_t t, bus_dmamap_t map,
    struct alpha_sgmap *sgmap)
{
	SGMAP_PTE_TYPE *pte, *page_table = sgmap->aps_pt;
	bus_addr_t osgva, sgva, esgva;
	int error, spill, seg, pteidx;

	for (seg = 0; seg < map->dm_nsegs; seg++) {
		/*
		 * XXX Always allocate a spill page for now.  Note
		 * the spill page is not needed for an in-bound-only
		 * transfer.
		 */
		if ((map->_dm_flags & BUS_DMA_READ) == 0)
			spill = 1;
		else
			spill = 0;

		sgva = map->dm_segs[seg].ds_addr & ~sgmap->aps_wbase;

		esgva = round_page(sgva + map->dm_segs[seg].ds_len);
		osgva = sgva = trunc_page(sgva);

		if (spill)
			esgva += PAGE_SIZE;

		/* Invalidate the PTEs for the mapping. */
		for (pteidx = sgva >> SGMAP_ADDR_PTEIDX_SHIFT;
		     sgva < esgva; sgva += PAGE_SIZE, pteidx++) {
			pte = &page_table[pteidx * SGMAP_PTE_SPACING];
#ifdef SGMAP_DEBUG
			if (__C(SGMAP_TYPE,_debug))
				printf("sgmap_unload:     pte = %p, "
				    "*pte = 0x%lx\n", pte, (u_long)(*pte));
#endif
			*pte = 0;
		}

		alpha_mb();

		/* Free the virtual address space used by the mapping. */
		mtx_enter(&sgmap->aps_mtx);
		error = extent_free(sgmap->aps_ex, osgva, (esgva - osgva),
		    EX_NOWAIT);
		mtx_leave(&sgmap->aps_mtx);
		if (error != 0)
			panic(__S(__C(SGMAP_TYPE,_unload)));
	}

	map->_dm_flags &= ~(BUS_DMA_READ|BUS_DMA_WRITE);

	/* Mark the mapping invalid. */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;
	map->_dm_window = NULL;
}
@


1.13
log
@Use extent_alloc_with_descr(9) and add a mutex to protect the extent.
This should make bus_dmamap_load(9) and bus_dmamap_unload(9) "mpsafe".

As a bonus this gets rid of a potential memory allocation in the IO path.

ok miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.12 2010/04/10 13:46:12 oga Exp $ */
d66 3
@


1.12
log
@pmap_extract() does the equivalent of vtophys if pmap_kernel(), so instead of
doing if (p != NULL) pmap_extract() else vtophys() in a loop, just do
pmap_extract unconditionally.

ok miod@@ (he found a typo, all hail miod!)
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.11 2008/06/26 05:42:08 ray Exp $ */
d63 1
d65 1
a65 1
	int s, pteidx, error, spill;
d131 4
a134 4
	s = splvm();
	error = extent_alloc(sgmap->aps_ex, sgvalen, alignment, 0, boundary,
	    (flags & BUS_DMA_NOWAIT) ? EX_NOWAIT : EX_WAITOK, &sgva);
	splx(s);
d397 1
a397 1
	int s, error, spill, seg, pteidx;
d433 1
a433 1
		s = splvm();
d436 1
a436 1
		splx(s);
@


1.11
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.10 2006/05/21 01:42:43 brad Exp $ */
d60 1
d83 4
d165 1
a165 4
		if (p != NULL)
			(void) pmap_extract(p->p_vmspace->vm_map.pmap, va, &pa);
		else
			pa = vtophys(va);
@


1.10
log
@Pay attention to BUS_DMA_READ; don't need to allocate a spill
page if it is set.

From NetBSD
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.9 2006/05/21 01:26:19 brad Exp $ */
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.9
log
@Implement dmamap_load_uio for SGMAPs.

From NetBSD
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.8 2006/05/12 20:48:19 brad Exp $ */
d95 9
a103 2
	/* XXX Always allocate a spill page for now. */
	spill = 1;
d212 6
d233 7
a239 4
	} else if (t->_next_window != NULL) {
		/* Give the next window a chance. */
		error = bus_dmamap_load(t->_next_window, map, buf, buflen,
		    p, flags);
d265 6
d296 1
d323 1
a323 1
#if 0
a327 1
#endif
a371 1
#if 0
a372 1
#endif
d388 3
d404 9
a412 2
		/* XXX Always have a spill page for now... */
		spill = 1;
d444 2
@


1.8
log
@Keep track of which DMA window was actually used to map the
request (not always the passed in DMA tag if we try direct-map
and then fall back to sgmap-mapped).  Use the actual window
when performing dmamap_sync and dmamap_unload operations.

From NetBSD

ok martin@@
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.7 2006/04/13 14:41:08 brad Exp $ */
d288 5
d294 67
a360 1
	panic(__S(__C(SGMAP_TYPE,_load_uio)) ": not implemented");
@


1.7
log
@Use PAGE_SIZE rather than NBPG.

From NetBSD

ok martin@@ miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.6 2006/03/27 20:46:44 brad Exp $ */
d219 1
d269 1
d347 1
@


1.6
log
@rev 1.30

Don't increase the segment index if we skipped a zero-length mbuf.

rev 1.22

Since the SGMAP buffer load subroutine doesn't need to modify
the segment index, don't pass it by reference.

From NetBSD

ok miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.5 2006/03/13 19:50:07 miod Exp $ */
d102 1
a102 1
	alignment = NBPG;
d106 1
a106 1
		sgvalen += NBPG;
d157 1
a157 1
	for (; va < endva; va += NBPG, pteidx++,
d317 1
a317 1
			esgva += NBPG;
d321 1
a321 1
		     sgva < esgva; sgva += NBPG, pteidx++) {
@


1.5
log
@Protect sgmap extents with splvm(); from NetBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.4 2004/11/09 19:17:01 claudio Exp $ */
d49 1
a49 2
			    struct proc *, int, int *,
			    struct alpha_sgmap *);
d62 1
a62 1
    size_t buflen, struct proc *p, int flags, int *segp,
d147 2
a148 2
	map->dm_segs[*segp].ds_addr = sgmap->aps_wbase | sgva | dmaoffset;
	map->dm_segs[*segp].ds_len = buflen;
d154 1
a154 1
		    map->dm_segs[0].ds_addr);
d158 1
a158 1
	    pte = &page_table[pteidx * SGMAP_PTE_SPACING]) {
d161 1
a161 2
			(void) pmap_extract(p->p_vmspace->vm_map.pmap, va,
			    &pa);
d207 1
a207 1
	    flags, &seg, sgmap);
d250 1
a250 1
	for (m = m0; m != NULL && error == 0; m = m->m_next, seg++) {
d254 2
a255 1
		    m->m_data, m->m_len, NULL, flags, &seg, sgmap);
@


1.4
log
@Do not map empty mbufs (m_len == 0) in bus_dmamap_load_mbuf() as these mappings
may disturb the dma as seen in ipw(4). Emtpy mbufs are at the beginning of the
mbuf chain and are as example a "side-effect" of a previous m_adj() call.
OK miod@@ mickey@@ jason@@ markus@@
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.3 2004/01/13 00:12:15 deraadt Exp $ */
d71 1
a71 1
	int pteidx, error, spill;
d126 1
d129 1
d306 1
a306 1
	int spill, seg, pteidx;
d335 5
a339 2
		if (extent_free(sgmap->aps_ex, osgva, (esgva - osgva),
		    EX_NOWAIT) != 0)
@


1.3
log
@support mbuf handling in alpha sgmap dma maps; from netbsd
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.2 2001/06/08 08:08:40 art Exp $ */
d250 3
a252 1
	for (m = m0; m != NULL && error == 0; m = m->m_next, seg++)
d255 1
@


1.2
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 2
a2 2
/* $OpenBSD: sgmap_typedep.c,v 1.1 2001/03/21 17:26:38 art Exp $ */
/* $NetBSD: sgmap_typedep.c,v 1.13 1999/07/08 18:05:23 thorpej Exp $ */
a40 14
#ifdef SGMAP_LOG

#ifndef SGMAP_LOGSIZE
#define	SGMAP_LOGSIZE	4096
#endif

struct sgmap_log_entry	__C(SGMAP_TYPE,_log)[SGMAP_LOGSIZE];
int			__C(SGMAP_TYPE,_log_next);
int			__C(SGMAP_TYPE,_log_last);
u_long			__C(SGMAP_TYPE,_log_loads);
u_long			__C(SGMAP_TYPE,_log_unloads);

#endif /* SGMAP_LOG */

d47 5
d53 1
a53 1
__C(SGMAP_TYPE,_init_spill_page_pte)()
d62 3
a64 8
__C(SGMAP_TYPE,_load)(t, map, buf, buflen, p, flags, sgmap)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	void *buf;
	bus_size_t buflen;
	struct proc *p;
	int flags;
	struct alpha_sgmap *sgmap;
d68 2
a69 2
	bus_addr_t dmaoffset;
	bus_size_t dmalen;
d71 1
a71 4
	int pteidx, error;
#ifdef SGMAP_LOG
	struct sgmap_log_entry sl;
#endif
d73 1
a73 3
	/*
	 * Initialize the spill page PTE if that hasn't already been done.
	 */
a77 9
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	/*
a81 1
	dmalen = buflen;
d86 2
a87 12
		printf("sgmap_load: dmaoffset = 0x%lx, dmalen = 0x%lx\n",
		    dmaoffset, dmalen);
	}
#endif

#ifdef SGMAP_LOG
	if (panicstr == NULL) {
		sl.sl_op = 1;
		sl.sl_sgmap = sgmap;
		sl.sl_origbuf = buf;
		sl.sl_pgoffset = dmaoffset;
		sl.sl_origlen = dmalen;
a93 4
	 *
	 * alpha_sgmap_alloc will deal with the appropriate spill page
	 * allocations.
	 *
d95 4
d101 18
a118 4
	if ((map->_dm_flags & DMAMAP_HAS_SGMAP) == 0) {
		error = alpha_sgmap_alloc(map, (endva - va), sgmap, flags);
		if (error)
			return (error);
d121 15
a135 1
	pteidx = map->_dm_sgva >> PGSHIFT;
d141 1
a141 1
		    "pte = %p (pt = %p)\n", map->_dm_sgva, pteidx, pte,
d145 3
a147 13
	/*
	 * Generate the DMA address.
	 */
	map->dm_segs[0].ds_addr = sgmap->aps_wbase |
	    (pteidx << SGMAP_ADDR_PTEIDX_SHIFT) | dmaoffset;
	map->dm_segs[0].ds_len = dmalen;

#ifdef SGMAP_LOG
	if (panicstr == NULL) {
		sl.sl_sgva = map->_dm_sgva;
		sl.sl_dmaaddr = map->dm_segs[0].ds_addr;
	}
#endif
d152 1
a152 2
		    "dma addr = 0x%lx\n", sgmap->aps_wbase,
		    (pteidx << SGMAP_ADDR_PTEIDX_SHIFT),
a155 3
	map->_dm_pteidx = pteidx;
	map->_dm_ptecnt = 0;

d157 2
a158 5
		pte = &page_table[pteidx * SGMAP_PTE_SPACING],
		map->_dm_ptecnt++) {
		/*
		 * Get the physical address for this segment.
		 */
d160 2
a161 1
			pmap_extract(p->p_vmspace->vm_map.pmap, va, &pa);
d165 1
a165 3
		/*
		 * Load the current PTE with this page.
		 */
d174 22
d197 1
a197 1
	 * ...and the prefetch-spill page.
d199 9
a207 9
	*pte = __C(SGMAP_TYPE,_prefetch_spill_page_pte);
	map->_dm_ptecnt++;
#ifdef SGMAP_DEBUG
	if (__C(SGMAP_TYPE,_debug)) {
		printf("sgmap_load:     spill page, pte = %p, *pte = 0x%lx\n",
		    pte, *pte);
		printf("sgmap_load:     pte count = %d\n", map->_dm_ptecnt);
	}
#endif
d211 12
a222 9
#ifdef SGMAP_LOG
	if (panicstr == NULL) {
		sl.sl_ptecnt = map->_dm_ptecnt;
		bcopy(&sl, &__C(SGMAP_TYPE,_log)[__C(SGMAP_TYPE,_log_next)],
		    sizeof(sl));
		__C(SGMAP_TYPE,_log_last) = __C(SGMAP_TYPE,_log_next);
		if (++__C(SGMAP_TYPE,_log_next) == SGMAP_LOGSIZE)
			__C(SGMAP_TYPE,_log_next) = 0;
		__C(SGMAP_TYPE,_log_loads)++;
d224 19
d245 11
a259 4
	map->dm_mapsize = buflen;
	map->dm_nsegs = 1;
	return (0);
}
d261 13
a273 8
int
__C(SGMAP_TYPE,_load_mbuf)(t, map, m, flags, sgmap)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct mbuf *m;
	int flags;
	struct alpha_sgmap *sgmap;
{
d275 1
a275 1
	panic(__S(__C(SGMAP_TYPE,_load_mbuf)) ": not implemented");
d279 2
a280 6
__C(SGMAP_TYPE,_load_uio)(t, map, uio, flags, sgmap)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct uio *uio;
	int flags;
	struct alpha_sgmap *sgmap;
d287 3
a289 8
__C(SGMAP_TYPE,_load_raw)(t, map, segs, nsegs, size, flags, sgmap)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_dma_segment_t *segs;
	int nsegs;
	bus_size_t size;
	int flags;
	struct alpha_sgmap *sgmap;
d296 2
a297 4
__C(SGMAP_TYPE,_unload)(t, map, sgmap)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct alpha_sgmap *sgmap;
d300 14
a313 19
	int ptecnt, pteidx;
#ifdef SGMAP_LOG
	struct sgmap_log_entry *sl;

	if (panicstr == NULL) {
		sl = &__C(SGMAP_TYPE,_log)[__C(SGMAP_TYPE,_log_next)];

		bzero(sl, sizeof(*sl));
		sl->sl_op = 0;
		sl->sl_sgmap = sgmap;
		sl->sl_sgva = map->_dm_sgva;
		sl->sl_dmaaddr = map->dm_segs[0].ds_addr;

		__C(SGMAP_TYPE,_log_last) = __C(SGMAP_TYPE,_log_next);
		if (++__C(SGMAP_TYPE,_log_next) == SGMAP_LOGSIZE)
			__C(SGMAP_TYPE,_log_next) = 0;
		__C(SGMAP_TYPE,_log_unloads)++;
	}
#endif
d315 4
a318 8
	/*
	 * Invalidate the PTEs for the mapping.
	 */
	for (ptecnt = map->_dm_ptecnt, pteidx = map->_dm_pteidx,
		pte = &page_table[pteidx * SGMAP_PTE_SPACING];
		ptecnt != 0;
		ptecnt--, pteidx++,
		pte = &page_table[pteidx * SGMAP_PTE_SPACING]) {
d320 13
a332 5
		if (__C(SGMAP_TYPE,_debug))
			printf("sgmap_unload:     pte = %p, *pte = 0x%lx\n",
			    pte, (u_long)(*pte));
#endif
		*pte = 0;
d335 1
a335 9
	/*
	 * Free the virtual address space used by the mapping
	 * if necessary.
	 */
	if ((map->_dm_flags & BUS_DMA_ALLOCNOW) == 0)
		alpha_sgmap_free(map, sgmap);
	/*
	 * Mark the mapping invalid.
	 */
@


1.1
log
@Move files from common to dev to be more like other archs
(and so that tab completion on "compile" works as on other archs. :))
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.2 2000/11/08 21:27:11 ericj Exp $ */
d188 1
a188 1
			pa = pmap_extract(p->p_vmspace->vm_map.pmap, va);
@


1.1.2.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.1 2001/03/21 17:26:38 art Exp $ */
@


1.1.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/* $OpenBSD: sgmap_typedep.c,v 1.1.2.1 2001/04/18 16:00:42 niklas Exp $ */
d188 1
a188 1
			pmap_extract(p->p_vmspace->vm_map.pmap, va, &pa);
@


1.1.2.3
log
@Merge of -current from two weeks ago into the SMP branch
@
text
@d1 2
a2 2
/* $OpenBSD$ */
/* $NetBSD: sgmap_typedep.c,v 1.17 2001/07/19 04:27:37 thorpej Exp $ */
d41 14
a60 5
int			__C(SGMAP_TYPE,_load_buffer)(bus_dma_tag_t,
			    bus_dmamap_t, void *buf, size_t buflen,
			    struct proc *, int, int *,
			    struct alpha_sgmap *);

d62 1
a62 1
__C(SGMAP_TYPE,_init_spill_page_pte)(void)
d71 8
a78 3
__C(SGMAP_TYPE,_load_buffer)(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    size_t buflen, struct proc *p, int flags, int *segp,
    struct alpha_sgmap *sgmap)
d82 2
a83 2
	bus_addr_t dmaoffset, sgva;
	bus_size_t sgvalen, boundary, alignment;
d85 4
a88 1
	int pteidx, error, spill;
d90 3
a92 1
	/* Initialize the spill page PTE if it hasn't been already. */
d97 9
d110 1
d115 12
a126 2
		printf("sgmap_load: dmaoffset = 0x%lx, buflen = 0x%lx\n",
		    dmaoffset, buflen);
d133 4
a137 4

	/* XXX Always allocate a spill page for now. */
	spill = 1;

d140 4
a143 18

	boundary = map->_dm_boundary;
	alignment = NBPG;

	sgvalen = (endva - va);
	if (spill) {
		sgvalen += NBPG;

		/*
		 * ARGH!  If the addition of the spill page bumped us
		 * over our boundary, we have to 2x the boundary limit.
		 */
		if (boundary && boundary < sgvalen) {
			alignment = boundary;
			do {
				boundary <<= 1;
			} while (boundary < sgvalen);
		}
d146 1
a146 15
#if 0
	printf("len 0x%lx -> 0x%lx, boundary 0x%lx -> 0x%lx -> ",
	    (endva - va), sgvalen, map->_dm_boundary, boundary);
#endif

	error = extent_alloc(sgmap->aps_ex, sgvalen, alignment, 0, boundary,
	    (flags & BUS_DMA_NOWAIT) ? EX_NOWAIT : EX_WAITOK, &sgva);
	if (error)
		return (error);

#if 0
	printf("error %d sgva 0x%lx\n", error, sgva);
#endif

	pteidx = sgva >> SGMAP_ADDR_PTEIDX_SHIFT;
d152 1
a152 1
		    "pte = %p (pt = %p)\n", sgva, pteidx, pte,
d156 13
a168 3
	/* Generate the DMA address. */
	map->dm_segs[*segp].ds_addr = sgmap->aps_wbase | sgva | dmaoffset;
	map->dm_segs[*segp].ds_len = buflen;
d173 2
a174 1
		    "dma addr = 0x%lx\n", sgmap->aps_wbase, sgva,
d178 3
d182 5
a186 2
	    pte = &page_table[pteidx * SGMAP_PTE_SPACING]) {
		/* Get the physical address for this segment. */
d188 1
a188 2
			(void) pmap_extract(p->p_vmspace->vm_map.pmap, va,
			    &pa);
d192 3
a194 1
		/* Load the current PTE with this page. */
d203 5
a207 3
	if (spill) {
		/* ...and the prefetch-spill page. */
		*pte = __C(SGMAP_TYPE,_prefetch_spill_page_pte);
d209 5
a213 6
		if (__C(SGMAP_TYPE,_debug)) {
			printf("sgmap_load:     spill page, pte = %p, "
			    "*pte = 0x%lx\n", pte, *pte);
			printf("sgmap_load:     pte count = %d\n",
			    map->_dm_ptecnt);
		}
a214 1
	}
d216 1
a216 2
	return (0);
}
d218 11
a228 20
int
__C(SGMAP_TYPE,_load)(bus_dma_tag_t t, bus_dmamap_t map, void *buf,
    bus_size_t buflen, struct proc *p, int flags, struct alpha_sgmap *sgmap)
{
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	seg = 0;
	error = __C(SGMAP_TYPE,_load_buffer)(t, map, buf, buflen, p,
	    flags, &seg, sgmap);

	alpha_mb();
d234 3
a236 10

	if (error == 0) {
		map->dm_mapsize = buflen;
		map->dm_nsegs = 1;
	} else if (t->_next_window != NULL) {
		/* Give the next window a chance. */
		error = bus_dmamap_load(t->_next_window, map, buf, buflen,
		    p, flags);
	}
	return (error);
d240 6
a245 2
__C(SGMAP_TYPE,_load_mbuf)(bus_dma_tag_t t, bus_dmamap_t map,
    struct mbuf *m0, int flags, struct alpha_sgmap *sgmap)
a246 13
	struct mbuf *m;
	int seg, error;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_mapsize = 0;
	map->dm_nsegs = 0;

#ifdef DIAGNOSTIC
	if ((m0->m_flags & M_PKTHDR) == 0)
		panic(__S(__C(SGMAP_TYPE,_load_mbuf)) ": no packet header");
#endif
d248 1
a248 31
	if (m0->m_pkthdr.len > map->_dm_size)
		return (EINVAL);

	seg = 0;
	error = 0;
	for (m = m0; m != NULL && error == 0; m = m->m_next, seg++)
		error = __C(SGMAP_TYPE,_load_buffer)(t, map,
		    m->m_data, m->m_len, NULL, flags, &seg, sgmap);

	alpha_mb();

#if defined(SGMAP_DEBUG) && defined(DDB)
	if (__C(SGMAP_TYPE,_debug) > 1)
		Debugger();
#endif

	if (error == 0) {
		map->dm_mapsize = m0->m_pkthdr.len;
		map->dm_nsegs = seg;
	} else {
		/* Need to back out what we've done so far. */
		map->dm_nsegs = seg - 1;
		__C(SGMAP_TYPE,_unload)(t, map, sgmap);
		if (t->_next_window != NULL) {
			/* Give the next window a chance. */
			error = bus_dmamap_load_mbuf(t->_next_window, map,
			    m0, flags);
		}
	}

	return (error);
d252 6
a257 2
__C(SGMAP_TYPE,_load_uio)(bus_dma_tag_t t, bus_dmamap_t map, struct uio *uio,
    int flags, struct alpha_sgmap *sgmap)
d264 8
a271 3
__C(SGMAP_TYPE,_load_raw)(bus_dma_tag_t t, bus_dmamap_t map,
    bus_dma_segment_t *segs, int nsegs, bus_size_t size, int flags,
    struct alpha_sgmap *sgmap)
d278 4
a281 2
__C(SGMAP_TYPE,_unload)(bus_dma_tag_t t, bus_dmamap_t map,
    struct alpha_sgmap *sgmap)
d284 19
a302 2
	bus_addr_t osgva, sgva, esgva;
	int spill, seg, pteidx;
d304 8
a311 16
	for (seg = 0; seg < map->dm_nsegs; seg++) {
		/* XXX Always have a spill page for now... */
		spill = 1;

		sgva = map->dm_segs[seg].ds_addr & ~sgmap->aps_wbase;

		esgva = round_page(sgva + map->dm_segs[seg].ds_len);
		osgva = sgva = trunc_page(sgva);

		if (spill)
			esgva += NBPG;

		/* Invalidate the PTEs for the mapping. */
		for (pteidx = sgva >> SGMAP_ADDR_PTEIDX_SHIFT;
		     sgva < esgva; sgva += NBPG, pteidx++) {
			pte = &page_table[pteidx * SGMAP_PTE_SPACING];
d313 5
a317 13
			if (__C(SGMAP_TYPE,_debug))
				printf("sgmap_unload:     pte = %p, "
				    "*pte = 0x%lx\n", pte, (u_long)(*pte));
#endif
			*pte = 0;
		}

		alpha_mb();

		/* Free the virtual address space used by the mapping. */
		if (extent_free(sgmap->aps_ex, osgva, (esgva - osgva),
		    EX_NOWAIT) != 0)
			panic(__S(__C(SGMAP_TYPE,_unload)));
d320 9
a328 1
	/* Mark the mapping invalid. */
@


