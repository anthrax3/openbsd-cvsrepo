head	1.84;
access;
symbols
	OPENBSD_6_1_BASE:1.84
	OPENBSD_6_0:1.83.0.2
	OPENBSD_6_0_BASE:1.83
	OPENBSD_5_9:1.82.0.2
	OPENBSD_5_9_BASE:1.82
	OPENBSD_5_8:1.78.0.4
	OPENBSD_5_8_BASE:1.78
	OPENBSD_5_7:1.75.0.2
	OPENBSD_5_7_BASE:1.75
	OPENBSD_5_6:1.72.0.4
	OPENBSD_5_6_BASE:1.72
	OPENBSD_5_5:1.71.0.4
	OPENBSD_5_5_BASE:1.71
	OPENBSD_5_4:1.63.0.6
	OPENBSD_5_4_BASE:1.63
	OPENBSD_5_3:1.63.0.4
	OPENBSD_5_3_BASE:1.63
	OPENBSD_5_2:1.63.0.2
	OPENBSD_5_2_BASE:1.63
	OPENBSD_5_1_BASE:1.62
	OPENBSD_5_1:1.62.0.2
	OPENBSD_5_0:1.60.0.4
	OPENBSD_5_0_BASE:1.60
	OPENBSD_4_9:1.60.0.2
	OPENBSD_4_9_BASE:1.60
	OPENBSD_4_8:1.59.0.8
	OPENBSD_4_8_BASE:1.59
	OPENBSD_4_7:1.59.0.4
	OPENBSD_4_7_BASE:1.59
	OPENBSD_4_6:1.59.0.6
	OPENBSD_4_6_BASE:1.59
	OPENBSD_4_5:1.59.0.2
	OPENBSD_4_5_BASE:1.59
	OPENBSD_4_4:1.55.0.2
	OPENBSD_4_4_BASE:1.55
	OPENBSD_4_3:1.54.0.2
	OPENBSD_4_3_BASE:1.54
	OPENBSD_4_2:1.52.0.2
	OPENBSD_4_2_BASE:1.52
	OPENBSD_4_1:1.49.0.2
	OPENBSD_4_1_BASE:1.49
	OPENBSD_4_0:1.47.0.2
	OPENBSD_4_0_BASE:1.47
	OPENBSD_3_9:1.45.0.2
	OPENBSD_3_9_BASE:1.45
	OPENBSD_3_8:1.42.0.6
	OPENBSD_3_8_BASE:1.42
	OPENBSD_3_7:1.42.0.4
	OPENBSD_3_7_BASE:1.42
	OPENBSD_3_6:1.42.0.2
	OPENBSD_3_6_BASE:1.42
	SMP_SYNC_A:1.41
	SMP_SYNC_B:1.41
	OPENBSD_3_5:1.40.0.2
	OPENBSD_3_5_BASE:1.40
	OPENBSD_3_4:1.38.0.2
	OPENBSD_3_4_BASE:1.38
	UBC_SYNC_A:1.37
	OPENBSD_3_3:1.37.0.2
	OPENBSD_3_3_BASE:1.37
	OPENBSD_3_2:1.36.0.2
	OPENBSD_3_2_BASE:1.36
	OPENBSD_3_1:1.33.0.2
	OPENBSD_3_1_BASE:1.33
	UBC_SYNC_B:1.36
	UBC:1.31.0.2
	UBC_BASE:1.31
	OPENBSD_3_0:1.17.0.2
	OPENBSD_3_0_BASE:1.17
	OPENBSD_2_9:1.12.0.2
	OPENBSD_2_9_BASE:1.12
	OPENBSD_2_8:1.6.0.8
	OPENBSD_2_8_BASE:1.6
	OPENBSD_2_7:1.6.0.6
	OPENBSD_2_7_BASE:1.6
	SMP:1.6.0.4
	SMP_BASE:1.6
	kame_19991208:1.6
	OPENBSD_2_6:1.6.0.2
	OPENBSD_2_6_BASE:1.6
	OPENBSD_2_5:1.5.0.10
	OPENBSD_2_5_BASE:1.5
	OPENBSD_2_4:1.5.0.8
	OPENBSD_2_4_BASE:1.5
	OPENBSD_2_3:1.5.0.6
	OPENBSD_2_3_BASE:1.5
	OPENBSD_2_2:1.5.0.4
	OPENBSD_2_2_BASE:1.5
	OPENBSD_2_1:1.5.0.2
	OPENBSD_2_1_BASE:1.5
	OPENBSD_2_0:1.4.0.2
	OPENBSD_2_0_BASE:1.4
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.84
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.83;
commitid	RlO92XR575sygHqm;

1.83
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.82;
commitid	N0upL0onl7Raz5yi;

1.82
date	2016.02.22.07.13.46;	author landry;	state Exp;
branches;
next	1.81;
commitid	8qRMizYrW35Z3d9h;

1.81
date	2015.12.04.06.21.52;	author jsg;	state Exp;
branches;
next	1.80;
commitid	EkfpuGOwKj2sE5I5;

1.80
date	2015.10.01.16.03.48;	author kettenis;	state Exp;
branches;
next	1.79;
commitid	XWF4k0c2g2s1Sgwf;

1.79
date	2015.09.08.21.28.35;	author kettenis;	state Exp;
branches;
next	1.78;
commitid	WSD3bUAMn8qMj0PM;

1.78
date	2015.07.20.00.16.16;	author kettenis;	state Exp;
branches;
next	1.77;
commitid	9NWFjmLiRuWXuw8P;

1.77
date	2015.06.23.19.50.48;	author miod;	state Exp;
branches;
next	1.76;
commitid	9LiVcmBG0RYcb1ae;

1.76
date	2015.05.02.20.50.08;	author miod;	state Exp;
branches;
next	1.75;
commitid	MuzIZLdNp4omYOtv;

1.75
date	2015.02.02.09.29.53;	author mlarkin;	state Exp;
branches;
next	1.74;
commitid	Gl0HfhXiwDmJ4Tqg;

1.74
date	2014.12.17.15.23.40;	author deraadt;	state Exp;
branches;
next	1.73;
commitid	m1QU4zfUVMSJqLJo;

1.73
date	2014.11.16.12.30.52;	author deraadt;	state Exp;
branches;
next	1.72;
commitid	yv0ECmCdICvq576h;

1.72
date	2014.03.29.18.09.28;	author guenther;	state Exp;
branches;
next	1.71;

1.71
date	2014.02.01.21.21.54;	author miod;	state Exp;
branches;
next	1.70;

1.70
date	2014.01.26.17.40.09;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2014.01.06.20.27.44;	author miod;	state Exp;
branches;
next	1.68;

1.68
date	2014.01.06.20.21.32;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2014.01.05.14.37.08;	author miod;	state Exp;
branches;
next	1.66;

1.66
date	2014.01.01.22.13.52;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2013.10.31.03.48.31;	author deraadt;	state Exp;
branches;
next	1.64;

1.64
date	2013.10.31.03.33.37;	author dlg;	state Exp;
branches;
next	1.63;

1.63
date	2012.04.10.15.50.52;	author guenther;	state Exp;
branches;
next	1.62;

1.62
date	2011.11.16.20.50.17;	author deraadt;	state Exp;
branches;
next	1.61;

1.61
date	2011.09.22.17.41.00;	author jasper;	state Exp;
branches;
next	1.60;

1.60
date	2010.11.28.21.01.41;	author miod;	state Exp;
branches;
next	1.59;

1.59
date	2009.01.27.22.14.12;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.57;

1.57
date	2008.09.12.12.27.26;	author blambert;	state Exp;
branches;
next	1.56;

1.56
date	2008.09.12.08.43.18;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2008.06.26.05.42.08;	author ray;	state Exp;
branches;
next	1.54;

1.54
date	2007.12.09.00.24.04;	author tedu;	state Exp;
branches;
next	1.53;

1.53
date	2007.09.03.17.29.58;	author miod;	state Exp;
branches;
next	1.52;

1.52
date	2007.05.26.20.26.50;	author pedro;	state Exp;
branches;
next	1.51;

1.51
date	2007.05.04.22.51.12;	author miod;	state Exp;
branches;
next	1.50;

1.50
date	2007.04.13.08.31.50;	author martin;	state Exp;
branches;
next	1.49;

1.49
date	2007.02.03.16.48.21;	author miod;	state Exp;
branches;
next	1.48;

1.48
date	2006.11.29.12.24.15;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2006.04.13.14.41.08;	author brad;	state Exp;
branches;
next	1.46;

1.46
date	2006.03.04.19.33.19;	author miod;	state Exp;
branches;
next	1.45;

1.45
date	2006.02.07.07.59.22;	author martin;	state Exp;
branches;
next	1.44;

1.44
date	2005.11.03.06.40.28;	author brad;	state Exp;
branches;
next	1.43;

1.43
date	2005.10.28.19.10.26;	author martin;	state Exp;
branches;
next	1.42;

1.42
date	2004.06.13.21.49.11;	author niklas;	state Exp;
branches;
next	1.41;

1.41
date	2004.06.08.20.13.21;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2003.12.22.19.59.37;	author jmc;	state Exp;
branches;
next	1.39;

1.39
date	2003.10.18.20.14.40;	author jmc;	state Exp;
branches;
next	1.38;

1.38
date	2003.06.02.23.27.43;	author millert;	state Exp;
branches;
next	1.37;

1.37
date	2003.02.18.13.14.43;	author jmc;	state Exp;
branches;
next	1.36;

1.36
date	2002.09.10.18.29.42;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2002.07.24.00.33.49;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2002.06.25.21.33.19;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2002.01.23.00.39.46;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.12.19.08.58.05;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.12.08.02.24.05;	author art;	state Exp;
branches
	1.31.2.1;
next	1.30;

1.30
date	2001.12.05.16.30.47;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.28.16.24.26;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.28.15.34.16;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.28.14.20.16;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.28.13.47.37;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.27.05.27.11;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.09.15.31.11;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.09.02.57.03;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.09.02.50.08;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.09.02.47.33;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.11.09.02.44.46;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.09.02.43.09;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.08.12.12.03.02;	author heko;	state Exp;
branches;
next	1.16;

1.16
date	2001.07.25.13.25.31;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.06.23.19.36.44;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.06.08.08.08.35;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.05.09.15.31.23;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.04.10.06.59.13;	author niklas;	state Exp;
branches;
next	1.11;

1.11
date	2001.03.16.14.10.23;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.03.16.09.06.03;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.04.13.37.44;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2000.11.08.19.16.59;	author ericj;	state Exp;
branches;
next	1.7;

1.7
date	2000.11.08.16.01.02;	author art;	state Exp;
branches;
next	1.6;

1.6
date	99.09.03.18.00.11;	author art;	state Exp;
branches
	1.6.4.1;
next	1.5;

1.5
date	96.10.30.22.38.20;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	96.07.29.22.57.53;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	96.06.18.09.42.26;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	95.12.14.03.52.43;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.49.40;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.49.40;	author deraadt;	state Exp;
branches;
next	;

1.6.4.1
date	2001.04.18.16.00.27;	author niklas;	state Exp;
branches;
next	1.6.4.2;

1.6.4.2
date	2001.07.04.10.14.23;	author niklas;	state Exp;
branches;
next	1.6.4.3;

1.6.4.3
date	2001.10.31.02.52.44;	author nate;	state Exp;
branches;
next	1.6.4.4;

1.6.4.4
date	2001.11.13.21.00.48;	author niklas;	state Exp;
branches;
next	1.6.4.5;

1.6.4.5
date	2001.12.05.00.39.08;	author niklas;	state Exp;
branches;
next	1.6.4.6;

1.6.4.6
date	2002.03.06.00.47.43;	author niklas;	state Exp;
branches;
next	1.6.4.7;

1.6.4.7
date	2002.03.12.11.54.17;	author ho;	state Exp;
branches;
next	1.6.4.8;

1.6.4.8
date	2003.03.27.23.18.06;	author niklas;	state Exp;
branches;
next	1.6.4.9;

1.6.4.9
date	2003.06.07.11.11.33;	author ho;	state Exp;
branches;
next	1.6.4.10;

1.6.4.10
date	2004.02.19.09.59.33;	author niklas;	state Exp;
branches;
next	1.6.4.11;

1.6.4.11
date	2004.06.08.21.07.17;	author niklas;	state Exp;
branches;
next	;

1.31.2.1
date	2002.01.31.22.55.04;	author niklas;	state Exp;
branches;
next	1.31.2.2;

1.31.2.2
date	2002.02.02.03.28.25;	author art;	state Exp;
branches;
next	1.31.2.3;

1.31.2.3
date	2002.10.29.00.28.00;	author art;	state Exp;
branches;
next	1.31.2.4;

1.31.2.4
date	2003.05.19.21.38.52;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.84
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@/* $OpenBSD: pmap.c,v 1.83 2016/06/07 06:23:19 dlg Exp $ */
/* $NetBSD: pmap.c,v 1.154 2000/12/07 22:18:55 thorpej Exp $ */

/*-
 * Copyright (c) 1998, 1999, 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center and by Chris G. Demetriou.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.6 (Berkeley) 5/27/94
 */

/*
 * DEC Alpha physical map management code.
 *
 * History:
 *
 *	This pmap started life as a Motorola 68851/68030 pmap,
 *	written by Mike Hibler at the University of Utah.
 *
 *	It was modified for the DEC Alpha by Chris Demetriou
 *	at Carnegie Mellon University.
 *
 *	Support for non-contiguous physical memory was added by
 *	Jason R. Thorpe of the Numerical Aerospace Simulation
 *	Facility, NASA Ames Research Center and Chris Demetriou.
 *
 *	Page table management and a major cleanup were undertaken
 *	by Jason R. Thorpe, with lots of help from Ross Harvey of
 *	Avalon Computer Systems and from Chris Demetriou.
 *
 *	Support for the new UVM pmap interface was written by
 *	Jason R. Thorpe.
 *
 *	Support for ASNs was written by Jason R. Thorpe, again
 *	with help from Chris Demetriou and Ross Harvey.
 *
 *	The locking protocol was written by Jason R. Thorpe,
 *	using Chuck Cranor's i386 pmap for UVM as a model.
 *
 *	TLB shootdown code was written by Jason R. Thorpe.
 *
 * Notes:
 *
 *	All page table access is done via K0SEG.  The one exception
 *	to this is for kernel mappings.  Since all kernel page
 *	tables are pre-allocated, we can use the Virtual Page Table
 *	to access PTEs that map K1SEG addresses.
 *
 *	Kernel page table pages are statically allocated in
 *	pmap_bootstrap(), and are never freed.  In the future,
 *	support for dynamically adding additional kernel page
 *	table pages may be added.  User page table pages are
 *	dynamically allocated and freed.
 *
 * Bugs/misfeatures:
 *
 *	- Some things could be optimized.
 */

/*
 *	Manages physical address maps.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/user.h>
#include <sys/buf.h>
#include <sys/atomic.h>
#ifdef SYSVSHM
#include <sys/shm.h>
#endif

#include <uvm/uvm.h>

#include <machine/atomic.h>
#include <machine/cpu.h>
#if defined(_PMAP_MAY_USE_PROM_CONSOLE) || defined(MULTIPROCESSOR)
#include <machine/rpb.h>
#endif

#ifdef DEBUG
#define	PDB_FOLLOW	0x0001
#define	PDB_INIT	0x0002
#define	PDB_ENTER	0x0004
#define	PDB_REMOVE	0x0008
#define	PDB_CREATE	0x0010
#define	PDB_PTPAGE	0x0020
#define	PDB_ASN		0x0040
#define	PDB_BITS	0x0080
#define	PDB_COLLECT	0x0100
#define	PDB_PROTECT	0x0200
#define	PDB_BOOTSTRAP	0x1000
#define	PDB_PARANOIA	0x2000
#define	PDB_WIRING	0x4000
#define	PDB_PVDUMP	0x8000

int debugmap = 0;
int pmapdebug = PDB_PARANOIA|PDB_FOLLOW|PDB_ENTER;
#endif

/*
 * Given a map and a machine independent protection code,
 * convert to an alpha protection code.
 */
#define pte_prot(m, p)	(protection_codes[m == pmap_kernel() ? 0 : 1][p])
int	protection_codes[2][8];

/*
 * kernel_lev1map:
 *
 *	Kernel level 1 page table.  This maps all kernel level 2
 *	page table pages, and is used as a template for all user
 *	pmap level 1 page tables.  When a new user level 1 page
 *	table is allocated, all kernel_lev1map PTEs for kernel
 *	addresses are copied to the new map.
 *
 *	The kernel also has an initial set of kernel level 2 page
 *	table pages.  These map the kernel level 3 page table pages.
 *	As kernel level 3 page table pages are added, more level 2
 *	page table pages may be added to map them.  These pages are
 *	never freed.
 *
 *	Finally, the kernel also has an initial set of kernel level
 *	3 page table pages.  These map pages in K1SEG.  More level
 *	3 page table pages may be added at run-time if additional
 *	K1SEG address space is required.  These pages are never freed.
 *
 * NOTE: When mappings are inserted into the kernel pmap, all
 * level 2 and level 3 page table pages must already be allocated
 * and mapped into the parent page table.
 */
pt_entry_t	*kernel_lev1map;

/*
 * Virtual Page Table.
 */
pt_entry_t	*VPT;

struct pmap	kernel_pmap_store
	[(PMAP_SIZEOF(ALPHA_MAXPROCS) + sizeof(struct pmap) - 1)
		/ sizeof(struct pmap)];

paddr_t    	avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vaddr_t		pmap_maxkvaddr;	/* VA of last avail page (pmap_growkernel) */

boolean_t	pmap_initialized;	/* Has pmap_init completed? */

u_long		pmap_pages_stolen;	/* instrumentation */

/*
 * This variable contains the number of CPU IDs we need to allocate
 * space for when allocating the pmap structure.  It is used to
 * size a per-CPU array of ASN and ASN Generation number.
 */
u_long		pmap_ncpuids;

#ifndef PMAP_PV_LOWAT
#define	PMAP_PV_LOWAT	16
#endif
int		pmap_pv_lowat = PMAP_PV_LOWAT;

/*
 * List of all pmaps, used to update them when e.g. additional kernel
 * page tables are allocated.  This list is kept LRU-ordered by
 * pmap_activate().
 */
TAILQ_HEAD(, pmap) pmap_all_pmaps;

/*
 * The pools from which pmap structures and sub-structures are allocated.
 */
struct pool pmap_pmap_pool;
struct pool pmap_l1pt_pool;
struct pool pmap_pv_pool;

/*
 * Address Space Numbers.
 *
 * On many implementations of the Alpha architecture, the TLB entries and
 * I-cache blocks are tagged with a unique number within an implementation-
 * specified range.  When a process context becomes active, the ASN is used
 * to match TLB entries; if a TLB entry for a particular VA does not match
 * the current ASN, it is ignored (one could think of the processor as
 * having a collection of <max ASN> separate TLBs).  This allows operating
 * system software to skip the TLB flush that would otherwise be necessary
 * at context switch time.
 *
 * Alpha PTEs have a bit in them (PG_ASM - Address Space Match) that
 * causes TLB entries to match any ASN.  The PALcode also provides
 * a TBI (Translation Buffer Invalidate) operation that flushes all
 * TLB entries that _do not_ have PG_ASM.  We use this bit for kernel
 * mappings, so that invalidation of all user mappings does not invalidate
 * kernel mappings (which are consistent across all processes).
 *
 * pma_asn always indicates to the next ASN to use.  When
 * pma_asn exceeds pmap_max_asn, we start a new ASN generation.
 *
 * When a new ASN generation is created, the per-process (i.e. non-PG_ASM)
 * TLB entries and the I-cache are flushed, the generation number is bumped,
 * and pma_asn is changed to indicate the first non-reserved ASN.
 *
 * We reserve ASN #0 for pmaps that use the global kernel_lev1map.  This
 * prevents the following scenario:
 *
 *	* New ASN generation starts, and process A is given ASN #0.
 *
 *	* A new process B (and thus new pmap) is created.  The ASN,
 *	  for lack of a better value, is initialized to 0.
 *
 *	* Process B runs.  It is now using the TLB entries tagged
 *	  by process A.  *poof*
 *
 * In the scenario above, in addition to the processor using using incorrect
 * TLB entries, the PALcode might use incorrect information to service a
 * TLB miss.  (The PALcode uses the recursively mapped Virtual Page Table
 * to locate the PTE for a faulting address, and tagged TLB entries exist
 * for the Virtual Page Table addresses in order to speed up this procedure,
 * as well.)
 *
 * By reserving an ASN for kernel_lev1map users, we are guaranteeing that
 * new pmaps will initially run with no TLB entries for user addresses
 * or VPT mappings that map user page tables.  Since kernel_lev1map only
 * contains mappings for kernel addresses, and since those mappings
 * are always made with PG_ASM, sharing an ASN for kernel_lev1map users is
 * safe (since PG_ASM mappings match any ASN).
 *
 * On processors that do not support ASNs, the PALcode invalidates
 * the TLB and I-cache automatically on swpctx.  We still still go
 * through the motions of assigning an ASN (really, just refreshing
 * the ASN generation in this particular case) to keep the logic sane
 * in other parts of the code.
 */
u_int	pmap_max_asn;		/* max ASN supported by the system */
				/* next ASN and current ASN generation */
struct pmap_asn_info pmap_asn_info[ALPHA_MAXPROCS];

/*
 * Locking:
 *
 *	* pm_mtx (per-pmap) - This lock protects all of the members
 *	  of the pmap structure itself.
 *
 *	* pvh_mtx (per-page) - This locks protects the list of mappings
 *	  of a (managed) physical page.
 *
 *	* pmap_all_pmaps_mtx - This lock protects the global list of
 *	  all pmaps.  Note that a pm_slock must never be held while this
 *	  lock is held.
 *
 *	* pmap_growkernel_mtx - This lock protects pmap_growkernel()
 *	  and the pmap_maxkvaddr variable.
 *
 *	  There is a lock ordering constraint for pmap_growkernel_mtx.
 *	  pmap_growkernel() acquires the locks in the following order:
 *
 *		pmap_growkernel_mtx -> pmap_all_pmaps_mtx ->
 *		    pmap->pm_mtx
 *
 *	Address space number management (global ASN counters and per-pmap
 *	ASN state) are not locked; they use arrays of values indexed
 *	per-processor.
 *
 *	All internal functions which operate on a pmap are called
 *	with the pmap already locked by the caller (which will be
 *	an interface function).
 */
struct mutex pmap_all_pmaps_mtx;
struct mutex pmap_growkernel_mtx;

#define PMAP_LOCK(pmap)		mtx_enter(&pmap->pm_mtx)
#define PMAP_UNLOCK(pmap)	mtx_leave(&pmap->pm_mtx)

#if defined(MULTIPROCESSOR)
/*
 * TLB Shootdown:
 *
 * When a mapping is changed in a pmap, the TLB entry corresponding to
 * the virtual address must be invalidated on all processors.  In order
 * to accomplish this on systems with multiple processors, messages are
 * sent from the processor which performs the mapping change to all
 * processors on which the pmap is active.  For other processors, the
 * ASN generation numbers for that processor is invalidated, so that
 * the next time the pmap is activated on that processor, a new ASN
 * will be allocated (which implicitly invalidates all TLB entries).
 *
 * Note, we can use the pool allocator to allocate job entries
 * since pool pages are mapped with K0SEG, not with the TLB.
 */
struct pmap_tlb_shootdown_job {
	TAILQ_ENTRY(pmap_tlb_shootdown_job) pj_list;
	vaddr_t pj_va;			/* virtual address */
	pmap_t pj_pmap;			/* the pmap which maps the address */
	pt_entry_t pj_pte;		/* the PTE bits */
};

/* If we have more pending jobs than this, we just nail the whole TLB. */
#define	PMAP_TLB_SHOOTDOWN_MAXJOBS	6

struct pmap_tlb_shootdown_q {
	TAILQ_HEAD(, pmap_tlb_shootdown_job) pq_head;
	TAILQ_HEAD(, pmap_tlb_shootdown_job) pq_free;
	int pq_pte;			/* aggregate low PTE bits */
	int pq_tbia;			/* pending global flush */
	struct mutex pq_mtx;		/* queue lock */
	struct pmap_tlb_shootdown_job pq_jobs[PMAP_TLB_SHOOTDOWN_MAXJOBS];
} pmap_tlb_shootdown_q[ALPHA_MAXPROCS];

#define	PSJQ_LOCK(pq, s)	mtx_enter(&(pq)->pq_mtx)
#define	PSJQ_UNLOCK(pq, s)	mtx_leave(&(pq)->pq_mtx)

void	pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *);
struct pmap_tlb_shootdown_job *pmap_tlb_shootdown_job_get
	    (struct pmap_tlb_shootdown_q *);
void	pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *,
	    struct pmap_tlb_shootdown_job *);
#endif /* MULTIPROCESSOR */

#define	PAGE_IS_MANAGED(pa)	(vm_physseg_find(atop(pa), NULL) != -1)

/*
 * Internal routines
 */
void	alpha_protection_init(void);
void	pmap_do_remove(pmap_t, vaddr_t, vaddr_t, boolean_t);
boolean_t pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *,
	    boolean_t, cpuid_t);
void	pmap_changebit(struct vm_page *, pt_entry_t, pt_entry_t, cpuid_t);

/*
 * PT page management functions.
 */
int	pmap_lev1map_create(pmap_t, cpuid_t);
void	pmap_lev1map_destroy(pmap_t);
int	pmap_ptpage_alloc(pmap_t, pt_entry_t *, int);
void	pmap_ptpage_free(pmap_t, pt_entry_t *);
void	pmap_l3pt_delref(pmap_t, vaddr_t, pt_entry_t *, cpuid_t);
void	pmap_l2pt_delref(pmap_t, pt_entry_t *, pt_entry_t *);
void	pmap_l1pt_delref(pmap_t, pt_entry_t *);

void	*pmap_l1pt_alloc(struct pool *, int, int *);
void	pmap_l1pt_free(struct pool *, void *);

struct pool_allocator pmap_l1pt_allocator = {
	pmap_l1pt_alloc, pmap_l1pt_free, 0,
};

void	pmap_l1pt_ctor(pt_entry_t *);

/*
 * PV table management functions.
 */
int	pmap_pv_enter(pmap_t, struct vm_page *, vaddr_t, pt_entry_t *,
	    boolean_t);
void	pmap_pv_remove(pmap_t, struct vm_page *, vaddr_t, boolean_t);
void	*pmap_pv_page_alloc(struct pool *, int, int *);
void	pmap_pv_page_free(struct pool *, void *);

struct pool_allocator pmap_pv_page_allocator = {
	pmap_pv_page_alloc, pmap_pv_page_free, 0,
};

#ifdef DEBUG
void	pmap_pv_dump(paddr_t);
#endif

#define	pmap_pv_alloc()		pool_get(&pmap_pv_pool, PR_NOWAIT)
#define	pmap_pv_free(pv)	pool_put(&pmap_pv_pool, (pv))

/*
 * ASN management functions.
 */
void	pmap_asn_alloc(pmap_t, cpuid_t);

/*
 * Misc. functions.
 */
boolean_t pmap_physpage_alloc(int, paddr_t *);
void	pmap_physpage_free(paddr_t);
int	pmap_physpage_addref(void *);
int	pmap_physpage_delref(void *);

/*
 * PMAP_ISACTIVE{,_TEST}:
 *
 *	Check to see if a pmap is active on the current processor.
 */
#define	PMAP_ISACTIVE_TEST(pm, cpu_id)					\
	(((pm)->pm_cpus & (1UL << (cpu_id))) != 0)

#if defined(DEBUG) && !defined(MULTIPROCESSOR)
#define	PMAP_ISACTIVE(pm, cpu_id)					\
({									\
	/*								\
	 * XXX This test is not MP-safe.				\
	 */								\
	int isactive_ = PMAP_ISACTIVE_TEST(pm, cpu_id);			\
									\
	if (curproc != NULL && curproc->p_vmspace != NULL &&		\
	    (pm) != pmap_kernel() &&					\
	    (isactive_ ^ ((pm) == curproc->p_vmspace->vm_map.pmap)))	\
		panic("PMAP_ISACTIVE, isa: %d pm: %p curpm:%p",		\
		    isactive_, (pm), curproc->p_vmspace->vm_map.pmap);	\
	(isactive_);							\
})
#else
#define	PMAP_ISACTIVE(pm, cpu_id)	PMAP_ISACTIVE_TEST(pm, cpu_id)
#endif /* DEBUG && !MULTIPROCESSOR */

/*
 * PMAP_ACTIVATE_ASN_SANITY:
 *
 *	DEBUG sanity checks for ASNs within PMAP_ACTIVATE.
 */
#ifdef DEBUG
#define	PMAP_ACTIVATE_ASN_SANITY(pmap, cpu_id)				\
do {									\
	struct pmap_asn_info *__pma = &(pmap)->pm_asni[(cpu_id)];	\
	struct pmap_asn_info *__cpma = &pmap_asn_info[(cpu_id)];	\
									\
	if ((pmap)->pm_lev1map == kernel_lev1map) {			\
		/*							\
		 * This pmap implementation also ensures that pmaps	\
		 * referencing kernel_lev1map use a reserved ASN	\
		 * ASN to prevent the PALcode from servicing a TLB	\
		 * miss	with the wrong PTE.				\
		 */							\
		if (__pma->pma_asn != PMAP_ASN_RESERVED) {		\
			printf("kernel_lev1map with non-reserved ASN "	\
			    "(line %d)\n", __LINE__);			\
			panic("PMAP_ACTIVATE_ASN_SANITY");		\
		}							\
	} else {							\
		if (__pma->pma_asngen != __cpma->pma_asngen) {		\
			/*						\
			 * ASN generation number isn't valid!		\
			 */						\
			printf("pmap asngen %lu, current %lu "		\
			    "(line %d)\n",				\
			    __pma->pma_asngen, 				\
			    __cpma->pma_asngen, 			\
			    __LINE__);					\
			panic("PMAP_ACTIVATE_ASN_SANITY");		\
		}							\
		if (__pma->pma_asn == PMAP_ASN_RESERVED) {		\
			/*						\
			 * DANGER WILL ROBINSON!  We're going to	\
			 * pollute the VPT TLB entries!			\
			 */						\
			printf("Using reserved ASN! (line %d)\n",	\
			    __LINE__);					\
			panic("PMAP_ACTIVATE_ASN_SANITY");		\
		}							\
	}								\
} while (0)
#else
#define	PMAP_ACTIVATE_ASN_SANITY(pmap, cpu_id)	/* nothing */
#endif

/*
 * PMAP_ACTIVATE:
 *
 *	This is essentially the guts of pmap_activate(), without
 *	ASN allocation.  This is used by pmap_activate(),
 *	pmap_lev1map_create(), and pmap_lev1map_destroy().
 *
 *	This is called only when it is known that a pmap is "active"
 *	on the current processor; the ASN must already be valid.
 */
#define	PMAP_ACTIVATE(pmap, p, cpu_id)					\
do {									\
	PMAP_ACTIVATE_ASN_SANITY(pmap, cpu_id);				\
									\
	(p)->p_addr->u_pcb.pcb_hw.apcb_ptbr =				\
	    ALPHA_K0SEG_TO_PHYS((vaddr_t)(pmap)->pm_lev1map) >> PGSHIFT; \
	(p)->p_addr->u_pcb.pcb_hw.apcb_asn =				\
	    (pmap)->pm_asni[(cpu_id)].pma_asn;				\
									\
	if ((p) == curproc) {						\
		/*							\
		 * Page table base register has changed; switch to	\
		 * our own context again so that it will take effect.	\
		 */							\
		(void) alpha_pal_swpctx((u_long)p->p_md.md_pcbpaddr);	\
	}								\
} while (0)

/*
 * PMAP_SET_NEEDISYNC:
 *
 *	Mark that a user pmap needs an I-stream synch on its
 *	way back out to userspace.
 */
#define	PMAP_SET_NEEDISYNC(pmap)	(pmap)->pm_needisync = ~0UL

/*
 * PMAP_SYNC_ISTREAM:
 *
 *	Synchronize the I-stream for the specified pmap.  For user
 *	pmaps, this is deferred until a process using the pmap returns
 *	to userspace.
 */
#if defined(MULTIPROCESSOR)
#define	PMAP_SYNC_ISTREAM_KERNEL()					\
do {									\
	alpha_pal_imb();						\
	alpha_broadcast_ipi(ALPHA_IPI_IMB);				\
} while (0)

#define	PMAP_SYNC_ISTREAM_USER(pmap)					\
do {									\
	alpha_multicast_ipi((pmap)->pm_cpus, ALPHA_IPI_AST);		\
	/* for curcpu, do it before userret() */			\
} while (0)
#else
#define	PMAP_SYNC_ISTREAM_KERNEL()	alpha_pal_imb()
#define	PMAP_SYNC_ISTREAM_USER(pmap)	/* done before userret() */
#endif /* MULTIPROCESSOR */

#define	PMAP_SYNC_ISTREAM(pmap)						\
do {									\
	if ((pmap) == pmap_kernel())					\
		PMAP_SYNC_ISTREAM_KERNEL();				\
	else								\
		PMAP_SYNC_ISTREAM_USER(pmap);				\
} while (0)

/*
 * PMAP_INVALIDATE_ASN:
 *
 *	Invalidate the specified pmap's ASN, so as to force allocation
 *	of a new one the next time pmap_asn_alloc() is called.
 *
 *	NOTE: THIS MUST ONLY BE CALLED IF AT LEAST ONE OF THE FOLLOWING
 *	CONDITIONS ARE TRUE:
 *
 *		(1) The pmap references the global kernel_lev1map.
 *
 *		(2) The pmap is not active on the current processor.
 */
#define	PMAP_INVALIDATE_ASN(pmap, cpu_id)				\
do {									\
	(pmap)->pm_asni[(cpu_id)].pma_asn = PMAP_ASN_RESERVED;		\
} while (0)

/*
 * PMAP_INVALIDATE_TLB:
 *
 *	Invalidate the TLB entry for the pmap/va pair.
 */
#define	PMAP_INVALIDATE_TLB(pmap, va, hadasm, isactive, cpu_id)		\
do {									\
	if ((hadasm) || (isactive)) {					\
		/*							\
		 * Simply invalidating the TLB entry and I-cache	\
		 * works in this case.					\
		 */							\
		ALPHA_TBIS((va));					\
	} else if ((pmap)->pm_asni[(cpu_id)].pma_asngen ==		\
		    pmap_asn_info[(cpu_id)].pma_asngen) {		\
		/*							\
		 * We can't directly invalidate the TLB entry		\
		 * in this case, so we have to force allocation		\
		 * of a new ASN the next time this pmap becomes		\
		 * active.						\
		 */							\
		PMAP_INVALIDATE_ASN((pmap), (cpu_id));			\
	}								\
		/*							\
		 * Nothing to do in this case; the next time the	\
		 * pmap becomes active on this processor, a new		\
		 * ASN will be allocated anyway.			\
		 */							\
} while (0)

/*
 * PMAP_KERNEL_PTE:
 *
 *	Get a kernel PTE.
 *
 *	If debugging, do a table walk.  If not debugging, just use
 *	the Virtual Page Table, since all kernel page tables are
 *	pre-allocated and mapped in.
 */
#ifdef DEBUG
#define	PMAP_KERNEL_PTE(va)						\
({									\
	pt_entry_t *l1pte_, *l2pte_;					\
									\
	l1pte_ = pmap_l1pte(pmap_kernel(), va);				\
	if (pmap_pte_v(l1pte_) == 0) {					\
		printf("kernel level 1 PTE not valid, va 0x%lx "	\
		    "(line %d)\n", (va), __LINE__);			\
		panic("PMAP_KERNEL_PTE");				\
	}								\
	l2pte_ = pmap_l2pte(pmap_kernel(), va, l1pte_);			\
	if (pmap_pte_v(l2pte_) == 0) {					\
		printf("kernel level 2 PTE not valid, va 0x%lx "	\
		    "(line %d)\n", (va), __LINE__);			\
		panic("PMAP_KERNEL_PTE");				\
	}								\
	pmap_l3pte(pmap_kernel(), va, l2pte_);				\
})
#else
#define	PMAP_KERNEL_PTE(va)	(&VPT[VPT_INDEX((va))])
#endif

/*
 * PMAP_SET_PTE:
 *
 *	Set a PTE to a specified value.
 */
#define	PMAP_SET_PTE(ptep, val)	*(ptep) = (val)

/*
 * PMAP_STAT_{INCR,DECR}:
 *
 *	Increment or decrement a pmap statistic.
 */
#define	PMAP_STAT_INCR(s, v)	atomic_add_ulong((unsigned long *)(&(s)), (v))
#define	PMAP_STAT_DECR(s, v)	atomic_sub_ulong((unsigned long *)(&(s)), (v))

/*
 * pmap_bootstrap:
 *
 *	Bootstrap the system to run with virtual memory.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_bootstrap(paddr_t ptaddr, u_int maxasn, u_long ncpuids)
{
	vsize_t lev2mapsize, lev3mapsize;
	pt_entry_t *lev2map, *lev3map;
	pt_entry_t pte;
	int i;
#ifdef MULTIPROCESSOR
	int j;
#endif

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_BOOTSTRAP))
		printf("pmap_bootstrap(0x%lx, %u)\n", ptaddr, maxasn);
#endif

	/*
	 * Compute the number of pages kmem_map will have.
	 */
	kmeminit_nkmempages();

	/*
	 * Figure out how many PTE's are necessary to map the kernel.
	 */
	lev3mapsize = (VM_PHYS_SIZE + 16 * NCARGS + PAGER_MAP_SIZE) /
	    PAGE_SIZE + (maxthread * UPAGES) + nkmempages;

#ifdef SYSVSHM
	lev3mapsize += shminfo.shmall;
#endif
	lev3mapsize = roundup(lev3mapsize, NPTEPG);

	/*
	 * Allocate a level 1 PTE table for the kernel.
	 * This is always one page long.
	 * IF THIS IS NOT A MULTIPLE OF PAGE_SIZE, ALL WILL GO TO HELL.
	 */
	kernel_lev1map = (pt_entry_t *)
	    pmap_steal_memory(sizeof(pt_entry_t) * NPTEPG, NULL, NULL);

	/*
	 * Allocate a level 2 PTE table for the kernel.
	 * These must map all of the level3 PTEs.
	 * IF THIS IS NOT A MULTIPLE OF PAGE_SIZE, ALL WILL GO TO HELL.
	 */
	lev2mapsize = roundup(howmany(lev3mapsize, NPTEPG), NPTEPG);
	lev2map = (pt_entry_t *)
	    pmap_steal_memory(sizeof(pt_entry_t) * lev2mapsize, NULL, NULL);

	/*
	 * Allocate a level 3 PTE table for the kernel.
	 * Contains lev3mapsize PTEs.
	 */
	lev3map = (pt_entry_t *)
	    pmap_steal_memory(sizeof(pt_entry_t) * lev3mapsize, NULL, NULL);

	/*
	 * Set up level 1 page table
	 */

	/* Map all of the level 2 pte pages */
	for (i = 0; i < howmany(lev2mapsize, NPTEPG); i++) {
		pte = (ALPHA_K0SEG_TO_PHYS(((vaddr_t)lev2map) +
		    (i*PAGE_SIZE)) >> PGSHIFT) << PG_SHIFT;
		pte |= PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		kernel_lev1map[l1pte_index(VM_MIN_KERNEL_ADDRESS +
		    (i*PAGE_SIZE*NPTEPG*NPTEPG))] = pte;
	}

	/* Map the virtual page table */
	pte = (ALPHA_K0SEG_TO_PHYS((vaddr_t)kernel_lev1map) >> PGSHIFT)
	    << PG_SHIFT;
	pte |= PG_V | PG_KRE | PG_KWE; /* NOTE NO ASM */
	kernel_lev1map[l1pte_index(VPTBASE)] = pte;
	VPT = (pt_entry_t *)VPTBASE;

#ifdef _PMAP_MAY_USE_PROM_CONSOLE
    {
	extern pt_entry_t prom_pte;			/* XXX */
	extern int prom_mapped;				/* XXX */

	if (pmap_uses_prom_console()) {
		/*
		 * XXX Save old PTE so we can remap the PROM, if
		 * XXX necessary.
		 */
		prom_pte = *(pt_entry_t *)ptaddr & ~PG_ASM;
	}
	prom_mapped = 0;

	/*
	 * Actually, this code lies.  The prom is still mapped, and will
	 * remain so until the context switch after alpha_init() returns.
	 */
    }
#endif

	/*
	 * Set up level 2 page table.
	 */
	/* Map all of the level 3 pte pages */
	for (i = 0; i < howmany(lev3mapsize, NPTEPG); i++) {
		pte = (ALPHA_K0SEG_TO_PHYS(((vaddr_t)lev3map) +
		    (i*PAGE_SIZE)) >> PGSHIFT) << PG_SHIFT;
		pte |= PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		lev2map[l2pte_index(VM_MIN_KERNEL_ADDRESS+
		    (i*PAGE_SIZE*NPTEPG))] = pte;
	}

	/* Initialize the pmap_growkernel_mtx. */
	mtx_init(&pmap_growkernel_mtx, IPL_NONE);

	/*
	 * Set up level three page table (lev3map)
	 */
	/* Nothing to do; it's already zeroed */

	/*
	 * Initialize `FYI' variables.  Note we're relying on
	 * the fact that BSEARCH sorts the vm_physmem[] array
	 * for us.
	 */
	avail_start = ptoa(vm_physmem[0].start);
	avail_end = ptoa(vm_physmem[vm_nphysseg - 1].end);

	pmap_maxkvaddr = VM_MIN_KERNEL_ADDRESS + lev3mapsize * PAGE_SIZE;

#if 0
	printf("avail_start = 0x%lx\n", avail_start);
	printf("avail_end = 0x%lx\n", avail_end);
#endif

	/*
	 * Initialize the pmap pools and list.
	 */
	pmap_ncpuids = ncpuids;
	pool_init(&pmap_pmap_pool, PMAP_SIZEOF(pmap_ncpuids), 0, IPL_NONE, 0,
	    "pmappl", &pool_allocator_single);
	pool_init(&pmap_l1pt_pool, PAGE_SIZE, 0, IPL_VM, 0,
	    "l1ptpl", &pmap_l1pt_allocator);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pvpl", &pmap_pv_page_allocator);

	TAILQ_INIT(&pmap_all_pmaps);

	/*
	 * Initialize the ASN logic.
	 */
	pmap_max_asn = maxasn;
	for (i = 0; i < ALPHA_MAXPROCS; i++) {
		pmap_asn_info[i].pma_asn = 1;
		pmap_asn_info[i].pma_asngen = 0;
	}

	/*
	 * Initialize the locks.
	 */
	mtx_init(&pmap_all_pmaps_mtx, IPL_NONE);

	/*
	 * Initialize kernel pmap.  Note that all kernel mappings
	 * have PG_ASM set, so the ASN doesn't really matter for
	 * the kernel pmap.  Also, since the kernel pmap always
	 * references kernel_lev1map, it always has an invalid ASN
	 * generation.
	 */
	memset(pmap_kernel(), 0, sizeof(pmap_kernel()));
	pmap_kernel()->pm_lev1map = kernel_lev1map;
	pmap_kernel()->pm_count = 1;
	for (i = 0; i < ALPHA_MAXPROCS; i++) {
		pmap_kernel()->pm_asni[i].pma_asn = PMAP_ASN_RESERVED;
		pmap_kernel()->pm_asni[i].pma_asngen =
		    pmap_asn_info[i].pma_asngen;
	}
	TAILQ_INSERT_TAIL(&pmap_all_pmaps, pmap_kernel(), pm_list);

#if defined(MULTIPROCESSOR)
	/*
	 * Initialize the TLB shootdown queues.
	 */
	for (i = 0; i < ALPHA_MAXPROCS; i++) {
		TAILQ_INIT(&pmap_tlb_shootdown_q[i].pq_head);
		TAILQ_INIT(&pmap_tlb_shootdown_q[i].pq_free);
		for (j = 0; j < PMAP_TLB_SHOOTDOWN_MAXJOBS; j++)
			TAILQ_INSERT_TAIL(&pmap_tlb_shootdown_q[i].pq_free,
			    &pmap_tlb_shootdown_q[i].pq_jobs[j], pj_list);
		mtx_init(&pmap_tlb_shootdown_q[i].pq_mtx, IPL_IPI);
	}
#endif

	/*
	 * Set up proc0's PCB such that the ptbr points to the right place
	 * and has the kernel pmap's (really unused) ASN.
	 */
	proc0.p_addr->u_pcb.pcb_hw.apcb_ptbr =
	    ALPHA_K0SEG_TO_PHYS((vaddr_t)kernel_lev1map) >> PGSHIFT;
	proc0.p_addr->u_pcb.pcb_hw.apcb_asn =
	    pmap_kernel()->pm_asni[cpu_number()].pma_asn;

	/*
	 * Mark the kernel pmap `active' on this processor.
	 */
	atomic_setbits_ulong(&pmap_kernel()->pm_cpus,
	    (1UL << cpu_number()));
}

#ifdef _PMAP_MAY_USE_PROM_CONSOLE
int
pmap_uses_prom_console(void)
{

#if defined(NEW_SCC_DRIVER)
	return (cputype == ST_DEC_21000);
#else
	return (cputype == ST_DEC_21000
	    || cputype == ST_DEC_3000_300
	    || cputype == ST_DEC_3000_500);
#endif /* NEW_SCC_DRIVER */
}
#endif /* _PMAP_MAY_USE_PROM_CONSOLE */

/*
 * pmap_steal_memory:		[ INTERFACE ]
 *
 *	Bootstrap memory allocator (alternative to vm_bootstrap_steal_memory()).
 *	This function allows for early dynamic memory allocation until the
 *	virtual memory system has been bootstrapped.  After that point, either
 *	kmem_alloc or malloc should be used.  This function works by stealing
 *	pages from the (to be) managed page pool, then implicitly mapping the
 *	pages (by using their k0seg addresses) and zeroing them.
 *
 *	It may be used once the physical memory segments have been pre-loaded
 *	into the vm_physmem[] array.  Early memory allocation MUST use this
 *	interface!  This cannot be used after vm_page_startup(), and will
 *	generate a panic if tried.
 *
 *	Note that this memory will never be freed, and in essence it is wired
 *	down.
 *
 *	Note: no locking is necessary in this function.
 */
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
{
	int bank, npgs, x;
	vaddr_t va;
	paddr_t pa;

	size = round_page(size);
	npgs = atop(size);

#if 0
	printf("PSM: size 0x%lx (npgs 0x%x)\n", size, npgs);
#endif

	for (bank = 0; bank < vm_nphysseg; bank++) {
		if (uvm.page_init_done == TRUE)
			panic("pmap_steal_memory: called _after_ bootstrap");

#if 0
		printf("     bank %d: avail_start 0x%lx, start 0x%lx, "
		    "avail_end 0x%lx\n", bank, vm_physmem[bank].avail_start,
		    vm_physmem[bank].start, vm_physmem[bank].avail_end);
#endif

		if (vm_physmem[bank].avail_start != vm_physmem[bank].start ||
		    vm_physmem[bank].avail_start >= vm_physmem[bank].avail_end)
			continue;

#if 0
		printf("             avail_end - avail_start = 0x%lx\n",
		    vm_physmem[bank].avail_end - vm_physmem[bank].avail_start);
#endif

		if ((vm_physmem[bank].avail_end - vm_physmem[bank].avail_start)
		    < npgs)
			continue;

		/*
		 * There are enough pages here; steal them!
		 */
		pa = ptoa(vm_physmem[bank].avail_start);
		vm_physmem[bank].avail_start += npgs;
		vm_physmem[bank].start += npgs;

		/*
		 * Have we used up this segment?
		 */
		if (vm_physmem[bank].avail_start == vm_physmem[bank].end) {
			if (vm_nphysseg == 1)
				panic("pmap_steal_memory: out of memory!");

			/* Remove this segment from the list. */
			vm_nphysseg--;
			for (x = bank; x < vm_nphysseg; x++) {
				/* structure copy */
				vm_physmem[x] = vm_physmem[x + 1];
			}
		}

		/*
		 * Fill these in for the caller; we don't modify them,
		 * but the upper layers still want to know.
		 */
		if (vstartp)
			*vstartp = VM_MIN_KERNEL_ADDRESS;
		if (vendp)
			*vendp = VM_MAX_KERNEL_ADDRESS;

		va = ALPHA_PHYS_TO_K0SEG(pa);
		memset((caddr_t)va, 0, size);
		pmap_pages_stolen += npgs;
		return (va);
	}

	/*
	 * If we got here, this was no memory left.
	 */
	panic("pmap_steal_memory: no memory to steal");
}

/*
 * pmap_init:			[ INTERFACE ]
 *
 *	Initialize the pmap module.  Called by uvm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_init(void)
{

#ifdef DEBUG
        if (pmapdebug & PDB_FOLLOW)
                printf("pmap_init()\n");
#endif

	/* initialize protection array */
	alpha_protection_init();

	/*
	 * Set a low water mark on the pv_entry pool, so that we are
	 * more likely to have these around even in extreme memory
	 * starvation.
	 */
	pool_setlowat(&pmap_pv_pool, pmap_pv_lowat);

	/*
	 * Now it is safe to enable pv entry recording.
	 */
	pmap_initialized = TRUE;

#if 0
	for (bank = 0; bank < vm_nphysseg; bank++) {
		printf("bank %d\n", bank);
		printf("\tstart = 0x%x\n", ptoa(vm_physmem[bank].start));
		printf("\tend = 0x%x\n", ptoa(vm_physmem[bank].end));
		printf("\tavail_start = 0x%x\n",
		    ptoa(vm_physmem[bank].avail_start));
		printf("\tavail_end = 0x%x\n",
		    ptoa(vm_physmem[bank].avail_end));
	}
#endif
}

/*
 * pmap_create:			[ INTERFACE ]
 *
 *	Create and return a physical map.
 */
pmap_t
pmap_create(void)
{
	pmap_t pmap;
	int i;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create()\n");
#endif

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK|PR_ZERO);

	pmap->pm_count = 1;
	for (i = 0; i < pmap_ncpuids; i++) {
		pmap->pm_asni[i].pma_asn = PMAP_ASN_RESERVED;
		/* XXX Locking? */
		pmap->pm_asni[i].pma_asngen = pmap_asn_info[i].pma_asngen;
	}
	mtx_init(&pmap->pm_mtx, IPL_VM);

	for (;;) {
		mtx_enter(&pmap_growkernel_mtx);
		i = pmap_lev1map_create(pmap, cpu_number());
		mtx_leave(&pmap_growkernel_mtx);
		if (i == 0)
			break;
		uvm_wait(__func__);
	}

	mtx_enter(&pmap_all_pmaps_mtx);
	TAILQ_INSERT_TAIL(&pmap_all_pmaps, pmap, pm_list);
	mtx_leave(&pmap_all_pmaps_mtx);

	return (pmap);
}

/*
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
 */
void
pmap_destroy(pmap_t pmap)
{
	int refs;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%p)\n", pmap);
#endif

	refs = atomic_dec_int_nv(&pmap->pm_count);
	if (refs > 0)
		return;

	/*
	 * Remove it from the global list of all pmaps.
	 */
	mtx_enter(&pmap_all_pmaps_mtx);
	TAILQ_REMOVE(&pmap_all_pmaps, pmap, pm_list);
	mtx_leave(&pmap_all_pmaps_mtx);

	mtx_enter(&pmap_growkernel_mtx);
	pmap_lev1map_destroy(pmap);
	mtx_leave(&pmap_growkernel_mtx);

	pool_put(&pmap_pmap_pool, pmap);
}

/*
 * pmap_reference:		[ INTERFACE ]
 *
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap_t pmap)
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%p)\n", pmap);
#endif

	atomic_inc_int(&pmap->pm_count);
}

/*
 * pmap_remove:			[ INTERFACE ]
 *
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
#endif

	pmap_do_remove(pmap, sva, eva, TRUE);
}

/*
 * pmap_do_remove:
 *
 *	This actually removes the range of addresses from the
 *	specified map.  It is used by pmap_collect() (does not
 *	want to remove wired mappings) and pmap_remove() (does
 *	want to remove wired mappings).
 */
void
pmap_do_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva, boolean_t dowired)
{
	pt_entry_t *l1pte, *l2pte, *l3pte;
	pt_entry_t *saved_l1pte, *saved_l2pte, *saved_l3pte;
	vaddr_t l1eva, l2eva, vptva;
	boolean_t needisync = FALSE;
	cpuid_t cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
#endif

	/*
	 * If this is the kernel pmap, we can use a faster method
	 * for accessing the PTEs (since the PT pages are always
	 * resident).
	 *
	 * Note that this routine should NEVER be called from an
	 * interrupt context; pmap_kremove() is used for that.
	 */
	if (pmap == pmap_kernel()) {
		PMAP_LOCK(pmap);

		KASSERT(dowired == TRUE);

		while (sva < eva) {
			l3pte = PMAP_KERNEL_PTE(sva);
			if (pmap_pte_v(l3pte)) {
#ifdef DIAGNOSTIC
				if (PAGE_IS_MANAGED(pmap_pte_pa(l3pte)) &&
				    pmap_pte_pv(l3pte) == 0)
					panic("pmap_remove: managed page "
					    "without PG_PVLIST for 0x%lx",
					    sva);
#endif
				needisync |= pmap_remove_mapping(pmap, sva,
				    l3pte, TRUE, cpu_id);
			}
			sva += PAGE_SIZE;
		}

		PMAP_UNLOCK(pmap);

		if (needisync)
			PMAP_SYNC_ISTREAM_KERNEL();
		return;
	}

#ifdef DIAGNOSTIC
	if (sva > VM_MAXUSER_ADDRESS || eva > VM_MAXUSER_ADDRESS)
		panic("pmap_remove: (0x%lx - 0x%lx) user pmap, kernel "
		    "address range", sva, eva);
#endif

	PMAP_LOCK(pmap);

	/*
	 * If we're already referencing the kernel_lev1map, there
	 * is no work for us to do.
	 */
	if (pmap->pm_lev1map == kernel_lev1map)
		goto out;

	saved_l1pte = l1pte = pmap_l1pte(pmap, sva);

	/*
	 * Add a reference to the L1 table to it won't get
	 * removed from under us.
	 */
	pmap_physpage_addref(saved_l1pte);

	for (; sva < eva; sva = l1eva, l1pte++) {
		l1eva = alpha_trunc_l1seg(sva) + ALPHA_L1SEG_SIZE;
		if (pmap_pte_v(l1pte)) {
			saved_l2pte = l2pte = pmap_l2pte(pmap, sva, l1pte);

			/*
			 * Add a reference to the L2 table so it won't
			 * get removed from under us.
			 */
			pmap_physpage_addref(saved_l2pte);

			for (; sva < l1eva && sva < eva; sva = l2eva, l2pte++) {
				l2eva =
				    alpha_trunc_l2seg(sva) + ALPHA_L2SEG_SIZE;
				if (pmap_pte_v(l2pte)) {
					saved_l3pte = l3pte =
					    pmap_l3pte(pmap, sva, l2pte);

					/*
					 * Add a reference to the L3 table so
					 * it won't get removed from under us.
					 */
					pmap_physpage_addref(saved_l3pte);

					/*
					 * Remember this sva; if the L3 table
					 * gets removed, we need to invalidate
					 * the VPT TLB entry for it.
					 */
					vptva = sva;

					for (; sva < l2eva && sva < eva;
					     sva += PAGE_SIZE, l3pte++) {
						if (pmap_pte_v(l3pte) &&
						    (dowired == TRUE ||
						     pmap_pte_w(l3pte) == 0)) {
							needisync |=
							    pmap_remove_mapping(
								pmap, sva,
								l3pte, TRUE,
								cpu_id);
						}
					}

					/*
					 * Remove the reference to the L3
					 * table that we added above.  This
					 * may free the L3 table.
					 */
					pmap_l3pt_delref(pmap, vptva,
					    saved_l3pte, cpu_id);
				}
			}

			/*
			 * Remove the reference to the L2 table that we
			 * added above.  This may free the L2 table.
			 */
			pmap_l2pt_delref(pmap, l1pte, saved_l2pte);
		}
	}

	/*
	 * Remove the reference to the L1 table that we added above.
	 * This may free the L1 table.
	 */
	pmap_l1pt_delref(pmap, saved_l1pte);

	if (needisync)
		PMAP_SYNC_ISTREAM_USER(pmap);

 out:
	PMAP_UNLOCK(pmap);
}

/*
 * pmap_page_protect:		[ INTERFACE ]
 *
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	pmap_t pmap;
	pv_entry_t pv;
	boolean_t needkisync = FALSE;
	cpuid_t cpu_id = cpu_number();
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    (prot == PROT_NONE && (pmapdebug & PDB_REMOVE)))
		printf("pmap_page_protect(%p, %x)\n", pg, prot);
#endif

	switch (prot) {
	case PROT_READ | PROT_WRITE | PROT_EXEC:
	case PROT_READ | PROT_WRITE:
		return;

	/* copy_on_write */
	case PROT_READ | PROT_EXEC:
	case PROT_READ:
		mtx_enter(&pg->mdpage.pvh_mtx);
		for (pv = pg->mdpage.pvh_list; pv != NULL; pv = pv->pv_next) {
			if (*pv->pv_pte & (PG_KWE | PG_UWE)) {
				*pv->pv_pte &= ~(PG_KWE | PG_UWE);
				PMAP_INVALIDATE_TLB(pv->pv_pmap, pv->pv_va,
				    pmap_pte_asm(pv->pv_pte),
				    PMAP_ISACTIVE(pv->pv_pmap, cpu_id), cpu_id);
				PMAP_TLB_SHOOTDOWN(pv->pv_pmap, pv->pv_va,
				    pmap_pte_asm(pv->pv_pte));
			}
		}
		mtx_leave(&pg->mdpage.pvh_mtx);
		PMAP_TLB_SHOOTNOW();
		return;

	/* remove_all */
	default:
		break;
	}

	mtx_enter(&pg->mdpage.pvh_mtx);
	while ((pv = pg->mdpage.pvh_list) != NULL) {
		pmap_reference(pv->pv_pmap);
		pmap = pv->pv_pmap;
		mtx_leave(&pg->mdpage.pvh_mtx);

		PMAP_LOCK(pmap);

		/*
		 * We dropped the pvlist lock before grabbing the pmap
		 * lock to avoid lock ordering problems.  This means
		 * we have to check the pvlist again since somebody
		 * else might have modified it.  All we care about is
		 * that the pvlist entry matches the pmap we just
		 * locked.  If it doesn't, unlock the pmap and try
		 * again.
		 */
		mtx_enter(&pg->mdpage.pvh_mtx);
		if ((pv = pg->mdpage.pvh_list) == NULL ||
		    pv->pv_pmap != pmap) {
			mtx_leave(&pg->mdpage.pvh_mtx);
			PMAP_UNLOCK(pmap);
			pmap_destroy(pmap);
			mtx_enter(&pg->mdpage.pvh_mtx);
			continue;
		}

#ifdef DEBUG
		if (pmap_pte_v(pmap_l2pte(pv->pv_pmap, pv->pv_va, NULL)) == 0 ||
		    pmap_pte_pa(pv->pv_pte) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_page_protect: bad mapping");
#endif
		if (pmap_remove_mapping(pmap, pv->pv_va, pv->pv_pte,
		    FALSE, cpu_id) == TRUE) {
			if (pmap == pmap_kernel())
				needkisync |= TRUE;
			else
				PMAP_SYNC_ISTREAM_USER(pmap);
		}
		mtx_leave(&pg->mdpage.pvh_mtx);
		PMAP_UNLOCK(pmap);
		pmap_destroy(pmap);
		mtx_enter(&pg->mdpage.pvh_mtx);
	}
	mtx_leave(&pg->mdpage.pvh_mtx);

	if (needkisync)
		PMAP_SYNC_ISTREAM_KERNEL();
}

/*
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protection on the specified range of this map
 *	as requested.
 */
void
pmap_protect(pmap_t pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	pt_entry_t *l1pte, *l2pte, *l3pte, bits;
	boolean_t isactive;
	boolean_t hadasm;
	vaddr_t l1eva, l2eva;
	cpuid_t cpu_id = cpu_number();
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%p, %lx, %lx, %x)\n",
		    pmap, sva, eva, prot);
#endif

	if ((prot & PROT_READ) == PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	PMAP_LOCK(pmap);

	bits = pte_prot(pmap, prot);
	isactive = PMAP_ISACTIVE(pmap, cpu_id);

	l1pte = pmap_l1pte(pmap, sva);
	for (; sva < eva; sva = l1eva, l1pte++) {
		l1eva = alpha_trunc_l1seg(sva) + ALPHA_L1SEG_SIZE;
		if (!pmap_pte_v(l1pte))
			continue;

		l2pte = pmap_l2pte(pmap, sva, l1pte);
		for (; sva < l1eva && sva < eva; sva = l2eva, l2pte++) {
			l2eva = alpha_trunc_l2seg(sva) + ALPHA_L2SEG_SIZE;
			if (!pmap_pte_v(l2pte))
				continue;

			l3pte = pmap_l3pte(pmap, sva, l2pte);
			for (; sva < l2eva && sva < eva;
			     sva += PAGE_SIZE, l3pte++) {
				if (!pmap_pte_v(l3pte))
					continue;

				if (pmap_pte_prot_chg(l3pte, bits)) {
					hadasm = (pmap_pte_asm(l3pte) != 0);
					pmap_pte_set_prot(l3pte, bits);
					PMAP_INVALIDATE_TLB(pmap, sva, hadasm,
					   isactive, cpu_id);
					PMAP_TLB_SHOOTDOWN(pmap, sva,
					   hadasm ? PG_ASM : 0);
				}
			}
		}
	}

	PMAP_TLB_SHOOTNOW();

	if (prot & PROT_EXEC)
		PMAP_SYNC_ISTREAM(pmap);

	PMAP_UNLOCK(pmap);
}

/*
 * pmap_enter:			[ INTERFACE ]
 *
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte can not be reclaimed.
 *
 *	Note:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	struct vm_page *pg;
	pt_entry_t *pte, npte, opte;
	paddr_t opa;
	boolean_t tflush = TRUE;
	boolean_t hadasm = FALSE;	/* XXX gcc -Wuninitialized */
	boolean_t needisync = FALSE;
	boolean_t setisync = FALSE;
	boolean_t isactive;
	boolean_t wired;
	cpuid_t cpu_id = cpu_number();
	int error = 0;
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
		       pmap, va, pa, prot, flags);
#endif
	pg = PHYS_TO_VM_PAGE(pa);
	isactive = PMAP_ISACTIVE(pmap, cpu_id);
	wired = (flags & PMAP_WIRED) != 0;

	/*
	 * Determine what we need to do about the I-stream.  If
	 * PROT_EXEC is set, we mark a user pmap as needing
	 * an I-sync on the way back out to userspace.  We always
	 * need an immediate I-sync for the kernel pmap.
	 */
	if (prot & PROT_EXEC) {
		if (pmap == pmap_kernel())
			needisync = TRUE;
		else {
			setisync = TRUE;
			needisync = (pmap->pm_cpus != 0);
		}
	}

	PMAP_LOCK(pmap);

	if (pmap == pmap_kernel()) {
#ifdef DIAGNOSTIC
		/*
		 * Sanity check the virtual address.
		 */
		if (va < VM_MIN_KERNEL_ADDRESS)
			panic("pmap_enter: kernel pmap, invalid va 0x%lx", va);
#endif
		pte = PMAP_KERNEL_PTE(va);
	} else {
		pt_entry_t *l1pte, *l2pte;

#ifdef DIAGNOSTIC
		/*
		 * Sanity check the virtual address.
		 */
		if (va >= VM_MAXUSER_ADDRESS)
			panic("pmap_enter: user pmap, invalid va 0x%lx", va);
#endif

		KASSERT(pmap->pm_lev1map != kernel_lev1map);

		/*
		 * Check to see if the level 1 PTE is valid, and
		 * allocate a new level 2 page table page if it's not.
		 * A reference will be added to the level 2 table when
		 * the level 3 table is created.
		 */
		l1pte = pmap_l1pte(pmap, va);
		if (pmap_pte_v(l1pte) == 0) {
			pmap_physpage_addref(l1pte);
			error = pmap_ptpage_alloc(pmap, l1pte, PGU_L2PT);
			if (error) {
				pmap_l1pt_delref(pmap, l1pte);
				if (flags & PMAP_CANFAIL)
					goto out;
				panic("pmap_enter: unable to create L2 PT "
				    "page");
			}
#ifdef DEBUG
			if (pmapdebug & PDB_PTPAGE)
				printf("pmap_enter: new level 2 table at "
				    "0x%lx\n", pmap_pte_pa(l1pte));
#endif
		}

		/*
		 * Check to see if the level 2 PTE is valid, and
		 * allocate a new level 3 page table page if it's not.
		 * A reference will be added to the level 3 table when
		 * the mapping is validated.
		 */
		l2pte = pmap_l2pte(pmap, va, l1pte);
		if (pmap_pte_v(l2pte) == 0) {
			pmap_physpage_addref(l2pte);
			error = pmap_ptpage_alloc(pmap, l2pte, PGU_L3PT);
			if (error) {
				pmap_l2pt_delref(pmap, l1pte, l2pte);
				if (flags & PMAP_CANFAIL)
					goto out;
				panic("pmap_enter: unable to create L3 PT "
				    "page");
			}
#ifdef DEBUG
			if (pmapdebug & PDB_PTPAGE)
				printf("pmap_enter: new level 3 table at "
				    "0x%lx\n", pmap_pte_pa(l2pte));
#endif
		}

		/*
		 * Get the PTE that will map the page.
		 */
		pte = pmap_l3pte(pmap, va, l2pte);
	}

	/* Remember all of the old PTE; used for TBI check later. */
	opte = *pte;

	/*
	 * Check to see if the old mapping is valid.  If not, validate the
	 * new one immediately.
	 */
	if (pmap_pte_v(pte) == 0) {
		/*
		 * No need to invalidate the TLB in this case; an invalid
		 * mapping won't be in the TLB, and a previously valid
		 * mapping would have been flushed when it was invalidated.
		 */
		tflush = FALSE;

		/*
		 * No need to synchronize the I-stream, either, for basically
		 * the same reason.
		 */
		setisync = needisync = FALSE;

		if (pmap != pmap_kernel()) {
			/*
			 * New mappings gain a reference on the level 3
			 * table.
			 */
			pmap_physpage_addref(pte);
		}
		goto validate_enterpv;
	}

	opa = pmap_pte_pa(pte);
	hadasm = (pmap_pte_asm(pte) != 0);

	if (opa == pa) {
		/*
		 * Mapping has not changed; must be a protection or
		 * wiring change.
		 */
		if (pmap_pte_w_chg(pte, wired ? PG_WIRED : 0)) {
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("pmap_enter: wiring change -> %d\n",
				    wired);
#endif
			/*
			 * Adjust the wiring count.
			 */
			if (wired)
				PMAP_STAT_INCR(pmap->pm_stats.wired_count, 1);
			else
				PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
		}

		/*
		 * Set the PTE.
		 */
		goto validate;
	}

	/*
	 * The mapping has changed.  We need to invalidate the
	 * old mapping before creating the new one.
	 */
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter: removing old mapping 0x%lx\n", va);
#endif
	if (pmap != pmap_kernel()) {
		/*
		 * Gain an extra reference on the level 3 table.
		 * pmap_remove_mapping() will delete a reference,
		 * and we don't want the table to be erroneously
		 * freed.
		 */
		pmap_physpage_addref(pte);
	}
	needisync |= pmap_remove_mapping(pmap, va, pte, TRUE, cpu_id);

 validate_enterpv:
	/*
	 * Enter the mapping into the pv_table if appropriate.
	 */
	if (pg != NULL) {
		error = pmap_pv_enter(pmap, pg, va, pte, TRUE);
		if (error) {
			pmap_l3pt_delref(pmap, va, pte, cpu_id);
			if (flags & PMAP_CANFAIL)
				goto out;
			panic("pmap_enter: unable to enter mapping in PV "
			    "table");
		}
	}

	/*
	 * Increment counters.
	 */
	PMAP_STAT_INCR(pmap->pm_stats.resident_count, 1);
	if (wired)
		PMAP_STAT_INCR(pmap->pm_stats.wired_count, 1);

 validate:
	/*
	 * Build the new PTE.
	 */
	npte = ((pa >> PGSHIFT) << PG_SHIFT) | pte_prot(pmap, prot) | PG_V;
	if (pg != NULL) {
		int attrs;

#ifdef DIAGNOSTIC
		if ((flags & PROT_MASK) & ~prot)
			panic("pmap_enter: access type exceeds prot");
#endif
		if (flags & PROT_WRITE)
			pg->mdpage.pvh_attrs |= (PGA_REFERENCED|PGA_MODIFIED);
		else if (flags & PROT_MASK)
			pg->mdpage.pvh_attrs |= PGA_REFERENCED;
		attrs = pg->mdpage.pvh_attrs;

		/*
		 * Set up referenced/modified emulation for new mapping.
		 */
		if ((attrs & PGA_REFERENCED) == 0)
			npte |= PG_FOR | PG_FOW | PG_FOE;
		else if ((attrs & PGA_MODIFIED) == 0)
			npte |= PG_FOW;

		/*
		 * Mapping was entered on PV list.
		 */
		npte |= PG_PVLIST;
	}
	if (wired)
		npte |= PG_WIRED;
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter: new pte = 0x%lx\n", npte);
#endif

	/*
	 * If the PALcode portion of the new PTE is the same as the
	 * old PTE, no TBI is necessary.
	 */
	if (PG_PALCODE(opte) == PG_PALCODE(npte))
		tflush = FALSE;

	/*
	 * Set the new PTE.
	 */
	PMAP_SET_PTE(pte, npte);

	/*
	 * Invalidate the TLB entry for this VA and any appropriate
	 * caches.
	 */
	if (tflush) {
		PMAP_INVALIDATE_TLB(pmap, va, hadasm, isactive, cpu_id);
		PMAP_TLB_SHOOTDOWN(pmap, va, hadasm ? PG_ASM : 0);
		PMAP_TLB_SHOOTNOW();
	}
	if (setisync)
		PMAP_SET_NEEDISYNC(pmap);
	if (needisync)
		PMAP_SYNC_ISTREAM(pmap);

out:
	PMAP_UNLOCK(pmap);

	return error;
}

/*
 * pmap_kenter_pa:		[ INTERFACE ]
 *
 *	Enter a va -> pa mapping into the kernel pmap without any
 *	physical->virtual tracking.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *pte, npte;
	cpuid_t cpu_id = cpu_number();
	boolean_t needisync = FALSE;
	pmap_t pmap = pmap_kernel();
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_kenter_pa(%lx, %lx, %x)\n",
		    va, pa, prot);
#endif

#ifdef DIAGNOSTIC
	/*
	 * Sanity check the virtual address.
	 */
	if (va < VM_MIN_KERNEL_ADDRESS)
		panic("pmap_kenter_pa: kernel pmap, invalid va 0x%lx", va);
#endif

	pte = PMAP_KERNEL_PTE(va);

	if (pmap_pte_v(pte) == 0)
		PMAP_STAT_INCR(pmap->pm_stats.resident_count, 1);
	if (pmap_pte_w(pte) == 0)
		PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);

	if ((prot & PROT_EXEC) != 0 || pmap_pte_exec(pte))
		needisync = TRUE;

	/*
	 * Build the new PTE.
	 */
	npte = ((pa >> PGSHIFT) << PG_SHIFT) | pte_prot(pmap_kernel(), prot) |
	    PG_V | PG_WIRED;

	/*
	 * Set the new PTE.
	 */
	PMAP_SET_PTE(pte, npte);
#if defined(MULTIPROCESSOR)
	alpha_mb();		/* XXX alpha_wmb()? */
#endif

	/*
	 * Invalidate the TLB entry for this VA and any appropriate
	 * caches.
	 */
	PMAP_INVALIDATE_TLB(pmap, va, TRUE, TRUE, cpu_id);
	PMAP_TLB_SHOOTDOWN(pmap, va, PG_ASM);
	PMAP_TLB_SHOOTNOW();

	if (needisync)
		PMAP_SYNC_ISTREAM_KERNEL();
}

/*
 * pmap_kremove:		[ INTERFACE ]
 *
 *	Remove a mapping entered with pmap_kenter_pa() starting at va,
 *	for size bytes (assumed to be page rounded).
 */
void
pmap_kremove(vaddr_t va, vsize_t size)
{
	pt_entry_t *pte;
	boolean_t needisync = FALSE;
	cpuid_t cpu_id = cpu_number();
	pmap_t pmap = pmap_kernel();
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_kremove(%lx, %lx)\n",
		    va, size);
#endif

#ifdef DIAGNOSTIC
	if (va < VM_MIN_KERNEL_ADDRESS)
		panic("pmap_kremove: user address");
#endif

	for (; size != 0; size -= PAGE_SIZE, va += PAGE_SIZE) {
		pte = PMAP_KERNEL_PTE(va);
		if (pmap_pte_v(pte)) {
#ifdef DIAGNOSTIC
			if (pmap_pte_pv(pte))
				panic("pmap_kremove: PG_PVLIST mapping for "
				    "0x%lx", va);
#endif
			if (pmap_pte_exec(pte))
				needisync = TRUE;

			/* Zap the mapping. */
			PMAP_SET_PTE(pte, PG_NV);
#if defined(MULTIPROCESSOR)
			alpha_mb();		/* XXX alpha_wmb()? */
#endif
			PMAP_INVALIDATE_TLB(pmap, va, TRUE, TRUE, cpu_id);
			PMAP_TLB_SHOOTDOWN(pmap, va, PG_ASM);

			/* Update stats. */
			PMAP_STAT_DECR(pmap->pm_stats.resident_count, 1);
			PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
		}
	}

	PMAP_TLB_SHOOTNOW();

	if (needisync)
		PMAP_SYNC_ISTREAM_KERNEL();
}

/*
 * pmap_unwire:			[ INTERFACE ]
 *
 *	Clear the wired attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *pte;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_unwire(%p, %lx)\n", pmap, va);
#endif

	PMAP_LOCK(pmap);

	pte = pmap_l3pte(pmap, va, NULL);
#ifdef DIAGNOSTIC
	if (pte == NULL || pmap_pte_v(pte) == 0)
		panic("pmap_unwire");
#endif

	/*
	 * If wiring actually changed (always?) clear the wire bit and
	 * update the wire count.  Note that wiring is not a hardware
	 * characteristic so there is no need to invalidate the TLB.
	 */
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, FALSE);
		PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
	}
#ifdef DIAGNOSTIC
	else {
		printf("pmap_unwire: wiring for pmap %p va 0x%lx "
		    "didn't change!\n", pmap, va);
	}
#endif

	PMAP_UNLOCK(pmap);
}

/*
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
 */
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *l1pte, *l2pte, *l3pte;
	boolean_t rv = FALSE;
	paddr_t pa;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%p, %lx) -> ", pmap, va);
#endif

	if (pmap == pmap_kernel()) {
		if (va < ALPHA_K0SEG_BASE) {
			/* nothing */
		} else if (va <= ALPHA_K0SEG_END) {
			pa = ALPHA_K0SEG_TO_PHYS(va);
			*pap = pa;
			rv = TRUE;
		} else {
			l3pte = PMAP_KERNEL_PTE(va);
			if (pmap_pte_v(l3pte)) {
				pa = pmap_pte_pa(l3pte) | (va & PGOFSET);
				*pap = pa;
				rv = TRUE;
			}
		}
		goto out_nolock;
	}

	PMAP_LOCK(pmap);

	l1pte = pmap_l1pte(pmap, va);
	if (pmap_pte_v(l1pte) == 0)
		goto out;

	l2pte = pmap_l2pte(pmap, va, l1pte);
	if (pmap_pte_v(l2pte) == 0)
		goto out;

	l3pte = pmap_l3pte(pmap, va, l2pte);
	if (pmap_pte_v(l3pte) == 0)
		goto out;

	pa = pmap_pte_pa(l3pte) | (va & PGOFSET);
	*pap = pa;
	rv = TRUE;
 out:
	PMAP_UNLOCK(pmap);
 out_nolock:
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		if (rv)
			printf("0x%lx\n", pa);
		else
			printf("failed\n");
	}
#endif
	return (rv);
}

/*
 * pmap_copy:			[ INTERFACE ]
 *
 *	Copy the mapping range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
/* call deleted in <machine/pmap.h> */

/*
 * pmap_collect:		[ INTERFACE ]
 *
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap_t pmap)
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%p)\n", pmap);
#endif

	/*
	 * If called for the kernel pmap, just return.  We
	 * handle this case in the event that we ever want
	 * to have swappable kernel threads.
	 */
	if (pmap == pmap_kernel())
		return;

	/*
	 * This process is about to be swapped out; free all of
	 * the PT pages by removing the physical mappings for its
	 * entire address space.  Note: pmap_do_remove() performs
	 * all necessary locking.
	 */
	pmap_do_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS, FALSE);
}

/*
 * pmap_activate:		[ INTERFACE ]
 *
 *	Activate the pmap used by the specified process.  This includes
 *	reloading the MMU context if the current process, and marking
 *	the pmap in use by the processor.
 *
 *	Note: We may use only spin locks here, since we are called
 *	by a critical section in cpu_switch()!
 */
void
pmap_activate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	cpuid_t cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_activate(%p)\n", p);
#endif

	/* Mark the pmap in use by this processor. */
	atomic_setbits_ulong(&pmap->pm_cpus, (1UL << cpu_id));

	/* Allocate an ASN. */
	pmap_asn_alloc(pmap, cpu_id);

	PMAP_ACTIVATE(pmap, p, cpu_id);
}

/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.  Note that we use only a single `atomic' operation,
 *	so no locking is necessary.
 */
void
pmap_deactivate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_deactivate(%p)\n", p);
#endif

	/*
	 * Mark the pmap no longer in use by this processor.
	 */
	atomic_clearbits_ulong(&pmap->pm_cpus, (1UL << cpu_number()));
}

/*
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and clear its contents, one machine dependent
 *	page at a time.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
	u_long *p0, *p1, *pend;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%lx)\n", phys);
#endif

	p0 = (u_long *)ALPHA_PHYS_TO_K0SEG(phys);
	p1 = NULL;
	pend = (u_long *)((u_long)p0 + PAGE_SIZE);

	/*
	 * Unroll the loop a bit, doing 16 quadwords per iteration.
	 * Do only 8 back-to-back stores, and alternate registers.
	 */
	do {
		__asm volatile(
		"# BEGIN loop body\n"
		"	addq	%2, (8 * 8), %1		\n"
		"	stq	$31, (0 * 8)(%0)	\n"
		"	stq	$31, (1 * 8)(%0)	\n"
		"	stq	$31, (2 * 8)(%0)	\n"
		"	stq	$31, (3 * 8)(%0)	\n"
		"	stq	$31, (4 * 8)(%0)	\n"
		"	stq	$31, (5 * 8)(%0)	\n"
		"	stq	$31, (6 * 8)(%0)	\n"
		"	stq	$31, (7 * 8)(%0)	\n"
		"					\n"
		"	addq	%3, (8 * 8), %0		\n"
		"	stq	$31, (0 * 8)(%1)	\n"
		"	stq	$31, (1 * 8)(%1)	\n"
		"	stq	$31, (2 * 8)(%1)	\n"
		"	stq	$31, (3 * 8)(%1)	\n"
		"	stq	$31, (4 * 8)(%1)	\n"
		"	stq	$31, (5 * 8)(%1)	\n"
		"	stq	$31, (6 * 8)(%1)	\n"
		"	stq	$31, (7 * 8)(%1)	\n"
		"	# END loop body"
		: "=r" (p0), "=r" (p1)
		: "0" (p0), "1" (p1)
		: "memory");
	} while (p0 < pend);
}

/*
 * pmap_copy_page:		[ INTERFACE ]
 *
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using memcpy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
	caddr_t s, d;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%lx, %lx)\n", src, dst);
#endif
        s = (caddr_t)ALPHA_PHYS_TO_K0SEG(src);
        d = (caddr_t)ALPHA_PHYS_TO_K0SEG(dst);
	memcpy(d, s, PAGE_SIZE);
}

/*
 * pmap_clear_modify:		[ INTERFACE ]
 *
 *	Clear the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	boolean_t rv = FALSE;
	cpuid_t cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%p)\n", pg);
#endif

	mtx_enter(&pg->mdpage.pvh_mtx);
	if (pg->mdpage.pvh_attrs & PGA_MODIFIED) {
		rv = TRUE;
		pmap_changebit(pg, PG_FOW, ~0, cpu_id);
		pg->mdpage.pvh_attrs &= ~PGA_MODIFIED;
	}
	mtx_leave(&pg->mdpage.pvh_mtx);

	return (rv);
}

/*
 * pmap_clear_reference:	[ INTERFACE ]
 *
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	boolean_t rv = FALSE;
	cpuid_t cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%p)\n", pg);
#endif

	mtx_enter(&pg->mdpage.pvh_mtx);
	if (pg->mdpage.pvh_attrs & PGA_REFERENCED) {
		rv = TRUE;
		pmap_changebit(pg, PG_FOR | PG_FOW | PG_FOE, ~0, cpu_id);
		pg->mdpage.pvh_attrs &= ~PGA_REFERENCED;
	}
	mtx_leave(&pg->mdpage.pvh_mtx);

	return (rv);
}

/*
 * pmap_is_referenced:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	boolean_t rv;

	rv = ((pg->mdpage.pvh_attrs & PGA_REFERENCED) != 0);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_is_referenced(%p) -> %c\n", pg, "FT"[rv]);
	}
#endif
	return (rv);
}

/*
 * pmap_is_modified:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
	boolean_t rv;

	rv = ((pg->mdpage.pvh_attrs & PGA_MODIFIED) != 0);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_is_modified(%p) -> %c\n", pg, "FT"[rv]);
	}
#endif
	return (rv);
}

/*
 * Miscellaneous support routines follow
 */

/*
 * alpha_protection_init:
 *
 *	Initialize Alpha protection code array.
 *
 *	Note: no locking is necessary in this function.
 */
void
alpha_protection_init(void)
{
	int prot, *kp, *up;

	kp = protection_codes[0];
	up = protection_codes[1];

	for (prot = 0; prot < 8; prot++) {
		kp[prot] = PG_ASM;
		up[prot] = 0;

		if (prot & PROT_READ) {
			kp[prot] |= PG_KRE;
			up[prot] |= PG_KRE | PG_URE;
		}
		if (prot & PROT_WRITE) {
			kp[prot] |= PG_KWE;
			up[prot] |= PG_KWE | PG_UWE;
		}
		if (prot & PROT_EXEC) {
			kp[prot] |= PG_EXEC | PG_KRE;
			up[prot] |= PG_EXEC | PG_KRE | PG_URE;
		} else {
			kp[prot] |= PG_FOE;
			up[prot] |= PG_FOE;
		}
	}
}

/*
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	Note: locking in this function is complicated by the fact
 *	that we can be called when the PV list is already locked.
 *	(pmap_page_protect()).  In this case, the caller must be
 *	careful to get the next PV entry while we remove this entry
 *	from beneath it.  We assume that the pmap itself is already
 *	locked; dolock applies only to the PV list.
 *
 *	Returns TRUE or FALSE, indicating if an I-stream sync needs
 *	to be initiated (for this CPU or for other CPUs).
 */
boolean_t
pmap_remove_mapping(pmap_t pmap, vaddr_t va, pt_entry_t *pte,
    boolean_t dolock, cpuid_t cpu_id)
{
	paddr_t pa;
	struct vm_page *pg;
	boolean_t onpv;
	boolean_t hadasm;
	boolean_t isactive;
	boolean_t needisync = FALSE;
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_mapping(%p, %lx, %p, %d, %ld)\n",
		       pmap, va, pte, dolock, cpu_id);
#endif

	/*
	 * PTE not provided, compute it from pmap and va.
	 */
	if (pte == PT_ENTRY_NULL) {
		pte = pmap_l3pte(pmap, va, NULL);
		if (pmap_pte_v(pte) == 0)
			return (FALSE);
	}

	pa = pmap_pte_pa(pte);
	onpv = (pmap_pte_pv(pte) != 0);
	if (onpv) {
		/*
		 * Remove it from the PV table such that nobody will
		 * attempt to modify the PTE behind our back.
		 */
		pg = PHYS_TO_VM_PAGE(pa);
		KASSERT(pg != NULL);
		pmap_pv_remove(pmap, pg, va, dolock);
	}

	hadasm = (pmap_pte_asm(pte) != 0);
	isactive = PMAP_ISACTIVE(pmap, cpu_id);

	/*
	 * Determine what we need to do about the I-stream.  If
	 * PG_EXEC was set, we mark a user pmap as needing an
	 * I-sync on the way out to userspace.  We always need
	 * an immediate I-sync for the kernel pmap.
	 */
	if (pmap_pte_exec(pte)) {
		if (pmap == pmap_kernel())
			needisync = TRUE;
		else {
			PMAP_SET_NEEDISYNC(pmap);
			needisync = (pmap->pm_cpus != 0);
		}
	}

	/*
	 * Update statistics
	 */
	if (pmap_pte_w(pte))
		PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
	PMAP_STAT_DECR(pmap->pm_stats.resident_count, 1);

	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("remove: invalidating pte at %p\n", pte);
#endif
	PMAP_SET_PTE(pte, PG_NV);

	PMAP_INVALIDATE_TLB(pmap, va, hadasm, isactive, cpu_id);
	PMAP_TLB_SHOOTDOWN(pmap, va, hadasm ? PG_ASM : 0);
	PMAP_TLB_SHOOTNOW();

	/*
	 * If we're removing a user mapping, check to see if we
	 * can free page table pages.
	 */
	if (pmap != pmap_kernel()) {
		/*
		 * Delete the reference on the level 3 table.  It will
		 * delete references on the level 2 and 1 tables as
		 * appropriate.
		 */
		pmap_l3pt_delref(pmap, va, pte, cpu_id);
	}

	return (needisync);
}

/*
 * pmap_changebit:
 *
 *	Set or clear the specified PTE bits for all mappings on the
 *	specified page.
 *
 *	Note: we assume that the pvlist is already locked.  There is no
 *	need to lock the pmap itself as amapping cannot be removed while
 *	we are holding the pvlist lock.
 */
void
pmap_changebit(struct vm_page *pg, u_long set, u_long mask, cpuid_t cpu_id)
{
	pv_entry_t pv;
	pt_entry_t *pte, npte;
	vaddr_t va;
	boolean_t hadasm, isactive;
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(0x%lx, 0x%lx, 0x%lx)\n",
		    VM_PAGE_TO_PHYS(pg), set, mask);
#endif

	MUTEX_ASSERT_LOCKED(&pg->mdpage.pvh_mtx);

	/*
	 * Loop over all current mappings setting/clearing as appropriate.
	 */
	for (pv = pg->mdpage.pvh_list; pv != NULL; pv = pv->pv_next) {
		va = pv->pv_va;

		pte = pv->pv_pte;
		npte = (*pte | set) & mask;
		if (*pte != npte) {
			hadasm = (pmap_pte_asm(pte) != 0);
			isactive = PMAP_ISACTIVE(pv->pv_pmap, cpu_id);
			PMAP_SET_PTE(pte, npte);
			PMAP_INVALIDATE_TLB(pv->pv_pmap, va, hadasm, isactive,
			    cpu_id);
			PMAP_TLB_SHOOTDOWN(pv->pv_pmap, va,
			    hadasm ? PG_ASM : 0);
		}
	}

	PMAP_TLB_SHOOTNOW();
}

/*
 * pmap_emulate_reference:
 *
 *	Emulate reference and/or modified bit hits.
 *	Return non-zero if this was an execute fault on a non-exec mapping,
 *	otherwise return 0.
 */
int
pmap_emulate_reference(struct proc *p, vaddr_t v, int user, int type)
{
	struct pmap *pmap;
	pt_entry_t faultoff, *pte;
	struct vm_page *pg;
	paddr_t pa;
	boolean_t didlock = FALSE;
	boolean_t exec = FALSE;
	cpuid_t cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_emulate_reference: %p, 0x%lx, %d, %d\n",
		    p, v, user, type);
#endif

	/*
	 * Convert process and virtual address to physical address.
	 */
	if (v >= VM_MIN_KERNEL_ADDRESS) {
		if (user)
			panic("pmap_emulate_reference: user ref to kernel");
		/*
		 * No need to lock here; kernel PT pages never go away.
		 */
		pte = PMAP_KERNEL_PTE(v);
	} else {
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic("pmap_emulate_reference: bad proc");
		if (p->p_vmspace == NULL)
			panic("pmap_emulate_reference: bad p_vmspace");
#endif
		pmap = p->p_vmspace->vm_map.pmap;
		PMAP_LOCK(pmap);
		didlock = TRUE;
		pte = pmap_l3pte(pmap, v, NULL);
		/*
		 * We'll unlock below where we're done with the PTE.
		 */
	}
	if (pte == NULL || !pmap_pte_v(pte)) {
		if (didlock)
			PMAP_UNLOCK(pmap);
		return (0);
	}
	exec = pmap_pte_exec(pte);
	if (!exec && type == ALPHA_MMCSR_FOE) {
		if (didlock)
			PMAP_UNLOCK(pmap);
		return (1);
	}
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("\tpte = %p, ", pte);
		printf("*pte = 0x%lx\n", *pte);
	}
#endif
#ifdef DEBUG				/* These checks are more expensive */
#ifndef MULTIPROCESSOR
	/*
	 * Quoting the Alpha ARM 14.3.1.4/5/6:
	 * ``The Translation Buffer may reload and cache the old PTE value
	 *   between the time the FOR (resp. FOW, FOE) fault invalidates the
	 *   old value from the Translation Buffer and the time software
	 *   updates the PTE in memory.  Software that depends on the
	 *   processor-provided invalidate must thus be prepared to take
	 *   another FOR (resp. FOW, FOE) fault on a page after clearing the
	 *   page's PTE<FOR(resp. FOW, FOE)> bit. The second fault will
	 *   invalidate the stale PTE from the Translation Buffer, and the
	 *   processor cannot load another stale copy. Thus, in the worst case,
	 *   a multiprocessor system will take an initial FOR (resp. FOW, FOE)
	 *   fault and then an additional FOR (resp. FOW, FOE) fault on each
	 *   processor. In practice, even a single repetition is unlikely.''
	 *
	 * In practice, spurious faults on the other processors happen, at
	 * least on fast 21264 or better processors.
	 */
	if (type == ALPHA_MMCSR_FOW) {
		if (!(*pte & (user ? PG_UWE : PG_UWE | PG_KWE))) {
			panic("pmap_emulate_reference(%d,%d): "
			    "write but unwritable pte 0x%lx",
			    user, type, *pte);
		}
		if (!(*pte & PG_FOW)) {
			panic("pmap_emulate_reference(%d,%d): "
			    "write but not FOW pte 0x%lx",
			    user, type, *pte);
		}
	} else {
		if (!(*pte & (user ? PG_URE : PG_URE | PG_KRE))) {
			panic("pmap_emulate_reference(%d,%d): "
			    "!write but unreadable pte 0x%lx",
			    user, type, *pte);
		}
		if (!(*pte & (PG_FOR | PG_FOE))) {
			panic("pmap_emulate_reference(%d,%d): "
			    "!write but not FOR|FOE pte 0x%lx",
			    user, type, *pte);
		}
	}
#endif /* MULTIPROCESSOR */
	/* Other diagnostics? */
#endif
	pa = pmap_pte_pa(pte);

	/*
	 * We're now done with the PTE.  If it was a user pmap, unlock
	 * it now.
	 */
	if (didlock)
		PMAP_UNLOCK(pmap);

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("\tpa = 0x%lx\n", pa);
#endif

	pg = PHYS_TO_VM_PAGE(pa);

#ifdef DIAGNOSTIC
	if (pg == NULL) {
		panic("pmap_emulate_reference(%p, 0x%lx, %d, %d): "
		    "pa 0x%lx (pte %p 0x%08lx) not managed",
		    p, v, user, type, pa, pte, *pte);
	}
#endif

	/*
	 * Twiddle the appropriate bits to reflect the reference
	 * and/or modification..
	 *
	 * The rules:
	 * 	(1) always mark page as used, and
	 *	(2) if it was a write fault, mark page as modified.
	 */

	mtx_enter(&pg->mdpage.pvh_mtx);
	if (type == ALPHA_MMCSR_FOW) {
		pg->mdpage.pvh_attrs |= (PGA_REFERENCED|PGA_MODIFIED);
		faultoff = PG_FOR | PG_FOW;
	} else {
		pg->mdpage.pvh_attrs |= PGA_REFERENCED;
		faultoff = PG_FOR;
		if (exec) {
			faultoff |= PG_FOE;
		}
	}
	pmap_changebit(pg, 0, ~faultoff, cpu_id);
	mtx_leave(&pg->mdpage.pvh_mtx);

	return (0);
}

#ifdef DEBUG
/*
 * pmap_pv_dump:
 *
 *	Dump the physical->virtual data for the specified page.
 */
void
pmap_pv_dump(paddr_t pa)
{
	struct vm_page *pg;
	pv_entry_t pv;

	pg = PHYS_TO_VM_PAGE(pa);

	printf("pa 0x%lx (attrs = 0x%x):\n", pa, pg->mdpage.pvh_attrs);
	mtx_enter(&pg->mdpage.pvh_mtx);
	for (pv = pg->mdpage.pvh_list; pv != NULL; pv = pv->pv_next)
		printf("     pmap %p, va 0x%lx\n",
		    pv->pv_pmap, pv->pv_va);
	mtx_leave(&pg->mdpage.pvh_mtx);
	printf("\n");
}
#endif
 
/*
 * vtophys:
 *
 *	Return the physical address corresponding to the K0SEG or
 *	K1SEG address provided.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
vtophys(vaddr_t vaddr)
{
	pt_entry_t *pte;
	paddr_t paddr = 0;

	if (vaddr < ALPHA_K0SEG_BASE)
		printf("vtophys: invalid vaddr 0x%lx", vaddr);
	else if (vaddr <= ALPHA_K0SEG_END)
		paddr = ALPHA_K0SEG_TO_PHYS(vaddr);
	else {
		pte = PMAP_KERNEL_PTE(vaddr);
		if (pmap_pte_v(pte))
			paddr = pmap_pte_pa(pte) | (vaddr & PGOFSET);
	}

#if 0
	printf("vtophys(0x%lx) -> 0x%lx\n", vaddr, paddr);
#endif

	return (paddr);
}

/******************** pv_entry management ********************/

/*
 * pmap_pv_enter:
 *
 *	Add a physical->virtual entry to the pv_table.
 */
int
pmap_pv_enter(pmap_t pmap, struct vm_page *pg, vaddr_t va, pt_entry_t *pte,
    boolean_t dolock)
{
	pv_entry_t newpv;

	/*
	 * Allocate and fill in the new pv_entry.
	 */
	newpv = pmap_pv_alloc();
	if (newpv == NULL)
		return (ENOMEM);
	newpv->pv_va = va;
	newpv->pv_pmap = pmap;
	newpv->pv_pte = pte;

	if (dolock)
		mtx_enter(&pg->mdpage.pvh_mtx);

#ifdef DEBUG
    {
	pv_entry_t pv;
	/*
	 * Make sure the entry doesn't already exist.
	 */
	for (pv = pg->mdpage.pvh_list; pv != NULL; pv = pv->pv_next) {
		if (pmap == pv->pv_pmap && va == pv->pv_va) {
			printf("pmap = %p, va = 0x%lx\n", pmap, va);
			panic("pmap_pv_enter: already in pv table");
		}
	}
    }
#endif

	/*
	 * ...and put it in the list.
	 */
	newpv->pv_next = pg->mdpage.pvh_list;
	pg->mdpage.pvh_list = newpv;

	if (dolock)
		mtx_leave(&pg->mdpage.pvh_mtx);

	return (0);
}

/*
 * pmap_pv_remove:
 *
 *	Remove a physical->virtual entry from the pv_table.
 */
void
pmap_pv_remove(pmap_t pmap, struct vm_page *pg, vaddr_t va, boolean_t dolock)
{
	pv_entry_t pv, *pvp;

	if (dolock)
		mtx_enter(&pg->mdpage.pvh_mtx);

	/*
	 * Find the entry to remove.
	 */
	for (pvp = &pg->mdpage.pvh_list, pv = *pvp;
	    pv != NULL; pvp = &pv->pv_next, pv = *pvp)
		if (pmap == pv->pv_pmap && va == pv->pv_va)
			break;

#ifdef DEBUG
	if (pv == NULL)
		panic("pmap_pv_remove: not in pv table");
#endif

	*pvp = pv->pv_next;

	if (dolock)
		mtx_leave(&pg->mdpage.pvh_mtx);

	pmap_pv_free(pv);
}

/*
 * pmap_pv_page_alloc:
 *
 *	Allocate a page for the pv_entry pool.
 */
void *
pmap_pv_page_alloc(struct pool *pp, int flags, int *slowdown)
{
	paddr_t pg;

	*slowdown = 0;
	if (pmap_physpage_alloc(PGU_PVENT, &pg))
		return ((void *)ALPHA_PHYS_TO_K0SEG(pg));
	return (NULL);
}

/*
 * pmap_pv_page_free:
 *
 *	Free a pv_entry pool page.
 */
void
pmap_pv_page_free(struct pool *pp, void *v)
{

	pmap_physpage_free(ALPHA_K0SEG_TO_PHYS((vaddr_t)v));
}

/******************** misc. functions ********************/

/*
 * pmap_physpage_alloc:
 *
 *	Allocate a single page from the VM system and return the
 *	physical address for that page.
 */
boolean_t
pmap_physpage_alloc(int usage, paddr_t *pap)
{
	struct vm_page *pg;
	paddr_t pa;

	/*
	 * Don't ask for a zeroed page in the L1PT case -- we will
	 * properly initialize it in the constructor.
	 */

	pg = uvm_pagealloc(NULL, 0, NULL, usage == PGU_L1PT ?
	    UVM_PGA_USERESERVE : UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (pg != NULL) {
		pa = VM_PAGE_TO_PHYS(pg);

#ifdef DIAGNOSTIC
		if (pg->wire_count != 0) {
			printf("pmap_physpage_alloc: page 0x%lx has "
			    "%d references\n", pa, pg->wire_count);
			panic("pmap_physpage_alloc");
		}
#endif
		*pap = pa;
		return (TRUE);
	}
	return (FALSE);
}

/*
 * pmap_physpage_free:
 *
 *	Free the single page table page at the specified physical address.
 */
void
pmap_physpage_free(paddr_t pa)
{
	struct vm_page *pg;

	if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL)
		panic("pmap_physpage_free: bogus physical page address");

#ifdef DIAGNOSTIC
	if (pg->wire_count != 0)
		panic("pmap_physpage_free: page still has references");
#endif

	uvm_pagefree(pg);
}

/*
 * pmap_physpage_addref:
 *
 *	Add a reference to the specified special use page.
 */
int
pmap_physpage_addref(void *kva)
{
	struct vm_page *pg;
	paddr_t pa;
	int rval;

	pa = ALPHA_K0SEG_TO_PHYS(trunc_page((vaddr_t)kva));
	pg = PHYS_TO_VM_PAGE(pa);

	rval = ++pg->wire_count;

	return (rval);
}

/*
 * pmap_physpage_delref:
 *
 *	Delete a reference to the specified special use page.
 */
int
pmap_physpage_delref(void *kva)
{
	struct vm_page *pg;
	paddr_t pa;
	int rval;

	pa = ALPHA_K0SEG_TO_PHYS(trunc_page((vaddr_t)kva));
	pg = PHYS_TO_VM_PAGE(pa);

#ifdef DIAGNOSTIC
	/*
	 * Make sure we never have a negative reference count.
	 */
	if (pg->wire_count == 0)
		panic("pmap_physpage_delref: reference count already zero");
#endif

	rval = --pg->wire_count;

	return (rval);
}

/******************** page table page management ********************/

/*
 * pmap_growkernel:		[ INTERFACE ]
 *
 *	Grow the kernel address space.  This is a hint from the
 *	upper layer to pre-allocate more kernel PT pages.
 */
vaddr_t
pmap_growkernel(vaddr_t maxkvaddr)
{
	struct pmap *kpm = pmap_kernel(), *pm;
	paddr_t ptaddr;
	pt_entry_t *l1pte, *l2pte, pte;
	vaddr_t va;
	int l1idx;

	mtx_enter(&pmap_growkernel_mtx);

	if (maxkvaddr <= pmap_maxkvaddr)
		goto out;		/* we are OK */

	va = pmap_maxkvaddr;

	while (va < maxkvaddr) {
		/*
		 * If there is no valid L1 PTE (i.e. no L2 PT page),
		 * allocate a new L2 PT page and insert it into the
		 * L1 map.
		 */
		l1pte = pmap_l1pte(kpm, va);
		if (pmap_pte_v(l1pte) == 0) {
			/*
			 * XXX PGU_NORMAL?  It's not a "traditional" PT page.
			 */
			if (uvm.page_init_done == FALSE) {
				/*
				 * We're growing the kernel pmap early (from
				 * uvm_pageboot_alloc()).  This case must
				 * be handled a little differently.
				 */
				ptaddr = ALPHA_K0SEG_TO_PHYS(
				    pmap_steal_memory(PAGE_SIZE, NULL, NULL));
			} else if (pmap_physpage_alloc(PGU_NORMAL,
				   &ptaddr) == FALSE)
				goto die;
			pte = (atop(ptaddr) << PG_SHIFT) |
			    PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
			*l1pte = pte;

			l1idx = l1pte_index(va);

			/* Update all the user pmaps. */
			mtx_enter(&pmap_all_pmaps_mtx);
			for (pm = TAILQ_FIRST(&pmap_all_pmaps);
			     pm != NULL; pm = TAILQ_NEXT(pm, pm_list)) {
				/* Skip the kernel pmap. */
				if (pm == pmap_kernel())
					continue;

				PMAP_LOCK(pm);
				KDASSERT(pm->pm_lev1map != kernel_lev1map);
				pm->pm_lev1map[l1idx] = pte;
				PMAP_UNLOCK(pm);
			}
			mtx_leave(&pmap_all_pmaps_mtx);
		}

		/*
		 * Have an L2 PT page now, add the L3 PT page.
		 */
		l2pte = pmap_l2pte(kpm, va, l1pte);
		KASSERT(pmap_pte_v(l2pte) == 0);
		if (uvm.page_init_done == FALSE) {
			/*
			 * See above.
			 */
			ptaddr = ALPHA_K0SEG_TO_PHYS(
			    pmap_steal_memory(PAGE_SIZE, NULL, NULL));
		} else if (pmap_physpage_alloc(PGU_NORMAL, &ptaddr) == FALSE)
			goto die;
		*l2pte = (atop(ptaddr) << PG_SHIFT) |
		    PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		va += ALPHA_L2SEG_SIZE;
	}

#if 0
	/* Invalidate the L1 PT cache. */
	pool_cache_invalidate(&pmap_l1pt_cache);
#endif

	pmap_maxkvaddr = va;

 out:
	mtx_leave(&pmap_growkernel_mtx);

	return (pmap_maxkvaddr);

 die:
	mtx_leave(&pmap_growkernel_mtx);
	panic("pmap_growkernel: out of memory");
}

/*
 * pmap_lev1map_create:
 *
 *	Create a new level 1 page table for the specified pmap.
 *
 *	Note: growkernel must already by held and the pmap either
 *	already locked or unreferenced globally.
 */
int
pmap_lev1map_create(pmap_t pmap, cpuid_t cpu_id)
{
	pt_entry_t *l1pt;

	KASSERT(pmap != pmap_kernel());
	KASSERT(pmap->pm_asni[cpu_id].pma_asn == PMAP_ASN_RESERVED);

	/* Don't sleep -- we're called with locks held. */
	l1pt = pool_get(&pmap_l1pt_pool, PR_NOWAIT);
	if (l1pt == NULL)
		return (ENOMEM);

	pmap_l1pt_ctor(l1pt);
	pmap->pm_lev1map = l1pt;

	return (0);
}

/*
 * pmap_lev1map_destroy:
 *
 *	Destroy the level 1 page table for the specified pmap.
 *
 *	Note: growkernel must already by held and the pmap either
 *	already locked or unreferenced globally.
 */
void
pmap_lev1map_destroy(pmap_t pmap)
{
	pt_entry_t *l1pt = pmap->pm_lev1map;

	KASSERT(pmap != pmap_kernel());

	/*
	 * Go back to referencing the global kernel_lev1map.
	 */
	pmap->pm_lev1map = kernel_lev1map;

	/*
	 * Free the old level 1 page table page.
	 */
	pool_put(&pmap_l1pt_pool, l1pt);
}

/*
 * pmap_l1pt_ctor:
 *
 *	Constructor for L1 PT pages.
 */
void
pmap_l1pt_ctor(pt_entry_t *l1pt)
{
	pt_entry_t pte;
	int i;

	/*
	 * Initialize the new level 1 table by zeroing the
	 * user portion and copying the kernel mappings into
	 * the kernel portion.
	 */
	for (i = 0; i < l1pte_index(VM_MIN_KERNEL_ADDRESS); i++)
		l1pt[i] = 0;

	for (i = l1pte_index(VM_MIN_KERNEL_ADDRESS);
	     i <= l1pte_index(VM_MAX_KERNEL_ADDRESS); i++)
		l1pt[i] = kernel_lev1map[i];

	/*
	 * Now, map the new virtual page table.  NOTE: NO ASM!
	 */
	pte = ((ALPHA_K0SEG_TO_PHYS((vaddr_t) l1pt) >> PGSHIFT) << PG_SHIFT) |
	    PG_V | PG_KRE | PG_KWE;
	l1pt[l1pte_index(VPTBASE)] = pte;
}

/*
 * pmap_l1pt_alloc:
 *
 *	Page allocator for L1 PT pages.
 *
 *	Note: The growkernel lock is held accross allocations
 *	from this pool, so we don't need to acquire it
 *	ourselves.
 */
void *
pmap_l1pt_alloc(struct pool *pp, int flags, int *slowdown)
{
	paddr_t ptpa;

	/*
	 * Attempt to allocate a free page.
	 */
	*slowdown = 0;
	if (pmap_physpage_alloc(PGU_L1PT, &ptpa) == FALSE)
		return (NULL);

	return ((void *) ALPHA_PHYS_TO_K0SEG(ptpa));
}

/*
 * pmap_l1pt_free:
 *
 *	Page freer for L1 PT pages.
 */
void
pmap_l1pt_free(struct pool *pp, void *v)
{

	pmap_physpage_free(ALPHA_K0SEG_TO_PHYS((vaddr_t) v));
}

/*
 * pmap_ptpage_alloc:
 *
 *	Allocate a level 2 or level 3 page table page, and
 *	initialize the PTE that references it.
 *
 *	Note: the pmap must already be locked.
 */
int
pmap_ptpage_alloc(pmap_t pmap, pt_entry_t *pte, int usage)
{
	paddr_t ptpa;

	/*
	 * Allocate the page table page.
	 */
	if (pmap_physpage_alloc(usage, &ptpa) == FALSE)
		return (ENOMEM);

	/*
	 * Initialize the referencing PTE.
	 */
	PMAP_SET_PTE(pte, ((ptpa >> PGSHIFT) << PG_SHIFT) |
	    PG_V | PG_KRE | PG_KWE | PG_WIRED |
	    (pmap == pmap_kernel() ? PG_ASM : 0));

	return (0);
}

/*
 * pmap_ptpage_free:
 *
 *	Free the level 2 or level 3 page table page referenced
 *	be the provided PTE.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_ptpage_free(pmap_t pmap, pt_entry_t *pte)
{
	paddr_t ptpa;

	/*
	 * Extract the physical address of the page from the PTE
	 * and clear the entry.
	 */
	ptpa = pmap_pte_pa(pte);
	PMAP_SET_PTE(pte, PG_NV);

#ifdef DEBUG
	pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
#endif
	pmap_physpage_free(ptpa);
}

/*
 * pmap_l3pt_delref:
 *
 *	Delete a reference on a level 3 PT page.  If the reference drops
 *	to zero, free it.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_l3pt_delref(pmap_t pmap, vaddr_t va, pt_entry_t *l3pte, cpuid_t cpu_id)
{
	pt_entry_t *l1pte, *l2pte;
	PMAP_TLB_SHOOTDOWN_CPUSET_DECL

	l1pte = pmap_l1pte(pmap, va);
	l2pte = pmap_l2pte(pmap, va, l1pte);

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_l3pt_delref: kernel pmap");
#endif

	if (pmap_physpage_delref(l3pte) == 0) {
		/*
		 * No more mappings; we can free the level 3 table.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_l3pt_delref: freeing level 3 table at "
			    "0x%lx\n", pmap_pte_pa(l2pte));
#endif
		pmap_ptpage_free(pmap, l2pte);

		/*
		 * We've freed a level 3 table, so we must
		 * invalidate the TLB entry for that PT page
		 * in the Virtual Page Table VA range, because
		 * otherwise the PALcode will service a TLB
		 * miss using the stale VPT TLB entry it entered
		 * behind our back to shortcut to the VA's PTE.
		 */
		PMAP_INVALIDATE_TLB(pmap,
		    (vaddr_t)(&VPT[VPT_INDEX(va)]), FALSE,
		    PMAP_ISACTIVE(pmap, cpu_id), cpu_id);
		PMAP_TLB_SHOOTDOWN(pmap,
		    (vaddr_t)(&VPT[VPT_INDEX(va)]), 0);
		PMAP_TLB_SHOOTNOW();

		/*
		 * We've freed a level 3 table, so delete the reference
		 * on the level 2 table.
		 */
		pmap_l2pt_delref(pmap, l1pte, l2pte);
	}
}

/*
 * pmap_l2pt_delref:
 *
 *	Delete a reference on a level 2 PT page.  If the reference drops
 *	to zero, free it.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_l2pt_delref(pmap_t pmap, pt_entry_t *l1pte, pt_entry_t *l2pte)
{
	KASSERT(pmap != pmap_kernel());
	if (pmap_physpage_delref(l2pte) == 0) {
		/*
		 * No more mappings in this segment; we can free the
		 * level 2 table.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_l2pt_delref: freeing level 2 table at "
			    "0x%lx\n", pmap_pte_pa(l1pte));
#endif
		pmap_ptpage_free(pmap, l1pte);

		/*
		 * We've freed a level 2 table, so delete the reference
		 * on the level 1 table.
		 */
		pmap_l1pt_delref(pmap, l1pte);
	}
}

/*
 * pmap_l1pt_delref:
 *
 *	Delete a reference on a level 1 PT page.  If the reference drops
 *	to zero, free it.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_l1pt_delref(pmap_t pmap, pt_entry_t *l1pte)
{
	KASSERT(pmap != pmap_kernel());
	pmap_physpage_delref(l1pte);
}

/******************** Address Space Number management ********************/

/*
 * pmap_asn_alloc:
 *
 *	Allocate and assign an ASN to the specified pmap.
 *
 *	Note: the pmap must already be locked.  This may be called from
 *	an interprocessor interrupt, and in that case, the sender of
 *	the IPI has the pmap lock.
 */
void
pmap_asn_alloc(pmap_t pmap, cpuid_t cpu_id)
{
	struct pmap_asn_info *pma = &pmap->pm_asni[cpu_id];
	struct pmap_asn_info *cpma = &pmap_asn_info[cpu_id];

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ASN))
		printf("pmap_asn_alloc(%p)\n", pmap);
#endif

	/*
	 * If the pmap is still using the global kernel_lev1map, there
	 * is no need to assign an ASN at this time, because only
	 * kernel mappings exist in that map, and all kernel mappings
	 * have PG_ASM set.  If the pmap eventually gets its own
	 * lev1map, an ASN will be allocated at that time.
	 *
	 * Only the kernel pmap will reference kernel_lev1map.  Do the
	 * same old fixups, but note that we no longer need the pmap
	 * to be locked if we're in this mode, since pm_lev1map will
	 * never change.
	 */
	if (pmap->pm_lev1map == kernel_lev1map) {
#ifdef DEBUG
		if (pmapdebug & PDB_ASN)
			printf("pmap_asn_alloc: still references "
			    "kernel_lev1map\n");
#endif
#if defined(MULTIPROCESSOR)
		/*
		 * In a multiprocessor system, it's possible to
		 * get here without having PMAP_ASN_RESERVED in
		 * pmap->pm_asni[cpu_id].pma_asn; see pmap_lev1map_destroy().
		 *
		 * So, what we do here, is simply assign the reserved
		 * ASN for kernel_lev1map users and let things
		 * continue on.  We do, however, let uniprocessor
		 * configurations continue to make its assertion.
		 */
		pma->pma_asn = PMAP_ASN_RESERVED;
#else
		KASSERT(pma->pma_asn == PMAP_ASN_RESERVED);
#endif /* MULTIPROCESSOR */
		return;
	}

	/*
	 * On processors which do not implement ASNs, the swpctx PALcode
	 * operation will automatically invalidate the TLB and I-cache,
	 * so we don't need to do that here.
	 */
	if (pmap_max_asn == 0) {
		/*
		 * Refresh the pmap's generation number, to
		 * simplify logic elsewhere.
		 */
		pma->pma_asngen = cpma->pma_asngen;
#ifdef DEBUG
		if (pmapdebug & PDB_ASN)
			printf("pmap_asn_alloc: no ASNs, using asngen %lu\n",
			    pma->pma_asngen);
#endif
		return;
	}

	/*
	 * Hopefully, we can continue using the one we have...
	 */
	if (pma->pma_asn != PMAP_ASN_RESERVED &&
	    pma->pma_asngen == cpma->pma_asngen) {
		/*
		 * ASN is still in the current generation; keep on using it.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_ASN) 
			printf("pmap_asn_alloc: same generation, keeping %u\n",
			    pma->pma_asn);
#endif
		return;
	}

	/*
	 * Need to assign a new ASN.  Grab the next one, incrementing
	 * the generation number if we have to.
	 */
	if (cpma->pma_asn > pmap_max_asn) {
		/*
		 * Invalidate all non-PG_ASM TLB entries and the
		 * I-cache, and bump the generation number.
		 */
		ALPHA_TBIAP();
		alpha_pal_imb();

		cpma->pma_asn = 1;
		cpma->pma_asngen++;
#ifdef DIAGNOSTIC
		if (cpma->pma_asngen == 0) {
			/*
			 * The generation number has wrapped.  We could
			 * handle this scenario by traversing all of
			 * the pmaps, and invalidating the generation
			 * number on those which are not currently
			 * in use by this processor.
			 *
			 * However... considering that we're using
			 * an unsigned 64-bit integer for generation
			 * numbers, on non-ASN CPUs, we won't wrap
			 * for approx. 585 million years, or 75 billion
			 * years on a 128-ASN CPU (assuming 1000 switch
			 * operations per second).
			 *
			 * So, we don't bother.
			 */
			panic("pmap_asn_alloc: too much uptime");
		}
#endif
#ifdef DEBUG
		if (pmapdebug & PDB_ASN)
			printf("pmap_asn_alloc: generation bumped to %lu\n",
			    cpma->pma_asngen);
#endif
	}

	/*
	 * Assign the new ASN and validate the generation number.
	 */
	pma->pma_asn = cpma->pma_asn++;
	pma->pma_asngen = cpma->pma_asngen;

#ifdef DEBUG
	if (pmapdebug & PDB_ASN)
		printf("pmap_asn_alloc: assigning %u to pmap %p\n",
		    pma->pma_asn, pmap);
#endif

	/*
	 * Have a new ASN, so there's no need to sync the I-stream
	 * on the way back out to userspace.
	 */
	atomic_clearbits_ulong(&pmap->pm_needisync, (1UL << cpu_id));
}

#if defined(MULTIPROCESSOR)
/******************** TLB shootdown code ********************/

/*
 * pmap_tlb_shootdown:
 *
 *	Cause the TLB entry for pmap/va to be shot down.
 *
 *	NOTE: The pmap must be locked here.
 */
void
pmap_tlb_shootdown(pmap_t pmap, vaddr_t va, pt_entry_t pte, u_long *cpumaskp)
{
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
	struct cpu_info *ci, *self = curcpu();
	u_long cpumask;
	CPU_INFO_ITERATOR cii;
#if 0
	int s;
#endif

	cpumask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self)
			continue;

		/*
		 * The pmap must be locked (unless its the kernel
		 * pmap, in which case it is okay for it to be
		 * unlocked), which prevents it from  becoming
		 * active on any additional processors.  This makes
		 * it safe to check for activeness.  If it's not
		 * active on the processor in question, then just
		 * mark it as needing a new ASN the next time it
		 * does, saving the IPI.  We always have to send
		 * the IPI for the kernel pmap.
		 *
		 * Note if it's marked active now, and it becomes
		 * inactive by the time the processor receives
		 * the IPI, that's okay, because it does the right
		 * thing with it later.
		 */
		 if (pmap != pmap_kernel() &&
		    PMAP_ISACTIVE(pmap, ci->ci_cpuid) == 0) {
			PMAP_INVALIDATE_ASN(pmap, ci->ci_cpuid);
			continue;
		}

		cpumask |= 1UL << ci->ci_cpuid;

		pq = &pmap_tlb_shootdown_q[ci->ci_cpuid];

		PSJQ_LOCK(pq, s);

		pq->pq_pte |= pte;

		/*
		 * If a global flush is already pending, we
		 * don't really have to do anything else.
		 */
		if (pq->pq_tbia) {
			PSJQ_UNLOCK(pq, s);
			continue;
		}

		pj = pmap_tlb_shootdown_job_get(pq);
		if (pj == NULL) {
			/*
			 * Couldn't allocate a job entry.  Just
			 * tell the processor to kill everything.
			 */
			pq->pq_tbia = 1;
		} else {
			pj->pj_pmap = pmap;
			pj->pj_va = va;
			pj->pj_pte = pte;
			TAILQ_INSERT_TAIL(&pq->pq_head, pj, pj_list);
		}

		PSJQ_UNLOCK(pq, s);
	}

	*cpumaskp |= cpumask;
}

/*
 * pmap_tlb_shootnow:
 *
 *	Process the TLB shootdowns that we have been accumulating
 *	for the specified processor set.
 */
void
pmap_tlb_shootnow(u_long cpumask)
{

	alpha_multicast_ipi(cpumask, ALPHA_IPI_SHOOTDOWN);
}

/*
 * pmap_do_tlb_shootdown:
 *
 *	Process pending TLB shootdown operations for this processor.
 */
void
pmap_do_tlb_shootdown(struct cpu_info *ci, struct trapframe *framep)
{
	u_long cpu_id = ci->ci_cpuid;
	u_long cpu_mask = (1UL << cpu_id);
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj;
#if 0
	int s;
#endif

	PSJQ_LOCK(pq, s);

	if (pq->pq_tbia) {
		if (pq->pq_pte & PG_ASM)
			ALPHA_TBIA();
		else
			ALPHA_TBIAP();
		pq->pq_tbia = 0;
		pmap_tlb_shootdown_q_drain(pq);
	} else {
		while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
			TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
			PMAP_INVALIDATE_TLB(pj->pj_pmap, pj->pj_va,
			    pj->pj_pte & PG_ASM,
			    pj->pj_pmap->pm_cpus & cpu_mask, cpu_id);
			pmap_tlb_shootdown_job_put(pq, pj);
		}
	}
	pq->pq_pte = 0;

	PSJQ_UNLOCK(pq, s);
}

/*
 * pmap_tlb_shootdown_q_drain:
 *
 *	Drain a processor's TLB shootdown queue.  We do not perform
 *	the shootdown operations.  This is merely a convenience
 *	function.
 *
 *	Note: We expect the queue to be locked.
 */
void
pmap_tlb_shootdown_q_drain(struct pmap_tlb_shootdown_q *pq)
{
	struct pmap_tlb_shootdown_job *pj;

	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		pmap_tlb_shootdown_job_put(pq, pj);
	}
}

/*
 * pmap_tlb_shootdown_job_get:
 *
 *	Get a TLB shootdown job queue entry.  This places a limit on
 *	the number of outstanding jobs a processor may have.
 *
 *	Note: We expect the queue to be locked.
 */
struct pmap_tlb_shootdown_job *
pmap_tlb_shootdown_job_get(struct pmap_tlb_shootdown_q *pq)
{
	struct pmap_tlb_shootdown_job *pj;

	pj = TAILQ_FIRST(&pq->pq_free);
	if (pj != NULL)
		TAILQ_REMOVE(&pq->pq_free, pj, pj_list);
	return (pj);
}

/*
 * pmap_tlb_shootdown_job_put:
 *
 *	Put a TLB shootdown job queue entry onto the free list.
 *
 *	Note: We expect the queue to be locked.
 */
void
pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *pq,
    struct pmap_tlb_shootdown_job *pj)
{
	TAILQ_INSERT_TAIL(&pq->pq_free, pj, pj_list);
}
#endif /* MULTIPROCESSOR */
@


1.83
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.82 2016/02/22 07:13:46 landry Exp $ */
d841 1
a841 1
	pool_init(&pmap_pmap_pool, PMAP_SIZEOF(pmap_ncpuids), 0, 0, 0,
d843 4
a846 7
	pool_setipl(&pmap_pmap_pool, IPL_NONE);
	pool_init(&pmap_l1pt_pool, PAGE_SIZE, 0, 0, 0, "l1ptpl",
	    &pmap_l1pt_allocator);
	pool_setipl(&pmap_l1pt_pool, IPL_VM);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
	    &pmap_pv_page_allocator);
	pool_setipl(&pmap_pv_pool, IPL_VM);
@


1.82
log
@Move #include <sys/mutex.h> from pmap.c to pmap.h, like every other archs
using struct mutex in pmap.h do. Fixes net-snmp on alpha.
ok deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.81 2015/12/04 06:21:52 jsg Exp $ */
d841 3
a843 2
	pool_init(&pmap_pmap_pool, PMAP_SIZEOF(pmap_ncpuids), 0, 0, 0, "pmappl",
	    &pool_allocator_single);
d846 1
d849 1
@


1.81
log
@add a missing mtx_leave before a panic to give ddb/reboot a better
chance of working.

ok deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.80 2015/10/01 16:03:48 kettenis Exp $ */
a142 1
#include <sys/mutex.h>
@


1.80
log
@Make the alpha pmap (more) mpsafe by protecting both the pmap itself and the
pv lists with a mutex.  This should make pmap_enter(9), pmap_remove(9) and
pmap_page_protect(9) safe to use without holding the kernel lock.  This
largely reverts rev. 1.75, but now of course the pmap locks are defined
to actually call mtx_enter(9) and mtx_leave(9).

ok visa@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.79 2015/09/08 21:28:35 kettenis Exp $ */
d3064 1
@


1.79
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.78 2015/07/20 00:16:16 kettenis Exp $ */
d144 1
d317 2
a318 2
 *	This pmap module uses two types of locks: `normal' (sleep)
 *	locks and `simple' (spin) locks.  They are used as follows:
d320 2
a321 10
 *	SIMPLE LOCKS
 *	------------
 *
 *	* pm_slock (per-pmap) - This lock protects all of the members
 *	  of the pmap structure itself.  This lock will be asserted
 *	  in pmap_activate() and pmap_deactivate() from a critical
 *	  section of cpu_switch(), and must never sleep.  Note that
 *	  in the case of the kernel pmap, interrupts which cause
 *	  memory allocation *must* be blocked while this lock is
 *	  asserted.
d334 1
a334 7
 *		    pmap->pm_slock
 *
 *	  But pmap_lev1map_create() is called with pmap->pm_slock held,
 *	  and also needs to acquire the pmap_growkernel_mtx.  So,
 *	  we require that the caller of pmap_lev1map_create() (currently,
 *	  the only caller is pmap_enter()) acquire pmap_growkernel_mtx
 *	  before acquiring pmap->pm_slock.
d347 3
a1075 2
 *
 *	Note: no locking is necessary in this function.
d1096 1
d1130 1
a1130 1
	refs = --pmap->pm_count;
d1162 1
a1162 1
	pmap->pm_count++;
a1181 1
	KERNEL_LOCK();
a1182 1
	KERNEL_UNLOCK();
d1216 2
d1236 2
d1249 2
d1256 1
a1256 1
		return;
d1336 3
d1351 1
a1351 1
	pv_entry_t pv, nextpv;
d1370 1
d1381 1
d1390 3
a1392 2
	for (pv = pg->mdpage.pvh_list; pv != NULL; pv = nextpv) {
		nextpv = pv->pv_next;
d1394 22
d1429 4
d1434 1
d1467 2
d1506 2
d1564 2
d1808 2
d1954 2
d1977 2
d2017 2
d2035 1
d2089 1
a2089 1
	 * entire address space.  Note: pmap_remove() performs
d2248 2
d2255 1
d2276 1
d2282 1
d2414 10
a2474 13
	/*
	 * If the mapping wasn't entered on the PV list, we're all done.
	 */
	if (onpv == FALSE)
		return (needisync);

	/*
	 * Remove it from the PV table.
	 */
	pg = PHYS_TO_VM_PAGE(pa);
	KASSERT(pg != NULL);
	pmap_pv_remove(pmap, pg, va, dolock);

d2484 3
a2486 3
 *	Note: we assume that the pv_head is already locked, and that
 *	the caller has acquired a PV->pmap mutex so that we can lock
 *	the pmaps as we encounter them.
d2503 2
d2541 1
d2569 2
d2576 5
d2583 2
a2593 2
	if (!pmap_pte_v(pte))
		panic("pmap_emulate_reference: invalid pte");
d2641 7
d2672 1
d2684 1
d2704 1
d2708 1
d2767 3
d2791 3
d2807 3
d2825 3
d3025 1
d3028 1
@


1.78
log
@Make pmap_remove() grab the kernel lock.  This is a big hammer but makes MP
machines work again with the unlocked reaper.

ok mpi@@, deraadt@@
no objection from miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.77 2015/06/23 19:50:48 miod Exp $ */
d853 1
a853 1
	    &pool_allocator_nointr);
@


1.77
log
@Disable some of the pmap_emulate_reference() DEBUG checks if option
MULTIPROCESSOR, and quote the alpha ARM to explain why; while there, make the
failure messages a bit more detailed.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.76 2015/05/02 20:50:08 miod Exp $ */
d1193 1
d1195 1
@


1.76
log
@fix build with option DEBUG
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.75 2015/02/02 09:29:53 mlarkin Exp $ */
d389 1
a389 1
	int pq_pte;			/* aggregate PTE bits */
d2539 19
d2559 10
a2568 4
		if (!(*pte & (user ? PG_UWE : PG_UWE | PG_KWE)))
			panic("pmap_emulate_reference: write but unwritable");
		if (!(*pte & PG_FOW))
			panic("pmap_emulate_reference: write but not FOW");
d2570 10
a2579 4
		if (!(*pte & (user ? PG_URE : PG_URE | PG_KRE)))
			panic("pmap_emulate_reference: !write but unreadable");
		if (!(*pte & (PG_FOR | PG_FOE)))
			panic("pmap_emulate_reference: !write but not FOR|FOE");
d2581 1
d2594 1
a2594 1
	if (pg == NULL)
d2596 3
a2598 1
		    "pa 0x%lx not managed", p, v, user, type, pa);
a3551 2

		pq->pq_pte = 0;
d3553 1
a3575 1
	pq->pq_pte = 0;
@


1.75
log
@
Remove some pmap locks that were #defined to be nothing (empty). Discussed
with many, ok kettenis@@.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.74 2014/12/17 15:23:40 deraadt Exp $ */
d271 2
a272 2
 * pmap_next_asn always indicates to the next ASN to use.  When
 * pmap_next_asn exceeds pmap_max_asn, we start a new ASN generation.
d276 1
a276 1
 * and pmap_next_asn is changed to indicate the first non-reserved ASN.
d1396 1
a1396 1
		    pmap_pte_pa(pv->pv_pte) != pa)
d2455 1
a2455 1
		    pa, set, mask);
d2668 1
a2668 1
	{
d2679 1
@


1.74
log
@remove simplelocks
ok tedu
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.73 2014/11/16 12:30:52 deraadt Exp $ */
a359 5
#define	PMAP_MAP_TO_HEAD_LOCK()		/* nothing */
#define	PMAP_MAP_TO_HEAD_UNLOCK()	/* nothing */
#define	PMAP_HEAD_TO_MAP_LOCK()		/* nothing */
#define	PMAP_HEAD_TO_MAP_UNLOCK()	/* nothing */

a1226 2
		PMAP_MAP_TO_HEAD_LOCK();

a1244 2
		PMAP_MAP_TO_HEAD_UNLOCK();

a1255 2
	PMAP_MAP_TO_HEAD_LOCK();

d1261 1
a1261 1
		goto out;
a1340 3

 out:
	PMAP_MAP_TO_HEAD_UNLOCK();
a1371 1
		PMAP_HEAD_TO_MAP_LOCK();
a1381 1
		PMAP_HEAD_TO_MAP_UNLOCK();
a1389 1
	PMAP_HEAD_TO_MAP_LOCK();
a1409 2

	PMAP_HEAD_TO_MAP_UNLOCK();
a1531 2
	PMAP_MAP_TO_HEAD_LOCK();

a1773 2
	PMAP_MAP_TO_HEAD_UNLOCK();

a2204 3

	PMAP_HEAD_TO_MAP_LOCK();

a2210 2
	PMAP_HEAD_TO_MAP_UNLOCK();

a2229 2
	PMAP_HEAD_TO_MAP_LOCK();

a2235 2
	PMAP_HEAD_TO_MAP_UNLOCK();

a2575 2
	PMAP_HEAD_TO_MAP_LOCK();

a2586 2

	PMAP_HEAD_TO_MAP_UNLOCK();
@


1.73
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.72 2014/03/29 18:09:28 guenther Exp $ */
a894 1
	simple_lock_init(&pmap_kernel()->pm_slock);
a1112 1
	simple_lock_init(&pmap->pm_slock);
a1145 1
	PMAP_LOCK(pmap);
a1146 2
	PMAP_UNLOCK(pmap);

a1177 1
	PMAP_LOCK(pmap);
a1178 1
	PMAP_UNLOCK(pmap);
a1232 1
		PMAP_LOCK(pmap);
a1251 1
		PMAP_UNLOCK(pmap);
a1265 1
	PMAP_LOCK(pmap);
a1353 1
	PMAP_UNLOCK(pmap);
a1387 1
			PMAP_LOCK(pv->pv_pmap);
a1395 1
			PMAP_UNLOCK(pv->pv_pmap);
a1410 1
		PMAP_LOCK(pmap);
a1422 1
		PMAP_UNLOCK(pmap);
a1457 2
	PMAP_LOCK(pmap);

a1494 2

	PMAP_UNLOCK(pmap);
a1551 1
	PMAP_LOCK(pmap);
a1794 1
	PMAP_UNLOCK(pmap);
a1940 2
	PMAP_LOCK(pmap);

a1961 2

	PMAP_UNLOCK(pmap);
a1999 2
	PMAP_LOCK(pmap);

a2015 1
	PMAP_UNLOCK(pmap);
a2495 2
		PMAP_LOCK(pv->pv_pmap);

a2506 1
		PMAP_UNLOCK(pv->pv_pmap);
a2525 1
	boolean_t didlock = FALSE;
a2552 2
		PMAP_LOCK(pmap);
		didlock = TRUE;
a2559 2
		if (didlock)
			PMAP_UNLOCK(pmap);
a2585 7
	/*
	 * We're now done with the PTE.  If it was a user pmap, unlock
	 * it now.
	 */
	if (didlock)
		PMAP_UNLOCK(pmap);

a2947 1
				PMAP_LOCK(pm);
a2949 1
				PMAP_UNLOCK(pm);
@


1.72
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.71 2014/02/01 21:21:54 miod Exp $ */
d1385 1
a1385 1
	    (prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE)))
d1390 2
a1391 2
	case VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE:
	case VM_PROT_READ|VM_PROT_WRITE:
d1395 2
a1396 2
	case VM_PROT_READ|VM_PROT_EXECUTE:
	case VM_PROT_READ:
d1468 1
a1468 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
d1510 1
a1510 1
	if (prot & VM_PROT_EXECUTE)
d1557 1
a1557 1
	 * VM_PROT_EXECUTE is set, we mark a user pmap as needing
d1561 1
a1561 1
	if (prot & VM_PROT_EXECUTE) {
d1759 1
a1759 1
		if ((flags & VM_PROT_ALL) & ~prot)
d1762 1
a1762 1
		if (flags & VM_PROT_WRITE)
d1764 1
a1764 1
		else if (flags & VM_PROT_ALL)
d1859 1
a1859 1
	if ((prot & VM_PROT_EXECUTE) != 0 || pmap_pte_exec(pte))
d2362 1
a2362 1
		if (prot & VM_PROT_READ) {
d2366 1
a2366 1
		if (prot & VM_PROT_WRITE) {
d2370 1
a2370 1
		if (prot & VM_PROT_EXECUTE) {
@


1.71
log
@pools aren't mp-safe; since the TLB shootdown code will allocate up to 6 job
entries per processor, don't bother using a pool and allocate the job entries
statically, and manage a per cpu free entries queue in addition to the in-use
queue. This avoids corrupting memory...
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.70 2014/01/26 17:40:09 miod Exp $ */
d2188 1
a2188 1
		__asm __volatile(
@


1.70
log
@Work in progress work towards SMP, heavily based upon NetBSD. The MP kernel
will boot multiuser, but will deadlock under load, and I can't find my
mistake yet.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.69 2014/01/06 20:27:44 miod Exp $ */
d388 3
d393 1
a394 1
	int pq_count;			/* number of pending requests */
d397 1
a402 5
/* If we have more pending jobs than this, we just nail the whole TLB. */
#define	PMAP_TLB_SHOOTDOWN_MAXJOBS	6

struct pool pmap_tlb_shootdown_job_pool;

d728 3
a901 3
	pool_init(&pmap_tlb_shootdown_job_pool,
	    sizeof(struct pmap_tlb_shootdown_job), 0, 0, 0, "pmaptlbpl", NULL);
	pool_setipl(&pmap_tlb_shootdown_job_pool, IPL_IPI);
d904 4
d3639 1
a3639 3
	if (pq->pq_count >= PMAP_TLB_SHOOTDOWN_MAXJOBS)
		return (NULL);
	pj = pool_get(&pmap_tlb_shootdown_job_pool, PR_NOWAIT);
d3641 1
a3641 1
		pq->pq_count++;
d3656 1
a3656 7

#ifdef DIAGNOSTIC
	if (pq->pq_count == 0)
		panic("pmap_tlb_shootdown_job_put: queue length inconsistency");
#endif
	pool_put(&pmap_tlb_shootdown_job_pool, pj);
	pq->pq_count--;
@


1.69
log
@Put PG_EXEC and PG_FOE into the PG_PROT mask, and make sure the default
pte protection masks, as initialized in alpha_protection_init(), set PG_FOE
by default when VM_PROT_EXECUTE is not set.

Also, change pmap_emulate_reference() to only clear PG_FOE if the affected
pte has executable permission.

This allows various pmap_pte_exec() checks (added to explicitely set PG_FOE)
to be removed.

All tests of regress/sys/kern/noexec now reliably pass on EV5. EV6 systems
still see spurious (but no longer 100% reproduceable) failures of the `catch
a signal' tests, which is likely caused by the effect of mprotect() removing
execute permission not taking effect correctly, despite PAL IMB being issued
(and no, this is not caused by the previous pmap_changebit() change), to be
investigated.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.68 2014/01/06 20:21:32 miod Exp $ */
d143 1
d214 3
a216 3
struct pmap	kernel_pmap_store;
u_int		kernel_pmap_asn_store[ALPHA_MAXPROCS];
u_long		kernel_pmap_asngen_store[ALPHA_MAXPROCS];
a249 2
struct pool pmap_asn_pool;
struct pool pmap_asngen_pool;
d309 3
a311 3
u_int	pmap_max_asn;			/* max ASN supported by the system */
u_int	pmap_next_asn[ALPHA_MAXPROCS];	/* next free ASN to use */
u_long	pmap_asn_generation[ALPHA_MAXPROCS]; /* current ASN generation */
d330 1
a330 1
 *	* pmap_all_pmaps_slock - This lock protects the global list of
d334 1
a334 1
 *	* pmap_growkernel_slock - This lock protects pmap_growkernel()
d337 12
d357 2
a358 2
struct simplelock pmap_all_pmaps_slock;
struct simplelock pmap_growkernel_slock;
d392 2
a393 1
	struct simplelock pq_slock;	/* spin lock on queue */
d396 2
a397 11
#define	PSJQ_LOCK(pq, s)						\
do {									\
	s = splvm();							\
	simple_lock(&(pq)->pq_slock);					\
} while (0)

#define	PSJQ_UNLOCK(pq, s)						\
do {									\
	simple_unlock(&(pq)->pq_slock);					\
	splx(s);							\
} while (0)
d404 1
d426 1
a426 1
void	pmap_lev1map_destroy(pmap_t, cpuid_t);
d430 2
a431 2
void	pmap_l2pt_delref(pmap_t, pt_entry_t *, pt_entry_t *, cpuid_t);
void	pmap_l1pt_delref(pmap_t, pt_entry_t *, cpuid_t);
d450 2
a451 1
struct pool_allocator pmap_pv_allocator = {
d454 1
d510 3
d520 1
a520 1
		if ((pmap)->pm_asn[(cpu_id)] != PMAP_ASN_RESERVED) {	\
d526 1
a526 2
		if ((pmap)->pm_asngen[(cpu_id)] != 			\
		    pmap_asn_generation[(cpu_id)]) {			\
d532 2
a533 2
			    (pmap)->pm_asngen[(cpu_id)], 		\
			    pmap_asn_generation[(cpu_id)],		\
d537 1
a537 1
		if ((pmap)->pm_asn[(cpu_id)] == PMAP_ASN_RESERVED) {	\
d568 2
a569 1
	(p)->p_addr->u_pcb.pcb_hw.apcb_asn = (pmap)->pm_asn[(cpu_id)];	\
d635 1
a635 1
	(pmap)->pm_asn[(cpu_id)] = PMAP_ASN_RESERVED;			\
d651 2
a652 2
	} else if ((pmap)->pm_asngen[(cpu_id)] == 			\
	    pmap_asn_generation[(cpu_id)]) {				\
d828 2
a829 2
	/* Initialize the pmap_growkernel_slock. */
	simple_lock_init(&pmap_growkernel_slock);
d855 1
a855 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
a858 4
	pool_init(&pmap_asn_pool, pmap_ncpuids * sizeof(u_int), 0, 0, 0,
	    "pmasnpl", &pool_allocator_nointr);
	pool_init(&pmap_asngen_pool, pmap_ncpuids * sizeof(u_long), 0, 0, 0,
	    "pmasngenpl", &pool_allocator_nointr);
d860 1
a860 1
	    &pmap_pv_allocator);
d869 2
a870 2
		pmap_next_asn[i] = 1;
		pmap_asn_generation[i] = 0;
d876 1
a876 1
	simple_lock_init(&pmap_all_pmaps_slock);
d885 1
a885 1
	memset(pmap_kernel(), 0, sizeof(struct pmap));
a887 2
	pmap_kernel()->pm_asn = kernel_pmap_asn_store;
	pmap_kernel()->pm_asngen = kernel_pmap_asngen_store;
d889 3
a891 2
		pmap_kernel()->pm_asn[i] = PMAP_ASN_RESERVED;
		pmap_kernel()->pm_asngen[i] = pmap_asn_generation[i];
d901 2
a902 2
	    sizeof(struct pmap_tlb_shootdown_job), 0, 0, 0, "pmaptlbpl",
	    NULL);
d905 1
a905 1
		simple_lock_init(&pmap_tlb_shootdown_q[i].pq_slock);
d916 1
a916 1
	    pmap_kernel()->pm_asn[cpu_number()];
a977 1

a1104 10
	pmap->pm_asn = pool_get(&pmap_asn_pool, PR_WAITOK);
	pmap->pm_asngen = pool_get(&pmap_asngen_pool, PR_WAITOK);

	/*
	 * Defer allocation of a new level 1 page table until
	 * the first new mapping is entered; just take a reference
	 * to the kernel kernel_lev1map.
	 */
	pmap->pm_lev1map = kernel_lev1map;

d1107 1
a1107 1
		pmap->pm_asn[i] = PMAP_ASN_RESERVED;
d1109 1
a1109 1
		pmap->pm_asngen[i] = pmap_asn_generation[i];
d1113 10
a1122 1
	simple_lock(&pmap_all_pmaps_slock);
d1124 1
a1124 1
	simple_unlock(&pmap_all_pmaps_slock);
a1143 2
	if (pmap == NULL)
		return;
d1155 1
a1155 1
	simple_lock(&pmap_all_pmaps_slock);
d1157 1
a1157 1
	simple_unlock(&pmap_all_pmaps_slock);
d1159 3
a1161 19
#ifdef DIAGNOSTIC
	/*
	 * Since the pmap is supposed to contain no valid
	 * mappings at this point, this should never happen.
	 */
	if (pmap->pm_lev1map != kernel_lev1map) {
		printf("pmap_destroy: pmap still contains valid mappings!\n");
		if (pmap->pm_nlev2)
			printf("pmap_destroy: %ld level 2 tables left\n",
			    pmap->pm_nlev2);
		if (pmap->pm_nlev3)
			printf("pmap_destroy: %ld level 3 tables left\n",
			    pmap->pm_nlev3);
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
		pmap_update(pmap);
		if (pmap->pm_lev1map != kernel_lev1map)
			panic("pmap_destroy: pmap_remove() didn't");
	}
#endif
a1162 2
	pool_put(&pmap_asn_pool, pmap->pm_asn);
	pool_put(&pmap_asngen_pool, pmap->pm_asngen);
d1179 4
a1182 5
	if (pmap != NULL) {
		PMAP_LOCK(pmap);
		pmap->pm_count++;
		PMAP_UNLOCK(pmap);
	}
a1226 3
	if (pmap == NULL)
		return;

d1347 1
a1347 1
			pmap_l2pt_delref(pmap, l1pte, saved_l2pte, cpu_id);
d1355 1
a1355 1
	pmap_l1pt_delref(pmap, saved_l1pte, cpu_id);
d1378 1
d1395 12
a1406 1
/* XXX */	pmap_changebit(pg, 0, ~(PG_KWE | PG_UWE), cpu_id);
d1408 1
d1427 6
a1432 17
		if (pmap_pte_w(pv->pv_pte) == 0) {
			if (pmap_remove_mapping(pmap, pv->pv_va, pv->pv_pte,
			    FALSE, cpu_id) == TRUE) {
				if (pmap == pmap_kernel())
					needkisync |= TRUE;
				else
					PMAP_SYNC_ISTREAM_USER(pmap);
			}
		}
#ifdef DEBUG
		else {
			if (pmapdebug & PDB_PARANOIA) {
				printf("%s wired mapping for %lx not removed\n",
				       "pmap_page_protect:", pa);
				printf("vm wire count %d\n", 
					PHYS_TO_VM_PAGE(pa)->wire_count);
			}
a1433 1
#endif
d1457 1
a1464 3
	if (pmap == NULL)
		return;

a1469 3
	if (prot & VM_PROT_WRITE)
		return;

d1478 22
a1499 25
		if (pmap_pte_v(l1pte)) {
			l2pte = pmap_l2pte(pmap, sva, l1pte);
			for (; sva < l1eva && sva < eva; sva = l2eva, l2pte++) {
				l2eva =
				    alpha_trunc_l2seg(sva) + ALPHA_L2SEG_SIZE;
				if (pmap_pte_v(l2pte)) {
					l3pte = pmap_l3pte(pmap, sva, l2pte);
					for (; sva < l2eva && sva < eva;
					     sva += PAGE_SIZE, l3pte++) {
						if (pmap_pte_v(l3pte) &&
						    pmap_pte_prot_chg(l3pte,
						    bits)) {
							hadasm =
							   (pmap_pte_asm(l3pte)
							    != 0);
							pmap_pte_set_prot(l3pte,
							   bits);
							PMAP_INVALIDATE_TLB(
							   pmap, sva, hadasm,
							   isactive, cpu_id);
							PMAP_TLB_SHOOTDOWN(
							   pmap, sva,
							   hadasm ? PG_ASM : 0);
						}
					}
d1505 2
d1541 1
d1590 1
a1590 14
		/*
		 * If we're still referencing the kernel kernel_lev1map,
		 * create a new level 1 page table.  A reference will be
		 * added to the level 1 table when the level 2 table is
		 * created.
		 */
		if (pmap->pm_lev1map == kernel_lev1map) {
			error = pmap_lev1map_create(pmap, cpu_id);
			if (error) {
				if (flags & PMAP_CANFAIL)
					goto out;
				panic("pmap_enter: unable to create lev1map");
			}
		}
d1603 1
a1603 1
				pmap_l1pt_delref(pmap, l1pte, cpu_id);
a1608 1
			pmap->pm_nlev2++;
d1627 1
a1627 1
				pmap_l2pt_delref(pmap, l1pte, l2pte, cpu_id);
a1632 1
			pmap->pm_nlev3++;
d1804 1
d1833 1
d1879 1
d1888 2
a1889 2
 *	Remove a mapping entered with pmap_kenter_pa()
 *	starting at va, for size bytes (assumed to be page rounded).
d1898 1
d1936 2
a1957 2
	if (pmap == NULL)
		return;
d2062 1
a2062 10
void
pmap_copy(pmap_t dst_pmap, pmap_t src_pmap, vaddr_t dst_addr, vsize_t len,
    vaddr_t src_addr)
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
#endif
}
d2121 1
a2121 3
	/*
	 * Mark the pmap in use by this processor.
	 */
d2124 1
a2124 5
	PMAP_LOCK(pmap);

	/*
	 * Allocate an ASN.
	 */
a2127 2

	PMAP_UNLOCK(pmap);
d2177 1
d2404 1
d2459 1
d2507 1
d2536 2
a2603 5
#if 0
	/*
	 * Can't do these, because cpu_fork call pmap_emulate_reference(),
	 * and the bits aren't guaranteed, for them...
	 */
a2614 1
#endif
d2635 2
a2636 1
		panic("pmap_emulate_reference(%p, 0x%lx, %d, %d): pa 0x%lx not managed", p, v, user, type, pa);
a2677 3
	static const char *usage[] = {
		"normal", "pvent", "l1pt", "l2pt", "l3pt",
	};
d2943 3
a2945 1
	int s, l1idx;
a2949 3
	s = splhigh();			/* to be safe */
	simple_lock(&pmap_growkernel_slock);

d2981 1
a2981 1
			simple_lock(&pmap_all_pmaps_slock);
d2989 1
a2989 4
				if (pm->pm_lev1map == kernel_lev1map) {
					PMAP_UNLOCK(pm);
					continue;
				}
d2993 1
a2993 1
			simple_unlock(&pmap_all_pmaps_slock);
d3021 2
a3022 2
	simple_unlock(&pmap_growkernel_slock);
	splx(s);
a3023 1
 out:
d3035 2
a3036 1
 *	Note: the pmap must already be locked.
d3043 2
a3044 9
#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_lev1map_create: got kernel pmap");

	if (pmap->pm_asn[cpu_id] != PMAP_ASN_RESERVED)
		panic("pmap_lev1map_create: pmap uses non-reserved ASN");
#endif

	simple_lock(&pmap_growkernel_slock);
d3046 1
d3048 1
a3048 2
	if (l1pt == NULL) {
		simple_unlock(&pmap_growkernel_slock);
a3049 1
	}
a3053 10
	simple_unlock(&pmap_growkernel_slock);

	/*
	 * The page table base has changed; if the pmap was active,
	 * reactivate it.
	 */
	if (PMAP_ISACTIVE(pmap, cpu_id)) {
		pmap_asn_alloc(pmap, cpu_id);
		PMAP_ACTIVATE(pmap, curproc, cpu_id);
	}
d3062 2
a3063 1
 *	Note: the pmap must already be locked.
d3066 1
a3066 1
pmap_lev1map_destroy(pmap_t pmap, cpuid_t cpu_id)
d3070 1
a3070 4
#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_lev1map_destroy: got kernel pmap");
#endif
a3077 22
	 * The page table base has changed; if the pmap was active,
	 * reactivate it.  Note that allocation of a new ASN is
	 * not necessary here:
	 *
	 *	(1) We've gotten here because we've deleted all
	 *	    user mappings in the pmap, invalidating the
	 *	    TLB entries for them as we go.
	 *
	 *	(2) kernel_lev1map contains only kernel mappings, which
	 *	    were identical in the user pmap, and all of
	 *	    those mappings have PG_ASM, so the ASN doesn't
	 *	    matter.
	 *
	 * We do, however, ensure that the pmap is using the
	 * reserved ASN, to ensure that no two pmaps never have
	 * clashing TLB entries.
	 */
	PMAP_INVALIDATE_ASN(pmap, cpu_id);
	if (PMAP_ISACTIVE(pmap, cpu_id))
		PMAP_ACTIVATE(pmap, curproc, cpu_id);

	/*
d3118 4
d3217 1
a3236 1
		pmap->pm_nlev3--;
d3251 1
d3257 1
a3257 1
		pmap_l2pt_delref(pmap, l1pte, l2pte, cpu_id);
d3270 1
a3270 2
pmap_l2pt_delref(pmap_t pmap, pt_entry_t *l1pte, pt_entry_t *l2pte,
    cpuid_t cpu_id)
d3272 1
a3272 6

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_l2pt_delref: kernel pmap");
#endif

a3283 1
		pmap->pm_nlev2--;
d3289 1
a3289 1
		pmap_l1pt_delref(pmap, l1pte, cpu_id);
d3302 1
a3302 1
pmap_l1pt_delref(pmap_t pmap, pt_entry_t *l1pte, cpuid_t cpu_id)
d3304 2
a3305 13

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_l1pt_delref: kernel pmap");
#endif

	if (pmap_physpage_delref(l1pte) == 0) {
		/*
		 * No more level 2 tables left, go back to the global
		 * kernel_lev1map.
		 */
		pmap_lev1map_destroy(pmap, cpu_id);
	}
d3315 3
a3317 1
 *	Note: the pmap must already be locked.
d3322 2
d3336 5
d3348 15
a3362 5
#ifdef DIAGNOSTIC
		if (pmap->pm_asn[cpu_id] != PMAP_ASN_RESERVED)
			panic("pmap_asn_alloc: kernel_lev1map without "
			    "PMAP_ASN_RESERVED");
#endif
d3376 1
a3376 1
		pmap->pm_asngen[cpu_id] = pmap_asn_generation[cpu_id];
d3380 1
a3380 1
			    pmap->pm_asngen[cpu_id]);
d3388 2
a3389 2
	if (pmap->pm_asn[cpu_id] != PMAP_ASN_RESERVED &&
	    pmap->pm_asngen[cpu_id] == pmap_asn_generation[cpu_id]) {
d3396 1
a3396 1
			    pmap->pm_asn[cpu_id]);
d3405 1
a3405 1
	if (pmap_next_asn[cpu_id] > pmap_max_asn) {
d3413 2
a3414 3
		pmap_next_asn[cpu_id] = 1;

		pmap_asn_generation[cpu_id]++;
d3416 1
a3416 1
		if (pmap_asn_generation[cpu_id] == 0) {
d3439 1
a3439 1
			    pmap_asn_generation[cpu_id]);
d3446 2
a3447 2
	pmap->pm_asn[cpu_id] = pmap_next_asn[cpu_id]++;
	pmap->pm_asngen[cpu_id] = pmap_asn_generation[cpu_id];
d3452 1
a3452 1
		    pmap->pm_asn[cpu_id], pmap);
d3469 2
d3473 1
a3473 1
pmap_tlb_shootdown(pmap_t pmap, vaddr_t va, pt_entry_t pte)
a3474 2
	u_long ipinum;
	cpuid_t i, cpu_id = cpu_number();
d3477 4
d3482 1
d3484 4
a3487 2
	for (i = 0; i < hwrpb->rpb_pcs_cnt; i++) {
		if (i == cpu_id || (cpus_running & (1UL << i)) == 0)
d3490 25
a3514 1
		pq = &pmap_tlb_shootdown_q[i];
d3518 11
a3529 1
		pq->pq_pte |= pte;
d3532 2
a3533 2
			 * Couldn't allocate a job entry.  Just do a
			 * TBIA[P].
d3535 1
a3535 5
			if (pq->pq_pte & PG_ASM)
				ipinum = ALPHA_IPI_SHOOTDOWN;
			else
				ipinum = ALPHA_IPI_IMB;
			alpha_send_ipi(i, ipinum);
a3540 1
			ipinum = ALPHA_IPI_SHOOTDOWN;
a3542 2
		alpha_send_ipi(i, ipinum);

d3545 15
d3570 1
a3570 1
	cpuid_t cpu_id = ci->ci_cpuid;
d3574 1
d3576 1
d3580 17
a3596 6
	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		PMAP_INVALIDATE_TLB(pj->pj_pmap, pj->pj_va,
		    pj->pj_pte & PG_ASM, pj->pj_pmap->pm_cpus & cpu_mask,
		    cpu_id);
		pmap_tlb_shootdown_job_put(pq, pj);
a3597 1
	pq->pq_pte = 0;
d3608 2
d3612 1
a3612 1
pmap_tlb_shootdown_q_drain(cpuid_t cpu_id, boolean_t all)
d3614 1
a3614 4
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj, *npj;
	pt_entry_t npte = 0;
	int s;
d3616 3
a3618 9
	PSJQ_LOCK(pq, s);

	for (pj = TAILQ_FIRST(&pq->pq_head); pj != NULL; pj = npj) {
		npj = TAILQ_NEXT(pj, pj_list);
		if (all || (pj->pj_pte & PG_ASM) == 0) {
			TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
			pmap_tlb_shootdown_job_put(pq, pj);
		} else
			npte |= pj->pj_pte;
d3620 1
a3620 3
	pq->pq_pte = npte;

	PSJQ_UNLOCK(pq, s);
@


1.68
log
@Remove I-sync stuff from pmap_changebit().  The AARM says that we
only have to sync the I-stream when the mapping is removed or changed,
and since the I-stream is fetch-only, changing protection bits does
not constitute changing the mapping (the VA->PA translation is still
the same).

From NetBSD
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.67 2014/01/05 14:37:08 miod Exp $ */
d1411 1
d1419 1
a1498 2
	if (!pmap_pte_exec(&bits))
		bits |= PG_FOE;
a1813 4
		/* Always force FOE on non-exec mappings. */
		if (!pmap_pte_exec(pte))
			npte |= PG_FOE;

d2409 17
a2425 33
		kp[prot] = 0; up[prot] = 0;
		switch (prot) {
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE:
			kp[prot] |= PG_ASM;
			up[prot] |= 0;
			break;

		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_EXECUTE:
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE:
			kp[prot] |= PG_EXEC;		/* software */
			up[prot] |= PG_EXEC;		/* software */
			/* FALLTHROUGH */

		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_NONE:
			kp[prot] |= PG_ASM | PG_KRE;
			up[prot] |= PG_URE | PG_KRE;
			break;

		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] |= PG_ASM | PG_KWE;
			up[prot] |= PG_UWE | PG_KWE;
			break;

		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			kp[prot] |= PG_EXEC;		/* software */
			up[prot] |= PG_EXEC;		/* software */
			/* FALLTHROUGH */

		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] |= PG_ASM | PG_KWE | PG_KRE;
			up[prot] |= PG_UWE | PG_URE | PG_KWE | PG_KRE;
			break;
a2549 3
 *
 *	XXX This routine could stand to have some I-stream
 *	XXX optimization done.
d2592 2
a2593 3
 *
 *	return non-zero if this was a FOE fault and the pte is not
 *	executable.
d2598 1
d2603 1
d2629 2
a2630 1
		PMAP_LOCK(p->p_vmspace->vm_map.pmap);
d2632 1
a2632 1
		pte = pmap_l3pte(p->p_vmspace->vm_map.pmap, v, NULL);
d2637 2
a2638 1
	if (!pmap_pte_exec(pte) && type == ALPHA_MMCSR_FOE) {
d2640 1
a2640 1
			PMAP_UNLOCK(p->p_vmspace->vm_map.pmap);
d2678 1
a2678 1
		PMAP_UNLOCK(p->p_vmspace->vm_map.pmap);
d2705 1
a2705 1
		faultoff = PG_FOR | PG_FOW | PG_FOE;
d2708 4
a2711 1
		faultoff = PG_FOR | PG_FOE;
a2712 5
	/*
	 * If the page is not PG_EXEC, pmap_changebit will automagically
	 * set PG_FOE (gross, but necessary if I don't want to change the
	 * whole API).
	 */
@


1.67
log
@Cleanup some leftovers from previous changes.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.66 2014/01/01 22:13:52 miod Exp $ */
a2580 1
	boolean_t needisync, needkisync = FALSE;
a2600 19
			/*
			 * Determine what we need to do about the I-stream.
			 * If PG_EXEC was set, we mark a user pmap as needing
			 * an I-sync on the way out to userspace.  We always
			 * need an immediate I-sync for the kernel pmap.
			 */
			needisync = FALSE;
			if (pmap_pte_exec(pte)) {
				if (pv->pv_pmap == pmap_kernel())
					needkisync = TRUE;
				else {
					PMAP_SET_NEEDISYNC(pv->pv_pmap);
					if (pv->pv_pmap->pm_cpus != 0)
						needisync = TRUE;
				}
			} else {
				/* Never clear FOE on non-exec mappings. */
				npte |= PG_FOE;
			}
a2601 2
			if (needisync)
				PMAP_SYNC_ISTREAM_USER(pv->pv_pmap);
a2608 3

	if (needkisync)
		PMAP_SYNC_ISTREAM_KERNEL();
@


1.66
log
@Switch alpha to __HAVE_VM_PAGE_MD. From NetBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.65 2013/10/31 03:48:31 deraadt Exp $ */
a253 7
 * Canonical names for PGU_* constants.
 */
#ifdef DIAGNOSTIC
const char *pmap_pgu_strings[] = PGU_STRINGS;
#endif

/*
d424 2
a425 3
void	pmap_ptpage_free(pmap_t, pt_entry_t *, pt_entry_t **);
void	pmap_l3pt_delref(pmap_t, vaddr_t, pt_entry_t *, cpuid_t,
	    pt_entry_t **);
a443 2
struct	pv_entry *pmap_pv_alloc(void);
void	pmap_pv_free(struct pv_entry *);
d453 3
d1361 1
a1361 1
					    saved_l3pte, cpu_id, NULL);
d1773 1
a1773 1
			pmap_l3pt_delref(pmap, va, pte, cpu_id, NULL);
a2175 8
	/*
	 * Move the pmap to the end of the LRU list.
	 */
	simple_lock(&pmap_all_pmaps_slock);
	TAILQ_REMOVE(&pmap_all_pmaps, pmap, pm_list);
	TAILQ_INSERT_TAIL(&pmap_all_pmaps, pmap, pm_list);
	simple_unlock(&pmap_all_pmaps_slock);

a2476 2
	struct pv_entry **pvp;
	pt_entry_t **ptp;
d2480 2
a2481 2
		printf("pmap_remove_mapping(%p, %lx, %p, %d, %ld, %p)\n",
		       pmap, va, pte, dolock, cpu_id, pvp);
a2483 3
	pvp = NULL;
	ptp = NULL;

d2542 1
a2542 1
		pmap_l3pt_delref(pmap, va, pte, cpu_id, ptp);
d2548 1
a2548 5
	if (onpv == FALSE) {
#ifdef DIAGNOSTIC
		if (pvp != NULL)
			panic("pmap_removing_mapping: onpv / pvp inconsistent");
#endif
a2549 1
	}
a2897 22
 * pmap_pv_alloc:
 *
 *	Allocate a pv_entry.
 */
struct pv_entry *
pmap_pv_alloc(void)
{
	return pool_get(&pmap_pv_pool, PR_NOWAIT);
}

/*
 * pmap_pv_free:
 *
 *	Free a pv_entry.
 */
void
pmap_pv_free(struct pv_entry *pv)
{
	pool_put(&pmap_pv_pool, pv);
}

/*
d3333 1
a3333 1
pmap_ptpage_free(pmap_t pmap, pt_entry_t *pte, pt_entry_t **ptp)
a3343 8
	/*
	 * Check to see if we're stealing the PT page.  If we are,
	 * zero it, and return the KSEG address of the page.
	 */
	if (ptp != NULL) {
		pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
		*ptp = (pt_entry_t *)ALPHA_PHYS_TO_K0SEG(ptpa);
	} else {
d3345 1
a3345 1
		pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
d3347 1
a3347 2
		pmap_physpage_free(ptpa);
	}
d3359 1
a3359 2
pmap_l3pt_delref(pmap_t pmap, vaddr_t va, pt_entry_t *l3pte, cpuid_t cpu_id,
    pt_entry_t **ptp)
d3380 1
a3380 1
		pmap_ptpage_free(pmap, l2pte, ptp);
d3433 1
a3433 1
		pmap_ptpage_free(pmap, l1pte, NULL);
@


1.65
log
@oops
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.63 2012/04/10 15:50:52 guenther Exp $ */
a231 6
/*
 * Storage for physical->virtual entries and page attributes.
 */
struct pv_head	*pv_table;
int		pv_table_npages;

a337 3
 *	* pvh_slock (per-pv_head) - This lock protects the PV list
 *	  for a specified managed page.
 *
a415 9
static __inline struct pv_head *
pa_to_pvh(paddr_t pa)
{
	int bank, pg;

	bank = vm_physseg_find(atop(pa), &pg);
	return (&vm_physmem[bank].pmseg.pvhead[pg]);
}

d423 1
a423 1
void	pmap_changebit(paddr_t, pt_entry_t, pt_entry_t, cpuid_t);
d449 3
a451 3
int	pmap_pv_enter(pmap_t, paddr_t, vaddr_t, pt_entry_t *, boolean_t);
void	pmap_pv_remove(pmap_t, paddr_t, vaddr_t, boolean_t,
	    struct pv_entry **);
a773 22
	 * Allocate memory for the pv_heads.  (A few more of the latter
	 * are allocated than are needed.)
	 *
	 * We could do this in pmap_init when we know the actual
	 * managed page pool size, but its better to use kseg0
	 * addresses rather than kernel virtual addresses mapped
	 * through the TLB.
	 */
	pv_table_npages = physmem;
	pv_table = (struct pv_head *)
	    pmap_steal_memory(sizeof(struct pv_head) * pv_table_npages,
	    NULL, NULL);

	/*
	 * ...and initialize the pv_entry list headers.
	 */
	for (i = 0; i < pv_table_npages; i++) {
		LIST_INIT(&pv_table[i].pvh_list);
		simple_lock_init(&pv_table[i].pvh_slock);
	}

	/*
a1054 3
	vsize_t		s;
	int		bank;
	struct pv_head	*pvh;
a1064 11
	 * Memory for the pv heads has already been allocated.
	 * Initialize the physical memory segments.
	 */
	pvh = pv_table;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		s = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvhead = pvh;
		pvh += s;
	}

	/*
a1403 1
	struct pv_head *pvh;
a1406 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a1420 1
		pvh = pa_to_pvh(pa);
d1422 1
a1422 3
		simple_lock(&pvh->pvh_slock);
/* XXX */	pmap_changebit(pa, 0, ~(PG_KWE | PG_UWE), cpu_id);
		simple_unlock(&pvh->pvh_slock);
a1429 1
	pvh = pa_to_pvh(pa);
d1431 2
a1432 3
	simple_lock(&pvh->pvh_slock);
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL; pv = nextpv) {
		nextpv = LIST_NEXT(pv, pv_list);
a1465 1
	simple_unlock(&pvh->pvh_slock);
d1564 1
a1564 1
	boolean_t managed;
d1581 1
a1581 1
	managed = PAGE_IS_MANAGED(pa);
d1777 2
a1778 2
	if (managed) {
		error = pmap_pv_enter(pmap, pa, va, pte, TRUE);
d1800 1
a1800 2
	if (managed) {
		struct pv_head *pvh = pa_to_pvh(pa);
a1806 1
		simple_lock(&pvh->pvh_slock);
d1808 1
a1808 1
			pvh->pvh_attrs |= (PGA_REFERENCED|PGA_MODIFIED);
d1810 2
a1811 3
			pvh->pvh_attrs |= PGA_REFERENCED;
		attrs = pvh->pvh_attrs;
		simple_unlock(&pvh->pvh_slock);
a2317 2
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a2325 2
	pvh = pa_to_pvh(pa);

a2326 1
	simple_lock(&pvh->pvh_slock);
d2328 1
a2328 1
	if (pvh->pvh_attrs & PGA_MODIFIED) {
d2330 2
a2331 2
		pmap_changebit(pa, PG_FOW, ~0, cpu_id);
		pvh->pvh_attrs &= ~PGA_MODIFIED;
a2333 1
	simple_unlock(&pvh->pvh_slock);
a2346 2
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a2354 2
	pvh = pa_to_pvh(pa);

a2355 1
	simple_lock(&pvh->pvh_slock);
d2357 1
a2357 1
	if (pvh->pvh_attrs & PGA_REFERENCED) {
d2359 2
a2360 2
		pmap_changebit(pa, PG_FOR | PG_FOW | PG_FOE, ~0, cpu_id);
		pvh->pvh_attrs &= ~PGA_REFERENCED;
a2362 1
	simple_unlock(&pvh->pvh_slock);
a2376 2
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d2379 1
a2379 2
	pvh = pa_to_pvh(pa);
	rv = ((pvh->pvh_attrs & PGA_REFERENCED) != 0);
a2396 2
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d2399 1
a2399 2
	pvh = pa_to_pvh(pa);
	rv = ((pvh->pvh_attrs & PGA_MODIFIED) != 0);
d2487 1
d2579 3
a2581 1
	pmap_pv_remove(pmap, pa, va, dolock, pvp);
d2600 1
a2600 1
pmap_changebit(paddr_t pa, u_long set, u_long mask, cpuid_t cpu_id)
a2601 1
	struct pv_head *pvh;
a2612 2
	if (!PAGE_IS_MANAGED(pa))
		return;
a2613 1
	pvh = pa_to_pvh(pa);
d2617 1
a2617 2
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list)) {
d2673 1
a2674 1
	struct pv_head *pvh;
d2754 3
d2758 1
a2758 1
	if (!PAGE_IS_MANAGED(pa))
a2769 1
	pvh = pa_to_pvh(pa);
a2771 1
	simple_lock(&pvh->pvh_slock);
d2774 1
a2774 1
		pvh->pvh_attrs |= (PGA_REFERENCED|PGA_MODIFIED);
d2777 1
a2777 1
		pvh->pvh_attrs |= PGA_REFERENCED;
d2785 1
a2785 1
	pmap_changebit(pa, 0, ~faultoff, cpu_id);
a2786 1
	simple_unlock(&pvh->pvh_slock);
d2801 1
a2801 1
	struct pv_head *pvh;
d2807 1
a2807 9
	pvh = pa_to_pvh(pa);

	simple_lock(&pvh->pvh_slock);

	printf("pa 0x%lx (attrs = 0x%x, usage = " /* ) */, pa, pvh->pvh_attrs);
	if (pvh->pvh_usage < PGU_NORMAL || pvh->pvh_usage > PGU_L3PT)
/* ( */		printf("??? %d):\n", pvh->pvh_usage);
	else
/* ( */		printf("%s):\n", usage[pvh->pvh_usage]);
d2809 2
a2810 2
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list))
a2813 2

	simple_unlock(&pvh->pvh_slock);
d2856 1
a2856 1
pmap_pv_enter(pmap_t pmap, paddr_t pa, vaddr_t va, pt_entry_t *pte,
a2858 1
	struct pv_head *pvh;
a2870 5
	pvh = pa_to_pvh(pa);

	if (dolock)
		simple_lock(&pvh->pvh_slock);

d2877 1
a2877 2
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list))
d2888 2
a2889 4
	LIST_INSERT_HEAD(&pvh->pvh_list, newpv, pv_list);

	if (dolock)
		simple_unlock(&pvh->pvh_slock);
d2900 1
a2900 2
pmap_pv_remove(pmap_t pmap, paddr_t pa, vaddr_t va, boolean_t dolock,
    struct pv_entry **pvp)
d2902 1
a2902 7
	struct pv_head *pvh;
	pv_entry_t pv;

	pvh = pa_to_pvh(pa);

	if (dolock)
		simple_lock(&pvh->pvh_slock);
d2907 2
a2908 2
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list))
d2917 1
a2917 4
	LIST_REMOVE(pv, pv_list);

	if (dolock)
		simple_unlock(&pvh->pvh_slock);
d2919 1
a2919 9
	/*
	 * If pvp is not NULL, this is pmap_pv_alloc() stealing an
	 * entry from another mapping, and we return the now unused
	 * entry in it.  Otherwise, free the pv_entry.
	 */
	if (pvp != NULL)
		*pvp = pv;
	else
		pmap_pv_free(pv);
d2930 1
a2930 72
	struct pv_head *pvh;
	struct pv_entry *pv;
	int bank, npg, pg;
	pt_entry_t *pte;
	pmap_t pvpmap;
	cpuid_t cpu_id;

	pv = pool_get(&pmap_pv_pool, PR_NOWAIT);
	if (pv != NULL)
		return (pv);

	/*
	 * We were unable to allocate one from the pool.  Try to
	 * steal one from another mapping.  At this point we know that:
	 *
	 *	(1) We have not locked the pv table, and we already have
	 *	    the map-to-head lock, so it is safe for us to do so here.
	 *
	 *	(2) The pmap that wants this entry *is* locked.  We must
	 *	    use simple_lock_try() to prevent deadlock from occurring.
	 *
	 * XXX Note that in case #2, there is an exception; it *is* safe to
	 * steal a mapping from the pmap that wants this entry!  We may want
	 * to consider passing the pmap to this function so that we can take
	 * advantage of this.
	 */

	/* XXX This search could probably be improved. */
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npg = vm_physmem[bank].end - vm_physmem[bank].start;
		for (pg = 0; pg < npg; pg++) {
			pvh = &vm_physmem[bank].pmseg.pvhead[pg];
			simple_lock(&pvh->pvh_slock);
			for (pv = LIST_FIRST(&pvh->pvh_list);
			     pv != NULL; pv = LIST_NEXT(pv, pv_list)) {
				pvpmap = pv->pv_pmap;

				/* Don't steal from kernel pmap. */
				if (pvpmap == pmap_kernel())
					continue;

				if (simple_lock_try(&pvpmap->pm_slock) == 0)
					continue;

				pte = pv->pv_pte;

				/* Don't steal wired mappings. */
				if (pmap_pte_w(pte)) {
					simple_unlock(&pvpmap->pm_slock);
					continue;
				}

				cpu_id = cpu_number();

				/*
				 * Okay!  We have a mapping we can steal;
				 * remove it and grab the pv_entry.
				 */
				if (pmap_remove_mapping(pvpmap, pv->pv_va,
				    pte, FALSE, cpu_id))
					PMAP_SYNC_ISTREAM(pvpmap);

				/* Unlock everything and return. */
				simple_unlock(&pvpmap->pm_slock);
				simple_unlock(&pvh->pvh_slock);
				return NULL;
			}
			simple_unlock(&pvh->pvh_slock);
		}
	}

	return (NULL);
a2940 1

a2983 1
	struct pv_head *pvh;
a2995 2
		pvh = pa_to_pvh(pa);
		simple_lock(&pvh->pvh_slock);
d2997 1
a2997 7
		if (pvh->pvh_usage != PGU_NORMAL) {
			printf("pmap_physpage_alloc: page 0x%lx is "
			    "in use (%s)\n", pa,
			    pmap_pgu_strings[pvh->pvh_usage]);
			panic("pmap_physpage_alloc");
		}
		if (pvh->pvh_refcnt != 0) {
d2999 1
a2999 1
			    "%d references\n", pa, pvh->pvh_refcnt);
a3002 2
		pvh->pvh_usage = usage;
		simple_unlock(&pvh->pvh_slock);
a3016 1
	struct pv_head *pvh;
a3021 3
	pvh = pa_to_pvh(pa);

	simple_lock(&pvh->pvh_slock);
d3023 1
a3023 3
	if (pvh->pvh_usage == PGU_NORMAL)
		panic("pmap_physpage_free: not in use?!");
	if (pvh->pvh_refcnt != 0)
a3025 2
	pvh->pvh_usage = PGU_NORMAL;
	simple_unlock(&pvh->pvh_slock);
d3038 1
a3038 1
	struct pv_head *pvh;
d3043 1
a3043 1
	pvh = pa_to_pvh(pa);
d3045 1
a3045 8
	simple_lock(&pvh->pvh_slock);
#ifdef DIAGNOSTIC
	if (pvh->pvh_usage == PGU_NORMAL)
		panic("pmap_physpage_addref: not a special use page");
#endif

	rval = ++pvh->pvh_refcnt;
	simple_unlock(&pvh->pvh_slock);
d3058 1
a3058 1
	struct pv_head *pvh;
d3063 1
a3063 9
	pvh = pa_to_pvh(pa);

	simple_lock(&pvh->pvh_slock);
#ifdef DIAGNOSTIC
	if (pvh->pvh_usage == PGU_NORMAL)
		panic("pmap_physpage_delref: not a special use page");
#endif

	rval = --pvh->pvh_refcnt;
d3069 2
a3070 2
	if (pvh->pvh_refcnt < 0)
		panic("pmap_physpage_delref: negative reference count");
d3072 2
a3073 1
	simple_unlock(&pvh->pvh_slock);
@


1.64
log
@init memory from pool_get after its allocated rather than using a pool
ctor.

tweaking, testing and ok deraadt@@
@
text
@d3542 1
a3542 1
void
@


1.63
log
@Make the KERN_NPROCS and KERN_MAXPROC sysctl()s and the RLIMIT_NPROC rlimit
count processes instead of threads.  New sysctl()s KERN_NTHREADS and
KERN_MAXTHREAD count and limit threads.  The nprocs and maxproc kernel
variables are replaced by nprocess, maxprocess, nthreads, and maxthread.

ok tedu@@ mikeb@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.62 2011/11/16 20:50:17 deraadt Exp $ */
d462 1
a462 1
int	pmap_l1pt_ctor(void *, void *, int);
a896 1
	pool_set_ctordtor(&pmap_l1pt_pool, pmap_l1pt_ctor, NULL, NULL);
d3440 1
d3509 1
a3509 1
 *	Pool cache constructor for L1 PT pages.
d3511 2
a3512 2
int
pmap_l1pt_ctor(void *arg, void *object, int flags)
d3514 1
a3514 1
	pt_entry_t *l1pt = object, pte;
a3534 2

	return (0);
d3542 1
a3542 1
void *
@


1.62
log
@Make userret() MI.  On architectures which jammed stuff into it in the
past, pull that code out seperately.
ok guenther miod
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.61 2011/09/22 17:41:00 jasper Exp $ */
d760 1
a760 1
	    PAGE_SIZE + (maxproc * UPAGES) + nkmempages;
@


1.61
log
@nowadays uvm_init() calls pmap_init(), not vm_init(); so update the comments.

ok ariane@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.60 2010/11/28 21:01:41 miod Exp $ */
d621 1
a621 1
	/* for curcpu, will happen in userret() */			\
d625 1
a625 1
#define	PMAP_SYNC_ISTREAM_USER(pmap)	/* will happen in userret() */
@


1.60
log
@Rename the pmap_growkernel() current limit variable from virtual_end to
pmap_maxkvaddr, to mimic what other pmap_growkernel()-capable pmaps do,
and to reduce confusion with what virtual_end (used to) mean.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.59 2009/01/27 22:14:12 miod Exp $ */
d1088 1
a1088 1
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
@


1.59
log
@Get rid of the last traces of uvm.pager_[se]va
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.58 2008/10/23 23:54:02 tedu Exp $ */
d219 1
a219 2
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
d262 1
d264 1
d352 1
a352 1
 *	  and the virtual_end variable.
d881 2
a882 2
	virtual_avail = VM_MIN_KERNEL_ADDRESS;
	virtual_end = VM_MIN_KERNEL_ADDRESS + lev3mapsize * PAGE_SIZE;
a886 2
	printf("virtual_avail = 0x%lx\n", virtual_avail);
	printf("virtual_end = 0x%lx\n", virtual_end);
a983 7
void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = VM_MIN_KERNEL_ADDRESS;
	*vendp = VM_MAX_KERNEL_ADDRESS;
}

d1069 1
a1069 1
			*vstartp = round_page(virtual_avail);
d1227 1
a1227 1
		printf("pmap_release: pmap still contains valid mappings!\n");
d1229 1
a1229 1
			printf("pmap_release: %ld level 2 tables left\n",
d1232 1
a1232 1
			printf("pmap_release: %ld level 3 tables left\n",
d1237 1
a1237 1
			panic("pmap_release: pmap_remove() didn't");
d3323 1
a3323 1
	if (maxkvaddr <= virtual_end)
d3329 1
a3329 1
	va = virtual_end;
d3401 1
a3401 1
	virtual_end = va;
d3407 1
a3407 1
	return (virtual_end);
@


1.58
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.57 2008/09/12 12:27:26 blambert Exp $ */
a2712 9

		/*
		 * XXX don't write protect pager mappings
		 */
		if (pv->pv_pmap == pmap_kernel() &&
/* XXX */	    mask == ~(PG_KWE | PG_UWE)) {
			if (va >= uvm.pager_sva && va < uvm.pager_eva)
				continue;
		}
@


1.57
log
@Remove bzero/memset calls after pool_gets by passing the PR_ZERO
flag to the pool_get call.

ok art@@, krw@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.56 2008/09/12 08:43:18 miod Exp $ */
d454 1
a454 1
void	*pmap_l1pt_alloc(struct pool *, int);
d471 1
a471 1
void	*pmap_pv_page_alloc(struct pool *, int);
d3162 1
a3162 1
pmap_pv_page_alloc(struct pool *pp, int flags)
d3166 1
d3562 1
a3562 1
pmap_l1pt_alloc(struct pool *pp, int flags)
d3569 1
@


1.56
log
@Don't forget to actually return a pa for the K0SEG case...
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.55 2008/06/26 05:42:08 ray Exp $ */
d1170 1
a1170 2
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	memset(pmap, 0, sizeof(*pmap));
@


1.55
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.54 2007/12/09 00:24:04 tedu Exp $ */
d2132 1
@


1.54
log
@big patch to simplify pool code.

remove pool_cache code.  it was barely used, and quite complex.  it's
silly to have both a "fast" and "faster" allocation interface.  provide
a ctor/dtor interface, and convert the few cache users to use it.  no
caching at this time.

use mutexes to protect pools.  they should be initialized with pool_setipl
if the pool may be used in an interrupt context, without existing spl
protection.

ok art deraadt thib
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.53 2007/09/03 17:29:58 miod Exp $ */
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.53
log
@Remove the pv and pte stealing code. Neither art@@ and I like it, the
theoretical gain of it is close to zero, and our moms told us stealing is bad.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.52 2007/05/26 20:26:50 pedro Exp $ */
a262 1
struct pool_cache pmap_l1pt_cache;
d905 1
a905 2
	pool_cache_init(&pmap_l1pt_cache, &pmap_l1pt_pool, pmap_l1pt_ctor,
	    NULL, NULL);
d3419 1
d3422 1
d3458 1
a3458 1
	l1pt = pool_cache_get(&pmap_l1pt_cache, PR_NOWAIT);
d3526 1
a3526 1
	pool_cache_put(&pmap_l1pt_cache, l1pt);
@


1.52
log
@Dynamic buffer cache. Initial diff from mickey@@, okay art@@ beck@@ toby@@
deraadt@@ dlg@@.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.51 2007/05/04 22:51:12 miod Exp $ */
a441 13
 * Optional argument passed to pmap_remove_mapping() for stealing mapping
 * resources.
 */
struct prm_thief {
	int	prmt_flags;		/* flags; what to steal */
	struct pv_entry *prmt_pv;	/* the stolen PV entry */
	pt_entry_t *prmt_ptp;		/* the stolen PT page */
};

#define	PRMT_PV		0x0001		/* steal the PV entry */
#define	PRMT_PTP	0x0002		/* steal the PT page */

/*
d447 1
a447 1
	    boolean_t, cpuid_t, struct prm_thief *);
a455 1
boolean_t pmap_ptpage_steal(pmap_t, int, paddr_t *);
d1354 1
a1354 1
				    l3pte, TRUE, cpu_id, NULL);
d1431 1
a1431 1
								cpu_id, NULL);
d1523 1
a1523 1
			    FALSE, cpu_id, NULL) == TRUE) {
d1852 1
a1852 1
	needisync |= pmap_remove_mapping(pmap, va, pte, TRUE, cpu_id, NULL);
d2585 1
a2585 1
    boolean_t dolock, cpuid_t cpu_id, struct prm_thief *prmt)
d2601 2
a2602 13
	if (prmt != NULL) {
		if (prmt->prmt_flags & PRMT_PV)
			pvp = &prmt->prmt_pv;
		else
			pvp = NULL;
		if (prmt->prmt_flags & PRMT_PTP)
			ptp = &prmt->prmt_ptp;
		else
			ptp = NULL;
	} else {
		pvp = NULL;
		ptp = NULL;
	}
a3084 1
	struct prm_thief prmt;
a3089 2
	prmt.prmt_flags = PRMT_PV;

d3138 1
a3138 1
				    pte, FALSE, cpu_id, &prmt))
d3144 1
a3144 1
				return (prmt.prmt_pv);
d3575 2
a3576 10
	if (pmap_physpage_alloc(PGU_L1PT, &ptpa) == FALSE) {
#if 0
		/*
		 * Yow!  No free pages!  Try to steal a PT page from
		 * another pmap!
		 */
		if (pmap_ptpage_steal(pmap, PGU_L1PT, &ptpa) == FALSE)
#endif
			return (NULL);
	}
d3609 2
a3610 8
	if (pmap_physpage_alloc(usage, &ptpa) == FALSE) {
		/*
		 * Yow!  No free pages!  Try to steal a PT page from
		 * another pmap!
		 */
		if (pmap_ptpage_steal(pmap, usage, &ptpa) == FALSE)
			return (ENOMEM);
	}
a3654 140
}

/*
 * pmap_ptpage_steal:
 *
 *	Steal a PT page from a pmap.
 */
boolean_t
pmap_ptpage_steal(pmap_t pmap, int usage, paddr_t *pap)
{
	struct pv_head *pvh;
	pmap_t spmap;
	int l1idx, l2idx, l3idx;
	pt_entry_t *lev2map, *lev3map;
	vaddr_t va;
	paddr_t pa;
	struct prm_thief prmt;
	cpuid_t cpu_id = cpu_number();
	boolean_t needisync = FALSE;

	prmt.prmt_flags = PRMT_PTP;
	prmt.prmt_ptp = NULL;

	/*
	 * We look for pmaps which do not reference kernel_lev1map (which
	 * would indicate that they are either the kernel pmap, or a user
	 * pmap with no valid mappings).  Since the list of all pmaps is
	 * maintained in an LRU fashion, we should get a pmap that is
	 * `more inactive' than our current pmap (although this may not
	 * always be the case).
	 *
	 * We start looking for valid L1 PTEs at the lowest address,
	 * go to that L2, look for the first valid L2 PTE, and steal
	 * that L3 PT page.
	 */
	simple_lock(&pmap_all_pmaps_slock);
	for (spmap = TAILQ_FIRST(&pmap_all_pmaps);
	     spmap != NULL; spmap = TAILQ_NEXT(spmap, pm_list)) {
		/*
		 * Skip the kernel pmap and ourselves.
		 */
		if (spmap == pmap_kernel() || spmap == pmap)
			continue;

		PMAP_LOCK(spmap);
		if (spmap->pm_lev1map == kernel_lev1map) {
			PMAP_UNLOCK(spmap);
			continue;
		}

		/*
		 * Have a candidate pmap.  Loop through the PT pages looking
		 * for one we can steal.
		 */
		for (l1idx = 0;
		     l1idx < l1pte_index(VM_MAXUSER_ADDRESS); l1idx++) {
			if (pmap_pte_v(&spmap->pm_lev1map[l1idx]) == 0)
				continue;

			lev2map = (pt_entry_t *)ALPHA_PHYS_TO_K0SEG(
			    pmap_pte_pa(&spmap->pm_lev1map[l1idx]));
			for (l2idx = 0; l2idx < NPTEPG; l2idx++) {
				if (pmap_pte_v(&lev2map[l2idx]) == 0)
					continue;
				lev3map = (pt_entry_t *)ALPHA_PHYS_TO_K0SEG(
				    pmap_pte_pa(&lev2map[l2idx]));
				for (l3idx = 0; l3idx < NPTEPG; l3idx++) {
					/*
					 * If the entry is valid and wired,
					 * we cannot steal this page.
					 */
					if (pmap_pte_v(&lev3map[l3idx]) &&
					    pmap_pte_w(&lev3map[l3idx]))
						break;
				}
				
				/*
				 * If we scanned all of the current L3 table
				 * without finding a wired entry, we can
				 * steal this page!
				 */
				if (l3idx == NPTEPG)
					goto found_one;
			}
		}

		/*
		 * Didn't find something we could steal in this
		 * pmap, try the next one.
		 */
		PMAP_UNLOCK(spmap);
		continue;

 found_one:
		/* ...don't need this anymore. */
		simple_unlock(&pmap_all_pmaps_slock);

		/*
		 * Okay!  We have a PT page we can steal.  l1idx and
		 * l2idx indicate which L1 PTP and L2 PTP we should
		 * use to compute the virtual addresses the L3 PTP
		 * maps.  Loop through all the L3 PTEs in this range
		 * and nuke the mappings for them.  When we're through,
		 * we'll have a PT page pointed to by prmt.prmt_ptp!
		 */
		for (l3idx = 0,
		     va = (l1idx * ALPHA_L1SEG_SIZE) +
		          (l2idx * ALPHA_L2SEG_SIZE);
		     l3idx < NPTEPG && prmt.prmt_ptp == NULL;
		     l3idx++, va += PAGE_SIZE) {
			if (pmap_pte_v(&lev3map[l3idx])) {
				needisync |= pmap_remove_mapping(spmap, va,
				    &lev3map[l3idx], TRUE, cpu_id, &prmt);
			}
		}

		if (needisync)
			PMAP_SYNC_ISTREAM(pmap);

		PMAP_UNLOCK(spmap);

#ifdef DIAGNOSTIC
		if (prmt.prmt_ptp == NULL)
			panic("pmap_ptptage_steal: failed");
		if (prmt.prmt_ptp != lev3map)
			panic("pmap_ptpage_steal: inconsistent");
#endif
		pa = ALPHA_K0SEG_TO_PHYS((vaddr_t)prmt.prmt_ptp);

		/*
		 * Don't bother locking here; the assignment is atomic.
		 */
		pvh = pa_to_pvh(pa);
		pvh->pvh_usage = usage;

		*pap = pa;
		return (TRUE);
	}
	simple_unlock(&pmap_all_pmaps_slock);
	return (FALSE);
@


1.51
log
@Faster pmap_extract() code for pmap_kernel, from NetBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.50 2007/04/13 08:31:50 martin Exp $ */
d780 2
a781 3
	lev3mapsize = (VM_PHYS_SIZE +
		nbuf * MAXBSIZE + 16 * NCARGS + PAGER_MAP_SIZE) / PAGE_SIZE +
		(maxproc * UPAGES) + nkmempages;
@


1.50
log
@get alpha SMP into a state where it at least compiles:

- add machine-dependent spinlock operations
- add basic interprocessor interrupt sending and receiving code

from NetBSD; ok miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.49 2007/02/03 16:48:21 miod Exp $ */
d2150 18
d2187 1
@


1.49
log
@Remove unused functionality from lockmgr():
- LK_EXCLUPGRADE is never used.
- LK_REENABLE is never used.
- LK_SETRECURSE is never used. Because of this, the lk_recurselevel
  field is always zero, so it can be removed to.
- the spinlock version (and LK_SPIN) is never used, since it was decided
  to use different locking structure for MP-safe protection.

Tested by many
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.48 2006/11/29 12:24:15 miod Exp $ */
d4111 1
a4111 1
				ipinum = ALPHA_IPI_TBIA;
d4113 1
a4113 1
				ipinum = ALPHA_IPI_TBIAP;
@


1.48
log
@Kernel stack can be swapped. This means that stuff that's on the stack
should never be referenced outside the context of the process to which
this stack belongs unless we do the PHOLD/PRELE dance. Loads of code
doesn't follow the rules here. Instead of trying to track down all
offenders and fix this hairy situation, it makes much more sense
to not swap kernel stacks.

From art@@, tested by many some time ago.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.47 2006/04/13 14:41:08 brad Exp $ */
a339 14
 *	READ/WRITE SPIN LOCKS
 *	---------------------
 *
 *	* pmap_main_lock - This lock is used to prevent deadlock and/or
 *	  provide mutex access to the pmap module.  Most operations lock
 *	  the pmap first, then PV lists as needed.  However, some operations,
 *	  such as pmap_page_protect(), lock the PV lists before locking
 *	  the pmaps.  To prevent deadlock, we require a mutex lock on the
 *	  pmap module if locking in the PV->pmap direction.  This is
 *	  implemented by acquiring a (shared) read lock on pmap_main_lock
 *	  if locking pmap->PV and a (exclusive) write lock if locking in
 *	  the PV->pmap direction.  Since only one thread can hold a write
 *	  lock at a time, this provides the mutex.
 *
a368 1
struct lock pmap_main_lock;
a371 10
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
#define	PMAP_MAP_TO_HEAD_LOCK() \
	spinlockmgr(&pmap_main_lock, LK_SHARED, NULL)
#define	PMAP_MAP_TO_HEAD_UNLOCK() \
	spinlockmgr(&pmap_main_lock, LK_RELEASE, NULL)
#define	PMAP_HEAD_TO_MAP_LOCK() \
	spinlockmgr(&pmap_main_lock, LK_EXCLUSIVE, NULL)
#define	PMAP_HEAD_TO_MAP_UNLOCK() \
	spinlockmgr(&pmap_main_lock, LK_RELEASE, NULL)
#else
a375 1
#endif /* MULTIPROCESSOR || LOCKDEBUG */
a943 1
	spinlockinit(&pmap_main_lock, "pmaplk", 0);
@


1.47
log
@Use PAGE_SIZE rather than NBPG.

From NetBSD

ok martin@@ miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.46 2006/03/04 19:33:19 miod Exp $ */
d2870 2
a2871 3
	 * Can't do these, because cpu_fork and cpu_swapin call
	 * pmap_emulate_reference(), and the bits aren't guaranteed,
	 * for them...
@


1.46
log
@Typos grab bag of the month, eyeballed by jmc@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.45 2006/02/07 07:59:22 martin Exp $ */
a118 4
 *	This pmap implementation only supports NBPG == PAGE_SIZE.
 *	In practice, this is not a problem since PAGE_SIZE is
 *	initialized to the hardware page size in alpha_init().
 *
d807 1
a807 1
		nbuf * MAXBSIZE + 16 * NCARGS + PAGER_MAP_SIZE) / NBPG +
d818 1
a818 1
	 * IF THIS IS NOT A MULTIPLE OF NBPG, ALL WILL GO TO HELL.
d826 1
a826 1
	 * IF THIS IS NOT A MULTIPLE OF NBPG, ALL WILL GO TO HELL.
@


1.45
log
@convert pmap_phys_address() to a define, consistent with other archs;
avoids losing information due to int in proto ...

thanks to KUDO Takashi for tracking this down

ok miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.44 2005/11/03 06:40:28 brad Exp $ */
d315 1
a315 1
 * TLB entires, the PALcode might use incorrect information to service a
d317 1
a317 1
 * to locate the PTE for a faulting address, and tagged TLB entires exist
@


1.44
log
@use splvm() here instead of splimp(). in currently unused MULTIPROCESSOR code.

ok martin@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.43 2005/10/28 19:10:26 martin Exp $ */
a2533 16
}

/*
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
pmap_phys_address(int ppn)
{

	return (ptoa(ppn));
@


1.43
log
@no more Mach-macros
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.42 2004/06/13 21:49:11 niklas Exp $ */
d439 1
a439 1
	s = splimp();							\
@


1.42
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d2549 1
a2549 1
	return (alpha_ptob(ppn));
@


1.41
log
@Use cpuid_t whenever possible, rather than an angry combination of long
there and u_long elsewhere.

ok marc@@ deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.40 2003/12/22 19:59:37 jmc Exp $ */
a389 5

#ifdef __OpenBSD__
#define spinlockinit(lock, name, flags)  lockinit(lock, 0, name, 0, flags)
#define spinlockmgr(lock, flags, slock) lockmgr(lock, flags, slock, curproc)
#endif
@


1.40
log
@typos from Jared Yanovich;
note: i only committed some of these.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.39 2003/10/18 20:14:40 jmc Exp $ */
d495 2
a496 2
	    boolean_t, long, struct prm_thief *);
void	pmap_changebit(paddr_t, pt_entry_t, pt_entry_t, long);
d501 2
a502 2
int	pmap_lev1map_create(pmap_t, long);
void	pmap_lev1map_destroy(pmap_t, long);
d506 1
a506 1
void	pmap_l3pt_delref(pmap_t, vaddr_t, pt_entry_t *, long,
d508 2
a509 2
void	pmap_l2pt_delref(pmap_t, pt_entry_t *, pt_entry_t *, long);
void	pmap_l1pt_delref(pmap_t, pt_entry_t *, long);
d540 1
a540 1
void	pmap_asn_alloc(pmap_t, long);
d1370 1
a1370 1
	long cpu_id = cpu_number();
d1531 1
a1531 1
	long cpu_id = cpu_number();
d1614 1
a1614 1
	long cpu_id = cpu_number();
d1705 1
a1705 1
	long cpu_id = cpu_number();
d2016 1
a2016 1
	long cpu_id = cpu_number();
d2080 1
a2080 1
	long cpu_id = cpu_number();
d2286 1
a2286 1
	long cpu_id = cpu_number();
d2436 1
a2436 1
	long cpu_id = cpu_number();
d2471 1
a2471 1
	long cpu_id = cpu_number();
d2633 1
a2633 1
    boolean_t dolock, long cpu_id, struct prm_thief *prmt)
d2757 1
a2757 1
pmap_changebit(paddr_t pa, u_long set, u_long mask, long cpu_id)
d2847 1
a2847 1
	long cpu_id = cpu_number();
d3144 1
a3144 1
	u_long cpu_id;
d3507 1
a3507 1
pmap_lev1map_create(pmap_t pmap, long cpu_id)
d3550 1
a3550 1
pmap_lev1map_destroy(pmap_t pmap, long cpu_id)
d3749 1
a3749 1
	u_long cpu_id = cpu_number();
d3883 1
a3883 1
pmap_l3pt_delref(pmap_t pmap, vaddr_t va, pt_entry_t *l3pte, long cpu_id,
d3940 1
a3940 1
    long cpu_id)
d3978 1
a3978 1
pmap_l1pt_delref(pmap_t pmap, pt_entry_t *l1pte, long cpu_id)
d4005 1
a4005 1
pmap_asn_alloc(pmap_t pmap, long cpu_id)
d4142 2
a4143 1
	u_long i, ipinum, cpu_id = cpu_number();
d4190 1
a4190 1
	u_long cpu_id = ci->ci_cpuid;
d4218 1
a4218 1
pmap_tlb_shootdown_q_drain(u_long cpu_id, boolean_t all)
@


1.39
log
@typos from Jared Yanovich;
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.38 2003/06/02 23:27:43 millert Exp $ */
d929 1
a929 1
	/* Nothing to do; it's already zero'd */
d3271 1
a3271 1
	 * Don't ask for a zero'd page in the L1PT case -- we will
@


1.38
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.37 2003/02/18 13:14:43 jmc Exp $ */
d949 1
a949 1
	 * Intialize the pmap pools and list.
d2725 1
a2725 1
	 * If the mapping wasn't enterd on the PV list, we're all done.
d2776 1
a2776 1
	 * Loop over all current mappings setting/clearing as appropos.
d3628 1
a3628 1
 *	Page alloctor for L1 PT pages.
d4089 1
a4089 1
			 * the pmaps, and invaldating the generation
@


1.37
log
@intial -> initial;

the great intial witch hunt, as prompted by tdeval@@

os-aix-dso.c: ok henning@@
ab.C: ok drahn@@
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.36 2002/09/10 18:29:42 art Exp $ */
d57 1
a57 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.36
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.35 2002/07/24 00:33:49 art Exp $ */
d867 1
a867 1
	 * ...and intialize the pv_entry list headers.
@


1.35
log
@Pretty nasty hack to make non-exec mappings work.
Instead of using FOE for just emulating references, we also keep track
of a pages executability and don't remove the FOE bit if the page
is not executable.

This is implmented with horrible hacks. Maybe when I have time, I'll
reimplment the whole pmap to allow this without ugly hacks (read: probably
not this decade).

The stack on alpha is now non-exec.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.34 2002/06/25 21:33:19 miod Exp $ */
d2358 1
a2358 1
pmap_zero_page(paddr_t phys)
d2360 1
d2414 1
a2414 1
pmap_copy_page(paddr_t src, paddr_t dst)
d2416 2
d3728 1
a3728 1
		pmap_zero_page(ptpa);
d3732 1
a3732 1
		pmap_zero_page(ptpa);
@


1.34
log
@No \n at the end of a panic() message... I thought all occurences had been
squashed already.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.33 2002/01/23 00:39:46 art Exp $ */
d1640 2
d1960 4
d2814 3
d2837 3
d2841 2
a2842 2
void
pmap_emulate_reference(struct proc *p, vaddr_t v, int user, int write)
d2853 1
a2853 1
		    p, v, user, write);
d2880 5
d2900 1
a2900 1
	if (write) {
d2929 1
a2929 1
		panic("pmap_emulate_reference(%p, 0x%lx, %d, %d): pa 0x%lx not managed", p, v, user, write, pa);
d2945 1
a2945 1
	if (write) {
d2952 5
d2961 2
@


1.33
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.32 2001/12/19 08:58:05 art Exp $ */
d573 1
a573 1
		panic("PMAP_ISACTIVE, isa: %d pm: %p curpm:%p\n",	\
@


1.32
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.31 2001/12/08 02:24:05 art Exp $ */
d515 6
a520 2
void	*pmap_l1pt_alloc(unsigned long, int, int);
void	pmap_l1pt_free(void *, unsigned long, int);
d532 5
a536 2
void	*pmap_pv_page_alloc(u_long, int, int);
void	pmap_pv_page_free(void *, u_long, int);
d957 1
a957 1
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
d959 1
a959 1
	    0, pmap_l1pt_alloc, pmap_l1pt_free, M_VMPMAP);
d963 1
a963 2
	    "pmasnpl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
d965 1
a965 2
	    "pmasngenpl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
d967 1
a967 1
	    0, pmap_pv_page_alloc, pmap_pv_page_free, M_VMPMAP);
d1011 1
a1011 1
	    0, NULL, NULL, M_VMPMAP);
d3211 1
a3211 1
pmap_pv_page_alloc(u_long size, int flags, int mtype)
d3226 1
a3226 1
pmap_pv_page_free(void *v, u_long size, int mtype)
d3608 1
a3608 1
pmap_l1pt_alloc(unsigned long sz, int flags, int mtype)
d3635 1
a3635 1
pmap_l1pt_free(void *v, unsigned long sz, int mtype)
@


1.31
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.30 2001/12/05 16:30:47 art Exp $ */
d812 1
a812 1
	lev3mapsize = (VM_PHYS_SIZE + (ubc_nwins << ubc_winshift) +
@


1.31.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.33 2002/01/23 00:39:46 art Exp $ */
d515 2
a516 6
void	*pmap_l1pt_alloc(struct pool *, int);
void	pmap_l1pt_free(struct pool *, void *);

struct pool_allocator pmap_l1pt_allocator = {
	pmap_l1pt_alloc, pmap_l1pt_free, 0,
};
d528 2
a529 5
void	*pmap_pv_page_alloc(struct pool *, int);
void	pmap_pv_page_free(struct pool *, void *);
struct pool_allocator pmap_pv_allocator = {
	pmap_pv_page_alloc, pmap_pv_page_free, 0,
};
d950 1
a950 1
	    &pool_allocator_nointr);
d952 1
a952 1
	    &pmap_l1pt_allocator);
d956 2
a957 1
	    "pmasnpl", &pool_allocator_nointr);
d959 2
a960 1
	    "pmasngenpl", &pool_allocator_nointr);
d962 1
a962 1
	    &pmap_pv_allocator);
d1006 1
a1006 1
	    NULL);
d3206 1
a3206 1
pmap_pv_page_alloc(struct pool *pp, int flags)
d3221 1
a3221 1
pmap_pv_page_free(struct pool *pp, void *v)
d3603 1
a3603 1
pmap_l1pt_alloc(struct pool *pp, int flags)
d3630 1
a3630 1
pmap_l1pt_free(struct pool *pp, void *v)
@


1.31.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.31.2.1 2002/01/31 22:55:04 niklas Exp $ */
d2776 9
@


1.31.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.31.2.2 2002/02/02 03:28:25 art Exp $ */
d573 1
a573 1
		panic("PMAP_ISACTIVE, isa: %d pm: %p curpm:%p",		\
a1639 2
	if (!pmap_pte_exec(&bits))
		bits |= PG_FOE;
a1957 4
		/* Always force FOE on non-exec mappings. */
		if (!pmap_pte_exec(pte))
			npte |= PG_FOE;

d2352 1
a2352 1
pmap_zero_page(struct vm_page *pg)
a2353 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d2407 1
a2407 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a2408 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
a2798 3
			} else {
				/* Never clear FOE on non-exec mappings. */
				npte |= PG_FOE;
a2818 3
 *
 *	return non-zero if this was a FOE fault and the pte is not
 *	executable.
d2820 2
a2821 2
int
pmap_emulate_reference(struct proc *p, vaddr_t v, int user, int type)
d2832 1
a2832 1
		    p, v, user, type);
a2858 5
	if (!pmap_pte_exec(pte) && type == ALPHA_MMCSR_FOE) {
		if (didlock)
			PMAP_UNLOCK(p->p_vmspace->vm_map.pmap);
		return (1);
	}
d2874 1
a2874 1
	if (type == ALPHA_MMCSR_FOW) {
d2903 1
a2903 1
		panic("pmap_emulate_reference(%p, 0x%lx, %d, %d): pa 0x%lx not managed", p, v, user, type, pa);
d2919 1
a2919 1
	if (type == ALPHA_MMCSR_FOW) {
a2925 5
	/*
	 * If the page is not PG_EXEC, pmap_changebit will automagically
	 * set PG_FOE (gross, but necessary if I don't want to change the
	 * whole API).
	 */
a2929 2

	return (0);
d3692 1
a3692 1
		pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
d3696 1
a3696 1
		pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
@


1.31.2.4
log
@sync
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d867 1
a867 1
	 * ...and initialize the pv_entry list headers.
@


1.30
log
@calculate and use nkmempages, not NKMEMCLUSTERS.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.29 2001/11/28 16:24:26 art Exp $ */
d1303 1
@


1.29
log
@more typedef zapping vm_page_t -> struct vm_page *
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.28 2001/11/28 15:34:16 art Exp $ */
d805 5
d814 1
a814 1
		(maxproc * UPAGES) + NKMEMCLUSTERS;
@


1.28
log
@Make pmap_update functions into nops so that we can have a consistent
pmap_update API (right now it's nop).
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.27 2001/11/28 14:20:16 art Exp $ */
d1518 1
a1518 1
pmap_page_protect(vm_page_t pg, vm_prot_t prot)
d2415 1
a2415 1
pmap_clear_modify(vm_page_t pg)
d2450 1
a2450 1
pmap_clear_reference(vm_page_t pg)
d2486 1
a2486 1
pmap_is_referenced(vm_page_t pg)
d2509 1
a2509 1
pmap_is_modified(vm_page_t pg)
@


1.27
log
@make pmap_virtual_space madatory in all pmaps.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.26 2001/11/28 14:13:06 art Exp $ */
a2220 21
}

/*
 * pmap_update:			[ INTERFACE ]
 *
 *	Require that all active physical maps contain no
 *	incorrect entries NOW, by processing any deferred
 *	pmap operations.
 */
void
pmap_update(void)
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_update()\n");
#endif

	/*
	 * Nothing to do; this pmap module does not defer any operations.
	 */
@


1.26
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.25 2001/11/28 13:47:37 art Exp $ */
d1038 7
@


1.25
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.24 2001/11/27 05:27:11 art Exp $ */
a2049 26
 * pmap_kenter_pgs:		[ INTERFACE ]
 *
 *	Enter a va -> pa mapping for the array of vm_page's into the
 *	kernel pmap without any physical->virtual tracking, starting
 *	at address va, for npgs pages.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_kenter_pgs(vaddr_t va, vm_page_t *pgs, int npgs)
{
	int i;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_kenter_pgs(%lx, %p, %d)\n",
		    va, pgs, npgs);
#endif

	for (i = 0; i < npgs; i++)
		pmap_kenter_pa(va + (PAGE_SIZE * i),
		    VM_PAGE_TO_PHYS(pgs[i]),
		    VM_PROT_READ|VM_PROT_WRITE);
}

/*
d2052 1
a2052 1
 *	Remove a mapping entered with pmap_kenter_pa() or pmap_kenter_pgs()
@


1.24
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.23 2001/11/09 15:31:11 art Exp $ */
d807 1
a807 1
	lev3mapsize = (VM_PHYS_SIZE + ubc_nwins * ubc_winsize +
d1690 1
a1690 1
	int error = KERN_SUCCESS;
d1747 1
a1747 1
			if (error != KERN_SUCCESS) {
d1764 1
a1764 1
			if (error != KERN_SUCCESS) {
d1789 1
a1789 1
			if (error != KERN_SUCCESS) {
d1895 1
a1895 1
		if (error != KERN_SUCCESS) {
d3055 1
a3055 1
		return (KERN_RESOURCE_SHORTAGE);
d3088 1
a3088 1
	return (KERN_SUCCESS);
d3530 1
a3530 1
		return (KERN_RESOURCE_SHORTAGE);
d3545 1
a3545 1
	return (KERN_SUCCESS);
d3692 1
a3692 1
			return (KERN_RESOURCE_SHORTAGE);
d3702 1
a3702 1
	return (KERN_SUCCESS);
@


1.23
log
@When calculating the initial size of the lev3map, use PAGER_MAP_SIZE, not a magic constant.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.22 2001/11/09 02:57:03 art Exp $ */
d807 2
a808 2
	lev3mapsize = (VM_PHYS_SIZE +
		nbuf * MAXBSIZE + + PAGER_MAP_SIZE + 16 * NCARGS) / NBPG +
@


1.22
log
@Ieeek. invalidate the pmap_l1pt_cache in pmap_growkernel.
How did this ever work?
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.17 2001/08/12 12:03:02 heko Exp $ */
a805 3
	 * The '512' comes from PAGER_MAP_SIZE in vm_pager_init().
	 * This should be kept in sync.
	 * We also reserve space for kmem_alloc_pageable() for vm_fork().
d808 1
a808 1
		nbuf * MAXBSIZE + 16 * NCARGS) / NBPG + 512 +
@


1.21
log
@Don't uvm_pagezero here, allocate a zeroed page instead.
@
text
@d3493 3
@


1.20
log
@Actually allow pa == 0 in pmap_extract, and don't set *pap if we fail.
@
text
@d3284 2
a3285 1
	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
a3286 2
		if (usage != PGU_L1PT)
			uvm_pagezero(pg);
@


1.19
log
@Update DEBUG code to match reality.
@
text
@d2188 2
a2189 1
	paddr_t pa = 0;
d2210 2
d2216 1
a2216 1
		if (pa)
d2222 1
a2222 2
	*pap = pa;
	return (pa != 0);
@


1.18
log
@No need to initialize the pmap_growkernel_slock twice.
No need to initialize the pmap_growkernel_slock twice.
@
text
@d1698 1
a1698 1
		       pmap, va, pa, prot, access_type);
@


1.17
log
@#(endif|else) foo is incorrect, make it #endif /* foo */
deraadt@@ ok
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.16 2001/07/25 13:25:31 art Exp $ */
a382 3
 *	* pmap_growkernel_slock - This lock protects pmap_growkernel()
 *	  and the virtual_end variable.
 *
a916 3

	/* Initialize the pmap_growkernel_slock. */
	simple_lock_init(&pmap_growkernel_slock);
@


1.16
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.15 2001/06/23 19:36:44 art Exp $ */
d1046 1
a1046 1
#endif _PMAP_MAY_USE_PROM_CONSOLE
@


1.15
log
@Use pool_cache for l1 ptes.
From NetBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.14 2001/06/08 08:08:35 art Exp $ */
d1686 2
a1687 3
void
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int wired,
	vm_prot_t access_type)
d1697 1
d1699 1
a1699 1
	int error;
a1705 3
	if (pmap == NULL)
		return;

d1708 1
a1756 1
#ifdef notyet
d1758 1
a1758 2
					return (error);
#endif
a1774 1
#ifdef notyet
d1776 1
a1776 2
					return (error);
#endif
a1799 1
#ifdef notyet
d1801 1
a1801 2
					return (error);
#endif
a1905 1
#ifdef notyet
d1907 1
a1907 2
				return (error);
#endif
d1930 1
a1930 1
		if ((access_type & VM_PROT_ALL) & ~prot)
d1934 1
a1934 1
		if (access_type & VM_PROT_WRITE)
d1936 1
a1936 1
		else if (access_type & VM_PROT_ALL)
d1986 1
d1990 1
a1990 3
#ifdef notyet	
	return (KERN_SUCCESS);
#endif	
@


1.14
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 2
a2 2
/* $OpenBSD: pmap.c,v 1.13 2001/05/09 15:31:23 art Exp $ */
/* $NetBSD: pmap.c,v 1.148 2000/09/22 05:23:37 thorpej Exp $ */
a129 2
 *
 *	- pmap_growkernel() should be implemented.
d270 2
d383 3
d518 5
d924 3
d955 4
d3294 5
d3301 2
a3302 1
		uvm_pagezero(pg);
d3531 1
a3531 3
	paddr_t ptpa;
	pt_entry_t pte;
	int i;
d3541 6
a3546 10
	/*
	 * Allocate a page for the level 1 table.
	 */
	if (pmap_physpage_alloc(PGU_L1PT, &ptpa) == FALSE) {
		/*
		 * Yow!  No free pages!  Try to steal a PT page from
		 * another pmap!
		 */
		if (pmap_ptpage_steal(pmap, PGU_L1PT, &ptpa) == FALSE)
			return (KERN_RESOURCE_SHORTAGE);
a3547 1
	pmap->pm_lev1map = (pt_entry_t *) ALPHA_PHYS_TO_K0SEG(ptpa);
d3549 1
a3549 7
	/*
	 * Initialize the new level 1 table by copying the
	 * kernel mappings into it.
	 */
	for (i = l1pte_index(VM_MIN_KERNEL_ADDRESS);
	     i <= l1pte_index(VM_MAX_KERNEL_ADDRESS); i++)
		pmap->pm_lev1map[i] = kernel_lev1map[i];
d3551 1
a3551 5
	/*
	 * Now, map the new virtual page table.  NOTE: NO ASM!
	 */
	pte = ((ptpa >> PGSHIFT) << PG_SHIFT) | PG_V | PG_KRE | PG_KWE;
	pmap->pm_lev1map[l1pte_index(VPTBASE)] = pte;
d3574 1
a3574 1
	paddr_t ptpa;
a3580 2
	ptpa = ALPHA_K0SEG_TO_PHYS((vaddr_t)pmap->pm_lev1map);

d3611 73
a3683 1
	pmap_physpage_free(ptpa);
d4209 1
a4209 1
pmap_do_tlb_shootdown(void)
d4211 1
a4211 1
	u_long cpu_id = cpu_number();
@


1.13
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.12 2001/04/10 06:59:13 niklas Exp $ */
d2186 2
a2187 2
paddr_t
pmap_extract(pmap_t pmap, vaddr_t va)
d2221 2
a2222 1
	return (pa);
@


1.12
log
@Fix for machines which need to enlarge the kernel address space, at least
1GB i386 machines needs this.  The fix is heavily based on Jason Thorpe's
found in NetBSD.  Here is his original commit message:

Instead of checking vm_physmem[<physseg>].pgs to determine if
uvm_page_init() has completed, add a boolean uvm.page_init_done,
and test against that.  Use this same boolean (rather than
pmap_initialized) in pmap_growkernel() to determine if we are
being called via uvm_page_init() to grow the kernel address space.

This fixes a problem on some i386 configurations where pmap_init()
itself was needing to have the kernel page table grown, and since
pmap_initialized was not yet set to TRUE, pmap_growkernel() was
choosing the wrong code path.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.11 2001/03/16 14:10:23 art Exp $ */
d2142 1
a2142 1
pmap_change_wiring(pmap_t pmap, vaddr_t va, int wired)
a2145 3
	if (wired)
		panic("pmap_change_wiring");

a2448 9
}

void
pmap_pageable(pmap, start, end, pageable)
	pmap_t		pmap;
	vaddr_t		start;
	vaddr_t		end;
	boolean_t	pageable;
{
@


1.11
log
@Some more pmap improvements from NetBSD.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.10 2001/03/16 09:06:03 art Exp $ */
a1067 1
#if 0
a1069 5
#else
		/* XXXX */
		if (vm_physmem[0].pgs)
			panic("vm_page_physget: called _after_ bootstrap");
#endif
a3444 1
#ifdef notyet
a3445 3
#else
			if (vm_physmem[0].pgs == NULL) {
#endif
a3485 1
#ifdef notyet
a3486 3
#else
		if (vm_physmem[0].pgs == NULL) {
#endif
@


1.10
log
@Implement pmap_growkernel. From NetBSD.
@
text
@d1 2
a2 2
/* $OpenBSD: pmap.c,v 1.9 2001/03/04 13:37:44 art Exp $ */
/* $NetBSD: pmap.c,v 1.132 2000/05/23 05:12:54 thorpej Exp $ */
d130 2
a163 4
#include <vm/vm.h>
#include <vm/vm_kern.h>
#include <vm/vm_page.h>

d446 12
a462 1
void	pmap_tlb_shootdown_q_drain __P((struct pmap_tlb_shootdown_q *));
d464 3
a466 3
	    __P((struct pmap_tlb_shootdown_q *));
void	pmap_tlb_shootdown_job_put __P((struct pmap_tlb_shootdown_q *,
	    struct pmap_tlb_shootdown_job *));
a470 2
static __inline struct pv_head *pa_to_pvh __P((paddr_t));

d472 1
a472 2
pa_to_pvh(pa)
	paddr_t pa;
d496 5
a500 4
void	alpha_protection_init __P((void));
boolean_t pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *,
	    boolean_t, long, struct prm_thief *));
void	pmap_changebit __P((paddr_t, pt_entry_t, pt_entry_t, long));
d505 9
a513 9
int	pmap_lev1map_create __P((pmap_t, long));
void	pmap_lev1map_destroy __P((pmap_t, long));
int	pmap_ptpage_alloc __P((pmap_t, pt_entry_t *, int));
boolean_t pmap_ptpage_steal __P((pmap_t, int, paddr_t *));
void	pmap_ptpage_free __P((pmap_t, pt_entry_t *, pt_entry_t **));
void	pmap_l3pt_delref __P((pmap_t, vaddr_t, pt_entry_t *, long,
	    pt_entry_t **));
void	pmap_l2pt_delref __P((pmap_t, pt_entry_t *, pt_entry_t *, long));
void	pmap_l1pt_delref __P((pmap_t, pt_entry_t *, long));
d518 7
a524 7
int	pmap_pv_enter __P((pmap_t, paddr_t, vaddr_t, pt_entry_t *, boolean_t));
void	pmap_pv_remove __P((pmap_t, paddr_t, vaddr_t, boolean_t,
	    struct pv_entry **));
struct	pv_entry *pmap_pv_alloc __P((void));
void	pmap_pv_free __P((struct pv_entry *));
void	*pmap_pv_page_alloc __P((u_long, int, int));
void	pmap_pv_page_free __P((void *, u_long, int));
d526 1
a526 1
void	pmap_pv_dump __P((paddr_t));
d532 1
a532 1
void	pmap_asn_alloc __P((pmap_t, long));
d537 4
a540 4
boolean_t pmap_physpage_alloc __P((int, paddr_t *));
void	pmap_physpage_free __P((paddr_t));
int	pmap_physpage_addref __P((void *));
int	pmap_physpage_delref __P((void *));
d645 8
a657 2
 *
 *	XXX Need MULTIPROCESSOR versions of these.
d659 13
d673 2
a674 2

#define	PMAP_SYNC_ISTREAM_USER(pmap)	(pmap)->pm_needisync = ~0UL
d765 7
d787 1
a787 4
pmap_bootstrap(ptaddr, maxasn, ncpuids)
	paddr_t ptaddr;
	u_int maxasn;
	u_long ncpuids;
d1020 1
a1020 1
pmap_uses_prom_console()
d1054 1
a1054 3
pmap_steal_memory(size, vstartp, vendp)
	vsize_t size;
	vaddr_t *vstartp, *vendp;
d1129 1
a1129 1
		bzero((caddr_t)va, size);
d1149 1
a1149 1
pmap_init()
d1207 1
a1207 1
pmap_create()
d1218 1
a1218 1
	bzero(pmap, sizeof(*pmap));
d1252 1
a1252 2
pmap_destroy(pmap)
	pmap_t pmap;
d1307 1
a1307 2
pmap_reference(pmap)
	pmap_t	pmap;
d1330 21
a1350 3
pmap_remove(pmap, sva, eva)
	pmap_t pmap;
	vaddr_t sva, eva;
d1378 2
d1461 3
a1463 1
						if (pmap_pte_v(l3pte)) {
d1511 1
a1511 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
d1516 1
a1516 1
	boolean_t needisync = FALSE;
a1525 7
	/*
	 * Even though we don't change the mapping of the page,
	 * we still flush the I-cache if VM_PROT_EXECUTE is set
	 * because we might be "adding" execute permissions to
	 * a previously non-execute page.
	 */

a1527 1
		alpha_pal_imb();	/* XXX XXX XXX */
a1531 1
		alpha_pal_imb();	/* XXX XXX XXX */
d1558 9
a1566 3
		if (pmap_pte_w(pv->pv_pte) == 0)
			needisync |= pmap_remove_mapping(pmap,
			    pv->pv_va, pv->pv_pte, FALSE, cpu_id, NULL);
d1580 2
a1581 2
	if (needisync)
		alpha_pal_imb();
d1594 1
a1594 4
pmap_protect(pmap, sva, eva, prot)
	pmap_t	pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
d1647 1
a1647 2
#if defined(MULTIPROCESSOR) && 0
							pmap_tlb_shootdown(
a1649 1
#endif
d1678 2
a1679 7
pmap_enter(pmap, va, pa, prot, wired, access_type)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int wired;
	vm_prot_t access_type;
d1686 2
a1687 1
	boolean_t needisync;
d1702 15
a1716 1
	needisync = isactive && (prot & VM_PROT_EXECUTE) != 0;
d1837 1
a1837 1
		needisync = FALSE;
d1972 1
a1972 1
	*pte = npte;
d1980 1
a1980 3
#if defined(MULTIPROCESSOR) && 0
		pmap_tlb_shootdown(pmap, va, hadasm ? PG_ASM : 0);
#endif
d1982 2
d2004 1
a2004 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d2044 4
a2047 1
	*pte = npte;
d2054 2
a2055 3
#if defined(MULTIPROCESSOR) && 0
	pmap_tlb_shootdown(pmap, va, PG_ASM);
#endif
d2070 1
a2070 4
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	vm_page_t *pgs;
	int npgs;
d2093 1
a2093 3
pmap_kremove(va, size)
	vaddr_t va;
	vsize_t size;
d2123 4
a2126 1
			*pte = PG_NV;
d2128 2
a2129 3
#if defined(MULTIPROCESSOR) && 0
			pmap_tlb_shootdown(pmap, va, PG_ASM);
#endif
d2148 1
a2148 3
pmap_change_wiring(pmap, va, wired)
	pmap_t		pmap;
	vaddr_t		va;
d2196 1
a2196 3
pmap_extract(pmap, va)
	pmap_t	pmap;
	vaddr_t va;
d2243 2
a2244 6
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t		dst_pmap;
	pmap_t		src_pmap;
	vaddr_t		dst_addr;
	vsize_t		len;
	vaddr_t		src_addr;
d2261 1
a2261 1
pmap_update()
d2285 1
a2285 2
pmap_collect(pmap)
	pmap_t		pmap;
d2294 8
d2307 1
a2307 1
	pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
d2321 1
a2321 2
pmap_activate(p)
	struct proc *p;
d2367 1
a2367 2
pmap_deactivate(p)
	struct proc *p;
d2386 2
a2387 2
 *	into virtual memory and using bzero to clear its contents, one
 *	machine dependent page at a time.
d2392 1
a2392 2
pmap_zero_page(phys)
	paddr_t phys;
d2441 1
a2441 1
 *	into virtual memory and using bcopy to copy the page, one machine
d2447 1
a2447 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d2457 1
a2457 1
	bcopy(s, d, PAGE_SIZE);
d2475 1
a2475 2
pmap_clear_modify(pg)
	struct vm_page *pg;
d2510 1
a2510 2
pmap_clear_reference(pg)
	struct vm_page *pg;
d2546 1
a2546 2
pmap_is_referenced(pg)
	struct vm_page *pg;
d2569 1
a2569 2
pmap_is_modified(pg)
	struct vm_page *pg;
d2595 1
a2595 2
pmap_phys_address(ppn)
	int ppn;
d2613 1
a2613 1
alpha_protection_init()
d2672 2
a2673 2
 *	Returns TRUE or FALSE, indicating if the I-stream needs to
 *	be synchronized.
d2676 2
a2677 7
pmap_remove_mapping(pmap, va, pte, dolock, cpu_id, prmt)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
	boolean_t dolock;
	long cpu_id;
	struct prm_thief *prmt;
d2683 1
a2683 1
	boolean_t needisync;
d2720 15
a2734 1
	needisync = isactive && (pmap_pte_exec(pte) != 0);
d2750 1
a2750 1
	*pte = PG_NV;
d2753 1
a2753 3
#if defined(MULTIPROCESSOR) && 0
	pmap_tlb_shootdown(pmap, va, hadasm ? PG_ASM : 0);
#endif
d2796 3
d2801 1
a2801 4
pmap_changebit(pa, set, mask, cpu_id)
	paddr_t pa;
	u_long set, mask;
	long cpu_id;
d2808 1
a2808 1
	boolean_t needisync = FALSE;
d2842 19
a2860 2
			needisync |= (isactive && (pmap_pte_exec(pte) != 0));
			*pte = npte;
d2863 1
a2863 2
#if defined(MULTIPROCESSOR) && 0
			pmap_tlb_shootdown(pv->pv_pmap, va,
a2864 1
#endif
d2869 2
a2870 6
	if (needisync) {
		alpha_pal_imb();
#if defined(MULTIPROCESSOR) && 0
		alpha_broadcast_ipi(ALPHA_IPI_IMB);
#endif
	}
d2879 1
a2879 5
pmap_emulate_reference(p, v, user, write)
	struct proc *p;
	vaddr_t v;
	int user;
	int write;
d2997 1
a2997 2
pmap_pv_dump(pa)
	paddr_t pa;
d3034 1
a3034 2
vtophys(vaddr)
	vaddr_t vaddr;
d3064 2
a3065 6
pmap_pv_enter(pmap, pa, va, pte, dolock)
	pmap_t pmap;
	paddr_t pa;
	vaddr_t va;
	pt_entry_t *pte;
	boolean_t dolock;
d3117 2
a3118 6
pmap_pv_remove(pmap, pa, va, dolock, pvp)
	pmap_t pmap;
	paddr_t pa;
	vaddr_t va;
	boolean_t dolock;
	struct pv_entry **pvp;
d3163 1
a3163 1
pmap_pv_alloc()
a3208 5
				/*
				 * XXX We know we're not going to try and
				 * XXX lock the kernel pmap, so we don't
				 * XXX have to block interrupts here.
				 */
d3228 1
a3228 1
					alpha_pal_imb();
d3248 1
a3248 2
pmap_pv_free(pv)
	struct pv_entry *pv;
d3260 1
a3260 3
pmap_pv_page_alloc(size, flags, mtype)
	u_long size;
	int flags, mtype;
d3275 1
a3275 4
pmap_pv_page_free(v, size, mtype)
	void *v;
	u_long size;
	int mtype;
d3290 1
a3290 3
pmap_physpage_alloc(usage, pap)
	int usage;
	paddr_t *pap;
d3330 1
a3330 2
pmap_physpage_free(pa)
	paddr_t pa;
d3359 1
a3359 2
pmap_physpage_addref(kva)
	void *kva;
d3386 1
a3386 2
pmap_physpage_delref(kva)
	void *kva;
d3533 1
a3533 3
pmap_lev1map_create(pmap, cpu_id)
	pmap_t pmap;
	long cpu_id;
d3593 1
a3593 3
pmap_lev1map_destroy(pmap, cpu_id)
	pmap_t pmap;
	long cpu_id;
d3646 1
a3646 4
pmap_ptpage_alloc(pmap, pte, usage)
	pmap_t pmap;
	pt_entry_t *pte;
	int usage;
d3665 1
a3665 1
	*pte = ((ptpa >> PGSHIFT) << PG_SHIFT) | \
d3667 1
a3667 1
	    (pmap == pmap_kernel() ? PG_ASM : 0);
d3681 1
a3681 4
pmap_ptpage_free(pmap, pte, ptp)
	pmap_t pmap;
	pt_entry_t *pte;
	pt_entry_t **ptp;
d3690 1
a3690 1
	*pte = PG_NV;
d3713 1
a3713 4
pmap_ptpage_steal(pmap, usage, pap)
	pmap_t pmap;
	int usage;
	paddr_t *pap;
d3759 2
a3760 1
		for (l1idx = 0; l1idx < NPTEPG; l1idx++) {
d3821 3
a3825 7
		if (needisync) {
			alpha_pal_imb();
#if defined(MULTIPROCESSOR) && 0
			alpha_broadcast_ipi(ALPHA_IPI_IMB);
#endif
		}

d3856 2
a3857 6
pmap_l3pt_delref(pmap, va, l3pte, cpu_id, ptp)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *l3pte;
	long cpu_id;
	pt_entry_t **ptp;
d3892 1
a3892 2
#if defined(MULTIPROCESSOR) && 0
		pmap_tlb_shootdown(pmap,
a3893 1
#endif
d3912 2
a3913 4
pmap_l2pt_delref(pmap, l1pte, l2pte, cpu_id)
	pmap_t pmap;
	pt_entry_t *l1pte, *l2pte;
	long cpu_id;
d3951 1
a3951 4
pmap_l1pt_delref(pmap, l1pte, cpu_id)
	pmap_t pmap;
	pt_entry_t *l1pte;
	long cpu_id;
d3978 1
a3978 3
pmap_asn_alloc(pmap, cpu_id)
	pmap_t pmap;
	long cpu_id;
a4096 1
#if 0	/* XXX Not sure if this is safe yet.  --thorpej */
a4101 1
#endif
d4113 1
a4113 4
pmap_tlb_shootdown(pmap, va, pte)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t pte;
a4119 2
	s = splimp();

d4121 1
a4121 1
		if (i == cpu_id || cpu_info[i].ci_dev == NULL)
d4123 1
d4125 3
a4127 1
		simple_lock(&pq->pq_slock);
a4138 2
			if (pq->pq_pte & PG_EXEC)
				ipinum |= ALPHA_IPI_IMB;
a4139 6

			/*
			 * Since we've nailed the whole thing, drain the
			 * job entries pending for that processor.
			 */
			pmap_tlb_shootdown_q_drain(pq);
d4145 1
a4145 1
			alpha_send_ipi(i, ALPHA_IPI_SHOOTDOWN);
d4147 4
a4150 1
		simple_unlock(&pq->pq_slock);
a4151 2

	splx(s);
d4160 1
a4160 1
pmap_do_tlb_shootdown()
d4168 1
a4168 3
	s = splimp();

	simple_lock(&pq->pq_slock);
a4176 3

	if (pq->pq_pte & PG_EXEC)
		alpha_pal_imb();
d4179 1
a4179 3
	simple_unlock(&pq->pq_slock);

	splx(s);
a4187 2
 *
 *	Note: We expect the queue to be locked.
d4190 1
a4190 2
pmap_tlb_shootdown_q_drain(pq)
	struct pmap_tlb_shootdown_q *pq;
d4192 6
a4197 1
	struct pmap_tlb_shootdown_job *pj;
d4199 7
a4205 3
	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		pmap_tlb_shootdown_job_put(pq, pj);
d4207 3
a4209 1
	pq->pq_pte = 0;
d4221 1
a4221 2
pmap_tlb_shootdown_job_get(pq)
	struct pmap_tlb_shootdown_q *pq;
d4225 1
a4225 1
	if (pq->pq_count == PMAP_TLB_SHOOTDOWN_MAXJOBS)
d4241 2
a4242 3
pmap_tlb_shootdown_job_put(pq, pj)
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
@


1.9
log
@typo
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.8 2000/11/08 19:16:59 ericj Exp $ */
d382 3
d395 1
d883 3
d899 1
a899 1
	virtual_end = VM_MIN_KERNEL_ADDRESS + lev3mapsize * NBPG;
d1098 1
a1098 1
			*vendp = trunc_page(virtual_end);
a3399 2
 *
 *	XXX Implement XXX
d3401 101
@


1.8
log
@add tags
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d395 1
a395 1
#define sinlockmgr(lock, flags, slock) lockmgr(lock, flags, slock, curproc)
@


1.7
log
@Merge in big portions of the improvements NetBSD did to their alpha port.
Highlights: UVM, PMAP_NEW, bus_dma (only on some buses for now), new hardware
support, possiblity for ELF, etc, etc. Too much to mention.

This is still work in progress. video consoles might be broken, otherwise
we have basically the same functionality as before plus more.
@
text
@d1 1
@


1.6
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 2
/*	$OpenBSD: pmap.c,v 1.5 1996/10/30 22:38:20 niklas Exp $	*/
/*	$NetBSD: pmap.c,v 1.17 1996/10/13 02:59:42 christos Exp $	*/
d3 74
a76 23
/*
 * Copyright (c) 1992, 1996 Carnegie Mellon University
 * All Rights Reserved.
 * 
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR
 * ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 * 
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 * 
 * any improvements or extensions that they make and grant Carnegie Mellon
 * the rights to redistribute these changes.
d80 26
a105 7
 *	File:	pmap.c
 *
 *	Author list
 *	vax:  Avadis Tevanian, Jr., Michael Wayne Young
 *	i386: Lance Berc, Mike Kupfer, Bob Baron, David Golub, Richard Draves
 *	alpha: Alessandro Forin
 *	{Net,Open}BSD/Alpha: Chris Demetriou
d107 1
a107 1
 *	Physical Map management code for DEC Alpha
d109 18
a126 1
 *	Manages physical address maps.
d128 1
a128 4
 *	This code was derived exclusively from information available in
 *	"Alpha Architecture Reference Manual", Richard L. Sites ed.
 *	Digital Press, Burlington, MA 01803
 *	ISBN 1-55558-098-X, Order no. EY-L520E-DP
d132 1
a132 6
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
d154 1
a163 1
#include <vm/vm_pageout.h>
d165 3
d169 3
a171 1
#include <machine/alpha_cpu.h>
d173 15
d189 2
a190 21
#define	VM_OBJECT_NULL	NULL
#define	VM_PAGE_NULL	NULL
#define	BYTE_SIZE	NBBY
#define	page_size	PAGE_SIZE
#define	ALPHA_PTE_GLOBAL ALPHA_PTE_ASM
#define	MACRO_BEGIN	do {
#define	MACRO_END	} while (0)
#define	K2SEG_BASE	ALPHA_K1SEG_BASE
#define	integer_t	long
#define	spl_t		int
#define	vm_page_fictitious_addr 0
#define	aligned_block_copy(src, dest, size) bcopy((void *)src, (void *)dest, size)
#define	db_printf	printf
#define	tbia		ALPHA_TBIA
#define	alphacache_Iflush alpha_pal_imb
#define cpu_number()	0
#define	check_simple_locks()
#define	K0SEG_TO_PHYS	ALPHA_K0SEG_TO_PHYS
#define	ISA_K0SEG(v)	(v >= ALPHA_K0SEG_BASE && v <= ALPHA_K0SEG_END)
#ifndef assert
#define	assert(x)
a192 28
vm_offset_t	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
vm_offset_t	mem_size;	/* memory size in bytes */
vm_offset_t	virtual_avail;	/* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */

/* XXX */
struct pv_entry *pmap_alloc_pv __P((void));
void pmap_free_pv __P((struct pv_entry *pv));
vm_page_t vm_page_grab __P((void));

vm_offset_t pmap_resident_extract __P((pmap_t, vm_offset_t));

/* For external use... */
vm_offset_t kvtophys(vm_offset_t virt)
{

	return pmap_resident_extract(kernel_pmap, virt);
}

/* ..but for internal use... */
#define phystokv(a)	ALPHA_PHYS_TO_K0SEG(a)
#define	kvtophys(p)	ALPHA_K0SEG_TO_PHYS((vm_offset_t)p)


/*
 *	Private data structures.
 */
d194 2
a195 2
 *	Map from MI protection codes to MD codes.
 *	Assume that there are three MI protection codes, all using low bits.
d197 2
a198 34
pt_entry_t	user_protection_codes[8];
pt_entry_t	kernel_protection_codes[8];

alpha_protection_init()
{
	register pt_entry_t	*kp, *up, prot;

	kp = kernel_protection_codes;
	up = user_protection_codes;
	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE:
			*kp++ = 0;
			*up++ = 0;
			break;
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_EXECUTE:
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE:
			*kp++ = ALPHA_PTE_KR;
			*up++ = ALPHA_PTE_UR|ALPHA_PTE_KR;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			*kp++ = ALPHA_PTE_KW;
			*up++ = ALPHA_PTE_UW|ALPHA_PTE_KW;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			*kp++ = ALPHA_PTE_KW|ALPHA_PTE_KR;
			*up++ = ALPHA_PTE_UW|ALPHA_PTE_UR|ALPHA_PTE_KW|ALPHA_PTE_KR;
			break;
		}
	}
}
d201 22
a222 2
 *	Given a map and a machine independent protection code,
 *	convert to a alpha protection code.
d224 1
a224 18

#define	alpha_protection(map, prot) \
	(((map) == kernel_pmap) ? kernel_protection_codes[prot] : \
				  user_protection_codes[prot])

/* Build the typical kernel pte */
#define	pte_ktemplate(t,pa,pr)						\
MACRO_BEGIN								\
	(t) = pa_to_pte(pa) | ALPHA_PTE_VALID | ALPHA_PTE_GLOBAL |	\
	  (alpha_protection(kernel_pmap,pr));				\
MACRO_END

/* build the typical pte */
#define	pte_template(m,t,pa,pr)						\
MACRO_BEGIN								\
	(t) = pa_to_pte(pa) | ALPHA_PTE_VALID |				\
	  (alpha_protection(m,pr));					\
MACRO_END
d227 1
a227 3
 *	For each vm_page_t, there is a list of all currently
 *	valid virtual mappings of that page.  An entry is
 *	a pv_entry_t; the list is the pv_table.
d229 1
d231 3
a233 5
typedef struct pv_entry {
	struct pv_entry	*next;		/* next pv_entry */
	pmap_t		pmap;		/* pmap where mapping lies */
	vm_offset_t	va;		/* virtual address for mapping */
} *pv_entry_t;
d235 4
a238 1
#define PV_ENTRY_NULL	((pv_entry_t) 0)
d240 1
a240 1
pv_entry_t	pv_head_table;		/* array of entries, one per page */
d242 1
a242 26
/*
 *	pv_list entries are kept on a list that can only be accessed
 *	with the pmap system locked (at SPLVM, not in the cpus_active set).
 *	The list is refilled from the pv_list_zone if it becomes empty.
 */
pv_entry_t	pv_free_list;		/* free list at SPLVM */
decl_simple_lock_data(, pv_free_list_lock)

#define	PV_ALLOC(pv_e) { \
	simple_lock(&pv_free_list_lock); \
	if ((pv_e = pv_free_list) != 0) { \
	    pv_free_list = pv_e->next; \
	} \
	simple_unlock(&pv_free_list_lock); \
}

#define	PV_FREE(pv_e) { \
	simple_lock(&pv_free_list_lock); \
	pv_e->next = pv_free_list; \
	pv_free_list = pv_e; \
	simple_unlock(&pv_free_list_lock); \
}

#if 0
zone_t		pv_list_zone;		/* zone of pv_entry structures */
#endif
d245 3
a247 3
 *	Each entry in the pv_head_table is locked by a bit in the
 *	pv_lock_table.  The lock bits are accessed by the physical
 *	address of the page they lock.
d249 1
a249 3

char	*pv_lock_table;		/* pointer to array of bits */
#define pv_lock_table_size(n)	(((n)+BYTE_SIZE-1)/BYTE_SIZE)
d252 1
a252 3
 *	First and last physical addresses that we maintain any information
 *	for.  Initialized to zero so that pmap operations done before
 *	pmap_init won't touch any non-existent structures.
d254 2
a255 3
vm_offset_t	vm_first_phys = (vm_offset_t) 0;
vm_offset_t	vm_last_phys  = (vm_offset_t) 0;
boolean_t	pmap_initialized = FALSE;/* Has pmap_init completed? */
d257 4
a260 10
/*
 *	Index into pv_head table, its lock bits, and the modify/reference
 *	bits starting at vm_first_phys.
 */

#define pa_index(pa)	(atop(pa - vm_first_phys))

#define pai_to_pvh(pai)		(&pv_head_table[pai])
#define lock_pvh_pai(pai)	(bit_lock(pai, pv_lock_table))
#define unlock_pvh_pai(pai)	(bit_unlock(pai, pv_lock_table))
d263 3
a265 2
 *	Array of physical page attributes for managed pages.
 *	One byte per physical page.
d267 1
a267 1
char	*pmap_phys_attributes;
d270 1
a270 1
 *	Physical page attributes.  Copy bits from PTE.
d272 4
a275 4
#define	PHYS_MODIFIED	(ALPHA_PTE_MOD>>16)	/* page modified */
#define	PHYS_REFERENCED	(ALPHA_PTE_REF>>16)	/* page referenced */

#define	pte_get_attributes(p)	((*p & (ALPHA_PTE_MOD|ALPHA_PTE_REF)) >> 16)
d278 1
a278 2
 *	Amount of virtual memory mapped by one
 *	page-directory entry.
d280 1
a280 3
#define	PDE_MAPPED_SIZE		(pdetova(1))
#define	PDE2_MAPPED_SIZE	(pde2tova(1))
#define	PDE3_MAPPED_SIZE	(pde3tova(1))
d283 59
a341 4
 *	We allocate page table pages directly from the VM system
 *	through this object.  It maps physical memory.
 */
vm_object_t	pmap_object = VM_OBJECT_NULL;
d344 21
a364 5
 *	Locking and TLB invalidation
 */

/*
 *	Locking Protocols:
d366 7
a372 10
 *	There are two structures in the pmap module that need locking:
 *	the pmaps themselves, and the per-page pv_lists (which are locked
 *	by locking the pv_lock_table entry that corresponds to the pv_head
 *	for the list in question.)  Most routines want to lock a pmap and
 *	then do operations in it that require pv_list locking -- however
 *	pmap_remove_all and pmap_copy_on_write operate on a physical page
 *	basis and want to do the locking in the reverse order, i.e. lock
 *	a pv_list and then go through all the pmaps referenced by that list.
 *	To protect against deadlock between these two cases, the pmap_lock
 *	is used.  There are three different locking protocols as a result:
d374 2
a375 2
 *  1.  pmap operations only (pmap_extract, pmap_access, ...)  Lock only
 *		the pmap.
d377 3
a379 4
 *  2.  pmap-based operations (pmap_enter, pmap_remove, ...)  Get a read
 *		lock on the pmap_lock (shared read), then lock the pmap
 *		and finally the pv_lists as needed [i.e. pmap lock before
 *		pv_list lock.]
d381 3
a383 4
 *  3.  pv_list-based operations (pmap_remove_all, pmap_copy_on_write, ...)
 *		Get a write lock on the pmap_lock (exclusive write); this
 *		also guaranteees exclusive access to the pv_lists.  Lock the
 *		pmaps as needed.
d385 27
a411 5
 *	At no time may any routine hold more than one pmap lock or more than
 *	one pv_list lock.  Because interrupt level routines can allocate
 *	mbufs and cause pmap_enter's, the pmap_lock and the lock on the
 *	kernel_pmap can only be held at splvm.
 */
d413 1
a413 1
#if	NCPUS > 1
d415 13
a427 3
 *	We raise the interrupt level to splvm, to block interprocessor
 *	interrupts during pmap operations.  We must take the CPU out of
 *	the cpus_active set while interrupts are blocked.
d429 6
a434 4
#define SPLVM(spl)	{ \
	spl = splvm(); \
	i_bit_clear(cpu_number(), &cpus_active); \
}
d436 6
a441 4
#define SPLX(spl)	{ \
	i_bit_set(cpu_number(), &cpus_active); \
	splx(spl); \
}
d443 2
a444 4
/*
 *	Lock on pmap system
 */
lock_data_t	pmap_system_lock;
d446 1
a446 1
volatile boolean_t	cpu_update_needed[NCPUS];
d448 6
a453 5
#define PMAP_READ_LOCK(pmap, spl) { \
	SPLVM(spl); \
	lock_read(&pmap_system_lock); \
	simple_lock(&(pmap)->lock); \
}
d455 1
a455 4
#define PMAP_WRITE_LOCK(spl) { \
	SPLVM(spl); \
	lock_write(&pmap_system_lock); \
}
d457 1
a457 5
#define PMAP_READ_UNLOCK(pmap, spl) { \
	simple_unlock(&(pmap)->lock); \
	lock_read_done(&pmap_system_lock); \
	SPLX(spl); \
}
d459 5
a463 4
#define PMAP_WRITE_UNLOCK(spl) { \
	lock_write_done(&pmap_system_lock); \
	SPLX(spl); \
}
d465 2
a466 3
#define PMAP_WRITE_TO_READ_LOCK(pmap) { \
	simple_lock(&(pmap)->lock); \
	lock_write_to_read(&pmap_system_lock); \
d469 9
a477 1
#define LOCK_PVH(index)		(lock_pvh_pai(index))
d479 2
a480 1
#define UNLOCK_PVH(index)	(unlock_pvh_pai(index))
d482 34
a515 23
#define PMAP_UPDATE_TLBS(pmap, s, e) \
{ \
	cpu_set	cpu_mask = 1 << cpu_number(); \
	cpu_set	users; \
 \
	/* Since the pmap is locked, other updates are locked */ \
	/* out, and any pmap_activate has finished. */ \
 \
	/* find other cpus using the pmap */ \
	users = (pmap)->cpus_using & ~cpu_mask; \
	if (users) { \
	    /* signal them, and wait for them to finish */ \
	    /* using the pmap */ \
	    signal_cpus(users, (pmap), (s), (e)); \
	    while ((pmap)->cpus_using & cpus_active & ~cpu_mask) \
		continue; \
	} \
 \
	/* invalidate our own TLB if pmap is in use */ \
	if ((pmap)->cpus_using & cpu_mask) { \
	    INVALIDATE_TLB((s), (e)); \
	} \
}
d517 4
a520 1
#else	NCPUS > 1
d522 7
a528 2
#define SPLVM(spl)
#define SPLX(spl)
d530 7
a536 8
#define PMAP_READ_LOCK(pmap, spl)	SPLVM(spl)
#define PMAP_WRITE_LOCK(spl)		SPLVM(spl)
#define PMAP_READ_UNLOCK(pmap, spl)	SPLX(spl)
#define PMAP_WRITE_UNLOCK(spl)		SPLX(spl)
#define PMAP_WRITE_TO_READ_LOCK(pmap)

#define LOCK_PVH(index)
#define UNLOCK_PVH(index)
d538 15
a552 7
#if 0 /*fix bug later */
#define PMAP_UPDATE_TLBS(pmap, s, e) { \
	/* invalidate our own TLB if pmap is in use */ \
	if ((pmap)->cpus_using) { \
	    INVALIDATE_TLB((s), (e)); \
	} \
}
d554 2
a555 4
#define PMAP_UPDATE_TLBS(pmap, s, e) { \
	    INVALIDATE_TLB((s), (e)); \
}
#endif
d557 44
a600 9
#endif	/* NCPUS > 1 */

#if 0
#define INVALIDATE_TLB(s, e) { \
	register vm_offset_t	v = s, ve = e; \
	while (v < ve) { \
	    tbis(v); v += ALPHA_PGBYTES; \
	} \
}
d602 1
a602 3
#define INVALIDATE_TLB(s, e) { \
	tbia(); \
}
d605 26
a630 4

#if	NCPUS > 1

void pmap_update_interrupt();
d633 7
a639 1
 *	Structures to keep track of pending TLB invalidations
d641 1
d643 1
a643 1
#define UPDATE_LIST_SIZE	4
d645 7
a651 7
struct pmap_update_item {
	pmap_t		pmap;		/* pmap to invalidate */
	vm_offset_t	start;		/* start address to invalidate */
	vm_offset_t	end;		/* end address to invalidate */
} ;

typedef	struct pmap_update_item	*pmap_update_item_t;
d654 11
a664 2
 *	List of pmap updates.  If the list overflows,
 *	the last entry is changed to invalidate all.
d666 4
a669 10
struct pmap_update_list {
	decl_simple_lock_data(,	lock)
	int			count;
	struct pmap_update_item	item[UPDATE_LIST_SIZE];
} ;
typedef	struct pmap_update_list	*pmap_update_list_t;

struct pmap_update_list	cpu_update_list[NCPUS];

#endif	/* NCPUS > 1 */
d672 3
a674 1
 *	Other useful macros.
d676 24
a699 2
#define current_pmap()		(vm_map_pmap(current_thread()->task->map))
#define pmap_in_use(pmap, cpu)	(((pmap)->cpus_using & (1 << (cpu))) != 0)
d701 28
a728 52
struct pmap	kernel_pmap_store;
pmap_t		kernel_pmap;

struct zone	*pmap_zone;		/* zone of pmap structures */

int		pmap_debug = 0;		/* flag for debugging prints */
int		ptes_per_vm_page;	/* number of hardware ptes needed
					   to map one VM page. */
unsigned int	inuse_ptepages_count = 0;	/* debugging */

extern char end;
/*
 * Page directory for kernel.
 */
pt_entry_t	*root_kpdes;

void pmap_remove_range();	/* forward */
#if	NCPUS > 1
void signal_cpus();		/* forward */
#endif	/* NCPUS > 1 */

int	pmap_max_asn;
void	pmap_expand __P((pmap_t, vm_offset_t));

/* XXX */
#define	PDB_BOOTSTRAP		0x00000001
#define	PDB_BOOTSTRAP_ALLOC	0x00000002
#define	PDB_UNMAP_PROM		0x00000004
#define	PDB_ACTIVATE		0x00000008
#define	PDB_DEACTIVATE		0x00000010
#define	PDB_TLBPID_INIT		0x00000020
#define	PDB_TLBPID_ASSIGN	0x00000040
#define	PDB_TLBPID_DESTROY	0x00000080
#define	PDB_ENTER		0x00000100
#define	PDB_CREATE		0x00000200
#define	PDB_PINIT		0x00000400
#define	PDB_EXPAND		0x00000800
#define	PDB_EXTRACT		0x00001000
#define	PDB_PTE			0x00002000
#define	PDB_RELEASE		0x00004000
#define	PDB_DESTROY		0x00008000
#define	PDB_COPY_PAGE		0x00010000
#define	PDB_ZERO_PAGE		0x00020000

#define	PDB_ANOMALOUS		0x20000000
#define	PDB_FOLLOW		0x40000000
#define PDB_VERBOSE		0x80000000

int pmapdebug = PDB_ANOMALOUS  |-1 /* -1 */;

#if defined(DEBUG) || 1
#define	DOPDB(x)	((pmapdebug & (x)) != 0)
d730 1
a730 1
#define	DOPDB(x)	0
a731 1
#define	DOVPDB(x)	(DOPDB(x) && DOPDB(PDB_VERBOSE))
d734 1
a734 3
 *	Given an offset and a map, compute the address of the
 *	pte.  If the address is invalid with respect to the map
 *	then PT_ENTRY_NULL is returned (and the map may need to grow).
d736 1
a736 1
 *	This is only used internally.
d738 2
a739 52
#define	pmap_pde(pmap, addr) (&(pmap)->dirbase[pdenum(addr)])

pt_entry_t *pmap_pte(pmap, addr)
	register pmap_t		pmap;
	register vm_offset_t	addr;
{
	register pt_entry_t	*ptp, *ptep;
	register pt_entry_t	pte;

	if (DOPDB(PDB_FOLLOW|PDB_PTE))
		printf("pmap_pte(%p, 0x%lx)\n", pmap, addr);

	if (pmap->dirbase == 0) {
		if (DOVPDB(PDB_FOLLOW|PDB_PTE))
			printf("pmap_pte: dirbase == 0\n");
		ptep = PT_ENTRY_NULL;
		goto out;
	}

	/* seg1 */
	pte = *pmap_pde(pmap,addr);
	if ((pte & ALPHA_PTE_VALID) == 0) {
		if (DOVPDB(PDB_FOLLOW|PDB_PTE))
			printf("pmap_pte: l1 not valid\n");
		ptep = PT_ENTRY_NULL;
		goto out;
	}

	/* seg2 */
	ptp = (pt_entry_t *)ptetokv(pte);
	pte = ptp[pte2num(addr)];
	if ((pte & ALPHA_PTE_VALID) == 0) {
		if (DOVPDB(PDB_FOLLOW|PDB_PTE))
			printf("pmap_pte: l2 not valid\n");
		ptep = PT_ENTRY_NULL;
		goto out;
	}

	/* seg3 */
	ptp = (pt_entry_t *)ptetokv(pte);
	ptep = &ptp[pte3num(addr)];

out:
	if (DOPDB(PDB_FOLLOW|PDB_PTE))
		printf("pmap_pte: returns %p\n", ptep);
	return (ptep);
}

#define DEBUG_PTE_PAGE	1

extern	vm_offset_t	virtual_avail, virtual_end;
extern	vm_offset_t	avail_start, avail_end;
d742 1
a742 3
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, and allocate the system page table.
 *	Called with mapping OFF.  Page_size must already be set.
d744 1
a744 5
 *	Parameters:
 *	avail_start	PA of first available physical page
 *	avail_end	PA of last available physical page
 *	virtual_avail	VA of first available page
 *	virtual_end	VA of last available page
d746 1
a747 8
vm_size_t	pmap_kernel_vm = 5;	/* each one 8 meg worth */

unsigned int
pmap_free_pages()
{
	return atop(avail_end - avail_start);
}

d749 8
a756 9
pmap_bootstrap(firstaddr, ptaddr, maxasn)
	vm_offset_t firstaddr, ptaddr;
	int maxasn;
{
	vm_offset_t	pa;
	pt_entry_t	template;
	pt_entry_t	*pde, *pte;
	vm_offset_t start;
        extern int firstusablepage, lastusablepage;
a757 1
	long npages;
d759 4
a762 3
        if (DOPDB(PDB_FOLLOW|PDB_BOOTSTRAP))
                printf("pmap_bootstrap(0x%lx, 0x%lx, %d)\n", firstaddr, ptaddr,
		    maxasn);
d764 9
a772 2
	/* must be page aligned */
	start = firstaddr = alpha_round_page(firstaddr);
d774 4
a777 6
#define valloc(name, type, num)						\
	    (name) = (type *)firstaddr;					\
	    firstaddr = ALIGN((vm_offset_t)((name)+(num)))
#define vallocsz(name, cast, size)					\
	    (name) = (cast)firstaddr;					\
	    firstaddr = ALIGN(firstaddr + size)
d780 3
a782 1
	 *	Initialize protection array.
d784 2
a785 1
	alpha_protection_init();
d788 3
a790 1
	 *	Set ptes_per_vm_page for general use.
d792 3
a794 1
	ptes_per_vm_page = page_size / ALPHA_PGBYTES;
d797 2
a798 3
	 *	The kernel's pmap is statically allocated so we don't
	 *	have to use pmap_create, which is unlikely to work
	 *	correctly at this part of the boot sequence.
d800 2
a801 10

	kernel_pmap = &kernel_pmap_store;

#if	NCPUS > 1
	lock_init(&pmap_system_lock, FALSE);	/* NOT a sleep lock */
#endif	/* NCPUS > 1 */

	simple_lock_init(&kernel_pmap->lock);

	kernel_pmap->ref_count = 1;
d804 2
a805 2
	 *	Allocate the kernel page directory, and put its
	 *	virtual address in root_kpdes.
d807 4
a810 1
	 *	No other physical memory has been allocated.
d812 4
a816 26
	vallocsz(root_kpdes, pt_entry_t *, PAGE_SIZE);
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: root_kpdes = %p\n", root_kpdes);
	kernel_pmap->dirbase = root_kpdes;
	kernel_pmap->dirpfn = alpha_btop(kvtophys((vm_offset_t)root_kpdes));

        /* First, copy mappings for things below VM_MIN_KERNEL_ADDRESS */
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: setting up root_kpdes (copy 0x%lx)\n",
		    pdenum(VM_MIN_KERNEL_ADDRESS) * sizeof root_kpdes[0]);
	bzero(root_kpdes, PAGE_SIZE);
        bcopy((caddr_t)ptaddr, root_kpdes,
            pdenum(VM_MIN_KERNEL_ADDRESS) * sizeof root_kpdes[0]);

	/*
	 *	Set up the virtual page table.
	 */
	pte_ktemplate(template, kvtophys(root_kpdes),
	    VM_PROT_READ | VM_PROT_WRITE);
	template &= ~ALPHA_PTE_GLOBAL;
	root_kpdes[pdenum(VPTBASE)] = template;
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: VPT PTE 0x%lx at 0x%lx)\n",
		    root_kpdes[pdenum(VPTBASE)], &root_kpdes[pdenum(VPTBASE)]);

#if 0
d818 1
a818 1
	 *	Rid of console's default mappings
d820 4
a823 3
	for (pde = pmap_pde(kernel_pmap,0);
	     pde < pmap_pde(kernel_pmap,VM_MIN_KERNEL_ADDRESS);)
		*pde++ = 0;
a824 1
#endif
d826 1
a826 4
	 *	Allocate the seg2 kernel page table entries from the front
	 *	of available physical memory.  Take enough to cover all of
	 *	the K2SEG range. But of course one page is enough for 8Gb,
	 *	and more in future chips ...
a827 1
#define	enough_kseg2()	(PAGE_SIZE)
d829 8
a836 7
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: allocating kvseg segment pages\n");
	vallocsz(pte, pt_entry_t *, enough_kseg2());		/* virtual */
	pa  = kvtophys(pte);					/* physical */
	bzero(pte, enough_kseg2());
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: kvseg segment pages at %p\n", pte);
d838 6
a843 1
#undef	enough_kseg2
d845 4
a848 3
	/*
	 *	Make a note of it in the seg1 table
	 */
d850 8
a857 11
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: inserting segment pages into root\n");
	tbia();
	pte_ktemplate(template,pa,VM_PROT_READ|VM_PROT_WRITE);
	pde = pmap_pde(kernel_pmap,K2SEG_BASE);
	i = ptes_per_vm_page;
	do {
	    *pde++ = template;
	    pte_increment_pa(template);
	    i--;
	} while (i > 0);
d860 2
a861 3
	 *	The kernel runs unmapped and cached (k0seg),
	 *	only dynamic data are mapped in k1seg.
	 *	==> No need to map it.
d863 2
d867 1
a867 1
	 *	But don't we need some seg2 pagetables to start with ?
d869 7
a875 15
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: allocating kvseg page table pages\n");
	pde = &pte[pte2num(K2SEG_BASE)];
	for (i = pmap_kernel_vm; i > 0; i--) {
	    register int j;

	    vallocsz(pte, pt_entry_t *, PAGE_SIZE);		/* virtual */
	    pa  = kvtophys(pte);				/* physical */
	    pte_ktemplate(template,pa,VM_PROT_READ|VM_PROT_WRITE);
	    bzero(pte, PAGE_SIZE);
	    j = ptes_per_vm_page;
	    do {
		*pde++ = template;
	        pte_increment_pa(template);
	    } while (--j > 0);
d879 1
a879 1
	 *	Fix up managed physical memory information.
d881 1
a881 6
	avail_start = ALPHA_K0SEG_TO_PHYS(firstaddr);
	avail_end = alpha_ptob(lastusablepage + 1);
	mem_size = avail_end - avail_start;
	if (DOVPDB(PDB_BOOTSTRAP))
		printf("pmap_bootstrap: avail: 0x%lx -> 0x%lx (0x%lx)\n",
		    avail_start, avail_end, mem_size);
d884 3
a886 2
	 *	Allocate memory for the pv_head_table and its
	 *	lock bits, and the reference/modify byte array.
d888 4
a891 2
	if (DOVPDB(PDB_BOOTSTRAP))
		printf("pmap_bootstrap: allocating page management data\n");
d893 6
a898 2
	npages = ((BYTE_SIZE * mem_size) /
	          (BYTE_SIZE * (PAGE_SIZE + sizeof (struct pv_entry) + 1) + 1));
d900 14
a913 2
	valloc(pv_head_table, struct pv_entry, npages);
	bzero(pv_head_table, sizeof (struct pv_entry) * npages);
d915 1
a915 2
	valloc(pv_lock_table, char, pv_lock_table_size(npages));
	bzero(pv_lock_table, pv_lock_table_size(npages));
d917 8
a924 2
	valloc(pmap_phys_attributes, char, npages);
	bzero(pmap_phys_attributes, sizeof (char) * npages);
d926 5
a930 7
	avail_start = alpha_round_page(ALPHA_K0SEG_TO_PHYS(firstaddr));
	if (npages > pmap_free_pages())
		panic("pmap_bootstrap");
	mem_size = avail_end - avail_start;
	if (DOVPDB(PDB_BOOTSTRAP))
		printf("pmap_bootstrap: avail: 0x%lx -> 0x%lx (0x%lx)\n",
		    avail_start, avail_end, mem_size);
d933 5
a937 1
	 *	Assert kernel limits (because of pmap_expand).
d939 11
d951 10
a960 5
	virtual_avail = alpha_round_page(K2SEG_BASE);
	virtual_end   = trunc_page(K2SEG_BASE + pde2tova(pmap_kernel_vm));
        if (DOVPDB(PDB_BOOTSTRAP)) {
		printf("pmap_bootstrap: virtual_avail = %p\n", virtual_avail);
		printf("pmap_bootstrap: virtual_end = %p\n", virtual_end);
d962 1
d965 2
a966 3
	 *	The distinguished tlbpid value of 0 is reserved for
	 *	the kernel pmap. Initialize the tlbpid allocator,
	 *	who knows about this.
d968 4
a971 2
	kernel_pmap->pid = 0;
	pmap_tlbpid_init(maxasn);
d973 5
a977 2
        if (DOVPDB(PDB_BOOTSTRAP))
                printf("pmap_bootstrap: leaving\n");
d980 3
a982 1
pmap_rid_of_console()
d984 8
a991 7
	pt_entry_t	*pde;
	/*
	 *	Rid of console's default mappings
	 */
	for (pde = pmap_pde(kernel_pmap,0L);
	     pde < pmap_pde(kernel_pmap,VM_MIN_KERNEL_ADDRESS);)
		*pde++ = 0;
d993 1
d996 13
a1008 6
 * Bootstrap memory allocator. This function allows for early dynamic
 * memory allocation until the virtual memory system has been bootstrapped.
 * After that point, either kmem_alloc or malloc should be used. This
 * function works by stealing pages from the (to be) managed page pool,
 * implicitly mapping them (by using their k0seg addresses),
 * and zeroing them.
d1010 4
a1013 3
 * It should be used from pmap_bootstrap till vm_page_startup, afterwards
 * it cannot be used, and will generate a panic if tried. Note that this
 * memory will never be freed, and in essence it is wired down.
d1015 11
d1027 3
a1029 6
void *
pmap_bootstrap_alloc(size)
	int size;
{
	vm_offset_t val;
	extern boolean_t vm_page_startup_initialized;
d1031 9
a1039 4
	if (DOPDB(PDB_FOLLOW|PDB_BOOTSTRAP_ALLOC))
		printf("pmap_bootstrap_alloc(%lx)\n", size);
	if (vm_page_startup_initialized)
		panic("pmap_bootstrap_alloc: called after startup initialized");
a1040 5
	val = ALPHA_PHYS_TO_K0SEG(avail_start);
	size = alpha_round_page(size);
	avail_start += size;
	if (avail_start > avail_end)			/* sanity */
		panic("pmap_bootstrap_alloc");
d1042 5
a1046 1
	bzero((caddr_t)val, size);
d1048 3
a1050 4
	if (DOVPDB(PDB_BOOTSTRAP_ALLOC))
		printf("pmap_bootstrap_alloc: returns %p\n", val);
	return ((void *)val);
}
d1052 4
a1055 13
/*
 * Unmap the PROM mappings.  PROM mappings are kept around
 * by pmap_bootstrap, so we can still use the prom's printf.
 * Basically, blow away all mappings in the level one PTE
 * table below VM_MIN_KERNEL_ADDRESS.  The Virtual Page Table
 * Is at the end of virtual space, so it's safe.
 */
void
pmap_unmap_prom()
{
	int i;
	extern int prom_mapped;
	extern pt_entry_t *rom_ptep, rom_pte;
d1057 3
a1059 2
	if (DOPDB(PDB_FOLLOW|PDB_UNMAP_PROM))
		printf("pmap_unmap_prom\n");
d1061 6
a1066 15
	/* XXX save old pte so that we can remap prom if necessary */
	rom_ptep = &root_kpdes[0];				/* XXX */
	rom_pte = *rom_ptep & ~ALPHA_PTE_ASM;			/* XXX */

	if (DOVPDB(PDB_UNMAP_PROM))
		printf("pmap_unmap_prom: zero 0x%lx, rom_pte was 0x%lx\n",
		    pdenum(VM_MIN_KERNEL_ADDRESS) * sizeof root_kpdes[0],
		    rom_pte);
	/* Mark all mappings before VM_MIN_KERNEL_ADDRESS as invalid. */
	bzero(root_kpdes, pdenum(VM_MIN_KERNEL_ADDRESS) * sizeof root_kpdes[0]);
	prom_mapped = 0;
	ALPHA_TBIA();
	if (DOVPDB(PDB_UNMAP_PROM))
		printf("pmap_unmap_prom: leaving\n");
}
d1068 14
a1081 11
/*
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
 */
void
pmap_init(phys_start, phys_end)
	vm_offset_t	phys_start, phys_end;
{
	vm_size_t	s;
	int		i;
d1083 8
a1090 17
	/*
	 *	Create the zone of physical maps,
	 *	and of the physical-to-virtual entries.
	 */
#if 0
	s = (vm_size_t) sizeof(struct pmap);
	pmap_zone = zinit(s, 400*s, 4096, FALSE, "pmap"); /* XXX */
	s = (vm_size_t) sizeof(struct pv_entry);
	pv_list_zone = zinit(s, 10000*s, 4096, FALSE, "pv_list"); /* XXX */
#endif

#if	NCPUS > 1
	/*
	 *	Set up the pmap request lists
	 */
	for (i = 0; i < NCPUS; i++) {
	    pmap_update_list_t	up = &cpu_update_list[i];
d1092 4
a1095 2
	    simple_lock_init(&up->lock);
	    up->count = 0;
a1097 4
	alpha_set_scb_entry( SCB_INTERPROC, pmap_update_interrupt);

#endif	/* NCPUS > 1 */

d1099 1
a1099 4
	 *	Only now, when all of the data structures are allocated,
	 *	can we set vm_first_phys and vm_last_phys.  If we set them
	 *	too soon, the kmem_alloc_wired above will try to use these
	 *	data structures and blow up.
d1101 1
a1101 4

	vm_first_phys = phys_start;
	vm_last_phys = phys_end;
	pmap_initialized = TRUE;
a1103 3
#define pmap_valid_page(x) ((avail_start <= x) && (x < avail_end))
#define valid_page(x) (pmap_initialized && pmap_valid_page(x))

d1105 1
a1105 1
 *	Routine:	pmap_page_table_page_alloc
d1107 2
a1108 1
 *	Allocates a new physical page to be used as a page-table page.
d1110 1
a1110 2
 *	Must be called with the pmap system and the pmap unlocked,
 *	since these must be unlocked to use vm_page_grab.
d1112 2
a1113 2
vm_offset_t
pmap_page_table_page_alloc()
d1115 8
a1122 2
	register vm_page_t	m;
	register vm_offset_t	pa;
d1124 2
a1125 1
	check_simple_locks();
d1128 2
a1129 3
	 *	We cannot allocate the pmap_object in pmap_init,
	 *	because it is called before the zone package is up.
	 *	Allocate it now if it is missing.
d1131 6
a1136 2
	if (pmap_object == VM_OBJECT_NULL)
	    pmap_object = vm_object_allocate(mem_size);
d1139 3
a1141 1
	 *	Allocate a VM page
d1143 1
a1143 2
	while ((m = vm_page_grab()) == VM_PAGE_NULL)
		vm_page_wait();
d1146 1
a1146 2
	 *	Map the page to its physical address so that it
	 *	can be found later.
d1148 1
a1148 8
	pa = m->phys_addr;
	vm_object_lock(pmap_object);
	vm_page_insert(m, pmap_object, pa);
	vm_page_lock_queues();
	vm_page_wire(m);
	inuse_ptepages_count++;
	vm_page_unlock_queues();
	vm_object_unlock(pmap_object);
d1150 11
a1160 28
	/*
	 *	Zero the page.
	 */
	bzero((void *)phystokv(pa), PAGE_SIZE);

	return pa;
}

/*
 *	Deallocate a page-table page.
 *	The page-table page must have all mappings removed,
 *	and be removed from its page directory.
 */
void
pmap_page_table_page_dealloc(pa)
	vm_offset_t	pa;
{
	vm_page_t	m;

	vm_object_lock(pmap_object);
	m = vm_page_lookup(pmap_object, pa);
	if (m == VM_PAGE_NULL)
	    panic("pmap_page_table_page_dealloc: page %#X not in object", pa);
	vm_page_lock_queues();
	vm_page_free(m);
	inuse_ptepages_count--;
	vm_page_unlock_queues();
	vm_object_unlock(pmap_object);
d1164 2
d1168 1
a1168 8
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
 *
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
d1171 1
a1171 2
pmap_create(size)
	vm_size_t size;
d1173 10
a1182 1
	register pmap_t p;
d1184 2
a1185 2
	if (DOPDB(PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create(%d)\n", size);
d1188 3
a1190 1
	 *	A software use-only map doesn't even need a map.
d1192 1
d1194 5
a1198 3
	if (size != 0) {
		p = PMAP_NULL;
		goto out;
d1200 1
d1202 3
a1204 7
	/* XXX: is it ok to wait here? */
	p = (pmap_t) malloc(sizeof *p, M_VMPMAP, M_WAITOK);
	if (p == NULL)
		panic("pmap_create: cannot allocate a pmap");

	bzero(p, sizeof (*p));
	pmap_pinit(p);
d1206 1
a1206 4
out:
	if (DOVPDB(PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create: returning %p\n", p);
	return (p);
d1209 6
d1216 2
a1217 2
pmap_pinit(p)
	struct pmap *p;
d1219 1
a1219 5
	register pmap_statistics_t stats;
	extern struct vmspace vmspace0;

	if (DOPDB(PDB_FOLLOW|PDB_PINIT))
		printf("pmap_init(%p)\n", p);
d1221 3
a1223 5
#if 0
	/* XXX cgd WHY NOT pmap_page_table_page_alloc()? */
	p->dirbase = (void *)kmem_alloc(kernel_map, ALPHA_PGBYTES);
#else
	p->dirbase = (void *)phystokv(pmap_page_table_page_alloc());
d1225 2
a1226 13
	if (p->dirbase == NULL)
		panic("pmap_create");
	p->dirpfn = alpha_btop(pmap_resident_extract(kernel_pmap,
	    (vm_offset_t)p->dirbase));

	if (DOVPDB(PDB_FOLLOW|PDB_PINIT))
		printf("pmap_init(%p): dirbase = %p, dirpfn = 0x%x\n", p,
		    p->dirbase, p->dirpfn);
	aligned_block_copy(root_kpdes, p->dirbase, ALPHA_PGBYTES);
	p->ref_count = 1;
	p->pid = -1;
	if (DOVPDB(PDB_FOLLOW|PDB_PINIT))
		printf("pmap_init(%p): first pde = 0x%lx\n", p->dirbase[0]);
d1228 3
a1230 2
	{
		pt_entry_t template;
d1232 2
a1233 10
		pte_ktemplate(template, kvtophys(p->dirbase),
		    VM_PROT_READ | VM_PROT_WRITE);
		template &= ~ALPHA_PTE_GLOBAL;
		p->dirbase[pdenum(VPTBASE)] = template;
	}
printf("PMAP_PINIT: FIRST ENT = 0x%lx\n", p->dirbase[0]);

	simple_lock_init(&p->lock);
	p->cpus_using = 0;
	p->hacking = 0;
d1236 1
a1236 1
	 *	Initialize statistics.
d1238 3
d1242 1
a1242 59
	stats = &p->stats;
	stats->resident_count = 0;
	stats->wired_count = 0;

out:
	if (DOVPDB(PDB_FOLLOW|PDB_PINIT))
		printf("pmap_init: leaving\n", p);
}

/*
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
 */

void pmap_destroy(p)
	register pmap_t	p;
{
	register int		c;
	register spl_t		s;

	if (DOPDB(PDB_FOLLOW|PDB_DESTROY))
		printf("pmap_destroy(%p)\n", p);

	if (p == PMAP_NULL)
		goto out;

	SPLVM(s);
	simple_lock(&p->lock);
	c = --p->ref_count;
	simple_unlock(&p->lock);
	SPLX(s);

	if (c == 0) {
		pmap_release(p);
		free(p, M_VMPMAP);
	}
out:
	if (DOVPDB(PDB_FOLLOW|PDB_DESTROY))
		printf("pmap_destroy: leaving\n");
}

void
pmap_release(p)
	pmap_t p;
{
	register pt_entry_t	*pdep, *ptep, *eptep;
	register vm_offset_t	pa;

	if (DOPDB(PDB_FOLLOW|PDB_RELEASE))
		printf("pmap_release(%p)\n", p);

	if (p->dirbase == NULL) {
		if (DOPDB(PDB_FOLLOW|PDB_ANOMALOUS|PDB_RELEASE))
			printf("pmap_release: already reclaimed\n");
		/* resources already reclaimed */
		goto out;
	}

d1244 2
a1245 2
	 *	Free the memory maps, then the
	 *	pmap structure.
d1247 11
a1257 14
	for (pdep = p->dirbase;
	     pdep < pmap_pde(p,VM_MIN_KERNEL_ADDRESS);
	     pdep += ptes_per_vm_page) {
	    if (*pdep & ALPHA_PTE_VALID) {
		pa = pte_to_pa(*pdep);

		ptep = (pt_entry_t *)phystokv(pa);
		eptep = ptep + NPTES;
		for (; ptep < eptep; ptep += ptes_per_vm_page ) {
		    if (*ptep & ALPHA_PTE_VALID)
			pmap_page_table_page_dealloc(pte_to_pa(*ptep));
		}
		pmap_page_table_page_dealloc(pa);
	    }
a1258 6
	pmap_tlbpid_destroy(p->pid, FALSE);

#if 0
	kmem_free(kernel_map, (vm_offset_t)p->dirbase, ALPHA_PGBYTES);
#else
	pmap_page_table_page_dealloc(kvtophys(p->dirbase));
a1259 1
	p->dirbase = NULL;
d1261 3
a1263 3
out:
	if (DOVPDB(PDB_FOLLOW|PDB_RELEASE))
		printf("pmap_release: leaving\n");
d1267 2
d1271 4
d1276 8
a1283 10
void pmap_reference(p)
	register pmap_t	p;
{
	spl_t	s;
	if (p != PMAP_NULL) {
		SPLVM(s);
		simple_lock(&p->lock);
		p->ref_count++;
		simple_unlock(&p->lock);
		SPLX(s);
d1288 3
a1290 4
 *	Remove a range of hardware page-table entries.
 *	The entries given are the first (inclusive)
 *	and last (exclusive) entries for the VM pages.
 *	The virtual address is the va for the first pte.
d1292 2
a1293 4
 *	The pmap must be locked.
 *	If the pmap is not the kernel pmap, the range must lie
 *	entirely within one pte-page.  This is NOT checked.
 *	Assumes that the pte-page exists.
d1295 15
d1311 2
a1312 11
/* static */
void pmap_remove_range(pmap, va, spte, epte)
	pmap_t			pmap;
	vm_offset_t		va;
	pt_entry_t		*spte;
	pt_entry_t		*epte;
{
	register pt_entry_t	*cpte;
	int			num_removed, num_unwired;
	int			pai;
	vm_offset_t		pa;
d1314 27
a1340 2
	num_removed = 0;
	num_unwired = 0;
d1342 2
a1343 2
	for (cpte = spte; cpte < epte;
	     cpte += ptes_per_vm_page, va += PAGE_SIZE) {
d1345 4
a1348 3
	    if (*cpte == 0)
		continue;
	    pa = pte_to_pa(*cpte);
d1350 5
a1354 3
	    num_removed++;
	    if (*cpte & ALPHA_PTE_WIRED)
		num_unwired++;
d1356 2
a1357 1
	    if (!valid_page(pa)) {
d1359 6
a1364 15
		/*
		 *	Outside range of managed physical memory.
		 *	Just remove the mappings.
		 */
		register int	i = ptes_per_vm_page;
		register pt_entry_t	*lpte = cpte;
		do {
		    *lpte = 0;
		    lpte++;
		} while (--i > 0);
		continue;
	    }

	    pai = pa_index(pa);
	    LOCK_PVH(pai);
d1366 1
a1366 56
	    /*
	     *	Get the modify and reference bits.
	     */
	    {
		register int		i;
		register pt_entry_t	*lpte;

		i = ptes_per_vm_page;
		lpte = cpte;
		do {
		    pmap_phys_attributes[pai] |= pte_get_attributes(lpte);
		    *lpte = 0;
		    lpte++;
		} while (--i > 0);
	    }

	    /*
	     *	Remove the mapping from the pvlist for
	     *	this physical page.
	     */
	    {
		register pv_entry_t	pv_h, prev, cur;

		pv_h = pai_to_pvh(pai);
		if (pv_h->pmap == PMAP_NULL) {
		    panic("pmap_remove: null pv_list!");
		}
		if (pv_h->va == va && pv_h->pmap == pmap) {
		    /*
		     * Header is the pv_entry.  Copy the next one
		     * to header and free the next one (we cannot
		     * free the header)
		     */
		    cur = pv_h->next;
		    if (cur != PV_ENTRY_NULL) {
			*pv_h = *cur;
			PV_FREE(cur);
		    }
		    else {
			pv_h->pmap = PMAP_NULL;
		    }
		}
		else {
		    cur = pv_h;
		    do {
			prev = cur;
			if ((cur = prev->next) == PV_ENTRY_NULL) {
			    panic("pmap-remove: mapping not in pv_list!");
			}
		    } while (cur->va != va || cur->pmap != pmap);
		    prev->next = cur->next;
		    PV_FREE(cur);
		}
		UNLOCK_PVH(pai);
	    }
	}
d1369 2
a1370 1
	 *	Update the counts
d1372 1
a1372 3
	pmap->stats.resident_count -= num_removed;
	pmap->stats.wired_count -= num_unwired;
}
d1374 4
a1377 5
/*
 *	One level up, iterate an operation on the
 *	virtual range va..eva, mapped by the 1st
 *	level pte spte.
 */
d1379 5
a1383 10
/* static */
void pmap_iterate_lev2(pmap, s, e, spte, operation)
	pmap_t			pmap;
	vm_offset_t		s, e;
	pt_entry_t		*spte;
	void			(*operation)();
{
	vm_offset_t		l;
	pt_entry_t		*epte;
	pt_entry_t		*cpte;
d1385 40
a1424 22
if (pmap_debug > 1) db_printf("iterate2(%x,%x,%x)", s, e, spte);
	while (s <  e) {
	    /* at most 1 << 23 virtuals per iteration */
	    l = roundup(s+1,PDE2_MAPPED_SIZE);
	    if (l > e)
	    	l = e;
	    if (*spte & ALPHA_PTE_VALID) {
		register int	n;
		cpte = (pt_entry_t *) ptetokv(*spte);
		n = pte3num(l);
		if (n == 0) n = SEG_MASK + 1;/* l == next segment up */
		epte = &cpte[n];
		cpte = &cpte[pte3num(s)];
		assert(epte >= cpte);
if (pmap_debug > 1) db_printf(" [%x %x, %x %x]", s, l, cpte, epte);
		operation(pmap, s, cpte, epte);
	    }
	    s = l;
	    spte++;
	}
if (pmap_debug > 1) db_printf("\n");
}
d1426 6
a1431 11
void
pmap_make_readonly(pmap, va, spte, epte)
	pmap_t			pmap;
	vm_offset_t		va;
	pt_entry_t		*spte;
	pt_entry_t		*epte;
{
	while (spte < epte) {
	    if (*spte & ALPHA_PTE_VALID)
		*spte &= ~ALPHA_PTE_WRITE;
	    spte++;
a1432 27
}

/*
 *	Remove the given range of addresses
 *	from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the hardware page size.
 */
vm_offset_t pmap_suspect_vs, pmap_suspect_ve;


void pmap_remove(map, s, e)
	pmap_t		map;
	vm_offset_t	s, e;
{
	spl_t			spl;
	register pt_entry_t	*pde;
	register pt_entry_t	*spte;
	vm_offset_t		l;

	if (map == PMAP_NULL)
		return;

if (pmap_debug || ((s > pmap_suspect_vs) && (s < pmap_suspect_ve))) 
db_printf("[%d]pmap_remove(%x,%x,%x)\n", cpu_number(), map, s, e);
	PMAP_READ_LOCK(map, spl);
d1435 2
a1436 1
	 *	Invalidate the translation buffer first
d1438 1
a1438 1
	PMAP_UPDATE_TLBS(map, s, e);
d1440 2
a1441 14
	pde = pmap_pde(map, s);
	while (s < e) {
	    /* at most (1 << 33) virtuals per iteration */
	    l = roundup(s+1, PDE_MAPPED_SIZE);
	    if (l > e)
		l = e;
	    if (*pde & ALPHA_PTE_VALID) {
		spte = (pt_entry_t *)ptetokv(*pde);
		spte = &spte[pte2num(s)];
		pmap_iterate_lev2(map, s, l, spte, pmap_remove_range);
	    }
	    s = l;
	    pde++;
	}
d1443 3
a1445 1
	PMAP_READ_UNLOCK(map, spl);
d1449 1
a1449 1
 *	Routine:	pmap_page_protect
d1451 2
a1452 3
 *	Function:
 *		Lower the permission for all mappings to a given
 *		page.
d1454 17
a1470 23
vm_offset_t pmap_suspect_phys;

void pmap_page_protect(phys, prot)
	vm_offset_t	phys;
	vm_prot_t	prot;
{
	pv_entry_t		pv_h, prev;
	register pv_entry_t	pv_e;
	register pt_entry_t	*pte;
	int			pai;
	register pmap_t		pmap;
	spl_t			spl;
	boolean_t		remove;

if (pmap_debug || (phys == pmap_suspect_phys)) db_printf("pmap_page_protect(%x,%x)\n", phys, prot);

	assert(phys != vm_page_fictitious_addr);
	if (!valid_page(phys)) {
	    /*
	     *	Not a managed page.
	     */
	    return;
	}
d1473 4
a1476 1
	 * Determine the new protection.
d1478 1
d1480 17
a1496 8
	    case VM_PROT_READ:
	    case VM_PROT_READ|VM_PROT_EXECUTE:
		remove = FALSE;
		break;
	    case VM_PROT_ALL:
		return;	/* nothing to do */
	    default:
		remove = TRUE;
d1500 17
a1516 82
	/*
	 *	Lock the pmap system first, since we will be changing
	 *	several pmaps.
	 */

	PMAP_WRITE_LOCK(spl);

	pai = pa_index(phys);
	pv_h = pai_to_pvh(pai);

	/*
	 * Walk down PV list, changing or removing all mappings.
	 * We do not have to lock the pv_list because we have
	 * the entire pmap system locked.
	 */
	if (pv_h->pmap != PMAP_NULL) {

	    prev = pv_e = pv_h;
	    do {
		pmap = pv_e->pmap;
		/*
		 * Lock the pmap to block pmap_extract and similar routines.
		 */
		simple_lock(&pmap->lock);

		{
		    register vm_offset_t va;

		    va = pv_e->va;
		    pte = pmap_pte(pmap, va);

		    /*
		     * Consistency checks.
		     */
		    /* assert(*pte & ALPHA_PTE_VALID); XXX */
		    /* assert(pte_to_phys(*pte) == phys); */

		    /*
		     * Invalidate TLBs for all CPUs using this mapping.
		     */
		    PMAP_UPDATE_TLBS(pmap, va, va + PAGE_SIZE);
		}

		/*
		 * Remove the mapping if new protection is NONE
		 * or if write-protecting a kernel mapping.
		 */
		if (remove || pmap == kernel_pmap) {
		    /*
		     * Remove the mapping, collecting any modify bits.
		     */
		    if (*pte & ALPHA_PTE_WIRED)
			panic("pmap_remove_all removing a wired page");

		    {
			register int	i = ptes_per_vm_page;

			do {
			    pmap_phys_attributes[pai] |= pte_get_attributes(pte);
			    *pte++ = 0;
			} while (--i > 0);
		    }

		    pmap->stats.resident_count--;

		    /*
		     * Remove the pv_entry.
		     */
		    if (pv_e == pv_h) {
			/*
			 * Fix up head later.
			 */
			pv_h->pmap = PMAP_NULL;
		    }
		    else {
			/*
			 * Delete this entry.
			 */
			prev->next = pv_e->next;
			PV_FREE(pv_e);
		    }
		}
d1518 6
a1523 14
		    /*
		     * Write-protect.
		     */
		    register int i = ptes_per_vm_page;

		    do {
			*pte &= ~ALPHA_PTE_WRITE;
			pte++;
		    } while (--i > 0);

		    /*
		     * Advance prev.
		     */
		    prev = pv_e;
d1525 3
d1529 2
a1530 1
		simple_unlock(&pmap->lock);
d1532 2
a1533 15
	    } while ((pv_e = prev->next) != PV_ENTRY_NULL);

	    /*
	     * If pv_head mapping was removed, fix it up.
	     */
	    if (pv_h->pmap == PMAP_NULL) {
		pv_e = pv_h->next;
		if (pv_e != PV_ENTRY_NULL) {
		    *pv_h = *pv_e;
		    PV_FREE(pv_e);
		}
	    }
	}

	PMAP_WRITE_UNLOCK(spl);
d1537 4
a1540 3
 *	Set the physical protection on the
 *	specified range of this map as requested.
 *	Will not increase permissions.
d1542 17
a1558 9
void pmap_protect(map, s, e, prot)
	pmap_t		map;
	vm_offset_t	s, e;
	vm_prot_t	prot;
{
	register pt_entry_t	*pde;
	register pt_entry_t	*spte, *epte;
	vm_offset_t		l;
	spl_t			spl;
d1560 1
a1560 1
	if (map == PMAP_NULL)
d1563 2
a1564 16
if (pmap_debug || ((s > pmap_suspect_vs) && (s < pmap_suspect_ve))) 
db_printf("[%d]pmap_protect(%x,%x,%x,%x)\n", cpu_number(), map, s, e, prot);
	/*
	 * Determine the new protection.
	 */
	switch (prot) {
	    case VM_PROT_READ|VM_PROT_EXECUTE:
		alphacache_Iflush();
	    case VM_PROT_READ:
		break;
	    case VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE:
		alphacache_Iflush();
	    case VM_PROT_READ|VM_PROT_WRITE:
		return;	/* nothing to do */
	    default:
		pmap_remove(map, s, e);
d1568 4
a1571 2
	SPLVM(spl);
	simple_lock(&map->lock);
d1573 2
a1574 4
	/*
	 *	Invalidate the translation buffer first
	 */
	PMAP_UPDATE_TLBS(map, s, e);
d1576 33
a1608 13
	pde = pmap_pde(map, s);
	while (s < e) {
	    /* at most (1 << 33) virtuals per iteration */
	    l = roundup(s+1, PDE_MAPPED_SIZE);
	    if (l > e)
		l = e;
	    if (*pde & ALPHA_PTE_VALID) {
		spte = (pt_entry_t *)ptetokv(*pde);
		spte = &spte[pte2num(s)];
		pmap_iterate_lev2(map, s, l, spte, pmap_make_readonly);
	    }
	    s = l;
	    pde++;
d1611 4
a1614 2
	simple_unlock(&map->lock);
	SPLX(spl);
d1618 2
d1627 1
a1627 1
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
d1632 25
a1656 26
pmap_enter(pmap, v, pa, prot, wired, access_type)
	register pmap_t		pmap;
	vm_offset_t		v;
	register vm_offset_t	pa;
	vm_prot_t		prot;
	boolean_t		wired;
	vm_prot_t		access_type;
{
	register pt_entry_t	*pte;
	register pv_entry_t	pv_h;
	register int		i, pai;
	pv_entry_t		pv_e;
	pt_entry_t		template;
	spl_t			spl;
	vm_offset_t		old_pa;

	if (DOPDB(PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%p, 0x%lx, 0x%lx, 0x%x, %d)\n",
		    pmap, v, pa, prot, wired);

	assert(pa != vm_page_fictitious_addr);
if (pmap_debug || ((v > pmap_suspect_vs) && (v < pmap_suspect_ve))) 
db_printf("[%d]pmap_enter(%x(%d), %x, %x, %x, %x)\n", cpu_number(), pmap, pmap->pid, v, pa, prot, wired);
	if (pmap == PMAP_NULL)
		goto out;
	assert(!pmap_max_asn || pmap->pid >= 0);
d1658 3
a1660 10
	/*
	 *	Must allocate a new pvlist entry while we're unlocked;
	 *	zalloc may cause pageout (which will lock the pmap system).
	 *	If we determine we need a pvlist entry, we will unlock
	 *	and allocate one.  Then we will retry, throwing away
	 *	the allocated entry later (if we no longer need it).
	 */
	pv_e = PV_ENTRY_NULL;
Retry:
	PMAP_READ_LOCK(pmap, spl);
d1662 2
a1663 5
	/*
	 *	Expand pmap to include this pte.  Assume that
	 *	pmap is always expanded to include enough hardware
	 *	pages to map one VM page.
	 */
d1665 2
a1666 1
	while ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL) {
d1668 1
a1668 1
		 *	Must unlock to expand the pmap.
d1670 6
a1675 1
		PMAP_READ_UNLOCK(pmap, spl);
d1677 7
a1683 1
		pmap_expand(pmap, v);
d1685 16
a1700 2
		PMAP_READ_LOCK(pmap, spl);
	}
d1702 26
a1727 41
	/*
	 *	Special case if the physical page is already mapped
	 *	at this address.
	 */
	old_pa = pte_to_pa(*pte);
	if (*pte && old_pa == pa) {
	    /*
	     *	May be changing its wired attribute or protection
	     */
		
	    if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter: same PA already mapped there (0x%lx)\n",
		    *pte);

	    if (wired && !(*pte & ALPHA_PTE_WIRED))
		pmap->stats.wired_count++;
	    else if (!wired && (*pte & ALPHA_PTE_WIRED))
		pmap->stats.wired_count--;

	    pte_template(pmap,template,pa,prot);
	    if (pmap == kernel_pmap)
		template |= ALPHA_PTE_GLOBAL;
	    if (wired)
		template |= ALPHA_PTE_WIRED;
	    PMAP_UPDATE_TLBS(pmap, v, v + PAGE_SIZE);
	    i = ptes_per_vm_page;
	    do {
		template |= (*pte & ALPHA_PTE_MOD);
		*pte = template;
		pte++;
		pte_increment_pa(template);
	    } while (--i > 0);
	}
	else {

	    /*
	     *	Remove old mapping from the PV list if necessary.
	     */
	    if (*pte) {
		if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
			printf("pmap_enter: removing old PTE (0x%lx)\n", *pte);
d1730 4
a1733 2
		 *	Invalidate the translation buffer,
		 *	then remove the mapping.
d1735 20
a1754 1
		PMAP_UPDATE_TLBS(pmap, v, v + PAGE_SIZE);
d1757 1
a1757 2
		 *	Don't free the pte page if removing last
		 *	mapping - we will immediately replace it.
d1759 2
a1760 3
		pmap_remove_range(pmap, v, pte,
				  pte + ptes_per_vm_page);
	    }
d1762 2
a1763 3
	    if (valid_page(pa)) {
		if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
			printf("pmap_enter: valid page\n");
d1765 5
d1771 3
a1773 2
		 *	Enter the mapping in the PV list for this
		 *	physical page.
d1775 1
d1777 5
a1781 3
		pai = pa_index(pa);
		LOCK_PVH(pai);
		pv_h = pai_to_pvh(pai);
d1783 6
a1788 11
		if (pv_h->pmap == PMAP_NULL) {
		    /*
		     *	No mappings yet
		     */
		    if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
			printf("pmap_enter: first mapping\n");
		    pv_h->va = v;
		    pv_h->pmap = pmap;
		    pv_h->next = PV_ENTRY_NULL;
		    if (prot & VM_PROT_EXECUTE)
			alphacache_Iflush();
d1790 5
a1794 3
		else {
		    if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
			printf("pmap_enter: second+ mapping\n");
d1796 10
a1805 29
#if	DEBUG
		    {
			/* check that this mapping is not already there */
			pv_entry_t	e = pv_h;
			while (e != PV_ENTRY_NULL) {
			    if (e->pmap == pmap && e->va == v)
				panic("pmap_enter: already in pv_list");
			    e = e->next;
			}
		    }
#endif	/* DEBUG */
		    
		    /*
		     *	Add new pv_entry after header.
		     */
		    if (pv_e == PV_ENTRY_NULL) {
			pv_e = pmap_alloc_pv();
#if 0
			PV_ALLOC(pv_e);
			if (pv_e == PV_ENTRY_NULL) {
			    UNLOCK_PVH(pai);
			    PMAP_READ_UNLOCK(pmap, spl);

			    /*
			     * Refill from zone.
			     */
			    pv_e = (pv_entry_t) zalloc(pv_list_zone);
			    goto Retry;
			}
d1807 8
a1814 108
		    }
		    pv_e->va = v;
		    pv_e->pmap = pmap;
		    pv_e->next = pv_h->next;
		    pv_h->next = pv_e;
		    /*
		     *	Remember that we used the pvlist entry.
		     */
		    pv_e = PV_ENTRY_NULL;
		}
		UNLOCK_PVH(pai);
	    }

	    /*
	     *	And count the mapping.
	     */

	    pmap->stats.resident_count++;
	    if (wired)
		pmap->stats.wired_count++;

	    /*
	     *	Build a template to speed up entering -
	     *	only the pfn changes.
	     */
	    pte_template(pmap,template,pa,prot);
	    if (pmap == kernel_pmap)
		template |= ALPHA_PTE_GLOBAL;
	    if (wired)
		template |= ALPHA_PTE_WIRED;
	    i = ptes_per_vm_page;
	    do {
		if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
			printf("pmap_enter: entering PTE 0x%lx at %p\n",
			    template, pte);
		*pte = template;
		pte++;
		pte_increment_pa(template);
	    } while (--i > 0);
	    ALPHA_TBIA();
	}

	if (pv_e != PV_ENTRY_NULL) {
	    PV_FREE(pv_e);
	}

	PMAP_READ_UNLOCK(pmap, spl);
out:
	if (DOVPDB(PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter: done\n");
}

/*
 *	Routine:	pmap_change_wiring
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 */
void pmap_change_wiring(map, v, wired)
	register pmap_t	map;
	vm_offset_t	v;
	boolean_t	wired;
{
	register pt_entry_t	*pte;
	register int		i;
	spl_t			spl;

if (pmap_debug) db_printf("pmap_change_wiring(%x,%x,%x)\n", map, v, wired);
	/*
	 *	We must grab the pmap system lock because we may
	 *	change a pte_page queue.
	 */
	PMAP_READ_LOCK(map, spl);

	if ((pte = pmap_pte(map, v)) == PT_ENTRY_NULL)
		panic("pmap_change_wiring: pte missing");

	if (wired && !(*pte & ALPHA_PTE_WIRED)) {
	    /*
	     *	wiring down mapping
	     */
	    map->stats.wired_count++;
	    i = ptes_per_vm_page;
	    do {
		*pte++ |= ALPHA_PTE_WIRED;
	    } while (--i > 0);
	}
	else if (!wired && (*pte & ALPHA_PTE_WIRED)) {
	    /*
	     *	unwiring mapping
	     */
	    map->stats.wired_count--;
	    i = ptes_per_vm_page;
	    do {
		*pte &= ~ALPHA_PTE_WIRED;
	    } while (--i > 0);
	}

	PMAP_READ_UNLOCK(map, spl);
}

/*
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
 */
d1816 5
a1820 11
vm_offset_t
pmap_extract(pmap, va)
	register pmap_t	pmap;
	vm_offset_t	va;
{
	register pt_entry_t	*pte;
	register vm_offset_t	pa;
	spl_t			spl;

	if (DOPDB(PDB_FOLLOW|PDB_EXTRACT))
		printf("pmap_extract(%p, 0x%lx)\n", pmap, va);
d1823 2
a1824 2
	 *	Special translation for kernel addresses in
	 *	K0 space (directly mapped to physical addresses).
d1826 12
a1837 5
	if (ISA_K0SEG(va)) {
		pa = K0SEG_TO_PHYS(va);
		if (DOPDB(PDB_FOLLOW|PDB_EXTRACT))
			printf("pmap_extract: returns 0x%lx\n", pa);
		goto out;
d1839 1
d1841 1
a1841 10
	SPLVM(spl);
	simple_lock(&pmap->lock);
	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
	    pa = (vm_offset_t) 0;
	else if (!(*pte & ALPHA_PTE_VALID))
	    pa = (vm_offset_t) 0;
	else
	    pa = pte_to_pa(*pte) + (va & ALPHA_OFFMASK);
	simple_unlock(&pmap->lock);

d1843 1
a1843 1
	 * Beware: this puts back this thread in the cpus_active set
d1845 12
a1856 15
	SPLX(spl);

out:
	if (DOPDB(PDB_FOLLOW|PDB_EXTRACT))
		printf("pmap_extract: returns 0x%lx\n", pa);
	return(pa);
}

vm_offset_t
pmap_resident_extract(pmap, va)
	register pmap_t	pmap;
	vm_offset_t	va;
{
	register pt_entry_t	*pte;
	register vm_offset_t	pa;
d1859 1
a1859 2
	 *	Special translation for kernel addresses in
	 *	K0 space (directly mapped to physical addresses).
d1861 3
a1863 44
	if (ISA_K0SEG(va)) {
		pa = K0SEG_TO_PHYS(va);
		goto out;
	}

	if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
	    pa = (vm_offset_t) 0;
	else if (!(*pte & ALPHA_PTE_VALID))
	    pa = (vm_offset_t) 0;
	else
	    pa = pte_to_pa(*pte) + (va & ALPHA_OFFMASK);

out:
	return(pa);
}

/*
 *	Routine:	pmap_expand
 *
 *	Expands a pmap to be able to map the specified virtual address.
 *
 *	Must be called with the pmap system and the pmap unlocked,
 *	since these must be unlocked to use vm_page_grab.
 *	Thus it must be called in a loop that checks whether the map
 *	has been expanded enough.
 */
void
pmap_expand(map, v)
	register pmap_t		map;
	register vm_offset_t	v;
{
	pt_entry_t		template;
	pt_entry_t		*pdp;
	register vm_page_t	m;
	register vm_offset_t	pa;
	register int		i;
	spl_t			spl;

	if (DOPDB(PDB_FOLLOW|PDB_EXPAND))
		printf("pmap_expand(%p, 0x%lx)\n", map, v);

	/* Would have to go through all maps to add this page */
	if (map == kernel_pmap)
		panic("pmap_expand");
d1865 1
d1867 1
a1867 2
	 *	Allocate a VM page for the level 2 page table entries,
	 *	if not already there.
d1869 4
a1872 3
	pdp = pmap_pde(map,v);
	if ((*pdp & ALPHA_PTE_VALID) == 0) {
		pt_entry_t	*pte;
d1874 11
a1884 4
		if (DOVPDB(PDB_FOLLOW|PDB_EXPAND))
			printf("pmap_expand: needs pde\n");

		pa = pmap_page_table_page_alloc();
d1887 1
a1887 4
		 * Re-lock the pmap and check that another thread has
		 * not already allocated the page-table page.  If it
		 * has, discard the new page-table page (and try
		 * again to make sure).
d1889 4
a1892 10
		PMAP_READ_LOCK(map, spl);

		if (*pdp & ALPHA_PTE_VALID) {
			/*
			 * Oops...
			 */
			PMAP_READ_UNLOCK(map, spl);
			pmap_page_table_page_dealloc(pa);
			return;
		}
d1895 1
a1895 1
		 * Map the page.
d1897 1
a1897 14
		i = ptes_per_vm_page;
		pte = pdp;
		pte_ktemplate(template,pa,VM_PROT_READ|VM_PROT_WRITE);
		if (map != kernel_pmap)
			template &= ~ALPHA_PTE_ASM;
		do {
			*pte = template;
			if (DOVPDB(PDB_FOLLOW|PDB_EXPAND))
				printf("pmap_expand: inserted l1 pte (0x%lx) at %p\n",
				   template, pte);
			pte++;
			pte_increment_pa(template);
		} while (--i > 0);
		PMAP_READ_UNLOCK(map, spl);
d1899 6
d1907 2
a1908 1
	 *	Allocate a level 3 page table.
d1910 2
d1913 4
a1916 1
	pa = pmap_page_table_page_alloc();
d1919 2
a1920 3
	 * Re-lock the pmap and check that another thread has
	 * not already allocated the page-table page.  If it
	 * has, we are done.
d1922 8
a1929 1
	PMAP_READ_LOCK(map, spl);
d1931 2
a1932 5
	if (pmap_pte(map, v) != PT_ENTRY_NULL) {
		PMAP_READ_UNLOCK(map, spl);
		pmap_page_table_page_dealloc(pa);
		return;
	}
d1934 3
a1936 25
	/*
	 *	Set the page directory entry for this page table.
	 *	If we have allocated more than one hardware page,
	 *	set several page directory entries.
	 */
	i = ptes_per_vm_page;
	pdp = (pt_entry_t *)ptetokv(*pdp);
	pdp = &pdp[pte2num(v)];
	pte_ktemplate(template,pa,VM_PROT_READ|VM_PROT_WRITE);
	if (map != kernel_pmap)
		template &= ~ALPHA_PTE_ASM;
	do {
		*pdp = template;
		if (DOVPDB(PDB_FOLLOW|PDB_EXPAND))
			printf("pmap_expand: inserted l2 pte (0x%lx) at %p\n",
			  template, pdp);
		pdp++;
		pte_increment_pa(template);
	} while (--i > 0);
	PMAP_READ_UNLOCK(map, spl);

out:
	if (DOVPDB(PDB_FOLLOW|PDB_EXPAND))
		printf("pmap_expand: leaving\n");
	return;
d1940 4
a1943 3
 *	Copy the range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
d1945 1
a1945 1
 *	This routine is only advisory and need not do anything.
d1947 15
a1961 12
#if	0
void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t		dst_pmap;
	pmap_t		src_pmap;
	vm_offset_t	dst_addr;
	vm_size_t	len;
	vm_offset_t	src_addr;
{
#ifdef	lint
	dst_pmap++; src_pmap++; dst_addr++; len++; src_addr++;
#endif	/* lint */
}
d1964 7
a1970 15
/*
 *	Routine:	pmap_collect
 *	Function:
 *		Garbage collects the physical map system for
 *		pages which are no longer used.
 *		Success need not be guaranteed -- that is, there
 *		may well be pages which are not referenced, but
 *		others may be collected.
 *	Usage:
 *		Called by the pageout daemon when pages are scarce.
 */
void pmap_collect(p)
	pmap_t 		p;
{
#if	notyet
d1972 1
a1972 5
	register pt_entry_t	*pdp, *ptp;
	pt_entry_t		*eptp;
	vm_offset_t		pa;
	spl_t			spl;
	int			wired;
d1974 4
a1977 2
	if (p == PMAP_NULL)
		return;
d1979 2
a1980 2
	if (p == kernel_pmap)
		return;
d1983 1
a1983 1
	 *	Garbage collect map.
d1985 2
a1986 9
	PMAP_READ_LOCK(p, spl);
	PMAP_UPDATE_TLBS(p, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
	pmap_tlbpid_destroy(p->pid, FALSE);

	for (pdp = p->dirbase;
	     pdp < pmap_pde(p,VM_MIN_KERNEL_ADDRESS);
	     pdp += ptes_per_vm_page)
	{
	    if (*pdp & ALPHA_PTE_VALID) {
d1988 4
a1991 78
		pa = pte_to_pa(*pdp);
		ptp = (pt_entry_t *)phystokv(pa);
		eptp = ptp + NPTES*ptes_per_vm_page;

		/*
		 * If the pte page has any wired mappings, we cannot
		 * free it.
		 */
		wired = 0;
		{
		    register pt_entry_t *ptep;
		    for (ptep = ptp; ptep < eptp; ptep++) {
			if (*ptep & ALPHA_PTE_WIRED) {
			    wired = 1;
			    break;
			}
		    }
		}
		if (!wired) {
		    /*
		     * Remove the virtual addresses mapped by this pte page.
		     */
.....		    pmap_remove_range_2(p,
				pdetova(pdp - p->dirbase),
				ptp,
				eptp);

		    /*
		     * Invalidate the page directory pointer.
		     */
		    {
			register int i = ptes_per_vm_page;
			register pt_entry_t *pdep = pdp;
			do {
			    *pdep++ = 0;
			} while (--i > 0);
		    }

		    PMAP_READ_UNLOCK(p, spl);

		    /*
		     * And free the pte page itself.
		     */
		    {
			register vm_page_t m;

			vm_object_lock(pmap_object);
			m = vm_page_lookup(pmap_object, pa);
			if (m == VM_PAGE_NULL)
			    panic("pmap_collect: pte page not in object");
			vm_page_lock_queues();
			vm_page_free(m);
			inuse_ptepages_count--;
			vm_page_unlock_queues();
			vm_object_unlock(pmap_object);
		    }

		    PMAP_READ_LOCK(p, spl);
		}
	    }
	}
	PMAP_READ_UNLOCK(p, spl);
	return;
#endif
}

/*
 *	Routine:	pmap_activate
 *	Function:
 *		Binds the given physical map to the given
 *		processor, and returns a hardware map description.
 */
void
pmap_activate(pmap, hwpcb, cpu)
	register pmap_t	pmap;
	struct alpha_pcb *hwpcb;
	int cpu;
{
d1993 7
a1999 16
        if (DOPDB(PDB_FOLLOW|PDB_ACTIVATE))
                printf("pmap_activate(%p, %p, %d)\n", pmap, hwpcb, cpu);

#if 0
	PMAP_ACTIVATE(my_pmap, th, my_cpu);
#else
        if (DOVPDB(PDB_ACTIVATE))
                printf("pmap_activate: old pid = %d\n", pmap->pid);
        if (pmap->pid < 0) pmap_tlbpid_assign(pmap);
	hwpcb->apcb_asn = pmap->pid;
        hwpcb->apcb_ptbr = pmap->dirpfn;
	if (pmap != kernel_pmap)
		pmap->cpus_using = TRUE;
        if (DOVPDB(PDB_ACTIVATE))
                printf("pmap_activate: new pid = %d, new ptbr = 0x%lx\n",
		    pmap->pid, pmap->dirpfn);
d2001 2
d2006 13
a2018 11
 *	Routine:	pmap_deactivate
 *	Function:
 *		Indicates that the given physical map is no longer
 *		in use on the specified processor.  (This is a macro
 *		in pmap.h)
 */
void
pmap_deactivate(pmap, hwpcb, cpu)
	register pmap_t	pmap;
	struct alpha_pcb *hwpcb;
	int cpu;
d2020 1
a2020 2
        if (DOPDB(PDB_FOLLOW|PDB_DEACTIVATE))
                printf("pmap_deactivate(%p, %p, %d)\n", pmap, hwpcb, cpu);
d2022 4
a2025 7
#if 0
	PMAP_DEACTIVATE(pmap, th, which_cpu);
#else
        if (DOVPDB(PDB_DEACTIVATE))
                printf("pmap_deactivate: pid = %d, ptbr = 0x%lx\n",
		    pmap->pid, pmap->dirpfn);
	pmap->cpus_using = FALSE;
d2027 5
d2035 4
a2038 3
 *	Routine:	pmap_kernel
 *	Function:
 *		Returns the physical map handle for the kernel.
d2040 46
a2085 4
#if	0
pmap_t pmap_kernel()
{
    	return (kernel_pmap);
a2086 1
#endif
d2089 5
a2093 2
 *	pmap_zero_page zeros the specified (machine independent) page.
 *	See machine/phys.c or machine/phys.s for implementation.
a2094 1
#if	1
d2096 3
a2098 2
pmap_zero_page(phys)
	register vm_offset_t	phys;
d2100 11
d2112 1
a2112 2
	if (DOPDB(PDB_FOLLOW|PDB_ZERO_PAGE))
		printf("pmap_zero_page(0x%lx)\n", phys);
d2114 5
a2118 1
	assert(phys != vm_page_fictitious_addr);
d2120 15
a2134 1
	bzero((void *)phystokv(phys), PAGE_SIZE);
d2136 1
a2136 2
	if (DOVPDB(PDB_FOLLOW|PDB_ZERO_PAGE))
		printf("pmap_zero_page: leaving\n");
a2137 1
#endif
d2140 4
a2143 2
 *	pmap_copy_page copies the specified (machine independent) page.
 *	See machine/phys.c or machine/phys.s for implementation.
d2145 4
a2148 4
#if 1	/* fornow */
void
pmap_copy_page(src, dst)
	vm_offset_t	src, dst;
d2150 8
d2159 3
a2161 2
	if (DOPDB(PDB_FOLLOW|PDB_COPY_PAGE))
		printf("pmap_copy_page(0x%lx, 0x%lx)\n", src, dst);
d2163 3
a2165 2
	assert(src != vm_page_fictitious_addr);
	assert(dst != vm_page_fictitious_addr);
d2167 3
a2169 1
	aligned_block_copy(phystokv(src), phystokv(dst), PAGE_SIZE);
d2171 12
a2182 2
	if (DOVPDB(PDB_FOLLOW|PDB_COPY_PAGE))
		printf("pmap_copy_page: leaving\n");
a2183 1
#endif
d2186 1
a2186 4
 *	Routine:	pmap_pageable
 *	Function:
 *		Make the specified pages (by pmap, offset)
 *		pageable (or not) as requested.
d2188 3
a2190 3
 *		A page which is not pageable may not take
 *		a fault; therefore, its page table entry
 *		must remain valid for the duration.
d2192 1
a2192 3
 *		This routine is merely advisory; pmap_enter
 *		will specify that these pages are to be wired
 *		down (or not) as appropriate.
d2195 11
a2205 8
pmap_pageable(pmap, start, end, pageable)
	pmap_t		pmap;
	vm_offset_t	start;
	vm_offset_t	end;
	boolean_t	pageable;
{
#ifdef	lint
	pmap++; start++; end++; pageable++;
d2210 5
a2214 1
 *	Clear specified attribute bits.
d2217 1
a2217 3
phys_attribute_clear(phys, bits)
	vm_offset_t	phys;
	int		bits;
a2218 6
	pv_entry_t		pv_h;
	register pv_entry_t	pv_e;
	register pt_entry_t	*pte;
	int			pai;
	register pmap_t		pmap;
	spl_t			spl;
d2220 4
a2223 7
	assert(phys != vm_page_fictitious_addr);
	if (!valid_page(phys)) {
	    /*
	     *	Not a managed page.
	     */
	    return;
	}
d2226 1
a2226 2
	 *	Lock the pmap system first, since we will be changing
	 *	several pmaps.
d2228 1
d2230 14
a2243 1
	PMAP_WRITE_LOCK(spl);
d2245 4
a2248 2
	pai = pa_index(phys);
	pv_h = pai_to_pvh(pai);
d2251 4
a2254 3
	 * Walk down PV list, clearing all modify or reference bits.
	 * We do not have to lock the pv_list because we have
	 * the entire pmap system locked.
d2256 1
a2256 48
	if (pv_h->pmap != PMAP_NULL) {
	    /*
	     * There are some mappings.
	     */
	    for (pv_e = pv_h; pv_e != PV_ENTRY_NULL; pv_e = pv_e->next) {

		pmap = pv_e->pmap;
		/*
		 * Lock the pmap to block pmap_extract and similar routines.
		 */
		simple_lock(&pmap->lock);

		{
		    register vm_offset_t va;

		    va = pv_e->va;
		    pte = pmap_pte(pmap, va);

#if	0
		    /*
		     * Consistency checks.
		     */
		    assert(*pte & ALPHA_PTE_VALID);
		    /* assert(pte_to_phys(*pte) == phys); */
#endif

		    /*
		     * Invalidate TLBs for all CPUs using this mapping.
		     */
		    PMAP_UPDATE_TLBS(pmap, va, va + PAGE_SIZE);
		}

		/*
		 * Clear modify or reference bits.
		 */
		{
		    register int	i = ptes_per_vm_page;
		    do {
			*pte &= ~bits;
		    } while (--i > 0);
		}
		simple_unlock(&pmap->lock);
	    }
	}

	pmap_phys_attributes[pai] &= ~ (bits >> 16);

	PMAP_WRITE_UNLOCK(spl);
d2260 8
a2267 1
 *	Check specified attribute bits.
d2269 3
a2271 4
boolean_t
phys_attribute_test(phys, bits)
	vm_offset_t	phys;
	int		bits;
d2273 2
a2274 6
	pv_entry_t		pv_h;
	register pv_entry_t	pv_e;
	register pt_entry_t	*pte;
	int			pai;
	register pmap_t		pmap;
	spl_t			spl;
d2276 4
a2279 7
	assert(phys != vm_page_fictitious_addr);
	if (!valid_page(phys)) {
	    /*
	     *	Not a managed page.
	     */
	    return (FALSE);
	}
d2282 1
a2282 2
	 *	Lock the pmap system first, since we will be checking
	 *	several pmaps.
d2284 1
d2286 7
a2292 4
	PMAP_WRITE_LOCK(spl);

	pai = pa_index(phys);
	pv_h = pai_to_pvh(pai);
d2294 1
a2294 4
	if (pmap_phys_attributes[pai] & (bits >> 16)) {
	    PMAP_WRITE_UNLOCK(spl);
	    return (TRUE);
	}
d2297 1
a2297 3
	 * Walk down PV list, checking all mappings.
	 * We do not have to lock the pv_list because we have
	 * the entire pmap system locked.
d2299 1
a2299 5
	if (pv_h->pmap != PMAP_NULL) {
	    /*
	     * There are some mappings.
	     */
	    for (pv_e = pv_h; pv_e != PV_ENTRY_NULL; pv_e = pv_e->next) {
d2301 1
a2301 5
		pmap = pv_e->pmap;
		/*
		 * Lock the pmap to block pmap_extract and similar routines.
		 */
		simple_lock(&pmap->lock);
d2303 2
a2304 2
		{
		    register vm_offset_t va;
d2306 15
a2320 2
		    va = pv_e->va;
		    pte = pmap_pte(pmap, va);
d2322 3
a2324 6
#if	0
		    /*
		     * Consistency checks.
		     */
		    assert(*pte & ALPHA_PTE_VALID);
		    /* assert(pte_to_phys(*pte) == phys); */
a2325 1
		}
d2327 4
a2330 19
		/*
		 * Check modify or reference bits.
		 */
		{
		    register int	i = ptes_per_vm_page;

		    do {
			if (*pte & bits) {
			    simple_unlock(&pmap->lock);
			    PMAP_WRITE_UNLOCK(spl);
			    return (TRUE);
			}
		    } while (--i > 0);
		}
		simple_unlock(&pmap->lock);
	    }
	}
	PMAP_WRITE_UNLOCK(spl);
	return (FALSE);
d2334 7
a2340 1
 *	Set specified attribute bits.  <ugly>
d2343 2
a2344 3
phys_attribute_set(phys, bits)
	vm_offset_t	phys;
	int		bits;
d2346 6
a2351 2
	int			pai;
	spl_t			spl;
d2353 2
a2354 7
	assert(phys != vm_page_fictitious_addr);
	if (!valid_page(phys)) {
	    /*
	     *	Not a managed page.
	     */
	    return;
	}
d2357 2
a2358 1
	 *	Lock the pmap system.
d2360 27
a2386 7

	PMAP_WRITE_LOCK(spl);

	pai = pa_index(phys);
	pmap_phys_attributes[pai]  |= (bits >> 16);

	PMAP_WRITE_UNLOCK(spl);
d2390 7
a2396 1
 *	Clear the modify bits on the specified physical page.
d2398 5
d2404 7
a2410 5
void pmap_clear_modify(phys)
	register vm_offset_t	phys;
{
if (pmap_debug) db_printf("pmap_clear_mod(%x)\n", phys);
	phys_attribute_clear(phys, ALPHA_PTE_MOD);
d2413 6
a2418 6
/*
 *	Set the modify bits on the specified physical page.
 */

void pmap_set_modify(phys)
	register vm_offset_t	phys;
a2419 2
if (pmap_debug) db_printf("pmap_set_mod(%x)\n", phys);
	phys_attribute_set(phys, ALPHA_PTE_MOD);
d2423 1
a2423 1
 *	pmap_is_modified:
d2425 1
a2425 2
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d2427 27
d2455 1
a2455 5
boolean_t pmap_is_modified(phys)
	register vm_offset_t	phys;
{
if (pmap_debug) db_printf("pmap_is_mod(%x)\n", phys);
	return (phys_attribute_test(phys, ALPHA_PTE_MOD));
d2459 1
a2459 1
 *	pmap_clear_reference:
d2463 13
d2477 16
a2492 6
void pmap_clear_reference(phys)
	vm_offset_t	phys;
{
if (pmap_debug) db_printf("pmap_clear_ref(%x)\n", phys);
	phys_attribute_clear(phys, ALPHA_PTE_REF);
}
d2495 1
a2495 1
 *	pmap_is_referenced:
d2500 3
a2502 3

boolean_t pmap_is_referenced(phys)
	vm_offset_t	phys;
d2504 12
a2515 2
if (pmap_debug) db_printf("pmap_is_ref(%x)\n", phys);
	return (phys_attribute_test(phys, ALPHA_PTE_REF));
a2517 1
#if	NCPUS > 1
d2519 22
a2540 58
*	    TLB Coherence Code (TLB "shootdown" code)
* 
* Threads that belong to the same task share the same address space and
* hence share a pmap.  However, they  may run on distinct cpus and thus
* have distinct TLBs that cache page table entries. In order to guarantee
* the TLBs are consistent, whenever a pmap is changed, all threads that
* are active in that pmap must have their TLB updated. To keep track of
* this information, the set of cpus that are currently using a pmap is
* maintained within each pmap structure (cpus_using). Pmap_activate() and
* pmap_deactivate add and remove, respectively, a cpu from this set.
* Since the TLBs are not addressable over the bus, each processor must
* flush its own TLB; a processor that needs to invalidate another TLB
* needs to interrupt the processor that owns that TLB to signal the
* update.
* 
* Whenever a pmap is updated, the lock on that pmap is locked, and all
* cpus using the pmap are signaled to invalidate. All threads that need
* to activate a pmap must wait for the lock to clear to await any updates
* in progress before using the pmap. They must ACQUIRE the lock to add
* their cpu to the cpus_using set. An implicit assumption made
* throughout the TLB code is that all kernel code that runs at or higher
* than splvm blocks out update interrupts, and that such code does not
* touch pageable pages.
* 
* A shootdown interrupt serves another function besides signaling a
* processor to invalidate. The interrupt routine (pmap_update_interrupt)
* waits for the both the pmap lock (and the kernel pmap lock) to clear,
* preventing user code from making implicit pmap updates while the
* sending processor is performing its update. (This could happen via a
* user data write reference that turns on the modify bit in the page
* table). It must wait for any kernel updates that may have started
* concurrently with a user pmap update because the IPC code
* changes mappings.
* Spinning on the VALUES of the locks is sufficient (rather than
* having to acquire the locks) because any updates that occur subsequent
* to finding the lock unlocked will be signaled via another interrupt.
* (This assumes the interrupt is cleared before the low level interrupt code 
* calls pmap_update_interrupt()). 
* 
* The signaling processor must wait for any implicit updates in progress
* to terminate before continuing with its update. Thus it must wait for an
* acknowledgement of the interrupt from each processor for which such
* references could be made. For maintaining this information, a set
* cpus_active is used. A cpu is in this set if and only if it can 
* use a pmap. When pmap_update_interrupt() is entered, a cpu is removed from
* this set; when all such cpus are removed, it is safe to update.
* 
* Before attempting to acquire the update lock on a pmap, a cpu (A) must
* be at least at the priority of the interprocessor interrupt
* (splip<=splvm). Otherwise, A could grab a lock and be interrupted by a
* kernel update; it would spin forever in pmap_update_interrupt() trying
* to acquire the user pmap lock it had already acquired. Furthermore A
* must remove itself from cpus_active.  Otherwise, another cpu holding
* the lock (B) could be in the process of sending an update signal to A,
* and thus be waiting for A to remove itself from cpus_active. If A is
* spinning on the lock at priority this will never happen and a deadlock
* will result.
*/
d2543 7
a2549 1
 *	Signal another CPU that it must flush its TLB
d2551 3
a2553 4
void    signal_cpus(use_list, pmap, start, end)
	cpu_set		use_list;
	pmap_t		pmap;
	vm_offset_t	start, end;
a2554 2
	register int		which_cpu, j;
	register pmap_update_list_t	update_list_p;
d2556 2
a2557 2
	while ((which_cpu = ffs(use_list)) != 0) {
	    which_cpu -= 1;	/* convert to 0 origin */
d2559 3
a2561 2
	    update_list_p = &cpu_update_list[which_cpu];
	    simple_lock(&update_list_p->lock);
d2563 11
a2573 18
	    j = update_list_p->count;
	    if (j >= UPDATE_LIST_SIZE) {
		/*
		 *	list overflowed.  Change last item to
		 *	indicate overflow.
		 */
		update_list_p->item[UPDATE_LIST_SIZE-1].pmap  = kernel_pmap;
		update_list_p->item[UPDATE_LIST_SIZE-1].start = VM_MIN_ADDRESS;
		update_list_p->item[UPDATE_LIST_SIZE-1].end   = VM_MAX_KERNEL_ADDRESS;
	    }
	    else {
		update_list_p->item[j].pmap  = pmap;
		update_list_p->item[j].start = start;
		update_list_p->item[j].end   = end;
		update_list_p->count = j+1;
	    }
	    cpu_update_needed[which_cpu] = TRUE;
	    simple_unlock(&update_list_p->lock);
d2575 2
a2576 5
	    if ((cpus_idle & (1 << which_cpu)) == 0)
		interrupt_processor(which_cpu);
	    use_list &= ~(1 << which_cpu);
	}
}
d2578 7
a2584 7
void process_pmap_updates(my_pmap)
	register pmap_t		my_pmap;
{
	register int		my_cpu = cpu_number();
	register pmap_update_list_t	update_list_p;
	register int		j;
	register pmap_t		pmap;
d2586 5
a2590 2
	update_list_p = &cpu_update_list[my_cpu];
	simple_lock(&update_list_p->lock);
d2592 4
a2595 4
	for (j = 0; j < update_list_p->count; j++) {
	    pmap = update_list_p->item[j].pmap;
	    if (pmap == my_pmap ||
		pmap == kernel_pmap) {
d2597 4
a2600 8
		INVALIDATE_TLB(update_list_p->item[j].start,
				update_list_p->item[j].end);
	    }
	}
	update_list_p->count = 0;
	cpu_update_needed[my_cpu] = FALSE;
	simple_unlock(&update_list_p->lock);
}
d2602 5
a2606 1
#if	MACH_KDB
d2608 6
a2613 9
static boolean_t db_interp_int[NCPUS];
int db_inside_pmap_update[NCPUS];
int suicide_cpu;

cpu_interrupt_to_db(i)
	int i;
{
	db_interp_int[i] = TRUE;
	interrupt_processor(i);
a2614 1
#endif
d2617 15
a2631 1
 *	Interrupt routine for TBIA requested from other processor.
d2633 22
a2654 5
void pmap_update_interrupt()
{
	register int		my_cpu;
	register pmap_t		my_pmap;
	spl_t			s;
d2656 13
a2668 1
	my_cpu = cpu_number();
d2670 7
a2676 9
	db_inside_pmap_update[my_cpu]++;
#if	MACH_KDB
	if (db_interp_int[my_cpu]) {
		db_interp_int[my_cpu] = FALSE;
		remote_db_enter();
		/* In case another processor modified text  */
		alphacache_Iflush();
if (cpu_number() == suicide_cpu) halt();
		goto out;	/* uhmmm, maybe should do updates just in case */
d2678 7
a2684 1
#endif
d2686 1
a2686 4
	 *	Exit now if we're idle.  We'll pick up the update request
	 *	when we go active, and we must not put ourselves back in
	 *	the active set because we'll never process the interrupt
	 *	while we're idle (thus hanging the system).
d2688 3
a2690 10
	if (cpus_idle & (1 << my_cpu))
	    goto out;

	if (current_thread() == THREAD_NULL)
	    my_pmap = kernel_pmap;
	else {
	    my_pmap = current_pmap();
	    if (!pmap_in_use(my_pmap, my_cpu))
		my_pmap = kernel_pmap;
	}
d2693 1
a2693 3
	 *	Raise spl to splvm (above splip) to block out pmap_extract
	 *	from IO code (which would put this cpu back in the active
	 *	set).
d2695 5
a2699 1
	s = splvm();
d2701 4
a2704 1
	do {
d2706 12
a2717 13
	    /*
	     *	Indicate that we're not using either user or kernel
	     *	pmap.
	     */
	    i_bit_clear(my_cpu, &cpus_active);

	    /*
	     *	Wait for any pmap updates in progress, on either user
	     *	or kernel pmap.
	     */
	    while (*(volatile int *)&my_pmap->lock.lock_data ||
		   *(volatile int *)&kernel_pmap->lock.lock_data)
		continue;
d2719 10
a2728 1
	    process_pmap_updates(my_pmap);
d2730 4
a2733 1
	    i_bit_set(my_cpu, &cpus_active);
d2735 1
a2735 5
	} while (cpu_update_needed[my_cpu]);
	
	splx(s);
out:
	db_inside_pmap_update[my_cpu]--;
d2737 1
a2737 1
#else	NCPUS > 1
d2739 8
a2746 1
 *	Dummy routine to satisfy external reference.
d2748 37
a2784 5
void pmap_update_interrupt()
{
	/* should never be called. */
}
#endif	/* NCPUS > 1 */
d2786 1
a2786 5
void
set_ptbr(pmap_t map, pcb_t pcb, boolean_t switchit)
{
	/* optimize later */
	vm_offset_t     pa;
d2788 15
a2802 9
	pa = pmap_resident_extract(kernel_pmap, (vm_offset_t)map->dirbase);
printf("set_ptbr (switch = %d): dirbase = 0x%lx, pa = 0x%lx\n", switchit, map->dirbase, pa);
	if (pa == 0)
		panic("set_ptbr");
#if 0
	pcb->mss.hw_pcb.ptbr = alpha_btop(pa);
	if (switchit) {
		pcb->mss.hw_pcb.asn = map->pid;
		swpctxt(kvtophys((vm_offset_t) pcb), &(pcb)->mss.hw_pcb.ksp);
d2804 5
a2808 6
#else
	pcb->pcb_hw.apcb_ptbr = alpha_btop(pa);
	if (switchit) {
                pcb->pcb_hw.apcb_asn = map->pid;
                swpctxt(kvtophys((vm_offset_t) pcb), &(pcb)->pcb_hw.apcb_ksp);
        }
d2810 1
d2813 2
a2814 3
/***************************************************************************
 *
 *	TLBPID Management
d2816 1
a2816 3
 *	This is basically a unique number generator, with the twist
 *	that numbers are in a given range (dynamically defined).
 *	All things considered, I did it right in the MIPS case.
d2818 18
d2837 33
d2871 16
a2886 2
/* above */
int	pmap_max_asn;
d2888 3
d2892 6
a2897 3
decl_simple_lock_data(static, tlbpid_lock)
static struct pmap **pids_in_use;
static int pmap_next_pid;
d2899 8
a2906 4
pmap_tlbpid_init(maxasn)
	int maxasn;
{
	simple_lock_init(&tlbpid_lock);
d2908 19
a2926 7
        if (DOVPDB(PDB_FOLLOW|PDB_TLBPID_INIT))
                printf("pmap_tlbpid_init: maxasn = %d\n", maxasn);

	pmap_max_asn = maxasn;
	if (maxasn == 0) {
		/* ASNs not implemented...  Is this the right way to check? */
		return;
d2928 1
a2928 4
	
	pids_in_use = (struct pmap **)
		pmap_bootstrap_alloc((maxasn + 1) * sizeof(struct pmap *));
	bzero(pids_in_use, (maxasn + 1) * sizeof(struct pmap *));
d2930 2
a2931 1
	pmap_next_pid = 1;
d2934 1
d2936 1
a2936 5
 * Axioms:
 *	- pmap_next_pid always points to a free one, unless the table is full;
 *	  in that case it points to a likely candidate for recycling.
 *	- pmap.pid prevents from making duplicates: if -1 there is no
 *	  pid for it, otherwise there is one and only one entry at that index.
d2938 1
a2938 3
 * pmap_tlbpid_assign	provides a tlbpid for the given pmap, creating
 *			a new one if necessary
 * pmap_tlbpid_destroy	returns a tlbpid to the pool of available ones
d2940 3
a2942 3

pmap_tlbpid_assign(map)
	struct pmap *map;
d2944 15
a2958 1
	register int pid, next_pid;
d2960 5
a2964 2
        if (DOVPDB(PDB_FOLLOW|PDB_TLBPID_ASSIGN))
                printf("pmap_tlbpid_assign: pmap %p had %d\n", map, map->pid);
d2966 28
a2993 1
	if (pmap_max_asn && map->pid < 0) {
d2995 3
a2997 1
		simple_lock(&tlbpid_lock);
d2999 2
a3000 22
		next_pid = pmap_next_pid;
		if (pids_in_use[next_pid]) {
			/* are we _really_ sure it's full ? */
			for (pid = 1; pid < pmap_max_asn; pid++)
				if (pids_in_use[pid] == PMAP_NULL) {
					/* aha! */
					next_pid = pid;
					goto got_a_free_one;
				}
			/* Table full */
			while (pids_in_use[next_pid]->cpus_using) {
				if (++next_pid == pmap_max_asn)
					next_pid = 1;
			}
			pmap_tlbpid_destroy(next_pid, TRUE);
		}
got_a_free_one:
		pids_in_use[next_pid] = map;
		map->pid = next_pid;
		if (++next_pid == pmap_max_asn)
			next_pid = 1;
		pmap_next_pid = next_pid;
d3002 1
a3002 5
		simple_unlock(&tlbpid_lock);
	}
        if (DOVPDB(PDB_FOLLOW|PDB_TLBPID_ASSIGN))
                printf("pmap_tlbpid_assign: pmap %p got %d\n", map, map->pid);
}
d3004 12
a3015 3
pmap_tlbpid_destroy(pid, locked)
	int 		pid;
	boolean_t	locked;
d3017 2
a3018 1
	struct pmap    *map;
d3020 9
a3028 2
        if (DOVPDB(PDB_FOLLOW|PDB_TLBPID_DESTROY))
                printf("pmap_tlbpid_destroy(%d, %d)\n", pid, locked);
d3030 1
a3030 2
	if (pid < 0)	/* no longer in use */
		return;
d3032 2
a3033 1
	assert(pmap_max_asn);
d3035 14
a3048 1
	if (!locked) simple_lock(&tlbpid_lock);
d3051 1
a3051 1
	 * Make the pid available, and the map unassigned.
d3053 4
a3056 4
	map = pids_in_use[pid];
	assert(map != NULL);
	pids_in_use[pid] = PMAP_NULL;
	map->pid = -1;
d3058 1
a3058 1
	if (!locked) simple_unlock(&tlbpid_lock);
d3061 17
a3077 1
#if	1 /* DEBUG */
d3079 2
a3080 4
print_pv_list()
{
	pv_entry_t	p;
	vm_offset_t	phys;
d3082 7
a3088 10
	db_printf("phys pages %x < p < %x\n", vm_first_phys, vm_last_phys);
	for (phys = vm_first_phys; phys < vm_last_phys; phys += PAGE_SIZE) {
		p = pai_to_pvh(pa_index(phys));
		if (p->pmap != PMAP_NULL) {
			db_printf("%x: %x %x\n", phys, p->pmap, p->va);
			while (p = p->next)
				db_printf("\t\t%x %x\n", p->pmap, p->va);
		}
	}
}
d3090 3
d3095 1
a3095 6
vm_offset_t
pmap_phys_address(ppn)
	int ppn;
{
	return(alpha_ptob(ppn));
}
d3097 2
a3098 8
void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
        pmap_t          dst_pmap;
        pmap_t          src_pmap;
        vm_offset_t     dst_addr;
        vm_size_t       len;
        vm_offset_t     src_addr;
{
}
d3100 9
a3108 2
void pmap_update()
{
d3111 7
a3117 2
vm_page_t
vm_page_grab()
d3119 7
a3125 2
        register vm_page_t      mem;
        int             spl;
d3127 3
a3129 7
        spl = splimp();                         /* XXX */
        simple_lock(&vm_page_queue_free_lock);
        if (vm_page_queue_free.tqh_first == NULL) {
                simple_unlock(&vm_page_queue_free_lock);
                splx(spl);
                return (NULL);
        }
d3131 1
a3131 2
        mem = vm_page_queue_free.tqh_first;
        TAILQ_REMOVE(&vm_page_queue_free, mem, pageq);
d3133 45
a3177 3
        cnt.v_free_count--;
        simple_unlock(&vm_page_queue_free_lock);
        splx(spl);
d3179 1
a3179 2
        mem->flags = PG_BUSY | PG_CLEAN | PG_FAKE;
        mem->wire_count = 0;
d3181 16
a3196 10
        /*
         *      Decide if we should poke the pageout daemon.
         *      We do this if the free count is less than the low
         *      water mark, or if the free count is less than the high
         *      water mark (but above the low water mark) and the inactive
         *      count is less than its target.
         *
         *      We don't have the counts locked ... if they change a little,
         *      it doesn't really matter.
         */
d3198 1
a3198 5
        if (cnt.v_free_count < cnt.v_free_min ||
            (cnt.v_free_count < cnt.v_free_target &&
             cnt.v_inactive_count < cnt.v_inactive_target))
                thread_wakeup((void *)&vm_pages_needed);
        return (mem);
d3201 8
a3208 2
int
vm_page_wait()
d3211 1
a3211 2
	assert_wait(&cnt.v_free_count, 0);
	thread_block();
d3215 3
a3217 1
 * Emulate reference and/or modified bit hits.
d3219 4
a3222 6
void
pmap_emulate_reference(p, v, user, write)
        struct proc *p;
        vm_offset_t v;
        int user;
        int write;
d3224 5
a3228 1
	/* XXX */
d3231 11
a3241 1
struct pv_page;
d3243 2
a3244 5
struct pv_page_info {
        TAILQ_ENTRY(pv_page) pgi_list;
        struct pv_entry *pgi_freelist;
        int pgi_nfree;
};
d3246 1
a3246 1
#define NPVPPG  ((NBPG - sizeof(struct pv_page_info)) / sizeof(struct pv_entry))
d3248 19
a3266 4
struct pv_page {
        struct pv_page_info pvp_pgi;
        struct pv_entry pvp_pv[NPVPPG];
};
d3268 13
a3280 28
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int             pv_nfree;

#define pv_next next

struct pv_entry *
pmap_alloc_pv()
{
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;

	if (pv_nfree == 0) {
		pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: kmem_alloc() failed");
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
a3281 4
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
d3283 4
a3286 1
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
d3288 1
a3288 1
	return pv;
d3291 5
d3297 815
a4111 2
pmap_free_pv(pv)
	struct pv_entry *pv;
d4113 1
a4113 2
	register struct pv_page *pvp;
	register int i;
d4115 3
a4117 14
	pvp = (struct pv_page *) trunc_page(pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
		break;
d4119 1
d4122 13
a4134 28
#if 0
sanity(pmap, addr)
        register pmap_t         pmap;
        register vm_offset_t    addr;
{
        register pt_entry_t     *ptp;
        register pt_entry_t     pte;
	
	printf("checking dirbase...\n");
	assert(pmap->dirbase != 0);
	printf("checking dirpfn...\n");
	assert(pmap->dirpfn == curproc->p_addr->u_pcb.pcb_hw.apcb_ptbr);
	printf("checking pid...\n");
	assert(pmap->pid == curproc->p_addr->u_pcb.pcb_hw.apcb_asn);

	
        /* seg1 */
        pte = *pmap_pde(pmap,addr);
        if ((pte & ALPHA_PTE_VALID) == 0)
                return(PT_ENTRY_NULL);
        /* seg2 */
        ptp = (pt_entry_t *)ptetokv(pte);
        pte = ptp[pte2num(addr)];
        if ((pte & ALPHA_PTE_VALID) == 0)
                return(PT_ENTRY_NULL);
        /* seg3 */
        ptp = (pt_entry_t *)ptetokv(pte);
        return(&ptp[pte3num(addr)]);
d4136 6
d4143 17
d4161 4
@


1.6.4.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 2
a2 2
/* $OpenBSD: pmap.c,v 1.12 2001/04/10 06:59:13 niklas Exp $ */
/* $NetBSD: pmap.c,v 1.148 2000/09/22 05:23:37 thorpej Exp $ */
d4 23
a26 74
/*-
 * Copyright (c) 1998, 1999, 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center and by Chris G. Demetriou.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.6 (Berkeley) 5/27/94
d30 7
a36 1
 * DEC Alpha physical map management code.
d38 1
a38 1
 * History:
d40 1
a40 24
 *	This pmap started life as a Motorola 68851/68030 pmap,
 *	written by Mike Hibler at the University of Utah.
 *
 *	It was modified for the DEC Alpha by Chris Demetriou
 *	at Carnegie Mellon University.
 *
 *	Support for non-contiguous physical memory was added by
 *	Jason R. Thorpe of the Numerical Aerospace Simulation
 *	Facility, NASA Ames Research Center and Chris Demetriou.
 *
 *	Page table management and a major cleanup were undertaken
 *	by Jason R. Thorpe, with lots of help from Ross Harvey of
 *	Avalon Computer Systems and from Chris Demetriou.
 *
 *	Support for the new UVM pmap interface was written by
 *	Jason R. Thorpe.
 *
 *	Support for ASNs was written by Jason R. Thorpe, again
 *	with help from Chris Demetriou and Ross Harvey.
 *
 *	The locking protocol was written by Jason R. Thorpe,
 *	using Chuck Cranor's i386 pmap for UVM as a model.
 *
 *	TLB shootdown code was written by Jason R. Thorpe.
d42 4
a45 22
 * Notes:
 *
 *	All page table access is done via K0SEG.  The one exception
 *	to this is for kernel mappings.  Since all kernel page
 *	tables are pre-allocated, we can use the Virtual Page Table
 *	to access PTEs that map K1SEG addresses.
 *
 *	Kernel page table pages are statically allocated in
 *	pmap_bootstrap(), and are never freed.  In the future,
 *	support for dynamically adding additional kernel page
 *	table pages may be added.  User page table pages are
 *	dynamically allocated and freed.
 *
 *	This pmap implementation only supports NBPG == PAGE_SIZE.
 *	In practice, this is not a problem since PAGE_SIZE is
 *	initialized to the hardware page size in alpha_init().
 *
 * Bugs/misfeatures:
 *
 *	- Some things could be optimized.
 *
 *	- pmap_growkernel() should be implemented.
d49 6
a54 1
 *	Manages physical address maps.
a75 1
#include <sys/pool.h>
d82 4
a85 1
#include <uvm/uvm.h>
a86 1
#include <machine/atomic.h>
d88 24
a111 2
#if defined(_PMAP_MAY_USE_PROM_CONSOLE) || defined(MULTIPROCESSOR)
#include <machine/rpb.h>
d114 23
a136 15
#ifdef DEBUG
#define	PDB_FOLLOW	0x0001
#define	PDB_INIT	0x0002
#define	PDB_ENTER	0x0004
#define	PDB_REMOVE	0x0008
#define	PDB_CREATE	0x0010
#define	PDB_PTPAGE	0x0020
#define	PDB_ASN		0x0040
#define	PDB_BITS	0x0080
#define	PDB_COLLECT	0x0100
#define	PDB_PROTECT	0x0200
#define	PDB_BOOTSTRAP	0x1000
#define	PDB_PARANOIA	0x2000
#define	PDB_WIRING	0x4000
#define	PDB_PVDUMP	0x8000
a137 3
int debugmap = 0;
int pmapdebug = PDB_PARANOIA|PDB_FOLLOW|PDB_ENTER;
#endif
d140 68
a207 2
 * Given a map and a machine independent protection code,
 * convert to an alpha protection code.
d209 10
a218 2
#define pte_prot(m, p)	(protection_codes[m == pmap_kernel() ? 0 : 1][p])
int	protection_codes[2][8];
d221 3
a223 22
 * kernel_lev1map:
 *
 *	Kernel level 1 page table.  This maps all kernel level 2
 *	page table pages, and is used as a template for all user
 *	pmap level 1 page tables.  When a new user level 1 page
 *	table is allocated, all kernel_lev1map PTEs for kernel
 *	addresses are copied to the new map.
 *
 *	The kernel also has an initial set of kernel level 2 page
 *	table pages.  These map the kernel level 3 page table pages.
 *	As kernel level 3 page table pages are added, more level 2
 *	page table pages may be added to map them.  These pages are
 *	never freed.
 *
 *	Finally, the kernel also has an initial set of kernel level
 *	3 page table pages.  These map pages in K1SEG.  More level
 *	3 page table pages may be added at run-time if additional
 *	K1SEG address space is required.  These pages are never freed.
 *
 * NOTE: When mappings are inserted into the kernel pmap, all
 * level 2 and level 3 page table pages must already be allocated
 * and mapped into the parent page table.
d225 21
a245 1
pt_entry_t	*kernel_lev1map;
d248 3
a250 1
 * Virtual Page Table.
a251 1
pt_entry_t	*VPT;
d253 11
a263 3
struct pmap	kernel_pmap_store;
u_int		kernel_pmap_asn_store[ALPHA_MAXPROCS];
u_long		kernel_pmap_asngen_store[ALPHA_MAXPROCS];
d265 4
a268 4
paddr_t    	avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
d270 1
a270 1
boolean_t	pmap_initialized;	/* Has pmap_init completed? */
d272 3
a274 1
u_long		pmap_pages_stolen;	/* instrumentation */
d277 2
a278 3
 * This variable contains the number of CPU IDs we need to allocate
 * space for when allocating the pmap structure.  It is used to
 * size a per-CPU array of ASN and ASN Generation number.
d280 1
a280 1
u_long		pmap_ncpuids;
d283 1
a283 1
 * Storage for physical->virtual entries and page attributes.
d285 2
a286 2
struct pv_head	*pv_table;
int		pv_table_npages;
d288 1
a288 4
#ifndef PMAP_PV_LOWAT
#define	PMAP_PV_LOWAT	16
#endif
int		pmap_pv_lowat = PMAP_PV_LOWAT;
d291 2
a292 3
 * List of all pmaps, used to update them when e.g. additional kernel
 * page tables are allocated.  This list is kept LRU-ordered by
 * pmap_activate().
d294 3
a296 1
TAILQ_HEAD(, pmap) pmap_all_pmaps;
d299 2
a300 1
 * The pools from which pmap structures and sub-structures are allocated.
d302 1
a302 4
struct pool pmap_pmap_pool;
struct pool pmap_asn_pool;
struct pool pmap_asngen_pool;
struct pool pmap_pv_pool;
d305 1
a305 1
 * Canonical names for PGU_* constants.
a306 1
const char *pmap_pgu_strings[] = PGU_STRINGS;
d309 1
a309 1
 * Address Space Numbers.
d311 10
a320 8
 * On many implementations of the Alpha architecture, the TLB entries and
 * I-cache blocks are tagged with a unique number within an implementation-
 * specified range.  When a process context becomes active, the ASN is used
 * to match TLB entries; if a TLB entry for a particular VA does not match
 * the current ASN, it is ignored (one could think of the processor as
 * having a collection of <max ASN> separate TLBs).  This allows operating
 * system software to skip the TLB flush that would otherwise be necessary
 * at context switch time.
d322 2
a323 6
 * Alpha PTEs have a bit in them (PG_ASM - Address Space Match) that
 * causes TLB entries to match any ASN.  The PALcode also provides
 * a TBI (Translation Buffer Invalidate) operation that flushes all
 * TLB entries that _do not_ have PG_ASM.  We use this bit for kernel
 * mappings, so that invalidation of all user mappings does not invalidate
 * kernel mappings (which are consistent across all processes).
d325 4
a328 2
 * pmap_next_asn always indicates to the next ASN to use.  When
 * pmap_next_asn exceeds pmap_max_asn, we start a new ASN generation.
d330 4
a333 3
 * When a new ASN generation is created, the per-process (i.e. non-PG_ASM)
 * TLB entries and the I-cache are flushed, the generation number is bumped,
 * and pmap_next_asn is changed to indicate the first non-reserved ASN.
d335 5
a339 34
 * We reserve ASN #0 for pmaps that use the global kernel_lev1map.  This
 * prevents the following scenario:
 *
 *	* New ASN generation starts, and process A is given ASN #0.
 *
 *	* A new process B (and thus new pmap) is created.  The ASN,
 *	  for lack of a better value, is initialized to 0.
 *
 *	* Process B runs.  It is now using the TLB entries tagged
 *	  by process A.  *poof*
 *
 * In the scenario above, in addition to the processor using using incorrect
 * TLB entires, the PALcode might use incorrect information to service a
 * TLB miss.  (The PALcode uses the recursively mapped Virtual Page Table
 * to locate the PTE for a faulting address, and tagged TLB entires exist
 * for the Virtual Page Table addresses in order to speed up this procedure,
 * as well.)
 *
 * By reserving an ASN for kernel_lev1map users, we are guaranteeing that
 * new pmaps will initially run with no TLB entries for user addresses
 * or VPT mappings that map user page tables.  Since kernel_lev1map only
 * contains mappings for kernel addresses, and since those mappings
 * are always made with PG_ASM, sharing an ASN for kernel_lev1map users is
 * safe (since PG_ASM mappings match any ASN).
 *
 * On processors that do not support ASNs, the PALcode invalidates
 * the TLB and I-cache automatically on swpctx.  We still still go
 * through the motions of assigning an ASN (really, just refreshing
 * the ASN generation in this particular case) to keep the logic sane
 * in other parts of the code.
 */
u_int	pmap_max_asn;			/* max ASN supported by the system */
u_int	pmap_next_asn[ALPHA_MAXPROCS];	/* next free ASN to use */
u_long	pmap_asn_generation[ALPHA_MAXPROCS]; /* current ASN generation */
d341 1
d343 8
a350 56
 * Locking:
 *
 *	This pmap module uses two types of locks: `normal' (sleep)
 *	locks and `simple' (spin) locks.  They are used as follows:
 *
 *	READ/WRITE SPIN LOCKS
 *	---------------------
 *
 *	* pmap_main_lock - This lock is used to prevent deadlock and/or
 *	  provide mutex access to the pmap module.  Most operations lock
 *	  the pmap first, then PV lists as needed.  However, some operations,
 *	  such as pmap_page_protect(), lock the PV lists before locking
 *	  the pmaps.  To prevent deadlock, we require a mutex lock on the
 *	  pmap module if locking in the PV->pmap direction.  This is
 *	  implemented by acquiring a (shared) read lock on pmap_main_lock
 *	  if locking pmap->PV and a (exclusive) write lock if locking in
 *	  the PV->pmap direction.  Since only one thread can hold a write
 *	  lock at a time, this provides the mutex.
 *
 *	SIMPLE LOCKS
 *	------------
 *
 *	* pm_slock (per-pmap) - This lock protects all of the members
 *	  of the pmap structure itself.  This lock will be asserted
 *	  in pmap_activate() and pmap_deactivate() from a critical
 *	  section of cpu_switch(), and must never sleep.  Note that
 *	  in the case of the kernel pmap, interrupts which cause
 *	  memory allocation *must* be blocked while this lock is
 *	  asserted.
 *
 *	* pvh_slock (per-pv_head) - This lock protects the PV list
 *	  for a specified managed page.
 *
 *	* pmap_all_pmaps_slock - This lock protects the global list of
 *	  all pmaps.  Note that a pm_slock must never be held while this
 *	  lock is held.
 *
 *	* pmap_growkernel_slock - This lock protects pmap_growkernel()
 *	  and the virtual_end variable.
 *
 *	Address space number management (global ASN counters and per-pmap
 *	ASN state) are not locked; they use arrays of values indexed
 *	per-processor.
 *
 *	All internal functions which operate on a pmap are called
 *	with the pmap already locked by the caller (which will be
 *	an interface function).
 */
struct lock pmap_main_lock;
struct simplelock pmap_all_pmaps_slock;
struct simplelock pmap_growkernel_slock;

#ifdef __OpenBSD__
#define spinlockinit(lock, name, flags)  lockinit(lock, 0, name, 0, flags)
#define spinlockmgr(lock, flags, slock) lockmgr(lock, flags, slock, curproc)
#endif
d352 4
a355 15
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
#define	PMAP_MAP_TO_HEAD_LOCK() \
	spinlockmgr(&pmap_main_lock, LK_SHARED, NULL)
#define	PMAP_MAP_TO_HEAD_UNLOCK() \
	spinlockmgr(&pmap_main_lock, LK_RELEASE, NULL)
#define	PMAP_HEAD_TO_MAP_LOCK() \
	spinlockmgr(&pmap_main_lock, LK_EXCLUSIVE, NULL)
#define	PMAP_HEAD_TO_MAP_UNLOCK() \
	spinlockmgr(&pmap_main_lock, LK_RELEASE, NULL)
#else
#define	PMAP_MAP_TO_HEAD_LOCK()		/* nothing */
#define	PMAP_MAP_TO_HEAD_UNLOCK()	/* nothing */
#define	PMAP_HEAD_TO_MAP_LOCK()		/* nothing */
#define	PMAP_HEAD_TO_MAP_UNLOCK()	/* nothing */
#endif /* MULTIPROCESSOR || LOCKDEBUG */
a356 1
#if defined(MULTIPROCESSOR)
d358 1
a358 13
 * TLB Shootdown:
 *
 * When a mapping is changed in a pmap, the TLB entry corresponding to
 * the virtual address must be invalidated on all processors.  In order
 * to accomplish this on systems with multiple processors, messages are
 * sent from the processor which performs the mapping change to all
 * processors on which the pmap is active.  For other processors, the
 * ASN generation numbers for that processor is invalidated, so that
 * the next time the pmap is activated on that processor, a new ASN
 * will be allocated (which implicitly invalidates all TLB entries).
 *
 * Note, we can use the pool allocator to allocate job entries
 * since pool pages are mapped with K0SEG, not with the TLB.
d360 1
a360 6
struct pmap_tlb_shootdown_job {
	TAILQ_ENTRY(pmap_tlb_shootdown_job) pj_list;
	vaddr_t pj_va;			/* virtual address */
	pmap_t pj_pmap;			/* the pmap which maps the address */
	pt_entry_t pj_pte;		/* the PTE bits */
};
d362 1
a362 6
struct pmap_tlb_shootdown_q {
	TAILQ_HEAD(, pmap_tlb_shootdown_job) pq_head;
	int pq_pte;			/* aggregate PTE bits */
	int pq_count;			/* number of pending requests */
	struct simplelock pq_slock;	/* spin lock on queue */
} pmap_tlb_shootdown_q[ALPHA_MAXPROCS];
d364 5
a368 5
#define	PSJQ_LOCK(pq, s)						\
do {									\
	s = splimp();							\
	simple_lock(&(pq)->pq_slock);					\
} while (0)
d370 4
a373 5
#define	PSJQ_UNLOCK(pq, s)						\
do {									\
	simple_unlock(&(pq)->pq_slock);					\
	splx(s);							\
} while (0)
d375 5
a379 2
/* If we have more pending jobs than this, we just nail the whole TLB. */
#define	PMAP_TLB_SHOOTDOWN_MAXJOBS	6
d381 4
a384 1
struct pool pmap_tlb_shootdown_job_pool;
d386 4
a389 5
struct pmap_tlb_shootdown_job *pmap_tlb_shootdown_job_get
	    (struct pmap_tlb_shootdown_q *);
void	pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *,
	    struct pmap_tlb_shootdown_job *);
#endif /* MULTIPROCESSOR */
d391 1
a391 1
#define	PAGE_IS_MANAGED(pa)	(vm_physseg_find(atop(pa), NULL) != -1)
d393 1
a393 4
static __inline struct pv_head *
pa_to_pvh(paddr_t pa)
{
	int bank, pg;
d395 22
a416 2
	bank = vm_physseg_find(atop(pa), &pg);
	return (&vm_physmem[bank].pmseg.pvhead[pg]);
d419 26
a444 9
/*
 * Optional argument passed to pmap_remove_mapping() for stealing mapping
 * resources.
 */
struct prm_thief {
	int	prmt_flags;		/* flags; what to steal */
	struct pv_entry *prmt_pv;	/* the stolen PV entry */
	pt_entry_t *prmt_ptp;		/* the stolen PT page */
};
d446 1
a446 2
#define	PRMT_PV		0x0001		/* steal the PV entry */
#define	PRMT_PTP	0x0002		/* steal the PT page */
d448 11
a458 34
/*
 * Internal routines
 */
void	alpha_protection_init(void);
void	pmap_do_remove(pmap_t, vaddr_t, vaddr_t, boolean_t);
boolean_t pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *,
	    boolean_t, long, struct prm_thief *);
void	pmap_changebit(paddr_t, pt_entry_t, pt_entry_t, long);

/*
 * PT page management functions.
 */
int	pmap_lev1map_create(pmap_t, long);
void	pmap_lev1map_destroy(pmap_t, long);
int	pmap_ptpage_alloc(pmap_t, pt_entry_t *, int);
boolean_t pmap_ptpage_steal(pmap_t, int, paddr_t *);
void	pmap_ptpage_free(pmap_t, pt_entry_t *, pt_entry_t **);
void	pmap_l3pt_delref(pmap_t, vaddr_t, pt_entry_t *, long,
	    pt_entry_t **);
void	pmap_l2pt_delref(pmap_t, pt_entry_t *, pt_entry_t *, long);
void	pmap_l1pt_delref(pmap_t, pt_entry_t *, long);

/*
 * PV table management functions.
 */
int	pmap_pv_enter(pmap_t, paddr_t, vaddr_t, pt_entry_t *, boolean_t);
void	pmap_pv_remove(pmap_t, paddr_t, vaddr_t, boolean_t,
	    struct pv_entry **);
struct	pv_entry *pmap_pv_alloc(void);
void	pmap_pv_free(struct pv_entry *);
void	*pmap_pv_page_alloc(u_long, int, int);
void	pmap_pv_page_free(void *, u_long, int);
#ifdef DEBUG
void	pmap_pv_dump(paddr_t);
d461 5
d467 1
a467 1
 * ASN management functions.
d469 10
a478 1
void	pmap_asn_alloc(pmap_t, long);
d481 2
a482 1
 * Misc. functions.
d484 10
a493 4
boolean_t pmap_physpage_alloc(int, paddr_t *);
void	pmap_physpage_free(paddr_t);
int	pmap_physpage_addref(void *);
int	pmap_physpage_delref(void *);
d496 1
a496 3
 * PMAP_ISACTIVE{,_TEST}:
 *
 *	Check to see if a pmap is active on the current processor.
d498 5
a502 2
#define	PMAP_ISACTIVE_TEST(pm, cpu_id)					\
	(((pm)->pm_cpus & (1UL << (cpu_id))) != 0)
d504 46
a549 18
#if defined(DEBUG) && !defined(MULTIPROCESSOR)
#define	PMAP_ISACTIVE(pm, cpu_id)					\
({									\
	/*								\
	 * XXX This test is not MP-safe.				\
	 */								\
	int isactive_ = PMAP_ISACTIVE_TEST(pm, cpu_id);			\
									\
	if (curproc != NULL && curproc->p_vmspace != NULL &&		\
	    (pm) != pmap_kernel() &&					\
	    (isactive_ ^ ((pm) == curproc->p_vmspace->vm_map.pmap)))	\
		panic("PMAP_ISACTIVE, isa: %d pm: %p curpm:%p\n",	\
		    isactive_, (pm), curproc->p_vmspace->vm_map.pmap);	\
	(isactive_);							\
})
#else
#define	PMAP_ISACTIVE(pm, cpu_id)	PMAP_ISACTIVE_TEST(pm, cpu_id)
#endif /* DEBUG && !MULTIPROCESSOR */
d551 2
a552 44
/*
 * PMAP_ACTIVATE_ASN_SANITY:
 *
 *	DEBUG sanity checks for ASNs within PMAP_ACTIVATE.
 */
#ifdef DEBUG
#define	PMAP_ACTIVATE_ASN_SANITY(pmap, cpu_id)				\
do {									\
	if ((pmap)->pm_lev1map == kernel_lev1map) {			\
		/*							\
		 * This pmap implementation also ensures that pmaps	\
		 * referencing kernel_lev1map use a reserved ASN	\
		 * ASN to prevent the PALcode from servicing a TLB	\
		 * miss	with the wrong PTE.				\
		 */							\
		if ((pmap)->pm_asn[(cpu_id)] != PMAP_ASN_RESERVED) {	\
			printf("kernel_lev1map with non-reserved ASN "	\
			    "(line %d)\n", __LINE__);			\
			panic("PMAP_ACTIVATE_ASN_SANITY");		\
		}							\
	} else {							\
		if ((pmap)->pm_asngen[(cpu_id)] != 			\
		    pmap_asn_generation[(cpu_id)]) {			\
			/*						\
			 * ASN generation number isn't valid!		\
			 */						\
			printf("pmap asngen %lu, current %lu "		\
			    "(line %d)\n",				\
			    (pmap)->pm_asngen[(cpu_id)], 		\
			    pmap_asn_generation[(cpu_id)],		\
			    __LINE__);					\
			panic("PMAP_ACTIVATE_ASN_SANITY");		\
		}							\
		if ((pmap)->pm_asn[(cpu_id)] == PMAP_ASN_RESERVED) {	\
			/*						\
			 * DANGER WILL ROBINSON!  We're going to	\
			 * pollute the VPT TLB entries!			\
			 */						\
			printf("Using reserved ASN! (line %d)\n",	\
			    __LINE__);					\
			panic("PMAP_ACTIVATE_ASN_SANITY");		\
		}							\
	}								\
} while (0)
d554 1
a554 1
#define	PMAP_ACTIVATE_ASN_SANITY(pmap, cpu_id)	/* nothing */
d556 1
d559 3
a561 1
 * PMAP_ACTIVATE:
d563 1
a563 6
 *	This is essentially the guts of pmap_activate(), without
 *	ASN allocation.  This is used by pmap_activate(),
 *	pmap_lev1map_create(), and pmap_lev1map_destroy().
 *
 *	This is called only when it is known that a pmap is "active"
 *	on the current processor; the ASN must already be valid.
d565 11
a575 16
#define	PMAP_ACTIVATE(pmap, p, cpu_id)					\
do {									\
	PMAP_ACTIVATE_ASN_SANITY(pmap, cpu_id);				\
									\
	(p)->p_addr->u_pcb.pcb_hw.apcb_ptbr =				\
	    ALPHA_K0SEG_TO_PHYS((vaddr_t)(pmap)->pm_lev1map) >> PGSHIFT; \
	(p)->p_addr->u_pcb.pcb_hw.apcb_asn = (pmap)->pm_asn[(cpu_id)];	\
									\
	if ((p) == curproc) {						\
		/*							\
		 * Page table base register has changed; switch to	\
		 * our own context again so that it will take effect.	\
		 */							\
		(void) alpha_pal_swpctx((u_long)p->p_md.md_pcbpaddr);	\
	}								\
} while (0)
d577 6
a582 7
/*
 * PMAP_SET_NEEDISYNC:
 *
 *	Mark that a user pmap needs an I-stream synch on its
 *	way back out to userspace.
 */
#define	PMAP_SET_NEEDISYNC(pmap)	(pmap)->pm_needisync = ~0UL
d584 8
a591 31
/*
 * PMAP_SYNC_ISTREAM:
 *
 *	Synchronize the I-stream for the specified pmap.  For user
 *	pmaps, this is deferred until a process using the pmap returns
 *	to userspace.
 */
#if defined(MULTIPROCESSOR)
#define	PMAP_SYNC_ISTREAM_KERNEL()					\
do {									\
	alpha_pal_imb();						\
	alpha_broadcast_ipi(ALPHA_IPI_IMB);				\
} while (0)

#define	PMAP_SYNC_ISTREAM_USER(pmap)					\
do {									\
	alpha_multicast_ipi((pmap)->pm_cpus, ALPHA_IPI_AST);		\
	/* for curcpu, will happen in userret() */			\
} while (0)
#else
#define	PMAP_SYNC_ISTREAM_KERNEL()	alpha_pal_imb()
#define	PMAP_SYNC_ISTREAM_USER(pmap)	/* will happen in userret() */
#endif /* MULTIPROCESSOR */

#define	PMAP_SYNC_ISTREAM(pmap)						\
do {									\
	if ((pmap) == pmap_kernel())					\
		PMAP_SYNC_ISTREAM_KERNEL();				\
	else								\
		PMAP_SYNC_ISTREAM_USER(pmap);				\
} while (0)
d593 9
a601 17
/*
 * PMAP_INVALIDATE_ASN:
 *
 *	Invalidate the specified pmap's ASN, so as to force allocation
 *	of a new one the next time pmap_asn_alloc() is called.
 *
 *	NOTE: THIS MUST ONLY BE CALLED IF AT LEAST ONE OF THE FOLLOWING
 *	CONDITIONS ARE TRUE:
 *
 *		(1) The pmap references the global kernel_lev1map.
 *
 *		(2) The pmap is not active on the current processor.
 */
#define	PMAP_INVALIDATE_ASN(pmap, cpu_id)				\
do {									\
	(pmap)->pm_asn[(cpu_id)] = PMAP_ASN_RESERVED;			\
} while (0)
d603 3
a605 29
/*
 * PMAP_INVALIDATE_TLB:
 *
 *	Invalidate the TLB entry for the pmap/va pair.
 */
#define	PMAP_INVALIDATE_TLB(pmap, va, hadasm, isactive, cpu_id)		\
do {									\
	if ((hadasm) || (isactive)) {					\
		/*							\
		 * Simply invalidating the TLB entry and I-cache	\
		 * works in this case.					\
		 */							\
		ALPHA_TBIS((va));					\
	} else if ((pmap)->pm_asngen[(cpu_id)] == 			\
	    pmap_asn_generation[(cpu_id)]) {				\
		/*							\
		 * We can't directly invalidate the TLB entry		\
		 * in this case, so we have to force allocation		\
		 * of a new ASN the next time this pmap becomes		\
		 * active.						\
		 */							\
		PMAP_INVALIDATE_ASN((pmap), (cpu_id));			\
	}								\
		/*							\
		 * Nothing to do in this case; the next time the	\
		 * pmap becomes active on this processor, a new		\
		 * ASN will be allocated anyway.			\
		 */							\
} while (0)
d607 5
a611 31
/*
 * PMAP_KERNEL_PTE:
 *
 *	Get a kernel PTE.
 *
 *	If debugging, do a table walk.  If not debugging, just use
 *	the Virtual Page Table, since all kernel page tables are
 *	pre-allocated and mapped in.
 */
#ifdef DEBUG
#define	PMAP_KERNEL_PTE(va)						\
({									\
	pt_entry_t *l1pte_, *l2pte_;					\
									\
	l1pte_ = pmap_l1pte(pmap_kernel(), va);				\
	if (pmap_pte_v(l1pte_) == 0) {					\
		printf("kernel level 1 PTE not valid, va 0x%lx "	\
		    "(line %d)\n", (va), __LINE__);			\
		panic("PMAP_KERNEL_PTE");				\
	}								\
	l2pte_ = pmap_l2pte(pmap_kernel(), va, l1pte_);			\
	if (pmap_pte_v(l2pte_) == 0) {					\
		printf("kernel level 2 PTE not valid, va 0x%lx "	\
		    "(line %d)\n", (va), __LINE__);			\
		panic("PMAP_KERNEL_PTE");				\
	}								\
	pmap_l3pte(pmap_kernel(), va, l2pte_);				\
})
#else
#define	PMAP_KERNEL_PTE(va)	(&VPT[VPT_INDEX((va))])
#endif
d613 1
a613 6
/*
 * PMAP_SET_PTE:
 *
 *	Set a PTE to a specified value.
 */
#define	PMAP_SET_PTE(ptep, val)	*(ptep) = (val)
d615 2
a616 7
/*
 * PMAP_STAT_{INCR,DECR}:
 *
 *	Increment or decrement a pmap statistic.
 */
#define	PMAP_STAT_INCR(s, v)	atomic_add_ulong((unsigned long *)(&(s)), (v))
#define	PMAP_STAT_DECR(s, v)	atomic_sub_ulong((unsigned long *)(&(s)), (v))
d619 3
a621 1
 * pmap_bootstrap:
d623 5
a627 1
 *	Bootstrap the system to run with virtual memory.
a628 1
 *	Note: no locking is necessary in this function.
d630 8
d639 3
a641 1
pmap_bootstrap(paddr_t ptaddr, u_int maxasn, u_long ncpuids)
d643 5
a647 3
	vsize_t lev2mapsize, lev3mapsize;
	pt_entry_t *lev2map, *lev3map;
	pt_entry_t pte;
d649 1
d651 3
a653 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_BOOTSTRAP))
		printf("pmap_bootstrap(0x%lx, %u)\n", ptaddr, maxasn);
#endif
d655 2
a656 9
	/*
	 * Figure out how many PTE's are necessary to map the kernel.
	 * The '512' comes from PAGER_MAP_SIZE in vm_pager_init().
	 * This should be kept in sync.
	 * We also reserve space for kmem_alloc_pageable() for vm_fork().
	 */
	lev3mapsize = (VM_PHYS_SIZE +
		nbuf * MAXBSIZE + 16 * NCARGS) / NBPG + 512 +
		(maxproc * UPAGES) + NKMEMCLUSTERS;
d658 6
a663 4
#ifdef SYSVSHM
	lev3mapsize += shminfo.shmall;
#endif
	lev3mapsize = roundup(lev3mapsize, NPTEPG);
d666 1
a666 3
	 * Allocate a level 1 PTE table for the kernel.
	 * This is always one page long.
	 * IF THIS IS NOT A MULTIPLE OF NBPG, ALL WILL GO TO HELL.
d668 1
a668 2
	kernel_lev1map = (pt_entry_t *)
	    pmap_steal_memory(sizeof(pt_entry_t) * NPTEPG, NULL, NULL);
d671 1
a671 3
	 * Allocate a level 2 PTE table for the kernel.
	 * These must map all of the level3 PTEs.
	 * IF THIS IS NOT A MULTIPLE OF NBPG, ALL WILL GO TO HELL.
d673 1
a673 3
	lev2mapsize = roundup(howmany(lev3mapsize, NPTEPG), NPTEPG);
	lev2map = (pt_entry_t *)
	    pmap_steal_memory(sizeof(pt_entry_t) * lev2mapsize, NULL, NULL);
d676 3
a678 2
	 * Allocate a level 3 PTE table for the kernel.
	 * Contains lev3mapsize PTEs.
d680 10
a689 2
	lev3map = (pt_entry_t *)
	    pmap_steal_memory(sizeof(pt_entry_t) * lev3mapsize, NULL, NULL);
d692 2
a693 2
	 * Allocate memory for the pv_heads.  (A few more of the latter
	 * are allocated than are needed.)
d695 1
a695 4
	 * We could do this in pmap_init when we know the actual
	 * managed page pool size, but its better to use kseg0
	 * addresses rather than kernel virtual addresses mapped
	 * through the TLB.
a696 4
	pv_table_npages = physmem;
	pv_table = (struct pv_head *)
	    pmap_steal_memory(sizeof(struct pv_head) * pv_table_npages,
	    NULL, NULL);
d698 26
d725 1
a725 1
	 * ...and intialize the pv_entry list headers.
d727 3
a729 4
	for (i = 0; i < pv_table_npages; i++) {
		LIST_INIT(&pv_table[i].pvh_list);
		simple_lock_init(&pv_table[i].pvh_slock);
	}
d731 1
d733 4
a736 1
	 * Set up level 1 page table
d738 1
d740 7
a746 8
	/* Map all of the level 2 pte pages */
	for (i = 0; i < howmany(lev2mapsize, NPTEPG); i++) {
		pte = (ALPHA_K0SEG_TO_PHYS(((vaddr_t)lev2map) +
		    (i*PAGE_SIZE)) >> PGSHIFT) << PG_SHIFT;
		pte |= PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		kernel_lev1map[l1pte_index(VM_MIN_KERNEL_ADDRESS +
		    (i*PAGE_SIZE*NPTEPG*NPTEPG))] = pte;
	}
d748 1
a748 6
	/* Map the virtual page table */
	pte = (ALPHA_K0SEG_TO_PHYS((vaddr_t)kernel_lev1map) >> PGSHIFT)
	    << PG_SHIFT;
	pte |= PG_V | PG_KRE | PG_KWE; /* NOTE NO ASM */
	kernel_lev1map[l1pte_index(VPTBASE)] = pte;
	VPT = (pt_entry_t *)VPTBASE;
d750 3
a752 4
#ifdef _PMAP_MAY_USE_PROM_CONSOLE
    {
	extern pt_entry_t prom_pte;			/* XXX */
	extern int prom_mapped;				/* XXX */
d754 11
a764 8
	if (pmap_uses_prom_console()) {
		/*
		 * XXX Save old PTE so we can remap the PROM, if
		 * XXX necessary.
		 */
		prom_pte = *(pt_entry_t *)ptaddr & ~PG_ASM;
	}
	prom_mapped = 0;
d767 3
a769 2
	 * Actually, this code lies.  The prom is still mapped, and will
	 * remain so until the context switch after alpha_init() returns.
a770 2
    }
#endif
d773 1
a773 1
	 * Set up level 2 page table.
d775 15
a789 7
	/* Map all of the level 3 pte pages */
	for (i = 0; i < howmany(lev3mapsize, NPTEPG); i++) {
		pte = (ALPHA_K0SEG_TO_PHYS(((vaddr_t)lev3map) +
		    (i*PAGE_SIZE)) >> PGSHIFT) << PG_SHIFT;
		pte |= PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		lev2map[l2pte_index(VM_MIN_KERNEL_ADDRESS+
		    (i*PAGE_SIZE*NPTEPG))] = pte;
a791 3
	/* Initialize the pmap_growkernel_slock. */
	simple_lock_init(&pmap_growkernel_slock);

d793 1
a793 1
	 * Set up level three page table (lev3map)
d795 6
a800 1
	/* Nothing to do; it's already zero'd */
d803 2
a804 3
	 * Initialize `FYI' variables.  Note we're relying on
	 * the fact that BSEARCH sorts the vm_physmem[] array
	 * for us.
d806 2
a807 4
	avail_start = ptoa(vm_physmem[0].start);
	avail_end = ptoa(vm_physmem[vm_nphysseg - 1].end);
	virtual_avail = VM_MIN_KERNEL_ADDRESS;
	virtual_end = VM_MIN_KERNEL_ADDRESS + lev3mapsize * PAGE_SIZE;
d809 2
a810 6
#if 0
	printf("avail_start = 0x%lx\n", avail_start);
	printf("avail_end = 0x%lx\n", avail_end);
	printf("virtual_avail = 0x%lx\n", virtual_avail);
	printf("virtual_end = 0x%lx\n", virtual_end);
#endif
d812 2
a813 14
	/*
	 * Intialize the pmap pools and list.
	 */
	pmap_ncpuids = ncpuids;
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
	pool_init(&pmap_asn_pool, pmap_ncpuids * sizeof(u_int), 0, 0, 0,
	    "pmasnpl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
	pool_init(&pmap_asngen_pool, pmap_ncpuids * sizeof(u_long), 0, 0, 0,
	    "pmasngenpl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
	    0, pmap_pv_page_alloc, pmap_pv_page_free, M_VMPMAP);
d815 2
a816 1
	TAILQ_INIT(&pmap_all_pmaps);
d818 2
a819 8
	/*
	 * Initialize the ASN logic.
	 */
	pmap_max_asn = maxasn;
	for (i = 0; i < ALPHA_MAXPROCS; i++) {
		pmap_next_asn[i] = 1;
		pmap_asn_generation[i] = 0;
	}
d821 7
a827 5
	/*
	 * Initialize the locks.
	 */
	spinlockinit(&pmap_main_lock, "pmaplk", 0);
	simple_lock_init(&pmap_all_pmaps_slock);
d830 1
a830 5
	 * Initialize kernel pmap.  Note that all kernel mappings
	 * have PG_ASM set, so the ASN doesn't really matter for
	 * the kernel pmap.  Also, since the kernel pmap always
	 * references kernel_lev1map, it always has an invalid ASN
	 * generation.
a831 11
	memset(pmap_kernel(), 0, sizeof(struct pmap));
	pmap_kernel()->pm_lev1map = kernel_lev1map;
	pmap_kernel()->pm_count = 1;
	pmap_kernel()->pm_asn = kernel_pmap_asn_store;
	pmap_kernel()->pm_asngen = kernel_pmap_asngen_store;
	for (i = 0; i < ALPHA_MAXPROCS; i++) {
		pmap_kernel()->pm_asn[i] = PMAP_ASN_RESERVED;
		pmap_kernel()->pm_asngen[i] = pmap_asn_generation[i];
	}
	simple_lock_init(&pmap_kernel()->pm_slock);
	TAILQ_INSERT_TAIL(&pmap_all_pmaps, pmap_kernel(), pm_list);
d833 5
a837 10
#if defined(MULTIPROCESSOR)
	/*
	 * Initialize the TLB shootdown queues.
	 */
	pool_init(&pmap_tlb_shootdown_job_pool,
	    sizeof(struct pmap_tlb_shootdown_job), 0, 0, 0, "pmaptlbpl",
	    0, NULL, NULL, M_VMPMAP);
	for (i = 0; i < ALPHA_MAXPROCS; i++) {
		TAILQ_INIT(&pmap_tlb_shootdown_q[i].pq_head);
		simple_lock_init(&pmap_tlb_shootdown_q[i].pq_slock);
a838 1
#endif
d841 3
a843 2
	 * Set up proc0's PCB such that the ptbr points to the right place
	 * and has the kernel pmap's (really unused) ASN.
d845 2
a846 4
	proc0.p_addr->u_pcb.pcb_hw.apcb_ptbr =
	    ALPHA_K0SEG_TO_PHYS((vaddr_t)kernel_lev1map) >> PGSHIFT;
	proc0.p_addr->u_pcb.pcb_hw.apcb_asn =
	    pmap_kernel()->pm_asn[cpu_number()];
d848 2
a849 5
	/*
	 * Mark the kernel pmap `active' on this processor.
	 */
	atomic_setbits_ulong(&pmap_kernel()->pm_cpus,
	    (1UL << cpu_number()));
d852 1
a852 3
#ifdef _PMAP_MAY_USE_PROM_CONSOLE
int
pmap_uses_prom_console(void)
d854 7
a860 8

#if defined(NEW_SCC_DRIVER)
	return (cputype == ST_DEC_21000);
#else
	return (cputype == ST_DEC_21000
	    || cputype == ST_DEC_3000_300
	    || cputype == ST_DEC_3000_500);
#endif /* NEW_SCC_DRIVER */
a861 1
#endif _PMAP_MAY_USE_PROM_CONSOLE
d864 6
a869 1
 * pmap_steal_memory:		[ INTERFACE ]
d871 3
a873 16
 *	Bootstrap memory allocator (alternative to vm_bootstrap_steal_memory()).
 *	This function allows for early dynamic memory allocation until the
 *	virtual memory system has been bootstrapped.  After that point, either
 *	kmem_alloc or malloc should be used.  This function works by stealing
 *	pages from the (to be) managed page pool, then implicitly mapping the
 *	pages (by using their k0seg addresses) and zeroing them.
 *
 *	It may be used once the physical memory segments have been pre-loaded
 *	into the vm_physmem[] array.  Early memory allocation MUST use this
 *	interface!  This cannot be used after vm_page_startup(), and will
 *	generate a panic if tried.
 *
 *	Note that this memory will never be freed, and in essence it is wired
 *	down.
 *
 *	Note: no locking is necessary in this function.
d875 4
a878 2
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
d880 2
a881 3
	int bank, npgs, x;
	vaddr_t va;
	paddr_t pa;
d883 4
a886 2
	size = round_page(size);
	npgs = atop(size);
d888 5
a892 3
#if 0
	printf("PSM: size 0x%lx (npgs 0x%x)\n", size, npgs);
#endif
d894 1
a894 3
	for (bank = 0; bank < vm_nphysseg; bank++) {
		if (uvm.page_init_done == TRUE)
			panic("pmap_steal_memory: called _after_ bootstrap");
d896 4
d901 13
a913 5
#if 0
		printf("     bank %d: avail_start 0x%lx, start 0x%lx, "
		    "avail_end 0x%lx\n", bank, vm_physmem[bank].avail_start,
		    vm_physmem[bank].start, vm_physmem[bank].avail_end);
#endif
d915 2
a916 3
		if (vm_physmem[bank].avail_start != vm_physmem[bank].start ||
		    vm_physmem[bank].avail_start >= vm_physmem[bank].avail_end)
			continue;
d918 15
a932 4
#if 0
		printf("             avail_end - avail_start = 0x%lx\n",
		    vm_physmem[bank].avail_end - vm_physmem[bank].avail_start);
#endif
d934 22
a955 3
		if ((vm_physmem[bank].avail_end - vm_physmem[bank].avail_start)
		    < npgs)
			continue;
d957 6
a962 6
		/*
		 * There are enough pages here; steal them!
		 */
		pa = ptoa(vm_physmem[bank].avail_start);
		vm_physmem[bank].avail_start += npgs;
		vm_physmem[bank].start += npgs;
d964 3
a966 14
		/*
		 * Have we used up this segment?
		 */
		if (vm_physmem[bank].avail_start == vm_physmem[bank].end) {
			if (vm_nphysseg == 1)
				panic("pmap_steal_memory: out of memory!");

			/* Remove this segment from the list. */
			vm_nphysseg--;
			for (x = bank; x < vm_nphysseg; x++) {
				/* structure copy */
				vm_physmem[x] = vm_physmem[x + 1];
			}
		}
d968 1
a968 8
		/*
		 * Fill these in for the caller; we don't modify them,
		 * but the upper layers still want to know.
		 */
		if (vstartp)
			*vstartp = round_page(virtual_avail);
		if (vendp)
			*vendp = VM_MAX_KERNEL_ADDRESS;
d970 1
a970 5
		va = ALPHA_PHYS_TO_K0SEG(pa);
		memset((caddr_t)va, 0, size);
		pmap_pages_stolen += npgs;
		return (va);
	}
d973 4
a976 1
	 * If we got here, this was no memory left.
d978 4
a981 1
	panic("pmap_steal_memory: no memory to steal");
d984 3
d988 1
a988 1
 * pmap_init:			[ INTERFACE ]
d990 1
a990 2
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
d992 2
a993 1
 *	Note: no locking is necessary in this function.
d995 2
a996 2
void
pmap_init(void)
d998 4
a1001 8
	vsize_t		s;
	int		bank;
	struct pv_head	*pvh;

#ifdef DEBUG
        if (pmapdebug & PDB_FOLLOW)
                printf("pmap_init()\n");
#endif
d1003 7
a1009 2
	/* initialize protection array */
	alpha_protection_init();
d1012 1
a1012 2
	 * Memory for the pv heads has already been allocated.
	 * Initialize the physical memory segments.
d1014 2
a1015 6
	pvh = pv_table;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		s = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvhead = pvh;
		pvh += s;
	}
d1018 2
a1019 3
	 * Set a low water mark on the pv_entry pool, so that we are
	 * more likely to have these around even in extreme memory
	 * starvation.
d1021 8
a1028 1
	pool_setlowat(&pmap_pv_pool, pmap_pv_lowat);
d1031 1
a1031 1
	 * Now it is safe to enable pv entry recording.
d1033 15
a1047 1
	pmap_initialized = TRUE;
d1049 9
a1057 11
#if 0
	for (bank = 0; bank < vm_nphysseg; bank++) {
		printf("bank %d\n", bank);
		printf("\tstart = 0x%x\n", ptoa(vm_physmem[bank].start));
		printf("\tend = 0x%x\n", ptoa(vm_physmem[bank].end));
		printf("\tavail_start = 0x%x\n",
		    ptoa(vm_physmem[bank].avail_start));
		printf("\tavail_end = 0x%x\n",
		    ptoa(vm_physmem[bank].avail_end));
	}
#endif
a1060 2
 * pmap_create:			[ INTERFACE ]
 *
d1063 8
a1070 1
 *	Note: no locking is necessary in this function.
d1073 2
a1074 1
pmap_create(void)
d1076 1
a1076 10
	pmap_t pmap;
	int i;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create()\n");
#endif

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	memset(pmap, 0, sizeof(*pmap));
d1078 2
a1079 2
	pmap->pm_asn = pool_get(&pmap_asn_pool, PR_WAITOK);
	pmap->pm_asngen = pool_get(&pmap_asngen_pool, PR_WAITOK);
d1082 1
a1082 3
	 * Defer allocation of a new level 1 page table until
	 * the first new mapping is entered; just take a reference
	 * to the kernel kernel_lev1map.
a1083 1
	pmap->pm_lev1map = kernel_lev1map;
d1085 3
a1087 5
	pmap->pm_count = 1;
	for (i = 0; i < pmap_ncpuids; i++) {
		pmap->pm_asn[i] = PMAP_ASN_RESERVED;
		/* XXX Locking? */
		pmap->pm_asngen[i] = pmap_asn_generation[i];
a1088 1
	simple_lock_init(&pmap->pm_slock);
d1090 7
a1096 3
	simple_lock(&pmap_all_pmaps_slock);
	TAILQ_INSERT_TAIL(&pmap_all_pmaps, pmap, pm_list);
	simple_unlock(&pmap_all_pmaps_slock);
d1098 4
a1101 1
	return (pmap);
a1103 6
/*
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
 */
d1105 2
a1106 1
pmap_destroy(pmap_t pmap)
d1108 5
a1112 1
	int refs;
d1114 5
a1118 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%p)\n", pmap);
d1120 16
a1135 2
	if (pmap == NULL)
		return;
d1137 6
a1142 3
	PMAP_LOCK(pmap);
	refs = --pmap->pm_count;
	PMAP_UNLOCK(pmap);
d1144 3
a1146 2
	if (refs > 0)
		return;
d1149 1
a1149 1
	 * Remove it from the global list of all pmaps.
a1150 3
	simple_lock(&pmap_all_pmaps_slock);
	TAILQ_REMOVE(&pmap_all_pmaps, pmap, pm_list);
	simple_unlock(&pmap_all_pmaps_slock);
d1152 3
a1154 18
#ifdef DIAGNOSTIC
	/*
	 * Since the pmap is supposed to contain no valid
	 * mappings at this point, this should never happen.
	 */
	if (pmap->pm_lev1map != kernel_lev1map) {
		printf("pmap_release: pmap still contains valid mappings!\n");
		if (pmap->pm_nlev2)
			printf("pmap_release: %ld level 2 tables left\n",
			    pmap->pm_nlev2);
		if (pmap->pm_nlev3)
			printf("pmap_release: %ld level 3 tables left\n",
			    pmap->pm_nlev3);
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
		if (pmap->pm_lev1map != kernel_lev1map)
			panic("pmap_release: pmap_remove() didn't");
	}
#endif
d1156 3
a1158 3
	pool_put(&pmap_asn_pool, pmap->pm_asn);
	pool_put(&pmap_asngen_pool, pmap->pm_asngen);
	pool_put(&pmap_pmap_pool, pmap);
d1162 3
a1164 3
 * pmap_reference:		[ INTERFACE ]
 *
 *	Add a reference to the specified pmap.
d1166 3
a1168 2
void
pmap_reference(pmap_t pmap)
d1170 14
d1185 3
a1187 8
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%p)\n", pmap);
#endif
	if (pmap != NULL) {
		PMAP_LOCK(pmap);
		pmap->pm_count++;
		PMAP_UNLOCK(pmap);
d1189 3
a1193 8
/*
 * pmap_remove:			[ INTERFACE ]
 *
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
d1195 2
a1196 1
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
d1198 33
d1232 4
a1235 3
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
d1237 10
d1248 11
a1258 1
	pmap_do_remove(pmap, sva, eva, TRUE);
d1262 4
a1265 1
 * pmap_do_remove:
d1267 4
a1270 4
 *	This actually removes the range of addresses from the
 *	specified map.  It is used by pmap_collect() (does not
 *	want to remove wired mappings) and pmap_remove() (does
 *	want to remove wired mappings).
d1272 7
a1278 2
void
pmap_do_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva, boolean_t dowired)
d1280 4
a1283 10
	pt_entry_t *l1pte, *l2pte, *l3pte;
	pt_entry_t *saved_l1pte, *saved_l2pte, *saved_l3pte;
	vaddr_t l1eva, l2eva, vptva;
	boolean_t needisync = FALSE;
	long cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
#endif
d1285 2
a1286 2
	if (pmap == NULL)
		return;
d1288 2
a1289 29
	/*
	 * If this is the kernel pmap, we can use a faster method
	 * for accessing the PTEs (since the PT pages are always
	 * resident).
	 *
	 * Note that this routine should NEVER be called from an
	 * interrupt context; pmap_kremove() is used for that.
	 */
	if (pmap == pmap_kernel()) {
		PMAP_MAP_TO_HEAD_LOCK();
		PMAP_LOCK(pmap);

		KASSERT(dowired == TRUE);

		while (sva < eva) {
			l3pte = PMAP_KERNEL_PTE(sva);
			if (pmap_pte_v(l3pte)) {
#ifdef DIAGNOSTIC
				if (PAGE_IS_MANAGED(pmap_pte_pa(l3pte)) &&
				    pmap_pte_pv(l3pte) == 0)
					panic("pmap_remove: managed page "
					    "without PG_PVLIST for 0x%lx",
					    sva);
#endif
				needisync |= pmap_remove_mapping(pmap, sva,
				    l3pte, TRUE, cpu_id, NULL);
			}
			sva += PAGE_SIZE;
		}
d1291 3
a1293 2
		PMAP_UNLOCK(pmap);
		PMAP_MAP_TO_HEAD_UNLOCK();
d1295 3
a1297 4
		if (needisync)
			PMAP_SYNC_ISTREAM_KERNEL();
		return;
	}
d1299 1
a1299 5
#ifdef DIAGNOSTIC
	if (sva > VM_MAXUSER_ADDRESS || eva > VM_MAXUSER_ADDRESS)
		panic("pmap_remove: (0x%lx - 0x%lx) user pmap, kernel "
		    "address range", sva, eva);
#endif
d1301 12
a1312 2
	PMAP_MAP_TO_HEAD_LOCK();
	PMAP_LOCK(pmap);
d1314 2
a1315 6
	/*
	 * If we're already referencing the kernel_lev1map, there
	 * is no work for us to do.
	 */
	if (pmap->pm_lev1map == kernel_lev1map)
		goto out;
d1317 56
a1372 1
	saved_l1pte = l1pte = pmap_l1pte(pmap, sva);
d1375 1
a1375 2
	 * Add a reference to the L1 table to it won't get
	 * removed from under us.
d1377 3
a1379 1
	pmap_physpage_addref(saved_l1pte);
d1381 5
a1385 4
	for (; sva < eva; sva = l1eva, l1pte++) {
		l1eva = alpha_trunc_l1seg(sva) + ALPHA_L1SEG_SIZE;
		if (pmap_pte_v(l1pte)) {
			saved_l2pte = l2pte = pmap_l2pte(pmap, sva, l1pte);
d1387 10
a1396 5
			/*
			 * Add a reference to the L2 table so it won't
			 * get removed from under us.
			 */
			pmap_physpage_addref(saved_l2pte);
d1398 22
a1419 42
			for (; sva < l1eva && sva < eva; sva = l2eva, l2pte++) {
				l2eva =
				    alpha_trunc_l2seg(sva) + ALPHA_L2SEG_SIZE;
				if (pmap_pte_v(l2pte)) {
					saved_l3pte = l3pte =
					    pmap_l3pte(pmap, sva, l2pte);

					/*
					 * Add a reference to the L3 table so
					 * it won't get removed from under us.
					 */
					pmap_physpage_addref(saved_l3pte);

					/*
					 * Remember this sva; if the L3 table
					 * gets removed, we need to invalidate
					 * the VPT TLB entry for it.
					 */
					vptva = sva;

					for (; sva < l2eva && sva < eva;
					     sva += PAGE_SIZE, l3pte++) {
						if (pmap_pte_v(l3pte) &&
						    (dowired == TRUE ||
						     pmap_pte_w(l3pte) == 0)) {
							needisync |=
							    pmap_remove_mapping(
								pmap, sva,
								l3pte, TRUE,
								cpu_id, NULL);
						}
					}

					/*
					 * Remove the reference to the L3
					 * table that we added above.  This
					 * may free the L3 table.
					 */
					pmap_l3pt_delref(pmap, vptva,
					    saved_l3pte, cpu_id, NULL);
				}
			}
d1421 11
a1431 6
			/*
			 * Remove the reference to the L2 table that we
			 * added above.  This may free the L2 table.
			 */
			pmap_l2pt_delref(pmap, l1pte, saved_l2pte, cpu_id);
		}
d1433 27
d1462 1
a1462 2
	 * Remove the reference to the L1 table that we added above.
	 * This may free the L1 table.
d1464 1
a1464 1
	pmap_l1pt_delref(pmap, saved_l1pte, cpu_id);
d1466 14
a1479 2
	if (needisync)
		PMAP_SYNC_ISTREAM_USER(pmap);
d1481 1
a1481 3
 out:
	PMAP_UNLOCK(pmap);
	PMAP_MAP_TO_HEAD_UNLOCK();
d1485 1
a1485 1
 * pmap_page_protect:		[ INTERFACE ]
d1487 3
a1489 2
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
d1491 5
a1495 2
void
pmap_page_protect(vm_page_t pg, vm_prot_t prot)
d1497 17
a1513 12
	pmap_t pmap;
	struct pv_head *pvh;
	pv_entry_t pv, nextpv;
	boolean_t needkisync = FALSE;
	long cpu_id = cpu_number();
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

#ifdef DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    (prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE)))
		printf("pmap_page_protect(%p, %x)\n", pg, prot);
#endif
d1515 3
d1519 8
a1526 15
	case VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE:
	case VM_PROT_READ|VM_PROT_WRITE:
		return;
	/* copy_on_write */
	case VM_PROT_READ|VM_PROT_EXECUTE:
	case VM_PROT_READ:
		pvh = pa_to_pvh(pa);
		PMAP_HEAD_TO_MAP_LOCK();
		simple_lock(&pvh->pvh_slock);
/* XXX */	pmap_changebit(pa, 0, ~(PG_KWE | PG_UWE), cpu_id);
		simple_unlock(&pvh->pvh_slock);
		PMAP_HEAD_TO_MAP_UNLOCK();
		return;
	/* remove_all */
	default:
d1530 81
a1610 21
	pvh = pa_to_pvh(pa);
	PMAP_HEAD_TO_MAP_LOCK();
	simple_lock(&pvh->pvh_slock);
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL; pv = nextpv) {
		nextpv = LIST_NEXT(pv, pv_list);
		pmap = pv->pv_pmap;

		PMAP_LOCK(pmap);
#ifdef DEBUG
		if (pmap_pte_v(pmap_l2pte(pv->pv_pmap, pv->pv_va, NULL)) == 0 ||
		    pmap_pte_pa(pv->pv_pte) != pa)
			panic("pmap_page_protect: bad mapping");
#endif
		if (pmap_pte_w(pv->pv_pte) == 0) {
			if (pmap_remove_mapping(pmap, pv->pv_va, pv->pv_pte,
			    FALSE, cpu_id, NULL) == TRUE) {
				if (pmap == pmap_kernel())
					needkisync |= TRUE;
				else
					PMAP_SYNC_ISTREAM_USER(pmap);
			}
a1611 1
#ifdef DEBUG
d1613 28
a1640 6
			if (pmapdebug & PDB_PARANOIA) {
				printf("%s wired mapping for %lx not removed\n",
				       "pmap_page_protect:", pa);
				printf("vm wire count %d\n", 
					PHYS_TO_VM_PAGE(pa)->wire_count);
			}
d1642 1
a1642 2
#endif
		PMAP_UNLOCK(pmap);
d1645 1
a1645 5
	if (needkisync)
		PMAP_SYNC_ISTREAM_KERNEL();

	simple_unlock(&pvh->pvh_slock);
	PMAP_HEAD_TO_MAP_UNLOCK();
d1649 3
a1651 4
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protection on the specified range of this map
 *	as requested.
d1653 4
a1656 2
void
pmap_protect(pmap_t pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d1658 4
a1661 11
	pt_entry_t *l1pte, *l2pte, *l3pte, bits;
	boolean_t isactive;
	boolean_t hadasm;
	vaddr_t l1eva, l2eva;
	long cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%p, %lx, %lx, %x)\n",
		    pmap, sva, eva, prot);
#endif
d1663 1
a1663 1
	if (pmap == NULL)
d1666 16
a1681 2
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
d1685 2
a1686 2
	if (prot & VM_PROT_WRITE)
		return;
d1688 4
a1691 4
	PMAP_LOCK(pmap);

	bits = pte_prot(pmap, prot);
	isactive = PMAP_ISACTIVE(pmap, cpu_id);
d1693 13
a1705 31
	l1pte = pmap_l1pte(pmap, sva);
	for (; sva < eva; sva = l1eva, l1pte++) {
		l1eva = alpha_trunc_l1seg(sva) + ALPHA_L1SEG_SIZE;
		if (pmap_pte_v(l1pte)) {
			l2pte = pmap_l2pte(pmap, sva, l1pte);
			for (; sva < l1eva && sva < eva; sva = l2eva, l2pte++) {
				l2eva =
				    alpha_trunc_l2seg(sva) + ALPHA_L2SEG_SIZE;
				if (pmap_pte_v(l2pte)) {
					l3pte = pmap_l3pte(pmap, sva, l2pte);
					for (; sva < l2eva && sva < eva;
					     sva += PAGE_SIZE, l3pte++) {
						if (pmap_pte_v(l3pte) &&
						    pmap_pte_prot_chg(l3pte,
						    bits)) {
							hadasm =
							   (pmap_pte_asm(l3pte)
							    != 0);
							pmap_pte_set_prot(l3pte,
							   bits);
							PMAP_INVALIDATE_TLB(
							   pmap, sva, hadasm,
							   isactive, cpu_id);
							PMAP_TLB_SHOOTDOWN(
							   pmap, sva,
							   hadasm ? PG_ASM : 0);
						}
					}
				}
			}
		}
d1708 2
a1709 4
	if (prot & VM_PROT_EXECUTE)
		PMAP_SYNC_ISTREAM(pmap);

	PMAP_UNLOCK(pmap);
a1712 2
 * pmap_enter:			[ INTERFACE ]
 *
d1720 1
a1720 1
 *	Note:  This is the only routine which MAY NOT lazy-evaluate
d1725 51
a1775 21
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int wired,
	vm_prot_t access_type)
{
	boolean_t managed;
	pt_entry_t *pte, npte, opte;
	paddr_t opa;
	boolean_t tflush = TRUE;
	boolean_t hadasm = FALSE;	/* XXX gcc -Wuninitialized */
	boolean_t needisync = FALSE;
	boolean_t setisync = FALSE;
	boolean_t isactive;
	long cpu_id = cpu_number();
	int error;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
		       pmap, va, pa, prot, access_type);
#endif
	if (pmap == NULL)
		return;
d1777 2
a1778 2
	managed = PAGE_IS_MANAGED(pa);
	isactive = PMAP_ISACTIVE(pmap, cpu_id);
d1781 31
a1811 12
	 * Determine what we need to do about the I-stream.  If
	 * VM_PROT_EXECUTE is set, we mark a user pmap as needing
	 * an I-sync on the way back out to userspace.  We always
	 * need an immediate I-sync for the kernel pmap.
	 */
	if (prot & VM_PROT_EXECUTE) {
		if (pmap == pmap_kernel())
			needisync = TRUE;
		else {
			setisync = TRUE;
			needisync = (pmap->pm_cpus != 0);
		}
d1813 1
d1815 6
a1820 2
	PMAP_MAP_TO_HEAD_LOCK();
	PMAP_LOCK(pmap);
a1821 2
	if (pmap == pmap_kernel()) {
#ifdef DIAGNOSTIC
d1823 2
a1824 1
		 * Sanity check the virtual address.
d1826 1
a1826 6
		if (va < VM_MIN_KERNEL_ADDRESS)
			panic("pmap_enter: kernel pmap, invalid va 0x%lx", va);
#endif
		pte = PMAP_KERNEL_PTE(va);
	} else {
		pt_entry_t *l1pte, *l2pte;
a1827 1
#ifdef DIAGNOSTIC
d1829 2
a1830 1
		 * Sanity check the virtual address.
d1832 7
a1838 3
		if (va >= VM_MAXUSER_ADDRESS)
			panic("pmap_enter: user pmap, invalid va 0x%lx", va);
#endif
d1841 2
a1842 4
		 * If we're still referencing the kernel kernel_lev1map,
		 * create a new level 1 page table.  A reference will be
		 * added to the level 1 table when the level 2 table is
		 * created.
d1844 16
a1859 9
		if (pmap->pm_lev1map == kernel_lev1map) {
			error = pmap_lev1map_create(pmap, cpu_id);
			if (error != KERN_SUCCESS) {
#ifdef notyet
				if (flags & PMAP_CANFAIL)
					return (error);
#endif
				panic("pmap_enter: unable to create lev1map");
			}
d1861 3
d1865 28
a1892 18
		/*
		 * Check to see if the level 1 PTE is valid, and
		 * allocate a new level 2 page table page if it's not.
		 * A reference will be added to the level 2 table when
		 * the level 3 table is created.
		 */
		l1pte = pmap_l1pte(pmap, va);
		if (pmap_pte_v(l1pte) == 0) {
			pmap_physpage_addref(l1pte);
			error = pmap_ptpage_alloc(pmap, l1pte, PGU_L2PT);
			if (error != KERN_SUCCESS) {
				pmap_l1pt_delref(pmap, l1pte, cpu_id);
#ifdef notyet
				if (flags & PMAP_CANFAIL)
					return (error);
#endif
				panic("pmap_enter: unable to create L2 PT "
				    "page");
a1893 5
			pmap->pm_nlev2++;
#ifdef DEBUG
			if (pmapdebug & PDB_PTPAGE)
				printf("pmap_enter: new level 2 table at "
				    "0x%lx\n", pmap_pte_pa(l1pte));
d1895 141
a2035 1
		}
d2037 4
a2040 26
		/*
		 * Check to see if the level 2 PTE is valid, and
		 * allocate a new level 3 page table page if it's not.
		 * A reference will be added to the level 3 table when
		 * the mapping is validated.
		 */
		l2pte = pmap_l2pte(pmap, va, l1pte);
		if (pmap_pte_v(l2pte) == 0) {
			pmap_physpage_addref(l2pte);
			error = pmap_ptpage_alloc(pmap, l2pte, PGU_L3PT);
			if (error != KERN_SUCCESS) {
				pmap_l2pt_delref(pmap, l1pte, l2pte, cpu_id);
#ifdef notyet
				if (flags & PMAP_CANFAIL)
					return (error);
#endif
				panic("pmap_enter: unable to create L3 PT "
				    "page");
			}
			pmap->pm_nlev3++;
#ifdef DEBUG
			if (pmapdebug & PDB_PTPAGE)
				printf("pmap_enter: new level 3 table at "
				    "0x%lx\n", pmap_pte_pa(l2pte));
#endif
		}
d2042 5
a2046 5
		/*
		 * Get the PTE that will map the page.
		 */
		pte = pmap_l3pte(pmap, va, l2pte);
	}
d2048 7
a2054 2
	/* Remember all of the old PTE; used for TBI check later. */
	opte = *pte;
d2057 2
a2058 2
	 * Check to see if the old mapping is valid.  If not, validate the
	 * new one immediately.
d2060 3
a2062 22
	if (pmap_pte_v(pte) == 0) {
		/*
		 * No need to invalidate the TLB in this case; an invalid
		 * mapping won't be in the TLB, and a previously valid
		 * mapping would have been flushed when it was invalidated.
		 */
		tflush = FALSE;

		/*
		 * No need to synchronize the I-stream, either, for basically
		 * the same reason.
		 */
		setisync = needisync = FALSE;

		if (pmap != pmap_kernel()) {
			/*
			 * New mappings gain a reference on the level 3
			 * table.
			 */
			pmap_physpage_addref(pte);
		}
		goto validate_enterpv;
d2065 6
a2070 2
	opa = pmap_pte_pa(pte);
	hadasm = (pmap_pte_asm(pte) != 0);
d2072 3
a2074 19
	if (opa == pa) {
		/*
		 * Mapping has not changed; must be a protection or
		 * wiring change.
		 */
		if (pmap_pte_w_chg(pte, wired ? PG_WIRED : 0)) {
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("pmap_enter: wiring change -> %d\n",
				    wired);
#endif
			/*
			 * Adjust the wiring count.
			 */
			if (wired)
				PMAP_STAT_INCR(pmap->pm_stats.wired_count, 1);
			else
				PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
		}
d2076 21
a2096 5
		/*
		 * Set the PTE.
		 */
		goto validate;
	}
d2098 2
a2099 18
	/*
	 * The mapping has changed.  We need to invalidate the
	 * old mapping before creating the new one.
	 */
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter: removing old mapping 0x%lx\n", va);
#endif
	if (pmap != pmap_kernel()) {
		/*
		 * Gain an extra reference on the level 3 table.
		 * pmap_remove_mapping() will delete a reference,
		 * and we don't want the table to be erroneously
		 * freed.
		 */
		pmap_physpage_addref(pte);
	}
	needisync |= pmap_remove_mapping(pmap, va, pte, TRUE, cpu_id, NULL);
d2101 3
a2103 16
 validate_enterpv:
	/*
	 * Enter the mapping into the pv_table if appropriate.
	 */
	if (managed) {
		error = pmap_pv_enter(pmap, pa, va, pte, TRUE);
		if (error != KERN_SUCCESS) {
			pmap_l3pt_delref(pmap, va, pte, cpu_id, NULL);
#ifdef notyet
			if (flags & PMAP_CANFAIL)
				return (error);
#endif
			panic("pmap_enter: unable to enter mapping in PV "
			    "table");
		}
	}
d2106 2
a2107 1
	 * Increment counters.
d2109 3
a2111 3
	PMAP_STAT_INCR(pmap->pm_stats.resident_count, 1);
	if (wired)
		PMAP_STAT_INCR(pmap->pm_stats.wired_count, 1);
d2113 2
a2114 8
 validate:
	/*
	 * Build the new PTE.
	 */
	npte = ((pa >> PGSHIFT) << PG_SHIFT) | pte_prot(pmap, prot) | PG_V;
	if (managed) {
		struct pv_head *pvh = pa_to_pvh(pa);
		int attrs;
d2116 1
a2116 11
#ifdef DIAGNOSTIC
		if ((access_type & VM_PROT_ALL) & ~prot)
			panic("pmap_enter: access type exceeds prot");
#endif
		simple_lock(&pvh->pvh_slock);
		if (access_type & VM_PROT_WRITE)
			pvh->pvh_attrs |= (PGA_REFERENCED|PGA_MODIFIED);
		else if (access_type & VM_PROT_ALL)
			pvh->pvh_attrs |= PGA_REFERENCED;
		attrs = pvh->pvh_attrs;
		simple_unlock(&pvh->pvh_slock);
d2119 4
a2122 1
		 * Set up referenced/modified emulation for new mapping.
d2124 10
a2133 4
		if ((attrs & PGA_REFERENCED) == 0)
			npte |= PG_FOR | PG_FOW | PG_FOE;
		else if ((attrs & PGA_MODIFIED) == 0)
			npte |= PG_FOW;
d2136 1
a2136 1
		 * Mapping was entered on PV list.
d2138 14
a2151 1
		npte |= PG_PVLIST;
a2152 6
	if (wired)
		npte |= PG_WIRED;
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter: new pte = 0x%lx\n", npte);
#endif
d2155 1
a2155 2
	 * If the PALcode portion of the new PTE is the same as the
	 * old PTE, no TBI is necessary.
d2157 2
a2158 2
	if (PG_PALCODE(opte) == PG_PALCODE(npte))
		tflush = FALSE;
d2161 3
a2163 1
	 * Set the new PTE.
d2165 1
a2165 1
	PMAP_SET_PTE(pte, npte);
d2167 4
a2170 7
	/*
	 * Invalidate the TLB entry for this VA and any appropriate
	 * caches.
	 */
	if (tflush) {
		PMAP_INVALIDATE_TLB(pmap, va, hadasm, isactive, cpu_id);
		PMAP_TLB_SHOOTDOWN(pmap, va, hadasm ? PG_ASM : 0);
a2171 4
	if (setisync)
		PMAP_SET_NEEDISYNC(pmap);
	if (needisync)
		PMAP_SYNC_ISTREAM(pmap);
d2173 25
a2197 6
	PMAP_UNLOCK(pmap);
	PMAP_MAP_TO_HEAD_UNLOCK();

#ifdef notyet	
	return (KERN_SUCCESS);
#endif	
d2201 3
a2203 1
 * pmap_kenter_pa:		[ INTERFACE ]
d2205 1
a2205 4
 *	Enter a va -> pa mapping into the kernel pmap without any
 *	physical->virtual tracking.
 *
 *	Note: no locking is necessary in this function.
d2207 12
a2218 12
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *pte, npte;
	long cpu_id = cpu_number();
	boolean_t needisync = FALSE;
	pmap_t pmap = pmap_kernel();

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_kenter_pa(%lx, %lx, %x)\n",
		    va, pa, prot);
d2221 21
a2241 7
#ifdef DIAGNOSTIC
	/*
	 * Sanity check the virtual address.
	 */
	if (va < VM_MIN_KERNEL_ADDRESS)
		panic("pmap_kenter_pa: kernel pmap, invalid va 0x%lx", va);
#endif
d2243 2
a2244 1
	pte = PMAP_KERNEL_PTE(va);
d2246 2
a2247 7
	if (pmap_pte_v(pte) == 0)
		PMAP_STAT_INCR(pmap->pm_stats.resident_count, 1);
	if (pmap_pte_w(pte) == 0)
		PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);

	if ((prot & VM_PROT_EXECUTE) != 0 || pmap_pte_exec(pte))
		needisync = TRUE;
d2250 1
a2250 1
	 * Build the new PTE.
d2252 9
a2260 2
	npte = ((pa >> PGSHIFT) << PG_SHIFT) | pte_prot(pmap_kernel(), prot) |
	    PG_V | PG_WIRED;
d2262 78
a2339 7
	/*
	 * Set the new PTE.
	 */
	PMAP_SET_PTE(pte, npte);
#if defined(MULTIPROCESSOR)
	alpha_mb();		/* XXX alpha_wmb()? */
#endif
d2341 2
a2342 6
	/*
	 * Invalidate the TLB entry for this VA and any appropriate
	 * caches.
	 */
	PMAP_INVALIDATE_TLB(pmap, va, TRUE, TRUE, cpu_id);
	PMAP_TLB_SHOOTDOWN(pmap, va, PG_ASM);
d2344 14
a2357 2
	if (needisync)
		PMAP_SYNC_ISTREAM_KERNEL();
d2361 11
a2371 10
 * pmap_kenter_pgs:		[ INTERFACE ]
 *
 *	Enter a va -> pa mapping for the array of vm_page's into the
 *	kernel pmap without any physical->virtual tracking, starting
 *	at address va, for npgs pages.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_kenter_pgs(vaddr_t va, vm_page_t *pgs, int npgs)
d2373 2
a2374 1
	int i;
d2376 7
a2382 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_kenter_pgs(%lx, %p, %d)\n",
		    va, pgs, npgs);
a2383 5

	for (i = 0; i < npgs; i++)
		pmap_kenter_pa(va + (PAGE_SIZE * i),
		    VM_PAGE_TO_PHYS(pgs[i]),
		    VM_PROT_READ|VM_PROT_WRITE);
d2387 3
a2389 4
 * pmap_kremove:		[ INTERFACE ]
 *
 *	Remove a mapping entered with pmap_kenter_pa() or pmap_kenter_pgs()
 *	starting at va, for size bytes (assumed to be page rounded).
d2391 2
a2392 2
void
pmap_kremove(vaddr_t va, vsize_t size)
d2394 2
a2395 31
	pt_entry_t *pte;
	boolean_t needisync = FALSE;
	long cpu_id = cpu_number();
	pmap_t pmap = pmap_kernel();

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_kremove(%lx, %lx)\n",
		    va, size);
#endif

#ifdef DIAGNOSTIC
	if (va < VM_MIN_KERNEL_ADDRESS)
		panic("pmap_kremove: user address");
#endif

	for (; size != 0; size -= PAGE_SIZE, va += PAGE_SIZE) {
		pte = PMAP_KERNEL_PTE(va);
		if (pmap_pte_v(pte)) {
#ifdef DIAGNOSTIC
			if (pmap_pte_pv(pte))
				panic("pmap_kremove: PG_PVLIST mapping for "
				    "0x%lx", va);
#endif
			if (pmap_pte_exec(pte))
				needisync = TRUE;

			/* Zap the mapping. */
			PMAP_SET_PTE(pte, PG_NV);
#if defined(MULTIPROCESSOR)
			alpha_mb();		/* XXX alpha_wmb()? */
a2396 12
			PMAP_INVALIDATE_TLB(pmap, va, TRUE, TRUE, cpu_id);
			PMAP_TLB_SHOOTDOWN(pmap, va, PG_ASM);

			/* Update stats. */
			PMAP_STAT_DECR(pmap->pm_stats.resident_count, 1);
			PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
		}
	}

	if (needisync)
		PMAP_SYNC_ISTREAM_KERNEL();
}
d2399 2
a2400 5
 * pmap_unwire:			[ INTERFACE ]
 *
 *	Clear the wired attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
d2402 1
d2404 2
a2405 1
pmap_change_wiring(pmap_t pmap, vaddr_t va, int wired)
a2406 1
	pt_entry_t *pte;
d2408 2
a2409 2
	if (wired)
		panic("pmap_change_wiring");
d2411 1
a2411 6
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_unwire(%p, %lx)\n", pmap, va);
#endif
	if (pmap == NULL)
		return;
d2413 1
a2413 1
	PMAP_LOCK(pmap);
d2415 3
a2417 20
	pte = pmap_l3pte(pmap, va, NULL);
#ifdef DIAGNOSTIC
	if (pte == NULL || pmap_pte_v(pte) == 0)
		panic("pmap_unwire");
#endif

	/*
	 * If wiring actually changed (always?) clear the wire bit and
	 * update the wire count.  Note that wiring is not a hardware
	 * characteristic so there is no need to invalidate the TLB.
	 */
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, FALSE);
		PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
	}
#ifdef DIAGNOSTIC
	else {
		printf("pmap_unwire: wiring for pmap %p va 0x%lx "
		    "didn't change!\n", pmap, va);
	}
a2419 3
	PMAP_UNLOCK(pmap);
}

d2421 2
a2422 4
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
d2424 4
a2427 2
paddr_t
pmap_extract(pmap_t pmap, vaddr_t va)
a2428 8
	pt_entry_t *l1pte, *l2pte, *l3pte;
	paddr_t pa = 0;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%p, %lx) -> ", pmap, va);
#endif
	PMAP_LOCK(pmap);
d2430 2
a2431 3
	l1pte = pmap_l1pte(pmap, va);
	if (pmap_pte_v(l1pte) == 0)
		goto out;
d2433 2
a2434 3
	l2pte = pmap_l2pte(pmap, va, l1pte);
	if (pmap_pte_v(l2pte) == 0)
		goto out;
d2436 1
a2436 3
	l3pte = pmap_l3pte(pmap, va, l2pte);
	if (pmap_pte_v(l3pte) == 0)
		goto out;
d2438 3
a2440 10
	pa = pmap_pte_pa(l3pte) | (va & PGOFSET);
 out:
	PMAP_UNLOCK(pmap);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		if (pa)
			printf("0x%lx\n", pa);
		else
			printf("failed\n");
	}
a2441 2
	return (pa);
}
d2444 4
a2447 1
 * pmap_copy:			[ INTERFACE ]
d2449 3
a2451 3
 *	Copy the mapping range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
d2453 3
a2455 1
 *	This routine is only advisory and need not do anything.
d2458 5
a2462 2
pmap_copy(pmap_t dst_pmap, pmap_t src_pmap, vaddr_t dst_addr, vsize_t len,
    vaddr_t src_addr)
d2464 2
a2465 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
d2470 1
a2470 5
 * pmap_update:			[ INTERFACE ]
 *
 *	Require that all active physical maps contain no
 *	incorrect entries NOW, by processing any deferred
 *	pmap operations.
d2473 3
a2475 1
pmap_update(void)
d2477 6
d2484 7
a2490 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_update()\n");
#endif
d2493 2
a2494 1
	 * Nothing to do; this pmap module does not defer any operations.
a2495 1
}
d2497 1
a2497 13
/*
 * pmap_collect:		[ INTERFACE ]
 *
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap_t pmap)
{
d2499 2
a2500 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%p)\n", pmap);
#endif
d2503 3
a2505 3
	 * If called for the kernel pmap, just return.  We
	 * handle this case in the event that we ever want
	 * to have swappable kernel threads.
d2507 46
a2552 2
	if (pmap == pmap_kernel())
		return;
d2554 1
a2554 7
	/*
	 * This process is about to be swapped out; free all of
	 * the PT pages by removing the physical mappings for its
	 * entire address space.  Note: pmap_remove() performs
	 * all necessary locking.
	 */
	pmap_do_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS, FALSE);
d2558 1
a2558 8
 * pmap_activate:		[ INTERFACE ]
 *
 *	Activate the pmap used by the specified process.  This includes
 *	reloading the MMU context if the current process, and marking
 *	the pmap in use by the processor.
 *
 *	Note: We may use only spin locks here, since we are called
 *	by a critical section in cpu_switch()!
d2560 4
a2563 2
void
pmap_activate(struct proc *p)
d2565 6
a2570 2
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	long cpu_id = cpu_number();
d2572 7
a2578 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_activate(%p)\n", p);
#endif
d2581 2
a2582 1
	 * Mark the pmap in use by this processor.
a2583 1
	atomic_setbits_ulong(&pmap->pm_cpus, (1UL << cpu_id));
d2585 4
a2588 7
	/*
	 * Move the pmap to the end of the LRU list.
	 */
	simple_lock(&pmap_all_pmaps_slock);
	TAILQ_REMOVE(&pmap_all_pmaps, pmap, pm_list);
	TAILQ_INSERT_TAIL(&pmap_all_pmaps, pmap, pm_list);
	simple_unlock(&pmap_all_pmaps_slock);
d2590 4
a2593 1
	PMAP_LOCK(pmap);
d2596 3
a2598 1
	 * Allocate an ASN.
d2600 5
a2604 1
	pmap_asn_alloc(pmap, cpu_id);
d2606 5
a2610 1
	PMAP_ACTIVATE(pmap, p, cpu_id);
d2612 2
a2613 2
	PMAP_UNLOCK(pmap);
}
d2615 2
a2616 14
/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.  Note that we use only a single `atomic' operation,
 *	so no locking is necessary.
 */
void
pmap_deactivate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
d2618 6
a2623 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_deactivate(%p)\n", p);
d2625 7
d2633 13
a2645 4
	/*
	 * Mark the pmap no longer in use by this processor.
	 */
	atomic_clearbits_ulong(&pmap->pm_cpus, (1UL << cpu_number()));
d2649 1
a2649 7
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and clear its contents, one machine dependent
 *	page at a time.
 *
 *	Note: no locking is necessary in this function.
d2652 3
a2654 1
pmap_zero_page(paddr_t phys)
d2656 2
a2657 1
	u_long *p0, *p1, *pend;
d2659 7
a2665 7
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%lx)\n", phys);
#endif

	p0 = (u_long *)ALPHA_PHYS_TO_K0SEG(phys);
	pend = (u_long *)((u_long)p0 + PAGE_SIZE);
d2668 1
a2668 2
	 * Unroll the loop a bit, doing 16 quadwords per iteration.
	 * Do only 8 back-to-back stores, and alternate registers.
d2670 7
a2676 27
	do {
		__asm __volatile(
		"# BEGIN loop body\n"
		"	addq	%2, (8 * 8), %1		\n"
		"	stq	$31, (0 * 8)(%0)	\n"
		"	stq	$31, (1 * 8)(%0)	\n"
		"	stq	$31, (2 * 8)(%0)	\n"
		"	stq	$31, (3 * 8)(%0)	\n"
		"	stq	$31, (4 * 8)(%0)	\n"
		"	stq	$31, (5 * 8)(%0)	\n"
		"	stq	$31, (6 * 8)(%0)	\n"
		"	stq	$31, (7 * 8)(%0)	\n"
		"					\n"
		"	addq	%3, (8 * 8), %0		\n"
		"	stq	$31, (0 * 8)(%1)	\n"
		"	stq	$31, (1 * 8)(%1)	\n"
		"	stq	$31, (2 * 8)(%1)	\n"
		"	stq	$31, (3 * 8)(%1)	\n"
		"	stq	$31, (4 * 8)(%1)	\n"
		"	stq	$31, (5 * 8)(%1)	\n"
		"	stq	$31, (6 * 8)(%1)	\n"
		"	stq	$31, (7 * 8)(%1)	\n"
		"	# END loop body"
		: "=r" (p0), "=r" (p1)
		: "0" (p0), "1" (p1)
		: "memory");
	} while (p0 < pend);
d2680 1
a2680 7
 * pmap_copy_page:		[ INTERFACE ]
 *
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using memcpy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: no locking is necessary in this function.
d2682 3
a2684 2
void
pmap_copy_page(paddr_t src, paddr_t dst)
d2686 3
a2688 1
	caddr_t s, d;
d2690 3
a2692 8
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%lx, %lx)\n", src, dst);
#endif
        s = (caddr_t)ALPHA_PHYS_TO_K0SEG(src);
        d = (caddr_t)ALPHA_PHYS_TO_K0SEG(dst);
	memcpy(d, s, PAGE_SIZE);
}
d2694 2
a2695 6
void
pmap_pageable(pmap, start, end, pageable)
	pmap_t		pmap;
	vaddr_t		start;
	vaddr_t		end;
	boolean_t	pageable;
d2697 2
d2702 1
a2702 1
 * pmap_clear_modify:		[ INTERFACE ]
d2704 2
a2705 1
 *	Clear the modify bits on the specified physical page.
d2707 3
a2709 2
boolean_t
pmap_clear_modify(vm_page_t pg)
d2711 2
a2712 25
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv = FALSE;
	long cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%p)\n", pg);
#endif

	pvh = pa_to_pvh(pa);

	PMAP_HEAD_TO_MAP_LOCK();
	simple_lock(&pvh->pvh_slock);

	if (pvh->pvh_attrs & PGA_MODIFIED) {
		rv = TRUE;
		pmap_changebit(pa, PG_FOW, ~0, cpu_id);
		pvh->pvh_attrs &= ~PGA_MODIFIED;
	}

	simple_unlock(&pvh->pvh_slock);
	PMAP_HEAD_TO_MAP_UNLOCK();

	return (rv);
d2716 1
a2716 1
 * pmap_clear_reference:	[ INTERFACE ]
d2720 3
a2722 2
boolean_t
pmap_clear_reference(vm_page_t pg)
d2724 2
a2725 25
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv = FALSE;
	long cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%p)\n", pg);
#endif

	pvh = pa_to_pvh(pa);

	PMAP_HEAD_TO_MAP_LOCK();
	simple_lock(&pvh->pvh_slock);

	if (pvh->pvh_attrs & PGA_REFERENCED) {
		rv = TRUE;
		pmap_changebit(pa, PG_FOR | PG_FOW | PG_FOE, ~0, cpu_id);
		pvh->pvh_attrs &= ~PGA_REFERENCED;
	}

	simple_unlock(&pvh->pvh_slock);
	PMAP_HEAD_TO_MAP_UNLOCK();

	return (rv);
d2729 1
a2729 1
 * pmap_is_referenced:		[ INTERFACE ]
d2734 3
a2736 2
boolean_t
pmap_is_referenced(vm_page_t pg)
d2738 2
a2739 12
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;

	pvh = pa_to_pvh(pa);
	rv = ((pvh->pvh_attrs & PGA_REFERENCED) != 0);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_is_referenced(%p) -> %c\n", pg, "FT"[rv]);
	}
#endif
	return (rv);
d2742 1
d2744 58
a2801 21
 * pmap_is_modified:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(vm_page_t pg)
{
	struct pv_head *pvh;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv;

	pvh = pa_to_pvh(pa);
	rv = ((pvh->pvh_attrs & PGA_MODIFIED) != 0);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_is_modified(%p) -> %c\n", pg, "FT"[rv]);
	}
#endif
	return (rv);
}
d2804 1
a2804 7
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
d2806 4
a2809 2
paddr_t
pmap_phys_address(int ppn)
d2811 8
d2820 23
a2842 1
	return (alpha_ptob(ppn));
d2845 2
a2846 13
/*
 * Miscellaneous support routines follow
 */

/*
 * alpha_protection_init:
 *
 *	Initialize Alpha protection code array.
 *
 *	Note: no locking is necessary in this function.
 */
void
alpha_protection_init(void)
d2848 4
a2851 1
	int prot, *kp, *up;
d2853 2
a2854 10
	kp = protection_codes[0];
	up = protection_codes[1];

	for (prot = 0; prot < 8; prot++) {
		kp[prot] = 0; up[prot] = 0;
		switch (prot) {
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE:
			kp[prot] |= PG_ASM;
			up[prot] |= 0;
			break;
d2856 4
a2859 5
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_EXECUTE:
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE:
			kp[prot] |= PG_EXEC;		/* software */
			up[prot] |= PG_EXEC;		/* software */
			/* FALLTHROUGH */
d2861 8
a2868 4
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_NONE:
			kp[prot] |= PG_ASM | PG_KRE;
			up[prot] |= PG_URE | PG_KRE;
			break;
d2870 1
a2870 4
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] |= PG_ASM | PG_KWE;
			up[prot] |= PG_UWE | PG_KWE;
			break;
d2872 3
a2874 5
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			kp[prot] |= PG_EXEC;		/* software */
			up[prot] |= PG_EXEC;		/* software */
			/* FALLTHROUGH */
d2876 5
a2880 6
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
			kp[prot] |= PG_ASM | PG_KWE | PG_KRE;
			up[prot] |= PG_UWE | PG_URE | PG_KWE | PG_KRE;
			break;
		}
	}
d2882 1
d2885 1
a2885 15
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	Note: locking in this function is complicated by the fact
 *	that we can be called when the PV list is already locked.
 *	(pmap_page_protect()).  In this case, the caller must be
 *	careful to get the next PV entry while we remove this entry
 *	from beneath it.  We assume that the pmap itself is already
 *	locked; dolock applies only to the PV list.
 *
 *	Returns TRUE or FALSE, indicating if an I-stream sync needs
 *	to be initiated (for this CPU or for other CPUs).
d2887 1
a2887 3
boolean_t
pmap_remove_mapping(pmap_t pmap, vaddr_t va, pt_entry_t *pte,
    boolean_t dolock, long cpu_id, struct prm_thief *prmt)
d2889 5
a2893 13
	paddr_t pa;
	boolean_t onpv;
	boolean_t hadasm;
	boolean_t isactive;
	boolean_t needisync = FALSE;
	struct pv_entry **pvp;
	pt_entry_t **ptp;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_mapping(%p, %lx, %p, %d, %ld, %p)\n",
		       pmap, va, pte, dolock, cpu_id, pvp);
#endif
d2895 9
a2903 12
	if (prmt != NULL) {
		if (prmt->prmt_flags & PRMT_PV)
			pvp = &prmt->prmt_pv;
		else
			pvp = NULL;
		if (prmt->prmt_flags & PRMT_PTP)
			ptp = &prmt->prmt_ptp;
		else
			ptp = NULL;
	} else {
		pvp = NULL;
		ptp = NULL;
d2905 1
a2905 1

d2907 4
a2910 1
	 * PTE not provided, compute it from pmap and va.
d2912 2
a2913 5
	if (pte == PT_ENTRY_NULL) {
		pte = pmap_l3pte(pmap, va, NULL);
		if (pmap_pte_v(pte) == 0)
			return (FALSE);
	}
d2915 6
a2920 18
	pa = pmap_pte_pa(pte);
	onpv = (pmap_pte_pv(pte) != 0);
	hadasm = (pmap_pte_asm(pte) != 0);
	isactive = PMAP_ISACTIVE(pmap, cpu_id);

	/*
	 * Determine what we need to do about the I-stream.  If
	 * PG_EXEC was set, we mark a user pmap as needing an
	 * I-sync on the way out to userspace.  We always need
	 * an immediate I-sync for the kernel pmap.
	 */
	if (pmap_pte_exec(pte)) {
		if (pmap == pmap_kernel())
			needisync = TRUE;
		else {
			PMAP_SET_NEEDISYNC(pmap);
			needisync = (pmap->pm_cpus != 0);
		}
d2924 3
a2926 1
	 * Update statistics
d2928 1
a2928 3
	if (pmap_pte_w(pte))
		PMAP_STAT_DECR(pmap->pm_stats.wired_count, 1);
	PMAP_STAT_DECR(pmap->pm_stats.resident_count, 1);
d2930 1
a2930 8
	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("remove: invalidating pte at %p\n", pte);
#endif
	PMAP_SET_PTE(pte, PG_NV);
d2932 13
a2944 2
	PMAP_INVALIDATE_TLB(pmap, va, hadasm, isactive, cpu_id);
	PMAP_TLB_SHOOTDOWN(pmap, va, hadasm ? PG_ASM : 0);
d2946 1
a2946 23
	/*
	 * If we're removing a user mapping, check to see if we
	 * can free page table pages.
	 */
	if (pmap != pmap_kernel()) {
		/*
		 * Delete the reference on the level 3 table.  It will
		 * delete references on the level 2 and 1 tables as
		 * appropriate.
		 */
		pmap_l3pt_delref(pmap, va, pte, cpu_id, ptp);
	}

	/*
	 * If the mapping wasn't enterd on the PV list, we're all done.
	 */
	if (onpv == FALSE) {
#ifdef DIAGNOSTIC
		if (pvp != NULL)
			panic("pmap_removing_mapping: onpv / pvp inconsistent");
#endif
		return (needisync);
	}
d2948 1
a2948 4
	/*
	 * Remove it from the PV table.
	 */
	pmap_pv_remove(pmap, pa, va, dolock, pvp);
d2950 5
a2954 1
	return (needisync);
d2956 1
a2956 1

d2958 1
a2958 11
 * pmap_changebit:
 *
 *	Set or clear the specified PTE bits for all mappings on the
 *	specified page.
 *
 *	Note: we assume that the pv_head is already locked, and that
 *	the caller has acquired a PV->pmap mutex so that we can lock
 *	the pmaps as we encounter them.
 *
 *	XXX This routine could stand to have some I-stream
 *	XXX optimization done.
d2960 6
d2967 1
a2967 1
pmap_changebit(paddr_t pa, u_long set, u_long mask, long cpu_id)
d2969 2
a2970 14
	struct pv_head *pvh;
	pv_entry_t pv;
	pt_entry_t *pte, npte;
	vaddr_t va;
	boolean_t hadasm, isactive;
	boolean_t needisync, needkisync = FALSE;

#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(0x%lx, 0x%lx, 0x%lx)\n",
		    pa, set, mask);
#endif
	if (!PAGE_IS_MANAGED(pa))
		return;
d2972 9
a2980 49
	pvh = pa_to_pvh(pa);
	/*
	 * Loop over all current mappings setting/clearing as appropos.
	 */
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list)) {
		va = pv->pv_va;

		/*
		 * XXX don't write protect pager mappings
		 */
		if (pv->pv_pmap == pmap_kernel() &&
/* XXX */	    mask == ~(PG_KWE | PG_UWE)) {
			if (va >= uvm.pager_sva && va < uvm.pager_eva)
				continue;
		}

		PMAP_LOCK(pv->pv_pmap);

		pte = pv->pv_pte;
		npte = (*pte | set) & mask;
		if (*pte != npte) {
			hadasm = (pmap_pte_asm(pte) != 0);
			isactive = PMAP_ISACTIVE(pv->pv_pmap, cpu_id);
			/*
			 * Determine what we need to do about the I-stream.
			 * If PG_EXEC was set, we mark a user pmap as needing
			 * an I-sync on the way out to userspace.  We always
			 * need an immediate I-sync for the kernel pmap.
			 */
			needisync = FALSE;
			if (pmap_pte_exec(pte)) {
				if (pv->pv_pmap == pmap_kernel())
					needkisync = TRUE;
				else {
					PMAP_SET_NEEDISYNC(pv->pv_pmap);
					if (pv->pv_pmap->pm_cpus != 0)
						needisync = TRUE;
				}
			}
			PMAP_SET_PTE(pte, npte);
			if (needisync)
				PMAP_SYNC_ISTREAM_USER(pv->pv_pmap);
			PMAP_INVALIDATE_TLB(pv->pv_pmap, va, hadasm, isactive,
			    cpu_id);
			PMAP_TLB_SHOOTDOWN(pv->pv_pmap, va,
			    hadasm ? PG_ASM : 0);
		}
		PMAP_UNLOCK(pv->pv_pmap);
d2982 7
a2988 3

	if (needkisync)
		PMAP_SYNC_ISTREAM_KERNEL();
d2991 3
a2993 2
/*
 * pmap_emulate_reference:
d2995 3
a2997 1
 *	Emulate reference and/or modified bit hits.
a2998 14
void
pmap_emulate_reference(struct proc *p, vaddr_t v, int user, int write)
{
	pt_entry_t faultoff, *pte;
	paddr_t pa;
	struct pv_head *pvh;
	boolean_t didlock = FALSE;
	long cpu_id = cpu_number();

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_emulate_reference: %p, 0x%lx, %d, %d\n",
		    p, v, user, write);
#endif
a2999 33
	/*
	 * Convert process and virtual address to physical address.
	 */
	if (v >= VM_MIN_KERNEL_ADDRESS) {
		if (user)
			panic("pmap_emulate_reference: user ref to kernel");
		/*
		 * No need to lock here; kernel PT pages never go away.
		 */
		pte = PMAP_KERNEL_PTE(v);
	} else {
#ifdef DIAGNOSTIC
		if (p == NULL)
			panic("pmap_emulate_reference: bad proc");
		if (p->p_vmspace == NULL)
			panic("pmap_emulate_reference: bad p_vmspace");
#endif
		PMAP_LOCK(p->p_vmspace->vm_map.pmap);
		didlock = TRUE;
		pte = pmap_l3pte(p->p_vmspace->vm_map.pmap, v, NULL);
		/*
		 * We'll unlock below where we're done with the PTE.
		 */
	}
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("\tpte = %p, ", pte);
		printf("*pte = 0x%lx\n", *pte);
	}
#endif
#ifdef DEBUG				/* These checks are more expensive */
	if (!pmap_pte_v(pte))
		panic("pmap_emulate_reference: invalid pte");
d3001 2
a3002 16
	/*
	 * Can't do these, because cpu_fork and cpu_swapin call
	 * pmap_emulate_reference(), and the bits aren't guaranteed,
	 * for them...
	 */
	if (write) {
		if (!(*pte & (user ? PG_UWE : PG_UWE | PG_KWE)))
			panic("pmap_emulate_reference: write but unwritable");
		if (!(*pte & PG_FOW))
			panic("pmap_emulate_reference: write but not FOW");
	} else {
		if (!(*pte & (user ? PG_URE : PG_URE | PG_KRE)))
			panic("pmap_emulate_reference: !write but unreadable");
		if (!(*pte & (PG_FOR | PG_FOE)))
			panic("pmap_emulate_reference: !write but not FOR|FOE");
	}
a3003 3
	/* Other diagnostics? */
#endif
	pa = pmap_pte_pa(pte);
d3005 8
a3012 6
	/*
	 * We're now done with the PTE.  If it was a user pmap, unlock
	 * it now.
	 */
	if (didlock)
		PMAP_UNLOCK(p->p_vmspace->vm_map.pmap);
d3014 2
a3015 8
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("\tpa = 0x%lx\n", pa);
#endif
#ifdef DIAGNOSTIC
	if (!PAGE_IS_MANAGED(pa))
		panic("pmap_emulate_reference(%p, 0x%lx, %d, %d): pa 0x%lx not managed", p, v, user, write, pa);
#endif
d3017 4
a3020 19
	/*
	 * Twiddle the appropriate bits to reflect the reference
	 * and/or modification..
	 *
	 * The rules:
	 * 	(1) always mark page as used, and
	 *	(2) if it was a write fault, mark page as modified.
	 */
	pvh = pa_to_pvh(pa);

	PMAP_HEAD_TO_MAP_LOCK();
	simple_lock(&pvh->pvh_slock);

	if (write) {
		pvh->pvh_attrs |= (PGA_REFERENCED|PGA_MODIFIED);
		faultoff = PG_FOR | PG_FOW | PG_FOE;
	} else {
		pvh->pvh_attrs |= PGA_REFERENCED;
		faultoff = PG_FOR | PG_FOE;
d3022 4
a3025 1
	pmap_changebit(pa, 0, ~faultoff, cpu_id);
d3027 1
a3027 2
	simple_unlock(&pvh->pvh_slock);
	PMAP_HEAD_TO_MAP_UNLOCK();
a3029 1
#ifdef DEBUG
d3031 5
a3035 1
 * pmap_pv_dump:
d3037 3
a3039 1
 *	Dump the physical->virtual data for the specified page.
d3041 3
a3043 2
void
pmap_pv_dump(paddr_t pa)
d3045 1
a3045 5
	struct pv_head *pvh;
	pv_entry_t pv;
	static const char *usage[] = {
		"normal", "pvent", "l1pt", "l2pt", "l3pt",
	};
d3047 2
a3048 1
	pvh = pa_to_pvh(pa);
d3050 1
a3050 1
	simple_lock(&pvh->pvh_slock);
d3052 1
a3052 5
	printf("pa 0x%lx (attrs = 0x%x, usage = " /* ) */, pa, pvh->pvh_attrs);
	if (pvh->pvh_usage < PGU_NORMAL || pvh->pvh_usage > PGU_L3PT)
/* ( */		printf("??? %d):\n", pvh->pvh_usage);
	else
/* ( */		printf("%s):\n", usage[pvh->pvh_usage]);
d3054 22
a3075 5
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list))
		printf("     pmap %p, va 0x%lx\n",
		    pv->pv_pmap, pv->pv_va);
	printf("\n");
d3077 4
a3080 1
	simple_unlock(&pvh->pvh_slock);
d3082 4
a3085 12
#endif
 
/*
 * vtophys:
 *
 *	Return the physical address corresponding to the K0SEG or
 *	K1SEG address provided.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
vtophys(vaddr_t vaddr)
d3087 1
a3087 12
	pt_entry_t *pte;
	paddr_t paddr = 0;

	if (vaddr < ALPHA_K0SEG_BASE)
		printf("vtophys: invalid vaddr 0x%lx", vaddr);
	else if (vaddr <= ALPHA_K0SEG_END)
		paddr = ALPHA_K0SEG_TO_PHYS(vaddr);
	else {
		pte = PMAP_KERNEL_PTE(vaddr);
		if (pmap_pte_v(pte))
			paddr = pmap_pte_pa(pte) | (vaddr & PGOFSET);
	}
d3089 2
a3090 3
#if 0
	printf("vtophys(0x%lx) -> 0x%lx\n", vaddr, paddr);
#endif
d3092 2
a3093 2
	return (paddr);
}
d3095 1
a3095 1
/******************** pv_entry management ********************/
d3097 1
a3097 11
/*
 * pmap_pv_enter:
 *
 *	Add a physical->virtual entry to the pv_table.
 */
int
pmap_pv_enter(pmap_t pmap, paddr_t pa, vaddr_t va, pt_entry_t *pte,
    boolean_t dolock)
{
	struct pv_head *pvh;
	pv_entry_t newpv;
d3100 1
a3100 1
	 * Allocate and fill in the new pv_entry.
d3102 7
a3108 6
	newpv = pmap_pv_alloc();
	if (newpv == NULL)
		return (KERN_RESOURCE_SHORTAGE);
	newpv->pv_va = va;
	newpv->pv_pmap = pmap;
	newpv->pv_pte = pte;
d3110 1
a3110 1
	pvh = pa_to_pvh(pa);
d3112 4
a3115 2
	if (dolock)
		simple_lock(&pvh->pvh_slock);
d3117 7
a3123 11
#ifdef DEBUG
	{
	pv_entry_t pv;
	/*
	 * Make sure the entry doesn't already exist.
	 */
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list))
		if (pmap == pv->pv_pmap && va == pv->pv_va) {
			printf("pmap = %p, va = 0x%lx\n", pmap, va);
			panic("pmap_pv_enter: already in pv table");
d3126 2
d3130 6
a3135 4
	/*
	 * ...and put it in the list.
	 */
	LIST_INSERT_HEAD(&pvh->pvh_list, newpv, pv_list);
d3137 8
a3144 2
	if (dolock)
		simple_unlock(&pvh->pvh_slock);
d3146 2
a3147 1
	return (KERN_SUCCESS);
d3150 2
a3151 8
/*
 * pmap_pv_remove:
 *
 *	Remove a physical->virtual entry from the pv_table.
 */
void
pmap_pv_remove(pmap_t pmap, paddr_t pa, vaddr_t va, boolean_t dolock,
    struct pv_entry **pvp)
d3153 2
a3154 4
	struct pv_head *pvh;
	pv_entry_t pv;

	pvh = pa_to_pvh(pa);
d3156 7
a3162 2
	if (dolock)
		simple_lock(&pvh->pvh_slock);
d3164 2
a3165 7
	/*
	 * Find the entry to remove.
	 */
	for (pv = LIST_FIRST(&pvh->pvh_list); pv != NULL;
	     pv = LIST_NEXT(pv, pv_list))
		if (pmap == pv->pv_pmap && va == pv->pv_va)
			break;
d3167 3
a3169 4
#ifdef DEBUG
	if (pv == NULL)
		panic("pmap_pv_remove: not in pv table");
#endif
d3171 2
a3172 1
	LIST_REMOVE(pv, pv_list);
d3174 10
a3183 2
	if (dolock)
		simple_unlock(&pvh->pvh_slock);
d3185 5
a3189 9
	/*
	 * If pvp is not NULL, this is pmap_pv_alloc() stealing an
	 * entry from another mapping, and we return the now unused
	 * entry in it.  Otherwise, free the pv_entry.
	 */
	if (pvp != NULL)
		*pvp = pv;
	else
		pmap_pv_free(pv);
d3192 2
a3193 7
/*
 * pmap_pv_alloc:
 *
 *	Allocate a pv_entry.
 */
struct pv_entry *
pmap_pv_alloc(void)
a3194 73
	struct pv_head *pvh;
	struct pv_entry *pv;
	int bank, npg, pg;
	pt_entry_t *pte;
	pmap_t pvpmap;
	u_long cpu_id;
	struct prm_thief prmt;

	pv = pool_get(&pmap_pv_pool, PR_NOWAIT);
	if (pv != NULL)
		return (pv);

	prmt.prmt_flags = PRMT_PV;

	/*
	 * We were unable to allocate one from the pool.  Try to
	 * steal one from another mapping.  At this point we know that:
	 *
	 *	(1) We have not locked the pv table, and we already have
	 *	    the map-to-head lock, so it is safe for us to do so here.
	 *
	 *	(2) The pmap that wants this entry *is* locked.  We must
	 *	    use simple_lock_try() to prevent deadlock from occurring.
	 *
	 * XXX Note that in case #2, there is an exception; it *is* safe to
	 * steal a mapping from the pmap that wants this entry!  We may want
	 * to consider passing the pmap to this function so that we can take
	 * advantage of this.
	 */

	/* XXX This search could probably be improved. */
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npg = vm_physmem[bank].end - vm_physmem[bank].start;
		for (pg = 0; pg < npg; pg++) {
			pvh = &vm_physmem[bank].pmseg.pvhead[pg];
			simple_lock(&pvh->pvh_slock);
			for (pv = LIST_FIRST(&pvh->pvh_list);
			     pv != NULL; pv = LIST_NEXT(pv, pv_list)) {
				pvpmap = pv->pv_pmap;

				/* Don't steal from kernel pmap. */
				if (pvpmap == pmap_kernel())
					continue;

				if (simple_lock_try(&pvpmap->pm_slock) == 0)
					continue;

				pte = pv->pv_pte;

				/* Don't steal wired mappings. */
				if (pmap_pte_w(pte)) {
					simple_unlock(&pvpmap->pm_slock);
					continue;
				}

				cpu_id = cpu_number();

				/*
				 * Okay!  We have a mapping we can steal;
				 * remove it and grab the pv_entry.
				 */
				if (pmap_remove_mapping(pvpmap, pv->pv_va,
				    pte, FALSE, cpu_id, &prmt))
					PMAP_SYNC_ISTREAM(pvpmap);

				/* Unlock everything and return. */
				simple_unlock(&pvpmap->pm_slock);
				simple_unlock(&pvh->pvh_slock);
				return (prmt.prmt_pv);
			}
			simple_unlock(&pvh->pvh_slock);
		}
	}
d3196 2
a3197 1
	return (NULL);
d3201 1
a3201 3
 * pmap_pv_free:
 *
 *	Free a pv_entry.
d3204 5
a3208 1
pmap_pv_free(struct pv_entry *pv)
d3210 2
d3213 1
a3213 2
	pool_put(&pmap_pv_pool, pv);
}
d3215 5
a3219 9
/*
 * pmap_pv_page_alloc:
 *
 *	Allocate a page for the pv_entry pool.
 */
void *
pmap_pv_page_alloc(u_long size, int flags, int mtype)
{
	paddr_t pg;
d3221 1
a3221 4
	if (pmap_physpage_alloc(PGU_PVENT, &pg))
		return ((void *)ALPHA_PHYS_TO_K0SEG(pg));
	return (NULL);
}
d3223 4
a3226 8
/*
 * pmap_pv_page_free:
 *
 *	Free a pv_entry pool page.
 */
void
pmap_pv_page_free(void *v, u_long size, int mtype)
{
d3228 2
a3229 2
	pmap_physpage_free(ALPHA_K0SEG_TO_PHYS((vaddr_t)v));
}
d3231 1
a3231 1
/******************** misc. functions ********************/
d3233 2
a3234 8
/*
 * pmap_physpage_alloc:
 *
 *	Allocate a single page from the VM system and return the
 *	physical address for that page.
 */
boolean_t
pmap_physpage_alloc(int usage, paddr_t *pap)
d3236 3
a3238 8
	struct vm_page *pg;
	struct pv_head *pvh;
	paddr_t pa;

	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (pg != NULL) {
		uvm_pagezero(pg);
		pa = VM_PAGE_TO_PHYS(pg);
d3240 18
a3257 2
		pvh = pa_to_pvh(pa);
		simple_lock(&pvh->pvh_slock);
d3259 2
a3260 11
		if (pvh->pvh_usage != PGU_NORMAL) {
			printf("pmap_physpage_alloc: page 0x%lx is "
			    "in use (%s)\n", pa,
			    pmap_pgu_strings[pvh->pvh_usage]);
			panic("pmap_physpage_alloc");
		}
		if (pvh->pvh_refcnt != 0) {
			printf("pmap_physpage_alloc: page 0x%lx has "
			    "%d references\n", pa, pvh->pvh_refcnt);
			panic("pmap_physpage_alloc");
		}
d3262 1
a3262 4
		pvh->pvh_usage = usage;
		simple_unlock(&pvh->pvh_slock);
		*pap = pa;
		return (TRUE);
d3264 1
a3264 1
	return (FALSE);
a3266 5
/*
 * pmap_physpage_free:
 *
 *	Free the single page table page at the specified physical address.
 */
d3268 2
a3269 1
pmap_physpage_free(paddr_t pa)
d3271 2
a3272 2
	struct pv_head *pvh;
	struct vm_page *pg;
d3274 16
a3289 2
	if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL)
		panic("pmap_physpage_free: bogus physical page address");
d3291 28
a3318 1
	pvh = pa_to_pvh(pa);
d3320 1
a3320 582
	simple_lock(&pvh->pvh_slock);
#ifdef DIAGNOSTIC
	if (pvh->pvh_usage == PGU_NORMAL)
		panic("pmap_physpage_free: not in use?!");
	if (pvh->pvh_refcnt != 0)
		panic("pmap_physpage_free: page still has references");
#endif
	pvh->pvh_usage = PGU_NORMAL;
	simple_unlock(&pvh->pvh_slock);

	uvm_pagefree(pg);
}

/*
 * pmap_physpage_addref:
 *
 *	Add a reference to the specified special use page.
 */
int
pmap_physpage_addref(void *kva)
{
	struct pv_head *pvh;
	paddr_t pa;
	int rval;

	pa = ALPHA_K0SEG_TO_PHYS(trunc_page((vaddr_t)kva));
	pvh = pa_to_pvh(pa);

	simple_lock(&pvh->pvh_slock);
#ifdef DIAGNOSTIC
	if (pvh->pvh_usage == PGU_NORMAL)
		panic("pmap_physpage_addref: not a special use page");
#endif

	rval = ++pvh->pvh_refcnt;
	simple_unlock(&pvh->pvh_slock);

	return (rval);
}

/*
 * pmap_physpage_delref:
 *
 *	Delete a reference to the specified special use page.
 */
int
pmap_physpage_delref(void *kva)
{
	struct pv_head *pvh;
	paddr_t pa;
	int rval;

	pa = ALPHA_K0SEG_TO_PHYS(trunc_page((vaddr_t)kva));
	pvh = pa_to_pvh(pa);

	simple_lock(&pvh->pvh_slock);
#ifdef DIAGNOSTIC
	if (pvh->pvh_usage == PGU_NORMAL)
		panic("pmap_physpage_delref: not a special use page");
#endif

	rval = --pvh->pvh_refcnt;

#ifdef DIAGNOSTIC
	/*
	 * Make sure we never have a negative reference count.
	 */
	if (pvh->pvh_refcnt < 0)
		panic("pmap_physpage_delref: negative reference count");
#endif
	simple_unlock(&pvh->pvh_slock);

	return (rval);
}

/******************** page table page management ********************/

/*
 * pmap_growkernel:		[ INTERFACE ]
 *
 *	Grow the kernel address space.  This is a hint from the
 *	upper layer to pre-allocate more kernel PT pages.
 */
vaddr_t
pmap_growkernel(vaddr_t maxkvaddr)
{
	struct pmap *kpm = pmap_kernel(), *pm;
	paddr_t ptaddr;
	pt_entry_t *l1pte, *l2pte, pte;
	vaddr_t va;
	int s, l1idx;

	if (maxkvaddr <= virtual_end)
		goto out;		/* we are OK */

	s = splhigh();			/* to be safe */
	simple_lock(&pmap_growkernel_slock);

	va = virtual_end;

	while (va < maxkvaddr) {
		/*
		 * If there is no valid L1 PTE (i.e. no L2 PT page),
		 * allocate a new L2 PT page and insert it into the
		 * L1 map.
		 */
		l1pte = pmap_l1pte(kpm, va);
		if (pmap_pte_v(l1pte) == 0) {
			/*
			 * XXX PGU_NORMAL?  It's not a "traditional" PT page.
			 */
			if (uvm.page_init_done == FALSE) {
				/*
				 * We're growing the kernel pmap early (from
				 * uvm_pageboot_alloc()).  This case must
				 * be handled a little differently.
				 */
				ptaddr = ALPHA_K0SEG_TO_PHYS(
				    pmap_steal_memory(PAGE_SIZE, NULL, NULL));
			} else if (pmap_physpage_alloc(PGU_NORMAL,
				   &ptaddr) == FALSE)
				goto die;
			pte = (atop(ptaddr) << PG_SHIFT) |
			    PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
			*l1pte = pte;

			l1idx = l1pte_index(va);

			/* Update all the user pmaps. */
			simple_lock(&pmap_all_pmaps_slock);
			for (pm = TAILQ_FIRST(&pmap_all_pmaps);
			     pm != NULL; pm = TAILQ_NEXT(pm, pm_list)) {
				/* Skip the kernel pmap. */
				if (pm == pmap_kernel())
					continue;

				PMAP_LOCK(pm);
				if (pm->pm_lev1map == kernel_lev1map) {
					PMAP_UNLOCK(pm);
					continue;
				}
				pm->pm_lev1map[l1idx] = pte;
				PMAP_UNLOCK(pm);
			}
			simple_unlock(&pmap_all_pmaps_slock);
		}

		/*
		 * Have an L2 PT page now, add the L3 PT page.
		 */
		l2pte = pmap_l2pte(kpm, va, l1pte);
		KASSERT(pmap_pte_v(l2pte) == 0);
		if (uvm.page_init_done == FALSE) {
			/*
			 * See above.
			 */
			ptaddr = ALPHA_K0SEG_TO_PHYS(
			    pmap_steal_memory(PAGE_SIZE, NULL, NULL));
		} else if (pmap_physpage_alloc(PGU_NORMAL, &ptaddr) == FALSE)
			goto die;
		*l2pte = (atop(ptaddr) << PG_SHIFT) |
		    PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		va += ALPHA_L2SEG_SIZE;
	}

	virtual_end = va;

	simple_unlock(&pmap_growkernel_slock);
	splx(s);

 out:
	return (virtual_end);

 die:
	panic("pmap_growkernel: out of memory");
}

/*
 * pmap_lev1map_create:
 *
 *	Create a new level 1 page table for the specified pmap.
 *
 *	Note: the pmap must already be locked.
 */
int
pmap_lev1map_create(pmap_t pmap, long cpu_id)
{
	paddr_t ptpa;
	pt_entry_t pte;
	int i;

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_lev1map_create: got kernel pmap");

	if (pmap->pm_asn[cpu_id] != PMAP_ASN_RESERVED)
		panic("pmap_lev1map_create: pmap uses non-reserved ASN");
#endif

	/*
	 * Allocate a page for the level 1 table.
	 */
	if (pmap_physpage_alloc(PGU_L1PT, &ptpa) == FALSE) {
		/*
		 * Yow!  No free pages!  Try to steal a PT page from
		 * another pmap!
		 */
		if (pmap_ptpage_steal(pmap, PGU_L1PT, &ptpa) == FALSE)
			return (KERN_RESOURCE_SHORTAGE);
	}
	pmap->pm_lev1map = (pt_entry_t *) ALPHA_PHYS_TO_K0SEG(ptpa);

	/*
	 * Initialize the new level 1 table by copying the
	 * kernel mappings into it.
	 */
	for (i = l1pte_index(VM_MIN_KERNEL_ADDRESS);
	     i <= l1pte_index(VM_MAX_KERNEL_ADDRESS); i++)
		pmap->pm_lev1map[i] = kernel_lev1map[i];

	/*
	 * Now, map the new virtual page table.  NOTE: NO ASM!
	 */
	pte = ((ptpa >> PGSHIFT) << PG_SHIFT) | PG_V | PG_KRE | PG_KWE;
	pmap->pm_lev1map[l1pte_index(VPTBASE)] = pte;

	/*
	 * The page table base has changed; if the pmap was active,
	 * reactivate it.
	 */
	if (PMAP_ISACTIVE(pmap, cpu_id)) {
		pmap_asn_alloc(pmap, cpu_id);
		PMAP_ACTIVATE(pmap, curproc, cpu_id);
	}
	return (KERN_SUCCESS);
}

/*
 * pmap_lev1map_destroy:
 *
 *	Destroy the level 1 page table for the specified pmap.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_lev1map_destroy(pmap_t pmap, long cpu_id)
{
	paddr_t ptpa;

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_lev1map_destroy: got kernel pmap");
#endif

	ptpa = ALPHA_K0SEG_TO_PHYS((vaddr_t)pmap->pm_lev1map);

	/*
	 * Go back to referencing the global kernel_lev1map.
	 */
	pmap->pm_lev1map = kernel_lev1map;

	/*
	 * The page table base has changed; if the pmap was active,
	 * reactivate it.  Note that allocation of a new ASN is
	 * not necessary here:
	 *
	 *	(1) We've gotten here because we've deleted all
	 *	    user mappings in the pmap, invalidating the
	 *	    TLB entries for them as we go.
	 *
	 *	(2) kernel_lev1map contains only kernel mappings, which
	 *	    were identical in the user pmap, and all of
	 *	    those mappings have PG_ASM, so the ASN doesn't
	 *	    matter.
	 *
	 * We do, however, ensure that the pmap is using the
	 * reserved ASN, to ensure that no two pmaps never have
	 * clashing TLB entries.
	 */
	PMAP_INVALIDATE_ASN(pmap, cpu_id);
	if (PMAP_ISACTIVE(pmap, cpu_id))
		PMAP_ACTIVATE(pmap, curproc, cpu_id);

	/*
	 * Free the old level 1 page table page.
	 */
	pmap_physpage_free(ptpa);
}

/*
 * pmap_ptpage_alloc:
 *
 *	Allocate a level 2 or level 3 page table page, and
 *	initialize the PTE that references it.
 *
 *	Note: the pmap must already be locked.
 */
int
pmap_ptpage_alloc(pmap_t pmap, pt_entry_t *pte, int usage)
{
	paddr_t ptpa;

	/*
	 * Allocate the page table page.
	 */
	if (pmap_physpage_alloc(usage, &ptpa) == FALSE) {
		/*
		 * Yow!  No free pages!  Try to steal a PT page from
		 * another pmap!
		 */
		if (pmap_ptpage_steal(pmap, usage, &ptpa) == FALSE)
			return (KERN_RESOURCE_SHORTAGE);
	}

	/*
	 * Initialize the referencing PTE.
	 */
	PMAP_SET_PTE(pte, ((ptpa >> PGSHIFT) << PG_SHIFT) |
	    PG_V | PG_KRE | PG_KWE | PG_WIRED |
	    (pmap == pmap_kernel() ? PG_ASM : 0));

	return (KERN_SUCCESS);
}

/*
 * pmap_ptpage_free:
 *
 *	Free the level 2 or level 3 page table page referenced
 *	be the provided PTE.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_ptpage_free(pmap_t pmap, pt_entry_t *pte, pt_entry_t **ptp)
{
	paddr_t ptpa;

	/*
	 * Extract the physical address of the page from the PTE
	 * and clear the entry.
	 */
	ptpa = pmap_pte_pa(pte);
	PMAP_SET_PTE(pte, PG_NV);

	/*
	 * Check to see if we're stealing the PT page.  If we are,
	 * zero it, and return the KSEG address of the page.
	 */
	if (ptp != NULL) {
		pmap_zero_page(ptpa);
		*ptp = (pt_entry_t *)ALPHA_PHYS_TO_K0SEG(ptpa);
	} else {
#ifdef DEBUG
		pmap_zero_page(ptpa);
#endif
		pmap_physpage_free(ptpa);
	}
}

/*
 * pmap_ptpage_steal:
 *
 *	Steal a PT page from a pmap.
 */
boolean_t
pmap_ptpage_steal(pmap_t pmap, int usage, paddr_t *pap)
{
	struct pv_head *pvh;
	pmap_t spmap;
	int l1idx, l2idx, l3idx;
	pt_entry_t *lev2map, *lev3map;
	vaddr_t va;
	paddr_t pa;
	struct prm_thief prmt;
	u_long cpu_id = cpu_number();
	boolean_t needisync = FALSE;

	prmt.prmt_flags = PRMT_PTP;
	prmt.prmt_ptp = NULL;

	/*
	 * We look for pmaps which do not reference kernel_lev1map (which
	 * would indicate that they are either the kernel pmap, or a user
	 * pmap with no valid mappings).  Since the list of all pmaps is
	 * maintained in an LRU fashion, we should get a pmap that is
	 * `more inactive' than our current pmap (although this may not
	 * always be the case).
	 *
	 * We start looking for valid L1 PTEs at the lowest address,
	 * go to that L2, look for the first valid L2 PTE, and steal
	 * that L3 PT page.
	 */
	simple_lock(&pmap_all_pmaps_slock);
	for (spmap = TAILQ_FIRST(&pmap_all_pmaps);
	     spmap != NULL; spmap = TAILQ_NEXT(spmap, pm_list)) {
		/*
		 * Skip the kernel pmap and ourselves.
		 */
		if (spmap == pmap_kernel() || spmap == pmap)
			continue;

		PMAP_LOCK(spmap);
		if (spmap->pm_lev1map == kernel_lev1map) {
			PMAP_UNLOCK(spmap);
			continue;
		}

		/*
		 * Have a candidate pmap.  Loop through the PT pages looking
		 * for one we can steal.
		 */
		for (l1idx = 0;
		     l1idx < l1pte_index(VM_MAXUSER_ADDRESS); l1idx++) {
			if (pmap_pte_v(&spmap->pm_lev1map[l1idx]) == 0)
				continue;

			lev2map = (pt_entry_t *)ALPHA_PHYS_TO_K0SEG(
			    pmap_pte_pa(&spmap->pm_lev1map[l1idx]));
			for (l2idx = 0; l2idx < NPTEPG; l2idx++) {
				if (pmap_pte_v(&lev2map[l2idx]) == 0)
					continue;
				lev3map = (pt_entry_t *)ALPHA_PHYS_TO_K0SEG(
				    pmap_pte_pa(&lev2map[l2idx]));
				for (l3idx = 0; l3idx < NPTEPG; l3idx++) {
					/*
					 * If the entry is valid and wired,
					 * we cannot steal this page.
					 */
					if (pmap_pte_v(&lev3map[l3idx]) &&
					    pmap_pte_w(&lev3map[l3idx]))
						break;
				}
				
				/*
				 * If we scanned all of the current L3 table
				 * without finding a wired entry, we can
				 * steal this page!
				 */
				if (l3idx == NPTEPG)
					goto found_one;
			}
		}

		/*
		 * Didn't find something we could steal in this
		 * pmap, try the next one.
		 */
		PMAP_UNLOCK(spmap);
		continue;

 found_one:
		/* ...don't need this anymore. */
		simple_unlock(&pmap_all_pmaps_slock);

		/*
		 * Okay!  We have a PT page we can steal.  l1idx and
		 * l2idx indicate which L1 PTP and L2 PTP we should
		 * use to compute the virtual addresses the L3 PTP
		 * maps.  Loop through all the L3 PTEs in this range
		 * and nuke the mappings for them.  When we're through,
		 * we'll have a PT page pointed to by prmt.prmt_ptp!
		 */
		for (l3idx = 0,
		     va = (l1idx * ALPHA_L1SEG_SIZE) +
		          (l2idx * ALPHA_L2SEG_SIZE);
		     l3idx < NPTEPG && prmt.prmt_ptp == NULL;
		     l3idx++, va += PAGE_SIZE) {
			if (pmap_pte_v(&lev3map[l3idx])) {
				needisync |= pmap_remove_mapping(spmap, va,
				    &lev3map[l3idx], TRUE, cpu_id, &prmt);
			}
		}

		if (needisync)
			PMAP_SYNC_ISTREAM(pmap);

		PMAP_UNLOCK(spmap);

#ifdef DIAGNOSTIC
		if (prmt.prmt_ptp == NULL)
			panic("pmap_ptptage_steal: failed");
		if (prmt.prmt_ptp != lev3map)
			panic("pmap_ptpage_steal: inconsistent");
#endif
		pa = ALPHA_K0SEG_TO_PHYS((vaddr_t)prmt.prmt_ptp);

		/*
		 * Don't bother locking here; the assignment is atomic.
		 */
		pvh = pa_to_pvh(pa);
		pvh->pvh_usage = usage;

		*pap = pa;
		return (TRUE);
	}
	simple_unlock(&pmap_all_pmaps_slock);
	return (FALSE);
}

/*
 * pmap_l3pt_delref:
 *
 *	Delete a reference on a level 3 PT page.  If the reference drops
 *	to zero, free it.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_l3pt_delref(pmap_t pmap, vaddr_t va, pt_entry_t *l3pte, long cpu_id,
    pt_entry_t **ptp)
{
	pt_entry_t *l1pte, *l2pte;

	l1pte = pmap_l1pte(pmap, va);
	l2pte = pmap_l2pte(pmap, va, l1pte);

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_l3pt_delref: kernel pmap");
#endif

	if (pmap_physpage_delref(l3pte) == 0) {
		/*
		 * No more mappings; we can free the level 3 table.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_l3pt_delref: freeing level 3 table at "
			    "0x%lx\n", pmap_pte_pa(l2pte));
#endif
		pmap_ptpage_free(pmap, l2pte, ptp);
		pmap->pm_nlev3--;

		/*
		 * We've freed a level 3 table, so we must
		 * invalidate the TLB entry for that PT page
		 * in the Virtual Page Table VA range, because
		 * otherwise the PALcode will service a TLB
		 * miss using the stale VPT TLB entry it entered
		 * behind our back to shortcut to the VA's PTE.
		 */
		PMAP_INVALIDATE_TLB(pmap,
		    (vaddr_t)(&VPT[VPT_INDEX(va)]), FALSE,
		    PMAP_ISACTIVE(pmap, cpu_id), cpu_id);
		PMAP_TLB_SHOOTDOWN(pmap,
		    (vaddr_t)(&VPT[VPT_INDEX(va)]), 0);

		/*
		 * We've freed a level 3 table, so delete the reference
		 * on the level 2 table.
		 */
		pmap_l2pt_delref(pmap, l1pte, l2pte, cpu_id);
	}
}

/*
 * pmap_l2pt_delref:
 *
 *	Delete a reference on a level 2 PT page.  If the reference drops
 *	to zero, free it.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_l2pt_delref(pmap_t pmap, pt_entry_t *l1pte, pt_entry_t *l2pte,
    long cpu_id)
{

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_l2pt_delref: kernel pmap");
#endif

	if (pmap_physpage_delref(l2pte) == 0) {
		/*
		 * No more mappings in this segment; we can free the
		 * level 2 table.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_l2pt_delref: freeing level 2 table at "
			    "0x%lx\n", pmap_pte_pa(l1pte));
a3321 322
		pmap_ptpage_free(pmap, l1pte, NULL);
		pmap->pm_nlev2--;

		/*
		 * We've freed a level 2 table, so delete the reference
		 * on the level 1 table.
		 */
		pmap_l1pt_delref(pmap, l1pte, cpu_id);
	}
}

/*
 * pmap_l1pt_delref:
 *
 *	Delete a reference on a level 1 PT page.  If the reference drops
 *	to zero, free it.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_l1pt_delref(pmap_t pmap, pt_entry_t *l1pte, long cpu_id)
{

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel())
		panic("pmap_l1pt_delref: kernel pmap");
#endif

	if (pmap_physpage_delref(l1pte) == 0) {
		/*
		 * No more level 2 tables left, go back to the global
		 * kernel_lev1map.
		 */
		pmap_lev1map_destroy(pmap, cpu_id);
	}
}

/******************** Address Space Number management ********************/

/*
 * pmap_asn_alloc:
 *
 *	Allocate and assign an ASN to the specified pmap.
 *
 *	Note: the pmap must already be locked.
 */
void
pmap_asn_alloc(pmap_t pmap, long cpu_id)
{

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ASN))
		printf("pmap_asn_alloc(%p)\n", pmap);
#endif

	/*
	 * If the pmap is still using the global kernel_lev1map, there
	 * is no need to assign an ASN at this time, because only
	 * kernel mappings exist in that map, and all kernel mappings
	 * have PG_ASM set.  If the pmap eventually gets its own
	 * lev1map, an ASN will be allocated at that time.
	 */
	if (pmap->pm_lev1map == kernel_lev1map) {
#ifdef DEBUG
		if (pmapdebug & PDB_ASN)
			printf("pmap_asn_alloc: still references "
			    "kernel_lev1map\n");
#endif
#ifdef DIAGNOSTIC
		if (pmap->pm_asn[cpu_id] != PMAP_ASN_RESERVED)
			panic("pmap_asn_alloc: kernel_lev1map without "
			    "PMAP_ASN_RESERVED");
#endif
		return;
	}

	/*
	 * On processors which do not implement ASNs, the swpctx PALcode
	 * operation will automatically invalidate the TLB and I-cache,
	 * so we don't need to do that here.
	 */
	if (pmap_max_asn == 0) {
		/*
		 * Refresh the pmap's generation number, to
		 * simplify logic elsewhere.
		 */
		pmap->pm_asngen[cpu_id] = pmap_asn_generation[cpu_id];
#ifdef DEBUG
		if (pmapdebug & PDB_ASN)
			printf("pmap_asn_alloc: no ASNs, using asngen %lu\n",
			    pmap->pm_asngen[cpu_id]);
#endif
		return;
	}

	/*
	 * Hopefully, we can continue using the one we have...
	 */
	if (pmap->pm_asn[cpu_id] != PMAP_ASN_RESERVED &&
	    pmap->pm_asngen[cpu_id] == pmap_asn_generation[cpu_id]) {
		/*
		 * ASN is still in the current generation; keep on using it.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_ASN) 
			printf("pmap_asn_alloc: same generation, keeping %u\n",
			    pmap->pm_asn[cpu_id]);
#endif
		return;
	}

	/*
	 * Need to assign a new ASN.  Grab the next one, incrementing
	 * the generation number if we have to.
	 */
	if (pmap_next_asn[cpu_id] > pmap_max_asn) {
		/*
		 * Invalidate all non-PG_ASM TLB entries and the
		 * I-cache, and bump the generation number.
		 */
		ALPHA_TBIAP();
		alpha_pal_imb();

		pmap_next_asn[cpu_id] = 1;

		pmap_asn_generation[cpu_id]++;
#ifdef DIAGNOSTIC
		if (pmap_asn_generation[cpu_id] == 0) {
			/*
			 * The generation number has wrapped.  We could
			 * handle this scenario by traversing all of
			 * the pmaps, and invaldating the generation
			 * number on those which are not currently
			 * in use by this processor.
			 *
			 * However... considering that we're using
			 * an unsigned 64-bit integer for generation
			 * numbers, on non-ASN CPUs, we won't wrap
			 * for approx. 585 million years, or 75 billion
			 * years on a 128-ASN CPU (assuming 1000 switch
			 * operations per second).
			 *
			 * So, we don't bother.
			 */
			panic("pmap_asn_alloc: too much uptime");
		}
#endif
#ifdef DEBUG
		if (pmapdebug & PDB_ASN)
			printf("pmap_asn_alloc: generation bumped to %lu\n",
			    pmap_asn_generation[cpu_id]);
#endif
	}

	/*
	 * Assign the new ASN and validate the generation number.
	 */
	pmap->pm_asn[cpu_id] = pmap_next_asn[cpu_id]++;
	pmap->pm_asngen[cpu_id] = pmap_asn_generation[cpu_id];

#ifdef DEBUG
	if (pmapdebug & PDB_ASN)
		printf("pmap_asn_alloc: assigning %u to pmap %p\n",
		    pmap->pm_asn[cpu_id], pmap);
#endif

	/*
	 * Have a new ASN, so there's no need to sync the I-stream
	 * on the way back out to userspace.
	 */
	atomic_clearbits_ulong(&pmap->pm_needisync, (1UL << cpu_id));
}

#if defined(MULTIPROCESSOR)
/******************** TLB shootdown code ********************/

/*
 * pmap_tlb_shootdown:
 *
 *	Cause the TLB entry for pmap/va to be shot down.
 */
void
pmap_tlb_shootdown(pmap_t pmap, vaddr_t va, pt_entry_t pte)
{
	u_long i, ipinum, cpu_id = cpu_number();
	struct pmap_tlb_shootdown_q *pq;
	struct pmap_tlb_shootdown_job *pj;
	int s;

	for (i = 0; i < hwrpb->rpb_pcs_cnt; i++) {
		if (i == cpu_id || (cpus_running & (1UL << i)) == 0)
			continue;

		pq = &pmap_tlb_shootdown_q[i];

		PSJQ_LOCK(pq, s);

		pj = pmap_tlb_shootdown_job_get(pq);
		pq->pq_pte |= pte;
		if (pj == NULL) {
			/*
			 * Couldn't allocate a job entry.  Just do a
			 * TBIA[P].
			 */
			if (pq->pq_pte & PG_ASM)
				ipinum = ALPHA_IPI_TBIA;
			else
				ipinum = ALPHA_IPI_TBIAP;
			alpha_send_ipi(i, ipinum);
		} else {
			pj->pj_pmap = pmap;
			pj->pj_va = va;
			pj->pj_pte = pte;
			TAILQ_INSERT_TAIL(&pq->pq_head, pj, pj_list);
			ipinum = ALPHA_IPI_SHOOTDOWN;
		}

		alpha_send_ipi(i, ipinum);

		PSJQ_UNLOCK(pq, s);
	}
}

/*
 * pmap_do_tlb_shootdown:
 *
 *	Process pending TLB shootdown operations for this processor.
 */
void
pmap_do_tlb_shootdown(void)
{
	u_long cpu_id = cpu_number();
	u_long cpu_mask = (1UL << cpu_id);
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj;
	int s;

	PSJQ_LOCK(pq, s);

	while ((pj = TAILQ_FIRST(&pq->pq_head)) != NULL) {
		TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
		PMAP_INVALIDATE_TLB(pj->pj_pmap, pj->pj_va,
		    pj->pj_pte & PG_ASM, pj->pj_pmap->pm_cpus & cpu_mask,
		    cpu_id);
		pmap_tlb_shootdown_job_put(pq, pj);
	}
	pq->pq_pte = 0;

	PSJQ_UNLOCK(pq, s);
}

/*
 * pmap_tlb_shootdown_q_drain:
 *
 *	Drain a processor's TLB shootdown queue.  We do not perform
 *	the shootdown operations.  This is merely a convenience
 *	function.
 */
void
pmap_tlb_shootdown_q_drain(u_long cpu_id, boolean_t all)
{
	struct pmap_tlb_shootdown_q *pq = &pmap_tlb_shootdown_q[cpu_id];
	struct pmap_tlb_shootdown_job *pj, *npj;
	pt_entry_t npte = 0;
	int s;

	PSJQ_LOCK(pq, s);

	for (pj = TAILQ_FIRST(&pq->pq_head); pj != NULL; pj = npj) {
		npj = TAILQ_NEXT(pj, pj_list);
		if (all || (pj->pj_pte & PG_ASM) == 0) {
			TAILQ_REMOVE(&pq->pq_head, pj, pj_list);
			pmap_tlb_shootdown_job_put(pq, pj);
		} else
			npte |= pj->pj_pte;
	}
	pq->pq_pte = npte;

	PSJQ_UNLOCK(pq, s);
}

/*
 * pmap_tlb_shootdown_job_get:
 *
 *	Get a TLB shootdown job queue entry.  This places a limit on
 *	the number of outstanding jobs a processor may have.
 *
 *	Note: We expect the queue to be locked.
 */
struct pmap_tlb_shootdown_job *
pmap_tlb_shootdown_job_get(struct pmap_tlb_shootdown_q *pq)
{
	struct pmap_tlb_shootdown_job *pj;

	if (pq->pq_count >= PMAP_TLB_SHOOTDOWN_MAXJOBS)
		return (NULL);
	pj = pool_get(&pmap_tlb_shootdown_job_pool, PR_NOWAIT);
	if (pj != NULL)
		pq->pq_count++;
	return (pj);
}

/*
 * pmap_tlb_shootdown_job_put:
 *
 *	Put a TLB shootdown job queue entry onto the free list.
 *
 *	Note: We expect the queue to be locked.
 */
void
pmap_tlb_shootdown_job_put(struct pmap_tlb_shootdown_q *pq,
    struct pmap_tlb_shootdown_job *pj)
{

#ifdef DIAGNOSTIC
	if (pq->pq_count == 0)
		panic("pmap_tlb_shootdown_job_put: queue length inconsistency");
#endif
	pool_put(&pmap_tlb_shootdown_job_pool, pj);
	pq->pq_count--;
}
#endif /* MULTIPROCESSOR */
@


1.6.4.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/* $OpenBSD$ */
/* $NetBSD: pmap.c,v 1.154 2000/12/07 22:18:55 thorpej Exp $ */
d130 2
a271 2
struct pool pmap_l1pt_pool;
struct pool_cache pmap_l1pt_cache;
a382 3
 *	* pmap_growkernel_slock - This lock protects pmap_growkernel()
 *	  and the virtual_end variable.
 *
a514 5
void	*pmap_l1pt_alloc(unsigned long, int, int);
void	pmap_l1pt_free(void *, unsigned long, int);

int	pmap_l1pt_ctor(void *, void *, int);

a915 3
	/* Initialize the pmap_growkernel_slock. */
	simple_lock_init(&pmap_growkernel_slock);

a943 4
	pool_init(&pmap_l1pt_pool, PAGE_SIZE, 0, 0, 0, "l1ptpl",
	    0, pmap_l1pt_alloc, pmap_l1pt_free, M_VMPMAP);
	pool_cache_init(&pmap_l1pt_cache, &pmap_l1pt_pool, pmap_l1pt_ctor,
	    NULL, NULL);
d2142 1
a2142 1
pmap_unwire(pmap_t pmap, vaddr_t va)
d2146 3
d2189 2
a2190 2
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
d2224 1
a2224 2
	*pap = pa;
	return (pa != 0);
d2454 9
a3289 5
	/*
	 * Don't ask for a zero'd page in the L1PT case -- we will
	 * properly initialize it in the constructor.
	 */

d3292 1
a3292 2
		if (usage != PGU_L1PT)
			uvm_pagezero(pg);
d3521 3
a3523 1
	pt_entry_t *l1pt;
d3533 10
a3542 6
	simple_lock(&pmap_growkernel_slock);

	l1pt = pool_cache_get(&pmap_l1pt_cache, PR_NOWAIT);
	if (l1pt == NULL) {
		simple_unlock(&pmap_growkernel_slock);
		return (KERN_RESOURCE_SHORTAGE);
d3544 1
d3546 7
a3552 1
	pmap->pm_lev1map = l1pt;
d3554 5
a3558 1
	simple_unlock(&pmap_growkernel_slock);
d3581 1
a3581 1
	pt_entry_t *l1pt = pmap->pm_lev1map;
d3588 2
d3620 1
a3620 73
	pool_cache_put(&pmap_l1pt_cache, l1pt);
}

/*
 * pmap_l1pt_ctor:
 *
 *	Pool cache constructor for L1 PT pages.
 */
int
pmap_l1pt_ctor(void *arg, void *object, int flags)
{
	pt_entry_t *l1pt = object, pte;
	int i;

	/*
	 * Initialize the new level 1 table by zeroing the
	 * user portion and copying the kernel mappings into
	 * the kernel portion.
	 */
	for (i = 0; i < l1pte_index(VM_MIN_KERNEL_ADDRESS); i++)
		l1pt[i] = 0;

	for (i = l1pte_index(VM_MIN_KERNEL_ADDRESS);
	     i <= l1pte_index(VM_MAX_KERNEL_ADDRESS); i++)
		l1pt[i] = kernel_lev1map[i];

	/*
	 * Now, map the new virtual page table.  NOTE: NO ASM!
	 */
	pte = ((ALPHA_K0SEG_TO_PHYS((vaddr_t) l1pt) >> PGSHIFT) << PG_SHIFT) |
	    PG_V | PG_KRE | PG_KWE;
	l1pt[l1pte_index(VPTBASE)] = pte;

	return (0);
}

/*
 * pmap_l1pt_alloc:
 *
 *	Page alloctor for L1 PT pages.
 */
void *
pmap_l1pt_alloc(unsigned long sz, int flags, int mtype)
{
	paddr_t ptpa;

	/*
	 * Attempt to allocate a free page.
	 */
	if (pmap_physpage_alloc(PGU_L1PT, &ptpa) == FALSE) {
#if 0
		/*
		 * Yow!  No free pages!  Try to steal a PT page from
		 * another pmap!
		 */
		if (pmap_ptpage_steal(pmap, PGU_L1PT, &ptpa) == FALSE)
#endif
			return (NULL);
	}

	return ((void *) ALPHA_PHYS_TO_K0SEG(ptpa));
}

/*
 * pmap_l1pt_free:
 *
 *	Page freer for L1 PT pages.
 */
void
pmap_l1pt_free(void *v, unsigned long sz, int mtype)
{

	pmap_physpage_free(ALPHA_K0SEG_TO_PHYS((vaddr_t) v));
d4146 1
a4146 1
pmap_do_tlb_shootdown(struct cpu_info *ci, struct trapframe *framep)
d4148 1
a4148 1
	u_long cpu_id = ci->ci_cpuid;
@


1.6.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.6.4.2 2001/07/04 10:14:23 niklas Exp $ */
d1046 1
a1046 1
#endif /* _PMAP_MAY_USE_PROM_CONSOLE */
d1686 3
a1688 2
int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
a1697 1
	boolean_t wired;
d1699 1
a1699 1
	int error = KERN_SUCCESS;
d1706 3
a1710 1
	wired = (flags & PMAP_WIRED) != 0;
d1759 1
d1761 2
a1762 1
					goto out;
d1779 1
d1781 2
a1782 1
					goto out;
d1806 1
d1808 2
a1809 1
					goto out;
d1914 1
d1916 2
a1917 1
				goto out;
d1940 1
a1940 1
		if ((flags & VM_PROT_ALL) & ~prot)
d1944 1
a1944 1
		if (flags & VM_PROT_WRITE)
d1946 1
a1946 1
		else if (flags & VM_PROT_ALL)
a1995 1
out:
d1999 3
a2001 1
	return error;
@


1.6.4.4
log
@Merge in -current
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d383 3
d809 3
d814 1
a814 1
		nbuf * MAXBSIZE + + PAGER_MAP_SIZE + 16 * NCARGS) / NBPG +
d924 3
d1704 1
a1704 1
		       pmap, va, pa, prot, flags);
d2194 1
a2194 2
	boolean_t rv = FALSE;
	paddr_t pa;
a2214 2
	*pap = pa;
	rv = TRUE;
d2219 1
a2219 1
		if (rv)
d2225 2
a2226 1
	return (rv);
d3288 1
a3288 2
	pg = uvm_pagealloc(NULL, 0, NULL, usage == PGU_L1PT ?
	    UVM_PGA_USERESERVE : UVM_PGA_USERESERVE|UVM_PGA_ZERO);
d3290 2
a3496 3

	/* Invalidate the L1 PT cache. */
	pool_cache_invalidate(&pmap_l1pt_cache);
@


1.6.4.5
log
@Merge in -current
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.6.4.4 2001/11/13 21:00:48 niklas Exp $ */
d807 2
a808 2
	lev3mapsize = (VM_PHYS_SIZE + (ubc_nwins << ubc_winshift) +
		nbuf * MAXBSIZE + 16 * NCARGS + PAGER_MAP_SIZE) / NBPG +
a1038 7
void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = VM_MIN_KERNEL_ADDRESS;
	*vendp = VM_MAX_KERNEL_ADDRESS;
}

d1511 1
a1511 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
d1690 1
a1690 1
	int error = 0;
d1747 1
a1747 1
			if (error) {
d1764 1
a1764 1
			if (error) {
d1789 1
a1789 1
			if (error) {
d1895 1
a1895 1
		if (error) {
d2050 26
d2078 1
a2078 1
 *	Remove a mapping entered with pmap_kenter_pa()
d2243 21
d2455 1
a2455 1
pmap_clear_modify(struct vm_page *pg)
d2490 1
a2490 1
pmap_clear_reference(struct vm_page *pg)
d2526 1
a2526 1
pmap_is_referenced(struct vm_page *pg)
d2549 1
a2549 1
pmap_is_modified(struct vm_page *pg)
d3055 1
a3055 1
		return (ENOMEM);
d3088 1
a3088 1
	return (0);
d3530 1
a3530 1
		return (ENOMEM);
d3545 1
a3545 1
	return (0);
d3692 1
a3692 1
			return (ENOMEM);
d3702 1
a3702 1
	return (0);
@


1.6.4.6
log
@Merge in trunk
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d515 2
a516 6
void	*pmap_l1pt_alloc(struct pool *, int);
void	pmap_l1pt_free(struct pool *, void *);

struct pool_allocator pmap_l1pt_allocator = {
	pmap_l1pt_alloc, pmap_l1pt_free, 0,
};
d528 2
a529 5
void	*pmap_pv_page_alloc(struct pool *, int);
void	pmap_pv_page_free(struct pool *, void *);
struct pool_allocator pmap_pv_allocator = {
	pmap_pv_page_alloc, pmap_pv_page_free, 0,
};
a804 5
	 * Compute the number of pages kmem_map will have.
	 */
	kmeminit_nkmempages();

	/*
d807 1
a807 1
	lev3mapsize = (VM_PHYS_SIZE +
d809 1
a809 1
		(maxproc * UPAGES) + nkmempages;
d945 1
a945 1
	    &pool_allocator_nointr);
d947 1
a947 1
	    &pmap_l1pt_allocator);
d951 2
a952 1
	    "pmasnpl", &pool_allocator_nointr);
d954 2
a955 1
	    "pmasngenpl", &pool_allocator_nointr);
d957 1
a957 1
	    &pmap_pv_allocator);
d1001 1
a1001 1
	    NULL);
a1297 1
		pmap_update(pmap);
d3200 1
a3200 1
pmap_pv_page_alloc(struct pool *pp, int flags)
d3215 1
a3215 1
pmap_pv_page_free(struct pool *pp, void *v)
d3597 1
a3597 1
pmap_l1pt_alloc(struct pool *pp, int flags)
d3624 1
a3624 1
pmap_l1pt_free(struct pool *pp, void *v)
@


1.6.4.7
log
@A bit on the way to make GENERIC compile in the SMP branch.
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.6.4.6 2002/03/06 00:47:43 niklas Exp $ */
d394 5
@


1.6.4.8
log
@Sync the SMP branch with 3.3
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d568 1
a568 1
		panic("PMAP_ISACTIVE, isa: %d pm: %p curpm:%p",		\
d862 1
a862 1
	 * ...and initialize the pv_entry list headers.
a1634 2
	if (!pmap_pte_exec(&bits))
		bits |= PG_FOE;
a1952 4
		/* Always force FOE on non-exec mappings. */
		if (!pmap_pte_exec(pte))
			npte |= PG_FOE;

d2347 1
a2347 1
pmap_zero_page(struct vm_page *pg)
a2348 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d2402 1
a2402 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a2403 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
a2802 3
			} else {
				/* Never clear FOE on non-exec mappings. */
				npte |= PG_FOE;
a2822 3
 *
 *	return non-zero if this was a FOE fault and the pte is not
 *	executable.
d2824 2
a2825 2
int
pmap_emulate_reference(struct proc *p, vaddr_t v, int user, int type)
d2836 1
a2836 1
		    p, v, user, type);
a2862 5
	if (!pmap_pte_exec(pte) && type == ALPHA_MMCSR_FOE) {
		if (didlock)
			PMAP_UNLOCK(p->p_vmspace->vm_map.pmap);
		return (1);
	}
d2878 1
a2878 1
	if (type == ALPHA_MMCSR_FOW) {
d2907 1
a2907 1
		panic("pmap_emulate_reference(%p, 0x%lx, %d, %d): pa 0x%lx not managed", p, v, user, type, pa);
d2923 1
a2923 1
	if (type == ALPHA_MMCSR_FOW) {
a2929 5
	/*
	 * If the page is not PG_EXEC, pmap_changebit will automagically
	 * set PG_FOE (gross, but necessary if I don't want to change the
	 * whole API).
	 */
a2933 2

	return (0);
d3696 1
a3696 1
		pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
d3700 1
a3700 1
		pmap_zero_page(PHYS_TO_VM_PAGE(ptpa));
@


1.6.4.9
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/* $OpenBSD: pmap.c,v 1.6.4.8 2003/03/27 23:18:06 niklas Exp $ */
d57 5
a61 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.6.4.10
log
@Merge of -current from two weeks ago into the SMP branch
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d924 1
a924 1
	/* Nothing to do; it's already zeroed */
d944 1
a944 1
	 * Initialize the pmap pools and list.
d2720 1
a2720 1
	 * If the mapping wasn't entered on the PV list, we're all done.
d2771 1
a2771 1
	 * Loop over all current mappings setting/clearing as appropriate.
d3266 1
a3266 1
	 * Don't ask for a zeroed page in the L1PT case -- we will
d3623 1
a3623 1
 *	Page allocator for L1 PT pages.
d4084 1
a4084 1
			 * the pmaps, and invalidating the generation
@


1.6.4.11
log
@sync to head
@
text
@d490 2
a491 2
	    boolean_t, cpuid_t, struct prm_thief *);
void	pmap_changebit(paddr_t, pt_entry_t, pt_entry_t, cpuid_t);
d496 2
a497 2
int	pmap_lev1map_create(pmap_t, cpuid_t);
void	pmap_lev1map_destroy(pmap_t, cpuid_t);
d501 1
a501 1
void	pmap_l3pt_delref(pmap_t, vaddr_t, pt_entry_t *, cpuid_t,
d503 2
a504 2
void	pmap_l2pt_delref(pmap_t, pt_entry_t *, pt_entry_t *, cpuid_t);
void	pmap_l1pt_delref(pmap_t, pt_entry_t *, cpuid_t);
d535 1
a535 1
void	pmap_asn_alloc(pmap_t, cpuid_t);
d1365 1
a1365 1
	cpuid_t cpu_id = cpu_number();
d1526 1
a1526 1
	cpuid_t cpu_id = cpu_number();
d1609 1
a1609 1
	cpuid_t cpu_id = cpu_number();
d1700 1
a1700 1
	cpuid_t cpu_id = cpu_number();
d2011 1
a2011 1
	cpuid_t cpu_id = cpu_number();
d2075 1
a2075 1
	cpuid_t cpu_id = cpu_number();
d2281 1
a2281 1
	cpuid_t cpu_id = cpu_number();
d2431 1
a2431 1
	cpuid_t cpu_id = cpu_number();
d2466 1
a2466 1
	cpuid_t cpu_id = cpu_number();
d2628 1
a2628 1
    boolean_t dolock, cpuid_t cpu_id, struct prm_thief *prmt)
d2752 1
a2752 1
pmap_changebit(paddr_t pa, u_long set, u_long mask, cpuid_t cpu_id)
d2842 1
a2842 1
	cpuid_t cpu_id = cpu_number();
d3139 1
a3139 1
	cpuid_t cpu_id;
d3502 1
a3502 1
pmap_lev1map_create(pmap_t pmap, cpuid_t cpu_id)
d3545 1
a3545 1
pmap_lev1map_destroy(pmap_t pmap, cpuid_t cpu_id)
d3744 1
a3744 1
	cpuid_t cpu_id = cpu_number();
d3878 1
a3878 1
pmap_l3pt_delref(pmap_t pmap, vaddr_t va, pt_entry_t *l3pte, cpuid_t cpu_id,
d3935 1
a3935 1
    cpuid_t cpu_id)
d3973 1
a3973 1
pmap_l1pt_delref(pmap_t pmap, pt_entry_t *l1pte, cpuid_t cpu_id)
d4000 1
a4000 1
pmap_asn_alloc(pmap_t pmap, cpuid_t cpu_id)
d4137 1
a4137 2
	u_long ipinum;
	cpuid_t i, cpu_id = cpu_number();
d4184 1
a4184 1
	cpuid_t cpu_id = ci->ci_cpuid;
d4212 1
a4212 1
pmap_tlb_shootdown_q_drain(cpuid_t cpu_id, boolean_t all)
@


1.5
log
@Merge to NetBSD 961020.  Retained our kernel APIs where NetBSD has changed.
-Wall -Wstrict-prototypes -Wmissing-prototypes too.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 1996/10/13 02:59:42 christos Exp $	*/
d1725 1
a1725 1
pmap_enter(pmap, v, pa, prot, wired)
d1731 1
@


1.4
log
@Add OpenBSD tags.  Adapt to OpenBSD *_intr_establish calling convention
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.8 1996/04/12 02:09:24 cgd Exp $	*/
/*	$NetBSD: pmap.c,v 1.8 1996/04/12 02:09:24 cgd Exp $	*/
d4 23
a26 67
/* 
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.6 (Berkeley) 5/27/94
 */

#ifdef XXX
/*
 * HP9000/300 series physical map management code.
 *
 * Supports:
 *	68020 with HP MMU	models 320, 350
 *	68020 with 68551 MMU	models 318, 319, 330 (all untested)
 *	68030 with on-chip MMU	models 340, 360, 370, 345, 375, 400
 *	68040 with on-chip MMU	models 380, 425, 433
 *
 * Notes:
 *	Don't even pay lip service to multiprocessor support.
 *
 *	We assume TLB entries don't have process tags (except for the
 *	supervisor/user distinction) so we only invalidate TLB entries
 *	when changing mappings for the current (or kernel) pmap.  This is
 *	technically not true for the 68551 but we flush the TLB on every
 *	context switch, so it effectively winds up that way.
 *
 *	Bitwise and/or operations are significantly faster than bitfield
 *	references so we use them when accessing STE/PTEs in the pmap_pte_*
 *	macros.  Note also that the two are not always equivalent; e.g.:
 *		(*(int *)pte & PG_PROT) [4] != pte->pg_prot [1]
 *	and a couple of routines that deal with protection and wiring take
 *	some shortcuts that assume the and/or definitions.
 *
 *	This implementation will only work for PAGE_SIZE == NBPG
 *	(i.e. 4096 bytes).
a27 1
#endif
d30 10
d42 7
d85 1
d88 139
d228 123
a350 89
#ifdef PMAPSTATS
struct {
	int collectscans;
	int collectpages;
	int kpttotal;
	int kptinuse;
	int kptmaxuse;
} kpt_stats;
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int nochange;	/* no change at all */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int pchange;	/* no mapping change, just protection */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
} enter_stats;
struct {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
} remove_stats;
struct {
	int calls;
	int changed;
	int alreadyro;
	int alreadyrw;
} protect_stats;
struct chgstats {
	int setcalls;
	int sethits;
	int setmiss;
	int clrcalls;
	int clrhits;
	int clrmiss;
} changebit_stats[16];
#endif

#ifdef DEBUG
#define PDB_FOLLOW	0x0001
#define PDB_INIT	0x0002
#define PDB_ENTER	0x0004
#define PDB_REMOVE	0x0008
#define PDB_CREATE	0x0010
#define PDB_PTPAGE	0x0020
#define PDB_CACHE	0x0040
#define PDB_BITS	0x0080
#define PDB_COLLECT	0x0100
#define PDB_PROTECT	0x0200
#define PDB_SEGTAB	0x0400
#define PDB_MULTIMAP	0x0800
#define PDB_BOOTSTRAP	0x1000
#define PDB_PARANOIA	0x2000
#define PDB_WIRING	0x4000
#define PDB_PVDUMP	0x8000

int debugmap = 0;
int pmapdebug = PDB_PARANOIA;
extern vm_offset_t pager_sva, pager_eva;
#endif

/*
 * Get STEs and PTEs for user/kernel address space
 */
#define	pmap_ste(m, v)	 (&((m)->pm_stab[vatoste((vm_offset_t)(v))]))
#define pmap_ste_v(m, v) (*pmap_ste(m, v) & PG_V)

#define pmap_pte(m, v)							\
	(&((m)->pm_ptab[NPTEPG * vatoste((vm_offset_t)(v)) +		\
	    vatopte((vm_offset_t)(v))]))
#define pmap_pte_pa(pte)	(PG_PFNUM(*(pte)) << PGSHIFT)
#define pmap_pte_prot(pte)	(*(pte) & PG_PROT)
#define pmap_pte_w(pte)		(*(pte) & PG_WIRED)
#define pmap_pte_v(pte)		(*(pte) & PG_V)

#define pmap_pte_set_w(pte, v) \
	if (v) *(u_long *)(pte) |= PG_WIRED; else *(u_long *)(pte) &= ~PG_WIRED
#define pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
d352 4
a355 3
#define pmap_pte_set_prot(pte, np)	{ *(pte) &= ~PG_PROT ; *(pte) |= (np); }
#define pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))
	
d358 1
a358 2
 * Given a map and a machine independent protection code,
 * convert to an hp300 protection code.
d360 105
a464 2
#define pte_prot(m, p)	(protection_codes[m == pmap_kernel() ? 0 : 1][p])
int	protection_codes[2][8];
d467 1
a467 1
 * Kernel page table page management.
d469 10
a478 7
struct kpt_page {
	struct kpt_page *kpt_next;	/* link on either used or free list */
	vm_offset_t	kpt_va;		/* always valid kernel VA */
	vm_offset_t	kpt_pa;		/* PA of this page (for speed) */
};
struct kpt_page *kpt_free_list, *kpt_used_list;
struct kpt_page *kpt_pages;
d481 2
a482 1
 * The Alpha's level-1 page table.
d484 10
a493 1
pt_entry_t	*Lev1map;
d496 1
a496 6
 * Kernel segment/page table and page table map.
 * The page table map gives us a level of indirection we need to dynamically
 * expand the page table.  It is essentially a copy of the segment table
 * with PTEs instead of STEs.  All are initialized in locore at boot time.
 * Segtabzero is an empty segment table which all processes share til they
 * reference something.
d498 2
a499 4
pt_entry_t	*Sysptmap;
pt_entry_t	*Sysmap;
vm_size_t	Sysptmapsize, Sysmapsize;
pt_entry_t	*Segtabzero, Segtabzeropte;
d502 1
a502 1
vm_map_t	st_map, pt_map;
d504 53
a556 9
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
vm_size_t	mem_size;	/* memory size in bytes */
vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
vm_offset_t	vm_first_phys;	/* PA of first managed page */
vm_offset_t	vm_last_phys;	/* PA just past last managed page */
boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
char		*pmap_attributes; /* reference and modify bits */
d559 5
a563 1
 * Internal routines
d565 47
a611 8
void alpha_protection_init __P((void));
void pmap_remove_mapping __P((pmap_t, vm_offset_t, pt_entry_t *, int));
void pmap_changebit	__P((vm_offset_t, u_long, boolean_t));
void pmap_enter_ptpage	__P((pmap_t, vm_offset_t));
#ifdef DEBUG
void pmap_pvdump	__P((vm_offset_t));
void pmap_check_wiring	__P((char *, vm_offset_t));
#endif
d613 1
a613 1
#define PAGE_IS_MANAGED(pa)	((pa) >= vm_first_phys && (pa) < vm_last_phys)
d615 2
a616 3
/* pmap_remove_mapping flags */
#define	PRM_TFLUSH	1
#define	PRM_CFLUSH	2
d619 10
a628 3
 * pmap_bootstrap:
 * Bootstrap the system to run with virtual memory.
 * firstaddr is the first unused kseg0 address (not page aligned).
d630 8
d639 3
a641 3
pmap_bootstrap(firstaddr, ptaddr)
	vm_offset_t firstaddr;
	vm_offset_t ptaddr;
d643 3
a645 1
	register int i;
d647 3
a649 2
	pt_entry_t pte;
	extern int firstusablepage, lastusablepage;
d651 3
a653 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_BOOTSTRAP))
		printf("pmap_bootstrap(0x%lx, 0x%lx)\n", firstaddr, ptaddr);
#endif
d658 2
a659 2
#define valloc(name, type, num)					\
	    (name) = (type *)firstaddr;				\
d661 3
d666 1
a666 2
	 * Allocate an empty prototype segment map for processes.
	 * This will be used until processes get their own.
d668 1
a668 13
	valloc(Segtabzero, pt_entry_t, NPTEPG);
        Segtabzeropte = (k0segtophys(Segtabzero) >> PGSHIFT) << PG_SHIFT;
	Segtabzeropte |= PG_V | PG_KRE | PG_KWE | PG_WIRED;

	/*
	 * Figure out how many PTE's are necessary to map the kernel.
	 * The '512' comes from PAGER_MAP_SIZE in vm_pager_init().
	 * This should be kept in sync.
	 * We also reserve space for kmem_alloc_pageable() for vm_fork().
	 */
	Sysmapsize = (VM_KMEM_SIZE + VM_MBUF_SIZE + VM_PHYS_SIZE +
		nbuf * MAXBSIZE + 16 * NCARGS) / NBPG + 512 + 256;
        Sysmapsize += maxproc * (btoc(ALPHA_STSIZE) + btoc(ALPHA_MAX_PTSIZE));
d670 4
a673 4
#ifdef SYSVSHM
	Sysmapsize += shminfo.shmall;
#endif
	Sysmapsize = roundup(Sysmapsize, NPTEPG);
d676 3
a678 3
	 * Allocate a level 1 PTE table for the kernel.
	 * This is always one page long.
	 * IF THIS IS NOT A MULTIPLE OF NBPG, ALL WILL GO TO HELL.
d680 10
a689 1
	valloc(Lev1map, pt_entry_t, NPTEPG);
d692 4
a695 3
	 * Allocate a level 2 PTE table for the kernel.
	 * These must map all of the level3 PTEs.
	 * IF THIS IS NOT A MULTIPLE OF NBPG, ALL WILL GO TO HELL.
a696 3
	Sysptmapsize = roundup(howmany(Sysmapsize, NPTEPG), NPTEPG);
	valloc(Sysptmap, pt_entry_t, Sysptmapsize);
	pmap_kernel()->pm_stab = Sysptmap;
d698 26
d725 1
a725 2
	 * Allocate a level 3 PTE table for the kernel.
	 * Contains Sysmapsize PTEs.
d727 3
a729 2
	valloc(Sysmap, pt_entry_t, Sysmapsize);
	pmap_kernel()->pm_ptab = Sysmap;
d731 1
d733 4
a736 2
	 * Allocate memory for page attributes.
	 * allocates a few more entries than we need, but that's safe.
d738 11
a748 1
	valloc(pmap_attributes, char, 1 + lastusablepage - firstusablepage);
d751 1
a751 5
	 * Allocate memory for pv_table.
	 * This will allocate more entries than we really need.
	 * We could do this in pmap_init when we know the actual
	 * phys_start and phys_end but its better to use kseg0 addresses
	 * rather than kernel virtual addresses mapped through the TLB.
d753 12
a764 2
	i = 1 + lastusablepage - alpha_btop(k0segtophys(firstaddr));
	valloc(pv_table, struct pv_entry, i);
d767 3
a769 1
	 * Clear allocated memory.
a770 2
	firstaddr = alpha_round_page(firstaddr);
	bzero((caddr_t)start, firstaddr - start);
d773 1
a773 1
	 * Set up level 1 page table
d775 5
d781 9
a789 11
	/* First, copy mappings for things below VM_MIN_KERNEL_ADDRESS */
	bcopy((caddr_t)ptaddr, Lev1map,
	    kvtol1pte(VM_MIN_KERNEL_ADDRESS) * sizeof Lev1map[0]);

	/* Second, map all of the level 2 pte pages */
	for (i = 0; i < howmany(Sysptmapsize, NPTEPG); i++) {
		pte = (k0segtophys(Sysptmap + (i*PAGE_SIZE)) >> PGSHIFT)
		    << PG_SHIFT;
		pte |= PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		Lev1map[kvtol1pte(VM_MIN_KERNEL_ADDRESS +
		    (i*PAGE_SIZE*NPTEPG*NPTEPG))] = pte;
a791 5
	/* Finally, map the virtual page table */
	pte = (k0segtophys(Lev1map) >> PGSHIFT) << PG_SHIFT;
	pte |= PG_V | PG_KRE | PG_KWE; /* NOTE NO ASM */
	Lev1map[kvtol1pte(VPTBASE)] = pte;
	
d793 1
a793 1
	 * Set up level 2 page table.
d795 6
a800 8
	/* Map all of the level 3 pte pages */
	for (i = 0; i < howmany(Sysmapsize, NPTEPG); i++) {
		pte = (k0segtophys(((caddr_t)Sysmap)+(i*PAGE_SIZE)) >> PGSHIFT)
		    << PG_SHIFT;
		pte |= PG_V | PG_ASM | PG_KRE | PG_KWE | PG_WIRED;
		Sysptmap[vatoste(VM_MIN_KERNEL_ADDRESS+
		    (i*PAGE_SIZE*NPTEPG))] = pte;
	}
d803 2
a804 1
	 * Set up level three page table (Sysmap)
d806 14
a819 1
	/* Nothing to do; it's already zero'd */
d821 3
a823 3
	avail_start = k0segtophys(firstaddr);
#if 1
	avail_end = alpha_ptob(lastusablepage + 1);
d825 3
a827 10
#else
	/* XXX why not lastusablepage + 1, & not include NBPG in mem_size? */
	avail_end = alpha_ptob(lastusablepage);
	mem_size = NBPG + avail_end - avail_start;
#endif
#if 0
	printf("avail_start = 0x%lx\n", avail_start);
	printf("avail_end = 0x%lx\n", avail_end);
	printf("mem_size = 0x%lx\n", mem_size);
#endif
d829 3
a831 2
	virtual_avail = VM_MIN_KERNEL_ADDRESS;
	virtual_end = VM_MIN_KERNEL_ADDRESS + Sysmapsize * NBPG;
d833 6
a838 2
	simple_lock_init(&pmap_kernel()->pm_lock);
	pmap_kernel()->pm_count = 1;
d841 3
a843 2
	 * Set up curproc's (i.e. proc 0's) PCB such that the ptbr
	 * points to the right place.
d845 5
a849 1
	curproc->p_addr->u_pcb.pcb_ptbr = k0segtophys(Lev1map) >> PGSHIFT;
d852 1
a852 9
/*
 * Unmap the PROM mappings.  PROM mappings are kept around
 * by pmap_bootstrap, so we can still use the prom's printf.
 * Basically, blow away all mappings in the level one PTE
 * table below VM_MIN_KERNEL_ADDRESS.  The Virtual Page Table
 * Is at the end of virtual space, so it's safe.
 */
void
pmap_unmap_prom()
d854 7
a860 17
	int i;
	extern int prom_mapped;
	extern pt_entry_t *rom_ptep, rom_pte;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_BOOTSTRAP))
		printf("pmap_unmap_prom\n");
#endif

	/* XXX save old pte so that we can remap prom if necessary */
	rom_ptep = &Lev1map[0];					/* XXX */
	rom_pte = *rom_ptep & ~PG_ASM;				/* XXX */

	/* Mark all mappings before VM_MIN_KERNEL_ADDRESS as invalid. */
	bzero(Lev1map, kvtol1pte(VM_MIN_KERNEL_ADDRESS) * sizeof Lev1map[0]);
	prom_mapped = 0;
	TBIA();
d868 2
a869 2
 * stealing virtual address space, then implicitly mapping the pages 
 * (by using their k0seg addresses) and zeroing them.
d875 1
d883 1
a883 2
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_BOOTSTRAP))
a884 1
#endif
d888 2
a889 2
	val = phystok0seg(avail_start);
	size = round_page(size);
d891 2
d895 3
d902 5
a906 3
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
d909 1
a909 2
pmap_init(phys_start, phys_end)
	vm_offset_t	phys_start, phys_end;
d911 3
a913 2
	vm_offset_t	addr, addr2;
        vm_size_t	s;
d915 18
a932 4
#ifdef DEBUG
        if (pmapdebug & PDB_FOLLOW)
                printf("pmap_init(%x, %x)\n", phys_start, phys_end);
#endif
d934 11
a944 2
	/* initialize protection array */
	alpha_protection_init();
d947 2
a948 1
	 * Allocate the segment table map
d950 6
a955 2
	s = maxproc * ALPHA_STSIZE;
	st_map = kmem_suballoc(kernel_map, &addr, &addr2, s, TRUE);
d957 1
d959 1
a959 1
	 * Allocate the page table map
d961 11
a971 3
	s = maxproc * ALPHA_MAX_PTSIZE;			/* XXX limit it */
	pt_map = kmem_suballoc(kernel_map, &addr, &addr2, s, TRUE);
	
d973 4
a976 1
	 * Now it is safe to enable pv_table recording.
d978 1
a980 4
#if 0
	printf("vm_first_phys = 0x%lx\n", vm_first_phys);
	printf("vm_last_phys = 0x%lx\n", vm_last_phys);
#endif
d984 3
d988 1
a988 2
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
d990 4
a993 2
 *	For now, VM is already on, we only need to map the
 *	specified memory.
d996 50
a1045 5
pmap_map(virt, start, end, prot)
	vm_offset_t	virt;
	vm_offset_t	start;
	vm_offset_t	end;
	int		prot;
d1047 11
a1057 10
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%lx, %lx, %lx, %lx)\n", virt, start, end, prot);
#endif
	while (start < end) {
		pmap_enter(pmap_kernel(), virt, start, prot, FALSE);
		virt += PAGE_SIZE;
		start += PAGE_SIZE;
	}
	return(virt);
d1074 1
a1074 1
	vm_size_t	size;
d1076 4
a1079 1
	register pmap_t pmap;
a1080 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create(%lx)\n", size);
#endif
d1082 1
a1082 1
	 * Software use map does not need a pmap
d1084 5
a1088 2
	if (size)
		return(NULL);
d1091 2
a1092 3
	pmap = (pmap_t) malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
#ifdef notifwewait
	if (pmap == NULL)
d1094 8
a1101 4
#endif
	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return (pmap);
a1103 4
/*
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
 */
d1105 2
a1106 2
pmap_pinit(pmap)
	register struct pmap *pmap;
d1108 5
d1114 5
a1118 3
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_pinit(%lx)\n", pmap);
d1120 28
d1149 1
a1149 4
	 * No need to allocate page table space yet but we do need a
	 * valid segment table.  Initially, we point everyone at the
	 * "null" segment table.  On the first pmap_enter, a real
	 * segment table will be allocated.
d1151 8
a1158 5
	pmap->pm_stab = Segtabzero;
	pmap->pm_stpte = Segtabzeropte;
	pmap->pm_stchanged = TRUE;
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
d1166 3
a1168 3
void
pmap_destroy(pmap)
	register pmap_t pmap;
d1170 8
a1177 1
	int count;
d1179 5
a1183 6
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%lx)\n", pmap);
#endif
	if (pmap == NULL)
		return;
d1185 3
a1187 6
	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		free((caddr_t)pmap, M_VMPMAP);
d1189 3
a1193 5
/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
 */
d1195 2
a1196 2
pmap_release(pmap)
	register struct pmap *pmap;
d1198 40
d1239 3
a1241 16
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_release(%lx)\n", pmap);
#endif
#ifdef notdef /* DIAGNOSTIC */
	/* count would be 0 from pmap_destroy... */
	simple_lock(&pmap->pm_lock);
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif
	if (pmap->pm_ptab)
		kmem_free_wakeup(pt_map, (vm_offset_t)pmap->pm_ptab,
				 ALPHA_MAX_PTSIZE);
	if (pmap->pm_stab != Segtabzero)
		kmem_free_wakeup(st_map, (vm_offset_t)pmap->pm_stab,
				 ALPHA_STSIZE);
d1247 3
a1249 3
void
pmap_reference(pmap)
	pmap_t	pmap;
d1251 7
a1257 8
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%lx)\n", pmap);
#endif
	if (pmap != NULL) {
		simple_lock(&pmap->pm_lock);
		pmap->pm_count++;
		simple_unlock(&pmap->pm_lock);
d1262 4
a1265 1
 *	Remove the given range of addresses from the specified map.
d1267 4
a1270 2
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
a1271 9
void
pmap_remove(pmap, sva, eva)
	register pmap_t pmap;
	register vm_offset_t sva, eva;
{
	register vm_offset_t nssva;
	register pt_entry_t *pte;
	boolean_t firstpage, needcflush;
	int flags;
d1273 25
a1297 4
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%lx, %lx, %lx)\n", pmap, sva, eva);
#endif
d1299 1
a1299 2
	if (pmap == NULL)
		return;
a1300 10
#ifdef PMAPSTATS
	remove_stats.calls++;
#endif
	firstpage = TRUE;
	needcflush = FALSE;
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
	while (sva < eva) {
		nssva = alpha_trunc_seg(sva) + ALPHA_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;
d1302 2
a1303 2
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
d1305 38
a1342 3
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
d1344 21
a1364 8
		/*
		 * Invalidate every valid mapping within this segment.
		 */
		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte)) {
				pmap_remove_mapping(pmap, sva, pte, flags);
				firstpage = FALSE;
d1366 3
a1368 2
			pte++;
			sva += PAGE_SIZE;
d1370 2
d1373 1
d1375 1
a1375 1
	 * Didn't do anything, no need for cache flushes
d1377 78
a1454 1
	if (firstpage)
d1456 26
d1485 1
a1485 1
 *	pmap_page_protect:
d1487 3
a1489 1
 *	Lower the permission for all mappings to a given page.
d1491 4
a1494 3
void
pmap_page_protect(pa, prot)
	vm_offset_t	pa;
d1497 17
a1513 10
	register pv_entry_t pv;
	int s;

#ifdef DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE))
		printf("pmap_page_protect(%lx, %lx)\n", pa, prot);
#endif
	if (!PAGE_IS_MANAGED(pa))
		return;
d1515 3
d1519 8
a1526 10
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
		return;
	/* copy_on_write */
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
/* XXX */	pmap_changebit(pa, PG_KWE | PG_UWE, FALSE);
		return;
	/* remove_all */
	default:
d1529 83
a1611 14
	pv = pa_to_pvh(pa);
	s = splimp();
	while (pv->pv_pmap != NULL) {
		register pt_entry_t *pte;

		pte = pmap_pte(pv->pv_pmap, pv->pv_va);
#ifdef DEBUG
		if (!pmap_ste_v(pv->pv_pmap, pv->pv_va) ||
		    pmap_pte_pa(pte) != pa)
			panic("pmap_page_protect: bad mapping");
#endif
		if (!pmap_pte_w(pte))
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
					    pte, PRM_TFLUSH|PRM_CFLUSH);
d1613 28
a1640 6
			pv = pv->pv_next;
#ifdef DEBUG
			if (pmapdebug & PDB_PARANOIA)
				printf("%s wired mapping for %lx not removed\n",
				       "pmap_page_protect:", pa);
#endif
d1642 1
d1644 2
a1645 1
	splx(s);
d1651 1
d1653 4
a1656 5
void
pmap_protect(pmap, sva, eva, prot)
	register pmap_t	pmap;
	register vm_offset_t sva, eva;
	vm_prot_t prot;
d1658 4
a1661 8
	register vm_offset_t nssva;
	register pt_entry_t *pte, bits;
	boolean_t firstpage, needtflush;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%lx, %lx, %lx, %lx)\n", pmap, sva, eva, prot);
#endif
d1663 1
a1663 1
	if (pmap == NULL)
d1666 16
a1681 5
#ifdef PMAPSTATS
	protect_stats.calls++;
#endif
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
a1683 2
	if (prot & VM_PROT_WRITE)
		return;
d1685 21
a1705 41
	bits = pte_prot(pmap, prot);
	needtflush = active_pmap(pmap);
	firstpage = TRUE;
	while (sva < eva) {
		nssva = alpha_trunc_seg(sva) + ALPHA_SEG_SIZE;
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}
		/*
		 * Change protection on mapping if it is valid and doesn't
		 * already have the correct protection.
		 */
		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte) && pmap_pte_prot_chg(pte, bits)) {
				pmap_pte_set_prot(pte, bits);
				if (needtflush)
					TBIS((caddr_t)sva);
#ifdef PMAPSTATS
				protect_stats.changed++;
#endif
				firstpage = FALSE;
			}
#ifdef PMAPSTATS
			else if (pmap_pte_v(pte)) {
				if (isro)
					protect_stats.alreadyro++;
				else
					protect_stats.alreadyrw++;
			}
#endif
			pte++;
			sva += PAGE_SIZE;
		}
d1707 3
d1725 41
a1765 34
pmap_enter(pmap, va, pa, prot, wired)
	register pmap_t pmap;
	vm_offset_t va;
	register vm_offset_t pa;
	vm_prot_t prot;
	boolean_t wired;
{
	register pt_entry_t *pte;
	register pt_entry_t npte;
	vm_offset_t opa;
	boolean_t checkpv = TRUE;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%lx, %lx, %lx, %lx, %lx)\n",
		       pmap, va, pa, prot, wired);
#endif
	if (pmap == NULL)
		return;

#ifdef PMAPSTATS
	if (pmap == pmap_kernel())
		enter_stats.kernel++;
	else
		enter_stats.user++;
#endif
	/*
	 * For user mapping, allocate kernel VM resources if necessary.
	 */
	if (pmap->pm_ptab == NULL)
		pmap->pm_ptab = (pt_entry_t *)
			kmem_alloc_wait(pt_map, ALPHA_MAX_PTSIZE);
	/*
	 * Segment table entry not valid, we need a new PT page
a1766 10
	if (!pmap_ste_v(pmap, va))
		pmap_enter_ptpage(pmap, va);

	pa = alpha_trunc_page(pa);
	pte = pmap_pte(pmap, va);
	opa = pmap_pte_pa(pte);
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: pte %lx, *pte %lx\n", pte, *pte);
#endif
d1768 1
a1768 7
	/*
	 * Mapping has not changed, must be protection or wiring change.
	 */
	if (opa == pa) {
#ifdef PMAPSTATS
		enter_stats.pwchange++;
#endif
d1770 1
a1770 4
		 * Wiring change, just update stats.
		 * We don't worry about wiring PT pages as they remain
		 * resident as long as there are valid mappings in them.
		 * Hence, if a user page is wired, the PT page will be also.
d1772 5
a1776 25
		if (pmap_pte_w_chg(pte, wired ? PG_WIRED : 0)) {
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("enter: wiring change -> %lx\n", wired);
#endif
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
#ifdef PMAPSTATS
			if (pmap_pte_prot(pte) == pte_prot(pmap, prot))
				enter_stats.wchange++;
#endif
		}
#ifdef PMAPSTATS
		else if (pmap_pte_prot(pte) != pte_prot(pmap, prot))
			enter_stats.pchange++;
		else
			enter_stats.nochange++;
#endif
		/*
		 * Retain cache inhibition status
		 */
		checkpv = FALSE;
		goto validate;
d1780 31
a1810 12
	 * Mapping has changed, invalidate old range and fall through to
	 * handle validating new mapping.
	 */
	if (opa) {
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: removing old mapping %lx\n", va);
#endif
		pmap_remove_mapping(pmap, va, pte, PRM_TFLUSH|PRM_CFLUSH);
#ifdef PMAPSTATS
		enter_stats.mchange++;
#endif
d1812 1
d1814 6
a1819 17
	/*
	 * If this is a new user mapping, increment the wiring count
	 * on this PT page.  PT pages are wired down as long as there
	 * is a valid mapping in the page.
	 */
	if (pmap != pmap_kernel())
		(void) vm_map_pageable(pt_map, trunc_page(pte),
				       round_page(pte+1), FALSE);

	/*
	 * Enter on the PV list if part of our managed memory
	 * Note that we raise IPL while manipulating pv_table
	 * since pmap_enter can be called at interrupt time.
	 */
	if (PAGE_IS_MANAGED(pa)) {
		register pv_entry_t pv, npv;
		int s;
a1820 10
#ifdef PMAPSTATS
		enter_stats.managed++;
#endif
		pv = pa_to_pvh(pa);
		s = splimp();
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: pv at %lx: %lx/%lx/%lx\n",
			       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
d1822 2
a1823 1
		 * No entries yet, use header as the first entry
d1825 2
a1826 11
		if (pv->pv_pmap == NULL) {
#ifdef PMAPSTATS
			enter_stats.firstpv++;
#endif
			pv->pv_va = va;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
			pv->pv_ptpte = NULL;
			pv->pv_ptpmap = NULL;
			pv->pv_flags = 0;
		}
d1828 2
a1829 2
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
d1831 29
d1861 32
a1892 17
#ifdef DEBUG
			for (npv = pv; npv; npv = npv->pv_next)
				if (pmap == npv->pv_pmap && va == npv->pv_va)
					panic("pmap_enter: already in pv_tab");
#endif
			npv = (pv_entry_t)
				malloc(sizeof *npv, M_VMPVENT, M_NOWAIT);
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_ptpte = NULL;
			npv->pv_ptpmap = NULL;
			npv->pv_flags = 0;
			pv->pv_next = npv;
#ifdef PMAPSTATS
			if (!npv->pv_next)
				enter_stats.secondpv++;
d1894 9
d1904 2
a1905 19
		splx(s);
	}
	/*
	 * Assumption: if it is not part of our managed memory
	 * then it must be device memory which may be volitile.
	 */
	else if (pmap_initialized) {
		checkpv = FALSE;
#ifdef PMAPSTATS
		enter_stats.unmanaged++;
#endif
	}

	/*
	 * Increment counters
	 */
	pmap->pm_stats.resident_count++;
	if (wired)
		pmap->pm_stats.wired_count++;
d1907 37
a1943 29
validate:
	/*
	 * Build the new PTE.
	 */
	npte = ((pa >> PGSHIFT) << PG_SHIFT) | pte_prot(pmap, prot) | PG_V;
	if (PAGE_IS_MANAGED(pa)) {
		if ((pmap_attributes[pa_index(pa)] & PMAP_ATTR_REF) == 0)
			npte |= PG_FOR | PG_FOW | PG_FOE;
		else if ((pmap_attributes[pa_index(pa)] & PMAP_ATTR_MOD) == 0)
			npte |= PG_FOW;
	}
	if (wired)
		npte |= PG_WIRED;
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: new pte value %lx\n", npte);
#endif
	/*
	 * Remember if this was a wiring-only change.
	 * If so, we need not flush the TLB and caches.
	 */
	wired = ((*pte ^ npte) == PG_WIRED);
	*pte = npte;
	if (!wired && active_pmap(pmap))
		TBIS((caddr_t)va);
#ifdef DEBUG
	if ((pmapdebug & PDB_WIRING) && pmap != pmap_kernel())
		pmap_check_wiring("enter", trunc_page(pmap_pte(pmap, va)));
#endif
d1953 3
a1955 4
void
pmap_change_wiring(pmap, va, wired)
	register pmap_t	pmap;
	vm_offset_t	va;
d1958 34
a1991 8
	register pt_entry_t *pte;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_change_wiring(%lx, %lx, %lx)\n", pmap, va, wired);
#endif
	if (pmap == NULL)
		return;
d1993 1
a1993 33
	pte = pmap_pte(pmap, va);
#ifdef DEBUG
	/*
	 * Page table page is not allocated.
	 * Should this ever happen?  Ignore it for now,
	 * we don't want to force allocation of unnecessary PTE pages.
	 */
	if (!pmap_ste_v(pmap, va)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_change_wiring: invalid STE for %lx\n", va);
		return;
	}
	/*
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
	 */
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_change_wiring: invalid PTE for %lx\n", va);
	}
#endif
	/*
	 * If wiring actually changed (always?) set the wire bit and
	 * update the wire count.  Note that wiring is not a hardware
	 * characteristic so there is no need to invalidate the TLB.
	 */
	if (pmap_pte_w_chg(pte, wired ? PG_WIRED : 0)) {
		pmap_pte_set_w(pte, wired);
		if (wired)
			pmap->pm_stats.wired_count++;
		else
			pmap->pm_stats.wired_count--;
	}
d2006 45
a2050 1
	vm_offset_t va;
d2052 2
a2053 2
	pt_entry_t pte;
	register vm_offset_t pa;
d2055 7
a2061 9
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%lx, %lx) -> ", pmap, va);
#endif
	pa = 0;
	if (pmap && pmap_ste_v(pmap, va)) {
		pte = *pmap_pte(pmap, va);
		if (pte & PG_V)
			pa = ctob(PG_PFNUM(pte)) | (va & PGOFSET);
d2064 8
a2071 4
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("%lx\n", pa);
#endif
d2076 124
d2206 1
d2214 3
a2216 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%lx, %lx, %lx, %lx, %lx)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
#endif
a2217 14

/*
 *	Require that all active physical maps contain no
 *	incorrect entries NOW.  [This update includes
 *	forcing updates of any address map caching.]
 *
 *	Generally used to insure that a thread about
 *	to run will see a semantically correct world.
 */
void pmap_update()
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_update()\n");
a2218 2
	TBIA();
}
d2231 2
a2232 3
void
pmap_collect(pmap)
	pmap_t		pmap;
d2234 12
a2245 11
	register vm_offset_t pa;
	register pv_entry_t pv;
	register pt_entry_t *pte;
	vm_offset_t kpa;
	int s;

#ifdef DEBUG
	pt_entry_t *ste;
	int opmapdebug;
#endif
	if (pmap != pmap_kernel())
d2248 16
a2263 10
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%lx)\n", pmap);
#endif
#ifdef PMAPSTATS
	kpt_stats.collectscans++;
#endif
	s = splimp();
	for (pa = vm_first_phys; pa < vm_last_phys; pa += PAGE_SIZE) {
		register struct kpt_page *kpt, **pkpt;
d2266 2
a2267 2
		 * Locate physical pages which are being used as kernel
		 * page table pages.
d2269 9
a2277 31
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptpte && pv->pv_ptpmap == pmap_kernel())
				break;
		} while (pv = pv->pv_next);
		if (pv == NULL)
			continue;
#ifdef DEBUG
		if (pv->pv_va < (vm_offset_t)Sysmap ||
		    pv->pv_va >= (vm_offset_t)Sysmap + ALPHA_MAX_PTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (pt_entry_t *)(pv->pv_va + ALPHA_PAGE_SIZE);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf("collect: freeing KPT page at %lx (ste %lx@@%lx)\n",
			       pv->pv_va, *pv->pv_ptpte, pv->pv_ptpte);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
d2279 38
d2318 6
a2323 1
		ste = pv->pv_ptpte;
d2325 31
a2355 43
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove_entry to take care of invalidating
		 * ST and Sysptmap entries.
		 */
		kpa = pmap_extract(pmap, pv->pv_va);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				    PRM_TFLUSH|PRM_CFLUSH);
		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != (struct kpt_page *)0;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef DEBUG
		if (kpt == (struct kpt_page *)0)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			       kpt->kpt_va, kpa);
#endif
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef PMAPSTATS
		kpt_stats.kptinuse--;
		kpt_stats.collectpages++;
#endif
#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste)
			printf("collect: kernel STE at %lx still valid (%lx)\n",
			       ste, *ste);
		ste = &Sysptmap[(pt_entry_t *)ste-pmap_ste(pmap_kernel(), 0)];
		if (*ste)
			printf("collect: kernel PTmap at %lx still valid (%lx)\n",
			       ste, *ste);
a2356 2
	}
	splx(s);
d2359 7
d2367 4
a2370 2
pmap_activate(pmap)
	register pmap_t pmap;
d2372 2
a2373 1
	int iscurproc;
d2375 7
a2381 3
#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_SEGTAB))
		printf("pmap_activate(%lx)\n", pmap);
d2383 1
d2385 9
a2393 2
	iscurproc = curproc != NULL && pmap == curproc->p_vmspace->vm_map.pmap;
	PMAP_ACTIVATE(pmap, iscurproc);
d2395 1
d2398 2
a2399 4
 *	pmap_zero_page zeros the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bzero to clear its contents, one machine dependent page
 *	at a time.
d2401 1
d2404 1
a2404 1
	vm_offset_t phys;
a2405 1
	caddr_t p;
d2407 10
a2416 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%lx)\n", phys);
a2417 3
	p = (caddr_t)phystok0seg(phys);
	bzero(p, PAGE_SIZE);
}
d2420 2
a2421 4
 *	pmap_copy_page copies the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bcopy to copy the page, one machine dependent page at a
 *	time.
d2423 1
d2426 1
a2426 1
	vm_offset_t src, dst;
a2427 1
	caddr_t s, d;
d2429 11
a2439 3
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%lx, %lx)\n", src, dst);
a2440 4
        s = (caddr_t)phystok0seg(src);
        d = (caddr_t)phystok0seg(dst);
	bcopy(s, d, PAGE_SIZE);
}
d2457 1
a2457 1
pmap_pageable(pmap, sva, eva, pageable)
d2459 2
a2460 1
	vm_offset_t	sva, eva;
d2463 2
a2464 52
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pageable(%lx, %lx, %lx, %lx)\n",
		       pmap, sva, eva, pageable);
#endif
	/*
	 * If we are making a PT page pageable then all valid
	 * mappings must be gone from that page.  Hence it should
	 * be all zeros and there is no need to clean it.
	 * Assumptions:
	 *	- we are called with only one page at a time
	 *	- PT pages have only one pv_table entry
	 */
	if (pmap == pmap_kernel() && pageable && sva + PAGE_SIZE == eva) {
		register pv_entry_t pv;
		register vm_offset_t pa;

#ifdef DEBUG
		if ((pmapdebug & (PDB_FOLLOW|PDB_PTPAGE)) == PDB_PTPAGE)
			printf("pmap_pageable(%lx, %lx, %lx, %lx)\n",
			       pmap, sva, eva, pageable);
#endif
		if (!pmap_ste_v(pmap, sva))
			return;
		pa = pmap_pte_pa(pmap_pte(pmap, sva));
		if (!PAGE_IS_MANAGED(pa))
			return;
		pv = pa_to_pvh(pa);
		if (pv->pv_ptpte == NULL)
			return;
#ifdef DEBUG
		if (pv->pv_va != sva || pv->pv_next) {
			printf("pmap_pageable: bad PT page va %lx next %lx\n",
			       pv->pv_va, pv->pv_next);
			return;
		}
#endif
		/*
		 * Mark it unmodified to avoid pageout
		 */
		pmap_clear_modify(pa);
#ifdef DEBUG
		if ((PHYS_TO_VM_PAGE(pa)->flags & PG_CLEAN) == 0) {
			printf("pa %lx: flags=%lx: not clean\n",
			       pa, PHYS_TO_VM_PAGE(pa)->flags);
			PHYS_TO_VM_PAGE(pa)->flags |= PG_CLEAN;
		}
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_pageable: PT page %lx(%lx) unmodified\n",
			       sva, *pmap_pte(pmap, sva));
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("pageable", sva);
a2465 1
	}
d2469 1
a2469 1
 *	Clear the modify bits on the specified physical page.
a2470 1

d2472 3
a2474 2
pmap_clear_modify(pa)
	vm_offset_t	pa;
d2476 13
a2488 9
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%lx)\n", pa);
#endif
	if (!PAGE_IS_MANAGED(pa))		/* XXX why not panic? */
		return;
	if ((pmap_attributes[pa_index(pa)] & PMAP_ATTR_MOD) != 0) {
		pmap_changebit(pa, PG_FOW, TRUE);
		pmap_attributes[pa_index(pa)] &= ~PMAP_ATTR_MOD;
a2489 1
}
d2491 46
a2536 5
/*
 *	pmap_clear_reference:
 *
 *	Clear the reference bit on the specified physical page.
 */
d2538 11
a2548 12
void pmap_clear_reference(pa)
	vm_offset_t	pa;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%lx)\n", pa);
#endif
	if (!PAGE_IS_MANAGED(pa))		/* XXX why not panic? */
		return;
	if ((pmap_attributes[pa_index(pa)] & PMAP_ATTR_REF) != 0) {
		pmap_changebit(pa, PG_FOR | PG_FOW | PG_FOE, TRUE);
		pmap_attributes[pa_index(pa)] &= ~PMAP_ATTR_REF;
d2550 4
d2557 1
a2557 4
 *	pmap_is_referenced:
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
a2558 1

d2560 3
a2562 2
pmap_is_referenced(pa)
	vm_offset_t	pa;
d2564 6
a2569 1
	boolean_t rv;
d2571 6
a2576 6
	if (!PAGE_IS_MANAGED(pa))		/* XXX why not panic? */
		return 0;
	rv = (pmap_attributes[pa_index(pa)] & PMAP_ATTR_REF) != 0;
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_is_referenced(%lx) -> %c\n", pa, "FT"[rv]);
a2577 3
#endif
	return rv;
}
d2579 6
a2584 6
/*
 *	pmap_is_modified:
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
d2586 2
a2587 5
boolean_t
pmap_is_modified(pa)
	vm_offset_t	pa;
{
	boolean_t rv;
d2589 3
a2591 6
	if (!PAGE_IS_MANAGED(pa))		/* XXX why not panic? */
		return 0;
	rv = (pmap_attributes[pa_index(pa)] & PMAP_ATTR_MOD) != 0;
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_is_modified(%lx) -> %c\n", pa, "FT"[rv]);
d2593 30
d2624 7
a2630 2
	return rv;
}
d2632 13
a2644 5
vm_offset_t
pmap_phys_address(ppn)
	int ppn;
{
	return(alpha_ptob(ppn));
d2648 1
a2648 1
 * Miscellaneous support routines follow
d2650 27
d2679 1
a2679 1
 * Initialize Alpha protection code array.
d2681 165
a2845 3
/* static */
void
alpha_protection_init()
d2847 4
a2850 1
	int prot, *kp, *up;
d2852 2
a2853 2
	kp = protection_codes[0];
	up = protection_codes[1];
d2855 8
a2862 23
	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE:
			*kp++ = PG_ASM;
			*up++ = 0;
			break;
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_EXECUTE:
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE:
			*kp++ = PG_ASM | PG_KRE;
			*up++ = PG_URE | PG_KRE;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			*kp++ = PG_ASM | PG_KWE;
			*up++ = PG_UWE | PG_KWE;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			*kp++ = PG_ASM | PG_KWE | PG_KRE;
			*up++ = PG_UWE | PG_URE | PG_KWE | PG_KRE;
			break;
		}
d2864 3
d2869 14
d2884 1
a2884 4
 * Invalidate a single page denoted by pmap/va.
 * If (pte != NULL), it is the already computed PTE for the page.
 * If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 * If (flags & PRM_CFLUSH), we must flush/invalidate any cache information.
d2886 7
a2892 20
/* static */
void
pmap_remove_mapping(pmap, va, pte, flags)
	register pmap_t pmap;
	register vm_offset_t va;
	register pt_entry_t *pte;
	int flags;
{
	register vm_offset_t pa;
	register pv_entry_t pv, npv;
	pmap_t ptpmap;
	pt_entry_t *ste;
	int s;
#ifdef DEBUG
	pt_entry_t opte;

	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove_mapping(%lx, %lx, %lx, %lx)\n",
		       pmap, va, pte, flags);
#endif
d2894 9
a2902 7
	/*
	 * PTE not provided, compute it from pmap and va.
	 */
	if (pte == PT_ENTRY_NULL) {
		pte = pmap_pte(pmap, va);
		if (*pte == PG_NV)
			return;
a2903 6
	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	opte = *pte;
#endif
#ifdef PMAPSTATS
	remove_stats.removes++;
d2906 4
a2909 1
	 * Update statistics
d2911 10
a2920 3
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;
d2923 3
a2925 1
	 * Invalidate the PTE after saving the reference modify info.
d2927 53
a2979 21
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf("remove: invalidating pte at %lx\n", pte);
#endif
	*pte = PG_NV;
	if ((flags & PRM_TFLUSH) && active_pmap(pmap))
		TBIS((caddr_t)va);
	/*
	 * For user mappings decrement the wiring count on
	 * the PT page.  We do this after the PTE has been
	 * invalidated because vm_map_pageable winds up in
	 * pmap_pageable which clears the modify bit for the
	 * PT page.
	 */
	if (pmap != pmap_kernel()) {
		(void) vm_map_pageable(pt_map, trunc_page(pte),
				       round_page(pte+1), TRUE);
#ifdef DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", trunc_page(pte));
#endif
d2981 6
a2986 30
	/*
	 * If this isn't a managed page, we are all done.
	 */
	if (!PAGE_IS_MANAGED(pa))
		return;
	/*
	 * Otherwise remove it from the PV table
	 * (raise IPL since we may be called at interrupt time).
	 */
	pv = pa_to_pvh(pa);
	ste = NULL;
	s = splimp();
	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		ste = pv->pv_ptpte;
		ptpmap = pv->pv_ptpmap;
		npv = pv->pv_next;
		if (npv) {
			npv->pv_flags = pv->pv_flags;
			*pv = *npv;
			free((caddr_t)npv, M_VMPVENT);
		} else
			pv->pv_pmap = NULL;
#ifdef PMAPSTATS
		remove_stats.pvfirst++;
d2988 11
a2998 74
	} else {
		for (npv = pv->pv_next; npv; npv = npv->pv_next) {
#ifdef PMAPSTATS
			remove_stats.pvsearch++;
#endif
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
			pv = npv;
		}
#ifdef DEBUG
		if (npv == NULL)
			panic("pmap_remove: PA not in pv_tab");
#endif
		ste = npv->pv_ptpte;
		ptpmap = npv->pv_ptpmap;
		pv->pv_next = npv->pv_next;
		free((caddr_t)npv, M_VMPVENT);
		pv = pa_to_pvh(pa);
	}
	/*
	 * If this was a PT page we must also remove the
	 * mapping from the associated segment table.
	 */
	if (ste) {
#ifdef PMAPSTATS
		remove_stats.ptinvalid++;
#endif
#ifdef DEBUG
		if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
			printf("remove: ste was %lx@@%lx pte was %lx@@%lx\n",
			       *ste, ste, opte, pmap_pte(pmap, va));
#endif
		*ste = PG_NV;
		/*
		 * If it was a user PT page, we decrement the
		 * reference count on the segment table as well,
		 * freeing it if it is now empty.
		 */
		if (ptpmap != pmap_kernel()) {
#ifdef DEBUG
			if (pmapdebug & (PDB_REMOVE|PDB_SEGTAB))
				printf("remove: stab %lx, refcnt %d\n",
				       ptpmap->pm_stab, ptpmap->pm_sref - 1);
			if ((pmapdebug & PDB_PARANOIA) &&
			    ptpmap->pm_stab != (pt_entry_t *)trunc_page(ste))
				panic("remove: bogus ste");
#endif
			if (--(ptpmap->pm_sref) == 0) {
#ifdef DEBUG
				if (pmapdebug&(PDB_REMOVE|PDB_SEGTAB))
					printf("remove: free stab %lx\n",
					       ptpmap->pm_stab);
#endif
				kmem_free_wakeup(st_map,
						 (vm_offset_t)ptpmap->pm_stab,
						 ALPHA_STSIZE);
				ptpmap->pm_stab = Segtabzero;
				ptpmap->pm_stpte = Segtabzeropte;
				ptpmap->pm_stchanged = TRUE;
				/*
				 * XXX may have changed segment table
				 * pointer for current process so
				 * update now to reload hardware.
				 * (curproc may be NULL if exiting.)
				 */
				if (curproc != NULL &&
				    ptpmap == curproc->p_vmspace->vm_map.pmap)
					PMAP_ACTIVATE(ptpmap, 1);
			}
#ifdef DEBUG
			else if (ptpmap->pm_sref < 0)
				panic("remove: sref < 0");
#endif
		}
d3000 2
a3001 8
		/*
		 * XXX this should be unnecessary as we have been
		 * flushing individual mappings as we go.
		 */
		if (ptpmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
d3003 17
a3019 2
		pv->pv_flags &= ~PV_PTPAGE;
		ptpmap->pm_ptpages--;
d3021 6
a3026 1
	splx(s);
d3029 16
a3044 23
/* static */
void
pmap_changebit(pa, bit, setem)
	register vm_offset_t pa;
	u_long bit;
	boolean_t setem;
{
	register pv_entry_t pv;
	register pt_entry_t *pte, npte;
	vm_offset_t va;
	int s;
	boolean_t firstpage = TRUE;
#ifdef PMAPSTATS
	struct chgstats *chgp;
#endif

#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(%lx, %lx, %s)\n",
		       pa, bit, setem ? "set" : "clear");
#endif
	if (!PAGE_IS_MANAGED(pa))
		return;
d3046 2
a3047 22
#ifdef PMAPSTATS
	chgp = &changebit_stats[(bit>>2)-1];
	if (setem)
		chgp->setcalls++;
	else
		chgp->clrcalls++;
#endif
	pv = pa_to_pvh(pa);
	s = splimp();
	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */
	if (pv->pv_pmap != NULL) {
#ifdef DEBUG
		int toflush = 0;
#endif
		for (; pv; pv = pv->pv_next) {
#ifdef DEBUG
			toflush |= (pv->pv_pmap == pmap_kernel()) ? 2 : 1;
#endif
			va = pv->pv_va;
d3049 1
a3049 5
			/*
			 * XXX don't write protect pager mappings
			 */
/* XXX */		if (bit == (PG_UWE | PG_KWE)) {
				extern vm_offset_t pager_sva, pager_eva;
d3051 1
a3051 3
				if (va >= pager_sva && va < pager_eva)
					continue;
			}
d3053 13
a3065 22
			pte = pmap_pte(pv->pv_pmap, va);
			if (setem)
				npte = *pte | bit;
			else
				npte = *pte & ~bit;
			if (*pte != npte) {
				*pte = npte;
				if (active_pmap(pv->pv_pmap))
					TBIS((caddr_t)va);
#ifdef PMAPSTATS
				if (setem)
					chgp->sethits++;
				else
					chgp->clrhits++;
#endif
			}
#ifdef PMAPSTATS
			else {
				if (setem)
					chgp->setmiss++;
				else
					chgp->clrmiss++;
d3067 1
a3067 1
#endif
d3069 8
d3078 2
a3079 1
	splx(s);
d3082 13
a3094 42
/* static */
void
pmap_enter_ptpage(pmap, va)
	register pmap_t pmap;
	register vm_offset_t va;
{
	register vm_offset_t ptpa;
	register pv_entry_t pv;
	pt_entry_t *ste;
	int s;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE))
		printf("pmap_enter_ptpage: pmap %lx, va %lx\n", pmap, va);
#endif
#ifdef PMAPSTATS
	enter_stats.ptpneeded++;
#endif
	/*
	 * Allocate a segment table if necessary.  Note that it is allocated
	 * from a private map and not pt_map.  This keeps user page tables
	 * aligned on segment boundaries in the kernel address space.
	 * The segment table is wired down.  It will be freed whenever the
	 * reference count drops to zero.
	 */
	if (pmap->pm_stab == Segtabzero) {
		pmap->pm_stab = (pt_entry_t *)
			kmem_alloc(st_map, ALPHA_STSIZE);
		pmap->pm_stpte = *kvtopte(pmap->pm_stab);
		pmap->pm_stchanged = TRUE;
		/*
		 * XXX may have changed segment table pointer for current
		 * process so update now to reload hardware.
		 */
		if (pmap == curproc->p_vmspace->vm_map.pmap)
			PMAP_ACTIVATE(pmap, 1);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: pmap %lx stab %lx(%lx)\n",
			       pmap, pmap->pm_stab, pmap->pm_stpte);
#endif
	}
d3096 1
a3096 2
	ste = pmap_ste(pmap, va);
	va = trunc_page((vm_offset_t)pmap_pte(pmap, va));
d3099 1
a3099 3
	 * In the kernel we allocate a page from the kernel PT page
	 * free list and map it into the kernel page table map (via
	 * pmap_enter).
d3101 9
a3109 2
	if (pmap == pmap_kernel()) {
		register struct kpt_page *kpt;
d3111 4
a3114 27
		s = splimp();
		if ((kpt = kpt_free_list) == (struct kpt_page *)0) {
			/*
			 * No PT pages available.
			 * Try once to free up unused ones.
			 */
#ifdef DEBUG
			if (pmapdebug & PDB_COLLECT)
				printf("enter: no KPT pages, collecting...\n");
#endif
			pmap_collect(pmap_kernel());
			if ((kpt = kpt_free_list) == (struct kpt_page *)0)
				panic("pmap_enter_ptpage: can't get KPT page");
		}
#ifdef PMAPSTATS
		if (++kpt_stats.kptinuse > kpt_stats.kptmaxuse)
			kpt_stats.kptmaxuse = kpt_stats.kptinuse;
#endif
		kpt_free_list = kpt->kpt_next;
		kpt->kpt_next = kpt_used_list;
		kpt_used_list = kpt;
		ptpa = kpt->kpt_pa;
		bzero((caddr_t)kpt->kpt_va, ALPHA_PAGE_SIZE);
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE)) {
			int ix = pmap_ste(pmap, va) - pmap_ste(pmap, 0);
d3116 7
a3122 2
			printf("enter: add &Sysptmap[%d]: %lx (KPT page %lx)\n",
			       ix, Sysptmap[ix], kpt->kpt_va);
a3123 2
#endif
		splx(s);
d3125 2
a3126 53
	/*
	 * For user processes we just simulate a fault on that location
	 * letting the VM system allocate a zero-filled page.
	 */
	else {
		/*
		 * Count the segment table reference now so that we won't
		 * lose the segment table when low on memory.
		 */
		pmap->pm_sref++;
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
			printf("enter: about to fault UPT pg at %lx\n", va);
#endif
		s = vm_fault(pt_map, va, VM_PROT_READ|VM_PROT_WRITE, FALSE);
		if (s != KERN_SUCCESS) {
			printf("vm_fault(pt_map, %lx, RW, 0) -> %d\n", va, s);
			panic("pmap_enter: vm_fault failed");
		}
		ptpa = pmap_extract(pmap_kernel(), va);
		/*
		 * Mark the page clean now to avoid its pageout (and
		 * hence creation of a pager) between now and when it
		 * is wired; i.e. while it is on a paging queue.
		 */
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_CLEAN;
#ifdef DEBUG
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_PTPAGE;
#endif
	}
	/*
	 * Locate the PV entry in the kernel for this PT page and
	 * record the STE address.  This is so that we can invalidate
	 * the STE when we remove the mapping for the page.
	 */
	pv = pa_to_pvh(ptpa);
	s = splimp();
	if (pv) {
		pv->pv_flags |= PV_PTPAGE;
		do {
			if (pv->pv_pmap == pmap_kernel() && pv->pv_va == va)
				break;
		} while (pv = pv->pv_next);
	}
#ifdef DEBUG
	if (pv == NULL)
		panic("pmap_enter_ptpage: PT page not entered");
#endif
	pv->pv_ptpte = ste;
	pv->pv_ptpmap = pmap;
#ifdef DEBUG
	if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
		printf("enter: new PT page at PA %lx, ste at %lx\n", ptpa, ste);
d3129 68
a3196 28
	/*
	 * Map the new PT page into the segment table.
	 * Reference count on the user segment tables incremented above
	 * to prevent race conditions.  Note that we don't use vm_map_pageable
	 * to keep the count like we do for PT pages, this is mostly because
	 * it would be difficult to identify ST pages in pmap_pageable to
	 * release them.  We also avoid the overhead of vm_map_pageable.
	 */
	*ste = ((ptpa >> PGSHIFT) << PG_SHIFT) | PG_KRE | PG_KWE | PG_V |
	    (pmap == pmap_kernel() ? PG_ASM : 0);
	if (pmap != pmap_kernel()) {
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter: stab %lx refcnt %d\n",
			       pmap->pm_stab, pmap->pm_sref);
#endif
	}
#if 0
	/*
	 * Flush stale TLB info.
	 */
	if (pmap == pmap_kernel())
		TBIAS();
	else
		TBIAU();
#endif
	pmap->pm_ptpages++;
	splx(s);
d3204 4
a3207 4
	struct proc *p;
	vm_offset_t v;
	int user;
	int write;
d3209 22
a3230 3
	pt_entry_t faultoff, *pte;
	vm_offset_t pa;
	char attr;
d3232 6
a3237 5
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_emulate_reference: 0x%lx, 0x%lx, %d, %d\n",
		    p, v, user, write);
#endif
d3239 11
a3249 7
	/*
	 * Convert process and virtual address to physical address.
	 */
	if (v >= VM_MIN_KERNEL_ADDRESS) {
		if (user)
			panic("pmap_emulate_reference: user ref to kernel");
		pte = kvtopte(v);
d3251 6
d3258 2
a3259 69
		if (p == NULL)
			panic("pmap_emulate_reference: bad proc");
		if (p->p_vmspace == NULL)
			panic("pmap_emulate_reference: bad p_vmspace");
#endif
		pte = pmap_pte(&p->p_vmspace->vm_pmap, v);
	}
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("\tpte = 0x%lx, ", pte);
		printf("*pte = 0x%lx\n", *pte);
	}
#endif
#ifdef DEBUG				/* These checks are more expensive */
	if (!pmap_pte_v(pte))
		panic("pmap_emulate_reference: invalid pte");
#if 0
	/*
	 * Can't do these, because cpu_fork and cpu_swapin call
	 * pmap_emulate_reference(), and the bits aren't guaranteed,
	 * for them...
	 */
	if (write) {
		if (!(*pte & (user ? PG_UWE : PG_UWE | PG_KWE)))
			panic("pmap_emulate_reference: write but unwritable");
		if (!(*pte & PG_FOW))
			panic("pmap_emulate_reference: write but not FOW");
	} else {
		if (!(*pte & (user ? PG_URE : PG_URE | PG_KRE)))
			panic("pmap_emulate_reference: !write but unreadable");
		if (!(*pte & (PG_FOR | PG_FOE)))
			panic("pmap_emulate_reference: !write but not FOR|FOE");
	}
#endif
	/* Other diagnostics? */
#endif
	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("\tpa = 0x%lx\n", pa);
#endif
#ifdef DIAGNOSTIC
	if (!PAGE_IS_MANAGED(pa))
		printf("WARNING: pmap_emulate_reference(0x%lx, 0x%lx, %d, %d): pa 0x%lx not managed\n", p, v, user, write, pa);
#endif

	/*
	 * Twiddle the appropriate bits to reflect the reference
	 * and/or modification..
	 *
	 * The rules:
	 * 	(1) always mark page as used, and
	 *	(2) if it was a write fault, mark page as modified.
	 */
	attr = PMAP_ATTR_REF;
	faultoff = PG_FOR | PG_FOE;
	if (write) {
		attr |= PMAP_ATTR_MOD;
		faultoff |= PG_FOW;
	}
	pmap_attributes[pa_index(pa)] |= attr;
	pmap_changebit(pa, faultoff, FALSE);
	if ((*pte & faultoff) != 0) {
#if 0
		/*
		 * This is apparently normal.  Why? -- cgd
		 * XXX because was being called on unmanaged pages?
		 */
		printf("warning: pmap_changebit didn't.");
d3261 1
a3261 2
		*pte &= ~faultoff;
		TBIS((caddr_t)v);
d3263 1
a3265 2
#ifdef DEBUG
/* static */
d3267 2
a3268 2
pmap_pvdump(pa)
	vm_offset_t pa;
d3270 2
a3271 1
	register pv_entry_t pv;
d3273 15
a3287 6
	printf("pa %lx", pa);
	for (pv = pa_to_pvh(pa); pv; pv = pv->pv_next)
		printf(" -> pmap %lx, va %lx, stpte %lx, ptpmap %lx, flags %lx",
		       pv->pv_pmap, pv->pv_va, pv->pv_ptpte, pv->pv_ptpmap,
		       pv->pv_flags);
	printf("\n");
d3290 28
a3317 14
/* static */
void
pmap_check_wiring(str, va)
	char *str;
	vm_offset_t va;
{
	vm_map_entry_t entry;
	pt_entry_t *pte;
	register int count;

	va = trunc_page(va);
	if (!pmap_ste_v(pmap_kernel(), va) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;
a3318 11
	if (!vm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
		return;
	}
	count = 0;
	for (pte = (pt_entry_t *)va; pte < (pt_entry_t *)(va+PAGE_SIZE); pte++)
		if (*pte)
			count++;
	if (entry->wired_count != count)
		printf("*%s*: %lx: w%d/a%d\n",
		       str, va, entry->wired_count, count);
@


1.3
log
@sync to 0616, retaining local diffs
@
text
@d1 1
@


1.2
log
@update to netbsd
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.7 1995/11/23 02:34:26 cgd Exp $	*/
d270 2
d411 2
a412 1
	avail_end = alpha_ptob(1 + lastusablepage);
d414 10
d538 4
d789 1
a789 1
	if (pa < vm_first_phys || pa >= vm_last_phys)
d1035 1
a1035 1
	if (pa >= vm_first_phys && pa < vm_last_phys) {
d1112 6
a1117 4
	if ((pmap_attributes[pa_index(pa)] & PMAP_ATTR_REF) == 0)
		npte |= PG_FOR | PG_FOW | PG_FOE;
	else if ((pmap_attributes[pa_index(pa)] & PMAP_ATTR_MOD) == 0)
		npte |= PG_FOW;
d1493 1
a1493 1
		if (pa < vm_first_phys || pa >= vm_last_phys)
d1536 2
d1557 2
d1578 2
d1602 2
d1739 1
a1739 1
	if (pa < vm_first_phys || pa >= vm_last_phys)
d1878 1
a1878 1
	if (pa < vm_first_phys || pa >= vm_last_phys)
d2186 4
d2211 1
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.6 1995/08/03 00:52:24 cgd Exp $	*/
a198 2
#define pmap_pte_m(pte)		(*(pte) & PG_MOD)
#define pmap_pte_u(pte)		(*(pte) & PG_USED)
d256 1
a256 1
char		*pmap_attributes;	/* reference and modify bits */
d261 1
a262 1
boolean_t pmap_testbit	__P((vm_offset_t, u_long));
d307 1
a307 1
	Segtabzeropte |= PG_V|PG_KRE|PG_KWE|PG_WIRED;
a313 2
	 * Note that UPAGES is added, to allow for the double-mapping
	 * of user structs that occurs at the start of the mapped kvm area
d316 1
a316 1
		nbuf * MAXBSIZE + 16 * NCARGS) / NBPG + 512 + 256 + UPAGES;
d381 1
a381 1
		pte |= PG_V|PG_ASM|PG_KRE|PG_KWE|PG_WIRED;
d388 1
a388 1
	pte |= PG_V|PG_KRE|PG_KWE; /* NOTE NO ASM */
d398 1
a398 1
		pte |= PG_V|PG_ASM|PG_KRE|PG_KWE|PG_WIRED;
d484 1
a484 1
	blkclr((caddr_t)val, size);
a496 1
	int		prot, *kp, *up;
d505 2
a506 27
	kp = protection_codes[0];
	up = protection_codes[1];

	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE:
			*kp++ = PG_ASM;
			*up++ = 0;
			break;
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_EXECUTE:
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE:
			*kp++ = PG_ASM|PG_KRE;
			*up++ = PG_URE|PG_KRE;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
			*kp++ = PG_ASM|PG_KWE;
			*up++ = PG_UWE|PG_KWE;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			*kp++ = PG_ASM|PG_KWE|PG_KRE;
			*up++ = PG_UWE|PG_URE|PG_KWE|PG_KRE;
			break;
		}
	}
d782 1
a782 1
/* XXX */	pmap_changebit(pa, PG_KWE|PG_UWE, FALSE);
d1094 5
a1098 2
	npte = ((pa >> PGSHIFT) << PG_SHIFT) | pte_prot(pmap, prot) |
	    (*pte & (PG_MOD|PG_USED)) | PG_V;
d1489 1
a1489 1
		pmap_changebit(pa, PG_MOD, FALSE);
d1517 4
a1520 1
	pmap_changebit(pa, PG_MOD, FALSE);
d1536 4
a1539 1
	pmap_changebit(pa, PG_USED, FALSE);
d1553 3
a1557 1
		boolean_t rv = pmap_testbit(pa, PG_USED);
a1558 1
		return(rv);
d1561 1
a1561 1
	return(pmap_testbit(pa, PG_USED));
d1575 3
a1579 1
		boolean_t rv = pmap_testbit(pa, PG_MOD);
a1580 1
		return(rv);
d1583 1
a1583 1
	return(pmap_testbit(pa, PG_MOD));
d1598 38
a1653 1
	long bits;
a1690 1
	bits = *pte & (PG_USED|PG_MOD);
a1826 4
	/*
	 * Update saved attributes for managed page
	 */
	pmap_attributes[pa_index(pa)] |= bits;
a1830 39
boolean_t
pmap_testbit(pa, bit)
	register vm_offset_t pa;
	u_long bit;
{
	register pv_entry_t pv;
	register pt_entry_t *pte;
	int s;

	if (pa < vm_first_phys || pa >= vm_last_phys)
		return(FALSE);

	pv = pa_to_pvh(pa);
	s = splimp();
	/*
	 * Check saved info first
	 */
	if (pmap_attributes[pa_index(pa)] & bit) {
		splx(s);
		return(TRUE);
	}
	/*
	 * Not found, check current mappings returning
	 * immediately if found.
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
			if (*pte & bit) {
				splx(s);
				return(TRUE);
			}
		}
	}
	splx(s);
	return(FALSE);
}

/* static */
a1863 5
	 * Clear saved attributes (modify, reference)
	 */
	if (!setem)
		pmap_attributes[pa_index(pa)] &= ~bit;
	/*
d1880 1
a1880 1
/* XXX */		if (bit == (PG_UWE|PG_KWE)) {
d2009 5
d2061 2
a2062 2
	 * Also increment the reference count on the segment table if this
	 * was a user page table page.  Note that we don't use vm_map_pageable
a2069 1
		pmap->pm_sref++;
d2087 99
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
