head	1.70;
access;
symbols
	OPENBSD_5_5:1.69.0.8
	OPENBSD_5_5_BASE:1.69
	OPENBSD_5_4:1.69.0.4
	OPENBSD_5_4_BASE:1.69
	OPENBSD_5_3:1.69.0.2
	OPENBSD_5_3_BASE:1.69
	OPENBSD_5_2:1.67.0.2
	OPENBSD_5_2_BASE:1.67
	OPENBSD_5_1_BASE:1.66
	OPENBSD_5_1:1.66.0.2
	OPENBSD_5_0:1.62.0.2
	OPENBSD_5_0_BASE:1.62
	OPENBSD_4_9:1.59.0.2
	OPENBSD_4_9_BASE:1.59
	OPENBSD_4_8:1.58.0.2
	OPENBSD_4_8_BASE:1.58
	OPENBSD_4_7:1.55.0.2
	OPENBSD_4_7_BASE:1.55
	OPENBSD_4_6:1.55.0.4
	OPENBSD_4_6_BASE:1.55
	OPENBSD_4_5:1.54.0.4
	OPENBSD_4_5_BASE:1.54
	OPENBSD_4_4:1.54.0.2
	OPENBSD_4_4_BASE:1.54
	OPENBSD_4_3:1.52.0.4
	OPENBSD_4_3_BASE:1.52
	OPENBSD_4_2:1.52.0.2
	OPENBSD_4_2_BASE:1.52
	OPENBSD_4_1:1.50.0.2
	OPENBSD_4_1_BASE:1.50
	OPENBSD_4_0:1.47.0.2
	OPENBSD_4_0_BASE:1.47
	OPENBSD_3_9:1.41.0.2
	OPENBSD_3_9_BASE:1.41
	OPENBSD_3_8:1.40.0.2
	OPENBSD_3_8_BASE:1.40
	OPENBSD_3_7:1.36.0.2
	OPENBSD_3_7_BASE:1.36
	OPENBSD_3_6:1.34.0.2
	OPENBSD_3_6_BASE:1.34
	SMP_SYNC_A:1.34
	SMP_SYNC_B:1.34
	OPENBSD_3_5:1.33.0.2
	OPENBSD_3_5_BASE:1.33
	OPENBSD_3_4:1.27.0.2
	OPENBSD_3_4_BASE:1.27
	UBC_SYNC_A:1.25
	OPENBSD_3_3:1.25.0.2
	OPENBSD_3_3_BASE:1.25
	OPENBSD_3_2:1.20.0.2
	OPENBSD_3_2_BASE:1.20
	OPENBSD_3_1:1.17.0.2
	OPENBSD_3_1_BASE:1.17
	UBC_SYNC_B:1.21
	UBC:1.13.0.2
	UBC_BASE:1.13
	SMP:1.3.0.2;
locks; strict;
comment	@ * @;


1.70
date	2014.03.18.22.36.34;	author miod;	state dead;
branches;
next	1.69;

1.69
date	2013.01.01.01.01.08;	author miod;	state Exp;
branches;
next	1.68;

1.68
date	2012.11.21.21.28.26;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2012.04.10.15.50.52;	author guenther;	state Exp;
branches;
next	1.66;

1.66
date	2011.11.01.21.20.55;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2011.09.28.18.36.01;	author miod;	state Exp;
branches;
next	1.64;

1.64
date	2011.09.27.20.35.44;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2011.09.22.17.41.00;	author jasper;	state Exp;
branches;
next	1.62;

1.62
date	2011.05.27.20.10.18;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.60;

1.60
date	2011.04.28.20.53.32;	author ariane;	state Exp;
branches;
next	1.59;

1.59
date	2010.12.06.20.57.16;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2010.06.29.20.30.32;	author guenther;	state Exp;
branches;
next	1.57;

1.57
date	2010.06.26.23.24.43;	author guenther;	state Exp;
branches;
next	1.56;

1.56
date	2010.05.13.20.37.00;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2009.04.06.20.37.52;	author oga;	state Exp;
branches;
next	1.54;

1.54
date	2008.06.26.05.42.11;	author ray;	state Exp;
branches;
next	1.53;

1.53
date	2008.06.14.10.55.20;	author mk;	state Exp;
branches;
next	1.52;

1.52
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.51;

1.51
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.50;

1.50
date	2007.03.04.16.59.03;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2007.02.17.19.08.58;	author miod;	state Exp;
branches;
next	1.48;

1.48
date	2007.02.17.19.08.16;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2006.08.22.21.03.56;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2006.06.24.13.22.15;	author miod;	state Exp;
branches;
next	1.45;

1.45
date	2006.06.20.20.40.18;	author miod;	state Exp;
branches;
next	1.44;

1.44
date	2006.06.17.16.29.11;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2006.06.11.20.48.51;	author miod;	state Exp;
branches;
next	1.42;

1.42
date	2006.06.11.20.44.20;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2005.11.04.21.51.35;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2005.08.01.11.56.45;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2005.08.01.11.54.24;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2005.04.27.00.12.41;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2005.04.26.21.12.27;	author martin;	state Exp;
branches;
next	1.36;

1.36
date	2004.12.25.23.02.24;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2004.11.30.07.41.52;	author martin;	state Exp;
branches;
next	1.34;

1.34
date	2004.05.20.09.20.42;	author kettenis;	state Exp;
branches;
next	1.33;

1.33
date	2004.01.01.01.12.54;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2003.12.26.18.54.24;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2003.12.14.19.06.59;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2003.11.02.13.47.28;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2003.10.13.18.41.11;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2003.10.09.22.12.24;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2003.06.02.23.27.48;	author millert;	state Exp;
branches;
next	1.26;

1.26
date	2003.06.02.07.06.56;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	2003.03.03.23.23.41;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2003.03.01.00.28.48;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2003.02.25.16.57.24;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2003.01.27.19.37.30;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2002.10.12.01.09.43;	author krw;	state Exp;
branches;
next	1.20;

1.20
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2002.06.02.22.16.52;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2002.04.16.20.49.49;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2002.03.14.01.26.35;	author millert;	state Exp;
branches;
next	1.16;

1.16
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2002.01.10.21.08.43;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2001.12.20.19.02.28;	author miod;	state dead;
branches;
next	1.13;

1.13
date	2001.12.16.23.06.04;	author miod;	state Exp;
branches
	1.13.2.1;
next	1.12;

1.12
date	2001.12.16.21.48.48;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2001.12.15.11.21.08;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2001.12.14.21.44.04;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2001.12.12.18.36.21;	author millert;	state Exp;
branches;
next	1.8;

1.8
date	2001.12.12.17.30.08;	author millert;	state Exp;
branches;
next	1.7;

1.7
date	2001.12.11.08.11.33;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2001.12.08.18.55.58;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2001.12.08.02.24.06;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2001.12.06.01.03.58;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2001.12.05.00.11.51;	author millert;	state Exp;
branches
	1.3.2.1;
next	1.2;

1.2
date	2001.12.02.02.01.52;	author millert;	state Exp;
branches;
next	1.1;

1.1
date	2001.11.30.20.54.52;	author miod;	state Exp;
branches;
next	;

1.3.2.1
date	2001.12.05.00.39.11;	author niklas;	state Exp;
branches;
next	1.3.2.2;

1.3.2.2
date	2002.03.06.01.05.03;	author niklas;	state Exp;
branches;
next	1.3.2.3;

1.3.2.3
date	2002.03.28.10.34.04;	author niklas;	state Exp;
branches;
next	1.3.2.4;

1.3.2.4
date	2003.03.27.23.28.43;	author niklas;	state Exp;
branches;
next	1.3.2.5;

1.3.2.5
date	2003.06.07.11.11.39;	author ho;	state Exp;
branches;
next	1.3.2.6;

1.3.2.6
date	2004.02.19.10.49.02;	author niklas;	state Exp;
branches;
next	1.3.2.7;

1.3.2.7
date	2004.06.05.23.10.51;	author niklas;	state Exp;
branches;
next	;

1.13.2.1
date	2002.01.31.22.55.13;	author niklas;	state Exp;
branches;
next	1.13.2.2;

1.13.2.2
date	2002.06.11.03.36.06;	author art;	state Exp;
branches;
next	1.13.2.3;

1.13.2.3
date	2002.10.29.00.28.05;	author art;	state Exp;
branches;
next	1.13.2.4;

1.13.2.4
date	2003.05.19.21.49.42;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.70
log
@Retire hp300, mvme68k and mvme88k ports. These ports have no users, keeping
this hardware alive is becoming increasingly difficult, and I should heed the
message sent by the three disks which have died on me over the last few days.

Noone sane will mourn these ports anyway. So long, and thanks for the fish.
@
text
@/*	$OpenBSD: pmap_motorola.c,v 1.69 2013/01/01 01:01:08 miod Exp $ */

/*
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/* 
 * Copyright (c) 1995 Theo de Raadt
 * 
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

/*
 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	8.6 (Berkeley) 5/27/94
 */

/*
 * m68k series physical map management code.
 *
 * Supports:
 *	68020 with HP MMU
 *    	68020 with 68851 MMU
 *	68030 with on-chip MMU
 *	68040 with on-chip MMU
 *	68060 with on-chip MMU
 *
 * Notes:
 *	Don't even pay lip service to multiprocessor support.
 *
 *	We assume TLB entries don't have process tags (except for the
 *	supervisor/user distinction) so we only invalidate TLB entries
 *	when changing mappings for the current (or kernel) pmap.  This is
 *	technically not true for the 68851 but we flush the TLB on every
 *	context switch, so it effectively winds up that way.
 *
 *	Bitwise and/or operations are significantly faster than bitfield
 *	references so we use them when accessing STE/PTEs in the pmap_pte_*
 *	macros.  Note also that the two are not always equivalent; e.g.:
 *		(*pte & PG_PROT) [4] != pte->pg_prot [1]
 *	and a couple of routines that deal with protection and wiring take
 *	some shortcuts that assume the and/or definitions.
 *
 *	This implementation will only work for PAGE_SIZE == NBPG
 *	(i.e. 4096 bytes).
 */

/*
 *	Manages physical address maps.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>

#include <machine/pte.h>

#include <uvm/uvm.h>

#include <machine/cpu.h>

#ifdef PMAP_DEBUG
#define PDB_FOLLOW	0x0001
#define PDB_INIT	0x0002
#define PDB_ENTER	0x0004
#define PDB_REMOVE	0x0008
#define PDB_CREATE	0x0010
#define PDB_PTPAGE	0x0020
#define PDB_CACHE	0x0040
#define PDB_BITS	0x0080
#define PDB_COLLECT	0x0100
#define PDB_PROTECT	0x0200
#define PDB_SEGTAB	0x0400
#define PDB_MULTIMAP	0x0800
#define PDB_PARANOIA	0x2000
#define PDB_WIRING	0x4000
#define PDB_PVDUMP	0x8000
#define PDB_ALL		0xFFFF

int pmapdebug = PDB_PARANOIA;

#define	PMAP_DPRINTF(l, x)	if (pmapdebug & (l)) printf x

#if defined(M68040) || defined(M68060)
int dowriteback = 1;	/* 68040: enable writeback caching */
int dokwriteback = 1;	/* 68040: enable writeback caching of kernel AS */
#endif
#else
#define	PMAP_DPRINTF(l, x)	/* nothing */
#endif	/* PMAP_DEBUG */

/*
 * Get STEs and PTEs for user/kernel address space
 */
#if defined(M68040) || defined(M68060)
#define	pmap_ste1(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
/* XXX assumes physically contiguous ST pages (if more than one) */
#define pmap_ste2(m, v) \
	(&((m)->pm_stab[(st_entry_t *)(*(u_int *)pmap_ste1(m, v) & SG4_ADDR1) \
			- (m)->pm_stpa + (((v) & SG4_MASK2) >> SG4_SHIFT2)]))
#define	pmap_ste(m, v)	\
	(&((m)->pm_stab[(vaddr_t)(v) \
			>> (mmutype <= MMU_68040 ? SG4_SHIFT1 : SG_ISHIFT)]))
#define pmap_ste_v(m, v) \
	(mmutype <= MMU_68040 \
	 ? ((*pmap_ste1(m, v) & SG_V) && \
	    (*pmap_ste2(m, v) & SG_V)) \
	 : (*pmap_ste(m, v) & SG_V))
#else
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
#define pmap_ste_v(m, v) (*pmap_ste(m, v) & SG_V)
#endif

#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))
#define pmap_pte_pa(pte)	(*(pte) & PG_FRAME)
#define pmap_pte_w(pte)		(*(pte) & PG_W)
#define pmap_pte_ci(pte)	(*(pte) & PG_CI)
#define pmap_pte_m(pte)		(*(pte) & PG_M)
#define pmap_pte_u(pte)		(*(pte) & PG_U)
#define pmap_pte_prot(pte)	(*(pte) & PG_PROT)
#define pmap_pte_v(pte)		(*(pte) & PG_V)

#define pmap_pte_set_w(pte, v) \
	if (v) *(pte) |= PG_W; else *(pte) &= ~PG_W
#define pmap_pte_set_prot(pte, v) \
	if (v) *(pte) |= PG_PROT; else *(pte) &= ~PG_PROT
#define pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

/*
 * Given a map and a machine independent protection code,
 * convert to an m68k protection code.
 */
#define pte_prot(p)	((p) & VM_PROT_WRITE ? PG_RW : PG_RO)

/*
 * Kernel page table page management.
 */
struct kpt_page {
	struct kpt_page *kpt_next;	/* link on either used or free list */
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
};
struct kpt_page *kpt_free_list, *kpt_used_list;
struct kpt_page *kpt_pages;

/*
 * Kernel segment/page table and page table map.
 * The page table map gives us a level of indirection we need to dynamically
 * expand the page table.  It is essentially a copy of the segment table
 * with PTEs instead of STEs.  All are initialized in locore at boot time.
 * Sysmap will initially contain VM_KERNEL_PT_PAGES pages of PTEs.
 * Segtabzero is an empty segment table which all processes share til they
 * reference something.
 */
st_entry_t	*Sysseg;
pt_entry_t	*Sysmap, *Sysptmap;
st_entry_t	*Segtabzero, *Segtabzeropa;
vsize_t		Sysptsize = VM_KERNEL_PT_PAGES;

#ifndef __HAVE_PMAP_DIRECT
extern caddr_t	CADDR1, CADDR2;
pt_entry_t	*caddr1_pte;	/* PTE for CADDR1 */
pt_entry_t	*caddr2_pte;	/* PTE for CADDR2 */
#endif

struct pmap	kernel_pmap_store;
struct vm_map	*st_map, *pt_map;
struct vm_map	st_map_store, pt_map_store;

paddr_t    	avail_start;	/* PA of first available physical page */
paddr_t		avail_end;	/* PA of last available physical page */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */

#if defined(M68040) || defined(M68060)
int		protostfree;	/* prototype (default) free ST map */
#endif

struct pool	pmap_pmap_pool;	/* memory pool for pmap structures */
struct pool	pmap_pv_pool;	/* memory pool for pv pages */

/*
 * Internal routines
 */
struct pv_entry	*pmap_alloc_pv(void);
void		 pmap_free_pv(struct pv_entry *);
void		 pmap_remove_flags(pmap_t, vaddr_t, vaddr_t, int);
void		 pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *, int);
boolean_t	 pmap_testbit(struct vm_page *, int);
void		 pmap_changebit(struct vm_page *, int, int);
int		 pmap_enter_ptpage(pmap_t, vaddr_t);
void		 pmap_ptpage_addref(vaddr_t);
int		 pmap_ptpage_delref(vaddr_t);
void		 pmap_collect1(paddr_t, paddr_t);


#ifdef PMAP_DEBUG
void pmap_pvdump(paddr_t);
void pmap_check_wiring(char *, vaddr_t);
#endif

/* pmap_remove_mapping flags */
#define	PRM_TFLUSH	0x01
#define	PRM_CFLUSH	0x02
#define	PRM_KEEPPTPAGE	0x04
#define	PRM_SKIPWIRED	0x08

static struct pv_entry *pa_to_pvh(paddr_t);
static struct pv_entry *pg_to_pvh(struct vm_page *);
int pmap_largekva = 0;

/*
 * Allow the kernel to grow up to Sysmap, until pmap_init has initialized.
 */
vaddr_t
pmap_growkernel(vaddr_t addr)
{
	return pmap_largekva ? VM_MAX_KERNEL_ADDRESS : (vaddr_t)Sysmap;
}

static __inline struct pv_entry *
pa_to_pvh(paddr_t pa)
{
	struct vm_page *pg;

	pg = PHYS_TO_VM_PAGE(pa);
	return &pg->mdpage.pvent;
}

static __inline struct pv_entry *
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}

#ifdef PMAP_STEAL_MEMORY
vaddr_t
pmap_steal_memory(size, vstartp, vendp)
	vsize_t size;
	vaddr_t *vstartp, *vendp;
{
	vaddr_t va;
	u_int npg;

	size = round_page(size);
	npg = atop(size);

	/* m68k systems which define PMAP_STEAL_MEMORY only have one segment. */
#ifdef DIAGNOSTIC
	if (vm_physmem[0].avail_end - vm_physmem[0].avail_start < npg)
		panic("pmap_steal_memory(%x): out of memory", size);
#endif

	va = ptoa(vm_physmem[0].avail_start);
	vm_physmem[0].avail_start += npg;
	vm_physmem[0].start += npg;

	if (vstartp != NULL)
		*vstartp = virtual_avail;
	if (vendp != NULL)
		*vendp = virtual_end;
	
	bzero((void *)va, size);
	return (va);
}
#else
/*
 * pmap_virtual_space:		[ INTERFACE ]
 *
 *	Report the range of available kernel virtual address
 *	space to the VM system during bootstrap.
 *
 *	This is only an interface function if we do not use
 *	pmap_steal_memory()!
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_virtual_space(vstartp, vendp)
	vaddr_t	*vstartp, *vendp;
{

	*vstartp = virtual_avail;
	*vendp = virtual_end;
}
#endif

/*
 * pmap_init:			[ INTERFACE ]
 *
 *	Initialize the pmap module.  Called by uvm_init(), to initialize any
 *	structures that the pmap system needs to map virtual memory.
 *
 *	Note: no locking is necessary in this function.
 */
void
pmap_init()
{
	vaddr_t		addr, addr2;
	vsize_t		s;
	int		rv;
	int		npages;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));

#ifndef __HAVE_PMAP_DIRECT
	/*
	 * Before we do anything else, initialize the PTE pointers
	 * used by pmap_zero_page() and pmap_copy_page().
	 */
	caddr1_pte = pmap_pte(pmap_kernel(), CADDR1);
	caddr2_pte = pmap_pte(pmap_kernel(), CADDR2);
#endif

	/*
	 * Now that kernel map has been allocated, we can mark as
	 * unavailable regions which we have mapped in pmap_bootstrap().
	 */
	PMAP_INIT_MD();

	PMAP_DPRINTF(PDB_INIT,
	    ("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
	    avail_start, avail_end, virtual_avail, virtual_end));

	/*
	 * Allocate memory the initial segment table.
	 */
	addr = uvm_km_zalloc(kernel_map, round_page(MACHINE_STSIZE));
	if (addr == 0)
		panic("pmap_init: can't allocate data structures");

	Segtabzero = (st_entry_t *) addr;
	pmap_extract(pmap_kernel(), addr, (paddr_t *)&Segtabzeropa);
#ifdef M68060
	if (mmutype == MMU_68060) {
		for (addr2 = addr; addr2 < addr + MACHINE_STSIZE;
		    addr2 += PAGE_SIZE) {
			pt_entry_t *pte;

			pte = pmap_pte(pmap_kernel(), addr2);
			*pte = (*pte | PG_CI) & ~PG_CCB;
			TBIS(addr2);
		}
		DCIS();
	}
#endif
	addr += MACHINE_STSIZE;

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: s0 %p(%p)\n",
	    Segtabzero, Segtabzeropa));

	/*
	 * Allocate physical memory for kernel PT pages and their management.
	 * We need 1 PT page per possible task plus some slop.
	 */
	npages = min(atop(MACHINE_MAX_KPTSIZE), maxprocess+16);
	s = ptoa(npages) + round_page(npages * sizeof(struct kpt_page));

	/*
	 * Now allocate the space and link the pages together to
	 * form the KPT free list.
	 */
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
	s = ptoa(npages);
	addr2 = addr + s;
	kpt_pages = &((struct kpt_page *)addr2)[npages];
	kpt_free_list = NULL;
	do {
		addr2 -= PAGE_SIZE;
		(--kpt_pages)->kpt_next = kpt_free_list;
		kpt_free_list = kpt_pages;
		kpt_pages->kpt_va = addr2;
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
#ifdef M68060
		if (mmutype == MMU_68060) {
			pt_entry_t *pte;

			pte = pmap_pte(pmap_kernel(), addr2);
			*pte = (*pte | PG_CI) & ~PG_CCB;
			TBIS(addr2);
		}
#endif
	} while (addr != addr2);
#ifdef M68060
	if (mmutype == MMU_68060)
		DCIS();
#endif

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: KPT: %ld pages from %lx to %lx\n",
	    atop(s), addr, addr + s));

	/*
	 * Allocate the segment table map and the page table map.
	 */
	s = maxprocess * MACHINE_STSIZE;
	st_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, 0, FALSE,
	    &st_map_store);

	pmap_largekva = 1;

	addr = (vaddr_t) Sysmap;
	if (uvm_map(kernel_map, &addr, MACHINE_MAX_PTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED))) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
		panic("pmap_init: bogons in the VM system!");
	}
	PMAP_DPRINTF(PDB_INIT,
	    ("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
	    Sysseg, Sysmap, Sysptmap));

	addr = MACHINE_PTBASE;
	if ((MACHINE_PTMAXSIZE / MACHINE_MAX_PTSIZE) < maxprocess) {
		s = MACHINE_PTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxprocess so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(3).
		 */
		maxprocess = (MACHINE_PTMAXSIZE / MACHINE_MAX_PTSIZE);
	} else
		s = (maxprocess * MACHINE_MAX_PTSIZE);
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, 0,
	    TRUE, &pt_map_store);

#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040) {
		protostfree = ~l2tobm(0);
		for (rv = MAXUL2SIZE; rv < sizeof(protostfree)*NBBY; rv++)
			protostfree &= ~l2tobm(rv);
	}
#endif

	/*
	 * Initialize the pmap pools.
	 */
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
	    NULL);
}

/*
 * pmap_alloc_pv:
 *
 *	Allocate a pv_entry.
 */
struct pv_entry *
pmap_alloc_pv()
{
	struct pv_entry *pv;

	pv = (struct pv_entry *)pool_get(&pmap_pv_pool, PR_NOWAIT);
	return pv;
}

/*
 * pmap_free_pv:
 *
 *	Free a pv_entry.
 */
void
pmap_free_pv(pv)
	struct pv_entry *pv;
{
	pool_put(&pmap_pv_pool, pv);
}

/*
 * pmap_create:			[ INTERFACE ]
 *
 *	Create and return a physical map.
 *
 *	Note: no locking is necessary in this function.
 */
pmap_t
pmap_create()
{
	pmap_t pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_create\n"));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK | PR_ZERO);

	/*
	 * No need to allocate page table space yet but we do need a
	 * valid segment table.  Initially, we point everyone at the
	 * "null" segment table.  On the first pmap_enter, a real
	 * segment table will be allocated.
	 */
	pmap->pm_stab = Segtabzero;
	pmap->pm_stpa = Segtabzeropa;
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040)
		pmap->pm_stfree = protostfree;
#endif
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);

	return pmap;
}

/*
 * pmap_destroy:		[ INTERFACE ]
 *
 *	Drop the reference count on the specified pmap, releasing
 *	all resources if the reference count drops to zero.
 */
void
pmap_destroy(pmap)
	pmap_t pmap;
{
	int count;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));
	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		if (pmap->pm_ptab) {
			pmap_remove(pmap_kernel(), (vaddr_t)pmap->pm_ptab,
			    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
			pmap_update(pmap_kernel());
			uvm_km_pgremove(uvm.kernel_object,
			    (vaddr_t)pmap->pm_ptab,
			    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
			uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
					   MACHINE_MAX_PTSIZE);
		}
		KASSERT(pmap->pm_stab == Segtabzero);
		pool_put(&pmap_pmap_pool, pmap);
	}
}

/*
 * pmap_reference:		[ INTERFACE ]
 *
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap)
	pmap_t	pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_reference(%p)\n", pmap));
	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}

/*
 * pmap_activate:		[ INTERFACE ]
 *
 *	Activate the pmap used by the specified process.  This includes
 *	reloading the MMU context of the current process, and marking
 *	the pmap in use by the processor.
 *
 *	Note: we may only use spin locks here, since we are called
 *	by a critical section in cpu_switch()!
 */
void
pmap_activate(p)
	struct proc *p;
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_SEGTAB,
	    ("pmap_activate(%p)\n", p));

	PMAP_ACTIVATE(pmap, p == curproc);
}

/*
 * pmap_deactivate:		[ INTERFACE ]
 *
 *	Mark that the pmap used by the specified process is no longer
 *	in use by the processor.
 *
 *	The comment above pmap_activate() wrt. locking applies here,
 *	as well.
 */
void
pmap_deactivate(p)
	struct proc *p;
{

	/* No action necessary in this pmap implementation. */
}

/*
 * pmap_remove:			[ INTERFACE ]
 *
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_remove(pmap, sva, eva)
	pmap_t pmap;
	vaddr_t sva, eva;
{
	int flags;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva));

	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
	pmap_remove_flags(pmap, sva, eva, flags);
}

void
pmap_remove_flags(pmap, sva, eva, flags)
	pmap_t pmap;
	vaddr_t sva, eva;
	int flags;
{
	vaddr_t nssva;
	pt_entry_t *pte;

	while (sva < eva) {
		nssva = m68k_trunc_seg(sva) + NBSEG;
		if (nssva == 0 || nssva > eva)
			nssva = eva;

		/*
		 * Invalidate every valid mapping within this segment.
		 */

		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {

			/*
			 * If this segment is unallocated,
			 * skip to the next segment boundary.
			 */

			if (!pmap_ste_v(pmap, sva)) {
				sva = nssva;
				break;
			}
			if (pmap_pte_v(pte)) {
				if ((flags & PRM_SKIPWIRED) == 0 ||
				    !pmap_pte_w(pte))
					pmap_remove_mapping(pmap, sva, pte,
					    flags);
			}
			pte++;
			sva += PAGE_SIZE;
		}
	}
}

/*
 * pmap_page_protect:		[ INTERFACE ]
 *
 *	Lower the permission for all mappings to a given page to
 *	the permissions specified.
 */
void
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t	prot;
{
	struct pv_entry *pv;
	int s;

#ifdef PMAP_DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    (prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE)))
		printf("pmap_page_protect(%lx, %x)\n", pg, prot);
#endif

	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pv = pg_to_pvh(pg);
		s = splvm();
		while (pv->pv_pmap != NULL) {
			pt_entry_t *pte;

			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
#ifdef PMAP_DEBUG
			if (!pmap_ste_v(pv->pv_pmap, pv->pv_va) ||
			    pmap_pte_pa(pte) != VM_PAGE_TO_PHYS(pg))
				panic("pmap_page_protect: bad mapping");
#endif
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
			    pte, PRM_TFLUSH|PRM_CFLUSH);
		}
		splx(s);
	} else if ((prot & VM_PROT_WRITE) == VM_PROT_NONE)
		pmap_changebit(pg, PG_RO, ~0);
}

/*
 * pmap_protect:		[ INTERFACE ]
 *
 *	Set the physical protection on the specified range of this map
 *	as requested.
 */
void
pmap_protect(pmap, sva, eva, prot)
	pmap_t		pmap;
	vaddr_t		sva, eva;
	vm_prot_t	prot;
{
	vaddr_t nssva;
	pt_entry_t *pte;
	boolean_t needtflush;
	int isro;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_protect(%p, %lx, %lx, %x)\n",
	    pmap, sva, eva, prot));

	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	isro = pte_prot(prot);
	needtflush = active_pmap(pmap);
	while (sva < eva) {
		nssva = m68k_trunc_seg(sva) + NBSEG;
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}
		/*
		 * Change protection on mapping if it is valid and doesn't
		 * already have the correct protection.
		 */
		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte) && pmap_pte_prot_chg(pte, isro)) {
#if defined(M68040) || defined(M68060)
				/*
				 * Clear caches if making RO (see section
				 * "7.3 Cache Coherency" in the manual).
				 */
				if (isro && mmutype <= MMU_68040) {
					paddr_t pa = pmap_pte_pa(pte);

					DCFP(pa);
					ICPP(pa);
				}
#endif
				pmap_pte_set_prot(pte, isro);
				if (needtflush)
					TBIS(sva);
			}
			pte++;
			sva += PAGE_SIZE;
		}
	}
}

/*
 * pmap_enter:			[ INTERFACE ]
 *
 *	Insert the given physical page (pa) at
 *	the specified virtual address (va) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte cannot be reclaimed.
 *
 *	Note: This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
int
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	pt_entry_t pte;

	pte = 0;
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040 && (pte_prot(prot) & PG_PROT) == PG_RW)
#ifdef PMAP_DEBUG
		if (dowriteback && (dokwriteback || pmap != pmap_kernel()))
#endif
		pte |= PG_CCB;
#endif
	return (pmap_enter_cache(pmap, va, pa, prot, flags, pte));
}

/*
 * Similar to pmap_enter(), but allows the caller to control the
 * cacheability of the mapping. However if it is found that this mapping
 * needs to be cache inhibited, the cache bits from the caller are ignored.
 */
int
pmap_enter_cache(pmap, va, pa, prot, flags, template)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
	pt_entry_t template;
{
	struct vm_page *pg;
	pt_entry_t *pte;
	int npte, error;
	paddr_t opa;
	boolean_t cacheable = TRUE;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter_cache(%p, %lx, %lx, %x, %x, %x)\n",
	    pmap, va, pa, prot, wired, template));

#ifdef DEBUG
#ifndef __HAVE_PMAP_DIRECT
	/*
	 * pmap_enter() should never be used for CADDR1 and CADDR2.
	 */
	if (pmap == pmap_kernel() &&
	    (va == (vaddr_t)CADDR1 || va == (vaddr_t)CADDR2))
		panic("pmap_enter: used for CADDR1 or CADDR2");
#endif
#endif

	/*
	 * For user mapping, allocate kernel VM resources if necessary.
	 */
	if (pmap->pm_ptab == NULL)
		pmap->pm_ptab = (pt_entry_t *)
			uvm_km_valloc_wait(pt_map, MACHINE_MAX_PTSIZE);

	/*
	 * Segment table entry not valid, we need a new PT page
	 */
	if (!pmap_ste_v(pmap, va)) {
		error = pmap_enter_ptpage(pmap, va);
		if (error != 0) {
			if  (flags & PMAP_CANFAIL)
				return (error);
			else
				panic("pmap_enter: out of address space");
		}
	}

	pa = trunc_page(pa);
	pte = pmap_pte(pmap, va);
	opa = pmap_pte_pa(pte);

	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));

	/*
	 * Mapping has not changed, must be protection or wiring change.
	 */
	if (opa == pa) {
		/*
		 * Wiring change, just update stats.
		 * We don't worry about wiring PT pages as they remain
		 * resident as long as there are valid mappings in them.
		 * Hence, if a user page is wired, the PT page will be also.
		 */
		if (pmap_pte_w_chg(pte, wired ? PG_W : 0)) {
			PMAP_DPRINTF(PDB_ENTER,
			    ("enter: wiring change -> %x\n", wired));
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
		}
		/*
		 * Retain cache inhibition status
		 */
		if (pmap_pte_ci(pte))
			cacheable = FALSE;
		goto validate;
	}

	/*
	 * Mapping has changed, invalidate old range and fall through to
	 * handle validating new mapping.
	 */
	if (opa) {
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: removing old mapping %lx\n", va));
		pmap_remove_mapping(pmap, va, pte,
		    PRM_TFLUSH|PRM_CFLUSH|PRM_KEEPPTPAGE);
	}

	/*
	 * If this is a new user mapping, increment the wiring count
	 * on this PT page.  PT pages are wired down as long as there
	 * is a valid mapping in the page.
	 */
	if (pmap != pmap_kernel()) {
		pmap_ptpage_addref(trunc_page((vaddr_t)pte));
	}

	/*
	 * Enter on the PV list if part of our managed memory
	 * Note that we raise IPL while manipulating the PV list
	 * since pmap_enter can be called at interrupt time.
	 */
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg != NULL) {
		struct pv_entry *pv, *npv;
		int s;

		pv = pg_to_pvh(pg);
		s = splvm();
		PMAP_DPRINTF(PDB_ENTER,
		    ("enter: pv at %p: %lx/%p/%p\n",
		    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
		/*
		 * No entries yet, use header as the first entry
		 */
		if (pv->pv_pmap == NULL) {
			pv->pv_va = va;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
			pv->pv_ptste = NULL;
			pv->pv_ptpmap = NULL;
			pv->pv_flags = 0;
		}
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
		else {
#ifdef PMAP_DEBUG
			for (npv = pv; npv; npv = npv->pv_next)
				if (pmap == npv->pv_pmap && va == npv->pv_va)
					panic("pmap_enter: already in pv_tab");
#endif
			npv = pmap_alloc_pv();
			if (npv == NULL) {
				if (flags & PMAP_CANFAIL) {
					splx(s);
					return (ENOMEM);
				} else
					panic("pmap_enter: pmap_alloc_pv() failed");
			}
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_ptste = NULL;
			npv->pv_ptpmap = NULL;
			npv->pv_flags = 0;
			pv->pv_next = npv;
		}

		/*
		 * Speed pmap_is_referenced() or pmap_is_modified() based
		 * on the hint provided in access_type.
		 */
#ifdef DIAGNOSTIC
		if ((flags & VM_PROT_ALL) & ~prot)
			panic("pmap_enter: access type exceeds prot");
#endif
		if (flags & VM_PROT_WRITE)
			pv->pv_flags |= (PG_U|PG_M);
		else if (flags & VM_PROT_ALL)
			pv->pv_flags |= PG_U;

		splx(s);
	}
	/*
	 * Assumption: if it is not part of our managed memory
	 * then it must be device memory which may be volatile.
	 */
	else
		cacheable = FALSE;

	/*
	 * Increment counters
	 */
	pmap->pm_stats.resident_count++;
	if (wired)
		pmap->pm_stats.wired_count++;

validate:
	/*
	 * Build the new PTE.
	 */
	npte = pa | pte_prot(prot) | (*pte & (PG_M|PG_U)) | PG_V;
	if (wired)
		npte |= PG_W;

#if defined(M68040) || defined(M68060)
	/* Don't cache if process can't take it, like SunOS ones.  */
	if (mmutype <= MMU_68040 && pmap != pmap_kernel() &&
	    (curproc->p_md.md_flags & MDP_UNCACHE_WX) &&
	    (prot & VM_PROT_EXECUTE) && (prot & VM_PROT_WRITE))
		cacheable = FALSE;
#endif

	if (!cacheable)
		npte |= PG_CI;
	else
		npte |= template;

	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));

	/*
	 * Remember if this was a wiring-only change.
	 * If so, we need not flush the TLB and caches.
	 */
	wired = ((*pte ^ npte) == PG_W);
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040 && !wired) {
		DCFP(pa);
		ICPP(pa);
	}
#endif
	*pte = npte;
	if (!wired && active_pmap(pmap))
		TBIS(va);
#ifdef PMAP_DEBUG
	if ((pmapdebug & PDB_WIRING) && pmap != pmap_kernel())
		pmap_check_wiring("enter", trunc_page((vaddr_t)pte));
#endif

	return (0);
}

void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	pt_entry_t pte;

	pte = pte_prot(prot);
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040 && (pte & (PG_PROT)) == PG_RW)
		pte |= PG_CCB;
#endif
	pmap_kenter_cache(va, pa, pte);
}

/*
 * Similar to pmap_kenter_pa(), but allows the caller to control the
 * cacheability of the mapping.
 */
void
pmap_kenter_cache(va, pa, template)
	vaddr_t va;
	paddr_t pa;
	pt_entry_t template;
{
	struct pmap *pmap = pmap_kernel();
	pt_entry_t *pte;
	int s, npte, error;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_cache(%lx, %lx, %x)\n", va, pa, prot));

	/*
	 * Segment table entry not valid, we need a new PT page
	 */

	if (!pmap_ste_v(pmap, va)) { 
		s = splvm();
		error = pmap_enter_ptpage(pmap, va);
		if (error != 0)
			panic("pmap_kenter_cache: out of address space");
		splx(s);
	}

	pa = trunc_page(pa);
	pte = pmap_pte(pmap, va);

	PMAP_DPRINTF(PDB_ENTER, ("kenter: pte %p, *pte %x\n", pte, *pte));
	KASSERT(!pmap_pte_v(pte));

	/*
	 * Increment counters
	 */

	pmap->pm_stats.resident_count++;
	pmap->pm_stats.wired_count++;

	/*
	 * Build the new PTE.
	 */

	npte = pa | template | PG_V | PG_W;

	PMAP_DPRINTF(PDB_ENTER, ("kenter: new pte value %x\n", npte));
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040) {
		DCFP(pa);
		ICPP(pa);
	}
#endif
	*pte = npte;
}

void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	struct pmap *pmap = pmap_kernel();
	vaddr_t sva, eva, nssva;
	pt_entry_t *pte;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_kremove(%lx, %lx)\n", va, len));

	sva = va;
	eva = va + len;
	while (sva < eva) {
		nssva = m68k_trunc_seg(sva) + NBSEG;
		if (nssva == 0 || nssva > eva)
			nssva = eva;

		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */

		if (!pmap_ste_v(pmap, sva)) {
			sva = nssva;
			continue;
		}

		/*
		 * Invalidate every valid mapping within this segment.
		 */

		pte = pmap_pte(pmap, sva);
		while (sva < nssva) {
			if (pmap_pte_v(pte)) {
#ifdef PMAP_DEBUG
				struct pv_entry *pv;
				int s;

				pv = pa_to_pvh(pmap_pte_pa(pte));
				s = splvm();
				while (pv->pv_pmap != NULL) {
					KASSERT(pv->pv_pmap != pmap_kernel() ||
					    pv->pv_va != sva);
					pv = pv->pv_next;
					if (pv == NULL) {
						break;
					}
				}
				splx(s);
#endif
				/*
				 * Update statistics
				 */

				pmap->pm_stats.wired_count--;
				pmap->pm_stats.resident_count--;

				/*
				 * Invalidate the PTE.
				 */

				*pte = PG_NV;
				TBIS(sva);
			}
			pte++;
			sva += PAGE_SIZE;
		}
	}
}

/*
 * pmap_unwire:			[ INTERFACE]
 *
 *	Clear the wired attribute for a map/virtual-address pair.
 *
 *	The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap, va)
	pmap_t		pmap;
	vaddr_t		va;
{
	pt_entry_t *pte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %lx)\n", pmap, va));

	pte = pmap_pte(pmap, va);
#ifdef PMAP_DEBUG
	/*
	 * Page table page is not allocated.
	 * Should this ever happen?  Ignore it for now,
	 * we don't want to force allocation of unnecessary PTE pages.
	 */
	if (!pmap_ste_v(pmap, va)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid STE for %lx\n", va);
		return;
	}
	/*
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
	 */
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid PTE for %lx\n", va);
	}
#endif
	/*
	 * If wiring actually changed (always?) set the wire bit and
	 * update the wire count.  Note that wiring is not a hardware
	 * characteristic so there is no need to invalidate the TLB.
	 */
	if (pmap_pte_w_chg(pte, 0)) {
		pmap_pte_set_w(pte, 0);
		pmap->pm_stats.wired_count--;
	}
}

/*
 * pmap_extract:		[ INTERFACE ]
 *
 *	Extract the physical address associated with the given
 *	pmap/virtual address pair.
 */
boolean_t
pmap_extract(pmap, va, pap)
	pmap_t	pmap;
	vaddr_t va;
	paddr_t *pap;
{
	boolean_t rv = FALSE;
	paddr_t pa;
	pt_entry_t *pte;

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_extract(%p, %lx) -> ", pmap, va));

#ifdef __HAVE_PMAP_DIRECT
	if (pmap == pmap_kernel() && trunc_page(va) > VM_MAX_KERNEL_ADDRESS) {
		if (pap != NULL)
			*pap = va;
		return (TRUE);
	}
#endif

	if (pmap_ste_v(pmap, va)) {
		pte = pmap_pte(pmap, va);
		if (pmap_pte_v(pte)) {
			pa = pmap_pte_pa(pte) | (va & ~PG_FRAME);
			if (pap != NULL)
				*pap = pa;
			rv = TRUE;
		}
	}
#ifdef PMAP_DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		if (rv)
			printf("%lx\n", pa);
		else
			printf("failed\n");
	}
#endif
	return (rv);
}

/*
 * pmap_collect:		[ INTERFACE ]
 *
 *	Garbage collects the physical map system for pages which are no
 *	longer used.  Success need not be guaranteed -- that is, there
 *	may well be pages which are not referenced, but others may be
 *	collected.
 *
 *	Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap)
	pmap_t		pmap;
{
	int flags;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_collect(%p)\n", pmap));

	if (pmap == pmap_kernel()) {
		int bank, s;

		/*
		 * XXX This is very bogus.  We should handle kernel PT
		 * XXX pages much differently.
		 */

		s = splvm();
		for (bank = 0; bank < vm_nphysseg; bank++)
			pmap_collect1(ptoa(vm_physmem[bank].start),
			    ptoa(vm_physmem[bank].end));
		splx(s);
	} else {
		/*
		 * This process is about to be swapped out; free all of
		 * the PT pages by removing the physical mappings for its
		 * entire address space.  Note: pmap_remove() performs
		 * all necessary locking.
		 */
		flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
		pmap_remove_flags(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS,
		    flags | PRM_SKIPWIRED);
		pmap_update(pmap);
	}
}

/*
 * pmap_collect1:
 *
 *	Garbage-collect KPT pages.  Helper for the above (bogus)
 *	pmap_collect().
 *
 *	Note: THIS SHOULD GO AWAY, AND BE REPLACED WITH A BETTER
 *	WAY OF HANDLING PT PAGES!
 */
void
pmap_collect1(startpa, endpa)
	paddr_t		startpa, endpa;
{
	paddr_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	paddr_t kpa;
#ifdef PMAP_DEBUG
	st_entry_t *ste;
	int opmapdebug = 0 /* XXX initialize to quiet gcc -Wall */;
#endif

	for (pa = startpa; pa < endpa; pa += PAGE_SIZE) {
		struct kpt_page *kpt, **pkpt;

		/*
		 * Locate physical pages which are being used as kernel
		 * page table pages.
		 */
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptste && pv->pv_ptpmap == pmap_kernel())
				break;
		} while ((pv = pv->pv_next));
		if (pv == NULL)
			continue;
#ifdef PMAP_DEBUG
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + MACHINE_MAX_PTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (pt_entry_t *)(pv->pv_va + PAGE_SIZE);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef PMAP_DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf("collect: freeing KPT page at %lx (ste %x@@%p)\n",
			       pv->pv_va, *pv->pv_ptste, pv->pv_ptste);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
		}

		ste = pv->pv_ptste;
#endif
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove_entry to take care of invalidating
		 * ST and Sysptmap entries.
		 */
		pmap_extract(pmap_kernel(), pv->pv_va, &kpa);
		pmap_remove_mapping(pmap_kernel(), pv->pv_va, PT_ENTRY_NULL,
				    PRM_TFLUSH|PRM_CFLUSH);
		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != NULL;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef PMAP_DEBUG
		if (kpt == NULL)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			       kpt->kpt_va, kpa);
#endif
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef PMAP_DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste & SG_V)
			printf("collect: kernel STE at %p still valid (%x)\n",
			       ste, *ste);
		ste = &Sysptmap[ste - pmap_ste(pmap_kernel(), 0)];
		if (*ste & SG_V)
			printf("collect: kernel PTmap at %p still valid (%x)\n",
			       ste, *ste);
#endif
	}
}

/*
 * pmap_zero_page:		[ INTERFACE ]
 *
 *	Zero the specified (machine independent) page by mapping the page
 *	into virtual memory and using bzero to clear its contents, one
 *	machine dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 */
void
pmap_zero_page(struct vm_page *pg)
{
#ifdef __HAVE_PMAP_DIRECT
	vaddr_t va = pmap_map_direct(pg);
	zeropage((void *)va);
#else
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
	int npte;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%lx)\n", phys));

	npte = phys | PG_V;

#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040) {
		/*
		 * Set copyback caching on the page; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte |= PG_CCB;
	}
#endif

	*caddr1_pte = npte;
	TBIS((vaddr_t)CADDR1);

	zeropage(CADDR1);

#ifdef PMAP_DEBUG
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);
#endif
#endif	/* __HAVE_PMAP_DIRECT */
}

/*
 * pmap_copy_page:		[ INTERFACE ]
 *
 *	Copy the specified (machine independent) page by mapping the page
 *	into virtual memory and using bcopy to copy the page, one machine
 *	dependent page at a time.
 *
 *	Note: WE DO NOT CURRENTLY LOCK THE TEMPORARY ADDRESSES!
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
#ifdef __HAVE_PMAP_DIRECT
	vaddr_t srcva = pmap_map_direct(srcpg);
	vaddr_t dstva = pmap_map_direct(dstpg);
	copypage((void *)srcva, (void *)dstva);
#else
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

	int npte1, npte2;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_copy_page(%lx, %lx)\n", src, dst));

	npte1 = src | PG_RO | PG_V;
	npte2 = dst | PG_V;

#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040) {
		/*
		 * Set copyback caching on the pages; this is required
		 * for cache consistency (since regular mappings are
		 * copyback as well).
		 */
		npte1 |= PG_CCB;
		npte2 |= PG_CCB;
	}
#endif

	*caddr1_pte = npte1;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = npte2;
	TBIS((vaddr_t)CADDR2);

	copypage(CADDR1, CADDR2);

#ifdef PMAP_DEBUG
	*caddr1_pte = PG_NV;
	TBIS((vaddr_t)CADDR1);

	*caddr2_pte = PG_NV;
	TBIS((vaddr_t)CADDR2);
#endif
#endif	/* __HAVE_PMAP_DIRECT */
}

/*
 * pmap_clear_modify:		[ INTERFACE ]
 *
 *	Clear the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
{
	boolean_t rv;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pg));

	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
	return rv;
}

/*
 * pmap_clear_reference:	[ INTERFACE ]
 *
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(pg)
	struct vm_page *pg;
{
	boolean_t rv;

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pg));

	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
	return rv;
}

/*
 * pmap_is_referenced:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
 */
boolean_t
pmap_is_referenced(pg)
	struct vm_page *pg;
{
#ifdef PMAP_DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pg, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pg, "FT"[rv]);
		return(rv);
	}
#endif
	return(pmap_testbit(pg, PG_U));
}

/*
 * pmap_is_modified:		[ INTERFACE ]
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(pg)
	struct vm_page *pg;
{
#ifdef PMAP_DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pg, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pg, "FT"[rv]);
		return(rv);
	}
#endif
	return(pmap_testbit(pg, PG_M));
}

/*
 * Miscellaneous support routines follow
 */

/*
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
 *
 *	If (flags & PRM_KEEPPTPAGE), we don't free the page table page
 *	if the reference drops to zero.
 */
void
pmap_remove_mapping(pmap, va, pte, flags)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
	int flags;
{
	struct vm_page *pg;
	paddr_t pa;
	struct pv_entry *pv, *prev, *cur;
	pmap_t ptpmap;
	st_entry_t *ste;
	int s, bits;
#ifdef PMAP_DEBUG
	pt_entry_t opte;
#endif

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    pmap, va, pte, flags));

	/*
	 * PTE not provided, compute it from pmap and va.
	 */

	if (pte == PT_ENTRY_NULL) {
		pte = pmap_pte(pmap, va);
		if (*pte == PG_NV)
			return;
	}
	pa = pmap_pte_pa(pte);
#ifdef PMAP_DEBUG
	opte = *pte;
#endif

#if defined(M68040) || defined(M68060)
	if ((mmutype <= MMU_68040) && (flags & PRM_CFLUSH)) {
		DCFP(pa);
		ICPP(pa);
	}
#endif

	/*
	 * Update statistics
	 */

	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */

	PMAP_DPRINTF(PDB_REMOVE, ("remove: invalidating pte at %p\n", pte));
	bits = *pte & (PG_U|PG_M);
	*pte = PG_NV;
	if ((flags & PRM_TFLUSH) && active_pmap(pmap))
		TBIS(va);

	/*
	 * For user mappings decrement the wiring count on
	 * the PT page.
	 */

	if (pmap != pmap_kernel()) {
		vaddr_t ptpva = trunc_page((vaddr_t)pte);
		int refs = pmap_ptpage_delref(ptpva);
#ifdef PMAP_DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", ptpva);
#endif

		/*
		 * If reference count drops to zero, and we're not instructed
		 * to keep it around, free the PT page.
		 */

		if (refs == 0 && (flags & PRM_KEEPPTPAGE) == 0) {
#ifdef DIAGNOSTIC
			struct pv_entry *pv;
#endif
			paddr_t pa;

			pa = pmap_pte_pa(pmap_pte(pmap_kernel(), ptpva));
			pg = PHYS_TO_VM_PAGE(pa);
#ifdef DIAGNOSTIC
			if (pg == NULL)
				panic("pmap_remove_mapping: unmanaged PT page");
			pv = pg_to_pvh(pg);
			if (pv->pv_ptste == NULL)
				panic("pmap_remove_mapping: ptste == NULL");
			if (pv->pv_pmap != pmap_kernel() ||
			    pv->pv_va != ptpva ||
			    pv->pv_next != NULL)
				panic("pmap_remove_mapping: "
				    "bad PT page pmap %p, va 0x%lx, next %p",
				    pv->pv_pmap, pv->pv_va, pv->pv_next);
#endif
			pmap_remove_mapping(pmap_kernel(), ptpva,
			    PT_ENTRY_NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(pg);
			PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
			    ("remove: PT page 0x%lx (0x%lx) freed\n",
			    ptpva, pa));
		}
	}

	/*
	 * If this isn't a managed page, we are all done.
	 */

	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
		return;

	/*
	 * Otherwise remove it from the PV table
	 * (raise IPL since we may be called at interrupt time).
	 */

	pv = pg_to_pvh(pg);
	s = splvm();

	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		ste = pv->pv_ptste;
		ptpmap = pv->pv_ptpmap;
		cur = pv->pv_next;
		if (cur != NULL) {
			cur->pv_flags = pv->pv_flags;
			*pv = *cur;
			pmap_free_pv(cur);
		} else
			pv->pv_pmap = NULL;
	} else {
		prev = pv;
		for (cur = pv->pv_next; cur != NULL; cur = cur->pv_next) {
			if (pmap == cur->pv_pmap && va == cur->pv_va)
				break;
			prev = cur;
		}
#ifdef PMAP_DEBUG
		if (cur == NULL)
			panic("pmap_remove: PA not in pv_tab");
#endif
		ste = cur->pv_ptste;
		ptpmap = cur->pv_ptpmap;
		prev->pv_next = cur->pv_next;
		pmap_free_pv(cur);
	}

	/*
	 * If this was a PT page we must also remove the
	 * mapping from the associated segment table.
	 */

	if (ste) {
		PMAP_DPRINTF(PDB_REMOVE|PDB_PTPAGE,
		    ("remove: ste was %x@@%p pte was %x@@%p\n",
		    *ste, ste, opte, pmap_pte(pmap, va)));
#if defined(M68040) || defined(M68060)
		if (mmutype <= MMU_68040) {
			st_entry_t *este = &ste[NPTEPG/SG4_LEV3SIZE];

			while (ste < este)
				*ste++ = SG_NV;
#ifdef PMAP_DEBUG
			ste -= NPTEPG/SG4_LEV3SIZE;
#endif
		} else
#endif
		*ste = SG_NV;

		/*
		 * If it was a user PT page, we decrement the
		 * reference count on the segment table as well,
		 * freeing it if it is now empty.
		 */

		if (ptpmap != pmap_kernel()) {
			PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
			    ("remove: stab %p, refcnt %d\n",
			    ptpmap->pm_stab, ptpmap->pm_sref - 1));
#ifdef PMAP_DEBUG
			if ((pmapdebug & PDB_PARANOIA) &&
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
				panic("remove: bogus ste");
#endif
			if (--(ptpmap->pm_sref) == 0) {
				PMAP_DPRINTF(PDB_REMOVE|PDB_SEGTAB,
				    ("remove: free stab %p\n",
				    ptpmap->pm_stab));
				pmap_remove(pmap_kernel(),
				    (vaddr_t)ptpmap->pm_stab,
				    (vaddr_t)ptpmap->pm_stab + MACHINE_STSIZE);
				pmap_update(pmap_kernel());
				uvm_pagefree(PHYS_TO_VM_PAGE((paddr_t)
				    ptpmap->pm_stpa));
				uvm_km_free_wakeup(st_map,
						(vaddr_t)ptpmap->pm_stab,
						MACHINE_STSIZE);
				ptpmap->pm_stab = Segtabzero;
				ptpmap->pm_stpa = Segtabzeropa;
#if defined(M68040) || defined(M68060)
				if (mmutype <= MMU_68040)
					ptpmap->pm_stfree = protostfree;
#endif

				/*
				 * XXX may have changed segment table
				 * pointer for current process so
				 * update now to reload hardware.
				 */

				if (active_user_pmap(ptpmap))
					PMAP_ACTIVATE(ptpmap, 1);
			}
#ifdef PMAP_DEBUG
			else if (ptpmap->pm_sref < 0)
				panic("remove: sref < 0");
#endif
		}
#if 0
		/*
		 * XXX this should be unnecessary as we have been
		 * flushing individual mappings as we go.
		 */
		if (ptpmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
#endif
		pv->pv_flags &= ~PV_PTPAGE;
		ptpmap->pm_ptpages--;
	}

	/*
	 * Update saved attributes for managed page
	 */

	pv->pv_flags |= bits;
	splx(s);
}

/*
 * pmap_testbit:
 *
 *	Test the modified/referenced bits of a physical page.
 */
boolean_t
pmap_testbit(pg, bit)
	struct vm_page *pg;
	int bit;
{
	struct pv_entry *pv, *pvl;
	pt_entry_t *pte;
	int s;

	s = splvm();
	pv = pg_to_pvh(pg);

	/*
	 * Check saved info first
	 */

	if (pv->pv_flags & bit) {
		splx(s);
		return(TRUE);
	}
	/*
	 * Not found.  Check current mappings, returning immediately if
	 * found.  Cache a hit to speed future lookups.
	 */
	if (pv->pv_pmap != NULL) {
		for (pvl = pv; pvl != NULL; pvl = pvl->pv_next) {
			pte = pmap_pte(pvl->pv_pmap, pvl->pv_va);
			if (*pte & bit) {
				pv->pv_flags |= bit;
				splx(s);
				return(TRUE);
			}
		}
	}
	splx(s);
	return(FALSE);
}

/*
 * pmap_changebit:
 *
 *	Change the modified/referenced bits, or other PTE bits,
 *	for a physical page.
 */
void
pmap_changebit(pg, set, mask)
	struct vm_page *pg;
	int set, mask;
{
	struct pv_entry *pv;
	pt_entry_t *pte, npte;
	vaddr_t va;
	int s;
#if defined(M68040) || defined(M68060)
	paddr_t pa;
#endif
#if defined(M68040) || defined(M68060)
	boolean_t firstpage = TRUE;
#endif

	PMAP_DPRINTF(PDB_BITS,
	    ("pmap_changebit(%lx, %x, %x)\n", pg, set, mask));

	s = splvm();
	pv = pg_to_pvh(pg);

	/*
	 * Clear saved attributes (modify, reference)
	 */

	pv->pv_flags &= mask;

	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */

	if (pv->pv_pmap != NULL) {
#ifdef PMAP_DEBUG
		int toflush = 0;
#endif
		for (; pv; pv = pv->pv_next) {
#ifdef PMAP_DEBUG
			toflush |= (pv->pv_pmap == pmap_kernel()) ? 2 : 1;
#endif
			va = pv->pv_va;
			pte = pmap_pte(pv->pv_pmap, va);
			npte = (*pte | set) & mask;
			if (*pte != npte) {
#if defined(M68040) || defined(M68060)
				/*
				 * If we are changing caching status or
				 * protection make sure the caches are
				 * flushed (but only once).
				 */
				if (firstpage && (mmutype <= MMU_68040) &&
				    ((set == PG_RO) ||
				     (set & PG_CMASK) ||
				     (mask & PG_CMASK) == 0)) {
					firstpage = FALSE;
					pa = VM_PAGE_TO_PHYS(pg);
					DCFP(pa);
					ICPP(pa);
				}
#endif
				*pte = npte;
				if (active_pmap(pv->pv_pmap))
					TBIS(va);
			}
		}
	}
	splx(s);
}

/*
 * pmap_enter_ptpage:
 *
 *	Allocate and map a PT page for the specified pmap/va pair.
 */
int
pmap_enter_ptpage(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	paddr_t ptpa;
	struct vm_page *pg;
	struct pv_entry *pv;
	st_entry_t *ste;
	int s;
#if defined(M68040) || defined(M68060)
	paddr_t stpa;
#endif

	PMAP_DPRINTF(PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE,
	    ("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va));

	/*
	 * Allocate a segment table if necessary.  Note that it is allocated
	 * from a private map and not pt_map.  This keeps user page tables
	 * aligned on segment boundaries in the kernel address space.
	 * The segment table is wired down.  It will be freed whenever the
	 * reference count drops to zero.
	 */
	if (pmap->pm_stab == Segtabzero) {
		pmap->pm_stab = (st_entry_t *)
			uvm_km_zalloc(st_map, MACHINE_STSIZE);
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab, 
			(paddr_t *)&pmap->pm_stpa);
#if defined(M68040) || defined(M68060)
		if (mmutype <= MMU_68040) {
#ifdef PMAP_DEBUG
			if (dowriteback && dokwriteback) {
#endif
			stpa = (paddr_t)pmap->pm_stpa;
#if defined(M68060)
			if (mmutype == MMU_68060) {
				while (stpa < (paddr_t)pmap->pm_stpa +
				    MACHINE_STSIZE) {
					pg = PHYS_TO_VM_PAGE(stpa);
					pmap_changebit(pg, PG_CI, ~PG_CCB);
					stpa += PAGE_SIZE;
				}
				DCIS();	/* XXX */
			} else
#endif
			{
				pg = PHYS_TO_VM_PAGE(stpa);
				pmap_changebit(pg, 0, ~PG_CCB);
			}
#ifdef PMAP_DEBUG
			}
#endif
			pmap->pm_stfree = protostfree;
		}
#endif
		/*
		 * XXX may have changed segment table pointer for current
		 * process so update now to reload hardware.
		 */
		if (active_user_pmap(pmap))
			PMAP_ACTIVATE(pmap, 1);

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: pmap %p stab %p(%p)\n",
		    pmap, pmap->pm_stab, pmap->pm_stpa));
	}

	ste = pmap_ste(pmap, va);
#if defined(M68040) || defined(M68060)
	/*
	 * Allocate level 2 descriptor block if necessary
	 */
	if (mmutype <= MMU_68040) {
		if (*ste == SG_NV) {
			int ix;
			caddr_t addr;

			ix = bmtol2(pmap->pm_stfree);
			if (ix == -1) {
				return (ENOMEM);
			}
			pmap->pm_stfree &= ~l2tobm(ix);
			addr = (caddr_t)&pmap->pm_stab[ix*SG4_LEV2SIZE];
			bzero(addr, SG4_LEV2SIZE*sizeof(st_entry_t));
			addr = (caddr_t)&pmap->pm_stpa[ix*SG4_LEV2SIZE];
			*ste = (u_int)addr | SG_RW | SG_U | SG_V;

			PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
			    ("enter: alloc ste2 %d(%p)\n", ix, addr));
		}
		ste = pmap_ste2(pmap, va);
		/*
		 * Since a level 2 descriptor maps a block of SG4_LEV3SIZE
		 * level 3 descriptors, we need a chunk of NPTEPG/SG4_LEV3SIZE
		 * (16) such descriptors (PAGE_SIZE/SG4_LEV3SIZE bytes) to map a
		 * PT page--the unit of allocation.  We set `ste' to point
		 * to the first entry of that chunk which is validated in its
		 * entirety below.
		 */
		ste = (st_entry_t *)((int)ste & ~(PAGE_SIZE/SG4_LEV3SIZE-1));

		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: ste2 %p (%p)\n", pmap_ste2(pmap, va), ste));
	}
#endif
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));

	/*
	 * In the kernel we allocate a page from the kernel PT page
	 * free list and map it into the kernel page table map (via
	 * pmap_enter).
	 */
	if (pmap == pmap_kernel()) {
		struct kpt_page *kpt;

		s = splvm();
		if ((kpt = kpt_free_list) == NULL) {
			/*
			 * No PT pages available.
			 * Try once to free up unused ones.
			 */
			PMAP_DPRINTF(PDB_COLLECT,
			    ("enter: no KPT pages, collecting...\n"));
			pmap_collect(pmap_kernel());
			if ((kpt = kpt_free_list) == NULL) {
				splx(s);
				return (ENOMEM);
			}
		}
		kpt_free_list = kpt->kpt_next;
		kpt->kpt_next = kpt_used_list;
		kpt_used_list = kpt;
		ptpa = kpt->kpt_pa;
		pg = PHYS_TO_VM_PAGE(ptpa);
		bzero((caddr_t)kpt->kpt_va, PAGE_SIZE);
		pmap_enter(pmap, va, ptpa, VM_PROT_READ | VM_PROT_WRITE,
		    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
#if defined(M68060)
		if (mmutype == MMU_68060)
			pmap_changebit(pg, PG_CI, ~PG_CCB);
#endif
		pmap_update(pmap);
#ifdef PMAP_DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE)) {
			int ix = pmap_ste(pmap, va) - pmap_ste(pmap, 0);

			printf("enter: add &Sysptmap[%d]: %x (KPT page %lx)\n",
			       ix, Sysptmap[ix], kpt->kpt_va);
		}
#endif
		splx(s);
	} else {

		/*
		 * For user processes we just allocate a page from the
		 * VM system.  Note that we set the page "wired" count to 1,
		 * which is what we use to check if the page can be freed.
		 * See pmap_remove_mapping().
		 *
		 * Count the segment table reference first so that we won't
		 * lose the segment table when low on memory.
		 */

		pmap->pm_sref++;
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
		    ("enter: about to alloc UPT pg at %lx\n", va));
		while ((pg = uvm_pagealloc(uvm.kernel_object, va, NULL,
		    UVM_PGA_ZERO)) == NULL) {
			uvm_wait("ptpage");
		}
		atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_FAKE);
		UVM_PAGE_OWN(pg, NULL);
		ptpa = VM_PAGE_TO_PHYS(pg);
		pmap_enter(pmap_kernel(), va, ptpa,
		    VM_PROT_READ | VM_PROT_WRITE,
		    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
		pmap_update(pmap_kernel());
	}
#if defined(M68040) || defined(M68060)
	/*
	 * Turn off copyback caching of page table pages,
	 * could get ugly otherwise.
	 */
#ifdef PMAP_DEBUG
	if (dowriteback && dokwriteback)
#endif
	if (mmutype <= MMU_68040) {
#ifdef PMAP_DEBUG
		pt_entry_t *pte = pmap_pte(pmap_kernel(), va);
		if ((pmapdebug & PDB_PARANOIA) && (*pte & PG_CCB) == 0)
			printf("%s PT no CCB: kva=%lx ptpa=%lx pte@@%p=%x\n",
			       pmap == pmap_kernel() ? "Kernel" : "User",
			       va, ptpa, pte, *pte);
#endif
#ifdef M68060
		if (mmutype == MMU_68060) {
			pmap_changebit(pg, PG_CI, ~PG_CCB);
			DCIS();
		} else
#endif
			pmap_changebit(pg, 0, ~PG_CCB);
	}
#endif
	/*
	 * Locate the PV entry in the kernel for this PT page and
	 * record the STE address.  This is so that we can invalidate
	 * the STE when we remove the mapping for the page.
	 */
	pv = pg_to_pvh(pg);
	s = splvm();
	if (pv) {
		pv->pv_flags |= PV_PTPAGE;
		do {
			if (pv->pv_pmap == pmap_kernel() && pv->pv_va == va)
				break;
		} while ((pv = pv->pv_next));
	}
#ifdef PMAP_DEBUG
	if (pv == NULL)
		panic("pmap_enter_ptpage: PT page not entered");
#endif
	pv->pv_ptste = ste;
	pv->pv_ptpmap = pmap;

	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE,
	    ("enter: new PT page at PA %lx, ste at %p\n", ptpa, ste));

	/*
	 * Map the new PT page into the segment table.
	 * Also increment the reference count on the segment table if this
	 * was a user page table page.  Note that we don't use vm_map_pageable
	 * to keep the count like we do for PT pages, this is mostly because
	 * it would be difficult to identify ST pages in pmap_pageable to
	 * release them.  We also avoid the overhead of vm_map_pageable.
	 */
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040) {
		st_entry_t *este;

		for (este = &ste[NPTEPG/SG4_LEV3SIZE]; ste < este; ste++) {
			*ste = ptpa | SG_U | SG_RW | SG_V;
			ptpa += SG4_LEV3SIZE * sizeof(st_entry_t);
		}
	} else
#endif
	*ste = (ptpa & SG_FRAME) | SG_RW | SG_V;
	if (pmap != pmap_kernel()) {
		PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
		    ("enter: stab %p refcnt %d\n",
		    pmap->pm_stab, pmap->pm_sref));
	}

#if defined(M68060)
	if (mmutype == MMU_68060) {
		/*
		 * Flush stale TLB info.
		 */
		if (pmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
	}
#endif
	pmap->pm_ptpages++;
	splx(s);
	return (0);
}

/*
 * pmap_ptpage_addref:
 *
 *	Add a reference to the specified PT page.
 */
void
pmap_ptpage_addref(ptpva)
	vaddr_t ptpva;
{
	struct vm_page *pg;

	simple_lock(&uvm.kernel_object->vmobjlock);
	pg = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	pg->wire_count++;
	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
	    ("ptpage addref: pg %p now %d\n", pg, pg->wire_count));
	simple_unlock(&uvm.kernel_object->vmobjlock);
}

/*
 * pmap_ptpage_delref:
 *
 *	Delete a reference to the specified PT page.
 */
int
pmap_ptpage_delref(ptpva)
	vaddr_t ptpva;
{
	struct vm_page *pg;
	int rv;

	simple_lock(&uvm.kernel_object->vmobjlock);
	pg = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	rv = --pg->wire_count;
	PMAP_DPRINTF(PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB,
	    ("ptpage delref: pg %p now %d\n", pg, pg->wire_count));
	simple_unlock(&uvm.kernel_object->vmobjlock);
	return (rv);
}

void
pmap_proc_iflush(p, va, len)
	struct proc	*p;
	vaddr_t		va;
	vsize_t		len;
{
	(void)cachectl(p, CC_EXTPURGE | CC_IPURGE, va, len);
}

#ifdef PMAP_DEBUG
/*
 * pmap_pvdump:
 *
 *	Dump the contents of the PV list for the specified physical page.
 */
void
pmap_pvdump(pa)
	paddr_t pa;
{
	struct pv_entry *pv;

	printf("pa %lx", pa);
	for (pv = pa_to_pvh(pa); pv; pv = pv->pv_next)
		printf(" -> pmap %p, va %lx, ptste %p, ptpmap %p, flags %x",
		       pv->pv_pmap, pv->pv_va, pv->pv_ptste, pv->pv_ptpmap,
		       pv->pv_flags);
	printf("\n");
}

/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
void
pmap_check_wiring(str, va)
	char *str;
	vaddr_t va;
{
	pt_entry_t *pte;
	paddr_t pa;
	struct vm_page *pg;
	int count;

	if (!pmap_ste_v(pmap_kernel(), va) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;

	pa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg->wire_count >= PAGE_SIZE / sizeof(pt_entry_t)) {
		printf("*%s*: 0x%lx: wire count %d\n", str, va, pg->wire_count);
		return;
	}

	count = 0;
	for (pte = (pt_entry_t *)va; pte < (pt_entry_t *)(va + PAGE_SIZE);
	    pte++)
		if (*pte)
			count++;
	if (pg->wire_count != count)
		printf("*%s*: 0x%lx: w%d/a%d\n",
		       str, va, pg->wire_count, count);
}
#endif /* PMAP_DEBUG */
@


1.69
log
@Remove unused mem_size global.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.68 2012/11/21 21:28:26 miod Exp $ */
@


1.68
log
@Trivial fixes for labels not followed by statements, which gcc 3 complains
about.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.67 2012/04/10 15:50:52 guenther Exp $ */
a271 1
vsize_t		mem_size;	/* memory size in bytes */
@


1.67
log
@Make the KERN_NPROCS and KERN_MAXPROC sysctl()s and the RLIMIT_NPROC rlimit
count processes instead of threads.  New sysctl()s KERN_NTHREADS and
KERN_MAXTHREAD count and limit threads.  The nprocs and maxproc kernel
variables are replaced by nprocess, maxprocess, nthreads, and maxthread.

ok tedu@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.66 2011/11/01 21:20:55 miod Exp $ */
d754 4
a757 5
				if ((flags & PRM_SKIPWIRED) &&
				    pmap_pte_w(pte))
					goto skip;
				pmap_remove_mapping(pmap, sva, pte, flags);
skip:
@


1.66
log
@Drop support for the HP MMU. It was only found on two hp300 systems (models
320 and 350), which have been unsupported since a bunch of release already,
because this annoying MMU does not have TT registers (or if it does, it is
not documented) and thus went in the way of PMAP_DIRECT for hp300.
In other words: remove a bunch of code which was either #ifdef'ed out or
had no chance to run in real life.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.65 2011/09/28 18:36:01 miod Exp $ */
d457 1
a457 1
	npages = min(atop(MACHINE_MAX_KPTSIZE), maxproc+16);
d498 1
a498 1
	s = maxproc * MACHINE_STSIZE;
d522 1
a522 1
	if ((MACHINE_PTMAXSIZE / MACHINE_MAX_PTSIZE) < maxproc) {
d526 1
a526 1
		 * page tables, so we lower maxproc so that fork()
d530 1
a530 1
		maxproc = (MACHINE_PTMAXSIZE / MACHINE_MAX_PTSIZE);
d532 1
a532 1
		s = (maxproc * MACHINE_MAX_PTSIZE);
@


1.65
log
@Oops, do not do last minute edits before commit... Make the previous change
compile.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.64 2011/09/27 20:35:44 miod Exp $ */
a730 3
#ifdef M68K_MMU_HP
	boolean_t firstpage, needcflush;
#endif
a731 4
#ifdef M68K_MMU_HP
	firstpage = TRUE;
	needcflush = FALSE;
#endif
a756 21
#ifdef M68K_MMU_HP
				if (pmap_aliasmask) {
					/*
					 * Purge kernel side of VAC to ensure
					 * we get the correct state of any
					 * hardware maintained bits.
					 */
					if (firstpage) {
						DCIS();
					}
					/*
					 * Remember if we may need to
					 * flush the VAC due to a non-CI
					 * mapping.
					 */
					if (!needcflush && !pmap_pte_ci(pte))
						needcflush = TRUE;

					firstpage = FALSE;
				}
#endif
a763 26
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Didn't do anything, no need for cache flushes
		 */
		if (firstpage)
			return;
		/*
		 * In a couple of cases, we don't need to worry about flushing
		 * the VAC:
		 * 	1. if this is a kernel mapping,
		 *	   we have already done it
		 *	2. if it is a user mapping not for the current process,
		 *	   it won't be there
		 */
		if (!active_user_pmap(pmap))
			needcflush = FALSE;
		if (needcflush) {
			if (pmap == pmap_kernel()) {
				DCIS();
			} else {
				DCIU();
			}
		}
	}
#endif
a821 3
#ifdef M68K_MMU_HP
	boolean_t firstpage;
#endif
a833 3
#ifdef M68K_MMU_HP
	firstpage = TRUE;
#endif
a852 12
#ifdef M68K_MMU_HP
				/*
				 * Purge kernel side of VAC to ensure we
				 * get the correct state of any hardware
				 * maintained bits.
				 *
				 * XXX do we need to clear the VAC in
				 * general to reflect the new protection?
				 */
				if (firstpage && pmap_aliasmask)
					DCIS();
#endif
a867 3
#ifdef M68K_MMU_HP
				firstpage = FALSE;
#endif
a928 3
#ifdef M68K_MMU_HP
	boolean_t checkpv = TRUE;
#endif
a992 3
#ifdef M68K_MMU_HP
		checkpv = FALSE;
#endif
a1068 43
#ifdef M68K_MMU_HP
			/*
			 * Since there is another logical mapping for the
			 * same page we may need to cache-inhibit the
			 * descriptors on those CPUs with external VACs.
			 * We don't need to CI if:
			 *
			 * - No two mappings belong to the same user pmaps.
			 *   Since the cache is flushed on context switches
			 *   there is no problem between user processes.
			 *
			 * - Mappings within a single pmap are a certain
			 *   magic distance apart.  VAs at these appropriate
			 *   boundaries map to the same cache entries or
			 *   otherwise don't conflict.
			 *
			 * To keep it simple, we only check for these special
			 * cases if there are only two mappings, otherwise we
			 * punt and always CI.
			 *
			 * Note that there are no aliasing problems with the
			 * on-chip data-cache when the WA bit is set.
			 */
			if (pmap_aliasmask) {
				if (pv->pv_flags & PV_CI) {
					PMAP_DPRINTF(PDB_CACHE,
					    ("enter: pa %lx already CI'ed\n",
					    pa));
					checkpv = cacheable = FALSE;
				} else if (npv->pv_next ||
					   ((pmap == pv->pv_pmap ||
					     pmap == pmap_kernel() ||
					     pv->pv_pmap == pmap_kernel()) &&
					    ((pv->pv_va & pmap_aliasmask) !=
					     (va & pmap_aliasmask)))) {
					PMAP_DPRINTF(PDB_CACHE,
					    ("enter: pa %lx CI'ing all\n",
					    pa));
					cacheable = FALSE;
					pv->pv_flags |= PV_CI;
				}
			}
#endif
d1090 1
a1090 4
	else {
#ifdef M68K_MMU_HP
		checkpv =
#endif
a1091 1
	}
a1100 8
#ifdef M68K_MMU_HP
	/*
	 * Purge kernel side of VAC to ensure we get correct state
	 * of HW bits so we don't clobber them.
	 */
	if (pmap_aliasmask)
		DCIS();
#endif
a1112 3
#ifdef M68K_MMU_HP
		checkpv =
#endif
a1115 3
#ifdef M68K_MMU_HP
	if (!checkpv && !cacheable)
#else
a1116 1
#endif
a1136 18
#ifdef M68K_MMU_HP
	/*
	 * The following is executed if we are entering a second
	 * (or greater) mapping for a physical page and the mappings
	 * may create an aliasing problem.  In this case we must
	 * cache inhibit the descriptors involved and flush any
	 * external VAC.
	 */
	if (checkpv && !cacheable) {
		pmap_changebit(pg, PG_CI, ~0);
		DCIA();
#ifdef PMAP_DEBUG
		if ((pmapdebug & (PDB_CACHE|PDB_PVDUMP)) ==
		    (PDB_CACHE|PDB_PVDUMP))
			pmap_pvdump(pa);
#endif
	}
#endif
a1226 3
#ifdef M68K_MMU_HP
	boolean_t firstpage, needcflush;
#endif
a1232 4
#ifdef M68K_MMU_HP
	firstpage = TRUE;
	needcflush = FALSE;
#endif
a1270 22
#ifdef M68K_MMU_HP
				if (pmap_aliasmask) {

					/*
					 * Purge kernel side of VAC to ensure
					 * we get the correct state of any
					 * hardware maintained bits.
					 */

					if (firstpage) {
						DCIS();
					}

					/*
					 * Remember if we may need to
					 * flush the VAC.
					 */

					needcflush = TRUE;
					firstpage = FALSE;
				}
#endif
a1288 30

#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Didn't do anything, no need for cache flushes
		 */

		if (firstpage)
			return;

		/*
		 * In a couple of cases, we don't need to worry about flushing
		 * the VAC:
		 *      1. if this is a kernel mapping,
		 *         we have already done it
		 *      2. if it is a user mapping not for the current process,
		 *         it won't be there
		 */

		if (!active_user_pmap(pmap))
			needcflush = FALSE;
		if (needcflush) {
			if (pmap == pmap_kernel()) {
				DCIS();
			} else {
				DCIU();
			}
		}
	}
#endif
a1559 9
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Cache-inhibit the mapping on VAC machines, as we would
		 * be wasting the cache load.
		 */
		npte |= PG_CI;
	}
#endif
a1609 10
#ifdef M68K_MMU_HP
	if (pmap_aliasmask) {
		/*
		 * Cache-inhibit the mapping on VAC machines, as we would
		 * be wasting the cache load.
		 */
		npte1 |= PG_CI;
		npte2 |= PG_CI;
	}
#endif
a1716 25
#ifdef M68K_MMU_HP
/*
 * pmap_prefer:			[ INTERFACE ]
 *
 *	Find the first virtual address >= *vap that does not
 *	cause a virtually-tagged cache alias problem.
 */
vaddr_t
pmap_prefer(vaddr_t foff, vaddr_t va)
{
	vsize_t d;

#ifdef M68K_MMU_MOTOROLA
	if (pmap_aliasmask)
#endif
	{
		d = foff - va;
		d &= pmap_aliasmask;
		va += d;
	}

	return va;
}
#endif /* M68K_MMU_HP */

a1765 21
#ifdef M68K_MMU_HP
	if (pmap_aliasmask && (flags & PRM_CFLUSH)) {

		/*
		 * Purge kernel side of VAC to ensure we get the correct
		 * state of any hardware maintained bits.
		 */

		DCIS();

		/*
		 * If this is a non-CI user mapping for the current process,
		 * flush the VAC.  Note that the kernel side was flushed
		 * above so we don't worry about non-CI kernel mappings.
		 */

		if (active_user_pmap(pmap) && !pmap_pte_ci(pte)) {
			DCIU();
		}
	}
#endif
a1891 19
#ifdef M68K_MMU_HP

	/*
	 * If only one mapping left we no longer need to cache inhibit
	 */

	if (pmap_aliasmask &&
	    pv->pv_pmap && pv->pv_next == NULL && (pv->pv_flags & PV_CI)) {
		PMAP_DPRINTF(PDB_CACHE,
		    ("remove: clearing CI for pa %lx\n", pa));
		pv->pv_flags &= ~PV_CI;
		pmap_changebit(pg, 0, ~PG_CI);
#ifdef PMAP_DEBUG
		if ((pmapdebug & (PDB_CACHE|PDB_PVDUMP)) ==
		    (PDB_CACHE|PDB_PVDUMP))
			pmap_pvdump(pa);
#endif
	}
#endif
a2010 7
#ifdef M68K_MMU_HP
	/*
	 * Flush VAC to get correct state of any hardware maintained bits.
	 */
	if (pmap_aliasmask && (bit & (PG_U|PG_M)))
		DCIS();
#endif
d2047 1
a2047 1
#if defined(M68K_MMU_HP) || defined(M68040) || defined(M68060)
a2077 10
#ifdef M68K_MMU_HP
			/*
			 * Flush VAC to ensure we get correct state of HW bits
			 * so we don't clobber them.
			 */
			if (firstpage && pmap_aliasmask) {
				firstpage = FALSE;
				DCIS();
			}
#endif
@


1.64
log
@Use a pool to allocate pv_entry from, instead of allocating whole pages and
maintaining our own free lists. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.63 2011/09/22 17:41:00 jasper Exp $ */
d561 2
@


1.63
log
@nowadays uvm_init() calls pmap_init(), not vm_init(); so update the comments.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.62 2011/05/27 20:10:18 miod Exp $ */
a275 3
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int		pv_nfree;

d281 1
a406 2
	TAILQ_INIT(&pv_page_freelist);

d548 3
a550 1
	    &pool_allocator_nointr);
d561 1
a561 29
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;

	if (pv_nfree == 0) {
		pvp = (struct pv_page *)uvm_km_kmemalloc(kernel_map,
		    uvm.kernel_object, PAGE_SIZE, UVM_KMF_NOWAIT);
		if (pvp == NULL)
			return NULL;
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = NULL;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = TAILQ_FIRST(&pv_page_freelist);
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		}
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == NULL)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
d574 1
a574 17
	struct pv_page *pvp;

	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		uvm_km_free(kernel_map, (vaddr_t)pvp, PAGE_SIZE);
		break;
	}
@


1.62
log
@Thou shall not use uninitialized TAILQs
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.61 2011/05/24 15:27:36 ariane Exp $ */
d394 1
a394 1
 *	Initialize the pmap module.  Called by vm_init(), to initialize any
@


1.61
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.60 2011/04/28 20:53:32 ariane Exp $ */
d409 2
d575 1
a575 1
		pv->pv_next = 0;
d587 1
a587 1
		if (pv == 0)
@


1.60
log
@Expose pmap_prefer parameters.
This will enable uvm_map to behave intelligently when allocating.

Prodded by deraadt to commit this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.59 2010/12/06 20:57:16 miod Exp $ */
d313 10
a422 13
	addr = (vaddr_t) Sysmap;
	if (uvm_map(kernel_map, &addr, MACHINE_MAX_PTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED))) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
		panic("pmap_init: bogons in the VM system!");
	}
a424 3
	    ("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
	    Sysseg, Sysmap, Sysptmap));
	PMAP_DPRINTF(PDB_INIT,
a462 12
	 * Verify that space will be allocated in region for which
	 * we already have kernel PT pages.
	 */
	addr = 0;
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv || (addr + s) >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	uvm_unmap(kernel_map, addr, addr + s);

	/*
d503 19
@


1.59
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.58 2010/06/29 20:30:32 guenther Exp $ */
a278 3
#if defined(M68K_MMU_HP)
extern int	pmap_aliasmask;	/* separation at which VA aliasing is ok */
#endif
@


1.58
log
@Remove COMPAT_HPUX.  No one wanted to support it and its fewmets were
blocking other cleanups
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.57 2010/06/26 23:24:43 guenther Exp $ */
d2007 2
a2008 3
void
pmap_prefer(foff, vap)
	vaddr_t foff, *vap;
a2009 1
	vaddr_t va;
a2015 1
		va = *vap;
d2018 1
a2018 1
		*vap = va + d;
d2020 2
@


1.57
log
@Don't #include <sys/user.h> into files that don't need the stuff
it defines.  In some cases, this means pulling in uvm.h or pcb.h
instead, but most of the inclusions were just noise.  Tested on
alpha, amd64, armish, hppa, i386, macpcc, sgi, sparc64, and vax,
mostly by krw and naddy.
ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.56 2010/05/13 20:37:00 miod Exp $ */
a292 3
#ifdef COMPAT_HPUX
int		 pmap_mapmulti(pmap_t, vaddr_t);
#endif
a2024 38

#ifdef COMPAT_HPUX
/*
 * pmap_mapmulti:
 *
 *	'PUX hack for dealing with the so called multi-mapped address space.
 *	The first 256mb is mapped in at every 256mb region from 0x10000000
 *	up to 0xF0000000.  This allows for 15 bits of tag information.
 *
 *	We implement this at the segment table level, the machine independent
 *	VM knows nothing about it.
 */
int
pmap_mapmulti(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	st_entry_t *ste, *bste;

#ifdef PMAP_DEBUG
	if (pmapdebug & PDB_MULTIMAP) {
		ste = pmap_ste(pmap, HPMMBASEADDR(va));
		printf("pmap_mapmulti(%p, %lx): bste %p(%x)",
		       pmap, va, ste, *ste);
		ste = pmap_ste(pmap, va);
		printf(" ste %p(%x)\n", ste, *ste);
	}
#endif
	bste = pmap_ste(pmap, HPMMBASEADDR(va));
	ste = pmap_ste(pmap, va);
	if (!(*ste & SG_V) && (*bste & SG_V)) {
		*ste = *bste;
		TBIAU();
		return (0);
	}
	return (EFAULT);
}
#endif /* COMPAT_HPUX */
@


1.56
log
@If __HAVE_PMAP_DIRECT, do not bother creating special mappings for use in
pmap_{copy,zero}_page, but use direct mapped pages instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.55 2009/04/06 20:37:52 oga Exp $ */
a151 1
#include <sys/user.h>
@


1.55
log
@Remove the only (commented) instance of UVM_PAGE_INLINE from the tree
now that it has been removed.  uncommenting that would not even have
been the correct way to enable it.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.54 2008/06/26 05:42:11 ray Exp $ */
d261 1
a262 1

d265 1
d406 1
d413 1
d1061 2
a1062 1
#ifdef DIAGNOSTIC
d1070 1
d1818 4
d1858 1
d1873 5
d1925 1
@


1.54
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.53 2008/06/14 10:55:20 mk Exp $ */
a156 1
/* #define UVM_PAGE_INLINE */
@


1.53
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.52 2007/04/13 18:57:49 art Exp $ */
a17 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.52
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.51 2007/04/04 17:44:45 art Exp $ */
d647 1
a647 2
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));
@


1.51
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.50 2007/03/04 16:59:03 miod Exp $ */
d2682 1
a2682 1
		pg->pg_flags &= ~(PG_BUSY|PG_FAKE);
@


1.50
log
@Oops, bring back pmap_collect1() and the check for pmap_kernel() in
pmap_collect() as we do an explicit pmap_collect(pmap_kernel()) in dire
memory situations.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.49 2007/02/17 19:08:58 miod Exp $ */
d2682 1
a2682 1
		pg->flags &= ~(PG_BUSY|PG_FAKE);
@


1.49
log
@If we __HAVE_PMAP_DIRECT, handle these mappings in pmap_extract().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.48 2007/02/17 19:08:16 miod Exp $ */
d311 1
d781 1
d1676 132
a1807 10
	/*
	 * This process is about to be swapped out; free all of
	 * the PT pages by removing the physical mappings for its
	 * entire address space.  Note: pmap_remove() performs
	 * all necessary locking.
	 */
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
	pmap_remove_flags(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS,
	    flags | PRM_SKIPWIRED);
	pmap_update(pmap);
@


1.48
log
@In pmap_collect():
- do not bother checking for pmap_kernel() and operating on it, as
  pmap_collect() will not be invoked for P_SYSTEM process.
- preserve wired pages while purging.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.47 2006/08/22 21:03:56 miod Exp $ */
d1627 8
@


1.47
log
@Implement pmap_steal_memory() if HAVE_PMAP_DIRECT; no change except for
faster uvm initialization.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.46 2006/06/24 13:22:15 miod Exp $ */
d304 1
a310 1
void		 pmap_collect1(pmap_t, paddr_t, paddr_t);
d319 4
a322 3
#define	PRM_TFLUSH	1
#define	PRM_CFLUSH	2
#define	PRM_KEEPPTPAGE	4
d767 14
a785 4
	int flags;

	PMAP_DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
	    ("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva));
a790 1
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
d813 3
d838 1
d1662 1
d1666 10
a1675 131
	if (pmap == pmap_kernel()) {
		int bank, s;

		/*
		 * XXX This is very bogus.  We should handle kernel PT
		 * XXX pages much differently.
		 */

		s = splvm();
		for (bank = 0; bank < vm_nphysseg; bank++)
			pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			    ptoa(vm_physmem[bank].end));
		splx(s);
	} else {
		/*
		 * This process is about to be swapped out; free all of
		 * the PT pages by removing the physical mappings for its
		 * entire address space.  Note: pmap_remove() performs
		 * all necessary locking.
		 */
		pmap_remove(pmap, VM_MIN_ADDRESS, VM_MAX_ADDRESS);
		pmap_update(pmap);
	}
}

/*
 * pmap_collect1:
 *
 *	Garbage-collect KPT pages.  Helper for the above (bogus)
 *	pmap_collect().
 *
 *	Note: THIS SHOULD GO AWAY, AND BE REPLACED WITH A BETTER
 *	WAY OF HANDLING PT PAGES!
 */
void
pmap_collect1(pmap, startpa, endpa)
	pmap_t		pmap;
	paddr_t		startpa, endpa;
{
	paddr_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	paddr_t kpa;
#ifdef PMAP_DEBUG
	st_entry_t *ste;
	int opmapdebug = 0 /* XXX initialize to quiet gcc -Wall */;
#endif

	for (pa = startpa; pa < endpa; pa += PAGE_SIZE) {
		struct kpt_page *kpt, **pkpt;

		/*
		 * Locate physical pages which are being used as kernel
		 * page table pages.
		 */
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptste && pv->pv_ptpmap == pmap_kernel())
				break;
		} while ((pv = pv->pv_next));
		if (pv == NULL)
			continue;
#ifdef PMAP_DEBUG
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + MACHINE_MAX_PTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (pt_entry_t *)(pv->pv_va + PAGE_SIZE);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef PMAP_DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf("collect: freeing KPT page at %lx (ste %x@@%p)\n",
			       pv->pv_va, *pv->pv_ptste, pv->pv_ptste);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
		}

		ste = pv->pv_ptste;
#endif
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove_entry to take care of invalidating
		 * ST and Sysptmap entries.
		 */
		pmap_extract(pmap, pv->pv_va, &kpa);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				    PRM_TFLUSH|PRM_CFLUSH);
		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != NULL;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef PMAP_DEBUG
		if (kpt == NULL)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			       kpt->kpt_va, kpa);
#endif
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef PMAP_DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste & SG_V)
			printf("collect: kernel STE at %p still valid (%x)\n",
			       ste, *ste);
		ste = &Sysptmap[ste - pmap_ste(pmap_kernel(), 0)];
		if (*ste & SG_V)
			printf("collect: kernel PTmap at %p still valid (%x)\n",
			       ste, *ste);
#endif
	}
@


1.46
log
@Add pmap_enter_cache(), similar to pmap_kenter_cache() but for managed
pages, and implement pmap_enter() as a particular case of it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.45 2006/06/20 20:40:18 miod Exp $ */
d341 31
d391 1
@


1.45
log
@Fix PMAP_DEBUG-only tests I botched in revision 1.13.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.44 2006/06/17 16:29:11 miod Exp $ */
d978 27
d1016 2
a1017 2
	    ("pmap_enter(%p, %lx, %lx, %x, %x)\n",
	    pmap, va, pa, prot, wired));
d1265 2
a1266 7
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040 && (npte & (PG_PROT|PG_CI)) == PG_RW)
#ifdef PMAP_DEBUG
		if (dowriteback && (dokwriteback || pmap != pmap_kernel()))
#endif
		npte |= PG_CCB;
#endif
@


1.44
log
@Introduce pmap_kenter_cache(), similar to pmap_kenter_pa() but allowing
the pte cache bits to be specified. Will be used very soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.43 2006/06/11 20:48:51 miod Exp $ */
d1719 1
a1719 1
		if (!(*ste & SG_V))
d1723 1
a1723 1
		if (!(*ste & SG_V))
@


1.43
log
@Protect a variable only used for the HP MMU if #ifdef M68K_MMU_HP blocks.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.42 2006/06/11 20:44:20 miod Exp $ */
d1294 20
d1319 1
a1319 1
	    ("pmap_kenter_pa(%lx, %lx, %x)\n", va, pa, prot));
d1329 1
a1329 1
			panic("pmap_kenter_pa: out of address space");
d1336 1
a1336 1
	PMAP_DPRINTF(PDB_ENTER, ("enter: pte %p, *pte %x\n", pte, *pte));
d1350 1
a1350 5
	npte = pa | pte_prot(prot) | PG_V | PG_W;
#if defined(M68040) || defined(M68060)
	if (mmutype <= MMU_68040 && (npte & (PG_PROT)) == PG_RW)
		npte |= PG_CCB;
#endif
d1352 1
a1352 1
	PMAP_DPRINTF(PDB_ENTER, ("enter: new pte value %x\n", npte));
@


1.42
log
@Move pmap_aliasmask declaration and initialization to MD code. No functional
change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.41 2005/11/04 21:51:35 miod Exp $ */
d983 1
d985 1
d1048 1
d1050 1
d1192 4
a1195 1
		checkpv = cacheable = FALSE;
d1226 4
a1229 1
		checkpv = cacheable = FALSE;
d1232 1
d1234 3
@


1.41
log
@Change DEBUG conditionals into PMAP_DEBUG; no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.40 2005/08/01 11:56:45 miod Exp $ */
d288 1
a288 1
int		pmap_aliasmask;	/* separation at which VA aliasing is ok */
@


1.40
log
@In pmap_{,k}remove(), be more careful when deciding to flush the caches, if the
kernel is configured with HP MMU support, but does not run with one.
This speeds up most hp300 systems quite a bit.

Tested millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.39 2005/08/01 11:54:24 miod Exp $ */
d169 1
a169 1
#ifdef DEBUG
a186 1
int debugmap = 0;
d197 1
a197 1
#endif	/* DEBUG */
d313 1
a313 1
#ifdef DEBUG
d840 1
a840 1
#ifdef DEBUG
d853 1
a853 1
#ifdef DEBUG
d1103 1
a1103 1
#ifdef DEBUG
d1226 1
a1226 1
#ifdef DEBUG
d1259 1
a1259 1
#ifdef DEBUG
d1266 1
a1266 1
#ifdef DEBUG
d1375 1
a1375 1
#ifdef DEBUG
d1480 1
a1480 1
#ifdef DEBUG
d1539 1
a1539 1
#ifdef DEBUG
d1610 1
a1610 1
#ifdef DEBUG
d1631 1
a1631 1
#ifdef DEBUG
d1647 1
a1647 1
#ifdef DEBUG
d1675 1
a1675 1
#ifdef DEBUG
d1685 1
a1685 1
#ifdef DEBUG
d1744 1
a1744 1
#ifdef DEBUG
d1802 1
a1802 1
#ifdef DEBUG
d1857 1
a1857 1
#ifdef DEBUG
d1877 1
a1877 1
#ifdef DEBUG
d1931 1
a1931 1
#ifdef DEBUG
d1983 1
a1983 1
#ifdef DEBUG
d2022 1
a2022 1
#ifdef DEBUG
d2059 1
a2059 1
#ifdef DEBUG
d2138 1
a2138 1
#ifdef DEBUG
d2159 1
a2159 1
#ifdef DEBUG
d2182 1
a2182 1
#ifdef DEBUG
d2199 1
a2199 1
#ifdef DEBUG
d2233 1
a2233 1
#ifdef DEBUG
d2350 1
a2350 1
#ifdef DEBUG
d2354 1
a2354 1
#ifdef DEBUG
d2432 1
a2432 1
#ifdef DEBUG
d2451 1
a2451 1
#ifdef DEBUG
d2544 1
a2544 1
#ifdef DEBUG
d2585 1
a2585 1
#ifdef DEBUG
d2589 1
a2589 1
#ifdef DEBUG
d2619 1
a2619 1
#ifdef DEBUG
d2719 1
a2719 1
#ifdef DEBUG
d2776 1
a2776 1
#endif /* DEBUG */
@


1.39
log
@Factorize cachectl() accross m68k platforms, and make the CC_ constants
public.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.38 2005/04/27 00:12:41 miod Exp $ */
d737 1
d739 1
d745 1
d748 1
d790 1
a793 1
				firstpage = FALSE;
a798 5
	/*
	 * Didn't do anything, no need for cache flushes
	 */
	if (firstpage)
		return;
d800 22
a821 15
	/*
	 * In a couple of cases, we don't need to worry about flushing
	 * the VAC:
	 * 	1. if this is a kernel mapping,
	 *	   we have already done it
	 *	2. if it is a user mapping not for the current process,
	 *	   it won't be there
	 */
	if (pmap_aliasmask && !active_user_pmap(pmap))
		needcflush = FALSE;
	if (needcflush) {
		if (pmap == pmap_kernel()) {
			DCIS();
		} else {
			DCIU();
d1341 1
d1343 1
d1350 1
d1353 1
d1411 1
a1426 1
				firstpage = FALSE;
d1433 5
a1437 3
	/*
	 * Didn't do anything, no need for cache flushes
	 */
d1439 2
a1440 3
	if (firstpage)
		return;
#ifdef M68K_MMU_HP
d1442 8
a1449 8
	/*
	 * In a couple of cases, we don't need to worry about flushing
	 * the VAC:
	 *      1. if this is a kernel mapping,
	 *         we have already done it
	 *      2. if it is a user mapping not for the current process,
	 *         it won't be there
	 */
d1451 8
a1458 7
	if (pmap_aliasmask && !active_user_pmap(pmap))
		needcflush = FALSE;
	if (needcflush) {
		if (pmap == pmap_kernel()) {
			DCIS();
		} else {
			DCIU();
@


1.38
log
@Replace the last user of pmap_map() with an inline version, and kill pmap_map()
for good.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.37 2005/04/26 21:12:27 martin Exp $ */
d2705 1
a2705 1
	(void)cachectl(p, 0x80000004, va, len);
@


1.37
log
@remove now obsolete mac68k_set_pte

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.36 2004/12/25 23:02:24 miod Exp $ */
a2765 33

/* XXX this should go out soon */
#ifdef mac68k
/*
 * pmap_map:
 *
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
 */
vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));

	while (spa < epa) {
		pmap_kenter_pa(va, spa, prot);
		va += PAGE_SIZE;
		spa += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	return (va);
}
#endif
@


1.36
log
@Use list and queue macros where applicable to make the code easier to read;
no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.35 2004/11/30 07:41:52 martin Exp $ */
a2796 15
}

void
mac68k_set_pte(va, pge)
	vaddr_t va;
	paddr_t pge;
{
extern	vaddr_t tmp_vpages[];
	register pt_entry_t *pte;

	if (va != tmp_vpages[0])
		return;

	pte = pmap_pte(pmap_kernel(), va);
	*pte = (pt_entry_t) pge;
@


1.35
log
@repair DEBUG kernels

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.34 2004/05/20 09:20:42 kettenis Exp $ */
d557 1
a557 1
		pvp = pv_page_freelist.tqh_first;
@


1.34
log
@Properly flush instruction cache for ptrace(PT_WRTIE_{DI}, ...) on powerpc
and m68k.
ok drahn@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.33 2004/01/01 01:12:54 miod Exp $ */
d2751 1
a2751 1
	if (pg->wire_count >= PAGE_SIZE / sizeof(struct pt_entry_t)) {
@


1.33
log
@Inline pmap_copy() and pmap_phys_address().
Also get rid of a spurious local initialization in pmap_remove_mapping().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.32 2003/12/26 18:54:24 miod Exp $ */
d2697 9
@


1.32
log
@A simple compile-time micro-optimization in pmap_protect() for
non-hp300 flavours.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.31 2003/12/14 19:06:59 miod Exp $ */
a1539 23
 * pmap_copy:		[ INTERFACE ]
 *
 *	Copy the mapping range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t		dst_pmap;
	pmap_t		src_pmap;
	vaddr_t		dst_addr;
	vsize_t		len;
	vaddr_t		src_addr;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
	    dst_pmap, src_pmap, dst_addr, len, src_addr));
}

/*
a1875 16
/*
 * pmap_phys_address:		[ INTERFACE ]
 *
 *	Return the physical address corresponding to the specified
 *	cookie.  Used by the device pager to decode a device driver's
 *	mmap entry point return value.
 *
 *	Note: no locking is necessary in this function.
 */
paddr_t
pmap_phys_address(ppn)
	int ppn;
{
	return(m68k_ptob(ppn));
}

a2101 1
	ste = ST_ENTRY_NULL;
d2263 1
a2264 1
	s = splvm();
d2324 1
a2325 1
	s = splvm();
@


1.31
log
@In pmap_testbit, always update the attribute flags for the pv list hander.

Fixes a regression introduced in the conversion from physseg to VM_PAGE_MD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.30 2003/11/02 13:47:28 miod Exp $ */
d875 1
a875 1
	boolean_t firstpage, needtflush;
d877 3
d892 1
d894 1
d941 1
d943 1
@


1.30
log
@pmap_remove_mapping() uses an unnecessary PHYS_TO_VM_PAGE call. Instead of
simply removing it, slightly reorganize the code to remove a pg_to_pvh() call
as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.29 2003/10/13 18:41:11 miod Exp $ */
d2292 1
a2292 1
	struct pv_entry *pv;
d2319 2
a2320 2
		for (; pv; pv = pv->pv_next) {
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
@


1.29
log
@More m68k pmap tweaks:
- simplify pmap_protect()
- simplify reference count managment by using 0 as a base instead of 1
  (from NetBSD)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.28 2003/10/09 22:12:24 miod Exp $ */
d2000 1
a2000 1
	struct pv_entry *pv, *npv;
d2146 5
a2150 5
		npv = pv->pv_next;
		if (npv) {
			npv->pv_flags = pv->pv_flags;
			*pv = *npv;
			pmap_free_pv(npv);
d2154 3
a2156 2
		for (npv = pv->pv_next; npv; npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
d2158 1
a2158 1
			pv = npv;
d2161 1
a2161 1
		if (npv == NULL)
d2164 4
a2167 6
		ste = npv->pv_ptste;
		ptpmap = npv->pv_ptpmap;
		pv->pv_next = npv->pv_next;
		pmap_free_pv(npv);
		pg = PHYS_TO_VM_PAGE(pa);
		pv = pg_to_pvh(pg);
@


1.28
log
@A few 68060 bugfixes with side effects:
- honor PRM_CFLUSH correctly for 0[46]0 in pmap_remove_mapping().
- be sure to flush caches in pmap_changebit for 060 too when necessary.
- make this compile if option DEBUG is defined.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.27 2003/06/02 23:27:48 millert Exp $ */
d841 5
a845 17
	switch (prot) {
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
		return;
	/* copy_on_write */
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_changebit(pg, PG_RO, ~0);
		return;
	/* remove_all */
	default:
		break;
	}
	pv = pg_to_pvh(pg);
	s = splvm();
	while (pv->pv_pmap != NULL) {
		pt_entry_t *pte;
d847 1
a847 1
		pte = pmap_pte(pv->pv_pmap, pv->pv_va);
d849 3
a851 3
		if (!pmap_ste_v(pv->pv_pmap, pv->pv_va) ||
		    pmap_pte_pa(pte) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_page_protect: bad mapping");
a852 1
		if (!pmap_pte_w(pte))
d854 1
a854 8
					    pte, PRM_TFLUSH|PRM_CFLUSH);
		else {
			pv = pv->pv_next;
			PMAP_DPRINTF(PDB_PARANOIA,
			    ("%s wired mapping for %lx not removed\n",
			     "pmap_page_protect:", pg));
			if (pv == NULL)
				break;
d856 3
a858 2
	}
	splx(s);
d2086 1
a2086 1
		 * If reference count drops to 1, and we're not instructed
d2090 1
a2090 1
		if (refs == 1 && (flags & PRM_KEEPPTPAGE) == 0) {
a2594 1
		pg->wire_count = 1;
d2776 1
a2776 1
	if (pg->wire_count < 1) {
d2786 1
a2786 1
	if ((pg->wire_count - 1) != count)
d2788 1
a2788 1
		       str, va, (pg->wire_count - 1), count);
@


1.27
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.26 2003/06/02 07:06:56 deraadt Exp $ */
d838 1
a838 1
		printf("pmap_page_protect(%lx, %x)\n", pa, prot);
d862 1
a862 1
		    pmap_pte_pa(pte) != pa)
d872 1
a872 1
			     "pmap_page_protect:", pa));
d2066 7
d2419 1
a2419 1
				if (firstpage && (mmutype == MMU_68040) &&
@


1.26
log
@licence cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.25 2003/03/03 23:23:41 miod Exp $ */
d80 1
a80 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.25
log
@Prefer pmap_kenter_pa() in pmap_map().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.24 2003/03/01 00:28:48 miod Exp $ */
a49 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed under OpenBSD by
 *	Theo de Raadt for Willowglen Singapore.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.24
log
@- only declare and provide pmap_map() for mac68k, until it bites the dust.
  Other m68k-based arches do not need it.
- do not wait to allocate struct pv_page. Either pmap_enter() has been
  invoked with PMAP_CANFAIL and can live with this, or we are close to
  fandagoland anyways.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.23 2003/02/25 16:57:24 miod Exp $ */
d2839 1
a2839 1
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
@


1.23
log
@Let pmap_remove_mapping() do its job correctly if option DIAGNOSTIC is
not defined (such as on RAMDISK kernels). Oops.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.22 2003/01/27 19:37:30 miod Exp $ */
d554 2
a555 1
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, PAGE_SIZE);
a609 30
 * pmap_map:
 *
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
 */
vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));

	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, 0);
		va += PAGE_SIZE;
		spa += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	return (va);
}

/*
d1131 1
a1131 1
					panic("pmap_enter: uvm_km_zalloc() failed");
d2134 1
a2134 1
			    NULL, PRM_TFLUSH|PRM_CFLUSH);
d2817 30
@


1.22
log
@Convert m68k pmap from physseg to VM_PAGE_MD.

This allows us to remove some ambiguities on how some functions are called,
remove some diagnostic checks for conditions that can never happen and
remove the ugly hack with "pmap_initialized".

Then, rework internal function interfaces and some logic so as to stop
fetching vm_page from a pa and the reverse every now and then - this makes
some pmap operations run much faster.

per art@@'s idea.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.21 2002/10/12 01:09:43 krw Exp $ */
d2148 1
a2149 1
			pg = PHYS_TO_VM_PAGE(pa);
@


1.21
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.20 2002/09/10 18:29:43 art Exp $ */
a293 1
int		page_cnt;	/* number of pages managed by VM system */
a294 3
boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
struct pv_entry	*pv_table;
char		*pmap_attributes;	/* reference and modify bits */
a311 1
void		 pmap_collect_pv(void);
d316 2
a317 2
boolean_t	 pmap_testbit(paddr_t, int);
void		 pmap_changebit(paddr_t, int, int);
d334 2
a335 2
#define	PAGE_IS_MANAGED(pa) \
	(pmap_initialized && IS_VM_PHYSADDR(pa))
d337 14
a350 15
#define	pa_to_pvh(pa)							\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.pvent[pg_];				\
})

#define	pa_to_attribute(pa)						\
({									\
	int bank_, pg_;							\
									\
	bank_ = vm_physseg_find(atop((pa)), &pg_);			\
	&vm_physmem[bank_].pmseg.attrs[pg_];				\
})
a384 2
	struct pv_entry	*pv;
	char		*attr;
a386 1
	int		bank;
d424 1
a424 2
	 * Allocate memory for random pmap data structures.  Includes the
	 * initial segment table, pv_head_table and pmap_attributes.
d426 1
a426 7
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++)
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
	s = MACHINE_STSIZE;				/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */
	s = round_page(s);
	addr = uvm_km_zalloc(kernel_map, s);
d447 2
a448 23
	pv_table = (struct pv_entry *) addr;
	addr += page_cnt * sizeof(struct pv_entry);

	pmap_attributes = (char *) addr;

	PMAP_DPRINTF(PDB_INIT, ("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
	    "tbl %p atr %p\n",
	    s, page_cnt, Segtabzero, Segtabzeropa,
	    pv_table, pmap_attributes));

	/*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}
a538 5

	/*
	 * Now it is safe to enable pv_table recording.
	 */
	pmap_initialized = TRUE;
a608 64
 * pmap_collect_pv:
 *
 *	Perform compaction on the PV list, called via pmap_collect().
 */
void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;

	TAILQ_INIT(&pv_page_collectlist);

	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp,
			    pvp_pgi.pgi_list);
			pv_nfree -= NPVPPG;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}

	if (pv_page_collectlist.tqh_first == 0)
		return;

	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
		if (ph->pv_pmap == 0)
			continue;
		s = splvm();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp,
					    pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: pgi_nfree inconsistent");
#endif
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}

	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		uvm_km_free(kernel_map, (vaddr_t)pvp, PAGE_SIZE);
	}
}

/*
a870 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d887 1
a887 1
		pmap_changebit(pa, PG_RO, ~0);
d893 1
a893 1
	pv = pa_to_pvh(pa);
d1024 1
d1120 1
a1120 1
	 * Note that we raise IPL while manipulating pv_table
d1123 2
a1124 1
	if (PAGE_IS_MANAGED(pa)) {
d1128 1
a1128 1
		pv = pa_to_pvh(pa);
d1223 1
a1223 1
			*pa_to_attribute(pa) |= (PG_U|PG_M);
d1225 1
a1225 1
			*pa_to_attribute(pa) |= PG_U;
d1233 1
a1233 1
	else if (pmap_initialized) {
d1303 1
a1303 1
		pmap_changebit(pa, PG_CI, ~0);
a1652 5

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
#endif
a1882 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1885 1
a1885 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pa));
d1887 2
a1888 2
	rv = pmap_testbit(pa, PG_M);
	pmap_changebit(pa, 0, ~PG_M);
a1900 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1903 1
a1903 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pa));
d1905 2
a1906 2
	rv = pmap_testbit(pa, PG_U);
	pmap_changebit(pa, 0, ~PG_U);
a1919 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1922 2
a1923 2
		boolean_t rv = pmap_testbit(pa, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pa, "FT"[rv]);
d1927 1
a1927 1
	return(pmap_testbit(pa, PG_U));
a1939 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1942 2
a1943 2
		boolean_t rv = pmap_testbit(pa, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pa, "FT"[rv]);
d1947 1
a1947 1
	return(pmap_testbit(pa, PG_M));
a2048 1
/* static */
d2056 1
d2149 2
a2150 1
			if (PAGE_IS_MANAGED(pa) == 0)
d2152 1
a2152 1
			pv = pa_to_pvh(pa);
d2164 1
a2164 1
			uvm_pagefree(PHYS_TO_VM_PAGE(pa));
d2175 2
a2176 1
	if (PAGE_IS_MANAGED(pa) == 0)
d2184 1
a2184 1
	pv = pa_to_pvh(pa);
d2218 2
a2219 1
		pv = pa_to_pvh(pa);
d2232 1
a2232 1
		pmap_changebit(pa, 0, ~PG_CI);
d2330 1
a2330 1
	*pa_to_attribute(pa) |= bits;
a2338 1
/* static */
d2340 2
a2341 2
pmap_testbit(pa, bit)
	paddr_t pa;
d2348 1
a2348 1
	pv = pa_to_pvh(pa);
d2355 1
a2355 1
	if (*pa_to_attribute(pa) & bit) {
d2374 1
a2374 1
				*pa_to_attribute(pa) |= bit;
a2389 1
/* static */
d2391 2
a2392 2
pmap_changebit(pa, set, mask)
	paddr_t pa;
d2399 3
d2407 1
a2407 1
	    ("pmap_changebit(%lx, %x, %x)\n", pa, set, mask));
d2409 1
a2409 1
	pv = pa_to_pvh(pa);
d2416 1
a2416 1
	*pa_to_attribute(pa) &= mask;
d2456 1
a2474 1
/* static */
d2514 2
a2515 1
					pmap_changebit(stpa, PG_CI, ~PG_CCB);
d2521 4
a2524 1
				pmap_changebit(stpa, 0, ~PG_CCB);
d2609 1
d2615 1
a2615 1
			pmap_changebit(ptpa, PG_CI, ~PG_CCB);
d2673 1
a2673 1
			pmap_changebit(ptpa, PG_CI, ~PG_CCB);
d2677 1
a2677 1
			pmap_changebit(ptpa, 0, ~PG_CCB);
d2685 1
a2685 1
	pv = pa_to_pvh(ptpa);
a2790 1
/* static */
a2811 1
/* static */
@


1.20
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.19 2002/06/02 22:16:52 miod Exp $ */
d422 1
a422 1
		panic("pmap_init: bogons in the VM system!\n");
@


1.19
log
@Implement PMAP_CANFAIL handling for m68k shared pmap module.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.18 2002/04/16 20:49:49 miod Exp $ */
d1883 1
a1883 2
pmap_zero_page(phys)
	paddr_t phys;
d1885 1
d1933 1
a1933 2
pmap_copy_page(src, dst)
	paddr_t src, dst;
d1935 3
@


1.18
log
@Merge pmap_pinit() inside pmap_create() and pmap_release() inside
pmap_destroy().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.17 2002/03/14 01:26:35 millert Exp $ */
d323 1
a323 1
void		 pmap_enter_ptpage(pmap_t, vaddr_t);
d597 2
a598 2
		if (pvp == 0)
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
d1132 1
a1132 1
	int npte;
d1161 9
a1169 2
	if (!pmap_ste_v(pmap, va))
		pmap_enter_ptpage(pmap, va);
d1260 7
d1433 1
a1433 1
	int s, npte;
d1444 3
a1446 1
		pmap_enter_ptpage(pmap, va);
d2583 1
a2583 1
void
d2658 3
a2660 2
			if (ix == -1)
				panic("enter: out of address space"); /* XXX */
d2704 4
a2707 2
			if ((kpt = kpt_free_list) == NULL)
				panic("pmap_enter_ptpage: can't get KPT page");
d2845 1
@


1.17
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.16 2002/01/23 00:39:47 art Exp $ */
a326 2
void		 pmap_pinit(pmap_t);
void		 pmap_release(pmap_t);
a760 18
	pmap_pinit(pmap);
	return (pmap);
}

/*
 * pmap_pinit:
 *
 *	Initialize a preallocated and zeroed pmap structure.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_create()!
 */
void
pmap_pinit(pmap)
	struct pmap *pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW|PDB_CREATE,
	    ("pmap_pinit(%p)\n", pmap));
d776 2
d797 11
a807 1
		pmap_release(pmap);
a809 33
}

/*
 * pmap_release:
 *
 *	Release the resources held by a pmap.
 *
 *	Note: THIS FUNCTION SHOULD BE MOVED INTO pmap_destroy().
 */
void
pmap_release(pmap)
	struct pmap *pmap;
{

	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_release(%p)\n", pmap));

#ifdef notdef /* DIAGNOSTIC */
	/* count would be 0 from pmap_destroy... */
	simple_lock(&pmap->pm_lock);
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif

	if (pmap->pm_ptab) {
		pmap_remove(pmap_kernel(), (vaddr_t)pmap->pm_ptab,
		    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
		pmap_update(pmap_kernel());
		uvm_km_pgremove(uvm.kernel_object, (vaddr_t)pmap->pm_ptab,
		    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
				   MACHINE_MAX_PTSIZE);
	}
	KASSERT(pmap->pm_stab == Segtabzero);
@


1.16
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.15 2002/01/10 21:08:43 miod Exp $ */
d314 3
a316 3
struct pv_entry	*pmap_alloc_pv __P((void));
void		 pmap_free_pv __P((struct pv_entry *));
void		 pmap_collect_pv __P((void));
d318 1
a318 1
int		 pmap_mapmulti __P((pmap_t, vaddr_t));
d320 9
a328 9
void		 pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));
boolean_t	 pmap_testbit __P((paddr_t, int));
void		 pmap_changebit __P((paddr_t, int, int));
void		 pmap_enter_ptpage __P((pmap_t, vaddr_t));
void		 pmap_ptpage_addref __P((vaddr_t));
int		 pmap_ptpage_delref __P((vaddr_t));
void		 pmap_collect1 __P((pmap_t, paddr_t, paddr_t));
void		 pmap_pinit __P((pmap_t));
void		 pmap_release __P((pmap_t));
d332 2
a333 2
void pmap_pvdump	__P((paddr_t));
void pmap_check_wiring	__P((char *, vaddr_t));
@


1.15
log
@Reintroduce the pmap_motorola framework.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.13 2001/12/16 23:06:04 miod Exp $ */
d577 1
a577 1
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_VMPMAP);
@


1.14
log
@Temporarily revert the pmap_motorola changes, as they may account for
some problems as well.
Requested by deraadt@@
@
text
@@


1.13
log
@Delay a cache invalidation in a 68060-only loop.
Also, better tests for validity bits in STEs.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.12 2001/12/16 21:48:48 miod Exp $ */
@


1.13.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.16 2002/01/23 00:39:47 art Exp $ */
d577 1
a577 1
	    &pool_allocator_nointr);
@


1.13.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.13.2.1 2002/01/31 22:55:13 niklas Exp $ */
d314 3
a316 3
struct pv_entry	*pmap_alloc_pv(void);
void		 pmap_free_pv(struct pv_entry *);
void		 pmap_collect_pv(void);
d318 1
a318 1
int		 pmap_mapmulti(pmap_t, vaddr_t);
d320 9
a328 7
void		 pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *, int);
boolean_t	 pmap_testbit(paddr_t, int);
void		 pmap_changebit(paddr_t, int, int);
int		 pmap_enter_ptpage(pmap_t, vaddr_t);
void		 pmap_ptpage_addref(vaddr_t);
int		 pmap_ptpage_delref(vaddr_t);
void		 pmap_collect1(pmap_t, paddr_t, paddr_t);
d332 2
a333 2
void pmap_pvdump(paddr_t);
void pmap_check_wiring(char *, vaddr_t);
d599 2
a600 2
		if (pvp == NULL)
			return NULL;
d763 18
a795 2

	return pmap;
d815 1
a815 11
		if (pmap->pm_ptab) {
			pmap_remove(pmap_kernel(), (vaddr_t)pmap->pm_ptab,
			    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
			pmap_update(pmap_kernel());
			uvm_km_pgremove(uvm.kernel_object,
			    (vaddr_t)pmap->pm_ptab,
			    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
			uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
					   MACHINE_MAX_PTSIZE);
		}
		KASSERT(pmap->pm_stab == Segtabzero);
d821 33
d1173 1
a1173 1
	int npte, error;
d1202 2
a1203 9
	if (!pmap_ste_v(pmap, va)) {
		error = pmap_enter_ptpage(pmap, va);
		if (error != 0) {
			if  (flags & PMAP_CANFAIL)
				return (error);
			else
				panic("pmap_enter: out of address space");
		}
	}
a1293 7
			if (npv == NULL) {
				if (flags & PMAP_CANFAIL) {
					splx(s);
					return (ENOMEM);
				} else
					panic("pmap_enter: uvm_km_zalloc() failed");
			}
d1460 1
a1460 1
	int s, npte, error;
d1471 1
a1471 3
		error = pmap_enter_ptpage(pmap, va);
		if (error != 0)
			panic("pmap_kenter_pa: out of address space");
d2608 1
a2608 1
int
d2683 2
a2684 3
			if (ix == -1) {
				return (ENOMEM);
			}
d2728 2
a2729 4
			if ((kpt = kpt_free_list) == NULL) {
				splx(s);
				return (ENOMEM);
			}
a2866 1
	return (0);
@


1.13.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.13.2.2 2002/06/11 03:36:06 art Exp $ */
d422 1
a422 1
		panic("pmap_init: bogons in the VM system!");
d1883 2
a1884 1
pmap_zero_page(struct vm_page *pg)
a1885 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d1933 2
a1934 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a1935 3
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

@


1.13.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d294 1
d296 3
d316 1
d321 2
a322 2
boolean_t	 pmap_testbit(struct vm_page *, int);
void		 pmap_changebit(struct vm_page *, int, int);
d339 2
a340 2
static struct pv_entry *pa_to_pvh(paddr_t);
static struct pv_entry *pg_to_pvh(struct vm_page *);
d342 15
a356 14
static __inline struct pv_entry *
pa_to_pvh(paddr_t pa)
{
	struct vm_page *pg;

	pg = PHYS_TO_VM_PAGE(pa);
	return &pg->mdpage.pvent;
}

static __inline struct pv_entry *
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}
d391 2
d395 1
d433 2
a434 1
	 * Allocate memory the initial segment table.
d436 7
a442 1
	addr = uvm_km_zalloc(kernel_map, round_page(MACHINE_STSIZE));
d463 23
a485 2
	PMAP_DPRINTF(PDB_INIT, ("pmap_init: s0 %p(%p)\n",
	    Segtabzero, Segtabzeropa));
d576 5
d596 1
a596 2
		pvp = (struct pv_page *)uvm_km_kmemalloc(kernel_map,
		    uvm.kernel_object, PAGE_SIZE, UVM_KMF_NOWAIT);
d651 94
d977 1
d994 1
a994 1
		pmap_changebit(pg, PG_RO, ~0);
d1000 1
a1000 1
	pv = pg_to_pvh(pg);
a1130 1
	struct vm_page *pg;
d1226 1
a1226 1
	 * Note that we raise IPL while manipulating the PV list
d1229 1
a1229 2
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg != NULL) {
d1233 1
a1233 1
		pv = pg_to_pvh(pg);
d1265 1
a1265 1
					panic("pmap_enter: pmap_alloc_pv() failed");
d1328 1
a1328 1
			pv->pv_flags |= (PG_U|PG_M);
d1330 1
a1330 1
			pv->pv_flags |= PG_U;
d1338 1
a1338 1
	else {
d1408 1
a1408 1
		pmap_changebit(pg, PG_CI, ~0);
d1758 5
d1993 1
d1996 1
a1996 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pg));
d1998 2
a1999 2
	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
d2012 1
d2015 1
a2015 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pg));
d2017 2
a2018 2
	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
d2032 1
d2035 2
a2036 2
		boolean_t rv = pmap_testbit(pg, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pg, "FT"[rv]);
d2040 1
a2040 1
	return(pmap_testbit(pg, PG_U));
d2053 1
d2056 2
a2057 2
		boolean_t rv = pmap_testbit(pg, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pg, "FT"[rv]);
d2061 1
a2061 1
	return(pmap_testbit(pg, PG_M));
d2163 1
a2170 1
	struct vm_page *pg;
a2261 1
			pg = PHYS_TO_VM_PAGE(pa);
d2263 1
a2263 1
			if (pg == NULL)
d2265 1
a2265 1
			pv = pg_to_pvh(pg);
d2276 2
a2277 2
			    PT_ENTRY_NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(pg);
d2288 1
a2288 2
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
d2296 1
a2296 1
	pv = pg_to_pvh(pg);
d2330 1
a2330 2
		pg = PHYS_TO_VM_PAGE(pa);
		pv = pg_to_pvh(pg);
d2343 1
a2343 1
		pmap_changebit(pg, 0, ~PG_CI);
d2441 1
a2441 1
	pv->pv_flags |= bits;
d2450 1
d2452 2
a2453 2
pmap_testbit(pg, bit)
	struct vm_page *pg;
d2460 1
a2460 1
	pv = pg_to_pvh(pg);
d2467 1
a2467 1
	if (pv->pv_flags & bit) {
d2486 1
a2486 1
				pv->pv_flags |= bit;
d2502 1
d2504 2
a2505 2
pmap_changebit(pg, set, mask)
	struct vm_page *pg;
a2511 3
#if defined(M68040) || defined(M68060)
	paddr_t pa;
#endif
d2517 1
a2517 1
	    ("pmap_changebit(%lx, %x, %x)\n", pg, set, mask));
d2519 1
a2519 1
	pv = pg_to_pvh(pg);
d2526 1
a2526 1
	pv->pv_flags &= mask;
a2565 1
					pa = VM_PAGE_TO_PHYS(pg);
d2584 1
d2624 1
a2624 2
					pg = PHYS_TO_VM_PAGE(stpa);
					pmap_changebit(pg, PG_CI, ~PG_CCB);
d2630 1
a2630 4
			{
				pg = PHYS_TO_VM_PAGE(stpa);
				pmap_changebit(pg, 0, ~PG_CCB);
			}
a2714 1
		pg = PHYS_TO_VM_PAGE(ptpa);
d2720 1
a2720 1
			pmap_changebit(pg, PG_CI, ~PG_CCB);
d2778 1
a2778 1
			pmap_changebit(pg, PG_CI, ~PG_CCB);
d2782 1
a2782 1
			pmap_changebit(pg, 0, ~PG_CCB);
d2790 1
a2790 1
	pv = pg_to_pvh(pg);
d2896 1
d2918 1
a2952 30
/*
 * pmap_map:
 *
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
 */
vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));

	while (spa < epa) {
		pmap_kenter_pa(va, spa, prot);
		va += PAGE_SIZE;
		spa += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	return (va);
}

@


1.12
log
@68060 CPUs need a few more TLB operations. Oops.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.11 2001/12/15 11:21:08 miod Exp $ */
a531 1
			DCIS();
d535 4
d1887 1
a1887 1
		if (*ste != SG_NV)
d1891 1
a1891 1
		if (*ste != SG_NV)
d2158 1
a2158 1
	if (*ste == SG_NV && (*bste & SG_V)) {
@


1.11
log
@Optimize (reduce) the pmap_changebit() calls for 68060 processors.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.10 2001/12/14 21:44:04 miod Exp $ */
d2850 11
a2860 8
#if 0
	/*
	 * Flush stale TLB info.
	 */
	if (pmap == pmap_kernel())
		TBIAS();
	else
		TBIAU();
@


1.10
log
@Remove the ugly protection_codes[] array, only used by the pte_prot()
macro, by a different version of the aforementioned macro.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.9 2001/12/12 18:36:21 millert Exp $ */
d2644 1
a2644 2
					pmap_changebit(stpa, 0, ~PG_CCB);
					pmap_changebit(stpa, PG_CI, ~0);
d2736 2
a2737 4
		if (mmutype == MMU_68060) {
			pmap_changebit(ptpa, 0, ~PG_CCB);
			pmap_changebit(ptpa, PG_CI, ~0);
		}
a2792 1
		pmap_changebit(ptpa, 0, ~PG_CCB);
d2795 1
a2795 1
			pmap_changebit(ptpa, PG_CI, ~0);
d2797 1
a2797 1
		}
d2799 1
@


1.9
log
@At Miod's request make the pte in pmap_extract() a pt_entry_t * for
consistency with the rest of pmap.  Also, use pmap_pte_pa() in
pmap_extract() instead of doing the equivalent inline.
No functional difference, just style...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.8 2001/12/12 17:30:08 millert Exp $ */
d174 1
d253 1
a253 2
#define pte_prot(m, p)	(protection_codes[p])
int	protection_codes[8];
d341 2
a342 2
#define	PAGE_IS_MANAGED(pa)	(pmap_initialized &&			\
				 vm_physseg_find(atop((pa)), NULL) != -1)
d1090 1
a1090 1
	isro = pte_prot(pmap, prot);
d1385 1
a1385 1
	npte = pa | pte_prot(pmap, prot) | (*pte & (PG_M|PG_U)) | PG_V;
d1489 1
a1489 1
	npte = pa | pte_prot(pmap, prot) | PG_V | PG_W;
@


1.8
log
@Don't return TRUE from pmap_extract if the pte is not valid.
From art@@ as munged by me.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.7 2001/12/11 08:11:33 miod Exp $ */
d1692 1
a1692 1
	u_int pte;
d1698 3
a1700 3
		pte = *(u_int *)pmap_pte(pmap, va);
		if (pmap_pte_v(&pte)) {
			pa = (pte & PG_FRAME) | (va & ~PG_FRAME);
@


1.7
log
@Repair 68060 operation; from art@@ and myself, spell-checked by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.6 2001/12/08 18:55:58 miod Exp $ */
d1699 1
a1699 1
		if (pte) {
@


1.6
log
@m68k_trunc_page -> trunc_page
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.5 2001/12/08 02:24:06 art Exp $ */
d454 5
a458 2
			pmap_changebit(addr2, PG_CCB, 0);
			pmap_changebit(addr2, PG_CI, 1);
d527 5
a531 2
			pmap_changebit(kpt_pages->kpt_pa, PG_CCB, 0);
			pmap_changebit(kpt_pages->kpt_pa, PG_CI, 1);
d1360 1
a1360 1
	 * then it must be device memory which may be volitile.
d2644 2
a2645 2
					pmap_changebit(stpa, PG_CCB, 0);
					pmap_changebit(stpa, PG_CI, 1);
d2738 2
a2739 2
			pmap_changebit(ptpa, PG_CCB, 0);
			pmap_changebit(ptpa, PG_CI, 1);
d2799 1
a2799 1
			pmap_changebit(ptpa, PG_CI, 1);
@


1.5
log
@Sprinkle pmap_update calls where relevant and some other
misc pmap usage fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.4 2001/12/06 01:03:58 miod Exp $ */
d1466 1
a1466 1
	pa = m68k_trunc_page(pa);
@


1.4
log
@Fix typos, from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.3 2001/12/05 00:11:51 millert Exp $ */
d835 1
d2410 1
@


1.3
log
@Update pmap_update macro for arches Art missed.  Still just a noop.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.2 2001/12/02 02:01:52 millert Exp $ */
d114 1
a114 1
 *    	68020 with 68551 MMU
d125 1
a125 1
 *	technically not true for the 68551 but we flush the TLB on every
@


1.3.2.1
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d733 1
a733 1
	pmap_update();
d1771 1
a1771 1
		pmap_update();
d2734 1
a2734 1
		pmap_update();
d2770 1
a2770 1
		pmap_update();
@


1.3.2.2
log
@Merge in trunk
@
text
@d114 1
a114 1
 *    	68020 with 68851 MMU
d125 1
a125 1
 *	technically not true for the 68851 but we flush the TLB on every
a173 1
/* #define UVM_PAGE_INLINE */
d252 2
a253 1
#define pte_prot(p)	((p) & VM_PROT_WRITE ? PG_RW : PG_RO)
d341 2
a342 2
#define	PAGE_IS_MANAGED(pa) \
	(pmap_initialized && IS_VM_PHYSADDR(pa))
d454 2
a455 5
			pt_entry_t *pte;

			pte = pmap_pte(pmap_kernel(), addr2);
			*pte = (*pte | PG_CI) & ~PG_CCB;
			TBIS(addr2);
d524 3
a526 5
			pt_entry_t *pte;

			pte = pmap_pte(pmap_kernel(), addr2);
			*pte = (*pte | PG_CI) & ~PG_CCB;
			TBIS(addr2);
a529 4
#ifdef M68060
	if (mmutype == MMU_68060)
		DCIS();
#endif
d568 1
a568 1
	    &pool_allocator_nointr);
d733 1
a733 1
	pmap_update(pmap_kernel());
a834 1
		pmap_update(pmap_kernel());
d1083 1
a1083 1
	isro = pte_prot(prot);
d1353 1
a1353 1
	 * then it must be device memory which may be volatile.
d1378 1
a1378 1
	npte = pa | pte_prot(prot) | (*pte & (PG_M|PG_U)) | PG_V;
d1465 1
a1465 1
	pa = trunc_page(pa);
d1482 1
a1482 1
	npte = pa | pte_prot(prot) | PG_V | PG_W;
d1685 1
a1685 1
	pt_entry_t *pte;
d1691 3
a1693 3
		pte = pmap_pte(pmap, va);
		if (pmap_pte_v(pte)) {
			pa = pmap_pte_pa(pte) | (va & ~PG_FRAME);
d1771 1
a1771 1
		pmap_update(pmap);
d1877 1
a1877 1
		if (!(*ste & SG_V))
d1881 1
a1881 1
		if (!(*ste & SG_V))
d2148 1
a2148 1
	if (!(*ste & SG_V) && (*bste & SG_V)) {
a2408 1
				pmap_update(pmap_kernel());
d2636 2
a2637 1
					pmap_changebit(stpa, PG_CI, ~PG_CCB);
d2729 4
a2732 2
		if (mmutype == MMU_68060)
			pmap_changebit(ptpa, PG_CI, ~PG_CCB);
d2734 1
a2734 1
		pmap_update(pmap);
d2770 1
a2770 1
		pmap_update(pmap_kernel());
d2788 1
d2791 1
a2791 1
			pmap_changebit(ptpa, PG_CI, ~PG_CCB);
d2793 1
a2793 1
		} else
a2794 1
			pmap_changebit(ptpa, 0, ~PG_CCB);
d2845 8
a2852 11

#if defined(M68060)
	if (mmutype == MMU_68060) {
		/*
		 * Flush stale TLB info.
		 */
		if (pmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
	}
@


1.3.2.3
log
@Merge in -current from about a week ago
@
text
@d314 3
a316 3
struct pv_entry	*pmap_alloc_pv(void);
void		 pmap_free_pv(struct pv_entry *);
void		 pmap_collect_pv(void);
d318 1
a318 1
int		 pmap_mapmulti(pmap_t, vaddr_t);
d320 9
a328 9
void		 pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *, int);
boolean_t	 pmap_testbit(paddr_t, int);
void		 pmap_changebit(paddr_t, int, int);
void		 pmap_enter_ptpage(pmap_t, vaddr_t);
void		 pmap_ptpage_addref(vaddr_t);
int		 pmap_ptpage_delref(vaddr_t);
void		 pmap_collect1(pmap_t, paddr_t, paddr_t);
void		 pmap_pinit(pmap_t);
void		 pmap_release(pmap_t);
d332 2
a333 2
void pmap_pvdump(paddr_t);
void pmap_check_wiring(char *, vaddr_t);
@


1.3.2.4
log
@Sync the SMP branch with 3.3
@
text
@d294 1
d296 3
d316 1
d321 3
a323 3
boolean_t	 pmap_testbit(struct vm_page *, int);
void		 pmap_changebit(struct vm_page *, int, int);
int		 pmap_enter_ptpage(pmap_t, vaddr_t);
d327 2
d341 2
a342 2
static struct pv_entry *pa_to_pvh(paddr_t);
static struct pv_entry *pg_to_pvh(struct vm_page *);
d344 15
a358 14
static __inline struct pv_entry *
pa_to_pvh(paddr_t pa)
{
	struct vm_page *pg;

	pg = PHYS_TO_VM_PAGE(pa);
	return &pg->mdpage.pvent;
}

static __inline struct pv_entry *
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}
d393 2
d397 1
d424 1
a424 1
		panic("pmap_init: bogons in the VM system!");
d435 2
a436 1
	 * Allocate memory the initial segment table.
d438 7
a444 1
	addr = uvm_km_zalloc(kernel_map, round_page(MACHINE_STSIZE));
d465 23
a487 2
	PMAP_DPRINTF(PDB_INIT, ("pmap_init: s0 %p(%p)\n",
	    Segtabzero, Segtabzeropa));
d578 5
d598 3
a600 4
		pvp = (struct pv_page *)uvm_km_kmemalloc(kernel_map,
		    uvm.kernel_object, PAGE_SIZE, UVM_KMF_NOWAIT);
		if (pvp == NULL)
			return NULL;
d653 94
d763 18
a795 2

	return pmap;
d815 1
a815 11
		if (pmap->pm_ptab) {
			pmap_remove(pmap_kernel(), (vaddr_t)pmap->pm_ptab,
			    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
			pmap_update(pmap_kernel());
			uvm_km_pgremove(uvm.kernel_object,
			    (vaddr_t)pmap->pm_ptab,
			    (vaddr_t)pmap->pm_ptab + MACHINE_MAX_PTSIZE);
			uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
					   MACHINE_MAX_PTSIZE);
		}
		KASSERT(pmap->pm_stab == Segtabzero);
d821 33
d1018 1
d1035 1
a1035 1
		pmap_changebit(pg, PG_RO, ~0);
d1041 1
a1041 1
	pv = pg_to_pvh(pg);
a1171 1
	struct vm_page *pg;
d1173 1
a1173 1
	int npte, error;
d1202 2
a1203 9
	if (!pmap_ste_v(pmap, va)) {
		error = pmap_enter_ptpage(pmap, va);
		if (error != 0) {
			if  (flags & PMAP_CANFAIL)
				return (error);
			else
				panic("pmap_enter: out of address space");
		}
	}
d1260 1
a1260 1
	 * Note that we raise IPL while manipulating the PV list
d1263 1
a1263 2
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg != NULL) {
d1267 1
a1267 1
		pv = pg_to_pvh(pg);
a1293 7
			if (npv == NULL) {
				if (flags & PMAP_CANFAIL) {
					splx(s);
					return (ENOMEM);
				} else
					panic("pmap_enter: pmap_alloc_pv() failed");
			}
d1355 1
a1355 1
			pv->pv_flags |= (PG_U|PG_M);
d1357 1
a1357 1
			pv->pv_flags |= PG_U;
d1365 1
a1365 1
	else {
d1435 1
a1435 1
		pmap_changebit(pg, PG_CI, ~0);
d1460 1
a1460 1
	int s, npte, error;
d1471 1
a1471 3
		error = pmap_enter_ptpage(pmap, va);
		if (error != 0)
			panic("pmap_kenter_pa: out of address space");
d1783 5
d1908 2
a1909 1
pmap_zero_page(struct vm_page *pg)
a1910 1
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
d1958 2
a1959 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a1960 3
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);

d2016 1
d2019 1
a2019 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%lx)\n", pg));
d2021 2
a2022 2
	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
d2035 1
d2038 1
a2038 1
	PMAP_DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%lx)\n", pg));
d2040 2
a2041 2
	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
d2055 1
d2058 2
a2059 2
		boolean_t rv = pmap_testbit(pg, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pg, "FT"[rv]);
d2063 1
a2063 1
	return(pmap_testbit(pg, PG_U));
d2076 1
d2079 2
a2080 2
		boolean_t rv = pmap_testbit(pg, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pg, "FT"[rv]);
d2084 1
a2084 1
	return(pmap_testbit(pg, PG_M));
d2186 1
a2193 1
	struct vm_page *pg;
a2284 1
			pg = PHYS_TO_VM_PAGE(pa);
d2286 1
a2286 1
			if (pg == NULL)
d2288 1
a2288 1
			pv = pg_to_pvh(pg);
d2299 2
a2300 2
			    PT_ENTRY_NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(pg);
d2311 1
a2311 2
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
d2319 1
a2319 1
	pv = pg_to_pvh(pg);
d2353 1
a2353 2
		pg = PHYS_TO_VM_PAGE(pa);
		pv = pg_to_pvh(pg);
d2366 1
a2366 1
		pmap_changebit(pg, 0, ~PG_CI);
d2464 1
a2464 1
	pv->pv_flags |= bits;
d2473 1
d2475 2
a2476 2
pmap_testbit(pg, bit)
	struct vm_page *pg;
d2483 1
a2483 1
	pv = pg_to_pvh(pg);
d2490 1
a2490 1
	if (pv->pv_flags & bit) {
d2509 1
a2509 1
				pv->pv_flags |= bit;
d2525 1
d2527 2
a2528 2
pmap_changebit(pg, set, mask)
	struct vm_page *pg;
a2534 3
#if defined(M68040) || defined(M68060)
	paddr_t pa;
#endif
d2540 1
a2540 1
	    ("pmap_changebit(%lx, %x, %x)\n", pg, set, mask));
d2542 1
a2542 1
	pv = pg_to_pvh(pg);
d2549 1
a2549 1
	pv->pv_flags &= mask;
a2588 1
					pa = VM_PAGE_TO_PHYS(pg);
d2607 2
a2608 1
int
d2647 1
a2647 2
					pg = PHYS_TO_VM_PAGE(stpa);
					pmap_changebit(pg, PG_CI, ~PG_CCB);
d2653 1
a2653 4
			{
				pg = PHYS_TO_VM_PAGE(stpa);
				pmap_changebit(pg, 0, ~PG_CCB);
			}
d2683 2
a2684 3
			if (ix == -1) {
				return (ENOMEM);
			}
d2728 2
a2729 4
			if ((kpt = kpt_free_list) == NULL) {
				splx(s);
				return (ENOMEM);
			}
a2734 1
		pg = PHYS_TO_VM_PAGE(ptpa);
d2740 1
a2740 1
			pmap_changebit(pg, PG_CI, ~PG_CCB);
d2798 1
a2798 1
			pmap_changebit(pg, PG_CI, ~PG_CCB);
d2802 1
a2802 1
			pmap_changebit(pg, 0, ~PG_CCB);
d2810 1
a2810 1
	pv = pg_to_pvh(pg);
a2866 1
	return (0);
d2915 1
d2937 1
a2971 30
/*
 * pmap_map:
 *
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 *
 *	Note: THIS FUNCTION IS DEPRECATED, AND SHOULD BE REMOVED!
 */
vaddr_t
pmap_map(va, spa, epa, prot)
	vaddr_t va;
	paddr_t spa, epa;
	int prot;
{

	PMAP_DPRINTF(PDB_FOLLOW,
	    ("pmap_map(%lx, %lx, %lx, %x)\n", va, spa, epa, prot));

	while (spa < epa) {
		pmap_kenter_pa(va, spa, prot);
		va += PAGE_SIZE;
		spa += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	return (va);
}

@


1.3.2.5
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.3.2.4 2003/03/27 23:28:43 niklas Exp $ */
d50 6
d86 5
a90 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.3.2.6
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d838 1
a838 1
		printf("pmap_page_protect(%lx, %x)\n", pg, prot);
d841 17
a857 5
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pv = pg_to_pvh(pg);
		s = splvm();
		while (pv->pv_pmap != NULL) {
			pt_entry_t *pte;
d859 1
a859 1
			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
d861 3
a863 3
			if (!pmap_ste_v(pv->pv_pmap, pv->pv_va) ||
			    pmap_pte_pa(pte) != VM_PAGE_TO_PHYS(pg))
				panic("pmap_page_protect: bad mapping");
d865 1
d867 8
a874 1
			    pte, PRM_TFLUSH|PRM_CFLUSH);
d876 2
a877 3
		splx(s);
	} else if ((prot & VM_PROT_WRITE) == VM_PROT_NONE)
		pmap_changebit(pg, PG_RO, ~0);
d894 1
a894 1
	boolean_t needtflush;
a895 3
#ifdef M68K_MMU_HP
	boolean_t firstpage;
#endif
a907 1
#ifdef M68K_MMU_HP
a908 1
#endif
a954 1
#ifdef M68K_MMU_HP
a955 1
#endif
d1552 23
d1911 16
d2019 1
a2019 1
	struct pv_entry *pv, *prev, *cur;
a2065 7
#if defined(M68040) || defined(M68060)
	if ((mmutype <= MMU_68040) && (flags & PRM_CFLUSH)) {
		DCFP(pa);
		ICPP(pa);
	}
#endif

d2098 1
a2098 1
		 * If reference count drops to zero, and we're not instructed
d2102 1
a2102 1
		if (refs == 0 && (flags & PRM_KEEPPTPAGE) == 0) {
d2146 1
d2158 5
a2162 5
		cur = pv->pv_next;
		if (cur != NULL) {
			cur->pv_flags = pv->pv_flags;
			*pv = *cur;
			pmap_free_pv(cur);
d2166 2
a2167 3
		prev = pv;
		for (cur = pv->pv_next; cur != NULL; cur = cur->pv_next) {
			if (pmap == cur->pv_pmap && va == cur->pv_va)
d2169 1
a2169 1
			prev = cur;
d2172 1
a2172 1
		if (cur == NULL)
d2175 6
a2180 4
		ste = cur->pv_ptste;
		ptpmap = cur->pv_ptpmap;
		prev->pv_next = cur->pv_next;
		pmap_free_pv(cur);
d2305 1
a2305 1
	struct pv_entry *pv, *pvl;
d2309 1
a2310 1
	pv = pg_to_pvh(pg);
d2332 2
a2333 2
		for (pvl = pv; pvl != NULL; pvl = pvl->pv_next) {
			pte = pmap_pte(pvl->pv_pmap, pvl->pv_va);
d2370 1
a2371 1
	pv = pg_to_pvh(pg);
d2412 1
a2412 1
				if (firstpage && (mmutype <= MMU_68040) &&
d2607 1
d2789 1
a2789 1
	if (pg->wire_count >= PAGE_SIZE / sizeof(struct pt_entry_t)) {
d2799 1
a2799 1
	if (pg->wire_count != count)
d2801 1
a2801 1
		       str, va, pg->wire_count, count);
@


1.3.2.7
log
@Merge with the trunk
@
text
@a2698 9
void
pmap_proc_iflush(p, va, len)
	struct proc	*p;
	vaddr_t		va;
	vsize_t		len;
{
	(void)cachectl(p, 0x80000004, va, len);
}

@


1.2
log
@1) kill old vm_*_t types (no real effect)
2) Change flag in uvm_km_suballoc() from VM_MAP_PAGEABLE to 0
3) Pass pmap_extract the address of Segtabzeropa, not Segtabzeropa itself
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap_motorola.c,v 1.1 2001/11/30 20:54:52 miod Exp $ */
d733 1
a733 1
	pmap_update();
d1771 1
a1771 1
		pmap_update();
d2734 1
a2734 1
		pmap_update();
d2770 1
a2770 1
		pmap_update();
@


1.1
log
@Common pmap defines and code for all m68k platforms using Motorola (or
compatible) MMU.
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d214 1
a214 1
	(&((m)->pm_stab[(vm_offset_t)(v) >> SG4_SHIFT1]))
d220 1
a220 1
	(&((m)->pm_stab[(vm_offset_t)(v) \
d228 1
a228 1
#define	pmap_ste(m, v)	 (&((m)->pm_stab[(vm_offset_t)(v) >> SG_ISHIFT]))
d232 1
a232 1
#define pmap_pte(m, v)	(&((m)->pm_ptab[(vm_offset_t)(v) >> PG_SHIFT]))
d278 1
a278 1
vm_size_t	Sysptsize = VM_KERNEL_PT_PAGES;
d289 5
a293 5
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
vm_size_t	mem_size;	/* memory size in bytes */
vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
d449 1
a449 1
	pmap_extract(pmap_kernel(), addr, (paddr_t *)Segtabzeropa);
d553 1
a553 1
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, VM_MAP_PAGEABLE,
d2718 1
a2718 1
			if ((kpt = kpt_free_list) == (struct kpt_page *)0)
d2963 2
a2964 2
	vm_offset_t va;
	vm_offset_t pge;
d2966 1
a2966 1
extern	vm_offset_t tmp_vpages[];
@

