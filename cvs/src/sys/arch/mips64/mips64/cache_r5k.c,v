head	1.15;
access;
symbols
	OPENBSD_6_1_BASE:1.15
	OPENBSD_6_0:1.15.0.4
	OPENBSD_6_0_BASE:1.15
	OPENBSD_5_9:1.15.0.2
	OPENBSD_5_9_BASE:1.15
	OPENBSD_5_8:1.14.0.8
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.14.0.2
	OPENBSD_5_7_BASE:1.14
	OPENBSD_5_6:1.14.0.4
	OPENBSD_5_6_BASE:1.14
	OPENBSD_5_5:1.9.0.4
	OPENBSD_5_5_BASE:1.9
	OPENBSD_5_4:1.8.0.4
	OPENBSD_5_4_BASE:1.8
	OPENBSD_5_3:1.8.0.2
	OPENBSD_5_3_BASE:1.8
	OPENBSD_5_2:1.3.0.2
	OPENBSD_5_2_BASE:1.3;
locks; strict;
comment	@ * @;


1.15
date	2016.01.05.05.27.54;	author visa;	state Exp;
branches;
next	1.14;
commitid	916i76I5mjNtTg33;

1.14
date	2014.06.17.18.58.35;	author miod;	state Exp;
branches;
next	1.13;
commitid	caxL3Eq4NCX7zjD9;

1.13
date	2014.03.31.20.21.19;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2014.03.29.18.09.30;	author guenther;	state Exp;
branches;
next	1.11;

1.11
date	2014.03.11.20.32.42;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2014.03.09.10.12.17;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2013.11.26.20.33.13;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	2012.10.03.11.18.23;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2012.09.29.19.24.31;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2012.09.29.19.13.15;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2012.09.29.19.02.26;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2012.09.29.18.54.38;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2012.06.24.20.24.46;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2012.06.24.16.26.04;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2012.06.23.21.56.06;	author miod;	state Exp;
branches;
next	;


desc
@@


1.15
log
@Some implementations of HitSyncDCache() call pmap_extract() for va->pa
conversion. Because pmap_extract() acquires the PTE mutex, a "locking
against myself" panic is triggered if the cache routine gets called in
a context where the mutex is already held.

In the pmap, all calls to HitSyncDCache() are for a whole page. Add a
new cache routine, HitSyncDCachePage(), which gets both the va and the
pa of a page. This removes the need of the va->pa conversion. The new
routine has the same signature as SyncDCachePage(), allowing reuse of
the same routine for cache implementations that do not need differences
between "Hit" and non-"Hit" routines.

With the diff, POWER Indigo2 R8000 boots multiuser again. Tested on sgi
GENERIC-IP27.MP and octeon GENERIC.MP, too.

Diff from miod@@, ok kettenis@@
@
text
@/*	$OpenBSD: cache_r5k.c,v 1.14 2014/06/17 18:58:35 miod Exp $	*/

/*
 * Copyright (c) 2012 Miodrag Vallat.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * Copyright (c) 1998-2004 Opsycon AB (www.opsycon.se)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

/*
 * Cache handling for R5000 processor and close relatives:
 * R4600/R4700, R5000, RM52xx, RM7xxx, RM9xxx.
 *
 * The following assumptions are made:
 * - L1 I$ is 2-way or 4-way (RM7k/RM9k), VIPT, 32 bytes/line, up to 32KB
 * - L1 D$ is 2-way or 4-way (RM7k/RM9k), VIPT, write-back, 32 bytes/line,
 *   up to 32KB
 * - `internal' L2 on RM7k/RM9k is 4-way, PIPT, write-back, 32 bytes/line,
 *   256KB total
 * - L3 (on RM7k/RM9k) or `external' L2 (all others) may not exist. If it
 *   does, it is direct-mapped, PIPT, write-through
 */

#include <sys/param.h>
#include <sys/systm.h>

#include <mips64/cache.h>
#include <machine/cpu.h>
#include <mips64/mips_cpu.h>

#include <uvm/uvm_extern.h>

#define	IndexInvalidate_I	0x00
#define	IndexWBInvalidate_D	0x01
#define	IndexWBInvalidate_S	0x03

#define	IndexStoreTag_S		0x0b

#define	HitInvalidate_D		0x11
#define	HitInvalidate_S		0x13

#define	HitWBInvalidate_D	0x15
#define	InvalidatePage_T	0x16	/* Only RM7k/RM9k */
#define	HitWBInvalidate_S	0x17
#define	InvalidatePage_S	0x17	/* Only RM527[0-1] */

#define	HitWriteback_D		0x19
#define	HitWriteback_S		0x1b

/*
 *  R5000 and RM52xx config register bits.
 */
#define	CF_5_SE		(1U << 12)	/* Secondary cache enable */
#define	CF_5_SC		(1U << 17)	/* Secondary cache not present */
#define	CF_5_SS		(3U << 20)	/* Secondary cache size */
#define	CF_5_SS_AL	20		/* Shift to align */

/*
 *  RM7000 config register bits.
 */
#define	CF_7_SE		(1U << 3)	/* Secondary cache enable */
#define	CF_7_SC		(1U << 31)	/* Secondary cache not present */
#define	CF_7_TE		(1U << 12)	/* Tertiary cache enable */
#define	CF_7_TC		(1U << 17)	/* Tertiary cache not present */
#define	CF_7_TS		(3U << 20)	/* Tertiary cache size */
#define	CF_7_TS_AL	20		/* Shift to align */


#define	R5K_LINE	32UL		/* internal cache line */
#define	R5K_PAGE	4096UL		/* external cache page */


/*
 * Cache configuration
 */
#define	CTYPE_HAS_IL2		0x01	/* Internal L2 Cache present */
#define	CTYPE_HAS_XL2		0x02	/* External L2 Cache present */
#define	CTYPE_HAS_XL3		0x04	/* External L3 Cache present */

#define	nop4()			__asm__ volatile \
	("nop; nop; nop; nop")
#define	nop10()			__asm__ volatile \
	("nop; nop; nop; nop; nop; nop; nop; nop; nop; nop")

#define	cache(op,offs,addr)	__asm__ volatile \
	("cache %0, %1(%2)" :: "i"(op), "i"(offs), "r"(addr) : "memory")

#define	reset_taglo()		__asm__ volatile \
	("mtc0 $zero, $28")	/* COP_0_TAG_LO */
#define	reset_taghi()		__asm__ volatile \
	("mtc0 $zero, $29")	/* COP_0_TAG_HI */

static __inline__ register_t
get_config(void)
{
	register_t cfg;
	__asm__ volatile ("mfc0 %0, $16" : "=r"(cfg)); /* COP_0_CONFIG */
	return cfg;
}

static __inline__ void
set_config(register_t cfg)
{
	__asm__ volatile ("mtc0 %0, $16" :: "r"(cfg)); /* COP_0_CONFIG */
	/* MTC0_HAZARD */
#ifdef CPU_RM7000
	nop10();
#else
	nop4();
#endif
}

static __inline__ void	mips5k_hitinv_primary(vaddr_t, vsize_t);
static __inline__ void	mips5k_hitinv_secondary(vaddr_t, vsize_t);
static __inline__ void	mips5k_hitwbinv_primary(vaddr_t, vsize_t);
static __inline__ void	mips5k_hitwbinv_secondary(vaddr_t, vsize_t);
static __inline__ void	mips5k_hitwb_primary(vaddr_t, vsize_t);
#if 0
static __inline__ void	mips5k_hitwb_secondary(vaddr_t, vsize_t);
#endif

void mips5k_l2_init(register_t);
void mips7k_l2_init(register_t);
void mips7k_l3_init(register_t);
void mips5k_c0_cca_update(register_t);
static void run_uncached(void (*)(register_t), register_t);

/*
 * Invoke a simple routine from uncached space (either CKSEG1 or uncached
 * XKPHYS).
 */

static void
run_uncached(void (*fn)(register_t), register_t arg)
{
	vaddr_t va;
	paddr_t pa;

	va = (vaddr_t)fn;
	if (IS_XKPHYS(va)) {
		pa = XKPHYS_TO_PHYS(va);
		va = PHYS_TO_XKPHYS(pa, CCA_NC);
	} else {
		pa = CKSEG0_TO_PHYS(va);
		va = PHYS_TO_CKSEG1(pa);
	}
	fn = (void (*)(register_t))va;

	(*fn)(arg);
}


/*
 * Initialize the external L2 cache of an R5000 (or close relative) processor.
 * INTENDED TO BE RUN UNCACHED - BE SURE TO CHECK THAT IT WON'T STORE ANYTHING
 * ON THE STACK IN THE ASSEMBLY OUTPUT EVERYTIME YOU CHANGE IT.
 */
void
mips5k_l2_init(register_t l2size)
{
	register vaddr_t va, eva;
	register register_t cfg;

	cfg = get_config();
	cfg |= CF_5_SE;
	set_config(cfg);

	reset_taglo();
	reset_taghi();

	va = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = va + l2size;
	while (va != eva) {
		cache(InvalidatePage_S, 0, va);
		va += R5K_PAGE;
	}
}

/*
 * Initialize the internal L2 cache of an RM7000 (or close relative) processor.
 * INTENDED TO BE RUN UNCACHED - BE SURE TO CHECK THAT IT WON'T STORE ANYTHING
 * ON THE STACK IN THE ASSEMBLY OUTPUT EVERYTIME YOU CHANGE IT.
 */
void
mips7k_l2_init(register_t l2size)
{
	register vaddr_t va, eva;
	register register_t cfg;

	cfg = get_config();
	cfg |= CF_7_SE;
	set_config(cfg);

	reset_taglo();
	reset_taghi();

	va = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = va + l2size;
	while (va != eva) {
		cache(IndexStoreTag_S, 0, va);
		va += R5K_LINE;
	}
	mips_sync();

	va = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = va + l2size;
	while (va != eva) {
		__asm__ volatile
		    ("lw $zero, 0(%0)" :: "r"(va));
		va += R5K_LINE;
	}
	mips_sync();

	va = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = va + l2size;
	while (va != eva) {
		cache(IndexStoreTag_S, 0, va);
		va += R5K_LINE;
	}
	mips_sync();
}

/*
 * Initialize the external L3 cache of an RM7000 (or close relative) processor.
 * INTENDED TO BE RUN UNCACHED - BE SURE TO CHECK THAT IT WON'T STORE ANYTHING
 * ON THE STACK IN THE ASSEMBLY OUTPUT EVERYTIME YOU CHANGE IT.
 */
void
mips7k_l3_init(register_t l3size)
{
	register vaddr_t va, eva;
	register register_t cfg;

	cfg = get_config();
	cfg |= CF_7_TE;
	set_config(cfg);

	reset_taglo();
	reset_taghi();

	va = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = va + l3size;
	while (va != eva) {
		cache(InvalidatePage_T, 0, va);
		va += R5K_PAGE;
	}
}

/*
 * Update the coherency of KSEG0.
 * INTENDED TO BE RUN UNCACHED - BE SURE TO CHECK THAT IT WON'T STORE ANYTHING
 * ON THE STACK IN THE ASSEMBLY OUTPUT EVERYTIME YOU CHANGE IT.
 */
void
mips5k_c0_cca_update(register_t cfg)
{
	set_config(cfg);

#if defined(CPU_R5000) || defined(CPU_RM7000)
	/*
	 * RM52xx and RM7000 hazard: after updating the K0 field of the Config
	 * register, the KSEG0 and CKSEG0 address segments should not be used
	 * for 5 cycles. The register modification and the use of these address
	 * segments should be separated by at least five (RM52xx) or
	 * ten (RM7000) integer instructions.
	 */
	nop10();
#endif
}

/*
 * Discover cache configuration and update cpu_info accordingly.
 * Initialize L2 and L3 caches found if necessary.
 */
void
Mips5k_ConfigCache(struct cpu_info *ci)
{
	register_t cfg, ncfg;
	uint setshift;

	cfg = cp0_get_config();

	/* L1 cache */
	ci->ci_l1inst.size = (1 << 12) << ((cfg >> 9) & 0x07); /* IC */
	ci->ci_l1inst.linesize = R5K_LINE;
	ci->ci_l1data.size = (1 << 12) << ((cfg >> 6) & 0x07); /* DC */
	ci->ci_l1data.linesize = R5K_LINE;

	/* sane defaults */
	setshift = 1;
	memset(&ci->ci_l2, 0, sizeof(struct cache_info));
	memset(&ci->ci_l3, 0, sizeof(struct cache_info));
	ci->ci_cacheconfiguration = 0;

	switch ((cp0_get_prid() >> 8) & 0xff) {
	default:
		/* shouldn't happen, really; but we can't panic here */
		break;
#ifdef CPU_R4600
	case MIPS_R4600:
	case MIPS_R4700:
		/* no external L2 interface */
		break;
#endif
#ifdef CPU_R5000
	case MIPS_R5000:
	case MIPS_RM52X0:
		/* optional external direct L2 cache */
		if ((cfg & CF_5_SC) == 0) {
			ci->ci_l2.size = (1 << 19) <<
			    ((cfg & CF_5_SS) >> CF_5_SS_AL);
			ci->ci_l2.linesize = R5K_LINE;
			ci->ci_l2.setsize = ci->ci_l2.size;
			ci->ci_l2.sets = 1;
		}
		if (ci->ci_l2.size != 0) {
			ci->ci_cacheconfiguration |= CTYPE_HAS_XL2;
			cfg |= CF_5_SE;
			run_uncached(mips5k_l2_init, ci->ci_l2.size);
		}
		break;
#endif	/* CPU_R5000 */
#ifdef CPU_RM7000
	case MIPS_RM7000:
	case MIPS_RM9000:
		setshift = 2;
		/* optional external direct L3 cache */
		if ((cfg & CF_7_TC) == 0) {
#ifndef L3SZEXT
			/*
			 * Assume L3 size is provided in the system information
			 * field of the Config register. This is usually the
			 * case on systems where the RM7k/RM9k processor is
			 * an upgrade from an R5000/RM52xx processor, such as
			 * the SGI O2.
			 */
			ci->ci_l3.size = (1 << 19) <<
			    ((cfg & CF_7_TS) >> CF_7_TS_AL);
			ci->ci_l3.linesize = R5K_LINE;
			ci->ci_l3.setsize = ci->ci_l3.size;
			ci->ci_l3.sets = 1;
#else
			/*
			 * Assume machdep has initialized ci_l3 for us.
			 */
#endif
		}
		if (ci->ci_l3.size != 0) {
			ci->ci_cacheconfiguration |= CTYPE_HAS_XL3;
			cfg |= CF_7_TE;
			run_uncached(mips7k_l3_init, ci->ci_l3.size);

		}
		/* internal 4-way L2 cache */
		if ((cfg & CF_7_SC) == 0) {
			ci->ci_l2.size = 256 * 1024;	/* fixed size */
			ci->ci_l2.linesize = R5K_LINE;
			ci->ci_l2.setsize = ci->ci_l2.size / 4;
			ci->ci_l2.sets = 4;
		}
		if (ci->ci_l2.size != 0) {
			ci->ci_cacheconfiguration |= CTYPE_HAS_IL2;
			if ((cfg & CF_7_SE) == 0) {
				cfg |= CF_7_SE;
				run_uncached(mips7k_l2_init, ci->ci_l2.size);
			}
		}
		break;
#endif	/* CPU_RM7000 */
	}

	ci->ci_l1inst.setsize = ci->ci_l1inst.size >> setshift;
	ci->ci_l1inst.sets = setshift == 2 ? 4 : 2;
	ci->ci_l1data.setsize = ci->ci_l1data.size >> setshift;
	ci->ci_l1data.sets = setshift == 2 ? 4 : 2;

	cache_valias_mask =
	    (max(ci->ci_l1inst.setsize, ci->ci_l1data.setsize) - 1) &
	    ~PAGE_MASK;

	if (cache_valias_mask != 0) {
		cache_valias_mask |= PAGE_MASK;
		pmap_prefer_mask = cache_valias_mask;
	}

	ci->ci_SyncCache = Mips5k_SyncCache;
	ci->ci_InvalidateICache = Mips5k_InvalidateICache;
	ci->ci_InvalidateICachePage = Mips5k_InvalidateICachePage;
	ci->ci_SyncICache = Mips5k_SyncICache;
	ci->ci_SyncDCachePage = Mips5k_SyncDCachePage;
	ci->ci_HitSyncDCachePage = Mips5k_HitSyncDCachePage;
	ci->ci_HitSyncDCache = Mips5k_HitSyncDCache;
	ci->ci_HitInvalidateDCache = Mips5k_HitInvalidateDCache;
	ci->ci_IOSyncDCache = Mips5k_IOSyncDCache;

	ncfg = (cfg & ~CFGR_CCA_MASK) | CCA_CACHED;
	if (cfg != ncfg)
		run_uncached(mips5k_c0_cca_update, ncfg);
}

/*
 * Writeback and invalidate all caches.
 */
void
Mips5k_SyncCache(struct cpu_info *ci)
{
	vaddr_t sva, eva;
#ifdef CPU_R4600
	/*
	 * Revision 1 R4600 need to perform `Index' cache operations with
	 * interrupt disabled, to make sure both ways are correctly updated.
	 */
	register_t sr = disableintr();
#endif

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = sva + ci->ci_l1inst.size;
	while (sva != eva) {
		cache(IndexInvalidate_I, 0, sva);
		sva += R5K_LINE;
	}

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = sva + ci->ci_l1data.size;
	while (sva != eva) {
		cache(IndexWBInvalidate_D, 0, sva);
		sva += R5K_LINE;
	}

#ifdef CPU_R4600
	setsr(sr);
#endif

#ifdef CPU_RM7000
	if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2) {
		sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
		eva = sva + ci->ci_l2.size;
		while (sva != eva) {
			cache(IndexWBInvalidate_S, 0, sva);
			sva += R5K_LINE;
		}
	} else
#endif
#ifdef CPU_R5000
	if (ci->ci_cacheconfiguration & CTYPE_HAS_XL2) {
		sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
		eva = sva + ci->ci_l2.size;
		reset_taglo();
		while (sva != eva) {
			cache(InvalidatePage_S, 0, sva);
			sva += R5K_PAGE;
		}
	} else
#endif
	{
	}

#ifdef CPU_RM7000
	if (ci->ci_cacheconfiguration & CTYPE_HAS_XL3) {
		sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
		eva = sva + ci->ci_l3.size;
		reset_taglo();
		while (sva != eva) {
			cache(InvalidatePage_T, 0, sva);
			sva += R5K_PAGE;
		}
	}
#endif

	mips_sync();
}

/*
 * Invalidate I$ for the given range.
 */
void
Mips5k_InvalidateICache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va, sva, eva, iva;
	vsize_t sz, offs;
#ifdef CPU_R4600
	/*
	 * Revision 1 R4600 need to perform `Index' cache operations with
	 * interrupt disabled, to make sure both ways are correctly updated.
	 */
	register_t sr = disableintr();
#endif

	/* extend the range to integral cache lines */
	va = _va & ~(R5K_LINE - 1);
	sz = ((_va + _sz + R5K_LINE - 1) & ~(R5K_LINE - 1)) - va;

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	offs = ci->ci_l1inst.setsize;
	/* keep only the index bits */
	sva |= va & (offs - 1);
	eva = sva + sz;

	switch (ci->ci_l1inst.sets) {
	default:
#ifdef CPU_RM7000
	case 4:
		while (sva != eva) {
			iva = sva;
			cache(IndexInvalidate_I, 0, iva);
			iva += offs;
			cache(IndexInvalidate_I, 0, iva);
			iva += offs;
			cache(IndexInvalidate_I, 0, iva);
			iva += offs;
			cache(IndexInvalidate_I, 0, iva);
			sva += R5K_LINE;
		}
		break;
#endif
#if defined(CPU_R5000) || defined(CPU_R4600)
	case 2:
		iva = sva + offs;
		while (sva != eva) {
			cache(IndexInvalidate_I, 0, iva);
			cache(IndexInvalidate_I, 0, sva);
			iva += R5K_LINE;
			sva += R5K_LINE;
		}
		break;
#endif
	}

#ifdef CPU_R4600
	setsr(sr);
#endif
	mips_sync();
}

/*
 * Register a given page for I$ invalidation.
 */
void
Mips5k_InvalidateICachePage(struct cpu_info *ci, vaddr_t va)
{
	/*
	 * Mark the particular page index bits as needing to be flushed.
	 */
	ci->ci_cachepending_l1i |=
	    1ULL << ((va & (ci->ci_l1inst.setsize - 1)) >> PAGE_SHIFT);
}

/*
 * Perform postponed I$ invalidation.
 */
void
Mips5k_SyncICache(struct cpu_info *ci)
{
	vaddr_t sva, eva, iva;
	vsize_t offs;
	uint64_t idx;
#ifdef CPU_R4600
	register_t sr;
#endif

	if (ci->ci_cachepending_l1i == 0)
		return;

#ifdef CPU_R4600
	/*
	 * Revision 1 R4600 need to perform `Index' cache operations with
	 * interrupt disabled, to make sure both ways are correctly updated.
	 */
	sr = disableintr();
#endif

	offs = ci->ci_l1inst.setsize;

	if (ci->ci_l1inst.setsize <= PAGE_SIZE) {
		ci->ci_cachepending_l1i = 0;
		sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
		eva = sva + offs;
		switch (ci->ci_l1inst.sets) {
		default:
#ifdef CPU_RM7000
		case 4:
			while (sva != eva) {
				iva = sva;
				cache(IndexInvalidate_I, 0, iva);
				iva += offs;
				cache(IndexInvalidate_I, 0, iva);
				iva += offs;
				cache(IndexInvalidate_I, 0, iva);
				iva += offs;
				cache(IndexInvalidate_I, 0, iva);
				sva += R5K_LINE;
			}
			break;
#endif
#if defined(CPU_R5000) || defined(CPU_R4600)
		case 2:
			iva = sva + offs;
			while (sva != eva) {
				cache(IndexInvalidate_I, 0, iva);
				cache(IndexInvalidate_I, 0, sva);
				iva += R5K_LINE;
				sva += R5K_LINE;
			}
			break;
#endif
		}
	} else {
		/*
		 * Iterate on all pending page index bits.
		 */
		for (idx = 0; ci->ci_cachepending_l1i != 0; idx++) {
			if ((ci->ci_cachepending_l1i & (1ULL << idx)) == 0)
				continue;

			sva = PHYS_TO_XKPHYS(idx << PAGE_SHIFT, CCA_CACHED);
			eva = sva + offs;
			switch (ci->ci_l1inst.sets) {
			default:
#ifdef CPU_RM7000
			case 4:
				while (sva != eva) {
					iva = sva;
					cache(IndexInvalidate_I, 0, iva);
					iva += offs;
					cache(IndexInvalidate_I, 0, iva);
					iva += offs;
					cache(IndexInvalidate_I, 0, iva);
					iva += offs;
					cache(IndexInvalidate_I, 0, iva);
					sva += R5K_LINE;
				}
				break;
#endif
#if defined(CPU_R5000) || defined(CPU_R4600)
			case 2:
				iva = sva + offs;
				while (sva != eva) {
					cache(IndexInvalidate_I, 0, iva);
					cache(IndexInvalidate_I, 0, sva);
					iva += R5K_LINE;
					sva += R5K_LINE;
				}
				break;
#endif
			}

			ci->ci_cachepending_l1i &= ~(1ULL << idx);
		}
	}

#ifdef CPU_R4600
	setsr(sr);
#endif
}

static __inline__ void
mips5k_hitwbinv_primary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWBInvalidate_D, 0, va);
		va += R5K_LINE;
	}
}

static __inline__ void
mips5k_hitwbinv_secondary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWBInvalidate_S, 0, va);
		cache(HitWBInvalidate_D, 0, va);	/* orphans in L1 */
		va += R5K_LINE;
	}
}

/*
 * Writeback D$ for the given page.
 */
void
Mips5k_SyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
{
#ifdef CPU_R4600
	/*
	 * Revision 1 R4600 need to perform `Index' cache operations with
	 * interrupt disabled, to make sure both ways are correctly updated.
	 */
	register_t sr = disableintr();
#endif

	switch (ci->ci_l1data.sets) {
	default:
#ifdef CPU_RM7000
	case 4:
		/*
		 * On RM7000 and RM9000, the D$ cache set is never larger than
		 * the page size, causing it to behave as a physically-indexed
		 * cache. We can thus use Hit operations on the physical
		 * address.
		 */
		if ((ci->ci_cacheconfiguration & CTYPE_HAS_IL2) == 0) {
			mips5k_hitwbinv_primary(PHYS_TO_XKPHYS(pa, CCA_CACHED),
			    PAGE_SIZE);
		} /* else
			done below */
		break;
#endif
#if defined(CPU_R5000) || defined(CPU_R4600)
	case 2:
	    {
		vaddr_t sva, eva, iva;
		vsize_t offs;

		sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
		offs = ci->ci_l1data.setsize;
		/* keep only the index bits */
		sva |= va & (offs - 1);
		eva = sva + PAGE_SIZE;

		iva = sva + offs;
		while (sva != eva) {
			cache(IndexWBInvalidate_D, 0, iva);
			cache(IndexWBInvalidate_D, 0, sva);
			iva += R5K_LINE;
			sva += R5K_LINE;
		}
	    }
		break;
#endif
	}

#ifdef CPU_R4600
	setsr(sr);
#endif

#ifdef CPU_RM7000
	if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2) {
		mips5k_hitwbinv_secondary(PHYS_TO_XKPHYS(pa, CCA_CACHED),
		    PAGE_SIZE);
	}
#endif

	mips_sync();
}

/*
 * Writeback D$ for the given page, which is expected to be currently
 * mapped, allowing the use of `Hit' operations. This is less aggressive
 * than using `Index' operations.
 */

void
Mips5k_HitSyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
{
	if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
		mips5k_hitwbinv_secondary(va, PAGE_SIZE);
	else {
#ifdef CPU_R4600
		/*
		 * R4600 revision 2 needs to load from an uncached address
		 * before any Hit or CreateDEX operation. Alternatively, 12
		 * nop (cycles) will empty the cache load buffer.
		 * We are only putting 10 here, and hope the overhead of the
		 * code around will provide the rest.
		 */
		nop10();
#endif
		mips5k_hitwbinv_primary(va, PAGE_SIZE);
	}

	mips_sync();
}

/*
 * Writeback D$ for the given range. Range is expected to be currently
 * mapped, allowing the use of `Hit' operations. This is less aggressive
 * than using `Index' operations.
 */

void
Mips5k_HitSyncDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va;
	vsize_t sz;

	/* extend the range to integral cache lines */
	va = _va & ~(R5K_LINE - 1);
	sz = ((_va + _sz + R5K_LINE - 1) & ~(R5K_LINE - 1)) - va;

	if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
		mips5k_hitwbinv_secondary(va, sz);
	else {
#ifdef CPU_R4600
		/*
		 * R4600 revision 2 needs to load from an uncached address
		 * before any Hit or CreateDEX operation. Alternatively, 12
		 * nop (cycles) will empty the cache load buffer.
		 * We are only putting 10 here, and hope the overhead of the
		 * code around will provide the rest.
		 */
		nop10();
#endif
		mips5k_hitwbinv_primary(va, sz);
	}

	mips_sync();
}

/*
 * Invalidate D$ for the given range. Range is expected to be currently
 * mapped, allowing the use of `Hit' operations. This is less aggressive
 * than using `Index' operations.
 */

static __inline__ void
mips5k_hitinv_primary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitInvalidate_D, 0, va);
		va += R5K_LINE;
	}
}

static __inline__ void
mips5k_hitinv_secondary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitInvalidate_S, 0, va);
		cache(HitInvalidate_D, 0, va);	/* orphans in L1 */
		va += R5K_LINE;
	}
}

void
Mips5k_HitInvalidateDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va;
	vsize_t sz;

	/* extend the range to integral cache lines */
	va = _va & ~(R5K_LINE - 1);
	sz = ((_va + _sz + R5K_LINE - 1) & ~(R5K_LINE - 1)) - va;

	if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
		mips5k_hitinv_secondary(va, sz);
	else {
#ifdef CPU_R4600
		/*
		 * R4600 revision 2 needs to load from an uncached address
		 * before any Hit or CreateDEX operation. Alternatively, 12
		 * nop (cycles) will empty the cache load buffer.
		 * We are only putting 10 here, and hope the overhead of the
		 * code around will provide the rest.
		 */
		nop10();
#endif
		mips5k_hitinv_primary(va, sz);
	}

	mips_sync();
}

static __inline__ void
mips5k_hitwb_primary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWriteback_D, 0, va);
		va += R5K_LINE;
	}
}

#if 0
static __inline__ void
mips5k_hitwb_secondary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWriteback_S, 0, va);
		cache(HitWriteback_D, 0, va);	/* orphans in L1 */
		va += R5K_LINE;
	}
}
#endif

/*
 * Backend for bus_dmamap_sync(). Enforce coherency of the given range
 * by performing the necessary cache writeback and/or invalidate
 * operations.
 */
void
Mips5k_IOSyncDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz, int how)
{
	vaddr_t va;
	vsize_t sz;
	int partial_start, partial_end;

	/*
	 * internal cache
	 */

	/* extend the range to integral cache lines */
	va = _va & ~(R5K_LINE - 1);
	sz = ((_va + _sz + R5K_LINE - 1) & ~(R5K_LINE - 1)) - va;

#ifdef CPU_R4600
	/*
	 * R4600 revision 2 needs to load from an uncached address
	 * before any Hit or CreateDEX operation. Alternatively, 12
	 * nop (cycles) will empty the cache load buffer.
	 * We are only putting 10 here, and hope the overhead of the
	 * code around will provide the rest.
	 */
	nop10();
#endif

	switch (how) {
	case CACHE_SYNC_R:
		/* writeback partial cachelines */
		if (((_va | _sz) & (R5K_LINE - 1)) != 0) {
			partial_start = va != _va;
			partial_end = va + sz != _va + _sz;
		} else {
			partial_start = partial_end = 0;
		}
		if (partial_start) {
#ifdef CPU_RM7000
			if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
				cache(HitWBInvalidate_S, 0, va);
#endif
			cache(HitWBInvalidate_D, 0, va);
			va += R5K_LINE;
			sz -= R5K_LINE;
		}
		if (sz != 0 && partial_end) {
			sz -= R5K_LINE;
#ifdef CPU_RM7000
			if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
				cache(HitWBInvalidate_S, 0, va + sz);
#endif
			cache(HitWBInvalidate_D, 0, va + sz);
		}

		if (sz != 0) {
#ifdef CPU_RM7000
			if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
				mips5k_hitinv_secondary(va, sz);
			else
#endif
				mips5k_hitinv_primary(va, sz);
		}
		break;

	case CACHE_SYNC_W:
#ifdef CPU_RM7000
		if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
			mips5k_hitwbinv_secondary(va, sz);	/* XXX */
		else
#endif
			mips5k_hitwb_primary(va, sz);
		break;

	case CACHE_SYNC_X:
#ifdef CPU_RM7000
		if (ci->ci_cacheconfiguration & CTYPE_HAS_IL2)
			mips5k_hitwbinv_secondary(va, sz);
		else
#endif
			mips5k_hitwbinv_primary(va, sz);
		break;
	}

	/*
	 * external cache
	 */

	switch (how) {
	case CACHE_SYNC_W:
		break;
	case CACHE_SYNC_X:
	case CACHE_SYNC_R:
#ifdef CPU_R5000
		if (ci->ci_cacheconfiguration & CTYPE_HAS_XL2) {
			/* align on external page size */
			va = _va & ~(R5K_PAGE - 1);
			sz = ((_va + _sz + R5K_PAGE - 1) - va) / R5K_PAGE;
			reset_taglo();
			while (sz != 0) {
				cache(InvalidatePage_S, 0, va);
				va += R5K_PAGE;
				sz--;
			}
		} else
#endif
#ifdef CPU_RM7000
		if (ci->ci_cacheconfiguration & CTYPE_HAS_XL3) {
			/* align on external page size */
			va = _va & ~(R5K_PAGE - 1);
			sz = ((_va + _sz + R5K_PAGE - 1) - va) / R5K_PAGE;
			reset_taglo();
			while (sz != 0) {
				cache(InvalidatePage_T, 0, va);
				va += R5K_PAGE;
				sz--;
			}
		} else
#endif
		{
		}
		break;
	}

	mips_sync();
}
@


1.14
log
@We need to be more aggressive flushing L2 entries on RM7000 systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.13 2014/03/31 20:21:19 miod Exp $	*/
d425 1
d780 28
@


1.13
log
@Due the virtually indexed nature of the L1 instruction cache on most mips
processors, every time a new text page is mapped in a pmap, the L1 I$ is
flushed for the va spanned by this page.

Since we map pages of our binaries upon demand, as they get faulted in, but
uvm_fault() tries to map the few neighbour pages, this can end up in a
bunch of pmap_enter() calls in a row, for executable mappings. If the L1
I$ is small enough, this can cause the whole L1 I$ cache to be flushed
several times.

Change pmap_enter() to postpone these flushes by only registering the
pending flushes, and have pmap_update() perform them. The cpu-specific
cache code can then optimize this to avoid unnecessary operations.

Tested on R4000SC, R4600SC, R5000SC, RM7000, R10000 with 4KB and 16KB
page sizes (coherent and non-coherent designs), and Loongson 2F by mikeb@@ and
me. Should not affect anything on Octeon since there is no way to flush a
subset of I$ anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.12 2014/03/29 18:09:30 guenther Exp $	*/
d81 3
d151 4
a798 1
#if 0
d801 1
a801 3
	else
#endif
	     {
a858 1
#if 0
d861 1
a861 3
	else
#endif
	     {
d878 27
d973 9
a982 1
	case CACHE_SYNC_W:
@


1.12
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.11 2014/03/11 20:32:42 miod Exp $	*/
d415 2
d559 121
@


1.11
log
@Simpler RM7000 L2 cache initialization code (no longer matches what pmon2000
did, but less awkward). Also make sure that the code changing the K0SEG CCA
value has enough nops, before returning to cached space, to match the
recommended procedure in the RM52xx and RM7000 erratas.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.10 2014/03/09 10:12:17 miod Exp $	*/
d111 1
a111 1
#define	nop4()			__asm__ __volatile__ \
d113 1
a113 1
#define	nop10()			__asm__ __volatile__ \
d116 1
a116 1
#define	cache(op,offs,addr)	__asm__ __volatile__ \
d119 1
a119 1
#define	reset_taglo()		__asm__ __volatile__ \
d121 1
a121 1
#define	reset_taghi()		__asm__ __volatile__ \
d128 1
a128 1
	__asm__ __volatile__ ("mfc0 %0, $16" : "=r"(cfg)); /* COP_0_CONFIG */
d135 1
a135 1
	__asm__ __volatile__ ("mtc0 %0, $16" :: "r"(cfg)); /* COP_0_CONFIG */
d235 1
a235 1
		__asm__ __volatile__
@


1.10
log
@Rework the per-cpu cache information. Use a common struct to store the line
size, the number of sets, and the total size (and the set size, for convenience)
per cache (I$, D$, L2, L3).
This allows cpu.c to print the number of ways (sets) of L2 and L3 caches from
the cache information, rather than hardcoding this from the processor type.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.9 2013/11/26 20:33:13 deraadt Exp $	*/
d152 1
d227 1
a228 1
		cache(IndexStoreTag_S, -4, va);
d235 2
a237 2
		__asm__ __volatile__
		    ("lw $zero, %0(%1)" :: "i"(-4), "r"(va));
d244 1
a245 1
		cache(IndexStoreTag_S, -4, va);
d277 22
d422 1
a422 1
		run_uncached(cp0_set_config, ncfg);
@


1.9
log
@1 << 31 cleanup.  Eitan Adler pointed out that there has been a
resurrection of the bad idiom in the tree.
sufficient review by miod, kettenis, tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.8 2012/10/03 11:18:23 miod Exp $	*/
d288 4
a291 4
	ci->ci_l1instcacheline = R5K_LINE;
	ci->ci_l1instcachesize = (1 << 12) << ((cfg >> 9) & 0x07); /* IC */
	ci->ci_l1datacacheline = R5K_LINE;
	ci->ci_l1datacachesize = (1 << 12) << ((cfg >> 6) & 0x07); /* DC */
a293 1
	ci->ci_cacheways = 2;
d295 2
a296 3
	ci->ci_l2line = 0;
	ci->ci_l2size = 0;
	ci->ci_l3size = 0;
d312 1
a312 1
		/* optional external L2 cache */
d314 1
a314 2
			ci->ci_l2line = R5K_LINE;
			ci->ci_l2size = (1 << 19) <<
d316 3
d320 1
a320 1
		if (ci->ci_l2size != 0) {
d323 1
a323 1
			run_uncached(mips5k_l2_init, ci->ci_l2size);
a329 1
		ci->ci_cacheways = 4;
d331 1
a331 1
		/* optional external L3 cache */
d341 1
a341 1
			ci->ci_l3size = (1 << 19) <<
d343 3
d348 1
a348 1
			 * Assume machdep has initialized ci_l3size for us.
d352 1
a352 1
		if (ci->ci_l3size != 0) {
d355 2
a356 1
			run_uncached(mips7k_l3_init, ci->ci_l3size);
d358 1
a358 1
		/* internal L2 cache */
d360 4
a363 2
			ci->ci_l2line = R5K_LINE;
			ci->ci_l2size = 256 * 1024;	/* fixed size */
d365 1
a365 1
		if (ci->ci_l2size != 0) {
d369 1
a369 1
				run_uncached(mips7k_l2_init, ci->ci_l2size);
d376 4
a379 2
	ci->ci_l1instcacheset = ci->ci_l1instcachesize >> setshift;
	ci->ci_l1datacacheset = ci->ci_l1datacachesize >> setshift;
d382 1
a382 1
	    (max(ci->ci_l1instcacheset, ci->ci_l1datacacheset) - 1) &
d418 1
a418 1
	eva = sva + ci->ci_l1instcachesize;
d425 1
a425 1
	eva = sva + ci->ci_l1datacachesize;
d438 1
a438 1
		eva = sva + ci->ci_l2size;
d448 1
a448 1
		eva = sva + ci->ci_l2size;
d462 1
a462 1
		eva = sva + ci->ci_l3size;
d495 1
a495 1
	offs = ci->ci_l1instcacheset;
d500 1
a500 1
	switch (ci->ci_cacheways) {
d575 1
a575 1
	switch (ci->ci_cacheways) {
d599 1
a599 1
		offs = ci->ci_l1datacacheset;
@


1.8
log
@Split ever-growing mips <machine/cpu.h> into what 99% of the kernel needs,
which will remain in <machine/cpu.h>, and a new mips_cpu.h containing only the
goriest md details, which are only of interest to a handful set of files; this
is similar in spirit to what alpha does, but here <machine/cpu.h> does not
include the new file.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.7 2012/09/29 19:24:31 miod Exp $	*/
d84 3
a86 3
#define	CF_5_SE		(1 << 12)	/* Secondary cache enable */
#define	CF_5_SC		(1 << 17)	/* Secondary cache not present */
#define	CF_5_SS		(3 << 20)	/* Secondary cache size */
d92 5
a96 5
#define	CF_7_SE		(1 << 3)	/* Secondary cache enable */
#define	CF_7_SC		(1 << 31)	/* Secondary cache not present */
#define	CF_7_TE		(1 << 12)	/* Tertiary cache enable */
#define	CF_7_TC		(1 << 17)	/* Tertiary cache not present */
#define	CF_7_TS		(3 << 20)	/* Tertiary cache size */
@


1.7
log
@Handle the coprocessor 0 cause and status registers as a 64 bit value now,
as some odd mips designs need moro than 32 bits in there. This causes a lot
of mechanical changes everywhere getsr() is used.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.6 2012/09/29 19:13:15 miod Exp $	*/
d63 1
@


1.6
log
@Add a few more coprocessor 0 cause and config registers defines.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.5 2012/09/29 19:02:26 miod Exp $	*/
d123 1
a123 1
static __inline__ uint32_t
d126 1
a126 1
	uint32_t cfg;
d132 1
a132 1
set_config(uint32_t cfg)
d148 4
a151 4
void mips5k_l2_init(uint32_t);
void mips7k_l2_init(uint32_t);
void mips7k_l3_init(uint32_t);
static void run_uncached(void (*)(uint32_t), uint32_t);
d159 1
a159 1
run_uncached(void (*fn)(uint32_t), uint32_t arg)
d172 1
a172 1
	fn = (void (*)(uint32_t))va;
d184 1
a184 1
mips5k_l2_init(uint32_t l2size)
d187 1
a187 1
	register uint32_t cfg;
d210 1
a210 1
mips7k_l2_init(uint32_t l2size)
d213 1
a213 1
	register uint32_t cfg;
d254 1
a254 1
mips7k_l3_init(uint32_t l3size)
d257 1
a257 1
	register uint32_t cfg;
d281 1
a281 1
	uint32_t cfg, ncfg;
d406 1
a406 1
	uint32_t sr = disableintr();
d479 1
a479 1
	uint32_t sr = disableintr();
d564 1
a564 1
	uint32_t sr = disableintr();
@


1.5
log
@Introduce assembly macros for specific processor hazards: tlb update, status
register update, status register update causing a change to the interrupt
enable flag, and a few other arcane ones. <mips64/asm.h> will provide
(supposedly sane) defaults, and <machine/asm.h> may override these with
better tuned versions.

Use these macros instead of random strings of nop in the various .S files
requiring hazard workarounds.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.4 2012/09/29 18:54:38 miod Exp $	*/
d389 1
a389 1
	ncfg = (cfg & ~7) | CCA_CACHED;
@


1.4
log
@Proide a mips_sync() macro to wrap asm("sync"), and replace gazillions of
such statements with it.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.3 2012/06/24 20:24:46 miod Exp $	*/
d135 1
a135 1
	/* ITLBNOPFIX */
@


1.3
log
@Since the RM7000 cache behaves as a physically-indexed cache due to the set
size being not larger than 4KB, use Hit operations on the pa instead of Index
operations on the va for each set in the SyncDCachePage routine, when running
with such a cache.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.2 2012/06/24 16:26:04 miod Exp $	*/
a117 2
#define	sync()			__asm__ __volatile__ ("sync" ::: "memory")

d228 1
a228 1
	sync();
d237 1
a237 1
	sync();
d245 1
a245 1
	sync();
d463 1
a463 1
	sync();
d525 1
a525 1
	sync();
d619 1
a619 1
	sync();
d657 1
a657 1
	sync();
d720 1
a720 1
	sync();
d842 1
a842 1
	sync();
@


1.2
log
@Add cache operation functions pointers to struct cpu_info; the various
cache lines and sizes are already there, after all.

The ConfigCache cache routine is responsible for filling these function
pointers; cache routine invocation macros are updated to use the cpu_info
fields, but may still be overriden in <machine/cpu.h> on platforms where
only one set of cache routines is used.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.1 2012/06/23 21:56:06 miod Exp $	*/
d122 2
d196 1
d222 1
d266 1
d530 25
a560 2
	vaddr_t sva, eva, iva;
	vsize_t offs;
a568 6
	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	offs = ci->ci_l1datacacheset;
	/* keep only the index bits */
	sva |= va & (offs - 1);
	eva = sva + PAGE_SIZE;

d573 11
a583 11
		while (sva != eva) {
			iva = sva;
			cache(IndexWBInvalidate_D, 0, iva);
			iva += offs;
			cache(IndexWBInvalidate_D, 0, iva);
			iva += offs;
			cache(IndexWBInvalidate_D, 0, iva);
			iva += offs;
			cache(IndexWBInvalidate_D, 0, iva);
			sva += R5K_LINE;
		}
d588 10
d605 1
d616 2
a617 14
		sva = PHYS_TO_XKPHYS(pa, CCA_CACHED);
		offs = ci->ci_l2size / 4;	/* hardcoded 4 way */
		eva = sva + PAGE_SIZE;
		while (sva != eva) {
			iva = sva;
			cache(IndexWBInvalidate_S, 0, iva);
			iva += offs;
			cache(IndexWBInvalidate_S, 0, iva);
			iva += offs;
			cache(IndexWBInvalidate_S, 0, iva);
			iva += offs;
			cache(IndexWBInvalidate_S, 0, iva);
			sva += R5K_LINE;
		}
a628 25

static __inline__ void
mips5k_hitwbinv_primary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWBInvalidate_D, 0, va);
		va += R5K_LINE;
	}
}

static __inline__ void
mips5k_hitwbinv_secondary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWBInvalidate_S, 0, va);
		cache(HitWBInvalidate_D, 0, va);	/* orphans in L1 */
		va += R5K_LINE;
	}
}
@


1.1
log
@Replace R5000 and R10000 family assembly cache routines with C equivalents,
which will be easier to maintain on the long run. Be sure to rm cache_r*.d in
your kernel compile directories after updating.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.c,v 1.5 2012/05/27 19:13:04 miod Exp $	*/
d378 7
@

