head	1.17;
access;
symbols
	OPENBSD_5_1_BASE:1.12
	OPENBSD_5_1:1.12.0.10
	OPENBSD_5_0:1.12.0.8
	OPENBSD_5_0_BASE:1.12
	OPENBSD_4_9:1.12.0.6
	OPENBSD_4_9_BASE:1.12
	OPENBSD_4_8:1.12.0.4
	OPENBSD_4_8_BASE:1.12
	OPENBSD_4_7:1.12.0.2
	OPENBSD_4_7_BASE:1.12
	OPENBSD_4_6:1.8.0.4
	OPENBSD_4_6_BASE:1.8
	OPENBSD_4_5:1.7.0.4
	OPENBSD_4_5_BASE:1.7
	OPENBSD_4_4:1.7.0.2
	OPENBSD_4_4_BASE:1.7
	OPENBSD_4_3:1.6.0.2
	OPENBSD_4_3_BASE:1.6
	OPENBSD_4_2:1.5.0.2
	OPENBSD_4_2_BASE:1.5
	OPENBSD_4_1:1.2.0.8
	OPENBSD_4_1_BASE:1.2
	OPENBSD_4_0:1.2.0.6
	OPENBSD_4_0_BASE:1.2
	OPENBSD_3_9:1.2.0.4
	OPENBSD_3_9_BASE:1.2
	OPENBSD_3_8:1.2.0.2
	OPENBSD_3_8_BASE:1.2
	OPENBSD_3_7:1.1.0.2
	OPENBSD_3_7_BASE:1.1;
locks; strict;
comment	@# @;


1.17
date	2012.06.23.21.56.06;	author miod;	state dead;
branches;
next	1.16;

1.16
date	2012.04.21.12.20.30;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2012.04.06.20.11.18;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2012.03.25.13.45.05;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2012.03.19.20.42.26;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2010.01.09.23.47.41;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2010.01.09.23.34.29;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2009.12.25.20.59.45;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2009.08.06.21.06.30;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2009.05.22.20.37.53;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2008.04.07.22.30.47;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2007.10.18.04.32.25;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2007.06.18.20.25.55;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2007.05.27.09.23.04;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2007.03.21.05.26.37;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2005.07.20.21.36.32;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2004.09.20.10.28.36;	author pefo;	state Exp;
branches;
next	;


desc
@@


1.17
log
@Replace R5000 and R10000 family assembly cache routines with C equivalents,
which will be easier to maintain on the long run. Be sure to rm cache_r*.d in
your kernel compile directories after updating.
@
text
@/*	$OpenBSD: cache_r10k.S,v 1.16 2012/04/21 12:20:30 miod Exp $ */

/*
 * Copyright (c) 2004 Opsycon AB (www.opsycon.se)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

/*
 * Processors supported:
 * R10000, R12000, R14000 and R16000.
 *
 * The following assumptions are made:
 * - L1 I$ is 2 way, 64 bytes/line
 * - L1 D$ is WB, 2 way, 32 bytes/line
 * - L2 is WT, 2 way
 */

#include <sys/errno.h>
#include <sys/syscall.h>

#include <machine/param.h>
#include <machine/asm.h>
#include <machine/cpu.h>
#include <machine/regnum.h>

#include "assym.h"

	.set	mips3

/*
 *  Skip the .h file. Noone else need to know!
 */

#define	IndexInvalidate_I	0x00
#define	IndexWBInvalidate_D	0x01
#define	IndexFlashInvalidate_T	0x02
#define	IndexWBInvalidate_S	0x03

#define	IndexLoadTag_I		0x04
#define	IndexLoadTag_D		0x05
#define	IndexLoadTag_S		0x07

#define	IndexStoreTag_I		0x08
#define	IndexStoreTag_D		0x09
#define	IndexStoreTag_S		0x0b

#define	CreateDirtyExclusive	0x09

#define	HitInvalidate_I		0x10
#define	HitInvalidate_D		0x11
#define	HitInvalidate_S		0x13

#define	Fill_I			0x14
#define	HitWBInvalidate_D	0x15
#define	HitWBInvalidate_S	0x17

#define	HitWB_I			0x18
#define	HitWB_D			0x19
#define	HitWB_S			0x1b

/*
 *  Define cache type definition bits. NOTE! the 3 lsb may NOT change!
 */
#define	CTYPE_DIR		0x0001	/* Cache is direct mapped */
#define	CTYPE_2WAY		0x0002	/* Cache is TWO way */
#define	CTYPE_4WAY		0x0004	/* Cache is FOUR way */
#define	CTYPE_WAYMASK		0x0007

#define	CTYPE_HAS_XL2		0x0200	/* External L2 Cache present */

	.set	noreorder		# Noreorder is default style!

/*----------------------------------------------------------------------------
 *
 * Mips10k_ConfigCache(struct cpu_info *ci)
 *
 *	Size and configure the caches.
 *	NOTE: should only be called from mips_init().
 *
 * Side effects:
 *	The size of the data cache is stored into ci_l1datacachesize.
 *	The size of instruction cache is stored into ci_l1instcachesize.
 *	Alignment mask for cache aliasing test is stored in cache_valias_mask.
 *	ci_l2size is set to the size of the secondary cache.
 *	ci_l3size is set to the size of the tertiary cache.
 *	ci_cacheways is set to 0 for direct mapped caches, 2 for two way
 *	caches and 4 for four way caches. This primarily indicates the
 *	primary cache associativity.
 *
 * Allocation:
 *	ta0, ta1 ta2 used to hold I and D set size and Alias mask.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_ConfigCache, 0)
	.set	noreorder
	mfc0	v0, COP_0_CONFIG		# Get configuration register

	srl	t1, v0, 29			# Get I cache size.
	and	t1, 7
	li	t2, 4096
	sllv	ta0, t2, t1			# ta0 = Initial I set size.

	li	t2, 64
	sw	t2, CI_L1INSTCACHELINE(a0)

	srl	t1, v0, 26			# Get D cache size.
	and	t1, 7
	li	t2, 4096			# Fixed page size.
	sllv	ta1, t2, t1

	li	t2, 32				# Get D cache line size.
	sw	t2, CI_L1DATACACHELINE(a0)

	li	t2, CTYPE_2WAY			# Assume two way cache
	li	ta3, 0				# Tertiary size 0.

	or	t2, CTYPE_HAS_XL2		# External L2 present.
	srl	t1, v0, 16			# Get L2 cache size.
	and	t1, 7
	li	ta2, 512*1024			# 512k per 'click'.
	sll	ta2, t1

/*
 * Get here with t2 = Cache type, ta0 = L1 I size, ta1 = L1 D size.
 * ta2 = secondary size, ta3 = tertiary size.
 */
ConfResult:
	sw	t2, CI_CACHECONFIGURATION(a0)	# Save cache attributes
	and	t2, CTYPE_WAYMASK		# isolate number of sets.
	sw	t2, CI_CACHEWAYS(a0)
	srl	t2, 1				# get div shift for set size.

	sw	ta2, CI_L2SIZE(a0)
	sw	ta3, CI_L3SIZE(a0)

	addu	t1, ta0, -1			# Use icache for alias mask
	srl	t1, t2
	and	t1, ~(PAGE_SIZE - 1)
	beqz	t1, 1f
	nop
	or	t1, (PAGE_SIZE - 1)
1:
#ifdef MULTIPROCESSOR
	PTR_L	ta2, cache_valias_mask
	or	t1, ta2				# Pick largest mask
#endif
	PTR_S	t1, cache_valias_mask
	PTR_S	t1, pmap_prefer_mask

	sw	ta0, CI_L1INSTCACHESIZE(a0)	# store cache size.
	srl	ta0, t2				# calculate set size.
	sw	ta0, CI_L1INSTCACHESET(a0)

	sw	ta1, CI_L1DATACACHESIZE(a0)	# store cache size.
	srl	ta1, t2				# calculate set size.
	sw	ta1, CI_L1DATACACHESET(a0)

	j	ra
	nop
END(Mips10k_ConfigCache)

/*----------------------------------------------------------------------------
 *
 * Mips10k_SyncCache(struct cpu_info *ci)
 *
 *	Sync ALL caches.
 *	No need to look at number of sets since we are cleaning out
 *	the entire cache and thus will address all sets anyway.
 *
 * Side effects:
 *	The contents of ALL caches are Invalidated or Synched.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_SyncCache, 0)
	.set	noreorder
	lw	t1, CI_L1INSTCACHESET(a0)
	lw	t2, CI_L1DATACACHESET(a0)

/*
 * Sync the instruction cache.
 */

	LOAD_XKPHYS(t0, CCA_CACHED)
	PTR_ADDU t1, t0, t1			# Compute end address
	PTR_SUBU t1, 128

1:
	cache	IndexInvalidate_I, 0(t0)
	cache	IndexInvalidate_I, 64(t0)

	cache	IndexInvalidate_I, 1(t0)
	cache	IndexInvalidate_I, 65(t0)

	bne	t0, t1, 1b
	PTR_ADDU t0, t0, 128

/*
 * Sync the data cache. Do L1 first. Indexed only operate on
 * the selected cache and differs from Hit in that sense.
 */

	LOAD_XKPHYS(t0, CCA_CACHED)
	PTR_ADDU t1, t0, t2			# End address
	PTR_SUBU t1, t1, 128
1:
	cache	IndexWBInvalidate_D, 0(t0)
	cache	IndexWBInvalidate_D, 32(t0)
	cache	IndexWBInvalidate_D, 64(t0)
	cache	IndexWBInvalidate_D, 96(t0)

	cache	IndexWBInvalidate_D, 1(t0)
	cache	IndexWBInvalidate_D, 33(t0)
	cache	IndexWBInvalidate_D, 65(t0)
	cache	IndexWBInvalidate_D, 97(t0)

	bne	t0, t1, 1b
	PTR_ADDU t0, t0, 128

/* Do L2 */
	LOAD_XKPHYS(t3, CCA_CACHED)
	lw	ta0, CI_L2SIZE(a0)
	PTR_SRL	ta0, ta0, 1			# proper set size
10:
	cache	IndexWBInvalidate_S, 0(t3)
	cache	IndexWBInvalidate_S, 1(t3)
	PTR_SUBU ta0, 64			# Fixed L2 cache line size.
	bgtz	ta0, 10b
	PTR_ADDU t3, 64

	j	ra
	nop
END(Mips10k_SyncCache)

/*----------------------------------------------------------------------------
 *
 * Mips10k_InvalidateICache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Invalidate the L1 instruction cache for at least range of va to
 *	va + len - 1.
 *
 * Side effects:
 *	The contents of the L1 Instruction cache are invalidated.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_InvalidateICache, 0)
	LOAD_XKPHYS(a3, CCA_CACHED)
	and	a1, 0x00ffffff			# Reduce addr to cache index
	PTR_ADDU a2, 63				# Round up size
	PTR_ADDU a2, a1				# Add extra from address
	and	a1, -64				# Align start address
	PTR_SUBU a2, a2, a1
	PTR_ADDU a1, a3				# a1 now new XKPHYS address
	srl	a2, a2, 6			# Number of unrolled loops
1:
	addu	a2, -1

	cache	IndexInvalidate_I, 0(a1)	# do set A
	cache	IndexInvalidate_I, 1(a1)	# do set B

	bne	a2, zero, 1b
	PTR_ADDU a1, 64

	j	ra
	nop
END(Mips10k_InvalidateICache)

/*----------------------------------------------------------------------------
 *
 * Mips10k_SyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
 *
 *	Sync the L1 data cache page for address va.
 *	The physical address is not used.
 *
 * Side effects:
 *	The contents of the cache is written back to primary memory.
 *	The cache line is invalidated.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_SyncDCachePage, 0)
	LOAD_XKPHYS(a3, CCA_CACHED)
	dsll	a1, (64 - 57)
	dsrl	a1, (64 - 57)
	PTR_ADDU a1, a3				# a1 now new XKPHYS address
	and	a1, ~PAGE_MASK			# Page align start address
	PTR_ADDU a2, a1, PAGE_SIZE-128

1:
	cache	IndexWBInvalidate_D, 0(a1)	# do set A
	cache	IndexWBInvalidate_D, 32(a1)
	cache	IndexWBInvalidate_D, 64(a1)
	cache	IndexWBInvalidate_D, 96(a1)

	cache	IndexWBInvalidate_D, 1(a1)	# do set B
	cache	IndexWBInvalidate_D, 33(a1)
	cache	IndexWBInvalidate_D, 65(a1)
	cache	IndexWBInvalidate_D, 97(a1)

	bne	a2, a1, 1b
	PTR_ADDU a1, 128

	j	ra
	nop
END(Mips10k_SyncDCachePage)

/*----------------------------------------------------------------------------
 *
 * Mips10k_HitSyncDCache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Sync data cache for range of va to va + len - 1.
 *	Only lines with matching address are flushed.
 *
 * Side effects:
 *	The contents of the L1 cache is written back to primary memory.
 *	The cache line is invalidated.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_HitSyncDCache, 0)
	beq	a2, zero, 3f			# size is zero!
	PTR_ADDU a2, 31				# Round up
	PTR_ADDU a2, a2, a1			# Add extra from address
	and	a1, a1, -32			# align address
	PTR_SUBU a2, a2, a1
	srl	a2, a2, 5			# Compute number of cache lines

1:
	PTR_ADDU a2, -1
	cache	HitWBInvalidate_D, 0(a1)
	bne	a2, zero, 1b
	PTR_ADDU a1, 32

3:
	j	ra
	nop
END(Mips10k_HitSyncDCache)

/*----------------------------------------------------------------------------
 *
 * _mips10k_HitSyncSCache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Sync secondary cache for range of va to va + len - 1.
 *	Only lines with matching address are flushed.
 *
 * Side effects:
 *	The contents of the L2 cache is written back to primary memory.
 *	The cache line is invalidated.
 *
 *----------------------------------------------------------------------------
 */
ALEAF(_mips10k_HitSyncSCache)
	beq	a2, zero, 3f			# size is zero!
	PTR_ADDU a2, a2, a1			# Add in extra from align
	and	a1, a1, -32			# Align address
	PTR_SUBU a2, a2, a1
1:
	PTR_ADDU a2, -32

	cache	HitWBInvalidate_S, 0(a1)

	bgtz	a2, 1b
	PTR_ADDU a1, 32

3:
	j	ra
	nop

/*----------------------------------------------------------------------------
 *
 * Mips10k_HitInvalidateDCache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Invalidate data cache for range of va to va + len - 1.
 *	Only lines with matching addresses are invalidated.
 *
 * Side effects:
 *	The L1 cache line is invalidated.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_HitInvalidateDCache, 0)
	beq	a2, zero, 3f			# size is zero!
	PTR_ADDU a2, a2, a1			# Add in extra from align
	and	a1, a1, -32			# Align address
	PTR_SUBU a2, a2, a1

1:
	PTR_ADDU a2, -32

	cache	HitInvalidate_D, 0(a1)

	bgtz	a2, 1b
	PTR_ADDU a1, 32

3:
	j	ra
	nop
END(Mips10k_HitInvalidateDCache)

/*----------------------------------------------------------------------------
 *
 * _mips10k_HitInvalidateSCache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Invalidate secondary cache for range of va to va + len - 1.
 *	Only lines with matching addresses are invalidated.
 *
 * Side effects:
 *	The L2 cache line is invalidated.
 *
 *----------------------------------------------------------------------------
 */
ALEAF(_mips10k_HitInvalidateSCache)
	beq	a2, zero, 3f			# size is zero!
	PTR_ADDU a2, a2, a1			# Add in extra from align
	and	a1, a1, -32			# Align address
	PTR_SUBU a2, a2, a1
1:
	PTR_ADDU a2, -32

	cache	HitInvalidate_S, 0(a1)

	bgtz	a2, 1b
	PTR_ADDU a1, 32

3:
	j	ra
	nop

/*----------------------------------------------------------------------------
 *
 * Mips10k_IOSyncDCache(struct cpu_info *ci, vaddr_t va, size_t len, int how)
 *
 *	Invalidate or flush data cache for range of va to va + len - 1.
 *
 * Side effects:
 *	If how == 0 (read), L1 and L2 caches are invalidated or flushed if
 *		the area does not match the alignment requirements. 
 *	If how == 1 (write), L1 and L2 caches are written back
 *		to memory and invalidated.
 *	If how == 2 (write-read), L1 and L2 caches are written back
 *		to memory and invalidated.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_IOSyncDCache, 0)
	beqz	a3, SyncRD			# Sync PREREAD
	nop
	addiu	a3, -1
	bnez	a3, SyncRDWB			# Sync PREWRITE+PREREAD
	nop

SyncWR:
	j	_mips10k_HitSyncSCache		# Do L2 cache
	nop					# L1 done in parallel

SyncRD:
	and	t0, a1, 63			# check if invalidate possible
	bnez	t0, SyncRDWB			# both address and size must
	and	t0, a2, 63			# be aligned at the cache size
	bnez	t0, SyncRDWB
	nop

	/*
	 *  Sync for aligned read, no writeback required.
	 */
	j	_mips10k_HitInvalidateSCache	# L2 cache
	nop					# L1 done in parallel

SyncRDWB:
	/*
	 *  Sync for unaligned read or write-read.
	 */
	j	_mips10k_HitSyncSCache		# L2 cache
	nop					# L1 done in parallel
END(Mips10k_IOSyncDCache)
@


1.16
log
@Rework the signature of the cache handling routines again. It makes more sense
to pass both the virtual and physical addresses of the page to clean to
SyncDCachePage, which is the only routine using `Index' operations on the data
cache, which might be virtually indexed at some levels but physically indexed
at others. On the other hand, it does not make any sense to pass a physical
address to routines using `Hit' operations (and they were discarding them
anyway).

In addition to making things cleaner, this fixes sporadic userland misbehaviour
(read: SIGSGEV) on RM7000 O2 systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.15 2012/04/06 20:11:18 miod Exp $ */
@


1.15
log
@Make the logic for PMAP_PREFER() and the logic, inside pmap, to do the
necessary cache coherency work wrt similar virtual indexes of different
physical pages, depending upon two distinct global variables, instead of
a shared one. R4000/R4400 VCE requires a 32KB mask for PMAP_PREFER, which
is otherwise not necessary for pmap coherency (especially since, on these
processors, only L1 uses virtual indexes, and the L1 size is not greater
than the page size, as we are using 16KB pages).
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.14 2012/03/25 13:45:05 miod Exp $ */
d32 5
a36 1
 * The cache line and number of ways are hardcoded.
a89 1
#define	CTYPE_HAS_IL2		0x0100	/* Internal L2 Cache present */
a90 1
#define	CTYPE_HAS_XL3		0x0400	/* External L3 Cache present */
d96 1
a96 1
 * Mips10k_ConfigCache(struct cpu_info *ci) --
a100 3
 * Results:
 *	Returns the value of the cpu configuration register.
 *
d186 1
a186 1
 * Mips10k_SyncCache(struct cpu_info *ci) --
a191 3
 * Results:
 *	None.
 *
d259 1
a259 3
 * Mips10k_InvalidateICache --
 *
 *	void Mips10k_SyncICache(struct cpu_info *ci, vaddr_t addr, size_t len)
d261 2
a262 6
 *	Invalidate the L1 instruction cache for at least range
 *	of addr to addr + len - 1.
 *	The address is reduced to a XKPHYS index to avoid TLB faults.
 *
 * Results:
 *	None.
d265 1
a265 1
 *	The contents of the L1 Instruction cache is flushed.
d293 1
a293 6
 * Mips10k_SyncDCachePage --
 *
 *	void Mips10k_SyncDCachePage(struct cpu_info *ci, vaddr_t addr)
 *
 *	Sync the L1 data cache page for address addr.
 *	The address is reduced to a XKPHYS index to avoid TLB faults.
d295 2
a296 2
 * Results:
 *	None.
d332 1
a332 1
 * Mips10k_HitSyncDCache --
d334 2
a335 10
 *	void Mips10k_HitSyncDCache(struct cpu_info *ci,
 *	    vaddr_t addr, size_t len)
 *
 *	Sync data cache for range of addr to addr + len - 1.
 *	The address can be any valid virtual address as long
 *	as no TLB invalid traps occur. Only lines with matching
 *	addr are flushed.
 *
 * Results:
 *	None.
a361 1

d364 1
a364 4
 * Mips10k_HitSyncSCache --
 *
 *	static void Mips10k_HitSyncSCache(struct cpu_info *ci,
 *	    vaddr_t addr, size_t len)
d366 2
a367 7
 *	Sync secondary cache for range of addr to addr + len - 1.
 *	The address can be any valid virtual address as long
 *	as no TLB invalid traps occur. Only lines with matching
 *	addr are flushed.
 *
 * Results:
 *	None.
d375 1
a375 1
LEAF(Mips10k_HitSyncSCache, 0)
a390 1
END(Mips10k_HitSyncSCache)
d394 1
a394 4
 * Mips10k_HitInvalidateDCache --
 *
 *	void Mips10k_HitInvalidateDCache(struct cpu_info *ci,
 *	    vaddr_t addr, size_t len)
d396 1
a396 3
 *	Invalidate data cache for range of addr to addr + len - 1.
 *	The address can be any valid address as long as no TLB misses occur.
 *	(Be sure to use cached K0SEG kernel addresses or mapped addresses)
a398 3
 * Results:
 *	None.
 *
a422 1

d425 1
a425 4
 * Mips10k_HitInvalidateSCache --
 *
 *	static void Mips10k_HitInvalidateSCache(struct cpu_info *ci,
 *	    vaddr_t addr, size_t len)
d427 1
a427 3
 *	Invalidate secondary cache for range of addr to addr + len - 1.
 *	The address can be any valid address as long as no TLB misses occur.
 *	(Be sure to use cached K0SEG kernel addresses or mapped addresses)
a429 3
 * Results:
 *	None.
 *
d435 1
a435 1
LEAF(Mips10k_HitInvalidateSCache, 0)
a450 1
END(Mips10k_HitInvalidateSCache)
d454 1
a454 4
 * Mips10k_IOSyncDCache --
 *
 *	void Mips10k_IOSyncDCache(struct cpu_info *ci, vaddr_t addr,
 *	    size_t len, int rw)
d456 1
a456 6
 *	Invalidate or flush data cache for range of addr to addr + len - 1.
 *	The address can be any valid address as long as no TLB misses occur.
 *	(Be sure to use cached K0SEG kernel addresses or mapped addresses)
 *
 * Results:
 *	None.
d459 3
a461 4
 *	If rw == 0 (read), L1 and L2 caches are invalidated or
 *		flushed if the area does not match the alignment
 *		requirements. 
 *	If rw == 1 (write), L1 and L2 caches are written back
d463 1
a463 1
 *	If rw == 2 (write-read), L1 and L2 caches are written back
d468 1
a468 4
NON_LEAF(Mips10k_IOSyncDCache, FRAMESZ(CF_SZ+2*REGSZ), ra)
	PTR_SUBU sp, FRAMESZ(CF_SZ+2*REGSZ)
	PTR_S	ra, CF_RA_OFFS+2*REGSZ(sp)
	REG_S	a1, CF_ARGSZ(sp)		# save args
d470 1
a470 1
	REG_S	a2, CF_ARGSZ+REGSZ(sp)
d476 1
a476 1
	jal	Mips10k_HitSyncSCache		# Do L2 cache
a477 2
	b	SyncDone
	PTR_L	ra, CF_RA_OFFS+2*REGSZ(sp)
d486 4
a489 4
/*
 *  Sync for aligned read, no writeback required.
 */
	jal	Mips10k_HitInvalidateSCache	# L2 cache
a491 6
	b	SyncDone
	PTR_L	ra, CF_RA_OFFS+2*REGSZ(sp)

/*
 *  Sync for unaligned read or write-read.
 */
d493 4
a496 1
	jal	Mips10k_HitSyncSCache		# L2 cache
a497 6

	PTR_L	ra, CF_RA_OFFS+2*REGSZ(sp)

SyncDone:
	j	ra
	PTR_ADDU sp, FRAMESZ(CF_SZ+2*REGSZ)
@


1.14
log
@Only set the low order bits of CpuCacheAliasMask if it is nonzero, regression
of previous computation fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.13 2012/03/19 20:42:26 miod Exp $ */
d105 1
a105 1
 *	Alignment mask for cache aliasing test is stored in CpuCacheAliasMask.
d167 1
a167 1
	PTR_L	ta2, CpuCacheAliasMask
d170 2
a171 1
	PTR_S	t1, CpuCacheAliasMask
@


1.13
log
@Recent uvm code (and maybe not-so-recent, but it did not explode^WKASSERT at
my face then...) depends upon PMAP_PREFER_ALIGN to be a power of two, minus one.

On mips64 with 4KB pages, the runtime variable used to compute PMAP_PREFER_ALIGN
had the low PAGE_SHIFT bits zeroed (for no good reason I'd say). Don't bother
zeroing them anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.12 2010/01/09 23:47:41 miod Exp $ */
d161 3
d165 1
@


1.12
log
@L2 cache line is at least 64 bytes long on r10k, so use 64 byte increments to
flush L2 in Mips10k_SyncCache().
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.11 2010/01/09 23:34:29 miod Exp $ */
d161 1
a161 1
	and	t1, ~(NBPG - 1)
d316 2
a317 2
	dsll	a1, 34
	dsrl	a1, 34
@


1.11
log
@Move cache information from global variables to per-cpu_info fields; this
allows processors with different cache sizes to be used.

Cache management routines now take a struct cpu_info * as first parameter.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.10 2009/12/25 20:59:45 miod Exp $ */
d243 2
a244 1
	lw	ta0, CI_L2SIZE(a0)		# XXX Need set size here.
d248 1
a248 1
	PTR_SUBU ta0, 32			# Fixed L2 cache line size.
d250 1
a250 1
	PTR_ADDU t3, 32
@


1.10
log
@Don't bother returning a value in *_InvalidateICache(), as it's supposed to be
a void function.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.9 2009/08/06 21:06:30 miod Exp $ */
d30 3
a32 5
 *  Processors supported:
 *  R10000
 *  R12000
 *  R14000
 *  R16000
a46 4
#define	LOAD_XKPHYS(reg, cca) \
	li	reg, cca | 0x10; \
	dsll	reg, reg, 59

d94 1
a94 1
 * Mips10k_ConfigCache --
d103 2
a104 2
 *	The size of the data cache is stored into CpuPrimaryDataCacheSize.
 *	The size of instruction cache is stored into CpuPrimaryInstCacheSize.
d106 3
a108 3
 *	CpuSecondaryCacheSize is set to the size of the secondary cache.
 *	CpuTertiaryCacheSize is set to the size of the tertiary cache.
 *	CpuNWayCache is set to 0 for direct mapped caches, 2 for two way
d127 1
a127 1
	sw	t2, CpuPrimaryInstCacheLSize
d135 1
a135 1
	sw	t2, CpuPrimaryDataCacheLSize
a137 1
	li	ta2, 0				# Secondary size 0.
d141 1
a141 1
	srl	t1, v0, 16			# Get I cache size.
d151 1
a151 4
	sw	v0, CpuConfigRegister
	mfc0	t3, COP_0_STATUS_REG
	sw	t2, CpuCacheType		# Save cache attributes
	sw	t3, CpuStatusRegister
d153 1
a153 1
	sw	t2, CpuNWayCache
d156 2
a157 2
	sw	ta2, CpuSecondaryCacheSize
	sw	ta3, CpuTertiaryCacheSize
d162 5
a166 1
	sw	t1, CpuCacheAliasMask
d168 1
a168 1
	sw	ta0, CpuPrimaryInstCacheSize	# store cache size.
d170 1
a170 1
	sw	ta0, CpuPrimaryInstSetSize
d172 1
a172 1
	sw	ta1, CpuPrimaryDataCacheSize	# store cache size.
d174 1
a174 1
	sw	ta1, CpuPrimaryDataSetSize
a175 5
#if 0
	and	v0, ~7
	or	v0, CCA_CACHED			# set cachable writeback kseg0
	mtc0	v0, COP_0_CONFIG		# establish any new config
#endif
d182 1
a182 1
 * Mips10k_SyncCache --
d198 2
a199 2
	lw	t1, CpuPrimaryInstSetSize
	lw	t2, CpuPrimaryDataSetSize
d243 1
a243 1
	lw	ta0, CpuSecondaryCacheSize	# XXX Need set size here.
d247 1
a247 1
	PTR_SUBU ta0, 32			# Fixed cache line size.
d259 1
a259 2
 *	void Mips10k_SyncICache(addr, len)
 *		vaddr_t addr, len;
d274 8
a281 8
	LOAD_XKPHYS(a2, CCA_CACHED)
	and	a0, 0x00ffffff			# Reduce addr to cache index
	PTR_ADDU a1, 63				# Round up size
	PTR_ADDU a1, a0				# Add extra from address
	and	a0, -64				# Align start address
	PTR_SUBU a1, a1, a0
	PTR_ADDU a0, a2				# a0 now new XKPHYS address
	srl	a1, a1, 6			# Number of unrolled loops
d283 1
a283 1
	addu	a1, -1
d285 2
a286 2
	cache	IndexInvalidate_I, 0(a0)	# do set A
	cache	IndexInvalidate_I, 1(a0)	# do set B
d288 2
a289 2
	bne	a1, zero, 1b
	PTR_ADDU a0, 64
d299 1
a299 2
 *	void Mips10k_SyncDCachePage(addr)
 *		vaddr_t addr;
d314 6
a319 6
	LOAD_XKPHYS(a2, CCA_CACHED)
	dsll	a0, 34
	dsrl	a0, 34
	PTR_ADDU a0, a2				# a0 now new XKPHYS address
	and	a0, ~PAGE_MASK			# Page align start address
	PTR_ADDU a1, a0, PAGE_SIZE-128
d322 9
a330 9
	cache	IndexWBInvalidate_D, 0(a0)	# do set A
	cache	IndexWBInvalidate_D, 32(a0)
	cache	IndexWBInvalidate_D, 64(a0)
	cache	IndexWBInvalidate_D, 96(a0)

	cache	IndexWBInvalidate_D, 1(a0)	# do set B
	cache	IndexWBInvalidate_D, 33(a0)
	cache	IndexWBInvalidate_D, 65(a0)
	cache	IndexWBInvalidate_D, 97(a0)
d332 2
a333 2
	bne	a1, a0, 1b
	PTR_ADDU a0, 128
d343 2
a344 2
 *	void Mips10k_HitSyncDCache(addr, len)
 *		vaddr_t addr, len;
a350 3
 *	Note: Use the CpuNWayCache flag to select 16 or 32 byte linesize.
 *	      All Nway cpu's now available have a fixed 32byte linesize.
 *
d361 6
a366 6
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, 31				# Round up
	PTR_ADDU a1, a1, a0			# Add extra from address
	and	a0, a0, -32			# align address
	PTR_SUBU a1, a1, a0
	srl	a1, a1, 5			# Compute number of cache lines
d369 4
a372 4
	PTR_ADDU a1, -1
	cache	HitWBInvalidate_D, 0(a0)
	bne	a1, zero, 1b
	PTR_ADDU a0, 32
d384 2
a385 2
 *	void Mips10k_HitSyncSCache(addr, len)
 *		vaddr_t addr, len;
d402 4
a405 4
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, a1, a0			# Add in extra from align
	and	a0, a0, -32			# Align address
	PTR_SUBU a1, a1, a0
d407 1
a407 1
	PTR_ADDU a1, -32
d409 1
a409 1
	cache	HitWBInvalidate_S, 0(a0)
d411 2
a412 2
	bgtz	a1, 1b
	PTR_ADDU a0, 32
d423 2
a424 2
 *	void Mips10k_HitInvalidateDCache(addr, len)
 *		vaddr_t addr, len;
d440 4
a443 4
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, a1, a0			# Add in extra from align
	and	a0, a0, -32			# Align address
	PTR_SUBU a1, a1, a0
d446 1
a446 1
	PTR_ADDU a1, -32
d448 1
a448 1
	cache	HitInvalidate_D, 0(a0)
d450 2
a451 2
	bgtz	a1, 1b
	PTR_ADDU a0, 32
d463 2
a464 2
 *	void Mips10k_HitInvalidateSCache(addr, len)
 *		vaddr_t addr, len;
d480 4
a483 4
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, a1, a0			# Add in extra from align
	and	a0, a0, -32			# Align address
	PTR_SUBU a1, a1, a0
d485 1
a485 1
	PTR_ADDU a1, -32
d487 1
a487 1
	cache	HitInvalidate_S, 0(a0)
d489 2
a490 2
	bgtz	a1, 1b
	PTR_ADDU a0, 32
d501 2
a502 3
 *	void Mips10k_IOSyncDCache(addr, len, rw)
 *		vaddr_t addr;
 *		int  len, rw;
a522 1

d525 5
a529 5
	REG_S	a0, CF_ARGSZ(sp)		# save args
	beqz	a2, SyncRD			# Sync PREREAD
	REG_S	a1, CF_ARGSZ+REGSZ(sp)
	addiu	a2, -1
	bnez	a2, SyncRDWB			# Sync PREWRITE+PREREAD
d539 1
a539 1
	and	t0, a0, 63			# check if invalidate possible
d541 1
a541 1
	and	t0, a1, 63			# be aligned at the cache size
@


1.9
log
@Remove _InvalidateICachePage cache op, it isn't used by anything.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.8 2009/05/22 20:37:53 miod Exp $ */
a281 1
 *	Must not touch v0.
d304 1
a304 1
	move	v0, zero
@


1.8
log
@Drop almost unused <machine/psl.h> on sgi; move USERMODE() definition from
there to trap.c which is its only user. This also cleans up multiple
inclusion of <machine/cpu.h> (because <machine/psl.h> includes it) in many
places.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.7 2008/04/07 22:30:47 miod Exp $ */
d33 2
a34 1
 *  R14000	(needs to be tested)
d183 2
a184 2
	and	v0, 0xfffffff8
	or	v0, 0x00000003			# set cachable writeback kseg0
a264 33

/*----------------------------------------------------------------------------
 *
 * Mips10k_InvalidateICachePage --
 *
 *	void Mips10k_InvalidateICachePage(addr)
 *		vaddr_t addr;
 *
 *	Invalidate the L1 instruction cache page given by addr.
 *
 * Results:
 *	Void.
 *
 * Side effects:
 *	The contents of the L1 Instruction cache is flushed.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Mips10k_InvalidateICachePage, 0)
	lw	v0, CpuNWayCache		# Cache properties
	and	a0, ~PAGE_MASK			# Page align start address
	PTR_ADDU a1, a0, PAGE_SIZE-128		# End address.
	addiu	v0, -2				# <0 1way, 0 = two, >0 four
1:
	cache	HitInvalidate_I, 0(a0)
	cache	HitInvalidate_I, 64(a0)

	bne	a0, a1, 1b
	PTR_ADDU a0, 128

	j	ra
	move	v0, zero
END(Mips10k_InvalidateICachePage)
@


1.7
log
@Use CCA_CACHED as the default CCA for all cached mappings and addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.6 2007/10/18 04:32:25 miod Exp $ */
a39 1
#include <machine/psl.h>
@


1.6
log
@No need to include <machine/pte.h> here.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.5 2007/06/18 20:25:55 miod Exp $ */
d216 1
a216 1
	LOAD_XKPHYS(t0, CCA_NONCOHERENT)
d235 1
a235 1
	LOAD_XKPHYS(t0, CCA_NONCOHERENT)
d253 1
a253 1
	LOAD_XKPHYS(t3, CCA_NONCOHERENT)
d320 1
a320 1
	LOAD_XKPHYS(a2, CCA_NONCOHERENT)
d361 1
a361 1
	LOAD_XKPHYS(a2, CCA_NONCOHERENT)
@


1.5
log
@Use a shorter form to load XKPHYS constants in .S code, shaves a few text
bytes, no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.4 2007/05/27 09:23:04 miod Exp $ */
a43 1
#include <machine/pte.h>
@


1.4
log
@Always use XKPHYS addresses to perform cache operations now, for consistency.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.3 2007/03/21 05:26:37 miod Exp $ */
d50 4
d217 1
a217 1
	LA	t0, XKPHYS_NONCOHERENT
d236 1
a236 1
	LA	t0, XKPHYS_NONCOHERENT
d254 1
a254 1
	LA	t3, XKPHYS_NONCOHERENT
d321 1
d327 1
a327 1
	PTR_ADDU a0, XKPHYS_NONCOHERENT		# a0 now new XKPHYS address
d362 1
d365 1
a365 1
	PTR_ADDU a0, XKPHYS_NONCOHERENT		# a0 now new XKPHYS address
@


1.3
log
@Mips_IOSyncDCache last argument is in the 0..2 range, not -1..1, so let
C and asm code agree on this; this causes no functional change on r10k
and fewer wt invalidates on r5k. ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.2 2005/07/20 21:36:32 miod Exp $ */
d213 1
a213 1
	LA	t0, KSEG0_BASE
d232 1
a232 1
	LA	t0, KSEG0_BASE
d250 1
a250 1
	LA	t3, KSEG0_BASE
d305 1
a305 1
 *	The address is reduced to a KSEG0 index to avoid TLB faults.
d322 1
a322 1
	PTR_ADDU a0, KSEG0_BASE			# a0 now new KSEG0 address
d345 1
a345 1
 *	The address is reduced to a KSEG0 index to avoid TLB faults.
d359 1
a359 1
	PTR_ADDU a0, KSEG0_BASE			# a0 now new KSEG0 address
@


1.2
log
@typos
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.S,v 1.1 2004/09/20 10:28:36 pefo Exp $ */
d575 1
a575 1
	addiu	a2, 1
@


1.1
log
@R10K cpu class cache support
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r5k.S,v 1.9 2004/09/16 10:13:37 miod Exp $ */
d593 1
a593 1
 *  Sync for aligned read, no writeback requierd.
@

