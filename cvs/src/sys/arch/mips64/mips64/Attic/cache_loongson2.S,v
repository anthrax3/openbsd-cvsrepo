head	1.9;
access;
symbols
	OPENBSD_5_1_BASE:1.4
	OPENBSD_5_1:1.4.0.10
	OPENBSD_5_0:1.4.0.8
	OPENBSD_5_0_BASE:1.4
	OPENBSD_4_9:1.4.0.6
	OPENBSD_4_9_BASE:1.4
	OPENBSD_4_8:1.4.0.4
	OPENBSD_4_8_BASE:1.4
	OPENBSD_4_7:1.4.0.2
	OPENBSD_4_7_BASE:1.4;
locks; strict;
comment	@# @;


1.9
date	2012.05.27.14.32.05;	author miod;	state dead;
branches;
next	1.8;

1.8
date	2012.04.21.12.20.30;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2012.04.06.20.11.18;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2012.03.25.13.45.05;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2012.03.19.20.42.26;	author miod;	state Exp;
branches;
next	1.4;

1.4
date	2010.01.09.23.34.29;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2009.12.25.21.02.15;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2009.11.19.20.16.27;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2009.08.06.21.11.39;	author miod;	state Exp;
branches;
next	;


desc
@@


1.9
log
@Replace Loongson2F assembly cache routines with equivalent C code. This will
make future maintainance easier.
@
text
@/*	$OpenBSD: cache_loongson2.S,v 1.8 2012/04/21 12:20:30 miod Exp $	*/

/*
 * Copyright (c) 2009 Miodrag Vallat.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * Copyright (c) 1998-2004 Opsycon AB (www.opsycon.se)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

/*
 * Processors supported:
 * Loongson 2E/2F (code could be modified to work on 2C by not hardcoding
 * the number of ways).
 * The L1 virtual index bits, as well as the cache line size (32 bytes),
 * are hardcoded.
 */

#include <machine/param.h>
#include <machine/asm.h>
#include <machine/cpu.h>
#include <machine/regnum.h>

#include "assym.h"

	.set	mips3

/* L1 cache operations */
#define	IndexInvalidate_I	0x00
#define	IndexWBInvalidate_D	0x01
#define	IndexLoadTag_D		0x05
#define	IndexStoreTag_D		0x09
#define	HitInvalidate_D		0x11
#define	HitWBInvalidate_D	0x15
#define	IndexLoadData_D		0x19
#define	IndexStoreData_D	0x1d

/* L2 cache operations */
#define	IndexWBInvalidate_S	0x03
#define	IndexLoadTag_S		0x07
#define	IndexStoreTag_S		0x0b
#define	HitInvalidate_S		0x13
#define	HitWBInvalidate_S	0x17
#define	IndexLoadData_S		0x1b
#define	IndexStoreData_S	0x1f

/*
 *  Define cache type definition bits. NOTE! the 3 lsb may NOT change!
 */
#define	CTYPE_DIR		0x0001	/* Cache is direct mapped */
#define	CTYPE_2WAY		0x0002	/* Cache is TWO way */
#define	CTYPE_4WAY		0x0004	/* Cache is FOUR way */
#define	CTYPE_WAYMASK		0x0007

#define	CTYPE_HAS_IL2		0x0100	/* Internal L2 Cache present */
#define	CTYPE_HAS_XL2		0x0200	/* External L2 Cache present */
#define	CTYPE_HAS_XL3		0x0400	/* External L3 Cache present */

	.set	noreorder		# Noreorder is default style!

/*----------------------------------------------------------------------------
 *
 * Loongson2_ConfigCache(struct cpu_info *ci) --
 *
 *	Setup various cache-dependent variables:
 *	The size of the data cache is stored into ci_l1datacachesize.
 *	The size of instruction cache is stored into ci_l1instcachesize.
 *	Alignment mask for cache aliasing test is stored in cache_valias_mask.
 *	ci_l2size is set to the size of the secondary cache.
 *	ci_l3size is set to the size of the tertiary cache.
 *	ci_cacheways is set to 0 for direct mapped caches, 2 for two way
 *	caches and 4 for four way caches. This primarily indicates the
 *	primary cache associativity.
 *
 * Allocation:
 *	ta0, ta1 ta2 used to hold I and D set size and Alias mask.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_ConfigCache, 0)
	mfc0	v0, COP_0_CONFIG		# Get configuration register

	srl	t1, v0, 9			# Get I cache size.
	and	t1, 7
	li	t2, 4096
	sllv	ta0, t2, t1			# ta0 = Initial I set size.

	and	t2, v0, 0x20
	srl	t2, t2, 1			# Get I cache line size.
	addu	t2, t2, 16
	sw	t2, CI_L1INSTCACHELINE(a0)

	srl	t1, v0, 6			# Get D cache size.
	and	t1, 7
	li	t2, 4096			# Fixed page size.
	sllv	ta1, t2, t1

	and	t2, v0, 0x10
	addu	t2, t2, 16			# Get D cache line size.
	sw	t2, CI_L1DATACACHELINE(a0)

	li	ta3, 0				# Tertiary size 0.

	li	ta2, 512 * 1024			# fixed L2 size...
	li	t2, (CTYPE_4WAY|CTYPE_HAS_IL2)	# caches are 4-way, internal L2

/*
 * Get here with t2 = Cache type, ta0 = L1 I size, ta1 = L1 D size.
 * ta2 = secondary size, ta3 = tertiary size.
 */
ConfResult:
	sw	t2, CI_CACHECONFIGURATION(a0)	# Save cache attributes
	and	t2, CTYPE_WAYMASK		# isolate number of sets.
	sw	t2, CI_CACHEWAYS(a0)
	srl	t2, 1				# get div shift for set size.

	sw	ta2, CI_L2SIZE(a0)
	sw	ta3, CI_L3SIZE(a0)

	addu	t1, ta0, -1			# Use icache for alias mask
	srl	t1, t2
	and	t1, ~(PAGE_SIZE - 1)
	beqz	t1, 1f
	nop
	or	t1, (PAGE_SIZE - 1)
1:
	PTR_S	t1, cache_valias_mask
	PTR_S	t1, pmap_prefer_mask

	sw	ta0, CI_L1INSTCACHESIZE(a0)	# store cache size.
	sw	ta1, CI_L1DATACACHESIZE(a0)	# store cache size.

	/*
	 * Cache way number encoding is done in the lowest bits, and
	 * these variables are not used. We make them nonzero so
	 * that `mi' code can divide by them if necessary.
	 */
	li	ta1, 1
	sw	ta1, CI_L1INSTCACHESET(a0)
	sw	ta1, CI_L1DATACACHESET(a0)

	j	ra
	nop
END(Loongson2_ConfigCache)

/*----------------------------------------------------------------------------
 *
 * Loongson2_SyncCache(struct cpu_info *ci) --
 *
 *	Sync ALL caches.
 *	No need to look at number of sets since we are cleaning out
 *	the entire cache and thus will address all sets anyway.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_SyncCache, 0)
	sync

	lw	t1, CI_L1INSTCACHESIZE(a0)
	srl	t1, t1, 2			# / 4ways
	lw	t2, CI_L1DATACACHESIZE(a0)
	srl	t2, t2, 2			# / 4ways

	/* L1 I$ */

	LOAD_XKPHYS(t0, CCA_CACHED)
	PTR_ADDU t1, t0, t1			# Compute end address
	PTR_SUBU t1, 32
1:
	cache	IndexInvalidate_I, 0(t0)
	bne	t0, t1, 1b
	PTR_ADDU t0, 32

	/* L1 D$ */

	LOAD_XKPHYS(t0, CCA_CACHED)
	PTR_ADDU t1, t0, t2			# End address
	PTR_SUBU t1, 32
1:
	cache	IndexWBInvalidate_D, 0(t0)
	cache	IndexWBInvalidate_D, 1(t0)
	cache	IndexWBInvalidate_D, 2(t0)
	cache	IndexWBInvalidate_D, 3(t0)
	bne	t0, t1, 1b
	PTR_ADDU t0, 32

	/* L2 */

	LOAD_XKPHYS(t0, CCA_CACHED)
	lw	t2, CI_L2SIZE(a0)
	srl	t2, 2				# because cache is 4 way
	PTR_ADDU t1, t0, t2
	PTR_SUBU t1, 32
1:
	cache	IndexWBInvalidate_S, 0(t0)
	cache	IndexWBInvalidate_S, 1(t0)
	cache	IndexWBInvalidate_S, 2(t0)
	cache	IndexWBInvalidate_S, 3(t0)
	bne	t0, t1, 1b
	PTR_ADDU t0, 32

	j	ra
	nop
END(Loongson2_SyncCache)

/*----------------------------------------------------------------------------
 *
 * Loongson2_SyncICache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Invalidate the L1 instruction cache for at least range
 *	of va to va + len - 1.
 *	The address is reduced to a XKPHYS index to avoid TLB faults.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_InvalidateICache, 0)
	andi	a1, ((1 << 14) - 1)		# only keep index bits
	PTR_ADDU a2, 31				# Round up size
	LOAD_XKPHYS(a3, CCA_CACHED)
	PTR_ADDU a2, a1				# Add extra from address
	dsrl	a1, a1, 5
	dsll	a1, a1, 5			# align address
	PTR_SUBU a2, a2, a1
	PTR_ADDU a1, a3				# a1 now new XKPHYS address
	dsrl	a2, a2, 5			# Number of unrolled loops
1:
	PTR_ADDU a2, -1
	cache	IndexInvalidate_I, 0(a1)
	bne	a2, zero, 1b
	PTR_ADDU a1, 32

	j	ra
	nop
END(Loongson2_InvalidateICache)

/*----------------------------------------------------------------------------
 *
 * Loongson2_SyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
 *
 *	Sync the L1 and L2 data cache page for address pa.
 *	The virtual address is not used.
 *
 *	The index for L1 is the low 14 bits of the virtual address. Since
 *	the page size is 2**14 bits, it is possible to access the page
 *	through any valid address.
 *	The index for L2 is the low 17 bits of the physical address.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_SyncDCachePage, 0)
	sync

	LOAD_XKPHYS(a1, CCA_CACHED)
	or	a1, a2				# a1 now new L1 address
	dsrl	a1, a1, PAGE_SHIFT
	dsll	a1, a1, PAGE_SHIFT		# page align pa
	move	a2, a1				# save for L2

	/* L1 */
	PTR_ADDU a3, a1, PAGE_SIZE-32
1:
	cache	HitWBInvalidate_D, 0(a1)
	bne	a3, a1, 1b
	PTR_ADDU a1, 32

	/* L2 */
	PTR_ADDU a3, a2, PAGE_SIZE-32
2:
	cache	HitWBInvalidate_S, 0(a2)
	bne	a3, a2, 2b
	PTR_ADDU a2, 32

	j	ra
	nop
END(Loongson2_SyncDCachePage)

/*----------------------------------------------------------------------------
 *
 * Loongson2_HitSyncDCache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Sync L1 and L2 data caches for range of va to va + len - 1.
 *	Since L2 is writeback, we need to operate on L1 first, to make sure
 *	L1 is clean. The usual mips strategy of doing L2 first, and then
 *	the L1 orphans, will not work as the orphans would only be pushed
 *	to L2, and not to physical memory.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_HitSyncDCache, 0)
	sync

	beq	a2, zero, 3f			# size is zero!
	PTR_ADDU a2, 31				# Round up
	PTR_ADDU a2, a2, a1			# Add extra from address
	dsrl	a1, a1, 5
	dsll	a1, a1, 5			# align to cacheline boundary
	PTR_SUBU a2, a2, a1
	dsrl	a2, a2, 5			# Compute number of cache lines

	move	a4, a2				# save for L2
	move	a3, a1

	/* L1 */
1:
	PTR_ADDU a2, -1
	cache	HitWBInvalidate_D, 0(a1)
	bne	a2, zero, 1b
	PTR_ADDU a1, 32

	/* L2 */
2:
	PTR_ADDU a4, -1
	cache	HitWBInvalidate_S, 0(a3)
	bne	a4, zero, 2b
	PTR_ADDU a3, 32

3:
	j	ra
	nop
END(Loongson2_HitSyncDCache)

/*----------------------------------------------------------------------------
 *
 * Loongson2_HitInvalidateDCache(struct cpu_info *ci, vaddr_t va, size_t len)
 *
 *	Invalidate L1 and L2 data caches for range of va to va + len - 1.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_HitInvalidateDCache, 0)
	sync

	beq	a2, zero, 3f			# size is zero!
	PTR_ADDU a2, 31				# Round up
	PTR_ADDU a2, a2, a1			# Add extra from address
	dsrl	a1, a1, 5
	dsll	a1, a1, 5			# align to cacheline boundary
	PTR_SUBU a2, a2, a1
	dsrl	a2, a2, 5			# Compute number of cache lines

	move	a4, a2				# save for L2
	move	a3, a1

	/* L1 */
1:
	PTR_ADDU a2, -1
	cache	HitInvalidate_D, 0(a1)
	bne	a2, zero, 1b
	PTR_ADDU a1, 32

	/* L2 */
2:
	PTR_ADDU a4, -1
	cache	HitInvalidate_S, 0(a3)
	bne	a4, zero, 2b
	PTR_ADDU a3, 32

3:
	j	ra
	nop
END(Loongson2_HitInvalidateDCache)

/*----------------------------------------------------------------------------
 *
 * Loongson2_IOSyncDCache(struct cpu_info *ci, vaddr_t va, size_t len, int how)
 *
 *	Invalidate or flush L1 and L2 data caches for range of va to
 *	va + len - 1.
 *
 *	If how == 0 (invalidate):
 *		L1 and L2 caches are invalidated or flushed if the area
 *		does not match the alignment requirements.
 *	If how == 1 (writeback):
 *		L1 and L2 are written back.
 *	If how == 2 (writeback and invalidate):
 *		L1 and L2 are written back to memory and invalidated (flushed).
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_IOSyncDCache, 0)
	sync

	beqz	a3, SyncInv			# Sync PREREAD
	nop

SyncWBInv:
	j	Loongson2_HitSyncDCache
	nop

SyncInv:
	or	t0, a1, a2			# check if invalidate possible
	and	t0, t0, 31			# both address and size must
	bnez	t0, SyncWBInv			# be aligned to the cache size
	nop

	j	Loongson2_HitInvalidateDCache
	nop
END(Loongson2_IOSyncDCache)
@


1.8
log
@Rework the signature of the cache handling routines again. It makes more sense
to pass both the virtual and physical addresses of the page to clean to
SyncDCachePage, which is the only routine using `Index' operations on the data
cache, which might be virtually indexed at some levels but physically indexed
at others. On the other hand, it does not make any sense to pass a physical
address to routines using `Hit' operations (and they were discarding them
anyway).

In addition to making things cleaner, this fixes sporadic userland misbehaviour
(read: SIGSGEV) on RM7000 O2 systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.7 2012/04/06 20:11:18 miod Exp $	*/
@


1.7
log
@Make the logic for PMAP_PREFER() and the logic, inside pmap, to do the
necessary cache coherency work wrt similar virtual indexes of different
physical pages, depending upon two distinct global variables, instead of
a shared one. R4000/R4400 VCE requires a 32KB mask for PMAP_PREFER, which
is otherwise not necessary for pmap coherency (especially since, on these
processors, only L1 uses virtual indexes, and the L1 size is not greater
than the page size, as we are using 16KB pages).
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.6 2012/03/25 13:45:05 miod Exp $	*/
d241 1
a241 3
 * Loongson2_InvalidateICache --
 *
 *	void Loongson2_SyncICache(struct cpu_info *ci, vaddr_t va, size_t len)
d271 1
a271 1
 * Loongson2_SyncDCachePage --
d273 2
a274 1
 *	void Loongson2_SyncDCachePage(struct cpu_info *ci, paddr_t pa)
d276 4
a279 1
 *	Sync the L1 and L2 data cache page for address pa.
d286 1
a286 1
	LOAD_XKPHYS(a2, CCA_CACHED)
d312 1
a312 1
 * Loongson2_HitSyncDCache --
d314 1
a314 4
 *	void Loongson2_HitSyncDCache(struct cpu_info *ci,
 *	    paddr_t pa, size_t len)
 *
 *	Sync L1 and L2 data caches for range of pa to pa + len - 1.
a331 2
	LOAD_XKPHYS(a3, CCA_CACHED)
	or	a1, a3				# build suitable va
d357 1
a357 1
 * Loongson2_HitInvalidateDCache --
d359 1
a359 4
 *	void Loongson2_HitInvalidateDCache(struct cpu_info *ci,
 *	    paddr_t pa, size_t len)
 *
 *	Invalidate L1 and L2 data caches for range of pa to pa + len - 1.
a372 2
	LOAD_XKPHYS(a3, CCA_CACHED)
	or	a1, a3				# build suitable va
d398 1
a398 4
 * Loongson2_IOSyncDCache --
 *
 *	void Loongson2_IOSyncDCache(struct cpu_info *ci,
 *	    paddr_t pa, size_t len, int how)
d400 2
a401 2
 *	Invalidate or flush L1 and L2 data caches for range of pa to
 *	pa + len - 1.
d413 1
a413 1
NON_LEAF(Loongson2_IOSyncDCache, FRAMESZ(CF_SZ+REGSZ), ra)
a415 2
	PTR_SUBU sp, FRAMESZ(CF_SZ+REGSZ)
	PTR_S	ra, CF_RA_OFFS+REGSZ(sp)
d420 1
a420 1
	jal	Loongson2_HitSyncDCache
a421 2
	b	SyncDone
	PTR_L	ra, CF_RA_OFFS+REGSZ(sp)
d429 1
a429 1
	jal	Loongson2_HitInvalidateDCache
a430 5
	PTR_L	ra, CF_RA_OFFS+REGSZ(sp)

SyncDone:
	j	ra
	PTR_ADDU sp, FRAMESZ(CF_SZ+REGSZ)
@


1.6
log
@Only set the low order bits of CpuCacheAliasMask if it is nonzero, regression
of previous computation fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.5 2012/03/19 20:42:26 miod Exp $	*/
d101 1
a101 1
 *	Alignment mask for cache aliasing test is stored in CpuCacheAliasMask.
d160 2
a161 1
	PTR_S	t1, CpuCacheAliasMask
@


1.5
log
@Recent uvm code (and maybe not-so-recent, but it did not explode^WKASSERT at
my face then...) depends upon PMAP_PREFER_ALIGN to be a power of two, minus one.

On mips64 with 4KB pages, the runtime variable used to compute PMAP_PREFER_ALIGN
had the low PAGE_SHIFT bits zeroed (for no good reason I'd say). Don't bother
zeroing them anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.4 2010/01/09 23:34:29 miod Exp $	*/
d155 3
d159 1
@


1.4
log
@Move cache information from global variables to per-cpu_info fields; this
allows processors with different cache sizes to be used.

Cache management routines now take a struct cpu_info * as first parameter.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.3 2009/12/25 21:02:15 miod Exp $	*/
d154 2
a155 2
	srl	t1, t2				# Some cpus have different
	and	t1, ~(PAGE_SIZE - 1)		# i and d cache sizes...
@


1.3
log
@Pass both the virtual address and the physical address of the memory range
when invoking the cache functions. The physical address is needed when
operating on physically-indexed caches, such as the L2 cache on Loongson
processors.

Preprocessor abuse makes sure that the physical address computation gets
compiled out when running on a kernel compiled for virtually-indexed
caches only, such as the sgi kernel.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.2 2009/11/19 20:16:27 miod Exp $	*/
d48 2
a60 4
#define	LOAD_XKPHYS(reg, cca) \
	li	reg, cca | 0x10; \
	dsll	reg, reg, 59

d96 1
a96 1
 * Loongson2_ConfigCache --
d99 2
a100 2
 *	The size of the data cache is stored into CpuPrimaryDataCacheSize.
 *	The size of instruction cache is stored into CpuPrimaryInstCacheSize.
d102 3
a104 3
 *	CpuSecondaryCacheSize is set to the size of the secondary cache.
 *	CpuTertiaryCacheSize is set to the size of the tertiary cache.
 *	CpuNWayCache is set to 0 for direct mapped caches, 2 for two way
d124 1
a124 1
	sw	t2, CpuPrimaryInstCacheLSize
d133 1
a133 1
	sw	t2, CpuPrimaryDataCacheLSize
d145 1
a145 4
	sw	v0, CpuConfigRegister
	mfc0	t3, COP_0_STATUS_REG
	sw	t2, CpuCacheType		# Save cache attributes
	sw	t3, CpuStatusRegister
d147 1
a147 1
	sw	t2, CpuNWayCache
d150 2
a151 2
	sw	ta2, CpuSecondaryCacheSize
	sw	ta3, CpuTertiaryCacheSize
d156 1
a156 1
	sw	t1, CpuCacheAliasMask
d158 2
a159 2
	sw	ta0, CpuPrimaryInstCacheSize	# store cache size.
	sw	ta1, CpuPrimaryDataCacheSize	# store cache size.
d167 2
a168 2
	sw	ta1, CpuPrimaryInstSetSize
	sw	ta1, CpuPrimaryDataSetSize
d176 1
a176 1
 * Loongson2_SyncCache --
d187 1
a187 1
	lw	t1, CpuPrimaryInstCacheSize
d189 1
a189 1
	lw	t2, CpuPrimaryDataCacheSize
d218 1
a218 1
	lw	t2, CpuSecondaryCacheSize
d238 1
a238 1
 *	void Loongson2_SyncICache(vaddr_t va, size_t len)
d247 9
a255 9
	andi	a0, ((1 << 14) - 1)		# only keep index bits
	PTR_ADDU a1, 31				# Round up size
	LOAD_XKPHYS(a2, CCA_CACHED)
	PTR_ADDU a1, a0				# Add extra from address
	dsrl	a0, a0, 5
	dsll	a0, a0, 5			# align address
	PTR_SUBU a1, a1, a0
	PTR_ADDU a0, a2				# a0 now new XKPHYS address
	dsrl	a1, a1, 5			# Number of unrolled loops
d257 4
a260 4
	PTR_ADDU a1, -1
	cache	IndexInvalidate_I, 0(a0)
	bne	a1, zero, 1b
	PTR_ADDU a0, 32
d270 1
a270 1
 *	void Loongson2_SyncDCachePage(paddr_t pa)
d279 5
a283 5
	LOAD_XKPHYS(a1, CCA_CACHED)
	or	a0, a1				# a0 now new L1 address
	dsrl	a0, a0, PAGE_SHIFT
	dsll	a0, a0, PAGE_SHIFT		# page align pa
	move	a1, a0				# save for L2
d286 1
a286 1
	PTR_ADDU a2, a0, PAGE_SIZE-32
d288 3
a290 3
	cache	HitWBInvalidate_D, 0(a0)
	bne	a2, a0, 1b
	PTR_ADDU a0, 32
d293 1
a293 1
	PTR_ADDU a2, a1, PAGE_SIZE-32
d295 3
a297 3
	cache	HitWBInvalidate_S, 0(a1)
	bne	a2, a1, 2b
	PTR_ADDU a1, 32
d307 2
a308 1
 *	void Loongson2_HitSyncDCache(paddr_t pa, size_t len)
d321 9
a329 9
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, 31				# Round up
	PTR_ADDU a1, a1, a0			# Add extra from address
	dsrl	a0, a0, 5
	dsll	a0, a0, 5			# align to cacheline boundary
	PTR_SUBU a1, a1, a0
	dsrl	a1, a1, 5			# Compute number of cache lines
	LOAD_XKPHYS(a2, CCA_CACHED)
	or	a0, a2				# build suitable va
d331 2
a332 2
	move	a3, a1				# save for L2
	move	a2, a0
d336 4
a339 4
	PTR_ADDU a1, -1
	cache	HitWBInvalidate_D, 0(a0)
	bne	a1, zero, 1b
	PTR_ADDU a0, 32
d343 4
a346 4
	PTR_ADDU a3, -1
	cache	HitWBInvalidate_S, 0(a2)
	bne	a3, zero, 2b
	PTR_ADDU a2, 32
d357 2
a358 1
 *	void Loongson2_HitInvalidateDCache(paddr_t pa, size_t len)
d367 9
a375 9
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, 31				# Round up
	PTR_ADDU a1, a1, a0			# Add extra from address
	dsrl	a0, a0, 5
	dsll	a0, a0, 5			# align to cacheline boundary
	PTR_SUBU a1, a1, a0
	dsrl	a1, a1, 5			# Compute number of cache lines
	LOAD_XKPHYS(a2, CCA_CACHED)
	or	a0, a2				# build suitable va
d377 2
a378 2
	move	a3, a1				# save for L2
	move	a2, a0
d382 4
a385 4
	PTR_ADDU a1, -1
	cache	HitInvalidate_D, 0(a0)
	bne	a1, zero, 1b
	PTR_ADDU a0, 32
d389 4
a392 4
	PTR_ADDU a3, -1
	cache	HitInvalidate_S, 0(a2)
	bne	a3, zero, 2b
	PTR_ADDU a2, 32
d403 2
a404 1
 *	void Loongson2_IOSyncDCache(paddr_t pa, size_t len, int how)
d424 1
a424 1
	beqz	a2, SyncInv			# Sync PREREAD
d434 1
a434 1
	or	t0, a0, a1			# check if invalidate possible
@


1.2
log
@Rename KSEG* defines to CKSEG* to match their names in 64 bit mode; also
define more 64 bit spaces.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_loongson2.S,v 1.1 2009/08/06 21:11:39 miod Exp $	*/
d45 3
a47 2
 *  Processors supported:
 *  Loongson 2E/2F
a49 2
#include <sys/syscall.h>

d63 1
a63 4
/*
 *  Skip the .h file. Noone else need to know!
 */

d66 8
a74 3

#define	IndexLoadTag_I		0x04
#define	IndexLoadTag_D		0x05
a75 3

#define	IndexStoreTag_I		0x08
#define	IndexStoreTag_D		0x09
a76 2

#define	HitInvalidate_D		0x11
a77 3

#define	Fill_I			0x14
#define	HitWBInvalidate_D	0x15
d79 2
a80 4

#define	HitWB_I			0x18
#define	HitWB_D			0x19
#define	HitWB_S			0x1b
d100 1
a100 4
 *	Size and configure the caches.
 *	NOTE: should only be called from mips_init().
 *
 * Side effects:
a115 7
	.set	noreorder
	LA	v0, 1f
	LA	v1, CKSEG1_BASE
	or	v0, v1
	jr	v0				# Switch to uncached.
	nop
1:
d139 1
a139 13
	LOAD_XKPHYS(t0, CCA_CACHED)
	li	ta2, 128 * 1024			# fixed L2 size...
	PTR_ADDU t1, t0, ta2
	sll	ta2, 2				# ...is 512KB
1:
	cache	IndexWBInvalidate_S, 0(t0)
	cache	IndexWBInvalidate_S, 1(t0)
	cache	IndexWBInvalidate_S, 2(t0)
	cache	IndexWBInvalidate_S, 3(t0)
	PTR_ADDU t0, t0, 32
	bne	t0, t1, 1b
	nop

d160 1
a160 1
	and	t1, ~(NBPG - 1)			# i and d cache sizes...
d166 5
a170 2
	/* Loongson 2E/2F: cache way information is in the lowest bits */
	sw	zero, CpuPrimaryInstSetSize
d172 1
a174 3
	and	v0, ~7
	or	v0, CCA_CACHED			# set cachable writeback kseg0
	mtc0	v0, COP_0_CONFIG		# establish any new config
a186 6
 * Results:
 *	None.
 *
 * Side effects:
 *	The contents of ALL caches are Invalidated or Synched.
 *
d190 2
a191 1
	.set	noreorder
d193 1
d195 3
a198 3
/*
 * Sync the instruction cache.
 */
d201 1
a201 2
	PTR_SUBU t1, 128

a203 4
	cache	IndexInvalidate_I, 32(t0)
	cache	IndexInvalidate_I, 64(t0)
	cache	IndexInvalidate_I, 96(t0)

d205 1
a205 1
	PTR_ADDU t0, t0, 128
d207 1
a207 4
/*
 * Sync the data cache. Do L1 first. Indexed only operate on
 * the selected cache and differs from Hit in that sense.
 */
d211 1
a211 1
	PTR_SUBU t1, t1, 128
d214 5
a218 3
	cache	IndexWBInvalidate_D, 32(t0)
	cache	IndexWBInvalidate_D, 64(t0)
	cache	IndexWBInvalidate_D, 96(t0)
d220 12
d233 1
a233 14
	PTR_ADDU t0, t0, 128

/* Do on chip L2 */
	LOAD_XKPHYS(t3, CCA_CACHED)
	lw	ta0, CpuSecondaryCacheSize
	sll	ta0, 2				# because cache is 4 way
10:
	cache	IndexWBInvalidate_S, 0(t3)
	cache	IndexWBInvalidate_S, 1(t3)
	cache	IndexWBInvalidate_S, 2(t3)
	cache	IndexWBInvalidate_S, 3(t3)
	PTR_SUBU ta0, 32			# Fixed cache line size.
	bgtz	ta0, 10b
	PTR_ADDU t3, 32
d243 1
a243 2
 *	void Loongson2_SyncICache(addr, len)
 *		vaddr_t addr, len;
d246 1
a246 1
 *	of addr to addr + len - 1.
a248 7
 * Results:
 *	None.
 *
 * Side effects:
 *	The contents of the L1 Instruction cache is flushed.
 *	Must not touch v0.
 *
d252 2
a253 2
	/* Loongson2: I$ index ops affect all ways */
	and	a0, 0x00ffffff			# Reduce addr to cache index
a254 1
	PTR_ADDU a1, 31				# Round up size
d256 2
a257 1
	and	a0, -32				# Align start address
d260 1
a260 1
	srl	a1, a1, 5			# Number of unrolled loops
d268 1
a268 1
	move	v0, zero
d275 1
a275 2
 *	void Loongson2_SyncDCachePage(addr)
 *		vaddr_t addr;
d277 1
a277 9
 *	Sync the L1 data cache page for address addr.
 *	The address is reduced to a XKPHYS index to avoid TLB faults.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The contents of the cache is written back to primary memory.
 *	The cache line is invalidated.
d282 7
a288 6
	LOAD_XKPHYS(a2, CCA_CACHED)
	dsll	a0, 34
	dsrl	a0, 34
	PTR_ADDU a0, a2				# a0 now new XKPHYS address
	and	a0, ~PAGE_MASK			# Page align start address
	PTR_ADDU a1, a0, PAGE_SIZE-128
d290 2
d293 3
a295 19
	cache	IndexWBInvalidate_D, 0(a0)	# do set A
	cache	IndexWBInvalidate_D, 32(a0)
	cache	IndexWBInvalidate_D, 64(a0)
	cache	IndexWBInvalidate_D, 96(a0)

	cache	IndexWBInvalidate_D, 1(a0)	# do set B
	cache	IndexWBInvalidate_D, 33(a0)
	cache	IndexWBInvalidate_D, 65(a0)
	cache	IndexWBInvalidate_D, 97(a0)

	cache	IndexWBInvalidate_D, 2(a0)	# do set C
	cache	IndexWBInvalidate_D, 34(a0)
	cache	IndexWBInvalidate_D, 66(a0)
	cache	IndexWBInvalidate_D, 98(a0)

	cache	IndexWBInvalidate_D, 3(a0)	# do set D
	cache	IndexWBInvalidate_D, 35(a0)
	cache	IndexWBInvalidate_D, 67(a0)
	cache	IndexWBInvalidate_D, 99(a0)
d297 6
a302 2
	bne	a1, a0, 1b
	PTR_ADDU a0, 128
d312 1
a312 2
 *	void Loongson2_HitSyncDCache(addr, len)
 *		vaddr_t addr, len;
d314 5
a318 15
 *	Sync data cache for range of addr to addr + len - 1.
 *	The address can be any valid virtual address as long
 *	as no TLB invalid traps occur. Only lines with matching
 *	addr are flushed.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The contents of the L1 cache is written back to primary memory.
 *	The cache line is invalidated.
 *
 * IMPORTANT NOTE:
 *	Since orphaned L1 cache entries will not be synched it is
 *	mandatory to pass over the L1 cache once after the L2 is done.
d323 2
d328 2
a329 1
	and	a0, a0, -32			# align address
d331 3
a333 1
	srl	a1, a1, 5			# Compute number of cache lines
d335 4
d345 7
a356 45

/*----------------------------------------------------------------------------
 *
 * Loongson2_HitSyncSCache --
 *
 *	void Loongson2_HitSyncSCache(addr, len)
 *		vaddr_t addr, len;
 *
 *	Sync secondary cache for range of addr to addr + len - 1.
 *	The address can be any valid virtual address as long
 *	as no TLB invalid traps occur. Only lines with matching
 *	addr are flushed.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The contents of the L2 cache is written back to primary memory.
 *	The cache line is invalidated.
 *
 * IMPORTANT NOTE:
 *	Since orphaned L1 cache entries will not be synched it is
 *	mandatory to pass over the L1 cache once after the L2 is done.
 *
 *----------------------------------------------------------------------------
 */
LEAF(Loongson2_HitSyncSCache, 0)
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, a1, a0			# Add in extra from align
	and	a0, a0, -32			# Align address
	PTR_SUBU a1, a1, a0
1:
	PTR_ADDU a1, -32

	cache	HitWBInvalidate_S, 0(a0)
	cache	HitWBInvalidate_D, 0(a0)	# Kill any orphans...

	bgtz	a1, 1b
	PTR_ADDU a0, 32

3:
	j	ra
	nop
END(Loongson2_HitSyncSCache)

d361 1
a361 7
 *	void Loongson2_HitInvalidateDCache(addr, len)
 *		vaddr_t addr, len;
 *
 *	Invalidate data cache for range of addr to addr + len - 1.
 *	The address can be any valid address as long as no TLB misses occur.
 *	(Be sure to use cached K0SEG kernel addresses or mapped addresses)
 *	Only lines with matching addresses are invalidated.
d363 1
a363 5
 * Results:
 *	None.
 *
 * Side effects:
 *	The L1 cache line is invalidated.
d368 2
d371 4
a374 2
	PTR_ADDU a1, a1, a0			# Add in extra from align
	and	a0, a0, -32			# Align address
d376 6
d383 1
d385 1
a385 2
	PTR_ADDU a1, -32

d387 2
d390 6
a395 2
	bgtz	a1, 1b
	PTR_ADDU a0, 32
a401 1

d404 1
a404 1
 * Loongson2_HitInvalidateSCache --
d406 1
a406 2
 *	void Loongson2_HitInvalidateSCache(addr, len)
 *		vaddr_t addr, len;
d408 2
a409 4
 *	Invalidate secondary cache for range of addr to addr + len - 1.
 *	The address can be any valid address as long as no TLB misses occur.
 *	(Be sure to use cached K0SEG kernel addresses or mapped addresses)
 *	Only lines with matching addresses are invalidated.
d411 7
a417 5
 * Results:
 *	None.
 *
 * Side effects:
 *	The L2 cache line is invalidated.
d421 2
a422 7
LEAF(Loongson2_HitInvalidateSCache, 0)
	beq	a1, zero, 3f			# size is zero!
	PTR_ADDU a1, a1, a0			# Add in extra from align
	and	a0, a0, -32			# Align address
	PTR_SUBU a1, a1, a0
1:
	PTR_ADDU a1, -32
d424 4
a427 5
	cache	HitInvalidate_S, 0(a0)
	cache	HitInvalidate_D, 0(a0)		# Orphans in L1

	bgtz	a1, 1b
	PTR_ADDU a0, 32
d429 2
a430 2
3:
	j	ra
a431 45
END(Loongson2_HitInvalidateSCache)

/*----------------------------------------------------------------------------
 *
 * Loongson2_IOSyncDCache --
 *
 *	void Loongson2_IOSyncDCache(addr, len, rw)
 *		vaddr_t addr;
 *		int len, rw;
 *
 *	Invalidate or flush data cache for range of addr to addr + len - 1.
 *	The address can be any valid address as long as no TLB misses occur.
 *	(Be sure to use cached K0SEG kernel addresses or mapped addresses)
 *
 *	In case of the existence of an external cache we invalidate pages
 *	which are in the given range ONLY if transfer direction is READ.
 *	The assumption here is a 'write through' external cache which is
 *	true for all now supported processors.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	If rw == 0 (read), L1 and on-chip L2 caches are invalidated or
 *		flushed if the area does not match the alignment
 *		requirements.
 *	If rw == 1 (write) or rw == 2 (write-read), L1 and on-chip L2 caches
 *		are written back to memory and invalidated.
 *
 *----------------------------------------------------------------------------
 */
NON_LEAF(Loongson2_IOSyncDCache, FRAMESZ(CF_SZ+2*REGSZ), ra)

	PTR_SUBU sp, FRAMESZ(CF_SZ+2*REGSZ)
	PTR_S	ra, CF_RA_OFFS+2*REGSZ(sp)
	REG_S	a0, CF_ARGSZ(sp)		# save args
	beqz	a2, SyncRD			# Sync PREREAD
	REG_S	a1, CF_ARGSZ+REGSZ(sp)

/*
 *  Sync for unaligned read or write-read.
 */
SyncRDWB:
	jal	Loongson2_HitSyncSCache		# Do internal L2 cache
	nop					# L1 done in parallel
d433 1
a433 1
	PTR_L	ra, CF_RA_OFFS+2*REGSZ(sp)
d435 4
a438 5
SyncRD:
	and	t0, a0, 31			# check if invalidate possible
	bnez	t0, SyncRDWB			# both address and size must
	and	t0, a1, 31			# be aligned at the cache size
	bnez	t0, SyncRDWB
d441 3
a443 7
/*
 *  Sync for aligned read, no writeback required.
 */
	jal	Loongson2_HitInvalidateSCache	# Internal L2 cache
	nop					# L1 done in parallel

	PTR_L	ra, CF_RA_OFFS+2*REGSZ(sp)
d447 1
a447 1
	PTR_ADDU sp, FRAMESZ(CF_SZ+2*REGSZ)
@


1.1
log
@Work in progress support for Loongson2E/2F processors; need option CPU_LOONGSON2
in the kernel to be brought in, due to invasive differences in tlb operation.
Comes with a separate cache operations file due to the cache being R5k-style
with R10k-style way number encoding.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d130 1
a130 1
	LA	v1, KSEG1_BASE
@

