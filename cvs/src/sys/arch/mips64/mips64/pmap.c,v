head	1.102;
access;
symbols
	OPENBSD_6_1:1.101.0.4
	OPENBSD_6_1_BASE:1.101
	OPENBSD_6_0:1.90.0.2
	OPENBSD_6_0_BASE:1.90
	OPENBSD_5_9:1.86.0.2
	OPENBSD_5_9_BASE:1.86
	OPENBSD_5_8:1.80.0.4
	OPENBSD_5_8_BASE:1.80
	OPENBSD_5_7:1.78.0.2
	OPENBSD_5_7_BASE:1.78
	OPENBSD_5_6:1.75.0.4
	OPENBSD_5_6_BASE:1.75
	OPENBSD_5_5:1.68.0.4
	OPENBSD_5_5_BASE:1.68
	OPENBSD_5_4:1.66.0.4
	OPENBSD_5_4_BASE:1.66
	OPENBSD_5_3:1.66.0.2
	OPENBSD_5_3_BASE:1.66
	OPENBSD_5_2:1.63.0.2
	OPENBSD_5_2_BASE:1.63
	OPENBSD_5_1_BASE:1.54
	OPENBSD_5_1:1.54.0.2
	OPENBSD_5_0:1.53.0.2
	OPENBSD_5_0_BASE:1.53
	OPENBSD_4_9:1.52.0.2
	OPENBSD_4_9_BASE:1.52
	OPENBSD_4_8:1.49.0.4
	OPENBSD_4_8_BASE:1.49
	OPENBSD_4_7:1.49.0.2
	OPENBSD_4_7_BASE:1.49
	OPENBSD_4_6:1.36.0.4
	OPENBSD_4_6_BASE:1.36
	OPENBSD_4_5:1.35.0.2
	OPENBSD_4_5_BASE:1.35
	OPENBSD_4_4:1.34.0.2
	OPENBSD_4_4_BASE:1.34
	OPENBSD_4_3:1.32.0.2
	OPENBSD_4_3_BASE:1.32
	OPENBSD_4_2:1.29.0.2
	OPENBSD_4_2_BASE:1.29
	OPENBSD_4_1:1.20.0.4
	OPENBSD_4_1_BASE:1.20
	OPENBSD_4_0:1.20.0.2
	OPENBSD_4_0_BASE:1.20
	OPENBSD_3_9:1.19.0.2
	OPENBSD_3_9_BASE:1.19
	OPENBSD_3_8:1.17.0.4
	OPENBSD_3_8_BASE:1.17
	OPENBSD_3_7:1.17.0.2
	OPENBSD_3_7_BASE:1.17
	OPENBSD_3_6:1.7.0.2
	OPENBSD_3_6_BASE:1.7;
locks; strict;
comment	@ * @;


1.102
date	2017.05.27.06.23.49;	author visa;	state Exp;
branches;
next	1.101;
commitid	kM0O2PeCHkV3k0ql;

1.101
date	2017.01.21.05.42.03;	author guenther;	state Exp;
branches;
next	1.100;
commitid	CHRb0fCqa8XxUAMH;

1.100
date	2017.01.02.18.19.34;	author visa;	state Exp;
branches;
next	1.99;
commitid	JrKb0mnWakH8wWRY;

1.99
date	2016.12.30.12.50.38;	author visa;	state Exp;
branches;
next	1.98;
commitid	atEqsgCt3pLCfH7E;

1.98
date	2016.12.30.12.42.27;	author visa;	state Exp;
branches;
next	1.97;
commitid	ilfKxh86OpI5TLMN;

1.97
date	2016.12.23.12.38.16;	author visa;	state Exp;
branches;
next	1.96;
commitid	2zEjAh909elqzG9L;

1.96
date	2016.12.22.15.33.36;	author visa;	state Exp;
branches;
next	1.95;
commitid	sw48d58czeutOmw2;

1.95
date	2016.11.21.13.50.22;	author visa;	state Exp;
branches;
next	1.94;
commitid	wIRejxveZtszCzwI;

1.94
date	2016.10.19.08.28.20;	author guenther;	state Exp;
branches;
next	1.93;
commitid	VoR9X3uHTxRSYC5r;

1.93
date	2016.10.08.05.49.09;	author guenther;	state Exp;
branches;
next	1.92;
commitid	z63v1DilayzHcfkw;

1.92
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.91;
commitid	RlO92XR575sygHqm;

1.91
date	2016.08.14.08.23.52;	author visa;	state Exp;
branches;
next	1.90;
commitid	at8t1tG4p7ou4Cj7;

1.90
date	2016.05.11.15.50.29;	author visa;	state Exp;
branches;
next	1.89;
commitid	y3u3BwlBdCPiPv83;

1.89
date	2016.04.24.13.35.45;	author visa;	state Exp;
branches;
next	1.88;
commitid	DelAyfqRJDQC48wN;

1.88
date	2016.04.24.04.25.03;	author visa;	state Exp;
branches;
next	1.87;
commitid	4SPr8onH3V5nhYod;

1.87
date	2016.04.23.12.53.28;	author visa;	state Exp;
branches;
next	1.86;
commitid	QKJRzveJj6rn02sF;

1.86
date	2016.02.01.16.15.18;	author visa;	state Exp;
branches;
next	1.85;
commitid	fDCG9fyhqhLX230e;

1.85
date	2016.01.10.10.22.56;	author visa;	state Exp;
branches;
next	1.84;
commitid	DXqb5NK6W32O0C6j;

1.84
date	2016.01.05.05.42.27;	author visa;	state Exp;
branches;
next	1.83;
commitid	ukfbQWS96rGQ8PtJ;

1.83
date	2016.01.05.05.27.54;	author visa;	state Exp;
branches;
next	1.82;
commitid	916i76I5mjNtTg33;

1.82
date	2015.12.31.04.25.51;	author visa;	state Exp;
branches;
next	1.81;
commitid	dVASB3mipffYF1KI;

1.81
date	2015.08.11.13.15.36;	author visa;	state Exp;
branches;
next	1.80;
commitid	d81Y8ajhJKGtpnuo;

1.80
date	2015.07.30.17.02.17;	author visa;	state Exp;
branches;
next	1.79;
commitid	XYIDuzLFr8gj5MG7;

1.79
date	2015.05.02.14.33.19;	author jsg;	state Exp;
branches;
next	1.78;
commitid	KFUSxRkAiP0GdiBd;

1.78
date	2014.12.17.15.05.54;	author deraadt;	state Exp;
branches;
next	1.77;
commitid	sW5BvVrBde2bHGC9;

1.77
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.76;
commitid	yv0ECmCdICvq576h;

1.76
date	2014.09.30.06.51.58;	author jmatthew;	state Exp;
branches;
next	1.75;
commitid	pUEUpP9FlbomZUiI;

1.75
date	2014.05.10.22.25.16;	author jasper;	state Exp;
branches;
next	1.74;

1.74
date	2014.04.04.20.52.05;	author miod;	state Exp;
branches;
next	1.73;

1.73
date	2014.03.31.20.21.19;	author miod;	state Exp;
branches;
next	1.72;

1.72
date	2014.03.22.00.01.04;	author miod;	state Exp;
branches;
next	1.71;

1.71
date	2014.03.21.21.49.45;	author miod;	state Exp;
branches;
next	1.70;

1.70
date	2014.03.21.21.39.35;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2014.03.10.21.17.58;	author miod;	state Exp;
branches;
next	1.68;

1.68
date	2014.02.08.09.34.04;	author miod;	state Exp;
branches;
next	1.67;

1.67
date	2014.01.08.17.12.18;	author miod;	state Exp;
branches;
next	1.66;

1.66
date	2012.10.03.22.46.07;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2012.09.29.21.37.03;	author miod;	state Exp;
branches;
next	1.64;

1.64
date	2012.09.29.19.11.08;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2012.05.10.21.12.26;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2012.04.25.22.07.35;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2012.04.24.20.02.03;	author miod;	state Exp;
branches;
next	1.60;

1.60
date	2012.04.21.12.20.30;	author miod;	state Exp;
branches;
next	1.59;

1.59
date	2012.04.19.18.12.40;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2012.04.06.20.11.18;	author miod;	state Exp;
branches;
next	1.57;

1.57
date	2012.03.25.13.52.52;	author miod;	state Exp;
branches;
next	1.56;

1.56
date	2012.03.19.21.56.49;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2012.03.19.20.42.26;	author miod;	state Exp;
branches;
next	1.54;

1.54
date	2011.09.22.17.41.00;	author jasper;	state Exp;
branches;
next	1.53;

1.53
date	2011.04.07.18.11.52;	author miod;	state Exp;
branches;
next	1.52;

1.52
date	2010.12.06.20.57.17;	author miod;	state Exp;
branches;
next	1.51;

1.51
date	2010.11.28.20.30.54;	author miod;	state Exp;
branches;
next	1.50;

1.50
date	2010.11.24.20.59.19;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2010.02.02.02.49.57;	author syuu;	state Exp;
branches;
next	1.48;

1.48
date	2010.01.09.23.34.29;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2010.01.09.20.33.16;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2010.01.05.06.44.58;	author syuu;	state Exp;
branches;
next	1.45;

1.45
date	2009.12.30.01.17.59;	author syuu;	state Exp;
branches;
next	1.44;

1.44
date	2009.12.28.07.18.39;	author syuu;	state Exp;
branches;
next	1.43;

1.43
date	2009.12.28.06.55.27;	author syuu;	state Exp;
branches;
next	1.42;

1.42
date	2009.12.25.21.02.15;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2009.12.07.18.58.34;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2009.11.22.00.07.04;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2009.11.19.20.16.27;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2009.11.18.20.58.51;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2009.07.23.19.24.55;	author miod;	state Exp;
branches;
next	1.36;

1.36
date	2009.04.25.20.35.31;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2008.09.23.04.34.02;	author miod;	state Exp;
branches;
next	1.34;

1.34
date	2008.06.14.10.55.20;	author mk;	state Exp;
branches;
next	1.33;

1.33
date	2008.04.07.22.30.48;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2008.02.11.20.40.32;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2008.01.15.19.44.50;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2007.10.18.04.32.08;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2007.07.18.20.06.07;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2007.06.20.16.51.17;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2007.05.03.19.34.00;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2007.04.27.18.17.19;	author miod;	state Exp;
branches;
next	1.25;

1.25
date	2007.04.27.18.14.13;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2007.04.24.16.48.45;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2007.04.20.16.03.55;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2007.04.14.14.54.30;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2007.04.14.14.52.39;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2006.06.06.17.34.21;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2006.01.04.20.26.46;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2005.12.10.11.45.43;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2004.10.20.12.49.15;	author pefo;	state Exp;
branches;
next	1.16;

1.16
date	2004.09.30.07.25.54;	author pefo;	state Exp;
branches;
next	1.15;

1.15
date	2004.09.29.17.39.20;	author pefo;	state Exp;
branches;
next	1.14;

1.14
date	2004.09.27.17.40.24;	author pefo;	state Exp;
branches;
next	1.13;

1.13
date	2004.09.23.12.38.28;	author pefo;	state Exp;
branches;
next	1.12;

1.12
date	2004.09.23.08.42.38;	author pefo;	state Exp;
branches;
next	1.11;

1.11
date	2004.09.22.07.27.22;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2004.09.21.05.54.31;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2004.09.17.19.19.08;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2004.09.16.07.25.26;	author miod;	state Exp;
branches;
next	1.7;

1.7
date	2004.09.09.22.11.38;	author pefo;	state Exp;
branches;
next	1.6;

1.6
date	2004.08.15.18.35.12;	author pefo;	state Exp;
branches;
next	1.5;

1.5
date	2004.08.11.10.21.08;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2004.08.10.20.28.13;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	2004.08.10.20.15.47;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2004.08.09.14.57.26;	author pefo;	state Exp;
branches;
next	1.1;

1.1
date	2004.08.06.20.56.03;	author pefo;	state Exp;
branches;
next	;


desc
@@


1.102
log
@Check cache_valias_mask earlier in the expression. The value is zero
on most systems, so this tweak should save an iota of CPU time.
@
text
@/*	$OpenBSD: pmap.c,v 1.101 2017/01/21 05:42:03 guenther Exp $	*/

/*
 * Copyright (c) 2001-2004 Opsycon AB  (www.opsycon.se / www.opsycon.com)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/user.h>
#include <sys/buf.h>
#include <sys/pool.h>
#ifdef SYSVSHM
#include <sys/shm.h>
#endif
#include <sys/atomic.h>

#include <mips64/cache.h>
#include <mips64/mips_cpu.h>
#include <machine/autoconf.h>
#include <machine/cpu.h>
#include <machine/vmparam.h>

#include <uvm/uvm.h>

extern void mem_zero_page(vaddr_t);

struct pool pmap_pmap_pool;
struct pool pmap_pv_pool;
struct pool pmap_pg_pool;

#define pmap_pv_alloc()		(pv_entry_t)pool_get(&pmap_pv_pool, PR_NOWAIT)
#define pmap_pv_free(pv)	pool_put(&pmap_pv_pool, (pv))

#ifndef PMAP_PV_LOWAT
#define PMAP_PV_LOWAT   16
#endif
int	pmap_pv_lowat = PMAP_PV_LOWAT;

uint	 pmap_alloc_tlbpid(struct proc *);
void	 pmap_do_page_cache(vm_page_t, u_int);
void	 pmap_do_remove(pmap_t, vaddr_t, vaddr_t);
int	 pmap_enter_pv(pmap_t, vaddr_t, vm_page_t, pt_entry_t *);
void	 pmap_remove_pv(pmap_t, vaddr_t, paddr_t);
void	 pmap_page_remove(struct vm_page *);
void	 pmap_page_wrprotect(struct vm_page *, vm_prot_t);
void	*pmap_pg_alloc(struct pool *, int, int *);
void	 pmap_pg_free(struct pool *, void *);

struct pool_allocator pmap_pg_allocator = {
	pmap_pg_alloc, pmap_pg_free
};

#define	pmap_invalidate_kernel_page(va) tlb_flush_addr(va)
#define	pmap_update_kernel_page(va, entry) tlb_update((va), (entry))

void	pmap_invalidate_user_page(pmap_t, vaddr_t);
void	pmap_invalidate_icache(pmap_t, vaddr_t, pt_entry_t);
void	pmap_update_user_page(pmap_t, vaddr_t, pt_entry_t);
#ifdef MULTIPROCESSOR
void	pmap_invalidate_icache_action(void *);
void	pmap_shootdown_page(pmap_t, vaddr_t);
void	pmap_shootdown_page_action(void *);
#else
#define	pmap_shootdown_page(pmap, va)	do { /* nothing */ } while (0)
#endif

#ifdef PMAPDEBUG
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
	int cachehit;	/* new entry forced valid entry out */
} enter_stats;
struct {
	int calls;
	int removes;
	int flushes;
	int pidflushes;	/* HW pid stolen */
	int pvfirst;
	int pvsearch;
} remove_stats;

#define PDB_FOLLOW	0x0001
#define PDB_INIT	0x0002
#define PDB_ENTER	0x0004
#define PDB_REMOVE	0x0008
#define PDB_CREATE	0x0010
#define PDB_PTPAGE	0x0020
#define PDB_PVENTRY	0x0040
#define PDB_BITS	0x0080
#define PDB_COLLECT	0x0100
#define PDB_PROTECT	0x0200
#define PDB_TLBPID	0x0400
#define PDB_PARANOIA	0x2000
#define PDB_WIRING	0x4000
#define PDB_PVDUMP	0x8000

#define DPRINTF(flag, printdata)	\
	if (pmapdebug & (flag)) 	\
		printf printdata;

#define stat_count(what)	atomic_inc_int(&(what))
int pmapdebug = PDB_ENTER|PDB_FOLLOW;

#else

#define DPRINTF(flag, printdata)
#define stat_count(what)

#endif	/* PMAPDEBUG */

static struct pmap	kernel_pmap_store
	[(PMAP_SIZEOF(MAXCPUS) + sizeof(struct pmap) - 1)
		/ sizeof(struct pmap)];
struct pmap *const kernel_pmap_ptr = kernel_pmap_store;

vaddr_t	virtual_start;  /* VA of first avail page (after kernel bss)*/
vaddr_t	virtual_end;	/* VA of last avail page (end of kernel AS) */

vaddr_t	pmap_prefer_mask;

static struct pmap_asid_info pmap_asid_info[MAXCPUS];

pt_entry_t	*Sysmap;		/* kernel pte table */
u_int		Sysmapsize;		/* number of pte's in Sysmap */
const vaddr_t	Sysmapbase = VM_MIN_KERNEL_ADDRESS;	/* for libkvm */

pt_entry_t	pg_xi;

void
pmap_invalidate_user_page(pmap_t pmap, vaddr_t va)
{
	u_long cpuid = cpu_number();
	u_long asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;

	if (pmap->pm_asid[cpuid].pma_asidgen ==
	    pmap_asid_info[cpuid].pma_asidgen) {
#ifdef CPU_R4000
		if (r4000_errata != 0)
			eop_tlb_flush_addr(pmap, va, asid);
		else
#endif
			tlb_flush_addr(va | asid);
	}
}

void
pmap_update_user_page(pmap_t pmap, vaddr_t va, pt_entry_t entry)
{
	u_long cpuid = cpu_number();
	u_long asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;

	if (pmap->pm_asid[cpuid].pma_asidgen ==
	    pmap_asid_info[cpuid].pma_asidgen)
		tlb_update(va | asid, entry);
}

#ifdef MULTIPROCESSOR

#define PMAP_ASSERT_LOCKED(pm)						\
do {									\
	if ((pm) != pmap_kernel())					\
		MUTEX_ASSERT_LOCKED(&(pm)->pm_mtx);			\
} while (0)

static inline void
pmap_lock(pmap_t pmap)
{
	if (pmap != pmap_kernel())
		mtx_enter(&pmap->pm_mtx);
}

static inline void
pmap_unlock(pmap_t pmap)
{
	if (pmap != pmap_kernel())
		mtx_leave(&pmap->pm_mtx);
}

static inline pt_entry_t
pmap_pte_cas(pt_entry_t *pte, pt_entry_t o, pt_entry_t n)
{
#ifdef MIPS_PTE64
	return atomic_cas_ulong((unsigned long *)pte, o, n);
#else
	return atomic_cas_uint((unsigned int *)pte, o, n);
#endif
}

struct pmap_invalidate_icache_arg {
	vaddr_t		va;
	pt_entry_t	entry;
};

void
pmap_invalidate_icache(pmap_t pmap, vaddr_t va, pt_entry_t entry)
{
	struct pmap_invalidate_icache_arg ii_args;
	unsigned long cpuid = cpu_number();
	unsigned long cpumask = 0;
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;

	CPU_INFO_FOREACH(cii, ci) {
		if (cpuset_isset(&cpus_running, ci) &&
		    pmap->pm_asid[ci->ci_cpuid].pma_asidgen != 0)
			cpumask |= 1ul << ci->ci_cpuid;
	}

	if (cpumask == 1ul << cpuid) {
		ci = curcpu();
		Mips_SyncDCachePage(ci, va, pfn_to_pad(entry));
		Mips_InvalidateICache(ci, va, PAGE_SIZE);
	} else if (cpumask != 0) {
		ii_args.va = va;
		ii_args.entry = entry;
		smp_rendezvous_cpus(cpumask, pmap_invalidate_icache_action,
		    &ii_args);
	}
}

void
pmap_invalidate_icache_action(void *arg)
{
	struct cpu_info *ci = curcpu();
	struct pmap_invalidate_icache_arg *ii_args = arg;

	Mips_SyncDCachePage(ci, ii_args->va, pfn_to_pad(ii_args->entry));
	Mips_InvalidateICache(ci, ii_args->va, PAGE_SIZE);
}

struct pmap_shootdown_page_arg {
	pmap_t		pmap;
	vaddr_t		va;
};

void
pmap_shootdown_page(pmap_t pmap, vaddr_t va)
{
	struct pmap_shootdown_page_arg sp_arg;
	struct cpu_info *ci, *self = curcpu();
	CPU_INFO_ITERATOR cii;
	unsigned int cpumask = 0;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci == self)
			continue;
		if (!cpuset_isset(&cpus_running, ci))
			continue;
		if (pmap != pmap_kernel()) {
			if (pmap->pm_asid[ci->ci_cpuid].pma_asidgen !=
			    pmap_asid_info[ci->ci_cpuid].pma_asidgen) {
				continue;
			} else if (ci->ci_curpmap != pmap) {
				pmap->pm_asid[ci->ci_cpuid].pma_asidgen = 0;
				continue;
			}
		}
		cpumask |= 1 << ci->ci_cpuid;
	}
	if (cpumask != 0) {
		sp_arg.pmap = pmap;
		sp_arg.va = va;
		smp_rendezvous_cpus(cpumask, pmap_shootdown_page_action,
		    &sp_arg);
	}
}

void
pmap_shootdown_page_action(void *arg)
{
	struct pmap_shootdown_page_arg *sp_arg = arg;

	if (sp_arg->pmap == pmap_kernel())
		pmap_invalidate_kernel_page(sp_arg->va);
	else
		pmap_invalidate_user_page(sp_arg->pmap, sp_arg->va);
}

#else /* MULTIPROCESSOR */

#define PMAP_ASSERT_LOCKED(pm)	do { /* nothing */ } while (0)
#define pmap_lock(pm)		do { /* nothing */ } while (0)
#define pmap_unlock(pm)		do { /* nothing */ } while (0)

void
pmap_invalidate_icache(pmap_t pmap, vaddr_t va, pt_entry_t entry)
{
	struct cpu_info *ci = curcpu();

	Mips_SyncDCachePage(ci, va, pfn_to_pad(entry));
	Mips_InvalidateICache(ci, va, PAGE_SIZE);
}

#endif /* MULTIPROCESSOR */

/*
 *	Bootstrap the system enough to run with virtual memory.
 */
void
pmap_bootstrap(void)
{
	u_int i;
#ifndef CPU_R8000
	pt_entry_t *spte;
#endif

	/*
	 * Create a mapping table for kernel virtual memory. This
	 * table is a linear table in contrast to the user process
	 * mapping tables which are built with segment/page tables.
	 * Create 1GB of map (this will only use 1MB of memory).
	 */
	virtual_start = VM_MIN_KERNEL_ADDRESS;
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	Sysmapsize = (VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS) /
	    PAGE_SIZE;
#ifndef CPU_R8000
	if (Sysmapsize & 1)
		Sysmapsize++;	/* force even number of pages */
#endif

	Sysmap = (pt_entry_t *)
	    uvm_pageboot_alloc(sizeof(pt_entry_t) * Sysmapsize);

	pool_init(&pmap_pmap_pool, PMAP_SIZEOF(ncpusfound), 0, IPL_NONE, 0,
	    "pmappl", NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pvpl", NULL);
	pool_init(&pmap_pg_pool, PMAP_PGSIZE, PMAP_PGSIZE, IPL_VM, 0,
	    "pmappgpl", &pmap_pg_allocator);

	pmap_kernel()->pm_count = 1;

#ifndef CPU_R8000
	/*
	 * The 64 bit Mips architecture stores the AND result
	 * of the Global bits in the pte pair in the on chip
	 * translation lookaside buffer. Thus invalid entries
	 * must have the Global bit set so when Entry LO and
	 * Entry HI G bits are ANDed together they will produce
	 * a global bit to store in the tlb.
	 */
	for (i = Sysmapsize, spte = Sysmap; i != 0; i--, spte++)
		*spte = PG_G;
#else
	bzero(Sysmap, sizeof(pt_entry_t) * Sysmapsize);
#endif
	tlb_set_gbase((vaddr_t)Sysmap, Sysmapsize);

	for (i = 0; i < MAXCPUS; i++) {
		pmap_asid_info[i].pma_asidgen = 1;
		pmap_asid_info[i].pma_asid = MIN_USER_ASID + 1;
	}

#if defined(CPU_MIPS64R2) && !defined(CPU_LOONGSON2)
	if (cp0_get_pagegrain() & PGRAIN_XIE)
		pg_xi = PG_XI;
#endif
}

/*
 *  Page steal allocator used during bootup.
 */
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *vstartp, vaddr_t *vendp)
{
	int i, j;
	uint npg;
	vaddr_t va;
	paddr_t pa;

#ifdef DIAGNOSTIC
	if (uvm.page_init_done) {
		panic("pmap_steal_memory: too late, vm is running!");
	}
#endif

	size = round_page(size);
	npg = atop(size);
	va = 0;

	for(i = 0; i < vm_nphysseg && va == 0; i++) {
		if (vm_physmem[i].avail_start != vm_physmem[i].start ||
		    vm_physmem[i].avail_start >= vm_physmem[i].avail_end) {
			continue;
		}

		if ((vm_physmem[i].avail_end - vm_physmem[i].avail_start) < npg)
			continue;

		pa = ptoa(vm_physmem[i].avail_start);
		vm_physmem[i].avail_start += npg;
		vm_physmem[i].start += npg;

		if (vm_physmem[i].avail_start == vm_physmem[i].end) {
			if (vm_nphysseg == 1)
				panic("pmap_steal_memory: out of memory!");

			vm_nphysseg--;
			for (j = i; j < vm_nphysseg; j++)
				vm_physmem[j] = vm_physmem[j + 1];
		}
		if (vstartp)
			*vstartp = round_page(virtual_start);
		if (vendp)
			*vendp = virtual_end;

#ifdef __sgi__
#ifndef CPU_R8000
		/*
		 * Return a CKSEG0 address whenever possible.
		 */
		if (pa + size < CKSEG_SIZE)
			va = PHYS_TO_CKSEG0(pa);
		else
#endif
			va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
#else
		va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
#endif

		bzero((void *)va, size);
		return (va);
	}

	panic("pmap_steal_memory: no memory to steal");
}

/*
 *	Initialize the pmap module.
 *	Called by uvm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
 */
void
pmap_init()
{

	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init()\n"));

#if 0 /* too early */
	pool_setlowat(&pmap_pv_pool, pmap_pv_lowat);
#endif
}

static pv_entry_t pg_to_pvh(struct vm_page *);
static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pv_ent;
}

/*
 *	Create and return a physical map.
 */
pmap_t
pmap_create()
{
	pmap_t pmap;
	int i;

extern struct vmspace vmspace0;
extern struct user *proc0paddr;

	DPRINTF(PDB_FOLLOW|PDB_CREATE, ("pmap_create()\n"));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK | PR_ZERO);

	mtx_init(&pmap->pm_mtx, IPL_VM);
	pmap->pm_count = 1;

	pmap->pm_segtab = pool_get(&pmap_pg_pool, PR_WAITOK | PR_ZERO);

	if (pmap == vmspace0.vm_map.pmap) {
		/*
		 * The initial process has already been allocated a TLBPID
		 * in mips_init().
		 */
		for (i = 0; i < ncpusfound; i++) {
			pmap->pm_asid[i].pma_asid = MIN_USER_ASID;
			pmap->pm_asid[i].pma_asidgen =
				pmap_asid_info[i].pma_asidgen;
		}
		proc0paddr->u_pcb.pcb_segtab = pmap->pm_segtab;
	} else {
		for (i = 0; i < ncpusfound; i++) {
			pmap->pm_asid[i].pma_asid = 0;
			pmap->pm_asid[i].pma_asidgen = 0;
		}
	}

	return (pmap);
}

/*
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
 */
void
pmap_destroy(pmap_t pmap)
{
	pt_entry_t **pde, *pte;
	int count;
	unsigned int i, j;
#ifdef PARANOIA
	unsigned int k;
#endif

	DPRINTF(PDB_FOLLOW|PDB_CREATE, ("pmap_destroy(%p)\n", pmap));

	count = atomic_dec_int_nv(&pmap->pm_count);
	if (count > 0)
		return;

	if (pmap->pm_segtab) {
		for (i = 0; i < PMAP_SEGTABSIZE; i++) {
			/* get pointer to segment map */
			if ((pde = pmap->pm_segtab->seg_tab[i]) == NULL)
				continue;
			for (j = 0; j < NPDEPG; j++) {
				if ((pte = pde[j]) == NULL)
					continue;
#ifdef PARANOIA
				for (k = 0; k < NPTEPG; k++) {
					if (pte[k] != PG_NV)
						panic("pmap_destroy(%p): "
						    "pgtab %p not empty at "
						    "index %u", pmap, pte, k);
				}
#endif
				pool_put(&pmap_pg_pool, pte);
#ifdef PARANOIA
				pde[j] = NULL;
#endif
			}
			pool_put(&pmap_pg_pool, pde);
#ifdef PARANOIA
			pmap->pm_segtab->seg_tab[i] = NULL;
#endif
		}
		pool_put(&pmap_pg_pool, pmap->pm_segtab);
#ifdef PARANOIA
		pmap->pm_segtab = NULL;
#endif
	}

	pool_put(&pmap_pmap_pool, pmap);
}

void
pmap_collect(pmap_t pmap)
{
	void *pmpg;
	pt_entry_t **pde, *pte;
	unsigned int i, j, k;
	unsigned int m, n;

	DPRINTF(PDB_FOLLOW, ("pmap_collect(%p)\n", pmap));

	/* There is nothing to garbage collect in the kernel pmap. */
	if (pmap == pmap_kernel())
		return;

	pmap_lock(pmap);

	/*
	 * When unlinking a directory page, the subsequent call to
	 * pmap_shootdown_page() lets any parallel lockless directory
	 * traversals end before the page gets freed.
	 */

	for (i = 0; i < PMAP_SEGTABSIZE; i++) {
		if ((pde = pmap->pm_segtab->seg_tab[i]) == NULL)
			continue;
		m = 0;
		for (j = 0; j < NPDEPG; j++) {
			if ((pte = pde[j]) == NULL)
				continue;
			n = 0;
			for (k = 0; k < NPTEPG; k++) {
				if (pte[k] & PG_V) {
					n++;
					break;
				}
			}
			if (n == 0) {
				pmpg = pde[j];
				pde[j] = NULL;
				pmap_shootdown_page(pmap, 0);
				pool_put(&pmap_pg_pool, pmpg);
			} else
				m++;
		}
		if (m == 0) {
			pmpg = pmap->pm_segtab->seg_tab[i];
			pmap->pm_segtab->seg_tab[i] = NULL;
			pmap_shootdown_page(pmap, 0);
			pool_put(&pmap_pg_pool, pmpg);
		}
	}

	pmap_unlock(pmap);
}

/*
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap_t pmap)
{

	DPRINTF(PDB_FOLLOW, ("pmap_reference(%p)\n", pmap));

	atomic_inc_int(&pmap->pm_count);
}

/*
 *      Make a new pmap (vmspace) active for the given process.
 */
void
pmap_activate(struct proc *p)
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	struct cpu_info *ci = curcpu();
	uint id;

	ci->ci_curpmap = pmap;
	p->p_addr->u_pcb.pcb_segtab = pmap->pm_segtab;
	id = pmap_alloc_tlbpid(p);
	if (p == ci->ci_curproc)
		tlb_set_pid(id);
}

/*
 *      Make a previously active pmap (vmspace) inactive.
 */
void
pmap_deactivate(struct proc *p)
{
	struct cpu_info *ci = curcpu();

	ci->ci_curpmap = NULL;
}

/*
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_do_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{
	vaddr_t ndsva, nssva;
	pt_entry_t ***seg, **pde, *pte, entry;
	paddr_t pa;
	struct cpu_info *ci = curcpu();

	PMAP_ASSERT_LOCKED(pmap);

	DPRINTF(PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT,
		("pmap_remove(%p, %p, %p)\n", pmap, (void *)sva, (void *)eva));

	stat_count(remove_stats.calls);

	if (pmap == pmap_kernel()) {
		/* remove entries from kernel pmap */
#ifdef DIAGNOSTIC
		if (sva < VM_MIN_KERNEL_ADDRESS ||
		    eva >= VM_MAX_KERNEL_ADDRESS || eva < sva)
			panic("pmap_remove(%p, %p): not in range",
			    (void *)sva, (void *)eva);
#endif
		pte = kvtopte(sva);
		for (; sva < eva; sva += PAGE_SIZE, pte++) {
			entry = *pte;
			if (!(entry & PG_V))
				continue;
			if (entry & PG_WIRED)
				atomic_dec_long(&pmap->pm_stats.wired_count);
			atomic_dec_long(&pmap->pm_stats.resident_count);
			pa = pfn_to_pad(entry);
			if ((entry & PG_CACHEMODE) == PG_CACHED)
				Mips_HitSyncDCachePage(ci, sva, pa);
			pmap_remove_pv(pmap, sva, pa);
			*pte = PG_NV | PG_G;
			/*
			 * Flush the TLB for the given address.
			 */
			pmap_invalidate_kernel_page(sva);
			pmap_shootdown_page(pmap_kernel(), sva);
			stat_count(remove_stats.flushes);
		}
		return;
	}

#ifdef DIAGNOSTIC
	if (eva > VM_MAXUSER_ADDRESS)
		panic("pmap_remove: uva not in range");
#endif
	/*
	 * Invalidate every valid mapping within the range.
	 */
	seg = &pmap_segmap(pmap, sva);
	for ( ; sva < eva; sva = nssva, seg++) {
		nssva = mips_trunc_seg(sva) + NBSEG;
		if (*seg == NULL)
			continue;
		pde = *seg + uvtopde(sva);
		for ( ; sva < eva && sva < nssva; sva = ndsva, pde++) {
			ndsva = mips_trunc_dir(sva) + NBDIR;
			if (*pde == NULL)
				continue;
			pte = *pde + uvtopte(sva);
			for ( ; sva < eva && sva < ndsva;
			    sva += PAGE_SIZE, pte++) {
				entry = *pte;
				if (!(entry & PG_V))
					continue;
				if (entry & PG_WIRED)
					atomic_dec_long(
					    &pmap->pm_stats.wired_count);
				atomic_dec_long(&pmap->pm_stats.resident_count);
				pa = pfn_to_pad(entry);
				if ((entry & PG_CACHEMODE) == PG_CACHED)
					Mips_SyncDCachePage(ci, sva, pa);
				pmap_remove_pv(pmap, sva, pa);
				*pte = PG_NV;
				/*
				 * Flush the TLB for the given address.
				 */
				pmap_invalidate_user_page(pmap, sva);
				pmap_shootdown_page(pmap, sva);
				stat_count(remove_stats.flushes);
			}
		}
	}
}

void
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{
	pmap_lock(pmap);
	pmap_do_remove(pmap, sva, eva);
	pmap_unlock(pmap);
}

/*
 * Makes all mappings to a given page read-only.
 */
void
pmap_page_wrprotect(struct vm_page *pg, vm_prot_t prot)
{
	struct cpu_info *ci = curcpu();
	pt_entry_t *pte, entry, p;
	pv_entry_t pv;

	p = PG_RO;
	if (!(prot & PROT_EXEC))
		p |= pg_xi;

	mtx_enter(&pg->mdpage.pv_mtx);
	for (pv = pg_to_pvh(pg); pv != NULL; pv = pv->pv_next) {
		if (pv->pv_pmap == pmap_kernel()) {
#ifdef DIAGNOSTIC
			if (pv->pv_va < VM_MIN_KERNEL_ADDRESS ||
			    pv->pv_va >= VM_MAX_KERNEL_ADDRESS)
				panic("%s(%p)", __func__, (void *)pv->pv_va);
#endif
			pte = kvtopte(pv->pv_va);
			entry = *pte;
			if (!(entry & PG_V))
				continue;
			if ((entry & PG_M) != 0 &&
			    (entry & PG_CACHEMODE) == PG_CACHED)
				Mips_HitSyncDCachePage(ci, pv->pv_va,
				    pfn_to_pad(entry));
			entry = (entry & ~(PG_M | PG_XI)) | p;
			*pte = entry;
			pmap_update_kernel_page(pv->pv_va, entry);
			pmap_shootdown_page(pmap_kernel(), pv->pv_va);
		} else if (pv->pv_pmap != NULL) {
			pte = pmap_pte_lookup(pv->pv_pmap, pv->pv_va);
			if (pte == NULL)
				continue;
			entry = *pte;
			if (!(entry & PG_V))
				continue;
			if ((entry & PG_M) != 0 &&
			    (entry & PG_CACHEMODE) == PG_CACHED)
				Mips_SyncDCachePage(ci, pv->pv_va,
				    pfn_to_pad(entry));
			entry = (entry & ~(PG_M | PG_XI)) | p;
			*pte = entry;
			pmap_update_user_page(pv->pv_pmap, pv->pv_va, entry);
			pmap_shootdown_page(pv->pv_pmap, pv->pv_va);
		}
	}
	mtx_leave(&pg->mdpage.pv_mtx);
}

/*
 * Removes all mappings to a given page.
 */
void
pmap_page_remove(struct vm_page *pg)
{
	pmap_t pmap;
	pv_entry_t pv;
	vaddr_t va;

	mtx_enter(&pg->mdpage.pv_mtx);
	while ((pv = pg_to_pvh(pg))->pv_pmap != NULL) {
		pmap = pv->pv_pmap;
		va = pv->pv_va;

		/*
		 * The PV list lock has to be released for pmap_do_remove().
		 * The lock ordering prevents locking the pmap before the
		 * release, so another CPU might remove or replace the page at
		 * the virtual address in the pmap. Continue with this PV entry
		 * only if the list head is unchanged after reacquiring
		 * the locks.
		 */
		pmap_reference(pmap);
		mtx_leave(&pg->mdpage.pv_mtx);
		pmap_lock(pmap);
		mtx_enter(&pg->mdpage.pv_mtx);
		if (pg_to_pvh(pg)->pv_pmap != pmap ||
		    pg_to_pvh(pg)->pv_va != va) {
			mtx_leave(&pg->mdpage.pv_mtx);
			pmap_unlock(pmap);
			pmap_destroy(pmap);
			mtx_enter(&pg->mdpage.pv_mtx);
			continue;
		}
		mtx_leave(&pg->mdpage.pv_mtx);

		pmap_do_remove(pmap, va, va + PAGE_SIZE);

		pmap_unlock(pmap);
		pmap_destroy(pmap);
		mtx_enter(&pg->mdpage.pv_mtx);
	}
	mtx_leave(&pg->mdpage.pv_mtx);
}

/*
 *	pmap_page_protect:
 *
 *	Lower the permission for all mappings to a given page.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	if (prot == PROT_NONE) {
		DPRINTF(PDB_REMOVE, ("pmap_page_protect(%p, 0x%x)\n", pg, prot));
	} else {
		DPRINTF(PDB_FOLLOW|PDB_PROTECT,
			("pmap_page_protect(%p, 0x%x)\n", pg, prot));
	}

	switch (prot) {
	case PROT_READ | PROT_WRITE:
	case PROT_MASK:
		break;

	/* copy_on_write */
	case PROT_READ:
	case PROT_READ | PROT_EXEC:
		pmap_page_wrprotect(pg, prot);
		break;

	/* remove_all */
	default:
		pmap_page_remove(pg);
		break;
	}
}

/*
 *	Set the physical protection on the
 *	specified range of this map as requested.
 */
void
pmap_protect(pmap_t pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	vaddr_t ndsva, nssva;
	pt_entry_t ***seg, **pde, *pte, entry, p;
	struct cpu_info *ci = curcpu();

	DPRINTF(PDB_FOLLOW|PDB_PROTECT,
		("pmap_protect(%p, %p, %p, 0x%x)\n",
		    pmap, (void *)sva, (void *)eva, prot));

	if ((prot & PROT_READ) == PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}

	p = (prot & PROT_WRITE) ? PG_M : PG_RO;
	if (!(prot & PROT_EXEC))
		p |= pg_xi;

	pmap_lock(pmap);

	if (pmap == pmap_kernel()) {
		/*
		 * Change entries in kernel pmap.
		 * This will trap if the page is writeable (in order to set
		 * the dirty bit) even if the dirty bit is already set. The
		 * optimization isn't worth the effort since this code isn't
		 * executed much. The common case is to make a user page
		 * read-only.
		 */
#ifdef DIAGNOSTIC
		if (sva < VM_MIN_KERNEL_ADDRESS ||
		    eva >= VM_MAX_KERNEL_ADDRESS || eva < sva)
			panic("pmap_protect(%p, %p): not in range",
			    (void *)sva, (void *)eva);
#endif
		pte = kvtopte(sva);
		for (; sva < eva; sva += PAGE_SIZE, pte++) {
			entry = *pte;
			if (!(entry & PG_V))
				continue;
			if ((entry & PG_M) != 0 /* && p != PG_M */)
				if ((entry & PG_CACHEMODE) == PG_CACHED)
					Mips_HitSyncDCachePage(ci, sva,
					    pfn_to_pad(entry));
			entry = (entry & ~(PG_M | PG_RO | PG_XI)) | p;
			*pte = entry;
			/*
			 * Update the TLB if the given address is in the cache.
			 */
			pmap_update_kernel_page(sva, entry);
			pmap_shootdown_page(pmap_kernel(), sva);
		}
		pmap_unlock(pmap);
		return;
	}

#ifdef DIAGNOSTIC
	if (eva > VM_MAXUSER_ADDRESS)
		panic("pmap_protect: uva not in range");
#endif
	/*
	 * Change protection on every valid mapping within the range.
	 */
	seg = &pmap_segmap(pmap, sva);
	for ( ; sva < eva; sva = nssva, seg++) {
		nssva = mips_trunc_seg(sva) + NBSEG;
		if (*seg == NULL)
			continue;
		pde = *seg + uvtopde(sva);
		for ( ; sva < eva && sva < nssva; sva = ndsva, pde++) {
			ndsva = mips_trunc_dir(sva) + NBDIR;
			if (*pde == NULL)
				continue;
			pte = *pde + uvtopte(sva);
			for ( ; sva < eva && sva < ndsva;
			    sva += PAGE_SIZE, pte++) {
				entry = *pte;
				if (!(entry & PG_V))
					continue;
				if ((entry & PG_M) != 0 /* && p != PG_M */ &&
				    (entry & PG_CACHEMODE) == PG_CACHED) {
					if (prot & PROT_EXEC) {
						/* This will also sync D$. */
						pmap_invalidate_icache(pmap,
						    sva, entry);
					} else
						Mips_SyncDCachePage(ci, sva,
						    pfn_to_pad(entry));
				}
				entry = (entry & ~(PG_M | PG_RO | PG_XI)) | p;
				*pte = entry;
				pmap_update_user_page(pmap, sva, entry);
				pmap_shootdown_page(pmap, sva);
			}
		}
	}

	pmap_unlock(pmap);
}

/*
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
 *	target physical map with the protection requested.
 *
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	pt_entry_t **pde, *pte, npte;
	vm_page_t pg;
	struct cpu_info *ci = curcpu();
	u_long cpuid = ci->ci_cpuid;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	DPRINTF(PDB_FOLLOW|PDB_ENTER,
		("pmap_enter(%p, %p, %p, 0x%x, 0x%x)\n",
		    pmap, (void *)va, (void *)pa, prot, flags));

	pmap_lock(pmap);

#ifdef DIAGNOSTIC
	if (pmap == pmap_kernel()) {
		stat_count(enter_stats.kernel);
		if (va < VM_MIN_KERNEL_ADDRESS ||
		    va >= VM_MAX_KERNEL_ADDRESS)
			panic("pmap_enter: kva %p", (void *)va);
	} else {
		stat_count(enter_stats.user);
		if (va >= VM_MAXUSER_ADDRESS)
			panic("pmap_enter: uva %p", (void *)va);
	}
#endif

	pg = PHYS_TO_VM_PAGE(pa);

	if (pg != NULL) {
		mtx_enter(&pg->mdpage.pv_mtx);

		/* Set page referenced/modified status based on flags */
		if (flags & PROT_WRITE)
			atomic_setbits_int(&pg->pg_flags,
			    PGF_ATTR_MOD | PGF_ATTR_REF);
		else if (flags & PROT_MASK)
			atomic_setbits_int(&pg->pg_flags, PGF_ATTR_REF);

		if (!(prot & PROT_WRITE)) {
			npte = PG_ROPAGE;
		} else {
			if (pmap == pmap_kernel()) {
				/*
				 * Don't bother to trap on kernel writes,
				 * just record page as dirty.
				 */
				npte = PG_RWPAGE;
			} else {
				if (pg->pg_flags & PGF_ATTR_MOD) {
					npte = PG_RWPAGE;
				} else {
					npte = PG_CWPAGE;
				}
			}
		}
		if (flags & PMAP_NOCACHE) {
			npte &= ~PG_CACHED;
			npte |= PG_UNCACHED;
		}

		stat_count(enter_stats.managed);
	} else {
		/*
		 * Assumption: if it is not part of our managed memory
		 * then it must be device memory which may be volatile.
		 */
		stat_count(enter_stats.unmanaged);
		if (prot & PROT_WRITE) {
			npte = PG_IOPAGE & ~PG_G;
		} else {
			npte = (PG_IOPAGE | PG_RO) & ~(PG_G | PG_M);
		}
	}

	if (!(prot & PROT_EXEC))
		npte |= pg_xi;

	if (pmap == pmap_kernel()) {
		if (pg != NULL) {
			if (pmap_enter_pv(pmap, va, pg, &npte) != 0) {
				if (flags & PMAP_CANFAIL) {
					mtx_leave(&pg->mdpage.pv_mtx);
					pmap_unlock(pmap);
					return ENOMEM;
				}
				panic("pmap_enter: pmap_enter_pv() failed");
			}
		}

		pte = kvtopte(va);
		if ((*pte & PG_V) && pa != pfn_to_pad(*pte)) {
			pmap_do_remove(pmap, va, va + PAGE_SIZE);
			stat_count(enter_stats.mchange);
		}
		if ((*pte & PG_V) == 0) {
			atomic_inc_long(&pmap->pm_stats.resident_count);
			if (wired)
				atomic_inc_long(&pmap->pm_stats.wired_count);
		} else {
			if ((*pte & PG_WIRED) != 0 && wired == 0)
				atomic_dec_long(&pmap->pm_stats.wired_count);
			else if ((*pte & PG_WIRED) == 0 && wired != 0)
				atomic_inc_long(&pmap->pm_stats.wired_count);
		}
		npte |= vad_to_pfn(pa) | PG_G;
		if (wired)
			npte |= PG_WIRED;

		/*
		 * Update the same virtual address entry.
		 */
		*pte = npte;
		pmap_update_kernel_page(va, npte);
		pmap_shootdown_page(pmap_kernel(), va);
		if (pg != NULL)
			mtx_leave(&pg->mdpage.pv_mtx);
		pmap_unlock(pmap);
		return 0;
	}

	/*
	 *  User space mapping. Do table build.
	 */
	if ((pde = pmap_segmap(pmap, va)) == NULL) {
		pde = pool_get(&pmap_pg_pool, PR_NOWAIT | PR_ZERO);
		if (pde == NULL) {
			if (flags & PMAP_CANFAIL) {
				if (pg != NULL)
					mtx_leave(&pg->mdpage.pv_mtx);
				pmap_unlock(pmap);
				return ENOMEM;
			}
			panic("%s: out of memory", __func__);
		}
		pmap_segmap(pmap, va) = pde;
	}
	if ((pte = pde[uvtopde(va)]) == NULL) {
		pte = pool_get(&pmap_pg_pool, PR_NOWAIT | PR_ZERO);
		if (pte == NULL) {
			if (flags & PMAP_CANFAIL) {
				if (pg != NULL)
					mtx_leave(&pg->mdpage.pv_mtx);
				pmap_unlock(pmap);
				return ENOMEM;
			}
			panic("%s: out of memory", __func__);
		}
		pde[uvtopde(va)] = pte;
	}

	if (pg != NULL) {
		if (pmap_enter_pv(pmap, va, pg, &npte) != 0) {
			if (flags & PMAP_CANFAIL) {
				mtx_leave(&pg->mdpage.pv_mtx);
				pmap_unlock(pmap);
				return ENOMEM;
			}
			panic("pmap_enter: pmap_enter_pv() failed");
		}
	}

	pte += uvtopte(va);

	/*
	 * Now validate mapping with desired protection/wiring.
	 * Assume uniform modified and referenced status for all
	 * MIPS pages in a OpenBSD page.
	 */
	if ((*pte & PG_V) && pa != pfn_to_pad(*pte)) {
		pmap_do_remove(pmap, va, va + PAGE_SIZE);
		stat_count(enter_stats.mchange);
	}
	if ((*pte & PG_V) == 0) {
		atomic_inc_long(&pmap->pm_stats.resident_count);
		if (wired)
			atomic_inc_long(&pmap->pm_stats.wired_count);
	} else {
		if ((*pte & PG_WIRED) != 0 && wired == 0)
			atomic_dec_long(&pmap->pm_stats.wired_count);
		else if ((*pte & PG_WIRED) == 0 && wired != 0)
			atomic_inc_long(&pmap->pm_stats.wired_count);
	}

	npte |= vad_to_pfn(pa);
	if (wired)
		npte |= PG_WIRED;

	if (pmap->pm_asid[cpuid].pma_asidgen == 
	    pmap_asid_info[cpuid].pma_asidgen) {
		DPRINTF(PDB_ENTER, ("pmap_enter: new pte 0x%08x tlbpid %u\n",
			npte, pmap->pm_asid[cpuid].pma_asid));
	} else {
		DPRINTF(PDB_ENTER, ("pmap_enter: new pte 0x%08x\n", npte));
	}

#ifdef CPU_R4000
	/*
	 * If mapping an executable page, check for the R4000 EOP bug, and
	 * flag it in the pte.
	 */
	if (r4000_errata != 0) {
		if (pg != NULL && (prot & PROT_EXEC)) {
			if ((pg->pg_flags & PGF_EOP_CHECKED) == 0)
				atomic_setbits_int(&pg->pg_flags,
				     PGF_EOP_CHECKED |
				     eop_page_check(pa));

			if (pg->pg_flags & PGF_EOP_VULN)
				npte |= PG_SP;
		}
	}
#endif

	*pte = npte;
	pmap_update_user_page(pmap, va, npte);
	pmap_shootdown_page(pmap, va);

	/*
	 * If mapping an executable page, invalidate ICache
	 * and make sure there are no pending writes.
	 */
	if (pg != NULL && (prot & PROT_EXEC)) {
		if ((npte & PG_CACHEMODE) == PG_CACHED) {
			/* This will also sync D$. */
			pmap_invalidate_icache(pmap, va, npte);
		} else
			Mips_InvalidateICache(ci, va, PAGE_SIZE);
	}

	if (pg != NULL)
		mtx_leave(&pg->mdpage.pv_mtx);
	pmap_unlock(pmap);

	return 0;
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pt_entry_t *pte, npte;

	DPRINTF(PDB_FOLLOW|PDB_ENTER,
		("pmap_kenter_pa(%p, %p, 0x%x)\n", (void *)va, (void *)pa, prot));

#ifdef DIAGNOSTIC
	if (va < VM_MIN_KERNEL_ADDRESS ||
	    va >= VM_MAX_KERNEL_ADDRESS)
		panic("pmap_kenter_pa: kva %p", (void *)va);
#endif

	npte = vad_to_pfn(pa) | PG_G | PG_WIRED;
	if (prot & PROT_WRITE)
		npte |= PG_RWPAGE;
	else
		npte |= PG_ROPAGE;
	if (!(prot & PROT_EXEC))
		npte |= pg_xi;
	pte = kvtopte(va);
	if ((*pte & PG_V) == 0) {
		atomic_inc_long(&pmap_kernel()->pm_stats.resident_count);
		atomic_inc_long(&pmap_kernel()->pm_stats.wired_count);
	} else {
		if ((*pte & PG_WIRED) == 0)
			atomic_inc_long(&pmap_kernel()->pm_stats.wired_count);
	}
	*pte = npte;
	pmap_update_kernel_page(va, npte);
	pmap_shootdown_page(pmap_kernel(), va);
}

/*
 *  Remove a mapping from the kernel map table. When doing this
 *  the cache must be synced for the VA mapped since we mapped
 *  pages behind the back of the VP tracking system. 
 */
void
pmap_kremove(vaddr_t va, vsize_t len)
{
	pt_entry_t *pte, entry;
	vaddr_t eva;
	struct cpu_info *ci = curcpu();

	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
		("pmap_kremove(%p, 0x%lx)\n", (void *)va, len));

	eva = va + len;
#ifdef DIAGNOSTIC
	if (va < VM_MIN_KERNEL_ADDRESS ||
	    eva >= VM_MAX_KERNEL_ADDRESS || eva < va)
		panic("pmap_kremove: va %p len %lx", (void *)va, len);
#endif
	pte = kvtopte(va);
	for (; va < eva; va += PAGE_SIZE, pte++) {
		entry = *pte;
		if (!(entry & PG_V))
			continue;
		if ((entry & PG_CACHEMODE) == PG_CACHED)
			Mips_HitSyncDCachePage(ci, va, pfn_to_pad(entry));
		*pte = PG_NV | PG_G;
		pmap_invalidate_kernel_page(va);
		pmap_shootdown_page(pmap_kernel(), va);
		atomic_dec_long(&pmap_kernel()->pm_stats.wired_count);
		atomic_dec_long(&pmap_kernel()->pm_stats.resident_count);
	}
}

void
pmap_unwire(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *pte;

	pmap_lock(pmap);

	if (pmap == pmap_kernel())
		pte = kvtopte(va);
	else {
		pte = pmap_pte_lookup(pmap, va);
		if (pte == NULL)
			goto out;
	}

	if (*pte & PG_V) {
		if (*pte & PG_WIRED) {
			*pte &= ~PG_WIRED;
			atomic_dec_long(&pmap->pm_stats.wired_count);
		}
	}

out:
	pmap_unlock(pmap);
}

/*
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
 */
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
{
	boolean_t rv = TRUE;
	paddr_t pa;
	pt_entry_t *pte;

	if (pmap == pmap_kernel()) {
		if (IS_XKPHYS(va))
			pa = XKPHYS_TO_PHYS(va);
#ifndef CPU_R8000
		else if (va >= (vaddr_t)CKSEG0_BASE &&
		    va < (vaddr_t)CKSEG0_BASE + CKSEG_SIZE)
			pa = CKSEG0_TO_PHYS(va);
		else if (va >= (vaddr_t)CKSEG1_BASE &&
		    va < (vaddr_t)CKSEG1_BASE + CKSEG_SIZE)
			pa = CKSEG1_TO_PHYS(va);
#endif
		else {
#ifdef DIAGNOSTIC
			if (va < VM_MIN_KERNEL_ADDRESS ||
			    va >= VM_MAX_KERNEL_ADDRESS)
				panic("pmap_extract(%p, %p)", pmap, (void *)va);
#endif
			pte = kvtopte(va);
			if (*pte & PG_V)
				pa = pfn_to_pad(*pte) | (va & PAGE_MASK);
			else
				rv = FALSE;
		}
	} else {
		pte = pmap_pte_lookup(pmap, va);
		if (pte == NULL) {
			rv = FALSE;
			goto out;
		}
		if (*pte & PG_V)
			pa = pfn_to_pad(*pte) | (va & PAGE_MASK);
	}
out:
	if (rv != FALSE)
		*pap = pa;

	DPRINTF(PDB_FOLLOW, ("pmap_extract(%p, %p)=%p(%d)",
		pmap, (void *)va, (void *)pa, rv));

	return (rv);
}

/*
 * Find first virtual address >= *vap that
 * will not cause cache aliases.
 */
vaddr_t
pmap_prefer(paddr_t foff, vaddr_t va)
{
	if (pmap_prefer_mask != 0)
		va += (foff - va) & pmap_prefer_mask;

	return va;
}

/*
 *	Copy the range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void
pmap_copy(pmap_t dst_pmap, pmap_t src_pmap, vaddr_t dst_addr, vsize_t len,
    vaddr_t src_addr)
{

	DPRINTF(PDB_FOLLOW,("pmap_copy(%p, %p, %p, 0x%lx, %p)\n",
	       dst_pmap, src_pmap, (void *)dst_addr, len, (void *)src_addr));
}

/*
 *	pmap_zero_page zeros the specified (machine independent) page.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t phys = VM_PAGE_TO_PHYS(pg);
	vaddr_t va;
	pv_entry_t pv;
	struct cpu_info *ci = curcpu();
	int df = 0;

	DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%p)\n", (void *)phys));

	va = (vaddr_t)PHYS_TO_XKPHYS(phys, CCA_CACHED);
	if (pg->pg_flags & PGF_UNCACHED)
		df = 1;
	else if (pg->pg_flags & PGF_CACHED) {
		mtx_enter(&pg->mdpage.pv_mtx);
		pv = pg_to_pvh(pg);
		df = ((pv->pv_va ^ va) & cache_valias_mask) != 0;
		if (df)
			Mips_SyncDCachePage(ci, pv->pv_va, phys);
		mtx_leave(&pg->mdpage.pv_mtx);
	}
	mem_zero_page(va);
	if (df || cache_valias_mask != 0)
		Mips_HitSyncDCachePage(ci, va, phys);

#ifdef CPU_R4000
	atomic_clearbits_int(&pg->pg_flags, PGF_EOP_CHECKED | PGF_EOP_VULN);
#endif
}

/*
 *	pmap_copy_page copies the specified (machine independent) page.
 *
 *	We do the copy phys to phys and need to check if there may be
 *	a virtual coherence problem. If so flush the cache for the
 *	areas before copying, and flush afterwards.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src, dst;
	vaddr_t s, d;
	int sf, df;
	pv_entry_t pv;
	struct cpu_info *ci = curcpu();

	sf = df = 0;
	src = VM_PAGE_TO_PHYS(srcpg);
	dst = VM_PAGE_TO_PHYS(dstpg);
	s = (vaddr_t)PHYS_TO_XKPHYS(src, CCA_CACHED);
	d = (vaddr_t)PHYS_TO_XKPHYS(dst, CCA_CACHED);

	DPRINTF(PDB_FOLLOW,
		("pmap_copy_page(%p, %p)\n", (void *)src, (void *)dst));

	mtx_enter(&srcpg->mdpage.pv_mtx);
	pv = pg_to_pvh(srcpg);
	if (srcpg->pg_flags & PGF_UNCACHED)
		sf = 1;
	else if (srcpg->pg_flags & PGF_CACHED) {
		sf = ((pv->pv_va ^ s) & cache_valias_mask) != 0;
		if (sf)
			Mips_SyncDCachePage(ci, pv->pv_va, src);
	}
	mtx_leave(&srcpg->mdpage.pv_mtx);

	mtx_enter(&dstpg->mdpage.pv_mtx);
	pv = pg_to_pvh(dstpg);
	if (dstpg->pg_flags & PGF_UNCACHED)
		df = 1;
	else if (dstpg->pg_flags & PGF_CACHED) {
		df = ((pv->pv_va ^ s) & cache_valias_mask) != 0;
		if (df)
			Mips_SyncDCachePage(ci, pv->pv_va, dst);
	}
	mtx_leave(&dstpg->mdpage.pv_mtx);

	memcpy((void *)d, (void *)s, PAGE_SIZE);

	if (sf)
		Mips_HitInvalidateDCache(ci, s, PAGE_SIZE);
	if (df || cache_valias_mask != 0)
		Mips_HitSyncDCachePage(ci, d, dst);

#ifdef CPU_R4000
	atomic_clearbits_int(&dstpg->pg_flags, PGF_EOP_CHECKED | PGF_EOP_VULN);
	atomic_setbits_int(&dstpg->pg_flags,
	    srcpg->pg_flags & (PGF_EOP_CHECKED | PGF_EOP_VULN));
#endif
}

/*
 *  Clear the modify bits on the specified physical page.
 *  Also sync the cache so it reflects the new clean state of the page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	pv_entry_t pv;
	pt_entry_t *pte, entry;
	boolean_t rv = FALSE;
	paddr_t pa;
	struct cpu_info *ci = curcpu();

	DPRINTF(PDB_FOLLOW,
		("pmap_clear_modify(%p)\n", (void *)VM_PAGE_TO_PHYS(pg)));

	mtx_enter(&pg->mdpage.pv_mtx);

	if (pg->pg_flags & PGF_ATTR_MOD) {
		atomic_clearbits_int(&pg->pg_flags, PGF_ATTR_MOD);
		rv = TRUE;
	}

	pa = VM_PAGE_TO_PHYS(pg);
	for (pv = pg_to_pvh(pg); pv != NULL; pv = pv->pv_next) {
		if (pv->pv_pmap == pmap_kernel()) {
#ifdef DIAGNOSTIC
			if (pv->pv_va < VM_MIN_KERNEL_ADDRESS ||
			    pv->pv_va >= VM_MAX_KERNEL_ADDRESS)
				panic("pmap_clear_modify(%p)",
				    (void *)pv->pv_va);
#endif
			pte = kvtopte(pv->pv_va);
			entry = *pte;
			if ((entry & PG_V) != 0 && (entry & PG_M) != 0) {
				if (pg->pg_flags & PGF_CACHED)
					Mips_HitSyncDCachePage(ci, pv->pv_va,
					    pfn_to_pad(entry));
				rv = TRUE;
				entry &= ~PG_M;
				*pte = entry;
				pmap_update_kernel_page(pv->pv_va, entry);
				pmap_shootdown_page(pmap_kernel(), pv->pv_va);
			}
		} else if (pv->pv_pmap != NULL) {
			pte = pmap_pte_lookup(pv->pv_pmap, pv->pv_va);
			if (pte == NULL)
				continue;
			entry = *pte;
			if ((entry & PG_V) != 0 && (entry & PG_M) != 0) {
				if (pg->pg_flags & PGF_CACHED)
					Mips_SyncDCachePage(ci, pv->pv_va, pa);
				rv = TRUE;
				entry &= ~PG_M;
				*pte = entry;
				pmap_update_user_page(pv->pv_pmap, pv->pv_va,
				    entry);
				pmap_shootdown_page(pv->pv_pmap, pv->pv_va);
			}
		}
	}

	mtx_leave(&pg->mdpage.pv_mtx);

	return rv;
}

/*
 *	pmap_clear_reference:
 *
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	boolean_t rv;

	DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%p)\n",
	    (void *)VM_PAGE_TO_PHYS(pg)));

	rv = (pg->pg_flags & PGF_ATTR_REF) != 0;
	atomic_clearbits_int(&pg->pg_flags, PGF_ATTR_REF);
	return rv;
}

/*
 *	pmap_is_referenced:
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	return (pg->pg_flags & PGF_ATTR_REF) != 0;
}

/*
 *	pmap_is_modified:
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
	return (pg->pg_flags & PGF_ATTR_MOD) != 0;
}

/*
 * Miscellaneous support routines not part of the pmap API
 */

/*
 * Sets the modify bit on the page that contains virtual address va.
 * Returns 0 on success. If the page is mapped read-only, the bit is left
 * unchanged and the function returns non-zero.
 */
int
pmap_emulate_modify(pmap_t pmap, vaddr_t va)
{
	pt_entry_t *pte, entry;
	paddr_t pa;
	vm_page_t pg;
#ifdef MULTIPROCESSOR
	/* Keep back TLB shootdowns. */
	register_t sr = disableintr();
	pt_entry_t old_entry;
#endif
	int rv = 0;

	if (pmap == pmap_kernel()) {
		pte = kvtopte(va);
	} else {
		pte = pmap_pte_lookup(pmap, va);
		if (pte == NULL)
			panic("%s: invalid page dir in pmap %p va %p", __func__,
			    pmap, (void *)va);
	}
	entry = *pte;
	if (!(entry & PG_V) || (entry & PG_M)) {
#ifdef MULTIPROCESSOR
		/* Another CPU might have changed the mapping. */
		if (pmap == pmap_kernel())
			pmap_update_kernel_page(trunc_page(va), entry);
		else
			pmap_update_user_page(pmap, trunc_page(va), entry);
		goto out;
#else
		panic("%s: invalid pte 0x%lx in pmap %p va %p", __func__,
		    (unsigned long)entry, pmap, (void *)va);
#endif
	}
	if (entry & PG_RO) {
		rv = 1;
		goto out;
	}
#ifdef MULTIPROCESSOR
	old_entry = entry;
	entry |= PG_M;
	if (pmap_pte_cas(pte, old_entry, entry) != old_entry) {
		/* Refault to resolve the conflict. */
		goto out;
	}
#else
	entry |= PG_M;
	*pte = entry;
#endif
	if (pmap == pmap_kernel())
		pmap_update_kernel_page(trunc_page(va), entry);
	else
		pmap_update_user_page(pmap, trunc_page(va), entry);
	pa = pfn_to_pad(entry);
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
		panic("%s: unmanaged page %p in pmap %p va %p", __func__,
		    (void *)pa, pmap, (void *)va);
	atomic_setbits_int(&pg->pg_flags, PGF_ATTR_MOD | PGF_ATTR_REF);
out:
#ifdef MULTIPROCESSOR
	setsr(sr);
#endif
	return rv;
}

/*
 *  Walk the PV tree for a physical page and change all its
 *  mappings to cached or uncached.
 */
void
pmap_do_page_cache(vm_page_t pg, u_int mode)
{
	pv_entry_t pv;
	pt_entry_t *pte, entry;
	pt_entry_t newmode;

	MUTEX_ASSERT_LOCKED(&pg->mdpage.pv_mtx);

	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_page_cache(%p)\n", pg));

	newmode = mode & PGF_CACHED ? PG_CACHED : PG_UNCACHED;

	for (pv = pg_to_pvh(pg); pv != NULL; pv = pv->pv_next) {
		if (pv->pv_pmap == pmap_kernel()) {
#ifdef DIAGNOSTIC
			if (pv->pv_va < VM_MIN_KERNEL_ADDRESS ||
			    pv->pv_va >= VM_MAX_KERNEL_ADDRESS)
				panic("pmap_page_cache(%p)", (void *)pv->pv_va);
#endif
			pte = kvtopte(pv->pv_va);
			entry = *pte;
			if (entry & PG_V) {
				entry = (entry & ~PG_CACHEMODE) | newmode;
				*pte = entry;
				pmap_update_kernel_page(pv->pv_va, entry);
				pmap_shootdown_page(pmap_kernel(), pv->pv_va);
			}
		} else if (pv->pv_pmap != NULL) {
			pte = pmap_pte_lookup(pv->pv_pmap, pv->pv_va);
			if (pte == NULL)
				continue;
			entry = *pte;
			if (entry & PG_V) {
				entry = (entry & ~PG_CACHEMODE) | newmode;
				*pte = entry;
				pmap_update_user_page(pv->pv_pmap, pv->pv_va,
				    entry);
				pmap_shootdown_page(pv->pv_pmap, pv->pv_va);
			}
		}
	}
	atomic_clearbits_int(&pg->pg_flags, PGF_CACHED | PGF_UNCACHED);
	atomic_setbits_int(&pg->pg_flags, mode);
}

void
pmap_page_cache(vm_page_t pg, u_int mode)
{
	mtx_enter(&pg->mdpage.pv_mtx);
	pmap_do_page_cache(pg, mode);
	mtx_leave(&pg->mdpage.pv_mtx);
}

/*
 * Allocate a hardware PID and return it.
 * It takes almost as much or more time to search the TLB for a
 * specific PID and flush those entries as it does to flush the entire TLB.
 * Therefore, when we allocate a new PID, we just take the next number. When
 * we run out of numbers, we flush the TLB, increment the generation count
 * and start over. PID zero is reserved for kernel use.
 * This is called only by switch().
 */
uint
pmap_alloc_tlbpid(struct proc *p)
{
	pmap_t pmap;
	uint id;
	struct cpu_info *ci = curcpu();
	u_long cpuid = ci->ci_cpuid;

	pmap = p->p_vmspace->vm_map.pmap;
	if (pmap->pm_asid[cpuid].pma_asidgen != 
	    pmap_asid_info[cpuid].pma_asidgen) {
		id = pmap_asid_info[cpuid].pma_asid;
		if (id >= PG_ASID_COUNT) {
			tlb_asid_wrap(ci);
			/* reserve tlbpid_gen == 0 to alway mean invalid */
			if (++pmap_asid_info[cpuid].pma_asidgen == 0)
				pmap_asid_info[cpuid].pma_asidgen = 1;
			id = MIN_USER_ASID;
		}
		pmap_asid_info[cpuid].pma_asid = id + 1;
		pmap->pm_asid[cpuid].pma_asid = id;
		pmap->pm_asid[cpuid].pma_asidgen = 
			pmap_asid_info[cpuid].pma_asidgen;
	} else {
		id = pmap->pm_asid[cpuid].pma_asid;
	}

	if (curproc) {
		DPRINTF(PDB_FOLLOW|PDB_TLBPID, 
			("pmap_alloc_tlbpid: curproc %d '%s' ",
				curproc->p_p->ps_pid, curproc->p_p->ps_comm));
	} else {
		DPRINTF(PDB_FOLLOW|PDB_TLBPID, 
			("pmap_alloc_tlbpid: curproc <none> "));
	}
	DPRINTF(PDB_FOLLOW|PDB_TLBPID, ("segtab %p tlbpid %u pid %d '%s'\n",
			pmap->pm_segtab, id, p->p_p->ps_pid, p->p_p->ps_comm));

	return (id);
}

/*
 * Enter the pmap and virtual address into the physical to virtual map table.
 */
int
pmap_enter_pv(pmap_t pmap, vaddr_t va, vm_page_t pg, pt_entry_t *npte)
{
	pv_entry_t pv, npv;

	MUTEX_ASSERT_LOCKED(&pg->mdpage.pv_mtx);

	pv = pg_to_pvh(pg);
	if (pv->pv_pmap == NULL) {
		/*
		 * No entries yet, use header as the first entry
		 */

		DPRINTF(PDB_PVENTRY,
			("pmap_enter: first pv: pmap %p va %p pa %p\n",
				pmap, (void *)va, (void *)VM_PAGE_TO_PHYS(pg)));

		stat_count(enter_stats.firstpv);

		pv->pv_va = va;
		if (*npte & PG_CACHED)
			atomic_setbits_int(&pg->pg_flags, PGF_CACHED);
		if (*npte & PG_UNCACHED)
			atomic_setbits_int(&pg->pg_flags, PGF_UNCACHED);
		pv->pv_pmap = pmap;
		pv->pv_next = NULL;
	} else {
		/*
		 * There is at least one other VA mapping this page.
		 * We'll place this entry after the header.
		 */

		if ((pg->pg_flags & PGF_CACHED) == 0) {
			/*
			 * If page is not mapped cached it's either because
			 * an uncached mapping was explicitely requested or
			 * we have a VAC situation.
			 * Map this page uncached as well.
			 */
			*npte = (*npte & ~PG_CACHEMODE) | PG_UNCACHED;
		}

		/*
		 * The entry may already be in the list if
		 * we are only changing the protection bits.
		 */
		for (npv = pv; npv; npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				return 0;
		}

		DPRINTF(PDB_PVENTRY,
			("pmap_enter: new pv: pmap %p va %p pg %p\n",
			    pmap, (void *)va, (void *)VM_PAGE_TO_PHYS(pg)));

		npv = pmap_pv_alloc();
		if (npv == NULL)
			return ENOMEM;

		if (cache_valias_mask != 0 && (*npte & PG_CACHED) != 0 &&
		    (pg->pg_flags & PGF_CACHED) != 0) {
			/*
			 * We have a VAC possibility.  Check if virtual
			 * address of current mappings are compatible
			 * with this new mapping. Only need to check first
			 * since all others have been checked compatible
			 * when added. If they are incompatible, update
			 * all mappings to be mapped uncached, flush the
			 * cache and set page to not be mapped cached.
			 */
			if (((pv->pv_va ^ va) & cache_valias_mask) != 0) {
#ifdef PMAPDEBUG
				printf("%s: uncaching page pa %p, va %p/%p\n",
				    __func__, (void *)VM_PAGE_TO_PHYS(pg),
				    (void *)pv->pv_va, (void *)va);
#endif
				pmap_do_page_cache(pg, 0);
				Mips_SyncDCachePage(curcpu(), pv->pv_va,
				    VM_PAGE_TO_PHYS(pg));
				*npte = (*npte & ~PG_CACHEMODE) | PG_UNCACHED;
			}
		}

		npv->pv_va = va;
		npv->pv_pmap = pmap;
		npv->pv_next = pv->pv_next;
		pv->pv_next = npv;

		if (!npv->pv_next)
			stat_count(enter_stats.secondpv);
	}

	return 0;
}

/*
 * Remove a physical to virtual address translation from the PV table.
 */
void
pmap_remove_pv(pmap_t pmap, vaddr_t va, paddr_t pa)
{
	pv_entry_t pv, npv;
	vm_page_t pg;

	DPRINTF(PDB_FOLLOW|PDB_PVENTRY,
		("pmap_remove_pv(%p, %p, %p)\n", pmap, (void *)va, (void *)pa));

	/*
	 * Remove page from the PV table
	 */
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
		return;

	mtx_enter(&pg->mdpage.pv_mtx);

	/*
	 * If we are removing the first entry on the list, copy up
	 * the next entry, if any, and free that pv item since the
	 * first root item can't be freed. Else walk the list.
	 */
	pv = pg_to_pvh(pg);
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		npv = pv->pv_next;
		if (npv) {
			*pv = *npv;
			pmap_pv_free(npv);
		} else {
			pv->pv_pmap = NULL;
			atomic_clearbits_int(&pg->pg_flags,
			    PG_PMAPMASK & ~PGF_PRESERVE);
		}
		stat_count(remove_stats.pvfirst);
	} else {
		for (npv = pv->pv_next; npv; pv = npv, npv = npv->pv_next) {
			stat_count(remove_stats.pvsearch);
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
		}
		if (npv != NULL) {
			pv->pv_next = npv->pv_next;
			pmap_pv_free(npv);
		} else {
#ifdef DIAGNOSTIC
			panic("pmap_remove_pv(%p, %p, %p) not found",
			    pmap, (void *)va, (void *)pa);
#endif
		}
	}

	if ((pg->pg_flags & (PGF_CACHED | PGF_UNCACHED)) == 0 &&
	    cache_valias_mask != 0 && pv->pv_pmap != NULL) {
		/*
		 * If this page had been mapped uncached due to aliasing,
		 * check if it can be mapped cached again after the current
		 * entry's removal.
		 */
		pv = pg_to_pvh(pg);
		va = pv->pv_va;
		for (pv = pv->pv_next; pv != NULL; pv = pv->pv_next) {
			if (((pv->pv_va ^ va) & cache_valias_mask) != 0)
				break;
		}

		if (pv == NULL) {
#ifdef PMAPDEBUG
			printf("%s: caching page pa %p, va %p again\n",
			    __func__, (void *)VM_PAGE_TO_PHYS(pg), (void *)va);
#endif
			pmap_do_page_cache(pg, PGF_CACHED);
		}
	}

	mtx_leave(&pg->mdpage.pv_mtx);
}

/*
 * Allocator for smaller-than-a-page structures pool (pm_segtab, and
 * second level page tables).  Pages backing this poll are mapped in
 * XKPHYS to avoid additional page faults when servicing a TLB miss.
 */

void *
pmap_pg_alloc(struct pool *pp, int flags, int *slowdown)
{
	vm_page_t pg;

	*slowdown = 0;
	for (;;) {
		pg = uvm_pagealloc(NULL, 0, NULL,
		    UVM_PGA_USERESERVE | UVM_PGA_ZERO);
		if (pg != NULL)
			break;

		*slowdown = 1;
		if (flags & PR_WAITOK)
			uvm_wait(__func__);
		else
			break;
	}

	if (pg != NULL)
		return (void *)PHYS_TO_XKPHYS(VM_PAGE_TO_PHYS(pg), CCA_CACHED);
	else
		return NULL;
}

void
pmap_pg_free(struct pool *pp, void *item)
{
	vaddr_t va = (vaddr_t)item;
	paddr_t pa = XKPHYS_TO_PHYS(va);
	vm_page_t pg = PHYS_TO_VM_PAGE(pa);

	if (cache_valias_mask)
		Mips_HitSyncDCachePage(curcpu(), va, pa);
	uvm_pagefree(pg);
}

void
pmap_proc_iflush(struct process *pr, vaddr_t va, vsize_t len)
{
#ifdef MULTIPROCESSOR
	struct pmap *pmap = vm_map_pmap(&pr->ps_vmspace->vm_map);
	CPU_INFO_ITERATOR cii;
	struct cpu_info *ci;

	CPU_INFO_FOREACH(cii, ci) {
		if (ci->ci_curpmap == pmap) {
			Mips_InvalidateICache(ci, va, len);
			break;
		}
	}
#else
	Mips_InvalidateICache(curcpu(), va, len);
#endif
}

#ifdef __HAVE_PMAP_DIRECT
vaddr_t
pmap_map_direct(vm_page_t pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	vaddr_t va;

#ifdef __sgi__
#ifndef CPU_R8000
	/*
	 * Return a CKSEG0 address whenever possible.
	 */
	if (pa < CKSEG_SIZE)
		va = PHYS_TO_CKSEG0(pa);
	else
#endif
		va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
#else
	va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
#endif

	return va;
}

vm_page_t
pmap_unmap_direct(vaddr_t va)
{
	paddr_t pa;
	vm_page_t pg;

#ifdef __sgi__
#ifndef CPU_R8000
	if (va >= CKSEG0_BASE)
		pa = CKSEG0_TO_PHYS(va);
	else
#endif
		pa = XKPHYS_TO_PHYS(va);
#else
	pa = XKPHYS_TO_PHYS(va);
#endif

	pg = PHYS_TO_VM_PAGE(pa);
	if (cache_valias_mask)
		Mips_HitSyncDCachePage(curcpu(), va, pa);

	return pg;
}
#endif

void
pmap_update(struct pmap *pmap)
{
	Mips_SyncICache(curcpu());
}
@


1.101
log
@p_comm is the process's command and isn't per thread, so move it from
struct proc to struct process.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2017/01/02 18:19:34 visa Exp $	*/
d1898 2
a1899 2
		if ((*npte & PG_CACHED) != 0 &&
		    (pg->pg_flags & PGF_CACHED) != 0 && cache_valias_mask != 0) {
@


1.100
log
@Allow freeing of newly empty directory pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2016/12/30 12:50:38 visa Exp $	*/
d1825 1
a1825 1
				curproc->p_p->ps_pid, curproc->p_comm));
d1831 1
a1831 1
			pmap->pm_segtab, id, p->p_p->ps_pid, p->p_comm));
@


1.99
log
@Ansify pmap_copy().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2016/12/30 12:42:27 visa Exp $	*/
a612 1
			m++;
d625 2
a626 1
			}
@


1.98
log
@When entering a mapping on write access, compute the PTE after making
the page dirty. This lets the system avoid an extra TLB modify fault
because the TLB mapping now allows writes immediately.

Noticed by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2016/12/23 12:38:16 visa Exp $	*/
d1440 2
a1441 6
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t dst_pmap;
	pmap_t src_pmap;
	vaddr_t dst_addr;
	vsize_t len;
	vaddr_t src_addr;
@


1.97
log
@Tweaks suggested by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2016/12/22 15:33:36 visa Exp $	*/
d1062 8
a1090 7

		/* Set page referenced/modified status based on flags */
		if (flags & PROT_WRITE)
			atomic_setbits_int(&pg->pg_flags,
			    PGF_ATTR_MOD | PGF_ATTR_REF);
		else if (flags & PROT_MASK)
			atomic_setbits_int(&pg->pg_flags, PGF_ATTR_REF);
@


1.96
log
@Extend the size of user virtual address space from 2GB to 1TB on mips64
by adding another level to page directories. This improves ASLR and
complements W^X added earlier on some systems, giving a notable update
to the architecture's security. Besides, there is now more room for
running tasks that hog memory.

Testing help from deraadt@@ and fcambus@@.
Platforms tested: loongson, octeon, sgi/IP27 and sgi/IP30
(IP30 also with 4KB pages).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2016/11/21 13:50:22 visa Exp $	*/
d616 1
a616 1
				if (pte[k] & PG_V)
d618 2
@


1.95
log
@Enabling Loongson 3A bits turned on a code path that uses a MIPS64r2
register on Loongson 2. This causes a boot failure on LS2 because
the CPU does not implement the register. Disable the code on LS2
similarly to mips64 pte.h.

Yeelong boot hang reported and fix tested by matthieu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2016/10/19 08:28:20 guenther Exp $	*/
d363 1
a363 1
	pool_init(&pmap_pg_pool, PMAP_L2SIZE, PMAP_L2SIZE, IPL_VM, 0,
d505 1
a505 2
	pmap->pm_segtab = (struct segtab *)pool_get(&pmap_pg_pool,
	    PR_WAITOK | PR_ZERO);
d536 1
d538 4
a549 6
		pt_entry_t *pte;
		int i;
#ifdef PARANOIA
		int j;
#endif

d552 1
a552 2
			pte = pmap->pm_segtab->seg_tab[i];
			if (!pte)
d554 12
d567 2
a568 3
			for (j = 0; j < NPTEPG; j++) {
				if (pte[j] != PG_NV)
					panic("pmap_destroy: segmap %p not empty at index %d", pte, j);
d570 1
a570 2
#endif
			pool_put(&pmap_pg_pool, pte);
d584 53
d686 2
a687 2
	vaddr_t nssva;
	pt_entry_t *pte, entry;
d733 5
a737 1
	while (sva < eva) {
d739 1
a739 8
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!(pte = pmap_segmap(pmap, sva))) {
			sva = nssva;
d741 4
a744 8
		}
		/*
		 * Invalidate every valid mapping within this segment.
		 */
		pte += uvtopte(sva);
		for (; sva < nssva; sva += PAGE_SIZE, pte++) {
			entry = *pte;
			if (!(entry & PG_V))
d746 22
a767 14
			if (entry & PG_WIRED)
				atomic_dec_long(&pmap->pm_stats.wired_count);
			atomic_dec_long(&pmap->pm_stats.resident_count);
			pa = pfn_to_pad(entry);
			if ((entry & PG_CACHEMODE) == PG_CACHED)
				Mips_SyncDCachePage(ci, sva, pa);
			pmap_remove_pv(pmap, sva, pa);
			*pte = PG_NV;
			/*
			 * Flush the TLB for the given address.
			 */
			pmap_invalidate_user_page(pmap, sva);
			pmap_shootdown_page(pmap, sva);
			stat_count(remove_stats.flushes);
d815 2
a816 1
			if ((pte = pmap_segmap(pv->pv_pmap, pv->pv_va)) == NULL)
a817 1
			pte += uvtopte(pv->pv_va);
d920 2
a921 2
	vaddr_t nssva;
	pt_entry_t *pte, entry, p;
d979 5
a983 1
	while (sva < eva) {
d985 1
a985 8
		if (nssva == 0 || nssva > eva)
			nssva = eva;
		/*
		 * If VA belongs to an unallocated segment,
		 * skip to the next segment boundary.
		 */
		if (!(pte = pmap_segmap(pmap, sva))) {
			sva = nssva;
d987 4
a990 8
		}
		/*
		 * Change protection on every valid mapping within this segment.
		 */
		pte += uvtopte(sva);
		for (; sva < nssva; sva += PAGE_SIZE, pte++) {
			entry = *pte;
			if (!(entry & PG_V))
d992 20
a1011 9
			if ((entry & PG_M) != 0 /* && p != PG_M */ &&
			    (entry & PG_CACHEMODE) == PG_CACHED) {
				if (prot & PROT_EXEC) {
					/* This will also sync D$. */
					pmap_invalidate_icache(pmap, sva,
					    entry);
				} else
					Mips_SyncDCachePage(ci, sva,
					    pfn_to_pad(entry));
a1012 4
			entry = (entry & ~(PG_M | PG_RO | PG_XI)) | p;
			*pte = entry;
			pmap_update_user_page(pmap, sva, entry);
			pmap_shootdown_page(pmap, sva);
d1031 1
a1031 1
	pt_entry_t *pte, npte;
d1152 14
a1165 1
	if ((pte = pmap_segmap(pmap, va)) == NULL) {
d1176 1
a1176 2

		pmap_segmap(pmap, va) = pte;
d1345 3
a1347 5
		if ((pte = pmap_segmap(pmap, va)) == NULL) {
			pmap_unlock(pmap);
			return;
		}
		pte += uvtopte(va);
d1357 1
d1398 2
a1399 1
		if (!(pte = pmap_segmap(pmap, va)))
d1401 1
a1401 6
		else {
			pte += uvtopte(va);
			if (*pte & PG_V)
				pa = pfn_to_pad(*pte) | (va & PAGE_MASK);
			else
				rv = FALSE;
d1403 2
d1406 1
d1589 2
a1590 1
			if ((pte = pmap_segmap(pv->pv_pmap, pv->pv_va)) == NULL)
a1591 1
			pte += uvtopte(pv->pv_va);
d1678 3
a1680 2
		if ((pte = pmap_segmap(pmap, va)) == NULL)
			panic("%s: invalid segmap in pmap %p va %p", __func__,
a1681 1
		pte += uvtopte(va);
d1762 10
a1771 11
			if ((pte = pmap_segmap(pv->pv_pmap, pv->pv_va))) {
				pte += uvtopte(pv->pv_va);
				entry = *pte;
				if (entry & PG_V) {
					entry = (entry & ~PG_CACHEMODE) | newmode;
					*pte = entry;
					pmap_update_user_page(pv->pv_pmap,
					    pv->pv_va, entry);
					pmap_shootdown_page(pv->pv_pmap,
					    pv->pv_va);
				}
@


1.94
log
@Change pmap_proc_iflush() to take a process instead of a proc
powerpc: rename second argument of pmap_proc_iflush() to match other archs

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2016/10/08 05:49:09 guenther Exp $	*/
d389 1
a389 1
#ifdef CPU_MIPS64R2
@


1.93
log
@Various printf claim to report the PID, so actually report that and not the TID

Build testing assistance from deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2016/09/15 02:00:17 dlg Exp $	*/
d1990 1
a1990 1
pmap_proc_iflush(struct proc *p, vaddr_t va, vsize_t len)
d1993 1
a1993 1
	struct pmap *pmap = vm_map_pmap(&p->p_vmspace->vm_map);
@


1.92
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2016/08/14 08:23:52 visa Exp $	*/
d1756 1
a1756 1
				curproc->p_pid, curproc->p_comm));
d1762 1
a1762 1
			pmap->pm_segtab, id, p->p_pid, p->p_comm));
@


1.91
log
@Utilize the TLB Execute-Inhibit bit with non-executable mappings on CPUs
that support the Execute-Inhibit exception. This makes user space W^X
effective on Octeon Plus and later Octeon versions.

Feedback from miod@@, thanks!
No objection from deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2016/05/11 15:50:29 visa Exp $	*/
d359 6
a364 7
	pool_init(&pmap_pmap_pool, PMAP_SIZEOF(ncpusfound), 0, 0, 0,"pmappl", NULL);
	pool_setipl(&pmap_pmap_pool, IPL_NONE);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0,"pvpl", NULL);
	pool_setipl(&pmap_pv_pool, IPL_VM);
	pool_init(&pmap_pg_pool, PMAP_L2SIZE, PMAP_L2SIZE, 0, 0, "pmappgpl",
	    &pmap_pg_allocator);
	pool_setipl(&pmap_pg_pool, IPL_VM);
@


1.90
log
@Another attempt to make the mips64 pmap MP-safe. Now at least
pmap_enter(9), pmap_remove(9) and pmap_page_protect(9) should be
safe to use without the kernel lock.

No objection from deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2016/04/24 13:35:45 visa Exp $	*/
d41 1
d68 1
a68 1
void	 pmap_page_wrprotect(struct vm_page *);
d160 2
d389 5
d724 1
a724 1
pmap_page_wrprotect(struct vm_page *pg)
d727 1
a727 1
	pt_entry_t *pte, entry;
d730 4
d750 1
a750 1
			entry = (entry & ~PG_M) | PG_RO;
d765 1
a765 1
			entry = (entry & ~PG_M) | PG_RO;
d843 1
a843 1
		pmap_page_wrprotect(pg);
d874 2
d903 1
a903 1
			entry = (entry & ~(PG_M | PG_RO)) | p;
d949 1
a949 1
			entry = (entry & ~(PG_M | PG_RO)) | p;
d1043 3
d1212 2
@


1.89
log
@Make pmap_invalidate_*_page() and pmap_update_*_page() operate only on
the local TLB and add pmap_shootdown_page() for invalidating remote
TLBs. Where the code previously updated a remote TLB entry, the code now
issues a remote invalidation, like in architectures with a
hardware-managed TLB. This eases work in the MP land because
invalidations do not need to strictly follow the order of page table
updates.

Almost as a side effect, pmap_emulate_modify() no longer touches remote
TLBs. It is no use to force a TLB D bit on other CPUs because they might
not need it. Moreover, a shootdown IPI probably has a higher overhead
than a local TLBModified exception.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2016/04/24 04:25:03 visa Exp $	*/
d62 2
d66 2
d189 30
d311 4
d357 1
d359 1
d362 1
d486 1
a486 1
	int i, s;
a492 1
	s = splvm();
a493 1
	splx(s);
d495 1
d530 1
a530 1
	int s, count;
d534 1
a534 1
	count = --pmap->pm_count;
a566 1
	s = splvm();
a567 1
	splx(s);
d579 1
a579 1
	pmap->pm_count++;
d617 1
a617 1
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
d624 2
a630 1
	KERNEL_LOCK();
d645 2
a646 2
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;
a658 1
		KERNEL_UNLOCK();
d687 2
a688 2
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;
d702 104
a805 1
	KERNEL_UNLOCK();
a815 4
	pv_entry_t pv;
	vaddr_t va;
	int s;

d831 1
a831 13
		pv = pg_to_pvh(pg);
		s = splvm();
		/*
		 * Loop over all current mappings setting/clearing as apropos.
		 */
		if (pv->pv_pmap != NULL) {
			for (; pv; pv = pv->pv_next) {
				va = pv->pv_va;
				pmap_protect(pv->pv_pmap, va, va + PAGE_SIZE,
				    prot);
			}
		}
		splx(s);
d836 2
a837 7
		pv = pg_to_pvh(pg);
		s = splvm();
		while (pv->pv_pmap != NULL) {
			va = pv->pv_va;
			pmap_remove(pv->pv_pmap, va, va + PAGE_SIZE);
		}
		splx(s);
d863 2
d897 1
d941 2
d967 2
d985 1
d1032 3
a1034 1
				if (flags & PMAP_CANFAIL)
d1036 1
d1043 1
a1043 1
			pmap_remove(pmap, va, va + PAGE_SIZE);
d1047 1
a1047 1
			pmap->pm_stats.resident_count++;
d1049 1
a1049 1
				pmap->pm_stats.wired_count++;
d1052 1
a1052 1
				pmap->pm_stats.wired_count--;
d1054 1
a1054 1
				pmap->pm_stats.wired_count++;
d1066 3
d1076 10
a1085 8
		unsigned int wflags = PR_WAITOK | PR_ZERO;

		if (flags & PMAP_CANFAIL)
			wflags |= PR_LIMITFAIL;
	
		pte = (pt_entry_t *)pool_get(&pmap_pg_pool, wflags);
		if (pte == NULL)
			return ENOMEM;	/* can only happen if PMAP_CANFAIL */
d1092 3
a1094 1
			if (flags & PMAP_CANFAIL)
d1096 1
d1109 1
a1109 1
		pmap_remove(pmap, va, va + PAGE_SIZE);
d1113 1
a1113 1
		pmap->pm_stats.resident_count++;
d1115 1
a1115 1
			pmap->pm_stats.wired_count++;
d1118 1
a1118 1
			pmap->pm_stats.wired_count--;
d1120 1
a1120 1
			pmap->pm_stats.wired_count++;
d1169 4
d1197 2
a1198 2
		pmap_kernel()->pm_stats.resident_count++;
		pmap_kernel()->pm_stats.wired_count++;
d1201 1
a1201 1
			pmap_kernel()->pm_stats.wired_count++;
d1239 2
a1240 2
		pmap_kernel()->pm_stats.wired_count--;
		pmap_kernel()->pm_stats.resident_count--;
d1249 2
d1254 2
a1255 1
		if ((pte = pmap_segmap(pmap, va)) == NULL) 
d1257 1
d1264 1
a1264 1
			pmap->pm_stats.wired_count--;
d1267 2
a1374 1
	pv = pg_to_pvh(pg);
d1378 2
d1383 1
d1419 1
d1428 3
d1439 1
a1464 1
	int s;
d1471 2
a1472 3
	pa = VM_PAGE_TO_PHYS(pg);
	pv = pg_to_pvh(pg);
	s = splvm();
d1478 2
a1479 1
	for (; pv != NULL; pv = pv->pv_next) {
d1516 2
a1517 1
	splx(s);
d1579 6
d1602 1
a1602 1
		return (0);
d1608 12
a1619 2
	if (entry & PG_RO)
		return (1);
d1622 1
a1622 1
	KERNEL_LOCK();
d1633 5
a1637 2
	KERNEL_UNLOCK();
	return (0);
d1645 1
a1645 1
pmap_page_cache(vm_page_t pg, u_int mode)
d1650 2
a1651 1
	int s;
a1655 1
	pv = pg_to_pvh(pg);
d1657 1
a1657 2
	s = splvm();
	for (; pv != NULL; pv = pv->pv_next) {
d1689 8
a1696 1
	splx(s);
d1756 2
a1757 1
	int s;
a1759 2

	s = splvm();
d1799 1
a1799 2
			if (pmap == npv->pv_pmap && va == npv->pv_va) {
				splx(s);
a1800 1
			}
d1808 1
a1808 2
		if (npv == NULL) {
			splx(s);
a1809 1
		}
d1828 1
a1828 1
				pmap_page_cache(pg, 0);
a1843 1
	splx(s);
a1854 1
	int s;
d1866 2
a1867 2
	pv = pg_to_pvh(pg);
	s = splvm();
d1873 1
d1921 1
a1921 1
			pmap_page_cache(pg, PGF_CACHED);
d1925 1
a1925 1
	splx(s);
@


1.88
log
@Keep pmap_update_{kernel,user}_page() inside pmap.c.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2016/04/23 12:53:28 visa Exp $	*/
d71 3
a77 3
void	pmap_invalidate_kernel_page(vaddr_t);
void	pmap_invalidate_kernel_page_action(void *);
void	pmap_invalidate_user_page_action(void *);
d79 2
a80 3
void	pmap_update_kernel_page(vaddr_t, pt_entry_t);
void	pmap_update_kernel_page_action(void *);
void	pmap_update_user_page_action(void *);
d82 1
a82 2
#define pmap_invalidate_kernel_page(va) tlb_flush_addr(va)
#define pmap_update_kernel_page(va, entry) tlb_update(va, entry)
a154 32

#ifdef MULTIPROCESSOR

struct pmap_invalidate_page_arg {
	pmap_t pmap;
	vaddr_t va;
};

void
pmap_invalidate_kernel_page(vaddr_t va)
{
	struct pmap_invalidate_page_arg arg;
	unsigned int cpumask = 0;
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;

	CPU_INFO_FOREACH(cii, ci) 
		if (cpuset_isset(&cpus_running, ci))
			cpumask |= 1 << ci->ci_cpuid;
	arg.va = va;

	smp_rendezvous_cpus(cpumask, pmap_invalidate_kernel_page_action, &arg);
}

void
pmap_invalidate_kernel_page_action(void *arg)
{
	vaddr_t va = ((struct pmap_invalidate_page_arg *)arg)->va;

	tlb_flush_addr(va);
}

d158 2
a159 4
	unsigned int cpuid = cpu_number();
	unsigned int cpumask = 0;
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
d161 8
a168 26
	CPU_INFO_FOREACH(cii, ci) 
		if (cpuset_isset(&cpus_running, ci)) {
			unsigned int i = ci->ci_cpuid;
			unsigned int m = 1 << i;
			if (pmap->pm_asid[i].pma_asidgen !=
			    pmap_asid_info[i].pma_asidgen)
				continue;
			else if (ci->ci_curpmap != pmap) {
				pmap->pm_asid[i].pma_asidgen = 0;
				continue;
			}
			cpumask |= m;
		}

	if (cpumask == 1 << cpuid) {
		u_long asid;

		asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;
		tlb_flush_addr(va | asid);
	} else if (cpumask) {
		struct pmap_invalidate_page_arg arg;
		arg.pmap = pmap;
		arg.va = va;

		smp_rendezvous_cpus(cpumask, pmap_invalidate_user_page_action,
			&arg);
a172 45
pmap_invalidate_user_page_action(void *arg)
{
	pmap_t pmap = ((struct pmap_invalidate_page_arg *)arg)->pmap;
	vaddr_t va = ((struct pmap_invalidate_page_arg *)arg)->va;
	unsigned int cpuid = cpu_number();
	u_long asid;

	asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;
	tlb_flush_addr(va | asid);
}

struct pmap_update_page_arg {
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t entry;
};

void
pmap_update_kernel_page(vaddr_t va, pt_entry_t entry)
{
	struct pmap_update_page_arg arg;
	unsigned long cpumask = 0;
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;

	CPU_INFO_FOREACH(cii, ci) 
		if (cpuset_isset(&cpus_running, ci))
			cpumask |= 1 << ci->ci_cpuid;

	arg.va = va;
	arg.entry = entry;
	smp_rendezvous_cpus(cpumask,
			    pmap_update_kernel_page_action, &arg);
}

void
pmap_update_kernel_page_action(void *arg)
{
	vaddr_t va = ((struct pmap_update_page_arg *)arg)->va;
	pt_entry_t entry = ((struct pmap_update_page_arg *)arg)->entry;

	tlb_update(va, entry);
}

void
d175 2
a176 4
	unsigned int cpuid = cpu_number();
	unsigned long cpumask = 0;
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
d178 2
a179 18
	CPU_INFO_FOREACH(cii, ci) 
		if (cpuset_isset(&cpus_running, ci)) {
			unsigned int i = ci->ci_cpuid;
			unsigned int m = 1 << i;
			if (pmap->pm_asid[i].pma_asidgen != 
			    pmap_asid_info[i].pma_asidgen)
				continue;
			else if (ci->ci_curpmap != pmap) {
				pmap->pm_asid[i].pma_asidgen = 0;
				continue;
			}
			cpumask |= m;
		}

	if (cpumask == 1 << cpuid) {
		u_long asid;

		asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;
a180 8
	} else if (cpumask) {
		struct pmap_update_page_arg arg;
		arg.pmap = pmap;
		arg.va = va;
		arg.entry = entry;
		smp_rendezvous_cpus(cpumask,
				    pmap_update_user_page_action, &arg);
	}
d183 1
a183 12
void
pmap_update_user_page_action(void *arg)
{
	pmap_t pmap = ((struct pmap_update_page_arg *)arg)->pmap;
	vaddr_t va = ((struct pmap_update_page_arg *)arg)->va;
	pt_entry_t entry = ((struct pmap_update_page_arg *)arg)->entry;
	unsigned int cpuid = cpu_number();
	u_long asid;

	asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;
	tlb_update(va | asid, entry);
}
d226 6
a231 1
#else
d233 1
a233 1
pmap_invalidate_user_page(pmap_t pmap, vaddr_t va)
d235 4
a238 2
	u_long cpuid = cpu_number();
	u_long asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;
d240 21
a260 8
	if (pmap->pm_asid[cpuid].pma_asidgen ==
	    pmap_asid_info[cpuid].pma_asidgen) {
#ifdef CPU_R4000
		if (r4000_errata != 0)
			eop_tlb_flush_addr(pmap, va, asid);
		else
#endif
			tlb_flush_addr(va | asid);
d265 1
a265 1
pmap_update_user_page(pmap_t pmap, vaddr_t va, pt_entry_t entry)
d267 1
a267 2
	u_long cpuid = cpu_number();
	u_long asid = pmap->pm_asid[cpuid].pma_asid << PG_ASID_SHIFT;
d269 4
a272 3
	if (pmap->pm_asid[cpuid].pma_asidgen ==
	    pmap_asid_info[cpuid].pma_asidgen)
		tlb_update(va | asid, entry);
d275 2
d285 2
a286 1
#endif
d617 1
d660 1
d773 1
d815 1
d934 1
d1016 1
d1062 1
d1095 1
d1342 1
d1355 3
a1357 1
				pmap_update_user_page(pv->pv_pmap, pv->pv_va, entry);
d1433 9
a1441 2
#ifdef DIAGNOSTIC
	if (!(entry & PG_V) || (entry & PG_M))
d1445 1
d1496 1
d1505 4
a1508 1
					pmap_update_user_page(pv->pv_pmap, pv->pv_va, entry);
@


1.87
log
@Sync dcaches and invalidate icaches of all active CPUs of a pmap when
making a page executable. This prevents some icache inconsistencies that
could arise when a process modifies its code pages and begins executing
them while switching between CPUs. These inconsistencies caused process
crashes on multiprocessor IP27/IP30 systems under load.

Crashes reported by deraadt@@
Feedback from Miod, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2016/02/01 16:15:18 visa Exp $	*/
d73 1
d79 1
d84 1
@


1.86
log
@Move modify bit emulation into pmap.c to gather pmap C code in one place.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2016/01/10 10:22:56 visa Exp $	*/
d72 1
d77 1
d317 42
d387 9
d909 7
a915 2
			if ((entry & PG_M) != 0 /* && p != PG_M */)
				if ((entry & PG_CACHEMODE) == PG_CACHED)
d918 1
d1123 2
a1124 1
	 * If mapping an executable page, invalidate ICache.
d1126 7
a1132 2
	if (pg != NULL && (prot & PROT_EXEC))
		Mips_InvalidateICache(ci, va, PAGE_SIZE);
@


1.85
log
@Back out the MP pmap diff for rework. The code does not work on
non-PMAP_DIRECT systems due to lock recursion.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2016/01/05 05:42:27 visa Exp $	*/
a1399 6
void
pmap_set_modify(struct vm_page *pg)
{
	atomic_setbits_int(&pg->pg_flags, PGF_ATTR_MOD | PGF_ATTR_REF);
}

d1447 3
a1449 1
 *	Return RO protection of page.
d1452 1
a1452 1
pmap_is_page_ro(pmap_t pmap, vaddr_t va, pt_entry_t entry)
d1454 35
a1488 1
	return ((entry & PG_RO) != 0);
a1489 1

@


1.84
log
@Remove PTE locking from pmap_extract() because it does not add any MP
safety. The function has to be called in a context where conflicting
pmap updates cannot happen. Otherwise the returned physical address
might not be valid.

Suggested by kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2016/01/05 05:27:54 visa Exp $	*/
a61 2
void	 pmap_do_remove(pmap_t, vaddr_t, vaddr_t);
void	 pmap_do_page_cache(vm_page_t, u_int);
a167 2
	MUTEX_ASSERT_LOCKED(&pmap_kernel()->pm_pte_mtx);

a191 2
	MUTEX_ASSERT_LOCKED(&pmap->pm_pte_mtx);

a246 2
	MUTEX_ASSERT_LOCKED(&pmap_kernel()->pm_pte_mtx);

a273 2
	MUTEX_ASSERT_LOCKED(&pmap->pm_pte_mtx);

a378 2
	pool_setipl(&pmap_pv_pool, IPL_VM);
	pool_setipl(&pmap_pg_pool, IPL_VM);
a379 2
	mtx_init(&pmap_kernel()->pm_dir_mtx, IPL_VM);
	mtx_init(&pmap_kernel()->pm_pte_mtx, IPL_VM);
d502 1
a502 1
	int i;
d509 1
d511 1
a512 2
	mtx_init(&pmap->pm_dir_mtx, IPL_VM);
	mtx_init(&pmap->pm_pte_mtx, IPL_VM);
d547 1
a547 1
	int count;
d551 1
a551 1
	count = atomic_dec_int_nv(&pmap->pm_count);
d584 1
d586 1
d598 1
a598 1
	atomic_inc_int(&pmap->pm_count);
d636 1
a636 1
pmap_do_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
a642 2
	MUTEX_ASSERT_LOCKED(&pmap->pm_dir_mtx);

d648 1
a656 1
		mtx_enter(&pmap->pm_pte_mtx);
d668 1
a674 4

			mtx_leave(&pmap->pm_pte_mtx);
			pmap_remove_pv(pmap, sva, pa);
			mtx_enter(&pmap->pm_pte_mtx);
d676 1
a676 1
		mtx_leave(&pmap->pm_pte_mtx);
a683 1
	mtx_enter(&pmap->pm_pte_mtx);
d710 1
a716 4

			mtx_leave(&pmap->pm_pte_mtx);
			pmap_remove_pv(pmap, sva, pa);
			mtx_enter(&pmap->pm_pte_mtx);
d719 1
a719 9
	mtx_leave(&pmap->pm_pte_mtx);
}

void
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
{
	mtx_enter(&pmap->pm_dir_mtx);
	pmap_do_remove(pmap, sva, eva);
	mtx_leave(&pmap->pm_dir_mtx);
a729 1
	pmap_t pmap;
d732 1
a748 1
		mtx_enter(&pg->mdpage.pv_mtx);
d750 1
a756 5
				/*
				 * It is safe to leave the page locked because
				 * pmap_protect() will only change PTEs due to
				 * the PROT_READ bit.
				 */
d761 1
a761 1
		mtx_leave(&pg->mdpage.pv_mtx);
d766 3
a768 3
		mtx_enter(&pg->mdpage.pv_mtx);
		while ((pv = pg_to_pvh(pg))->pv_pmap != NULL) {
			pmap = pv->pv_pmap;
d770 1
a770 28

			/*
			 * The PV list lock has to be released for
			 * pmap_do_remove(). The lock ordering prevents locking
			 * the pmap before the release, so another CPU might
			 * remove or replace the page at the virtual address in
			 * the pmap. Continue with this PV entry only if the
			 * list head is unchanged after reacquiring the locks.
			 */
			pmap_reference(pmap);
			mtx_leave(&pg->mdpage.pv_mtx);
			mtx_enter(&pmap->pm_dir_mtx);
			mtx_enter(&pg->mdpage.pv_mtx);
			if (pg_to_pvh(pg)->pv_pmap != pmap ||
			    pg_to_pvh(pg)->pv_va != va) {
				mtx_leave(&pg->mdpage.pv_mtx);
				mtx_leave(&pmap->pm_dir_mtx);
				pmap_destroy(pmap);
				mtx_enter(&pg->mdpage.pv_mtx);
				continue;
			}
			mtx_leave(&pg->mdpage.pv_mtx);

			pmap_do_remove(pmap, va, va + PAGE_SIZE);

			mtx_leave(&pmap->pm_dir_mtx);
			pmap_destroy(pmap);
			mtx_enter(&pg->mdpage.pv_mtx);
d772 1
a772 1
		mtx_leave(&pg->mdpage.pv_mtx);
a798 1
		mtx_enter(&pmap->pm_pte_mtx);
a828 1
		mtx_leave(&pmap->pm_pte_mtx);
a835 1
	mtx_enter(&pmap->pm_pte_mtx);
a864 1
	mtx_leave(&pmap->pm_pte_mtx);
a901 2
	mtx_enter(&pmap->pm_dir_mtx);

a904 2
		mtx_enter(&pg->mdpage.pv_mtx);

a948 1
		mtx_enter(&pmap->pm_pte_mtx);
d951 1
a951 4
				if (flags & PMAP_CANFAIL) {
					mtx_leave(&pmap->pm_pte_mtx);
					mtx_leave(&pg->mdpage.pv_mtx);
					mtx_leave(&pmap->pm_dir_mtx);
a952 1
				}
d959 1
a959 5
			mtx_leave(&pmap->pm_pte_mtx);
			if (pg != NULL)
				mtx_leave(&pg->mdpage.pv_mtx);

			pmap_do_remove(pmap, va, va + PAGE_SIZE);
a960 6

			if (pg != NULL)
				mtx_enter(&pg->mdpage.pv_mtx);
			mtx_enter(&pmap->pm_pte_mtx);

			KASSERT((*pte & PG_V) == 0);
a980 5

		mtx_leave(&pmap->pm_pte_mtx);
		if (pg != NULL)
			mtx_leave(&pg->mdpage.pv_mtx);
		mtx_leave(&pmap->pm_dir_mtx);
a986 1
	mtx_enter(&pmap->pm_pte_mtx);
d994 1
a994 5
		if (pte == NULL) {
			mtx_leave(&pmap->pm_pte_mtx);
			if (pg != NULL)
				mtx_leave(&pg->mdpage.pv_mtx);
			mtx_leave(&pmap->pm_dir_mtx);
a995 1
		}
d1002 1
a1002 4
			if (flags & PMAP_CANFAIL) {
				mtx_leave(&pmap->pm_pte_mtx);
				mtx_leave(&pg->mdpage.pv_mtx);
				mtx_leave(&pmap->pm_dir_mtx);
a1003 1
			}
d1016 1
a1016 5
		mtx_leave(&pmap->pm_pte_mtx);
		if (pg != NULL)
			mtx_leave(&pg->mdpage.pv_mtx);

		pmap_do_remove(pmap, va, va + PAGE_SIZE);
a1017 6

		if (pg != NULL)
			mtx_enter(&pg->mdpage.pv_mtx);
		mtx_enter(&pmap->pm_pte_mtx);

		KASSERT((*pte & PG_V) == 0);
a1068 4
	mtx_leave(&pmap->pm_pte_mtx);
	if (pg != NULL)
		mtx_leave(&pg->mdpage.pv_mtx);
	mtx_leave(&pmap->pm_dir_mtx);
a1090 2
	mtx_enter(&pmap_kernel()->pm_dir_mtx);
	mtx_enter(&pmap_kernel()->pm_pte_mtx);
a1100 2
	mtx_leave(&pmap_kernel()->pm_pte_mtx);
	mtx_leave(&pmap_kernel()->pm_dir_mtx);
a1123 2
	mtx_enter(&pmap_kernel()->pm_dir_mtx);
	mtx_enter(&pmap_kernel()->pm_pte_mtx);
a1135 2
	mtx_leave(&pmap_kernel()->pm_pte_mtx);
	mtx_leave(&pmap_kernel()->pm_dir_mtx);
a1142 1
	mtx_enter(&pmap->pm_pte_mtx);
a1156 1
	mtx_leave(&pmap->pm_pte_mtx);
d1263 1
a1263 1
	mtx_enter(&pg->mdpage.pv_mtx);
a1266 1
		pv = pg_to_pvh(pg);
a1270 2
	mtx_leave(&pg->mdpage.pv_mtx);

a1304 1
	mtx_enter(&srcpg->mdpage.pv_mtx);
a1312 3
	mtx_leave(&srcpg->mdpage.pv_mtx);

	mtx_enter(&dstpg->mdpage.pv_mtx);
a1320 1
	mtx_leave(&dstpg->mdpage.pv_mtx);
d1346 1
d1353 3
a1355 2
	mtx_enter(&pg->mdpage.pv_mtx);

d1361 1
a1361 4
	pa = VM_PAGE_TO_PHYS(pg);
	pv = pg_to_pvh(pg);
	for (; pv != NULL && pv->pv_pmap != NULL; pv = pv->pv_next) {
		mtx_enter(&pv->pv_pmap->pm_pte_mtx);
d1380 1
a1380 1
		} else {
a1393 1
		mtx_leave(&pv->pv_pmap->pm_pte_mtx);
d1395 1
a1395 2

	mtx_leave(&pg->mdpage.pv_mtx);
d1461 1
d1467 1
a1467 1
pmap_do_page_cache(vm_page_t pg, u_int mode)
d1472 1
a1472 2

	MUTEX_ASSERT_LOCKED(&pg->mdpage.pv_mtx);
d1477 1
d1479 2
a1480 3
	pv = pg_to_pvh(pg);
	for (; pv != NULL && pv->pv_pmap != NULL; pv = pv->pv_next) {
		mtx_enter(&pv->pv_pmap->pm_pte_mtx);
d1494 1
a1494 1
		} else {
a1504 1
		mtx_leave(&pv->pv_pmap->pm_pte_mtx);
d1508 1
a1508 8
}

void
pmap_page_cache(vm_page_t pg, u_int mode)
{
	mtx_enter(&pg->mdpage.pv_mtx);
	pmap_do_page_cache(pg, mode);
	mtx_leave(&pg->mdpage.pv_mtx);
d1568 1
a1568 4

	MUTEX_ASSERT_LOCKED(&pmap->pm_dir_mtx);
	MUTEX_ASSERT_LOCKED(&pg->mdpage.pv_mtx);
	MUTEX_ASSERT_LOCKED(&pmap->pm_pte_mtx);
d1572 1
d1612 2
a1613 1
			if (pmap == npv->pv_pmap && va == npv->pv_va)
d1615 1
d1623 2
a1624 1
		if (npv == NULL)
d1626 1
d1645 1
a1645 4
				mtx_leave(&pmap->pm_pte_mtx);
				pmap_do_page_cache(pg, 0);
				mtx_enter(&pmap->pm_pte_mtx);

d1661 1
d1673 1
a1673 2

	MUTEX_ASSERT_LOCKED(&pmap->pm_dir_mtx);
d1685 2
a1686 2
	mtx_enter(&pg->mdpage.pv_mtx);

a1691 1
	pv = pg_to_pvh(pg);
a1719 1
	pv = pg_to_pvh(pg);
d1727 1
d1739 1
a1739 1
			pmap_do_page_cache(pg, PGF_CACHED);
d1743 1
a1743 1
	mtx_leave(&pg->mdpage.pv_mtx);
@


1.83
log
@Some implementations of HitSyncDCache() call pmap_extract() for va->pa
conversion. Because pmap_extract() acquires the PTE mutex, a "locking
against myself" panic is triggered if the cache routine gets called in
a context where the mutex is already held.

In the pmap, all calls to HitSyncDCache() are for a whole page. Add a
new cache routine, HitSyncDCachePage(), which gets both the va and the
pa of a page. This removes the need of the va->pa conversion. The new
routine has the same signature as SyncDCachePage(), allowing reuse of
the same routine for cache implementations that do not need differences
between "Hit" and non-"Hit" routines.

With the diff, POWER Indigo2 R8000 boots multiuser again. Tested on sgi
GENERIC-IP27.MP and octeon GENERIC.MP, too.

Diff from miod@@, ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2015/12/31 04:25:51 visa Exp $	*/
a1311 1
			mtx_enter(&pmap->pm_pte_mtx);
a1316 1
			mtx_leave(&pmap->pm_pte_mtx);
a1318 1
		mtx_enter(&pmap->pm_pte_mtx);
a1327 1
		mtx_leave(&pmap->pm_pte_mtx);
@


1.82
log
@Protect mips64 pmap and pvlist structs with a mutex to make pmap
operations MP-safe. Tested on octeon and sgi (IP27, IP30).

Feedback from kettenis@@ long ago
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2015/08/11 13:15:36 visa Exp $	*/
d672 1
a672 1
		for(; sva < eva; sva += PAGE_SIZE, pte++) {
d681 1
a681 1
				Mips_HitSyncDCache(ci, sva, PAGE_SIZE);
d882 2
a883 1
					Mips_HitSyncDCache(ci, sva, PAGE_SIZE);
d1249 1
a1249 1
			Mips_HitSyncDCache(ci, va, PAGE_SIZE);
d1403 1
a1403 1
		Mips_HitSyncDCache(ci, va, PAGE_SIZE);
d1462 1
a1462 1
		Mips_HitSyncDCache(ci, d, PAGE_SIZE);
d1509 2
a1510 2
					Mips_HitSyncDCache(ci, pv->pv_va,
					    PAGE_SIZE);
d1933 1
a1933 1
		Mips_HitSyncDCache(curcpu(), va, PAGE_SIZE);
d1999 1
a1999 1
		Mips_HitSyncDCache(curcpu(), va, PAGE_SIZE);
@


1.81
log
@Remove unnecessary pmap == NULL checks in mips64 pmap.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2015/07/30 17:02:17 visa Exp $	*/
d62 2
d170 2
d196 2
d253 2
d282 2
d389 2
d392 2
d516 1
a516 1
	int i, s;
a522 1
	s = splvm();
a523 1
	splx(s);
d525 2
d561 1
a561 1
	int s, count;
d565 1
a565 1
	count = --pmap->pm_count;
a597 1
	s = splvm();
a598 1
	splx(s);
d610 1
a610 1
	pmap->pm_count++;
d648 1
a648 1
pmap_remove(pmap_t pmap, vaddr_t sva, vaddr_t eva)
d655 2
a661 1
	KERNEL_LOCK();
d670 1
a681 1
			pmap_remove_pv(pmap, sva, pa);
d688 4
d693 1
a693 1
		KERNEL_UNLOCK();
d701 1
a727 1
			pmap_remove_pv(pmap, sva, pa);
d734 4
d740 9
a748 1
	KERNEL_UNLOCK();
d759 1
a761 1
	int s;
d778 1
a779 1
		s = splvm();
d786 5
d795 1
a795 1
		splx(s);
d800 3
a802 3
		pv = pg_to_pvh(pg);
		s = splvm();
		while (pv->pv_pmap != NULL) {
d804 28
a831 1
			pmap_remove(pv->pv_pmap, va, va + PAGE_SIZE);
d833 1
a833 1
		splx(s);
d860 1
d890 1
d898 1
d928 1
d966 2
d971 2
d1017 1
d1020 4
a1023 1
				if (flags & PMAP_CANFAIL)
d1025 1
d1032 5
a1036 1
			pmap_remove(pmap, va, va + PAGE_SIZE);
d1038 6
d1064 5
d1075 1
d1083 5
a1087 1
		if (pte == NULL)
d1089 1
d1096 4
a1099 1
			if (flags & PMAP_CANFAIL)
d1101 1
d1114 5
a1118 1
		pmap_remove(pmap, va, va + PAGE_SIZE);
d1120 6
d1177 4
d1203 2
d1215 2
d1240 2
d1254 2
d1263 1
d1278 1
d1311 1
d1317 1
d1320 1
d1330 1
d1389 1
a1389 1
	pv = pg_to_pvh(pg);
d1393 1
d1398 2
d1434 1
d1443 3
d1454 1
a1479 1
	int s;
d1486 2
a1487 3
	pa = VM_PAGE_TO_PHYS(pg);
	pv = pg_to_pvh(pg);
	s = splvm();
d1493 4
a1496 1
	for (; pv != NULL; pv = pv->pv_next) {
d1515 1
a1515 1
		} else if (pv->pv_pmap != NULL) {
d1529 1
d1531 2
a1532 1
	splx(s);
a1597 1

d1603 1
a1603 1
pmap_page_cache(vm_page_t pg, u_int mode)
d1608 2
a1609 1
	int s;
d1614 1
d1616 2
a1617 3

	s = splvm();
	for (; pv != NULL; pv = pv->pv_next) {
d1631 1
a1631 1
		} else if (pv->pv_pmap != NULL) {
d1642 1
d1646 8
a1653 1
	splx(s);
d1713 4
a1716 1
	int s;
a1719 1
	s = splvm();
d1759 1
a1759 2
			if (pmap == npv->pv_pmap && va == npv->pv_va) {
				splx(s);
a1760 1
			}
d1768 1
a1768 2
		if (npv == NULL) {
			splx(s);
a1769 1
		}
d1788 4
a1791 1
				pmap_page_cache(pg, 0);
a1806 1
	splx(s);
d1818 2
a1819 1
	int s;
d1831 2
a1832 2
	pv = pg_to_pvh(pg);
	s = splvm();
d1838 1
d1867 1
a1874 1
		pv = pg_to_pvh(pg);
d1886 1
a1886 1
			pmap_page_cache(pg, PGF_CACHED);
d1890 1
a1890 1
	splx(s);
@


1.80
log
@Make mips64 pmap prepared for the unlocked reaper.

ok miod@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2015/05/02 14:33:19 jsg Exp $	*/
d598 1
a598 2
	if (pmap)
		pmap->pm_count++;
a646 3

	if (pmap == NULL)
		return;
@


1.79
log
@add missing splx calls
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2014/12/17 15:05:54 deraadt Exp $	*/
d126 1
a126 1
#define stat_count(what)	(what)++
d652 1
d680 1
d723 1
@


1.78
log
@pretty easy removal of simplelocks
ok miod (a while back)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2014/11/16 12:30:58 deraadt Exp $	*/
d1613 1
@


1.77
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2014/09/30 06:51:58 jmatthew Exp $	*/
a379 1
	simple_lock_init(&pmap_kernel()->pm_lock);
a512 1
	simple_lock_init(&pmap->pm_lock);
a550 1
	simple_lock(&pmap->pm_lock);
a551 1
	simple_unlock(&pmap->pm_lock);
d598 1
a598 2
	if (pmap) {
		simple_lock(&pmap->pm_lock);
a599 2
		simple_unlock(&pmap->pm_lock);
	}
@


1.76
log
@implement atomic operations using ll/sc, and convert rw_cas and callers of the
pre-existing atomics to match.

tested on sgi (octane) and octeon (erl)
ok miod@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2014/05/10 22:25:16 jasper Exp $	*/
d742 1
a742 1
	if (prot == VM_PROT_NONE) {
d750 2
a751 2
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
d755 2
a756 2
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
d799 1
a799 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
d804 1
a804 1
	p = (prot & VM_PROT_WRITE) ? PG_M : PG_RO;
d912 1
a912 1
		if (!(prot & VM_PROT_WRITE)) {
d935 1
a935 1
		if (flags & VM_PROT_WRITE)
d938 1
a938 1
		else if (flags & VM_PROT_ALL)
d948 1
a948 1
		if (prot & VM_PROT_WRITE) {
d1055 1
a1055 1
		if (pg != NULL && (prot & VM_PROT_EXECUTE)) {
d1073 1
a1073 1
	if (pg != NULL && (prot & VM_PROT_EXECUTE))
d1094 1
a1094 1
	if (prot & VM_PROT_WRITE)
@


1.75
log
@various format string fixes and remove -Wno-format from octeon

feedback/ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2014/04/04 20:52:05 miod Exp $	*/
d38 1
@


1.74
log
@Second step of the R4000 EOP errata WAR: when pmap invalidates a page which
is currently being covered by the wired TLB entries, flush them, so that,
if the process' pc is still running in a vulnerable page, the WAR will
reapply immediately and fault the next page.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2014/03/31 20:21:19 miod Exp $	*/
d651 1
a651 1
		("pmap_remove(%p, %p, %p)\n", pmap, sva, eva));
d663 2
a664 1
			panic("pmap_remove(%p, %p): not in range", sva, eva);
d742 1
a742 1
		DPRINTF(PDB_REMOVE, ("pmap_page_protect(%p, %p)\n", pg, prot));
d745 1
a745 1
			("pmap_page_protect(%p, %p)\n", pg, prot));
d795 2
a796 1
		("pmap_protect(%p, %p, %p, %p)\n", pmap, sva, eva, prot));
d817 2
a818 1
			panic("pmap_protect(%p, %p): not in range", sva, eva);
d892 2
a893 1
		("pmap_enter(%p, %p, %p, %p, %p)\n", pmap, va, pa, prot, flags));
d900 1
a900 1
			panic("pmap_enter: kva %p", va);
d904 1
a904 1
			panic("pmap_enter: uva %p", va);
d1084 1
a1084 1
		("pmap_kenter_pa(%p, %p, 0x%x)\n", va, pa, prot));
d1089 1
a1089 1
		panic("pmap_kenter_pa: kva %p", va);
d1121 2
a1122 1
	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_kremove(%p, %p)\n", va, len));
d1128 1
a1128 1
		panic("pmap_kremove: va %p len %p", va, len);
d1193 1
a1193 1
				panic("pmap_extract(%p, %p)", pmap, va);
d1215 2
a1216 1
	DPRINTF(PDB_FOLLOW, ("pmap_extract(%p, %p)=%p(%d)", pmap, va, pa, rv));
d1250 2
a1251 2
	DPRINTF(PDB_FOLLOW, ("pmap_copy(%p, %p, %p, %p, %p)\n",
	       dst_pmap, src_pmap, dst_addr, len, src_addr));
d1266 1
a1266 1
	DPRINTF(PDB_FOLLOW, ("pmap_zero_page(%p)\n", phys));
d1308 2
a1309 1
	DPRINTF(PDB_FOLLOW, ("pmap_copy_page(%p, %p)\n", src, dst));
d1356 2
a1357 1
	DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%p)\n", VM_PAGE_TO_PHYS(pg)));
d1372 2
a1373 1
				panic("pmap_clear_modify(%p)", pv->pv_va);
d1422 2
a1423 1
	DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%p)\n", VM_PAGE_TO_PHYS(pg)));
d1491 1
a1491 1
				panic("pmap_page_cache(%p)", pv->pv_va);
d1586 1
a1586 1
				pmap, va, VM_PAGE_TO_PHYS(pg)));
d1625 1
a1625 1
			    pmap, va, VM_PAGE_TO_PHYS(pg)));
d1645 1
a1645 1
#ifdef PMAP_DEBUG
d1647 2
a1648 2
				    __func__, VM_PAGE_TO_PHYS(pg),
				    pv->pv_va, va);
d1681 1
a1681 1
		("pmap_remove_pv(%p, %p, %p)\n", pmap, va, pa));
d1720 1
a1720 1
			    pmap, va, pa);
d1740 1
a1740 1
#ifdef PMAP_DEBUG
d1742 1
a1742 1
			    __func__, VM_PAGE_TO_PHYS(pg), va);
@


1.73
log
@Due the virtually indexed nature of the L1 instruction cache on most mips
processors, every time a new text page is mapped in a pmap, the L1 I$ is
flushed for the va spanned by this page.

Since we map pages of our binaries upon demand, as they get faulted in, but
uvm_fault() tries to map the few neighbour pages, this can end up in a
bunch of pmap_enter() calls in a row, for executable mappings. If the L1
I$ is small enough, this can cause the whole L1 I$ cache to be flushed
several times.

Change pmap_enter() to postpone these flushes by only registering the
pending flushes, and have pmap_update() perform them. The cpu-specific
cache code can then optimize this to avoid unnecessary operations.

Tested on R4000SC, R4600SC, R5000SC, RM7000, R10000 with 4KB and 16KB
page sizes (coherent and non-coherent designs), and Loongson 2F by mikeb@@ and
me. Should not affect anything on Octeon since there is no way to flush a
subset of I$ anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2014/03/22 00:01:04 miod Exp $	*/
d322 8
a329 2
	    pmap_asid_info[cpuid].pma_asidgen)
		tlb_flush_addr(va | asid);
@


1.72
log
@Second draft of my attempt to workaround the infamous R4000 end-of-page errata,
affecting R4000 processors revision 2.x and below (found on most R4000 Indigo
and a few R4000 Indy).

Since this errata gets triggered by TLB misses when the code flow crosses a
page boundary, this code attempts to identify code pages prone to trigger the
errata, and force the next page to be mapped for at least as long as the
current pc lies in the troublesome page, by creating wiring extra TLB entries.
These entries get recycled in a lazy-but-aggressive-enough way, either because
of context switches, or because of further tlb exceptions reaching trap().

The errata workaround code is only compiled on R4000-capable kernels (i.e.
sgi GENERIC-IP22 and nothing else), and only enabled on affected processors
(i.e. not on R4000 revision 3, or on R4400).

There is still room for improvemnt in unlucky cases, but in this simple enough
incarnation, this allows my R4000 2.2 Indigo to finally reliably boot multiuser,
even though both /sbin/init and /bin/sh contain code pages which can trigger
the errata.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2014/03/21 21:49:45 miod Exp $	*/
d1845 6
@


1.71
log
@Rename the symbolic constants for the pmap-specific vm_pag pg_flags from
PV_xxx to PGF_xxx for consistency (these are not stored in pvlist entries
anymore since years). The PG_ prefix can't be used here because of name
conflicts with <machine/pte.h> names, and I'd rather not rename the pte
constants.

No functional change. But it makes my life easier.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2014/03/21 21:39:35 miod Exp $	*/
a61 1
void	 pmap_page_cache(vm_page_t, int);
d1038 18
d1060 1
a1060 1
	 *  If mapping a memory space address invalidate ICache.
d1268 4
d1321 6
d1457 1
a1457 1
pmap_page_cache(vm_page_t pg, int mode)
@


1.70
log
@Allow for two more pmap-specific bits in vm_page pg_flags. Define
PG_PMAPMASK as all the possible pmap-specific bits (similar to the other
PG_fooMASK) to make sure MI code does not need to be updated, the next time
more bits are allocated to greedy pmaps.

No functional change, soon to be used by the (greedy) mips64 pmap.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2014/03/10 21:17:58 miod Exp $	*/
d912 1
a912 1
				if (pg->pg_flags & PV_ATTR_MOD) {
d927 1
a927 1
			    PV_ATTR_MOD | PV_ATTR_REF);
d929 1
a929 1
			atomic_setbits_int(&pg->pg_flags, PV_ATTR_REF);
d1241 1
a1241 1
	if (pg->pg_flags & PV_UNCACHED)
d1243 1
a1243 1
	else if (pg->pg_flags & PV_CACHED) {
d1278 1
a1278 1
	if (srcpg->pg_flags & PV_UNCACHED)
d1280 1
a1280 1
	else if (srcpg->pg_flags & PV_CACHED) {
d1286 1
a1286 1
	if (dstpg->pg_flags & PV_UNCACHED)
d1288 1
a1288 1
	else if (dstpg->pg_flags & PV_CACHED) {
d1321 2
a1322 2
	if (pg->pg_flags & PV_ATTR_MOD) {
		atomic_clearbits_int(&pg->pg_flags, PV_ATTR_MOD);
d1336 1
a1336 1
				if (pg->pg_flags & PV_CACHED)
d1350 1
a1350 1
				if (pg->pg_flags & PV_CACHED)
d1367 1
a1367 1
	atomic_setbits_int(&pg->pg_flags, PV_ATTR_MOD | PV_ATTR_REF);
d1382 2
a1383 2
	rv = (pg->pg_flags & PV_ATTR_REF) != 0;
	atomic_clearbits_int(&pg->pg_flags, PV_ATTR_REF);
d1396 1
a1396 1
	return (pg->pg_flags & PV_ATTR_REF) != 0;
d1408 1
a1408 1
	return (pg->pg_flags & PV_ATTR_MOD) != 0;
d1439 1
a1439 1
	newmode = mode & PV_CACHED ? PG_CACHED : PG_UNCACHED;
d1469 1
a1469 1
	atomic_clearbits_int(&pg->pg_flags, PV_CACHED | PV_UNCACHED);
d1549 1
a1549 1
			atomic_setbits_int(&pg->pg_flags, PV_CACHED);
d1551 1
a1551 1
			atomic_setbits_int(&pg->pg_flags, PV_UNCACHED);
d1560 1
a1560 1
		if ((pg->pg_flags & PV_CACHED) == 0) {
d1591 1
a1591 1
		    (pg->pg_flags & PV_CACHED) != 0 && cache_valias_mask != 0) {
d1662 1
a1662 1
			    PG_PMAPMASK & ~PV_PRESERVE);
d1682 1
a1682 1
	if ((pg->pg_flags & (PV_CACHED | PV_UNCACHED)) == 0 &&
d1701 1
a1701 1
			pmap_page_cache(pg, PV_CACHED);
@


1.69
log
@Support PMAP_NOCACHE in pmap_enter() flags. If set when mapping a managed
page, the pte is created uncached.

Make sure pmap_enter_pv() honours the cache bits of the pte, instead of
assuming it will only get called for cached pages. Have it set PV_UNCACHED
in the pv flags for the page, if this is the first use of this page and the
mapping is not cached. Only check for a virtual aliasing cache condition if
the new mapping is cached.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2014/02/08 09:34:04 miod Exp $	*/
d1662 1
a1662 2
			    (PG_PMAP0 | PG_PMAP1 | PG_PMAP2 | PG_PMAP3) &
			    ~PV_PRESERVE);
@


1.68
log
@Allow page table entries to be either 32 bits wide (the existing state of the
code), or 64 bits wide, if option MIPS_PTE64.
64-bit ptes allow for physical memory beyond 16GB (34 bits) to be addressable
by the pmap code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2014/01/08 17:12:18 miod Exp $	*/
d919 4
d1548 4
a1551 1
		atomic_setbits_int(&pg->pg_flags, PV_CACHED);
d1590 2
a1591 1
		if ((pg->pg_flags & PV_CACHED) != 0 && cache_valias_mask != 0) {
@


1.67
log
@Be sure to have pmap_extract() return FALSE for existing, but non-valid user
pmap ptes.  Found the hard way by tobiasu@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2012/10/03 22:46:07 miod Exp $	*/
d980 1
a980 2
	if (!(pte = pmap_segmap(pmap, va))) {
		pt_entry_t *ptepg;
d986 2
a987 2
		ptepg = (pt_entry_t *)pool_get(&pmap_pg_pool, wflags);
		if (ptepg == NULL)
d990 1
a990 1
		pmap_segmap(pmap, va) = pte = ptepg;
d1417 1
a1417 1
	return (entry & PG_RO);
@


1.66
log
@Don't include <mips64/archtype.h> unless you really need it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2012/09/29 21:37:03 miod Exp $	*/
d1175 4
a1178 1
			pa = pfn_to_pad(*pte) | (va & PAGE_MASK);
@


1.65
log
@Basic R8000 processor support. R8000 processors require MMU-specific code,
exception-specific code, clock-specific code, and L1 cache-specific code. L2
cache is per-design, of which only two exist: SGI Power Indigo2 (IP26) and SGI
Power Challenge (IP21) and are not covered by this commit.

R8000 processors also are 64-bit only processors with 64-bit coprocessor 0
registers, and lack so-called ``compatibility'' memory spaces allowing 32-bit
code to run with sign-extended addresses and registers.

The intrusive changes are covered by #ifdef CPU_R8000 stanzas. However,
trap() is split into a high-level wrapper and a new function, itsa(),
responsible for the actual trap servicing (which name couldn't be helped
because I'm an incorrigible punster). While an R8000 exception may cause
(via trap() ) multiple exceptions to be serviced, non-R8000 processors will
always service one exception in trap(), but they are nevertheless affected
by this code split.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2012/09/29 19:11:08 miod Exp $	*/
a38 1
#include <mips64/archtype.h>
@


1.64
log
@Kill the mostly unused VMTLB_xxx and VMNUM_xxx defines. Move all tlb
knowledge to <machine/pte.h>. Add specific routines for tlb handling setup
(at cpu initialization time) and tlb ASID wrap.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2012/05/10 21:12:26 miod Exp $	*/
d347 1
d349 1
d362 1
d365 1
d378 1
d389 4
d448 1
d455 1
d569 1
a569 1
					panic("pmap_destroy: segmap not empty");
d1151 1
d1158 1
d1770 1
d1777 1
d1793 1
d1797 1
@


1.63
log
@The uvm_map() changes introduced about two months ago yield a different
memory allocation pattern, exposing aliasing bugs in the mips64 pmap, on
kernels where virtual aliasing may happen (that is, IP32 kernels on R5000,
R10000 and R12000 O2).

Fix these (or at least, sweep them better under the rug) by:
- always flush caches in pmap_copy_page() and pmap_zero_page(), if the
  destination page is mapped uncached.
- when removing a mapping of a managed page which has been forced uncached
  due to virtual aliasing conflict, map it back cached as soon as the
  remaining mappings are non conflicting.
- writeback, instead of simply invalidating, pool pages and page table
  pages, upon release, for there might be stale data in the cache.

While these, apply more paranoia and only perform cache operations on pages
which have been mapped with the cache enabled.

Initially reported by mikeb@@ on an R12k O2 (that will teach me to use an
RM7000-powered O2, without virtual aliasing, to test IP32 kernels).
Verified on an R5k O2, as well as a custom IP30 kernel with page size
forced to 4KB (to introduce virtual aliasing).

This diff tested on O2 (R5k, RM7k, R12k), IP30, IP35, as well as on
Loongson (no aliasing) by mikeb@@ and I.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2012/04/25 22:07:35 miod Exp $	*/
d210 1
a210 1
		asid = pmap->pm_asid[cpuid].pma_asid << VMTLB_PID_SHIFT;
d230 1
a230 1
	asid = pmap->pm_asid[cpuid].pma_asid << VMTLB_PID_SHIFT;
d292 1
a292 1
		asid = pmap->pm_asid[cpuid].pma_asid << VMTLB_PID_SHIFT;
d313 1
a313 1
	asid = pmap->pm_asid[cpuid].pma_asid << VMTLB_PID_SHIFT;
d321 1
a321 1
	u_long asid = pmap->pm_asid[cpuid].pma_asid << VMTLB_PID_SHIFT;
d332 1
a332 1
	u_long asid = pmap->pm_asid[cpuid].pma_asid << VMTLB_PID_SHIFT;
d387 1
a387 1
		pmap_asid_info[i].pma_asid = 2;
d507 1
a507 1
		 * in mach_init().
d510 1
a510 1
			pmap->pm_asid[i].pma_asid = 1;
d651 1
a651 1
		for(; sva < eva; sva += NBPG, pte++) {
d692 1
a692 1
		for (; sva < nssva; sva += NBPG, pte++) {
d775 1
a775 2
	pt_entry_t *pte, entry;
	u_int p;
d803 1
a803 1
		for (; sva < eva; sva += NBPG, pte++) {
d840 1
a840 1
		for (; sva < nssva; sva += NBPG, pte++) {
d942 1
a942 1
			pmap_remove(pmap, va, va + NBPG);
d1000 1
a1000 1
		pmap_remove(pmap, va, va + NBPG);
d1477 2
a1478 2
		if (id >= VMNUM_PIDS) {
			tlb_flush(ci->ci_hw.tlbsize);
d1482 1
a1482 1
			id = 1;
@


1.62
log
@Skip cache flushes in pmap_zero_page() and pmap_copy_page() on systems
without virtual aliasing, or for pages which are not currently mapped
cached. 1+% speed increase on sgi IP27 and loongson kernels.

Tested on LS2F, R4400, R5000, R12000 and R16000.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2012/04/24 20:02:03 miod Exp $	*/
d632 2
d658 4
a661 1
			pmap_remove_pv(pmap, sva, pfn_to_pad(entry));
d699 4
a702 1
			pmap_remove_pv(pmap, sva, pfn_to_pad(entry));
d1095 2
a1096 1
		Mips_HitSyncDCache(ci, va, PAGE_SIZE);
d1218 1
a1218 1
	int df = cache_valias_mask != 0;
d1224 6
a1229 3
	if ((pg->pg_flags & PV_CACHED) &&
	    (df = ((pv->pv_va ^ va) & cache_valias_mask) != 0)) {
		Mips_SyncDCachePage(ci, pv->pv_va, phys);
d1232 1
a1232 1
	if (df)
d1252 1
a1252 1
	sf = df = cache_valias_mask != 0;
d1261 6
a1266 3
	if ((srcpg->pg_flags & PV_CACHED) &&
	    (sf = ((pv->pv_va ^ s) & cache_valias_mask) != 0)) {
		Mips_SyncDCachePage(ci, pv->pv_va, src);
d1269 6
a1274 3
	if ((dstpg->pg_flags & PV_CACHED) &&
	    (df = ((pv->pv_va ^ d) & cache_valias_mask) != 0)) {
		Mips_SyncDCachePage(ci, pv->pv_va, dst);
d1280 2
a1281 2
		Mips_HitSyncDCache(ci, s, PAGE_SIZE);
	if (df)
d1296 2
d1301 1
a1307 2
	if (pg->pg_flags & PV_CACHED)
		Mips_SyncDCachePage(curcpu(), pv->pv_va, VM_PAGE_TO_PHYS(pg));
d1319 3
d1333 2
d1420 1
a1420 1
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_page_uncache(%p)\n", pg));
d1422 1
a1422 1
	newmode = mode & PV_UNCACHED ? PG_UNCACHED : PG_CACHED;
d1440 1
a1440 1
		} else {
d1535 6
a1540 1
		if (pg->pg_flags & PV_UNCACHED) {
d1542 4
a1545 3
			 * If page is mapped uncached it's either because
			 * an uncached mapping was requested or we have a
			 * VAC situation. Map this page uncached as well.
a1547 20
		} else if (cache_valias_mask != 0) {
			/*
			 * We have a VAC possibility.  Check if virtual
			 * address of current mappings are compatible
			 * with this new mapping. Only need to check first
			 * since all others have been checked compatible
			 * when added. If they are incompatible, remove
			 * all mappings, flush the cache and set page
			 * to be mapped uncached.
			 */
			if (((pv->pv_va ^ va) & cache_valias_mask) != 0) {
#ifdef PMAP_DEBUG
				printf("pmap_enter: VAC for pa %p, %p !=  %p\n",
				    VM_PAGE_TO_PHYS(pg), npv->pv_va, va);
#endif
				pmap_page_cache(pg, PV_UNCACHED);
				Mips_SyncDCachePage(curcpu(), pv->pv_va,
				    VM_PAGE_TO_PHYS(pg));
				*npte = (*npte & ~PG_CACHEMODE) | PG_UNCACHED;
			}
d1551 1
a1551 4
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 *
		 * Note: the entry may already be in the table if
d1569 24
a1614 1
	struct cpu_info *ci = curcpu();
a1633 2
		if (pg->pg_flags & PV_CACHED)
			Mips_SyncDCachePage(ci, va, pa);
a1651 2
			if (pg->pg_flags & PV_CACHED)
				Mips_SyncDCachePage(ci, va, pa);
d1661 24
d1726 2
a1727 1
	Mips_HitInvalidateDCache(curcpu(), va, PAGE_SIZE);
d1789 1
a1789 1
		Mips_HitInvalidateDCache(curcpu(), va, PAGE_SIZE);
@


1.61
log
@Add support for wired mappings, using the last unused bit in the PTE.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2012/04/06 20:11:18 miod Exp $	*/
d1209 1
d1216 1
a1216 1
	    ((pv->pv_va ^ va) & cache_valias_mask) != 0) {
d1220 2
a1221 1
	Mips_HitSyncDCache(ci, va, PAGE_SIZE);
d1236 1
a1236 2
	int df = 1;
	int sf = 1;
d1240 1
d1261 1
a1261 1
	if (sf) {
d1263 2
a1264 2
	}
	Mips_HitSyncDCache(ci, d, PAGE_SIZE);
@


1.60
log
@Rework the signature of the cache handling routines again. It makes more sense
to pass both the virtual and physical addresses of the page to clean to
SyncDCachePage, which is the only routine using `Index' operations on the data
cache, which might be virtually indexed at some levels but physically indexed
at others. On the other hand, it does not make any sense to pass a physical
address to routines using `Hit' operations (and they were discarding them
anyway).

In addition to making things cleaner, this fixes sporadic userland misbehaviour
(read: SIGSGEV) on RM7000 O2 systems.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2012/04/19 18:12:40 miod Exp $	*/
a25 2
 *
 *	XXX This code needs some major rewriting.
d72 1
a72 1
static void pmap_invalidate_user_page(pmap_t, vaddr_t);
d74 5
a78 5
static void pmap_invalidate_kernel_page(vaddr_t);
static void pmap_invalidate_kernel_page_action(void *);
static void pmap_invalidate_user_page_action(void *);
static void pmap_update_kernel_page_action(void *);
static void pmap_update_user_page_action(void *);
d161 1
a161 1
static void
d177 1
a177 1
static void
d185 1
a185 1
static void
d222 1
a222 1
static void
d258 1
a258 1
static void
d279 1
a279 1
      		if (pmap->pm_asid[i].pma_asidgen != 
d304 1
a304 1
static void
d317 1
a317 1
static void
d653 2
d691 2
d864 1
a933 4
		npte |= vad_to_pfn(pa) | PG_G;
		if ((*pte & PG_V) == 0) {
			pmap->pm_stats.resident_count++;
		}
d938 13
d992 15
d1008 3
a1018 8
	if ((*pte & PG_V) && pa != pfn_to_pad(*pte)) {
		pmap_remove(pmap, va, va + NBPG);
		stat_count(enter_stats.mchange);
	}

	if ((*pte & PG_V) == 0) {
		pmap->pm_stats.resident_count++;
	}
d1045 1
a1045 1
	npte = vad_to_pfn(pa) | PG_G;
d1051 1
a1051 1
	if ((*pte & PG_V) == 0)
d1053 5
d1090 1
d1098 16
a1113 1
	/* XXX this pmap does not handle wired mappings yet... */
@


1.59
log
@Be sure to update the currently active ASID in pmap_activate() if invoked on
behalf of curproc.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2012/04/06 20:11:18 miod Exp $	*/
d800 1
a800 2
					Mips_HitSyncDCache(ci, sva,
					    pfn_to_pad(entry), PAGE_SIZE);
d1029 2
d1060 1
a1060 1
		Mips_HitSyncDCache(ci, va, pfn_to_pad(entry), PAGE_SIZE);
d1063 1
d1176 1
a1176 1
	Mips_HitSyncDCache(ci, va, phys, PAGE_SIZE);
d1217 1
a1217 1
		Mips_HitSyncDCache(ci, s, src, PAGE_SIZE);
d1219 1
a1219 1
	Mips_HitSyncDCache(ci, d, dst, PAGE_SIZE);
d1631 1
a1631 1
	Mips_HitInvalidateDCache(curcpu(), va, pa, PAGE_SIZE);
d1693 1
a1693 1
		Mips_HitInvalidateDCache(curcpu(), va, pa, PAGE_SIZE);
@


1.58
log
@Make the logic for PMAP_PREFER() and the logic, inside pmap, to do the
necessary cache coherency work wrt similar virtual indexes of different
physical pages, depending upon two distinct global variables, instead of
a shared one. R4000/R4400 VCE requires a 32KB mask for PMAP_PREFER, which
is otherwise not necessary for pmap coherency (especially since, on these
processors, only L1 uses virtual indexes, and the L1 size is not greater
than the page size, as we are using 16KB pages).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2012/03/25 13:52:52 miod Exp $	*/
d603 1
d607 3
a609 1
	pmap_alloc_tlbpid(p);
@


1.57
log
@Move cache handling routines related definitions to a dedicated header file,
rather than abusing <machine/cpu.h>.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2012/03/19 21:56:49 miod Exp $	*/
d147 2
d1125 2
a1126 2
	if (CpuCacheAliasMask != 0)
		va += (foff - va) & CpuCacheAliasMask;
d1167 1
a1167 1
	    ((pv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
d1200 1
a1200 1
	    (sf = ((pv->pv_va ^ s) & CpuCacheAliasMask) != 0)) {
d1205 1
a1205 1
	    (df = ((pv->pv_va ^ d) & CpuCacheAliasMask) != 0)) {
d1468 1
a1468 1
		} else if (CpuCacheAliasMask != 0) {
d1478 1
a1478 1
			if (((pv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
d1687 1
a1687 1
	if (CpuCacheAliasMask)
@


1.56
log
@On sgi, use CKSEG0 addresses whenever possible for pmap_map_direct and u area
pages. This will allow R5000-based systems with physical memory fitting in
CKSEG0 to use 16KB pages and direct maps (since only XKPHYS accesses trigger
the XKPHYS coherency errata on these processors).
Tested on IP32, IP30 and IP27 (and loongson too as well).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2012/03/19 20:42:26 miod Exp $	*/
d41 3
a44 1
#include <machine/autoconf.h>
a45 1
#include <mips64/archtype.h>
@


1.55
log
@Recent uvm code (and maybe not-so-recent, but it did not explode^WKASSERT at
my face then...) depends upon PMAP_PREFER_ALIGN to be a power of two, minus one.

On mips64 with 4KB pages, the runtime variable used to compute PMAP_PREFER_ALIGN
had the low PAGE_SHIFT bits zeroed (for no good reason I'd say). Don't bother
zeroing them anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2011/09/22 17:41:00 jasper Exp $	*/
a142 1
psize_t	mem_size;	/* memory size in bytes */
d439 1
a439 2
		 * If we are running with a 32 bit ARCBios (i.e. kernel
		 * linked in CKSEG0), return a CKSEG0 address whenever possible.
d441 3
a443 2
		if (IS_XKPHYS((vaddr_t)&pmap_steal_memory) ||
		    pa + size >= CKSEG_SIZE)
a444 2
		else
			va = PHYS_TO_CKSEG0(pa);
d446 1
a446 1
			va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
d1651 13
a1663 1
	vaddr_t va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
d1671 11
a1681 2
	paddr_t pa = XKPHYS_TO_PHYS(va);
	vm_page_t pg = PHYS_TO_VM_PAGE(pa);
d1683 1
@


1.54
log
@nowadays uvm_init() calls pmap_init(), not vm_init(); so update the comments.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2011/04/07 18:11:52 miod Exp $	*/
d1126 1
a1126 1
		va += (foff - va) & (CpuCacheAliasMask | PAGE_MASK);
@


1.53
log
@%x->%p in various format strings, to avoid truncating values.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2010/12/06 20:57:17 miod Exp $	*/
d461 1
a461 1
 *	Called by vm_init, to initialize any structures that the pmap
@


1.52
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2010/11/28 20:30:54 miod Exp $	*/
d537 1
a537 1
	DPRINTF(PDB_FOLLOW|PDB_CREATE, ("pmap_destroy(%x)\n", pmap));
d586 1
a586 1
	DPRINTF(PDB_FOLLOW, ("pmap_reference(%x)\n", pmap));
d633 1
a633 1
		("pmap_remove(%x, %x, %x)\n", pmap, sva, eva));
d1504 1
a1504 1
			("pmap_enter: new pv: pmap %x va %x pg %p\n",
d1580 1
a1580 1
			panic("pmap_remove_pv(%x, %x, %x) not found",
@


1.51
log
@Enable __HAVE_PMAP_DIRECT on mips64, unless the kernel is configured to
run on R5000 family processors (e.g. sgi GENERIC-IP32), where direct XKPHYS
mappings hit a silicon bug.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2010/11/24 20:59:19 miod Exp $	*/
d1122 2
a1123 2
void
pmap_prefer(paddr_t foff, vaddr_t *vap)
d1125 4
a1128 7
	if (CpuCacheAliasMask != 0) {
#if 1
		*vap += (foff - *vap) & (CpuCacheAliasMask | PAGE_MASK);
#else
		*vap += (*vap ^ foff) & CpuCacheAliasMask;
#endif
	}
@


1.50
log
@Implement a real pmap_proc_iflush() instead of relying on trap.c to perform
copious cache flushes behind our back.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2010/02/02 02:49:57 syuu Exp $	*/
d1651 23
@


1.49
log
@Skip calling smp_rendezvous if it's not necessary.
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2010/01/09 23:34:29 miod Exp $	*/
d1631 19
@


1.48
log
@Move cache information from global variables to per-cpu_info fields; this
allows processors with different cache sizes to be used.

Cache management routines now take a struct cpu_info * as first parameter.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2010/01/09 20:33:16 miod Exp $	*/
a72 1
static void pmap_invalidate_kernel_page(vaddr_t);
d75 1
d80 2
d188 1
a188 1
	struct pmap_invalidate_page_arg arg;
d207 2
a208 2
	arg.pmap = pmap;
	arg.va = va;
d210 10
a219 1
	smp_rendezvous_cpus(cpumask, pmap_invalidate_user_page_action, &arg);
d270 1
a270 1
	struct pmap_update_page_arg arg;
d289 13
a301 5
	arg.pmap = pmap;
	arg.va = va;
	arg.entry = entry;
	smp_rendezvous_cpus(cpumask,
			    pmap_update_user_page_action, &arg);
a317 6
pmap_invalidate_kernel_page(vaddr_t va)
{
	tlb_flush_addr(va);
}

static void
a325 6
}

void
pmap_update_kernel_page(vaddr_t va, pt_entry_t entry)
{
	tlb_update(va, entry);
@


1.47
log
@Define struct cpu_hwinfo, to hold hardware specific information about each
processor (instead of sys_config.cpu[]), and pass it in the attach_args
when attaching cpu devices.

This allows per-cpu information to be gathered late in the bootstrap process,
and not be limited by an arbitrary MAX_CPUS limit; this will suit IP27 and
IP35 systems better.

While there, use this information to make sure delay() uses the speed
information from the cpu it is invoked on.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2010/01/05 06:44:58 syuu Exp $	*/
d757 1
d790 1
a790 1
					Mips_HitSyncDCache(sva,
d828 1
a828 1
					Mips_SyncDCachePage(sva,
d851 2
a852 1
	u_long cpuid = cpu_number();
d995 1
a995 1
		Mips_InvalidateICache(va, PAGE_SIZE);
d1034 1
d1049 1
a1049 1
		Mips_HitSyncDCache(va, pfn_to_pad(entry), PAGE_SIZE);
d1156 1
d1164 1
a1164 1
		Mips_SyncDCachePage(pv->pv_va, phys);
d1167 1
a1167 1
	Mips_HitSyncDCache(va, phys, PAGE_SIZE);
d1185 1
d1197 1
a1197 1
		Mips_SyncDCachePage(pv->pv_va, src);
d1202 1
a1202 1
		Mips_SyncDCachePage(pv->pv_va, dst);
d1208 1
a1208 1
		Mips_HitSyncDCache(s, src, PAGE_SIZE);
d1210 1
a1210 1
	Mips_HitSyncDCache(d, dst, PAGE_SIZE);
d1234 1
a1234 1
		Mips_SyncDCachePage(pv->pv_va, VM_PAGE_TO_PHYS(pg));
d1480 1
a1480 1
				Mips_SyncDCachePage(pv->pv_va,
d1530 1
d1551 1
a1551 1
			Mips_SyncDCachePage(va, pa);
d1571 1
a1571 1
				Mips_SyncDCachePage(va, pa);
d1622 1
a1622 1
	Mips_HitInvalidateDCache(va, pa, PAGE_SIZE);
@


1.46
log
@Dynamic allocation for ASID and ASID generation number on struct pmap. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2009/12/30 01:17:59 syuu Exp $	*/
d1388 2
a1389 1
	u_long cpuid = cpu_number();
d1396 1
a1396 1
			tlb_flush(sys_config.cpu[0].tlbsize);
@


1.45
log
@curcpu()->ci_curpmap added. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2009/12/28 07:18:39 syuu Exp $	*/
d136 4
a139 2

struct pmap	kernel_pmap_store;
d145 1
a145 2
u_int		tlbpid_gen[MAXCPUS];	/* TLB PID generation count */
u_int		tlbpid_cnt[MAXCPUS];	/* next available TLB PID */
d195 2
a196 1
			if (pmap->pm_tlbgen[i] != tlbpid_gen[i])
d199 1
a199 1
				pmap->pm_tlbgen[i] = 0;
d219 1
a219 1
	asid = pmap->pm_tlbpid[cpuid] << VMTLB_PID_SHIFT;
d268 2
a269 1
			if (pmap->pm_tlbgen[i] != tlbpid_gen[i])
d272 1
a272 1
				pmap->pm_tlbgen[i] = 0;
d294 1
a294 1
	asid = pmap->pm_tlbpid[cpuid] << VMTLB_PID_SHIFT;
d308 1
a308 1
	u_long asid = pmap->pm_tlbpid[cpuid] << VMTLB_PID_SHIFT;
d310 2
a311 1
	if (pmap->pm_tlbgen[cpuid] == tlbpid_gen[cpuid])
d325 1
a325 1
	u_long asid = pmap->pm_tlbpid[cpuid] << VMTLB_PID_SHIFT;
d327 2
a328 1
	if (pmap->pm_tlbgen[cpuid] == tlbpid_gen[cpuid])
d337 1
a337 1
pmap_bootstrap()
d359 1
a359 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,"pmappl", NULL);
d379 2
a380 2
		tlbpid_gen[i] = 1;
		tlbpid_cnt[i] = 2;
d504 4
a507 3
		for (i = 0; i < MAXCPUS; i++) {
			pmap->pm_tlbpid[i] = 1;
			pmap->pm_tlbgen[i] = tlbpid_gen[i];
d511 3
a513 3
		for (i = 0; i < MAXCPUS; i++) {
			pmap->pm_tlbpid[i] = 0;
			pmap->pm_tlbgen[i] = 0;
d970 2
a971 1
	if (pmap->pm_tlbgen[cpuid] == tlbpid_gen[cpuid]) {
d973 1
a973 1
			npte, pmap->pm_tlbpid[cpuid]));
d1391 3
a1393 2
	if (pmap->pm_tlbgen[cpuid] != tlbpid_gen[cpuid]) {
		id = tlbpid_cnt[cpuid];
d1397 2
a1398 2
			if (++tlbpid_gen[cpuid] == 0)
				tlbpid_gen[cpuid] = 1;
d1401 4
a1404 3
		tlbpid_cnt[cpuid] = id + 1;
		pmap->pm_tlbpid[cpuid] = id;
		pmap->pm_tlbgen[cpuid] = tlbpid_gen[cpuid];
d1406 1
a1406 1
		id = pmap->pm_tlbpid[cpuid];
@


1.44
log
@Fix compile error caused from previous commit
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2009/12/28 06:55:27 syuu Exp $	*/
d196 1
a196 1
			else if (!(pmap->pm_active & m)) {
d268 1
a268 1
			else if (!(pmap->pm_active & m)) {
d589 1
a589 2
	unsigned int cpuid = cpu_number();
	unsigned int cpumask = (1 << cpuid);
d591 1
a591 1
	atomic_setbits_int(&pmap->pm_active, cpumask);
d602 1
a602 3
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
	unsigned int cpuid = cpu_number();
	unsigned int cpumask = (1 << cpuid);
d604 1
a604 1
	atomic_clearbits_int(&pmap->pm_active, cpumask);
@


1.43
log
@MP-safe pmap implemented, enable IPI in interrupt handler to avoid deadlock.
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2009/12/25 21:02:15 miod Exp $	*/
a476 1
	vaddr_t va;
@


1.42
log
@Pass both the virtual address and the physical address of the memory range
when invoking the cache functions. The physical address is needed when
operating on physically-indexed caches, such as the L2 cache on Loongson
processors.

Preprocessor abuse makes sure that the physical address computation gets
compiled out when running on a kernel compiled for virtually-indexed
caches only, such as the sgi kernel.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2009/12/07 18:58:34 miod Exp $	*/
d73 9
d143 2
a144 2
u_int		tlbpid_gen = 1;		/* TLB PID generation count */
u_int		tlbpid_cnt = 2;		/* next available TLB PID */
d151 177
d372 5
d477 2
a478 1
	int s;
d500 4
a503 2
		pmap->pm_tlbpid = 1;
		pmap->pm_tlbgen = tlbpid_gen;
d506 4
a509 2
		pmap->pm_tlbpid = 0;
		pmap->pm_tlbgen = 0;
d590 4
d604 5
a608 1
	/* Empty */
d649 1
a649 1
			tlb_flush_addr(sva);
d685 2
a686 5
			if (pmap->pm_tlbgen == tlbpid_gen) {
				tlb_flush_addr(sva |
				    (pmap->pm_tlbpid << VMTLB_PID_SHIFT));
				stat_count(remove_stats.flushes);
			}
d794 1
a794 1
			tlb_update(sva, entry);
d829 1
a829 4
			if (pmap->pm_tlbgen == tlbpid_gen)
				tlb_update(sva |
				    (pmap->pm_tlbpid << VMTLB_PID_SHIFT),
				    entry);
d848 1
d931 1
a931 1
		tlb_update(va, npte);
d968 1
a968 1
	if (pmap->pm_tlbgen == tlbpid_gen) {
d970 1
a970 1
			npte, pmap->pm_tlbpid));
d984 1
a984 3
	if (pmap->pm_tlbgen == tlbpid_gen) {
		tlb_update(va | (pmap->pm_tlbpid << VMTLB_PID_SHIFT), npte);
	}
d1016 1
a1016 1
	tlb_update(va, npte);
d1045 1
a1045 1
		tlb_flush_addr(va);
d1241 1
a1241 1
				tlb_update(pv->pv_va, entry);
d1252 1
a1252 4
				if (pv->pv_pmap->pm_tlbgen == tlbpid_gen)
					tlb_update(pv->pv_va |
					    (pv->pv_pmap->pm_tlbpid <<
					      VMTLB_PID_SHIFT), entry);
d1352 1
a1352 1
				tlb_update(pv->pv_va, entry);
d1361 1
a1361 4
					if (pv->pv_pmap->pm_tlbgen == tlbpid_gen)
						tlb_update(pv->pv_va |
						    (pv->pv_pmap->pm_tlbpid <<
						     VMTLB_PID_SHIFT), entry);
d1385 1
d1388 2
a1389 2
	if (pmap->pm_tlbgen != tlbpid_gen) {
		id = tlbpid_cnt;
d1393 2
a1394 2
			if (++tlbpid_gen == 0)
				tlbpid_gen = 1;
d1397 3
a1399 3
		tlbpid_cnt = id + 1;
		pmap->pm_tlbpid = id;
		pmap->pm_tlbgen = tlbpid_gen;
d1401 1
a1401 1
		id = pmap->pm_tlbpid;
@


1.41
log
@Use a pool to manage pmap pte pages and top level segment table, instead of
directly allocating pages from uvm; this will allow us to eventually use
a different kernel page size without having to alter the pmap structures
layout.
No functional change; measured slowdown of 1.6% for 4KB page kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2009/11/22 00:07:04 miod Exp $	*/
d586 2
a587 1
					Mips_HitSyncDCache(sva, PAGE_SIZE);
d624 2
a625 1
					Mips_SyncDCachePage(sva);
d846 1
a846 1
		Mips_HitSyncDCache(va, PAGE_SIZE);
d917 1
a917 1
	*vap += (foff - *vap) & (CpuCacheAliasMask | PAGE_MASK);
d919 1
a919 1
	*vap += (*vap ^ foff) & CpuCacheAliasMask;
d960 1
a960 1
		Mips_SyncDCachePage(pv->pv_va);
d963 1
a963 1
	Mips_HitSyncDCache(va, PAGE_SIZE);
d992 1
a992 1
		Mips_SyncDCachePage(pv->pv_va);
d997 1
a997 1
		Mips_SyncDCachePage(pv->pv_va);
d1003 1
a1003 1
		Mips_HitSyncDCache(s, PAGE_SIZE);
d1005 1
a1005 1
	Mips_HitSyncDCache(d, PAGE_SIZE);
d1029 1
a1029 1
		Mips_SyncDCachePage(pv->pv_va);
d1277 2
a1278 1
				Mips_SyncDCachePage(pv->pv_va);
d1347 1
a1347 1
			Mips_SyncDCachePage(va);
d1367 1
a1367 1
				Mips_SyncDCachePage(va);
d1415 2
a1416 1
	vm_page_t pg;
d1418 1
a1418 2
	Mips_HitInvalidateDCache(va, PAGE_SIZE);
	pg = PHYS_TO_VM_PAGE(XKPHYS_TO_PHYS(va));
@


1.40
log
@Allow mips ports to override VM_{MIN,MAX}_KERNEL_ADDRESS, and provide the
address as a kernel variable for use by libkvm.

On sgi IP27 and IP30 kernels, use XKSEG instead of CKSSEG; this will allow
kernel KVM size to grow in the future if necessary.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2009/11/19 20:16:27 miod Exp $	*/
d52 1
d62 10
a71 6
uint	pmap_alloc_tlbpid(struct proc *);
int	pmap_enter_pv(pmap_t, vaddr_t, vm_page_t, pt_entry_t *);
int	pmap_page_alloc(vaddr_t *);
void	pmap_page_free(vaddr_t);
void	pmap_page_cache(vm_page_t, int);
void	pmap_remove_pv(pmap_t, vaddr_t, paddr_t);
d170 2
d184 1
a184 1
	for (i = 0, spte = Sysmap; i < Sysmapsize; i++, spte++)
d235 1
d240 2
a241 1
		if (IS_XKPHYS((vaddr_t)&pmap_steal_memory))
d245 3
a285 1
	vaddr_t va;
d300 2
a301 6
	while (pmap_page_alloc(&va) != 0) {
		/* XXX What else can we do?  Deadlocks?  */
		uvm_wait("pmap_create");
	}

	pmap->pm_segtab = (struct segtab *)va;
d355 1
a355 1
			pmap_page_free((vaddr_t)pte);
d360 1
a360 1
		pmap_page_free((vaddr_t)pmap->pm_segtab);
d430 3
a432 2
		if (sva < VM_MIN_KERNEL_ADDRESS || eva < sva)
			panic("pmap_remove: kva not in range");
d575 3
a577 2
		if (sva < VM_MIN_KERNEL_ADDRESS || eva < sva)
			panic("pmap_protect: kva not in range");
d655 2
a656 1
		if (va < VM_MIN_KERNEL_ADDRESS)
d738 2
a739 1
		vaddr_t nva;
d741 6
a746 5
		while (pmap_page_alloc(&nva) != 0) {
			if (flags & PMAP_CANFAIL)
				return ENOMEM;
			uvm_wait("pmap_enter");
		}
d748 1
a748 1
		pmap_segmap(pmap, va) = pte = (pt_entry_t *)nva;
d804 6
d833 6
a839 1
	eva = va + len;
d913 1
d919 1
d1031 5
d1146 5
a1178 28
 *  Use this function to allocate pages for the mapping tables.
 *  Mapping tables are walked by the TLB miss code and are mapped in
 *  XKPHYS to avoid additional page faults when servicing a TLB miss.
 */
int
pmap_page_alloc(vaddr_t *ret)
{
	vm_page_t pg;

	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE | UVM_PGA_ZERO);
	if (pg == NULL)
		return ENOMEM;

	*ret = PHYS_TO_XKPHYS(VM_PAGE_TO_PHYS(pg), CCA_CACHED);
	return 0;
}

void
pmap_page_free(vaddr_t va)
{
	vm_page_t pg;

	Mips_HitInvalidateDCache(va, PAGE_SIZE);
	pg = PHYS_TO_VM_PAGE(XKPHYS_TO_PHYS(va));
	uvm_pagefree(pg);
}

/*
d1375 42
@


1.39
log
@Rename KSEG* defines to CKSEG* to match their names in 64 bit mode; also
define more 64 bit spaces.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2009/11/18 20:58:51 miod Exp $	*/
d134 1
@


1.38
log
@Stricter type usage (width and signedness); first step towards 64 bit ptes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2009/07/23 19:24:55 miod Exp $	*/
d229 1
a229 1
		 * linked in KSEG0), return a KSEG0 address whenever possible.
d234 1
a234 1
			va = PHYS_TO_KSEG0(pa);
d848 6
a853 6
		else if (va >= (vaddr_t)KSEG0_BASE &&
		    va < (vaddr_t)KSEG0_BASE + KSEG_SIZE)
			pa = KSEG0_TO_PHYS(va);
		else if (va >= (vaddr_t)KSEG1_BASE &&
		    va < (vaddr_t)KSEG1_BASE + KSEG_SIZE)
			pa = KSEG1_TO_PHYS(va);
@


1.37
log
@Get rid of bus_mem_add_mapping().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2009/04/25 20:35:31 miod Exp $	*/
d61 2
a62 2
int	pmap_alloc_tlbpid(struct proc *);
int	pmap_enter_pv(pmap_t, vaddr_t, vm_page_t, u_int *);
d130 1
a130 1
int		tlbpid_cnt = 2;		/* next available TLB PID */
d187 1
a187 1
	int npg;
d231 3
a233 2
		if ((vaddr_t)&pmap_steal_memory - KSEG0_BASE < KSEG_SIZE &&
		    pa + size < KSEG_SIZE)
a234 2
		else
			va = PHYS_TO_XKPHYS(pa, CCA_CACHED);
d473 2
a474 2
				tlb_flush_addr(sva | (pmap->pm_tlbpid <<
					VMTLB_PID_SHIFT));
d617 3
a619 2
				tlb_update(sva | (pmap->pm_tlbpid <<
					VMTLB_PID_SHIFT), entry);
d755 1
a755 1
		DPRINTF(PDB_ENTER, ("pmap_enter: new pte %x tlbpid %d\n",
d963 1
a963 1
	    (sf = ((pv->pv_va ^ (long)s) & CpuCacheAliasMask) != 0)) {
d968 1
a968 1
	    (df = ((pv->pv_va ^ (long)d) & CpuCacheAliasMask) != 0)) {
d1023 3
a1025 2
					tlb_update(pv->pv_va | (pv->pv_pmap->pm_tlbpid <<
						VMTLB_PID_SHIFT), entry);
d1089 1
a1089 1
pmap_is_page_ro(pmap_t pmap, vaddr_t va, int entry)
d1104 1
a1104 1
	u_int newmode;
d1130 3
a1132 2
						tlb_update(pv->pv_va | (pv->pv_pmap->pm_tlbpid <<
							VMTLB_PID_SHIFT), entry);
d1179 1
a1179 1
int
d1183 1
a1183 1
	int id;
d1210 1
a1210 1
	DPRINTF(PDB_FOLLOW|PDB_TLBPID, ("segtab %p tlbpid %d pid %d '%s'\n",
d1220 1
a1220 1
pmap_enter_pv(pmap_t pmap, vaddr_t va, vm_page_t pg, u_int *npte)
@


1.36
log
@Make pmap_steal_memory() return KSEG0 addresses only for kernels linked at
KSEG0 addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2008/09/23 04:34:02 miod Exp $	*/
a1364 37
}

/*==================================================================*/
/* Bus space map utility functions */

int
bus_mem_add_mapping(bus_addr_t bpa, bus_size_t size, int cacheable,
			bus_space_handle_t *bshp)
{
	bus_addr_t vaddr;
	bus_addr_t spa, epa;
	bus_size_t off;
	int len;

	spa = trunc_page(bpa);
	epa = bpa + size;
	off = bpa - spa;
	len = size+off;

	vaddr = uvm_km_valloc_wait(kernel_map, len);
	*bshp = vaddr + off;
#ifdef DEBUG_BUS_MEM_ADD_MAPPING
	printf("map bus %x size %x to %x vbase %x\n", bpa, size, *bshp, spa);
#endif
	for (; len > 0; len -= NBPG) {
		pt_entry_t *pte, npte;

		npte = vad_to_pfn(spa) | PG_G;
		npte |= PG_V | PG_M | PG_IOPAGE;
		pte = kvtopte(vaddr);
		*pte = npte;
		tlb_update(vaddr, npte);

		spa += NBPG;
		vaddr += NBPG;
	}
	return 0;
@


1.35
log
@In pmap_steal_memory(), when a memory segment is completely allocated,
remove it correctly from the array.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2008/06/14 10:55:20 mk Exp $	*/
d228 2
a229 1
		 * Prefer KSEG0 addresses for now, whenever possible.
d231 2
a232 1
		if (pa + size < KSEG_SIZE)
@


1.34
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2008/04/07 22:30:48 miod Exp $	*/
d186 1
a186 1
	int i, j, x;
d219 2
a220 2
			for (j = i; j < vm_nphysseg; x++)
				vm_physmem[x] = vm_physmem[x + 1];
@


1.33
log
@Use CCA_CACHED as the default CCA for all cached mappings and addresses.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2008/02/11 20:40:32 miod Exp $	*/
d281 1
a281 1
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
a282 1
	bzero(pmap, sizeof(*pmap));
@


1.32
log
@A couple fixes:
- evict the memory from cache in pmap_page_free().
- make sure to sync cache in pmap_protect() if it marks a modified cacheable
  page as read-only.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2008/01/15 19:44:50 miod Exp $	*/
a42 1
#include <machine/memconf.h>
d233 1
a233 1
			va = PHYS_TO_XKPHYS(pa, CCA_NONCOHERENT);
d927 1
a927 1
	va = (vaddr_t)PHYS_TO_XKPHYS(phys, CCA_NONCOHERENT);
d955 2
a956 2
	s = (vaddr_t)PHYS_TO_XKPHYS(src, CCA_NONCOHERENT);
	d = (vaddr_t)PHYS_TO_XKPHYS(dst, CCA_NONCOHERENT);
d1153 1
a1153 1
	*ret = PHYS_TO_XKPHYS(VM_PAGE_TO_PHYS(pg), CCA_NONCOHERENT);
@


1.31
log
@Revert r1.15 and flush pages (if cached) in pmap_remove_pv on every invocation,
instead of when removing the last page. It looked correct, but is defeated
by ``index'' cache invalidates.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2007/10/18 04:32:08 miod Exp $	*/
a347 1
			Mips_HitInvalidateDCache((vaddr_t)pte, PAGE_SIZE);
d575 3
d612 3
d1163 1
@


1.30
log
@Get rid of the silly union for mips pte. No functional change except
pmap.h now includes pte.h.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2007/07/18 20:06:07 miod Exp $	*/
d1327 2
a1337 1
			Mips_SyncDCachePage(va);
d1347 2
@


1.29
log
@Prefer ``hit'' cache operations when flushing kernel memory.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2007/06/20 16:51:17 miod Exp $	*/
a40 1
#include <machine/pte.h>
d178 1
a178 1
		spte->pt_entry = PG_G;
d344 1
a344 1
				if ((pte+j)->pt_entry)
d411 1
a411 2
	pt_entry_t *pte;
	unsigned entry;
a421 2
		pt_entry_t *pte;

d429 1
a429 1
			entry = pte->pt_entry;
d434 1
a434 1
			pte->pt_entry = PG_NV | PG_G;
d465 1
a465 1
			entry = pte->pt_entry;
d470 1
a470 1
			pte->pt_entry = PG_NV;
d545 1
a545 2
	pt_entry_t *pte;
	u_int entry;
d573 1
a573 1
			entry = pte->pt_entry;
d577 1
a577 1
			pte->pt_entry = entry;
d607 1
a607 1
			entry = pte->pt_entry;
d611 1
a611 1
			pte->pt_entry = entry;
d631 1
a631 2
	pt_entry_t *pte;
	u_int npte;
d702 1
a702 1
		if (!(pte->pt_entry & PG_V)) {
d705 1
a705 1
		if ((pte->pt_entry & PG_V) && pa != pfn_to_pad(pte->pt_entry)) {
d713 1
a713 1
		pte->pt_entry = npte;
d756 1
a756 1
	if ((pte->pt_entry & PG_V) && pa != pfn_to_pad(pte->pt_entry)) {
d761 1
a761 1
	if (!(pte->pt_entry & PG_V)) {
d764 1
a764 1
	pte->pt_entry = npte;
d781 1
a781 2
	pt_entry_t *pte;
	u_int npte;
d792 1
a792 1
	pte->pt_entry = npte;
d804 1
a804 1
	pt_entry_t *pte;
a805 1
	u_int entry;
d812 1
a812 1
		entry = pte->pt_entry;
d816 1
a816 1
		pte->pt_entry = PG_NV | PG_G;
d856 2
a857 3
			if (pte->pt_entry & PG_V)
				pa = pfn_to_pad(pte->pt_entry) |
				    (va & PAGE_MASK);
d866 1
a866 1
			pa = pfn_to_pad(pte->pt_entry) | (va & PAGE_MASK);
d983 1
a983 2
	pt_entry_t *pte;
	unsigned entry;
d1001 1
a1001 1
			entry = pte->pt_entry;
d1005 1
a1005 1
				pte->pt_entry = entry;
d1012 1
a1012 1
			entry = pte->pt_entry;
d1016 1
a1016 1
				pte->pt_entry = entry;
d1097 1
a1097 2
	pt_entry_t *pte;
	u_int entry;
d1110 1
a1110 1
			entry = pte->pt_entry;
d1113 1
a1113 1
				pte->pt_entry = entry;
d1119 1
a1119 1
				entry = pte->pt_entry;
d1122 1
a1122 1
					pte->pt_entry = entry;
d1381 1
a1381 2
		pt_entry_t *pte;
		u_int npte;
d1386 1
a1386 1
		pte->pt_entry = npte;
@


1.28
log
@Protect pool operations with splvm.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2007/05/03 19:34:00 miod Exp $	*/
d823 1
a823 1
		Mips_SyncDCachePage(va);
d981 1
a981 1
	Mips_SyncDCachePage(d);
@


1.27
log
@Enable support for > 512MB of physical memory on mips64 systems, by using
XKPHYS instead of KSEG[01] for direct mappings.

Then, detect memory above 256MB on O2 by poking at the CRIME registers
(ARCbios will not report memory above 256MB, which is mapped above 1GB
physical, to the system), and add it to the UVM managed memory.

Tested on r5k, rm5200 and r10k with and without more than 256MB, matching
hinv reports in all cases. CRIME memory decoding based on a diff from
kettenis@@ in december 2005.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2007/04/27 18:17:19 miod Exp $	*/
d275 2
d282 1
d284 1
d321 1
a321 1
	int count;
d361 1
d363 1
d438 1
a441 1
			pte->pt_entry = PG_NV | PG_G;
d497 1
a505 3
	if (pg == NULL)
		return;

d515 1
d526 1
d532 1
d537 1
a555 3
	if (pmap == NULL)
		return;

d995 1
d1000 1
d1033 1
d1110 1
a1115 2
	atomic_clearbits_int(&pg->pg_flags, PV_CACHED | PV_UNCACHED);
	atomic_setbits_int(&pg->pg_flags, mode);
d1117 1
d1141 3
d1226 1
d1230 1
d1293 2
a1294 1
		if (npv == NULL)
d1296 1
d1306 1
d1318 1
d1331 1
d1347 1
a1347 1
			Mips_SyncDCachePage(pv->pv_va);
d1366 1
@


1.26
log
@In pmap_enter_pv(), when uncaching a page due to a VAC conflict, only flush
the existing cached va range, instead of the whole cache.

Tested on rm5200 and r12k.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2007/04/27 18:14:13 miod Exp $	*/
d157 3
a159 1
	    PAGE_SIZE + 1;	/* + 1 to be even */
d228 9
a236 1
		va = PHYS_TO_KSEG0(pa);
d238 1
a238 1
		return(va);
d837 1
a837 1
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pa)
d840 1
d844 9
a852 4
		if (va >= (long)KSEG0_BASE &&
		    va < (long)(KSEG0_BASE + KSEG_SIZE)) {
			*pa = (long)KSEG0_TO_PHYS(va);
		} else {
d854 2
a855 1
			if (va < VM_MIN_KERNEL_ADDRESS) {
a856 1
			}
d860 2
a861 1
				*pa = pfn_to_pad(pte->pt_entry);
d870 1
a870 1
			*pa = pfn_to_pad(pte->pt_entry);
d874 1
a874 1
		*pa |= va & PGOFSET;
d876 1
a876 1
	DPRINTF(PDB_FOLLOW, ("pmap_extract(%p, %p)=%p(%d)", pmap, va, *pa, rv));
d927 1
a927 1
	va = (vaddr_t)PHYS_TO_KSEG0(phys);
d955 2
a956 2
	s = (vaddr_t)PHYS_TO_KSEG0(src);
	d = (vaddr_t)PHYS_TO_KSEG0(dst);
d1138 1
a1138 1
 *  KSEG0 to avoid additional page faults when servicing a TLB miss.
d1149 1
a1149 1
	*ret = PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(pg));
d1158 1
a1158 1
	pg = PHYS_TO_VM_PAGE(KSEG0_TO_PHYS(va));
@


1.25
log
@More pmap changes from the potpourri pool:
- use wm_page pg_flags pmap-reserved flags to store attributes, instead of
  defining a vm_page_md field.
- use atomic operations to touch the above mentioned flags.
- never create ptes with PG_RO and PG_M set (this was harmless anyway).
- in pmap_clear_modify(), do not flush cache if the page was mapped uncached.

Tested on r5k, rm5200, r10k and r12k.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2007/04/24 16:48:45 miod Exp $	*/
d1240 1
a1240 1
				Mips_SyncCache();
@


1.24
log
@It's never too late to write "too late" correctly.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2007/04/20 16:03:55 miod Exp $	*/
d254 1
a254 1
	return &pg->mdpage.pvent;
d457 1
a457 3
			if (!pfn_is_ext(entry)) {/* padr > 32 bits */
				pmap_remove_pv(pmap, sva, pfn_to_pad(entry));
			}
a642 4
		pv_entry_t pv;

		pv = pg_to_pvh(pg);

d646 1
a646 1
			if ((int64_t)va < 0) {
d653 1
a653 1
				if (pv->pv_flags & PV_ATTR_MOD) {
d663 2
a664 1
			pv->pv_flags |= PV_ATTR_MOD | PV_ATTR_REF;
d666 1
a666 1
			pv->pv_flags |= PV_ATTR_REF;
d678 1
a678 1
			npte = PG_IOPAGE & ~(PG_G | PG_M);
d692 1
a692 1
		npte |= vad_to_pfn(pa) | PG_ROPAGE | PG_G;
d912 1
a912 1
	if ((pv->pv_flags & PV_CACHED) &&
d944 1
a944 1
	if ((pv->pv_flags & PV_CACHED) &&
d949 1
a949 1
	if ((pv->pv_flags & PV_CACHED) &&
a958 5
#if 0	/* XXX TODO: Why can't we trust the following? */
	if (df || (pv->pv_pmap == NULL) || (pv->pv_flags & PV_EXEC)) {
		Mips_SyncDCachePage(d);
	}
#else
a959 1
#endif
d977 2
a978 2
	if (pv->pv_flags & PV_ATTR_MOD) {
		pv->pv_flags &= ~PV_ATTR_MOD;
d981 2
a982 1
	Mips_SyncDCachePage(pv->pv_va);
d1016 1
a1016 4
	pv_entry_t pv;

	pv = pg_to_pvh(pg);
	pv->pv_flags |= PV_ATTR_MOD | PV_ATTR_REF;
a1026 1
	pv_entry_t pv;
d1031 2
a1032 3
	pv = pg_to_pvh(pg);
	rv = (pv->pv_flags & PV_ATTR_REF) != 0;
	pv->pv_flags &= ~PV_ATTR_REF;
d1045 1
a1045 4
	pv_entry_t pv;

	pv = pg_to_pvh(pg);
	return (pv->pv_flags & PV_ATTR_REF) != 0;
d1057 1
a1057 4
	pv_entry_t pv;

	pv = pg_to_pvh(pg);
	return (pv->pv_flags & PV_ATTR_MOD) != 0;
d1090 2
a1091 1
	pv->pv_flags = (pv->pv_flags & ~(PV_CACHED|PV_UNCACHED)) | mode;
d1213 1
a1213 1
		pv->pv_flags = PV_CACHED;
d1217 1
a1217 1
		if (pv->pv_flags & PV_UNCACHED) {
d1220 1
a1220 1
			 * an uncached mapping was requested of we have a
a1261 1
		/* can this cause us to recurse forever? */
a1267 1
		npv->pv_flags = pv->pv_flags;
a1304 1
			npv->pv_flags = pv->pv_flags;
d1309 3
a1311 1
			pv->pv_flags &= PV_PRESERVE;
@


1.23
log
@Segment tables are integral multiples of the page size in LP64 mode, so
do not manage a freelist ourselves, but release them in pmap_destroy().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2007/04/14 14:54:30 miod Exp $	*/
d193 1
a193 1
		panic("pmap_steal_memory: to late, vm is running!");
@


1.22
log
@Crank KVM from a ridiculous pedro-sized 256MB to 1GB; needed for upcoming MI
changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2007/04/14 14:52:39 miod Exp $	*/
d66 1
a130 1
struct segtab	*free_segtab;		/* free list kept locally */
d264 1
a264 1
	int i;
a274 7
	if (free_segtab) {
		pmap->pm_segtab = free_segtab;
		free_segtab = *(struct segtab **)free_segtab;
		pmap->pm_segtab->seg_tab[0] = NULL;
	} else {
		struct segtab *stp;
		vaddr_t va;
d276 4
a279 4
		while (pmap_page_alloc(&va) != 0) {
			/* XXX What else can we do?  Deadlocks?  */
			uvm_wait("pmap_create");
		}
d281 1
a281 1
		pmap->pm_segtab = stp = (struct segtab *)va;
a282 7
		i = NBPG / sizeof(struct segtab);
		while (--i != 0) {
			stp++;
			*(struct segtab **)stp = free_segtab;
			free_segtab = stp;
		}
	}
d336 2
a337 1
			uvm_pagefree(PHYS_TO_VM_PAGE(KSEG0_TO_PHYS(pte)));
d339 1
d341 2
a342 2
		*(struct segtab **)pmap->pm_segtab = free_segtab;
		free_segtab = pmap->pm_segtab;
d344 1
a1146 2
	pv_entry_t pv;
	vaddr_t va;
d1152 8
a1159 2
	pv = pg_to_pvh(pg);
	va = PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(pg));
d1161 2
a1162 2
	*ret = va;
	return 0;
@


1.21
log
@Correctly handle pv_flags for pages used to store ptes, so that they
are in a correct state when we uvm_pagefree() them.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2006/06/06 17:34:21 miod Exp $	*/
d128 1
a128 2
vaddr_t	virtual_end =	/* VA of last avail page (end of kernel AS) */
	VM_MIN_KERNEL_ADDRESS + 65536 /* minimal Sysmapsize */ * PAGE_SIZE;
d144 1
a144 1
	int i;
a146 1

d151 1
a151 1
	 * Create at least 256MB of map even if physmem is smaller.
d153 2
a154 4
	if (physmem < 65536)
		Sysmapsize = 65536;
	else
		Sysmapsize = physmem;
d156 2
a157 2
	virtual_start = VM_MIN_KERNEL_ADDRESS;
	virtual_end = VM_MIN_KERNEL_ADDRESS + Sysmapsize * NBPG;
d159 2
a160 1
	Sysmap = (pt_entry_t *)uvm_pageboot_alloc(sizeof(pt_entry_t) * Sysmapsize);
d176 1
a176 1
	for(i = 0, spte = Sysmap; i < Sysmapsize; i++, spte++)
d423 1
a423 1
		if (sva < VM_MIN_KERNEL_ADDRESS || eva > virtual_end)
d571 1
a571 1
		if (sva < VM_MIN_KERNEL_ADDRESS || eva > virtual_end)
d644 1
a644 1
		if (va < VM_MIN_KERNEL_ADDRESS || va >= virtual_end)
d854 1
a854 1
			if (va < VM_MIN_KERNEL_ADDRESS || va >= virtual_end) {
@


1.20
log
@In pmap_enter(), when deciding to flush the icache for a page, invoke the
TLB-friendly cache function. From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2006/01/04 20:26:46 miod Exp $	*/
a1169 6
	if ((pv->pv_flags & PV_CACHED) &&
	    ((pv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
		Mips_SyncDCachePage(pv->pv_va);
	}
	pv->pv_va = va;
	pv->pv_flags = PV_CACHED;
d1337 1
a1337 1
			npv->pv_flags |= pv->pv_flags & PV_PRESERVE;
d1342 1
@


1.19
log
@Initialize virtual_end to a decent value, so that VM_MAX_KERNEL_ADDRESS is
meaningful before pmap_bootstrap() is invoked; gets us valid buffer numbers
computation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2005/12/10 11:45:43 miod Exp $	*/
d782 2
a783 8
#if 0
	/* XXX The following test have previously been unreliable!!! */
	if (pg != NULL &&
	    (prot & (VM_PROT_READ | VM_PROT_EXECUTE)) ==
	    (VM_PROT_READ | VM_PROT_EXECUTE))
#endif
	if (pg != NULL)
		Mips_InvalidateICachePage(va);
@


1.18
log
@{en,re}trys -> {en,re}tries; eyeballed by jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2004/10/20 12:49:15 pefo Exp $	*/
d128 2
a129 1
vaddr_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
d231 1
d234 1
a234 4
	if (va == 0)
		panic("pmap_steal_memory: no memory to steal");

	return(va);
d307 2
a308 3
		proc0paddr->u_pcb.pcb_segtab = (void *)pmap->pm_segtab;
	}
	else {
a1390 5

#ifdef DEBUG
	if (phys_map == NULL)
		panic("bus_mem_add_mapping when phys map not ready!");
#endif
@


1.17
log
@Fix some 64 bit address problems.
Some function names made more unique.
Other changes for the upcoming Origin 200 support.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2004/09/30 07:25:54 pefo Exp $	*/
d173 1
a173 1
	 * translation lookaside buffer. Thus invalid entrys
@


1.16
log
@put back ifdef PMAP_DEBUG on VAC warn. don't confuse ppl.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2004/09/29 17:39:20 pefo Exp $	*/
d114 1
a114 1
int pmapdebug = 0x0;
@


1.15
log
@move cache syncing for now. delayed sync needs more debug
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2004/09/27 17:40:24 pefo Exp $	*/
d1279 1
d1282 1
@


1.14
log
@Change busdma to map coherent dmamem memory as uncached if the host
system has non-coherent caches. This will help some drivers to work better.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 2004/09/23 12:38:28 pefo Exp $	*/
a1255 2
		Mips_SyncDCachePage(pv->pv_va);

d1354 1
@


1.13
log
@Clean clean clean... should be OK now
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 2004/09/23 08:42:38 pefo Exp $	*/
a51 7
/* flags for pv_entry */
#define	PV_UNCACHED	0x0001		/* Page is mapped unchached */
#define	PV_CACHED	0x0002		/* Page has been cached */
#define	PV_ATTR_MOD	0x0004
#define	PV_ATTR_REF	0x0008
#define	PV_PRESERVE (PV_UNCACHED|PV_CACHED|PV_ATTR_MOD|PV_ATTR_REF)

a1280 1
#ifdef PMAPDEBUG
a1282 1
#endif
@


1.12
log
@More pmap bugs cleaned out. Some old, some new.
Better structured pmap.c.
Evil still in there, more work to do.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 2004/09/22 07:27:22 miod Exp $	*/
d178 6
a183 5
	 * The R4?00 stores only one copy of the Global bit in the
	 * translation lookaside buffer for each 2 page entry.
	 * Thus invalid entrys must have the Global bit set so
	 * when Entry LO and Entry HI G bits are anded together
	 * they will produce a global bit to store in the tlb.
d193 1
a193 3
pmap_steal_memory(size, vstartp, vendp)
	vsize_t size;
	vaddr_t *vstartp, *vendp;
d196 1
a196 1
	int npgs;
d207 1
a207 1
	npgs = atop(size);
d216 1
a216 1
		if ((vm_physmem[i].avail_end - vm_physmem[i].avail_start) < npgs) {
a217 1
		}
d220 2
a221 2
		vm_physmem[i].avail_start += npgs;
		vm_physmem[i].start += npgs;
d224 1
a224 1
			if (vm_nphysseg == 1) {
a225 1
			}
d228 1
a228 1
			for(j = i; j < vm_nphysseg; x++) {
a229 1
			}
d231 1
a231 1
		if (vstartp) {
d233 1
a233 2
		}
		if (vendp) {
a234 1
		}
d236 1
a236 1
		memset((caddr_t)va, 0, size);
d239 1
a239 1
	if (va == 0) {
a240 1
	}
d331 1
a331 2
pmap_destroy(pmap)
	pmap_t pmap;
d377 1
a377 2
pmap_reference(pmap)
	pmap_t pmap;
d393 1
a393 2
pmap_activate(p)
	struct proc *p;
a395 1

a396 1

d404 1
a404 2
pmap_deactivate(p)
	struct proc *p;
d416 1
a416 3
pmap_remove(pmap, sva, eva)
	pmap_t pmap;
	vaddr_t sva, eva;
d426 2
a427 1
	if (pmap == NULL) {
a428 1
	}
d502 1
a502 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
d553 1
a553 4
pmap_protect(pmap, sva, eva, prot)
	pmap_t pmap;
	vaddr_t sva, eva;
	vm_prot_t prot;
d557 1
a557 1
	unsigned entry;
a626 3
			/*
			 * Update the TLB if the given address is in the cache.
			 */
d644 1
a644 6
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d804 1
a804 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d828 1
a828 3
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d849 1
a849 3
pmap_unwire(pmap, va)
	pmap_t pmap;
	vaddr_t va;
d861 1
a861 4
pmap_extract(pmap, va, pa)
	pmap_t	pmap;
	vaddr_t va;
	paddr_t *pa;
d903 1
a903 3
pmap_prefer(foff, vap)
	paddr_t foff;
	vaddr_t *vap;
d1053 1
a1053 2
pmap_set_modify(pg)
	struct vm_page *pg;
d1067 1
a1067 2
pmap_clear_reference(pg)
	struct vm_page *pg;
d1087 1
a1087 2
pmap_is_referenced(pg)
	struct vm_page *pg;
d1102 1
a1102 2
pmap_is_modified(pg)
	struct vm_page *pg;
d1206 1
a1206 2
pmap_alloc_tlbpid(p)
	struct proc *p;
a1333 1
 * Returns TRUE if it was the last mapping and cached, else FALSE.
d1353 3
a1355 4
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry. In either case we free the now unused entry.
a1364 1
			Mips_SyncDCachePage(pv->pv_va);
@


1.11
log
@Partially revert 1.9, the new mod/ref accounting has issues with mmap().
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2004/09/16 07:25:26 miod Exp $	*/
a40 2
#include <uvm/uvm.h>

d45 1
d48 2
d52 1
d57 1
d71 1
a100 2
#define stat_count(what)	(what)++

d116 5
d122 6
d261 1
a261 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_INIT)) {
		printf("pmap_init()\n");
	}
#endif
a274 24
int
pmap_page_alloc(vaddr_t *ret)
{
	vm_page_t pg;
	pv_entry_t pv;
	vaddr_t va;

	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE | UVM_PGA_ZERO);
	if (pg == NULL)
		return ENOMEM;

	pv = pg_to_pvh(pg);
	va = PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(pg));
	if ((pv->pv_flags & PV_CACHED) &&
	    ((pv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
		Mips_SyncDCachePage(pv->pv_va);
	}
	pv->pv_va = va;
	pv->pv_flags = PV_CACHED;

	*ret = va;
	return 0;
}

d286 1
a286 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE)) {
		printf("pmap_create()\n");
	}
#endif
d343 1
a343 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE)) {
		printf("pmap_destroy(%x)\n", pmap);
	}
#endif
d366 1
a366 1
					panic("pmap_release: segmap not empty");
d389 2
a390 5
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_reference(%x)\n", pmap);
	}
#endif
d437 3
a439 4
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT)) {
		printf("pmap_remove(%x, %x, %x)\n", pmap, sva, eva);
	}
a440 1
#endif
d463 1
a463 1
			pte->pt_entry = PG_NV | PG_G; /* See above about G bit */
a464 1
#ifdef PMAPDEBUG
a465 1
#endif
a504 1
#ifdef PMAPDEBUG
a505 1
#endif
d524 5
a528 4
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    ((prot == VM_PROT_NONE) && (pmapdebug & PDB_REMOVE))) {
		printf("pmap_page_protect(%x, %x)\n", pg, prot);
d530 1
a530 1
#endif
d580 3
a582 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) {
		printf("pmap_protect(%x, %x, %x, %x)\n", pmap, sva, eva, prot);
	}
#endif
a657 65
 *	Return RO protection of page.
 */
int
pmap_is_page_ro(pmap, va, entry)
	pmap_t	    pmap;
	vaddr_t va;
	int	entry;
{
	return(entry & PG_RO);
}

/*
 *	pmap_page_cache:
 *
 *	Change all mappings of a page to cached/uncached.
 */
void
pmap_page_cache(pg,mode)
	vm_page_t pg;
	int mode;
{
	pv_entry_t pv;
	pt_entry_t *pte;
	unsigned entry;
	unsigned newmode;

#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER)) {
		printf("pmap_page_uncache(%x)\n", pa);
	}
#endif
	newmode = mode & PV_UNCACHED ? PG_UNCACHED : PG_CACHED;
	pv = pg_to_pvh(pg);
	while (pv) {
		pv->pv_flags = (pv->pv_flags & ~PV_UNCACHED) | mode;
		if (pv->pv_pmap == pmap_kernel()) {
			/*
			 * Change entries in kernel pmap.
			 */
			pte = kvtopte(pv->pv_va);
			entry = pte->pt_entry;
			if (entry & PG_V) {
				entry = (entry & ~PG_CACHEMODE) | newmode;
				pte->pt_entry = entry;
				tlb_update(pv->pv_va, entry);
			}
		}
		else {
			if ((pte = pmap_segmap(pv->pv_pmap, pv->pv_va))) {
				pte += uvtopte(pv->pv_va);
				entry = pte->pt_entry;
				if (entry & PG_V) {
					entry = (entry & ~PG_CACHEMODE) | newmode;
					pte->pt_entry = entry;
					if (pv->pv_pmap->pm_tlbgen == tlbpid_gen)
						tlb_update(pv->pv_va | (pv->pv_pmap->pm_tlbpid <<
							VMTLB_PID_SHIFT), entry);
				}
			}
		}
		pv = pv->pv_next;
	}
}

/*
d678 3
a680 6
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER)) {
		printf("pmap_enter(%x, %x, %x, %x, %x)\n",
		       pmap, va, pa, prot, stat);
	}
#endif
d683 1
a683 3
#ifdef PMAPDEBUG
		enter_stats.kernel++;
#endif
d687 1
a687 3
#ifdef PMAPDEBUG
		enter_stats.user++;
#endif
d696 1
a696 1
		pv_entry_t pv, npv;
d703 1
a703 1
			if ((int)va < 0) {
a708 3
#if 0 /*XXX*/
				pg->flags &= ~PG_CLEAN;
#endif
d710 1
a710 1
				if (!(pg->flags & PG_CLEAN)) {
d718 6
a723 1
#ifdef PMAPDEBUG
a724 96
#endif
		/*
		 * Enter the pmap and virtual address into the
		 * physical to virtual map table.
		 */

#ifdef PMAPDEBUG
		if (pmapdebug & PDB_ENTER) {
			printf("pmap_enter: pv %x: was %x/%x/%x\n",
			       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
		}
#endif

		if (pv->pv_pmap == NULL) {
			/*
			 * No entries yet, use header as the first entry
			 */

#ifdef PMAPDEBUG
			if (pmapdebug & PDB_PVENTRY) {
				printf("pmap_enter: first pv: pmap %x va %x pa %p\n",
					pmap, va, pa);
			}
			stat_count(enter_stats.firstpv);
#endif

			Mips_SyncDCachePage(pv->pv_va);

			pv->pv_va = va;
			pv->pv_flags = PV_CACHED;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
		} else {
			if (pv->pv_flags & PV_UNCACHED) {
				npte = (npte & ~PG_CACHEMODE) | PG_UNCACHED;
			} else if (CpuCacheAliasMask != 0) {
			/*
			 * There is at least one other VA mapping this page.
			 * Check if they are cache index compatible. If not
			 * remove all mappings, flush the cache and set page
			 * to be mapped uncached. Caching will be restored
			 * when pages are mapped compatible again. NOT!
			 */
				for (npv = pv; npv; npv = npv->pv_next) {
					/*
					 * Check cache aliasing incompatibility
					 */
					if (((npv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
						printf("pmap_enter: uncached mapping for pa %p, va %p !=  %p.\n", pa, npv->pv_va, va);
						pmap_page_cache(pg,PV_UNCACHED);
						Mips_SyncCache();
						pv->pv_flags &= ~PV_CACHED;
						npte = (npte & ~PG_CACHEMODE) | PG_UNCACHED;
						break;
					}
				}
			}

			/*
			 * There is at least one other VA mapping this page.
			 * Place this entry after the header.
			 *
			 * Note: the entry may already be in the table if
			 * we are only changing the protection bits.
			 */
			for (npv = pv; npv; npv = npv->pv_next) {
				if (pmap == npv->pv_pmap && va == npv->pv_va) {
					goto fnd;
				}
			}

#ifdef PMAPDEBUG
			if (pmapdebug & PDB_PVENTRY) {
				printf("pmap_enter: new pv: pmap %x va %x pa %p\n",
					pmap, va, pa);
			}
#endif

			/* can this cause us to recurse forever? */
			npv = pmap_pv_alloc();
			if (npv == NULL) {
				panic("pmap_pv_alloc() failed");
			}
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_flags = pv->pv_flags;
			pv->pv_next = npv;

#ifdef PMAPDEBUG
			if (!npv->pv_next)
				stat_count(enter_stats.secondpv);
#endif
		fnd:
			;
		}
d728 1
a728 1
		 * then it must be device memory which may be volitile.
a729 1
#ifdef PMAPDEBUG
a730 1
#endif
d739 8
d752 1
a752 1
		if (pa != pfn_to_pad(pte->pt_entry)) {
a753 1
#ifdef PMAPDEBUG
a754 1
#endif
d762 1
a762 1
		return (KERN_SUCCESS);
d780 8
d796 5
a800 7

#ifdef PMAPDEBUG
	if (pmapdebug & PDB_ENTER) {
		printf("pmap_enter: new pte %x", npte);
		if (pmap->pm_tlbgen == tlbpid_gen)
			printf(" tlbpid %d", pmap->pm_tlbpid);
		printf("\n");
a801 1
#endif
d803 1
a803 1
	if (pa != pfn_to_pad(pte->pt_entry)) {
a804 1
#ifdef PMAPDEBUG
a805 1
#endif
d814 1
d816 11
a826 7
		/*
		 *  If mapping a memory space address invalidate ICache.
		 */
		if (pg != NULL) {
			Mips_InvalidateICachePage(va);
		}
	}
d840 2
a841 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER)) {
		printf("pmap_kenter_pa(%lx, %lx, %x)\n", va, pa, prot);
	}
#endif
d853 5
a857 27
void
pmap_kenter_cache(va, pa, prot, cache)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int cache;
{
	pt_entry_t *pte;
	u_int npte;

#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER)) {
		printf("pmap_kenter_cache(%lx, %lx, %x)\n", va, pa, prot);
	}
#endif

	npte = vad_to_pfn(pa) | PG_G;
	if (prot & VM_PROT_WRITE) {
		npte |= PG_M | cache;
	} else {
		npte |= PG_RO | cache;
	}
	pte = kvtopte(va);
	pte->pt_entry = npte;
	tlb_update(va, npte);
}

d867 1
a867 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE)) {
		printf("pmap_kremove(%lx, %lx)\n", va, len);
	}
#endif
d873 1
a873 1
		if (entry & PG_V) {
a874 1
		}
a903 6
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_extract(%x, %x) -> ", pmap, va);
	}
#endif

d905 2
a906 1
		if (va >= (long)KSEG0_BASE && va < (long)(KSEG0_BASE + KSEG_SIZE)) {
d911 1
a911 1
				panic("pmap_extract(%x, %x)", pmap, va);
d931 1
a931 5
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_extract: pa %x\n", *pa);
	}
#endif
d945 1
d947 3
d968 2
a969 6
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_copy(%x, %x, %x, %x, %x)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
	}
#endif
d982 1
a982 5
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_zero_page(%x)\n", phys);
	}
#endif
d1004 2
a1005 12
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
#if 0
	int *s, *d;
/*	if (CpuCacheAliasMask == 0) {  XXX */
	s = (int *)PHYS_TO_KSEG0(src);
	d = (int *)PHYS_TO_KSEG0(dst);

	memcpy(d, s, PAGE_SIZE);

#else
	int *s, *d, *end;
a1007 1
	int tmp0, tmp1, tmp2, tmp3;
d1010 6
a1015 7
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_copy_page(%x, %x)\n", src, dst);
	}
#endif
	s = (int *)(vaddr_t)PHYS_TO_KSEG0(src);
	d = (int *)(vaddr_t)PHYS_TO_KSEG0(dst);
d1019 1
a1019 1
		   (sf = ((pv->pv_va ^ (long)s) & CpuCacheAliasMask) != 0)) {
d1024 1
a1024 1
		   (df = ((pv->pv_va ^ (long)d) & CpuCacheAliasMask) != 0)) {
d1028 1
a1028 7
	end = s + PAGE_SIZE / sizeof(int);
	do {
		tmp0 = s[0]; tmp1 = s[1]; tmp2 = s[2]; tmp3 = s[3];
		d[0] = tmp0; d[1] = tmp1; d[2] = tmp2; d[3] = tmp3;
		s += 4;
		d += 4;
	} while (s != end);
d1031 1
a1031 1
		Mips_HitSyncDCache((vaddr_t)PHYS_TO_KSEG0(src), PAGE_SIZE);
d1035 1
a1035 1
		Mips_HitSyncDCachePage(dst);
d1038 1
a1038 2
	Mips_HitSyncDCache((vaddr_t)PHYS_TO_KSEG0(dst), PAGE_SIZE);
#endif
d1043 2
a1044 1
 *	Clear the modify bits on the specified physical page.
d1047 1
a1047 2
pmap_clear_modify(pg)
	struct vm_page *pg;
d1049 3
a1051 3
#ifdef PMAPDEBUG
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
d1054 33
a1086 3
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_clear_modify(%x)\n", pa);
d1088 2
a1089 2
#endif
	return(rv);
d1096 4
a1099 1
	pg->flags &= ~PG_CLEAN;
d1111 4
a1114 3
#ifdef PMAPDEBUG
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif
d1116 4
a1119 6
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_clear_reference(%x)\n", pa);
	}
#endif
	return(FALSE);
d1132 4
a1135 1
	return (FALSE);
d1148 17
a1164 1
	return (FALSE);
d1167 7
a1173 3
paddr_t
pmap_phys_address(ppn)
	int ppn;
d1175 6
d1182 26
a1207 3
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_phys_address(%x)\n", ppn);
a1208 2
#endif
	return (ptoa(ppn));
d1212 3
a1214 1
 * Miscellaneous support routines
d1216 23
d1273 60
d1334 20
a1353 6
	if (pmapdebug & (PDB_FOLLOW|PDB_TLBPID)) {
		if (curproc) {
			printf("pmap_alloc_tlbpid: curproc %d '%s' ",
				curproc->p_pid, curproc->p_comm);
		} else {
			printf("pmap_alloc_tlbpid: curproc <none> ");
d1355 17
a1371 2
		printf("segtab %x tlbpid %d pid %d '%s'\n",
			pmap->pm_segtab, id, p->p_pid, p->p_comm);
a1372 1
#endif
d1374 1
a1374 1
	return (id);
d1378 1
a1378 1
 * Remove a physical to virtual address translation.
d1382 1
a1382 4
pmap_remove_pv(pmap, va, pa)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
d1387 2
a1388 5
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PVENTRY)) {
		printf("pmap_remove_pv(%x, %x, %x)\n", pmap, va, pa);
	}
#endif
d1391 1
a1391 2
	 * Remove page from the PV table (raise IPL since we
	 * may be called at interrupt time).
d1407 1
d1412 1
a1413 1
#ifdef PMAPDEBUG
d1415 1
a1415 3
#endif
	}
	else {
a1416 1
#ifdef PMAPDEBUG
a1417 1
#endif
d1419 1
a1419 1
				goto fnd;
d1421 4
d1426 2
a1427 1
		panic("pmap_remove_pv(%x, %x, %x) not found\n", pmap, va, pa);
d1429 1
a1429 3
	fnd:
		pv->pv_next = npv->pv_next;
		pmap_pv_free(npv);
d1461 9
a1469 3
		pmap_kenter_cache(vaddr, spa,
			VM_PROT_READ | VM_PROT_WRITE,
			cacheable ? PG_IOPAGE : PG_IOPAGE); /* XXX */
@


1.10
log
@Previous commit removed proper caching when growing segmaps by mistake;
spotted by pefo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 2004/09/17 19:19:08 miod Exp $	*/
d41 2
a46 1
#include <machine/vmparam.h>
a48 2
#include <uvm/uvm.h>

a50 1
/* flags for pv_entry */
a67 1
int	pmap_enter_pv(pmap_t, vaddr_t, vm_page_t, u_int *);
d114 2
a115 3
int pmapdebugflag = 0x0;
#define pmapdebug pmapdebugflag
#endif /* PMAPDEBUG */
d280 1
a280 1
	   ((pv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
d389 1
a389 1
					panic("pmap_destroy: segmap not empty");
d673 1
a673 1
		pte += (sva >> PGSHIFT) & (NPTEPG - 1);
d699 1
a699 1
	return (entry & PG_RO);
d737 2
a738 1
		} else {
d740 1
a740 1
				pte += (pv->pv_va >> PGSHIFT) & (NPTEPG - 1);
d801 1
a801 1
		pv_entry_t pv;
a804 6
		/* Set page referenced/modified status based on flags */
		if (flags & VM_PROT_WRITE)
			pv->pv_flags |= PV_ATTR_MOD | PV_ATTR_REF;
		else if (flags & VM_PROT_ALL)
			pv->pv_flags |= PV_ATTR_REF;

d808 1
a808 1
			if ((int64_t)va < 0) {
d814 3
d818 1
a818 1
				if (pv->pv_flags & PV_ATTR_MOD) {
d829 95
d927 1
a927 1
		 * then it must be device memory which may be volatile.
a939 8
		if (pg != NULL) {
			if (pmap_enter_pv(pmap, va, pg, &npte) != 0) {
				if (flags & PMAP_CANFAIL)
					return ENOMEM;
				panic("pmap_enter: pmap_enter_pv() failed");
			}
		}

d945 1
a945 1
		if ((pte->pt_entry & PG_V) && pa != pfn_to_pad(pte->pt_entry)) {
d957 1
a957 1
		return 0;
d975 1
a975 9
	if (pg != NULL) {
		if (pmap_enter_pv(pmap, va, pg, &npte) != 0) {
			if (flags & PMAP_CANFAIL)
				return ENOMEM;
			panic("pmap_enter: pmap_enter_pv() failed");
		}
	}

	pte += (va >> PGSHIFT) & (NPTEPG - 1);
d983 1
d993 1
a993 1
	if ((pte->pt_entry & PG_V) && pa != pfn_to_pad(pte->pt_entry)) {
d1010 1
a1010 3
		if (pg != NULL &&
		    (prot & (VM_PROT_READ | VM_PROT_EXECUTE)) ==
		    (VM_PROT_READ | VM_PROT_EXECUTE))
d1012 1
d1034 1
a1034 1
	if (prot & VM_PROT_WRITE) {
d1036 1
a1036 1
	} else {
a1037 1
	}
d1061 1
a1061 1
		npte |= PG_V | PG_M | cache;
d1063 1
a1063 1
		npte |= PG_V | PG_RO | cache;
d1128 1
a1128 2
		if (va >= (long)KSEG0_BASE &&
		    va < (long)(KSEG0_BASE + KSEG_SIZE)) {
d1133 1
a1133 2
				printf("pmap_extract(%x, %x) -> ", pmap, va);
				panic("pmap_extract");
d1146 1
a1146 1
			pte += (va >> PGSHIFT) & (NPTEPG - 1);
d1155 1
a1155 1
		printf("pmap_extract: rv %d pa %x\n", rv, *pa);
d1171 1
a1171 15
#if 1
	vaddr_t va = *vap;
	long d, m;

	m = CpuCacheAliasMask;
	if (m == 0)		/* m=0 => no cache aliasing */
		return;

	m = (m | (m - 1)) + 1;	/* Value from mask */
	d = foff - va;
	d &= (m - 1);
	*vap = va + d;
#else
	*vap += (*vap ^ foff) & CpuCacheAliasMask;
#endif
a1197 21
#ifndef pmap_update
/*
 *	Require that all active physical maps contain no
 *	incorrect entries NOW.  [This update includes
 *	forcing updates of any address map caching.]
 *
 *	Generally used to insure that a thread about
 *	to run will see a semantically correct world.
 */
void
pmap_update(pmap)
	pmap_t pmap;
{
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW) {
		printf("pmap_update()\n");
	}
#endif
}
#endif

d1298 4
a1301 2
	pv_entry_t pv;
	boolean_t rv;
a1304 1
		paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1308 1
a1308 8

	/*
	 * XXX - we should walk the pv list, and flush any dirty pages too.
	 */
	pv = pg_to_pvh(pg);
	rv = (pv->pv_flags & PV_ATTR_MOD) != 0;
	pv->pv_flags &= ~PV_ATTR_MOD;
	return rv;
d1315 1
a1315 4
	pv_entry_t pv;

	pv = pg_to_pvh(pg);
	pv->pv_flags |= PV_ATTR_MOD | PV_ATTR_REF;
d1327 3
a1329 2
	pv_entry_t pv;
	boolean_t rv;
a1332 1
		paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1336 1
a1336 5

	pv = pg_to_pvh(pg);
	rv = (pv->pv_flags & PV_ATTR_REF) != 0;
	pv->pv_flags &= ~PV_ATTR_REF;
	return rv;
d1349 1
a1349 4
	pv_entry_t pv;

	pv = pg_to_pvh(pg);
	return (pv->pv_flags & PV_ATTR_REF) != 0;
d1362 1
a1362 4
	pv_entry_t pv;

	pv = pg_to_pvh(pg);
	return (pv->pv_flags & PV_ATTR_MOD) != 0;
d1411 1
a1411 2
	}
	else {
a1431 105
 * Enter the pmap and virtual address into the
 * physical to virtual map table.
 */
int
pmap_enter_pv(pmap, va, pg, npte)
	pmap_t pmap;
	vaddr_t va;
	vm_page_t pg;
	u_int *npte;
{
	pv_entry_t pv, npv;
#ifdef PMAPDEBUG
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#endif

	pv = pg_to_pvh(pg);

	if (pv->pv_pmap == NULL) {
		/*
		 * No entries yet, use header as the first entry
		 */

#ifdef PMAPDEBUG
		if (pmapdebug & PDB_PVENTRY) {
			printf("pmap_enter: first pv: pmap %x va %x pa %p\n",
				pmap, va, pa);
		}
		stat_count(enter_stats.firstpv);
#endif

		Mips_SyncDCachePage(pv->pv_va);

		pv->pv_va = va;
		pv->pv_flags = PV_CACHED;
		pv->pv_pmap = pmap;
		pv->pv_next = NULL;
	} else {
		if (pv->pv_flags & PV_UNCACHED) {
			*npte = (*npte & ~PG_CACHEMODE) | PG_UNCACHED;
		} else if (CpuCacheAliasMask != 0) {
			/*
			 * There is at least one other VA mapping this page.
			 * Check if they are cache index compatible. If not
			 * remove all mappings, flush the cache and set page
			 * to be mapped uncached. Caching will be restored
			 * when pages are mapped compatible again. NOT!
			 */
			for (npv = pv; npv; npv = npv->pv_next) {
				/*
				 * Check cache aliasing incompatibility
				 */
				if (((npv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
#ifdef PMAPDEBUG
					printf("pmap_enter: uncached mapping for pa %p, va %p !=  %p.\n",
					    pa, npv->pv_va, va);
#endif
					pmap_page_cache(pg, PV_UNCACHED);
					Mips_SyncCache();
					pv->pv_flags &= ~PV_CACHED;
					*npte = (*npte & ~PG_CACHEMODE) | PG_UNCACHED;
					break;
				}
			}
		}

		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 *
		 * Note: the entry may already be in the table if
		 * we are only changing the protection bits.
		 */
		for (npv = pv; npv; npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va) {
				return 0;
			}
		}

#ifdef PMAPDEBUG
		if (pmapdebug & PDB_PVENTRY) {
			printf("pmap_enter: new pv: pmap %x va %x pa %p\n",
			    pmap, va, pa);
		}
#endif

		/* can this cause us to recurse forever? */
		npv = pmap_pv_alloc();
		if (npv == NULL)
			return ENOMEM;
		npv->pv_va = va;
		npv->pv_pmap = pmap;
		npv->pv_next = pv->pv_next;
		npv->pv_flags = pv->pv_flags;
		pv->pv_next = npv;

#ifdef PMAPDEBUG
		if (!npv->pv_next)
			stat_count(enter_stats.secondpv);
#endif
	}

	return 0;
}

/*
a1467 2
			npv->pv_flags |=
			    (pv->pv_flags & (PV_ATTR_MOD | PV_ATTR_REF));
d1476 2
a1477 1
	} else {
d1483 1
a1483 1
				break;
a1484 4
		if (npv != NULL) {
			pv->pv_next = npv->pv_next;
			pmap_pv_free(npv);
		} else {
d1486 1
a1486 2
			panic("pmap_remove_pv(%x, %x, %x) not found",
			    pmap, va, pa);
d1488 3
a1490 1
		}
@


1.9
log
@Long-due mips pmap potpourri:
- un-__P()ee.
- remove splimp() protection.
- allow pmap_extract() to return FALSE, dammit!
  XXX some code under arch/mips64 considers pa == 0 as failure, instead
      of checking the return value of pmap_extract(). Free commits.
- do not peek at uvm vm_page flags for ref/mod accounting, but use real
  MD flags in pv lists, and implement pmap_is_* and pmap_clear_*, so uvm
  (which knows better) does the right thing.
- debug code #ifdef PMAPDEBUG, not #ifdef DIAGNOSTIC.
- merge pmap_init() and pmap_pinit(), pmap_destroy() and pmap_release(),
  and update comments.
- better pmap_enter() logic, from NetBSD.
- do not use IS_VM_PHYSADDR() to know if we can use PHYS_TO_VM_PAGE()
  unchecked. Instead, compute PHYS_TO_VM_PAGE(), and check it against
  NULL. Saves a vm_physseg_find() call each time.
- pass vm_page_t to internal function whenever possible, rather than
  paddr_t, so that PHYS_TO_VM_PAGE() needs not be computed again.
- in the same way, do not compute PHYS_TO_KSEG0(foo) several times in a
  row (though PHYS_TO_KSEG0 is trivial).
- allow userspace address to cross 0x80000000 (may be useful in the
  future).
- implement PMAP_CANFAIL.

As a bonus:
- switch from HAVE_PMAP_PHYSSEG to HAVE_VM_PAGE_MD.
- remove dead (pre-pmap_pv_pool) code.
- KNF fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 2004/09/16 07:25:26 miod Exp $	*/
d71 1
d270 24
a321 2
		vm_page_t pg;
		pv_entry_t pv;
d324 3
a326 14
		do {
			pg = uvm_pagealloc(NULL, 0, NULL,
			    UVM_PGA_USERESERVE | UVM_PGA_ZERO);
			if (pg == NULL) {
				/* XXX What else can we do?  Deadlocks?  */
				uvm_wait("pmap_create");
			}
		} while (pg == NULL);

		pv = pg_to_pvh(pg);
		va = PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(pg));
		if ((pv->pv_flags & PV_CACHED) &&
		   ((pv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
			Mips_SyncDCachePage(pv->pv_va);
a327 2
		pv->pv_va = va;
		pv->pv_flags = PV_CACHED;
d883 1
a883 1
		vm_page_t npg;	/* do not clobber pg! */
d885 5
a889 9
		do {
			npg = uvm_pagealloc(NULL, 0, NULL,
			    UVM_PGA_USERESERVE | UVM_PGA_ZERO);
			if (npg == NULL) {
				if (flags & PMAP_CANFAIL)
					return ENOMEM;
				uvm_wait("pmap_enter");
			}
		} while (npg == NULL);
d891 1
a891 2
		pmap_segmap(pmap, va) = pte = (pt_entry_t *)
		    (vaddr_t)PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(npg));
@


1.8
log
@``viritual'' is a virtual word and this is a real tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 2004/09/09 22:11:38 pefo Exp $	*/
a40 2
#include <uvm/uvm.h>

d45 1
d48 3
a50 1
extern void mem_zero_page (vaddr_t);
d52 1
a52 6
typedef struct pv_entry {
	struct pv_entry	*pv_next;	/* next pv_entry */
	struct pmap	*pv_pmap;	/* pmap where mapping lies */
	vaddr_t		pv_va;		/* virtual address for mapping */
	int		pv_flags;	/* Some flags for the mapping */
} *pv_entry_t;
d55 2
a56 7

/*
 * Local pte bits used only here
 */
#define PG_RO		0x40000000

pv_entry_t pv_table;			/* array of entries, one per page */
a60 3
void    *pmap_pv_page_alloc(u_long, int, int);
void    pmap_pv_page_free(void *, u_long, int);

d69 4
a72 4
void pmap_pinit __P((struct pmap *pmap));
void pmap_release __P((pmap_t pmap));
boolean_t pmap_physpage_alloc(paddr_t *);
void    pmap_physpage_free(paddr_t);
d74 1
a74 1
#ifdef DIAGNOSTIC
d118 1
a118 7

#else

#define stat_count(what)
#define pmapdebug (0)

#endif /* DIAGNOSTIC */
a142 1
	int n;
a160 13
	/*
	 * Allocate memory for pv_table.
	 * This will allocate more entries than we really need.
	 * We could do this in pmap_init when we know the actual
	 * phys_start and phys_end but its better to use kseg0 addresses
	 * rather than kernel virtual addresses mapped through the TLB.
	 */
	i = 0;
	for( n = 0; n < MAXMEMSEGS; n++) {
		i += mem_layout[n].mem_last_page - mem_layout[n].mem_first_page + 1;
	}
	pv_table = (struct pv_entry *)uvm_pageboot_alloc(sizeof(struct pv_entry) * i);

a163 2
	/* XXX need to decide how to set cnt.v_page_size */

d191 1
d195 1
a249 3
	vsize_t s;
	int bank;
	pv_entry_t pv;
d251 1
d255 1
a255 7

	pv = pv_table;
	for(bank = 0; bank < vm_nphysseg; bank++) {
		s = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		pv += s;
	}
d262 3
a264 4
inline struct pv_entry *pa_to_pvh __P((paddr_t));
inline struct pv_entry *
pa_to_pvh(pa)
    paddr_t pa;
d266 1
a266 4
	int i, p;

	i = vm_physseg_find(atop((pa)), &p);
	return(&vm_physmem[i].pmseg.pvent[p]);
a270 9
 *
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
 *
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
d276 3
d280 1
d284 1
a287 16
	pmap_pinit(pmap);
	return (pmap);
}

/*
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
 */
void
pmap_pinit(pmap)
	struct pmap *pmap;
{
	int i;
	int s;
extern struct vmspace vmspace0;
extern struct user *proc0paddr;
a291 1
		s = splimp();
d295 1
a295 3
		splx(s);
	}
	else {
d297 1
a297 1
		vm_page_t mem;
d299 1
d302 3
a304 2
			mem = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE|UVM_PGA_ZERO);
			if (mem == NULL) {
d306 1
a306 1
				uvm_wait("ppinit");
d308 1
a308 1
		} while (mem == NULL);
d310 4
a313 3
		pv = pa_to_pvh(VM_PAGE_TO_PHYS(mem));
		if (pv->pv_flags & PV_CACHED &&
		   ((pv->pv_va ^ PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem))) & CpuCacheAliasMask) != 0) {
d316 1
a316 1
		pv->pv_va = PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem));
d319 1
a319 2
		pmap->pm_segtab = stp = (struct segtab *)
			(long)PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem));
a321 1
		s = splimp();
a326 1
		splx(s);
d341 2
d356 1
d360 1
a360 10
	if (pmap) {
		simple_lock(&pmap->pm_lock);
		count = --pmap->pm_count;
		simple_unlock(&pmap->pm_lock);
		if (count == 0) {
			pmap_release(pmap);
			pool_put(&pmap_pmap_pool, pmap);
		}
	}
}
d362 5
a366 13
/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_release(pmap)
	pmap_t pmap;
{

	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE)) {
		printf("pmap_release(%x)\n", pmap);
	}
d371 1
a371 2
		int s;
#ifdef PARANIOA
d383 1
a383 1
					panic("pmap_release: segmap not empty");
a389 1
		s = splimp();
a391 1
		splx(s);
d394 2
d406 1
d410 1
d457 1
a457 2
	stat_count(remove_stats.calls);

d461 2
d487 1
d489 1
d529 1
d531 1
a546 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a548 1
	int s, i;
d550 1
d553 1
a553 1
		printf("pmap_page_protect(%x, %x)\n", pa, prot);
d555 2
a556 1
	if (!IS_VM_PHYSADDR(pa)) {
a557 1
	}
d567 1
a567 2
		pv = pa_to_pvh(pa);
		s = splimp();
d574 2
a575 7

				/*
				 * XXX don't write protect pager mappings
				 */
				if (va >= uvm.pager_sva && va < uvm.pager_eva)
					continue;
				pmap_protect(pv->pv_pmap, va, va + PAGE_SIZE, prot);
a577 1
		splx(s);
d582 4
a585 7
		i = 0;
		pv = pa_to_pvh(pa);
		s = splimp();
		while (pv->pv_pmap != NULL && i < 10) {
			i++;
			pmap_remove(pv->pv_pmap, pv->pv_va,
				    pv->pv_va + PAGE_SIZE);
a586 1
		splx(s);
d605 1
d609 1
d620 1
a620 1
	if (!pmap->pm_segtab) {
d693 1
a693 1
	return(entry & PG_RO);
d702 2
a703 2
pmap_page_cache(pa,mode)
	vaddr_t pa;
a709 1
	int s;
d711 1
d715 1
a715 4
	if (!IS_VM_PHYSADDR(pa)) {
		return;
	}

d717 1
a717 2
	pv = pa_to_pvh(pa);
	s = splimp();
d720 4
a723 4
		if (!pv->pv_pmap->pm_segtab) {
		/*
		 * Change entries in kernel pmap.
		 */
d731 1
a731 2
		}
		else {
a745 2

	splx(s);
d758 1
a758 1
pmap_enter(pmap, va, pa, prot, stat)
d763 1
a763 1
	int stat;
d767 1
a767 2
	vm_page_t mem;
	int is_physaddr;
d769 1
d774 1
d777 1
d779 1
d783 1
d785 1
d791 6
a796 1
	is_physaddr = IS_VM_PHYSADDR(pa);
d798 5
a802 3
	if (is_physaddr) {
		pv_entry_t pv, npv;
		int s;
d807 1
a807 4
			vm_page_t mem;

			mem = PHYS_TO_VM_PAGE(pa);
			if ((int)va < 0) {
a812 3
#if 0 /*XXX*/
				mem->flags &= ~PG_CLEAN;
#endif
d814 1
a814 1
				if (!(mem->flags & PG_CLEAN)) {
d822 1
d824 2
a825 92
		/*
		 * Enter the pmap and virtual address into the
		 * physical to virtual map table.
		 */
		pv = pa_to_pvh(pa);
		s = splimp();

		if (pmapdebug & PDB_ENTER) {
			printf("pmap_enter: pv %x: was %x/%x/%x\n",
			       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
		}

		if (pv->pv_pmap == NULL) {
			/*
			 * No entries yet, use header as the first entry
			 */

			if (pmapdebug & PDB_PVENTRY) {
				printf("pmap_enter: first pv: pmap %x va %x pa %p\n",
					pmap, va, pa);
			}
			stat_count(enter_stats.firstpv);

			Mips_SyncDCachePage(pv->pv_va);

			pv->pv_va = va;
			pv->pv_flags = PV_CACHED;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
		} else {
			if (pv->pv_flags & PV_UNCACHED) {
				npte = (npte & ~PG_CACHEMODE) | PG_UNCACHED;
			} else if (CpuCacheAliasMask != 0) {
			/*
			 * There is at least one other VA mapping this page.
			 * Check if they are cache index compatible. If not
			 * remove all mappings, flush the cache and set page
			 * to be mapped uncached. Caching will be restored
			 * when pages are mapped compatible again. NOT!
			 */
				for (npv = pv; npv; npv = npv->pv_next) {
					/*
					 * Check cache aliasing incompatibility
					 */
					if (((npv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
						printf("pmap_enter: uncached mapping for pa %p, va %p !=  %p.\n", pa, npv->pv_va, va);
						pmap_page_cache(pa,PV_UNCACHED);
						Mips_SyncCache();
						pv->pv_flags &= ~PV_CACHED;
						npte = (npte & ~PG_CACHEMODE) | PG_UNCACHED;
						break;
					}
				}
			}

			/*
			 * There is at least one other VA mapping this page.
			 * Place this entry after the header.
			 *
			 * Note: the entry may already be in the table if
			 * we are only changing the protection bits.
			 */
			for (npv = pv; npv; npv = npv->pv_next) {
				if (pmap == npv->pv_pmap && va == npv->pv_va) {
					goto fnd;
				}
			}

			if (pmapdebug & PDB_PVENTRY) {
				printf("pmap_enter: new pv: pmap %x va %x pa %p\n",
					pmap, va, pa);
			}

			/* can this cause us to recurse forever? */
			npv = pmap_pv_alloc();
			if (npv == NULL) {
				panic("pmap_pv_alloc() failed");
			}
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_flags = pv->pv_flags;
			pv->pv_next = npv;

			if (!npv->pv_next)
				stat_count(enter_stats.secondpv);
		fnd:
			;
		}
		splx(s);
	}
	else {
d828 1
a828 1
		 * then it must be device memory which may be volitile.
d830 1
d832 1
d841 8
d854 1
a854 1
		if (pa != pfn_to_pad(pte->pt_entry)) {
d856 1
d858 1
d866 1
a866 1
		return (KERN_SUCCESS);
d873 2
a874 1
		pv_entry_t pv;
d876 6
a881 4
			mem = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE|UVM_PGA_ZERO);
			if (mem == NULL) {
				/* XXX What else can we do?  Deadlocks?  */
				uvm_wait("penter");
d883 1
a883 1
		} while (mem == NULL);
d885 9
a893 4
		pv = pa_to_pvh(VM_PAGE_TO_PHYS(mem));
		if (pv->pv_flags & PV_CACHED &&
		   ((pv->pv_va ^ PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem))) & CpuCacheAliasMask) != 0) {
			Mips_SyncDCachePage(pv->pv_va);
d895 1
a895 2
		pv->pv_va = PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem));
		pv->pv_flags = PV_CACHED;
a896 3
		pmap_segmap(pmap, va) = pte = (pt_entry_t *)
			(long)PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem));
	}
d904 2
a905 6
	if (is_physaddr) {
		npte |= vad_to_pfn(pa);
	}
	else {
		npte |= vad_to_pfn(pa);
	}
d912 1
d914 1
a914 1
	if (pa != pfn_to_pad(pte->pt_entry)) {
d916 1
d918 1
d926 1
a926 3
		int s, i;
		s = splimp();
		i = tlb_update(va | (pmap->pm_tlbpid << VMTLB_PID_SHIFT), npte);
d931 3
a933 1
		if (is_physaddr) {
a934 2
		}
		splx(s);
d937 1
a937 1
	return (KERN_SUCCESS);
d949 1
d953 1
a953 1

d958 1
a958 2
	}
	else {
d976 1
d980 1
a980 1

d984 3
a986 4
		npte |= PG_M | cache;
	}
	else {
		npte |= PG_RO | cache;
d1002 1
d1006 1
d1026 1
d1041 4
d1048 1
d1050 3
a1052 2
	if (!pmap->pm_segtab) {
		if (va >= (long)KSEG0_BASE && va < (long)(KSEG0_BASE + KSEG_SIZE)) {
d1054 1
a1054 2
		}
		else {
d1061 5
a1065 8
			*pa = pfn_to_pad(kvtopte(va)->pt_entry);
		}
	}
	else {
		pt_entry_t *pte;

		if (!(pte = pmap_segmap(pmap, va))) {
			*pa = 0;
d1067 3
d1075 1
a1075 1
	if (*pa)
d1078 1
d1080 1
a1080 1
		printf("pmap_extract: pa %x\n", *pa);
d1082 1
d1084 1
a1084 1
	return (TRUE);
d1129 1
d1134 1
d1150 1
d1154 1
d1165 1
a1165 1
	vaddr_t p;
d1168 1
d1172 1
d1174 4
a1177 4
	p = (vaddr_t)PHYS_TO_KSEG0(phys);
	pv = pa_to_pvh(phys);
	if (pv->pv_flags & PV_CACHED &&
		   ((pv->pv_va ^ (int)p) & CpuCacheAliasMask) != 0) {
d1180 2
a1181 2
	mem_zero_page(p);
	Mips_HitSyncDCache(p, PAGE_SIZE);
d1211 1
d1215 3
a1217 2
	s = (int *)(long)PHYS_TO_KSEG0(src);
	d = (int *)(long)PHYS_TO_KSEG0(dst);
d1219 2
a1220 2
	pv = pa_to_pvh(src);
	if (pv->pv_flags & PV_CACHED &&
d1224 2
a1225 2
	pv = pa_to_pvh(dst);
	if (pv->pv_flags & PV_CACHED &&
d1258 2
a1259 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t rv = FALSE;
d1261 1
d1263 1
d1266 19
a1284 1
	return(rv);
d1296 2
a1297 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1299 1
d1301 1
d1304 6
a1309 1
	return(FALSE);
d1322 4
a1325 1
	return (FALSE);
d1338 4
a1341 1
	return (FALSE);
d1349 1
d1353 1
d1395 1
d1400 1
a1400 2
		}
		else {
d1406 1
d1412 105
d1527 1
a1527 1
	int s;
d1529 1
d1533 1
d1539 2
a1540 1
	if (!IS_VM_PHYSADDR(pa)) {
a1541 1
	}
d1543 1
a1543 2
	pv = pa_to_pvh(pa);
	s = splimp();			/* XXX not in nbsd */
d1553 2
d1557 1
a1557 2
		}
		else {
d1560 1
d1562 2
a1563 2
	}
	else {
d1565 1
d1567 1
d1569 1
a1569 1
				goto fnd;
d1571 4
d1576 2
a1577 2
		printf("pmap_remove_pv(%x, %x, %x) not found\n", pmap, va, pa);
		panic("pmap_remove_pv");
d1579 1
a1579 3
	fnd:
		pv->pv_next = npv->pv_next;
		pmap_pv_free(npv);
a1580 65
	splx(s);
	return;
}

/*
 * pmap_pv_page_alloc:
 *
 *      Allocate a page for the pv_entry pool.
 */
void *
pmap_pv_page_alloc(u_long size, int flags, int mtype)
{
	paddr_t pg;

	if (pmap_physpage_alloc(&pg))
		return ((void *)(long)PHYS_TO_KSEG0(pg));
	return (NULL);
}

/*
 * pmap_pv_page_free:
 *
 *      Free a pv_entry pool page.
 */
void
pmap_pv_page_free(void *v, u_long size, int mtype)
{
	pmap_physpage_free(KSEG0_TO_PHYS((vaddr_t)v));
}

/*
 * pmap_physpage_alloc:
 *
 *	Allocate a single page from the VM system and return the
 *	physical address for that page.
 */
boolean_t
pmap_physpage_alloc(paddr_t *pap)
{
	struct vm_page *pg;
	paddr_t pa;

	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (pg != NULL) {
		pa = VM_PAGE_TO_PHYS(pg);
		*pap = pa;
		return (TRUE);
	}
	return (FALSE);
}

/*
 * pmap_physpage_free:
 *
 *	Free a single pmap metadate page at the specified physical address.
 */
void
pmap_physpage_free(paddr_t pa)
{
	struct vm_page *pg;

	if ((pg = PHYS_TO_VM_PAGE(pa)) == NULL) {
		panic("pmap_physpage_free: bogus physical page address");
	}
	uvm_pagefree(pg);
d1600 6
a1605 5
	if (phys_map == NULL) {
		printf("ouch, add mapping when phys map not ready!\n");
	} else {
		vaddr = uvm_km_valloc_wait(kernel_map, len);
	}
a1618 1

@


1.7
log
@Kernel moves to 64 bit. A few more tweaks when binutils is updated.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 2004/08/15 18:35:12 pefo Exp $	*/
d1334 1
a1334 1
 *	a viritual coherence problem. If so flush the cache for the
@


1.6
log
@cast
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 2004/08/11 10:21:08 deraadt Exp $	*/
d1365 1
a1365 1
		   (sf = ((pv->pv_va ^ (int)s) & CpuCacheAliasMask) != 0)) {
d1370 1
a1370 1
		   (df = ((pv->pv_va ^ (int)d) & CpuCacheAliasMask) != 0)) {
@


1.5
log
@spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 2004/08/10 20:28:13 deraadt Exp $	*/
d384 1
a384 1
			PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem));
d1054 1
a1054 1
			PHYS_TO_KSEG0(VM_PAGE_TO_PHYS(mem));
d1205 2
a1206 2
		if (va >= KSEG0_BASE && va < (KSEG0_BASE + KSEG_SIZE)) {
			*pa = KSEG0_TO_PHYS(va);
d1360 2
a1361 2
	s = (int *)PHYS_TO_KSEG0(src);
	d = (int *)PHYS_TO_KSEG0(dst);
d1591 1
a1591 1
		return ((void *) PHYS_TO_KSEG0(pg));
@


1.4
log
@spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 2004/08/10 20:15:47 deraadt Exp $	*/
d73 2
a74 2
#define pmap_pv_alloc()         (pv_entry_t)pool_get(&pmap_pv_pool, PR_NOWAIT)
#define pmap_pv_free(pv)        pool_put(&pmap_pv_pool, (pv))
d217 2
a218 2
        vsize_t size;
        vaddr_t *vstartp, *vendp;
d235 1
a235 1
                    vm_physmem[i].avail_start >= vm_physmem[i].avail_end) {
d503 1
a503 1
        struct proc *p;
d505 1
a505 1
        pmap_t pmap = p->p_vmspace->vm_map.pmap;
d517 1
a517 1
        struct proc *p;
d519 1
a519 1
        /* Empty */
d776 1
a776 1
	int         entry;
d1104 3
a1106 3
        vaddr_t va;
        paddr_t pa;
        vm_prot_t prot;
d1108 2
a1109 2
        pt_entry_t *pte;
        u_int npte;
d1111 2
a1112 2
        if (pmapdebug & (PDB_FOLLOW|PDB_ENTER)) {
                printf("pmap_kenter_pa(%lx, %lx, %x)\n", va, pa, prot);
d1116 4
a1119 4
        npte = vad_to_pfn(pa) | PG_G;
        if (prot & VM_PROT_WRITE) {
                npte |= PG_RWPAGE;
        }
d1121 5
a1125 5
                npte |= PG_ROPAGE;
        }
        pte = kvtopte(va);
        pte->pt_entry = npte;
        tlb_update(va, npte);
d1130 3
a1132 3
        vaddr_t va;
        paddr_t pa;
        vm_prot_t prot;
d1135 2
a1136 2
        pt_entry_t *pte;
        u_int npte;
d1138 2
a1139 2
        if (pmapdebug & (PDB_FOLLOW|PDB_ENTER)) {
                printf("pmap_kenter_cache(%lx, %lx, %x)\n", va, pa, prot);
d1143 4
a1146 4
        npte = vad_to_pfn(pa) | PG_G;
        if (prot & VM_PROT_WRITE) {
                npte |= PG_M | cache;
        }
d1148 5
a1152 5
                npte |= PG_RO | cache;
        }
        pte = kvtopte(va);
        pte->pt_entry = npte;
        tlb_update(va, npte);
d1157 2
a1158 2
        vaddr_t va;
        vsize_t len;
d1160 15
a1174 15
        pt_entry_t *pte;
        vaddr_t eva;
        u_int entry;

        if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE)) {
                printf("pmap_kremove(%lx, %lx)\n", va, len);
	}

        pte = kvtopte(va);
        eva = va + len;
        for (; va < eva; va += PAGE_SIZE, pte++) {
                entry = pte->pt_entry;
                if (entry & PG_V) {
                        continue;
                }
d1178 1
a1178 1
        }
d1183 2
a1184 2
        pmap_t pmap;
        vaddr_t va;
d1253 1
a1253 1
	if (m == 0)             /* m=0 => no cache aliasing */
d1588 1
a1588 1
        paddr_t pg;
d1590 3
a1592 3
        if (pmap_physpage_alloc(&pg))
                return ((void *) PHYS_TO_KSEG0(pg));
        return (NULL);
d1603 1
a1603 1
        pmap_physpage_free(KSEG0_TO_PHYS((vaddr_t)v));
d1648 1
a1648 1
                        bus_space_handle_t *bshp)
d1650 16
a1665 16
        bus_addr_t vaddr;
        bus_addr_t spa, epa;
        bus_size_t off;
        int len;

        spa = trunc_page(bpa);
        epa = bpa + size;
        off = bpa - spa;
        len = size+off;

        if (phys_map == NULL) {
                printf("ouch, add mapping when phys map not ready!\n");
        } else {
                vaddr = uvm_km_valloc_wait(kernel_map, len);
        }
        *bshp = vaddr + off;
d1667 1
a1667 1
        printf("map bus %x size %x to %x vbase %x\n", bpa, size, *bshp, spa);
d1669 8
a1676 8
        for (; len > 0; len -= NBPG) {
                pmap_kenter_cache(vaddr, spa,
                        VM_PROT_READ | VM_PROT_WRITE,
                        cacheable ? PG_IOPAGE : PG_IOPAGE); /* XXX */
                spa += NBPG;
                vaddr += NBPG;
        }
        return 0;
@


1.3
log
@spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 2004/08/09 14:57:26 pefo Exp $	*/
d234 1
a234 1
		if(vm_physmem[i].avail_start != vm_physmem[i].start ||
d239 1
a239 1
		if((vm_physmem[i].avail_end - vm_physmem[i].avail_start) < npgs) {
d247 2
a248 2
		if(vm_physmem[i].avail_start == vm_physmem[i].end) {
			if(vm_nphysseg == 1) {
d257 1
a257 1
		if(vstartp) {
d260 1
a260 1
		if(vendp) {
d267 1
a267 1
	if(va == 0) {
d376 1
a376 1
		if(pv->pv_flags & PV_CACHED &&
d539 1
a539 1
	if(pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT)) {
d542 1
a542 1
	if(pmap == NULL) {
d546 1
a546 1
	if(pmap == pmap_kernel()) {
d551 1
a551 1
		if(sva < VM_MIN_KERNEL_ADDRESS || eva > virtual_end)
d557 1
a557 1
			if(!(entry & PG_V))
d596 1
a596 1
			if(!pfn_is_ext(entry)) {/* padr > 32 bits */
d880 1
a880 1
	if(is_physaddr) {
d953 1
a953 1
					if(((npv->pv_va ^ va) & CpuCacheAliasMask) != 0) {
d984 1
a984 1
			if(npv == NULL) {
d1019 1
a1019 1
		if(pa != pfn_to_pad(pte->pt_entry)) {
d1046 1
a1046 1
		if(pv->pv_flags & PV_CACHED &&
d1076 1
a1076 1
	if(pa != pfn_to_pad(pte->pt_entry)) {
d1322 1
a1322 1
	if(pv->pv_flags & PV_CACHED &&
d1364 1
a1364 1
	if(pv->pv_flags & PV_CACHED &&
d1369 1
a1369 1
	if(pv->pv_flags & PV_CACHED &&
d1382 1
a1382 1
	if(sf) {
d1386 1
a1386 1
	if(df || (pv->pv_pmap == NULL) || (pv->pv_flags & PV_EXEC)) {
d1590 1
a1590 1
        if(pmap_physpage_alloc(&pg))
@


1.2
log
@Big cleanup. Removed some unused obsolete stuff and fixed copyrights
on some files. Arcbios support is now in, thus detects memorysize and cpu
clock frequency.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.1 2004/08/06 20:56:03 pefo Exp $	*/
d5 1
a5 1
 * 
d203 1
a203 1
	 * translation lookaside buffer for each 2 page entry. 
d234 1
a234 1
		if(vm_physmem[i].avail_start != vm_physmem[i].start || 
d253 1
a253 1
			for(j = i; j < vm_nphysseg; x++) { 
d295 1
a295 1
	} 
d621 1
a621 1
{ 
d1069 1
a1069 1
	if (pmapdebug & PDB_ENTER) { 
@


1.1
log
@initial mips64
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d4 1
a4 1
 * Copyright (c) 2001-2003 Opsycon AB  (www.opsycon.se / www.opsycon.com)
a13 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Opsycon AB, Sweden.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
d189 1
a189 1
		i += mem_layout[n].mem_size;
d1489 1
a1489 1
			tlb_flush(sys_config.cpu.tlbsize);
@

