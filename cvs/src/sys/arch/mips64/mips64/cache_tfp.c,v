head	1.7;
access;
symbols
	OPENBSD_6_2_BASE:1.7
	OPENBSD_6_1:1.6.0.8
	OPENBSD_6_1_BASE:1.6
	OPENBSD_6_0:1.6.0.4
	OPENBSD_6_0_BASE:1.6
	OPENBSD_5_9:1.6.0.2
	OPENBSD_5_9_BASE:1.6
	OPENBSD_5_8:1.4.0.8
	OPENBSD_5_8_BASE:1.4
	OPENBSD_5_7:1.4.0.2
	OPENBSD_5_7_BASE:1.4
	OPENBSD_5_6:1.4.0.4
	OPENBSD_5_6_BASE:1.4
	OPENBSD_5_5:1.2.0.6
	OPENBSD_5_5_BASE:1.2
	OPENBSD_5_4:1.2.0.2
	OPENBSD_5_4_BASE:1.2
	OPENBSD_5_3:1.1.0.2
	OPENBSD_5_3_BASE:1.1;
locks; strict;
comment	@ * @;


1.7
date	2017.06.22.14.40.20;	author visa;	state Exp;
branches;
next	1.6;
commitid	F1hjg9bferpqzaaR;

1.6
date	2016.01.05.05.27.54;	author visa;	state Exp;
branches;
next	1.5;
commitid	916i76I5mjNtTg33;

1.5
date	2015.09.20.11.50.05;	author miod;	state Exp;
branches;
next	1.4;
commitid	N7YVdHrcSakW71nw;

1.4
date	2014.03.31.20.21.19;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2014.03.09.10.12.17;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2013.06.28.18.56.10;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2012.09.29.21.37.03;	author miod;	state Exp;
branches;
next	;


desc
@@


1.7
log
@Improve the coalescing of instruction cache flushes on R8000 in order
not to perform redundant work. There is no direct way to flush the
cache; rather, the kernel evicts unwanted bits from the cache by
executing a block of instructions, which is expensive.

With this diff, make build time decreases about 0.4%.

Diff from miod@@
@
text
@/*	$OpenBSD: cache_tfp.c,v 1.6 2016/01/05 05:27:54 visa Exp $	*/

/*
 * Copyright (c) 2012 Miodrag Vallat.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * R8000 cache routines.
 *
 * These routines only handle the L1 cache found onboard the R8000.
 * The L2 (Streaming Cache) cache handling is apparently quite different
 * accross R8000-based designs (well... the two of them: IP21 and IP26),
 * and is handled on a per-platform basis.
 */

#include <sys/param.h>
#include <sys/systm.h>

#include <mips64/cache.h>
#include <machine/cpu.h>

#include <uvm/uvm_extern.h>

void	tfp_dctw_zero(vaddr_t);
void	tfp_inval_icache(vsize_t);

#define	TFP_DCTW_STEP	16UL			/* 4 words per tag */

void
tfp_ConfigCache(struct cpu_info *ci)
{
	register_t cfg;

	cfg = cp0_get_config();

	/*
	 * XXX It would make sense to trust the configuration register,
	 * XXX but at least on my system it would report a 32KB I$, while
	 * XXX ARCS reports the architected 16KB.
	 * XXX Better not trust anything from the configuration register,
	 * XXX then.
	 */
#if 0
	ci->ci_l1inst.size = (1 << 11) << ((cfg >> 9) & 0x07); /* IC */
	if (cfg & (1 << 5))	/* IB */
		ci->ci_l1inst.linesize = 32;
	else
		ci->ci_l1inst.linesize = 16;

	ci->ci_l1data.size = (1 << 12) << ((cfg >> 6) & 0x07); /* DC */
	if (cfg & (1 << 4))	/* DB */
		ci->ci_l1data.linesize = 32;
	else
		ci->ci_l1data.linesize = 16;
#else
	ci->ci_l1inst.size = 16384;
	ci->ci_l1inst.linesize = TFP_DCTW_STEP;	/* but handled in 32b chunks */
	ci->ci_l1data.size = 16384;
	ci->ci_l1data.linesize = TFP_DCTW_STEP;
#endif

	/* R8000 L1 caches are direct */
	ci->ci_l1inst.setsize = ci->ci_l1inst.size;
	ci->ci_l1inst.sets = 1;
	ci->ci_l1data.setsize = ci->ci_l1data.size;
	ci->ci_l1data.sets = 1;

	cache_valias_mask =
	    (max(ci->ci_l1inst.size, ci->ci_l1data.size) - 1) &
	    ~PAGE_MASK;
#ifdef DIAGNOSTIC
	if (cache_valias_mask != 0)
		panic("page size is too small");
#endif

	/* R8000 L2 cache are platform-specific, and not covered here */
	memset(&ci->ci_l2, 0, sizeof(struct cache_info));
	memset(&ci->ci_l3, 0, sizeof(struct cache_info));

	ci->ci_SyncCache = tfp_SyncCache;
	ci->ci_InvalidateICache = tfp_InvalidateICache;
	ci->ci_InvalidateICachePage = tfp_InvalidateICachePage;
	ci->ci_SyncICache = tfp_SyncICache;
	ci->ci_SyncDCachePage = tfp_SyncDCachePage;
	ci->ci_HitSyncDCachePage = tfp_SyncDCachePage;
	ci->ci_HitSyncDCache = tfp_HitSyncDCache;
	ci->ci_HitInvalidateDCache = tfp_HitInvalidateDCache;
	ci->ci_IOSyncDCache = tfp_IOSyncDCache;
}

/*
 * Writeback and invalidate all caches.
 */
void
tfp_SyncCache(struct cpu_info *ci)
{
	vaddr_t va, eva;
	register_t sr;

	tfp_InvalidateICache(ci, 0, ci->ci_l1inst.size);

	sr = disableintr();
	eva = ci->ci_l1data.size;
	for (va = 0; va < eva; va += TFP_DCTW_STEP)
		tfp_dctw_zero(va);
	setsr(sr);
}

/*
 * Invalidate I$ for the given range.
 */
void
tfp_InvalidateICache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va;
	vsize_t sz;
	void (*inval_subr)(vsize_t);

	if (ci->ci_cachepending_l1i != 0)
		return;

	if (_sz >= ci->ci_l1inst.size) {
		ci->ci_cachepending_l1i = 1;
	} else {
		/* extend the range to multiple of 32 bytes */
		va = _va & ~(32UL - 1);
		sz = ((_va + _sz + 32 - 1) & ~(32UL - 1)) - va;

		/* compute cache offset */
		va &= (ci->ci_l1inst.size - 1);
		inval_subr = (void (*)(vsize_t))
		    ((vaddr_t)tfp_inval_icache + va);
		(*inval_subr)(sz);
	}
}

/*
 * Register a given page for I$ invalidation.
 */
void
tfp_InvalidateICachePage(struct cpu_info *ci, vaddr_t va)
{
	/*
	 * Since the page size matches the size of the instruction cache,
	 * all we need to do here is remember there are postponed flushes.
	 */
	ci->ci_cachepending_l1i = 1;
}

/*
 * Perform postponed I$ invalidation.
 */
void
tfp_SyncICache(struct cpu_info *ci)
{
	if (ci->ci_cachepending_l1i != 0) {
		tfp_inval_icache(ci->ci_l1inst.size);
		ci->ci_cachepending_l1i = 0;
	}
}

/*
 * Writeback D$ for the given page.
 */
void
tfp_SyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
{
	/* nothing to do, D$ is write-through */
}

/*
 * Writeback D$ for the given range.
 */
void
tfp_HitSyncDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	/* nothing to do, D$ is write-through */
}

/*
 * Invalidate D$ for the given range.
 */
void
tfp_HitInvalidateDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va, eva;
	vsize_t sz;
	register_t sr;

	/* extend the range to multiples of the D$ tag span */
	va = _va & ~(TFP_DCTW_STEP - 1);
	sz = ((_va + _sz + TFP_DCTW_STEP - 1) & ~(TFP_DCTW_STEP - 1)) - va;

	sr = disableintr();
	for (eva = va + sz; va < eva; va += TFP_DCTW_STEP)
		tfp_dctw_zero(va);
	setsr(sr);
}

/*
 * Backend for bus_dmamap_sync(). Enforce coherency of the given range
 * by performing the necessary cache writeback and/or invalidate
 * operations.
 */
void
tfp_IOSyncDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz, int how)
{
	switch (how) {
	case CACHE_SYNC_R:
	case CACHE_SYNC_X:
		tfp_HitInvalidateDCache(ci, _va, _sz);
		break;
	case CACHE_SYNC_W:
		/* nothing to do */
		break;
	}
}
@


1.6
log
@Some implementations of HitSyncDCache() call pmap_extract() for va->pa
conversion. Because pmap_extract() acquires the PTE mutex, a "locking
against myself" panic is triggered if the cache routine gets called in
a context where the mutex is already held.

In the pmap, all calls to HitSyncDCache() are for a whole page. Add a
new cache routine, HitSyncDCachePage(), which gets both the va and the
pa of a page. This removes the need of the va->pa conversion. The new
routine has the same signature as SyncDCachePage(), allowing reuse of
the same routine for cache implementations that do not need differences
between "Hit" and non-"Hit" routines.

With the diff, POWER Indigo2 R8000 boots multiuser again. Tested on sgi
GENERIC-IP27.MP and octeon GENERIC.MP, too.

Diff from miod@@, ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_tfp.c,v 1.5 2015/09/20 11:50:05 miod Exp $	*/
d131 3
d135 1
a135 2
		tfp_inval_icache(ci->ci_l1inst.size);
		ci->ci_cachepending_l1i = 0;
@


1.5
log
@Reset the `pending I$ flushes' flag in tfp_InvalidateICache() if invalidating
the whole I$.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_tfp.c,v 1.4 2014/03/31 20:21:19 miod Exp $	*/
d97 1
@


1.4
log
@Due the virtually indexed nature of the L1 instruction cache on most mips
processors, every time a new text page is mapped in a pmap, the L1 I$ is
flushed for the va spanned by this page.

Since we map pages of our binaries upon demand, as they get faulted in, but
uvm_fault() tries to map the few neighbour pages, this can end up in a
bunch of pmap_enter() calls in a row, for executable mappings. If the L1
I$ is small enough, this can cause the whole L1 I$ cache to be flushed
several times.

Change pmap_enter() to postpone these flushes by only registering the
pending flushes, and have pmap_update() perform them. The cpu-specific
cache code can then optimize this to avoid unnecessary operations.

Tested on R4000SC, R4600SC, R5000SC, RM7000, R10000 with 4KB and 16KB
page sizes (coherent and non-coherent designs), and Loongson 2F by mikeb@@ and
me. Should not affect anything on Octeon since there is no way to flush a
subset of I$ anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_tfp.c,v 1.3 2014/03/09 10:12:17 miod Exp $	*/
d69 1
a69 1
	ci->ci_l1inst.linesize = 32;
d71 1
a71 1
	ci->ci_l1data.linesize = 32;
d83 4
d132 1
@


1.3
log
@Rework the per-cpu cache information. Use a common struct to store the line
size, the number of sets, and the total size (and the set size, for convenience)
per cache (I$, D$, L2, L3).
This allows cpu.c to print the number of ways (sets) of L2 and L3 caches from
the cache information, rather than hardcoding this from the processor type.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_tfp.c,v 1.2 2013/06/28 18:56:10 miod Exp $	*/
d90 2
d138 25
@


1.2
log
@Fix unitialized variable; Maxime Villard.
Interestingly enough, gcc did not warn about this because the register allocator
had optimized the uninitialized `eva' local into using the same register as the
initialized `va' local. Therefore it had been initialized. *facepalm*
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_tfp.c,v 1.1 2012/09/29 21:37:03 miod Exp $	*/
d56 1
d58 1
a58 1
		ci->ci_l1instcacheline = 32;
d60 1
a60 2
		ci->ci_l1instcacheline = 16;
	ci->ci_l1instcachesize = (1 << 11) << ((cfg >> 9) & 0x07); /* IC */
d62 1
d64 1
a64 1
		ci->ci_l1datacacheline = 32;
d66 1
a66 2
		ci->ci_l1datacacheline = 16;
	ci->ci_l1datacachesize = (1 << 12) << ((cfg >> 6) & 0x07); /* DC */
d68 4
a71 4
	ci->ci_l1instcacheline = 32;
	ci->ci_l1datacacheline = 32;
	ci->ci_l1instcachesize = 16384;
	ci->ci_l1datacachesize = 16384;
d75 4
a78 3
	ci->ci_cacheways = 1;
	ci->ci_l1instcacheset = ci->ci_l1instcachesize;
	ci->ci_l1datacacheset = ci->ci_l1datacachesize;
d81 1
a81 1
	    (max(ci->ci_l1instcachesize, ci->ci_l1datacachesize) - 1) &
d85 2
a86 3
	ci->ci_l2line = 0;
	ci->ci_l2size = 0;
	ci->ci_l3size = 0;
d105 1
a105 1
	tfp_InvalidateICache(ci, 0, ci->ci_l1instcachesize);
d108 1
a108 1
	eva = ci->ci_l1datacachesize;
d124 2
a125 2
	if (_sz >= ci->ci_l1instcachesize) {
		tfp_inval_icache(ci->ci_l1instcachesize);
d132 1
a132 1
		va &= (ci->ci_l1instcachesize - 1);
@


1.1
log
@Basic R8000 processor support. R8000 processors require MMU-specific code,
exception-specific code, clock-specific code, and L1 cache-specific code. L2
cache is per-design, of which only two exist: SGI Power Indigo2 (IP26) and SGI
Power Challenge (IP21) and are not covered by this commit.

R8000 processors also are 64-bit only processors with 64-bit coprocessor 0
registers, and lack so-called ``compatibility'' memory spaces allowing 32-bit
code to run with sign-extended addresses and registers.

The intrusive changes are covered by #ifdef CPU_R8000 stanzas. However,
trap() is split into a high-level wrapper and a new function, itsa(),
responsible for the actual trap servicing (which name couldn't be helped
because I'm an incorrigible punster). While an R8000 exception may cause
(via trap() ) multiple exceptions to be serviced, non-R8000 processors will
always service one exception in trap(), but they are nevertheless affected
by this code split.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d108 1
a108 1
	va = ci->ci_l1datacachesize;
@

