head	1.67;
access;
symbols
	OPENBSD_6_0:1.66.0.2
	OPENBSD_6_0_BASE:1.66
	OPENBSD_5_9:1.65.0.2
	OPENBSD_5_9_BASE:1.65
	OPENBSD_5_8:1.64.0.6
	OPENBSD_5_8_BASE:1.64
	OPENBSD_5_7:1.64.0.2
	OPENBSD_5_7_BASE:1.64
	OPENBSD_5_6:1.63.0.10
	OPENBSD_5_6_BASE:1.63
	OPENBSD_5_5:1.63.0.8
	OPENBSD_5_5_BASE:1.63
	OPENBSD_5_4:1.63.0.4
	OPENBSD_5_4_BASE:1.63
	OPENBSD_5_3:1.63.0.2
	OPENBSD_5_3_BASE:1.63
	OPENBSD_5_2:1.60.0.8
	OPENBSD_5_2_BASE:1.60
	OPENBSD_5_1_BASE:1.60
	OPENBSD_5_1:1.60.0.6
	OPENBSD_5_0:1.60.0.4
	OPENBSD_5_0_BASE:1.60
	OPENBSD_4_9:1.60.0.2
	OPENBSD_4_9_BASE:1.60
	OPENBSD_4_8:1.59.0.2
	OPENBSD_4_8_BASE:1.59
	OPENBSD_4_7:1.58.0.2
	OPENBSD_4_7_BASE:1.58
	OPENBSD_4_6:1.40.0.4
	OPENBSD_4_6_BASE:1.40
	OPENBSD_4_5:1.33.0.6
	OPENBSD_4_5_BASE:1.33
	OPENBSD_4_4:1.33.0.4
	OPENBSD_4_4_BASE:1.33
	OPENBSD_4_3:1.33.0.2
	OPENBSD_4_3_BASE:1.33
	OPENBSD_4_2:1.31.0.2
	OPENBSD_4_2_BASE:1.31
	OPENBSD_4_1:1.22.0.2
	OPENBSD_4_1_BASE:1.22
	OPENBSD_4_0:1.21.0.2
	OPENBSD_4_0_BASE:1.21
	OPENBSD_3_9:1.19.0.2
	OPENBSD_3_9_BASE:1.19
	OPENBSD_3_8:1.18.0.2
	OPENBSD_3_8_BASE:1.18
	OPENBSD_3_7:1.11.0.2
	OPENBSD_3_7_BASE:1.11
	OPENBSD_3_6:1.4.0.2
	OPENBSD_3_6_BASE:1.4;
locks; strict;
comment	@ * @;


1.67
date	2016.08.16.13.03.58;	author visa;	state Exp;
branches;
next	1.66;
commitid	6TiwRQZbesPVamIE;

1.66
date	2016.03.06.19.42.27;	author mpi;	state Exp;
branches;
next	1.65;
commitid	cyYKarj4qRTft4gD;

1.65
date	2015.09.13.20.38.45;	author kettenis;	state Exp;
branches;
next	1.64;
commitid	MW9yRxgkKFAx03on;

1.64
date	2014.09.30.06.51.58;	author jmatthew;	state Exp;
branches;
next	1.63;
commitid	pUEUpP9FlbomZUiI;

1.63
date	2012.10.03.11.18.23;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2012.09.29.19.13.15;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2012.09.29.18.54.38;	author miod;	state Exp;
branches;
next	1.60;

1.60
date	2010.09.20.06.33.47;	author matthew;	state Exp;
branches;
next	1.59;

1.59
date	2010.04.21.03.03.26;	author deraadt;	state Exp;
branches;
next	1.58;

1.58
date	2010.02.13.14.04.45;	author miod;	state Exp;
branches;
next	1.57;

1.57
date	2010.01.18.16.57.46;	author miod;	state Exp;
branches;
next	1.56;

1.56
date	2010.01.09.23.43.43;	author miod;	state Exp;
branches;
next	1.55;

1.55
date	2009.11.27.00.08.27;	author syuu;	state Exp;
branches;
next	1.54;

1.54
date	2009.11.26.23.32.46;	author syuu;	state Exp;
branches;
next	1.53;

1.53
date	2009.11.22.00.31.03;	author syuu;	state Exp;
branches;
next	1.52;

1.52
date	2009.11.22.00.19.49;	author syuu;	state Exp;
branches;
next	1.51;

1.51
date	2009.11.21.23.28.14;	author syuu;	state Exp;
branches;
next	1.50;

1.50
date	2009.11.19.20.15.04;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2009.10.22.22.08.54;	author miod;	state Exp;
branches;
next	1.48;

1.48
date	2009.10.22.20.59.24;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2009.10.22.20.39.16;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2009.10.22.20.10.44;	author miod;	state Exp;
branches;
next	1.45;

1.45
date	2009.10.22.20.05.27;	author miod;	state Exp;
branches;
next	1.44;

1.44
date	2009.10.22.18.46.48;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2009.10.22.18.22.11;	author miod;	state Exp;
branches;
next	1.42;

1.42
date	2009.10.07.08.35.47;	author syuu;	state Exp;
branches;
next	1.41;

1.41
date	2009.08.06.21.05.49;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2009.06.10.18.05.31;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2009.05.27.18.58.15;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2009.05.22.20.37.53;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2009.05.21.16.08.56;	author miod;	state Exp;
branches;
next	1.36;

1.36
date	2009.05.21.16.08.04;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2009.04.25.20.35.06;	author miod;	state Exp;
branches;
next	1.34;

1.34
date	2009.03.20.18.41.06;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2008.02.20.19.13.38;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2007.10.14.18.52.10;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2007.07.16.20.20.08;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2007.06.20.20.47.33;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2007.06.20.16.50.39;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2007.06.18.20.24.48;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2007.05.29.18.10.43;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2007.05.09.19.20.09;	author miod;	state Exp;
branches;
next	1.25;

1.25
date	2007.05.07.18.42.13;	author kettenis;	state Exp;
branches;
next	1.24;

1.24
date	2007.03.23.21.07.38;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2007.03.15.10.22.29;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2006.12.24.20.30.35;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2006.05.11.19.57.45;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2006.03.04.19.33.21;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2005.12.20.06.57.52;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2005.08.14.11.02.30;	author miod;	state Exp;
branches;
next	1.17;

1.17
date	2005.07.20.21.55.50;	author miod;	state Exp;
branches;
next	1.16;

1.16
date	2005.07.18.02.43.25;	author fgsch;	state Exp;
branches;
next	1.15;

1.15
date	2005.06.07.02.29.30;	author henning;	state Exp;
branches;
next	1.14;

1.14
date	2005.05.30.12.51.13;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2005.05.29.03.20.40;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	2005.05.25.23.17.47;	author niklas;	state Exp;
branches;
next	1.11;

1.11
date	2005.01.31.21.35.50;	author grange;	state Exp;
branches;
next	1.10;

1.10
date	2005.01.18.15.03.38;	author grange;	state Exp;
branches;
next	1.9;

1.9
date	2004.10.08.07.14.57;	author grange;	state Exp;
branches;
next	1.8;

1.8
date	2004.09.27.19.20.49;	author pefo;	state Exp;
branches;
next	1.7;

1.7
date	2004.09.24.14.22.49;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2004.09.21.05.51.15;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2004.09.20.10.29.57;	author pefo;	state Exp;
branches;
next	1.4;

1.4
date	2004.08.10.20.15.47;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	2004.08.10.08.07.35;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	2004.08.09.14.57.26;	author pefo;	state Exp;
branches;
next	1.1;

1.1
date	2004.08.06.20.56.03;	author pefo;	state Exp;
branches;
next	;


desc
@@


1.67
log
@Remove RM7000/RM9000-specific performance counter code. It originates
from PMON2000 and has not been enabled on OpenBSD.

Suggested by and ok miod@@ (after seeing a quad_t cleanup patch of mine)
@
text
@/*	$OpenBSD: interrupt.c,v 1.66 2016/03/06 19:42:27 mpi Exp $ */

/*
 * Copyright (c) 2001-2004 Opsycon AB  (www.opsycon.se / www.opsycon.com)
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
 * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/user.h>
#include <sys/atomic.h>

#include <uvm/uvm_extern.h>

#include <machine/cpu.h>
#include <mips64/mips_cpu.h>
#include <machine/intr.h>
#include <machine/frame.h>

#ifdef DDB
#include <mips64/db_machdep.h>
#include <ddb/db_sym.h>
#endif

void	dummy_splx(int);
void	interrupt(struct trapframe *);

static struct evcount soft_count;
static int soft_irq = 0;

uint32_t idle_mask;
int	last_low_int;

struct {
	uint32_t int_mask;
	uint32_t (*int_hand)(uint32_t, struct trapframe *);
} cpu_int_tab[NLOWINT];

int_f	*splx_hand = &dummy_splx;

/*
 *  Modern versions of MIPS processors have extended interrupt
 *  capabilities. How these are handled differs from implementation
 *  to implementation. This code tries to hide away some of these
 *  in "higher level" interrupt code.
 *
 *  Basically there are <n> interrupt inputs to the processor and
 *  typically the HW designer ties these interrupts to various
 *  sources in the HW. The low level code does not deal with interrupts
 *  in more than it dispatches handling to the code that has registered
 *  an interrupt handler for that particular interrupt. More than one
 *  handler can register to an interrupt input and one handler may register
 *  for more than one interrupt input. A handler is only called once even
 *  if it registers for more than one interrupt input.
 *
 *  The interrupt mechanism in this port uses a delayed masking model
 *  where interrupts are not really masked when doing an spl(). Instead
 *  a masked interrupt will be taken and validated in the various
 *  handlers. If the handler finds that an interrupt is masked it will
 *  register this interrupt as pending and return a new mask to this
 *  code that will turn off the interrupt hardware wise. Later when
 *  the pending interrupt is unmasked it will be processed as usual
 *  and the regular hardware mask will be restored.
 */

/*
 * Handle an interrupt. Both kernel and user mode are handled here.
 *
 * The interrupt handler is called with the CR_INT bits set that
 * were given when the handler was registered.
 * The handler should return a similar word with a mask indicating
 * which CR_INT bits have been handled.
 */

void
interrupt(struct trapframe *trapframe)
{
	struct cpu_info *ci = curcpu();
	u_int32_t pending;
	int i, s;

	/*
	 *  Paranoic? Perhaps. But if we got here with the enable
	 *  bit reset a mtc0 COP_0_STATUS_REG may have been interrupted.
	 *  If this was a disable and the pipeline had advanced long
	 *  enough... i don't know but better safe than sorry...
	 *  The main effect is not the interrupts but the spl mechanism.
	 */
	if (!(trapframe->sr & SR_INT_ENAB))
		return;

	ci->ci_intrdepth++;

#ifdef DEBUG_INTERRUPT
	trapdebug_enter(ci, trapframe, T_INT);
#endif
	atomic_inc_int(&uvmexp.intrs);

	/* Mask out interrupts from cause that are unmasked */
	pending = trapframe->cause & CR_INT_MASK & trapframe->sr;

	if (pending & SOFT_INT_MASK_0) {
		clearsoftintr0();
		atomic_inc_long((unsigned long *)&soft_count.ec_count);
	}

	for (i = 0; i <= last_low_int; i++) {
		uint32_t active;
		active = cpu_int_tab[i].int_mask & pending;
		if (active != 0)
			(*cpu_int_tab[i].int_hand)(active, trapframe);
	}

	/*
	 * Dispatch soft interrupts if current ipl allows them.
	 */
	if (ci->ci_ipl < IPL_SOFTINT && ci->ci_softpending != 0) {
		s = splsoft();
		dosoftint();
		__asm__ (".set noreorder\n");
		ci->ci_ipl = s;	/* no-overhead splx */
		mips_sync();
		__asm__ (".set reorder\n");
	}

	ci->ci_intrdepth--;
}


/*
 * Set up handler for external interrupt events.
 * Use CR_INT_<n> to select the proper interrupt condition to dispatch on.
 * We also enable the software ints here since they are always on.
 */
void
set_intr(int pri, uint32_t mask,
    uint32_t (*int_hand)(uint32_t, struct trapframe *))
{
	if ((idle_mask & SOFT_INT_MASK) == 0) {
		evcount_attach(&soft_count, "soft", &soft_irq);
		idle_mask |= SOFT_INT_MASK;
	}
	if (pri < 0 || pri >= NLOWINT)
		panic("set_intr: too high priority (%d), increase NLOWINT",
		    pri);

	if (pri > last_low_int)
		last_low_int = pri;

	if ((mask & ~CR_INT_MASK) != 0)
		panic("set_intr: invalid mask 0x%x", mask);

	if (cpu_int_tab[pri].int_mask != 0 &&
	   (cpu_int_tab[pri].int_mask != mask ||
	    cpu_int_tab[pri].int_hand != int_hand))
		panic("set_intr: int already set at pri %d", pri);

	cpu_int_tab[pri].int_hand = int_hand;
	cpu_int_tab[pri].int_mask = mask;
	idle_mask |= mask;
}

void
intr_barrier(void *cookie)
{
	sched_barrier(NULL);
}

void
dummy_splx(int newcpl)
{
	/* Dummy handler */
}

/*
 *  splinit() is special in that sense that it require us to update
 *  the interrupt mask in the CPU since it may be the first time we arm
 *  the interrupt system. This function is called right after
 *  autoconfiguration has completed in autoconf.c.
 *  We enable everything in idle_mask.
 */
void
splinit()
{
	struct proc *p = curproc;
	struct pcb *pcb = &p->p_addr->u_pcb;

	/*
	 * Update proc0 pcb to contain proper values.
	 */
#ifdef RM7000_ICR
	pcb->pcb_context.val[12] = (idle_mask << 8) & IC_INT_MASK;
#endif
	pcb->pcb_context.val[11] = (pcb->pcb_regs.sr & ~SR_INT_MASK) |
	    (idle_mask & SR_INT_MASK);

	spl0();
	(void)updateimask(0);
}

int
splraise(int newipl)
{
	struct cpu_info *ci = curcpu();
        int oldipl;

	__asm__ (".set noreorder\n");
	oldipl = ci->ci_ipl;
	if (oldipl < newipl) {
		/* XXX to kill warning about dla being used in a delay slot */
		__asm__("nop");
		ci->ci_ipl = newipl;
	}
	mips_sync();
	__asm__ (".set reorder\n");
	return oldipl;
}

void
splx(int newipl)
{
	(*splx_hand)(newipl);
}

int
spllower(int newipl)
{
	struct cpu_info *ci = curcpu();
	int oldipl;

	oldipl = ci->ci_ipl;
	splx(newipl);
	return oldipl;
}
@


1.66
log
@Rename mips64's trap_frame into trapframe.

For coherency with other archs and in order to use it in MI code.

ok visa@@, tobiasu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.65 2015/09/13 20:38:45 kettenis Exp $ */
a42 2
#include <mips64/rm7000.h>

a128 5

#ifdef RM7K_PERFCNTR
	if (pending & CR_INT_PERF)
		rm7k_perfintr(trapframe);
#endif
@


1.65
log
@intr_barrier(9) for loongson, octeon and sgi.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.64 2014/09/30 06:51:58 jmatthew Exp $ */
d51 1
a51 1
void	interrupt(struct trap_frame *);
d61 1
a61 1
	uint32_t (*int_hand)(uint32_t, struct trap_frame *);
d101 1
a101 1
interrupt(struct trap_frame *trapframe)
d167 1
a167 1
    uint32_t (*int_hand)(uint32_t, struct trap_frame *))
@


1.64
log
@implement atomic operations using ll/sc, and convert rw_cas and callers of the
pre-existing atomics to match.

tested on sgi (octane) and octeon (erl)
ok miod@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.63 2012/10/03 11:18:23 miod Exp $ */
d191 6
@


1.63
log
@Split ever-growing mips <machine/cpu.h> into what 99% of the kernel needs,
which will remain in <machine/cpu.h>, and a new mips_cpu.h containing only the
goriest md details, which are only of interest to a handful set of files; this
is similar in spirit to what alpha does, but here <machine/cpu.h> does not
include the new file.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.62 2012/09/29 19:13:15 miod Exp $ */
d34 1
d122 1
a122 1
	atomic_add_int(&uvmexp.intrs, 1);
d129 1
a129 1
		atomic_add_uint64(&soft_count.ec_count, 1);
@


1.62
log
@Add a few more coprocessor 0 cause and config registers defines.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.61 2012/09/29 18:54:38 miod Exp $ */
d38 1
@


1.61
log
@Proide a mips_sync() macro to wrap asm("sync"), and replace gazillions of
such statements with it.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.60 2010/09/20 06:33:47 matthew Exp $ */
d123 1
a123 1
	pending = trapframe->cause & CR_IPEND & trapframe->sr;
d167 1
a167 1
	if ((idle_mask & SOFT_INT_MASK) == 0)
d169 2
d178 1
a178 1
	if ((mask & ~CR_IPEND) != 0)
d188 1
a188 1
	idle_mask |= mask | SOFT_INT_MASK;
@


1.60
log
@Get rid of evcount's support for arranging counters in a tree
hierarchy.  Everything attached to a single root node anyway, so at
best we had a bush.

"i think it is good" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.59 2010/04/21 03:03:26 deraadt Exp $ */
d150 2
a151 1
		__asm__ ("sync\n\t.set reorder\n");
d234 2
a235 1
	__asm__ ("sync\n\t.set reorder\n");
@


1.59
log
@more cleanup to cope with the change that tries to make proc.h not act
like it is everything.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.58 2010/02/13 14:04:45 miod Exp $ */
d167 1
a167 2
		evcount_attach(&soft_count, "soft", (void *)&soft_irq,
		    &evcount_intr);
@


1.58
log
@Since we don't use the saved ipl in pcb anymore, remove it from the struct
layout.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.57 2010/01/18 16:57:46 miod Exp $ */
d32 1
@


1.57
log
@Make trapdebug code MP-safe.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.56 2010/01/09 23:43:43 miod Exp $ */
a209 1
	pcb->pcb_context.val[13] = IPL_NONE;
@


1.56
log
@Make interrupt depth counters per-cpu.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.55 2009/11/27 00:08:27 syuu Exp $ */
d117 1
a117 1
	trapdebug_enter(trapframe, 0);
@


1.55
log
@atomic counter increment for SMP.
ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.54 2009/11/26 23:32:46 syuu Exp $ */
d114 2
d151 2
@


1.54
log
@Now IPI can interrupt to clock interrupt handler.
It prevents deadlock with TLB shootdown and clock interrupt.
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.53 2009/11/22 00:31:03 syuu Exp $ */
d117 1
a117 1
	uvmexp.intrs++;
d124 1
a124 1
		soft_count.ec_count++;
@


1.53
log
@Fix compile error in uniprocessor kernel.
ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.52 2009/11/22 00:19:49 syuu Exp $ */
a116 6

#ifdef MULTIPROCESSOR
	if (ci->ci_ipl < IPL_SCHED)
		__mp_lock(&kernel_lock);
#endif

a148 5

#ifdef MULTIPROCESSOR
	if (ci->ci_ipl < IPL_SCHED)
		__mp_unlock(&kernel_lock);
#endif
@


1.52
log
@Correct cording style.
ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.51 2009/11/21 23:28:14 syuu Exp $ */
d118 1
d121 1
d156 1
d159 1
@


1.51
log
@mplock, rw_cas implemented
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.50 2009/11/19 20:15:04 miod Exp $ */
d118 1
a118 1
	if(ci->ci_ipl < IPL_SCHED)
d120 1
d153 2
a154 1
	if(ci->ci_ipl < IPL_SCHED)
@


1.50
log
@All callers of updateimask() immediately enable interrupts afterwards, so do
it in updateimask() itself. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.49 2009/10/22 22:08:54 miod Exp $ */
d118 2
d152 2
@


1.49
log
@Completely overhaul interrupt handling on sgi. Cpu state now only stores a
logical IPL level, and per-platform (IP27/IP30/IP32) code will from the
necessary hardware mask registers.

This allows the use of more than one interrupt mask register. Also, the
generic (platform independent) interrupt code shrinks a lot, and the actual
interrupt handler chains and masking information is now per-platform private
data.

Interrupt dispatching is generated from a template; more routines will be
added to the template to reduce platform-specific changes and share as much
code as possible.

Tested on IP27, IP30, IP32 and IP35.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.48 2009/10/22 20:59:24 miod Exp $ */
a202 1
	u_int32_t sr;
d215 1
a215 3
	sr = updateimask(0);
	sr |= SR_INT_ENAB;
	setsr(sr);
@


1.48
log
@With the splx() changes, it is no longer necessary to remember which interrupt
sources were masked and saved in ci_ipending, as splx() will unmask what needs
to be unmasked anyway. ci_ipending only now needs to store pending soft
interrupts, so rename it to ci_softpending.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.47 2009/10/22 20:39:16 miod Exp $ */
a31 1
#include <sys/signalvar.h>
a32 5
#include <sys/malloc.h>
#include <sys/device.h>
#ifdef KTRACE
#include <sys/ktrace.h>
#endif
d34 2
a35 1
#include <machine/trap.h>
a37 1
#include <machine/autoconf.h>
a38 2
#include <machine/regnum.h>
#include <machine/atomic.h>
a41 2
#include <mips64/archtype.h>

a52 2
uint32_t imask[NIPLS];

d89 1
a89 1
 * Handle an interrupt. Both kernel and user mode is handled here.
d102 1
a102 3
	u_int32_t cause;
	int i;
	uint32_t xcpl;
a121 1
	cause = pending;
d123 1
a123 1
	if (cause & SOFT_INT_MASK_0) {
d129 1
a129 1
	if (cause & CR_INT_PERF) {
a130 2
		cause &= ~CR_INT_PERF;
	}
d136 2
a137 3
		if (active) {
			cause &= ~(*cpu_int_tab[i].int_hand)(active, trapframe);
		}
d141 1
a141 1
	 *  Reenable all non served hardware levels.
d143 6
a148 8
#if 0
	/* XXX the following should, when req., change the IC reg as well */
	setsr((trapframe->sr & ~pending) | SR_INT_ENAB);
#endif

	xcpl = splsoft();
	if (ci->ci_softpending & ~xcpl) {
		dosoftint(xcpl);
a149 4

	__asm__ (" .set noreorder\n");
	ci->ci_cpl = xcpl;
	__asm__ (" sync\n .set reorder\n");
d154 3
a156 4
 *  Set up handler for external interrupt events.
 *  Use CR_INT_<n> to select the proper interrupt
 *  condition to dispatch on. We also enable the
 *  software ints here since they are always on.
d160 1
a160 1
	uint32_t (*int_hand)(uint32_t, struct trap_frame *))
d163 5
a167 4
		evcount_attach(&soft_count, "soft", (void *)&soft_irq, &evcount_intr);
	if (pri < 0 || pri >= NLOWINT) {
		panic("set_intr: to high priority");
	}
d172 1
a172 1
	if ((mask & ~CR_IPEND) != 0) {
a173 1
	}
d177 1
a177 1
	    cpu_int_tab[pri].int_hand != int_hand)) {
a178 1
	}
a184 2
struct intrhand *intrhand[INTMASKSIZE];

d208 1
a208 1
	pcb->pcb_context.val[13] = 0;	/* IPL_NONE */
d222 1
a222 1
splraise(int newcpl)
d225 1
a225 1
        int oldcpl;
d227 9
a235 5
	__asm__ (" .set noreorder\n");
	oldcpl = ci->ci_cpl;
	ci->ci_cpl = oldcpl | newcpl;
	__asm__ (" sync\n .set reorder\n");
	return (oldcpl);
d239 1
a239 1
splx(int newcpl)
d241 1
a241 1
	(*splx_hand)(newcpl);
d245 1
a245 1
spllower(int newcpl)
d248 1
a248 1
	int oldcpl;
d250 3
a252 3
	oldcpl = ci->ci_cpl;
	splx(newcpl);
	return (oldcpl);
@


1.47
log
@The recent cleanups make blatantly visible that the pending_int handler
does almost exactly what splx() is doing if ipending is zero, and triggers
soft interrupts as well.

So don't bother checking for ipending in splx, and always invoke pending_int,
which gets renamed as splx_handler for consistency.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.46 2009/10/22 20:10:44 miod Exp $ */
d167 1
a167 1
	if ((ci->ci_ipending & SINT_ALLMASK) & ~xcpl) {
@


1.46
log
@unifdef -DIMASK_EXTERNAL to the mips code. Support for interrupt masking at
coprocessor 0 sr level might come back in the future if hardware support
requires it, but at the moment it's getting in the way of larger changes.
``In the Attic, noone can hear you scream''
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.45 2009/10/22 20:05:27 miod Exp $ */
d57 2
d73 1
a73 3
void dummy_do_pending_int(int);

int_f *pending_hand = &dummy_do_pending_int;
d88 1
a88 1
 *  if it register for more than one interrupt input.
d97 1
a97 1
 *  and the hardware mask will be restored.
a100 22
 *  Interrupt mapping is as follows:
 *
 *  irq can be between 1 and 10. This maps to CPU IPL2..IPL11.
 *  The two software interrupts IPL0 and IPL1 are reserved for
 *  kernel functions. IPL13 is used for the performance counters
 *  in the RM7000. IPL12 extra timer is currently not used.
 *
 *  irq's maps into the software spl register to the bit corresponding
 *  to its status/mask bit in the cause/sr register shifted right eight
 *  places.
 *
 *  A well designed system uses the CPUs interrupt inputs in a way, such
 *  that masking can be done according to the IPL in the CPU status and
 *  interrupt control register. However support for an external masking
 *  register is provided but will cause a slightly higher overhead when
 *  used. When an external masking register is used, no masking in the
 *  CPU is done. Instead a fixed mask is set and used throughout.
 */

void interrupt(struct trap_frame *);

/*
d104 1
a104 1
 * was given when the handlers was registered that needs servicing.
d106 1
a106 1
 * which CR_INT bits that has been served.
d125 1
a125 1
	if (!(trapframe->sr & SR_INT_ENAB)) {
a126 1
	}
d214 1
a214 1
dummy_do_pending_int(int newcpl)
d265 1
a265 10
	struct cpu_info *ci = curcpu();

	if (ci->ci_ipending & ~newcpl)
		(*pending_hand)(newcpl);
	else {
		__asm__ (" .set noreorder\n");
		ci->ci_cpl = newcpl;
		__asm__ (" sync\n .set reorder\n");
		hw_setintrmask(newcpl);
	}
a277 1

@


1.45
log
@Replace intrmask_t with uint32_t. This types only describes interrupt masks
in the coprocessor 0 status register (coupled with ICR on rm7k/rm9k), and
may be completely alien to real hardware interrupt masks, so don't make
things unnecessary confusing.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.44 2009/10/22 18:46:48 miod Exp $ */
a295 1
#ifdef IMASK_EXTERNAL
a296 1
#endif
@


1.44
log
@Change the #define controlling use of RM7k/RM9k coprocessor 0 ICR to
RM7000_ICR, instead of IMASK_EXTERNAL, since they are actually different
concepts. This code remains disabled since RM7000_ICR is not defined anywhere
at the moment.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.43 2009/10/22 18:22:11 miod Exp $ */
d61 1
a61 1
intrmask_t imask[NIPLS];
d63 1
a63 1
intrmask_t idle_mask;
d67 2
a68 2
	intrmask_t int_mask;
	intrmask_t (*int_hand)(intrmask_t, struct trap_frame *);
d138 1
a138 1
	intrmask_t xcpl;
d174 1
a174 1
		intrmask_t active;
d207 2
a208 2
set_intr(int pri, intrmask_t mask,
	intrmask_t (*int_hand)(intrmask_t, struct trap_frame *))
@


1.43
log
@Do not bother invoking hw_setintrmask() in splinit(), spl0() will do it for
us via splx().
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.42 2009/10/07 08:35:47 syuu Exp $ */
d260 1
d262 1
@


1.42
log
@ipending, cpl moved into cpu_info
OK miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.41 2009/08/06 21:05:49 miod Exp $ */
a267 3
#ifdef IMASK_EXTERNAL
	hw_setintrmask(0);
#endif
@


1.41
log
@Only compile RM7000 performance counter support if defined(RM7K_PERFCNTR).
This code needs to be cleaned up, and made more generic to work with other
processors counters as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.40 2009/06/10 18:05:31 miod Exp $ */
a60 3
volatile intrmask_t cpl;
volatile intrmask_t ipending;

d134 1
d190 1
a190 1
	if ((ipending & SINT_ALLMASK) & ~xcpl) {
d195 1
a195 1
	cpl = xcpl;
d276 1
d280 2
a281 2
	oldcpl = cpl;
	cpl = oldcpl | newcpl;
d289 3
a291 1
	if (ipending & ~newcpl)
d295 1
a295 1
		cpl = newcpl;
d306 1
d309 1
a309 1
	oldcpl = cpl;
@


1.40
log
@Switch sgi to per-process AST, and move ast() from interrupt.c to trap.c
where it can use userret() instead of duplicating it.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.39 2009/05/27 18:58:15 miod Exp $ */
d168 1
d173 1
@


1.39
log
@Rename the ast processing function from softintr() to ast(), to reduce
confusion. Make sure this function is invoked with interrupts enabled now.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.38 2009/05/22 20:37:53 miod Exp $ */
d62 1
a62 1
volatile intrmask_t ipending, astpending;
a123 1
void ast(void);
a231 25
}

/*
 * This is called from MipsUserIntr() if astpending is set.
 */
void
ast()
{
	struct proc *p = curproc;
	int sig;

	uvmexp.softs++;

	astpending = 0;
	if (p->p_flag & P_OWEUPC) {
		ADDUPROF(p);
	}
	if (want_resched)
		preempt(NULL);

	/* inline userret(p) */

	while ((sig = CURSIG(p)) != 0)		/* take pending signals */
		postsig(sig);
	p->p_cpu->ci_schedstate.spc_curpriority = p->p_priority = p->p_usrpri;
@


1.38
log
@Drop almost unused <machine/psl.h> on sgi; move USERMODE() definition from
there to trap.c which is its only user. This also cleans up multiple
inclusion of <machine/cpu.h> (because <machine/psl.h> includes it) in many
places.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.37 2009/05/21 16:08:56 miod Exp $ */
d124 1
a124 1
void softintr(void);
d239 1
a239 1
softintr()
@


1.37
log
@In splinit(), adjust proc0 pcb values, for kthread to start with correct
interrupt masks.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.36 2009/05/21 16:08:04 miod Exp $ */
a40 1
#include <machine/psl.h>
@


1.36
log
@Make sure splx() reenables hardware interrupt sources, even there aren't
any such interrupts marked as pending.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.35 2009/04/25 20:35:06 miod Exp $ */
d279 2
d282 8
@


1.35
log
@typo in comments
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.34 2009/03/20 18:41:06 miod Exp $ */
a289 1
#ifndef INLINE_SPLRAISE
d301 12
d314 13
@


1.34
log
@Switch sgi to __HAVE_GENERIC_SOFT_INTERRUPTS.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.33 2008/02/20 19:13:38 miod Exp $ */
d273 1
a273 1
 *  autoconfiguration has compleeted in autoconf.c.
@


1.33
log
@More dead includes and functions noone will mourn.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.32 2007/10/14 18:52:10 miod Exp $ */
a38 1
#include <net/netisr.h>
a78 2
int netisr;

d191 3
a193 16
	xcpl = splsoftnet();
	if ((ipending & SINT_CLOCKMASK) & ~xcpl) {
		atomic_clearbits_int(&ipending, SINT_CLOCKMASK);
		softclock();
	}
	if ((ipending & SINT_NETMASK) & ~xcpl) {
		extern int netisr;
		int isr;

		atomic_clearbits_int(&ipending, SINT_NETMASK);
		while ((isr = netisr) != 0) {
			atomic_clearbits_int(&netisr, isr);

#define DONETISR(b,f)   if (isr & (1 << (b)))   f();
#include <net/netisr_dispatch.h>
		}
a195 6
#ifdef notyet
	if ((ipending & SINT_TTYMASK) & ~xcpl) {
		atomic_clearbits_int(&ipending, SINT_TTYMASK);
		compoll(NULL);
	}
#endif
a260 3

intrmask_t intem = 0x0;
intrmask_t intrtype[INTMASKSIZE], intrmask[INTMASKSIZE], intrlevel[INTMASKSIZE];
a262 241
/*======================================================================*/

#if 0

/*
 *	Generic interrupt handling code.
 *      ================================
 *
 *  This code can be used for interrupt models where only the
 *  processor status register has to be changed to mask/unmask.
 *  HW specific setup can be done in a MD function that can then
 *  call this function to use the generic interrupt code.
 */
static int fakeintr(void *);
static int fakeintr(void *a) {return 0;}

/*
 *  Establish an interrupt handler called from the dispatcher.
 *  The interrupt function established should return zero if
 *  there was nothing to serve (no int) and non zero when an
 *  interrupt was serviced.
 *  Interrupts are numbered from 1 and up where 1 maps to HW int 0.
 */
void *
generic_intr_establish(icp, irq, type, level, ih_fun, ih_arg, ih_what)
	void *icp;
        u_long irq;	/* XXX pci_intr_handle_t compatible XXX */
        int type;
        int level;
        int (*ih_fun)(void *);
        void *ih_arg;
        char *ih_what;
{
	struct intrhand **p, *q, *ih;
	static struct intrhand fakehand = {NULL, fakeintr};
	int edge;

static int initialized = 0;

	if (!initialized) {
/*INIT CODE HERE*/
		initialized = 1;
	}

	if (irq > 62 || irq < 1) {
		panic("intr_establish: illegal irq %d", irq);
	}
	irq += 1;	/* Adjust for softint 1 and 0 */

	/* no point in sleeping unless someone can free memory. */
	ih = malloc(sizeof *ih, M_DEVBUF, cold ? M_NOWAIT : M_WAITOK);
	if (ih == NULL)
		panic("intr_establish: can't malloc handler info");

	if (type == IST_NONE || type == IST_PULSE)
		panic("intr_establish: bogus type");

	switch (intrtype[irq]) {
	case IST_EDGE:
	case IST_LEVEL:
		if (type == intrtype[irq])
			break;
	}

	switch (type) {
	case IST_EDGE:
		edge |= 1 << irq;
		break;
	case IST_LEVEL:
		edge &= ~(1 << irq);
		break;
	}

	/*
	 * Figure out where to put the handler.
	 * This is O(N^2), but we want to preserve the order, and N is
	 * generally small.
	 */
	for (p = &intrhand[irq]; (q = *p) != NULL; p = &q->ih_next)
		;

	/*
	 * Actually install a fake handler momentarily, since we might be doing
	 * this with interrupts enabled and don't want the real routine called
	 * until masking is set up.
	 */
	fakehand.ih_level = level;
	*p = &fakehand;

	generic_intr_makemasks();

	/*
	 * Poke the real handler in now.
	 */
	ih->ih_fun = ih_fun;
	ih->ih_arg = ih_arg;
	ih->ih_next = NULL;
	ih->ih_level = level;
	ih->ih_irq = irq;
	ih->ih_what = ih_what;
	evcount_attach(&ih->ih_count, ih_what, (void *)&ih->ih_irq,
	    &evcount_intr);
	*p = ih;

	return (ih);
}

void
generic_intr_disestablish(void *p1, void *p2)
{
}

/*
 *  Regenerate interrupt masks to reflect reality.
 */
void
generic_intr_makemasks()
{
	int irq, level;
	struct intrhand *q;

	/* First, figure out which levels each IRQ uses. */
	for (irq = 0; irq < INTMASKSIZE; irq++) {
		int levels = 0;
		for (q = intrhand[irq]; q; q = q->ih_next)
			levels |= 1 << q->ih_level;
		intrlevel[irq] = levels;
	}

	/* Then figure out which IRQs use each level. */
	for (level = IPL_NONE; level < NIPLS; level++) {
		register int irqs = 0;
		for (irq = 0; irq < INTMASKSIZE; irq++)
			if (intrlevel[irq] & (1 << level))
				irqs |= 1 << irq;
		imask[level] = irqs | SINT_ALLMASK;
	}

	/*
	 * There are tty, network and disk drivers that use free() at interrupt
	 * time, so imp > (tty | net | bio).
	 *
	 * Enforce a hierarchy that gives slow devices a better chance at not
	 * dropping data.
	 */
	imask[IPL_NET] |= imask[IPL_BIO];
	imask[IPL_TTY] |= imask[IPL_NET];
	imask[IPL_VM] |= imask[IPL_TTY];
	imask[IPL_CLOCK] |= imask[IPL_VM] | SPL_CLOCKMASK;

	/*
	 * These are pseudo-levels.
	 */
	imask[IPL_NONE] = 0;
	imask[IPL_HIGH] = -1;

	/* And eventually calculate the complete masks. */
	for (irq = 0; irq < INTMASKSIZE; irq++) {
		register int irqs = 1 << irq;
		for (q = intrhand[irq]; q; q = q->ih_next)
			irqs |= imask[q->ih_level];
		intrmask[irq] = irqs | SINT_ALLMASK;
	}

	/* Lastly, determine which IRQs are actually in use. */
	irq = 0;
	for (level = 0; level < INTMASKSIZE; level++) {
		if (intrhand[level]) {
			irq |= 1 << level;
		}
	}
	intem = irq;
}

void
generic_do_pending_int(int newcpl)
{
	struct intrhand *ih;
	int vector;
	intrmask_t hwpend;
	struct trap_frame cf;
	static volatile int processing;

	/* Don't recurse... but change the mask. */
	if (processing) {
		__asm__ (" .set noreorder\n");
		cpl = newcpl;
		__asm__ (" sync\n .set reorder\n");
		return;
	}
	processing = 1;

	/* XXX Fake a trapframe for clock pendings... */
	cf.pc = (int)&generic_do_pending_int;
	cf.sr = 0;
	cf.cpl = cpl;

	hwpend = ipending & ~newcpl;	/* Do pendings being unmasked */
	hwpend &= ~(SINT_ALLMASK);
	atomic_clearbits_int(&ipending, hwpend);
	intem |= hwpend;
	while (hwpend) {
		vector = ffs(hwpend) - 1;
		hwpend &= ~(1L << vector);
		ih = intrhand[vector];
		while (ih) {
			ih->frame = &cf;
			if ((*ih->ih_fun)(ih->ih_arg)) {
				ih->ih_count.ec_count++;
			}
			ih = ih->ih_next;
		}
	}
	if ((ipending & SINT_CLOCKMASK) & ~newcpl) {
		atomic_clearbits_int(&ipending, SINT_CLOCKMASK);
		softclock();
	}
	if ((ipending & SINT_NETMASK) & ~newcpl) {
		int isr = netisr;
		netisr = 0;
		atomic_clearbits_int(&ipending, SINT_NETMASK);
#define	DONETISR(b,f)	if (isr & (1 << (b)))   f();
#include <net/netisr_dispatch.h>
	}

#ifdef NOTYET
	if ((ipending & SINT_TTYMASK) & ~newcpl) {
		atomic_clearbits_int(&ipending, SINT_TTYMASK);
		compoll(NULL);
	}
#endif

	__asm__ (" .set noreorder\n");
	cpl = newcpl;
	__asm__ (" sync\n .set reorder\n");
	updateimask(newcpl);	/* Update CPU mask ins SR register */
	processing = 0;
}

#endif

a288 38

#if 0

/*
 *  Process interrupts. The parameter pending has non-masked interrupts.
 */
intrmask_t
generic_iointr(intrmask_t pending, struct trap_frame *cf)
{
	struct intrhand *ih;
	intrmask_t caught, vm;
	int v;

	caught = 0;

	atomic_setbits_int(&ipending, (pending >> 8) & cpl);
	pending &= ~(cpl << 8);
	cf->sr &= ~((ipending << 8) & SR_INT_MASK);
	cf->ic &= ~(ipending & IC_INT_MASK);

	for (v = 2, vm = 0x400; pending != 0 && v < 16 ; v++, vm <<= 1) {
		if (pending & vm) {
			ih = intrhand[v];

			while (ih) {
				ih->frame = cf;
				if ((*ih->ih_fun)(ih->ih_arg)) {
					caught |= vm;
					ih->ih_count.ec_count++;
				}
				ih = ih->ih_next;
			}
		}
	}
	return caught;
}

#endif
@


1.32
log
@registred -> registered
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.31 2007/07/16 20:20:08 miod Exp $ */
a43 1
#include <machine/pio.h>
@


1.31
log
@Change idle_mask to be made of the cop0 SR bits unshifted and the cop0 IC bits
shifted, instead of the other way around; this shaves a few instructions.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.30 2007/06/20 20:47:33 miod Exp $ */
d92 1
a92 1
 *  in more than it dispatches handling to the code that has registred
d135 1
a135 1
 * was given when the handlers was registred that needs servicing.
@


1.30
log
@Do not hardcode imask[] size when intializing it.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.29 2007/06/20 16:50:39 miod Exp $ */
d235 1
a235 1
	if (!(idle_mask & (SOFT_INT_MASK >> 8)))
d256 1
a256 1
	idle_mask |= (mask | SOFT_INT_MASK) >> 8;
@


1.29
log
@Make sure IPL_CLOCK blocks device interrupts.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.28 2007/06/18 20:24:48 miod Exp $ */
d419 1
a419 1
	for (level = 0; level < 5; level++) {
@


1.28
log
@Disable instruction reordering around cpl assignments.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.27 2007/05/29 18:10:43 miod Exp $ */
d430 1
a430 4
	 */
	imask[IPL_VM] |= imask[IPL_TTY] | imask[IPL_NET] | imask[IPL_BIO];

	/*
a433 1
	imask[IPL_TTY] |= imask[IPL_NET] | imask[IPL_BIO];
d435 3
@


1.27
log
@Use atomic operations to operate on netisr, instead of clearing it at splhigh.
This changes nothing on legacy architectures, but is a bit faster (and simpler)
on the interesting ones.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.26 2007/05/09 19:20:09 miod Exp $ */
d219 1
d221 1
d475 1
d477 1
d522 1
d524 1
@


1.26
log
@Comment out ``generic'' interrupt routines, which are not used at the moment.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.25 2007/05/07 18:42:13 kettenis Exp $ */
d202 2
a203 2
		int isr = netisr;
		netisr = 0;
d205 3
d210 1
@


1.25
log
@Move sgo to __HAVE_CPUINFO.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.24 2007/03/23 21:07:38 miod Exp $ */
d151 1
a151 1
	 *  If this was a disable and the pipleine had advanced long
d186 1
a186 6
#if 0
if ((pending & cause & ~(SOFT_INT_MASK_1|SOFT_INT_MASK_0)) != 0) {
printf("Unhandled interrupt %x:%x\n", cause, pending);
//Debugger();
}
#endif
d209 1
a209 1
#ifdef NOTYET
d285 2
d519 2
d548 2
d583 2
@


1.24
log
@Real atomic_{set,clear}bits_int implementation, and replace similar
{set,clr}_ipending with the above routines.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.23 2007/03/15 10:22:29 art Exp $ */
d280 1
a280 1
	curpriority = p->p_priority = p->p_usrpri;
@


1.23
log
@Since p_flag is often manipulated in interrupts and without biglock
it's a good idea to use atomic.h operations on it. This mechanic
change updates all bit operations on p_flag to atomic_{set,clear}bits_int.

Only exception is that P_OWEUPC is set by MI code before calling
need_proftick and it's automatically cleared by ADDUPC. There's
no reason for MD handling of that flag since everyone handles it the
same way.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.22 2006/12/24 20:30:35 miod Exp $ */
d49 1
d202 1
a202 1
		clr_ipending(SINT_CLOCKMASK);
d209 1
a209 1
		clr_ipending(SINT_NETMASK);
d216 1
a216 1
		clr_ipending(SINT_TTYMASK);
d484 1
a484 1
	clr_ipending(hwpend);
d499 1
a499 1
		clr_ipending(SINT_CLOCKMASK);
d505 1
a505 1
		clr_ipending(SINT_NETMASK);
d512 1
a512 1
		clr_ipending(SINT_TTYMASK);
d561 1
a561 1
	set_ipending((pending >> 8) & cpl);
@


1.22
log
@Define PROC_PC. Then, since profiling information is being reported in
statclock(), do not bother doing this in userret() anymore. As a result,
userret() does not need its pc and ticks arguments, simplify.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.21 2006/05/11 19:57:45 miod Exp $ */
a269 1
		p->p_flag &= ~P_OWEUPC;
@


1.21
log
@One more (!foo & BAR), tested deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.20 2006/03/04 19:33:21 miod Exp $ */
d127 2
a128 2
void interrupt (struct trap_frame *);
void softintr (void);
a258 1
 * This is very similar to the tail of trap().
d267 1
a267 4
	/* take pending signals */
	while ((sig = CURSIG(p)) != 0)
		postsig(sig);
	p->p_priority = p->p_usrpri;
d273 1
a273 4
	if (want_resched) {
		/*	
		 * We're being preempted.
		 */
d275 6
a280 4
		while ((sig = CURSIG(p)) != 0)
			postsig(sig);
	}
	curpriority = p->p_priority;
a281 1

@


1.20
log
@Typos grab bag of the month, eyeballed by jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.19 2005/12/20 06:57:52 miod Exp $ */
d233 1
a233 1
	if (!idle_mask & (SOFT_INT_MASK >> 8))
@


1.19
log
@Trim include files list.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.18 2005/08/14 11:02:30 miod Exp $ */
d84 1
a84 1
 *  capabilites. How these are handeled differs from implementation
d99 1
a99 1
 *  a masked interrupt will be taken and validiated in the various
d116 1
a116 1
 *  to it's status/mask bit in the cause/sr register shifted right eight
d121 2
a122 2
 *  interrupt vontrol register. However support for an external masking
 *  register is provided but will case a slightly higher overhead when
d131 1
a131 1
 * Handle an interrupt. Both kernel and user mode is handeled here.
@


1.18
log
@catched->caught
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.17 2005/07/20 21:55:50 miod Exp $ */
a58 3
#include "atm.h"
#include "bridge.h"
#include "ppp.h"
@


1.17
log
@Always do the netisr_dispatch dance, rather than trying to be smart and use a
three-line #if construct around it, which is wrong since the addition of
kernel pppoe and bluetooth code.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.16 2005/07/18 02:43:25 fgsch Exp $ */
d565 1
a565 1
	intrmask_t catched, vm;
d568 1
a568 1
	catched = 0;
d582 1
a582 1
					catched |= vm;
d589 1
a589 1
	return catched;
@


1.16
log
@remove trailing newline in panic(9); ok millert@@ and deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.15 2005/06/07 02:29:30 henning Exp $ */
a206 3
#if defined(INET) || defined(INET6) || defined(NETATALK) || defined(IMP) || \
    defined(IPX) || defined(NS) || NATM > 0 || \
    NPPP > 0 || NBRIDGE > 0
a214 1
#endif
a330 1
extern int cold;
a510 3
#if defined(INET) || defined(INET6) || defined(NETATALK) || defined(IMP) || \
    defined(IPX) || defined(NS) || NATM > 0 || \
    NPPP > 0 || NBRIDGE > 0
a517 1
#endif
@


1.15
log
@CCITT about to bite the dust, remove special casing in archs and drivers
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.14 2005/05/30 12:51:13 art Exp $ */
d345 1
a345 1
		panic("intr_establish: illegal irq %d\n", irq);
@


1.14
log
@Just use preempt(NULL) like every other architecture in this code path,
don't roll our own.

ok miod@@ pefo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.13 2005/05/29 03:20:40 deraadt Exp $ */
d208 1
a208 1
    defined(IPX) || defined(NS) || defined(CCITT) || NATM > 0 || \
d517 1
a517 1
    defined(IPX) || defined(NS) || defined(CCITT) || NATM > 0 || \
@


1.13
log
@sched work by niklas and art backed out; causes panics
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.11 2005/01/31 21:35:50 grange Exp $ */
d285 2
a286 9
		int s;

		/*
		 * Since we are curproc, clock will normally just change
		 * our priority without moving us from one queue to another
		 * (since the running process is not on a queue.)
		 * If that happened after we put ourselves on the run queue
		 * but before we switched, we might not be on the queue
		 * indicated by our priority.
d288 1
a288 5
		s = splstatclock();
		setrunqueue(p);
		p->p_stats->p_ru.ru_nivcsw++;
		mi_switch();
		splx(s);
@


1.12
log
@This patch is mortly art's work and was done *a year* ago.  Art wants to thank
everyone for the prompt review and ok of this work ;-)  Yeah, that includes me
too, or maybe especially me.  I am sorry.

Change the sched_lock to a mutex. This fixes, among other things, the infamous
"telnet localhost &" problem.  The real bug in that case was that the sched_lock
which is by design a non-recursive lock, was recursively acquired, and not
enough releases made us hold the lock in the idle loop, blocking scheduling
on the other processors.  Some of the other processors would hold the biglock though,
which made it impossible for cpu 0 to enter the kernel...  A nice deadlock.
Let me just say debugging this for days just to realize that it was all fixed
in an old diff noone ever ok'd was somewhat of an anti-climax.

This diff also changes splsched to be correct for all our architectures.
@
text
@d285 2
d288 6
a293 1
		 * We're being preempted.
d295 5
a299 1
		preempt(NULL);
@


1.11
log
@Un-__P.

ok pefo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.10 2005/01/18 15:03:38 grange Exp $ */
a284 2
		int s;

d286 1
a286 6
		 * Since we are curproc, clock will normally just change
		 * our priority without moving us from one queue to another
		 * (since the running process is not on a queue.)
		 * If that happened after we put ourselves on the run queue
		 * but before we switched, we might not be on the queue
		 * indicated by our priority.
d288 1
a288 5
		s = splstatclock();
		setrunqueue(p);
		p->p_stats->p_ru.ru_nivcsw++;
		mi_switch();
		splx(s);
@


1.10
log
@Move rm7000.h file from sgi to mips64.

ok pefo@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.9 2004/10/08 07:14:57 grange Exp $ */
d339 1
a339 1
        int (*ih_fun) __P((void *));
@


1.9
log
@Compile netisr code only for network-capable kernels.

ok pefo@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.8 2004/09/27 19:20:49 pefo Exp $ */
d50 1
a50 1
#include <machine/rm7000.h>
@


1.8
log
@Rewrite parts of the interrupt system to achive:

o Remove do_pending code and take a real int instead. The performance
  impact seems to be very low and it simplifies the code considerably.

o Allow interrupt nesting at first level. Run softints with HW ints
  enabled.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.7 2004/09/24 14:22:49 deraadt Exp $ */
d59 4
d207 3
d218 1
d527 3
d537 1
@


1.7
log
@new style interrupt counters; pefo ok
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.6 2004/09/21 05:51:15 miod Exp $ */
d59 3
d75 3
a77 1
void_f *pending_hand;
d144 1
a144 1
	intrmask_t pcpl;
a156 3
	uvmexp.intrs++;
	pcpl = splhigh() ;	/* Turn off all and get current SW mask */

d161 4
a164 7
	pending = trapframe->cause & CR_IPEND;
#ifdef IMASK_EXTERNAL
	pending &= idle_mask << 8;
#else
	ipending |= (pending >> 8) & pcpl;
	pending &= ~(pcpl << 8);
#endif
d167 5
d194 2
a195 1
	splx((trapframe->sr & ~cause & SR_INT_MASK) | SR_INT_ENAB);
d198 12
a209 3
	if (pending & SOFT_INT_MASK_0) {
		clearsoftintr0();
		uvmexp.softs++;
d212 5
a216 3
#ifndef IMASK_EXTERNAL
	trapframe->sr &= ~((pcpl << 8) & SR_INT_MASK);
	trapframe->ic &= ~(pcpl & IC_INT_MASK);
d218 1
a218 1
	splx(pcpl);	/* Process pendings. */
d232 2
d479 1
a479 1
generic_do_pending_int()
a482 1
	intrmask_t pcpl;
d485 1
a485 1
static volatile int processing;
d487 3
a489 2
	/* Don't recurse... */
	if (processing)
d491 1
a493 3
/* XXX interrupt vulnerable when changing ipending */
	pcpl = splhigh();		/* Turn off all */

d497 1
a497 1
	cf.cpl = pcpl;
d499 1
a499 1
	hwpend = ipending & ~pcpl;	/* Do now unmasked pendings */
d501 1
a501 1
	ipending &= ~hwpend;
d515 2
a516 2
	if ((ipending & SINT_CLOCKMASK) & ~pcpl) {
		ipending &= ~SINT_CLOCKMASK;
d519 1
a519 1
	if ((ipending & SINT_NETMASK) & ~pcpl) {
d522 1
a522 1
		ipending &= ~SINT_NETMASK;
d528 2
a529 2
	if ((ipending & SINT_TTYMASK) & ~pcpl) {
		ipending &= ~SINT_TTYMASK;
d534 2
a535 2
	cpl = pcpl;		/* Don't use splx... we are here already! */
	updateimask(pcpl);	/* Update CPU mask ins SR register */
d539 6
d578 5
a611 4
volatile intrmask_t cpl;
volatile intrmask_t ipending, astpending;

intrmask_t imask[NIPLS];
@


1.6
log
@Nuke commons.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.5 2004/09/20 10:29:57 pefo Exp $ */
a380 1
	ih->ih_count = 0;
d385 2
d494 1
a494 1
				ih->ih_count++;
d564 1
a564 1
					ih->ih_count++;
@


1.5
log
@Add support for R10K cpu class
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.4 2004/08/10 20:15:47 deraadt Exp $ */
d59 15
d584 4
@


1.4
log
@spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.3 2004/08/10 08:07:35 mickey Exp $ */
d556 14
@


1.3
log
@use generic net/netisr_dispatch.h; pefo@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.2 2004/08/09 14:57:26 pefo Exp $ */
d5 1
a5 1
 * 
d292 1
a292 1
void *   
d456 1
a456 1
		return;	
@


1.2
log
@Big cleanup. Removed some unused obsolete stuff and fixed copyrights
on some files. Arcbios support is now in, thus detects memorysize and cpu
clock frequency.
@
text
@d1 1
a1 1
/*	$OpenBSD: interrupt.c,v 1.1 2004/08/06 20:56:03 pefo Exp $ */
d491 2
a492 49
#ifdef  INET
#include "ether.h"
		if (NETHER > 0 && isr & (1 << NETISR_ARP)) {
			arpintr();
		}

		if (isr & (1 << NETISR_IP)) {
			ipintr();
		}
#endif
#ifdef INET6
		if (isr & (1 << NETISR_IPV6)) {
			ip6intr();
		}
#endif
#ifdef NETATALK
		if (isr & (1 << NETISR_ATALK)) {
			atintr();
		}
#endif
#ifdef  IMP
		if (isr & (1 << NETISR_IMP)) {
			impintr();
		}
#endif
#ifdef  NS
		if (isr & (1 << NETISR_NS)) {
			nsintr();
		}
#endif
#ifdef  ISO
		if (isr & (1 << NETISR_ISO)) {
			clnlintr();
		}
#endif
#ifdef  CCITT
		if (isr & (1 << NETISR_CCITT)) {
			ccittintr();
		}
#endif
#include "ppp.h"
		if (NPPP > 0 && isr & (1 << NETISR_PPP)) {
			pppintr();
		}

#include "bridge.h"
		if (NBRIDGE > 0 && isr & (1 << NETISR_BRIDGE)) {
			bridgeintr();
		}
@


1.1
log
@initial mips64
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
a13 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Opsycon AB, Sweden.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@

