head	1.7;
access;
symbols
	OPENBSD_6_1:1.7.0.8
	OPENBSD_6_1_BASE:1.7
	OPENBSD_6_0:1.7.0.4
	OPENBSD_6_0_BASE:1.7
	OPENBSD_5_9:1.7.0.2
	OPENBSD_5_9_BASE:1.7
	OPENBSD_5_8:1.6.0.8
	OPENBSD_5_8_BASE:1.6
	OPENBSD_5_7:1.6.0.2
	OPENBSD_5_7_BASE:1.6
	OPENBSD_5_6:1.6.0.4
	OPENBSD_5_6_BASE:1.6
	OPENBSD_5_5:1.3.0.10
	OPENBSD_5_5_BASE:1.3
	OPENBSD_5_4:1.3.0.6
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.4
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.3.0.2
	OPENBSD_5_2_BASE:1.3;
locks; strict;
comment	@ * @;


1.7
date	2016.01.05.05.27.54;	author visa;	state Exp;
branches;
next	1.6;
commitid	916i76I5mjNtTg33;

1.6
date	2014.03.31.20.21.19;	author miod;	state Exp;
branches;
next	1.5;

1.5
date	2014.03.29.18.09.30;	author guenther;	state Exp;
branches;
next	1.4;

1.4
date	2014.03.09.10.12.17;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2012.06.24.20.22.49;	author miod;	state Exp;
branches;
next	1.2;

1.2
date	2012.06.24.16.26.04;	author miod;	state Exp;
branches;
next	1.1;

1.1
date	2012.06.23.21.56.06;	author miod;	state Exp;
branches;
next	;


desc
@@


1.7
log
@Some implementations of HitSyncDCache() call pmap_extract() for va->pa
conversion. Because pmap_extract() acquires the PTE mutex, a "locking
against myself" panic is triggered if the cache routine gets called in
a context where the mutex is already held.

In the pmap, all calls to HitSyncDCache() are for a whole page. Add a
new cache routine, HitSyncDCachePage(), which gets both the va and the
pa of a page. This removes the need of the va->pa conversion. The new
routine has the same signature as SyncDCachePage(), allowing reuse of
the same routine for cache implementations that do not need differences
between "Hit" and non-"Hit" routines.

With the diff, POWER Indigo2 R8000 boots multiuser again. Tested on sgi
GENERIC-IP27.MP and octeon GENERIC.MP, too.

Diff from miod@@, ok kettenis@@
@
text
@/*	$OpenBSD: cache_r10k.c,v 1.6 2014/03/31 20:21:19 miod Exp $	*/

/*
 * Copyright (c) 2012 Miodrag Vallat.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * Cache handling code for R10000 processor family (R10000, R12000,
 * R14000, R16000).
 *
 * R10000 caches are :
 * - L1 I$ is 2-way, VIPT, 64 bytes/line, 32KB total
 * - L1 D$ is 2-way, VIPT, write-back, 32 bytes/line, 32KB total
 * - L2 is 2-way, PIPT, write-back, 64 or 128 bytes/line, 512KB up to 16MB
 */

#include <sys/param.h>
#include <sys/systm.h>

#include <mips64/cache.h>
#include <machine/cpu.h>

#include <uvm/uvm_extern.h>

#define	IndexInvalidate_I	0x00
#define	IndexWBInvalidate_D	0x01
#define	IndexWBInvalidate_S	0x03

#define	HitInvalidate_D		0x11
#define	HitInvalidate_S		0x13

#define	CacheBarrier		0x14

#define	HitWBInvalidate_D	0x15
#define	HitWBInvalidate_S	0x17

#define	cache(op,set,addr) \
    __asm__ volatile \
      ("cache %0, %1(%2)" :: "i"(op), "i"(set), "r"(addr) : "memory")

static __inline__ void	mips10k_hitinv_primary(vaddr_t, vsize_t);
static __inline__ void	mips10k_hitinv_secondary(vaddr_t, vsize_t, vsize_t);
static __inline__ void	mips10k_hitwbinv_primary(vaddr_t, vsize_t);
static __inline__ void	mips10k_hitwbinv_secondary(vaddr_t, vsize_t, vsize_t);

#define	R10K_L1I_LINE	64UL
#define	R10K_L1D_LINE	32UL

void
Mips10k_ConfigCache(struct cpu_info *ci)
{
	uint32_t cfg, valias_mask;

	cfg = cp0_get_config();

	ci->ci_l1inst.size = (1 << 12) << ((cfg >> 29) & 0x07);	/* IC */
	ci->ci_l1inst.linesize = R10K_L1I_LINE;
	ci->ci_l1inst.setsize = ci->ci_l1inst.size / 2;
	ci->ci_l1inst.sets = 2;

	ci->ci_l1data.size = (1 << 12) << ((cfg >> 26) & 0x07);	/* DC */
	ci->ci_l1data.linesize = R10K_L1D_LINE;
	ci->ci_l1data.setsize = ci->ci_l1data.size / 2;
	ci->ci_l1data.sets = 2;

	ci->ci_l2.size = (1 << 19) << ((cfg >> 16) & 0x07);
	ci->ci_l2.linesize = (cfg & (1 << 13)) ? 128 : 64;
	ci->ci_l2.setsize = ci->ci_l2.size / 2;
	ci->ci_l2.sets = 2;

	memset(&ci->ci_l3, 0, sizeof(struct cache_info));

	valias_mask = (max(ci->ci_l1inst.setsize, ci->ci_l1data.setsize) - 1) &
	    ~PAGE_MASK;

	if (valias_mask != 0) {
		valias_mask |= PAGE_MASK;
#ifdef MULTIPROCESSOR
		if (valias_mask > cache_valias_mask) {
#endif
			cache_valias_mask = valias_mask;
			pmap_prefer_mask = valias_mask;
#ifdef MULTIPROCESSOR
		}
#endif
	}

	ci->ci_SyncCache = Mips10k_SyncCache;
	ci->ci_InvalidateICache = Mips10k_InvalidateICache;
	ci->ci_InvalidateICachePage = Mips10k_InvalidateICachePage;
	ci->ci_SyncICache = Mips10k_SyncICache;
	ci->ci_SyncDCachePage = Mips10k_SyncDCachePage;
	ci->ci_HitSyncDCachePage = Mips10k_HitSyncDCachePage;
	ci->ci_HitSyncDCache = Mips10k_HitSyncDCache;
	ci->ci_HitInvalidateDCache = Mips10k_HitInvalidateDCache;
	ci->ci_IOSyncDCache = Mips10k_IOSyncDCache;
}

static __inline__ void
mips10k_hitwbinv_primary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWBInvalidate_D, 0, va);
		va += R10K_L1D_LINE;
	}
}

static __inline__ void
mips10k_hitwbinv_secondary(vaddr_t va, vsize_t sz, vsize_t line)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitWBInvalidate_S, 0, va);
		va += line;
	}
}

static __inline__ void
mips10k_hitinv_primary(vaddr_t va, vsize_t sz)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitInvalidate_D, 0, va);
		va += R10K_L1D_LINE;
	}
}

static __inline__ void
mips10k_hitinv_secondary(vaddr_t va, vsize_t sz, vsize_t line)
{
	vaddr_t eva;

	eva = va + sz;
	while (va != eva) {
		cache(HitInvalidate_S, 0, va);
		va += line;
	}
}

/*
 * Writeback and invalidate all caches.
 */
void
Mips10k_SyncCache(struct cpu_info *ci)
{
	vaddr_t sva, eva;

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = sva + ci->ci_l1inst.setsize;
	while (sva != eva) {
		cache(IndexInvalidate_I, 0, sva);
		cache(IndexInvalidate_I, 1, sva);
		sva += R10K_L1I_LINE;
	}

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = sva + ci->ci_l1data.setsize;
	while (sva != eva) {
		cache(IndexWBInvalidate_D, 0, sva);
		cache(IndexWBInvalidate_D, 1, sva);
		sva += R10K_L1D_LINE;
	}

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	eva = sva + ci->ci_l2.setsize;
	while (sva != eva) {
		cache(IndexWBInvalidate_S, 0, sva);
		cache(IndexWBInvalidate_S, 1, sva);
		sva += ci->ci_l2.linesize;
	}
}

/*
 * Invalidate I$ for the given range.
 */
void
Mips10k_InvalidateICache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va, sva, eva;
	vsize_t sz;

	/* extend the range to integral cache lines */
	va = _va & ~(R10K_L1I_LINE - 1);
	sz = ((_va + _sz + R10K_L1I_LINE - 1) & ~(R10K_L1I_LINE - 1)) - va;

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	/* keep only the index bits */
	sva |= va & ((1UL << 14) - 1);
	eva = sva + sz;
	while (sva != eva) {
		cache(IndexInvalidate_I, 0, sva);
		cache(IndexInvalidate_I, 1, sva);
		sva += R10K_L1I_LINE;
	}
}

/*
 * Register a given page for I$ invalidation.
 */
void
Mips10k_InvalidateICachePage(struct cpu_info *ci, vaddr_t va)
{
#if PAGE_SHIFT < 14
	/*
	 * Mark the particular page index bits as needing to be flushed.
	 */
	ci->ci_cachepending_l1i |=
	    1ULL << ((va & ((1UL << 14) - 1)) >> PAGE_SHIFT);
#else
	/*
	 * Since the page size matches the I$ set size, all we need to do
	 * here is remember there are postponed flushes.
	 */
	ci->ci_cachepending_l1i = 1;
#endif
}

/*
 * Perform postponed I$ invalidation.
 */
void
Mips10k_SyncICache(struct cpu_info *ci)
{
	vaddr_t sva, eva;

	if (ci->ci_cachepending_l1i != 0) {
#if PAGE_SHIFT < 14
		/*
		 * Iterate on all pending page index bits.
		 */
		uint64_t idx;

		for (idx = 0; ci->ci_cachepending_l1i != 0; idx++) {
			if ((ci->ci_cachepending_l1i & (1ULL << idx)) == 0)
				continue;

			sva = PHYS_TO_XKPHYS(idx << PAGE_SHIFT, CCA_CACHED);
			eva = sva + PAGE_SIZE;
			while (sva != eva) {
				cache(IndexInvalidate_I, 0, sva);
				cache(IndexInvalidate_I, 1, sva);
				sva += R10K_L1I_LINE;
			}

			ci->ci_cachepending_l1i &= ~(1ULL << idx);
		}
#else
		/* inline Mips10k_InvalidateICache(ci, 0, PAGE_SIZE); */
		sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
		eva = sva + PAGE_SIZE;
		while (sva != eva) {
			cache(IndexInvalidate_I, 0, sva);
			cache(IndexInvalidate_I, 1, sva);
			sva += R10K_L1I_LINE;
		}
		ci->ci_cachepending_l1i = 0;
#endif
	}
}

/*
 * Writeback D$ for the given page.
 */
void
Mips10k_SyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
{
#if PAGE_SHIFT < 14
	vaddr_t sva, eva;

	sva = PHYS_TO_XKPHYS(0, CCA_CACHED);
	/* keep only the index bits */
	sva += va & ((1UL << 14) - 1);
	eva = sva + PAGE_SIZE;
	while (sva != eva) {
		cache(IndexWBInvalidate_D, 0 + 0 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 0 + 1 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 0 + 2 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 0 + 3 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 1 + 0 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 1 + 1 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 1 + 2 * R10K_L1D_LINE, sva);
		cache(IndexWBInvalidate_D, 1 + 3 * R10K_L1D_LINE, sva);
		sva += 4 * R10K_L1D_LINE;
	}
#else
	/*
	 * Since the page size is not smaller than the L1 virtual index space,
	 * it is cheaper to use Hit operations to make sure we do not affect
	 * innocent cache lines.
	 */
	mips10k_hitwbinv_primary(PHYS_TO_XKPHYS(pa, CCA_CACHED), PAGE_SIZE);
#endif
}

/*
 * Writeback D$ for the given page, which is expected to be currently
 * mapped, allowing the use of `Hit' operations. This is less aggressive
 * than using `Index' operations.
 */

void
Mips10k_HitSyncDCachePage(struct cpu_info *ci, vaddr_t va, paddr_t pa)
{
	mips10k_hitwbinv_primary(va, PAGE_SIZE);
}

/*
 * Writeback D$ for the given range. Range is expected to be currently
 * mapped, allowing the use of `Hit' operations. This is less aggressive
 * than using `Index' operations.
 */

void
Mips10k_HitSyncDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va;
	vsize_t sz;

	/* extend the range to integral cache lines */
	va = _va & ~(R10K_L1D_LINE - 1);
	sz = ((_va + _sz + R10K_L1D_LINE - 1) & ~(R10K_L1D_LINE - 1)) - va;
	mips10k_hitwbinv_primary(va, sz);
}

/*
 * Invalidate D$ for the given range. Range is expected to be currently
 * mapped, allowing the use of `Hit' operations. This is less aggressive
 * than using `Index' operations.
 */

void
Mips10k_HitInvalidateDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz)
{
	vaddr_t va;
	vsize_t sz;

	/* extend the range to integral cache lines */
	va = _va & ~(R10K_L1D_LINE - 1);
	sz = ((_va + _sz + R10K_L1D_LINE - 1) & ~(R10K_L1D_LINE - 1)) - va;
	mips10k_hitinv_primary(va, sz);
}

/*
 * Backend for bus_dmamap_sync(). Enforce coherency of the given range
 * by performing the necessary cache writeback and/or invalidate
 * operations.
 */
void
Mips10k_IOSyncDCache(struct cpu_info *ci, vaddr_t _va, size_t _sz, int how)
{
	vaddr_t va;
	vsize_t sz;
	vsize_t line;
	int partial_start, partial_end;

	line = ci->ci_l2.linesize;
	/* extend the range to integral cache lines */
	if (line == 64) {
		va = _va & ~(64UL - 1);
		sz = ((_va + _sz + 64UL - 1) & ~(64UL - 1)) - va;
	} else {
		va = _va & ~(128UL - 1);
		sz = ((_va + _sz + 128UL - 1) & ~(128UL - 1)) - va;
	}

	switch (how) {
	case CACHE_SYNC_R:
		/* writeback partial cachelines */
		if (((_va | _sz) & (line - 1)) != 0) {
			partial_start = va != _va;
			partial_end = va + sz != _va + _sz;
		} else {
			partial_start = partial_end = 0;
		}
		if (partial_start) {
			cache(HitWBInvalidate_S, 0, va);
			va += line;
			sz -= line;
		}
		if (sz != 0 && partial_end) {
			cache(HitWBInvalidate_S, 0, va + sz - line);
			sz -= line;
		}
		if (sz != 0) {
			if (line == 64)
				mips10k_hitinv_secondary(va, sz, 64UL);
			else
				mips10k_hitinv_secondary(va, sz, 128UL);
		}
		break;
	case CACHE_SYNC_X:
	case CACHE_SYNC_W:
		if (line == 64)
			mips10k_hitwbinv_secondary(va, sz, 64UL);
		else
			mips10k_hitwbinv_secondary(va, sz, 128UL);
		break;
	}
}
@


1.6
log
@Due the virtually indexed nature of the L1 instruction cache on most mips
processors, every time a new text page is mapped in a pmap, the L1 I$ is
flushed for the va spanned by this page.

Since we map pages of our binaries upon demand, as they get faulted in, but
uvm_fault() tries to map the few neighbour pages, this can end up in a
bunch of pmap_enter() calls in a row, for executable mappings. If the L1
I$ is small enough, this can cause the whole L1 I$ cache to be flushed
several times.

Change pmap_enter() to postpone these flushes by only registering the
pending flushes, and have pmap_update() perform them. The cpu-specific
cache code can then optimize this to avoid unnecessary operations.

Tested on R4000SC, R4600SC, R5000SC, RM7000, R10000 with 4KB and 16KB
page sizes (coherent and non-coherent designs), and Loongson 2F by mikeb@@ and
me. Should not affect anything on Octeon since there is no way to flush a
subset of I$ anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.c,v 1.5 2014/03/29 18:09:30 guenther Exp $	*/
d105 1
d312 12
@


1.5
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.c,v 1.4 2014/03/09 10:12:17 miod Exp $	*/
d102 2
d212 64
@


1.4
log
@Rework the per-cpu cache information. Use a common struct to store the line
size, the number of sets, and the total size (and the set size, for convenience)
per cache (I$, D$, L2, L3).
This allows cpu.c to print the number of ways (sets) of L2 and L3 caches from
the cache information, rather than hardcoding this from the processor type.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.c,v 1.3 2012/06/24 20:22:49 miod Exp $	*/
d50 1
a50 1
    __asm__ __volatile__ \
@


1.3
log
@Get the L2 line size from the configuration register instead of hardcoding the
smallest possible size; and on kernels with 16KB pages, where no virtual
aliasing happens, use Hit operations on the pa instead of index operations
on the va for all sets in the SyncDCachePage routine.

Tested by mikeb@@ on IP32 and myself on IP27/28/30/35
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.c,v 1.2 2012/06/24 16:26:04 miod Exp $	*/
d68 14
a81 2
	ci->ci_l1instcacheline = R10K_L1I_LINE;
	ci->ci_l1instcachesize = (1 << 12) << ((cfg >> 29) & 0x07);	/* IC */
d83 1
a83 2
	ci->ci_l1datacacheline = R10K_L1D_LINE;
	ci->ci_l1datacachesize = (1 << 12) << ((cfg >> 26) & 0x07);	/* DC */
d85 1
a85 10
	ci->ci_l2line = (cfg & (1 << 13)) ? 128 : 64;
	ci->ci_l2size = (1 << 19) << ((cfg >> 16) & 0x07);

	ci->ci_l3size = 0;

	ci->ci_cacheways = 2;
	ci->ci_l1instcacheset = ci->ci_l1instcachesize / 2;
	ci->ci_l1datacacheset = ci->ci_l1datacachesize / 2;

	valias_mask = (max(ci->ci_l1instcacheset, ci->ci_l1datacacheset) - 1) &
d165 1
a165 1
	eva = sva + ci->ci_l1instcacheset;
d173 1
a173 1
	eva = sva + ci->ci_l1datacacheset;
d181 1
a181 1
	eva = sva + ci->ci_l2size / 2;
d185 1
a185 1
		sva += ci->ci_l2line;
d296 1
a296 1
	line = ci->ci_l2line;
@


1.2
log
@Add cache operation functions pointers to struct cpu_info; the various
cache lines and sizes are already there, after all.

The ConfigCache cache routine is responsible for filling these function
pointers; cache routine invocation macros are updated to use the cpu_info
fields, but may still be overriden in <machine/cpu.h> on platforms where
only one set of cache routines is used.
@
text
@d1 1
a1 1
/*	$OpenBSD: cache_r10k.c,v 1.1 2012/06/23 21:56:06 miod Exp $	*/
d54 1
a54 1
static __inline__ void	mips10k_hitinv_secondary(vaddr_t, vsize_t);
d56 1
a56 1
static __inline__ void	mips10k_hitwbinv_secondary(vaddr_t, vsize_t);
d74 1
a74 1
	ci->ci_l2line = 64;	/* XXX could be 128 */
d119 1
a119 1
mips10k_hitwbinv_secondary(vaddr_t va, vsize_t sz)
d126 1
a126 1
		va += 64UL;
d143 1
a143 1
mips10k_hitinv_secondary(vaddr_t va, vsize_t sz)
d150 1
a150 1
		va += 64UL;
d183 1
a183 1
		sva += 64UL;
d217 1
d235 8
d291 1
d294 9
a302 3
	/* extend the range to integral L2 cache lines */
	va = _va & ~(64UL - 1);
	sz = ((_va + _sz + 64UL - 1) & ~(64UL - 1)) - va;
d307 1
a307 1
		if (((_va | _sz) & (64UL - 1)) != 0) {
d315 2
a316 2
			va += 64UL;
			sz -= 64UL;
d319 8
a326 2
			cache(HitWBInvalidate_S, 0, va + sz - 64UL);
			sz -= 64UL;
a327 1
		mips10k_hitinv_secondary(va, sz);
d331 5
a335 1
		mips10k_hitwbinv_secondary(va, sz);
@


1.1
log
@Replace R5000 and R10000 family assembly cache routines with C equivalents,
which will be easier to maintain on the long run. Be sure to rm cache_r*.d in
your kernel compile directories after updating.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d97 7
@

