head	1.46;
access;
symbols
	OPENBSD_6_0:1.44.0.2
	OPENBSD_6_0_BASE:1.44
	OPENBSD_5_9:1.42.0.2
	OPENBSD_5_9_BASE:1.42
	OPENBSD_5_8:1.38.0.6
	OPENBSD_5_8_BASE:1.38
	OPENBSD_5_7:1.38.0.2
	OPENBSD_5_7_BASE:1.38
	OPENBSD_5_6:1.36.0.4
	OPENBSD_5_6_BASE:1.36
	OPENBSD_5_5:1.31.0.4
	OPENBSD_5_5_BASE:1.31
	OPENBSD_5_4:1.29.0.2
	OPENBSD_5_4_BASE:1.29
	OPENBSD_5_3:1.28.0.4
	OPENBSD_5_3_BASE:1.28
	OPENBSD_5_2:1.28.0.2
	OPENBSD_5_2_BASE:1.28
	OPENBSD_5_1_BASE:1.26
	OPENBSD_5_1:1.26.0.4
	OPENBSD_5_0:1.26.0.2
	OPENBSD_5_0_BASE:1.26
	OPENBSD_4_9:1.24.0.2
	OPENBSD_4_9_BASE:1.24
	OPENBSD_4_8:1.20.0.4
	OPENBSD_4_8_BASE:1.20
	OPENBSD_4_7:1.20.0.2
	OPENBSD_4_7_BASE:1.20
	OPENBSD_4_6:1.14.0.10
	OPENBSD_4_6_BASE:1.14
	OPENBSD_4_5:1.14.0.6
	OPENBSD_4_5_BASE:1.14
	OPENBSD_4_4:1.14.0.4
	OPENBSD_4_4_BASE:1.14
	OPENBSD_4_3:1.14.0.2
	OPENBSD_4_3_BASE:1.14
	OPENBSD_4_2:1.12.0.2
	OPENBSD_4_2_BASE:1.12
	OPENBSD_4_1:1.10.0.4
	OPENBSD_4_1_BASE:1.10
	OPENBSD_4_0:1.10.0.2
	OPENBSD_4_0_BASE:1.10
	OPENBSD_3_9:1.8.0.2
	OPENBSD_3_9_BASE:1.8
	OPENBSD_3_8:1.7.0.2
	OPENBSD_3_8_BASE:1.7
	OPENBSD_3_7:1.6.0.2
	OPENBSD_3_7_BASE:1.6
	OPENBSD_3_6:1.3.0.2
	OPENBSD_3_6_BASE:1.3;
locks; strict;
comment	@ * @;


1.46
date	2016.12.22.15.33.36;	author visa;	state Exp;
branches;
next	1.45;
commitid	sw48d58czeutOmw2;

1.45
date	2016.12.06.16.27.33;	author visa;	state Exp;
branches;
next	1.44;
commitid	pOY8Ir82l05sgYAt;

1.44
date	2016.05.11.15.50.29;	author visa;	state Exp;
branches;
next	1.43;
commitid	y3u3BwlBdCPiPv83;

1.43
date	2016.04.24.04.25.03;	author visa;	state Exp;
branches;
next	1.42;
commitid	4SPr8onH3V5nhYod;

1.42
date	2016.02.01.16.18.30;	author visa;	state Exp;
branches;
next	1.41;
commitid	glOmjpYRBtIou7SI;

1.41
date	2016.02.01.16.15.18;	author visa;	state Exp;
branches;
next	1.40;
commitid	fDCG9fyhqhLX230e;

1.40
date	2016.01.10.10.22.56;	author visa;	state Exp;
branches;
next	1.39;
commitid	DXqb5NK6W32O0C6j;

1.39
date	2015.12.31.04.25.51;	author visa;	state Exp;
branches;
next	1.38;
commitid	dVASB3mipffYF1KI;

1.38
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.37;
commitid	eahBabNpxnDWKzqJ;

1.37
date	2014.12.17.15.05.52;	author deraadt;	state Exp;
branches;
next	1.36;
commitid	sW5BvVrBde2bHGC9;

1.36
date	2014.04.03.18.29.37;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2014.03.31.20.21.19;	author miod;	state Exp;
branches;
next	1.34;

1.34
date	2014.03.22.00.01.04;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2014.03.21.21.49.45;	author miod;	state Exp;
branches;
next	1.32;

1.32
date	2014.03.10.21.17.58;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2014.02.08.09.34.04;	author miod;	state Exp;
branches;
next	1.30;

1.30
date	2014.01.30.18.16.41;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2013.05.29.20.36.12;	author pirofti;	state Exp;
branches;
next	1.28;

1.28
date	2012.05.10.21.12.26;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2012.04.06.20.11.18;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2011.04.28.20.46.30;	author ariane;	state Exp;
branches;
next	1.25;

1.25
date	2011.03.23.16.54.36;	author pirofti;	state Exp;
branches;
next	1.24;

1.24
date	2010.12.26.15.40.59;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2010.12.06.20.57.17;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2010.11.28.20.30.51;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2010.11.24.20.59.17;	author miod;	state Exp;
branches;
next	1.20;

1.20
date	2010.02.02.02.49.57;	author syuu;	state Exp;
branches;
next	1.19;

1.19
date	2010.01.05.06.44.58;	author syuu;	state Exp;
branches;
next	1.18;

1.18
date	2009.12.30.01.17.59;	author syuu;	state Exp;
branches;
next	1.17;

1.17
date	2009.12.28.06.55.27;	author syuu;	state Exp;
branches;
next	1.16;

1.16
date	2009.12.07.18.58.32;	author miod;	state Exp;
branches;
next	1.15;

1.15
date	2009.11.18.20.58.50;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2007.10.18.04.32.09;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2007.04.27.18.14.11;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2007.04.14.14.52.37;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2006.06.20.06.26.57;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2006.05.29.21.50.08;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	2005.11.23.11.22.10;	author mickey;	state Exp;
branches;
next	1.7;

1.7
date	2005.08.07.07.29.44;	author miod;	state Exp;
branches;
next	1.6;

1.6
date	2004.09.27.17.40.24;	author pefo;	state Exp;
branches;
next	1.5;

1.5
date	2004.09.23.08.42.38;	author pefo;	state Exp;
branches;
next	1.4;

1.4
date	2004.09.17.19.19.05;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2004.08.10.20.28.13;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2004.08.06.22.39.13;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	2004.08.06.20.56.02;	author pefo;	state Exp;
branches;
next	;


desc
@@


1.46
log
@Extend the size of user virtual address space from 2GB to 1TB on mips64
by adding another level to page directories. This improves ASLR and
complements W^X added earlier on some systems, giving a notable update
to the architecture's security. Besides, there is now more room for
running tasks that hog memory.

Testing help from deraadt@@ and fcambus@@.
Platforms tested: loongson, octeon, sgi/IP27 and sgi/IP30
(IP30 also with 4KB pages).
@
text
@/*      $OpenBSD: pmap.h,v 1.45 2016/12/06 16:27:33 visa Exp $ */

/*
 * Copyright (c) 1987 Carnegie-Mellon University
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Ralph Campbell.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	from: @@(#)pmap.h	8.1 (Berkeley) 6/10/93
 */

#ifndef	_MIPS64_PMAP_H_
#define	_MIPS64_PMAP_H_

#include <sys/mutex.h>

#ifdef	_KERNEL

#include <machine/pte.h>

/*
 * The user address space is currently limited to 1TB (0x0 - 0x10000000000).
 *
 * The user address space is mapped using a two level structure where
 * the virtual addresses bits are split in three groups:
 *   segment:directory:page:offset
 * where:
 * - offset are the in-page offsets (PAGE_SHIFT bits)
 * - page are the third level page table index
 *   (PMAP_PGSHIFT - Log2(pt_entry_t) bits)
 * - directory are the second level page table (directory) index
 *   (PMAP_PGSHIFT - Log2(void *) bits)
 * - segment are the first level page table (segment) index
 *   (PMAP_PGSHIFT - Log2(void *) bits)
 *
 * This scheme allows Segment, directory and page tables have the same size
 * (1 << PMAP_PGSHIFT bytes, regardless of the pt_entry_t size) to be able to
 * share the same allocator.
 *
 * Note: The kernel doesn't use the same data structures as user programs.
 * All the PTE entries are stored in a single array in Sysmap which is
 * dynamically allocated at boot time.
 */

#if defined(MIPS_PTE64) && PAGE_SHIFT == 12
#error "Cannot use MIPS_PTE64 with 4KB pages."
#endif

/*
 * Size of page table structs (page tables, page directories,
 * and segment table) used by this pmap.
 */

#define	PMAP_PGSHIFT		12
#define	PMAP_PGSIZE		(1UL << PMAP_PGSHIFT)

#define	NPDEPG			(PMAP_PGSIZE / sizeof(void *))
#define	NPTEPG			(PMAP_PGSIZE / sizeof(pt_entry_t))

/*
 * Segment sizes
 */

#define	SEGSHIFT		(PAGE_SHIFT+PMAP_PGSHIFT*2-PTE_LOG-3)
#define	DIRSHIFT		(PAGE_SHIFT+PMAP_PGSHIFT-PTE_LOG)
#define	NBSEG			(1UL << SEGSHIFT)
#define	NBDIR			(1UL << DIRSHIFT)
#define	SEGOFSET		(NBSEG - 1)
#define	DIROFSET		(NBDIR - 1)

#define	mips_trunc_seg(x)	((vaddr_t)(x) & ~SEGOFSET)
#define	mips_trunc_dir(x)	((vaddr_t)(x) & ~DIROFSET)
#define	mips_round_seg(x)	(((vaddr_t)(x) + SEGOFSET) & ~SEGOFSET)
#define	mips_round_dir(x)	(((vaddr_t)(x) + DIROFSET) & ~DIROFSET)
#define	pmap_segmap(m, v)	((m)->pm_segtab->seg_tab[((v) >> SEGSHIFT)])

/* number of segments entries */
#define	PMAP_SEGTABSIZE		(PMAP_PGSIZE / sizeof(void *))

struct segtab {
	pt_entry_t	**seg_tab[PMAP_SEGTABSIZE];
};

struct pmap_asid_info {
	u_int			pma_asid;	/* address space tag */
	u_int			pma_asidgen;	/* TLB PID generation number */
};

/*
 * Machine dependent pmap structure.
 */
typedef struct pmap {
	struct mutex		pm_mtx;		/* pmap lock */
	int			pm_count;	/* pmap reference count */
	struct pmap_statistics	pm_stats;	/* pmap statistics */
	struct segtab		*pm_segtab;	/* pointers to pages of PTEs */
	struct pmap_asid_info	pm_asid[1];	/* ASID information */
} *pmap_t;

/*
 * Compute the sizeof of a pmap structure.  Subtract one because one
 * ASID info structure is already included in the pmap structure itself.
 */
#define	PMAP_SIZEOF(x)							\
	(ALIGN(sizeof(struct pmap) +					\
	       (sizeof(struct pmap_asid_info) * ((x) - 1))))

/* machine-dependent pg_flags */
#define	PGF_UNCACHED	PG_PMAP0	/* Page is explicitely uncached */
#define	PGF_CACHED	PG_PMAP1	/* Page is currently cached */
#define	PGF_ATTR_MOD	PG_PMAP2
#define	PGF_ATTR_REF	PG_PMAP3
#define	PGF_EOP_CHECKED	PG_PMAP4
#define	PGF_EOP_VULN	PG_PMAP5
#define	PGF_PRESERVE	(PGF_ATTR_MOD | PGF_ATTR_REF)

#define	PMAP_NOCACHE	PMAP_MD0

extern	struct pmap *const kernel_pmap_ptr;

#define	pmap_resident_count(pmap)       ((pmap)->pm_stats.resident_count)
#define	pmap_wired_count(pmap)		((pmap)->pm_stats.wired_count)
#define	pmap_kernel()			(kernel_pmap_ptr)

#define	PMAP_STEAL_MEMORY		/* Enable 'stealing' during boot */

#define	PMAP_PREFER(pa, va)		pmap_prefer(pa, va)

extern vaddr_t pmap_prefer_mask;
/* pmap prefer alignment */
#define	PMAP_PREFER_ALIGN()						\
	(pmap_prefer_mask ? pmap_prefer_mask + 1 : 0)
/* pmap prefer offset in alignment */
#define	PMAP_PREFER_OFFSET(of)		((of) & pmap_prefer_mask)

void	pmap_bootstrap(void);
vaddr_t	pmap_prefer(vaddr_t, vaddr_t);
int	pmap_emulate_modify(pmap_t, vaddr_t);
void	pmap_page_cache(vm_page_t, u_int);

#define	pmap_unuse_final(p)		do { /* nothing yet */ } while (0)
#define	pmap_remove_holes(vm)		do { /* nothing */ } while (0)

/*
 * Most R5000 processors (and related families) have a silicon bug preventing
 * the ll/sc (and lld/scd) instructions from honouring the caching mode
 * when accessing XKPHYS addresses.
 *
 * Since pool memory is allocated with pmap_map_direct() if __HAVE_PMAP_DIRECT,
 * and many structures containing fields which will be used with
 * <machine/atomic.h> routines are allocated from pools, __HAVE_PMAP_DIRECT can
 * not be defined on systems which may use flawed processors.
 */
#if !defined(CPU_R5000) && !defined(CPU_RM7000)
#define	__HAVE_PMAP_DIRECT
vaddr_t	pmap_map_direct(vm_page_t);
vm_page_t pmap_unmap_direct(vaddr_t);
#endif

/*
 * MD flags to pmap_enter:
 */

#define	PMAP_PA_MASK	~((paddr_t)PAGE_MASK)

/* Kernel virtual address to page table entry */
#define	kvtopte(va) \
	(Sysmap + (((vaddr_t)(va) - VM_MIN_KERNEL_ADDRESS) >> PAGE_SHIFT))
/* User virtual address to pte page entry */
#define	uvtopte(va)	(((va) >> PAGE_SHIFT) & (NPTEPG -1))
#define	uvtopde(va)	(((va) >> DIRSHIFT) & (NPDEPG - 1))

static inline pt_entry_t *
pmap_pte_lookup(struct pmap *pmap, vaddr_t va)
{
	pt_entry_t **pde, *pte;

	if ((pde = pmap_segmap(pmap, va)) == NULL)
		return NULL;
	if ((pte = pde[uvtopde(va)]) == NULL)
		return NULL;
	return pte + uvtopte(va);
}

extern	pt_entry_t *Sysmap;		/* kernel pte table */
extern	u_int Sysmapsize;		/* number of pte's in Sysmap */

#endif	/* _KERNEL */

#if !defined(_LOCORE)
typedef struct pv_entry {
	struct pv_entry	*pv_next;	/* next pv_entry */
	struct pmap	*pv_pmap;	/* pmap where mapping lies */
	vaddr_t		pv_va;		/* virtual address for mapping */
} *pv_entry_t;

struct vm_page_md {
	struct mutex	pv_mtx;		/* pv list lock */
	struct pv_entry pv_ent;		/* pv list of this seg */
};

#define	VM_MDPAGE_INIT(pg) \
	do { \
		mtx_init(&(pg)->mdpage.pv_mtx, IPL_VM); \
		(pg)->mdpage.pv_ent.pv_next = NULL; \
		(pg)->mdpage.pv_ent.pv_pmap = NULL; \
		(pg)->mdpage.pv_ent.pv_va = 0; \
	} while (0)

#endif	/* !_LOCORE */

#endif	/* !_MIPS64_PMAP_H_ */
@


1.45
log
@Get PTE shift using PTE_LOG.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.44 2016/05/11 15:50:29 visa Exp $ */
d48 1
a48 1
 * The user address space is currently limited to 2Gb (0x0 - 0x80000000).
d52 1
a52 1
 *   segment:page:offset
d55 4
a58 2
 * - page are the second level page table index
 *   (PMAP_L2SHIFT - Log2(pt_entry_t) bits)
d60 1
a60 1
 *   (PMAP_L2SHIFT - Log2(void *) bits)
d62 2
a63 2
 * This scheme allows Segment and page tables have the same size
 * (1 << PMAP_L2SHIFT bytes, regardless of the pt_entry_t size) to be able to
d71 4
d76 2
a77 2
 * Size of second level page structs (page tables, and segment table) used
 * by this pmap.
d80 2
a81 6
#ifdef MIPS_PTE64
#define	PMAP_L2SHIFT		14
#else
#define	PMAP_L2SHIFT		12
#endif
#define	PMAP_L2SIZE		(1UL << PMAP_L2SHIFT)
d83 2
a84 1
#define	NPTEPG			(PMAP_L2SIZE / sizeof(pt_entry_t))
d90 2
a91 1
#define	SEGSHIFT		(PAGE_SHIFT + PMAP_L2SHIFT - PTE_LOG)
d93 1
d95 1
d98 1
d100 1
d104 1
a104 1
#define	PMAP_SEGTABSIZE		(PMAP_L2SIZE / sizeof(void *))
d107 1
a107 1
	pt_entry_t	*seg_tab[PMAP_SEGTABSIZE];
a133 1

a166 1
#define	pmap_collect(x)			do { /* nothing */ } while (0)
d197 13
@


1.44
log
@Another attempt to make the mips64 pmap MP-safe. Now at least
pmap_enter(9), pmap_remove(9) and pmap_page_protect(9) should be
safe to use without the kernel lock.

No objection from deraadt@@
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.43 2016/04/24 04:25:03 visa Exp $ */
d87 1
a87 5
#ifdef MIPS_PTE64
#define	SEGSHIFT		(PAGE_SHIFT + PMAP_L2SHIFT - 3)
#else
#define	SEGSHIFT		(PAGE_SHIFT + PMAP_L2SHIFT - 2)
#endif
@


1.43
log
@Keep pmap_update_{kernel,user}_page() inside pmap.c.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.42 2016/02/01 16:18:30 visa Exp $ */
d41 2
d115 1
d209 1
d215 1
@


1.42
log
@Zap stray pmap_kenter_cache() prototype.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.41 2016/02/01 16:15:18 visa Exp $ */
a163 7

void	pmap_update_user_page(pmap_t, vaddr_t, pt_entry_t);
#ifdef MULTIPROCESSOR
void	pmap_update_kernel_page(vaddr_t, pt_entry_t);
#else
#define	pmap_update_kernel_page(va, entry)	tlb_update(va, entry)
#endif
@


1.41
log
@Move modify bit emulation into pmap.c to gather pmap C code in one place.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.40 2016/01/10 10:22:56 visa Exp $ */
a156 1
void	pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cache);
@


1.40
log
@Back out the MP pmap diff for rework. The code does not work on
non-PMAP_DIRECT systems due to lock recursion.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.39 2015/12/31 04:25:51 visa Exp $ */
a156 1
int	pmap_is_page_ro( pmap_t, vaddr_t, pt_entry_t);
d159 1
a159 1
void	pmap_set_modify(vm_page_t);
@


1.39
log
@Protect mips64 pmap and pvlist structs with a mutex to make pmap
operations MP-safe. Tested on octeon and sgi (IP27, IP30).

Feedback from kettenis@@ long ago
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.38 2015/02/15 21:34:33 miod Exp $ */
a40 2
#include <sys/mutex.h>

a99 14
/*
 * Concurrency control
 *
 * - struct pmap:
 *   - pm_dir_mtx must be held when adding or removing a mapping in the pmap.
 *   - pm_pte_mtx must be held when modifying the page directory or page table
 *     entries. In addition, the lock protects the subfields of field pm_stats.
 *
 * - struct vm_page_md:
 *   - pv_mtx protects the physical-to-virtual list.
 *
 * The order for locking is pm_dir_mtx -> pv_mtx -> pm_pte_mtx.
 */

a112 2
	struct mutex		pm_dir_mtx;	/* page directory lock */
	struct mutex		pm_pte_mtx;	/* page table entry lock */
a214 1
	struct mutex	pv_mtx;		/* pv list lock */
a219 1
		mtx_init(&(pg)->mdpage.pv_mtx, IPL_VM); \
@


1.38
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.37 2014/12/17 15:05:52 deraadt Exp $ */
d41 2
d102 14
d129 2
d233 1
d239 1
@


1.37
log
@pretty easy removal of simplelocks
ok miod (a while back)
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.36 2014/04/03 18:29:37 miod Exp $ */
d165 1
a165 1
#define	pmap_remove_holes(map)		do { /* nothing */ } while (0)
@


1.36
log
@Do not keep the EOP check bits in PGF_PRESERVE.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.35 2014/03/31 20:21:19 miod Exp $ */
a113 1
	simple_lock_data_t	pm_lock;	/* lock on pmap */
@


1.35
log
@Due the virtually indexed nature of the L1 instruction cache on most mips
processors, every time a new text page is mapped in a pmap, the L1 I$ is
flushed for the va spanned by this page.

Since we map pages of our binaries upon demand, as they get faulted in, but
uvm_fault() tries to map the few neighbour pages, this can end up in a
bunch of pmap_enter() calls in a row, for executable mappings. If the L1
I$ is small enough, this can cause the whole L1 I$ cache to be flushed
several times.

Change pmap_enter() to postpone these flushes by only registering the
pending flushes, and have pmap_update() perform them. The cpu-specific
cache code can then optimize this to avoid unnecessary operations.

Tested on R4000SC, R4600SC, R5000SC, RM7000, R10000 with 4KB and 16KB
page sizes (coherent and non-coherent designs), and Loongson 2F by mikeb@@ and
me. Should not affect anything on Octeon since there is no way to flush a
subset of I$ anyway.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.34 2014/03/22 00:01:04 miod Exp $ */
d136 1
a136 1
#define	PGF_PRESERVE	(PGF_ATTR_MOD | PGF_ATTR_REF | PGF_EOP_CHECKED | PGF_EOP_VULN)
@


1.34
log
@Second draft of my attempt to workaround the infamous R4000 end-of-page errata,
affecting R4000 processors revision 2.x and below (found on most R4000 Indigo
and a few R4000 Indy).

Since this errata gets triggered by TLB misses when the code flow crosses a
page boundary, this code attempts to identify code pages prone to trigger the
errata, and force the next page to be mapped for at least as long as the
current pc lies in the troublesome page, by creating wiring extra TLB entries.
These entries get recycled in a lazy-but-aggressive-enough way, either because
of context switches, or because of further tlb exceptions reaching trap().

The errata workaround code is only compiled on R4000-capable kernels (i.e.
sgi GENERIC-IP22 and nothing else), and only enabled on affected processors
(i.e. not on R4000 revision 3, or on R4400).

There is still room for improvemnt in unlucky cases, but in this simple enough
incarnation, this allows my R4000 2.2 Indigo to finally reliably boot multiuser,
even though both /sbin/init and /bin/sh contain code pages which can trigger
the errata.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.33 2014/03/21 21:49:45 miod Exp $ */
a155 2

#define	pmap_update(x)			do { /* nothing */ } while (0)
@


1.33
log
@Rename the symbolic constants for the pmap-specific vm_pag pg_flags from
PV_xxx to PGF_xxx for consistency (these are not stored in pvlist entries
anymore since years). The PG_ prefix can't be used here because of name
conflicts with <machine/pte.h> names, and I'd rather not rename the pte
constants.

No functional change. But it makes my life easier.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.32 2014/03/10 21:17:58 miod Exp $ */
d90 1
a90 1
#define NBSEG			(1UL << SEGSHIFT)
d93 3
a95 3
#define mips_trunc_seg(x)	((vaddr_t)(x) & ~SEGOFSET)
#define mips_round_seg(x)	(((vaddr_t)(x) + SEGOFSET) & ~SEGOFSET)
#define pmap_segmap(m, v)	((m)->pm_segtab->seg_tab[((v) >> SEGSHIFT)])
d98 1
a98 1
#define PMAP_SEGTABSIZE		(PMAP_L2SIZE / sizeof(void *))
d134 3
a136 1
#define	PGF_PRESERVE	(PGF_ATTR_MOD | PGF_ATTR_REF)
d142 1
a142 1
#define pmap_resident_count(pmap)       ((pmap)->pm_stats.resident_count)
d144 1
a144 1
#define pmap_kernel()			(kernel_pmap_ptr)
d148 1
a148 1
#define PMAP_PREFER(pa, va)		pmap_prefer(pa, va)
d152 1
a152 1
#define PMAP_PREFER_ALIGN()						\
d155 1
a155 1
#define PMAP_PREFER_OFFSET(of)		((of) & pmap_prefer_mask)
d164 1
a164 1
void	pmap_page_cache(vm_page_t, int);
d167 1
a167 1
#define pmap_unuse_final(p)		do { /* nothing yet */ } while (0)
d170 1
a170 1
void pmap_update_user_page(pmap_t, vaddr_t, pt_entry_t);
d172 1
a172 1
void pmap_update_kernel_page(vaddr_t, pt_entry_t);
d174 1
a174 1
#define pmap_update_kernel_page(va, entry) tlb_update(va, entry)
d197 1
a197 1
#define PMAP_PA_MASK	~((paddr_t)PAGE_MASK)
d203 1
a203 1
#define uvtopte(va)	(((va) >> PAGE_SHIFT) & (NPTEPG -1))
@


1.32
log
@Support PMAP_NOCACHE in pmap_enter() flags. If set when mapping a managed
page, the pte is created uncached.

Make sure pmap_enter_pv() honours the cache bits of the pte, instead of
assuming it will only get called for cached pages. Have it set PV_UNCACHED
in the pv flags for the page, if this is the first use of this page and the
mapping is not cached. Only check for a virtual aliasing cache condition if
the new mapping is cached.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.31 2014/02/08 09:34:04 miod Exp $ */
d129 6
a134 6
/* flags for pv_entry */
#define	PV_UNCACHED	PG_PMAP0	/* Page is explicitely uncached */
#define	PV_CACHED	PG_PMAP1	/* Page is currently cached */
#define	PV_ATTR_MOD	PG_PMAP2
#define	PV_ATTR_REF	PG_PMAP3
#define	PV_PRESERVE	(PV_ATTR_MOD | PV_ATTR_REF)
@


1.31
log
@Allow page table entries to be either 32 bits wide (the existing state of the
code), or 64 bits wide, if option MIPS_PTE64.
64-bit ptes allow for physical memory beyond 16GB (34 bits) to be addressable
by the pmap code.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.30 2014/01/30 18:16:41 miod Exp $ */
d135 2
@


1.30
log
@Move declaration of struct vm_page_md from <machine/vmparam.h> to
<machine/pmap.h> where it belongs, and compensate in <uvm/uvm_extern.h>
by including <uvm/uvm_pmap.h> before <uvm/uvm_page.h>. Tested on all
MACHINE_ARCH but amd64 and i386 (and hppa64).
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.29 2013/05/29 20:36:12 pirofti Exp $ */
d49 12
a60 4
 * virtual address bits 30..22 are used to index into a segment table which
 * points to a page worth of PTEs (4096 page can hold 1024 PTEs).
 * Bits 21..12 are then used to index a PTE which describes a page within
 * a segment.
d72 3
d76 1
d79 2
d85 3
a87 1
/* -2 below is for log2(sizeof pt_entry_t) */
d89 1
d97 1
d194 9
@


1.29
log
@Add PMAP_PA_MASK to mips64. Needed for loongson hibernate.

Okay miod@@.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.28 2012/05/10 21:12:26 miod Exp $ */
d178 20
@


1.28
log
@The uvm_map() changes introduced about two months ago yield a different
memory allocation pattern, exposing aliasing bugs in the mips64 pmap, on
kernels where virtual aliasing may happen (that is, IP32 kernels on R5000,
R10000 and R12000 O2).

Fix these (or at least, sweep them better under the rug) by:
- always flush caches in pmap_copy_page() and pmap_zero_page(), if the
  destination page is mapped uncached.
- when removing a mapping of a managed page which has been forced uncached
  due to virtual aliasing conflict, map it back cached as soon as the
  remaining mappings are non conflicting.
- writeback, instead of simply invalidating, pool pages and page table
  pages, upon release, for there might be stale data in the cache.

While these, apply more paranoia and only perform cache operations on pages
which have been mapped with the cache enabled.

Initially reported by mikeb@@ on an R12k O2 (that will teach me to use an
RM7000-powered O2, without virtual aliasing, to test IP32 kernels).
Verified on an R5k O2, as well as a custom IP30 kernel with page size
forced to 4KB (to introduce virtual aliasing).

This diff tested on O2 (R5k, RM7k, R12k), IP30, IP35, as well as on
Loongson (no aliasing) by mikeb@@ and I.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.27 2012/04/06 20:11:18 miod Exp $ */
d170 6
@


1.27
log
@Make the logic for PMAP_PREFER() and the logic, inside pmap, to do the
necessary cache coherency work wrt similar virtual indexes of different
physical pages, depending upon two distinct global variables, instead of
a shared one. R4000/R4400 VCE requires a 32KB mask for PMAP_PREFER, which
is otherwise not necessary for pmap coherency (especially since, on these
processors, only L1 uses virtual indexes, and the L1 size is not greater
than the page size, as we are using 16KB pages).
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.26 2011/04/28 20:46:30 ariane Exp $ */
d112 2
a113 2
#define	PV_UNCACHED	PG_PMAP0	/* Page is mapped unchached */
#define	PV_CACHED	PG_PMAP1	/* Page has been cached */
@


1.26
log
@Expose pmap_prefer parameters.
This will enable future uvm_map code to make intelligent decisions during
allocation.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.25 2011/03/23 16:54:36 pirofti Exp $ */
d128 1
a128 1
extern vaddr_t CpuCacheAliasMask;	/* from mips64/mips64/cpu.c */
d131 1
a131 1
	(CpuCacheAliasMask ? CpuCacheAliasMask + 1 : 0)
d133 1
a133 1
#define PMAP_PREFER_OFFSET(of)		((of) & CpuCacheAliasMask)
@


1.25
log
@Normalize sentinel. Use _MACHINE_*_H_ and _<ARCH>_*_H_ properly and consitently.

Discussed and okay drahn@@. Okay deraadt@@.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.24 2010/12/26 15:40:59 miod Exp $ */
d127 7
@


1.24
log
@Kill pmap_phys_address(), and force every driver's mmap() routine to return
a physical address [more precisely, something suitable to pass to pmap_enter()'sphysical address argument].

This allows MI drivers to implement mmap() routines without having to know
about the pmap_phys_address() implementation and #ifdef obfuscation.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.23 2010/12/06 20:57:17 miod Exp $ */
d38 2
a39 2
#ifndef	_MIPS_PMAP_H_
#define	_MIPS_PMAP_H_
d166 1
a166 1
#endif	/* !_MIPS_PMAP_H_ */
@


1.23
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.22 2010/11/28 20:30:51 miod Exp $ */
a122 1
#define	pmap_phys_address(ppn)		ptoa(ppn)
@


1.22
log
@Enable __HAVE_PMAP_DIRECT on mips64, unless the kernel is configured to
run on R5000 family processors (e.g. sgi GENERIC-IP32), where direct XKPHYS
mappings hit a silicon bug.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.21 2010/11/24 20:59:17 miod Exp $ */
d134 1
a134 1
void	pmap_prefer(vaddr_t, vaddr_t *);
@


1.21
log
@Implement a real pmap_proc_iflush() instead of relying on trap.c to perform
copious cache flushes behind our back.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.20 2010/02/02 02:49:57 syuu Exp $ */
d148 17
@


1.20
log
@Skip calling smp_rendezvous if it's not necessary.
ok miod@@
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.19 2010/01/05 06:44:58 syuu Exp $ */
d46 1
a46 6
 * The user address space is 2Gb (0x0 - 0x80000000).
 * User programs are laid out in memory as follows:
 *			address
 *	USRTEXT		0x00400000
 *	USRDATA		0x10000000
 *	USRSTACK	0x7FFFFFFF
a53 3
 * The wired entries in the TLB will contain the following:
 *	0-1	(UPAGES)	for curproc user struct and kernel stack.
 *
a138 1
#define pmap_proc_iflush(p,va,len)	do { /* nothing yet (handled in trap now) */ } while (0)
@


1.19
log
@Dynamic allocation for ASID and ASID generation number on struct pmap. ok miod@@
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.18 2009/12/30 01:17:59 syuu Exp $ */
d152 1
d154 3
@


1.18
log
@curcpu()->ci_curpmap added. ok miod@@
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.17 2009/12/28 06:55:27 syuu Exp $ */
d94 5
a105 2
	u_int			pm_tlbpid[MAXCPUS];	/* address space tag */
	u_int			pm_tlbgen[MAXCPUS];	/* TLB PID generation number */
d107 1
d110 8
d126 1
a126 1
extern	struct pmap kernel_pmap_store;
d130 1
a130 1
#define pmap_kernel()			(&kernel_pmap_store)
@


1.17
log
@MP-safe pmap implemented, enable IPI in interrupt handler to avoid deadlock.
ok miod@@
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.16 2009/12/07 18:58:32 miod Exp $ */
a102 1
	int                     pm_active;
@


1.16
log
@Use a pool to manage pmap pte pages and top level segment table, instead of
directly allocating pages from uvm; this will allow us to eventually use
a different kernel page size without having to alter the pmap structures
layout.
No functional change; measured slowdown of 1.6% for 4KB page kernels.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.15 2009/11/18 20:58:50 miod Exp $ */
d101 3
a103 2
	u_int			pm_tlbpid;	/* address space tag */
	u_int			pm_tlbgen;	/* TLB PID generation number */
d140 2
@


1.15
log
@Stricter type usage (width and signedness); first step towards 64 bit ptes.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.14 2007/10/18 04:32:09 miod Exp $ */
d67 17
d88 1
a88 1
#define PMAP_SEGTABSIZE		512
@


1.14
log
@Get rid of the silly union for mips pte. No functional change except
pmap.h now includes pte.h.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.13 2007/09/10 18:49:45 miod Exp $ */
d84 1
a84 1
	int			pm_tlbpid;	/* address space tag */
d111 1
a111 1
int	pmap_is_page_ro( pmap_t, vaddr_t, int);
@


1.13
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.12 2007/04/27 18:14:11 miod Exp $ */
d43 2
a72 2
union pt_entry;

d74 1
a74 1
	union pt_entry	*seg_tab[PMAP_SEGTABSIZE];
@


1.12
log
@More pmap changes from the potpourri pool:
- use wm_page pg_flags pmap-reserved flags to store attributes, instead of
  defining a vm_page_md field.
- use atomic operations to touch the above mentioned flags.
- never create ptes with PG_RO and PG_M set (this was harmless anyway).
- in pmap_clear_modify(), do not flush cache if the page was mapped uncached.

Tested on r5k, rm5200, r10k and r12k.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.11 2007/04/14 14:52:37 miod Exp $ */
d120 1
@


1.11
log
@Correctly handle pv_flags for pages used to store ptes, so that they
are in a correct state when we uvm_pagefree() them.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.10 2006/06/20 06:26:57 miod Exp $ */
d91 4
a94 4
#define	PV_UNCACHED	0x0001		/* Page is mapped unchached */
#define	PV_CACHED	0x0002		/* Page has been cached */
#define	PV_ATTR_MOD	0x0004
#define	PV_ATTR_REF	0x0008
@


1.10
log
@Define an empty pmap_collect().
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.9 2006/05/29 21:50:08 deraadt Exp $ */
d95 1
a95 1
#define	PV_PRESERVE (PV_UNCACHED|PV_CACHED|PV_ATTR_MOD|PV_ATTR_REF)
@


1.9
log
@clean userland namespace a bit more; ok miod
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.8 2005/11/23 11:22:10 mickey Exp $ */
d108 1
a108 1
#define	pmap_update(x)			/* nothing */
d117 3
a119 2
#define pmap_proc_iflush(p,va,len)	/* nothing yet (handled in trap now) */
#define pmap_unuse_final(p)		/* nothing yet */
@


1.8
log
@there is no vtophys here
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.7 2005/08/07 07:29:44 miod Exp $ */
d41 2
a88 1
#ifdef	_KERNEL
@


1.7
log
@Remove advertising clause from UCB licenses; ok deraad@@
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.6 2004/09/27 17:40:24 pefo Exp $ */
a117 2

paddr_t vtophys(void *);
@


1.6
log
@Change busdma to map coherent dmamem memory as uncached if the host
system has non-coherent caches. This will help some drivers to work better.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.5 2004/09/23 08:42:38 pefo Exp $ */
d19 1
a19 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.5
log
@More pmap bugs cleaned out. Some old, some new.
Better structured pmap.c.
Evil still in there, more work to do.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.4 2004/09/17 19:19:05 miod Exp $ */
d92 8
d118 1
@


1.4
log
@Long-due mips pmap potpourri:
- un-__P()ee.
- remove splimp() protection.
- allow pmap_extract() to return FALSE, dammit!
  XXX some code under arch/mips64 considers pa == 0 as failure, instead
      of checking the return value of pmap_extract(). Free commits.
- do not peek at uvm vm_page flags for ref/mod accounting, but use real
  MD flags in pv lists, and implement pmap_is_* and pmap_clear_*, so uvm
  (which knows better) does the right thing.
- debug code #ifdef PMAPDEBUG, not #ifdef DIAGNOSTIC.
- merge pmap_init() and pmap_pinit(), pmap_destroy() and pmap_release(),
  and update comments.
- better pmap_enter() logic, from NetBSD.
- do not use IS_VM_PHYSADDR() to know if we can use PHYS_TO_VM_PAGE()
  unchecked. Instead, compute PHYS_TO_VM_PAGE(), and check it against
  NULL. Saves a vm_physseg_find() call each time.
- pass vm_page_t to internal function whenever possible, rather than
  paddr_t, so that PHYS_TO_VM_PAGE() needs not be computed again.
- in the same way, do not compute PHYS_TO_KSEG0(foo) several times in a
  row (though PHYS_TO_KSEG0 is trivial).
- allow userspace address to cross 0x80000000 (may be useful in the
  future).
- implement PMAP_CANFAIL.

As a bonus:
- switch from HAVE_PMAP_PHYSSEG to HAVE_VM_PAGE_MD.
- remove dead (pre-pmap_pv_pool) code.
- KNF fixes.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.3 2004/08/10 20:28:13 deraadt Exp $ */
d96 2
a97 1
#define pmap_kernel() (&kernel_pmap_store)
d101 1
a101 1
#define PMAP_PREFER(pa, va)	pmap_prefer(pa, va)
@


1.3
log
@spacing
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.2 2004/08/06 22:39:13 deraadt Exp $ */
a90 6
/*
 * Defines for pmap_attributes[phys_mach_page];
 */
#define PMAP_ATTR_MOD	0x01	/* page has been modified */
#define PMAP_ATTR_REF	0x02	/* page has been referenced */

a91 1
extern	char *pmap_attributes;		/* reference and modify bits */
d104 5
a108 9
void pmap_prefer(vaddr_t, vaddr_t *);

void pmap_bootstrap(void);
int pmap_is_page_ro( pmap_t, vaddr_t, int);
int pmap_alloc_tlbpid(struct proc *);
void pmap_remove_pv(pmap_t, vaddr_t, vaddr_t);
int pmap_is_pa_mapped(vaddr_t);
vaddr_t pmap_pa_to_va(paddr_t);
void pmap_page_cache(vaddr_t, int);
a111 2

void pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cache);
@


1.2
log
@rename sparc kill_user_windows() to pmap_unuse_final().  provide empty stubs
on all other architectures.  remove last architecture dependent #ifdef from
uvm code.
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.1 2004/08/06 20:56:02 pefo Exp $ */
d3 1
a3 1
/* 
d56 1
a56 1
 * Bits 21..12 are then used to index a PTE which describes a page within 
@


1.1
log
@initial mips64
@
text
@d1 1
a1 1
/*      $OpenBSD: pmap.h,v 1.2 1999/01/27 04:46:05 imp Exp $ */
d121 2
a122 1
#define pmap_proc_iflush(p,va,len) /* nothing yet (handled in trap now) */
@

